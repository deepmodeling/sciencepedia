{"hands_on_practices": [{"introduction": "理论推导表明，为了防止信号在深层网络中消失或爆炸，不同的激活函数需要搭配特定的权重初始化策略。这个练习旨在通过蒙特卡洛模拟，亲手验证这一核心原则[@problem_id:3199598]。我们将经验性地观察，对于tanh和ReLU这两种常用的激活函数，Xavier初始化和He初始化是如何分别确保信号方差在网络层间稳定传播的，从而为有效的模型训练奠定基础。", "problem": "给定一个具有 $L$ 层的全连接前馈网络，将第 $l$ 层的预激活定义为 $z^{(l)} = W^{(l)} a^{(l-1)}$，将后激活定义为 $a^{(l)} = \\phi\\!\\left(z^{(l)}\\right)$，其中 $a^{(0)} = x$。假设偏置为零，权重独立同分布，输入 $x \\in \\mathbb{R}^{n}$ 的分量独立、均值为零且方差有限。考虑两种广泛使用的随机权重初始化策略：Xavier（Glorot）正态初始化和 He（Kaiming）正态初始化，以及两种激活函数：$\\tanh$ 和修正线性单元（$\\mathrm{ReLU}$）。目标是通过蒙特卡洛模拟，经验性地验证在哪种情况下，初始化策略能够近似地保持预激活在各层之间的方差不变，即在给定激活函数的情况下，对于所有层 $l \\in \\{1,\\dots,L\\}$，都有 $\\operatorname{Var}\\!\\left(z^{(l)}\\right) \\approx \\operatorname{Var}(x)$。\n\n使用的基本原理：\n- 初始化时权重和激活在各坐标上的独立性，以及独立变量和的方差线性性。\n- $\\tanh$ 和 $\\mathrm{ReLU}$ 作为逐点非线性函数的定义。\n- 对于一个数组 $Y \\in \\mathbb{R}^{s \\times d}$，沿 $s$ 个样本的样本方差估计器定义为 $\\widehat{\\operatorname{Var}}(Y) = \\frac{1}{d}\\sum_{j=1}^{d} \\left(\\frac{1}{s}\\sum_{i=1}^{s} Y_{ij}^{2} - \\left(\\frac{1}{s}\\sum_{i=1}^{s} Y_{ij}\\right)^{2} \\right)$。\n\n你的程序必须：\n1. 构建具有指定 $L$ 和层宽度的网络，其中为简化起见，每层的形状为 $(n_{\\text{in}}, n_{\\text{out}})$ 且 $n_{\\text{in}} = n_{\\text{out}}$。对每层权重 $W^{(l)}$ 使用 Xavier 正态初始化或 He 正态初始化。使用每种策略指定的方差进行零均值正态初始化；不添加任何偏置。\n2. 将输入 $x$ 抽取为 $s$ 个维度为 $n$ 的独立样本，每个分量服从 $\\mathcal{N}(0,1)$ 分布，即均值为零，方差为 1。\n3. 对于每一层 $l$，通过对每个坐标的样本方差进行平均，计算经验预激活方差 $\\widehat{\\operatorname{Var}}\\!\\left(z^{(l)}\\right)$，并类似地计算经验输入方差 $\\widehat{\\operatorname{Var}}(x)$。将第 $l$ 层的相对偏差定义为 $$\\delta^{(l)} = \\frac{\\left|\\widehat{\\operatorname{Var}}\\!\\left(z^{(l)}\\right) - \\widehat{\\operatorname{Var}}(x)\\right|}{\\widehat{\\operatorname{Var}}(x)}.$$ 如果 $\\max_{1 \\le l \\le L} \\delta^{(l)} \\le \\varepsilon$，则认为一个测试用例保持了方差，容差 $\\varepsilon = 0.25$。\n4. 使用固定种子 $12345$ 的伪随机数生成器以确保可复现性。\n\n测试套件：\n- 情况 1：$L=5$，宽度 $n=32$，样本数 $s=8000$，激活函数 $\\tanh$，初始化 Xavier 正态。\n- 情况 2：$L=5$，宽度 $n=32$，样本数 $s=8000$，激活函数 $\\tanh$，初始化 He 正态。\n- 情况 3：$L=5$，宽度 $n=32$，样本数 $s=8000$，激活函数 $\\mathrm{ReLU}$，初始化 He 正态。\n- 情况 4：$L=5$，宽度 $n=32$，样本数 $s=8000$，激活函数 $\\mathrm{ReLU}$，初始化 Xavier 正态。\n- 情况 5：$L=1$，宽度 $n=32$，样本数 $s=20000$，激活函数 $\\tanh$，初始化 Xavier 正态。\n- 情况 6：$L=1$，宽度 $n=32$，样本数 $s=20000$，激活函数 $\\mathrm{ReLU}$，初始化 He 正态。\n- 情况 7：$L=15$，宽度 $n=16$，样本数 $s=8000$，激活函数 $\\tanh$，初始化 Xavier 正态。\n- 情况 8：$L=15$，宽度 $n=16$，样本数 $s=8000$，激活函数 $\\mathrm{ReLU}$，初始化 He 正态。\n\n最终输出格式：\n你的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表（例如 $\\left[\\text{result}_1,\\text{result}_2,\\dots\\right]$），其中每个 $\\text{result}_i$ 是一个布尔值，表示第 $i$ 个测试用例是否保持了方差，顺序与上述测试套件完全一致。", "solution": "该问题被评估为有效。它在科学上基于深度学习的既定原则，特别是关于权重初始化及其对信号传播的影响。问题提法清晰，提供了所有必要的参数、定义和明确的成功标准。它没有矛盾、歧义和事实错误。因此，我们可以着手解决。\n\n目标是经验性地验证深度神经网络中各层预激活 $z^{(l)}$ 的方差得以保持的条件。此分析的核心在于连续各层预激活方差之间的递归关系。\n\n让我们考虑第 $l$ 层单个神经元的预激活：\n$$ z_i^{(l)} = \\sum_{j=1}^{n_{l-1}} W_{ij}^{(l)} a_j^{(l-1)} $$\n这里，$W_{ij}^{(l)}$ 是连接第 $l-1$ 层神经元 $j$ 和第 $l$ 层神经元 $i$ 的权重，$a_j^{(l-1)}$ 是前一层神经元 $j$ 的激活。问题规定偏置为零，权重 $W_{ij}^{(l)}$ 从零均值分布中抽取，输入分量 $x_j$（构成 $a_j^{(0)}$）也具有零均值。我们假设在初始化时，所有 $j$ 的激活 $a_j^{(l-1)}$ 与权重 $W_{ij}^{(l)}$ 相互独立且同分布。此外，如果激活 $a^{(l-1)}$ 是对称激活函数应用于零均值输入 $z^{(l-1)}$ 的输出，那么它们也将具有零均值。对于 $\\mathrm{ReLU}$ 激活函数，情况并非如此，但由于权重本身是零均值的，因此得到的预激活 $z^{(l)}$ 仍将具有零均值：$\\mathbb{E}[z_i^{(l)}] = \\sum_j \\mathbb{E}[W_{ij}^{(l)}] \\mathbb{E}[a_j^{(l-1)}] = \\sum_j 0 \\cdot \\mathbb{E}[a_j^{(l-1)}] = 0$。\n\n在这些条件下，$z_i^{(l)}$ 的方差由下式给出：\n$$ \\operatorname{Var}(z_i^{(l)}) = \\operatorname{Var}\\left(\\sum_{j=1}^{n_{l-1}} W_{ij}^{(l)} a_j^{(l-1)}\\right) $$\n由于和中各项的独立性（因为权重和前一层的激活是独立的），和的方差等于方差的和：\n$$ \\operatorname{Var}(z_i^{(l)}) = \\sum_{j=1}^{n_{l-1}} \\operatorname{Var}(W_{ij}^{(l)} a_j^{(l-1)}) $$\n对于两个独立的随机变量 $U$ 和 $V$，如果至少其中一个均值为零（例如 $\\mathbb{E}[U]=0$），则 $\\operatorname{Var}(UV) = \\mathbb{E}[U^2V^2] - (\\mathbb{E}[UV])^2 = \\mathbb{E}[U^2]\\mathbb{E}[V^2] - (\\mathbb{E}[U]\\mathbb{E}[V])^2 = \\operatorname{Var}(U)\\operatorname{Var}(V)$。由于 $\\mathbb{E}[W_{ij}^{(l)}] = 0$，我们有：\n$$ \\operatorname{Var}(z_i^{(l)}) = \\sum_{j=1}^{n_{l-1}} \\operatorname{Var}(W_{ij}^{(l)}) \\operatorname{Var}(a_j^{(l-1)}) $$\n假设第 $l$ 层的所有权重都是从方差为 $\\operatorname{Var}(W^{(l)})$ 的分布中独立同分布抽取的，并且第 $l-1$ 层的所有激活都是方差为 $\\operatorname{Var}(a^{(l-1)})$ 的独立同分布，则上式简化为：\n$$ \\operatorname{Var}(z^{(l)}) = n_{l-1} \\operatorname{Var}(W^{(l)}) \\operatorname{Var}(a^{(l-1)}) $$\n其中 $n_{l-1}$ 是第 $l-1$ 层的神经元数量。为了保持稳定的信号传播，我们需要方差保持不变，即 $\\operatorname{Var}(z^{(l)}) \\approx \\operatorname{Var}(z^{(l-1)})$。这需要仔细选择权重初始化方差 $\\operatorname{Var}(W^{(l)})$，以抵消激活函数 $\\phi$ 对方差的影响，该影响由项 $\\operatorname{Var}(a^{(l-1)}) = \\operatorname{Var}(\\phi(z^{(l-1)}))$ 捕获。\n\n**激活函数分析**\n\n1.  **$\\tanh$ 激活函数：** 双曲正切函数 $\\tanh(z)$ 关于原点对称（$\\tanh(0)=0$），并且在小输入时其行为类似于恒等函数（当 $z \\approx 0$ 时，$\\tanh(z) \\approx z$）。如果我们假设预激活 $z^{(l-1)}$ 集中在零附近，这是初始训练阶段的理想状态，那么 $\\operatorname{Var}(a^{(l-1)}) = \\operatorname{Var}(\\tanh(z^{(l-1)})) \\approx \\operatorname{Var}(z^{(l-1)})$。将此代入我们的传播方程可得：\n    $$ \\operatorname{Var}(z^{(l)}) \\approx n_{l-1} \\operatorname{Var}(W^{(l)}) \\operatorname{Var}(z^{(l-1)}) $$\n    为了实现 $\\operatorname{Var}(z^{(l)}) \\approx \\operatorname{Var}(z^{(l-1)})$，我们必须设置 $n_{l-1} \\operatorname{Var}(W^{(l)}) = 1$。\n    **Xavier (Glorot) 正态初始化** 正是为这种情况设计的。它将从 $\\mathcal{N}(0, \\sigma^2)$ 中抽取的权重 $W^{(l)}$ 的方差设置为：\n    $$ \\operatorname{Var}(W^{(l)}) = \\frac{2}{n_{l-1} + n_l} $$\n    在我们的特定问题中，$n_{l-1} = n_l = n$，因此这变为 $\\operatorname{Var}(W^{(l)}) = \\frac{2}{2n} = \\frac{1}{n} = \\frac{1}{n_{l-1}}$。这个选择完美地满足了条件 $n_{l-1} \\operatorname{Var}(W^{(l)}) = 1$。因此，Xavier 初始化有望为 $\\tanh$ 激活函数保持方差。\n\n2.  **$\\mathrm{ReLU}$ 激活函数：** 修正线性单元 $\\mathrm{ReLU}(z) = \\max(0, z)$ 不是对称的。对于一个零均值、对称的输入分布 $z^{(l-1)}$（如高斯分布），恰好一半的输入将被置为零。这会影响方差。令 $z \\sim \\mathcal{N}(0, \\sigma_z^2)$。输出 $a = \\mathrm{ReLU}(z)$ 的方差为 $\\operatorname{Var}(a) = \\mathbb{E}[a^2] - (\\mathbb{E}[a])^2$。\n    平方激活的期望为 $\\mathbb{E}[a^2] = \\mathbb{E}[\\max(0, z)^2] = \\int_0^\\infty z^2 p(z) dz$。由于正态分布 $p(z)$ 的对称性，这个积分是 $z^2$ 的总积分的一半：$\\mathbb{E}[a^2] = \\frac{1}{2} \\int_{-\\infty}^\\infty z^2 p(z) dz = \\frac{1}{2}\\mathbb{E}[z^2] = \\frac{1}{2}\\operatorname{Var}(z)$。\n    因此，对于 $\\mathrm{ReLU}$，我们有 $\\operatorname{Var}(a^{(l-1)}) \\approx \\frac{1}{2}\\operatorname{Var}(z^{(l-1)})$。方差传播方程变为：\n    $$ \\operatorname{Var}(z^{(l)}) \\approx n_{l-1} \\operatorname{Var}(W^{(l)}) \\left(\\frac{1}{2}\\operatorname{Var}(z^{(l-1)})\\right) $$\n    为了保持方差，我们必须有 $n_{l-1} \\operatorname{Var}(W^{(l)}) \\frac{1}{2} = 1$，这意味着 $\\operatorname{Var}(W^{(l)}) = \\frac{2}{n_{l-1}}$。\n    **He (Kaiming) 正态初始化** 是为这种情况设计的。它将方差设置为：\n    $$ \\operatorname{Var}(W^{(l)}) = \\frac{2}{n_{l-1}} $$\n    这个选择恰好满足条件。因此，He 初始化有望为 $\\mathrm{ReLU}$ 激活函数保持方差。不匹配的组合（例如，$\\tanh$ 与 He，$\\mathrm{ReLU}$ 与 Xavier）预计将分别导致方差爆炸或消失。\n\n**模拟程序**\n\n程序将为 8 个测试用例中的每一个实现蒙特卡洛模拟。对于每种情况：\n1.  使用值 $12345$ 为伪随机数生成器设定种子，以确保可复现性。\n2.  生成一个输入数据矩阵 $x \\in \\mathbb{R}^{s \\times n}$，其中每个元素都从 $\\mathcal{N}(0, 1)$ 中抽取。使用提供的公式计算经验输入方差 $\\widehat{\\operatorname{Var}}(x)$。\n3.  网络逐层处理，从 $l=1$ 到 $L$。在每一层中，权重矩阵 $W^{(l)}$ 根据指定策略（Xavier 或 He）所要求的方差，从零均值正态分布中初始化。\n4.  计算预激活 $z^{(l)} = a^{(l-1)} W^{(l)}$。\n5.  计算经验方差 $\\widehat{\\operatorname{Var}}(z^{(l)})$。计算相对偏差 $\\delta^{(l)} = |\\widehat{\\operatorname{Var}}(z^{(l)}) - \\widehat{\\operatorname{Var}}(x)| / \\widehat{\\operatorname{Var}}(x)$。\n6.  跟踪所有层中的最大相对偏差 $\\max_{1 \\le l \\le L} \\delta^{(l)}$。\n7.  计算后激活 $a^{(l)} = \\phi(z^{(l)})$，作为下一层的输入。\n8.  遍历所有层后，如果 $\\max_{l} \\delta^{(l)} \\le \\varepsilon$（其中 $\\varepsilon = 0.25$），则认为该测试用例保持了方差。此检查将为每个用例生成一个布尔结果，然后报告该结果。", "answer": "```python\nimport numpy as np\n\ndef tanh(x):\n    \"\"\"Hyperbolic tangent activation function.\"\"\"\n    return np.tanh(x)\n\ndef relu(x):\n    \"\"\"Rectified Linear Unit (ReLU) activation function.\"\"\"\n    return np.maximum(0, x)\n\ndef empirical_variance(Y):\n    \"\"\"\n    Computes the empirical variance as defined in the problem statement.\n    For an array Y of shape (s, d), this is the average of the biased sample \n    variances of each of the d columns.\n    \n    Formula: Var(Y) = (1/d) * sum_j [ (1/s) * sum_i(Y_ij^2) - ((1/s) * sum_i(Y_ij))^2 ]\n    This is equivalent to the mean of np.var(Y, axis=0).\n    \"\"\"\n    if Y.ndim == 1:\n        # If input is a 1D array, treat it as (s, 1)\n        return np.var(Y)\n    return np.mean(np.var(Y, axis=0))\n\ndef run_simulation(L, n, s, activation_func, init_strategy, seed, epsilon):\n    \"\"\"\n    Runs a single simulation for a given network configuration.\n\n    Args:\n        L (int): Number of layers.\n        n (int): Width of each layer.\n        s (int): Number of samples.\n        activation_func (callable): The activation function to use.\n        init_strategy (str): The weight initialization strategy ('xavier' or 'he').\n        seed (int): The seed for the random number generator.\n        epsilon (float): The tolerance for variance preservation.\n\n    Returns:\n        bool: True if variance is preserved, False otherwise.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    \n    # Generate input data x of shape (s, n) from N(0, 1)\n    x = rng.normal(loc=0.0, scale=1.0, size=(s, n))\n    var_x = empirical_variance(x)\n    \n    a_prev = x\n    max_deviation = 0.0\n    \n    # Propagate through the network for L layers\n    for l in range(1, L + 1):\n        # Layer dimensions are (n_in, n_out)\n        n_in = n\n        n_out = n\n        \n        # Determine standard deviation for weight initialization\n        if init_strategy == 'xavier':\n            # Variance = 2 / (n_in + n_out)\n            std_dev = np.sqrt(2.0 / (n_in + n_out))\n        elif init_strategy == 'he':\n            # Variance = 2 / n_in\n            std_dev = np.sqrt(2.0 / n_in)\n        else:\n            raise ValueError(f\"Unknown initialization strategy: {init_strategy}\")\n            \n        # Initialize weights W of shape (n_in, n_out)\n        W = rng.normal(loc=0.0, scale=std_dev, size=(n_in, n_out))\n        \n        # Compute pre-activations z = a_prev @ W of shape (s, n_out)\n        z = a_prev @ W\n        \n        # Compute empirical variance and relative deviation\n        var_z = empirical_variance(z)\n        if var_x > 1e-9: # Avoid division by zero\n            deviation = np.abs(var_z - var_x) / var_x\n        else:\n            deviation = np.abs(var_z)\n\n        if deviation > max_deviation:\n            max_deviation = deviation\n            \n        # Compute post-activations for the next layer\n        a_prev = activation_func(z)\n        \n    # Check if the maximum deviation is within the tolerance\n    return max_deviation = epsilon\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    # Define test cases as (L, n, s, activation_name, init_strategy_name)\n    test_cases = [\n        (5, 32, 8000, 'tanh', 'xavier'),\n        (5, 32, 8000, 'tanh', 'he'),\n        (5, 32, 8000, 'relu', 'he'),\n        (5, 32, 8000, 'relu', 'xavier'),\n        (1, 32, 20000, 'tanh', 'xavier'),\n        (1, 32, 20000, 'relu', 'he'),\n        (15, 16, 8000, 'tanh', 'xavier'),\n        (15, 16, 8000, 'relu', 'he'),\n    ]\n    \n    # Map string names to functions and parameters\n    activation_map = {'tanh': tanh, 'relu': relu}\n    epsilon = 0.25\n    seed = 12345\n    \n    results = []\n    \n    # Run simulation for each test case\n    for case in test_cases:\n        L, n, s, act_name, init_name = case\n        activation_func = activation_map[act_name]\n        \n        # Each test case is an independent experiment, so we use the same seed,\n        # creating a new RNG instance for each to ensure reproducibility.\n        result = run_simulation(L, n, s, activation_func, init_name, seed, epsilon)\n        results.append(result)\n        \n    # Format and print the final output as a single-line list of booleans\n    # e.g., [True,False,True,False,True,True,True,True]\n    # np.bool_ maps to Python's True/False, and str() correctly converts them.\n    # The problem asks for booleans, and Python's `str(True)` is 'True'.\n    print(f\"[{','.join(map(str, results))}]\")\n\n# Execute the solution\nsolve()\n```", "id": "3199598"}, {"introduction": "良好的初始化不仅关乎信号的稳定传播，更决定了网络在训练初期学习有效特征的能力。本练习将引导我们探索一个更深层次的问题：权重方差如何影响ReLU网络中神经元的初始“激活”比例，以及这种稀疏性如何关联到学习到的特征空间的“有效维度”[@problem_id:3199561]。通过测量激活稀疏度和特征矩阵的“稳定秩”（stable rank），我们将量化评估初始化对网络初始表征能力的关键影响。", "problem": "您需要设计并实现一个计算协议，用于测量在不同权重方差下修正线性单元 (ReLU) 层初始化时的神经元激活稀疏度，并将此稀疏度与训练第一个周期后特征的有效维度相关联。该协议必须实例化为一个完整的可运行程序，该程序创建一个合成数据集，初始化一个单隐藏层网络，为不同的方差超参数测量初始化稀疏度，对均方误差目标执行一次全批量梯度下降更新，然后量化在单次更新后获得的隐藏表示的有效维度。程序必须输出一行包含所有测试用例结果的聚合数据，其格式必须与下文指定的完全一致。\n\n推导和协议设计的基本依据：\n- 修正线性单元 (ReLU) 非线性由函数 $ \\phi(z) = \\max(0,z) $ 定义，并逐元素应用。\n- 如果 $ z $ 是一个零均值、对称的实值随机变量（例如，高斯分布 $ \\mathcal{N}(0,\\sigma_z^2) $），那么 $ \\mathbb{P}(z  0) = \\mathbb{P}(z  0) = \\tfrac{1}{2} $。\n- 对于具有独立同分布 (i.i.d.) 条目的独立输入 $ x \\in \\mathbb{R}^d $ 和权重 $ w \\in \\mathbb{R}^d $（其条目独立、零均值且对称），根据中心极限定理，预激活值 $ z = w^\\top x $ 近似对称且零均值，其方差由权重方差控制。\n- 特征矩阵 $ H \\in \\mathbb{R}^{N \\times m} $ 的有效维度可以通过稳定秩来量化，其定义为 $ \\mathrm{sr}(H) = \\lVert H \\rVert_F^2 / \\lVert H \\rVert_2^2 $，其中 $ \\lVert \\cdot \\rVert_F $ 是弗罗贝尼乌斯范数，$ \\lVert \\cdot \\rVert_2 $ 是谱范数（最大奇异值）。该度量对 $ H $ 的全局缩放不变，并被广泛用于捕捉奇异值的分布离散程度，从而指示有效维度。\n\n需要实现的协议：\n1. 生成一个合成数据集 $ X \\in \\mathbb{R}^{N \\times d} $，其条目为独立同分布的标准正态分布，以及一个回归目标 $ y \\in \\mathbb{R}^N $，定义为 $ y = X u $，其中 $ u \\in \\mathbb{R}^d $ 的条目为独立同分布的标准正态分布。使用固定的随机种子以确保可复现性。\n2. 考虑一个具有 $ m $ 个隐藏单元的单隐藏层网络，参数为 $ W \\in \\mathbb{R}^{m \\times d} $ 和 $ v \\in \\mathbb{R}^m $，输出为 $ \\hat{y} = \\phi(X W^\\top) v $。将偏置初始化为 $ 0 $。\n3. 对于每个测试用例的权重方差参数 $ \\sigma^2 $：\n   - 用独立同分布的条目 $ \\sim \\mathcal{N}\\!\\left(0, \\frac{\\sigma^2}{d}\\right) $ 初始化 $ W $，并用独立同分布的条目 $ \\sim \\mathcal{N}\\!\\left(0, \\frac{1}{m}\\right) $ 初始化 $ v $。这种缩放使预激活值的方差近似地由 $ \\sigma^2 $ 控制。\n   - 计算初始化时的隐藏激活值 $ H = \\phi(X W^\\top) $。\n   - 测量初始化稀疏度，即 $ H $ 中零的比例，$ s = \\frac{1}{N m} \\sum_{i=1}^N \\sum_{j=1}^m \\mathbf{1}\\{ H_{ij} = 0 \\} $。\n   - 对均方误差 (MSE) 目标 $ L = \\frac{1}{N} \\lVert \\hat{y} - y \\rVert_2^2 $ 执行一次学习率 $ \\alpha  0 $ 的全批量梯度下降步骤，以获得更新后的参数 $ W' $ 和 $ v' $。\n   - 计算更新后的隐藏激活值 $ H' = \\phi(X (W')^\\top) $。通过减去列均值来中心化 $ H' $ 以获得 $ \\tilde{H}' $，并将稳定秩 $ r = \\lVert \\tilde{H}' \\rVert_F^2 / \\lVert \\tilde{H}' \\rVert_2^2 $ 作为第一个周期后的有效维度。\n   - 记录数据对 $ [s, r] $。\n4. 汇总所有测试用例的结果，并在一行中以指定的确切格式打印出来。\n\n具体参数值和测试套件：\n- 使用 $ N = 512 $ 个样本，$ d = 128 $ 输入维度，$ m = 256 $ 个隐藏单元，以及学习率 $ \\alpha = 0.01 $。\n- 使用以下权重方差 $ \\sigma^2 $ 的测试套件：$ [0.01, 0.5, 2.0, 5.0, 10.0] $。该集合覆盖了小方差、中等方差（包括与修正线性单元 (ReLU) 的 He 初始化相当的值，其对预激活值使用 $ \\sigma^2 \\approx 2 $）以及大方差范围。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果，每个元素对应一个测试用例，本身是一个包含两个浮点数的列表 $ [s, r] $，其中 $ s $ 是初始化稀疏度，$ r $ 是一个周期后的稳定秩。例如，输出形式必须为 $ [[s_1,r_1],[s_2,r_2],\\dots] $，不含任何附加文本。\n\n此问题不涉及物理单位或角度。所有报告的量都必须是实数。程序必须是自包含的，仅使用指定的库，并且不得要求任何输入。", "solution": "该问题要求设计并实现一个计算实验，以研究单隐藏层修正线性单元 (ReLU) 网络中初始权重的方差、初始化时产生的激活稀疏度以及单步梯度下降后隐藏层表示的有效维度之间的关系。\n\n解决方案首先建立协议各部分的理论基础，然后进行计算实现。\n\n**理论框架**\n\n1.  **数据集和网络模型**：我们构建一个合成回归问题。输入数据矩阵 $X \\in \\mathbb{R}^{N \\times d}$ 包含 $N$ 个维度为 $d$ 的样本，其条目独立地从标准正态分布 $X_{ij} \\sim \\mathcal{N}(0, 1)$ 中抽取。目标向量 $y \\in \\mathbb{R}^N$ 生成为输入的线性组合，$y = X u$，其中真实权重向量 $u \\in \\mathbb{R}^d$ 的条目也从 $\\mathcal{N}(0, 1)$ 中抽取。这种设置为实验提供了一个简单、良好控制的环境。\n\n    网络模型是一个具有 $m$ 个隐藏单元且无偏置项的单隐藏层感知器。输出由 $\\hat{y} = \\phi(X W^\\top) v$ 给出，其中 $W \\in \\mathbb{R}^{m \\times d}$ 是输入到隐藏层的权重矩阵，$v \\in \\mathbb{R}^m$ 是隐藏层到输出层的权重向量，$\\phi(z) = \\max(0, z)$ 是逐元素应用的 ReLU 激活函数。\n\n2.  **权重初始化和激活稀疏度**：权重 $W$ 使用来自零均值正态分布的独立同分布 (i.i.d.) 条目进行初始化，$W_{jk} \\sim \\mathcal{N}(0, \\sigma^2/d)$。缩放因子 $1/d$ 至关重要。考虑样本 $i$ 和神经元 $j$ 的预激活值 $z_{ij} = \\sum_{k=1}^d W_{jk} X_{ik}$。由于 $W_{jk}$ 和 $X_{ik}$ 都是独立的零均值随机变量，它们乘积的期望为 $\\mathbb{E}[W_{jk}X_{ik}] = \\mathbb{E}[W_{jk}]\\mathbb{E}[X_{ik}] = 0$，因此 $\\mathbb{E}[z_{ij}] = 0$。\n\n    方差由下式给出：\n    $$ \\mathrm{Var}(z_{ij}) = \\sum_{k=1}^d \\mathrm{Var}(W_{jk} X_{ik}) = \\sum_{k=1}^d \\mathbb{E}[(W_{jk} X_{ik})^2] - (\\mathbb{E}[W_{jk}X_{ik}])^2 $$\n    $$ \\mathrm{Var}(z_{ij}) = \\sum_{k=1}^d \\mathbb{E}[W_{jk}^2] \\mathbb{E}[X_{ik}^2] = \\sum_{k=1}^d \\mathrm{Var}(W_{jk}) \\mathrm{Var}(X_{ik}) = \\sum_{k=1}^d \\left(\\frac{\\sigma^2}{d}\\right)(1) = d \\cdot \\frac{\\sigma^2}{d} = \\sigma^2 $$\n    根据中心极限定理，对于较大的 $d$，预激活值 $z_{ij}$ 近似服从正态分布，$z_{ij} \\approx \\mathcal{N}(0, \\sigma^2)$。如果 ReLU 神经元的预激活值为非正数，它就会变得“非激活”或“死亡”。由于 $\\mathcal{N}(0, \\sigma^2)$ 是一个以零为中心的对称分布，发生这种情况的概率是 $\\mathbb{P}(z_{ij} \\le 0) = 0.5$。因此，后激活矩阵 $H = \\phi(X W^\\top)$ 中零条目的预期比例，即初始化稀疏度 $s$，理论上是 $0.5$，与方差参数 $\\sigma^2$ 无关。本实验将凭经验测量该值。\n\n3.  **梯度下降更新**：对均方误差 (MSE) 损失函数 $L = \\frac{1}{N} \\lVert \\hat{y} - y \\rVert_2^2$ 执行一次全批量梯度下降步骤。需要计算损失函数关于参数 $W$ 和 $v$ 的梯度。设 $e = \\hat{y} - y$ 为误差向量，$Z = XW^\\top$ 为预激活矩阵。使用链式法则推导梯度：\n\n    -   对于输出层权重 $v$：\n        $$ \\frac{\\partial L}{\\partial v} = \\frac{2}{N} H^\\top e $$\n    -   对于输入层权重 $W$：\n        $$ \\frac{\\partial L}{\\partial W} = \\left( \\left( \\frac{2}{N} e v^\\top \\right) \\odot \\phi'(Z) \\right)^\\top X $$\n        其中 $\\odot$ 表示哈达玛（逐元素）积，$\\phi'(Z)$ 是 ReLU 函数的逐元素导数，它是一个指示函数矩阵 $\\mathbf{1}_{Z_{ij}0}$。\n\n    使用指定的学习率 $\\alpha$，参数更新如下：$W' = W - \\alpha \\frac{\\partial L}{\\partial W}$ 和 $v' = v - \\alpha \\frac{\\partial L}{\\partial v}$。\n\n4.  **有效维度**：更新后，计算新的隐藏激活值 $H' = \\phi(X(W')^\\top)$。为了评估这些激活向量所张成的特征空间的维度，我们使用稳定秩。首先，通过减去每列（特征）的均值来中心化激活矩阵，得到 $\\tilde{H}'$。然后计算稳定秩 $r$：\n    $$ r = \\mathrm{sr}(\\tilde{H}') = \\frac{\\lVert \\tilde{H}' \\rVert_F^2}{\\lVert \\tilde{H}' \\rVert_2^2} = \\frac{\\sum_{i=1}^{\\min(N, m)} \\lambda_i}{\\lambda_{\\max}} $$\n    其中 $\\lVert \\cdot \\rVert_F$ 是弗罗贝尼乌斯范数，$\\lVert \\cdot \\rVert_2$ 是谱范数（最大奇异值），$\\{\\lambda_i\\}$ 是协方差矩阵 $\\tilde{H}'^\\top \\tilde{H}'$ 的特征值（它们是 $\\tilde{H}'$ 奇异值的平方）。稳定秩衡量了矩阵的能量在其奇异值之间分布的均匀程度。一个接近矩阵真实秩的值表示一个条件良好的特征空间，其奇异值分布均匀；而一个接近 $1$ 的值则表明矩阵的能量集中在单个主导方向上。\n\n整个协议系统地改变初始权重方差 $\\sigma^2$，以通过梯度更新机制，观察其对由稳定秩量化的学习特征表示结构的影响。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements the computational protocol to measure neuron activation sparsity\n    at initialization and effective dimensionality after one training epoch.\n    \"\"\"\n    # 4. Concrete parameter values and test suite\n    N = 512  # number of samples\n    d = 128  # input dimension\n    m = 256  # number of hidden units\n    alpha = 0.01  # learning rate\n    sigma_squared_values = [0.01, 0.5, 2.0, 5.0, 10.0]\n\n    # Use a fixed random seed for reproducibility of the entire experiment.\n    rng = np.random.default_rng(seed=42)\n\n    # 1. Generate a single synthetic dataset for all test cases.\n    X = rng.standard_normal(size=(N, d))\n    u = rng.standard_normal(size=(d,))\n    y = X @ u\n\n    # Utility function for the ReLU activation.\n    def relu(z):\n        return np.maximum(0, z)\n\n    # Utility function for the derivative of the ReLU activation.\n    def relu_prime(z):\n        return (z > 0).astype(z.dtype)\n\n    results = []\n    # 3. Loop through each test case for the weight variance parameter.\n    for sigma_sq in sigma_squared_values:\n        # Initialize network parameters W and v for the current test case.\n        # The initialization must be inside the loop to be fresh for each sigma_sq.\n        w_std = np.sqrt(sigma_sq / d)\n        v_std = np.sqrt(1 / m)\n        W = rng.normal(loc=0.0, scale=w_std, size=(m, d))\n        v = rng.normal(loc=0.0, scale=v_std, size=(m,))\n\n        # Compute pre-activations and activations at initialization.\n        Z = X @ W.T\n        H = relu(Z)\n\n        # Measure initialization sparsity (s).\n        s = np.mean(H == 0)\n\n        # Perform one full-batch gradient descent step.\n        # Compute the prediction and the error.\n        y_hat = H @ v\n        e = y_hat - y\n\n        # Compute gradients.\n        grad_v = (2 / N) * (H.T @ e)\n        \n        grad_Z_elementwise = (2 / N) * np.outer(e, v)\n        grad_Z = grad_Z_elementwise * relu_prime(Z)\n        grad_W = grad_Z.T @ X\n\n        # Update parameters to get W' and v'.\n        W_prime = W - alpha * grad_W\n        # v_prime is not used for the final calculation but is part of the update.\n        # v_prime = v - alpha * grad_v\n\n        # Compute updated hidden activations H' with the new weights W'.\n        H_prime = relu(X @ W_prime.T)\n\n        # Center H' by subtracting column means to get tilde(H').\n        H_prime_centered = H_prime - H_prime.mean(axis=0)\n\n        # Compute the Frobenius norm squared of the centered activations.\n        fro_norm_sq = np.sum(H_prime_centered**2)\n\n        # To compute the spectral norm squared, we find the largest eigenvalue\n        # of the covariance matrix H_tilde.T @ H_tilde. This is more stable\n        # and efficient than a full SVD for this purpose.\n        # The matrix is symmetric and positive semi-definite.\n        covariance_matrix = H_prime_centered.T @ H_prime_centered\n        eigenvalues = np.linalg.eigvalsh(covariance_matrix)\n        spec_norm_sq = np.max(eigenvalues)\n\n        # Compute the stable rank (r), handling the case of a zero matrix.\n        if spec_norm_sq > 1e-12:\n            r = fro_norm_sq / spec_norm_sq\n        else:\n            # If the largest eigenvalue is zero, all are zero. This implies\n            # the matrix is zero, so its effective dimension is zero.\n            r = 0.0\n\n        # Record the pair [s, r].\n        results.append([s, r])\n    \n    # 4. Aggregate and print results in the specified format.\n    # The map(str, ...) converts each inner list [s, r] into its string\n    # representation, e.g., \"[0.5, 128.3]\".\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```", "id": "3199561"}, {"introduction": "在验证了标准初始化方案的有效性后，我们能否更进一步，从第一性原理出发，为网络设计一个满足特定动态特性的全新初始化策略？本练习将挑战我们去推导一组权重方差，使得网络中所有参数在训练开始时都具有大致相同的“有效学习率”，即拥有相等的预期平方更新量 $E[(\\Delta w^{(\\ell)}_{ij})^{2}]$ [@problem_id:3199526]。这项任务要求我们精确地平衡前向传播的激活方差和反向传播的梯度方差，从而将理论知识应用于实践设计中。", "problem": "考虑一个具有三个仿射层的全连接前馈网络。设输入层的宽度为 $n_{0} = 256$，第一个隐藏层的宽度为 $n_{1} = 128$，第二个隐藏层的宽度为 $n_{2} = 64$，输出层的宽度为 $n_{3} = 10$。隐藏层使用修正线性单元（ReLU）激活函数，输出层是线性的。将第 $\\ell$ 层的激活表示为 $a^{(\\ell)}$，预激活表示为 $z^{(\\ell)}$。第 $\\ell$ 层的权重从方差为 $\\sigma_{\\ell}^{2}$ 的零均值高斯分布中独立同分布地初始化，偏置初始化为零。假设在初始化时以下条件成立：\n- 输入具有独立同分布的分量，其均值为零，二阶矩为 $E\\!\\left[(a^{(0)}_{i})^{2}\\right] = F_{0} = 1$。\n- 预激活 $z^{(\\ell)}$ 是零均值且对称的，对于 ReLU，可以取 $E\\!\\left[\\phi(z)^{2}\\right] = \\frac{1}{2}E\\!\\left[z^{2}\\right]$ 和 $E\\!\\left[\\phi'(z)^{2}\\right] = \\frac{1}{2}$。\n- 对于所使用的损失函数，在初始化时，输出层的反向传播梯度具有二阶矩 $E\\!\\left[(\\delta^{(3)}_{j})^{2}\\right] = B_{3} = 1$，其中 $\\delta^{(\\ell)}$ 表示第 $\\ell$ 层的反向传播误差信号。\n- 独立性近似成立，因此混合二阶矩可以分解（例如，$E\\!\\left[(\\delta^{(\\ell)}_{j} a^{(\\ell-1)}_{i})^{2}\\right] \\approx E\\!\\left[(\\delta^{(\\ell)}_{j})^{2}\\right] E[(a^{(\\ell-1)}_{i})^{2}]$）。\n\n假设训练使用学习率为 $\\eta = 0.02$ 的随机梯度下降（SGD），权重更新为 $\\Delta w^{(\\ell)}_{ij} = -\\eta \\frac{\\partial \\mathcal{L}}{\\partial w^{(\\ell)}_{ij}}$。您需要设计初始化方差 $\\sigma_{1}^{2}, \\sigma_{2}^{2}, \\sigma_{3}^{2}$，使得在初始化时，期望平方更新 $E\\!\\left[(\\Delta w^{(\\ell)}_{ij})^{2}\\right]$ 在所有层上都相等，并等于目标值 $U = 1 \\times 10^{-6}$。\n\n仅从 SGD 更新规则、标准反向传播恒等式以及上述独立性假设出发，推导使 $E\\!\\left[(\\Delta w^{(\\ell)}_{ij})^{2}\\right]$ 在各层间保持一致所需的对 $\\sigma_{\\ell}^{2}$ 的约束，针对该架构求解这些约束，并计算 $\\sigma_{1}^{2}, \\sigma_{2}^{2}, \\sigma_{3}^{2}$ 的值。将您的最终答案表示为行向量 $(\\sigma_{1}^{2}, \\sigma_{2}^{2}, \\sigma_{3}^{2})$，并将每个条目四舍五入到四位有效数字。不需要单位。", "solution": "用户希望求解神经网络的初始化方差。\n\n### 步骤 1：提取已知条件\n- 网络架构：一个 $3$ 层的全连接前馈网络。\n- 层的宽度：$n_{0} = 256$（输入），$n_{1} = 128$（隐藏），$n_{2} = 64$（隐藏），$n_{3} = 10$（输出）。\n- 激活函数：隐藏层（$1$ 和 $2$）使用 ReLU，输出层（$3$）为线性。\n- 权重初始化：$w^{(\\ell)}_{ij} \\sim \\mathcal{N}(0, \\sigma_{\\ell}^{2})$，对于每一层 $\\ell \\in \\{1, 2, 3\\}$ 均为独立同分布。\n- 偏置初始化：$b^{(\\ell)}_{i} = 0$。\n- 输入统计量：$a^{(0)}$ 的分量是独立同分布的，其均值为 $E[a^{(0)}_{i}] = 0$，二阶矩为 $E[(a^{(0)}_{i})^{2}] = F_{0} = 1$。\n- 对于零均值、对称的预激活 $z$ 的近似：\n    - 对于 ReLU，$E[\\phi(z)^{2}] = \\frac{1}{2}E[z^{2}]$。\n    - 对于 ReLU，$E[\\phi'(z)^{2}] = \\frac{1}{2}$。\n- 输出梯度统计量：$E[(\\delta^{(3)}_{j})^{2}] = B_{3} = 1$。\n- 独立性近似：$E[(\\delta^{(\\ell)}_{j} a^{(\\ell-1)}_{i})^{2}] \\approx E[(\\delta^{(\\ell)}_{j})^{2}] E[(a^{(\\ell-1)}_{i})^{2}]$。\n- SGD 参数：学习率 $\\eta = 0.02$，权重更新 $\\Delta w^{(\\ell)}_{ij} = -\\eta \\frac{\\partial \\mathcal{L}}{\\partial w^{(\\ell)}_{ij}}$。\n- 目标条件：期望平方更新 $E[(\\Delta w^{(\\ell)}_{ij})^{2}]$ 在所有层上都一致，且等于 $U = 1 \\times 10^{-6}$。\n\n### 步骤 2：使用提取的已知条件进行验证\n该问题定义明确，在深度学习理论领域具有科学依据。它研究了初始化时的梯度动态，这是设计初始化方案（如 Glorot/Bengio 和 He 初始化）的核心课题。所给条件是完整、一致的，并且没有违反任何科学或数学原理。为 ReLU 激活及其导数的矩所提供的近似值是该研究领域的标准做法。该问题可以形式化，并导出一个唯一的、有意义的解。\n\n### 步骤 3：结论与行动\n问题是有效的。下面是完整的、有理有据的解答。\n\n目标是找到权重方差 $\\sigma_{1}^{2}$、$\\sigma_{2}^{2}$ 和 $\\sigma_{3}^{2}$，以满足对每一层 $\\ell \\in \\{1, 2, 3\\}$ 都有 $E[(\\Delta w^{(\\ell)}_{ij})^{2}] = U$ 的条件。\n\n首先，我们用网络中的量来表示期望平方权重更新。权重更新规则由 $\\Delta w^{(\\ell)}_{ij} = -\\eta \\frac{\\partial \\mathcal{L}}{\\partial w^{(\\ell)}_{ij}}$ 给出。期望平方更新为：\n$$E[(\\Delta w^{(\\ell)}_{ij})^{2}] = E\\left[\\left(-\\eta \\frac{\\partial \\mathcal{L}}{\\partial w^{(\\ell)}_{ij}}\\right)^{2}\\right] = \\eta^{2} E\\left[\\left(\\frac{\\partial \\mathcal{L}}{\\partial w^{(\\ell)}_{ij}}\\right)^{2}\\right]$$\n损失 $\\mathcal{L}$ 相对于权重 $w^{(\\ell)}_{ij}$（连接第 $\\ell-1$ 层的神经元 $j$ 和第 $\\ell$ 层的神经元 $i$）的梯度的标准反向传播规则是 $\\frac{\\partial \\mathcal{L}}{\\partial w^{(\\ell)}_{ij}} = \\delta^{(\\ell)}_{i} a^{(\\ell-1)}_{j}$，其中 $\\delta^{(\\ell)}_{i} = \\frac{\\partial \\mathcal{L}}{\\partial z^{(\\ell)}_{i}}$ 是第 $\\ell$ 层神经元 $i$ 的预激活 $z^{(\\ell)}_{i}$ 处的误差信号，而 $a^{(\\ell-1)}_{j}$ 是第 $\\ell-1$ 层神经元 $j$ 的激活。\n\n将此代入期望平方更新的表达式，并应用给定的独立性近似：\n$$E[(\\Delta w^{(\\ell)}_{ij})^{2}] = \\eta^{2} E[(\\delta^{(\\ell)}_{i} a^{(\\ell-1)}_{j})^{2}] \\approx \\eta^{2} E[(\\delta^{(\\ell)}_{i})^{2}] E[(a^{(\\ell-1)}_{j})^{2}]$$\n我们将第 $\\ell$ 层激活的平均二阶矩（即方差，因为均值为零）定义为 $F_{\\ell} = E[(a^{(\\ell)}_{k})^{2}]$，并将第 $\\ell$ 层反向传播误差信号的平均二阶矩定义为 $B_{\\ell} = E[(\\delta^{(\\ell)}_{k})^{2}]$。条件变为：\n$$\\eta^{2} B_{\\ell} F_{\\ell-1} = U \\quad \\text{for } \\ell = 1, 2, 3$$\n\n为了求解 $\\sigma_{\\ell}^{2}$，我们必须推导出 $F_{\\ell}$ 和 $B_{\\ell}$ 的递推关系。\n\n**前向传播（激活的方差 $F_{\\ell}$）：**\n第 $\\ell$ 层的预激活为 $z^{(\\ell)}_{i} = \\sum_{j=1}^{n_{\\ell-1}} w^{(\\ell)}_{ij} a^{(\\ell-1)}_{j}$（偏置为零）。由于所有变量都是零均值的，$z^{(\\ell)}_{i}$ 也是零均值的。其方差为：\n$$E[(z^{(\\ell)}_{i})^{2}] = E\\left[\\left(\\sum_{j=1}^{n_{\\ell-1}} w^{(\\ell)}_{ij} a^{(\\ell-1)}_{j}\\right)^{2}\\right] = \\sum_{j=1}^{n_{\\ell-1}} E[(w^{(\\ell)}_{ij})^{2}] E[(a^{(\\ell-1)}_{j})^{2}] = n_{\\ell-1} \\sigma_{\\ell}^{2} F_{\\ell-1}$$\n这里，我们利用了权重和激活在不同索引上是独立同分布的这一事实。\n激活为 $a^{(\\ell)}_{i} = \\phi(z^{(\\ell)}_{i})$。\n对于使用 ReLU 激活的隐藏层（$\\ell=1, 2$），我们使用近似 $F_{\\ell} = E[(a^{(\\ell)}_{i})^{2}] = E[\\phi(z^{(\\ell)}_{i})^{2}] = \\frac{1}{2}E[(z^{(\\ell)}_{i})^{2}]$。\n$$F_{\\ell} = \\frac{1}{2} n_{\\ell-1} \\sigma_{\\ell}^{2} F_{\\ell-1} \\quad \\text{for } \\ell = 1, 2$$\n输出层（$\\ell=3$）是线性的，所以 $\\phi(z)=z$，因此 $a^{(3)}_{i}=z^{(3)}_{i}$。对于这一层，最终的方程组不需要这个关系。\n给定 $F_{0}=1$。\n$F_{1} = \\frac{1}{2} n_{0} \\sigma_{1}^{2} F_{0} = \\frac{1}{2} n_{0} \\sigma_{1}^{2}$。\n$F_{2} = \\frac{1}{2} n_{1} \\sigma_{2}^{2} F_{1} = \\frac{1}{2} n_{1} \\sigma_{2}^{2} \\left(\\frac{1}{2} n_{0} \\sigma_{1}^{2}\\right) = \\frac{1}{4} n_{0} n_{1} \\sigma_{1}^{2} \\sigma_{2}^{2}$。\n\n**反向传播（误差信号的方差 $B_{\\ell}$）：**\n误差信号通过 $\\delta^{(\\ell)}_{j} = \\left(\\sum_{i=1}^{n_{\\ell+1}} w^{(\\ell+1)}_{ij} \\delta^{(\\ell+1)}_{i}\\right) \\phi'(z^{(\\ell)}_{j})$ 进行反向传播。\n方差 $B_{\\ell} = E[(\\delta^{(\\ell)}_{j})^{2}]$ 为：\n$$B_{\\ell} = E\\left[\\left(\\left(\\sum_{i=1}^{n_{\\ell+1}} w^{(\\ell+1)}_{ij} \\delta^{(\\ell+1)}_{i}\\right) \\phi'(z^{(\\ell)}_{j})\\right)^{2}\\right] = E\\left[\\left(\\sum_{i=1}^{n_{\\ell+1}} w^{(\\ell+1)}_{ij} \\delta^{(\\ell+1)}_{i}\\right)^{2}\\right] E[(\\phi'(z^{(\\ell)}_{j}))^{2}]$$\n第一项展开为 $\\sum_{i=1}^{n_{\\ell+1}} E[(w^{(\\ell+1)}_{ij})^{2}] E[(\\delta^{(\\ell+1)}_{i})^{2}] = n_{\\ell+1} \\sigma_{\\ell+1}^{2} B_{\\ell+1}$。\n导数项 $E[(\\phi'(z^{(\\ell)}_{j}))^{2}]$ 取决于第 $\\ell$ 层的激活函数。误差信号 $\\delta^{(1)}$ 和 $\\delta^{(2)}$ 分别基于第 $1$ 层和第 $2$ 层的预激活计算，这两层都使用 ReLU。因此，对于 $\\ell=1, 2$，我们使用近似 $E[\\phi'(z)^{2}]=\\frac{1}{2}$。\n$$B_{\\ell} = \\frac{1}{2} n_{\\ell+1} \\sigma_{\\ell+1}^{2} B_{\\ell+1} \\quad \\text{for } \\ell = 1, 2$$\n给定 $B_{3}=1$。\n$B_{2} = \\frac{1}{2} n_{3} \\sigma_{3}^{2} B_{3} = \\frac{1}{2} n_{3} \\sigma_{3}^{2}$。\n$B_{1} = \\frac{1}{2} n_{2} \\sigma_{2}^{2} B_{2} = \\frac{1}{2} n_{2} \\sigma_{2}^{2} \\left(\\frac{1}{2} n_{3} \\sigma_{3}^{2}\\right) = \\frac{1}{4} n_{2} n_{3} \\sigma_{2}^{2} \\sigma_{3}^{2}$。\n\n现在我们使用目标条件 $\\eta^{2} B_{\\ell} F_{\\ell-1} = U$ 构造一个方程组。\n- 对于层 $\\ell=3$：$\\eta^{2} B_{3} F_{2} = U$。由于 $B_{3}=1$，我们得到 $\\eta^{2} F_{2} = U$，所以 $F_{2} = U / \\eta^{2}$。\n- 对于层 $\\ell=1$：$\\eta^{2} B_{1} F_{0} = U$。由于 $F_{0}=1$，我们得到 $\\eta^{2} B_{1} = U$，所以 $B_{1} = U / \\eta^{2}$。\n- 对于层 $\\ell=2$：$\\eta^{2} B_{2} F_{1} = U$。\n\n将递推关系代入这些条件：\n1. $F_{2} = \\frac{1}{4} n_{0} n_{1} \\sigma_{1}^{2} \\sigma_{2}^{2} = \\frac{U}{\\eta^{2}} \\implies n_{0} n_{1} \\sigma_{1}^{2} \\sigma_{2}^{2} = \\frac{4U}{\\eta^{2}}$\n2. $B_{1} = \\frac{1}{4} n_{2} n_{3} \\sigma_{2}^{2} \\sigma_{3}^{2} = \\frac{U}{\\eta^{2}} \\implies n_{2} n_{3} \\sigma_{2}^{2} \\sigma_{3}^{2} = \\frac{4U}{\\eta^{2}}$\n3. $\\eta^{2} (\\frac{1}{2} n_{3} \\sigma_{3}^{2}) (\\frac{1}{2} n_{0} \\sigma_{1}^{2}) = U \\implies n_{0} n_{3} \\sigma_{1}^{2} \\sigma_{3}^{2} = \\frac{4U}{\\eta^{2}}$\n\n令 $K = 4U/\\eta^{2}$。我们得到一个关于 $\\sigma_{1}^{2}, \\sigma_{2}^{2}, \\sigma_{3}^{2}$ 的方程组：\n- $\\sigma_{1}^{2} \\sigma_{2}^{2} = K / (n_{0} n_{1})$\n- $\\sigma_{2}^{2} \\sigma_{3}^{2} = K / (n_{2} n_{3})$\n- $\\sigma_{1}^{2} \\sigma_{3}^{2} = K / (n_{0} n_{3})$\n\n为了解这个方程组，我们可以将这三个方程相乘：\n$(\\sigma_{1}^{2} \\sigma_{2}^{2} \\sigma_{3}^{2})^{2} = \\frac{K^{3}}{(n_{0} n_{1})(n_{2} n_{3})(n_{0} n_{3})} = \\frac{K^{3}}{n_{0}^{2} n_{1} n_{2} n_{3}^{2}}$\n取平方根（方差为正）：\n$\\sigma_{1}^{2} \\sigma_{2}^{2} \\sigma_{3}^{2} = \\frac{K^{3/2}}{n_{0} n_{3} \\sqrt{n_{1} n_{2}}}$\n\n现在我们可以分离出每个方差：\n$\\sigma_{1}^{2} = \\frac{\\sigma_{1}^{2} \\sigma_{2}^{2} \\sigma_{3}^{2}}{\\sigma_{2}^{2} \\sigma_{3}^{2}} = \\frac{K^{3/2} / (n_{0} n_{3} \\sqrt{n_{1} n_{2}})}{K / (n_{2} n_{3})} = \\frac{\\sqrt{K} n_{2} n_{3}}{n_{0} n_{3} \\sqrt{n_{1} n_{2}}} = \\frac{\\sqrt{K}}{n_{0}} \\sqrt{\\frac{n_{2}}{n_{1}}}$\n$\\sigma_{2}^{2} = \\frac{\\sigma_{1}^{2} \\sigma_{2}^{2} \\sigma_{3}^{2}}{\\sigma_{1}^{2} \\sigma_{3}^{2}} = \\frac{K^{3/2} / (n_{0} n_{3} \\sqrt{n_{1} n_{2}})}{K / (n_{0} n_{3})} = \\frac{\\sqrt{K}}{\\sqrt{n_{1} n_{2}}}$\n$\\sigma_{3}^{2} = \\frac{\\sigma_{1}^{2} \\sigma_{2}^{2} \\sigma_{3}^{2}}{\\sigma_{1}^{2} \\sigma_{2}^{2}} = \\frac{K^{3/2} / (n_{0} n_{3} \\sqrt{n_{1} n_{2}})}{K / (n_{0} n_{1})} = \\frac{\\sqrt{K} n_{0} n_{1}}{n_{0} n_{3} \\sqrt{n_{1} n_{2}}} = \\frac{\\sqrt{K}}{n_{3}} \\sqrt{\\frac{n_{1}}{n_{2}}}$\n\n现在我们代入给定的数值：\n$n_{0} = 256$, $n_{1} = 128$, $n_{2} = 64$, $n_{3} = 10$, $\\eta = 0.02$, $U = 1 \\times 10^{-6}$。\n$K = \\frac{4U}{\\eta^{2}} = \\frac{4 \\times (1 \\times 10^{-6})}{(0.02)^{2}} = \\frac{4 \\times 10^{-6}}{4 \\times 10^{-4}} = 10^{-2}$。\n$\\sqrt{K} = \\sqrt{10^{-2}} = 0.1$。\n\n$\\sigma_{1}^{2} = \\frac{0.1}{256} \\sqrt{\\frac{64}{128}} = \\frac{0.1}{256} \\sqrt{\\frac{1}{2}} = \\frac{0.1}{256\\sqrt{2}} \\approx 0.000276213$\n$\\sigma_{2}^{2} = \\frac{0.1}{\\sqrt{128 \\times 64}} = \\frac{0.1}{\\sqrt{8192}} = \\frac{0.1}{64\\sqrt{2}} \\approx 0.00110485$\n$\\sigma_{3}^{2} = \\frac{0.1}{10} \\sqrt{\\frac{128}{64}} = 0.01 \\sqrt{2} \\approx 0.01414213$\n\n将每个结果四舍五入到四位有效数字：\n$\\sigma_{1}^{2} \\approx 2.762 \\times 10^{-4} = 0.0002762$\n$\\sigma_{2}^{2} \\approx 1.105 \\times 10^{-3} = 0.001105$\n$\\sigma_{3}^{2} \\approx 0.01414$\n\n解是行向量 $(\\sigma_{1}^{2}, \\sigma_{2}^{2}, \\sigma_{3}^{2})$。", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n0.0002762  0.001105  0.01414\n\\end{pmatrix}\n}\n$$", "id": "3199526"}]}