{"hands_on_practices": [{"introduction": "理论是理解深度学习概念的基石，但通过经验验证来巩固这些理论同样至关重要。此练习旨在通过编码实践，让您亲手验证不同权重初始化策略对深度网络中信号传播的影响。您将搭建一个简单的全连接网络，并观察在使用双曲正切（$ \\tanh $）和修正线性单元（ReLU）激活函数时，Xavier初始化与He初始化各自的表现，从而直观地理解为何特定的初始化方法与特定的激活函数是“天作之合”[@problem_id:3199598]。", "problem": "给定一个包含 $L$ 层的全连接前馈网络，将第 $l$ 层的预激活定义为 $z^{(l)} = W^{(l)} a^{(l-1)}$，后激活定义为 $a^{(l)} = \\phi\\!\\left(z^{(l)}\\right)$，其中 $a^{(0)} = x$。假设偏置为零，权重独立同分布，输入 $x \\in \\mathbb{R}^{n}$ 的分量独立、均值为零且方差有限。考虑两种广泛使用的随机权重初始化策略：Xavier (Glorot) 正态初始化和 He (Kaiming) 正态初始化，以及两种激活函数：$\\tanh$ 和修正线性单元 ($\\mathrm{ReLU}$)。目标是通过蒙特卡洛模拟，经验性地验证在哪种情况下，初始化策略能够近似保持各层预激活的方差，即在给定激活函数下，对于所有层 $l \\in \\{1,\\dots,L\\}$，都有 $\\operatorname{Var}\\!\\left(z^{(l)}\\right) \\approx \\operatorname{Var}(x)$。\n\n使用的基本原理：\n- 初始化时权重和激活在各坐标上的独立性，以及独立变量求和的方差线性性质。\n- $\\tanh$ 和 $\\mathrm{ReLU}$ 作为逐点非线性函数的定义。\n- 对于一个数组 $Y \\in \\mathbb{R}^{s \\times d}$，沿 $s$ 个样本的样本方差估计器定义为 $\\widehat{\\operatorname{Var}}(Y) = \\frac{1}{d}\\sum_{j=1}^{d} \\left(\\frac{1}{s}\\sum_{i=1}^{s} Y_{ij}^{2} - \\left(\\frac{1}{s}\\sum_{i=1}^{s} Y_{ij}\\right)^{2} \\right)$。\n\n你的程序必须：\n1. 构建具有指定 $L$ 和层宽度的网络，其中每层的形状为 $(n_{\\text{in}}, n_{\\text{out}})$，为简化起见，这里 $n_{\\text{in}} = n_{\\text{out}}$。对每层的权重 $W^{(l)}$ 使用 Xavier 正态初始化或 He 正态初始化。使用每种策略规定的方差进行零均值正态初始化；不添加任何偏置。\n2. 将输入 $x$ 抽取为 $s$ 个维度为 $n$ 的独立样本，每个分量服从 $\\mathcal{N}(0,1)$ 分布，即均值为零，方差为 1。\n3. 对每一层 $l$，通过对每个坐标的样本方差取平均，计算预激活的经验方差 $\\widehat{\\operatorname{Var}}\\!\\left(z^{(l)}\\right)$，并类似地计算输入经验方差 $\\widehat{\\operatorname{Var}}(x)$。将第 $l$ 层的相对偏差定义为 $$\\delta^{(l)} = \\frac{\\left|\\widehat{\\operatorname{Var}}\\!\\left(z^{(l)}\\right) - \\widehat{\\operatorname{Var}}(x)\\right|}{\\widehat{\\operatorname{Var}}(x)}$$。如果一个测试用例满足 $\\max_{1 \\le l \\le L} \\delta^{(l)} \\le \\varepsilon$（容差 $\\varepsilon = 0.25$），则认为其保持了方差。\n4. 使用带有固定种子 $12345$ 的伪随机数生成器以确保可复现性。\n\n测试套件：\n- 案例 1：$L=5$，宽度 $n=32$，样本数 $s=8000$，激活函数 $\\tanh$，初始化 Xavier 正态。\n- 案例 2：$L=5$，宽度 $n=32$，样本数 $s=8000$，激活函数 $\\tanh$，初始化 He 正态。\n- 案例 3：$L=5$，宽度 $n=32$，样本数 $s=8000$，激活函数 $\\mathrm{ReLU}$，初始化 He 正态。\n- 案例 4：$L=5$，宽度 $n=32$，样本数 $s=8000$，激活函数 $\\mathrm{ReLU}$，初始化 Xavier 正态。\n- 案例 5：$L=1$，宽度 $n=32$，样本数 $s=20000$，激活函数 $\\tanh$，初始化 Xavier 正态。\n- 案例 6：$L=1$，宽度 $n=32$，样本数 $s=20000$，激活函数 $\\mathrm{ReLU}$，初始化 He 正态。\n- 案例 7：$L=15$，宽度 $n=16$，样本数 $s=8000$，激活函数 $\\tanh$，初始化 Xavier 正态。\n- 案例 8：$L=15$，宽度 $n=16$，样本数 $s=8000$，激活函数 $\\mathrm{ReLU}$，初始化 He 正态。\n\n最终输出格式：\n你的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表（例如，$\\left[\\text{result}_1,\\text{result}_2,\\dots\\right]$），其中每个 $\\text{result}_i$ 是一个布尔值，表示第 $i$ 个测试用例是否保持了方差，顺序与上述测试套件完全一致。", "solution": "该问题经评估有效。它在科学上基于深度学习的既定原则，特别是关于权重初始化及其对信号传播的影响。该问题定义明确，提供了所有必要的参数、定义和清晰客观的成功标准。它没有矛盾、歧义和事实错误。因此，我们可以着手解决。\n\n目标是经验性地验证深度神经网络中预激活 $z^{(l)}$ 的方差在各层之间得以保持的条件。此分析的核心在于连续层中预激活方差之间的递归关系。\n\n让我们考虑第 $l$ 层单个神经元的预激活：\n$$ z_i^{(l)} = \\sum_{j=1}^{n_{l-1}} W_{ij}^{(l)} a_j^{(l-1)} $$\n这里，$W_{ij}^{(l)}$ 是连接第 $l-1$ 层神经元 $j$ 与第 $l$ 层神经元 $i$ 的权重，$a_j^{(l-1)}$ 是前一层神经元 $j$ 的激活值。问题指定偏置为零，权重 $W_{ij}^{(l)}$ 从零均值分布中抽取，输入分量 $x_j$（构成 $a_j^{(0)}$）也具有零均值。我们假设在初始化时，所有 $j$ 的激活 $a_j^{(l-1)}$ 都独立于权重 $W_{ij}^{(l)}$ 并且是同分布的。此外，如果激活 $a^{(l-1)}$ 是对称激活函数应用于零均值输入 $z^{(l-1)}$ 的输出，它们也将具有零均值。对于 $\\mathrm{ReLU}$ 激活，情况并非如此，但由于权重本身是零均值的，得到的预激活 $z^{(l)}$ 仍将具有零均值：$\\mathbb{E}[z_i^{(l)}] = \\sum_j \\mathbb{E}[W_{ij}^{(l)}] \\mathbb{E}[a_j^{(l-1)}] = \\sum_j 0 \\cdot \\mathbb{E}[a_j^{(l-1)}] = 0$。\n\n在这些条件下，$z_i^{(l)}$ 的方差由下式给出：\n$$ \\operatorname{Var}(z_i^{(l)}) = \\operatorname{Var}\\left(\\sum_{j=1}^{n_{l-1}} W_{ij}^{(l)} a_j^{(l-1)}\\right) $$\n由于和中各项是独立的（因为权重和前一层的激活是独立的），和的方差等于方差的和：\n$$ \\operatorname{Var}(z_i^{(l)}) = \\sum_{j=1}^{n_{l-1}} \\operatorname{Var}(W_{ij}^{(l)} a_j^{(l-1)}) $$\n对于两个独立的随机变量 $U$ 和 $V$，如果至少一个具有零均值（例如，$\\mathbb{E}[U]=0$），则 $\\operatorname{Var}(UV) = \\mathbb{E}[U^2V^2] - (\\mathbb{E}[UV])^2 = \\mathbb{E}[U^2]\\mathbb{E}[V^2] - (\\mathbb{E}[U]\\mathbb{E}[V])^2 = \\operatorname{Var}(U)\\operatorname{Var}(V)$。由于 $\\mathbb{E}[W_{ij}^{(l)}] = 0$，我们有：\n$$ \\operatorname{Var}(z_i^{(l)}) = \\sum_{j=1}^{n_{l-1}} \\operatorname{Var}(W_{ij}^{(l)}) \\operatorname{Var}(a_j^{(l-1)}) $$\n假设第 $l$ 层的所有权重都是从方差为 $\\operatorname{Var}(W^{(l)})$ 的分布中独立同分布地抽取的，并且第 $l-1$ 层的所有激活都是方差为 $\\operatorname{Var}(a^{(l-1)})$ 的独立同分布变量，则上式简化为：\n$$ \\operatorname{Var}(z^{(l)}) = n_{l-1} \\operatorname{Var}(W^{(l)}) \\operatorname{Var}(a^{(l-1)}) $$\n其中 $n_{l-1}$ 是第 $l-1$ 层的神经元数量。为了保持稳定的信号传播，我们需要保持方差，即 $\\operatorname{Var}(z^{(l)}) \\approx \\operatorname{Var}(z^{(l-1)})$。这需要仔细选择权重初始化的方差 $\\operatorname{Var}(W^{(l)})$，以抵消激活函数 $\\phi$ 对方差的影响，该影响由项 $\\operatorname{Var}(a^{(l-1)}) = \\operatorname{Var}(\\phi(z^{(l-1)}))$ 捕获。\n\n**激活函数分析**\n\n1.  **$\\tanh$ 激活：** 双曲正切函数 $\\tanh(z)$ 关于原点对称（$\\tanh(0)=0$），且对于小输入，其行为类似于恒等函数（当 $z \\approx 0$ 时，$\\tanh(z) \\approx z$）。如果我们假设预激活 $z^{(l-1)}$ 集中在零附近（这是初始训练阶段的理想状态），那么 $\\operatorname{Var}(a^{(l-1)}) = \\operatorname{Var}(\\tanh(z^{(l-1)})) \\approx \\operatorname{Var}(z^{(l-1)})$。将此代入我们的传播方程中，得到：\n    $$ \\operatorname{Var}(z^{(l)}) \\approx n_{l-1} \\operatorname{Var}(W^{(l)}) \\operatorname{Var}(z^{(l-1)}) $$\n    为实现 $\\operatorname{Var}(z^{(l)}) \\approx \\operatorname{Var}(z^{(l-1)})$，我们必须设置 $n_{l-1} \\operatorname{Var}(W^{(l)}) = 1$。\n    **Xavier (Glorot) 正态初始化**正是为这种情况设计的。它将从 $\\mathcal{N}(0, \\sigma^2)$ 中抽取的权重 $W^{(l)}$ 的方差设置为：\n    $$ \\operatorname{Var}(W^{(l)}) = \\frac{2}{n_{l-1} + n_l} $$\n    在我们的特定问题中，$n_{l-1} = n_l = n$，所以这变为 $\\operatorname{Var}(W^{(l)}) = \\frac{2}{2n} = \\frac{1}{n} = \\frac{1}{n_{l-1}}$。这个选择完美地满足了条件 $n_{l-1} \\operatorname{Var}(W^{(l)}) = 1$。因此，Xavier 初始化预计能为 $\\tanh$ 激活函数保持方差。\n\n2.  **$\\mathrm{ReLU}$ 激活：** 修正线性单元 $\\mathrm{ReLU}(z) = \\max(0, z)$ 是不对称的。对于零均值、对称的输入分布 $z^{(l-1)}$（如高斯分布），恰好一半的输入将被设为零。这会影响方差。设 $z \\sim \\mathcal{N}(0, \\sigma_z^2)$。输出 $a = \\mathrm{ReLU}(z)$ 的方差是 $\\operatorname{Var}(a) = \\mathbb{E}[a^2] - (\\mathbb{E}[a])^2$。\n    平方激活的期望是 $\\mathbb{E}[a^2] = \\mathbb{E}[\\max(0, z)^2] = \\int_0^\\infty z^2 p(z) dz$。由于正态分布 $p(z)$ 的对称性，此积分为 $z^2$ 的总积分的一半：$\\mathbb{E}[a^2] = \\frac{1}{2} \\int_{-\\infty}^\\infty z^2 p(z) dz = \\frac{1}{2}\\mathbb{E}[z^2] = \\frac{1}{2}\\operatorname{Var}(z)$。\n    因此，对于 $\\mathrm{ReLU}$，我们有 $\\operatorname{Var}(a^{(l-1)}) \\approx \\frac{1}{2}\\operatorname{Var}(z^{(l-1)})$。方差传播方程变为：\n    $$ \\operatorname{Var}(z^{(l)}) \\approx n_{l-1} \\operatorname{Var}(W^{(l)}) \\left(\\frac{1}{2}\\operatorname{Var}(z^{(l-1)})\\right) $$\n    为了保持方差，我们必须有 $n_{l-1} \\operatorname{Var}(W^{(l)}) \\frac{1}{2} = 1$，这意味着 $\\operatorname{Var}(W^{(l)}) = \\frac{2}{n_{l-1}}$。\n    **He (Kaiming) 正态初始化**就是为这种情况设计的。它将方差设置为：\n    $$ \\operatorname{Var}(W^{(l)}) = \\frac{2}{n_{l-1}} $$\n    这个选择恰好满足了条件。因此，He 初始化预计能为 $\\mathrm{ReLU}$ 激活函数保持方差。不匹配的组合（例如，$\\tanh$ 与 He，$\\mathrm{ReLU}$ 与 Xavier）预计将分别导致方差爆炸或消失。\n\n**模拟过程**\n\n程序将为 8 个测试用例中的每一个实施蒙特卡洛模拟。对于每个案例：\n1.  用值 $12345$ 为伪随机数生成器设置种子以确保可复现性。\n2.  生成一个输入数据矩阵 $x \\in \\mathbb{R}^{s \\times n}$，其中每个元素从 $\\mathcal{N}(0, 1)$ 中抽取。使用提供的公式计算输入经验方差 $\\widehat{\\operatorname{Var}}(x)$。\n3.  从 $l=1$ 到 $L$ 逐层处理网络。在每一层中，权重矩阵 $W^{(l)}$ 根据指定的策略（Xavier 或 He）所要求的方差，从零均值正态分布中初始化。\n4.  计算预激活 $z^{(l)} = a^{(l-1)} W^{(l)}$。\n5.  计算经验方差 $\\widehat{\\operatorname{Var}}(z^{(l)})$。计算相对偏差 $\\delta^{(l)} = |\\widehat{\\operatorname{Var}}(z^{(l)}) - \\widehat{\\operatorname{Var}}(x)| / \\widehat{\\operatorname{Var}}(x)$。\n6.  追踪所有层中的最大相对偏差 $\\max_{1 \\le l \\le L} \\delta^{(l)}$。\n7.  计算后激活 $a^{(l)} = \\phi(z^{(l)})$ 作为下一层的输入。\n8.  遍历所有层后，如果 $\\max_{l} \\delta^{(l)} \\le \\varepsilon$（其中 $\\varepsilon = 0.25$），则该测试用例被认为保持了方差。该检查将为每个案例生成一个布尔结果，然后报告该结果。", "answer": "```python\nimport numpy as np\n\ndef tanh(x):\n    \"\"\"Hyperbolic tangent activation function.\"\"\"\n    return np.tanh(x)\n\ndef relu(x):\n    \"\"\"Rectified Linear Unit (ReLU) activation function.\"\"\"\n    return np.maximum(0, x)\n\ndef empirical_variance(Y):\n    \"\"\"\n    Computes the empirical variance as defined in the problem statement.\n    For an array Y of shape (s, d), this is the average of the biased sample \n    variances of each of the d columns.\n    \n    Formula: Var(Y) = (1/d) * sum_j [ (1/s) * sum_i(Y_ij^2) - ((1/s) * sum_i(Y_ij))^2 ]\n    This is equivalent to the mean of np.var(Y, axis=0).\n    \"\"\"\n    if Y.ndim == 1:\n        # If input is a 1D array, treat it as (s, 1)\n        return np.var(Y)\n    return np.mean(np.var(Y, axis=0))\n\ndef run_simulation(L, n, s, activation_func, init_strategy, seed, epsilon):\n    \"\"\"\n    Runs a single simulation for a given network configuration.\n\n    Args:\n        L (int): Number of layers.\n        n (int): Width of each layer.\n        s (int): Number of samples.\n        activation_func (callable): The activation function to use.\n        init_strategy (str): The weight initialization strategy ('xavier' or 'he').\n        seed (int): The seed for the random number generator.\n        epsilon (float): The tolerance for variance preservation.\n\n    Returns:\n        bool: True if variance is preserved, False otherwise.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    \n    # Generate input data x of shape (s, n) from N(0, 1)\n    x = rng.normal(loc=0.0, scale=1.0, size=(s, n))\n    var_x = empirical_variance(x)\n    \n    a_prev = x\n    max_deviation = 0.0\n    \n    # Propagate through the network for L layers\n    for l in range(1, L + 1):\n        # Layer dimensions are (n_in, n_out)\n        n_in = n\n        n_out = n\n        \n        # Determine standard deviation for weight initialization\n        if init_strategy == 'xavier':\n            # Variance = 2 / (n_in + n_out)\n            std_dev = np.sqrt(2.0 / (n_in + n_out))\n        elif init_strategy == 'he':\n            # Variance = 2 / n_in\n            std_dev = np.sqrt(2.0 / n_in)\n        else:\n            raise ValueError(f\"Unknown initialization strategy: {init_strategy}\")\n            \n        # Initialize weights W of shape (n_in, n_out)\n        W = rng.normal(loc=0.0, scale=std_dev, size=(n_in, n_out))\n        \n        # Compute pre-activations z = a_prev @ W of shape (s, n_out)\n        z = a_prev @ W\n        \n        # Compute empirical variance and relative deviation\n        var_z = empirical_variance(z)\n        if var_x > 1e-9: # Avoid division by zero\n            deviation = np.abs(var_z - var_x) / var_x\n        else:\n            deviation = np.abs(var_z)\n\n        if deviation > max_deviation:\n            max_deviation = deviation\n            \n        # Compute post-activations for the next layer\n        a_prev = activation_func(z)\n        \n    # Check if the maximum deviation is within the tolerance\n    return max_deviation = epsilon\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    # Define test cases as (L, n, s, activation_name, init_strategy_name)\n    test_cases = [\n        (5, 32, 8000, 'tanh', 'xavier'),\n        (5, 32, 8000, 'tanh', 'he'),\n        (5, 32, 8000, 'relu', 'he'),\n        (5, 32, 8000, 'relu', 'xavier'),\n        (1, 32, 20000, 'tanh', 'xavier'),\n        (1, 32, 20000, 'relu', 'he'),\n        (15, 16, 8000, 'tanh', 'xavier'),\n        (15, 16, 8000, 'relu', 'he'),\n    ]\n    \n    # Map string names to functions and parameters\n    activation_map = {'tanh': tanh, 'relu': relu}\n    epsilon = 0.25\n    seed = 12345\n    \n    results = []\n    \n    # Run simulation for each test case\n    for case in test_cases:\n        L, n, s, act_name, init_name = case\n        activation_func = activation_map[act_name]\n        \n        # Each test case is an independent experiment, so we use the same seed,\n        # creating a new RNG instance for each to ensure reproducibility.\n        result = run_simulation(L, n, s, activation_func, init_name, seed, epsilon)\n        results.append(result)\n        \n    # Format and print the final output as a single-line list of booleans\n    # e.g., [True,False,True,False,True,True,True,True]\n    # np.bool_ maps to Python's True/False, and str() correctly converts them.\n    # The problem asks for booleans, and Python's `str(True)` is 'True'.\n    print(f\"[{','.join(map(str, results))}]\")\n\n# Execute the solution\nsolve()\n```", "id": "3199598"}, {"introduction": "在前一个练习中，我们验证了He初始化在ReLU网络中前向传播的稳定性。然而，一个健康的神经网络不仅需要稳定的前向信号流，更关键的是要有稳定的反向梯度流，以确保网络可以有效训练。此练习将引导您深入探究He初始化如何解决深度ReLU网络中的梯度消失问题，您将通过模拟计算并比较前向传播的“能量”（激活值的二阶矩）和反向传播的“能量”（梯度的二阶矩），从而验证He初始化是如何同时稳定这两个过程的[@problem_id:3194508]。", "problem": "考虑一个深度为 $L$、使用整流线性单元 (ReLU) 非线性的全连接前馈神经网络。设层索引为 $l \\in \\{1,\\dots,L\\}$，每层的宽度（单元数）为 $n$，输入批次大小为 $B$。将第 $l$ 层的激活后值记为 $x_l \\in \\mathbb{R}^{B \\times n}$，激活前值记为 $z_l \\in \\mathbb{R}^{B \\times n}$。前向传播遵循规则 $z_l = x_{l-1} W_l$ 和 $x_l = \\phi(z_l)$，其中 $\\phi$ 是逐元素定义的 ReLU 函数 $\\phi(u) = \\max(0,u)$，$W_l \\in \\mathbb{R}^{n \\times n}$ 是第 $l$ 层的权重矩阵。假设 $x_0$ 的条目是独立同分布 (i.i.d.) 的，服从零均值和单位方差的高斯分布。在初始化时，假设每个 $W_l$ 的条目都是独立同分布、零均值，且与 $x_{l-1}$ 无关，并考虑两种初始化方案：He 初始化，其方差为 $\\mathrm{Var}(W_{l,ij}) = \\frac{2}{n}$；以及 Xavier (Glorot) 初始化，其方差为 $\\mathrm{Var}(W_{l,ij}) = \\frac{1}{n}$。偏置为零。\n\n目标是从基本原理出发分析梯度消失问题。您必须从以下基础出发：\n- 用于前馈神经网络反向传播的微积分链式法则。\n- 方差的定义 $\\mathrm{Var}[X] = \\mathbb{E}[X^2] - (\\mathbb{E}[X])^2$ 以及针对独立零均值随机变量求和的方差独立性法则，即如果 $U_i$ 相互独立且均值为零，则 $\\mathrm{Var}\\left[\\sum_i U_i\\right] = \\sum_i \\mathrm{Var}[U_i]$。\n- ReLU 作用于零均值对称输入时的性质：其导数是一个指示函数 $\\phi'(u) = \\mathbb{1}\\{u0\\}$，在对称性下，它以 $\\frac{1}{2}$ 的概率充当门（gate）。\n\n利用这些基础，推断在初始化时矩（moment）是如何在前向和反向传播中变化的。具体来说，将第 $l$ 层的前向“能量”定义为二阶原始矩 $m_l^{\\text{fwd}} = \\mathbb{E}\\big[x_l^2\\big]$（期望是对批次和单元计算的），并将第 $l$ 层的反向“能量”定义为梯度关于激活前值的二阶原始矩 $m_l^{\\text{bwd}} = \\mathbb{E}\\big[(\\partial L/\\partial z_l)^2\\big]$，其中损失函数为平方范数损失 $L = \\tfrac{1}{2}\\lVert x_L\\rVert_2^2$，该损失是按样本计算，不进行批次平均。在 He 初始化和独立性假设下，说明为什么在初始化时，$m_l^{\\text{fwd}}$ 和 $m_l^{\\text{bwd}}$ 在各层之间都保持在 $1$ 附近。将其与 ReLU 下的 Xavier 初始化进行对比，并解释为什么随着深度增加，Xavier 初始化会倾向于减小 $m_l^{\\text{fwd}}$，从而导致梯度消失的情况。\n\n您的程序必须通过蒙特卡洛模拟来经验性地验证这些陈述。对于每个测试用例，按规定构建一个随机网络和批次，执行一次前向传递以计算所有的 $z_l$ 和 $x_l$，并使用链式法则执行一次反向传递以计算所有 $l$ 的 $\\partial L/\\partial z_l$。对于每一层，计算：\n- 前向二阶原始矩 $m_l^{\\text{fwd}}$，作为 $x_l$ 中元素平方的经验均值。\n- 反向二阶原始矩 $m_l^{\\text{bwd}}$，作为 $\\partial L/\\partial z_l$ 中元素平方的经验均值。\n通过计算前向和反向传播在各层中与 $1$ 的最大绝对偏差来汇总偏差，即 $\\Delta^{\\text{fwd}} = \\max_l \\lvert m_l^{\\text{fwd}} - 1\\rvert$ 和 $\\Delta^{\\text{bwd}} = \\max_l \\lvert m_l^{\\text{bwd}} - 1\\rvert$。如果 $\\Delta^{\\text{fwd}}$ 和 $\\Delta^{\\text{bwd}}$ 都严格小于容差 $\\varepsilon = 0.2$，则测试用例“通过”。\n\n测试套件：\n- 用例 1：He 初始化，$L=1$，$n=64$，$B=2000$。\n- 用例 2：He 初始化，$L=20$，$n=64$，$B=2000$。\n- 用例 3：He 初始化，$L=40$，$n=64$，$B=2000$。\n- 用例 4：Xavier 初始化，$L=20$，$n=64$，$B=2000$。\n\n答案格式：\n- 您的程序应生成单行输出，其中包含四个测试用例的通过/失败结果，格式为方括号括起来的逗号分隔列表，例如 $\\texttt{[True,True,True,False]}$。\n\n不涉及物理单位或角度单位。所有期望和方差都是无量纲的。请完全按照描述实现模拟和计算，不使用任何外部文件或输入，并使用固定的随机种子以确保可复现性。", "solution": "该问题是有效的，因为它科学地基于深度学习的原理，问题陈述清晰且提供了所有必要信息，并且表述客观。我们接下来进行理论分析和随后的经验验证。\n\n分析的核心在于推导在网络初始化时，激活值（前向传递）和梯度（反向传递）的二阶原始矩在各层之间传播的递推关系。我们将前向“能量”定义为 $m_l^{\\text{fwd}} = \\mathbb{E}\\big[x_{l,ik}^2\\big]$，反向“能量”定义为 $m_l^{\\text{bwd}} = \\mathbb{E}\\big[(\\partial L/\\partial z_{l,ik})^2\\big]$，其中期望是针对批次维度 $i$、单元维度 $k$ 以及随机初始化计算的。\n\n**前向传播分析**\n\n前向传递由 $z_l = x_{l-1} W_l$ 和 $x_l = \\phi(z_l)$ 定义，其中 $\\phi$ 是 ReLU 函数。我们考虑单个激活前元素 $z_{l,ik} = \\sum_{j=1}^{n} x_{l-1, ij} W_{l,jk}$。在初始化时，权重 $W_{l,jk}$ 与输入 $x_{l-1, ij}$ 相互独立，并且二者均值为零。因此，在 $\\mathbb{E}[x_{l-1, ij}] = 0$ 的假设下，$\\mathbb{E}[z_{l,ik}] = \\sum_{j=1}^{n} \\mathbb{E}[x_{l-1, ij}] \\mathbb{E}[W_{l,jk}] = 0$。这对于输入层 $x_0$ 成立，并且由于更新的对称性，我们假设它对后续层也近似成立。\n\n$z_{l,ik}$ 的方差为 $\\mathrm{Var}(z_{l,ik}) = \\mathbb{E}[z_{l,ik}^2]$，因为其均值为零。利用求和项的独立性：\n$$ \\mathrm{Var}(z_{l,ik}) = \\mathrm{Var}\\left(\\sum_{j=1}^{n} x_{l-1, ij} W_{l,jk}\\right) = \\sum_{j=1}^{n} \\mathrm{Var}(x_{l-1, ij} W_{l,jk}) $$\n根据 $x_{l-1}$ 和 $W_l$ 的独立性及其零均值特性，$\\mathrm{Var}(AB) = \\mathbb{E}[A^2]\\mathbb{E}[B^2] = \\mathrm{Var}(A)\\mathrm{Var}(B)$。因此：\n$$ \\mathrm{Var}(z_{l,ik}) = \\sum_{j=1}^{n} \\mathrm{Var}(x_{l-1, ij}) \\mathrm{Var}(W_{l,jk}) = n \\cdot \\mathrm{Var}(x_{l-1}) \\cdot \\mathrm{Var}(W_l) $$\n这里，$\\mathrm{Var}(x_{l-1})$ 是 $x_{l-1}$ 中任意元素的方差，$\\mathrm{Var}(W_l)$ 是 $W_l$ 中任意元素的方差。\n激活值为 $x_l = \\phi(z_l)$。我们需要二阶矩 $m_l^{\\text{fwd}} = \\mathbb{E}[x_l^2] = \\mathbb{E}[\\phi(z_l)^2]$。对于一个零均值对称输入 $z_l$（根据中心极限定理近似为高斯分布），ReLU 函数 $\\phi(u)=\\max(0,u)$ 实际上将分布的一半置零。输出的二阶矩是输入二阶矩的一半：$\\mathbb{E}[\\phi(z_l)^2] = \\frac{1}{2}\\mathbb{E}[z_l^2]$。\n因此，$m_l^{\\text{fwd}} = \\frac{1}{2} \\mathbb{E}[z_l^2] = \\frac{1}{2} \\mathrm{Var}(z_l)$。\n\n综合这些结果，我们得到前向能量的递推关系：\n$$ m_l^{\\text{fwd}} = \\frac{1}{2} n \\cdot \\mathrm{Var}(x_{l-1}) \\cdot \\mathrm{Var}(W_l) $$\n我们做一个标准近似，即激活值的均值保持在零附近，因此 $\\mathrm{Var}(x_{l-1}) \\approx \\mathbb{E}[x_{l-1}^2] = m_{l-1}^{\\text{fwd}}$。这得到：\n$$ m_l^{\\text{fwd}} \\approx \\frac{1}{2} n \\cdot m_{l-1}^{\\text{fwd}} \\cdot \\mathrm{Var}(W_l) $$\n基本情况是输入层 $x_0$，其条目为独立同分布，均值为 $0$，方差为 $1$。因此，$m_0^{\\text{fwd}} = \\mathbb{E}[x_0^2] = \\mathrm{Var}(x_0) = 1$。\n\n*   **He 初始化**：当 $\\mathrm{Var}(W_l) = \\frac{2}{n}$ 时，递推关系变为 $m_l^{\\text{fwd}} \\approx \\frac{1}{2} n \\cdot m_{l-1}^{\\text{fwd}} \\cdot \\frac{2}{n} = m_{l-1}^{\\text{fwd}}$。从 $m_0^{\\text{fwd}}=1$ 开始，前向能量在各层之间得以保持，即对所有 $l$ 都有 $m_l^{\\text{fwd}} \\approx 1$。\n*   **Xavier 初始化**：当 $\\mathrm{Var}(W_l) = \\frac{1}{n}$ 时，递推关系变为 $m_l^{\\text{fwd}} \\approx \\frac{1}{2} n \\cdot m_{l-1}^{\\text{fwd}} \\cdot \\frac{1}{n} = \\frac{1}{2} m_{l-1}^{\\text{fwd}}$。这导致指数衰减：$m_l^{\\text{fwd}} \\approx (\\frac{1}{2})^l m_0^{\\text{fwd}} = (\\frac{1}{2})^l$。随着网络深度 $L$ 的增加，激活值的能量会消失。\n\n**反向传播分析**\n\n关于激活前值的梯度由链式法则给出：$\\frac{\\partial L}{\\partial z_l} = \\frac{\\partial L}{\\partial z_{l+1}} \\frac{\\partial z_{l+1}}{\\partial x_l} \\frac{\\partial x_l}{\\partial z_l}$。设 $\\delta_l = \\frac{\\partial L}{\\partial z_l}$。单个分量为：\n$$ \\delta_{l,ik} = \\left( (\\delta_{l+1} W_{l+1}^T)_{ik} \\right) \\cdot \\phi'(z_{l,ik}) = \\left( \\sum_{j=1}^n \\delta_{l+1, ij} W_{l+1, kj} \\right) \\cdot \\phi'(z_{l,ik}) $$\n我们想求 $m_l^{\\text{bwd}} = \\mathbb{E}[\\delta_{l,ik}^2]$。在初始化时，括号中的项与 $\\phi'(z_{l,ik})$ 无关。因此，$\\mathbb{E}[\\delta_{l,ik}^2] = \\mathbb{E}[(\\sum_j \\dots)^2] \\cdot \\mathbb{E}[(\\phi'(z_{l,ik}))^2]$。\nReLU 的导数是 $\\phi'(u) = \\mathbb{1}\\{u0\\}$，所以 $(\\phi'(u))^2 = \\phi'(u)$。对于一个对称的、零均值的 $z_{l,ik}$，$\\mathbb{P}(z_{l,ik}0) = \\frac{1}{2}$，因此 $\\mathbb{E}[(\\phi'(z_{l,ik}))^2] = \\frac{1}{2}$。\n这个和的方差是 $\\mathrm{Var}(\\sum_j \\delta_{l+1, ij} W_{l+1, kj}) = n \\cdot \\mathrm{Var}(\\delta_{l+1}) \\cdot \\mathrm{Var}(W_{l+1})$。假设 $\\mathbb{E}[\\delta_{l+1}] \\approx 0$，则其值为 $n \\cdot m_{l+1}^{\\text{bwd}} \\cdot \\mathrm{Var}(W_{l+1})$。\n\n结合这些，我们得到反向递推关系：\n$$ m_l^{\\text{bwd}} \\approx \\frac{1}{2} n \\cdot m_{l+1}^{\\text{bwd}} \\cdot \\mathrm{Var}(W_{l+1}) $$\n对于第 $L$ 层的基本情况，损失函数是每个样本的 $L = \\frac{1}{2} \\|x_L\\|_2^2$。梯度为 $\\delta_L = \\frac{\\partial L}{\\partial z_L} = \\frac{\\partial L}{\\partial x_L}\\frac{\\partial x_L}{\\partial z_L}$。这里，$\\frac{\\partial L}{\\partial x_L} = x_L$，而 $\\frac{\\partial x_L}{\\partial z_L}$ 是一个由 $\\phi'(z_L)$ 组成的对角矩阵。所以 $\\delta_L = x_L \\odot \\phi'(z_L)$。因为 $x_L = \\phi(z_L)$，我们有 $\\delta_L = \\phi(z_L) \\odot \\phi'(z_L) = \\phi(z_L) = x_L$。\n因此，反向能量的基本情况是 $m_L^{\\text{bwd}} = \\mathbb{E}[\\delta_L^2] = \\mathbb{E}[x_L^2] = m_L^{\\text{fwd}}$。\n\n*   **He 初始化**：当 $\\mathrm{Var}(W_{l+1}) = \\frac{2}{n}$ 时，递推关系为 $m_l^{\\text{bwd}} \\approx m_{l+1}^{\\text{bwd}}$。根据前向传递分析，$m_L^{\\text{fwd}} \\approx 1$，因此 $m_L^{\\text{bwd}} \\approx 1$。反向传播时，我们发现对所有 $l$ 都有 $m_l^{\\text{bwd}} \\approx 1$。梯度能量得以保持。\n*   **Xavier 初始化**：当 $\\mathrm{Var}(W_{l+1}) = \\frac{1}{n}$ 时，递推关系为 $m_l^{\\text{bwd}} \\approx \\frac{1}{2} m_{l+1}^{\\text{bwd}}$。当梯度从第 $L$ 层传播到第 $1$ 层时，其能量呈指数衰减。这就是梯度消失问题。早期层的梯度变得过小，无法有效促进学习。\n\n模拟将经验性地验证这两种截然不同的行为。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Runs Monte Carlo simulations to verify the signal propagation properties of\n    He and Xavier initializations in deep ReLU networks.\n    \"\"\"\n    # Set a fixed random seed for reproducibility.\n    np.random.seed(42)\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {'init': 'he', 'L': 1, 'n': 64, 'B': 2000, 'name': 'Case 1'},\n        {'init': 'he', 'L': 20, 'n': 64, 'B': 2000, 'name': 'Case 2'},\n        {'init': 'he', 'L': 40, 'n': 64, 'B': 2000, 'name': 'Case 3'},\n        {'init': 'xavier', 'L': 20, 'n': 64, 'B': 2000, 'name': 'Case 4'},\n    ]\n\n    results = []\n    for case in test_cases:\n        passes_test = run_simulation(\n            init_scheme=case['init'],\n            L=case['L'],\n            n=case['n'],\n            B=case['B']\n        )\n        results.append(passes_test)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef run_simulation(init_scheme: str, L: int, n: int, B: int) -> bool:\n    \"\"\"\n    Performs a single forward and backward pass for a given network configuration\n    and checks if the deviation criteria are met.\n\n    Args:\n        init_scheme: Either 'he' or 'xavier'.\n        L: Depth of the network.\n        n: Width of each layer.\n        B: Batch size.\n\n    Returns:\n        A boolean indicating if the test case passes.\n    \"\"\"\n    # 1. Initialization\n    # Input data: i.i.d. Gaussian with zero mean and unit variance.\n    x0 = np.random.randn(B, n)\n\n    # Determine weight variance based on initialization scheme.\n    if init_scheme == 'he':\n        var_w = 2.0 / n\n    elif init_scheme == 'xavier':\n        var_w = 1.0 / n\n    else:\n        raise ValueError(\"Unknown initialization scheme.\")\n    \n    std_w = np.sqrt(var_w)\n\n    # Initialize weights for L layers.\n    weights = [np.random.randn(n, n) * std_w for _ in range(L)]\n\n    # 2. Forward Pass\n    z_values = {}\n    x_values = {0: x0}\n    \n    x_current = x0\n    for l in range(1, L + 1):\n        # Linear transformation\n        z_l = x_current @ weights[l-1]\n        # ReLU activation\n        x_l = np.maximum(0, z_l)\n        \n        z_values[l] = z_l\n        x_values[l] = x_l\n        x_current = x_l\n    \n    # 3. Compute Forward Moments\n    m_fwd = []\n    for l in range(1, L + 1):\n        # Calculate empirical second raw moment over batch and units.\n        moment = np.mean(x_values[l]**2)\n        m_fwd.append(moment)\n\n    # 4. Backward Pass\n    dz_values = {}\n    \n    # Base case: Gradient of the loss w.r.t final pre-activations.\n    # For L = 0.5 * ||x_L||^2, dL/dz_L = x_L.\n    # This simplification comes from dL/dx_L = x_L and dL/dz_L = dL/dx_L * phi'(z_L),\n    # which simplifies to x_L * phi'(z_L) = phi(z_L) = x_L.\n    dz_L = x_values[L]\n    dz_values[L] = dz_L\n    \n    # Propagate gradients backward from L-1 to 1.\n    dz_current = dz_L\n    for l in range(L - 1, 0, -1):\n        # Gradient w.r.t previous activation layer\n        dx_l = dz_current @ weights[l].T\n        \n        # Derivative of ReLU\n        phi_prime_l = (z_values[l] > 0).astype(float)\n        \n        # Gradient w.r.t pre-activation layer\n        dz_l = dx_l * phi_prime_l\n        \n        dz_values[l] = dz_l\n        dz_current = dz_l\n\n    # 5. Compute Backward Moments\n    m_bwd = []\n    for l in range(1, L + 1):\n        # Calculate empirical second raw moment over batch and units.\n        moment = np.mean(dz_values[l]**2)\n        m_bwd.append(moment)\n\n    # 6. Check Pass/Fail Condition\n    epsilon = 0.2\n    \n    # Maximum absolute deviation from 1 for forward moments\n    delta_fwd = np.max(np.abs(np.array(m_fwd) - 1))\n    \n    # Maximum absolute deviation from 1 for backward moments\n    delta_bwd = np.max(np.abs(np.array(m_bwd) - 1))\n\n    return delta_fwd  epsilon and delta_bwd  epsilon\n\nif __name__ == '__main__':\n    solve()\n```", "id": "3194508"}, {"introduction": "掌握了He初始化的基本原理及其在标准网络结构中的应用后，真正的挑战在于如何将其思想推广到更现代、更复杂的架构中。现实世界中的网络通常包含残差连接（residual connections）和丢弃（dropout）等组件，这些都会改变信号传播的动态。本练习要求您从第一性原理出发，为一种包含ReLU、残差连接和“倒置丢弃”（inverted dropout）的复合网络块推导出一个定制化的He风格初始化准则，这不仅能检验您对理论的掌握程度，更能培养您将核心原理应用于新问题的设计与分析能力[@problem_id:3134404]。", "problem": "考虑一个基于注意力的多层感知器 (MLP) 内的残差块，它通过以下方式将输入向量 $x \\in \\mathbb{R}^{d}$ 映射到输出 $y \\in \\mathbb{R}^{d}$：\n$$\ny \\;=\\; x \\;+\\; \\text{Dropout}\\big(\\phi(Wx)\\big),\n$$\n其中 $\\phi(x)$ 表示逐元素应用的修正线性单元 (ReLU) 非线性，并且使用的 dropout 是标准的“倒置”dropout：对于每个坐标 $i$，$\\text{Dropout}(u)_i = \\frac{m_i}{p} u_i$，其中掩码 $m_i \\sim \\text{Bernoulli}(p)$ 是独立的，而 $p \\in (0,1]$ 是保留概率。假设：\n- $x$ 的条目是独立同分布的，均值为零，每个坐标的二阶矩为 $\\mathbb{E}[x_i^{2}] = v$，对于某个 $v  0$。\n- 权重矩阵 $W \\in \\mathbb{R}^{d \\times d}$ 的条目是独立的，均值为零，方差为 $\\mathbb{V}[W_{ij}] = \\sigma_w^{2}/n$，其中 $n=d$ 是扇入 (fan-in)。\n- 在平均场机制下，预激活值 $z = Wx$ 的坐标近似为均值为零的高斯分布，并且在初始化时被视为与 $x$ 无关。\n\n仅使用关于独立和、二阶矩以及类高斯输入对称性的核心定义和经过充分检验的事实，为该残差块建立一个定制的 He 风格准则：选择 $\\sigma_w^{2}$，使得 $y$ 中两个相加分量的逐坐标二阶矩是平衡的，\n$$\n\\mathbb{E}[x_i^{2}] \\;=\\; \\mathbb{E}\\!\\left[\\big(\\text{Dropout}(\\phi(Wx))_i\\big)^{2}\\right].\n$$\n从第一性原理出发，推导出一个满足上述平衡条件的 $\\sigma_w^{2}$ 关于 $p$ 的闭式表达式。你的最终答案必须是单个解析表达式。不需要四舍五入，也不涉及单位。最终的 $\\sigma_w^{2}$ 仅用 $p$ 的符号形式表示。", "solution": "我们从给定的块结构和假设开始。目标是在初始化时，对每个坐标 $i$ 强制实现平衡：\n$$\n\\mathbb{E}[x_i^{2}] \\;=\\; \\mathbb{E}\\!\\left[\\big(\\text{Dropout}(\\phi(Wx))_i\\big)^{2}\\right]\n$$\n根据假设，$\\mathbb{E}[x_i^{2}] = v$。我们必须用 $\\sigma_w^{2}$ 和 $p$ 来表示右侧的表达式。\n\n第一步：计算预激活值的二阶矩。设 $z = Wx$，并将其第 $i$ 个坐标表示为 $z_i = \\sum_{j=1}^{n} W_{ij} x_j$。利用独立性、零均值以及独立项之和的二阶矩等于各项二阶矩之和这一事实，我们有\n$$\n\\mathbb{E}[z_i^{2}] \\;=\\; \\sum_{j=1}^{n} \\mathbb{E}[W_{ij}^{2}]\\,\\mathbb{E}[x_j^{2}]\n\\;=\\; \\sum_{j=1}^{n} \\Big(\\frac{\\sigma_w^{2}}{n}\\Big)\\, v\n\\;=\\; \\sigma_w^{2}\\, v.\n$$\n这里用到了扇入 $n$ 和每个权重的方差为 $\\sigma_w^{2}/n$。\n\n第二步：通过修正线性单元 (ReLU)。定义 $\\phi(z_i) = \\max(0, z_i)$。对于一个零均值对称的输入分布（如我们对 $z_i$ 所假设的），经过 ReLU 后的二阶矩遵循一个对称性质。具体来说，对于任何零均值对称的随机变量 $Z$，指示函数 $1_{\\{Z0\\}}$ 恰好选择了其一半的质量，并且由于 $Z^{2}$ 是一个偶函数，正半部分对 $\\mathbb{E}[Z^{2}]$ 的贡献恰好是一半：\n$$\n\\mathbb{E}[(\\max(0, Z))^{2}] \\;=\\; \\mathbb{E}[Z^{2}\\,1_{\\{Z0\\}}] \\;=\\; \\frac{1}{2}\\,\\mathbb{E}[Z^{2}].\n$$\n将此应用于 $z_i$，我们得到\n$$\n\\mathbb{E}[\\phi(z_i)^{2}] \\;=\\; \\frac{1}{2}\\,\\mathbb{E}[z_i^{2}] \\;=\\; \\frac{1}{2}\\,\\sigma_w^{2}\\, v.\n$$\n\n第三步：应用倒置 dropout。对于倒置 dropout，每个坐标 $i$ 都有：\n$$\n\\text{Dropout}(\\phi(z))_i \\;=\\; \\frac{m_i}{p}\\,\\phi(z_i),\n$$\n其中 $m_i \\sim \\text{Bernoulli}(p)$ 独立于 $\\phi(z_i)$ 以及其他所有变量。其二阶矩为\n$$\n\\mathbb{E}\\!\\left[\\big(\\tfrac{m_i}{p}\\,\\phi(z_i)\\big)^{2}\\right]\n\\;=\\; \\mathbb{E}\\!\\left[\\big(\\tfrac{m_i}{p}\\big)^{2}\\right]\\;\\mathbb{E}[\\phi(z_i)^{2}]\n\\;=\\; \\frac{\\mathbb{E}[m_i]}{p^{2}}\\;\\mathbb{E}[\\phi(z_i)^{2}]\n\\;=\\; \\frac{p}{p^{2}}\\;\\mathbb{E}[\\phi(z_i)^{2}]\n\\;=\\; \\frac{1}{p}\\,\\mathbb{E}[\\phi(z_i)^{2}],\n$$\n因为对于伯努利掩码有 $m_i^{2} = m_i$，并且 $\\mathbb{E}[m_i] = p$。代入第二步的结果，得到\n$$\n\\mathbb{E}\\!\\left[\\big(\\text{Dropout}(\\phi(Wx))_i\\big)^{2}\\right]\n\\;=\\; \\frac{1}{p}\\cdot \\frac{1}{2}\\,\\sigma_w^{2}\\, v\n\\;=\\; \\frac{\\sigma_w^{2}}{2p}\\, v.\n$$\n\n第四步：强制执行平衡准则。问题所要求的条件是\n$$\n\\mathbb{E}[x_i^{2}] \\;=\\; \\mathbb{E}\\!\\left[\\big(\\text{Dropout}(\\phi(Wx))_i\\big)^{2}\\right].\n$$\n代入 $\\mathbb{E}[x_i^{2}] = v$ 和上面的表达式，我们得到\n$$\nv \\;=\\; \\frac{\\sigma_w^{2}}{2p}\\, v.\n$$\n因为 $v0$，我们可以将两边同时除以 $v$ 来分离出 $\\sigma_w^{2}$：\n$$\n1 \\;=\\; \\frac{\\sigma_w^{2}}{2p}\n\\quad\\Longrightarrow\\quad\n\\sigma_w^{2} \\;=\\; 2p.\n$$\n\n因此，为了在初始化时平衡跳跃连接路径和 ReLU-dropout 残差路径的逐坐标二阶矩，应选择权重方差缩放因子，使得 $W$ 的每个条目具有方差 $\\mathbb{V}[W_{ij}] = \\frac{2p}{n}$，即在此处使用的扇入参数化中，$\\sigma_w^{2} = 2p$。", "answer": "$$\\boxed{2p}$$", "id": "3134404"}]}