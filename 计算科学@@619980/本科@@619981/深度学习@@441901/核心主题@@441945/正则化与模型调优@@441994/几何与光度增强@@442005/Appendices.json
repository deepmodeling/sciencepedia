{"hands_on_practices": [{"introduction": "光度增强（如调整亮度和对比度）是计算机视觉中常用的技术。然而，这些变换的效果可能因其在图像处理流程中的应用位置而异，尤其是在涉及非线性变换（如伽马校正）时。本练习通过一个简化的相机模型，让您动手探索在相机的线性原始（RAW）空间与非线性显示空间中应用增强的区别，从而帮助您理解如何实现跨设备更一致的增强效果。[@problem_id:3129353]", "problem": "考虑一个简化的相机成像管线来研究光度增强。设一个合成的一维图像由一个包含 $N$ 个样本的向量 $x \\in [0,1]^N$ 表示，该向量近似于相机原始域 (raw domain) 中的归一化线性传感器测量值。假设 $x$ 是一个均匀斜坡，$x_i = \\frac{i}{N-1}$，其中 $i \\in \\{0,1,\\dots,N-1\\}$，因此 $x_0 = 0$ 且 $x_{N-1} = 1$。定义两种增强算子：亮度抖动和对比度抖动。因子为 $\\beta > 0$ 的亮度抖动是算子 $B_{\\beta}(x) = \\operatorname{clip}_{[0,1]}(\\beta x)$，其中 $\\operatorname{clip}_{[0,1]}(z)$ 表示将 $z$ 的每个分量裁剪到区间 $[0,1]$ 内。因子为 $\\kappa > 0$ 的对比度抖动是算子 $C_{\\kappa}(z) = \\operatorname{clip}_{[0,1]}\\big((z - \\mu_z)\\kappa + \\mu_z\\big)$，其中 $\\mu_z = \\frac{1}{N}\\sum_{i=0}^{N-1} z_i$ 是 $z$ 的平均强度。定义组合抖动算子 $J_{\\beta,\\kappa}(x) = C_{\\kappa}(B_{\\beta}(x))$。\n\n相机管线通过色调映射函数将原始测量值 $x$ 映射到显示参考域 (display-referred domain)。对于伽马参数为 $g_c > 0$ 的相机 $c$，定义色调映射为 $T_{g_c}(x) = \\operatorname{clip}_{[0,1]}(x)^{1/g_c}$，该运算按分量施加。这是标准红绿蓝 (sRGB) 中使用的伽马编码的一个简化模型，通常用指数接近 $1/2.2$ 的幂律来近似。真实的图像信号处理 (ISP) 管线包含更多步骤，但对于本问题，色调映射 $T_{g_c}$ 足以捕捉关键的非线性特性。\n\n我们比较两个增强阶段：\n- ISP前抖动 (Pre-ISP jitter): $y^{\\text{pre}}_{c} = T_{g_c}\\big(J_{\\beta,\\kappa}(x)\\big)$。\n- ISP后抖动 (Post-ISP jitter): $y^{\\text{post}}_{c} = J_{\\beta,\\kappa}\\big(T_{g_c}(x)\\big)$。\n\n为量化跨相机不变性，考虑一组伽马值为 $\\{g_c\\}_{c=1}^{M}$ 的相机。对于阶段 $s \\in \\{\\text{pre}, \\text{post}\\}$ 和像素索引 $i$，设 $\\sigma^{(s)}_i$ 为在像素 $i$ 处跨相机的标准差：\n$$\n\\sigma^{(s)}_i = \\sqrt{\\frac{1}{M}\\sum_{c=1}^{M}\\left(y^{(s)}_{c,i} - \\bar{y}^{(s)}_{i}\\right)^2}, \\quad \\text{其中 } \\bar{y}^{(s)}_{i} = \\frac{1}{M}\\sum_{c=1}^{M} y^{(s)}_{c,i}.\n$$\n定义阶段 $s$ 的差异度 (discrepancy) 为这些逐像素标准差的平均值：\n$$\nD^{(s)} = \\frac{1}{N}\\sum_{i=0}^{N-1} \\sigma^{(s)}_i.\n$$\n较小的 $D^{(s)}$ 表示跨相机的不变性更高。对于每个测试用例，确定ISP前阶段是否比ISP后阶段产生更低的差异度，即 $D^{(\\text{pre})}  D^{(\\text{post})}$ 是否成立。\n\n使用 $N = 1024$。您的程序必须精确实现上述定义，并评估以下测试套件，每个测试用例由一组相机伽马参数和抖动参数 $(\\beta,\\kappa)$ 指定：\n- 用例 1：$\\{g_c\\} = \\{2.0, 2.2, 2.4\\}$，$(\\beta,\\kappa) = (1.1, 1.2)$。\n- 用例 2：$\\{g_c\\} = \\{2.0, 2.4, 2.8\\}$，$(\\beta,\\kappa) = (1.0, 1.0)$。\n- 用例 3：$\\{g_c\\} = \\{1.6, 2.2, 2.8\\}$，$(\\beta,\\kappa) = (1.5, 1.5)$。\n- 用例 4：$\\{g_c\\} = \\{2.2, 2.2, 2.2\\}$，$(\\beta,\\kappa) = (1.3, 0.7)$。\n\n对于每个用例，按定义计算 $D^{(\\text{pre})}$ 和 $D^{(\\text{post})}$，然后生成一个布尔结果 $\\mathsf{pre\\_better}$，如果 $D^{(\\text{pre})}  D^{(\\text{post})}$ 则为 $\\text{True}$，否则为 $\\text{False}$。不涉及物理单位；强度是无量纲的，且限制在 $[0,1]$ 范围内。未使用角度。\n\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表（例如，$[\\text{True},\\text{False},\\text{True},\\text{False}]$），结果按用例 1 到用例 4 的顺序排列。", "solution": "该问题被评估为有效。它在科学上基于数字图像处理的原理，问题提法清晰，具有唯一且可计算的解，并以客观的数学形式表述。\n\n任务是针对四种不同场景，确定在非线性色调映射操作（ISP前）之前应用光度增强，是否比在其之后应用（ISP后）能在不同相机模型间产生更小的可变性。可变性的度量是一个差异度指标 $D^{(s)}$，定义为最终图像强度在一组相机上的逐像素标准差的平均值。较低的差异度表示较高的不变性。\n\n解决方案首先通过算法定义成像管线和增强过程的各个组件，然后将它们组合起来，计算ISP前和ISP后两个阶段的差异度。\n\n**1. 基本定义和结构**\n\n初始输入是一个一维合成图像 $x$，一个包含 $N$ 个样本的向量。当 $N=1024$ 时，该向量被定义为一个线性斜坡：\n$$\nx_i = \\frac{i}{N-1} \\quad \\text{其中 } i \\in \\{0, 1, \\dots, N-1\\}\n$$\n这确保了输入强度值覆盖了整个归一化的 $[0, 1]$ 范围。\n\n所有操作都定义在向量上，并被裁剪到有效的归一化强度范围 $[0, 1]$。按分量裁剪的函数表示为 $\\operatorname{clip}_{[0,1]}(z)$。\n\n**2. 光度增强算子**\n\n定义了两种抖动算子：\n\n- **亮度抖动：** 该算子 $B_{\\beta}$ 将图像强度按因子 $\\beta  0$ 进行缩放。\n$$\nB_{\\beta}(z) = \\operatorname{clip}_{[0,1]}(\\beta z)\n$$\n- **对比度抖动：** 该算子 $C_{\\kappa}$ 围绕图像平均强度 $\\mu_z$ 按因子 $\\kappa  0$ 调整强度的分布范围。\n$$\nC_{\\kappa}(z) = \\operatorname{clip}_{[0,1]}\\big((z - \\mu_z)\\kappa + \\mu_z\\big), \\quad \\text{其中 } \\mu_z = \\frac{1}{N}\\sum_{i=0}^{N-1} z_i\n$$\n组合抖动算子 $J_{\\beta,\\kappa}$ 先应用亮度抖动，再应用对比度抖动：\n$$\nJ_{\\beta,\\kappa}(x) = C_{\\kappa}(B_{\\beta}(x))\n$$\n\n**3. 相机色调映射**\n\n相机的图像信号处理器 (ISP) 由一个非线性色调映射函数 $T_{g_c}$ 建模，该函数使用相机特定的参数 $g_c  0$ 执行伽马校正。\n$$\nT_{g_c}(x) = \\operatorname{clip}_{[0,1]}(x)^{1/g_c}\n$$\n该操作按分量施加。对输入 $x$ 进行裁剪可确保幂函数的底为非负数，尽管在此问题中，由于先前的操作，$T_{g_c}$ 的输入始终在 $[0, 1]$ 范围内。\n\n**4. 处理管线**\n\n我们评估两种应用增强的不同管线：\n\n- **ISP前抖动：** 在线性原始图像 $x$ 上执行增强。然后，生成的图像由每个相机的色调映射处理。对于伽马值为 $g_c$ 的相机 $c$，输出图像 $y^{\\text{pre}}_c$ 为：\n$$\ny^{\\text{pre}}_{c} = T_{g_c}\\big(J_{\\beta,\\kappa}(x)\\big)\n$$\n- **ISP后抖动：** 线性原始图像 $x$ 首先由每个相机的色调映射处理。然后，增强操作被应用于每个生成的显示参考图像。\n$$\ny^{\\text{post}}_{c} = J_{\\beta,\\kappa}\\big(T_{g_c}(x)\\big)\n$$\n\n问题的核心在于抖动算子 $J_{\\beta,\\kappa}$ 和非线性色调映射 $T_{g_c}$ 的非交换性。\n\n**5. 差异度计算**\n\n为量化每个管线阶段 $s \\in \\{\\text{pre}, \\text{post}\\}$ 的跨相机不变性，我们首先计算在一组 $M$ 个相机上的逐像素标准差 $\\sigma^{(s)}_i$。\n$$\n\\sigma^{(s)}_i = \\sqrt{\\frac{1}{M}\\sum_{c=1}^{M}\\left(y^{(s)}_{c,i} - \\bar{y}^{(s)}_{i}\\right)^2}\n$$\n其中 $\\bar{y}^{(s)}_{i} = \\frac{1}{M}\\sum_{c=1}^{M} y^{(s)}_{c,i}$ 是所有相机在像素 $i$ 处的平均强度。这是总体标准差。\n\n阶段 $s$ 的总差异度 $D^{(s)}$ 是这些标准差在所有 $N$ 个像素上的平均值：\n$$\nD^{(s)} = \\frac{1}{N}\\sum_{i=0}^{N-1} \\sigma^{(s)}_i\n$$\n\n**6. 算法实现计划**\n\n对于每个由一组伽马值 $\\{g_c\\}_{c=1}^{M}$ 和抖动参数 $(\\beta, \\kappa)$ 定义的测试用例：\n\n1.  生成大小为 $N=1024$ 的初始向量 $x$。\n2.  **计算 $D^{(\\text{pre})}$：**\n    a.  计算抖动后的图像 $x' = J_{\\beta,\\kappa}(x)$。\n    b.  对于每个 $g_c$，计算 $y^{\\text{pre}}_c = T_{g_c}(x')$。这将产生一组 $M$ 个向量，$\\{y^{\\text{pre}}_c\\}$。\n    c.  用这些向量构成一个形状为 $(M, N)$ 的矩阵。\n    d.  沿相机轴（轴 0）计算标准差，得到一个由 $\\sigma^{(\\text{pre})}_i$ 值组成的向量。\n    e.  计算该向量的平均值以得到 $D^{(\\text{pre})}$。\n\n3.  **计算 $D^{(\\text{post})}$：**\n    a.  对于每个 $g_c$，计算色调映射后的图像 $x'_c = T_{g_c}(x)$。这将产生一组 $M$ 个向量，$\\{x'_c\\}$。\n    b.  对于每个 $x'_c$，计算最终图像 $y^{\\text{post}}_c = J_{\\beta,\\kappa}(x'_c)$。\n    c.  用这些最终向量 $\\{y^{\\text{post}}_c\\}$ 构成一个形状为 $(M, N)$ 的矩阵。\n    d.  沿相机轴（轴 0）计算标准差以获得 $\\sigma^{(\\text{post})}_i$。\n    e.  计算该向量的平均值以得到 $D^{(\\text{post})}$。\n\n4.  **比较并得出结论：** 测试用例的最终结果是表达式 $D^{(\\text{pre})}  D^{(\\text{post})}$ 的布尔值。在所有 $g_c$ 都相同的特殊情况（用例 4）下，将得到 $D^{(\\text{pre})} = 0$ 和 $D^{(\\text{post})} = 0$，因此比较 $0  0$ 的结果为 $\\text{False}$。\n\n此过程将对指定的四个测试用例中的每一个执行。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem by calculating and comparing discrepancy for pre-ISP and post-ISP augmentations\n    for a given set of test cases.\n    \"\"\"\n\n    N = 1024\n    test_cases = [\n        # Case 1: {g_c} = {2.0, 2.2, 2.4}, (beta, kappa) = (1.1, 1.2)\n        ({'gammas': [2.0, 2.2, 2.4], 'beta': 1.1, 'kappa': 1.2}),\n        # Case 2: {g_c} = {2.0, 2.4, 2.8}, (beta, kappa) = (1.0, 1.0)\n        ({'gammas': [2.0, 2.4, 2.8], 'beta': 1.0, 'kappa': 1.0}),\n        # Case 3: {g_c} = {1.6, 2.2, 2.8}, (beta, kappa) = (1.5, 1.5)\n        ({'gammas': [1.6, 2.2, 2.8], 'beta': 1.5, 'kappa': 1.5}),\n        # Case 4: {g_c} = {2.2, 2.2, 2.2}, (beta, kappa) = (1.3, 0.7)\n        ({'gammas': [2.2, 2.2, 2.2], 'beta': 1.3, 'kappa': 0.7}),\n    ]\n\n    results = []\n\n    # Create the base synthetic image x\n    # x_i = i / (N - 1) for i in {0, ..., N-1}\n    x = np.linspace(0.0, 1.0, N)\n\n    def brightness_jitter(img, beta):\n        \"\"\"Applies brightness jitter B_beta(z) = clip_01(beta * z).\"\"\"\n        return np.clip(beta * img, 0.0, 1.0)\n\n    def contrast_jitter(img, kappa):\n        \"\"\"Applies contrast jitter C_kappa(z).\"\"\"\n        # For a single image (1D array)\n        if img.ndim == 1:\n            mu_z = np.mean(img)\n            return np.clip((img - mu_z) * kappa + mu_z, 0.0, 1.0)\n        # For a batch of images (2D array), operating on each row\n        elif img.ndim == 2:\n            mu_z = np.mean(img, axis=1, keepdims=True)\n            return np.clip((img - mu_z) * kappa + mu_z, 0.0, 1.0)\n        else:\n            raise ValueError(\"Input must be 1D or 2D array.\")\n\n    def combined_jitter(img, beta, kappa):\n        \"\"\"Applies combined jitter J_{beta,kappa}(z) = C_kappa(B_beta(z)).\"\"\"\n        img_bright = brightness_jitter(img, beta)\n        img_contrast = contrast_jitter(img_bright, kappa)\n        return img_contrast\n\n    def tone_map(img, gammas):\n        \"\"\"Applies tone mapping T_g(z) = clip_01(z)^(1/g).\"\"\"\n        # Ensure gammas is a column vector for broadcasting\n        g_vec = np.array(gammas).reshape(-1, 1)\n        # Definition is clip(x)^(1/g), but inputs are already in [0,1].\n        # We clip for strict adherence to the formula.\n        img_clipped = np.clip(img, 0.0, 1.0)\n        return np.power(img_clipped, 1.0 / g_vec)\n\n    for case in test_cases:\n        gammas = case['gammas']\n        beta = case['beta']\n        kappa = case['kappa']\n        M = len(gammas)\n\n        # --- Pre-ISP Jitter Pipeline ---\n        # y_pre = T_g(J(x))\n        # 1. Apply combined jitter to the single raw image x.\n        x_jittered = combined_jitter(x, beta, kappa)\n\n        # 2. Apply tone mapping for all cameras. Broadcasting handles this efficiently.\n        Y_pre = tone_map(x_jittered, gammas) # Shape (M, N)\n\n        # 3. Calculate discrepancy D_pre\n        # Population standard deviation across cameras (axis=0), ddof=0 is default\n        sigma_pre_i = np.std(Y_pre, axis=0, ddof=0)\n        D_pre = np.mean(sigma_pre_i)\n\n        # --- Post-ISP Jitter Pipeline ---\n        # y_post = J(T_g(x))\n        # 1. Apply tone mapping to raw image x for each camera.\n        X_tone_mapped = tone_map(x, gammas) # Shape (M, N)\n        \n        # 2. Apply combined jitter to the batch of M tone-mapped images.\n        Y_post = combined_jitter(X_tone_mapped, beta, kappa) # Shape (M, N)\n\n        # 3. Calculate discrepancy D_post\n        sigma_post_i = np.std(Y_post, axis=0, ddof=0)\n        D_post = np.mean(sigma_post_i)\n\n        # --- Comparison ---\n        pre_is_better = D_pre  D_post\n        results.append(pre_is_better)\n        \n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3129353"}, {"introduction": "遮挡是现实世界中常见的挑战，因此在训练数据中模拟遮挡是一种重要的几何增强方法。然而，并非所有遮挡都是相同的：一个随机的方块遮挡（如Cutout）与一个沿物体边缘的条状遮挡可能对模型提出不同的挑战。本练习通过一个合成的实例分割任务，让您比较通用随机遮挡与结构化语义遮挡的效果，从而深入理解如何设计更符合现实世界统计特性的数据增强策略。[@problem_id:3129324]", "problem": "您将解决一个纯合成的实例分割鲁棒性评估问题，该问题旨在比较两种由数据增强驱动的代理模型在受控遮挡下对二值图像的处理能力。任务是实现一个程序，该程序能够构建一个真值对象掩码，应用指定的遮挡物，模拟对应两种增强策略的两个代理预测器，然后在一系列测试用例中测量鲁棒性提升度量。\n\n使用的基本原理和定义：\n- 二值图像是像素坐标离散网格的一个子集。设图像大小为 $H \\times W$，其中 $H \\in \\mathbb{N}$ 且 $W \\in \\mathbb{N}$。前景掩码表示为一个集合 $S \\subseteq \\{0,1,\\dots,H-1\\} \\times \\{0,1,\\dots,W-1\\}$。\n- 真值对象掩码 $G$ 是一个由左上角和右下角坐标定义的轴对齐实心矩形。该轴对齐矩形内的所有像素都属于 $G$。\n- 遮挡物是一个轴对齐的实心矩形 $R$，它会将其与对象重叠区域的像素值置零。观测掩码为 $M = G \\setminus R$，对应于遮挡后对象的可视部分。\n- 形态学闭运算由带有结构元素 $B$ 的标准集合运算定义为 $\\mathrm{Close}(A,B) = (A \\oplus B) \\ominus B$，其中 $\\oplus$ 表示使用足迹 $B$ 在二值集合上进行的膨胀操作，$\\ominus$ 表示腐蚀操作。您必须实现与离散网格和足迹卷积一致的二值膨胀和二值腐蚀语义。\n- 两个代理预测器模拟了使用不同增强分布训练的模型：\n  1. 随机擦除（类似 cutout）的代理，记为 $P_R$：使用边长为 $k_R$ 像素的全为1的方形结构元素 $B_R \\in \\{0,1\\}^{k_R \\times k_R}$，并计算 $P_R = \\mathrm{Close}(M, B_R)$。\n  2. 语义对齐遮挡物（其边缘与对象边缘对齐的几何遮挡）的代理，记为 $P_S$：使用两个全为1的线性结构元素，即水平线 $B_{S,h} \\in \\{0,1\\}^{1 \\times k_S}$ 和垂直线 $B_{S,v} \\in \\{0,1\\}^{k_S \\times 1}$，并计算 $P_S = \\mathrm{Close}(M, B_{S,h}) \\cup \\mathrm{Close}(M, B_{S,v})$。\n- 任意预测 $P$ 与真值 $G$ 之间的交并比 (IoU, Intersection over Union) 为\n$$\n\\mathrm{IoU}(P,G) = \\frac{|P \\cap G|}{|P \\cup G|},\n$$\n其中 $|\\cdot|$ 表示有限集的基数。鲁棒性提升定义为\n$$\n\\Delta = \\mathrm{IoU}(P_S,G) - \\mathrm{IoU}(P_R,G).\n$$\n\n角度单位：所有矩形都是轴对齐的；不存在非轴向角。本问题不涉及物理单位。\n\n为每个测试用例实现以下步骤：\n1. 构建一个大小为 $H \\times W$ 的二值图像，其中 $H = 64$ 且 $W = 64$。\n2. 根据给定的左上角坐标 $(y_0, x_0) = (16,16)$ 和右下角坐标 $(y_1, x_1) = (48,48)$ 构建真值矩形 $G$。索引从零开始，范围在自然数组意义上是半开的，即所有满足 $16 \\le y  48$ 和 $16 \\le x  48$ 的整数坐标 $(y, x)$ 都属于 $G$。\n3. 通过将重叠区域置零来应用指定的遮挡矩形 $R$，从而形成 $M = G \\setminus R$。如果遮挡物不与 $G$ 相交，则 $M=G$。如果遮挡物完全覆盖 $G$，则 $M = \\varnothing$。\n4. 按照上述方法计算 $P_R$ 和 $P_S$，其中 $k_R = 5$ 且 $k_S = 7$。\n5. 计算 $\\mathrm{IoU}(P_R,G)$ 和 $\\mathrm{IoU}(P_S,G)$，然后计算该用例的 $\\Delta$。\n6. 报告 $\\Delta$ 值，四舍五入到4位小数。\n\n遮挡场景测试套件：\n- 用例 A（理想情况，对齐的垂直条带，其宽度小于语义线长度）：遮挡矩形的左上角为 $(y_0^o, x_0^o) = (20,16)$，右下角为 $(y_1^o, x_1^o) = (44,22)$，该矩形与对象的左边缘重叠，宽度为 $6$ 像素。\n- 用例 B（中心随机擦除，孔洞宽度大于两个结构元素）：遮挡矩形的左上角为 $(y_0^o, x_0^o) = (27,27)$，右下角为 $(y_1^o, x_1^o) = (37,37)$，形成一个 $10 \\times 10$ 的中心孔洞。\n- 用例 C（对齐的水平条带，其宽度大于语义线长度）：遮挡矩形的左上角为 $(y_0^o, x_0^o) = (16,24)$，右下角为 $(y_1^o, x_1^o) = (24,40)$，该矩形与对象的上边缘重叠，高度为 $8$ 像素。\n- 用例 D（无遮挡边界情况）：不应用遮挡物；等效于遮挡矩形为空。\n- 用例 E（极端遮挡，完全覆盖）：遮挡矩形与 $G$ 相同，即从 $(16,16)$ 到 $(48,48)$。\n\n您的程序应生成单行输出，其中包含按顺序排列的用例 A 到 E 的鲁棒性提升值，格式为一个由方括号括起来的、逗号分隔的浮点数列表，每个浮点数四舍五入到4位小数。例如，一个正确格式的输出行为 $[\\Delta_A,\\Delta_B,\\Delta_C,\\Delta_D,\\Delta_E]$，每个 $\\Delta$ 显示4位小数且无空格。", "solution": "该问题要求在不同遮挡场景下，对两个用于实例分割鲁棒性的代理模型进行计算评估。评估在合成的二值图像上进行。任务的核心是实现指定的图像生成流程，包括形态学操作，并计算鲁棒性提升度量 $\\Delta$。\n\n首先，我们为该问题建立数学和计算框架。图像空间是一个大小为 $H \\times W$ 的离散像素网格，其中 $H=64$ 且 $W=64$。二值掩码表示为像素网格 $\\{0, 1, \\dots, H-1\\} \\times \\{0, 1, \\dots, W-1\\}$ 的子集，在计算上作为 $H \\times W$ 的布尔数组处理。\n\n真值对象掩码 $G$ 是一个由其左上角坐标 $(y_0, x_0) = (16, 16)$ 和右下角坐标 $(y_1, x_1) = (48, 48)$ 定义的轴对齐实心矩形。$G$ 中的像素集合 $(y, x)$ 由 $G = \\{(y,x) \\in \\mathbb{Z}^2 \\mid 16 \\le y  48 \\text{ and } 16 \\le x  48\\}$ 给出。$G$ 中的总像素数为 $|G| = (48-16) \\times (48-16) = 32 \\times 32 = 1024$。\n\n遮挡由另一个轴对齐的矩形 $R$ 建模。代表对象可见部分的观测掩码 $M$ 是集合差 $M = G \\setminus R$。在布尔数组方面，这对应于真值掩码数组与遮挡物掩码数组的取反进行逻辑与操作。\n\n预测模型的核心在于形态学操作。对于一个二值图像 $A$ 和一个结构元素 $B$（两者都表示为像素坐标集），二值膨胀和腐蚀定义如下：\n- 膨胀：$A \\oplus B = \\bigcup_{a \\in A, b \\in B} \\{a+b\\}$。此操作会扩展前景区域的边界。\n- 腐蚀：$A \\ominus B = \\{z \\mid z+b \\in A \\text{ for all } b \\in B\\}$。此操作会收缩前景区域的边界。\n这些操作将使用 `scipy.ndimage.binary_dilation` 和 `scipy.ndimage.binary_erosion` 函数来实现，它们提供了高效且正确的实现。\n\n一个集合 $A$ 关于结构元素 $B$ 的形态学闭运算定义为先膨胀后腐蚀：$\\mathrm{Close}(A,B) = (A \\oplus B) \\ominus B$。此操作能有效填充前景对象中的小孔洞和闭合窄缝隙。\n\n定义了两个代理模型，用于模拟使用不同增强策略训练的预测器：\n\n1.  预测器 $P_R$，随机擦除的代理：该模型使用方形结构元素 $B_R$ 进行形态学闭运算。$B_R$ 是一个大小为 $k_R \\times k_R$ 的全1矩阵，其中 $k_R=5$。预测结果为 $P_R = \\mathrm{Close}(M, B_R)$。该模型预计能有效修复小的、各向同性（紧凑）的孔洞。\n\n2.  预测器 $P_S$，语义对齐遮挡物的代理：该模型结合了使用两个线性结构元素的闭运算：一个大小为 $1 \\times k_S$ 的水平线 $B_{S,h}$ 和一个大小为 $k_S \\times 1$ 的垂直线 $B_{S,v}$，其中 $k_S=7$。预测结果是这两个闭运算结果的并集：$P_S = \\mathrm{Close}(M, B_{S,h}) \\cup \\mathrm{Close}(M, B_{S,v})$。据推测，该模型更擅长重建被与对象主轴对齐的细长形状遮挡的部分。\n\n这些预测器的性能通过交并比 (IoU) 度量进行量化，该度量是相对于真值掩码 $G$ 计算的：\n$$\n\\mathrm{IoU}(P,G) = \\frac{|P \\cap G|}{|P \\cup G|}\n$$\n对于布尔数组 `P_arr` 和 `G_arr`，其计算方式为 `np.sum(P_arr  G_arr) / np.sum(P_arr | G_arr)`。\n\n最终要报告的度量是鲁棒性提升 $\\Delta$，定义为两个预测器 IoU 分数的差值：\n$$\n\\Delta = \\mathrm{IoU}(P_S,G) - \\mathrm{IoU}(P_R,G)\n$$\n正值的 $\\Delta$ 表示语义对齐预测器 $P_S$ 对被遮挡对象的重建效果优于随机擦除预测器 $P_R$。\n\n每个测试用例的流程如下：\n1.  定义图像尺寸 $H=64, W=64$。将真值掩码 $G$ 创建为一个 $64 \\times 64$ 的布尔数组。\n2.  针对具体用例，将遮挡物掩码 $R$ 创建为一个布尔数组。通过使用一个空掩码来处理无遮挡情况。\n3.  通过应用遮挡来计算观测掩码 $M$：`M = G  (~R)`。\n4.  将结构元素 $B_R$、$B_{S,h}$ 和 $B_{S,v}$ 定义为具有指定维度的布尔 NumPy 数组。\n5.  通过用 $B_R$ 膨胀 $M$，然后用 $B_R$ 腐蚀结果来计算预测 $P_R$。\n6.  首先计算 $P_{S,h} = \\mathrm{Close}(M, B_{S,h})$ 和 $P_{S,v} = \\mathrm{Close}(M, B_{S,v})$，然后取它们的逻辑并集 `P_Sh | P_Sv`，从而计算出预测 $P_S$。\n7.  计算 $\\mathrm{IoU}(P_R,G)$ 和 $\\mathrm{IoU}(P_S,G)$。\n8.  计算 $\\Delta = \\mathrm{IoU}(P_S,G) - \\mathrm{IoU}(P_R,G)$。\n9.  将最终的 $\\Delta$ 值四舍五入到4位小数。\n\n对五个指定的遮挡场景（用例 A-E）中的每一个重复此过程，并将结果汇总成指定的输出格式。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.ndimage import binary_erosion, binary_dilation\n\ndef solve():\n    \"\"\"\n    Implements the full pipeline to evaluate robustness of two proxy models\n    under various occlusion scenarios and computes the robustness improvement metric.\n    \"\"\"\n    H, W = 64, 64\n    k_R, k_S = 5, 7\n\n    # Define the ground-truth object mask G\n    G = np.zeros((H, W), dtype=bool)\n    G[16:48, 16:48] = True\n\n    # Define structuring elements for morphological operations\n    se_R = np.ones((k_R, k_R), dtype=bool)\n    se_S_h = np.ones((1, k_S), dtype=bool)\n    se_S_v = np.ones((k_S, 1), dtype=bool)\n\n    # Test suite of occlusion scenarios\n    test_cases = [\n        # Case A: Aligned vertical strip\n        {'name': 'A', 'rect': (20, 16, 44, 22)},\n        # Case B: Central random erasing\n        {'name': 'B', 'rect': (27, 27, 37, 37)},\n        # Case C: Aligned horizontal strip\n        {'name': 'C', 'rect': (16, 24, 24, 40)},\n        # Case D: No occlusion\n        {'name': 'D', 'rect': None},\n        # Case E: Extreme occlusion (full coverage)\n        {'name': 'E', 'rect': (16, 16, 48, 48)},\n    ]\n\n    results = []\n\n    for case in test_cases:\n        # 1. Construct the occluder mask R\n        R = np.zeros((H, W), dtype=bool)\n        if case['rect']:\n            y0, x0, y1, x1 = case['rect']\n            R[y0:y1, x0:x1] = True\n\n        # 2. Compute the observed mask M\n        M = G  (~R)\n\n        # 3. Compute P_R prediction\n        # P_R = Close(M, B_R)\n        dilated_M_R = binary_dilation(M, structure=se_R)\n        P_R = binary_erosion(dilated_M_R, structure=se_R)\n\n        # 4. Compute P_S prediction\n        # P_S = Close(M, B_S_h) U Close(M, B_S_v)\n        dilated_M_Sh = binary_dilation(M, structure=se_S_h)\n        closed_M_Sh = binary_erosion(dilated_M_Sh, structure=se_S_h)\n        \n        dilated_M_Sv = binary_dilation(M, structure=se_S_v)\n        closed_M_Sv = binary_erosion(dilated_M_Sv, structure=se_S_v)\n        \n        P_S = closed_M_Sh | closed_M_Sv\n\n        # 5. Compute IoU for both predictors\n        def iou(P, G_mask):\n            intersection = np.sum(P  G_mask)\n            union = np.sum(P | G_mask)\n            if union == 0:\n                # This case implies both P and G are empty.\n                # In this problem, G is never empty, so union is only 0 if P is also G,\n                # which isn't the case for empty masks. However, for robustness:\n                return 1.0 if intersection == 0 else 0.0\n            return intersection / union\n\n        iou_R = iou(P_R, G)\n        iou_S = iou(P_S, G)\n        \n        # 6. Compute robustness improvement Delta\n        delta = iou_S - iou_R\n        \n        # 7. Round and store the result\n        results.append(round(delta, 4))\n    \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == '__main__':\n    solve()\n```", "id": "3129324"}, {"introduction": "传统的数据增强通常依赖于从预设范围中随机抽取的固定策略，但这不一定是最佳选择。更高级的方法是让模型自身“学习”如何进行增强。本练习将指导您实现一个数据依赖的、可学习的光度增强层，其中一个小型多层感知机（MLP）会为每个输入图像动态生成最佳的亮度和对比度参数，并与主分类任务一起进行端到端的训练。通过这个实践，您将掌握构建自适应增强策略的核心技术，并探索其对模型泛化能力的影响及潜在的“坍塌”风险。[@problem_id:3129310]", "problem": "您将处理一个关于合成图像光度变换的二元分类任务。每个输入图像是一个大小为 $H \\times W$ 的矩阵，像素强度在 $[0,1]$ 范围内，并被展平为一个向量 $x \\in \\mathbb{R}^{D}$，其中 $D = H \\cdot W$。标签为 $y \\in \\{0,1\\}$。此任务的基础由以下经过充分检验的事实和核心定义构成：\n- 经验风险最小化 (ERM)：给定一个带参数的模型，数据集上的经验风险是样本上的平均损失，训练过程旨在最小化此风险。\n- 二元逻辑回归：对于一个输入 $x$，分类器使用参数 $(w,b)$，其中 $w \\in \\mathbb{R}^{D}$ 且 $b \\in \\mathbb{R}$，计算 logit $z = w^{\\top} x + b$，并预测 $\\hat{p} = \\sigma(z)$，其中 $\\sigma(\\cdot)$ 是 logistic sigmoid 函数 $\\sigma(t) = \\frac{1}{1 + e^{-t}}$。对于标签 $y \\in \\{0,1\\}$，二元交叉熵损失为 $\\ell(\\hat{p}, y) = -\\left(y \\log(\\hat{p}) + (1-y)\\log(1-\\hat{p})\\right)$。\n- 光度增强：图像的亮度和对比度变换被建模为 $x \\mapsto \\mathrm{clip}(\\alpha x + \\beta, 0, 1)$，其中 $\\alpha \\in \\mathbb{R}$ 是对比度缩放因子，$\\beta \\in \\mathbb{R}$ 是亮度偏移量，而 $\\mathrm{clip}$ 函数将每个分量裁剪到区间 $[0,1]$ 内。\n- 通过可学习的多层感知机 (MLP) 实现的数据依赖增强：对于一个输入 $x$，一个带有参数 $\\phi$ 的小型多层感知机 (MLP) 输出光度参数 $\\theta(x) = (\\alpha(x), \\beta(x))$。该 MLP 由一个带有双曲正切激活函数的隐藏层组成。输出通过双曲正切函数并进行缩放以符合目标范围。\n\n您将实现的程序必须：\n1. 构建一个合成数据集。类别 $0$ 的图像包含一个中心靠近图像左侧的高斯斑点，类别 $1$ 的图像包含一个中心靠近图像右侧的高斯斑点，每个斑点都有小的随机位移和加性噪声。使用 $H = 16$，$W = 16$，并生成 $N_{\\mathrm{train}} = 120$ 个训练样本和 $N_{\\mathrm{val}} = 120$ 个验证样本。像素强度必须在 $[0,1]$ 范围内。\n2. 为训练定义三种增强模式：\n   - 无增强：$x \\mapsto x$。\n   - 数据无关的随机光度增强：$x \\mapsto \\mathrm{clip}(\\alpha x + \\beta, 0, 1)$，其中 $\\alpha$ 从 $[1 - a_{\\mathrm{max}}, 1 + a_{\\mathrm{max}}]$ 中均匀采样，$\\beta$ 从 $[-b_{\\mathrm{max}}, b_{\\mathrm{max}}]$ 中均匀采样，对每个样本和每个训练步骤独立进行。\n   - 数据依赖的学习型增强：一个 MLP 将 $x$ 映射到参数 $\\theta(x) = (\\alpha(x), \\beta(x))$。该 MLP 的输入维度为 $D$，隐藏维度为 $h$，输出维度为 $2$。设隐藏层激活前的值为 $u = W_1 x + b_1$，隐藏层激活值为 $h(x) = \\tanh(u)$，输出层激活前的值为 $o = W_2 h(x) + b_2$，并定义\n   $$\n   \\alpha(x) = 1 + a_{\\mathrm{scale}} \\tanh(o_0), \\quad \\beta(x) = b_{\\mathrm{scale}} \\tanh(o_1).\n   $$\n   变换后的输入为 $x' = \\mathrm{clip}(\\alpha(x)\\, x + \\beta(x), 0, 1)$。为抑制极端变换，在目标函数中加入一个与 $((\\alpha(x) - 1)^2 + \\beta(x)^2)$ 成正比的正则化项，其系数为 $\\lambda_{\\mathrm{reg}} \\geq 0$。\n3. 使用全批量梯度下降法训练一个二元逻辑回归分类器 $(w,b)$，进行固定步数 $T$ 的训练。在学习型增强模式下，通过链式法则对光度变换进行微分，联合更新 MLP 的参数。将 $\\mathrm{clip}$ 函数在开区间 $(0,1)$ 之外的导数视为零，在区间内的导数视为标准导数。\n4. 训练后，在验证集上评估分类器（不使用增强）以衡量其泛化能力。计算：\n   - 训练准确率，即在 $\\hat{p}$ 上使用 $0.5$ 的决策阈值时，训练样本中被正确分类的比例。\n   - 验证准确率，类似地在不使用增强的验证集上计算。\n   - 泛化差距，定义为训练准确率减去验证准确率（一个浮点数）。\n   - 坍塌指标，定义如下：对于学习型增强模式，使用最终的 MLP 参数为每个训练样本计算 $x'_i$。设 $s_i$ 是 $x'_i$ 中像素值的标准差。计算所有训练样本的平均标准差 $\\bar{s}$。如果 $\\bar{s}  s_{\\mathrm{thr}}$，则判断为发生坍塌，其中 $s_{\\mathrm{thr}}$ 是一个固定的阈值。对于非学习型模式，将坍塌指标设为 $0$。坍塌指标必须是一个整数：如果坍塌则为 $1$，否则为 $0$。所有量均为无单位量，不适用物理单位。\n\n您的实现必须使用以下包含四种参数设置的测试套件，以探索正常路径、边界和边缘情况：\n- 情况 $1$（基线，无增强）：\n  - 模式：none。\n  - $a_{\\mathrm{scale}} = 0$, $b_{\\mathrm{scale}} = 0$, $\\lambda_{\\mathrm{reg}} = 0$, $T = 100$, 隐藏维度 $h = 8$, $s_{\\mathrm{thr}} = 0.05$。\n- 情况 $2$（随机增强，中等范围）：\n  - 模式：random。\n  - $a_{\\mathrm{max}} = 0.2$, $b_{\\mathrm{max}} = 0.1$, $T = 100$, $s_{\\mathrm{thr}} = 0.05$。\n- 情况 $3$（学习型增强，强正则化）：\n  - 模式：learned。\n  - $a_{\\mathrm{scale}} = 0.3$, $b_{\\mathrm{scale}} = 0.15$, $\\lambda_{\\mathrm{reg}} = 1.0$, $T = 100$, 隐藏维度 $h = 8$, $s_{\\mathrm{thr}} = 0.05$。\n- 情况 $4$（学习型增强，弱正则化和较大范围 — 坍塌的边缘情况）：\n  - 模式：learned。\n  - $a_{\\mathrm{scale}} = 1.0$, $b_{\\mathrm{scale}} = 0.5$, $\\lambda_{\\mathrm{reg}} = 0.0$, $T = 100$, 隐藏维度 $h = 8$, $s_{\\mathrm{thr}} = 0.05$。\n\n对于每种情况，您的程序必须按以下顺序输出一个包含四个值的列表：验证准确率（浮点数）、训练准确率（浮点数）、泛化差距（浮点数）、坍塌指标（整数）。您的程序应生成单行输出，其中包含用方括号括起来的、以逗号分隔的结果列表（例如：\"[[val_acc1,train_acc1,gen_gap1,collapse1],[val_acc2,train_acc2,gen_gap2,collapse2],...]\"）。不应打印任何其他文本。", "solution": "该问题是有效的，因为它在科学上基于已建立的机器学习原理，定义明确且包含所有必要参数，并且其表述是客观的。任务是在一个合成数据集上，为一个二元逻辑回归分类器实现并评估三种不同的数据增强策略。该解决方案被设计成一个自包含的数值实验，结构上分为四个主要部分：数据集生成、模型实现、通过梯度下降进行训练以及评估。\n\n### 1. 合成数据集生成\n构建一个合成数据集来表示一个简单的二元分类问题。图像大小为 $H \\times W = 16 \\times 16$，展平为向量 $x \\in \\mathbb{R}^{256}$。像素强度在 $[0, 1]$ 范围内。我们生成 $N_{\\mathrm{train}} = 120$ 个训练样本和 $N_{\\mathrm{val}} = 120$ 个验证样本，类别均衡。\n- **类别 $0$ ($y=0$)**：图像包含一个中心靠近图像左侧的二维高斯斑点，大约在 $(c_x, c_y) = (W/4, H/2) = (4, 8)$。\n- **类别 $1$ ($y=1$)**：图像包含一个中心靠近图像右侧的高斯斑点，大约在 $(c_x, c_y) = (3W/4, H/2) = (12, 8)$。\n每个斑点的中心都通过小的随机位移进行扰动，以引入可变性。对于中心在 $(c_x, c_y)$ 的斑点，坐标为 $(i, j)$ 的像素强度由 $I(i,j) = \\exp\\left(-\\frac{(i-c_y)^2 + (j-c_x)^2}{2\\sigma^2}\\right)$ 给出，其中标准差 $\\sigma$ 固定。然后应用加性高斯噪声，并将最终的像素值裁剪到 $[0, 1]$ 范围内。\n\n### 2. 模型架构\n使用了两个主要模型：一个逻辑回归分类器和一个可学习的增强网络。\n\n- **逻辑回归分类器**：该分类器有参数 $(w, b)$，其中 $w \\in \\mathbb{R}^{D}$ 且 $b \\in \\mathbb{R}$。对于输入向量 $x'$，它计算一个 logit $z = x'^{\\top}w + b$ 并预测类别概率 $\\hat{p} = \\sigma(z)$，其中 $\\sigma(t) = 1 / (1+e^{-t})$ 是 sigmoid 函数。参数初始化为零。\n\n- **可学习的增强网络**：对于 `learned` 模式，一个带有一个隐藏层的多层感知机 (MLP) 为给定的输入图像 $x$ 生成光度变换参数 $(\\alpha(x), \\beta(x))$。\n    - 该 MLP 有一个大小为 $D=256$ 的输入层，一个大小为 $h=8$ 且使用双曲正切 ($\\tanh$) 激活函数的隐藏层，以及一个大小为 $2$ 的输出层。设 MLP 参数为 $\\phi = \\{W_1, b_1, W_2, b_2\\}$。\n    - 隐藏层：$h(x) = \\tanh(W_1^{\\top}x + b_1)$。\n    - 输出 logits：$o(x) = W_2^{\\top}h(x) + b_2$。\n    - 变换参数从输出 logits $o = (o_0, o_1)$ 导出：\n      $$\n      \\alpha(x) = 1 + a_{\\mathrm{scale}} \\tanh(o_0)\n      $$\n      $$\n      \\beta(x) = b_{\\mathrm{scale}} \\tanh(o_1)\n      $$\n    - 变换后的图像为 $x' = \\mathrm{clip}(\\alpha(x)x + \\beta(x), 0, 1)$。MLP 权重使用 Glorot (Xavier) 初始化方法进行初始化。\n\n### 3. 通过全批量梯度下降进行训练\n模型使用全批量梯度下降法训练 $T=100$ 步。目标函数取决于增强模式。对于所有模式，主要的损失分量是训练批次上的平均二元交叉熵 (BCE)：\n$$\n\\mathcal{L}_{\\mathrm{BCE}} = -\\frac{1}{N_{\\mathrm{train}}} \\sum_{i=1}^{N_{\\mathrm{train}}} \\left[ y_i \\log(\\hat{p}_i) + (1-y_i)\\log(1-\\hat{p}_i) \\right]\n$$\n其中 $\\hat{p}_i$ 是对第 $i$ 个（可能经过增强的）样本 $x'_i$ 的预测。\n\n- **模式 `none`**：$x' = x$。更新分类器参数 $(w, b)$ 以最小化 $\\mathcal{L}_{\\mathrm{BCE}}$。\n- **模式 `random`**：在每个训练步骤中，对每个样本 $x_i$ 应用一个新的变换 $x'_i = \\mathrm{clip}(\\alpha_i x_i + \\beta_i, 0, 1)$，其中 $\\alpha_i \\sim U(1-a_{\\mathrm{max}}, 1+a_{\\mathrm{max}})$ 且 $\\beta_i \\sim U(-b_{\\mathrm{max}}, b_{\\mathrm{max}})$。更新分类器参数以最小化增强批次上的 $\\mathcal{L}_{\\mathrm{BCE}}$。\n- **模式 `learned`**：目标函数包含一个正则化项，以惩罚极端变换：\n$$\n\\mathcal{L} = \\mathcal{L}_{\\mathrm{BCE}} + \\mathcal{L}_{\\mathrm{reg}} = \\mathcal{L}_{\\mathrm{BCE}} + \\frac{\\lambda_{\\mathrm{reg}}}{N_{\\mathrm{train}}} \\sum_{i=1}^{N_{\\mathrm{train}}} \\left( (\\alpha(x_i) - 1)^2 + \\beta(x_i)^2 \\right)\n$$\n分类器参数 $(w, b)$ 和 MLP 参数 $\\phi = \\{W_1, b_1, W_2, b_2\\}$ 通过沿 $\\mathcal{L}$ 的梯度下降进行联合更新。梯度使用链式法则（反向传播）计算，对整个计算图进行微分，包括裁剪函数（其在 $(0, 1)$ 上的导数视为 $1$，在其他地方视为 $0$）。\n\n总损失 $\\mathcal{L}$ 相对于分类器参数 $(w, b)$ 的梯度为：\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial w} = \\frac{1}{N_{\\mathrm{train}}} X'^{\\top}(\\hat{p} - Y) \\quad\\quad \\frac{\\partial \\mathcal{L}}{\\partial b} = \\frac{1}{N_{\\mathrm{train}}} \\sum_{i=1}^{N_{\\mathrm{train}}} (\\hat{p}_i - y_i)\n$$\n其中 $X'$ 是增强后的训练样本矩阵。对于学习型模式，相对于 MLP 参数 $\\phi$ 的梯度是通过从 $\\alpha$ 和 $\\beta$ 上的梯度继续反向传播来计算的：\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial \\alpha_i} = \\left( \\frac{\\partial \\mathcal{L}_{\\mathrm{BCE}}}{\\partial x'_i} \\cdot \\frac{\\partial x'_i}{\\partial \\alpha_i} \\right) + \\frac{\\partial \\mathcal{L}_{\\mathrm{reg}}}{\\partial \\alpha_i} \\quad\\quad \\frac{\\partial \\mathcal{L}}{\\partial \\beta_i} = \\left( \\frac{\\partial \\mathcal{L}_{\\mathrm{BCE}}}{\\partial x'_i} \\cdot \\frac{\\partial x'_i}{\\partial \\beta_i} \\right) + \\frac{\\partial \\mathcal{L}_{\\mathrm{reg}}}{\\partial \\beta_i}\n$$\n然后将这些梯度反向传播通过 MLP 以求得 $\\frac{\\partial \\mathcal{L}}{\\partial \\phi}$。\n\n### 4. 评估\n训练后，使用四个指标评估分类器的性能。所有准确率评估都在原始的、未增强的数据上进行。\n\n- **训练准确率**：在训练集 $X_{\\mathrm{train}}$ 中被正确分类的样本比例，使用 $0.5$ 的决策阈值。\n- **验证准确率**：在验证集 $X_{\\mathrm{val}}$ 中被正确分类的样本比例。\n- **泛化差距**：训练准确率与验证准确率之差。较小的差距表明更好的泛化能力。\n- **坍塌指标**：对于 `learned` 模式，该指标量化了一种失败模式，即增强学会了“擦除”输入图像的特征，通常是为了最小化训练损失。对于每个训练样本 $x_i$，应用最终学到的变换得到 $x'_i$。计算像素的标准差 $s_i = \\mathrm{std}(x'_i)$。如果这些标准差的平均值 $\\bar{s} = \\frac{1}{N_{\\mathrm{train}}} \\sum_i s_i$ 低于阈值 $s_{\\mathrm{thr}}=0.05$，则坍塌指标设为 $1$；否则为 $0$。对于 `none` 和 `random` 模式，坍塌指标始终为 $0$。\n\n实现将对四个指定的测试用例中的每一个执行这些步骤，并以所需格式报告结果指标。固定的随机种子确保了可复现性。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the specified experiments and print results.\n    \"\"\"\n    # Define constants\n    H, W = 16, 16\n    D = H * W\n    N_TRAIN, N_VAL = 120, 120\n    RANDOM_SEED = 42\n\n    def generate_dataset(n_samples, h_dim, w_dim, seed_offset=0):\n        \"\"\"Generates a synthetic dataset of Gaussian blobs.\"\"\"\n        np.random.seed(RANDOM_SEED + seed_offset)\n        d_dim = h_dim * w_dim\n        x_data = np.zeros((n_samples, d_dim))\n        y_data = np.zeros((n_samples, 1))\n        \n        xx, yy = np.meshgrid(np.arange(w_dim), np.arange(h_dim))\n        \n        for i in range(n_samples):\n            y_data[i] = i % 2\n            if y_data[i] == 0: # Class 0: blob on the left\n                center_x = w_dim / 4.0\n                center_y = h_dim / 2.0\n            else: # Class 1: blob on the right\n                center_x = 3 * w_dim / 4.0\n                center_y = h_dim / 2.0\n            \n            shift_x = np.random.uniform(-w_dim / 8.0, w_dim / 8.0)\n            shift_y = np.random.uniform(-h_dim / 8.0, h_dim / 8.0)\n            \n            cx, cy = center_x + shift_x, center_y + shift_y\n            \n            sigma = 2.0\n            img = np.exp(-((xx - cx)**2 + (yy - cy)**2) / (2 * sigma**2))\n            noise = np.random.normal(0, 0.1, img.shape)\n            img += noise\n            \n            x_data[i, :] = np.clip(img, 0, 1).flatten()\n            \n        return x_data, y_data\n\n    def sigmoid(z):\n        \"\"\"Computes the sigmoid function.\"\"\"\n        return 1.0 / (1.0 + np.exp(-z))\n\n    def evaluate(X, Y, w, b):\n        \"\"\"Computes classification accuracy.\"\"\"\n        z = X @ w + b\n        p_hat = sigmoid(z)\n        preds = (p_hat > 0.5).astype(int)\n        accuracy = np.mean(preds == Y)\n        return accuracy\n\n    def run_experiment(params):\n        \"\"\"Runs a single experiment based on the given parameters.\"\"\"\n        np.random.seed(RANDOM_SEED)\n\n        # Generate data\n        X_train, Y_train = generate_dataset(N_TRAIN, H, W, seed_offset=0)\n        X_val, Y_val = generate_dataset(N_VAL, H, W, seed_offset=1)\n\n        # Unpack params\n        mode = params['mode']\n        T = params.get('T', 100)\n        h_dim = params.get('h', 8)\n        s_thr = params.get('s_thr', 0.05)\n        \n        # Hyperparameters\n        lr_clf = 0.5\n        lr_aug = 0.01\n\n        # Initialize models\n        w = np.zeros((D, 1))\n        b = 0.0\n        phi = {}\n        if mode == 'learned':\n            # Glorot initialization\n            phi['W1'] = np.random.randn(D, h_dim) * np.sqrt(1. / D)\n            phi['b1'] = np.zeros(h_dim)\n            phi['W2'] = np.random.randn(h_dim, 2) * np.sqrt(1. / h_dim)\n            phi['b2'] = np.zeros(2)\n\n        # Training loop\n        for _ in range(T):\n            X_train_aug = X_train\n            alpha, beta, pre_clip, h_act = None, None, None, None\n\n            if mode == 'random':\n                a_max = params['a_max']\n                b_max = params['b_max']\n                alpha_rand = np.random.uniform(1 - a_max, 1 + a_max, (N_TRAIN, 1))\n                beta_rand = np.random.uniform(-b_max, b_max, (N_TRAIN, 1))\n                X_train_aug = np.clip(alpha_rand * X_train + beta_rand, 0, 1)\n            \n            elif mode == 'learned':\n                a_scale = params['a_scale']\n                b_scale = params['b_scale']\n                lambda_reg = params['lambda_reg']\n\n                # Augmenter forward pass\n                u = X_train @ phi['W1'] + phi['b1']\n                h_act = np.tanh(u)\n                o = h_act @ phi['W2'] + phi['b2']\n                o0, o1 = o[:, 0:1], o[:, 1:2]\n                tanh_o0, tanh_o1 = np.tanh(o0), np.tanh(o1)\n                \n                alpha = 1 + a_scale * tanh_o0\n                beta = b_scale * tanh_o1\n                \n                pre_clip = alpha * X_train + beta\n                X_train_aug = np.clip(pre_clip, 0, 1)\n\n            # Classifier forward pass\n            z = X_train_aug @ w + b\n            p_hat = sigmoid(z)\n            eps = 1e-9\n\n            # Loss\n            bce_loss = -np.mean(Y_train * np.log(p_hat + eps) + (1 - Y_train) * np.log(1 - p_hat + eps))\n            \n            # Gradients for classifier\n            grad_z = (p_hat - Y_train) / N_TRAIN\n            grad_w = X_train_aug.T @ grad_z\n            grad_b = np.sum(grad_z)\n            \n            # Update classifier\n            w -= lr_clf * grad_w\n            b -= lr_clf * grad_b\n\n            if mode == 'learned':\n                # Gradients for augmenter\n                grad_X_aug = grad_z @ w.T\n                dclip_mask = (pre_clip > 0)  (pre_clip  1)\n                grad_pre_clip = grad_X_aug * dclip_mask\n\n                grad_alpha_bce = np.sum(grad_pre_clip * X_train, axis=1, keepdims=True)\n                grad_beta_bce = np.sum(grad_pre_clip, axis=1, keepdims=True)\n\n                grad_alpha_reg = lambda_reg * (2 * (alpha - 1)) / N_TRAIN\n                grad_beta_reg = lambda_reg * (2 * beta) / N_TRAIN\n\n                grad_alpha = grad_alpha_bce + grad_alpha_reg\n                grad_beta = grad_beta_bce + grad_beta_reg\n\n                # Backprop through MLP\n                grad_o0 = grad_alpha * a_scale * (1 - tanh_o0**2)\n                grad_o1 = grad_beta * b_scale * (1 - tanh_o1**2)\n                grad_o = np.hstack([grad_o0, grad_o1])\n\n                grad_W2 = h_act.T @ grad_o\n                grad_b2 = np.sum(grad_o, axis=0)\n                grad_h_act = grad_o @ phi['W2'].T\n\n                grad_u = grad_h_act * (1 - h_act**2)\n\n                grad_W1 = X_train.T @ grad_u\n                grad_b1 = np.sum(grad_u, axis=0)\n                \n                # Update augmenter\n                phi['W1'] -= lr_aug * grad_W1\n                phi['b1'] -= lr_aug * grad_b1\n                phi['W2'] -= lr_aug * grad_W2\n                phi['b2'] -= lr_aug * grad_b2\n                \n        # Evaluation\n        train_acc = evaluate(X_train, Y_train, w, b)\n        val_acc = evaluate(X_val, Y_val, w, b)\n        gen_gap = train_acc - val_acc\n        \n        collapse_indicator = 0\n        if mode == 'learned':\n            u = X_train @ phi['W1'] + phi['b1']\n            h_act = np.tanh(u)\n            o = h_act @ phi['W2'] + phi['b2']\n            alpha = 1 + params['a_scale'] * np.tanh(o[:, 0:1])\n            beta = params['b_scale'] * np.tanh(o[:, 1:2])\n            X_train_final_aug = np.clip(alpha * X_train + beta, 0, 1)\n            \n            s = np.std(X_train_final_aug, axis=1)\n            s_mean = np.mean(s)\n            \n            if s_mean  s_thr:\n                collapse_indicator = 1\n\n        return [val_acc, train_acc, gen_gap, collapse_indicator]\n\n    # Test cases from the problem statement\n    test_cases = [\n        {'mode': 'none', 'T': 100, 'h': 8, 's_thr': 0.05},\n        {'mode': 'random', 'a_max': 0.2, 'b_max': 0.1, 'T': 100, 's_thr': 0.05},\n        {'mode': 'learned', 'a_scale': 0.3, 'b_scale': 0.15, 'lambda_reg': 1.0, 'T': 100, 'h': 8, 's_thr': 0.05},\n        {'mode': 'learned', 'a_scale': 1.0, 'b_scale': 0.5, 'lambda_reg': 0.0, 'T': 100, 'h': 8, 's_thr': 0.05},\n    ]\n\n    results = []\n    for params in test_cases:\n        result = run_experiment(params)\n        results.append(result)\n\n    # Format output\n    output_str = ','.join([f\"[{','.join(map(str, r))}]\" for r in results])\n    print(f\"[{output_str}]\")\n\nsolve()\n```", "id": "3129310"}]}