{"hands_on_practices": [{"introduction": "理论是重要的，但直观的理解往往能带来更深刻的领悟。这个实践练习将引导你通过编码，在一个精心设计的合成损失函数上直观地比较网格搜索和随机搜索。你将亲手构建一个带有狭窄“山脊”的超参数空间，并观察轴对齐的网格搜索是如何轻易地错失最优解区域的，而随机搜索则凭借其灵活性成功定位目标。[@problem_id:3133087]", "problem": "将深度学习中的超参数优化视为在超参数空间上最小化经验风险函数的任务。设超参数空间为矩形 $\\mathcal{H} = [\\eta_{\\min}, \\eta_{\\max}] \\times [\\lambda_{\\min}, \\lambda_{\\max}]$。由超参数引起的经验风险可以抽象为一个旨在呈现尖锐山脊的合成景观 $f(\\eta, \\lambda)$。假设有以下一类景观，其具有一个由角度 $\\theta$（以弧度为单位）定向的单一狭长山谷（低损失山脊）：\n$$\nf(\\eta, \\lambda) = \\frac{\\left(\\cos\\theta \\cdot \\eta + \\sin\\theta \\cdot \\lambda - c\\right)^2}{w^2} + \\beta \\left(\\eta^2 + \\lambda^2\\right),\n$$\n其中 $c$ 是定位山脊的标量偏移量，$w > 0$ 是控制尖锐度的山脊半宽，而 $\\beta > 0$ 是碗状曲率系数。目标是最小化 $f(\\eta, \\lambda)$。\n\n基本原理：\n- 超参数搜索问题是在固定的评估预算下找到 $\\arg\\min_{(\\eta,\\lambda) \\in \\mathcal{H}} f(\\eta,\\lambda)$。这基于经验风险最小化原则以及网格搜索和随机搜索的定义。\n- 轴对齐网格搜索在等距点的轴对齐格上评估 $f$。具体来说，为 $\\eta$ 选择 $n_\\eta$ 个点，为 $\\lambda$ 选择 $n_\\lambda$ 个点，并评估所有 $n_\\eta \\cdot n_\\lambda$ 种组合。\n- 随机搜索在从 $\\mathcal{H}$ 中独立同分布 (i.i.d.) 均匀采样的 $N$ 个点上评估 $f$。使用带种子的随机数生成器 (RNG) 以确保可复现的采样。\n\n您必须在合成景观上实现轴对齐网格搜索和随机搜索，并展示当山脊与坐标轴未对齐时，轴对齐网格搜索可能失败而随机搜索成功的案例。成功的概念定义为找到一个小于或等于指定阈值 $\\tau$ 的函数值，该阈值代表测试用例的“近乎最优”损失。\n\n角度单位规范：所有角度（变量 $\\theta$）必须以弧度表示。\n\n实现以下包含三个案例的测试套件。对于每个案例，程序必须：\n- 使用给定的参数构造 $f(\\eta,\\lambda)$。\n- 使用指定的预算 $n_\\eta$ 和 $n_\\lambda$ 运行轴对齐网格搜索。\n- 使用指定的预算 $N$ 和随机种子运行随机搜索。\n- 计算一个布尔值，指示在该案例中随机搜索是否成功而网格搜索失败。\n\n所有测试的超参数域：$[\\eta_{\\min}, \\eta_{\\max}] = [-1, 1]$ 和 $[\\lambda_{\\min}, \\lambda_{\\max}] = [-1, 1]$。\n\n测试案例：\n- 案例 A（轴对齐山脊；“理想路径”）：\n  - $\\theta = 0$, $c = 0$, $w = 0.25$, $\\beta = 0.05$, $n_\\eta = 7$, $n_\\lambda = 7$, $N = 49$, 种子 = 123, 阈值 $\\tau = 0.5$。\n  - 解读：山脊与 $\\eta$ 轴对齐且足够宽。两种方法都应该成功。\n- 案例 B（对角线山脊；未对齐；网格搜索失败，随机搜索成功）：\n  - $\\theta = \\pi/4$, $c = 0.1$, $w = 0.03$, $\\beta = 0.05$, $n_\\eta = 7$, $n_\\lambda = 7$, $N = 200$, 种子 = 456, 阈值 $\\tau = 1.0$。\n  - 解读：山脊狭窄且未对齐。轴对齐的网格样本无法足够接近；具有更大预算的随机搜索应该会成功。\n- 案例 C（对角线山脊；极度尖锐；两者都失败）：\n  - $\\theta = \\pi/4$, $c = 0.1$, $w = 0.004$, $\\beta = 0.05$, $n_\\eta = 7$, $n_\\lambda = 7$, $N = 49$, 种子 = 789, 阈值 $\\tau = 0.4$。\n  - 解读：山脊极度尖锐，使得在给定预算下，两种方法都不太可能达到阈值。\n\n一个方法在一个案例中的成功标准：其在评估点上观测到的最小值 $\\min f$ 小于或等于该案例的 $\\tau$。\n\n输出规范：\n- 对于每个案例，生成一个布尔值，当且仅当随机搜索成功且网格搜索失败时，该值为真。\n- 您的程序应生成单行输出，其中包含用方括号括起来的逗号分隔列表形式的结果，例如：`[True,False,True]`。", "solution": "该问题经评估有效。它在科学上基于数值优化和机器学习的原理，特别是超参数调整。该问题是适定 (well-posed) 的，提供了一个明确定义的数学函数、对两种搜索算法（网格搜索和随机搜索）的清晰描述，以及针对三个不同测试案例的完整参数集。目标精确，成功标准有正式定义。没有矛盾、歧义或事实不准确之处。\n\n这个问题的核心是比较轴对齐网格搜索和均匀随机搜索在有界超参数域 $\\mathcal{H} = [-1, 1] \\times [-1, 1]$ 上最小化合成损失函数 $f(\\eta, \\lambda)$ 的功效。该函数由以下公式给出：\n$$\nf(\\eta, \\lambda) = \\frac{\\left(\\cos\\theta \\cdot \\eta + \\sin\\theta \\cdot \\lambda - c\\right)^2}{w^2} + \\beta \\left(\\eta^2 + \\lambda^2\\right)\n$$\n该函数旨在模拟超参数优化中遇到的具有挑战性的景观。它包含两个主要部分：\n$1$. 项 $\\frac{\\left(\\cos\\theta \\cdot \\eta + \\sin\\theta \\cdot \\lambda - c\\right)^2}{w^2}$ 定义了一个抛物线形的山谷。该项的最小值是 $0$，出现在由 $\\cos\\theta \\cdot \\eta + \\sin\\theta \\cdot \\lambda - c = 0$ 定义的直线上。这条直线代表了一个低损失值的“山脊”。参数 $\\theta$ 控制这个山脊在 $(\\eta, \\lambda)$ 平面上的方向，$c$ 决定其与原点的偏移量，而 $w > 0$ 控制其宽度。较小的 $w$ 对应于更尖锐、更狭窄的山脊，使其成为任何搜索算法的更小目标。\n$2$. 项 $\\beta \\left(\\eta^2 + \\lambda^2\\right)$ 为景观添加了一个凸的、碗状的分量，其中 $\\beta > 0$ 是一个曲率系数。该项起到一种正则化的作用，确保存在唯一的全局最小值，且位于原点 $(0,0)$ 附近。$f$ 的最终最小值是在位于山脊上和靠近原点之间的一种平衡。\n\n两种搜索算法的实现如下：\n\n**轴对齐网格搜索：** 该方法在构成一个规则的、轴对齐网格的点上评估函数 $f(\\eta, \\lambda)$。对于 $\\eta$ 超参数的 $n_\\eta$ 个点和 $\\lambda$ 超参数的 $n_\\lambda$ 个点的预算，我们生成两组等距点：\n$$\n\\eta_i = -1 + i \\frac{2}{n_\\eta - 1}, \\quad i = 0, 1, \\dots, n_\\eta - 1\n$$\n$$\n\\lambda_j = -1 + j \\frac{2}{n_\\lambda - 1}, \\quad j = 0, 1, \\dots, n_\\lambda - 1\n$$\n然后，在所有 $n_\\eta \\times n_\\lambda$ 个点对 $(\\eta_i, \\lambda_j)$ 上评估该函数。该方法的主要局限性在于其采样模式。如果一个狭窄的低损失山脊与网格轴不对齐（即 $\\theta$ 不是 $\\pi/2$ 的倍数），所有网格点都有可能落在山脊的高损失斜坡上，从而有效地“错过”最优区域。\n\n**随机搜索：** 该方法在从搜索空间 $\\mathcal{H}$ 中独立均匀采样的 $N$ 个点上评估 $f(\\eta, \\lambda)$。每个点 $(\\eta_k, \\lambda_k)$（对于 $k=1, \\dots, N$）都是从 $[-1, 1] \\times [-1, 1]$ 上的均匀分布中抽取的。正如 Bergstra 和 Bengio (2012) 所强调的，随机搜索的主要优势在于其对损失景观几何形状的鲁棒性。通过随机采样，它不受固定的轴对齐约束。对于搜索空间中任何面积不为零的子区域（例如一个狭窄的山脊），在其中放置一个样本的概率随着样本数量 $N$ 的增加而增加。这使得当问题的“有效维度”较低时（即当函数值主要由一小部分或参数组合决定时，如同一个由山脊主导的景观），在统计上更有可能找到一个好的解决方案。\n\n测试案例的评估过程如下：\n\n**案例 A：** $\\theta = 0$, $w = 0.25$。山脊与 $\\lambda$ 轴对齐（因为 $\\cos(0)=1, \\sin(0)=0$，山脊位于 $\\eta-c=0$，即 $\\eta=0$）。网格非常适合沿该轴采样。山脊也相对较宽（$w=0.25$）。预计网格搜索和随机搜索都能找到低 $f$ 值的点，从而成功。条件 `random_succeeds and not grid_succeeds` 将为 `False`。\n\n**案例 B：** $\\theta = \\pi/4$, $w = 0.03$。山脊是倾斜且狭窄的。对于 $n_\\eta = 7, n_\\lambda = 7$，网格点沿每个轴的间距为 $2/(7-1) \\approx 0.333$。鉴于山脊的狭窄性（$w=0.03$），所有 $49$ 个网格点极有可能都远离损失最小的线，导致函数值较高。因此，预计网格搜索将失败。随机搜索有更大的预算，为 $N=200$ 个样本，它有显著更高的概率将至少一个样本放置在狭窄的对角线山谷内，从而达到低于阈值 $\\tau=1.0$ 的函数值。该条件预计为 `True`。\n\n**案例 C：** $\\theta = \\pi/4$, $w = 0.004$。山脊是倾斜且极度尖锐的。低损失区域的有效面积非常小。在 $N=49$ 个样本的有限预算下，一个随机点落入这个微小区域的概率非常低。同样，网格搜索将因与案例 B 相同的原因而失败。因此，预计两种方法都将无法找到一个值低于阈值 $\\tau=0.4$ 的点。该条件将为 `False`。\n\n实现将计算每种方法在每个案例中找到的最小函数值，并将其与指定的阈值 $\\tau$ 进行比较，以确定成功或失败，最终计算所需的布尔输出。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and compares grid search and random search on synthetic \n    hyperparameter optimization landscapes.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"name\": \"Case A\",\n            \"theta\": 0.0,\n            \"c\": 0.0,\n            \"w\": 0.25,\n            \"beta\": 0.05,\n            \"n_eta\": 7,\n            \"n_lambda\": 7,\n            \"N\": 49,\n            \"seed\": 123,\n            \"tau\": 0.5\n        },\n        {\n            \"name\": \"Case B\",\n            \"theta\": np.pi / 4.0,\n            \"c\": 0.1,\n            \"w\": 0.03,\n            \"beta\": 0.05,\n            \"n_eta\": 7,\n            \"n_lambda\": 7,\n            \"N\": 200,\n            \"seed\": 456,\n            \"tau\": 1.0\n        },\n        {\n            \"name\": \"Case C\",\n            \"theta\": np.pi / 4.0,\n            \"c\": 0.1,\n            \"w\": 0.004,\n            \"beta\": 0.05,\n            \"n_eta\": 7,\n            \"n_lambda\": 7,\n            \"N\": 49,\n            \"seed\": 789,\n            \"tau\": 0.4\n        }\n    ]\n\n    def landscape_function(eta, lamb, theta, c, w, beta):\n        \"\"\"\n        Calculates the value of the synthetic loss function f(eta, lambda).\n        This function is vectorized to work with numpy arrays.\n        \"\"\"\n        term1_numerator = np.cos(theta) * eta + np.sin(theta) * lamb - c\n        term1 = (term1_numerator**2) / (w**2)\n        term2 = beta * (eta**2 + lamb**2)\n        return term1 + term2\n\n    def run_grid_search(params):\n        \"\"\"\n        Performs an axis-aligned grid search and checks for success.\n        \"\"\"\n        eta_points = np.linspace(-1.0, 1.0, params['n_eta'])\n        lambda_points = np.linspace(-1.0, 1.0, params['n_lambda'])\n        eta_grid, lambda_grid = np.meshgrid(eta_points, lambda_points)\n\n        f_values = landscape_function(\n            eta_grid, lambda_grid,\n            params['theta'], params['c'], params['w'], params['beta']\n        )\n\n        min_f = np.min(f_values)\n        return min_f = params['tau']\n\n    def run_random_search(params):\n        \"\"\"\n        Performs a random search and checks for success.\n        Uses a fixed seed for reproducibility.\n        \"\"\"\n        np.random.seed(params['seed'])\n        eta_samples = np.random.uniform(-1.0, 1.0, size=params['N'])\n        lambda_samples = np.random.uniform(-1.0, 1.0, size=params['N'])\n\n        f_values = landscape_function(\n            eta_samples, lambda_samples,\n            params['theta'], params['c'], params['w'], params['beta']\n        )\n\n        min_f = np.min(f_values)\n        return min_f = params['tau']\n\n    results = []\n    for case in test_cases:\n        # Run both search methods for the current case\n        grid_succeeds = run_grid_search(case)\n        random_succeeds = run_random_search(case)\n        \n        # Determine if random search succeeds AND grid search fails\n        case_result = random_succeeds and not grid_succeeds\n        results.append(case_result)\n\n    # Final print statement in the exact required format.\n    # The map to str is necessary as the list contains booleans.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3133087"}, {"introduction": "超参数之间并非总是相互独立的，一个参数的设置可能会极大地影响另一个参数的最优值。本练习旨在模拟一种被称为“阻塞效应”的常见调参困境，即固定一个超参数在次优值上会导致对其他参数的搜索无法找到全局最优解。通过在一个对数尺度上定义的合成损失函数上进行实验，你将量化随机搜索在克服这种阻塞效应方面的优势。[@problem_id:3133107]", "problem": "要求您编写一个完整的、确定性的程序，该程序模拟并量化在网格搜索中将一个超参数固定在次优值时的阻塞效应，并将其与随机搜索逃离此类阻塞的能力进行比较。背景是一个与深度学习超参数调优相关的合成验证损失曲面，它有两个标量超参数：学习率，表示为 $\\,\\eta\\,$，和权重衰减，表示为 $\\,\\lambda\\,$。目标是最小化损失函数 $\\,f(\\eta,\\lambda)\\,$。\n\n基本基础和定义：\n- 一种搜索策略会评估一组超参数对，并返回观测到的 $\\,f\\,$ 的最佳值。\n- 带阻塞的网格搜索：将一个超参数固定在选定值 $\\,\\lambda_{\\mathrm{fix}}\\,$，而另一个超参数 $\\,\\eta\\,$ 在 $\\,N\\,$ 个网格点上进行评估。\n- 随机搜索：两个超参数从指定的分布中联合采样 $\\,N\\,$ 个独立点。\n\n合成目标函数：\n- 定义 $\\,x = \\log_{10}(\\eta)\\,$ 和 $\\,y = \\log_{10}(\\lambda)\\,$。\n- 设搜索域为 $\\,x \\in [-4,0]\\,$ 和 $\\,y \\in [-5,-1]\\,$，这对应于 $\\,\\eta \\in [10^{-4},10^{0}]\\,$ 和 $\\,\\lambda \\in [10^{-5},10^{-1}]\\,$.\n- 定义\n$$\nf(\\eta,\\lambda) \\;=\\; (x+2)^2 \\;+\\; 2.5\\,(y+3)^2 \\;+\\; 0.3\\,(x+2)(y+3) \\;+\\; 0.1\\,\\sin(5x)\\,\\sin(5y),\n$$\n其中 $\\,x=\\log_{10}(\\eta)\\,$ 和 $\\,y=\\log_{10}(\\lambda)\\,$。该函数编码了一个在 $\\,x=-2\\,$ 和 $\\,y=-3\\,$ 附近（即 $\\,\\eta=10^{-2}\\,$ 和 $\\,\\lambda=10^{-3}\\,$ 附近）的盆地，具有温和的相互作用和波纹。\n\n需要实现的算法：\n- 带阻塞的网格搜索：\n  - 给定预算 $\\,N\\,$ 和一个固定的 $\\,\\lambda_{\\mathrm{fix}}\\,$，通过在闭区间 $\\,[-4,0]\\,$ 上为 $\\,x=\\log_{10}(\\eta)\\,$ 选择 $\\,N\\,$ 个等间距点（即包括两个端点），然后将每个 $\\,x\\,$ 映射到 $\\,\\eta=10^x\\,$，来构建 $\\,N\\,$ 个 $\\,\\eta\\,$ 的值。\n  - 在这些 $\\,N\\,$ 个点上评估 $\\,f(\\eta,\\lambda_{\\mathrm{fix}})\\,$ 并报告最小值。当 $\\,N=1\\,$ 时，唯一的网格点是左端点 $\\,x=-4\\,$。\n- 随机搜索：\n  - 给定预算 $\\,N\\,$ 和一个随机种子 $\\,s\\,$，从 $\\,x \\sim \\mathrm{Uniform}([-4,0])\\,$ 和 $\\,y \\sim \\mathrm{Uniform}([-5,-1])\\,$ 中抽取 $\\,N\\,$ 个独立样本。将它们映射到 $\\,\\eta=10^x\\,$ 和 $\\,\\lambda=10^y\\,$。\n  - 在这些 $\\,N\\,$ 个点上评估 $\\,f(\\eta,\\lambda)\\,$ 并报告最小值。\n\n每个测试用例需报告的量：\n- 定义随机搜索相对于带阻塞的网格搜索的优势为\n$$\n\\Delta \\;=\\; f_{\\min}^{\\mathrm{grid}} \\;-\\; f_{\\min}^{\\mathrm{rand}},\n$$\n其中 $\\,f_{\\min}^{\\mathrm{grid}}\\,$ 是带阻塞的网格搜索找到的最佳值，$\\,f_{\\min}^{\\mathrm{rand}}\\,$ 是在相同预算 $\\,N\\,$ 下随机搜索找到的最佳值。一个正的 $\\,\\Delta\\,$ 值表示随机搜索获得了更低（更好）的损失。\n\n测试套件：\n- 使用以下测试用例，每个用例指定为一个三元组 $\\,(\\,N,\\,\\lambda_{\\mathrm{fix}},\\,s\\,)\\,$：\n  - 用例 $\\,1$：$\\,(\\,30,\\,10^{-1},\\,2027\\,)\\,$。\n  - 用例 $\\,2$：$\\,(\\,1,\\,10^{-1},\\,123\\,)\\,$。\n  - 用例 $\\,3$：$\\,(\\,30,\\,10^{-3},\\,2027\\,)\\,$。\n  - 用例 $\\,4$：$\\,(\\,10,\\,10^{-2},\\,42\\,)\\,$。\n对于每个用例，使用前文明确说明的算法和域来计算 $\\,\\Delta\\,$。必须通过为该用例的生成器设置种子 $\\,s\\,$ 来使随机性变得确定。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含四个测试用例的结果 $\\,\\Delta\\,$，按顺序排列，格式为用方括号括起来的逗号分隔列表。每个数字都应格式化为定点小数，小数点后恰好有 $\\,6\\,$ 位数字，例如 `[0.123456,-0.010000,0.000001,0.250000]`。", "solution": "该问题是有效的，因为它在科学上基于超参数优化的原理，在数学上是适定的，并为所有必需的组件提供了完整而明确的规范。通过实现指定的算法和目标函数，可以得出一个确定性的解。\n\n问题的核心是在一个合成的验证损失曲面上，比较两种超参数搜索策略——带阻塞的网格搜索和随机搜索——的性能。性能通过优势 $\\Delta$ 来量化，它衡量随机搜索找到的损失比带阻塞的网格搜索找到的损失要好（低）多少。\n\n分析在超参数的对数空间中进行，这是机器学习中的标准做法，适用于像学习率和权重衰减这类通常对性能有乘法效应的参数。超参数是学习率 $\\eta$ 和权重衰减 $\\lambda$。它们的对数变换是 $x = \\log_{10}(\\eta)$ 和 $y = \\log_{10}(\\lambda)$。\n\n搜索域在这个对数空间中被定义为一个矩形：$x \\in [-4, 0]$ 和 $y \\in [-5, -1]$。这对应于 $\\eta \\in [10^{-4}, 10^0]$ 和 $\\lambda \\in [10^{-5}, 10^{-1}]$。\n\n需要最小化的合成损失函数由下式给出：\n$$f(x, y) = (x+2)^2 + 2.5(y+3)^2 + 0.3(x+2)(y+3) + 0.1\\sin(5x)\\sin(5y)$$\n这个函数是一个以 $(x,y)=(-2,-3)$ 附近为中心的二次碗形，代表了一个良好超参数的区域，并带有一个交互项和一个小的、高频的正弦分量，以模仿真实损失景观的复杂、非凸特性。\n\n求解过程首先实现这个函数，然后按规定实现两种搜索算法。\n\n**1. 带阻塞的网格搜索算法**\n\n该算法模拟了一种常见但可能存在缺陷的调优策略，即固定一个超参数，同时改变另一个。\n给定 $N$ 次评估的预算和一个固定的超参数值 $\\lambda_{\\mathrm{fix}}$，其在对数空间中的对应值为 $y_{\\mathrm{fix}} = \\log_{10}(\\lambda_{\\mathrm{fix}})$。然后搜索被限制在搜索域内的直线 $y = y_{\\mathrm{fix}}$ 上。\n\n该算法通过在另一个超参数 $x$ 的整个范围 $[-4, 0]$ 上创建一个均匀网格，来生成 $N$ 个评估点。\n- 如果 $N1$，网格点由 `linspace` 生成，创建 $N$ 个从 $-4$ 到 $0$（含两端）的等间距点。这些点是 $x_i = -4 + i \\cdot \\frac{4}{N-1}$，其中 $i \\in \\{0, 1, \\dots, N-1\\}$。\n- 如果 $N=1$，问题规定评估的单点是区间的左端点 $x_0 = -4$。\n\n损失函数 $f(x, y_{\\mathrm{fix}})$ 在这 $N$ 个网格点 $(x_i, y_{\\mathrm{fix}})$ 中的每一个点上进行评估。在这些评估中找到的最小值就是搜索的结果 $f_{\\min}^{\\mathrm{grid}}$。\n$$f_{\\min}^{\\mathrm{grid}} = \\min_{i \\in \\{0, \\dots, N-1\\}} f(x_i, y_{\\mathrm{fix}})$$\n\n**2. 随机搜索算法**\n\n该算法通过同时采样两个超参数来更广泛地探索超参数空间。\n给定 $N$ 次评估的预算和一个用于可复现性的随机种子 $s$，该算法按以下步骤进行：\n首先，使用种子 $s$ 初始化一个伪随机数生成器。然后，从搜索域中采样 $N$ 个独立点 $(x_j, y_j)$，其中 $j \\in \\{1, \\dots, N\\}$。采样分布在其各自的区间上是均匀的：\n$$x_j \\sim \\mathrm{Uniform}([-4, 0])$$\n$$y_j \\sim \\mathrm{Uniform}([-5, -1])$$\n损失函数 $f(x_j, y_j)$ 在这 $N$ 个随机采样的点中的每一个点上进行评估。找到的最小值就是搜索的结果 $f_{\\min}^{\\mathrm{rand}}$。\n$$f_{\\min}^{\\mathrm{rand}} = \\min_{j \\in \\{1, \\dots, N\\}} f(x_j, y_j)$$\n\n**3. 优势计算与测试用例执行**\n\n对于每个以元组 $(N, \\lambda_{\\mathrm{fix}}, s)$ 形式给出的测试用例，都根据上述算法计算 $f_{\\min}^{\\mathrm{grid}}$ 和 $f_{\\min}^{\\mathrm{rand}}$。然后，随机搜索相对于带阻塞的网格搜索的优势 $\\Delta$ 计算如下：\n$$\\Delta = f_{\\min}^{\\mathrm{grid}} - f_{\\min}^{\\mathrm{rand}}$$\n$\\Delta$ 的正值表示随机搜索找到了比带阻塞的网格搜索更好（更低）的损失值。\n\n对四个测试用例中的每一个都执行此过程：\n- 用例 $1$：$(N, \\lambda_{\\mathrm{fix}}, s) = (30, 10^{-1}, 2027)$\n- 用例 $2$：$(N, \\lambda_{\\mathrm{fix}}, s) = (1, 10^{-1}, 123)$\n- 用例 $3$：$(N, \\lambda_{\\mathrm{fix}}, s) = (30, 10^{-3}, 2027)$\n- 用例 $4$：$(N, \\lambda_{\\mathrm{fix}}, s) = (10, 10^{-2}, 42)$\n\n最终答案中的 Python 程序精确地实现了这些步骤，计算了四个 $\\Delta$ 值，并根据指定的输出格式对它们进行了格式化。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem by simulating and comparing\n    blocked grid search and random search for hyperparameter optimization.\n    \"\"\"\n\n    def objective_function(x, y):\n        \"\"\"\n        Computes the synthetic validation loss f(x, y).\n        x = log10(eta), y = log10(lambda).\n        \"\"\"\n        term1 = (x + 2.0)**2\n        term2 = 2.5 * (y + 3.0)**2\n        term3 = 0.3 * (x + 2.0) * (y + 3.0)\n        term4 = 0.1 * np.sin(5.0 * x) * np.sin(5.0 * y)\n        return term1 + term2 + term3 + term4\n\n    def grid_search_blocked(N, lambda_fix):\n        \"\"\"\n        Performs blocked grid search.\n        Fixes lambda (and thus y) and searches over a grid of eta (x) values.\n        \"\"\"\n        y_fix = np.log10(lambda_fix)\n\n        if N == 1:\n            x_points = np.array([-4.0])\n        else:\n            x_points = np.linspace(-4.0, 0.0, N)\n        \n        # Evaluate the objective function for all x points at the fixed y.\n        # This can be vectorized since objective_function is written with numpy ops.\n        y_points = np.full_like(x_points, y_fix)\n        losses = objective_function(x_points, y_points)\n        \n        return np.min(losses)\n\n    def random_search(N, s):\n        \"\"\"\n        Performs random search.\n        Samples N points (x, y) from the uniform distribution over the domain.\n        \"\"\"\n        # Create a random number generator with the specified seed for reproducibility.\n        rng = np.random.default_rng(s)\n        \n        # Sample N points from the search space.\n        x_samples = rng.uniform(-4.0, 0.0, N)\n        y_samples = rng.uniform(-5.0, -1.0, N)\n        \n        # Evaluate the objective function at the sampled points.\n        losses = objective_function(x_samples, y_samples)\n        \n        return np.min(losses)\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (N, lambda_fix, s)\n        (30, 1e-1, 2027),  # Case 1\n        (1, 1e-1, 123),   # Case 2\n        (30, 1e-3, 2027),  # Case 3\n        (10, 1e-2, 42),   # Case 4\n    ]\n\n    results = []\n    for N, lambda_fix, s in test_cases:\n        # Calculate the minimum loss found by each method\n        f_min_grid = grid_search_blocked(N, lambda_fix)\n        f_min_rand = random_search(N, s)\n        \n        # Calculate the advantage of random search\n        delta = f_min_grid - f_min_rand\n        results.append(delta)\n\n    # Final print statement in the exact required format.\n    # Each result is formatted to six decimal places.\n    print(f\"[{','.join([f'{r:.6f}' for r in results])}]\")\n\nsolve()\n```", "id": "3133107"}, {"introduction": "在前两个实践中，我们通过编程直观地感受了随机搜索的优势。现在，是时候深入其背后的数学原理了。这个练习要求你运用概率论和最优化理论，从数学上推导随机搜索的期望表现，并与网格搜索在最坏情况下的性能下界进行对比。通过这个推导，你将从根本上理解为什么在样本量相同时，随机搜索在高维空间中通常比网格搜索更有效。[@problem_id:3133146]", "problem": "您正在通过随机搜索或网格搜索来调整深度神经网络的超参数，超参数域为 $\\Lambda = [0,1]^d$。设 $g:\\Lambda \\to [0,1]$ 表示验证准确率作为超参数的函数，并假设 $g$ 对于欧几里得范数是 $L$-利普希茨（Lipschitz）的，即对所有 $\\lambda,\\lambda' \\in \\Lambda$ 都有 $|g(\\lambda) - g(\\lambda')| \\le L \\|\\lambda - \\lambda'\\|_{2}$。设 $g^{\\star} = \\max_{\\lambda \\in \\Lambda} g(\\lambda)$ 表示域中的最优验证准确率。\n\n随机搜索从 $\\Lambda$ 中独立且均匀地采样超参数 $\\lambda_{1},\\dots,\\lambda_{n}$。将得到的验证准确率表示为 $X_{1} = g(\\lambda_{1}),\\dots,X_{n} = g(\\lambda_{n})$。为进行此分析，我们将 $X_{i}$ 的边际分布建模为在 $[0,1]$ 上受支持的独立同分布（i.i.d.）随机变量，其累积分布函数为 $F$。特别地，假设在 $[0,1]$ 上 $F(x) = x$（即 $X_{i} \\sim \\mathrm{Uniform}(0,1)$），当不假设任何结构时，这是在 $[0,1]$ 上的准确率的一个常见的无信息先验。\n\n网格搜索使用一个规则网格，每个坐标轴上有 $m$ 个点（包括端点），因此网格点的总数为 $n = m^{d}$，其中整数 $m \\ge 2$。每个坐标轴上的网格间距为 $h = \\frac{1}{m-1}$。\n\n仅使用累积分布函数、独立性和顺序统计量的定义，推导：\n- 在 $n$ 次试验后，随机搜索下预期最佳验证准确率的解析表达式 $E[\\max\\{X_{1},\\dots,X_{n}\\}]$，特别是在 $F(x) = x$（在 $[0,1]$ 上）的情况下的表达式。\n- $n$ 点网格搜索所能达到的最佳验证准确率的最坏情况下的下界，用 $g^{\\star}$、$L$、$d$ 和 $n$ 表示。\n\n将您的最终结果报告为一个双元素行矩阵，其第一个元素是在均匀模型下 $E[\\max\\{X_{1},\\dots,X_{n}\\}]$ 的精确表达式，第二个元素是网格搜索的下界，该下界通过消去 $m$ 纯粹用 $g^{\\star}$、$L$、$d$ 和 $n$ 表示。不要提供任何数值近似；最终答案必须是一个没有单位的精确解析表达式。", "solution": "用户提供了一个问题陈述，要求推导与超参数优化策略相关的两个量：随机搜索的预期最佳准确率和网格搜索的最坏情况下的下界。\n\n### 问题验证\n\n**步骤 1：提取已知条件**\n- 超参数域：$\\Lambda = [0,1]^d$。\n- 验证准确率函数：$g:\\Lambda \\to [0,1]$。\n- $g$ 的利普希茨（Lipschitz）性质：对所有 $\\lambda,\\lambda' \\in \\Lambda$ 都有 $|g(\\lambda) - g(\\lambda')| \\le L \\|\\lambda - \\lambda'\\|_{2}$。\n- 最优验证准确率：$g^{\\star} = \\max_{\\lambda \\in \\Lambda} g(\\lambda)$。\n- **随机搜索**：\n    - 样本 $\\lambda_{1},\\dots,\\lambda_{n}$ 是从 $\\Lambda$ 中独立同分布均匀采样的。\n    - 验证准确率 $X_{i} = g(\\lambda_{i})$ 被建模为独立同分布的随机变量。\n    - $X_i$ 的累积分布函数（CDF）为 $F(x) = x$，$x \\in [0,1]$。\n- **网格搜索**：\n    - 每个坐标轴上有 $m$ 个点的规则网格。\n    - 网格点总数：$n = m^{d}$，其中 $m \\ge 2$ 为整数。\n    - 网格间距：$h = \\frac{1}{m-1}$。\n\n**步骤 2：使用提取的已知条件进行验证**\n该问题在机器学习和优化理论领域具有科学依据。所使用的概念（随机搜索、网格搜索、利普希茨连续性、顺序统计量）是标准且定义明确的。该问题是适定的（well-posed），为推导所要求的量提供了足够的信息。假设验证准确率 $X_i$ 服从 $\\mathrm{Uniform}(0,1)$ 分布被明确说明是为了分析而做出的建模选择，这在算法的理论比较中是常见的做法。该问题是客观的，使用了精确的数学语言，并且不包含矛盾或歧义。\n\n**步骤 3：结论与行动**\n该问题被判定为**有效**。将提供完整解答。\n\n---\n\n### 解答推导\n\n该问题包含两部分。第一部分是计算随机搜索的预期最佳准确率，第二部分是找到网格搜索所能达到的最佳准确率的最坏情况下的下界。\n\n#### 第 1 部分：随机搜索的预期最佳准确率\n\n设 $X_1, X_2, \\dots, X_n$ 是验证准确率的一个独立同分布样本。问题陈述，这些被建模为累积分布函数（CDF）为 $F(x) = x$（对于 $x \\in [0,1]$）的随机变量，这对应于一个标准均匀分布，$X_i \\sim \\mathrm{Uniform}(0,1)$。我们需要求这些样本的最大值的期望值，$E[\\max\\{X_1, \\dots, X_n\\}]$。\n\n设 $Y = \\max\\{X_1, \\dots, X_n\\}$。随机变量 $Y$ 是样本的第 $n$ 个顺序统计量。为了求其期望，我们首先确定它的累积分布函数 $F_Y(y)$。\n根据定义，$F_Y(y) = P(Y \\le y)$。\n一组随机变量的最大值小于或等于 $y$ 的充要条件是，该组中的所有变量都小于或等于 $y$。\n$$F_Y(y) = P(\\max\\{X_1, \\dots, X_n\\} \\le y) = P(X_1 \\le y, X_2 \\le y, \\dots, X_n \\le y)$$\n由于随机变量 $X_i$ 是独立的，联合概率是边际概率的乘积：\n$$F_Y(y) = \\prod_{i=1}^n P(X_i \\le y)$$\n由于这些变量也是同分布的，其累积分布函数为 $F(x)$，因此可以简化为：\n$$F_Y(y) = [F(y)]^n$$\n已知当 $x \\in [0,1]$ 时 $F(x)=x$，则当 $y \\in [0,1]$ 时，$Y$ 的累积分布函数为：\n$$F_Y(y) = y^n$$\n一个在 $[0,1]$ 上受支持的非负随机变量 $Y$ 的期望值可以用其累积分布函数计算：$E[Y] = \\int_0^1 (1 - F_Y(y)) dy$。\n代入 $F_Y(y)$ 的表达式：\n$$E[Y] = \\int_0^1 (1 - y^n) dy$$\n计算该积分：\n$$E[Y] = \\left[ y - \\frac{y^{n+1}}{n+1} \\right]_0^1 = \\left(1 - \\frac{1^{n+1}}{n+1}\\right) - \\left(0 - \\frac{0^{n+1}}{n+1}\\right) = 1 - \\frac{1}{n+1}$$\n$$E[Y] = \\frac{n+1-1}{n+1} = \\frac{n}{n+1}$$\n因此，在给定模型下，随机搜索的预期最佳验证准确率为 $\\frac{n}{n+1}$。\n\n#### 第 2 部分：网格搜索的最坏情况下下界\n\n对于网格搜索，我们需要找到所能达到的最佳准确率 $\\max_{\\lambda \\in \\Lambda_{grid}} g(\\lambda)$ 的一个下界，其中 $\\Lambda_{grid}$ 是 $n$ 个网格点的集合。这个界限必须对任何 $L$-利普希茨函数 $g$ 成立。最坏情况对应于一个函数 $g$ 及其最大值 $g^\\star$ 的位置，使得网格搜索的表现尽可能差。\n\n设 $\\lambda^\\star \\in \\Lambda = [0,1]^d$ 是一个超参数向量，在该向量上达到了真正的最优验证准确率，即 $g(\\lambda^\\star) = g^\\star$。网格搜索方法不知道 $\\lambda^\\star$；它只在 $\\Lambda_{grid}$ 中的点上评估 $g$。它找到的值是 $\\max_{\\lambda_i \\in \\Lambda_{grid}} g(\\lambda_i)$。\n\n我们必须找到 $\\lambda^\\star$ 离最近的网格点可以有多远。网格在 $d$ 个轴的每一个上都有 $m$ 个等间距的点，包括端点 $0$ 和 $1$。每个轴上的网格间距是 $h = \\frac{1}{m-1}$。沿任意轴的网格点坐标为 $\\{0, h, 2h, \\dots, (m-1)h=1\\}$。\n\n对于任意点 $\\lambda = (c_1, \\dots, c_d) \\in \\Lambda$，对于每个坐标 $c_j$，在该轴上存在一个网格坐标 $p_j$，使得 $|c_j - p_j| \\le \\frac{h}{2}$。设 $\\lambda_p = (p_1, \\dots, p_d)$ 是由这些最近坐标构成的网格点。$\\lambda$ 和 $\\lambda_p$ 之间的欧几里得距离的平方是有界的：\n$$\\|\\lambda - \\lambda_p\\|_2^2 = \\sum_{j=1}^d (c_j - p_j)^2 \\le \\sum_{j=1}^d \\left(\\frac{h}{2}\\right)^2 = d \\frac{h^2}{4}$$\n取平方根，我们发现对于任意点 $\\lambda \\in \\Lambda$，都存在一个网格点 $\\lambda_p \\in \\Lambda_{grid}$ 使得：\n$$\\|\\lambda - \\lambda_p\\|_2 \\le \\frac{h\\sqrt{d}}{2}$$\n这个界是紧的，代表了域中任意点到最近网格点的最大距离（在网格超单元的中心达到）。\n\n现在，我们将此应用于最优点 $\\lambda^\\star$。必须存在一个网格点，我们称之为 $\\lambda_{closest} \\in \\Lambda_{grid}$，使得：\n$$\\|\\lambda^\\star - \\lambda_{closest}\\|_2 \\le \\frac{h\\sqrt{d}}{2}$$\n函数 $g$ 是 $L$-利普希茨的，这意味着 $|g(\\lambda^\\star) - g(\\lambda_{closest})| \\le L \\|\\lambda^\\star - \\lambda_{closest}\\|_2$。由于 $g(\\lambda^\\star)=g^\\star$ 是最大值，我们可以写成：\n$$g^\\star - g(\\lambda_{closest}) \\le L \\|\\lambda^\\star - \\lambda_{closest}\\|_2$$\n使用我们找到的距离界限：\n$$g^\\star - g(\\lambda_{closest}) \\le L \\frac{h\\sqrt{d}}{2}$$\n重新整理这个不等式，得到网格点 $\\lambda_{closest}$ 处准确率的一个下界：\n$$g(\\lambda_{closest}) \\ge g^\\star - \\frac{Lh\\sqrt{d}}{2}$$\n网格搜索找到的最佳准确率是 $\\max_{\\lambda_i \\in \\Lambda_{grid}} g(\\lambda_i)$。由于 $\\lambda_{closest}$ 是网格点之一，找到的最大准确率必须至少与 $g(\\lambda_{closest})$ 一样大：\n$$\\max_{\\lambda_i \\in \\Lambda_{grid}} g(\\lambda_i) \\ge g(\\lambda_{closest})$$\n结合这些不等式，可以得到最坏情况下的下界：\n$$\\max_{\\lambda_i \\in \\Lambda_{grid}} g(\\lambda_i) \\ge g^\\star - \\frac{Lh\\sqrt{d}}{2}$$\n问题要求这个界用 $g^\\star, L, d$ 和 $n$ 来表示。我们使用给定的关系 $h = \\frac{1}{m-1}$ 和 $n=m^d$。从 $n=m^d$，我们解出 $m$ 得到 $m=n^{1/d}$。我们将此代入 $h$ 的表达式中：\n$$h = \\frac{1}{n^{1/d} - 1}$$\n将这个 $h$ 的表达式代入下界中：\n$$\\max_{\\lambda_i \\in \\Lambda_{grid}} g(\\lambda_i) \\ge g^\\star - \\frac{L\\sqrt{d}}{2(n^{1/d} - 1)}$$\n这就是网格搜索所期望的最坏情况下下界。\n\n### 最终表达式\n\n所要求的两个表达式是：\n1.  随机搜索的预期最佳准确率：$\\frac{n}{n+1}$。\n2.  网格搜索的最坏情况下下界：$g^\\star - \\frac{L\\sqrt{d}}{2(n^{1/d} - 1)}$。\n\n这些将按要求格式化为一个双元素行矩阵。", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{n}{n+1}  g^{\\star} - \\frac{L\\sqrt{d}}{2(n^{1/d} - 1)}\n\\end{pmatrix}\n}\n$$", "id": "3133146"}]}