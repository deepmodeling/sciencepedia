{"hands_on_practices": [{"introduction": "理论是实践的基石。在深入探讨Xavier初始化的经验效果之前，我们首先需要从其核心数学原理入手。这项练习将引导你推导Xavier（或称Glorot）均匀分布初始化的边界值$a$，并利用它来预测网络第一层神经元预激活值的方差，从而让你亲手验证该初始化方法如何实现其保持信号尺度的设计目标。[@problem_id:3200147]", "problem": "考虑一个全连接前馈神经网络，其输入层大小为 $\\mathbb{R}^{784}$，一个隐藏层大小为 $\\mathbb{R}^{256}$，输出层大小为 $\\mathbb{R}^{10}$。设隐藏层的预激活为 $z^{(1)} = W^{(1)} x$，输出层的预激活为 $z^{(2)} = W^{(2)} h$，其中 $h$ 是对 $z^{(1)}$ 应用逐点非线性函数得到的隐藏层激活。假设在初始化时满足以下条件：\n- 输入分量是独立同分布的（i.i.d.），均值为零，方差为 $\\operatorname{Var}(x_{i}) = 1$。\n- 所有偏置均为零。\n- 非线性函数是平滑的，并且在初始化时在零点附近被线性化，因此其局部增益近似为 $1$。\n- 每个权重矩阵 $W^{(\\ell)}$ 的条目都是从支撑在对称区间 $\\left[-a_{\\ell}, a_{\\ell}\\right]$ 上的零均值均匀分布中独立抽取的。\n- 独立性近似适用，因此在向前和向后传播方差时，不同项之间的协方差为零。\n\n设计目标是选择权重方差，以便在线性化和独立性假设下，信号的尺度在前向和后向传播中保持平衡。具体来说，如果一个具有 $f_{\\text{in}}$ 个输入和 $f_{\\text{out}}$ 个输出的层的前向方差保持因子为 $f_{\\text{in}} \\operatorname{Var}(W)$，后向方差保持因子为 $f_{\\text{out}} \\operatorname{Var}(W)$，则通过将这两个因子的算术平均值匹配为 $1$ 来实现对称折衷。仅使用此原则、在 $\\left[-a, a\\right]$ 上的均匀分布的方差公式以及独立随机变量之和的方差传播规则，完成以下任务：\n- 确定层 $\\mathbb{R}^{784} \\to \\mathbb{R}^{256}$ 中 $W^{(1)}$ 的边界 $a_{1}$。\n- 确定层 $\\mathbb{R}^{256} \\to \\mathbb{R}^{10}$ 中 $W^{(2)}$ 的边界 $a_{2}$。\n- 使用您推导出的 $W^{(1)}$ 的边界，预测初始化时隐藏层预激活 $z^{(1)}$ 的均方根（RMS；root mean square），其中 RMS 定义为 $\\sqrt{\\mathbb{E}\\!\\left[(z^{(1)}_{j})^{2}\\right]}$。\n\n仅报告 $z^{(1)}$ 的隐藏层预激活 RMS 的数值作为您的最终答案。将您的最终答案四舍五入到 $4$ 位有效数字。", "solution": "` 标签：\n        1.  从问题验证结论开始。\n        2.  推导权重的通用方差公式 $\\operatorname{Var}(W)$。\n        3.  将此通用方差与均匀分布的边界 $a$ 相关联，推导出 $a$ 的公式。\n        4.  将此公式应用于特定层，以找到 $a_1$ 和 $a_2$ 的表达式。\n        5.  推导隐藏层预激活的方差表达式 $\\operatorname{Var}(z^{(1)}_j)$。\n        6.  解释由于均值为零，RMS 是方差的平方根。\n        7.  将第一层的值（$f_{\\text{in},1}$，$f_{\\text{out},1}$）代入方差表达式。\n        8.  计算数值并按要求进行四舍五入。\n\n    *   最终检查问题措辞：“一个具有 $f_{\\text{in}}$ 个输入和 $f_{\\text{out}}$ 个输出的层的前向方差保持因子为 $f_{\\text{in}} \\operatorname{Var}(W)$”。这个量是 $\\frac{\\text{Var}(\\text{output})}{\\text{Var}(\\text{input})}$。我的推导 $\\operatorname{Var}(z^{(1)}_j) = (f_{\\text{in}} \\operatorname{Var}(W^{(1)})) \\times \\operatorname{Var}(x)$ 证实了这一点。问题陈述与我的推导一致。\n\n    *   这个问题设计得很好。它给了你“因子”以及一个组合它们的规则，所以你甚至不需要从第一性原理推导这些因子，只需应用规则，然后用结果来计算激活方差。措辞精确，避免了歧义。\n\n    *   规则是：\n        $\\frac{\\text{forward factor} + \\text{backward factor}}{2} = 1$\n        $\\frac{f_{\\text{in}}\\text{Var}(W) + f_{\\text{out}}\\text{Var}(W)}{2} = 1$\n        这导出 $\\text{Var}(W) = \\frac{2}{f_{\\text{in}} + f_{\\text{out}}}$。这是问题的核心。\n\n    *   层中一个神经元的预激活 $z_j$ 的方差是：\n        $\\text{Var}(z_j) = \\text{Var}(\\sum_i w_i x_i) = \\sum_i \\text{Var}(w_i x_i)$\n        $= \\sum_i (\\text{Var}(w_i)\\text{Var}(x_i) + \\text{Var}(w_i)(\\mathbb{E}[x_i])^2 + \\text{Var}(x_i)(\\mathbb{E}[w_i])^2)$\n        给定 $\\mathbb{E}[x_i]=0$ 和 $\\mathbb{E}[w_i]=0$，这可以简化为：\n        $\\text{Var}(z_j) = \\sum_i \\text{Var}(w_i)\\text{Var}(x_i)$\n        由于权重是独立同分布的，输入也是独立同分布的，\n        $\\text{Var}(z_j) = \\sum_{i=1}^{f_{\\text{in}}} \\text{Var}(W)\\text{Var}(x) = f_{\\text{in}}\\text{Var}(W)\\text{Var}(x)$。\n        给定 $\\text{Var}(x)=1$，我们有 $\\text{Var}(z_j) = f_{\\text{in}}\\text{Var}(W)$。\n\n    *   对于第一个隐藏层：\n        $\\text{Var}(z^{(1)}_j) = f_{\\text{in},1} \\text{Var}(W^{(1)})$。\n        我们刚刚确定了 $\\text{Var}(W^{(1)}) = \\frac{2}{f_{\\text{in},1} + f_{\\text{out},1}}$。\n        所以，$\\text{Var}(z^{(1)}_j) = f_{\\text{in},1} \\left( \\frac{2}{f_{\\text{in},1} + f_{\\text{out},1}} \\right)$。\n        $f_{\\text{in},1} = 784$\n        $f_{\\text{out},1} = 256$\n        $\\text{Var}(z^{(1)}_j) = 784 \\left( \\frac{2}{784 + 256} \\right) = 784 \\left( \\frac{2}{1040} \\right) = \\frac{1568}{1040} = \\frac{98}{65}$。\n\n    *   要求计算 RMS。RMS($Z$) = $\\sqrt{\\mathbb{E}[Z^2]}$。对于 $z^{(1)}_j$，我们有 $\\mathbb{E}[z^{(1)}_j] = 0$，所以 $\\text{Var}(z^{(1)}_j) = \\mathbb{E}[(z^{(1)}_j)^2] - (\\mathbb{E}[z^{(1)}_j])^2 = \\mathbb{E}[(z^{(1)}_j)^2]$。因此，RMS($z^{(1)}_j$) = $\\sqrt{\\text{Var}(z^{(1)}_j)} = \\sqrt{\\frac{98}{65}}$。\n\n    *   数值计算和四舍五入是最后步骤。\n        $\\sqrt{98/65} \\approx 1.227881226...$\n        四位有效数字：$1.228$。\n\n    *   整个工作流程看起来是正确的，并且经过了双重检查。我现在将着手编写解决方案。", "answer": "$$\n\\boxed{1.228}\n$$", "id": "3200147"}, {"introduction": "理论的优美需要通过实验来验证。Xavier初始化与He初始化之所以并存，是因为它们分别适用于不同类型的激活函数。这项实践将通过蒙特卡洛模拟，直观地展示不同初始化策略与激活函数（$\\tanh$与$\\mathrm{ReLU}$）的组合效果，让你亲眼见证匹配的组合如何稳定信号方差，而不匹配的组合又如何导致信号的消失或爆炸。[@problem_id:3199598]", "problem": "给定一个包含 $L$ 层的全连接前馈网络，将第 $l$ 层的预激活定义为 $z^{(l)} = W^{(l)} a^{(l-1)}$，将后激活定义为 $a^{(l)} = \\phi\\!\\left(z^{(l)}\\right)$，其中 $a^{(0)} = x$。假设偏置为零，权重独立同分布，输入 $x \\in \\mathbb{R}^{n}$ 的分量独立、均值为零且方差有限。考虑两种广泛使用的随机权重初始化策略：Xavier (Glorot) 正态初始化和 He (Kaiming) 正态初始化，以及两种激活函数：$\\tanh$ 和修正线性单元 ($\\mathrm{ReLU}$)。目标是通过蒙特卡洛模拟，经验性地验证在哪种情况下，初始化策略能够近似保持预激活在各层间的方差不变，即在给定激活函数下，对于所有层 $l \\in \\{1,\\dots,L\\}$，都有 $\\operatorname{Var}\\!\\left(z^{(l)}\\right) \\approx \\operatorname{Var}(x)$。\n\n使用的基本原理：\n- 初始化时权重和激活在各坐标上的独立性，以及独立变量和的方差线性性质。\n- $\\tanh$ 和 $\\mathrm{ReLU}$ 作为逐点非线性函数的定义。\n- 对于一个数组 $Y \\in \\mathbb{R}^{s \\times d}$，沿 $s$ 个样本定义的样本方差估计量为 $$\\widehat{\\operatorname{Var}}(Y) = \\frac{1}{d}\\sum_{j=1}^{d} \\left(\\frac{1}{s}\\sum_{i=1}^{s} Y_{ij}^{2} - \\left(\\frac{1}{s}\\sum_{i=1}^{s} Y_{ij}\\right)^{2} \\right)$$。\n\n你的程序必须：\n1. 构建具有指定 $L$ 和层宽度的网络，其中每层的形状为 $(n_{\\text{in}}, n_{\\text{out}})$，为简化起见，此处 $n_{\\text{in}} = n_{\\text{out}}$。使用 Xavier 正态分布或 He 正态分布初始化每层的权重 $W^{(l)}$。使用由每种策略规定的方差进行零均值正态初始化；不添加任何偏置。\n2. 将输入 $x$ 抽取为 $s$ 个维度为 $n$ 的独立样本，每个分量服从 $\\mathcal{N}(0,1)$ 分布，即均值为零，方差为 1。\n3. 对于每一层 $l$，通过对每个坐标的样本方差进行平均，计算 $s$ 个样本上的经验预激活方差 $\\widehat{\\operatorname{Var}}\\!\\left(z^{(l)}\\right)$，并类似地计算经验输入方差 $\\widehat{\\operatorname{Var}}(x)$。将第 $l$ 层的相对偏差定义为 $$\\delta^{(l)} = \\frac{\\left|\\widehat{\\operatorname{Var}}\\!\\left(z^{(l)}\\right) - \\widehat{\\operatorname{Var}}(x)\\right|}{\\widehat{\\operatorname{Var}}(x)}.$$ 如果 $\\max_{1 \\le l \\le L} \\delta^{(l)} \\le \\varepsilon$，则认为一个测试用例保持了方差，容差 $\\varepsilon = 0.25$。\n4. 使用固定种子为 $12345$ 的伪随机数生成器以确保可复现性。\n\n测试套件：\n- 用例 1：$L=5$，宽度 $n=32$，样本数 $s=8000$，激活函数 $\\tanh$，初始化方法 Xavier 正态分布。\n- 用例 2：$L=5$，宽度 $n=32$，样本数 $s=8000$，激活函数 $\\tanh$，初始化方法 He 正态分布。\n- 用例 3：$L=5$，宽度 $n=32_，样本数 $s=8000$，激活函数 $\\mathrm{ReLU}$，初始化方法 He 正态分布。\n- 用例 4：$L=5$，宽度 $n=32$，样本数 $s=8000$，激活函数 $\\mathrm{ReLU}$，初始化方法 Xavier 正态分布。\n- 用例 5：$L=1$，宽度 $n=32$，样本数 $s=20000$，激活函数 $\\tanh$，初始化方法 Xavier 正态分布。\n- 用例 6：$L=1$，宽度 $n=32$，样本数 $s=20000$，激活函数 $\\mathrm{ReLU}$，初始化方法 He 正态分布。\n- 用例 7：$L=15$，宽度 $n=16$，样本数 $s=8000$，激活函数 $\\tanh$，初始化方法 Xavier 正态分布。\n- 用例 8：$L=15$，宽度 $n=16$，样本数 $s=8000$，激活函数 $\\mathrm{ReLU}$，初始化方法 He 正态分布。\n\n最终输出格式：\n你的程序应生成一行输出，其中包含一个用方括号括起来的逗号分隔列表（例如 $[\\text{result}_1,\\text{result}_2,\\dots]$），其中每个 $\\text{result}_i$ 是一个布尔值，表示第 $i$ 个测试用例是否保持了方差，顺序与上述测试用例套件完全一致。", "solution": "该问题经评估有效。它在科学上基于深度学习的既定原则，特别是关于权重初始化及其对信号传播的影响。问题陈述清晰，提供了所有必要的参数、定义和明确的成功标准。它没有矛盾、歧义和事实错误。因此，我们可以着手解决。\n\n目标是经验性地验证在何种条件下，深度神经网络各层之间的预激活 $z^{(l)}$ 的方差能够得以保持。此分析的核心在于连续层中预激活方差之间的递归关系。\n\n让我们考虑第 $l$ 层单个神经元的预激活：\n$$ z_i^{(l)} = \\sum_{j=1}^{n_{l-1}} W_{ij}^{(l)} a_j^{(l-1)} $$\n此处，$W_{ij}^{(l)}$ 是连接第 $l-1$ 层神经元 $j$ 与第 $l$ 层神经元 $i$ 的权重，$a_j^{(l-1)}$ 是前一层神经元 $j$ 的激活值。问题规定偏置为零，权重 $W_{ij}^{(l)}$ 从均值为零的分布中抽取，并且输入分量 $x_j$（构成 $a_j^{(0)}$）也具有零均值。我们假设在初始化时，所有 $j$ 的激活值 $a_j^{(l-1)}$ 均与权重 $W_{ij}^{(l)}$ 独立，并且是同分布的。此外，如果激活值 $a^{(l-1)}$ 是对称激活函数应用于零均值输入 $z^{(l-1)}$ 的输出，它们也将具有零均值。对于 $\\mathrm{ReLU}$ 激活函数，情况并非如此，但由于权重本身是零均值的，得到的预激活 $z^{(l)}$ 仍将具有零均值：$\\mathbb{E}[z_i^{(l)}] = \\sum_j \\mathbb{E}[W_{ij}^{(l)}] \\mathbb{E}[a_j^{(l-1)}] = \\sum_j 0 \\cdot \\mathbb{E}[a_j^{(l-1)}] = 0$。\n\n在这些条件下，$z_i^{(l)}$ 的方差由下式给出：\n$$ \\operatorname{Var}(z_i^{(l)}) = \\operatorname{Var}\\left(\\sum_{j=1}^{n_{l-1}} W_{ij}^{(l)} a_j^{(l-1)}\\right) $$\n由于和中的各项是独立的（因为权重和前一层的激活是独立的），和的方差等于方差的和：\n$$ \\operatorname{Var}(z_i^{(l)}) = \\sum_{j=1}^{n_{l-1}} \\operatorname{Var}(W_{ij}^{(l)} a_j^{(l-1)}) $$\n对于两个独立的随机变量 $U$ 和 $V$，如果至少其中一个的均值为零（例如 $\\mathbb{E}[U]=0$），则 $\\operatorname{Var}(UV) = \\mathbb{E}[U^2V^2] - (\\mathbb{E}[UV])^2 = \\mathbb{E}[U^2]\\mathbb{E}[V^2] - (\\mathbb{E}[U]\\mathbb{E}[V])^2 = \\operatorname{Var}(U)\\operatorname{Var}(V)$。由于 $\\mathbb{E}[W_{ij}^{(l)}] = 0$，我们有：\n$$ \\operatorname{Var}(z_i^{(l)}) = \\sum_{j=1}^{n_{l-1}} \\operatorname{Var}(W_{ij}^{(l)}) \\operatorname{Var}(a_j^{(l-1)}) $$\n假设第 $l$ 层的所有权重都是从方差为 $\\operatorname{Var}(W^{(l)})$ 的分布中独立同分布地抽取的，并且第 $l-1$ 层的所有激活值也是从方差为 $\\operatorname{Var}(a^{(l-1)})$ 的分布中独立同分布地抽取的，那么上式可以简化为：\n$$ \\operatorname{Var}(z^{(l)}) = n_{l-1} \\operatorname{Var}(W^{(l)}) \\operatorname{Var}(a^{(l-1)}) $$\n其中 $n_{l-1}$ 是第 $l-1$ 层的神经元数量。为了维持稳定的信号传播，我们需要保持方差不变，即 $\\operatorname{Var}(z^{(l)}) \\approx \\operatorname{Var}(z^{(l-1)})$。这需要仔细选择权重初始化方差 $\\operatorname{Var}(W^{(l)})$，以抵消激活函数 $\\phi$ 对方差的影响，这种影响由项 $\\operatorname{Var}(a^{(l-1)}) = \\operatorname{Var}(\\phi(z^{(l-1)}))$ 捕获。\n\n**激活函数分析**\n\n1.  **$\\tanh$ 激活函数：** 双曲正切函数 $\\tanh(z)$ 关于原点对称（$\\tanh(0)=0$），并且对于小输入其行为类似于恒等函数（当 $z \\approx 0$ 时，$\\tanh(z) \\approx z$）。如果我们假设预激活 $z^{(l-1)}$ 集中在零附近（这在初始训练阶段是理想状态），那么 $\\operatorname{Var}(a^{(l-1)}) = \\operatorname{Var}(\\tanh(z^{(l-1)})) \\approx \\operatorname{Var}(z^{(l-1)})$。将此代入我们的传播方程中，得到：\n    $$ \\operatorname{Var}(z^{(l)}) \\approx n_{l-1} \\operatorname{Var}(W^{(l)}) \\operatorname{Var}(z^{(l-1)}) $$\n    为实现 $\\operatorname{Var}(z^{(l)}) \\approx \\operatorname{Var}(z^{(l-1)})$，我们必须设置 $n_{l-1} \\operatorname{Var}(W^{(l)}) = 1$。\n    **Xavier (Glorot) 正态初始化**正是为这种情况设计的。它将从 $\\mathcal{N}(0, \\sigma^2)$ 中抽取的权重 $W^{(l)}$ 的方差设置为：\n    $$ \\operatorname{Var}(W^{(l)}) = \\frac{2}{n_{l-1} + n_l} $$\n    在我们的特定问题中，$n_{l-1} = n_l = n$，因此这变为 $\\operatorname{Var}(W^{(l)}) = \\frac{2}{2n} = \\frac{1}{n} = \\frac{1}{n_{l-1}}$。这个选择完美地满足了条件 $n_{l-1} \\operatorname{Var}(W^{(l)}) = 1$。因此，预期 Xavier 初始化对于 $\\tanh$ 激活函数能够保持方差。\n\n2.  **$\\mathrm{ReLU}$ 激活函数：** 修正线性单元 $\\mathrm{ReLU}(z) = \\max(0, z)$ 不是对称的。对于一个零均值、对称的输入分布 $z^{(l-1)}$（如高斯分布），恰好一半的输入将被设置为零。这会影响方差。设 $z \\sim \\mathcal{N}(0, \\sigma_z^2)$。输出 $a = \\mathrm{ReLU}(z)$ 的方差为 $\\operatorname{Var}(a) = \\mathbb{E}[a^2] - (\\mathbb{E}[a])^2$。\n    激活值平方的期望是 $\\mathbb{E}[a^2] = \\mathbb{E}[\\max(0, z)^2] = \\int_0^\\infty z^2 p(z) dz$。由于正态分布 $p(z)$ 的对称性，这个积分是 $z^2$ 总积分的一半：$\\mathbb{E}[a^2] = \\frac{1}{2} \\int_{-\\infty}^\\infty z^2 p(z) dz = \\frac{1}{2}\\mathbb{E}[z^2] = \\frac{1}{2}\\operatorname{Var}(z)$。\n    所以，对于 $\\mathrm{ReLU}$，我们有 $\\operatorname{Var}(a^{(l-1)}) \\approx \\frac{1}{2}\\operatorname{Var}(z^{(l-1)})$。方差传播方程变为：\n    $$ \\operatorname{Var}(z^{(l)}) \\approx n_{l-1} \\operatorname{Var}(W^{(l)}) \\left(\\frac{1}{2}\\operatorname{Var}(z^{(l-1)})\\right) $$\n    为了保持方差，我们必须有 $n_{l-1} \\operatorname{Var}(W^{(l)}) \\frac{1}{2} = 1$，这意味着 $\\operatorname{Var}(W^{(l)}) = \\frac{2}{n_{l-1}}$。\n    **He (Kaiming) 正态初始化**是为这种情况设计的。它将方差设置为：\n    $$ \\operatorname{Var}(W^{(l)}) = \\frac{2}{n_{l-1}} $$\n    这个选择恰好满足了条件。因此，预期 He 初始化对于 $\\mathrm{ReLU}$ 激活函数能够保持方差。预测不匹配的组合（例如，$\\tanh$ 与 He，$\\mathrm{ReLU}$ 与 Xavier）将分别导致方差爆炸或消失。\n\n**模拟过程**\n\n该程序将为 8 个测试用例中的每一个执行蒙特卡洛模拟。对于每个用例：\n1.  使用种子值 $12345$ 初始化伪随机数生成器，以确保可复现性。\n2.  生成一个输入数据矩阵 $x \\in \\mathbb{R}^{s \\times n}$，其中每个元素都从 $\\mathcal{N}(0, 1)$ 中抽取。使用提供的公式计算经验输入方差 $\\widehat{\\operatorname{Var}}(x)$。\n3.  网络逐层处理，从 $l=1$到$L$。在每一层中，权重矩阵 $W^{(l)}$ 从一个零均值正态分布中初始化，其方差由指定的策略（Xavier 或 He）决定。\n4.  计算预激活 $z^{(l)} = a^{(l-1)} W^{(l)}$。\n5.  计算经验方差 $\\widehat{\\operatorname{Var}}(z^{(l)})$。计算相对偏差 $\\delta^{(l)} = |\\widehat{\\operatorname{Var}}(z^{(l)}) - \\widehat{\\operatorname{Var}}(x)| / \\widehat{\\operatorname{Var}}(x)$。\n6.  跟踪所有层中的最大相对偏差 $\\max_{1 \\le l \\le L} \\delta^{(l)}$。\n7.  计算后激活 $a^{(l)} = \\phi(z^{(l)})$，作为下一层的输入。\n8.  遍历所有层后，如果 $\\max_{l} \\delta^{(l)} \\le \\varepsilon$（其中 $\\varepsilon = 0.25$），则认为该测试用例保持了方差。此检查将为每个用例产生一个布尔结果，然后报告该结果。", "answer": "```python\nimport numpy as np\n\ndef tanh(x):\n    \"\"\"Hyperbolic tangent activation function.\"\"\"\n    return np.tanh(x)\n\ndef relu(x):\n    \"\"\"Rectified Linear Unit (ReLU) activation function.\"\"\"\n    return np.maximum(0, x)\n\ndef empirical_variance(Y):\n    \"\"\"\n    Computes the empirical variance as defined in the problem statement.\n    For an array Y of shape (s, d), this is the average of the biased sample \n    variances of each of the d columns.\n    \n    Formula: Var(Y) = (1/d) * sum_j [ (1/s) * sum_i(Y_ij^2) - ((1/s) * sum_i(Y_ij))^2 ]\n    This is equivalent to the mean of np.var(Y, axis=0).\n    \"\"\"\n    if Y.ndim == 1:\n        # If input is a 1D array, treat it as (s, 1)\n        return np.var(Y)\n    return np.mean(np.var(Y, axis=0))\n\ndef run_simulation(L, n, s, activation_func, init_strategy, seed, epsilon):\n    \"\"\"\n    Runs a single simulation for a given network configuration.\n\n    Args:\n        L (int): Number of layers.\n        n (int): Width of each layer.\n        s (int): Number of samples.\n        activation_func (callable): The activation function to use.\n        init_strategy (str): The weight initialization strategy ('xavier' or 'he').\n        seed (int): The seed for the random number generator.\n        epsilon (float): The tolerance for variance preservation.\n\n    Returns:\n        bool: True if variance is preserved, False otherwise.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    \n    # Generate input data x of shape (s, n) from N(0, 1)\n    x = rng.normal(loc=0.0, scale=1.0, size=(s, n))\n    var_x = empirical_variance(x)\n    \n    a_prev = x\n    max_deviation = 0.0\n    \n    # Propagate through the network for L layers\n    for l in range(1, L + 1):\n        # Layer dimensions are (n_in, n_out)\n        n_in = n\n        n_out = n\n        \n        # Determine standard deviation for weight initialization\n        if init_strategy == 'xavier':\n            # Variance = 2 / (n_in + n_out)\n            std_dev = np.sqrt(2.0 / (n_in + n_out))\n        elif init_strategy == 'he':\n            # Variance = 2 / n_in\n            std_dev = np.sqrt(2.0 / n_in)\n        else:\n            raise ValueError(f\"Unknown initialization strategy: {init_strategy}\")\n            \n        # Initialize weights W of shape (n_in, n_out)\n        W = rng.normal(loc=0.0, scale=std_dev, size=(n_in, n_out))\n        \n        # Compute pre-activations z = a_prev @ W of shape (s, n_out)\n        z = a_prev @ W\n        \n        # Compute empirical variance and relative deviation\n        var_z = empirical_variance(z)\n        if var_x > 1e-9: # Avoid division by zero\n            deviation = np.abs(var_z - var_x) / var_x\n        else:\n            deviation = np.abs(var_z)\n\n        if deviation > max_deviation:\n            max_deviation = deviation\n            \n        # Compute post-activations for the next layer\n        a_prev = activation_func(z)\n        \n    # Check if the maximum deviation is within the tolerance\n    return max_deviation = epsilon\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    # Define test cases as (L, n, s, activation_name, init_strategy_name)\n    test_cases = [\n        (5, 32, 8000, 'tanh', 'xavier'),\n        (5, 32, 8000, 'tanh', 'he'),\n        (5, 32, 8000, 'relu', 'he'),\n        (5, 32, 8000, 'relu', 'xavier'),\n        (1, 32, 20000, 'tanh', 'xavier'),\n        (1, 32, 20000, 'relu', 'he'),\n        (15, 16, 8000, 'tanh', 'xavier'),\n        (15, 16, 8000, 'relu', 'he'),\n    ]\n    \n    # Map string names to functions and parameters\n    activation_map = {'tanh': tanh, 'relu': relu}\n    epsilon = 0.25\n    seed = 12345\n    \n    results = []\n    \n    # Run simulation for each test case\n    for case in test_cases:\n        L, n, s, act_name, init_name = case\n        activation_func = activation_map[act_name]\n        \n        # Each test case is an independent experiment, so we use the same seed,\n        # creating a new RNG instance for each to ensure reproducibility.\n        result = run_simulation(L, n, s, activation_func, init_name, seed, epsilon)\n        results.append(result)\n        \n    # Format and print the final output as a single-line list of booleans\n    # e.g., [True,False,True,False,True,True,True,True]\n    # np.bool_ maps to Python's True/False, and str() correctly converts them.\n    # The problem asks for booleans, and Python's `str(True)` is 'True'.\n    print(f\"[{','.join(str(r) for r in results)}]\")\n\n# Execute the solution\n# solve()\n# The user might try to run this, so I will comment out the final call.\n# Actually, the instruction is just to provide the polished content. The code is an answer. It should be runnable.\n# I will make a small change to the print statement to match the python version and be more robust.\n# The `str(True)` is 'True', which is correct, but `str(np.bool_(True))` is also 'True'.\n# `map(str, results)` is cleaner. I will change the f-string to use that.\n# `print(f\"[{','.join(map(str, results))}]\")` is better than what I had `str(r) for r in results`. Let me rewrite it.\n# No, `','.join(str(r) for r in results)` is a valid generator expression. `map(str, results)` is more idiomatic. I will use map.\nprint(f\"[{','.join(map(str, results))}]\")\n```", "id": "3199598"}, {"introduction": "掌握一个原则比记住一个公式更为重要。Xavier和He初始化并非孤立的技巧，而是方差保持这一更普适原则的具体应用。这项挑战性的练习将带你超越标准场景，为带泄露修正线性单元（Leaky ReLU）这一激活函数推导一个量身定制的初始化方案，从而深化你对信号在网络中传播的动态平衡的理解。[@problem_id:3200172]", "problem": "考虑一个具有 $fan_{in}$ 个输入和 $fan_{out}$ 个输出的全连接层。设神经元 $j$ 的预激活值为 $z_{j}=\\sum_{i=1}^{fan_{in}} W_{ji} x_{i}$，偏置为零。假设输入 $\\{x_{i}\\}_{i=1}^{fan_{in}}$ 是独立同分布 (i.i.d.) 的，满足 $\\mathbb{E}[x_{i}]=0$，$\\mathbb{E}[x_{i}^{2}]=q$，且其分布关于 $0$ 对称，并且权重 $\\{W_{ji}\\}$ 是独立同分布的，与输入无关，满足 $\\mathbb{E}[W_{ji}]=0$ 和 $\\mathrm{Var}(W_{ji})=\\sigma^{2}$。非线性函数是带泄露的修正线性单元 (Leaky ReLU)，定义为 $\\phi(z)=\\max(z,0)+\\alpha\\,\\min(z,0)$，其中负斜率参数满足 $\\alpha\\in(0,1)$。\n\n仅使用期望的线性性质、独立和的方差可加性以及 $z_{j}$ 关于 $0$ 的对称性，推导 $\\sigma^{2}$ 的缩放关系，使得输出二阶矩 $\\mathbb{E}[\\phi(z_{j})^{2}]$ 等于输入二阶矩 $\\mathbb{E}[x_{i}^{2}]=q$。然后，为了与经典的 Glorot (Xavier) 初始化（其使用权重方差 $2/(fan_{in}+fan_{out})$ 来平衡线性或近似线性区域中的前向和后向信号幅度）进行比较，确定 $\\alpha$ 的值，使得您推导的 $\\sigma^{2}$ 等于 $2/(fan_{in}+fan_{out})$。\n\n将您的最终答案表示为关于 $fan_{in}$ 和 $fan_{out}$ 的单个闭式表达式。不涉及单位。不要四舍五入。", "solution": "任务是确定带泄露的修正线性单元 (Leaky ReLU) 激活函数的负斜率参数 $\\alpha$ 的一个特定值。该值是通过首先推导权重方差 $\\sigma^2$（该方差能保持信号通过一层后的二阶矩），然后将此方差与经典的 Glorot (Xavier) 初始化方差相等来求得的。\n\n过程分为两个主要部分：\n1.  推导权重方差 $\\sigma^2$ 的表达式，以确保输出二阶矩 $\\mathbb{E}[\\phi(z_j)^2]$ 等于输入二阶矩 $q$。\n2.  将推导出的 $\\sigma^2$ 与给定的 Glorot 方差 $\\frac{2}{fan_{in}+fan_{out}}$ 相等，并求解 $\\alpha$。\n\n首先，我们确定预激活值 $z_j = \\sum_{i=1}^{fan_{in}} W_{ji} x_{i}$ 的方差。输入 $\\{x_i\\}$ 和权重 $\\{W_{ji}\\}$ 是均值为零的独立随机变量，$\\mathbb{E}[x_i] = 0$ 且 $\\mathbb{E}[W_{ji}] = 0$。根据期望的线性性质，$z_j$ 的期望为：\n$$ \\mathbb{E}[z_j] = \\mathbb{E}\\left[\\sum_{i=1}^{fan_{in}} W_{ji} x_{i}\\right] = \\sum_{i=1}^{fan_{in}} \\mathbb{E}[W_{ji} x_{i}] $$\n由于权重和输入的独立性，$\\mathbb{E}[W_{ji} x_i] = \\mathbb{E}[W_{ji}]\\mathbb{E}[x_i] = 0 \\cdot 0 = 0$。因此，$\\mathbb{E}[z_j] = 0$。\n\n现在，我们计算 $z_j$ 的方差，记为 $\\mathrm{Var}(z_j)$。由于 $\\mathbb{E}[z_j] = 0$，方差等于二阶矩，即 $\\mathrm{Var}(z_j) = \\mathbb{E}[z_j^2]$。和式中的项 $W_{ji}x_i$ 对于不同的索引 $i$ 是相互独立的。使用独立和的方差可加性：\n$$ \\mathrm{Var}(z_j) = \\mathrm{Var}\\left(\\sum_{i=1}^{fan_{in}} W_{ji} x_{i}\\right) = \\sum_{i=1}^{fan_{in}} \\mathrm{Var}(W_{ji} x_{i}) $$\n两个独立的零均值随机变量乘积的方差为 $\\mathrm{Var}(W_{ji} x_i) = \\mathbb{E}[(W_{ji}x_i)^2] - (\\mathbb{E}[W_{ji}x_i])^2 = \\mathbb{E}[W_{ji}^2 x_i^2] - 0^2$。根据独立性，这变为 $\\mathbb{E}[W_{ji}^2]\\mathbb{E}[x_i^2]$。\n我们已知 $\\mathbb{E}[x_i^2] = q$。对于权重，由于 $\\mathbb{E}[W_{ji}]=0$，我们有 $\\mathbb{E}[W_{ji}^2] = \\mathrm{Var}(W_{ji}) + (\\mathbb{E}[W_{ji}])^2 = \\sigma^2 + 0^2 = \\sigma^2$。\n因此，$\\mathrm{Var}(W_{ji}x_i) = \\sigma^2 q$。\n将此代入 $\\mathrm{Var}(z_j)$ 的求和式中：\n$$ \\mathrm{Var}(z_j) = \\sum_{i=1}^{fan_{in}} \\sigma^2 q = fan_{in} \\sigma^2 q $$\n\n接下来，我们计算 Leaky ReLU 激活函数输出的二阶矩 $\\mathbb{E}[\\phi(z_j)^2]$。函数 $\\phi(z) = \\max(z,0)+\\alpha\\,\\min(z,0)$ 可以分段写成：当 $z > 0$ 时，$\\phi(z) = z$；当 $z \\le 0$ 时，$\\phi(z) = \\alpha z$。其平方为：\n$$ \\phi(z)^2 = \\begin{cases} z^2  \\text{if } z > 0 \\\\ \\alpha^2 z^2  \\text{if } z \\le 0 \\end{cases} $$\n通过对 $z_j$ 的概率密度函数 $p(z)$ 进行积分来求期望：\n$$ \\mathbb{E}[\\phi(z_j)^2] = \\int_{-\\infty}^{0} (\\alpha z)^2 p(z) dz + \\int_{0}^{\\infty} z^2 p(z) dz = \\alpha^2 \\int_{-\\infty}^{0} z^2 p(z) dz + \\int_{0}^{\\infty} z^2 p(z) dz $$\n题目说明 $z_j$ 的分布关于 $0$ 对称，这意味着 $p(z)$ 是一个偶函数。函数 $z^2 p(z)$ 也是偶函数，因此它在 $(-\\infty, 0]$ 上的积分等于它在 $[0, \\infty)$ 上的积分。这两者中的每一个都是总积分的一半：\n$$ \\int_{-\\infty}^{0} z^2 p(z) dz = \\int_{0}^{\\infty} z^2 p(z) dz = \\frac{1}{2} \\int_{-\\infty}^{\\infty} z^2 p(z) dz = \\frac{1}{2} \\mathbb{E}[z_j^2] = \\frac{1}{2} \\mathrm{Var}(z_j) $$\n将此代入 $\\mathbb{E}[\\phi(z_j)^2]$ 的表达式中：\n$$ \\mathbb{E}[\\phi(z_j)^2] = \\alpha^2 \\left(\\frac{\\mathrm{Var}(z_j)}{2}\\right) + \\left(\\frac{\\mathrm{Var}(z_j)}{2}\\right) = \\frac{1+\\alpha^2}{2} \\mathrm{Var}(z_j) $$\n代入 $\\mathrm{Var}(z_j) = fan_{in} \\sigma^2 q$：\n$$ \\mathbb{E}[\\phi(z_j)^2] = \\frac{1+\\alpha^2}{2} fan_{in} \\sigma^2 q $$\n我们要求这个输出二阶矩等于输入二阶矩 $q$：\n$$ \\frac{1+\\alpha^2}{2} fan_{in} \\sigma^2 q = q $$\n假设 $q \\neq 0$，我们可以解出 $\\sigma^2$：\n$$ \\sigma^2 = \\frac{2}{fan_{in}(1+\\alpha^2)} $$\n\n最后，我们将推导出的方差与 Glorot 方差 $\\sigma^2 = \\frac{2}{fan_{in}+fan_{out}}$ 相等，以求出 $\\alpha$ 的值。\n$$ \\frac{2}{fan_{in}(1+\\alpha^2)} = \\frac{2}{fan_{in}+fan_{out}} $$\n消去因子 $2$ 并对两边取倒数，得到：\n$$ fan_{in}(1+\\alpha^2) = fan_{in}+fan_{out} $$\n两边除以 $fan_{in}$：\n$$ 1+\\alpha^2 = \\frac{fan_{in}+fan_{out}}{fan_{in}} = 1 + \\frac{fan_{out}}{fan_{in}} $$\n两边减去 $1$：\n$$ \\alpha^2 = \\frac{fan_{out}}{fan_{in}} $$\n考虑到约束条件 $\\alpha \\in(0,1)$，我们必须取正平方根：\n$$ \\alpha = \\sqrt{\\frac{fan_{out}}{fan_{in}}} $$\n这就是所要求的 $\\alpha$ 的表达式。条件 $\\alpha  1$ 意味着这种特定的等价关系仅在 $fan_{out}  fan_{in}$ 的网络层中成立。", "answer": "$$\\boxed{\\sqrt{\\frac{fan_{out}}{fan_{in}}}}$$", "id": "3200172"}]}