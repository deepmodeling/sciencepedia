{"hands_on_practices": [{"introduction": "现代深度学习中，像 Adam 这样的自适应优化器无处不在。然而，尽管它们被广泛使用，其内部复杂的更新机制——包括梯度裁剪、动量估计和偏差校正——对许多学习者来说仍像一个“黑箱”。本练习旨在通过动手实践揭开 Adam 优化器的神秘面纱。通过亲手实现单步更新过程，你将深入理解各个组件如何协同作用，从而更好地进行模型调试与超参数调优 [@problem_id:3135417]。", "problem": "你被要求通过有效步长归一化因子的视角，来形式化并量化 Adam 优化器中全局范数梯度裁剪与自适应矩估计之间的相互作用。从基于梯度的优化、指数移动平均和全局范数裁剪这些基础且经过广泛检验的定义出发。\n\n给定一个深度学习模型中的参数向量及其梯度，将梯度的全局范数裁剪定义如下。对于一个梯度向量 $\\mathbf{g} \\in \\mathbb{R}^{d}$ 和一个裁剪阈值 $c \\in \\mathbb{R}_{>0}$，裁剪后的梯度 $\\tilde{\\mathbf{g}}$ 为\n$$\n\\tilde{\\mathbf{g}} \\;=\\; \\mathbf{g} \\cdot \\min\\!\\left(1, \\frac{c}{\\lVert \\mathbf{g} \\rVert_{2}}\\right),\n$$\n约定如果 $\\lVert \\mathbf{g} \\rVert_{2} = 0$ 则 $\\tilde{\\mathbf{g}} = \\mathbf{0}$。\n\n在 Adam 优化器中，定义在时间步 $t \\in \\mathbb{N}$ 时，带有超参数 $\\beta_{1} \\in [0,1)$、$\\beta_{2} \\in [0,1)$ 的一阶矩和二阶矩的指数移动平均：\n- 一阶矩估计 $m_{t}$ 是 $\\tilde{\\mathbf{g}}$ 的指数移动平均，衰减率为 $\\beta_{1}$。\n- 二阶矩估计 $v_{t}$ 是 $\\tilde{\\mathbf{g}}$ 逐元素平方的指数移动平均，衰减率为 $\\beta_{2}$。\n使用由 $t$、$\\beta_{1}$ 和 $\\beta_{2}$ 决定的标准偏差校正因子，实现偏差校正以获得 $\\hat{m}_{t}$ 和 $\\hat{v}_{t}$。逐元素的参数更新是通过将 $\\hat{m}_{t}$ 除以由 $\\varepsilon \\in \\mathbb{R}_{>0}$ 正则化的 $\\hat{v}_{t}$ 的平方根进行归一化，然后乘以学习率 $\\alpha \\in \\mathbb{R}_{>0}$ 来构成的。更新向量记为 $\\Delta \\theta \\in \\mathbb{R}^{d}$。\n\n定义相对于在裁剪梯度上进行普通随机梯度下降步长的有效步长归一化因子为\n$$\ns_{\\text{eff}} \\;=\\; \n\\begin{cases}\n\\dfrac{\\lVert \\Delta \\theta \\rVert_{2}}{\\alpha \\,\\lVert \\tilde{\\mathbf{g}} \\rVert_{2}}, & \\text{若 } \\lVert \\tilde{\\mathbf{g}} \\rVert_{2} > 0 \\\\\n0, & \\text{若 } \\lVert \\tilde{\\mathbf{g}} \\rVert_{2} = 0\n\\end{cases}\n$$\n以及，类似地，定义相对于未裁剪梯度的有效因子为\n$$\nr_{\\text{unclipped}} \\;=\\;\n\\begin{cases}\n\\dfrac{\\lVert \\Delta \\theta \\rVert_{2}}{\\alpha \\,\\lVert \\mathbf{g} \\rVert_{2}}, & \\text{若 } \\lVert \\mathbf{g} \\rVert_{2} > 0 \\\\\n0, & \\text{若 } \\lVert \\mathbf{g} \\rVert_{2} = 0\n\\end{cases}\n$$\n同时定义裁剪指示器\n$$\n\\text{clipped} \\;=\\; \\big(\\lVert \\mathbf{g} \\rVert_{2} > c\\big).\n$$\n\n你的任务是：\n- 从上述基本定义出发，在给定输入 $\\mathbf{g}$、$m_{t-1}$、$v_{t-1}$、$t$ 以及超参数 $\\alpha$、$\\beta_{1}$、$\\beta_{2}$、$\\varepsilon$ 和 $c$ 的情况下，推导单步 Adam 更新 $\\Delta \\theta$。\n- 为下面的每个测试用例计算 $s_{\\text{eff}}$、$r_{\\text{unclipped}}$ 和 $\\text{clipped}$。\n\n测试套件。对于每个用例，所有向量都在 $\\mathbb{R}^{3}$ 中，数值采用标准十进制或科学记数法表示：\n1. $d = 3$, $\\mathbf{g} = [\\,0.1,\\,-0.2,\\,0.05\\,]$, $m_{t-1} = [\\,0,\\,0,\\,0\\,]$, $v_{t-1} = [\\,0,\\,0,\\,0\\,]$, $t = 1$, $\\alpha = 0.01$, $\\beta_{1} = 0.9$, $\\beta_{2} = 0.999$, $\\varepsilon = 10^{-8}$, $c = 1.0$。\n2. $d = 3$, $\\mathbf{g} = [\\,10.0,\\,0.0,\\,0.0\\,]$, $m_{t-1} = [\\,0,\\,0,\\,0\\,]$, $v_{t-1} = [\\,0,\\,0,\\,0\\,]$, $t = 1$, $\\alpha = 0.001$, $\\beta_{1} = 0.9$, $\\beta_{2} = 0.999$, $\\varepsilon = 10^{-8}$, $c = 1.0$。\n3. $d = 3$, $\\mathbf{g} = [\\,0.5,\\,0.5,\\,0.5\\,]$, $m_{t-1} = [\\,0.1,\\,-0.1,\\,0.0\\,]$, $v_{t-1} = [\\,0.04,\\,0.01,\\,0.09\\,]$, $t = 10$, $\\alpha = 0.005$, $\\beta_{1} = 0.8$, $\\beta_{2} = 0.9$, $\\varepsilon = 10^{-6}$, $c = 0.5$。\n4. $d = 3$, $\\mathbf{g} = [\\,0.0,\\,0.0,\\,0.0\\,]$, $m_{t-1} = [\\,0.01,\\,-0.02,\\,0.0\\,]$, $v_{t-1} = [\\,0.001,\\,0.004,\\,0.0009\\,]$, $t = 5$, $\\alpha = 0.01$, $\\beta_{1} = 0.9$, $\\beta_{2} = 0.99$, $\\varepsilon = 10^{-8}$, $c = 0.1$。\n5. $d = 3$, $\\mathbf{g} = [\\,10^{-4},\\,-2\\cdot 10^{-4},\\,3\\cdot 10^{-4}\\,]$, $m_{t-1} = [\\,0,\\,0,\\,0\\,]$, $v_{t-1} = [\\,0,\\,0,\\,0\\,]$, $t = 1$, $\\alpha = 10^{-2}$, $\\beta_{1} = 0.0$, $\\beta_{2} = 0.0$, $\\varepsilon = 10^{-2}$, $c = 10^{-3}$。\n\n你的程序应生成单行输出，其中包含一个用方括号括起来的、逗号分隔的结果列表。每个测试用例的结果必须是 $[\\,s_{\\text{eff}},\\,r_{\\text{unclipped}},\\,\\text{clipped}\\,]$ 形式的列表，并按此顺序排列。因此，最终输出必须看起来像 [[case1_values],[case2_values],...,[case5_values]]，该行中任何地方都不能有空格。所有浮点值必须以标准十进制或科学记数法打印，其格式由典型编程语言的默认转换产生，布尔值必须是 True 或 False。本问题不需要用户输入，也不涉及任何物理单位或角度度量。所有计算必须严格按照上述规定执行，并且比率必须使用在 $s_{\\text{eff}}$ 和 $r_{\\text{unclipped}}$ 定义中为零分母指定的约定。", "solution": "该问题是有效的，因为它具有科学依据、问题设定良好、客观，并为得出唯一解提供了所有必要信息。它基于深度学习优化领域的标准、可形式化的定义。\n\n任务是为几个测试用例计算三个量：相对于裁剪梯度的有效步长归一化因子（$s_{\\text{eff}}$）、相对于未裁剪梯度的有效步长归一化因子（$r_{\\text{unclipped}}$）和一个布尔裁剪指示器（$\\text{clipped}$）。这需要实现带有梯度裁剪的 Adam 优化器的单步计算。步骤如下：\n\n### 步骤 1：梯度裁剪\n首先，我们计算原始梯度向量 $\\mathbf{g} \\in \\mathbb{R}^{d}$ 的 L$2$-范数，记为 $\\lVert \\mathbf{g} \\rVert_{2}$。\n$$\n\\lVert \\mathbf{g} \\rVert_{2} = \\sqrt{\\sum_{i=1}^{d} g_i^2}\n$$\n裁剪指示器 $\\text{clipped}$ 是一个布尔值，通过将此范数与裁剪阈值 $c \\in \\mathbb{R}_{>0}$ 进行比较来确定：\n$$\n\\text{clipped} = (\\lVert \\mathbf{g} \\rVert_{2} > c)\n$$\n然后对梯度 $\\mathbf{g}$ 进行裁剪以生成 $\\tilde{\\mathbf{g}}$。如果 $\\mathbf{g}$ 的范数超过 $c$，梯度向量将被缩放，使其范数为 $c$。否则，它保持不变。这由以下公式表示：\n$$\n\\tilde{\\mathbf{g}} = \\mathbf{g} \\cdot \\min\\left(1, \\frac{c}{\\lVert \\mathbf{g} \\rVert_{2}}\\right)\n$$\n对于零梯度有一个特殊约定：如果 $\\lVert \\mathbf{g} \\rVert_{2} = 0$，则 $\\tilde{\\mathbf{g}} = \\mathbf{0}$。如果我们定义当 $\\lVert \\mathbf{g} \\rVert_{2}=0$ 时缩放因子为 $1$ 以避免除以零，那么裁剪公式的乘法性质可以自然地处理这种情况，因为 $\\mathbf{g}$ 本身就是零向量。\n\n### 步骤 2：Adam 矩更新\nAdam 优化器维护梯度（一阶矩）及其平方（二阶矩）的指数移动平均值。给定在时间步 $t-1$ 的先前矩估计 $m_{t-1}$ 和 $v_{t-1}$，以及衰减率 $\\beta_1, \\beta_2 \\in [0, 1)$，使用裁剪后的梯度 $\\tilde{\\mathbf{g}}$ 计算在时间步 $t$ 的新估计。\n\n一阶矩估计 $m_t$ 更新如下：\n$$\nm_t = \\beta_1 m_{t-1} + (1 - \\beta_1) \\tilde{\\mathbf{g}}\n$$\n二阶矩估计 $v_t$ 使用裁剪梯度的逐元素平方 $\\tilde{\\mathbf{g}}^2$ 进行更新：\n$$\nv_t = \\beta_2 v_{t-1} + (1 - \\beta_2) \\tilde{\\mathbf{g}}^2\n$$\n\n### 步骤 3：偏差校正\n初始矩估计通常初始化为零向量。这会引入一个趋向于零的偏差，尤其是在优化的初始阶段。Adam 通过将矩估计除以依赖于时间步 $t$ 和衰减率的偏差校正因子来校正这种偏差。\n\n偏差校正后的一阶矩估计 $\\hat{m}_t$ 是：\n$$\n\\hat{m}_t = \\frac{m_t}{1 - \\beta_1^t}\n$$\n偏差校正后的二阶矩估计 $\\hat{v}_t$ 是：\n$$\n\\hat{v}_t = \\frac{v_t}{1 - \\beta_2^t}\n$$\n\n### 步骤 4：参数更新向量\n参数更新向量 $\\Delta\\theta$ 是使用偏差校正后的矩估计计算的。一阶矩估计 $\\hat{m}_t$ 作为更新方向，并通过二阶矩估计 $\\hat{v}_t$ 的平方根进行逐元素归一化。为了数值稳定性，在分母中添加了一个小常数 $\\varepsilon \\in \\mathbb{R}_{>0}$。结果乘以学习率 $\\alpha \\in \\mathbb{R}_{>0}$。\n$$\n\\Delta\\theta = \\alpha \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\varepsilon}\n$$\n请注意，通常参数更新规则是 $\\theta_t = \\theta_{t-1} - \\text{步长}$。这里要求的向量 $\\Delta\\theta$ 只是更新量本身，并且由于我们只关心它的范数 $\\lVert \\Delta\\theta \\rVert_{2}$，所以符号是无关紧要的。\n\n### 步骤 5：最终度量计算\n计算出参数更新向量 $\\Delta\\theta$ 后，我们现在可以计算所需的度量。\n\n计算更新向量的 L$2$-范数 $\\lVert \\Delta\\theta \\rVert_2$ 和裁剪梯度的 L$2$-范数 $\\lVert \\tilde{\\mathbf{g}} \\rVert_2$。\n\n相对于裁剪梯度的有效步长归一化因子 $s_{\\text{eff}}$ 定义为：\n$$\ns_{\\text{eff}} = \n\\begin{cases}\n\\dfrac{\\lVert \\Delta \\theta \\rVert_{2}}{\\alpha \\,\\lVert \\tilde{\\mathbf{g}} \\rVert_{2}},  & \\text{若 } \\lVert \\tilde{\\mathbf{g}} \\rVert_{2} > 0 \\\\\n0,  & \\text{若 } \\lVert \\tilde{\\mathbf{g}} \\rVert_{2} = 0\n\\end{cases}\n$$\n相对于未裁剪梯度的有效因子 $r_{\\text{unclipped}}$ 定义为：\n$$\nr_{\\text{unclipped}} = \n\\begin{cases}\n\\dfrac{\\lVert \\Delta \\theta \\rVert_{2}}{\\alpha \\,\\lVert \\mathbf{g} \\rVert_{2}},  & \\text{若 } \\lVert \\mathbf{g} \\rVert_{2} > 0 \\\\\n0,  & \\text{若 } \\lVert \\mathbf{g} \\rVert_{2} = 0\n\\end{cases}\n$$\n将这些步骤应用于每个测试用例以获得最终结果。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes Adam update metrics for a series of test cases.\n    \"\"\"\n    test_cases = [\n        # 1. d=3, g=[0.1,-0.2,0.05], m_prev=[0,0,0], v_prev=[0,0,0], t=1, alpha=0.01, beta1=0.9, beta2=0.999, eps=1e-8, c=1.0\n        {'g': np.array([0.1, -0.2, 0.05]), 'm_prev': np.array([0.0, 0.0, 0.0]), 'v_prev': np.array([0.0, 0.0, 0.0]), 't': 1, 'alpha': 0.01, 'beta1': 0.9, 'beta2': 0.999, 'epsilon': 1e-8, 'c': 1.0},\n        # 2. d=3, g=[10.0,0.0,0.0], m_prev=[0,0,0], v_prev=[0,0,0], t=1, alpha=0.001, beta1=0.9, beta2=0.999, eps=1e-8, c=1.0\n        {'g': np.array([10.0, 0.0, 0.0]), 'm_prev': np.array([0.0, 0.0, 0.0]), 'v_prev': np.array([0.0, 0.0, 0.0]), 't': 1, 'alpha': 0.001, 'beta1': 0.9, 'beta2': 0.999, 'epsilon': 1e-8, 'c': 1.0},\n        # 3. d=3, g=[0.5,0.5,0.5], m_prev=[0.1,-0.1,0.0], v_prev=[0.04,0.01,0.09], t=10, alpha=0.005, beta1=0.8, beta2=0.9, eps=1e-6, c=0.5\n        {'g': np.array([0.5, 0.5, 0.5]), 'm_prev': np.array([0.1, -0.1, 0.0]), 'v_prev': np.array([0.04, 0.01, 0.09]), 't': 10, 'alpha': 0.005, 'beta1': 0.8, 'beta2': 0.9, 'epsilon': 1e-6, 'c': 0.5},\n        # 4. d=3, g=[0,0,0], m_prev=[0.01,-0.02,0], v_prev=[0.001,0.004,0.0009], t=5, alpha=0.01, beta1=0.9, beta2=0.99, eps=1e-8, c=0.1\n        {'g': np.array([0.0, 0.0, 0.0]), 'm_prev': np.array([0.01, -0.02, 0.0]), 'v_prev': np.array([0.001, 0.004, 0.0009]), 't': 5, 'alpha': 0.01, 'beta1': 0.9, 'beta2': 0.99, 'epsilon': 1e-8, 'c': 0.1},\n        # 5. d=3, g=[1e-4, -2e-4, 3e-4], m_prev=[0,0,0], v_prev=[0,0,0], t=1, alpha=1e-2, beta1=0.0, beta2=0.0, eps=1e-2, c=1e-3\n        {'g': np.array([1e-4, -2e-4, 3e-4]), 'm_prev': np.array([0.0, 0.0, 0.0]), 'v_prev': np.array([0.0, 0.0, 0.0]), 't': 1, 'alpha': 1e-2, 'beta1': 0.0, 'beta2': 0.0, 'epsilon': 1e-2, 'c': 1e-3},\n    ]\n\n    all_results = []\n\n    for case in test_cases:\n        g, m_prev, v_prev = case['g'], case['m_prev'], case['v_prev']\n        t, alpha, beta1, beta2, epsilon, c = case['t'], case['alpha'], case['beta1'], case['beta2'], case['epsilon'], case['c']\n\n        # Step 1: Gradient Clipping\n        g_norm = np.linalg.norm(g)\n        \n        clipped = bool(g_norm > c)\n\n        if g_norm > 0:\n            clip_factor = min(1.0, c / g_norm)\n            g_tilde = g * clip_factor\n        else:\n            g_tilde = np.zeros_like(g)\n\n        # Step 2: Adam Moment Updates\n        m_t = beta1 * m_prev + (1 - beta1) * g_tilde\n        v_t = beta2 * v_prev + (1 - beta2) * (g_tilde ** 2)\n\n        # Step 3: Bias Correction\n        m_hat = m_t / (1 - beta1**t)\n        v_hat = v_t / (1 - beta2**t)\n        \n        # Step 4: Parameter Update Vector\n        delta_theta = alpha * m_hat / (np.sqrt(v_hat) + epsilon)\n        \n        # Step 5: Final Metric Calculation\n        delta_theta_norm = np.linalg.norm(delta_theta)\n        g_tilde_norm = np.linalg.norm(g_tilde)\n\n        if g_tilde_norm > 0:\n            s_eff = delta_theta_norm / (alpha * g_tilde_norm)\n        else:\n            s_eff = 0.0\n\n        if g_norm > 0:\n            r_unclipped = delta_theta_norm / (alpha * g_norm)\n        else:\n            r_unclipped = 0.0\n            \n        all_results.append([s_eff, r_unclipped, clipped])\n\n    # Format the final output string exactly as required\n    result_strings = []\n    for res in all_results:\n        # Convert each item in the sublist to its string representation\n        # str(True) is 'True', str(False) is 'False', which is correct\n        s = '[' + ','.join(map(str, res)) + ']'\n        result_strings.append(s)\n    \n    final_output = '[' + ','.join(result_strings) + ']'\n    print(final_output)\n\nsolve()\n```", "id": "3135417"}, {"introduction": "优化器并非孤立工作，它总是与权重衰减 (weight decay) 等正则化技术协同作用。一个常见且关键的实践问题是：当我们调整学习率 $\\alpha$ 时，应该如何相应地调整权重衰减系数 $\\lambda$？本练习将从第一性原理出发，推导出一个简单而有效的缩放法则来解答这个问题。通过这个实践，你不仅能掌握一个实用的调优技巧，还能深刻理解传统 L2 正则化与 AdamW 等现代优化器中“解耦权重衰减”的本质区别 [@problem_id:3135392]。", "problem": "要求您对深度学习模型中基于梯度的优化里的权重衰减，形式化并测试一个超参数缩放启发式方法。目标是在学习率变化时，保持有效的单步正则化强度恒定。请从基于梯度的更新和欧几里得范数正则化的核心定义出发，避免使用任何直接跳到结果的快捷公式。\n\n令 $w_t \\in \\mathbb{R}$ 表示在步骤 $t$ 的单个标量参数，令 $\\alpha \\in \\mathbb{R}_{\\ge 0}$ 表示学习率，令 $\\lambda \\in \\mathbb{R}_{\\ge 0}$ 表示权重衰减或欧几里得范数（L2）正则化系数，并令 $g_t \\in \\mathbb{R}$ 表示在没有施加正则化时，损失函数关于 $w_t$ 的数据依赖梯度。假设使用以下经过充分测试的更新规则之一：\n- 不带动量的随机梯度下降（SGD）：$w_{t+1} = w_t - \\alpha \\nabla_w \\mathcal{L}(w_t)$，其中欧几里得范数（L2）正则化项包含在损失函数中，因此梯度包含一个依赖于 $\\lambda$ 和 $w_t$ 的可加项。\n- 随机梯度下降（SGD）的动量变体：使用一个速度 $v_t \\in \\mathbb{R}$，并通过一个系数为 $\\beta \\in [0,1)$ 的线性递推关系进行更新，同时对欧几里得范数（L2）正则化采用与上述相同的处理方式。\n- 使用解耦权重衰减的自适应矩估计（AdamW）：数据梯度的一阶矩和二阶矩通过系数 $\\beta_1, \\beta_2 \\in [0,1)$ 进行维护，欧几里得范数（L2）惩罚项作为一个与数据梯度路径明确分离的乘性衰减来应用。\n\n对于给定的优化器、学习率和权重衰减，在数据驱动梯度不存在的状态下，将每步的有效正则化定义为乘性收缩因子\n$$\ns(\\alpha, \\lambda; w_t) \\equiv \\frac{|w_{t+1}|}{|w_t|},\n$$\n当更新在 $g_t = 0$ 且任何内部状态（如速度或矩）在步骤 $t$ 被初始化为零的情况下执行时。该定义隔离了正则化机制在单步上的影响，并忽略了任何数据驱动的变化。\n\n任务 A（推导）：仅使用所描述的更新规则以及欧几里得范数（L2）正则化和解耦权重衰减的定义，推导一个关联 $\\lambda$ 和 $\\alpha$ 的条件，以在 $\\alpha$ 变化时保持 $s(\\alpha, \\lambda; w_t)$ 不变。您的推导必须基于基本的单步更新动态，并且不应依赖任何未加说明的恒等式。\n\n任务 B（程序）：实现一个程序，在给定基准 $(\\alpha_0, \\lambda_0)$ 的情况下，评估当 $\\alpha$ 变化时 $\\lambda$ 的两种候选缩放定律：\n1. 正比缩放：$\\lambda_{\\mathrm{prop}}(\\alpha) = \\lambda_0 \\cdot \\frac{\\alpha}{\\alpha_0}$。\n2. 反比缩放：$\\lambda_{\\mathrm{inv}}(\\alpha) = \\lambda_0 \\cdot \\frac{\\alpha_0}{\\alpha}$。\n\n对每个测试用例，执行以下操作：\n1. 在给定的非零初始权重 $w_0$、 $g_t=0$ 且任何内部优化器状态初始化为零的条件下，计算基准收缩率 $s_0 = s(\\alpha_0, \\lambda_0; w_0)$。\n2. 对于用例的学习率列表中的每个 $\\alpha$，在相同的初始条件下，使用给定优化器的精确单步更新规则，计算 $s_{\\mathrm{prop}}(\\alpha) = s(\\alpha, \\lambda_{\\mathrm{prop}}(\\alpha); w_0)$ 和 $s_{\\mathrm{inv}}(\\alpha) = s(\\alpha, \\lambda_{\\mathrm{inv}}(\\alpha); w_0)$。\n3. 每个测试用例返回两个布尔值：\n   - 如果对于列表中的所有 $\\alpha$，都有 $|s_{\\mathrm{inv}}(\\alpha) - s_0| \\le \\varepsilon$，则第一个布尔值为真。\n   - 如果对于列表中的所有 $\\alpha$，都有 $|s_{\\mathrm{prop}}(\\alpha) - s_0| \\le \\varepsilon$，则第二个布尔值为真。\n此处 $\\varepsilon > 0$ 是用于数值比较的给定容差。\n\n您必须实现的测试套件包括以下五个用例，每个用例提供一个基准 $(\\alpha_0, \\lambda_0)$、一个非零初始权重 $w_0$、一组要测试的学习率，以及一个优化器及其系数。在所有用例中，都以 $g_t = 0$ 和步骤 $t$ 的内部状态为零来评估单次更新。\n- 用例 1（理想情况，耦合正则化在损失函数中）：优化器为不带动量的 SGD，基准 $\\alpha_0 = 0.1$, $\\lambda_0 = 0.01$, $w_0 = 1.0$，测试学习率 $\\{0.05, 0.1, 0.2\\}$，容差 $\\varepsilon = 10^{-12}$。\n- 用例 2（动量，耦合正则化在损失函数中）：优化器为带 $\\beta = 0.9$ 的动量 SGD，基准 $\\alpha_0 = 0.1$, $\\lambda_0 = 0.01$, $w_0 = 1.0$，测试学习率 $\\{0.05, 0.1, 0.2\\}$，容差 $\\varepsilon = 10^{-12}$。\n- 用例 3（解耦权重衰减）：优化器为 AdamW，其中 $\\beta_1 = 0.9$, $\\beta_2 = 0.999$, $\\varepsilon_{\\mathrm{adam}} = 10^{-8}$，基准 $\\alpha_0 = 0.001$, $\\lambda_0 = 0.01$, $w_0 = 1.0$，测试学习率 $\\{0.0005, 0.001, 0.002\\}$，容差 $\\varepsilon = 10^{-12}$。\n- 用例 4（边界情况：零权重衰减）：优化器为不带动量的 SGD，基准 $\\alpha_0 = 0.05$, $\\lambda_0 = 0.0$, $w_0 = 1.0$，测试学习率 $\\{0.01, 0.05, 0.1\\}$，容差 $\\varepsilon = 10^{-12}$。\n- 用例 5（跨尺度覆盖）：优化器为 AdamW，其中 $\\beta_1 = 0.9$, $\\beta_2 = 0.999$, $\\varepsilon_{\\mathrm{adam}} = 10^{-8}$，基准 $\\alpha_0 = 10^{-2}$, $\\lambda_0 = 5 \\cdot 10^{-3}$, $w_0 = 1.0$，测试学习率 $\\{10^{-4}, 10^{-3}, 10^{-2}, 10^{-1}\\}$，容差 $\\varepsilon = 10^{-12}$。\n\n必需的最终输出格式：您的程序应生成一行输出，其中包含一个扁平的 Python 布尔值列表，每个测试用例恰好有两个布尔值，并按上述用例的顺序排列。对于每个用例，首先附加反比缩放的布尔值，然后附加正比缩放的布尔值。例如，一个包含五个用例的运行必须以“[b1_inv,b1_prop,b2_inv,b2_prop,b3_inv,b3_prop,b4_inv,b4_prop,b5_inv,b5_prop]”的确切格式打印单行。", "solution": "任务是推导一个用于权重衰减的超参数缩放启发式方法，然后通过编程来测试它。目标是在学习率变化时，保持有效的正则化强度恒定。单步的有效正则化由乘性收缩因子 $s(\\alpha, \\lambda; w_t) \\equiv \\frac{|w_{t+1}|}{|w_t|}$ 定义，该因子在零数据依赖梯度（$g_t=0$）和步骤 $t$ 的内部优化器状态为零的特定条件下计算。令 $w_t \\in \\mathbb{R}$ 为一个标量权重，$\\alpha \\in \\mathbb{R}_{\\ge 0}$ 为学习率，$\\lambda \\in \\mathbb{R}_{\\ge 0}$ 为权重衰减系数。我们假设 $w_t \\neq 0$。\n\n**任务 A：不变性条件的推导**\n\n我们将分析每种指定优化器在给定条件（$g_t=0$，零初始状态）下的单步更新规则，以推导出保持 $s(\\alpha, \\lambda; w_t)$ 恒定所需的 $\\alpha$ 和 $\\lambda$ 之间的关系。\n\n**1. 不带动量的随机梯度下降（SGD）**\n\n包含欧几里得范数（L2）正则化的总损失函数是 $\\mathcal{L}_{\\text{total}}(w_t) = \\mathcal{L}(w_t) + \\frac{\\lambda}{2} w_t^2$。关于 $w_t$ 的梯度是 $\\nabla_w \\mathcal{L}_{\\text{total}}(w_t) = \\nabla_w \\mathcal{L}(w_t) + \\lambda w_t$。令 $g_t = \\nabla_w \\mathcal{L}(w_t)$ 为梯度的依赖数据部分。那么完整的梯度是 $g_t + \\lambda w_t$。\n\nSGD 更新规则是：\n$$w_{t+1} = w_t - \\alpha \\nabla_w \\mathcal{L}_{\\text{total}}(w_t) = w_t - \\alpha (g_t + \\lambda w_t)$$\n在 $g_t=0$ 的条件下，更新简化为：\n$$w_{t+1} = w_t - \\alpha (\\lambda w_t) = (1 - \\alpha \\lambda) w_t$$\n因此，收缩因子 $s$ 是：\n$$s(\\alpha, \\lambda; w_t) = \\frac{|w_{t+1}|}{|w_t|} = \\frac{|(1 - \\alpha \\lambda) w_t|}{|w_t|} = |1 - \\alpha \\lambda|$$\n为了使 $s(\\alpha, \\lambda; w_t)$ 对 $\\alpha$ 的变化保持不变， $|1 - \\alpha \\lambda|$ 的值必须保持恒定。假设更新是稳定的（即 $1 - \\alpha \\lambda \\ge 0$），这意味着乘积 $\\alpha \\lambda$ 必须是常数。\n$$\\alpha \\lambda = C$$\n其中 $C$ 是一个常数。给定一个基准对 $(\\alpha_0, \\lambda_0)$，该常数为 $C = \\alpha_0 \\lambda_0$。因此，对于任何新的学习率 $\\alpha$，对应的权重衰减 $\\lambda$ 必须满足 $\\lambda = \\frac{\\alpha_0 \\lambda_0}{\\alpha}$。这就是反比缩放定律。\n\n**2. 动量 SGD**\n\n该优化器维护一个速度向量 $v_t$。更新规则针对总梯度 $\\nabla_w \\mathcal{L}_{\\text{total}}(w_t) = g_t + \\lambda w_t$ 给出：\n$$v_{t+1} = \\beta v_t + (g_t + \\lambda w_t)$$\n$$w_{t+1} = w_t - \\alpha v_{t+1}$$\n其中 $\\beta \\in [0, 1)$ 是动量系数。\n我们应用条件：$g_t=0$ 并且步骤 $t$ 的内部状态为零，即 $v_t=0$。\n速度更新变为：\n$$v_{t+1} = \\beta(0) + (0 + \\lambda w_t) = \\lambda w_t$$\n将此代入参数更新规则：\n$$w_{t+1} = w_t - \\alpha (\\lambda w_t) = (1 - \\alpha \\lambda) w_t$$\n这与 SGD 的情况相同。收缩因子同样是 $s(\\alpha, \\lambda; w_t) = |1 - \\alpha \\lambda|$，不变性条件是 $\\alpha \\lambda = \\text{常数}$。这再次导出了反比缩放定律。\n\n**3. AdamW（解耦权重衰减）**\n\n在 AdamW 中，权重衰减与基于梯度的更新是“解耦”的。更新在概念上分两步执行：首先应用权重衰减，然后基于数据梯度 $g_t$ 应用 Adam 更新。一种常见的实现形式是：\n$$w_{t+1} = w_t (1 - \\alpha \\lambda) - \\alpha \\cdot \\text{AdamUpdate}(g_t, m_t, v_t)$$\n`AdamUpdate` 项依赖于矩估计：\n$$m_{t+1} = \\beta_1 m_t + (1-\\beta_1) g_t$$\n$$v_{t+1} = \\beta_2 v_t + (1-\\beta_2) g_t^2$$\n以及它们的偏差校正版本 $\\hat{m}_{t+1}$ 和 $\\hat{v}_{t+1}$。更新项与 $\\frac{\\hat{m}_{t+1}}{\\sqrt{\\hat{v}_{t+1}} + \\varepsilon_{\\mathrm{adam}}}$ 成正比。\n\n在给定条件下，$g_t=0$，并且步骤 $t$ 的内部状态为零（$m_t=0, v_t=0$）。\n矩更新得出：\n$$m_{t+1} = \\beta_1(0) + (1-\\beta_1)(0) = 0$$\n$$v_{t+1} = \\beta_2(0) + (1-\\beta_2)(0^2) = 0$$\n因此，它们的偏差校正版本也为零，$\\hat{m}_{t+1}=0$ 和 $\\hat{v}_{t+1}=0$。整个 `AdamUpdate` 项变为零：\n$$\\alpha \\cdot \\text{AdamUpdate}(g_t=0, m_t=0, v_t=0) = 0$$\nAdamW 更新规则简化为：\n$$w_{t+1} = w_t (1 - \\alpha \\lambda) - 0 = (1 - \\alpha \\lambda) w_t$$\n再次地，在指定条件下，该更新与前面的情况相同。收缩因子是 $s(\\alpha, \\lambda; w_t) = |1 - \\alpha \\lambda|$，不变性条件是 $\\alpha \\lambda = \\text{常数}$。\n\n**推导结论**\n\n对于所有三种优化器——带耦合正则化的 SGD、带耦合正则化的动量 SGD，以及带解耦权重衰减的 AdamW——有效的单步正则化收缩因子 $s(\\alpha, \\lambda; w_t)$ 对 $\\alpha$ 的变化保持不变，当且仅当乘积 $\\alpha \\lambda$ 保持恒定。这对应于**反比缩放定律**：$\\lambda(\\alpha) = \\lambda_0 \\cdot \\frac{\\alpha_0}{\\alpha}$。而正比缩放定律 $\\lambda(\\alpha) = \\lambda_0 \\cdot \\frac{\\alpha}{\\alpha_0}$ 会导致乘积 $\\alpha \\lambda$ 与 $\\alpha^2$ 成比例缩放，因此无法保持收缩因子恒定。\n\n以下程序将根据提供的测试用例，从数值上验证这一结论。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef update_sgd(w_t, alpha, lambda_val, **kwargs):\n    \"\"\"\n    Performs a single update step for SGD with coupled L2 regularization.\n    Conditions: g_t = 0, no internal state.\n    \"\"\"\n    g_t = 0.0\n    total_gradient = g_t + lambda_val * w_t\n    w_t_plus_1 = w_t - alpha * total_gradient\n    return w_t_plus_1\n\ndef update_momentum_sgd(w_t, alpha, lambda_val, beta, **kwargs):\n    \"\"\"\n    Performs a single update step for Momentum SGD with coupled L2 regularization.\n    Conditions: g_t = 0, velocity v_t = 0.\n    \"\"\"\n    g_t = 0.0\n    v_t = 0.0  # Zero initial state\n    total_gradient = g_t + lambda_val * w_t\n    v_t_plus_1 = beta * v_t + total_gradient\n    w_t_plus_1 = w_t - alpha * v_t_plus_1\n    return w_t_plus_1\n\ndef update_adamw(w_t, alpha, lambda_val, beta1, beta2, epsilon_adam, **kwargs):\n    \"\"\"\n    Performs a single update step for AdamW with decoupled weight decay.\n    Conditions: g_t = 0, moments m_t = 0, v_t = 0.\n    \"\"\"\n    g_t = 0.0\n    m_t = 0.0  # Zero initial state\n    v_t = 0.0  # Zero initial state\n    step = 0  # Simulation starts at step 0\n    step_plus_1 = step + 1\n\n    # Gradient processing part, which becomes zero under g_t=0 and zeroed state\n    m_t_plus_1 = beta1 * m_t + (1.0 - beta1) * g_t\n    v_t_plus_1 = beta2 * v_t + (1.0 - beta2) * g_t**2\n\n    # Bias correction. 1 - beta**(step+1) is safe since step+1=1.\n    m_hat = m_t_plus_1 / (1.0 - beta1**step_plus_1)\n    v_hat = v_t_plus_1 / (1.0 - beta2**step_plus_1)\n\n    grad_update_term = m_hat / (np.sqrt(v_hat) + epsilon_adam)\n\n    # Decoupled weight decay update\n    w_t_plus_1 = w_t * (1.0 - alpha * lambda_val) - alpha * grad_update_term\n    return w_t_plus_1\n\ndef solve():\n    \"\"\"\n    Solves the problem by running all test cases and printing the results.\n    \"\"\"\n    test_cases = [\n        # Case 1 (happy path, coupled regularization in loss)\n        {'optimizer': 'SGD', 'alpha0': 0.1, 'lambda0': 0.01, 'w0': 1.0, \n         'test_alphas': [0.05, 0.1, 0.2], 'epsilon': 1e-12, 'params': {}},\n        # Case 2 (momentum, coupled regularization in loss)\n        {'optimizer': 'MomentumSGD', 'alpha0': 0.1, 'lambda0': 0.01, 'w0': 1.0, \n         'test_alphas': [0.05, 0.1, 0.2], 'epsilon': 1e-12, 'params': {'beta': 0.9}},\n        # Case 3 (decoupled weight decay)\n        {'optimizer': 'AdamW', 'alpha0': 0.001, 'lambda0': 0.01, 'w0': 1.0, \n         'test_alphas': [0.0005, 0.001, 0.002], 'epsilon': 1e-12, \n         'params': {'beta1': 0.9, 'beta2': 0.999, 'epsilon_adam': 1e-8}},\n        # Case 4 (edge case: zero weight decay)\n        {'optimizer': 'SGD', 'alpha0': 0.05, 'lambda0': 0.0, 'w0': 1.0, \n         'test_alphas': [0.01, 0.05, 0.1], 'epsilon': 1e-12, 'params': {}},\n        # Case 5 (coverage across scales)\n        {'optimizer': 'AdamW', 'alpha0': 1e-2, 'lambda0': 5e-3, 'w0': 1.0, \n         'test_alphas': [1e-4, 1e-3, 1e-2, 1e-1], 'epsilon': 1e-12, \n         'params': {'beta1': 0.9, 'beta2': 0.999, 'epsilon_adam': 1e-8}}\n    ]\n    \n    optimizer_updates = {\n        'SGD': update_sgd,\n        'MomentumSGD': update_momentum_sgd,\n        'AdamW': update_adamw\n    }\n\n    final_results = []\n    for case in test_cases:\n        alpha0 = case['alpha0']\n        lambda0 = case['lambda0']\n        w0 = case['w0']\n        epsilon = case['epsilon']\n        update_func = optimizer_updates[case['optimizer']]\n\n        # 1. Compute baseline shrinkage s0\n        w1_base = update_func(w0, alpha0, lambda0, **case['params'])\n        if w0 == 0.0:\n            s0 = 0.0 if w1_base == 0.0 else float('inf')\n        else:\n            s0 = abs(w1_base / w0)\n\n        is_inv_stable = True\n        is_prop_stable = True\n\n        for alpha in case['test_alphas']:\n            # 2. Test inverse-proportional scaling\n            # Handle alpha=0 case, though not in test data.\n            lambda_inv = lambda0 * (alpha0 / alpha) if alpha != 0.0 else float('inf')\n            w1_inv = update_func(w0, alpha, lambda_inv, **case['params'])\n            s_inv = abs(w1_inv / w0) if w0 != 0 else (0.0 if w1_inv == 0.0 else float('inf'))\n            if abs(s_inv - s0) > epsilon:\n                is_inv_stable = False\n\n            # 3. Test proportional scaling\n            # Handle alpha0=0 case, though not in test data.\n            if alpha0 != 0.0:\n                lambda_prop = lambda0 * (alpha / alpha0)\n            else:\n                 lambda_prop = 0.0 if lambda0 == 0.0 else float('inf')\n            w1_prop = update_func(w0, alpha, lambda_prop, **case['params'])\n            s_prop = abs(w1_prop / w0) if w0 != 0.0 else (0.0 if w1_prop == 0.0 else float('inf'))\n            if abs(s_prop - s0) > epsilon:\n                is_prop_stable = False\n        \n        final_results.append(is_inv_stable)\n        final_results.append(is_prop_stable)\n    \n    # Format the final output string exactly as required.\n    # str(True) -> 'True', str(False) -> 'False'\n    print(f\"[{','.join(map(str, final_results))}]\")\n\nsolve()\n```", "id": "3135392"}, {"introduction": "在掌握了单步更新和核心超参数的互动之后，我们可以探索更高级的训练策略来进一步提升模型性能。随机权重平均 (Stochastic Weight Averaging, SWA) 就是一种简单而有效的技术，它通过在训练后期对模型权重进行平均，帮助模型找到更宽阔、更平坦的损失函数盆地，从而提升泛化能力。本练习将理论模型与实践相结合，引导你分析 SWA 的一个关键超参数——模型平均的频率 $F$，从而让你从更宏观的视角理解训练过程的动态性 [@problem_id:3135390]。", "problem": "您将研究如何为随机权重平均 (Stochastic Weight Averaging, SWA) 调整检查点平均频率和平滑权重。其形式化过程如下。考虑一个使用随机梯度下降训练的深度学习模型，该模型处于一个局部最小值附近，其损失可以进行二次近似。假设在检查点时刻的训练迭代值被建模为一个平稳标量过程 $\\{x_t\\}_{t \\ge 0}$，其均值为零，协方差函数为 $\\mathrm{Cov}(x_t, x_{t-k}) = \\sigma^2 \\rho^{|k|}$，其中 $\\sigma^2 > 0$ 和 $\\rho \\in [0,1)$ 分别表示平稳方差和连续检查点之间的几何自相关性。检查点每 $F \\in \\mathbb{N}$ 步保存一次，连续保存的检查点之间的相关性被建模为 $\\rho(F) = \\exp(-F / \\tau)$，其中 $\\tau > 0$ 是一个固定的相关时间尺度。您将使用指数移动平均 (Exponential Moving Average, EMA) 构建一个指数随机权重平均 (SWA) 估计量，其更新规则为 $y_t = (1 - \\alpha) y_{t-1} + \\alpha x_t$，其中 $\\alpha \\in (0,1]$ 是平均系数，且该更新仅在检查点时刻应用。$\\alpha = 1$ 的特殊情况对应于使用最新的检查点而不进行平均。在验证损失 $L(w)$ 于最小值点 $w^\\star$ 附近的局部二次近似 $L(w) \\approx L(w^\\star) + \\frac{1}{2} (w - w^\\star)^\\top H (w - w^\\star)$（其中 $H \\succ 0$）下，估计量的预期超额验证损失与其误差协方差的迹成正比。在一个标量简化模型中，使用一个有效曲率常数 $c > 0$（该常数吸收了因子 $\\frac{1}{2}$ 和一个有效曲率尺度），通过平均得到的预期验证改进（相对于使用单个检查点的基线）为\n$\\Delta(F,\\alpha) = c \\left( \\sigma^2 - \\mathrm{Var}(y_t) \\right)$，\n其中 $\\mathrm{Var}(y_t)$ 是 EMA 输出在检查点时刻的平稳方差。\n\n任务：\n1) 从给定的定义和以下基本事实出发：(i) 预期超额二次损失与方差成正比；(ii) EMA 是一个线性时不变滤波器，其脉冲响应权重为 $\\alpha (1-\\alpha)^k$（对于 $k \\in \\mathbb{N}\\cup\\{0\\}$）。仅使用协方差结构 $\\mathrm{Cov}(x_t, x_{t-k}) = \\sigma^2 \\rho^{|k|}$ 和平稳性条件下的 EMA 定义 $y_t = \\sum_{k=0}^{\\infty} \\alpha (1-\\alpha)^k x_{t-k}$，推导出 $\\mathrm{Var}(y_t)$ 关于 $\\sigma^2$、$\\alpha$ 和 $\\rho$ 的闭式表达式。然后通过代入 $\\rho = \\exp(-F/\\tau)$ 来表示 $\\Delta(F,\\alpha)$。\n2) 实现一个程序，使用推导出的闭式和固定常数 $c = 0.5$、$\\sigma^2 = 1.0$ 和 $\\tau = 20.0$，为下面测试套件中的每种情况计算 $\\Delta(F,\\alpha)$。所有数值常数都是无量纲的。将每个结果四舍五入到6位小数。\n\n测试套件（每种情况是一个按所列顺序进行评估的对 $(F,\\alpha)$）：\n- $(F,\\alpha) = (1, 0.1)$\n- $(F,\\alpha) = (10, 0.1)$\n- $(F,\\alpha) = (50, 0.1)$\n- $(F,\\alpha) = (15, 1.0)$\n- $(F,\\alpha) = (5, 0.05)$\n- $(F,\\alpha) = (5, 0.5)$\n- $(F,\\alpha) = (1, 0.9)$\n\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，列表中的每个值都四舍五入到6位小数，例如 $[v_1,v_2,\\dots,v_7]$，其中每个 $v_i$ 是一个浮点数。", "solution": "此问题被评估为有效，因为它具有科学依据、是适定的且客观的。它提供了一套自洽、一致的定义和约束，用于推导解决方案并执行所需的计算。\n\n主要任务是推导预期验证改进 $\\Delta(F,\\alpha)$ 的闭式表达式，然后为一组给定的参数计算其值。该改进定义为 $\\Delta(F,\\alpha) = c \\left( \\sigma^2 - \\mathrm{Var}(y_t) \\right)$，其中 $y_t$ 是随机权重平均 (SWA) 估计量。这要求我们首先找到过程 $y_t$ 的平稳方差。\n\n过程 $\\{x_t\\}_{t \\ge 0}$ 表示检查点时刻的模型迭代值。它是一个零均值的平稳标量过程，即 $\\mathbb{E}[x_t] = 0$，协方差函数为 $\\mathrm{Cov}(x_t, x_j) = \\sigma^2 \\rho^{|t-j|}$。因此，任何单个检查点迭代值的方差为 $\\mathrm{Var}(x_t) = \\mathrm{Cov}(x_t, x_t) = \\sigma^2 \\rho^0 = \\sigma^2$。\n\nSWA 估计量 $y_t$ 是通过对检查点迭代值进行指数移动平均 (EMA) 形成的：$y_t = (1 - \\alpha) y_{t-1} + \\alpha x_t$，其中 $\\alpha \\in (0,1]$。由于 $x_t$ 是一个零均值平稳过程，且 EMA 是一个线性时不变滤波器，因此输出过程 $y_t$ 也是一个零均值平稳过程，所以 $\\mathbb{E}[y_t] = 0$。因此 $y_t$ 的方差为 $\\mathrm{Var}(y_t) = \\mathbb{E}[y_t^2]$。\n\n为了求得 $\\mathrm{Var}(y_t)$，我们使用 $y_t$ 的递归定义。\n$$\ny_t^2 = \\left((1 - \\alpha) y_{t-1} + \\alpha x_t\\right)^2 = (1-\\alpha)^2 y_{t-1}^2 + \\alpha^2 x_t^2 + 2\\alpha(1-\\alpha) y_{t-1} x_t\n$$\n对两边取期望：\n$$\n\\mathbb{E}[y_t^2] = (1-\\alpha)^2 \\mathbb{E}[y_{t-1}^2] + \\alpha^2 \\mathbb{E}[x_t^2] + 2\\alpha(1-\\alpha) \\mathbb{E}[y_{t-1} x_t]\n$$\n由于平稳性，$\\mathrm{Var}(y_t) = \\mathbb{E}[y_t^2] = \\mathbb{E}[y_{t-1}^2]$。我们将这个平稳方差记为 $V_y$。此外，$\\mathbb{E}[x_t^2] = \\mathrm{Var}(x_t) = \\sigma^2$。方程变为：\n$$\nV_y = (1-\\alpha)^2 V_y + \\alpha^2 \\sigma^2 + 2\\alpha(1-\\alpha) \\mathbb{E}[y_{t-1} x_t]\n$$\n我们需要计算互相关项 $\\mathbb{E}[y_{t-1} x_t]$。我们使用 EMA 的无穷级数表示形式 $y_{t-1} = \\sum_{k=0}^{\\infty} \\alpha (1-\\alpha)^k x_{t-1-k}$。\n$$\n\\mathbb{E}[y_{t-1} x_t] = \\mathbb{E}\\left[ \\left( \\sum_{k=0}^{\\infty} \\alpha (1-\\alpha)^k x_{t-1-k} \\right) x_t \\right]\n$$\n根据期望的线性性质：\n$$\n\\mathbb{E}[y_{t-1} x_t] = \\sum_{k=0}^{\\infty} \\alpha (1-\\alpha)^k \\mathbb{E}[x_{t-1-k} x_t]\n$$\n由于 $\\mathbb{E}[x_t]=0$，则 $\\mathbb{E}[x_{t-1-k} x_t] = \\mathrm{Cov}(x_{t-1-k}, x_t) = \\sigma^2 \\rho^{|t - (t-1-k)|} = \\sigma^2 \\rho^{|k+1|} = \\sigma^2 \\rho^{k+1}$，因为 $k \\ge 0$。\n$$\n\\mathbb{E}[y_{t-1} x_t] = \\sum_{k=0}^{\\infty} \\alpha (1-\\alpha)^k \\sigma^2 \\rho^{k+1} = \\alpha \\sigma^2 \\rho \\sum_{k=0}^{\\infty} \\left((1-\\alpha)\\rho\\right)^k\n$$\n这是一个公比为 $(1-\\alpha)\\rho$ 的几何级数。由于 $\\rho \\in [0,1)$ 且 $\\alpha \\in (0,1]$，我们有 $|(1-\\alpha)\\rho| < 1$。该级数收敛于 $\\frac{1}{1 - (1-\\alpha)\\rho}$。\n$$\n\\mathbb{E}[y_{t-1} x_t] = \\frac{\\alpha \\sigma^2 \\rho}{1 - (1-\\alpha)\\rho}\n$$\n现在，我们将其代回 $V_y$ 的方程中：\n$$\nV_y(1 - (1-\\alpha)^2) = \\alpha^2 \\sigma^2 + 2\\alpha(1-\\alpha) \\frac{\\alpha \\sigma^2 \\rho}{1 - (1-\\alpha)\\rho}\n$$\n左边是 $V_y (1 - (1-2\\alpha+\\alpha^2)) = V_y (2\\alpha-\\alpha^2) = V_y \\alpha(2-\\alpha)$。\n右边是 $\\alpha^2 \\sigma^2 \\left( 1 + \\frac{2(1-\\alpha)\\rho}{1 - (1-\\alpha)\\rho} \\right) = \\alpha^2 \\sigma^2 \\left( \\frac{1 - (1-\\alpha)\\rho + 2(1-\\alpha)\\rho}{1 - (1-\\alpha)\\rho} \\right) = \\alpha^2 \\sigma^2 \\frac{1+(1-\\alpha)\\rho}{1 - (1-\\alpha)\\rho}$。\n令两边相等：\n$$\nV_y \\alpha(2-\\alpha) = \\alpha^2 \\sigma^2 \\frac{1+(1-\\alpha)\\rho}{1 - (1-\\alpha)\\rho}\n$$\n对于 $\\alpha \\neq 0$，我们可以除以 $\\alpha$：\n$$\nV_y (2-\\alpha) = \\alpha \\sigma^2 \\frac{1+(1-\\alpha)\\rho}{1 - (1-\\alpha)\\rho}\n$$\n求解 $V_y = \\mathrm{Var}(y_t)$：\n$$\n\\mathrm{Var}(y_t) = \\sigma^2 \\frac{\\alpha}{2-\\alpha} \\frac{1+(1-\\alpha)\\rho}{1-(1-\\alpha)\\rho}\n$$\n这就是 SWA 估计量方差的闭式表达式。\n\n验证改进为 $\\Delta(F,\\alpha) = c \\left( \\sigma^2 - \\mathrm{Var}(y_t) \\right)$。代入 $\\mathrm{Var}(y_t)$ 的表达式：\n$$\n\\Delta(F,\\alpha) = c \\sigma^2 \\left( 1 - \\frac{\\alpha}{2-\\alpha} \\frac{1+(1-\\alpha)\\rho}{1-(1-\\alpha)\\rho} \\right)\n$$\n我们简化括号中的项：\n$$\n1 - \\frac{\\alpha(1+(1-\\alpha)\\rho)}{(2-\\alpha)(1-(1-\\alpha)\\rho)} = \\frac{(2-\\alpha)(1-(1-\\alpha)\\rho) - \\alpha(1+(1-\\alpha)\\rho)}{(2-\\alpha)(1-(1-\\alpha)\\rho)}\n$$\n分子展开为 $2 - 2\\alpha - (2-\\alpha-\\alpha(1-\\alpha))\\rho = 2(1-\\alpha) - (2-3\\alpha+\\alpha^2)\\rho$。这似乎很复杂，让我们重新检查一下。\n分子：$(2-\\alpha)(1-(1-\\alpha)\\rho) - \\alpha(1+(1-\\alpha)\\rho) = (2-\\alpha) - (2-\\alpha)(1-\\alpha)\\rho - \\alpha - \\alpha(1-\\alpha)\\rho = (2-\\alpha-\\alpha) - ((2-\\alpha)(1-\\alpha) + \\alpha(1-\\alpha))\\rho = 2(1-\\alpha) - (1-\\alpha)(2-\\alpha+\\alpha)\\rho = 2(1-\\alpha) - 2(1-\\alpha)\\rho = 2(1-\\alpha)(1-\\rho)$。\n所以括号内简化为 $\\frac{2(1-\\alpha)(1-\\rho)}{(2-\\alpha)(1-(1-\\alpha)\\rho)}$。\n$\\Delta(F,\\alpha)$ 的完整表达式变为：\n$$\n\\Delta(F,\\alpha) = c \\sigma^2 \\frac{2(1-\\alpha)(1-\\rho)}{(2-\\alpha)(1-(1-\\alpha)\\rho)}\n$$\n最后，我们代入给定的相关性模型 $\\rho = \\exp(-F/\\tau)$：\n$$\n\\Delta(F,\\alpha) = c \\sigma^2 \\frac{2(1-\\alpha)(1 - e^{-F/\\tau})}{(2-\\alpha)(1-(1-\\alpha)e^{-F/\\tau})}\n$$\n这是用于计算的最终闭式表达式。如果 $\\alpha=1$，分子为 $0$，因此 $\\Delta(F,1)=0$，这是正确的，因为没有进行平均。\n\n计算将使用提供的常数 $c=0.5$、$\\sigma^2=1.0$ 和 $\\tau=20.0$。\n因此，计算公式为：\n$$\n\\Delta(F,\\alpha) = 0.5 \\cdot 1.0 \\cdot \\frac{2(1-\\alpha)(1 - e^{-F/20.0})}{(2-\\alpha)(1-(1-\\alpha)e^{-F/20.0})} = \\frac{(1-\\alpha)(1 - e^{-F/20.0})}{(2-\\alpha)(1-(1-\\alpha)e^{-F/20.0})}\n$$", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the expected validation improvement from Stochastic Weight Averaging (SWA)\n    for a given set of test cases.\n    \"\"\"\n    # Define the fixed constants from the problem statement.\n    # c: effective curvature constant\n    # sigma2: stationary variance of the base process\n    # tau: correlation timescale\n    c = 0.5\n    sigma2 = 1.0\n    tau = 20.0\n\n    # Define the test cases from the problem statement as a list of (F, alpha) tuples.\n    # F: checkpointing frequency\n    # alpha: EMA averaging coefficient\n    test_cases = [\n        (1, 0.1),\n        (10, 0.1),\n        (50, 0.1),\n        (15, 1.0),\n        (5, 0.05),\n        (5, 0.5),\n        (1, 0.9),\n    ]\n\n    results = []\n    \n    # The derived formula for the validation improvement Delta(F, alpha) is:\n    # Delta(F, alpha) = c * sigma^2 * [2 * (1-alpha) * (1 - exp(-F/tau))] / \n    #                   [(2-alpha) * (1 - (1-alpha) * exp(-F/tau))]\n    \n    for F, alpha in test_cases:\n        # The case alpha = 1 implies no averaging, so the improvement is 0.\n        # The formula correctly handles this since (1-alpha) becomes 0.\n        if alpha == 1.0:\n            delta_val = 0.0\n        else:\n            # Calculate the correlation rho based on checkpoint frequency F and timescale tau\n            rho_F = np.exp(-F / tau)\n            \n            # The simplified calculation formula using c=0.5 and sigma2=1.0:\n            # delta_val = (1 - alpha) * (1 - rho_F) / ((2 - alpha) * (1 - (1 - alpha) * rho_F))\n            \n            numerator = (1 - alpha) * (1 - rho_F)\n            denominator = (2 - alpha) * (1 - (1 - alpha) * rho_F)\n            \n            # Denominator is guaranteed to be non-zero for alpha in (0, 1] and F >= 0.\n            \n            # Calculate the final improvement Delta\n            delta_val = numerator / denominator\n            \n        # Format the result to 6 decimal places and add to the list.\n        # Using f-string formatting ensures exactly 6 decimal places, avoiding\n        # potential floating point representation issues with round().\n        results.append(f\"{delta_val:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3135390"}]}