{"hands_on_practices": [{"introduction": "实例归一化 (Instance Normalization) 的核心在于其能够分离并控制内容的“风格”，这在风格迁移等任务中至关重要。此练习将带你从第一性原理出发，推导仿射变换参数 $\\gamma$ 和 $\\beta$ 如何精确地将特征图的均值和标准差调整到目标值，从而揭示 IN 控制特征统计量的底层数学机制。通过这个推导，你将深刻理解这些可学习参数的真正作用 [@problem_id:3138582]。", "problem": "考虑一个卷积神经网络 (CNN) 中用于空间维度为 $H \\times W$ 的图像的单通道特征图。实例归一化 (IN) 计算每个实例、每个通道在空间网格上的统计数据。设源特征图表示为 $\\{x_{ij}\\}_{i=1,\\dots,H;\\, j=1,\\dots,W}$，其在空间维度上的源均值为 $\\mu_{s}$，源标准差为 $\\sigma_{s}$，其中均值和标准差定义如下\n$$\n\\mu_{s} \\triangleq \\frac{1}{HW} \\sum_{i=1}^{H} \\sum_{j=1}^{W} x_{ij}, \\quad \\sigma_{s} \\triangleq \\sqrt{\\frac{1}{HW} \\sum_{i=1}^{H} \\sum_{j=1}^{W} \\left(x_{ij} - \\mu_{s}\\right)^{2}},\n$$\n并假设 $\\sigma_{s} > 0$。假设我们希望生成一个变换后的特征图\n$$\ny_{ij} \\triangleq \\gamma \\cdot \\frac{x_{ij} - \\mu_{s}}{\\sigma_{s}} + \\beta\n$$\n其在 $(H,W)$ 上的均值和标准差分别等于指定的目标风格统计量 $\\mu_{t}$ 和 $\\sigma_{t}$，且 $\\sigma_{t} > 0$。\n\n仅使用均值、方差和标准差的核心定义，确定仿射参数 $(\\gamma, \\beta)$ 的值，以实现在 $(H,W)$ 空间维度上达到目标均值 $\\mu_{t}$ 和目标标准差 $\\sigma_{t}$。为保证唯一性，为 $\\gamma$ 选择正的缩放解。将你的最终答案表示为 $(\\gamma, \\beta)$ 的闭式解析表达式。", "solution": "用户想要确定实例归一化 (IN) 变换的仿射参数 $(\\gamma, \\beta)$。该变换定义为：\n$$\ny_{ij} \\triangleq \\gamma \\cdot \\frac{x_{ij} - \\mu_{s}}{\\sigma_{s}} + \\beta\n$$\n其中 $\\{x_{ij}\\}$ 是均值为 $\\mu_s$、标准差为 $\\sigma_s$ 的源特征图。目标是让变换后的特征图 $\\{y_{ij}\\}$ 具有目标均值 $\\mu_t$ 和目标标准差 $\\sigma_t$。我们已知 $\\sigma_s > 0$ 且 $\\sigma_t > 0$。\n\n首先，我们定义一个标准化变量 $\\hat{x}_{ij}$ 作为归一化后的源特征：\n$$\n\\hat{x}_{ij} \\triangleq \\frac{x_{ij} - \\mu_{s}}{\\sigma_{s}}\n$$\n现在，该变换可以更紧凑地写为：\n$$\ny_{ij} = \\gamma \\hat{x}_{ij} + \\beta\n$$\n我们的策略是首先计算标准化特征图 $\\{\\hat{x}_{ij}\\}$ 的均值和标准差，然后利用这些性质来找到满足 $\\{y_{ij}\\}$ 目标条件的 $\\gamma$ 和 $\\beta$ 的值。\n\n让我们计算 $\\{\\hat{x}_{ij}\\}$ 的均值，记为 $\\mu_{\\hat{x}}$。\n$$\n\\mu_{\\hat{x}} = \\frac{1}{HW} \\sum_{i=1}^{H} \\sum_{j=1}^{W} \\hat{x}_{ij}\n$$\n代入 $\\hat{x}_{ij}$ 的定义：\n$$\n\\mu_{\\hat{x}} = \\frac{1}{HW} \\sum_{i=1}^{H} \\sum_{j=1}^{W} \\left( \\frac{x_{ij} - \\mu_{s}}{\\sigma_{s}} \\right)\n$$\n利用求和的线性性质，我们可以将各项分开：\n$$\n\\mu_{\\hat{x}} = \\frac{1}{\\sigma_{s}} \\left( \\frac{1}{HW} \\sum_{i=1}^{H} \\sum_{j=1}^{W} x_{ij} - \\frac{1}{HW} \\sum_{i=1}^{H} \\sum_{j=1}^{W} \\mu_{s} \\right)\n$$\n根据定义，$\\mu_s = \\frac{1}{HW} \\sum_{i=1}^{H} \\sum_{j=1}^{W} x_{ij}$。第二项简化为 $\\frac{1}{HW} (HW \\cdot \\mu_s) = \\mu_s$。\n$$\n\\mu_{\\hat{x}} = \\frac{1}{\\sigma_{s}} (\\mu_{s} - \\mu_{s}) = 0\n$$\n因此，标准化特征图的均值为 $0$。\n\n接下来，我们计算 $\\{\\hat{x}_{ij}\\}$ 的方差，记为 $\\sigma_{\\hat{x}}^2$。\n$$\n\\sigma_{\\hat{x}}^2 = \\frac{1}{HW} \\sum_{i=1}^{H} \\sum_{j=1}^{W} (\\hat{x}_{ij} - \\mu_{\\hat{x}})^2\n$$\n由于 $\\mu_{\\hat{x}} = 0$，上式简化为：\n$$\n\\sigma_{\\hat{x}}^2 = \\frac{1}{HW} \\sum_{i=1}^{H} \\sum_{j=1}^{W} \\hat{x}_{ij}^2 = \\frac{1}{HW} \\sum_{i=1}^{H} \\sum_{j=1}^{W} \\left( \\frac{x_{ij} - \\mu_{s}}{\\sigma_{s}} \\right)^2\n$$\n$$\n\\sigma_{\\hat{x}}^2 = \\frac{1}{\\sigma_{s}^2} \\left( \\frac{1}{HW} \\sum_{i=1}^{H} \\sum_{j=1}^{W} (x_{ij} - \\mu_{s})^2 \\right)\n$$\n括号中的项是源方差 $\\sigma_{s}^2$ 的定义。\n$$\n\\sigma_{\\hat{x}}^2 = \\frac{1}{\\sigma_{s}^2} (\\sigma_{s}^2) = 1\n$$\n$\\{\\hat{x}_{ij}\\}$ 的标准差为 $\\sigma_{\\hat{x}} = \\sqrt{1} = 1$。标准化特征图的均值为 $0$，标准差为 $1$。\n\n现在，我们可以通过对 $y_{ij} = \\gamma \\hat{x}_{ij} + \\beta$ 施加目标统计量来确定参数 $\\gamma$ 和 $\\beta$。\n\n目标特征图的均值 $\\mu_y$ 必须等于 $\\mu_t$。\n$$\n\\mu_y = \\frac{1}{HW} \\sum_{i=1}^{H} \\sum_{j=1}^{W} y_{ij} = \\frac{1}{HW} \\sum_{i=1}^{H} \\sum_{j=1}^{W} (\\gamma \\hat{x}_{ij} + \\beta)\n$$\n再次利用求和的线性性质：\n$$\n\\mu_y = \\gamma \\left( \\frac{1}{HW} \\sum_{i=1}^{H} \\sum_{j=1}^{W} \\hat{x}_{ij} \\right) + \\frac{1}{HW} \\sum_{i=1}^{H} \\sum_{j=1}^{W} \\beta\n$$\n$$\n\\mu_y = \\gamma \\mu_{\\hat{x}} + \\beta\n$$\n代入 $\\mu_{\\hat{x}} = 0$：\n$$\n\\mu_y = \\gamma (0) + \\beta = \\beta\n$$\n我们要求 $\\mu_y = \\mu_t$，因此我们得到 $\\beta$ 的值：\n$$\n\\beta = \\mu_t\n$$\n\n目标特征图的标准差 $\\sigma_y$ 必须等于 $\\sigma_t$。我们首先计算方差 $\\sigma_y^2$。\n$$\n\\sigma_y^2 = \\frac{1}{HW} \\sum_{i=1}^{H} \\sum_{j=1}^{W} (y_{ij} - \\mu_y)^2\n$$\n代入 $y_{ij} = \\gamma \\hat{x}_{ij} + \\beta$ 和 $\\mu_y = \\beta$：\n$$\n\\sigma_y^2 = \\frac{1}{HW} \\sum_{i=1}^{H} \\sum_{j=1}^{W} ((\\gamma \\hat{x}_{ij} + \\beta) - \\beta)^2 = \\frac{1}{HW} \\sum_{i=1}^{H} \\sum_{j=1}^{W} (\\gamma \\hat{x}_{ij})^2\n$$\n$$\n\\sigma_y^2 = \\gamma^2 \\left( \\frac{1}{HW} \\sum_{i=1}^{H} \\sum_{j=1}^{W} \\hat{x}_{ij}^2 \\right)\n$$\n括号中的项是 $\\{\\hat{x}_{ij}\\}$ 的方差，即 $\\sigma_{\\hat{x}}^2 = 1$。\n$$\n\\sigma_y^2 = \\gamma^2 \\sigma_{\\hat{x}}^2 = \\gamma^2 (1) = \\gamma^2\n$$\n标准差 $\\sigma_y$ 是方差的平方根：\n$$\n\\sigma_y = \\sqrt{\\gamma^2} = |\\gamma|\n$$\n我们要求 $\\sigma_y = \\sigma_t$。这给出了条件：\n$$\n|\\gamma| = \\sigma_t\n$$\n由于 $\\sigma_t > 0$，这会产生两个可能的解：$\\gamma = \\sigma_t$ 或 $\\gamma = -\\sigma_t$。问题陈述中指定为 $\\gamma$ 选择正的缩放解。因此，我们必须选择：\n$$\n\\gamma = \\sigma_t\n$$\n\n达到目标统计量的仿射参数是 $(\\gamma, \\beta) = (\\sigma_t, \\mu_t)$。", "answer": "$$\n\\boxed{\\gamma = \\sigma_t, \\quad \\beta = \\mu_t}\n$$", "id": "3138582"}, {"introduction": "在实际训练中，批量大小 (batch size) 是一个关键超参数，但它对不同归一化层的影响截然不同。本练习是一个思想实验，旨在对比实例归一化 (IN) 与批量归一化 (BN) 在小批量大小场景下的表现差异，从而加深你对 IN 批量无关特性的理解。通过分析统计量估计的稳定性，你将能够推断出为何 IN 在某些情况下比 BN 更具鲁棒性 [@problem_id:3138579]。", "problem": "考虑一个为图像分类任务训练的卷积神经网络，其使用了批量归一化（Batch Normalization, BN）或实例归一化（Instance Normalization, IN）层。设批量大小用 $B$ 表示。假设你进行一个对照实验，其中除 $B$ 以外的所有超参数都保持固定，并将 $B$ 从 $64$ 减小到 $1$。验证准确率在一个与训练集同分布的留出集上进行测量。从BN使用逐通道的批量级统计量进行归一化、IN使用逐实例、逐通道的统计量进行归一化的核心定义出发，并结合统计估计中经过充分检验的事实（例如，样本均值的方差随着样本数量的增加而减小），推断当 $B$ 缩小时，BN与IN的验证准确率将如何变化。你的推理应当依赖于IN的逐实例统计量 $(\\mu_{nc},\\sigma_{nc})$ 与 $B$ 的独立性，以及批量级估计量的可靠性如何依赖于 $B$。\n\n哪个选项最能描述预期的趋势？\n\nA. 当 $B$ 从 $64$ 减小到 $1$ 时，由于批次统计量噪声更大以及训练与测试归一化不匹配，BN的验证准确率显著下降，而IN的验证准确率大致保持稳定，因为其逐实例统计量 $(\\mu_{nc},\\sigma_{nc})$ 不依赖于 $B$。\n\nB. 当 $B$ 从 $64$ 减小到 $1$ 时，BN的验证准确率大致保持不变，而IN的验证准确率急剧下降，因为批次多样性的减少损害了IN的逐实例统计量 $(\\mu_{nc},\\sigma_{nc})$。\n\nC. 当 $B$ 从 $64$ 减小到 $1$ 时，BN和IN的验证准确率都得到提升，因为较小的批次起到了稳定学习到的归一化的正则化器的作用。\n\nD. 当 $B$ 从 $64$ 减小到 $1$ 时，BN的验证准确率得到提升，因为批次统计量变得更能代表每个单独的样本，而IN的验证准确率由于对逐实例特征的过度归一化而恶化。", "solution": "用户希望我扮演一位严谨的STEM教授，解决一个关于深度学习中批量归一化（BN）和实例归一化（IN）的问题。\n\n### 第一步：提取已知条件\n- **背景：** 一个用于图像分类的卷积神经网络（CNN）。\n- **归一化层：** 批量归一化（BN）或实例归一化（IN）。\n- **变量：** 批量大小，$B$。\n- **实验：** $B$ 从 $64$ 减小到 $1$，所有其他超参数保持固定。\n- **度量指标：** 在一个留出集上的验证准确率。\n- **核心定义：**\n    - BN使用逐通道、批量级的统计量进行归一化。\n    - IN使用逐实例、逐通道的统计量 $(\\mu_{nc}, \\sigma_{nc})$ 进行归一化。\n- **给定原则/事实：**\n    - 样本均值的方差随着样本数量的增加而减小。\n    - IN的逐实例统计量 $(\\mu_{nc}, \\sigma_{nc})$ 与 $B$ 无关。\n    - 批量级估计量的可靠性依赖于 $B$。\n- **问题：** 推断当 $B$ 从 $64$ 缩小时，BN与IN的验证准确率将如何变化。\n\n### 第二步：使用提取的已知条件进行验证\n问题陈述具有科学依据，表述清晰且客观。\n1.  **科学合理性：** 该问题基于深度学习中已确立的概念（CNN、BN、IN）和统计学（样本均值、方差）。所提供的BN和IN的定义是正确的，样本大小与估计量可靠性之间的关系是基本的统计学原理。所描述的场景是机器学习研究中的标准实验设置。\n2.  **问题的明确性：** 问题提供了一个清晰的实验设置，并要求基于所提供的定义和原则进行推断。有足够的信息来得出一个唯一的、合乎逻辑的关于预期趋势的结论。\n3.  **客观性：** 语言技术性强且精确。它建立了一个清晰的推理框架，没有引入主观因素。\n\n### 第三步：判断与行动\n该问题是**有效的**。将根据所提供的原则推导出解决方案。\n\n### 推导\n设归一化层的输入张量为 $x$，其维度为 $(B, C, H, W)$，其中 $B$ 是批量大小，$C$ 是通道数，$H$ 和 $W$ 是空间高度和宽度。该张量的一个元素表示为 $x_{nchw}$。\n\n#### 批量归一化（BN）分析\n根据定义，BN计算每个通道 $c$ 在整个小批量上的统计量。均值 $\\mu_c$ 和方差 $\\sigma^2_c$ 的计算公式如下：\n$$ \\mu_c = \\frac{1}{B \\cdot H \\cdot W} \\sum_{n=1}^{B} \\sum_{h=1}^{H} \\sum_{w=1}^{W} x_{nchw} $$\n$$ \\sigma^2_c = \\frac{1}{B \\cdot H \\cdot W} \\sum_{n=1}^{B} \\sum_{h=1}^{H} \\sum_{w=1}^{W} (x_{nchw} - \\mu_c)^2 $$\n用于估计这些统计量的数据点数量为 $B \\cdot H \\cdot W$。问题指出，估计量的可靠性取决于样本数量。\n- 当 $B$ 很大时（例如，$B=64$），样本数量多，$\\mu_c$ 和 $\\sigma^2_c$ 是对整个数据集分布的真实通道统计量的相对稳定和可靠的估计。\n- 随着 $B$ 的减小，用于估计的样本数量减少。得到的统计量变得更加依赖于该小批量内的特定实例，使其成为真实总体统计量的“有噪声的”估计量。在 $B=1$ 的极端情况下，统计量是根据单个图像计算的。\n\n在推理（验证）期间，BN不计算批次统计量。而是使用在训练期间估计的总体统计量，通常通过指数移动平均得到：\n$$ \\hat{\\mu}_c \\leftarrow (1-m) \\hat{\\mu}_c + m \\mu_{c, \\text{batch}} $$\n$$ \\hat{\\sigma}^2_c \\leftarrow (1-m) \\hat{\\sigma}^2_c + m \\sigma^2_{c, \\text{batch}} $$\n其中 $m$ 是一个动量项。当使用小的 $B$进行训练时，批次统计量 $\\mu_{c, \\text{batch}}$ 和 $\\sigma^2_{c, \\text{batch}}$ 噪声非常大。这会导致两个关键问题：\n1.  **有噪声的训练更新：** 有噪声的统计量可能导致梯度不稳定，从而阻碍训练过程。\n2.  **训练-测试不匹配：** 训练时用于归一化的统计量（来自一个微小且有噪声的批次）与验证时使用的运行平均统计量有显著差异。这种差异可能导致性能大幅下降，因为网络在评估时所处的归一化条件与训练时不同。\n\n因此，随着 $B$ 从 $64$ 减小到 $1$，使用BN的模型的验证准确率预计会显著下降。\n\n#### 实例归一化（IN）分析\n根据定义，IN为每个实例 $n$ 和每个通道 $c$ 独立地计算统计量。均值 $\\mu_{nc}$ 和方差 $\\sigma^2_{nc}$ 的计算公式如下：\n$$ \\mu_{nc} = \\frac{1}{H \\cdot W} \\sum_{h=1}^{H} \\sum_{w=1}^{W} x_{nchw} $$\n$$ \\sigma^2_{nc} = \\frac{1}{H \\cdot W} \\sum_{h=1}^{H} \\sum_{w=1}^{W} (x_{nchw} - \\mu_{nc})^2 $$\n对特定实例 $n$ 的计算只涉及该实例的数据 ($x_{n \\cdot \\cdot \\cdot}$)。它不依赖于批次中的任何其他实例。批量大小 $B$ 并未出现在 $\\mu_{nc}$ 和 $\\sigma^2_{nc}$ 的公式中。\n这意味着应用于任何给定图像的归一化是完全相同的，无论它是大小为 $B=64$ 的批次的一部分，还是大小为 $B=1$ 的“批次”的一部分。如问题所述，归一化统计量与 $B$ 无关。\n此外，IN在推理期间的行为与其在训练期间的行为相同：它为每个测试样本计算并应用实例特定的统计量。在归一化方案中不存在训练-测试差异。\n因此，随着 $B$ 从 $64$ 减小到 $1$，使用IN的模型的验证准确率预计将大致保持稳定。\n\n### 逐项分析选项\n\n**A. 当 $B$ 从 $64$ 减小到 $1$ 时，由于批次统计量噪声更大以及训练与测试归一化不匹配，BN的验证准确率显著下降，而IN的验证准确率大致保持稳定，因为其逐实例统计量 $(\\mu_{nc},\\sigma_{nc})$ 不依赖于 $B$。**\n这一陈述与上述推导完全一致。它正确地指出了BN在小批量下性能下降的两个关键原因：统计估计中噪声的增加以及由此导致的训练与推理时归一化统计量的不匹配。它也正确地指出IN的性能是稳定的，因为其操作与批量大小无关。\n**结论：正确**\n\n**B. 当 $B$ 从 $64$ 减小到 $1$ 时，BN的验证准确率大致保持不变，而IN的验证准确率急剧下降，因为批次多样性的减少损害了IN的逐实例统计量 $(\\mu_{nc},\\sigma_{nc})$。**\n这个陈述在两方面都是错误的。众所周知，BN的性能高度依赖于批量大小，因此其准确率不会保持不变。IN的统计量是逐实例计算的，完全独立于批次多样性或大小。批次多样性的减少不会损害IN的统计量。\n**结论：错误**\n\n**C. 当 $B$ 从 $64$ 减小到 $1$ 时，BN和IN的验证准确率都得到提升，因为较小的批次起到了稳定学习到的归一化的正则化器的作用。**\n这个陈述是错误的。虽然小批量会给梯度更新带来噪声，有时会产生正则化效应，但这不太可能克服BN统计估计的根本问题。对于BN，小批量会*破坏*归一化统计量的稳定性，而不是稳定它们。对于IN，批量大小与归一化过程的稳定性无关。两者性能都会提升的说法与公认的经验性发现相悖。\n**结论：错误**\n\n**D. 当 $B$ 从 $64$ 减小到 $1$ 时，BN的验证准确率得到提升，因为批次统计量变得更能代表每个单独的样本，而IN的验证准确率由于对逐实例特征的过度归一化而恶化。**\n这个陈述是错误的。BN的目标不是让统计量代表单个样本，而是近似整个数据分布的统计量。使它们特定于单个样本（如 $B=1$ 时的情况）是导致训练-测试不匹配问题的根源，这会降低而不是提高准确率。关于IN的准确率因“过度归一化”而恶化的说法是一个定义不清且未经证实的断言。\n**结论：错误**", "answer": "$$\\boxed{A}$$", "id": "3138579"}, {"introduction": "将理论转化为代码是深度学习实践中的关键一步，而张量的维度和广播 (broadcasting) 规则是常见的陷阱。本练习要求你亲手实现一个正确的 IN 版本和一个有缺陷的版本，通过具体的数值例子来揭示一个常见的实现错误：在计算统计量时错误地混合了通道信息。这个实践将极大地增强你对张量操作的理解，并帮助你写出更健壮的归一化层代码 [@problem_id:3138633]。", "problem": "要求您在一个深度学习中标准的四维张量设置下，实现并测试实例归一化 (Instance Normalization, IN) 的广播语义。处理一个特征张量 $X \\in \\mathbb{R}^{B \\times C \\times H \\times W}$，其中 $B$ 是批量大小，$C$ 是通道数，$H$ 和 $W$ 是空间维度。目标是实现一个正确的逐实例、逐通道的归一化，并揭示一个常见的实现陷阱，即通过使用跨所有通道的逐实例均值导致广播混合了不同通道。\n\n从以下基本前提开始：\n- 对于任何有限的实数集合，样本均值是这些值的平均值，样本方差是与均值的平方偏差的平均值。\n- 对于 $X \\in \\mathbb{R}^{B \\times C \\times H \\times W}$，为每个固定的 $(b,c)$ 对在空间轴上计算统计量是有意义的。在平方根内添加一个小的正常数 $\\epsilon$ 以确保数值稳定性。\n\n任务：\n1. 实现一个函数，对于每个批量索引 $b \\in \\{0,\\dots,B-1\\}$ 和通道索引 $c \\in \\{0,\\dots,C-1\\}$，仅在空间维度 $\\{0,\\dots,H-1\\} \\times \\{0,\\dots,W-1\\}$ 上计算均值 $\\mu_{b,c} \\in \\mathbb{R}$ 和方差 $\\sigma^2_{b,c} \\in \\mathbb{R}$，将它们广播到形状 $\\mathbb{R}^{B \\times C \\times 1 \\times 1}$，并用它们来归一化 $X$，然后应用由 $\\gamma \\in \\mathbb{R}^{C}$ 和 $\\beta \\in \\mathbb{R}^{C}$ 指定的逐通道仿射变换。归一化必须严格地是逐实例、逐通道地在空间维度上进行，并且仿射参数必须在正确的轴上广播，绝不混合通道。\n2. 实现一个故意错误的变体，它为每个批量索引 $b$ 计算跨所有通道和空间维度的逐实例均值 $\\tilde{\\mu}_{b} \\in \\mathbb{R}$ 和方差 $\\tilde{\\sigma}^2_{b} \\in \\mathbb{R}$（形状为 $\\mathbb{R}^{B \\times 1 \\times 1 \\times 1}$），将这些值跨通道广播，然后应用相同的逐通道仿射变换。这通过使用 $\\tilde{\\mu}_{b}$ 和 $\\tilde{\\sigma}^2_{b}$ 替代 $\\mu_{b,c}$ 和 $\\sigma^2_{b,c}$，模拟了混合通道的广播陷阱。\n3. 对于下面的每个测试用例，计算正确归一化后的输出与错误归一化后的输出之间的最大绝对差。使用 $\\epsilon = 10^{-5}$ 并确保所有计算都以实数运算执行。仿射参数必须逐通道广播，而不是逐批量或逐空间位置广播。\n\n定义（不要在此处直接实现公式；在您的解决方案中从定义推导它们）：\n- 对于固定的 $(b,c)$，空间轴上的均值是 $H \\times W$ 个条目 $X_{b,c,h,w}$ 在 $(h,w)$ 遍历所有空间索引时的平均值。\n- 对于固定的 $(b,c)$，空间轴上的方差是 $X_{b,c,h,w}$ 与其均值的平方偏差在 $(h,w)$ 遍历所有空间索引时的平均值。\n- 对于错误的变体，批量索引 $b$ 的均值是 $X_{b,:,:,:}$ 中所有 $C \\times H \\times W$ 个条目的平均值，批量索引 $b$ 的方差是它们平方偏差的平均值。\n\n测试套件：\n- 案例 A（正常路径，不同通道统计数据）：\n  - 维度：$B=2$, $C=2$, $H=2$, $W=2$。\n  - 张量 $X$ 由以下指定：\n    $$X_{0,0,:,:} = \\begin{bmatrix} 0  2 \\\\ 4  6 \\end{bmatrix}, \\quad X_{0,1,:,:} = \\begin{bmatrix} -6  -4 \\\\ -2  0 \\end{bmatrix},$$\n    $$X_{1,0,:,:} = \\begin{bmatrix} 10  12 \\\\ 14  16 \\end{bmatrix}, \\quad X_{1,1,:,:} = \\begin{bmatrix} -10  -12 \\\\ -14  -16 \\end{bmatrix}.$$\n  - 仿射参数：$\\gamma = [1, 1]$, $\\beta = [0, 0]$。\n- 案例 B（边缘案例，一个批量，三个通道，一个空间维度退化）：\n  - 维度：$B=1$, $C=3$, $H=1$, $W=4$。\n  - 张量 $X$ 由以下指定：\n    $$X_{0,0,:,:} = \\begin{bmatrix} 1  2  3  4 \\end{bmatrix}, \\quad X_{0,1,:,:} = \\begin{bmatrix} 10  10  10  10 \\end{bmatrix},$$\n    $$X_{0,2,:,:} = \\begin{bmatrix} -1  -2  -3  -4 \\end{bmatrix}.$$\n  - 仿射参数：$\\gamma = [1, 0.5, 2]$, $\\beta = [0, 1, -1]$。\n- 案例 C（边界情况，由于只有一个通道，错误的广播是无害的）：\n  - 维度：$B=1$, $C=1$, $H=2$, $W=2$。\n  - 张量 $X$ 由以下指定：\n    $$X_{0,0,:,:} = \\begin{bmatrix} 5  7 \\\\ 9  11 \\end{bmatrix}.$$\n  - 仿射参数：$\\gamma = [1]$, $\\beta = [0]$。\n- 案例 D（极端案例，每个通道只有一个空间元素）：\n  - 维度：$B=2$, $C=3$, $H=1$, $W=1$。\n  - 张量 $X$ 由以下指定：\n    $$X_{0,0,0,0} = 0, \\quad X_{0,1,0,0} = 10, \\quad X_{0,2,0,0} = -10,$$\n    $$X_{1,0,0,0} = 5, \\quad X_{1,1,0,0} = 15, \\quad X_{1,2,0,0} = -5.$$\n  - 仿射参数：$\\gamma = [1, 1, 1]$, $\\beta = [0, 0, 0]$。\n\n输出规格：\n- 您的程序应生成单行输出，其中包含案例 A、B、C 和 D 的最大绝对差，按此顺序排列，形式为用方括号括起来的逗号分隔列表。例如，输出格式必须与以下完全一样：\n- \"您的程序应生成单行输出，其中包含用方括号括起来的逗号分隔列表形式的结果（例如，[resultA,resultB,resultC,resultD]）。\"\n- 每个结果都必须是浮点数。\n\n本问题不涉及物理单位或角度单位。所有数字都是纯实数值。请根据上述定义确保科学准确性和正确的广播语义；在计算逐通道统计数据时不要混合通道。所有计算均使用 $\\epsilon = 10^{-5}$。", "solution": "该问题要求对一个四维张量 $X \\in \\mathbb{R}^{B \\times C \\times H \\times W}$ 实现并比较两种归一化变体，其中 $B$ 是批量大小，$C$ 是通道数，$H, W$ 是空间维度。一个变体是正确的实例归一化 (Instance Normalization, IN)，另一个是故意设计的错误版本，它会错误地跨通道计算统计数据。\n\n设输入张量为 $X$，维度为 $(B, C, H, W)$。设 $\\gamma \\in \\mathbb{R}^{C}$ 和 $\\beta \\in \\mathbb{R}^{C}$ 为逐通道的可学习仿射参数（缩放和移位）。使用一个小的常数 $\\epsilon = 10^{-5}$ 来确保数值稳定性。\n\n首先，我们定义正确的实例归一化过程。对于每个实例 $b \\in \\{0, \\dots, B-1\\}$ 和每个通道 $c \\in \\{0, \\dots, C-1\\}$，均值 $\\mu_{b,c}$ 和方差 $\\sigma^2_{b,c}$ 在空间维度 $(H, W)$ 上被独立计算。空间元素的数量为 $N_{sp} = H \\times W$。\n\n实例 $b$ 和通道 $c$ 的均值为：\n$$ \\mu_{b,c} = \\frac{1}{N_{sp}} \\sum_{h=0}^{H-1} \\sum_{w=0}^{W-1} X_{b,c,h,w} $$\n实例 $b$ 和通道 $c$ 的方差为：\n$$ \\sigma^2_{b,c} = \\frac{1}{N_{sp}} \\sum_{h=0}^{H-1} \\sum_{w=0}^{W-1} (X_{b,c,h,w} - \\mu_{b,c})^2 $$\n使用这些统计数据，张量 $X$ 被归一化。请注意，对于每个元素 $X_{b,c,h,w}$，都使用相应的 $\\mu_{b,c}$ 和 $\\sigma^2_{b,c}$。在计算上，这是通过将有效形状为 $\\mathbb{R}^{B \\times C \\times 1 \\times 1}$ 的 $\\mu$ 和 $\\sigma^2$ 张量广播到完整的 $\\mathbb{R}^{B \\times C \\times H \\times W}$ 张量上来实现的。归一化后的张量 $\\hat{X}$ 由下式给出：\n$$ \\hat{X}_{b,c,h,w} = \\frac{X_{b,c,h,w} - \\mu_{b,c}}{\\sqrt{\\sigma^2_{b,c} + \\epsilon}} $$\n最后，应用逐通道的仿射变换。缩放因子 $\\gamma_c$ 和移位因子 $\\beta_c$ 会跨批量和空间维度进行广播。正确的最终输出 $Y$ 为：\n$$ Y_{b,c,h,w} = \\gamma_c \\hat{X}_{b,c,h,w} + \\beta_c $$\n\n其次，我们定义错误的归一化过程。该变体模拟了一种常见的实现错误，即统计数据是跨通道计算而不是逐通道计算的。对于每个实例 $b \\in \\{0, \\dots, B-1\\}$，在所有通道和空间维度 $(C, H, W)$ 上计算单个均值 $\\tilde{\\mu}_{b}$ 和方差 $\\tilde{\\sigma}^2_{b}$。每个实例的元素数量为 $N_{all} = C \\times H \\times W$。\n\n逐实例均值为：\n$$ \\tilde{\\mu}_{b} = \\frac{1}{N_{all}} \\sum_{c=0}^{C-1} \\sum_{h=0}^{H-1} \\sum_{w=0}^{W-1} X_{b,c,h,w} $$\n逐实例方差为：\n$$ \\tilde{\\sigma}^2_{b} = \\frac{1}{N_{all}} \\sum_{c=0}^{C-1} \\sum_{h=0}^{H-1} \\sum_{w=0}^{W-1} (X_{b,c,h,w} - \\tilde{\\mu}_{b})^2 $$\n然后，这些有效形状为 $\\mathbb{R}^{B \\times 1 \\times 1 \\times 1}$ 的统计数据被错误地跨所有通道广播用于归一化。错误归一化后的张量 $\\tilde{X}$ 由下式给出：\n$$ \\tilde{X}_{b,c,h,w} = \\frac{X_{b,c,h,w} - \\tilde{\\mu}_{b}}{\\sqrt{\\tilde{\\sigma}^2_{b} + \\epsilon}} $$\n仿射变换的应用方式与正确版本相同，保持其逐通道的特性。错误的最终输出 $\\tilde{Y}$ 为：\n$$ \\tilde{Y}_{b,c,h,w} = \\gamma_c \\tilde{X}_{b,c,h,w} + \\beta_c $$\n\n最后的任务是为每个测试用例计算两个输出之间的最大绝对差：\n$$ \\text{MaxAbsDiff} = \\max_{b,c,h,w} | Y_{b,c,h,w} - \\tilde{Y}_{b,c,h,w} | $$\n在我们的实现中，我们将使用 `numpy` 的 `mean` 和 `var` 函数，并附带适当的 `axis` 和 `keepdims=True` 参数，来高效地计算这些统计数据并利用广播机制。对于以大小为 $C$ 的一维数组形式给出的仿射参数 $\\gamma$ 和 $\\beta$，我们会将它们重塑为 $(1, C, 1, 1)$ 以确保正确的逐通道广播。差值计算将是一个直接的逐元素操作，然后找到最大值。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and compares correct and buggy Instance Normalization\n    for a 4D tensor, and calculates the maximum absolute difference\n    between their outputs for a suite of test cases.\n    \"\"\"\n    epsilon = 1e-5\n\n    test_cases = [\n        # Case A: Happy path, B=2, C=2, H=2, W=2\n        {\n            \"X\": np.array([\n                [[[0, 2], [4, 6]], [[-6, -4], [-2, 0]]],\n                [[[10, 12], [14, 16]], [[-10, -12], [-14, -16]]]\n            ], dtype=float),\n            \"gamma\": np.array([1, 1], dtype=float),\n            \"beta\": np.array([0, 0], dtype=float)\n        },\n        # Case B: Edge case, B=1, C=3, H=1, W=4\n        {\n            \"X\": np.array([\n                [[[1, 2, 3, 4]]],\n                [[[10, 10, 10, 10]]],\n                [[[-1, -2, -3, -4]]]\n            ], dtype=float).reshape(1, 3, 1, 4),\n            \"gamma\": np.array([1, 0.5, 2], dtype=float),\n            \"beta\": np.array([0, 1, -1], dtype=float)\n        },\n        # Case C: Boundary case, B=1, C=1, H=2, W=2\n        {\n            \"X\": np.array([[[[5, 7], [9, 11]]]], dtype=float),\n            \"gamma\": np.array([1], dtype=float),\n            \"beta\": np.array([0], dtype=float)\n        },\n        # Case D: Extreme case, B=2, C=3, H=1, W=1\n        {\n            \"X\": np.array([\n                [[[0]], [[10]], [[-10]]],\n                [[[5]], [[15]], [[-5]]]\n            ], dtype=float),\n            \"gamma\": np.array([1, 1, 1], dtype=float),\n            \"beta\": np.array([0, 0, 0], dtype=float)\n        }\n    ]\n\n    results = []\n\n    def correct_instance_norm(X, gamma, beta, eps):\n        C = X.shape[1]\n        \n        # Per-instance, per-channel statistics over spatial dimensions (axes 2, 3)\n        mu = np.mean(X, axis=(2, 3), keepdims=True)\n        var = np.var(X, axis=(2, 3), keepdims=True)\n\n        X_normalized = (X - mu) / np.sqrt(var + eps)\n\n        # Reshape affine parameters for broadcasting\n        gamma_reshaped = gamma.reshape(1, C, 1, 1)\n        beta_reshaped = beta.reshape(1, C, 1, 1)\n        \n        return gamma_reshaped * X_normalized + beta_reshaped\n\n    def buggy_instance_norm(X, gamma, beta, eps):\n        C = X.shape[1]\n        \n        # Per-instance statistics over channel and spatial dimensions (axes 1, 2, 3)\n        mu = np.mean(X, axis=(1, 2, 3), keepdims=True)\n        var = np.var(X, axis=(1, 2, 3), keepdims=True)\n\n        X_normalized = (X - mu) / np.sqrt(var + eps)\n        \n        # Reshape affine parameters for broadcasting\n        gamma_reshaped = gamma.reshape(1, C, 1, 1)\n        beta_reshaped = beta.reshape(1, C, 1, 1)\n        \n        return gamma_reshaped * X_normalized + beta_reshaped\n\n    for case in test_cases:\n        X = case[\"X\"]\n        gamma = case[\"gamma\"]\n        beta = case[\"beta\"]\n\n        Y_correct = correct_instance_norm(X, gamma, beta, epsilon)\n        Y_buggy = buggy_instance_norm(X, gamma, beta, epsilon)\n\n        max_abs_diff = np.max(np.abs(Y_correct - Y_buggy))\n        results.append(max_abs_diff)\n    \n    # Print the results in the specified format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3138633"}]}