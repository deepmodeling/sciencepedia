{"hands_on_practices": [{"introduction": "标准循环神经网络（RNN）面临梯度消失或爆炸的问题，难以长期保存信息。长短期记忆（LSTM）网络引入了“遗忘门”机制，以显式地控制其记忆单元中信息的保留或丢弃。本练习将通过一个受控实验，让您亲手调节遗忘门的偏置项 $b_f$，观察其对信息保留能力的影响，从而直观地理解 LSTM 能够处理比简单 RNN 更长依赖关系的核心机制。[@problem_id:3191179]", "problem": "考虑一个初始化的单细胞长短期记忆（LSTM）循环，用于研究遗忘门偏置在长序列上保留信息的效果。标准的单元更新由以下经过充分检验的方程定义：细胞状态 $c_t$ 按 $c_t = f_t \\cdot c_{t-1} + i_t \\cdot g_t$ 演变，其中遗忘门 $f_t = \\sigma(z_f(t))$，输入门 $i_t = \\sigma(z_i(t))$，候选更新 $g_t = \\tanh(z_g(t))$。logistic sigmoid 函数为 $\\sigma(x) = \\dfrac{1}{1 + e^{-x}}$，双曲正切为 $\\tanh(x)$。在使用均值为零的权重进行随机初始化时，通常通过设置遗忘门偏置 $b_f$ 来控制遗忘门保留信息的初始倾向。\n\n我们定义一个受控的、纯数学的初始化方案变体，以分离保留动态：\n- 对于所有时间步 $t \\geq 1$，门和候选值的激活前的值满足 $z_f(t) = b_f$、$z_i(t) = 0$ 和 $z_g(t) = 0$，这意味着 $f_t = \\sigma(b_f)$、$i_t = \\sigma(0)$ 和 $g_t = \\tanh(0)$。\n- 在时间 $t = 0$ 时，一次性写入设置了初始记忆 $c_0$，其中 $c_0 = 1$（无量纲）。对于 $t \\geq 1$，没有进一步的写入，因此对于 $t \\geq 1$，有 $i_t \\cdot g_t = 0$。\n\n仅从上述定义和事实出发，推导经过 $L$ 步后的保留率，其定义为比率 $R(L, b_f) = \\dfrac{c_L}{c_0}$，并实现一个程序，该程序为一系列遗忘门偏置值 $b_f$ 和依赖长度 $L$ 计算此保留率。\n\n测试套件：\n- 遗忘门偏置值 $b_f \\in \\{-3, -1, 0, 1, 3\\}$，即 $b_f$ 等于 $-3, -1, 0, 1, 3$。\n- 依赖长度 $L \\in \\{0, 5, 20, 100, 200\\}$，即 $L$ 等于 $0, 5, 20, 100, 200$。\n\n对于每个有序对 $(b_f, L)$，先按升序迭代 $b_f$，然后按升序迭代 $L$（即对该配对按字典序排序），计算保留率 $R(L, b_f)$，结果为一个实数，并将其报告为四舍五入到 $6$ 位小数的浮点数。本问题中没有物理单位；所有量均为无量纲。您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，列表中的结果严格按照指定的配对顺序排列：首先是 $b_f = -3$ 与所有 $L$ 的组合，然后是 $b_f = -1$ 与所有 $L$ 的组合，接着是 $b_f = 0$ 与所有 $L$ 的组合，然后是 $b_f = 1$ 与所有 $L$ 的组合，最后是 $b_f = 3$ 与所有 $L$ 的组合。例如，输出格式必须像 \"[r_1,r_2,...,r_{25}]\" 这样，其中每个 $r_k$ 是对应测试用例计算出的浮点数。每个测试用例的答案都是浮点数。", "solution": "用户希望我解决一个关于 LSTM 单元的问题。\n\n### 步骤 1：问题验证\n\n我将首先验证问题陈述。\n\n#### 步骤 1.1：提取已知条件\n\n已知条件如下：\n- LSTM 细胞状态更新方程：$c_t = f_t \\cdot c_{t-1} + i_t \\cdot g_t$\n- 遗忘门激活：$f_t = \\sigma(z_f(t))$\n- 输入门激活：$i_t = \\sigma(z_i(t))$\n- 候选更新：$g_t = \\tanh(z_g(t))$\n- Logistic sigmoid 函数：$\\sigma(x) = \\dfrac{1}{1 + e^{-x}}$\n- 双曲正切函数：$\\tanh(x)$\n- 对于所有时间步 $t \\geq 1$ 的受控模型条件：\n    - 遗忘门激活前的值：$z_f(t) = b_f$\n    - 输入门激活前的值：$z_i(t) = 0$\n    - 候选更新激活前的值：$z_g(t) = 0$\n- 对于 $t \\geq 1$ 的写入条件：$i_t \\cdot g_t = 0$\n- 在 $t = 0$ 时的初始条件：$c_0 = 1$\n- 保留率的定义：$R(L, b_f) = \\dfrac{c_L}{c_0}$\n- 测试套件参数：\n    - 遗忘门偏置值 $b_f \\in \\{-3, -1, 0, 1, 3\\}$\n    - 依赖长度 $L \\in \\{0, 5, 20, 100, 200\\}$\n- 输出格式：对于每个按字典序排列的配对 $(b_f, L)$，输出一个逗号分隔的浮点数值列表，保留 $6$ 位小数。\n\n#### 步骤 1.2：使用提取的已知条件进行验证\n\n根据既定标准对问题进行验证。\n\n1.  **科学或事实的健全性**：该问题具有科学依据。它使用了长短期记忆（LSTM）单元的标准、普遍接受的方程。引入的简化条件（$z_f(t) = b_f$，$z_i(t) = 0$，$z_g(t) = 0$）是一个明确定义的数学构造，旨在分离并研究遗忘门偏置对记忆保留的影响。这是科学建模中一种常见且有效的技术，通过分析简化案例来理解复杂系统。对于 $t \\geq 1$，$i_t \\cdot g_t = 0$ 的陈述与给定的激活前的值一致，因为 $g_t = \\tanh(z_g(t)) = \\tanh(0) = 0$，这使得无论 $i_t$ 的值如何，该乘积都为零。\n\n2.  **适定性与完整性**：该问题是适定的且自洽的。它提供了一个清晰的初始条件（$c_0=1$）、一个用于细胞状态 $c_t$ 的确定性递推关系，以及一个待计算量（$R(L, b_f)$）的精确定义。所有必要的变量和函数都已明确定义。测试用例已指定，所需的输出格式也毫不含糊。这种结构保证了存在唯一、稳定且有意义的解，并且该解是可以计算的。\n\n3.  **客观性**：该问题以精确、客观的数学语言陈述。它没有歧义、主观断言或基于观点的陈述。\n\n#### 步骤 1.3：结论与行动\n\n问题陈述是**有效的**。这是一个根植于深度学习既定原则的、定义明确的数学问题。我现在将着手推导解决方案。\n\n### 步骤 2：推导与求解\n\n目标是推导出保留率 $R(L, b_f) = \\dfrac{c_L}{c_0}$ 的表达式，并为给定的测试套件计算其值。\n\nLSTM 细胞状态的基本递推关系如下：\n$$c_t = f_t \\cdot c_{t-1} + i_t \\cdot g_t$$\n\n问题定义了一个简化的方案，其中对于所有时间步 $t \\geq 1$，输入写入项为零：\n$$i_t \\cdot g_t = 0 \\quad \\text{for } t \\geq 1$$\n这个简化与提供的激活前的值是一致的，因为 $z_g(t) = 0$ 意味着 $g_t = \\tanh(0) = 0$。\n\n将此条件代入细胞状态方程，对于 $t \\geq 1$ 的递推关系简化为一个纯乘法更新：\n$$c_t = f_t \\cdot c_{t-1}$$\n\n遗忘门激活值 $f_t$ 由其激活前的值 $z_f(t)$ 决定。对于 $t \\geq 1$，该值保持不变：\n$$z_f(t) = b_f$$\n因此，对于所有 $t \\geq 1$，遗忘门的输出 $f_t$ 也是恒定的。我们用 $f$ 表示这个恒定值：\n$$f = f_t = \\sigma(z_f(t)) = \\sigma(b_f) = \\frac{1}{1 + e^{-b_f}}$$\n\n对于 $t \\geq 1$ 的递推关系现在变成了一个公比为 $f$ 的简单几何级数：\n$$c_t = f \\cdot c_{t-1}$$\n\n我们可以从初始状态 $c_0$ 开始展开这个递推关系：\n- 当 $t=1$ 时：$c_1 = f \\cdot c_0$\n- 当 $t=2$ 时：$c_2 = f \\cdot c_1 = f \\cdot (f \\cdot c_0) = f^2 \\cdot c_0$\n- 当 $t=3$ 时：$c_3 = f \\cdot c_2 = f \\cdot (f^2 \\cdot c_0) = f^3 \\cdot c_0$\n\n通过归纳法，对于任何时间步 $L \\geq 1$，细胞状态 $c_L$ 由下式给出：\n$$c_L = f^L \\cdot c_0$$\n\n保留率 $R(L, b_f)$ 定义为比率 $\\dfrac{c_L}{c_0}$。\n对于 $L \\geq 1$，我们有：\n$$R(L, b_f) = \\frac{c_L}{c_0} = \\frac{f^L \\cdot c_0}{c_0} = f^L$$\n代入 $f$ 的表达式，我们得到对于 $L \\geq 1$ 的保留率公式：\n$$R(L, b_f) = \\left(\\sigma(b_f)\\right)^L = \\left(\\frac{1}{1 + e^{-b_f}}\\right)^L$$\n\n一个特殊情况是 $L=0$。根据定义，经过 $0$ 步后的保留率是：\n$$R(0, b_f) = \\frac{c_0}{c_0} = 1$$\n如果我们采用 $x^0 = 1$ 的约定，这个结果也与通用公式一致。\n\n因此，完整的保留率公式为：\n$$\nR(L, b_f) =\n\\begin{cases}\n1  \\text{if } L = 0 \\\\\n\\left(\\dfrac{1}{1 + e^{-b_f}}\\right)^L  \\text{if } L  0\n\\end{cases}\n$$\n\n实现将包括一个程序，该程序按字典序遍历指定的 $(b_f, L)$ 配对。对于每个配对，它将使用推导出的公式计算 $R(L, b_f)$，并将 $L=0$ 作为基本情况处理。结果将被收集，格式化为 $6$ 位小数，并打印为单个用方括号括起来的逗号分隔字符串。`numpy` 库适合进行指数计算。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the memory retention in a simplified LSTM cell for a sweep of\n    forget-gate biases and dependency lengths.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    b_f_values = [-3, -1, 0, 1, 3]\n    L_values = [0, 5, 20, 100, 200]\n\n    # This structure ensures lexicographic ordering on the pair (b_f, L).\n    test_cases = [(b_f, L) for b_f in b_f_values for L in L_values]\n\n    results = []\n    \n    for b_f, L in test_cases:\n        # The retention R(L, b_f) is defined as c_L / c_0.\n        # The recurrence relation c_t = f * c_{t-1} for t = 1 with c_0 given.\n        # This leads to c_L = f^L * c_0, so R(L, b_f) = f^L.\n        # The forget gate activation f is constant: f = sigma(b_f).\n\n        # Case 1: L = 0 (0 steps)\n        # By definition, R(0, b_f) = c_0 / c_0 = 1.\n        if L == 0:\n            retention = 1.0\n        # Case 2: L  0\n        else:\n            # Calculate the sigmoid function for the given bias b_f.\n            # sigma(x) = 1 / (1 + e^(-x))\n            sigma_b_f = 1.0 / (1.0 + np.exp(-b_f))\n            \n            # Calculate retention as sigma(b_f) raised to the power of L.\n            retention = sigma_b_f ** L\n        \n        results.append(retention)\n\n    # Format the final output string as a comma-separated list of floats\n    # rounded to 6 decimal places, enclosed in square brackets.\n    formatted_results = [f\"{res:.6f}\" for res in results]\n    output_string = f\"[{','.join(formatted_results)}]\"\n    \n    # The final print statement must produce only the specified output format.\n    print(output_string)\n\nsolve()\n\n```", "id": "3191179"}, {"introduction": "虽然 LSTM 对标准 RNN 进行了改进，但其顺序处理的本质仍然是处理极长依赖关系的瓶颈。作为 Transformer 架构核心的注意力机制为此提供了一种截然不同的解决方案。本练习将对比 RNN 顺序、逐步传递信息的方式与注意力机制的直接访问能力，后者允许序列中的任意两个位置直接交互，而不受距离限制。通过分析一个需要追踪嵌套结构的经典任务，您将学会识别 RNN 和注意力模型在处理长程依赖问题时各自的根本性架构优势与局限。[@problem_id:3191175]", "problem": "考虑典型的嵌套括号识别任务，该任务需要一个类堆栈内存来处理层级结构。设输入为一个长度为 $T$ 的序列，其字母表为 $\\{ \"(\", \")\" \\}$，编码为 $x_t \\in \\{+1, -1\\}$，其中 $t \\in \\{1,\\dots,T\\}$，$+1$ 表示一个左括号，$-1$ 表示一个右括号。定义累积和前缀过程 $s_t = \\sum_{i=1}^t x_i$，其中 $s_0 = 0$。一个字符串是平衡的，当且仅当 $s_T = 0$ 且对于所有 $t \\in \\{0,\\dots,T\\}$ 都有 $s_t \\ge 0$。最大嵌套深度等于 $\\max_{t} s_t$。深度为 $D$ 的最难的平衡字符串是长度为 $T$ 的字符串（$T = 2D$），由 $D$ 个左括号后跟 $D$ 个右括号组成。对于这种字符串，第一个左括号必须与最后一个右括号匹配，从而产生 $\\delta = 2D - 1$ 的依赖距离。\n\n您将在此任务上比较两种抽象的模型族：\n1. 一个标准循环神经网络 (RNN)，其隐藏状态更新公式为 $h_{t+1} = \\phi(W_h h_t + W_x x_{t+1} + b)$，并有一个读取 $h_T$ 的最终分类器。假设每步的雅可比算子范数以一个常数 $\\lambda  0$ 为界，即对所有步骤都有 $\\| J_t \\|_2 \\le \\lambda$，其中 $J_t = \\partial h_{t+1}/\\partial h_t$。这是一个标准的、广泛使用的界，其中 $\\lambda = \\| W_h \\|_2 L_\\phi$， $L_\\phi$ 是激活函数的利普希茨常数。\n2. 一个单层点积自注意力机制，其局部注意力窗口大小为 $w \\in \\mathbb{N}$，意味着每个位置只能直接关注距离最多为 $w$ 的位置。设 $p \\in \\{0, 1\\}$ 表示是否存在绝对位置编码（$p=1$）或不存在（$p=0$）。不存在位置编码意味着在单个注意力层中，内容相同的词元（token）在位置上是无法区分的。\n\n从第一性原理出发，使用以下推理来判断在所述抽象下，每个模型原则上是否能在未经训练的情况下，成功处理深度为 $D$ 的最坏情况平衡字符串：\n- 对于 RNN，使用链式法则和算子范数的次乘性，可以得到最早的词元对 $h_T$ 影响的一个上界，如下所示：\n$$\n\\left\\| \\frac{\\partial h_T}{\\partial h_0} \\right\\|_2 \\le \\prod_{t=0}^{T-1} \\|J_t\\|_2 \\le \\lambda^T.\n$$\n使用容忍阈值 $\\varepsilon = 10^{-3}$，当且仅当 $\\varepsilon \\le \\lambda^T \\le \\varepsilon^{-1}$ 时，我们宣布 RNN 成功。这排除了梯度消失（$\\lambda^T  \\varepsilon$）和梯度爆炸（$\\lambda^T > \\varepsilon^{-1}$）这两种学习长期依赖的失败模式。\n- 对于单层局部自注意力模型，该模型只能直接关联距离在 $w$ 内的词元。最坏情况下的匹配距离是 $\\delta = 2D - 1$。因此，仅当 $p = 1$（位置可区分）且 $w \\ge \\delta$ 时，模型才能成功。否则，它会因为局部性（$w  \\delta$）或位置不可区分性（$p=0$）而失败。\n\n您的任务是实现一个程序，对每个测试用例应用上述标准，并为每个用例返回一个分类整数代码：\n- 如果 RNN 和注意力模型都成功，返回 $3$。\n- 如果只有 RNN 成功，返回 $2$。\n- 如果只有注意力模型成功，返回 $1$。\n- 如果两者都失败，返回 $0$。\n\n使用以下测试套件，它涵盖了一般情况、局部性受限的注意力、梯度消失的 RNN、在更大深度下的权衡、缺少位置编码以及一个浅层边缘情况：\n- 用例 1：$D = 5$, $\\lambda = 0.95$, $w = 10$, $p = 1$。\n- 用例 2：$D = 5$, $\\lambda = 0.95$, $w = 5$, $p = 1$。\n- 用例 3：$D = 50$, $\\lambda = 0.9$, $w = 200$, $p = 1$。\n- 用例 4：$D = 100$, $\\lambda = 0.99$, $w = 50$, $p = 1$。\n- 用例 5：$D = 100$, $\\lambda = 1.05$, $w = 1000$, $p = 0$。\n- 用例 6：$D = 1$, $\\lambda = 0.5$, $w = 1$, $p = 1$。\n\n实现细节：\n- 对每个用例，计算 $T = 2D$，$\\delta = 2D - 1$，并使用 $\\varepsilon = 10^{-3}$。\n- 通过关于 $\\lambda^T$ 相对于 $\\varepsilon$ 的不等式来判断 RNN 是否成功。\n- 通过检查 $p = 1$ 和 $w \\ge \\delta$ 来判断注意力模型是否成功。\n- 按上述定义将每个用例的结果编码为单个整数。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，例如 $[r_1,r_2,\\dots,r_6]$，其中每个 $r_i$ 是按上文列出顺序的第 $i$ 个测试用例的整数代码。", "solution": "该问题要求对一个标准循环神经网络（RNN）和一个单层局部自注意力模型，在识别一类特定的平衡括号字符串任务上进行对比分析。该分析基于处理长期依赖的抽象第一性原理标准。\n\n首先，我们形式化问题背景。输入是一个长度为 $T$ 的序列 $x_1, \\dots, x_T$，其中 $x_t=+1$ 代表一个左括号 `(`，$x_t=-1$ 代表一个右括号 `)`。嵌套深度为 $D$ 的“最难”平衡字符串被定义为 $D$ 个左括号后跟 $D$ 个右括号。这种结构意味着序列长度为 $T = 2D$。关键的依赖关系存在于时间 $t=1$ 的第一个左括号和时间 $t=T=2D$ 的最后一个右括号之间。此依赖的距离为 $\\delta = T-1 = 2D-1$。\n\n每个模型的成功与否由一组预定义的标准决定。我们将根据这些规则分析每个模型类别。\n\n**模型 1：标准循环神经网络 (RNN)**\n\nRNN 逐步处理序列，通过递推关系 $h_{t+1} = \\phi(W_h h_t + W_x x_{t+1} + b)$ 更新其隐藏状态 $h_t$。网络学习长程依赖的能力，根本上关系到早期输入的信息如何通过这个递推关系传播以影响最终状态 $h_T$。这种传播在数学上由最终状态相对于初始状态的梯度 $\\frac{\\partial h_T}{\\partial h_0}$ 来描述。使用链式法则，这可以表示为每个时间步的雅可比矩阵的乘积：\n$$\n\\frac{\\partial h_T}{\\partial h_0} = \\frac{\\partial h_T}{\\partial h_{T-1}} \\frac{\\partial h_{T-1}}{\\partial h_{T-2}} \\cdots \\frac{\\partial h_1}{\\partial h_0} = \\prod_{t=0}^{T-1} J_t\n$$\n其中 $J_t = \\frac{\\partial h_{t+1}}{\\partial h_t}$。取算子范数并利用其次乘性（$\\|AB\\| \\le \\|A\\|\\|B\\|$)，我们得到该梯度范数的一个上界：\n$$\n\\left\\| \\frac{\\partial h_T}{\\partial h_0} \\right\\|_2 \\le \\prod_{t=0}^{T-1} \\|J_t\\|_2\n$$\n问题提供了一个统一的界 $\\|J_t\\|_2 \\le \\lambda$ 对所有 $t$ 成立。这将表达式简化为：\n$$\n\\left\\| \\frac{\\partial h_T}{\\partial h_0} \\right\\|_2 \\le \\lambda^T\n$$\n这个界是梯度消失和梯度爆炸概念的核心。如果 $\\lambda  1$，随着序列长度 $T$ 的增加，界 $\\lambda^T$ 会指数级衰减到 $0$。这意味着早期输入的影响会变得微乎其微，使得模型无法学习长期依赖。这就是梯度消失问题。相反，如果 $\\lambda > 1$，这个界会指数级增长，可能导致训练不稳定。这就是梯度爆炸问题。\n\n为 RNN 定义的成功标准旨在避免这些病态区域。使用容忍阈值 $\\varepsilon = 10^{-3}$，当且仅当梯度界在一个稳定范围内时，RNN 被认为是成功的：\n$$\n\\varepsilon \\le \\lambda^T \\le \\varepsilon^{-1}\n$$\n\n**模型 2：单层局部自注意力**\n\n自注意力机制将每个词元的表示计算为其他词元表示的加权平均。在窗口大小为 $w$ 的局部自注意力模型中，位置 $i$ 处的每个词元只能直接关注（即交换信息）范围在 $[i-w, i+w]$ 内的词元。要解决给定任务，模型必须能够将 $t=1$ 处的第一个左括号与 $t=T$ 处的最后一个右括号关联起来。这些词元之间的距离是 $\\delta = T-1 = 2D-1$。对于单层局部注意力，要在这两个词元之间建立直接路径，其窗口大小 $w$ 必须足够大以跨越这个距离，即 $w \\ge \\delta$。\n\n此外，问题指明了最坏情况字符串由 $D$ 个相同的左括号后跟 $D$ 个相同的右括号组成。如果没有一种按位置区分词元的机制，自注意力层就无法区分第一个 `(` 和第二个 `(`，从而无法学习正确的嵌套结构。绝对位置编码提供了这一关键信息。参数 $p=1$ 表示其存在，而 $p=0$ 表示其不存在。\n\n因此，单层局部自注意力模型的成功取决于两个条件：\n1. 必须有位置信息：$p=1$。\n2. 注意力窗口必须足够宽以捕捉最长的依赖关系：$w \\ge \\delta$。\n当且仅当两个条件都满足时，模型才成功。\n\n**逐案评估**\n\n现在我们将这些标准应用于每个测试用例，使用 $\\varepsilon = 10^{-3}$。\n\n**用例 1：**$D=5$, $\\lambda = 0.95$, $w=10$, $p=1$。\n- 推导值：$T = 2D = 10$，$\\delta = 2D-1 = 9$。\n- RNN：我们计算 $\\lambda^T = 0.95^{10} \\approx 0.5987$。条件是 $0.001 \\le 0.5987 \\le 1000$。该条件成立。**RNN 成功。**\n- 注意力模型：我们检查 $p=1$（真）和 $w \\ge \\delta$，即 $10 \\ge 9$（真）。**注意力模型成功。**\n- 结果：两者都成功，代码 = $3$。\n\n**用例 2：**$D=5$, $\\lambda = 0.95$, $w=5$, $p=1$。\n- 推导值：$T = 10$，$\\delta = 9$。\n- RNN：我们计算 $\\lambda^T = 0.95^{10} \\approx 0.5987$。条件是 $0.001 \\le 0.5987 \\le 1000$。该条件成立。**RNN 成功。**\n- 注意力模型：我们检查 $p=1$（真）和 $w \\ge \\delta$，即 $5 \\ge 9$（假）。**注意力模型失败。**\n- 结果：只有 RNN 成功，代码 = $2$。\n\n**用例 3：**$D=50$, $\\lambda = 0.9$, $w=200$, $p=1$。\n- 推导值：$T = 2D = 100$，$\\delta = 2D-1 = 99$。\n- RNN：我们计算 $\\lambda^T = 0.9^{100} \\approx 2.656 \\times 10^{-5}$。这个值小于 $\\varepsilon = 0.001$。梯度消失。**RNN 失败。**\n- 注意力模型：我们检查 $p=1$（真）和 $w \\ge \\delta$，即 $200 \\ge 99$（真）。**注意力模型成功。**\n- 结果：只有注意力模型成功，代码 = $1$。\n\n**用例 4：**$D=100$, $\\lambda = 0.99$, $w=50$, $p=1$。\n- 推导值：$T = 2D = 200$，$\\delta = 2D-1 = 199$。\n- RNN：我们计算 $\\lambda^T = 0.99^{200} \\approx 0.134$。条件是 $0.001 \\le 0.134 \\le 1000$。该条件成立。**RNN 成功。**\n- 注意力模型：我们检查 $p=1$（真）和 $w \\ge \\delta$，即 $50 \\ge 199$（假）。**注意力模型失败。**\n- 结果：只有 RNN 成功，代码 = $2$。\n\n**用例 5：**$D=100$, $\\lambda = 1.05$, $w=1000$, $p=0$。\n- 推导值：$T = 200$，$\\delta = 199$。\n- RNN：我们计算 $\\lambda^T = 1.05^{200} \\approx 17293$。这个值大于 $\\varepsilon^{-1} = 1000$。梯度爆炸。**RNN 失败。**\n- 注意力模型：我们检查 $p=1$。该条件为假（$p=0$）。模型无法区分词元位置。**注意力模型失败。**\n- 结果：两者都失败，代码 = $0$。\n\n**用例 6：**$D=1$, $\\lambda = 0.5$, $w=1$, $p=1$。\n- 推导值：$T = 2D = 2$，$\\delta = 2D-1 = 1$。字符串是 `()`。\n- RNN：我们计算 $\\lambda^T = 0.5^2 = 0.25$。条件是 $0.001 \\le 0.25 \\le 1000$。该条件成立。**RNN 成功。**\n- 注意力模型：我们检查 $p=1$（真）和 $w \\ge \\delta$，即 $1 \\ge 1$（真）。**注意力模型成功。**\n- 结果：两者都成功，代码 = $3$。\n\n这些结果被合成为一个最终的整数代码列表。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Applies success criteria to an RNN and a local self-attention model\n    for a series of test cases on a parenthesis-matching task.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # Each case is a tuple (D, lambda, w, p).\n    test_cases = [\n        (5, 0.95, 10, 1),  # Case 1\n        (5, 0.95, 5, 1),   # Case 2\n        (50, 0.9, 200, 1), # Case 3\n        (100, 0.99, 50, 1),# Case 4\n        (100, 1.05, 1000, 0),# Case 5\n        (1, 0.5, 1, 1),    # Case 6\n    ]\n\n    # The tolerance threshold epsilon.\n    epsilon = 1e-3\n\n    results = []\n    for D, lam, w, p in test_cases:\n        # Calculate derived problem parameters.\n        T = 2 * D\n        delta = 2 * D - 1\n\n        # --- RNN Success Criterion ---\n        # The RNN succeeds if its gradient norm bound lambda^T is within a stable range.\n        # This prevents vanishing (lambda^T  epsilon) and exploding (lambda^T > 1/epsilon) gradients.\n        lambda_T = np.power(lam, T)\n        rnn_succeeds = (epsilon = lambda_T = 1 / epsilon)\n\n        # --- Attention Success Criterion ---\n        # The single-layer local attention model succeeds if:\n        # 1. Positional encodings are present (p=1) to distinguish identical tokens.\n        # 2. The local window size (w) is large enough to span the maximum dependency distance (delta).\n        attention_succeeds = (p == 1 and w >= delta)\n\n        # --- Determine the final result code ---\n        # 3: both succeed\n        # 2: only RNN succeeds\n        # 1: only attention succeeds\n        # 0: both fail\n        if rnn_succeeds and attention_succeeds:\n            code = 3\n        elif rnn_succeeds and not attention_succeeds:\n            code = 2\n        elif not rnn_succeeds and attention_succeeds:\n            code = 1\n        else: # both fail\n            code = 0\n            \n        results.append(code)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3191175"}, {"introduction": "前一个练习展示了注意力机制的强大能力，但这种能力是以巨大的计算和内存成本为代价的，特别是其与序列长度相关的二次方复杂度。在真实世界的应用中，尤其是在处理像 Long Range Arena (LRA) 基准测试中的超长序列时，必须在模型性能和硬件限制之间进行权衡。本练习将让您扮演机器学习工程师的角色，为提升效率进行优化。您将为不同的注意力策略（如密集注意力、局部注意力和线性注意力）以及梯度检查点等技术进行成本建模，从而在严格的内存预算下选择最高效的训练方案，这是训练大规模模型的关键技能。[@problem_id:3191168]", "problem": "要求您在严格的图形处理单元（GPU）内存预算下，为长距离竞技场（Long Range Arena, LRA）任务中的序列模型，建模并优化其训练时的计算-内存权衡。您的程序必须为每个给定案例选择一种训练策略，该策略在满足硬性内存预算的同时，最小化总浮点运算次数（作为时间的代理）。必须从每个案例指定的一组策略子集中进行选择。\n\n基本定义和假设（所有公式仅由此推导）：\n- 设批量大小为 $b$，序列长度为 $n$，模型（嵌入）维度为 $d$，注意力头数量为 $h$，层数为 $L$，头维度 $d_h = d / h$（假设 $d$ 可被 $h$ 整除）。\n- 所有激活值和中间张量均使用 $32$ 位浮点数。每个存储的标量消耗 $4$ 字节。将此常数表示为 $\\beta_b = 4$ 字节/标量。\n- 训练期间，会存储激活值以用于反向传播。对于一个具有密集自注意力机制的 Transformer 层，需要存储的基本激活值包括：\n  1. 查询 $Q$、键 $K$、值 $V$ 以及注意力投影的输出 $O$，它们的形状均为 $(b, h, n, d_h)$，在跨头相乘后相当于 $(b, n, d)$ 个标量。\n  2. 两个形状均为 $(b, n, d)$ 的前馈（多层感知机）激活值（非线性激活前后各一个），建模为一个简化的对，以捕捉主要的存储开销。\n  3. 注意力 logit 和注意力概率，形状均为 $(b, h, n, n)$。\n- 由此，对于一个具有密集注意力的层，存储的标量总数的主导项为：\n  - “与 token 数量线性相关”部分：$Q, K, V, O$ 产生 $4 \\cdot b \\cdot n \\cdot d$ 个标量；两个前馈激活值增加 $2 \\cdot b \\cdot n \\cdot d$ 个标量。因此，每层的与 token 数量线性相关的标量等于 $\\alpha_1 \\cdot b \\cdot n \\cdot d$，其中 $\\alpha_1 = 6$。\n  - “与注意力相关的平方”部分：注意力 logit 和概率共同产生 $\\alpha_2 \\cdot b \\cdot h \\cdot n^2$ 个标量，其中 $\\alpha_2 = 2$。\n- 对于一个有 $L$ 层的模型，如果一个内存项是“每层”的，则将其乘以 $L$ 以获得总的激活内存。\n- 忽略参数和优化器状态的内存，在类似 LRA 的设置下，这部分内存远小于长序列的激活内存。并假设没有额外的开销。所有的内存计算必须通过标量计数乘以 $\\beta_b$ 来以字节表示。\n- 对于计算量，计算浮点运算次数（为简化和保证策略间的一致性，将一次乘加运算视为一个单位）。每层：\n  - $Q, K, V, O$ 的投影计算成本为 $4 \\cdot b \\cdot n \\cdot d \\cdot d$ 个运算单位（每个 token 上的矩阵-矩阵乘积）。\n  - 密集注意力的成本项与形成 $QK^\\top$ 和乘以 $V$ 成正比，两者的收缩维度均为 $d_h$。将其建模为每层 $2 \\cdot b \\cdot h \\cdot n^2 \\cdot d_h$ 个运算单位。\n  - 窗口大小为 $w$ 的局部（窗口化）注意力，在两个注意力乘法中用 $n \\cdot w$ 替换 $n^2$，得到每层 $2 \\cdot b \\cdot h \\cdot n \\cdot w \\cdot d_h$ 个运算单位。\n  - 线性注意力（核化或等效方法）移除了显式的注意力矩阵，并使用流式累积。将此注意力组件建模为每层 $k_\\ell \\cdot b \\cdot h \\cdot n \\cdot d_h$ 个运算单位，其中常数 $k_\\ell = 4$。\n- 跨层的梯度检查点技术通过在反向传播中重新计算中间结果，用计算换取内存。当启用检查点时，对单层使用以下模型：\n  - 激活内存乘以 $r_{\\mathrm{ckpt}} = 1/2$（大致只存储层的输入/输出）。\n  - 计算量乘以 $g_{\\mathrm{ckpt}} = 2$（每层的前向传播中间结果在反向传播中被重新计算）。\n- 策略：\n  - 策略 $0$：密集注意力（无检查点）。\n  - 策略 $1$：带检查点的密集注意力（内存按 $r_{\\mathrm{ckpt}}$ 缩放，计算量按 $g_{\\mathrm{ckpt}}$ 缩放）。\n  - 策略 $2$：窗口大小为 $w$ 的局部注意力（无检查点）。\n  - 策略 $3$：线性注意力（无检查点）。\n- 一个策略是可行的，当且仅当其总激活内存（以字节为单位）小于或等于该案例给定的内存预算。在案例的可行且允许的策略中，选择总运算次数最小的那个。如果出现平局，选择策略索引最小的那个。如果没有允许的策略是可行的，则该案例输出 $-1$。\n\n任务：\n1. 仅从上述形状和定义出发，推导出每种策略的每层激活内存（以标量计数），然后使用 $\\beta_b = 4$ 将其转换为字节。跨 $L$ 层聚合，以获得一个案例-策略对的总激活内存。\n2. 从基本矩阵运算出发，推导出每种策略的每层运算次数，跨 $L$ 层聚合，并在适用的情况下计入检查点乘数 $g_{\\mathrm{ckpt}}$。\n3. 实现一个程序，对于下面的每个测试案例，枚举允许的策略，根据内存预算进行筛选，并选择总运算次数最小的可行策略，通过最小索引来打破平局。为每个案例返回所选的策略索引（整数）。输出中不需要物理单位，只需整数索引。\n\n测试套件：\n每个案例是一个元组 $(n, d, h, L, b, M_{\\mathrm{bytes}}, w, \\text{allowed})$，其中 $M_{\\mathrm{bytes}}$ 是内存预算（以字节为单位），$w$ 是局部注意力的窗口大小（仅由策略 2 使用），$\\text{allowed}$ 是允许的策略索引列表。\n- 案例 $1$：$(n = 1024, d = 512, h = 8, L = 6, b = 4, M_{\\mathrm{bytes}} = 2147483648, w = 256, \\text{allowed} = [0, 1, 2, 3])$。\n- 案例 $2$：$(n = 4096, d = 512, h = 8, L = 6, b = 1, M_{\\mathrm{bytes}} = 6000000000, w = 256, \\text{allowed} = [0, 1, 2, 3])$。\n- 案例 $3$：$(n = 16384, d = 512, h = 8, L = 6, b = 1, M_{\\mathrm{bytes}} = 2000000000, w = 256, \\text{allowed} = [0, 2, 3])$。\n- 案例 $4$：$(n = 4096, d = 512, h = 8, L = 6, b = 2, M_{\\mathrm{bytes}} = 1409286144, w = 256, \\text{allowed} = [2])$。这是一个边界测试，当总内存恰好等于预算时，策略是可行的。\n- 案例 $5$：$(n = 2048, d = 512, h = 8, L = 6, b = 4, M_{\\mathrm{bytes}} = 3523215360, w = 1024, \\text{allowed} = [0, 1, 2])$。这个案例确保了检查点可以使密集模型变得可行，而在该预算下，不带检查点的密集注意力和局部注意力都不可行。\n\n最终输出格式要求：\n- 您的程序应生成一行输出，其中包含一个用方括号括起来的逗号分隔列表（例如 $[r_1,r_2,\\dots]$），其中每个 $r_i$ 是按上述顺序排列的案例 $i$ 所选的整数策略索引。", "solution": "任务是，在给定的内存预算下，通过最小化总浮点运算次数（计算量），来确定序列模型的最佳训练策略。选择是针对几个测试案例，从一组允许的策略中进行的。第一步，问题验证，已经完成，问题被认为是有效且定义明确的。解决方案首先为每种策略推导内存使用和计算成本的数学模型，然后实现一个算法来为每个案例找到最优策略。\n\n### 1. 内存和计算公式的推导\n\n我们为四种策略中的每一种推导出总激活内存（$M$，以字节为单位）和总计算成本（$C$，以浮点运算次数为单位）。公式是基于提供的参数推导的：批量大小 $b$、序列长度 $n$、模型维度 $d$、头数量 $h$、头维度 $d_h = d/h$、层数 $L$ 和局部注意力窗口大小 $w$。常数包括：$\\beta_b = 4$ 字节/标量、与 token 数量线性相关的标量系数 $\\alpha_1 = 6$、与注意力相关的平方标量系数 $\\alpha_2 = 2$、线性注意力计算常数 $k_\\ell=4$、检查点内存比率 $r_{\\text{ckpt}}=1/2$ 和检查点计算乘数 $g_{\\text{ckpt}}=2$。\n\n对于局部注意力（策略 2），必须做出一个关键的解释。问题陈述，对于计算量，二次依赖关系 $n^2$ 被线性依赖关系 $nw$ 所取代。一个标准的假设是，这种替换也适用于存储注意力矩阵所关联的内存项，这对于创建一个有意义的模型是必要的。否则，局部注意力将不会比密集注意力提供内存优势。这一解释得到了问题对测试案例 4 的描述的证实，该案例被描述为一个可行性恰好成立的边界情况，这仅在局部注意力的内存与 $nw$ 成比例时才成立。\n\n**策略 0：密集注意力**\n-   **激活内存 ($M_0$)**：总标量计数是与 token 数量线性相关部分和与注意力相关的平方部分之和，再乘以层数 $L$。\n    $$ \\text{scalars}_0 = L \\cdot (\\alpha_1 bnd + \\alpha_2 bhn^2) = L \\cdot (6bnd + 2bhn^2) $$\n    使用 $\\beta_b = 4$ 转换为字节：\n    $$ M_0 = \\beta_b \\cdot L \\cdot (6bnd + 2bhn^2) = 4L(6bnd + 2bhn^2) $$\n-   **计算成本 ($C_0$)**：每层的成本是投影成本和注意力计算成本之和。我们使用恒等式 $h \\cdot d_h = d$。\n    $$ c_{\\text{layer},0} = 4bnd^2 + 2bhn^2d_h = 4bnd^2 + 2bn^2d $$\n    $L$ 层的总成本：\n    $$ C_0 = L(4bnd^2 + 2bn^2d) $$\n\n**策略 1：带梯度检查点的密集注意力**\n此策略通过应用检查点乘数来修改策略 0。\n-   **激活内存 ($M_1$)**：内存按 $r_{\\text{ckpt}} = 1/2$ 缩放。\n    $$ M_1 = M_0 \\cdot r_{\\text{ckpt}} = \\frac{1}{2} \\cdot 4L(6bnd + 2bhn^2) = 2L(6bnd + 2bhn^2) $$\n-   **计算成本 ($C_1$)**：计算量按 $g_{\\text{ckpt}} = 2$ 缩放。\n    $$ C_1 = C_0 \\cdot g_{\\text{ckpt}} = 2 \\cdot L(4bnd^2 + 2bn^2d) $$\n\n**策略 2：局部注意力**\n此策略将对 $n$ 的二次依赖关系替换为对 $n \\cdot w$ 的依赖关系。\n-   **激活内存 ($M_2$)**：内存模型中的 $n^2$ 项被替换为 $nw$。\n    $$ M_2 = \\beta_b \\cdot L \\cdot (\\alpha_1 bnd + \\alpha_2 bhnw) = 4L(6bnd + 2bhnw) $$\n-   **计算成本 ($C_2$)**：计算模型中的 $n^2$ 项被替换为 $nw$。\n    $$ C_2 = L(4bnd^2 + 2bhnwd_h) = L(4bnd^2 + 2bnwd) $$\n\n**策略 3：线性注意力**\n此策略消除了显式的 $n \\times n$ 注意力矩阵，从而移除了二次内存项。\n-   **激活内存 ($M_3$)**：仅保留激活内存中与 token 数量线性相关的部分。\n    $$ M_3 = \\beta_b \\cdot L \\cdot (\\alpha_1 bnd) = 4L(6bnd) = 24Lbnd $$\n-   **计算成本 ($C_3$)**：注意力计算成本被一个线性时间等效项所取代。\n    $$ C_3 = L(4bnd^2 + k_\\ell bhnd_h) = L(4bnd^2 + 4bnd) $$\n\n### 2. 算法设计\n\n对于由 $(n, d, h, L, b, M_{\\mathrm{bytes}}, w, \\text{allowed})$ 指定的每个测试案例，执行以下算法：\n\n1.  初始化一个空列表 `feasible_strategies`，用于存储满足内存预算的策略。\n2.  对于提供的 `allowed` 列表中的每个策略索引 `s`：\n    a. 使用上面推导的公式和给定的案例参数，计算总激活内存 $M_s$ 和总计算量 $C_s$。\n    b. 检查可行性：如果 $M_s \\le M_{\\mathrm{bytes}}$，则将元组 $(C_s, s)$ 添加到 `feasible_strategies` 列表中。\n3.  在评估完所有允许的策略后：\n    a. 如果 `feasible_strategies` 为空，则没有允许的策略是可行的。此案例的结果为 $-1$。\n    b. 如果 `feasible_strategies` 不为空，则对其进行排序。主排序键是计算成本 $C_s$（升序），次排序键是策略索引 $s$（升序），以按要求打破平局。\n    c. 最优策略是与排序后列表的第一个元素对应的策略。结果是此元素的策略索引。\n4.  收集所有测试案例的结果，并格式化为单个字符串 `[r_1,r_2,...]` 作为最终输出。此过程保证在所有可行选项中找到计算成本最低的策略，并遵守打破平局的规则。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Models and optimizes training-time compute-memory trade-offs for sequence models.\n\n    For each test case, this function selects the training strategy that minimizes\n    the total floating-point operation count while satisfying a hard memory budget.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (n, d, h, L, b, M_bytes, w, allowed)\n        (1024, 512, 8, 6, 4, 2147483648, 256, [0, 1, 2, 3]),\n        (4096, 512, 8, 6, 1, 6000000000, 256, [0, 1, 2, 3]),\n        (16384, 512, 8, 6, 1, 2000000000, 256, [0, 2, 3]),\n        (4096, 512, 8, 6, 2, 1409286144, 256, [2]),\n        (2048, 512, 8, 6, 4, 3523215360, 1024, [0, 1, 2]),\n    ]\n\n    results = []\n    for case in test_cases:\n        n, d, h, L, b, M_bytes, w, allowed = case\n        d_h = d // h\n\n        # Constants from problem\n        beta_b = 4  # bytes per scalar\n        k_ell = 4   # linear attention compute constant\n\n        # --- Memory and Compute Formulas ---\n\n        # Strategy 0: Dense attention\n        mem0 = beta_b * L * (6 * b * n * d + 2 * b * h * n**2)\n        comp0 = L * (4 * b * n * d**2 + 2 * b * n**2 * d)\n\n        # Strategy 1: Dense attention with checkpointing\n        mem1 = mem0 / 2\n        comp1 = comp0 * 2\n\n        # Strategy 2: Local attention\n        mem2 = beta_b * L * (6 * b * n * d + 2 * b * h * n * w)\n        comp2 = L * (4 * b * n * d**2 + 2 * b * n * w * d)\n\n        # Strategy 3: Linear attention\n        mem3 = beta_b * L * (6 * b * n * d)\n        comp3 = L * (4 * b * n * d**2 + k_ell * b * n * d)\n\n        # Store strategy performance in a list of tuples: (memory, compute)\n        strategies_perf = [\n            (mem0, comp0),\n            (mem1, comp1),\n            (mem2, comp2),\n            (mem3, comp3)\n        ]\n\n        feasible_options = []\n        for strat_idx in allowed:\n            memory, compute = strategies_perf[strat_idx]\n            if memory = M_bytes:\n                # Store as (compute, strategy_index) for sorting\n                feasible_options.append((compute, strat_idx))\n\n        if not feasible_options:\n            results.append(-1)\n        else:\n            # Sort by compute cost (primary key), then by strategy index (secondary key)\n            feasible_options.sort()\n            best_strategy_index = feasible_options[0][1]\n            results.append(best_strategy_index)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3191168"}]}