{"hands_on_practices": [{"introduction": "为了真正理解双向循环神经网络 (BiRNN) 的威力，我们可以设计一个单向模型无法完美解决的任务。这个练习 [@problem_id:3103026] 构建了一个合成的序列标注问题，通过它，我们可以从第一性原理出发，定量地计算出模型因能“看到”未来信息而获得的准确率提升。这是一个绝佳的实践，能帮助你深入理解双向处理的核心价值。", "problem": "您将处理一个合成序列分类任务，该任务旨在分离出循环神经网络中双向性的优势。考虑一个二元标记序列 $\\{x_t\\}_{t=1}^L$，其中每个 $x_t \\in \\{0,1\\}$ 都是从参数为 $p$ 的伯努利分布中独立同分布地抽取的，即对所有 $t$ 都有 $x_t \\sim \\mathrm{Bernoulli}(p)$。对于每个内部位置 $t$（即所有满足 $2 \\le t \\le L-1$ 的 $t$），您必须根据以下邻居约束的恒等规则分配一个标签 $y_t \\in \\{0,1\\}$：当且仅当 $x_t$ 等于其两个邻居的逻辑与，即 $x_t = x_{t-1} \\land x_{t+1}$ 时，$y_t = 1$；否则，$y_t = 0$。\n\n基本原理：循环神经网络（RNN）是一种因果模型，其中时间 $t$ 的隐藏状态 $h_t$ 是当前输入 $x_t$ 和前一个隐藏状态 $h_{t-1}$ 的函数，而输出 $o_t$ 则依赖于 $h_t$。双向循环神经网络（BiRNN）会同时从前向和后向处理序列，计算隐藏状态 $h_t^{\\rightarrow}$ 和 $h_t^{\\leftarrow}$，并根据 $h_t^{\\rightarrow}$ 和 $h_t^{\\leftarrow}$ 两者形成输出 $o_t$。因此，一个在时间 $t$ 进行预测的仅前向因果模型不能依赖于 $x_{t+1}$，而一个双向模型在时间 $t$ 的预测则可能同时依赖于 $x_{t-1}$ 和 $x_{t+1}$。\n\n您的任务是，从第一性原理出发，不依赖任何预先提供的目标公式，推导出两种理想化预测器在仅评估内部位置 $t \\in \\{2,3,\\dots,L-1\\}$ 时的期望分类准确率：\n\n- 最佳的仅前向因果预测器，在时间 $t$，它可以访问 $x_{t-1}$ 和 $x_t$ 以及分布参数 $p$，但不能访问 $x_{t+1}$。该预测器必须仅使用不违反相对于 $t+1$ 的因果性的信息来决定 $\\hat{y}_t \\in \\{0,1\\}$。其期望准确率应在上述数据生成过程下计算。\n- 一个理想化的双向预测器，在时间 $t$，它可以访问 $x_{t-1}$、$x_t$ 和 $x_{t+1}$ 以及参数 $p$。其期望准确率应在相同的数据生成过程下计算。\n\n您必须编写一个完整的、可运行的程序，该程序针对每个测试用例计算三个浮点数量：仅前向因果预测器在内部位置的期望准确率，双向预测器在内部位置的期望准确率，以及定义为双向和仅前向期望准确率之差的准确率增益。角度和物理单位不适用于此任务。\n\n测试套件：\n- $(L,p) = (3,0)$\n- $(L,p) = (3,1)$\n- $(L,p) = (5,0.5)$\n- $(L,p) = (9,0.25)$\n- $(L,p) = (9,0.9)$\n- $(L,p) = (12,0.5)$\n\n最终输出格式：\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果，每个测试用例对应一个包含上述三个浮点数的子列表。例如，输出必须类似于 $[\\,[a_1,b_1,c_1],\\,[a_2,b_2,c_2],\\,\\dots\\,]$，不含任何额外文本。所有值都应根据推导精确计算，并以标准十进制浮点数形式报告。", "solution": "该问题是有效的，因为它具有科学依据、适定且客观。它提出了一个在统计决策理论和概率论中可形式化的任务，与理解因果模型（如单向RNN）和非因果模型（如双向RNN）之间的功能差异直接相关。所有必要信息均已提供，问题没有矛盾或歧义。\n\n我们现在将为指定的两种预测器推导期望分类准确率。推导是针对一个任意内部位置 $t$ 进行的，其中 $2 \\le t \\le L-1$。由于标记 $x_t$ 是独立同分布的（i.i.d.），结果将与 $t$ 的具体选择和序列长度 $L$ 无关。\n\n数据生成过程由一个二元标记序列 $\\{x_t\\}_{t=1}^L$ 定义，其中每个标记 $x_t \\in \\{0,1\\}$ 是从参数为 $p$ 的伯努利分布中抽取的：\n$$ P(x_t=1) = p $$\n$$ P(x_t=0) = 1-p $$\n对于内部位置 $t$，真实标签 $y_t \\in \\{0,1\\}$ 由以下规则确定：\n$$ y_t = 1 \\text{ 当且仅当 } x_t = (x_{t-1} \\land x_{t+1}) $$\n其中 $\\land$ 代表逻辑与运算符。否则，$y_t=0$。\n\n预测器 $\\hat{y}_t$ 的准确率是其与真实标签 $y_t$ 匹配的概率，即 $P(\\hat{y}_t = y_t)$。我们寻求期望准确率，对于这个平稳过程，它就是这个概率。一个最优预测器，在给定信息集的情况下，通过选择具有最高后验概率的标签来最大化这个概率。\n\n**1. 理想化的双向预测器**\n\n理想化的双向预测器在时间 $t$ 可以访问完整的三元组 $(x_{t-1}, x_t, x_{t+1})$。标记规则 $y_t = 1 \\iff x_t = (x_{t-1} \\land x_{t+1})$ 完全依赖于这三个值。凭借对这个三元组的完全了解，预测器可以计算出 $x_{t-1} \\land x_{t+1}$ 的值，并将其与 $x_t$ 进行比较。因此，对于预测器来说，$y_t$ 的结果是确定性的。\n\n对于任何给定的三元组，预测器可以将其预测 $\\hat{y}_t$ 设置为与真实标签 $y_t$ 完全相等。例如，如果它观察到 $(x_{t-1}, x_t, x_{t+1}) = (1, 1, 1)$，它计算 $x_{t-1} \\land x_{t+1} = 1 \\land 1 = 1$。因为这等于 $x_t=1$，所以真实标签是 $y_t=1$，预测器就设置 $\\hat{y}_t=1$。如果它观察到 $(x_{t-1}, x_t, x_{t+1}) = (1, 0, 1)$，它计算 $x_{t-1} \\land x_{t+1} = 1$。这不等于 $x_t=0$，所以真实标签是 $y_t=0$，预测器就设置 $\\hat{y}_t=0$。\n\n由于预测 $\\hat{y}_t$ 总能与真实标签 $y_t$ 相等，所以正确预测的概率是 $1$。\n$$ P(\\hat{y}_t = y_t | x_{t-1}, x_t, x_{t+1}) = 1 $$\n期望准确率 $A_{bi}$ 是这个条件概率在所有可能的三元组上的期望，结果就是 $1$。\n$$ A_{bi} = E[P(\\hat{y}_t = y_t | x_{t-1}, x_t, x_{t+1})] = 1 $$\n\n**2. 最佳的仅前向因果预测器**\n\n仅前向因果预测器可以访问 $(x_{t-1}, x_t)$ 和参数 $p$，但不能访问 $x_{t+1}$。最佳预测器将选择在给定可用信息下最大化后验概率的标签 $\\hat{y}_t$：\n$$ \\hat{y}_t = \\arg\\max_{k \\in \\{0,1\\}} P(y_t=k | x_{t-1}, x_t) $$\n这种最优策略的准确率是 $P(\\hat{y}_t = y_t)$。我们通过对观察到的 $(x_{t-1}, x_t)$ 的四种可能组合的条件准确率进行求和来计算这个值，并按每种组合的概率进行加权。\n\n让我们分析观察到的组合 $(x_{t-1}, x_t)$ 的四种情况：\n\n**情况 A：$(x_{t-1}, x_t) = (0, 0)$**\n$y_t=1$ 的条件是 $x_t = (x_{t-1} \\land x_{t+1})$。代入已知值，我们得到 $0 = (0 \\land x_{t+1})$。由于无论 $x_{t+1}$ 是什么，$0 \\land x_{t+1}$ 总是 $0$，这简化为 $0=0$，永远成立。因此，$y_t$ 确定性地为 $1$。\n最优预测是 $\\hat{y}_t=1$。这种情况下的准确率是 $1$。\n这种情况的概率是 $P(x_{t-1}=0, x_t=0) = (1-p)(1-p) = (1-p)^2$。\n\n**情况 B：$(x_{t-1}, x_t) = (0, 1)$**\n$y_t=1$ 的条件是 $1 = (0 \\land x_{t+1})$，这简化为 $1=0$。这永远不成立。因此，$y_t$ 确定性地为 $0$。\n最优预测是 $\\hat{y}_t=0$。这种情况下的准确率是 $1$。\n这种情况的概率是 $P(x_{t-1}=0, x_t=1) = (1-p)p$。\n\n**情况 C：$(x_{t-1}, x_t) = (1, 0)$**\n$y_t=1$ 的条件是 $0 = (1 \\land x_{t+1})$，这简化为 $0 = x_{t+1}$。因此，如果 $x_{t+1}=0$ 则 $y_t=1$，如果 $x_{t+1}=1$ 则 $y_t=0$。由于 $x_{t+1}$ 未知，我们使用其概率分布。\n$P(y_t=1 | x_{t-1}=1, x_t=0) = P(x_{t+1}=0) = 1-p$。\n$P(y_t=0 | x_{t-1}=1, x_t=0) = P(x_{t+1}=1) = p$。\n如果 $1-p > p$ (即 $p  0.5$)，最优预测器选择 $\\hat{y}_t=1$；如果 $p > 1-p$ (即 $p > 0.5$)，则选择 $\\hat{y}_t=0$。如果 $p=0.5$，任一选择都会产生相同的准确率。这种情况下的准确率是多数类的概率，即 $\\max(p, 1-p)$。\n这种情况的概率是 $P(x_{t-1}=1, x_t=0) = p(1-p)$。\n\n**情况 D：$(x_{t-1}, x_t) = (1, 1)$**\n$y_t=1$ 的条件是 $1 = (1 \\land x_{t+1})$，这简化为 $1 = x_{t+1}$。因此，如果 $x_{t+1}=1$ 则 $y_t=1$，如果 $x_{t+1}=0$ 则 $y_t=0$。\n$P(y_t=1 | x_{t-1}=1, x_t=1) = P(x_{t+1}=1) = p$。\n$P(y_t=0 | x_{t-1}=1, x_t=1) = P(x_{t+1}=0) = 1-p$。\n在这种情况下，最优预测器的准确率同样是多数类的概率，即 $\\max(p, 1-p)$。\n这种情况的概率是 $P(x_{t-1}=1, x_t=1) = p \\cdot p = p^2$。\n\n**仅前向预测器的总期望准确率 ($A_{fwd}$)**\n我们将每种情况的准确率按其概率加权求和：\n$$ A_{fwd} = 1 \\cdot P(x_{t-1}=0, x_t=0) + 1 \\cdot P(x_{t-1}=0, x_t=1) + \\max(p, 1-p) \\cdot P(x_{t-1}=1, x_t=0) + \\max(p, 1-p) \\cdot P(x_{t-1}=1, x_t=1) $$\n$$ A_{fwd} = 1 \\cdot (1-p)^2 + 1 \\cdot p(1-p) + \\max(p, 1-p) \\cdot p(1-p) + \\max(p, 1-p) \\cdot p^2 $$\n对各项进行因式分解：\n$$ A_{fwd} = ((1-p)^2 + p(1-p)) + \\max(p, 1-p) \\cdot (p(1-p) + p^2) $$\n$$ A_{fwd} = (1-p)(1-p+p) + \\max(p, 1-p) \\cdot p(1-p+p) $$\n$$ A_{fwd} = (1-p) \\cdot 1 + \\max(p, 1-p) \\cdot p \\cdot 1 $$\n$$ A_{fwd} = (1-p) + p \\cdot \\max(p, 1-p) $$\n我们可以将其表示为 $p$ 的分段函数：\n- 如果 $p  0.5$，则 $\\max(p, 1-p) = 1-p$。$A_{fwd} = (1-p) + p(1-p) = (1-p)(1+p) = 1-p^2$。\n- 如果 $p \\ge 0.5$，则 $\\max(p, 1-p) = p$。$A_{fwd} = (1-p) + p(p) = 1-p+p^2$。\n\n**3. 准确率增益**\n\n准确率增益是双向和仅前向期望准确率之间的差值。\n$$ \\text{Gain} = A_{bi} - A_{fwd} = 1 - [(1-p) + p \\cdot \\max(p, 1-p)] $$\n$$ \\text{Gain} = p - p \\cdot \\max(p, 1-p) = p(1 - \\max(p, 1-p)) $$\n使用恒等式 $1 - \\max(a,b) = \\min(1-a, 1-b)$，我们有：\n$$ \\text{Gain} = p \\cdot \\min(p, 1-p) $$\n- 如果 $p  0.5$，则 $\\min(p, 1-p) = p$。$\\text{Gain} = p \\cdot p = p^2$。\n- 如果 $p \\ge 0.5$，则 $\\min(p, 1-p) = 1-p$。$\\text{Gain} = p(1-p)$。\n\n这些推导出的公式可以直接计算每个测试用例所需的数量。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the expected accuracies for forward-only and bidirectional predictors\n    on a synthetic sequence task.\n    \"\"\"\n    # Test suite: each tuple is (L, p)\n    # L is not used in the calculation since the expected accuracy for any\n    # interior position is independent of sequence length for an i.i.d. process.\n    test_cases = [\n        (3, 0.0),\n        (3, 1.0),\n        (5, 0.5),\n        (9, 0.25),\n        (9, 0.9),\n        (12, 0.5),\n    ]\n\n    results = []\n    for L, p in test_cases:\n        # Calculate the expected accuracy of the best possible forward-only predictor.\n        # The derived formula is A_fwd = (1-p) + p * max(p, 1-p).\n        # This can be written as a piecewise function:\n        # if p  0.5, A_fwd = 1 - p**2\n        # if p >= 0.5, A_fwd = 1 - p + p**2\n        if p  0.5:\n            acc_fwd = 1.0 - p**2\n        else:\n            acc_fwd = 1.0 - p + p**2\n\n        # The idealized bidirectional predictor has access to all necessary information\n        # (x_{t-1}, x_t, x_{t+1}) to determine the label y_t with certainty.\n        # Therefore, its accuracy is always 1.0.\n        acc_bi = 1.0\n\n        # The accuracy gain is the difference between the two.\n        accuracy_gain = acc_bi - acc_fwd\n\n        # Append the results for this test case.\n        results.append([acc_fwd, acc_bi, accuracy_gain])\n\n    # Format the output string as required: [[a1,b1,c1],[a2,b2,c2],...]\n    sublist_strings = []\n    for sublist in results:\n        # Convert each float in the sublist to a string and join with commas\n        # Enclose the result in square brackets\n        sublist_strings.append(f\"[{','.join(map(str, sublist))}]\")\n    \n    # Join all sublist strings with commas and enclose in the final brackets\n    final_output_string = f\"[{','.join(sublist_strings)}]\"\n\n    print(final_output_string)\n\nsolve()\n```", "id": "3103026"}, {"introduction": "既然我们已经明白了为何需要同时利用过去和未来的上下文，下一个自然而然的问题就是如何最优地融合这两部分信息。这个练习 [@problem_id:3103018] 将前向和后向的隐藏状态建模为对一个真实潜在信号的带噪观测，并要求你推导出最佳的线性组合权重。通过解决这个问题，你将在 BiRNN 的抽象架构和统计估计理论之间建立起一座桥梁，学会如何根据信息的“可靠性”来赋权。", "problem": "考虑一个双向循环神经网络 (BRNN)，其中时间 $t$ 的前向隐藏状态 $h_t^{\\rightarrow}$ 和后向隐藏状态 $h_t^{\\leftarrow}$ 用于估计一个未观测到的潜信号 $s_t$。假设一个线性高斯生成模型，其中每个隐藏状态都是 $s_t$ 的一个无偏噪声观测：\n$$\nh_t^{\\rightarrow} = s_t + \\epsilon_t^{\\rightarrow}, \\quad h_t^{\\leftarrow} = s_t + \\epsilon_t^{\\leftarrow},\n$$\n其中 $\\epsilon_t^{\\rightarrow}$ 和 $\\epsilon_t^{\\leftarrow}$ 是联合高斯噪声项，其均值为零，方差为 $\\operatorname{Var}(\\epsilon_t^{\\rightarrow}) = \\sigma_{\\rightarrow,t}^{2}$ 和 $\\operatorname{Var}(\\epsilon_t^{\\leftarrow}) = \\sigma_{\\leftarrow,t}^{2}$，协方差为 $\\operatorname{Cov}(\\epsilon_t^{\\rightarrow}, \\epsilon_t^{\\leftarrow}) = c_t$。考虑一个线性融合估计器\n$$\n\\hat{s}_t(g_t) = g_t\\, h_t^{\\rightarrow} + \\left(1 - g_t\\right) h_t^{\\leftarrow},\n$$\n其中标量融合权重为 $g_t \\in \\mathbb{R}$。从均方误差 (MSE) 的定义出发，推导在上述模型下最小化期望平方误差 $\\mathbb{E}\\big[(\\hat{s}_t(g_t) - s_t)^{2}\\big]$ 的 $g_t$ 值。\n\n然后，使用以下噪声统计数据评估推导出的表达式：\n$$\n\\sigma_{\\rightarrow,t}^{2} = 0.36, \\quad \\sigma_{\\leftarrow,t}^{2} = 0.81, \\quad c_t = 0.12.\n$$\n另外，假设 BRNN 中的一个学习门控机制定义为\n$$\ng_t^{\\mathrm{learn}} = \\sigma\\!\\left(w_{\\rightarrow} h_t^{\\rightarrow} + w_{\\leftarrow} h_t^{\\leftarrow} + b\\right),\n$$\n其中 $\\sigma(x)$ 是 logistic sigmoid 函数 $\\sigma(x) = \\frac{1}{1 + \\exp(-x)}$。对于特定的时间 $t$，令\n$$\nh_t^{\\rightarrow} = 1.4, \\quad h_t^{\\leftarrow} = 0.6, \\quad w_{\\rightarrow} = 0.5, \\quad w_{\\leftarrow} = 0.25, \\quad b = \\ln(3) - 0.85.\n$$\n计算 $g_t^{\\mathrm{learn}}$ 并将其与您上面推导的解析最优 $g_t$ 进行比较。您的最终答案必须是绝对差\n$$\n\\left|g_t^{\\mathrm{learn}} - g_t^{\\star}\\right|,\n$$\n以纯数字形式表示。将您的最终答案四舍五入到四位有效数字。", "solution": "用户想要在双向循环神经网络 (BRNN) 的背景下，找到一个最优推导的融合权重和一个学习到的门值之间的绝对差。\n\n首先，我将验证问题陈述。\n\n### 第1步：提取已知条件\n- 前向隐藏状态：$h_t^{\\rightarrow}$\n- 后向隐藏状态：$h_t^{\\leftarrow}$\n- 潜信号：$s_t$\n- 生成模型：$h_t^{\\rightarrow} = s_t + \\epsilon_t^{\\rightarrow}$ 和 $h_t^{\\leftarrow} = s_t + \\epsilon_t^{\\leftarrow}$。\n- 噪声属性：$\\epsilon_t^{\\rightarrow}, \\epsilon_t^{\\leftarrow}$ 是联合高斯分布，其 $\\mathbb{E}[\\epsilon_t^{\\rightarrow}] = 0$, $\\mathbb{E}[\\epsilon_t^{\\leftarrow}] = 0$, $\\operatorname{Var}(\\epsilon_t^{\\rightarrow}) = \\sigma_{\\rightarrow,t}^{2}$, $\\operatorname{Var}(\\epsilon_t^{\\leftarrow}) = \\sigma_{\\leftarrow,t}^{2}$，以及 $\\operatorname{Cov}(\\epsilon_t^{\\rightarrow}, \\epsilon_t^{\\leftarrow}) = c_t$。\n- 线性融合估计器：$\\hat{s}_t(g_t) = g_t\\, h_t^{\\rightarrow} + \\left(1 - g_t\\right) h_t^{\\leftarrow}$，其中 $g_t \\in \\mathbb{R}$。\n- 目标：最小化均方误差 (MSE)，$\\mathbb{E}\\big[(\\hat{s}_t(g_t) - s_t)^{2}\\big]$。\n- 噪声统计的数值：$\\sigma_{\\rightarrow,t}^{2} = 0.36$, $\\sigma_{\\leftarrow,t}^{2} = 0.81$, $c_t = 0.12$。\n- 学习门：$g_t^{\\mathrm{learn}} = \\sigma\\!\\left(w_{\\rightarrow} h_t^{\\rightarrow} + w_{\\leftarrow} h_t^{\\leftarrow} + b\\right)$，其中 $\\sigma(x) = \\frac{1}{1 + \\exp(-x)}$。\n- 学习门的数值：$h_t^{\\rightarrow} = 1.4$, $h_t^{\\leftarrow} = 0.6$, $w_{\\rightarrow} = 0.5$, $w_{\\leftarrow} = 0.25$, $b = \\ln(3) - 0.85$。\n- 最终答案：计算 $|g_t^{\\mathrm{learn}} - g_t^{\\star}|$，四舍五入到四位有效数字。\n\n### 第2步：使用提取的已知条件进行验证\n该问题在科学上基于统计估计理论及其在神经网络中的应用。线性高斯模型是分析的标准框架。最小化 MSE 以找到最优线性估计器是一个基本且适定（well-posed）的问题。学习门的结构是 RNN 中门控机制的典型结构。所有必要的常数和条件都已提供，并且没有矛盾。该问题是客观且可形式化的。\n\n### 第3步：结论和行动\n问题是有效的。我现在将着手解决。\n\n问题的第一部分是推导最小化 MSE 的最优融合权重 $g_t^{\\star}$。估计误差由 $e_t = \\hat{s}_t(g_t) - s_t$ 给出。\n代入 $\\hat{s}_t$、$h_t^{\\rightarrow}$ 和 $h_t^{\\leftarrow}$ 的表达式：\n$$\ne_t = \\left( g_t h_t^{\\rightarrow} + (1 - g_t) h_t^{\\leftarrow} \\right) - s_t\n$$\n$$\ne_t = g_t (s_t + \\epsilon_t^{\\rightarrow}) + (1 - g_t) (s_t + \\epsilon_t^{\\leftarrow}) - s_t\n$$\n$$\ne_t = g_t s_t + g_t \\epsilon_t^{\\rightarrow} + s_t - g_t s_t + \\epsilon_t^{\\leftarrow} - g_t \\epsilon_t^{\\leftarrow} - s_t\n$$\n$$\ne_t = g_t \\epsilon_t^{\\rightarrow} + (1 - g_t) \\epsilon_t^{\\leftarrow}\n$$\nMSE 是平方误差的期望值，$\\mathbb{E}[e_t^2]$。\n$$\n\\text{MSE}(g_t) = \\mathbb{E}\\left[ \\left( g_t \\epsilon_t^{\\rightarrow} + (1 - g_t) \\epsilon_t^{\\leftarrow} \\right)^2 \\right]\n$$\n展开平方项并利用期望的线性性质：\n$$\n\\text{MSE}(g_t) = g_t^2 \\mathbb{E}[(\\epsilon_t^{\\rightarrow})^2] + (1 - g_t)^2 \\mathbb{E}[(\\epsilon_t^{\\leftarrow})^2] + 2 g_t (1 - g_t) \\mathbb{E}[\\epsilon_t^{\\rightarrow} \\epsilon_t^{\\leftarrow}]\n$$\n鉴于噪声项的均值为零，这些期望对应于方差和协方差：\n$$\n\\mathbb{E}[(\\epsilon_t^{\\rightarrow})^2] = \\operatorname{Var}(\\epsilon_t^{\\rightarrow}) = \\sigma_{\\rightarrow,t}^2\n$$\n$$\n\\mathbb{E}[(\\epsilon_t^{\\leftarrow})^2] = \\operatorname{Var}(\\epsilon_t^{\\leftarrow}) = \\sigma_{\\leftarrow,t}^2\n$$\n$$\n\\mathbb{E}[\\epsilon_t^{\\rightarrow} \\epsilon_t^{\\leftarrow}] = \\operatorname{Cov}(\\epsilon_t^{\\rightarrow}, \\epsilon_t^{\\leftarrow}) = c_t\n$$\n因此，作为 $g_t$ 函数的 MSE 是：\n$$\n\\text{MSE}(g_t) = g_t^2 \\sigma_{\\rightarrow,t}^2 + (1 - g_t)^2 \\sigma_{\\leftarrow,t}^2 + 2 g_t (1 - g_t) c_t\n$$\n为了找到最小化 MSE 的 $g_t$ 值，我们对 $g_t$ 求导并将导数设为零：\n$$\n\\frac{d}{dg_t}\\text{MSE}(g_t) = 2 g_t \\sigma_{\\rightarrow,t}^2 + 2 (1 - g_t)(-1) \\sigma_{\\leftarrow,t}^2 + (2 g_t(1) - 2 g_t^2)' c_t\n$$\n$$\n\\frac{d}{dg_t}\\text{MSE}(g_t) = 2 g_t \\sigma_{\\rightarrow,t}^2 - 2 (1 - g_t) \\sigma_{\\leftarrow,t}^2 + 2(1 - 2g_t) c_t\n$$\n将导数设为零：\n$$\n2 g_t \\sigma_{\\rightarrow,t}^2 - 2 \\sigma_{\\leftarrow,t}^2 + 2 g_t \\sigma_{\\leftarrow,t}^2 + 2 c_t - 4 g_t c_t = 0\n$$\n将包含 $g_t$ 的项组合在一起：\n$$\ng_t (2 \\sigma_{\\rightarrow,t}^2 + 2 \\sigma_{\\leftarrow,t}^2 - 4 c_t) = 2 \\sigma_{\\leftarrow,t}^2 - 2 c_t\n$$\n求解最优权重 $g_t^{\\star}$：\n$$\ng_t^{\\star} = \\frac{2 \\sigma_{\\leftarrow,t}^2 - 2 c_t}{2 \\sigma_{\\rightarrow,t}^2 + 2 \\sigma_{\\leftarrow,t}^2 - 4 c_t} = \\frac{\\sigma_{\\leftarrow,t}^2 - c_t}{\\sigma_{\\rightarrow,t}^2 + \\sigma_{\\leftarrow,t}^2 - 2c_t}\n$$\n接下来，我们使用给定的噪声统计数据评估此表达式：$\\sigma_{\\rightarrow,t}^{2} = 0.36$，$\\sigma_{\\leftarrow,t}^{2} = 0.81$ 和 $c_t = 0.12$。\n$$\ng_t^{\\star} = \\frac{0.81 - 0.12}{0.36 + 0.81 - 2(0.12)} = \\frac{0.69}{1.17 - 0.24} = \\frac{0.69}{0.93} = \\frac{69}{93} = \\frac{23}{31}\n$$\n现在，我们计算学习门 $g_t^{\\mathrm{learn}}$ 的值。sigmoid 函数的输入是 $z = w_{\\rightarrow} h_t^{\\rightarrow} + w_{\\leftarrow} h_t^{\\leftarrow} + b$。\n使用所提供的值 $h_t^{\\rightarrow} = 1.4$, $h_t^{\\leftarrow} = 0.6$, $w_{\\rightarrow} = 0.5$, $w_{\\leftarrow} = 0.25$ 和 $b = \\ln(3) - 0.85$：\n$$\nz = (0.5)(1.4) + (0.25)(0.6) + (\\ln(3) - 0.85)\n$$\n$$\nz = 0.7 + 0.15 + \\ln(3) - 0.85\n$$\n$$\nz = 0.85 + \\ln(3) - 0.85 = \\ln(3)\n$$\n门的值是 $g_t^{\\mathrm{learn}} = \\sigma(z) = \\sigma(\\ln(3))$。\n$$\ng_t^{\\mathrm{learn}} = \\frac{1}{1 + \\exp(-\\ln(3))} = \\frac{1}{1 + \\exp(\\ln(3^{-1}))} = \\frac{1}{1 + \\frac{1}{3}} = \\frac{1}{\\frac{4}{3}} = \\frac{3}{4}\n$$\n最后，我们计算绝对差 $|g_t^{\\mathrm{learn}} - g_t^{\\star}|$。\n$$\n\\left| \\frac{3}{4} - \\frac{23}{31} \\right| = \\left| \\frac{3 \\times 31}{4 \\times 31} - \\frac{23 \\times 4}{31 \\times 4} \\right| = \\left| \\frac{93 - 92}{124} \\right| = \\frac{1}{124}\n$$\n为了提供最终答案，我们将这个分数转换为小数，并四舍五入到四位有效数字。\n$$\n\\frac{1}{124} \\approx 0.008064516...\n$$\n第一个有效数字是 $8$。前四位有效数字是 $8$，$0$，$6$ 和 $4$。下一位数字是 $5$，所以我们将最后一位有效数字向上舍入。\n结果是 $0.008065$。", "answer": "$$\\boxed{0.008065}$$", "id": "3103018"}]}