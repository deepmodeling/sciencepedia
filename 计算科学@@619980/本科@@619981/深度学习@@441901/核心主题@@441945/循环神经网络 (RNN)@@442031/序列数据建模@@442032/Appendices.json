{"hands_on_practices": [{"introduction": "“注意力” (Attention) 机制是现代序列模型（如 Transformer）的核心。为了直观地理解其工作原理，本练习将通过一个简单的序列反转任务来动手实现一个基础的注意力模型。通过对比全双向注意力 (bidirectional attention) 和因果受限注意力 (causal attention) 的表现，我们将揭示模型能够“看到”的上下文范围对于解决序列任务的决定性影响 [@problem_id:3153617]。", "problem": "考虑一个有限的实值序列，建模为一个时间有序集合 $\\{x_t\\}_{t=1}^T$，其中 $T$ 是一个正整数，且对于所有 $t \\in \\{1,\\dots,T\\}$，$x_t \\in \\mathbb{R}$。目标映射是由 $y_{1:T}$ 定义的可逆序列，其中对于所有 $t \\in \\{1,\\dots,T\\}$，$y_t = x_{T - t + 1}$。您将实现并评估两种编码器-解码器注意力变体，以测试在建模此映射时对双向上下文的利用情况，并使用一个基于标准定义的单头交叉注意力机制。\n\n使用的基本依据和定义：\n- 设编码器从输入序列 $\\{x_i\\}_{i=1}^T$ 生成键值对 $\\{(k_i, v_i)\\}_{i=1}^T$，其中 $k_i$ 是一个标量键，$v_i$ 是一个标量值。对于此任务，设定对于所有 $i \\in \\{1,\\dots,T\\}$，$k_i = i$ 且 $v_i = x_i$，因此键代表位置，值承载内容。\n- 设解码器在时间 $t$ 生成一个标量查询 $q_t$，该查询仅依赖于已知的序列长度 $T$ 和当前的时间索引 $t$。对于此任务，设定对于所有 $t \\in \\{1,\\dots,T\\}$，$q_t = T - t + 1$，以在索引空间中表示反转所需的索引对齐。\n- 使用基于能量函数的标准 Softmax 加权来计算注意力权重。将解码器查询 $q_t$ 和编码器键 $k_i$ 之间的能量 $e_{t,i}$ 定义为索引空间中负平方距离，并由一个正的锐度参数 $\\alpha$ 缩放，具体为 $e_{t,i} = -\\alpha \\cdot (k_i - q_t)^2$，其中 $\\alpha$ 固定为 20。在允许的掩码集 $\\mathcal{M}_t \\subseteq \\{1,\\dots,T\\}$ 上，时间 $t$ 的注意力权重为：对于 $i \\in \\mathcal{M}_t$，$w_{t,i} = \\frac{\\exp(e_{t,i})}{\\sum_{j \\in \\mathcal{M}_t} \\exp(e_{t,j})}$；对于 $i \\notin \\mathcal{M}_t$，$w_{t,i} = 0$。\n- 时间 $t$ 的预测输出是注意力加权和 $\\hat{y}_t = \\sum_{i \\in \\mathcal{M}_t} w_{t,i} \\cdot v_i$。\n- 必须评估两种交叉注意力掩码变体：\n  1. 全双向交叉注意力：对于所有 $t$，$\\mathcal{M}_t = \\{1,2,\\dots,T\\}$。\n  2. 因果受限交叉注意力：对于所有 $t$，$\\mathcal{M}_t = \\{1,2,\\dots,t\\}$，因此解码器在时间 $t$ 只能关注到（并包括）位置 $t$ 为止的编码器位置。\n\n对于每个测试用例，计算在每种掩码变体下，预测与目标反转序列之间的均方误差 (MSE)：\n$$E_{\\text{full}} = \\frac{1}{T} \\sum_{t=1}^T \\left(\\hat{y}_t^{\\text{full}} - y_t\\right)^2,$$\n$$E_{\\text{causal}} = \\frac{1}{T} \\sum_{t=1}^T \\left(\\hat{y}_t^{\\text{causal}} - y_t\\right)^2.$$\n\n您的程序必须实现此评估，并以下面指定的精确最终输出格式生成结果。\n\n测试套件：\n使用以下测试序列来覆盖一般情况、符号变化、边界情况和交替符号幅度：\n- 案例 $\\mathsf{A}$：$T = 6$, $x_{1:6} = [\\,1,\\,2,\\,3,\\,4,\\,5,\\,6\\,]$。\n- 案例 $\\mathsf{B}$：$T = 5$, $x_{1:5} = [\\,3,\\,-1,\\,0.5,\\,7,\\,2\\,]$。\n- 案例 $\\mathsf{C}$：$T = 1$, $x_{1:1} = [\\,42\\,]$。\n- 案例 $\\mathsf{D}$：$T = 8$, $x_{1:8} = [\\,0.1,\\,-0.2,\\,0.3,\\,-0.4,\\,0.5,\\,-0.6,\\,0.7,\\,-0.8\\,]$。\n\n最终输出格式：\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的聚合结果。对于每个案例，按 $\\mathsf{A}, \\mathsf{B}, \\mathsf{C}, \\mathsf{D}$ 的顺序，输出 $E_{\\text{full}}$，然后是 $E_{\\text{causal}}$。因此，最终输出必须是以下形式\n$[\\,E_{\\text{full}}^{\\mathsf{A}},\\,E_{\\text{causal}}^{\\mathsf{A}},\\,E_{\\text{full}}^{\\mathsf{B}},\\,E_{\\text{causal}}^{\\mathsf{B}},\\,E_{\\text{full}}^{\\mathsf{C}},\\,E_{\\text{causal}}^{\\mathsf{C}},\\,E_{\\text{full}}^{\\mathsf{D}},\\,E_{\\text{causal}}^{\\mathsf{D}}\\,]$。\n所有输出必须是 $\\mathbb{R}$ 中的实值浮点数。", "solution": "该问题被评估为有效。它在科学上基于深度学习中注意力机制的原理，问题定义良好，所有必要的参数和函数都有定义，并且陈述客观。满足有效问题的所有条件。\n\n任务是使用简化的单头注意力机制对序列反转映射 $y_t = x_{T-t+1}$ 进行建模。我们评估两种变体：一种具有完全双向上下文，另一种具有因果限制。\n\n注意力机制的核心是在每个时间步 $t$ 计算输出 $\\hat{y}_t$，作为输入值 $\\{v_i\\}_{i=1}^T$ 的加权平均。权重 $w_{t,i}$ 决定了在步骤 $t$ 的解码器对步骤 $i$ 的编码器输出给予多少“关注”。\n\n各组成部分定义如下：\n-   输入序列 $\\{x_i\\}_{i=1}^T$ 提供值，因此对于 $i \\in \\{1, \\dots, T\\}$，$v_i = x_i$。\n-   键由输入序列中的位置定义，$k_i = i$。\n-   每个解码器时间步 $t$ 的查询定义为 $q_t = T - t + 1$。这个定义至关重要，因为 $q_t$ 表示原始序列中对应于时间步 $t$ 目标值的索引；即，$y_t = x_{T-t+1} = v_{T-t+1} = v_{q_t}$。因此，查询直接指定了正确信息的位置。\n\n注意力权重 $w_{t,i}$ 通过 softmax 函数从能量函数 $e_{t,i}$ 导出。能量定义为键和查询索引之间经过缩放的负平方距离：\n$$e_{t,i} = -\\alpha \\cdot (k_i - q_t)^2$$\n锐度参数 $\\alpha = 20$。对于允许的索引集 $\\mathcal{M}_t$ 的权重是：\n$$w_{t,i} = \\frac{\\exp(e_{t,i})}{\\sum_{j \\in \\mathcal{M}_t} \\exp(e_{t,j})}$$\n预测输出即为加权和：\n$$\\hat{y}_t = \\sum_{i \\in \\mathcal{M}_t} w_{t,i} \\cdot v_i$$\n\n**1. 全双向交叉注意力**\n\n在此变体中，对于所有 $t \\in \\{1, \\dots, T\\}$，掩码为 $\\mathcal{M}_t^{\\text{full}} = \\{1, 2, \\dots, T\\}$。这意味着解码器可以在任何时间关注输入序列中的任何位置。\n\n对于任何给定的 $t$，查询是 $q_t$。当 $k_i = q_t$ 时，即当键索引 $i$ 与查询索引 $q_t$ 完全匹配时，能量 $e_{t,i}$ 达到其最大值 0。对于所有其他键 $k_i \\neq q_t$，能量为负。鉴于 $\\alpha=20$ 的值很大，$\\exp(e_{t,i})$ 项将在 $i = q_t$ 处呈现尖锐的峰值。因此，softmax 函数将产生一个权重 $w_{t, q_t} \\approx 1$，而对于所有 $i \\neq q_t$，$w_{t,i} \\approx 0$。\n\n因此，预测输出为：\n$$\\hat{y}_t^{\\text{full}} = \\sum_{i=1}^T w_{t,i} v_i \\approx 1 \\cdot v_{q_t} + \\sum_{i \\neq q_t} 0 \\cdot v_i = v_{q_t}$$\n代入 $v_i$ 和 $q_t$ 的定义，我们得到：\n$$\\hat{y}_t^{\\text{full}} \\approx v_{T-t+1} = x_{T-t+1} = y_t$$\n预测 $\\hat{y}_t^{\\text{full}}$ 是真实目标 $y_t$ 的一个非常接近的近似。唯一的误差来源是来自其他键的非零贡献，由于 softmax 的锐度，这个贡献极小。因此，对于所有测试用例，均方误差 $E_{\\text{full}}$ 预计将接近于零。\n\n**2. 因果受限交叉注意力**\n\n在此变体中，掩码为 $\\mathcal{M}_t^{\\text{causal}} = \\{1, 2, \\dots, t\\}$。解码器在时间 $t$ 只能关注输入序列的前 $t$ 个位置。这模拟了一种情景，即系统相对于其自身的处理时间，无法“看到”输入序列的“未来”。\n\n分析取决于目标键 $q_t$ 是否在允许的掩码 $\\mathcal{M}_t^{\\text{causal}}$ 内。\n-   **情况 1：$q_t \\in \\mathcal{M}_t^{\\text{causal}}$**。这等价于 $T - t + 1 \\le t$，可简化为 $t \\ge (T+1)/2$。对于解码过程的后半部分，目标键是可访问的。与全注意力情况一样，该机制将正确地聚焦于 $k_{q_t}$，从而得出准确的预测 $\\hat{y}_t^{\\text{causal}} \\approx y_t$。\n\n-   **情况 2：$q_t \\notin \\mathcal{M}_t^{\\text{causal}}$**。这种情况发生在 $t  (T+1)/2$ 时。对于解码过程的前半部分，索引正确值的键 $q_t$ 不在允许的集合 $\\{1, \\dots, t\\}$ 中。注意力机制必须从可用集合中选择一个键。对于 $i \\in \\{1, \\dots, t\\}$，能量 $e_{t,i} = -\\alpha (i - q_t)^2$ 在距离 $|i - q_t|$ 最小时最大化（即负得最少）。由于在这种情况下 $q_t > t$，最近的可用键是 $i=t$。因此，softmax 权重将高度集中在最后一个可用键 $i=t$ 上。\n\n预测变为：\n$$\\hat{y}_t^{\\text{causal}} \\approx v_t = x_t$$\n然而，目标是 $y_t = x_{T-t+1}$。除非输入序列恰好是对称的（$x_t = x_{T-t+1}$），否则预测将与目标有很大差异。对于一般序列，这种差异将导致对于每个 $t  (T+1)/2$ 产生一个大的平方误差 $(\\hat{y}_t^{\\text{causal}} - y_t)^2$。\n\n因此，均方误差 $E_{\\text{causal}}$ 将显著大于零，误差贡献主要来自初始时间步。\n\n对于 $T=1$ 的特殊情况，条件 $t \\ge (1+1)/2$ 简化为 $t \\ge 1$。由于 $t=1$ 是唯一的时间步，情况 1 总是适用。因果和全注意力掩码是相同的：$\\mathcal{M}_1^{\\text{full}} = \\{1\\}$ 和 $\\mathcal{M}_1^{\\text{causal}} = \\{1\\}$。两种变体的计算是相同的，得到 $\\hat{y}_1 = v_1 = y_1$。因此，对于 $T=1$，$E_{\\text{full}}$ 和 $E_{\\text{causal}}$ 都将精确为 0。\n\n以下程序实现了此逻辑，以计算指定测试用例的误差。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes sequence reversal errors for two attention variants.\n    \"\"\"\n\n    def compute_errors(x_seq: list[float]) - tuple[float, float]:\n        \"\"\"\n        Calculates MSE for full and causal attention on a sequence.\n\n        Args:\n            x_seq: The input sequence of real numbers.\n\n        Returns:\n            A tuple containing (E_full, E_causal).\n        \"\"\"\n        T = len(x_seq)\n        if T == 0:\n            return 0.0, 0.0\n            \n        x_seq_np = np.array(x_seq, dtype=float)\n        alpha = 20.0\n\n        # Define keys, values, and target sequence based on problem statement.\n        # k are 1-based indices.\n        k = np.arange(1, T + 1)\n        v = x_seq_np\n        y_target = x_seq_np[::-1]\n\n        y_hat_full = np.zeros(T)\n        y_hat_causal = np.zeros(T)\n\n        for t_idx in range(T):\n            t = t_idx + 1  # 1-based time index t\n            q_t = T - t + 1\n\n            # 1. Full bidirectional cross-attention\n            # The decoder can attend to all encoder positions {1, ..., T}.\n            energies_full = -alpha * (k - q_t)**2\n            # Use numerically stable softmax by shifting energies.\n            stable_energies_full = energies_full - np.max(energies_full)\n            weights_full = np.exp(stable_energies_full) / np.sum(np.exp(stable_energies_full))\n            y_hat_full[t_idx] = np.sum(weights_full * v)\n\n            # 2. Causally restricted cross-attention\n            # The decoder at time t can only attend to encoder positions {1, ..., t}.\n            k_causal = k[:t]\n            v_causal = v[:t]\n            energies_causal = -alpha * (k_causal - q_t)**2\n            # Use numerically stable softmax.\n            stable_energies_causal = energies_causal - np.max(energies_causal)\n            weights_causal = np.exp(stable_energies_causal) / np.sum(np.exp(stable_energies_causal))\n            y_hat_causal[t_idx] = np.sum(weights_causal * v_causal)\n\n        # Compute Mean Squared Error (MSE) for both variants.\n        mse_full = np.mean((y_hat_full - y_target)**2)\n        mse_causal = np.mean((y_hat_causal - y_target)**2)\n\n        return mse_full, mse_causal\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        ([1, 2, 3, 4, 5, 6]),                             # Case A\n        ([3, -1, 0.5, 7, 2]),                             # Case B\n        ([42]),                                           # Case C\n        ([0.1, -0.2, 0.3, -0.4, 0.5, -0.6, 0.7, -0.8])     # Case D\n    ]\n\n    results = []\n    for case in test_cases:\n        E_full, E_causal = compute_errors(case)\n        results.append(E_full)\n        results.append(E_causal)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3153617"}, {"introduction": "并非所有序列任务都能用简单的线性模型解决。本练习构建了一个基于特定基序 (motif) 奇偶性判断的逻辑任务，该任务本质上是非线性可分的。通过对比一个简单的逻辑回归模型和一个能够精确追踪序列状态的算法模型，我们将具体看到为什么处理序列数据通常需要更强大的、能够捕捉复杂状态依赖关系的架构 [@problem_id:3153539]。", "problem": "给定符号集 $\\Sigma = \\{A,B,C,D,X\\}$ 上的有限序列。定义两个基序 (motif) $m_1 = \\text{AB}$ 和 $m_2 = \\text{CD}$。基序的一次出现指该基序作为两个连续符号出现。定义基序之间的邻接为没有任何中间符号的直接相连：子串 $ABCD$ 和 $CDAB$ 是邻接对，其中一个基序紧跟着另一个基序，它们之间没有任何符号。对于任何序列，定义 $c_1$ 为 $m_1$ 的出现次数，$c_2$ 为 $m_2$ 的出现次数，两者都使用以下排除规则进行计数：如果 $m_1$ 的某次出现与 $m_2$ 的某次出现邻接（形成 $ABCD$ 或 $CDAB$），则这两次出现都不被计数；否则，该次出现被正常计数。定义二进制奇偶性 $p_1 = c_1 \\bmod 2$ 和 $p_2 = c_2 \\bmod 2$，并定义标签 $y$ 为这些奇偶性的异或（XOR）值，即 $y = p_1 \\oplus p_2$，其中 $\\oplus$ 表示异或，等价于模 $2$ 加法。\n\n从序列模型和二元分类的基本定义出发，编写一个完整的程序来完成以下任务：\n\n1. 构建一个训练数据集和一个测试数据集，方法如下：每个序列的长度 $L = 32$，从 $\\Sigma$ 中独立且均匀随机地抽取符号构成。使用固定的随机种子以确保可复现性。对于训练集，确保四种奇偶性组合 $(p_1,p_2) \\in \\{(0,0),(0,1),(1,0),(1,1)\\}$ 完全均衡，总共 $N_{\\text{train}} = 800$ 个序列（每种奇偶性组合 $200$ 个）。对于测试集，确保四种奇偶性组合完全均衡，总共 $N_{\\text{test}} = 200$ 个序列（每种奇偶性组合 $50$ 个）。\n\n2. 将模型 A 实现为一个使用逻辑回归的线性分类器。序列的特征向量为 $x = [p_1, p_2, 1]$，其中 $1$ 是一个偏置项。将权重向量 $w$ 初始化为零向量，并对带有 $\\ell_2$ 正则化系数 $\\lambda$ 和学习率 $\\eta$ 的平均二元交叉熵执行梯度下降。使用 $E = 50$ 个周期，$\\eta = 0.1$，$\\lambda = 0.0$。由于数据完全均衡且 $w$ 从零开始，您的实现应尊重基本公式所隐含的驻点属性。如果逻辑输出严格大于 $0.5$，则预测 $\\hat{y} = 1$，否则预测 $\\hat{y} = 0$。\n\n3. 将模型 B 实现为一个序列状态更新模型，该模型从左到右扫描序列，并根据邻接排除规则更新反映 $p_1$ 和 $p_2$ 的两个内部奇偶性状态。该模型必须能够在两个基序被检测为邻接时（对于 $ABCD$ 和 $CDAB$ 两种情况）取消奇偶性的增量，从而对依赖于序列中相对位置的跨基序相互作用进行建模。扫描完序列后，它输出预测标签 $\\hat{y} = p_1 \\oplus p_2$。\n\n4. 通过计算训练准确率（正确预测标签的比例，以浮点数表示）和测试准确率来评估两个模型。此外，在以下边界情况序列上评估两个模型，报告它们的预测标签（以整数表示）：\n   - $s_1 = \\text{\"XXXX\"}$（无基序），\n   - $s_2 = \\text{\"ABCD\"}$（必须被排除的邻接基序），\n   - $s_3 = \\text{\"ABXXXX\"}$（仅 $m_1$ 出现一次，非邻接），\n   - $s_4 = \\text{\"XXXXCD\"}$（仅 $m_2$ 出现一次，非邻接）。\n\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，按以下顺序排列结果：\n$[$模型 A 训练准确率, 模型 A 测试准确率, 模型 B 训练准确率, 模型 B 测试准确率, 模型 A 对 $s_1$ 的预测, 模型 B 对 $s_1$ 的预测, 模型 A 对 $s_2$ 的预测, 模型 B 对 $s_2$ 的预测, 模型 A 对 $s_3$ 的预测, 模型 B 对 $s_3$ 的预测, 模型 A 对 $s_4$ 的预测, 模型 B 对 $s_4$ 的预测$]$。\n\n所有数学实体必须用 LaTeX 书写。不涉及物理单位或角度单位。输出的准确率为浮点数，预测值为整数。问题必须在没有外部数据或用户输入的情况下解决，并且程序必须遵守指定的执行环境。", "solution": "该问题要求对用于序列数据二元分类任务的两个模型进行比较分析。第一步，对建模和评估都至关重要，是严格定义将序列映射到其真实标签的函数。\n\n**1. 真实标签计算**\n\n序列的标签 $y$ 由基序计数的奇偶性决定，即 $y = p_1 \\oplus p_2$，其中 $p_1 = c_1 \\bmod 2$，$p_2 = c_2 \\bmod 2$。计数 $c_1$（对于基序 $m_1 = \\text{AB}$）和 $c_2$（对于基序 $m_2 = \\text{CD}$）遵循一个关键的排除规则：如果 $m_1$ 的一次出现与 $m_2$ 的一次出现紧邻，形成 $ABCD$ 或 $CDAB$，那么在该特定邻接中的 $m_1$ 和 $m_2$ 出现都将作废，不计入总数。\n\n为了正确实现此规则，必须以优先检测这些邻接对的方式处理序列。贪心的从左到右扫描是一种合适的算法。我们遍历序列，在每个位置 $i$：\n1.  检查长度为四个字符的子串 $ABCD$ 或 $CDAB$。如果找到，我们通过将扫描指针前移 $4$ 个位置来使两个基序均无效。\n2.  如果没有找到这样的邻接对，我们检查长度为两个字符的基序 $m_1 = \\text{AB}$ 或 $m_2 = \\text{CD}$。如果找到，我们增加相应的计数器 $c_1$ 或 $c_2$。\n3.  然后将扫描指针前移 $1$ 个位置以继续搜索。\n\n在扫描完整个长度为 $L=32$ 的序列后，使用最终的计数 $c_1$ 和 $c_2$ 来计算奇偶性 $p_1$ 和 $p_2$，并随后计算标签 $y$。这整个过程定义了确定性的真实标签函数 $f(\\text{sequence}) = y$。\n\n**2. 数据集构建**\n\n问题指定了创建一个包含 $N_{\\text{train}} = 800$ 个样本的训练集和一个包含 $N_{\\text{test}} = 200$ 个样本的测试集。两个数据集都必须在四种可能的奇偶性组合 $(p_1, p_2) \\in \\{(0,0), (0,1), (1,0), (1,1)\\}$ 上完全均衡。这通过从字母表 $\\Sigma = \\{A,B,C,D,X\\}$ 中随机独立地生成序列，使用上述标签函数计算它们的 $(p_1, p_2)$ 对，并将它们分箱，直到四个箱子中的每一个都达到期望的大小（训练集为 $200$，测试集为 $50$）。固定的随机种子确保了数据集的可复现性。\n\n**3. 模型 A：线性分类器（逻辑回归）**\n\n模型 A 是一个逻辑回归分类器。它对序列的特征表示是向量 $x = [p_1, p_2, 1]$，其中 $p_1$ 和 $p_2$ 是真实的奇偶性，$1$ 是偏置项。模型的预测基于逻辑函数 $\\hat{y}_{\\text{proba}} = \\sigma(w^T x) = (1 + e^{-w^T x})^{-1}$，其中 $w = [w_1, w_2, w_b]$ 是权重向量。如果 $\\hat{y}_{\\text{proba}} > 0.5$，则二元预测为 $\\hat{y} = 1$，否则为 $\\hat{y} = 0$。\n\n该模型使用梯度下降进行训练，以最小化带有 $\\ell_2$ 正则化的平均二元交叉熵损失：\n$$J(w) = -\\frac{1}{N} \\sum_{i=1}^{N} \\left[ y_i \\log(\\hat{y}_i) + (1-y_i)\\log(1-\\hat{y}_i) \\right] + \\frac{\\lambda}{2N} \\|w\\|_2^2$$\n关于权重的梯度是：\n$$\\nabla_w J(w) = \\frac{1}{N} \\sum_{i=1}^{N} (\\hat{y}_i - y_i) x_i + \\frac{\\lambda}{N} w$$\n问题指定在 $w = [0, 0, 0]$ 处初始化，正则化系数 $\\lambda = 0.0$。目标函数是 $y = p_1 \\oplus p_2$。模型看到的四个特征-标签对是：\n- $p_1=0, p_2=0 \\implies x=[0,0,1], y=0$\n- $p_1=0, p_2=1 \\implies x=[0,1,1], y=1$\n- $p_1=1, p_2=0 \\implies x=[1,0,1], y=1$\n- $p_1=1, p_2=1 \\implies x=[1,1,1], y=0$\n\n这是经典的、线性不可分的异或问题。让我们在初始点 $w=[0,0,0]$ 评估梯度。对于任何输入 $x_i$，初始预测为 $\\hat{y}_i = \\sigma(0 \\cdot x_i) = \\sigma(0) = 0.5$。由于数据集是完全均衡的，对于每个特征 $j \\in \\{1, 2, \\text{bias}\\}$，项 $\\sum_{i=1}^{N} (\\hat{y}_i - y_i) x_{ij}$ 由于对称性而总和为零。例如，对于第一个权重 $w_1$，来自四个均衡类别的对梯度的贡献与 $(0.5-0) \\cdot 0 + (0.5-1) \\cdot 0 + (0.5-1) \\cdot 1 + (0.5-0) \\cdot 1 = 0$ 成比例。类似的计算表明 $w_2$ 和 $w_b$ 的梯度也为零。当 $\\lambda=0.0$ 时，正则化项的梯度也为零。\n因此，在 $w=[0,0,0]$ 处，总梯度 $\\nabla_w J(w)$ 是零向量。梯度下降不会更新权重；在所有 $E=50$ 个周期中，它们将保持为 $[0,0,0]$。因此，模型 A 将始终计算出 $\\sigma(0) = 0.5$ 的逻辑输出，根据指定的规则（如果严格大于 $0.5$ 则 $\\hat{y}=0$），这意味着它将总是预测 $\\hat{y} = 0$。在一个 $50\\%$ 的标签为 $y=0$、$50\\%$ 的标签为 $y=1$ 的均衡数据集上，该模型将获得恰好 $0.5$ 的准确率。\n\n**4. 模型 B：序列状态更新模型**\n\n模型 B 被定义为一个序列模型，它直接实现了在邻接排除规则下计算奇偶性的逻辑。该模型并非从统计意义上学习数据；相反，它是真实标签函数 $f(\\text{sequence})$ 的一个完美算法实现。它从左到右扫描序列，维持与 $c_1$ 和 $c_2$ 对应的内部状态，正确处理 $ABCD$ 和 $CDAB$ 对的抵消逻辑。在扫描完整个序列后，它计算最终的奇偶性并输出标签 $\\hat{y} = p_1 \\oplus p_2$。根据其定义，该模型的预测将始终与真实标签 $y$ 相同。因此，它在任何数据集（训练、测试或其他）上的准确率都将是 $1.0$。\n\n**5. 边界情况序列评估**\n\n- $s_1 = \\text{\"XXXX\"}$: 无基序。$c_1=0, c_2=0 \\implies p_1=0, p_2=0 \\implies y=0$。\n  - 模型 A 接收到特征向量 $[0,0,1]$ 并预测 $\\hat{y}=0$。\n  - 模型 B 计算真实标签并预测 $\\hat{y}=0$。\n- $s_2 = \\text{\"ABCD\"}$: 邻接的基序 $m_1$ 和 $m_2$。根据排除规则，两者都不计数。$c_1=0, c_2=0 \\implies p_1=0, p_2=0 \\implies y=0$。\n  - 模型 A 接收到特征向量 $[0,0,1]$ 并预测 $\\hat{y}=0$。\n  - 模型 B 计算真实标签并预测 $\\hat{y}=0$。\n- $s_3 = \\text{\"ABXXXX\"}$: 一个非邻接的 $m_1$。$c_1=1, c_2=0 \\implies p_1=1, p_2=0 \\implies y=1$。\n  - 模型 A 接收到特征向量 $[1,0,1]$ 并预测 $\\hat{y}=0$。\n  - 模型 B 计算真实标签并预测 $\\hat{y}=1$。\n- $s_4 = \\text{\"XXXXCD\"}$: 一个非邻接的 $m_2$。$c_1=0, c_2=1 \\implies p_1=0, p_2=1 \\implies y=1$。\n  - 模型 A 接收到特征向量 $[0,1,1]$ 并预测 $\\hat{y}=0$。\n  - 模型 B 计算真实标签并预测 $\\hat{y}=1$。\n\n这些结果展示了一个简单的线性模型（模型 A）在处理线性不可分问题时的根本局限性，与之相对的是一个算法上正确的有状态模型（模型 B），它完美地捕捉了任务底层的序列依赖关系。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to orchestrate dataset generation, model training, evaluation,\n    and printing the final results.\n    \"\"\"\n    \n    # --- Problem Constants ---\n    L = 32\n    ALPHABET = ['A', 'B', 'C', 'D', 'X']\n    MOTIF1 = \"AB\"\n    MOTIF2 = \"CD\"\n    ADJACENT1 = \"ABCD\"\n    ADJACENT2 = \"CDAB\"\n    SEED = 42\n\n    N_TRAIN_TOTAL = 800\n    N_TEST_TOTAL = 200\n    N_CLASSES = 4\n    N_TRAIN_PER_CLASS = N_TRAIN_TOTAL // N_CLASSES\n    N_TEST_PER_CLASS = N_TEST_TOTAL // N_CLASSES\n    \n    EPOCHS = 50\n    LEARNING_RATE = 0.1\n    LAMBDA_REG = 0.0\n\n    # --- Core Logic for Label Calculation ---\n    def calculate_y_and_parities(sequence):\n        \"\"\"\n        Calculates motif counts (c1, c2), parities (p1, p2), and the final\n        label y for a given sequence, respecting the adjacency exclusion rule.\n        \"\"\"\n        c1, c2 = 0, 0\n        i = 0\n        n = len(sequence)\n        while i  n - 1:\n            if i = n - 4 and sequence[i:i+4] in (ADJACENT1, ADJACENT2):\n                i += 4\n                continue\n            \n            sub = sequence[i:i+2]\n            if sub == MOTIF1:\n                c1 += 1\n            elif sub == MOTIF2:\n                c2 += 1\n            \n            i += 1\n            \n        p1 = c1 % 2\n        p2 = c2 % 2\n        y = p1 ^ p2\n        return p1, p2, y\n\n    # --- Dataset Generation ---\n    def generate_dataset(num_samples_per_class, seed_offset):\n        \"\"\"\n        Generates a balanced dataset with a specified number of samples for each\n        of the four (p1, p2) parity combinations.\n        \"\"\"\n        rng = np.random.default_rng(SEED + seed_offset)\n        bins = {(0, 0): [], (0, 1): [], (1, 0): [], (1, 1): []}\n        \n        num_generated = 0\n        while any(len(b)  num_samples_per_class for b in bins.values()):\n            num_generated += 1\n            seq = \"\".join(rng.choice(ALPHABET, size=L))\n            p1, p2, y = calculate_y_and_parities(seq)\n            parity_pair = (p1, p2)\n            \n            if len(bins[parity_pair])  num_samples_per_class:\n                bins[parity_pair].append({'sequence': seq, 'p1': p1, 'p2': p2, 'y': y})\n\n        dataset = []\n        for key in sorted(bins.keys()):\n            dataset.extend(bins[key])\n            \n        return dataset\n\n    # --- Model A: Logistic Regression ---\n    class LogisticRegression:\n        def __init__(self):\n            self.weights = np.zeros(3, dtype=np.float64)\n\n        def _sigmoid(self, z):\n            # Clip to avoid overflow/underflow in np.exp\n            z_clipped = np.clip(z, -500, 500)\n            return 1.0 / (1.0 + np.exp(-z_clipped))\n\n        def train(self, X, y, eta, epochs, lambda_reg):\n            n_samples = X.shape[0]\n            w = self.weights.copy()\n            \n            for _ in range(epochs):\n                z = X @ w\n                y_hat = self._sigmoid(z)\n                gradient = (1 / n_samples) * X.T @ (y_hat - y) + (lambda_reg / n_samples) * w\n                w -= eta * gradient\n                \n            self.weights = w\n\n        def predict(self, X):\n            z = X @ self.weights\n            y_hat_proba = self._sigmoid(z)\n            return (y_hat_proba > 0.5).astype(int)\n\n    # --- Model B: Sequential State-Update Model ---\n    class SequentialModel:\n        def predict(self, sequences):\n            predictions = []\n            for seq in sequences:\n                _, _, y = calculate_y_and_parities(seq)\n                predictions.append(y)\n            return np.array(predictions)\n            \n    # --- Main Execution ---\n\n    # 1. Construct datasets\n    train_data = generate_dataset(N_TRAIN_PER_CLASS, seed_offset=0)\n    test_data = generate_dataset(N_TEST_PER_CLASS, seed_offset=1)\n\n    # Prepare data for Model A\n    X_train = np.array([[d['p1'], d['p2'], 1] for d in train_data], dtype=np.float64)\n    y_train = np.array([d['y'] for d in train_data], dtype=np.float64)\n    X_test = np.array([[d['p1'], d['p2'], 1] for d in test_data], dtype=np.float64)\n    y_test = np.array([d['y'] for d in test_data], dtype=np.float64)\n    \n    # 2. Evaluate Model A\n    model_a = LogisticRegression()\n    model_a.train(X_train, y_train, eta=LEARNING_RATE, epochs=EPOCHS, lambda_reg=LAMBDA_REG)\n    \n    pred_train_a = model_a.predict(X_train)\n    acc_train_a = np.mean(pred_train_a == y_train)\n\n    pred_test_a = model_a.predict(X_test)\n    acc_test_a = np.mean(pred_test_a == y_test)\n    \n    # 3. Evaluate Model B\n    model_b = SequentialModel()\n    \n    train_sequences = [d['sequence'] for d in train_data]\n    pred_train_b = model_b.predict(train_sequences)\n    acc_train_b = np.mean(pred_train_b == y_train)\n\n    test_sequences = [d['sequence'] for d in test_data]\n    pred_test_b = model_b.predict(test_sequences)\n    acc_test_b = np.mean(pred_test_b == y_test)\n\n    # 4. Evaluate on edge cases\n    edge_cases = [\"XXXX\", \"ABCD\", \"ABXXXX\", \"XXXXCD\"]\n    edge_case_preds = []\n    \n    for seq in edge_cases:\n        p1, p2, _ = calculate_y_and_parities(seq)\n        x_edge = np.array([[p1, p2, 1]], dtype=np.float64)\n        \n        pred_a = model_a.predict(x_edge)[0]\n        pred_b = model_b.predict([seq])[0]\n        \n        edge_case_preds.extend([pred_a, pred_b])\n\n    # 5. Format and print final output\n    results = [\n        acc_train_a, acc_test_a,\n        acc_train_b, acc_test_b,\n    ] + edge_case_preds\n    \n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3153539"}, {"introduction": "模型架构的选择对其学习能力有着深刻的影响。本练习探讨了一个需要捕捉“非局部”依赖关系 (non-local dependencies) 的经典任务：回文子序列检测。我们将分析时间卷积网络 (Temporal Convolutional Network, TCN) 的能力局限，并从理论上理解为什么其有限的感受野 (receptive field) 使其从根本上无法完美解决此类问题 [@problem_id:3153536]。", "problem": "给定一个在字母表 $\\{0,1\\}$ 上的二进制序列 $x_{1:T}$、一个目标长度 $k$，以及一个时间卷积网络 (TCN) 架构的描述，该架构的特征为核大小 $K$、扩张率 $d$（在各层中恒定）和层数 $L$。任务包含两部分：(i) 判断 $x_{1:T}$ 是否包含一个长度至少为 $k$ 的回文子序列；以及 (ii) 分析任何具有由 $K$、$d$ 和 $L$ 决定的有限感受野的 TCN，原则上是否能够仅使用其有限感受野和一个带有时序排列不变聚合的逐位置读出机制，为所有二进制序列实现对 (i) 中属性的精确检测器。\n\n使用的定义和约束：\n- $x_{1:T}$ 的子序列是形如 $x_{i_1},x_{i_2},\\dots,x_{i_m}$ 的任意序列，其中 $1 \\le i_1  i_2  \\cdots  i_m \\le T$。\n- 如果对于所有 $j \\in \\{1,\\dots,m\\}$，都有 $y_j = y_{m-j+1}$，则序列 $y_{1:m}$ 是一个回文。\n- 长度为 $k$ 的回文子序列是 $x_{1:T}$ 的一个子序列 $y_{1:k}$，且该子序列是回文。\n- 一个具有 $L$ 层、核大小 $K$、恒定扩张率 $d$ 且无步幅的一维因果 TCN，在每个输出位置的有限感受野大小 $R$ 由下式给出：\n$$\nR = 1 + (K-1)\\,d\\,L.\n$$\n- 对于架构局限性分析，考虑一个分类器，其计算方式为：将 TCN 应用于 $x_{1:T}$ 以在每个时间索引处生成一个特征，然后进行任意可测量的逐位置读出和跨时间的排列不变规约（例如，最大值算子）。有限感受野约束意味着每个逐位置特征仅依赖于输入的一个长度为 $R$ 的连续片段。\n\n程序要求：\n1. 实现一个正确的算法，在给定 $x_{1:T}$ 的情况下，返回一个布尔值，指示 $x_{1:T}$ 是否包含一个长度至少为 $k$ 的回文子序列。该算法必须从第一性原理推导得出，不依赖任何外部模型。\n2. 使用如上所述的 $K$、$d$ 和 $L$ 计算感受野 $R$，并返回一个如下定义的布尔不可能性证明：如果 $k > R$ 则返回 true（意味着任何此类有限感受野的 TCN，无论参数如何，都无法在所有二进制序列上为该属性实现一个精确的决策函数）；否则返回 false（意味着仅凭有限感受野大小本身并不能排除精确可解性）。\n\n测试套件：\n为以下四种情况提供结果。在每种情况下，输入序列以字面二进制字符串形式给出，$k$、$K$、$d$、$L$ 以整数形式给出。\n- 案例 A (一般情况): $x = \\text{\"0100101101\"}$, $k = 5$, $K = 3$, $d = 1$, $L = 2$。\n- 案例 B (边界情况 $k=1$): $x = \\text{\"101001\"}$, $k = 1$, $K = 2$, $d = 1$, $L = 1$。\n- 案例 C (不可能的目标长度 $k > T$): $x = \\text{\"01010101\"}$, $k = 9$, $K = 3$, $d = 2$, $L = 1$。\n- 案例 D (标签为真但存在有限域限制 $k>R$): $x = \\text{\"0010100\"}$, $k = 6$, $K = 2$, $d = 3$, $L = 1$。\n\n输出规范：\n- 对于每种情况，输出一个数对 $[P,I]$，其中 $P$ 是关于 $x_{1:T}$ 中是否存在长度至少为 $k$ 的回文子序列的布尔答案，而 $I$ 是如上定义的布尔不可能性证明。\n- 您的程序应生成单行输出，其中包含用方括号括起来的逗号分隔的结果列表，每个数对的格式为 $[P,I]$，且无空格。例如：$[[\\text{True},\\text{False}],[\\text{False},\\text{True}],\\dots]$。", "solution": "该问题提出了一个关于二进制序列 $x_{1:T}$ 的双重任务。首先，我们必须判断它是否包含一个长度至少为给定整数 $k$ 的回文子序列。我们将此布尔结果表示为 $P$。其次，我们分析一类特定的时间卷积网络 (TCN) 架构，并判断这样的网络是否从根本上无法为所有可能的序列解决第一个任务。这个决策由一个布尔不可能性证明 $I$ 来体现。该 TCN 架构由核大小 $K$、恒定扩张因子 $d$ 和层数 $L$ 定义。\n\n### 第 (i) 部分：回文子序列的存在性 ($P$)\n\n第一个任务的核心是找出给定序列 $x_{1:T}$ 的最长回文子序列 (LPS) 的长度，并将其与目标长度 $k$ 进行比较。如果 LPS 的长度大于或等于 $k$，则 $P$ 为真；否则为假。\n\n序列分析中的一个基本定理指出，序列 $S$ 的最长回文子序列的长度等于 $S$ 与其反转序列 $S_{rev}$ 的最长公共子序列 (LCS) 的长度。公共子序列是通过从两个序列中删除零个或多个字符形成的。回文子序列正读和反读都相同，所以如果它是 $S$ 的子序列，它也必须是 $S_{rev}$ 的子序列。这就证明了 $\\text{length}(\\text{LPS}(S)) \\le \\text{length}(\\text{LCS}(S, S_{rev}))$。反向的论证，即 $S$ 和 $S_{rev}$ 的一个 LCS 对应于 $S$ 的一个回文子序列，也是一个标准结论，从而证实了长度的相等性。\n\n因此，问题被简化为计算 $x_{1:T}$ 与其反转序列 $x_{rev}$ 的 LCS 的长度。这是一个经典的动态规划问题。设 $x$ 的长度为 $T$。设 $dp[i][j]$ 为 $x$ 的前缀 $x_{1:i}$ 与 $x_{rev}$ 的前缀 $(x_{rev})_{1:j}$ 之间的 LCS 长度。对于从 $1$ 到 $T$ 的 $i$ 和从 $1$ 到 $T$ 的 $j$，递推关系定义如下：\n$$\ndp[i][j] =\n\\begin{cases}\n    1 + dp[i-1][j-1]   \\text{如果 } x_i = (x_{rev})_j \\\\\n    \\max(dp[i-1][j], dp[i][j-1])   \\text{如果 } x_i \\neq (x_{rev})_j\n\\end{cases}\n$$\n边界条件是对于所有 $i$，$dp[i][0] = 0$，以及对于所有 $j$，$dp[0][j] = 0$。最终结果，即 $x$ 的 LPS 的长度，由 $dp[T][T]$ 给出。\n\n布尔值 $P$ 随后由以下条件确定：\n$$P = (\\text{LCS}(x, x_{rev}) \\ge k)$$\n为提高效率，我们可以处理两种边界情况：\n1. 如果 $k > T$，则不可能形成长度为 $k$ 的子序列。因此，$P$ 为假。\n2. 如果 $k \\le 1$ 且 $T > 0$，序列中的单个字符构成一个长度为 1 的回文。因此，$P$ 为真。\n\n### 第 (ii) 部分：TCN 不可能性证明 ($I$)\n\n问题的第二部分关注基于 TCN 的分类器的理论局限性。TCN 架构的特点是其有限感受野，即影响单个输出特征的连续输入窗口的大小。对于一个具有 $L$ 层、核大小 $K$ 和恒定扩张率 $d$ 的因果 TCN，其感受野大小 $R$ 由下式给出：\n$$R = 1 + (K-1) \\cdot d \\cdot L$$\n分类器模型将此 TCN 应用于输入序列，在每个时间步生成特征。这些特征随后通过一个逐位置读出和一个排列不变聚合函数（例如，最大值、总和或平均值）来生成最终的分类结果。\n\n关键的洞见在于，对局部生成特征的排列不变聚合使模型仅对输入序列中存在的尺寸为 $R$ 的局部模式的多重集敏感，而非其时间顺序。然而，回文子序列是一个非局部属性。其构成字符，例如第一个和最后一个，在输入序列中可以被任意大的距离分隔。\n\n如果不可能性证明 $I$ 定义为当 $k > R$ 时为真。这个条件标志着一个根本性的限制。如果我们需要检测的回文子序列的长度 $k$ 大于 TCN 的感受野大小 $R$，那么网络无法保证对所有可能的输入序列做出正确决策。为了证明这一点，可以构造一对序列 $S_1$ 和 $S_2$，具有以下性质：\n1. $S_1$ 包含一个长度至少为 $k$ 的回文子序列。\n2. $S_2$ 不包含长度至少为 $k$ 的回文子序列。\n3. 对于 $S_1$ 和 $S_2$，所有长度为 $R$ 的连续子序列的多重集是相同的。\n\n由于 TCN 分类器的输出仅仅依赖于这些局部窗口的多重集，它将为 $S_1$ 和 $S_2$ 生成相同的输出，不可避免地会错误分类其中一个。例如，一个长度为 $k$ 的回文子序列的第一个和最后一个字符可能位于索引 $i_1$ 和 $i_k$ 处，使得 $i_k - i_1 + 1 > R$。TCN 的任何单个感受野都无法同时观察到这两个字符以检查它们是否匹配。排列不变池化阻止了网络保留连接这些远距离字符所需的位置信息。\n\n因此，如果 $k > R$，该架构类别从根本上无法可靠地解决这个问题。布尔证明 $I$ 计算如下：\n$$I = (k > R)$$\n如果 $I$ 为真，它证明了这种不可能性。如果 $I$ 为假，则意味着感受野足够大，有可能捕获一个长度为 $k$ 的回文，所以这一特定的推理路线并不能排除可解性。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem for the given test suite.\n    For each case, it computes two boolean values:\n    P: Whether a palindromic subsequence of length at least k exists.\n    I: The impossibility certificate based on the TCN's receptive field.\n    \"\"\"\n    \n    # (x, k, K, d, L)\n    test_cases = [\n        (\"0100101101\", 5, 3, 1, 2),  # Case A\n        (\"101001\", 1, 2, 1, 1),      # Case B\n        (\"01010101\", 9, 3, 2, 1),   # Case C\n        (\"0010100\", 6, 2, 3, 1),     # Case D\n    ]\n\n    results = []\n    for case in test_cases:\n        result = _solve_case(*case)\n        # Format the boolean as \"True\" or \"False\" with capital T/F\n        results.append(f\"[{str(result[0])},{str(result[1])}]\")\n    \n    print(f\"[{','.join(results)}]\")\n\ndef _lcs_length(s1: str, s2: str) -> int:\n    \"\"\"\n    Computes the length of the Longest Common Subsequence of two strings.\n    \"\"\"\n    m, n = len(s1), len(s2)\n    \n    # dp[i][j] will store the length of LCS of s1[0...i-1] and s2[0...j-1]\n    dp = np.zeros((m + 1, n + 1), dtype=int)\n    \n    for i in range(1, m + 1):\n        for j in range(1, n + 1):\n            if s1[i-1] == s2[j-1]:\n                dp[i, j] = 1 + dp[i-1, j-1]\n            else:\n                dp[i, j] = max(dp[i-1, j], dp[i, j-1])\n                \n    return dp[m, n]\n\ndef _solve_case(x: str, k: int, K: int, d: int, L: int) -> list[bool, bool]:\n    \"\"\"\n    Processes a single test case.\n    \n    Returns:\n        A list [P, I] where P is the palindromic subsequence existence flag\n        and I is the impossibility certificate.\n    \"\"\"\n    \n    # Part 1: Palindromic subsequence existence (P)\n    T = len(x)\n    \n    p_exists = False\n    if k > T:\n        # A subsequence cannot be longer than the sequence itself.\n        p_exists = False\n    elif k = 1:\n        # Any non-empty sequence has a palindromic subsequence of length 1.\n        # An empty sequence has no subsequence. The problem assumes x_1:T.\n        p_exists = T > 0\n    else:\n        # General case: Length of Longest Palindromic Subsequence >= k.\n        # This is equivalent to LCS(x, reverse(x)) >= k.\n        x_rev = x[::-1]\n        lps_len = _lcs_length(x, x_rev)\n        p_exists = lps_len >= k\n        \n    # Part 2: Impossibility certificate (I)\n    # R = 1 + (K-1) * d * L\n    receptive_field = 1 + (K - 1) * d * L\n    is_impossible = k > receptive_field\n    \n    return [p_exists, is_impossible]\n\nsolve()\n```", "id": "3153536"}]}