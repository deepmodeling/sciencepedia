{"hands_on_practices": [{"introduction": "理论上，堆叠循环神经网络（RNN）的深度赋予了其构建层次化表征的能力，但这具体是如何运作的呢？本练习通过一个明确的类比来揭示这一机制：我们将一个确定性的、按时钟周期工作的堆叠RNN设计为一棵二叉树归约电路。通过解决分层奇偶校验和多数投票这两个任务，你将亲手实现并观察到，RNN的层级深度如何直接对应于布尔电路的深度，从而直观地理解深层网络如何逐步构建复杂的计算。[@problem_id:3176027]", "problem": "要求您形式化并实现一个确定性的、时钟控制的、堆叠式递归神经网络 (RNN)，该网络模拟对一维二进制输入序列的二叉树归约操作。该模型应用于解决两个任务，处理长度为2的幂的序列：一个是分层奇偶校验任务，另一个是分层多数决策任务。实现必须严格遵守下述定义和约束，最终程序必须以单行、机器可检查的格式为指定的测试套件生成结果。\n\n基本原理和定义：\n- 递归神经网络 (RNN) 由隐藏状态更新公式 $h_t = \\phi(W_h h_{t-1} + W_x x_t + b)$ 定义，其中 $h_t \\in \\mathbb{R}^d$ 是时间 $t$ 的隐藏状态，$x_t \\in \\mathbb{R}^m$ 是时间 $t$ 的输入，$W_h \\in \\mathbb{R}^{d \\times d}$ 和 $W_x \\in \\mathbb{R}^{d \\times m}$ 是权重矩阵，$b \\in \\mathbb{R}^d$ 是偏置，$\\phi$ 是非线性激活函数，例如双曲正切函数 $\\tanh$。\n- 堆叠（深度）RNN 是 $L$ 个循环层的组合，其中第 $l-1$ 层的输出序列作为第 $l$ 层在相同时间索引处的输入序列。\n- 在本问题中，您将使用堆叠式 RNN 的解释来实现一个确定性的、时钟控制的二叉树归约方案：对于长度为 $N = 2^L$ 的序列，第 $l$ 层（$l \\in \\{1, 2, \\dots, L\\}$）仅在时间索引 $t$ 满足 $t \\bmod 2^l = 2^l - 1$（块结束）时输出一个值，该值通过一个固定的、已知的成对单元组合来自第 $l-1$ 层在时间 $t - 2^{l-1}$ 和 $t$ 的两个值。在非输出时间，该层输出一个中性占位符 $0$，此占位符不会被进一步使用。这种时钟控制机制实现了一个深度为 $L$ 的二叉树电路，明确了层深度与布尔电路深度之间的关系。\n\n输入与编码约定：\n- 输入字母表为二进制 $\\{0, 1\\}$。每个比特 $b_t \\in \\{0,1\\}$ 在输入到第0层时被映射为一个有符号值 $s_t = 2 b_t - 1 \\in \\{-1, +1\\}$。\n- 对于分层奇偶校验任务，在每一层，成对组合函数必须通过精确计算乘积 $u v$ 来近似比特上的布尔异或 (XOR) 操作（当 $u, v \\in \\{-1, +1\\}$ 时），因为当且仅当两个比特相等时 $u v = +1$，当且仅当它们不同时 $u v = -1$。您必须实现一个固定的两层 $\\tanh$ 网络，在四个角点输入 $(u, v) \\in \\{-1, +1\\}^2$ 上实现此映射：\n  1. 使用带有偏移量的仿射半空间定义隐藏特征，该偏移量可分离四个角点：\n     $$h_1 = \\tanh\\big(\\alpha \\, (u + v - 0.5)\\big), \\quad h_2 = \\tanh\\big(\\alpha \\, (-u - v - 0.5)\\big),$$\n     $$h_3 = \\tanh\\big(\\alpha \\, (u - v - 0.5)\\big), \\quad h_4 = \\tanh\\big(\\alpha \\, (v - u - 0.5)\\big).$$\n  2. 线性组合这些特征以形成预输出\n     $$y_{\\text{lin}} = h_1 + h_2 - h_3 - h_4.$$\n  3. 应用输出非线性\n     $$y = \\tanh(\\beta \\, y_{\\text{lin}}).$$\n  对于 $u, v \\in \\{-1, +1\\}$ 和足够大的正增益 $\\alpha$ 和 $\\beta$，这会得到 $y \\approx \\operatorname{sign}(u v) \\in \\{-1, +1\\}$，在四个角点上精确匹配所需的乘积。\n- 对于分层多数决策任务，正确的最终决策取决于和 $S = \\sum_{t=1}^N s_t$ 的符号，其中 $s_t \\in \\{-1, +1\\}$。您必须使用一个线性成对组合函数\n  $$g_{\\text{sum}}(u, v) = u + v,$$\n  以便第 $L$ 层的输出等于精确的总和 $S$。最终的多数决策是 $1$ 如果 $S > 0$，否则为 $0$（平局映射到 $0$）。这种线性路径是允许的，并突显了多数决策是线性可聚合的，不像奇偶校验需要非线性。\n- 在奇偶校验单元中，选择并使用固定常量 $\\alpha = 6$ 和 $\\beta = 3$。\n\n堆叠式、时钟控制的归约：\n- 设 $N = 2^L$，其中 $L \\ge 2$ 为整数。设 $z^{(0)}_t = 2 b_t - 1$ 对于 $t \\in \\{0, 1, \\dots, N-1\\}$。\n- 对于第 $l$ 层（$l \\in \\{1, \\dots, L\\}$）和每个时间 $t \\in \\{0, 1, \\dots, N-1\\}$：\n  - 如果 $t \\bmod 2^l = 2^l - 1$，则设置\n    $$z^{(l)}_t = g\\left(z^{(l-1)}_{t - 2^{l-1}}, \\; z^{(l-1)}_{t}\\right),$$\n    其中对于奇偶校验任务，$g = g_{\\text{parity}}$，对于多数决策任务，$g = g_{\\text{sum}}$。\n  - 否则设置 $z^{(l)}_t = 0$（未使用的占位符）。\n- 在深度 $L$ 的最终标量输出是在时间 $t = N - 1$ 时的发射值，即 $o = z^{(L)}_{N-1}$。\n\n标签解码：\n- 奇偶校验标签：设 $P = \\prod_{t=1}^N (2 b_t - 1) \\in \\{-1, +1\\}$ 为有符号比特的乘积。对于偶数 $N$，当且仅当 $P = -1$ 时，XOR奇偶校验比特为 $1$；当且仅当 $P = +1$ 时为 $0$。因此，对于奇偶校验任务和偶数 $N$，将模型输出 $o$ 解码为 $\\hat{y}_{\\text{parity}} = 1$ 如果 $o  0$，否则为 $0$。\n- 多数决策标签：设 $S = \\sum_{t=1}^N (2 b_t - 1)$。将模型输出 $o$ 解码为 $\\hat{y}_{\\text{majority}} = 1$ 如果 $o  0$，否则为 $0$。\n\n测试套件：\n- 评估四种涵盖不同深度和任务的情况：\n  1. 对所有长度为 $N = 4$（即 $L = 2$）的序列进行奇偶校验，以浮点数形式报告准确率。\n  2. 对所有长度为 $N = 8$（即 $L = 3$）的序列进行奇偶校验，以浮点数形式报告准确率。\n  3. 对所有长度为 $N = 4$ 的序列进行多数决策，以浮点数形式报告准确率。\n  4. 对所有长度为 $N = 8$ 的序列进行多数决策，以浮点数形式报告准确率。\n- 每个准确率必须在所有序列的完整集合 $\\{0,1\\}^N$ 上计算，产生一个在 $[0,1]$ 区间内的值，作为正确预测的标准分数（不带百分号）。\n- 不涉及任何物理单位或角度单位。\n\n要求的最终输出格式：\n- 您的程序应生成单行输出，其中包含一个逗号分隔的列表，用方括号括起来，顺序如下：\n  $$[\\text{acc\\_parity\\_4}, \\text{acc\\_parity\\_8}, \\text{acc\\_majority\\_4}, \\text{acc\\_majority\\_8}],$$\n  其中每个准确率是四舍五入到三位小数的浮点数。例如，一个有效的输出行可能如下所示：\n  $$[0.875,0.996,1.000,1.000].$$\n\n实现约束：\n- 使用固定常量 $\\alpha = 6$ 和 $\\beta = 3$ 精确实现成对奇偶校验单元。\n- 将多数决策单元实现为精确的线性和。\n- 按照描述实现时钟控制的堆叠式归约，用于长度 $N \\in \\{4, 8\\}$。\n- 不要训练任何参数；所有计算必须是确定性的，并基于指定的公式。\n- 枚举每个 $N$ 的所有二进制序列以计算精确的准确率。\n\n您最终提交的必须是一个完整的、可运行的程序，该程序执行这些计算并以所要求的精确单行格式打印结果。", "solution": "用户提供了一个有效的、定义明确的问题陈述，其原理基于深度学习和计算神经科学。任务是形式化并实现一种特定类型的堆叠式递归神经网络 (RNN)，该网络确定性地模拟一个二叉树归约算法。然后，该模型被应用于解决长度为 $N=2^L$ 的二进制序列上的分层奇偶校验和多数决策任务。解决方案要求实现指定的算法和函数，在所有可能的 $N=4$ 和 $N=8$ 的输入序列上评估其性能，并报告准确率。\n\n解决方案的结构如下：首先，实现每个任务的成对组合函数。其次，构建主要的时钟控制归约算法。最后，将这些组件集成到一个评估框架中，以计算所需的准确率。\n\n### 1. 成对组合函数\n\n二叉树归约的核心是一个递归应用的成对组合函数 $g$。问题为每个任务指定了两种不同的函数。\n\n**多数决策任务**：多数决策任务是线性可分的。目标是确定输入序列中 $1$ 的数量是否超过 $0$ 的数量。使用指定的输入编码，即一个比特 $b_t \\in \\{0, 1\\}$ 被映射为一个有符号值 $s_t=2b_t-1 \\in \\{-1, +1\\}$，这等价于检查总和 $S = \\sum_{t=0}^{N-1} s_t$ 的符号。由于加法的结合律，这个和可以分层计算。指定的成对函数是一个简单的线性和：\n$$g_{\\text{sum}}(u, v) = u + v$$\n在二叉树结构中应用此函数，将导致根节点的最终输出是所有叶（输入）值的精确总和。\n\n**奇偶校验任务**：奇偶校验任务（等同于两个比特的异或）是一个经典的非线性问题。目标是确定 $1$ 的数量是否为奇数。使用有符号比特编码，这对应于计算乘积 $P = \\prod_{t=0}^{N-1} s_t$ 的符号。因此，成对组合函数必须近似其两个输入的乘积，即 $g(u, v) \\approx u \\cdot v$。问题提供了一个特定的、固定参数的两层神经网络来实现这个函数 $g_{\\text{parity}}$：\n$$g_{\\text{parity}}(u, v) = \\tanh(\\beta \\, y_{\\text{lin}})$$\n其中，线性预输出 $y_{\\text{lin}}$ 由下式给出：\n$$y_{\\text{lin}} = h_1 + h_2 - h_3 - h_4$$\n隐藏特征 $h_i$ 由通过 $\\tanh$ 激活函数的仿射变换定义：\n$$h_1 = \\tanh\\big(\\alpha \\, (u + v - 0.5)\\big)$$\n$$h_2 = \\tanh\\big(\\alpha \\, (-u - v - 0.5)\\big)$$\n$$h_3 = \\tanh\\big(\\alpha \\, (u - v - 0.5)\\big)$$\n$$h_4 = \\tanh\\big(\\alpha \\, (v - u - 0.5)\\big)$$\n常量固定为 $\\alpha = 6$ 和 $\\beta = 3$。高增益 $\\alpha$ 确保隐藏单元在其饱和区工作，从而在 $(u,v)$ 空间中创建清晰的决策边界，以分离四个角点 $(\\pm 1, \\pm 1)$。线性组合经过加权，以便当 $u$ 和 $v$ 符号相同时产生正的 $y_{\\text{lin}}$，当它们符号相反时产生负的 $y_{\\text{lin}}$。最终的高增益 $\\beta$ 使输出更加尖锐，确保对于输入 $(u,v) \\in \\{-1, +1\\}^2$，输出 $y$ 极度接近所需乘积 $uv$。这种设计是鲁棒的，意味着即使输入 $u$ 和 $v$ 不完全是 $\\pm 1$ 但很接近（可能发生在网络的更高层），该函数仍然会计算出一个非常接近正确符号值的输出。\n\n### 2. 堆叠式、时钟控制的归约算法\n\n二叉树归约是使用具有特定“时钟控制”机制的堆叠式 RNN 架构实现的。网络在输入层（第0层）之上有 $L=\\log_2 N$ 个层。\n\n设 $z^{(l)}_t$ 为第 $l$ 层在时间步 $t$ 的神经元激活值。\n- **第0层**：输入层简单地持有有符号的二进制序列：$z^{(0)}_t = 2b_t - 1$，对于 $t \\in \\{0, 1, \\dots, N-1\\}$。\n- **第 $l$ 层 ($l \\in \\{1, \\dots, L\\}$)**：激活值 $z^{(l)}_t$ 仅在特定的时间步计算。计算的条件是 $t \\bmod 2^l = 2^l - 1$。这意味着第 $l$ 层的神经元只在一个大小为 $2^l$ 的块结束时产生输出。在这些时刻，它组合来自前一层 $l-1$ 的两个输出，这两个输出本身标志着两个大小为 $2^{l-1}$ 的相邻子块的结束。更新规则是：\n$$z^{(l)}_t = g\\left(z^{(l-1)}_{t - 2^{l-1}}, \\; z^{(l-1)}_{t}\\right)$$\n对于不满足条件的所有其他时间步，输出是一个占位符值 $z^{(l)}_t = 0$。这确保了函数 $g$ 总是使用来自前一层的有效、已计算的输入来调用。\n整个网络的最终输出是在最后一个时间步 $N-1$、最后一层 $L$ 计算出的单个值：\n$$o = z^{(L)}_{N-1}$$\n这个过程精确地镜像了一个二叉树计算，其中叶节点是输入序列，每个内部节点对其两个子节点计算函数 $g$。\n\n### 3. 评估与预期准确率\n\n通过在 $N=4$ 和 $N=8$ 的所有 $2^N$ 个可能的二进制输入序列上计算准确率来评估性能。\n\n- **多数决策任务**：模型输出 $o$ 是精确的和 $S = \\sum s_t$。解码规则是 $\\hat{y}_{\\text{majority}} = 1$ 如果 $o  0$，否则为 $0$。真实标签遵循基于 $S$ 的相同规则。由于模型的计算是精确的，预测将始终与真实标签匹配。因此，对于 $N=4$ 和 $N=8$，准确率预期都为 $1.0$。\n\n- **奇偶校验任务**：模型输出 $o$ 是真实乘积 $P=\\prod s_t$ 的一个高精度近似。解码规则是 $\\hat{y}_{\\text{parity}} = 1$ 如果 $o  0$，否则为 $0$。真實标签是 $1$ 如果 $P=-1$，$0$ 如果 $P=+1$。$g_{\\text{parity}}$ 函数的鲁棒设计，及其高增益饱和非线性，确保了输出 $o$ 的符号将正确匹配真实乘积 $P$ 的符号。因此，预计所有输入的预测都将是正确的，对于 $N=4$ 和 $N=8$ 都将产生 $1.0$ 的准确率。\n\n实现将系统地生成每个二进制序列，运行归约算法以获得模型输出，解码预测，计算真实标签，并统计正确预测的数量以得出最终准确率。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nimport math\n\ndef solve():\n    \"\"\"\n    Formalizes and implements a deterministic, clocked, stacked RNN \n    that emulates a binary-tree reduction for parity and majority tasks.\n    \"\"\"\n\n    ALPHA = 6.0\n    BETA = 3.0\n\n    def g_parity(u, v):\n        \"\"\"\n        Pairwise combine function for the parity task, approximating u*v.\n        \"\"\"\n        h1 = np.tanh(ALPHA * (u + v - 0.5))\n        h2 = np.tanh(ALPHA * (-u - v - 0.5))\n        h3 = np.tanh(ALPHA * (u - v - 0.5))\n        h4 = np.tanh(ALPHA * (v - u - 0.5))\n        y_lin = h1 + h2 - h3 - h4\n        return np.tanh(BETA * y_lin)\n\n    def g_sum(u, v):\n        \"\"\"\n        Pairwise combine function for the majority task, computing u+v.\n        \"\"\"\n        return u + v\n\n    def run_reduction(binary_sequence, task):\n        \"\"\"\n        Runs the stacked, clocked reduction algorithm on a binary sequence.\n        \"\"\"\n        N = len(binary_sequence)\n        # Sequence length N must be a power of two.\n        L = int(math.log2(N))\n        \n        # z_history[l] stores the activations for layer l.\n        z_history = [np.zeros(N, dtype=np.float64) for _ in range(L + 1)]\n\n        # Layer 0: Input layer with signed bits.\n        z_history[0] = np.array([2 * b - 1 for b in binary_sequence], dtype=np.float64)\n        \n        g = g_parity if task == 'parity' else g_sum\n        \n        # Layers l = 1 to L\n        for l in range(1, L + 1):\n            block_size = 2**l\n            step_size = 2**(l-1)\n            # Iterate through all time steps t = 0 to N-1.\n            for t in range(N):\n                # An output is emitted only at the end of a block.\n                if (t + 1) % block_size == 0:\n                    input1 = z_history[l-1][t - step_size]\n                    input2 = z_history[l-1][t]\n                    z_history[l][t] = g(input1, input2)\n        \n        # The final output is at layer L, time N-1.\n        return z_history[L][N-1]\n\n    def get_true_label(binary_sequence, task):\n        \"\"\"\n        Computes the ground truth label for a given sequence and task.\n        \"\"\"\n        signed_sequence = np.array([2 * b - 1 for b in binary_sequence], dtype=np.float64)\n        \n        if task == 'parity':\n            # For even N, parity is 1 iff the product of signed bits is -1.\n            product = np.prod(signed_sequence)\n            return 1 if product  0 else 0\n        elif task == 'majority':\n            # Majority is 1 iff the sum of signed bits is > 0.\n            total_sum = np.sum(signed_sequence)\n            return 1 if total_sum > 0 else 0\n        else:\n            raise ValueError(\"Unknown task\")\n\n    def evaluate_task(N, task):\n        \"\"\"\n        Evaluates the model's accuracy for a given N and task over all 2^N sequences.\n        \"\"\"\n        num_correct = 0\n        num_total = 2**N\n        \n        for i in range(num_total):\n            # Generate the i-th binary sequence of length N.\n            # Bits are ordered from most to least significant.\n            binary_sequence = [(i >> j)  1 for j in range(N - 1, -1, -1)]\n            \n            # Get model prediction.\n            model_output = run_reduction(binary_sequence, task)\n            \n            if task == 'parity':\n                # Decode model output for parity.\n                prediction = 1 if model_output  0 else 0\n            else: # task == 'majority'\n                # Decode model output for majority.\n                prediction = 1 if model_output > 0 else 0\n            \n            # Get true label.\n            true_label = get_true_label(binary_sequence, task)\n            \n            if prediction == true_label:\n                num_correct += 1\n                \n        return num_correct / num_total\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (4, 'parity'),\n        (8, 'parity'),\n        (4, 'majority'),\n        (8, 'majority')\n    ]\n\n    results = []\n    for N, task in test_cases:\n        accuracy = evaluate_task(N, task)\n        results.append(accuracy)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{r:.3f}' for r in results)}]\")\n\nsolve()\n```", "id": "3176027"}, {"introduction": "理解了堆叠结构如何实现层次化计算后，我们可以进一步探究它如何执行更复杂的、有状态的算法。本练习将挑战你使用一个双层堆叠长短期记忆（LSTM）网络来模拟一个下推自动机（PDA），以识别经典的上下文无关语言 $L = \\{ a^n b^n \\mid n \\in \\mathbb{N}_0 \\}$。通过精心设计网络参数，你将实现层级间的任务分离——一个层用于检测输入序列所处的阶段（读取 $a$ 或 $b$），另一层则充当计数器（模拟堆栈），这清晰地展示了堆叠架构中功能专业化的强大能力。[@problem_id:3175992]", "problem": "你的任务是确定一个由长短期记忆 (LSTM) 层组成的堆叠循环神经网络 (RNN) 何时可以模拟一个下推自动机 (PDA) 来识别嵌套结构。考虑形式语言 $L = \\{ a^n b^n \\mid n \\in \\mathbb{N}_0 \\}$，其中 $\\mathbb{N}_0$ 表示非负整数集。字母表为 $\\{a, b\\}$，输入是从该字母表中抽取的有限符号序列。此项任务的基本基础包括深度学习和自动机理论中以下经过充分检验的定义。\n\n对于单个 LSTM 层，在时间步 $t$，输入向量为 $x_t$，上一个隐藏状态为 $h_{t-1}$，上一个单元状态为 $c_{t-1}$，其门和状态定义如下：\n$$ i_t = \\sigma(W_i x_t + U_i h_{t-1} + b_i), \\quad f_t = \\sigma(W_f x_t + U_f h_{t-1} + b_f), \\quad o_t = \\sigma(W_o x_t + U_o h_{t-1} + b_o), $$\n$$ \\tilde{c}_t = \\tanh(W_c x_t + U_c h_{t-1} + b_c), \\quad c_t = f_t \\odot c_{t-1} + i_t \\odot \\tilde{c}_t, \\quad h_t = o_t \\odot \\tanh(c_t), $$\n其中 $\\sigma$ 是 logistic sigmoid 函数，$\\tanh$ 是双曲正切函数，$\\odot$ 表示逐元素乘法。在一个有 $\\ell$ 层的堆叠架构中，第 $\\ell$ 层的输入是 $x_t^{(\\ell)} = h_t^{(\\ell-1)}$，其中 $x_t^{(1)}$ 由外部输入编码定义。一个下推自动机 (PDA) 通过在读取 $a$ 符号时增加一个计数器，在读取 $b$ 符号时减少计数器来识别 $L$，同时强制一旦观察到任何 $b$ 之后，就不能再出现 $a$，并且计数器永远不能为负。如果在输入结束时计数器返回到 $0$，则接受该输入。\n\n你的任务是：\n- 从上述定义中，推导出双层堆叠 LSTM 能够模拟用于识别语言 $L$ 的 PDA 行为的条件，这些条件应施加于门和内部信号上。特别地，要指明第一层如何产生一个内部信号来指示是否已见过任何 $b$，以及第二层如何更新其单元状态 $c_t^{(2)}$ 中的一个计数器，以便在 $b$ 阶段开始前为 $a$ 执行 $+1$ 更新，此后为 $b$ 执行 $-1$ 更新。你的推导必须从给定的 LSTM 方程和 PDA 接受语义出发，并论证一些原则性要求，例如门饱和到接近 $0$ 或 $1$ 的值、对计数器维度的单调更新，以及各层之间的角色分离。避免使用跳过这些推理步骤的快捷公式。\n- 构建一个自包含的程序，该程序无需训练，通过精心选择的、与你推导出的条件一致的确定性门行为来模拟一个双层堆叠 LSTM。第一层应产生包括以下内容的输出：\n  1. 一个指示器式的直通信号，表明当前符号的身份（是 $a$ 还是 $b$）。\n  2. 一个单调的内部信号，当观察到 $b$ 时该信号增加（以便模型能够检测是否已见过任何 $b$）。\n  第二层应在其单元状态 $c_t^{(2)}$ 中维护一个有符号计数器，在读取 $a$ 且尚未出现 $b$ 时加 $+1$，在读取 $b$ 时减 $-1$。你的程序必须强制执行 PDA 约束：一旦见过任何 $b$，再遇到 $a$ 就是无效的，并且计数器决不能低于 $0$。当且仅当最终计数器 $c_T^{(2)}$ 在一个小的容差 $\\epsilon$ 内为 $0$，没有无效的 $a$ 出现在 $b$ 之后，且没有任何前缀导致计数器变为负数时，程序才应接受输入。\n- 对符号使用以下独热输入编码：$a \\mapsto [1, 0]$，$b \\mapsto [0, 1]$。跟踪第一层演变的单元状态 $c_t^{(1)}$ 和第二层的 $c_t^{(2)}$。根据你的设计，使用根据需要饱和到 $0$ 或 $1$ 的门值，显式计算 $c_t^{(2)}$ 中的计数器。\n- 实现并评估以下在 $\\{a,b\\}$ 上的序列测试套件：\n  1. 空字符串 $\"\"$。\n  2. $\"ab\"$。\n  3. $\"aaabbb\"$。\n  4. $\"aabbb\"$。\n  5. $\"aaabb\"$。\n  6. $\"aba\"$。\n  7. $\"b\"$。\n  8. $\"aaaabbbb\"$。\n- 使用布尔决策为每个序列定义精确的接受条件。使用一个具有合理小正值（例如 $\\epsilon = 10^{-3}$）的容差 $\\epsilon$ 来将最终计数器与 $0$ 进行比较。\n- 最终输出格式：你的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，顺序与测试套件相同（例如 $[r_1,r_2,\\dots,r_8]$），其中每个 $r_i$ 是测试用例 $i$ 的布尔接受结果。\n\n不涉及物理单位或角度；所有输出均为布尔值。程序必须是完全自包含的，无需用户输入或外部文件即可运行。其逻辑必须从数学编程的角度来看是普遍可理解的，并且应该可以在任何现代编程语言中实现，但你必须按规定生成 Python 代码。", "solution": "我们从长短期记忆 (LSTM) 的定义开始。对于一个 LSTM，其单元状态更新为\n$$ c_t = f_t \\odot c_{t-1} + i_t \\odot \\tilde{c}_t, $$\n门的激活值为\n$$ i_t = \\sigma(W_i x_t + U_i h_{t-1} + b_i), \\quad f_t = \\sigma(W_f x_t + U_f h_{t-1} + b_f), \\quad o_t = \\sigma(W_o x_t + U_o h_{t-1} + b_o), $$\n候选值为\n$$ \\tilde{c}_t = \\tanh(W_c x_t + U_c h_{t-1} + b_c). $$\n在堆叠架构中，第 $\\ell$ 层的输入是前一层的隐藏输出：\n$$ x_t^{(\\ell)} = h_t^{(\\ell-1)}, \\quad h_t^{(\\ell)} = o_t^{(\\ell)} \\odot \\tanh(c_t^{(\\ell)}). $$\n\n用于语言 $L = \\{ a^n b^n \\mid n \\in \\mathbb{N}_0 \\}$ 的下推自动机 (PDA) 必须遵守以下语义约束：\n- 读取 $a$ 符号时，它执行推入（压栈）操作，每个 $a$ 使栈高增加 $+1$。\n- 遇到第一个 $b$ 时，它转换到弹出阶段，此后执行弹出（弹栈）操作，每个 $b$ 使栈高减少 $-1$。\n- 在见过任何 $b$ 之后，不能再出现 $a$。\n- 在任何前缀处，栈高都不得为负。\n- 接受条件要求输入结束时最终栈高恰好为 $0$。\n\n为了用一个双层堆叠 LSTM 模拟这些行为，我们需要以下从 LSTM 方程推导出的原则性条件。\n\n第 $1$ 层条件（特征提取和阶段检测）：\n- 该层必须输出编码当前符号身份以及到目前为止是否遇到过任何 $b$ 的信号。设单元状态 $c_t^{(1)} \\in \\mathbb{R}^3$ 的结构如下：\n  - $c_{t,0}^{(1)}$ 跟踪在时间 $t$ 的 $a$ 的直通指示器，\n  - $c_{t,1}^{(1)}$ 跟踪在时间 $t$ 的 $b$ 的直通指示器，\n  - $c_{t,2}^{(1)}$ 累积到目前为止看到的 $b$ 符号的数量。\n- 为实现此目的，选择使用 $\\sigma$ 函数饱和到接近 $0$ 或 $1$ 的值的门，以便：\n  - 对于维度 $0$（$a$ 指示器），设置 $f_{t,0}^{(1)} \\approx 0$ 以在每一步重置， $i_{t,0}^{(1)} \\approx \\mathbb{I}\\{x_t \\text{ is } a\\}$，以及 $\\tilde{c}_{t,0}^{(1)} \\approx 1$，从而得到 $c_{t,0}^{(1)} \\approx \\mathbb{I}\\{x_t \\text{ is } a\\}$。此处 $\\mathbb{I}\\{\\cdot\\}$ 表示指示函数。这与 LSTM 更新是一致的，因为当 $x_t$ 是 $a$ 时 $c_{t,0}^{(1)} = f_{t,0}^{(1)} c_{t-1,0}^{(1)} + i_{t,0}^{(1)} \\tilde{c}_{t,0}^{(1)} \\approx 0 + 1 \\cdot 1$，当 $x_t$ 是 $b$ 时结果 $\\approx 0$。\n  - 对于维度 $1$（$b$ 指示器），设置 $f_{t,1}^{(1)} \\approx 0$，$i_{t,1}^{(1)} \\approx \\mathbb{I}\\{x_t \\text{ is } b\\}$，以及 $\\tilde{c}_{t,1}^{(1)} \\approx 1$，得到 $c_{t,1}^{(1)} \\approx \\mathbb{I}\\{x_t \\text{ is } b\\}$。\n  - 对于维度 $2$（$b$ 阶段累加器），设置 $f_{t,2}^{(1)} \\approx 1$ 以保留记忆，$i_{t,2}^{(1)} \\approx \\mathbb{I}\\{x_t \\text{ is } b\\}$，以及 $\\tilde{c}_{t,2}^{(1)} \\approx 1$，产生 $c_{t,2}^{(1)} \\approx c_{t-1,2}^{(1)} + \\mathbb{I}\\{x_t \\text{ is } b\\}$，即一个对 $b$ 观测次数的非递减计数器。当 $o_{t}^{(1)} \\approx 1$ 时，我们有 $h_t^{(1)} \\approx \\tanh(c_t^{(1)})$，其第三个分量在任何 $b$ 出现前接近 $0$，一旦看到 $b$ 符号后便向 $1$ 增加。\n- 这些门的行为可以通过 LSTM 方程实现，方法是选择 $W_i, U_i$ 和 $b_i$ 以在期望情况下产生大的正预激活值，在其他情况下产生大的负预激活值，从而确保饱和。同时，选择 $W_c, U_c$ 和 $b_c$ 将指示器式维度的 $\\tilde{c}_t$ 设置为近似常数值，将累加器的 $\\tilde{c}_t$ 设置为 $1$。\n\n第 $2$ 层条件（模拟栈高的计数器管理）：\n- 第二层在 $c_t^{(2)} \\in \\mathbb{R}$ 中维护一个代表栈高的标量计数器。它必须：\n  - 在 $b$ 阶段尚未开始时，为 $a$ 增加 $+1$，\n  - 在任何时候，为 $b$ 减少 $-1$，\n  - 在 $b$ 阶段开始后，对 $a$ 保持不变（因为这是无效情况），同时升起一个外部错误标志以阻止接受。\n- 设 $\\phi_t \\in \\{0,1\\}$ 是一个从第一层导出的阶段指示器，其中如果 $c_{t,2}^{(1)}$ 接近 $0$，则 $\\phi_t \\approx 0$，一旦见过任何 $b$ 后则 $\\phi_t \\approx 1$。然后选择门使得 $f_t^{(2)} \\approx 1$，$i_t^{(2)} \\approx 1$，并将 $\\tilde{c}_t^{(2)}$ 设计为\n  $$ \\tilde{c}_t^{(2)} \\approx (1 - \\phi_t) \\cdot \\mathbb{I}\\{x_t \\text{ is } a\\} - \\mathbb{I}\\{x_t \\text{ is } b\\}, $$\n  这会产生\n  $$ c_t^{(2)} \\approx c_{t-1}^{(2)} + \\tilde{c}_t^{(2)}. $$\n  这与 LSTM 动态是一致的，因为 $f_t^{(2)} \\approx 1$ 保留了前一个计数器，而 $i_t^{(2)} \\approx 1$ 添加了带符号的更新。使用 $\\tanh$ 确保 $\\tilde{c}_t^{(2)} \\in [-1,1]$，与 $+1$ 和 $-1$ 的增减量相符。\n- 错误检测条件：\n  - 如果 $\\phi_t \\approx 1$ 且 $x_t$ 是 $a$，则因 $a$ 出现在 $b$ 之后而标记为无效。\n  - 如果 $c_t^{(2)}$ 曾降到 $0$ 以下，则因下溢而标记为无效（PDA 试图从空栈中弹出）。\n- 接受条件：\n  $$ \\text{accept} \\iff \\left(|c_T^{(2)}| \\le \\epsilon\\right) \\land \\left(\\text{no invalid } a \\text{ after } b\\right) \\land \\left(\\text{no underflow occurred}\\right), $$\n  其中 $\\epsilon$ 是一个小的正容差（例如 $\\epsilon = 10^{-3}$），用于解释由门饱和引起的任何微小数值偏差。\n\n与这些原则一致的算法设计：\n- 用 $a \\mapsto [1,0]$ 和 $b \\mapsto [0,1]$ 编码输入。\n- 如上所述，实现具有三个单元维度的第一层，使用具有大增益的 sigmoid 函数来近似指示器。使用 $f_{t,0}^{(1)} = 0$，$i_{t,0}^{(1)} \\approx \\sigma(K(x_{a,t} - 0.5))$，$\\tilde{c}_{t,0}^{(1)} = 1$；对于维度 $1$ 中的 $b$ 也类似；对于维度 $2$，使用 $f_{t,2}^{(1)} = 1$，$i_{t,2}^{(1)} \\approx \\sigma(K(x_{b,t} - 0.5))$，$\\tilde{c}_{t,2}^{(1)} = 1$，其中 $K$ 很大（例如 $K=20$）。\n- 实现第二层计数器，其中 $f_t^{(2)} = 1$，$i_t^{(2)} = 1$，且 $\\tilde{c}_t^{(2)}$ 设置为 $(1 - \\phi_t)\\cdot \\mathbb{I}\\{x_t \\text{ is } a\\} - \\mathbb{I}\\{x_t \\text{ is } b\\}$，其中 $\\phi_t$ 通过对 $c_{t,2}^{(1)}$ 进行阈值化确定（例如，如果 $c_{t,2}^{(1)}  0.5$ 则 $\\phi_t = 1$，否则为 $0$）。\n- 跟踪计数器 $c_t^{(2)}$、下溢条件 $c_t^{(2)}  0$ 以及无效的“$a$ 在 $b$ 之后”条件。\n\n正确性论证：\n- 在任何 $b$ 出现之前，$\\phi_t \\approx 0$，且 $\\tilde{c}_t^{(2)} \\approx \\mathbb{I}\\{x_t \\text{ is } a\\} - 0$，因此每个 $a$ 计数器增加 $+1$。观察到 $b$ 之后，$\\phi_t \\approx 1$，且 $\\tilde{c}_t^{(2)} \\approx 0 - \\mathbb{I}\\{x_t \\text{ is } b\\}$，因此每个 $b$ 计数器减少 $-1$。这精确地模拟了 PDA 对 $a^n b^n$ 的推入/弹出计数。包含在 $b$ 之后出现 $a$ 的无效序列不会增加计数器，并被标记为无效；含有过多 $b$ 的序列会导致计数器变为负数，触发下溢。接受条件要求计数器返回到 $0$ 且无任何违规，这与 PDA 的接受语义相匹配。\n- 第 1 层的门将角色分离为指示器直通和单调阶段检测，而第 2 层仅专注于计数器更新，这提供了堆叠架构中典型的必要关注点分离。使用饱和门来近似 $0$ 和 $1$ 确保了更新在时间上是鲁棒和一致的。\n\n测试套件覆盖范围理据：\n- 空字符串 $\"\"$ 测试边界情况 $n=0$。\n- $\"ab\"$ 测试最小的非空有效情况。\n- $\"aaabbb\"$ 和 $\"aaaabbbb\"$ 测试具有多次增减的一般有效情况。\n- $\"aabbb\"$ 和 $\"aaabb\"$ 测试计数不匹配以及下溢与非零最终计数器的情况。\n- $\"aba\"$ 测试 $a$ 出现在 $b$ 之后的无效条件。\n- $\"b\"$ 测试在空栈上以弹出开始，导致下溢的情况。\n\n最终程序实现了此设计，并按指定顺序输出单行布尔值 $[r_1,r_2,\\dots,r_8]$。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef sigma(z):\n    return 1.0 / (1.0 + np.exp(-z))\n\ndef tanh(z):\n    return np.tanh(z)\n\ndef accept_sequence(seq, K=20.0, eps=1e-3):\n    \"\"\"\n    Simulate a two-layer stacked LSTM designed to emulate a PDA for L = {a^n b^n}.\n    Layer 1 (size 3):\n      - c1[0]: indicator-like pass-through for 'a' at time t\n      - c1[1]: indicator-like pass-through for 'b' at time t\n      - c1[2]: monotone accumulator of whether any 'b' has been seen (counts b's)\n    Layer 2 (size 1):\n      - c2: counter of stack height, +1 for 'a' before any 'b', -1 for 'b'\n    Acceptance: final c2 ~ 0, no 'a' after 'b', and no underflow (c2  0 at any prefix).\n    \"\"\"\n    # Initialize states for layer 1\n    c1 = np.zeros(3, dtype=float)\n    h1 = np.zeros(3, dtype=float)\n    # Initialize state for layer 2 counter\n    c2 = 0.0\n\n    invalid_a_after_b = False\n    underflow = False\n\n    for ch in seq:\n        # One-hot encoding for input at time t\n        if ch == 'a':\n            xa, xb = 1.0, 0.0\n        elif ch == 'b':\n            xa, xb = 0.0, 1.0\n        else:\n            # Invalid character: reject immediately\n            return False\n\n        # Layer 1 gate computations and updates\n        # Dimension 0: 'a' indicator-like pass-through\n        i0 = sigma(K * (xa - 0.5))  # ~1 if 'a', ~0 if 'b'\n        f0 = 0.0\n        g0 = 1.0\n        c1[0] = f0 * c1[0] + i0 * g0\n        h1[0] = tanh(c1[0])  # output gate ~1\n\n        # Dimension 1: 'b' indicator-like pass-through\n        i1 = sigma(K * (xb - 0.5))  # ~1 if 'b', ~0 if 'a'\n        f1 = 0.0\n        g1 = 1.0\n        c1[1] = f1 * c1[1] + i1 * g1\n        h1[1] = tanh(c1[1])  # output gate ~1\n\n        # Dimension 2: accumulator of b's (phase detector)\n        i2 = sigma(K * (xb - 0.5))  # ~1 on 'b', ~0 on 'a'\n        f2 = 1.0\n        g2 = 1.0\n        c1[2] = f2 * c1[2] + i2 * g2\n        h1[2] = tanh(c1[2])  # increases toward 1 after any 'b'\n\n        # Phase indicator: whether any 'b' has been seen (threshold on c1[2])\n        b_phase = c1[2] > 0.5\n\n        # Detect invalid 'a' after 'b'\n        if xa > 0.5 and b_phase:\n            invalid_a_after_b = True\n\n        # Layer 2 counter update: +1 for 'a' before any 'b', -1 for 'b'\n        inc = 1 if (xa > 0.5 and not b_phase) else 0\n        dec = -1 if xb > 0.5 else 0\n        # LSTM-like update with f=1, i=1, candidate = inc + dec\n        c2 = c2 + (inc + dec)\n\n        # Underflow check: counter must never go negative\n        if c2  -eps:\n            underflow = True\n\n    # Acceptance: final counter ~ 0, and no invalid events\n    accept = (abs(c2) = eps) and (not invalid_a_after_b) and (not underflow)\n    return accept\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        \"\",           # empty string\n        \"ab\",         # minimal valid\n        \"aaabbb\",     # valid, n=3\n        \"aabbb\",      # invalid: too many b\n        \"aaabb\",      # invalid: too many a\n        \"aba\",        # invalid: a after b\n        \"b\",          # invalid: starts with b, underflow\n        \"aaaabbbb\"    # valid, n=4\n    ]\n\n    results = []\n    for seq in test_cases:\n        result = accept_sequence(seq)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(str(res).lower() for res in results)}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3175992"}, {"introduction": "堆叠RNN不仅能够执行复杂的计算，其不同层级所形成的表征也具有不同的特性。底层通常捕捉局部和具体的特征，而高层则整合信息形成更抽象、长程的表征。本练习通过将注意力机制与堆叠RNN结合，提供了一种量化分析这种层次化表征的方法。你将研究当注意力机制的“查询”来自顶层，而“键”和“值”取自不同深度层时，模型的“注意力跨度”会如何变化，从而揭示信息在网络深度维度上的流动与分布规律。[@problem_id:3175991]", "problem": "要求您设计并实现一个确定性的数值实验，以分离地研究在堆叠循环神经网络（RNN）的不同深度附加一个单头、因果、缩放点积注意力机制，如何改变序列上时间滞后的有效感受野。请从第一性原理出发：仅使用堆叠循环神经网络和缩放点积注意力的核心定义，并推导出当查询（query）取自顶层、键（key）/值（value）取自指定层时，由注意力机制引起的注意力跨度分布。\n\n首先，以下是经过充分检验的定义和事实：\n\n- 循环神经网络（RNN）由一个隐藏状态的递归关系指定。对于层索引 $l \\in \\{1,\\dots,L\\}$ 和时间步 $t \\in \\{0,\\dots,T-1\\}$，隐藏状态 $h_t^{(l)} \\in \\mathbb{R}^{d_h^{(l)}}$ 定义如下\n  $$h_t^{(1)} = \\tanh\\!\\left(U^{(1)} x_t + R^{(1)} h_{t-1}^{(1)} + b^{(1)}\\right), \\quad h_{-1}^{(1)} = 0,$$\n  $$h_t^{(l)} = \\tanh\\!\\left(U^{(l)} h_t^{(l-1)} + R^{(l)} h_{t-1}^{(l)} + b^{(l)}\\right), \\quad h_{-1}^{(l)} = 0 \\text{ for } l \\ge 2,$$\n  其中 $x_t \\in \\mathbb{R}^{d_x}$ 是时间步 $t$ 的输入，$U^{(l)} \\in \\mathbb{R}^{d_h^{(l)} \\times d_{in}^{(l)}}$，$R^{(l)} \\in \\mathbb{R}^{d_h^{(l)} \\times d_h^{(l)}}$，$b^{(l)} \\in \\mathbb{R}^{d_h^{(l)}}$，并且对于 $l \\ge 2$，$d_{in}^{(1)} = d_x$，$d_{in}^{(l)} = d_h^{(l-1)}$。非线性函数为双曲正切函数。\n\n- 具有查询维度 $d_k$ 的单头缩放点积注意力的定义如下。给定查询 $q_t \\in \\mathbb{R}^{d_k}$、键 $k_s \\in \\mathbb{R}^{d_k}$ 以及一个只允许关注索引 $s \\le t$ 的因果掩码，注意力权重为\n  $$\\alpha_{t,s} = \\frac{\\exp\\!\\left(\\frac{q_t^\\top k_s}{\\sqrt{d_k}}\\right)}{\\sum_{u=0}^t \\exp\\!\\left(\\frac{q_t^\\top k_u}{\\sqrt{d_k}}\\right)} \\quad \\text{for } s \\in \\{0,\\dots,t\\}.$$\n  查询和键由线性投影形成：$q_t = W_Q h_t^{(L)}$ 和 $k_s = W_K h_s^{(\\ell)}$，其中 $W_Q \\in \\mathbb{R}^{d_k \\times d_h^{(L)}}$ 和 $W_K \\in \\mathbb{R}^{d_k \\times d_h^{(\\ell)}}$，$\\ell \\in \\{1,\\dots,L\\}$ 表示提供键（和值，尽管此处未使用值）的层。\n\n将非负时间滞后 $d \\in \\{0,1,\\dots,T-1\\}$ 上的注意力跨度分布定义为在所有时间步上均匀聚合分配给每个滞后的注意力质量的分布：\n$$p(d) = \\frac{1}{T} \\sum_{t=0}^{T-1} \\sum_{s=0}^{t} \\alpha_{t,s} \\, \\mathbf{1}\\{t-s = d\\}.$$\n根据定义，$\\sum_{d=0}^{T-1} p(d) = 1$。这个 $p(d)$ 量化了纯粹由注意力机制引起的、在相对位置上的有效感受野，其条件是哪个内部层提供键。\n\n您的任务是：\n\n- 实现一个如上所述的具有 $L$ 层的堆叠 RNN。\n- 生成输入 $x_t$ 作为独立同分布的高斯向量，其均值为零，方差为单位一，即 $x_t$ 的每个分量都从 $\\mathcal{N}(0,1)$ 中采样。\n- 对于指定的提供键的层索引 $\\ell$，通过 $q_t = W_Q h_t^{(L)}$ 和 $k_s = W_K h_s^{(\\ell)}$ 构建查询和键，然后计算所有 $t$ 和 $s \\le t$ 的因果注意力权重 $\\alpha_{t,s}$。\n- 计算上面定义的注意力跨度分布 $p(d)$。\n- 所有权重矩阵必须从独立的零均值高斯分布中初始化，并采用适合各层的缩放以避免无意义的饱和。使用 $U^{(l)}_{ij} \\sim \\mathcal{N}(0, 1/d_{in}^{(l)})$，$R^{(l)}_{ij} \\sim \\mathcal{N}(0, 1/d_h^{(l)})$，$W_Q \\sim \\mathcal{N}(0, 1/d_h^{(L)})$，$W_K \\sim \\mathcal{N}(0, 1/d_h^{(\\ell)})$，并且 $b^{(l)} = 0$。所有随机抽样必须通过指定的伪随机种子来确保确定性。\n\n为以下测试套件计算 $p(d)$。每个测试用例指定了 $L$、输入维度 $d_x$、隐藏维度 $\\{d_h^{(l)}\\}_{l=1}^L$、查询/键维度 $d_k$、序列长度 $T$、提供键的层索引 $\\ell$ 以及两个种子：一个用于模型参数，一个用于输入序列。当两个用例共享相同的种子和维度，仅在 $\\ell$ 上有所不同时，除了提供键的层的差异外，模型和输入必须完全相同。\n\n- 测试用例1（正常路径，双层基线）：\n  - $L = 2$，$d_x = 5$，$(d_h^{(1)}, d_h^{(2)}) = (8, 8)$，$d_k = 6$，$T = 20$，$\\ell = 1$，$\\text{model\\_seed} = 2025$，$\\text{input\\_seed} = 100$。\n- 测试用例2（与用例1模型和输入相同，但键来自顶层）：\n  - $L = 2$，$d_x = 5$，$(d_h^{(1)}, d_h^{(2)}) = (8, 8)$，$d_k = 6$，$T = 20$，$\\ell = 2$，$\\text{model\\_seed} = 2025$，$\\text{input\\_seed} = 100$。\n- 测试用例3（更深的堆叠，键来自最深层）：\n  - $L = 3$，$d_x = 5$，$(d_h^{(1)}, d_h^{(2)}, d_h^{(3)}) = (8, 6, 10)$，$d_k = 6$，$T = 30$，$\\ell = 3$，$\\text{model\\_seed} = 777$，$\\text{input\\_seed} = 555$。\n- 测试用例4（与用例3模型和输入相同，但键来自第一层）：\n  - $L = 3$，$d_x = 5$，$(d_h^{(1)}, d_h^{(2)}, d_h^{(3)}) = (8, 6, 10)$，$d_k = 6$，$T = 30$，$\\ell = 1$，$\\text{model\\_seed} = 777$，$\\text{input\\_seed} = 555$。\n- 测试用例5（边界情况：最小序列长度）：\n  - $L = 2$，$d_x = 4$，$(d_h^{(1)}, d_h^{(2)}) = (6, 6)$，$d_k = 4$，$T = 1$，$\\ell = 2$，$\\text{model\\_seed} = 9$，$\\text{input\\_seed} = 9$。\n\n实现和数值要求：\n\n- 使用双曲正切非线性函数和将注意力限制在索引 $s \\le t$ 的因果掩码。\n- 完全按照规定使用缩放点积因子 $1/\\sqrt{d_k}$。\n- 为保证Softmax中的数值稳定性，在进行指数运算前，减去允许窗口内的最大分数。\n- 将每个输出分布 $p(d)$ 的每个元素四舍五入到6位小数。\n\n最终输出格式：\n\n- 您的程序应生成单行输出，其中包含一个由方括号括起来的逗号分隔列表。每个元素对应一个测试用例，并且本身必须是该用例的四舍五入后的分布值列表 $[p(0), p(1), \\dots, p(T-1)]$。例如，整体结构必须是\n  $$\\text{[ [p\\_case1(0),...,p\\_case1(T\\_1-1)], [p\\_case2(0),...,p\\_case2(T\\_2-1)], \\dots ]}$$\n前后不带任何附加文本。\n\n此问题不涉及任何物理单位、角度或百分比。所有量都是无量纲的实数。请通过使用上面给出的精确定义来确保科学真实性，不要引入任何辅助的启发式方法或近似。", "solution": "该问题要求设计并实现一个数值实验，以确定由集成在堆叠循环神经网络（RNN）中的单头因果注意力机制所引发的注意力跨度分布 $p(d)$。该分布量化了在时间滞后 $d$ 上的有效感受野，其形状预计会根据RNN中提供键和值的层 $\\ell$ 的不同而变化。整个过程必须是确定性的，由指定的伪随机数种子控制。\n\n该解决方案分四个主要阶段实施，遵循问题陈述中列出的原则。\n\n### 1. 模型和数据初始化\n\n第一步是构建计算模型和输入数据。此过程由 `model_seed` 和 `input_seed` 控制，以确保可复现性。\n\n**模型参数**：堆叠RNN有 $L$ 层，参数为 $\\{U^{(l)}, R^{(l)}, b^{(l)}\\}_{l=1}^L$。注意力机制的参数为 $W_Q$ 和 $W_K$。所有偏置向量 $b^{(l)}$ 都初始化为零。权重矩阵通过从零均值高斯分布中抽样来初始化，其方差根据扇入（fan-in）进行缩放，这是一种促进信号稳定传播的标准技术。\n- 对于RNN层，权重初始化如下：\n  $$U^{(l)}_{ij} \\sim \\mathcal{N}(0, 1/d_{in}^{(l)}), \\quad R^{(l)}_{ij} \\sim \\mathcal{N}(0, 1/d_h^{(l)})$$\n  其中 $d_{in}^{(1)} = d_x$（输入维度），对于 $l \\ge 2$，$d_{in}^{(l)} = d_h^{(l-1)}$。$d_h^{(l)}$ 是第 $l$ 层的隐藏状态维度。\n- 对于注意力机制，查询和键的投影矩阵初始化如下：\n  $$W_Q \\sim \\mathcal{N}(0, 1/d_h^{(L)}), \\quad W_K \\sim \\mathcal{N}(0, 1/d_h^{(\\ell)})$$\n  其中 $L$ 是总层数，$\\ell$ 是提供键的层的索引。$W_K$ 的初始化明确依赖于所选源层 $\\ell$ 的维度。对于共享 `model_seed` 但 $\\ell$ 不同的测试用例，共享参数（$U^{(l)}, R^{(l)}, W_Q$）的随机抽样序列是相同的，这确保了底层模型除了功能上依赖于 $\\ell$ 的组件（$W_K$）之外是相同的。\n\n**输入数据**：输入序列 $\\{x_t\\}_{t=0}^{T-1}$ 由 $T$ 个向量组成，每个向量的维度为 $d_x$。每个向量 $x_t$ 的每个分量都是使用以 `input_seed` 为种子的随机数生成器从标准正态分布 $x_{t,i} \\sim \\mathcal{N}(0, 1)$ 中独立抽取的。\n\n### 2. 堆叠RNN前向传播\n\n模型和数据初始化后，下一步是计算所有层 $l \\in \\{1, \\dots, L\\}$ 和所有时间步 $t \\in \\{0, \\dots, T-1\\}$ 的隐藏状态 $h_t^{(l)}$。这是通过沿时间展开网络来完成的。隐藏状态初始化为零，即对于所有 $l$，$h_{-1}^{(l)} = 0$。\n\n计算按每个时间步 $t$ 顺序进行：\n- 对于第一层（$l=1$），隐藏状态是根据当前输入 $x_t$ 和同一层的上一个隐藏状态 $h_{t-1}^{(1)}$ 计算的：\n  $$h_t^{(1)} = \\tanh(U^{(1)} x_t + R^{(1)} h_{t-1}^{(1)})$$\n- 对于所有后续层（$l \\ge 2$），隐藏状态是根据当前时间步下一层的隐藏状态 $h_t^{(l-1)}$ 和同一层的上一个隐藏状态 $h_{t-1}^{(l)}$ 计算的：\n  $$h_t^{(l)} = \\tanh(U^{(l)} h_t^{(l-1)} + R^{(l)} h_{t-1}^{(l)})$$\n这个过程从 $t=0$ 迭代到 $T-1$，并且对于每个 $t$，从 $l=1$ 迭代到 $L$。所有计算出的隐藏状态 $\\{h_t^{(l)}\\}$ 都被存储起来，以供下一阶段使用。\n\n### 3. 因果注意力权重计算\n\n注意力机制计算一组权重 $\\alpha_{t,s}$，这些权重指定了时间 $t$ 的输出对来自时间 $s$ 的输入的“关注”程度。\n\n- **查询和键**：查询由RNN的最后一层 $h_t^{(L)}$ 生成，而键由指定的层 $\\ell$，$h_s^{(\\ell)}$ 生成。这是实验的核心，因为改变 $\\ell$ 会改变注意力机制的信息来源。\n  $$q_t = W_Q h_t^{(L)}, \\quad k_s = W_K h_s^{(\\ell)}$$\n- **注意力分数**：对于每个查询 $q_t$，会与所有满足 $s \\le t$ 的键 $k_s$ 计算分数，这遵循了时间序列数据的因果结构。分数使用缩放点积计算：\n  $$\\text{score}(t,s) = \\frac{q_t^\\top k_s}{\\sqrt{d_k}}$$\n  其中 $d_k$ 是查询和键的维度。\n- **Softmax归一化**：使用softmax函数将分数转换为概率分布（即注意力权重 $\\alpha_{t,s}$），该函数应用于所有允许的源位置 $u \\in \\{0, \\dots, t\\}$：\n  $$\\alpha_{t,s} = \\frac{\\exp(\\text{score}(t,s))}{\\sum_{u=0}^t \\exp(\\text{score}(t,u))}$$\n  为了数值稳定性，在进行指数运算之前，会从所有分数中减去给定 $t$ 的最大分数 $\\max_{u \\le t} \\{\\text{score}(t,u)\\}$。这可以防止浮点溢出，而不会改变最终的分布。该计算对每个时间步 $t \\in \\{0, \\dots, T-1\\}$ 执行。\n\n### 4. 注意力跨度分布计算\n\n最后一步是将计算出的注意力权重 $\\alpha_{t,s}$ 聚合到注意力跨度分布 $p(d)$ 中。该分布衡量了分配给特定时间滞后 $d = t-s$ 的平均注意力权重。\n\n该分布定义为：\n$$p(d) = \\frac{1}{T} \\sum_{t=0}^{T-1} \\sum_{s=0}^{t} \\alpha_{t,s} \\, \\mathbf{1}\\{t-s = d\\}$$\n其中 $\\mathbf{1}\\{\\cdot\\}$ 是指示函数。该公式可以重写为对每个滞后 $d$ 的贡献的平均值：\n$$p(d) = \\frac{1}{T} \\sum_{t=d}^{T-1} \\alpha_{t, t-d}$$\n在计算上，这等同于对 $T \\times T$ 注意力矩阵（其中 $\\alpha_{t,s}$ 是其元素）的第 $d$ 条次对角线上的元素求和，然后除以 $T$。得到的向量 $p = [p(0), p(1), \\dots, p(T-1)]$ 是一个概率分布。然后，根据要求将该向量的每个元素四舍五入到6位小数。通过对每个测试用例执行这整个过程，我们得到了所需的分布集合。", "answer": "```python\nimport numpy as np\n\ndef compute_dist_for_case(L, d_x, d_h, d_k, T, ell, model_seed, input_seed):\n    \"\"\"\n    Computes the attention span distribution for a single test case.\n    \"\"\"\n    # 1. Initialize RNGs and Model Parameters\n    model_rng = np.random.default_rng(model_seed)\n    \n    U_mats, R_mats = [], []\n    d_in_l = d_x\n    for l in range(L):\n        d_h_l = d_h[l]\n        \n        # U^(l) matrix\n        U_std = (1 / d_in_l)**0.5\n        U_mats.append(model_rng.normal(loc=0.0, scale=U_std, size=(d_h_l, d_in_l)))\n        \n        # R^(l) matrix\n        R_std = (1 / d_h_l)**0.5\n        R_mats.append(model_rng.normal(loc=0.0, scale=R_std, size=(d_h_l, d_h_l)))\n        \n        d_in_l = d_h_l\n    \n    # W_Q matrix for queries\n    d_h_L = d_h[L - 1]\n    W_Q_std = (1 / d_h_L)**0.5\n    W_Q = model_rng.normal(loc=0.0, scale=W_Q_std, size=(d_k, d_h_L))\n    \n    # W_K matrix for keys (ell is 1-indexed, convert to 0-indexed)\n    d_h_ell = d_h[ell - 1]\n    W_K_std = (1 / d_h_ell)**0.5\n    W_K = model_rng.normal(loc=0.0, scale=W_K_std, size=(d_k, d_h_ell))\n\n    # Initialize Input Data\n    input_rng = np.random.default_rng(input_seed)\n    X = input_rng.normal(loc=0.0, scale=1.0, size=(T, d_x))\n    \n    if T == 0:\n        return []\n\n    # 2. Stacked RNN Forward Pass\n    # Store hidden states h_t^(l) for t in -1..T-1\n    # H[l][t+1] stores h_t^(l)\n    H = [np.zeros((T + 1, d)) for d in d_h]\n    \n    for t in range(T):\n        t_idx = t + 1 # Index for current time step in H\n        \n        # Layer l=1 (0-indexed)\n        h_tm1_1 = H[0][t_idx - 1]\n        h_t_1 = np.tanh(U_mats[0] @ X[t] + R_mats[0] @ h_tm1_1)\n        H[0][t_idx] = h_t_1\n        \n        # Layers l > 1\n        for l in range(1, L):\n            h_t_lm1 = H[l - 1][t_idx]\n            h_tm1_l = H[l][t_idx - 1]\n            h_t_l = np.tanh(U_mats[l] @ h_t_lm1 + R_mats[l] @ h_tm1_l)\n            H[l][t_idx] = h_t_l\n    \n    # Extract hidden states for t=0..T-1\n    H_L_states = H[L - 1][1:]\n    H_ell_states = H[ell - 1][1:]\n\n    # 3. Attention Weight Calculation\n    Q = H_L_states @ W_Q.T  # Shape (T, d_k)\n    K = H_ell_states @ W_K.T  # Shape (T, d_k)\n    \n    alpha = np.zeros((T, T))\n    sqrt_dk = np.sqrt(d_k)\n    \n    for t in range(T):\n        q_t = Q[t]\n        # Causal keys: s = t\n        K_causal = K[0:t + 1]\n        \n        scores = (q_t @ K_causal.T) / sqrt_dk\n        \n        # Numerically stable softmax\n        stable_scores = scores - np.max(scores)\n        exp_scores = np.exp(stable_scores)\n        attention_weights = exp_scores / np.sum(exp_scores)\n        \n        alpha[t, 0:t + 1] = attention_weights\n\n    # 4. Attention Span Distribution\n    p = np.zeros(T)\n    for d in range(T):\n        # Summing alpha_{t, t-d} for t from d to T-1\n        # This is the -d'th diagonal of the alpha matrix\n        p[d] = np.sum(np.diag(alpha, k=-d))\n    \n    p /= T\n    \n    return np.round(p, 6).tolist()\n\ndef solve():\n    test_cases = [\n        { \"L\": 2, \"d_x\": 5, \"d_h\": (8, 8), \"d_k\": 6, \"T\": 20, \"ell\": 1, \"model_seed\": 2025, \"input_seed\": 100 },\n        { \"L\": 2, \"d_x\": 5, \"d_h\": (8, 8), \"d_k\": 6, \"T\": 20, \"ell\": 2, \"model_seed\": 2025, \"input_seed\": 100 },\n        { \"L\": 3, \"d_x\": 5, \"d_h\": (8, 6, 10), \"d_k\": 6, \"T\": 30, \"ell\": 3, \"model_seed\": 777, \"input_seed\": 555 },\n        { \"L\": 3, \"d_x\": 5, \"d_h\": (8, 6, 10), \"d_k\": 6, \"T\": 30, \"ell\": 1, \"model_seed\": 777, \"input_seed\": 555 },\n        { \"L\": 2, \"d_x\": 4, \"d_h\": (6, 6), \"d_k\": 4, \"T\": 1, \"ell\": 2, \"model_seed\": 9, \"input_seed\": 9 },\n    ]\n\n    results = []\n    for case in test_cases:\n        p_dist = compute_dist_for_case(**case)\n        results.append(p_dist)\n\n    # Format output as a list of lists string representation\n    output_str = \"[\" + \",\".join(str(res).replace(\" \", \"\") for res in results) + \"]\"\n    print(output_str)\n\nsolve()\n```", "id": "3175991"}]}