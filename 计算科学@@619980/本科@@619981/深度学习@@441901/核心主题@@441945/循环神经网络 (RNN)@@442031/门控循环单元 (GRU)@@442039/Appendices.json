{"hands_on_practices": [{"introduction": "为了理解 GRU 如何从数据中学习，我们必须探究损失函数的梯度是如何反向传播并更新其内部参数的。这个练习将通过推导更新门参数的梯度来揭示这一过程，让您亲手计算并理解当前输入 $x_t$ 和前一时刻的隐藏状态 $h_{t-1}$ 是如何共同塑造学习信号的。通过这个基础的微积分练习[@problem_id:3128076]，您将对 GRU 的核心训练动态建立起直观的认识。", "problem": "考虑一个单隐藏单元（$1$维）的门控循环单元（GRU）。设时间 $t$ 的输入为标量 $x_t \\in \\mathbb{R}$，前一时刻的隐藏状态为标量 $h_{t-1} \\in \\mathbb{R}$。该 GRU 具有用于更新门、重置门和候选状态的标量参数。将更新门参数表示为 $w_z, u_z, b_z \\in \\mathbb{R}$，重置门参数表示为 $w_r, u_r, b_r \\in \\mathbb{R}$，候选状态参数表示为 $w_n, u_n, b_n \\in \\mathbb{R}$。GRU 的更新由以下方程定义：\n$$z_t = \\sigma\\!\\left(w_z x_t + u_z h_{t-1} + b_z\\right),$$\n$$r_t = \\sigma\\!\\left(w_r x_t + u_r h_{t-1} + b_r\\right),$$\n$$\\tilde{h}_t = \\tanh\\!\\left(w_n x_t + u_n \\left(r_t h_{t-1}\\right) + b_n\\right),$$\n$$h_t = (1 - z_t)h_{t-1} + z_t \\tilde{h}_t,$$\n其中 $\\sigma(a) = \\frac{1}{1 + \\exp(-a)}$ 是 logistic sigmoid 函数，$\\tanh(a) = \\frac{\\exp(a) - \\exp(-a)}{\\exp(a) + \\exp(-a)}$ 是双曲正切函数。所有符号均表示标量。在求导时，将 $x_t$ 和 $h_{t-1}$ 视为时间 $t$ 的给定输入，并将除 $w_z, u_z, b_z$ 之外的所有参数都视为常数。\n\n从上述核心定义和基本微积分法则（链式法则以及 $\\sigma$ 和 $\\tanh$ 的导数）出发，推导出偏导数 $\\frac{\\partial h_t}{\\partial w_z}$、$\\frac{\\partial h_t}{\\partial u_z}$ 和 $\\frac{\\partial h_t}{\\partial b_z}$ 的闭式表达式，用 $x_t$、$h_{t-1}$、$z_t$ 和 $\\tilde{h}_t$（以及根据需要使用的初等函数）来表示。你的推导应清楚地表明来自 $x_t$ 和 $h_{t-1}$ 的学习信号如何进入这些梯度。\n\n请按 $\\frac{\\partial h_t}{\\partial w_z}$、$\\frac{\\partial h_t}{\\partial u_z}$、$\\frac{\\partial h_t}{\\partial b_z}$ 的顺序，以单行矩阵的形式，提供你的最终答案，答案为三个闭式解析表达式。不需要进行数值计算或四舍五入。", "solution": "该问题是良定的，有科学依据，并包含获得唯一解所需的所有信息。这是深度学习领域的一个标准推导，需要将基本微积分法则应用于门控循环单元（GRU）的定义方程。\n\n目标是计算隐藏状态 $h_t$ 相对于更新门参数 $w_z$、$u_z$ 和 $b_z$ 的偏导数。隐藏状态 $h_t$ 由以下方程给出：\n$$h_t = (1 - z_t)h_{t-1} + z_t \\tilde{h}_t$$\n这里，$h_{t-1}$ 是前一时刻的隐藏状态，$\\tilde{h}_t$ 是候选隐藏状态，$z_t$ 是更新门的输出。参数 $w_z$、$u_z$ 和 $b_z$ 仅通过它们在计算 $z_t$ 中的作用来影响 $h_t$。根据问题陈述，为了本次求导，所有其他量，包括 $\\tilde{h}_t$、$r_t$ 及其参数（$w_r, u_r, b_r, w_n, u_n, b_n$），以及输入 $x_t$ 和 $h_{t-1}$，都被视为常数。\n\n我们将使用链式法则进行求导。对于通用参数 $p \\in \\{w_z, u_z, b_z\\}$， $h_t$ 对 $p$ 的偏导数为：\n$$\\frac{\\partial h_t}{\\partial p} = \\frac{\\partial h_t}{\\partial z_t} \\frac{\\partial z_t}{\\partial p}$$\n\n首先，我们计算 $h_t$ 相对于 $z_t$ 的偏导数。$h_t$ 的方程可以重写为 $h_t = h_{t-1} - z_t h_{t-1} + z_t \\tilde{h}_t$。由于在此情境下 $\\tilde{h}_t$ 和 $h_{t-1}$ 被视为常数，求导过程很简单：\n$$\\frac{\\partial h_t}{\\partial z_t} = \\frac{\\partial}{\\partial z_t} \\left( h_{t-1} - z_t h_{t-1} + z_t \\tilde{h}_t \\right) = -h_{t-1} + \\tilde{h}_t = \\tilde{h}_t - h_{t-1}$$\n\n接下来，我们计算 $\\frac{\\partial z_t}{\\partial p}$ 这一项。更新门 $z_t$ 定义为：\n$$z_t = \\sigma(w_z x_t + u_z h_{t-1} + b_z)$$\n设 sigmoid 函数的自变量为 $a_z = w_z x_t + u_z h_{t-1} + b_z$。因此，$z_t = \\sigma(a_z)$。再次使用链式法则：\n$$\\frac{\\partial z_t}{\\partial p} = \\frac{\\partial \\sigma(a_z)}{\\partial a_z} \\frac{\\partial a_z}{\\partial p} = \\sigma'(a_z) \\frac{\\partial a_z}{\\partial p}$$\nlogistic sigmoid 函数 $\\sigma(a) = \\frac{1}{1 + \\exp(-a)}$ 的导数是 $\\sigma'(a) = \\sigma(a)(1 - \\sigma(a))$。因此，$\\sigma'(a_z) = z_t(1 - z_t)$。这给了我们：\n$$\\frac{\\partial z_t}{\\partial p} = z_t(1 - z_t) \\frac{\\partial a_z}{\\partial p}$$\n\n现在，我们必须为三个参数 $p \\in \\{w_z, u_z, b_z\\}$ 中的每一个计算 $\\frac{\\partial a_z}{\\partial p}$。\n1. 对于 $p = w_z$：\n$$\\frac{\\partial a_z}{\\partial w_z} = \\frac{\\partial}{\\partial w_z}(w_z x_t + u_z h_{t-1} + b_z) = x_t$$\n2. 对于 $p = u_z$：\n$$\\frac{\\partial a_z}{\\partial u_z} = \\frac{\\partial}{\\partial u_z}(w_z x_t + u_z h_{t-1} + b_z) = h_{t-1}$$\n3. 对于 $p = b_z$：\n$$\\frac{\\partial a_z}{\\partial b_z} = \\frac{\\partial}{\\partial b_z}(w_z x_t + u_z h_{t-1} + b_z) = 1$$\n\n最后，我们将这些结果结合起来，得到所需偏导数的最终表达式。\n\n对于 $\\frac{\\partial h_t}{\\partial w_z}$：\n$$\\frac{\\partial h_t}{\\partial w_z} = \\frac{\\partial h_t}{\\partial z_t} \\frac{\\partial z_t}{\\partial w_z} = (\\tilde{h}_t - h_{t-1}) \\left( z_t(1 - z_t) \\frac{\\partial a_z}{\\partial w_z} \\right) = (\\tilde{h}_t - h_{t-1}) z_t(1 - z_t) x_t$$\n这可以写作 $x_t z_t (1 - z_t) (\\tilde{h}_t - h_{t-1})$。这个表达式表明，关于输入权重 $w_z$ 的梯度与输入 $x_t$ 成正比。\n\n对于 $\\frac{\\partial h_t}{\\partial u_z}$：\n$$\\frac{\\partial h_t}{\\partial u_z} = \\frac{\\partial h_t}{\\partial z_t} \\frac{\\partial z_t}{\\partial u_z} = (\\tilde{h}_t - h_{t-1}) \\left( z_t(1 - z_t) \\frac{\\partial a_z}{\\partial u_z} \\right) = (\\tilde{h}_t - h_{t-1}) z_t(1 - z_t) h_{t-1}$$\n这可以写作 $h_{t-1} z_t (1 - z_t) (\\tilde{h}_t - h_{t-1})$。这个表达式表明，关于循环权重 $u_z$ 的梯度与前一时刻的隐藏状态 $h_{t-1}$ 成正比。\n\n对于 $\\frac{\\partial h_t}{\\partial b_z}$：\n$$\\frac{\\partial h_t}{\\partial b_z} = \\frac{\\partial h_t}{\\partial z_t} \\frac{\\partial z_t}{\\partial b_z} = (\\tilde{h}_t - h_{t-1}) \\left( z_t(1 - z_t) \\frac{\\partial a_z}{\\partial b_z} \\right) = (\\tilde{h}_t - h_{t-1}) z_t(1 - z_t) (1)$$\n这可以写作 $z_t (1 - z_t) (\\tilde{h}_t - h_{t-1})$。关于偏置项 $b_z$ 的梯度除了通过对 $z_t$ 的影响外，不直接依赖于 $x_t$ 或 $h_{t-1}$ 的具体值。\n\n在这三种情况下，学习信号都受到两个关键因素的调节：\n- 项 $z_t(1 - z_t)$，它是 sigmoid 门的导数。当 $z_t=0.5$ 时，该项达到最大值，而当 $z_t$ 趋近于 $0$ 或 $1$ 时，该项趋近于 $0$。这意味着当门处于“不确定”状态时学习最活跃，而当门饱和（完全打开或关闭）时学习减慢，这一特性可能导致梯度消失问题。\n- 项 $(\\tilde{h}_t - h_{t-1})$，它表示新候选状态与前一状态之间的差值。梯度与此差值成正比。如果候选状态与前一状态非常不同，更新门参数的梯度会更大，反映了它们在决定最终状态 $h_t$ 时的重要性增加。相反，如果 $h_{t-1}$ 和 $\\tilde{h}_t$ 几乎相同，更新门的值影响很小，其相关梯度也很小。", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\nx_t z_t (1 - z_t) (\\tilde{h}_t - h_{t-1})  h_{t-1} z_t (1 - z_t) (\\tilde{h}_t - h_{t-1})  z_t (1 - z_t) (\\tilde{h}_t - h_{t-1})\n\\end{pmatrix}\n}\n$$", "id": "3128076"}, {"introduction": "了解了 GRU 的学习方式后，我们来探索其最关键的特性——记忆。本练习[@problem_id:3128117]采用一个合成的“复制任务”，通过一个思想实验来量化更新门 $z_t$ 如何直接控制信息在网络中的保留时长。通过推导更新门的平均值与记忆“半衰期”（即时间尺度 $\\tau$）之间的精确关系，我们将把长期依赖这一抽象概念变得具体可衡量。", "problem": "要求您为门控循环单元（GRU）设计并分析一个合成的“复制”任务，该任务旨在分离并量化更新门如何控制记忆时间尺度。请在纯数学环境下，使用标量隐藏状态进行分析。使用以下核心定义和假设作为您推理的基础：\n\n- GRU 使用更新门来更新其隐藏状态。对于标量隐藏状态，在时间步 $t$ 的递推关系建模为 $h_t = (1 - z_t) h_{t-1} + z_t \\tilde{h}_t$，其中 $z_t \\in (0,1)$ 是更新门，$\\tilde{h}_t$ 是候选状态。\n- 在合成复制任务中，一个长度为 $T$ 的输入序列在 $T$ 个时间步内被写入隐藏状态，然后网络接收 $T$ 个时间步的中性输入（在此期间候选状态为零，即 $\\tilde{h}_t = 0$），最后是一个读出步骤。如果在读出步骤时，原始存储的隐藏分量所保留的幅值至少是其在写入阶段结束后立即的幅值的分数 $\\rho \\in (0,1)$，则认为读出是可靠的。\n- 在为期 $T$ 个时间步的中性输入延迟阶段，更新门变量 $z_t$ 是独立同分布的，均值为 $\\bar{z}$，并且每一步的乘法保留因子等于 $(1 - z_t)$。\n\n任务：\n- 从第一性原理出发，推导保证在读出步骤可靠保留信息的条件，并确定为满足长度为 $T$ 的序列的可靠性阈值 $\\rho$ 所需的最小平均更新门 $\\bar{z}_{\\min}$。请仅用 $T$ 和 $\\rho$ 来表示 $\\bar{z}_{\\min}$。\n- 通过为恒定门控情况定义每步保留因子和 $\\tau$ 之间的精确映射，将您的结果与由更新门控制的特征时间尺度 $\\tau$ 关联起来。给出您推导的 $\\bar{z}_{\\min}$ 所对应的最小时间尺度 $\\tau_{\\min}$，并提供其在小 $\\bar{z}$ 情况下的近似值，该近似值用 $T$ 和 $\\rho$ 表示。\n\n测试套件：\n为以下参数对 $(T,\\rho)$ 计算 $\\bar{z}_{\\min}$ 和 $\\tau_{\\min}$：\n- 情况 1：$T = 10$，$\\rho = 0.5$。\n- 情况 2：$T = 50$，$\\rho = 0.9$。\n- 情况 3 (边界)：$T = 1$，$\\rho = 0.99$。\n- 情况 4 (边缘)：$T = 100$，$\\rho = 0.01$。\n- 情况 5：$T = 8$，$\\rho = 0.8$。\n\n输出规格：\n- 您的程序必须生成单行输出，其中包含所有五个情况的结果。结果是一个以逗号分隔的列表，每个元素代表一个情况的结果，并用方括号括起来。\n- 每个情况的结果必须是一个包含两个元素的列表 $[\\bar{z}_{\\min}, \\tau_{\\min}]$，两个值都需四舍五入到 $6$ 位小数。\n- 因此，最终输出的形式必须是 $[[\\bar{z}_{\\min}^{(1)}, \\tau_{\\min}^{(1)}],[\\bar{z}_{\\min}^{(2)}, \\tau_{\\min}^{(2)}],\\dots,[\\bar{z}_{\\min}^{(5)}, \\tau_{\\min}^{(5)}]]$。", "solution": "问题陈述已经过严格验证，被认为是有效的。它在科学上基于门控循环单元（GRU）的既定数学公式，并就其记忆属性提出了一个明确定义的问题。合成“复制”任务是分析循环神经网络行为的标准方法。其语言客观，参数明确，允许得出唯一解。\n\n在“最小平均更新门 $\\bar{z}_{\\min}$”这一表述中存在一个轻微的歧义。正如稍后将展示的，为实现可靠保留而推导出的要求为平均更新门设定了一个上限，即 $\\bar{z} \\le C$，其中 $C$ 是由问题参数决定的常数。在这种情况下，“最小值”的定义不是唯一的，因为任何趋近于 0 的值都将满足该条件。最合乎逻辑且科学上最合理的解释是，问题要求的是此不等式的临界边界值，即允许的最大平均更新门。这个值代表了在仍然满足保留标准的情况下最快的“遗忘”速率。因此，我们将把 $\\bar{z}_{\\min}$ 解释为这个我们将要推导的边界值。\n\n解题过程分为三个部分。首先，我们从第一性原理推导对平均更新门 $\\bar{z}$ 的要求。其次，我们建立 $\\bar{z}$ 与记忆时间尺度 $\\tau$ 之间的关系。第三，我们使用这些结果来计算所需的量。\n\n**1. 平均更新门要求的推导**\n\n问题将时间步 $t$ 的标量隐藏状态 $h_t$ 的更新定义为：\n$$h_t = (1 - z_t) h_{t-1} + z_t \\tilde{h}_t$$\n这里，$z_t \\in (0,1)$ 是更新门激活值，$\\tilde{h}_t$ 是候选状态。\n\n该任务涉及一个 $T$ 步的写入阶段，随后是一个 $T$ 步的中性输入延迟阶段。设 $h_T$ 为写入阶段结束时的隐藏状态。在随后的延迟阶段中，从时间步 $t=T+1$到 $t=2T$，输入是中性的，这被指定为候选状态为零：$\\tilde{h}_t = 0$。\n\n对于 $t \\in \\{T+1, \\dots, 2T\\}$，递推关系简化为：\n$$h_t = (1 - z_t) h_{t-1}$$\n我们可以在延迟阶段的 $T$ 个时间步上展开这个递推关系，以用初始状态 $h_T$ 表示最终状态 $h_{2T}$：\n$$h_{T+1} = (1 - z_{T+1}) h_T$$\n$$h_{T+2} = (1 - z_{T+2}) h_{T+1} = (1 - z_{T+2})(1 - z_{T+1}) h_T$$\n$$\\vdots$$\n$$h_{2T} = \\left( \\prod_{i=1}^{T} (1 - z_{T+i}) \\right) h_T$$\n经过 $T$ 个延迟步骤后的总保留因子是乘积 $\\prod_{i=1}^{T} (1 - z_{T+i})$。\n\n可靠性条件规定，在读出步骤的隐藏状态幅值 $|h_{2T}|$ 必须至少是写入阶段结束后其幅值 $|h_T|$ 的一个分数 $\\rho \\in (0,1)$：\n$$|h_{2T}| \\ge \\rho |h_T|$$\n代入 $h_{2T}$ 的表达式：\n$$\\left| \\left( \\prod_{i=1}^{T} (1 - z_{T+i}) \\right) h_T \\right| \\ge \\rho |h_T|$$\n由于 $z_t \\in (0,1)$，项 $(1 - z_t)$ 总是正的，因此它们的乘积也是正的。我们可以简化该表达式（假设 $h_T \\neq 0$）：\n$$\\left( \\prod_{i=1}^{T} (1 - z_{T+i}) \\right) \\ge \\rho$$\n问题陈述指出，延迟阶段的更新门变量 $z_t$ 是均值为 $\\bar{z}$ 的独立同分布（i.i.d.）随机变量。因此，关于保留因子的条件必须在期望的意义上进行解释。我们对不等式两边取期望：\n$$E\\left[ \\prod_{i=1}^{T} (1 - z_{T+i}) \\right] \\ge \\rho$$\n由于 $z_t$ 变量的独立性，乘积的期望等于期望的乘积：\n$$\\prod_{i=1}^{T} E[1 - z_{T+i}] \\ge \\rho$$\n因为变量是同分布的，且 $E[z_t] = \\bar{z}$，我们有 $E[1 - z_t] = 1 - E[z_t] = 1 - \\bar{z}$。不等式变为：\n$$\\prod_{i=1}^{T} (1 - \\bar{z}) \\ge \\rho$$\n$$(1 - \\bar{z})^T \\ge \\rho$$\n为了找到 $\\bar{z}$ 的边界值，我们解这个不等式。由于 $1 - \\bar{z}  0$ 且 $\\rho  0$，我们可以对两边取 $T$ 次方根：\n$$1 - \\bar{z} \\ge \\rho^{1/T}$$\n$$\\bar{z} \\le 1 - \\rho^{1/T}$$\n这个不等式明确了可靠保留的要求。如前所述，我们将 $\\bar{z}_{\\min}$ 定义为该条件的边界值，也就是 $\\bar{z}$ 的最大允许值：\n$$\\bar{z}_{\\min} = 1 - \\rho^{1/T}$$\n\n**2. 最小时间尺度 $\\tau_{\\min}$ 的推导**\n\n特征时间尺度 $\\tau$ 是通过将离散的每步保留因子与连续的指数衰减过程 $e^{-1/\\tau}$ 相关联来定义的。对于一个恒定的门控值 $z_t = \\bar{z}$，每步保留因子是 $(1 - \\bar{z})$。其映射关系为：\n$$1 - \\bar{z} = e^{-1/\\tau}$$\n求解 $\\tau$ 得到它与 $\\bar{z}$ 的关系：\n$$\\ln(1 - \\bar{z}) = -1/\\tau$$\n$$\\tau = -\\frac{1}{\\ln(1 - \\bar{z})}$$\n一个较小的 $\\bar{z}$ 对应于一个较大的保留因子 $(1 - \\bar{z})$，从而对应一个更长的记忆时间尺度 $\\tau$。要求 $\\bar{z} \\le \\bar{z}_{\\min}$ 转化为对所需最小时间尺度的要求。最小时间尺度 $\\tau_{\\min}$ 对应于 $\\bar{z}$ 的最大允许值，即 $\\bar{z}_{\\min}$。\n$$\\tau_{\\min} = -\\frac{1}{\\ln(1 - \\bar{z}_{\\min})}$$\n代入我们关于 $\\bar{z}_{\\min}$ 的表达式：\n$$\\tau_{\\min} = -\\frac{1}{\\ln(1 - (1 - \\rho^{1/T}))} = -\\frac{1}{\\ln(\\rho^{1/T})}$$\n使用对数性质 $\\ln(a^b) = b \\ln(a)$：\n$$\\tau_{\\min} = -\\frac{1}{\\frac{1}{T} \\ln(\\rho)} = -\\frac{T}{\\ln(\\rho)}$$\n\n**3. 小 $\\bar{z}$ 近似**\n\n问题还要求对时间尺度进行小 $\\bar{z}$ 近似。一般关系是 $\\tau = -1/\\ln(1-\\bar{z})$。对于较小的 $x$ 值，$\\ln(1-x)$ 的泰勒级数展开为 $\\ln(1-x) \\approx -x - x^2/2 - \\dots$。其一阶近似为 $\\ln(1-x) \\approx -x$。\n将此应用于时间尺度方程，得到小 $\\bar{z}$ 近似：\n$$\\tau \\approx -\\frac{1}{-\\bar{z}} = \\frac{1}{\\bar{z}}$$\n这表明对于小的更新门值，记忆时间尺度约等于平均门激活值的倒数。通过将此关系与我们推导出的 $\\bar{z}_{\\min}$ 表达式结合使用，可以得到 $\\tau_{\\min}$ 关于 $T$ 和 $\\rho$ 的近似值：\n$$\\tau_{\\min, \\text{approx}} = \\frac{1}{\\bar{z}_{\\min}} = \\frac{1}{1 - \\rho^{1/T}}$$\n最终的计算将使用 $\\bar{z}_{\\min}$ 和 $\\tau_{\\min}$ 的精确公式。\n\n用于计算的公式是：\n$$ \\bar{z}_{\\min} = 1 - \\rho^{1/T} $$\n$$ \\tau_{\\min} = -\\frac{T}{\\ln(\\rho)} $$", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the GRU memory timescale problem for the given test cases.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    # Each case is a tuple (T, rho).\n    test_cases = [\n        (10, 0.5),   # Case 1\n        (50, 0.9),   # Case 2\n        (1, 0.99),   # Case 3\n        (100, 0.01), # Case 4\n        (8, 0.8),    # Case 5\n    ]\n\n    results_as_strings = []\n    for T, rho in test_cases:\n        # T is the sequence length (and delay duration).\n        # rho is the reliability threshold for retention.\n\n        # From first principles, the condition on the mean update gate z_bar is:\n        #   (1 - z_bar)^T = rho\n        # This implies z_bar = 1 - rho^(1/T).\n        # The problem asks for z_min, which is interpreted as the boundary value\n        # (the maximum permissible value) of this inequality.\n        z_min = 1 - np.power(rho, 1/T)\n\n        # The timescale tau is related to z_bar by (1 - z_bar) = exp(-1/tau).\n        #   tau = -1 / ln(1 - z_bar)\n        # The minimal required timescale tau_min corresponds to the maximal z_bar.\n        #   tau_min = -1 / ln(1 - z_min) = -1 / ln(rho^(1/T)) = -T / ln(rho).\n        tau_min = -T / np.log(rho)\n\n        # Format the results as a string \"[z_min_val,tau_min_val]\" with 6 decimal places.\n        case_result_str = f\"[{z_min:.6f},{tau_min:.6f}]\"\n        results_as_strings.append(case_result_str)\n\n    # The final output must be a single line in the specified format:\n    # [[z_min_1,tau_min_1],[z_min_2,tau_min_2],...]\n    final_output = f\"[{','.join(results_as_strings)}]\"\n    print(final_output)\n\n# Run the solver.\nsolve()\n```", "id": "3128117"}, {"introduction": "既然我们已经掌握了 GRU 的内部机制和记忆原理，现在让我们将其置于一个更广阔的实践背景中进行考量。这个练习[@problem_id:3128080]将通过模型容量和泛化能力的视角，对 GRU 及其“近亲”LSTM 进行比较。通过分析模型参数数量如何影响预测的测试误差，您将发现为什么在小数据集上，结构更简洁的 GRU 往往是避免过拟合的更优选择。", "problem": "您的任务是形式化并计算两种循环序列模型之间的原则性比较：门控循环单元（Gated Recurrent Units, GRU）和长短期记忆（Long Short-Term Memory, LSTM）网络。目标是展示在小数据集上，由于参数较少，GRU 在哪些情况下优于 LSTM，量化预测测试误差的改善，并将此改善与容量控制联系起来。\n\n出发点：使用经验风险最小化（Empirical Risk Minimization）和通过结构风险最小化（Structural Risk Minimization）实现的容量控制作为基本基础。假设预测的测试均方误差是一个不可约的噪声项与一个随模型大小增长、随样本大小减小的容量惩罚项之和。具体来说，将预测的测试均方误差建模为 $$\\mathcal{E}_{\\text{test}} = \\sigma^2 + \\lambda \\cdot \\frac{p}{N},$$ 其中 $\\sigma^2$ 是不可约的噪声方差，$\\lambda$ 是一个捕捉问题复杂度和训练方案的正常数，$p$ 是网络中可训练参数的总数，$N$ 是训练序列的数量。\n\n用于从第一性原理推导参数计数的架构组件定义：\n- 一个门控循环单元（GRU）在每个时间步有 $3$ 个不同的仿射变换：一个用于更新门，一个用于重置门，一个用于候选隐藏状态。每个变换将一个维度为 $x$ 的输入和维度为 $h$ 的前一个隐藏状态连接到维度为 $h$ 的当前隐藏状态，并包含维度为 $h$ 的偏置。\n- 一个长短期记忆（LSTM）网络在每个时间步有 $4$ 个不同的仿射变换：分别用于输入门、遗忘门、单元候选和输出门。每个变换将一个维度为 $x$ 的输入和维度为 $h$ 的前一个隐藏状态连接到维度为 $h$ 的当前隐藏状态，并包含维度为 $h$ 的偏置。\n- 两种模型都后接一个线性输出层，该层将维度为 $h$ 的隐藏状态映射到维度为 $y$ 的输出，并带有一个维度为 $y$ 的偏置。\n\n根据这些定义，推导出可训练参数的总数 $p_{\\text{GRU}}$ 和 $p_{\\text{LSTM}}$，作为 $x$、$h$ 和 $y$ 的函数；然后使用预测测试均方误差模型 $\\mathcal{E}_{\\text{test}} = \\sigma^2 + \\lambda \\cdot \\frac{p}{N}$，为每个测试用例计算 LSTM 和 GRU 之间的预测测试误差差值，定义为 $$\\Delta = \\mathcal{E}_{\\text{LSTM}} - \\mathcal{E}_{\\text{GRU}}.$$ 报告每个测试用例的 $\\Delta$ 值，四舍五入到六位小数。正的 $\\Delta$ 值表示，在给定的小数据集上，根据容量控制，预测 GRU 的测试误差低于 LSTM。\n\n实现一个程序，该程序：\n- 采用以下固定的参数值测试套件（见下文）。\n- 对于每个测试用例，计算 $p_{\\text{GRU}}$、$p_{\\text{LSTM}}$、$\\mathcal{E}_{\\text{GRU}}$、$\\mathcal{E}_{\\text{LSTM}}$ 和 $\\Delta$。\n- 生成单行输出，其中包含一个用方括号括起来的逗号分隔列表的结果（例如，$[\\Delta_1,\\Delta_2,\\Delta_3,\\Delta_4]$），每个 $\\Delta$ 值四舍五入到六位小数。\n\n测试套件（每个元组为 $(x,h,y,N,\\sigma^2,\\lambda)$）：\n- 情况 $1$（正常路径，小数据集）：$(x=\\;5,\\; h=\\;16,\\; y=\\;1,\\; N=\\;64,\\; \\sigma^2=\\;0.05,\\; \\lambda=\\;0.05)$\n- 情况 $2$（边界情况，较大数据集，容量惩罚减弱）：$(x=\\;5,\\; h=\\;16,\\; y=\\;1,\\; N=\\;2048,\\; \\sigma^2=\\;0.05,\\; \\lambda=\\;0.05)$\n- 情况 $3$（边缘情况，最小隐藏层大小）：$(x=\\;1,\\; h=\\;1,\\; y=\\;1,\\; N=\\;32,\\; \\sigma^2=\\;0.01,\\; \\lambda=\\;0.05)$\n- 情况 $4$（小数据集，较大隐藏层大小，放大容量差异）：$(x=\\;8,\\; h=\\;32,\\; y=\\;2,\\; N=\\;16,\\; \\sigma^2=\\;0.02,\\; \\lambda=\\;0.05)$\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表的结果，例如 $[\\Delta_1,\\Delta_2,\\Delta_3,\\Delta_4]$，每个值四舍五入到六位小数，且无空格。", "solution": "该问题已经过验证，被认为是自洽的、科学上基于统计学习理论的，并且是适定问题。所有必需的变量和定义都已提供，不存在矛盾。该问题是一个形式化的练习，旨在推导和比较两种常见的循环神经网络架构（GRU 和 LSTM）的参数复杂性，并使用简化的容量控制模型分析这种复杂性对预测测试误差的影响。\n\n解决方案分两个阶段进行。首先，我们根据所提供的定义，为门控循环单元（GRU）和长短期记忆（LSTM）网络推导可训练参数数量的通用公式。其次，我们使用这些公式计算每个测试用例的预测测试误差差值 $\\Delta$。\n\nGRU 和 LSTM 单元的核心都由仿射变换组成。一个将维度为 $d_{\\text{in}}$ 的输入向量映射到维度为 $d_{\\text{out}}$ 的输出向量的仿射变换形式为 $\\mathbf{y} = W\\mathbf{x} + \\mathbf{b}$。权重矩阵 $W$ 的维度为 $d_{\\text{out}} \\times d_{\\text{in}}$，贡献 $d_{\\text{out}} \\cdot d_{\\text{in}}$ 个参数。偏置向量 $\\mathbf{b}$ 的维度为 $d_{\\text{out}}$，贡献 $d_{\\text{out}}$ 个参数。这样一个变换的总参数数量为 $d_{\\text{out}} \\cdot d_{\\text{in}} + d_{\\text{out}}$。\n\n对于 GRU 和 LSTM 单元，在给定时间步，每个内部门的输入是外部输入向量 $\\mathbf{x}_t$（维度为 $x$）和前一个隐藏状态 $\\mathbf{h}_{t-1}$（维度为 $h$）的拼接。因此，循环单元内部仿射变换的输入维度为 $d_{\\text{in}} = x + h$。这些变换的输出是一个维度为 $h$ 的向量，所以 $d_{\\text{out}} = h$。因此，单个门变换的参数数量为 $h \\cdot (x + h) + h = hx + h^2 + h$。\n\n根据问题定义：\n一个 GRU 单元有 $3$ 个这样的仿射变换（用于更新门、重置门和候选激活）。GRU 单元中的总参数数量 $p_{\\text{GRU,cell}}$ 为：\n$$p_{\\text{GRU,cell}} = 3 \\cdot (h^2 + hx + h)$$\n一个 LSTM 单元有 $4$ 个这样的仿射变换（用于输入门、遗忘门、单元候选和输出门）。LSTM 单元中的总参数数量 $p_{\\text{LSTM,cell}}$ 为：\n$$p_{\\text{LSTM,cell}} = 4 \\cdot (h^2 + hx + h)$$\n\n两种架构都后接一个相同的线性输出层。该层将维度为 $h$ 的最终隐藏状态映射到维度为 $y$ 的输出。对于此变换，$d_{\\text{in}} = h$ 且 $d_{\\text{out}} = y$。输出层中的参数数量 $p_{\\text{out}}$ 为：\n$$p_{\\text{out}} = yh + y$$\n\n每个模型的总可训练参数数量 $p$ 是循环单元和输出层中参数的总和。\n$$p_{\\text{GRU}} = p_{\\text{GRU,cell}} + p_{\\text{out}} = 3(h^2 + hx + h) + (hy + y)$$\n$$p_{\\text{LSTM}} = p_{\\text{LSTM,cell}} + p_{\\text{out}} = 4(h^2 + hx + h) + (hy + y)$$\n\n问题要求计算预测测试误差的差值，$\\Delta = \\mathcal{E}_{\\text{LSTM}} - \\mathcal{E}_{\\text{GRU}}$。预测的测试误差由模型 $\\mathcal{E}_{\\text{test}} = \\sigma^2 + \\lambda \\cdot \\frac{p}{N}$ 给出。\n代入误差的表达式：\n$$\\Delta = \\left(\\sigma^2 + \\lambda \\frac{p_{\\text{LSTM}}}{N}\\right) - \\left(\\sigma^2 + \\lambda \\frac{p_{\\text{GRU}}}{N}\\right)$$\n不可约误差项 $\\sigma^2$ 被消去，表达式简化为：\n$$\\Delta = \\frac{\\lambda}{N} (p_{\\text{LSTM}} - p_{\\text{GRU}})$$\n现在，我们计算参数数量的差值：\n$$p_{\\text{LSTM}} - p_{\\text{GRU}} = \\left(4(h^2 + hx + h) + (hy + y)\\right) - \\left(3(h^2 + hx + h) + (hy + y)\\right)$$\n来自相同输出层的参数 $(hy + y)$ 被消去，剩下：\n$$p_{\\text{LSTM}} - p_{\\text{GRU}} = (4 - 3)(h^2 + hx + h) = h^2 + hx + h$$\n将此结果代回 $\\Delta$ 的表达式中，我们得到用于计算的最终公式：\n$$\\Delta = \\frac{\\lambda}{N} (h^2 + hx + h)$$\n这优雅地证明了，在此模型下，预测误差的差异仅仅是 LSTM 单元增加的参数数量的函数，该函数受问题复杂度 $\\lambda$ 的缩放，并与样本数 $N$ 成反比。正的 $\\Delta$ 值表明，由于其更简洁（参数更少），GRU 具有性能优势。\n\n现在我们将此公式应用于每个测试用例：\n\n情况 1：$(x=5, h=16, y=1, N=64, \\sigma^2=0.05, \\lambda=0.05)$\n$$\\Delta_1 = \\frac{0.05}{64} (16^2 + 16 \\cdot 5 + 16) = \\frac{0.05}{64} (256 + 80 + 16) = \\frac{0.05}{64} (352) = 0.275$$\n\n情况 2：$(x=5, h=16, y=1, N=2048, \\sigma^2=0.05, \\lambda=0.05)$\n$$\\Delta_2 = \\frac{0.05}{2048} (16^2 + 16 \\cdot 5 + 16) = \\frac{0.05}{2048} (352) \\approx 0.00859375$$\n\n情况 3：$(x=1, h=1, y=1, N=32, \\sigma^2=0.01, \\lambda=0.05)$\n$$\\Delta_3 = \\frac{0.05}{32} (1^2 + 1 \\cdot 1 + 1) = \\frac{0.05}{32} (1 + 1 + 1) = \\frac{0.05}{32} (3) = 0.0046875$$\n\n情况 4：$(x=8, h=32, y=2, N=16, \\sigma^2=0.02, \\lambda=0.05)$\n$$\\Delta_4 = \\frac{0.05}{16} (32^2 + 32 \\cdot 8 + 32) = \\frac{0.05}{16} (1024 + 256 + 32) = \\frac{0.05}{16} (1312) = 4.1$$\n\n然后将这些结果四舍五入到六位小数，用于最终输出。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the difference in predicted test error between LSTM and GRU models\n    based on a simplified capacity control model.\n    \"\"\"\n    # Test suite: (x, h, y, N, sigma_sq, lambda_val)\n    test_cases = [\n        # Case 1 (happy path, small dataset)\n        (5, 16, 1, 64, 0.05, 0.05),\n        # Case 2 (boundary with larger dataset where capacity penalty diminishes)\n        (5, 16, 1, 2048, 0.05, 0.05),\n        # Case 3 (edge case with minimal hidden size)\n        (1, 1, 1, 32, 0.01, 0.05),\n        # Case 4 (small dataset with larger hidden size amplifying capacity differences)\n        (8, 32, 2, 16, 0.02, 0.05)\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        x, h, y, N, sigma_sq, lambda_val = case\n\n        # The difference in the number of parameters between an LSTM and a GRU cell is\n        # p_diff = (4 * (h^2 + h*x + h)) - (3 * (h^2 + h*x + h))\n        # p_diff = h^2 + h*x + h\n        p_diff = h**2 + h * x + h\n        \n        # The difference in predicted test error is Delta = (lambda/N) * p_diff\n        delta = (lambda_val / N) * p_diff\n        \n        # The problem statement requires rounding to six decimal places.\n        # Using an f-string with a format specifier handles both rounding and formatting.\n        results.append(f\"{delta:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3128080"}]}