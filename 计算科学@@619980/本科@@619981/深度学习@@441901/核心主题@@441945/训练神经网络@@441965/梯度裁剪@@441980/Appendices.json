{"hands_on_practices": [{"introduction": "为了给理解梯度裁剪打下坚实的基础，我们首先探究一个来自经典优化领域的相关概念：信赖域方法。这个练习 [@problem_id:3153333] 将引导你通过比较模型的预测改善与实际函数值的改善，来推导优化步骤的接受与否的逻辑。理解这种“信任”局部模型的原则，是领会为何需要在复杂模型中控制更新步长的关键。", "problem": "在基于模型的无导数优化 (DFO) 中，一种常见的方法是信赖域 (TR) 框架，它构建一个局部代理模型来近似一个未知的目标函数。设 $f:\\mathbb{R}^{n}\\to\\mathbb{R}$ 是一个光滑但没有闭式解的函数，并设 $m_{k}:\\mathbb{R}^{n}\\to\\mathbb{R}$ 是在迭代点 $x_{k}\\in\\mathbb{R}^{n}$ 处的一个局部代理模型。试验步 $s_{k}$ 是通过在信赖域 $\\{s:\\|s\\|\\leq \\Delta_{k}\\}$ 上（近似）最小化 $m_{k}$ 来计算的，其中 $\\Delta_{k}0$ 是信赖域半径。将实际下降量和预测下降量分别定义为差值 $f(x_{k})-f(x_{k}+s_{k})$ 和 $m_{k}(0)-m_{k}(s_{k})$。仅从这些定义和以下指导原则出发：当实际改进与预测改进有意义地一致时，信赖域机制应接受一个步长，并应调整 $\\Delta_{k}$ 以保持模型的可靠性。请完成以下任务：\n\n1) 基于比率\n$$\n\\rho_{k}\\;=\\;\\frac{f(x_{k})-f(x_{k}+s_{k})}{m_{k}(0)-m_{k}(s_{k})},\n$$\n推导一个步长接受准则，包括一对有原则的阈值 $0  \\eta_{1}  \\eta_{2}  1$ 和用于缩小和扩大信赖域半径的乘法更新因子 $\\gamma_{\\mathrm{dec}}\\in(0,1)$ 和 $\\gamma_{\\mathrm{inc}}1$。您的推导必须从实际下降量和预测下降量的核心定义出发，并基于以下意图：模型的可信度应仅限于其预测与观测到的下降量一致的程度。\n\n2) 构建一个具体的一维病态例子，其中比率 $\\rho_{k}$ 会因为模型偏差而产生误导。使用以下数据来实例化该例子并计算一个数值：\n- 目标函数 $f(x)=(x-1)^{2}$。\n- 当前迭代点 $x_{k}=0$ 和信赖域半径 $\\Delta_{k}=0.5$。\n- 二次模型 $m_{k}(s)=a+bs+\\tfrac{1}{2}cs^{2}$，系数为 $b=-0.2$ 和 $c=0.1$（$a$ 可取任意值，因为它在预测下降量中被抵消）。\n- 试验步 $s_{k}$ 是 $m_{k}(s)$ 在区间 $\\{-\\Delta_{k}\\le s\\le \\Delta_{k}\\}$ 上的精确极小值点。\n\n计算此例子的 $\\rho_{k}$ 值，并用文字解释为何它对模型质量具有误导性。给出您的最终答案，即 $\\rho_{k}$ 的值，四舍五入到四位有效数字。无需单位。", "solution": "该问题是有效的。它在科学上基于数值优化的理论，特别是基于模型的无导数信赖域方法。该问题是适定的、客观的，并包含了推导概念框架和计算所需数值的所有必要信息。\n\n问题包括两部分。第一部分要求从第一性原理出发，推导标准的信赖域步长接受和半径更新机制。第二部分要求计算一个病态情况下的特定性能比率 $\\rho_k$，并解释为何该比率具有误导性。\n\n**第一部分：信赖域更新规则的推导**\n\n信赖域方法的核心在于管理局部代理模型 $m_k(s)$ 与真实目标函数 $f(x)$ 之间的一致性。对于试验步 $s_k$，衡量这种一致性的量是比率 $\\rho_k$：\n$$\n\\rho_{k} = \\frac{\\text{ared}}{\\text{pred}} = \\frac{f(x_{k})-f(x_{k}+s_{k})}{m_{k}(0)-m_{k}(s_{k})}\n$$\n这里，`ared` 是目标函数的“实际下降量”，`pred` 是根据模型得出的“预测下降量”。我们正在最小化 $f$，因此假设 `pred` 是正的，因为选择 $s_k$ 是为了减小模型值 $m_k$。\n\n指导原则是，如果实际下降量是预测下降量的一个有意义的部分，则接受步长 $s_k$，并根据模型预测结果的准确程度调整信赖域半径 $\\Delta_k$。\n\n我们可以分析 $\\rho_k$ 的值来创建一套规则：\n\n情况1：模型与函数的一致性极好。\n如果 $\\rho_k$ 接近 1，那么 `ared` $\\approx$ `pred`，表明模型在信赖域内是函数行为的一个高度准确的预测器。如果 $\\rho_k  1$，实际下降量甚至比预测的还要好，这也是一个成功步长的标志。我们可以将这些“非常好”的结果归为一类。为了将其形式化，我们使用一个阈值 $\\eta_2 \\in (0, 1)$。如果 $\\rho_k > \\eta_2$，则认为模型的性能极好。因此，应该接受步长 $s_k$ ($x_{k+1} = x_k + s_k$) 并且我们对模型的信任度应该增加。这就为扩大下一次迭代的信赖域提供了理由，以允许采取更具雄心的步长：$\\Delta_{k+1} = \\gamma_{\\text{inc}} \\Delta_k$，其中 $\\gamma_{\\text{inc}} > 1$。\n\n情况2：模型与函数的一致性差。\n如果 $\\rho_k$ 很小且为正，或者是负数，那么模型是一个差的预测器。如果 $\\rho_k \\le 0$，尽管模型预测会下降，但步长根本未能减小目标函数（`ared` $\\le 0$）。为了处理这种情况，我们引入一个阈值 $\\eta_1$，且 $0  \\eta_1  \\eta_2  1$。如果 $\\rho_k \\le \\eta_1$，则一致性很差。实际下降量要么是负的，要么是所承诺下降量的微不足道的一部分。在这种情况下，必须拒绝步长 $s_k$ ($x_{k+1} = x_k$)，因为它是无效的。差的预测表明信赖域对于模型来说太大了，不足以保证其可靠性。因此，必须缩小半径：$\\Delta_{k+1} = \\gamma_{\\text{dec}} \\Delta_k$，其中 $\\gamma_{\\text{dec}} \\in (0, 1)$。\n\n情况3：模型与函数的一致性可接受。\n这种中间情况发生在 $\\eta_1  \\rho_k \\le \\eta_2$ 时。此时，实际下降量是预测下降量的足够大的一部分，可以认为步长是成功的。因此，接受步长：$x_{k+1} = x_k + s_k$。然而，模型的准确性不足以保证通过扩大信赖域来增加我们的信任度。反之，它也没有不准确到需要缩小信赖域的程度。最合乎逻辑的行动是通过保持半径不变来维持当前的信任水平：$\\Delta_{k+1} = \\Delta_k$。\n\n总之，推导出的规则是：\n1.  如果 $\\rho_k \\le \\eta_1$：拒绝该步。设置 $x_{k+1} = x_k$ 并缩小半径：$\\Delta_{k+1} = \\gamma_{\\text{dec}} \\Delta_k$。\n2.  如果 $\\eta_1  \\rho_k \\le \\eta_2$：接受该步。设置 $x_{k+1} = x_k + s_k$ 并保持半径不变：$\\Delta_{k+1} = \\Delta_k$。\n3.  如果 $\\rho_k > \\eta_2$：接受该步。设置 $x_{k+1} = x_k + s_k$ 并扩大半径：$\\Delta_{k+1} = \\gamma_{\\text{inc}} \\Delta_k$。\n\n这些规则从调整信赖域大小以反映代理模型的观测可靠性的原则中逻辑地得出。\n\n**第二部分：病态例子的计算与分析**\n\n我们被给予以下数据：\n- 目标函数：$f(x) = (x-1)^2$\n- 当前迭代点：$x_k = 0$\n- 信赖域半径：$\\Delta_k = 0.5$\n- 二次模型：$m_k(s) = a + bs + \\frac{1}{2}cs^2$，其中 $b = -0.2$ 和 $c = 0.1$。\n$m_k(s) = a - 0.2s + 0.05s^2$。\n\n首先，我们通过在信赖域，即区间 $[-\\Delta_k, \\Delta_k] = [-0.5, 0.5]$ 上最小化 $m_k(s)$ 来找到试验步 $s_k$。\n二次函数 $m_k(s)$ 的无约束极小值点可以通过将其导数设为零来找到：\n$$\nm_k'(s) = -0.2 + 0.1s = 0 \\implies s = \\frac{0.2}{0.1} = 2\n$$\n二阶导数为 $m_k''(s) = 0.1 > 0$，证实了这是一个极小值点。\n无约束极小值点 $s=2$ 位于信赖域 $[-0.5, 0.5]$ 之外。由于 $m_k(s)$ 是一个凸抛物线，它在区间上的最小值必定出现在离无约束极小值点最近的边界点上。在 $[-0.5, 0.5]$ 中离 $2$ 最近的点是 $0.5$。\n因此，试验步是 $s_k = 0.5$。\n\n接下来，我们计算预测下降量 `pred`：\n$$\n\\text{pred} = m_k(0) - m_k(s_k) = m_k(0) - m_k(0.5)\n$$\n$m_k(0) = a - 0.2(0) + 0.05(0)^2 = a$。\n$m_k(0.5) = a - 0.2(0.5) + 0.05(0.5)^2 = a - 0.1 + 0.05(0.25) = a - 0.1 + 0.0125 = a - 0.0875$。\n$$\n\\text{pred} = a - (a - 0.0875) = 0.0875\n$$\n\n现在，我们计算实际下降量 `ared`：\n$$\n\\text{ared} = f(x_k) - f(x_k + s_k) = f(0) - f(0 + 0.5) = f(0) - f(0.5)\n$$\n$f(0) = (0-1)^2 = 1$。\n$f(0.5) = (0.5-1)^2 = (-0.5)^2 = 0.25$。\n$$\n\\text{ared} = 1 - 0.25 = 0.75\n$$\n\n最后，我们计算比率 $\\rho_k$：\n$$\n\\rho_k = \\frac{\\text{ared}}{\\text{pred}} = \\frac{0.75}{0.0875} = \\frac{7500}{875} = \\frac{60}{7} \\approx 8.571428...\n$$\n四舍五入到四位有效数字，我们得到 $\\rho_k \\approx 8.571$。\n\n**关于 $\\rho_k$ 具有误导性的解释：**\n计算出的值 $\\rho_k \\approx 8.571$ 是一个非常大的正数。根据第一部分推导的规则，这将被归类为一个“非常好”的步长（因为对于任何合理的 $\\eta_2  1$ 都有 $\\rho_k \\gg \\eta_2$），这表明模型 $m_k$ 是真实函数 $f$ 的一个极好的近似。这将导致算法扩大信赖域 $\\Delta_k$。\n\n然而，这个结论存在严重缺陷。$\\rho_k$ 的高值并不表示模型好，而是一个坏模型造成的假象。让我们比较模型与真实函数在当前点 $x_k=0$ 处的性质。$f$ 在 $x_k=0$ 附近的行为由 $g(s) = f(x_k+s) = f(s) = (s-1)^2 = s^2 - 2s + 1$ 描述。\n- 真实函数在 $s=0$ 处的梯度是 $g'(0) = -2$。\n- 真实函数在 $s=0$ 处的曲率（二阶导数）是 $g''(0) = 2$。\n\n模型的性质由其系数给出：\n- 模型在 $s=0$ 处的梯度是 $m_k'(0) = b = -0.2$。\n- 模型的曲率是 $m_k''(s) = c = 0.1$。\n\n模型表现出严重的偏差：它将真实梯度的量值低估了10倍（$-0.2$ 对比 $-2$），并将真实曲率低估了20倍（$0.1$ 对比 $2$）。这是一个非常差的局部模型。\n\n$\\rho_k$ 的值之所以大，是因为分母 `pred` $= 0.0875$ 极小。这个小的预测下降量是模型不正确的、过小的梯度和曲率的直接后果。模型是“平坦”和“保守”的，只预测了微小的改进。而分子 `ared` $= 0.75$ 很大，因为真实函数要陡峭得多。一个大数与一个很小的数之比是一个大数。\n\n因此，比率 $\\rho_k \\approx 8.571$ 是具有误导性的。它发出了模型质量极佳的信号，而实际上，它是一个严重低估函数局部几何性质的极差模型的症状。一个盲目相信这个高 $\\rho_k$ 值的算法会扩大信赖域，从而在更广的区域上依赖一个明显很差的模型，这可能会降低优化过程的性能或使其停滞。这个例子突显了在存在显著模型偏差的情况下，$\\rho_k$ 比率的一个关键弱点。", "answer": "$$\\boxed{8.571}$$", "id": "3153333"}, {"introduction": "现在，让我们将这些思想直接应用于深度学习中的核心技术——梯度裁剪。这个动手实践 [@problem_id:3131435] 将挑战你实现自适应的裁剪阈值，而非使用固定的阈值，你会用到例如中位数和中位数绝对偏差（MAD）等稳健统计量。通过这个编码练习，你将通过实验验证这些稳健的方法如何在存在梯度异常值时显著提高训练的稳定性。", "problem": "考虑一个基于梯度的离散时间优化过程，其在步骤 $t$ 的参数更新由 $w_{t+1} = w_t - \\eta \\cdot \\hat{g}_t$ 给出，其中 $\\eta > 0$ 是学习率，$\\hat{g}_t$ 是原始梯度 $g_t$ 的裁剪版本。当梯度 $g_t$ 的欧几里得范数超过一个阈值时，梯度裁剪会对其进行缩放，这可以防止因步长过大而导致训练不稳定。形式上，令 $r_t = \\lVert g_t \\rVert_2$ 表示 $g_t$ 的欧几里得范数。通过将 $g_t$ 投影到以原点为中心、半径为 $\\tau_t$ 的闭合欧几里得球上，来定义裁剪后的梯度 $\\hat{g}_t$，这等效于将 $g_t$ 乘以因子 $\\min\\{1, \\tau_t / r_t\\}$。裁剪后的步长范数则为 $u_t = \\eta \\cdot \\min\\{r_t, \\tau_t\\}$。\n\n你将实现自适应裁剪阈值 $\\tau_t$，该阈值根据过去梯度范数的一个滑动窗口计算得出，以捕捉近期的动态。为了以科学严谨的方式设计和比较阈值，请从以下应用于过去梯度范数的有限窗口 $W_t$ 的描述性统计核心定义开始。对于窗口 $W_t = \\{r_{t-k} \\mid 1 \\le k \\le \\min\\{W, t-1\\}\\}$，定义：\n- 样本均值 $\\mu_t$ 为 $\\mu_t = \\frac{1}{|W_t|} \\sum_{x \\in W_t} x$。\n- 样本中位数 $m_t$ 为 $W_t$ 的中间顺序统计量（当 $|W_t|$ 为偶数时，则为两个中间值的平均值）。\n- 中位数绝对偏差 (MAD) 为 $\\mathrm{MAD}_t = \\operatorname{median}\\big(\\{|x - m_t| : x \\in W_t\\}\\big)$。\n\n使用这些统计量来指定 $\\tau_t$ 的三个自适应裁剪规则：\n1. 基于均值的阈值，该阈值随样本均值 $\\mu_t$ 缩放。\n2. 基于中位数的阈值，该阈值随样本中位数 $m_t$ 缩放。\n3. 一个稳健的阈值，该阈值结合了样本中位数 $m_t$ 和中位数绝对偏差 $\\mathrm{MAD}_t$，并使用一个固定的比例因子 $\\kappa = 1.4826$，其动机源于正态分布的波动。\n\n在每种情况下，缩放常数 $c > 0$ 都乘以相关的统计量。当窗口 $W_t$ 为空时（即在 $t = 1$ 时），对所有方案设置 $\\tau_1 = c \\cdot r_1$。对于 $t > 1$，从之前的 $\\min\\{W, t-1\\}$ 个梯度范数构建 $W_t$。\n\n你的程序必须为每种裁剪规则计算在 $T$ 个步骤的固定时间范围内，裁剪后步长范数 $u_t$ 的经验方差，其定义为\n$$\n\\operatorname{Var}(u) = \\frac{1}{T} \\sum_{t=1}^{T} \\left(u_t - \\bar{u}\\right)^2, \\quad \\text{其中} \\quad \\bar{u} = \\frac{1}{T} \\sum_{t=1}^{T} u_t.\n$$\n使用这个方差来量化稳定性：较低的方差表示更新更稳定。\n\n然后，为每个测试案例报告两个浮点数，用以比较稳健统计量与基于均值的阈值的稳定性：\n- 比率 $R_{\\text{median}} = \\frac{\\operatorname{Var}(u)_{\\text{median}}}{\\operatorname{Var}(u)_{\\text{mean}}}$。\n- 比率 $R_{\\text{mad}} = \\frac{\\operatorname{Var}(u)_{\\text{mad}}}{\\operatorname{Var}(u)_{\\text{mean}}}$。\n\n小于 $1$ 的值表示相对于基于均值的阈值，稳定性有所提高。使用以下由梯度范数序列组成的确定性测试套件计算这些比率，每个序列都由一个平滑的基线加上注入的异常值构成。对于所有情况，基线定义为\n$$\nr_t^{\\text{base}} = b + a \\cdot \\sin\\left(\\frac{2\\pi t}{P}\\right),\n$$\n然后在列出的索引处用指定的异常值覆盖 $r_t$。所有量都是无量纲的，不适用任何物理单位。\n\n对四个测试案例使用以下参数：\n\n- 案例 1 (理想情况，无异常值)：$T = 60$，$W = 10$，$c = 1.0$，$\\eta = 0.05$，$b = 1.0$，$a = 0.2$，$P = 12$。无异常值：对所有 $t$，$r_t = r_t^{\\text{base}}$。\n- 案例 2 (单个大异常值)：与案例 1 相同的基线和超参数；设置 $r_{30} = 15.0$。\n- 案例 3 (多个大异常值)：与案例 1 相同的基线和超参数；设置 $r_{15} = 25.0$，$r_{35} = 25.0$，$r_{50} = 25.0$。\n- 案例 4 (边界窗口大小)：$T = 60$，$W = 1$，$c = 1.0$，$\\eta = 0.05$，$b = 1.0$，$a = 0.2$，$P = 12$；设置 $r_{30} = 50.0$。\n\n实现细节：\n- 对每个案例，根据基线和异常值定义，确定性地构建完整序列 $\\{r_t\\}_{t=1}^{T}$。\n- 对每个 $t$，构建 $W_t = \\{r_{t-k}\\}$，其中 $1 \\le k \\le \\min\\{W, t-1\\}$。\n- 对于三种裁剪规则，使用来自 $W_t$ 的相应统计量和上述缩放常数计算阈值。\n- 计算裁剪后的步长范数 $u_t = \\eta \\cdot \\min\\{r_t, \\tau_t\\}$。\n- 对每种裁剪规则，使用上面给出的总体方差公式计算 $\\operatorname{Var}(u)$。\n- 对每个案例，按顺序输出浮点数对 $\\left(R_{\\text{median}}, R_{\\text{mad}}\\right)$，并将四个案例的结果合并成一个扁平列表。\n\n最终输出格式：\n你的程序应生成一行输出，其中包含一个用方括号括起来的逗号分隔列表（例如，$[r_1,r_2,r_3,r_4,r_5,r_6,r_7,r_8]$），其中 $r_{2k-1}$ 和 $r_{2k}$ 分别是案例 $k$ 的 $R_{\\text{median}}$ 和 $R_{\\text{mad}}$。", "solution": "问题陈述经评估有效。它在科学上基于数值优化和稳健统计的原理，问题定义良好，具有清晰完整的定义和约束，并且表达客观。通过直接应用所提供的公式，可以推导出唯一的解。\n\n核心任务是比较三种自适应梯度裁剪策略的稳定性。稳定性通过在固定时间范围 $T$ 内裁剪后步长范数 $u_t$ 的经验方差来量化。较低的方差意味着优化过程中的参数更新更稳定、更少波动。这三种策略基于过去梯度范数的滑动窗口 $W_t$ 上计算的统计量来定义裁剪阈值 $\\tau_t$。\n\n首先，我们建立原始梯度范数序列 $\\{r_t\\}_{t=1}^T$，它作为我们分析的输入信号。对于每个测试案例，该序列是通过使用带有注入异常值的基准正弦函数确定性地生成的。基线由下式给出：\n$$\nr_t^{\\text{base}} = b + a \\cdot \\sin\\left(\\frac{2\\pi t}{P}\\right)\n$$\n其中 $a$ 是振幅，$b$ 是垂直偏移量，$P$ 是周期。然后在指定的时间步 $t$ 用特定值覆盖 $r_t$，以模拟异常梯度，这种情况在训练大型神经网络时很常见，并可能破坏学习过程的稳定性。\n\n问题的核心在于定义自适应裁剪阈值 $\\tau_t$。裁剪后的梯度 $\\hat{g}_t$ 是通过将原始梯度 $g_t$ 乘以一个因子 $\\min\\{1, \\tau_t / r_t\\}$ 得到的，其中 $r_t = \\lVert g_t \\rVert_2$。这确保了裁剪后梯度的范数不超过 $\\tau_t$。因此，我们记为裁剪后步长范数 $u_t$ 的更新步长范数为：\n$$\nu_t = \\eta \\cdot \\lVert \\hat{g}_t \\rVert_2 = \\eta \\cdot \\lVert g_t \\cdot \\min\\{1, \\tau_t / r_t\\} \\rVert_2 = \\eta \\cdot r_t \\cdot \\min\\{1, \\tau_t / r_t\\} = \\eta \\cdot \\min\\{r_t, \\tau_t\\}\n$$\n其中 $\\eta$ 是学习率。\n\n设置 $\\tau_t$ 的三个规则是基于从过去梯度范数的窗口 $W_t = \\{r_{t-k} \\mid 1 \\le k \\le \\min\\{W, t-1\\}\\}$ 计算出的统计量。对于第一步 $t=1$，窗口 $W_1$ 是空的。根据规定，所有三个规则的阈值都初始化为 $\\tau_1 = c \\cdot r_1$。对于后续步骤 ($t>1$)，我们定义：\n\n1.  **基于均值的阈值**：此规则使用窗口中范数的样本均值，$\\mu_t = \\frac{1}{|W_t|} \\sum_{x \\in W_t} x$。阈值与此均值成正比：\n    $$\n    \\tau_{t, \\text{mean}} = c \\cdot \\mu_t\n    $$\n    这种方法很简单，但对窗口中的异常值很敏感，因为单个较大的值会显著拉高均值。\n\n2.  **基于中位数的阈值**：此规则使用样本中位数 $m_t = \\operatorname{median}(W_t)$，它是一种稳健的集中趋势度量。阈值为：\n    $$\n    \\tau_{t, \\text{median}} = c \\cdot m_t\n    $$\n    由于中位数对异常值具有抵抗力，当梯度范数出现突然的峰值时，预计该阈值会比基于均值的阈值更稳定。\n\n3.  **基于稳健 MAD 的阈值**：此规则通过引入一种统计离散度的度量——中位数绝对偏差 (MAD) 来增强稳健性，其定义为 $\\mathrm{MAD}_t = \\operatorname{median}(\\{|x - m_t| : x \\in W_t\\})$。常数 $\\kappa = 1.4826$ 用于缩放 MAD，使得 $\\kappa \\cdot \\mathrm{MAD}_t$ 成为正态分布标准差的一致估计量。问题陈述指出，阈值“结合了样本中位数 $m_t$ 和中位数绝对偏差 $\\mathrm{MAD}_t$”，并且在所有情况下，缩放常数 $c$ 都“乘以相关的统计量”。对这些陈述最一致的解释是，将该统计量定义为“均值加标准差”的稳健模拟，从而得出公式：\n    $$\n    \\tau_{t, \\text{mad}} = c \\cdot (m_t + \\kappa \\cdot \\mathrm{MAD}_t)\n    $$\n    这种方法同时基于位置的稳健度量（$m_t$）和离散度的稳健度量（$\\mathrm{MAD}_t$）来设置阈值，旨在即使存在异常值也能实现高度稳定的行为。\n\n对于四个测试案例中的每一个，我们执行以下过程。首先，我们生成整个序列 $\\{r_t\\}_{t=1}^T$。然后，对于每个时间步 $t=1, \\dots, T$，我们根据上述规则计算三个阈值 $\\tau_{t, \\text{mean}}$、$\\tau_{t, \\text{median}}$ 和 $\\tau_{t, \\text{mad}}$。随后，我们计算相应的裁剪后步长范数 $\\{u_{t, \\text{mean}}\\}_{t=1}^T$、$\\{u_{t, \\text{median}}\\}_{t=1}^T$ 和 $\\{u_{t, \\text{mad}}\\}_{t=1}^T$。\n\n最后，我们通过计算其经验总体方差来量化每个步长范数序列的稳定性：\n$$\n\\operatorname{Var}(u) = \\frac{1}{T} \\sum_{t=1}^{T} \\left(u_t - \\bar{u}\\right)^2, \\quad \\text{其中} \\quad \\bar{u} = \\frac{1}{T} \\sum_{t=1}^{T} u_t\n$$\n为了比较性能，我们计算稳健方法与基准的基于均值的方法的方差之比：\n-   $R_{\\text{median}} = \\frac{\\operatorname{Var}(u)_{\\text{median}}}{\\operatorname{Var}(u)_{\\text{mean}}}$\n-   $R_{\\text{mad}} = \\frac{\\operatorname{Var}(u)_{\\text{mad}}}{\\operatorname{Var}(u)_{\\text{mean}}}$\n\n小于 $1$ 的比率表明，对于给定的梯度范数序列，相应的稳健方法比基于均值的方法提供更稳定的步长更新。对于案例 4，窗口大小 $W=1$，当 $t>1$ 时，统计量基于单个值 $r_{t-1}$。在这种情况下，$\\mu_t = m_t = r_{t-1}$ 且 $\\mathrm{MAD}_t = 0$。因此，所有三个阈值规则都简化为 $\\tau_t = c \\cdot r_{t-1}$，使得这三种方法完全相同。因此，产生的方差将相等，两个比率都将恰好为 $1.0$。这可作为实现的一个关键内部一致性检查。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print the results.\n    \"\"\"\n    \n    test_cases = [\n        # Case 1: No outliers\n        {'T': 60, 'W': 10, 'c': 1.0, 'eta': 0.05, 'b': 1.0, 'a': 0.2, 'P': 12, 'outliers': {}},\n        # Case 2: Single large outlier\n        {'T': 60, 'W': 10, 'c': 1.0, 'eta': 0.05, 'b': 1.0, 'a': 0.2, 'P': 12, 'outliers': {30: 15.0}},\n        # Case 3: Multiple large outliers\n        {'T': 60, 'W': 10, 'c': 1.0, 'eta': 0.05, 'b': 1.0, 'a': 0.2, 'P': 12, 'outliers': {15: 25.0, 35: 25.0, 50: 25.0}},\n        # Case 4: Boundary window size W=1\n        {'T': 60, 'W': 1, 'c': 1.0, 'eta': 0.05, 'b': 1.0, 'a': 0.2, 'P': 12, 'outliers': {30: 50.0}},\n    ]\n\n    KAPPA = 1.4826\n    \n    results = []\n    for case_params in test_cases:\n        ratios = compute_variance_ratios(case_params, KAPPA)\n        results.extend(ratios)\n\n    print(f\"[{','.join(f'{r:.8f}' for r in results)}]\")\n\ndef compute_variance_ratios(params, kappa):\n    \"\"\"\n    Computes the variance ratios for a single test case.\n    \n    Args:\n        params (dict): A dictionary of parameters for the test case.\n        kappa (float): The scaling constant for MAD.\n        \n    Returns:\n        tuple: A tuple containing (R_median, R_mad).\n    \"\"\"\n    T = params['T']\n    W = params['W']\n    c = params['c']\n    eta = params['eta']\n    b = params['b']\n    a = params['a']\n    P = params['P']\n    outliers = params['outliers']\n\n    # 1. Generate the sequence of raw gradient norms {r_t}\n    t_steps = np.arange(1, T + 1)\n    r_sequence = b + a * np.sin(2 * np.pi * t_steps / P)\n    for t, val in outliers.items():\n        # Problem uses 1-based indexing for time steps\n        if 1 = t = T:\n            r_sequence[t - 1] = val\n\n    u_mean = np.zeros(T)\n    u_median = np.zeros(T)\n    u_mad = np.zeros(T)\n\n    # 2. Iterate through time steps to compute thresholds and clipped norms\n    for t_idx in range(T):\n        t = t_idx + 1\n        r_t = r_sequence[t_idx]\n\n        if t == 1:\n            # Special case for t=1\n            tau_t_mean = c * r_t\n            tau_t_median = c * r_t\n            tau_t_mad = c * r_t\n        else:\n            # For t > 1, use trailing window\n            win_size = min(W, t - 1)\n            window = r_sequence[t_idx - win_size : t_idx]\n            \n            # Compute statistics\n            mu_t = np.mean(window)\n            m_t = np.median(window)\n            mad_t = np.median(np.abs(window - m_t))\n\n            # Compute thresholds for the three rules\n            tau_t_mean = c * mu_t\n            tau_t_median = c * m_t\n            tau_t_mad = c * (m_t + kappa * mad_t)\n        \n        # 3. Compute clipped step norms\n        u_mean[t_idx] = eta * min(r_t, tau_t_mean)\n        u_median[t_idx] = eta * min(r_t, tau_t_median)\n        u_mad[t_idx] = eta * min(r_t, tau_t_mad)\n\n    # 4. Compute population variances\n    # np.var computes population variance by default (ddof=0)\n    var_mean = np.var(u_mean)\n    var_median = np.var(u_median)\n    var_mad = np.var(u_mad)\n    \n    # Handle case where denominator is zero to avoid NaN\n    if var_mean == 0:\n        # This implies all step norms were identical, so others should be too.\n        # Ratios are 1 if variances are equal, or NaN if they differ.\n        # Safe to assume 1.0 in this context.\n        r_median = 1.0 if var_median == 0 else np.nan\n        r_mad = 1.0 if var_mad == 0 else np.nan\n    else:\n        r_median = var_median / var_mean\n        r_mad = var_mad / var_mean\n\n    return r_median, r_mad\n\nsolve()\n```", "id": "3131435"}, {"introduction": "梯度裁剪可以被看作是动态正则化的一种形式。为了将其置于更广阔的背景下，最后一个练习 [@problem_id:3143737] 将通过显式惩罚项来探索正则化，这是统计学习的基石。通过在线性回归的设定下比较不同强度的惩罚如何影响模型选择，你将对模型拟合与复杂度之间的普适性权衡获得更深的直觉，而这也是梯度裁剪等技术背后的根本原则。", "problem": "考虑一个标准的线性回归设定，其中响应变量为 $y \\in \\mathbb{R}^n$，设计矩阵为 $X \\in \\mathbb{R}^{n \\times d}$，数据根据 $y = X \\beta + \\varepsilon$ 生成，其中 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I_n)$。普通最小二乘法（OLS）为任何包含截距项和前 $p-1$ 个特征的嵌套模型提供系数估计（因此每个候选模型有 $p$ 个参数，包括截距项）。残差平方和（RSS）用于量化样本内误差。广义信息准则（GIC）是一种信息准则，其定义中的惩罚项与参数数量成正比。Mallows' $C_p$ 是一个经典的准则，旨在产生对样本外预测误差的无偏估计。\n\n你的任务是通过受控实验，将 Mallows' $C_p$ 与惩罚项为 $c\\,p$ 的广义信息准则（GIC）联系起来。具体来说，你将改变 GIC 中的惩罚乘数 $c$ 来模拟惩罚不足和惩罚过度的情况，并改变 Mallows' $C_p$ 中使用的外部噪声方差估计量的尺度以观察类似的行为。你必须证明 GIC 中改变 $c$ 与 Mallows' $C_p$ 中改变 $\\hat{\\sigma}^2$ 的尺度因子之间的映射关系。\n\n你必须遵循以下基本原则：\n- 假设线性模型 $y = X \\beta + \\varepsilon$ 以及普通最小二乘法（OLS）的性质，包括在正确指定的模型中的无偏性，以及模型复杂度与自由度之间的关系。\n- 使用残差平方和（RSS）来量化拟合质量，并使用来自全模型的噪声方差的无偏估计量。\n\n程序规范：\n1.  按如下方式确定性地合成数据：\n    -   使用固定的随机种子 $42$。\n    -   设观测值为 $n = 150$，特征为 $d = 10$。\n    -   从 $\\mathcal{N}(0,1)$ 中独立抽取 $X_{\\text{feat}} \\in \\mathbb{R}^{n \\times d}$ 的条目。\n    -   形成增广设计矩阵 $X = [\\mathbf{1}_n, X_{\\text{feat}}]$，其中 $\\mathbf{1}_n$ 是 $n$ 维的全一向量（截距列）。因此 $X \\in \\mathbb{R}^{n \\times (d+1)}$。\n    -   设置真实系数向量 $\\beta \\in \\mathbb{R}^{d+1}$，使其 $\\beta_0 = 0$（截距），$\\beta_1 = 3.0$，$\\beta_2 = -2.0$，$\\beta_3 = 1.0$，$\\beta_4 = 0.5$，其余所有条目均为 $0$。\n    -   设真实噪声标准差为 $\\sigma_{\\text{true}} = 1.0$，即 $\\varepsilon \\sim \\mathcal{N}(0, 1.0^2 I_n)$，并生成 $y = X \\beta + \\varepsilon$。\n2.  对于每个嵌套模型大小 $p \\in \\{1, 2, \\dots, d+1\\}$（其中 $p$ 计算截距和前 $p-1$ 个特征），计算 OLS 拟合及其对应的残差平方和 $\\text{RSS}(p)$。\n3.  使用无偏估计量 $\\hat{\\sigma}^2 = \\text{RSS}(d+1) / (n - (d+1))$，从全模型（$p = d+1$）中估计噪声方差。\n4.  将带惩罚参数 $c$ 的广义信息准则（GIC）定义为一个准则，对于模型大小 $p$，该准则在样本内拟合项上增加一个与 $c\\,p$ 成比例的项。你必须通过最小化一个使用 $\\text{RSS}(p)$ 和 $\\hat{\\sigma}^2$ 并包含惩罚项 $c\\,p$ 的形式的准则，来实现 GIC 下的模型选择。\n5.  使用从第一性原理推导出的规范定义（在解决方案中提供推导过程）来实现 Mallows' $C_p$ 下的模型选择。使用一个缩放后的外部噪声方差估计 $\\hat{\\sigma}^2_k = k \\cdot \\hat{\\sigma}^2$，其中 $k$ 是一个正尺度因子。\n6.  对于下面的每个测试用例，计算：\n    -   在 GIC 下选择的模型大小，记为 $p_{\\text{GIC}}(c)$。\n    -   在使用缩放方差 $\\hat{\\sigma}^2_k$ 的 Mallows' $C_p$ 下选择的模型大小，记为 $p_{C_p}(k)$。\n    -   报告整数差值 $p_{\\text{GIC}}(c) - p_{C_p}(k)$。\n\n测试套件：\n使用以下 $(c, k)$ 对集合，这些集合旨在测试匹配和不匹配的映射、理想路径以及边界条件：\n-   情况 1：$(c, k) = (2.0, 1.0)$。\n-   情况 2：$(c, k) = (3.0, 1.5)$。\n-   情况 3：$(c, k) = (1.0, 1.0)$。\n-   情况 4：$(c, k) = (4.0, 1.0)$。\n-   情况 5：$(c, k) = (2.0, 0.5)$。\n-   情况 6：$(c, k) = (0.0, 1.0)$。\n-   情况 7：$(c, k) = (8.0, 1.0)$。\n-   情况 8：$(c, k) = (2.0, 1.25)$。\n\n最终输出格式：\n你的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表（例如，“[r_1,r_2,\\dots,r_8]”），其中 $r_i$ 是上述第 $i$ 个测试用例的整数 $p_{\\text{GIC}}(c_i) - p_{C_p}(k_i)$。不应打印任何其他文本。", "solution": "该问题要求在线性回归的背景下，研究两种模型选择准则——广义信息准则（GIC）和 Mallows' $C_p$ 之间的关系。我们将建立它们的理论联系，然后使用一个确定性生成的数据集通过计算来验证这一联系。\n\n设定为标准线性模型 $y = X \\beta + \\varepsilon$，其中 $y \\in \\mathbb{R}^n$ 是响应向量，$X \\in \\mathbb{R}^{n \\times (d+1)}$ 是设计矩阵（包含截距项），$\\beta \\in \\mathbb{R}^{d+1}$ 是真实系数向量，$\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I_n)$ 是独立同分布的高斯噪声向量。\n\n我们考虑一组嵌套的候选模型，其中第 $p$ 个模型使用设计矩阵 $X$ 的前 $p$ 列。对于每个候选模型，我们可以计算普通最小二乘法（OLS）拟合及其对应的残差平方和，记为 $\\text{RSS}(p)$。一个好的模型选择准则在样本内拟合（低 $\\text{RSS}(p)$）和模型复杂度（低 $p$）之间取得平衡。\n\n首先，让我们形式化 Mallows' $C_p$ 准则。Mallows' $C_p$ 旨在成为真实均方预测误差（按噪声方差 $\\sigma^2$ 缩放）的无偏估计。对于一个有 $p$ 个参数的模型，该统计量定义为：\n$$ C_p = \\frac{\\text{RSS}(p)}{\\hat{\\sigma}^2} + 2p - n $$\n这里，$\\hat{\\sigma}^2$ 是真实误差方差 $\\sigma^2$ 的一个无偏估计。一个常见的选择，也是问题中指定的选择，是来自全模型（假设是无偏的）的估计量：\n$$ \\hat{\\sigma}^2 = \\frac{\\text{RSS}(d+1)}{n - (d+1)} $$\n由于对于所有模型，$n$ 是一个常数，因此通过最小化 $C_p$ 来选择模型等价于最小化 $\\frac{\\text{RSS}(p)}{\\hat{\\sigma}^2} + 2p$ 这个量。\n\n该问题引入了 Mallows' $C_p$ 的一个变体，它使用了一个缩放后的方差估计 $\\hat{\\sigma}^2_k = k \\cdot \\hat{\\sigma}^2$，其中 $k$ 是某个正尺度因子。要最小化的准则变为：\n$$ \\min_{p} \\left( \\frac{\\text{RSS}(p)}{k \\hat{\\sigma}^2} + 2p \\right) $$\n由于 $k$ 和 $\\hat{\\sigma}^2$ 对于关于 $p$ 的最小化是正常数，我们可以将目标函数乘以 $k \\hat{\\sigma}^2$ 而不改变最小值的位置。这得到一个等价的最小化问题：\n$$ \\min_{p} \\left( \\text{RSS}(p) + (2k) \\cdot p \\cdot \\hat{\\sigma}^2 \\right) $$\n这种形式将准则表示为样本内误差（$\\text{RSS}(p)$）和一个与参数数量 $p$ 呈线性的惩罚项之和。\n\n接下来，我们形式化广义信息准则（GIC）。GIC 代表了一族准则，其形式为一个拟合优度项加上一个对模型复杂度的惩罚项。问题指定了一个与 $c \\cdot p$ 成比例的惩罚项。为了与上面推导出的 Mallows' $C_p$ 公式进行直接比较，我们将要最小化的 GIC 分数定义为：\n$$ \\text{GIC\\_Score}(p) = \\text{RSS}(p) + c \\cdot p \\cdot \\hat{\\sigma}^2 $$\n这是一个 GIC 的有效实例，其中每个额外参数的惩罚是 $c \\cdot \\hat{\\sigma}^2$。通过此准则选择的模型 $p_{\\text{GIC}}(c)$ 是最小化该分数的模型。\n\n通过比较缩放后的 Mallows' $C_p$ 和我们的 GIC 公式的目标函数，我们可以建立一个直接的映射关系。\n$$ \\text{缩放的 } C_p \\text{ 目标函数：} \\quad \\text{RSS}(p) + (2k) \\cdot p \\cdot \\hat{\\sigma}^2 $$\n$$ \\text{GIC 目标函数：} \\quad \\text{RSS}(p) + c \\cdot p \\cdot \\hat{\\sigma}^2 $$\n这两个准则在数学上是相同的，当且仅当惩罚乘数相等，即：\n$$ c = 2k $$\n因此，我们预测，在 $c = 2k$ 的条件下，使用惩罚乘数 $c$ 的 GIC 所选择的模型将与使用方差缩放因子 $k$ 的 Mallows' $C_p$ 所选择的模型相同。在这种情况下，$p_{\\text{GIC}}(c) - p_{C_p}(k) = 0$。\n- 如果 $c > 2k$，GIC 对复杂度的惩罚比相应的缩放 $C_p$ 更强，这将偏爱参数较少的模型。我们期望 $p_{\\text{GIC}}(c) \\le p_{C_p}(k)$。\n- 如果 $c  2k$，GIC 施加的惩罚较弱，偏爱更复杂的模型。我们期望 $p_{\\text{GIC}}(c) \\ge p_{C_p}(k)$。\n\n计算过程如下：\n1.  为保证可复现性，将随机种子设为 $42$。根据问题规范生成数据：$n=150$ 个观测值，$d=10$ 个特征，一个指定的真实系数向量 $\\beta$，以及 $\\sigma_{\\text{true}}=1.0$ 的高斯噪声。\n2.  对于从 $1$ 到 $d+1=11$ 的每个嵌套模型大小 $p$，使用设计矩阵 $X$ 的前 $p$ 列拟合相应的 OLS 模型。\n3.  存储每个 $p$ 得到的 $\\text{RSS}(p)$。\n4.  使用来自全模型（$p=11$）的 $\\text{RSS}$ 计算无偏噪声方差估计 $\\hat{\\sigma}^2$。\n5.  对于测试套件中的每个 $(c, k)$ 对：\n    a. 计算所有 $p \\in \\{1, \\dots, 11\\}$ 的 GIC 分数：$\\text{GIC\\_Score}(p) = \\text{RSS}(p) + c \\cdot p \\cdot \\hat{\\sigma}^2$。\n    b. 找到最优模型大小 $p_{\\text{GIC}}(c) = \\arg\\min_{p} \\text{GIC\\_Score}(p)$。\n    c. 计算所有 $p \\in \\{1, \\dots, 11\\}$ 的等价 Mallows' $C_p$ 分数：$\\text{C}_p\\text{\\_Score}(p) = \\text{RSS}(p) + 2k \\cdot p \\cdot \\hat{\\sigma}^2$。\n    d. 找到最优模型大小 $p_{C_p}(k) = \\arg\\min_{p} \\text{C}_p\\text{\\_Score}(p)$。\n    e. 计算并存储整数差 $p_{\\text{GIC}}(c) - p_{C_p}(k)$。\n6.  最终输出是这些差值的列表。\n\n这个过程将数值上证明在 $c=2k$ 时的等价性，以及当该条件不成立时的惩罚不足/过度效应。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the model selection problem by comparing GIC and Mallows' Cp.\n    \"\"\"\n    # 1. Synthesize data deterministically as specified.\n    seed = 42\n    n = 150\n    d = 10\n    \n    rng = np.random.default_rng(seed)\n\n    # Draw features from N(0,1)\n    X_feat = rng.normal(loc=0.0, scale=1.0, size=(n, d))\n    \n    # Form the augmented design matrix X with an intercept column.\n    X = np.hstack((np.ones((n, 1)), X_feat))\n\n    # Set the true coefficient vector beta.\n    beta_true = np.zeros(d + 1)\n    beta_true[0] = 0.0  # Intercept\n    beta_true[1] = 3.0\n    beta_true[2] = -2.0\n    beta_true[3] = 1.0\n    beta_true[4] = 0.5\n    # beta_true[5:] are implicitly 0.0\n\n    # Generate the response variable y.\n    sigma_true = 1.0\n    epsilon = rng.normal(loc=0.0, scale=sigma_true, size=n)\n    y = X @ beta_true + epsilon\n\n    # 2. For each nested model size p, compute RSS(p).\n    p_values = np.arange(1, d + 2)  # Model sizes from 1 to d+1 (11)\n    rss_values = []\n    \n    for p in p_values:\n        X_p = X[:, :p]\n        # Use np.linalg.lstsq to perform OLS.\n        # It returns a tuple, where the second element is the sum of squared residuals (RSS).\n        # The returned residuals is a 1-element array, so we extract the value with [0].\n        # If the model fits perfectly, residuals is an empty array.\n        res = np.linalg.lstsq(X_p, y, rcond=None)[1]\n        \n        if res.size > 0:\n            rss_values.append(res[0])\n        else:\n            # This case occurs for a perfect fit, RSS = 0.\n            rss_values.append(0.0)\n            \n    rss_values = np.array(rss_values)\n\n    # 3. Estimate the noise variance from the full model.\n    rss_full = rss_values[-1]\n    degrees_of_freedom_full = n - (d + 1)\n    sigma_hat_sq = rss_full / degrees_of_freedom_full\n\n    # Test suite of (c, k) pairs.\n    test_cases = [\n        (2.0, 1.0),\n        (3.0, 1.5),\n        (1.0, 1.0),\n        (4.0, 1.0),\n        (2.0, 0.5),\n        (0.0, 1.0),\n        (8.0, 1.0),\n        (2.0, 1.25),\n    ]\n\n    results = []\n    for c, k in test_cases:\n        # 4. Compute selected model size under GIC.\n        # The criterion to minimize is RSS(p) + c * p * sigma_hat^2.\n        gic_scores = rss_values + c * p_values * sigma_hat_sq\n        # Find the model size p that minimizes the score.\n        # np.argmin returns the 0-based index of the minimum.\n        p_gic = p_values[np.argmin(gic_scores)]\n\n        # 5. Compute selected model size under scaled Mallows' Cp.\n        # The criterion is RSS(p)/(k*sigma_hat^2) + 2p, which is equivalent to\n        # minimizing RSS(p) + 2k * p * sigma_hat^2.\n        cp_scores = rss_values + 2 * k * p_values * sigma_hat_sq\n        p_cp = p_values[np.argmin(cp_scores)]\n\n        # 6. Compute and store the difference.\n        difference = p_gic - p_cp\n        results.append(int(difference))\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3143737"}]}