{"hands_on_practices": [{"introduction": "自动微分是现代深度学习的引擎，但它的内部工作原理常常像一个“黑箱”。为了揭开这个黑箱的神秘面纱，最有价值的方法之一就是亲手执行一次反向模式自动微分的过程。这个练习 [@problem_id:3100431] 将引导你为一个具体函数构建计算图（或称“磁带”），并手动通过一系列向量-雅可比积（VJP）拉取操作来计算梯度，从而让你像计算机一样思考，真正掌握其核心机制。", "problem": "在深度学习的背景下，考虑反向模式自动微分（AD）。通过使用链式法则反向遍历计算图，可以高效地计算标量损失函数关于参数的梯度。我们关注的函数是一个标量映射 $f:\\mathbb{R}^{2}\\to\\mathbb{R}$，由 $f(x,y)=\\sin(xy)+\\frac{\\exp(x)}{y}$ 给出，其中 $y\\neq 0$。请仅使用与计算图兼容的基本运算（乘法、正弦、指数和除法），构建一个用于计算 $f(x,y)$ 的最小中间变量集，并构造一个记录这些运算的父子关系的“磁带”（tape）。然后，利用复合函数的链式法则原理和向量-雅可比积（VJP）的概念，手动推导获得梯度 $\\nabla f(x,y)$ 所需的反向传播（VJP拉取）的精确序列。你的推导应清楚地指明反向遍历磁带的顺序，以及在每一步中对输入伴随变量的局部贡献。请以行向量的形式提供 $\\nabla f(x,y)$ 的最终解析表达式。不要进行四舍五入；最终答案必须是精确的符号表达式。", "solution": "问题陈述有效。它具有科学依据、良构、客观，并包含足够的信息以推导出唯一且有意义的解。该任务涉及将反向模式自动微分（AD）——一个计算微积分和深度学习中的基石算法——应用于一个可微函数。该过程是可形式化的，并与既定原则保持一致。\n\n我们的任务是使用反向模式AD的原理，计算函数 $f:\\mathbb{R}^{2}\\to\\mathbb{R}$（由 $f(x,y)=\\sin(xy)+\\frac{\\exp(x)}{y}$ 给出，其中 $y \\neq 0$）的梯度 $\\nabla f(x,y)$。这包括一个构建计算图和评估函数的前向传播，以及一个传播梯度的反向传播。\n\n首先，我们将函数分解为一系列基本运算。这个序列定义了计算图，或称为“磁带”。设输入为 $v_1 = x$ 和 $v_2 = y$。\n\n**前向传播：构建计算图**\n\n$f(x,y)$ 的求值过程可以由以下最小中间变量集表示：\n1.  $v_3 = v_1 \\cdot v_2 = x \\cdot y$\n2.  $v_4 = \\sin(v_3) = \\sin(xy)$\n3.  $v_5 = \\exp(v_1) = \\exp(x)$\n4.  $v_6 = \\frac{v_5}{v_2} = \\frac{\\exp(x)}{y}$\n5.  $v_7 = v_4 + v_6 = \\sin(xy) + \\frac{\\exp(x)}{y} = f(x,y)$\n\n这个序列构成了前向传播。磁带（tape）记录了这些运算及其依赖关系：$(v_3, \\text{mul}, v_1, v_2)$、$(v_4, \\sin, v_3)$、$(v_5, \\exp, v_1)$、$(v_6, \\text{div}, v_5, v_2)$、$(v_7, \\text{add}, v_4, v_6)$。\n\n**反向传播：使用链式法则计算梯度**\n\n反向传播计算最终输出 $v_7$ 关于每个中间变量 $v_i$ 的偏导数，这些偏导数被称为伴随变量，记作 $\\bar{v}_i = \\frac{\\partial f}{\\partial v_i} = \\frac{\\partial v_7}{\\partial v_i}$。该过程首先将输出节点的伴随变量初始化为1，即 $\\bar{v}_7 = \\frac{\\partial v_7}{\\partial v_7} = 1$。所有其他伴随变量均初始化为0。然后我们以逆拓扑顺序遍历该图。\n\n核心原理是链式法则。对于一个运算 $v_k = g(v_i, v_j, \\dots)$，其父节点的伴随变量通过累加子节点的伴随变量乘以局部偏导数来更新：\n$$ \\bar{v}_i = \\bar{v}_i + \\bar{v}_k \\frac{\\partial v_k}{\\partial v_i} $$\n$$ \\bar{v}_j = \\bar{v}_j + \\bar{v}_k \\frac{\\partial v_k}{\\partial v_j} $$\n... 以此类推。此操作实际上是一次向量-雅可比积（VJP）拉取。\n\n让我们按照前向传播的逆序计算伴随变量：\n\n1.  **开始：** 初始化伴随变量：$\\bar{v}_1=0, \\bar{v}_2=0, \\bar{v}_3=0, \\bar{v}_4=0, \\bar{v}_5=0, \\bar{v}_6=0$。\n    设置反向传播的种子：$\\bar{v}_7 = 1$。\n\n2.  **节点 $v_7 = v_4 + v_6$：**\n    父节点为 $v_4$ 和 $v_6$。\n    局部偏导数：$\\frac{\\partial v_7}{\\partial v_4} = 1$，$\\frac{\\partial v_7}{\\partial v_6} = 1$。\n    更新父节点伴随变量：\n    $\\bar{v}_4 \\leftarrow \\bar{v}_4 + \\bar{v}_7 \\cdot \\frac{\\partial v_7}{\\partial v_4} = 0 + 1 \\cdot 1 = 1$。\n    $\\bar{v}_6 \\leftarrow \\bar{v}_6 + \\bar{v}_7 \\cdot \\frac{\\partial v_7}{\\partial v_6} = 0 + 1 \\cdot 1 = 1$。\n    当前状态：$\\bar{v}_4=1, \\bar{v}_6=1$。\n\n3.  **节点 $v_6 = \\frac{v_5}{v_2}$：**\n    父节点为 $v_5$ 和 $v_2$。\n    局部偏导数：$\\frac{\\partial v_6}{\\partial v_5} = \\frac{1}{v_2}$，$\\frac{\\partial v_6}{\\partial v_2} = -\\frac{v_5}{v_2^2}$。\n    更新父节点伴随变量：\n    $\\bar{v}_5 \\leftarrow \\bar{v}_5 + \\bar{v}_6 \\cdot \\frac{\\partial v_6}{\\partial v_5} = 0 + 1 \\cdot \\frac{1}{v_2} = \\frac{1}{y}$。\n    $\\bar{v}_2 \\leftarrow \\bar{v}_2 + \\bar{v}_6 \\cdot \\frac{\\partial v_6}{\\partial v_2} = 0 + 1 \\cdot \\left(-\\frac{v_5}{v_2^2}\\right) = -\\frac{\\exp(x)}{y^2}$。\n    当前状态：$\\bar{v}_5 = \\frac{1}{y}$，$\\bar{v}_2=-\\frac{\\exp(x)}{y^2}$。\n\n4.  **节点 $v_5 = \\exp(v_1)$：**\n    父节点为 $v_1$。\n    局部偏导数：$\\frac{\\partial v_5}{\\partial v_1} = \\exp(v_1)$。\n    更新父节点伴随变量：\n    $\\bar{v}_1 \\leftarrow \\bar{v}_1 + \\bar{v}_5 \\cdot \\frac{\\partial v_5}{\\partial v_1} = 0 + \\frac{1}{y} \\cdot \\exp(v_1) = \\frac{\\exp(x)}{y}$。\n    当前状态：$\\bar{v}_1 = \\frac{\\exp(x)}{y}$。\n\n5.  **节点 $v_4 = \\sin(v_3)$：**\n    父节点为 $v_3$。\n    局部偏导数：$\\frac{\\partial v_4}{\\partial v_3} = \\cos(v_3)$。\n    更新父节点伴随变量：\n    $\\bar{v}_3 \\leftarrow \\bar{v}_3 + \\bar{v}_4 \\cdot \\frac{\\partial v_4}{\\partial v_3} = 0 + 1 \\cdot \\cos(v_3) = \\cos(xy)$。\n    当前状态：$\\bar{v}_3 = \\cos(xy)$。\n\n6.  **节点 $v_3 = v_1 \\cdot v_2$：**\n    父节点为 $v_1$ 和 $v_2$。注意，$v_1$ 和 $v_2$ 已经从其他路径接收了梯度；我们累加新的贡献。\n    局部偏导数：$\\frac{\\partial v_3}{\\partial v_1} = v_2$，$\\frac{\\partial v_3}{\\partial v_2} = v_1$。\n    更新父节点伴随变量：\n    $\\bar{v}_1 \\leftarrow \\bar{v}_1 + \\bar{v}_3 \\cdot \\frac{\\partial v_3}{\\partial v_1} = \\frac{\\exp(x)}{y} + \\cos(xy) \\cdot v_2 = \\frac{\\exp(x)}{y} + y \\cos(xy)$。\n    $\\bar{v}_2 \\leftarrow \\bar{v}_2 + \\bar{v}_3 \\cdot \\frac{\\partial v_3}{\\partial v_2} = -\\frac{\\exp(x)}{y^2} + \\cos(xy) \\cdot v_1 = -\\frac{\\exp(x)}{y^2} + x \\cos(xy)$。\n\n当我们计算完所有输入节点的伴随变量后，过程终止。\n最终梯度是输入变量伴随变量的最终值：\n$$ \\frac{\\partial f}{\\partial x} = \\bar{v}_1 = y \\cos(xy) + \\frac{\\exp(x)}{y} $$\n$$ \\frac{\\partial f}{\\partial y} = \\bar{v}_2 = x \\cos(xy) - \\frac{\\exp(x)}{y^2} $$\n\n梯度向量 $\\nabla f(x,y)$ 是这些偏导数组成的行向量：\n$$ \\nabla f(x,y) = \\begin{pmatrix} \\frac{\\partial f}{\\partial x} & \\frac{\\partial f}{\\partial y} \\end{pmatrix} = \\begin{pmatrix} y \\cos(xy) + \\frac{\\exp(x)}{y} & x \\cos(xy) - \\frac{\\exp(x)}{y^2} \\end{pmatrix} $$\n此推导过程严格遵循了反向模式自动微分的机械化步骤。", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\ny \\cos(xy) + \\frac{\\exp(x)}{y} & x \\cos(xy) - \\frac{\\exp(x)}{y^2}\n\\end{pmatrix}\n}\n$$", "id": "3100431"}, {"introduction": "在掌握了自动微分的基本机理后，下一步是探究其在实践中可能遇到的陷阱。直接、机械地应用求导法则有时会导致数值溢出等问题，从而破坏计算的稳定性。这个练习 [@problem_id:3100433] 将通过一个常见的激活函数 `softplus`，即 $y=\\ln(1+\\exp(x))$，来展示这一挑战，并教会你如何通过设计自定义梯度来解决它，这对于构建稳健的深度学习模型至关重要。", "problem": "你的任务是为函数 $y=\\text{softplus}(x)$（即 $y=\\log(1+e^{x})$）构建一个数值稳定的反向模式自动微分（AD）原语，该原语提供一个自定义的向量-雅可比积（VJP）。向量-雅可比积（VJP）将一个余切向量（上游敏感度）$v$ 映射为 $v$ 与 $y$ 相对于 $x$ 的梯度的乘积。你的实现必须在 $|x|\\gg 0$ 的情况下（特别是在 $x=100$ 和 $x=-100$ 附近）具有鲁棒性，并且必须在一组更广泛的值上进行测试。\n\n从导数的数学定义和链式法则出发，实现该计算的两个版本：\n- 一个基准的“朴素 AD”版本，该版本通过将规则直接应用于表达式 $y=\\log(1+e^{x})$ 来计算其值和梯度，不进行任何为数值稳定性而做的代数重排。\n- 一个自定义 VJP 版本，该版本返回 $y$ 和一个可调用对象，给定 $v$ 时，该可调用对象返回 VJP $v\\cdot \\frac{dy}{dx}$。其中，本原值 $y$ 和梯度 $\\frac{dy}{dx}$ 的计算对于大的正值和负值 $x$ 都是数值稳定的。\n\n设计一个测试套件，包含以下输入标量值 $x$：$100$、$ -100$、$0$、$10000$、$-10000$。对于每个测试用例，计算：\n1. 朴素 AD 的本原值和自定义 VJP 的本原值是否在 $10^{-12}$ 的绝对容差内一致，并且两者都是有限值。\n2. 朴素 AD 的梯度和自定义 VJP 的梯度是否在 $10^{-12}$ 的绝对容差内一致，并且两者都是有限值。\n\n你的程序的最终输出必须是单行，包含一个用方括号括起来的逗号分隔列表，由布尔值组成，顺序为 $[\\text{agree\\_y}(x_1),\\text{agree\\_grad}(x_1),\\text{agree\\_y}(x_2),\\text{agree\\_grad}(x_2),\\dots]$。其中，当且仅当两个本原值都是有限的且它们的绝对差小于或等于 $10^{-12}$ 时，$\\text{agree\\_y}(x)$ 为真。$\\text{agree\\_grad}(x)$ 的定义与梯度类似。不涉及物理单位，所有角度（如果有的话）都以弧度为单位，但此问题不使用角度。\n\n你的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果（例如 $[\\text{result}_1,\\text{result}_2,\\text{result}_3,\\dots]$）。", "solution": "该问题要求为 softplus 函数 $y(x) = \\log(1+e^x)$ 实现并比较两种反向模式自动微分（AD）原语。其中一个实现是公式的“朴素”直接转换，另一个是为数值稳定性而设计的“自定义 VJP”。\n\n首先，我们定义函数及其导数。softplus 函数由下式给出：\n$$y(x) = \\log(1+e^x)$$\n使用链式法则，其相对于 $x$ 的导数为：\n$$\\frac{dy}{dx} = \\frac{1}{1+e^x} \\cdot \\frac{d}{dx}(1+e^x) = \\frac{e^x}{1+e^x}$$\n这个导数是 logistic sigmoid 函数，通常表示为 $\\sigma(x)$。\n\n一个函数 $y=f(x)$ 的反向模式 AD 原语通常由两部分组成：一个计算本原输出值 $y$ 的前向传播，以及一个提供向量-雅可比积（VJP）的函数。VJP 通过关系 $\\bar{x} = J_f^T \\bar{y}$ 将输出的余切空间中的向量 $\\bar{y}$ 映射到输入的余切空间中的向量 $\\bar{x}$，其中 $J_f$ 是 $f$ 的雅可比矩阵。对于标量函数 $y=f(x)$，雅可比矩阵就是标量导数 $\\frac{dy}{dx}$。余切“向量”$\\bar{y}$ 是一个标量，在问题陈述中表示为 $v$。因此，VJP 是标量积 $v \\cdot \\frac{dy}{dx}$。恢复梯度本身的一个标准方法是使用 $v=1$ 来评估 VJP。\n\n我们现在将分析朴素实现的数值稳定性，并推导出一个稳定的替代方案。\n\n**朴素 AD 实现**\n直接实现使用如下所示的公式：\n- 本原值：$y = \\log(1+e^x)$\n- 梯度：$\\frac{dy}{dx} = \\frac{e^x}{1+e^x}$\n\n让我们分析这些表达式在浮点运算中对于极端 $x$ 值的行为。\n1.  **大的正值 $x$（例如，$x \\to \\infty$）：** 项 $e^x$呈指数级增长。对于足够大的 $x$（例如，在标准的 64 位浮点数中 $x > 709.78$），$e^x$会溢出，产生一个无穷大值（`inf`）。\n    - 朴素本原值：$y = \\log(1 + \\text{inf}) = \\log(\\text{inf}) = \\text{inf}$。这在数值上是不正确的。渐近地，对于大的 $x$，$y(x) \\approx \\log(e^x) = x$。\n    - 朴素梯度：$\\frac{dy}{dx} = \\frac{\\text{inf}}{1+\\text{inf}} = \\frac{\\text{inf}}{\\text{inf}}$，其计算结果为 `NaN`（非数字）。这也是不正确的。渐近地，梯度应趋近于 $1$。\n\n2.  **大的负值 $x$（例如，$x \\to -\\infty$）：** 项 $e^x$ 会下溢为 $0$。\n    - 朴素本原值：$y = \\log(1+0) = 0$。这在数值上是稳定和正确的。\n    - 朴素梯度：$\\frac{dy}{dx} = \\frac{0}{1+0} = 0$。这也是一个稳定和正确的计算。\n\n显然，由于对于大的正输入会发生溢出，朴素实现是有缺陷的。\n\n**自定义 VJP：一个数值稳定的实现**\n为了创建一个稳定的实现，我们必须对表达式进行代数重排，以避免在 $x$ 为大的正值时计算 $e^x$。\n\n对于本原值 $y(x)$，当 $x > 0$ 时：\n$$y(x) = \\log(1+e^x) = \\log\\left(e^x \\cdot (e^{-x} + 1)\\right) = \\log(e^x) + \\log(1+e^{-x}) = x + \\log(1+e^{-x})$$\n在这种形式下，对于大的正值 $x$，我们计算 $e^{-x}$，它会安全地下溢到 $0$。表达式 $x + \\log(1+e^{-x})$ 正确地计算出约等于 $x$ 的值，避免了溢出。对于 $x \\le 0$，原始表达式 $y(x) = \\log(1+e^x)$ 是稳定的。\n\n对于梯度 $\\frac{dy}{dx}$，当 $x > 0$ 时：\n$$\\frac{dy}{dx} = \\frac{e^x}{1+e^x} = \\frac{e^x \\cdot e^{-x}}{(1+e^x) \\cdot e^{-x}} = \\frac{1}{e^{-x} + 1}$$\n这种形式也通过使用 $e^{-x}$ 避免了 $x$ 为大的正值时发生溢出，并正确地计算出约等于 $1$ 的值。对于 $x \\le 0$，原始表达式 $\\frac{dy}{dx} = \\frac{e^x}{1+e^x}$ 是稳定的。\n\n结合这些观察，我们可以定义一个分段函数来进行稳定计算：\n- **稳定本原值 $y(x)$**：\n$$\ny_{stable}(x) = \n\\begin{cases} \nx + \\log(1+e^{-x}) & \\text{if } x > 0 \\\\\n\\log(1+e^x) & \\text{if } x \\le 0 \n\\end{cases}\n$$\n\n- **稳定梯度 $\\frac{dy}{dx}(x)$**：\n$$\ng_{stable}(x) = \n\\begin{cases} \n\\frac{1}{1+e^{-x}} & \\text{if } x \\ge 0 \\\\\n\\frac{e^x}{1+e^x} & \\text{if } x  0 \n\\end{cases}\n$$\n梯度的分割点选择为 $x=0$，在该点两个表达式的计算结果都为 $\\frac{1}{2}$，从而确保了连续性。对第一种情况使用 $x \\ge 0$ 确保了点 $x=0$ 被正确处理。\n\n自定义 VJP 原语将实现这些稳定计算。它将计算 $y_{stable}$ 和 $g_{stable}$，然后返回 $y_{stable}$ 和一个可调用函数（一个闭包），该函数接收 $v$ 并返回乘积 $v \\cdot g_{stable}$。\n\n然后，测试套件将比较朴素原语和稳定原语在一组输入值上的输出，检查其有限性和在指定容差内的符合度，从而证明朴素方法的失败和稳定方法的正确性。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and compares naive vs. numerically stable reverse-mode AD\n    primitives for the softplus function.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [100.0, -100.0, 0.0, 10000.0, -10000.0]\n    tolerance = 1e-12\n\n    def naive_ad_primitive(x: float):\n        \"\"\"\n        Computes the softplus primal and gradient using a direct,\n        numerically unstable implementation.\n        \"\"\"\n        # Catch numpy warnings about overflow/invalid values during naive computation\n        with np.errstate(over='ignore', invalid='ignore'):\n            # Primal: y = log(1 + exp(x))\n            primal_y = np.log(1.0 + np.exp(x))\n            \n            # Gradient: dy/dx = exp(x) / (1 + exp(x))\n            grad = np.exp(x) / (1.0 + np.exp(x))\n        \n        return primal_y, grad\n\n    def custom_vjp_primitive(x: float):\n        \"\"\"\n        Computes the softplus primal and a VJP callable using a\n        numerically stable implementation.\n        \"\"\"\n        # Stable primal computation\n        if x > 0:\n            primal_y = x + np.log(1.0 + np.exp(-x))\n        else:\n            primal_y = np.log(1.0 + np.exp(x))\n        \n        # Stable gradient computation\n        if x >= 0:\n            grad = 1.0 / (1.0 + np.exp(-x))\n        else:\n            grad = np.exp(x) / (1.0 + np.exp(x))\n\n        # The VJP is a function that takes an upstream gradient v\n        # and multiplies it by the local gradient.\n        def vjp_callable(v: float):\n            return v * grad\n            \n        return primal_y, vjp_callable\n\n    results = []\n    for x_val in test_cases:\n        # Get results from the naive implementation\n        y_naive, grad_naive = naive_ad_primitive(x_val)\n        \n        # Get results from the custom stable implementation\n        # The gradient is recovered by calling the VJP with v=1.0\n        y_custom, vjp_fn = custom_vjp_primitive(x_val)\n        grad_custom = vjp_fn(1.0)\n        \n        # 1. Compare primal values (y)\n        y_are_finite = np.isfinite(y_naive) and np.isfinite(y_custom)\n        if y_are_finite:\n            y_agree = np.abs(y_naive - y_custom) = tolerance\n        else:\n            y_agree = False\n        results.append(y_agree)\n        \n        # 2. Compare gradient values (dy/dx)\n        grad_are_finite = np.isfinite(grad_naive) and np.isfinite(grad_custom)\n        if grad_are_finite:\n            grad_agree = np.abs(grad_naive - grad_custom) = tolerance\n        else:\n            grad_agree = False\n        results.append(grad_agree)\n\n    # Final print statement in the exact required format.\n    # str(bool) gives 'True'/'False', which needs to be lowercased.\n    print(f\"[{','.join(map(lambda b: str(b).lower(), results))}]\")\n\nsolve()\n```", "id": "3100433"}, {"introduction": "一个常见且深刻的问题是：自动微分如何处理像 $\\mathrm{ReLU}$ 这样在某些点上不可导的函数？这个最后的练习 [@problem_id:3100437] 深入探讨了这一微妙但至关重要的议题，它揭示了情况比初看起来要复杂得多。你将分析一个复合函数 $\\mathrm{ReLU}(x)^3$ 在其“尖角”点 $x=0$ 的行为，并理解为何自动微分的链式法则应用有时能够“优雅地”绕过不可导问题，同时发现这可能给模型训练带来的“梯度卡住”的陷阱。", "problem": "考虑标量函数 $f:\\mathbb{R}\\to\\mathbb{R}$，其表达式为 $f(x)=\\mathrm{ReLU}(x)^3$，其中 $\\mathrm{ReLU}(x)=\\max(0,x)$。使用导数的极限定义和链式法则作为基础工具，分析函数 $f$ 在 $x=0$ 处的行为及其对基于梯度的训练的影响。特别是，请论述在不可微点处的次梯度，以及自动微分（AD）框架如何通过不同的选择来实现 $\\mathrm{ReLU}'(0)$。假设对于标量参数 $w$ 和有限目标值 $t\\in\\mathbb{R}$，平方误差损失函数为 $L(w)=\\frac{1}{2}(f(w)-t)^2$，并考虑初始化 $w=0$。选择所有正确的表述。\n\nA. 函数 $f$ 在 $x=0$ 处的次微分是区间 $[0,1]$，因此不同的自动微分（AD）框架可以合法地返回 $[0,1]$ 中的任意值作为 $x=0$ 处的梯度；这可能会在初始化时显著改变训练更新。\n\nB. 函数 $f$ 在 $x=0$ 处可微，且 $f'(0)=0$，因此任何使用链式法则的自动微分（AD）框架在 $x=0$ 处计算出的梯度均为 $0$，这与它对 $\\mathrm{ReLU}'(0)$ 的选择无关。\n\nC. 即使AD框架选择 $\\mathrm{ReLU}'(0)=1$，通过链式法则得到的函数 $f$ 在 $x=0$ 处的梯度也为 $0$。\n\nD. 对于平方损失函数 $L(w)=\\frac{1}{2}(f(w)-t)^2$，初始化 $w=0$ 会使得对于任意有限目标值 $t$，在 $w=0$ 处都有 $\\frac{dL}{dw}=0$，因此基于梯度的训练在第一次更新时不会离开 $w=0$。\n\nE. 因为函数 $f$ 在 $x=0$ 处不可微，AD框架必须在此处返回非数值（$\\text{NaN}$）；这通常会使训练不稳定。", "solution": "该问题陈述具有科学依据、提法恰当且客观。所有给出的定义和条件在数学上均是合理的，并且足以进行严谨的分析。该问题是有效的。\n\n我们将分析函数 $f(x)=\\mathrm{ReLU}(x)^3$ 及其在基于梯度的优化问题中的作用。\n\n首先，我们使用导数的极限定义来分析 $f(x)$ 在 $x=0$ 处的可微性。函数 $f(x)$ 可以写成分段形式：\n$$\nf(x) = \\left(\\max(0,x)\\right)^3 =\n\\begin{cases}\nx^3  \\text{if } x \\ge 0 \\\\\n0  \\text{if } x  0\n\\end{cases}\n$$\n在 $x=0$ 处的导数（如果存在）由以下极限给出：\n$$ f'(0) = \\lim_{h \\to 0} \\frac{f(0+h) - f(0)}{h} $$\n我们有 $f(0) = (\\max(0,0))^3 = 0^3 = 0$。该极限变为：\n$$ f'(0) = \\lim_{h \\to 0} \\frac{f(h)}{h} $$\n为了计算这个极限，我们必须检查左极限和右极限。\n右极限：\n$$ \\lim_{h \\to 0^+} \\frac{f(h)}{h} = \\lim_{h \\to 0^+} \\frac{h^3}{h} = \\lim_{h \\to 0^+} h^2 = 0 $$\n左极限：\n$$ \\lim_{h \\to 0^-} \\frac{f(h)}{h} = \\lim_{h \\to 0^-} \\frac{0}{h} = \\lim_{h \\to 0^-} 0 = 0 $$\n由于左极限和右极限都存在且相等，所以导数存在且 $f'(0)=0$。因此，函数 $f(x)$ 在 $x=0$ 处是可微的。\n\n接下来，我们分析自动微分（AD）框架将如何使用链式法则来计算这个导数。设 $u(x) = \\mathrm{ReLU}(x)$ 且 $g(u) = u^3$。那么 $f(x) = g(u(x))$。一个AD系统会机械地应用链式法则：\n$$ \\frac{df}{dx} = \\frac{dg}{du} \\cdot \\frac{du}{dx} $$\n基本函数的导数是 $\\frac{dg}{du} = 3u^2$ 和 $\\frac{du}{dx} = \\mathrm{ReLU}'(x)$。将 $u(x)$ 代回：\n$$ \\frac{df}{dx} = 3(\\mathrm{ReLU}(x))^2 \\cdot \\mathrm{ReLU}'(x) $$\n在 $x=0$ 处，该表达式变为：\n$$ \\left.\\frac{df}{dx}\\right|_{x=0} = 3(\\mathrm{ReLU}(0))^2 \\cdot \\mathrm{ReLU}'(0) = 3(0)^2 \\cdot \\mathrm{ReLU}'(0) = 0 \\cdot \\mathrm{ReLU}'(0) $$\n函数 $\\mathrm{ReLU}(x)$ 在 $x=0$ 处是不可微的，所以 $\\mathrm{ReLU}'(0)$ 在技术上是未定义的。AD框架通过从 $\\mathrm{ReLU}$ 在 $0$ 处的次微分（即区间 $[0,1]$）中指定一个特定的值来处理这个问题。对于 $\\mathrm{ReLU}'(0)$ 的常见选择是 $0$ 或 $1$。然而，无论为 $\\mathrm{ReLU}'(0)$ 选择哪个有限值，乘积 $0 \\cdot \\mathrm{ReLU}'(0)$ 始终为 $0$。因此，任何实现链式法则的AD框架都会计算出 $f(x)$ 在 $x=0$ 处的导数为 $0$。\n\n现在，让我们考虑损失函数 $L(w) = \\frac{1}{2}(f(w)-t)^2$ 和初始化 $w=0$。损失函数关于 $w$ 的梯度可通过链式法则求得：\n$$ \\frac{dL}{dw} = (f(w)-t) \\cdot f'(w) $$\n在初始化 $w=0$ 处，梯度为：\n$$ \\left.\\frac{dL}{dw}\\right|_{w=0} = (f(0)-t) \\cdot f'(0) $$\n使用我们之前计算出的值 $f(0)=0$ 和 $f'(0)=0$，我们得到：\n$$ \\left.\\frac{dL}{dw}\\right|_{w=0} = (0-t) \\cdot 0 = -t \\cdot 0 = 0 $$\n这个结果对任何有限的目标值 $t$ 都成立。在一个标准的梯度下降更新步骤 $w_{k+1} = w_k - \\eta \\frac{dL}{dw}$ 中，如果初始参数是 $w_0=0$ 且梯度为 $0$，那么更新将是 $w_1 = 0 - \\eta \\cdot 0 = 0$。参数将不会改变。\n\n基于这些分析，我们来评估每个选项：\n\nA. 函数 $f$ 在 $x=0$ 处的次微分是区间 $[0,1]$，因此不同的自动微分（AD）框架可以合法地返回 $[0,1]$ 中的任意值作为 $x=0$ 处的梯度；这可能会在初始化时显著改变训练更新。\n这是**不正确**的。一个函数在其可微点处的次微分是包含其导数的单元素集合。既然我们已经证明了 $f'(0)=0$，那么 $f$ 在 $x=0$ 处的次微分就是 $\\partial f(0) = \\{0\\}$。区间 $[0,1]$ 是 $\\mathrm{ReLU}(x)$ 在 $x=0$ 处的次微分，而不是 $f(x) = \\mathrm{ReLU}(x)^3$ 的。因此，AD框架计算出的 $f$ 在 $x=0$ 处的梯度明确地为 $0$。\n\nB. 函数 $f$ 在 $x=0$ 处可微，且 $f'(0)=0$，因此任何使用链式法则的自动微分（AD）框架在 $x=0$ 处计算出的梯度均为 $0$，这与它对 $\\mathrm{ReLU}'(0)$ 的选择无关。\n这是**正确**的。我们的分析表明该陈述的两个部分都是正确的。函数在 $x=0$ 处可微，导数为 $0$。机械地应用链式法则得到的表达式为 $3(\\mathrm{ReLU}(0))^2 \\cdot \\mathrm{ReLU}'(0) = 0$，这与为 $\\mathrm{ReLU}'(0)$ 选择的有限值无关。\n\nC. 即使AD框架选择 $\\mathrm{ReLU}'(0)=1$，通过链式法则得到的函数 $f$ 在 $x=0$ 处的梯度也为 $0$。\n这是**正确**的。这是在B选项中确立的一般原则的一个特例。如果 $\\mathrm{ReLU}'(0)=1$，计算结果为 $\\left.\\frac{df}{dx}\\right|_{x=0} = 3(\\mathrm{ReLU}(0))^2 \\cdot 1 = 3(0)^2 \\cdot 1 = 0$。\n\nD. 对于平方损失函数 $L(w)=\\frac{1}{2}(f(w)-t)^2$，初始化 $w=0$ 会使得对于任意有限目标值 $t$，在 $w=0$ 处都有 $\\frac{dL}{dw}=0$，因此基于梯度的训练在第一次更新时不会离开 $w=0$。\n这是**正确**的。如前所导，$\\left.\\frac{dL}{dw}\\right|_{w=0} = (f(0)-t)f'(0) = (-t)(0) = 0$。梯度为零意味着标准的梯度下降步骤不会对参数 $w$ 产生任何改变。训练在初始化时会“卡住”。\n\nE. 因为函数 $f$ 在 $x=0$ 处不可微，AD框架必须在此处返回非数值（$\\text{NaN}$）；这通常会使训练不稳定。\n这是**不正确**的。其前提“$f$ 在 $x=0$ 处不可微”是错误的。我们已经证明了 $f$ 在 $x=0$ 处是可微的。此外，AD框架的设计是为了处理像 $\\mathrm{ReLU}$ 这样的常见函数的不可微点，它们通过返回一个有效的次梯度（例如，$0$ 或 $1$）而不是 $\\text{NaN}$ 来实现。", "answer": "$$\\boxed{BCD}$$", "id": "3100437"}]}