{"hands_on_practices": [{"introduction": "为了真正理解反向传播，我们首先要回归其数学本质。这个练习将引导你为一个最简单的线性神经元模型手动计算梯度和Hessian矩阵。通过这个过程，你将直观地看到反向传播作为链式法则的应用，是如何帮助我们理解损失函数的局部形状，并判断参数是否达到了一个局部最小值，从而将抽象的算法与微积分和优化的基本原理紧密联系起来 [@problem_id:3099996]。", "problem": "给定一个具有线性激活的单神经元模型，由参数函数 $f(x; \\theta) = W x + b$ 定义，其中 $\\theta = (W, b)$，$W \\in \\mathbb{R}$，$b \\in \\mathbb{R}$。训练集由三个输入-输出对 $(x_i, y_i)$ 组成，其中 $i = 1, 2, 3$，具体为 $(x_1, y_1) = (0, 1)$、$(x_2, y_2) = (1, 3)$ 和 $(x_3, y_3) = (2, 5)$。经验风险是平方误差的二分之一和，定义为\n$$\nJ(\\theta) = \\frac{1}{2} \\sum_{i=1}^{3} \\left(f(x_i; \\theta) - y_i\\right)^{2}。\n$$\n从微积分链式法则的基本定义以及梯度和 Hessian 矩阵（二阶偏导数矩阵）的定义出发，完成以下任务：\n1. 选择参数 $W$ 和 $b$ 以精确拟合这三个数据点，即对于每一个 $i \\in \\{1, 2, 3\\}$ 都有 $f(x_i; \\theta) = y_i$。\n2. 使用反向传播（即应用于模型计算图的链式法则），推导梯度 $\\nabla_{\\theta} J(\\theta)$ 并在第1部分中选择的精确拟合参数处求值。\n3. 推导 $J(\\theta)$ 相对于 $\\theta$ 的 Hessian 矩阵 $H(\\theta)$ 并在精确拟合参数处求值。计算其最小特征值 $\\lambda_{\\min}(H)$。\n4. 根据 $\\lambda_{\\min}(H)$ 的符号，简要说明该精确拟合点是 $J(\\theta)$ 的局部最小值点还是鞍点。\n\n请给出在解处 $\\lambda_{\\min}(H)$ 的精确值作为最终答案。无需四舍五入。", "solution": "该问题已经过验证，科学上是合理的、适定的、客观的，并包含足够的信息以得到唯一解。\n\n任务是分析单线性神经元模型 $f(x; \\theta) = W x + b$（参数为 $\\theta = (W, b)$）的经验风险函数 $J(\\theta)$。风险定义为三个数据点 $(x_1, y_1) = (0, 1)$、$(x_2, y_2) = (1, 3)$ 和 $(x_3, y_3) = (2, 5)$ 上的平方误差的二分之一和。风险函数为：\n$$\nJ(W, b) = \\frac{1}{2} \\sum_{i=1}^{3} \\left( (Wx_i + b) - y_i \\right)^{2}\n$$\n代入给定的数据点：\n$$\nJ(W, b) = \\frac{1}{2} \\left[ (W(0) + b - 1)^{2} + (W(1) + b - 3)^{2} + (W(2) + b - 5)^{2} \\right]\n$$\n$$\nJ(W, b) = \\frac{1}{2} \\left[ (b - 1)^{2} + (W + b - 3)^{2} + (2W + b - 5)^{2} \\right]\n$$\n\n**1. 寻找精确拟合参数 $\\theta^* = (W, b)$**\n\n为实现精确拟合，模型必须对所有 $i \\in \\{1, 2, 3\\}$ 满足 $f(x_i; \\theta) = y_i$。这为 $W$ 和 $b$ 导出一个线性方程组：\n\\begin{enumerate}\n    \\item 对于 $(x_1, y_1) = (0, 1)$：$W(0) + b = 1 \\implies b = 1$。\n    \\item 对于 $(x_2, y_2) = (1, 3)$：$W(1) + b = 3 \\implies W + b = 3$。\n    \\item 对于 $(x_3, y_3) = (2, 5)$：$W(2) + b = 5 \\implies 2W + b = 5$。\n\\end{enumerate}\n将第一个方程中的 $b = 1$ 代入第二个方程，得到 $W + 1 = 3$，这意味着 $W = 2$。\n我们必须验证这些值是否满足第三个方程：$2W + b = 2(2) + 1 = 4 + 1 = 5$，这与 $y_3 = 5$ 一致。\n因此，精确拟合的参数为 $W = 2$ 和 $b = 1$。我们将此点表示为 $\\theta^* = (2, 1)$。\n\n**2. 在 $\\theta^*$ 处推导并计算梯度 $\\nabla_{\\theta} J(\\theta)$**\n\n$J(\\theta)$ 相对于 $\\theta = (W, b)$ 的梯度为 $\\nabla_{\\theta} J = \\begin{pmatrix} \\frac{\\partial J}{\\partial W} \\\\ \\frac{\\partial J}{\\partial b} \\end{pmatrix}$。\n根据反向传播方法的要求使用链式法则，我们将每个点的误差定义为 $e_i(\\theta) = f(x_i; \\theta) - y_i = Wx_i + b - y_i$。损失函数为 $J = \\frac{1}{2} \\sum_{i=1}^3 e_i^2$。\n偏导数为：\n$$\n\\frac{\\partial J}{\\partial W} = \\sum_{i=1}^{3} \\frac{\\partial J}{\\partial e_i} \\frac{\\partial e_i}{\\partial W} = \\sum_{i=1}^{3} e_i \\cdot x_i = \\sum_{i=1}^{3} (Wx_i + b - y_i) x_i\n$$\n$$\n\\frac{\\partial J}{\\partial b} = \\sum_{i=1}^{3} \\frac{\\partial J}{\\partial e_i} \\frac{\\partial e_i}{\\partial b} = \\sum_{i=1}^{3} e_i \\cdot 1 = \\sum_{i=1}^{3} (Wx_i + b - y_i)\n$$\n在精确拟合点 $\\theta^* = (2, 1)$ 处，根据定义，误差项为零：对于所有 $i$，有 $e_i(\\theta^*) = Wx_i + b - y_i = 0$。\n因此，在 $\\theta^*$ 处计算梯度：\n$$\n\\frac{\\partial J}{\\partial W}\\bigg|_{\\theta^*} = \\sum_{i=1}^{3} (0) x_i = 0\n$$\n$$\n\\frac{\\partial J}{\\partial b}\\bigg|_{\\theta^*} = \\sum_{i=1}^{3} (0) = 0\n$$\n在精确拟合点的梯度是零向量：$\\nabla_{\\theta} J(\\theta^*) = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$。这证实了 $\\theta^*$ 是损失函数 $J(\\theta)$ 的一个临界点。\n\n**3. 推导 Hessian 矩阵 $H(\\theta)$ 并计算其最小特征值**\n\nHessian 矩阵 $H(\\theta)$ 包含 $J(\\theta)$ 的二阶偏导数：\n$$\nH(\\theta) = \\begin{pmatrix} \\frac{\\partial^2 J}{\\partial W^2} & \\frac{\\partial^2 J}{\\partial W \\partial b} \\\\ \\frac{\\partial^2 J}{\\partial b \\partial W} & \\frac{\\partial^2 J}{\\partial b^2} \\end{pmatrix}\n$$\n我们通过对一阶偏导数求导来计算这些值：\n$$\n\\frac{\\partial^2 J}{\\partial W^2} = \\frac{\\partial}{\\partial W} \\left[ \\sum_{i=1}^{3} (Wx_i + b - y_i) x_i \\right] = \\sum_{i=1}^{3} x_i^2\n$$\n$$\n\\frac{\\partial^2 J}{\\partial b \\partial W} = \\frac{\\partial}{\\partial b} \\left[ \\sum_{i=1}^{3} (Wx_i + b - y_i) x_i \\right] = \\sum_{i=1}^{3} x_i\n$$\n$$\n\\frac{\\partial^2 J}{\\partial b^2} = \\frac{\\partial}{\\partial b} \\left[ \\sum_{i=1}^{3} (Wx_i + b - y_i) \\right] = \\sum_{i=1}^{3} 1 = 3\n$$\n注意，正如预期的那样，$\\frac{\\partial^2 J}{\\partial W \\partial b} = \\frac{\\partial^2 J}{\\partial b \\partial W}$。Hessian 矩阵是常数，不依赖于 $W$ 或 $b$。我们使用给定的输入 $x_1=0$、$x_2=1$、$x_3=2$ 来计算这些和：\n$$\n\\sum_{i=1}^{3} x_i^2 = 0^2 + 1^2 + 2^2 = 0 + 1 + 4 = 5\n$$\n$$\n\\sum_{i=1}^{3} x_i = 0 + 1 + 2 = 3\n$$\nHessian 矩阵为：\n$$\nH = \\begin{pmatrix} 5 & 3 \\\\ 3 & 3 \\end{pmatrix}\n$$\n$H$ 的特征值 $\\lambda$ 是特征方程 $\\det(H - \\lambda I) = 0$ 的根：\n$$\n\\det \\begin{pmatrix} 5-\\lambda & 3 \\\\ 3 & 3-\\lambda \\end{pmatrix} = (5-\\lambda)(3-\\lambda) - (3)(3) = 0\n$$\n$$\n15 - 8\\lambda + \\lambda^2 - 9 = 0\n$$\n$$\n\\lambda^2 - 8\\lambda + 6 = 0\n$$\n使用二次公式 $\\lambda = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}$：\n$$\n\\lambda = \\frac{8 \\pm \\sqrt{(-8)^2 - 4(1)(6)}}{2} = \\frac{8 \\pm \\sqrt{64 - 24}}{2} = \\frac{8 \\pm \\sqrt{40}}{2}\n$$\n化简 $\\sqrt{40} = \\sqrt{4 \\cdot 10} = 2\\sqrt{10}$：\n$$\n\\lambda = \\frac{8 \\pm 2\\sqrt{10}}{2} = 4 \\pm \\sqrt{10}\n$$\n两个特征值为 $\\lambda_1 = 4 + \\sqrt{10}$ 和 $\\lambda_2 = 4 - \\sqrt{10}$。最小特征值为 $\\lambda_{\\min}(H) = 4 - \\sqrt{10}$。\n\n**4. 对临界点 $\\theta^*$ 进行分类**\n\n为了对临界点 $\\theta^*$ 进行分类，我们检查在该点处计算的 Hessian 矩阵的特征值的符号。由于 $H$ 是常数，我们使用刚刚计算出的特征值。\n我们知道 $3 = \\sqrt{9} < \\sqrt{10} < \\sqrt{16} = 4$。\n因此，最小特征值 $\\lambda_{\\min}(H) = 4 - \\sqrt{10}$ 是正的，因为 $4 > \\sqrt{10}$。\n最大特征值 $\\lambda_{\\max}(H) = 4 + \\sqrt{10}$ 显然也是正的。\n由于 Hessian 矩阵的两个特征值都是正的，所以该 Hessian 矩阵是正定的。根据二阶偏导数检验，Hessian 矩阵为正定的临界点是局部最小值点。对于这个二次损失函数，它也是唯一的全局最小值点。该精确拟合点是一个局部最小值点。\n最终答案是最小特征值的值。", "answer": "$$\n\\boxed{4 - \\sqrt{10}}\n$$", "id": "3099996"}, {"introduction": "在理论之上，动手实现是检验理解的唯一标准。然而，我们如何确保自己编写的反向传播代码是正确的呢？这个练习将介绍一种在实践中至关重要的调试技术：梯度检验（gradient checking）。你将通过一个数值方法（有限差分）来近似梯度，并将其与反向传播计算出的解析梯度进行比较，从而验证你实现的正确性，这是每一位深度学习实践者都必须掌握的核心技能 [@problem_id:3100954]。", "problem": "通过将一个两层神经网络的解析梯度与有限差分近似进行比较，构建一个对反向传播算法的验证。目标是数值上确认，在使用前向有限差分时，两个梯度之间的差异表现为$\\epsilon$阶截断误差，即误差为$\\mathcal{O}(\\epsilon)$。\n\n使用以下纯数学设置。\n\n- 网络架构和数据：\n  - 输入维度为 $d = 3$，隐藏层有 $h = 3$ 个单元并使用双曲正切激活函数，输出层维度为 $o = 1$ 并使用线性输出。\n  - 给定一个大小为 $n = 4$ 的小批量（mini-batch），输入矩阵 $X \\in \\mathbb{R}^{4 \\times 3}$ 和目标向量 $y \\in \\mathbb{R}^{4 \\times 1}$ 如下：\n    $$\n    X =\n    \\begin{bmatrix}\n    0.2 & -0.1 & 0.4 \\\\\n    -0.5 & 0.3 & 0.1 \\\\\n    0.0 & -0.2 & 0.2 \\\\\n    0.1 & 0.4 & -0.3\n    \\end{bmatrix}, \\quad\n    y =\n    \\begin{bmatrix}\n    0.5 \\\\ -0.1 \\\\ 0.2 \\\\ 0.0\n    \\end{bmatrix}.\n    $$\n  - 测试用例 A 的参数固定如下：\n    $$\n    W_1 =\n    \\begin{bmatrix}\n    0.3 & -0.1 & 0.2 \\\\\n    -0.4 & 0.5 & 0.1 \\\\\n    0.2 & 0.3 & -0.2\n    \\end{bmatrix}, \\quad\n    b_1 =\n    \\begin{bmatrix}\n    0.05 \\\\ -0.02 \\\\ 0.01\n    \\end{bmatrix}, \\quad\n    W_2 =\n    \\begin{bmatrix}\n    0.6 & -0.7 & 0.2\n    \\end{bmatrix}, \\quad\n    b_2 =\n    \\begin{bmatrix}\n    0.03\n    \\end{bmatrix}.\n    $$\n    对于测试用例 B，使用相同的形状，但将 $W_1, b_1, W_2, b_2$ 的每个条目都乘以因子 $0.1$。\n\n- 前向模型和损失：\n  - 对于 $X$ 的每一行 $x_i^\\top$，定义隐藏层预激活 $z_{1,i}^\\top = x_i^\\top W_1^\\top + b_1^\\top$，隐藏层激活 $h_i^\\top = \\tanh(z_{1,i}^\\top)$，输出层预激活 $z_{2,i} = h_i^\\top W_2^\\top + b_2$，以及预测值 $\\hat{y}_i = z_{2,i}$。\n  - 定义均方误差损失\n    $$\n    L(W_1,b_1,W_2,b_2) = \\frac{1}{2n} \\sum_{i=1}^{n} \\left( \\hat{y}_i - y_i \\right)^2.\n    $$\n\n- 通过反向传播计算解析梯度：\n  - 使用多变量微积分、链式法则以及导数恒等式 $\\frac{d}{dz}\\tanh(z) = 1 - \\tanh^2(z)$，推导 $L$ 关于所有参数的梯度并加以实现。\n  - 使用以下顺序和内存布局，将参数集展平为单个向量 $\\theta \\in \\mathbb{R}^{p}$，其中 $p = 16$：\n    1. 按行主序展平 $W_1 \\in \\mathbb{R}^{3 \\times 3}$。\n    2. 追加 $b_1 \\in \\mathbb{R}^{3}$。\n    3. 按行主序展平 $W_2 \\in \\mathbb{R}^{1 \\times 3}$。\n    4. 追加 $b_2 \\in \\mathbb{R}^{1}$。\n\n- 有限差分近似：\n  - 对于给定的 $\\epsilon > 0$ 和 $\\mathbb{R}^p$ 中的标准基向量 $e_k$，使用前向差分来近似 $\\nabla_{\\theta} L$ 的第 $k$ 个分量\n    $$\n    \\frac{L(\\theta + \\epsilon e_k) - L(\\theta)}{\\epsilon}.\n    $$\n  - 使用步长列表\n    $$\n    \\mathcal{E} = \\left[ 10^{-1}, \\; 3 \\cdot 10^{-2}, \\; 10^{-2}, \\; 3 \\cdot 10^{-3}, \\; 10^{-3}, \\; 3 \\cdot 10^{-4}, \\; 10^{-4} \\right].\n    $$\n\n- 误差度量和阶数验证：\n  - 对于每个 $\\epsilon \\in \\mathcal{E}$，计算解析梯度和有限差分梯度之间差值的欧几里得范数，\n    $$\n    \\mathrm{err}(\\epsilon) = \\left\\| \\nabla_{\\theta} L - g_{\\mathrm{FD}}(\\epsilon) \\right\\|_2.\n    $$\n  - 对于连续的 $\\epsilon_i > \\epsilon_{i+1}$，计算经验阶数\n    $$\n    s_i = \\frac{\\log\\left( \\mathrm{err}(\\epsilon_i) / \\mathrm{err}(\\epsilon_{i+1}) \\right)}{\\log\\left( \\epsilon_i / \\epsilon_{i+1} \\right)}.\n    $$\n  - 为每个测试用例定义两个布尔检查：\n    1. 令 $s_{\\mathrm{med}}$ 为 $\\{ s_i \\}$ 的中位数。如果 $0.8 \\le s_{\\mathrm{med}} \\le 1.2$，则定义 $\\mathrm{pass\\_order}$ 为真。\n    2. 如果 $\\mathrm{err}(\\epsilon)$ 在 $\\mathcal{E}$ 的前 5 个值上严格递减，即对于 $\\epsilon \\in \\{ 10^{-1}, 3\\cdot 10^{-2}, 10^{-2}, 3\\cdot 10^{-3}, 10^{-3} \\}$，则定义 $\\mathrm{pass\\_mono}$ 为真。\n\n- 测试套件：\n  - 两个测试用例由参数集指定：\n    - 测试用例 A：参数与上文给出的完全相同。\n    - 测试用例 B：与测试用例 A 相比，参数形状相同，但每个条目都乘以 $0.1$。\n  - 在两种情况下，都使用相同的 $X$、$y$ 和 $\\mathcal{E}$。\n\n- 要求的程序行为和最终输出格式：\n  - 您的程序必须实现前向模型，从第一性原理出发推导并计算通过反向传播得到的解析梯度，为每个 $\\epsilon \\in \\mathcal{E}$ 计算有限差分梯度，并评估误差范数和经验阶数。\n  - 对于每个测试用例，生成一个包含四个条目的列表：$[ \\mathrm{err}(\\min \\mathcal{E}), \\; s_{\\mathrm{med}}, \\; \\mathrm{pass\\_order}, \\; \\mathrm{pass\\_mono} ]$。\n  - 最终的程序输出必须是单行，其中包含一个列表，该列表内含两个按用例生成的列表，格式严格遵循用方括号括起来的逗号分隔列表，例如：\n    $$\n    \\left[ [a_1, a_2, a_3, a_4], [b_1, b_2, b_3, b_4] \\right]\n    $$\n    其中每个 $a_j$ 和 $b_j$ 是布尔值或浮点数。不得打印任何其他文本。\n  - 此问题不涉及任何物理单位。\n\n您的实现必须是自包含的，且不得读取任何输入。它必须完全按照上面提供的方式使用指定的数值。", "solution": "目标是数值上验证两层神经网络反向传播算法的正确性。这是通过将解析计算的梯度与通过有限差分法获得的数值近似进行比较来实现的。主要验证标准是确认解析梯度和数值梯度之间的误差随着有限差分步长 $\\epsilon$ 线性减小，这是前向差分格式一阶截断误差 $\\mathcal{O}(\\epsilon)$ 的特征。\n\n### 数学模型和损失函数\n\n该神经网络架构由一个输入层、一个隐藏层和一个输出层组成。\n- 输入 $X \\in \\mathbb{R}^{n \\times d}$ ($n=4, d=3$)\n- 隐藏层有 $h=3$ 个单元和 $\\tanh$ 激活函数。\n- 输出层有 $o=1$ 个单元和线性激活函数。\n- 参数：$W_1 \\in \\mathbb{R}^{h \\times d}$，$b_1 \\in \\mathbb{R}^{h \\times 1}$，$W_2 \\in \\mathbb{R}^{o \\times h}$，$b_2 \\in \\mathbb{R}^{o \\times 1}$。\n\n一个小批量（mini-batch）$X$ 的前向传播由以下矩阵运算定义：\n1.  **隐藏层预激活**：隐藏层的线性变换由 $Z_1 = X W_1^\\top + \\mathbf{1}b_1^\\top$ 给出，其中 $\\mathbf{1} \\in \\mathbb{R}^{n \\times 1}$ 是一个全1向量，其与 $b_1^\\top$ 的乘积通过广播（broadcasting）处理。结果矩阵为 $Z_1 \\in \\mathbb{R}^{n \\times h}$。\n2.  **隐藏层激活**：双曲正切激活函数被逐元素应用：$H = \\tanh(Z_1)$，其中 $H \\in \\mathbb{R}^{n \\times h}$。\n3.  **输出层预激活**：第二个线性变换产生输出预激活：$Z_2 = H W_2^\\top + \\mathbf{1}b_2^\\top$。结果矩阵为 $Z_2 \\in \\mathbb{R}^{n \\times o}$。\n4.  **预测**：网络输出是线性的，所以预测值 $\\hat{Y}$ 等于预激活：$\\hat{Y} = Z_2 \\in \\mathbb{R}^{n \\times o}$。\n\n网络的性能由均方误差（MSE）损失函数量化，该函数在小批量数据上取平均：\n$$\nL = \\frac{1}{2n} \\sum_{i=1}^{n} (\\hat{y}_i - y_i)^2 = \\frac{1}{2n} \\|\\hat{Y} - Y\\|_F^2\n$$\n其中 $Y \\in \\mathbb{R}^{n \\times o}$ 是真实目标值矩阵。\n\n### 通过反向传播计算解析梯度\n\n本任务的核心是使用多变量链式法则推导损失 $L$ 相对于每个参数（$W_1, b_1, W_2, b_2$）的梯度。这个过程被称为反向传播。我们将损失关于矩阵 $M$ 的梯度记为 $\\delta_M = \\frac{\\partial L}{\\partial M}$。\n\n1.  **输出端的梯度**：损失函数关于网络预测值 $\\hat{Y}$ 的梯度为：\n    $$\n    \\delta_{\\hat{Y}} = \\frac{\\partial L}{\\partial \\hat{Y}} = \\frac{1}{n} (\\hat{Y} - Y)\n    $$\n    由于 $\\hat{Y} =Z_2$，我们有 $\\delta_{Z_2} = \\delta_{\\hat{Y}}$。\n\n2.  **输出层（$W_2, b_2$）的梯度**：\n    对 $Z_2 = H W_2^\\top + \\mathbf{1}b_2^\\top$ 使用链式法则：\n    $$\n    \\delta_{W_2} = \\frac{\\partial L}{\\partial W_2} = \\delta_{Z_2}^\\top H\n    $$\n    $$\n    \\delta_{b_2} = \\frac{\\partial L}{\\partial b_2} = (\\mathrm{sum}(\\delta_{Z_2}, \\text{axis}=0))^\\top = \\delta_{Z_2}^\\top \\mathbf{1}\n    $$\n\n3.  **将梯度传播到隐藏层**：\n    梯度被反向传播到隐藏层的激活值 $H$：\n    $$\n    \\delta_H = \\frac{\\partial L}{\\partial H} = \\delta_{Z_2} W_2\n    $$\n    接下来，使用 $\\frac{d}{dz}\\tanh(z) = 1 - \\tanh^2(z)$，将梯度传播通过 $\\tanh$ 激活函数：\n    $$\n    \\delta_{Z_1} = \\frac{\\partial L}{\\partial Z_1} = \\delta_H \\odot (1 - H^2)\n    $$\n    其中 $\\odot$ 表示逐元素（哈达玛）积。\n\n4.  **隐藏层（$W_1, b_1$）的梯度**：\n    最后，对 $Z_1 = X W_1^\\top + \\mathbf{1}b_1^\\top$ 使用链式法则：\n    $$\n    \\delta_{W_1} = \\frac{\\partial L}{\\partial W_1} = \\delta_{Z_1}^\\top X\n    $$\n    $$\n    \\delta_{b_1} = \\frac{\\partial L}{\\partial b_1} = (\\mathrm{sum}(\\delta_{Z_1}, \\text{axis}=0))^\\top = \\delta_{Z_1}^\\top \\mathbf{1}\n    $$\n\n这些矩阵形式的方程为计算解析梯度提供了一个完整的算法。\n\n### 数值验证\n\n为了验证解析梯度，我们将其与数值近似进行比较。\n\n- **参数向量化**：所有网络参数（$W_1, b_1, W_2, b_2$）都被展平并连接成一个单一向量 $\\theta \\in \\mathbb{R}^{p}$，其中 $p=16$。指定的顺序是 $W_1$（行主序）、$b_1$、$W_2$（行主序）和 $b_2$。\n\n- **有限差分近似**：使用一阶前向差分公式来近似梯度。梯度向量的第 $k$ 个分量估计如下：\n  $$\n  g_{\\mathrm{FD},k}(\\epsilon) = \\frac{L(\\theta + \\epsilon e_k) - L(\\theta)}{\\epsilon}\n  $$\n  其中 $e_k$ 是第 $k$ 个标准基向量，$\\epsilon$ 是一个很小的步长。\n\n- **误差分析和阶数验证**：\n  解析梯度 $\\nabla_\\theta L$ 和有限差分近似 $g_{\\mathrm{FD}}(\\epsilon)$ 之间的差异通过它们差值的欧几里得范数来度量：\n  $$\n  \\mathrm{err}(\\epsilon) = \\| \\nabla_\\theta L - g_{\\mathrm{FD}}(\\epsilon) \\|_2\n  $$\n  对于一阶方法，该误差预计与 $\\epsilon$ 成正比，即 $\\mathrm{err}(\\epsilon) \\approx C\\epsilon$。这意味着对于两个步长 $\\epsilon_i$ 和 $\\epsilon_{i+1}$，其误差之比应约等于步长本身之比。为了量化这种关系，我们计算经验收敛阶数 $s_i$：\n  $$\n  s_i = \\frac{\\log(\\mathrm{err}(\\epsilon_i) / \\mathrm{err}(\\epsilon_{i+1}))}{\\log(\\epsilon_i / \\epsilon_{i+1})}\n  $$\n  $s_i \\approx 1$ 的值确认了预期的一阶收敛性，从而验证了解析梯度实现的正确性。为了稳健性，我们使用计算出的 $s_i$ 值的中位数。如果这个中位阶数 $s_{\\mathrm{med}}$ 在 $[0.8, 1.2]$ 范围内，并且对于初始较大的 $\\epsilon$ 值，误差是单调递减的，则认为验证成功。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Constructs and solves the backpropagation verification problem.\n    \"\"\"\n    \n    # --- Givens from the problem statement ---\n    X = np.array([\n        [0.2, -0.1, 0.4],\n        [-0.5, 0.3, 0.1],\n        [0.0, -0.2, 0.2],\n        [0.1, 0.4, -0.3]\n    ])\n    y = np.array([\n        [0.5],\n        [-0.1],\n        [0.2],\n        [0.0]\n    ])\n    n = X.shape[0]  # Mini-batch size, n=4\n\n    # Parameters for Test Case A\n    W1_a = np.array([\n        [0.3, -0.1, 0.2],\n        [-0.4, 0.5, 0.1],\n        [0.2, 0.3, -0.2]\n    ])\n    b1_a = np.array([[0.05], [-0.02], [0.01]])\n    W2_a = np.array([[0.6, -0.7, 0.2]])\n    b2_a = np.array([[0.03]])\n    \n    test_cases_params = [\n        (W1_a, b1_a, W2_a, b2_a),\n        (W1_a * 0.1, b1_a * 0.1, W2_a * 0.1, b2_a * 0.1) # Test Case B\n    ]\n\n    epsilons = [1e-1, 3e-2, 1e-2, 3e-3, 1e-3, 3e-4, 1e-4]\n    \n    # --- Helper functions for parameter manipulation ---\n    def params_to_vec(W1, b1, W2, b2):\n        return np.concatenate([\n            W1.flatten(),\n            b1.flatten(),\n            W2.flatten(),\n            b2.flatten()\n        ])\n\n    def vec_to_params(theta):\n        W1 = theta[0:9].reshape(3, 3)\n        b1 = theta[9:12].reshape(3, 1)\n        W2 = theta[12:15].reshape(1, 3)\n        b2 = theta[15:16].reshape(1, 1)\n        return W1, b1, W2, b2\n        \n    # --- Core functions for forward/backward pass ---\n    def forward_pass(W1, b1, W2, b2):\n        Z1 = X @ W1.T + b1.T\n        H = np.tanh(Z1)\n        Z2 = H @ W2.T + b2\n        Y_hat = Z2\n        loss = (1 / (2 * n)) * np.sum((Y_hat - y)**2)\n        return loss, Z1, H, Y_hat\n\n    def analytical_gradient(W1, b1, W2, b2):\n        _loss, _Z1, H, Y_hat = forward_pass(W1, b1, W2, b2)\n        \n        # Backward pass\n        dZ2 = (1 / n) * (Y_hat - y)\n        dW2 = dZ2.T @ H\n        db2 = np.sum(dZ2, axis=0, keepdims=True)\n        \n        dH = dZ2 @ W2\n        dZ1 = dH * (1 - H**2)\n        dW1 = dZ1.T @ X\n        db1 = np.sum(dZ1, axis=0, keepdims=True).T\n        \n        return params_to_vec(dW1, db1, dW2, db2)\n\n    def run_verification(params):\n        W1, b1, W2, b2 = params\n        \n        # 1. Compute analytical gradient\n        grad_analytic = analytical_gradient(W1, b1, W2, b2)\n        \n        # 2. Compute finite-difference gradient for each epsilon\n        theta_base = params_to_vec(W1, b1, W2, b2)\n        loss_base, _, _, _ = forward_pass(W1, b1, W2, b2)\n        \n        errors = []\n        for eps in epsilons:\n            grad_fd = np.zeros_like(theta_base)\n            for k in range(len(theta_base)):\n                theta_p = np.copy(theta_base)\n                theta_p[k] += eps\n                \n                W1_p, b1_p, W2_p, b2_p = vec_to_params(theta_p)\n                loss_p, _, _, _ = forward_pass(W1_p, b1_p, W2_p, b2_p)\n                \n                grad_fd[k] = (loss_p - loss_base) / eps\n            \n            error = np.linalg.norm(grad_analytic - grad_fd)\n            errors.append(error)\n\n        # 3. Compute empirical orders\n        orders = []\n        for i in range(len(epsilons) - 1):\n            s_i = np.log(errors[i] / errors[i+1]) / np.log(epsilons[i] / epsilons[i+1])\n            orders.append(s_i)\n        \n        s_med = np.median(orders)\n        \n        # 4. Perform boolean checks\n        pass_order = 0.8 = s_med = 1.2\n        pass_mono = all(errors[i] > errors[i+1] for i in range(4))\n\n        # 5. Collect results\n        err_min_eps = errors[-1]\n        \n        return [err_min_eps, s_med, bool(pass_order), bool(pass_mono)]\n\n    # --- Main execution loop ---\n    final_results = []\n    for case_params in test_cases_params:\n        result = run_verification(case_params)\n        final_results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, final_results))}]\")\n\nsolve()\n\n```", "id": "3100954"}, {"introduction": "一个在数学上正确的算法在实际计算机上运行时，可能会因为浮点数的精度限制而变得脆弱。这个高级练习将带你直面神经网络实现中的一个核心挑战：数值稳定性。通过实现著名的`log-sum-exp`技巧，你将学会如何重写数学表达式以避免在处理极大或极小输入值时发生上溢或下溢，从而确保你的反向传播代码不仅正确，而且在各种极端情况下依然稳健可靠 [@problem_id:3181541]。", "problem": "考虑函数 $f:\\mathbb{R}^n\\to\\mathbb{R}$，其定义为 $f(\\mathbf{z})=\\log\\left(\\sum_{i=1}^{n} e^{z_i}\\right)$，其中 $\\mathbf{z}=(z_1,z_2,\\dots,z_n)\\in\\mathbb{R}^n$。该函数在深度学习中被广泛用作模型和损失函数的构建模块。您的任务是为 $f(\\mathbf{z})$ 实现一个数值稳定的反向传播过程，计算其梯度 $\\nabla_{\\mathbf{z}} f(\\mathbf{z})$，并探究当 $\\mathbf{z}$ 的分量绝对值较大时的数值稳定性问题。请从微积分的基本原理（链式法则、指数函数的导数以及自然对数的导数）出发，不要使用任何预先推导好的“捷径”公式。仅使用这些原理，推导出 $\\frac{\\partial f}{\\partial z_k}$ 关于 $\\mathbf{z}$ 的数学表达式。然后，设计一个数值稳定的算法来计算 $\\nabla_{\\mathbf{z}} f(\\mathbf{z})$，该算法在 $\\lvert z_i\\rvert$ 较大时能避免上溢和下溢。具体来说，您需要实现 Log-Sum-Exp (LSE) 技巧：计算 $m=\\max_i z_i$，并利用恒等式 $\\log\\left(\\sum_{i=1}^{n} e^{z_i}\\right)=m+\\log\\left(\\sum_{i=1}^{n} e^{z_i-m}\\right)$ 来稳定前向和反向计算。\n\n您的程序必须：\n- 使用上述恒等式（其中 $m=\\max_i z_i$）实现一个数值稳定的前向函数 $F(\\mathbf{z})=\\log\\left(\\sum_{i=1}^{n} e^{z_i}\\right)$。\n- 使用您推导的表达式实现反向函数，该函数通过减去 $m$ 来进行数值稳定的计算，并返回 $\\nabla_{\\mathbf{z}} f(\\mathbf{z})$。\n- 实现一个朴素（不稳定）的反向函数，该函数不减去 $m$ 而是直接使用 $e^{z_i}$。\n- 通过使用数值稳定的前向计算，实现 $\\nabla_{\\mathbf{z}} f(\\mathbf{z})$ 的中心有限差分近似。每次扰动一个坐标，步长为 $\\varepsilon$。使用 $\\varepsilon=10^{-6}$。\n\n对于下面测试套件中的每个测试用例向量 $\\mathbf{z}$，计算：\n$1.$ 数值稳定的解析梯度与中心有限差分近似之间的最大绝对差值 $d_{\\text{stable}}=\\max_k \\left\\lvert \\left(\\nabla_{\\mathbf{z}} f(\\mathbf{z})\\right)_k - \\left(\\nabla_{\\mathbf{z}} f(\\mathbf{z})\\right)^{\\text{FD}}_k \\right\\rvert$。\n$2.$ 一个布尔值 $b_{\\text{naive}}$，如果朴素梯度包含任何非有限值（即 $\\infty$ 或 $\\mathrm{NaN}$），则为 $\\text{True}$，否则为 $\\text{False}$。\n\n测试套件（每个 $\\mathbf{z}$ 都是一个行向量）：\n- 用例 1：$\\mathbf{z}=[1000,1000,1000]$。\n- 用例 2：$\\mathbf{z}=[1000,-1000,0]$。\n- 用例 3：$\\mathbf{z}=[-1000,-1000,-999]$。\n- 用例 4：$\\mathbf{z}=[800,-800,800,-800]$。\n\n覆盖性设计 rationale：\n- 用例 1 测试对称性和大的相等值（梯度分量预期相等）。\n- 用例 2 测试同时包含极大正值和极大负值的极端差异情况。\n- 用例 3 测试当所有分量都为非常大的负数但彼此接近时，可能发生的近似抵消和下溢风险。\n- 用例 4 测试多个大数量级分量重复出现的情况。\n\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，列表项按 $[d_{\\text{stable}}^{(1)},b_{\\text{naive}}^{(1)},d_{\\text{stable}}^{(2)},b_{\\text{naive}}^{(2)},d_{\\text{stable}}^{(3)},b_{\\text{naive}}^{(3)},d_{\\text{stable}}^{(4)},b_{\\text{naive}}^{(4)}]$ 的顺序排列，其中上标表示用例编号。所有浮点数必须以默认的浮点表示法输出，布尔值必须输出为 $\\text{True}$ 或 $\\text{False}$。本问题不涉及任何物理单位、角度单位或百分比。", "solution": "该问题是有效的。它在科学上基于微积分和数值分析的原理，问题阐述清晰，目标明确，并为获得唯一解提供了所有必要的信息和约束。\n\n题目要求我们为函数 $f:\\mathbb{R}^n \\to \\mathbb{R}$（定义为 $f(\\mathbf{z}) = \\log\\left(\\sum_{i=1}^{n} e^{z_i}\\right)$，其中 $\\mathbf{z} = (z_1, z_2, \\dots, z_n)$）推导并实现一个数值稳定的反向传播过程。\n\n### 1. 梯度的数学推导\n\n主要任务是求出 $f(\\mathbf{z})$ 的梯度，即偏导数向量 $\\nabla_{\\mathbf{z}} f(\\mathbf{z}) = \\left(\\frac{\\partial f}{\\partial z_1}, \\frac{\\partial f}{\\partial z_2}, \\dots, \\frac{\\partial f}{\\partial z_n}\\right)$。我们将使用微积分基本原理推导单个分量 $\\frac{\\partial f}{\\partial z_k}$ 的表达式。\n\n我们使用链式法则来分解函数 $f(\\mathbf{z})$。定义一个中间变量 $S(\\mathbf{z})$ 和一个函数 $g(S)$：\n$$ S(\\mathbf{z}) = \\sum_{i=1}^{n} e^{z_i} $$\n$$ g(S) = \\log(S) $$\n因此，$f(\\mathbf{z}) = g(S(\\mathbf{z}))$。\n\n根据多元链式法则，$f$ 对 $z_k$ 的偏导数为：\n$$ \\frac{\\partial f}{\\partial z_k} = \\frac{d g}{d S} \\cdot \\frac{\\partial S}{\\partial z_k} $$\n\n首先，我们计算 $g(S)$ 对 $S$ 的导数：\n$$ \\frac{d g}{d S} = \\frac{d}{dS} \\log(S) = \\frac{1}{S} $$\n\n接下来，我们计算 $S(\\mathbf{z})$ 对 $z_k$ 的偏导数：\n$$ \\frac{\\partial S}{\\partial z_k} = \\frac{\\partial}{\\partial z_k} \\left(\\sum_{i=1}^{n} e^{z_i}\\right) $$\n由于微分算子的线性性质，我们可以将导数移到求和号内部：\n$$ \\frac{\\partial S}{\\partial z_k} = \\sum_{i=1}^{n} \\frac{\\partial}{\\partial z_k} (e^{z_i}) $$\n导数 $\\frac{\\partial}{\\partial z_k} (e^{z_i})$ 的值取决于 $i$ 是否等于 $k$。\n- 如果 $i=k$，那么 $\\frac{\\partial}{\\partial z_k} (e^{z_k}) = e^{z_k}$。\n- 如果 $i \\neq k$，那么 $z_i$ 相对于 $z_k$ 被视为常数，所以 $\\frac{\\partial}{\\partial z_k} (e^{z_i}) = 0$。\n\n因此，求和式中只有 $i=k$ 的项不为零：\n$$ \\frac{\\partial S}{\\partial z_k} = e^{z_k} $$\n\n将这些结果代回链式法则方程，我们得到：\n$$ \\frac{\\partial f}{\\partial z_k} = \\frac{1}{S} \\cdot e^{z_k} = \\frac{1}{\\sum_{i=1}^{n} e^{z_i}} \\cdot e^{z_k} = \\frac{e^{z_k}}{\\sum_{i=1}^{n} e^{z_i}} $$\n这个表达式给出了梯度 $\\nabla_{\\mathbf{z}} f(\\mathbf{z})$ 的第 $k$ 个分量。因此，梯度向量是对输入向量 $\\mathbf{z}$ 应用 softmax 函数的结果。\n\n### 2. 数值稳定性与 Log-Sum-Exp 技巧\n\n推导出的表达式 $\\frac{\\partial f}{\\partial z_k} = \\frac{e^{z_k}}{\\sum_{i=1}^{n} e^{z_i}}$ 在数值上是不稳定的。\n- **上溢 (Overflow)**：如果任何一个 $z_i$ 是一个大的正数（例如 $1000$），$e^{z_i}$ 将会超过标准浮点类型能表示的最大值，导致无穷大（`inf`）。随后的除法 `inf/inf` 将导致 `NaN`（非数值）。\n- **下溢 (Underflow)**：如果所有的 $z_i$ 都是大的负数（例如 $-1000$），每个 $e^{z_i}$ 将会下溢到 $0$。分母中的和将变为 $0$，导致除以零，这同样会产生 `inf` 或 `NaN`。\n\n为了解决这个问题，我们使用“Log-Sum-Exp”（LSE）技巧。令 $m = \\max_{i} z_i$。我们可以用非零常数 $e^{-m}$ 乘以梯度表达式的分子和分母：\n$$ \\frac{\\partial f}{\\partial z_k} = \\frac{e^{z_k} \\cdot e^{-m}}{\\left(\\sum_{i=1}^{n} e^{z_i}\\right) \\cdot e^{-m}} = \\frac{e^{z_k - m}}{\\sum_{i=1}^{n} e^{z_i - m}} $$\n这个稳定化的公式可以避免上溢，因为每个指数的参数 $z_i - m$ 总是小于或等于 $0$。因此，每个项 $e^{z_i - m}$ 的值都在 $(0, 1]$ 范围内。最大值为 $1$，在 $z_i = m$ 时取到。这个稳定的公式也避免了由下溢引起的除零错误，因为分母求和项中至少有一项是 $e^0 = 1$，从而确保了和始终至少为 $1$。\n\n同样地，该技巧也应用于 $f(\\mathbf{z})$ 本身的前向传播计算：\n$$ f(\\mathbf{z}) = \\log\\left(\\sum_{i=1}^{n} e^{z_i}\\right) = \\log\\left(e^m \\sum_{i=1}^{n} e^{z_i-m}\\right) = \\log(e^m) + \\log\\left(\\sum_{i=1}^{n} e^{z_i-m}\\right) = m + \\log\\left(\\sum_{i=1}^{n} e^{z_i-m}\\right) $$\n此形式用于数值稳定的前向计算。\n\n### 3. 算法实现\n\n题目要求实现四个函数并比较它们的结果。\n\n1.  **数值稳定的前向函数 $F(\\mathbf{z})$**：该函数首先计算 $m = \\max_i z_i$，然后返回 $m + \\log\\left(\\sum_{i=1}^{n} e^{z_i-m}\\right)$ 的值。\n\n2.  **数值稳定的反向函数 $\\nabla_{\\mathbf{z}} f(\\mathbf{z})$**：该函数使用稳定化的 softmax 公式计算梯度。它首先找到 $m = \\max_i z_i$。然后计算分子向量 $[e^{z_1-m}, e^{z_2-m}, \\dots, e^{z_n-m}]$ 和分母的和 $\\sum_{i=1}^{n} e^{z_i-m}$。最终的梯度向量通过将每个分子除以该和得到。\n\n3.  **朴素反向函数**：该函数直接为每个分量 $k$ 实现原始公式 $\\frac{e^{z_k}}{\\sum_{i=1}^{n} e^{z_i}}$。对于给定的测试用例，由于上溢或下溢，预计会产生非有限值（`inf` 或 `NaN`）。\n\n4.  **中心有限差分近似 $(\\nabla_{\\mathbf{z}} f(\\mathbf{z}))^{\\text{FD}}$**：这作为一个数值基准，用于验证解析梯度实现的正确性。对于每个分量 $k$，它使用中心差分公式和一个小步长 $\\varepsilon = 10^{-6}$ 来近似偏导数：\n    $$ \\left(\\nabla_{\\mathbf{z}} f(\\mathbf{z})\\right)^{\\text{FD}}_k = \\frac{F(\\mathbf{z} + \\varepsilon \\mathbf{e}_k) - F(\\mathbf{z} - \\varepsilon \\mathbf{e}_k)}{2\\varepsilon} $$\n    其中 $\\mathbf{e}_k$ 是标准基向量，其第 $k$ 个索引处为 $1$，其余位置为 $0$。为了在输入值很大时获得有意义的结果，此计算必须使用数值稳定的前向函数 $F(\\mathbf{z})$。\n\n程序将对提供的测试向量执行这些函数，并计算稳定解析梯度与有限差分近似之间的最大绝对差值 $d_{\\text{stable}}$，以及一个布尔标志 $b_{\\text{naive}}$，用于指示朴素梯度计算是否产生了任何非有限数值。", "answer": "```python\nimport numpy as np\n\ndef stable_forward_lse(z: np.ndarray) - float:\n    \"\"\"\n    Computes the Log-Sum-Exp function in a numerically stable way.\n    f(z) = log(sum(exp(z_i)))\n    \"\"\"\n    m = np.max(z)\n    # The Log-Sum-Exp identity: m + log(sum(exp(z_i - m)))\n    return m + np.log(np.sum(np.exp(z - m)))\n\ndef backward_stable(z: np.ndarray) - np.ndarray:\n    \"\"\"\n    Computes the gradient of the Log-Sum-Exp function in a stable manner.\n    This is equivalent to the softmax function.\n    grad_k = exp(z_k - m) / sum(exp(z_i - m))\n    \"\"\"\n    m = np.max(z)\n    shifted_exp_z = np.exp(z - m)\n    return shifted_exp_z / np.sum(shifted_exp_z)\n\ndef backward_naive(z: np.ndarray) - np.ndarray:\n    \"\"\"\n    Computes the gradient of the Log-Sum-Exp function naively.\n    This is prone to overflow/underflow.\n    grad_k = exp(z_k) / sum(exp(z_i))\n    \"\"\"\n    # This is expected to fail for large inputs\n    exp_z = np.exp(z)\n    sum_exp_z = np.sum(exp_z)\n    return exp_z / sum_exp_z\n\ndef gradient_finite_difference(z: np.ndarray, epsilon: float = 1e-6) - np.ndarray:\n    \"\"\"\n    Computes the gradient of the Log-Sum-Exp function using central finite differences.\n    \"\"\"\n    n = z.shape[0]\n    grad_fd = np.zeros(n, dtype=np.float64)\n    \n    # Use a copy of z to avoid modifying the original\n    z_temp = z.astype(np.float64)\n\n    for i in range(n):\n        # Store original value\n        original_zi = z_temp[i]\n        \n        # Calculate f(z + epsilon * e_i)\n        z_temp[i] = original_zi + epsilon\n        f_plus = stable_forward_lse(z_temp)\n        \n        # Calculate f(z - epsilon * e_i)\n        z_temp[i] = original_zi - epsilon\n        f_minus = stable_forward_lse(z_temp)\n        \n        # Restore original value\n        z_temp[i] = original_zi\n        \n        # Central difference formula\n        grad_fd[i] = (f_plus - f_minus) / (2 * epsilon)\n        \n    return grad_fd\n\ndef solve():\n    \"\"\"\n    Main function to run test cases and produce the final output.\n    \"\"\"\n    test_cases = [\n        np.array([1000.0, 1000.0, 1000.0]),\n        np.array([1000.0, -1000.0, 0.0]),\n        np.array([-1000.0, -1000.0, -999.0]),\n        np.array([800.0, -800.0, 800.0, -800.0])\n    ]\n    \n    epsilon = 1e-6\n    results = []\n\n    for z in test_cases:\n        # 1. Compute stable analytical gradient and finite difference approximation\n        grad_stable = backward_stable(z)\n        grad_fd = gradient_finite_difference(z, epsilon)\n        \n        # Calculate the maximum absolute difference\n        d_stable = np.max(np.abs(grad_stable - grad_fd))\n        \n        # 2. Compute naive gradient and check for non-finite values\n        # Suppress RuntimeWarning for overflow in exp and invalid value in divide\n        with np.errstate(over='ignore', invalid='ignore'):\n             grad_naive = backward_naive(z)\n        \n        b_naive = np.any(~np.isfinite(grad_naive))\n        \n        results.append(d_stable)\n        results.append(b_naive)\n        \n    output_parts = []\n    for item in results:\n        if isinstance(item, (bool, np.bool_)):\n            output_parts.append(str(item))\n        else:\n            output_parts.append(f\"{item}\")\n            \n    print(f\"[{','.join(output_parts)}]\")\n\nsolve()\n```", "id": "3181541"}]}