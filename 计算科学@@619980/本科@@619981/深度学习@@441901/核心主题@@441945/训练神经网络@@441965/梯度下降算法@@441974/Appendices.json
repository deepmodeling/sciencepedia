{"hands_on_practices": [{"introduction": "理论联系实际是掌握梯度下降法的关键。这个练习将指导你从零开始实现一个基础的梯度下降算法，并将其应用于一个经典的计算化学问题中：寻找莫尔斯势（Morse potential）描述的双原子分子的最低能量构型。通过为不同的初始条件（高度压缩、平衡、拉伸）编写代码来找到能量最低点，你将亲身体验梯度下降法是如何迭代地“走下坡路”以定位最小值的，并理解诸如步长选择（line search）等实际工程考量的重要性[@problem_id:2463075]。", "problem": "给定一维空间中的双原子莫尔斯势，其由键长变量 $r$ 定义为\n$$\nV(r; D_e, a, r_e) = D_e \\left(1 - e^{-a \\, (r - r_e)}\\right)^2,\n$$\n其中 $D_e \\gt 0$ 是解离能参数，$a \\gt 0$ 控制曲率，$r_e \\gt 0$ 是平衡键长。在本问题中，所有量均为无量纲。考虑一个在 $r_0$ 处初始化并按如下方式更新的迭代序列 $\\{r_k\\}_{k=0}^{\\infty}$\n$$\nr_{k+1} = r_k - \\alpha_k \\, \\frac{dV}{dr}(r_k),\n$$\n其中步长 $\\alpha_k \\gt 0$ 在每次迭代中选择，以确保能量序列 $\\{V(r_k)\\}$ 是单调不增的，并在梯度大小满足 $\\left|\\frac{dV}{dr}(r_k)\\right| \\le \\varepsilon$ 或达到最大迭代次数时终止。设充分下降条件为\n$$\nV(r_k - \\alpha_k \\, \\tfrac{dV}{dr}(r_k)) \\le V(r_k) - c \\, \\alpha_k \\left(\\tfrac{dV}{dr}(r_k)\\right)^2,\n$$\n其中收缩因子 $\\rho \\in (0,1)$ 用于缩减 $\\alpha_k$ 直至条件成立。将 $c$ 和 $\\rho$ 视为每个测试用例的给定参数。您的任务是为每个测试用例计算最终位置 $r_\\star$、最终能量 $V(r_\\star)$、执行的迭代次数、所记录的所有迭代能量序列是否单调不增，以及在集合 $\\{0,1,2,4,8,16\\}$ 中属于已执行迭代范围内的迭代索引处的能量采样列表（仅包括存在的那些索引）。\n\n使用以下参数集测试套件。对于每种情况，所有符号和数字的定义如上，且所有量均为无量纲：\n- 情况1（高度压缩起始）：$D_e = 1.0$，$a = 1.5$，$r_e = 1.4$，$r_0 = 0.2$，初始试探步长 $= 1.0$，$c = 1.0 \\times 10^{-4}$，$\\rho = 0.5$，$\\varepsilon = 1.0 \\times 10^{-8}$，最大迭代次数 $= 2000$。\n- 情况2（平衡点起始）：$D_e = 1.0$，$a = 1.5$，$r_e = 1.4$，$r_0 = 1.4$，初始试探步长 $= 1.0$，$c = 1.0 \\times 10^{-4}$，$\\rho = 0.5$，$\\varepsilon = 1.0 \\times 10^{-12}$，最大迭代次数 $= 100$。\n- 情况3（拉伸起始）：$D_e = 1.0$，$a = 1.5$，$r_e = 1.4$，$r_0 = 4.0$，初始试探步长 $= 1.0$，$c = 1.0 \\times 10^{-4}$，$\\rho = 0.5$，$\\varepsilon = 1.0 \\times 10^{-10}$，最大迭代次数 $= 3000$。\n- 情况4（更陡峭的势，高度压缩起始）：$D_e = 2.0$，$a = 3.0$，$r_e = 1.2$，$r_0 = 0.1$，初始试探步长 $= 1.0$，$c = 1.0 \\times 10^{-4}$，$\\rho = 0.5$，$\\varepsilon = 1.0 \\times 10^{-8}$，最大迭代次数 $= 5000$。\n\n对于每个测试用例，您的程序必须计算：\n- $r_\\star$ 作为最终迭代位置，\n- $V(r_\\star)$，\n- 执行的总迭代次数，\n- 一个布尔值，指示完整的记录能量序列是否单调不增，\n- 在该情况下可用的、在迭代索引 $\\{0,1,2,4,8,16\\}$ 处的能量采样列表。\n\n最终输出格式：\n您的程序应生成单行输出，其中包含所有四个测试用例结果的JSON数组。输出必须是一个包含四个元素的单一JSON数组，每个测试用例对应一个元素，其中每个元素本身是以下形式的JSON数组\n$$\n[r_\\star, \\, V(r_\\star), \\, \\text{number\\_of\\_iterations}, \\, \\text{monotone\\_flag}, \\, \\text{sampled\\_energies}],\n$$\n其中 $\\text{monotone\\_flag}$ 是一个JSON布尔值，$\\text{sampled\\_energies}$ 是一个JSON数组，包含在可用索引 $\\{0,1,2,4,8,16\\}$ 处的浮点数值。例如，一个有效的整体输出形状是\n$$\n\\bigl[ [\\cdots], [\\cdots], [\\cdots], [\\cdots] \\bigr].\n$$\n所有计算均使用上述定义的无量纲莫尔斯势和指定参数。不应生成任何绘图，也无需用户输入。单行JSON数组是唯一的输出。", "solution": "问题陈述已经过严格评估，被认为是有效的。它具有科学依据，问题定义良好、客观，并包含进行唯一且可验证的求解所需的所有必要信息。任务是实现带有回溯线搜索的最速下降优化算法，为几组参数找到一维莫尔斯势的最小值。\n\n莫尔斯势描述了双原子分子的势能随其核间距 $r$ 变化的函数，由下式给出：\n$$\nV(r; D_e, a, r_e) = D_e \\left(1 - e^{-a \\, (r - r_e)}\\right)^2\n$$\n在此，$D_e > 0$ 是势阱深度（解离能），$a > 0$ 是控制势阱宽度的参数，$r_e > 0$ 是对应于势能最小值的平衡键距。\n\n任务的核心是执行几何优化，在这个一维案例中，这等同于找到使 $V(r)$ 最小化的 $r$ 值。最速下降法是一种迭代优化算法，它通过沿函数梯度的反方向迈出一步来更新当前位置 $r_k$。在此一维背景下，梯度即一阶导数 $\\frac{dV}{dr}$。更新规则为：\n$$\nr_{k+1} = r_k - \\alpha_k \\frac{dV}{dr}(r_k)\n$$\n其中 $\\alpha_k > 0$ 是第 $k$ 次迭代的步长。\n\n首先，我们必须计算莫尔斯势的解析导数。使用链式法则，我们得到：\n$$\n\\frac{dV}{dr}(r) = \\frac{d}{dr} \\left[ D_e \\left(1 - e^{-a \\, (r - r_e)}\\right)^2 \\right]\n$$\n$$\n\\frac{dV}{dr}(r) = 2 D_e \\left(1 - e^{-a \\, (r - r_e)}\\right) \\cdot \\frac{d}{dr} \\left(1 - e^{-a \\, (r - r_e)}\\right)\n$$\n$$\n\\frac{dV}{dr}(r) = 2 D_e \\left(1 - e^{-a \\, (r - r_e)}\\right) \\cdot \\left( -e^{-a \\, (r - r_e)} \\cdot (-a) \\right)\n$$\n$$\n\\frac{dV}{dr}(r) = 2 a D_e \\left(1 - e^{-a \\, (r - r_e)}\\right) e^{-a \\, (r - r_e)}\n$$\n这个梯度，我们称之为 $g_k = \\frac{dV}{dr}(r_k)$，决定了最速下降方向 $-g_k$。\n\n算法的一个关键组成部分是步长 $\\alpha_k$ 的确定。不当的大步长可能导致算法越过最小值而发散，而过小的步长则可能导致收敛速度过慢。问题指定了一种由 Armijo 充分下降条件控制的回溯线搜索策略。该条件确保所采取的步长能有效降低势能。条件如下：\n$$\nV(r_{k+1}) \\le V(r_k) + c \\, \\alpha_k \\, \\nabla V(r_k)^T p_k\n$$\n在我们的的一维情况下，搜索方向为 $p_k = -g_k = -\\frac{dV}{dr}(r_k)$，这变为：\n$$\nV(r_k - \\alpha_k g_k) \\le V(r_k) - c \\, \\alpha_k g_k^2\n$$\n其中 $c$ 是一个小的正常数，对于所有测试用例均给定为 $c = 1.0 \\times 10^{-4}$。回溯过程从一个初始试探步长 $\\alpha_{trial}$（给定为 $1.0$）开始，并用一个收缩因子 $\\rho \\in (0,1)$（给定为 $0.5$）反复减小它，直到满足 Armijo 条件。\n\n每个测试用例的总体算法流程如下：\n1. 初始化迭代计数器 $k=0$ 和位置 $r_0$ 为给定的起始值。\n2. 开始一个循环，最多持续设定的最大迭代次数。\n3. 在每次迭代 $k$ 时，计算梯度 $g_k = \\frac{dV}{dr}(r_k)$。\n4. 检查收敛性：如果梯度大小 $|g_k|$ 小于或等于指定的容差 $\\varepsilon$，则算法已收敛。循环终止。\n5. 如果未收敛，执行回溯线搜索以找到合适的步长 $\\alpha_k$：\n    a. 初始化 $\\alpha = \\alpha_{trial}$。\n    b. 当 $V(r_k - \\alpha g_k) > V(r_k) - c \\alpha g_k^2$ 时，更新 $\\alpha \\leftarrow \\rho \\alpha$。\n    c. 设置 $\\alpha_k = \\alpha$。\n6. 更新位置：$r_{k+1} = r_k - \\alpha_k g_k$。\n7. 在每一步存储能量 $V(r_k)$，以监控优化进程并验证单调性。\n8. 增加迭代计数器，$k \\leftarrow k+1$。\n9. 如果循环因达到最大迭代次数而完成，则过程以当前状态终止。\n\n终止时，将最终位置 $r_\\star$、最终能量 $V(r_\\star)$、总迭代次数、能量序列单调性的布尔标志以及在指定迭代 $(\\{0, 1, 2, 4, 8, 16\\})$ 处的能量采样列表进行汇编。由于强制执行了充分下降条件，单调性检查 $V(r_{i+1}) \\le V(r_i)$（对所有 $i$）预期会通过。\n\n此系统性程序将应用于四个指定的测试用例中的每一个。情况2作为一个有用的诊断案例，因为它从精确的最小值 $r_0 = r_e = 1.4$ 开始，此处的梯度为零。算法应正确地在迭代 $k=0$ 时以零梯度终止。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nimport json\n\ndef solve():\n    \"\"\"\n    Solves the Morse potential optimization problem for a suite of test cases\n    using the steepest descent algorithm with a backtracking line search.\n    \"\"\"\n\n    def morse_potential(r, De, a, re):\n        \"\"\"Calculates the Morse potential V(r).\"\"\"\n        return De * (1 - np.exp(-a * (r - re)))**2\n\n    def morse_gradient(r, De, a, re):\n        \"\"\"Calculates the gradient (derivative) of the Morse potential dV/dr.\"\"\"\n        exp_term = np.exp(-a * (r - re))\n        return 2 * a * De * (1 - exp_term) * exp_term\n\n    def run_steepest_descent(params):\n        \"\"\"\n        Performs the steepest descent optimization for a single test case.\n        \"\"\"\n        De = params['De']\n        a = params['a']\n        re = params['re']\n        r_current = params['r0']\n        initial_alpha = params['initial_alpha']\n        c = params['c']\n        rho = params['rho']\n        epsilon = params['epsilon']\n        max_iter = params['max_iter']\n\n        energies = []\n        sample_indices = {0, 1, 2, 4, 8, 16}\n        sampled_energies = []\n        \n        k = 0\n        while k  max_iter:\n            # Calculate energy and gradient at current position\n            V_current = morse_potential(r_current, De, a, re)\n            energies.append(V_current)\n\n            # Sample energy if current iteration index is in the sample set\n            if k in sample_indices:\n                sampled_energies.append(V_current)\n            \n            grad = morse_gradient(r_current, De, a, re)\n\n            # Check for convergence\n            if np.abs(grad) = epsilon:\n                break\n\n            # Backtracking line search to find step size alpha\n            alpha = initial_alpha\n            while True:\n                r_next = r_current - alpha * grad\n                V_next = morse_potential(r_next, De, a, re)\n                \n                # Armijo condition for sufficient decrease\n                if V_next = V_current - c * alpha * grad**2:\n                    break\n                alpha *= rho\n\n            # Update position\n            r_current = r_next\n            k += 1\n\n        # Final state after loop termination\n        r_star = r_current\n        V_star = morse_potential(r_star, De, a, re)\n        \n        # In case max_iter is reached, the last energy hasn't been added\n        if k == max_iter:\n            energies.append(V_star)\n            if k in sample_indices:\n                sampled_energies.append(V_star)\n\n        num_iterations = k\n        \n        # Check if the sequence of energies is monotonically nonincreasing\n        is_monotone = True\n        if len(energies) > 1:\n            # Using a small tolerance for floating point comparisons\n            for i in range(len(energies) - 1):\n                if energies[i+1] > energies[i] + 1e-12: # V(k+1) should be = V(k)\n                    is_monotone = False\n                    break\n        \n        return [r_star, V_star, num_iterations, is_monotone, sampled_energies]\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {'name': 'Case 1', 'De': 1.0, 'a': 1.5, 're': 1.4, 'r0': 0.2, 'initial_alpha': 1.0, 'c': 1.0e-4, 'rho': 0.5, 'epsilon': 1.0e-8, 'max_iter': 2000},\n        {'name': 'Case 2', 'De': 1.0, 'a': 1.5, 're': 1.4, 'r0': 1.4, 'initial_alpha': 1.0, 'c': 1.0e-4, 'rho': 0.5, 'epsilon': 1.0e-12, 'max_iter': 100},\n        {'name': 'Case 3', 'De': 1.0, 'a': 1.5, 're': 1.4, 'r0': 4.0, 'initial_alpha': 1.0, 'c': 1.0e-4, 'rho': 0.5, 'epsilon': 1.0e-10, 'max_iter': 3000},\n        {'name': 'Case 4', 'De': 2.0, 'a': 3.0, 're': 1.2, 'r0': 0.1, 'initial_alpha': 1.0, 'c': 1.0e-4, 'rho': 0.5, 'epsilon': 1.0e-8, 'max_iter': 5000},\n    ]\n\n    all_results = []\n    for case_params in test_cases:\n        result = run_steepest_descent(case_params)\n        all_results.append(result)\n\n    # Use json.dumps for robust JSON formatting\n    print(json.dumps(all_results))\n\nsolve()\n```", "id": "2463075"}, {"introduction": "梯度下降法的成功在很大程度上取决于学习率 $\\eta$ 的选择。一个过大的学习率可能导致算法“矫枉过正”，在最小值附近来回震荡甚至发散。本练习通过一个精心设计的二维二次势能面，揭示了一种梯度下降法的典型失败模式：极限环（limit cycle）。你将通过解析推导，精确计算出导致优化器在两个点之间无限振荡、永不收敛的特定学习率，从而深刻理解学习率对算法稳定性的决定性影响，尤其是在损失函数具有“狭长山谷”结构的病态条件（ill-conditioned）问题中[@problem_id:2463052]。", "problem": "在计算化学中，通常通过最小化势能面 (PES) 来进行局域几何优化。考虑一个近似于局域二次势阱的二维势能面 (PES)，其以无量纲单位表示为\n$$\nE(x,y) = \\tfrac{1}{2}\\big(x^{2} + 3\\,y^{2}\\big).\n$$\n应用步长参数 $s$ 恒定的最速下降算法 (SD) 来最小化 $E(x,y)$。根据定义，坐标 $\\mathbf{r}_{k} = (x_{k},y_{k})$ 的最速下降更新公式为\n$$\n\\mathbf{r}_{k+1} = \\mathbf{r}_{k} - s\\,\\nabla E(\\mathbf{r}_{k}).\n$$\n假设初始条件为 $\\mathbf{r}_{0} = (0,y_{0})$，其中 $y_{0} \\neq 0$。\n\n确定恒定步长 $s$ 的值（表示为一个精确的、简化的分数），使得最速下降迭代过程陷入一个非收敛的两点极限环中，在 $(0,y_{0})$ 和 $(0,-y_{0})$ 之间永久交替。使用无量纲单位，答案中不要包含单位。无需四舍五入。", "solution": "所述问题定义明确且科学上合理。我们着手进行推导。\n\n势能面 (PES) 由以下函数给出：\n$$\nE(x,y) = \\frac{1}{2}(x^{2} + 3y^{2})\n$$\n最速下降 (SD) 算法根据以下规则更新坐标 $\\mathbf{r}_{k} = (x_{k}, y_{k})$：\n$$\n\\mathbf{r}_{k+1} = \\mathbf{r}_{k} - s \\nabla E(\\mathbf{r}_{k})\n$$\n其中 $s$ 是恒定步长。\n\n首先，我们必须计算能量函数 $\\nabla E(x,y)$ 的梯度。其偏导数为：\n$$\n\\frac{\\partial E}{\\partial x} = \\frac{\\partial}{\\partial x} \\left( \\frac{1}{2}x^{2} + \\frac{3}{2}y^{2} \\right) = x\n$$\n$$\n\\frac{\\partial E}{\\partial y} = \\frac{\\partial}{\\partial y} \\left( \\frac{1}{2}x^{2} + \\frac{3}{2}y^{2} \\right) = 3y\n$$\n因此，梯度向量为：\n$$\n\\nabla E(x,y) = \\begin{pmatrix} x \\\\ 3y \\end{pmatrix}\n$$\n将梯度代入最速下降更新规则，我们得到坐标 $(x_{k}, y_{k})$ 的分量递推关系：\n$$\n\\begin{pmatrix} x_{k+1} \\\\ y_{k+1} \\end{pmatrix} = \\begin{pmatrix} x_{k} \\\\ y_{k} \\end{pmatrix} - s \\begin{pmatrix} x_{k} \\\\ 3y_{k} \\end{pmatrix}\n$$\n这产生了两个独立的方程：\n$$\nx_{k+1} = x_{k} - s x_{k} = (1 - s) x_{k}\n$$\n$$\ny_{k+1} = y_{k} - s (3y_{k}) = (1 - 3s) y_{k}\n$$\n初始条件为 $\\mathbf{r}_{0} = (0, y_{0})$，其中 $y_{0} \\neq 0$。\n\n我们来分析 $x$ 坐标的行为。当 $x_{0} = 0$ 时，$x$ 的递推关系给出：\n$$\nx_{1} = (1-s)x_{0} = (1-s)(0) = 0\n$$\n通过归纳法，可以轻易证明对于所有整数 $k \\ge 0$，$x_{k} = 0$。这与指定的位于 $y$ 轴上的极限环点 $(0, y_{0})$ 和 $(0, -y_{0})$ 一致。\n\n现在，我们分析 $y$ 坐标的行为。问题指出，系统进入一个两点极限环，在 $(0, y_{0})$ 和 $(0, -y_{0})$ 之间交替。这意味着对于偶数 $k$，$\\mathbf{r}_{k} = (0, y_{0})$；对于奇数 $k$，$\\mathbf{r}_{k} = (0, -y_{0})$。\n\n我们考虑从 $k=0$ 到 $k=1$ 的第一步迭代。\n我们从 $\\mathbf{r}_{0} = (0, y_{0})$ 开始。下一个点必须是 $\\mathbf{r}_{1} = (0, -y_{0})$。\n使用 $k=0$ 时 $y$ 的递推关系：\n$$\ny_{1} = (1 - 3s) y_{0}\n$$\n为使迭代与极限环匹配，我们需要 $y_{1} = -y_{0}$。因此：\n$$\n-y_{0} = (1 - 3s) y_{0}\n$$\n由于题目给出 $y_{0} \\neq 0$，我们可以将等式两边同时除以 $y_{0}$：\n$$\n-1 = 1 - 3s\n$$\n求解该方程以得到步长 $s$：\n$$\n3s = 1 - (-1) = 2\n$$\n$$\ns = \\frac{2}{3}\n$$\n我们必须验证 $s$ 的这个值能否无限维持该极限环。如果 $s = \\frac{2}{3}$，则 $y$ 的递推关系变为：\n$$\ny_{k+1} = \\left(1 - 3 \\cdot \\frac{2}{3}\\right) y_{k} = (1 - 2) y_{k} = -y_{k}\n$$\n我们来检查迭代序列：\n当 $k=0$ 时：$\\mathbf{r}_{0} = (0, y_{0})$。\n当 $k=1$ 时：$y_{1} = -y_{0}$，因此 $\\mathbf{r}_{1} = (0, -y_{0})$。\n当 $k=2$ 时：$y_{2} = -y_{1} = -(-y_{0}) = y_{0}$，因此 $\\mathbf{r}_{2} = (0, y_{0})$。\n当 $k=3$ 时：$y_{3} = -y_{2} = -y_{0}$，因此 $\\mathbf{r}_{3} = (0, -y_{0})$。\n点序列为 $(0, y_{0}), (0, -y_{0}), (0, y_{0}), (0, -y_{0}), \\dots$，这正是指定的两点极限环。该算法永远不会收敛到位于 $(0,0)$ 的最小值。\n\n产生这种行为的恒定步长 $s$ 的值为 $\\frac{2}{3}$。", "answer": "$$\\boxed{\\frac{2}{3}}$$", "id": "2463052"}, {"introduction": "面对标准梯度下降法在狭长山谷中“之”字形前进、收敛缓慢的问题，我们如何改进？动量法（Momentum）是解决这一问题的关键技术之一。该方法通过引入一个“动量”项，累积过去的梯度信息，从而在平坦方向上加速前进，同时抑制在陡峭方向上的振荡。本练习要求你从第一性原理出发，对比分析标准梯度下降（GD）与重球动量法（Heavy-Ball momentum, HB）在典型二次函数上的表现，并推导出最优的学习率 $\\eta$ 和动量参数 $\\beta$。通过这个过程，你将定量地理解动量法为何能显著加速收敛，尤其是在高条件数的优化问题中[@problem_id:3186112]。", "problem": "考虑严格凸二次目标函数 $f(x,y) = \\frac{1}{2}\\left(a x^{2} + b y^{2}\\right)$，其中 $a  b  0$ 且 $a \\gg b$。其梯度为 $\\nabla f(x,y) = \\left(a x, b y\\right)$，Hessian矩阵为对角矩阵，特征值为 $a$ 和 $b$。你将从第一性原理出发，比较梯度下降法 (GD) 和重球动量法 (HB) 在此目标函数上的表现。\n\n仅从以下迭代算法的定义出发\n$$(\\text{GD})\\quad \\mathbf{z}_{t+1} = \\mathbf{z}_{t} - \\eta \\nabla f(\\mathbf{z}_{t}),$$\n$$(\\text{HB})\\quad \\mathbf{z}_{t+1} = \\mathbf{z}_{t} - \\eta \\nabla f(\\mathbf{z}_{t}) + \\beta\\left(\\mathbf{z}_{t} - \\mathbf{z}_{t-1}\\right),$$\n其中 $\\mathbf{z}_{t} = (x_{t}, y_{t})$，推导并分析沿Hessian矩阵的每个特征方向上导出的一维递归关系。\n\n你的任务是：\n1. 对于GD，推导沿特征值 $\\lambda \\in \\{a,b\\}$ 的标量收缩因子，并确定能够最小化在集合 $\\{a,b\\}$ 上的最坏情况谱半径的学习率 $\\eta$。用 $a$ 和 $b$ 表示你的结果。\n2. 对于HB，推导沿特征值 $\\lambda \\in \\{a,b\\}$ 的标量二阶递归关系，然后选择 $\\eta$ 和 $\\beta$ 以在满足稳定性的条件下最小化在集合 $\\{a,b\\}$ 上的最坏情况谱半径。仅使用上述算法定义和基本的线性动力系统推理。将你的最优 $\\eta$ 和 $\\beta$ 表示为关于 $a$ 和 $b$ 的闭式解析表达式，并说明HB方法所达到的相应最小化最坏情况谱半径（作为 $a$ 和 $b$ 的函数）。\n3. 用文字讨论当 $a \\gg b$ 时出现的过冲和振荡之间的权衡，包括 $\\beta$ 和 $\\eta$ 如何影响在狭窄谷地中的轨迹行为，并提出一个具体的实验：指定 $a \\gg b$ 的 $(a,b)$、一个初始化点 $(x_{0},y_{0})$ 和 $(x_{-1},y_{-1})$，以及在你推导出的最优参数下，你会记录哪些指标来比较GD与HB。\n\n将你的最终答案报告为最优重球法参数对 $(\\eta^{\\star}, \\beta^{\\star})$（用 $a$ 和 $b$ 表示）。无需四舍五入。使用LaTeX的 $\\texttt{pmatrix}$ 环境将该参数对格式化为一个 $1 \\times 2$ 的行矩阵。", "solution": "该问题要求在严格凸二次目标函数 $f(x,y) = \\frac{1}{2}(ax^2 + by^2)$（其中 $a  b  0$）上分析梯度下降法 (GD) 和重球动量法 (HB)。该分析需从第一性原理出发，通过研究Hessian矩阵各特征方向上的一维递归关系来进行。\n\n目标函数可以写成向量形式 $f(\\mathbf{z}) = \\frac{1}{2}\\mathbf{z}^T H \\mathbf{z}$，其中 $\\mathbf{z} = \\begin{pmatrix} x \\\\ y \\end{pmatrix}$ 且 $H = \\begin{pmatrix} a  0 \\\\ 0  b \\end{pmatrix}$。梯度为 $\\nabla f(\\mathbf{z}) = H\\mathbf{z}$。Hessian矩阵 $H$ 的特征值为 $\\lambda_1 = a$ 和 $\\lambda_2 = b$，对应的特征向量沿着坐标轴方向。优化算法的动力学沿着这些轴是解耦的。我们可以通过研究一个一般特征值 $\\lambda \\in \\{a,b\\}$ 的标量递归关系来分析收敛性。\n\n### 1. 梯度下降法 (GD) 分析\n\nGD的更新规则由下式给出：\n$$ \\mathbf{z}_{t+1} = \\mathbf{z}_{t} - \\eta \\nabla f(\\mathbf{z}_{t}) $$\n代入 $\\nabla f(\\mathbf{z}_t) = H\\mathbf{z}_t$，我们得到：\n$$ \\mathbf{z}_{t+1} = \\mathbf{z}_t - \\eta H \\mathbf{z}_t = (I - \\eta H) \\mathbf{z}_t $$\n这是一个线性动力系统。对于 $\\mathbf{z}_t$ 沿着Hessian矩阵 $H$ 的某个特征值 $\\lambda$ 对应的特征向量方向上的分量，其标量更新规则为：\n$$ z_{t+1}^{(\\lambda)} = (1 - \\eta \\lambda) z_t^{(\\lambda)} $$\n项 $c(\\eta, \\lambda) = 1 - \\eta\\lambda$ 是该分量的收缩因子。对于一个迭代方法，要使其收敛，其所有模式的收缩因子的模都必须小于1。总体收敛率由最大的模决定，即迭代矩阵 $I - \\eta H$ 的谱半径：\n$$ \\rho(I - \\eta H) = \\max_{\\lambda \\in \\{a, b\\}} |1 - \\eta \\lambda| $$\n为保证收敛，我们需要 $\\rho  1$，这意味着对于 $\\lambda=a$ 和 $\\lambda=b$，都有 $|1 - \\eta\\lambda|  1$。即 $-1  1 - \\eta\\lambda  1$，可简化为 $0  \\eta\\lambda  2$。由于 $ab0$，此式必须对 $\\lambda=a$ 成立，从而得到稳定性条件 $0  \\eta  \\frac{2}{a}$。\n\n我们的目标是找到能最小化最坏情况谱半径的学习率 $\\eta$：\n$$ \\eta^{\\star} = \\arg\\min_{\\eta} \\max(|1 - \\eta a|, |1 - \\eta b|) $$\n两个关于 $\\eta$ 的线性函数的最大值的最小值出现在它们的绝对值相等时。即 $|1 - \\eta a| = |1 - \\eta b|$。由于 $\\eta  0$ 且 $a  b$，有 $1-\\eta a  1-\\eta b$。为使它们的模相等，我们必须有：\n$$ 1 - \\eta b = -(1 - \\eta a) = \\eta a - 1 $$\n求解 $\\eta$：\n$$ 2 = \\eta a + \\eta b = \\eta(a+b) $$\n$$ \\eta_{GD}^{\\star} = \\frac{2}{a+b} $$\n这个 $\\eta$ 值在稳定范围内，因为 $a+b  a$，所以 $\\frac{2}{a+b}  \\frac{2}{a}$。相应的最小谱半径是：\n$$ \\rho_{GD} = \\left|1 - b \\left(\\frac{2}{a+b}\\right)\\right| = \\left|\\frac{a+b-2b}{a+b}\\right| = \\frac{a-b}{a+b} $$\n用条件数 $\\kappa = a/b$ 表示，这等于 $\\rho_{GD} = \\frac{\\kappa-1}{\\kappa+1}$。\n\n### 2. 重球动量法 (HB) 分析\n\nHB的更新规则是：\n$$ \\mathbf{z}_{t+1} = \\mathbf{z}_{t} - \\eta \\nabla f(\\mathbf{z}_{t}) + \\beta(\\mathbf{z}_{t} - \\mathbf{z}_{t-1}) $$\n代入 $\\nabla f(\\mathbf{z}_t) = H\\mathbf{z}_t$ 并考虑对应于特征值 $\\lambda$ 的模式的标量递归关系：\n$$ z_{t+1} = z_t - \\eta \\lambda z_t + \\beta(z_t - z_{t-1}) $$\n$$ z_{t+1} = (1 - \\eta\\lambda + \\beta)z_t - \\beta z_{t-1} $$\n这是一个二阶线性齐次递推关系。其动力学行为由其特征多项式的根决定：\n$$ \\mu^2 - (1 - \\eta\\lambda + \\beta)\\mu + \\beta = 0 $$\n为使系统稳定，两个根 $\\mu_1$ 和 $\\mu_2$ 的模都必须小于1。收敛率由 $\\rho_\\lambda = \\max(|\\mu_1|, |\\mu_2|)$ 决定。根为：\n$$ \\mu = \\frac{1 - \\eta\\lambda + \\beta \\pm \\sqrt{(1 - \\eta\\lambda + \\beta)^2 - 4\\beta}}{2} $$\n根的性质取决于判别式 $D = (1 - \\eta\\lambda + \\beta)^2 - 4\\beta$。\n如果 $D \\ge 0$，根为实数，谱半径为 $\\rho_\\lambda = \\frac{|1 - \\eta\\lambda + \\beta| + \\sqrt{D}}{2}$。\n如果 $D  0$，根为一对共轭复数，其模为 $|\\mu| = \\sqrt{\\text{根的乘积}} = \\sqrt{\\beta}$。谱半径为 $\\rho_\\lambda = \\sqrt{\\beta}$。\n\n为了最小化最坏情况谱半径 $\\max_{\\lambda \\in \\{a,b\\}} \\rho_\\lambda$，我们试图使 $\\lambda=a$ 和 $\\lambda=b$ 的谱半径相等且尽可能小。最优解出现在两个特征值的动力学都处于复数根区域且位于实数根区域的边界上时，即判别式为非正，且两者的谱半径均为 $\\sqrt{\\beta}$。这要求：\n$$ (1 - \\eta\\lambda + \\beta)^2 - 4\\beta \\le 0 \\quad \\text{for } \\lambda \\in \\{a,b\\} $$\n$$ |1 - \\eta\\lambda + \\beta| \\le 2\\sqrt{\\beta} $$\n这等价于 $-2\\sqrt{\\beta} \\le 1 - \\eta\\lambda + \\beta \\le 2\\sqrt{\\beta}$，整理后可得：\n$$ (1-\\sqrt{\\beta})^2 \\le \\eta\\lambda \\le (1+\\sqrt{\\beta})^2 $$\n为了最小化 $\\beta$（从而最小化谱半径 $\\sqrt{\\beta}$），我们希望这个区间尽可能小，同时仍然包含 $[\\eta b, \\eta a]$。这可以通过让区间的端点匹配来实现：\n$$ \\eta b = (1 - \\sqrt{\\beta})^2 $$\n$$ \\eta a = (1 + \\sqrt{\\beta})^2 $$\n用第二个方程除以第一个方程得到：\n$$ \\frac{a}{b} = \\left(\\frac{1 + \\sqrt{\\beta}}{1 - \\sqrt{\\beta}}\\right)^2 $$\n取平方根并求解 $\\sqrt{\\beta}$：\n$$ \\sqrt{\\frac{a}{b}} = \\frac{1 + \\sqrt{\\beta}}{1 - \\sqrt{\\beta}} \\implies \\sqrt{a}(1-\\sqrt{\\beta}) = \\sqrt{b}(1+\\sqrt{\\beta}) $$\n$$ \\sqrt{a} - \\sqrt{a}\\sqrt{\\beta} = \\sqrt{b} + \\sqrt{b}\\sqrt{\\beta} $$\n$$ \\sqrt{a} - \\sqrt{b} = (\\sqrt{a} + \\sqrt{b})\\sqrt{\\beta} $$\n这得出 $\\sqrt{\\beta^{\\star}} = \\frac{\\sqrt{a}-\\sqrt{b}}{\\sqrt{a}+\\sqrt{b}}$。由于 $ab0$，我们有 $0  \\sqrt{\\beta^{\\star}}  1$，这是一个有效的参数。最优动量参数是：\n$$ \\beta^{\\star} = \\left(\\frac{\\sqrt{a}-\\sqrt{b}}{\\sqrt{a}+\\sqrt{b}}\\right)^2 $$\n现在，我们使用 $\\eta a = (1+\\sqrt{\\beta})^2$ 来找到最优学习率 $\\eta^{\\star}$。首先，我们计算 $1+\\sqrt{\\beta^{\\star}}$：\n$$ 1+\\sqrt{\\beta^{\\star}} = 1 + \\frac{\\sqrt{a}-\\sqrt{b}}{\\sqrt{a}+\\sqrt{b}} = \\frac{(\\sqrt{a}+\\sqrt{b}) + (\\sqrt{a}-\\sqrt{b})}{\\sqrt{a}+\\sqrt{b}} = \\frac{2\\sqrt{a}}{\\sqrt{a}+\\sqrt{b}} $$\n将此代入关于 $\\eta$ 的方程中：\n$$ \\eta^{\\star} a = \\left(\\frac{2\\sqrt{a}}{\\sqrt{a}+\\sqrt{b}}\\right)^2 = \\frac{4a}{(\\sqrt{a}+\\sqrt{b})^2} $$\n$$ \\eta^{\\star} = \\frac{4}{(\\sqrt{a}+\\sqrt{b})^2} $$\n使用这些最优参数，两种模式的谱半径均为 $\\sqrt{\\beta^{\\star}}$，因此HB的最坏情况谱半径为：\n$$ \\rho_{HB} = \\frac{\\sqrt{a}-\\sqrt{b}}{\\sqrt{a}+\\sqrt{b}} $$\n用条件数 $\\kappa=a/b$ 表示，这等于 $\\rho_{HB} = \\frac{\\sqrt{\\kappa}-1}{\\sqrt{\\kappa}+1}$。这个速率显著优于GD的速率 $\\frac{\\kappa-1}{\\kappa+1}$，特别是对于大的 $\\kappa$。\n\n### 3. 讨论与实验提议\n\n当 $a \\gg b$ 时，目标函数 $f(x,y)$ 具有一个细长的窄谷形状。条件数 $\\kappa = a/b$ 很大。$f$ 的等高线是高度拉长的椭圆。梯度在 $x$ 方向（特征值 $a$）上很陡峭，而在 $y$ 方向（特征值 $b$）上很平缓。\n\n对于GD，最优学习率 $\\eta_{GD}^{\\star} = \\frac{2}{a+b} \\approx \\frac{2}{a}$ 非常小，它受到最大特征值 $a$ 的限制以确保稳定性。这个小学习率导致沿着平坦谷地（$y$ 方向）的进展非常缓慢，因为更新量 $- \\eta \\nabla_y f = -\\eta b y$ 非常微小。该算法表现出典型的“之”字形行为，在窄谷之间来回振荡，同时沿着谷底缓慢地向最小值点爬行。\n\n对于HB，动量项 $\\beta(\\mathbf{z}_t - \\mathbf{z}_{t-1})$ 充当了过去更新的记忆。对于 $a \\gg b$ 时的最优参数，我们有 $\\beta^{\\star} \\approx (1 - 2\\sqrt{b/a})^2 \\approx 1 - 4\\sqrt{b/a}$，这个值接近于1。这样大的 $\\beta$ 值确保了更新是在许多步上取平均。跨越谷地（在 $x$ 方向上）的振荡梯度分量会随着时间的推移而相互抵消，而沿着谷地（在 $y$ 方向上）的一致的、微小的梯度分量则会累积起来，从而建立速度并加速前进。最优学习率 $\\eta_{HB}^{\\star} = \\frac{4}{(\\sqrt{a}+\\sqrt{b})^2} \\approx \\frac{4}{a}$ 大约是GD的两倍。这个较大的学习率被动量项稳定了。其权衡之处在于潜在的过冲；动量可能导致迭代点“飞过”最小值点，并以比GD更大的振幅在其周围振荡，但总体轨迹更平滑，收敛速度也快得多。\n\n一个比较这两种方法的具体实验如下：\n- **问题设置**：令 $a=100$ 和 $b=1$。这给出的条件数为 $\\kappa=100$。目标函数为 $f(x,y) = 50x^2 + 0.5y^2$。\n- **算法参数**：\n  - GD: $\\eta_{GD}^{\\star} = \\frac{2}{100+1} = \\frac{2}{101} \\approx 0.0198$。\n  - HB:\n    - $\\beta^{\\star} = \\left(\\frac{\\sqrt{100}-\\sqrt{1}}{\\sqrt{100}+\\sqrt{1}}\\right)^2 = \\left(\\frac{9}{11}\\right)^2 = \\frac{81}{121} \\approx 0.6694$。\n    - $\\eta^{\\star} = \\frac{4}{(\\sqrt{100}+\\sqrt{1})^2} = \\frac{4}{11^2} = \\frac{4}{121} \\approx 0.0331$。\n- **初始化**：从一个在陡峭方向上偏离中心、在平缓方向上远离最小值的点开始，例如 $\\mathbf{z}_0 = (x_0, y_0) = (1, 10)$。对于HB，通过设置 $\\mathbf{z}_{-1} = \\mathbf{z}_0$ 来初始化为零速度。\n- **指标**：两种算法都运行固定次数的迭代（例如，$N=100$）。对每次迭代 $t=0, 1, \\dots, N$，记录：\n  1. 迭代点位置 $\\mathbf{z}_t = (x_t, y_t)$。\n  2. 目标函数值 $f(\\mathbf{z}_t)$。\n  3. 梯度范数 $\\|\\nabla f(\\mathbf{z}_t)\\|_2$。\n- **分析**：\n  - 在 $f(x,y)$ 的二维等高线图上绘制GD和HB的轨迹 $(\\mathbf{z}_0, \\mathbf{z}_1, \\dots, \\mathbf{z}_N)$。这将直观展示GD的“之”字形行为与HB更平滑、更快速的路径。\n  - 在半对数坐标（$f$ 的对数 vs. 线性 $t$）上绘制两种方法的 $f(\\mathbf{z}_t)$ 与 $t$ 的关系图。这将清晰地显示线性收敛率，并证明HB的收敛斜率要陡峭得多，表明收敛更快。\n  - 在半对数坐标上绘制 $\\|\\nabla f(\\mathbf{z}_t)\\|_2$ 与 $t$ 的关系图，这为收敛到临界点提供了另一个视角。", "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{4}{(\\sqrt{a}+\\sqrt{b})^{2}}  \\left(\\frac{\\sqrt{a}-\\sqrt{b}}{\\sqrt{a}+\\sqrt{b}}\\right)^{2} \\end{pmatrix}}\n$$", "id": "3186112"}]}