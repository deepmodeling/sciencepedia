{"hands_on_practices": [{"introduction": "要真正理解梯度爆炸，最好的方法莫过于亲手揭示其数学根源。这项练习将指导你完成一个受控的计算实验，以探究梯度爆炸的根本原因。通过构建一个深度线性网络并精确控制其权重矩阵的谱半径（即算子范数），你将根据第一性原理，凭经验验证梯度范数与网络深度 $L$ 和谱半径 $\\alpha$ 之间存在的指数关系——$\\alpha^L$ 定律。[@problem_id:3185019] 这个实验为你理解训练不稳定性提供了坚实的理论和实证基础。", "problem": "你需要实现一个受控计算实验，使用带谱归一化的深度线性模型来研究深度学习中的梯度爆炸现象。该实验必须基于基本原理，特别是复合函数的链式法则、算子范数的定义以及范数的次乘性。从以下基础开始：向量微积分中的复合函数链式法则、矩阵算子 $2$-范数的定义 $\\|W\\|_{2} = \\max_{\\|x\\|_{2}=1} \\|W x\\|_{2}$，以及次乘性 $\\|A B\\|_{2} \\le \\|A\\|_{2}\\|B\\|_{2}$。除这些基础知识外，不要假定任何更高级的公式。\n\n你将构建一个深度为 $L$、使用权重共享和线性损失的深度线性网络。具体来说，考虑一个输入向量 $x \\in \\mathbb{R}^{d}$，一个在所有层之间共享的方阵权重矩阵 $W \\in \\mathbb{R}^{d \\times d}$，以及一个输出 $y = W^{L} x$。使用线性标量损失 $L_{\\text{loss}} = u^{\\top} y$，其中 $u \\in \\mathbb{R}^{d}$ 是一个固定的读出向量。我们关注的梯度是损失 $L_{\\text{loss}}$ 相对于输入 $x$ 的梯度。你的任务是，当权重矩阵经过谱归一化使其算子范数 $\\|W\\|_{2} = \\alpha$ 时，根据经验估计范数 $\\|\\nabla_{x} L_{\\text{loss}}\\|_{2}$ 如何作为目标谱半径 $\\alpha$ 的函数进行缩放。\n\n实现以下受控设置：\n- 使用谱归一化，定义为 $W = \\alpha \\,\\bar{W}/\\|\\bar{W}\\|_{2}$，其中 $\\bar{W} \\in \\mathbb{R}^{d \\times d}$ 是一个半正定矩阵，由 $\\bar{W} = A A^{\\top}$ 构建，而 $A \\in \\mathbb{R}^{d \\times d}$ 是一个具有独立标准正态分布条目的随机矩阵。这保证了 $\\bar{W}$ 是对称半正定的，并且 $\\|W\\|_{2} = \\alpha$。\n- 选择读出向量 $u$ 为 $\\bar{W}$（等价于 $W$）的单位范数主特征向量，以确保在重复应用下增长的最坏情况对齐。\n- 使用固定的输入维度 $d = 8$ 和固定的随机种子 $s = 12345$ 来构建 $A$，以使结果是确定性的。\n- 对于每个深度 $L$，在一组固定的正值上改变 $\\alpha$，并对每个 $\\alpha$ 测量 $\\|\\nabla_{x} L_{\\text{loss}}\\|_{2}$。然后，在对数空间中通过线性回归拟合一个形式为 $\\|\\nabla_{x} L_{\\text{loss}}\\|_{2} \\approx C \\,\\alpha^{p}$ 的模型，以估计该深度的指数 $p$。\n\n你的程序必须实现以下测试套件和估计协议：\n- 维度和种子：$d = 8$， $s = 12345$。\n- 深度：$L \\in \\{1, 3, 5, 7\\}$。\n- 目标谱半径：$\\alpha \\in \\{0.25, 0.5, 1.0, 1.2, 1.5\\}$。\n- 对于每个固定的 $L$，使用上述带有权重共享和线性损失的构造，在每个 $\\alpha$ 处计算 $\\|\\nabla_{x} L_{\\text{loss}}\\|_{2}$，然后通过使用普通最小二乘法拟合 $\\log \\|\\nabla_{x} L_{\\text{loss}}\\|_{2}$ 与 $\\log \\alpha$ 的关系来估计指数 $p$，从而获得斜率。使用自然对数。\n- 因为所有的 $\\alpha$ 值都是无量纲的，没有物理单位，所以不需要单位转换。不涉及角度。\n\n你的程序应该生成单行输出，其中包含按上述深度顺序列出的每个深度的拟合指数，格式为方括号内以逗号分隔的列表（例如，“[result1,result2,result3,result4]”）。结果必须是浮点数。\n\n约束和澄清：\n- 你必须通过显式地构建梯度 $\\nabla_{x} L_{\\text{loss}} = (W^{\\top})^{L} u$ 并计算其欧几里得范数，来为所构建的线性网络精确计算 $\\|\\nabla_{x} L_{\\text{loss}}\\|_{2}$。不要使用自动微分库。\n- 你必须使用指定的谱归一化过程 $W = \\alpha \\,\\bar{W}/\\|\\bar{W}\\|_{2}$，并且必须使用 $\\bar{W}$ 的单位长度主特征向量作为 $u$。\n- 最终程序不得要求任何输入，并且必须使用固定的种子 $s$ 以保证可复现性。", "solution": "问题陈述已经过验证，并被确定是合理的。它在科学上基于应用于深度学习的线性代数和向量微积分原理，其设定良好，具有完整且一致的参数集，并且其表述是客观的。该任务是一个精心设计的计算实验，旨在受控环境中凭经验研究梯度爆炸问题。\n\n该实验旨在揭示权重矩阵的谱半径与相对于输入的梯度幅度之间的关系。我们将首先从基本原理推导出理论期望，然后描述计算验证的实现。\n\n### 理论基础\n\n1.  **模型与梯度计算**：\n    深度线性网络由函数 $f(x) = y = W^L x$ 定义，其中 $W \\in \\mathbb{R}^{d \\times d}$ 是一个跨 $L$ 层共享的权重矩阵，而 $x \\in \\mathbb{R}^d$ 是输入。损失由输出的线性函数给出：$L_{\\text{loss}} = u^{\\top} y = u^{\\top} W^L x$。损失是向量 $x$ 的标量函数。损失相对于输入的梯度 $\\nabla_x L_{\\text{loss}}$ 可以使用链式法则求得。认识到 $L_{\\text{loss}}$ 的形式为 $c^{\\top}x$，其中向量 $c = (W^L)^{\\top} u$，梯度就是 $\\nabla_x L_{\\text{loss}} = c$。\n    因此，梯度由下式给出：\n    $$\n    \\nabla_x L_{\\text{loss}} = (W^L)^{\\top} u = (W^{\\top})^L u\n    $$\n\n2.  **权重矩阵 $W$ 的性质**：\n    权重矩阵 $W$ 是通过谱归一化构建的。首先，从一个条目取自标准正态分布的随机矩阵 $A$ 创建一个矩阵 $\\bar{W} = A A^{\\top}$。\n    *   $\\bar{W}$ 是对称的，因为 $(\\bar{W})^{\\top} = (A A^{\\top})^{\\top} = (A^{\\top})^{\\top} A^{\\top} = A A^{\\top} = \\bar{W}$。\n    *   $\\bar{W}$ 是半正定的，因为对于任何向量 $v \\in \\mathbb{R}^d$，都有 $v^{\\top}\\bar{W}v = v^{\\top}A A^{\\top}v = (A^{\\top}v)^{\\top}(A^{\\top}v) = \\|A^{\\top}v\\|_2^2 \\ge 0$。\n    最终的权重矩阵是 $W = \\alpha \\frac{\\bar{W}}{\\|\\bar{W}\\|_2}$，其中 $\\alpha  0$ 是目标谱半径（算子 $2$-范数）。由于 $\\bar{W}$ 是对称的，$W$ 也是对称的，这意味着 $W^{\\top} = W$。这可将梯度表达式简化为：\n    $$\n    \\nabla_x L_{\\text{loss}} = W^L u\n    $$\n    $W$ 的算子 $2$-范数是：\n    $$\n    \\|W\\|_2 = \\left\\| \\alpha \\frac{\\bar{W}}{\\|\\bar{W}\\|_2} \\right\\|_2 = \\alpha \\frac{\\|\\bar{W}\\|_2}{\\|\\bar{W}\\|_2} = \\alpha\n    $$\n    这证实了该构造正确地将 $W$ 的谱半径设置为 $\\alpha$。\n\n3.  **通过选择特征向量实现最坏情况对齐**：\n    问题指定读出向量 $u$ 必须是 $\\bar{W}$ 的单位范数主特征向量。由于 $\\bar{W}$ 是对称半正定的，其算子 $2$-范数 $\\|\\bar{W}\\|_2$ 等于其最大特征值 $\\lambda_{\\text{max}}(\\bar{W})$。主特征向量 $u$ 是与此特征值对应的特征向量，因此 $\\bar{W}u = \\lambda_{\\text{max}}(\\bar{W})u = \\|\\bar{W}\\|_2 u$。\n\n    这种对 $u$ 的选择也使其成为 $W$ 的主特征向量：\n    $$\n    Wu = \\left( \\alpha \\frac{\\bar{W}}{\\|\\bar{W}\\|_2} \\right) u = \\frac{\\alpha}{\\|\\bar{W}\\|_2} (\\bar{W}u) = \\frac{\\alpha}{\\|\\bar{W}\\|_2} (\\|\\bar{W}\\|_2 u) = \\alpha u\n    $$\n    因此，$u$ 是 $W$ 的一个特征向量，其特征值为 $\\alpha$。\n\n4.  **梯度范数缩放**：\n    我们现在可以计算将 $W$ 重复应用于 $u$ 的结果：\n    $$\n    W^L u = W^{L-1}(Wu) = W^{L-1}(\\alpha u) = \\alpha W^{L-1}u = \\dots = \\alpha^L u\n    $$\n    梯度的范数则为：\n    $$\n    \\|\\nabla_x L_{\\text{loss}}\\|_2 = \\|W^L u\\|_2 = \\|\\alpha^L u\\|_2 = |\\alpha|^L \\|u\\|_2\n    $$\n    由于 $\\alpha  0$ 且 $u$ 是一个单位范数向量（$\\|u\\|_2 = 1$），该表达式简化为一个精确的等式：\n    $$\n    \\|\\nabla_x L_{\\text{loss}}\\|_2 = \\alpha^L\n    $$\n    这个理论结果预测，梯度的范数按谱半径 $\\alpha$ 的 $L$ 次方进行缩放。\n\n5.  **指数的经验估计**：\n    实验旨在将测量数据拟合到模型 $\\|\\nabla_x L_{\\text{loss}}\\|_2 \\approx C \\alpha^p$。通过对两边取自然对数，我们得到一个线性关系：\n    $$\n    \\ln(\\|\\nabla_x L_{\\text{loss}}\\|_2) \\approx \\ln(C) + p \\ln(\\alpha)\n    $$\n    这对应于一条斜率为 $p$、截距为 $\\ln(C)$ 的直线。根据我们的理论推导，我们期望数据能完美拟合这个模型，其中 $p = L$ 且 $C=1$（即 $\\ln(C)=0$）。程序将对每个深度 $L$ 的数据对 $(\\ln(\\alpha), \\ln(\\|\\nabla_x L_{\\text{loss}}\\|_2))$ 执行线性回归，以估计斜率 $p$。\n\n### 算法设计\n\n该 Python 脚本按如下方式实现所述实验：\n1.  **初始化**：设置固定参数 $d=8$、随机种子 $s=12345$，以及要测试的深度 $L$ 和谱半径 $\\alpha$ 的列表。\n2.  **矩阵生成**：使用 NumPy 库和指定的种子生成随机矩阵 $A$，并构造对称半正定矩阵 $\\bar{W} = AA^{\\top}$。\n3.  **特征分解**：使用 `numpy.linalg.eigh` 计算 $\\bar{W}$ 的特征值和特征向量，该函数针对对称矩阵进行了优化。最大特征值给出 $\\|\\bar{W}\\|_2$，对应的特征向量即为所需的读出向量 $u$。\n4.  **迭代实验**：\n    - 遍历每个深度 $L \\in \\{1, 3, 5, 7\\}$。\n    - 在此循环内，遍历每个谱半径 $\\alpha \\in \\{0.25, 0.5, 1.0, 1.2, 1.5\\}$。\n    - 对于每对 $(L, \\alpha)$：\n        a. 构造权重矩阵 $W = \\alpha \\frac{\\bar{W}}{\\|\\bar{W}\\|_2}$。\n        b. 通过将 $W$ 迭代应用于 $u$ 共 $L$ 次来计算梯度向量 $g = W^L u$。\n        c. 计算欧几里得范数 $\\|g\\|_2$。\n        d. 存储对数转换后的值 $\\ln(\\alpha)$ 和 $\\ln(\\|g\\|_2)$。\n5.  **指数估计**：在固定深度 $L$ 下收集了所有 $\\alpha$ 值的数据后，使用 `scipy.stats.linregress` 对收集的对数转换数据执行普通最小二乘法。所得回归线的斜率即为估计的指数 $p$。\n6.  **输出**：收集每个深度 $L$ 的估计指数，并以指定格式 `[p1,p3,p5,p7]` 打印。结果应非常接近理论值 $[1.0, 3.0, 5.0, 7.0]$。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import linregress\n\ndef solve():\n    \"\"\"\n    Implements a controlled experiment to investigate the exploding gradient\n    phenomenon in a deep linear network, grounded in first principles.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    d = 8\n    s = 12345\n    depths = [1, 3, 5, 7]\n    alphas = [0.25, 0.5, 1.0, 1.2, 1.5]\n\n    # Set the random seed for deterministic and reproducible results.\n    np.random.seed(s)\n\n    # Construct the positive semidefinite matrix W_bar = A * A^T.\n    # This matrix is guaranteed to be symmetric positive semidefinite.\n    A = np.random.randn(d, d)\n    W_bar = A @ A.T\n\n    # Compute eigenvalues and eigenvectors of the symmetric matrix W_bar.\n    # np.linalg.eigh is used for Hermitian/symmetric matrices. It returns\n    # eigenvalues in ascending order and corresponding orthonormal eigenvectors.\n    eigenvalues, eigenvectors = np.linalg.eigh(W_bar)\n\n    # The operator 2-norm of a symmetric PSD matrix is its largest eigenvalue.\n    norm_W_bar = eigenvalues[-1]\n\n    # The readout vector u is the unit-norm dominant eigenvector of W_bar.\n    # This corresponds to the eigenvector for the largest eigenvalue.\n    u = eigenvectors[:, -1]\n\n    results = []\n    # Iterate over each specified depth L.\n    for L in depths:\n        log_alphas = []\n        log_grad_norms = []\n\n        # For each depth, iterate over the range of target spectral radii alpha.\n        for alpha in alphas:\n            # Construct the weight matrix W with spectral radius alpha.\n            # W = alpha * W_bar / ||W_bar||_2\n            W = alpha * W_bar / norm_W_bar\n\n            # Since W is symmetric, the gradient grad(L_loss) w.r.t. x is W^L * u.\n            # We compute this iteratively for numerical stability and efficiency.\n            grad = u\n            for _ in range(L):\n                grad = W @ grad\n\n            # Compute the L2 norm of the resulting gradient vector.\n            grad_norm = np.linalg.norm(grad)\n\n            # Store the natural logarithms for regression analysis.\n            # We fit log(grad_norm) = p * log(alpha) + log(C).\n            log_alphas.append(np.log(alpha))\n            log_grad_norms.append(np.log(grad_norm))\n\n        # Perform ordinary least squares regression in log-space.\n        # The slope of the regression is the desired exponent p.\n        regression_result = linregress(x=log_alphas, y=log_grad_norms)\n        exponent_p = regression_result.slope\n        results.append(exponent_p)\n    \n    # As per the problem, the theoretical expectation is that p = L.\n    # The printed results should be very close to [1.0, 3.0, 5.0, 7.0].\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```", "id": "3185019"}, {"introduction": "在从理论上验证了梯度爆炸的成因之后，本次练习将场景切换到一个更具体的多层感知机（MLP）中。你将首先在一个简化的模型中亲眼见证梯度范数的指数级增长，然后着手实现两种广泛应用的梯度裁剪技术——范数裁剪和值裁剪——来抑制这个问题。[@problem_id:3184988] 这个实践环节旨在连接理论与实际应用，让你掌握缓解梯度爆炸的关键实用技能。", "problem": "考虑一个具有 $L$ 个层的全连接多层感知机 (MLP)，每个层的宽度为 $d$ 且不含偏置项。假设激活函数为恒等函数，且每个层的权重矩阵为 $W_\\ell = \\alpha I_d$，其中 $I_d$ 是 $d \\times d$ 的单位矩阵，$\\alpha \\in \\mathbb{R}$。设网络输入为 $x_0 \\in \\mathbb{R}^d$，网络输出为 $y \\in \\mathbb{R}^d$，目标为 $t \\in \\mathbb{R}^d$。标量损失由标准均方误差定义：$\\ell(y,t) = \\frac{1}{2}\\|y - t\\|_2^2$。仅使用微积分的链式法则、矩阵乘法性质和欧几里得范数定义，推导梯度 $\\nabla_{x_0} \\ell$ 的显式表达式，及其大小相对于参数 $\\alpha$ 和 $L$ 的缩放关系。然后，实现两种梯度裁剪策略：范数裁剪和值裁剪。范数裁剪对于一个阈值 $c  0$，将梯度 $g \\in \\mathbb{R}^d$ 乘以因子 $\\min\\left(1, \\frac{c}{\\|g\\|_2}\\right)$。值裁剪对于一个阈值 $v  0$，将每个分量 $g_i$ 限制在区间 $[-v, v]$ 内。\n\n你的程序必须对每个提供的测试用例计算以下量值：\n- 比率 $R = \\frac{\\|\\nabla_{x_0} \\ell\\|_2}{\\|\\nabla_{y} \\ell\\|_2}$。\n- 裁剪前的欧几里得范数 $\\|\\nabla_{x_0} \\ell\\|_2$。\n- 经过范数裁剪后的梯度的欧几里得范数。\n- 经过值裁剪后的梯度的欧几里得范数。\n\n使用双精度浮点运算。不涉及物理单位。不涉及角度。所有返回的数值必须是浮点数。\n\n测试套件（每个元组为 $(\\alpha, L, d, x_0, t, c, v)$）：\n- 情况 $1$ (一般情况)：$(\\alpha, L, d) = (1.2, 8, 5)$, $x_0 = [1.0, -0.5, 0.25, 2.0, -1.5]$, $t = [0.0, 0.0, 0.0, 0.0, 0.0]$, $c = 10.0$, $v = 3.0$。\n- 情况 $2$ (边界 $\\alpha = 1$)：$(\\alpha, L, d) = (1.0, 7, 4)$, $x_0 = [0.1, -0.2, 0.3, -0.4]$, $t = [0.5, -0.5, 0.5, -0.5]$, $c = 5.0$, $v = 2.5$。\n- 情况 $3$ (爆炸区域)：$(\\alpha, L, d) = (2.0, 12, 4)$, $x_0 = [0.01, 0.02, -0.03, 0.04]$, $t = [0.0, 0.0, 0.0, 0.0]$, $c = 1.0$, $v = 0.5$。\n- 情况 $4$ (消失区域)：$(\\alpha, L, d) = (0.8, 20, 6)$, $x_0 = [-0.2, 0.1, -0.1, 0.05, -0.05, 0.025]$, $t = [1.0, 1.0, 1.0, 1.0, 1.0, 1.0]$, $c = 0.1$, $v = 0.05$。\n- 情况 $5$ (边界 $L = 0$)：$(\\alpha, L, d) = (1.7, 0, 3)$, $x_0 = [2.0, -1.0, 0.5]$, $t = [-1.0, 0.0, 1.0]$, $c = 0.5$, $v = 0.3$。\n- 情况 $6$ (标量边缘情况)：$(\\alpha, L, d) = (3.0, 15, 1)$, $x_0 = [0.1]$, $t = [-0.2]$, $c = 2.0$, $v = 1.0$。\n\n你的程序应生成单行输出，其中包含一个用方括号括起来的、以逗号分隔的结果列表。每个元素对应一个测试用例，并且本身是一个按精确顺序排列的、包含四个浮点数的列表：$[R, \\|\\nabla_{x_0} \\ell\\|_2, \\|\\text{norm\\_clipped}(\\nabla_{x_0} \\ell)\\|_2, \\|\\text{value\\_clipped}(\\nabla_{x_0} \\ell)\\|_2]$。例如，最终输出应类似于 $[[r_1,n_1,cn_1,vn_1],[r_2,n_2,cn_2,vn_2],\\dots]$。", "solution": "我们从第一性原理出发，并将推导限制在微积分的链式法则、矩阵乘法的基本性质和欧几里得范数之内。该多层感知机 (MLP) 有 $L$ 个层，宽度为 $d$，每个权重矩阵为 $W_\\ell = \\alpha I_d$，激活函数为恒等函数。前向映射是线性映射的复合。用 $x_0 \\in \\mathbb{R}^d$ 表示输入，用 $y \\in \\mathbb{R}^d$ 表示输出。由于激活函数是恒等函数，第 $\\ell$ 层的逐层变换为 $x_\\ell = W_\\ell x_{\\ell-1}$。因为对于所有 $\\ell$，都有 $W_\\ell = \\alpha I_d$，所以我们有\n$$\nx_\\ell = \\alpha I_d x_{\\ell-1} = \\alpha x_{\\ell-1}.\n$$\n通过对 $L$ 个层进行归纳，\n$$\ny = x_L = \\alpha x_{L-1} = \\alpha^2 x_{L-2} = \\cdots = \\alpha^L x_0.\n$$\n损失是均方误差\n$$\n\\ell(y,t) = \\frac{1}{2} \\|y - t\\|_2^2,\n$$\n其关于 $y$ 的梯度是\n$$\n\\nabla_y \\ell = y - t.\n$$\n使用链式法则，关于输入 $x_0$ 的梯度由 $\\nabla_{x_0} \\ell = J^\\top \\nabla_y \\ell$ 给出，其中 $J = \\frac{\\partial y}{\\partial x_0}$ 是前向映射的雅可比矩阵。从 $y = \\alpha^L x_0$ 可知，雅可比矩阵是\n$$\nJ = \\alpha^L I_d.\n$$\n因此，\n$$\n\\nabla_{x_0} \\ell = (\\alpha^L I_d)^\\top (y - t) = \\alpha^L (y - t).\n$$\n因此，欧几里得范数的缩放关系为\n$$\n\\|\\nabla_{x_0} \\ell\\|_2 = |\\alpha|^L \\|y - t\\|_2.\n$$\n由此可得，比率\n$$\nR = \\frac{\\|\\nabla_{x_0} \\ell\\|_2}{\\|\\nabla_y \\ell\\|_2} = \\frac{|\\alpha|^L \\|y - t\\|_2}{\\|y - t\\|_2} = |\\alpha|^L.\n$$\n此推导表明，当 $|\\alpha|  1$ 且 $L$ 足够大时，梯度的大小随 $L$ 呈指数增长，这就是梯度爆炸现象。相反，当 $|\\alpha|  1$ 时，梯度的大小会衰减，表现为梯度消失。对于边界情况 $|\\alpha| = 1$ 或 $L = 0$，比率为 $R = 1$。\n\n我们现在定义两种裁剪策略来缓解梯度爆炸：\n- 范数裁剪：对于梯度向量 $g \\in \\mathbb{R}^d$ 和阈值 $c  0$，定义\n$$\ng_{\\text{norm-clip}} = g \\cdot \\min\\left(1, \\frac{c}{\\|g\\|_2}\\right).\n$$\n当 $\\|g\\|_2 \\le c$ 时，这不会改变 $g$；当 $\\|g\\|_2  c$ 时，它会将 $g$ 按比例缩小，使得 $\\|g_{\\text{norm-clip}}\\|_2 = c$。\n- 值裁剪：对于阈值 $v  0$，定义\n$$\n(g_{\\text{value-clip}})_i = \\max(-v, \\min(g_i, v)) \\quad \\text{for each component } i.\n$$\n这会将每个条目限制在区间 $[-v, v]$ 内，可能会减小范数并限制每个分量的大小。\n\n每个测试用例的算法计划：\n1. 解析 $(\\alpha, L, d, x_0, t, c, v)$。\n2. 计算前向输出 $y = \\alpha^L x_0$。\n3. 计算 $\\nabla_y \\ell = y - t$。\n4. 计算 $\\nabla_{x_0} \\ell = \\alpha^L (y - t)$。\n5. 计算 $R = \\frac{\\|\\nabla_{x_0} \\ell\\|_2}{\\|\\nabla_y \\ell\\|_2}$。\n6. 计算未裁剪的范数 $\\|\\nabla_{x_0} \\ell\\|_2$。\n7. 应用范数裁剪得到 $g_{\\text{norm-clip}}$ 并计算其范数。\n8. 应用值裁剪得到 $g_{\\text{value-clip}}$ 并计算其范数。\n9. 返回列表 $[R, \\|\\nabla_{x_0} \\ell\\|_2, \\|g_{\\text{norm-clip}}\\|_2, \\|g_{\\text{value-clip}}\\|_2]$。\n\n边缘情况考虑：\n- 对于 $L = 0$，有 $y = x_0$，因此 $J = I_d$，且 $R = 1$；梯度未被放大。\n- 对于 $|\\alpha| = 1$，无论 $L$ 为何值，$R = 1$；梯度的大小得以保持。\n- 对于 $d = 1$，同样的推导适用，范数与绝对值一致；值裁剪可能与范数裁剪有很大不同，因为它直接限制唯一的那个分量。\n- 数值稳定性：双精度可以处理所选的 $\\alpha$ 和 $L$ 值，它们产生的范数高达约 $10^{14}$，这在合理的浮点动态范围内。\n\n程序汇总所有情况的结果，并以指定的单行格式打印它们。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef forward_output(alpha: float, L: int, x0: np.ndarray) - np.ndarray:\n    \"\"\"Compute y = alpha^L * x0 for the identity-activation MLP with W_l = alpha I.\"\"\"\n    return (alpha ** L) * x0\n\ndef loss_grad_wrt_y(y: np.ndarray, t: np.ndarray) - np.ndarray:\n    \"\"\"Gradient of 0.5 * ||y - t||^2 with respect to y is (y - t).\"\"\"\n    return y - t\n\ndef input_grad(alpha: float, L: int, grad_y: np.ndarray) - np.ndarray:\n    \"\"\"Gradient wrt input x0 given grad wrt y and Jacobian J = alpha^L * I.\"\"\"\n    return (alpha ** L) * grad_y\n\ndef norm_clip(g: np.ndarray, c: float) - np.ndarray:\n    \"\"\"Clip gradient by global norm threshold c.\"\"\"\n    norm = np.linalg.norm(g)\n    if norm == 0.0:\n        return g.copy()\n    scale = min(1.0, c / norm)\n    return g * scale\n\ndef value_clip(g: np.ndarray, v: float) - np.ndarray:\n    \"\"\"Clip gradient by value threshold v (component-wise clamp).\"\"\"\n    return np.clip(g, -v, v)\n\ndef compute_case(alpha, L, d, x0_list, t_list, c, v):\n    x0 = np.array(x0_list, dtype=float)\n    t = np.array(t_list, dtype=float)\n    assert x0.shape == (d,) or (x0.shape == () and d==1), \"x0 must have shape (d,)\"\n    assert t.shape == (d,) or (t.shape == () and d==1), \"t must have shape (d,)\"\n\n    y = forward_output(alpha, L, x0)\n    gy = loss_grad_wrt_y(y, t)\n    gx = input_grad(alpha, L, gy)\n\n    norm_gy = np.linalg.norm(gy)\n    norm_gx = np.linalg.norm(gx)\n\n    # Ratio R = ||grad_x0|| / ||grad_y||\n    R = float(norm_gx / norm_gy) if norm_gy != 0.0 else float('inf')\n\n    gx_norm_clipped = norm_clip(gx, c)\n    gx_value_clipped = value_clip(gx, v)\n\n    return [R, norm_gx, float(np.linalg.norm(gx_norm_clipped)), float(np.linalg.norm(gx_value_clipped))]\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each case: (alpha, L, d, x0, t, c, v)\n    test_cases = [\n        (1.2, 8, 5,  [1.0, -0.5, 0.25, 2.0, -1.5], [0.0, 0.0, 0.0, 0.0, 0.0], 10.0, 3.0),\n        (1.0, 7, 4,  [0.1, -0.2, 0.3, -0.4], [0.5, -0.5, 0.5, -0.5], 5.0, 2.5),\n        (2.0, 12, 4, [0.01, 0.02, -0.03, 0.04], [0.0, 0.0, 0.0, 0.0], 1.0, 0.5),\n        (0.8, 20, 6, [-0.2, 0.1, -0.1, 0.05, -0.05, 0.025], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0], 0.1, 0.05),\n        (1.7, 0, 3,  [2.0, -1.0, 0.5], [-1.0, 0.0, 1.0], 0.5, 0.3),\n        (3.0, 15, 1, [0.1], [-0.2], 2.0, 1.0),\n    ]\n\n    results = []\n    for case in test_cases:\n        alpha, L, d, x0, t, c, v = case\n        result = compute_case(alpha, L, d, x0, t, c, v)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    # Single line, comma-separated list enclosed in square brackets, each element a list of four floats.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3184988"}, {"introduction": "并非所有解决方案都生而平等。这项高级练习将挑战你超越“应用修复”的层面，转而从解析上深入比较两种裁剪策略：按范数裁剪与按值裁剪。通过高维概率分析，你将揭示为何保持梯度方向至关重要，以及哪种方法能更好地实现这一目标。[@problem_id:3185069] 这将为你提供一个关于这些优化工具更深层次的几何直觉。", "problem": "考虑一个深度神经网络层，其梯度向量建模为 $g \\in \\mathbb{R}^{d}$，分量为 $g_{i} = a z_{i}$，其中 $a  0$ 是一个捕捉梯度爆炸尺度的标量，$(z_{i})_{i=1}^{d}$ 是来自标准正态分布 $\\mathcal{N}(0,1)$ 的独立同分布（i.i.d.）样本。设裁剪阈值为 $\\tau  0$。定义了两种梯度裁剪方案如下：\n- 按值裁剪：$h^{\\mathrm{val}}_{i} = \\max(-\\tau, \\min(g_{i}, \\tau))$，对每个坐标 $i$。\n- 按范数裁剪：$h^{\\mathrm{norm}} = \\min\\!\\left(1, \\frac{\\tau}{\\|g\\|_{2}}\\right) g$，其中 $\\|g\\|_{2}$ 是欧几里得范数。\n\n使用两个非零向量 $x, y \\in \\mathbb{R}^{d}$ 之间的余弦相似度定义，\n$$\nC(x,y) = \\frac{x \\cdot y}{\\|x\\|_{2} \\, \\|y\\|_{2}},\n$$\n分析在高维梯度爆炸情况（其中 $d \\to \\infty$ 且 $a/\\tau \\to \\infty$）下的极限。从第一性原理和概率论中经过充分检验的事实出发，推导极限余弦相似度 $C_{\\mathrm{val}} = \\lim C(g, h^{\\mathrm{val}})$ 和 $C_{\\mathrm{norm}} = \\lim C(g, h^{\\mathrm{norm}})$，然后计算极限差值\n$$\n\\Delta = C_{\\mathrm{norm}} - C_{\\mathrm{val}}.\n$$\n将 $\\Delta$ 以单个实数的形式给出，四舍五入到四位有效数字。本问题不涉及单位。", "solution": "该问题提法明确，具有科学依据，并包含得出唯一解所需的所有信息。我们可以开始推导。\n\n问题要求计算极限差值 $\\Delta = C_{\\mathrm{norm}} - C_{\\mathrm{val}}$，即梯度向量 $g$ 与其裁剪后版本 $h^{\\mathrm{norm}}$（按范数裁剪）和 $h^{\\mathrm{val}}$（按值裁剪）之间的余弦相似度之差。分析在以下极限条件下进行：维度 $d \\to \\infty$ 且梯度爆炸尺度因子相对于裁剪阈值的比值 $a/\\tau \\to \\infty$。\n\n首先，我们分析按范数裁剪的余弦相似度 $C_{\\mathrm{norm}} = \\lim C(g, h^{\\mathrm{norm}})$。\n梯度向量为 $g \\in \\mathbb{R}^{d}$，其分量为 $g_{i} = a z_{i}$，其中 $z_{i} \\sim \\mathcal{N}(0,1)$ 是独立同分布的随机变量且 $a  0$。\n$g$ 的欧几里得范数的平方是 $\\|g\\|_{2}^{2} = \\sum_{i=1}^{d} g_{i}^{2} = \\sum_{i=1}^{d} (a z_{i})^{2} = a^{2} \\sum_{i=1}^{d} z_{i}^{2}$。\n变量 $z_{i}^{2}$ 是独立同分布的，其均值为 $E[z_{i}^{2}] = \\mathrm{Var}(z_{i}) + (E[z_{i}])^{2} = 1 + 0^{2} = 1$。\n根据大数定律 (LLN)，当 $d \\to \\infty$ 时，样本均值依概率收敛于期望值：\n$$\n\\frac{1}{d} \\|g\\|_{2}^{2} = \\frac{a^{2}}{d} \\sum_{i=1}^{d} z_{i}^{2} \\xrightarrow{p} a^{2} E[z_{i}^{2}] = a^{2}\n$$\n这意味着对于大的 $d$，$\\|g\\|_{2}^{2} \\approx a^{2}d$，因此 $\\|g\\|_{2} \\approx a\\sqrt{d}$。\n当 $d \\to \\infty$ 时，$\\|g\\|_{2} \\to \\infty$ 以概率 $1$ 成立。在 $a/\\tau \\to \\infty$ 的情况下，对于足够大的 $d$，几乎可以肯定 $\\|g\\|_{2} \\gg \\tau$。\n按范数裁剪的定义是 $h^{\\mathrm{norm}} = \\min\\!\\left(1, \\frac{\\tau}{\\|g\\|_{2}}\\right) g$。因为 $\\|g\\|_{2}  \\tau$，最小值计算为 $\\frac{\\tau}{\\|g\\|_{2}}$。\n因此，在指定的极限下，$h^{\\mathrm{norm}} = \\frac{\\tau}{\\|g\\|_{2}} g$。\n裁剪后的向量 $h^{\\mathrm{norm}}$ 只是原始向量 $g$ 的一个重新缩放。\n余弦相似度的定义为 $C(x,y) = \\frac{x \\cdot y}{\\|x\\|_{2} \\|y\\|_{2}}$。对于 $x=g$ 和 $y=h^{\\mathrm{norm}}$：\n$$\nC(g, h^{\\mathrm{norm}}) = \\frac{g \\cdot \\left(\\frac{\\tau}{\\|g\\|_{2}} g\\right)}{\\|g\\|_{2} \\left\\|\\frac{\\tau}{\\|g\\|_{2}} g\\right\\|_{2}} = \\frac{\\frac{\\tau}{\\|g\\|_{2}} (g \\cdot g)}{\\|g\\|_{2} \\left|\\frac{\\tau}{\\|g\\|_{2}}\\right| \\|g\\|_{2}} = \\frac{\\frac{\\tau}{\\|g\\|_{2}} \\|g\\|_{2}^{2}}{\\|g\\|_{2} \\frac{\\tau}{\\|g\\|_{2}} \\|g\\|_{2}} = \\frac{\\tau \\|g\\|_{2}}{\\tau \\|g\\|_{2}} = 1\n$$\n这在 $g$ 不是零向量的情况下成立，而这种情况发生的概率为零。因此，极限余弦相似度为 $1$。\n$$\nC_{\\mathrm{norm}} = 1\n$$\n\n接下来，我们分析按值裁剪的余弦相似度 $C_{\\mathrm{val}} = \\lim C(g, h^{\\mathrm{val}})$。\n余弦相似度的公式涉及点积和范数，它们是各分量的和。当 $d \\to \\infty$ 时，我们再次可以应用大数定律。\n$$\nC(g, h^{\\mathrm{val}})^{2} = \\frac{(g \\cdot h^{\\mathrm{val}})^{2}}{\\|g\\|_{2}^{2} \\|h^{\\mathrm{val}}\\|_{2}^{2}} = \\frac{\\left(\\frac{1}{d}\\sum_{i=1}^{d} g_{i} h^{\\mathrm{val}}_{i}\\right)^{2}}{\\left(\\frac{1}{d}\\sum_{i=1}^{d} g_{i}^{2}\\right) \\left(\\frac{1}{d}\\sum_{i=1}^{d} (h^{\\mathrm{val}}_{i})^{2}\\right)}\n$$\n当 $d \\to \\infty$ 时，此表达式依概率收敛于：\n$$\nC_{\\mathrm{val}}^{2} = \\frac{(E[g_{i} h^{\\mathrm{val}}_{i}])^{2}}{E[g_{i}^{2}] E[(h^{\\mathrm{val}}_{i})^{2}]}\n$$\n由于各分量是独立同分布的，我们可以省略下标 $i$。设 $z \\sim \\mathcal{N}(0,1)$，$g=az$，以及 $h^{\\mathrm{val}} = \\max(-\\tau, \\min(g, \\tau)) = \\mathrm{clip}(az, -\\tau, \\tau)$。$z$ 的概率密度函数 (PDF) 是 $\\phi(z) = \\frac{1}{\\sqrt{2\\pi}} \\exp(-z^{2}/2)$。\n我们需要在 $c = \\tau/a \\to 0$ 的极限下计算三个期望值。\n\n1.  $E[g^{2}] = E[(az)^{2}] = a^{2}E[z^{2}] = a^{2}$。\n\n2.  $E[g h^{\\mathrm{val}}] = \\int_{-\\infty}^{\\infty} (az) \\mathrm{clip}(az, -\\tau, \\tau) \\phi(z) dz$。\n裁剪函数意味着当 $|az| \\le \\tau$ (即 $|z| \\le c$) 时 $h^{\\mathrm{val}}$ 为 $az$，当 $|az|  \\tau$ (即 $|z|  c$) 时为 $\\tau \\cdot \\mathrm{sgn}(z)$。\n$$\nE[g h^{\\mathrm{val}}] = \\int_{-c}^{c} (az)(az) \\phi(z) dz + \\int_{c}^{\\infty} (az)(\\tau) \\phi(z) dz + \\int_{-\\infty}^{-c} (az)(-\\tau) \\phi(z) dz\n$$\n$$\nE[g h^{\\mathrm{val}}] = a^{2}\\int_{-c}^{c} z^{2}\\phi(z)dz + a\\tau \\int_{c}^{\\infty}z\\phi(z)dz - a\\tau\\int_{-\\infty}^{-c}z\\phi(z)dz\n$$\n根据 $z\\phi(z)$ 的对称性，最后两个积分合并为 $2a\\tau \\int_{c}^{\\infty} z\\phi(z)dz$。积分 $\\int z\\phi(z)dz = -\\phi(z)$。所以，$\\int_{c}^{\\infty} z\\phi(z)dz = [-\\phi(z)]_{c}^{\\infty} = \\phi(c)$。\n$$\nE[g h^{\\mathrm{val}}] = a^{2}\\int_{-c}^{c} z^{2}\\phi(z)dz + 2a\\tau \\phi(c)\n$$\n\n3.  $E[(h^{\\mathrm{val}})^{2}] = \\int_{-\\infty}^{\\infty} (\\mathrm{clip}(az, -\\tau, \\tau))^{2} \\phi(z) dz$。\n$$\nE[(h^{\\mathrm{val}})^{2}] = \\int_{-c}^{c} (az)^{2} \\phi(z) dz + \\int_{|z|c} \\tau^{2} \\phi(z) dz = a^{2}\\int_{-c}^{c} z^{2}\\phi(z)dz + \\tau^{2}P(|z|c)\n$$\n\n我们现在构建 $C_{\\mathrm{val}}$ 的表达式，并计算当 $c \\to 0$ 时的极限。\n$$\nC_{\\mathrm{val}} = \\lim_{c \\to 0} \\frac{E[g h^{\\mathrm{val}}]}{\\sqrt{E[g^{2}] E[(h^{\\mathrm{val}})^{2}]}} = \\lim_{c \\to 0} \\frac{a^{2}\\int_{-c}^{c} z^{2}\\phi(z)dz + 2a\\tau \\phi(c)}{a \\sqrt{a^{2}\\int_{-c}^{c} z^{2}\\phi(z)dz + \\tau^{2}P(|z|c)}}\n$$\n我们代入 $a = \\tau/c$，并将分子和分母同除以 $\\tau^{2}/c$：\n$$\nC_{\\mathrm{val}} = \\lim_{c \\to 0} \\frac{\\frac{1}{c}\\int_{-c}^{c} z^{2}\\phi(z)dz + 2\\phi(c)}{\\sqrt{\\frac{1}{c^{2}}\\int_{-c}^{c} z^{2}\\phi(z)dz + P(|z|c)}}\n$$\n我们计算当 $c \\to 0$ 时各项的极限：\n-   分子第一项（使用洛必达法则）：$\\lim_{c \\to 0} \\frac{\\int_{-c}^{c} z^{2}\\phi(z)dz}{c} = \\lim_{c \\to 0} \\frac{\\frac{d}{dc}\\int_{-c}^{c} z^{2}\\phi(z)dz}{1} = \\lim_{c \\to 0} (c^{2}\\phi(c) - (-c)^{2}\\phi(-c)(-1)) = \\lim_{c \\to 0} 2c^{2}\\phi(c) = 0$。\n-   分子第二项：$\\lim_{c \\to 0} 2\\phi(c) = 2\\phi(0) = 2 \\frac{1}{\\sqrt{2\\pi}} = \\sqrt{\\frac{2}{\\pi}}$。\n-   分母第一项（使用洛必达法则两次）：$\\lim_{c \\to 0} \\frac{\\int_{-c}^{c} z^{2}\\phi(z)dz}{c^{2}} = \\lim_{c \\to 0} \\frac{2c^{2}\\phi(c)}{2c} = \\lim_{c \\to 0} c\\phi(c) = 0$。\n-   分母第二项：$\\lim_{c \\to 0} P(|z|c) = P(|z|0) = 1$。\n\n将这些极限代回 $C_{\\mathrm{val}}$ 的表达式中：\n$$\nC_{\\mathrm{val}} = \\frac{0 + \\sqrt{\\frac{2}{\\pi}}}{\\sqrt{0 + 1}} = \\sqrt{\\frac{2}{\\pi}}\n$$\n\n最后，我们计算差值 $\\Delta = C_{\\mathrm{norm}} - C_{\\mathrm{val}}$。\n$$\n\\Delta = 1 - \\sqrt{\\frac{2}{\\pi}}\n$$\n为了获得数值，我们使用 $\\pi \\approx 3.14159...$：\n$$\n\\sqrt{\\frac{2}{\\pi}} \\approx \\sqrt{0.63661977...} \\approx 0.79788456...\n$$\n$$\n\\Delta \\approx 1 - 0.79788456 = 0.20211544...\n$$\n四舍五入到四位有效数字，我们得到 $0.2021$。", "answer": "$$\n\\boxed{0.2021}\n$$", "id": "3185069"}]}