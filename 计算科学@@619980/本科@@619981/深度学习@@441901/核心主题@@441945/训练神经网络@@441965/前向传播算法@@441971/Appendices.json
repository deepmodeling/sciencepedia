{"hands_on_practices": [{"introduction": "整流线性单元（$\\mathrm{ReLU}$）是神经网络中的一个基本非线性构建模块。这个练习将帮助你理解单个神经元的激活如何在几何上对应于将输入空间沿一个超平面（由 $w_i^\\top x + b_i = 0$ 定义）切分的，并进一步探索多个神经元如何共同作用，将输入空间划分为具有不同激活模式的区域。通过构建特定的权重 $W$、偏置 $b$ 和输入 $x$，你将深入理解形如 $a = \\mathrm{ReLU}(Wx+b)$ 的网络层在前向传播中的决策机制。[@problem_id:3185431]", "problem": "考虑一个带有修正线性单元（ReLU）激活的单层前馈网络，其前向传播被逐元素地定义为 $a = \\mathrm{ReLU}(W x + b)$。修正线性单元（ReLU）函数定义为 $\\mathrm{ReLU}(z) = \\max(0, z)$。设输入维度为 $n = 2$，单元数量为 $m = 4$。在整个问题中，用 $w_1, w_2, w_3, w_4 \\in \\mathbb{R}^2$ 表示 $W$ 的行向量。\n\n你将分析如何通过精心选择的 $x$、$W$ 和 $b$ 来强制产生恰好 $k=2$ 个非零激活，然后从第一性原理出发，推断由 $W$ 和 $b$ 导出的 $\\mathbb{R}^2$ 区域划分如何依赖于它们。\n\n选择所有正确的选项。\n\nA. 设 $W$ 的行向量为 $w_1 = (1, 0)$，$w_2 = (-1, 0)$，$w_3 = (0, 1)$，$w_4 = (0, -1)$，且 $b = \\mathbf{0} \\in \\mathbb{R}^4$。对于 $x = (1, 1)$，前向传播产生恰好 $k = 2$ 个非零激活。在此配置下，导出的区域边界是超平面 $x_1 = 0$ 和 $x_2 = 0$。\n\nB. 使用与选项A中相同的 $W$ 和 $b = \\mathbf{0}$，选择 $x = (0, 0)$ 会产生恰好 $k = 2$ 个非零激活。\n\nC. 使用与选项A中相同的 $W$，选择 $b = (0.5, 0.5, -0.5, -0.5)$ 和 $x = (0, 0)$。那么前向传播产生恰好 $k = 2$ 个非零激活。\n\nD. 当 $b = \\mathbf{0}$ 时，将整个矩阵 $W$ 乘以任意正常数 $\\alpha > 0$ 不会改变 $\\mathbb{R}^2$ 到恒定激活模式区域的划分（即，对于 $x \\in \\mathbb{R}^2$，$a$ 的支撑集不变）。\n\nE. 当 $b = \\mathbf{0}$ 时，将任意单行 $w_i$ 替换为 $-w_i$ 会保持几何超平面边界 $\\{x \\in \\mathbb{R}^2 : w_i^{\\top} x = 0\\}$ 不变，但会翻转该边界的哪一侧对于单元 $i$ 是激活的，从而改变分配给每个区域的激活模式。\n\nF. 对于任意选择的 $W \\in \\mathbb{R}^{m \\times n}$ 和 $b \\in \\mathbb{R}^m$，当 $x$ 在 $\\mathbb{R}^n$ 上变化时，所实现的独特激活模式的数量恰好是 $2^m$。", "solution": "首先验证问题陈述，以确保其科学上合理、定义明确且客观。\n\n### 第1步：提取给定信息\n- 神经网络是一个单层前馈网络。\n- 激活函数是修正线性单元（ReLU），逐元素应用：$\\mathrm{ReLU}(z) = \\max(0, z)$。\n- 前向传播由方程 $a = \\mathrm{ReLU}(W x + b)$ 定义。\n- 输入向量维度为 $n = 2$，所以 $x \\in \\mathbb{R}^2$。\n- 单元数量（输出维度）为 $m = 4$，所以 $a, b \\in \\mathbb{R}^4$ 且 $W \\in \\mathbb{R}^{4 \\times 2}$。\n- 权重矩阵 $W$ 的行向量表示为 $w_1, w_2, w_3, w_4 \\in \\mathbb{R}^2$。\n\n### 第2步：使用提取的给定信息进行验证\n该问题描述了一个标准的单神经元层网络，这是深度学习中的一个基本构建块。其组成部分——输入向量、权重矩阵、偏置向量、ReLU激活函数以及前向传播方程——在该领域都是标准且定义明确的。选项中提出的问题是关于该系统在不同参数化下行为的具体、可检验的数学论断。\n\n- **科学基础：** 该问题基于人工神经网络已建立的数学框架。所有定义和概念都是标准的。\n- **定义明确：** 该问题提供了评估选项中论断所需的所有必要信息。每个选项都提出了一个清晰、可证伪的假设。\n- **客观性：** 该问题以精确的数学语言陈述，没有歧义或主观内容。\n\n### 第3步：结论与行动\n问题陈述是有效的。将对选项进行全面解答和评估。\n\n分析的核心涉及预激活向量 $z = Wx + b$，其分量为 $z_i = w_i^\\top x + b_i$，其中 $i \\in \\{1, 2, 3, 4\\}$。第 $i$ 个激活是 $a_i = \\mathrm{ReLU}(z_i)$。一个激活 $a_i$ 是非零的当且仅当其对应的预激活 $z_i$ 是正的，即 $a_i > 0 \\iff w_i^\\top x + b_i > 0$。第 $i$ 个单元的激活区域和非激活区域之间的边界是由方程 $w_i^\\top x + b_i = 0$ 定义的超平面（在 $\\mathbb{R}^2$ 中是一条直线）。\n\n### 逐项分析\n\n**A. 设 $W$ 的行向量为 $w_1 = (1, 0)$，$w_2 = (-1, 0)$，$w_3 = (0, 1)$，$w_4 = (0, -1)$，且 $b = \\mathbf{0} \\in \\mathbb{R}^4$。对于 $x = (1, 1)$，前向传播产生恰好 $k = 2$ 个非零激活。在此配置下，导出的区域边界是超平面 $x_1 = 0$ 和 $x_2 = 0$。**\n\n首先，我们用给定的值计算预激活向量 $z = Wx + b$。\n$W = \\begin{pmatrix} 1  & 0 \\\\ -1  & 0 \\\\ 0  & 1 \\\\ 0  & -1 \\end{pmatrix}$，$x = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$，$b = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix}$。\n\n$z = Wx = \\begin{pmatrix} 1  & 0 \\\\ -1  & 0 \\\\ 0  & 1 \\\\ 0  & -1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 1 \\cdot 1 + 0 \\cdot 1 \\\\ -1 \\cdot 1 + 0 \\cdot 1 \\\\ 0 \\cdot 1 + 1 \\cdot 1 \\\\ 0 \\cdot 1 + (-1) \\cdot 1 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ -1 \\\\ 1 \\\\ -1 \\end{pmatrix}$。\n\n接下来，我们应用ReLU函数得到激活向量 $a$：\n$a = \\mathrm{ReLU}(z) = \\mathrm{ReLU}\\left(\\begin{pmatrix} 1 \\\\ -1 \\\\ 1 \\\\ -1 \\end{pmatrix}\\right) = \\begin{pmatrix} \\max(0, 1) \\\\ \\max(0, -1) \\\\ \\max(0, 1) \\\\ \\max(0, -1) \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\\\ 0 \\end{pmatrix}$。\n非零激活的数量为 $k=2$（$a_1$ 和 $a_3$）。该陈述的这部分是正确的。\n\n现在，我们分析区域边界。边界由 $w_i^\\top x + b_i = 0$ 定义。由于 $b = \\mathbf{0}$，这简化为 $w_i^\\top x = 0$。\n设 $x = (x_1, x_2)^\\top$。\n- 对于 $i=1$：$w_1^\\top x = (1, 0) \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} = x_1 = 0$。\n- 对于 $i=2$：$w_2^\\top x = (-1, 0) \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} = -x_1 = 0 \\implies x_1 = 0$。\n- 对于 $i=3$：$w_3^\\top x = (0, 1) \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} = x_2 = 0$。\n- 对于 $i=4$：$w_4^\\top x = (0, -1) \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} = -x_2 = 0 \\implies x_2 = 0$。\n不同的边界超平面集合是 $\\{x_1 = 0\\}$ 和 $\\{x_2 = 0\\}$。这些是坐标轴。该陈述的这部分也是正确的。\n\n结论：**正确**。\n\n**B. 使用与选项A中相同的 $W$ 和 $b = \\mathbf{0}$，选择 $x = (0, 0)$ 会产生恰好 $k = 2$ 个非零激活。**\n\n使用选项A中的 $W$，$b = \\mathbf{0}$，以及 $x = (0, 0)^\\top$。\n$z = Wx + b = W\\mathbf{0} + \\mathbf{0} = \\mathbf{0} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix}$。\n$a = \\mathrm{ReLU}(z) = \\mathrm{ReLU}(\\mathbf{0}) = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix}$。\n非零激活的数量是 $k=0$。陈述声称 $k=2$。\n\n结论：**错误**。\n\n**C. 使用与选项A中相同的 $W$，选择 $b = (0.5, 0.5, -0.5, -0.5)$ 和 $x = (0, 0)$。那么前向传播产生恰好 $k = 2$ 个非零激活。**\n\n使用选项A中的 $W$，$x = (0, 0)^\\top$，以及 $b = (0.5, 0.5, -0.5, -0.5)^\\top$。\n$z = Wx + b = W\\mathbf{0} + b = b = \\begin{pmatrix} 0.5 \\\\ 0.5 \\\\ -0.5 \\\\ -0.5 \\end{pmatrix}$。\n$a = \\mathrm{ReLU}(z) = \\mathrm{ReLU}\\left(\\begin{pmatrix} 0.5 \\\\ 0.5 \\\\ -0.5 \\\\ -0.5 \\end{pmatrix}\\right) = \\begin{pmatrix} \\max(0, 0.5) \\\\ \\max(0, 0.5) \\\\ \\max(0, -0.5) \\\\ \\max(0, -0.5) \\end{pmatrix} = \\begin{pmatrix} 0.5 \\\\ 0.5 \\\\ 0 \\\\ 0 \\end{pmatrix}$。\n非零激活的数量为 $k=2$（$a_1$ 和 $a_2$）。\n\n结论：**正确**。\n\n**D. 当 $b = \\mathbf{0}$ 时，将整个矩阵 $W$ 乘以任意正常数 $\\alpha > 0$ 不会改变 $\\mathbb{R}^2$ 到恒定激活模式区域的划分（即，对于 $x \\in \\mathbb{R}^2$，$a$ 的支撑集不变）。**\n\n激活模式由预激活 $z_i$ 的符号决定。当 $b = \\mathbf{0}$ 时，我们有 $z_i = w_i^\\top x$。第 $i$ 个单元在 $w_i^\\top x > 0$ 时激活。\n设缩放后的权重矩阵为 $W' = \\alpha W$，其中 $\\alpha > 0$。新的权重向量为 $w'_i = \\alpha w_i$。\n新的预激活为 $z'_i = (w'_i)^\\top x = (\\alpha w_i)^\\top x = \\alpha(w_i^\\top x)$。\n由于 $\\alpha > 0$，$z'_i$ 的符号与 $z_i = w_i^\\top x$ 的符号相同。\n- $z'_i > 0 \\iff \\alpha(w_i^\\top x) > 0 \\iff w_i^\\top x > 0$。\n- $z'_i \\le 0 \\iff \\alpha(w_i^\\top x) \\le 0 \\iff w_i^\\top x \\le 0$。\n对于任何给定的输入 $x$，激活单元的集合（激活模式）保持不变。边界超平面由 $w_i^\\top x = 0$ 定义。对于缩放后的系统，它们由 $(\\alpha w_i)^\\top x = 0$ 定义，这等价于 $w_i^\\top x = 0$。边界的几何位置不变。因此，输入空间到恒定激活模式区域的划分不变。\n\n结论：**正确**。\n\n**E. 当 $b = \\mathbf{0}$ 时，将任意单行 $w_i$ 替换为 $-w_i$ 会保持几何超平面边界 $\\{x \\in \\mathbb{R}^2 : w_i^{\\top} x = 0\\}$ 不变，但会翻转该边界的哪一侧对于单元 $i$ 是激活的，从而改变分配给每个区域的激活模式。**\n\n设 $w'_i = -w_i$。\n单元 $i$ 的原始边界是满足 $w_i^\\top x = 0$ 的点集 $x$。\n新的边界是满足 $(w'_i)^\\top x = 0$ 的点集 $x$。这就是 $(-w_i)^\\top x = - (w_i^\\top x) = 0$，等价于 $w_i^\\top x = 0$。几何边界确实没有改变。\n\n单元 $i$ 的原始激活区域是 $w_i^\\top x > 0$ 的地方。\n单元 $i$ 的新激活区域是 $(w'_i)^\\top x > 0$ 的地方，即 $(-w_i)^\\top x > 0$，或 $-(w_i^\\top x) > 0$。这简化为 $w_i^\\top x < 0$。\n单元 $i$ 的激活区域已经从超平面的一侧翻转到另一侧。因此，对于任何不在边界上的点 $x$，第 $i$ 个单元的激活状态将与原始配置相比被翻转。这改变了划分中每个区域的激活模式（表示哪些单元激活的0和1向量）。\n\n结论：**正确**。\n\n**F. 对于任意选择的 $W \\in \\mathbb{R}^{m \\times n}$ 和 $b \\in \\mathbb{R}^m$，当 $x$ 在 $\\mathbb{R}^n$ 上变化时，所实现的独特激活模式的数量恰好是 $2^m$。**\n\n一个激活模式是一个长度为 $m$ 的二进制向量，所以总共有 $2^m$ 种可能的模式。问题是，对于*任意*选择的 $W$ 和 $b$，是否都能实现所有这些模式。\n超平面 $w_i^\\top x + b_i = 0$（对于 $i=1, \\dots, m$）将输入空间 $\\mathbb{R}^n$ 划分为多个区域。在每个开放、连通的区域内，所有 $w_i^\\top x + b_i$ 的符号都是恒定的，因此激活模式是恒定的。因此，独特激活模式的数量受这些超平面创建的区域数量的限制。\n$m$ 个超平面能够将 $\\mathbb{R}^n$ 划分成的最大区域数由Zaslavsky公式给出，对于处于一般位置的超平面，该公式为 $\\sum_{j=0}^{n} \\binom{m}{j}$。\n在我们的例子中，$n=2$ 且 $m=4$。最大区域数为：\n$$ \\sum_{j=0}^{2} \\binom{4}{j} = \\binom{4}{0} + \\binom{4}{1} + \\binom{4}{2} = 1 + 4 + \\frac{4 \\cdot 3}{2} = 1 + 4 + 6 = 11 $$\n可能的激活模式总数为 $2^m = 2^4 = 16$。\n由于区域数（11）小于可能的模式数（16），因此不可能实现所有 $2^m$ 种模式。该陈述是普遍性的（“对于任意选择...”），所以一个反例就足以证伪它。上述几何论证表明，对于 $n=2, m=4$，无论如何选择 $W, b$ 都无法实现这一点（即使不考虑会进一步减少区域数量的退化情况，如平行或重合的超平面）。例如，如果我们选择所有 $w_i$ 都相同，比如对所有 $i$ 都有 $w_i = (1,0)$，并且 $b=\\mathbf{0}$，我们只得到一个独特的超平面 $x_1=0$。这会创建两个区域（$x_1>0$ 和 $x_1<0$），只实现两种模式：所有单元开启或所有单元关闭。\n\n结论：**错误**。", "answer": "$$\\boxed{ACDE}$$", "id": "3185431"}, {"introduction": "在理论上理解了前向传播的计算过程后，掌握实践中可能遇到的问题同样至关重要。本练习模拟了一个在深度学习编程中常见的微妙错误：张量形状不匹配。你将分析一个偏置向量 $b^{(1)}$ 的形状错误如何通过数值计算库的广播（broadcasting）机制，导致整个前向传播的计算流发生意想不到的改变，从而强调在实践中追踪张量形状的极端重要性。[@problem_id:3185351]", "problem": "一个双层多层感知机（MLP）的前向传播过程定义如下：给定一个输入列向量 $x \\in \\mathbb{R}^{d_{\\mathrm{in}} \\times 1}$，第一层的参数 $W^{(1)} \\in \\mathbb{R}^{d \\times d_{\\mathrm{in}}}$ 和 $b^{(1)} \\in \\mathbb{R}^{d \\times 1}$，以及第二层的参数 $W^{(2)} \\in \\mathbb{R}^{o \\times d}$ 和 $b^{(2)} \\in \\mathbb{R}^{o \\times 1}$，前向传播过程为\n$$\nz^{(1)} = W^{(1)} x + b^{(1)}, \\quad a^{(1)} = \\sigma\\!\\left(z^{(1)}\\right), \\quad z^{(2)} = W^{(2)} a^{(1)} + b^{(2)},\n$$\n其中 $\\sigma(\\cdot)$ 是任意一种按元素（element-wise）施加的激活函数。假设一位从业者意外地将第一层的偏置（bias）存储为形状为 $\\left(1,d\\right)$ 的行向量，而不是形状为 $\\left(d,1\\right)$ 的列向量，同时他使用的数值计算库对按元素操作实现了标准的广播（broadcasting）机制：从末尾开始比较维度，如果两个维度相等或其中一个为 $1$，则它们是兼容的，大小为 $1$ 的维度会被复制以匹配另一个维度。例如，将形状为 $\\left(d,1\\right)$ 和 $\\left(1,d\\right)$ 的数组相加，会产生一个形状为 $\\left(d,d\\right)$ 的数组，其元素是广播后元素的逐对相加之和。\n\n假设矩阵-矩阵和矩阵-向量乘法遵循标准的线性代数规则，并且不会在内部维度上进行广播。在这些条件下，关于在前向传播中使用形状为 $\\left(1,d\\right)$ 的 $b^{(1)}$ 所产生影响的最准确描述是什么？\n\n- A. $z^{(1)}$ 的计算会引发形状错误，因为 $\\left(d,1\\right)$ 和 $\\left(1,d\\right)$ 无法相加；在这种情况下无法进行广播。\n- B. 计算通过广播继续进行，生成 $z^{(1)} \\in \\mathbb{R}^{d \\times d}$，其元素为 $z^{(1)}_{ij} = \\left(W^{(1)}x\\right)_{i1} + b^{(1)}_{1j}$；因此 $a^{(1)} \\in \\mathbb{R}^{d \\times d}$ 且 $z^{(2)} \\in \\mathbb{R}^{o \\times d}$，就好像处理了一个大小为 $d$ 的伪批次（pseudo-batch），其中每一列都有不同的偏置偏移量。\n- C. 计算库会隐式地将 $b^{(1)}$ 转置为形状 $\\left(d,1\\right)$，因此 $z^{(1)} \\in \\mathbb{R}^{d \\times 1}$ 被正确计算，没有产生意外影响。\n- D. 计算库通过求和将 $b^{(1)}$ 折叠成一个标量来进行广播，因此 $z^{(1)} \\in \\mathbb{R}^{d \\times 1}$ 等于 $W^{(1)}x$ 加上一个加到所有隐藏单元上的单一标量偏移。\n- E. 形状不匹配只影响 $z^{(1)}$ 的数值，不影响其形状；$z^{(1)} \\in \\mathbb{R}^{d \\times 1}$ 仍然是结果，其中每个条目都被 $d$ 个偏置分量的平均值所偏移。\n\n选择唯一最佳答案，并根据矩阵乘法和广播的定义进行推理来证明你的选择。", "solution": "用户希望我根据一套关于广播和矩阵乘法的特定规则，来分析神经网络第一层偏置向量形状不匹配的后果。\n\n### 问题验证\n\n**步骤 1：提取已知条件**\n- 输入向量：$x \\in \\mathbb{R}^{d_{\\mathrm{in}} \\times 1}$\n- 第一层权重矩阵：$W^{(1)} \\in \\mathbb{R}^{d \\times d_{\\mathrm{in}}}$\n- 错误的第一层偏置向量：$b^{(1)}$ 的形状为 $(1, d)$ 而不是正确的 $(d, 1)$。\n- 第二层权重矩阵：$W^{(2)} \\in \\mathbb{R}^{o \\times d}$\n- 第二层偏置向量：$b^{(2)} \\in \\mathbb{R}^{o \\times 1}$\n- 前向传播方程：\n  1. $z^{(1)} = W^{(1)} x + b^{(1)}$\n  2. $a^{(1)} = \\sigma(z^{(1)})$，其中 $\\sigma(\\cdot)$ 是一个按元素施加的激活函数。\n  3. $z^{(2)} = W^{(2)} a^{(1)} + b^{(2)}$\n- 按元素操作（如加法）的广播规则：\n  - 从末尾开始比较维度。\n  - 如果两个维度相等或其中一个为 $1$，则它们是兼容的。\n  - 大小为 $1$ 的维度会被复制（拉伸）以匹配另一个维度。\n  - 提供的例子：将形状为 $(d, 1)$ 的数组与形状为 $(1, d)$ 的数组相加会产生一个形状为 $(d, d)$ 的数组。\n- 矩阵乘法规则：遵循标准的线性代数规则；不应用广播。\n\n**步骤 2：使用提取的已知条件进行验证**\n- **科学依据：** 该问题牢固地基于多层感知机的标准架构和前向传播算法，这些都是深度学习的核心概念。所描述的广播机制并非任意设定；它精确匹配了像 NumPy（TensorFlow 使用）和 PyTorch 这样的主流数值计算库的行为，使得该问题与机器学习工程实践高度相关。\n- **良构性：** 该问题是良构的。所有矩阵和向量的形状都已给出，运算有明确定义，处理形状不匹配（广播）的规则也已明确说明。这些条件足以唯一确定计算的结果。\n- **客观性：** 该问题以精确、客观的数学语言陈述，没有歧义或主观解释的余地。\n\n问题陈述是有效的。它描述了机器学习数值编程中一个常见且现实的错误场景，并要求根据一套清晰的计算规则对其后果进行逻辑推导。\n\n**步骤 3：结论与行动**\n问题有效。我将继续进行推导。\n\n### 推导与选项分析\n\n让我们一步步追踪前向传播过程，应用给定的规则和形状。\n\n1.  **$W^{(1)}x$ 的计算**：\n    - 权重矩阵 $W^{(1)}$ 的形状为 $(d, d_{\\mathrm{in}})$。\n    - 输入向量 $x$ 的形状为 $(d_{\\mathrm{in}}, 1)$。\n    - 矩阵-向量积 $W^{(1)}x$ 是一个标准的线性代数运算。内部维度（$d_{\\mathrm{in}}$ 和 $d_{\\mathrm{in}}$）匹配。\n    - 结果矩阵，我们称之为 $p$，将具有外部维度，即 $(d, 1)$。\n    - 所以，$p = W^{(1)}x \\in \\mathbb{R}^{d \\times 1}$。\n\n2.  **$z^{(1)} = W^{(1)}x + b^{(1)}$ 的计算**：\n    - 这是 $p \\in \\mathbb{R}^{d \\times 1}$ 与错误的偏置向量 $b^{(1)} \\in \\mathbb{R}^{1 \\times d}$ 的相加。\n    - 根据问题中的广播规则，这是一个按元素操作，其中形状会被变得兼容。\n    - 让我们对齐形状并从末尾开始比较维度：\n      - $p$ 的形状：$(d, 1)$\n      - $b^{(1)}$ 的形状：$(1, d)$\n    - 末尾的维度是 $1$ 和 $d$。它们是兼容的，结果维度是 $d$。\n    - 接下来的维度是 $d$ 和 $1$。它们也是兼容的，结果维度是 $d$。\n    - 因此，输出 $z^{(1)}$ 的形状是 $(d, d)$。\n    - 广播机制的工作方式如下：\n        - 矩阵 $p$（形状为 $(d, 1)$）沿其第二个维度复制 $d$ 次，成为一个 $(d, d)$ 矩阵。这个广播后的矩阵的每一列都是 $p$ 的一个副本。\n        - 矩阵 $b^{(1)}$（形状为 $(1, d)$）沿其第一个维度复制 $d$ 次，成为一个 $(d, d)$ 矩阵。这个广播后的矩阵的每一行都是 $b^{(1)}$ 的一个副本。\n    - 加法在这两个广播后的 $(d,d)$ 矩阵上按元素执行。结果 $z^{(1)}$ 在位置 $(i, j)$ 处的元素由来自广播后矩阵的相应元素之和给出。\n    - 设 $p_i$ 是列向量 $p = W^{(1)}x$ 的第 $i$ 个元素，设 $b_j$ 是行向量 $b^{(1)}$ 的第 $j$ 个元素。\n    - 因此元素 $z^{(1)}_{ij}$ 是 $p_i + b_j$。更正式地说，使用问题中的符号，$(W^{(1)}x)$ 是一个列向量，其第 $i$ 个元素表示为 $(W^{(1)}x)_{i1}$，而行向量 $b^{(1)}$ 的第 $j$ 个元素是 $b^{(1)}_{1j}$。\n    - 因此，$z^{(1)}_{ij} = (W^{(1)}x)_{i1} + b^{(1)}_{1j}$。结果矩阵是 $z^{(1)} \\in \\mathbb{R}^{d \\times d}$。\n\n3.  **$a^{(1)} = \\sigma(z^{(1)})$ 的计算**：\n    - 激活函数 $\\sigma(\\cdot)$ 是按元素施加的。\n    - 此操作不改变其输入矩阵的形状。\n    - 由于 $z^{(1)} \\in \\mathbb{R}^{d \\times d}$，输出 $a^{(1)}$ 也将在 $\\mathbb{R}^{d \\times d}$ 中。\n\n4.  **$z^{(2)} = W^{(2)}a^{(1)} + b^{(2)}$ 的计算**：\n    - 首先，我们计算矩阵积 $W^{(2)}a^{(1)}$。\n    - $W^{(2)}$ 的形状是 $(o, d)$。\n    - $a^{(1)}$ 的形状是 $(d, d)$。\n    - 由于内部维度（$d$ 和 $d$）匹配，乘法是有效的。\n    - 这个乘法的结果是一个形状为 $(o, d)$ 的矩阵。\n    - 接下来，我们将第二层的偏置 $b^{(2)} \\in \\mathbb{R}^{o \\times 1}$ 加到上一步得到的 $(o, d)$ 矩阵上。\n    - 同样，应用广播：\n      - 形状 1：$(o, d)$\n      - 形状 2：$(o, 1)$\n    - 末尾的维度是 $d$ 和 $1$。它们是兼容的，结果维度是 $d$。\n    - 接下来的维度是 $o$ 和 $o$。它们相等且兼容，结果维度是 $o$。\n    - 因此 $z^{(2)}$ 的最终形状是 $(o, d)$。\n    - 计算过程不会引发错误，但会产生形状为 $(o, d)$ 的输出 $z^{(2)}$，而不是正确的 $(o, 1)$。\n\n### 逐项分析\n\n- **A. $z^{(1)}$ 的计算会引发形状错误，因为 $\\left(d,1\\right)$ 和 $\\left(1,d\\right)$ 无法相加；在这种情况下无法进行广播。**\n  - **结论：错误。** 问题明确指出使用了实现广播的数值计算库。提示中给出的例子，“将形状为 $\\left(d,1\\right)$ 和 $\\left(1,d\\right)$ 的数组相加，会产生一个 $\\left(d,d\\right)$ 数组”，直接反驳了这个选项。计算不会失败；它会产生一个意外形状的结果。\n\n- **B. 计算通过广播继续进行，生成 $z^{(1)} \\in \\mathbb{R}^{d \\times d}$，其元素为 $z^{(1)}_{ij} = \\left(W^{(1)}x\\right)_{i1} + b^{(1)}_{1j}$；因此 $a^{(1)} \\in \\mathbb{R}^{d \\times d}$ 且 $z^{(2)} \\in \\mathbb{R}^{o \\times d}$，就好像处理了一个大小为 $d$ 的伪批次（pseudo-batch），其中每一列都有不同的偏置偏移量。**\n  - **结论：正确。** 这个选项准确地描述了上述推导的每一步。\n    - 它正确地指出计算通过广播继续进行。\n    - 它正确地识别出 $z^{(1)}$ 的结果形状为 $(d, d)$，即 $\\mathbb{R}^{d \\times d}$。\n    - 它正确地给出了 $z^{(1)}$ 元素的公式，该公式是由广播一个列向量和一个行向量得出的。\n    - 它正确地推断出 $a^{(1)}$ 的形状也将是 $(d,d)$。\n    - 它在与 $W^{(2)}$ 相乘并与 $b^{(2)}$ 进行广播加法后，正确地推断出 $z^{(2)}$ 的最终形状为 $(o, d)$。\n    - 最后的类比是对结果的一个合理解释：单个输入 $x$ 被有效地转换为 $d$ 个不同的激活向量（$a^{(1)}$ 的列），然后由第二层处理，这模仿了一个批处理过程，其中伪批次中的每个“项目”都收到了不同的偏置修正。\n\n- **C. 计算库会隐式地将 $b^{(1)}$ 转置为形状 $\\left(d,1\\right)$，因此 $z^{(1)} \\in \\mathbb{R}^{d \\times 1}$ 被正确计算，没有产生意外影响。**\n  - **结论：错误。** 这描述了一种假设的“智能”行为，与明确说明的广播规则不一致。规则是基于复制大小为 $1$ 的维度，而不是通过转置数组来强制维度一致。遵循给定规则会得到一个 $(d, d)$ 矩阵，而不是一个修正后的 $(d, 1)$ 向量。\n\n- **D. 计算库通过求和将 $b^{(1)}$ 折叠成一个标量来进行广播，因此 $z^{(1)} \\in \\mathbb{R}^{d \\times 1}$ 等于 $W^{(1)}x$ 加上一个加到所有隐藏单元上的单一标量偏移。**\n  - **结论：错误。** 这提出了一个完全不同的广播机制，涉及聚合（求和）和折叠维度。问题中描述的广播规则纯粹基于复制来扩展维度。没有发生求和或折叠。此外，这将导致形状为 $(d, 1)$，我们的分析表明这是错误的。\n\n- **E. 形状不匹配只影响 $z^{(1)}$ 的数值，不影响其形状；$z^{(1)} \\in \\mathbb{R}^{d \\times 1}$ 仍然是结果，其中每个条目都被 $d$ 个偏置分量的平均值所偏移。**\n  - **结论：错误。** 这个选项有两个错误的论断。首先，它声称 $z^{(1)}$ 的形状不变，而这正是发生的主要错误——形状从预期的 $(d, 1)$ 变为 $(d, d)$。其次，它提出了一个平均机制，这不属于问题中定义的标准广播规则的一部分。", "answer": "$$\\boxed{B}$$", "id": "3185351"}, {"introduction": "接下来，让我们从经典的多层感知机（MLP）组件转向驱动现代Transformer模型的核心引擎：自注意力机制。这个练习将通过一个具体而微小的例子，引导你一步步手动完成缩放点积注意力（Scaled Dot-Product Attention）$A=\\mathrm{softmax}(QK^\\top/\\sqrt{d_k})V$ 的完整前向传播计算。这有助于揭开注意力机制的神秘面纱，让你直观地理解序列中的不同部分是如何相互作用，从而生成富有上下文信息的表示。[@problem_id:3185352]", "problem": "考虑一个单头自注意力层的前向传播过程，该过程基于查询向量和键向量之间的点积，通过键维度 $d_k$ 的平方根进行缩放，逐行应用softmax函数以获得注意力权重，然后与值向量进行矩阵乘法以产生输出。使用以下包含三个词元（token）且键维度 $d_k=2$ 的小型合成示例。查询矩阵 $Q$、键矩阵 $K$ 和值矩阵 $V$ 均相等，并由下式给出\n$$\nQ=K=V=\\begin{pmatrix}\n1  0 \\\\\n0  1 \\\\\n1  0\n\\end{pmatrix}.\n$$\n您必须仅从基本定义出发：向量的点积、通过 $\\sqrt{d_k}$ 进行的平方根缩放、逐行应用由 $\\mathrm{softmax}(x)_i=\\frac{\\exp(x_i)}{\\sum_j \\exp(x_j)}$ 定义的softmax函数，以及标准矩阵乘法。在不使用任何快捷公式的情况下，计算此示例中缩放点积注意力 (SDPA) 的前向传播。通过分析 $Q$、$K$ 和 $V$ 的结构，确定由于对称性，输出的哪些行是相同的。然后，计算这些相同输出行的第一个分量的共同值。将您的最终数值结果四舍五入到四位有效数字，并以无量纲量的形式报告。", "solution": "该问题要求针对给定的一组输入矩阵 $Q$、$K$ 和 $V$，计算单头缩放点积注意力 (SDPA) 层的前向传播。最终输出应为从得到的注意力输出矩阵中导出的一个特定数值。该过程由以下公式决定：\n$$Z = \\mathrm{Attention}(Q, K, V) = \\mathrm{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n其中 $Z$ 是输出矩阵。我们被要求从基本定义出发，将计算分解为顺序步骤。\n\n给定的输入是：\n查询、键和值矩阵是相同的：\n$$Q=K=V=\\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 1 & 0 \\end{pmatrix}$$\n键的维度给定为 $d_k=2$。\n\n步骤 1：计算点积得分矩阵 $QK^T$。\n首先，我们求键矩阵 $K$ 的转置：\n$$K^T = \\begin{pmatrix} 1 & 0 & 1 \\\\ 0 & 1 & 0 \\end{pmatrix}$$\n接下来，我们执行矩阵乘法 $QK^T$：\n$$QK^T = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 1 & 0 \\end{pmatrix} \\begin{pmatrix} 1 & 0 & 1 \\\\ 0 & 1 & 0 \\end{pmatrix}$$\n这个乘积产生一个 $3 \\times 3$ 的注意力分数矩阵。其元素计算如下：\n$$(QK^T)_{11} = (1)(1) + (0)(0) = 1$$\n$$(QK^T)_{12} = (1)(0) + (0)(1) = 0$$\n$$(QK^T)_{13} = (1)(1) + (0)(0) = 1$$\n$$(QK^T)_{21} = (0)(1) + (1)(0) = 0$$\n$$(QK^T)_{22} = (0)(0) + (1)(1) = 1$$\n$$(QK^T)_{23} = (0)(1) + (1)(0) = 0$$\n$$(QK^T)_{31} = (1)(1) + (0)(0) = 1$$\n$$(QK^T)_{32} = (1)(0) + (0)(1) = 0$$\n$$(QK^T)_{33} = (1)(1) + (0)(0) = 1$$\n得到的分数矩阵是：\n$$S_{raw} = QK^T = \\begin{pmatrix} 1 & 0 & 1 \\\\ 0 & 1 & 0 \\\\ 1 & 0 & 1 \\end{pmatrix}$$\n\n步骤 2：用 $1/\\sqrt{d_k}$ 缩放分数矩阵。\n由于 $d_k=2$，缩放因子为 $1/\\sqrt{2}$。缩放后的分数矩阵，我们记为 $S$，是：\n$$S = \\frac{QK^T}{\\sqrt{d_k}} = \\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1 & 0 & 1 \\\\ 0 & 1 & 0 \\\\ 1 & 0 & 1 \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{\\sqrt{2}} & 0 & \\frac{1}{\\sqrt{2}} \\\\ 0 & \\frac{1}{\\sqrt{2}} & 0 \\\\ \\frac{1}{\\sqrt{2}} & 0 & \\frac{1}{\\sqrt{2}} \\end{pmatrix}$$\n\n步骤 3：通过对 $S$ 逐行应用softmax函数来计算注意力权重矩阵 $A_w$。\n向量 $x$ 的softmax函数定义为 $\\mathrm{softmax}(x)_i=\\frac{\\exp(x_i)}{\\sum_j \\exp(x_j)}$。\n\n对于 $S$ 的第一行，$s_1 = (\\frac{1}{\\sqrt{2}}, 0, \\frac{1}{\\sqrt{2}})$：\nsoftmax 的分母是 $\\sum_{j=1}^3 \\exp(s_{1j}) = \\exp(\\frac{1}{\\sqrt{2}}) + \\exp(0) + \\exp(\\frac{1}{\\sqrt{2}}) = 2\\exp(\\frac{1}{\\sqrt{2}}) + 1$。\n$A_w$ 第一行的分量是：\n$$A_{w,11} = \\frac{\\exp(\\frac{1}{\\sqrt{2}})}{2\\exp(\\frac{1}{\\sqrt{2}}) + 1}, \\quad A_{w,12} = \\frac{\\exp(0)}{2\\exp(\\frac{1}{\\sqrt{2}}) + 1} = \\frac{1}{2\\exp(\\frac{1}{\\sqrt{2}}) + 1}, \\quad A_{w,13} = \\frac{\\exp(\\frac{1}{\\sqrt{2}})}{2\\exp(\\frac{1}{\\sqrt{2}}) + 1}$$\n\n对于 $S$ 的第二行，$s_2 = (0, \\frac{1}{\\sqrt{2}}, 0)$：\nsoftmax 的分母是 $\\sum_{j=1}^3 \\exp(s_{2j}) = \\exp(0) + \\exp(\\frac{1}{\\sqrt{2}}) + \\exp(0) = 2 + \\exp(\\frac{1}{\\sqrt{2}})$。\n$A_w$ 第二行的分量是：\n$$A_{w,21} = \\frac{1}{2 + \\exp(\\frac{1}{\\sqrt{2}})}, \\quad A_{w,22} = \\frac{\\exp(\\frac{1}{\\sqrt{2}})}{2 + \\exp(\\frac{1}{\\sqrt{2}})}, \\quad A_{w,23} = \\frac{1}{2 + \\exp(\\frac{1}{\\sqrt{2}})}$$\n\n$S$ 的第三行 $s_3 = (\\frac{1}{\\sqrt{2}}, 0, \\frac{1}{\\sqrt{2}})$ 与第一行 $s_1$ 相同。因此，注意力矩阵 $A_w$ 的第三行与其第一行相同：对于 $j \\in \\{1, 2, 3\\}$，有 $A_{w,3j} = A_{w,1j}$。\n\n步骤 4：计算最终输出矩阵 $Z = A_w V$。\n$$Z = A_w V = \\begin{pmatrix} A_{w,11} & A_{w,12} & A_{w,13} \\\\ A_{w,21} & A_{w,22} & A_{w,23} \\\\ A_{w,31} & A_{w,32} & A_{w,33} \\end{pmatrix} \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 1 & 0 \\end{pmatrix}$$\n输出矩阵 $Z$ 的行是 $V$ 的行的线性组合。设 $v_1, v_2, v_3$ 为 $V$ 的行，则 $v_1 = (1, 0)$, $v_2 = (0, 1)$, $v_3 = (1, 0)$。注意 $v_1=v_3$。\n\n问题要求识别由于对称性而相同的输出行。输入查询向量为 $q_1=(1,0)$、$q_2=(0,1)$ 和 $q_3=(1,0)$。因为 $q_1=q_3$ 且键矩阵 $K$ 是共享的，所以第一个和第三个查询的注意力分数将是相同的。如步骤 2 所述，缩放后的分数矩阵 $S$ 的第一行和第三行是相同的。这导致注意力权重矩阵 $A_w$ 的第一行和第三行相同，如步骤 3 所示。\n输出 $Z$ 的第一行和第三行是：\n$$Z_1 = A_{w,11}v_1 + A_{w,12}v_2 + A_{w,13}v_3$$\n$$Z_3 = A_{w,31}v_1 + A_{w,32}v_2 + A_{w,33}v_3$$\n由于对所有 $j$ 都有 $A_{w,1j} = A_{w,3j}$，我们得到 $Z_1 = Z_3$。输出的第一行和第三行是相同的。\n\n问题接着要求计算这些相同输出行的第一个分量的共同值。我们将计算第一个输出行 $Z_{11}$ 的第一个分量。\n$$Z_1 = (Z_{11}, Z_{12}) = A_{w,11}(1, 0) + A_{w,12}(0, 1) + A_{w,13}(1, 0) = (A_{w,11} + A_{w,13}, A_{w,12})$$\n第一个分量是 $Z_{11} = A_{w,11} + A_{w,13}$。从步骤 3 中，我们知道 $A_{w,11} = A_{w,13}$。因此：\n$$Z_{11} = 2A_{w,11} = 2 \\left( \\frac{\\exp(\\frac{1}{\\sqrt{2}})}{2\\exp(\\frac{1}{\\sqrt{2}}) + 1} \\right) = \\frac{2\\exp(\\frac{1}{\\sqrt{2}})}{2\\exp(\\frac{1}{\\sqrt{2}}) + 1}$$\n\n最后，我们计算数值并将其四舍五入到四位有效数字。\n指数的值为 $\\frac{1}{\\sqrt{2}} \\approx 0.70710678$。\n指数项为 $\\exp(\\frac{1}{\\sqrt{2}}) \\approx 2.02813039$。\n将此代入 $Z_{11}$ 的表达式中：\n$$Z_{11} = \\frac{2 \\times 2.02813039}{2 \\times 2.02813039 + 1} = \\frac{4.05626078}{5.05626078} \\approx 0.80218349$$\n将此结果四舍五入到四位有效数字得到 $0.8022$。", "answer": "$$\\boxed{0.8022}$$", "id": "3185352"}]}