{"hands_on_practices": [{"introduction": "理论学习之后，最好的检验方法莫过于亲手实践。本练习将指导你从零开始构建一个微型自动微分（AD）引擎。通过实现核心的反向传播逻辑，你将揭开深度学习框架背后梯度计算的神秘面纱，并掌握使用数值梯度检验来验证其正确性的关键调试技巧。[@problem_id:3107983]", "problem": "您需要使用计算图实现一个最小化的反向模式自动微分（AD）引擎，并通过将解析导数与数值中心差分进行比较来验证梯度。所使用的基本原理是计算图的定义和来自微积分的链式法则。具体来说，将一个已知的函数节点 $f(x)=x^2$ 注入一个标量模型图中，并验证在相邻节点上的梯度传播是否正确。\n\n使用标准数学运算和函数，构建一个包含输入 $x$ 以及常量 $w$ 和 $c$ 的标量计算图，如下所示：$f(x)=x^2$，$u(x)=\\sin(f(x))$，$v(x)=w\\cdot f(x)+c$ 和 $y(x)=u(x)+\\exp(v(x))$。三角函数 $\\sin$ 必须以弧度为单位进行计算。您的 AD 引擎必须通过反向模式计算以下四个节点输出相对于 $x$ 的解析导数：$f(x)$、$u(x)$、$v(x)$ 和 $y(x)$。另外，使用步长 $h=10^{-6}$ 为这四个标量函数中的每一个计算数值中心差分近似值：\n$$\n\\frac{d}{dx} s(x) \\approx \\frac{s(x+h) - s(x-h)}{2h}.\n$$\n\n使用绝对容差 $\\tau_{\\mathrm{abs}}=10^{-7}$ 和相对容差 $\\tau_{\\mathrm{rel}}=10^{-7}$ 比较解析导数与数值导数。如果在这些容差范围内，两个值足够接近，则单个导数的比较被认为是可接受的。在下方的测试套件中，对每个测试值 $x$，为四个节点输出 $f(x)$、$u(x)$、$v(x)$ 和 $y(x)$ 中的每一个执行此比较。\n\n在整个过程中使用以下固定常量：$w=10^{-6}$ 和 $c=0.7$。在以下输入值测试套件上评估程序：$x\\in\\{-3.0,\\,-10^{-8},\\,0.0,\\,0.5,\\,1000.0\\}$。\n\n您的程序必须是一个完整、可运行的程序，并且能够：\n- 构建所述的计算图。\n- 在该图上实现反向模式 AD，以计算 $f(x)$、$u(x)$、$v(x)$ 和 $y(x)$ 相对于 $x$ 的解析导数。\n- 为相同的函数计算数值中心差分导数。\n- 使用指定的容差比较解析导数与数值导数，并为每个 $x$ 处的每次比较生成一个布尔值。\n\n最终输出格式：\n- 程序应生成单行输出，其中包含一个由方括号括起来的列表的逗号分隔列表。每个内部列表按给定顺序对应一个测试输入 $x$，并按 $\\big[f,u,v,y\\big]$ 的顺序包含四个布尔值，指示梯度是否在容差范围内匹配。例如，格式应类似于 $[[\\text{True},\\text{True},\\text{True},\\text{True}],[\\text{True},\\text{False},\\text{True},\\text{True}],\\dots]$，不含空格。\n- 此问题不涉及物理单位，且如上所述，角度必须以弧度为单位。\n\n不允许用户输入；程序必须按原样运行，并严格按照指定格式打印一行输出。", "solution": "用户提供的问题已经过分析并被确定为 **有效**。这是一个在数值计算和自动微分领域内定义良好、有科学依据的问题，所有必要的参数和条件都已指明。\n\n任务是为一个标量计算图实现一个反向模式自动微分（AD）引擎，并用数值微分来验证其结果。解决方案分以下几个步骤进行：\n\n1.  **计算图表示**：解决方案的核心是一个 `Var` 类，它代表计算图中的一个节点。每个 `Var` 对象存储一个标量 `value` 和它的 `grad`（梯度），该梯度是最终输出相对于此节点值的导数。通过重载标准的 Python 运算符（`+`、`*`、`**`）和为数学函数（`sin`、`exp`）定义方法，可以动态地构建图。当对 `Var` 对象执行操作时，会创建一个新的 `Var` 对象，并将其操作结果存储在 `value` 中。至关重要的是，这个新节点还保留了对其父节点的引用以及一个 `_backward` 函数，该函数知道如何根据链式法则将其梯度传播回其父节点。\n\n2.  **前向传播**：对于给定的输入值 $x$，我们将图的叶节点 $x$、$w$ 和 $c$ 实例化为 `Var` 对象。\n    -   $x_{val} \\in \\{-3.0, -10^{-8}, 0.0, 0.5, 1000.0\\}$\n    -   $w = 10^{-6}$\n    -   $c = 0.7$\n    然后通过应用指定的操作序列来构建图：\n    -   $f = x^2$\n    -   $u = \\sin(f)$\n    -   $v = w \\cdot f + c$\n    -   $y = u + \\exp(v)$\n    每个节点的创建都会计算其 `value`，这实际上是在执行一次图的前向传播。\n\n3.  **反向传播**：反向模式 AD 通过从输出节点向后传播梯度来计算梯度。为了找到节点 $s$ 相对于输入 $x$ 的导数（表示为 $\\frac{ds}{dx}$），我们从 $s$ 开始启动一次反向传播。对于每个感兴趣的节点 $s \\in \\{f, u, v, y\\}$，这是通过以下算法实现的：\n    a.  **拓扑排序**：对计算图进行拓扑排序，从最终节点 $y$ 开始，向后遍历其所有祖先节点。这确保了在反向传播时，一个节点的梯度在其被用于计算其父节点的梯度之前已经完全计算好。\n    b.  **梯度重置**：图中所有节点的 `grad` 属性被重置为 $0.0$。\n    c.  **设定种子**：我们想要计算其导数的节点 $s$ 的梯度被设置为 $1.0$。这是基准情况，因为 $\\frac{ds}{ds} = 1$。\n    d.  **传播**：我们以反向拓扑顺序遍历节点。对每个节点，调用其 `_backward` 方法。该方法使用节点自身的 `grad` 和其父节点的 `value`，根据局部偏导数（链式法则）来更新父节点的 `grad`。例如，对于一个节点 $z = \\text{op}(a, b)$，`_backward` 调用将执行 `a.grad += z.grad * \\frac{\\partial z}{\\partial a}` 和 `b.grad += z.grad * \\frac{\\partial z}{\\partial b}`。\n    e.  **结果**：反向传播完成后，输入节点 `x` 的 `grad` 属性将包含最终所需的导数 $\\frac{ds}{dx}$。对 $f, u, v,$ 和 $y$ 重复此过程。\n\n4.  **数值微分**：为了验证，对每个函数 $s(x) \\in \\{f(x), u(x), v(x), y(x)\\}$ 使用中心差分公式来近似其导数：\n    $$\n    \\frac{d}{dx} s(x) \\approx \\frac{s(x+h) - s(x-h)}{2h}\n    $$\n    这是使用一个小的步长 $h = 10^{-6}$ 计算的。\n\n5.  **比较**：将来自 AD 的解析梯度与数值近似值进行比较。如果绝对差在由绝对容差 $\\tau_{\\mathrm{abs}} = 10^{-7}$ 和相对容差 $\\tau_{\\mathrm{rel}} = 10^{-7}$ 定义的容差范围内，则确认匹配。检查公式为：\n    $$\n    |\\text{analytic} - \\text{numerical}| \\le (\\tau_{\\mathrm{abs}} + \\tau_{\\mathrm{rel}} \\cdot |\\text{numerical}|)\n    $$\n    `numpy.isclose` 函数直接实现了这一逻辑。对于每个测试值 $x$，会生成四个布尔结果，分别对应于 $\\frac{df}{dx}, \\frac{du}{dx}, \\frac{dv}{dx},$ 和 $\\frac{dy}{dx}$ 的比较。\n\n6.  **最终输出**：将布尔结果收集到列表中，每个测试值 $x$ 对应一个内部列表。然后将这些列表格式化为问题指定的单个字符串，例如 `[[True,True,True,True],[...]]`，并打印到标准输出。", "answer": "```python\nimport numpy as np\n\nclass Var:\n    \"\"\"\n    A variable class for reverse-mode automatic differentiation.\n    Each Var instance represents a node in the computational graph, holding a scalar\n    value and its gradient with respect to the final output.\n    \"\"\"\n\n    def __init__(self, value, _parents=()):\n        self.value = float(value)\n        self.grad = 0.0\n        # A function to propagate gradient to parents\n        self._backward = lambda: None\n        # The set of parent nodes that created this node\n        self._parents = set(_parents)\n\n    def __repr__(self):\n        return f\"Var(value={self.value}, grad={self.grad})\"\n\n    def __add__(self, other):\n        other = other if isinstance(other, Var) else Var(other)\n        out = Var(self.value + other.value, (self, other))\n\n        def _backward():\n            self.grad += out.grad\n            other.grad += out.grad\n        out._backward = _backward\n        return out\n\n    def __radd__(self, other):\n        return self + other\n\n    def __mul__(self, other):\n        other = other if isinstance(other, Var) else Var(other)\n        out = Var(self.value * other.value, (self, other))\n\n        def _backward():\n            self.grad += other.value * out.grad\n            other.grad += self.value * out.grad\n        out._backward = _backward\n        return out\n\n    def __rmul__(self, other):\n        return self * other\n\n    def __pow__(self, p):\n        assert isinstance(p, (int, float)), \"Only supporting scalar powers\"\n        out = Var(self.value ** p, (self,))\n\n        def _backward():\n            self.grad += p * (self.value ** (p - 1)) * out.grad\n        out._backward = _backward\n        return out\n\n    def sin(self):\n        out = Var(np.sin(self.value), (self,))\n\n        def _backward():\n            self.grad += np.cos(self.value) * out.grad\n        out._backward = _backward\n        return out\n\n    def exp(self):\n        out = Var(np.exp(self.value), (self,))\n\n        def _backward():\n            self.grad += np.exp(self.value) * out.grad\n        out._backward = _backward\n        return out\n\ndef solve():\n    \"\"\"\n    Main function to run the AD engine and validation.\n    \"\"\"\n    # Define fixed constants and test parameters\n    W_VAL = 1e-6\n    C_VAL = 0.7\n    H = 1e-6\n    TAU_ABS = 1e-7\n    TAU_REL = 1e-7\n    TEST_CASES = [-3.0, -10e-9, 0.0, 0.5, 1000.0]\n\n    # Define the scalar functions for numerical differentiation\n    def s_f(x_val): return x_val**2\n    def s_u(x_val): return np.sin(s_f(x_val))\n    def s_v(x_val): return W_VAL * s_f(x_val) + C_VAL\n    def s_y(x_val): return s_u(x_val) + np.exp(s_v(x_val))\n    \n    scalar_funcs = [s_f, s_u, s_v, s_y]\n    \n    all_case_results = []\n    \n    for x_val in TEST_CASES:\n        # ---- Analytic Derivatives via Reverse-Mode AD ----\n\n        # 1. Build the computational graph (forward pass)\n        x = Var(x_val)\n        w = Var(W_VAL)\n        c = Var(C_VAL)\n        \n        f = x**2\n        u = f.sin()\n        v = w * f + c\n        y = u + v.exp()\n        \n        nodes_of_interest = [f, u, v, y]\n\n        # 2. Prepare for backward pass: Get all nodes via topological sort\n        topo_order = []\n        visited = set()\n        def build_topo(node):\n            if node not in visited:\n                visited.add(node)\n                for parent in node._parents:\n                    build_topo(parent)\n                topo_order.append(node)\n        \n        build_topo(y)\n        all_graph_nodes = visited\n\n        analytic_grads = []\n        for node_to_diff in nodes_of_interest:\n            # 3. Reset gradients for all nodes in the graph\n            for node in all_graph_nodes:\n                node.grad = 0.0\n            \n            # 4. Seed the gradient of the node we're differentiating\n            node_to_diff.grad = 1.0\n            \n            # 5. Propagate gradients backward through the sorted graph\n            for node in reversed(topo_order):\n                node._backward()\n            \n            # 6. The gradient is now in the input variable `x`\n            analytic_grads.append(x.grad)\n\n        # ---- Numerical Derivatives via Central Differences ----\n        numerical_grads = []\n        for func in scalar_funcs:\n            grad = (func(x_val + H) - func(x_val - H)) / (2 * H)\n            numerical_grads.append(grad)\n            \n        # ---- Comparison ----\n        current_case_results = []\n        for a_grad, n_grad in zip(analytic_grads, numerical_grads):\n            is_match = np.isclose(a_grad, n_grad, rtol=TAU_REL, atol=TAU_ABS)\n            current_case_results.append(bool(is_match))\n        \n        all_case_results.append(current_case_results)\n\n    # Format the final output string\n    inner_strings = [f\"[{','.join(map(str, res))}]\" for res in all_case_results]\n    final_output = f\"[{','.join(inner_strings)}]\"\n    print(final_output)\n\nsolve()\n```", "id": "3107983"}, {"introduction": "自动微分的反向传播算法在本质上是链式法则的递归应用，但其计算结果有一个非常直观的物理解释。本练习将引导你将总梯度分解为计算图中从输入到输出的所有独立路径上的梯度贡献之和。这种“路径求和”的视角有助于加深对梯度如何在复杂网络中分配和流动的理解。[@problem_id:3108040]", "problem": "考虑一个以弧度为单位的标量输入 $x$ 流经一个作为有向无环图 (DAG) 的计算图。节点计算以下确定性变换：\n$$a = x^{2}, \\quad b = \\sin(x), \\quad c = a \\cdot b, \\quad d = \\exp(a), \\quad y = c + d, \\quad L = \\ln\\!\\big(1 + y\\big).$$\n假设对于所考虑的 $x$ 值，所有函数都是可微的。仅使用链式法则和计算图的定义（其中梯度通过局部偏导数沿边传播），研究此 DAG 上从 $L$ 回到 $x$ 的线性化梯度传播。\n\n任务：\n- 在通过将局部导数因子放置在每条边上而得到的线性化梯度图中，枚举从 $x$ 到 $L$ 的所有不同简单有向路径。计算此类路径的数量 $N$。\n- 通过对所有路径的贡献求和，推导出总梯度 $\\frac{\\partial L}{\\partial x}$ 的精确符号表达式，其中每条路径的贡献是其局部导数因子的乘积。\n- 以 $x$ 的形式给出 $\\frac{\\partial L}{\\partial x}$ 的最终简化表达式。\n\n将最终答案以一个行矩阵的形式给出，其中包含 $N$ 和 $\\frac{\\partial L}{\\partial x}$ 的精确符号表达式。无需四舍五入。角度以弧度为单位，不涉及物理单位。除了链式法则和计算图及反向传播（梯度反向传播）的基本定义外，不要使用任何快捷公式。", "solution": "首先对问题陈述进行严格的验证过程。\n\n### 步骤 1：提取给定信息\n提供的数据和定义如下：\n- 标量输入: $x$\n- 计算图节点和变换：\n  - $a = x^{2}$\n  - $b = \\sin(x)$\n  - $c = a \\cdot b$\n  - $d = \\exp(a)$\n  - $y = c + d$\n  - $L = \\ln(1 + y)$\n- 假设：对于所考虑的 $x$ 值，所有函数都是可微的。\n- 任务：\n  1. 枚举从 $x$ 到 $L$ 的所有不同简单有向路径并计数 ($N$)。\n  2. 通过对路径贡献求和，推导出 $\\frac{\\partial L}{\\partial x}$ 的符号表达式。\n  3. 以 $x$ 的形式给出 $\\frac{\\partial L}{\\partial x}$ 的简化表达式。\n- 最终答案格式：一个包含 $N$ 和 $\\frac{\\partial L}{\\partial x}$ 的符号表达式的行矩阵。\n\n### 步骤 2：使用提取的给定信息进行验证\n根据验证标准对问题进行分析。\n- **科学依据：** 该问题基于微积分的基本原理（链式法则）及其在自动微分（特别是反向模式自动微分或反向传播）中的应用，这是深度学习和计算科学中的一个核心概念。所有函数都是标准的数学函数。该问题在科学上是合理的。\n- **适定性：** 计算图是一个有向无环图 (DAG)，每个节点都有定义良好且可微的函数。这种结构保证了唯一的导数 $\\frac{\\partial L}{\\partial x}$ 存在并且可以被计算。该问题是适定的。\n- **客观性：** 问题使用精确的数学语言和符号陈述，没有任何主观性或歧义。\n- 该问题没有表现出任何诸如科学不合理、不完整、矛盾或依赖于定义不明确的术语等缺陷。它是指定领域内的一个标准的、可形式化的练习。\n\n### 步骤 3：结论与行动\n该问题是**有效的**。将提供完整的解决方案。\n\n### 解题推导\n该问题要求我们通过将计算概念化为一个图，并沿所有从输入 $x$ 到输出 $L$ 的路径应用链式法则，来计算总导数 $\\frac{\\partial L}{\\partial x}$。\n\n**任务 1：路径枚举**\n\n首先，我们根据给定的依赖关系确定计算图的结构。这些依赖关系定义了图的有向边，其中一条边 $(u, v)$ 意味着 $v$ 是由 $u$ 计算得出的。\n- $x \\rightarrow a$\n- $x \\rightarrow b$\n- $a \\rightarrow c$\n- $b \\rightarrow c$\n- $a \\rightarrow d$\n- $c \\rightarrow y$\n- $d \\rightarrow y$\n- $y \\rightarrow L$\n\n我们必须找到从输入节点 $x$ 到最终输出节点 $L$ 的所有不同的简单有向路径。通过追踪从 $x$ 到 $L$ 的连接：\n1. 一条路径可以从 $x$ 到 $a$。从 $a$ 出发，它可以继续到 $c$ 或 $d$。\n   - 如果它继续到 $c$，路径必须接着到 $y$，最后到 $L$。这给出了路径：$x \\rightarrow a \\rightarrow c \\rightarrow y \\rightarrow L$。\n   - 如果它继续到 $d$，路径必须接着到 $y$，最后到 $L$。这给出了路径：$x \\rightarrow a \\rightarrow d \\rightarrow y \\rightarrow L$。\n2. 一条路径可以从 $x$ 到 $b$。从 $b$ 出发，它只能继续到 $c$。从 $c$ 出发，它必须到 $y$，然后到 $L$。这给出了路径：$x \\rightarrow b \\rightarrow c \\rightarrow y \\rightarrow L$。\n\n这些是从 $x$到 $L$ 的三条唯一的简单有向路径。\n因此，此类路径的数量为 $N = 3$。\n\n**任务 2：总梯度推导**\n\n总导数 $\\frac{\\partial L}{\\partial x}$ 是这 3 条路径各自贡献的总和。单条路径的贡献是沿其边的局部偏导数的乘积。\n\n首先，我们计算所有必要的局部偏导数：\n- $\\frac{\\partial L}{\\partial y} = \\frac{\\partial}{\\partial y} \\ln(1 + y) = \\frac{1}{1 + y}$\n- $\\frac{\\partial y}{\\partial c} = \\frac{\\partial}{\\partial c} (c + d) = 1$\n- $\\frac{\\partial y}{\\partial d} = \\frac{\\partial}{\\partial d} (c + d) = 1$\n- $\\frac{\\partial c}{\\partial a} = \\frac{\\partial}{\\partial a} (a \\cdot b) = b$\n- $\\frac{\\partial c}{\\partial b} = \\frac{\\partial}{\\partial b} (a \\cdot b) = a$\n- $\\frac{\\partial d}{\\partial a} = \\frac{\\partial}{\\partial a} \\exp(a) = \\exp(a)$\n- $\\frac{\\partial a}{\\partial x} = \\frac{\\partial}{\\partial x} (x^{2}) = 2x$\n- $\\frac{\\partial b}{\\partial x} = \\frac{\\partial}{\\partial x} (\\sin(x)) = \\cos(x)$\n\n接下来，我们通过将沿路径的导数相乘来计算每条路径的贡献：\n- **路径 1 贡献 ($\\mathcal{P}_1$):** $x \\rightarrow a \\rightarrow c \\rightarrow y \\rightarrow L$\n$$ \\mathcal{P}_1 = \\frac{\\partial L}{\\partial y} \\frac{\\partial y}{\\partial c} \\frac{\\partial c}{\\partial a} \\frac{\\partial a}{\\partial x} = \\left(\\frac{1}{1 + y}\\right) \\cdot (1) \\cdot (b) \\cdot (2x) = \\frac{2xb}{1 + y} $$\n- **路径 2 贡献 ($\\mathcal{P}_2$):** $x \\rightarrow b \\rightarrow c \\rightarrow y \\rightarrow L$\n$$ \\mathcal{P}_2 = \\frac{\\partial L}{\\partial y} \\frac{\\partial y}{\\partial c} \\frac{\\partial c}{\\partial b} \\frac{\\partial b}{\\partial x} = \\left(\\frac{1}{1 + y}\\right) \\cdot (1) \\cdot (a) \\cdot (\\cos(x)) = \\frac{a \\cos(x)}{1 + y} $$\n- **路径 3 贡献 ($\\mathcal{P}_3$):** $x \\rightarrow a \\rightarrow d \\rightarrow y \\rightarrow L$\n$$ \\mathcal{P}_3 = \\frac{\\partial L}{\\partial y} \\frac{\\partial y}{\\partial d} \\frac{\\partial d}{\\partial a} \\frac{\\partial a}{\\partial x} = \\left(\\frac{1}{1 + y}\\right) \\cdot (1) \\cdot (\\exp(a)) \\cdot (2x) = \\frac{2x \\exp(a)}{1 + y} $$\n\n总导数是这些路径贡献的总和：\n$$ \\frac{\\partial L}{\\partial x} = \\mathcal{P}_1 + \\mathcal{P}_2 + \\mathcal{P}_3 = \\frac{2xb}{1 + y} + \\frac{a \\cos(x)}{1 + y} + \\frac{2x \\exp(a)}{1 + y} $$\n$$ \\frac{\\partial L}{\\partial x} = \\frac{2xb + a \\cos(x) + 2x \\exp(a)}{1 + y} $$\n\n**任务 3：最终简化表达式**\n\n为了获得以 $x$ 表示的最终表达式，我们将中间变量（$a, b, y$）的定义代入 $\\frac{\\partial L}{\\partial x}$ 的表达式中。\n- $a = x^{2}$\n- $b = \\sin(x)$\n- $y = c + d = (a \\cdot b) + \\exp(a) = x^{2}\\sin(x) + \\exp(x^{2})$\n\n将这些代入分子：\n$$ 2xb + a \\cos(x) + 2x \\exp(a) = 2x(\\sin(x)) + (x^{2})\\cos(x) + 2x \\exp(x^{2}) $$\n代入分母：\n$$ 1 + y = 1 + x^{2}\\sin(x) + \\exp(x^{2}) $$\n\n将这些组合起来，得到总梯度的最终符号表达式：\n$$ \\frac{\\partial L}{\\partial x} = \\frac{2x \\sin(x) + x^{2} \\cos(x) + 2x \\exp(x^{2})}{1 + x^{2} \\sin(x) + \\exp(x^{2})} $$\n该表达式无法以有意义的方式进一步简化。\n\n所要求的两个结果是 $N=3$ 和 $\\frac{\\partial L}{\\partial x}$ 的表达式。", "answer": "$$ \\boxed{ \\begin{pmatrix} 3 & \\frac{2x \\sin(x) + x^{2} \\cos(x) + 2x \\exp(x^{2})}{1 + x^{2} \\sin(x) + \\exp(x^{2})} \\end{pmatrix} } $$", "id": "3108040"}, {"introduction": "在实际应用中，一个理论上正确的计算图可能因为数值问题而彻底失效。本练习将通过一个经典的例子——`softplus`函数，展示两种数学上等价但数值稳定性截然不同的计算图。通过分析和推导，你将理解为何以及如何重构计算图以避免浮点数溢出，这是保证模型训练稳定性的重要实践技能。[@problem_id:3108012]", "problem": "考虑由计算图表示的函数 $f(x)=\\ln\\!\\big(1+\\exp(x)\\big)$，其中计算图是一个有向无环图，其节点是基本操作，其边承载中间值。反向模式自动微分 (AD) 是一种算法，它在这样的图上应用微积分的链式法则，将敏感度从输出反向传播到输入。您将分析 $f(x)$ 的两个图：一个朴素图和一个数值稳定图。所有推导必须从链式法则和所涉及的基本函数的基本性质开始。\n\n朴素图：\n1. $a=\\exp(x)$，\n2. $b=1+a$，\n3. $y=\\ln(b)$。\n\n稳定图使用了一种重参数化方法，以避免当 $x$ 为大的正数时发生溢出：\n1. $m=\\max(x,0)$，\n2. $s=-|x|$，\n3. $c=\\exp(s)$，\n4. $d=\\ln(1+c)$，\n5. $y=m+d$。\n\n任务：\n1. 使用反向模式自动微分 (AD)，通过链式法则组合局部导数，从朴素图中推导出 $\\frac{dy}{dx}$ 的符号表达式。将您的结果完全用 $x$ 表示。\n2. 使用反向模式自动微分 (AD)，从稳定图中推导出 $\\frac{dy}{dx}$ 的符号表达式。由于 $m=\\max(x,0)$ 和 $|x|$ 是分段定义的，请分别对 $x\\ge 0$ 和 $x  0$ 进行推导，然后简化为一个对所有实数 $x$ 都有效的单一解析表达式。\n3. 通过检查每个图上的局部导数，简要解释为什么稳定图相对于朴素图，在不改变精确梯度的情况下，能够减轻大 $|x|$ 时的数值上溢或下溢。\n4. 提供您最终简化的梯度表达式 $\\frac{dy}{dx}$，作为一个单一的封闭形式解析表达式。这是唯一会被评分的项目。不需要数值四舍五入，也不适用任何物理单位。", "solution": "该问题已经过验证，被认为是数值计算和自动微分领域中一个有效、自洽且适定的问题。它在科学上基于已建立的数学原理，并展示了一种确保数值稳定性的标准技术，即 log-sum-exp 技巧。\n\n我们考虑的函数是 $f(x) = \\ln(1+\\exp(x))$。我们的任务是使用两种不同的计算图推导其导数 $\\frac{dy}{dx}$，并解释第二种图的数值优势。在反向模式自动微分 (AD) 的背景下，我们使用符号 $\\bar{v}$ 来表示最终输出 $y$ 对中间变量 $v$ 的导数，即 $\\bar{v} = \\frac{dy}{dv}$。该过程从 $\\bar{y}=1$ 开始，并使用链式法则通过图向后传播梯度。\n\n**任务1：从朴素图中推导导数**\n\n朴素计算图由以下操作序列定义：\n1. $a = \\exp(x)$\n2. $b = 1 + a$\n3. $y = \\ln(b)$\n\n我们应用反向模式 AD 来求 $\\frac{dy}{dx} = \\bar{x}$。\n\n1.  初始梯度为 $\\bar{y} = \\frac{dy}{dy} = 1$。\n2.  将梯度从 $y$ 传播到 $b$。局部导数为 $\\frac{dy}{db} = \\frac{d}{db}(\\ln(b)) = \\frac{1}{b}$。$b$ 的伴随值是：\n    $$ \\bar{b} = \\bar{y} \\frac{dy}{db} = 1 \\cdot \\frac{1}{b} = \\frac{1}{b} $$\n3.  将梯度从 $b$ 传播到 $a$。局部导数为 $\\frac{db}{da} = \\frac{d}{da}(1+a) = 1$。$a$ 的伴随值是：\n    $$ \\bar{a} = \\bar{b} \\frac{db}{da} = \\frac{1}{b} \\cdot 1 = \\frac{1}{b} $$\n4.  将梯度从 $a$ 传播到 $x$。局部导数为 $\\frac{da}{dx} = \\frac{d}{dx}(\\exp(x)) = \\exp(x)$。$x$ 的伴随值是：\n    $$ \\bar{x} = \\bar{a} \\frac{da}{dx} = \\frac{1}{b} \\cdot \\exp(x) $$\n5.  为了用 $x$ 表示最终结果，我们回代中间变量：$b = 1+a = 1+\\exp(x)$。\n    $$ \\frac{dy}{dx} = \\bar{x} = \\frac{\\exp(x)}{1+\\exp(x)} $$\n\n**任务2：从稳定图中推导导数**\n\n稳定图由以下公式给出：\n1. $m = \\max(x, 0)$\n2. $s = -|x|$\n3. $c = \\exp(s)$\n4. $d = \\ln(1+c)$\n5. $y = m + d$\n\n首先，我们验证这个公式在数学上等价于原始函数 $f(x) = \\ln(1+\\exp(x))$。\n对于 $x \\ge 0$：$|x|=x$，因此 $m=x$ 且 $s=-x$。那么 $y = x + \\ln(1+\\exp(-x)) = \\ln(\\exp(x)) + \\ln(1+\\exp(-x)) = \\ln(\\exp(x)(1+\\exp(-x))) = \\ln(\\exp(x)+1)$。\n对于 $x  0$：$|x|=-x$，因此 $m=0$ 且 $s=x$。那么 $y = 0 + \\ln(1+\\exp(x)) = \\ln(1+\\exp(x))$。\n该公式对所有 $x \\in \\mathbb{R}$ 确实是正确的。\n\n我们通过考虑分段函数 $m(x)$ 和 $s(x)$ 的两种情况来推导 $\\frac{dy}{dx}$。一般地，在图结构 $y=m(x)+d(c(s(x)))$ 上使用链式法则，我们有：\n$$ \\frac{dy}{dx} = \\frac{dm}{dx} + \\frac{dd}{dc} \\frac{dc}{ds} \\frac{ds}{dx} $$\n局部导数为：\n$\\frac{dd}{dc} = \\frac{1}{1+c}$\n$\\frac{dc}{ds} = \\exp(s)$\n\n情况1：$x > 0$\n对于 $x > 0$，我们有 $m(x)=x$ 和 $s(x)=-|x|=-x$。关于 $x$ 的导数是：\n$\\frac{dm}{dx} = 1$\n$\\frac{ds}{dx} = -1$\n将这些代入链式法则表达式中：\n$$ \\frac{dy}{dx} = 1 + \\left(\\frac{1}{1+c}\\right)(\\exp(s))(-1) = 1 - \\frac{\\exp(s)}{1+c} $$\n现在，我们用 $x$ 来表示它。对于 $x > 0$，$s=-x$ 且 $c=\\exp(-x)$。\n$$ \\frac{dy}{dx} = 1 - \\frac{\\exp(-x)}{1+\\exp(-x)} = \\frac{(1+\\exp(-x)) - \\exp(-x)}{1+\\exp(-x)} = \\frac{1}{1+\\exp(-x)} $$\n为了证明这与之前的结果等价，我们将分子和分母同乘以 $\\exp(x)$：\n$$ \\frac{dy}{dx} = \\frac{1 \\cdot \\exp(x)}{(1+\\exp(-x))\\exp(x)} = \\frac{\\exp(x)}{\\exp(x)+1} $$\n\n情况2：$x  0$\n对于 $x  0$，我们有 $m(x)=0$ 和 $s(x)=-|x|=-(-x)=x$。关于 $x$ 的导数是：\n$\\frac{dm}{dx} = 0$\n$\\frac{ds}{dx} = 1$\n将这些代入链式法则表达式中：\n$$ \\frac{dy}{dx} = 0 + \\left(\\frac{1}{1+c}\\right)(\\exp(s))(1) = \\frac{\\exp(s)}{1+c} $$\n现在，我们用 $x$ 来表示它。对于 $x  0$，$s=x$ 且 $c=\\exp(x)$。\n$$ \\frac{dy}{dx} = \\frac{\\exp(x)}{1+\\exp(x)} $$\n\n两种情况都得出了相同的解析表达式。表达式 $\\frac{\\exp(x)}{1+\\exp(x)}$ 对所有 $x \\in \\mathbb{R}$ 都是连续的。在 $x=0$ 处，其值为 $\\frac{\\exp(0)}{1+\\exp(0)} = \\frac{1}{2}$，这是正确的导数。因此，这个单一表达式是 $f(x)$ 对所有实数 $x$ 的导数。\n\n**任务3：数值稳定性的解释**\n\n稳定图减轻了数值上溢问题，这在朴素图中对于大的正数 $x$ 是一个关键问题。\n\n- **朴素图分析**：第一个操作是 $a = \\exp(x)$。如果 $x$ 是一个大的正数（例如，$x=1000$），$\\exp(x)$ 将超过标准浮点类型的最大可表示值，导致上溢错误。计算失败。对于梯度 $\\frac{\\exp(x)}{1+\\exp(x)}$，朴素的浮点计算将导致 $\\frac{\\infty}{\\infty}$，其结果为 NaN（非数字）。\n\n- **稳定图分析**：稳定图的构造方式使得指数函数的参数 $s = -|x|$ 始终为非正数（$s \\le 0$）。因此，中间值 $c = \\exp(s)$ 始终在范围 $(0, 1]$ 内。这防止了指数计算中任何上溢的可能性。\n  - 对于大的正数 $x$：计算过程为 $y = x + \\ln(1+\\exp(-x))$。项 $\\exp(-x)$ 正确地发生下溢变为 $0$ 而不会出错，导致 $y \\approx x$，这是正确的渐进行为。梯度由对应于 $x>0$ 情况的表达式 $\\frac{dy}{dx} = \\frac{1}{1+\\exp(-x)}$ 计算。对于大的正数 $x$，其计算结果为 $\\frac{1}{1+0}=1$，这是正确的极限，避免了朴素方法中的 `NaN` 结果。\n  - 对于大的负数 $x$：朴素图和稳定图都计算 $\\exp(x)$，它会下溢到 $0$。在这种情况下，两种形式都是数值稳定的。稳定图的主要优势特别体现在大的正数 $x$ 上。\n\n总之，稳定图在保持数学等价性的同时，重构了计算过程，以确保指数函数的参数永远不会是大的正数，从而避免了上溢，并为函数值及其梯度在整个 $x$ 的定义域上产生数值稳定的结果。\n\n**任务4：最终简化梯度表达式**\n\n如任务1和任务2中所推导，对所有实数 $x$ 均有效的梯度 $\\frac{dy}{dx}$ 的简化、封闭形式解析表达式是 sigmoid 函数。\n$$ \\frac{dy}{dx} = \\frac{\\exp(x)}{1+\\exp(x)} $$\n这也可以写作 $\\frac{1}{1+\\exp(-x)}$，当 $x$ 为大的正数时，这是计算上更数值稳定的形式。两个表达式在数学上是等价的。", "answer": "$$\\boxed{\\frac{\\exp(x)}{1+\\exp(x)}}$$", "id": "3108012"}]}