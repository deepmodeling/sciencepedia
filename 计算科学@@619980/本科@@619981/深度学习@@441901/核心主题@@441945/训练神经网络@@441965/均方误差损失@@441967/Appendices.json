{"hands_on_practices": [{"introduction": "理论学习之后，最好的检验方式就是实践。本节将通过一系列精心设计的动手练习，帮助你将均方误差（MSE）的抽象概念转化为扎实的技能。我们首先从一个简单的计算开始，直观地感受均方误差与绝对误差在惩罚预测偏差时的不同之处。这个练习将清晰地揭示为何MSE对较大误差的反应更为“敏感”，为你理解其在模型训练中的行为特性打下基础。", "problem": "某气象研究所的一个数据科学团队正在评估一种新的天气预测模型。该模型的准确性通过损失函数进行评估，损失函数用以量化对不正确预测的惩罚。设某天的真实测量温度用 $y$ 表示，模型的预测温度用 $\\hat{y}$ 表示。\n\n团队正在考虑两种常见的损失函数：\n1.  平方误差损失，定义为 $L_2(y, \\hat{y}) = (y - \\hat{y})^2$。\n2.  绝对误差损失，定义为 $L_1(y, \\hat{y}) = |y - \\hat{y}|$。\n\n在南极的一个特定测试日，该模型的温度预测偏差恰好为 $3.5$ 开尔文。计算对于这个特定的预测，平方误差损失所赋予的惩罚与绝对误差损失所赋予的惩罚之比的数值。", "solution": "该问题要求计算在给定预测误差下，平方误差损失与绝对误差损失的比值。设真实温度为 $y$，预测温度为 $\\hat{y}$。\n\n平方误差损失由以下公式给出：\n$$L_2(y, \\hat{y}) = (y - \\hat{y})^2$$\n\n绝对误差损失由以下公式给出：\n$$L_1(y, \\hat{y}) = |y - \\hat{y}|$$\n\n我们已知预测误差的大小为 $3.5$ 开尔文。这意味着真实值与预测值之间的绝对差为 $3.5$。在数学上，这可以写成：\n$$|y - \\hat{y}| = 3.5$$\n\n现在，我们可以计算在此特定误差下每种损失函数的值。\n\n对于绝对误差损失，我们可以直接代入给定的误差大小：\n$$L_1 = |y - \\hat{y}| = 3.5$$\n来自绝对误差损失的惩罚是 $3.5$。\n\n对于平方误差损失，我们使用 $(y-\\hat{y})^2 = (|y-\\hat{y}|)^2$ 这一事实。代入给定的误差大小：\n$$L_2 = (y - \\hat{y})^2 = (|y - \\hat{y}|)^2 = (3.5)^2$$\n计算其值：\n$$L_2 = 3.5 \\times 3.5 = 12.25$$\n来自平方误差损失的惩罚是 $12.25$。\n\n问题要求计算平方误差损失与绝对误差损失的比值。我们称这个比值为 $R$。\n$$R = \\frac{L_2(y, \\hat{y})}{L_1(y, \\hat{y})}$$\n\n代入计算出的损失值：\n$$R = \\frac{12.25}{3.5}$$\n\n为了化简这个分数，我们可以将 $12.25$ 表示为 $(3.5)^2$：\n$$R = \\frac{(3.5)^2}{3.5} = 3.5$$\n\n因此，对于 $3.5$ 开尔文的预测误差，平方误差损失所带来的惩罚是绝对误差损失所带来惩罚的 $3.5$ 倍。温度的单位（误差单位为开尔文，$L_2$ 的单位为开尔文的平方，$L_1$ 的单位为开尔文）在比值中相互抵消，最终得到一个无量纲的量。", "answer": "$$\\boxed{3.5}$$", "id": "1931773"}, {"introduction": "在深度学习中，损失函数通过反向传播指导模型参数的更新。下一个练习将带你深入这一核心机制，推导在使用均方误差损失时，一个常见的网络层——层归一化（Layer Normalization）中可学习参数 $\\gamma$ 和 $\\beta$ 的梯度。通过这个“笔和纸”的练习，你将实践微积分的链式法则在真实神经网络环境中的应用，这对于理解和设计新的模型架构至关重要。", "problem": "考虑一个深度学习中的监督学习场景，其小批量大小 (mini-batch size) 为 $B$，每个样本的输出维度为 $d$。对于每个样本 $b \\in \\{1,\\dots,B\\}$，网络产生一个原始输出向量 $\\hat{\\mathbf{y}}^{(b)} \\in \\mathbb{R}^{d}$，并根据一个目标向量 $\\mathbf{t}^{(b)} \\in \\mathbb{R}^{d}$ 进行训练。模型在输出端应用层归一化 (layer normalization)，使用一个在所有特征上共享的标量缩放参数 $\\gamma \\in \\mathbb{R}$ 和一个标量平移参数 $\\beta \\in \\mathbb{R}$。对于每个样本 $b$，定义样本均值\n$$\n\\mu^{(b)} \\equiv \\frac{1}{d} \\sum_{i=1}^{d} \\hat{y}^{(b)}_{i},\n$$\n和样本标准差\n$$\n\\sigma^{(b)} \\equiv \\sqrt{\\frac{1}{d} \\sum_{i=1}^{d} \\left(\\hat{y}^{(b)}_{i} - \\mu^{(b)}\\right)^{2} + \\epsilon},\n$$\n其中 $\\epsilon > 0$ 是一个固定的常数。归一化后的输出分量为\n$$\ny^{(b)}_{\\text{norm},i} \\equiv \\gamma \\,\\frac{\\hat{y}^{(b)}_{i} - \\mu^{(b)}}{\\sigma^{(b)}} + \\beta \\quad \\text{for } i \\in \\{1,\\dots,d\\}.\n$$\n训练使用在批次和特征上平均的均方误差 (Mean Squared Error, MSE) 损失，\n$$\nL \\equiv \\frac{1}{2 B d} \\sum_{b=1}^{B} \\sum_{i=1}^{d} \\left(y^{(b)}_{\\text{norm},i} - t^{(b)}_{i}\\right)^{2}.\n$$\n请仅从 $L$ 的定义、$\\mu^{(b)}$ 和 $\\sigma^{(b)}$ 的定义以及微分学的标准法则（特别是链式法则和线性性质）出发，推导两个偏导数 $\\frac{\\partial L}{\\partial \\gamma}$ 和 $\\frac{\\partial L}{\\partial \\beta}$ 的闭式表达式，并用 $\\gamma$、$\\beta$、$\\hat{\\mathbf{y}}^{(b)}$、$\\mathbf{t}^{(b)}$、$\\mu^{(b)}$、$\\sigma^{(b)}$、$B$、$d$ 和 $\\epsilon$ 来表示。将最终答案表示为两个没有未定义符号的显式解析求和。不需要数值近似，也不需要四舍五入。你的最终答案必须只包含这两个导数。", "solution": "问题要求推导均方误差 (MSE) 损失函数 $L$ 关于标量层归一化参数 $\\gamma$ 和 $\\beta$ 的偏导数。推导过程将从给定的定义出发，使用微分学的标准法则，尤其是链式法则。\n\n损失函数 $L$ 定义为：\n$$L \\equiv \\frac{1}{2 B d} \\sum_{b=1}^{B} \\sum_{i=1}^{d} \\left(y^{(b)}_{\\text{norm},i} - t^{(b)}_{i}\\right)^{2}$$\n样本 $b$ 和特征 $i$ 的归一化输出 $y^{(b)}_{\\text{norm},i}$ 由下式给出：\n$$y^{(b)}_{\\text{norm},i} \\equiv \\gamma \\,\\frac{\\hat{y}^{(b)}_{i} - \\mu^{(b)}}{\\sigma^{(b)}} + \\beta$$\n样本均值 $\\mu^{(b)}$ 和样本标准差 $\\sigma^{(b)}$ 是从原始网络输出 $\\hat{\\mathbf{y}}^{(b)}$ 计算得出的。因此，它们不依赖于参数 $\\gamma$ 和 $\\beta$。这对于后续的微分是一个关键的观察。\n\n我们可以通过应用链式法则，建立 $L$ 关于一个通用参数 $\\theta$ 的导数的一般公式。\n$$\\frac{\\partial L}{\\partial \\theta} = \\frac{\\partial}{\\partial \\theta} \\left[ \\frac{1}{2 B d} \\sum_{b=1}^{B} \\sum_{i=1}^{d} \\left(y^{(b)}_{\\text{norm},i} - t^{(b)}_{i}\\right)^{2} \\right]$$\n根据微分的线性性质，我们可以将微分算子移到求和符号内部：\n$$\\frac{\\partial L}{\\partial \\theta} = \\frac{1}{2 B d} \\sum_{b=1}^{B} \\sum_{i=1}^{d} \\frac{\\partial}{\\partial \\theta} \\left(y^{(b)}_{\\text{norm},i} - t^{(b)}_{i}\\right)^{2}$$\n对平方项应用链式法则，得到：\n$$\\frac{\\partial L}{\\partial \\theta} = \\frac{1}{2 B d} \\sum_{b=1}^{B} \\sum_{i=1}^{d} 2 \\left(y^{(b)}_{\\text{norm},i} - t^{(b)}_{i}\\right) \\frac{\\partial}{\\partial \\theta}\\left(y^{(b)}_{\\text{norm},i} - t^{(b)}_{i}\\right)$$\n目标值 $t^{(b)}_{i}$ 相对于模型参数是固定常数，因此它们的导数为零。表达式简化为：\n$$\\frac{\\partial L}{\\partial \\theta} = \\frac{1}{B d} \\sum_{b=1}^{B} \\sum_{i=1}^{d} \\left(y^{(b)}_{\\text{norm},i} - t^{(b)}_{i}\\right) \\frac{\\partial y^{(b)}_{\\text{norm},i}}{\\partial \\theta}$$\n这个通用形式为求得所需的偏导数提供了一个模板。\n\n**1. 关于 $\\gamma$ 的偏导数推导 ($\\frac{\\partial L}{\\partial \\gamma}$)**\n\n我们在通用公式中设 $\\theta = \\gamma$。第一步是计算 $y^{(b)}_{\\text{norm},i}$ 关于 $\\gamma$ 的偏导数：\n$$\\frac{\\partial y^{(b)}_{\\text{norm},i}}{\\partial \\gamma} = \\frac{\\partial}{\\partial \\gamma} \\left( \\gamma \\,\\frac{\\hat{y}^{(b)}_{i} - \\mu^{(b)}}{\\sigma^{(b)}} + \\beta \\right)$$\n由于 $\\beta$、$\\mu^{(b)}$ 和 $\\sigma^{(b)}$ 相对于 $\\gamma$ 是常数，该导数为：\n$$\\frac{\\partial y^{(b)}_{\\text{norm},i}}{\\partial \\gamma} = \\frac{\\hat{y}^{(b)}_{i} - \\mu^{(b)}}{\\sigma^{(b)}}$$\n将此结果代入 $\\frac{\\partial L}{\\partial \\theta}$ 的通用公式中：\n$$\\frac{\\partial L}{\\partial \\gamma} = \\frac{1}{B d} \\sum_{b=1}^{B} \\sum_{i=1}^{d} \\left(y^{(b)}_{\\text{norm},i} - t^{(b)}_{i}\\right) \\left(\\frac{\\hat{y}^{(b)}_{i} - \\mu^{(b)}}{\\sigma^{(b)}}\\right)$$\n最后，为了仅用指定的输入变量来表示结果，我们代入 $y^{(b)}_{\\text{norm},i}$ 的定义：\n$$\\frac{\\partial L}{\\partial \\gamma} = \\frac{1}{B d} \\sum_{b=1}^{B} \\sum_{i=1}^{d} \\left( \\gamma \\,\\frac{\\hat{y}^{(b)}_{i} - \\mu^{(b)}}{\\sigma^{(b)}} + \\beta - t^{(b)}_{i} \\right) \\left( \\frac{\\hat{y}^{(b)}_{i} - \\mu^{(b)}}{\\sigma^{(b)}} \\right)$$\n\n**2. 关于 $\\beta$ 的偏导数推导 ($\\frac{\\partial L}{\\partial \\beta}$)**\n\n我们在通用公式中设 $\\theta = \\beta$。我们首先计算 $y^{(b)}_{\\text{norm},i}$ 关于 $\\beta$ 的偏导数：\n$$\\frac{\\partial y^{(b)}_{\\text{norm},i}}{\\partial \\beta} = \\frac{\\partial}{\\partial \\beta} \\left( \\gamma \\,\\frac{\\hat{y}^{(b)}_{i} - \\mu^{(b)}}{\\sigma^{(b)}} + \\beta \\right)$$\n第一项相对于 $\\beta$ 是常数，所以导数就是：\n$$\\frac{\\partial y^{(b)}_{\\text{norm},i}}{\\partial \\beta} = 1$$\n将此结果代入 $\\frac{\\partial L}{\\partial \\theta}$ 的通用公式中：\n$$\\frac{\\partial L}{\\partial \\beta} = \\frac{1}{B d} \\sum_{b=1}^{B} \\sum_{i=1}^{d} \\left(y^{(b)}_{\\text{norm},i} - t^{(b)}_{i}\\right) (1)$$\n再次，我们代入 $y^{(b)}_{\\text{norm},i}$ 的定义以获得最终表达式：\n$$\\frac{\\partial L}{\\partial \\beta} = \\frac{1}{B d} \\sum_{b=1}^{B} \\sum_{i=1}^{d} \\left( \\gamma \\,\\frac{\\hat{y}^{(b)}_{i} - \\mu^{(b)}}{\\sigma^{(b)}} + \\beta - t^{(b)}_{i} \\right)$$\n\n这两个推导出的表达式是用问题陈述中指定的变量表示的闭式解析求和。", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{1}{B d} \\sum_{b=1}^{B} \\sum_{i=1}^{d} \\left( \\gamma \\frac{\\hat{y}^{(b)}_{i} - \\mu^{(b)}}{\\sigma^{(b)}} + \\beta - t^{(b)}_{i} \\right) \\left( \\frac{\\hat{y}^{(b)}_{i} - \\mu^{(b)}}{\\sigma^{(b)}} \\right)  \\\\\n\\frac{1}{B d} \\sum_{b=1}^{B} \\sum_{i=1}^{d} \\left( \\gamma \\frac{\\hat{y}^{(b)}_{i} - \\mu^{(b)}}{\\sigma^{(b)}} + \\beta - t^{(b)}_{i} \\right)\n\\end{pmatrix}\n}\n$$", "id": "3148553"}, {"introduction": "均方误差损失的一个基本统计特性是，其最优解对应于数据的条件均值。为了加深对这一本质的理解，我们将通过一个结合了理论推导和编程实践的练习来探索样本权重对MSE最优解的影响。你将首先从第一性原理出发，证明加权MSE的最优解是数据的加权平均值，然后通过一个模拟实验，验证样本重复（一种特殊的加权形式）如何“拉动”模型的预测结果。", "problem": "考虑一个监督学习场景，其中模型根据离散输入预测一个标量目标。设数据集为 $\\{(x_i,y_i)\\}_{i=1}^n$，其中 $x_i \\in \\mathcal{X}$ 且 $y_i \\in \\mathbb{R}$。将单样本损失定义为平方误差 $\\ell(\\hat{y},y) = (\\hat{y}-y)^2$，并将子集 $\\mathcal{D} \\subseteq \\{1,\\dots,n\\}$ 上的均方误差（MSE）经验风险定义为 $L(\\theta \\mid \\mathcal{D}) = \\sum_{i \\in \\mathcal{D}} (\\theta - y_i)^2$，其中标量预测器 $\\theta$ 在该子集内是恒定的。在具有离散输入的条件预测设置中，考虑一个模型，它为每个输入值预测一个单独的常数：对于每个 $x \\in \\mathcal{X}$，模型返回 $f(x) = \\theta_x$，经验风险按输入值分解为 $L_{\\text{emp}}(\\{\\theta_x\\}) = \\sum_{x \\in \\mathcal{X}} \\sum_{i : x_i = x} (\\theta_x - y_i)^2$。\n\n从上述定义和凸优化的基本原理（例如，通过将导数置零来最小化一个可微的凸二次函数）出发，推导当该输入的样本可能被重复时，哪个标量 $\\theta_x$ 能够最小化限制在单个输入值 $x$ 上的经验风险。设每个样本索引 $i$ 带有一个非负整数重复计数 $c_i \\in \\{0,1,2,\\dots\\}$，并假设输入 $x$ 的经验风险为 $L_x(\\theta_x) = \\sum_{i : x_i = x} c_i \\, (\\theta_x - y_i)^2$。用 $\\{y_i\\}$ 和 $\\{c_i\\}$ 表示该最小化器，并解释为什么重复计数实际上充当了权重，可以将最小化器从未加权的条件均值上偏移。\n\n然后，实现一个程序来执行以下确定性模拟。使用一个包含两个输入值和以下标签的数据集：\n- 对于 $x=0$，设标签为 $y$ 值 $[1.0, 3.0]$。\n- 对于 $x=1$，设标签为 $y$ 值 $[0.0, 4.0]$。\n\n对于下述每个测试用例，每个样本的重复计数由与各输入值的标签顺序对齐的数组指定。例如，对于 $x=0$，第一个计数应用于 $y=1.0$，第二个应用于 $y=3.0$；对于 $x=1$，第一个计数应用于 $y=0.0$，第二个应用于 $y=4.0$。对于每个测试用例：\n- 计算原始的每个输入的经验最小化器（所有重复计数等于1）。\n- 使用提供的计数计算重复样本的每个输入的经验最小化器。\n- 对于每个输入值 $x \\in \\{0,1\\}$，输出差值 $\\Delta_x = \\theta_x^{\\text{dup}} - \\theta_x^{\\text{orig}}$。\n\n设计一个覆盖不同重复场景的测试套件：\n- 测试用例 1 (正常路径): $x=0$ 计数 $[1,1]$，$x=1$ 计数 $[1,3]$。\n- 测试用例 2 (边界，无重复): $x=0$ 计数 $[1,1]$，$x=1$ 计数 $[1,1]$。\n- 测试用例 3 (边缘，x=0处重度重复): $x=0$ 计数 $[5,1]$，$x=1$ 计数 $[1,1]$。\n- 测试用例 4 (边缘，两个输入均平衡重复): $x=0$ 计数 $[2,2]$，$x=1$ 计数 $[2,2]$。\n- 测试用例 5 (边缘，通过零重复移除 x=1 中的一个标签): $x=0$ 计数 $[1,1]$，$x=1$ 计数 $[0,2]$。\n\n要求的最终输出格式是单行文本，包含一个结果列表，每个测试用例一个结果，其中每个结果是该用例的列表 $[\\Delta_0,\\Delta_1]$。格式必须是方括号括起来的逗号分隔列表，例如 $[[\\Delta_0^{(1)},\\Delta_1^{(1)}],[\\Delta_0^{(2)},\\Delta_1^{(2)}],\\dots]$，数值以标准十进制浮点数表示。不涉及物理单位或角度单位；所有数值输出都应为实数。您的程序应只生成包含此聚合列表的一行文本，不得有任何其他文本。", "solution": "该问题要求推导最优标量预测器 $\\theta_x$，以最小化特定输入值 $x$ 的加权经验风险。风险函数定义为平方误差的加权和。\n\n设与离散输入值 $x \\in \\mathcal{X}$ 对应的样本索引集表示为 $I_x = \\{i \\mid x_i = x\\}$。对于此输入值，包含非负整数重复计数 $c_i$ 的经验风险由下式给出：\n$$\nL_x(\\theta_x) = \\sum_{i \\in I_x} c_i (\\theta_x - y_i)^2\n$$\n此处，$\\theta_x$ 是对输入 $x$ 的恒定预测，$\\{y_i\\}_{i \\in I_x}$ 是对应的目标值，$\\{c_i\\}_{i \\in I_x}$ 是重复计数。\n\n函数 $L_x(\\theta_x)$ 是一个关于 $\\theta_x$ 的二次函数。它可以展开为：\n$$\nL_x(\\theta_x) = \\sum_{i \\in I_x} c_i (\\theta_x^2 - 2\\theta_x y_i + y_i^2) = \\left(\\sum_{i \\in I_x} c_i\\right)\\theta_x^2 - \\left(2\\sum_{i \\in I_x} c_i y_i\\right)\\theta_x + \\left(\\sum_{i \\in I_x} c_i y_i^2\\right)\n$$\n只要 $\\theta_x^2$ 项的系数 $\\sum_{i \\in I_x} c_i$ 为正，这就是一个关于 $\\theta_x$ 的开口向上的抛物线。如果输入 $x$ 至少有一个样本的计数 $c_i > 0$，则此条件成立。在此条件下，该函数是严格凸的，并具有唯一的全局最小值。\n\n为了找到最小化 $L_x(\\theta_x)$ 的 $\\theta_x$ 值，我们应用最优性的一阶必要条件，即对 $L_x(\\theta_x)$ 关于 $\\theta_x$ 求导并将其置零。\n\n$$\n\\frac{dL_x(\\theta_x)}{d\\theta_x} = \\frac{d}{d\\theta_x} \\left( \\sum_{i \\in I_x} c_i (\\theta_x - y_i)^2 \\right)\n$$\n\n根据微分的线性性质，我们可以将导数移到求和符号内部：\n$$\n\\frac{dL_x(\\theta_x)}{d\\theta_x} = \\sum_{i \\in I_x} \\frac{d}{d\\theta_x} \\left( c_i (\\theta_x - y_i)^2 \\right)\n$$\n\n应用链式法则，其中 $\\frac{d}{du}(u^2) = 2u$：\n$$\n\\frac{dL_x(\\theta_x)}{d\\theta_x} = \\sum_{i \\in I_x} c_i \\cdot 2(\\theta_x - y_i) \\cdot \\frac{d}{d\\theta_x}(\\theta_x - y_i) = \\sum_{i \\in I_x} 2 c_i (\\theta_x - y_i)\n$$\n\n将导数置零以找到临界点：\n$$\n\\sum_{i \\in I_x} 2 c_i (\\theta_x - y_i) = 0\n$$\n\n我们可以除以常数 $2$ 并分配求和：\n$$\n\\sum_{i \\in I_x} c_i \\theta_x - \\sum_{i \\in I_x} c_i y_i = 0\n$$\n\n由于 $\\theta_x$ 是关于索引 $i$ 的常数，它可以从第一个求和中提出来：\n$$\n\\theta_x \\left( \\sum_{i \\in I_x} c_i \\right) = \\sum_{i \\in I_x} c_i y_i\n$$\n\n求解 $\\theta_x$，我们得到最小化器，记作 $\\theta_x^*$：\n$$\n\\theta_x^* = \\frac{\\sum_{i \\in I_x} c_i y_i}{\\sum_{i \\in I_x} c_i}\n$$\n这假设 $\\sum_{i \\in I_x} c_i \\neq 0$。如果所有计数都为零，$L_x(\\theta_x)$ 将恒等于零，任何 $\\theta_x$ 都将是最小化器。然而，问题背景确保在所有测试用例中此分母都不为零。\n\n为确认此临界点是最小值，我们检查二阶导数：\n$$\n\\frac{d^2L_x(\\theta_x)}{d\\theta_x^2} = \\frac{d}{d\\theta_x} \\left( \\sum_{i \\in I_x} 2 c_i (\\theta_x - y_i) \\right) = \\sum_{i \\in I_x} 2 c_i\n$$\n由于计数 $c_i$ 是非负整数且至少有一个为正，二阶导数 $\\sum_{i \\in I_x} 2 c_i > 0$。正的二阶导数证实了函数 $L_x(\\theta_x)$ 是严格凸的，且临界点 $\\theta_x^*$ 确实是一个唯一的全局最小值。\n\n推导出的 $\\theta_x^*$ 表达式是目标值 $\\{y_i\\}_{i \\in I_x}$ 的加权平均，其中权重是重复计数 $\\{c_i\\}_{i \\in I_x}$。在没有重复的情况下，所有计数均为 $c_i=1$。表达式随之简化为：\n$$\n\\theta_x^* \\text{ (unweighted)} = \\frac{\\sum_{i \\in I_x} y_i}{\\sum_{i \\in I_x} 1} = \\frac{\\sum_{i \\in I_x} y_i}{|I_x|}\n$$\n这是给定输入 $x$ 的目标值的未加权条件均值。\n\n当计数 $c_i$ 不均匀时，它们充当权重，赋予具有较高重复次数的样本更大的重要性。具有大计数 $c_i$ 的样本 $(x_i, y_i)$ 将对加权平均的分子和分母做出更显著的贡献。因此，最优预测器 $\\theta_x^*$ 被“拉向”与更高计数相关联的那些 $y_i$ 值。这说明了重复计数，或更普遍的样本权重，如何能够使平方误差损失的最小化器偏离未加权的条件均值，而偏向于被认为更重要的特定数据点。\n\n该模拟将应用此公式，在不同的重复场景下计算最小化器，并将产生的偏差量化为差值 $\\Delta_x = \\theta_x^{\\text{dup}} - \\theta_x^{\\text{orig}}$。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Derives and simulates the effect of sample duplication on the minimizer\n    of the Mean Squared Error loss for a conditional model.\n    \"\"\"\n    \n    # Dataset of labels for each discrete input value x.\n    y_values = {\n        0: np.array([1.0, 3.0]),\n        1: np.array([0.0, 4.0])\n    }\n    \n    # Test cases defining the duplication counts for each label.\n    test_cases = [\n        # Test case 1 (normal path)\n        {'counts_x0': [1, 1], 'counts_x1': [1, 3]},\n        # Test case 2 (boundary, no duplication)\n        {'counts_x0': [1, 1], 'counts_x1': [1, 1]},\n        # Test case 3 (edge, heavy duplication in x=0)\n        {'counts_x0': [5, 1], 'counts_x1': [1, 1]},\n        # Test case 4 (edge, balanced duplication in both inputs)\n        {'counts_x0': [2, 2], 'counts_x1': [2, 2]},\n        # Test case 5 (edge, removal of one label in x=1 via zero count)\n        {'counts_x0': [1, 1], 'counts_x1': [0, 2]},\n    ]\n    \n    def compute_minimizer(y, counts):\n        \"\"\"\n        Computes the weighted average that minimizes the squared error.\n        theta_x = (sum(c_i * y_i)) / (sum(c_i))\n        \"\"\"\n        c = np.array(counts, dtype=float)\n        numerator = np.sum(c * y)\n        denominator = np.sum(c)\n        if denominator == 0:\n            # This case will not be reached in this problem but is good practice.\n            return np.nan \n        return numerator / denominator\n\n    # Compute the original minimizers (all duplication counts are 1).\n    orig_counts = [1, 1]\n    theta0_orig = compute_minimizer(y_values[0], orig_counts)\n    theta1_orig = compute_minimizer(y_values[1], orig_counts)\n\n    results = []\n    \n    for case in test_cases:\n        counts0 = case['counts_x0']\n        counts1 = case['counts_x1']\n        \n        # Compute the duplicated (weighted) minimizers.\n        theta0_dup = compute_minimizer(y_values[0], counts0)\n        theta1_dup = compute_minimizer(y_values[1], counts1)\n        \n        # Calculate the differences.\n        delta0 = theta0_dup - theta0_orig\n        delta1 = theta1_dup - theta1_orig\n        \n        results.append([delta0, delta1])\n        \n    # Format the final output string as a list of lists.\n    # e.g., [[d0,d1],[d0,d1],...]\n    output_str = f\"[{','.join(f'[{d[0]},{d[1]}]' for d in results)}]\"\n    print(output_str)\n\nsolve()\n```", "id": "3148476"}]}