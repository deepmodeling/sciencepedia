{"hands_on_practices": [{"introduction": "随机梯度下降（SGD）是一种迭代优化算法。为了理解其工作原理，最好的方法就是从单次更新步骤开始。本练习将通过一个具体实例，向您展示SGD如何利用单个样本的梯度来近似整体损失函数的梯度，从而执行一次参数更新[@problem_id:2206637]。这个基础计算是掌握SGD算法核心机制的关键。", "problem": "一个迭代优化算法被用来寻找一个最小化成本函数的参数 $x$。总成本函数是几个分量函数的平均值：$F(x) = \\frac{1}{N}\\sum_{i=1}^{N} f_i(x)$。在这个特定情况下，分量函数是二次的，由 $f_i(x) = (x - c_i)^2$ 给出，其中常数 $c_i = i$，$i = 1, 2, \\dots, 10$，因此 $N=10$。\n\n优化过程从参数的初始猜测值 $x_0$ 开始。在每一步中，通过仅使用一个随机选择的分量函数 $f_j(x)$，从当前估计值 $x_k$ 计算出新的估计值 $x_{k+1}$。更新规则定义为：\n$$x_{k+1} = x_k - \\eta \\left( \\frac{d f_j(x)}{dx} \\bigg|_{x=x_k} \\right)$$\n其中 $\\eta$ 是一个称为学习率的常数。\n\n给定初始参数值 $x_0 = 10.0$ 和学习率 $\\eta = 0.1$，计算经过一次更新步骤后的参数 $x_1$ 的值。对于这第一步，使用的分量函数是索引为 $j=5$ 的 $f_j(x)$。", "solution": "我们已知分量函数的形式为 $f_{i}(x) = (x - c_{i})^{2}$，其中 $c_{i} = i$。对于第一次更新，所选择的索引为 $j=5$，因此 $f_{5}(x) = (x - 5)^{2}$。\n\n更新规则是\n$$\nx_{k+1} = x_{k} - \\eta \\left.\\frac{d f_{j}(x)}{dx}\\right|_{x=x_{k}}.\n$$\n使用幂法则和链式法则，所选分量的导数是\n$$\n\\frac{d f_{5}(x)}{dx} = \\frac{d}{dx}\\left[(x - 5)^{2}\\right] = 2(x - 5).\n$$\n在当前迭代值 $x_{0} = 10$ 处求值，得到\n$$\n\\left.\\frac{d f_{5}(x)}{dx}\\right|_{x=10} = 2(10 - 5) = 10.\n$$\n当学习率为 $\\eta = 0.1$ 时，更新变为\n$$\nx_{1} = x_{0} - \\eta \\cdot 10 = 10 - 0.1 \\times 10 = 10 - 1 = 9.\n$$\n因此，在使用 $f_{5}$ 进行一次更新步骤后，参数值为 $x_{1} = 9$。", "answer": "$$\\boxed{9}$$", "id": "2206637"}, {"introduction": "在掌握了单步更新的基础后，我们可以将SGD应用于更真实的机器学习模型中。SGD的强大之处在于其广泛的适用性，即使是对于复杂的非线性模型[@problem_id:2206657]。此练习要求您为一个非线性回归模型推导其SGD更新规则，这将锻炼您在面对新模型和自定义损失函数时应用梯度下降法的关键能力。", "problem": "在一个用于预测正值的机器学习模型中，预测值 $\\hat{y}$ 通过公式 $\\hat{y} = \\exp(w^T x)$ 与一个 $d$ 维特征向量 $x$ 相关联，其中 $w$ 是一个 $d$ 维权重向量。目标是调整权重 $w$，以使预测值尽可能接近真实观测值。\n\n模型的性能由一个损失函数来衡量。对于一个包含 $m$ 个样本的数据集 $\\{(x_i, y_i)\\}_{i=1}^m$，总损失定义为模型预测值与真实值之间平方差的和：\n$$\nL(w) = \\sum_{i=1}^{m} \\left( \\exp(w^T x_i) - y_i \\right)^2\n$$\n我们使用随机梯度下降 (SGD) 这一迭代优化算法来最小化该损失。在每次迭代中，SGD 仅使用单个数据样本来近似总损失的梯度。\n\n假设当前的权重向量为 $w$。选中一个数据样本 $(x_k, y_k)$。使用该样本，以学习率 $\\eta > 0$ 执行一步 SGD 更新。请推导出更新后的权重向量 $w_{new}$ 的表达式。您的最终表达式应以 $w$、$\\eta$、$x_k$ 和 $y_k$ 表示。", "solution": "我们考虑所选样本 $(x_{k}, y_{k})$ 的单样本损失，\n$$\n\\ell_{k}(w) = \\left(\\exp\\!\\left(w^{T} x_{k}\\right) - y_{k}\\right)^{2}.\n$$\n为了执行 SGD 更新，我们需要计算 $\\ell_{k}(w)$ 关于 $w$ 的梯度。令 $f(w) = \\exp(w^{T} x_{k}) - y_{k}$。则 $\\ell_{k}(w) = f(w)^{2}$，因此根据链式法则，\n$$\n\\nabla_{w} \\ell_{k}(w) = 2 f(w)\\, \\nabla_{w} f(w).\n$$\n接下来，计算 $\\nabla_{w} f(w)$。由于 $f(w) = \\exp(w^{T} x_{k}) - y_{k}$ 且 $\\nabla_{w}(w^{T} x_{k}) = x_{k}$，我们有\n$$\n\\nabla_{w} f(w) = \\exp\\!\\left(w^{T} x_{k}\\right) \\, x_{k}.\n$$\n因此，\n$$\n\\nabla_{w} \\ell_{k}(w) = 2 \\left(\\exp\\!\\left(w^{T} x_{k}\\right) - y_{k}\\right) \\exp\\!\\left(w^{T} x_{k}\\right) x_{k}.\n$$\n学习率为 $\\eta>0$ 的 SGD 更新是\n$$\nw_{\\text{new}} = w - \\eta \\nabla_{w} \\ell_{k}(w),\n$$\n这得到\n$$\nw_{\\text{new}} = w - 2 \\eta \\left(\\exp\\!\\left(w^{T} x_{k}\\right) - y_{k}\\right) \\exp\\!\\left(w^{T} x_{k}\\right) x_{k}.\n$$", "answer": "$$\\boxed{w - 2 \\eta \\left(\\exp\\!\\left(w^{T} x_{k}\\right) - y_{k}\\right)\\exp\\!\\left(w^{T} x_{k}\\right) x_{k}}$$", "id": "2206657"}, {"introduction": "在应用SGD时，一个至关重要的环节是选择合适的学习率 $\\eta$。学习率决定了我们沿梯度反方向前进的步长，一个过大的学习率可能会导致更新步骤“越过”最优点，引起参数值的剧烈震荡甚至发散，使得损失值不降反升。本练习通过一个具体的数值例子，直观地揭示了学习率选择不当所带来的风险，帮助您理解其在训练过程中的关键作用[@problem_id:2206673]。", "problem": "在机器学习背景下，我们通常通过最小化损失函数来优化模型的参数。考虑一个带有单个标量参数 $w$ 的简化模型。与单个数据点相关的损失函数由 $L(w) = \\frac{1}{2} c w^2$ 给出，其中最小损失发生在 $w=0$ 处。该参数使用随机梯度下降（SGD）算法进行更新。在步骤 $k$ 处参数的更新规则由 $w_{k+1} = w_k - \\eta \\nabla L(w_k)$ 给出，其中 $\\nabla L(w_k)$ 是在 $w_k$ 处计算的损失函数的梯度，而 $\\eta$ 是学习率。\n\n假设参数的初始值为 $w_0 = 4.0$。模型参数设置为 $c = 0.75$，学习率为 $\\eta = 3.2$。计算经过 3 个更新步骤后参数 $w$ 的值（即，求 $w_3$）。\n\n将最终答案四舍五入至三位有效数字。", "solution": "损失为 $L(w)=\\frac{1}{2} c w^{2}$。其梯度通过微分得到：\n$$\n\\nabla L(w)=\\frac{\\mathrm{d}}{\\mathrm{d}w}\\left(\\frac{1}{2} c w^{2}\\right)=c w.\n$$\nSGD 更新规则为\n$$\nw_{k+1}=w_{k}-\\eta \\nabla L(w_{k})=w_{k}-\\eta c w_{k}=(1-\\eta c)\\,w_{k}.\n$$\n这个线性递推关系的解为\n$$\nw_{k}=(1-\\eta c)^{k} w_{0}.\n$$\n代入 $c=0.75$、$\\eta=3.2$ 和 $w_{0}=4.0$，\n$$\n1-\\eta c=1-(3.2)(0.75)=1-2.4=-1.4,\n$$\n所以\n$$\nw_{3}=(-1.4)^{3}\\cdot 4.0=-10.976.\n$$\n四舍五入到三位有效数字得到 $-11.0$。", "answer": "$$\\boxed{-11.0}$$", "id": "2206673"}]}