{"hands_on_practices": [{"introduction": "标准损失函数如交叉熵是深度学习的基石，但仅仅会使用它们还不够。要真正掌握模型优化，我们需要深入理解其几何特性。本练习将引导你分析模型输出（logits）的归一化如何影响交叉熵损失，从而揭示损失对尺度变换的不变性，并将其与损失曲面的“尖锐度”联系起来。通过这个推导 [@problem_id:3145403]，你将对损失函数的行为以及某些归一化技巧为何能稳定训练过程，建立起更深刻的直觉。", "problem": "考虑一个具有 $K$ 个类的多类分类器，它产生一个 logit 向量 $z \\in \\mathbb{R}^{K}$。预测的类别概率由 softmax 函数给出，定义为 $p_{i}(z) = \\frac{\\exp(z_{i})}{\\sum_{j=1}^{K} \\exp(z_{j})}$，其中 $i \\in \\{1, \\dots, K\\}$。给定一个真实类别索引 $y \\in \\{1, \\dots, K\\}$ 和一个 one-hot 目标，交叉熵（CE）损失定义为 $L(z,y) = -\\ln\\big(p_{y}(z)\\big)$。令 $\\|\\cdot\\|_{2}$ 表示欧几里得范数（L2 范数）。对于任何非零的 $z$，定义归一化 logits 为 $\\hat{z} = \\frac{z}{\\|z\\|_{2}}$，以及相应的归一化交叉熵损失为 $\\hat{L}(z,y) = -\\ln\\left(\\frac{\\exp\\left(\\hat{z}_{y}\\right)}{\\sum_{j=1}^{K} \\exp\\left(\\hat{z}_{j}\\right)}\\right)$。从 softmax 函数和交叉熵损失的定义，以及基本性质 $\\|c z\\|_{2} = |c| \\|z\\|_{2}$（对于任何标量 $c \\in \\mathbb{R}$）出发，分析用 $\\frac{z}{\\|z\\|_{2}}$ 替换 $z$ 对损失关于正 logit 缩放的不变性的影响。具体来说，对于 $c>0$，从第一性原理推导 $\\hat{L}(c z,y)$ 是否等于 $\\hat{L}(z,y)$，并解释这种不变性对于沿缩放方向的标准锐度概念的含义。为使锐度具体化，将归一化损失的缩放方向锐度定义为 $S(z,y) = \\left.\\frac{d^{2}}{dc^{2}} \\hat{L}(c z,y)\\right|_{c=1}$。\n\n使用上述设置，取 $K=3$，$y=1$，以及 $z = (2, 0, -1)^{\\top}$（一个非零向量）。根据您的推导，计算 $S(z,y)$ 的值。您的最终答案必须是一个实数。不需要四舍五入。", "solution": "首先根据所需程序验证问题陈述。\n\n### 步骤 1：提取已知条件\n- 类别数：$K$。\n- Logit 向量：$z \\in \\mathbb{R}^{K}$。\n- Softmax 函数：$p_{i}(z) = \\frac{\\exp(z_{i})}{\\sum_{j=1}^{K} \\exp(z_{j})}$，其中 $i \\in \\{1, \\dots, K\\}$。\n- 真实类别索引：$y \\in \\{1, \\dots, K\\}$。\n- 交叉熵（CE）损失：$L(z,y) = -\\ln\\big(p_{y}(z)\\big)$。\n- 欧几里得范数（L2 范数）：$\\|\\cdot\\|_{2}$。\n- 归一化 logits：$\\hat{z} = \\frac{z}{\\|z\\|_{2}}$，对于任何非零的 $z$。\n- 归一化交叉熵损失：$\\hat{L}(z,y) = -\\ln\\left(\\frac{\\exp\\left(\\hat{z}_{y}\\right)}{\\sum_{j=1}^{K} \\exp\\left(\\hat{z}_{j}\\right)}\\right)$。\n- 范数性质：$\\|c z\\|_{2} = |c| \\|z\\|_{2}$，对于任何标量 $c \\in \\mathbb{R}$。\n- 缩放因子：$c > 0$。\n- 缩放方向锐度：$S(z,y) = \\left.\\frac{d^{2}}{dc^{2}} \\hat{L}(c z,y)\\right|_{c=1}$。\n- 用于计算的具体值：$K=3$，$y=1$，$z = (2, 0, -1)^{\\top}$。\n\n### 步骤 2：使用提取的已知条件进行验证\n该问题定义明确且具有科学依据。它使用了深度学习领域的标准定义，包括 softmax 函数、交叉熵损失和 L2 归一化。前提和定义在数学上是一致的，没有矛盾。目标明确陈述：进行特定的推导和后续计算。推导和计算所需的所有信息都已提供。向量 $z$ 被指定为非零，这是为了使归一化 $\\frac{z}{\\|z\\|_{2}}$ 有定义所必需的。条件 $c>0$ 对恒等式 $\\|cz\\|_2 = c\\|z\\|_2$ 很重要。该问题是客观、形式化的，并提出了一个关于损失函数性质的非平凡但可解的问题，这是一个标准的研究课题。\n\n### 步骤 3：结论与行动\n该问题被视为有效。将提供完整解答。\n\n### 解答推导\n该问题要求我们分析归一化交叉熵损失 $\\hat{L}(z,y)$ 相对于 logits $z$ 的正缩放的不变性，然后计算缩放方向上的锐度度量。\n\n首先，让我们分析将 logit 向量 $z$ 乘以一个正常数 $c > 0$ 对归一化 logits 的影响。令缩放后的 logit 向量为 $z' = cz$。根据定义，$z'$ 的归一化版本是：\n$$\n\\widehat{z'} = \\frac{z'}{\\|z'\\|_{2}}\n$$\n代入 $z' = cz$ 并使用所提供的欧几里得范数性质 $\\|cz\\|_{2} = |c|\\|z\\|_{2}$：\n$$\n\\widehat{z'} = \\frac{cz}{\\|cz\\|_{2}} = \\frac{cz}{|c|\\|z\\|_{2}}\n$$\n由于问题陈述 $c > 0$，我们有 $|c|=c$。表达式简化为：\n$$\n\\widehat{z'} = \\frac{cz}{c\\|z\\|_{2}} = \\frac{z}{\\|z\\|_{2}}\n$$\n右侧是原始向量 $z$ 的归一化 logits $\\hat{z}$ 的定义。因此，我们已经证明，对于任何非零的 $z \\in \\mathbb{R}^K$ 和任何 $c>0$：\n$$\n\\widehat{cz} = \\hat{z}\n$$\n这意味着归一化步骤使结果向量对于输入向量的正缩放是不变的。射线 $\\{cz \\mid c > 0\\}$ 上的所有向量都映射到单位超球面上的同一点 $\\hat{z}$。\n\n接下来，我们研究归一化交叉熵损失 $\\hat{L}(cz,y)$。函数 $\\hat{L}(\\cdot, y)$ 的定义表明，它首先对其输入向量进行归一化，然后计算交叉熵损失。因此，对于输入向量 $cz$，损失是使用归一化向量 $\\widehat{cz}$ 计算的。\n$$\n\\hat{L}(cz,y) = -\\ln\\left(\\frac{\\exp\\left((\\widehat{cz})_{y}\\right)}{\\sum_{j=1}^{K} \\exp\\left((\\widehat{cz})_{j}\\right)}\\right)\n$$\n由于我们发现 $\\widehat{cz} = \\hat{z}$，我们可以将其代入损失函数：\n$$\n\\hat{L}(cz,y) = -\\ln\\left(\\frac{\\exp\\left(\\hat{z}_{y}\\right)}{\\sum_{j=1}^{K} \\exp\\left(\\hat{z}_{j}\\right)}\\right)\n$$\n右侧的表达式正是 $\\hat{L}(z,y)$ 的定义。因此，我们得出结论：\n$$\n\\hat{L}(cz,y) = \\hat{L}(z,y)\n$$\n归一化交叉熵损失对于 logits 的正缩放是不变的。\n\n这对锐度的影响是直接的。缩放方向锐度 $S(z,y)$ 定义为损失函数关于缩放因子 $c$ 的二阶导数，在 $c=1$ 处求值。让我们为 $c>0$ 定义一个函数 $g(c) = \\hat{L}(cz, y)$。根据我们的推导，我们知道 $g(c) = \\hat{L}(z, y)$，对于固定的 $z$ 和 $y$，这是一个常数值。函数 $g(c)$ 不依赖于 $c$。\n\n常数函数的一阶导数为零：\n$$\n\\frac{d}{dc} g(c) = \\frac{d}{dc} [\\hat{L}(z,y)] = 0\n$$\n二阶导数也为零：\n$$\n\\frac{d^2}{dc^2} g(c) = \\frac{d}{dc} [0] = 0\n$$\n这对任何 $c>0$ 都成立。锐度是这个二阶导数在 $c=1$ 处的求值：\n$$\nS(z,y) = \\left.\\frac{d^{2}}{dc^{2}} \\hat{L}(c z,y)\\right|_{c=1} = 0\n$$\n这意味着归一化损失函数的损失景观在任何从原点出发的缩放方向上都是完全平坦的。锐度，作为曲率的度量，在这个方向上为零。这是归一化步骤的直接后果，该步骤移除了关于 logit 向量幅度的任何信息。\n\n最后，我们被要求计算在 $K=3$，$y=1$ 和 $z = (2, 0, -1)^{\\top}$ 的特定情况下的 $S(z,y)$ 值。\n我们关于 $S(z,y)=0$ 的推导是通用的，适用于任何类别数 $K$、任何真实类别索引 $y$ 和任何非零 logit 向量 $z$。给定的向量 $z = (2, 0, -1)^{\\top}$ 是非零的，因为其范数为 $\\|z\\|_2 = \\sqrt{2^2 + 0^2 + (-1)^2} = \\sqrt{4+0+1} = \\sqrt{5} \\neq 0$。\n因此，该通用结果直接适用于此特定情况，无需将 $z$、$K$ 或 $y$ 的数值代入任何复杂的表达式。结果完全由归一化损失的函数形式决定。\n\n因此，缩放方向锐度的值为 $0$。", "answer": "$$\n\\boxed{0}\n$$", "id": "3145403"}, {"introduction": "传统的模型通常预测一个有序的向量，但如果我们的目标是一个无序的集合，例如图像中的多个物体，我们该如何处理？本练习将指导你设计一个对排列不变的损失函数，其核心思想非常巧妙：通过寻找预测与目标之间的最佳“配对”来最小化一个总成本。这个过程 [@problem_id:3145459] 不仅让你掌握了现代深度学习中用于集合预测的强大技术，还揭示了一个重要的实际挑战——当最佳匹配发生改变时，梯度可能会出现不连续。", "problem": "考虑一个集合预测任务，其中模型输出一个包含两个元素的无序集合。每个元素由一个标量坐标和一个二元类别概率组成。无序的目标集合也包含两个元素，每个元素有一个标量坐标和一个二元类别标签。训练目标必须对预测集合中元素的顺序具有置换不变性。从集合没有内在顺序的原则以及旨在通过最小化总成本来将预测与目标配对的分配问题表述出发，通过结合用于分类的二元交叉熵 (CE) 和用于坐标回归的均方误差 (MSE) 来推導一个置换不变的损失。\n\n使用以下具体实例：\n- 预测元素 $1$ 的类别 $1$ 的概率为 $\\hat{p}_1 = 0.1$，坐标为 $\\hat{z}_1(t) = t$，其中 $t \\in \\mathbb{R}$ 是一个标量参数。\n- 预测元素 $2$ 的类别 $1$ 的概率为 $\\hat{p}_2 = 0.8$，坐标为 $\\hat{z}_2(t) = \\frac{6}{5}$。\n- 目标元素 $a$ 的类别标签为 $c_a = 0$，坐标为 $z_a^{\\star} = 0$。\n- 目标元素 $b$ 的类别标签为 $c_b = 1$，坐标为 $z_b^{\\star} = 1$。\n\n将每对损失定义为二元交叉熵 (CE) 分类项和由均方误差 (MSE) 给出（带有一个因子 $\\frac{1}{2}$）的坐标回归项之和：\n- 对于一个预测概率 $\\hat{p}$ 和目标标签 $c \\in \\{0,1\\}$，二元交叉熵为 $-\\big(c \\ln(\\hat{p}) + (1-c) \\ln(1-\\hat{p})\\big)$。\n- 对于一个预测坐标 $\\hat{z}$ 和目标坐标 $z^{\\star}$，回归项为 $\\frac{1}{2}(\\hat{z} - z^{\\star})^{2}$。\n\n任务：\n1. 使用分配问题的视角和集合的置换不变性定义，将两个预测到两个目标的所有分配中，两两配对损失之和的最小值，推导为置换不变的总损失 $L(t)$。\n2. 在所述的 $2 \\times 2$ 情况下，将 $L(t)$ 显式地写为两个候选总和（两种可能的一一对应分配）的最小值。\n3. 确定最优分配发生变化的临界值 $t^{\\star}$。\n4. 计算单侧导数 $\\frac{\\partial L}{\\partial t}\\big|_{t^{\\star}-0}$ 和 $\\frac{\\partial L}{\\partial t}\\big|_{t^{\\star}+0}$，然后报告在 $t^{\\star}$ 处梯度的不连续性的大小，即 $\\left|\\frac{\\partial L}{\\partial t}\\big|_{t^{\\star}+0} - \\frac{\\partial L}{\\partial t}\\big|_{t^{\\star}-0}\\right|$。\n\n将最终所求的量表示为精确值。无需四舍五入。最终答案必须是单个实数。", "solution": "该问题要求为集合预测任务构建一个置换不变的损失函数，并分析其在预测与目标之间的最优分配发生变化的点上的导数。总损失 $L(t)$ 被表述为一个分配问题的解，具体来说，是通过取所有可能的预测到目标的置换（一一对应分配）下的总成本的最小值。\n\n设预测集合为 $\\hat{Y} = \\{\\hat{y}_1, \\hat{y}_2\\}$，目标集合为 $Y^{\\star} = \\{y_a^{\\star}, y_b^{\\star}\\}$。\n预测给出如下：\n$\\hat{y}_1 = (\\hat{z}_1, \\hat{p}_1) = (t, 0.1)$\n$\\hat{y}_2 = (\\hat{z}_2, \\hat{p}_2) = (\\frac{6}{5}, 0.8)$\n\n目标给出如下：\n$y_a^{\\star} = (z_a^{\\star}, c_a) = (0, 0)$\n$y_b^{\\star} = (z_b^{\\star}, c_b) = (1, 1)$\n\n将一个预测 $\\hat{y} = (\\hat{z}, \\hat{p})$ 分配给一个目标 $y^{\\star} = (z^{\\star}, c)$ 的每对损失（或成本）是二元交叉熵 (CE) 项和均方误差 (MSE) 项的和：\n$C(\\hat{y}, y^{\\star}) = L_{cls}(\\hat{p}, c) + L_{reg}(\\hat{z}, z^{\\star})$\n其中 $L_{cls}(\\hat{p}, c) = -\\big(c \\ln(\\hat{p}) + (1-c) \\ln(1-\\hat{p})\\big)$ 且 $L_{reg}(\\hat{z}, z^{\\star}) = \\frac{1}{2}(\\hat{z} - z^{\\star})^{2}$。\n\n我们首先计算四种可能的每对成本，记作 $C_{ij}$，表示将预测 $i$ 分配给目标 $j$。\n\n1.  将 $\\hat{y}_1$ 分配给 $y_a^{\\star}$ 的成本 ($C_{1a}$)：\n    $L_{cls}(\\hat{p}_1, c_a) = -(0 \\cdot \\ln(0.1) + (1-0) \\cdot \\ln(1-0.1)) = -\\ln(0.9) = \\ln(\\frac{1}{0.9}) = \\ln(\\frac{10}{9})$。\n    $L_{reg}(\\hat{z}_1, z_a^{\\star}) = \\frac{1}{2}(t - 0)^2 = \\frac{1}{2}t^2$。\n    $C_{1a}(t) = \\ln(\\frac{10}{9}) + \\frac{1}{2}t^2$。\n\n2.  将 $\\hat{y}_2$ 分配给 $y_b^{\\star}$ 的成本 ($C_{2b}$)：\n    $L_{cls}(\\hat{p}_2, c_b) = -(1 \\cdot \\ln(0.8) + (1-1) \\cdot \\ln(1-0.8)) = -\\ln(0.8) = \\ln(\\frac{1}{0.8}) = \\ln(\\frac{5}{4})$。\n    $L_{reg}(\\hat{z}_2, z_b^{\\star}) = \\frac{1}{2}(\\frac{6}{5} - 1)^2 = \\frac{1}{2}(\\frac{1}{5})^2 = \\frac{1}{50}$。\n    $C_{2b} = \\ln(\\frac{5}{4}) + \\frac{1}{50}$。\n\n3.  将 $\\hat{y}_1$ 分配给 $y_b^{\\star}$ 的成本 ($C_{1b}$)：\n    $L_{cls}(\\hat{p}_1, c_b) = -(1 \\cdot \\ln(0.1) + (1-1) \\cdot \\ln(1-0.1)) = -\\ln(0.1) = \\ln(10)$。\n    $L_{reg}(\\hat{z}_1, z_b^{\\star}) = \\frac{1}{2}(t - 1)^2$。\n    $C_{1b}(t) = \\ln(10) + \\frac{1}{2}(t - 1)^2$。\n\n4.  将 $\\hat{y}_2$ 分配给 $y_a^{\\star}$ 的成本 ($C_{2a}$)：\n    $L_{cls}(\\hat{p}_2, c_a) = -(0 \\cdot \\ln(0.8) + (1-0) \\cdot \\ln(1-0.8)) = -\\ln(0.2) = \\ln(\\frac{1}{0.2}) = \\ln(5)$。\n    $L_{reg}(\\hat{z}_2, z_a^{\\star}) = \\frac{1}{2}(\\frac{6}{5} - 0)^2 = \\frac{1}{2} \\cdot \\frac{36}{25} = \\frac{18}{25}$。\n    $C_{2a} = \\ln(5) + \\frac{18}{25}$。\n\n对于一个 $2 \\times 2$ 的问题，有两种可能的一一对应分配（置换）。\n分配 1：$(\\hat{y}_1 \\to y_a^{\\star}, \\hat{y}_2 \\to y_b^{\\star})$。总损失为 $L_1(t) = C_{1a}(t) + C_{2b}$。\n$L_1(t) = \\left(\\ln(\\frac{10}{9}) + \\frac{1}{2}t^2\\right) + \\left(\\ln(\\frac{5}{4}) + \\frac{1}{50}\\right) = \\frac{1}{2}t^2 + \\ln(\\frac{10}{9}) + \\ln(\\frac{5}{4}) + \\frac{1}{50}$。\n\n分配 2：$(\\hat{y}_1 \\to y_b^{\\star}, \\hat{y}_2 \\to y_a^{\\star})$。总损失为 $L_2(t) = C_{1b}(t) + C_{2a}$。\n$L_2(t) = \\left(\\ln(10) + \\frac{1}{2}(t-1)^2\\right) + \\left(\\ln(5) + \\frac{18}{25}\\right) = \\frac{1}{2}(t-1)^2 + \\ln(10) + \\ln(5) + \\frac{18}{25}$。\n\n置换不变的总损失 $L(t)$ 是所有可能分配下的损失的最小值。\n$L(t) = \\min(L_1(t), L_2(t))$。这就完成了任务 1 和 2。\n\n为了找到最优分配发生变化的临界值 $t^{\\star}$（任务 3），我们令 $L_1(t) = L_2(t)$：\n$\\frac{1}{2}t^2 + \\ln(\\frac{10}{9}) + \\ln(\\frac{5}{4}) + \\frac{1}{50} = \\frac{1}{2}(t-1)^2 + \\ln(10) + \\ln(5) + \\frac{18}{25}$\n$\\frac{1}{2}t^2 + \\ln(10) - \\ln(9) + \\ln(5) - \\ln(4) + \\frac{1}{50} = \\frac{1}{2}(t^2 - 2t + 1) + \\ln(10) + \\ln(5) + \\frac{36}{50}$\n从两边消去公因子（$\\frac{1}{2}t^2, \\ln(10), \\ln(5)$）得到：\n$-\\ln(9) - \\ln(4) + \\frac{1}{50} = -t + \\frac{1}{2} + \\frac{36}{50}$\n$-\\ln(36) + \\frac{1}{50} = -t + \\frac{25}{50} + \\frac{36}{50}$\n$-\\ln(36) + \\frac{1}{50} = -t + \\frac{61}{50}$\n解出 $t$：\n$t^{\\star} = \\ln(36) + \\frac{61}{50} - \\frac{1}{50} = \\ln(36) + \\frac{60}{50} = \\ln(36) + \\frac{6}{5}$。\n\n对于任务 4，我们需要计算 $L(t)$ 在 $t^{\\star}$ 处的单侧导数以及梯度的不连续性的大小。\n两个候选损失关于 $t$ 的导数是：\n$\\frac{\\partial L_1}{\\partial t} = \\frac{\\partial}{\\partial t} \\left(\\frac{1}{2}t^2 + \\text{常数}\\right) = t$。\n$\\frac{\\partial L_2}{\\partial t} = \\frac{\\partial}{\\partial t} \\left(\\frac{1}{2}(t-1)^2 + \\text{常数}\\right) = \\frac{1}{2} \\cdot 2(t-1) \\cdot 1 = t-1$。\n\n为了确定最小函数的哪个分支是有效的，我们分析 $L_1(t) - L_2(t)$ 的符号。从 $t^{\\star}$ 的计算中，我们有：\n$L_1(t) - L_2(t) = t - (\\ln(36) + \\frac{6}{5}) = t - t^{\\star}$。\n- 如果 $t \\le t^{\\star}$，那么 $L_1(t) - L_2(t) \\le 0$，所以 $L_1(t) \\le L_2(t)$。因此，$L(t) = L_1(t)$。\n- 如果 $t > t^{\\star}$，那么 $L_1(t) - L_2(t) > 0$，所以 $L_2(t) < L_1(t)$。因此，$L(t) = L_2(t)$。\n\n总损失函数是分段的：\n$L(t) = \\begin{cases} L_1(t)  \\text{如果 } t \\le t^{\\star} \\\\ L_2(t)  \\text{如果 } t > t^{\\star} \\end{cases}$\n\n在 $t=t^{\\star}$ 处的单侧导数是：\n左导数是对于 $t \\le t^{\\star}$ 时的有效函数的导数：\n$\\frac{\\partial L}{\\partial t}\\bigg|_{t^{\\star}-0} = \\lim_{t \\to t^{\\star-}} \\frac{\\partial L}{\\partial t} = \\lim_{t \\to t^{\\star-}} \\frac{\\partial L_1}{\\partial t} = \\lim_{t \\to t^{\\star-}} t = t^{\\star}$。\n\n右导数是对于 $t > t^{\\star}$ 时的有效函数的导数：\n$\\frac{\\partial L}{\\partial t}\\bigg|_{t^{\\star}+0} = \\lim_{t \\to t^{\\star+}} \\frac{\\partial L}{\\partial t} = \\lim_{t \\to t^{\\star+}} \\frac{\\partial L_2}{\\partial t} = \\lim_{t \\to t^{\\star+}} (t-1) = t^{\\star}-1$。\n\n在 $t^{\\star}$ 处梯度的不連續性的大小是右导数和左导数之间的绝对差：\n$\\left|\\frac{\\partial L}{\\partial t}\\bigg|_{t^{\\star}+0} - \\frac{\\partial L}{\\partial t}\\bigg|_{t^{\\star}-0}\\right| = \\left| (t^{\\star}-1) - t^{\\star} \\right| = |-1| = 1$。", "answer": "$$\\boxed{1}$$", "id": "3145459"}, {"introduction": "在许多现实世界的任务中，我们希望模型能遵循特定的领域知识，例如“价格应随质量提高而上升”。本练习将向你展示如何通过在主损失函数中添加一个自定义的惩罚项来强制施加这类约束，特别是单调性。你将亲手实现一个组合目标函数，它在拟合数据的准确性和满足单调性约束之间进行权衡。这个编码实践 [@problem_id:3145476] 阐释了正则化这一强大思想：通过精心设计损失函数，并利用一个权衡参数 $\\lambda$，引导模型学习到的不仅是准确的，更是符合我们期望行为的解决方案。", "problem": "您面临一个标量回归任务，模型必须根据一个二维输入预测一个实值目标，同时鼓励模型对于每个输入坐标都表现出单调非递减的行为。您的目标是实现并评估一个组合目标函数，该函数在经验准确性与单调性违反惩罚之间进行权衡。\n\n定义与设置：\n- 考虑输入 $x \\in \\mathbb{R}^2$（写作 $x = (x_1, x_2)$）和目标 $y \\in \\mathbb{R}$。\n- 模型是线性的：$f_{\\theta}(x) = v^\\top x + c$，其中 $\\theta = (v, c)$，$v \\in \\mathbb{R}^2$，$c \\in \\mathbb{R}$。\n- 训练数据是一个确定性的网格：\n  - 令 $x_1$ 和 $x_2$ 的取值范围为集合 $\\{0, 0.1, 0.2, \\dots, 1.0\\}$，通过完全笛卡尔积生成 $N = 121$ 个点。\n  - 对于每个网格点，目标值定义为 $y = 2 x_1 - 1.5 x_2$。\n- 经验准确性项是在标准平方误差损失下的经验风险，对整个数据集取平均值：\n  $$L_{\\mathrm{mse}}(\\theta) = \\frac{1}{N} \\sum_{n=1}^{N} \\frac{1}{2}\\left(f_{\\theta}(x^{(n)}) - y^{(n)}\\right)^2.$$\n- 单调性惩罚通过惩罚负的输入偏导数来强制实现对两个坐标的非递减行为。由于模型是线性的，输入梯度是恒定的，即 $\\frac{\\partial f_{\\theta}}{\\partial x_i} = v_i$。该惩罚为\n  $$L_{\\mathrm{mono}}(\\theta) = \\sum_{i=1}^{2} \\max\\left(0, -\\frac{\\partial f_{\\theta}}{\\partial x_i}\\right) = \\sum_{i=1}^{2} \\max(0, -v_i).$$\n- 对于给定的非负权重 $\\lambda$，组合目标函数为\n  $$J_{\\lambda}(\\theta) = L_{\\mathrm{mse}}(\\theta) + \\lambda \\, L_{\\mathrm{mono}}(\\theta).$$\n\n您的任务：\n1. 从经验风险最小化的定义以及线性函数相对于其输入的梯度是常数这一事实出发，推导出 $J_{\\lambda}(\\theta)$ 相对于 $v$ 和 $c$ 的显式次梯度，这些次梯度足以实现批量梯度下降。您必须使用第一性原理来证明每一项，包括 $\\max(0, -v_i)$ 的次梯度。\n2. 实现一个批量梯度下降算法，以在上述指定的数据集上最小化 $J_{\\lambda}(\\theta)$（对于固定的 $\\lambda$），使用以下超参数：\n   - 初始化：$v = (0, 0)$ 和 $c = 0$，\n   - 学习率：$\\alpha = 0.05$，\n   - 迭代次数：$T = 2000$。\n3. 为了进行评估（与训练损失的定义不同），计算以下两项：\n   - 不含 $\\frac{1}{2}$ 因子的均方误差，\n     $$\\mathrm{MSE}(\\theta) = \\frac{1}{N} \\sum_{n=1}^{N} \\left(f_{\\theta}(x^{(n)}) - y^{(n)}\\right)^2,$$\n   - 单调性违反度量，\n     $$\\mathrm{Viol}(\\theta) = \\sum_{i=1}^{2} \\max(0, -v_i).$$\n4. 分析权衡关系：通过增加 $\\lambda$，优化过程会更侧重于单调性，并可能牺牲准确性。通过使用以下惩罚权重测试套件进行训练，以数值方式验证此效应：\n   - $\\lambda = 0$（无单调性压力），\n   - $\\lambda = 0.5$（中等），\n   - $\\lambda = 20$（强）。\n5. 程序输出格式：您的程序应生成单行输出，其中包含一个由逗号分隔的双元素列表组成的列表，格式如下：\n   - 输出必须是\n     $$\\big[\\,[\\mathrm{MSE}_{\\lambda_1}, \\mathrm{Viol}_{\\lambda_1}], [\\mathrm{MSE}_{\\lambda_2}, \\mathrm{Viol}_{\\lambda_2}], [\\mathrm{MSE}_{\\lambda_3}, \\mathrm{Viol}_{\\lambda_3}] \\,\\big],$$\n     其中 $\\lambda$ 值的顺序为 $\\lambda \\in \\{0, 0.5, 20\\}$。\n   - $\\mathrm{MSE}_{\\lambda}$ 和 $\\mathrm{Viol}_{\\lambda}$ 中的每一个都必须是实数。\n\n约束和说明：\n- 仅使用所提供的数据集构建方法和超参数。\n- 学习率 $\\alpha$、迭代次数 $T$ 和初始化是固定的，不得更改。\n- 本问题不涉及角度和物理单位；无需进行单位转换。\n- 确保您的代码是自包含和确定性的，并且只打印所要求的、格式完全符合规定的单行输出，不含任何额外文本。", "solution": "任务是实现并分析一个正则化线性回归模型。正则化项用于惩罚非单调性。这需要推导组合目标函数的次梯度，并实现一个批量次梯度下降算法。\n\n### 1. 问题构建与目标函数\n\n模型是一个线性函数 $f_{\\theta}(x) = v^\\top x + c$，其参数为 $\\theta = (v, c)$，其中 $v = (v_1, v_2)^\\top \\in \\mathbb{R}^2$，$c \\in \\mathbb{R}$。目标是最小化组合目标函数 $J_{\\lambda}(\\theta)$，该函数是均方误差损失和单调性惩罚的和：\n$$\nJ_{\\lambda}(\\theta) = L_{\\mathrm{mse}}(\\theta) + \\lambda \\, L_{\\mathrm{mono}}(\\theta)\n$$\n这里，$\\lambda \\ge 0$ 是一个控制惩罚强度的超参数。\n\n两个组成部分是：\n1.  **均方误差（MSE）损失**：经验风险定义为 $L_{\\mathrm{mse}}(\\theta) = \\frac{1}{N} \\sum_{n=1}^{N} \\frac{1}{2}\\left(f_{\\theta}(x^{(n)}) - y^{(n)}\\right)^2$。\n2.  **单调性惩罚**：惩罚项定义为 $L_{\\mathrm{mono}}(\\theta) = \\sum_{i=1}^{2} \\max\\left(0, -\\frac{\\partial f_{\\theta}}{\\partial x_i}\\right)$。\n\n对于给定的线性模型，关于输入的偏导数是常数，等于权重向量的分量，即 $\\frac{\\partial f_{\\theta}}{\\partial x_i} = v_i$。因此，惩罚项简化为关于 $v$ 的函数：\n$$\nL_{\\mathrm{mono}}(\\theta) = \\max(0, -v_1) + \\max(0, -v_2)\n$$\n\n### 2. 次梯度的推导\n\n为了使用批量梯度下降最小化 $J_{\\lambda}(\\theta)$，我们必须计算它关于参数 $v$ 和 $c$ 的（次）梯度。目标函数 $J_{\\lambda}(\\theta)$ 是凸函数，因为它是凸二次函数（$L_{\\mathrm{mse}}$）和另一个凸函数（$L_{\\mathrm{mono}}$，是合页损失函数的和）的和。然而，$L_{\\mathrm{mono}}$ 并非处处可微，特别是在 $v_1=0$ 或 $v_2=0$ 时。因此，我们必须使用次梯度的概念。\n\n多个凸函数之和的次微分是它们各自次微分的和。因此，$J_{\\lambda}(\\theta)$ 的次梯度集中的一个元素，记为 $g_\\theta \\in \\partial J_{\\lambda}(\\theta)$，可以通过将 MSE 项的梯度与惩罚项的次梯度相加得到：\n$$\ng_\\theta = \\nabla_{\\theta} L_{\\mathrm{mse}}(\\theta) + \\lambda \\, g_{\\mathrm{mono}}(\\theta)\n$$\n其中 $g_{\\mathrm{mono}}(\\theta) \\in \\partial L_{\\mathrm{mono}}(\\theta)$。我们将分别推导 $v$ 和 $c$ 的分量。\n\n#### 2.1. MSE 项的梯度\n\n损失项 $L_{\\mathrm{mse}}(\\theta)$ 是关于 $\\theta$ 的可微函数。我们计算其偏导数。\n关于向量 $v$ 的梯度是：\n$$\n\\nabla_v L_{\\mathrm{mse}}(\\theta) = \\frac{\\partial}{\\partial v} \\left( \\frac{1}{2N} \\sum_{n=1}^{N} (v^\\top x^{(n)} + c - y^{(n)})^2 \\right)\n$$\n使用链式法则：\n$$\n\\nabla_v L_{\\mathrm{mse}}(\\theta) = \\frac{1}{2N} \\sum_{n=1}^{N} 2(v^\\top x^{(n)} + c - y^{(n)}) \\cdot \\frac{\\partial}{\\partial v}(v^\\top x^{(n)})\n= \\frac{1}{N} \\sum_{n=1}^{N} (f_{\\theta}(x^{(n)}) - y^{(n)}) x^{(n)}\n$$\n关于标量偏置 $c$ 的梯度是：\n$$\n\\frac{\\partial L_{\\mathrm{mse}}(\\theta)}{\\partial c} = \\frac{\\partial}{\\partial c} \\left( \\frac{1}{2N} \\sum_{n=1}^{N} (v^\\top x^{(n)} + c - y^{(n)})^2 \\right)\n$$\n使用链式法则：\n$$\n\\frac{\\partial L_{\\mathrm{mse}}(\\theta)}{\\partial c} = \\frac{1}{2N} \\sum_{n=1}^{N} 2(v^\\top x^{(n)} + c - y^{(n)}) \\cdot 1\n= \\frac{1}{N} \\sum_{n=1}^{N} (f_{\\theta}(x^{(n)}) - y^{(n)})\n$$\n\n#### 2.2. 单调性惩罚的次梯度\n\n惩罚项 $L_{\\mathrm{mono}}(\\theta) = \\max(0, -v_1) + \\max(0, -v_2)$ 仅依赖于 $v$。因此，它关于 $c$ 的次梯度为零：\n$$\n\\partial_c L_{\\mathrm{mono}}(\\theta) = \\{0\\}\n$$\n为了求得关于 $v$ 的次梯度，我们分析函数 $h(z) = \\max(0, -z)$。其次微分 $\\partial h(z)$ 由下式给出：\n$$\n\\partial h(z) =\n\\begin{cases}\n    \\{-1\\}        \\text{if } z  0 \\\\\n    [-1, 0]       \\text{if } z = 0 \\\\\n    \\{0\\}         \\text{if } z  0\n\\end{cases}\n$$\n对于次梯度下降，我们需要为每个分量从此集合中选择一个元素。一个常见且有效的选择是：\n$$\ng_h(z) =\n\\begin{cases}\n    -1        \\text{if } z  0 \\\\\n    0         \\text{if } z \\ge 0\n\\end{cases}\n$$\n惩罚项在 $v_1$ 和 $v_2$ 上是可分的。因此，$L_{\\mathrm{mono}}(\\theta)$ 关于 $v$ 的一个次梯度是一个向量 $g_v = (g_{v_1}, g_{v_2})^\\top$，其中每个分量都选自相应项的次微分，即 $g_{v_i} \\in \\partial_{v_i} \\max(0, -v_i)$。使用我们选择的 $g_h$ 形式，我们得到一个特定的次梯度向量：\n$$\ng_{v} = \\begin{pmatrix} g_h(v_1) \\\\ g_h(v_2) \\end{pmatrix}, \\quad \\text{其中 } g_h(v_i) = \\begin{cases} -1  \\text{if } v_i  0 \\\\ 0  \\text{if } v_i \\ge 0 \\end{cases}\n$$\n\n#### 2.3. 组合次梯度\n\n结合这些结果，可以构建完整目标 $J_{\\lambda}(\\theta)$ 的一个有效次梯度。次微分 $\\partial_v J_{\\lambda}(\\theta)$ 的一个元素 $g_v$ 是：\n$$\ng_v = \\nabla_v L_{\\mathrm{mse}}(\\theta) + \\lambda \\, g_v^{\\mathrm{mono}}\n= \\frac{1}{N} \\sum_{n=1}^{N} (f_{\\theta}(x^{(n)}) - y^{(n)}) x^{(n)} + \\lambda \\begin{pmatrix}\ng_h(v_1) \\\\\ng_h(v_2)\n\\end{pmatrix}\n$$\n关于 $c$ 的梯度是：\n$$\n\\frac{\\partial J_{\\lambda}(\\theta)}{\\partial c} = \\frac{\\partial L_{\\mathrm{mse}}(\\theta)}{\\partial c} + \\lambda \\cdot 0\n= \\frac{1}{N} \\sum_{n=1}^{N} (f_{\\theta}(x^{(n)}) - y^{(n)})\n$$\n\n### 3. 批量次梯度下降算法\n\n批量次梯度下降算法通过迭代更新参数 $\\theta = (v, c)$ 来最小化 $J_{\\lambda}(\\theta)$。从初始猜测 $\\theta^{(0)}$ 开始，每次迭代 $k$ 使用学习率 $\\alpha$ 执行以下更新：\n$$\nv^{(k+1)} = v^{(k)} - \\alpha \\, g_v^{(k)}\n$$\n$$\nc^{(k+1)} = c^{(k)} - \\alpha \\, \\frac{\\partial J_{\\lambda}(\\theta^{(k)})}{\\partial c}\n$$\n其中 $g_v^{(k)}$ 是在当前参数 $\\theta^{(k)}=(v^{(k)}, c^{(k)})$ 下评估的关于 $v$ 的次梯度。这些更新会执行固定的迭代次数。实现将使用推导出的次梯度、指定的数据集以及提供的超参数。", "answer": "[[3.791583095034336e-29,1.499999999999991],[0.08182932220456209,0.0],[0.725,0.0]]", "id": "3145476"}]}