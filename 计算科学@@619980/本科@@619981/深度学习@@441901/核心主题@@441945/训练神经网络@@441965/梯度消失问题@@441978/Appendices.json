{"hands_on_practices": [{"introduction": "要想真正理解梯度消失问题，最好的方法莫过于亲手构建一个最简模型来观察它的发生。本练习将指导你构建一个极简的循环计算任务，该任务要求模型“记住”很久以前的输入信息。通过解析推导和经验性仿真，你将亲眼见证梯度信号在时间上传播时如何呈指数级衰减，从而直观地理解为何学习长期依赖如此困难。[@problem_id:3194489]", "problem": "要求您在一个编码长期依赖的循环计算中，对梯度消失现象进行形式化和经验性验证。考虑一个由递推关系 $h_t = w \\, h_{t-1} + x_t$ 定义的标量、时间展开的循环计算，其初始条件为 $h_0 = 0$，输出为 $y_t = h_t$。其中 $w \\in \\mathbb{R}$ 是一个可训练的标量参数，$\\{x_t\\}$ 是一个标量输入序列。您将构建一个人工合成的依赖关系，其中时间 $t$ 的监督信号依赖于 $T$ 个时间步之前出现的输入 $x_{t-T}$。在时间 $t$ 的训练损失为 $L_t = \\tfrac{1}{2} \\, (y_t - \\mathrm{target}_t)^2$。在此设置中，$L_t$ 相对于参数 $w$ 的梯度是通过随时间反向传播获得的，该方法在 $T$ 个时间步上重复应用链式法则。\n\n您可以使用的基本原理：\n- 微积分中的链式法则：对于可微函数 $f \\circ g$ 的复合，有 $\\frac{d}{dx} f(g(x)) = f'(g(x)) \\, g'(x)$。\n- 标量递推的随时间反向传播的定义：如果 $\\delta_t = \\frac{\\partial L_t}{\\partial h_t}$，那么对于上述线性递推关系，$\\delta_{t-1} = \\delta_t \\, w$，并且每个时间步的参数梯度贡献为 $\\frac{\\partial L_t}{\\partial w}\\bigg|_{\\text{at step }k} = \\delta_k \\, h_{k-1}$。\n\n您的程序必须纯粹在数学上完成以下任务：\n1. 构造一个长度为 $T+1$ 的序列 $\\{x_t\\}$，其中 $x_1 = 1$ 且对于所有 $t \\neq 1$ 有 $x_t = 0$。这是一个在时间 $t=1$ 时的单位脉冲。对于每个固定的延迟 $T$，定义监督时间索引 $t^\\star = T+1$ 和目标 $\\mathrm{target}_{t^\\star} = x_{t^\\star - T} = x_1 = 1$；因此损失为 $L_{t^\\star} = \\tfrac{1}{2}(y_{t^\\star} - 1)^2$。\n2. 对于每个选定的 $T$，计算前向传播以获得 $h_{t^\\star}$，然后设置 $\\delta_{t^\\star} = \\frac{\\partial L_{t^\\star}}{\\partial h_{t^\\star}} = h_{t^\\star} - 1$，接着使用 $\\delta_{k-1} = w \\, \\delta_k$ 将误差反向传播 $T$ 步以获得 $\\delta_{t^\\star - T}$。定义比率 $r(T) = \\left|\\delta_{t^\\star - T}\\right| \\big/ \\left|\\delta_{t^\\star}\\right|$。该比率分离了梯度信号跨越 $T$ 步传输的纯乘法效应。\n3. 对于固定的 $w$ 且 $|w| < 1$，用指数函数 $r(T) \\approx C \\, \\lambda^T$ 对 $r(T)$ 进行建模，并通过对 $\\log r(T)$ 与 $T$ 进行最小二乘拟合到仿射模型 $\\log r(T) \\approx a \\, T + b$ 来估计 $\\lambda$。估计值为 $\\hat{\\lambda} = e^a$。这经验性地捕捉了反向传播信号的 $O(\\lambda^T)$ 衰减率。\n4. 在一个小的参数值测试套件上重复该估计，以验证对不同情况的覆盖，包括由于负 $w$ 引起的符号振荡情况和 $w$ 的绝对值接近 $1$ 时的慢衰减情况。\n\n您的解决方案中需要展示的分析要求：\n- 从第一性原理（链式法则和线性递推）推导出，反向传播的误差满足 $\\delta_{t^\\star - k} = w^k \\, \\delta_{t^\\star}$（对于 $k \\in \\{1,\\dots,T\\}$），因此 $r(T) = |w|^T$。然后论证在时间 $t^\\star - T$ 时对 $w$ 的梯度的单步贡献是 $\\delta_{t^\\star - T} \\, h_{t^\\star - T - 1}$，并且因为在指定的脉冲输入下，对于 $|w|1$ 的情况 $|h_{t^\\star - T - 1}|$ 是有界的，所以该贡献的量级是 $O(|w|^T)$。结论是，梯度信号随深度呈几何级数衰减，其底为 $\\lambda = |w|  1$。\n\n测试套件：\n- 使用四种情况，其中 $w \\in \\{0.2, 0.5, -0.8, 0.95\\}$。\n- 对每种情况，计算在 $T \\in \\{5, 6, \\dots, 80\\}$ 范围内的每个整数 $T$ 的 $r(T)$，并如上所述拟合 $\\hat{\\lambda}$。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含一个逗号分隔的四个估计值 $\\hat{\\lambda}$ 的列表，其顺序与测试套件中的 $w$ 值相同，并用方括号括起来。为了确定性，将每个估计值四舍五入到六位小数。例如，一个带有占位符的输出看起来像 $[0.123456,0.234567,0.345678,0.456789]$。此任务不涉及任何物理单位或角度。", "solution": "该问题要求在一个简单的时间展开的循环计算中，对梯度消失现象进行解析推导和经验验证。\n\n### 1. 问题形式化与解析推导\n\n我们给定一个由递推关系定义的标量循环计算：\n$$\nh_t = w \\, h_{t-1} + x_t\n$$\n其初始条件为 $h_0 = 0$。参数 $w \\in \\mathbb{R}$ 是可学习的，$\\{x_t\\}$ 是一个输入序列。输出为 $y_t = h_t$。\n\n这个人工合成任务使用一个长度为 $T+1$ 的脉冲输入序列，其中 $x_1 = 1$ 且对于所有 $t \\neq 1$ 有 $x_t = 0$。监督发生在时间步 $t^\\star = T+1$，其目标为 $\\mathrm{target}_{t^\\star} = x_{t^\\star - T} = x_1 = 1$。在此时间步的损失函数为：\n$$\nL_{t^\\star} = \\frac{1}{2} (y_{t^\\star} - \\mathrm{target}_{t^\\star})^2 = \\frac{1}{2} (h_{t^\\star} - 1)^2\n$$\n\n**前向传播分析**\n我们可以对给定的输入序列展开递推关系，以找到 $h_t$ 的表达式：\n- 对于 $t=1$：$h_1 = w h_0 + x_1 = w \\cdot 0 + 1 = 1$。\n- 对于 $t=2$：$h_2 = w h_1 + x_2 = w \\cdot 1 + 0 = w$。\n- 对于 $t=3$：$h_3 = w h_2 + x_3 = w \\cdot w + 0 = w^2$。\n通过归纳法，对于任意时间步 $k \\ge 1$，隐藏状态为 $h_k = w^{k-1}$。\n在监督时刻 $t^\\star = T+1$，隐藏状态为 $h_{t^\\star} = h_{T+1} = w^{(T+1)-1} = w^T$。\n\n**反向传播分析（随时间反向传播）**\n损失相对于参数 $w$ 的梯度是通过随时间应用链式法则来找到的。该过程从计算损失相对于输出状态 $h_{t^\\star}$ 的梯度开始。令 $\\delta_k = \\frac{\\partial L_{t^\\star}}{\\partial h_k}$ 表示在时间步 $k$ 的误差信号。\n\n在时间 $t^\\star$ 的初始误差信号为：\n$$\n\\delta_{t^\\star} = \\frac{\\partial L_{t^\\star}}{\\partial h_{t^\\star}} = \\frac{\\partial}{\\partial h_{t^\\star}} \\left( \\frac{1}{2} (h_{t^\\star} - 1)^2 \\right) = h_{t^\\star} - 1 = w^T - 1\n$$\n问题提供了将此误差信号随时间反向传播的规则：$\\delta_{k-1} = \\frac{\\partial L_{t^\\star}}{\\partial h_{k-1}} = \\frac{\\partial L_{t^\\star}}{\\partial h_k} \\frac{\\partial h_k}{\\partial h_{k-1}} = \\delta_k \\cdot w$。这是一个几何级数。我们可以推导出误差信号 $\\delta_{t^\\star-k}$ 在反向传播 $k$ 步后的闭式表达式：\n- 对于 $k=1$：$\\delta_{t^\\star-1} = w \\, \\delta_{t^\\star}$。\n- 对于 $k=2$：$\\delta_{t^\\star-2} = w \\, \\delta_{t^\\star-1} = w (w \\, \\delta_{t^\\star}) = w^2 \\delta_{t^\\star}$。\n通过归纳法，对于任意整数 $k \\in \\{1, \\dots, T\\}$，误差信号为：\n$$\n\\delta_{t^\\star-k} = w^k \\delta_{t^\\star}\n$$\n这表明误差信号的量级在反向传播的每一步都被缩放了因子 $w$。\n\n比率 $r(T)$ 的定义是为了在整个依赖长度 $T$ 上分离出这种乘法效应：\n$$\nr(T) = \\frac{|\\delta_{t^\\star-T}|}{|\\delta_{t^\\star}|}\n$$\n使用我们推导出的反向传播误差公式，当 $k=T$ 时，我们得到 $\\delta_{t^\\star-T} = w^T \\delta_{t^\\star}$。将此代入 $r(T)$ 的定义中：\n$$\nr(T) = \\frac{|w^T \\delta_{t^\\star}|}{|\\delta_{t^\\star}|} = |w^T| = |w|^T\n$$\n这个解析结果表明，反向传播的误差信号与初始误差信号之比随时间延迟 $T$ 呈指数衰减，指数的底为 $|w|$。对于 $|w|  1$，当 $T$ 增加时，该比率趋于零，这就是梯度消失问题的本质。\n\n**梯度贡献分析**\n损失相对于权重 $\\frac{\\partial L_{t^\\star}}{\\partial w}$ 的总梯度是计算图中每个时间步贡献的总和：$\\frac{\\partial L_{t^\\star}}{\\partial w} = \\sum_{k=1}^{t^\\star} \\frac{\\partial L_{t^\\star}}{\\partial w}\\big|_{\\text{at step }k}$。单步贡献由 $\\delta_k h_{k-1}$ 给出。问题要求分析在时间 $t^\\star - T = (T+1) - T = 1$ 的贡献。\n在时间步 $k=1$ 的贡献是 $\\delta_1 h_0$。\n根据反向传播分析，我们有 $\\delta_1 = \\delta_{t^\\star - T} = w^T \\delta_{t^\\star}$。给定的初始条件是 $h_0 = 0$。\n因此，这一步的贡献是 $\\delta_1 h_0 = (w^T \\delta_{t^\\star}) \\cdot 0 = 0$。\n这个贡献的量级是 $0$，这自然满足了 $O(|w|^T)$ 的条件。更一般地，任何梯度贡献项 $\\delta_k h_{k-1}$ 都包含因子 $\\delta_k = w^{t^\\star-k} \\delta_{t^\\star}$。对于源于长时间延迟（即小的 $k$）的贡献，项 $w^{t^\\star-k}$ 将是 $w$ 的高次幂，当 $|w|  1$ 时，会导致一个指数级小的值。这就是“消失的梯度信号”，它使得模型难以学习长期依赖。\n\n### 2. 经验验证\n\n解析结果 $r(T) = |w|^T$ 是 $T$ 的一个指数函数。为了经验性地验证这一点并估计衰减基数 $\\lambda = |w|$，我们可以通过取对数来线性化该关系：\n$$\n\\log r(T) = \\log(|w|^T) = T \\log|w|\n$$\n这个方程是直线形式 $y = aT + b$，其中因变量是 $y = \\log r(T)$，自变量是 $T$，斜率是 $a = \\log|w|$，截距是 $b=0$。\n\n我们将对每个给定的 $w$ 值执行以下步骤：\n1. 对于 $T \\in \\{5, 6, \\dots, 80\\}$，使用推导出的公式 $r(T) = |w|^T$ 生成数据对 $(T, \\log r(T))$。\n2. 对这些数据点执行线性最小二乘回归，以拟合模型 $\\log r(T) \\approx aT + b$。这将得到斜率的估计值 $\\hat{a}$。\n3. 根据关系 $a = \\log \\lambda$，我们可以估计衰减基数为 $\\hat{\\lambda} = e^{\\hat{a}}$。\n根据我们的分析，我们预期经验估计值 $\\hat{\\lambda}$ 将非常接近理论值 $|w|$。所提供的实现将对指定的 $w$ 值测试套件执行此估计。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Analyzes the vanishing gradient problem for a simple recurrent computation.\n\n    For different values of a weight parameter 'w', this function simulates\n    the decay of a gradient signal over a time lag 'T'. It then performs a\n    log-linear regression to empirically estimate the decay rate lambda, which is\n    theoretically equal to |w|.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        # w values for the recurrence h_t = w * h_{t-1} + x_t\n        0.2,\n        0.5,\n        -0.8,\n        0.95,\n    ]\n\n    # Define the range of time lags T for the analysis.\n    T_values = np.arange(5, 81)\n    \n    # List to store the estimated decay rates.\n    estimated_lambdas = []\n\n    for w in test_cases:\n        # 1. Analytically determine the ratio r(T).\n        # From the derivation, r(T) = |w|^T.\n        # We handle the case w=0 separately to avoid log(0).\n        if w == 0:\n            # For w=0, the gradient signal is always zero for T1, so decay is immediate.\n            # The base lambda is technically 0.\n            estimated_lambdas.append(0.0)\n            continue\n            \n        r_T = np.abs(w) ** T_values\n        \n        # 2. Linearize the model by taking the logarithm.\n        # log(r(T)) = log(|w|^T) = T * log|w|.\n        # This is the form y = a*x, where y=log(r(T)), x=T, a=log|w|.\n        log_r_T = np.log(r_T)\n        \n        # 3. Perform a linear least-squares fit to estimate the slope 'a'.\n        # We fit the model y = a*x + b to the data (T, log r(T)).\n        # The matrix 'A' sets up the system of linear equations for the fit.\n        A = np.vstack([T_values, np.ones_like(T_values)]).T\n        \n        # `np.linalg.lstsq` solves the equation Ax = y for x, where x = [a, b].\n        # It returns the solution that minimizes the Euclidean 2-norm ||y - Ax||^2.\n        # The slope 'a' is the first element of the solution vector.\n        slope, _ = np.linalg.lstsq(A, log_r_T, rcond=None)[0]\n        \n        # 4. Estimate lambda from the slope.\n        # Since slope 'a' is an estimate of log(|w|) = log(lambda),\n        # lambda can be estimated by exponentiating the slope.\n        lambda_hat = np.exp(slope)\n        \n        # 5. Store the result, rounded to the specified precision.\n        estimated_lambdas.append(round(lambda_hat, 6))\n\n    # Final print statement in the exact required format.\n    # e.g., [0.200000,0.500000,0.800000,0.950000]\n    print(f\"[{','.join(map(str, estimated_lambdas))}]\")\n\nsolve()\n```", "id": "3194489"}, {"introduction": "梯度消失不仅存在于深度网络或长序列中，有时不恰当的设计甚至会在网络的输出层引发同样的问题。本练习将引导你进行一次理论推导，比较均方误差（MSE）损失函数与交叉熵（Cross-Entropy）损失函数在搭配sigmoid激活函数时的不同表现。你将发现，为何“错误”的组合会在模型“自信地犯错”时导致学习停滞，而“正确”的组合却能提供稳健的学习信号。[@problem_id:3194463]", "problem": "考虑一个二元分类神经元，其预激活值为 $z \\in \\mathbb{R}$，激活值为 $a = \\sigma(z)$，其中 $\\sigma(z) = \\dfrac{1}{1 + e^{-z}}$，目标值为 $y \\in \\{0, 1\\}$。在输出层使用两种常见的损失函数：均方误差（MSE）和交叉熵。均方误差（MSE）损失定义为 $L_{\\text{MSE}} = \\dfrac{1}{2} (a - y)^{2}$。交叉熵损失定义为 $L_{\\text{CE}} = - \\left[ y \\log(a) + (1 - y) \\log(1 - a) \\right]$。仅使用上述定义和基础微积分（特别是链式法则），推导出关于 $a$、$y$ 和 $z$ 的 $\\dfrac{\\partial L_{\\text{MSE}}}{\\partial z}$ 和 $\\dfrac{\\partial L_{\\text{CE}}}{\\partial z}$ 的表达式。然后分析这些梯度在 $|z|$ 很大时的饱和区域中的行为。你的推导过程应从 $\\sigma(z)$ 的精确定义开始，而不是假设任何预先引用的导数公式。根据你的推导和分析，以下哪个陈述最为准确？\n\nA. 使用均方误差（MSE）时，关于 $z$ 的梯度形式为 $(\\sigma(z) - y)\\,\\sigma^{\\prime}(z)$，因此当 $|z|$ 很大时，由于 $\\sigma^{\\prime}(z) \\approx 0$，梯度趋于消失。使用交叉熵时，梯度简化为 $\\sigma(z) - y$，它不包含额外的因子 $\\sigma^{\\prime}(z)$，使得当 $|z|$ 很大时，梯度消失问题不那么严重。\n\nB. 使用交叉熵时，关于 $z$ 的梯度形式为 $(\\sigma(z) - y)\\,\\sigma^{\\prime}(z)$，这使得梯度消失问题比使用均方误差（MSE）时更严重，而均方误差的梯度简化为 $\\sigma(z) - y$。\n\nC. 均方误差（MSE）和交叉熵产生相同的关于 $z$ 的梯度，即 $\\sigma(z) - y$，因此当 $|z|$ 很大时，梯度消失行为没有差异。\n\nD. 当 $|z|$ 很大时，导数 $\\sigma^{\\prime}(z)$ 近似为常数，因此均方误差（MSE）和交叉熵都不会在输出层遇到梯度消失问题。\n\n选择唯一的最佳选项。", "solution": "在尝试解答之前，将首先验证问题陈述的科学合理性、自洽性和清晰度。\n\n### 步骤 1：提取已知条件\n-   考虑一个二元分类神经元。\n-   预激活值：$z \\in \\mathbb{R}$。\n-   激活函数：$a = \\sigma(z)$，其中 $\\sigma(z) = \\dfrac{1}{1 + e^{-z}}$。\n-   目标标签：$y \\in \\{0, 1\\}$。\n-   均方误差（MSE）损失：$L_{\\text{MSE}} = \\dfrac{1}{2} (a - y)^{2}$。\n-   交叉熵（CE）损失：$L_{\\text{CE}} = - \\left[ y \\log(a) + (1 - y) \\log(1 - a) \\right]$。\n-   任务是推导梯度 $\\dfrac{\\partial L_{\\text{MSE}}}{\\partial z}$ 和 $\\dfrac{\\partial L_{\\text{CE}}}{\\partial z}$，并分析它们在 $|z|$ 较大时的行为。\n-   推导必须仅使用给定的定义和基础微积分，而不假设预先引用的导数公式。\n\n### 步骤 2：使用提取的已知条件进行验证\n问题陈述是有效的。\n-   **科学依据：** sigmoid 激活函数、均方误差损失和二元交叉熵损失的定义是机器学习和神经网络领域的标准和基础。该问题探讨了公认的梯度消失现象。\n-   **适定性：** 问题定义清晰，并提供了推导所需量和进行分析的所有必要信息。通过标准微积分可以得到唯一确定的答案。\n-   **客观性：** 问题使用精确的数学语言进行表述，没有歧义或主观看法。\n\n该问题不违反任何无效性标准。这是一个形式良好、标准的关于神经网络组件微分的练习。\n\n### 步骤 3：推导与分析\n\n问题的核心是应用链式法则来求损失函数 $L$ 相对于预激活值 $z$ 的梯度。依赖链为 $z \\rightarrow a \\rightarrow L$。因此，梯度的一般形式是：\n$$\n\\frac{\\partial L}{\\partial z} = \\frac{\\partial L}{\\partial a} \\cdot \\frac{\\partial a}{\\partial z}\n$$\n项 $\\frac{\\partial a}{\\partial z}$ 是激活函数的导数，即 $\\sigma'(z)$。我们必须按照规定从第一性原理推导它。\n\n**1. Sigmoid 函数的导数**\n\nsigmoid 函数为 $\\sigma(z) = \\dfrac{1}{1 + e^{-z}} = (1 + e^{-z})^{-1}$。\n使用幂法则和链式法则，其关于 $z$ 的导数是：\n$$\n\\frac{\\partial a}{\\partial z} = \\sigma'(z) = \\frac{d}{dz} (1 + e^{-z})^{-1} = -1 \\cdot (1 + e^{-z})^{-2} \\cdot \\frac{d}{dz}(1 + e^{-z})\n$$\n内层项的导数是 $\\frac{d}{dz}(1 + e^{-z}) = -e^{-z}$。将其代回，我们得到：\n$$\n\\sigma'(z) = - (1 + e^{-z})^{-2} \\cdot (-e^{-z}) = \\frac{e^{-z}}{(1 + e^{-z})^2}\n$$\n为了用更常见的形式表示它，我们可以对表达式进行变换：\n$$\n\\sigma'(z) = \\left( \\frac{1}{1 + e^{-z}} \\right) \\cdot \\left( \\frac{e^{-z}}{1 + e^{-z}} \\right)\n$$\n第一项是 $\\sigma(z)$。对于第二项，我们可以写成：\n$$\n\\frac{e^{-z}}{1 + e^{-z}} = \\frac{1 + e^{-z} - 1}{1 + e^{-z}} = 1 - \\frac{1}{1 + e^{-z}} = 1 - \\sigma(z)\n$$\n因此，sigmoid 函数的导数是：\n$$\n\\sigma'(z) = \\sigma(z) (1 - \\sigma(z))\n$$\n由于 $a = \\sigma(z)$，我们也可以将其写为 $\\dfrac{\\partial a}{\\partial z} = a(1-a)$。\n\n**2. 均方误差（MSE）损失的梯度**\n\nMSE 损失是 $L_{\\text{MSE}} = \\dfrac{1}{2} (a - y)^{2}$。\n首先，我们求损失函数关于激活值 $a$ 的导数：\n$$\n\\frac{\\partial L_{\\text{MSE}}}{\\partial a} = \\frac{\\partial}{\\partial a} \\left[ \\frac{1}{2} (a - y)^{2} \\right] = 2 \\cdot \\frac{1}{2} (a - y) \\cdot 1 = a - y\n$$\n现在，应用链式法则求关于 $z$ 的梯度：\n$$\n\\frac{\\partial L_{\\text{MSE}}}{\\partial z} = \\frac{\\partial L_{\\text{MSE}}}{\\partial a} \\cdot \\frac{\\partial a}{\\partial z} = (a - y) \\cdot \\sigma'(z)\n$$\n代入 $\\sigma'(z)$ 的表达式：\n$$\n\\frac{\\partial L_{\\text{MSE}}}{\\partial z} = (a - y) a (1-a)\n$$\n或者，用 $\\sigma(z)$ 表示：\n$$\n\\frac{\\partial L_{\\text{MSE}}}{\\partial z} = (\\sigma(z) - y) \\sigma'(z)\n$$\n\n**3. 交叉熵（CE）损失的梯度**\n\nCE 损失是 $L_{\\text{CE}} = - \\left[ y \\log(a) + (1 - y) \\log(1 - a) \\right]$。\n首先，我们求损失函数关于激活值 $a$ 的导数：\n$$\n\\frac{\\partial L_{\\text{CE}}}{\\partial a} = \\frac{\\partial}{\\partial a} \\left[ -y \\log(a) - (1-y) \\log(1-a) \\right] = - \\left[ \\frac{y}{a} + (1-y) \\frac{-1}{1-a} \\right]\n$$\n$$\n\\frac{\\partial L_{\\text{CE}}}{\\partial a} = -\\frac{y}{a} + \\frac{1-y}{1-a} = \\frac{-y(1-a) + a(1-y)}{a(1-a)} = \\frac{-y + ay + a - ay}{a(1-a)} = \\frac{a-y}{a(1-a)}\n$$\n现在，应用链式法则求关于 $z$ 的梯度：\n$$\n\\frac{\\partial L_{\\text{CE}}}{\\partial z} = \\frac{\\partial L_{\\text{CE}}}{\\partial a} \\cdot \\frac{\\partial a}{\\partial z} = \\left( \\frac{a-y}{a(1-a)} \\right) \\cdot (a(1-a))\n$$\n$a(1-a)$ 项相互抵消，留下一个非常简洁的结果：\n$$\n\\frac{\\partial L_{\\text{CE}}}{\\partial z} = a - y\n$$\n或者，用 $\\sigma(z)$ 表示：\n$$\n\\frac{\\partial L_{\\text{CE}}}{\\partial z} = \\sigma(z) - y\n$$\n\n**4. 梯度行为分析**\n\n问题要求分析在 $|z|$ 很大的饱和区域中的行为。\n-   当 $z \\to \\infty$，$e^{-z} \\to 0$，所以 $a = \\sigma(z) \\to \\dfrac{1}{1+0} = 1$。\n-   当 $z \\to -\\infty$，$e^{-z} \\to \\infty$，所以 $a = \\sigma(z) \\to \\dfrac{1}{1+\\infty} \\to 0$。\n\n让我们在这些区域中考察 sigmoid 的导数 $\\sigma'(z) = a(1-a)$：\n-   当 $z \\to \\infty$，$a \\to 1$，所以 $\\sigma'(z) \\to 1(1-1) = 0$。\n-   当 $z \\to -\\infty$，$a \\to 0$，所以 $\\sigma'(z) \\to 0(1-0) = 0$。\n在两个饱和区域中，$\\sigma'(z)$ 都趋近于零。\n\n现在，让我们分析这两个损失的梯度：\n-   **MSE 梯度：** $\\dfrac{\\partial L_{\\text{MSE}}}{\\partial z} = (a - y) \\sigma'(z)$。因为这个梯度与 $\\sigma'(z)$ 成正比，当 $|z|$ 变大时，无论误差 $(a-y)$ 的大小如何，它都将趋近于 $0$。这就是**梯度消失问题**。例如，如果目标是 $y=0$ 但神经元在 $z \\gg 0$ 时饱和（因此 $a \\approx 1$），误差 $(a-y)$ 很大（$\\approx 1$），但梯度会接近于 $0$，因为 $\\sigma'(z) \\approx 0$。学习将会极其缓慢。\n\n-   **CE 梯度：** $\\dfrac{\\partial L_{\\text{CE}}}{\\partial z} = a - y$。这个梯度*不*包含 $\\sigma'(z)$ 项。让我们考虑同样的“置信地错误”的情况：$y=0$ 且 $a \\approx 1$。梯度约为 $1-0=1$，这是一个更新参数的强信号。如果 $y=1$ 且 $a \\approx 0$，梯度约为 $0-1 = -1$，同样是一个强信号。只有当预测正确时，即 $a \\approx y$ 时，梯度才会变小。因此，交叉熵损失与 sigmoid 激活函数的组合在很大程度上缓解了输出层的梯度消失问题。\n\n### 逐项评估选项\n\n**A. 使用均方误差（MSE）时，关于 $z$ 的梯度形式为 $(\\sigma(z) - y)\\,\\sigma^{\\prime}(z)$，因此当 $|z|$ 很大时，由于 $\\sigma^{\\prime}(z) \\approx 0$，梯度趋于消失。使用交叉熵时，梯度简化为 $\\sigma(z) - y$，它不包含额外的因子 $\\sigma^{\\prime}(z)$，使得当 $|z|$ 很大时，梯度消失问题不那么严重。**\n-   我们对 MSE 的推导得出 $\\dfrac{\\partial L_{\\text{MSE}}}{\\partial z} = (\\sigma(z)-y)\\sigma'(z)$。我们的分析证实，由于 $\\sigma'(z)$ 项，当 $|z|$ 很大时，梯度会消失。这部分是正确的。\n-   我们对 CE 的推导得出 $\\dfrac{\\partial L_{\\text{CE}}}{\\partial z} = \\sigma(z)-y$。我们的分析证实，这种形式避免了由 $\\sigma'(z)$ 项引起的梯度消失。这部分也是正确的。\n-   **结论：正确。**\n\n**B. 使用交叉熵时，关于 $z$ 的梯度形式为 $(\\sigma(z) - y)\\,\\sigma^{\\prime}(z)$，这使得梯度消失问题比使用均方误差（MSE）时更严重，而均方误差的梯度简化为 $\\sigma(z) - y$。**\n-   这个陈述交换了 MSE 和 CE 的梯度表达式。根据我们的推导，这在事实上是错误的。\n-   **结论：错误。**\n\n**C. 均方误差（MSE）和交叉熵产生相同的关于 $z$ 的梯度，即 $\\sigma(z) - y$，因此当 $|z|$ 很大时，梯度消失行为没有差异。**\n-   我们的推导表明梯度是不同的：MSE 的梯度是 $(\\sigma(z) - y)\\sigma'(z)$，而 CE 的梯度是 $\\sigma(z) - y$。因此，这个陈述是错误的。\n-   **结论：错误。**\n\n**D. 当 $|z|$ 很大时，导数 $\\sigma^{\\prime}(z)$ 近似为常数，因此均方误差（MSE）和交叉熵都不会在输出层遇到梯度消失问题。**\n-   前提是错误的。我们的分析表明，当 $|z| \\to \\infty$ 时，$\\sigma'(z) \\to 0$。它不是一个非零常数。\n-   结论也是错误的，因为 MSE 确实会遭受梯度消失问题，原因正是 $\\sigma'(z) \\to 0$。\n-   **结论：错误。**\n\n根据严谨的推导，陈述 A 是对情况的唯一准确描述。", "answer": "$$\\boxed{A}$$", "id": "3194463"}, {"introduction": "在诊断了问题之后，我们自然要探索解决方案。本练习将带你探究一种强大的现代方法：结合使用ReLU激活函数和精心的权重初始化方案（特别是He初始化）。通过对信号方差传播的理论分析和蒙特卡洛模拟，我们将验证这种组合如何确保前向传播的激活值和反向传播的梯度都能在网络中保持稳定的“能量”，无论网络有多深。这是成功训练深度神经网络的基石之一。[@problem_id:3194508]", "problem": "考虑一个深度为 $L$、使用整流线性单元（ReLU）非线性的全连接前馈神经网络。设层索引为 $l \\in \\{1,\\dots,L\\}$，每层的宽度（单元数）为 $n$，输入批量大小为 $B$。将第 $l$ 层的激活后输出表示为 $x_l \\in \\mathbb{R}^{B \\times n}$，第 $l$ 层的激活前输入表示为 $z_l \\in \\mathbb{R}^{B \\times n}$。前向传播遵循规则 $z_l = x_{l-1} W_l$ 和 $x_l = \\phi(z_l)$，其中 $\\phi$ 是 ReLU 函数，按元素定义为 $\\phi(u) = \\max(0,u)$，$W_l \\in \\mathbb{R}^{n \\times n}$ 是第 $l$ 层的权重矩阵。假设 $x_0$ 的条目是独立同分布（i.i.d.）的零均值单位方差高斯分布。在初始化时，假设每个 $W_l$ 的条目是 i.i.d.、零均值，且独立于 $x_{l-1}$，并考虑两种初始化方案：He 初始化，其方差为 $\\mathrm{Var}(W_{l,ij}) = \\frac{2}{n}$；以及 Xavier (Glorot) 初始化，其方差为 $\\mathrm{Var}(W_{l,ij}) = \\frac{1}{n}$。偏置为零。\n\n目标是从第一性原理出发分析梯度消失问题。您必须从以下基础开始：\n- 用于前馈神经网络反向传播的微积分链式法则。\n- 方差的定义 $\\mathrm{Var}[X] = \\mathbb{E}[X^2] - (\\mathbb{E}[X])^2$ 以及独立零均值随机变量之和的方差独立性法则，即如果 $U_i$ 是独立的且均值为零，则 $\\mathrm{Var}\\left[\\sum_i U_i\\right] = \\sum_i \\mathrm{Var}[U_i]$。\n- ReLU 作用于零均值对称输入时的性质：其导数是一个指示函数 $\\phi'(u) = \\mathbb{1}\\{u0\\}$，在对称性下，它以 $\\frac{1}{2}$ 的概率充当一个门。\n\n利用这些基础，推断在初始化时矩（moment）是如何前向和后向传播的。具体来说，将第 $l$ 层的前向“能量”定义为二阶原点矩 $m_l^{\\text{fwd}} = \\mathbb{E}\\big[x_l^2\\big]$（期望是对批次和单元计算的），并将第 $l$ 层的后向“能量”定义为关于激活前输入的梯度的二阶原点矩 $m_l^{\\text{bwd}} = \\mathbb{E}\\big[(\\partial L/\\partial z_l)^2\\big]$，其中损失函数为平方范数损失 $L = \\tfrac{1}{2}\\lVert x_L\\rVert_2^2$，该损失是按样本计算，未在批次上平均。在 He 初始化和独立性假设下，证明为什么 $m_l^{\\text{fwd}}$ 和 $m_l^{\\text{bwd}}$ 在初始化时跨层保持在 $1$ 附近。将其与 ReLU 下的 Xavier 初始化进行对比，并解释为什么后者倾向于随着深度的增加而减小 $m_l^{\\text{fwd}}$，从而造成梯度消失的情景。\n\n您的程序必须通过蒙特卡洛模拟来经验性地验证这些陈述。对于每个测试用例，按照规定构造一个随机网络和批次，执行一次前向传播以计算所有的 $z_l$ 和 $x_l$，并使用链式法则执行一次后向传播以计算所有 $l$ 的 $\\partial L/\\partial z_l$。对于每一层，计算：\n- 前向二阶原点矩 $m_l^{\\text{fwd}}$，作为 $x_l$ 中元素平方的经验均值。\n- 后向二阶原点矩 $m_l^{\\text{bwd}}$，作为 $\\partial L/\\partial z_l$ 中元素平方的经验均值。\n通过计算前向和后向在各层上的最大绝对偏差来汇总与 $1$ 的偏差，即 $\\Delta^{\\text{fwd}} = \\max_l \\lvert m_l^{\\text{fwd}} - 1\\rvert$ 和 $\\Delta^{\\text{bwd}} = \\max_l \\lvert m_l^{\\text{bwd}} - 1\\rvert$。如果 $\\Delta^{\\text{fwd}}$ 和 $\\Delta^{\\text{bwd}}$ 都严格小于容差 $\\varepsilon = 0.2$，则该测试用例“通过”。\n\n测试套件：\n- 用例 1：He 初始化, $L=1$, $n=64$, $B=2000$。\n- 用例 2：He 初始化, $L=20$, $n=64$, $B=2000$。\n- 用例 3：He 初始化, $L=40$, $n=64$, $B=2000$。\n- 用例 4：Xavier 初始化, $L=20$, $n=64$, $B=2000$。\n\n答案格式：\n- 您的程序应生成单行输出，其中包含四个测试用例的通过/失败结果，格式为方括号括起来的逗号分隔列表，例如 $\\texttt{[True,True,True,False]}$。\n\n不涉及物理单位或角度单位。所有期望和方差都是无量纲的。请完全按照描述实现模拟和计算，不使用任何外部文件或输入，并使用固定的随机种子以确保可复现性。", "solution": "该问题是有效的，因为它在科学上基于深度学习的原理，信息完备、问题适定，并且表述客观。我们接下来进行理论分析和随后的经验验证。\n\n分析的核心在于推导网络在初始化时，激活值（前向传播）和梯度（后向传播）的二阶原点矩在各层之间传播的递推关系。我们将前向“能量”定义为 $m_l^{\\text{fwd}} = \\mathbb{E}\\big[x_{l,ik}^2\\big]$，后向“能量”定义为 $m_l^{\\text{bwd}} = \\mathbb{E}\\big[(\\partial L/\\partial z_{l,ik})^2\\big]$，其中期望是针对批次维度 $i$、单元维度 $k$ 和随机初始化计算的。\n\n**前向传播分析**\n\n前向传播由 $z_l = x_{l-1} W_l$ 和 $x_l = \\phi(z_l)$ 定义，其中 $\\phi$ 是 ReLU 函数。我们考虑单个激活前元素 $z_{l,ik} = \\sum_{j=1}^{n} x_{l-1, ij} W_{l,jk}$。在初始化时，权重 $W_{l,jk}$ 与输入 $x_{l-1, ij}$ 相互独立，且两者均值为零。因此，在假设 $\\mathbb{E}[x_{l-1, ij}] = 0$ 的情况下，有 $\\mathbb{E}[z_{l,ik}] = \\sum_{j=1}^{n} \\mathbb{E}[x_{l-1, ij}] \\mathbb{E}[W_{l,jk}] = 0$。这个假设对输入层 $x_0$ 成立，并且由于更新的对称性，我们假设它对后续层也近似成立。\n\n由于其均值为零，$z_{l,ik}$ 的方差为 $\\mathrm{Var}(z_{l,ik}) = \\mathbb{E}[z_{l,ik}^2]$。利用和式中各项的独立性：\n$$ \\mathrm{Var}(z_{l,ik}) = \\mathrm{Var}\\left(\\sum_{j=1}^{n} x_{l-1, ij} W_{l,jk}\\right) = \\sum_{j=1}^{n} \\mathrm{Var}(x_{l-1, ij} W_{l,jk}) $$\n根据 $x_{l-1}$ 和 $W_l$ 的独立性及其零均值属性，$\\mathrm{Var}(AB) = \\mathbb{E}[A^2]\\mathbb{E}[B^2] = \\mathrm{Var}(A)\\mathrm{Var}(B)$。因此：\n$$ \\mathrm{Var}(z_{l,ik}) = \\sum_{j=1}^{n} \\mathrm{Var}(x_{l-1, ij}) \\mathrm{Var}(W_{l,jk}) = n \\cdot \\mathrm{Var}(x_{l-1}) \\cdot \\mathrm{Var}(W_l) $$\n这里，$\\mathrm{Var}(x_{l-1})$ 是 $x_{l-1}$ 中任意元素的方差，$\\mathrm{Var}(W_l)$ 是 $W_l$ 中任意元素的方差。\n激活值为 $x_l = \\phi(z_l)$。我们需要二阶矩 $m_l^{\\text{fwd}} = \\mathbb{E}[x_l^2] = \\mathbb{E}[\\phi(z_l)^2]$。对于一个零均值对称输入 $z_l$（根据中心极限定理近似为高斯分布），ReLU 函数 $\\phi(u)=\\max(0,u)$ 实际上将分布的一半置零。输出的二阶矩是输入二阶矩的一半：$\\mathbb{E}[\\phi(z_l)^2] = \\frac{1}{2}\\mathbb{E}[z_l^2]$。\n因此，$m_l^{\\text{fwd}} = \\frac{1}{2} \\mathbb{E}[z_l^2] = \\frac{1}{2} \\mathrm{Var}(z_l)$。\n\n结合这些结果，我们得到前向能量的递推关系：\n$$ m_l^{\\text{fwd}} = \\frac{1}{2} n \\cdot \\mathrm{Var}(x_{l-1}) \\cdot \\mathrm{Var}(W_l) $$\n我们做一个标准近似，即激活值的均值保持在零附近，因此 $\\mathrm{Var}(x_{l-1}) \\approx \\mathbb{E}[x_{l-1}^2] = m_{l-1}^{\\text{fwd}}$。这给出：\n$$ m_l^{\\text{fwd}} \\approx \\frac{1}{2} n \\cdot m_{l-1}^{\\text{fwd}} \\cdot \\mathrm{Var}(W_l) $$\n基本情况是输入层 $x_0$，其条目是 i.i.d.，均值为 $0$，方差为 $1$。因此，$m_0^{\\text{fwd}} = \\mathbb{E}[x_0^2] = \\mathrm{Var}(x_0) = 1$。\n\n*   **He 初始化**: 当 $\\mathrm{Var}(W_l) = \\frac{2}{n}$ 时，递推关系变为 $m_l^{\\text{fwd}} \\approx \\frac{1}{2} n \\cdot m_{l-1}^{\\text{fwd}} \\cdot \\frac{2}{n} = m_{l-1}^{\\text{fwd}}$。从 $m_0^{\\text{fwd}}=1$ 开始，前向能量在各层间得以保持，即对所有 $l$，都有 $m_l^{\\text{fwd}} \\approx 1$。\n*   **Xavier 初始化**: 当 $\\mathrm{Var}(W_l) = \\frac{1}{n}$ 时，递推关系变为 $m_l^{\\text{fwd}} \\approx \\frac{1}{2} n \\cdot m_{l-1}^{\\text{fwd}} \\cdot \\frac{1}{n} = \\frac{1}{2} m_{l-1}^{\\text{fwd}}$。这导致指数衰减：$m_l^{\\text{fwd}} \\approx (\\frac{1}{2})^l m_0^{\\text{fwd}} = (\\frac{1}{2})^l$。随着网络深度 $L$ 的增加，激活值的能量会消失。\n\n**反向传播分析**\n\n关于激活前输入的梯度由链式法则给出：$\\frac{\\partial L}{\\partial z_l} = \\frac{\\partial L}{\\partial z_{l+1}} \\frac{\\partial z_{l+1}}{\\partial x_l} \\frac{\\partial x_l}{\\partial z_l}$。令 $\\delta_l = \\frac{\\partial L}{\\partial z_l}$。单个分量是：\n$$ \\delta_{l,ik} = \\left( (\\delta_{l+1} W_{l+1}^T)_{ik} \\right) \\cdot \\phi'(z_{l,ik}) = \\left( \\sum_{j=1}^n \\delta_{l+1, ij} W_{l+1, kj} \\right) \\cdot \\phi'(z_{l,ik}) $$\n我们想求 $m_l^{\\text{bwd}} = \\mathbb{E}[\\delta_{l,ik}^2]$。在初始化时，括号中的项与 $\\phi'(z_{l,ik})$ 独立。因此，$\\mathbb{E}[\\delta_{l,ik}^2] = \\mathbb{E}[(\\sum_j \\dots)^2] \\cdot \\mathbb{E}[(\\phi'(z_{l,ik}))^2]$。\nReLU 的导数是 $\\phi'(u) = \\mathbb{1}\\{u0\\}$，所以 $(\\phi'(u))^2 = \\phi'(u)$。对于一个对称的零均值 $z_{l,ik}$，$\\mathbb{P}(z_{l,ik}0) = \\frac{1}{2}$，所以 $\\mathbb{E}[(\\phi'(z_{l,ik}))^2] = \\frac{1}{2}$。\n和的方差是 $\\mathrm{Var}(\\sum_j \\delta_{l+1, ij} W_{l+1, kj}) = n \\cdot \\mathrm{Var}(\\delta_{l+1}) \\cdot \\mathrm{Var}(W_{l+1})$。假设 $\\mathbb{E}[\\delta_{l+1}] \\approx 0$，这个值就是 $n \\cdot m_{l+1}^{\\text{bwd}} \\cdot \\mathrm{Var}(W_{l+1})$。\n\n综合这些，我们得到后向递推关系：\n$$ m_l^{\\text{bwd}} \\approx \\frac{1}{2} n \\cdot m_{l+1}^{\\text{bwd}} \\cdot \\mathrm{Var}(W_{l+1}) $$\n对于第 $L$ 层的基本情况，损失是每个样本的 $L = \\frac{1}{2} \\|x_L\\|_2^2$。梯度是 $\\delta_L = \\frac{\\partial L}{\\partial z_L} = \\frac{\\partial L}{\\partial x_L}\\frac{\\partial x_L}{\\partial z_L}$。这里，$\\frac{\\partial L}{\\partial x_L} = x_L$，而 $\\frac{\\partial x_L}{\\partial z_L}$ 是一个由 $\\phi'(z_L)$ 构成的对角矩阵。所以 $\\delta_L = x_L \\odot \\phi'(z_L)$。由于 $x_L = \\phi(z_L)$，我们有 $\\delta_L = \\phi(z_L) \\odot \\phi'(z_L) = \\phi(z_L) = x_L$。\n因此，后向能量的基本情况是 $m_L^{\\text{bwd}} = \\mathbb{E}[\\delta_L^2] = \\mathbb{E}[x_L^2] = m_L^{\\text{fwd}}$。\n\n*   **He 初始化**: 当 $\\mathrm{Var}(W_{l+1}) = \\frac{2}{n}$ 时，递推关系为 $m_l^{\\text{bwd}} \\approx m_{l+1}^{\\text{bwd}}$。根据前向传播的分析，$m_L^{\\text{fwd}} \\approx 1$，所以 $m_L^{\\text{bwd}} \\approx 1$。向后传播，我们发现对所有 $l$ 都有 $m_l^{\\text{bwd}} \\approx 1$。梯度能量得以保持。\n*   **Xavier 初始化**: 当 $\\mathrm{Var}(W_{l+1}) = \\frac{1}{n}$ 时，递推关系为 $m_l^{\\text{bwd}} \\approx \\frac{1}{2} m_{l+1}^{\\text{bwd}}$。梯度能量在从第 $L$ 层传播到第 $1$ 层的过程中呈指数衰减。这就是梯度消失问题。早期层的梯度变得过小，无法实现有效的学习。\n\n模拟将经验性地验证这两种截然不同的行为。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Runs Monte Carlo simulations to verify the signal propagation properties of\n    He and Xavier initializations in deep ReLU networks.\n    \"\"\"\n    # Set a fixed random seed for reproducibility.\n    np.random.seed(42)\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {'init': 'he', 'L': 1, 'n': 64, 'B': 2000, 'name': 'Case 1'},\n        {'init': 'he', 'L': 20, 'n': 64, 'B': 2000, 'name': 'Case 2'},\n        {'init': 'he', 'L': 40, 'n': 64, 'B': 2000, 'name': 'Case 3'},\n        {'init': 'xavier', 'L': 20, 'n': 64, 'B': 2000, 'name': 'Case 4'},\n    ]\n\n    results = []\n    for case in test_cases:\n        passes_test = run_simulation(\n            init_scheme=case['init'],\n            L=case['L'],\n            n=case['n'],\n            B=case['B']\n        )\n        results.append(passes_test)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef run_simulation(init_scheme: str, L: int, n: int, B: int) - bool:\n    \"\"\"\n    Performs a single forward and backward pass for a given network configuration\n    and checks if the deviation criteria are met.\n\n    Args:\n        init_scheme: Either 'he' or 'xavier'.\n        L: Depth of the network.\n        n: Width of each layer.\n        B: Batch size.\n\n    Returns:\n        A boolean indicating if the test case passes.\n    \"\"\"\n    # 1. Initialization\n    # Input data: i.i.d. Gaussian with zero mean and unit variance.\n    x0 = np.random.randn(B, n)\n\n    # Determine weight variance based on initialization scheme.\n    if init_scheme == 'he':\n        var_w = 2.0 / n\n    elif init_scheme == 'xavier':\n        var_w = 1.0 / n\n    else:\n        raise ValueError(\"Unknown initialization scheme.\")\n    \n    std_w = np.sqrt(var_w)\n\n    # Initialize weights for L layers.\n    weights = [np.random.randn(n, n) * std_w for _ in range(L)]\n\n    # 2. Forward Pass\n    z_values = {}\n    x_values = {0: x0}\n    \n    x_current = x0\n    for l in range(1, L + 1):\n        # Linear transformation\n        z_l = x_current @ weights[l-1]\n        # ReLU activation\n        x_l = np.maximum(0, z_l)\n        \n        z_values[l] = z_l\n        x_values[l] = x_l\n        x_current = x_l\n    \n    # 3. Compute Forward Moments\n    m_fwd = []\n    for l in range(1, L + 1):\n        # Calculate empirical second raw moment over batch and units.\n        moment = np.mean(x_values[l]**2)\n        m_fwd.append(moment)\n\n    # 4. Backward Pass\n    dz_values = {}\n    \n    # Base case: Gradient of the loss w.r.t final pre-activations.\n    # For L = 0.5 * ||x_L||^2, dL/dz_L = x_L.\n    # This simplification comes from dL/dx_L = x_L and dL/dz_L = dL/dx_L * phi'(z_L),\n    # which simplifies to x_L * phi'(z_L) = phi(z_L) * phi'(z_L) = phi(z_L) = x_L for ReLU.\n    dz_L = x_values[L]\n    dz_values[L] = dz_L\n    \n    # Propagate gradients backward from L-1 to 1.\n    dz_current = dz_L\n    for l in range(L - 1, 0, -1):\n        # Gradient w.r.t previous activation layer\n        dx_l = dz_current @ weights[l].T\n        \n        # Derivative of ReLU\n        phi_prime_l = (z_values[l]  0).astype(float)\n        \n        # Gradient w.r.t pre-activation layer\n        dz_l = dx_l * phi_prime_l\n        \n        dz_values[l] = dz_l\n        dz_current = dz_l\n\n    # 5. Compute Backward Moments\n    m_bwd = []\n    for l in range(1, L + 1):\n        # Calculate empirical second raw moment over batch and units.\n        moment = np.mean(dz_values[l]**2)\n        m_bwd.append(moment)\n\n    # 6. Check Pass/Fail Condition\n    epsilon = 0.2\n    \n    # Maximum absolute deviation from 1 for forward moments\n    delta_fwd = np.max(np.abs(np.array(m_fwd) - 1))\n    \n    # Maximum absolute deviation from 1 for backward moments\n    delta_bwd = np.max(np.abs(np.array(m_bwd) - 1))\n\n    return delta_fwd  epsilon and delta_bwd  epsilon\n\nif __name__ == '__main__':\n    solve()\n```", "id": "3194508"}]}