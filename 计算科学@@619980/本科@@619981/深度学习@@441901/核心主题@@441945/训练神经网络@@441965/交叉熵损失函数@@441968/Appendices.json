{"hands_on_practices": [{"introduction": "我们的第一个练习至关重要。理解交叉熵损失与Softmax函数组合的梯度推导，是揭示神经网络在分类任务中如何学习的关键。我们将一步步分解这个基础的计算过程，以展示其最终简洁而优美的形式，即预测概率与真实标签之间的差异[@problem_id:3110719]。", "problem": "考虑一个多类分类器，它为 $K$ 个类别输出一个 logits 向量 $\\mathbf{z} \\in \\mathbb{R}^{K}$。预测的类别概率是通过应用 softmax 函数得到的，对于每个类别索引 $i \\in \\{1,2,\\dots,K\\}$，其定义为\n$$\n\\hat{p}_{i}(\\mathbf{z}) = \\frac{\\exp(z_{i})}{\\sum_{j=1}^{K} \\exp(z_{j})}.\n$$\n设观察到的标签表示为一个独热（one-hot）向量 $\\mathbf{y} \\in \\{0,1\\}^{K}$，满足 $\\sum_{i=1}^{K} y_{i} = 1$。训练目标是交叉熵（CE）损失函数，\n$$\n\\text{CE}(\\mathbf{z},\\mathbf{y}) = -\\sum_{i=1}^{K} y_{i} \\ln\\!\\big(\\hat{p}_{i}(\\mathbf{z})\\big).\n$$\n仅从这些定义出发，推导关于 logits $\\mathbf{z}$ 的梯度 $\\nabla_{\\mathbf{z}} \\text{CE}(\\mathbf{z},\\mathbf{y})$。你的推导必须使用第一性原理和标准的微积分法则，清晰地指明在何处应用了乘法法则、链式法则以及 softmax 函数的性质。然后，根据损失函数如何响应预测分布与独热标签之间的差异，来解释梯度中出现的每一项。\n\n最后，为了进行具体计算，请在 logits $\\mathbf{z} = [0,\\,0,\\,\\ln 2]$ 和标签 $\\mathbf{y} = [0,\\,1,\\,0]$ 处评估你推导出的梯度。请精确地表示最终的数值结果，无需四舍五入。", "solution": "该问题是有效的。Softmax 函数和交叉熵损失的定义在深度学习领域是标准的，推导它们的梯度是一个定义明确的数学练习。\n\n我们的目标是计算交叉熵（CE）损失函数关于 logits 向量 $\\mathbf{z}$ 的梯度，记为 $\\nabla_{\\mathbf{z}} \\text{CE}(\\mathbf{z},\\mathbf{y})$。该梯度是一个向量，其第 $k$ 个分量是损失函数关于第 $k$ 个 logit 的偏导数，即 $\\frac{\\partial \\text{CE}}{\\partial z_k}$。\n\n损失函数由下式给出：\n$$\n\\text{CE}(\\mathbf{z},\\mathbf{y}) = L = -\\sum_{i=1}^{K} y_{i} \\ln\\!\\big(\\hat{p}_{i}(\\mathbf{z})\\big)\n$$\n我们首先应用求和法则求导，以找到关于任意 logit $z_k$ 的偏导数：\n$$\n\\frac{\\partial L}{\\partial z_k} = \\frac{\\partial}{\\partial z_k} \\left( -\\sum_{i=1}^{K} y_{i} \\ln(\\hat{p}_{i}) \\right) = -\\sum_{i=1}^{K} y_{i} \\frac{\\partial}{\\partial z_k} \\ln(\\hat{p}_{i})\n$$\n使用链式法则，自然对数的导数是 $\\frac{d}{dx}\\ln(u) = \\frac{1}{u}\\frac{du}{dx}$。应用此法则，我们得到：\n$$\n\\frac{\\partial L}{\\partial z_k} = -\\sum_{i=1}^{K} \\frac{y_{i}}{\\hat{p}_{i}} \\frac{\\partial \\hat{p}_{i}}{\\partial z_k}\n$$\n下一步是计算 softmax 函数输出 $\\hat{p}_i$ 关于 logit $z_k$ 的偏导数。Softmax 函数是：\n$$\n\\hat{p}_{i} = \\frac{\\exp(z_{i})}{\\sum_{j=1}^{K} \\exp(z_{j})}\n$$\n我们使用除法法则求导，即 $(\\frac{f}{g})' = \\frac{f'g - fg'}{g^2}$。令 $f(z_k) = \\exp(z_i)$ 和 $g(z_k) = \\sum_{j=1}^{K} \\exp(z_j)$。\n分子和分母关于 $z_k$ 的导数是：\n$$\n\\frac{\\partial}{\\partial z_k} \\exp(z_i) = \\begin{cases} \\exp(z_i)  & \\text{if } i=k \\\\ 0  & \\text{if } i \\neq k \\end{cases} = \\delta_{ik} \\exp(z_i)\n$$\n其中 $\\delta_{ik}$ 是克罗内克（Kronecker）δ函数。\n$$\n\\frac{\\partial}{\\partial z_k} \\sum_{j=1}^{K} \\exp(z_j) = \\exp(z_k)\n$$\n应用除法法则：\n$$\n\\frac{\\partial \\hat{p}_{i}}{\\partial z_k} = \\frac{(\\delta_{ik} \\exp(z_i)) \\left( \\sum_{j=1}^{K} \\exp(z_j) \\right) - \\exp(z_i) \\exp(z_k)}{\\left( \\sum_{j=1}^{K} \\exp(z_j) \\right)^2}\n$$\n我们可以将其分解为两部分来简化：\n$$\n\\frac{\\partial \\hat{p}_{i}}{\\partial z_k} = \\frac{\\delta_{ik} \\exp(z_i)}{\\sum_{j=1}^{K} \\exp(z_j)} - \\frac{\\exp(z_i)}{\\sum_{j=1}^{K} \\exp(z_j)} \\frac{\\exp(z_k)}{\\sum_{j=1}^{K} \\exp(z_j)}\n$$\n识别出 softmax 函数的定义，即 $\\hat{p}_i = \\frac{\\exp(z_i)}{\\sum_j \\exp(z_j)}$ 和 $\\hat{p}_k = \\frac{\\exp(z_k)}{\\sum_j \\exp(z_j)}$，我们可以将表达式重写为：\n$$\n\\frac{\\partial \\hat{p}_{i}}{\\partial z_k} = \\delta_{ik} \\hat{p}_i - \\hat{p}_i \\hat{p}_k = \\hat{p}_i (\\delta_{ik} - \\hat{p}_k)\n$$\nSoftmax 函数导数的这个一般形式可以分为两种情况：\n1.  如果 $i = k$：$\\frac{\\partial \\hat{p}_{i}}{\\partial z_i} = \\hat{p}_i(1 - \\hat{p}_i)$\n2.  如果 $i \\neq k$：$\\frac{\\partial \\hat{p}_{i}}{\\partial z_k} = \\hat{p}_i(0 - \\hat{p}_k) = -\\hat{p}_i \\hat{p}_k$\n\n现在，我们将此结果代回到损失梯度的表达式中：\n$$\n\\frac{\\partial L}{\\partial z_k} = -\\sum_{i=1}^{K} \\frac{y_{i}}{\\hat{p}_{i}} \\frac{\\partial \\hat{p}_{i}}{\\partial z_k} = -\\sum_{i=1}^{K} \\frac{y_{i}}{\\hat{p}_{i}} \\left( \\hat{p}_i (\\delta_{ik} - \\hat{p}_k) \\right)\n$$\n$\\hat{p}_i$ 项相互抵消，从而显著简化了表达式：\n$$\n\\frac{\\partial L}{\\partial z_k} = -\\sum_{i=1}^{K} y_{i} (\\delta_{ik} - \\hat{p}_k)\n$$\n我们可以将求和分为两部分：\n$$\n\\frac{\\partial L}{\\partial z_k} = -\\left( \\sum_{i=1}^{K} y_{i}\\delta_{ik} - \\sum_{i=1}^{K} y_{i}\\hat{p}_k \\right)\n$$\n我们来计算每个求和：\n- 第一个求和 $\\sum_{i=1}^{K} y_{i}\\delta_{ik}$，仅当 $i=k$ 时非零，此时其值为 $y_k$。因此，$\\sum_{i=1}^{K} y_{i}\\delta_{ik} = y_k$。\n- 在第二个求和中，$\\hat{p}_k$ 相对于求和索引 $i$ 是一个常数。所以，$\\sum_{i=1}^{K} y_{i}\\hat{p}_k = \\hat{p}_k \\sum_{i=1}^{K} y_i$。因为 $\\mathbf{y}$ 是一个独热向量，我们知道 $\\sum_{i=1}^{K} y_i = 1$。因此，该求和等于 $\\hat{p}_k$。\n\n将这些结果代回：\n$$\n\\frac{\\partial L}{\\partial z_k} = -(y_k - \\hat{p}_k) = \\hat{p}_k - y_k\n$$\n这个非常简洁的结果表明，交叉熵损失关于第 $k$ 个 logit 的偏导数是类别 $k$ 的预测概率与类别 $k$ 的真实标签之差。因此，完整的梯度向量是：\n$$\n\\nabla_{\\mathbf{z}} \\text{CE}(\\mathbf{z},\\mathbf{y}) = \\hat{\\mathbf{p}}(\\mathbf{z}) - \\mathbf{y}\n$$\n\n项 $\\hat{p}_k - y_k$ 的解释如下。在通过梯度下降进行训练期间，logit $z_k$ 根据 $z_k \\leftarrow z_k - \\alpha (\\hat{p}_k - y_k)$ 进行更新，其中 $\\alpha > 0$ 是学习率。\n- 如果类别 $k$ 是真实类别，那么 $y_k=1$。梯度分量是 $\\hat{p}_k - 1$，这是一个负数或零（因为 $\\hat{p}_k \\leq 1$）。更新规则变为 $z_k \\leftarrow z_k - \\alpha(\\text{负值})$，这会增加 $z_k$。这将推高正确类别的 logit，从而使其概率 $\\hat{p}_k$ 趋向目标值 $1$。\n- 如果类别 $k$ 不是真实类别，那么 $y_k=0$。梯度分量是 $\\hat{p}_k - 0 = \\hat{p}_k$，这是一个正数或零（因为 $\\hat{p}_k \\geq 0$）。更新规则变为 $z_k \\leftarrow z_k - \\alpha(\\text{正值})$，这会减小 $z_k$。这将拉低不正确类别的 logits，从而使其概率趋向目标值 $0$。\n本质上，梯度向量 $\\hat{\\mathbf{p}} - \\mathbf{y}$ 代表了预测概率分布与真实独热分布之间的误差或残差。\n\n最后，我们针对给定的具体案例评估该梯度：\n- Logits: $\\mathbf{z} = [0, 0, \\ln 2]$。这意味着 $K=3$。\n- 标签: $\\mathbf{y} = [0, 1, 0]$。\n\n首先，我们使用 softmax 函数计算预测概率 $\\hat{\\mathbf{p}}$：\n分母是 $\\sum_{j=1}^{3} \\exp(z_j) = \\exp(0) + \\exp(0) + \\exp(\\ln 2) = 1 + 1 + 2 = 4$。\n概率是：\n$$\n\\hat{p}_1 = \\frac{\\exp(z_1)}{4} = \\frac{\\exp(0)}{4} = \\frac{1}{4}\n$$\n$$\n\\hat{p}_2 = \\frac{\\exp(z_2)}{4} = \\frac{\\exp(0)}{4} = \\frac{1}{4}\n$$\n$$\n\\hat{p}_3 = \\frac{\\exp(z_3)}{4} = \\frac{\\exp(\\ln 2)}{4} = \\frac{2}{4} = \\frac{1}{2}\n$$\n所以，预测概率向量是 $\\hat{\\mathbf{p}} = [\\frac{1}{4}, \\frac{1}{4}, \\frac{1}{2}]$。\n\n现在，我们计算梯度 $\\nabla_{\\mathbf{z}} \\text{CE} = \\hat{\\mathbf{p}} - \\mathbf{y}$：\n$$\n\\nabla_{\\mathbf{z}} \\text{CE} = \\begin{pmatrix} \\frac{1}{4}, & \\frac{1}{4}, & \\frac{1}{2} \\end{pmatrix} - \\begin{pmatrix} 0, & 1, & 0 \\end{pmatrix}\n$$\n$$\n\\nabla_{\\mathbf{z}} \\text{CE} = \\begin{pmatrix} \\frac{1}{4} - 0, & \\frac{1}{4} - 1, & \\frac{1}{2} - 0 \\end{pmatrix}\n$$\n$$\n\\nabla_{\\mathbf{z}} \\text{CE} = \\begin{pmatrix} \\frac{1}{4}, & -\\frac{3}{4}, & \\frac{1}{2} \\end{pmatrix}\n$$", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{1}{4} & -\\frac{3}{4} & \\frac{1}{2}\n\\end{pmatrix}\n}\n$$", "id": "3110719"}, {"introduction": "虽然准确率告诉我们模型的预测是对是错，但交叉熵还度量了模型的“置信度”。本练习将通过一个实际场景，展示为何交叉熵通常是监控训练进度和决定（例如）何时停止训练的更敏感、信息更丰富的指标[@problem_id:3110736]。", "problem": "一个多类别分类器为每个输入 $x$ 生成一个在类别 $\\{A,B,C\\}$ 上的概率向量 $\\hat{\\mathbf{p}}(x)$。考虑一个包含 $n=4$ 个样本的固定验证集，下面给出了这些样本的真实标签以及模型在周期 $t=1$ 和周期 $t=2$ 的预测概率。对于每个样本 $i$，其真实类别由 $y_i \\in \\{A,B,C\\}$ 表示，模型输出一个概率向量 $(\\hat{p}_A,\\hat{p}_B,\\hat{p}_C)$。\n\n- 样本 $1$：$y_1=A$\n  - 周期 $1$：$(0.55,\\,0.40,\\,0.05)$\n  - 周期 $2$：$(0.70,\\,0.25,\\,0.05)$\n- 样本 $2$：$y_2=B$\n  - 周期 $1$：$(0.60,\\,0.35,\\,0.05)$\n  - 周期 $2$：$(0.55,\\,0.40,\\,0.05)$\n- 样本 $3$：$y_3=C$\n  - 周期 $1$：$(0.24,\\,0.24,\\,0.52)$\n  - 周期 $2$：$(0.20,\\,0.20,\\,0.60)$\n- 样本 $4$：$y_4=B$\n  - 周期 $1$：$(0.49,\\,0.51,\\,0.00)$\n  - 周期 $2$：$(0.49,\\,0.51,\\,0.00)$\n\n仅使用以下基本定义。\n\n- 周期 $t$ 的验证准确率是分数 $\\frac{1}{n}\\sum_{i=1}^{n}\\mathbf{1}\\{\\arg\\max_{c\\in\\{A,B,C\\}}\\hat{p}_c^{(t)}(x_i)=y_i\\}$。\n- 周期 $t$ 的平均负对数似然 (NLL)，对于独热标签等价于平均交叉熵 (CE)，其值为 $\\frac{1}{n}\\sum_{i=1}^{n}\\big(-\\log \\hat{p}^{(t)}_{y_i}(x_i)\\big)$，其中 $\\log$ 是自然对数。\n- NLL 的指数移动平均 (EMA) 定义为 $\\text{EMA}_t=\\alpha L_t+(1-\\alpha)\\text{EMA}_{t-1}$，其中 $L_t$ 是周期 $t$ 的平均 NLL，$\\alpha\\in(0,1]$，并初始化 $\\text{EMA}_1=L_1$。取 $\\alpha=0.5$。\n\n基于以上定义和数据，以下哪些陈述是正确的？\n\nA. 从周期 $1$ 到周期 $2$，验证准确率保持不变，而平均负对数似然严格减小。因此，与监控负对数似然移动平均值的规则相比，一个在准确率平台期持续 $1$ 个周期时就停止的早停规则会显得为时过早。\n\nB. 从周期 $1$ 到周期 $2$，验证准确率和平均负对数似然都保持不变；因此在周期 $2$，两种指标都会产生相同的停止决策。\n\nC. 从周期 $1$ 到周期 $2$，验证准确率增加，平均负对数似然也增加。\n\nD. 平滑参数为 $\\alpha=0.5$ 的负对数似然的 2 周期指数移动平均值从周期 1 到周期 2 是减小的。", "solution": "该问题是有效的，它通过一个具体的数值例子，测试了对准确率、负对数似然（NLL）和指数移动平均（EMA）这些核心评估指标的理解。所有必要的定义和数据都已提供。\n\n### 解题推导\n\n我们将按顺序计算和分析准确率、平均 NLL 和 NLL 的 EMA。\n\n**1. 计算验证准确率**\n\n准确率衡量的是模型预测类别与真实类别相符的样本比例。预测类别是概率最高的那个类别。\n\n**周期 $t=1$ 的准确率 ($Acc_1$)**：\n-   样本 1 ($y_1=A$): $\\hat{\\mathbf{p}}^{(1)}=(0.55, 0.40, 0.05)$。预测为 A。**正确**。\n-   样本 2 ($y_2=B$): $\\hat{\\mathbf{p}}^{(1)}=(0.60, 0.35, 0.05)$。预测为 A。**错误**。\n-   样本 3 ($y_3=C$): $\\hat{\\mathbf{p}}^{(1)}=(0.24, 0.24, 0.52)$。预测为 C。**正确**。\n-   样本 4 ($y_4=B$): $\\hat{\\mathbf{p}}^{(1)}=(0.49, 0.51, 0.00)$。预测为 B。**正确**。\n总共有 3 个样本预测正确，所以 $Acc_1 = \\frac{3}{4} = 0.75$。\n\n**周期 $t=2$ 的准确率 ($Acc_2$)**：\n-   样本 1 ($y_1=A$): $\\hat{\\mathbf{p}}^{(2)}=(0.70, 0.25, 0.05)$。预测为 A。**正确**。\n-   样本 2 ($y_2=B$): $\\hat{\\mathbf{p}}^{(2)}=(0.55, 0.40, 0.05)$。预测为 A。**错误**。\n-   样本 3 ($y_3=C$): $\\hat{\\mathbf{p}}^{(2)}=(0.20, 0.20, 0.60)$。预测为 C。**正确**。\n-   样本 4 ($y_4=B$): $\\hat{\\mathbf{p}}^{(2)}=(0.49, 0.51, 0.00)$。预测为 B。**正确**。\n总共仍有 3 个样本预测正确，所以 $Acc_2 = \\frac{3}{4} = 0.75$。\n\n**结论**: 从周期 1 到周期 2，验证准确率保持不变。\n\n**2. 计算平均负对数似然 (NLL)**\n\n平均 NLL ($L_t$) 是基于模型赋给真实类别的概率计算的。\n\n**周期 $t=1$ 的平均 NLL ($L_1$)**：\n模型赋给真实类别的概率分别为 $\\hat{p}^{(1)}_{y_1}=0.55$, $\\hat{p}^{(1)}_{y_2}=0.35$, $\\hat{p}^{(1)}_{y_3}=0.52$, 和 $\\hat{p}^{(1)}_{y_4}=0.51$。\n$$L_1 = \\frac{1}{4} \\left[ -\\log(0.55) - \\log(0.35) - \\log(0.52) - \\log(0.51) \\right]$$\n\n**周期 $t=2$ 的平均 NLL ($L_2$)**：\n模型赋给真实类别的概率分别为 $\\hat{p}^{(2)}_{y_1}=0.70$, $\\hat{p}^{(2)}_{y_2}=0.40$, $\\hat{p}^{(2)}_{y_3}=0.60$, 和 $\\hat{p}^{(2)}_{y_4}=0.51$。\n$$L_2 = \\frac{1}{4} \\left[ -\\log(0.70) - \\log(0.40) - \\log(0.60) - \\log(0.51) \\right]$$\n\n**比较 $L_1$ 和 $L_2$**:\n我们不需要计算精确的数值，只需比较每个样本的损失变化。\n-   样本 1: 概率从 0.55 增加到 0.70。由于 $\\log(x)$ 是增函数，$-\\log(x)$ 是减函数，所以该样本的 NLL 减小。\n-   样本 2: 概率从 0.35 增加到 0.40。该样本的 NLL 减小。\n-   样本 3: 概率从 0.52 增加到 0.60。该样本的 NLL 减小。\n-   样本 4: 概率保持 0.51 不变。该样本的 NLL 不变。\n\n由于三个样本的 NLL 严格减小，一个样本的 NLL 不变，它们的平均值必然严格减小。因此，$L_2 < L_1$。\n\n**结论**: 平均负对数似然严格减小。\n\n**3. 计算 NLL 的指数移动平均 (EMA)**\n\nEMA 的定义为 $\\text{EMA}_t=\\alpha L_t+(1-\\alpha)\\text{EMA}_{t-1}$，其中 $\\alpha=0.5$ 且 $\\text{EMA}_1=L_1$。\n\n-   周期 1: $\\text{EMA}_1 = L_1$。\n-   周期 2: $\\text{EMA}_2 = 0.5 L_2 + 0.5 \\text{EMA}_1 = 0.5 L_2 + 0.5 L_1$。\n\n比较 $\\text{EMA}_1$ 和 $\\text{EMA}_2$：\n$$\\text{EMA}_2 - \\text{EMA}_1 = (0.5 L_2 + 0.5 L_1) - L_1 = 0.5 L_2 - 0.5 L_1 = 0.5(L_2 - L_1)$$\n因为我们已经证明 $L_2 < L_1$，所以差值 $(L_2 - L_1)$ 是负数。因此，$\\text{EMA}_2 - \\text{EMA}_1 < 0$，这意味着 $\\text{EMA}_2 < \\text{EMA}_1$。\n\n**结论**: NLL 的 EMA 从周期 1 到周期 2 是减小的。\n\n### 逐项分析\n\n-   **A**: “从周期 $1$ 到周期 $2$，验证准确率保持不变，而平均负对数似然严格减小。”—— 这部分正确。 “因此，...一个在准确率平台期持续 $1$ 个周期时就停止的早停规则会显得为时过早。”—— 这个推论也正确。准确率指标显示模型没有进步，但 NLL 指标显示模型仍在学习（变得更自信），因此基于准确率的早停会过早终止训练。**所以 A 是正确的。**\n\n-   **B**: “从周期 $1$ 到周期 $2$，验证准确率和平均负对数似然都保持不变”—— 错误，因为 NLL 减小了。\n\n-   **C**: “从周期 $1$ 到周期 $2$，验证准确率增加...”—— 错误，准确率保持不变。\n\n-   **D**: “...负对数似然的 2 周期指数移动平均值从周期 1 到周期 2 是减小的。”—— 这是正确的，我们已经证明了 $\\text{EMA}_2 < \\text{EMA}_1$。**所以 D 是正确的。**\n\n综上所述，陈述 A 和 D 是正确的。", "answer": "$$\\boxed{AD}$$", "id": "3110736"}, {"introduction": "一个常见的混淆点是损失函数与评估指标之间的区别。本练习旨在探索这一差异，通过比较关注概率校准的交叉熵与衡量排序质量的ROC-AUC指标。通过具体的例子，你将看到改善其中一个并不必然会改善另一个[@problem_id:3110742]。", "problem": "考虑标签为 $y_i \\in \\{0,1\\}$、预测概率为 $s_i \\in (0,1)$ 的二元分类问题。对于预测值为 $s_i$ 的独立伯努利观测，其负对数似然（NLL）定义为 $-\\sum_{i} \\left[ y_i \\log s_i + (1-y_i)\\log(1-s_i) \\right]$，这等于经验交叉熵（CE）。受试者工作特征曲线（ROC）下面积（AUC）定义为随机选择的正样本得分高于随机选择的负样本得分的概率，在经验有限样本设置中，这是 $s_{\\text{pos}} > s_{\\text{neg}}$ 的正负样本对的比例，其中平局计为一半。\n\n你将分析两个具体场景，并根据这些定义进行推理。\n\n场景 1（排序不变，NLL 降低）。设标签为 $y = [1,1,0,0,0]$，两组预测值为\n$s^{\\text{old}} = [0.80, 0.70, 0.40, 0.30, 0.20]$ 和 $s^{\\text{new1}} = [0.95, 0.85, 0.20, 0.15, 0.10]$。\n请注意，在 $s^{\\text{old}}$ 和 $s^{\\text{new1}}$ 中，两个正样本的概率都大于所有三个负样本的概率，因此正负样本得分的相对排序得以保留。\n\n场景 2（排序变差，NLL 降低）。设标签为 $y = [1,1,1,0,0,0]$，两组预测值为\n$s^{\\text{old2}} = [0.60, 0.70, 0.80, 0.40, 0.30, 0.20]$ 和 $s^{\\text{new2}} = [0.95, 0.70, 0.99, 0.05, 0.75, 0.01]$。\n观察到，在 $s^{\\text{new2}}$ 中，一个得分为 0.75 的负样本超过了一个得分为 0.70 的正样本，造成了至少一次正负样本倒置，而其他样本则更接近其标签。\n\n仅使用给出的定义和这些数据，判断下列哪些陈述是正确的：\n\nA. 从 $s^{\\text{old}}$ 变为 $s^{\\text{new1}}$ 会降低 NLL，但 ROC-AUC 保持不变，因为正负样本之间的 $s_i$ 排序得以保留。\n\nB. 从 $s^{\\text{old2}}$ 变为 $s^{\\text{new2}}$ 会降低 NLL，但会减少 ROC-AUC，因为至少有一对正负样本的顺序发生了翻转，这表明优化 CE 可能会恶化一个基于排序的指标。\n\nC. 交叉熵直接优化 ROC-AUC，因为两者都仅依赖于分数的相对排序；因此，CE 的任何改善必然会增加 ROC-AUC。\n\nD. 对所有预测概率应用任何严格递增的变换都会使 NLL 和 ROC-AUC 保持不变。\n\n选择所有正确的选项。", "solution": "该问题是有效的。它通过两个精心设计的场景，探讨了负对数似然（NLL，即交叉熵）和 ROC-AUC 这两个重要评估指标之间的关系和区别。问题定义清晰，数据具体，允许进行直接计算和推断。\n\n### 解题推导\n\n我们将逐一分析每个选项。\n\n**分析选项 A**\n\n-   **场景 1**: 标签 $y = [1,1,0,0,0]$ (2个正样本, 3个负样本)。\n    -   旧预测 $s^{\\text{old}} = [0.80, 0.70, 0.40, 0.30, 0.20]$。\n    -   新预测 $s^{\\text{new1}} = [0.95, 0.85, 0.20, 0.15, 0.10]$。\n\n-   **ROC-AUC 计算**:\n    -   对于 $s^{\\text{old}}$，正样本得分为 $\\{0.80, 0.70\\}$，负样本得分为 $\\{0.40, 0.30, 0.20\\}$。所有正样本得分都高于所有负样本得分。总共有 $2 \\times 3 = 6$ 个正负样本对，所有对都正确排序。因此，AUC = $6/6 = 1.0$。\n    -   对于 $s^{\\text{new1}}$，正样本得分为 $\\{0.95, 0.85\\}$，负样本得分为 $\\{0.20, 0.15, 0.10\\}$。同样，所有正样本得分都高于所有负样本得分。因此，AUC = $6/6 = 1.0$。\n    -   ROC-AUC 保持不变。\n\n-   **NLL 变化**:\n    -   NLL (交叉熵) 惩罚的是预测概率与真实标签 (0或1) 之间的差距。\n    -   对于正样本 ($y=1$)，预测概率从 $\\{0.80, 0.70\\}$ 变为 $\\{0.95, 0.85\\}$，更接近 1。这会降低 NLL 的对应项 (例如，$-\\log(0.95) < -\\log(0.80)$)。\n    -   对于负样本 ($y=0$)，预测概率从 $\\{0.40, 0.30, 0.20\\}$ 变为 $\\{0.20, 0.15, 0.10\\}$，更接近 0。这也会降低 NLL 的对应项 (例如，$-\\log(1-0.20) < -\\log(1-0.40)$)。\n    -   由于模型对所有样本的预测都变得更加“自信”且正确，总 NLL 严格降低。\n\n-   **结论**: 陈述 A \"会降低 NLL，但 ROC-AUC 保持不变\" 是正确的。\n\n**分析选项 B**\n\n-   **场景 2**: 标签 $y = [1,1,1,0,0,0]$ (3个正样本, 3个负样本)。\n    -   旧预测 $s^{\\text{old2}} = [0.60, 0.70, 0.80, 0.40, 0.30, 0.20]$。\n    -   新预测 $s^{\\text{new2}} = [0.95, 0.70, 0.99, 0.05, 0.75, 0.01]$。\n\n-   **ROC-AUC 计算**:\n    -   对于 $s^{\\text{old2}}$，正样本得分为 $\\{0.60, 0.70, 0.80\\}$，负样本得分为 $\\{0.40, 0.30, 0.20\\}$。所有正样本得分都高于所有负样本得分。总共有 $3 \\times 3 = 9$ 个正负样本对，全部正确排序。AUC = $9/9 = 1.0$。\n    -   对于 $s^{\\text{new2}}$，正样本得分为 $\\{0.95, 0.70, 0.99\\}$，负样本得分为 $\\{0.05, 0.75, 0.01\\}$。我们检查排序。有一个负样本得分为 0.75，它高于一个正样本得分 0.70。这构成了一个排序错误。在总共 9 个正负样本对中，有 8 个是正确的，1 个是错误的。因此，AUC = $8/9$。\n    -   ROC-AUC 从 1.0 减少到 $8/9$。\n\n-   **NLL 变化**:\n    -   比较 $s^{\\text{old2}}$ 和 $s^{\\text{new2}}$，大多数预测都变得更加校准：正样本 $0.60 \\to 0.95$, $0.80 \\to 0.99$；负样本 $0.40 \\to 0.05$, $0.20 \\to 0.01$。这些变化都极大地降低了 NLL。\n    -   虽然有一个负样本的预测变得很差 ($0.30 \\to 0.75$)，导致其 NLL 项 $-\\log(1-0.75)$ 增大，但其他样本 NLL 的显著减小通常会主导总 NLL 的变化，使其总体降低。我们可以验证这一点：$NLL_{old2} \\approx 2.18$，$NLL_{new2} \\approx 1.87$。NLL 确实降低了。\n\n-   **结论**: 陈述 B \"会降低 NLL，但会减少 ROC-AUC\" 是正确的。这个例子完美地展示了优化交叉熵（以获得更好的校准概率）有时会损害纯粹基于排序的指标。\n\n**分析选项 C**\n\n-   该陈述声称“交叉熵直接优化 ROC-AUC，因为两者都仅依赖于分数的相对排序”。\n-   这个前提是错误的。交叉熵（NLL）依赖于预测概率的**绝对值**，而 ROC-AUC 仅依赖于**相对排序**。\n-   选项 B 已经提供了一个明确的反例：交叉熵降低了，而 ROC-AUC 也降低了。因此，改善 CE 并不必然会增加 ROC-AUC。\n\n-   **结论**: 陈述 C 是错误的。\n\n**分析选项 D**\n\n-   该陈述声称“对所有预测概率应用任何严格递增的变换都会使 NLL 和 ROC-AUC 保持不变”。\n-   对于 **ROC-AUC**：这是正确的。因为严格递增的变换（例如，$\\log(s)$ 或 $s^2$）会保持分数的顺序，而 AUC 只依赖于顺序。\n-   对于 **NLL**：这是错误的。NLL 的公式是 $-\\sum [y_i \\log s_i + (1-y_i)\\log(1-s_i)]$。如果将 $s_i$ 替换为 $f(s_i)$，其中 $f$ 是一个严格递增的变换（例如 $f(s) = \\sqrt{s}$），那么 $\\log s_i$ 会变为 $\\log f(s_i)$，这将改变 NLL 的值。\n\n-   **结论**: 陈述 D 是错误的。\n\n综上所述，只有选项 A 和 B 是正确的。", "answer": "$$\\boxed{AB}$$", "id": "3110742"}]}