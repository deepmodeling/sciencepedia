{"hands_on_practices": [{"introduction": "最速下降法 (Steepest Descent, SD) 的核心挑战之一是步长 $\\alpha$ 的选择。一个过大或不合适的步长不仅会减慢收敛速度，甚至可能导致算法完全发散。这个练习将通过一个简单的简谐振子模型，让你亲手编写代码来探索步长选择对收敛性的决定性影响，并验证稳定收敛的理论条件。[@problem_id:2463036]", "problem": "给定一个一维简谐振子势，定义为 $V(x) = \\tfrac{1}{2} k x^2$，其中 $k > 0$ 是给定的力常数，$x$ 是标量坐标。考虑由下式给出的固定步长最速下降（SD）迭代法\n$$\nx_{n+1} = x_n - \\alpha \\,\\frac{dV}{dx}(x_n),\n$$\n其中步长 $\\alpha \\in \\mathbb{R}$ 为常数，初始值为 $x_0 \\in \\mathbb{R}$。对于该势能，其精确极小值点位于 $x^\\star = 0$。将收敛到极小值点定义为：对于给定的最大迭代次数 $N_{\\max}$ 和容差 $\\varepsilon > 0$，存在某个索引 $n \\le N_{\\max}$ 使得 $|x_n| \\le \\varepsilon$。如果在 $N_{\\max}$ 次迭代内不存在这样的 $n$，则判断为不收敛。\n\n请编写一个完整、可运行的程序，对以下每个测试用例，应用该迭代法来确定收敛或不收敛，并输出一个布尔值结果：\n- 测试用例 1：$k = 3.0$, $\\alpha = 0.3$, $x_0 = 1.0$, $N_{\\max} = 5000$, $\\varepsilon = 10^{-12}$。\n- 测试用例 2：$k = 3.0$, $\\alpha = 0.7$, $x_0 = 1.0$, $N_{\\max} = 5000$, $\\varepsilon = 10^{-12}$。\n- 测试用例 3：$k = 2.0$, $\\alpha = 1.0$, $x_0 = 1.0$, $N_{\\max} = 5000$, $\\varepsilon = 10^{-12}$。\n- 测试用例 4：$k = 1.5$, $\\alpha = 0.0$, $x_0 = 1.0$, $N_{\\max} = 5000$, $\\varepsilon = 10^{-12}$。\n- 测试用例 5：$k = 1.5$, $\\alpha = -0.2$, $x_0 = 1.0$, $N_{\\max} = 5000$, $\\varepsilon = 10^{-12}$。\n- 测试用例 6：$k = 2.0$, $\\alpha = 0.99$, $x_0 = 1.0$, $N_{\\max} = 5000$, $\\varepsilon = 10^{-12}$。\n\n在本计算练习中，所有量均视为无量纲，最终答案为不带物理单位的布尔值。您的程序必须将六个测试用例的结果汇总到单行输出中，该行包含一个由方括号括起来的、以逗号分隔的布尔值列表，不含任何额外文本。例如，输出必须看起来像单行 Python 风格的列表。", "solution": "问题陈述已经过审查，并被确定为有效。它具有科学依据、提法明确、客观，并包含了获得唯一解所需的所有信息。该任务是一个将数值优化应用于基本物理模型的标准练习。我们着手进行分析和求解。\n\n该问题要求确定固定步长最速下降（SD）算法的收敛性，用于最小化由下式给出的一维简谐振子势：\n$$\nV(x) = \\frac{1}{2} k x^2\n$$\n其中 $k > 0$ 是力常数。该势能的极小值点位于 $x^\\star = 0$。\n\nSD 迭代定义为：\n$$\nx_{n+1} = x_n - \\alpha g_n\n$$\n其中 $g_n$ 是势能在 $x_n$ 处的梯度，$\\alpha$ 是一个常数步长。对于这个一维问题，梯度就是对 $x$ 的导数：\n$$\ng_n = \\frac{dV}{dx}(x_n) = k x_n\n$$\n将该导数代入迭代公式，得到一个线性递推关系：\n$$\nx_{n+1} = x_n - \\alpha (k x_n) = (1 - \\alpha k) x_n\n$$\n这是一个公比为 $r = 1 - \\alpha k$ 的等比数列。该递推关系的解由下式给出：\n$$\nx_n = (1 - \\alpha k)^n x_0\n$$\n为了使序列 $\\{x_n\\}$ 在 $n \\to \\infty$ 时收敛到极小值点 $x^\\star = 0$，公比的绝对值必须小于 1：\n$$\n|r| = |1 - \\alpha k| < 1\n$$\n该不等式等价于以下两个条件：\n$$\n-1 < 1 - \\alpha k < 1\n$$\n让我们分别分析这两个不等式。\n1. 右侧不等式：\n$$\n1 - \\alpha k < 1 \\implies -\\alpha k < 0\n$$\n由于力常数 $k$ 已知为正（$k > 0$），此式可简化为 $\\alpha > 0$。\n2. 左侧不等式：\n$$\n-1 < 1 - \\alpha k \\implies \\alpha k < 2\n$$\n同样，由于 $k > 0$，此式可简化为 $\\alpha < 2/k$。\n\n综合这两个结果，该势能下最速下降算法收敛到最小值的充分必要条件是：\n$$\n0 < \\alpha < \\frac{2}{k}\n$$\n如果满足此条件，序列 $\\{x_n\\}$ 将收敛到 0。我们现在必须根据这个解析条件来评估每个测试用例。计算上，收敛定义为在某次迭代 $n \\le N_{\\max}$ 时达到 $|x_n| \\le \\varepsilon$ 的状态。\n\n用例 1：$k = 3.0$, $\\alpha = 0.3$, $x_0 = 1.0$, $N_{\\max} = 5000$, $\\varepsilon = 10^{-12}$。\n$\\alpha$ 的收敛范围是 $0 < \\alpha < 2/3.0$，约等于 $0 < \\alpha < 0.667$。给定的步长 $\\alpha = 0.3$ 在此范围内。公比为 $r = 1 - (0.3)(3.0) = 0.1$。由于 $|r| < 1$，迭代将会收敛。结果是 **True**。\n\n用例 2：$k = 3.0$, $\\alpha = 0.7$, $x_0 = 1.0$, $N_{\\max} = 5000$, $\\varepsilon = 10^{-12}$。\n$\\alpha$ 的收敛范围是 $0 < \\alpha < 0.667$。步长 $\\alpha = 0.7$ 在此范围之外。公比为 $r = 1 - (0.7)(3.0) = 1 - 2.1 = -1.1$。由于 $|r| > 1$，序列将会发散，后续点将交替变号且数值增大。结果是 **False**。\n\n用例 3：$k = 2.0$, $\\alpha = 1.0$, $x_0 = 1.0$, $N_{\\max} = 5000$, $\\varepsilon = 10^{-12}$。\n$\\alpha$ 的收敛范围是 $0 < \\alpha < 2/2.0$，即 $0 < \\alpha < 1.0$。步长 $\\alpha = 1.0$ 是一个边界情况，此时 $\\alpha = 2/k$。公比为 $r = 1 - (1.0)(2.0) = -1.0$。序列变为 $x_{n+1} = -x_n$。从 $x_0 = 1.0$ 开始，值将是 $1.0, -1.0, 1.0, -1.0, \\dots$。其绝对值 $|x_n|$ 保持在 $1.0$ 不变，永远不会小于或等于 $\\varepsilon = 10^{-12}$。它不收敛。结果是 **False**。\n\n用例 4：$k = 1.5$, $\\alpha = 0.0$, $x_0 = 1.0$, $N_{\\max} = 5000$, $\\varepsilon = 10^{-12}$。\n步长是 $\\alpha = 0.0$。这是收敛条件 $0 < \\alpha < 2/k$ 的一个边界情况。迭代变为 $x_{n+1} = x_n - 0 \\cdot (kx_n) = x_n$。位置永远不会偏离其初始值 $x_0 = 1.0$。由于 $|1.0| > \\varepsilon = 10^{-12}$，收敛条件永远不会满足。算法停滞。结果是 **False**。\n\n用例 5：$k = 1.5$, $\\alpha = -0.2$, $x_0 = 1.0$, $N_{\\max} = 5000$, $\\varepsilon = 10^{-12}$。\n步长 $\\alpha = -0.2$ 为负，违反了收敛条件 $\\alpha > 0$。公比为 $r = 1 - (-0.2)(1.5) = 1 + 0.3 = 1.3$。由于 $|r| > 1$，序列将单调发散。结果是 **False**。\n\n用例 6：$k = 2.0$, $\\alpha = 0.99$, $x_0 = 1.0$, $N_{\\max} = 5000$, $\\varepsilon = 10^{-12}$。\n$\\alpha$ 的收敛范围是 $0 < \\alpha < 2/2.0$，即 $0 < \\alpha < 1.0$。步长 $\\alpha = 0.99$ 在此范围内。公比为 $r = 1 - (0.99)(2.0) = 1 - 1.98 = -0.98$。由于 $|r| = |-0.98| < 1$，迭代将会收敛，尽管由于公比接近 -1，收敛会很慢，并导致振荡收敛。结果是 **True**。\n\n现在将构建一个计算程序，为每个用例实现迭代，并验证这些分析结论。程序将最多循环 $N_{\\max}$ 次迭代，在 $n=0, 1, \\dots, N_{\\max}$ 的每一步检查收敛条件 $|x_n| \\le \\varepsilon$。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the steepest descent convergence problem for six test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # Each case is a tuple: (k, alpha, x0, N_max, epsilon)\n    test_cases = [\n        (3.0, 0.3, 1.0, 5000, 1e-12),\n        (3.0, 0.7, 1.0, 5000, 1e-12),\n        (2.0, 1.0, 1.0, 5000, 1e-12),\n        (1.5, 0.0, 1.0, 5000, 1e-12),\n        (1.5, -0.2, 1.0, 5000, 1e-12),\n        (2.0, 0.99, 1.0, 5000, 1e-12),\n    ]\n\n    results = []\n    for case in test_cases:\n        k, alpha, x0, N_max, epsilon = case\n        \n        # Initialize position x and convergence flag\n        x = x0\n        converged = False\n\n        # The problem asks if there is an index n <= N_max such that |x_n| <= epsilon.\n        # This means we check x_0, x_1, ..., x_{N_max}.\n        # This requires N_max+1 checks and N_max updates.\n        for n in range(N_max + 1):\n            \n            # Check for convergence at the current position x_n\n            if abs(x) <= epsilon:\n                converged = True\n                break\n\n            # To avoid an unnecessary update after the last check, we add this condition.\n            if n < N_max:\n                # Calculate the gradient for the simple harmonic oscillator potential V(x) = 1/2*k*x^2.\n                # The gradient (derivative) is dV/dx = k*x.\n                gradient = k * x\n                \n                # Apply the steepest descent update rule.\n                x = x - alpha * gradient\n        \n        results.append(converged)\n\n    # Final print statement in the exact required format.\n    # The output should be a string that looks like a Python list of booleans.\n    # str(True) is 'True', str(False) is 'False', so this works as intended.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2463036"}, {"introduction": "在更真实的各向异性势能面（形状像狭长的山谷）上，固定步长的最速下降法会遇到更棘手的问题。这个练习展示了一种特殊的失效模式——极限环 (limit cycle)，即优化器在两个点之间无限振荡，永远无法到达最小值点。通过精确计算导致这种病态行为的步长，你将深刻理解为何简单固定步长的方法在实际问题中是不可靠的。[@problem_id:2463052]", "problem": "在计算化学中，局域几何优化通常通过最小化势能面（PES）来实现。考虑一个近似为局域二次势阱的二维势能面（PES），其以无量纲单位表示为\n$$\nE(x,y) = \\tfrac{1}{2}\\big(x^{2} + 3\\,y^{2}\\big).\n$$\n应用具有常数步长参数 $s$ 的最速下降算法（SD）来最小化 $E(x,y)$。根据定义，坐标 $\\mathbf{r}_{k} = (x_{k},y_{k})$ 的最速下降更新公式为\n$$\n\\mathbf{r}_{k+1} = \\mathbf{r}_{k} - s\\,\\nabla E(\\mathbf{r}_{k}).\n$$\n假设初始条件为 $\\mathbf{r}_{0} = (0,y_{0})$，其中 $y_{0} \\neq 0$。\n\n确定常数步长 $s$ 的值（表示为一个精确的、简化的分数），使得最速下降迭代陷入一个不收敛的两点极限环，永远在 $(0,y_{0})$ 和 $(0,-y_{0})$ 之间交替。使用无量纲单位，答案中不要包含单位。无需四舍五入。", "solution": "该问题提法恰当且科学上合理。我们着手进行推导。\n\n势能面（PES）由以下函数给出：\n$$\nE(x,y) = \\frac{1}{2}(x^{2} + 3y^{2})\n$$\n最速下降（SD）算法根据以下规则更新坐标 $\\mathbf{r}_{k} = (x_{k}, y_{k})$：\n$$\n\\mathbf{r}_{k+1} = \\mathbf{r}_{k} - s \\nabla E(\\mathbf{r}_{k})\n$$\n其中 $s$ 是常数步长。\n\n首先，我们必须计算能量函数的梯度 $\\nabla E(x,y)$。其偏导数为：\n$$\n\\frac{\\partial E}{\\partial x} = \\frac{\\partial}{\\partial x} \\left( \\frac{1}{2}x^{2} + \\frac{3}{2}y^{2} \\right) = x\n$$\n$$\n\\frac{\\partial E}{\\partial y} = \\frac{\\partial}{\\partial y} \\left( \\frac{1}{2}x^{2} + \\frac{3}{2}y^{2} \\right) = 3y\n$$\n因此，梯度向量为：\n$$\n\\nabla E(x,y) = \\begin{pmatrix} x \\\\ 3y \\end{pmatrix}\n$$\n将梯度代入最速下降更新规则，我们得到坐标 $(x_{k}, y_{k})$ 的分量形式递推关系：\n$$\n\\begin{pmatrix} x_{k+1} \\\\ y_{k+1} \\end{pmatrix} = \\begin{pmatrix} x_{k} \\\\ y_{k} \\end{pmatrix} - s \\begin{pmatrix} x_{k} \\\\ 3y_{k} \\end{pmatrix}\n$$\n这得到两个独立的方程：\n$$\nx_{k+1} = x_{k} - s x_{k} = (1 - s) x_{k}\n$$\n$$\ny_{k+1} = y_{k} - s (3y_{k}) = (1 - 3s) y_{k}\n$$\n初始条件给定为 $\\mathbf{r}_{0} = (0, y_{0})$，其中 $y_{0} \\neq 0$。\n\n我们来分析 $x$ 坐标的行为。当 $x_{0} = 0$ 时，$x$ 的递推关系给出：\n$$\nx_{1} = (1-s)x_{0} = (1-s)(0) = 0\n$$\n通过归纳法，可以轻易证明对于所有整数 $k \\ge 0$，都有 $x_{k} = 0$。这与指定的极限环点 $(0, y_{0})$ 和 $(0, -y_{0})$ 相符，这些点位于 $y$ 轴上。\n\n现在，我们分析 $y$ 坐标的行为。问题指出，系统进入一个两点极限环，在 $(0, y_{0})$ 和 $(0, -y_{0})$ 之间交替。这意味着对于偶数 $k$，有 $\\mathbf{r}_{k} = (0, y_{0})$，而对于奇数 $k$，有 $\\mathbf{r}_{k} = (0, -y_{0})$。\n\n我们考虑从 $k=0$ 到 $k=1$ 的第一步迭代。\n我们从 $\\mathbf{r}_{0} = (0, y_{0})$ 开始。下一个点必须是 $\\mathbf{r}_{1} = (0, -y_{0})$。\n使用 $k=0$ 时 $y$ 的递推关系：\n$$\ny_{1} = (1 - 3s) y_{0}\n$$\n为使迭代与极限环匹配，我们需要 $y_{1} = -y_{0}$。因此：\n$$\n-y_{0} = (1 - 3s) y_{0}\n$$\n因为给定 $y_{0} \\neq 0$，我们可以将等式两边同时除以 $y_{0}$：\n$$\n-1 = 1 - 3s\n$$\n求解这个关于步长 $s$ 的方程：\n$$\n3s = 1 - (-1) = 2\n$$\n$$\ns = \\frac{2}{3}\n$$\n我们必须验证 $s$ 的这个值能否无限维持该极限环。如果 $s = \\frac{2}{3}$，则 $y$ 的递推关系变为：\n$$\ny_{k+1} = \\left(1 - 3 \\cdot \\frac{2}{3}\\right) y_{k} = (1 - 2) y_{k} = -y_{k}\n$$\n我们来检验迭代序列：\n当 $k=0$ 时：$\\mathbf{r}_{0} = (0, y_{0})$。\n当 $k=1$ 时：$y_{1} = -y_{0}$，所以 $\\mathbf{r}_{1} = (0, -y_{0})$。\n当 $k=2$ 时：$y_{2} = -y_{1} = -(-y_{0}) = y_{0}$，所以 $\\mathbf{r}_{2} = (0, y_{0})$。\n当 $k=3$ 时：$y_{3} = -y_{2} = -y_{0}$，所以 $\\mathbfr_{3} = (0, -y_{0})$。\n点的序列是 $(0, y_{0}), (0, -y_{0}), (0, y_{0}), (0, -y_{0}), \\dots$，这恰好是指定的两点极限环。该算法永远不会收敛到位于 $(0,0)$ 的最小值点。\n\n产生这种行为的常数步长 $s$ 的值是 $\\frac{2}{3}$。", "answer": "$$\\boxed{\\frac{2}{3}}$$", "id": "2463052"}, {"introduction": "前面的练习暴露了最速下降法的弱点，现在我们引入一种更强大的算法：共轭梯度法 (Conjugate Gradient, CG)。该练习将让你通过一个关键的数值实验，直接比较 SD 和 CG 的性能。你将编写程序，在一系列“病态程度”（由矩阵的条件数 $\\kappa$ 衡量）不断增加的势能面上测试这两种算法，从而定量地见证 CG 方法在处理复杂优化问题时的巨大优势。[@problem_id:2463021]", "problem": "给定二次能量函数 $E(\\mathbf{x}) = \\tfrac{1}{2}\\,\\mathbf{x}^{\\mathsf{T}}\\mathbf{A}\\mathbf{x}$，其中 $\\mathbf{A}$ 是一个大小为 $n \\times n$ 的实对称正定矩阵。考虑使用两种迭代方法来寻找 $E(\\mathbf{x})$ 的极小值点：最速下降法 (SD) 和共轭梯度法 (CG)。对于每种方法，给定矩阵 $\\mathbf{A}$ 和初始点 $\\mathbf{x}_0$，迭代次数定义为满足以下基于残差的停止准则\n$$\n\\frac{\\lVert \\mathbf{A}\\mathbf{x}_m \\rVert_2}{\\lVert \\mathbf{A}\\mathbf{x}_0 \\rVert_2} \\le \\varepsilon\n$$\n的最小非负整数 $m$，其中 $\\varepsilon$ 是一个指定的容差，$\\lVert \\cdot \\rVert_2$ 是欧几里得范数。如果在指定的迭代次数上限 $m_{\\max}$ 内仍未满足该准则，则将 $m_{\\max}$ 作为迭代次数报告。\n\n通过设置 $n = 60$，选择所有分量均为1的初始向量 $\\mathbf{x}_0 \\in \\mathbb{R}^n$，并对于给定的条件数 $\\kappa \\ge 1$，将 $\\mathbf{A}_\\kappa$ 的对角线元素 $\\lambda_i$ 定义为从 $1$ 到 $\\kappa$ 的等比数列，来构造一系列具有指定条件数的对角测试矩阵 $\\mathbf{A}_\\kappa$：\n$$\n\\lambda_i = \\kappa^{\\frac{i-1}{n-1}}, \\quad i = 1,2,\\dots,n,\n$$\n从而使得 $\\mathbf{A}_\\kappa = \\mathrm{diag}(\\lambda_1,\\lambda_2,\\dots,\\lambda_n)$ 具有 $\\mathrm{cond}_2(\\mathbf{A}_\\kappa) = \\kappa$。对两种方法均使用基于残差的停止容差 $\\varepsilon = 10^{-8}$ 和迭代次数上限 $m_{\\max} = 100000$。\n\n测试套件：\n- 矩阵维度：$n = 60$。\n- 初始向量：$\\mathbf{x}_0 = (1,1,\\dots,1)^{\\mathsf{T}} \\in \\mathbb{R}^{60}$。\n- 容差：$\\varepsilon = 10^{-8}$。\n- 迭代次数上限：$m_{\\max} = 100000$。\n- 待测试的条件数：$\\kappa \\in \\{1,5,20,100,500\\}$。\n\n对于测试套件中的每个 $\\kappa$，计算：\n- 最速下降法满足停止准则所需的迭代次数 $m_{\\mathrm{SD}}(\\kappa)$（如果在上限内未满足，则为 $m_{\\max}$）。\n- 共轭梯度法满足停止准则所需的迭代次数 $m_{\\mathrm{CG}}(\\kappa)$（如果在上限内未满足，则为 $m_{\\max}$）。\n\n最终输出格式：\n您的程序应生成单行输出，该输出是一个由方括号括起来的逗号分隔列表，每个测试用例对应一对内部数对。每个内部数对必须是两个整数 $[m_{\\mathrm{SD}}(\\kappa),m_{\\mathrm{CG}}(\\kappa)]$，其顺序与所列测试条件数的顺序相同，并且行内任何地方都不能有空格。具体来说，对于五个指定的测试用例，要求的输出格式是形如\n$$\n\\bigl[\\,[m_{\\mathrm{SD}}(1),m_{\\mathrm{CG}}(1)],\\,[m_{\\mathrm{SD}}(5),m_{\\mathrm{CG}}(5)],\\,[m_{\\mathrm{SD}}(20),m_{\\mathrm{CG}}(20)],\\,[m_{\\mathrm{SD}}(100),m_{\\mathrm{CG}}(100)],\\,[m_{\\mathrm{SD}}(500),m_{\\mathrm{CG}}(500)]\\,\\bigr]\n$$\n的单行。这些条目必须是整数，并且必须完全按照所示格式出现，只有方括号和逗号，没有空格。", "solution": "问题陈述经过严格分析并被认定为有效。\n\n**1. 问题验证**\n\n**步骤1：提取的已知条件**\n- 能量函数：$E(\\mathbf{x}) = \\tfrac{1}{2}\\,\\mathbf{x}^{\\mathsf{T}}\\mathbf{A}\\mathbf{x}$。\n- 矩阵 $\\mathbf{A}$：一个大小为 $n \\times n$ 的实对称正定矩阵。\n- 迭代方法：最速下降法 (SD) 和共轭梯度法 (CG)。\n- 迭代次数 $m$：满足停止准则的最小非负整数。\n- 停止准则：$\\frac{\\lVert \\mathbf{A}\\mathbf{x}_m \\rVert_2}{\\lVert \\mathbf{A}\\mathbf{x}_0 \\rVert_2} \\le \\varepsilon$。\n- 容差：$\\varepsilon = 10^{-8}$。\n- 迭代上限：$m_{\\max} = 100000$。\n- 测试矩阵构造：对于一个条件数 $\\kappa$，$\\mathbf{A}_\\kappa = \\mathrm{diag}(\\lambda_1, \\dots, \\lambda_n)$，其中 $\\lambda_i = \\kappa^{\\frac{i-1}{n-1}}$，$i=1, \\dots, n$。\n- 矩阵维度：$n = 60$。\n- 初始向量：$\\mathbf{x}_0 = (1,1,\\dots,1)^{\\mathsf{T}}$。\n- 测试套件：$\\kappa \\in \\{1, 5, 20, 100, 500\\}$。\n- 任务：为每个 $\\kappa$ 计算迭代次数 $m_{\\mathrm{SD}}(\\kappa)$ 和 $m_{\\mathrm{CG}}(\\kappa)$。\n\n**步骤2：使用提取的已知条件进行验证**\n- **科学上合理：** 该问题是数值线性代数中的一个标准练习，旨在比较两种基本优化算法在定义明确的二次型上的收敛速度。最小化 $E(\\mathbf{x})$ 等价于求解线性系统 $\\mathbf{A}\\mathbf{x} = \\mathbf{0}$，由于 $\\mathbf{A}$ 是正定的，其唯一解为 $\\mathbf{x} = \\mathbf{0}$。基于残差的停止准则是迭代方法中的一种标准做法。\n- **适定性：** 该问题是定义明确的。测试矩阵的构造是无歧义的。对角线元素 $\\lambda_i$ 构成一个从 $\\lambda_1 = \\kappa^{\\frac{0}{n-1}} = 1$ 到 $\\lambda_n = \\kappa^{\\frac{n-1}{n-1}} = \\kappa$ 的等比数列。对于对称正定矩阵，其2-范数条件数是最大特征值与最小特征值之比，即 $\\mathrm{cond}_2(\\mathbf{A}_\\kappa) = \\frac{\\lambda_{\\max}}{\\lambda_{\\min}} = \\frac{\\kappa}{1} = \\kappa$。因此，该构造是正确的。所有参数（$n, \\mathbf{x}_0, \\varepsilon, m_{\\max}, \\kappa$ 值）都已明确给出。解的存在性和唯一性得到保证。\n- **客观性：** 该问题以精确、客观的数学语言表述，没有歧义或主观论断。\n\n**步骤3：结论与行动**\n该问题是有效的，因为它是科学合理的、适定的和客观的。将提供一个解决方案。\n\n**2. 解法推导**\n\n问题要求最小化二次函数 $E(\\mathbf{x}) = \\tfrac{1}{2}\\,\\mathbf{x}^{\\mathsf{T}}\\mathbf{A}\\mathbf{x}$。该函数的最小值出现在其关于 $\\mathbf{x}$ 的梯度为零的地方。梯度由下式给出：\n$$\n\\nabla E(\\mathbf{x}) = \\mathbf{A}\\mathbf{x}\n$$\n将梯度设为零，我们得到线性系统 $\\mathbf{A}\\mathbf{x} = \\mathbf{0}$。由于矩阵 $\\mathbf{A}$ 被指定为正定矩阵，它是可逆的，因此 $E(\\mathbf{x})$ 的唯一极小值点是零解 $\\mathbf{x}^* = \\mathbf{0}$。\n\n问题在于，从 $\\mathbf{x}_0 = (1,1,\\dots,1)^{\\mathsf{T}}$ 开始，求最速下降法（SD）和共轭梯度法（CG）收敛到此解所需的迭代次数。收敛性由停止准则 $\\frac{\\lVert \\mathbf{A}\\mathbf{x}_m \\rVert_2}{\\lVert \\mathbf{A}\\mathbf{x}_0 \\rVert_2} \\le \\varepsilon$ 来衡量。向量 $\\mathbf{r}_m = \\mathbf{0} - \\mathbf{A}\\mathbf{x}_m = -\\mathbf{A}\\mathbf{x}_m$ 是系统 $\\mathbf{A}\\mathbf{x} = \\mathbf{0}$ 的残差。因此，该准则等价于一个相对残差停止准则：$\\frac{\\lVert \\mathbf{r}_m \\rVert_2}{\\lVert \\mathbf{r}_0 \\rVert_2} \\le \\varepsilon$。\n\n测试矩阵 $\\mathbf{A}_\\kappa$ 是对角矩阵，其元素为 $\\lambda_i = \\kappa^{\\frac{i-1}{n-1}}$（$i=1, \\dots, n=60$）。这种构造确保了特征值在 $\\lambda_{\\min} = 1$ 和 $\\lambda_{\\max} = \\kappa$ 之间呈对数间隔分布，从而得到一个具有指定条件数 $\\mathrm{cond}_2(\\mathbf{A}_\\kappa) = \\kappa$ 的矩阵。\n\n**最速下降法 (SD) 算法**\n最速下降法是一种迭代优化算法，每一步都沿负梯度方向移动。对于函数 $E(\\mathbf{x})$，在迭代点 $\\mathbf{x}_k$ 处的搜索方向为 $\\mathbf{p}_k = -\\nabla E(\\mathbf{x}_k) = -\\mathbf{A}\\mathbf{x}_k$。下一个迭代点通过线性搜索找到：$\\mathbf{x}_{k+1} = \\mathbf{x}_k + \\alpha_k \\mathbf{p}_k$。使 $E(\\mathbf{x}_{k+1})$ 最小化的最优步长 $\\alpha_k$ 由下式给出：\n$$\n\\alpha_k = \\frac{\\mathbf{p}_k^{\\mathsf{T}}\\mathbf{p}_k}{\\mathbf{p}_k^{\\mathsf{T}}\\mathbf{A}\\mathbf{p}_k} = \\frac{(-\\mathbf{A}\\mathbf{x}_k)^{\\mathsf{T}}(-\\mathbf{A}\\mathbf{x}_k)}{(-\\mathbf{A}\\mathbf{x}_k)^{\\mathsf{T}}\\mathbf{A}(-\\mathbf{A}\\mathbf{x}_k)}\n$$\n如果我们定义残差 $\\mathbf{r}_k = -\\mathbf{A}\\mathbf{x}_k$，则算法简化为：\n1. 初始化 $\\mathbf{x}_0$，$\\mathbf{r}_0 = -\\mathbf{A}\\mathbf{x}_0$。\n2. 对于 $k=0, 1, 2, \\dots$：\n   a. 计算步长：$\\alpha_k = \\frac{\\mathbf{r}_k^{\\mathsf{T}}\\mathbf{r}_k}{\\mathbf{r}_k^{\\mathsf{T}}\\mathbf{A}\\mathbf{r}_k}$。\n   b. 更新解：$\\mathbf{x}_{k+1} = \\mathbf{x}_k + \\alpha_k \\mathbf{r}_k$。\n   c. 更新残差：$\\mathbf{r}_{k+1} = \\mathbf{r}_k - \\alpha_k \\mathbf{A}\\mathbf{r}_k$。\n   d. 检查停止准则。\n\n**共轭梯度法 (CG) 算法**\n共轭梯度法通过选择 $\\mathbf{A}$-正交（或“共轭”）的搜索方向来改进最速下降法。这可以防止破坏在先前方向上取得的最小化进展，并保证在精确算术下至多 $n$ 步收敛。\n1. 初始化 $\\mathbf{x}_0$，$\\mathbf{r}_0 = -\\mathbf{A}\\mathbf{x}_0$，$\\mathbf{p}_0 = \\mathbf{r}_0$。\n2. 对于 $k=0, 1, 2, \\dots$：\n   a. 计算步长：$\\alpha_k = \\frac{\\mathbf{r}_k^{\\mathsf{T}}\\mathbf{r}_k}{\\mathbf{p}_k^{\\mathsf{T}}\\mathbf{A}\\mathbf{p}_k}$。\n   b. 更新解：$\\mathbf{x}_{k+1} = \\mathbf{x}_k + \\alpha_k \\mathbf{p}_k$。\n   c. 更新残差：$\\mathbf{r}_{k+1} = \\mathbf{r}_k - \\alpha_k \\mathbf{A}\\mathbf{p}_k$。\n   d. 检查停止准则。\n   e. 计算改进因子：$\\beta_k = \\frac{\\mathbf{r}_{k+1}^{\\mathsf{T}}\\mathbf{r}_{k+1}}{\\mathbf{r}_k^{\\mathsf{T}}\\mathbf{r}_k}$。\n   f. 更新搜索方向：$\\mathbf{p}_{k+1} = \\mathbf{r}_{k+1} + \\beta_k \\mathbf{p}_k$。\n\n实现将遵循这些算法。它将从 $m=1$ 到 $m_{\\max}$ 检查每个迭代点 $\\mathbf{x}_m$ 的停止准则，并报告第一个满足准则的 $m$ 值。如果循环完成，则报告 $m_{\\max}$。由于矩阵 $\\mathbf{A}_\\kappa$ 是对角矩阵，矩阵-向量乘积 $\\mathbf{A}\\mathbf{v}$ 可以通过对角线元素（特征值）与向量 $\\mathbf{v}$ 进行逐元素相乘来高效地计算。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve_sd(lambdas, x0, tol, max_iter):\n    \"\"\"\n    Solves Ax=0 using Steepest Descent for a diagonal matrix A.\n\n    Args:\n        lambdas (np.ndarray): Diagonal entries of matrix A.\n        x0 (np.ndarray): Initial vector.\n        tol (float): Tolerance for the stopping criterion.\n        max_iter (int): Maximum number of iterations.\n\n    Returns:\n        int: The number of iterations required for convergence.\n    \"\"\"\n    x = x0.copy()\n    \n    # The term in the stopping criterion is v_m = A @ x_m.\n    # We use the fact that r_m = -v_m for the system Ax=0.\n    v0 = lambdas * x0\n    norm_v0 = np.linalg.norm(v0)\n    \n    # If x0 is the solution, iterations = 0.\n    if norm_v0 == 0:\n        return 0\n\n    # For SD, the search direction is the residual r.\n    # For Ax=0, r = 0 - Ax = -Ax.\n    r = -v0\n    \n    for m in range(1, max_iter + 1):\n        # Efficiently compute A @ r since A is diagonal\n        Ar = lambdas * r\n        \n        # Calculate optimal step size alpha\n        # np.dot is used for vector dot products.\n        r_sq_norm = np.dot(r, r)\n        alpha = r_sq_norm / np.dot(r, Ar)\n        \n        # Update solution and residual\n        x += alpha * r\n        r -= alpha * Ar\n        \n        # Check stopping criterion: ||A*x_m|| / ||A*x_0|| <= tol\n        # Since r_m = -A*x_m, we can use norm(r) which is ||r_m||.\n        norm_v_m = np.linalg.norm(r)\n        \n        if norm_v_m / norm_v0 <= tol:\n            return m\n            \n    return max_iter\n\ndef solve_cg(lambdas, x0, tol, max_iter):\n    \"\"\"\n    Solves Ax=0 using Conjugate Gradient for a diagonal matrix A.\n\n    Args:\n        lambdas (np.ndarray): Diagonal entries of matrix A.\n        x0 (np.ndarray): Initial vector.\n        tol (float): Tolerance for the stopping criterion.\n        max_iter (int): Maximum number of iterations.\n\n    Returns:\n        int: The number of iterations required for convergence.\n    \"\"\"\n    x = x0.copy()\n    \n    v0 = lambdas * x0\n    norm_v0 = np.linalg.norm(v0)\n    \n    if norm_v0 == 0:\n        return 0\n    \n    r = -v0\n    p = r.copy() # Initial search direction\n    rs_old = np.dot(r, r)\n    \n    for m in range(1, max_iter + 1):\n        # Efficiently compute A @ p\n        Ap = lambdas * p\n        \n        alpha = rs_old / np.dot(p, Ap)\n        \n        # Update solution and residual\n        x += alpha * p\n        r -= alpha * Ap\n        \n        rs_new = np.dot(r, r)\n        \n        # Check stopping criterion using the norm of the new residual\n        norm_v_m = np.sqrt(rs_new)\n        if norm_v_m / norm_v0 <= tol:\n            return m\n            \n        # Update search direction\n        beta = rs_new / rs_old\n        p = r + beta * p\n        \n        rs_old = rs_new\n            \n    return max_iter\n\ndef solve():\n    \"\"\"\n    Main function to run the simulations and print the results.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [1.0, 5.0, 20.0, 100.0, 500.0]\n\n    # Parameters from problem description\n    n = 60\n    x0 = np.ones(n)\n    tol = 1e-8\n    max_iter = 100000\n    \n    results = []\n    for kappa in test_cases:\n        # Construct the diagonal entries (eigenvalues) of matrix A\n        if kappa == 1.0:\n            lambdas = np.ones(n)\n        else:\n            i = np.arange(1, n + 1)\n            power = (i - 1) / (n - 1)\n            lambdas = np.power(kappa, power)\n        \n        # Run Steepest Descent and Conjugate Gradient methods\n        m_sd = solve_sd(lambdas, x0, tol, max_iter)\n        m_cg = solve_cg(lambdas, x0, tol, max_iter)\n        \n        result_str = f\"[{m_sd},{m_cg}]\"\n        results.append(result_str)\n\n    # Final print statement in the exact required format.\n    print(f\"[[{','.join(results)}]]\")\n\nsolve()\n\n```", "id": "2463021"}]}