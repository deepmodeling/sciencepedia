{"hands_on_practices": [{"introduction": "为了让机器学习势函数（MLP）能够理解化学环境，直接使用原子在空间中的笛卡尔坐标是行不通的，因为这些坐标会随着整个系统的平移和旋转而改变。一个优雅的解决方案是引入“对称函数” (Symmetry Functions)，它能将原子坐标转化为一个固定大小且在这些变换下保持不变的局部原子环境“指纹”。这个练习将指导你从基本对称性原理出发，亲手实现一个基础的双体对称函数 $G^2$，将抽象的物理原理与具体的编程实践联系起来。[@problem_id:2457438]", "problem": "实现一个程序，用于推导和计算氩 (Ar) 原子的基本双体 Behler–Parrinello 型对称函数，并在一个小的几何结构测试集上对其进行评估。其目的是将机器学习原子间势的不变性要求与一个具体的描述符联系起来，并展示在不同参数选择下的数值行为。总体背景是，系统的总势能面可以近似为原子贡献的总和，每个原子贡献都取决于其邻域的局域化、对称不变的表示，正如在高维神经网络势 (HDNNP) 中那样。你的任务是从不变性原理出发，推导一个双体径向对称函数并加以实现。\n\n从以下基本原则出发：\n- 标量势能的平移和旋转不变性意味着，原子的局域描述符必须由内坐标（例如原子间距离）构建。\n- 对于有限范围的相互作用以及学习映射中的局域性，在一个有限半径处施加一个平滑的截断，使得超出截断半径的远距离原子没有贡献，并且力保持良好性状。\n- 为了解析原子周围的径向分布，使用一个径向基，其宽度和中心参数可调，从而使表示能够在不同长度尺度上区分环境。\n\n根据这些原则，为选定的中心原子 $i$ 推导并实现一个形式如下的双体径向对称函数 $G^2$：\n- 对邻近原子 $j \\neq i$ 的求和，\n- 一个平滑的、有限范围的截断函数，该函数是 $C^1$ 连续的，并在截断半径处等于零，\n- 以及一个局域化的径向权重，它可以在选定距离周围移动和锐化。\n\n在你的推导和实现中，具体指定并使用以下形式：\n- 使用余弦截断函数\n$$\nf_c(r; R_c) = \n\\begin{cases}\n\\dfrac{1}{2}\\left[\\cos\\!\\left(\\dfrac{\\pi r}{R_c}\\right) + 1\\right], & r \\le R_c,\\\\\n0, & r > R_c,\n\\end{cases}\n$$\n其中余弦函数的参数以弧度为单位。\n- 使用类高斯径向基\n$$\n\\exp\\!\\left[-\\eta\\,(r - R_s)^2\\right],\n$$\n其中 $\\eta$ 为宽度参数，$R_s$ 为位移参数。\n- 将它们组合成双体对称函数\n$$\nG_i^{2}(\\eta, R_s, R_c) = \\sum_{j \\ne i} \\exp\\!\\left[-\\eta\\,(r_{ij} - R_s)^2\\right]\\, f_c(r_{ij}; R_c),\n$$\n其中 $r_{ij}$ 是原子 $i$ 和 $j$ 之间的欧几里得距离。\n\n所有原子都是氩 (Ar)，被视为单一化学物种，因此不需要依赖于物种的权重。距离 $r_{ij}$、截断半径 $R_c$ 和位移 $R_s$ 必须以埃 (Ångström) 为单位，$\\eta$ 的单位为 $\\text{Å}^{-2}$。余弦函数必须以弧度作为其参数。\n\n程序要求：\n- 实现一个函数，给定一组笛卡尔坐标 (以埃为单位)、中心原子的索引 $i$ 以及参数 $(\\eta, R_s, R_c)$，使用上述公式计算 $G_i^{2}(\\eta, R_s, R_c)$。\n- 使用标准的三维欧几里得距离。不应用周期性边界条件。\n- 数值稳定性：排除自身相互作用 ($j = i$)。距离 $r_{ij}$ 严格非负；除了排除自身相互作用外，不对 $r_{ij} = 0$ 进行特殊处理。\n\n测试集：\n对以下五个案例中的每一个评估 $G_i^{2}$。每个案例指定了 $(\\text{positions}, i, R_c, \\eta, R_s)$，其中所有距离以埃为单位，$\\eta$ 以 $\\text{Å}^{-2}$ 为单位：\n- 案例 A:\n  - 位置: $\\big[(0,0,0),(2.0,0,0),(0,3.0,0),(0,0,4.0)\\big]$\n  - $i = 0$\n  - $R_c = 5.0$\n  - $\\eta = 0.5$\n  - $R_s = 0.0$\n- 案例 B:\n  - 位置: $\\big[(0,0,0)\\big]$\n  - $i = 0$\n  - $R_c = 3.0$\n  - $\\eta = 1.0$\n  - $R_s = 0.0$\n- 案例 C:\n  - 位置: $\\big[(0,0,0),(5.0,0,0),(-5.0,0,0)\\big]$\n  - $i = 0$\n  - $R_c = 5.0$\n  - $\\eta = 1.0$\n  - $R_s = 0.0$\n- 案例 D:\n  - 位置: $\\big[(0,0,0),(2.0,0,0),(0,3.0,0),(0,0,4.0)\\big]$\n  - $i = 0$\n  - $R_c = 5.0$\n  - $\\eta = 2.0$\n  - $R_s = 2.5$\n- 案例 E:\n  - 位置: $\\big[(0,0,0),(2.0,0,0),(0,3.0,0),(0,0,4.0)\\big]$\n  - $i = 1$\n  - $R_c = 5.0$\n  - $\\eta = 0.5$\n  - $R_s = 0.0$\n\n输出规格：\n- 对每个案例，计算一个单一的浮点值 $G_i^{2}$。\n- 使用标准舍入法将每个结果四舍五入到恰好 $6$ 位小数。\n- 你的程序应生成单行输出，其中包含用方括号括起来的、以逗号分隔的结果列表，顺序为 A, B, C, D, E。例如，使用通用占位符的输出应如下所示：“[0.123456,0.000000,0.000000,1.234567,0.654321]”。", "solution": "所提出的问题是有效的、科学上合理的且定义明确的。它要求推导和实现一个双体径向对称函数，这是现代机器学习原子间势（例如高维神经网络势 HDNNP）的一个基本组成部分。我们将首先从第一性原理出发推导出该函数的形式，然后详细说明其计算算法。\n\n一个原子系统的势能 $E$ 是一个标量。为使其具有物理意义，它必须在整个系统的平移和旋转下保持不变，以及在相同原子的置换下保持不变。在 HDNNP 方案中，总能量被分解为原子贡献 $E_i$，其中 $E = \\sum_i E_i$。每个原子能量 $E_i$ 是原子 $i$ 局域环境的函数，由一组描述符或“对称函数” $\\{G_i\\}$ 来表征。因此，这些对称函数本身必须对上述变换保持不变。\n\n1.  **平移和旋转不变性**：这些对称性要求原子 $i$ 的描述符必须仅依赖于其局域环境的内坐标，而不是全局坐标系中原子的绝对笛卡尔坐标。最简单的一组内坐标由中心原子 $i$ 与其邻居 $j$ 之间的标量距离 $r_{ij}$ 组成。这些距离的任何函数 $G_i = F(\\{r_{ij}\\}_{j \\neq i})$ 对原子组合的刚性平移和旋转都自动保持不变。\n\n2.  **置换不变性**：原子 $i$ 的能量贡献绝不能依赖于其相同邻居的任意标记。如果原子 $j$ 和 $k$ 属于同一物种，交换它们绝不能改变描述符的值。满足此条件的最简单的数学结构是对所有邻居求和。因此，我们提出一个形式为 $G_i = \\sum_{j \\neq i} g(r_{ij})$ 的描述符，其中 $g$ 是原子间距离的某个函数。这种形式是双体对称函数的基础。\n\n3.  **局域性与平滑性**：物理相互作用本质上是局域的；非常遥远的原子的影响可以忽略不计。为了对此进行建模，我们引入一个平滑的截断函数 $f_c(r_{ij}; R_c)$，它乘以每个邻居的贡献。该函数在小距离处必须等于 $1$，并随着距离 $r_{ij}$ 接近截断半径 $R_c$ 而平滑地趋于 $0$。对于距离 $r_{ij} > R_c$，贡献恰好为零。对平滑性的要求，特别是 $C^1$ 连续性（一阶导数连续），至关重要。作用在原子上的力被计算为势能的负梯度，$\\mathbf{F}_k = -\\nabla_{\\mathbf{r}_k} E$。能量一阶导数的不连续性会导致不合物理的、无穷大的力。提供的余弦截断函数为：\n    $$\n    f_c(r; R_c) = \n    \\begin{cases}\n    \\frac{1}{2}\\left[\\cos\\left(\\frac{\\pi r}{R_c}\\right) + 1\\right], & r \\le R_c,\\\\\n    0, & r > R_c.\n    \\end{cases}\n    $$\n    在截断半径 $r = R_c$ 处，函数值为 $f_c(R_c; R_c) = \\frac{1}{2}[\\cos(\\pi) + 1] = \\frac{1}{2}[-1 + 1] = 0$，确保了连续性。其导数为 $f'_c(r; R_c) = -\\frac{\\pi}{2R_c}\\sin(\\frac{\\pi r}{R_c})$。在 $r = R_c$ 处，导数为 $f'_c(R_c; R_c) = -\\frac{\\pi}{2R_c}\\sin(\\pi) = 0$，这与 $r > R_c$ 时零函数的导数相匹配。因此，该函数按要求是 $C^1$ 连续的。\n\n4.  **径向解析能力**：截断函数的简单求和只能提供截断球内邻居的加权计数。为了创建一个能区分不同径向结构的描述符，我们引入一个径向基函数。指定的的高斯形式 $\\exp[-\\eta(r_{ij} - R_s)^2]$ 正是为了这个目的。该函数以距离 $R_s$ 为中心，其特征宽度由参数 $\\eta$ 控制。较大的 $\\eta$ 对应于更窄、更尖锐的高斯峰。通过使用一组具有不同参数 $(\\eta, R_s)$ 的此类函数，人们可以解析中心原子 $i$ 周围邻居的径向分布。\n\n结合这四个原则——使用距离带来的不变性，求和带来的置换对称性，平滑截断带来的局域性，以及径向基带来的解析能力——我们得到了指定的双体径向对称函数，记为 $G_i^2$：\n$$\nG_i^{2}(\\eta, R_s, R_c) = \\sum_{j \\ne i} \\exp\\!\\left[-\\eta\\,(r_{ij} - R_s)^2\\right]\\, f_c(r_{ij}; R_c)\n$$\n求和遍及系统中的所有原子 $j$，但不包括中心原子 $i$。对于每个邻居 $j$，我们仅当其与原子 $i$ 的距离 $r_{ij}$ 小于或等于截断半径 $R_c$ 时才计算其贡献。\n\n计算流程如下：\n给定 $N$ 个原子的笛卡尔坐标集 $\\{\\mathbf{r}_k\\}_{k=0,..,N-1}$，中心原子的索引 $i$，以及参数 $\\eta, R_s, R_c$：\n1.  将对称函数值 $G_i^2$ 初始化为 $0$。\n2.  确定中心原子 $\\mathbf{r}_i$ 的坐标向量。\n3.  遍历所有其他原子 $j$，其中 $j \\in \\{0, 1, ..., N-1\\}$ 且 $j \\neq i$。\n4.  对每个邻居 $j$，计算欧几里得距离 $r_{ij} = ||\\mathbf{r}_j - \\mathbf{r}_i|| = \\sqrt{(x_j-x_i)^2 + (y_j-y_i)^2 + (z_j-z_i)^2}$。\n5.  检查是否 $r_{ij} \\le R_c$。如果不是，则来自原子 $j$ 的贡献为 $0$，我们继续处理下一个邻居。\n6.  如果 $r_{ij} \\le R_c$，则计算该项的两个分量：\n    -   径向基项：$T_{\\text{rad}} = \\exp[-\\eta(r_{ij} - R_s)^2]$。\n    -   截断函数项：$T_{\\text{cut}} = \\frac{1}{2}[\\cos(\\frac{\\pi r_{ij}}{R_c}) + 1]$。\n7.  将这些项的乘积 $T_{\\text{rad}} \\times T_{\\text{cut}}$ 加到 $G_i^2$ 的运行总和中。\n8.  遍历完所有邻居 $j$ 后，最终的总和就是原子 $i$ 的对称函数值。\n\n现在将实现此流程，并将其应用于五个指定的测试案例。所有单位必须一致；距离 ($r_{ij}$, $R_s$, $R_c$) 以埃 ($\\text{Å}$) 为单位，参数 $\\eta$ 的单位为 $\\text{Å}^{-2}$，从而确保指数的参数是无量纲的。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem by computing the Behler-Parrinello G2 symmetry function\n    for a series of test cases.\n    \"\"\"\n\n    def compute_g2(positions, i, R_c, eta, R_s):\n        \"\"\"\n        Computes the G2 symmetry function for a central atom i.\n        \n        Args:\n            positions (np.ndarray): Array of shape (N, 3) with Cartesian coordinates.\n            i (int): Index of the central atom.\n            R_c (float): Cutoff radius in Angstrom.\n            eta (float): Width parameter in Angstrom^-2.\n            R_s (float): Shift parameter in Angstrom.\n        \n        Returns:\n            float: The computed value of the G2 symmetry function.\n        \"\"\"\n        if positions.shape[0] <= 1:\n            return 0.0\n\n        central_atom_pos = positions[i]\n        g2_value = 0.0\n\n        for j in range(positions.shape[0]):\n            if i == j:\n                continue\n\n            neighbor_pos = positions[j]\n            # Calculate Euclidean distance\n            r_ij = np.linalg.norm(central_atom_pos - neighbor_pos)\n\n            # Apply the cutoff condition\n            if r_ij <= R_c:\n                # Cosine cutoff function\n                fc = 0.5 * (np.cos(np.pi * r_ij / R_c) + 1.0)\n                \n                # Gaussian-like radial basis function\n                radial_term = np.exp(-eta * (r_ij - R_s)**2)\n                \n                # Add contribution to the sum\n                g2_value += radial_term * fc\n        \n        return g2_value\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case A\n        {'positions': np.array([[0.0, 0.0, 0.0], [2.0, 0.0, 0.0], [0.0, 3.0, 0.0], [0.0, 0.0, 4.0]]),\n         'i': 0, 'R_c': 5.0, 'eta': 0.5, 'R_s': 0.0},\n        # Case B\n        {'positions': np.array([[0.0, 0.0, 0.0]]),\n         'i': 0, 'R_c': 3.0, 'eta': 1.0, 'R_s': 0.0},\n        # Case C\n        {'positions': np.array([[0.0, 0.0, 0.0], [5.0, 0.0, 0.0], [-5.0, 0.0, 0.0]]),\n         'i': 0, 'R_c': 5.0, 'eta': 1.0, 'R_s': 0.0},\n        # Case D\n        {'positions': np.array([[0.0, 0.0, 0.0], [2.0, 0.0, 0.0], [0.0, 3.0, 0.0], [0.0, 0.0, 4.0]]),\n         'i': 0, 'R_c': 5.0, 'eta': 2.0, 'R_s': 2.5},\n        # Case E\n        {'positions': np.array([[0.0, 0.0, 0.0], [2.0, 0.0, 0.0], [0.0, 3.0, 0.0], [0.0, 0.0, 4.0]]),\n         'i': 1, 'R_c': 5.0, 'eta': 0.5, 'R_s': 0.0},\n    ]\n\n    results = []\n    for case in test_cases:\n        result = compute_g2(\n            positions=case['positions'],\n            i=case['i'],\n            R_c=case['R_c'],\n            eta=case['eta'],\n            R_s=case['R_s']\n        )\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    # The format string \"{:.6f}\" handles rounding to 6 decimal places.\n    print(f\"[{','.join([f'{r:.6f}' for r in results])}]\")\n\nsolve()\n```", "id": "2457438"}, {"introduction": "在学习了如何为原子构造不变性特征之后，下一个关键是确保模型的整体架构能够尊重相同种类原子的不可区分性，即排列不变性。本题是一个精妙的思想实验，它利用高度对称的苯分子 ($D_{6h}$)，来揭示一个缺少排列不变性的朴素模型所存在的根本缺陷。通过分析这种模型的“失败”，你将能深刻体会为何现代机器学习势函数架构（例如，基于原子能量加和或图神经网络）是不可或缺的设计。[@problem_id:2457453]", "problem": "考虑一个平面苯分子 $C_6H_6$，其处于理想的 $D_{6h}$ 几何构型，所有碳-碳键和碳-氢键的键长均相等，且所有原子都位于 $z=0$ 平面内。设原子核位置为 $\\{\\mathbf{R}_i\\}_{i=1}^{12}$，原子序数为 $\\{Z_i\\}_{i=1}^{12}$，其中 $Z_i \\in \\{6,1\\}$ 分别代表碳原子或氢原子。在玻恩-奥本海默（BO）近似下，该系统的精确势能 $E(\\{\\mathbf{R}_i,Z_i\\})$ 是一个标量，它在具有相同原子序数的原子核之间的任何索引置换下都保持不变。\n\n假设我们训练一个用于计算能量的机器学习势 (MLP)，记为 $E_\\theta(\\mathbf{X})$，其中 $\\mathbf{X} \\in \\mathbb{R}^{36}$ 是通过按固定的输入顺序 $(x_1,y_1,z_1,\\dots,x_{12},y_{12},z_{12})$ 拼接 12 个原子的笛卡尔坐标而形成的。模型 $E_\\theta$ 是一个直接作用于 $\\mathbf{X}$ 的通用前馈神经网络，其通过在一个包含 $N$ 个苯分子几何构型的数据集上最小化与参考能量 $E^\\ast$ 的均方误差来进行训练，该数据集中的所有构型都使用相同的常规原子索引。\n\n训练后，该模型在两个代表相同苯分子物理几何构型的输入上进行评估：$\\mathbf{X}^{(1)}$ 使用常规索引，而 $\\mathbf{X}^{(2)}$ 是通过对 $\\mathbf{X}^{(1)}$ 的六个碳原子索引和相应的六个氢原子索引应用一个循环置换 $\\pi$ 得到的，也就是说，$\\pi$ 将 $(\\mathrm{C}_1,\\mathrm{C}_2,\\dots,\\mathrm{C}_6)$ 映射到 $(\\mathrm{C}_2,\\mathrm{C}_3,\\dots,\\mathrm{C}_1)$，同样将 $(\\mathrm{H}_1,\\mathrm{H}_2,\\dots,\\mathrm{H}_6)$ 映射到 $(\\mathrm{H}_2,\\mathrm{H}_3,\\dots,\\mathrm{H}_1)$，而不改变笛卡尔坐标本身。用 $P_\\pi$ 表示相应地置换 $\\mathbf{X}$ 分量的线性算子，从而使得 $\\mathbf{X}^{(2)}=P_\\pi \\mathbf{X}^{(1)}$。\n\n关于此场景，以下哪些陈述是正确的？\n\nA. 即使 $\\mathbf{X}^{(1)}$ 和 $\\mathbf{X}^{(2)}$ 对应于相同的物理几何构型，一个作用于固定顺序坐标拼接且仅在一种索引方式上训练的模型 $E_\\theta$ 通常会得出 $E_\\theta(\\mathbf{X}^{(1)}) \\ne E_\\theta(\\mathbf{X}^{(2)})$，从而违反了相同原子间的置换不变性。\n\nB. 定义一个对称化预测器 $E^{\\mathrm{sym}}_\\theta(\\mathbf{X}) = \\frac{1}{|\\mathcal{G}|}\\sum_{\\pi \\in \\mathcal{G}} E_\\theta(P_\\pi \\mathbf{X})$，其中 $\\mathcal{G}$ 是对六个碳原子和六个氢原子（在同类原子内）进行置换的置换群，这会使得 $E^{\\mathrm{sym}}_\\theta$ 对于任何输入 $\\mathbf{X}$ 都具有严格的置换不变性。\n\nC. 将苯分子的坐标在平面内旋转 $60^\\circ$ 同时保持原有的原子索引，会产生一个与纯粹的索引置换等效的构型，因此任何旋转不变的模型也自动地对相同原子具有置换不变性。\n\nD. 使用通过可交换操作（如求和）来聚合原子或邻居信息的架构或描述符（例如，以原子为中心的对称函数结合原子贡献的求和，或带有和池化的消息传递图神经网络），其产生的预测根据其构造本身就对相同原子的置换具有不变性。\n\nE. 尽管 $E_\\theta(\\mathbf{X}^{(1)}) \\ne E_\\theta(\\mathbf{X}^{(2)})$ 是可能的，但相应的力 $\\mathbf{F}_i(\\mathbf{X}) = -\\nabla_{\\mathbf{R}_i} E_\\theta(\\mathbf{X})$ 在 $D_{6h}$ 几何构型下仍然必须遵守六个等效碳原子间的置换对称性，因此由 $E_\\theta$ 驱动的分子动力学在这种对称情况下仍然是物理上正确的。", "solution": "该问题陈述在科学上是有效的且阐述清晰。它探讨了物理和化学中势能面所需的基本置换不变性，并正确地构建了一个场景，用以测试一个因其构造而缺乏此对称性的机器学习势的性质。我们现在将逐一评估每个陈述。\n\n问题的核心在于，精确势能 $E(\\{\\mathbf{R}_i,Z_i\\})$ 是粒子坐标*集合*的函数，其中相同粒子的顺序无关紧要。然而，所提出的机器学习势 $E_\\theta(\\mathbf{X})$ 是一个*有序向量* $\\mathbf{X}$ 的函数，其中向量中每个元素的位置都是独特的。一个通用的前馈神经网络，就其本质而言，会对不同的输入神经元施加不同的权重，这意味着它的设计并非旨在对其输入的置换保持不变。\n\nA. **即使 $\\mathbf{X}^{(1)}$ 和 $\\mathbf{X}^{(2)}$ 对应于相同的物理几何构型，一个作用于固定顺序坐标拼接且仅在一种索引方式上训练的模型 $E_\\theta$ 通常会得出 $E_\\theta(\\mathbf{X}^{(1)}) \\ne E_\\theta(\\mathbf{X}^{(2)})$，从而违反了相同原子间的置换不变性。**\n\n该陈述指出了所述朴素MLP架构的核心缺陷。模型 $E_\\theta$ 是一个 $36$ 维向量 $\\mathbf{X}$ 的通用函数。假设一个简单的模型为 $E_\\theta(\\mathbf{X}) = \\sigma(\\mathbf{W}\\mathbf{X} + \\mathbf{b})$，其中 $\\mathbf{W}$ 是一个权重矩阵。输入 $\\mathbf{X}^{(1)}$ 和 $\\mathbf{X}^{(2)}$ 通过一个置换相关联，$\\mathbf{X}^{(2)} = P_\\pi \\mathbf{X}^{(1)}$，其中 $P_\\pi$ 是一个重排向量元素的矩阵。对于一个通用的非线性函数 $E_\\theta$，没有数学上的理由可以期望 $E_\\theta(\\mathbf{X}^{(1)})$ 等于 $E_\\theta(P_\\pi \\mathbf{X}^{(1)})$。神经网络学习的是固定的输入槽位与目标能量之间的特定关联。置换输入实际上是向网络提供了一个全新的数据点，网络对此数据点的预测并没有被约束为必须与原先相同。仅使用一种常规索引的训练方案，并没有为网络提供任何能让它学习到这种对称性的信息。因此，通常情况下，$E_\\theta(\\mathbf{X}^{(1)}) \\ne E_\\theta(\\mathbf{X}^{(2)})$。这直接违反了置换不变性的物理原理。\n\n**结论：正确。**\n\nB. **定义一个对称化预测器 $E^{\\mathrm{sym}}_\\theta(\\mathbf{X}) = \\frac{1}{|\\mathcal{G}|}\\sum_{\\pi \\in \\mathcal{G}} E_\\theta(P_\\pi \\mathbf{X})$，其中 $\\mathcal{G}$ 是对六个碳原子和六个氢原子（在同类原子内）进行置换的置换群，这会使得 $E^{\\mathrm{sym}}_\\theta$ 对于任何输入 $\\mathbf{X}$ 都具有严格的置换不变性。**\n\n该陈述提出了一种通过对所有相关置换群 $\\mathcal{G}$ 进行平均来强制施加所需对称性的方法。让我们测试 $E^{\\mathrm{sym}}_\\theta$ 在任意置换 $\\sigma \\in \\mathcal{G}$ 作用下的不变性。我们将算子 $P_\\sigma$ 应用于输入 $\\mathbf{X}$ 并计算该函数：\n$$E^{\\mathrm{sym}}_\\theta(P_\\sigma \\mathbf{X}) = \\frac{1}{|\\mathcal{G}|}\\sum_{\\pi \\in \\mathcal{G}} E_\\theta(P_\\pi (P_\\sigma \\mathbf{X}))$$\n两个置换算子 $P_\\pi$ 和 $P_\\sigma$ 的复合对应于群乘积的置换算子 $P_{\\pi\\sigma}$。因此，我们有：\n$$E^{\\mathrm{sym}}_\\theta(P_\\sigma \\mathbf{X}) = \\frac{1}{|\\mathcal{G}|}\\sum_{\\pi \\in \\mathcal{G}} E_\\theta(P_{\\pi\\sigma} \\mathbf{X})$$\n根据群论中的重排定理，如果 $\\sigma$ 是 $\\mathcal{G}$ 的一个固定元素，并且 $\\pi$ 遍历 $\\mathcal{G}$ 的所有元素，那么乘积 $\\pi' = \\pi\\sigma$ 也将恰好遍历一次 $\\mathcal{G}$ 的所有元素。因此，我们可以将求和变量从 $\\pi$ 更改为 $\\pi'$：\n$$E^{\\mathrm{sym}}_\\theta(P_\\sigma \\mathbf{X}) = \\frac{1}{|\\mathcal{G}|}\\sum_{\\pi' \\in \\mathcal{G}} E_\\theta(P_{\\pi'} \\mathbf{X}) = E^{\\mathrm{sym}}_\\theta(\\mathbf{X})$$\n这表明 $E^{\\mathrm{sym}}_\\theta(\\mathbf{X})$ 在任何置换 $\\sigma \\in \\mathcal{G}$ 的作用下确实是不变的。该技术是将函数投影到群表示的完全对称子空间上的标准方法。\n\n**结论：正确。**\n\nC. **将苯分子的坐标在平面内旋转 $60^\\circ$ 同时保持原有的原子索引，会产生一个与纯粹的索引置换等效的构型，因此任何旋转不变的模型也自动地对相同原子具有置换不变性。**\n\n该陈述包含一个逻辑谬误。对于所提到的特定情况，其第一部分是正确的：将一个理想的 $D_{6h}$ 苯分子旋转 $60^\\circ$（一个 $C_6$ 旋转）会将分子映射到其自身，使得原子 $i$ 的新坐标与原子 $j$ 的旧坐标相同，其中 $j$ 是原先位于 $i$ 的新位置的原子。这个映射等价于问题中描述的置换 $\\pi$。然而，从这一特定观察中得出的结论是错误的。旋转不变性和置换不变性是根本上不同的对称性。旋转不变性意味着对于任何旋转算子 $R$, $E(\\{\\mathbf{R}_i\\}) = E(\\{R\\mathbf{R}_i\\})$。置换不变性意味着对于相同原子的任何置换 $\\pi$, $E(\\{\\mathbf{R}_i\\}) = E(\\{\\mathbf{R}_{\\pi(i)}\\})$。一个普适的置换（例如，交换两个不相邻的碳原子）不能由整个分子的简单旋转来表示。反之，对一个非对称原子构型的普适旋转也不对应于任何置换。一个模型可以对一种对称性保持不变，而无需对另一种对称性保持不变。例如，一个仅依赖于原子间距离的势，$E = \\sum_{i<j} f(Z_i, Z_j, ||\\mathbf{R}_i - \\mathbf{R}_j||)$，是旋转和平移不变的，但只有当函数 $f$ 对其原子类型参数是对称时，它才是置换不变的，而这一点无法保证。更尖锐地说，该论证试图从一个单一的、高度对称的特例来证明一个普遍性质，这是无效的推理。\n\n**结论：不正确。**\n\nD. **使用通过可交换操作（如求和）来聚合原子或邻居信息的架构或描述符（例如，以原子为中心的对称函数结合原子贡献的求和，或带有和池化的消息传递图神经网络），其产生的预测根据其构造本身就对相同原子的置换具有不变性。**\n\n该陈述准确地描述了现代成功的机器学习势的基本设计原则。这些模型通过将总能量表示为原子贡献的总和来避免固定顺序问题：\n$$E = \\sum_{i=1}^{N_{\\text{atoms}}} E_i$$\n在这里，原子能量 $E_i$ 是由一个函数（例如，神经网络）计算的，该函数依赖于原子 $i$ 的原子序数 $Z_i$ 及其局部环境的描述。至关重要的是，用于计算 $E_i$ 的函数对于所有相同类型的原子都是相同的。例如，所有碳原子使用一个网络 $f_C$，所有氢原子使用另一个网络 $f_H$。局部环境的描述本身被构造成对邻近原子的置换是不变的，通常是通过另一个求和或可交换操作来实现。因为外层的求和 $\\sum_i$ 是一个可交换操作，交换任意两个相同原子的索引（例如，碳原子 $j$ 和碳原子 $k$）仅仅改变了求和式中两个相同项（$E_j$ 和 $E_k$ 都使用 $f_C$ 计算）的顺序，而总能量 $E$ 的值保持不变。像 Behler-Parrinello 势和许多图神经网络等架构都建立在这一原则之上。\n\n**结论：正确。**\n\nE. **尽管 $E_\\theta(\\mathbf{X}^{(1)}) \\ne E_\\theta(\\mathbf{X}^{(2)})$ 是可能的，但相应的力 $\\mathbf{F}_i(\\mathbf{X}) = -\\nabla_{\\mathbf{R}_i} E_\\theta(\\mathbf{X})$ 在 $D_{6h}$ 几何构型下仍然必须遵守六个等效碳原子间的置换对称性，因此由 $E_\\theta$ 驱动的分子动力学在这种对称情况下仍然是物理上正确的。**\n\n该陈述是严重错误的。力是势能的负梯度：$\\mathbf{F}(\\mathbf{X}) = -\\nabla E_\\theta(\\mathbf{X})$。如果能量 $E_\\theta$ 在某个对称操作（如置换）下不是不变的，那么它的梯度（力）也不会是等变的。力的等变性意味着，如果原子坐标被置换，$\\mathbf{X}' = P_\\pi \\mathbf{X}$，那么新的力向量就是旧力向量的置换，$\\mathbf{F}(\\mathbf{X}') = P_\\pi \\mathbf{F}(\\mathbf{X})$。一个非不变的势不满足这个条件。对于 $D_{6h}$ 几何构型，所有原子上的真实物理力都为零。一个训练良好的模型可能会为它在训练中见过的输入 $\\mathbf{X}^{(1)}$ 预测出接近零的力。然而，对于代表相同物理状态的置换输入 $\\mathbf{X}^{(2)}$，非不变模型 $E_\\theta$ 将产生一个不同的能量 $E_\\theta(\\mathbf{X}^{(2)}) \\ne E_\\theta(\\mathbf{X}^{(1)})$。这意味着 $\\mathbf{X}^{(2)}$ 不再是势 $E_\\theta$ 的一个驻点。因此，力 $\\mathbf{F}(\\mathbf{X}^{(2)}) = -\\nabla E_\\theta(\\mathbf{X}^{(2)})$ 将不为零。这是一个非物理的结果：模型预测平衡构型会受到力，这些力会将其撕裂，而原因仅仅是原子被重新标记了。由这种势驱动的分子动力学将是非物理且不稳定的。\n\n**结论：不正确。**", "answer": "$$\\boxed{ABD}$$", "id": "2457453"}, {"introduction": "将静态的概念应用于动态的模拟是掌握机器学习势函数的关键一步。机器学习势函数最强大的用途之一是加速分子动力学（MD）模拟，但为其构建一个完备的训练集通常成本高昂。本次练习将介绍一种被称为“在飞学习”（on-the-fly active learning）的尖端解决方案，即在模拟过程中通过不确定性度量 $U(r)$ 智能地判断何时需要调用更精确的量子力学方法来“主动”改进模型。通过这个练习，你将实现一个简化但完整的“在飞学习”循环，体验其在自动化构建势函数方面的强大之处。[@problem_id:2457458]", "problem": "实现一个完整、可运行的程序，该程序为一个双原子分子模拟一个一维、约化单位的分子动力学系统，同时为机器学习势 (MLP) 执行一个“即时”主动学习循环。MLP 经训练以逼近一个参考的量子力学 (QM) 势及其力。当不确定性度量超过阈值时，模拟必须暂停，触发一次 QM 查询以获取精确的能量和力，更新训练集，重新训练 MLP，然后恢复动力学模拟。\n\n该系统是一个单一内坐标键长 $r(t)$，在约化单位下其约化质量为 $\\mu = 1$。参考的量子力学势能是 Morse 势\n$$\nV_{\\mathrm{QM}}(r) = D_e \\left(1 - e^{-a (r - r_e)}\\right)^2,\n$$\n其中常数在约化单位下为 $D_e = 0.20$，$a = 1.5$ 和 $r_e = 1.0$。相应的力是\n$$\nF_{\\mathrm{QM}}(r) = - \\frac{d V_{\\mathrm{QM}}}{dr}.\n$$\n\nMLP 必须是一个基于 4 次多项式基的线性模型。定义基\n$$\n\\phi(r) = \\begin{bmatrix} 1 \\\\ r \\\\ r^2 \\\\ r^3 \\\\ r^4 \\end{bmatrix}, \\quad \\phi'(r) = \\frac{d\\phi}{dr} = \\begin{bmatrix} 0 \\\\ 1 \\\\ 2 r \\\\ 3 r^2 \\\\ 4 r^3 \\end{bmatrix}.\n$$\n设模型参数向量为 $w \\in \\mathbb{R}^5$。预测的能量和力是\n$$\n\\hat{E}(r) = w^\\top \\phi(r), \\quad \\hat{F}(r) = - w^\\top \\phi'(r).\n$$\n训练数据由三元组 $(r_i, E_i, F_i)$ 组成，其中 $E_i = V_{\\mathrm{QM}}(r_i)$ 且 $F_i = F_{\\mathrm{QM}}(r_i)$。通过线性最小二乘法拟合 $w$，以最小化能量和力的残差平方和（权重相等），并使用强度为 $\\lambda = 10^{-8}$ 的 Tikhonov (岭) 正则化。也就是说，构建一个线性系统，其中行 $\\phi(r_i)$ 对应目标 $E_i$，行 $-\\phi'(r_i)$ 对应目标 $F_i$，求解\n$$\n\\min_w \\left\\|A w - y\\right\\|_2^2 + \\lambda \\left\\|w\\right\\|_2^2.\n$$\n\n为进行不确定性估计，定义一个由两个模型组成的委员会。设 $w^{(A)}$ 在当前训练集的偶数索引成员上训练（使用从零开始的索引），而 $w^{(B)}$ 在奇数索引成员上训练。在坐标 $r$ 处的不确定性度量是\n$$\nU(r) = \\left| \\hat{F}^{(A)}(r) - \\hat{F}^{(B)}(r) \\right|.\n$$\n此外，定义一个在当前完整训练集上训练的“生产”模型 $w^{(P)}$；此模型用于在分子动力学积分过程中生成力。\n\n用位于 $r = r_e$ 和 $r = r_e + 0.20$ 的两个 QM 点初始化训练集。然后使用 velocity-Verlet 方案对质量为 $\\mu = 1$ 的位置 $r$ 和速度 $v$ 进行 $N$ 步、时间步长为 $\\Delta t$ 的分子动力学模拟：\n$$\nv_{n+\\frac{1}{2}} = v_n + \\frac{\\Delta t}{2} \\, a(r_n), \\quad r_{n+1} = r_n + \\Delta t \\, v_{n+\\frac{1}{2}}, \\quad v_{n+1} = v_{n+\\frac{1}{2}} + \\frac{\\Delta t}{2} \\, a(r_{n+1}),\n$$\n其中 $a(r) = \\hat{F}^{(P)}(r) / \\mu$ 是根据当前生产模型计算出的加速度。在每一步中，计算用于积分的力之前，先评估 $U(r_n)$。如果 $U(r_n) > \\tau$ 并且到目前为止的 QM 查询次数严格小于预算 $B$，则在 $r_n$ 处执行一次 QM 查询，即将 $(r_n, V_{\\mathrm{QM}}(r_n), F_{\\mathrm{QM}}(r_n))$ 添加到训练集中，重新训练委员会模型和生产模型，并增加 QM 查询计数器。然后使用更新后的生产模型继续进行积分。\n\n经过 $N$ 步后，使用最终的生产模型，沿访问过的轨迹位置 $\\{r_1, r_2, \\dots, r_N\\}$ 评估平均绝对力误差：\n$$\n\\mathrm{MAE} = \\frac{1}{N} \\sum_{k=1}^{N} \\left| \\hat{F}^{(P)}(r_k) - F_{\\mathrm{QM}}(r_k) \\right|.\n$$\n\n所有量均使用约化单位。以约化力单位（能量单位/长度单位）表示平均绝对误差。QM 查询计数是无量纲的。\n\n您的程序必须精确地实现上述规范，并运行以下测试套件。每个测试用例是一个元组 $(N, \\Delta t, \\tau, B, r_0, v_0)$:\n\n- 测试 1: $N = 200$, $\\Delta t = 0.01$, $\\tau = 0.05$, $B = 10$, $r_0 = 1.30$, $v_0 = 0.00$。\n- 测试 2: $N = 200$, $\\Delta t = 0.01$, $\\tau = 1.00$, $B = 10$, $r_0 = 1.30$, $v_0 = 0.00$。\n- 测试 3: $N = 200$, $\\Delta t = 0.01$, $\\tau = 0.005$, $B = 3$, $r_0 = 1.30$, $v_0 = 0.00$。\n- 测试 4: $N = 100$, $\\Delta t = 0.01$, $\\tau = 0.02$, $B = 10$, $r_0 = 2.00$, $v_0 = 0.00$。\n\n对每个测试用例，计算两个值：触发的 QM 查询总数 $Q$（不包括两个初始种子点）和上文定义的平均绝对力误差 $\\mathrm{MAE}$。您的程序应产生单行输出，其中包含这四个测试结果的扁平列表，顺序如下：\n$$\n\\left[Q_1, \\mathrm{MAE}_1, Q_2, \\mathrm{MAE}_2, Q_3, \\mathrm{MAE}_3, Q_4, \\mathrm{MAE}_4\\right].\n$$\n以所示格式精确打印一行，值由逗号分隔并用方括号括起来。不涉及角度或百分比；除了此处说明的约化单位外，不需要进行单位转换。", "solution": "该问题陈述具有科学依据、提法良定且客观。它为分子动力学机器学习势领域的计算任务提供了完整的规范。所有必需的参数、初始条件、算法和评估指标都得到了明确的定义。因此，该问题被认为是有效的。\n\n解决方案要求实现一个一维分子动力学（MD）模拟，其中集成了用于机器学习势（MLP）的“即时”主动学习程序。该系统由一个约化质量为 $\\mu = 1$ 的单个粒子组成，该粒子沿坐标 $r$ 运动。训练 MLP 以再现参考的量子力学（QM）势能面。\n\n实现的核心组件如下：\n\n1.  **参考 QM 模型**：真实的势能由 Morse 势给出，$V_{\\mathrm{QM}}(r) = D_e \\left(1 - e^{-a (r - r_e)}\\right)^2$。提供的参数为（在约化单位下）$D_e = 0.20$，$a = 1.5$ 和 $r_e = 1.0$。相应的解析力，即势的负梯度，是 $F_{\\mathrm{QM}}(r) = - \\frac{d V_{\\mathrm{QM}}}{dr} = -2 a D_e (1 - e^{-a (r - r_e)}) e^{-a(r-r_e)}$。这些函数作为训练 MLP 和评估其准确性的基准真相。\n\n2.  **机器学习势 (MLP)**：MLP 是一个由 4 次多项式基构建的线性模型。在给定位置 $r$ 的基向量是 $\\phi(r) = \\begin{bmatrix} 1 & r & r^2 & r^3 & r^4 \\end{bmatrix}^\\top$。MLP 将能量预测为这些基函数的线性组合：$\\hat{E}(r) = w^\\top \\phi(r)$，其中 $w \\in \\mathbb{R}^5$ 是模型权重的向量。此类势的一个关键特征是解析可微性，允许直接计算力，这被称为力匹配（force-matching）。预测的力是 $\\hat{F}(r) = - \\frac{d\\hat{E}}{dr} = - w^\\top \\phi'(r)$，其中 $\\phi'(r) = \\frac{d\\phi}{dr} = \\begin{bmatrix} 0 & 1 & 2r & 3r^2 & 4r^3 \\end{bmatrix}^\\top$。\n\n3.  **MLP 训练**：权重向量 $w$ 通过最小化一个包含一组训练点 $\\{(r_i, E_i, F_i)\\}$ 的能量和力误差的损失函数来优化，其中 $E_i = V_{\\mathrm{QM}}(r_i)$ 且 $F_i = F_{\\mathrm{QM}}(r_i)$。该优化问题是一个带有 Tikhonov（岭）正则化的线性最小二乘拟合，其公式为：\n    $$\n    \\min_w \\left\\|A w - y\\right\\|_2^2 + \\lambda \\left\\|w\\right\\|_2^2\n    $$\n    此处，矩阵 $A$ 和向量 $y$ 是通过堆叠每个训练点的贡献来构建的。对于一个包含 $M$ 个训练点的集合，$A$ 是一个 $2M \\times 5$ 的矩阵，其中前 $M$ 行是 $\\phi(r_i)^\\top$，后 $M$ 行是 $-\\phi'(r_i)^\\top$。向量 $y$ 是一个 $2M$ 维的向量，包含目标能量 $E_i$ 和其后的目标力 $F_i$。正则化强度为 $\\lambda = 10^{-8}$。这是一个标准的岭回归问题，其解析解为 $w = (A^\\top A + \\lambda I)^{-1} A^\\top y$，其中 $I$ 是 $5 \\times 5$ 的单位矩阵。该线性系统通过数值方法求解。\n\n4.  **主动学习和不确定性量化**：该模拟采用主动学习策略，仅在模型不确定的区域添加新的训练点，从而高效地提高 MLP 的准确性。不确定性是使用一个由两个模型组成的委员会来估计的，其权重分别为 $w^{(A)}$ 和 $w^{(B)}$。模型 $w^{(A)}$ 在偶数索引的训练数据子集上训练（使用从零开始的索引），而 $w^{(B)}$ 在奇数索引的子集上训练。在位置 $r$ 处的不确定性度量被定义为两个委员会模型预测的力之间差异的大小：$U(r) = \\left| \\hat{F}^{(A)}(r) - \\hat{F}^{(B)}(r) \\right|$。第三个“生产”模型，其权重为 $w^{(P)}$，在整个训练集上训练，并用于传播动力学。\n\n5.  **分子动力学模拟**：系统的轨迹使用 velocity-Verlet 算法进行积分。给定在时间步 $n$ 的位置 $r_n$ 和速度 $v_n$，在步骤 $n+1$ 的状态计算如下：\n    $$\n    \\begin{aligned}\n    v_{n+\\frac{1}{2}} &= v_n + \\frac{\\Delta t}{2} \\frac{\\hat{F}^{(P)}(r_n)}{\\mu} \\\\\n    r_{n+1} &= r_n + \\Delta t \\, v_{n+\\frac{1}{2}} \\\\\n    v_{n+1} &= v_{n+\\frac{1}{2}} + \\frac{\\Delta t}{2} \\frac{\\hat{F}^{(P)}(r_{n+1})}{\\mu}\n    \\end{aligned}\n    $$\n    其中 $\\Delta t$ 是时间步长，加速度由生产模型的力 $\\hat{F}^{(P)}$ 导出。\n\n完整的模拟过程如下：\n- 训练集用位于 $r=r_e=1.0$ 和 $r=r_e+0.20=1.2$ 的两个 QM 数据点进行初始化。三个模型（$w^{(A)}$、$w^{(B)}$、$w^{(P)}$）在此初始集上进行训练。\n- 从 $(r_0, v_0)$ 开始的 MD 模拟运行 $N$ 步。\n- 在每一步 $n$ 的开始，当系统位于位置 $r_n$ 时，计算不确定性 $U(r_n)$。\n- 如果 $U(r_n)$ 大于阈值 $\\tau$ 且已触发的 QM 查询次数小于预算 $B$，则在 $r_n$ 处生成一个新的 QM 数据点并添加到训练集中。然后使用更新后的集合重新训练所有三个模型。触发的查询计数 $Q$ 会增加。\n- 然后，velocity-Verlet 算法使用来自（可能已更新的）生产模型 $\\hat{F}^{(P)}$ 的力将系统传播到下一个状态。新位置 $r_{n+1}$ 被存储。\n- 经过 $N$ 步后，模拟结束。在生成的轨迹 $\\{r_1, r_2, \\dots, r_N\\}$ 上计算力的平均绝对误差（MAE）。这是通过对每个存储的位置 $r_k$，将*最终*生产模型 $\\hat{F}^{(P)}(r_k)$ 的力与真实 QM 力 $F_{\\mathrm{QM}}(r_k)$ 进行比较来完成的。\n- 每个测试用例的两个输出是触发的 QM 查询总数 $Q$ 和最终的 MAE。\n\n提供的 Python 程序实现了这整个过程，遍历指定的测试用例并产生所需的输出。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import linalg\n\ndef solve():\n    \"\"\"\n    Main function to orchestrate the active learning MD simulation for all test cases.\n    \"\"\"\n    \n    # --- Global Constants in Reduced Units ---\n    D_e = 0.20\n    a_morse = 1.5  # Renamed to avoid conflict with acceleration 'a'\n    r_e = 1.0\n    mu = 1.0\n    lambda_reg = 1e-8\n    \n    # --- Quantum Mechanical (QM) Ground Truth ---\n    def V_QM(r):\n        \"\"\"Calculates the Morse potential energy.\"\"\"\n        return D_e * (1.0 - np.exp(-a_morse * (r - r_e)))**2\n\n    def F_QM(r):\n        \"\"\"Calculates the analytical force from the Morse potential.\"\"\"\n        exp_term = np.exp(-a_morse * (r - r_e))\n        return -2.0 * a_morse * D_e * (1.0 - exp_term) * exp_term\n\n    # --- Machine Learning Potential (MLP) Module ---\n    def get_phi(r):\n        \"\"\"Returns the polynomial basis vector.\"\"\"\n        return np.array([1.0, r, r**2, r**3, r**4])\n\n    def get_phi_prime(r):\n        \"\"\"Returns the derivative of the polynomial basis vector.\"\"\"\n        return np.array([0.0, 1.0, 2.0*r, 3.0*r**2, 4.0*r**3])\n\n    def train(r_train, lam_reg):\n        \"\"\"\n        Trains the linear MLP model using ridge regression on both energy and forces.\n        \"\"\"\n        num_points = len(r_train)\n        if num_points == 0:\n            return np.zeros(5)\n\n        A = np.zeros((2 * num_points, 5))\n        y = np.zeros(2 * num_points)\n\n        for i, r_i in enumerate(r_train):\n            E_i = V_QM(r_i)\n            F_i = F_QM(r_i)\n            \n            A[i, :] = get_phi(r_i)\n            A[num_points + i, :] = -get_phi_prime(r_i)\n            \n            y[i] = E_i\n            y[num_points + i] = F_i\n\n        AtA = A.T @ A\n        AtA_reg = AtA + lam_reg * np.identity(5)\n        Aty = A.T @ y\n        \n        # Solve the linear system (A^T A + lambda I) w = A^T y\n        w = linalg.solve(AtA_reg, Aty, assume_a='sym')\n        return w\n\n    def predict_F(r, w):\n        \"\"\"Predicts the force using the trained MLP model.\"\"\"\n        return -w.T @ get_phi_prime(r)\n\n    def run_simulation(N, dt, tau, B, r0, v0):\n        \"\"\"\n        Runs a single active learning MD simulation for a given set of parameters.\n        \"\"\"\n        # 1. Initialize training set and query counter\n        training_set_r = [r_e, r_e + 0.20]\n        qm_queries_triggered = 0\n\n        # 2. Initial model training\n        w_P = train(training_set_r, lambda_reg)\n        w_A = train(training_set_r[0::2], lambda_reg)\n        w_B = train(training_set_r[1::2], lambda_reg)\n\n        r_current = r0\n        v_current = v0\n        r_trajectory = []\n\n        # 3. Main MD loop\n        for _ in range(N):\n            r_n = r_current\n\n            # 3a. Active Learning: Check uncertainty and retrain if needed\n            F_A = predict_F(r_n, w_A)\n            F_B = predict_F(r_n, w_B)\n            uncertainty = abs(F_A - F_B)\n\n            if uncertainty > tau and qm_queries_triggered < B:\n                training_set_r.append(r_n)\n                qm_queries_triggered += 1\n                \n                # Retrain all three models with the new data point\n                w_P = train(training_set_r, lambda_reg)\n                w_A = train(training_set_r[0::2], lambda_reg)\n                w_B = train(training_set_r[1::2], lambda_reg)\n\n            # 3b. Velocity-Verlet Integration\n            # First half-step for velocity\n            force_n = predict_F(r_n, w_P)\n            a_n = force_n / mu\n            v_half = v_current + 0.5 * dt * a_n\n\n            # Full-step for position\n            r_next = r_current + dt * v_half\n            r_trajectory.append(r_next)\n            \n            # Second half-step for velocity\n            force_next = predict_F(r_next, w_P)\n            a_next = force_next / mu\n            v_next = v_half + 0.5 * dt * a_next\n            \n            # Update state for the next iteration\n            r_current = r_next\n            v_current = v_next\n        \n        # 4. Final Evaluation: Calculate Mean Absolute Error (MAE)\n        total_abs_error = 0.0\n        for r_k in r_trajectory:\n            f_pred = predict_F(r_k, w_P) # Use final production model\n            f_true = F_QM(r_k)\n            total_abs_error += abs(f_pred - f_true)\n            \n        mae = total_abs_error / N\n        \n        return qm_queries_triggered, mae\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (200, 0.01, 0.05, 10, 1.30, 0.00),\n        (200, 0.01, 1.00, 10, 1.30, 0.00),\n        (200, 0.01, 0.005, 3, 1.30, 0.00),\n        (100, 0.01, 0.02, 10, 2.00, 0.00),\n    ]\n\n    results = []\n    for case in test_cases:\n        N, dt, tau, B, r0, v0 = case\n        q, mae = run_simulation(N, dt, tau, B, r0, v0)\n        results.extend([q, mae])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2457458"}]}