{"hands_on_practices": [{"introduction": "任何神经网络势的第一步都是以一种尊重物理对称性（如平移和旋转不变性）的方式来表示局部原子环境。这个练习将指导你从第一性原理出发，实现一个基础的描述符——Behler-Parrinello 型二体对称函数 $G^2$。通过这个实践，你将具体理解数学形式如何确保物理不变性，这是构建任何可靠原子势的基石。[@problem_id:2457438]", "problem": "实现一个程序，用于推导和计算一种针对氩（Ar）原子的基本的双体 Behler–Parrinello 类型对称函数，并在一个小的几何结构测试集上对其进行评估。其目的是将机器学习原子间势的不变性要求与一个具体的描述符联系起来，并展示在不同参数选择下的数值行为。总体背景是，一个系统的总势能面可以近似为各项原子贡献之和，其中每一项都取决于其邻域的局部、对称不变的表示，正如高维神经网络势（HDNNP）中那样。你的任务是从不变性原理出发，推导一种双体径向对称函数并加以实现。\n\n从以下基本原则出发：\n- 标量势能的平移和旋转不变性意味着原子的局域描述符必须由内部坐标（例如原子间距离）构建。\n- 对于有限范围的相互作用和学习映射中的局域性，在一个有限半径处施加一个平滑的截断，使得超出截断半径的远处原子没有贡献，且力保持良好性质。\n- 为了解析原子周围的径向分布，使用具有可调宽度和中心参数的径向基，从而使该表示能够区分不同长度尺度下的环境。\n\n从这些原理出发，推导并实现一个针对选定中心原子 $i$ 的双体径向对称函数 $G^2$，其形式为\n- 对邻近原子 $j \\neq i$ 的求和，\n- 一个平滑的、有限范围的截断函数，该函数是 $C^1$-连续的，并在截断半径处等于零，\n- 以及一个可以围绕选定距离进行平移和锐化的局部径向权重。\n\n在你的推导和实现中，具体指定并使用以下形式：\n- 使用余弦截断函数\n$$\nf_c(r; R_c) = \n\\begin{cases}\n\\dfrac{1}{2}\\left[\\cos\\!\\left(\\dfrac{\\pi r}{R_c}\\right) + 1\\right], & r \\le R_c,\\\\\n0, & r > R_c,\n\\end{cases}\n$$\n其中余弦函数的参数以弧度为单位。\n- 使用类高斯径向基\n$$\n\\exp\\!\\left[-\\eta\\,(r - R_s)^2\\right],\n$$\n其中包含宽度参数 $\\eta$ 和位移参数 $R_s$。\n- 将它们组合成双体对称函数\n$$\nG_i^{2}(\\eta, R_s, R_c) = \\sum_{j \\ne i} \\exp\\!\\left[-\\eta\\,(r_{ij} - R_s)^2\\right]\\, f_c(r_{ij}; R_c),\n$$\n其中 $r_{ij}$ 是原子 $i$ 和 $j$ 之间的欧几里得距离。\n\n所有原子均为氩（Ar）原子，被视为单一化学物种，因此不需要依赖于物种的权重。距离 $r_{ij}$、截断半径 $R_c$ 和位移 $R_s$ 必须以埃（Ångström）为单位，$\\eta$ 以 $\\text{Å}^{-2}$ 为单位。余弦函数必须以弧度为单位接收其参数。\n\n程序要求：\n- 实现一个函数，该函数在给定一组笛卡尔坐标（单位为埃）、中心原子的索引 $i$ 和参数 $(\\eta, R_s, R_c)$ 的情况下，使用上述公式计算 $G_i^{2}(\\eta, R_s, R_c)$。\n- 使用标准的三维欧几里得距离。不应用周期性边界条件。\n- 数值稳定性：排除自相互作用（$j = i$）。距离 $r_{ij}$ 是严格非负的；除了排除自相互作用外，无需对 $r_{ij} = 0$ 进行特殊处理。\n\n测试集：\n对以下五个案例中的每一个，评估 $G_i^{2}$。每个案例都指定了 $(\\text{positions}, i, R_c, \\eta, R_s)$，其中所有距离单位为埃，$\\eta$ 的单位为 $\\text{Å}^{-2}$：\n- 案例 A:\n  - positions: $\\big[(0,0,0),(2.0,0,0),(0,3.0,0),(0,0,4.0)\\big]$\n  - $i = 0$\n  - $R_c = 5.0$\n  - $\\eta = 0.5$\n  - $R_s = 0.0$\n- 案例 B:\n  - positions: $\\big[(0,0,0)\\big]$\n  - $i = 0$\n  - $R_c = 3.0$\n  - $\\eta = 1.0$\n  - $R_s = 0.0$\n- 案例 C:\n  - positions: $\\big[(0,0,0),(5.0,0,0),(-5.0,0,0)\\big]$\n  - $i = 0$\n  - $R_c = 5.0$\n  - $\\eta = 1.0$\n  - $R_s = 0.0$\n- 案例 D:\n  - positions: $\\big[(0,0,0),(2.0,0,0),(0,3.0,0),(0,0,4.0)\\big]$\n  - $i = 0$\n  - $R_c = 5.0$\n  - $\\eta = 2.0$\n  - $R_s = 2.5$\n- 案例 E:\n  - positions: $\\big[(0,0,0),(2.0,0,0),(0,3.0,0),(0,0,4.0)\\big]$\n  - $i = 1$\n  - $R_c = 5.0$\n  - $\\eta = 0.5$\n  - $R_s = 0.0$\n\n输出规范：\n- 对每个案例，计算一个浮点数值 $G_i^{2}$。\n- 使用标准四舍五入将每个结果精确到 $6$ 位小数。\n- 你的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，结果按 A、B、C、D、E 的顺序排列。例如，使用通用占位符的输出应如下所示：\"[0.123456,0.000000,0.000000,1.234567,0.654321]\"。", "solution": "所提出的问题是有效的、科学上合理的且定义明确的。它要求推导和实现一种双体径向对称函数，这是现代机器学习原子间势（如高维神经网络势 HDNNP）的一个基本组成部分。我们将首先从第一性原理推导出该函数的形式，然后详细说明其计算算法。\n\n原子系统的势能 $E$ 是一个标量。为了使其具有物理意义，它必须在整个系统的平移和旋转，以及相同原子的置换下保持不变。在 HDNNP 方案中，总能量被分解为原子贡献 $E_i$，其中 $E = \\sum_i E_i$。每个原子能量 $E_i$ 是原子 $i$ 局域环境的函数，由一组描述符或“对称函数” $\\{G_i\\}$ 来表征。因此，这些对称函数本身必须对上述变换保持不变。\n\n1.  **平移和旋转不变性**：这些对称性决定了原子 $i$ 的描述符必须仅依赖其局域环境的内部坐标，而不是全局坐标系中原子的绝对笛卡尔坐标。最简单的内部坐标集由中心原子 $i$ 与其邻原子 $j$ 之间的标量距离 $r_{ij}$ 组成。任何这些距离的函数，$G_i = F(\\{r_{ij}\\}_{j \\neq i})$，对于原子集合的刚性平移和旋转都自动保持不变。\n\n2.  **置换不变性**：原子 $i$ 的能量贡献不得依赖于其相同邻近原子的任意标记。如果原子 $j$ 和 $k$ 属于同一物种，交换它们不能改变描述符的值。满足此条件的最简单数学构造是对所有邻原子求和。因此，我们提出一种形式为 $G_i = \\sum_{j \\neq i} g(r_{ij})$ 的描述符，其中 $g$ 是原子间距离的某个函数。这种形式是双体对称函数的基础。\n\n3.  **局域性和平滑性**：物理相互作用本质上是局域的；非常遥远的原子的影响可以忽略不计。为了对此建模，我们引入一个平滑的截断函数 $f_c(r_{ij}; R_c)$，它与每个邻原子的贡献相乘。对于小距离，该函数值必须等于 $1$；当距离 $r_{ij}$ 接近截断半径 $R_c$ 时，该函数值平滑地趋于 $0$。对于距离 $r_{ij} > R_c$，其贡献完全为零。对平滑性的要求，特别是 $C^1$-连续性（一阶导数连续），至关重要。原子所受的力计算为势能的负梯度，$\\mathbf{F}_k = -\\nabla_{\\mathbf{r}_k} E$。能量一阶导数的不连续性将导致不符合物理的、无限大的力。提供的余弦截断函数是：\n    $$\n    f_c(r; R_c) = \n    \\begin{cases}\n    \\frac{1}{2}\\left[\\cos\\left(\\frac{\\pi r}{R_c}\\right) + 1\\right], & r \\le R_c,\\\\\n    0, & r > R_c.\n    \\end{cases}\n    $$\n    在截断半径 $r = R_c$ 处，函数值为 $f_c(R_c; R_c) = \\frac{1}{2}[\\cos(\\pi) + 1] = \\frac{1}{2}[-1 + 1] = 0$，确保了连续性。其导数为 $f'_c(r; R_c) = -\\frac{\\pi}{2R_c}\\sin(\\frac{\\pi r}{R_c})$。在 $r = R_c$ 处，导数为 $f'_c(R_c; R_c) = -\\frac{\\pi}{2R_c}\\sin(\\pi) = 0$，这与 $r > R_c$ 时零函数的导数相匹配。因此，该函数按要求是 $C^1$-连续的。\n\n4.  **径向分辨率**：截断函数的简单求和只能提供截断球内邻原子的加权计数。为了创建一个能区分不同径向结构的描述符，我们引入一个径向基函数。指定的高斯形式 $\\exp[-\\eta(r_{ij} - R_s)^2]$ 即用于此目的。这个函数以距离 $R_s$ 为中心，其特征宽度由参数 $\\eta$ 控制。较大的 $\\eta$ 对应更窄、峰值更尖锐的高斯函数。通过使用一组具有不同参数 $(\\eta, R_s)$ 的此类函数，可以解析中心原子 $i$ 周围邻原子的径向分布。\n\n结合这四个原理——使用距离带来的不变性、求和带来的置换对称性、平滑截断带来的局域性以及径向基带来的分辨率——我们得到了指定的双体径向对称函数，记为 $G_i^2$：\n$$\nG_i^{2}(\\eta, R_s, R_c) = \\sum_{j \\ne i} \\exp\\!\\left[-\\eta\\,(r_{ij} - R_s)^2\\right]\\, f_c(r_{ij}; R_c)\n$$\n求和遍及系统中的所有原子 $j$，不包括中心原子 $i$。对于每个邻原子 $j$，仅当其与原子 $i$ 的距离 $r_{ij}$ 小于或等于截断半径 $R_c$ 时，我们才计算其贡献。\n\n计算过程如下：\n给定 $N$ 个原子的笛卡尔坐标集 $\\{\\mathbf{r}_k\\}_{k=0,..,N-1}$、一个中心原子索引 $i$ 以及参数 $\\eta$、$R_s$ 和 $R_c$：\n1.  将对称函数值 $G_i^2$ 初始化为 $0$。\n2.  确定中心原子的坐标向量 $\\mathbf{r}_i$。\n3.  遍历所有其他原子 $j$，其中 $j \\in \\{0, 1, ..., N-1\\}$ 且 $j \\neq i$。\n4.  对于每个邻原子 $j$，计算欧几里得距离 $r_{ij} = ||\\mathbf{r}_j - \\mathbf{r}_i|| = \\sqrt{(x_j-x_i)^2 + (y_j-y_i)^2 + (z_j-z_i)^2}$。\n5.  检查是否 $r_{ij} \\le R_c$。如果不是，原子 $j$ 的贡献为 $0$，我们继续处理下一个邻原子。\n6.  如果 $r_{ij} \\le R_c$，则计算该项的两个分量：\n    -   径向基项：$T_{\\text{rad}} = \\exp[-\\eta(r_{ij} - R_s)^2]$。\n    -   截断函数项：$T_{\\text{cut}} = \\frac{1}{2}[\\cos(\\frac{\\pi r_{ij}}{R_c}) + 1]$。\n7.  将这些项的乘积 $T_{\\text{rad}} \\times T_{\\text{cut}}$ 加到 $G_i^2$ 的累加和中。\n8.  遍历所有邻原子 $j$ 后，最终的和即为原子 $i$ 的对称函数值。\n\n现在将实现此过程并将其应用于五个指定的测试案例。所有单位必须一致；距离（$r_{ij}$、$R_s$、$R_c$）以埃（$\\text{Å}$）为单位，参数 $\\eta$ 以 $\\text{Å}^{-2}$ 为单位，以确保指数的参数是无量纲的。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem by computing the Behler-Parrinello G2 symmetry function\n    for a series of test cases.\n    \"\"\"\n\n    def compute_g2(positions, i, R_c, eta, R_s):\n        \"\"\"\n        Computes the G2 symmetry function for a central atom i.\n        \n        Args:\n            positions (np.ndarray): Array of shape (N, 3) with Cartesian coordinates.\n            i (int): Index of the central atom.\n            R_c (float): Cutoff radius in Angstrom.\n            eta (float): Width parameter in Angstrom^-2.\n            R_s (float): Shift parameter in Angstrom.\n        \n        Returns:\n            float: The computed value of the G2 symmetry function.\n        \"\"\"\n        if positions.shape[0] <= 1:\n            return 0.0\n\n        central_atom_pos = positions[i]\n        g2_value = 0.0\n\n        for j in range(positions.shape[0]):\n            if i == j:\n                continue\n\n            neighbor_pos = positions[j]\n            # Calculate Euclidean distance\n            r_ij = np.linalg.norm(central_atom_pos - neighbor_pos)\n\n            # Apply the cutoff condition\n            if r_ij <= R_c:\n                # Cosine cutoff function\n                fc = 0.5 * (np.cos(np.pi * r_ij / R_c) + 1.0)\n                \n                # Gaussian-like radial basis function\n                radial_term = np.exp(-eta * (r_ij - R_s)**2)\n                \n                # Add contribution to the sum\n                g2_value += radial_term * fc\n        \n        return g2_value\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case A\n        {'positions': np.array([[0.0, 0.0, 0.0], [2.0, 0.0, 0.0], [0.0, 3.0, 0.0], [0.0, 0.0, 4.0]]),\n         'i': 0, 'R_c': 5.0, 'eta': 0.5, 'R_s': 0.0},\n        # Case B\n        {'positions': np.array([[0.0, 0.0, 0.0]]),\n         'i': 0, 'R_c': 3.0, 'eta': 1.0, 'R_s': 0.0},\n        # Case C\n        {'positions': np.array([[0.0, 0.0, 0.0], [5.0, 0.0, 0.0], [-5.0, 0.0, 0.0]]),\n         'i': 0, 'R_c': 5.0, 'eta': 1.0, 'R_s': 0.0},\n        # Case D\n        {'positions': np.array([[0.0, 0.0, 0.0], [2.0, 0.0, 0.0], [0.0, 3.0, 0.0], [0.0, 0.0, 4.0]]),\n         'i': 0, 'R_c': 5.0, 'eta': 2.0, 'R_s': 2.5},\n        # Case E\n        {'positions': np.array([[0.0, 0.0, 0.0], [2.0, 0.0, 0.0], [0.0, 3.0, 0.0], [0.0, 0.0, 4.0]]),\n         'i': 1, 'R_c': 5.0, 'eta': 0.5, 'R_s': 0.0},\n    ]\n\n    results = []\n    for case in test_cases:\n        result = compute_g2(\n            positions=case['positions'],\n            i=case['i'],\n            R_c=case['R_c'],\n            eta=case['eta'],\n            R_s=case['R_s']\n        )\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    # The format string \"{:.6f}\" handles rounding to 6 decimal places.\n    print(f\"[{','.join([f'{r:.6f}' for r in results])}]\")\n\nsolve()\n```", "id": "2457438"}, {"introduction": "在创建了描述符之后，下一步是训练神经网络模型以拟合能量数据。这个练习模拟了一个常见的现实世界问题：训练集中存在错误的“第一性原理”数据点。通过向一个理想数据集中“投毒”并观察其对所得势能面的剧烈影响，你将对数据管理的极端重要性以及神经网络势的敏感性获得一个实践性的认识。[@problem_id:2456326]", "problem": "现给定一个双原子分子的一维基准问题，其真实势能面 (PES) 由莫尔斯势定义。设作为键长函数的真实能量由下式给出\n$$\nE_{\\text{true}}(r) = D_e \\left(1 - e^{-a (r - r_e)}\\right)^2 - D_e,\n$$\n其中参数 $D_e = 4.744$ 电子伏特 (eV)，$r_e = 0.7414$ 埃 ($\\text{\\AA}$)，以及 $a = 1.942$ $\\text{\\AA}^{-1}$。所有距离都必须以 $\\text{\\AA}$ 为单位，所有能量都必须以 eV 为单位。\n\n考虑一个高维神经网络势 (NNP) 模型，该模型将一个对称函数向量映射到一个预测能量。定义长度为 $M = 8$ 的以原子为中心的径向描述符向量 $\\mathbf{G}(r) \\in \\mathbb{R}^M$ 如下：\n$$\nG_i(r) = f_c(r)\\,\\exp\\!\\left(-\\eta \\left(r - R_{s,i}\\right)^2\\right),\n$$\n其中平滑截断函数 $f_c(r)$ 为\n$$\nf_c(r) =\n\\begin{cases}\n\\frac{1}{2}\\left[\\cos\\!\\left(\\pi \\frac{r}{R_c}\\right) + 1\\right], & r \\le R_c,\\\\\n0, & r > R_c,\n\\end{cases}\n$$\n其中 $R_c = 3.0$ $\\text{\\AA}$，$\\eta = 4.0$ $\\text{\\AA}^{-2}$，中心点 $R_{s,i}$ 由列表 $R_{s} = \\{0.6, 0.9, 1.2, 1.5, 1.8, 2.1, 2.4, 2.7\\}$（单位为 $\\text{\\AA}$）给出（即，$R_{s,1} = 0.6$，$R_{s,2} = 0.9$，$R_{s,3} = 1.2$，$R_{s,4} = 1.5$，$R_{s,5} = 1.8$，$R_{s,6} = 2.1$，$R_{s,7} = 2.4$，$R_{s,8} = 2.7$）。\n\n将神经网络势定义为\n$$\nE_{\\boldsymbol{\\theta}}(r) = \\mathbf{w}_2^\\top \\,\\phi\\!\\left(\\mathbf{W}_1\\,\\mathbf{G}(r) + \\mathbf{b}_1\\right) + b_2,\n$$\n其中 $\\phi(x) = \\tanh(x)$ 逐元素应用，$\\mathbf{W}_1 \\in \\mathbb{R}^{H \\times M}$，$\\mathbf{b}_1 \\in \\mathbb{R}^{H}$，$\\mathbf{w}_2 \\in \\mathbb{R}^{H}$，$b_2 \\in \\mathbb{R}$，且隐藏层大小为 $H = 10$。模型参数 $\\boldsymbol{\\theta} = \\{\\mathbf{W}_1, \\mathbf{b}_1, \\mathbf{w}_2, b_2\\}$ 将通过最小化均方误差目标函数来获得：\n$$\n\\mathcal{L}(\\boldsymbol{\\theta}) = \\frac{1}{N}\\sum_{n=1}^{N}\\left(E_{\\boldsymbol{\\theta}}(r_n) - E_{\\text{true}}(r_n)\\right)^2,\n$$\n给定训练对 $\\{(r_n, E_{\\text{true}}(r_n))\\}_{n=1}^{N}$。使用 $N_{\\text{train}} = 64$ 个训练距离 $r_n$，这些距离在闭区间 $[0.5, 2.8]$ $\\text{\\AA}$ 上均匀分布。\n\n参数估计必须通过批量梯度下降执行，并遵循以下精确规格：学习率 $\\alpha = 10^{-2}$，迭代次数 $T = 3000$，权重初始化使用等于 $42$ 的固定伪随机种子，其中 $\\mathbf{W}_1$ 和 $\\mathbf{w}_2$ 的条目分别来自均值为零、标准差为 $\\sigma_{W_1} = 1/\\sqrt{M}$ 和 $\\sigma_{w_2} = 1/\\sqrt{H}$ 的独立正态分布抽样，而 $\\mathbf{b}_1$ 和 $b_2$ 进行零初始化。不允许存在其他随机性。\n\n通过在干净数据集 $\\{(r_n, E_{\\text{true}}(r_n))\\}_{n=1}^{N}$ 上进行训练来构建一个基线模型。然后，通过将特定训练距离处的能量值替换为旨在模仿高度错误的密度泛函理论 (DFT) 标签的错误值，来构建四个投毒数据集。对于给定的目标距离 $r^\\star$ 和偏移量 $\\Delta$，找出使 $|r_k - r^\\star|$ 最小化的训练索引 $k$，并将相应的能量标签设置为 $E_{\\text{true}}(r_k) + \\Delta$；如果多个目标映射到同一个 $k$，则通过加法累积偏移量。四个投毒场景如下：\n- 场景 1 (边界情况)：无投毒 (目标集为空)。\n- 场景 2：在 $r^\\star = 0.7414$ $\\text{\\AA}$ 处有一个偏移量为 $\\Delta = +8.0$ eV 的单一投毒点。\n- 场景 3：在 $r^\\star = 2.2$ $\\text{\\AA}$ 和 $r^\\star = 2.4$ $\\text{\\AA}$ 处有两个投毒点，每个点的偏移量均为 $\\Delta = -4.0$ eV。\n- 场景 4 (边缘情况)：在 $r^\\star = 0.6$ $\\text{\\AA}$ 处有一个偏移量为 $\\Delta = -10.0$ eV 的投毒点。\n\n对于每个投毒场景，使用与基线模型相同的优化设置和初始化种子，从头开始重新训练一个新模型。为进行评估，在闭区间 $[0.5, 3.0]$ $\\text{\\AA}$ 上定义一个包含 $N_{\\text{eval}} = 201$ 个距离的均匀网格，并计算以下三个定量指标，以捕捉投毒模型的预测 PES 与基线模型的预测 PES 之间的差异：\n1. 均方根能量偏差\n$$\nm_1 = \\sqrt{\\frac{1}{N_{\\text{eval}}}\\sum_{j=1}^{N_{\\text{eval}}}\\left(E_{\\boldsymbol{\\theta}^{\\text{poison}}}(r_j) - E_{\\boldsymbol{\\theta}^{\\text{base}}}(r_j)\\right)^2}\\ \\text{in eV}.\n$$\n2. 评估网格上预测的平衡键长（最小能量位置）的绝对位移，\n$$\nm_2 = \\left|\\operatorname*{argmin}_{j}\\,E_{\\boldsymbol{\\theta}^{\\text{poison}}}(r_j) - \\operatorname*{argmin}_{j}\\,E_{\\boldsymbol{\\theta}^{\\text{base}}}(r_j)\\right|\\ \\text{in }\\text{\\AA},\n$$\n其中 $\\operatorname*{argmin}_{j}$ 返回离散最小值所在的网格值 $r_j$。\n3. 网格上预测力的最大绝对偏差，\n$$\nm_3 = \\max_{j}\\,\\left|F_{\\boldsymbol{\\theta}^{\\text{poison}}}(r_j) - F_{\\boldsymbol{\\theta}^{\\text{base}}}(r_j)\\right|\\ \\text{in eV}/\\text{\\AA},\n$$\n其中力定义为 $F_{\\boldsymbol{\\theta}}(r) = -\\frac{d}{dr}E_{\\boldsymbol{\\theta}}(r)$，并且导数必须使用均匀网格隐含的网格间距，通过有限差分在评估网格上进行近似计算。\n\n此一维问题中不出现角度。所有输出都必须以上述指定单位报告。您的程序必须按照所列顺序为四个场景中的每一个计算 $(m_1, m_2, m_3)$。将每个标量值四舍五入到恰好六位小数。\n\n您的程序应生成单行输出，其中包含四个场景（从 1 到 4）的结果，格式为列表的列表\n$$\n\\big[\\,[m_{1,1}, m_{2,1}, m_{3,1}],\\ [m_{1,2}, m_{2,2}, m_{3,2}],\\ [m_{1,3}, m_{2,3}, m_{3,3}],\\ [m_{1,4}, m_{2,4}, m_{3,4}]\\,\\big],\n$$\n每个数值条目都四舍五入到六位小数。", "solution": "问题陈述已通过验证。\n\n**步骤1：提取的已知条件**\n\n- **真实势能面 (PES):** 莫尔斯势，$E_{\\text{true}}(r) = D_e \\left(1 - e^{-a (r - r_e)}\\right)^2 - D_e$。\n  - 参数：$D_e = 4.744$ eV, $r_e = 0.7414$ Å, $a = 1.942$ Å⁻¹。\n- **对称函数描述符：** 对于键长为 r 的双原子体系，描述符向量为 $\\mathbf{G}(r) \\in \\mathbb{R}^M$，其中 $M=8$。\n  - 分量：$G_i(r) = f_c(r)\\,\\exp\\!\\left(-\\eta \\left(r - R_{s,i}\\right)^2\\right)$。\n  - 截断函数：$f_c(r) = \\frac{1}{2}\\left[\\cos\\!\\left(\\pi \\frac{r}{R_c}\\right) + 1\\right]$ 对于 $r \\le R_c$，否则为 $0$。\n  - 参数：$R_c = 3.0$ Å, $\\eta = 4.0$ Å⁻², 以及中心点 $R_{s} = \\{0.6, 0.9, 1.2, 1.5, 1.8, 2.1, 2.4, 2.7\\}$ Å。\n- **神经网络势 (NNP) 模型：** 单隐藏层感知器。\n  - 架构：$E_{\\boldsymbol{\\theta}}(r) = \\mathbf{w}_2^\\top \\,\\phi\\!\\left(\\mathbf{W}_1\\,\\mathbf{G}(r) + \\mathbf{b}_1\\right) + b_2$。\n  - 隐藏层大小：$H = 10$。\n  - 激活函数：$\\phi(x) = \\tanh(x)$。\n  - 参数：$\\boldsymbol{\\theta} = \\{\\mathbf{W}_1, \\mathbf{b}_1, \\mathbf{w}_2, b_2\\}$。\n- **训练流程**\n  - 目标：最小化均方误差，$\\mathcal{L}(\\boldsymbol{\\theta}) = \\frac{1}{N}\\sum_{n=1}^{N}\\left(E_{\\boldsymbol{\\theta}}(r_n) - E_{\\text{true}}(r_n)\\right)^2$。\n  - 数据集：$N_{\\text{train}} = 64$ 个点，距离 $r_n$ 在 $[0.5, 2.8]$ Å 区间内均匀分布。\n  - 优化器：批量梯度下降。\n  - 超参数：学习率 $\\alpha = 10^{-2}$，迭代次数 $T = 3000$。\n  - 初始化：固定的伪随机种子 $42$。权重 $\\mathbf{W}_1$ 和 $\\mathbf{w}_2$ 来自均值为零、标准差分别为 $\\sigma_{W_1} = 1/\\sqrt{M}$ 和 $\\sigma_{w_2} = 1/\\sqrt{H}$ 的正态分布。偏置 $\\mathbf{b}_1, b_2$ 初始化为零。\n- **数据投毒场景：** 在使 $|r_k - r^\\star|$ 最小化的索引 $k$ 处，将训练能量标签 $E_{\\text{true}}(r_k)$ 修改为 $E_{\\text{true}}(r_k) + \\Delta$。\n  - 场景 1：无投毒。\n  - 场景 2：$r^\\star = 0.7414$ Å，$\\Delta = +8.0$ eV。\n  - 场景 3：$r^\\star = 2.2$ Å，$\\Delta = -4.0$ eV 以及 $r^\\star = 2.4$ Å，$\\Delta = -4.0$ eV。\n  - 场景 4：$r^\\star = 0.6$ Å，$\\Delta = -10.0$ eV。\n- **评估指标：** 在 $[0.5, 3.0]$ Å 区间内含 $N_{\\text{eval}} = 201$ 个距离的网格上，将每个投毒模型与一个基线模型（在未投毒数据上训练）进行比较。\n  1. $m_1$：均方根能量偏差 (eV)。\n  2. $m_2$：平衡键长的绝对位移 (Å)。\n  3. $m_3$：预测力的最大绝对偏差 (eV/Å)，其中力 $F = -dE/dr$ 通过有限差分近似。\n\n**步骤2：已知条件的验证**\n\n对问题的有效性进行审查。\n- **科学上成立：** 是的。该问题描述了计算化学中用于测试神经网络势的一个简化但标准的基准问题。莫尔斯势是双原子相互作用的经典模型，而 Behler-Parrinello 类型的 NNP 架构代表了该领域的一种基础方法。所有参数和单位在物理上都是一致的。\n- **适定性：** 是的。该任务是一个定义明确的监督回归问题，具有确定性的训练过程（从固定初始化开始的批量梯度下降）。这保证了存在唯一且可计算的解。\n- **客观性：** 是的。该问题使用精确的数学定义和定量度量进行陈述，没有任何主观或模棱两可的语言。\n\n该问题未违反任何无效标准。它是一个完整、一致且具有科学意义的计算任务。\n\n**步骤3：结论与行动**\n\n问题**有效**。将提供一个解决方案。\n\n**基于原理的解决方案**\n\n目标是通过训练多个模型并将其预测与基线进行比较，来量化错误训练数据点对神经网络势 (NNP) 的影响。解决方案的核心涉及实现一个完整的机器学习工作流程：数据生成、特征化、模型训练和评估。\n\n首先，我们使用莫尔斯势定义真实势能面 (PES)：\n$$\nE_{\\text{true}}(r) = D_e \\left(1 - e^{-a (r - r_e)}\\right)^2 - D_e\n$$\n该函数为我们的训练数据集提供目标能量标签，该数据集包含从区间 $[0.5, 2.8]$ Å 均匀采样的 $N_{\\text{train}} = 64$ 个距离 $r_n$。\n\n接着，我们定义从键长 $r$ 到特征向量 $\\mathbf{G}(r) \\in \\mathbb{R}^M$ 的变换，该向量作为 NNP 的输入。这是通过一组 $M=8$ 个以原子为中心的径向对称函数完成的：\n$$\nG_i(r) = f_c(r)\\,\\exp\\!\\left(-\\eta \\left(r - R_{s,i}\\right)^2\\right)\n$$\n其中基于余弦的截断函数 $f_c(r)$ 确保描述符在截断半径 $R_c = 3.0$ Å 处平滑地变为零。这种特征化将原始几何信息转换为适合机器学习模型的格式，并且对相同原子的排列具有不变性（这在双原子情况下是一个微不足道的性质，但对于通用 NNP 而言是一个关键的设计原则）。\n\nNNP 本身是一个标准的前馈神经网络，具有一个大小为 $H=10$ 的隐藏层和双曲正切激活函数 $\\phi(x) = \\tanh(x)$。对于输入描述符向量 $\\mathbf{G}(r)$，预测能量由下式给出：\n$$\nE_{\\boldsymbol{\\theta}}(r) = \\mathbf{w}_2^\\top \\,\\phi\\!\\left(\\mathbf{W}_1\\,\\mathbf{G}(r) + \\mathbf{b}_1\\right) + b_2\n$$\n参数 $\\boldsymbol{\\theta} = \\{\\mathbf{W}_1, \\mathbf{b}_1, \\mathbf{w}_2, b_2\\}$ 通过最小化整个数据集上预测能量与训练标签之间的均方误差 (MSE) 损失函数来进行优化：\n$$\n\\mathcal{L}(\\boldsymbol{\\theta}) = \\frac{1}{N_{\\text{train}}}\\sum_{n=1}^{N_{\\text{train}}}\\left(E_{\\boldsymbol{\\theta}}(r_n) - E_{\\text{label}}(r_n)\\right)^2\n$$\n优化使用批量梯度下降法进行，学习率为 $\\alpha=10^{-2}$，迭代 $T=3000$ 次。损失函数相对于参数的梯度是通过反向传播算法导出的。对于参数 $P \\in \\boldsymbol{\\theta}$，更新规则为：\n$$\nP \\leftarrow P - \\alpha \\frac{\\partial \\mathcal{L}}{\\partial P}\n$$\n诸如 $\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{W}_1}$ 之类的梯度是通过从输出层到输入层反向应用链式法则来计算的。例如，第一层权重 $\\mathbf{W}_1$ 的梯度是输入描述符 $\\mathbf{G}$、输出误差以及激活函数导数的函数。固定的随机种子确保每次训练运行的初始参数值都相同，从而隔离了数据投毒的影响。\n\n我们首先通过在干净数据集（其中 $E_{\\text{label}}(r_n) = E_{\\text{true}}(r_n)$）上训练一个模型 $\\boldsymbol{\\theta}^{\\text{base}}$ 来建立基线。然后，对于三个投毒场景中的每一个，我们通过在特定点更改能量标签来创建一个新的训练数据集。对于每个“投毒”数据集，我们都从相同的初始状态训练一个新模型 $\\boldsymbol{\\theta}^{\\text{poison}}$。\n\n最后，我们评估投毒的影响。我们在 $[0.5, 3.0]$ Å 范围内生成一个包含 $N_{\\text{eval}} = 201$ 个距离的精细评估网格。对于每个投毒模型，我们计算相对于基线模型在此网格上预测的三个指标：\n1.  能量的均方根偏差 $m_1$，用于衡量预测 PES 的平均全局变化。\n2.  预测平衡键长的位移 $m_2$，用于探究对分子关键物理性质的影响。通过为每个模型找到最小化预测能量的网格点 $r_j$ 来确定。\n3.  力的最大绝对偏差 $m_3$，用于评估对能量导数的影响，这对于分子动力学等应用至关重要。力 $F(r) = -dE/dr$ 在评估网格上使用中心有限差分进行数值近似。\n\n第一个场景，即无投毒的场景，作为一个对照。其“投毒”模型与基线模型完全相同，因此所有三个偏差指标 $(m_1, m_2, m_3)$ 都必然为零。对于其他场景，这些指标为 NNP 对训练数据中局部错误的敏感性提供了定量洞察。", "answer": "```python\nimport numpy as np\n\n# Global constants defined in the problem statement\nDE = 4.744\nRE = 0.7414\nA = 1.942\nRC = 3.0\nETA = 4.0\nRS = np.array([0.6, 0.9, 1.2, 1.5, 1.8, 2.1, 2.4, 2.7])\nM = 8\nH = 10\nN_TRAIN = 64\nN_EVAL = 201\nR_TRAIN_MIN, R_TRAIN_MAX = 0.5, 2.8\nR_EVAL_MIN, R_EVAL_MAX = 0.5, 3.0\nLR = 1e-2\nT = 3000\nSEED = 42\n\ndef morse_potential(r):\n    \"\"\"Calculates the Morse potential energy.\"\"\"\n    return DE * (1 - np.exp(-A * (r - RE)))**2 - DE\n\ndef cutoff_function(r):\n    \"\"\"Calculates the smooth cutoff function value.\"\"\"\n    return np.where(r <= RC, 0.5 * (np.cos(np.pi * r / RC) + 1), 0)\n\ndef compute_G_matrix(r_vec):\n    \"\"\"Computes the matrix of symmetry function vectors for a vector of distances.\"\"\"\n    r_vec = r_vec.reshape(-1, 1)\n    fc_r = cutoff_function(r_vec)\n    G = fc_r * np.exp(-ETA * (r_vec - RS)**2)\n    return G\n\ndef initialize_parameters(rng):\n    \"\"\"Initializes NNP parameters according to the specification.\"\"\"\n    # Using HxM for W1 to match problem's math notation\n    W1 = rng.normal(0, 1 / np.sqrt(M), size=(H, M))\n    b1 = np.zeros((H, 1))\n    w2 = rng.normal(0, 1 / np.sqrt(H), size=(H, 1))\n    b2 = 0.0\n    return {\"W1\": W1, \"b1\": b1, \"w2\": w2, \"b2\": b2}\n\ndef forward_pass(G, params):\n    \"\"\"Performs a forward pass through the NNP.\"\"\"\n    # G is (N, M), W1 is (H, M) -> W1 @ G.T is (H, N)\n    Z1 = params[\"W1\"] @ G.T + params[\"b1\"]  # Shape (H, N)\n    A1 = np.tanh(Z1)  # Shape (H, N)\n    # A1.T is (N, H), w2 is (H, 1) -> A1.T @ w2 is (N, 1)\n    E_pred = A1.T @ params[\"w2\"] + params[\"b2\"]  # Shape (N, 1)\n    \n    # an_z1 contains intermediate values needed for backpropagation\n    cache = {\"Z1\": Z1, \"A1\": A1}\n    return E_pred, cache\n\ndef train_model(r_train, E_train):\n    \"\"\"Trains an NNP model using batch gradient descent.\"\"\"\n    rng = np.random.default_rng(SEED)\n    params = initialize_parameters(rng)\n    \n    N = len(r_train)\n    G_train = compute_G_matrix(r_train)\n    E_train_col = E_train.reshape(-1, 1)\n\n    for _ in range(T):\n        # Forward pass\n        E_pred, cache = forward_pass(G_train, params)\n        \n        # Backward pass (Gradient Calculation)\n        d_E_pred = (2 / N) * (E_pred - E_train_col)  # Shape (N, 1)\n        \n        # Gradients for output layer\n        d_b2 = np.sum(d_E_pred)\n        d_w2 = cache[\"A1\"] @ d_E_pred  # A1 is (H,N), d_E_pred is (N,1) -> (H,1)\n        \n        # Backpropagate to hidden layer\n        d_A1_T = d_E_pred @ params[\"w2\"].T # (N,1) @ (1,H) -> (N,H)\n        d_Z1_T = d_A1_T * (1 - cache[\"A1\"].T**2) # (N,H) * (N,H) -> (N,H)\n        \n        # Gradients for input layer\n        d_b1 = np.sum(d_Z1_T, axis=0, keepdims=True).T # (H,1)\n        d_W1 = d_Z1_T.T @ G_train # (H,N) @ (N,M) -> (H,M)\n        \n        # Update parameters\n        params[\"W1\"] -= LR * d_W1\n        params[\"b1\"] -= LR * d_b1\n        params[\"w2\"] -= LR * d_w2\n        params[\"b2\"] -= LR * d_b2\n        \n    return params\n\ndef poison_dataset(E_base, r_train, targets):\n    \"\"\"Modifies the energy labels based on poisoning targets.\"\"\"\n    E_poisoned = np.copy(E_base)\n    # Accumulate offsets for targets that map to the same point\n    modifications = {}\n    for r_star, delta in targets:\n        k = np.argmin(np.abs(r_train - r_star))\n        modifications[k] = modifications.get(k, 0) + delta\n    \n    for k, total_delta in modifications.items():\n        E_poisoned[k] += total_delta\n        \n    return E_poisoned\n    \ndef solve():\n    # 1. Setup grids and true data\n    r_train = np.linspace(R_TRAIN_MIN, R_TRAIN_MAX, N_TRAIN)\n    r_eval = np.linspace(R_EVAL_MIN, R_EVAL_MAX, N_EVAL)\n    dr_eval = (R_EVAL_MAX - R_EVAL_MIN) / (N_EVAL - 1)\n    E_true_train = morse_potential(r_train)\n    \n    # 2. Define poisoning scenarios\n    scenarios = [\n        {\"targets\": []},\n        {\"targets\": [(0.7414, 8.0)]},\n        {\"targets\": [(2.2, -4.0), (2.4, -4.0)]},\n        {\"targets\": [(0.6, -10.0)]},\n    ]\n\n    # 3. Train baseline model\n    params_base = train_model(r_train, E_true_train)\n    G_eval = compute_G_matrix(r_eval)\n    E_pred_base, _ = forward_pass(G_eval, params_base)\n    \n    # Pre-calculate baseline properties for evaluation\n    idx_min_base = np.argmin(E_pred_base)\n    r_min_base = r_eval[idx_min_base]\n    F_base = -np.gradient(E_pred_base.flatten(), dr_eval)\n\n    all_results = []\n\n    # 4. Handle all scenarios\n    for i, scen in enumerate(scenarios):\n        if i == 0:\n            # Scenario 1 is the baseline vs itself, so deviations are zero.\n            results = [0.0, 0.0, 0.0]\n        else:\n            # Create poisoned dataset\n            E_train_poisoned = poison_dataset(E_true_train, r_train, scen[\"targets\"])\n            \n            # Train model from scratch on poisoned data\n            params_poison = train_model(r_train, E_train_poisoned)\n            \n            # Evaluate on the fine grid\n            E_pred_poison, _ = forward_pass(G_eval, params_poison)\n            \n            # Compute metrics\n            m1 = np.sqrt(np.mean((E_pred_poison - E_pred_base)**2))\n            \n            idx_min_poison = np.argmin(E_pred_poison)\n            r_min_poison = r_eval[idx_min_poison]\n            m2 = np.abs(r_min_poison - r_min_base)\n\n            F_poison = -np.gradient(E_pred_poison.flatten(), dr_eval)\n            m3 = np.max(np.abs(F_poison - F_base))\n            \n            results = [m1, m2, m3]\n\n        all_results.append([round(x, 6) for x in results])\n\n    # 5. Format and print final output\n    print(str(all_results).replace(\" \", \"\"))\n\nsolve()\n\n```", "id": "2456326"}, {"introduction": "一个势函数的最终考验是其在分子动力学模拟中的表现。这个练习揭示了神经网络势的对称性与基本物理守恒定律之间的深刻联系。通过使用一个故意违反平移和旋转不变性的势函数来模拟一个系统，你将直接观察到线动量和角动量的不守恒，从而深刻理解为何这些对称性不仅是数学上的要求，更是物理上的必需。[@problem_id:2456269]", "problem": "给定一个分子动力学（MD）的约化单位模型，其势能的构造方式类似于高维神经网络势（NNP），并带有一个小的、受控的、对完美平移和旋转不变性的违背，以模拟一个非完美不变的学习模型。所有量都应在没有物理量纲的约化单位中处理。考虑$N$个粒子，其位置为$\\mathbf{r}_i \\in \\mathbb{R}^3$，速度为$\\mathbf{v}_i \\in \\mathbb{R}^3$，质量为$m_i = 1$（对所有$i \\in \\{1,\\dots,N\\}$）。总势能为\n$$\nE(\\{\\mathbf{r}_i\\}_{i=1}^N) = \\sum_{i=1}^N \\left[g\\!\\left(S_i\\right) + \\varepsilon_t\\,\\boldsymbol{c}\\cdot \\mathbf{r}_i + \\varepsilon_r\\,\\mathbf{r}_i^\\mathsf{T}\\mathbf{Q}\\,\\mathbf{r}_i\\right],\n$$\n其中\n$$\nS_i = \\sum_{\\substack{j=1 \\\\ j\\neq i}}^N \\exp\\!\\left(-\\alpha\\left(\\|\\mathbf{r}_i - \\mathbf{r}_j\\| - r_0\\right)\\right), \\quad g(x) = a\\,x + b\\,\\tanh(x),\n$$\n且常数为\n$$\n\\alpha = 1.2,\\quad r_0 = 1.0,\\quad a = 0.5,\\quad b = 0.2,\n$$\n$$\n\\boldsymbol{c} = \\begin{bmatrix}0.3\\\\-0.2\\\\0.1\\end{bmatrix},\\quad\n\\mathbf{Q} = \\begin{bmatrix}0.3 & 0.1 & 0.0\\\\ 0.1 & -0.2 & 0.05\\\\ 0.0 & 0.05 & -0.1\\end{bmatrix}.\n$$\n令$\\mathbf{F}_i = -\\nabla_{\\mathbf{r}_i} E$为粒子$i$上的力。运动方程为牛顿方程\n$$\n\\frac{d\\mathbf{r}_i}{dt} = \\mathbf{v}_i,\\qquad \\frac{d\\mathbf{v}_i}{dt} = \\mathbf{F}_i.\n$$\n你必须使用以下针对时间步$\\Delta t$和$K$个步长的离散时间更新来演化系统：\n1. 设置加速度$\\mathbf{a}_i^{(n)} = \\mathbf{F}_i(\\{\\mathbf{r}_k^{(n)}\\}_{k=1}^N)$。\n2. 更新位置\n$$\n\\mathbf{r}_i^{(n+1)} = \\mathbf{r}_i^{(n)} + \\Delta t\\,\\mathbf{v}_i^{(n)} + \\tfrac{1}{2}\\,\\Delta t^2\\,\\mathbf{a}_i^{(n)}.\n$$\n3. 评估新的加速度$\\mathbf{a}_i^{(n+1)} = \\mathbf{F}_i(\\{\\mathbf{r}_k^{(n+1)}\\}_{k=1}^N)$。\n4. 更新速度\n$$\n\\mathbf{v}_i^{(n+1)} = \\mathbf{v}_i^{(n)} + \\tfrac{1}{2}\\,\\Delta t\\,\\left(\\mathbf{a}_i^{(n)} + \\mathbf{a}_i^{(n+1)}\\right).\n$$\n将第$n$步时相对于原点的总线动量和总角动量定义为\n$$\n\\mathbf{P}^{(n)} = \\sum_{i=1}^N \\mathbf{v}_i^{(n)},\\qquad\n\\mathbf{L}^{(n)} = \\sum_{i=1}^N \\mathbf{r}_i^{(n)} \\times \\mathbf{v}_i^{(n)}.\n$$\n对于下述每个测试用例，从指定的初始条件开始，以时间步长$\\Delta t$将系统演化$K$步，并计算漂移幅度\n$$\np_{\\mathrm{drift}} = \\left\\|\\mathbf{P}^{(K)} - \\mathbf{P}^{(0)}\\right\\|_2,\\qquad\n\\ell_{\\mathrm{drift}} = \\left\\|\\mathbf{L}^{(K)} - \\mathbf{L}^{(0)}\\right\\|_2.\n$$\n所有输出都必须以约化单位报告，并四舍五入到$6$位小数。\n\n测试套件（每行指定一个用例$[N,\\{\\mathbf{r}_i(0)\\}_{i=1}^N,\\{\\mathbf{v}_i(0)\\}_{i=1}^N,\\varepsilon_t,\\varepsilon_r,\\Delta t,K]$）：\n- 用例 A（名义不变性，多粒子）：\n$[\\,3,\\ \\{(-0.8,0.0,0.0),\\ (0.4,0.692820323,0.0),\\ (0.4,-0.692820323,0.0)\\},\\ \\{(0.0,0.0,0.0),\\ (0.0,0.0,0.0),\\ (0.0,0.0,0.0)\\},\\ 0.0,\\ 0.0,\\ 0.002,\\ 5000\\,]$。\n- 用例 B（小的不变性泄漏，多粒子）：\n$[\\,3,\\ \\{(-0.8,0.0,0.0),\\ (0.4,0.692820323,0.0),\\ (0.4,-0.692820323,0.0)\\},\\ \\{(0.0,0.0,0.0),\\ (0.0,0.0,0.0),\\ (0.0,0.0,0.0)\\},\\ 0.001,\\ 0.0005,\\ 0.002,\\ 5000\\,]$。\n- 用例 C（带泄漏的单粒子边界情况）：\n$[\\,1,\\ \\{(0.3,0.4,0.1)\\},\\ \\{(0.0,0.0,0.0)\\},\\ 0.002,\\ 0.001,\\ 0.002,\\ 4000\\,]$。\n\n你的程序应产生单行输出，其中包含结果，格式为方括号括起来的逗号分隔列表：\n$[[p_A,\\ell_A],[p_B,\\ell_B],[p_C,\\ell_C]]$，其中$p_X$和$\\ell_X$是对应情况$X\\in\\{\\mathrm{A},\\mathrm{B},\\mathrm{C}\\}$的漂移幅度，每个值都以约化单位表示并四舍五入到6位小数。不允许有其他输出。", "solution": "该问题要求使用速度Verlet积分算法，对一个由自定义势能函数控制的、包含$N$个粒子的约化单位系统进行模拟。任务是计算在固定数量的时间步长内，三种不同测试用例的总线动量和角动量的漂移。该问题具有科学依据，提法明确，并提供了所有必要的参数和初始条件。它代表了一个有效的计算物理问题。\n\n解决方案步骤如下：\n1.  通过对势能$E$关于粒子位置$\\mathbf{r}_i$取负梯度，推导出每个粒子$i$上的力$\\mathbf{F}_i$的解析表达式。\n2.  实现一个数值函数，用于计算任何给定粒子位置构型$\\{\\mathbf{r}_i\\}$的这些力。\n3.  实现速度Verlet算法，以随时间演化系统的位置$\\{\\mathbf{r}_i\\}$和速度$\\{\\mathbf{v}_i\\}$。\n4.  对三个指定的测试用例中的每一个执行模拟。\n5.  计算问题陈述中定义的总线动量漂移幅度$p_{\\mathrm{drift}}$和总角动量漂移幅度$\\ell_{\\mathrm{drift}}$。\n\n首先，我们必须推导任意粒子$k$上的力$\\mathbf{F}_k = -\\nabla_{\\mathbf{r}_k} E$。势能$E$由下式给出：\n$$\nE(\\{\\mathbf{r}_i\\}) = \\sum_{i=1}^N \\left[g\\!\\left(S_i\\right) + \\varepsilon_t\\,\\boldsymbol{c}\\cdot \\mathbf{r}_i + \\varepsilon_r\\,\\mathbf{r}_i^\\mathsf{T}\\mathbf{Q}\\,\\mathbf{r}_i\\right]\n$$\n梯度$\\nabla_{\\mathbf{r}_k}$作用于每一项。包含$\\varepsilon_t$和$\\varepsilon_r$的项仅在$i=k$时有贡献，因为当$i \\neq k$时它们与$\\mathbf{r}_i$无关。这些“泄漏”项的梯度是直截了当的：\n$$\n\\nabla_{\\mathbf{r}_k} (\\varepsilon_t\\,\\boldsymbol{c}\\cdot \\mathbf{r}_k) = \\varepsilon_t \\boldsymbol{c}\n$$\n$$\n\\nabla_{\\mathbf{r}_k} (\\varepsilon_r\\,\\mathbf{r}_k^\\mathsf{T}\\mathbf{Q}\\,\\mathbf{r}_k) = \\varepsilon_r (\\mathbf{Q} + \\mathbf{Q}^\\mathsf{T})\\mathbf{r}_k = 2\\varepsilon_r \\mathbf{Q}\\,\\mathbf{r}_k\n$$\n这里我们利用了给定矩阵$\\mathbf{Q}$是对称的这一事实。\n\n多体项$\\sum_{i=1}^N g(S_i)$的梯度更为复杂。使用链式法则：\n$$\n\\nabla_{\\mathbf{r}_k} \\sum_{i=1}^N g(S_i) = \\sum_{i=1}^N \\frac{dg}{dS_i} \\nabla_{\\mathbf{r}_k} S_i = \\sum_{i=1}^N g'(S_i) \\nabla_{\\mathbf{r}_k} S_i\n$$\n梯度$\\nabla_{\\mathbf{r}_k} S_i$仅在$S_i$依赖于$\\mathbf{r}_k$时非零。$S_i = \\sum_{j\\neq i} \\exp(-\\alpha(\\|\\mathbf{r}_i - \\mathbf{r}_j\\| - r_0)) = \\sum_{j\\neq i} f_{ij}$。当$i=k$或求和索引$j$之一等于$k$时，会出现这种依赖关系。\n\n对于$i=k$：\n$$\n\\nabla_{\\mathbf{r}_k} S_k = \\nabla_{\\mathbf{r}_k} \\sum_{j\\neq k} f_{kj} = \\sum_{j\\neq k} f_{kj} (-\\alpha) \\nabla_{\\mathbf{r}_k} \\|\\mathbf{r}_k - \\mathbf{r}_j\\| = \\sum_{j\\neq k} \\left(-\\alpha f_{kj} \\frac{\\mathbf{r}_k - \\mathbf{r}_j}{\\|\\mathbf{r}_k - \\mathbf{r}_j\\|}\\right)\n$$\n对于$i \\neq k$：$S_i$求和中唯一依赖于$\\mathbf{r}_k$的项是$f_{ik}$。\n$$\n\\nabla_{\\mathbf{r}_k} S_i = \\nabla_{\\mathbf{r}_k} f_{ik} = f_{ik} (-\\alpha) \\nabla_{\\mathbf{r}_k} \\|\\mathbf{r}_i - \\mathbf{r}_k\\| = f_{ik} (-\\alpha) \\frac{\\mathbf{r}_k - \\mathbf{r}_i}{\\|\\mathbf{r}_i - \\mathbf{r}_k\\|} = \\alpha f_{ik} \\frac{\\mathbf{r}_i - \\mathbf{r}_k}{\\|\\mathbf{r}_i - \\mathbf{r}_k\\|}\n$$\n结合这些结果，能量的对称部分的梯度为：\n$$\n\\nabla_{\\mathbf{r}_k} \\left(\\sum_i g(S_i)\\right) = g'(S_k) \\nabla_{\\mathbf{r}_k} S_k + \\sum_{i\\neq k} g'(S_i) \\nabla_{\\mathbf{r}_k} S_i\n$$\n代入梯度的表达式，并将第二项中的求和索引$i$重新标记为$j$可得：\n$$\n= g'(S_k) \\sum_{j\\neq k} \\left(-\\alpha f_{kj} \\frac{\\mathbf{r}_k - \\mathbf{r}_j}{r_{kj}}\\right) + \\sum_{j\\neq k} g'(S_j) \\left(\\alpha f_{jk} \\frac{\\mathbf{r}_j - \\mathbf{r}_k}{r_{jk}}\\right)\n$$\n其中$r_{kj} = \\|\\mathbf{r}_k-\\mathbf{r}_j\\|$。由于$f_{kj}=f_{jk}$，$r_{kj}=r_{jk}$，且$\\mathbf{r}_j-\\mathbf{r}_k = -(\\mathbf{r}_k-\\mathbf{r}_j)$，这可以简化为：\n$$\n= -\\alpha \\sum_{j\\neq k} (g'(S_k) + g'(S_j)) f_{kj} \\frac{\\mathbf{r}_k - \\mathbf{r}_j}{r_{kj}}\n$$\n$g(x) = a\\,x + b\\,\\tanh(x)$的导数是$g'(x) = a + b\\,\\mathrm{sech}^2(x) = a+b(1-\\tanh^2(x))$。\n\n粒子$k$上的总力是总梯度的负值：\n$$\n\\mathbf{F}_k = \\alpha \\sum_{j\\neq k} \\left(g'(S_k) + g'(S_j)\\right) f_{kj} \\frac{\\mathbf{r}_k - \\mathbf{r}_j}{r_{kj}} - \\varepsilon_t\\boldsymbol{c} - 2\\varepsilon_r\\mathbf{Q}\\mathbf{r}_k\n$$\n对于单粒子情况（$N=1$），对$j\\neq k$的求和为空，因此$S_1=0$。力简化为$\\mathbf{F}_1 = - \\varepsilon_t \\boldsymbol{c} - 2\\varepsilon_r \\mathbf{Q} \\mathbf{r}_1$。\n\n动力学系统使用速度Verlet算法以时间步长$\\Delta t$演化$K$步。由于质量$m_i=1$，加速度为$\\mathbf{a}_i = \\mathbf{F}_i$。\n1.  初始化$\\mathbf{r}^{(0)}, \\mathbf{v}^{(0)}$。计算$\\mathbf{a}^{(0)} = \\mathbf{F}(\\{\\mathbf{r}_k^{(0)}\\})$。\n2.  对于$n=0, \\dots, K-1$：\n    a.  $\\mathbf{r}_i^{(n+1)} = \\mathbf{r}_i^{(n)} + \\Delta t\\,\\mathbf{v}_i^{(n)} + \\tfrac{1}{2}\\,\\Delta t^2\\,\\mathbf{a}_i^{(n)}$\n    b.  $\\mathbf{a}_i^{(n+1)} = \\mathbf{F}_i(\\{\\mathbf{r}_k^{(n+1)}\\})$\n    c.  $\\mathbf{v}_i^{(n+1)} = \\mathbf{v}_i^{(n)} + \\tfrac{1}{2}\\,\\Delta t\\,\\left(\\mathbf{a}_i^{(n)} + \\mathbf{a}_i^{(n+1)}\\right)$\n计算初始总线动量$\\mathbf{P}^{(0)} = \\sum_i \\mathbf{v}_i^{(0)}$和角动量$\\mathbf{L}^{(0)} = \\sum_i \\mathbf{r}_i^{(0)} \\times \\mathbf{v}_i^{(0)}$。$K$步之后，计算最终值$\\mathbf{P}^{(K)}$和$\\mathbf{L}^{(K)}$，并确定差值的欧几里得范数，$p_{\\mathrm{drift}} = \\|\\mathbf{P}^{(K)} - \\mathbf{P}^{(0)}\\|$和$\\ell_{\\mathrm{drift}} = \\|\\mathbf{L}^{(K)} - \\mathbf{L}^{(0)}\\|$。\n该实现使用 `numpy` 进行高效的力的矢量化计算和状态更新。\n\n-   用例 A ($\\varepsilon_t = 0, \\varepsilon_r = 0$)：势能对平移和旋转是完全不变的。预期线动量和角动量守恒，因此漂移应接近于零，仅受数值精度的限制。\n-   用例 B ($\\varepsilon_t > 0, \\varepsilon_r > 0$)：不变性被明确破坏。净力$\\sum_i \\mathbf{F}_i$和净力矩$\\sum_i \\mathbf{r}_i \\times \\mathbf{F}_i$非零，导致$\\mathbf{P}$和$\\mathbf{L}$出现长期漂移。预期会出现非零的漂移幅度。\n-   用例 C ($N=1$)：粒子受到外场作用。由于它从静止开始，它会加速，导致非零的最终动量和角动量，因此产生显著的漂移。\n\n提供的Python代码实现了这一逻辑，以解决这三种情况。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run simulations for all test cases and print results.\n    \"\"\"\n\n    def calculate_forces(r, N, alpha, r0, a, b, eps_t, eps_r, c_vec, Q_mat):\n        \"\"\"\n        Calculates the forces on all particles.\n        \"\"\"\n        if N == 1:\n            force = -eps_t * c_vec - 2 * eps_r * (Q_mat @ r[0])\n            return force.reshape(1, 3)\n\n        # Vectorized calculation for N > 1\n        # Pairwise differences and distances\n        diffs = r[:, np.newaxis, :] - r[np.newaxis, :, :]  # Shape (N, N, 3)\n        dists = np.linalg.norm(diffs, axis=2)  # Shape (N, N)\n        \n        # Avoid division by zero for diagonal elements, which are not used anyway\n        dists_safe = np.copy(dists)\n        np.fill_diagonal(dists_safe, 1.0)\n        unit_vectors = diffs / dists_safe[:, :, np.newaxis]\n\n        # Calculate exponential term f_ij\n        f_exp = np.exp(-alpha * (dists - r0))\n        np.fill_diagonal(f_exp, 0)  # Sum is over j != i\n\n        # Calculate S_i and g'(S_i)\n        S = np.sum(f_exp, axis=1)  # Shape (N,)\n        tanh_S = np.tanh(S)\n        g_prime_S = a + b * (1 - tanh_S**2)  # Shape (N,)\n\n        # Calculate symmetric part of the force\n        g_prime_sum = g_prime_S[:, np.newaxis] + g_prime_S[np.newaxis, :] # Shape (N, N)\n        term_scalar = alpha * g_prime_sum * f_exp  # Shape (N, N)\n        \n        force_sym_contrib = term_scalar[:, :, np.newaxis] * unit_vectors\n        force_sym = np.sum(force_sym_contrib, axis=1)\n\n        # Calculate leak part of the force\n        force_leak = -eps_t * c_vec - 2 * eps_r * (r @ Q_mat)\n\n        forces = force_sym + force_leak\n        return forces\n\n    def run_simulation(N, r_init, v_init, eps_t, eps_r, dt, K):\n        \"\"\"\n        Runs a single MD simulation for a given set of parameters.\n        \"\"\"\n        # Constants\n        alpha = 1.2\n        r0_const = 1.0\n        a = 0.5\n        b = 0.2\n        c_vec = np.array([0.3, -0.2, 0.1])\n        Q_mat = np.array([[0.3, 0.1, 0.0], \n                          [0.1, -0.2, 0.05], \n                          [0.0, 0.05, -0.1]])\n\n        # Initial state\n        r = np.array(r_init, dtype=float)\n        v = np.array(v_init, dtype=float)\n        \n        # Initial momentum and angular momentum (masses m_i = 1)\n        P0 = np.sum(v, axis=0) if N > 0 else np.zeros(3)\n        L0 = np.sum(np.cross(r, v), axis=0) if N > 0 else np.zeros(3)\n\n        # Initial acceleration\n        acc = calculate_forces(r, N, alpha, r0_const, a, b, eps_t, eps_r, c_vec, Q_mat)\n\n        # Main loop (Velocity Verlet)\n        for _ in range(K):\n            # Update positions\n            r += dt * v + 0.5 * dt**2 * acc\n            # Store old acceleration\n            acc_old = acc\n            # Calculate new acceleration\n            acc = calculate_forces(r, N, alpha, r0_const, a, b, eps_t, eps_r, c_vec, Q_mat)\n            # Update velocities\n            v += 0.5 * dt * (acc_old + acc)\n\n        # Final momentum and angular momentum\n        PK = np.sum(v, axis=0) if N > 0 else np.zeros(3)\n        LK = np.sum(np.cross(r, v), axis=0) if N > 0 else np.zeros(3)\n\n        # Calculate drifts\n        p_drift = np.linalg.norm(PK - P0)\n        l_drift = np.linalg.norm(LK - L0)\n\n        return p_drift, l_drift\n\n    # Test cases from the problem statement\n    test_cases = [\n        (3, [(-0.8, 0.0, 0.0), (0.4, 0.692820323, 0.0), (0.4, -0.692820323, 0.0)], \n            [(0.0, 0.0, 0.0), (0.0, 0.0, 0.0), (0.0, 0.0, 0.0)], \n            0.0, 0.0, 0.002, 5000), # Case A\n        (3, [(-0.8, 0.0, 0.0), (0.4, 0.692820323, 0.0), (0.4, -0.692820323, 0.0)], \n            [(0.0, 0.0, 0.0), (0.0, 0.0, 0.0), (0.0, 0.0, 0.0)], \n            0.001, 0.0005, 0.002, 5000), # Case B\n        (1, [(0.3, 0.4, 0.1)], \n            [(0.0, 0.0, 0.0)], \n            0.002, 0.001, 0.002, 4000) # Case C\n    ]\n    \n    results = []\n    for case in test_cases:\n        p_drift, l_drift = run_simulation(*case)\n        results.append((p_drift, l_drift))\n\n    formatted_results = [f\"[{p:.6f},{l:.6f}]\" for p, l in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "2456269"}]}