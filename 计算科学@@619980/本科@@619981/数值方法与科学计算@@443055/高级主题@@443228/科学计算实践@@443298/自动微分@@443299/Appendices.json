{"hands_on_practices": [{"introduction": "要真正理解自动微分，最好的方法莫过于亲手构建一个。本练习将指导您从零开始实现前向模式自动微分。我们将通过创建一个“对偶数” (Dual Number) 类来探索其核心思想，该类将一个数值与其导数值封装在一起。通过重载基本的算术运算符（如加、减、乘、除），您将能够让任何用这些运算符写成的函数在计算其自身值的同时，自动计算出其导数值，从而揭示出前向模式自动微分的精髓。[@problem_id:3207038]", "problem": "要求您在一种高级语言中使用运算符重载来实现前向模式自动微分（forward mode AD）。其核心思想是通过基本运算传播导数，从而跟踪表达式相对于单个标量输入的值和导数。您将创建一个类，该类表示一个由实数值及其相对于某个自变量的导数组成的数对。然后，您必须使用这个类在一组输入上计算一个特定多项式及其导数的值。实现不能依赖任何预构建的自动微分工具或符号操作库。\n\n实现一个类，其实例表示带有导数信息的数字。该类必须支持标准算术运算，以便在计算中使用该类时，可以自动计算值和导数。实现必须支持加法、减法、乘法、除法和整数幂运算。与内置数值类型的交互必须在运算符的任意一侧都能自然地进行。\n\n使用该类在多个输入点上计算多项式函数\n$$\nf(x) = 3x^{5} - 2x^{3} + 7x - 11\n$$\n及其导数。对于每个输入 $x$，您的程序应使用前向模式自动微分计算函数值 $f(x)$ 和导数 $f'(x)$，并使用普通浮点数计算和标准微积分计算相同两个量以进行验证。对于每个输入 $x$，计算绝对误差\n$$\ne_{\\text{val}} = \\lvert f_{\\text{AD}}(x) - f(x) \\rvert, \\quad e_{\\text{der}} = \\lvert f'_{\\text{AD}}(x) - f'(x) \\rvert,\n$$\n其中 $f_{\\text{AD}}(x)$ 和 $f'_{\\text{AD}}(x)$ 分别表示您的自动微分计算返回的值和导数。\n\n不涉及角度单位。不涉及物理单位。\n\n测试套件：\n- 在以下有序输入列表 $[0.0, 1.0, -1.0, 2.5, 10.0]$ 上进行评估。这些输入涵盖了边界输入 $0.0$、简单整数输入 $1.0$ 和 $-1.0$、非整数输入 $2.5$ 以及一个较大幅值的输入 $10.0$。\n\n程序要求：\n- 使用算术运算符一次性定义多项式 $f(x)$；无论 $x$ 是内置实数还是您的类的实例，这一定义都必须以相同的方式工作。\n- 对于指定顺序中的每个测试输入 $x$，创建一个表示导数为 $1.0$ 的自变量的实例，计算多项式以获得 $f_{\\text{AD}}(x)$ 和 $f'_{\\text{AD}}(x)$，使用普通浮点数算术计算同一多项式以获得 $f(x)$，使用标准微积分计算解析导数 $f'(x)$，并计算如上定义的 $e_{\\text{val}}$ 和 $e_{\\text{der}}$。\n- 按与输入相同的顺序收集结果。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表。每个元素必须是对应输入的双元素列表 $[e_{\\text{val}}, e_{\\text{der}}]$。例如，打印的行应类似于\n\"[ [e_val_for_x0, e_der_for_x0], [e_val_for_x1, e_der_for_x1], ... ]\"\n除了打印函数产生的空格外，没有其他空格要求，但它必须是单行，并以该语言的自然列表语法编码一个列表的列表。\n\n您的程序必须是自包含的，并且不得从用户或任何外部文件读取任何输入。所提供测试套件的结果必须是打印输出行的唯一内容。", "solution": "我们使用成熟的微分法则来推导针对单个标量输入的前向模式自动微分（自动微分 (AD)）。其基础是标准的微分法则：线性法则、乘法法则、除法法则、链式法则以及整数指数的幂法则。\n\n我们通过为每个中间量关联一个数对 $\\left(v, d\\right)$ 来表示对自变量 $x$ 的计算，其中 $v$ 是实数值，而 $d = \\frac{dv}{dx}$ 是关于 $x$ 的导数。现在我们直接从微积分推导如何通过算术运算传播这些数对。\n\n加法和减法：\n假设 $u = (u, u')$ 和 $v = (v, v')$ 表示两个关于 $x$ 的可微量。那么\n$$\n(u + v, \\, \\frac{d}{dx}(u+v)) = (u + v, \\, u' + v'),\n$$\n减法也类似，\n$$\n(u - v, \\, \\frac{d}{dx}(u-v)) = (u - v, \\, u' - v').\n$$\n这些都源于微分的线性性质。\n\n乘法：\n使用乘法法则，\n$$\n\\frac{d}{dx}(uv) = u'v + uv'.\n$$\n因此，对于数对 $(u,u')$ 和 $(v,v')$，其乘积变为\n$$\n(uv, \\, u'v + uv').\n$$\n\n除法：\n使用除法法则，\n$$\n\\frac{d}{dx}\\left(\\frac{u}{v}\\right) = \\frac{u'v - uv'}{v^{2}},\n$$\n因此传播后的数对是\n$$\n\\left(\\frac{u}{v}, \\, \\frac{u'v - uv'}{v^{2}}\\right),\n$$\n前提是 $v \\neq 0$。\n\n整数次幂：\n对于整数 $n$，根据幂法则和链式法则，\n$$\n\\frac{d}{dx}\\left(u^{n}\\right) = n u^{n-1} u'.\n$$\n因此传播后的数对是\n$$\n\\left(u^{n}, \\, n u^{n-1} u'\\right).\n$$\n对于一个实常数指数 $p$，当 $u > 0$ 时，相同的表达式 $\\left(u^{p}, \\, p u^{p-1} u'\\right)$ 成立，但对于本任务，由于目标函数是多项式，正确支持整数指数就足够了。\n\n前向模式 AD 实现：\n我们将每个量编码为一个存储两个实数 $(v,d)$ 的类实例。该类重载了算术运算符，以便当它参与表达式计算时，会应用上述传播公式。与内置实数的交互通过将实数 $c$ 视为数对 $(c, 0)$ 来处理。自变量 $x$ 表示为 $(x,1)$。\n\n目标函数：\n我们考虑多项式\n$$\nf(x) = 3x^{5} - 2x^{3} + 7x - 11.\n$$\n因为 $f$ 是一个多项式，当它用重载算术表示一次后，就能自动计算出值和导数。为了验证，我们使用从标准微积分中得到的其解析导数：\n$$\nf'(x) = 15x^{4} - 6x^{2} + 7.\n$$\n\n算法步骤：\n1. 定义一个类，其实例存储 $(v,d)$，并实现如上推导的重载运算符 $+$, $-$, $\\times$, $\\div$ 和整数次幂运算。\n2. 使用算术运算符一次性定义 $f(x)$。这一定义可以应用于内置实数或该类的实例。\n3. 对于每个测试输入 $x \\in \\{0.0, 1.0, -1.0, 2.5, 10.0\\}$，创建种子变量 $(x,1)$，计算 $f$ 以获得 $(f_{\\text{AD}}(x), f'_{\\text{AD}}(x))$，将 $f(x)$ 和 $f'(x)$ 作为普通浮点数计算进行求值，并计算\n$$\ne_{\\text{val}} = \\lvert f_{\\text{AD}}(x) - f(x) \\rvert, \\quad e_{\\text{der}} = \\lvert f'_{\\text{AD}}(x) - f'(x) \\rvert.\n$$\n4. 按指定顺序为每个 $x$ 收集数对 $[e_{\\text{val}}, e_{\\text{der}}]$，并将它们作为单个列表打印在同一行上。\n\n正确性：\n传播规则直接从微分的线性性质、乘法法则、除法法则和幂法则推导而来。由于多项式是通过重复应用这些基本运算构成的，因此前向模式 AD 计算在任何点 $x$ 处都能在浮点舍入误差范围内得出 $f$ 的精确解析导数。因此，在所有测试输入中，绝对误差 $e_{\\text{val}}$ 和 $e_{\\text{der}}$ 预计会非常小，通常接近于零，仅受浮点舍入误差影响。\n\n输出：\n程序打印单行，表示一个由双元素列表 $[e_{\\text{val}}, e_{\\text{der}}]$ 组成的列表，每个测试用例一个，顺序与输入相同。不产生其他输出。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\nclass Dual:\n    \"\"\"\n    Dual number for forward-mode automatic differentiation with respect to a single scalar variable.\n    Each instance represents a pair (value, derivative).\n    \"\"\"\n    __slots__ = (\"val\", \"der\")\n\n    def __init__(self, val, der=0.0):\n        self.val = float(val)\n        self.der = float(der)\n\n    @staticmethod\n    def _coerce(other):\n        if isinstance(other, Dual):\n            return other\n        else:\n            return Dual(other, 0.0)\n\n    # Addition\n    def __add__(self, other):\n        o = Dual._coerce(other)\n        return Dual(self.val + o.val, self.der + o.der)\n\n    def __radd__(self, other):\n        return self.__add__(other)\n\n    # Subtraction\n    def __sub__(self, other):\n        o = Dual._coerce(other)\n        return Dual(self.val - o.val, self.der - o.der)\n\n    def __rsub__(self, other):\n        o = Dual._coerce(other)\n        return Dual(o.val - self.val, o.der - self.der)\n\n    # Multiplication\n    def __mul__(self, other):\n        o = Dual._coerce(other)\n        # (u, u') * (v, v') = (uv, u'v + uv')\n        return Dual(self.val * o.val, self.der * o.val + self.val * o.der)\n\n    def __rmul__(self, other):\n        return self.__mul__(other)\n\n    # True division\n    def __truediv__(self, other):\n        o = Dual._coerce(other)\n        # (u, u') / (v, v') = (u/v, (u'v - uv')/v^2)\n        denom = o.val * o.val\n        return Dual(self.val / o.val, (self.der * o.val - self.val * o.der) / denom)\n\n    def __rtruediv__(self, other):\n        o = Dual._coerce(other)\n        # o / self\n        denom = self.val * self.val\n        return Dual(o.val / self.val, (o.der * self.val - o.val * self.der) / denom)\n\n    # Unary negation\n    def __neg__(self):\n        return Dual(-self.val, -self.der)\n\n    # Power: support real (int/float) exponents, commonly used for integer exponents in polynomials\n    def __pow__(self, power):\n        if isinstance(power, (int, float)):\n            if power == 0:\n                # x**0 = 1, derivative 0\n                return Dual(1.0, 0.0)\n            # For real power p: d(x**p) = p * x**(p - 1) * dx\n            primal = self.val ** power\n            deriv = power * (self.val ** (power - 1.0)) * self.der\n            return Dual(primal, deriv)\n        else:\n            raise TypeError(\"Power must be a real number for Dual.__pow__\")\n\ndef poly(x):\n    # f(x) = 3x^5 - 2x^3 + 7x - 11\n    return 3 * (x ** 5) - 2 * (x ** 3) + 7 * x - 11\n\ndef poly_float(x):\n    return 3.0 * (x ** 5) - 2.0 * (x ** 3) + 7.0 * x - 11.0\n\ndef poly_derivative_float(x):\n    # f'(x) = 15x^4 - 6x^2 + 7\n    return 15.0 * (x ** 4) - 6.0 * (x ** 2) + 7.0\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [0.0, 1.0, -1.0, 2.5, 10.0]\n\n    results = []\n    for x in test_cases:\n        # Seed the independent variable: derivative w.r.t. x is 1\n        dx = Dual(x, 1.0)\n        y = poly(dx)              # Dual result: (value, derivative)\n        f_val = poly_float(x)     # Float value\n        f_der = poly_derivative_float(x)  # Analytical derivative\n\n        val_err = abs(y.val - f_val)\n        der_err = abs(y.der - f_der)\n\n        results.append([val_err, der_err])\n\n    # Final print statement in the exact required format: a single line list of [val_err, der_err] pairs.\n    # Format with reasonable precision for readability.\n    formatted = \"[\" + \",\".join(f\"[{val:.12g},{der:.12g}]\" for val, der in results) + \"]\"\n    print(formatted)\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3207038"}, {"introduction": "在掌握了前向模式之后，我们将转向其功能更强大但概念上稍显复杂的“兄弟”——反向模式自动微分，它也是驱动现代深度学习框架（如 PyTorch 和 TensorFlow）的核心引擎。为了揭开其神秘面纱，本练习将引导您进行一次“纸上推演”。您将为一个给定的多元函数手动构建其计算图（即“磁带”），然后遵循链式法则，一步步地反向传播梯度，亲身体验反向模式如何高效地计算一个标量输出对所有输入的梯度。[@problem_id:3100431]", "problem": "考虑深度学习背景下的反向模式自动微分（AD），其中标量损失函数关于参数的梯度是通过使用链式法则反向遍历计算图来高效计算的。我们感兴趣的函数是标量映射 $f:\\mathbb{R}^{2}\\to\\mathbb{R}$，由 $f(x,y)=\\sin(xy)+\\frac{\\exp(x)}{y}$ 给出，其中 $y\\neq 0$。仅使用与计算图兼容的基本运算（乘法、正弦、指数和除法），构建一个用于评估 $f(x,y)$ 的最小中间变量集，并创建一个记录这些运算的父子关系的计算记录（tape）。然后，利用复合函数的链式法则原理和向量-雅可比积（VJP）的概念，手动推导获得梯度 $\\nabla f(x,y)$ 所需的反向传播（VJP 拉回）的确切序列。你的推导应清楚地标明反向遍历计算记录的顺序以及每一步中对输入伴随变量的局部贡献。以行向量的形式给出 $\\nabla f(x,y)$ 的最终解析表达式。不要四舍五入；最终答案必须是精确的符号表达式。", "solution": "问题陈述被认为是有效的。它具有科学依据，定义明确，客观，并包含足够的信息来推导出唯一且有意义的解。该任务涉及将反向模式自动微分（AD）应用于一个可微函数，AD是计算微积分和深度学习中的一个基石算法。该过程是可形式化的，并与既定原则相符。\n\n我们的任务是使用反向模式AD的原理，计算函数 $f:\\mathbb{R}^{2}\\to\\mathbb{R}$（由 $f(x,y)=\\sin(xy)+\\frac{\\exp(x)}{y}$ 给出，其中 $y \\neq 0$）的梯度 $\\nabla f(x,y)$。这包括一个构建计算图并评估函数的前向传播过程，以及一个传播梯度的反向传播过程。\n\n首先，我们将函数分解为一系列基本运算。这个序列定义了计算图，或称为“计算记录”（tape）。设输入为 $v_1 = x$ 和 $v_2 = y$。\n\n**前向传播：构建计算图**\n\n$f(x,y)$ 的评估可以由以下最小中间变量集表示：\n1.  $v_3 = v_1 \\cdot v_2 = x \\cdot y$\n2.  $v_4 = \\sin(v_3) = \\sin(xy)$\n3.  $v_5 = \\exp(v_1) = \\exp(x)$\n4.  $v_6 = \\frac{v_5}{v_2} = \\frac{\\exp(x)}{y}$\n5.  $v_7 = v_4 + v_6 = \\sin(xy) + \\frac{\\exp(x)}{y} = f(x,y)$\n\n这个序列构成了前向传播。计算记录（tape）记录了这些操作及其依赖关系：$(v_3, \\text{mul}, v_1, v_2)$, $(v_4, \\sin, v_3)$, $(v_5, \\exp, v_1)$, $(v_6, \\text{div}, v_5, v_2)$, $(v_7, \\text{add}, v_4, v_6)$。\n\n**反向传播：使用链式法则计算梯度**\n\n反向传播计算最终输出 $v_7$ 相对于每个中间变量 $v_i$ 的偏导数，这些偏导数表示为伴随变量 $\\bar{v}_i = \\frac{\\partial f}{\\partial v_i} = \\frac{\\partial v_7}{\\partial v_i}$。该过程从初始化输出节点的伴随变量为 $1$ 开始，即 $\\bar{v}_7 = \\frac{\\partial v_7}{\\partial v_7} = 1$。所有其他伴随变量初始化为 $0$。然后我们按反向拓扑顺序遍历该图。\n\n核心原理是链式法则。对于一个运算 $v_k = g(v_i, v_j, \\dots)$，其父节点的伴随变量通过累加子节点的伴随变量乘以局部偏导数来更新：\n$$ \\bar{v}_i = \\bar{v}_i + \\bar{v}_k \\frac{\\partial v_k}{\\partial v_i} $$\n$$ \\bar{v}_j = \\bar{v}_j + \\bar{v}_k \\frac{\\partial v_k}{\\partial v_j} $$\n... 以此类推。这个操作实际上是一个向量-雅可比积（VJP）拉回。\n\n让我们按照前向传播的逆序计算伴随变量：\n\n1.  **开始：** 初始化伴随变量：$\\bar{v}_1=0, \\bar{v}_2=0, \\bar{v}_3=0, \\bar{v}_4=0, \\bar{v}_5=0, \\bar{v}_6=0$。\n    设置反向传播的种子：$\\bar{v}_7 = 1$。\n\n2.  **节点 $v_7 = v_4 + v_6$：**\n    父节点是 $v_4$ 和 $v_6$。\n    局部偏导数：$\\frac{\\partial v_7}{\\partial v_4} = 1$，$\\frac{\\partial v_7}{\\partial v_6} = 1$。\n    更新父节点伴随变量：\n    $\\bar{v}_4 \\leftarrow \\bar{v}_4 + \\bar{v}_7 \\cdot \\frac{\\partial v_7}{\\partial v_4} = 0 + 1 \\cdot 1 = 1$。\n    $\\bar{v}_6 \\leftarrow \\bar{v}_6 + \\bar{v}_7 \\cdot \\frac{\\partial v_7}{\\partial v_6} = 0 + 1 \\cdot 1 = 1$。\n    当前状态：$\\bar{v}_4=1, \\bar{v}_6=1$。\n\n3.  **节点 $v_6 = \\frac{v_5}{v_2}$：**\n    父节点是 $v_5$ 和 $v_2$。\n    局部偏导数：$\\frac{\\partial v_6}{\\partial v_5} = \\frac{1}{v_2}$，$\\frac{\\partial v_6}{\\partial v_2} = -\\frac{v_5}{v_2^2}$。\n    更新父节点伴随变量：\n    $\\bar{v}_5 \\leftarrow \\bar{v}_5 + \\bar{v}_6 \\cdot \\frac{\\partial v_6}{\\partial v_5} = 0 + 1 \\cdot \\frac{1}{v_2} = \\frac{1}{y}$。\n    $\\bar{v}_2 \\leftarrow \\bar{v}_2 + \\bar{v}_6 \\cdot \\frac{\\partial v_6}{\\partial v_2} = 0 + 1 \\cdot \\left(-\\frac{v_5}{v_2^2}\\right) = -\\frac{\\exp(x)}{y^2}$。\n    当前状态：$\\bar{v}_5 = \\frac{1}{y}$，$\\bar{v}_2=-\\frac{\\exp(x)}{y^2}$。\n\n4.  **节点 $v_5 = \\exp(v_1)$：**\n    父节点是 $v_1$。\n    局部偏导数：$\\frac{\\partial v_5}{\\partial v_1} = \\exp(v_1)$。\n    更新父节点伴随变量：\n    $\\bar{v}_1 \\leftarrow \\bar{v}_1 + \\bar{v}_5 \\cdot \\frac{\\partial v_5}{\\partial v_1} = 0 + \\frac{1}{y} \\cdot \\exp(v_1) = \\frac{\\exp(x)}{y}$。\n    当前状态：$\\bar{v}_1 = \\frac{\\exp(x)}{y}$。\n\n5.  **节点 $v_4 = \\sin(v_3)$：**\n    父节点是 $v_3$。\n    局部偏导数：$\\frac{\\partial v_4}{\\partial v_3} = \\cos(v_3)$。\n    更新父节点伴随变量：\n    $\\bar{v}_3 \\leftarrow \\bar{v}_3 + \\bar{v}_4 \\cdot \\frac{\\partial v_4}{\\partial v_3} = 0 + 1 \\cdot \\cos(v_3) = \\cos(xy)$。\n    当前状态：$\\bar{v}_3 = \\cos(xy)$。\n\n6.  **节点 $v_3 = v_1 \\cdot v_2$：**\n    父节点是 $v_1$ 和 $v_2$。注意 $v_1$ 和 $v_2$ 已经从其他路径接收了梯度；我们累加新的贡献。\n    局部偏导数：$\\frac{\\partial v_3}{\\partial v_1} = v_2$, $\\frac{\\partial v_3}{\\partial v_2} = v_1$。\n    更新父节点伴随变量：\n    $\\bar{v}_1 \\leftarrow \\bar{v}_1 + \\bar{v}_3 \\cdot \\frac{\\partial v_3}{\\partial v_1} = \\frac{\\exp(x)}{y} + \\cos(xy) \\cdot v_2 = \\frac{\\exp(x)}{y} + y \\cos(xy)$。\n    $\\bar{v}_2 \\leftarrow \\bar{v}_2 + \\bar{v}_3 \\cdot \\frac{\\partial v_3}{\\partial v_2} = -\\frac{\\exp(x)}{y^2} + \\cos(xy) \\cdot v_1 = -\\frac{\\exp(x)}{y^2} + x \\cos(xy)$。\n\n当所有输入节点的伴随变量都计算完毕后，过程终止。\n最终的梯度是输入变量伴随变量的最终值：\n$$ \\frac{\\partial f}{\\partial x} = \\bar{v}_1 = y \\cos(xy) + \\frac{\\exp(x)}{y} $$\n$$ \\frac{\\partial f}{\\partial y} = \\bar{v}_2 = x \\cos(xy) - \\frac{\\exp(x)}{y^2} $$\n\n梯度向量 $\\nabla f(x,y)$ 是这些偏导数组成的行向量：\n$$ \\nabla f(x,y) = \\begin{pmatrix} \\frac{\\partial f}{\\partial x}  \\frac{\\partial f}{\\partial y} \\end{pmatrix} = \\begin{pmatrix} y \\cos(xy) + \\frac{\\exp(x)}{y}  x \\cos(xy) - \\frac{\\exp(x)}{y^2} \\end{pmatrix} $$\n此推导严格遵循了反向模式自动微分的机械步骤。", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\ny \\cos(xy) + \\frac{\\exp(x)}{y}  x \\cos(xy) - \\frac{\\exp(x)}{y^2}\n\\end{pmatrix}\n}\n$$", "id": "3100431"}, {"introduction": "理论上的数学公式在计算机的有限精度世界中可能表现出意想不到的行为。本练习将引导我们从理论走向实践，探讨自动微分中一个至关重要的方面：数值稳定性。我们将聚焦于倒数函数 $y = 1/x$，通过比较其导数的“朴素”实现与“稳定”实现，您将发现，即使是代数上等价的公式，在面对接近零等极端输入时，其数值精度也可能天差地别。这个实践将教会您在编写科学计算代码时，不仅要关注数学上的正确性，更要警惕数值计算中的陷阱。[@problem_id:3207130]", "problem": "使用前向模式和反向模式，为标量倒数函数实现一个小型自动微分系统。仅从以下基础出发：将导数定义为链式法则给出的线性映射、使用满足 $\\varepsilon^2 = 0$ 的无穷小量 $\\varepsilon$ 的前向模式对偶数模型，以及反向模式的伴随（也称敏感度）传播观点，推导倒数节点的数值稳定更新规则，并在一系列输入（包括接近零的值）上测量梯度精度。\n\n您的任务是：\n\n1) 推导倒数节点的前向模式规则，其中原值为 $y = 1/x$。使用对偶数定义，即前向模式值表示为 $x + \\dot{x}\\,\\varepsilon$，且其复合遵循链式法则。以此为基础，推导切线值 $\\dot{y}$ 的更新，使其以原输出值而非重新计算输入值的幂来表示。这应产生一个重用已计算的 $y$ 的公式，以形成一个数值稳定的更新。\n\n2) 推导倒数节点的反向模式规则。在反向模式中，定义 $y$ 的伴随（敏感度）$\\bar{y}$，并展示如何通过一个依赖于原值的拉回（pullback）将其传播到输入敏感度 $\\bar{x}$。使用原输出值 $y$ 来表示该更新。\n\n3) 在程序中实现四种规则：\n- 前向模式朴素倒数：直接将 $\\dot{y}$ 作为 $x$ 的函数进行传播。\n- 前向模式稳定倒数：尽可能仅使用已计算的原输出值来传播 $\\dot{y}$。\n- 反向模式朴素倒数：直接将 $\\bar{x}$ 作为 $x$ 的函数进行传播。\n- 反向模式稳定倒数：尽可能仅使用已计算的原输出值来传播 $\\bar{x}$。\n\n4) 验证方法。对于给定集合中的每个 $x$，使用微积分基本定义计算倒数节点的解析导数，并将其与四种自动微分实现所产生的导数进行比较。使用相对误差，对每个 $x$ 定义为\n$$\n\\mathrm{rel\\_err}(x) \\;=\\; \\frac{\\lvert g_{\\mathrm{AD}}(x) - g_{\\mathrm{exact}}(x)\\rvert}{\\lvert g_{\\mathrm{exact}}(x)\\rvert},\n$$\n只要分母是有限且非零。在误差聚合中，忽略任何会产生非有限精确导数或非有限自动微分导数的 $x$。\n\n5) 测试套件。在以下三个输入集上评估并聚合最大相对误差，这些输入集旨在覆盖典型范围、近零输入以及接近双精度下不应溢出的最大有限导数的边界值：\n- 情况 A (一般范围): $[-123.45,\\,-0.0314159,\\,-10^{-8},\\,10^{-8},\\,0.0314159,\\,123.45]$。\n- 情况 B (近零但有限的导数): $[1.2345\\times 10^{-120},\\,-4.321\\times 10^{-110},\\,7.89\\times 10^{-130},\\,-9.99\\times 10^{-140},\\,5.0\\times 10^{-150},\\,-8.0\\times 10^{-150}]$。\n- 情况 C (导数接近溢出但仍为有限的边界值): $[1.5\\times 10^{-154},\\,-1.5\\times 10^{-154},\\,2.2\\times 10^{-154},\\,-2.2\\times 10^{-154}]$。\n\n对于每种情况，计算四个聚合指标：\n- 前向模式朴素法的最大相对误差。\n- 前向模式稳定法的最大相对误差。\n- 反向模式朴素法的最大相对误差。\n- 反向模式稳定法的最大相对误差。\n\n6) 最终输出格式。您的程序应生成单行输出，其中包含所有十二个结果，按情况 A、B、C 的顺序排列，每种情况贡献上述四个指标，形式为方括号内由逗号分隔的列表。顺序是：\n$[\\mathrm{A\\_FwdNaive},\\mathrm{A\\_FwdStab},\\mathrm{A\\_RevNaive},\\mathrm{A\\_RevStab},\\mathrm{B\\_FwdNaive},\\mathrm{B\\_FwdStab},\\mathrm{B\\_RevNaive},\\mathrm{B\\_RevStab},\\mathrm{C\\_FwdNaive},\\mathrm{C\\_FwdStab},\\mathrm{C\\_RevNaive},\\mathrm{C\\_RevStab}]$。\n仅作为格式示例，形如 $[r_1,r_2,r_3,r_4,r_5,r_6,r_7,r_8,r_9,r_{10},r_{11},r_{12}]$ 的列表是可以接受的。您的程序不得读取输入，且必须使用双精度算术计算并打印这十二个值。不涉及物理单位。所有角度（如有）必须以弧度为单位；但是，此任务中不需要三角函数。", "solution": "该问题要求为标量倒数函数 $y = f(x) = 1/x$ 推导并实现四种不同的自动微分 (AD) 规则。这些规则分为前向和反向 AD 模式的“朴素”和“稳定”两类。目标是比较它们的数值精度，特别是对于挑战双精度浮点算术极限的输入。\n\n### 1. 准备工作：解析导数\n\n所考虑的函数是倒数函数：\n$$\ny = f(x) = \\frac{1}{x} = x^{-1}\n$$\n解析导数通过微分的幂法则获得，它将作为我们进行精度验证的基准事实：\n$$\ng_{\\mathrm{exact}}(x) = \\frac{dy}{dx} = -1 \\cdot x^{-2} = -\\frac{1}{x^2}\n$$\n问题的核心是评估 AD 方法能多接近地逼近这个精确导数 $g_{\\mathrm{exact}}(x)$。\n\n### 2. 前向模式自动微分\n\n前向模式 AD 基于导数在计算图中的前向传播。我们使用对偶数表示，其中一个数 $v$ 及其关于某个自变量的导数 $\\dot{v}$ 被组合成一个单一实体 $v + \\dot{v}\\varepsilon$，并具有属性 $\\varepsilon^2 = 0$。\n\n设输入由对偶数 $x + \\dot{x}\\varepsilon$ 表示。我们希望找到输出对应的对偶数 $y + \\dot{y}\\varepsilon$。\n$$\ny + \\dot{y}\\varepsilon = f(x + \\dot{x}\\varepsilon) = \\frac{1}{x + \\dot{x}\\varepsilon}\n$$\n为了分离原值 ($y$) 和切线值 ($\\dot{y}$) 分量，我们进行一个类似于一阶泰勒展开的代数操作：\n$$\n\\frac{1}{x + \\dot{x}\\varepsilon} = \\frac{1}{x(1 + \\frac{\\dot{x}}{x}\\varepsilon)} = \\frac{1}{x} \\left(1 + \\frac{\\dot{x}}{x}\\varepsilon\\right)^{-1}\n$$\n使用几何级数展开 $(1+u)^{-1} = 1 - u + u^2 - \\dots$ 其中 $u = \\frac{\\dot{x}}{x}\\varepsilon$，并利用 $\\varepsilon^2=0$ 的性质将级数截断到线性项：\n$$\n\\frac{1}{x} \\left(1 - \\frac{\\dot{x}}{x}\\varepsilon + \\mathcal{O}(\\varepsilon^2)\\right) = \\frac{1}{x} - \\frac{\\dot{x}}{x^2}\\varepsilon\n$$\n通过将此结果与 $y + \\dot{y}\\varepsilon$ 的形式进行比较，我们提取出更新规则：\n- 原值更新：$y = \\frac{1}{x}$\n- 切线值更新：$\\dot{y} = -\\frac{\\dot{x}}{x^2}$\n\n由此，我们推导出所需的两个前向模式规则。为了计算导数 $f'(x)$，我们设置种子切线值 $\\dot{x}=1$。\n\n**前向模式朴素规则：**\n该规则通过直接使用输入原值 $x$ 来计算切线值来实现。\n$$\n\\dot{y} = -\\frac{1}{x^2}\n$$\n此规则需要计算 $x^2$ 然后执行一次除法。\n\n**前向模式稳定规则：**\n此规则的构建是为了重用已计算的原输出值 $y = 1/x$。简单地代入切线更新公式可得：\n$$\n\\dot{y} = -\\frac{\\dot{x}}{x^2} = -\\dot{x} \\left(\\frac{1}{x}\\right)^2 = -\\dot{x} y^2\n$$\n当 $\\dot{x}=1$ 时，规则变为：\n$$\n\\dot{y} = -y^2\n$$\n这个变体先计算 $y=1/x$，然后对结果求平方，从而避免了涉及输入 $x$ 的重新计算。\n\n### 3. 反向模式自动微分\n\n反向模式 AD 将敏感度（或称伴随）从计算的输出向后传播到其输入。变量 $v$ 的伴随，记为 $\\bar{v}$，定义为最终标量目标函数 $L$ 相对于 $v$ 的偏导数，即 $\\bar{v} = \\frac{\\partial L}{\\partial v}$。\n\n对于函数 $y=f(x)$，伴随的链式法则将输入伴随 $\\bar{x}$ 与输出伴随 $\\bar{y}$ 联系起来：\n$$\n\\bar{x} = \\frac{\\partial L}{\\partial x} = \\frac{\\partial L}{\\partial y} \\frac{dy}{dx} = \\bar{y} \\frac{dy}{dx}\n$$\n项 $\\frac{dy}{dx}$ 是节点的局部偏导数。在我们的例子中，$\\frac{dy}{dx} = -1/x^2$。将其代入伴随传播规则得到：\n$$\n\\bar{x} = \\bar{y} \\left(-\\frac{1}{x^2}\\right)\n$$\n为了求函数 $f(x)$ 本身的导数，我们可以将目标函数概念化为函数输出，即 $L=y$。在这种情况下，种子伴随为 $\\bar{y} = \\frac{\\partial L}{\\partial y} = \\frac{\\partial y}{\\partial y} = 1$。由此产生的输入伴随 $\\bar{x}$ 即为 $\\frac{dL}{dx} = \\frac{dy}{dx}$。\n\n**反向模式朴素规则：**\n该更新直接用输入原值 $x$ 表示。当 $\\bar{y}=1$ 时：\n$$\n\\bar{x} = -\\frac{1}{x^2}\n$$\n这个计算规则与朴素前向模式规则相同。\n\n**反向模式稳定规则：**\n此规则利用了原值 $y = 1/x$，该值是在任何反向模式 AD 系统中反向传播之前必须执行的前向传播过程中计算的。\n$$\n\\bar{x} = \\bar{y} \\left(-\\frac{1}{x^2}\\right) = -\\bar{y} \\left(\\frac{1}{x}\\right)^2 = -\\bar{y} y^2\n$$\n当 $\\bar{y}=1$ 时，规则变为：\n$$\n\\bar{x} = -y^2\n$$\n这个计算规则与稳定前向模式规则相同。区别在于 AD 的概念框架，但执行的浮点运算是相同的。\n\n### 4. 验证与实现\n\n将推导出的四种规则实现为不同的函数。它们的数值精度将与解析导数 $g_{\\mathrm{exact}}(x) = -1/x^2$ 进行比较评估。比较的度量是每个输入值 $x$ 的相对误差：\n$$\n\\mathrm{rel\\_err}(x) = \\frac{\\lvert g_{\\mathrm{AD}}(x) - g_{\\mathrm{exact}}(x)\\rvert}{\\lvert g_{\\mathrm{exact}}(x)\\rvert}\n$$\n测试协议要求对每个规则，在指定的输入集上聚合最大相对误差。任何导致 AD 计算的导数或精确导数出现非有限值的输入 $x$ 都将从误差计算中排除，同样被排除的还有精确导数为零的情况（对于有限非零的 $x$ 不会发生）。实现将使用 `np.double` 来强制执行双精度算术，并使用 `np.isfinite` 进行有效性检查。\n\n“朴素”和“稳定”实现之间的区别在于浮点运算的顺序。\n- 朴素：`g = -1.0 / (x * x)`\n- 稳定：`y = 1.0 / x`, `g = -y * y`\n尽管在代数上等价，但由于舍入误差，这两个操作序列可能会产生不同的结果，而本实验旨在测量的正是这种差异的大小，这是数值分析中的一个基本概念。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Derives and compares four AD rules for the reciprocal function y = 1/x.\n    The rules are: forward naive, forward stabilized, reverse naive, and reverse stabilized.\n    Their numerical accuracy is tested against the analytical derivative on three sets of inputs.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = {\n        'A': np.array([-123.45, -0.0314159, -1e-8, 1e-8, 0.0314159, 123.45], dtype=np.double),\n        'B': np.array([1.2345e-120, -4.321e-110, 7.89e-130, -9.99e-140, 5.0e-150, -8.0e-150], dtype=np.double),\n        'C': np.array([1.5e-154, -1.5e-154, 2.2e-154, -2.2e-154], dtype=np.double)\n    }\n\n    # 1. Analytical derivative (ground truth)\n    def g_exact(x: np.double) - np.double:\n        \"\"\"Computes the exact derivative of 1/x, which is -1/x^2.\"\"\"\n        return np.double(-1.0) / (x * x)\n\n    # 3. Implementations of the four AD rules\n    def forward_naive(x: np.double) - np.double:\n        \"\"\"Forward mode, naive rule: computes derivative from input x.\"\"\"\n        x_dot = np.double(1.0)\n        # Derivative is computed directly as -x_dot / (x*x)\n        y_dot = -x_dot / (x * x)\n        return y_dot\n\n    def forward_stabilized(x: np.double) - np.double:\n        \"\"\"Forward mode, stabilized rule: reuses primal output y.\"\"\"\n        x_dot = np.double(1.0)\n        # Primal computation\n        y = np.double(1.0) / x\n        # Derivative computed using primal output y: -x_dot * y^2\n        y_dot = -x_dot * y * y\n        return y_dot\n        \n    def reverse_naive(x: np.double) - np.double:\n        \"\"\"Reverse mode, naive rule: computes adjoint update from input x.\"\"\"\n        y_bar = np.double(1.0)\n        # Adjoint update computed directly as -y_bar / (x*x)\n        x_bar = -y_bar / (x * x)\n        return x_bar\n\n    def reverse_stabilized(x: np.double) - np.double:\n        \"\"\"Reverse mode, stabilized rule: reuses primal output y.\"\"\"\n        y_bar = np.double(1.0)\n        # Primal computation (from forward pass)\n        y = np.double(1.0) / x\n        # Adjoint update computed using primal output y: -y_bar * y^2\n        x_bar = -y_bar * y * y\n        return x_bar\n\n    # 4. Verification method\n    def rel_err(g_ad: np.double, g_e: np.double) - np.double:\n        \"\"\"Computes relative error.\"\"\"\n        return np.abs(g_ad - g_e) / np.abs(g_e)\n\n    all_results = []\n    \n    # Process cases in the specified order A, B, C\n    case_order = ['A', 'B', 'C']\n    ad_funcs = [forward_naive, forward_stabilized, reverse_naive, reverse_stabilized]\n\n    for case_name in case_order:\n        inputs = test_cases[case_name]\n        # max_errors for [FwdNaive, FwdStab, RevNaive, RevStab]\n        max_errors = [np.double(0.0), np.double(0.0), np.double(0.0), np.double(0.0)]\n\n        for x_val in inputs:\n            g_e = g_exact(x_val)\n            \n            # Compute derivatives from all 4 methods\n            ad_results = [f(x_val) for f in ad_funcs]\n\n            # Validation step: ignore non-finite results or zero denominator\n            if not np.isfinite(g_e) or g_e == 0.0:\n                continue\n            if not all(np.isfinite(g) for g in ad_results):\n                continue\n            \n            # Calculate and update max relative errors for the current case\n            for i in range(4):\n                error = rel_err(ad_results[i], g_e)\n                if error  max_errors[i]:\n                    max_errors[i] = error\n        \n        all_results.extend(max_errors)\n        \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, all_results))}]\")\n\nsolve()\n```", "id": "3207130"}]}