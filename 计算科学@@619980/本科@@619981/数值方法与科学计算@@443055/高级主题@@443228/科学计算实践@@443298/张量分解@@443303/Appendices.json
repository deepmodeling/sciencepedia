{"hands_on_practices": [{"introduction": "第一个实践练习旨在让你亲手操作典型多线性分解（CP分解）的基本定义。你将编写代码，从其因子矩阵出发构建一个张量，并实现至关重要的“展开”（或称“矩阵化”）操作，这是许多张量算法的核心。这个练习将帮助你巩固从抽象的外积求和到其具体矩阵表示之间的联系。[@problem_id:3282221]", "problem": "给定正整数 $I$、$J$、$K$ 和 $R$，请从第一性原理出发，根据定义构造一个三阶张量 $X \\in \\mathbb{R}^{I \\times J \\times K}$，使其规范多元 (CP) 分解（也称为规范多元 (CP) 秩）恰好为 $R$。规范多元 (CP) 分解将一个张量表示为有限个秩-1 外积的和。根据定义，一个三模态的秩-1 张量是三个非零向量的外积。具体来说，如果对于 $r \\in \\{1,\\dots,R\\}$，有 $a_r \\in \\mathbb{R}^{I}$，$b_r \\in \\mathbb{R}^{J}$ 和 $c_r \\in \\mathbb{R}^{K}$，则张量\n$$\nX \\;=\\; \\sum_{r=1}^{R} \\; a_r \\circ b_r \\circ c_r\n$$\n是一个最多有 $R$ 项的 CP 表示，其中外积 $a_r \\circ b_r \\circ c_r$ 的元素为\n$$\n\\left(a_r \\circ b_r \\circ c_r\\right)_{i,j,k} \\;=\\; a_{r,i}\\, b_{r,j}\\, c_{r,k}, \\quad \\text{for } i \\in \\{1,\\dots,I\\}, \\; j \\in \\{1,\\dots,J\\}, \\; k \\in \\{1,\\dots,K\\}.\n$$\n您的任务是编写一个完整的程序，在给定一小组参数集 $(I,J,K,R)$ 和用于确保可复现性的种子的情况下，对每个参数集执行以下操作：\n\n1) 使用提供的随机种子，生成因子矩阵 $A \\in \\mathbb{R}^{I \\times R}$、$B \\in \\mathbb{R}^{J \\times R}$ 和 $C \\in \\mathbb{R}^{K \\times R}$，其元素为从标准正态分布中抽取的独立同分布样本，以确保可复现性。然后，严格遵循上述定义，通过对 $A$、$B$ 和 $C$ 的列所隐含的 $R$ 个秩-1 外积求和来构成张量 $X$。\n\n2) 仅使用张量沿某一模式进行矩阵化（也称展开）的核心定义，将 $X$ 实现为模式-1、模式-2 和模式-3 展开，分别得到矩阵 $X_{(1)} \\in \\mathbb{R}^{I \\times (JK)}$、$X_{(2)} \\in \\mathbb{R}^{J \\times (IK)}$ 和 $X_{(3)} \\in \\mathbb{R}^{K \\times (IJ)}$。展开应通过按字典序排列 $X$ 的元素来构建，这与常规定义一致，即所选模式的索引保留为行索引，其余索引合并为列索引。从 $(i,j,k)$ 到线性列索引的精确映射在您的展开和折叠操作中必须是自洽的。\n\n3) 从第一性原理出发，利用 $A$、$B$ 和 $C$ 推导出一个由外积定义所隐含的 $X$ 在模式-1 下的代数等价矩阵化表达式，并用它通过将所得矩阵折叠回与您所选展开定义一致的三维数组来重构张量 $\\widehat{X}$。计算残差的 Frobenius 范数\n$$\n\\| X - \\widehat{X} \\|_F \\;=\\; \\sqrt{ \\sum_{i=1}^{I} \\sum_{j=1}^{J} \\sum_{k=1}^{K} \\left( X_{i,j,k} - \\widehat{X}_{i,j,k} \\right)^2 }.\n$$\n将此残差报告为一个浮点数。正确实现的推导必须产生一个接近于 $0$ 的残差（在数值舍入误差范围内）。\n\n4) 计算每个展开式 $X_{(1)}$、$X_{(2)}$ 和 $X_{(3)}$ 的数值矩阵秩（使用奇异值分解 (SVD)，这是一种经过充分检验的数值方法）。使用标准的数值秩准则：如果矩阵 $M$ 的奇异值为 $\\sigma_1 \\ge \\sigma_2 \\ge \\cdots$，则数值秩是大于某个容差的奇异值的数量，其中对于一个 $m \\times n$ 的矩阵 $M$ 和双精度算术的机器精度 $\\varepsilon$，一个安全的容差为\n$$\n\\tau \\;=\\; \\max\\{m,n\\} \\,\\varepsilon \\,\\sigma_1,\n$$\n将每个秩报告为一个整数。\n\n测试套件。按以下确切顺序，对下列参数集运行您的程序。对于每种情况，按指定设置随机种子。仅在第四种情况下，在对随机因子进行采样后，将 $A$、$B$ 和 $C$ 各自的第二列替换为第一列（从而创建一个列相关的边缘情况，其中两个秩-1 项变得相同）：\n- 情况 1：$(I,J,K,R) = (4,5,6,3)$，种子为 $101$。\n- 情况 2：$(I,J,K,R) = (3,3,3,1)$，种子为 $202$。\n- 情况 3：$(I,J,K,R) = (5,4,4,6)$，种子为 $303$。\n- 情况 4：$(I,J,K,R) = (3,4,2,2)$，种子为 $404$，并进行上述列相关修改。\n\n对于每种情况，生成一个四元组输出：\n- Frobenius 残差 $\\|X - \\widehat{X}\\|_F$，以浮点数形式表示。\n- $X_{(1)}$ 的数值秩，以整数形式表示。\n- $X_{(2)}$ 的数值秩，以整数形式表示。\n- $X_{(3)}$ 的数值秩，以整数形式表示。\n\n最终输出格式。您的程序应生成单行输出，其中包含四个案例的结果，格式为一个由四个四元素列表组成的逗号分隔列表，不含空格，并用方括号括起，例如：\n$$\n\\text{print } [[r_{1},r_{2},r_{3},r_{4}],\\,[r_{1},r_{2},r_{3},r_{4}],\\,[r_{1},r_{2},r_{3},r_{4}],\\,[r_{1},r_{2},r_{3},r_{4}]].\n$$\n您的程序不得读取任何输入。此问题不涉及物理单位。所有角度（若有使用）必须以弧度为单位。所有分数必须以小数形式报告，而非百分比。", "solution": "该问题被评估为**有效**。这是一个适定、科学上合理且客观的问题，源于张量分解的数值方法领域。所有定义和参数都已明确说明，从而可以得到唯一且可复现的解。这些任务要求对规范多元 (CP) 分解、张量展开（矩阵化）、Khatri-Rao 积以及数值秩计算有基本的理解，所有这些都是数值和科学计算中的标准概念。\n\n解决方案按照问题中概述的四个阶段进行：\n1.  从其 CP 因子构造三阶张量。\n2.  实现所有三种模式的张量展开。\n3.  从矩阵化表示重构张量以验证代数等价性。\n4.  计算展开张量的数值矩阵秩。\n\n**1. 从 CP 分解构造张量**\n\n根据给定的规范多元 (CP) 秩 $R$，构造一个三阶张量 $X \\in \\mathbb{R}^{I \\times J \\times K}$。该张量定义为 $R$ 个秩-1 张量的和：\n$$\nX = \\sum_{r=1}^{R} a_r \\circ b_r \\circ c_r\n$$\n向量 $a_r \\in \\mathbb{R}^{I}$、$b_r \\in \\mathbb{R}^{J}$ 和 $c_r \\in \\mathbb{R}^{K}$ 分别是三个因子矩阵 $A \\in \\mathbb{R}^{I \\times R}$、$B \\in \\mathbb{R}^{J \\times R}$ 和 $C \\in \\mathbb{R}^{K \\times R}$ 的列。这些矩阵的元素是使用指定的种子从标准正态分布中生成的，以确保可复现性。张量 $X$ 的元素级定义是：\n$$\nX_{i,j,k} = \\sum_{r=1}^{R} A_{i,r} B_{j,r} C_{k,r}\n$$\n对于 $i \\in \\{0, \\dots, I-1\\}$、$j \\in \\{0, \\dots, J-1\\}$ 和 $k \\in \\{0, \\dots, K-1\\}$（使用从 0 开始的索引以与实现保持一致）。这个求和可以使用爱因斯坦求和约定高效实现，例如通过 `numpy.einsum('ir,jr,kr->ijk', A, B, C)`。\n\n**2. 张量展开（矩阵化）**\n\n展开（或矩阵化）是将张量的元素重新排列成矩阵的过程。张量的模式-$n$ 展开将沿模式 $n$ 的纤维排列为结果矩阵的列。然而，问题指定将所选模式的索引保留为行索引。对于一个三阶张量 $X \\in \\mathbb{R}^{I \\times J \\times K}$：\n\n-   **模式-1 展开**：$X_{(1)} \\in \\mathbb{R}^{I \\times (JK)}$。元素 $X_{i,j,k}$ 映射到条目 $(X_{(1)})_{i,p}$，其中 $p$ 是对应于对 $(j,k)$ 的线性索引。我们采用标准的字典序，其中最后一个索引变化最快（C 风格排序），因此 $p = j \\cdot K + k$。这通过重塑张量来实现。\n-   **模式-2 展开**：$X_{(2)} \\in \\mathbb{R}^{J \\times (IK)}$。元素 $X_{i,j,k}$ 映射到 $(X_{(2)})_{j,p}$，其中 $p = i \\cdot K + k$。这通过首先将张量维度从 $(I,J,K)$ 转置为 $(J,I,K)$ 然后重塑来实现。\n-   **模式-3 展开**：$X_{(3)} \\in \\mathbb{R}^{K \\times (IJ)}$。元素 $X_{i,j,k}$ 映射到 $(X_{(3)})_{k,p}$，其中 $p = i \\cdot J + j$。这通过将维度转置为 $(K,I,J)$ 然后重塑来实现。\n\n**3. 矩阵化的 CP 表示与重构**\n\nCP 分解的一个基本性质是其在矩阵化形式下的简洁表示。$X$ 的模式-1 展开可以表示为：\n$$\nX_{(1)} = A (B \\odot C)^T\n$$\n此处，$\\odot$ 表示 Khatri-Rao 积，它是一种列式 Kronecker 积。矩阵 $B \\odot C \\in \\mathbb{R}^{(JK) \\times R}$ 的第 $r$ 列是 $B$ 和 $C$ 的第 $r$ 列的 Kronecker 积，即 $(B \\odot C)_{:,r} = b_r \\otimes c_r$。这个公式与我们选择的展开约定（其中列索引为 $p=jK+k$）是一致的。\n\n为了验证我们的实现，我们首先计算矩阵化形式 $\\widehat{X}_{(1)} = A (B \\odot C)^T$。然后，我们将这个矩阵“折叠”回一个大小为 $I \\times J \\times K$ 的张量 $\\widehat{X}$。这个折叠操作是展开的逆操作，通过将 $\\widehat{X}_{(1)}$ 重塑为维度 $(I, J, K)$ 来完成。最后，我们计算残差的 Frobenius 范数 $\\|X - \\widehat{X}\\|_F$。一个正确实现的过程将产生一个接近机器精度的残差，从而证实了代数等价性。\n\n**4. 数值秩计算**\n\n矩阵的数值秩是通过计算其大于指定容差的奇异值的数量来确定的。奇异值通过奇异值分解 (SVD) 获得。对于一个具有奇异值 $\\sigma_1 \\ge \\sigma_2 \\ge \\dots$ 的 $m \\times n$ 矩阵 $M$，容差由 $\\tau = \\max\\{m,n\\} \\cdot \\varepsilon \\cdot \\sigma_1$ 给出，其中 $\\varepsilon$ 是双精度浮点数的机器精度。数值秩是满足 $\\sigma_i > \\tau$ 的奇异值 $\\sigma_i$ 的数量。\n\n对于一个由 $R$ 个线性无关分量（对于随机生成的因子，这以概率 1 成立）构成的 CP 分解所构造的张量 $X$，其模式-$n$ 展开的秩由 $\\text{rank}(X_{(n)}) = \\min(\\text{dim}_n, R)$ 给出，其中 $\\text{dim}_n$ 是模式 $n$ 的维度。因此，我们预期 $\\text{rank}(X_{(1)}) = \\min(I, R)$，$\\text{rank}(X_{(2)}) = \\min(J, R)$ 和 $\\text{rank}(X_{(3)}) = \\min(K, R)$。在情况 4 中，两个分量被故意设为相同，将张量的有效秩降至 1。因此，所有展开的数值秩预期都为 1。", "answer": "```python\nimport numpy as np\n\ndef construct_tensor(A, B, C):\n    \"\"\"\n    Constructs a 3rd-order tensor from its CP factor matrices.\n    X_ijk = sum_r A_ir * B_jr * C_kr\n    \"\"\"\n    return np.einsum('ir,jr,kr->ijk', A, B, C)\n\ndef unfold(tensor, mode):\n    \"\"\"\n    Unfolds a 3rd-order tensor into a matrix.\n    mode 0: I x (JK)\n    mode 1: J x (IK)\n    mode 2: K x (IJ)\n    \"\"\"\n    if mode == 0:\n        return tensor.reshape(tensor.shape[0], -1)\n    if mode == 1:\n        return np.transpose(tensor, (1, 0, 2)).reshape(tensor.shape[1], -1)\n    if mode == 2:\n        return np.transpose(tensor, (2, 0, 1)).reshape(tensor.shape[2], -1)\n    raise ValueError(\"Mode must be 0, 1, or 2.\")\n\ndef fold(matrix, mode, shape):\n    \"\"\"\n    Folds a matrix back into a 3rd-order tensor.\n    This is the inverse of the unfold operation.\n    \"\"\"\n    I, J, K = shape\n    if mode == 0:\n        return matrix.reshape((I, J, K))\n    if mode == 1:\n        return np.transpose(matrix.reshape((J, I, K)), (1, 0, 2))\n    if mode == 2:\n        return np.transpose(matrix.reshape((K, I, J)), (2, 0, 1))\n    raise ValueError(\"Mode must be 0, 1, or 2.\")\n\ndef khatri_rao(A, B):\n    \"\"\"\n    Computes the Khatri-Rao product of two matrices A and B.\n    This is a column-wise Kronecker product.\n    If A is (I, R) and B is (J, R), the result is (I*J, R).\n    The r-th column of the result is kron(A[:,r], B[:,r]).\n    \"\"\"\n    if A.shape[1] != B.shape[1]:\n        raise ValueError(\"Matrices must have the same number of columns.\")\n    \n    # Efficient implementation using Einstein summation and reshaping.\n    # einsum produces a tensor T of shape (I, J, R) where T[i,j,r] = A[i,r] * B[j,r].\n    # Reshaping to (I*J, R) flattens the first two dimensions, with the second\n    # index (j) varying fastest, which corresponds to the Kronecker product.\n    return np.einsum('ir,jr->ijr', A, B).reshape(A.shape[0] * B.shape[0], A.shape[1])\n\ndef numerical_rank(matrix):\n    \"\"\"\n    Computes the numerical rank of a matrix using SVD.\n    \"\"\"\n    if matrix.size == 0 or np.all(matrix==0):\n        return 0\n        \n    m, n = matrix.shape\n    s = np.linalg.svd(matrix, compute_uv=False)\n    \n    if s[0] == 0:\n        return 0\n\n    eps = np.finfo(float).eps\n    tolerance = max(m, n) * eps * s[0]\n    \n    return np.sum(s > tolerance)\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and produce the final output.\n    \"\"\"\n    test_cases = [\n        {'I': 4, 'J': 5, 'K': 6, 'R': 3, 'seed': 101, 'mod': False},\n        {'I': 3, 'J': 3, 'K': 3, 'R': 1, 'seed': 202, 'mod': False},\n        {'I': 5, 'J': 4, 'K': 4, 'R': 6, 'seed': 303, 'mod': False},\n        {'I': 3, 'J': 4, 'K': 2, 'R': 2, 'seed': 404, 'mod': True},\n    ]\n\n    all_results = []\n    \n    for case in test_cases:\n        I, J, K, R, seed, mod = case['I'], case['J'], case['K'], case['R'], case['seed'], case['mod']\n        \n        # 1. Generate factor matrices and construct the tensor X\n        np.random.seed(seed)\n        A = np.random.randn(I, R)\n        B = np.random.randn(J, R)\n        C = np.random.randn(K, R)\n        \n        if mod:\n            # Special modification for Case 4\n            A[:, 1] = A[:, 0]\n            B[:, 1] = B[:, 0]\n            C[:, 1] = C[:, 0]\n            \n        X = construct_tensor(A, B, C)\n\n        # 2. Unfold X along all three modes\n        X1 = unfold(X, 0)\n        X2 = unfold(X, 1)\n        X3 = unfold(X, 2)\n\n        # 3. Reconstruct X from matricized form and compute residual\n        # The unfolding convention used (last index fastest) corresponds to X_1 = A (B \\odot C)^T\n        # Note: The convention for column ordering of X_1 matters. Reshape uses C-order\n        # (last index varies fastest). This means the columns are indexed by j,k where k\n        # varies fastest. This corresponds to C \\odot B in some literature.\n        # However, our khatri_rao(B,C) gives columns kron(b_r, c_r), which has elements B_jr * C_kr.\n        # This matches the C-style unfolding.\n        KR_BC = khatri_rao(B, C)\n        X1_hat_mat = A @ KR_BC.T\n        X_hat = fold(X1_hat_mat, 0, (I, J, K))\n        residual = np.linalg.norm(X - X_hat)\n\n        # 4. Compute numerical ranks of unfoldings\n        rank1 = numerical_rank(X1)\n        rank2 = numerical_rank(X2)\n        rank3 = numerical_rank(X3)\n        \n        all_results.append((residual, rank1, rank2, rank3))\n\n    # Format the final output string as specified\n    formatted_results = []\n    for res_tuple in all_results:\n        # Use repr() for the float to get a standard, high-precision representation\n        s = f\"[{repr(res_tuple[0])},{res_tuple[1]},{res_tuple[2]},{res_tuple[3]}]\"\n        formatted_results.append(s)\n        \n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3282221"}, {"introduction": "除了CP模型，Tucker分解为表示张量数据提供了另一种通常更紧凑的方式。其中的一个关键挑战是选择合适的多线性秩。本实践将介绍一种标准的、基于能量的方法来确定这些秩，该方法构成了强大的高阶奇异值分解（HOSVD）算法的基础。[@problem_id:3282068]", "problem": "考虑一个实数 $N$ 阶张量 $X \\in \\mathbb{R}^{I_1 \\times I_2 \\times \\cdots \\times I_N}$，其模-$n$ 展开 $X_{(n)} \\in \\mathbb{R}^{I_n \\times \\prod_{m \\neq n} I_m}$ 是通过重新排序条目形成的，使得模 $n$ 的索引沿行变化，而所有其他模式的索引沿列拼接。考虑一个实数矩阵的奇异值分解 (SVD) $M = U \\Sigma V^\\top$，其中 $U$ 和 $V$ 是标准正交的，$\\Sigma$ 是对角矩阵，其对角线元素为非负奇异值。Frobenius 范数 $\\|M\\|_F$ 满足 $\\|M\\|_F^2 = \\sum_i \\sigma_i^2$，其中 $\\sigma_i$ 是 $M$ 的奇异值。Tucker 分解将 $X$ 表示为 $X = G \\times_1 A^{(1)} \\times_2 A^{(2)} \\cdots \\times_N A^{(N)}$，其中 $G \\in \\mathbb{R}^{R_1 \\times R_2 \\times \\cdots \\times R_N}$ 是一个核心张量，$A^{(n)} \\in \\mathbb{R}^{I_n \\times R_n}$ 是因子矩阵；元组 $(R_1, R_2, \\ldots, R_N)$ 被称为多线性秩。典范多项分解 (CP)（也称为典范多项 (CP) 模型）是另一种基本的张量分解方法，它将张量表示为秩-1 项的和，但它不是本问题的重点。\n\n设计并实现一个程序，在给定一个张量 $X$ 和一个目标能量阈值 $\\tau \\in [0,1]$ 的情况下，通过分析每个展开 $X_{(n)}$ 的奇异值，推荐一个合适的 Tucker 多线性秩 $(R_1, R_2, \\ldots, R_N)$。您的程序必须：\n- 为每个模式 $n \\in \\{1,2,\\ldots,N\\}$ 构建模-$n$ 展开 $X_{(n)}$。\n- 通过 SVD 计算 $X_{(n)}$ 的奇异值。\n- 对于每个模式 $n$，选择最小的整数 $R_n$，使得前 $R_n$ 个奇异值所捕获的平方奇异值之和的分数至少为 $\\tau$，即 $R_n$ 是满足 $\\sum_{i=1}^{R_n} \\sigma_i^2 \\big/ \\sum_{i=1}^{S_n} \\sigma_i^2 \\ge \\tau$ 的最小整数，其中 $S_n$ 是 $X_{(n)}$ 的奇异值总数，$\\sigma_i$ 是 $X_{(n)}$ 的奇异值。如果 $\\sum_{i=1}^{S_n} \\sigma_i^2 = 0$，则定义 $R_n = 0$。\n\n使用以下测试套件。在所有情况下，均不出现角度，也没有物理单位，因此无需进行单位转换。通过使用指定的固定伪随机种子来确保可重复性。\n\n1. 合成低多线性秩张量：\n   - 维度：$(I_1, I_2, I_3) = (6, 5, 4)$。\n   - 使用伪随机种子 $7$，通过对高斯随机矩阵进行 QR 分解来构建具有标准正交列的因子矩阵：$A^{(1)} \\in \\mathbb{R}^{6 \\times 2}$，$A^{(2)} \\in \\mathbb{R}^{5 \\times 3}$，$A^{(3)} \\in \\mathbb{R}^{4 \\times 2}$。\n   - 使用相同的种子 $7$，构建一个核心张量 $G \\in \\mathbb{R}^{2 \\times 3 \\times 2}$，其条目来自标准正态分布。\n   - 形成 $X = G \\times_1 A^{(1)} \\times_2 A^{(2)} \\times_3 A^{(3)}$。\n   - 阈值：$\\tau = 0.99$。\n\n2. 随机满秩张量：\n   - 维度：$(I_1, I_2, I_3) = (5, 5, 5)$。\n   - 条目来自伪随机种子为 $123$ 的标准正态分布。\n   - 阈值：$\\tau = 0.90$。\n\n3. 全能量要求：\n   - 维度：$(I_1, I_2, I_3) = (3, 4, 2)$。\n   - 条目来自伪随机种子为 $202$ 的标准正态分布。\n   - 阈值：$\\tau = 1.00$。\n\n4. 零张量：\n   - 维度：$(I_1, I_2, I_3) = (4, 3, 2)$。\n   - 所有条目均等于 $0$。\n   - 阈值：$\\tau = 0.50$。\n\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果，每个结果对应一个测试用例，并且本身是一个包含每个模式建议秩的列表 $[R_1, R_2, R_3]$。例如，输出格式应为 $[[R_1,R_2,R_3],[R_1,R_2,R_3],[R_1,R_2,R_3],[R_1,R_2,R_3]]$ 的形式，只包含整数，没有多余的空格或文本。", "solution": "目标是设计一种方法，为给定 $N$ 阶张量 $X \\in \\mathbb{R}^{I_1 \\times I_2 \\times \\cdots \\times I_N}$ 的 Tucker 分解确定一个合适的多线性秩 $(R_1, R_2, \\ldots, R_N)$。Tucker 分解将 $X$ 近似为 $X \\approx G \\times_1 A^{(1)} \\times_2 A^{(2)} \\cdots \\times_N A^{(N)}$，其中 $G \\in \\mathbb{R}^{R_1 \\times R_2 \\times \\cdots \\times R_N}$ 是核心张量，$A^{(n)} \\in \\mathbb{R}^{I_n \\times R_n}$ 是因子矩阵。所提出的秩 $(R_1, \\ldots, R_N)$ 的选择应确保在每个模式中保留张量“能量”的指定分数 $\\tau \\in [0, 1]$。此过程是高阶奇异值分解 (HOSVD) 算法的基石。\n\n基本原理是，张量的 Frobenius 范数的平方（通常解释为其总能量）在展开操作下是不变的。也就是说，对于任何模式 $n \\in \\{1, 2, \\ldots, N\\}$，都有 $\\|X\\|_F^2 = \\|X_{(n)}\\|_F^2$，其中 $X_{(n)} \\in \\mathbb{R}^{I_n \\times \\prod_{m \\neq n} I_m}$ 是张量 $X$ 的模-$n$ 展开。矩阵的 Frobenius 范数通过恒等式 $\\|M\\|_F^2 = \\sum_i \\sigma_i^2$ 与其奇异值 $\\sigma_i$ 直接相关。模-$n$ 展开 $X_{(n)}$ 的奇异值量化了能量在该模式的主成分上的分布。截断 SVD 在 Frobenius 范数意义下提供了矩阵的最佳低秩近似。类似地，HOSVD 通过选择最少数目的奇异向量 $R_n$ 来截断每个模式的基，这些奇异向量对应的奇异值捕获了该模式中至少 $\\tau$ 分数的能量。\n\n确定多线性秩 $(R_1, R_2, \\ldots, R_N)$ 的算法按以下步骤进行：\n\n1.  **模-$n$ 展开（矩阵化）**：对于从 $1$ 到 $N$ 的每个模式 $n$，张量 $X$ 被展开成一个矩阵 $X_{(n)} \\in \\mathbb{R}^{I_n \\times J_n}$，其中 $J_n = \\prod_{m \\neq n} I_m$。一个元素 $X(i_1, i_2, \\ldots, i_N)$ 被映射到条目 $(X_{(n)})_{i_n, j}$，其中列索引 $j$ 对应于以一致顺序（例如，字典序）排列的扁平化索引 $(i_1, \\ldots, i_{n-1}, i_{n+1}, \\ldots, i_N)$。\n\n2.  **奇异值分解 (SVD)**：计算每个展开 $X_{(n)}$ 的 SVD：$X_{(n)} = U^{(n)} \\Sigma^{(n)} (V^{(n)})^\\top$。我们主要关心的是奇异值 $\\sigma_i^{(n)}$，它们是 $\\Sigma^{(n)}$ 的对角线元素，按非递增顺序排序。\n\n3.  **通过能量准则选择秩**：对于每个模式 $n$，我们计算总能量，即平方奇异值的和：$E_n = \\|X_{(n)}\\|_F^2 = \\sum_{i=1}^{S_n} (\\sigma_i^{(n)})^2$，其中 $S_n = \\min(I_n, J_n)$ 是奇异值的总数。然后我们找到满足能量保持准则的最小整数 $R_n$：\n    $$\n    \\frac{\\sum_{i=1}^{R_n} (\\sigma_i^{(n)})^2}{\\sum_{i=1}^{S_n} (\\sigma_i^{(n)})^2} \\ge \\tau\n    $$\n    这通过计算平方奇异值的累积和，并找到归一化累积和达到或超过阈值 $\\tau$ 的第一个索引来实现。秩 $R_n$ 是此索引加一。\n\n4.  **处理零张量**：如果张量 $X$ 是零张量，则会出现特殊情况。在这种情况下，其所有展开 $X_{(n)}$ 都是零矩阵，所有奇异值都为零。总能量 $\\sum_{i=1}^{S_n} (\\sigma_i^{(n)})^2$ 为 $0$。根据规定，这种模式的秩定义为 $R_n=0$，这在概念上和数学上都是合理的，因为零张量具有秩-$(0, \\ldots, 0)$ 表示。\n\n该实现将处理四个测试用例。值得注意的是，第一个用例要求通过模-$n$ 积从已知的低秩核心张量 $G$ 和因子矩阵 $A^{(n)}$ 构建张量 $X$。模-$n$ 积定义为 $Y = \\mathcal{X} \\times_n A$，在计算上等效于将 $\\mathcal{X}$ 矩阵化为 $\\mathcal{X}_{(n)}$，执行矩阵乘法 $Y_{(n)} = A \\mathcal{X}_{(n)}$，然后将 $Y_{(n)}$ 折叠回张量。对于一系列乘积 $X = G \\times_1 A^{(1)} \\times_2 A^{(2)} \\cdots \\times_N A^{(N)}$，该操作是迭代地应用的。随后的测试用例探讨了随机满秩张量、全能量要求 ($\\tau=1.0$) 和零张量的情景，从而验证了所实现算法的鲁棒性。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef mode_n_product(tensor, matrix, n):\n    \"\"\"\n    Computes the mode-n product of a tensor and a matrix.\n    \n    Args:\n        tensor (np.ndarray): The input tensor.\n        matrix (np.ndarray): The matrix to multiply with.\n        n (int): The mode along which to multiply (0-indexed).\n\n    Returns:\n        np.ndarray: The resulting tensor.\n    \"\"\"\n    tensor_shape = tensor.shape\n    I_n = tensor_shape[n]\n    \n    # Check if matrix dimensions are compatible\n    if matrix.shape[1] != I_n:\n        raise ValueError(f\"Matrix dimension {matrix.shape[1]} does not match tensor mode dimension {I_n}\")\n\n    # Unfold the tensor\n    unfolded_tensor = np.moveaxis(tensor, n, 0).reshape(I_n, -1)\n    \n    # Perform matrix multiplication\n    prod_unfolded = matrix @ unfolded_tensor\n    \n    # Fold the result back into a tensor\n    J_n = matrix.shape[0]\n    new_shape_list = list(tensor_shape)\n    new_shape_list[n] = J_n\n    \n    other_dims_indices = [i for i in range(tensor.ndim) if i != n]\n    other_dims = [tensor_shape[i] for i in other_dims_indices]\n\n    # Create the shape for the reshaped, pre-transposed tensor\n    folded_shape = [J_n] + other_dims\n    folded_tensor = prod_unfolded.reshape(folded_shape)\n    \n    # Move the new axis back to its original position\n    result_tensor = np.moveaxis(folded_tensor, 0, n)\n    \n    return result_tensor\n\ndef unfold(tensor, n):\n    \"\"\"\n    Unfolds a tensor into a matrix along a specified mode.\n    \n    Args:\n        tensor (np.ndarray): The input tensor.\n        n (int): The mode to unfold along (0-indexed).\n\n    Returns:\n        np.ndarray: The mode-n unfolding of the tensor.\n    \"\"\"\n    return np.moveaxis(tensor, n, 0).reshape(tensor.shape[n], -1)\n\ndef find_multilinear_rank(X, tau):\n    \"\"\"\n    Suggests a multilinear rank for a tensor based on an energy threshold.\n    \"\"\"\n    ranks = []\n    for n in range(X.ndim):\n        X_n = unfold(X, n)\n        \n        # SVD returns singular values in descending order\n        singular_values = np.linalg.svd(X_n, compute_uv=False)\n        \n        s_squared = singular_values**2\n        total_energy = np.sum(s_squared)\n        \n        if total_energy == 0:\n            R_n = 0\n        else:\n            cumulative_energy = np.cumsum(s_squared)\n            # Use a small tolerance for floating point comparison with tau=1.0\n            energy_fraction = cumulative_energy / total_energy\n            \n            candidates = np.where(energy_fraction >= tau - 1e-9)[0]\n            \n            if candidates.size > 0:\n                R_n = candidates[0] + 1\n            else:\n                # This case is for tau=1.0 where numerical precision might cause\n                # the last element of energy_fraction to be slightly less than 1.0\n                R_n = len(singular_values)\n                \n        ranks.append(R_n)\n    return ranks\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the results.\n    \"\"\"\n    \n    # --- Test Case 1: Synthetic low multilinear rank tensor ---\n    def construct_test_case_1():\n        seed = 7\n        rng = np.random.default_rng(seed)\n        I = (6, 5, 4)\n        R = (2, 3, 2)\n        \n        A1_rand = rng.standard_normal((I[0], R[0]))\n        A1, _ = np.linalg.qr(A1_rand, mode='reduced')\n        \n        A2_rand = rng.standard_normal((I[1], R[1]))\n        A2, _ = np.linalg.qr(A2_rand, mode='reduced')\n\n        A3_rand = rng.standard_normal((I[2], R[2]))\n        A3, _ = np.linalg.qr(A3_rand, mode='reduced')\n        \n        factors = [A1, A2, A3]\n        \n        G = rng.standard_normal(R)\n        \n        # Reconstruct X using mode-n products\n        X = G\n        for n in range(len(I)):\n            X = mode_n_product(X, factors[n], n)\n        return X\n\n    X1 = construct_test_case_1()\n    tau1 = 0.99\n\n    # --- Test Case 2: Random full tensor ---\n    seed2 = 123\n    rng2 = np.random.default_rng(seed2)\n    X2 = rng2.standard_normal((5, 5, 5))\n    tau2 = 0.90\n    \n    # --- Test Case 3: Full energy requirement ---\n    seed3 = 202\n    rng3 = np.random.default_rng(seed3)\n    X3 = rng3.standard_normal((3, 4, 2))\n    tau3 = 1.00\n    \n    # --- Test Case 4: Zero tensor ---\n    X4 = np.zeros((4, 3, 2))\n    tau4 = 0.50\n\n    test_cases = [\n        (X1, tau1),\n        (X2, tau2),\n        (X3, tau3),\n        (X4, tau4)\n    ]\n\n    results = []\n    for X, tau in test_cases:\n        ranks = find_multilinear_rank(X, tau)\n        results.append(ranks)\n\n    # Final print statement in the exact required format.\n    result_strings = [str(r).replace(' ', '') for r in results]\n    print(f\"[{','.join(result_strings)}]\")\n\nsolve()\n```", "id": "3282068"}, {"introduction": "在许多现实世界的应用中，数据往往是不完整的。这个高级实践练习将向你展示如何通过构建并求解一个加权最小二乘问题，来调整CP分解以处理缺失数据。你将从头开始实现一个稳健的CP-WLS（加权最小二乘CP）算法，这是一种强大的张量补全技术。[@problem_id:3282166]", "problem": "您必须在 Python 中实现一个完整、可运行的程序，通过最小化加权最小二乘 (Weighted Least Squares, WLS) 目标函数，来计算存在缺失数据情况下的规范多元 (Canonical Polyadic, CP) 分解。其数学基础始于 CP 分解的核心定义和加权最小二乘目标函数。设张量为一个 $N$ 阶数组 $ \\mathcal{X} \\in \\mathbb{R}^{I_0 \\times I_1 \\times \\cdots \\times I_{N-1}} $，CP 分解将 $ \\mathcal{X} $ 近似为 $R$ 个秩-1 分量的和。该近似由下式给出\n$$\n\\mathcal{\\hat{X}}[i_0,i_1,\\dots,i_{N-1}] = \\sum_{r=1}^{R} \\prod_{n=0}^{N-1} A^{(n)}[i_n, r],\n$$\n其中 $ A^{(n)} \\in \\mathbb{R}^{I_n \\times R} $ 是因子矩阵。加权最小二乘的目标函数，其权重 $ W[i_0,\\dots,i_{N-1}] \\in \\{0,1\\} $ 表示缺失（$0$）或观测到（$1$）的条目，为\n$$\n\\min_{\\{A^{(n)}\\}} \\; \\sum_{i_0=0}^{I_0-1} \\cdots \\sum_{i_{N-1}=0}^{I_{N-1}-1} W[i_0,\\dots,i_{N-1}] \\, \\left( \\mathcal{X}[i_0,\\dots,i_{N-1}] - \\mathcal{\\hat{X}}[i_0,\\dots,i_{N-1}] \\right)^2.\n$$\n您必须从第一性原理出发，设计一种科学上合理且数值上可靠的优化方法。使用针对因子矩阵的交替块坐标下降法，其中每个子问题都作为一个带有 $ \\ell_2 $ 正则化项 $ \\lambda = 10^{-6} $ 的加权最小二乘问题来求解，以确保数值稳定性。您必须避免使用问题陈述中的任何捷径公式；您使用的所有更新规则都应从所述目标函数一致地推导出来。当某个模态中一整行的权重均为零时，应实现稳健的处理机制，通过保持该行不变或将其设置为零，来确保在此类退化情况下的行为是明确定义的。\n\n实现所需的定义：\n- 设两个矩阵 $ U \\in \\mathbb{R}^{I \\times R} $ 和 $ V \\in \\mathbb{R}^{J \\times R} $ 的 Khatri-Rao 积是列向的 Kronecker 积 $ U \\odot V \\in \\mathbb{R}^{(I J) \\times R} $。\n- 设 $ \\mathcal{X} $ 的模-$n$ 展开是矩阵 $ X_{(n)} \\in \\mathbb{R}^{I_n \\times \\prod_{m \\neq n} I_m} $，通过置换轴使得模 $n$ 成为第一维度，并按特定、一致的顺序将剩余的轴展平得到。您必须在所有模态上使用一致的展开方案，并使其与其它因子矩阵相应的 Khatri-Rao 排序相匹配。\n\n您的程序必须为以下测试套件实现上述优化方法。每个测试用例通过对给定因子矩阵的外积求和，并添加具有指定标准差 $ \\sigma $ 和随机种子的 Gaussian 噪声来构造一个合成张量。然后，形成一个二元权重张量 $ W $，在指定的缺失索引处为零，在其它地方为一。CP-WLS 算法必须以等于生成秩的秩 $ R $ 运行 $ T = 200 $ 次迭代，正则化参数 $ \\lambda = 10^{-6} $，并使用指定的种子对因子矩阵进行固定的随机初始化。拟合后，计算观测到的均方根误差 (Root Mean Squared Error, RMSE)，其定义为\n$$\n\\mathrm{RMSE} = \\sqrt{ \\frac{ \\sum_{i_0,\\dots,i_{N-1}} W[i_0,\\dots,i_{N-1}] \\, \\left( \\mathcal{X}[i_0,\\dots,i_{N-1}] - \\mathcal{\\hat{X}}[i_0,\\dots,i_{N-1}] \\right)^2 }{ \\sum_{i_0,\\dots,i_{N-1}} W[i_0,\\dots,i_{N-1}] } }.\n$$\n返回每个测试用例的 RMSE。\n\n测试套件规范：\n\n- 用例 $1$（存在部分缺失条目的理想情况）：\n  - 阶数 $N = 3$，维度 $ (I_0,I_1,I_2) = (4,3,2) $，秩 $ R = 2 $。\n  - 生成因子矩阵：\n    $$\n    A^{(0)} = \\begin{bmatrix}\n    1.0  0.5 \\\\\n    0.8  -0.2 \\\\\n    0.3  1.2 \\\\\n    1.1  0.7\n    \\end{bmatrix}, \\quad\n    A^{(1)} = \\begin{bmatrix}\n    0.9  -0.4 \\\\\n    0.1  0.5 \\\\\n    1.0  0.3\n    \\end{bmatrix}, \\quad\n    A^{(2)} = \\begin{bmatrix}\n    1.0  0.2 \\\\\n    0.5  -1.0\n    \\end{bmatrix}.\n    $$\n  - 噪声标准差 $ \\sigma = 0.01 $，种子为 $ 42 $。\n  - 缺失条目（在这些索引处将权重设为零）：$ (0,0,1), (1,2,0), (2,1,1), (3,0,0) $。\n  - 算法的初始化种子：$ 202 $。\n\n- 用例 $2$（所有条目均被观测到，更高的秩）：\n  - 阶数 $N = 3$，维度 $ (I_0,I_1,I_2) = (3,3,3) $，秩 $ R = 3 $。\n  - 生成因子矩阵：\n    $$\n    A^{(0)} = \\begin{bmatrix}\n    0.5  1.0  -0.5 \\\\\n    0.2  -0.3  0.7 \\\\\n    1.2  0.1  0.4\n    \\end{bmatrix}, \\quad\n    A^{(1)} = \\begin{bmatrix}\n    -0.6  0.9  0.3 \\\\\n    0.8  0.1  -0.2 \\\\\n    0.5  -1.1  0.6\n    \\end{bmatrix}, \\quad\n    A^{(2)} = \\begin{bmatrix}\n    0.7  0.4  -0.9 \\\\\n    1.0  -0.5  0.2 \\\\\n    0.3  0.8  0.1\n    \\end{bmatrix}.\n    $$\n  - 噪声标准差 $ \\sigma = 0.02 $，种子为 $ 0 $。\n  - 所有条目均被观测到：对所有索引，权重 $ W[i_0,i_1,i_2] = 1 $。\n  - 算法的初始化种子：$ 202 $。\n\n- 用例 $3$（一个模态中存在完全缺失的切片以及额外的缺失条目的边缘情况）：\n  - 阶数 $N = 3$，维度 $ (I_0,I_1,I_2) = (5,4,3) $，秩 $ R = 2 $。\n  - 生成因子矩阵：\n    $$\n    A^{(0)} = \\begin{bmatrix}\n    0.9  -0.1 \\\\\n    0.4  0.2 \\\\\n    -0.3  0.8 \\\\\n    1.0  0.5 \\\\\n    0.7  -0.6\n    \\end{bmatrix}, \\quad\n    A^{(1)} = \\begin{bmatrix}\n    0.5  1.1 \\\\\n    -0.2  0.3 \\\\\n    0.9  -0.4 \\\\\n    0.1  0.6\n    \\end{bmatrix}, \\quad\n    A^{(2)} = \\begin{bmatrix}\n    1.0  0.2 \\\\\n    -0.5  0.7 \\\\\n    0.3  -0.9\n    \\end{bmatrix}.\n    $$\n  - 噪声标准差 $ \\sigma = 0.015 $，种子为 $ 123 $。\n  - 缺失条目：所有 $ i_0 = 0 $ 的索引均缺失，外加 $ (2,1,2) $ 和 $ (4,3,1) $。\n  - 算法的初始化种子：$ 202 $。\n\n实现要求：\n- 仅使用 Python $3.12$ 和 NumPy 库 $1.23.5$。\n- 实现一个通用的 $N$ 模态 CP-WLS 算法，该算法具有跨模态的交替更新、一致的展开方案以及与该方案对齐的 Khatri-Rao 积。\n- 使用 $T = 200$ 次迭代和 $ \\lambda = 10^{-6} $ 的正则化。\n- 在对所有模态完成一次完整扫描后，执行列归一化操作：将除 $ A^{(0)} $ 外的所有因子矩阵的列除以它们的 $ \\ell_2 $ 范数，并将范数的乘积吸收到 $ A^{(0)} $ 的每个分量中，以保持尺度的稳定性。\n- 按上述定义计算最终的观测 RMSE。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含三个测试用例的 RMSE 值，格式为方括号内的逗号分隔列表，例如 $ [r_1,r_2,r_3] $。每个 $ r_k $ 都必须是一个浮点值。", "solution": "该问题要求通过最小化加权最小二乘 (Weighted Least Squares, WLS) 目标函数，为存在缺失条目的张量实现规范多元 (Canonical Polyadic, CP) 分解。解决方案是使用交替优化方案从第一性原理出发开发的。\n\n### 数学公式\n\nCP 分解通过一个由 $R$ 个秩-1 分量组成的模型张量 $ \\mathcal{\\hat{X}} $ 来近似一个给定的 $N$ 阶张量 $ \\mathcal{X} \\in \\mathbb{R}^{I_0 \\times \\cdots \\times I_{N-1}} $。$ \\mathcal{\\hat{X}} $ 的元素由下式给出：\n$$\n\\mathcal{\\hat{X}}[i_0,i_1,\\dots,i_{N-1}] = \\sum_{r=1}^{R} \\prod_{n=0}^{N-1} A^{(n)}[i_n, r]\n$$\n其中 $ A^{(n)} \\in \\mathbb{R}^{I_n \\times R} $ 是因子矩阵。\n\n对于一个含有缺失数据的张量，我们给定一个权重张量 $ W \\in \\{0, 1\\}^{I_0 \\times \\cdots \\times I_{N-1}} $，其中对于观测到的条目，$W[\\dots] = 1$，对于缺失的条目，$W[\\dots] = 0$。优化问题是找到能最小化 $\\ell_2$ 正则化的加权平方误差和的因子矩阵：\n$$\n\\min_{\\{A^{(n)}\\}} \\; \\frac{1}{2} \\sum_{i_0=0}^{I_0-1} \\cdots \\sum_{i_{N-1}=0}^{I_{N-1}-1} W[\\dots] \\left( \\mathcal{X}[\\dots] - \\mathcal{\\hat{X}}[\\dots] \\right)^2 + \\frac{\\lambda}{2} \\sum_{n=0}^{N-1} \\|A^{(n)}\\|_F^2\n$$\n其中 $ \\| \\cdot \\|_F $ 是 Frobenius 范数，$ \\lambda $ 是正则化参数。\n\n### 通过交替块坐标下降进行优化\n\n该目标函数是非凸的，这使得同时求解所有因子矩阵的全局解变得难以处理。一种标准且有效的启发式方法是使用交替块坐标下降法，通常称为交替最小二乘法 (Alternating Least Squares, ALS)。在这种方法中，我们一次只对一个因子矩阵 $ A^{(k)} $进行优化，同时保持所有其他因子矩阵 $ \\{A^{(n)}\\}_{n \\neq k} $ 固定。对所有模态 $k = 0, \\dots, N-1$ 迭代重复此过程。\n\n### 更新规则的推导\n\n让我们推导特定因子矩阵 $ A^{(k)} $ 的更新规则。当所有其他因子都固定时，目标函数可以使用张量的模-$k$ 展开来重写。模-$k$ 展开，记为 $X_{(k)}$，是一个大小为 $I_k \\times (\\prod_{m \\neq k} I_m)$ 的矩阵，其中行对应于模 $k$ 的索引。\n\nCP 模型可以用矩阵化的形式表示为：\n$$\nX_{(k)} \\approx A^{(k)} \\left( \\mathbf{C}^{(k)} \\right)^T\n$$\n在这里，$\\mathbf{C}^{(k)}$ 是除 $A^{(k)}$ 外所有因子矩阵的 Khatri-Rao 积。此积中矩阵的具体排序必须与展开后张量的列排序一致。如果我们将 $\\mathcal{X}$ 展开为 $X_{(k)}$，使得列索引 $j$ 对应于多重索引 $(i_m)_{m \\neq k}$，其中最低的模态索引变化最快（在 NumPy 中为 `order='F'`），那么 $\\mathbf{C}^{(k)}$ 必须构造为：\n$$\n\\mathbf{C}^{(k)} = A^{(m_p)} \\odot A^{(m_{p-1})} \\odot \\cdots \\odot A^{(m_1)}\n$$\n其中 $m_1  m_2  \\dots  m_p$ 是除 $k$ 之外的模态的排序索引。在实现中，这是通过将 Khatri-Rao 积迭代地应用于其他因子矩阵的排序列表来实现的。\n\n通过这种形式，目标函数作为仅关于 $A^{(k)}$ 的函数，可以按行解耦。对于 $A^{(k)}$ 的每一行 $i_k$（由行向量 $a_{i_k}^{(k)}$ 表示），其子问题是一个独立的正则化加权最小二乘问题：\n$$\n\\min_{a_{i_k}^{(k)}} \\frac{1}{2} \\| \\sqrt{w_{i_k}} \\odot (x_{i_k} - a_{i_k}^{(k)} (\\mathbf{C}^{(k)})^T) \\|_2^2 + \\frac{\\lambda}{2} \\|a_{i_k}^{(k)}\\|_2^2\n$$\n其中 $x_{i_k}$ 和 $w_{i_k}$ 分别是展开矩阵 $X_{(k)}$ 和 $W_{(k)}$ 的第 $i_k$ 行，$\\odot$ 是逐元素乘积。原始的正则化项被分配到各个子问题中。\n\n这是一个标准的岭回归问题。通过将关于 $a_{i_k}^{(k)}$ 的梯度设为零来找到解，这会产生以下关于行向量转置 $(a_{i_k}^{(k)})^T$ 的 $R \\times R$ 线性系统：\n$$\n\\left( (\\mathbf{C}^{(k)})^T \\mathrm{diag}(w_{i_k}) \\mathbf{C}^{(k)} + \\lambda I_R \\right) (a_{i_k}^{(k)})^T = (\\mathbf{C}^{(k)})^T \\mathrm{diag}(w_{i_k}) (x_{i_k})^T\n$$\n其中 $I_R$ 是 $R \\times R$ 的单位矩阵。\n\n### 算法设计与实现\n\n算法流程如下：\n\n1.  **初始化**：使用指定的随机种子，从标准正态分布中抽取数值来初始化因子矩阵 $\\{A^{(n)}\\}$。\n2.  **迭代更新**：对于固定次数的迭代（$T=200$）：\n    a.  **模态迭代**：对于每个模态 $n = 0, \\dots, N-1$：\n        i.  **形成 $\\mathbf{C}^{(n)}$**：计算除 $A^{(n)}$ 外所有因子矩阵 $\\{A^{(m)}\\}_{m \\neq n}$ 的 Khatri-Rao 积。这些矩阵按模态索引排序以匹配展开约定。\n        ii. **展开张量**：将数据张量 $\\mathcal{X}$ 和权重张量 $W$ 展开为 $X_{(n)}$ 和 $W_{(n)}$。\n        iii. **逐行更新**：对于 $A^{(n)}$ 的每一行 $i_n = 0, \\dots, I_n-1$：\n            - 从 $W_{(n)}$ 中提取权重向量 $w_{i_n}$。\n            - **边缘情况处理**：如果 $w_{i_n}$ 中的所有权重都为零（即，整个切片都缺失），则矩阵 $(\\mathbf{C}^{(n)})^T \\mathrm{diag}(w_{i_n}) \\mathbf{C}^{(n)}$ 和线性系统右侧的向量都将变为零。该系统简化为 $\\lambda I_R (a_{i_n}^{(n)})^T = 0$，这意味着 $(a_{i_n}^{(n)})^T=0$。因此，该行被设置为零向量。\n            - **求解线性系统**：对于非退化行，构建并求解 $R \\times R$ 系统以得到 $(a_{i_n}^{(n)})^T$。为避免显式构造大的对角矩阵 $\\mathrm{diag}(w_{i_n})$，使用广播机制高效地计算乘积：`(C_n.T * w_i) @ C_n` 和 `(C_n.T * w_i) @ x_i.T`。\n    b.  **归一化**：在对所有模态完成一次完整扫描后，对因子矩阵的列进行归一化，使其具有单位 $\\ell_2$ 范数。这是确保数值稳定性以及管理 CP 分解固有的尺度模糊性（即可以将 $A^{(n)}$ 的一列乘以 $\\alpha$ 同时将 $A^{(m)}$ 的一列乘以 $1/\\alpha$ 而不改变乘积）的关键步骤。这些范数被累积并吸收到第一个因子矩阵 $A^{(0)}$ 的列中。\n3.  **重构与评估**：在最后一次迭代之后，从拟合的因子矩阵重构模型张量 $\\hat{\\mathcal{X}}$。使用指定的公式计算最终的观测均方根误差 (RMSE)：\n$$\n\\mathrm{RMSE} = \\sqrt{ \\frac{ \\sum_{i_0,\\dots,i_{N-1}} W[\\dots] \\, \\left( \\mathcal{X}[\\dots] - \\mathcal{\\hat{X}}[\\dots] \\right)^2 }{ \\sum_{i_0,\\dots,i_{N-1}} W[\\dots] } }\n$$\n\n这种基于第一性原理的方法确保了 CP-WLS 算法的稳健和正确实现，该实现直接从其数学定义推导而来。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef khatri_rao(matrices):\n    \"\"\"\n    Computes the Khatri-Rao product of a list of matrices.\n    The product is computed iteratively: M_1 @ M_2 @ ... @ M_k.\n    \"\"\"\n    if not matrices:\n        return None\n    if len(matrices) == 1:\n        return matrices[0]\n\n    result = matrices[0]\n    for i in range(1, len(matrices)):\n        mat = matrices[i]\n        I_res, R = result.shape\n        I_mat, _ = mat.shape\n        \n        # Use einsum for column-wise outer products, then reshape.\n        # 'ir,jr-ijr' computes the outer product of each corresponding column r.\n        # reshape with 'F' order ensures the first index (from `result`) varies fastest,\n        # matching the unfolding convention.\n        result = np.einsum('ir,jr-ijr', result, mat).reshape(I_res * I_mat, R, order='F')\n    return result\n\ndef reconstruct(factors):\n    \"\"\"\n    Reconstructs a full tensor from its CP factor matrices.\n    \"\"\"\n    if not factors:\n        return None\n    \n    R = factors[0].shape[1]\n    N = len(factors)\n    shape = tuple(f.shape[0] for f in factors)\n    \n    # Efficient reconstruction using einsum\n    # Build the einsum string dynamically, e.g., 'ir,jr,kr->ijk' for N=3\n    indices = 'abcdefghijklmnopqrstuvwxyz'\n    input_terms = [f'{indices[n]}r' for n in range(N)]\n    output_term = indices[:N]\n    einsum_str = f\"{','.join(input_terms)}-{output_term}\"\n    \n    return np.einsum(einsum_str, *factors)\n\n\ndef cp_wls(X, W, R, T, lmbda, init_seed):\n    \"\"\"\n    Computes the CP decomposition for a tensor with missing data using\n    Weighted Alternating Least Squares (WALS).\n    \"\"\"\n    N = X.ndim\n    dims = X.shape\n    \n    rng = np.random.default_rng(init_seed)\n    factors = [rng.standard_normal((dims[n], R)) for n in range(N)]\n\n    for _ in range(T):\n        for n in range(N):\n            # 1. Form the Khatri-Rao product of all other factor matrices.\n            modes_other = sorted([m for m in range(N) if m != n])\n            kr_matrices = [factors[m] for m in modes_other]\n            C_n = khatri_rao(kr_matrices)\n            \n            # 2. Unfold data and weight tensors along mode n with Fortran ordering.\n            Xn = np.moveaxis(X, n, 0).reshape((dims[n], -1), order='F')\n            Wn = np.moveaxis(W, n, 0).reshape((dims[n], -1), order='F')\n            \n            # 3. Update each row of the factor matrix A^{(n)}.\n            new_An = np.zeros_like(factors[n])\n            for i in range(dims[n]):\n                wi = Wn[i, :]\n                \n                if np.sum(wi)  1e-12:\n                    continue  # Row remains zero\n                \n                # Setup and solve the regularized weighted linear least squares for the row.\n                M = (C_n.T * wi) @ C_n + lmbda * np.eye(R)\n                v = (C_n.T * wi) @ Xn[i, :].T\n                \n                try:\n                    solved_row = np.linalg.solve(M, v)\n                    new_An[i, :] = solved_row\n                except np.linalg.LinAlgError:\n                    # Failsafe for singular matrix, unlikely with regularization.\n                    new_An[i, :] = 0.0\n\n            factors[n] = new_An\n\n        # 4. Column normalization after a full sweep.\n        col_norms_product = np.ones(R)\n        for mode_idx in range(1, N):\n            norms = np.linalg.norm(factors[mode_idx], axis=0)\n            non_zero_indices = norms > 1e-12\n            if np.any(non_zero_indices):\n                factors[mode_idx][:, non_zero_indices] /= norms[non_zero_indices]\n            norms[norms  1e-12] = 1.0\n            col_norms_product *= norms\n        \n        factors[0] *= col_norms_product\n    \n    return factors\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    # Global algorithm parameters\n    T = 200\n    lmbda = 1e-6\n    init_seed = 202\n\n    test_cases = [\n        # Case 1: Happy path\n        {\n            \"dims\": (4, 3, 2), \"R\": 2, \"sigma\": 0.01, \"noise_seed\": 42,\n            \"gen_factors\": [\n                np.array([[1.0, 0.5], [0.8, -0.2], [0.3, 1.2], [1.1, 0.7]]),\n                np.array([[0.9, -0.4], [0.1, 0.5], [1.0, 0.3]]),\n                np.array([[1.0, 0.2], [0.5, -1.0]])\n            ],\n            \"missing_indices\": [(0, 0, 1), (1, 2, 0), (2, 1, 1), (3, 0, 0)]\n        },\n        # Case 2: Fully observed\n        {\n            \"dims\": (3, 3, 3), \"R\": 3, \"sigma\": 0.02, \"noise_seed\": 0,\n            \"gen_factors\": [\n                np.array([[0.5, 1.0, -0.5], [0.2, -0.3, 0.7], [1.2, 0.1, 0.4]]),\n                np.array([[-0.6, 0.9, 0.3], [0.8, 0.1, -0.2], [0.5, -1.1, 0.6]]),\n                np.array([[0.7, 0.4, -0.9], [1.0, -0.5, 0.2], [0.3, 0.8, 0.1]])\n            ],\n            \"missing_indices\": []\n        },\n        # Case 3: Edge case with a missing slice\n        {\n            \"dims\": (5, 4, 3), \"R\": 2, \"sigma\": 0.015, \"noise_seed\": 123,\n            \"gen_factors\": [\n                np.array([[0.9, -0.1], [0.4, 0.2], [-0.3, 0.8], [1.0, 0.5], [0.7, -0.6]]),\n                np.array([[0.5, 1.1], [-0.2, 0.3], [0.9, -0.4], [0.1, 0.6]]),\n                np.array([[1.0, 0.2], [-0.5, 0.7], [0.3, -0.9]])\n            ],\n            \"missing_indices\": [\n                (0, i1, i2) for i1 in range(4) for i2 in range(3)\n            ] + [(2, 1, 2), (4, 3, 1)]\n        }\n    ]\n\n    rmse_results = []\n    \n    for case in test_cases:\n        # 1. Generate synthetic data\n        X_true = reconstruct(case[\"gen_factors\"])\n        rng_noise = np.random.default_rng(case[\"noise_seed\"])\n        noise = rng_noise.standard_normal(size=case[\"dims\"]) * case[\"sigma\"]\n        X_noisy = X_true + noise\n        \n        # 2. Create weight tensor W\n        W = np.ones(case[\"dims\"])\n        if case[\"missing_indices\"]:\n            # Need to handle combined list and generator\n            indices_to_set = list(case[\"missing_indices\"])\n            for idx in indices_to_set:\n                W[idx] = 0\n            \n        # 3. Run CP-WLS algorithm\n        fitted_factors = cp_wls(X_noisy, W, case[\"R\"], T, lmbda, init_seed)\n        \n        # 4. Reconstruct tensor from fitted factors\n        X_hat = reconstruct(fitted_factors)\n        \n        # 5. Compute observed RMSE\n        squared_error = W * (X_noisy - X_hat)**2\n        sum_sq_err = np.sum(squared_error)\n        num_observed = np.sum(W)\n        \n        rmse = np.sqrt(sum_sq_err / num_observed) if num_observed > 0 else 0.0\n        rmse_results.append(rmse)\n\n    print(f\"[{','.join(f'{r:.8f}' for r in rmse_results)}]\")\n\nsolve()\n```", "id": "3282166"}]}