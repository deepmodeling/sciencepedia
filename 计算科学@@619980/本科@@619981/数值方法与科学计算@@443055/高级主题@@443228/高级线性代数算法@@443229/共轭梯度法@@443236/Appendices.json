{"hands_on_practices": [{"introduction": "第一个练习是热身，旨在通过一个可控的步骤来揭开共轭梯度算法的神秘面纱。通过手动计算第一次迭代更新，你将对算法的核心要素——残差 $r_k$、搜索方向 $p_k$ 和步长 $\\alpha_k$ ——建立具体的感知。在进入更复杂的编程实现之前，这个基础练习对于培养算法直觉至关重要。 [@problem_id:1393666]", "problem": "考虑线性方程组 $Ax=b$，其中矩阵 $A$ 和向量 $b$ 由下式给出：\n$$\nA = \\begin{pmatrix} 2  -1 \\\\ -1  3 \\end{pmatrix}, \\quad b = \\begin{pmatrix} 4 \\\\ -3 \\end{pmatrix}\n$$\n矩阵 $A$ 是对称正定的。\n\n从初始猜测 $x_0 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$ 开始，应用一次共轭梯度法迭代，求出第一次更新后的解 $x_1$。\n\n将您的答案表示为一个行矩阵，其中包含 $x_1$ 的分量，并以精确分数形式表示。", "solution": "我们对从 $x_{0}$ 开始的对称正定矩阵 $A$ 应用共轭梯度法。标准的第一次迭代公式为：\n$$\nr_{0} = b - A x_{0}, \\quad p_{0} = r_{0}, \\quad \\alpha_{0} = \\frac{r_{0}^{T} r_{0}}{p_{0}^{T} A p_{0}}, \\quad x_{1} = x_{0} + \\alpha_{0} p_{0}.\n$$\n给定 $x_{0} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$ 和 $b = \\begin{pmatrix} 4 \\\\ -3 \\end{pmatrix}$，计算\n$$\nr_{0} = b - A x_{0} = \\begin{pmatrix} 4 \\\\ -3 \\end{pmatrix}, \\quad p_{0} = r_{0} = \\begin{pmatrix} 4 \\\\ -3 \\end{pmatrix}.\n$$\n接下来，计算 $A p_{0}$：\n$$\nA p_{0} = \\begin{pmatrix} 2  -1 \\\\ -1  3 \\end{pmatrix} \\begin{pmatrix} 4 \\\\ -3 \\end{pmatrix} = \\begin{pmatrix} 2 \\cdot 4 + (-1)(-3) \\\\ (-1) \\cdot 4 + 3 \\cdot (-3) \\end{pmatrix} = \\begin{pmatrix} 11 \\\\ -13 \\end{pmatrix}.\n$$\n计算标量积：\n$$\nr_{0}^{T} r_{0} = 4^{2} + (-3)^{2} = 16 + 9 = 25, \\quad p_{0}^{T} A p_{0} = r_{0}^{T} (A p_{0}) = 4 \\cdot 11 + (-3) \\cdot (-13) = 44 + 39 = 83.\n$$\n因此，\n$$\n\\alpha_{0} = \\frac{r_{0}^{T} r_{0}}{p_{0}^{T} A p_{0}} = \\frac{25}{83}.\n$$\n更新解：\n$$\nx_{1} = x_{0} + \\alpha_{0} p_{0} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} + \\frac{25}{83} \\begin{pmatrix} 4 \\\\ -3 \\end{pmatrix} = \\begin{pmatrix} \\frac{100}{83} \\\\ -\\frac{75}{83} \\end{pmatrix}.\n$$\n表示为行矩阵，$x_{1}$ 的分量为 $\\begin{pmatrix} \\frac{100}{83}  -\\frac{75}{83} \\end{pmatrix}$。", "answer": "$$\\boxed{\\begin{pmatrix} \\frac{100}{83}  -\\frac{75}{83} \\end{pmatrix}}$$", "id": "1393666"}, {"introduction": "共轭梯度法为何如此高效？本练习将深入探讨其最优雅的理论特性之一：收敛速度与矩阵特征值数量之间的关系。你将编写一个程序，以数值方式验证对于一个具有 $k$ 个不同特征值的对称正定矩阵，CG 方法在精确算术下至多在 $k$ 步内收敛。这个练习能让你深刻理解 CG 的快速收敛性，并阐明为何它在特定问题上是首选算法。 [@problem_id:3216667]", "problem": "您需要从第一性原理出发，对共轭梯度法进行推理，然后实现一个完整的、可运行的程序，该程序构造具有指定数量不同特征值的显式对称正定矩阵，并测量共轭梯度法达到指定容差所需的迭代次数。数学目标是证明，在精确算术中，共轭梯度法的收敛步数最多为不同特征值的数量。计算目标是在精心选择的对角测试矩阵上数值验证这一行为。\n\n从以下基础出发，不要引用任何未经证明的捷径结果：\n- 对称正定（SPD）矩阵的定义：一个实矩阵 $A \\in \\mathbb{R}^{n \\times n}$ 是SPD的，如果对于所有非零 $x \\in \\mathbb{R}^{n}$ 都有 $x^{\\top} A x \\gt 0$ 且 $A = A^{\\top}$。\n- Krylov子空间的定义：$\\mathcal{K}_{k}(A,r_{0}) = \\operatorname{span}\\{r_{0}, A r_{0}, \\dots, A^{k-1} r_{0}\\}$，由初始残差 $r_{0} = b - A x_{0}$ 构建。\n- 共轭梯度法的Galerkin条件：在第 $k$ 次迭代时，误差 $e_{k} = x_{\\star} - x_{k}$ 与 $\\mathcal{K}_{k}(A,r_{0})$（在 $A$-内积下）正交，且 $x_{k}$ 在仿射空间 $x_{0} + \\mathcal{K}_{k}(A,r_{0})$ 上最小化误差的 $A$-范数。\n\n仅使用这些原理，解释为什么对于一个恰有 $k$ 个不同特征值的SPD矩阵 $A$，当初始残差在每个特征空间上都有非零投影时，共轭梯度法在精确算术中必须在最多 $k$ 次迭代内产生精确解。然后，说明如果初始残差在某个特征空间上的投影为零，会发生什么情况。\n\n接下来，实现一个程序，该程序：\n- 构造具有指定正特征值多重集的对角SPD矩阵 $A$。对于对角线元素为 $\\{\\lambda_{i}\\}_{i=1}^{n}$ 且每个 $\\lambda_{i} \\gt 0$ 的对角矩阵，该矩阵是SPD的。\n- 使用共轭梯度法从初始猜测 $x_{0}$ 求解 $A x = b$，计算满足 $\\|r_{k}\\|_{2} \\le \\text{tol} \\cdot \\|r_{0}\\|_{2}$ 所需的迭代次数，其中 $r_{k} = b - A x_{k}$ 且 $\\|\\cdot\\|_{2}$ 表示欧几里得范数。\n- 对于每个测试用例，报告所用的整数迭代次数。\n\n测试套件和覆盖范围：\n- 使用以下四个测试用例。在每个用例中，$A$ 是对角矩阵，对角线元素按重数列表，$x_{0}$ 是零向量，且 $\\text{tol} = 10^{-12}$。\n  1. 理想情况，恰有三个不同特征值：$A = \\operatorname{diag}([\\,2,2,2,5,5,5,11,11\\,])$，因此在 $n = 8$ 维空间中有 $k = 3$ 个不同特征值 $\\{2,5,11\\}$。使用 $b = \\mathbf{1} \\in \\mathbb{R}^{8}$（所有元素均为 $1$）。\n  2. 边界情况，只有一个不同特征值：$A = \\operatorname{diag}([\\,7,7,7,7,7,7\\,])$，因此在 $n = 6$ 维空间中有 $k = 1$ 个。使用 $b = [\\,1,2,3,4,5,6\\,]^{\\top}$。\n  3. 两个不同特征值，重数均衡：$A = \\operatorname{diag}([\\,1,1,1,1,1,9,9,9,9,9\\,])$，因此在 $n = 10$ 维空间中有 $k = 2$ 个。使用 $b = [\\,1,2,3,4,5,6,7,8,9,10\\,]^{\\top}$。\n  4. 微妙的边缘情况，有三个不同特征值，但在一个特征空间上的投影为零：$A = \\operatorname{diag}([\\,2,2,2,4,4,4,8,8,8\\,])$，因此在 $n = 9$ 维空间中，矩阵层面有 $k = 3$ 个不同特征值，但设置 $b = [\\,1,1,1,0,0,0,2,2,2\\,]^{\\top}$，使得与特征值 $4$ 对应的分量恰好为零，从而使 $r_{0}$ 中存在的有效特征值数量等于 $2$。\n\n实现要求：\n- 共轭梯度例程必须显式实现，并且必须在密集的NumPy数组上操作。\n- 在所有测试中均使用 $x_{0} = 0$，对于每个大小为 $n$ 的问题，最大迭代限制为 $n$。\n- 停止准则是 $\\|r_{k}\\|_{2} \\le \\text{tol} \\cdot \\|r_{0}\\|_{2}$，其中 $\\text{tol} = 10^{-12}$。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含用方括号括起来的逗号分隔列表形式的结果，不含空格。具体来说，其格式必须为 $[\\text{it}_{1},\\text{it}_{2},\\text{it}_{3},\\text{it}_{4}]$，其中 $\\text{it}_{j}$ 是共轭梯度法用于测试用例 $j$ 的整数迭代次数。\n\n不允许外部输入。代码必须是自包含的，并且可以按原样运行。", "solution": "该问题是有效的。这是一个在数值线性代数领域中提法明确且有科学依据的问题，要求对共轭梯度法的一个基本性质进行理论推导和计算验证。所有必要的数据和条件都已提供，问题没有歧义、矛盾或科学上的不健全之处。\n\n我们的任务是从第一性原理出发，推导求解线性系统 $A x = b$ 的共轭梯度（CG）法的收敛行为，其中 $A \\in \\mathbb{R}^{n \\times n}$ 是一个具有 $k$ 个不同特征值的对称正定（SPD）矩阵。然后我们将通过数值方法验证这一行为。\n\n提供的基本原理是：\n1.  一个SPD矩阵 $A$ 满足对于所有非零 $x \\in \\mathbb{R}^{n}$ 都有 $x^{\\top} A x > 0$ 且 $A = A^{\\top}$。\n2.  Krylov子空间定义为 $\\mathcal{K}_{j}(A,r_{0}) = \\operatorname{span}\\{r_{0}, A r_{0}, \\dots, A^{j-1} r_{0}\\}$，其中 $r_{0} = b - A x_{0}$ 是初始残差。\n3.  CG迭代量 $x_{j}$ 从仿射子空间 $x_{0} + \\mathcal{K}_{j}(A,r_{0})$ 中选择，以最小化误差的 $A$-范数 $\\|e_{j}\\|_{A} = \\sqrt{e_{j}^{\\top} A e_{j}}$，其中 $e_{j} = x_{\\star} - x_{j}$ 且 $x_{\\star} = A^{-1}b$ 是精确解。这等价于Galerkin条件，即残差 $r_{j} = b - Ax_{j}$ 与子空间 $\\mathcal{K}_{j}(A,r_{0})$ 正交。\n\n让我们开始推导。第 $j$ 次迭代量 $x_{j}$ 位于 $x_{0} + \\mathcal{K}_{j}(A,r_{0})$ 中，这意味着向量 $x_{j} - x_{0}$ 是Krylov子空间基向量的线性组合。因此，我们可以将 $x_{j} - x_{0}$ 写成一个作用于 $r_0$ 的、关于 $A$ 的最高次数为 $j-1$ 的多项式：\n$$\nx_{j} - x_{0} = P_{j-1}(A) r_{0}\n$$\n其中 $P_{j-1}$ 是一个最高次数为 $j-1$ 的多项式。\n\n第 $j$ 次迭代的误差是 $e_j = x_\\star - x_j$。我们可以用初始误差 $e_0 = x_\\star - x_0$ 来表示它。\n$$\ne_{j} = x_{\\star} - (x_{0} + P_{j-1}(A)r_{0}) = (x_{\\star} - x_{0}) - P_{j-1}(A)r_{0} = e_{0} - P_{j-1}(A)r_{0}\n$$\n初始残差通过 $r_0 = b - Ax_0 = A x_\\star - A x_0 = A(x_\\star - x_0) = A e_0$ 与初始误差相关。将此代入 $e_j$ 的表达式中：\n$$\ne_{j} = e_{0} - P_{j-1}(A) A e_{0} = (I - A P_{j-1}(A)) e_{0}\n$$\n让我们定义一个新的多项式 $Q_{j}(t) = 1 - t P_{j-1}(t)$。这个多项式 $Q_{j}$ 的次数最多为 $j$，并且根据其构造，它满足 $Q_{j}(0) = 1$。通过这个定义，误差可以紧凑地写成：\n$$\ne_{j} = Q_{j}(A) e_{0}\n$$\nCG方法的核心属性是它选择迭代量 $x_j$（从而选择多项式 $P_{j-1}$，或等价地选择 $Q_j$）来最小化误差的 $A$-范数 $\\|e_j\\|_A$。因此，CG算法在第 $j$ 步生成的多项式 $Q_j$ 是以下最小化问题的解：\n$$\n\\|e_{j}\\|_{A} = \\min_{\\substack{Q \\in \\mathcal{P}_j \\\\ Q(0)=1}} \\|Q(A) e_{0}\\|_{A}\n$$\n其中 $\\mathcal{P}_j$ 是所有次数最多为 $j$ 的多项式的集合。\n\n现在，让我们利用 $A$ 的谱性质来分析这个最小化问题。由于 $A$ 是对称的，它有一套完备的正交归一特征向量 $v_1, \\dots, v_n$，对应着实特征值 $\\lambda_1, \\dots, \\lambda_n$。由于 $A$ 是正定的，所有 $\\lambda_i > 0$。我们可以在这个特征向量基上展开初始误差 $e_0$：$e_{0} = \\sum_{i=1}^{n} c_i v_i$。\n\n误差 $e_j = Q_j(A)e_0$ 的 $A$-范数平方是：\n$$\n\\|e_{j}\\|_{A}^2 = e_{j}^{\\top} A e_{j} = (Q_{j}(A)e_{0})^{\\top} A (Q_{j}(A)e_{0}) = e_{0}^{\\top} Q_{j}(A)^{\\top} A Q_{j}(A) e_{0}\n$$\n由于 $A$ 是对称的，任何关于 $A$ 的多项式也是对称的，所以 $Q_{j}(A)^{\\top} = Q_{j}(A)$。这得到：\n$$\n\\|e_{j}\\|_{A}^2 = e_{0}^{\\top} A Q_{j}(A)^2 e_{0} = \\left(\\sum_{i=1}^{n} c_i v_i\\right)^{\\top} A Q_{j}(A)^2 \\left(\\sum_{l=1}^{n} c_l v_l\\right)\n$$\n使用 $A v_i = \\lambda_i v_i$ 和 $Q_j(A)v_i = Q_j(\\lambda_i)v_i$，以及特征向量的正交归一性（$v_i^\\top v_l = \\delta_{il}$）：\n$$\n\\|e_{j}\\|_{A}^2 = \\left(\\sum_{i=1}^{n} c_i v_i\\right)^{\\top} \\left(\\sum_{l=1}^{n} c_l \\lambda_l Q_{j}(\\lambda_l)^2 v_l\\right) = \\sum_{i=1}^{n} c_i^2 \\lambda_i Q_{j}(\\lambda_i)^2\n$$\n现在，假设矩阵 $A$ 恰有 $k$ 个不同的特征值，我们记为 $\\mu_1, \\mu_2, \\dots, \\mu_k$。我们想证明CG算法最多在 $k$ 次迭代内收敛。这需要证明 $e_k = 0$。根据CG的最小化性质，如果我们能找到*任何*一个满足 $\\tilde{Q}(0)=1$ 的多项式 $\\tilde{Q} \\in \\mathcal{P}_k$ 使得误差为零，那么由CG找到的多项式 $Q_k$ 将导致一个小于或等于零的误差范数，这意味着误差本身必须为零。\n\n让我们构造这样一个多项式。考虑多项式 $m_A(t) = \\prod_{i=1}^{k} (t - \\mu_i)$，它是 $A$ 的最小多项式。它的次数为 $k$。我们需要一个在每个 $\\mu_i$ 处为零但在 $t=0$ 处等于 $1$ 的多项式。我们可以将此多项式定义为：\n$$\n\\tilde{Q}_{k}(t) = \\frac{\\prod_{i=1}^{k} (t - \\mu_i)}{\\prod_{i=1}^{k} (0 - \\mu_i)} = \\prod_{i=1}^{k} \\frac{t - \\mu_i}{-\\mu_i} = \\prod_{i=1}^{k} \\left(1 - \\frac{t}{\\mu_i}\\right)\n$$\n这个多项式 $\\tilde{Q}_{k}(t)$ 的次数为 $k$ 且满足 $\\tilde{Q}_{k}(0) = 1$。关键的是，对于 $A$ 的任何特征值 $\\lambda_j$，$\\lambda_j$ 必定是不同特征值 $\\mu_1, \\dots, \\mu_k$ 中的一个。因此，对于所有 $j=1, \\dots, n$，都有 $\\tilde{Q}_{k}(\\lambda_j) = 0$。\n\n让我们在第 $j=k$ 步评估与此多项式对应的误差范数：\n$$\n\\|\\tilde{Q}_{k}(A) e_{0}\\|_{A}^2 = \\sum_{i=1}^{n} c_i^2 \\lambda_i \\tilde{Q}_{k}(\\lambda_i)^2 = \\sum_{i=1}^{n} c_i^2 \\lambda_i (0)^2 = 0\n$$\n这意味着 $\\tilde{Q}_{k}(A)e_0 = 0$。由于CG算法在第 $k$ 步找到最小化此误差范数的多项式 $Q_k$，我们必然有：\n$$\n\\|e_k\\|_A^2 = \\|Q_k(A)e_0\\|_A^2 \\le \\|\\tilde{Q}_k(A)e_0\\|_A^2 = 0\n$$\n由于对于SPD矩阵，$A$-范数是一个真范数，所以 $\\|e_k\\|_A = 0$ 当且仅当 $e_k = 0$。因此，在第 $k$ 步，误差为零，这意味着 $x_k = x_\\star$，即精确解。这完成了证明：在精确算术中，CG最多在 $k$ 次迭代内收敛，其中 $k$ 是 $A$ 的不同特征值的数量。这个论证隐含地假设了初始误差 $e_0$（因此也包括 $r_0$）在所有 $k$ 个不同特征值的特征空间上都有非零投影（即所有相关的 $c_i \\neq 0$）。\n\n现在，我们来论证如果初始残差 $r_0$（因此 $e_0=A^{-1}r_0$）在与某些不同特征值对应的特征空间上的投影为零时会发生什么。设 $A$ 的 $k$ 个不同特征值的集合为 $\\Sigma = \\{\\mu_1, \\dots, \\mu_k\\}$。假设 $e_0$ 的构造使其与这些特征值的一个子集所关联的特征空间正交。这意味着在特征向量展开式 $e_0 = \\sum c_i v_i$ 中，对于其特征值 $\\lambda_i$ 属于此子集的任何特征向量 $v_i$，其系数 $c_i$ 均为零。设对应系数 $c_i$ 不全为零的不同特征值的集合为 $\\Sigma' = \\{\\mu'_{1}, \\dots, \\mu'_{k'}\\} \\subset \\Sigma$，其中 $k'  k$。现在，误差范数的求和只对与 $\\Sigma'$ 中特征值对应的索引 $i$ 进行：\n$$\n\\|e_j\\|_A^2 = \\sum_{\\lambda_i \\in \\Sigma'} c_i^2 \\lambda_i Q_j(\\lambda_i)^2\n$$\n为了使这个和为零，我们不再需要一个在所有 $\\Sigma$ 上为零的多项式，而只需要一个在 $\\Sigma'$ 上为零的多项式。我们可以构造一个次数为 $k'$ 的这样的多项式：\n$$\n\\tilde{Q}_{k'}(t) = \\prod_{i=1}^{k'} \\left(1 - \\frac{t}{\\mu'_i}\\right)\n$$\n这个多项式的次数为 $k'$，满足 $\\tilde{Q}_{k'}(0)=1$，并且对于所有 $\\mu'_i \\in \\Sigma'$ 的特征值都为零。与前面的论证相同，这表明在第 $j=k'$ 步，CG会找到一个解 $x_{k'}$ 使得 $e_{k'}=0$。因此，CG最多在 $k'$ 次迭代内收敛，其中 $k'$ 是被初始残差向量 $r_0$ “看到”的 $A$ 的不同特征值的数量。测试用例4旨在展示这一特定现象。初始残差 $r_0=b$ 仅在对应于特征值2和8的特征空间中有分量，因此预计在2步内收敛，而不是3步。\n\n我们现在将实现CG算法并测试这些理论预测。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef conjugate_gradient(A, b, x0, tol, max_iter):\n    \"\"\"\n    Solves the system of linear equations Ax = b using the Conjugate Gradient\n    method.\n\n    Args:\n        A (np.ndarray): A symmetric positive-definite square matrix.\n        b (np.ndarray): The right-hand side vector.\n        x0 (np.ndarray): The initial guess for the solution.\n        tol (float): The relative tolerance for the residual norm.\n        max_iter (int): The maximum number of iterations allowed.\n\n    Returns:\n        int: The number of iterations performed.\n    \"\"\"\n    x = x0.copy()\n    r = b - A @ x\n    \n    norm_r0 = np.linalg.norm(r)\n    \n    # If the initial guess is already the solution, 0 iterations.\n    if norm_r0 == 0:\n        return 0\n        \n    p = r.copy()\n    rs_old = np.dot(r, r)\n    \n    for i in range(max_iter):\n        Ap = A @ p\n        alpha = rs_old / np.dot(p, Ap)\n        \n        x = x + alpha * p\n        r = r - alpha * Ap\n        \n        rs_new = np.dot(r, r)\n        \n        # Check the stopping criterion\n        if np.sqrt(rs_new) = tol * norm_r0:\n            return i + 1\n            \n        p = r + (rs_new / rs_old) * p\n        rs_old = rs_new\n        \n    # Return max_iter if convergence was not reached within the limit.\n    return max_iter\n\ndef solve():\n    \"\"\"\n    Sets up and runs the four test cases for the Conjugate Gradient method\n    to verify its convergence properties related to distinct eigenvalues.\n    \"\"\"\n    tol = 1e-12\n\n    # Test Case 1: 3 distinct eigenvalues\n    # n=8, k=3\n    diag1 = [2]*3 + [5]*3 + [11]*2\n    A1 = np.diag(diag1)\n    b1 = np.ones(A1.shape[0])\n    x0_1 = np.zeros(A1.shape[0])\n    \n    # Test Case 2: 1 distinct eigenvalue\n    # n=6, k=1\n    diag2 = [7]*6\n    A2 = np.diag(diag2)\n    b2 = np.arange(1, A2.shape[0] + 1)\n    x0_2 = np.zeros(A2.shape[0])\n\n    # Test Case 3: 2 distinct eigenvalues\n    # n=10, k=2\n    diag3 = [1]*5 + [9]*5\n    A3 = np.diag(diag3)\n    b3 = np.arange(1, A3.shape[0] + 1)\n    x0_3 = np.zeros(A3.shape[0])\n    \n    # Test Case 4: 3 distinct eigenvalues, but r0 is in a 2-eigenvalue subspace\n    # n=9, matrix k=3, effective k=2\n    diag4 = [2]*3 + [4]*3 + [8]*3\n    A4 = np.diag(diag4)\n    b4 = np.array([1,1,1, 0,0,0, 2,2,2], dtype=float)\n    x0_4 = np.zeros(A4.shape[0])\n\n    test_cases = [\n        (A1, b1, x0_1),\n        (A2, b2, x0_2),\n        (A3, b3, x0_3),\n        (A4, b4, x0_4)\n    ]\n\n    results = []\n    for A, b, x0 in test_cases:\n        n = A.shape[0]\n        iterations = conjugate_gradient(A, b, x0, tol, max_iter=n)\n        results.append(iterations)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3216667"}, {"introduction": "在许多大规模科学计算问题中，矩阵 $A$ 过于庞大，无法显式存储在内存中。本练习将指导你实现一个“无矩阵”的 CG 求解器，这正是该方法在实际中的常用形式。通过仅利用矩阵与向量的乘积（即算子 $v \\mapsto Av$）来与矩阵交互，你将学习如何求解泊松方程等问题，从而体验共轭梯度法在真实世界应用中的强大威力与可扩展性。 [@problem_id:3216688]", "problem": "您的任务是为对称正定（SPD）线性系统设计并实现一个无矩阵（matrix-free）的共轭梯度（CG）求解器，该求解器仅使用一个实现矩阵向量乘积 $v \\mapsto Av$ 的函数，而从不显式地构造矩阵 $A$。您的求解器必须从基本原理出发：将 SPD 矩阵 $A$ 的 $Ax=b$ 求解问题视为一个最小化严格凸二次目标函数的过程，该函数的梯度即为残差。然后推导出一个算法，该算法迭代地寻找相对于 $A$ 相互共轭的方向，并沿这些方向执行线性搜索。您的实现必须严格地只使用由回调函数提供的 $Ap$ 形式的乘积。初始猜测必须是零向量。停止准则必须使用基于初始残差范数的相对残差准则：在第一个满足 $\\lVert r_k \\rVert_2 \\le \\text{tol} \\cdot \\lVert r_0 \\rVert_2$ 的迭代 $k$ 处停止，其中 $r_k = b - Ax_k$，$r_0$ 是初始残差。如果 $\\lVert r_0 \\rVert_2 = 0$，求解器必须立即返回，迭代次数为零。求解器还必须遵守用户指定的最大迭代次数。\n\n您的程序必须将您的求解器应用于以下四个测试用例，每个用例都纯粹通过矩阵向量乘积来描述。在每个测试中，还需从一个已知的精确解 $x_{\\star}$ 构造右端项 $b = A x_{\\star}$，以便您可以测量计算解的误差。在所有三角函数表达式中，使用以弧度为单位的角度。\n\n测试用例 1（一维泊松算子）：令 $n = 10$。通过对任意 $x \\in \\mathbb{R}^n$ 的作用隐式定义 $A$，得到 $y = Ax \\in \\mathbb{R}^n$，其分量由 $y_i = 2 x_i - x_{i-1} - x_{i+1}$ 给出，并约定 $x_0 = 0$ 和 $x_{n+1} = 0$。这对应于主对角线为 $2$、次对角线为 $-1$ 的标准三对角算子（狄利克雷边界条件）。定义精确解为 $x_{\\star,i} = \\sin\\!\\left(\\pi i/(n+1)\\right)$，其中 $i = 1,2,\\dots,n$，并设置 $b = A x_{\\star}$。使用容差 $\\text{tol} = 10^{-12}$ 和最大迭代次数 $n$。报告所用的迭代次数和最大绝对误差 $\\max_i |x_i - x_{\\star,i}|$。\n\n测试用例 2（零右端项）：重用测试用例 1 中的算子 $A$ 和大小 $n = 10$。令 $b$ 为 $\\mathbb{R}^n$ 中的零向量，令 $x_{\\star}$ 为零向量。使用容差 $\\text{tol} = 10^{-12}$ 和最大迭代次数 $n$。报告迭代次数和最大绝对误差。\n\n测试用例 3（单位算子）：令 $n = 50$。对于所有 $x \\in \\mathbb{R}^n$，定义 $A$ 为 $Ax = x$。令精确解为 $x_{\\star,i} = \\sin(i)$，其中 $i = 1,2,\\dots,n$，并设置 $b = A x_{\\star} = x_{\\star}$。使用容差 $\\text{tol} = 10^{-12}$ 和最大迭代次数 $n$。报告迭代次数和最大绝对误差。\n\n测试用例 4（二维泊松算子）：令 $n_x = 20$ 和 $n_y = 20$，并定义 $N = n_x n_y$。将向量 $x \\in \\mathbb{R}^N$ 表示为行主序的数组 $X \\in \\mathbb{R}^{n_y \\times n_x}$。通过带有齐次狄利克雷边界条件的五点拉普拉斯算子隐式定义 $A$：对于内部索引 $(i,j)$（$1 \\le i \\le n_y$，$1 \\le j \\le n_x$），定义 $(AX)_{i,j} = 4 X_{i,j} - X_{i-1,j} - X_{i+1,j} - X_{i,j-1} - X_{i,j+1}$，并约定网格外的任何邻居贡献为零。然后，$(AX)$ 被展平回 $\\mathbb{R}^N$ 中的一个向量。取 $x_{\\star}$ 为 $\\mathbb{R}^N$ 中的全一向量，并设置 $b = A x_{\\star}$。使用容差 $\\text{tol} = 10^{-8}$ 和最大迭代次数 $N$。报告迭代次数和最大绝对误差。\n\n您的程序必须实现一个单一的求解器，通过接收一个计算 $v \\mapsto Av$ 的可调用对象、一个右端项 $b$、一个容差和一个最大迭代次数来处理上述所有情况，并返回近似解和迭代次数。对于每个测试，按指定顺序计算报告的配对：首先是迭代次数，然后是最大绝对误差。\n\n最终输出格式：您的程序应生成一行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果，每个元素是对应于测试用例 1 到 4 的一个双元素列表。例如：[[i1,e1],[i2,e2],[i3,e3],[i4,e4]]，其中 $i\\ell$ 是测试用例 $\\ell$ 的迭代次数，$e\\ell$ 是其最大绝对误差。", "solution": "该问题是有效的。它要求从基本原理出发，设计并实现一个无矩阵的共轭梯度（CG）求解器，并将其应用于四个来自数值线性代数的、定义明确且科学上合理的测试用例。问题陈述是自包含的、一致的，并提供了所有必要的参数和规范。\n\n共轭梯度法是一种用于求解线性方程组 $Ax=b$ 的迭代算法，其中矩阵 $A$ 是对称正定（SPD）的。该问题指定求解器应为“无矩阵的”，这意味着它不能显式构造矩阵 $A$。相反，它应仅依赖于一个函数，该函数为任何给定向量 $v$ 计算矩阵向量乘积 $v \\mapsto Av$。\n\n系统 $Ax=b$ 的解等价于严格凸二次目标函数的唯一最小化子：\n$$f(x) = \\frac{1}{2}x^\\top A x - b^\\top x$$\n该函数的梯度 $\\nabla f(x) = Ax - b$ 是残差向量 $r = b - Ax$ 的负值。在解 $x_{\\star}$ 处，梯度为零，即 $Ax_{\\star} - b = 0$。\n\nCG 方法生成一系列收敛到 $x_{\\star}$ 的近似解 $x_0, x_1, \\dots$。从初始猜测 $x_0$ 开始，每个后续的迭代解都是通过沿选定的搜索方向 $p_k$ 移动得到的：\n$$x_{k+1} = x_k + \\alpha_k p_k$$\n步长 $\\alpha_k$ 的选择是为了最小化目标函数在方向 $p_k$ 上的值。这是一个一维最小化问题，通过将 $f(x_k + \\alpha p_k)$ 对 $\\alpha$ 的导数设为零来求解：\n$$\\frac{d}{d\\alpha} f(x_k + \\alpha p_k) = \\nabla f(x_{k+1})^\\top p_k = (A(x_k + \\alpha_k p_k) - b)^\\top p_k = (Ax_k - b + \\alpha_k Ap_k)^\\top p_k = (-r_k + \\alpha_k A p_k)^\\top p_k = 0$$\n求解 $\\alpha_k$ 可得到最优步长：\n$$\\alpha_k = \\frac{r_k^\\top p_k}{p_k^\\top A p_k}$$\n\nCG方法的核心创新在于搜索方向 $p_k$ 的选择。它们被选择为相互 $A$-共轭的，即满足条件：\n$$p_i^\\top A p_j = 0 \\quad \\text{for} \\quad i \\neq j$$\n此性质确保了在步骤 $k$ 中沿方向 $p_k$ 进行的最小化不会干扰先前步骤中已实现的最小化。\n\n搜索方向是迭代构造的。第一个搜索方向 $p_0$ 取为初始残差 $r_0 = b - Ax_0$，这是从 $x_0$ 开始的最速下降方向。每个后续方向 $p_{k+1}$ 由新残差 $r_{k+1}$ 构造，使其与前一个方向 $p_k$ 是 $A$-共轭的。这通过设置以下方式实现：\n$$p_{k+1} = r_{k+1} + \\beta_k p_k$$\n选择系数 $\\beta_k$ 来强制实现 $A$-共轭性，$p_{k+1}^\\top A p_k = 0$。代入 $p_{k+1}$ 的表达式可得：\n$$(r_{k+1} + \\beta_k p_k)^\\top A p_k = r_{k+1}^\\top A p_k + \\beta_k p_k^\\top A p_k = 0 \\implies \\beta_k = -\\frac{r_{k+1}^\\top A p_k}{p_k^\\top A p_k}$$\n通过涉及残差和搜索方向性质的代数操作，$\\beta_k$ 和 $\\alpha_k$ 的这个表达式可以被简化，从而得到一个非常高效的算法。一个关键性质是残差是相互正交的，即当 $i \\ne j$ 时 $r_i^\\top r_j = 0$。这导致了关系式 $p_k^\\top r_k = r_k^\\top r_k$ 以及一个计算上更简便的 $\\beta_k$ 公式。\n\n完整的算法流程如下，从指定的初始猜测 $x_0 = 0$ 开始：\n\n1.  初始化：\n    $k = 0$\n    $x_0 = 0$\n    $r_0 = b - Ax_0 = b$\n    $p_0 = r_0$\n    $\\rho_0 = r_0^\\top r_0$\n\n2.  检查平凡解：如果 $\\lVert r_0 \\rVert_2 = \\sqrt{\\rho_0} = 0$，则解为 $x_0 = 0$。以 $0$ 次迭代终止。否则，计算停止容差阈值 $\\tau = \\text{tol} \\cdot \\lVert r_0 \\rVert_2$。\n\n3.  迭代 $k = 0, 1, 2, \\dots, \\text{max\\_iter}-1$：\n    a. 计算矩阵向量乘积：$v_k = A p_k$。\n    b. 计算步长：$\\alpha_k = \\frac{\\rho_k}{p_k^\\top v_k}$。\n    c. 更新解：$x_{k+1} = x_k + \\alpha_k p_k$。\n    d. 更新残差：$r_{k+1} = r_k - \\alpha_k v_k$。\n    e. 计算新残差的范数平方：$\\rho_{k+1} = r_{k+1}^\\top r_{k+1}$。\n    f. 检查收敛性：如果 $\\sqrt{\\rho_{k+1}} \\le \\tau$，则终止并返回 $x_{k+1}$ 和迭代次数 $k+1$。\n    g. 更新搜索方向：\n       i. $\\beta_k = \\frac{\\rho_{k+1}}{\\rho_k}$\n       ii. $p_{k+1} = r_{k+1} + \\beta_k p_k$\n    h. 为下一次迭代做准备：$\\rho_k \\leftarrow \\rho_{k+1}$。\n\n4.  如果循环完成但未收敛，则返回当前解 $x_{\\text{max\\_iter}}$ 和迭代次数 $\\text{max\\_iter}$。\n\n该算法每次迭代仅需一次矩阵向量乘积，使其非常适合无矩阵应用。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef conjugate_gradient(matvec, b, tol, max_iter):\n    \"\"\"\n    Implements a matrix-free Conjugate Gradient (CG) solver.\n\n    Solves the symmetric positive-definite linear system Ax = b using the CG\n    method. The matrix A is implicitly provided via a function `matvec` that\n    computes the matrix-vector product Av.\n\n    Args:\n        matvec (callable): A function that takes a vector v and returns Av.\n        b (np.ndarray): The right-hand side vector of the linear system.\n        tol (float): The relative tolerance for the residual norm stopping criterion.\n        max_iter (int): The maximum number of iterations allowed.\n\n    Returns:\n        tuple: A tuple containing:\n            - np.ndarray: The approximate solution vector x.\n            - int: The number of iterations performed.\n    \"\"\"\n    x = np.zeros_like(b, dtype=float)\n    r = b.copy()  # Since x_0 is zero, r_0 = b - A*0 = b\n    p = r.copy()\n\n    rs_old_sq = np.dot(r, r)\n    r0_norm = np.sqrt(rs_old_sq)\n\n    if r0_norm == 0.0:\n        return x, 0\n\n    stop_norm = tol * r0_norm\n\n    for i in range(max_iter):\n        Ap = matvec(p)\n        alpha = rs_old_sq / np.dot(p, Ap)\n\n        x += alpha * p\n        r -= alpha * Ap\n\n        rs_new_sq = np.dot(r, r)\n\n        if np.sqrt(rs_new_sq) = stop_norm:\n            return x, i + 1\n\n        p = r + (rs_new_sq / rs_old_sq) * p\n        rs_old_sq = rs_new_sq\n\n    return x, max_iter\n\ndef solve():\n    \"\"\"\n    Sets up and runs the four test cases for the CG solver.\n    \"\"\"\n    # Test case 1: 1D Poisson operator\n    n1 = 10\n    def matvec1(x):\n        y = np.zeros_like(x)\n        y[0] = 2.0 * x[0] - x[1]\n        y[-1] = 2.0 * x[-1] - x[-2]\n        y[1:-1] = 2.0 * x[1:-1] - x[:-2] - x[2:]\n        return y\n    idx1 = np.arange(1, n1 + 1)\n    x_star1 = np.sin(np.pi * idx1 / (n1 + 1))\n    b1 = matvec1(x_star1)\n    tol1 = 1e-12\n    max_iter1 = n1\n\n    # Test case 2: Zero right-hand side\n    n2 = 10\n    def matvec2(x): # Same operator as case 1\n        y = np.zeros_like(x)\n        y[0] = 2.0 * x[0] - x[1]\n        y[-1] = 2.0 * x[-1] - x[-2]\n        y[1:-1] = 2.0 * x[1:-1] - x[:-2] - x[2:]\n        return y\n    x_star2 = np.zeros(n2)\n    b2 = np.zeros(n2)\n    tol2 = 1e-12\n    max_iter2 = n2\n\n    # Test case 3: Identity operator\n    n3 = 50\n    def matvec3(x):\n        return x\n    idx3 = np.arange(1, n3 + 1)\n    x_star3 = np.sin(idx3)\n    b3 = x_star3.copy()  # Since A is identity, b = x_star\n    tol3 = 1e-12\n    max_iter3 = n3\n\n    # Test case 4: 2D Poisson operator\n    nx4, ny4 = 20, 20\n    N4 = nx4 * ny4\n    def matvec4(x_flat):\n        X = x_flat.reshape((ny4, nx4))\n        Y = 4.0 * X\n        Y[1:, :] -= X[:-1, :]    # Subtract contribution from neighbor above\n        Y[:-1, :] -= X[1:, :]   # Subtract contribution from neighbor below\n        Y[:, 1:] -= X[:, :-1]   # Subtract contribution from neighbor left\n        Y[:, :-1] -= X[:, 1:]   # Subtract contribution from neighbor right\n        return Y.flatten()\n    x_star4 = np.ones(N4)\n    b4 = matvec4(x_star4)\n    tol4 = 1e-8\n    max_iter4 = N4\n\n    test_cases = [\n        (matvec1, b1, tol1, max_iter1, x_star1),\n        (matvec2, b2, tol2, max_iter2, x_star2),\n        (matvec3, b3, tol3, max_iter3, x_star3),\n        (matvec4, b4, tol4, max_iter4, x_star4),\n    ]\n\n    results = []\n    for matvec, b, tol, max_iter, x_star in test_cases:\n        x_sol, iters = conjugate_gradient(matvec, b, tol, max_iter)\n        error = np.max(np.abs(x_sol - x_star))\n        results.append([iters, error])\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3216688"}]}