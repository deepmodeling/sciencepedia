{"hands_on_practices": [{"introduction": "高效求解大型稀疏系统需要专门的数据结构。本练习将指导你使用压缩稀疏行（CSR）格式实现共轭梯度（CG）方法，这是一种存储稀疏矩阵的工业标准。你将构建核心的矩阵向量乘法函数，它是大多数迭代求解器的心脏，从而将理论付诸实践。[@problem_id:3244695]", "problem": "您的任务是实现一个用于稀疏线性系统的迭代求解器，该求解器直接对以压缩稀疏行（CSR）格式存储的矩阵进行操作。重点是为CSR结构正确实现矩阵向量乘积，然后在一个迭代方法中使用它来求解一个对称正定线性系统。目标是求解形如 $A x = b$ 的方程组，其中 $A$ 是一个大型稀疏矩阵，并通过残差范数报告计算解的质量。\n\n推导的基本依据：从有限维实向量空间上的线性系统 $A x = b$ 的定义出发，假设 $A$ 是对称正定的，并使用欧几里得内积和范数。使用以下经过充分检验的事实：最小化一个严格凸的二次泛函可以得到唯一解，并且可以构造克雷洛夫子空间方法来生成相对于 $A$ 共轭的搜索方向。\n\n您必须实现：\n- 一个以压缩稀疏行（CSR）格式表示的稀疏矩阵，由三个数组定义：$data$、$indices$ 和 $indptr$，其中 $indptr$ 标定行，$indices$ 存储每个非零元素的列索引，$data$ 存储相应的非零值。\n- 一个函数，仅使用CSR表示来计算矩阵向量乘积 $y = A x$，而不将 $A$ 转换为任何密集结构或使用任何外部稀疏线性代数库函数。\n- 一个迭代求解器，使用矩阵向量乘积来求解 $A x = b$ 以得到 $x$，其终止条件基于相对残差范数准则 $\\|r_k\\|_2 / \\|b\\|_2 \\le \\varepsilon$，其中 $r_k = b - A x_k$，$\\varepsilon$ 是一个预设的容差。使用共轭梯度（CG）法，该方法适用于对称正定矩阵。\n\n约束条件：\n- 您不得将 $A$ 构建为密集数组，也不得使用任何密集矩阵乘法来计算 $A x$。所有乘法必须通过您实现的CSR矩阵向量乘积来执行。\n- 您的实现对于所提供的测试用例应保持数值稳定，并能正确处理稀疏模式。\n- 除非另有规定，否则使用初始猜测值 $x_0 = 0$。\n- 对残差使用欧几里得二范数 $\\|\\cdot\\|_2$，并使用相对残差准则 $\\|r_k\\|_2 / \\|b\\|_2 \\le \\varepsilon$作为停止条件。\n\n测试套件：\n在您的程序中实现以下三个测试用例。对于每个用例，计算终止时的最终相对残差范数 $\\|r_k\\|_2 / \\|b\\|_2$。\n\n- 测试用例1（理想情况，结构化对称正定系统）：设 $A$ 是在大小为 $3 \\times 3$ 的内部网格上，对二维拉普拉斯算子使用狄利克雷边界条件进行的五点有限差分离散化。对于每个内部点，将其对角线元素设置为 $4$，并将其有效上、下、左、右邻居对应的元素设置为 $-1$。这将产生一个 $n \\times n$ 的矩阵，其中 $n = 9$。设 $b$ 是所有元素均为 $1$ 的 $n$ 维向量。使用容差 $\\varepsilon = 10^{-10}$ 和最大迭代次数 $k_{\\max} = 1000$。\n\n- 测试用例2（边界情况，纯对角对称正定系统）：设 $A$ 为对角矩阵，其元素为 $[2, 3, 5, 7]$，因此 $n = 4$。设 $b = [1, 2, 3, 4]$。使用容差 $\\varepsilon = 10^{-12}$ 和最大迭代次数 $k_{\\max} = 100$。\n\n- 测试用例3（病态对称正定系统）：设 $A$ 为对角矩阵，其元素为 $[10^{-6}, 10^{-3}, 1, 10^{3}, 10^{6}]$，因此 $n = 5$。设 $b = [1, 1, 1, 1, 1]$。使用容差 $\\varepsilon = 10^{-12}$ 和最大迭代次数 $k_{\\max} = 200$。\n\n最终输出格式：\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表的结果，按测试用例的顺序排列，例如 $[result_1,result_2,result_3]$，其中每个 $result_i$ 是测试用例 $i$ 的最终相对残差范数。输出必须是实数（浮点值）。不应打印任何附加文本。", "solution": "该问题是有效的。这是一个适定的、具有科学依据的数值线性代数问题，要求实现共轭梯度算法来求解存储在压缩稀疏行（CSR）格式中的稀疏对称正定线性系统。所有参数和测试用例都定义清晰，构成了科学计算中的一个标准练习。\n\n任务是求解线性方程组 $A x = b$，其中 $A \\in \\mathbb{R}^{n \\times n}$ 是一个稀疏、对称且正定（SPD）的矩阵，$x \\in \\mathbb{R}^n$ 是未知向量，$b \\in \\mathbb{R}^n$ 是一个给定向量。\n\n该系统的解等价于找到二次泛函 $f(x) = \\frac{1}{2} x^T A x - x^T b$ 的唯一最小化子。该泛函的梯度是 $\\nabla f(x) = A x - b$，在解处为零。由于 $A$ 是对称正定的， $f(x)$ 的海森矩阵（即 $A$）是正定的，这证实了 $f(x)$ 是严格凸的并且有唯一的最小值。\n\n迭代方法，特别是克雷洛夫子空间方法，非常适用于大型稀疏系统。共轭梯度（CG）法是对称正定（SPD）系统的标准选择。它构造了一系列相对于 $A$ 共轭的搜索方向 $\\{p_k\\}$ （即，对于 $i \\ne j$，$p_i^T A p_j = 0$），这保证了在精确算术中，最多 $n$ 次迭代即可收敛到精确解。\n\n任何用于稀疏系统的迭代方法的核心都是高效的矩阵向量乘积。我们首先定义压缩稀疏行（CSR）格式以及相应的矩阵向量乘积。\n\n一个稀疏矩阵 $A$ 以CSR格式由三个数组表示：\n1. `data`：一个实值数组，按行主序包含 $A$ 的所有非零元素。\n2. `indices`：一个整数数组，存储 `data` 数组中每个对应值的列索引。\n3. `indptr`（索引指针）：一个大小为 $n+1$ 的整数数组。第 $i$ 行的非零元素存储在 `data` 中，从索引 `indptr[i]` 开始，直到（但不包括）索引 `indptr[i+1]`。因此，第 $i$ 行的非零元素数量为 `indptr[i+1] - indptr[i]`。\n\n矩阵向量乘积 $y = A x$ 的计算方法如下。对于从 $0$ 到 $n-1$ 的每一行 $i$，分量 $y_i$ 由 $A$ 的第 $i$ 行与 $x$ 的点积给出。在CSR格式中，这表示为：\n$$ y_i = \\sum_{k=\\text{indptr}[i]}^{\\text{indptr}[i+1]-1} \\text{data}[k] \\cdot x_{\\text{indices}[k]} $$\n这种计算避免了存储或操作 $A$ 的零元素。\n\n共轭梯度算法的步骤如下：\n1. 初始化：\n    - 设置一个初始猜测值 $x_0$。根据问题要求，$x_0 = 0$。\n    - 计算初始残差：$r_0 = b - A x_0$。由于 $x_0 = 0$，$r_0 = b$。\n    - 设置初始搜索方向：$p_0 = r_0$。\n    - 计算初始残差的范数平方：$\\rho_0 = r_0^T r_0$。\n    - 计算 $b$ 的范数：$\\|b\\|_2$。\n2. 对 $k = 0, 1, 2, \\dots$ 进行迭代，直到收敛或达到最大迭代次数：\n    a. 计算矩阵向量乘积：$v_k = A p_k$。\n    b. 计算步长：$\\alpha_k = \\frac{\\rho_k}{p_k^T v_k}$。\n    c. 更新解向量：$x_{k+1} = x_k + \\alpha_k p_k$。\n    d. 更新残差：$r_{k+1} = r_k - \\alpha_k v_k$。\n    e. 检查收敛性：将相对残差范数与容差 $\\varepsilon$ 进行比较。如果 $\\|r_{k+1}\\|_2 / \\|b\\|_2 \\le \\varepsilon$，则算法终止。\n    f. 计算新残差的范数平方：$\\rho_{k+1} = r_{k+1}^T r_{k+1}$。\n    g. 计算搜索方向的改进因子：$\\beta_k = \\frac{\\rho_{k+1}}{\\rho_k}$。\n    h. 更新搜索方向：$p_{k+1} = r_{k+1} + \\beta_k p_k$。\n    i. 为下一次迭代设置 $\\rho_k \\leftarrow \\rho_{k+1}$。\n\n此过程将应用于三个指定的测试用例。\n\n测试用例1：矩阵 $A$ 是一个 $9 \\times 9$ 的矩阵，来自于在 $3 \\times 3$ 网格上对二维拉普拉斯算子使用五点中心差分格式的离散化。一个网格点 $(i, j)$ 其中 $i, j \\in \\{0, 1, 2\\}$ 被映射到一个单一索引 $k = 3i + j$。对于每一行 $k$，对角线元素 $A_{k,k}$ 是 $4$，而对应于有效的北、南、东、西邻居的元素是 $-1$。向量 $b$ 是一个全为一的向量。已知该矩阵是对称正定的。\n\n测试用例2：矩阵 $A$ 是一个 $4 \\times 4$ 的对角矩阵，所有对角线元素均为正，$A = \\text{diag}(2, 3, 5, 7)$。这显然是对称正定的。向量 $b$ 是 $[1, 2, 3, 4]^T$。\n\n测试用例3：矩阵 $A$ 是一个 $5 \\times 5$ 的对角矩阵，$A = \\text{diag}(10^{-6}, 10^{-3}, 1, 10^3, 10^6)$。它是对称正定的，但其条件数 $\\kappa_2(A) = \\frac{\\max(\\lambda_i)}{\\min(\\lambda_i)} = \\frac{10^6}{10^{-6}} = 10^{12}$ 非常大。这测试了求解器在病态问题上的性能。向量 $b$ 是一个全为一的向量。\n\n实现将以CSR格式构建这些矩阵并应用CG求解器，报告最终的相对残差范数。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to set up and run the test cases for the Conjugate Gradient solver.\n    \"\"\"\n\n    def generate_laplacian_csr(grid_size):\n        \"\"\"\n        Generates the CSR representation of the 5-point Laplacian on a 2D grid.\n        \"\"\"\n        n = grid_size * grid_size\n        data = []\n        indices = []\n        indptr = [0]\n        \n        for k in range(n):\n            i, j = k // grid_size, k % grid_size\n            \n            # Temporary storage for one row's non-zero entries\n            row_entries = {}\n            \n            # Diagonal element\n            row_entries[k] = 4.0\n            \n            # Neighbor connections\n            # North\n            if i > 0:\n                neighbor_idx = (i - 1) * grid_size + j\n                row_entries[neighbor_idx] = -1.0\n            # South\n            if i  grid_size - 1:\n                neighbor_idx = (i + 1) * grid_size + j\n                row_entries[neighbor_idx] = -1.0\n            # West\n            if j > 0:\n                neighbor_idx = i * grid_size + (j - 1)\n                row_entries[neighbor_idx] = -1.0\n            # East\n            if j  grid_size - 1:\n                neighbor_idx = i * grid_size + (j + 1)\n                row_entries[neighbor_idx] = -1.0\n\n            # Sort by column index and append to CSR lists\n            sorted_cols = sorted(row_entries.keys())\n            for col in sorted_cols:\n                data.append(row_entries[col])\n                indices.append(col)\n            indptr.append(len(data))\n            \n        return np.array(data, dtype=float), np.array(indices, dtype=int), np.array(indptr, dtype=int), n\n\n    def csr_matvec(n, data, indices, indptr, x):\n        \"\"\"\n        Computes the matrix-vector product y = A*x for a matrix in CSR format.\n        \"\"\"\n        y = np.zeros(n, dtype=float)\n        for i in range(n):\n            row_sum = 0.0\n            start = indptr[i]\n            end = indptr[i+1]\n            for k in range(start, end):\n                j = indices[k]\n                val = data[k]\n                row_sum += val * x[j]\n            y[i] = row_sum\n        return y\n\n    def conjugate_gradient(n, data, indices, indptr, b, tol, max_iter):\n        \"\"\"\n        Solves A*x = b using the Conjugate Gradient method for a CSR matrix.\n        \"\"\"\n        x = np.zeros(n, dtype=float)\n        # For x_0 = 0, the initial residual r_0 is b.\n        r = b.copy()\n        p = r.copy()\n        rs_old = np.dot(r, r)\n        norm_b = np.linalg.norm(b)\n\n        if norm_b == 0.0:\n            return 0.0\n\n        rel_res = np.sqrt(rs_old) / norm_b\n        if rel_res  tol:\n            return rel_res\n\n        for k in range(max_iter):\n            Ap = csr_matvec(n, data, indices, indptr, p)\n            \n            p_dot_Ap = np.dot(p, Ap)\n            # If p_dot_Ap is zero or negative, the matrix might not be SPD\n            # or we might have found the exact solution.\n            if p_dot_Ap = 0:\n                break\n                \n            alpha = rs_old / p_dot_Ap\n            x += alpha * p\n            r -= alpha * Ap\n            \n            rs_new = np.dot(r, r)\n            rel_res = np.sqrt(rs_new) / norm_b\n\n            if rel_res  tol:\n                break\n            \n            p = r + (rs_new / rs_old) * p\n            rs_old = rs_new\n        \n        # After loop, compute final residual for verification\n        final_residual_norm = np.linalg.norm(b - csr_matvec(n, data, indices, indptr, x))\n        final_rel_res = final_residual_norm / norm_b if norm_b > 0 else 0.0\n\n        return final_rel_res\n\n    # === Define the test cases from the problem statement. ===\n\n    # Test Case 1: 2D Laplacian on 3x3 grid\n    data1, indices1, indptr1, n1 = generate_laplacian_csr(grid_size=3)\n    b1 = np.ones(n1, dtype=float)\n    tol1 = 1e-10\n    max_iter1 = 1000\n\n    # Test Case 2: Diagonal system\n    n2 = 4\n    data2 = np.array([2.0, 3.0, 5.0, 7.0])\n    indices2 = np.array([0, 1, 2, 3], dtype=int)\n    indptr2 = np.array([0, 1, 2, 3, 4], dtype=int)\n    b2 = np.array([1.0, 2.0, 3.0, 4.0])\n    tol2 = 1e-12\n    max_iter2 = 100\n\n    # Test Case 3: Ill-conditioned diagonal system\n    n3 = 5\n    data3 = np.array([1e-6, 1e-3, 1.0, 1e3, 1e6])\n    indices3 = np.array([0, 1, 2, 3, 4], dtype=int)\n    indptr3 = np.array([0, 1, 2, 3, 4, 5], dtype=int)\n    b3 = np.ones(n3, dtype=float)\n    tol3 = 1e-12\n    max_iter3 = 200\n    \n    test_cases = [\n        (n1, data1, indices1, indptr1, b1, tol1, max_iter1),\n        (n2, data2, indices2, indptr2, b2, tol2, max_iter2),\n        (n3, data3, indices3, indptr3, b3, tol3, max_iter3),\n    ]\n\n    results = []\n    for case in test_cases:\n        n, data, indices, indptr, b, tol, max_iter = case\n        final_rel_res = conjugate_gradient(n, data, indices, indptr, b, tol, max_iter)\n        results.append(final_rel_res)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3244695"}, {"introduction": "共轭梯度法功能强大，但其收敛性保证依赖于矩阵的对称正定（SPD）性。本练习将挑战你探究当此条件不被满足时会发生什么。通过将CG方法应用于一个对称不定矩阵，你将能精确定位其算法失效的内在机理，从而更深刻地理解该算法的理论根基。[@problem_id:3244707]", "problem": "考虑求解形如 $A x = b$ 的线性系统，其中 $A \\in \\mathbb{R}^{n \\times n}$ 是对称矩阵，而 $b \\in \\mathbb{R}^{n}$。共轭梯度（CG）方法传统上应用于对称正定矩阵。当 $A$ 为对称正定矩阵时，其良定性基于对严格凸二次泛函 $\\varphi(x) = \\tfrac{1}{2} x^{T} A x - b^{T} x$ 的最小化。在这种情况下，所选的搜索方向是相互 A-共轭的，并且沿着每个搜索方向的步长被选择以强制下一个残差与当前搜索方向正交。对于对称但不定的矩阵，二次泛函不是严格凸的，并且某些曲率量可能变为非正值，这使得通常的CG步长无定义。\n\n您的任务是从第一性原理出发，为对称矩阵实现非预处理的共轭梯度（CG）迭代，并检测出该方法因非正曲率量而变得不良定义的精确迭代次数。该实现必须：\n\n- 对每个测试用例，从零初始猜测 $x_{0} = 0$ 开始。\n- 使用为对称正定矩阵沿 A-共轭方向最小化二次泛函而推导出的标准CG更新逻辑。\n- 在第 $k$ 次迭代时，计算步长之前，评估曲率量 $p_{k}^{T} A p_{k}$。如果 $p_{k}^{T} A p_{k} \\le 0$，算法必须立即停止并报告在第 $k$ 次迭代时失败。\n- 使用残差 $r_{k} = b - A x_{k}$ 的欧几里得范数 $\\|r_{k}\\|_{2}$ 作为成功收敛的停止准则。\n- 对残差范数收敛性测试使用容差 $\\varepsilon = 10^{-10}$。\n- 对于一个维度为 $n$ 的系统，最多使用 $m = n$ 次迭代。\n\n每个测试用例的返回值规范：\n\n- 如果方法成功收敛（即，对于某个 $k$，在未遇到 $p_{k}^{T} A p_{k} \\le 0$ 的情况下有 $\\|r_{k}\\|_{2} \\le \\varepsilon$），则返回一个等于首次满足容差时已完成迭代次数的正整数。\n- 如果方法在第 $k$ 次迭代时遇到 $p_{k}^{T} A p_{k} \\le 0$，则返回负整数 $-(k+1)$，它唯一地编码了失败的从零开始的迭代索引（这避免了与-0的歧义）。\n\n测试套件：\n\n- 测试用例 1（对称正定，预期成功收敛）：\n  - $A_{1} = \\begin{bmatrix} 4  1  0 \\\\ 1  3  1 \\\\ 0  1  2 \\end{bmatrix}$，\n  - $b_{1} = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix}$，\n  - $x_{0} = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\end{bmatrix}$，\n  - $\\varepsilon = 10^{-10}$，$m = 3$。\n- 测试用例 2（对称不定，开始时曲率除以零）：\n  - $A_{2} = \\begin{bmatrix} 1  0 \\\\ 0  -1 \\end{bmatrix}$，\n  - $b_{2} = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}$，\n  - $x_{0} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$，\n  - $\\varepsilon = 10^{-10}$，$m = 2$。\n- 测试用例 3（对称不定，后期出现负曲率）：\n  - $A_{3} = \\begin{bmatrix} 2  0 \\\\ 0  -1 \\end{bmatrix}$，\n  - $b_{3} = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}$，\n  - $x_{0} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$，\n  - $\\varepsilon = 10^{-10}$，$m = 2$。\n\n此问题不适用角度单位和物理单位。\n\n最终输出格式：\n\n- 您的程序应生成单行输出，其中包含用方括号括起来的逗号分隔的结果列表。该列表必须按顺序包含与三个测试用例相对应的三个整数。例如，一个有效的输出可以是 $[a,b,c]$，其中 $a$、$b$ 和 $c$ 是为每个测试用例定义的如上所述的整数。", "solution": "用户提供的问题是数值线性代数中一个良定且有科学依据的练习，要求实现共轭梯度（CG）方法，并进行特定修改以处理对称不定矩阵。该问题是有效的，并且可以按所述方式解决。\n\n该问题要求实现非预处理的共轭梯度（CG）方法来求解线性系统 $A x = b$，其中 $A \\in \\mathbb{R}^{n \\times n}$ 是一个对称矩阵。标准的CG方法是为对称正定（SPD）矩阵推导的，在这种情况下它保证收敛。该方法的基础在于对二次泛函 $\\varphi(x) = \\frac{1}{2} x^T A x - b^T x$ 的迭代最小化。该泛函的梯度是 $\\nabla\\varphi(x) = A x - b = -r(x)$，其中 $r(x)$ 是残差。对于一个SPD矩阵 $A$，$\\varphi(x)$ 是一个严格凸的抛物面，在解 $x$ 处有唯一的全局最小值，其中 $\\nabla\\varphi(x) = 0$。\n\nCG算法生成一个迭代序列 $x_{k+1} = x_k + \\alpha_k p_k$，其中 $p_k$ 是搜索方向，$\\alpha_k$ 是步长。搜索方向被构造成 A-共轭（或 A-正交）的，即对于 $i \\neq j$ 有 $p_i^T A p_j = 0$。步长 $\\alpha_k$ 的选择是为了沿方向 $p_k$ 最小化 $\\varphi(x_{k+1})$，从而得到公式：\n$$\n\\alpha_k = \\frac{r_k^T r_k}{p_k^T A p_k}\n$$\n分母中的量 $p_k^T A p_k$ 是 $\\varphi(x)$ 在方向 $p_k$ 上的曲率。对于一个SPD矩阵 $A$，此项等价于搜索方向的A-范数的平方，即 $\\|p_k\\|_A^2$，对于任何非零的 $p_k$ 它都严格大于0。这种正性保证了 $\\alpha_k$ 是良定义且为正的，从而确保了一个能够减小 $\\varphi(x)$ 值的下降步。\n\n然而，如果 $A$ 是对称但不定的，它可能具有非正特征值。因此，二次泛函 $\\varphi(x)$ 不再是凸的，并可能含有鞍点。在迭代过程中，曲率项 $p_k^T A p_k$ 可能变为零或负值。\n\\begin{itemize}\n    \\item 如果 $p_k^T A p_k = 0$，步长 $\\alpha_k$ 无定义，导致除以零。\n    \\item 如果 $p_k^T A p_k  0$，步长 $\\alpha_k$ 变为负值。更新步骤会朝着增加 $\\varphi(x)$ 的方向移动，违反了CG的最小化原则。\n\\end{itemize}\n在任何一种情况下，标准的CG算法都会崩溃。问题要求我们在第 $k$ 次迭代时，当 $p_k^T A p_k \\le 0$ 时精确地检测到这种失败。\n\n要实现的算法如下，从 $x_0 = 0$ 开始：\n\n1.  初始化：\n    $k = 0$\n    $x_0 = 0$\n    $r_0 = b - A x_0 = b$\n    $p_0 = r_0$\n    $\\rho_0 = r_0^T r_0$\n\n2.  对 $k = 0, 1, 2, \\dots, m-1$ 进行迭代，其中 $m=n$ 是最大迭代次数：\n    a. 计算矩阵向量乘积 $v_k = A p_k$。\n    b. **失败检测：** 计算曲率 $\\gamma_k = p_k^T v_k$。如果 $\\gamma_k \\le 0$，则方法失败。停止并返回值 $-(k+1)$。\n    c. 计算步长：$\\alpha_k = \\rho_k / \\gamma_k$。\n    d. 更新解：$x_{k+1} = x_k + \\alpha_k p_k$。\n    e. 更新残差：$r_{k+1} = r_k - \\alpha_k v_k$。\n    f. **收敛检查：** 计算新残差的欧几里得范数 $\\|r_{k+1}\\|_2$。如果 $\\|r_{k+1}\\|_2 \\le \\varepsilon = 10^{-10}$，则方法已收敛。停止并返回已完成的迭代次数 $k+1$。\n    g. 为下一次迭代做准备：\n        i.  计算新残差的范数平方：$\\rho_{k+1} = r_{k+1}^T r_{k+1}$。\n        ii. 计算用于更新搜索方向的系数：$\\beta_k = \\rho_{k+1} / \\rho_k$。\n        iii. 更新搜索方向：$p_{k+1} = r_{k+1} + \\beta_k p_k$。\n        iv. 为下一步更新残差范数的平方：$\\rho_k \\leftarrow \\rho_{k+1}$。\n\n此过程将应用于所提供的三个测试用例中的每一个。对于第一种情况（SPD矩阵），我们期望成功收敛。对于第二和第三种情况（不定矩阵），我们期望在特定迭代中因非正曲率而失败。返回值按照规定被编码为正整数表示成功，负整数表示失败。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem by implementing the Conjugate Gradient method\n    with a breakdown check for symmetric indefinite matrices.\n    \"\"\"\n\n    def conjugate_gradient_solver(A, b, tolerance, max_iterations):\n        \"\"\"\n        Implements the unpreconditioned Conjugate Gradient (CG) method.\n\n        Args:\n            A (np.ndarray): A symmetric n x n matrix.\n            b (np.ndarray): A vector of size n.\n            tolerance (float): The convergence tolerance for the residual norm.\n            max_iterations (int): The maximum number of iterations.\n\n        Returns:\n            int: A positive integer k for successful convergence in k iterations,\n                 or a negative integer -(k+1) for failure at zero-based iteration k.\n        \"\"\"\n        # Start with the zero initial guess x_0 = 0.\n        x = np.zeros_like(b, dtype=float)\n        \n        # Initial residual r_0 = b - A*x_0 = b.\n        r = b.copy()\n        \n        # Initial search direction p_0 = r_0.\n        p = r.copy()\n        \n        # Squared norm of the initial residual.\n        rs_old = np.dot(r, r)\n\n        # Check for immediate convergence (if b is close to zero vector).\n        if np.sqrt(rs_old) = tolerance:\n            # Per problem, return a positive integer. 0 is not positive.\n            # This case will not be triggered by the test suite.\n            # Returning 0 would conventionally mean 0 iterations.\n            return 0\n\n        # Main iteration loop.\n        for k in range(max_iterations):\n            # Compute the matrix-vector product A*p_k.\n            Ap = np.dot(A, p)\n            \n            # Evaluate the curvature quantity p_k^T * A * p_k.\n            p_T_Ap = np.dot(p, Ap)\n            \n            # If curvature is non-positive, CG breakdown occurs.\n            if p_T_Ap = 0:\n                return -(k + 1)\n            \n            # Calculate step length alpha_k.\n            alpha = rs_old / p_T_Ap\n            \n            # Update solution: x_{k+1} = x_k + alpha_k * p_k.\n            x += alpha * p\n            \n            # Update residual: r_{k+1} = r_k - alpha_k * A*p_k.\n            r -= alpha * Ap\n            \n            # Calculate the squared norm of the new residual.\n            rs_new = np.dot(r, r)\n            \n            # Check for convergence using the Euclidean norm of the new residual.\n            if np.sqrt(rs_new) = tolerance:\n                return k + 1\n            \n            # Update search direction: p_{k+1} = r_{k+1} + beta_k * p_k, where beta_k = rs_new / rs_old.\n            p = r + (rs_new / rs_old) * p\n            \n            # Update the squared residual norm for the next iteration.\n            rs_old = rs_new\n            \n        # If the method reaches max_iterations without converging, this is also a failure.\n        # This path should not be taken for the specified test cases.\n        # A conventional return for this state might be e.g., -(max_iterations + 1).\n        # We assume the problem constraints ensure one of the explicit conditions is met.\n        return -(max_iterations + 1) # Placeholder for non-convergence.\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test case 1\n        {\n            \"A\": np.array([[4, 1, 0], [1, 3, 1], [0, 1, 2]], dtype=float),\n            \"b\": np.array([1, 2, 3], dtype=float),\n        },\n        # Test case 2\n        {\n            \"A\": np.array([[1, 0], [0, -1]], dtype=float),\n            \"b\": np.array([1, 1], dtype=float),\n        },\n        # Test case 3\n        {\n            \"A\": np.array([[2, 0], [0, -1]], dtype=float),\n            \"b\": np.array([1, 1], dtype=float),\n        },\n    ]\n\n    tolerance = 1e-10\n    results = []\n\n    for case in test_cases:\n        A = case[\"A\"]\n        b = case[\"b\"]\n        n = A.shape[0]\n        max_iter = n\n        result = conjugate_gradient_solver(A, b, tolerance, max_iter)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```", "id": "3244707"}, {"introduction": "尽管共轭梯度法存在最坏情况下的收敛边界，但其实际性能往往要好得多，并与矩阵的谱特性及初始猜测紧密相关。本练习探讨了一种“幸运崩溃”情景，即初始残差恰好是矩阵的一个特征向量。你将看到这种特殊情况如何仅用一步迭代就得到精确解，这揭示了Krylov子空间方法与底层特征空间之间的深刻联系。[@problem_id:3244757]", "problem": "考虑对称稀疏矩阵 $A \\in \\mathbb{R}^{4 \\times 4}$ 和右端项 $b \\in \\mathbb{R}^{4}$，其形式如下\n$$\nA = \\begin{pmatrix}\n7  0  0  0 \\\\\n0  4  -1  0 \\\\\n0  -1  3  -1 \\\\\n0  0  -1  5\n\\end{pmatrix}, \n\\quad\nb = \\begin{pmatrix}\n1 \\\\ 0 \\\\ 0 \\\\ 0\n\\end{pmatrix}.\n$$\n设初始猜测为 $x_{0} = \\begin{pmatrix}0 \\\\ 0 \\\\ 0 \\\\ 0\\end{pmatrix}$，并定义初始残差为 $r_{0} = b - A x_{0} = b$。考虑从归一化残差 $q_{1} = r_{0} / \\|r_{0}\\|$ 开始，对 $A$ 应用 Lanczos 过程。回顾一下，Lanczos 算法使用带有标量 $\\alpha_{k}$ 和 $\\beta_{k}$ 的三项递推，为 Krylov 子空间构建一个标准正交基，并在此基上将 $A$ 三对角化。\n\n使用 Krylov 子空间和 Lanczos 递推的基本定义，判断该过程是否在第一步出现“幸运中断”（lucky breakdown），并解释为什么这种中断会导致 Krylov 方法提前得到精确解。然后，对于共轭梯度 (CG) 方法，从 $x_{0}$ 开始，初始搜索方向等于 $r_{0}$，计算唯一的标量 $\\eta$，使得单步更新 $x_{1} = x_{0} + \\eta r_{0}$ 精确满足 $A x_{1} = b$。\n\n你的最终答案必须是 $\\eta$ 的单一值（无单位）。如果你选择用小数表示 $\\eta$，它必须是精确的或以闭式解析表达式给出；本题无需进行四舍五入。", "solution": "问题要求我们使用 Krylov 子空间方法分析线性系统 $Ax=b$ 的求解过程。给定对称矩阵 $A \\in \\mathbb{R}^{4 \\times 4}$、向量 $b \\in \\mathbb{R}^{4}$ 和初始猜测 $x_0 \\in \\mathbb{R}^{4}$：\n$$\nA = \\begin{pmatrix}\n7  0  0  0 \\\\\n0  4  -1  0 \\\\\n0  -1  3  -1 \\\\\n0  0  -1  5\n\\end{pmatrix}, \n\\quad\nb = \\begin{pmatrix}\n1 \\\\ 0 \\\\ 0 \\\\ 0\n\\end{pmatrix},\n\\quad\nx_{0} = \\begin{pmatrix}\n0 \\\\ 0 \\\\ 0 \\\\ 0\n\\end{pmatrix}.\n$$\n\n首先，我们验证该问题。矩阵 $A$ 是对称的。我们可以检查它是否为正定矩阵。$A$ 是一个块对角矩阵，由一个 $1 \\times 1$ 的块 $(7)$ 和一个 $3 \\times 3$ 的块组成。第一个块的特征值为 $7$。对于第二个块 $A' = \\begin{pmatrix} 4  -1  0 \\\\ -1  3  -1 \\\\ 0  -1  5 \\end{pmatrix}$，其顺序主子式为 $4 > 0$，$4(3)-(-1)^2 = 11 > 0$，以及 $\\det(A') = 4(15-1) - (-1)(-5) = 56-5 = 51 > 0$。由于所有顺序主子式均为正，因此 $A'$ 是正定的。所以，$A$ 是对称正定 (SPD) 矩阵，这是共轭梯度法收敛的一个标准条件。该问题是适定的且有科学依据。\n\n第一步是计算初始残差 $r_0$：\n$$\nr_0 = b - A x_0\n$$\n由于 $x_0$ 是零向量，所以 $A x_0$ 也是零向量。因此，\n$$\nr_0 = b = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix}.\n$$\n\n问题要求我们考虑 Lanczos 过程并判断是否会发生“幸运中断”。Lanczos 过程为 Krylov 子空间 $\\mathcal{K}_k(A, r_0) = \\text{span}\\{r_0, Ar_0, \\dots, A^{k-1}r_0\\}$ 生成一个标准正交基 $\\{q_1, q_2, \\dots\\}$。第一个向量 $q_1$ 是归一化的初始残差。\n$r_0$ 的范数是 $\\|r_0\\|_2 = \\sqrt{1^2 + 0^2 + 0^2 + 0^2} = 1$。\n所以，第一个 Lanczos 向量是：\n$$\nq_1 = \\frac{r_0}{\\|r_0\\|_2} = r_0 = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix}.\n$$\nLanczos 算法使用三项递推。我们来计算向量 $A q_1$：\n$$\nA q_1 = A r_0 = \\begin{pmatrix} 7  0  0  0 \\\\ 0  4  -1  0 \\\\ 0  -1  3  -1 \\\\ 0  0  -1  5 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 7 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix}.\n$$\n我们观察到 $A q_1 = 7 q_1$。这意味着 $q_1$（以及 $r_0$）是 $A$ 的一个特征向量，其对应的特征值为 $\\lambda=7$。\n\nLanczos 递推关系由 $\\beta_{k+1} q_{k+1} = A q_k - \\alpha_k q_k - \\beta_k q_{k-1}$ 给出，其中 $\\beta_1 q_0 = 0$。对于第一步（$k=1$）：\n$$\n\\beta_2 q_2 = A q_1 - \\alpha_1 q_1.\n$$\n系数 $\\alpha_1$ 定义为 $\\alpha_1 = q_1^T A q_1$。\n$$\n\\alpha_1 = q_1^T (7 q_1) = 7 q_1^T q_1 = 7 \\|q_1\\|_2^2 = 7(1)^2 = 7.\n$$\n现在我们计算未归一化的下一个向量：\n$$\n\\beta_2 q_2 = A q_1 - \\alpha_1 q_1 = 7 q_1 - 7 q_1 = 0.\n$$\n标量 $\\beta_{k+1}$ 是右侧向量的范数。因此，\n$$\n\\beta_2 = \\|A q_1 - \\alpha_1 q_1\\|_2 = \\|0\\|_2 = 0.\n$$\n当某个 $k  n$ 出现 $\\beta_{k+1} = 0$ 时，Lanczos 过程发生中断。当这种情况发生时，Krylov 子空间 $\\mathcal{K}_k(A, q_1)$ 是 $A$ 的一个不变子空间。在本例中，由于 $\\beta_2=0$，过程在第一步就终止了。这被称为“幸运中断”，因为它意味着线性系统的精确解位于仿射空间 $x_0 + \\mathcal{K}_1(A, r_0)$ 内。发生这种情况是因为初始残差 $r_0$ 是 $A$ 的一个特征向量。$A$ 关于 $r_0$ 的最小多项式次数为 1，因此 CG 方法保证能在一步内找到精确解。\n\n现在，我们计算标量 $\\eta$，使得单步更新 $x_1 = x_0 + \\eta r_0$ 精确满足 $A x_1 = b$。\n我们将 $x_1$ 的表达式代入线性系统：\n$$\nA(x_0 + \\eta r_0) = b.\n$$\n利用矩阵-向量乘积的线性性质，我们得到：\n$$\nA x_0 + \\eta A r_0 = b.\n$$\n由于 $x_0 = 0$，我们有 $A x_0 = 0$。方程简化为：\n$$\n\\eta A r_0 = b.\n$$\n我们已经计算过 $A r_0$：\n$$\nA r_0 = \\begin{pmatrix} 7 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix}.\n$$\n我们还知道 $b = r_0 = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix}$。\n将这些代入我们关于 $\\eta$ 的方程：\n$$\n\\eta \\begin{pmatrix} 7 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix}.\n$$\n这个向量方程等价于以下标量方程：\n$$\n7 \\eta = 1.\n$$\n解出 $\\eta$，我们得到：\n$$\n\\eta = \\frac{1}{7}.\n$$\n问题陈述中提到了共轭梯度 (CG) 方法。让我们使用 CG 公式来验证这个结果。在 CG 的第一步中，搜索方向是 $p_0 = r_0$。步长 $\\alpha_0$ 计算如下：\n$$\n\\alpha_0 = \\frac{r_0^T r_0}{p_0^T A p_0} = \\frac{r_0^T r_0}{r_0^T A r_0}.\n$$\n我们计算分子和分母：\n$$\nr_0^T r_0 = \\begin{pmatrix} 1  0  0  0 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix} = 1.\n$$\n$$\nr_0^T A r_0 = \\begin{pmatrix} 1  0  0  0 \\end{pmatrix} \\begin{pmatrix} 7 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix} = 7.\n$$\n因此，CG 步长为：\n$$\n\\alpha_0 = \\frac{1}{7}.\n$$\n更新后的解为 $x_1 = x_0 + \\alpha_0 p_0 = x_0 + \\frac{1}{7}r_0$。这与问题中给出的形式相符，其中 $\\eta = \\alpha_0 = \\frac{1}{7}$。使用这个 $\\eta$ 进行单步更新可以得到精确解，因为初始残差是矩阵 $A$ 的一个特征向量。", "answer": "$$\n\\boxed{\\frac{1}{7}}\n$$", "id": "3244757"}]}