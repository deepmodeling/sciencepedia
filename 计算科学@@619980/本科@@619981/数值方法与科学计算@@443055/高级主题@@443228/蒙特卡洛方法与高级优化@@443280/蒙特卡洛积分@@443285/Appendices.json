{"hands_on_practices": [{"introduction": "理解蒙特卡洛积分的最佳方式莫过于亲手实践。这项入门练习将带你入门，将核心原理应用于一个多维积分问题——在这种情况下，传统方法可能会变得相当复杂。通过这个计算材料总质量的例子，你将巩固对蒙特卡洛积分基本公式及其应用的理解 [@problem_id:1376816]。", "problem": "一位材料科学家正在研究一种在边长为1米的立方体模具中制造的新合金。坐标系与模具对齐，使其占据由 $0 \\le x \\le 1$、$0 \\le y \\le 1$ 和 $0 \\le z \\le 1$ 定义的区域。研究发现，一种特殊硬化剂的浓度 $C$ 仅在由不等式 $0 \\le z \\le y \\le x \\le 1$ 定义的模具特定子区域内非零。在该子区域内，点 $(x,y,z)$ 处的浓度由函数 $C(x,y,z) = k x y z$ 描述，其中 $k$ 是一个常数。在该区域之外，浓度为零。模具中硬化剂的总质量由浓度函数在整个1立方米模具体积上的积分给出。\n\n为了估算这个总质量，一个自动化测量系统在一组 $N=5$ 个样本点处探测浓度，这些点被假定为单位立方体内均匀随机抽样的代表。这五个点的坐标是：\n$P_1 = (0.8, 0.7, 0.2)$\n$P_2 = (0.9, 0.5, 0.6)$\n$P_3 = (0.6, 0.8, 0.3)$\n$P_4 = (0.5, 0.4, 0.3)$\n$P_5 = (0.7, 0.9, 0.8)$\n\n给定浓度常数 $k = 4.8 \\text{ kg/m}^6$，请根据这五个样本点计算模具中硬化剂总质量的数值估计值。你的答案以千克（kg）为单位，并四舍五入到三位有效数字。", "solution": "总质量是浓度在单位立方体上的体积积分：\n$$\nM=\\iiint_{[0,1]^{3}} C(x,y,z)\\,dV.\n$$\n在体积为 $V=1$ 的域上进行均匀随机抽样，使用 $N$ 个样本 $\\{P_{i}\\}_{i=1}^{N}$ 的蒙特卡洛估计量为\n$$\n\\widehat{M}=\\frac{V}{N}\\sum_{i=1}^{N} C(P_{i})=\\frac{1}{N}\\sum_{i=1}^{N} C(P_{i}).\n$$\n此处，如果 $0 \\le z \\le y \\le x \\le 1$，则 $C(x,y,z)=k\\,x y z$，否则 $C=0$。对每个样本评估指示条件 $0 \\le z \\le y \\le x \\le 1$：\n- $P_{1}=(0.8,0.7,0.2)$: $0.2 \\le 0.7 \\le 0.8 \\le 1$ 成立，所以贡献值为 $k\\cdot 0.8\\cdot 0.7\\cdot 0.2=0.112\\,k$。\n- $P_{2}=(0.9,0.5,0.6)$: $0.6 \\le 0.5$ 不成立，贡献值为 $0$。\n- $P_{3}=(0.6,0.8,0.3)$: $0.8 \\le 0.6$ 不成立，贡献值为 $0$。\n- $P_{4}=(0.5,0.4,0.3)$: $0.3 \\le 0.4 \\le 0.5 \\le 1$ 成立，贡献值为 $k\\cdot 0.5\\cdot 0.4\\cdot 0.3=0.06\\,k$。\n- $P_{5}=(0.7,0.9,0.8)$: $0.9 \\le 0.7$ 不成立，贡献值为 $0$。\n因此\n$$\n\\sum_{i=1}^{5} C(P_{i})=(0.112+0.06)\\,k=0.172\\,k,\n$$\n且\n$$\n\\widehat{M}=\\frac{1}{5}\\cdot 0.172\\,k=0.0344\\,k.\n$$\n代入 $k=4.8$ 得\n$$\n\\widehat{M}=0.0344\\times 4.8=0.16512,\n$$\n四舍五入到三位有效数字后，结果为 $0.165$ 千克。", "answer": "$$\\boxed{0.165}$$", "id": "1376816"}, {"introduction": "要真正理解一种方法的优势，最好的办法就是将其与其它方法进行比较。本练习将蒙特卡洛积分与一种经典的确定性方法——辛普森法则——进行对比，并特意选择了一个带有“尖点”（即不可导点）的函数进行积分。通过这个实践，你将揭示蒙特卡洛方法的一个关键优势：其收敛速度在很大程度上不受函数光滑性的影响，这使其在处理复杂的现实世界问题时表现得非常稳健 [@problem_id:3253419]。", "problem": "您需要实现并分析积分 $$I = \\int_{0}^{1} \\lvert x - 0.3 \\rvert \\, dx,$$ 的两种数值估计方法，重点关注 $$x = 0.3$$ 处的不可微点（尖点）如何影响收敛性和方差。工作应完全基于纯数学术语：没有物理单位，也没有角度。您的程序必须是自包含的，并且无需任何用户输入即可生成指定的输出。\n\n使用的基本原理：\n- 将确定性积分等同于期望值：对于一个随机变量 $$U \\sim \\mathrm{Uniform}(0,1),$$ $$\\mathbb{E}[f(U)] = \\int_{0}^{1} f(x) \\, dx.$$\n- 独立性条件下蒙特卡洛（MC）估计量的性质。\n- 用于在具有偶数个子区间的均匀剖分上进行数值积分的复化辛普森法则。\n\n定义 $$f(x) = \\lvert x - 0.3 \\rvert.$$ 考虑使用 $$N$$ 个独立同分布样本 $$U_1, U_2, \\dots, U_N \\sim \\mathrm{Uniform}(0,1)$$ 的蒙特卡洛估计量，以及在 $$[0,1]$$ 上使用 $$m$$ 个均匀子区间的复化辛普森法则，其中 $$m$$ 是一个偶数。重点在于 $$x = 0.3$$ 处的尖点如何改变误差行为。\n\n您的程序必须执行以下任务：\n1. 从第一性原理出发，解析地计算 $$I$$ 的精确值。\n2. 对于蒙特卡洛估计量：\n   - 每个测试用例使用一个固定的伪随机数生成器种子，从 $$\\mathrm{Uniform}(0,1)$$ 中抽取 $$N$$ 个样本 $$U_i$$，并计算样本均值 $$\\hat{I}_N = \\frac{1}{N} \\sum_{i=1}^{N} f(U_i).$$\n   - 使用总体约定（除以 $$N$$）计算观测到的 $$f(U_i)$$ 值的样本方差，这样即使在 $$N = 1$$ 时它仍然有定义。\n   - 通过解析地推导 $$\\operatorname{Var}(f(U))$$ 来计算理论标准误 $$\\sqrt{\\operatorname{Var}(f(U))/N}.$$\n3. 对于复化辛普森法则：\n   - 在 $$[0,1]$$ 上实现具有 $$m$$ 个偶数子区间和均匀间距 $$h = 1/m$$ 的复化辛普森法则。使用标准权重公式 $$\\frac{h}{3} \\left[f(x_0) + 4 \\sum f(x_{2j+1}) + 2 \\sum f(x_{2j}) + f(x_m)\\right]$$ 其中 $$x_k = k h.$$\n   - 计算辛普森近似值 $$S_m$$ 及其相对于精确值 $$I$$ 的绝对误差。\n4. 对于这两种方法，报告其相对于精确值 $$I$$ 的绝对误差。\n5. 使用以下测试套件来评估不同场景：\n   - 情况 $$1$$（理想路径，尖点与网格节点对齐）：$$N = 100,$$ $$m = 10,$$ 种子 $$= 12345.$$\n   - 情况 $$2$$（理想路径，尖点不与节点对齐）：$$N = 100,$$ $$m = 12,$$ 种子 $$= 54321.$$\n   - 情况 $$3$$（蒙特卡洛和辛普森法则的边界条件）：$$N = 1,$$ $$m = 2,$$ 种子 $$= 7.$$\n   - 情况 $$4$$（大样本量和细网格，尖点与节点对齐）：$$N = 10000,$$ $$m = 100,$$ 种子 $$= 2025.$$\n   根据复化辛普森法则的要求，所有 $$m$$ 均为偶数。\n\n您的程序必须生成单行输出，其中包含用方括号括起来的、以逗号分隔的结果列表。每个测试用例的结果本身必须是包含七个浮点数的列表，顺序如下：\n- $$\\hat{I}_N$$（蒙特卡洛估计值），\n- 使用除以 $$N$$ 计算的 $$f(U_i)$$ 的样本方差，\n- 理论标准误 $$\\sqrt{\\operatorname{Var}(f(U))/N},$$\n- $$S_m$$（复化辛普森估计值），\n- 精确值 $$I,$$\n- 蒙特卡洛的绝对误差 $$\\lvert \\hat{I}_N - I \\rvert,$$\n- 辛普森法则的绝对误差 $$\\lvert S_m - I \\rvert.$$\n\n因此，最终输出格式是表示列表的列表的单行，例如 $$[[\\ldots],[\\ldots],[\\ldots],[\\ldots]].$$", "solution": "在继续之前，对问题进行验证。\n\n### 步骤 1：提取已知条件\n- 待评估积分：$$I = \\int_{0}^{1} \\lvert x - 0.3 \\rvert \\, dx$$。\n- 函数定义：$$f(x) = \\lvert x - 0.3 \\rvert$$。\n- 用于蒙特卡洛的随机变量：$$U \\sim \\mathrm{Uniform}(0,1)$$。\n- 蒙特卡洛估计量：$$\\hat{I}_N = \\frac{1}{N} \\sum_{i=1}^{N} f(U_i)$$，其中 $$U_i \\sim \\mathrm{Uniform}(0,1)$$ 独立同分布。\n- 样本方差定义：使用总体约定（除以 $$N$$）。\n- 理论标准误：$$\\sqrt{\\operatorname{Var}(f(U))/N}$$。\n- 复化辛普森法则：使用 $$m$$ 个偶数子区间，间距 $$h = 1/m$$，点 $$x_k = k h$$，以及公式 $$S_m = \\frac{h}{3} \\left[f(x_0) + 4 \\sum_{j=0}^{m/2-1} f(x_{2j+1}) + 2 \\sum_{j=1}^{m/2-1} f(x_{2j}) + f(x_m)\\right]$$。\n- 测试用例：\n  - 情况 1：$$N = 100$$，$$m = 10$$，种子 $$= 12345$$。\n  - 情况 2：$$N = 100$$，$$m = 12$$，种子 $$= 54321$$。\n  - 情况 3：$$N = 1$$，$$m = 2$$，种子 $$= 7$$。\n  - 情况 4：$$N = 10000$$，$$m = 100$$，种子 $$= 2025$$。\n- 每个案例的所需输出：一个包含七个浮点数的列表：$$\\hat{I}_N$$，$$f(U_i)$$ 的样本方差，理论标准误，$$S_m$$，精确值 $$I$$，蒙特卡洛的绝对误差 $$\\lvert \\hat{I}_N - I \\rvert$$，辛普森法则的绝对误差 $$\\lvert S_m - I \\rvert$$。\n\n### 步骤 2：使用提取的已知条件进行验证\n根据验证标准对问题进行评估。\n- **有科学依据**：该问题是数值分析中的一个标准练习，比较了一种随机方法（蒙特卡洛）和一种确定性求积法则（辛普森法则）。函数 $$f(x) = \\lvert x - 0.3 \\rvert$$ 是一个用于说明数值方法在非光滑函数上行为的经典例子。所有引用的原理都是数学和科学计算的基础。\n- **适定性**：问题已完全指定。积分定义明确。两种估计量的公式都已明确给出。每个测试用例的所有参数（$$N, m$$, 种子）都已提供。样本方差的定义已明确，以避免歧义。\n- **客观性**：问题以精确的数学语言陈述，没有任何主观性或偏见。\n\n该问题没有表现出任何无效性缺陷。它是数值方法领域一个定义明确、可形式化且可验证的问题。\n\n### 步骤 3：结论与行动\n该问题是**有效的**。将提供一个解决方案。\n\n### 解决方案的基于原理的设计\n\n解决方案包括三个主要部分：精确量的解析计算、蒙特卡洛估计量的实现以及复化辛普森法则的实现。\n\n#### 1. 解析计算\n\n首先，我们必须计算积分 $$I$$ 的精确值和 $$f(U)$$ 的理论方差，以便对数值方法进行精确评估。\n\n**积分 $$I$$ 的精确值**\n被积函数 $$f(x) = \\lvert x - 0.3 \\rvert$$ 在 $$x = 0.3$$ 处有一个不可微点。为了对其进行积分，我们在此点划分定义域：\n$$I = \\int_{0}^{1} \\lvert x - 0.3 \\rvert \\, dx = \\int_{0}^{0.3} -(x - 0.3) \\, dx + \\int_{0.3}^{1} (x - 0.3) \\, dx$$\n$$I = \\left[ 0.3x - \\frac{x^2}{2} \\right]_{0}^{0.3} + \\left[ \\frac{x^2}{2} - 0.3x \\right]_{0.3}^{1}$$\n$$I = \\left( 0.3^2 - \\frac{0.3^2}{2} \\right) - 0 + \\left( \\frac{1^2}{2} - 0.3 \\cdot 1 \\right) - \\left( \\frac{0.3^2}{2} - 0.3^2 \\right)$$\n$$I = \\left( \\frac{0.09}{2} \\right) + (0.5 - 0.3) - \\left( -\\frac{0.09}{2} \\right) = 0.045 + 0.2 + 0.045 = 0.29$$\n所以，积分的精确值为 $$I = 0.29$$。\n\n**$$f(U)$$ 的理论方差，其中 $$U \\sim \\mathrm{Uniform}(0,1)$$**\n方差由 $$\\operatorname{Var}(f(U)) = \\mathbb{E}[f(U)^2] - (\\mathbb{E}[f(U)])^2$$ 给出。\n我们已经知道 $$\\mathbb{E}[f(U)] = \\int_{0}^{1} f(x) \\, dx = I = 0.29$$。\n我们计算二阶矩 $$\\mathbb{E}[f(U)^2]$$：\n$$\\mathbb{E}[f(U)^2] = \\int_{0}^{1} f(x)^2 \\, dx = \\int_{0}^{1} (\\lvert x - 0.3 \\rvert)^2 \\, dx = \\int_{0}^{1} (x - 0.3)^2 \\, dx$$\n$$\\mathbb{E}[f(U)^2] = \\int_{0}^{1} (x^2 - 0.6x + 0.09) \\, dx = \\left[ \\frac{x^3}{3} - 0.3x^2 + 0.09x \\right]_{0}^{1}$$\n$$\\mathbb{E}[f(U)^2] = \\left( \\frac{1}{3} - 0.3 + 0.09 \\right) - 0 = \\frac{1}{3} - 0.21 = \\frac{100 - 63}{300} = \\frac{37}{300}$$\n现在，我们可以求出方差：\n$$\\operatorname{Var}(f(U)) = \\frac{37}{300} - (0.29)^2 = \\frac{37}{300} - 0.0841 = \\frac{37}{300} - \\frac{841}{10000} = \\frac{3700 - 2523}{30000} = \\frac{1177}{30000}$$\n这就是精确的理论方差。\n\n#### 2. 蒙特卡洛积分\n\n蒙特卡洛方法通过在从相关分布中抽取的随机点 $$U_i$$ 处进行 $$N$$ 次函数求值的样本均值来估计积分 $$I = \\mathbb{E}[f(U)]$$。\n- **估计量**：$$\\hat{I}_N = \\frac{1}{N} \\sum_{i=1}^{N} f(U_i)$$。\n- **样本方差**：问题指定使用总体公式（除以 $$N$$）：$$\\hat{\\sigma}^2_N = \\frac{1}{N} \\sum_{i=1}^{N} (f(U_i) - \\hat{I}_N)^2$$。这是 NumPy 中的 `ddof=0` 约定。\n- **理论标准误**：中心极限定理表明，蒙特卡洛估计量的误差近似服从标准差为 $$\\sigma / \\sqrt{N}$$ 的正态分布，其中 $$\\sigma^2 = \\operatorname{Var}(f(U))$$。这个量，$$\\sqrt{\\operatorname{Var}(f(U))/N}$$，是均值的理论标准误。\n对于每个测试用例，我们将使用给定的种子生成 $$N$$ 个样本，计算估计值 $$\\hat{I}_N$$、其样本方差、理论标准误以及绝对误差 $$\\lvert \\hat{I}_N - I \\rvert$$。\n\n#### 3. 复化辛普森法则\n\n这是一种确定性求积法则，用分段二次多项式来近似被积函数。\n- **公式**：$$S_m = \\frac{h}{3} \\sum_{j=0}^{m/2-1} \\left( f(x_{2j}) + 4f(x_{2j+1}) + f(x_{2j+2}) \\right)$$，其中 $$h=1/m$$。\n- **收敛性分析**：复化辛普森法则的标准误差界为 $$O(h^4)$$ 阶或 $$O(m^{-4})$$ 阶，但这依赖于被积函数四阶连续可微。我们的函数 $$f(x) = \\lvert x-0.3 \\rvert$$ 在 $$x=0.3$$ 处不可微。其四阶导数在该点未定义。因此，收敛速度将显著慢于 $$O(m^{-4})$$。误差主要由包含尖点的子区间上的近似所决定。\n- **具体情况**：\n  - 当 $$m=10$$, $$h=0.1$$ 时，$$x=0.3$$ 处的尖点是一个网格点（$$x_3$$）。误差将局限于在跨越该尖点的区间 $$[x_2, x_4] = [0.2, 0.4]$$ 上单次应用辛普森法则。\n  - 当 $$m=12$$, $$h=1/12$$ 时，$$x=0.3$$ 处的尖点不是一个网格点。它位于区间 $$[x_3, x_4] = [0.25, 0.33\\dots]$$ 内。误差将源于包含此尖点的区间 $$[x_2, x_4]$$ 上的近似。\n对于每个测试用例，我们构建具有 $$m+1$$ 个点的均匀网格，在这些点上计算 $$f(x)$$ 的值，应用求和公式得到 $$S_m$$，并计算绝对误差 $$\\lvert S_m - I \\rvert$$。\n\n这些步骤将针对每个指定的测试用例用 Python 实现。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and analyzes Monte Carlo and Simpson's rule estimators for a\n    given integral with a non-differentiable point.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (N, m, seed)\n        (100, 10, 12345),\n        (100, 12, 54321),\n        (1, 2, 7),\n        (10000, 100, 2025)\n    ]\n\n    # Analytical values derived in the solution\n    exact_I = 0.29\n    # Theoretical variance Var(f(U)) = 1177 / 30000\n    theoretical_variance = 1177.0 / 30000.0\n\n    # The function to integrate: f(x) = |x - 0.3|\n    def f(x):\n        return np.abs(x - 0.3)\n\n    results = []\n    for N, m, seed in test_cases:\n        # --- Monte Carlo Method ---\n        rng = np.random.default_rng(seed)\n        samples = rng.uniform(0, 1, N)\n        f_samples = f(samples)\n\n        # Monte Carlo estimate\n        I_hat_N = np.mean(f_samples)\n        \n        # Sample variance (population convention, divide by N)\n        # np.var default is ddof=0, which is the population variance.\n        sample_var = np.var(f_samples)\n        \n        # Theoretical standard error of the mean\n        theo_std_err = np.sqrt(theoretical_variance / N)\n        \n        # Absolute error of Monte Carlo estimate\n        mc_abs_err = np.abs(I_hat_N - exact_I)\n\n        # --- Composite Simpson's Rule ---\n        h = 1.0 / m\n        # Generate m+1 points from 0 to 1\n        x_grid = np.linspace(0, 1, m + 1)\n        y_grid = f(x_grid)\n\n        # Simpson's formula: S_m = (h/3) * [f(x_0) + 4*f(x_1) + 2*f(x_2) + ... + f(x_m)]\n        # We can implement this with slicing for efficiency.\n        # Sum of odd-indexed terms: y[1], y[3], ..., y[m-1]\n        sum_odd = np.sum(y_grid[1:-1:2])\n        # Sum of even-indexed terms (excluding endpoints): y[2], y[4], ..., y[m-2]\n        sum_even = np.sum(y_grid[2:-1:2])\n        \n        S_m = (h / 3.0) * (y_grid[0] + y_grid[-1] + 4 * sum_odd + 2 * sum_even)\n\n        # Absolute error of Simpson's estimate\n        simpson_abs_err = np.abs(S_m - exact_I)\n\n        # Collect the seven required values\n        case_result = [\n            I_hat_N,\n            sample_var,\n            theo_std_err,\n            S_m,\n            exact_I,\n            mc_abs_err,\n            simpson_abs_err\n        ]\n        results.append(case_result)\n\n    # Format the final output as a string representing a list of lists.\n    # The default str() for a list includes spaces, which we need to remove\n    # to strictly match the requested format.\n    case_strings = []\n    for res in results:\n        # Using repr() to get a standard representation, can also use f-strings\n        res_str = f\"[{','.join(repr(val) for val in res)}]\"\n        case_strings.append(res_str)\n    \n    final_output = f\"[{','.join(case_strings)}]\"\n    \n    # Final print statement in the exact required format.\n    print(final_output)\n\nsolve()\n```", "id": "3253419"}, {"introduction": "现在，让我们进入一项更高级的综合性实践，它融合了多种强大的蒙特卡洛技巧。对于像在无穷区间上积分这类具有挑战性的问题，基本的蒙特卡洛方法可能效率不高。本练习引入了一种精妙的策略：将积分域分割，并对不同部分采用不同技术——对中心区域使用简单蒙特卡洛，对尾部区域则使用重要性采样 [@problem_id:3253443]。更进一步，通过自适应地选择分割点以最小化总方差，这项练习让你一窥现代科学计算中优化计算效率的前沿方法。", "problem": "实现一个完整的、可运行的程序，通过随机模拟估算函数 $x \\mapsto e^{-x^{2}}$ 在整个实数轴上的积分。我们感兴趣的积分是\n$$\nI \\equiv \\int_{-\\infty}^{\\infty} e^{-x^{2}} \\, dx.\n$$\n的值。\n您必须使用一个两部分的蒙特卡洛（Monte Carlo, MC）策略，其中实数轴在一个对称阈值 $A > 0$ 处被分割成一个中心区域和两个尾部。中心部分是 $\\int_{-A}^{A} e^{-x^{2}} \\, dx$，尾部部分是 $\\int_{|x|>A} e^{-x^{2}} \\, dx$。中心积分必须使用在区间 $[-A, A]$ 上的均匀分布，通过简单蒙特卡洛方法进行估算。尾部积分必须使用重要性采样方法进行估算，其中在 $[A, \\infty)$ 上使用指数提议分布，其率参数线性依赖于 $A$。阈值 $A$ 必须通过一个引导性研究（pilot study）从一个预设的网格中自适应地选择。该研究会为每个候选 $A$ 估算中心和尾部估计量的方差贡献，并预测在固定的总样本预算下，通过在两个部分之间优化分配主样本可实现的最小方差。\n\n仅从以下基本原理出发：\n- 积分作为概率密度下的期望的定义，以及期望的无偏样本均值估计量。\n- 使用提议密度构建重要性采样估计量，以及在绝对连续条件下其相关的无偏性。\n- 独立样本均值的方差，以及独立和的方差可加性。\n\n您不得依赖被积函数的任何闭式反导数或特殊函数恒等式。算法应基于上述基础从第一性原理设计，并应从无偏性和方差控制的角度证明每一步的合理性。程序必须执行以下步骤：\n1. 对于给定的候选 $A$，通过从 $[-A, A]$ 上的均匀分布中采样 $U$ 并使用变换后变量 $G = (2A)\\, e^{-U^{2}}$ 的样本均值来定义一个中心估计量。\n2. 对于给定的候选 $A$，通过重要性采样方法为正尾部定义一个尾部估计量，其提议密度为 $q_{A}(y) = \\lambda_{A} \\, e^{-\\lambda_{A}(y - A)}$ (对于 $y \\ge A$)，并通过对称性反射来覆盖两个尾部。这里 $\\lambda_{A} = \\rho A$，其中 $\\rho > 0$ 是一个给定的常数。通过从 $q_{A}$ 中抽取一个样本 $Y$，定义变换后的变量 $H = 2 \\, e^{-Y^{2}} / q_{A}(Y)$；$H$ 的样本均值是两个尾部总贡献的无偏估计量。\n3. 对于一个候选阈值 $A$ 的网格和每个部分的固定引导样本量 $n_{0}$，为每个 $A$ 估算 $G$ 和 $H$ 的单次抽样方差。在所有 $A$ 中使用一组共同的随机数，以确保引导阶段消耗的随机变量数量固定，且与网格大小无关。使用这些引导方差估计和主样本总预算 $N_{\\text{main}}$，为每个 $A$ 确定在约束 $n_{\\text{center}} + n_{\\text{tail}} = N_{\\text{main}}$ 下，能够使两个独立估计量之和的预测方差最小化的样本分配。选择产生最小预测方差的 $A$。如果出现平局，选择最小化器中最小的 $A$。\n4. 使用选定的 $A$，以最优整数分配 $n_{\\text{center}}$ 和 $n_{\\text{tail}}$（每个至少为 $1$）运行主抽样，以获得最终的点估计 $\\widehat{I}$，即两个样本均值之和。通过将在主样本上计算出的 $G$ 和 $H$ 的样本方差分别除以其样本量，然后求和再开方，来计算估计的标准误 $\\widehat{\\mathrm{se}}$。\n5. 实现必须确保生成的基础随机变量总数等于 $N_{\\text{total}} = 2 n_{0} + N_{\\text{main}}$，其中 $n_{0}$ 个变量用于中心部分的引导研究，$n_{0}$ 个变量用于尾部部分的引导研究，$N_{\\text{main}}$ 个变量用于主阶段的两个部分。在引导阶段使用共同随机数，具体如下：生成一个包含 $n_{0}$ 个在 $[0,1]$ 上的均匀分布随机变量的向量，并为每个候选 $A$ 将其变换到 $[-A, A]$ 以获得中心部分的引导抽样；生成一个包含 $n_{0}$ 个单位率指数分布随机变量的向量，并为每个候选 $A$ 将其变换到尾部提议分布以获得尾部部分的引导抽样。\n\n每个测试用例的输入参数在下面完整指定；您的程序必须硬编码这些测试用例，为每个用例执行自适应选择和估算，并完全按照指定要求打印所需的输出。不允许用户输入。\n\n测试套件：\n- 用例 1：种子 $= 123456$，$N_{\\text{total}} = 100000$，引导样本量 $n_{0} = 2000$，$A$ 的候选网格是从 $0.5$ 到 $2.5$ 的 $9$ 个线性间隔点（含端点），以及 $\\rho = 1.25$。\n- 用例 2：种子 $= 2024$，$N_{\\text{total}} = 8000$，引导样本量 $n_{0} = 1000$，$A$ 的候选网格是从 $0.2$ 到 $1.4$ 的 $7$ 个线性间隔点（含端点），以及 $\\rho = 1.0$。\n- 用例 3：种子 $= 77$，$N_{\\text{total}} = 40000$，引导样本量 $n_{0} = 1500$，$A$ 的候选网格是从 $0.8$ 到 $3.0$ 的 $12$ 个线性间隔点（含端点），以及 $\\rho = 0.75$。\n\n实现细节和约束：\n- 使用双精度算术。\n- 仅使用以给定种子初始化的伪随机数生成器以确保可复现性。对于在 $[A, \\infty)$ 上速率为 $\\lambda_{A}$ 的指数提议分布，您可以生成一个单位率指数变量 $E$ 并将其映射为 $Y = A + E / \\lambda_{A}$；在这种情况下，$q_{A}(Y) = \\lambda_{A} \\, e^{-\\lambda_{A}(Y - A)} = \\lambda_{A} \\, e^{-E}$，变换后的变量可以等价地写为 $H = \\frac{2}{\\lambda_{A}} \\, \\exp\\!\\left(-\\left(A + \\frac{E}{\\lambda_{A}}\\right)^{2} + E \\right)$。\n- 对于每个候选 $A$，定义 $N_{\\text{main}} = N_{\\text{total}} - 2 n_{0}$，在提供的测试套件中保证为正数。使用 $G$ 和 $H$ 的引导单次抽样方差估计，来预测在该 $A$ 值下通过最优分配可实现的最小方差。然后如上所述自适应地选择 $A$，计算相应的最优分配，并执行主抽样。\n- 最终标准误计算公式为 $\\widehat{\\mathrm{se}} = \\sqrt{\\widehat{\\sigma}^2_{G}/n_{\\text{center}} + \\widehat{\\sigma}^2_{H}/n_{\\text{tail}}}$，其中 $\\widehat{\\sigma}_{G}^{2}$ 和 $\\widehat{\\sigma}_{H}^{2}$ 是在主样本上计算的 $G$ 和 $H$ 的无偏样本方差。\n\n答案规范和输出格式：\n- 对于每个测试用例，输出一个包含 5 个值的列表，顺序完全如下：点估计 $\\widehat{I}$、估计的标准误 $\\widehat{\\mathrm{se}}$、选定的阈值 $A$、整数 $n_{\\text{center}}$ 和整数 $n_{\\text{tail}}$。\n- 将三个浮点数输出四舍五入到恰好 6 位小数。两个分配计数必须打印为整数。\n- 您的程序应生成单行输出，其中包含三个用例的结果，格式为逗号分隔的列表之列表，例如 $[[v_{11},v_{12},v_{13},v_{14},v_{15}],[v_{21},v_{22},v_{23},v_{24},v_{25}],[v_{31},v_{32},v_{33},v_{34},v_{35}]]$，其中每个 $v_{ij}$ 是如上指定的数字。", "solution": "该问题要求使用一个两阶段自适应蒙特卡洛方法来估算高斯积分 $I \\equiv \\int_{-\\infty}^{\\infty} e^{-x^{2}} \\, dx$。该解决方案从第一性原理出发，即一个积分可以表示为一个统计期望，以及使用样本均值作为无偏估计量。\n\n### 第1部分：积分分解与估计量构建\n\n整个实数轴 $(-\\infty, \\infty)$ 上的积分 $I$ 在一个对称阈值 $A > 0$ 处被划分为一个中心积分和一个尾部积分。\n$$\nI = \\int_{-A}^{A} e^{-x^{2}} \\, dx + \\int_{|x|>A} e^{-x^{2}} \\, dx = I_{\\text{center}}(A) + I_{\\text{tail}}(A)\n$$\n这两个部分被独立估计，然后将它们的估计值相加。\n\n**中心积分估计量：**\n中心积分 $I_{\\text{center}}(A)$ 使用简单蒙特卡洛积分进行估计。我们可以将该积分表示为一个随机变量的期望。设 $U$ 是一个在区间 $[-A, A]$ 上均匀分布的随机变量，其概率密度函数（PDF）为 $p(u) = 1/(2A)$ (对于 $u \\in [-A, A]$)。那么该积分为：\n$$\nI_{\\text{center}}(A) = \\int_{-A}^{A} e^{-x^{2}} \\, dx = \\int_{-A}^{A} \\left( (2A) e^{-x^{2}} \\right) \\frac{1}{2A} \\, dx = E_{U \\sim \\text{Unif}[-A, A]} [G(U)]\n$$\n其中 $G(U) = (2A)e^{-U^{2}}$。$I_{\\text{center}}(A)$ 的一个无偏估计量是 $n_{\\text{center}}$ 次独立抽取的 $G$ 的样本均值：\n$$\n\\widehat{I}_{\\text{center}} = \\frac{1}{n_{\\text{center}}} \\sum_{i=1}^{n_{\\text{center}}} G_i, \\quad \\text{其中 } G_i = (2A)e^{-U_i^2} \\text{ 且 } U_i \\sim \\text{Unif}[-A, A]\n$$\n\n**尾部积分估计量：**\n根据对称性，尾部积分是 $I_{\\text{tail}}(A) = \\int_{|x|>A} e^{-x^2} \\, dx = 2 \\int_{A}^{\\infty} e^{-x^2} \\, dx$。该积分使用重要性采样进行估计，这种方法适用于快速衰减的被积函数。我们在定义域 $[A, \\infty)$ 上引入一个概率密度函数为 $q_{A}(y)$ 的提议分布。该积分可以写为：\n$$\nI_{\\text{tail}}(A) = 2 \\int_{A}^{\\infty} \\frac{e^{-y^2}}{q_A(y)} q_A(y) \\, dy = E_{Y \\sim q_A} \\left[ \\frac{2 e^{-Y^2}}{q_A(Y)} \\right]\n$$\n问题指定了一个移位的指数提议密度 $q_A(y) = \\lambda_A e^{-\\lambda_A(y-A)}$ (对于 $y \\ge A$)，其中率参数 $\\lambda_A$ 是阈值的线性函数，$\\lambda_A = \\rho A$，其中 $\\rho > 0$ 是一个给定的常数。$I_{\\text{tail}}(A)$ 的一个无偏估计量是 $n_{\\text{tail}}$ 次独立抽取的随机变量 $H(Y) = 2e^{-Y^2}/q_A(Y)$ 的样本均值：\n$$\n\\widehat{I}_{\\text{tail}} = \\frac{1}{n_{\\text{tail}}} \\sum_{i=1}^{n_{\\text{tail}}} H_i, \\quad \\text{其中 } H_i = \\frac{2e^{-Y_i^2}}{q_A(Y_i)} \\text{ 且 } Y_i \\sim q_A\n$$\n为了实际生成样本 $Y_i$，我们可以使用逆变换采样。如果 $U' \\sim \\text{Unif}[0,1]$，那么 $E = -\\ln(U')$ 是一个标准的指数变量（率参数为1）。从 $q_A$ 中抽取一个样本 $Y$ 可以通过 $Y = A + E/\\lambda_A$ 得到。将此代入 $H$ 的表达式中，得到一个更便于计算的形式：\n$$\nH = \\frac{2e^{-(A+E/\\lambda_A)^2}}{\\lambda_A e^{-\\lambda_A((A+E/\\lambda_A)-A)}} = \\frac{2e^{-(A+E/\\lambda_A)^2}}{\\lambda_A e^{-E}} = \\frac{2}{\\lambda_A} \\exp\\left(-\\left(A + \\frac{E}{\\lambda_A}\\right)^2 + E\\right)\n$$\n其中 $E \\sim \\text{Exp}(1)$。\n\n### 第2部分：方差最小化与最优分配\n\n总积分估计值为 $\\widehat{I} = \\widehat{I}_{\\text{center}} + \\widehat{I}_{\\text{tail}}$。由于两个估计量是独立的，总估计值的方差是它们方差的和：\n$$\n\\text{Var}(\\widehat{I}) = \\text{Var}(\\widehat{I}_{\\text{center}}) + \\text{Var}(\\widehat{I}_{\\text{tail}}) = \\frac{\\sigma_G^2(A)}{n_{\\text{center}}} + \\frac{\\sigma_H^2(A)}{n_{\\text{tail}}}\n$$\n其中 $\\sigma_G^2(A) = \\text{Var}(G)$ 且 $\\sigma_H^2(A) = \\text{Var}(H)$。我们的目标是选择样本量 $n_{\\text{center}}$ 和 $n_{\\text{tail}}$ 来最小化此方差，同时受限于主模拟的固定总预算 $n_{\\text{center}} + n_{\\text{tail}} = N_{\\text{main}}$。\n\n使用 Lagrange 乘数法或简单代入法，可以发现最优分配与相应估计量的标准差成正比：\n$$\nn_{\\text{center}}^* = N_{\\text{main}} \\frac{\\sigma_G(A)}{\\sigma_G(A) + \\sigma_H(A)}, \\quad n_{\\text{tail}}^* = N_{\\text{main}} \\frac{\\sigma_H(A)}{\\sigma_G(A) + \\sigma_H(A)}\n$$\n将这些最优（实数值）分配代回方差公式，得到给定 $A$ 时可实现的最小方差：\n$$\nV_{\\text{min}}(A) = \\frac{(\\sigma_G(A) + \\sigma_H(A))^2}{N_{\\text{main}}}\n$$\n\n### 第3部分：阈值 $A$ 的自适应选择\n\n最优阈值 $A$ 是未知的。我们从一个预设的候选值网格中自适应地选择它。这是通过一个两阶段过程完成的。\n\n**第1阶段：引导性研究**\n进行一项引导性研究，以估计网格中每个候选 $A$ 的标准差 $\\sigma_G(A)$ 和 $\\sigma_H(A)$。\n1. 为中心部分和尾部分别抽取一个大小为 $n_0$ 的小型引导样本。\n2. 为确保在不同 $A$ 值之间进行公平比较，使用了共同随机数（Common Random Numbers, CRN）。为中心部分生成一组 $n_0$ 个在 $[0,1]$ 上的基础均匀分布随机变量，为尾部部分生成一组 $n_0$ 个基础标准指数随机变量。\n3. 对于网格中的每个 $A$，这些基础随机数被变换以分别生成 $G$ 和 $H$ 的样本。\n4. 从这些引导样本中计算出样本方差 $\\widehat{\\sigma}_{G, \\text{pilot}}^2(A)$ 和 $\\widehat{\\sigma}_{H, \\text{pilot}}^2(A)$。\n5. 对于每个 $A$，我们接着通过使用估计的标准差 $\\widehat{\\sigma}_{G, \\text{pilot}}(A)$ 和 $\\widehat{\\sigma}_{H, \\text{pilot}}(A)$，在最优分配下预测可实现的最小方差 $V_{\\text{min}}(A)$。最小化 $V_{\\text{min}}(A)$ 等价于最小化估计的标准差之和 $\\widehat{\\sigma}_{G, \\text{pilot}}(A) + \\widehat{\\sigma}_{H, \\text{pilot}}(A)$。\n6. 产生最小预测方差的候选 $A$ 被选为最优阈值 $A^*$。平局通过选择最小的 $A$ 来打破。\n\n**第2阶段：主模拟**\n1. 使用选定的 $A^*$ 和相应的引导标准差估计值 $\\widehat{\\sigma}_G(A^*)$ 和 $\\widehat{\\sigma}_H(A^*)$，通过对理想的小数分配进行四舍五入来确定最优整数样本分配 $n_{\\text{center}}$ 和 $n_{\\text{tail}}$，同时确保它们的和为 $N_{\\text{main}}$ 且每个都至少为 1。\n2. 分别为中心和尾部估计量生成大小为 $n_{\\text{center}}$ 和 $n_{\\text{tail}}$ 的新的、独立的随机样本。\n3. 最终估计值 $\\widehat{I}$ 计算为 $G$ 和 $H$ 的主样本均值之和。\n4. 最终估计值的标准误 $\\widehat{\\text{se}}(\\widehat{I})$ 使用主模拟的样本方差计算：\n   $$\n   \\widehat{\\text{se}} = \\sqrt{\\frac{\\widehat{\\sigma}_{G, \\text{main}}^2}{n_{\\text{center}}} + \\frac{\\widehat{\\sigma}_{H, \\text{main}}^2}{n_{\\text{tail}}}}\n   $$\n   其中 $\\widehat{\\sigma}_{G, \\text{main}}^2$ 和 $\\widehat{\\sigma}_{H, \\text{main}}^2$ 是 $G$ 和 $H$ 的主样本的无偏样本方差。整个过程确保使用的随机变量总数为引导阶段的 $2n_0$ 加上主阶段的 $N_{\\text{main}}$，总计为 $N_{\\text{total}}$。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef process_case(case_params):\n    \"\"\"\n    Processes a single test case for the adaptive Monte Carlo integration.\n    \"\"\"\n    seed = case_params['seed']\n    N_total = case_params['N_total']\n    n_0 = case_params['n_0']\n    A_grid_start, A_grid_stop, A_grid_num = case_params['A_grid_params']\n    A_grid = np.linspace(A_grid_start, A_grid_stop, A_grid_num)\n    rho = case_params['rho']\n\n    rng = np.random.default_rng(seed)\n    N_main = N_total - 2 * n_0\n\n    # 1. Pilot Study with Common Random Numbers (CRN)\n    # Generate base random numbers once for the pilot stage.\n    pilot_central_uniforms = rng.uniform(size=n_0)  # For U in [-A, A]\n    pilot_tail_exponentials = rng.exponential(size=n_0)  # For E ~ Exp(1)\n\n    pilot_results = []\n    for A in A_grid:\n        lambda_A = rho * A\n\n        # Central estimator pilot calculations\n        U_pilot = A * (2 * pilot_central_uniforms - 1)\n        G_pilot = (2 * A) * np.exp(-U_pilot**2)\n        var_G = np.var(G_pilot, ddof=1)\n\n        # Tail estimator pilot calculations\n        # H = (2/lambda_A) * exp(-(A + E/lambda_A)^2 + E)\n        H_pilot = (2 / lambda_A) * np.exp(-(A + pilot_tail_exponentials / lambda_A)**2 + pilot_tail_exponentials)\n        var_H = np.var(H_pilot, ddof=1)\n        \n        if not np.isfinite(var_G) or not np.isfinite(var_H):\n            total_std_dev = np.inf\n            std_G, std_H = np.inf, np.inf\n        else:\n            std_G = np.sqrt(var_G)\n            std_H = np.sqrt(var_H)\n            total_std_dev = std_G + std_H\n\n        pilot_results.append({'total_std': total_std_dev, 'A': A, 'std_G': std_G, 'std_H': std_H})\n\n    # 2. Select Optimal A\n    # Find A that minimizes the predicted total standard deviation.\n    # The sort first by 'total_std', then by 'A' for tie-breaking.\n    best_pilot_run = sorted(pilot_results, key=lambda x: (x['total_std'], x['A']))[0]\n    A_opt = best_pilot_run['A']\n    std_G_opt = best_pilot_run['std_G']\n    std_H_opt = best_pilot_run['std_H']\n\n    # 3. Determine Main Sample Allocation\n    # Calculate ideal allocation and round to nearest integer.\n    if std_G_opt + std_H_opt == 0:\n        n_center = N_main // 2\n    else:\n        n_center_ideal = N_main * std_G_opt / (std_G_opt + std_H_opt)\n        n_center = int(np.round(n_center_ideal))\n\n    # Enforce allocation constraints: n_center, n_tail >= 1\n    if n_center  1:\n        n_center = 1\n    elif n_center > N_main - 1:\n        n_center = N_main - 1\n    n_tail = N_main - n_center\n\n    # 4. Main Monte Carlo Simulation\n    # Generate new independent random numbers for the main run.\n    \n    # Central part\n    U_main = rng.uniform(low=-A_opt, high=A_opt, size=n_center)\n    G_main = (2 * A_opt) * np.exp(-U_main**2)\n    I_hat_center = np.mean(G_main)\n    # Unbiased sample variance. If sample size is 1, variance is not estimable from sample, so we take it as 0.\n    var_G_main = np.var(G_main, ddof=1) if n_center > 1 else 0.0\n\n    # Tail part\n    lambda_A_opt = rho * A_opt\n    E_main = rng.exponential(size=n_tail)\n    H_main = (2 / lambda_A_opt) * np.exp(-(A_opt + E_main / lambda_A_opt)**2 + E_main)\n    I_hat_tail = np.mean(H_main)\n    var_H_main = np.var(H_main, ddof=1) if n_tail > 1 else 0.0\n\n    # 5. Combine Results\n    I_hat = I_hat_center + I_hat_tail\n    \n    # Final standard error computed from main sample variances\n    se_hat = np.sqrt(var_G_main / n_center + var_H_main / n_tail)\n\n    return [I_hat, se_hat, A_opt, n_center, n_tail]\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print results.\n    \"\"\"\n    test_cases = [\n        {'seed': 123456, 'N_total': 100000, 'n_0': 2000, 'A_grid_params': (0.5, 2.5, 9), 'rho': 1.25},\n        {'seed': 2024, 'N_total': 8000, 'n_0': 1000, 'A_grid_params': (0.2, 1.4, 7), 'rho': 1.0},\n        {'seed': 77, 'N_total': 40000, 'n_0': 1500, 'A_grid_params': (0.8, 3.0, 12), 'rho': 0.75}\n    ]\n\n    all_results = []\n    for case in test_cases:\n        result = process_case(case)\n        all_results.append(result)\n\n    # Format the output as specified\n    result_strings = []\n    for res in all_results:\n        # res[0:3] are floats, res[3:5] are ints\n        formatted_list = [\n            f\"{res[0]:.6f}\",\n            f\"{res[1]:.6f}\",\n            f\"{res[2]:.6f}\",\n            str(res[3]),\n            str(res[4])\n        ]\n        result_strings.append(f\"[{','.join(formatted_list)}]\")\n    \n    print(f\"[{','.join(result_strings)}]\")\n\nsolve()\n```", "id": "3253443"}]}