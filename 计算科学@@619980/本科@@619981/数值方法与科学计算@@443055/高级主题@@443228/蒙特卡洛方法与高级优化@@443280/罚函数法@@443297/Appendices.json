{"hands_on_practices": [{"introduction": "这个实践将从最简单的非平凡情形入手：一个凸二次目标函数带单个线性等式约束。本练习将引导你推导原约束问题和罚函数的精确解[@problem_id:3261466]。通过分析这些解的差异，你将具体理解罚方法的工作原理，并量化其收敛速度，观察解的误差和约束违反度如何随着罚参数 $ \\mu $ 的增大而减小。", "problem": "考虑由 $f(x) = \\sum_{i=1}^{n} x_i^2$ 定义的无约束目标，其中 $x \\in \\mathbb{R}^n$，以及由 $g(x) = c^\\top x - 1 = 0$ 给出的单个等式约束，其中 $c \\in \\mathbb{R}^n$ 是一个固定的非零向量。罚函数法为罚参数 $\\mu > 0$ 构建一个惩罚目标函数 $F_\\mu(x) = f(x) + \\mu \\, g(x)^2$。从无约束和约束最小化的核心定义以及凸二次函数的性质出发，您的任务是：\n- 任务 A：使用基本原理，推导在约束 $g(x) = 0$ 下 $f(x)$ 的约束极小点 $x^\\star$。\n- 任务 B：使用基本原理，推导给定 $\\mu$ 时惩罚目标函数 $F_\\mu(x)$ 的无约束极小点 $x_\\mu$，并证明当 $\\mu \\to +\\infty$ 时，$x_\\mu \\to x^\\star$。\n- 任务 C：估计决策变量中的误差（由 $\\|x_\\mu - x^\\star\\|_2$ 度量）和约束违反度（由 $|g(x_\\mu)|$ 度量）随 $\\mu$ 增大而衰减的速率。使用大O表示法将速率表示为 $\\mu$ 和 $c$ 的范数的函数。\n\n您的程序必须实现所推导的公式，并为每个测试用例计算以下量值：\n- 决策变量的误差范数：$\\|x_\\mu - x^\\star\\|_2$。\n- 绝对约束违反度：$|g(x_\\mu)|$。\n- 缩放后的误差范数：$\\mu \\, \\|x_\\mu - x^\\star\\|_2$。\n- 缩放后的约束违反度：$\\mu \\, |g(x_\\mu)|$。\n\n所有数值结果必须报告为无量纲量（无物理单位）。结果应四舍五入到 $8$ 位小数。\n\n使用以下参数值测试套件，其中每个用例都是一个对 $(c, \\mu)$，其中 $c \\in \\mathbb{R}^n$ 且 $\\mu \\in \\mathbb{R}$：\n- 用例 $1$ (标准情况)：$n = 3$, $c = [1, 2, -1]$, $\\mu = 10$。\n- 用例 $2$ (罚强度边界情况)：$n = 3$, $c = [1, 2, -1]$, $\\mu = 0.1$。\n- 用例 $3$ (渐近状态)：$n = 3$, $c = [1, 2, -1]$, $\\mu = 10^6$。\n- 用例 $4$ (小范数约束向量)：$n = 3$, $c = [10^{-3}, 0, 0]$, $\\mu = 10$。\n- 用例 $5$ (大范数约束向量)：$n = 3$, $c = [3, 4, 0]$, $\\mu = 10$。\n\n您的程序应生成单行输出，其中包含用方括号括起来的逗号分隔列表形式的结果。每个测试用例的结果必须是包含四个浮点数的列表，顺序固定为 $[\\|x_\\mu - x^\\star\\|_2, |g(x_\\mu)|, \\mu \\, \\|x_\\mu - x^\\star\\|_2, \\mu \\, |g(x_\\mu)|]$，并四舍五入到 $8$ 位小数。因此，最终输出行的格式为 $[[r_{1,1}, r_{1,2}, r_{1,3}, r_{1,4}],[r_{2,1}, r_{2,2}, r_{2,3}, r_{2,4}],\\dots,[r_{5,1}, r_{5,2}, r_{5,3}, r_{5,4}]]$，不含空格。", "solution": "该问题被验证为具有科学依据、问题明确且客观。所有必要信息均已提供，并且任务在约束优化和罚函数法的既定数学框架内得到了明确定义。\n\n该问题要求对一个简单的凸优化问题分析二次罚函数法。给定目标函数 $f(x) = \\sum_{i=1}^{n} x_i^2 = x^\\top x$ 和单个线性等式约束 $g(x) = c^\\top x - 1 = 0$，其中 $x \\in \\mathbb{R}^n$，$c \\in \\mathbb{R}^n$ 是一个固定的非零向量。二次罚函数法通过求解一系列无约束问题来近似该约束问题的解，其惩罚目标函数为 $F_\\mu(x) = f(x) + \\mu \\, g(x)^2 = x^\\top x + \\mu(c^\\top x - 1)^2$，罚参数为 $\\mu > 0$。\n\n我们的分析按要求分三部分进行。\n\n任务 A：推导约束极小点 $x^\\star$。\n为了找到在约束 $g(x)=0$ 下 $f(x)$ 的极小点，我们使用拉格朗日乘子法。拉格朗日函数 $\\mathcal{L}(x, \\lambda)$ 定义为：\n$$ \\mathcal{L}(x, \\lambda) = f(x) - \\lambda g(x) = x^\\top x - \\lambda (c^\\top x - 1) $$\n最优性的一阶必要条件要求拉格朗日函数关于 $x$ 的梯度为零向量，并且约束必须满足。\n$$ \\nabla_x \\mathcal{L}(x, \\lambda) = 2x - \\lambda c = 0 $$\n$$ g(x) = c^\\top x - 1 = 0 $$\n从第一个条件，我们用拉格朗日乘子 $\\lambda$ 来表示最优解 $x^\\star$：\n$$ x^\\star = \\frac{\\lambda}{2} c $$\n将其代入约束方程，可以求解 $\\lambda$：\n$$ c^\\top \\left(\\frac{\\lambda}{2} c\\right) - 1 = 0 $$\n$$ \\frac{\\lambda}{2} (c^\\top c) = 1 $$\n$$ \\frac{\\lambda}{2} \\|c\\|_2^2 = 1 $$\n由于 $c$ 是非零向量，$\\|c\\|_2^2 > 0$，因此我们可以解出 $\\lambda$：\n$$ \\lambda = \\frac{2}{\\|c\\|_2^2} $$\n将 $\\lambda$ 的值代回 $x^\\star$ 的表达式，我们得到约束极小点：\n$$ x^\\star = \\frac{1}{2} \\left( \\frac{2}{\\|c\\|_2^2} \\right) c = \\frac{c}{\\|c\\|_2^2} = \\frac{c}{c^\\top c} $$\n由于目标函数 $f(x)$ 是严格凸的，并且约束定义了一个凸集（一个超平面），因此这个驻点是唯一的全局极小点。\n\n任务 B：推导无约束极小点 $x_\\mu$ 及收敛性分析。\n惩罚目标函数为 $F_\\mu(x) = x^\\top x + \\mu(c^\\top x - 1)^2$。这是一个无约束的严格凸函数，因此其唯一的极小点 $x_\\mu$ 可以通过将其梯度设为零来找到。\n$$ \\nabla_x F_\\mu(x) = \\nabla_x (x^\\top x) + \\mu \\nabla_x ((c^\\top x - 1)^2) $$\n对罚函数项使用链式法则，其中 $\\nabla_x(u^2) = 2u \\nabla_x u$，我们得到：\n$$ \\nabla_x F_\\mu(x) = 2x + \\mu \\cdot 2(c^\\top x - 1) \\cdot c = 0 $$\n除以 $2$ 并重新整理各项，得到：\n$$ x_\\mu + \\mu(c^\\top x_\\mu - 1)c = 0 $$\n$$ x_\\mu + \\mu(c^\\top x_\\mu)c - \\mu c = 0 $$\n$$ x_\\mu + \\mu c(c^\\top x_\\mu) = \\mu c $$\n这可以写成一个线性系统：$(I + \\mu c c^\\top) x_\\mu = \\mu c$。为了求解 $x_\\mu$，我们可以使用 Sherman-Morrison 公式来求 $(I + \\mu c c^\\top)$ 的逆，得到：\n$$(I + \\mu c c^\\top)^{-1} = I - \\frac{\\mu c c^\\top}{1 + \\mu c^\\top c}$$\n应用这个逆来求解 $x_\\mu$：\n$$ x_\\mu = \\left(I - \\frac{\\mu c c^\\top}{1 + \\mu \\|c\\|_2^2}\\right) (\\mu c) = \\mu c - \\frac{\\mu^2 c (c^\\top c)}{1 + \\mu \\|c\\|_2^2} = \\mu c - \\frac{\\mu^2 \\|c\\|_2^2 c}{1 + \\mu \\|c\\|_2^2} $$\n简化 $c$ 的标量系数：\n$$ \\mu - \\frac{\\mu^2 \\|c\\|_2^2}{1 + \\mu \\|c\\|_2^2} = \\frac{\\mu(1 + \\mu \\|c\\|_2^2) - \\mu^2 \\|c\\|_2^2}{1 + \\mu \\|c\\|_2^2} = \\frac{\\mu}{1 + \\mu \\|c\\|_2^2} $$\n因此，惩罚函数的极小点是：\n$$ x_\\mu = \\frac{\\mu}{1 + \\mu \\|c\\|_2^2} c $$\n为了证明当 $\\mu \\to +\\infty$ 时 $x_\\mu \\to x^\\star$，我们对 $x_\\mu$ 的表达式取极限：\n$$ \\lim_{\\mu \\to +\\infty} x_\\mu = \\lim_{\\mu \\to +\\infty} \\left( \\frac{\\mu}{1 + \\mu \\|c\\|_2^2} \\right) c $$\n我们可以将标量分数的分子和分母同时除以 $\\mu$：\n$$ \\lim_{\\mu \\to +\\infty} \\left( \\frac{1}{1/\\mu + \\|c\\|_2^2} \\right) c = \\left( \\frac{1}{0 + \\|c\\|_2^2} \\right) c = \\frac{c}{\\|c\\|_2^2} $$\n这个极限恰好是 $x^\\star$ 的表达式，这证实了收敛性 $x_\\mu \\to x^\\star$。\n\n任务 C：估计衰减速率。\n我们现在分析误差和约束违反度趋近于零的速率。\n\n首先，我们分析决策变量中的误差 $\\|x_\\mu - x^\\star\\|_2$。\n$$ x_\\mu - x^\\star = \\frac{\\mu}{1 + \\mu \\|c\\|_2^2} c - \\frac{1}{\\|c\\|_2^2} c = \\left( \\frac{\\mu \\|c\\|_2^2 - (1 + \\mu \\|c\\|_2^2)}{(1 + \\mu \\|c\\|_2^2)\\|c\\|_2^2} \\right) c = \\frac{-1}{(1 + \\mu \\|c\\|_2^2)\\|c\\|_2^2} c $$\n取欧几里得范数：\n$$ \\|x_\\mu - x^\\star\\|_2 = \\left\\| \\frac{-c}{(1 + \\mu \\|c\\|_2^2)\\|c\\|_2^2} \\right\\|_2 = \\frac{\\|c\\|_2}{(1 + \\mu \\|c\\|_2^2)\\|c\\|_2^2} = \\frac{1}{(1 + \\mu \\|c\\|_2^2)\\|c\\|_2} $$\n对于大的 $\\mu$，与 $\\mu \\|c\\|_2^2$ 相比，项 $1$ 可以忽略不计。因此，渐近行为是：\n$$ \\|x_\\mu - x^\\star\\|_2 \\approx \\frac{1}{(\\mu \\|c\\|_2^2)\\|c\\|_2} = \\frac{1}{\\mu \\|c\\|_2^3} $$\n这表明误差范数以 $O(\\mu^{-1})$ 的速率衰减。具体来说，$\\|x_\\mu - x^\\star\\|_2 = O(\\mu^{-1}\\|c\\|_2^{-3})$。\n\n接下来，我们分析约束违反度 $|g(x_\\mu)|$。\n$$ g(x_\\mu) = c^\\top x_\\mu - 1 = c^\\top \\left(\\frac{\\mu}{1 + \\mu \\|c\\|_2^2} c\\right) - 1 = \\frac{\\mu (c^\\top c)}{1 + \\mu \\|c\\|_2^2} - 1 $$\n$$ g(x_\\mu) = \\frac{\\mu \\|c\\|_2^2 - (1 + \\mu \\|c\\|_2^2)}{1 + \\mu \\|c\\|_2^2} = \\frac{-1}{1 + \\mu \\|c\\|_2^2} $$\n取绝对值得到约束违反度：\n$$ |g(x_\\mu)| = \\frac{1}{1 + \\mu \\|c\\|_2^2} $$\n对于大的 $\\mu$，渐近行为是：\n$$ |g(x_\\mu)| \\approx \\frac{1}{\\mu \\|c\\|_2^2} $$\n这表明约束违反度也以 $O(\\mu^{-1})$ 的速率衰减。具体来说，$|g(x_\\mu)| = O(\\mu^{-1}\\|c\\|_2^{-2})$。\n\n用于实现的公式总结：\n对于给定的 $c \\in \\mathbb{R}^n$ 和 $\\mu > 0$：\n1. 误差范数：$\\|x_\\mu - x^\\star\\|_2 = \\frac{1}{(1 + \\mu \\|c\\|_2^2)\\|c\\|_2}$\n2. 约束违反度：$|g(x_\\mu)| = \\frac{1}{1 + \\mu \\|c\\|_2^2}$\n3. 缩放后的误差范数：$\\mu \\|x_\\mu - x^\\star\\|_2 = \\frac{\\mu}{(1 + \\mu \\|c\\|_2^2)\\|c\\|_2}$\n4. 缩放后的约束违反度：$\\mu |g(x_\\mu)| = \\frac{\\mu}{1 + \\mu \\|c\\|_2^2}$\n这些公式将用于计算测试用例所需的量值。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements the derived formulas for the penalty method analysis\n    and computes results for the specified test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # Each case is a tuple (c, mu), where c is a list of floats\n    # and mu is a float.\n    test_cases = [\n        # Case 1 (happy path)\n        ([1.0, 2.0, -1.0], 10.0),\n        # Case 2 (boundary in penalty strength)\n        ([1.0, 2.0, -1.0], 0.1),\n        # Case 3 (asymptotic regime)\n        ([1.0, 2.0, -1.0], 1e6),\n        # Case 4 (small-norm constraint vector)\n        ([1e-3, 0.0, 0.0], 10.0),\n        # Case 5 (large-norm constraint vector)\n        ([3.0, 4.0, 0.0], 10.0),\n    ]\n\n    all_results = []\n    for c_list, mu in test_cases:\n        c = np.array(c_list)\n\n        # Calculate squared L2 norm of c\n        c_norm_sq = np.dot(c, c)\n        \n        # Calculate L2 norm of c\n        c_norm = np.sqrt(c_norm_sq)\n\n        # The term (1 + mu * ||c||^2) appears in all denominators\n        denominator = 1.0 + mu * c_norm_sq\n\n        # Task C derived the following exact formulas:\n        # ||x_mu - x*||_2 = 1 / ((1 + mu * ||c||^2) * ||c||)\n        # |g(x_mu)| = 1 / (1 + mu * ||c||^2)\n\n        # Calculate error norm in the decision variable\n        error_norm = 1.0 / (denominator * c_norm)\n\n        # Calculate absolute constraint violation\n        constraint_violation = 1.0 / denominator\n\n        # Calculate scaled error norm\n        scaled_error_norm = mu * error_norm\n\n        # Calculate scaled constraint violation\n        scaled_constraint_violation = mu * constraint_violation\n\n        # Store the four computed values for the current test case\n        case_result = [\n            error_norm,\n            constraint_violation,\n            scaled_error_norm,\n            scaled_constraint_violation,\n        ]\n        all_results.append(case_result)\n\n    # Format the final output string as specified in the problem statement.\n    # The output is a list of lists, with no spaces and 8 decimal places.\n    case_strings = []\n    for res in all_results:\n        # Format each number to 8 decimal places and join into a string like \"[n1,n2,n3,n4]\"\n        num_strings = [f\"{val:.8f}\" for val in res]\n        case_strings.append(f\"[{','.join(num_strings)}]\")\n    \n    # Join the case strings into a final string like \"[[...],[...],...]\"\n    final_output = f\"[{','.join(case_strings)}]\"\n    \n    # Final print statement in the exact required format.\n    print(final_output)\n\nsolve()\n```", "id": "3261466"}, {"introduction": "现实世界中的问题通常是非凸的，这带来了巨大的挑战。本实践将探讨当我们将罚方法应用于非凸目标函数时会发生什么[@problem_id:3261519]。你将实现一个基于网格的搜索算法，以可视化罚函数的地形，并识别“虚假”局部最小值——这些是罚方法产生的、但并未解决原始问题的“人造”解。这个练习突显了罚方法在非凸优化中的一个关键局限性。", "problem": "您将使用数值方法和科学计算中的罚函数法，实现并分析一个由约束优化问题产生的非凸惩罚目标。目标是为几个惩罚参数算法化地找出惩罚函数的伪局部最小值，并报告出现了多少个这样的伪最小值。在整个过程中，所有三角函数的角度都必须以弧度解释。\n\n考虑以下包含两个实变量 $x$ 和 $y$ 的原始约束问题：\n最小化非凸函数\n$$\nf(x,y) = (x^2 - 1)^2 + (y^2 - 1)^2 + \\tfrac{1}{2}\\,\\sin(3x)\\sin(3y),\n$$\n服从等式约束\n$$\ng(x,y) = x + y = 0,\n$$\n以及不等式约束\n$$\nh(x,y) = x^2 + y^2 - 3 \\le 0.\n$$\n使用二次罚函数法，为惩罚参数 $r \\ge 0$ 定义惩罚目标：\n$$\nF_r(x,y) = f(x,y) + \\frac{r}{2}\\,g(x,y)^2 + \\frac{r}{2}\\,\\max\\{0, h(x,y)\\}^2.\n$$\n由于 $f$ 是非凸的，因此对于所有的 $r$ 值，该惩罚目标 $F_r$ 都是非凸的。\n\n如果存在点 $(x^\\ast,y^\\ast)$ 的一个邻域，在该邻域内 $F_r$ 在 $(x^\\ast,y^\\ast)$ 处取得最小值，则称该点为 $F_r$ 的一个局部最小值。如果 $F_r$ 的一个局部最小值 $(x^\\ast,y^\\ast)$ 对原始问题是不可行的，即它不满足 $g(x^\\ast,y^\\ast)=0$ 或 $h(x^\\ast,y^\\ast)\\le 0$，则称其为伪局部最小值。\n\n您的任务是从第一性原理出发，设计一个算法，通过离散化来近似有界域上 $F_r$ 的局部最小值集合，然后对这些局部最小值进行分类，判断哪些是伪局部最小值。请使用以下基本依据：\n- 根据邻域内函数值定义的局部最小值。\n- 通过为违反约束的行为增加非负惩罚项来构造罚函数法。\n- 三角函数角度以弧度计量的解释。\n\n您的算法必须遵循以下规范。\n- 离散化域：方形域 $[-2,2]\\times[-2,2]$。\n- 网格分辨率：沿每个轴有 $401$ 个点的均匀网格。\n- 局部最小值检测：在网格上使用 $8$-邻域准则。如果一个网格点 $(i,j)$ 的函数值小于或等于其所有 $8$ 个相邻邻居的值，并且严格小于其中至少一个邻居的值，则该点为局部最小值。使用数值容差 $\\varepsilon_{\\text{lm}}=10^{-12}$ 来比较浮点数值。\n- 可行性分类：在坐标 $(x,y)$ 处检测到的局部最小值，如果满足 $|g(x,y)|\\le \\tau$ 和 $h(x,y)\\le \\tau$ (容差 $\\tau = 10^{-6}$)，则为可行的。任何不满足此准则的已检测局部最小值都是伪局部最小值。\n\n惩罚参数测试集：\n- 情况 1：$r=0$（无惩罚；边界情况）。\n- 情况 2：$r=1$（小惩罚）。\n- 情况 3：$r=10$（中等惩罚）。\n- 情况 4：$r=200$（大惩罚）。\n\n对于每种情况，根据上述规则计算网格上 $F_r$ 的伪局部最小值的数量。\n\n最终输出格式：\n您的程序应生成单行输出，其中包含按测试集顺序排列、用方括号括起来的逗号分隔列表形式的结果。例如，如果四种情况下的伪局部最小值数量分别为 $a$、$b$、$c$ 和 $d$，程序必须打印\n$$\n[a,b,c,d]\n$$\n打印在单行上，不含空格。", "solution": "用户提供了一个数值优化领域的问题，具体涉及将二次罚函数法应用于非凸约束优化问题时产生的伪局部最小值的分析。任务是实现一个基于网格离散化和邻居比较的特定算法，以计算给定惩罚参数集下这些伪最小值的数量。\n\n### 问题验证\n\n**步骤 1：提取给定条件**\n\n*   **目标函数：** $f(x,y) = (x^2 - 1)^2 + (y^2 - 1)^2 + \\tfrac{1}{2}\\,\\sin(3x)\\sin(3y)$\n*   **等式约束：** $g(x,y) = x + y = 0$\n*   **不等式约束：** $h(x,y) = x^2 + y^2 - 3 \\le 0$\n*   **惩罚目标函数：** $F_r(x,y) = f(x,y) + \\frac{r}{2}\\,g(x,y)^2 + \\frac{r}{2}\\,\\max\\{0, h(x,y)\\}^2$，惩罚参数为 $r \\ge 0$。\n*   **伪局部最小值的定义：** 如果 $F_r$ 的一个局部最小值 $(x^\\ast,y^\\ast)$ 对原始问题是不可行的，即它不满足 $g(x^\\ast,y^\\ast)=0$ 或 $h(x^\\ast,y^\\ast)\\le 0$，则该局部最小值是伪的。\n*   **离散化域：** $[-2,2]\\times[-2,2]$。\n*   **网格分辨率：** 沿每个轴有 401 个点的均匀网格。\n*   **局部最小值检测准则：** 如果一个网格点 $(i,j)$ 的函数值小于或等于其所有 $8$ 个相邻邻居的值，并且严格小于其中至少一个邻居的值，则该点为局部最小值。\n*   **局部最小值数值容差：** $\\varepsilon_{\\text{lm}}=10^{-12}$。\n*   **可行性分类准则：** 在 $(x,y)$ 处检测到的局部最小值，如果 $|g(x,y)|\\le \\tau$ 且 $h(x,y)\\le \\tau$（容差 $\\tau = 10^{-6}$），则为可行的。否则，它是伪的。\n*   **惩罚参数测试集：** $r \\in \\{0, 1, 10, 200\\}$。\n*   **角度约定：** 所有三角函数的参数均以弧度为单位。\n*   **最终输出格式：** `[a,b,c,d]`，其中 $a,b,c,d$ 分别是对应测试用例的伪局部最小值数量。\n\n**步骤 2：使用提取的给定条件进行验证**\n\n根据验证标准对问题进行评估：\n\n*   **科学基础：** 该问题牢固地植根于约束优化和数值方法的理论。罚函数法是一种标准技术，所涉及的函数是明确定义的数学构造。该设置在科学上是合理的。\n*   **适定性：** 该问题是适定的。它要求根据一套完全指定的规则和数值容差，计算一个特定的、可数的量（网格上的伪局部最小值数量）。对于每个输入 $r$，算法唯一确定输出。\n*   **客观性：** 该问题使用精确、客观的数学语言表述。所有分类标准（局部最小值、可行性）都给出了明确的数值容差，没有主观解释的余地。\n*   **完整性和一致性：** 问题陈述是自洽的，提供了所有必要的函数、常数、域和算法定义。没有矛盾之处。\n\n**步骤 3：结论与行动**\n\n该问题是**有效的**。它是数值分析中一个明确定义的计算任务。现在开始求解过程。\n\n### 求解\n\n问题的核心是实现一个数值方案，以在离散网格上识别和分类惩罚目标函数 $F_r(x,y)$ 的局部最小值。解决方案是根据所提供的算法规范，系统地进行开发的。\n\n**1. 问题表述**\n\n原始约束优化问题是最小化非凸目标函数，\n$$\nf(x,y) = (x^2 - 1)^2 + (y^2 - 1)^2 + \\tfrac{1}{2}\\,\\sin(3x)\\sin(3y)\n$$\n服从以下约束\n$$\ng(x,y) = x + y = 0 \\quad \\text{(等式)}\n$$\n$$\nh(x,y) = x^2 + y^2 - 3 \\le 0 \\quad \\text{(不等式)}\n$$\n二次罚函数法通过在目标函数中增加仅在违反约束时才非零的惩罚项，将此问题转化为一个无约束问题。得到的惩罚目标函数是：\n$$\nF_r(x,y) = f(x,y) + \\frac{r}{2}\\,g(x,y)^2 + \\frac{r}{2}\\,(\\max\\{0, h(x,y)\\})^2\n$$\n其中 $r \\ge 0$ 是惩罚参数。当 $r \\to \\infty$ 时，$F_r(x,y)$ 的最小值预计会收敛到原始约束问题的解。伪最小值是对于有限的 $r$，$F_r$ 的不满足原始约束的局部最小值。\n\n**2. 算法设计**\n\n指定的算法按以下步骤进行：\n\n*   **离散化：** 将连续的方形域 $[-2,2]\\times[-2,2]$ 离散化为一个 $401 \\times 401$ 的均匀网格。每个轴上的步长为 $\\Delta x = \\Delta y = (2 - (-2)) / (401 - 1) = 0.01$。网格点为 $(x_j, y_i)$，其中 $x_j = -2 + j \\cdot \\Delta x$ 且 $y_i = -2 + i \\cdot \\Delta y$，对于 $i,j \\in \\{0, 1, \\dots, 400\\}$。\n\n*   **函数求值：** 对于每个测试用例 $r \\in \\{0, 1, 10, 200\\}$，在网格上的每个点计算函数 $F_r(x,y)$ 的值，得到一个 $401 \\times 401$ 的值矩阵，我们将其表示为 $\\mathbf{F}$。元素 $\\mathbf{F}_{i,j}$ 对应于值 $F_r(x_j, y_i)$。\n\n*   **局部最小值识别：** 我们遍历网格的内部（$1 \\le i,j \\le 399$），检查每个点是否具有局部最小值的性质。一个值为 $V_c = \\mathbf{F}_{i,j}$ 的网格点 $(x_j, y_i)$ 是局部最小值，当且仅当它相对于其八个邻居 $V_n$ 满足两个条件：\n    1.  对于所有八个邻居，满足 $V_c \\le V_n + \\varepsilon_{\\text{lm}}$。\n    2.  对于至少一个邻居，满足 $V_c  V_n - \\varepsilon_{\\text{lm}}$。\n    使用容差 $\\varepsilon_{\\text{lm}} = 10^{-12}$ 进行稳健的浮点数比较。这个定义能正确处理倾斜区域，同时避免将整个平坦的高原区域归类为最小值。\n\n*   **可行性与伪特性分类：** 对每个在网格坐标 $(x_j, y_i)$ 处识别出的局部最小值，都会根据原始约束进行可行性测试。使用容差 $\\tau = 10^{-6}$。如果满足以下条件，则该最小值被视为**可行的**：\n    $$\n    |g(x_j, y_i)| \\le \\tau \\quad \\text{且} \\quad h(x_j, y_i) \\le \\tau\n    $$\n    如果这些条件中任何一个不满足，该局部最小值就被归类为**伪的**。\n\n*   **计数：** 对于每个 $r$ 值，计算在网格上找到的伪局部最小值的总数。\n\n**3. 惩罚参数分析**\n\n参数 $r$ 控制对违反约束的惩罚权重。\n*   当 $r=0$ 时，我们有 $F_0(x,y) = f(x,y)$。算法将找到原始目标函数的局部最小值，完全忽略约束。鉴于正弦项的振荡性质和多项式部分的四个“井”，预计会有许多局部最小值。其中大部分将不满足约束，因此将被计为伪最小值。\n*   随着 $r$ 的增加（$r=1, 10, 200$），惩罚项对远离可行区域（圆 $x^2+y^2=3$ 内的线段 $y=-x$）的点增加了高昂的“代价”。这在 $F_r$ 的函数图像上沿着可行集形成了一个深深的“峡谷”。位于此峡谷之外的局部最小值被提升到更高的函数值并趋于消失。剩余的最小值被推向可行集。因此，我们预计随着 $r$ 的增加，伪局部最小值的数量会减少。对于足够大的 $r$，$F_r$ 的局部最小值将位于非常接近或恰好在可行集上的网格点，从而导致伪最小值的计数很低或为零。\n\n该算法使用 Python 实现，并遵循这些原则。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and analyzes a non-convex penalized objective to find the number of\n    spurious local minima for different penalty parameters.\n    \"\"\"\n\n    # Define the test cases (penalty parameters) from the problem statement.\n    test_cases = [0, 1, 10, 200]\n    \n    # List to store the count of spurious minima for each case.\n    results = []\n\n    # --- Problem-specific functions ---\n    def f_obj(x, y):\n        \"\"\"Vectorized objective function f(x, y).\"\"\"\n        return (x**2 - 1)**2 + (y**2 - 1)**2 + 0.5 * np.sin(3 * x) * np.sin(3 * y)\n\n    def g_constraint(x, y):\n        \"\"\"Vectorized equality constraint g(x, y).\"\"\"\n        return x + y\n\n    def h_constraint(x, y):\n        \"\"\"Vectorized inequality constraint h(x, y).\"\"\"\n        return x**2 + y**2 - 3\n\n    def F_penalized(x, y, r):\n        \"\"\"Vectorized penalized objective function F_r(x, y).\"\"\"\n        penalty_g = (r / 2) * g_constraint(x, y)**2\n        penalty_h = (r / 2) * np.maximum(0, h_constraint(x, y))**2\n        return f_obj(x, y) + penalty_g + penalty_h\n\n    # --- Main loop for each test case ---\n    for r in test_cases:\n        # --- Algorithm Specification ---\n        N = 401  # Grid resolution\n        domain_min, domain_max = -2, 2\n        eps_lm = 1e-12  # Tolerance for local minimum detection\n        tau = 1e-6  # Tolerance for feasibility classification\n\n        # 1. Discretization of the domain\n        x_range = np.linspace(domain_min, domain_max, N)\n        y_range = np.linspace(domain_min, domain_max, N)\n        # Note: meshgrid with 'xy' indexing (default) means xx varies with the second index (columns)\n        # and yy varies with the first index (rows). F_values[i, j] corresponds to (x[j], y[i]).\n        xx, yy = np.meshgrid(x_range, y_range)\n\n        # 2. Evaluation of the penalized function on the grid\n        F_values = F_penalized(xx, yy, r)\n\n        spurious_minima_count = 0\n\n        # 3. Local Minimum Detection and Classification\n        # Iterate through the interior points of the grid.\n        for i in range(1, N - 1):\n            for j in range(1, N - 1):\n                center_val = F_values[i, j]\n\n                # Extract the 8 neighbors' values\n                neighbors = np.array([\n                    F_values[i - 1, j - 1], F_values[i - 1, j], F_values[i - 1, j + 1],\n                    F_values[i,     j - 1],                     F_values[i,     j + 1],\n                    F_values[i + 1, j - 1], F_values[i + 1, j], F_values[i + 1, j + 1]\n                ])\n\n                # Check the two conditions for a local minimum\n                is_le_all_neighbors = np.all(center_val = neighbors + eps_lm)\n                is_lt_one_neighbor = np.any(center_val  neighbors - eps_lm)\n\n                if is_le_all_neighbors and is_lt_one_neighbor:\n                    # This grid point is a local minimum. Now classify it.\n                    x_coord = x_range[j]\n                    y_coord = y_range[i]\n                    \n                    # 4. Feasibility Classification\n                    g_val = g_constraint(x_coord, y_coord)\n                    h_val = h_constraint(x_coord, y_coord)\n                    \n                    is_feasible = (np.abs(g_val) = tau) and (h_val = tau)\n                    \n                    if not is_feasible:\n                        spurious_minima_count += 1\n        \n        results.append(spurious_minima_count)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3261519"}, {"introduction": "标准二次罚方法的一个主要实践缺陷是，它需要极大的罚参数才能达到高精度，但这通常会导致子问题出现严重的数值病态。本实践将通过在一个病态问题上比较标准罚方法及其强大的后继者——增广拉格朗日方法，来直面这个问题[@problem_id:3261582]。你将实现这两种方法，并亲眼观察增广拉格朗日方法如何在不需要罚参数趋于无穷大的情况下，实现更高的精度和更快的收敛，从而保持数值稳定性。", "problem": "考虑一个在$2$维欧几里得空间中的等式约束优化问题：\n$$\n\\min_{x \\in \\mathbb{R}^2} \\; f(x) \\quad \\text{subject to} \\quad c(x) = a^\\top x - b = 0,\n$$\n其中 $f(x) = \\tfrac{1}{2} x^\\top Q x$，矩阵 $Q \\in \\mathbb{R}^{2 \\times 2}$ 是病态的，$a \\in \\mathbb{R}^2$，$b \\in \\mathbb{R}$。病态性是通过使 $Q$ 为对角矩阵且其对角线上的元素相差几个数量级来引入的。约束是线性的，由向量 $a$ 和标量 $b$ 指定。您将实现并比较两种方法：标准的二次罚函数法和增广拉格朗日法。\n\n从二次罚函数法的基本定义出发，构建一系列无约束最小化问题\n$$\n\\min_{x \\in \\mathbb{R}^2} \\; f(x) + \\tfrac{\\rho}{2} \\, c(x)^2,\n$$\n其中当约束违反量 $|c(x)|$ 不在规定的容差范围内时，罚参数 $\\rho$ 会增加。对于每个固定的 $\\rho$，通过求解一阶最优性条件来精确地执行内部无约束最小化，这将产生一个对称正定（SPD）线性系统。继续以几何级数方式增加 $\\rho$，直到约束违反量低于容差或达到最大外部迭代次数。记录所需的外部迭代次数、最终的约束违反量 $|c(x)|$ 和最终的目标函数值 $f(x)$。\n\n从增广拉格朗日法的基本定义出发，构建一系列无约束最小化问题\n$$\n\\min_{x \\in \\mathbb{R}^2} \\; f(x) + \\lambda \\, c(x) + \\tfrac{\\mu}{2} \\, c(x)^2,\n$$\n其中包含拉格朗日乘子估计值 $\\lambda$ 和罚参数 $\\mu$。对于每个固定的 $(\\lambda, \\mu)$，通过求解一阶最优性条件来精确地执行内部无约束最小化，这将产生一个对称正定线性系统。得到最小化子后，使用从乘子法派生出的经典法则更新拉格朗日乘子，并且如果约束违反量没有充分减小，则以几何级数方式调整 $\\mu$。当约束违反量 $|c(x)|$ 在容差范围内或达到最大外部迭代次数时终止。记录所需的外部迭代次数、最终的约束违反量 $|c(x)|$ 和最终的目标函数值 $f(x)$。\n\n实现这两种方法来解决具有以下固定数据的问题：\n- 矩阵为 $Q = \\operatorname{diag}(q_1, q_2)$。\n- 约束由 $a = [1, 1]^\\top$ 和 $b = 1$ 定义。\n\n实现程序以处理以下测试套件。每个测试用例指定 $(q_1, q_2)$、罚函数法参数 $(\\rho_0, \\gamma_\\rho)$、增广拉格朗日法参数 $(\\mu_0, \\gamma_\\mu)$ 和容差：\n- 测试用例 $1$ (非常病态):\n  - $q_1 = 10^{-6}$, $q_2 = 1$,\n  - 罚函数法：初始 $\\rho_0 = 1$，几何因子 $\\gamma_\\rho = 10$，\n  - 增广拉格朗日法：初始 $\\mu_0 = 1$，几何因子 $\\gamma_\\mu = 2$，\n  - 容差 $\\text{tol} = 10^{-8}$。\n- 测试用例 $2$ (中等病态):\n  - $q_1 = 10^{-4}$, $q_2 = 1$,\n  - 罚函数法：初始 $\\rho_0 = 10^{-1}$，几何因子 $\\gamma_\\rho = 5$，\n  - 增广拉格朗日法：初始 $\\mu_0 = \\tfrac{1}{2}$，几何因子 $\\gamma_\\mu = 2$，\n  - 容差 $\\text{tol} = 10^{-6}$。\n- 测试用例 $3$ (极端病态):\n  - $q_1 = 10^{-8}$, $q_2 = 1$,\n  - 罚函数法：初始 $\\rho_0 = 10^{-3}$，几何因子 $\\gamma_\\rho = 10$，\n  - 增广拉格朗日法：初始 $\\mu_0 = \\tfrac{1}{2}$，几何因子 $\\gamma_\\mu = 3$，\n  - 容差 $\\text{tol} = 10^{-10}$。\n\n对于每个测试用例，您的程序必须：\n- 运行罚函数法并报告 $(N_P, V_P, F_P)$，其中 $N_P$ 是外部迭代次数，$V_P$ 是最终约束违反量 $|c(x)|$，$F_P$ 是最终目标函数值 $f(x)$。\n- 运行增广拉格朗日法并报告 $(N_A, V_A, F_A)$，其定义与前者类似。\n\n设计选择：\n- 对于内部求解，使用从一阶最优性条件获得的的精确解，这会产生维度为 $2$ 的 SPD 线性系统。\n- 为每种方法设置最多 $100$ 次外部迭代以避免无限循环。\n- 对 $\\rho$ 和 $\\mu$ 使用带有指定因子的几何更新规则。\n- 对于增广拉格朗日法，使用经典的乘子法规则更新 $\\lambda$，并且仅当与前一次迭代相比约束违反量没有充分减小时才增加 $\\mu$。\n\n您的程序应生成单行输出，其中包含用方括号括起来的逗号分隔列表形式的结果。外部列表必须为每个测试用例包含一个内部列表，每个内部列表按 $[N_P,N_A,V_P,V_A,F_P,F_A]$ 的顺序排列。例如，最终输出格式必须是：\n$$\n\\big[ \\, [N_P^{(1)},N_A^{(1)},V_P^{(1)},V_A^{(1)},F_P^{(1)},F_A^{(1)}], \\; [N_P^{(2)},N_A^{(2)},V_P^{(2)},V_A^{(2)},F_P^{(2)},F_A^{(2)}], \\; [N_P^{(3)},N_A^{(3)},V_P^{(3)},V_A^{(3)},F_P^{(3)},F_A^{(3)}] \\, \\big].\n$$\n所有报告的值必须是实数或整数，不需要物理单位，也不涉及角度。", "solution": "用户提供了一个适定的约束优化问题。该问题具有科学依据，是客观的，并包含了解决该问题所需的所有必要信息。因此，它被认为是有效的。\n\n该问题要求使用两种不同的方法解决一个等式约束二次规划（QP）问题：二次罚函数法和增广拉格朗日法。我们将分析每种方法，推导实现所需的方程，然后提出算法。\n\n该优化问题定义为：\n$$\n\\min_{x \\in \\mathbb{R}^2} \\; f(x) = \\frac{1}{2} x^\\top Q x\n$$\n$$\n\\text{subject to} \\quad c(x) = a^\\top x - b = 0\n$$\n其中 $x = [x_1, x_2]^\\top$，$Q = \\operatorname{diag}(q_1, q_2)$ 是一个正定但病态的矩阵，$a = [1, 1]^\\top$，$b = 1$。\n\n### 二次罚函数法\n\n该方法通过在目标函数中添加一个惩罚约束违反的罚项，将约束问题转化为一系列无约束问题。对于给定的罚参数 $\\rho  0$，带罚项的目标函数是：\n$$\nP(x; \\rho) = f(x) + \\frac{\\rho}{2} c(x)^2 = \\frac{1}{2} x^\\top Q x + \\frac{\\rho}{2} (a^\\top x - b)^2\n$$\n在外部迭代 $k$ 中，对于固定的 $\\rho_k$，我们通过求解无约束问题 $\\min_x P(x; \\rho_k)$ 来找到最小化子 $x_{k+1}$。一阶最优性条件是 $\\nabla_x P(x; \\rho_k) = 0$。梯度为：\n$$\n\\nabla_x P(x; \\rho_k) = \\nabla f(x) + \\rho_k c(x) \\nabla c(x) = Qx + \\rho_k (a^\\top x - b) a\n$$\n将梯度设为零可得：\n$$\nQx + \\rho_k (a^\\top x) a - \\rho_k b a = 0\n$$\n这可以重排为一个关于 $x$ 的线性方程组：\n$$\n(Q + \\rho_k a a^\\top) x = \\rho_k b a\n$$\n矩阵 $H_P(\\rho_k) = Q + \\rho_k a a^\\top$ 是罚函数 $P(x; \\rho_k)$ 的黑塞矩阵（Hessian）。由于 $Q$ 是正定的，$a a^\\top$ 是半正定的，因此对于任何 $\\rho_k  0$，$H_P(\\rho_k)$ 都是对称正定（SPD）的。在外部循环的每一步，我们求解这个 $2 \\times 2$ 的线性系统以得到 $x_{k+1}$。然后增加罚参数，即 $\\rho_{k+1} = \\gamma_\\rho \\rho_k$（其中 $\\gamma_\\rho  1$），并重复此过程。\n\n当约束违反量 $|c(x_{k+1})| = |a^\\top x_{k+1} - b|$ 小于指定的容差 $\\text{tol}$ 时，该方法终止。该方法的一个已知缺点是，当 $\\rho_k \\to \\infty$（这是 $c(x) \\to 0$ 所必需的）时，黑塞矩阵 $H_P(\\rho_k)$ 的条件数会无界增长，导致数值上病态的线性系统。\n\n### 增广拉格朗日法\n\n这种方法，也称为乘子法，通过在目标函数中加入拉格朗日乘子的估计值来增强简单的罚函数方法。这通常会带来更好的收敛性质，并避免了罚参数趋于无穷大的需要。无约束子问题涉及最小化增广拉格朗日函数：\n$$\n\\mathcal{L}_A(x, \\lambda; \\mu) = f(x) + \\lambda c(x) + \\frac{\\mu}{2} c(x)^2\n$$\n其中 $\\lambda$ 是拉格朗日乘子估计值，$\\mu  0$ 是一个罚参数。在外部迭代 $k$ 中，我们使用当前的估计值 $\\lambda_k$ 和 $\\mu_k$ 来通过求解 $\\min_x \\mathcal{L}_A(x, \\lambda_k; \\mu_k)$ 找到下一个迭代点 $x_{k+1}$。一阶最优性条件是 $\\nabla_x \\mathcal{L}_A(x, \\lambda_k; \\mu_k) = 0$：\n$$\n\\nabla_x \\mathcal{L}_A = \\nabla f(x) + \\lambda_k \\nabla c(x) + \\mu_k c(x) \\nabla c(x) = Qx + \\lambda_k a + \\mu_k (a^\\top x - b) a\n$$\n将梯度设为零并重排，得到线性系统：\n$$\n(Q + \\mu_k a a^\\top) x = (\\mu_k b - \\lambda_k) a\n$$\n黑塞矩阵 $H_A(\\mu_k) = Q + \\mu_k a a^\\top$ 在结构上与罚函数法的黑塞矩阵相同，并且也是对称正定的。求解出 $x_{k+1}$ 后，使用以下规则更新拉格朗日乘子估计值：\n$$\n\\lambda_{k+1} = \\lambda_k + \\mu_k c(x_{k+1})\n$$\n罚参数 $\\mu_k$ 不需要趋于无穷大。它只需要足够大，以使增广拉格朗日函数的黑塞矩阵在解的邻域内是正定的，而这一点在这里已经得到保证，因为 $Q$ 是正定的。在实践中，如果约束违反量的减少不充分，则增加 $\\mu_k$。我们将采用这样的规则：如果对于 $k0$ 有 $|c(x_{k+1})|  0.25 |c(x_k)|$，我们就更新 $\\mu_{k+1} = \\gamma_\\mu \\mu_k$；否则，$\\mu_{k+1} = \\mu_k$。这种策略使得该方法能够在对罚参数要求宽松得多的条件下收敛到可行和最优解，从而避免了二次罚函数法固有的严重病态问题。\n\n### 算法实现\n\n对于这两种方法，我们都实现一个外部循环来迭代地精化解。每个外部迭代都涉及构建和求解一个 $2 \\times 2$ 的 SPD 线性系统。当约束违反量 $|c(x)|$ 低于容差 $\\text{tol}$ 或达到最大 $100$ 次迭代时，循环终止。\n\n对于给定的问题参数 $a = [1, 1]^\\top$ 和 $b=1$，矩阵 $H_P$ 和 $H_A$ 为：\n$$\nH_P(\\rho_k) = \\begin{pmatrix} q_1 + \\rho_k  \\rho_k \\\\ \\rho_k  q_2 + \\rho_k \\end{pmatrix}, \\quad H_A(\\mu_k) = \\begin{pmatrix} q_1 + \\mu_k  \\mu_k \\\\ \\mu_k  q_2 + \\mu_k \\end{pmatrix}\n$$\n右端向量为：\n$$\nd_P = \\rho_k b a = \\begin{pmatrix} \\rho_k \\\\ \\rho_k \\end{pmatrix}, \\quad d_A = (\\mu_k b - \\lambda_k) a = \\begin{pmatrix} \\mu_k - \\lambda_k \\\\ \\mu_k - \\lambda_k \\end{pmatrix}\n$$\n该实现将处理三个指定的测试用例，报告罚函数法（P）和增广拉格朗日法（A）的迭代次数（$N$）、最终约束违反量（$V$）和最终目标函数值（$F$）。", "answer": "```python\nimport numpy as np\n\ndef penalty_method(Q, a, b, rho0, gamma_rho, tol, max_iter):\n    \"\"\"\n    Solves a constrained QP using the quadratic penalty method.\n    \"\"\"\n    rho = rho0\n    x = np.zeros(a.shape)\n    \n    for k in range(max_iter):\n        # Form the Hessian and the right-hand side of the linear system\n        H = Q + rho * np.outer(a, a)\n        d = rho * b * a\n        \n        try:\n            x = np.linalg.solve(H, d)\n        except np.linalg.LinAlgError:\n            # This should not occur with the given problem structure (SPD Hessian)\n            return max_iter, np.inf, np.inf\n\n        # Calculate constraint violation\n        violation = np.abs(np.dot(a, x) - b)\n        \n        # Check for convergence\n        if violation  tol:\n            obj_val = 0.5 * np.dot(x, Q @ x)\n            return k + 1, violation, obj_val\n        \n        # Update the penalty parameter\n        rho *= gamma_rho\n        \n    # If max_iter is reached\n    obj_val = 0.5 * np.dot(x, Q @ x)\n    violation = np.abs(np.dot(a, x) - b)\n    return max_iter, violation, obj_val\n\ndef augmented_lagrangian_method(Q, a, b, mu0, gamma_mu, tol, max_iter):\n    \"\"\"\n    Solves a constrained QP using the augmented Lagrangian method.\n    \"\"\"\n    mu = mu0\n    lambda_ = 0.0  # Lagrange multiplier estimate\n    x = np.zeros(a.shape)\n    prev_violation = np.inf\n    \n    for k in range(max_iter):\n        # Form the Hessian and the right-hand side of the linear system\n        H = Q + mu * np.outer(a, a)\n        d = (mu * b - lambda_) * a\n        \n        try:\n            x = np.linalg.solve(H, d)\n        except np.linalg.LinAlgError:\n            return max_iter, np.inf, np.inf\n        \n        # Calculate constraint violation\n        c_x = np.dot(a, x) - b\n        violation = np.abs(c_x)\n        \n        # Check for convergence\n        if violation  tol:\n            obj_val = 0.5 * np.dot(x, Q @ x)\n            return k + 1, violation, obj_val\n        \n        # Update mu if constraint violation does not decrease sufficiently\n        if k > 0 and violation > 0.25 * prev_violation:\n            mu *= gamma_mu\n        \n        # Update lambda\n        lambda_ += mu * c_x\n        \n        prev_violation = violation\n        \n    # If max_iter is reached\n    obj_val = 0.5 * np.dot(x, Q @ x)\n    violation = np.abs(np.dot(a, x) - b)\n    return max_iter, violation, obj_val\n\ndef solve():\n    \"\"\"\n    Main function to run test cases and print results.\n    \"\"\"\n    test_cases = [\n        # Test case 1 (very ill-conditioned)\n        {'q1': 1e-6, 'q2': 1.0, 'rho0': 1.0, 'gamma_rho': 10.0, 'mu0': 1.0, 'gamma_mu': 2.0, 'tol': 1e-8},\n        # Test case 2 (moderately ill-conditioned)\n        {'q1': 1e-4, 'q2': 1.0, 'rho0': 0.1, 'gamma_rho': 5.0, 'mu0': 0.5, 'gamma_mu': 2.0, 'tol': 1e-6},\n        # Test case 3 (extremely ill-conditioned)\n        {'q1': 1e-8, 'q2': 1.0, 'rho0': 1e-3, 'gamma_rho': 10.0, 'mu0': 0.5, 'gamma_mu': 3.0, 'tol': 1e-10}\n    ]\n    \n    a = np.array([1.0, 1.0])\n    b = 1.0\n    max_iter = 100\n    \n    all_results = []\n    \n    for case in test_cases:\n        Q = np.diag([case['q1'], case['q2']])\n        \n        # Run Penalty Method\n        Np, Vp, Fp = penalty_method(Q, a, b, case['rho0'], case['gamma_rho'], case['tol'], max_iter)\n        \n        # Run Augmented Lagrangian Method\n        Na, Va, Fa = augmented_lagrangian_method(Q, a, b, case['mu0'], case['gamma_mu'], case['tol'], max_iter)\n        \n        case_results = [Np, Na, Vp, Va, Fp, Fa]\n        all_results.append(case_results)\n\n    # Format the output string as a list of lists\n    inner_strings = []\n    for res in all_results:\n        # Format each number to string\n        inner_strings.append(f\"[{','.join(map(str, res))}]\")\n    output_str = f\"[{','.join(inner_strings)}]\"\n    \n    print(output_str)\n\nsolve()\n```", "id": "3261582"}]}