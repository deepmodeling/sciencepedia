{"hands_on_practices": [{"introduction": "掌握Broyden–Fletcher–Goldfarb–Shanno (BFGS)算法的第一步是熟悉其核心的更新公式。本练习将让你聚焦于BFGS更新中的一个关键部分——秩一校正项，通过具体计算来理解算法的构成模块。这个基础计算是理解算法后续更复杂行为的起点 [@problem_id:2195881]。", "problem": "在数值优化领域，Broyden–Fletcher–Goldfarb–Shanno (BFGS) 算法是一种流行的拟牛顿法，用于求解无约束非线性优化问题。在每次迭代中，该算法都会近似目标函数的 Hessian 矩阵。从一个初始近似 $B_k$ 开始，下一个近似 $B_{k+1}$ 使用以下更新公式计算：\n$$B_{k+1} = B_k + \\frac{y_k y_k^T}{y_k^T s_k} - \\frac{B_k s_k s_k^T B_k}{s_k^T B_k s_k}$$\n其中 $s_k$ 是所用的步长，$y_k$ 是梯度的变化量。该更新包括将两个秩一矩阵加到当前的近似 $B_k$ 上。\n\n考虑 BFGS 算法的单次迭代，其中当前的 Hessian 近似是 $2 \\times 2$ 的单位矩阵，$B_k = I$。步长向量由 $s_k = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$ 给出，梯度变化向量为 $y_k = \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix}$。\n\n计算 $B_{k+1}$ 的负秩一更新项，该项由表达式 $-\\frac{B_k s_k s_k^T B_k}{s_k^T B_k s_k}$ 给出。你的最终答案应该是一个 $2 \\times 2$ 矩阵。", "solution": "题目要求我们计算负秩一更新项\n$$-\\frac{B_{k}s_{k}s_{k}^{T}B_{k}}{s_{k}^{T}B_{k}s_{k}}$$\n当 $B_{k}=I$，$s_{k}=\\begin{pmatrix}1 \\\\ 0\\end{pmatrix}$ 时，并且 $y_{k}$ 已给出但计算此项时用不到。\n\n首先，利用 $B_{k}=I$ 来简化分子：\n$$B_{k}s_{k} = Is_{k} = s_{k}, \\quad \\text{且} \\quad B_{k}s_{k}s_{k}^{T}B_{k} = s_{k}s_{k}^{T}.$$\n\n接下来，计算分母：\n$$s_{k}^{T}B_{k}s_{k} = s_{k}^{T}Is_{k} = s_{k}^{T}s_{k}.$$\n对于 $s_{k}=\\begin{pmatrix}1 \\\\ 0\\end{pmatrix}$，我们有\n$$s_{k}^{T}s_{k} = 1^{2} + 0^{2} = 1.$$\n\n因此，负秩一更新项变为\n$$-\\frac{B_{k}s_{k}s_{k}^{T}B_{k}}{s_{k}^{T}B_{k}s_{k}} = -\\frac{s_{k}s_{k}^{T}}{s_{k}^{T}s_{k}} = -s_{k}s_{k}^{T}.$$\n\n明确计算 $s_{k}s_{k}^{T}$：\n$$s_{k}s_{k}^{T} = \\begin{pmatrix}1 \\\\ 0\\end{pmatrix}\\begin{pmatrix}1 & 0\\end{pmatrix} = \\begin{pmatrix}1 & 0 \\\\ 0 & 0\\end{pmatrix}.$$\n\n因此，负秩一更新项是\n$$-\\begin{pmatrix}1 & 0 \\\\ 0 & 0\\end{pmatrix} = \\begin{pmatrix}-1 & 0 \\\\ 0 & 0\\end{pmatrix}.$$", "answer": "$$\\boxed{\\begin{pmatrix}-1 & 0 \\\\ 0 & 0\\end{pmatrix}}$$", "id": "2195881"}, {"introduction": "在理解了BFGS的更新机制之后，我们来探讨一个更深层次的问题：为什么要确保Hessian矩阵的近似 $B_k$ 是正定的？正定性保证了我们找到的搜索方向是一个下降方向，从而使函数值在迭代中减小。本练习通过一个精心设计的反例，直观地揭示了当 $B_k$ 非正定时，算法可能会生成一个上升方向，从而导致迭代失败 [@problem_id:2195908]。", "problem": "在采用拟牛顿法的迭代优化算法中，第 $k$ 次迭代的搜索方向 $p_k$ 通过求解线性系统 $B_k p_k = -\\nabla f_k$ 来确定，其中 $B_k$ 是海森矩阵的当前近似，而 $\\nabla f_k$ 是目标函数 $f$ 在迭代点 $x_k$ 处的梯度。如果沿着方向 $p_k$ 走一小步会减小函数值，那么该方向就被视为一个下降方向，这在数学上对应于条件 $p_k^T \\nabla f_k < 0$。\n\n假设在某次迭代 $k$ 中，梯度由下式给出\n$$\n\\nabla f_k = \\begin{pmatrix} 1 \\\\ -2 \\end{pmatrix}\n$$\n且对称的海森矩阵近似为\n$$\nB_k = \\begin{pmatrix} 1 & 2 \\\\ 2 & 1 \\end{pmatrix}\n$$\n根据这些值，下列哪个陈述正确地描述了计算出的搜索方向 $p_k$？\n\nA. 搜索方向是上升方向。\n\nB. 搜索方向是下降方向。\n\nC. 搜索方向与梯度正交。\n\nD. 对于搜索方向，拟牛顿方程无解。\n\nE. 搜索方向是零向量，表示算法应该停止。", "solution": "我们使用拟牛顿搜索方向的定义：求解线性系统 $B_{k} p_{k} = -\\nabla f_{k}$，然后检验下降条件 $p_{k}^{T} \\nabla f_{k} < 0$。\n\n给定 $\\nabla f_{k} = \\begin{pmatrix} 1 \\\\ -2 \\end{pmatrix}$ 和 $B_{k} = \\begin{pmatrix} 1 & 2 \\\\ 2 & 1 \\end{pmatrix}$，我们求解\n$$\n\\begin{pmatrix} 1 & 2 \\\\ 2 & 1 \\end{pmatrix} \\begin{pmatrix} p_{1} \\\\ p_{2} \\end{pmatrix} = - \\begin{pmatrix} 1 \\\\ -2 \\end{pmatrix} = \\begin{pmatrix} -1 \\\\ 2 \\end{pmatrix}.\n$$\n这得到线性系统\n$$\np_{1} + 2 p_{2} = -1, \\quad 2 p_{1} + p_{2} = 2.\n$$\n从第一个方程，得到 $p_{1} = -1 - 2 p_{2}$。代入第二个方程，得到\n$$\n2(-1 - 2 p_{2}) + p_{2} = 2 \\;\\Rightarrow\\; -2 - 4 p_{2} + p_{2} = 2 \\;\\Rightarrow\\; -3 p_{2} = 4 \\;\\Rightarrow\\; p_{2} = -\\frac{4}{3}.\n$$\n那么\n$$\np_{1} = -1 - 2\\left(-\\frac{4}{3}\\right) = -1 + \\frac{8}{3} = \\frac{5}{3}.\n$$\n因此\n$$\np_{k} = \\begin{pmatrix} \\frac{5}{3} \\\\ -\\frac{4}{3} \\end{pmatrix}.\n$$\n为对方向进行分类，计算方向导数：\n$$\np_{k}^{T} \\nabla f_{k} = \\begin{pmatrix} \\frac{5}{3} & -\\frac{4}{3} \\end{pmatrix} \\begin{pmatrix} 1 \\\\ -2 \\end{pmatrix} = \\frac{5}{3} + \\frac{8}{3} = \\frac{13}{3} > 0.\n$$\n由于 $p_{k}^{T} \\nabla f_{k} > 0$，沿 $p_{k}$ 方向走一个小的正步长会使 $f$ 增加，因此 $p_{k}$ 是一个上升方向。该系统有唯一解，因为 $\\det(B_{k}) = 1 \\cdot 1 - 2 \\cdot 2 = -3 \\neq 0$，且 $p_{k} \\neq 0$ 并且与梯度不正交。", "answer": "$$\\boxed{A}$$", "id": "2195908"}, {"introduction": "将理论付诸实践是检验学习成果的最佳方式。这个综合性练习要求你动手编写一个完整的BFGS算法，并将其应用于一个非严格凸函数的最小化问题。通过观察算法在存在“平坦”最优解集区域的行为，你将对拟牛顿法在更复杂、更现实场景中的表现有更深刻的认识 [@problem_id:3264863]。", "problem": "考虑最小化一个二次连续可微但非严格凸的函数。构造这样一个函数如下：设 $f:\\mathbb{R}^2\\to\\mathbb{R}$ 定义为 $f(x_1,x_2)=x_1^2$。该函数是凸的，但非严格凸，因为其海森矩阵是半正定的，且具有非平凡零空间。实现 Broyden–Fletcher–Goldfarb–Shanno (BFGS) 拟牛顿法，在 $\\mathbb{R}^2$ 上最小化函数 $f$，使用逆海森矩阵近似和满足 Armijo 充分下降条件的回溯线搜索。算法应在 $\\|\\nabla f(x_k)\\|_2 \\le \\varepsilon$ 或达到最大迭代次数 $N_{\\max}$ 后停止。\n\n从以下基本概念出发：\n- 通过二阶导数表征的凸性定义：如果函数 $f$ 的海森矩阵 $\\nabla^2 f(x)$ 在其定义域内对所有 $x$ 都是半正定的，则该函数是凸的。\n- 基于梯度的下降范式：迭代更新 $x_{k+1}=x_k+\\alpha_k p_k$，其中 $p_k$ 是一个下降方向，$\\alpha_k>0$ 满足充分下降条件。\n- 拟牛顿法：使用逆海森矩阵的迭代近似 $H_k$ 来定义搜索方向 $p_k=-H_k\\nabla f(x_k)$，其中 $H_k$ 的更新需满足割线条件并保持正定性。\n\n你的程序必须：\n- 实现函数 $f(x_1,x_2)=x_1^2$ 及其梯度 $\\nabla f(x_1,x_2)=(2x_1,0)$。\n- 初始化 $H_0=\\gamma I$，其中 $I$ 是单位矩阵，$\\gamma>0$ 是每个测试用例提供的标量。\n- 使用带有 Armijo 条件 $f(x_k+\\alpha p_k)\\le f(x_k)+c_1\\alpha\\nabla f(x_k)^\\top p_k$ 的回溯线搜索，其中固定的 $c_1$ 和回溯因子 $\\beta\\in(0,1)$ 由你选择，但在所有测试用例中保持不变。\n- 根据由拟牛顿割线条件推导出的标准 BFGS 逆矩阵更新公式来更新 $H_k$，前提是曲率条件 $s_k^\\top y_k>0$ 成立，其中 $s_k=x_{k+1}-x_k$ 且 $y_k=\\nabla f(x_{k+1})-\\nabla f(x_k)$。\n\n设计一个包含以下参数集的测试套件，每个参数集指定了初始点 $(x_1^{(0)},x_2^{(0)})$、容差 $\\varepsilon$、最大迭代次数 $N_{\\max}$ 和初始缩放因子 $\\gamma$：\n- 测试 1：$(x_1^{(0)},x_2^{(0)})=(5,7)$，$\\varepsilon=10^{-9}$，$N_{\\max}=50$，$\\gamma=1$。\n- 测试 2：$(x_1^{(0)},x_2^{(0)})=(0,3)$，$\\varepsilon=10^{-9}$，$N_{\\max}=50$，$\\gamma=1$。\n- 测试 3：$(x_1^{(0)},x_2^{(0)})=(-10,-2)$，$\\varepsilon=10^{-9}$，$N_{\\max}=50$，$\\gamma=1$。\n- 测试 4：$(x_1^{(0)},x_2^{(0)})=(10^6,10^6)$，$\\varepsilon=10^{-12}$，$N_{\\max}=100$，$\\gamma=1$。\n- 测试 5：$(x_1^{(0)},x_2^{(0)})=(5,7)$，$\\varepsilon=10^{-9}$，$N_{\\max}=50$，$\\gamma=10$。\n\n对于每个测试，通过检查 $|x_{2,\\text{final}}-x_{2,\\text{initial}}|\\le\\tau$ 来计算第二个坐标是否在用户定义的容差 $\\tau$ 内保持不变，其中应为该测试设置 $\\tau=\\varepsilon$。\n\n你的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果。每个测试的结果必须是 $[x_{1,\\text{final}},x_{2,\\text{final}},f(x_{\\text{final}}),\\text{iterations},\\text{x2\\_unchanged}]$ 形式的列表，其中 $x_{1,\\text{final}}$ 和 $x_{2,\\text{final}}$ 是浮点数，$f(x_{\\text{final}})$ 是浮点数，$\\text{iterations}$ 是整数，$\\text{x2\\_unchanged}$ 是一个布尔值，表示 $x_2$ 是否在容差内保持不变。最终输出将所有提供的测试用例的结果汇总到一个列表中，例如 $[[x_{1,\\text{final}}^{(1)},x_{2,\\text{final}}^{(1)},\\dots],[x_{1,\\text{final}}^{(2)},x_{2,\\text{final}}^{(2)},\\dots],\\dots]$。", "solution": "该问题要求实现并分析 Broyden–Fletcher–Goldfarb–Shanno (BFGS) 拟牛顿优化算法，用于最小化一个非严格凸函数。问题陈述的有效性得到确认，因为它在科学上基于数值优化理论，是适定的、客观的，并为实现和测试提供了一套完整的规范。\n\n任务的核心是应用形式为 $x_{k+1} = x_k + \\alpha_k p_k$ 的迭代下降法，其中 $x_k \\in \\mathbb{R}^2$ 是当前对最小值的估计，$p_k$ 是搜索方向，$\\alpha_k > 0$ 是步长。\n\n**1. 目标函数及其性质**\n待最小化的函数是 $f:\\mathbb{R}^2 \\to \\mathbb{R}$，定义为：\n$$ f(x_1, x_2) = x_1^2 $$\n$f$ 的梯度是其偏导数向量：\n$$ \\nabla f(x_1, x_2) = \\begin{pmatrix} \\frac{\\partial f}{\\partial x_1} \\\\ \\frac{\\partial f}{\\partial x_2} \\end{pmatrix} = \\begin{pmatrix} 2x_1 \\\\ 0 \\end{pmatrix} $$\n$f$ 的海森矩阵包含其二阶偏导数：\n$$ \\nabla^2 f(x_1, x_2) = \\begin{pmatrix} \\frac{\\partial^2 f}{\\partial x_1^2} & \\frac{\\partial^2 f}{\\partial x_1 \\partial x_2} \\\\ \\frac{\\partial^2 f}{\\partial x_2 \\partial x_1} & \\frac{\\partial^2 f}{\\partial x_2^2} \\end{pmatrix} = \\begin{pmatrix} 2 & 0 \\\\ 0 & 0 \\end{pmatrix} $$\n一个二次可微函数是凸函数的充要条件是其海森矩阵在其定义域内的所有点上都是半正定的。$\\nabla^2 f$ 的特征值为 $\\lambda_1 = 2$ 和 $\\lambda_2 = 0$。由于所有特征值均非负，海森矩阵是半正定的，因此 $f$ 是凸函数。然而，一个函数要成为严格凸函数，其海森矩阵必须是正定的（所有特征值严格为正）。零特征值的存在意味着 $f$ 不是严格凸的。$f$ 的最小化子集是 $x_1=0$ 的整条直线，即 $\\{(0, c) \\mid c \\in \\mathbb{R}\\}$，因为 $f(0, c) = 0$ 是全局最小值。\n\n**2. BFGS 算法**\nBFGS 方法是一种拟牛顿法，它避免了海森矩阵及其逆矩阵的显式计算。取而代之的是，它维护一个逆海森矩阵的近似值，记为 $H_k$，并在每次迭代中对其进行更新。\n\n**a. 初始化**\n该过程从一个初始点 $x_0$ 开始。初始逆海森矩阵近似 $H_0$ 通常被设置为一个缩放的单位矩阵，$H_0 = \\gamma I$，其中 $\\gamma > 0$ 是一个缩放因子，$I$ 是单位矩阵。一个常见的选择是 $\\gamma=1$。\n\n**b. 迭代步骤**\n对于每次迭代 $k = 0, 1, 2, \\dots$：\n1.  **计算搜索方向**：搜索方向 $p_k$ 使用当前的逆海森矩阵近似和梯度计算得出：\n    $$ p_k = -H_k \\nabla f(x_k) $$\n    这个方向类似于牛顿方向 $p_k = -[\\nabla^2 f(x_k)]^{-1} \\nabla f(x_k)$。\n\n2.  **使用线搜索确定步长**：找到步长 $\\alpha_k > 0$ 以确保函数值有足够的下降。采用回溯线搜索。从一个初始猜测（例如 $\\alpha = 1$）开始，步长被连续乘以一个因子 $\\beta \\in (0,1)$ 进行缩减，直到满足 Armijo 条件：\n    $$ f(x_k + \\alpha p_k) \\le f(x_k) + c_1 \\alpha \\nabla f(x_k)^\\top p_k $$\n    这里，$c_1$ 是一个小常数，通常为 $c_1 = 10^{-4}$。我们将使用 $c_1 = 10^{-4}$ 和 $\\beta = 0.5$。一旦找到合适的 $\\alpha$，我们就设置 $\\alpha_k = \\alpha$。\n\n3.  **更新位置**：通过执行一步来找到下一个迭代点：\n    $$ x_{k+1} = x_k + \\alpha_k p_k $$\n\n4.  **更新逆海森矩阵**：将 $H_k$ 更新为 $H_{k+1}$，以融入从最近一步中收集到的关于函数曲率的信息。我们定义：\n    -   $s_k = x_{k+1} - x_k = \\alpha_k p_k$（位置步长）\n    -   $y_k = \\nabla f(x_{k+1}) - \\nabla f(x_k)$（梯度变化）\n    仅当曲率条件 $y_k^\\top s_k > 0$ 成立时才执行 BFGS 更新。此条件确保如果 $H_k$ 是正定的，那么更新后的矩阵 $H_{k+1}$ 也保持正定。逆海森矩阵的更新公式为：\n    $$ H_{k+1} = \\left(I - \\frac{s_k y_k^\\top}{y_k^\\top s_k}\\right) H_k \\left(I - \\frac{y_k s_k^\\top}{y_k^\\top s_k}\\right) + \\frac{s_k s_k^\\top}{y_k^\\top s_k} $$\n    如果不满足曲率条件，则跳过更新，并设置 $H_{k+1} = H_k$。\n\n**c. 终止条件**\n当梯度的范数低于指定的容差 $\\varepsilon$（即 $\\|\\nabla f(x_k)\\|_2 \\le \\varepsilon$）或达到最大迭代次数 $N_{\\max}$ 时，算法终止。\n\n**3. 在特定函数上的应用**\n对于这个问题，一个关键的观察是梯度 $\\nabla f(x) = (2x_1, 0)^\\top$ 的结构。其第二个分量始终为零。这对 BFGS 算法的行为有深远的影响。\n\n让我们假设初始逆海森矩阵是对角的，$H_0 = \\gamma I = \\begin{pmatrix} \\gamma & 0 \\\\ 0 & \\gamma \\end{pmatrix}$。\n初始搜索方向是 $p_0 = -H_0 \\nabla f(x_0) = -\\begin{pmatrix} \\gamma & 0 \\\\ 0 & \\gamma \\end{pmatrix} \\begin{pmatrix} 2x_{1,0} \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} -2\\gamma x_{1,0} \\\\ 0 \\end{pmatrix}$。\n搜索方向的第二个分量为零。因此，步进 $x_1 = x_0 + \\alpha_0 p_0$ 只会改变 $x_0$ 的第一个坐标。第二个坐标 $x_{2,1}$ 将等于 $x_{2,0}$。\n\n现在考虑用于 BFGS 更新的向量：\n$s_0 = x_1 - x_0 = \\alpha_0 p_0 = (-2\\alpha_0\\gamma x_{1,0}, 0)^\\top$。\n$y_0 = \\nabla f(x_1) - \\nabla f(x_0) = (2x_{1,1} - 2x_{1,0}, 0)^\\top$。\n$s_0$ 和 $y_0$ 的第二个分量都等于零。让我们将它们表示为 $s_0 = (s_{1,0}, 0)^\\top$ 和 $y_0 = (y_{1,0}, 0)^\\top$。\n\n当我们计算更新所需的矩阵时，我们发现它们的结构不会在新的海森矩阵近似中引入任何非对角元素：\n$s_0 y_0^\\top = \\begin{pmatrix} s_{1,0}y_{1,0} & 0 \\\\ 0 & 0 \\end{pmatrix}$\n$s_0 s_0^\\top = \\begin{pmatrix} s_{1,0}^2 & 0 \\\\ 0 & 0 \\end{pmatrix}$\n鉴于 $H_0$ 是对角的，并且更新公式中的所有矩阵要么是对角的，要么具有保持对角性的结构（$V H V^\\top$ 其中 $H$ 和 $V$ 是对角的），因此得到的矩阵 $H_1$ 也将是对角的。\n\n通过归纳法，如果 $H_k$ 是对角的，那么 $p_k = -H_k \\nabla f(x_k)$ 的第二个分量将为零。这意味着 $s_k$ 的第二个分量将为零。$y_k$ 的第二个分量也将为零。因此，$H_{k+1}$ 将保持对角。\n\n这证明了该问题的一个关键性质：**如果初始逆海森矩阵近似 $H_0$ 是对角的，那么所有后续的近似 $H_k$ 也将是对角的，并且算法永远不会改变迭代点的 $x_2$ 坐标。** 优化过程完全在由 $x_1$ 轴定义的子空间内进行，实际上将问题简化为在一维空间中搜索 $g(x_1) = x_1^2$ 的最小值。\n\n所提供的测试用例都使用 $H_0 = \\gamma I$，这是一个对角矩阵。因此，我们预期在所有测试中 $x_2$ 坐标都将保持不变。算法将收敛到点 $(0, x_{2,0})$，这是全局最小化子集中的一个点。对于二次函数，已知带有精确线搜索的 BFGS 算法最多在 $n$ 次迭代内收敛，其中 $n$ 是空间的维度。在这里，由于问题的特定结构，我们观察到更快的收敛速度（1 或 2 步）。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef bfgs_minimize(f, grad_f, x0, epsilon, n_max, gamma):\n    \"\"\"\n    Minimizes a function using the BFGS quasi-Newton method.\n\n    Args:\n        f: The objective function.\n        grad_f: The gradient of the objective function.\n        x0: The initial point.\n        epsilon: The tolerance for the gradient norm stopping criterion.\n        n_max: The maximum number of iterations.\n        gamma: The scaling factor for the initial inverse Hessian approximation.\n    \n    Returns:\n        A tuple (x_final, f_final, k) containing the final point,\n        final function value, and number of iterations.\n    \"\"\"\n    # Line search parameters\n    c1 = 1e-4\n    beta = 0.5\n    \n    x = np.array(x0, dtype=float)\n    n = len(x)\n    H = gamma * np.identity(n)\n    \n    for k in range(n_max):\n        g = grad_f(x)\n        \n        # Check stopping criterion\n        if np.linalg.norm(g) = epsilon:\n            return x, f(x), k\n            \n        # Compute search direction\n        p = -H @ g\n        \n        # Backtracking line search (Armijo condition)\n        alpha = 1.0\n        f_x = f(x)\n        g_dot_p = g.T @ p\n        \n        while True:\n            x_new = x + alpha * p\n            if f(x_new) = f_x + c1 * alpha * g_dot_p:\n                break\n            alpha *= beta\n        \n        s = x_new - x\n        x = x_new\n        \n        g_new = grad_f(x)\n        y = g_new - g\n        \n        # BFGS inverse Hessian update\n        # Check curvature condition\n        y_dot_s = y.T @ s\n        if y_dot_s > 1e-12: # Use a small threshold for numerical stability\n            rho = 1.0 / y_dot_s\n            I = np.identity(n)\n            \n            term1 = I - rho * np.outer(s, y)\n            term2 = I - rho * np.outer(y, s)\n            \n            H = term1 @ H @ term2 + rho * np.outer(s, s)\n            \n    return x, f(x), n_max\n\ndef solve():\n    \"\"\"\n    Main function to run the BFGS algorithm on the specified test cases.\n    \"\"\"\n    # The objective function f(x1, x2) = x1^2\n    def f(x):\n        return x[0]**2\n\n    # The gradient of the objective function\n    def grad_f(x):\n        return np.array([2 * x[0], 0.0])\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {'x0': (5, 7), 'eps': 1e-9, 'n_max': 50, 'gamma': 1},\n        {'x0': (0, 3), 'eps': 1e-9, 'n_max': 50, 'gamma': 1},\n        {'x0': (-10, -2), 'eps': 1e-9, 'n_max': 50, 'gamma': 1},\n        {'x0': (10**6, 10**6), 'eps': 1e-12, 'n_max': 100, 'gamma': 1},\n        {'x0': (5, 7), 'eps': 1e-9, 'n_max': 50, 'gamma': 10},\n    ]\n\n    results = []\n    for case in test_cases:\n        x0 = case['x0']\n        eps = case['eps']\n        \n        x_final, f_final, iterations = bfgs_minimize(\n            f, grad_f, x0, eps, case['n_max'], case['gamma']\n        )\n        \n        x1_final, x2_final = x_final[0], x_final[1]\n        \n        # Check if the second coordinate remained unchanged within tolerance\n        tau = eps\n        x2_unchanged = abs(x2_final - x0[1]) = tau\n        \n        results.append([x1_final, x2_final, f_final, iterations, x2_unchanged])\n\n    # Format the final output string as specified a list of lists.\n    sub_results_str = []\n    for r in results:\n        # Manually format each sub-list to control spacing and boolean representation\n        sub_results_str.append(f\"[{r[0]},{r[1]},{r[2]},{r[3]},{str(r[4])}]\")\n    \n    final_output_str = f\"[{','.join(sub_results_str)}]\"\n    \n    # Final print statement in the exact required format.\n    print(final_output_str)\n\nsolve()\n```", "id": "3264863"}]}