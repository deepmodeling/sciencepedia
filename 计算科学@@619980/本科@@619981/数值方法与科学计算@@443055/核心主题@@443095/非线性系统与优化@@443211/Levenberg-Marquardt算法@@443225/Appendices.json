{"hands_on_practices": [{"introduction": "任何非线性最小二乘问题的求解都始于对问题进行局部线性化，而雅可比矩阵（Jacobian matrix）正是实现这一目标的关键。它描述了模型输出对每个参数微小变化的敏感度，为构建算法的每一步指明了方向。这个练习将让你专注于计算雅可比矩阵这一基本功，为一个具体的非线性模型和参数点计算出其数值。[@problem_id:2217052]", "problem": "在非线性优化领域，像Levenberg-Marquardt算法这类算法通过最小化残差平方和，将模型函数拟合到一组数据点。此过程中的一个关键组成部分是模型函数的雅可比矩阵。\n\n考虑一个用于描述物质浓度随时间变化的模型，该模型由以下双参数有理函数给出：\n$$f(t; a, b) = \\frac{a}{1 + bt}$$\n其中，$t$ 是自变量（例如时间），而 $\\mathbf{p} = [a, b]^T$ 是待确定的参数向量。\n\n假设我们收集了以下三个数据点 $(t_i, y_i)$：\n- $(t_1, y_1) = (1.0, 2.5)$\n- $(t_2, y_2) = (2.0, 1.8)$\n- $(t_3, y_3) = (4.0, 1.1)$\n\n针对此拟合问题的雅可比矩阵 $\\mathbf{J}$ 是一个 $m \\times n$ 矩阵，其中 $m$ 是数据点的数量，$n$ 是参数的数量。其元素定义为 $J_{ij} = \\frac{\\partial f(t_i; \\mathbf{p})}{\\partial p_j}$。\n\n计算在初始参数估计 $\\mathbf{p}_0 = [a_0, b_0]^T = [3.0, 0.5]^T$ 处，雅可比矩阵 $\\mathbf{J}$ 的数值。所有给定的数值都是无量纲的。\n\n将最终答案表示为一个 $3 \\times 2$ 矩阵，每个元素四舍五入保留三位有效数字。", "solution": "给定模型函数 $f(t; a, b) = \\dfrac{a}{1 + bt}$ 以及参数向量 $\\mathbf{p} = [a, b]^{T}$ 对应的雅可比矩阵，其定义为 $J_{ij} = \\dfrac{\\partial f(t_{i}; \\mathbf{p})}{\\partial p_{j}}$。因此，$\\mathbf{J}$ 的每一行对应一个数据点 $t_{i}$，两列分别对应关于 $a$ 和 $b$ 的偏导数。\n\n首先，以符号形式计算偏导数。将 $f(t; a, b)$ 写为 $a(1 + bt)^{-1}$。然后，使用幂法则和链式法则：\n$$\n\\frac{\\partial f}{\\partial a} = (1 + bt)^{-1} = \\frac{1}{1 + bt},\n$$\n$$\n\\frac{\\partial f}{\\partial b} = a \\cdot \\frac{\\partial}{\\partial b}(1 + bt)^{-1} = a \\cdot \\left[-(1 + bt)^{-2} \\cdot t\\right] = -\\frac{a t}{(1 + bt)^{2}}.\n$$\n\n因此，对于每个数据点 $t_{i}$，雅可比矩阵的对应行是：\n$$\n\\left[\\frac{1}{1 + b t_{i}},\\ -\\frac{a t_{i}}{(1 + b t_{i})^{2}}\\right].\n$$\n\n在初始估计值 $\\mathbf{p}_{0} = [a_{0}, b_{0}]^{T} = [3.0, 0.5]^{T}$ 和给定的 $t$ 值处进行求值。\n\n对于 $t_{1} = 1$：\n$$\n1 + b_{0} t_{1} = 1 + 0.5 \\cdot 1 = \\frac{3}{2},\\quad \\frac{\\partial f}{\\partial a} = \\frac{2}{3},\\quad \\frac{\\partial f}{\\partial b} = -\\frac{3 \\cdot 1}{(3/2)^{2}} = -\\frac{12}{9} = -\\frac{4}{3}.\n$$\n\n对于 $t_{2} = 2$：\n$$\n1 + b_{0} t_{2} = 1 + 0.5 \\cdot 2 = 2,\\quad \\frac{\\partial f}{\\partial a} = \\frac{1}{2},\\quad \\frac{\\partial f}{\\partial b} = -\\frac{3 \\cdot 2}{2^{2}} = -\\frac{6}{4} = -\\frac{3}{2}.\n$$\n\n对于 $t_{3} = 4$：\n$$\n1 + b_{0} t_{3} = 1 + 0.5 \\cdot 4 = 3,\\quad \\frac{\\partial f}{\\partial a} = \\frac{1}{3},\\quad \\frac{\\partial f}{\\partial b} = -\\frac{3 \\cdot 4}{3^{2}} = -\\frac{12}{9} = -\\frac{4}{3}.\n$$\n\n组装雅可比矩阵，并将每个元素四舍五入保留三位有效数字：\n$$\n\\mathbf{J}(\\mathbf{p}_{0}) \\approx\n\\begin{pmatrix}\n0.667  -1.33 \\\\\n0.500  -1.50 \\\\\n0.333  -1.33\n\\end{pmatrix}.\n$$", "answer": "$$\\boxed{\\begin{pmatrix}0.667  -1.33 \\\\ 0.500  -1.50 \\\\ 0.333  -1.33\\end{pmatrix}}$$", "id": "2217052"}, {"introduction": "列文伯格-马夸尔特算法的精髓在于它巧妙地融合了高斯-牛顿法（Gauss-Newton method）的快速收敛性和梯度下降法（Gradient Descent）的稳定性。这个练习通过一个精心设计的简化问题，揭示了纯粹的高斯-牛顿法在雅可比矩阵接近奇异时可能出现的振荡或发散问题。你将亲眼见证并理解LM算法中的阻尼项 $\\lambda$ 是如何像一个“安全阀”一样，有效抑制这种不稳定性，确保迭代过程的稳健性。[@problem_id:3232802]", "problem": "您的任务是编写一个完整的、可运行的程序，用于分析高斯-牛顿（Gauss-Newton）方法与作为其稳定化形式的列文伯格-马夸尔特（Levenberg-Marquardt, LM）方法，并将其应用于一个单参数非线性最小二乘问题。目标是通过原理性推导和具体实现来证明，当雅可比矩阵（Jacobian）接近奇异时，无阻尼的高斯-牛顿迭代步长可能会振荡或发散，而添加列文伯格-马夸尔特阻尼项可以稳定迭代过程。\n\n从以下基本概念开始：\n- 非线性最小二乘的目标函数为 $F(x) = \\tfrac{1}{2}\\lVert r(x)\\rVert_2^2$，其中 $r(x)$ 是残差函数向量。\n- 在 $x$ 处的一阶泰勒线性化为 $r(x + \\Delta) \\approx r(x) + J(x)\\Delta$，其中 $J(x)$ 是 $r(x)$ 的雅可比矩阵。\n- 最小化该线性化模型可得到正规方程组（normal equations）$J(x)^\\top J(x)\\Delta = -J(x)^\\top r(x)$。\n- 列文伯格-马夸特（LM）方法通过一个阻尼项 $\\lambda I$（其中 $I$ 是单位矩阵）来增广正规矩阵，求解 $\\big(J(x)^\\top J(x) + \\lambda I\\big)\\Delta = -J(x)^\\top r(x)$，其中 $\\lambda > 0$。\n\n您的程序必须实例化一个一维残差模型 $r(x) = \\sin(x)$（角度以弧度为单位），并执行高斯-牛顿（无阻尼，对应于 $\\lambda = 0$）和列文伯格-马夸特（有阻尼，使用指定的 $\\lambda > 0$）两种迭代。该残差模型意味着最小二乘目标函数为 $F(x) = \\tfrac{1}{2}\\sin^2(x)$。在这种情况下，雅可比矩阵 $J(x)$ 是一个标量。您必须：\n- 从基本定义出发，推导出针对一维残差 $r(x) = \\sin(x)$ 的高斯-牛顿和LM更新规则。\n- 使用推导出的更新规则，为两种方法实现固定迭代次数的求解器。\n- 证明在 $J(x)$ 接近奇异的点附近（特别是在 $x \\approx \\tfrac{\\pi}{2} + k\\pi$ 附近，其中 $k$ 为整数），无阻尼的高斯-牛顿迭代会表现出振荡或发散（出现大的交替步长或步长超出有界域），而带有正阻尼参数的LM方法通过限制步长大小来稳定迭代过程。\n\n角度必须以弧度为单位。不涉及任何物理单位。您的算法除了恒定的LM阻尼外，不得使用任何线搜索或信赖域自适应方法。\n\n测试套件与输出规范：\n- 使用以下测试用例，每个用例指定为一个元组 $(x_0, \\lambda, N, L)$：\n  1. 一个接近奇异的雅可比矩阵用例，以引发不稳定性：$x_0 = \\tfrac{\\pi}{2} - 10^{-6}$，$\\lambda = 1.0$，$N = 10$，$L = 50$。\n  2. 一个振荡但可恢复的用例：$x_0 = 1.4$，$\\lambda = 0.5$，$N = 10$，$L = 50$。\n  3. 一个远离奇异点的理想路径用例：$x_0 = 2.0$，$\\lambda = 0.1$，$N = 10$，$L = 50$。\n- 对每个用例，运行 $N$ 次无阻尼高斯-牛顿迭代（即，对于高斯-牛顿法设置 $\\lambda = 0$）和 $N$ 次使用给定 $\\lambda$ 的LM迭代。对于每个用例中的每种方法，计算：\n  - 最终残差范数 $|r(x_N)|$，作为浮点数。\n  - 一个稳定性标志，作为布尔值，如果所有迭代点 $x_k$ 都满足 $|x_k| \\le L$，则为真，否则为假。\n  - 最大步长幅度 $\\max_k |\\Delta_k|$，作为浮点数，其中 $\\Delta_k$ 是第 $k$ 次迭代的更新量。\n- 您的程序应生成单行输出，其格式为一个 Python 列表的字符串表示。该列表包含多个子列表，每个子列表对应一个测试用例，并按以下顺序包含六个值：$[\\text{gn\\_final\\_residual}, \\text{lm\\_final\\_residual}, \\text{gn\\_stable}, \\text{lm\\_stable}, \\text{gn\\_max\\_step}, \\text{lm\\_max\\_step}]$。例如，输出格式必须类似于 `[[r_1_gn, r_1_lm, b_1_gn, b_1_lm, s_1_gn, s_1_lm], [r_2_gn, ...]]`，其中 `r` 是浮点数，`b` 是布尔值。\n\n科学真实性与覆盖范围：\n- 接近奇异的用例强制了一个边界条件，在该条件下，无阻尼方法会遇到接近于零的雅可比矩阵，这可能导致巨大的步长。LM阻尼项 $\\lambda I$ 必须减轻这种不稳定性。\n- 振荡用例展示了无阻尼高斯-牛顿法的大幅交替步长，这些步长在LM阻尼下会减小。\n- 理想路径用例展示了两种方法的收敛性。\n- 角度以弧度为单位。\n- 所有输出必须是指定的基本类型（布尔值和浮点数）。", "solution": "该问题要求对一个特定的一维非线性最小二乘问题，验证并实现高斯-牛顿（Gauss-Newton, GN）方法及其莱文伯格-马夸特（Levenberg-Marquardt, LM）稳定化。其目标是展示当雅可比矩阵接近奇异时，LM稳定化如何修正无阻尼高斯-牛顿方法固有的不稳定性。\n\n首先，我们建立理论基础。一般的非线性最小二乘问题旨在最小化一个目标函数 $F(x)$，该函数定义为残差向量 $r(x)$ 的欧几里得范数平方的一半：\n$$\nF(x) = \\frac{1}{2}\\lVert r(x)\\rVert_2^2\n$$\n高斯-牛顿方法在每次迭代中通过围绕当前迭代点 $x_k$ 线性化残差函数 $r(x)$ 来近似目标函数。更新步长 $\\Delta$ 通过求解以下线性化最小二乘问题得到：\n$$\n\\min_{\\Delta} \\frac{1}{2}\\lVert r(x_k) + J(x_k)\\Delta\\rVert_2^2\n$$\n其中 $J(x_k)$ 是 $r(x)$ 在 $x_k$ 处的雅可比矩阵。这个线性问题的解由正规方程组给出：\n$$\nJ(x_k)^\\top J(x_k)\\Delta_{GN} = -J(x_k)^\\top r(x_k)\n$$\n莱文伯格-马夸特方法引入一个阻尼参数 $\\lambda > 0$ 来正则化该问题，这在矩阵 $J(x_k)^\\top J(x_k)$ 是奇异或病态（ill-conditioned）时尤其有用。LM更新步长 $\\Delta_{LM}$ 通过求解修正后的正规方程组得到：\n$$\n\\left(J(x_k)^\\top J(x_k) + \\lambda I\\right)\\Delta_{LM} = -J(x_k)^\\top r(x_k)\n$$\n其中 $I$ 是单位矩阵。\n\n现在，我们将这些通用公式特化到给定的一维问题，其中残差是一个标量函数 $r(x) = \\sin(x)$。\n目标函数变为：\n$$\nF(x) = \\frac{1}{2}(r(x))^2 = \\frac{1}{2}\\sin^2(x)\n$$\n$F(x)$ 的最小值出现在 $\\sin(x) = 0$ 的地方，即对于任意整数 $n$，在 $x = n\\pi$ 处。\n在这个一维案例中，雅可比矩阵 $J(x)$ 是一个 $1 \\times 1$ 的矩阵（一个标量），对应于 $r(x)$ 的一阶导数：\n$$\nJ(x) = \\frac{dr}{dx} = \\frac{d}{dx}(\\sin(x)) = \\cos(x)\n$$\n当 $J(x) = \\cos(x) \\approx 0$ 时，雅可比矩阵是（接近）奇异的，这发生在 $x \\approx \\frac{\\pi}{2} + k\\pi$（对于任意整数 $k$）处。\n\n我们为两种方法推导具体的更新规则。\n对于高斯-牛顿法（无阻尼，等效于 $\\lambda=0$），正规方程为：\n$$\n(\\cos(x_k))^\\top (\\cos(x_k))\\Delta_{GN} = -(\\cos(x_k))^\\top (\\sin(x_k))\n$$\n$$\n\\cos^2(x_k) \\Delta_{GN} = -\\sin(x_k)\\cos(x_k)\n$$\n假设 $\\cos(x_k) \\neq 0$，我们可以解出步长 $\\Delta_{GN}$：\n$$\n\\Delta_{GN} = -\\frac{\\sin(x_k)\\cos(x_k)}{\\cos^2(x_k)} = -\\frac{\\sin(x_k)}{\\cos(x_k)} = -\\tan(x_k)\n$$\n因此，高斯-牛顿更新规则是：\n$$\nx_{k+1} = x_k + \\Delta_{GN} = x_k - \\tan(x_k)\n$$\n当 $x_k$ 接近雅可比矩阵奇异的点时，即 $x_k \\to \\frac{\\pi}{2} + k\\pi$，该方法的不稳定性变得明显。在这些点，$\\cos(x_k) \\to 0$，导致 $|\\tan(x_k)| \\to \\infty$。更新步长 $\\Delta_{GN}$ 会变得任意大，从而导致发散或剧烈振荡。\n\n对于莱文伯格-马夸特法，修正后的正规方程包含阻尼项 $\\lambda > 0$。在这个一维情况下，单位矩阵 $I$ 是标量 $1$：\n$$\n(\\cos^2(x_k) + \\lambda)\\Delta_{LM} = -\\sin(x_k)\\cos(x_k)\n$$\n由于 $\\cos^2(x_k) \\ge 0$ 且 $\\lambda > 0$，项 $(\\cos^2(x_k) + \\lambda)$ 始终为正，且以 $\\lambda$ 为下界远离零。因此，该系统总是良态的（well-conditioned）。LM更新步长为：\n$$\n\\Delta_{LM} = -\\frac{\\sin(x_k)\\cos(x_k)}{\\cos^2(x_k) + \\lambda}\n$$\n莱文伯格-马夸特更新规则是：\n$$\nx_{k+1} = x_k + \\Delta_{LM} = x_k - \\frac{\\sin(x_k)\\cos(x_k)}{\\cos^2(x_k) + \\lambda}\n$$\n现在考虑在奇异点附近的行为，此处 $\\cos(x_k) \\to 0$。在此极限下，分子 $\\sin(x_k)\\cos(x_k)$ 也趋近于 $0$。分母趋近于 $\\lambda$。因此，步长 $\\Delta_{LM}$ 趋近于 $0$。阻尼项有效地限制了步长大小，防止了在无阻尼高斯-牛顿方法中观察到的灾难性行为。这确保了稳定性并促进了收敛，即使从雅可比奇异区域附近开始也是如此。\n\n接下来的实现将通过对给定的测试用例迭代这两个更新规则，以数值方式展示这一推导出的行为。结果将量化最终残差、在给定界限内的迭代序列稳定性以及所采取步长的大小。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Analyzes Gauss-Newton and Levenberg-Marquardt methods for a 1D\n    nonlinear least-squares problem, demonstrating LM stabilization.\n    \"\"\"\n\n    # Test cases as tuples of (x0, lambda, N, L)\n    test_cases = [\n        (np.pi / 2.0 - 1e-6, 1.0, 10, 50.0), # Near-singular Jacobian case\n        (1.4, 0.5, 10, 50.0),               # Oscillatory-but-recoverable case\n        (2.0, 0.1, 10, 50.0)                # Happy-path case\n    ]\n\n    results = []\n\n    def run_iterations(x0, N, L, damp_lambda):\n        \"\"\"\n        Performs N iterations of a solver for the objective F(x) = 0.5*sin^2(x).\n\n        Args:\n            x0 (float): Initial guess.\n            N (int): Number of iterations.\n            L (float): Stability bound for |x_k|.\n            damp_lambda (float): Damping parameter. If 0, use Gauss-Newton.\n                                 If > 0, use Levenberg-Marquardt.\n\n        Returns:\n            A tuple containing:\n            - final_residual (float): |r(x_N)|.\n            - is_stable (bool): True if all |x_k| = L.\n            - max_step (float): Maximum magnitude of any step delta_k.\n        \"\"\"\n        x = x0\n        stable = True\n        max_step_mag = 0.0\n\n        for _ in range(N):\n            # The Jacobian is singular when cos(x) is zero.\n            # Avoid division by zero for the pure Gauss-Newton case if x is exactly pi/2 + k*pi.\n            # np.tan will handle large values gracefully, returning inf.\n            cos_x = np.cos(x)\n            sin_x = np.sin(x)\n\n            if damp_lambda == 0:  # Gauss-Newton\n                # Delta = -tan(x)\n                # Avoid explicit division by zero if cos_x is extremely small.\n                # np.tan handles this by returning large numbers or inf.\n                delta = -np.tan(x)\n\n            else:  # Levenberg-Marquardt\n                # Delta = -sin(x)cos(x) / (cos^2(x) + lambda)\n                numerator = -sin_x * cos_x\n                denominator = cos_x**2 + damp_lambda\n                delta = numerator / denominator\n\n            if np.isinf(delta) or np.isnan(delta):\n                # If step is infinite/NaN, it's definitively unstable and huge.\n                # To assign a finite but large value for max_step.\n                # Using 2*L is arbitrary but indicates a large step.\n                current_step_mag = 2 * L \n                x = x + (2 * L * np.sign(-delta) if not np.isnan(delta) else 0)\n            else:\n                current_step_mag = abs(delta)\n\n            if current_step_mag > max_step_mag:\n                max_step_mag = current_step_mag\n\n            x = x + delta\n            \n            if abs(x) > L:\n                stable = False\n        \n        final_residual = abs(np.sin(x))\n        \n        return final_residual, stable, max_step_mag\n\n    for x0, lm_lambda, N, L in test_cases:\n        # Run Gauss-Newton (undamped, lambda = 0)\n        gn_final_residual, gn_stable, gn_max_step = run_iterations(x0, N, L, 0)\n\n        # Run Levenberg-Marquardt (damped)\n        lm_final_residual, lm_stable, lm_max_step = run_iterations(x0, N, L, lm_lambda)\n\n        case_results = [\n            gn_final_residual,\n            lm_final_residual,\n            gn_stable,\n            lm_stable,\n            gn_max_step,\n            lm_max_step,\n        ]\n        results.append(case_results)\n\n    # Format the final output string exactly as specified.\n    # The default str() representation for lists includes spaces, which is acceptable.\n    # The boolean values will be represented as 'True' and 'False'.\n    output_str = \"[\" + \",\".join(map(str, results)) + \"]\"\n    \n    print(output_str)\n\nsolve()\n```", "id": "3232802"}, {"introduction": "一个真正强大的LM求解器，其核心在于能够根据每一步迭代的“成效”自动调整阻尼策略。这个综合性练习将引导你从零开始，构建一个完整的、带有自适应阻尼控制的LM算法。你将实现基于增益比（gain ratio）的判断机制，让算法能够智能地在“谨慎”的梯度下降行为和“激进”的高斯-牛顿行为之间切换，从而高效、稳健地解决复杂的非线性拟合问题。[@problem_id:3256843]", "problem": "您的任务是实现一个完整的、自包含的程序，该程序使用 Levenberg–Marquardt 算法解决一个非线性最小二乘拟合问题，其中阻尼参数根据平方和的实际减少量与预测减少量之比自动调整。推导和实现必须从非线性最小二乘目标的基本定义开始，并最终形成一个在科学上和数值上都健全的算法。\n\n从非线性最小二乘目标的核心定义开始。设参数向量为 $p \\in \\mathbb{R}^m$，自变量向量为 $x \\in \\mathbb{R}^n$，观测数据（因变量）为 $y \\in \\mathbb{R}^n$，模型是一个函数 $f:\\mathbb{R}^n \\times \\mathbb{R}^m \\to \\mathbb{R}^n$，它根据参数预测数据。通过 $r(p) = y - f(x,p)$ 定义残差向量 $r(p) \\in \\mathbb{R}^n$，并通过 $F(p) = \\frac{1}{2}\\lVert r(p)\\rVert_2^2$ 定义目标函数。定义雅可比矩阵 $J(p) \\in \\mathbb{R}^{n \\times m}$，其元素为 $J_{ij}(p) = \\frac{\\partial r_i(p)}{\\partial p_j}$。您的程序必须在每次迭代中使用中心有限差分来数值近似雅可比矩阵。\n\n在每次迭代中，使用 $r(p)$ 和 $J(p)$ 围绕当前参数 $p$ 进行二阶近似，构建 $F(p)$ 的局部二次模型，并通过求解一个阻尼正规方程来计算一个在 Gauss–Newton 方向和梯度下降方向之间取得平衡的参数增量。该算法必须使用一个增益比来决定是接受还是拒绝提议的步长，该增益比用于比较 $F(p)$ 的实际减少量与用于计算步长的局部模型所预测的减少量。阻尼参数必须基于此增益比自动调整：当步长有效时减小阻尼，当步长无效时增加阻尼。使用一个终止准则，当梯度的无穷范数很小、参数更新量很小或目标函数的变化可忽略不计时，停止迭代。\n\n在正弦模型中使用的角度量 $\\phi$ 必须以弧度为单位进行处理和计算。\n\n您的程序必须为以下三个测试用例实现上述算法，每个用例都有特定的模型、数据集和初始参数猜测。对于每个测试用例，返回拟合后的参数向量，形式为浮点数列表，四舍五入到六位小数。\n\n测试用例 1（一般收敛性）：\n- 模型：$f(x,p) = p_1 \\exp(p_2 x) + p_3$，参数为 $p = [p_1,p_2,p_3]$。\n- 数据：$x$ 由 $x_i = 0.0 + i\\Delta$ 定义，其中 $i=0,1,\\dots,30$ 且 $\\Delta = 0.1$，因此 $x \\in [0,3.0]$，有 $31$ 个点。观测值由 $y = 2.0\\exp(-0.7 x) + 0.5 + 0.01\\sin(5x)$ 定义。\n- 初始参数：$p^{(0)} = [1.0,-0.2,0.0]$。\n\n测试用例 2（陡峭非线性）：\n- 模型：逻辑斯谛曲线 $f(x,p) = \\frac{p_1}{1 + \\exp(-p_2(x - p_3))}$，参数为 $p = [p_1,p_2,p_3]$。\n- 数据：$x$ 由 $x_i = -1.5 + i\\Delta$ 定义，其中 $i=0,1,\\dots,14$ 且 $\\Delta = \\frac{2.0 - (-1.5)}{14}$，因此 $x \\in [-1.5,2.0]$，有 $15$ 个点。观测值由 $y = \\frac{1.5}{1 + \\exp(-3.0(x - 0.5))} + 0.02\\cos(3x)$ 定义。\n- 初始参数：$p^{(0)} = [1.0,1.0,0.0]$。\n\n测试用例 3（灵敏度的近简并性）：\n- 模型：角频率固定为 $\\omega = 2.5$ 弧度/单位$x$ 的正弦曲线，$f(x,p) = p_1 \\cos(\\omega x + p_2) + p_3$，参数为 $p = [p_1,p_2,p_3]$ 且 $\\phi = p_2$ 以弧度为单位。\n- 数据：$x$ 由 $x_i = 0.0 + i\\Delta$ 定义，其中 $i=0,1,\\dots,20$ 且 $\\Delta = 0.1$，因此 $x \\in [0,2.0]$，有 $21$ 个点。观测值由 $y = 0.8\\cos(2.5 x + 0.4) - 0.05 + 0.01\\sin(7x)$ 定义。\n- 初始参数：$p^{(0)} = [0.5,0.0,0.0]$。\n\n算法要求：\n- 使用中心有限差分计算雅可比矩阵，扰动步长与 $\\sqrt{\\varepsilon}(1 + |p_j|)$ 成正比，其中 $\\varepsilon$ 是双精度浮点运算的机器精度。\n- 在每次迭代中构建并求解一个阻尼正规方程以获得参数增量。\n- 计算目标函数的实际减少量和用于步长计算的局部模型所预测的减少量。使用它们的比率来决定是否接受步长并调整阻尼参数。\n- 当任何标准的小量准则被满足时终止：梯度的无穷范数很小、参数变化很小或目标函数的变化很小。同时，包含一个最大迭代次数上限以确保算法停止。\n- 在整个计算过程中，正弦模型中的角度 $\\phi$ 都必须以弧度计算。\n\n您的程序应生成单行输出，其格式为一个 Python 列表的字符串表示。该列表包含三个子列表，每个子列表为对应测试用例的拟合参数值。每个参数值都应四舍五入到六位小数。最终输出格式必须严格符合以下示例格式：\n'[[p11,p12,p13],[p21,p22,p23],[p31,p32,p33]]'", "solution": "经评估，用户提供的问题是**有效的**。这是一个来自数值方法领域的适定、有科学依据且计算上可行的任务。它要求实现用于非线性最小二乘的 Levenberg-Marquardt 算法，这是科学计算中一个标准且重要的技术。所有必要的组成部分，包括目标函数、算法流程、测试模型、数据集和初始条件，都已提供且内部一致。\n\n在此，从基本原理出发推导出一个完整的解决方案。\n\n### 1. 非线性最小二乘问题\n\n问题的核心是找到一组能够最好地将模型拟合到一组观测数据的参数。设参数向量为 $p \\in \\mathbb{R}^m$，自变量向量为 $x \\in \\mathbb{R}^n$，相应的观测数据向量为 $y \\in \\mathbb{R}^n$。一个模型函数 $f(x, p)$ 将参数 $p$ 和自变量 $x$ 映射到一组预测数据点。目标是找到参数向量 $p$，以最小化观测数据 $y$ 与预测数据 $f(x,p)$ 之间差值的平方和。\n\n这可以形式化地通过定义一个残差向量 $r(p) \\in \\mathbb{R}^n$ 来表达：\n$$\nr(p) = y - f(x, p)\n$$\n目标是最小化标量目标函数 $F(p)$，其定义为残差向量的欧几里得范数平方的一半：\n$$\nF(p) = \\frac{1}{2}\\lVert r(p)\\rVert_2^2 = \\frac{1}{2} \\sum_{i=1}^{n} r_i(p)^2\n$$\n由于模型 $f(x,p)$ 通常是参数 $p$ 的非线性函数，这是一个非线性最小二乘问题，必须使用迭代方法求解。\n\n### 2. 通过局部近似的迭代解法\n\nLevenberg-Marquardt 算法是一个寻找 $F(p)$ 局部最小值的迭代过程。从一个初始猜测 $p^{(0)}$ 开始，该算法生成一系列参数向量 $p^{(1)}, p^{(2)}, \\dots$，这些向量逐步减小 $F(p)$ 的值。每次迭代计算一个步长向量 $h$ 来更新当前的参数向量：$p_{k+1} = p_k + h$。\n\n步长 $h$ 是通过在当前点 $p_k$ 周围构建目标函数 $F(p)$ 的局部二次模型来找到的。该模型是通过使用一阶泰勒级数展开来线性化残差向量 $r(p_k+h)$ 构建的：\n$$\nr(p_k + h) \\approx r(p_k) + J(p_k)h\n$$\n其中 $J(p_k)$ 是残差向量 $r$ 在 $p_k$ 处求值的雅可比矩阵。雅可比矩阵的元素由 $J_{ij}(p) = \\frac{\\partial r_i(p)}{\\partial p_j}$ 给出。由于 $r(p) = y - f(x,p)$，这等价于 $J_{ij}(p) = -\\frac{\\partial f_i(x,p)}{\\partial p_j}$。\n\n将残差的这个线性近似代入目标函数 $F(p_k+h)$，得到一个局部二次模型 $L(h)$：\n$$\nF(p_k + h) = \\frac{1}{2}\\lVert r(p_k+h) \\rVert_2^2 \\approx \\frac{1}{2}\\lVert r(p_k) + J(p_k)h \\rVert_2^2 = L(h)\n$$\n展开平方范数得到：\n$$\nL(h) = \\frac{1}{2} (r_k + J_k h)^T (r_k + J_k h) = \\frac{1}{2} (r_k^T r_k + 2h^T J_k^T r_k + h^T J_k^T J_k h)\n$$\n$$\nL(h) = F(p_k) + h^T (J_k^T r_k) + \\frac{1}{2} h^T (J_k^T J_k) h\n$$\n其中 $r_k = r(p_k)$ 且 $J_k = J(p_k)$。\n\n### 3. Gauss-Newton 步和 Levenberg-Marquardt 步\n\n选择步长 $h$ 以最小化这个二次模型 $L(h)$。将 $L(h)$ 对 $h$ 的梯度设为零，得到：\n$$\n\\nabla_h L(h) = J_k^T r_k + (J_k^T J_k) h = 0\n$$\n这导出了 **Gauss-Newton 正规方程**：\n$$\n(J_k^T J_k) h_{gn} = -J_k^T r_k\n$$\n步长 $h_{gn}$ 是 Gauss-Newton 步。当矩阵 $J_k^T J_k$ 是良态时，此方法效果很好。然而，如果 $J_k^T J_k$ 是奇异或病态的，步长可能会过大且无效。\n\n**Levenberg-Marquardt 算法** 通过引入一个阻尼参数 $\\lambda \\ge 0$ 来解决这个问题。步长 $h_{lm}$ 是通过求解一个修正的（或阻尼的）正规方程来找到的：\n$$\n(J_k^T J_k + \\lambda I) h_{lm} = -J_k^T r_k\n$$\n其中 $I$ 是单位矩阵。$\\lambda I$ 项是一个正则化项。\n- 如果 $\\lambda$ 很小，步长 $h_{lm}$ 近似于 Gauss-Newton 步 $h_{gn}$。\n- 如果 $\\lambda$ 很大，$\\lambda I$ 项在 $J_k^T J_k$ 中占主导地位，方程近似为 $\\lambda I h_{lm} \\approx -J_k^T r_k$，这意味着 $h_{lm} \\approx -\\frac{1}{\\lambda} (J_k^T r_k)$。由于目标函数的梯度是 $\\nabla F(p_k) = J_k^T r_k$，步长变成一个沿最速下降方向的小步。\n\n因此，参数 $\\lambda$ 自适应地将快速收敛的 Gauss-Newton 方法与稳健但较慢的梯度下降方法融合在一起。\n\n### 4. 算法实现细节\n\n#### 数值雅可比矩阵近似\n雅可比矩阵 $J_k$ 在每次迭代中使用**中心有限差分**进行计算。对于每个参数 $p_j$（向量 $p$ 的第 $j$ 个分量），计算一个小的扰动 $\\delta_j$：\n$$\n\\delta_j = \\sqrt{\\varepsilon} (1 + |p_j|)\n$$\n其中 $\\varepsilon$ 是双精度浮点数的机器精度。雅可比矩阵的第 $j$ 列，表示残差对参数 $p_j$ 的灵敏度，然后近似为：\n$$\nJ_{\\cdot, j}(p_k) = \\frac{\\partial r(p_k)}{\\partial p_j} \\approx \\frac{r(p_k + \\delta_j e_j) - r(p_k - \\delta_j e_j)}{2\\delta_j} = -\\frac{f(x, p_k + \\delta_j e_j) - f(x, p_k - \\delta_j e_j)}{2\\delta_j}\n$$\n其中 $e_j$ 是第 $j$ 个标准基向量。\n\n#### 阻尼参数自适应\n提议的步长 $h=h_{lm}$ 的成功与否通过一个**增益比** $\\rho$ 来评估。该比率比较了目标函数的实际减少量与局部二次模型 $L(h)$ 预测的减少量。\n- **实际减少量**: $\\Delta F_{actual} = F(p_k) - F(p_k + h)$\n- **预测减少量**: $\\Delta F_{pred} = L(0) - L(h) = -h^T J_k^T r_k - \\frac{1}{2}h^T (J_k^T J_k) h$\n\n增益比为 $\\rho = \\frac{\\Delta F_{actual}}{\\Delta F_{pred}}$。自适应策略如下：\n1.  如果 $\\rho$ 为正且显著（例如 $\\rho > 10^{-4}$），则步长是“有效的”。接受该步长（$p_{k+1} = p_k + h$），并减小阻尼 $\\lambda$ 以在下一次迭代中更接近更快的 Gauss-Newton 方法。一个常见的更新规则是 $\\lambda \\leftarrow \\lambda \\cdot \\max(1/3, 1-(2\\rho-1)^3)$。\n2.  如果 $\\rho$ 很小或为负，则步长是“无效的”。拒绝该步长（$p_{k+1} = p_k$），并增加阻尼 $\\lambda$ 以朝向更稳健的梯度下降方向移动。然后算法在同一次迭代 $k$ 中使用更大的 $\\lambda$ 重新计算步长 $h$。一个常见的规则是 $\\lambda \\leftarrow \\lambda \\cdot v$，其中 $v$ 是一个乘法因子，通常从 $v=2$ 开始，并在连续拒绝时增加。\n\n$\\lambda$ 的初始值根据问题的规模来选择，例如，$\\lambda_0 = \\tau \\cdot \\max_{ii}((J_0^T J_0)_{ii})$，其中 $\\tau$ 是一个小常数（例如 $10^{-4}$）。\n\n#### 终止准则\n当满足以下条件之一时，迭代过程终止，表明已找到满意的解：\n1.  **小梯度**：目标函数的梯度大小接近于零：$\\lVert J_k^T r_k \\rVert_{\\infty} \\le \\epsilon_g$。\n2.  **小参数更新**：参数向量的相对变化可以忽略不计：$\\lVert p_{k+1} - p_k \\rVert_2 \\le \\epsilon_p (\\lVert p_k \\rVert_2 + \\epsilon_p)$。\n3.  **小目标函数变化**：目标函数值的相对变化可以忽略不计：$|F(p_{k+1}) - F(p_k)| \\le \\epsilon_f (F(p_k) + \\epsilon_f)$。\n4.  **最大迭代次数**：达到预定的最大迭代次数 $k_{max}$，以防止无限循环。\n\n这里，$\\epsilon_g, \\epsilon_p, \\epsilon_f$ 是小的容差值（例如 $10^{-8}$）。\n\n### 5. Levenberg-Marquardt 算法总结\n完整的算法如下：\n\n1.  **初始化**：\n    - 选择一个初始参数猜测 $p_0$。\n    - 设置容差 $\\epsilon_g, \\epsilon_p, \\epsilon_f$ 和最大迭代次数 $k_{max}$。\n    - 计算初始残差 $r_0 = y - f(x, p_0)$ 和目标函数 $F_0 = \\frac{1}{2}r_0^T r_0$。\n    - 计算初始雅可比矩阵 $J_0$ 和梯度 $g_0 = J_0^T r_0$。\n    - 初始化阻尼参数 $\\lambda_0$ 和增加因子 $v$。\n    - 设置迭代计数器 $k = 0$。\n\n2.  **主循环**：当终止准则未满足时：\n    a. 求解阻尼正规方程 $(J_k^T J_k + \\lambda_k I) h = -g_k$ 以获得步长 $h$。\n    b. 评估提议的新参数向量 $p_{new} = p_k + h$。\n    c. 计算增益比 $\\rho = \\frac{F(p_k) - F(p_{new})}{\\Delta F_{pred}}$。\n    d. **如果 $\\rho > \\epsilon_{\\rho}$**：\n        i. 接受步长：$p_{k+1} = p_{new}$。\n        ii. 更新目标函数 $F_{k+1}$、残差 $r_{k+1}$、雅可比矩阵 $J_{k+1}$ 和梯度 $g_{k+1}$。\n        iii. 减小阻尼：$\\lambda_{k+1} = \\lambda_k \\cdot \\max(1/3, 1-(2\\rho-1)^3)$ 并重置 $v=2$。\n        iv. 检查关于步长和目标函数变化的终止准则。\n        v. 增加 $k \\leftarrow k+1$。\n    e. **否则（$\\rho \\le \\epsilon_{\\rho}$）**：\n        i. 拒绝步长。参数不更新。\n        ii. 增加阻尼：$\\lambda_k \\leftarrow \\lambda_k \\cdot v$，然后 $v \\leftarrow 2v$。\n        iii. 从步骤 2a 开始，使用新的 $\\lambda_k$ 重复。\n\n3.  **终止**：返回最终的参数向量 $p_k$。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef levenberg_marquardt(model_func, p_init, x_data, y_data, max_iter=100, tol_g=1e-8, tol_p=1e-8, tol_f=1e-8):\n    \"\"\"\n    Implements the Levenberg-Marquardt algorithm for nonlinear least squares.\n\n    Args:\n        model_func: The model function f(x, p).\n        p_init: Initial guess for the parameters.\n        x_data: Independent variable data.\n        y_data: Dependent variable data (observations).\n        max_iter: Maximum number of iterations.\n        tol_g: Tolerance for the infinity norm of the gradient.\n        tol_p: Tolerance for the parameter update step.\n        tol_f: Tolerance for the change in the objective function.\n\n    Returns:\n        The fitted parameter vector.\n    \"\"\"\n    p_k = np.array(p_init, dtype=float)\n    m = len(p_k)\n    eps = np.finfo(np.float64).eps\n\n    # Initial evaluation\n    r_k = y_data - model_func(x_data, p_k)\n    F_k = 0.5 * np.dot(r_k, r_k)\n\n    # Jacobian calculation function\n    def calculate_jacobian(p):\n        J = np.zeros((len(x_data), m))\n        for j in range(m):\n            delta = np.sqrt(eps) * (1.0 + np.abs(p[j]))\n            p_plus = p.copy()\n            p_minus = p.copy()\n            p_plus[j] += delta\n            p_minus[j] -= delta\n            \n            # Central difference for jacobian of residual r = y - f\n            # dr/dp = -df/dp\n            # Using (f(p - d) - f(p + d)) / 2d to get -df/dp\n            f_plus = model_func(x_data, p_plus)\n            f_minus = model_func(x_data, p_minus)\n\n            J[:, j] = (f_minus - f_plus) / (2.0 * delta)\n        return J\n\n    J_k = calculate_jacobian(p_k)\n    g_k = np.dot(J_k.T, r_k)\n\n    if np.linalg.norm(g_k, np.inf) < tol_g:\n        return p_k\n\n    # Initial damping parameter\n    A = np.dot(J_k.T, J_k)\n    tau = 1e-4\n    mu = tau * np.max(np.diag(A))\n    nu = 2.0\n    \n    k = 0\n    while k < max_iter:\n        \n        while True:\n            H_lm = A + mu * np.identity(m)\n            try:\n                h = np.linalg.solve(H_lm, -g_k)\n            except np.linalg.LinAlgError:\n                mu *= nu\n                nu *= 2.0\n                continue\n\n            p_new = p_k + h\n            r_new = y_data - model_func(x_data, p_new)\n            F_new = 0.5 * np.dot(r_new, r_new)\n\n            # Predicted reduction\n            # pred_red = - (h^T * g_k + 0.5 * h^T * A * h)\n            pred_red = - (np.dot(h, g_k) + 0.5 * np.dot(h, np.dot(A, h)))\n            \n            # Gain ratio\n            actual_red = F_k - F_new\n            \n            if pred_red > 0: # Avoid division by zero or negative\n                rho = actual_red / pred_red\n            else:\n                rho = -1.0\n\n            if rho > 0:  # Step is accepted\n                # Check termination criteria on parameter and function change\n                p_norm = np.linalg.norm(p_k)\n                if np.linalg.norm(h) < tol_p * (p_norm + tol_p):\n                    return p_new\n                \n                F_norm = F_k\n                if abs(actual_red) < tol_f * (F_norm + tol_f):\n                    return p_new\n\n                p_k = p_new\n                F_k = F_new\n                r_k = r_new\n                \n                J_k = calculate_jacobian(p_k)\n                A = np.dot(J_k.T, J_k)\n                g_k = np.dot(J_k.T, r_k)\n\n                # Check termination on gradient\n                if np.linalg.norm(g_k, np.inf) < tol_g:\n                    return p_k\n\n                mu = mu * max(1/3.0, 1 - (2*rho - 1)**3)\n                nu = 2.0\n                break # Exit inner loop and start next iteration\n            else: # Step is rejected\n                mu *= nu\n                nu *= 2.0\n        \n        k += 1\n\n    return p_k\n\n\ndef solve():\n    \"\"\"\n    Defines and solves the three test cases specified in the problem.\n    \"\"\"\n    \n    # Test Case 1: Exponential Model\n    def model1(x, p):\n        return p[0] * np.exp(p[1] * x) + p[2]\n    \n    x1 = np.linspace(0.0, 3.0, 31)\n    y1 = 2.0 * np.exp(-0.7 * x1) + 0.5 + 0.01 * np.sin(5 * x1)\n    p_init1 = [1.0, -0.2, 0.0]\n    \n    # Test Case 2: Logistic Model\n    def model2(x, p):\n        return p[0] / (1.0 + np.exp(-p[1] * (x - p[2])))\n        \n    x2 = np.linspace(-1.5, 2.0, 15)\n    y2 = 1.5 / (1.0 + np.exp(-3.0 * (x2 - 0.5))) + 0.02 * np.cos(3 * x2)\n    p_init2 = [1.0, 1.0, 0.0]\n    \n    # Test Case 3: Sinusoidal Model\n    def model3(x, p):\n        omega = 2.5\n        return p[0] * np.cos(omega * x + p[1]) + p[2]\n        \n    x3 = np.linspace(0.0, 2.0, 21)\n    y3 = 0.8 * np.cos(2.5 * x3 + 0.4) - 0.05 + 0.01 * np.sin(7 * x3)\n    p_init3 = [0.5, 0.0, 0.0]\n\n    test_cases = [\n        (model1, p_init1, x1, y1),\n        (model2, p_init2, x2, y2),\n        (model3, p_init3, x3, y3),\n    ]\n\n    results = []\n    for model_func, p_init, x_data, y_data in test_cases:\n        p_fit = levenberg_marquardt(model_func, p_init, x_data, y_data)\n        rounded_p = [round(p, 6) for p in p_fit]\n        results.append(str(rounded_p).replace(\" \", \"\"))\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nif __name__ == '__main__':\n    solve()\n```", "id": "3256843"}]}