{"hands_on_practices": [{"introduction": "在着手处理复杂的非线性问题之前，一个明智的起点是检查是否可以通过简单的变换将其简化。本练习将通过一个经典的物理场景——RC电路的电压衰减——来引导你实践这种方法。你将学习如何通过对数变换将一个指数模型线性化，从而能够应用标准的线性最小二乘法来估计电路的时间常数 [@problem_id:2191267]。", "problem": "一名工程专业的学生需要测定一个简单的电阻-电容 (RC) 电路的时间常数。电容器初始时被充电，然后通过电阻器放电。使用示波器在多个时间点 $t$ 测量了电容器两端的电压 $V$，得到以下数据集：\n- 在 $t = 1.0$ s 时，$V = 3.03$ V\n- 在 $t = 2.0$ s 时，$V = 1.84$ V\n- 在 $t = 3.0$ s 时，$V = 1.12$ V\n- 在 $t = 4.0$ s 时，$V = 0.68$ V\n\n该过程的理论模型由 $V(t) = V_0 \\exp(-t/\\tau)$ 给出，其中 $V_0$ 是 $t=0$ 时的电压，$\\tau$ 是电路的时间常数。通过将此非线性模型转换为线性形式，并对所提供的数据应用线性最小二乘法，确定时间常数 $\\tau$ 的最佳估计值。\n\n请以秒为单位表示时间常数 $\\tau$ 的答案，并四舍五入到三位有效数字。", "solution": "我们从 RC 放电模型 $V(t)=V_{0}\\exp(-t/\\tau)$ 开始。对等式两边取自然对数，得到一个关于 $t$ 的线性关系：\n$$\n\\ln V(t)=\\ln V_{0}-\\frac{1}{\\tau}t.\n$$\n定义 $y=\\ln V$ 和 $x=t$。那么模型就变为 $y=b+mx$，其中 $b=\\ln V_{0}$ 且 $m=-\\frac{1}{\\tau}$。对数据点 $(x_{i},y_{i})$（$i=1,\\dots,n$）使用线性最小二乘法，斜率 $m$ 为\n$$\nm=\\frac{n\\sum x_{i}y_{i}-\\left(\\sum x_{i}\\right)\\left(\\sum y_{i}\\right)}{n\\sum x_{i}^{2}-\\left(\\sum x_{i}\\right)^{2}},\n$$\n时间常数为 $\\tau=-\\frac{1}{m}$。\n\n根据测量数据：\n- $(t_{i},V_{i})$: $(1.0,3.03)$, $(2.0,1.84)$, $(3.0,1.12)$, $(4.0,0.68)$。\n- 计算 $y_{i}=\\ln V_{i}$：\n$$\n\\ln(3.03)\\approx 1.10856262,\\quad \\ln(1.84)\\approx 0.60976535,\\quad \\ln(1.12)\\approx 0.11332914,\\quad \\ln(0.68)\\approx -0.38566200.\n$$\n现在计算所需的和（$n=4$）：\n$$\n\\sum x_{i}=1+2+3+4=10,\\quad \\sum x_{i}^{2}=1+4+9+16=30,\n$$\n$$\n\\sum y_{i}\\approx 1.10856262+0.60976535+0.11332914-0.38566200\\approx 1.44599511,\n$$\n$$\n\\sum x_{i}y_{i}\\approx 1\\cdot 1.10856262+2\\cdot 0.60976535+3\\cdot 0.11332914+4\\cdot(-0.38566200)\\approx 1.12543274.\n$$\n代入斜率公式：\n$$\nm=\\frac{4\\cdot 1.12543274-10\\cdot 1.44599511}{4\\cdot 30-10^{2}}=\\frac{4.50173096-14.4599511}{20}\\approx -0.49791101.\n$$\n因此，\n$$\n\\tau=-\\frac{1}{m}\\approx \\frac{1}{0.49791101}\\approx 2.00839\\ \\text{s}.\n$$\n四舍五入到三位有效数字，最佳估计值为 $2.01$ s。", "answer": "$$\\boxed{2.01}$$", "id": "2191267"}, {"introduction": "当模型无法被有效线性化时，我们就需要采用迭代算法，而高斯-牛顿法是解决非线性最小二乘问题的基石。本练习旨在揭开该算法的“神秘面纱”，要求你手动完成一次完整的迭代计算，以拟合放射性同位素的衰变数据 [@problem_id:2191241]。通过亲手计算残差向量、雅可比矩阵并求解正规方程，你将对高斯-牛顿法如何逐步逼近最优解建立起一个具体而深刻的理解。", "problem": "一位实验物理学家正在研究一种新合成同位素的放射性衰变。样本的活度，即每秒的衰变次数，在几个时间点进行了测量。收集到的数据如下：\n\n- 在时间 $t=0.0$ 小时，活度为 $95$ 贝克勒尔 (Bq)。\n- 在时间 $t=1.0$ 小时，活度为 $35$ Bq。\n- 在时间 $t=2.0$ 小时，活度为 $13$ Bq。\n\n活度 $A(t)$ 作为时间 $t$ 的函数的理论模型由指数衰变定律给出：\n$$ A(t; C, \\lambda) = C e^{-\\lambda t} $$\n其中 $C$ 是 $t=0$ 时的初始活度，$\\lambda$ 是衰变常数，单位为小时的倒数。\n\n为了从实验数据中估计参数 $C$ 和 $\\lambda$，该物理学家决定使用高斯-牛顿法进行非线性最小二乘拟合。目标是找到参数矢量 $\\mathbf{x} = (C, \\lambda)^T$，以最小化模型预测值与测量数据之间差值的平方和。\n\n高斯-牛顿算法通过迭代来优化参数估计值。从一个初始猜测 $\\mathbf{x}_0$ 开始，通过求解称为正规方程的线性系统来找到参数的更新量 $\\Delta\\mathbf{x}$：\n$$ (J^T J) \\Delta\\mathbf{x} = -J^T \\mathbf{f} $$\n其中 $\\mathbf{f}$ 是残差矢量（模型预测值减去测量数据），$J$ 是模型函数关于参数的雅可比矩阵，两者都在当前猜测值下进行计算。然后，新的估计值为 $\\mathbf{x}_1 = \\mathbf{x}_0 + \\Delta\\mathbf{x}$。\n\n从初始猜测 $\\mathbf{x}_0 = (C_0, \\lambda_0) = (100.0, 0.900)$ 开始，执行恰好一次高斯-牛顿法迭代，以找到更新后的参数估计值 $\\mathbf{x}_1 = (C_1, \\lambda_1)$。\n\n给出更新后参数 $C_1$ 和 $\\lambda_1$ 的数值。将你的最终答案四舍五入到三位有效数字。", "solution": "我们用 $A(t;C,\\lambda)=C\\exp(-\\lambda t)$ 来模拟活度。对于数据 $(t_{i},y_{i})$，其中 $(t_{0},y_{0})=(0,95)$、$(t_{1},y_{1})=(1,35)$、$(t_{2},y_{2})=(2,13)$，在 $(C,\\lambda)$ 处的残差为\n$$\nr_{i}(C,\\lambda)=C\\exp(-\\lambda t_{i})-y_{i}.\n$$\n残差矢量关于 $(C,\\lambda)$ 的雅可比矩阵的行向量为\n$$\n\\left[\\frac{\\partial r_{i}}{\\partial C},\\frac{\\partial r_{i}}{\\partial \\lambda}\\right]=\\left[\\exp(-\\lambda t_{i}),-Ct_{i}\\exp(-\\lambda t_{i})\\right].\n$$\n在初始猜测 $\\mathbf{x}_{0}=(C_{0},\\lambda_{0})=(100,0.9)$ 处，定义\n$$\na_{0}=\\exp(-0.9\\cdot 0)=1,\\quad a_{1}=\\exp(-0.9)=0.4065696597405991,\\quad a_{2}=\\exp(-1.8)=0.16529888822158653,\n$$\n并注意 $a_{1}^{2}=a_{2}$，$a_{2}^{2}=\\exp(-3.6)$，$a_{2}^{3}=\\exp(-5.4)$。模型值为\n$$\nA(t_{0})=100a_{0}=100,\\quad A(t_{1})=100a_{1}=40.65696597405991,\\quad A(t_{2})=100a_{2}=16.529888822158653,\n$$\n因此残差矢量 $\\mathbf{f}$ (模型值减去数据值) 为\n$$\n\\mathbf{f}=\\begin{pmatrix}100-95\\\\ 100a_{1}-35\\\\ 100a_{2}-13\\end{pmatrix}=\\begin{pmatrix}5\\\\ 5.65696597405991\\\\ 3.529888822158653\\end{pmatrix}.\n$$\n在 $\\mathbf{x}_{0}$ 处的雅可比矩阵为\n$$\nJ=\\begin{pmatrix}\na_{0}  -C_{0}\\cdot 0\\cdot a_{0}\\\\\na_{1}  -C_{0}\\cdot 1\\cdot a_{1}\\\\\na_{2}  -C_{0}\\cdot 2\\cdot a_{2}\n\\end{pmatrix}\n=\\begin{pmatrix}\n1  0\\\\\na_{1}  -100a_{1}\\\\\na_{2}  -200a_{2}\n\\end{pmatrix}.\n$$\n计算 $J^{T}J$ 和 $J^{T}\\mathbf{f}$。使用 $a_{1}^{2}=a_{2}$，\n$$\nJ^{T}J=\\begin{pmatrix}\n1+a_{1}^{2}+a_{2}^{2}  a_{1}(-100a_{1})+a_{2}(-200a_{2})\\\\\na_{1}(-100a_{1})+a_{2}(-200a_{2})  (100a_{1})^{2}+(200a_{2})^{2}\n\\end{pmatrix}\n=\\begin{pmatrix}\n1+a_{2}+a_{2}^{2}  -100a_{2}-200a_{2}^{2}\\\\\n-100a_{2}-200a_{2}^{2}  10000a_{2}+40000a_{2}^{2}\n\\end{pmatrix}.\n$$\n数值上，\n$$\nJ^{T}J=\\begin{pmatrix}\n1.1926226106688791  -21.994633311617165\\\\\n-21.994633311617165  2745.937780107568\n\\end{pmatrix}.\n$$\n接下来，\n$$\nJ^{T}\\mathbf{f}=\\begin{pmatrix}\na_{0}r_{0}+a_{1}r_{1}+a_{2}r_{2}\\\\\n0\\cdot r_{0}+(-100a_{1})r_{1}+(-200a_{2})r_{2}\n\\end{pmatrix}.\n$$\n使用 $r_{1}=100a_{1}-35$ 和 $r_{2}=100a_{2}-13$，第一个分量简化为\n$$\na_{0}r_{0}+a_{1}r_{1}+a_{2}r_{2}=5+87a_{2}+100a_{2}^{2}-35a_{1},\n$$\n第二个分量简化为\n$$\n(-100a_{1})r_{1}+(-200a_{2})r_{2}=3500a_{1}-7400a_{2}-20000a_{2}^{2}.\n$$\n数值上，\n$$\nJ^{T}\\mathbf{f}=\\begin{pmatrix}7.883437429086315\\\\ -346.69241269349484\\end{pmatrix}.\n$$\n高斯-牛顿法的正规方程为 $(J^{T}J)\\Delta\\mathbf{x}=-J^{T}\\mathbf{f}$。令 $M=J^{T}J$ 且 $\\mathbf{b}=-J^{T}\\mathbf{f}=\\begin{pmatrix}-7.883437429086315\\\\ 346.69241269349484\\end{pmatrix}$。通过克莱姆法则求解 $M\\Delta\\mathbf{x}=\\mathbf{b}$。行列式简化为\n$$\n\\det(M)=(1+a_{2}+a_{2}^{2})(10000a_{2}+40000a_{2}^{2})-(100a_{2}+200a_{2}^{2})^{2}=10000a_{2}+40000a_{2}^{2}+10000a_{2}^{3},\n$$\n其数值为\n$$\n\\det(M)=2791.1035895336945.\n$$\n对于 $\\Delta C$，分子为\n$$\nN_{1}=b_{1}M_{22}-b_{2}M_{12}=(10000a_{2}+40000a_{2}^{2})b_{1}+(100a_{2}+200a_{2}^{2})b_{2},\n$$\n其简化为\n$$\nN_{1}=700000\\,a_{2}^{2}a_{1}-50000\\,a_{2}-330000\\,a_{2}^{2}-1000000\\,a_{2}^{3}.\n$$\n数值上，\n$$\nN_{1}=-14022.056184528922,\\qquad \\Delta C=\\frac{N_{1}}{\\det(M)}\\approx -5.0238394.\n$$\n对于 $\\Delta\\lambda$，分子为\n$$\nN_{2}=M_{11}b_{2}-M_{12}b_{1}=(1+a_{2}+a_{2}^{2})b_{2}+(100a_{2}+200a_{2}^{2})b_{1},\n$$\n其简化为\n$$\nN_{2}=-3500a_{1}+3500a_{2}^{2}a_{1}+6900a_{2}+17700a_{2}^{2}.\n$$\n数值上，\n$$\nN_{2}=240.07989483777672,\\qquad \\Delta\\lambda=\\frac{N_{2}}{\\det(M)}\\approx 0.0860161.\n$$\n更新参数：\n$$\nC_{1}=C_{0}+\\Delta C=100-5.0238394\\approx 94.9762,\\quad \\lambda_{1}=\\lambda_{0}+\\Delta\\lambda=0.9+0.0860161\\approx 0.986016.\n$$\n四舍五入到三位有效数字，\n$$\nC_{1}\\approx 95.0,\\qquad \\lambda_{1}\\approx 0.986.\n$$", "answer": "$$\\boxed{\\begin{pmatrix}95.0  0.986\\end{pmatrix}}$$", "id": "2191241"}, {"introduction": "手动计算虽然富有启发性，但在解决实际问题时效率低下。本练习将引导你将高斯-牛顿算法付诸实践，通过编写代码来实现自动化拟合，并探讨一个更深层次的统计问题：我们应该选择直接的非线性拟合还是变换后的线性拟合？ [@problem_id:3256693] 通过对比加性噪声和乘性噪声两种不同情景下的拟合效果，本练习将揭示选择正确拟合策略的重要性，并加深你对模型假设的理解。", "problem": "考虑非线性模型 $y=\\alpha x^{\\beta}$，其中 $x>0$ 且 $\\alpha>0$。您将研究两种噪声模型和两种针对 $(\\alpha,\\beta)$ 的估计策略：在原始变量中进行非线性最小二乘法 (NLS) 估计，以及在对数变换后进行普通最小二乘法 (OLS) 估计（对数-对数回归）。这两种噪声模型是：加性噪声，其中 $y_i=\\alpha x_i^{\\beta}+\\varepsilon_i$；以及乘性噪声，其中 $y_i=\\alpha x_i^{\\beta}e^{\\varepsilon_i}$。假定加性噪声 $\\varepsilon_i$ 是独立同分布的，且均值为零；乘性噪声被建模为 $e^{\\varepsilon_i}$，其中 $\\varepsilon_i$ 是独立同分布的，且均值为零。所有对数运算均使用自然对数。\n\n从最小化 $\\sum_{i=1}^{n} r_i(\\theta)^2$ 的参数 $\\theta=(\\alpha,\\beta)$ 的非线性最小二乘法的一般定义出发，通过对残差进行一阶泰勒近似并求解相关的最小二乘线性问题，来推导高斯-牛顿法的更新步骤。其中 $r_i(\\theta)$ 表示第 $i$ 个数据的残差。推导必须从原始、未变换变量中的残差定义 $r_i(\\theta)$ 开始，然后通过定义残差关于参数的雅可比矩阵来继续。该算法必须实现一个带有明确停止准则的实用更新，以确保当初始猜测接近解时，能够收敛到局部极小值。\n\n实现两个估计器：\n- 一个用于加性噪声模型 $y_i=\\alpha x_i^{\\beta}+\\varepsilon_i$ 的在原始变量中的 NLS 估计器，使用带阻尼或回溯策略的高斯-牛顿法以确保下降。将 $\\alpha$ 参数化为 $e^{a}$（其中 $a\\in\\mathbb{R}$）以强制 $\\alpha>0$。\n- 一个用于乘性噪声模型 $y_i=\\alpha x_i^{\\beta}e^{\\varepsilon_i}$ 的对数-对数 OLS 估计器，通过普通最小二乘法拟合 $\\log(y_i)=\\log(\\alpha)+\\beta\\log(x_i)+\\varepsilon_i$。仅使用 $y_i>0$ 的数据点（因为对数对非正值无定义）。如果满足 $y_i>0$ 的点少于两个，则在初始化 NLS 时回退到一个合理的默认初始猜测。\n\n对于这两个估计器，计算参数估计值 $(\\widehat{\\alpha},\\widehat{\\beta})$，并使用以下误差度量来评估其准确性：\n$$\nE=\\sqrt{\\left(\\log(\\widehat{\\alpha})-\\log(\\alpha_{\\text{true}})\\right)^2+\\left(\\widehat{\\beta}-\\beta_{\\text{true}}\\right)^2}.\n$$\n此误差度量在 $(\\log(\\alpha),\\beta)$ 参数化下比较估计值，以确保一个尺度一致的测量。此处的对数为自然对数。\n\n使用固定的伪随机生成器种子和以下测试套件生成合成观测数据。对于每种情况，$x$ 值是预定义的，您必须根据指定的模型和噪声类型模拟 $y$。在乘性噪声情况下，设置 $\\varepsilon_i\\sim\\mathcal{N}(0,\\sigma_{\\log}^2)$ 并使用 $y_i=\\alpha x_i^{\\beta}e^{\\varepsilon_i}$。在加性噪声情况下，设置 $\\varepsilon_i\\sim\\mathcal{N}(0,\\sigma^2)$ 并使用 $y_i=\\alpha x_i^{\\beta}+\\varepsilon_i$。对所有随机抽样使用单个固定的种子 $42$ 以确保可复现性。\n\n测试套件参数集：\n1. 乘性噪声（中等）：$\\alpha_{\\text{true}}=2.5$，$\\beta_{\\text{true}}=1.2$，$\\sigma_{\\log}=0.25$，$x$ 在 $0.1$ 到 $100$ 的对数网格上取 $80$ 个点（含两端）。\n2. 加性噪声（中等）：$\\alpha_{\\text{true}}=5.0$，$\\beta_{\\text{true}}=0.5$，$\\sigma=0.5$，$x$ 在 $0.2$ 到 $10.0$ 的均匀网格上取 $80$ 个点（含两端）。\n3. 乘性噪声（小）：$\\alpha_{\\text{true}}=1.0$，$\\beta_{\\text{true}}=2.0$，$\\sigma_{\\log}=0.05$，$x$ 在 $1.0$ 到 $100.0$ 的对数网格上取 $60$ 个点（含两端）。\n4. 加性噪声（小）：$\\alpha_{\\text{true}}=3.0$，$\\beta_{\\text{true}}=1.1$，$\\sigma=0.05$，$x$ 在 $0.1$ 到 $50.0$ 的对数网格上取 $60$ 个点（含两端）。\n5. 乘性噪声（大）：$\\alpha_{\\text{true}}=10.0$，$\\beta_{\\text{true}}=0.3$，$\\sigma_{\\log}=0.6$，$x$ 在 $0.5$ 到 $50.0$ 的对数网格上取 $40$ 个点（含两端）。\n\n对于每个测试用例，计算两种估计：一种是通过在原始变量中进行 NLS 得到，另一种是通过对数-对数 OLS 得到。然后通过误差度量 $E$ 评估在该用例中哪种方法更好。如果一种方法获得的误差 $E$ 严格小于另一种方法，则认为该方法更好。\n\n最终输出格式规范：\n- 对于上述顺序中的每个测试用例 $k$，输出一个布尔值，指示理论上与噪声模型匹配的方法是否比另一种方法获得了更低的误差。也就是说，如果对数-对数 OLS 误差低于 NLS 误差，则为用例 $1$、$3$ 和 $5$ 输出 $\\text{True}$；如果 NLS 误差低于对数-对数 OLS 误差，则为用例 $2$ 和 $4$ 输出 $\\text{True}$。否则输出 $\\text{False}$。\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表的结果（例如，\"[True,False,True,True,False]\"）。", "solution": "该问题要求推导并实现两个用于非线性模型 $y = \\alpha x^{\\beta}$ 参数 $(\\alpha, \\beta)$ 的估计器。第一个估计器基于针对加性噪声模型的非线性最小二乘法 (NLS)，并使用高斯-牛顿算法实现。第二个是应用于模型对数变换的普通最小二乘 (OLS) 估计器，它适用于乘性噪声。\n\n### 高斯-牛顿法的推导 ###\n\nNLS 估计器是为加性噪声模型 $y_i = \\alpha x_i^{\\beta} + \\varepsilon_i$ 设计的。最小二乘法的目标是找到使残差平方和最小化的参数值。第 $i$ 个数据点的残差是观测值 $y_i$ 与模型预测值 $f(x_i; \\theta)$ 之间的差。\n\n参数向量为 $\\theta = (a, \\beta)$，我们使用重参数化 $\\alpha = e^a$ 来强制约束 $\\alpha > 0$。因此，模型函数为 $f(x_i; a, \\beta) = e^a x_i^{\\beta}$。\n残差由下式给出：\n$$\nr_i(a, \\beta) = y_i - f(x_i; a, \\beta) = y_i - e^a x_i^{\\beta}\n$$\n要最小化的目标函数是残差平方和 $S(\\theta)$：\n$$\nS(a, \\beta) = \\sum_{i=1}^{n} r_i(a, \\beta)^2 = \\|\\mathbf{r}(a, \\beta)\\|_2^2\n$$\n其中 $\\mathbf{r}$ 是残差向量 $[r_1, r_2, \\dots, r_n]^T$。\n\n高斯-牛顿算法是解决非线性最小二乘问题的迭代过程。从初始猜测 $\\theta_k = (a_k, \\beta_k)$ 开始，它寻找一个步长 $\\Delta\\theta = (\\Delta a, \\Delta \\beta)$ 来将估计更新为 $\\theta_{k+1} = \\theta_k + \\Delta\\theta$。该步长是通过使用一阶泰勒展开在 $\\theta_k$ 周围线性化残差向量 $\\mathbf{r}$ 来找到的：\n$$\n\\mathbf{r}(\\theta_k + \\Delta\\theta) \\approx \\mathbf{r}(\\theta_k) + \\mathbf{J}(\\theta_k) \\Delta\\theta\n$$\n此处，$\\mathbf{J}(\\theta_k)$ 是残差向量 $\\mathbf{r}$ 关于参数 $\\theta$ 的雅可比矩阵，在 $\\theta_k$ 处求值。然后，算法求解最小化近似残差范数的线性最小二乘问题：\n$$\n\\min_{\\Delta\\theta} \\|\\mathbf{r}(\\theta_k) + \\mathbf{J}(\\theta_k) \\Delta\\theta\\|_2^2\n$$\n这个线性问题的解 $\\Delta\\theta$ 由正规方程给出：\n$$\n(\\mathbf{J}_k^T \\mathbf{J}_k) \\Delta\\theta = -\\mathbf{J}_k^T \\mathbf{r}_k\n$$\n其中 $\\mathbf{J}_k = \\mathbf{J}(\\theta_k)$ 且 $\\mathbf{r}_k = \\mathbf{r}(\\theta_k)$。\n\n为构建雅可比矩阵 $\\mathbf{J}$，我们计算每个残差 $r_i$ 关于参数 $a$ 和 $\\beta$ 的偏导数：\n\\begin{enumerate}\n    \\item 关于 $a$ 的导数：\n    $$\n    \\frac{\\partial r_i}{\\partial a} = \\frac{\\partial}{\\partial a} (y_i - e^a x_i^\\beta) = - \\frac{\\partial}{\\partial a} (e^a x_i^\\beta) = -e^a x_i^\\beta\n    $$\n    \\item 关于 $\\beta$ 的导数：\n    $$\n    \\frac{\\partial r_i}{\\partial \\beta} = \\frac{\\partial}{\\partial \\beta} (y_i - e^a x_i^\\beta) = -e^a \\frac{\\partial}{\\partial \\beta} (x_i^\\beta) = -e^a x_i^\\beta \\ln(x_i)\n    $$\n\\end{enumerate}\n雅可比矩阵 $\\mathbf{J}$ 是一个 $n \\times 2$ 矩阵，其第 $i$ 行由下式给出：\n$$\n\\mathbf{J}_i(a, \\beta) = \\left[ \\frac{\\partial r_i}{\\partial a}, \\frac{\\partial r_i}{\\partial \\beta} \\right] = \\left[ -e^a x_i^\\beta, -e^a x_i^\\beta \\ln(x_i) \\right]\n$$\n在每次迭代 $k$ 中，通过求解 $2 \\times 2$ 线性正规方程组来计算步长 $\\Delta\\theta_k$。一种数值上更稳定的方法是直接求解 $\\Delta\\theta_k$ 的线性最小二乘问题：\n$$\n\\mathbf{J}_k \\Delta\\theta_k \\approx -\\mathbf{r}_k\n$$\n为确保收敛，可能不会采用完整的步长 $\\Delta\\theta_k$。而是使用阻尼更新：\n$$\n\\theta_{k+1} = \\theta_k + \\gamma_k \\Delta\\theta_k\n$$\n阻尼因子（或步长）$\\gamma_k \\in (0, 1]$ 通过回溯线搜索来选择，以满足下降条件 $S(\\theta_{k+1})  S(\\theta_k)$。迭代过程持续进行，直到满足停止准则，例如更新步长的范数 $\\|\\gamma_k \\Delta\\theta_k\\|$ 低于指定的容差。最终参数为 $(\\widehat{\\alpha}, \\widehat{\\beta}) = (e^{\\widehat{a}}, \\widehat{\\beta})$。\n\n### 对数-对数 OLS 估计器 ###\n\n对数-对数 OLS 估计器适用于乘性噪声模型 $y_i = \\alpha x_i^{\\beta}e^{\\varepsilon_i}$，其中 $\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma_{\\log}^2)$。对两边取自然对数可将模型线性化：\n$$\n\\ln(y_i) = \\ln(\\alpha x_i^{\\beta}e^{\\varepsilon_i}) = \\ln(\\alpha) + \\beta\\ln(x_i) + \\varepsilon_i\n$$\n这个变换后的方程在参数 $\\ln(\\alpha)$ 和 $\\beta$ 上是线性的。令 $z_i = \\ln(y_i)$，$c = \\ln(\\alpha)$，以及 $w_i = \\ln(x_i)$。模型变为：\n$$\nz_i = c + \\beta w_i + \\varepsilon_i\n$$\n这是一个标准的线性回归模型。参数 $(c, \\beta)$ 可以通过使用普通最小二乘法 (OLS) 最小化对数域中的误差平方和来估计。OLS 解提供了估计值 $(\\hat{c}, \\hat{\\beta})$，由此我们得到 $\\alpha$ 的估计值为 $\\widehat{\\alpha} = e^{\\hat{c}}$。此方法仅对 $y_i > 0$ 的数据点有效。\n\n### 实现策略 ###\n\n对于每个测试用例，根据指定的模型（加性或乘性噪声）生成合成数据 $(x_i, y_i)$。\n将两个估计器都应用于生成的数据：\n\\begin{enumerate}\n    \\item 首先计算对数-对数 OLS 估计器。它提供参数估计值 $(\\widehat{\\alpha}_{\\text{OLS}}, \\widehat{\\beta}_{\\text{OLS}})$，并同时作为 NLS 算法的高质量初始猜测。如果 OLS 失败（由于正的 $y_i$ 值少于两个），则使用默认猜测。\n    \\item 然后，从 OLS 推导的或默认的初始猜测开始，使用带回溯线搜索的高斯-牛顿法运行 NLS 估计器，以获得 $(\\widehat{\\alpha}_{\\text{NLS}}, \\widehat{\\beta}_{\\text{NLS}})$。\n\\end{enumerate}\n每个估计值 $(\\widehat{\\alpha}, \\widehat{\\beta})$ 的准确性使用误差度量 $E$ 进行评估：\n$$\nE = \\sqrt{(\\ln(\\widehat{\\alpha}) - \\ln(\\alpha_{\\text{true}}))^2 + (\\widehat{\\beta} - \\beta_{\\text{true}})^2}\n$$\n最后，对于每个测试用例，我们确定理论上与噪声模型匹配的估计方法（乘性噪声对应 OLS，加性噪声对应 NLS）是否比替代方法产生严格更低的误差 $E$。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef ols_estimator(x, y):\n    \"\"\"\n    Computes parameter estimates for y = alpha * x**beta using log-log OLS.\n    Returns (alpha_hat, beta_hat) or (None, None) if estimation is not possible.\n    \"\"\"\n    # Filter for positive y values, as log is undefined otherwise\n    positive_mask = y > 0\n    if np.sum(positive_mask)  2:\n        return None, None  # Not enough data for regression\n\n    x_log = np.log(x[positive_mask])\n    y_log = np.log(y[positive_mask])\n\n    # OLS for: log(y) = log(alpha) + beta * log(x)\n    # This is a linear model Z = A*p, where Z=y_log, p=[log(alpha), beta]\n    A = np.vstack([np.ones_like(x_log), x_log]).T\n    \n    # Solve the linear least squares problem\n    p, _, _, _ = np.linalg.lstsq(A, y_log, rcond=None)\n    \n    log_alpha_hat, beta_hat = p[0], p[1]\n    alpha_hat = np.exp(log_alpha_hat)\n    \n    return alpha_hat, beta_hat\n\ndef nls_estimator(x, y, initial_guess, max_iter=100, tol=1e-8):\n    \"\"\"\n    Computes parameter estimates for y = alpha * x**beta + eps using NLS.\n    Uses Gauss-Newton with backtracking line search.\n    \"\"\"\n    alpha_0, beta_0 = initial_guess\n    # Work with theta = [log(alpha), beta] to enforce alpha > 0\n    a_k = np.log(alpha_0) if alpha_0 > 0 else 0.0\n    b_k = beta_0\n    \n    for _ in range(max_iter):\n        alpha_k = np.exp(a_k)\n        y_pred = alpha_k * (x ** b_k)\n        \n        # Residuals\n        r = y - y_pred\n        S_k = np.sum(r**2)\n        \n        # Jacobian matrix w.r.t. parameters [a, beta]\n        # d(r)/da = -exp(a)*x^b = -y_pred\n        # d(r)/db = -exp(a)*x^b*log(x) = -y_pred*log(x)\n        J = np.vstack([-y_pred, -y_pred * np.log(x)]).T\n        \n        # Solve J*delta_theta = -r using least squares\n        try:\n            delta_theta, _, _, _ = np.linalg.lstsq(J, -r, rcond=None)\n        except np.linalg.LinAlgError:\n            # Fails if Jacobian is singular, break iteration\n            break\n            \n        # Backtracking line search\n        gamma = 1.0\n        for _ in range(10): # Max 10 backtracking steps\n            a_next, b_next = a_k + gamma * delta_theta[0], b_k + gamma * delta_theta[1]\n            y_pred_next = np.exp(a_next) * (x ** b_next)\n            S_next = np.sum((y - y_pred_next)**2)\n            \n            if S_next  S_k:\n                a_k, b_k = a_next, b_next\n                break\n            gamma /= 2.0\n        else: # Line search failed to find a better point\n            break\n            \n        # Check for convergence\n        if np.linalg.norm(gamma * delta_theta)  tol:\n            break\n            \n    alpha_hat = np.exp(a_k)\n    beta_hat = b_k\n    \n    return alpha_hat, beta_hat\n    \ndef calculate_error(alpha_hat, beta_hat, alpha_true, beta_true):\n    \"\"\"\n    Calculates the error metric E.\n    \"\"\"\n    log_alpha_hat = np.log(alpha_hat)\n    log_alpha_true = np.log(alpha_true)\n    error = np.sqrt((log_alpha_hat - log_alpha_true)**2 + (beta_hat - beta_true)**2)\n    return error\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and produce the final output.\n    \"\"\"\n    test_cases = [\n        {'id': 1, 'noise_type': 'multiplicative', 'alpha_true': 2.5, 'beta_true': 1.2, 'noise_param': 0.25,\n         'x_grid': 'log', 'x_range': (0.1, 100), 'n_points': 80},\n        {'id': 2, 'noise_type': 'additive', 'alpha_true': 5.0, 'beta_true': 0.5, 'noise_param': 0.5,\n         'x_grid': 'uniform', 'x_range': (0.2, 10.0), 'n_points': 80},\n        {'id': 3, 'noise_type': 'multiplicative', 'alpha_true': 1.0, 'beta_true': 2.0, 'noise_param': 0.05,\n         'x_grid': 'log', 'x_range': (1.0, 100.0), 'n_points': 60},\n        {'id': 4, 'noise_type': 'additive', 'alpha_true': 3.0, 'beta_true': 1.1, 'noise_param': 0.05,\n         'x_grid': 'log', 'x_range': (0.1, 50.0), 'n_points': 60},\n        {'id': 5, 'noise_type': 'multiplicative', 'alpha_true': 10.0, 'beta_true': 0.3, 'noise_param': 0.6,\n         'x_grid': 'log', 'x_range': (0.5, 50.0), 'n_points': 40},\n    ]\n\n    rng = np.random.default_rng(42)\n    results = []\n    \n    for case in test_cases:\n        # Generate data\n        if case['x_grid'] == 'log':\n            x = np.logspace(np.log10(case['x_range'][0]), np.log10(case['x_range'][1]), case['n_points'])\n        else: # uniform\n            x = np.linspace(case['x_range'][0], case['x_range'][1], case['n_points'])\n            \n        y_true = case['alpha_true'] * (x ** case['beta_true'])\n        \n        if case['noise_type'] == 'multiplicative':\n            epsilon = rng.normal(0, case['noise_param'], case['n_points'])\n            y_obs = y_true * np.exp(epsilon)\n        else: # additive\n            epsilon = rng.normal(0, case['noise_param'], case['n_points'])\n            y_obs = y_true + epsilon\n\n        # OLS estimator (for multiplicative model  NLS initial guess)\n        alpha_ols, beta_ols = ols_estimator(x, y_obs)\n\n        # NLS estimator (for additive model)\n        if alpha_ols is None:\n            # OLS failed, use a neutral default guess for NLS\n            initial_guess = (1.0, 1.0)\n        else:\n            initial_guess = (alpha_ols, beta_ols)\n            \n        alpha_nls, beta_nls = nls_estimator(x, y_obs, initial_guess)\n\n        # Evaluate errors\n        error_ols = calculate_error(alpha_ols, beta_ols, case['alpha_true'], case['beta_true']) if alpha_ols is not None else float('inf')\n        error_nls = calculate_error(alpha_nls, beta_nls, case['alpha_true'], case['beta_true'])\n        \n        # Compare methods\n        if case['noise_type'] == 'multiplicative':\n            # Matching method is OLS\n            is_matching_method_better = error_ols  error_nls\n        else:\n            # Matching method is NLS\n            is_matching_method_better = error_nls  error_ols\n            \n        results.append(is_matching_method_better)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3256693"}]}