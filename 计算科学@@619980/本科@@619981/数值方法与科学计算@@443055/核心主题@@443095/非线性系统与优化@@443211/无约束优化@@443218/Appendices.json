{"hands_on_practices": [{"introduction": "最速下降法是无约束优化的基石，它沿着负梯度方向进行迭代搜索。然而，该方法的性能严重依赖于步长 $\\alpha$ 的选择。本练习将引导你通过分析一个二次目标函数，深入探究固定步长的最速下降法的行为，并学习如何利用迭代矩阵的谱特性来精确判断算法是快速收敛、缓慢收敛还是发散。[@problem_id:3285028]", "problem": "考虑在 $\\mathbb{R}^n$ 上对一个二次连续可微的目标函数进行无约束最小化。在数值方法中，一个基本情况是严格凸二次目标函数，\n$$\nf(x) = \\tfrac{1}{2} x^{\\top} Q x - b^{\\top} x,\n$$\n其中 $Q \\in \\mathbb{R}^{n \\times n}$ 是对称正定矩阵，$b \\in \\mathbb{R}^{n}$。唯一的最小化点 $x^{\\star}$ 满足 $Q x^{\\star} = b$。采用固定步长 $\\alpha > 0$ 的最速下降法 (SD) 的更新公式为\n$$\nx_{k+1} = x_k - \\alpha \\nabla f(x_k),\n$$\n其中 $\\nabla f(x) = Q x - b$。\n\n采用固定步长 $\\alpha$ 的最速下降法的解析行为可以表现为快速收敛、缓慢收敛或发散。本问题中的分类必须基于误差序列的渐近线性速率。定义误差 $e_k = x_k - x^{\\star}$ 和渐近线性速率\n$$\nr_{\\mathrm{asym}}(\\alpha) = \\limsup_{k \\to \\infty} \\frac{\\|e_{k+1}\\|_2}{\\|e_k\\|_2}.\n$$\n使用以下规则对每个测试用例的行为进行分类：\n- 如果 $r_{\\mathrm{asym}}(\\alpha) \\ge 1$（包括边界情况 $r_{\\mathrm{asym}}(\\alpha) = 1$），则返回整数 $0$ 表示发散或不收敛。\n- 如果 $0.95 \\le r_{\\mathrm{asym}}(\\alpha)  1$，则返回整数 $1$ 表示缓慢收敛。\n- 如果 $0 \\le r_{\\mathrm{asym}}(\\alpha)  0.95$，则返回整数 $2$ 表示可接受的收敛速度。\n\n你的程序必须：\n- 为给定的二次实例实现固定步长 $\\alpha$ 的 SD 迭代，从指定的初始点 $x_0$ 开始，并运行 $N$ 次迭代。\n- 通过求解 $Q x^{\\star} = b$ 计算 $x^{\\star}$，并经验性地观察误差范数 $\\|e_k\\|_2$。\n- 使用与渐近线性速率定义一致的、有数学原理支持的准则来证明每个测试用例分类的合理性（例如，通过分析误差上的诱导线性迭代及其谱性质）。\n- 生成一行输出，其中包含所有测试用例的分类结果，形式为方括号括起来的逗号分隔列表（例如，$[0,1,2]$）。每个列表元素必须是如上定义的整数。\n\n本问题不涉及物理单位和角度。所有数值阈值（如 $0.95$）都必须视为十进制数。\n\n测试套件（每个测试提供 $(Q, b, x_0, \\alpha, N)$）：\n1. $Q = \\operatorname{diag}(1, 100)$，$b = (1, 1)^{\\top}$，$x_0 = (10, -10)^{\\top}$，$\\alpha = 0.05$，$N = 50$。\n2. $Q = \\operatorname{diag}(1, 100)$，$b = (1, 1)^{\\top}$，$x_0 = (10, -10)^{\\top}$，$\\alpha = 0.02$，$N = 50$。\n3. $Q = \\operatorname{diag}(1, 100)$，$b = (1, 1)^{\\top}$，$x_0 = (10, -10)^{\\top}$，$\\alpha = 0.019$，$N = 200$。\n4. $Q = \\operatorname{diag}(2, 2)$，$b = (1, 1)^{\\top}$，$x_0 = (10, -10)^{\\top}$，$\\alpha = 0.2$，$N = 50$。\n5. $Q = \\operatorname{diag}(0.2, 0.1)$，$b = (1, 1)^{\\top}$，$x_0 = (10, -10)^{\\top}$，$\\alpha = 12$，$N = 50$。\n\n你的程序应生成一行输出，其中包含用方括号括起来的逗号分隔的结果列表（例如，$[result1,result2,result3,result4,result5]$）。", "solution": "该问题要求对最速下降法 (SD) 的收敛行为进行分类。该方法应用于一个无约束最小化问题，其目标函数为严格凸二次函数 $f(x) = \\tfrac{1}{2} x^{\\top} Q x - b^{\\top} x$，其中 $x \\in \\mathbb{R}^n$，$Q \\in \\mathbb{R}^{n \\times n}$ 是对称正定 (SPD) 矩阵，$b \\in \\mathbb{R}^{n}$。分类必须基于给定固定步长 $\\alpha  0$ 下的渐近线性收敛速率 $r_{\\mathrm{asym}}(\\alpha)$。\n\nSD 迭代由更新规则 $x_{k+1} = x_k - \\alpha \\nabla f(x_k)$ 定义。对于给定的二次目标函数，其梯度为 $\\nabla f(x) = Qx - b$。将其代入迭代公式可得：\n$$\nx_{k+1} = x_k - \\alpha(Qx_k - b) = (I - \\alpha Q)x_k + \\alpha b\n$$\n其中 $I$ 是相应维度的单位矩阵。\n\n$f(x)$ 的唯一最小化点，记为 $x^{\\star}$，是线性方程组 $\\nabla f(x^\\star) = Qx^{\\star} - b = 0$ 的解，这意味着 $Qx^{\\star} = b$。第 k 次迭代的误差定义为向量 $e_k = x_k - x^{\\star}$。我们可以为误差向量建立一个递推关系：\n$$\ne_{k+1} = x_{k+1} - x^{\\star} = \\left( (I - \\alpha Q)x_k + \\alpha b \\right) - x^{\\star}\n$$\n代入 $b = Qx^{\\star}$，我们可以写出 $\\alpha b = \\alpha Qx^{\\star}$。这使我们能够将误差递推式表示为：\n$$\ne_{k+1} = (I - \\alpha Q)x_k + \\alpha Qx^{\\star} - x^{\\star} = (I - \\alpha Q)x_k - (I - \\alpha Q)x^{\\star}\n$$\n通过提出公因子矩阵 $(I - \\alpha Q)$，我们得到线性误差动态：\n$$\ne_{k+1} = (I - \\alpha Q) e_k\n$$\n这个方程表明，每一步的误差是通过将前一步的误差乘以固定的迭代矩阵 $G = I - \\alpha Q$ 得到的。误差大小 $\\|e_k\\|_2$ 的长期行为由该迭代矩阵的谱半径 $\\rho(G)$ 决定，谱半径是其所有特征值中绝对值的最大值。渐近线性收敛速率正是这个谱半径：\n$$\nr_{\\mathrm{asym}}(\\alpha) = \\limsup_{k \\to \\infty} \\frac{\\|e_{k+1}\\|_2}{\\|e_k\\|_2} = \\rho(G) = \\rho(I - \\alpha Q)\n$$\n由于矩阵 $Q$ 是对称的，它是可对角化的，并且其所有特征值（记为 $\\lambda_i$）都是实数。由于 $Q$ 也是正定的，其所有特征值都是正数，即 $\\lambda_i  0$。矩阵 $G = I - \\alpha Q$ 的特征值由 $\\mu_i = 1 - \\alpha \\lambda_i$ 给出。因此，$G$ 的谱半径是：\n$$\n\\rho(G) = \\max_i |\\mu_i| = \\max_i |1 - \\alpha \\lambda_i|\n$$\n对于固定的 $\\alpha  0$，函数 $g(\\lambda) = |1 - \\alpha \\lambda|$ 是一个V形函数，其值取决于 $\\lambda$ 到 $1/\\alpha$ 的距离。该函数在 $Q$ 的特征值集合上的最大值将在最小特征值 $\\lambda_{\\min}$ 或最大特征值 $\\lambda_{\\max}$ 处取得。因此，渐近速率可以直接使用 $Q$ 的最小和最大特征值来计算：\n$$\nr_{\\mathrm{asym}}(\\alpha) = \\max \\left( |1 - \\alpha \\lambda_{\\min}|, |1 - \\alpha \\lambda_{\\max}| \\right)\n$$\n这个公式为渐近速率提供了一个精确的解析值，这优于从有限次迭代（$N$ 次）中进行的经验估计。收敛分类仅依赖于 $Q$ 和 $\\alpha$；初始点 $x_0$ 和迭代次数 $N$ 对于确定渐近速率是无关的。收敛的条件是 $r_{\\mathrm{asym}}(\\alpha)  1$，这当且仅当 $0  \\alpha  2/\\lambda_{\\max}$ 时成立。\n\n我们现在将这个解析结果应用于每个测试用例。\n\n情况 1：$Q = \\operatorname{diag}(1, 100)$，$\\alpha = 0.05$。\n对角矩阵 $Q$ 的特征值是其对角元素。因此，$\\lambda_{\\min} = 1$ 且 $\\lambda_{\\max} = 100$。\n$r_{\\mathrm{asym}}(0.05) = \\max(|1 - (0.05)(1)|, |1 - (0.05)(100)|) = \\max(|1 - 0.05|, |1 - 5|) = \\max(0.95, 4.0) = 4.0$。\n由于 $r_{\\mathrm{asym}}(\\alpha) = 4.0 \\ge 1$，该方法发散。分类为 $0$。\n\n情况 2：$Q = \\operatorname{diag}(1, 100)$，$\\alpha = 0.02$。\n特征值为 $\\lambda_{\\min} = 1$ 和 $\\lambda_{\\max} = 100$。步长 $\\alpha=0.02$ 正好在稳定性边界上，$\\alpha = 2/\\lambda_{\\max} = 2/100 = 0.02$。\n$r_{\\mathrm{asym}}(0.02) = \\max(|1 - (0.02)(1)|, |1 - (0.02)(100)|) = \\max(|1 - 0.02|, |1 - 2|) = \\max(0.98, 1.0) = 1.0$。\n由于 $r_{\\mathrm{asym}}(\\alpha) = 1.0 \\ge 1$，该方法未能收敛。分类为 $0$。\n\n情况 3：$Q = \\operatorname{diag}(1, 100)$，$\\alpha = 0.019$。\n特征值为 $\\lambda_{\\min} = 1$ 和 $\\lambda_{\\max} = 100$。\n$r_{\\mathrm{asym}}(0.019) = \\max(|1 - (0.019)(1)|, |1 - (0.019)(100)|) = \\max(|1 - 0.019|, |1 - 1.9|) = \\max(0.981, 0.9) = 0.981$。\n由于 $0.95 \\le 0.981  1$，收敛被分类为缓慢。分类为 $1$。\n\n情况 4：$Q = \\operatorname{diag}(2, 2)$，$\\alpha = 0.2$。\n特征值相同：$\\lambda_{\\min} = \\lambda_{\\max} = 2$。\n$r_{\\mathrm{asym}}(0.2) = |1 - (0.2)(2)| = |1 - 0.4| = 0.6$。\n由于 $0 \\le 0.6  0.95$，收敛被分类为可接受的。分类为 $2$。\n\n情况 5：$Q = \\operatorname{diag}(0.2, 0.1)$，$\\alpha = 12$。\n特征值为 $\\lambda_{\\min} = 0.1$ 和 $\\lambda_{\\max} = 0.2$。稳定性边界为 $2/\\lambda_{\\max} = 2/0.2 = 10$。步长 $\\alpha = 12$ 在此范围之外。\n$r_{\\mathrm{asym}}(12) = \\max(|1 - (12)(0.1)|, |1 - (12)(0.2)|) = \\max(|1 - 1.2|, |1 - 2.4|) = \\max(0.2, 1.4) = 1.4$。\n由于 $r_{\\mathrm{asym}}(\\alpha) = 1.4 \\ge 1$，该方法发散。分类为 $0$。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the convergence classification for several Steepest Descent scenarios\n    on a quadratic objective function. The classification is based on the analytical\n    asymptotic convergence rate derived from the spectral properties of the iteration matrix.\n    \"\"\"\n    \n    # Test suite format: (Q, b, x0, alpha, N)\n    test_cases = [\n        (np.diag([1.0, 100.0]), np.array([1.0, 1.0]), np.array([10.0, -10.0]), 0.05, 50),\n        (np.diag([1.0, 100.0]), np.array([1.0, 1.0]), np.array([10.0, -10.0]), 0.02, 50),\n        (np.diag([1.0, 100.0]), np.array([1.0, 1.0]), np.array([10.0, -10.0]), 0.019, 200),\n        (np.diag([2.0, 2.0]), np.array([1.0, 1.0]), np.array([10.0, -10.0]), 0.2, 50),\n        (np.diag([0.2, 0.1]), np.array([1.0, 1.0]), np.array([10.0, -10.0]), 12.0, 50),\n    ]\n\n    results = []\n    for Q, b, x0, alpha, N in test_cases:\n        # The convergence classification for a quadratic objective depends only on the \n        # matrix Q and the step size alpha. The parameters b, x0, and N do not affect\n        # the asymptotic rate.\n        \n        # The analytical asymptotic rate, r_asym, is the spectral radius of the \n        # iteration matrix G = I - alpha*Q. For a symmetric matrix Q, this is:\n        # r_asym = max(|1 - alpha*lambda_min|, |1 - alpha*lambda_max|)\n        # where lambda_min and lambda_max are the minimum and maximum eigenvalues of Q.\n        \n        # For the diagonal matrices in the test cases, the eigenvalues are simply the\n        # diagonal elements.\n        eigenvalues = np.diag(Q)\n        lambda_min = np.min(eigenvalues)\n        lambda_max = np.max(eigenvalues)\n        \n        # Calculate the asymptotic convergence rate\n        rate = max(abs(1.0 - alpha * lambda_min), abs(1.0 - alpha * lambda_max))\n        \n        # Classify the behavior based on the problem's rules for the rate.\n        classification = 0 # Default: divergence or non-convergence\n        if rate  1.0:\n            if rate  0.95:\n                classification = 2 # Acceptable convergence\n            else: # 0.95 = rate  1.0\n                classification = 1 # Slow convergence\n        \n        results.append(classification)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3285028"}, {"introduction": "牛顿法利用二阶信息，收敛速度通常远超一阶方法，但其“纯粹”形式在处理非凸函数时可能变得不稳定甚至失效。本练习将指导你实现一个在“真实世界”中稳健的牛顿法求解器。你将学习如何使用 Cholesky 分解来检验并修正非正定的 Hessian 矩阵，并结合回溯线搜索来保证算法的全局收敛性，这是构建高效优化工具的关键一步。[@problem_id:3255780]", "problem": "你需要实现一个用于无约束优化的牛顿类方法，该方法通过 Cholesky 分解检查海森矩阵的正定性，并在分解失败时修改搜索方向。算法设计必须源自光滑目标函数的二阶泰勒模型，并且必须使用强制执行充分下降条件的线搜索。实现必须是完全确定性的，仅使用下面提供的固定参数值，并以精确指定的格式生成单行输出。\n\n推导的起点。设 $f:\\mathbb{R}^n\\to\\mathbb{R}$ 是二次连续可微的。在点 $\\mathbf{x}_k\\in\\mathbb{R}^n$ 附近，$f$ 的二阶泰勒模型为\n$$\nm_k(\\mathbf{s}) \\equiv f(\\mathbf{x}_k) + \\nabla f(\\mathbf{x}_k)^\\top \\mathbf{s} + \\tfrac{1}{2}\\,\\mathbf{s}^\\top \\nabla^2 f(\\mathbf{x}_k)\\,\\mathbf{s},\n$$\n其中 $\\nabla f(\\mathbf{x}_k)$ 是梯度，$\\nabla^2 f(\\mathbf{x}_k)$ 是海森矩阵。牛顿搜索方向是通过最小化 $m_k(\\mathbf{s})$ 获得的，这需要求解该模型的平稳性条件。当 $\\mathbf{x}_k$ 处的海森矩阵是对称正定（SPD）时，这将产生一个唯一的搜索方向，该方向是二次模型的局部极小值点。\n\n正定性检查与修正。为判断海森矩阵是否为对称正定（SPD），尝试进行 Cholesky 分解。如果 $\\nabla^2 f(\\mathbf{x}_k)$ 允许 Cholesky 分解，则其为 SPD。如果分解失败，则通过添加对角位移 $\\tau_k \\mathbf{I}$（其中 $\\tau_k0$）来修正二次模型，并按几何级数增加 $\\tau_k$，直到 $\\nabla^2 f(\\mathbf{x}_k)+\\tau_k \\mathbf{I}$ 的 Cholesky 分解成功为止。使用得到的位移系统来定义一个下降方向。如果由于数值问题，计算出的方向不满足严格下降条件，则退回到最速下降方向。\n\n通过回溯线搜索实现全局化。使用回溯线搜索来强制执行 Armijo 条件。从完整步长开始，并按几何级数缩减步长，直到满足充分下降条件为止。\n\n终止条件。当梯度的欧几里得范数低于给定容差，或步长度量低于给定容差，或达到最大迭代次数时停止。\n\n实现要求。\n- 对所有测试用例使用以下固定参数：初始步长 $\\alpha_0=1$，Armijo 参数 $c=10^{-4}$，回溯缩减因子 $\\rho=0.5$，初始对角位移 $\\tau_0=10^{-6}$ 并在需要时按因子 $10$ 进行几何增长，梯度范数容差 $\\varepsilon_g=10^{-8}$，步长容差 $\\varepsilon_s=10^{-12}$，以及最大迭代次数 $N_{\\max}=200$。\n- 海森矩阵的正定性检查必须使用 Cholesky 分解。如果对 $\\nabla^2 f(\\mathbf{x}_k)$ 分解失败，则使用 $\\nabla^2 f(\\mathbf{x}_k)+\\tau_k \\mathbf{I}$，其中 $\\tau_k$ 从 $\\tau_0$ 开始按几何级数增长，直到 Cholesky 分解成功。\n- 使用 Armijo 回溯线搜索，如果满足以下条件，则接受 $\\alpha$：\n$$\nf(\\mathbf{x}_k+\\alpha \\mathbf{d}_k) \\le f(\\mathbf{x}_k) + c\\,\\alpha\\,\\nabla f(\\mathbf{x}_k)^\\top \\mathbf{d}_k.\n$$\n- 如果 $\\|\\nabla f(\\mathbf{x}_k)\\|_2 \\le \\varepsilon_g$ 或 $\\alpha\\|\\mathbf{d}_k\\|_2 \\le \\varepsilon_s$，或在 $N_{\\max}$ 次迭代后终止。\n\n测试套件。实现您的求解器并在以下四个测试用例上运行。每个函数都在 $\\mathbb{R}^2$ 上定义，并指定了起始点。\n- 用例 A (严格凸二次；理想情况): $f_1(\\mathbf{x})=\\tfrac{1}{2}\\mathbf{x}^\\top Q \\mathbf{x} + \\mathbf{b}^\\top \\mathbf{x}$，其中 $Q=\\begin{bmatrix}4  1\\\\ 1  3\\end{bmatrix}$ 且 $\\mathbf{b}=\\begin{bmatrix}-1\\\\ 2\\end{bmatrix}$，起始点为 $\\mathbf{x}^{(0)}=\\begin{bmatrix}2\\\\ -1\\end{bmatrix}$。使用精确梯度 $\\nabla f_1(\\mathbf{x})=Q\\mathbf{x}+\\mathbf{b}$ 和海森矩阵 $\\nabla^2 f_1(\\mathbf{x})=Q$。\n- 用例 B (非凸，起始点处海森矩阵不定): Himmelblau 函数 $f_2(x,y)=(x^2+y-11)^2+(x+y^2-7)^2$，起始点为 $\\mathbf{x}^{(0)}=\\begin{bmatrix}0\\\\ 0\\end{bmatrix}$。使用梯度 $\\nabla f_2(x,y)=\\begin{bmatrix}4x(x^2+y-11)+2(x+y^2-7)\\\\ 2(x^2+y-11)+4y(x+y^2-7)\\end{bmatrix}$ 和\n$$\n\\nabla^2 f_2(x,y)=\\begin{bmatrix}\n8x^2+4(x^2+y-11)+2  4x+4y\\\\\n4x+4y  8y^2+4(x+y^2-7)+2\n\\end{bmatrix}.\n$$\n- 用例 C (原点附近的鞍点，海森矩阵不定): $f_3(x,y)=x^2-y^2+0.1\\,x^4+0.1\\,y^4$，起始点为 $\\mathbf{x}^{(0)}=\\begin{bmatrix}0.1\\\\ 0.1\\end{bmatrix}$。使用梯度 $\\nabla f_3(x,y)=\\begin{bmatrix}2x+0.4\\,x^3\\\\ -2y+0.4\\,y^3\\end{bmatrix}$ 和海森矩阵 $\\nabla^2 f_3(x,y)=\\begin{bmatrix}2+1.2\\,x^2  0\\\\ 0  -2+1.2\\,y^2\\end{bmatrix}$。\n- 用例 D (平坦，在极小值点处海森矩阵奇异): $f_4(x,y)=(x^2+y^2)^2$，起始点为 $\\mathbf{x}^{(0)}=\\begin{bmatrix}10^{-3}\\\\ -10^{-3}\\end{bmatrix}$。使用梯度 $\\nabla f_4(x,y)=\\begin{bmatrix}4x(x^2+y^2)\\\\ 4y(x^2+y^2)\\end{bmatrix}$ 和海森矩阵 $\\nabla^2 f_4(x,y)=\\begin{bmatrix}12x^2+4y^2  8xy\\\\ 8xy  12y^2+4x^2\\end{bmatrix}$。\n\n程序输出。对于每个用例，返回一个包含五个值的列表：最终迭代点 $\\mathbf{x}_\\star$ 的两个坐标（按顺序）、最终目标函数值 $f(\\mathbf{x}_\\star)$、所用的迭代次数，以及一个整数标志。如果至少有一次使用了对角位移 $\\tau_k0$（即，原始海森矩阵在任何迭代中 Cholesky 分解失败），则该标志为 $1$，否则为 $0$。将四个用例的结果按 A、B、C、D 的顺序汇总到一个列表中。您的程序应生成单行输出，其中包含用逗号分隔的列表形式的结果，并用方括号括起来，不含空格，例如\n$[ [\\dots],[\\dots],[\\dots],[\\dots] ]$\n但任何地方都不能有空格，例如，\n$[[a_1,a_2,a_3,a_4,a_5],[b_1,b_2,b_3,b_4,b_5],[c_1,c_2,c_3,c_4,c_5],[d_1,d_2,d_3,d_4,d_5]]$，\n其中每个 $a_i,b_i,c_i,d_i$ 是如上所述的实数或整数。不应打印任何其他输出。", "solution": "该问题要求实现一个用于无约束优化的牛顿类方法。该算法必须包含一个处理非正定海森矩阵的机制和一个确保全局收敛的线搜索策略。下文从第一性原理详细阐述了算法的推导过程，并最终给出一个完整的程序性描述。\n\n一个无约束优化问题旨在为给定的目标函数 $f: \\mathbb{R}^n \\to \\mathbb{R}$ 找到一个极小值点 $\\mathbf{x}_\\star$。迭代方法生成一个点序列 $\\{\\mathbf{x}_k\\}$，旨在收敛到 $\\mathbf{x}_\\star$。从 $\\mathbf{x}_k$ 到 $\\mathbf{x}_{k+1}$ 的更新由下式给出\n$$\n\\mathbf{x}_{k+1} = \\mathbf{x}_k + \\alpha_k \\mathbf{d}_k,\n$$\n其中 $\\mathbf{d}_k$ 是一个搜索方向，$\\alpha_k  0$ 是一个步长。\n\n牛顿方法的核心在于用一个二次模型来近似目标函数 $f$ 在当前迭代点 $\\mathbf{x}_k$ 处的值。假设 $f$ 是二次连续可微的，其在 $\\mathbf{x}_k$ 附近的二阶泰勒展开为\n$$\nf(\\mathbf{x}_k + \\mathbf{s}) \\approx m_k(\\mathbf{s}) \\equiv f(\\mathbf{x}_k) + \\nabla f(\\mathbf{x}_k)^\\top \\mathbf{s} + \\frac{1}{2}\\,\\mathbf{s}^\\top \\nabla^2 f(\\mathbf{x}_k)\\,\\mathbf{s},\n$$\n其中 $\\mathbf{s} = \\mathbf{x} - \\mathbf{x}_k$ 是步长。我们将梯度记为 $\\mathbf{g}_k = \\nabla f(\\mathbf{x}_k)$，海森矩阵记为 $H_k = \\nabla^2 f(\\mathbf{x}_k)$。则模型为\n$$\nm_k(\\mathbf{s}) = f_k + \\mathbf{g}_k^\\top \\mathbf{s} + \\frac{1}{2}\\,\\mathbf{s}^\\top H_k\\,\\mathbf{s}.\n$$\n牛顿搜索方向 $\\mathbf{d}_k$ 是最小化此二次模型的步长 $\\mathbf{s}$。最小值的必要条件是模型关于 $\\mathbf{s}$ 的梯度为零：\n$$\n\\nabla m_k(\\mathbf{s}) = \\mathbf{g}_k + H_k \\mathbf{s} = \\mathbf{0}.\n$$\n这产生牛顿线性方程组：\n$$\nH_k \\mathbf{s} = -\\mathbf{g}_k.\n$$\n如果海森矩阵 $H_k$ 是对称正定（SPD）的，那么它是可逆的，并且模型 $m_k(\\mathbf{s})$ 是严格凸的，具有唯一的极小值点。该方程组的解 $\\mathbf{d}_k = -H_k^{-1} \\mathbf{g}_k$ 是纯牛顿方向。此外，如果 $H_k$ 是 SPD 的，$\\mathbf{d}_k$ 保证是一个下降方向，意味着它与梯度的夹角大于90度：\n$$\n\\mathbf{g}_k^\\top \\mathbf{d}_k = \\mathbf{g}_k^\\top (-H_k^{-1} \\mathbf{g}_k) = -\\mathbf{g}_k^\\top H_k^{-1} \\mathbf{g}_k  0,\n$$\n因为一个 SPD 矩阵的逆也是 SPD 的。\n\n当海森矩阵 $H_k$ 不是正定时，会出现一个重大挑战。在这种情况下，二次模型 $m_k(\\mathbf{s})$ 可能没有唯一的极小值点（它可能下方无界），并且计算出的牛顿方向可能不是一个下降方向。指定的算法通过修改海森矩阵来解决此问题。$H_k$ 的正定性通过尝试 Cholesky 分解 $H_k = LL^\\top$ 来测试，其中 $L$ 是一个下三角矩阵。当且仅当 $H_k$ 是 SPD 时，此分解才能成功。\n\n如果 $H_k$ 的 Cholesky 分解失败，则通过添加一个缩放的单位矩阵来修改海森矩阵，这是一种与 Levenberg-Marquardt 方法相关的技术。我们求解一个修正后的系统：\n$$\n(H_k + \\tau_k \\mathbf{I}) \\mathbf{d}_k = -\\mathbf{g}_k,\n$$\n其中 $\\mathbf{I}$ 是单位矩阵，$\\tau_k  0$ 是一个正则化参数。加上 $\\tau_k \\mathbf{I}$ 会使海森矩阵的特征值增加 $\\tau_k$。对于足够大的 $\\tau_k$，矩阵 $H_k + \\tau_k \\mathbf{I}$ 保证是正定的。该算法自适应地找到一个合适的 $\\tau_k$。它从一个较小的值 $\\tau_0 = 10^{-6}$ 开始，并尝试对 $H_k + \\tau_k \\mathbf{I}$ 进行 Cholesky 分解。如果失败，$\\tau_k$ 会按因子 $10$ 进行几何增长，直到分解成功。令 $\\hat{H}_k = H_k + \\tau_k \\mathbf{I}$ 为最终的、SPD 的修正海森矩阵。然后通过求解 $\\hat{H}_k \\mathbf{d}_k = -\\mathbf{g}_k$ 来计算搜索方向。作为对数值不稳定性的保障，算法必须通过检查 $\\mathbf{g}_k^\\top \\mathbf{d}_k  0$ 来验证计算出的 $\\mathbf{d}_k$ 确实是一个下降方向。如果此条件不成立，算法将退回到最基本的下降方向，即最速下降方向 $\\mathbf{d}_k = -\\mathbf{g}_k$。\n\n一旦获得合适的下降方向 $\\mathbf{d}_k$，必须选择一个步长 $\\alpha_k  0$ 以确保目标函数有充分下降并保证收敛。这通过强制执行 Armijo 条件的回溯线搜索来实现：\n$$\nf(\\mathbf{x}_k + \\alpha \\mathbf{d}_k) \\le f(\\mathbf{x}_k) + c \\alpha \\mathbf{g}_k^\\top \\mathbf{d}_k,\n$$\n对于一个常数 $c \\in (0,1)$，此处指定为 $c=10^{-4}$。线搜索从完整步长 $\\alpha = \\alpha_0 = 1$ 开始。如果不满足 Armijo 条件，则步长按因子 $\\rho = 0.5$ 缩减，即 $\\alpha \\leftarrow \\rho \\alpha$，并重复检查，直到条件成立。\n\n当满足以下任一标准时，整个迭代过程终止：\n1.  梯度范数足够小：$\\|\\mathbf{g}_k\\|_2 \\le \\varepsilon_g$，其中 $\\varepsilon_g = 10^{-8}$。这表明 $\\mathbf{x}_k$ 接近一个驻点。\n2.  步长变得可以忽略不计：$\\alpha_k \\|\\mathbf{d}_k\\|_2 \\le \\varepsilon_s$，其中 $\\varepsilon_s=10^{-12}$。这表明进一步的进展微乎其微。\n3.  达到最大迭代次数：$k \\ge N_{\\max}$，其中 $N_{\\max} = 200$。这可以防止无限循环。\n\n总而言之，算法在每次迭代 $k$ 中按以下步骤进行：\n1.  计算梯度 $\\mathbf{g}_k = \\nabla f(\\mathbf{x}_k)$ 和海森矩阵 $H_k = \\nabla^2 f(\\mathbf{x}_k)$。\n2.  基于 $\\|\\mathbf{g}_k\\|_2$ 检查终止条件。\n3.  找到一个合适的搜索方向 $\\mathbf{d}_k$：\n    a. 初始化 $\\tau = 0$。\n    b. 循环：尝试对 $H_k + \\tau \\mathbf{I}$ 进行 Cholesky 分解。\n    c. 如果成功，求解 $(H_k + \\tau \\mathbf{I}) \\mathbf{d}_k = -\\mathbf{g}_k$ 并跳出循环。\n    d. 如果不成功，记录需要进行修正。如果 $\\tau=0$，则设置 $\\tau = \\tau_0$，否则设置 $\\tau = 10\\tau$。重复循环。\n    e. 作为保障，如果 $\\mathbf{g}_k^\\top \\mathbf{d}_k \\ge 0$，则设置 $\\mathbf{d}_k = -\\mathbf{g}_k$。\n4.  执行回溯线搜索以找到满足 Armijo 条件的步长 $\\alpha_k$。\n5.  基于步长 $\\alpha_k \\|\\mathbf{d}_k\\|_2$ 检查终止条件。\n6.  更新迭代点：$\\mathbf{x}_{k+1} = \\mathbf{x}_k + \\alpha_k \\mathbf{d}_k$。\n7.  基于最大迭代次数检查终止条件。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the specified test cases through the modified Newton solver.\n    \"\"\"\n\n    # --- Algorithmic Parameters ---\n    PARAMS = {\n        'alpha0': 1.0,\n        'c': 1e-4,\n        'rho': 0.5,\n        'tau0': 1e-6,\n        'tau_factor': 10.0,\n        'eps_g': 1e-8,\n        'eps_s': 1e-12,\n        'N_max': 200,\n    }\n\n    def newton_modified(f, grad, hess, x0, params):\n        \"\"\"\n        Implements a modified Newton method for unconstrained optimization.\n        \"\"\"\n        x = np.array(x0, dtype=float)\n        hessian_modified_flag = 0\n\n        for k in range(params['N_max']):\n            fk = f(x)\n            gk = grad(x)\n\n            # Termination condition on gradient norm\n            if np.linalg.norm(gk) = params['eps_g']:\n                return x.tolist(), fk, k, hessian_modified_flag\n\n            Hk = hess(x)\n            tau = 0.0\n            \n            while True:\n                try:\n                    # Attempt Cholesky factorization\n                    H_mod = Hk\n                    if tau > 0:\n                        H_mod = Hk + tau * np.identity(len(x))\n                    \n                    L = np.linalg.cholesky(H_mod)\n                    \n                    # Solve (Hk + tau*I)d = -gk\n                    # 1. Solve L y = -gk\n                    y = np.linalg.solve(L, -gk)\n                    # 2. Solve L^T d = y\n                    dk = np.linalg.solve(L.T, y)\n                    break \n                except np.linalg.LinAlgError:\n                    # Cholesky factorization failed, matrix is not SPD\n                    hessian_modified_flag = 1\n                    if tau == 0.0:\n                        tau = params['tau0']\n                    else:\n                        tau *= params['tau_factor']\n\n            # Safeguard: ensure descent direction\n            if gk @ dk >= 0:\n                dk = -gk\n\n            # Backtracking line search with Armijo condition\n            alpha = params['alpha0']\n            while True:\n                x_new = x + alpha * dk\n                if f(x_new) = fk + params['c'] * alpha * (gk @ dk):\n                    break\n                alpha *= params['rho']\n                # Failsafe for alpha becoming too small, though step condition should handle it\n                if alpha  1e-16:\n                   break\n\n            # Termination condition on step size\n            if alpha * np.linalg.norm(dk) = params['eps_s']:\n                return x_new.tolist(), f(x_new), k + 1, hessian_modified_flag\n            \n            x = x_new\n        \n        # Reached max iterations\n        return x.tolist(), f(x), params['N_max'], hessian_modified_flag\n\n    # --- Test Cases ---\n    \n    # Case A: Strictly convex quadratic\n    Q_A = np.array([[4.0, 1.0], [1.0, 3.0]])\n    b_A = np.array([-1.0, 2.0])\n    f_A = lambda x: 0.5 * x.T @ Q_A @ x + b_A.T @ x\n    grad_A = lambda x: Q_A @ x + b_A\n    hess_A = lambda x: Q_A\n    x0_A = [2.0, -1.0]\n\n    # Case B: Himmelblau's function\n    f_B = lambda x: (x[0]**2 + x[1] - 11)**2 + (x[0] + x[1]**2 - 7)**2\n    grad_B = lambda x: np.array([\n        4*x[0]*(x[0]**2 + x[1] - 11) + 2*(x[0] + x[1]**2 - 7),\n        2*(x[0]**2 + x[1] - 11) + 4*x[1]*(x[0] + x[1]**2 - 7)\n    ])\n    hess_B = lambda x: np.array([\n        [12*x[0]**2 + 4*x[1] - 42, 4*x[0] + 4*x[1]],\n        [4*x[0] + 4*x[1], 12*x[1]**2 + 4*x[0] - 26]\n    ])\n    x0_B = [0.0, 0.0]\n\n    # Case C: Saddle point function\n    f_C = lambda x: x[0]**2 - x[1]**2 + 0.1*x[0]**4 + 0.1*x[1]**4\n    grad_C = lambda x: np.array([2*x[0] + 0.4*x[0]**3, -2*x[1] + 0.4*x[1]**3])\n    hess_C = lambda x: np.array([\n        [2 + 1.2*x[0]**2, 0.0],\n        [0.0, -2 + 1.2*x[1]**2]\n    ])\n    x0_C = [0.1, 0.1]\n\n    # Case D: Flat function with singular Hessian at minimizer\n    f_D = lambda x: (x[0]**2 + x[1]**2)**2\n    grad_D = lambda x: np.array([\n        4*x[0]*(x[0]**2 + x[1]**2),\n        4*x[1]*(x[0]**2 + x[1]**2)\n    ])\n    hess_D = lambda x: np.array([\n        [12*x[0]**2 + 4*x[1]**2, 8*x[0]*x[1]],\n        [8*x[0]*x[1], 12*x[1]**2 + 4*x[0]**2]\n    ])\n    x0_D = [1e-3, -1e-3]\n    \n    test_cases = [\n        (f_A, grad_A, hess_A, x0_A),\n        (f_B, grad_B, hess_B, x0_B),\n        (f_C, grad_C, hess_C, x0_C),\n        (f_D, grad_D, hess_D, x0_D),\n    ]\n\n    all_results = []\n    for f, grad, hess, x0 in test_cases:\n        x_star_coords, f_star, iters, mod_flag = newton_modified(f, grad, hess, x0, PARAMS)\n        result_list = x_star_coords + [f_star, iters, mod_flag]\n        all_results.append(result_list)\n        \n    # Format the output string without any spaces.\n    def format_list(lst):\n        s = []\n        for item in lst:\n            if isinstance(item, list):\n                s.append(format_list(item))\n            elif isinstance(item, float):\n                # Use a format that avoids scientific notation for small numbers if possible\n                # and prints enough precision, but string conversion handles this well.\n                s.append(str(item))\n            else:\n                s.append(str(item))\n        return '[' + ','.join(s) + ']'\n        \n    output_str = format_list(all_results).replace(\" \", \"\")\n    print(output_str)\n\nsolve()\n```", "id": "3255780"}, {"introduction": "大多数优化算法都建立在目标函数光滑（即可微）的假设之上。本练习将探索当这一核心假设被打破时会发生什么。你将研究一个基于梯度的下降方法，在一个连续但不可微的函数上，如何因“尖点”的存在而计算出误导性的数值梯度，最终导致算法停滞在离真正最小值很远的地方。[@problem_id:3285108]", "problem": "你需要构建并分析一个一维、连续但不可微的无约束优化问题。考虑为实数 $x$ 定义的函数\n$$\nf(x) = |x| - \\beta \\exp\\!\\big(-(x-\\mu)^2\\big),\n$$\n其中 $\\beta$ 和 $\\mu$ 是给定的常数。函数 $f(x)$ 对所有实数 $x$ 都是连续的，但由于绝对值项 $|x|$ 的存在，它在 $x = 0$ 处不可微。众所周知，这种不可微性会给假定函数可微的基于梯度的方法带来问题。\n\n从基本定义出发，函数 $f(x)$ 在点 $x$ 处的导数由以下极限定义\n$$\nf'(x) = \\lim_{h \\to 0} \\frac{f(x+h) - f(x)}{h},\n$$\n前提是该极限存在。当 $f(x)$ 在某点不可微时，依赖此极限的基于梯度的方法可能是不适定的或产生误导性的数值梯度。在实践中，数值方法通常使用中心有限差分这一经过充分检验的公式来近似导数：\n$$\ng(x;h) \\approx \\frac{f(x+h) - f(x-h)}{2h},\n$$\n其中 $h$ 是一个很小的步长。\n\n你的任务是：\n- 实现一个朴素梯度下降法，该方法使用中心有限差分导数近似值 $g(x;h)$ 来更新迭代点。更新规则为\n$$\nx_{k+1} = x_k - \\alpha \\, g(x_k;h),\n$$\n其中 $\\alpha$ 是步长。\n- 使用以下停止准则\n$$\n|g(x_k;h)|  \\tau \\quad \\text{或} \\quad k \\geq N,\n$$\n其中 $\\tau$ 是梯度容差，$N$ 是最大迭代次数。\n\n将常数设置为 $\\beta = 4$ 和 $\\mu = 3$，这使得函数连续，在 $x=0$ 处有一个不可微点，并在 $x \\approx \\mu$ 附近有一个平滑的深井。你必须使用两阶段搜索方法，在有界区间上计算 $f(x)$ 的全局最小化子 $x^\\star$ 的数值真值：\n- 在 $[a,b] = [-2,6]$ 上以均匀步长 $\\Delta_c = 10^{-2}$ 进行粗略搜索，以找到一个粗略的最小化子 $x_c$，\n- 在局部邻域 $[x_c - r, x_c + r]$（其中 $r = 5 \\times 10^{-2}$）内以步长 $\\Delta_f = 10^{-4}$ 进行精细搜索，以找到 $x^\\star$ 和 $f(x^\\star)$。\n\n针对以下参数值测试套件运行梯度下降法，这些测试涵盖了在不可微点处的失败、对容差的敏感性以及典型情况：\n- 测试 $1$：$x_0 = 0.0$，$\\alpha = 0.1$，$h = 10^{-6}$，$\\tau = 10^{-2}$，$N = 200$。\n- 测试 $2$：$x_0 = -0.1$，$\\alpha = 0.2$，$h = 10^{-6}$，$\\tau = 10^{-2}$，$N = 200$。\n- 测试 $3$：$x_0 = 3.0$，$\\alpha = 0.2$，$h = 10^{-6}$，$\\tau = 10^{-6}$，$N = 200$。\n- 测试 $4$：$x_0 = -1.0$，$\\alpha = 1.0$，$h = 10^{-6}$，$\\tau = 10^{-6}$，$N = 50$。\n\n对于每次测试，通过检查最终迭代点 $x_{\\text{final}}$ 是否在计算出的全局最小化子 $x^\\star$ 的一个容差范围内来确定一个布尔成功指示符：\n$$\n\\text{success} = \\big(|x_{\\text{final}} - x^\\star| \\leq 10^{-2}\\big).\n$$\n\n你的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果，例如 $[r_1,r_2,r_3,r_4]$，其中每个 $r_i$ 是一个布尔值，表示第 $i$ 个测试用例是否成功。\n\n此问题不涉及物理单位或角度。\n\n通过使用连续性和可微性的定义、标准的有限差分近似以及所描述的明确定义的梯度下降算法和停止准则来确保科学真实性。避免走捷径：算法必须明确实现所述的数值导数和停止规则，并且必须通过上述的两阶段搜索来计算真值。输出必须严格遵循指定的格式。", "solution": "用户提供了一个定义明确的数值优化问题。验证过程确认该问题具有科学依据、数学上一致且规范完整。因此，将提供一个解决方案。\n\n目标是分析一个应用于连续但不可微函数的朴素梯度下降算法。要最小化的函数由下式给出：\n$$\nf(x) = |x| - \\beta \\exp\\!\\big(-(x-\\mu)^2\\big)\n$$\n常数设置为 $\\beta = 4$ 和 $\\mu = 3$。由于绝对值项 $|x|$ 的存在，该函数在 $x = 0$ 处有一个尖点（不可微点），并在 $x = \\mu = 3$ 附近有一个平滑的深最小值。\n\n所规定的梯度下降法使用梯度的数值近似，因为在 $x=0$ 处真导数不存在。采用中心有限差分公式来计算该数值梯度：\n$$\ng(x;h) = \\frac{f(x+h) - f(x-h)}{2h}\n$$\n其中 $h$ 是一个很小的步长。在第 $k$ 次迭代中，位置 $x_k$ 的迭代更新规则是：\n$$\nx_{k+1} = x_k - \\alpha \\, g(x_k;h)\n$$\n其中 $\\alpha$ 是学习率或步长。当数值梯度的绝对值小于容差 $\\tau$ 或达到最大迭代次数 $N$ 时，算法终止。\n\n数值梯度 $g(x;h)$ 在不可微点 $x=0$ 处的行为至关重要。让我们来计算它：\n$$\ng(0;h) = \\frac{f(h) - f(-h)}{2h} = \\frac{\\left( |h| - \\beta e^{-(h-\\mu)^2} \\right) - \\left( |-h| - \\beta e^{-(-h-\\mu)^2} \\right)}{2h}\n$$\n对于一个小的正数 $h$，$|h| = h$ 且 $|-h| = h$。表达式简化为：\n$$\ng(0;h) = \\frac{h - \\beta e^{-(h-\\mu)^2} - h + \\beta e^{-(h+\\mu)^2}}{2h} = \\frac{\\beta}{2h} \\left( e^{-(h+\\mu)^2} - e^{-(h-\\mu)^2} \\right)\n$$\n对于小的 $y$，使用泰勒展开 $e^y \\approx 1+y$，并注意到当 $h \\to 0$ 时，$(h\\pm\\mu)^2 \\approx \\mu^2 \\pm 2\\mu h$，我们得到：\n$$\ne^{-(\\mu \\pm h)^2} \\approx e^{-(\\mu^2 \\pm 2\\mu h)} = e^{-\\mu^2} e^{\\mp 2\\mu h} \\approx e^{-\\mu^2} (1 \\mp 2\\mu h)\n$$\n将此代入 $g(0;h)$ 的表达式中：\n$$\ng(0;h) \\approx \\frac{\\beta e^{-\\mu^2}}{2h} \\left( (1-2\\mu h) - (1+2\\mu h) \\right) = \\frac{\\beta e^{-\\mu^2}}{2h}(-4\\mu h) = -2\\beta\\mu e^{-\\mu^2}\n$$\n对于 $\\beta=4$ 和 $\\mu=3$，这给出 $g(0;h) \\approx -2(4)(3)e^{-9} = -24e^{-9} \\approx -0.00296$。在 $x=0$ 处的这个有限非零的数值梯度值决定了算法在遇到该点时的行为，如果梯度的绝对值小于容差 $\\tau$，可能会导致其“卡住”，否则会越过该点。\n\n为了评估优化的成功与否，必须首先确定全局最小化子 $x^\\star$ 的数值真值。这通过指定的两阶段网格搜索来完成：\n1.  **粗略搜索**：在区间 $[-2, 6]$ 上以 $\\Delta_c = 10^{-2}$ 的步长评估函数 $f(x)$。在此粗略网格上产生 $f(x)$ 最小值的点被识别为 $x_c$。\n2.  **精细搜索**：在 $x_c$ 周围的一个小邻域，即区间 $[x_c - 5 \\times 10^{-2}, x_c + 5 \\times 10^{-2}]$ 内，以更精细的步长 $\\Delta_f = 10^{-4}$ 进行搜索。此精细网格上的最小化子被视为真值全局最小化子 $x^\\star$。\n\n然后对四个不同的测试用例执行梯度下降算法。对于每个用例，将最终迭代点 $x_{\\text{final}}$ 与真值 $x^\\star$进行比较。如果 $|x_{\\text{final}} - x^\\star| \\leq 10^{-2}$，则宣布成功。\n\n每个测试用例的分析如下：\n-   **测试 1**：$x_0 = 0.0$，$\\tau = 10^{-2}$。算法从不可微点开始。数值梯度 $|g(0;h)| \\approx 0.00296$，小于容差 $\\tau = 0.01$。停止准则立即满足，算法终止于 $x_{\\text{final}} \\approx 0.0$。这与真实最小化子 $x^\\star \\approx 2.87$ 相差甚远，因此预期此情况会失败。\n-   **测试 2**：$x_0 = -0.1$，$\\alpha = 0.2$，$\\tau = 10^{-2}$。从尖点附近开始，算法首先跨过 $x=0$ 步入正域。从那里开始，梯度持续将迭代点引向 $x=3$ 附近的深井。在合理的步长下，预期它会收敛到全局最小值的盆地，从而成功。\n-   **测试 3**：$x_0 = 3.0$，$\\tau = 10^{-6}$。起始点非常接近平滑最小值的位置。算法将精化该位置，并且在严格的容差下，它将执行多次迭代以找到一个数值梯度非常小的点。预期它能准确找到 $x^\\star$ 并成功。\n-   **测试 4**：$x_0 = -1.0$，$\\alpha = 1.0$。大步长 $\\alpha=1.0$ 是关键特征。第一步将迭代点从 $x_0=-1.0$ 移动到 $x_1 \\approx 0.0$。下一步将其移动到一个小的正值，$x_2 \\approx 0.00296$。随后的步骤，在梯度约为 $1.0$ 的情况下，将其移动到 $x_3 \\approx 0.00296 - 1.0 = -0.997$。算法在 $x=0$ 附近进入振荡，无法逃逸至全局最小值。它将在达到最大迭代次数后终止，远离 $x^\\star$，预期会失败。\n\n最终输出将是一个布尔值列表，对应于每个测试用例的成功或失败。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the unconstrained optimization problem as specified.\n    \"\"\"\n    # Define constants from the problem statement\n    BETA = 4.0\n    MU = 3.0\n\n    # Define the objective function f(x)\n    def f(x):\n        \"\"\"\n        The function to be minimized: f(x) = |x| - beta * exp(-(x-mu)^2).\n        \"\"\"\n        return np.abs(x) - BETA * np.exp(-(x - MU)**2)\n\n    # Step 1: Compute the ground truth global minimizer x_star\n    # Coarse search\n    coarse_grid = np.arange(-2.0, 6.0, 1e-2)\n    coarse_values = f(coarse_grid)\n    x_c = coarse_grid[np.argmin(coarse_values)]\n\n    # Fine search\n    r = 5e-2\n    fine_grid = np.arange(x_c - r, x_c + r, 1e-4)\n    fine_values = f(fine_grid)\n    x_star = fine_grid[np.argmin(fine_values)]\n\n    # Step 2: Implement the gradient descent algorithm\n    def gradient_descent(x0, alpha, h, tau, N):\n        \"\"\"\n        Performs naive gradient descent using a central finite difference\n        approximation for the gradient.\n        \"\"\"\n        x_k = x0\n\n        # Central finite difference gradient approximation\n        def g(x, h_val):\n            return (f(x + h_val) - f(x - h_val)) / (2 * h_val)\n\n        for _ in range(N):\n            grad = g(x_k, h)\n            if np.abs(grad)  tau:\n                break\n            x_k = x_k - alpha * grad\n        \n        return x_k\n\n    # Step 3: Run the test suite and evaluate success\n    test_cases = [\n        # (x0, alpha, h, tau, N)\n        (0.0, 0.1, 1e-6, 1e-2, 200),\n        (-0.1, 0.2, 1e-6, 1e-2, 200),\n        (3.0, 0.2, 1e-6, 1e-6, 200),\n        (-1.0, 1.0, 1e-6, 1e-6, 50),\n    ]\n\n    results = []\n    success_tolerance = 1e-2\n\n    for case in test_cases:\n        x0, alpha, h, tau, N = case\n        x_final = gradient_descent(x0, alpha, h, tau, N)\n        \n        # Determine success based on the specified criterion\n        success = np.abs(x_final - x_star) = success_tolerance\n        results.append(success)\n\n    # Step 4: Print the results in the specified format\n    # The default str() for a boolean is 'True' or 'False'.\n    # This matches the general description of a boolean list.\n    print(f\"[{','.join(map(lambda b: str(b).lower(), results))}]\")\n\n# Execute the main function\nsolve()\n```", "id": "3285108"}]}