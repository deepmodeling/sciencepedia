{"hands_on_practices": [{"introduction": "在数值优化中，最简单也最广泛应用的线搜索策略是依赖于 Armijo 准则的回溯法。然而，理论上的算法与能够稳定运行的代码之间存在差距。本练习 [@problem_id:3247812] 将指导你从零开始构建一个回溯线搜索算法，并特别强调代码的鲁棒性，确保它能妥善处理在实际问题中因定义域限制而经常出现的非有限函数值。", "problem": "要求您实现一个稳健的回溯线搜索算法，以选择用于最小化可微目标函数的步长。当目标函数在试验点返回非有限值时，该实现必须具有弹性。您的程序必须是一个完整的、可运行的程序，执行以下任务。\n\n给定一个可微函数 $f : \\mathbb{R}^n \\to \\mathbb{R}$ 及其梯度 $\\nabla f$、一个当前点 $\\mathbf{x} \\in \\mathbb{R}^n$ 和一个候选搜索方向 $\\mathbf{p} \\in \\mathbb{R}^n$，实现一个回溯线搜索，尝试找到满足充分下降（Armijo）条件的步长 $\\alpha \\in \\mathbb{R}_{0}$\n$$\nf(\\mathbf{x} + \\alpha \\mathbf{p}) \\le f(\\mathbf{x}) + c_1 \\alpha \\, \\nabla f(\\mathbf{x})^\\top \\mathbf{p},\n$$\n其中 $c_1 \\in (0,1)$ 是一个固定常数。回溯方案从初始步长 $\\alpha_0 \\in \\mathbb{R}_{0}$ 开始，并以因子 $\\tau \\in (0,1)$ 对其进行乘法缩减，直到满足 Armijo 条件或达到最大缩减次数 $N_{\\max} \\in \\mathbb{N}$。如果 $\\nabla f(\\mathbf{x})^\\top \\mathbf{p} \\ge 0$，您的实现必须立即返回 $\\alpha = 0$，因为 $\\mathbf{p}$ 不是一个下降方向。在搜索过程中，如果 $f(\\mathbf{x} + \\alpha \\mathbf{p})$ 的计算结果为非有限值（不是实数），例如非数值或无穷大，您的方法必须将此试验视为不可接受，并通过设置 $\\alpha \\leftarrow \\tau \\alpha$ 继续回溯。\n\n您的实现必须对所有测试用例使用以下固定参数：初始步长 $\\alpha_0 = 1$，缩减因子 $\\tau = 0.5$，Armijo 参数 $c_1 = 10^{-4}$，以及最大回溯步数 $N_{\\max} = 50$。如果在 $N_{\\max}$ 次缩减后没有找到可接受的步长，则返回 $\\alpha = 0$。\n\n您的程序必须运行以下测试套件，其中涵盖了一般情况和边界情况。对于向量参数，请在欧几里得意义上解释运算。\n\n- 测试用例 1（理想情况，最速下降法的二次函数）：$f(\\mathbf{x}) = \\tfrac{1}{2}\\|\\mathbf{x}\\|_2^2$，$\\nabla f(\\mathbf{x}) = \\mathbf{x}$，$\\mathbf{x} = [1,-2]^\\top$，$\\mathbf{p} = -\\nabla f(\\mathbf{x})$，报告找到的 $\\alpha$。\n- 测试用例 2（定义域限制导致非有限试验，标量）：$f(x) = \\log(x)$，定义域为 $x \\in \\mathbb{R}_{0}$，$\\nabla f(x) = 1/x$，$x = 0.1$，$p = -\\nabla f(x)$，报告找到的 $\\alpha$。\n- 测试用例 3（混合定义域限制，二维）：$f(\\mathbf{x}) = \\log(x_1) + x_2^2$，定义域为 $x_1 \\in \\mathbb{R}_{0}$，$\\nabla f(\\mathbf{x}) = [1/x_1,\\; 2 x_2]^\\top$，$\\mathbf{x} = [0.2, 10]^\\top$，$\\mathbf{p} = -\\nabla f(\\mathbf{x})$，报告找到的 $\\alpha$。\n- 测试用例 4（非下降方向）：$f(\\mathbf{x}) = \\tfrac{1}{2}\\|\\mathbf{x}\\|_2^2$，$\\nabla f(\\mathbf{x}) = \\mathbf{x}$，$\\mathbf{x} = [1,1]^\\top$，$\\mathbf{p} = [1,0]^\\top$，报告找到的 $\\alpha$。\n- 测试用例 5（与梯度正交的方向，方向导数为零）：$f(\\mathbf{x}) = \\tfrac{1}{2}\\|\\mathbf{x}\\|_2^2$，$\\nabla f(\\mathbf{x}) = \\mathbf{x}$，$\\mathbf{x} = [1,0]^\\top$，$\\mathbf{p} = [0,1]^\\top$，报告找到的 $\\alpha$。\n\n所有计算都是纯数学的；不涉及物理单位或角度。\n\n最终输出格式：您的程序应生成一行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果，按上述测试的顺序排列，例如 $[r_1,r_2,r_3,r_4,r_5]$，其中每个 $r_i$ 是您的线搜索为测试用例 $i$ 返回的数值步长 $\\alpha$。", "solution": "问题陈述已经过分析并被确定为有效。这是一个数值优化领域中定义明确的问题，为实现和测试回溯线搜索算法提供了清晰、完整且数学上合理的规范。所有必要的函数、参数和测试用例都已提供，并且要求是客观和明确的。\n\n核心任务是实现一个回溯线搜索算法，为优化算法找到一个合适的步长 $\\alpha$。搜索 $\\alpha$ 的目的是找到一个点 $\\mathbf{x} + \\alpha \\mathbf{p}$，当从点 $\\mathbf{x}$ 沿搜索方向 $\\mathbf{p}$ 移动时，该点能使目标函数 $f$ 得到充分的下降。\n\n搜索方向 $\\mathbf{p}$ 必须是一个下降方向，这意味着它必须与负梯度 $-\\nabla f(\\mathbf{x})$ 形成一个锐角。在数学上，这个条件表示为负的方向导数：$\\nabla f(\\mathbf{x})^\\top \\mathbf{p}  0$。如果不满足此条件，即 $\\nabla f(\\mathbf{x})^\\top \\mathbf{p} \\ge 0$，则即使对于无穷小的步长，也不能保证函数值会下降，算法必须返回步长 $\\alpha = 0$。\n\n可接受步长的标准是充分下降条件，也称为 Armijo 条件：\n$$\nf(\\mathbf{x} + \\alpha \\mathbf{p}) \\le f(\\mathbf{x}) + c_1 \\alpha \\, \\nabla f(\\mathbf{x})^\\top \\mathbf{p}\n$$\n这里，$c_1$ 是区间 $(0, 1)$ 内的常数，通常是一个小数，如指定的 $c_1 = 10^{-4}$。这个条件确保了函数的实际减少量 $f(\\mathbf{x}) - f(\\mathbf{x} + \\alpha \\mathbf{p})$ 至少是线性预测减少量 $-\\alpha \\, \\nabla f(\\mathbf{x})^\\top \\mathbf{p}$ 的一部分。\n\n回溯算法是一个迭代过程，用于找到满足此条件的 $\\alpha$。其过程如下：\n\n1.  **初始化**：给定一个起始点 $\\mathbf{x}$、一个搜索方向 $\\mathbf{p}$、函数 $f$ 及其梯度 $\\nabla f$。为当前搜索预先计算常数值：$f_x = f(\\mathbf{x})$ 和方向导数 $g_p = \\nabla f(\\mathbf{x})^\\top \\mathbf{p}$。\n\n2.  **方向检查**：立即验证 $\\mathbf{p}$ 是否为下降方向。如果 $g_p \\ge 0$，返回 $\\alpha=0$ 并终止。\n\n3.  **迭代搜索**：\n    a. 从一个初始试验步长开始，$\\alpha \\leftarrow \\alpha_0$。根据规定，$\\alpha_0 = 1$。\n    b. 在一个最多运行 $N_{\\max} = 50$ 次迭代的循环中：\n        i.  构造一个试验点 $\\mathbf{x}_{\\text{new}} = \\mathbf{x} + \\alpha \\mathbf{p}$。\n        ii. 在该点计算目标函数的值，$f_{\\text{new}} = f(\\mathbf{x}_{\\text{new}})$。\n        iii. 问题要求对非有限值进行稳健处理。如果 $f_{\\text{new}}$ 不是一个有限实数（例如 `inf`、`-inf` 或 `NaN`），则该试验步长无效。Armijo 条件被视为不满足。\n        iv. 如果 $f_{\\text{new}}$ 是有限的，检查是否满足 Armijo 条件 $f_{\\text{new}} \\le f_x + c_1 \\alpha g_p$。\n        v. 如果条件满足，$\\alpha$ 就是一个可接受的步长。搜索成功终止，返回此 $\\alpha$ 值。\n        vi. 如果条件不满足（由于不满足不等式或 $f_{\\text{new}}$ 为非有限值），则缩减步长（回溯）。新的步长变为 $\\alpha \\leftarrow \\tau \\alpha$，其中 $\\tau = 0.5$ 是缩减因子。然后循环继续使用这个较小的 $\\alpha$。\n\n4.  **失败终止**：如果在 $N_{\\max}$ 次缩减后循环结束仍未找到合适的 $\\alpha$，则线搜索失败。根据规定，算法必须返回 $\\alpha = 0$。\n\n该算法将被实现并应用于提供的五个测试用例，这些测试用例旨在验证其在各种情况下的正确性：一个标准的凸问题、具有导致非有限函数值的定义域约束的问题，以及涉及非下降或正交搜索方向的情况。", "answer": "```python\nimport numpy as np\n\ndef backtracking_line_search(f, grad_f, x, p):\n    \"\"\"\n    Performs a backtracking line search to find a step size alpha satisfying\n    the Armijo (sufficient decrease) condition.\n\n    Args:\n        f (callable): The objective function f(x).\n        grad_f (callable): The gradient of the objective function, grad_f(x).\n        x (np.ndarray): The current point.\n        p (np.ndarray): The search direction.\n\n    Returns:\n        float: The accepted step size alpha.\n    \"\"\"\n    # Fixed parameters as specified in the problem statement\n    alpha_0 = 1.0\n    tau = 0.5\n    c1 = 1e-4\n    n_max = 50\n\n    # Ensure x and p are numpy arrays for consistent vector operations\n    x = np.asarray(x, dtype=float)\n    p = np.asarray(p, dtype=float)\n\n    # Step 0: Pre-computation and Initial Check\n    # A robust implementation would also check if f(x) is finite, but we assume it\n    # is for the given test cases.\n    fx = f(x)\n    grad_x = grad_f(x)\n    gp = np.dot(grad_x, p)\n\n    # If the direction is not a descent direction, return alpha = 0.\n    if gp >= 0:\n        return 0.0\n\n    # Step 1: Initialization\n    alpha = alpha_0\n    \n    # Step 2: Iteration Loop\n    for _ in range(n_max):\n        x_new = x + alpha * p\n        f_new = f(x_new)\n        \n        # Check for non-finite values and the Armijo condition\n        if np.isfinite(f_new) and f_new = fx + c1 * alpha * gp:\n            return alpha\n        \n        # Backtrack: reduce alpha\n        alpha *= tau\n        \n    # Step 3: Termination (Failure)\n    # If loop finishes, no suitable alpha was found\n    return 0.0\n\ndef solve():\n    \"\"\"\n    Sets up and runs the test suite for the backtracking line search.\n    \"\"\"\n    \n    # Suppress runtime warnings from numpy.log for non-positive inputs\n    # The algorithm is designed to handle the resulting nan/inf values.\n    # We use a context manager to keep this suppression local.\n    with np.errstate(invalid='ignore', divide='ignore'):\n        \n        # --- Test Case 1: Happy path, quadratic with steepest descent ---\n        f1 = lambda x: 0.5 * np.linalg.norm(x)**2\n        grad_f1 = lambda x: x\n        x1 = np.array([1.0, -2.0])\n        p1 = -grad_f1(x1)\n        alpha1 = backtracking_line_search(f1, grad_f1, x1, p1)\n\n        # --- Test Case 2: Domain restriction causing non-finite trials ---\n        # Note: We represent scalar problems with 1-element arrays for consistency.\n        f2 = lambda x: np.log(x[0]) if x[0] > 0 else -np.inf\n        grad_f2 = lambda x: np.array([1.0 / x[0]])\n        x2 = np.array([0.1])\n        p2 = -grad_f2(x2)\n        alpha2 = backtracking_line_search(f2, grad_f2, x2, p2)\n        \n        # --- Test Case 3: Mixed domain restriction, two dimensions ---\n        f3 = lambda x: np.log(x[0]) + x[1]**2 if x[0] > 0 else -np.inf\n        grad_f3 = lambda x: np.array([1.0 / x[0], 2.0 * x[1]])\n        x3 = np.array([0.2, 10.0])\n        p3 = -grad_f3(x3)\n        alpha3 = backtracking_line_search(f3, grad_f3, x3, p3)\n        \n        # --- Test Case 4: Non-descent direction ---\n        f4 = lambda x: 0.5 * np.linalg.norm(x)**2\n        grad_f4 = lambda x: x\n        x4 = np.array([1.0, 1.0])\n        p4 = np.array([1.0, 0.0])\n        alpha4 = backtracking_line_search(f4, grad_f4, x4, p4)\n\n        # --- Test Case 5: Direction orthogonal to the gradient ---\n        f5 = lambda x: 0.5 * np.linalg.norm(x)**2\n        grad_f5 = lambda x: x\n        x5 = np.array([1.0, 0.0])\n        p5 = np.array([0.0, 1.0])\n        alpha5 = backtracking_line_search(f5, grad_f5, x5, p5)\n        \n    results = [alpha1, alpha2, alpha3, alpha4, alpha5]\n    \n    # Final print statement in the exact required format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3247812"}, {"introduction": "尽管标准的回溯法很稳健，但它有时可能过于保守，导致步长过小，收敛缓慢。一个自然而然的改进思路便是对算法进行增强。本练习 [@problem_id:3247771] 介绍了一种常见的启发式策略：“扩张阶段”。该策略允许算法在初始步长被接受时，尝试探索更大、更具挑战性的步长，从而有可能加速整个优化过程。", "problem": "实现一个改进的线性搜索方法，它通过一个由充分下降（Armijo）条件控制的扩展阶段来增强经典的回溯法。从基本原理出发：对于一个连续可微函数 $f:\\mathbb{R}^n\\to\\mathbb{R}$ 和在点 $x$ 处的一个下降方向 $p$，如果步长 $\\alpha0$ 满足以下条件，则该步长是可接受的：\n$$\nf(x+\\alpha p)\\;\\le\\; f(x) + c_1\\,\\alpha\\,\\nabla f(x)^\\top p,\n$$\n其中 $c_1\\in(0,1)$ 且 $\\nabla f(x)^\\top p  0$。改进的线性搜索过程如下：\n- 如果初始步长 $\\alpha_0$ 满足 Armijo 条件，则尝试进入一个扩展阶段，通过重复尝试 $\\gamma\\alpha_0,\\;\\gamma^2\\alpha_0,\\dots$ 直到 Armijo 条件不再满足，并返回最后一个被接受的步长。\n- 否则，执行标准回溯，通过重复尝试 $\\beta\\alpha_0,\\;\\beta^2\\alpha_0,\\dots$ 直到满足 Armijo 条件，并返回第一个被接受的步长。\n- 如果 $\\nabla f(x)^\\top p\\ge 0$，则将 $p$ 视为非下降方向，并立即返回 $\\alpha=0$，不沿 $p$ 方向计算 $f$ 的值。\n\n仅使用梯度、方向导数和充分下降（Armijo）条件的基本定义。不要引用任何未经证明的捷径。为确保科学真实性，请在 $\\mathbb{R}^n$ 上使用二阶连续可微的测试函数，并在标准欧几里得内积中将所有向量视为列向量。\n\n你的程序必须：\n- 实现这个具有参数 $\\alpha_0$、$c_1$、$\\beta$、$\\gamma$、最小允许步长 $\\alpha_{\\min}$、最大回溯迭代次数 $N_{\\text{bt}}$ 和最大扩展步数 $N_{\\text{exp}}$ 的改进线性搜索方法。\n- 对于每个测试用例，计算并返回所选的步长 $\\alpha$。\n- 如果回溯在 $\\alpha\\alpha_{\\min}$ 或超过 $N_{\\text{bt}}$ 次迭代之前未能找到可接受的步长，则返回最后尝试的 $\\alpha$。\n- 在最终输出中，将每个返回的 $\\alpha$ 四舍五入到10位小数。\n\n用于评估不同行为的测试套件：\n- 情况 A（在一维移位二次函数上触发回溯）：\n  - $f(x) = (x-3)^2$，$\\nabla f(x) = 2(x-3)$。\n  - $x = [0]$，$p = -\\nabla f(x)$。\n  - 参数：$\\alpha_0 = 1$，$c_1 = 10^{-4}$，$\\beta = 0.5$，$\\gamma = 2$，$\\alpha_{\\min} = 10^{-16}$，$N_{\\text{bt}}=100$，$N_{\\text{exp}}=50$。\n- 情况 B（初始步长被接受，扩展阶段增长到以原点为中心的一维二次函数上的最大可接受步长）：\n  - $f(x) = \\tfrac{1}{2}x^2$，$\\nabla f(x) = x$。\n  - $x = [1]$，$p = -\\nabla f(x)$。\n  - 参数：$\\alpha_0 = 0.25$，$c_1 = 10^{-4}$，$\\beta = 0.5$，$\\gamma = 2$，$\\alpha_{\\min} = 10^{-16}$，$N_{\\text{bt}}=100$，$N_{\\text{exp}}=50$。\n- 情况 C（需要多次回溯步骤的病态二维二次函数）：\n  - $f(x) = \\tfrac{1}{2}x^\\top Q x$，其中 $Q=\\mathrm{diag}(1,100)$，$\\nabla f(x) = Qx$。\n  - $x = [1,1]^\\top$，$p = -\\nabla f(x)$。\n  - 参数：$\\alpha_0 = 1$，$c_1 = 10^{-4}$，$\\beta = 0.5$，$\\gamma = 2$，$\\alpha_{\\min} = 10^{-16}$，$N_{\\text{bt}}=100$，$N_{\\text{exp}}=50$。\n- 情况 D（非下降方向的边界情况）：\n  - $f(x) = \\tfrac{1}{2}x^\\top x$，$\\nabla f(x) = x$。\n  - $x = [1,0]^\\top$，$p = [1,0]^\\top$ 以至于 $\\nabla f(x)^\\top p \\ge 0$。\n  - 参数：$\\alpha_0 = 1$，$c_1 = 10^{-4}$，$\\beta = 0.5$，$\\gamma = 2$，$\\alpha_{\\min} = 10^{-16}$，$N_{\\text{bt}}=100$，$N_{\\text{exp}}=50$。\n\n最终输出格式：\n- 你的程序应生成一行输出，其中包含按 A、B、C、D 顺序排列的所选步长，形式为用方括号括起来的逗号分隔列表。每个条目必须四舍五入到10位小数。例如，输出必须类似于 $[\\alpha_A,\\alpha_B,\\alpha_C,\\alpha_D]$，不含空格，其中每个 $\\alpha_\\cdot$ 都精确打印到小数点后10位。", "solution": "任务是实现一个改进的线性搜索算法，用于在数值优化中寻找合适的步长 $\\alpha$。该算法在标准的回溯线性搜索基础上增加了一个扩展阶段，而选择哪个阶段则取决于初始步长 $\\alpha_0$ 是否满足充分下降条件。整个过程由充分下降（Armijo）条件控制。\n\n### 基础：充分下降（Armijo）条件\n\n对于一个连续可微的函数 $f: \\mathbb{R}^n \\to \\mathbb{R}$、一个点 $x \\in \\mathbb{R}^n$ 和一个搜索方向 $p \\in \\mathbb{R}^n$，Armijo 条件指出，如果一个步长 $\\alpha > 0$ 能够使函数值产生“充分下降”，那么它就是可接受的。该条件的形式化表达为：\n$$\nf(x + \\alpha p) \\le f(x) + c_1 \\alpha \\nabla f(x)^\\top p\n$$\n这里，$c_1$ 是区间 $(0, 1)$ 内的一个常数，通常是一个很小的值，如 $10^{-4}$。项 $\\nabla f(x)^\\top p$ 代表函数 $f$ 在点 $x$ 沿方向 $p$ 的方向导数。为了使线性搜索有意义，$p$ 必须是一个下降方向，这意味着从 $x$ 沿 $p$ 方向移动应能局部地减小函数值。这通过方向导数为负来保证：$\\nabla f(x)^\\top p  0$。不等式的右侧定义了一条关于 $\\alpha$ 的直线，它从 $f(x)$（当 $\\alpha=0$ 时）开始，斜率为 $c_1 \\nabla f(x)^\\top p$。由于 $c_1 \\in (0,1)$ 且 $\\nabla f(x)^\\top p  0$，这条直线的负斜率比 $\\alpha=0$ 处的切线（其斜率为 $\\nabla f(x)^\\top p$）更平缓。该条件要求新的函数值 $f(x+\\alpha p)$ 位于这条直线上或其下方，这样既能保证 $f$ 值有不可忽略的减小，又防止了过小的步长被接受。\n\n### 改进的线性搜索算法\n\n所提出的算法遵循一个结构化的两阶段方法。\n\n1.  **下降方向检查**：算法首先验证 $p$ 是否为一个有效的下降方向。它计算方向导数 $\\nabla f(x)^\\top p$。如果 $\\nabla f(x)^\\top p \\ge 0$，$p$ 就不是一个下降方向（或与梯度正交，无法提供下降）。在这种情况下，沿 $p$ 方向无法取得进展，算法立即终止，返回步长 $\\alpha = 0$。\n\n2.  **初始步长评估**：如果 $p$ 是一个下降方向，算法会测试一个初始试验步长 $\\alpha_0 > 0$。此测试的结果决定了后续的阶段：\n    - 如果 $\\alpha_0$ 满足 Armijo 条件，这意味着该步长是可接受的，甚至可能过于保守。这将触发**扩展阶段**。\n    - 如果 $\\alpha_0$ 未能满足 Armijo 条件，说明该步长过于激进。这将触发标准的**回溯阶段**。\n\n### 扩展阶段\n\n当初始步长 $\\alpha_0$ 被认为是可接受的时，便启动此阶段。它的目标是寻找一个更大、更激进的步长，该步长仍然满足 Armijo 条件，从而可能加速收敛。\n- **过程**：我们从最后一个已知的可接受步长开始，最初为 $\\alpha_0$。然后，我们通过将当前步长乘以一个扩展因子 $\\gamma > 1$ 来迭代地尝试更大的步长。设试验步长序列为 $\\alpha_k = \\gamma \\alpha_{k-1}$，其中 $k=1, 2, \\dots$。\n- **终止**：只要每个新步长 $\\alpha_k$ 都满足 Armijo 条件，扩展就会继续。当某个试验步长 $\\alpha_k$ *不满足*该条件时，此阶段终止。算法随后返回前一个成功验证的步长 $\\alpha_{k-1}$。为了防止搜索时间过长，设置了最大扩展步数 $N_{\\text{exp}}$。如果循环在 $N_{\\text{exp}}$ 次成功扩展后完成，则返回最终的、最大的可接受步长。\n\n### 回溯阶段\n\n当初始步长 $\\alpha_0$ 过大时，启动此阶段。它遵循经典的回溯方法来寻找一个更小的、可接受的步长。\n- **过程**：从 $\\alpha_0$ 开始，算法通过乘以一个回溯因子 $\\beta \\in (0, 1)$ 来迭代地减小步长。设试验步长序列为 $\\alpha_k = \\beta \\alpha_{k-1}$，其中 $k=1, 2, \\dots$。\n- **终止**：一旦某个试验步长 $\\alpha_k$ 满足 Armijo 条件，搜索就立即终止。然后返回这第一个可接受的步长。搜索过程受最大回溯迭代次数 $N_{\\text{bt}}$ 和最小允许步长 $\\alpha_{\\min}$ 的限制。如果在超过 $N_{\\text{bt}}$ 次迭代或步长降至 $\\alpha_{\\min}$ 以下之前未能找到可接受的步长，搜索将终止并返回最后尝试的（未成功的）步长。\n\n### 应用于测试案例\n\n让我们对每个给定的案例追踪算法的执行过程。参数为 $c_1=10^{-4}$，$\\beta=0.5$，$\\gamma=2$，$\\alpha_{\\min}=10^{-16}$，$N_{\\text{bt}}=100$，$N_{\\text{exp}}=50$。\n\n- **情况 A**：$f(x)=(x-3)^2$，$x=[0]$，$p = -\\nabla f(x)$。\n  初始点为 $x=0$，$f(x)=9$。梯度为 $\\nabla f(x) = 2(x-3)$，所以 $\\nabla f(0)=-6$。搜索方向为 $p=-(-6)=6$。方向导数为 $\\nabla f(x)^\\top p = (-6)(6) = -36  0$。对于 $\\alpha_0=1$，新点为 $x+\\alpha_0 p = 0+1(6)=6$，在该点 $f(6)=9$。Armijo 条件为 $f(6) \\le f(0) + c_1 \\alpha_0 (\\nabla f(x)^\\top p)$，即 $9 \\le 9 + 10^{-4}(1)(-36)$，或 $9 \\le 8.9964$。此条件为假。启动回溯。\n  下一个尝试的步长是 $\\alpha_1 = \\beta \\alpha_0 = 0.5(1) = 0.5$。新点为 $x+\\alpha_1 p = 0+0.5(6)=3$，在该点 $f(3)=0$。条件为 $0 \\le 9 + 10^{-4}(0.5)(-36)$，即 $0 \\le 8.9982$。此条件为真。算法返回这个第一个被接受的步长。\n  结果：$\\alpha = 0.5$。\n\n- **情况 B**：$f(x)=\\frac{1}{2}x^2$，$x=[1]$，$p = -\\nabla f(x)$。\n  初始点为 $x=1$，$f(x)=0.5$。梯度为 $\\nabla f(x) = x$，所以 $\\nabla f(1)=1$。搜索方向为 $p=-1$。方向导数为 $\\nabla f(x)^\\top p = (1)(-1)=-1  0$。对于 $\\alpha_0=0.25$，新点为 $x+\\alpha_0 p = 1-0.25=0.75$，在该点 $f(0.75)=0.28125$。Armijo 条件为 $0.28125 \\le 0.5 + 10^{-4}(0.25)(-1)$，即 $0.28125 \\le 0.499975$。此条件为真。启动扩展。\n  - 上一个接受的 $\\alpha=0.25$。尝试 $\\alpha_1 = \\gamma \\alpha_0 = 2(0.25) = 0.5$。$f(1-0.5)=f(0.5)=0.125$。条件：$0.125 \\le 0.5 + 10^{-4}(0.5)(-1) = 0.49995$。真。\n  - 上一个接受的 $\\alpha=0.5$。尝试 $\\alpha_2 = \\gamma(0.5) = 1.0$。$f(1-1.0)=f(0)=0$。条件：$0 \\le 0.5 + 10^{-4}(1.0)(-1) = 0.4999$。真。\n  - 上一个接受的 $\\alpha=1.0$。尝试 $\\alpha_3 = \\gamma(1.0) = 2.0$。$f(1-2.0)=f(-1)=0.5$。条件：$0.5 \\le 0.5 + 10^{-4}(2.0)(-1) = 0.4998$。假。\n  扩展停止。返回最后一个被接受的步长。\n  结果：$\\alpha = 1.0$。\n\n- **情况 C**：$f(x) = \\frac{1}{2}x^\\top Q x$，其中 $Q=\\mathrm{diag}(1,100)$，$x=[1,1]^\\top$，$p=-\\nabla f(x)$。\n  初始点为 $x=[1,1]^\\top$，$f(x)=50.5$。梯度为 $\\nabla f(x)=Qx=[1,100]^\\top$。搜索方向为 $p=[-1,-100]^\\top$。方向导数为 $\\nabla f(x)^\\top p = -1 - 10000 = -10001  0$。对于 $\\alpha_0=1$，Armijo 条件不满足。启动回溯。\n  步长 $\\alpha$ 被重复减半。当 $\\alpha$ 缩减到 $(0.5)^5 = 0.03125$ 时，条件仍不满足。在下一步，当 $\\alpha_6 = (0.5)^6 = 0.015625$ 时，Armijo 条件首次被满足。因此，这是第一个被接受的步长。\n  结果：$\\alpha = 0.015625$。\n\n- **情况 D**：$f(x)=\\frac{1}{2}x^\\top x$，$x=[1,0]^\\top$，$p=[1,0]^\\top$。\n  梯度为 $\\nabla f(x)=x=[1,0]^\\top$。方向导数为 $\\nabla f(x)^\\top p = [1,0]\\,[1,0]^\\top = 1$。由于 $1 \\ge 0$，$p$ 不是一个下降方向。算法的第一个检查捕获了这一点。\n  结果：$\\alpha = 0$。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef modified_line_search(f, grad_f, x, p, alpha0, c1, beta, gamma, alpha_min, n_bt, n_exp):\n    \"\"\"\n    Implements a modified line search with an expansion phase.\n\n    Args:\n        f (callable): The objective function f(x).\n        grad_f (callable): The gradient of the objective function, grad_f(x).\n        x (np.ndarray): The current point.\n        p (np.ndarray): The search direction.\n        alpha0 (float): The initial step size.\n        c1 (float): The Armijo condition constant.\n        beta (float): The backtracking reduction factor.\n        gamma (float): The expansion factor.\n        alpha_min (float): The minimum allowable step size.\n        n_bt (int): Maximum number of backtracking iterations.\n        n_exp (int): Maximum number of expansion steps.\n\n    Returns:\n        float: The selected step size alpha.\n    \"\"\"\n    grad_val = grad_f(x)\n    # Ensure vectors are 1D arrays for consistent dot product\n    dir_deriv = np.dot(grad_val.flatten(), p.flatten())\n\n    if dir_deriv >= 0:\n        return 0.0\n\n    f_x = f(x)\n    alpha = alpha0\n\n    # Test the initial step size\n    f_new = f(x + alpha * p)\n    armijo_rhs = f_x + c1 * alpha * dir_deriv\n\n    if f_new = armijo_rhs:\n        # Initial step is accepted, try to expand it\n        for _ in range(n_exp):\n            alpha_test = gamma * alpha\n            f_new_test = f(x + alpha_test * p)\n            armijo_rhs_test = f_x + c1 * alpha_test * dir_deriv\n            if f_new_test > armijo_rhs_test:\n                # Expansion failed, return the last accepted step\n                return alpha\n            # Expansion succeeded, update alpha\n            alpha = alpha_test\n        # Max expansion steps reached, all were successful\n        return alpha\n    else:\n        # Initial step is too large, perform backtracking\n        for _ in range(n_bt):\n            alpha *= beta\n            if alpha  alpha_min:\n                return alpha # Return last tried alpha if it's too small\n            \n            f_new = f(x + alpha * p)\n            armijo_rhs = f_x + c1 * alpha * dir_deriv\n            if f_new = armijo_rhs:\n                return alpha # First accepted step found\n        \n        # Backtracking failed to find a step within n_bt iterations\n        return alpha # Return the last tried alpha\n\ndef solve():\n    \"\"\"\n    Defines and runs the test suite for the modified line search algorithm.\n    \"\"\"\n    # Common parameters\n    c1 = 1e-4\n    beta = 0.5\n    gamma = 2.0\n    alpha_min = 1e-16\n    n_bt = 100\n    n_exp = 50\n\n    # Case A: Backtracking triggers\n    f_A = lambda x: (x[0] - 3.0)**2\n    grad_f_A = lambda x: np.array([2.0 * (x[0] - 3.0)])\n    x_A = np.array([0.0])\n    p_A = -grad_f_A(x_A)\n    alpha0_A = 1.0\n\n    # Case B: Expansion triggers\n    f_B = lambda x: 0.5 * x[0]**2\n    grad_f_B = lambda x: np.array([x[0]])\n    x_B = np.array([1.0])\n    p_B = -grad_f_B(x_B)\n    alpha0_B = 0.25\n\n    # Case C: Ill-conditioned quadratic\n    Q_C = np.diag([1.0, 100.0])\n    f_C = lambda x: 0.5 * x.T @ Q_C @ x\n    grad_f_C = lambda x: Q_C @ x\n    x_C = np.array([1.0, 1.0])\n    p_C = -grad_f_C(x_C)\n    alpha0_C = 1.0\n\n    # Case D: Non-descent direction\n    f_D = lambda x: 0.5 * x.T @ x\n    grad_f_D = lambda x: x\n    x_D = np.array([1.0, 0.0])\n    p_D = np.array([1.0, 0.0])\n    alpha0_D = 1.0\n\n    test_cases = [\n        (f_A, grad_f_A, x_A, p_A, alpha0_A),\n        (f_B, grad_f_B, x_B, p_B, alpha0_B),\n        (f_C, grad_f_C, x_C, p_C, alpha0_C),\n        (f_D, grad_f_D, x_D, p_D, alpha0_D),\n    ]\n\n    results = []\n    for f, grad_f, x, p, alpha0 in test_cases:\n        alpha = modified_line_search(f, grad_f, x, p, alpha0, c1, beta, gamma, alpha_min, n_bt, n_exp)\n        results.append(alpha)\n\n    # Round results to 10 decimal places and format the output string\n    # Expected results from running the code:\n    # A: 0.5\n    # B: 1.0\n    # C: 0.015625\n    # D: 0.0\n    # The formatted_results will reflect these computed values.\n    formatted_results = [f\"{round(r, 10):.10f}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3247771"}, {"introduction": "Armijo 准则保证了函数值的充分下降，但它本身并不能排除那些过小的、无法接受的步长。为了构建更强大的线搜索，我们引入了强 Wolfe 准则，它在 Armijo 准则的基础上增加了一个“曲率条件”。这套准则构成了现代优化软件的基石。这最后一个练习 [@problem_id:3247710] 将挑战你将这种更复杂的线搜索方法集成到一个完整的梯度下降求解器中，并在著名的难题——Rosenbrock 函数上进行测试。", "problem": "实现一个完整、可运行的程序，在最速下降法中对 Rosenbrock 函数执行线性搜索。目标函数是二维 Rosenbrock 函数，对于向量 $x = (x_1, x_2)$ 定义为 $f(x) = (1 - x_1)^2 + 100 (x_2 - x_1^2)^2$。该算法必须使用基于准则的线性搜索，该准则确保沿下降方向有充分的函数值下降和适当的曲率，并且当主线性搜索未能找到步长时，必须能够回退到简单的回溯策略。程序应计算每次迭代选择的步长序列 $ \\alpha_k $，直到收敛或达到最大迭代次数。程序无需繪圖，必须以数值形式输出这些序列。\n\n程序必须实现以下要求：\n\n- 在每次迭代 $k$ 中，使用最速下降方向 $p_k = - \\nabla f(x_k)$。\n- 实现一个强 Wolfe 线性搜索，以寻找一个步长 $ \\alpha $，该步长需满足沿直线 $x_k + \\alpha p_k$ 的充分下降条件和曲率条件。\n- 如果强 Wolfe 线性搜索未能返回有效的 $ \\alpha $，则回退到 Armijo 回溯法，并使用一个缩减因子和最小步长阈值。\n- 当梯度 $ \\nabla f(x_k) $ 的欧几里得范数 $ \\lVert \\nabla f(x_k) \\rVert_2 $ 小于指定的容差，或迭代次数达到指定的最大值时，终止最速下降迭代。\n\n假设所有计算都是无单位的。不使用角度。不使用百分比。\n\n您的实现必须在以下一组测试用例上进行测试，这些用例共同检验了典型、边界和边缘行为：\n\n- 测试用例 $1$ (典型的困难起点): $x_0 = (-1.2, 1.0)$，$c_1 = 10^{-4}$，$c_2 = 0.9$，初始步长猜测值 $ \\alpha_0 = 1.0$，最大迭代次数 $30$，容差 $10^{-8}$。\n- 测试用例 $2$ (从原点开始): $x_0 = (0.0, 0.0)$，$c_1 = 10^{-4}$，$c_2 = 0.9$，初始步长猜测值 $ \\alpha_0 = 1.0$，最大迭代次数 $30$，容差 $10^{-8}$。\n- 测试用例 $3$ (在最小值点上的边界情况): $x_0 = (1.0, 1.0)$，$c_1 = 10^{-4}$，$c_2 = 0.9$，初始步长猜测值 $ \\alpha_0 = 1.0$，最大迭代次数 $10$，容差 $10^{-12}$。这应该会产生一个空序列，因为起点已经是最优解。\n- 测试用例 $4$ (备选曲率参数和初始猜测值): $x_0 = (-1.2, 1.0)$，$c_1 = 10^{-4}$，$c_2 = 0.1$，初始步长猜测值 $ \\alpha_0 = 0.5$，最大迭代次数 $20$，容差 $10^{-8}$。\n\n对于 Armijo 回溯回退策略，使用缩减因子 $ \\tau = 0.5 $ 和最小步长阈值 $ \\alpha_{\\min} = 10^{-8} $。\n\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，列表内无空格。每个结果本身是按顺序排列的针对一个测试用例所选步长 $ \\alpha_k $ 的列表。例如，最终输出的格式必须是 $[R_1,R_2,R_3,R_4]$，其中 $R_i$ 是测试用例 $i$ 的 $ \\alpha_k $ 浮点数列表（例如：$[[0.5,0.25],[0.75],[\\,], [0.5,0.5,0.25]]$，但没有任何空格）。", "solution": "该问题要求实现一个针对二维 Rosenbrock 函数的最速下降优化算法，该算法包含一个复杂的线性搜索过程。解决方案首先详细阐述数学原理，然后描述算法组件。\n\n### 1. 数学公式\n\n目标函数是 Rosenbrock 函数，这是一个标准的优化算法基准测试函数，以其狭窄的抛物线形山谷而闻名。对于向量 $x = (x_1, x_2)$，它定义为：\n$$\nf(x) = (1 - x_1)^2 + 100(x_2 - x_1^2)^2\n$$\n为了实现像最速下降法这样的基于梯度的方法，我们必须计算 $f(x)$ 的梯度，记为 $\\nabla f(x)$。其偏导数为：\n$$\n\\frac{\\partial f}{\\partial x_1} = -2(1 - x_1) + 100 \\cdot 2(x_2 - x_1^2) \\cdot (-2x_1) = -2(1 - x_1) - 400x_1(x_2 - x_1^2)\n$$\n$$\n\\frac{\\partial f}{\\partial x_2} = 100 \\cdot 2(x_2 - x_1^2) = 200(x_2 - x_1^2)\n$$\n因此，梯度向量为：\n$$\n\\nabla f(x) = \\begin{pmatrix} -2(1 - x_1) - 400x_1(x_2 - x_1^2) \\\\ 200(x_2 - x_1^2) \\end{pmatrix}\n$$\n\n### 2. 最速下降算法\n\n最速下降法是一种迭代算法，用于寻找函数的局部最小值。从初始点 $x_0$ 开始，它使用以下更新规则生成一系列点 $\\{x_k\\}$：\n$$\nx_{k+1} = x_k + \\alpha_k p_k\n$$\n其中 $k$ 是迭代索引，$p_k$ 是搜索方向，$\\alpha_k > 0$ 是步长。对于最速下降法，搜索方向选择为下降最快的方向，即梯度的负方向：\n$$\np_k = - \\nabla f(x_k)\n$$\n迭代持续进行，直到满足终止准则。在本问题中，如果梯度的欧几里得范数低于指定的容差，即 $\\lVert \\nabla f(x_k) \\rVert_2  \\text{tolerance}$，或者迭代次数达到最大值 $k \\ge \\text{max\\_iterations}$，则过程停止。\n\n### 3. 线性搜索子问题与 Wolfe 条件\n\n算法的一个关键部分是确定每次迭代的步长 $\\alpha_k$。这是一个一维优化问题：找到一个 $\\alpha_k$ 以最小化函数在搜索方向上的值，即 $\\phi(\\alpha) = f(x_k + \\alpha p_k)$。与寻找精确的最小值不同，找到一个能提供足够进展的 $\\alpha_k$ 更为高效。强 Wolfe 条件为此提供了一种标准而有效的方法。\n\n**强 Wolfe 条件** 包含步长 $\\alpha$ 必须满足的两个不等式：\n1.  **充分下降条件 (Armijo 条件):**\n    $$\n    f(x_k + \\alpha p_k) \\le f(x_k) + c_1 \\alpha \\nabla f(x_k)^T p_k\n    $$\n    此条件确保步长 $\\alpha$ 能使目标函数 $f$ 获得充分的下降。其中 $c_1$ 是一个常数，通常很小，例如 $c_1 = 10^{-4}$。\n\n2.  **曲率条件:**\n    $$\n    |\\nabla f(x_k + \\alpha p_k)^T p_k| \\le c_2 |\\nabla f(x_k)^T p_k|\n    $$\n    此条件确保新点的函数斜率得到充分减小，从而防止算法采用过小的步长。参数 $c_2$ 满足 $c_1  c_2  1$。对于下降方向，$\\nabla f(x_k)^T p_k  0$，因此该条件可写为 $|\\nabla f(x_k + \\alpha p_k)^T p_k| \\le -c_2 \\nabla f(x_k)^T p_k$。\n\n### 4. 线性搜索实现\n\n一个稳健的、用于寻找满足强 Wolfe 条件的步长的线性搜索算法通常包含两个阶段：区间限定阶段和随后的缩放阶段。\n\n1.  **区间限定 (Bracketing)**：从 $\\alpha$ 的一个初始猜测值开始，算法生成一系列试验步长，不断扩大搜索区间，直到该区间包含一个或多个满足 Wolfe条件的点。\n2.  **缩放 (Zooming)**：一旦确定了一个已知包含合适步长的区间 $[\\alpha_{\\text{lo}}, \\alpha_{\\text{hi}}]$，就使用一种程序（例如，二分法或插值法）来放大该区间，以找到满足强 Wolfe 条件的点。\n\n该实现将为此搜索设置一个内部迭代限制。如果在此限制内未能找到合适的 $\\alpha$，则认为线性搜索“失败”，并触发回退机制。\n\n### 5. 回退策略：Armijo 回溯法\n\n如果主体的强 Wolfe 线性搜索失败，算法将回退到一个更简单但更稳健的方法：Armijo 回溯法。该方法保证仅满足充分下降条件。算法如下：\n\n1.  从一个初始步长开始，例如 $\\alpha = \\alpha_0$。\n2.  当充分下降条件未满足时：\n    $$\n    f(x_k + \\alpha p_k) > f(x_k) + c_1 \\alpha \\nabla f(x_k)^T p_k\n    $$\n    将步长乘以一个因子 $\\tau \\in (0, 1)$ 进行缩减：\n    $$\n    \\alpha \\leftarrow \\tau \\alpha\n    $$\n3.  当条件满足时，或当 $\\alpha$ 小于最小阈值 $\\alpha_{\\min}$ 时，过程停止。\n\n### 6. 完整算法摘要\n\n最终程序为每个指定的测试用例集成了这些组件：\n\n1.  初始化起始点 $x_0$ 和参数 ($c_1, c_2, \\alpha_0, \\text{max\\_iterations}, \\text{tolerance}$) 。\n2.  开始主循环， $k = 0, 1, \\dots, \\text{max\\_iterations}-1$。\n3.  在每次迭代 $k$ 时，计算梯度 $\\nabla f(x_k)$ 及其范数。如果范数低于容差，则终止并返回到目前为止找到的步长序列。\n4.  设置搜索方向 $p_k = - \\nabla f(x_k)$。\n5.  调用强 Wolfe 线性搜索函数，使用初始猜测值 $\\alpha_0$ 来寻找步长 $\\alpha_k$。\n6.  如果 Wolfe 搜索失败（返回失败信号），则调用 Armijo 回溯函数来寻找 $\\alpha_k$。回溯的起始 $\\alpha$ 是当前迭代的初始猜测值。\n7.  存储计算出的步长 $\\alpha_k$。\n8.  更新位置：$x_{k+1} = x_k + \\alpha_k p_k$。\n9.  循环终止后（因收敛或达到最大迭代次数），为每个测试用例按指定格式输出所有存储的 $\\alpha_k$ 值列表。测试用例 3 中的起始点 $x_0 = (1.0, 1.0)$ 是全局最小值点，因此梯度范数为 $0$，导致立即终止并生成一个空的步长列表。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef rosenbrock(x):\n    \"\"\"\n    Computes the Rosenbrock function value.\n    f(x) = (1 - x_1)^2 + 100 * (x_2 - x_1^2)^2\n    \"\"\"\n    return (1 - x[0])**2 + 100 * (x[1] - x[0]**2)**2\n\ndef rosenbrock_grad(x):\n    \"\"\"\n    Computes the gradient of the Rosenbrock function.\n    \"\"\"\n    grad_x1 = -2 * (1 - x[0]) - 400 * x[0] * (x[1] - x[0]**2)\n    grad_x2 = 200 * (x[1] - x[0]**2)\n    return np.array([grad_x1, grad_x2])\n\ndef _zoom(phi, phi_prime, alpha_lo, alpha_hi, phi_0, phi_prime_0, c1, c2, max_iter=20):\n    \"\"\"\n    Helper function for strong_wolfe_linesearch.\n    Narrows down an interval known to contain a point satisfying the Wolfe conditions.\n    Based on Algorithm 3.6 in Nocedal  Wright, \"Numerical Optimization\".\n    \"\"\"\n    for _ in range(max_iter):\n        # Use bisection to find a trial step length.\n        alpha_j = (alpha_lo + alpha_hi) / 2.0\n        \n        phi_j = phi(alpha_j)\n\n        if phi_j > phi_0 + c1 * alpha_j * phi_prime_0 or phi_j >= phi(alpha_lo):\n            alpha_hi = alpha_j\n        else:\n            phi_prime_j = phi_prime(alpha_j)\n            if abs(phi_prime_j) = -c2 * phi_prime_0:\n                return alpha_j\n            \n            if phi_prime_j * (alpha_hi - alpha_lo) >= 0:\n                alpha_hi = alpha_lo\n            \n            alpha_lo = alpha_j\n        \n        # Terminate if interval becomes too small to prevent precision issues\n        if abs(alpha_hi - alpha_lo)  1e-12:\n            break\n            \n    # If the loop finishes without finding a point, it failed.\n    # Return the last valid point satisfying sufficient decrease, if any.\n    if phi(alpha_lo) = phi_0 + c1 * alpha_lo * phi_prime_0 and abs(phi_prime(alpha_lo)) = -c2 * phi_prime_0:\n        return alpha_lo\n\n    return None\n\ndef strong_wolfe_linesearch(f, grad, x, p, alpha_init, c1, c2, max_iter=20):\n    \"\"\"\n    Performs a line search to find a step length satisfying the strong Wolfe conditions.\n    Based on Algorithm 3.5 in Nocedal  Wright.\n    Returns the step length alpha, or None if it fails.\n    \"\"\"\n    phi = lambda alpha: f(x + alpha * p)\n    phi_prime = lambda alpha: np.dot(grad(x + alpha * p), p)\n\n    phi_0 = phi(0)\n    phi_prime_0 = phi_prime(0)\n\n    alpha_prev = 0.0\n    alpha_i = alpha_init\n\n    for i in range(max_iter):\n        phi_i = phi(alpha_i)\n\n        if (phi_i > phi_0 + c1 * alpha_i * phi_prime_0) or (i > 0 and phi_i >= phi(alpha_prev)):\n            return _zoom(phi, phi_prime, alpha_prev, alpha_i, phi_0, phi_prime_0, c1, c2)\n\n        phi_prime_i = phi_prime(alpha_i)\n\n        if abs(phi_prime_i) = -c2 * phi_prime_0:\n            return alpha_i\n\n        if phi_prime_i >= 0:\n            return _zoom(phi, phi_prime, alpha_i, alpha_prev, phi_0, phi_prime_0, c1, c2)\n        \n        # If we reach here, expand the search. A simple expansion is used.\n        alpha_prev = alpha_i\n        alpha_i = alpha_i * 2.0\n\n    return None\n\ndef armijo_backtracking(f, grad, x, p, alpha_init, c1, tau, alpha_min):\n    \"\"\"\n    Fallback line search using Armijo backtracking.\n    \"\"\"\n    phi_0 = f(x)\n    phi_prime_0 = np.dot(grad(x), p)\n    \n    alpha = alpha_init\n\n    while f(x + alpha * p) > phi_0 + c1 * alpha * phi_prime_0:\n        alpha = tau * alpha\n        if alpha  alpha_min:\n            return alpha_min\n    \n    return alpha\n\ndef steepest_descent(x0, c1, c2, alpha_init_guess, max_iter, tol, tau, alpha_min):\n    \"\"\"\n    Main steepest descent optimization loop.\n    \"\"\"\n    x = np.array(x0, dtype=float)\n    alphas = []\n\n    for k in range(max_iter):\n        grad_val = rosenbrock_grad(x)\n        grad_norm = np.linalg.norm(grad_val)\n\n        if grad_norm  tol:\n            break\n\n        p = -grad_val\n        \n        # Primary line search: strong Wolfe\n        alpha = strong_wolfe_linesearch(rosenbrock, rosenbrock_grad, x, p, alpha_init_guess, c1, c2)\n        \n        # Fallback line search: Armijo\n        if alpha is None:\n            alpha = armijo_backtracking(rosenbrock, rosenbrock_grad, x, p, alpha_init_guess, c1, tau, alpha_min)\n            \n        alphas.append(alpha)\n        x = x + alpha * p\n\n    return alphas\n\ndef solve():\n    \"\"\"\n    Main execution function to run all test cases.\n    \"\"\"\n    test_cases = [\n        # x0, c1, c2, alpha_init_guess, max_iter, tol\n        {'x0': [-1.2, 1.0], 'c1': 1e-4, 'c2': 0.9, 'alpha_init': 1.0, 'max_iter': 30, 'tol': 1e-8},\n        {'x0': [0.0, 0.0], 'c1': 1e-4, 'c2': 0.9, 'alpha_init': 1.0, 'max_iter': 30, 'tol': 1e-8},\n        {'x0': [1.0, 1.0], 'c1': 1e-4, 'c2': 0.9, 'alpha_init': 1.0, 'max_iter': 10, 'tol': 1e-12},\n        {'x0': [-1.2, 1.0], 'c1': 1e-4, 'c2': 0.1, 'alpha_init': 0.5, 'max_iter': 20, 'tol': 1e-8},\n    ]\n\n    # Armijo fallback parameters\n    tau = 0.5\n    alpha_min = 1e-8\n\n    all_results = []\n    for case in test_cases:\n        alphas = steepest_descent(\n            case['x0'], case['c1'], case['c2'], case['alpha_init'],\n            case['max_iter'], case['tol'], tau, alpha_min\n        )\n        all_results.append(alphas)\n\n    # Format the output string to match the required format exactly.\n    result_strings = [str(r).replace(' ', '') for r in all_results]\n    print(f\"[{','.join(result_strings)}]\")\n\nsolve()\n```", "id": "3247710"}]}