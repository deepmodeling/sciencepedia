{"hands_on_practices": [{"introduction": "在最速下降法中，最首要的一步是确定从哪里开始以及朝哪个方向前进。本练习将通过一个经典的优化测试函数——Rosenbrock函数，来实践如何计算初始下降方向[@problem_id:2221567]。通过计算函数在起点的梯度，你将掌握确定最速下降路径的核心原理，即沿负梯度方向移动。", "problem": "在数值优化领域，新算法的性能通常使用一组标准测试函数进行基准测试。其中一个函数是 Rosenbrock 函数，由于其狭窄的抛物线形山谷，该函数难以最小化。\n\n考虑 Rosenbrock 函数的二维版本，其表达式为\n$$f(x,y) = (a-x)^2 + b(y-x^2)^2$$\n其中 $a$ 和 $b$ 是正实数常量。\n\n一个迭代最小化算法从点 $(x_0, y_0) = (0, 0)$ 开始。该算法的第一步是确定初始搜索方向。这个方向被定义为函数值从起始点下降最快的向量方向。\n\n确定这个初始搜索方向向量 $\\mathbf{d}_0$。将您的答案表示为用常量 $a$ 和 $b$ 表示的列向量。", "solution": "函数 $f$ 在某点下降最快的方向是负梯度方向，因此从 $(x_{0},y_{0})=(0,0)$ 出发的初始搜索方向是 $\\mathbf{d}_{0}=-\\nabla f(0,0)$。\n\n计算 $f(x,y)=(a-x)^{2}+b(y-x^{2})^{2}$ 的梯度：\n- 关于 $x$ 的偏导数为\n$$\n\\frac{\\partial f}{\\partial x}=2(x-a)-4bx(y-x^{2}).\n$$\n- 关于 $y$ 的偏导数为\n$$\n\\frac{\\partial f}{\\partial y}=2b(y-x^{2}).\n$$\n\n在 $(0,0)$ 处计算这些值：\n$$\n\\frac{\\partial f}{\\partial x}(0,0)=-2a,\\qquad \\frac{\\partial f}{\\partial y}(0,0)=0.\n$$\n因此，\n$$\n\\nabla f(0,0)=\\begin{pmatrix}-2a\\\\0\\end{pmatrix},\n\\quad\n\\mathbf{d}_{0}=-\\nabla f(0,0)=\\begin{pmatrix}2a\\\\0\\end{pmatrix}.\n$$", "answer": "$$\\boxed{\\begin{pmatrix}2a\\\\0\\end{pmatrix}}$$", "id": "2221567"}, {"introduction": "确定了方向之后，我们能走多远？步长（或学习率）的选择对算法的成败至关重要。这个练习将通过一个最简单的一维二次函数 $f(x) = x^2$ 来深入探讨不同学习率如何影响算法的行为[@problem_id:3279015]。你将通过分析推导和编写代码，将算法的轨迹精确地分类为收敛、振荡或发散等不同模式，从而对这个关键的超参数建立深刻的直觉。", "problem": "您需要分析最速下降法（也称为梯度下降法）在不同恒定学习率下，应用于标量目标函数 $f(x) = x^2$ 时的行为。请仅从基本定义出发：可微函数的梯度，以及最速下降更新作为沿欧几里得范数下负梯度方向的步进。基于这些定义，推导出针对此特定函数的迭代过程，然后根据学习率对轨迹行为进行分类。初始条件固定为 $x_0 = 1.0$，这是一个无量纲的标量。\n\n您的程序必须通过对推导出的迭代式进行推理，来为每个指定的学习率值实现轨迹行为的分类，而不是通过数值模拟来调整阈值。对于每个学习率，根据迭代序列是收敛、发散还是有界不收敛，从以下互斥类别中精确地分配一个整数代码：\n\n- 代码 $0$：收敛，幅值单调且无符号变化；迭代值趋近于最小值点且从不改变符号。\n- 代码 $1$：收敛，振荡；迭代值在符号交替的同时趋近于最小值点。\n- 代码 $2$：一步收敛至最小值点；一次迭代后，迭代值即为最小值点。\n- 代码 $3$：静止；每次迭代值均等于初始值，因此没有向最小值点前进。\n- 代码 $4$：有界，不收敛的周期为2的行为；幅值保持恒定，而符号无限交替。\n- 代码 $5$：发散，符号单调；幅值无界增长且符号从不改变。\n- 代码 $6$：发散，振荡；幅值无界增长且符号交替。\n\n使用 $x_0 = 1.0$。不涉及角度。没有物理单位。\n\n测试集。您的程序必须按以下顺序评估以下学习率集合：\n- $\\eta = 0.1$\n- $\\eta = 0.6$\n- $\\eta = 0.5$\n- $\\eta = 0.0$\n- $\\eta = 1.0$\n- $\\eta = 1.1$\n- $\\eta = -0.1$\n- $\\eta = 2.0$\n- $\\eta = 0.99$\n\n最终输出格式。您的程序应生成单行输出，其中包含用方括号括起来的逗号分隔列表形式的结果。每个测试用例的结果是上面定义的整数代码，并且列表必须与测试集的顺序相同。例如，一个有效的输出看起来像“[0,1,2,3]”。", "solution": "该问题是有效的，因为它具有科学依据、问题明确且客观。它提出了一个数值优化中的标准练习，可以通过严格的数学推导来解决。所有必要信息都已提供，问题没有矛盾或含糊之处。\n\n对最速下降法的分析始于其基本定义。该方法通过以下更新规则生成一个迭代序列 $\\{x_k\\}$ 来最小化目标函数 $f(x)$：\n$$\nx_{k+1} = x_k - \\eta \\nabla f(x_k)\n$$\n其中 $\\eta$ 是学习率，$\\nabla f(x_k)$ 是 $f$ 在 $x_k$ 处的梯度。\n\n目标函数为标量二次函数 $f(x) = x^2$。对于单变量的标量函数，其梯度是它对 $x$ 的导数。\n$$\n\\nabla f(x) = \\frac{d}{dx}(x^2) = 2x\n$$\n将此梯度代入最速下降更新规则，我们得到针对该问题的特定迭代式：\n$$\nx_{k+1} = x_k - \\eta (2x_k)\n$$\n通过提取 $x_k$ 因子，可以简化此表达式：\n$$\nx_{k+1} = (1 - 2\\eta) x_k\n$$\n这是一个线性一阶齐次递推关系。给定初始条件 $x_0 = 1.0$，第 $k$ 次迭代的闭式解为：\n$$\nx_k = (1 - 2\\eta)^k x_0 = (1 - 2\\eta)^k\n$$\n当 $k \\to \\infty$ 时，序列 $\\{x_k\\}$ 的行为完全由常数因子 $c = 1 - 2\\eta$ 的值决定。我们可以通过根据 $c$ 的结果值对 $\\eta$ 的可能值进行划分来分析轨迹。$f(x) = x^2$ 的最小值点在 $x=0$。\n\n分析按 $\\eta$ 的值分情况进行：\n\n情况 1：$\\eta  0$。在这种情况下，$-2\\eta > 0$，所以 $c = 1 - 2\\eta > 1$。序列为 $x_k = c^k$，由于 $c > 1$ 且 $x_0 = 1 > 0$，该序列发散到 $+\\infty$。迭代值均为正，其幅值无界增长。这对应于符号单调的发散轨迹。\n分类：**代码 5**。\n\n情况 2：$\\eta = 0$。在这种情况下，$c = 1 - 2(0) = 1$。更新规则变为 $x_{k+1} = x_k$。每次迭代值均等于初始值 $x_0 = 1.0$。序列是静止的，没有向最小值点前进。\n分类：**代码 3**。\n\n情况 3：$0  \\eta  0.5$。在此范围内，我们有 $0  2\\eta  1$，这意味着 $0  1 - 2\\eta  1$，所以 $0  c  1$。序列 $x_k = c^k$ 是一个单调递减至 $0$ 的正数序列。迭代值收敛到最小值点而不改变符号。\n分类：**代码 0**。\n\n情况 4：$\\eta = 0.5$。在这种情况下，$c = 1 - 2(0.5) = 1 - 1 = 0$。第二次迭代值为 $x_1 = c \\cdot x_0 = 0$。所有后续的迭代值 $x_k$（对于 $k \\ge 1$）都将是 $0$。这是一步内精确收敛到最小值点。\n分类：**代码 2**。\n\n情况 5：$0.5  \\eta  1$。在此范围内，我们有 $1  2\\eta  2$，这意味着 $-1  1 - 2\\eta  0$，所以 $-1  c  0$。底数 $c$ 是负数，因此 $x_k = c^k$ 的符号交替。由于 $|c|  1$，幅值 $|x_k| = |c|^k$ 单调递减至 $0$。轨迹是振荡且收敛的。\n分类：**代码 1**。\n\n情况 6：$\\eta = 1$。在这种情况下，$c = 1 - 2(1) = -1$。序列为 $x_k = (-1)^k x_0 = (-1)^k$。迭代值在 $1$ 和 $-1$ 之间交替。轨迹是有界的，周期为2；它不收敛。\n分类：**代码 4**。\n\n情况 7：$\\eta > 1$。在这种情况下，$2\\eta > 2$，这意味着 $1 - 2\\eta  -1$，所以 $c  -1$。由于 $c$ 是负数，$x_k = c^k$ 的符号交替。由于 $|c| > 1$，幅值 $|x_k| = |c|^k$ 无界增长。轨迹是发散且振荡的。\n分类：**代码 6**。\n\n这些推导出的条件为任何给定的学习率 $\\eta$ 提供了一个完整且互斥的分类，这是基于对迭代公式的严格分析，符合要求。程序将实现这一逻辑。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Analyzes the behavior of gradient descent for f(x)=x^2 for a given\n    set of learning rates and classifies the trajectory.\n    \"\"\"\n\n    # The test suite of learning rates provided in the problem statement.\n    test_cases = [\n        0.1,\n        0.6,\n        0.5,\n        0.0,\n        1.0,\n        1.1,\n        -0.1,\n        2.0,\n        0.99,\n    ]\n\n    def classify_trajectory(eta: float) - int:\n        \"\"\"\n        Classifies the trajectory behavior based on the analytical conditions\n        derived for the learning rate eta.\n\n        The iteration is x_{k+1} = (1 - 2*eta) * x_k. The behavior depends on\n        the factor c = 1 - 2*eta.\n\n        - Code 0: Convergent, monotone (0  c  1  = 0  eta  0.5)\n        - Code 1: Convergent, oscillatory (-1  c  0 = 0.5  eta  1)\n        - Code 2: One-step convergence (c = 0 = eta = 0.5)\n        - Code 3: Stationary (c = 1 = eta = 0)\n        - Code 4: Bounded, non-convergent periodic (c = -1 = eta = 1)\n        - Code 5: Divergent, monotone (c  1 = eta  0)\n        - Code 6: Divergent, oscillatory (c  -1 = eta  1)\n\n        Args:\n            eta: The learning rate (a float).\n\n        Returns:\n            An integer code representing the trajectory behavior.\n        \"\"\"\n        if eta  0:\n            return 5\n        elif eta == 0:\n            return 3\n        elif 0  eta  0.5:\n            return 0\n        elif eta == 0.5:\n            return 2\n        elif 0.5  eta  1:\n            return 1\n        elif eta == 1:\n            return 4\n        else:  # eta > 1\n            return 6\n\n    results = []\n    for eta_val in test_cases:\n        code = classify_trajectory(eta_val)\n        results.append(code)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3279015"}, {"introduction": "最速下降法并非万能，它在某些特定问题上会表现不佳。这个练习将揭示该方法的一个主要挑战：病态问题（ill-conditioned problems）[@problem_id:3279006]。你将亲手构建不同病态程度的二次函数，并观察即便在每一步都采用最优步长的情况下，问题的几何形态（由其Hessian矩阵的条件数衡量）如何显著减慢收敛速度。", "problem": "您需要研究在二维严格凸二次函数上使用精确线搜索的最速下降法（也称为梯度下降法），并展示由病态条件引起的慢收敛现象。\n\n考虑一个形式如下的二次目标函数\n$$\nf(x) \\;=\\; \\tfrac{1}{2}\\,(x - x^\\star)^\\top A (x - x^\\star),\n$$\n其中 $x \\in \\mathbb{R}^2$，$x^\\star \\in \\mathbb{R}^2$ 是唯一的极小值点，而 $A \\in \\mathbb{R}^{2 \\times 2}$ 是对称正定（SPD）矩阵。在以下所有情况中，取 $x^\\star = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$，因此 $f(x) = \\tfrac{1}{2}\\,x^\\top A x$。梯度为 $\\nabla f(x)$，最速下降迭代为\n$$\nx_{k+1} \\;=\\; x_k \\;-\\; \\alpha_k \\,\\nabla f(x_k),\n$$\n其中步长 $\\alpha_k$ 通过精确线搜索选择，以使 $f(x_k - \\alpha\\,\\nabla f(x_k))$ 关于 $\\alpha \\in \\mathbb{R}$ 最小化。\n\n您将使用旋转和特征值设计一族具有指定条件数的SPD矩阵。令\n$$\nR(\\theta) \\;=\\; \\begin{bmatrix}\n\\cos\\theta  -\\sin\\theta \\\\\n\\sin\\theta  \\cos\\theta\n\\end{bmatrix}, \\qquad\n\\Lambda \\;=\\; \\operatorname{diag}(\\lambda_{\\min}, \\lambda_{\\max}), \\qquad\nA \\;=\\; R(\\theta)\\,\\Lambda\\,R(\\theta)^\\top,\n$$\n这样 $A$ 的谱为 $\\{\\lambda_{\\min}, \\lambda_{\\max}\\}$，且在2-范数下的条件数为 $\\kappa(A) = \\lambda_{\\max}/\\lambda_{\\min}$。角度必须以弧度为单位指定。\n\n任务：\n1. 从第一性原理出发，通过最小化关于 $\\alpha$ 的 $f(x_k - \\alpha\\,\\nabla f(x_k))$，推导应用于上述形式的严格凸二次函数 $f(x)$ 的最速下降法的精确线搜索步长规则。\n2. 为上述由 $A$ 定义的二次函数实现带有此精确线搜索规则的最速下降法，从初始点 $x_0 = \\begin{bmatrix} 3 \\\\ -1 \\end{bmatrix}$ 开始。在恰好 $T$ 次迭代后停止，除非梯度范数在此之前变为零，在这种情况下您可以立即停止。\n3. 对于下面测试套件中的每种情况，计算减少因子\n$$\nr_T \\;=\\; \\frac{f(x_T)}{f(x_0)},\n$$\n其中 $x_T$ 是 $T$ 步后的迭代点（如果方法因梯度为零而提前终止，则为最终迭代点）。该比率是无量纲的。\n\n测试套件（角度以弧度为单位）：\n- 情况 S（球形，边界条件）：$\\lambda_{\\min} = 10$，$\\lambda_{\\max} = 10$，$\\theta = 0.7$，$T = 10$。\n- 情况 M（中度病态）：$\\lambda_{\\min} = 1$，$\\lambda_{\\max} = 100$，$\\theta = \\pi/6$，$T = 10$。\n- 情况 H（高度病态）：$\\lambda_{\\min} = 1$，$\\lambda_{\\max} = 10^4$，$\\theta = \\pi/4$，$T = 10$。\n\n您的程序必须：\n- 使用给定的 $\\lambda_{\\min}$、$\\lambda_{\\max}$ 和 $\\theta$ 为每种情况构造 $A$。\n- 从 $x_0 = \\begin{bmatrix} 3 \\\\ -1 \\end{bmatrix}$ 开始，运行带有精确线搜索的最速下降法 $T$ 次迭代。\n- 计算并记录每种情况的 $r_T$。\n\n最终输出格式：\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表的结果（例如，\"[result1,result2,result3]\"），其中三个条目是三个 $r_T$ 的值（按 S、M、H 的顺序），作为浮点数。不应打印其他任何文本。", "solution": "该问题被评估为有效的，因为它具有科学依据、适定、客观，并包含足够的信息以获得唯一解。它代表了优化领域中的一个标准数值实验，旨在演示病态条件对最速下降法收敛性的影响。\n\n所需的任务是：\n1.  推导严格凸二次函数的精确线搜索步长公式。\n2.  使用该步长实现最速下降法。\n3.  为三种特定情况计算函数值减少因子 $r_T = f(x_T)/f(x_0)$。\n\n**1. 精确线搜索步长的推导**\n\n最速下降算法使用以下更新规则生成一个迭代序列：\n$$\nx_{k+1} = x_k - \\alpha_k \\nabla f(x_k)\n$$\n其中 $\\alpha_k > 0$ 是步长。精确线搜索选择 $\\alpha_k$ 以沿搜索方向 $d_k = -\\nabla f(x_k)$ 最小化目标函数。我们定义一个一维函数 $\\phi(\\alpha) = f(x_k + \\alpha d_k) = f(x_k - \\alpha g_k)$，其中 $g_k = \\nabla f(x_k)$。\n\n目标函数是严格凸二次形式 $f(x) = \\frac{1}{2}x^\\top A x$，其中 $A$ 是一个对称正定（SPD）矩阵。\n该函数的梯度是 $\\nabla f(x) = \\frac{1}{2}(A + A^\\top)x$。由于 $A$ 是对称的（$A=A^\\top$），梯度可以简化为：\n$$\ng_k = \\nabla f(x_k) = A x_k\n$$\n我们将更新规则代入目标函数以得到 $\\phi(\\alpha)$:\n$$\n\\phi(\\alpha) = f(x_k - \\alpha g_k) = \\frac{1}{2} (x_k - \\alpha g_k)^\\top A (x_k - \\alpha g_k)\n$$\n展开此表达式，我们得到：\n$$\n\\phi(\\alpha) = \\frac{1}{2} (x_k^\\top A x_k - \\alpha x_k^\\top A g_k - \\alpha g_k^\\top A x_k + \\alpha^2 g_k^\\top A g_k)\n$$\n由于 $g_k^\\top A x_k$ 是一个标量，它等于其转置，即 $(g_k^\\top A x_k)^\\top = x_k^\\top A^\\top g_k$。因为 $A$ 是对称的，所以这等于 $x_k^\\top A g_k$。因此，$\\alpha$ 的两个线性项是相同的。\n$$\n\\phi(\\alpha) = \\frac{1}{2} x_k^\\top A x_k - \\alpha g_k^\\top A x_k + \\frac{1}{2} \\alpha^2 g_k^\\top A g_k\n$$\n这是一个关于 $\\alpha$ 的标量二次函数。为了找到最小化 $\\phi(\\alpha)$ 的 $\\alpha$ 值，我们计算它关于 $\\alpha$ 的导数并将其设为零。\n$$\n\\frac{d\\phi}{d\\alpha} = -g_k^\\top A x_k + \\alpha g_k^\\top A g_k = 0\n$$\n解出 $\\alpha$，我们得到最优步长，记为 $\\alpha_k$：\n$$\n\\alpha_k = \\frac{g_k^\\top A x_k}{g_k^\\top A g_k}\n$$\n我们可以使用关系式 $g_k = A x_k$ 来简化此公式。分子变为 $g_k^\\top (A x_k) = g_k^\\top g_k$。因此，步长为：\n$$\n\\alpha_k = \\frac{g_k^\\top g_k}{g_k^\\top A g_k}\n$$\n这就是用于二次目标函数的精确线搜索步长的著名公式。二阶导数 $\\frac{d^2\\phi}{d\\alpha^2} = g_k^\\top A g_k$ 是正的，因为 $A$ 是正定的（并假设 $g_k \\neq 0$），这证实了 $\\alpha_k$ 确实产生一个最小值。如果 $g_k = 0$，则迭代点 $x_k$ 已经是 $f(x)$ 的极小值点，迭代终止。\n\n**2. 算法实现与执行**\n\n通过实现带有推导出的精确线搜索的最速下降法来进行数值实验。对于每种情况（S, M, H），执行以下步骤：\n1.  使用指定的参数 $\\lambda_{\\min}$、$\\lambda_{\\max}$ 和 $\\theta$ 构造 SPD 矩阵 $A = R(\\theta)\\Lambda R(\\theta)^\\top$。\n2.  初始点设为 $x_0 = \\begin{bmatrix} 3 \\\\ -1 \\end{bmatrix}$。计算初始函数值 $f(x_0)$。\n3.  最速下降迭代 $x_{k+1} = x_k - \\alpha_k g_k$ 运行恰好 $T$ 步，除非梯度范数变为零。在每一步 $k$，计算梯度 $g_k = A x_k$ 和步长 $\\alpha_k = (g_k^\\top g_k) / (g_k^\\top A g_k)$。\n4.  经过 $T$ 次迭代（或提前终止）后，最终的迭代点是 $x_T$。\n5.  计算最终的函数值 $f(x_T)$。\n6.  计算并存储减少因子 $r_T = f(x_T) / f(x_0)$。\n\n对于情况 S，$\\lambda_{\\min}=\\lambda_{\\max}$，所以条件数 $\\kappa(A)=1$。矩阵 $A$ 是单位矩阵的标量倍数，$A=10I$。$f(x)$ 的水平集是圆形，最速下降法在一步之内收敛到极小值点 $x^\\star=0$。因此，$x_1=x_T=0$，$f(x_T)=0$，且 $r_T=0$。\n\n对于情况 M 和 H，条件数分别为 $\\kappa(A)=100$ 和 $\\kappa(A)=10^4$。水平集是拉长的椭圆。最速下降路径将表现出典型的锯齿形行为，导致收敛缓慢。减少因子 $r_T$ 预计将显著大于 0，并且情况 H 的值会比情况 M 的值大，这表明随着病态条件的增强，性能会下降。所提供的 Python 代码实现了此过程以计算所需的值。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef run_steepest_descent(lambda_min: float, lambda_max: float, theta: float, T: int, x0: np.ndarray) - float:\n    \"\"\"\n    Performs steepest descent with exact line search for a quadratic function.\n\n    Args:\n        lambda_min: The minimum eigenvalue of the matrix A.\n        lambda_max: The maximum eigenvalue of the matrix A.\n        theta: The rotation angle in radians for constructing A.\n        T: The number of iterations.\n        x0: The starting point as a numpy array.\n\n    Returns:\n        The reduction factor r_T = f(x_T) / f(x_0).\n    \"\"\"\n    # 1. Construct the matrix A\n    c, s = np.cos(theta), np.sin(theta)\n    R = np.array([[c, -s], [s, c]])\n    Lambda = np.diag([lambda_min, lambda_max])\n    A = R @ Lambda @ R.T\n\n    # Define the objective function\n    def f(x: np.ndarray) - float:\n        return 0.5 * x.T @ A @ x\n\n    # 2. Calculate initial function value\n    f_x0 = f(x0)\n    \n    # If starting at the minimum, reduction is not well-defined,\n    # but we can consider f(x_T) = f(x_0) = 0, so r_T would be 1.\n    # A more sensible interpretation for f(x_0)=0 is r_T=0, as there's 0 to reduce.\n    if f_x0 == 0.0:\n        return 0.0\n\n    # 3. Run steepest descent iterations\n    x = x0.copy()\n    for _ in range(T):\n        # Calculate gradient\n        g = A @ x\n        \n        # Calculate squared norm of the gradient\n        g_dot_g = g.T @ g\n        \n        # If gradient is zero, we have reached the minimum. Stop early.\n        if g_dot_g == 0.0:\n            x = np.zeros_like(x0)  # The minimum is at x=0\n            break\n            \n        # Calculate exact step length alpha\n        # Note: g.T @ A @ g cannot be zero if A is SPD and g is non-zero.\n        alpha = g_dot_g / (g.T @ A @ g)\n        \n        # Update iterate x\n        x = x - alpha * g\n        \n    # 4. Compute final function value and reduction factor\n    f_xT = f(x)\n    r_T = f_xT / f_x0\n    \n    return r_T\n\ndef solve():\n    \"\"\"\n    Defines test cases, runs the steepest descent for each, and prints results.\n    \"\"\"\n    # Initial point for all cases\n    x0 = np.array([3.0, -1.0])\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case S (spherical)\n        {'name': 'S', 'params': {'lambda_min': 10.0, 'lambda_max': 10.0, 'theta': 0.7, 'T': 10}},\n        # Case M (moderately ill-conditioned)\n        {'name': 'M', 'params': {'lambda_min': 1.0, 'lambda_max': 100.0, 'theta': np.pi/6, 'T': 10}},\n        # Case H (highly ill-conditioned)\n        {'name': 'H', 'params': {'lambda_min': 1.0, 'lambda_max': 10**4, 'theta': np.pi/4, 'T': 10}},\n    ]\n\n    results = []\n    for case in test_cases:\n        result = run_steepest_descent(\n            lambda_min=case['params']['lambda_min'],\n            lambda_max=case['params']['lambda_max'],\n            theta=case['params']['theta'],\n            T=case['params']['T'],\n            x0=x0\n        )\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3279006"}]}