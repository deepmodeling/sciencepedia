{"hands_on_practices": [{"introduction": "这项实践将我们带回最小二乘法的核心：最小化误差平方和。你将不再使用标准公式，而是从第一性原理出发，为一个带有特定约束（即多项式必须穿过原点 $p(0)=0$）的二次多项式推导求解过程。该练习旨在加深你对正规方程如何直接从优化问题中产生，以及如何针对带约束模型进行调整的理解。[@problem_id:3223314]", "problem": "考虑由$7$个点 $(x_i,y_i)$ 组成的离散数据集，其中\n$(x_1,y_1)=(-3,18)$, $(x_2,y_2)=(-2,9)$, $(x_3,y_3)=(-1,4)$, $(x_4,y_4)=(0,3)$, $(x_5,y_5)=(1,6)$, $(x_6,y_6)=(2,13)$, $(x_7,y_7)=(3,24)$。这些值是在所列的 $x$ 值上对潜在关系 $y=2x^2+x+3$ 进行采样得到的。设 $p(x)$ 是一个约束为通过原点的二次多项式，即 $p(0)=0$，并将其写作 $p(x)=a x^2 + b x$。\n\n仅使用离散最小二乘逼近的基本定义——即 $p(x)$ 在给定数据上最小化残差平方和 $\\sum_{i=1}^{7} \\big(p(x_i)-y_i\\big)^2$——来确定系数 $a$ 和 $b$，并给出满足 $p(0)=0$ 的最佳拟合二次多项式 $p(x)$ 的显式解析表达式。请以精确形式表示最终答案，无需四舍五入。", "solution": "问题是要求解二次多项式 $p(x) = ax^2 + bx$ 的系数 $a$ 和 $b$，使得该多项式能为给定的7个数据点提供最佳的离散最小二乘逼近。该多项式被约束为通过原点，其形式 $p(0) = a(0)^2 + b(0) = 0$ 满足此约束。\n\n最小二乘法的基本原理是最小化残差平方和 $E$，即模型预测值 $p(x_i)$ 与观测数据值 $y_i$ 之间差的平方和。误差函数 $E(a,b)$ 由下式给出：\n$$E(a,b) = \\sum_{i=1}^{7} \\left(p(x_i) - y_i\\right)^2 = \\sum_{i=1}^{7} \\left(ax_i^2 + bx_i - y_i\\right)^2$$\n为了找到使该函数最小化的 $a$ 和 $b$ 的值，我们必须找到 $E$ 对 $a$ 和 $b$ 的偏导数都等于零的临界点。\n$$ \\frac{\\partial E}{\\partial a} = 0 \\quad \\text{和} \\quad \\frac{\\partial E}{\\partial b} = 0 $$\n计算关于 $a$ 的偏导数：\n$$ \\frac{\\partial E}{\\partial a} = \\sum_{i=1}^{7} 2(ax_i^2 + bx_i - y_i) \\cdot \\frac{\\partial}{\\partial a}(ax_i^2 + bx_i - y_i) = \\sum_{i=1}^{7} 2(ax_i^2 + bx_i - y_i)(x_i^2) $$\n令其等于零并除以 $2$：\n$$ \\sum_{i=1}^{7} (ax_i^4 + bx_i^3 - y_i x_i^2) = 0 $$\n$$ a\\left(\\sum_{i=1}^{7} x_i^4\\right) + b\\left(\\sum_{i=1}^{7} x_i^3\\right) = \\sum_{i=1}^{7} y_i x_i^2 $$\n这是两个正规方程中的第一个。\n\n计算关于 $b$ 的偏导数：\n$$ \\frac{\\partial E}{\\partial b} = \\sum_{i=1}^{7} 2(ax_i^2 + bx_i - y_i) \\cdot \\frac{\\partial}{\\partial b}(ax_i^2 + bx_i - y_i) = \\sum_{i=1}^{7} 2(ax_i^2 + bx_i - y_i)(x_i) $$\n令其等于零并除以 $2$：\n$$ \\sum_{i=1}^{7} (ax_i^3 + bx_i^2 - y_i x_i) = 0 $$\n$$ a\\left(\\sum_{i=1}^{7} x_i^3\\right) + b\\left(\\sum_{i=1}^{7} x_i^2\\right) = \\sum_{i=1}^{7} y_i x_i $$\n这是第二个正规方程。\n\n下一步是根据给定的数据集计算所需的各项总和：\n$(x_1, y_1)=(-3, 18)$, $(x_2, y_2)=(-2, 9)$, $(x_3, y_3)=(-1, 4)$, $(x_4, y_4)=(0, 3)$, $(x_5, y_5)=(1, 6)$, $(x_6, y_6)=(2, 13)$, $(x_7, y_7)=(3, 24)$。\n$x$ 值的集合是 $\\{ -3, -2, -1, 0, 1, 2, 3 \\}$。这个集合关于 $x=0$ 对称，这使得 $x_i$ 的奇次幂之和简化为零。\n\n我们计算各项和：\n$$ \\sum_{i=1}^{7} x_i^2 = (-3)^2 + (-2)^2 + (-1)^2 + 0^2 + 1^2 + 2^2 + 3^2 = 9 + 4 + 1 + 0 + 1 + 4 + 9 = 28 $$\n$$ \\sum_{i=1}^{7} x_i^3 = (-3)^3 + (-2)^3 + (-1)^3 + 0^3 + 1^3 + 2^3 + 3^3 = -27 - 8 - 1 + 0 + 1 + 8 + 27 = 0 $$\n$$ \\sum_{i=1}^{7} x_i^4 = (-3)^4 + (-2)^4 + (-1)^4 + 0^4 + 1^4 + 2^4 + 3^4 = 81 + 16 + 1 + 0 + 1 + 16 + 81 = 196 $$\n接下来，我们计算涉及 $y_i$ 的各项和：\n$$ \\sum_{i=1}^{7} y_i x_i = (18)(-3) + (9)(-2) + (4)(-1) + (3)(0) + (6)(1) + (13)(2) + (24)(3) \\\\ = -54 - 18 - 4 + 0 + 6 + 26 + 72 = -76 + 104 = 28 $$\n$$ \\sum_{i=1}^{7} y_i x_i^2 = (18)(-3)^2 + (9)(-2)^2 + (4)(-1)^2 + (3)(0)^2 + (6)(1)^2 + (13)(2)^2 + (24)(3)^2 \\\\ = 18(9) + 9(4) + 4(1) + 3(0) + 6(1) + 13(4) + 24(9) \\\\ = 162 + 36 + 4 + 0 + 6 + 52 + 216 = 476 $$\n现在，我们将这些数值代入正规方程：\n1. $a(196) + b(0) = 476$\n2. $a(0) + b(28) = 28$\n\n这形成了一个关于 $a$ 和 $b$ 的简单的、解耦的线性方程组。\n由第一个方程可得：\n$$ 196a = 476 \\implies a = \\frac{476}{196} $$\n为了简化分数，我们可以用公因数来除分子和分母。两者都可以被 $4$ 整除：\n$$ a = \\frac{476 \\div 4}{196 \\div 4} = \\frac{119}{49} $$\n现在我们注意到 $119 = 7 \\times 17$ 且 $49 = 7 \\times 7$：\n$$ a = \\frac{7 \\times 17}{7 \\times 7} = \\frac{17}{7} $$\n由第二个方程可得：\n$$ 28b = 28 \\implies b = 1 $$\n系数为 $a = \\frac{17}{7}$ 和 $b = 1$。得到的最佳拟合二次多项式是：\n$$ p(x) = \\frac{17}{7}x^2 + x $$\n题目说明数据来源于关系 $y = 2x^2 + x + 3$。最小二乘拟合精确地得到了系数 $b=1$。这是因为对于一组对称的 $x_i$ 值和由 $y_i=f(x_i)$ 生成的数据，函数偶部和奇部之间的互相关项会消失。系数 $a$ 是 $\\frac{17}{7} \\approx 2.428$，这与原始系数 $2$ 不同，因为模型 $p(x)$ 被强制在 $x=0$ 时为 $0$，而数据点是 $(0,3)$。最小二乘法通过调整曲率参数 $a$ 来最小化所有点的总平方误差，以适应原点处较大的残差。", "answer": "$$\\boxed{p(x) = \\frac{17}{7}x^2 + x}$$", "id": "3223314"}, {"introduction": "虽然最小二乘法本质上是为线性模型设计的，但通过巧妙的变换，其威力可以扩展到特定类型的非线性关系。本实践将演示一种广泛使用的技术——线性化，即通过取对数将像幂律这样的非线性模型转化为线性模型。掌握这一技巧后，你便能将线性最小二乘法应用于更广泛的实际问题中。[@problem_id:3223297]", "problem": "给定一个离散数据点集合，假定其遵循 $y = a x^k$ 形式的幂律关系，其中 $x > 0$ 且 $y > 0$。任务是使用离散最小二乘近似法，通过自然对数将模型线性化为 $\\log(y) = \\log(a) + k \\log(x)$，来估计参数 $a$ 和 $k$。此推导的基本原理是，离散最小二乘近似法找到的参数能够最小化残差平方和，对于线性模型 $m(u) = \\beta_0 + \\beta_1 u$，目标函数定义为 $\\sum_{i=1}^n (v_i - m(u_i))^2$，其中 $(u_i,v_i)$ 是变换后的数据。您必须：\n- 在变换空间中使用自然对数（以 $e$ 为底）构建离散最小二乘问题。\n- 从最小化残差平方和的第一性原理出发，推导出表征线性化模型最小化子的条件，然后实现一个算法，为给定的数据集计算估计值 $\\hat{a}$ 和 $\\hat{k}$。\n- 使用以下测试数据套件，每个套件都指定为 $x$ 值列表和相应的 $y$ 值列表。所有 $x$ 和 $y$ 均为严格正实数。\n\n测试套件：\n1. 顺利路径（具有整数指数的精确幂律）：\n   - $x$: $[1,2,3,4,5]$\n   - $y$: $[3,12,27,48,75]$\n2. 次线性幂律周围的噪声测量：\n   - $x$: $[0.2,0.5,0.8,1.1,1.4,1.7,2.0,2.3]$\n   - $y$: $[0.4145670020285,0.6236681800069,0.8130343166190,0.9439279633550,1.0489213395436,1.1969255615947,1.2600642840744,1.3990405194275]$\n3. 具有小 $x$ 的负指数（宽动态范围，精确值）：\n   - $x$: $[0.01,0.02,0.05,0.1,0.2,0.5]$\n   - $y$: $[199.0535852767,114.2585902281,54.9185573277,31.5478672240,18.1194915919,8.70550563296]$\n4. 具有大动态范围和轻微测量噪声的重复 $x$ 值：\n   - $x$: $[1,1,10,10,100,1000]$\n   - $y$: $[1.25,1.2575,10.0283949630875,9.87945741908125,80.4460614212,620.21920059]$\n\n您的程序必须：\n- 对每个测试用例，通过自然对数变换数据，建立线性最小二乘问题，求解变换空间中线性模型的参数，然后将解映射回原始空间中的估计值 $a$ 和 $k$。\n- 生成单行输出，其中包含所有测试用例的结果，结果为一个逗号分隔的列表，并用方括号括起来。每个测试用例的结果本身是一个 $[\\hat{a},\\hat{k}]$ 形式的双元素列表。每个 $\\hat{a}$ 和 $\\hat{k}$ 必须打印为四舍五入到六位小数的浮点数。\n\n最终输出格式必须严格如下：\n- 一行，除了分隔逗号内数字所需的空格外，没有多余的空格，呈现为 $[[\\hat{a}_1,\\hat{k}_1],[\\hat{a}_2,\\hat{k}_2],[\\hat{a}_3,\\hat{k}_3],[\\hat{a}_4,\\hat{k}_4]]$。\n\n始终使用自然对数（即以 $e$ 为底）。不涉及物理单位、角度或百分比；所有量均为无量纲实数。程序必须是自包含的，不需要任何输入，并且仅使用与通过线性化实现的离散最小二乘近似一致的数值操作。", "solution": "问题是确定幂律模型 $y = a x^k$ 的参数 $a$ 和 $k$，使其最能拟合给定的数据点集 $(x_i, y_i)$，其中 $i=1, \\dots, n$。问题规定数据满足 $x_i > 0$ 且 $y_i > 0$。所用方法是对模型的线性化版本应用离散最小二乘近似。\n\n**步骤 1：问题公式化与线性化**\n给定的幂律关系是：\n$$y = a x^k$$\n为了线性化该模型，我们对两边取自然对数（以 $e$ 为底）。这是允许的，因为 $x$ 和 $y$ 都是严格为正的。\n$$\\ln(y) = \\ln(a x^k)$$\n利用对数的性质 $\\ln(AB) = \\ln(A) + \\ln(B)$ 和 $\\ln(A^p) = p \\ln(A)$，我们得到：\n$$\\ln(y) = \\ln(a) + \\ln(x^k)$$\n$$\\ln(y) = \\ln(a) + k \\ln(x)$$\n该方程具有线性关系的形式。为了明确这一点，我们引入变量替换。令：\n- $v = \\ln(y)$\n- $u = \\ln(x)$\n- $\\beta_0 = \\ln(a)$\n- $\\beta_1 = k$\n\n将这些代入变换后的方程，得到线性模型：\n$$v = \\beta_0 + \\beta_1 u$$\n给定 $n$ 个数据点 $(x_1, y_1), (x_2, y_2), \\dots, (x_n, y_n)$ 的集合，我们可以将它们变换为一组新的点 $(u_1, v_1), (u_2, v_2), \\dots, (u_n, v_n)$，其中 $u_i = \\ln(x_i)$ 且 $v_i = \\ln(y_i)$。我们的目标是找到定义这些变换后点的最佳拟合线的参数 $\\beta_0$ 和 $\\beta_1$。\n\n**步骤 2：离散最小二乘原理**\n离散最小二乘原理指出，最佳拟合线是使残差平方和最小化的那条线。第 $i$ 个点的残差是观测值 $v_i$ 与模型预测值 $\\beta_0 + \\beta_1 u_i$ 之间的差。残差平方和 $S$ 是参数 $\\beta_0$ 和 $\\beta_1$ 的函数：\n$$S(\\beta_0, \\beta_1) = \\sum_{i=1}^{n} (v_i - (\\beta_0 + \\beta_1 u_i))^2$$\n\n**步骤 3：正规方程的推导**\n为了找到最小化 $S$ 的 $\\beta_0$ 和 $\\beta_1$ 的值，我们应用微积分中的标准方法：求 $S$ 关于 $\\beta_0$ 和 $\\beta_1$ 的偏导数，并令它们等于零。这将确定函数 $S$ 的临界点。\n\n首先，关于 $\\beta_0$ 的偏导数：\n$$ \\frac{\\partial S}{\\partial \\beta_0} = \\sum_{i=1}^{n} \\frac{\\partial}{\\partial \\beta_0} (v_i - \\beta_0 - \\beta_1 u_i)^2 = \\sum_{i=1}^{n} 2(v_i - \\beta_0 - \\beta_1 u_i)(-1) $$\n令其为零：\n$$ -2 \\sum_{i=1}^{n} (v_i - \\beta_0 - \\beta_1 u_i) = 0 $$\n$$ \\sum_{i=1}^{n} v_i - \\sum_{i=1}^{n} \\beta_0 - \\sum_{i=1}^{n} \\beta_1 u_i = 0 $$\n$$ \\sum_{i=1}^{n} v_i - n\\beta_0 - \\beta_1 \\sum_{i=1}^{n} u_i = 0 $$\n整理后得到第一个正规方程：\n$$ n\\beta_0 + \\left(\\sum_{i=1}^{n} u_i\\right) \\beta_1 = \\sum_{i=1}^{n} v_i $$\n\n其次，关于 $\\beta_1$ 的偏导数：\n$$ \\frac{\\partial S}{\\partial \\beta_1} = \\sum_{i=1}^{n} \\frac{\\partial}{\\partial \\beta_1} (v_i - \\beta_0 - \\beta_1 u_i)^2 = \\sum_{i=1}^{n} 2(v_i - \\beta_0 - \\beta_1 u_i)(-u_i) $$\n令其为零：\n$$ -2 \\sum_{i=1}^{n} u_i(v_i - \\beta_0 - \\beta_1 u_i) = 0 $$\n$$ \\sum_{i=1}^{n} (u_i v_i - \\beta_0 u_i - \\beta_1 u_i^2) = 0 $$\n$$ \\sum_{i=1}^{n} u_i v_i - \\beta_0 \\sum_{i=1}^{n} u_i - \\beta_1 \\sum_{i=1}^{n} u_i^2 = 0 $$\n整理后得到第二个正规方程：\n$$ \\left(\\sum_{i=1}^{n} u_i\\right) \\beta_0 + \\left(\\sum_{i=1}^{n} u_i^2\\right) \\beta_1 = \\sum_{i=1}^{n} u_i v_i $$\n\n**步骤 4：求解正规方程**\n我们现在得到了一个关于两个未知数 $\\beta_0$ 和 $\\beta_1$ 的两个线性方程组：\n$$\n\\begin{cases}\nn\\beta_0 + (\\sum u_i) \\beta_1 = \\sum v_i \\\\\n(\\sum u_i) \\beta_0 + (\\sum u_i^2) \\beta_1 = \\sum u_i v_i\n\\end{cases}\n$$\n求解这个方程组（例如，使用克莱姆法则或代入法）可以得到 $\\beta_0$ 和 $\\beta_1$ 的解。解的分母是系数矩阵的行列式 $D = n(\\sum u_i^2) - (\\sum u_i)^2$。$\\beta_1$ 的解是：\n$$ \\beta_1 = \\frac{n(\\sum u_i v_i) - (\\sum u_i)(\\sum v_i)}{n(\\sum u_i^2) - (\\sum u_i)^2} $$\n一旦知道了 $\\beta_1$，我们可以从第一个正规方程中求出 $\\beta_0$：\n$$ \\beta_0 = \\frac{1}{n} \\left( \\sum v_i - \\beta_1 \\sum u_i \\right) = \\bar{v} - \\beta_1 \\bar{u} $$\n其中 $\\bar{u} = \\frac{1}{n}\\sum u_i$ 和 $\\bar{v} = \\frac{1}{n}\\sum v_i$ 是算术平均值。\n\n**步骤 5：映射回原始参数**\n原始参数的估计值，记为 $\\hat{a}$ 和 $\\hat{k}$，通过反向变量替换得到：\n$$ \\hat{k} = \\beta_1 $$\n由 $\\beta_0 = \\ln(a)$，我们有：\n$$ \\hat{a} = e^{\\beta_0} $$\n此过程提供了幂律模型参数的估计值，这些估计值最小化了对数变换空间中的平方误差和。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves for power-law parameters a and k for given datasets using\n    discrete least squares on the linearized model.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # 1. Happy path (exact power-law with integer exponent)\n        (\n            [1, 2, 3, 4, 5],\n            [3, 12, 27, 48, 75]\n        ),\n        # 2. Noisy measurements around a sub-linear power-law\n        (\n            [0.2, 0.5, 0.8, 1.1, 1.4, 1.7, 2.0, 2.3],\n            [0.4145670020285, 0.6236681800069, 0.8130343166190, 0.9439279633550,\n             1.0489213395436, 1.1969255615947, 1.2600642840744, 1.3990405194275]\n        ),\n        # 3. Negative exponent with small x (wide dynamic range, exact values)\n        (\n            [0.01, 0.02, 0.05, 0.1, 0.2, 0.5],\n            [199.0535852767, 114.2585902281, 54.9185573277, 31.5478672240,\n             18.1194915919, 8.70550563296]\n        ),\n        # 4. Repeated x values with large dynamic range and mild measurement noise\n        (\n            [1, 1, 10, 10, 100, 1000],\n            [1.25, 1.2575, 10.0283949630875, 9.87945741908125,\n             80.4460614212, 620.21920059]\n        )\n    ]\n\n    results = []\n    for x_vals, y_vals in test_cases:\n        # Convert data to numpy arrays for vectorized operations\n        x = np.array(x_vals, dtype=np.float64)\n        y = np.array(y_vals, dtype=np.float64)\n\n        # Linearize the model: log(y) = log(a) + k*log(x)\n        # Transformed variables: v = log(y), u = log(x)\n        # Linear model: v = beta0 + beta1*u, where beta0 = log(a), beta1 = k\n        u = np.log(x)\n        v = np.log(y)\n\n        # Number of data points\n        n = len(u)\n\n        # Calculate the necessary sums for the normal equations\n        sum_u = np.sum(u)\n        sum_v = np.sum(v)\n        sum_uv = np.sum(u * v)\n        sum_u_sq = np.sum(u**2)\n\n        # Solve the normal equations for beta0 and beta1\n        # beta1 = (n * sum_uv - sum_u * sum_v) / (n * sum_u_sq - sum_u**2)\n        # beta0 = (sum_v - beta1 * sum_u) / n\n        \n        denominator = n * sum_u_sq - sum_u**2\n        \n        # This condition is met if not all x_i are the same\n        if denominator == 0:\n            # Should not happen with the given test data\n            raise ValueError(\"All x values are the same; cannot solve.\")\n\n        beta1 = (n * sum_uv - sum_u * sum_v) / denominator\n        beta0 = (sum_v - beta1 * sum_u) / n\n\n        # Map beta0 and beta1 back to the original power-law parameters a and k\n        # k = beta1\n        # a = exp(beta0)\n        k_hat = beta1\n        a_hat = np.exp(beta0)\n        \n        # Round the results to six decimal places as required\n        a_rounded = round(a_hat, 6)\n        k_rounded = round(k_hat, 6)\n        \n        results.append([a_rounded, k_rounded])\n    \n    # Format the final output string exactly as specified, without extra spaces\n    # Example format: [[a1,k1],[a2,k2],[a3,k3],[a4,k4]]\n    formatted_results = [f\"[{res[0]},{res[1]}]\" for res in results]\n    final_string = f\"[{','.join(formatted_results)}]\"\n\n    print(final_string)\n\nsolve()\n```", "id": "3223297"}, {"introduction": "在数值计算的世界里，由于浮点运算的限制，一个理论上正确的算法在实践中未必能得到精确的结果。本实践将深入探讨数值稳定性这一关键概念，通过比较求解最小二乘问题的两种不同方法：直接的正规方程法和更稳健的QR分解法。你将亲眼看到为何在专业软件中QR分解法通常更受青睐，尤其是在处理矩阵列向量近似线性相关的病态数据时。[@problem_id:3223242]", "problem": "您将处理一个离散数据拟合任务，其中使用离散最小二乘近似将多项式模型拟合到数据点。目标是研究极其接近但又不同的横坐标点如何影响数值稳定性。您必须实现两个求解器，并通过精心选择的度量标准来量化对稳定性的影响。\n\n从以下基础开始：给定一组点 $\\{(x_i,y_i)\\}_{i=1}^n$ 和一个 $m$ 次多项式模型，构造设计矩阵（范德蒙矩阵）$V$，其 $(i,j)$ 项为 $x_i^{j-1}$，其中 $i=1,\\dots,n$ 且 $j=1,\\dots,m+1$。然后，离散最小二乘近似问题就是找到系数 $c \\in \\mathbb{R}^{m+1}$，以最小化残差范数的平方 $\\lVert Vc - y \\rVert_2^2$，其中 $y \\in \\mathbb{R}^n$ 是观测值向量。\n\n您的程序必须：\n- 使用两种方法计算多项式系数：\n  1. 求解与残差平方最小化相关的一阶最优性条件，使用最优性条件的矩阵来获得系数向量 $c_{\\mathrm{NE}}$（这种方法通常被称为通过正规方程求解）。\n  2. 使用正交三角（QR）分解，将 $V$ 写为 $V = QR$，其中 $Q$ 是正交矩阵， $R$ 是上三角矩阵，然后通过回代法求解系数向量 $c_{\\mathrm{QR}}$。\n- 通过计算以下指标来量化数值稳定性：\n  - 范德蒙矩阵 $V$ 的 $2$-范数条件数 $\\kappa_2(V)$。\n  - 矩阵 $V^\\top V$ 的 $2$-范数条件数 $\\kappa_2(V^\\top V)$。\n  - 两个系数向量之间的相对差异：$\\dfrac{\\lVert c_{\\mathrm{NE}} - c_{\\mathrm{QR}} \\rVert_2}{\\lVert c_{\\mathrm{QR}} \\rVert_2}$。\n  - 残差范数的绝对差异：$\\big|\\lVert y - V c_{\\mathrm{NE}} \\rVert_2 - \\lVert y - V c_{\\mathrm{QR}} \\rVert_2\\big|$。\n\n使用基础的三次多项式 $p(x) = 1 - 2x + 3x^2 - x^3$ 作为生成模型，并通过 $y_i = p(x_i) + \\delta_i$ 形成数据，其中 $\\delta_i = 10^{-8}(i+1)$ 对于 $i \\in \\{0,1,\\dots,n-1\\}$。没有随机性；所有值都是确定性的。此问题不涉及物理单位。\n\n为以下测试套件实现解决方案，每个案例指定横坐标 $x_i$ 和次数 $m$：\n- 案例 1（点间距良好，理想情况）：$x = [\\,0.0,\\,0.5,\\,1.0,\\,1.5,\\,2.0\\,]$，$m = 3$。\n- 案例 2（两点极其接近）：$x = [\\,0.0,\\,1.0,\\,1.0 + 10^{-12},\\,2.0,\\,3.0\\,]$，$m = 3$。\n- 案例 3（更接近，接近双精度浮点数的极限（但不相等））：$x = [\\,-1.0,\\,0.0,\\,1.0,\\,1.0 + 10^{-15},\\,2.0\\,]$，$m = 3$。\n\n您的程序必须为每个案例生成一个列表 $[\\,\\kappa_2(V),\\,\\kappa_2(V^\\top V),\\,\\text{relative\\_coefficient\\_difference},\\,\\text{residual\\_norm\\_difference}\\,]$。最终输出应为单行，包含一个用方括号括起来的、由这些列表组成的逗号分隔列表，格式如 $[[a_1,b_1,c_1,d_1],[a_2,b_2,c_2,d_2],[a_3,b_3,c_3,d_3]]$，其中所有的 $a_k$、$b_k$、$c_k$ 和 $d_k$ 都是您的程序计算出的浮点数。\n\n不应读取任何用户输入。输出必须是确定性和可复现的。不使用角度，也不涉及任何物理单位。所有量都以浮点数表示。最终输出格式必须严格为上述指定的单行打印。", "solution": "本任务旨在分析两种不同方法在解决离散多项式最小二乘问题时的数值稳定性：正规方程法和 QR 分解法。稳定性将在输入数据点极其接近的条件下进行评估，这种情况已知会导致问题的设计矩阵出现病态。\n\n离散最小二乘近似问题旨在寻找多项式 $p_m(x) = \\sum_{j=0}^{m} c_j x^j$ 的系数 $c \\in \\mathbb{R}^{m+1}$，以最佳拟合给定的一组 $n$ 个数据点 $\\{(x_i, y_i)\\}_{i=0}^{n-1}$。“最佳拟合”的定义是最小化残差平方和 $S(c) = \\sum_{i=0}^{n-1} (p_m(x_i) - y_i)^2$。该目标函数可以以矩阵形式表示为最小化残差向量的 2-范数的平方：\n$$\nS(c) = \\lVert Vc - y \\rVert_2^2\n$$\n其中 $y = [y_0, y_1, \\dots, y_{n-1}]^\\top$ 是观测值向量，$c = [c_0, c_1, \\dots, c_m]^\\top$ 是多项式系数向量，$V$ 是一个 $n \\times (m+1)$ 的范德蒙矩阵，其元素定义为 $V_{ij} = x_i^j$，其中 $i \\in \\{0, \\dots, n-1\\}$ 且 $j \\in \\{0, \\dots, m\\}$。\n\n问题指定使用生成模型 $p(x) = 1 - 2x + 3x^2 - x^3$ 来创建数据。观测值 $y_i$ 由 $y_i = p(x_i) + \\delta_i$ 给出，其中 $\\delta_i = 10^{-8}(i+1)$ 是一个小的确定性扰动。我们将比较两种求解系数向量 $c$ 的方法。\n\n**方法 1：正规方程**\n\n残差平方 $S(c)$ 的最小值出现在其关于 $c$ 的梯度为零的地方。梯度为：\n$$\n\\nabla_c S(c) = \\nabla_c (Vc - y)^\\top(Vc - y) = \\nabla_c (c^\\top V^\\top V c - 2y^\\top V c + y^\\top y) = 2V^\\top V c - 2V^\\top y\n$$\n将梯度设为零，即 $\\nabla_c S(c) = 0$，得到称为正规方程的线性方程组：\n$$\n(V^\\top V) c = V^\\top y\n$$\n解，我们记为 $c_{\\mathrm{NE}}$，可以通过求解这个 $(m+1) \\times (m+1)$ 的方程组得到。该方法的一个关键问题是矩阵 $V^\\top V$ 的条件数是 $V$ 的条件数的平方：$\\kappa_2(V^\\top V) = [\\kappa_2(V)]^2$。如果 $V$ 是病态的（即具有很大的条件数），$V^\\top V$ 将会是严重病态的。这可能导致计算出的解 $c_{\\mathrm{NE}}$ 对扰动和浮点误差高度敏感。\n\n**方法 2：QR 分解**\n\n此方法避免了显式构造 $V^\\top V$。它依赖于设计矩阵 $V$ 的 QR 分解，将其分解为乘积 $V = QR$，其中 $Q$ 是一个具有标准正交列的 $n \\times (m+1)$ 矩阵（$Q^\\top Q = I_{m+1}$），$R$ 是一个 $(m+1) \\times (m+1)$ 的上三角矩阵。这是“瘦”或“简化”QR 分解。\n\n将 $V=QR$ 代入目标函数，我们得到：\n$$\n\\lVert Vc - y \\rVert_2^2 = \\lVert QRc - y \\rVert_2^2\n$$\n由于 $Q$ 具有标准正交列，从左侧乘以 $Q^\\top$ 不会改变 $Q$ 的列空间中向量的 2-范数。虽然完整的论证涉及完整的 QR 分解，但结果是该最小化问题等价于求解上三角方程组：\n$$\nRc = Q^\\top y\n$$\n该方程组可以通过回代法高效且稳定地求解出系数向量，记为 $c_{\\mathrm{QR}}$。该方法的数值优势在于它操作的矩阵（$Q$ 和 $R$）的条件数与原始矩阵 $V$ 相关，而不是其平方。因此，对于求解最小二乘问题，QR 分解通常比正规方程在数值上稳定得多。\n\n**数值稳定性分析**\n\n为了量化数值稳定性的差异，我们将为每个测试案例计算四个度量标准：\n$1$. 范德蒙矩阵的 2-范数条件数 $\\kappa_2(V)$。这衡量了最小二乘问题本身的敏感性。\n$2$. 正规方程矩阵的 2-范数条件数 $\\kappa_2(V^\\top V)$。这突显了条件数的平方效应。\n$3$. 计算出的系数向量的相对差异 $\\dfrac{\\lVert c_{\\mathrm{NE}} - c_{\\mathrm{QR}} \\rVert_2}{\\lVert c_{\\mathrm{QR}} \\rVert_2}$。这衡量了两种方法得出的解的偏离程度。解 $c_{\\mathrm{QR}}$ 被认为是更准确的基准。\n$4$. 最终残差向量范数的绝对差异 $\\big|\\lVert y - V c_{\\mathrm{NE}} \\rVert_2 - \\lVert y - V c_{\\mathrm{QR}} \\rVert_2\\big|$。一个稳定的算法应产生一个更接近最小化真实残差的解。\n\n这些测试用例旨在展示当横坐标 $x_i$ 变得更接近时，这些指标如何变化，这会增加 $V$ 的病态度。\n\n**每个测试案例的流程：**\n$1$. 给定横坐标 $x$ 和多项式次数 $m=3$，使用 $y_i = (1 - 2x_i + 3x_i^2 - x_i^3) + 10^{-8}(i+1)$ 构建 $y$ 值向量。\n$2$. 构造 $5 \\times 4$ 的范德蒙矩阵 $V$。\n$3$. 求解 $(V^\\top V)c_{\\mathrm{NE}} = V^\\top y$ 得到 $c_{\\mathrm{NE}}$。\n$4$. 计算 QR 分解 $V=QR$ 并求解 $Rc_{\\mathrm{QR}} = Q^\\top y$ 得到 $c_{\\mathrm{QR}}$。\n$5$. 计算并记录四个指定的度量标准。\n此流程将为所有三个提供的测试案例实施。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the discrete least squares problem using Normal Equations and QR factorization\n    for three test cases, and computes metrics to evaluate numerical stability.\n    \"\"\"\n\n    # Generative polynomial: p(x) = 1 - 2x + 3x^2 - x^3\n    # The coefficients are for powers x^0, x^1, x^2, x^3\n    poly_coeffs_gen = np.array([1.0, -2.0, 3.0, -1.0])\n    \n    def p(x_vals, coeffs):\n        \"\"\"Evaluates a polynomial with given coefficients at x_vals.\"\"\"\n        # Using Horner's method implicitly via np.polyval\n        # The coefficients need to be in descending order of power\n        return np.polyval(coeffs[::-1], x_vals)\n\n    test_cases = [\n        # Case 1: Well-spaced points\n        {'x': np.array([0.0, 0.5, 1.0, 1.5, 2.0]), 'm': 3},\n        # Case 2: Two points extremely close\n        {'x': np.array([0.0, 1.0, 1.0 + 1e-12, 2.0, 3.0]), 'm': 3},\n        # Case 3: Closer still, near double precision limits\n        {'x': np.array([-1.0, 0.0, 1.0, 1.0 + 1e-15, 2.0]), 'm': 3},\n    ]\n\n    results = []\n\n    for case in test_cases:\n        x = case['x']\n        m = case['m']\n        n = len(x)\n\n        # 1. Generate data\n        delta = 1e-8 * (np.arange(n) + 1)\n        y = p(x, poly_coeffs_gen) + delta\n\n        # 2. Construct Vandermonde matrix V\n        # V_ij = x_i^j, for j=0,...,m\n        V = np.vander(x, m + 1, increasing=True)\n\n        # 3. Solve with Normal Equations\n        # (V^T V) c = V^T y\n        try:\n            VTV = V.T @ V\n            VTy = V.T @ y\n            c_NE = np.linalg.solve(VTV, VTy)\n        except np.linalg.LinAlgError:\n            # Handle cases where VTV is singular to machine precision\n            c_NE = np.full(m + 1, np.nan)\n\n\n        # 4. Solve with QR factorization\n        # V = QR, solve Rc = Q^T y\n        Q, R = np.linalg.qr(V, mode='reduced')\n        QTY = Q.T @ y\n        c_QR = np.linalg.solve(R, QTY)\n\n        # 5. Calculate metrics\n        # Metric 1: Condition number of V\n        kappa_V = np.linalg.cond(V, 2)\n\n        # Metric 2: Condition number of V^T V\n        kappa_VTV = np.linalg.cond(VTV, 2)\n\n        # Metric 3: Relative difference in coefficients\n        # Use QR solution as the more accurate baseline\n        # Handle NaN case for c_NE\n        if np.any(np.isnan(c_NE)):\n            rel_coeff_diff = np.inf\n        else:\n            norm_c_QR = np.linalg.norm(c_QR, 2)\n            if norm_c_QR == 0:\n                rel_coeff_diff = np.linalg.norm(c_NE - c_QR, 2)\n            else:\n                rel_coeff_diff = np.linalg.norm(c_NE - c_QR, 2) / norm_c_QR\n\n        # Metric 4: Absolute difference in residual norms\n        if np.any(np.isnan(c_NE)):\n            res_norm_diff = np.inf\n        else:\n            resid_NE = y - V @ c_NE\n            resid_QR = y - V @ c_QR\n            norm_resid_NE = np.linalg.norm(resid_NE, 2)\n            norm_resid_QR = np.linalg.norm(resid_QR, 2)\n            res_norm_diff = abs(norm_resid_NE - norm_resid_QR)\n        \n        case_results = [kappa_V, kappa_VTV, rel_coeff_diff, res_norm_diff]\n        results.append(case_results)\n\n    # Final print statement in the exact required format.\n    # str(results).replace(' ', '') ensures the output matches the required format\n    # e.g., \"[[a,b,c],[d,e,f]]\" with no spaces.\n    print(str(results).replace(' ', ''))\n\nsolve()\n\n```", "id": "3223242"}]}