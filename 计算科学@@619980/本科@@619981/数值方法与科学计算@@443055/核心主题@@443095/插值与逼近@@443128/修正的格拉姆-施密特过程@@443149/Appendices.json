{"hands_on_practices": [{"introduction": "我们为什么更倾向于使用修正的格拉姆-施密特（MGS）方法，而不是概念上更简单的经典格拉姆-施密特（CGS）方法？本实践将提供一个直接且可视化的答案。通过从一组“特征脸”基向量中重建图像，你将亲眼见证当处理非常相似的向量时（这是真实数据中的常见情况），CGS会如何因正交性丧失而失败，而MGS则能保持其稳健性。这个练习突显了数值稳定性在实际应用中的关键重要性。[@problem_id:3252976]", "problem": "您将处理一个简化的“特征脸”（eigenfaces）设定，其中每张人脸图像表示为实向量空间 $\\mathbb{R}^d$ 中的一个列向量。设 $A \\in \\mathbb{R}^{d \\times m}$ 表示数据矩阵，其列为 $m$ 个人脸向量。通过对 $A$ 的列求平均值来定义平均人脸向量 $\\mu \\in \\mathbb{R}^d$，并定义均值中心化数据矩阵 $M = A - \\mu \\mathbf{1}^\\top$，其中 $\\mathbf{1} \\in \\mathbb{R}^m$ 表示全一向量。考虑使用两种不同的正交规范化（orthonormalization）过程为 $M$ 的列空间构造一个标准正交基：经典 Gram-Schmidt（CGS）过程和修正 Gram-Schmidt（MGS）过程。两种过程都在 $\\mathbb{R}^d$ 上的标准欧几里得内积下操作，旨在生成列式标准正交基。\n\n从内积空间、正交性和标准正交基的基本定义出发，实现经典 Gram-Schmidt（CGS）过程和修正 Gram-Schmidt（MGS）过程，为 $M$ 的列空间生成标准正交基。利用标准正交基允许构造到子空间上的正交投影这一性质，对每个基计算原始数据的线性重构：将 $M$ 投影到前 $k$ 个基向量的张成空间（span）上，然后加回平均向量 $\\mu$。通过相对 Frobenius 范数重构误差来量化重构质量，该误差定义为残差的 Frobenius 范数除以原始矩阵的 Frobenius 范数。\n\n您的程序必须以科学合理的方式生成模拟“非常相似的人脸”的合成数据集，方法是对一个共同的基础向量施加微小扰动。为保证可复现性，请使用确定性随机数生成。对于每个数据集，计算均值中心化矩阵 $M$，使用 CGS 和 MGS 构造标准正交基，并对每个指定的 $k$ 值，计算 CGS 基和 MGS 基的相对 Frobenius 重构误差。报告每个数据集和每个 $k$ 值的误差差异 $\\mathrm{err}_{\\mathrm{CGS}} - \\mathrm{err}_{\\mathrm{MGS}}$。\n\n请使用以下测试套件。在所有情况下，令 $d = 64$，以使人脸向量对应于 $8 \\times 8$ 大小的展平图像。令 $k$ 的取值范围为 $\\{1, 2, 3\\}$。\n\n- 测试用例 1（一般情况，中等相似度人脸）：\n  - 参数：$d = 64$，$m = 10$，噪声标准差 $\\sigma = 10^{-3}$，随机种子 $42$，重复标志（duplicates flag）设置为 false。\n- 测试用例 2（极度相似人脸，接近重复的列以考验数值稳定性）：\n  - 参数：$d = 64$，$m = 10$，噪声标准差 $\\sigma = 10^{-8}$，随机种子 $7$，重复标志设置为 true（一半的列被构造成与前面某一列几乎重复）。\n- 测试用例 3（边界情况，图像少且相似度高，可能存在有效低秩）：\n  - 参数：$d = 64$，$m = 4$，噪声标准差 $\\sigma = 10^{-6}$，随机种子 $99$，重复标志设置为 true。\n\n科学真实性要求：\n- 基础人脸向量必须是 $8 \\times 8$ 网格上的一个平滑、非平凡的模式，例如图像中心二维高斯分布与平缓水平梯度的混合，并缩放到单位 $\\ell_2$ 范数。\n- 每张人脸通过对基础向量应用微小的全局强度缩放和偏置，并添加标准差为 $\\sigma$ 的独立高斯噪声来生成。\n- 当重复标志为 true 时，通过与一个接近 $1$ 的系数进行线性组合，并添加尺度为 $\\sigma$ 的噪声，将一半的列构造成与前面某一列几乎重复。\n\n误差度量：\n- 对于选定的 $k \\in \\{1, 2, 3\\}$ 和一个标准正交基 $Q \\in \\mathbb{R}^{d \\times r}$（其中 $r \\leq m$），将 $M$ 投影到 $Q$ 的前 $k$ 列的张成空间上，然后加回 $\\mu$，以此来定义 $A$ 的重构。计算相对 Frobenius 范数重构误差，即 $A$ 与其重构之差的 Frobenius 范数除以 $A$ 的 Frobenius 范数。\n\n实现约束：\n- 实现 CGS 和 MGS 以从 $M$ 生成标准正交基。如果在正交规范化过程中，某个候选向量的欧几里得范数低于一个很小的阈值，则应将其丢弃，以避免除以在有限精度下被视为零的数。使用一个适合双精度算术的固定阈值。\n- 仅使用 $\\mathbb{R}^d$ 的欧几里得内积结构以及标准正交基和正交投影的性质。不要使用主成分分析（PCA）或奇异值分解（SVD）。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含一个由三个项目组成的逗号分隔列表。每个项目对应一个测试用例，其本身是一个包含三个浮点数的列表 $[\\Delta_1, \\Delta_2, \\Delta_3]$，其中 $\\Delta_k = \\mathrm{err}_{\\mathrm{CGS}}(k) - \\mathrm{err}_{\\mathrm{MGS}}(k)$，对于 $k \\in \\{1,2,3\\}$。输出行必须具有确切的格式：\n  - 示例结构：$[[\\Delta_1,\\Delta_2,\\Delta_3],[\\Delta_1,\\Delta_2,\\Delta_3],[\\Delta_1,\\Delta_2,\\Delta_3]]$。\n\n不涉及物理单位。不使用角度。不得使用百分比；误差必须表示为十进制浮点数。程序必须是自包含的，不需要用户输入，并且仅依赖于指定的运行时环境。", "solution": "我们从欧几里得内积空间和正交投影的基本定义开始。设 $\\mathbb{R}^d$ 配备其标准内积 $\\langle x, y \\rangle = \\sum_{i=1}^d x_i y_i$，并定义欧几里得范数 $\\|x\\|_2 = \\sqrt{\\langle x, x \\rangle}$。$\\mathbb{R}^d$ 中的一组向量 $\\{q_1, q_2, \\dots, q_r\\}$ 是标准正交的，如果对于 $i \\neq j$ 有 $\\langle q_i, q_j \\rangle = 0$，并且对于所有 $i$ 有 $\\|q_i\\|_2 = 1$。给定一个标准正交集 $\\{q_1, \\dots, q_r\\}$，到 $\\mathrm{span}\\{q_1, \\dots, q_r\\}$ 上的正交投影 $P$ 将向量 $x \\in \\mathbb{R}^d$ 映射到该子空间中唯一的向量，该向量使 $\\|x - y\\|_2$ 在子空间中的所有 $y$ 上最小化；该投影的特征是 $x - P x$ 与该子空间正交。对于一个标准正交基矩阵 $Q \\in \\mathbb{R}^{d \\times r}$，其列为 $\\{q_i\\}$，向量 $x$ 的投影可以使用内积系数 $\\langle q_i, x \\rangle$ 和标准正交关系来构造。\n\n在此基础上，我们考虑为均值中心化矩阵 $M = A - \\mu \\mathbf{1}^\\top$ 的列空间构造标准正交基，其中 $A \\in \\mathbb{R}^{d \\times m}$ 是数据矩阵，$\\mu \\in \\mathbb{R}^d$ 是 $A$ 的列的平均向量。我们比较两种过程：\n\n- 经典 Gram-Schmidt (CGS)：从概念上讲，在第 $j$ 步，我们通过使用与原始输入向量的内积，从第 $j$ 个输入向量中移除其在前序已构造的标准正交向量上的投影，从而形成其正交分量，然后如果结果的范数足够大，则将其归一化为单位范数。在精确算术中，这将产生一个与输入向量张成相同子空间的标准正交集。在浮点算术中，当输入向量接近线性相关时，CGS 可能会失去正交性，因为减去几乎相等的量会加剧舍入误差，并且使用与原始向量计算的内积并不能减轻累积的数值相关性。\n- 修正 Gram-Schmidt (MGS)：在第 $j$ 步，我们沿着每个先前构造的标准正交向量，迭代地移除当前工作向量的分量，每次减法后都更新工作向量，然后如果最终结果的范数足够大，则对其进行归一化。在浮点算术中，MGS 在数值上更稳定，因为其再正交化的方式可以减少舍入误差的增长，并更好地保持所构造基向量之间的正交性。\n\n两种过程都依赖于内积空间内正交性和归一化的核心定义。基于投影的重构利用了这样一个性质：对于一个标准正交基，最小二乘意义下的最佳近似是通过投影到所选基向量的张成空间上获得的。具体来说，如果 $Q \\in \\mathbb{R}^{d \\times r}$ 具有标准正交列，并且我们选择 $k \\leq r$，那么将 $M$ 投影到 $Q$ 的前 $k$ 列的张成空间上，会得到子空间中的一个近似 $\\widehat{M}$。重构后的数据矩阵则为 $\\widehat{A} = \\mu \\mathbf{1}^\\top + \\widehat{M}$，其重构质量通过相对 Frobenius 范数误差来量化\n$$\n\\mathrm{err} = \\frac{\\|A - \\widehat{A}\\|_F}{\\|A\\|_F},\n$$\n其中 $\\|\\cdot\\|_F$ 表示 Frobenius 范数 $\\|X\\|_F = \\sqrt{\\sum_{i,j} X_{ij}^2}$。\n\n算法设计：\n- 生成模拟“非常相似的人脸”的合成数据集。在一个 $8 \\times 8$ 的网格上使用一个平滑、非平凡的模式（例如，以图像中点为中心的二维高斯分布加上一个平缓的水平梯度）构造一个基础人脸向量，然后将其缩放到单位 $\\ell_2$ 范数。每个脸部向量都是通过对基础向量应用一个小的全局强度缩放和偏置，并添加标准差为 $\\sigma$ 的独立高斯噪声而获得的。当重复标志为 true 时，通过将某一列乘以一个接近 1 的系数并添加尺度为 $\\sigma$ 的小高斯噪声，创建一半的列使其成为前面某一列的近似副本。这将产生几乎线性相关的列，从而对数值稳定性构成考验。\n- 计算平均向量 $\\mu$（通过对 $A$ 的列求平均）并形成 $M = A - \\mu \\mathbf{1}^\\top$。\n- 实现 CGS 和 MGS 从 $M$ 生成标准正交基。为确保在有限精度算术中的科学真实性，如果候选向量的范数低于一个小阈值（例如，在双精度下与机器精度相当的容差），则丢弃该向量以避免不稳定的归一化。\n- 对于每个 $k \\in \\{1, 2, 3\\}$，通过将 $M$ 投影到 $Q$ 的前 $k$ 个基向量的张成空间上（对于 CGS 和 MGS 两种基），然后加回均值 $\\mu$ 来重构 $\\widehat{A}$，并计算相对 Frobenius 重构误差。如果可用的基向量少于 $k$ 个，则使用所有可用的基向量。\n- 报告每个 $k$ 和每个测试用例的差异 $\\Delta_k = \\mathrm{err}_{\\mathrm{CGS}}(k) - \\mathrm{err}_{\\mathrm{MGS}}(k)$。\n\n为什么这个比较有意义：\n- 在精确算术中，CGS 和 MGS 都生成张成相同子空间的标准正交基，并且对于一个固定的 $k$，到前 $k$ 个基向量张成空间上的正交投影仅取决于该子空间，而不取决于特定的标准正交基。然而，在浮点算术中，当向量接近线性相关时，CGS 可能会因失去正交性而受到影响，导致生成的基并非完全标准正交，因此其投影会偏离理想情况。已知 MGS 可以减轻这种效应。因此，在非常相似的人脸数据集上，相对于 CGS，MGS 通常能为较小的 $k$ 值产生更准确的重构，而差异 $\\Delta_k$ 则为这种稳定性差距提供了定量评估。\n\n测试套件覆盖范围：\n- 测试用例 1 运用了一个通用的“理想路径”，包含中等相似度的人脸（$d = 64$, $m = 10$, $\\sigma = 10^{-3}$），在这种情况下，CGS 和 MGS 的表现应该相当，但由于有限精度，可能会出现微小差异。\n- 测试用例 2 引入了极其相似的人脸和近似重复的列（$d = 64$, $m = 10$, $\\sigma = 10^{-8}$，重复标志为 true），通过强烈的正交性损失风险来考验 CGS，并突显 MGS 的稳定性。\n- 测试用例 3 提供了一个边界情况，图像少且相似度高（$d = 64$, $m = 4$, $\\sigma = 10^{-6}$，重复标志为 true），可能产生一个有效低秩的 $M$，因此请求 $k = 3$ 会超过可靠基向量的数量，从而确保实现能正确处理截断的基。\n\n程序将三个测试用例的结果汇总到单行输出中，其格式严格为 $[[\\Delta_1,\\Delta_2,\\Delta_3],[\\Delta_1,\\Delta_2,\\Delta_3],[\\Delta_1,\\Delta_2,\\Delta_3]]$，其中每个内部列表对应一个测试用例，其中的条目对应于 $k \\in \\{1,2,3\\}$。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef make_base_face_vector(d: int) -> np.ndarray:\n    \"\"\"\n    Create a smooth, nontrivial base face vector over an 8x8 grid:\n    a mixture of a 2D Gaussian centered at the image midpoint and a gentle horizontal gradient.\n    The resulting vector is scaled to unit l2 norm.\n    \"\"\"\n    side = int(np.sqrt(d))\n    if side * side != d:\n        raise ValueError(\"d must be a perfect square for an 8x8 (or side x side) image grid.\")\n    x = np.linspace(0.0, 1.0, side)\n    y = np.linspace(0.0, 1.0, side)\n    xx, yy = np.meshgrid(x, y, indexing=\"xy\")\n    # 2D Gaussian centered near (0.5, 0.5) with moderate spread\n    sigma = 0.18\n    gaussian = np.exp(-((xx - 0.5)**2 + (yy - 0.5)**2) / (2.0 * sigma**2))\n    # Gentle horizontal gradient\n    gradient = 0.3 * xx\n    base = gaussian + gradient\n    base_vec = base.reshape(d).astype(np.float64)\n    # Normalize to unit l2 norm\n    norm = np.linalg.norm(base_vec)\n    if norm == 0.0:\n        return base_vec\n    return base_vec / norm\n\n\ndef generate_faces(d: int, m: int, sigma: float, seed: int, duplicates: bool) -> np.ndarray:\n    \"\"\"\n    Generate a synthetic face dataset A of shape (d, m) with \"very similar faces\":\n    - Start from a smooth base face vector.\n    - Apply small global intensity scaling and bias.\n    - Add independent Gaussian noise with standard deviation sigma.\n    - If duplicates is True, make half of the columns near duplicates of an earlier column.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    base = make_base_face_vector(d)\n    A = np.zeros((d, m), dtype=np.float64)\n\n    # Generate m faces with small global variations and noise.\n    for j in range(m):\n        gain = rng.uniform(-0.05, 0.05)  # small scaling\n        bias = rng.uniform(-0.02, 0.02)  # small bias\n        noise = rng.normal(loc=0.0, scale=sigma, size=d)\n        # Add a tiny structured variation correlated with horizontal gradient to avoid triviality\n        side = int(np.sqrt(d))\n        xx = np.linspace(0.0, 1.0, side)\n        grad_x = np.repeat(xx[np.newaxis, :], side, axis=0).reshape(d)\n        structured = 0.01 * bias * grad_x\n        A[:, j] = base * (1.0 + gain) + bias + structured + noise\n\n    if duplicates and m >= 2:\n        # Make half the columns (from index m//2 onward) near duplicates of earlier ones.\n        # Use a coefficient close to 1 and add small noise of scale sigma.\n        base_idx = 0\n        coeff = 0.999999\n        for j in range(m // 2, m):\n            noise = rng.normal(loc=0.0, scale=sigma, size=d)\n            A[:, j] = coeff * A[:, base_idx] + noise\n            # Cycle base_idx to duplicate different earlier columns if possible\n            base_idx = (base_idx + 1) % (m // 2 if m // 2 > 0 else 1)\n\n    return A\n\n\ndef mean_center_columns(A: np.ndarray) -> tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Compute the mean vector mu and mean-centered matrix M = A - mu * 1^T.\n    \"\"\"\n    mu = np.mean(A, axis=1)\n    M = A - mu[:, None]\n    return mu, M\n\n\ndef cgs_orthonormal_basis(M: np.ndarray, tol: float = 1e-12) -> np.ndarray:\n    \"\"\"\n    Classical Gram-Schmidt (CGS) to form an orthonormal basis of the column space of M.\n    At step j, compute inner products with the original column M[:, j] for all existing basis vectors,\n    subtract the combined projection once, then normalize if norm exceeds tol.\n    \"\"\"\n    d, m = M.shape\n    Q_cols = []\n    for j in range(m):\n        a_j = M[:, j].copy()\n        # Compute projection coefficients using the original a_j\n        coeffs = [float(np.dot(q, a_j)) for q in Q_cols]\n        # Subtract combined projection\n        v = a_j.copy()\n        for coeff, q in zip(coeffs, Q_cols):\n            v -= coeff * q\n        nrm = np.linalg.norm(v)\n        if nrm > tol:\n            q_new = v / nrm\n            Q_cols.append(q_new)\n        # If nrm = tol, discard as numerically zero\n    if len(Q_cols) == 0:\n        return np.zeros((d, 0), dtype=np.float64)\n    return np.column_stack(Q_cols)\n\n\ndef mgs_orthonormal_basis(M: np.ndarray, tol: float = 1e-12) -> np.ndarray:\n    \"\"\"\n    Modified Gram-Schmidt (MGS) to form an orthonormal basis of the column space of M.\n    At step j, iteratively remove components of the working vector along each existing basis vector,\n    updating the working vector after each subtraction, then normalize if norm exceeds tol.\n    \"\"\"\n    d, m = M.shape\n    Q_cols = []\n    for j in range(m):\n        v = M[:, j].copy()\n        for i in range(len(Q_cols)):\n            alpha = float(np.dot(Q_cols[i], v))\n            v -= alpha * Q_cols[i]\n        nrm = np.linalg.norm(v)\n        if nrm > tol:\n            q_new = v / nrm\n            Q_cols.append(q_new)\n        # If nrm = tol, discard as numerically zero\n    if len(Q_cols) == 0:\n        return np.zeros((d, 0), dtype=np.float64)\n    return np.column_stack(Q_cols)\n\n\ndef reconstruction_error_relative(A: np.ndarray, Q: np.ndarray, mu: np.ndarray, k: int) -> float:\n    \"\"\"\n    Compute the relative Frobenius norm reconstruction error when projecting onto the span\n    of the first k columns of Q (assumed columns are orthonormal), then adding back mu.\n    If fewer than k basis vectors are available, use all available vectors.\n    \"\"\"\n    M = A - mu[:, None]\n    if Q.shape[1] == 0 or k == 0:\n        M_hat = np.zeros_like(M)\n    else:\n        kk = min(k, Q.shape[1])\n        Qk = Q[:, :kk]\n        # Orthogonal projection onto span(Qk)\n        M_hat = Qk @ (Qk.T @ M)\n    A_hat = mu[:, None] + M_hat\n    num = np.linalg.norm(A - A_hat, ord='fro')\n    den = np.linalg.norm(A, ord='fro')\n    # Avoid division by zero (shouldn't happen with nontrivial data)\n    return float(num / den) if den != 0.0 else 0.0\n\n\ndef run_case(d: int, m: int, sigma: float, seed: int, duplicates: bool, k_list: list[int]) -> list[float]:\n    \"\"\"\n    Generate dataset, compute CGS and MGS bases, and return a list of differences\n    err_CGS(k) - err_MGS(k) for each k in k_list.\n    \"\"\"\n    A = generate_faces(d=d, m=m, sigma=sigma, seed=seed, duplicates=duplicates)\n    mu, M = mean_center_columns(A)\n    Q_cgs = cgs_orthonormal_basis(M, tol=1e-12)\n    Q_mgs = mgs_orthonormal_basis(M, tol=1e-12)\n\n    diffs = []\n    for k in k_list:\n        err_cgs = reconstruction_error_relative(A, Q_cgs, mu, k)\n        err_mgs = reconstruction_error_relative(A, Q_mgs, mu, k)\n        diffs.append(err_cgs - err_mgs)\n    return diffs\n\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test case 1: general, moderately similar faces\n        {\"d\": 64, \"m\": 10, \"sigma\": 1e-3, \"seed\": 42, \"duplicates\": False},\n        # Test case 2: extremely similar faces, near-duplicate columns\n        {\"d\": 64, \"m\": 10, \"sigma\": 1e-8, \"seed\": 7, \"duplicates\": True},\n        # Test case 3: boundary case, few images, high similarity\n        {\"d\": 64, \"m\": 4, \"sigma\": 1e-6, \"seed\": 99, \"duplicates\": True},\n    ]\n    k_list = [1, 2, 3]\n\n    results = []\n    for case in test_cases:\n        diffs = run_case(\n            d=case[\"d\"], m=case[\"m\"], sigma=case[\"sigma\"],\n            seed=case[\"seed\"], duplicates=case[\"duplicates\"],\n            k_list=k_list\n        )\n        results.append(diffs)\n\n    # Final print statement in the exact required format.\n    # Print a single line: a list of three lists, each with three floats.\n    print(f\"[{','.join('[' + ','.join(map(str, res)) + ']' for res in results)}]\")\n\nsolve()\n```", "id": "3252976"}, {"introduction": "在见证了CGS的实际失败之后，让我们现在来量化MGS在其自身极限下的数值行为。在这个练习中，你将构建一组由小参数 $\\delta$ 控制的、几乎线性相关的向量。通过检验 $Q R$ 分解中 $R$ 矩阵的对角线元素和生成的 $Q$ 矩阵的正交性，你将对MGS在压力下的表现以及其数值精度开始失效的临界点，获得更深刻的定量理解。[@problem_id:3253014]", "problem": "你将研究修正的 Gram-Schmidt (MGS) 正交规范化过程在应用于一个通过构造而近似线性相关的列序列时的数值行为。在具有标准内积的实欧几里得空间中进行全部工作，其中向量 $x$ 在单位向量 $u$ 上的投影为 $u(u^{\\top}x)$，一个标准正交集 $\\{q_1,\\dots,q_k\\}$ 满足当 $i\\neq j$ 时 $q_i^{\\top}q_j = 0$ 且 $\\lVert q_i\\rVert_2 = 1$。MGS 算法是通过相继减去已计算出的标准正交向量的投影来实现正交规范化定义的迭代过程。\n\n根据以下规则构造一个矩阵 $A\\in\\mathbb{R}^{n\\times m}$，其中 $n=m$：\n- 固定 $n=m=12$。\n- 令第一列 $a_1\\in\\mathbb{R}^n$ 由 $(a_1)_j = 1/j$ 给出，其中 $j=1,\\dots,n$。\n- 对于给定的标量扰动参数 $\\delta\\in\\mathbb{R}$，通过 $a_{i+1} = a_i + \\delta e_i$ 递归定义各列，其中 $i=1,\\dots,m-1$，而 $e_i$ 是 $\\mathbb{R}^n$ 中的第 $i$ 个标准基向量。\n\n对于每个固定的 $\\delta$，计算修正的 Gram-Schmidt (MGS) 分解 $A=QR$，其中 $Q\\in\\mathbb{R}^{n\\times m}$ 的列 $q_i$ 在精确算术中是标准正交的，而 $R\\in\\mathbb{R}^{m\\times m}$ 是上三角矩阵，当第 $i$ 步 MGS 产生非零向量时，其对角线元素 $R_{ii}>0$。仅使用标准双精度浮点运算。\n\n你的程序必须：\n1. 从正交规范化的核心定义开始实现 MGS：在第 $i$ 步，从当前向量中减去其在先前计算出的 $q_j$（其中 $j < i$）上的投影。\n2. 对于每个 $\\delta$，从 MGS 分解得到的矩阵 $Q(\\delta)$ 和 $R(\\delta)$ 中计算以下两个诊断指标：\n   - $R$ 的最小正对角线元素：$\\min\\{R_{ii}(\\delta) \\mid R_{ii}(\\delta) > 0, \\, i=1,\\dots,m\\}$。\n   - 相邻列内积的最大值：$\\max\\{|q_i(\\delta)^{\\top}q_{i+1}(\\delta)| \\mid \\lVert q_i(\\delta)\\rVert_2 > 0, \\lVert q_{i+1}(\\delta)\\rVert_2 > 0, \\, i=1,\\dots,m-1\\}$。\n3. 对以下测试套件中的每个 $\\delta$ 值执行此过程：$\\delta \\in \\{10^{-1}, 10^{-8}, 10^{-12}, 10^{-16}, 10^{-20}\\}$。\n4. 您的程序应生成单行输出，其中包含一个列表的列表。每个内部列表应包含三个浮点数：$[\\delta, \\min\\{R_{ii}\\}, \\max\\{|q_i^{\\top}q_{i+1}|\\}]$。输出行的格式必须为：`[[d1, m1, a1], [d2, m2, a2], ...]`，且不含任何额外文本。", "solution": "用户提供的问题已经过分析和验证。所有给定条件、约束和定义在科学上都是合理的、一致的且适定的。该问题是线性代数中的一个标准数值实验，旨在研究修正的 Gram-Schmidt (MGS) 过程在应用于近似奇异矩阵时的数值稳定性。这是一个有效的问题。\n\n解决方案分三个主要阶段进行：构建测试矩阵 $A(\\delta)$，应用修正的 Gram-Schmidt 正交规范化获得分解 $A=QR$，以及从得到的矩阵 $Q$ 和 $R$ 计算指定的数值诊断指标。\n\n**1. 矩阵构建**\n\n对于测试套件中的每个标量扰动参数 $\\delta$，我们构造一个维度为 $n=m=12$ 的方阵 $A \\in \\mathbb{R}^{n \\times m}$。\n第一列 $a_1 \\in \\mathbb{R}^{12}$ 由其分量 $(a_1)_j = 1/j$ 定义，其中 $j=1, \\dots, 12$。\n后续列是递归生成的。对于 $i=1, \\dots, 11$，第 $(i+1)$ 列定义为 $a_{i+1} = a_i + \\delta e_i$，其中 $e_i$ 是第 $i$ 个标准基向量（一个在第 $i$ 个位置为 1，其余位置为 0 的向量）。\n\n这种构造确保了对于小的 $\\delta$ 值，相邻的列 $a_i$ 和 $a_{i+1}$ 非常接近，使得列向量集合近似线性相关。因此，当 $\\delta \\to 0$ 时，矩阵 $A$ 变得越来越病态。这是测试正交规范化算法数值极限的关键机制。对于小于相对机器精度（对于双精度浮点数约为 $10^{-16}$）的 $\\delta$ 值，项 $\\delta e_i$ 可能会在浮点加法 $a_i + \\delta e_i$ 过程中丢失，导致数值上完全相同的列，即 $a_{i+1} = a_i$。\n\n**2. 修正的 Gram-Schmidt (MGS) 算法**\n\nGram-Schmidt 过程将一组线性无关的向量 $\\{a_1, \\dots, a_m\\}$ 转换为一个张成相同子空间的标准正交集 $\\{q_1, \\dots, q_m\\}$。该过程可以表示为矩阵分解 $A=QR$，其中 $Q$ 具有标准正交列 $q_i$，而 $R$ 是一个上三角矩阵。\n\n修正的 Gram-Schmidt (MGS) 算法是一种特定的实现方式，以其相对于经典 Gram-Schmidt (CGS) 算法更优越的数值稳定性而闻名。关键区别在于运算顺序。MGS 在每个新的标准正交向量 $q_i$ 计算出来后，立即将其余的向量对其进行正交化。这减少了舍入误差的累积。\n\nMGS 算法的实现如下，我们从矩阵 $A$ 的一个工作副本（表示为 $V$）开始：\n初始化 $V=A$。\n对于 $i = 1, \\dots, m$：\n1.  计算当前向量 $v_i$ 的 $2$-范数：$R_{ii} = \\lVert v_i \\rVert_2$。\n2.  如果 $R_{ii} > 0$，则将向量归一化以产生下一个标准正交向量：$q_i = v_i / R_{ii}$。\n3.  如果 $R_{ii} = 0$，则向量 $a_i$ 与前面的向量线性相关。得到的标准正交向量 $q_i$ 是一个零向量，并且此步骤不执行投影。\n4.  对于所有后续向量 $v_j$（其中 $j = i+1, \\dots, m$），减去它们在新标准正交向量 $q_i$ 上的投影。这通过首先计算投影系数 $R_{ij} = q_i^{\\top} v_j$，然后更新向量 $v_j \\leftarrow v_j - R_{ij} q_i$ 来完成。\n\n此过程填充了 $Q$ 的列和 $R$ 的上三角项。\n\n**3. 诊断计算**\n\n对于每个 $\\delta$ 值，在计算出 $Q(\\delta)$ 和 $R(\\delta)$ 后，计算两个诊断量以评估数值行为：\n\n-   **$R$ 的最小正对角线元素 ($\\min\\{R_{ii}\\}$):** 对角线元素 $R_{ii}$ 表示每个向量在与集合中所有前面的向量正交化后的范数。一个小的 $R_{ii}$ 值表明原始向量 $a_i$ 近似是向量 $\\{a_1, \\dots, a_{i-1}\\}$ 的线性组合。我们计算 $\\min\\{R_{ii}(\\delta) \\mid R_{ii}(\\delta)>0, \\, i=1,\\dots,m\\}$。该指标衡量了 $A(\\delta)$ 列集合的线性相关性程度。\n\n-   **相邻列内积的最大值 ($\\max\\{|q_i^{\\top}q_{i+1}|\\}$):** 在精确算术中，$Q$ 的列是完全标准正交的，即当 $i \\neq j$ 时 $q_i^{\\top}q_j = 0$。在有限精度算术中，舍入误差会导致正交性的损失。这通过计算结果向量的内积来衡量。我们计算 $\\max\\{|q_i(\\delta)^{\\top}q_{i+1}(\\delta)| \\mid \\lVert q_i(\\delta)\\rVert_2>0,\\ \\lVert q_{i+1}(\\delta)\\rVert_2>0,\\ i=1,\\dots,m-1\\}$。该值量化了相邻列之间正交性的最坏情况损失，这是数值不稳定的一个已知指标。\n\n通过对指定的 $\\delta$ 值测试套件执行此过程，我们可以观察到当输入矩阵接近奇异时，MGS 分解的数值质量如何下降。我们预计随着 $\\delta$ 变小，$\\min\\{R_{ii}\\}$ 会减小，而 $\\max\\{|q_i^{\\top}q_{i+1}|\\}$ 会增大。对于低于机器精度的 $\\delta$ 值，我们预计会出现崩溃，即列在数值上变得相同，导致 $R$ 的对角线上出现零项，并且正交性度量值为零。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem by constructing matrices for different delta values,\n    applying Modified Gram-Schmidt, and computing the required diagnostics.\n    \"\"\"\n\n    def modified_gram_schmidt(A):\n        \"\"\"\n        Performs Modified Gram-Schmidt orthonormalization on the columns of matrix A.\n\n        Args:\n            A (np.ndarray): The input matrix with dimensions n x m.\n\n        Returns:\n            tuple[np.ndarray, np.ndarray]: A tuple (Q, R) where Q is an n x m matrix\n            with orthonormal columns and R is an m x m upper triangular matrix.\n        \"\"\"\n        n, m = A.shape\n        # Create a working copy of A to be modified in place.\n        V = A.copy()\n        Q = np.zeros((n, m), dtype=float)\n        R = np.zeros((m, m), dtype=float)\n\n        for i in range(m):\n            # Compute the norm of the current vector.\n            R[i, i] = np.linalg.norm(V[:, i])\n\n            # Normalize to get the i-th orthonormal vector, if possible.\n            if R[i, i] > 0:\n                Q[:, i] = V[:, i] / R[i, i]\n            else:\n                # The column is linearly dependent on previous columns.\n                # Q[:, i] remains a zero vector, and no projections can be done.\n                continue\n\n            # Orthogonalize all subsequent vectors against the new orthonormal vector.\n            for j in range(i + 1, m):\n                R[i, j] = np.dot(Q[:, i], V[:, j])\n                V[:, j] -= R[i, j] * Q[:, i]\n\n        return Q, R\n\n    # Define the test cases from the problem statement.\n    test_cases = [1e-1, 1e-8, 1e-12, 1e-16, 1e-20]\n    n = 12\n    m = 12\n    results = []\n\n    for delta in test_cases:\n        # 1. Construct the matrix A for the given delta.\n        A = np.zeros((n, m), dtype=float)\n        \n        # Define the first column a_1.\n        A[:, 0] = 1.0 / np.arange(1, n + 1)\n        \n        # Recursively define subsequent columns a_{i+1} = a_i + delta * e_i.\n        for i in range(m - 1):\n            # e_i is the i-th standard basis vector (0-indexed).\n            e_i = np.zeros(n, dtype=float)\n            e_i[i] = 1.0\n            A[:, i + 1] = A[:, i] + delta * e_i\n\n        # 2. Perform Modified Gram-Schmidt factorization.\n        Q, R = modified_gram_schmidt(A)\n\n        # 3. Compute the two required diagnostics.\n        \n        # Diagnostic 1: Minimum positive diagonal entry of R.\n        diag_R = np.diag(R)\n        positive_diagonals = diag_R[diag_R > 0]\n        if positive_diagonals.size > 0:\n            min_diag = np.min(positive_diagonals)\n        else:\n            min_diag = 0.0\n\n        # Diagnostic 2: Maximum adjacent-column inner product magnitude of Q.\n        max_adj_dot = 0.0\n        for i in range(m - 1):\n            q_i = Q[:, i]\n            q_i_plus_1 = Q[:, i + 1]\n            \n            # Per problem spec, consider only non-zero vectors.\n            if np.linalg.norm(q_i) > 0 and np.linalg.norm(q_i_plus_1) > 0:\n                dot_product = np.abs(np.dot(q_i, q_i_plus_1))\n                if dot_product > max_adj_dot:\n                    max_adj_dot = dot_product\n        \n        result_tuple = [delta, min_diag, max_adj_dot]\n        results.append(result_tuple)\n\n    # Final print statement in the exact required format.\n    # The format is [[d1, m1, a1],[d2, m2, a2],...], which is created\n    # by joining the string representation of each inner list.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3253014"}, {"introduction": "现在我们已经理解了MGS的稳定性，可以将其作为一个可靠的工具来解决其他具有挑战性的数值问题。本实践将处理多项式插值问题，其中使用范德蒙德矩阵的标准方法是出了名的病态。你将使用MGS对范德蒙德矩阵进行稳定的QR分解，从而准确地找到插值多项式的系数，即使在直接方法会因数值误差而失败的情况下也能成功。[@problem_id:3253110]", "problem": "给定一组节点和相应的函数值，要求计算以单项式基表示的唯一插值多项式的系数。预期的数值策略是，在给定节点上构建范德蒙矩阵，并使用修正的Gram-Schmidt (MGS) 过程对其列向量关于标准离散内积进行正交化。然后，利用得到的正交-三角分解来稳定地计算插值系数，而无需显式地对任何矩阵求逆。\n\n使用的基础知识：\n- 两个实列向量 $u \\in \\mathbb{R}^m$ 和 $v \\in \\mathbb{R}^m$ 的离散内积为 $\\langle u, v \\rangle = \\sum_{i=1}^m u_i v_i$。\n- 对于次数至多为 $n-1$ 的多项式，在节点 $x_0, x_1, \\dots, x_{m-1}$ 上的范德蒙矩阵的元素为 $V_{i,j} = x_i^j$，其中 $i = 0, \\dots, m-1$ 且 $j = 0, \\dots, n-1$。\n- 如果节点互不相同且 $m = n$，则范德蒙矩阵是非奇异的，并且存在一个唯一的插值多项式 $p(x) = \\sum_{j=0}^{n-1} c_j x^j$ 满足对所有 $i$ 都有 $p(x_i) = y_i$。\n- 修正的Gram-Schmidt (MGS) 过程通过使用离散内积迭代地减去投影，从一个线性无关向量集构造一个标准正交向量集，这比经典的Gram-Schmidt方法在数值上更稳定。\n\n您的任务：\n1. 实现修正的Gram-Schmidt (MGS) 过程，对给定实矩阵 $V \\in \\mathbb{R}^{m \\times n}$ 的列向量关于离散内积进行标准正交化，生成一个标准正交矩阵 $Q \\in \\mathbb{R}^{m \\times n}$ 和一个上三角矩阵 $R \\in \\mathbb{R}^{n \\times n}$，使得 $V = Q R$。\n2. 对于每个测试用例，使用给定的节点 $x_0, \\dots, x_{n-1}$ 和单项式基 $\\{1, x, x^2, \\dots, x^{n-1}\\}$ 构建方形范德蒙矩阵 $V \\in \\mathbb{R}^{n \\times n}$。\n3. 使用您的 MGS 分解来计算唯一插值多项式 $p(x) = \\sum_{j=0}^{n-1} c_j x^j$ 的单项式系数 $c_0, \\dots, c_{n-1}$，该多项式满足测试用例中所有节点的 $p(x_i) = y_i$。通过利用 MGS 产生的标准正交性以及适当的三角求解来完成此操作；不要显式地对任何矩阵求逆。\n4. 按幂的升序报告系数向量，即 $[c_0, c_1, \\dots, c_{n-1}]$。将每个系数四舍五入到小数点后10位，并将结果打印为包含列表的列表的单行。使用标准十进制表示法（非科学记数法），不要包含任何额外文本。\n\n角度单位和物理单位不适用于此任务。所有输入和输出都是无单位的实数。\n\n测试套件：\n为以下四个插值数据集提供您程序的输出。在每种情况下，节点都是互不相同的，并定义了一个方形范德蒙系统。\n- 情况A（理想二次曲线）：节点 $x = [-1.0, 0.0, 1.0]$，值 $y = [-2.0, 2.0, 4.0]$。\n- 情况B（带混合符号的三次曲线）：节点 $x = [0.0, 1.0, 2.0, 3.0]$，值 $y = [1.0, 0.0, -3.0, -20.0]$。\n- 情况C（轻度病态几何）：节点 $x = [0.0, 0.0001, 0.0002]$，值 $y = [1.0, 1.00020003, 1.00040012]$。\n- 情况D（边界情况，常数多项式）：节点 $x = [7.5]$，值 $y = [4.2]$。\n\n最终输出格式：\n您的程序应生成单行输出，其中包含一个逗号分隔的列表的列表形式的结果，每个内部列表给出对应一个测试用例的四舍五入后的系数，顺序与上述情况相同。例如，输出的形状必须为\n$[[c^{(A)}_0, \\dots], [c^{(B)}_0, \\dots], [c^{(C)}_0, \\dots], [c^{(D)}_0, \\dots]]$\n并且必须完全像一个类似Python的列表字面量一样打印在单行上，每个系数都四舍五入到小数点后10位。", "solution": "用户提供的问题已经过分析，并被确定为**有效**。这是一个数值线性代数中的适定问题，它基于已建立的数学原理，并包含获得唯一解所需的所有必要信息。\n\n任务是找到插值多项式 $p(x) = \\sum_{j=0}^{n-1} c_j x^j$ 的系数 $c = [c_0, c_1, \\dots, c_{n-1}]^T$。给定一组 $n$ 个互不相同的节点 $\\{x_i\\}_{i=0}^{n-1}$ 和相应的值 $\\{y_i\\}_{i=0}^{n-1}$，插值条件 $p(x_i) = y_i$ (对于 $i=0, \\dots, n-1$) 构成一个线性方程组。该系统可以用矩阵形式表示为 $Vc = y$，其中 $V$ 是范德蒙矩阵，$c$ 是未知系数向量，$y$ 是函数值向量。\n\n对于单项式基 $\\{1, x, x^2, \\dots, x^{n-1}\\}$，范德蒙矩阵 $V \\in \\mathbb{R}^{n \\times n}$ 的元素为 $V_{ij} = x_i^j$，其中 $i, j \\in \\{0, 1, \\dots, n-1\\}$。该线性系统为：\n$$\n\\begin{pmatrix}\nx_0^0  x_0^1  \\dots  x_0^{n-1} \\\\\nx_1^0  x_1^1  \\dots  x_1^{n-1} \\\\\n\\vdots  \\vdots  \\ddots  \\vdots \\\\\nx_{n-1}^0  x_{n-1}^1  \\dots  x_{n-1}^{n-1}\n\\end{pmatrix}\n\\begin{pmatrix}\nc_0 \\\\\nc_1 \\\\\n\\vdots \\\\\nc_{n-1}\n\\end{pmatrix}\n=\n\\begin{pmatrix}\ny_0 \\\\\ny_1 \\\\\n\\vdots \\\\\ny_{n-1}\n\\end{pmatrix}\n$$\n通过对 $V$ 求逆来直接求解该系统在数值上是不稳定的，特别是当节点彼此靠近时，$V$ 会变得病态。一种更鲁棒的方法，如题目所指定，是使用 $V$ 的 QR 分解。由于其相对于经典版本的优越数值稳定性，问题要求使用修正的Gram-Schmidt (MGS) 算法来计算此分解。\n\nMGS 算法产生一个分解 $V = QR$，其中 $Q \\in \\mathbb{R}^{n \\times n}$ 是一个具有标准正交列的矩阵，$R \\in \\mathbb{R}^{n \\times n}$ 是一个上三角矩阵。$Q$ 的列构成了 $V$ 的列空间的一个标准正交基。$Q$ 列的标准正交性意味着 $Q^T Q = I$，其中 $I$ 是 $n \\times n$ 的单位矩阵。\n\n步骤如下：\n1.  **修正的Gram-Schmidt (MGS) 分解**：设 $V$ 的列向量为 $v_0, v_1, \\dots, v_{n-1}$。我们迭代地计算 $Q$ 的列向量 $q_0, q_1, \\dots, q_{n-1}$ 和 $R$ 的元素。\n    对于 $j = 0, \\dots, n-1$：\n    a. $R$ 的对角元素是当前向量的范数：$R_{jj} = ||v_j||_2$。\n    b. $Q$ 的第 $j$ 列是单位化后的向量：$q_j = v_j / R_{jj}$。\n    c. 对于所有后续向量 $v_k$（其中 $k > j$），我们减去其在 $q_j$ 上的投影。投影系数为 $R_{jk} = \\langle q_j, v_k \\rangle = q_j^T v_k$。\n    d. 剩余向量被更新：$v_k \\leftarrow v_k - R_{jk} q_j$，对于 $k = j+1, \\dots, n-1$。\n    MGS 的一个关键特征是，步骤 (d) 中的投影使用新正交化的向量 $q_j$ 来更新剩余向量，这减少了影响经典Gram-Schmidt方法的正交性损失。\n\n2.  **求解系统**：我们将 $V = QR$ 代入线性系统 $Vc = y$：\n    $$QRc = y$$\n    从左侧乘以 $Q^T$ 得到：\n    $$Q^T Q R c = Q^T y$$\n    由于 $Q^T Q = I$，系统简化为：\n    $$Rc = Q^T y$$\n    令 $z = Q^T y$。系统变为 $Rc = z$，这是一个上三角系统：\n    $$\n    \\begin{pmatrix}\n    R_{00}  R_{01}  \\dots  R_{0,n-1} \\\\\n    0  R_{11}  \\dots  R_{1,n-1} \\\\\n    \\vdots  \\vdots  \\ddots  \\vdots \\\\\n    0  0  \\dots  R_{n-1,n-1}\n    \\end{pmatrix}\n    \\begin{pmatrix}\n    c_0 \\\\\n    c_1 \\\\\n    \\vdots \\\\\n    c_{n-1}\n    \\end{pmatrix}\n    =\n    \\begin{pmatrix}\n    z_0 \\\\\n    z_1 \\\\\n    \\vdots \\\\\n    z_{n-1}\n    \\end{pmatrix}\n    $$\n\n3.  **回代**：这个三角系统可以通过回代法高效地求解 $c$，从最后一行开始向上进行。\n    $$c_{n-1} = \\frac{z_{n-1}}{R_{n-1,n-1}}$$\n    对于 $i = n-2, \\dots, 0$：\n    $$c_i = \\frac{1}{R_{ii}} \\left( z_i - \\sum_{j=i+1}^{n-1} R_{ij} c_j \\right)$$\n    此过程无需计算任何矩阵的逆即可得到所需的系数向量 $c$，提供了一个数值稳定的解。\n\n最终系数按要求四舍五入到小数点后10位。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes interpolating polynomial coefficients using MGS QR factorization.\n    \"\"\"\n\n    def modified_gram_schmidt(A):\n        \"\"\"\n        Computes the QR factorization of a matrix A using the Modified Gram-Schmidt algorithm.\n\n        Args:\n            A (np.ndarray): A matrix of shape (m, n) with linearly independent columns.\n\n        Returns:\n            Q (np.ndarray): A matrix of shape (m, n) with orthonormal columns.\n            R (np.ndarray): An upper triangular matrix of shape (n, n) such that A = QR.\n        \"\"\"\n        m, n = A.shape\n        Q = np.zeros((m, n), dtype=float)\n        R = np.zeros((n, n), dtype=float)\n        \n        # Use a copy of A for the vectors that are iteratively modified.\n        v = A.copy().astype(float)\n\n        for j in range(n):\n            # Calculate the norm of the j-th vector for the diagonal of R.\n            norm_vj = np.linalg.norm(v[:, j])\n            \n            # Since nodes are distinct, V is non-singular and norm will not be zero.\n            R[j, j] = norm_vj\n            # Normalize the vector to get the j-th column of Q.\n            Q[:, j] = v[:, j] / R[j, j]\n            \n            # Orthogonalize remaining vectors against the new orthonormal vector q_j.\n            for k in range(j + 1, n):\n                # Calculate the projection coefficient, which is an element of R.\n                R[j, k] = np.dot(Q[:, j], v[:, k])\n                # Subtract the projection from the k-th vector.\n                v[:, k] = v[:, k] - R[j, k] * Q[:, j]\n                \n        return Q, R\n\n    def back_substitution(R, z):\n        \"\"\"\n        Solves the upper triangular system Rc = z for c.\n\n        Args:\n            R (np.ndarray): An upper triangular square matrix.\n            z (np.ndarray): A vector.\n\n        Returns:\n            np.ndarray: The solution vector c.\n        \"\"\"\n        n = R.shape[0]\n        c = np.zeros(n, dtype=float)\n        for i in range(n - 1, -1, -1):\n            # Calculate the sum of R_ij * c_j for j > i\n            s = np.dot(R[i, i + 1:], c[i + 1:])\n            # R is non-singular, so R[i, i] is non-zero.\n            c[i] = (z[i] - s) / R[i, i]\n        return c\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case A: happy path quadratic\n        {'x': [-1.0, 0.0, 1.0], 'y': [-2.0, 2.0, 4.0]},\n        # Case B: cubic with mixed signs\n        {'x': [0.0, 1.0, 2.0, 3.0], 'y': [1.0, 0.0, -3.0, -20.0]},\n        # Case C: mildly ill-conditioned geometry\n        {'x': [0.0, 0.0001, 0.0002], 'y': [1.0, 1.00020003, 1.00040012]},\n        # Case D: boundary, constant polynomial\n        {'x': [7.5], 'y': [4.2]},\n    ]\n    \n    all_results = []\n    \n    for case in test_cases:\n        x_nodes = np.array(case['x'], dtype=float)\n        y_values = np.array(case['y'], dtype=float)\n        \n        n = len(x_nodes)\n        \n        # 1. Construct the Vandermonde matrix.\n        #    increasing=True corresponds to the basis {1, x, x^2, ...}.\n        V = np.vander(x_nodes, N=n, increasing=True)\n        \n        # 2. Perform MGS QR factorization.\n        Q, R = modified_gram_schmidt(V)\n        \n        # 3. Compute z = Q^T * y.\n        z = Q.T @ y_values\n        \n        # 4. Solve Rc = z using back substitution.\n        c = back_substitution(R, z)\n        \n        # 5. Round coefficients to 10 decimal places and store.\n        rounded_c = [round(val, 10) for val in c]\n        all_results.append(rounded_c)\n\n    # Final print statement in the exact required format.\n    # The string representation of a list of lists of floats.\n    print(all_results)\n\nsolve()\n```", "id": "3253110"}]}