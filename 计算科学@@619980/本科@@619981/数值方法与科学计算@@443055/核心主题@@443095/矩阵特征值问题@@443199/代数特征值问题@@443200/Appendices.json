{"hands_on_practices": [{"introduction": "上手实践的第一步，我们将从最基本的迭代特征值算法——幂法（Power Method）开始。虽然幂法原理简单，但在面对数值计算的挑战时，其行为可能非常微妙。本练习 [@problem_id:3282259] 将引导你构建一个“近似亏损”的矩阵，其特征向量几乎线性相关，从而揭示幂法在实际应用中可能遇到的收敛缓慢和潜在的陷阱。通过这个实践，你将深化对迭代法收敛理论和数值稳定性的理解。", "problem": "你将研究幂法在代数特征值问题中，在一个精心构造的近似亏损矩阵上的行为。幂法试图通过将矩阵重复应用于一个向量并进行归一化来近似一个主特征对。本研究的基础包括特征值和特征向量的定义、可对角化性的概念以及幂迭代的定义。具体来说，如果一个矩阵 $A \\in \\mathbb{R}^{n \\times n}$ 有一个最大模的特征值（谱半径），并且初始向量在相应右特征向量方向上有一个非零分量，那么幂法会趋向于与该特征向量对齐；否则，它将无法做到这一点。\n\n按如下方式构造一个近似亏损的 $2 \\times 2$ 矩阵。设 $S = \\begin{bmatrix} 1  1 \\\\ 0  \\varepsilon \\end{bmatrix}$，其中 $\\varepsilon = 10^{-8}$，并设 $J = \\operatorname{diag}(\\lambda_1,\\lambda_2)$，其中 $\\lambda_1 = 1 + \\delta$ 和 $\\lambda_2 = 1$，$\\delta = 10^{-4}$。定义\n$$\nA \\;=\\; S J S^{-1}.\n$$\n根据构造，$A$ 是可对角化的，其右特征向量由 $S$ 的列给出，即与 $\\lambda_1$ 相关的 $s_1 = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$ 和与 $\\lambda_2$ 相关的 $s_2 = \\begin{bmatrix} 1 \\\\ \\varepsilon \\end{bmatrix}$。因为 $\\varepsilon$ 非常小，所以两个特征向量几乎平行，特征基是病态的，这使得 $A$ 近似亏损。左特征向量（$S^{-1}$ 的行）决定了初始向量在右特征向量基中表示时的分解系数。\n\n实现由以下公式定义的幂法迭代：\n$$\ny_{k+1} \\;=\\; A x_k,\\quad x_{k+1} \\;=\\; \\frac{y_{k+1}}{\\|y_{k+1}\\|_2},\n$$\n从一个给定的初始向量 $x_0 \\in \\mathbb{R}^2 \\setminus \\{0\\}$ 开始。在规定的迭代次数 $N$ 之后，报告瑞利商\n$$\n\\rho(x) \\;=\\; \\frac{x^\\top A x}{x^\\top x},\n$$\n作为与最终方向 $x$ 相关联的特征值的估计。通过比较绝对差 $|\\rho(x) - \\lambda_1|$ 和 $|\\rho(x) - \\lambda_2|$，使用此估计来判断迭代是数值上与主特征值 $\\lambda_1$ 对齐还是与次主特征值 $\\lambda_2$ 对齐。\n\n你的程序必须：\n- 根据指定的 $S$（$\\varepsilon = 10^{-8}$）和 $J$（$\\lambda_1 = 1 + 10^{-4}$ 和 $\\lambda_2 = 1$）构造 $A$。\n- 实现幂法，迭代次数恰好为 $N = 1000$ 次（不要使用自适应停止准则）。\n- 对于下面测试套件中的每个初始向量，计算最终迭代向量 $x_N$（每一步都进行归一化）及其瑞利商 $\\rho(x_N)$，然后输出一个布尔值，如果最终瑞利商更接近 $\\lambda_1$ 而不是 $\\lambda_2$，则为真，否则为假。\n\n初始向量测试套件：\n- 情况 1（精确的次主特征向量）：$x_0^{(1)} = s_2 = \\begin{bmatrix} 1 \\\\ \\varepsilon \\end{bmatrix}$。\n- 情况 2（极度接近次主特征向量）：$x_0^{(2)} = s_2 + 10^{-30} s_1$。\n- 情况 3（精确的主特征向量）：$x_0^{(3)} = s_1 = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$。\n- 情况 4（近似正交于左主特征向量）：$x_0^{(4)} = \\begin{bmatrix} \\varepsilon^{-1} + 1 \\\\ 1 \\end{bmatrix}$。\n\n基于第一性原理的预期定性行为：\n- 在情况 1 中，因为 $x_0$ 正是 $\\lambda_2$ 的一个特征向量，所有的迭代向量都将保持在 $s_2$ 的张成空间内，所以幂法将产生一个（在舍入误差范围内）等于 $\\lambda_2$ 的瑞利商，即相对于主特征值的错误特征值。\n- 在情况 2 中，$s_1$ 的系数极小，且 $|\\lambda_1/\\lambda_2|$ 仅略大于 $1$，经过 $1000$ 次迭代后，主分量仍然可以忽略不计；该方法似乎收敛到错误的特征向量。\n- 在情况 3 中，该方法立即产生主特征对。\n- 在情况 4 中，$s_1$ 的系数相比于 $s_2$ 的系数非常小，因此在 $1000$ 次迭代内，该方法仍将数值上与次主特征向量对齐。\n\n最终输出格式：\n- 你的程序应生成一行输出，其中包含上述四种情况的四个布尔结果，形式为用方括号括起来的逗号分隔列表，例如，“[True,False,True,False]”。\n- 不涉及物理单位或角度；所有数值量均为无量纲实数。\n\n该问题要求你从特征值和特征向量、对角化、左特征向量和右特征向量以及幂法结构的定义出发进行推理。你必须避免使用超出这些定义的任何捷径。通过使用指定的参数确保科学真实性，并按照上述说明提供一个稳健的实现。", "solution": "用户提出了一个数值线性代数领域的问题，具体涉及幂法在一个近似亏损矩阵上的行为。该问题要求实现幂法，并针对一组精心选择的初始向量分析其收敛性。\n\n### 问题验证\n问题陈述已经过验证，被认为是合理的。\n- **已知条件**：所有参数（$\\varepsilon = 10^{-8}$，$\\delta = 10^{-4}$）、矩阵（$S$、$J$ 以及由此产生的 $A$）、算法规格（幂法，迭代次数 $N=1000$，使用 $L_2$ 范数归一化）、初始条件（四个测试向量 $x_0$）和输出要求（将最终瑞利商与已知特征值进行布尔比较）都已明确给出。\n- **科学依据**：该问题基于代数特征值问题的基本和标准原理，包括矩阵对角化、左和右特征向量以及幂迭代法。构造一个近似亏损矩阵来研究数值稳定性和收敛性是数值分析中的经典教学工具。\n- **适定性与客观性**：该问题是无歧义的、自包含的和客观的。它指定了一个确定性的计算任务，对每种情况都能产生唯一、可验证的结果。\n\n该问题有效，将提供解决方案。\n\n### 基于原理的解决方案\n核心任务是分析幂法在具有病态特征基的矩阵 $A$ 上的性能。幂法是一种迭代算法，旨在找到矩阵的主特征值和相应的特征向量。其行为由初始向量在矩阵特征向量基中的分解决定。\n\n**1. 矩阵构造与性质**\n\n矩阵 $A$ 构造为 $A = S J S^{-1}$，其中：\n$S = \\begin{bmatrix} 1  1 \\\\ 0  \\varepsilon \\end{bmatrix}$，$\\varepsilon = 10^{-8}$。\n$J = \\operatorname{diag}(\\lambda_1, \\lambda_2)$，其中 $\\lambda_1 = 1 + \\delta = 1 + 10^{-4}$ 和 $\\lambda_2 = 1$。\n\n$S$ 的列是 $A$ 的右特征向量：\n- $s_1 = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$ 对应于主特征值 $\\lambda_1 = 1.0001$。\n- $s_2 = \\begin{bmatrix} 1 \\\\ \\varepsilon \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 10^{-8} \\end{bmatrix}$ 对应于次主特征值 $\\lambda_2 = 1$。\n\n由于 $\\varepsilon$ 非常小，$s_1$ 和 $s_2$ 两个特征向量几乎平行，使得特征向量基是病态的。这意味着 $A$ 是一个近似亏损矩阵。\n\n$S$ 的逆矩阵计算为 $S^{-1} = \\frac{1}{\\det(S)} \\begin{bmatrix} \\varepsilon  -1 \\\\ 0  1 \\end{bmatrix} = \\frac{1}{\\varepsilon} \\begin{bmatrix} \\varepsilon  -1 \\\\ 0  1 \\end{bmatrix} = \\begin{bmatrix} 1  -1/\\varepsilon \\\\ 0  1/\\varepsilon \\end{bmatrix}$。$S^{-1}$ 的行是 $A$ 的左特征向量。\n\n矩阵 $A$ 可以明确计算出来：\n$$\nA = S J S^{-1} = \\begin{bmatrix} 1  1 \\\\ 0  \\varepsilon \\end{bmatrix} \\begin{bmatrix} 1+\\delta  0 \\\\ 0  1 \\end{bmatrix} \\begin{bmatrix} 1  -1/\\varepsilon \\\\ 0  1/\\varepsilon \\end{bmatrix}\n$$\n$$\nA = \\begin{bmatrix} 1  1 \\\\ 0  \\varepsilon \\end{bmatrix} \\begin{bmatrix} 1+\\delta  -(1+\\delta)/\\varepsilon \\\\ 0  1/\\varepsilon \\end{bmatrix} = \\begin{bmatrix} 1+\\delta  -(1+\\delta)/\\varepsilon + 1/\\varepsilon \\\\ 0  1 \\end{bmatrix} = \\begin{bmatrix} 1+\\delta  -\\delta/\\varepsilon \\\\ 0  1 \\end{bmatrix}\n$$\n所以，$A = \\begin{bmatrix} 1.0001  -10^{-4}/10^{-8} \\\\ 0  1 \\end{bmatrix} = \\begin{bmatrix} 1.0001  -10000 \\\\ 0  1 \\end{bmatrix}$。\n\n**2. 幂法动力学**\n\n幂法迭代为 $x_{k+1} = \\frac{A x_k}{\\|A x_k\\|_2}$。从 $x_0$ 开始进行 $N$ 次迭代后，得到的向量与 $A^N x_0$ 成比例。设初始向量 $x_0$ 在特征向量基中分解为 $x_0 = c_1 s_1 + c_2 s_2$。那么：\n$$\nA^N x_0 = A^N (c_1 s_1 + c_2 s_2) = c_1 \\lambda_1^N s_1 + c_2 \\lambda_2^N s_2\n$$\n向量 $A^N x_0$ 可以重写为：\n$$\nA^N x_0 = \\lambda_2^N \\left( c_1 \\left(\\frac{\\lambda_1}{\\lambda_2}\\right)^N s_1 + c_2 s_2 \\right)\n$$\n迭代向量 $x_N$ 的方向取决于括号中两项的相对大小。要收敛到主特征向量 $s_1$，需要第一项主导第二项。如果放大因子 $(\\lambda_1/\\lambda_2)^N$ 足够大，能够克服可能很小的初始系数比 $|c_1/c_2|$，这种情况就会发生。\n\n在这个问题中，放大因子是 $(\\frac{1+\\delta}{1})^N = (1+10^{-4})^{1000}$。使用近似式 $(1+x)^n \\approx e^{nx}$（对于小 $x$），我们有 $(1+10^{-4})^{1000} \\approx e^{1000 \\cdot 10^{-4}} = e^{0.1} \\approx 1.10517$。这个相对较小的放大因子意味着幂法将收敛得非常缓慢。\n\n**3. 测试案例分析**\n\n每种情况的最终输出是 `True`，如果计算出的瑞利商 $\\rho(x_N) = x_N^\\top A x_N$ 更接近 $\\lambda_1$ 而不是 $\\lambda_2$。这等价于检查 $\\rho(x_N)  (\\lambda_1 + \\lambda_2)/2 = 1 + \\delta/2$ 是否成立。\n\n- **情况 1**：$x_0^{(1)} = s_2$。\n初始向量恰好是次主特征向量。这里，$c_1=0$ 且 $c_2=1$。幂法迭代将在所有步骤中都保持在 $\\lambda_2$ 的特征空间内：对所有 $k0$，都有 $x_k = s_2/\\|s_2\\|_2$。因此，$\\rho(x_N)$ 将等于 $\\lambda_2 = 1$（在浮点误差范围内）。这比 $\\lambda_1$ 更接近 $\\lambda_2$。结果是 **False**。\n\n- **情况 2**：$x_0^{(2)} = s_2 + 10^{-30} s_1$。\n初始向量的系数为 $c_1 = 10^{-30}$ 和 $c_2 = 1$。$N=1000$ 次迭代后分量大小的比率为：\n$$\n\\frac{\\|c_1 (\\lambda_1/\\lambda_2)^N s_1\\|}{\\|c_2 s_2\\|} \\approx \\frac{|c_1|}{|c_2|} \\left(\\frac{\\lambda_1}{\\lambda_2}\\right)^N \\approx 10^{-30} \\times 1.105 \\approx 1.105 \\times 10^{-30}\n$$\n这个比率非常小。向量 $x_{1000}$ 在数值上将无法与 $s_2$ 区分。瑞利商将极其接近 $\\lambda_2$。结果是 **False**。\n\n- **情况 3**：$x_0^{(3)} = s_1$。\n初始向量是主特征向量，所以 $c_1=1$ 且 $c_2=0$。迭代立即产生主特征向量：对所有 $k$，都有 $x_k = s_1/\\|s_1\\|_2$。瑞利商 $\\rho(x_N)$ 将等于 $\\lambda_1 = 1.0001$。这更接近 $\\lambda_1$。结果是 **True**。\n\n- **情况 4**：$x_0^{(4)} = \\begin{bmatrix} \\varepsilon^{-1} + 1 \\\\ 1 \\end{bmatrix}$。\n为了找到系数 $c_1, c_2$，我们解方程 $x_0^{(4)} = c_1 s_1 + c_2 s_2$。这等价于 $\\begin{pmatrix} c_1 \\\\ c_2 \\end{pmatrix} = S^{-1} x_0^{(4)}$。\n$$\n\\begin{pmatrix} c_1 \\\\ c_2 \\end{pmatrix} = \\begin{bmatrix} 1  -1/\\varepsilon \\\\ 0  1/\\varepsilon \\end{bmatrix} \\begin{bmatrix} \\varepsilon^{-1} + 1 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} (\\varepsilon^{-1} + 1) - 1/\\varepsilon \\\\ 1/\\varepsilon \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 1/\\varepsilon \\end{bmatrix}\n$$\n所以，$c_1 = 1$ 且 $c_2 = 1/\\varepsilon = 10^8$。初始向量沿 $s_2$ 的分量比其沿 $s_1$ 的分量大 $10^8$ 倍。$N=1000$ 次迭代后分量大小的比率为：\n$$\n\\frac{\\|c_1 (\\lambda_1/\\lambda_2)^N s_1\\|}{\\|c_2 s_2\\|} \\approx \\frac{|c_1|}{|c_2|} \\left(\\frac{\\lambda_1}{\\lambda_2}\\right)^N = \\varepsilon \\times (1+\\delta)^{1000} \\approx 10^{-8} \\times 1.105 \\approx 1.105 \\times 10^{-8}\n$$\n这个比率非常小。尽管在主特征向量方向上有非零分量，但由于收敛缓慢和极端的初始不平衡，迭代向量 $x_{1000}$ 仍然被次主特征向量 $s_2$ 绝对主导。$\\rho(x_N)$ 将非常接近 $\\lambda_2$。结果是 **False**。\n\n因此，预期的输出是 `[False, False, True, False]`。这表明对于特征值彼此接近的近似亏损矩阵，如果初始向量没有充分地与主特征向量对齐，幂法可能会表现出极其缓慢的收敛，看似收敛到次主特征向量。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the power method problem for a nearly-defective matrix.\n    \"\"\"\n    # 1. Define constants and parameters from the problem statement.\n    epsilon = 1e-8\n    delta = 1e-4\n    lambda1 = 1.0 + delta\n    lambda2 = 1.0\n    N = 1000\n\n    # 2. Construct the matrix A.\n    # A = S J S^-1, where S = [[1, 1], [0, epsilon]] and J = diag(lambda1, lambda2).\n    # This simplifies to A = [[1+delta, -delta/epsilon], [0, 1]].\n    A = np.array([\n        [1.0 + delta, -delta / epsilon],\n        [0.0, 1.0]\n    ], dtype=np.float64)\n\n    # 3. Define the initial vectors for the test suite.\n    # Right eigenvectors s1 and s2\n    s1 = np.array([1.0, 0.0], dtype=np.float64)\n    s2 = np.array([1.0, epsilon], dtype=np.float64)\n\n    # Case 1: Exact subdominant eigenvector\n    x0_1 = s2\n    \n    # Case 2: Extremely close to subdominant eigenvector\n    x0_2 = s2 + 1e-30 * s1\n    \n    # Case 3: Exact dominant eigenvector\n    x0_3 = s1\n    \n    # Case 4: Nearly orthogonal to the left dominant eigenvector\n    x0_4 = np.array([1.0/epsilon + 1.0, 1.0], dtype=np.float64)\n\n    test_cases = [x0_1, x0_2, x0_3, x0_4]\n    \n    results = []\n\n    # 4. Iterate through each test case.\n    for x0 in test_cases:\n        # Check for zero vector, though not expected in these cases.\n        if np.linalg.norm(x0) == 0:\n            # Handle this unlikely edge case.\n            # A zero vector will remain zero, rho is undefined.\n            # The problem assumes x0 is non-zero.\n            # We can arbitrarily assign a result or raise an error.\n            # Let's assume it doesn't happen.\n            pass\n\n        x = x0.copy()\n\n        # 5. Implement the power method for N iterations.\n        # The normalization is part of the loop.\n        for _ in range(N):\n            y = A @ x\n            norm_y = np.linalg.norm(y)\n            if norm_y == 0: # Should not happen with this non-singular matrix A\n                x = np.zeros_like(x)\n                break\n            x = y / norm_y\n        \n        x_N = x\n\n        # 6. Compute the Rayleigh quotient for the final iterate x_N.\n        # rho(x) = (x.T * A * x) / (x.T * x)\n        # Since x_N is normalized, its L2 norm is 1, so x_N.T @ x_N = 1.\n        # rho(x_N) = x_N.T @ (A @ x_N)\n        rho = x_N.T @ A @ x_N\n        \n        # 7. Compare absolute differences to determine which eigenvalue is closer.\n        is_closer_to_lambda1 = abs(rho - lambda1)  abs(rho - lambda2)\n        results.append(is_closer_to_lambda1)\n\n    # 8. Format and print the final output as specified.\n    # str(bool) gives 'True' or 'False' with capital letters, as in the example.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3282259"}, {"introduction": "在了解了基本方法的局限性后，我们转向利用矩阵结构的特化算法。对称三对角矩阵在科学与工程领域中十分常见，其特殊性质催生了极为稳健的特征值求解器。本练习 [@problem_id:3282393] 要求你实现结合了Sturm序列的二分法，这是一种强大的技术，能够精确计算并定位给定区间内的每一个特征值。这个过程将让你体会到，充分利用问题结构是设计高效数值算法的关键。", "problem": "实现一个完整的、可运行的程序，该程序基于第一性原理解决对称三对角矩阵的代数特征值问题中的以下任务。代数特征值问题要求寻找标量 $\\lambda$，使得对于某个非零向量 $v$，方阵 $A$ 满足 $A v = \\lambda v$。对于一个具有对角线元素 $\\{d_i\\}_{i=1}^n$ 和次/超对角线元素 $\\{e_i\\}_{i=1}^{n-1}$ 的实对称三对角矩阵 $A \\in \\mathbb{R}^{n \\times n}$，其特征值均为实数。该问题的基本依据是特征值的定义，即特征方程 $\\det(A - x I) = 0$ 的根，以及顺序主子式的性质，还有通过对 $A - x I$ 进行无主元选择的对称消去法得到 $L D L^\\top$ 分解的惯性定理解释。Sturm 序列性质指出，小于或等于标量 $x$ 的特征值数量等于从顺序主子式派生的序列中的符号变化次数（或者等价地，等于 $A - x I$ 的 $L D L^\\top$ 分解中对角主元的负数个数）。请利用这些基础来逻辑地设计您的算法。\n\n您的程序必须：\n- 对每个测试用例，根据给定的 $\\{d_i\\}$ 和 $\\{e_i\\}$ 隐式构造对称三对角矩阵 $A$（如果不需要，不要形成稠密矩阵）。\n- 实现一个函数，该函数接收一个实数标量 $x$，通过计算 $A - x I$ 无主元选择的 $L D L^\\top$ 分解所隐含的 Sturm 序列计数，返回 $A$ 的小于或等于 $x$ 的特征值数量。\n- 利用计数函数关于 $x$ 的单调性，并结合二分法，在给定的闭区间 $[a,b]$（包含端点）内，以绝对公差 $\\tau$ 逼近 $A$ 的所有特征值。二分法程序必须通过将目标索引与中点处的计数值进行比较，来逐个定位每个特征值。\n- 正确处理重数：如果一个特征值的代数重数大于1，那么在相应测试用例的输出列表中，它必须出现相应次数。\n- 不得使用任何稠密矩阵特征值求解器或黑盒求根器。\n\n数值要求：\n- 绝对公差必须为 $\\tau = 10^{-10}$。\n- 区间 $[a,b]$ 是闭区间：包括任何满足 $a \\le \\lambda \\le b$ 的特征值 $\\lambda$。\n- 不涉及角度；不需要角度单位。\n- 不涉及物理单位。\n\n测试套件：\n对于下面的每个测试用例，使用给定的序列 $\\{d_i\\}$ 和 $\\{e_i\\}$ 以及区间 $[a,b]$。程序必须为每个用例计算出 $A$ 在 $[a,b]$ 内的所有特征值的排序列表（按非递减顺序），每个特征值的近似绝对误差最多为 $\\tau$。\n\n- 测试用例 1（理想情况，多个不同的内部特征值）：\n  - $n = 3$, $d = [2,2,2]$, $e = [-1,-1]$, 区间 $[a,b] = [0.5, 3.5]$。\n- 测试用例 2（边界包含和区间内多个特征值）：\n  - $n = 5$, $d = [2,2,2,2,2]$, $e = [-1,-1,-1,-1]$, 区间 $[a,b] = [0.0, 1.0]$。\n- 测试用例 3（无特征值的区间）：\n  - $n = 4$, $d = [0.0, 1.0, 0.0, 1.0]$, $e = [0.1, 0.1, 0.1]$, 区间 $[a,b] = [10.0, 11.0]$。\n- 测试用例 4（最小维度）：\n  - $n = 1$, $d = [3.14159]$, $e = []$, 区间 $[a,b] = [3.14159, 3.14159]$。\n- 测试用例 5（因次对角线为零导致的重数，闭区间位于一个重根上）：\n  - $n = 3$, $d = [1.0, 1.0, 2.0]$, $e = [0.0, 0.0]$, 区间 $[a,b] = [1.0, 1.0]$。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的列表的列表，以逗号分隔。每个内部列表对应一个测试用例，并包含 $[a,b]$ 内按非递减顺序排列的特征值。每个特征值表示为精确到10位小数的四舍五入值。例如，一个有效的输出看起来像 $[[x_{1,1},x_{1,2}],[x_{2,1}],[]]$，其中每个 $x_{i,j}$ 显示到10位小数。\n\n不要读取任何输入；所有数据均已在上方指定，并且必须嵌入到程序中。程序必须是完整的，并且可以直接运行。", "solution": "该问题要求实现一个算法，用于寻找给定闭区间 $[a,b]$ 内实对称三对角矩阵 $A \\in \\mathbb{R}^{n \\times n}$ 的所有特征值。该算法必须基于第一性原理进行设计，依据 Sturm 序列性质和二分法，而不借助任何黑盒特征值求解器。\n\n一个实对称三对角矩阵 $A$ 由其对角线元素 $\\{d_i\\}_{i=1}^n$ 和其次对角线元素 $\\{e_i\\}_{i=1}^{n-1}$ 定义：\n$$\nA = \\begin{pmatrix}\nd_1  e_1   \\\\\ne_1  d_2  e_2  \\\\\n  \\ddots  \\ddots  \\ddots \\\\\n   e_{n-2}  d_{n-1}  e_{n-1} \\\\\n    e_{n-1}  d_n\n\\end{pmatrix}\n$$\n特征值 $\\lambda$ 是特征方程 $\\det(A - xI) = 0$ 的根。所指定方法的核心是对于任意实数标量 $x$，确定 $A$ 的小于或等于 $x$ 的特征值数量。设这个计数为 $\\nu_{\\le}(x)$。问题陈述假设这个计数可以通过 Sturm 序列性质找到，该性质将 $\\nu_{\\le}(x)$ 与移位矩阵 $A - xI$ 的 $LDL^\\top$ 分解的主元联系起来。\n\n让我们将其形式化。Sylvester 惯性定理指出，对称矩阵 $B$ 的正、负和零特征值的数量在合同变换 $C B C^\\top$ 下是不变的。对 $B = A-xI$ 进行无主元选择的高斯消去法会产生一个 $LDL^\\top$ 分解，其中 $L$ 是单位下双对角矩阵，而 $D$ 是由主元 $\\{\\delta_k\\}_{k=1}^n$ 构成的对角矩阵。惯性定理意味着 $A-xI$ 的负特征值数量等于 $D$ 中负主元 $\\delta_k$ 的数量。$A-xI$ 的一个特征值形式为 $\\lambda_j - x$。因此，负主元的数量等于 $A$ 的满足 $\\lambda_j - x  0$ 的特征值 $\\lambda_j$ 的数量，也就是严格小于 $x$ 的特征值计数，记作 $\\nu_{}(x)$。\n\n问题陈述指定使用一个函数来计算“小于或等于” $x$ 的特征值数量。为了与此保持一致，我们将“负主元的数量”准则解释为非正主元（$\\delta_k \\le 0$）的数量。这将给出一个计数 $\\nu_{\\le}(x)$，它包括任何恰好等于 $x$ 的特征值。主元通过以下递推关系计算：\n$$\n\\delta_1 = d_1 - x\n$$\n$$\n\\delta_k = (d_k - x) - \\frac{e_{k-1}^2}{\\delta_{k-1}} \\quad \\text{for } k = 2, \\dots, n\n$$\n如果某个主元 $\\delta_{k-1}$ 为零，这个递推就会失败。这种情况发生在 $x$ 是顺序主子矩阵 $A_{k-1}$ 的一个特征值时。在这种情况下，项 $e_{k-1}^2/\\delta_{k-1}$ 会变为无穷大。对于一个不可约三对角矩阵（所有 $i$ 的 $e_i \\neq 0$），如果 $\\delta_{k-1} \\to 0$，那么 $\\delta_k$ 会趋于 $-\\infty$ 或 $+\\infty$。一个稳健的数值实现会将一个零主元 $\\delta_{k-1}$ 处理为一个非常小的数，这会迫使 $\\delta_k$ 成为一个绝对值很大的负数（因为 $e_{k-1}^20$）。因此，一个零主元 $\\delta_{k-1}$ 会导致 $\\delta_k$ 被计为非正数。这种处理方式使计数函数变得稳健。\n\n如果任何次对角线元素 $e_i$ 为零，则矩阵 $A$ 是块对角矩阵。$A$ 的特征值集合是其对角块特征值的并集。因此，计数函数 $\\nu_{\\le}(x)$ 可以通过对每个不可约三对角子块的计数求和来计算。\n\n有了稳健的 $\\nu_{\\le}(x)$ 函数，我们可以使用二分法找到任何指定的特征值 $\\lambda_k$（其中 $\\lambda_1 \\le \\lambda_2 \\le \\dots \\le \\lambda_n$）。第 $k$ 个特征值 $\\lambda_k$ 是使得 $\\nu_{\\le}(x) \\ge k$ 的最小的 $x$ 值。二分法算法在搜索区间 `[low, high]` 内通过反复测试中点 `mid` 来寻找 $\\lambda_k$：\n- 如果 $\\nu_{\\le}(\\text{mid}) \\ge k$，那么 $\\lambda_k \\le \\text{mid}$，因此我们更新 `high = mid`。\n- 如果 $\\nu_{\\le}(\\text{mid})  k$，那么 $\\lambda_k  \\text{mid}$，因此我们更新 `low = mid`。\n这个过程持续进行，直到 `high - low` 小于所期望的公差 $\\tau$。\n\n为了找到所有特征值，我们首先使用 Gerschgorin 圆盘定理建立一个保证包含所有特征值的区间 $[g_{\\min}, g_{\\max}]$。对于对称三对角矩阵，这个区间是 $[d_i - r_i, d_i + r_i]$ 对 $i=1,\\dots,n$ 的并集，其中 $r_1 = |e_1|$, $r_n = |e_{n-1}|$, 并且对于 $i=2,\\dots,n-1$, $r_i = |e_{i-1}| + |e_i|$。\n\n整体算法如下：\n1.  对于由 $\\{d_i\\}$ 和 $\\{e_i\\}$ 定义的给定矩阵，计算 Gerschgorin 界 $[g_{\\min}, g_{\\max}]$。\n2.  从 $k=1$ 迭代到 $n$，以找到每个特征值 $\\lambda_k$。\n3.  对于每个 $k$，在区间 $[g_{\\min}, g_{\\max}]$（或一个逐步收紧的区间，例如 $[\\lambda_{k-1}, g_{\\max}]$）上使用二分法，以在公差 $\\tau$ 内找到 $\\lambda_k$ 的近似值。\n4.  在计算出所有 $n$ 个特征值后，筛选结果列表，只保留那些落在指定问题区间 $[a,b]$ 内的特征值。\n5.  按要求收集并格式化所有测试用例的结果。\n\n这个策略能够正确处理特征值的重数，因为如果 $\\lambda_k$ 和 $\\lambda_{k+1}$ 相等，对它们的二分搜索都将收敛到同一个值。它是稳健的，并且直接建立在指定的第一性原理之上。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main solver function that processes all test cases specified in the problem.\n    \"\"\"\n    \n    # --- Core Algorithmic Components ---\n\n    def _count_le_irreducible(x, d, e):\n        \"\"\"\n        Counts eigenvalues = x for an irreducible symmetric tridiagonal matrix.\n        This is the core of the Sturm sequence method, using the pivot recurrence\n        from LDL^T factorization of (A - xI).\n        \"\"\"\n        n = len(d)\n        if n == 0:\n            return 0\n        \n        count = 0\n        delta = 1.0  # Initial value, will be updated in the first iteration.\n        \n        for i in range(n):\n            diag_shifted = d[i] - x\n            if i == 0:\n                delta = diag_shifted\n            else:\n                # This check handles the case where a pivot is zero.\n                # If delta_{k-1} is 0, the next pivot delta_k tends to -infinity,\n                # ensuring it's counted as non-positive.\n                # We use a large negative number to simulate this robustly.\n                if delta == 0.0:\n                    delta = -1e30 \n                else:\n                    delta = diag_shifted - (e[i - 1] ** 2) / delta\n            \n            if delta = 0.0:\n                count += 1\n        return count\n\n    def count_le(x, d, e):\n        \"\"\"\n        Counts eigenvalues = x for a general symmetric tridiagonal matrix.\n        Handles block-diagonal structure by splitting at zero off-diagonal elements.\n        \"\"\"\n        n = len(d)\n        if n == 0:\n            return 0\n        \n        total_count = 0\n        start_idx = 0\n        for i in range(n - 1):\n            if e[i] == 0.0:\n                # Process the block from start_idx up to i.\n                total_count += _count_le_irreducible(x, d[start_idx:i + 1], e[start_idx:i])\n                start_idx = i + 1\n        \n        # Process the final block.\n        total_count += _count_le_irreducible(x, d[start_idx:], e[start_idx:n - 1])\n        return total_count\n\n    def get_gerschgorin_bounds(d, e):\n        \"\"\"\n        Computes an interval [g_min, g_max] that contains all eigenvalues.\n        \"\"\"\n        n = len(d)\n        if n == 0:\n            return 0.0, 0.0\n        if n == 1:\n            return d[0], d[0]\n\n        g_min = float('inf')\n        g_max = float('-inf')\n\n        for i in range(n):\n            radius = 0.0\n            if i > 0:\n                radius += abs(e[i - 1])\n            if i  n - 1:\n                radius += abs(e[i])\n            \n            g_min = min(g_min, d[i] - radius)\n            g_max = max(g_max, d[i] + radius)\n            \n        return g_min, g_max\n\n    def find_all_eigenvalues(d, e, tol=1e-10):\n        \"\"\"\n        Finds all eigenvalues of the matrix using bisection and the count_le function.\n        \"\"\"\n        n = len(d)\n        if n == 0:\n            return []\n\n        g_min, g_max = get_gerschgorin_bounds(d, e)\n        eigenvalues = []\n        \n        search_low = g_min\n        for k in range(1, n + 1):\n            low = search_low\n            high = g_max\n\n            # Bisection to find the k-th eigenvalue\n            while high - low > tol:\n                mid = low + (high - low) / 2\n                if mid == low or mid == high: # Reached precision limit\n                    break\n                \n                # count_le(mid) >= k means lambda_k is in [low, mid]\n                if count_le(mid, d, e) >= k:\n                    high = mid\n                else:\n                    low = mid\n            \n            eig_k = (low + high) / 2\n            eigenvalues.append(eig_k)\n            # Optimization: The next eigenvalue must be >= the current one.\n            search_low = eig_k\n\n        return eigenvalues\n\n    # --- Test Suite and Main Execution Logic ---\n    \n    test_cases = [\n        # Test case 1\n        {\"d\": [2.0, 2.0, 2.0], \"e\": [-1.0, -1.0], \"interval\": [0.5, 3.5]},\n        # Test case 2\n        {\"d\": [2.0, 2.0, 2.0, 2.0, 2.0], \"e\": [-1.0, -1.0, -1.0, -1.0], \"interval\": [0.0, 1.0]},\n        # Test case 3\n        {\"d\": [0.0, 1.0, 0.0, 1.0], \"e\": [0.1, 0.1, 0.1], \"interval\": [10.0, 11.0]},\n        # Test case 4\n        {\"d\": [3.14159], \"e\": [], \"interval\": [3.14159, 3.14159]},\n        # Test case 5\n        {\"d\": [1.0, 1.0, 2.0], \"e\": [0.0, 0.0], \"interval\": [1.0, 1.0]},\n    ]\n    \n    TOLERANCE = 1e-10\n    all_results = []\n\n    for case in test_cases:\n        d_vec = case[\"d\"]\n        e_vec = case[\"e\"]\n        a, b = case[\"interval\"]\n\n        all_eigs = find_all_eigenvalues(d_vec, e_vec, TOLERANCE)\n        \n        # Filter eigenvalues to be within the specified interval [a, b].\n        # A small margin related to tolerance is used for robust endpoint inclusion.\n        filtered_eigs = [eig for eig in all_eigs if a - TOLERANCE = eig and eig = b + TOLERANCE]\n        \n        all_results.append(filtered_eigs)\n\n    # --- Final Output Formatting ---\n    \n    output_str = \"[\"\n    for i, res_list in enumerate(all_results):\n        formatted_list = [f\"{val:.10f}\" for val in res_list]\n        output_str += f\"[{','.join(formatted_list)}]\"\n        if i  len(all_results) - 1:\n            output_str += \",\"\n    output_str += \"]\"\n    \n    print(output_str)\n\nsolve()\n```", "id": "3282393"}, {"introduction": "对于大规模和非对称的特征值问题，我们需要更通用的高级方法。本练习 [@problem_id:3282272] 将介绍现代数值线性代数的基石之一——Arnoldi迭代法。你将通过构建Krylov子空间，将一个大矩阵投影到一个小得多的矩阵上，并用后者的特征值（称为Ritz值）来近似原矩阵的特征值。这项实践将为你提供处理高维复杂问题所必需的投影方法的实战经验。", "problem": "考虑代数特征值问题：对于一个方阵 $A \\in \\mathbb{R}^{n \\times n}$，寻找标量 $\\lambda \\in \\mathbb{C}$ 和非零向量 $x \\in \\mathbb{C}^n$，使得 $A x = \\lambda x$。一种基于投影的特征值近似方法通过构造一个维度为 $m$ 的克雷洛夫子空间来进行，该子空间定义为 $K_m(A,b) = \\operatorname{span}\\{b, A b, A^2 b, \\dots, A^{m-1} b\\}$，其中 $b \\in \\mathbb{R}^n$ 是一个非零起始向量。使用阿诺尔迪迭代，可以为 $K_m(A,b)$ 建立一个标准正交基，然后构建一个小的上海森伯格矩阵，该矩阵表示 $A$ 在此子空间上的作用。这个小矩阵的特征值被称为里兹特征值，它们提供了对 $A$ 的真实特征值的近似。\n\n任务：从上述定义出发，实现阿诺尔迪迭代来为 $K_m(A,b)$ 构造一个标准正交基以及相关的上海森伯格矩阵，然后计算里兹特征值作为 $A$ 的特征值的近似值。对于下面的每个测试用例，通过计算以下量来衡量近似的质量：对于里兹特征值集合 $\\{r_i\\}$ 和 $A$ 的真实特征值集合 $\\{\\lambda_j\\}$，返回单个浮点数\n$$\n\\max_i \\min_j \\left| r_i - \\lambda_j \\right|.\n$$\n该量是在所有里兹特征值中，其与最近的真实特征值在复平面上的绝对差的最大值。报告每个测试用例的这个值。\n\n使用以下参数值的测试套件实现程序。每个测试用例是一个三元组 $(A, b, m)$：\n\n- 测试用例 1 (一般非对称、可对角化、“理想情况”)：$A$ 由 $A = S D S^{-1}$ 形成，其中\n$$\nS = \\begin{bmatrix}\n1  0.2  -0.1  0  0  0 \\\\\n0  1  0.3  0  0  0 \\\\\n0  0  1  0.4  0  0 \\\\\n0  0  0  1  0.5  0 \\\\\n0  0  0  0  1  0.6 \\\\\n0  0  0  0  0  1\n\\end{bmatrix},\\quad\nD = \\operatorname{diag}(5,4,3,2,1,-1),\n$$\n$b = [1,1,1,1,1,1]^T$，且 $m = 4$。\n\n- 测试用例 2 (边界条件 $m = 1$)：$A$ 是一个 $5 \\times 5$ 的三对角矩阵，其对角线上为 $2$，次对角线和超对角线上为 $-1$，\n$$\nA = \\begin{bmatrix}\n2  -1  0  0  0 \\\\\n-1  2  -1  0  0 \\\\\n0  -1  2  -1  0 \\\\\n0  0  -1  2  -1 \\\\\n0  0  0  -1  2\n\\end{bmatrix},\n$$\n$b = [1,0,0,0,0]^T$，且 $m = 1$。\n\n- 测试用例 3 (亏损矩阵边界情况)：$A$ 是一个大小为 $6 \\times 6$、特征值为 $3$ 的若尔当块，即 $A = 3 I + J$，其中 $J$ 在超对角线上为 $1$，其他位置为 $0$，\n$$\nA = \\begin{bmatrix}\n3  1  0  0  0  0 \\\\\n0  3  1  0  0  0 \\\\\n0  0  3  1  0  0 \\\\\n0  0  0  3  1  0 \\\\\n0  0  0  0  3  1 \\\\\n0  0  0  0  0  3\n\\end{bmatrix},\n$$\n$b = [1,1,1,1,1,1]^T$，且 $m = 3$。\n\n- 测试用例 4 (满维 $m = n$，对称情况)：$A$ 是一个对称的 $4 \\times 4$ 矩阵，\n$$\nA = \\begin{bmatrix}\n4  1  0  0 \\\\\n1  3  1  0 \\\\\n0  1  2  1 \\\\\n0  0  1  1\n\\end{bmatrix},\n$$\n$b = [1,1,1,1]^T$，且 $m = 4$。\n\n您的程序必须为每个测试用例计算量 $\\max_i \\min_j |r_i - \\lambda_j|$，并生成单行输出，其中包含用方括号括起来的逗号分隔的结果列表。每个值都必须是四舍五入到 $8$ 位小数的浮点数，不含任何附加文本。例如，输出行的格式为 $[v_1,v_2,v_3,v_4]$，其中每个 $v_k$ 是为测试用例 $k$ 计算并四舍五入到 $8$ 位小数的浮点数。不涉及物理单位或角度，所有计算都是纯数值的。", "solution": "用户提供了一个来自数值线性代数领域的有效问题陈述。任务是实现阿诺尔迪迭代以近似矩阵的特征值，并评估这些近似在几个测试用例中的质量。\n\n### 阿诺尔迪迭代简介\n\n代数特征值问题旨在为给定的方阵 $A \\in \\mathbb{R}^{n \\times n}$ 找到标量 $\\lambda$ 和非零向量 $x$，使得 $A x = \\lambda x$。对于大型矩阵，直接计算所有特征值的成本可能过高。投影方法通过用一个小得多的矩阵的特征值来近似 $A$ 的特征值，提供了一种高效的替代方案。\n\n阿诺尔迪迭代是一种在克雷洛夫子空间上操作的投影方法。对于矩阵 $A$ 和起始向量 $b$，第 $m$ 个克雷洛夫子空间定义为：\n$$\nK_m(A,b) = \\operatorname{span}\\{b, A b, A^2 b, \\dots, A^{m-1} b\\}\n$$\n该子空间的维度最多为 $m$。核心思想是在此子空间内找到对 $A$ 的特征值的最佳近似。这通过为 $K_m(A,b)$ 构造一个标准正交基，然后表示 $A$ 相对于该基的作用来实现。\n\n### 算法\n\n阿诺尔迪迭代采用修正的格拉姆-施密特过程来为克雷洛夫子空间 $K_m(A,b)$ 建立一个标准正交基 $\\{q_1, q_2, \\dots, q_m\\}$。基向量作为矩阵 $Q_m = [q_1, q_2, \\dots, q_m] \\in \\mathbb{R}^{n \\times m}$ 的列进行存储。\n\n算法流程如下：\n\n1.  **初始化**：将起始向量 $b$ 标准化以获得第一个基向量。假设 $b \\neq 0$：\n    $$\n    q_1 = \\frac{b}{\\|b\\|_2}\n    $$\n2.  **迭代**：对于 $k=1, 2, \\dots, m$：\n    a. 通过将 $A$ 应用于最新的基向量来生成一个新向量：\n       $$\n       v = A q_k\n       $$\n    b. 将 $v$ 与之前所有的基向量 $q_1, \\dots, q_k$ 正交化。此投影的系数构成了海森伯格矩阵 $H_m$ 的第 $k$ 列：\n       $$\n       \\text{for } j=1, \\dots, k: \\quad h_{j,k} = q_j^T v \\quad \\text{and} \\quad v \\leftarrow v - h_{j,k} q_j\n       $$\n    c. 所得向量 $v$ 的范数成为海森伯格矩阵的次对角线元素：\n       $$\n       h_{k+1, k} = \\|v\\|_2\n       $$\n    d. 如果 $h_{k+1, k}$ 为零（或数值上接近零），则算法找到了一个 $A$-不变子空间并终止。这被称为“中断”(breakdown)。\n    e. 将 $v$ 标准化以获得下一个基向量：\n       $$\n       q_{k+1} = \\frac{v}{h_{k+1, k}}\n       $$\n\n经过 $m$ 步后（假设没有中断），此过程产生阿诺尔迪分解：\n$$\nA Q_m = Q_{m+1} \\tilde{H}_m\n$$\n其中 $Q_m \\in \\mathbb{R}^{n \\times m}$ 和 $Q_{m+1} \\in \\mathbb{R}^{n \\times (m+1)}$ 是具有标准正交列的矩阵，而 $\\tilde{H}_m \\in \\mathbb{R}^{(m+1) \\times m}$ 是一个上海森伯格矩阵。矩阵 $H_m \\in \\mathbb{R}^{m \\times m}$ 由 $\\tilde{H}_m$ 的前 $m$ 行构成。这个 $H_m$ 是 $A$ 投影到克雷洛夫子空间上的表示，即 $H_m = Q_m^T A Q_m$。\n\n### 里兹特征值与误差度量\n\n这个小的 $m \\times m$ 海森伯格矩阵 $H_m$ 的特征值被称为**里兹特征值**。这些值，记为 $\\{r_i\\}_{i=1}^m$，可作为 $A$ 的真实特征值（我们记为 $\\{\\lambda_j\\}_{j=1}^n$）的近似值。\n\n为了衡量这种近似的质量，我们计算里兹特征值集合与真实特征值集合之间的豪斯多夫距离。具体的度量标准是：\n$$\n\\max_i \\min_j \\left| r_i - \\lambda_j \\right|\n$$\n该公式计算每个里兹特征值 $r_i$ 与最近的真实特征值 $\\lambda_j$ 之间的绝对距离。最终报告的值是这些最小距离中的最大值。\n\n### 实现计划\n\n解决方案将实现为一个 Python 程序，对每个测试用例 $(A, b, m)$ 遵循以下步骤：\n\n1.  **构造**：按规定构造矩阵 $A \\in \\mathbb{R}^{n \\times n}$ 和起始向量 $b \\in \\mathbb{R}^n$。\n2.  **计算真实特征值**：使用标准的数值库函数计算 $A$ 的全部特征值集合 $\\{\\lambda_j\\}$。\n3.  **执行阿诺尔迪迭代**：实现上述算法以生成 $m \\times m$ 的上海森伯格矩阵 $H_m$。如果未完成 $m$ 次迭代就找到了不变子空间，实现将通过返回一个更小的矩阵来处理潜在的中断。\n4.  **计算里兹特征值**：计算所得矩阵 $H_m$ 的特征值 $\\{r_i\\}$。\n5.  **计算度量**：通过找到复平面上两组特征值之间最小距离的最大值来计算误差度量 $\\max_i \\min_j |r_i - \\lambda_j|$。\n6.  **格式化输出**：收集每个测试用例的度量值，并将最终输出格式化为用方括号括起来、逗号分隔的浮点数列表，每个浮点数四舍五入到 $8$ 位小数。\n\n所有矩阵和向量操作都将使用 `numpy` 库执行。由于输入矩阵和向量是实数，阿诺尔迪迭代将使用实数算术进行。所得的特征值可能是复数，`numpy` 的特征值求解器会自动处理这种情况。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef arnoldi_iteration(A, b, m):\n    \"\"\"\n    Performs Arnoldi iteration to find the Hessenberg matrix H.\n\n    Args:\n        A (np.ndarray): The square matrix.\n        b (np.ndarray): The starting vector.\n        m (int): The dimension of the Krylov subspace.\n\n    Returns:\n        np.ndarray: The m x m Hessenberg matrix H_m.\n    \"\"\"\n    n = A.shape[0]\n    \n    # Use real types since A and b are real. Eigenvalues may be complex later.\n    H = np.zeros((m + 1, m), dtype=np.float64)\n    Q = np.zeros((n, m + 1), dtype=np.float64)\n\n    Q[:, 0] = b / np.linalg.norm(b)\n\n    for k in range(m):\n        v = A @ Q[:, k]\n        for j in range(k + 1):\n            H[j, k] = np.dot(Q[:, j].T, v)\n            v = v - H[j, k] * Q[:, j]\n\n        h_next_k = np.linalg.norm(v)\n\n        # Handle breakdown: invariant subspace found\n        if h_next_k  1e-12:\n            return H[:k + 1, :k + 1]\n\n        H[k + 1, k] = h_next_k\n        Q[:, k + 1] = v / h_next_k\n        \n    return H[:m, :m]\n\ndef calculate_approximation_error(ritz_vals, true_vals):\n    \"\"\"\n    Calculates the approximation error metric max_i min_j |r_i - lambda_j|.\n\n    Args:\n        ritz_vals (np.ndarray): Array of Ritz eigenvalues.\n        true_vals (np.ndarray): Array of true eigenvalues.\n\n    Returns:\n        float: The calculated error metric.\n    \"\"\"\n    if ritz_vals.size == 0:\n        return 0.0\n\n    # Reshape for broadcasting\n    ritz_vals_col = ritz_vals[:, np.newaxis]\n    \n    # Calculate the matrix of absolute differences\n    abs_diff_matrix = np.abs(ritz_vals_col - true_vals)\n    \n    # Find the minimum distance for each Ritz value to any true eigenvalue\n    min_dists = np.min(abs_diff_matrix, axis=1)\n    \n    # The error is the maximum of these minimum distances\n    error = np.max(min_dists)\n    \n    return error\n\ndef solve():\n    \"\"\"\n    Main function to run the test cases and print the results.\n    \"\"\"\n    # Test case 1\n    S1 = np.array([\n        [1, 0.2, -0.1, 0, 0, 0],\n        [0, 1, 0.3, 0, 0, 0],\n        [0, 0, 1, 0.4, 0, 0],\n        [0, 0, 0, 1, 0.5, 0],\n        [0, 0, 0, 0, 1, 0.6],\n        [0, 0, 0, 0, 0, 1]\n    ], dtype=np.float64)\n    D1 = np.diag([5, 4, 3, 2, 1, -1])\n    A1 = S1 @ D1 @ np.linalg.inv(S1)\n    b1 = np.ones(6)\n    m1 = 4\n\n    # Test case 2\n    n2 = 5\n    A2 = np.diag(2 * np.ones(n2)) + np.diag(-1 * np.ones(n2 - 1), k=1) + np.diag(-1 * np.ones(n2 - 1), k=-1)\n    b2 = np.zeros(n2)\n    b2[0] = 1\n    m2 = 1\n\n    # Test case 3\n    n3 = 6\n    A3 = 3 * np.eye(n3) + np.diag(np.ones(n3 - 1), k=1)\n    b3 = np.ones(n3)\n    m3 = 3\n\n    # Test case 4\n    A4 = np.array([\n        [4, 1, 0, 0],\n        [1, 3, 1, 0],\n        [0, 1, 2, 1],\n        [0, 0, 1, 1]\n    ], dtype=np.float64)\n    b4 = np.ones(4)\n    m4 = 4\n\n    test_cases = [\n        (A1, b1, m1),\n        (A2, b2, m2),\n        (A3, b3, m3),\n        (A4, b4, m4)\n    ]\n\n    results = []\n    for A, b, m in test_cases:\n        # Step 1: Compute true eigenvalues\n        true_eigenvalues = np.linalg.eigvals(A)\n        \n        # Step 2: Perform Arnoldi iteration\n        H_m = arnoldi_iteration(A, b, m)\n        \n        # Step 3: Compute Ritz eigenvalues\n        ritz_eigenvalues = np.linalg.eigvals(H_m)\n        \n        # Step 4: Calculate the error metric\n        error = calculate_approximation_error(ritz_eigenvalues, true_eigenvalues)\n        results.append(error)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join([f'{r:.8f}' for r in results])}]\")\n\nsolve()\n```", "id": "3282272"}]}