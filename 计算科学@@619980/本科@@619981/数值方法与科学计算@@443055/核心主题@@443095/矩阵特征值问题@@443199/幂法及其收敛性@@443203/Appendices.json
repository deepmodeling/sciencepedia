{"hands_on_practices": [{"introduction": "理论知识需要通过实践来巩固。本练习将指导您实现幂迭代法，并重点关注一个关键的实践细节：如何判断迭代是否收敛。您将基于连续迭代向量之间的对齐程度，来制定并测试一个稳健的停止准则，这正是实际数值软件中常用的技术。[@problem_id:3283230]", "problem": "要求您为幂法设计、论证并实现一个停止准则。幂法是一种用于近似计算方阵主特征向量的迭代算法。考虑一个实方阵 $A \\in \\mathbb{R}^{n \\times n}$ 和一个非零初始向量 $x_0 \\in \\mathbb{R}^n$。幂法通过重复应用矩阵 $A$ 并按欧几里得范数进行重新归一化来生成迭代序列。设 $x_k$ 表示第 $k$ 步的归一化迭代向量，并设 $x_k$ 和 $x_{k+1}$ 之间的夹角以弧度为单位度量。该夹角的余弦值是这两个单位向量的内积。\n\n任务：\n- 从特征值和特征向量的基本定义、矩阵-向量乘法的线性性以及欧几里得范数出发，推导一个实用的停止准则，其决策变量是连续归一化迭代向量 $x_k$ 和 $x_{k+1}$ 之间夹角的余弦值。您的准则必须仅依赖于此余弦值和用户指定的容差 $\\tau  0$，并且必须对迭代向量的缩放保持不变。不要假设任何快捷公式；从幂法在特征基中的行为的第一性原理出发推导该准则。\n- 实现一个程序，应用幂法并使用您的停止准则。使用欧几里得范数（$2$-范数）对每个迭代向量进行归一化。在每一步 $k$，使用单位向量的内积计算 $x_k$ 和 $x_{k+1}$ 之间的余弦值，并仅使用此余弦值和给定的容差 $\\tau$ 来决定是否停止。\n- 如果该方法在第 $k$ 次迭代时（即在 $k$ 次矩阵-向量乘法之后）满足停止准则，您的程序应记录整数 $k$。如果该方法在规定的最大迭代次数 $K_{\\max}$ 内未满足该准则，则为该测试用例记录整数 $-1$。\n- 角度在概念上以弧度为单位，但程序应仅使用余弦值。\n\n使用以下参数值的测试套件：\n- 测试用例 1（一般成功路径，对称矩阵，具有单一主特征值）：$A = \\begin{bmatrix} 5.0  0.0  0.0 \\\\ 0.0  3.0  0.0 \\\\ 0.0  0.0  1.0 \\end{bmatrix}$，$x_0 = \\begin{bmatrix} 1.0 \\\\ 1.0 \\\\ 1.0 \\end{bmatrix}$，$\\tau = 10^{-6}$，$K_{\\max} = 1000$。\n- 测试用例 2（非对称上三角矩阵，对齐速度较快但可能存在符号效应）：$A = \\begin{bmatrix} 2.0  1.0  0.0 \\\\ 0.0  4.0  1.0 \\\\ 0.0  0.0  1.0 \\end{bmatrix}$，$x_0 = \\begin{bmatrix} 1.0 \\\\ 2.0 \\\\ 3.0 \\end{bmatrix}$，$\\tau = 10^{-9}$，$K_{\\max} = 5000$。\n- 测试用例 3（由于主特征值和次主特征值的模几乎相等，导致收敛缓慢）：$A = \\begin{bmatrix} 1.0  0.0 \\\\ 0.0  0.99 \\end{bmatrix}$，$x_0 = \\begin{bmatrix} 1.0 \\\\ 0.1 \\end{bmatrix}$，$\\tau = 10^{-6}$，$K_{\\max} = 5000$。\n- 测试用例 4（边缘情况，最大模特征值不唯一，可能不收敛）：$A = \\begin{bmatrix} 2.0  0.0 \\\\ 0.0  -2.0 \\end{bmatrix}$，$x_0 = \\begin{bmatrix} 1.0 \\\\ 1.0 \\end{bmatrix}$，$\\tau = 10^{-6}$，$K_{\\max} = 1000$。\n\n您的程序应生成一行输出，其中包含所有测试用例的迭代次数，形式为用方括号括起来的逗号分隔列表。例如，如果四个测试用例的计数分别为 $12$、$37$、$-1$ 和 $9$，则输出必须是下面这一行\n$$[12,37,-1,9]$$\n不得打印任何其他文本。所有数字输出必须是整数。本问题不涉及物理单位。角度应理解为以弧度为单位，但程序仅对余弦值（无量纲）进行操作。", "solution": "该问题要求基于连续迭代向量之间的夹角，为幂法推导并实现一个停止准则。\n\n### 停止准则的推导\n\n设 $A$ 是一个可对角化的 $n \\times n$ 实矩阵，$A \\in \\mathbb{R}^{n \\times n}$。设其特征值为 $\\lambda_1, \\lambda_2, \\ldots, \\lambda_n$，对应的特征向量为 $v_1, v_2, \\ldots, v_n$。这些特征向量 $\\{v_i\\}_{i=1}^n$ 构成了 $\\mathbb{R}^n$ 的一组基。幂法旨在寻找与主特征值对应的特征向量，主特征值是指具有严格最大模的特征值。我们假设存在这样的一个特征值，即 $|\\lambda_1|  |\\lambda_2| \\ge |\\lambda_3| \\ge \\ldots \\ge |\\lambda_n|$。与特征值 $\\lambda$ 相关联的特征向量 $v$ 满足基本方程 $A v = \\lambda v$。\n\n幂法是一种迭代算法，从一个非零初始向量 $x^{(0)} \\in \\mathbb{R}^n$ 开始。这个初始向量可以表示为特征向量的线性组合：\n$$x^{(0)} = c_1 v_1 + c_2 v_2 + \\cdots + c_n v_n$$\n为了使该方法成功，初始向量在主特征向量 $v_1$ 方向上的分量必须非零，即 $c_1 \\neq 0$。\n\n该方法的核心是重复将矩阵 $A$ 应用于向量。经过 $k$ 次应用后，我们得到：\n$$A^k x^{(0)} = A^k \\left( \\sum_{i=1}^{n} c_i v_i \\right) = \\sum_{i=1}^{n} c_i (A^k v_i) = \\sum_{i=1}^{n} c_i (\\lambda_i^k v_i)$$\n提出主特征值项 $\\lambda_1^k$：\n$$A^k x^{(0)} = \\lambda_1^k \\left( c_1 v_1 + c_2 \\left(\\frac{\\lambda_2}{\\lambda_1}\\right)^k v_2 + \\cdots + c_n \\left(\\frac{\\lambda_n}{\\lambda_1}\\right)^k v_n \\right)$$\n由于主导条件 $|\\lambda_1|  |\\lambda_i|$ (对于所有 $i  1$)，比值 $|\\lambda_i / \\lambda_1|$ 均小于 1。当迭代次数 $k$ 趋于无穷大时，对于 $i=2, \\ldots, n$，项 $(\\lambda_i / \\lambda_1)^k$ 趋近于 0。因此，向量 $A^k x^{(0)}$ 的方向越来越接近主特征向量 $v_1$ 的方向：\n$$\\lim_{k \\to \\infty} \\frac{A^k x^{(0)}}{\\lambda_1^k} = c_1 v_1$$\n\n幂法算法生成一个归一化的迭代向量序列。设 $x^{(k)}$ 为经过 $k$ 次矩阵乘法后的归一化迭代向量。过程如下：\n1. 从一个初始向量 $x_0$ 开始并将其归一化：$x^{(0)} = x_0 / \\|x_0\\|_2$。\n2. 对于 $k = 1, 2, \\ldots$，计算下一个迭代向量：\n   $$z^{(k)} = A x^{(k-1)}$$\n   $$x^{(k)} = \\frac{z^{(k)}}{\\|z^{(k)}\\|_2}$$\n向量 $x^{(k)}$ 与 $A^k x_0$ 平行。根据上述分析，当 $k$ 变大时，向量序列 $\\{x^{(k)}\\}$ 在方向上收敛于主特征向量 $v_1$。也就是说，$x^{(k)}$ 变得几乎与 $v_1$ 平行。\n\n我们可以通过度量连续迭代向量之间的变化来构建一个停止准则。如果方法正在收敛，那么 $x^{(k-1)}$ 和 $x^{(k)}$ 的方向应该变得几乎相同。当 $x^{(k-1)}$ 已经是主特征向量的一个良好近似时，即 $x^{(k-1)} \\approx \\pm v_1/\\|v_1\\|_2$，那么应用 $A$ 会得到 $A x^{(k-1)} \\approx A(\\pm v_1/\\|v_1\\|_2) = \\pm (\\lambda_1/\\|v_1\\|_2) v_1$。再次对该向量进行归一化，会得到一个与 $v_1$ 平行的向量。因此，$x^{(k)}$ 也将几乎与 $v_1$ 平行，从而也与 $x^{(k-1)}$ 平行。\n\n两个连续的归一化迭代向量 $x^{(k-1)}$ 和 $x^{(k)}$ 之间的夹角 $\\theta_k$ 是收敛的一个度量。由于它们是单位向量，它们之间夹角的余弦值就是它们的内积：\n$$\\cos(\\theta_k) = (x^{(k-1)})^T x^{(k)}$$\n随着方法的收敛，这些向量变得共线，意味着 $\\theta_k \\to 0$ 或 $\\theta_k \\to \\pi$。这对应于 $\\cos(\\theta_k) \\to 1$ 或 $\\cos(\\theta_k) \\to -1$。极限的符号取决于主特征值 $\\lambda_1$ 的符号。如果 $\\lambda_1  0$，迭代向量将始终指向同一方向，并且 $\\cos(\\theta_k) \\to 1$。如果 $\\lambda_1  0$，方向将在每次迭代时翻转，并且 $\\cos(\\theta_k) \\to -1$。\n\n一个稳健的准则必须能处理这两种情况。我们可以通过考虑余弦的绝对值 $|\\cos(\\theta_k)| = |(x^{(k-1)})^T x^{(k)}|$ 来实现这一点。随着方法的收敛，这个值趋近于 1。当 $|\\cos(\\theta_k)|$ 足够接近 1 时，我们可以停止迭代。\n\n我们将偏离完美对齐的偏差定义为 $\\delta_k = 1 - |\\cos(\\theta_k)| = 1 - |(x^{(k-1)})^T x^{(k)}|$。这个量是非负的，并且在收敛时趋近于 0。问题指定了一个用户定义的容差 $\\tau  0$。一个实用且尺度不变的停止准则是，当这个偏差小于容差时终止过程。\n\n停止准则是：在第 $k$ 次迭代时（即在第 $k$ 次矩阵-向量乘法之后），从 $x^{(k-1)}$ 计算归一化的迭代向量 $x^{(k)}$。如果条件\n$$1 - |(x^{(k-1)})^T x^{(k)}|  \\tau$$\n满足，则迭代停止，结果为 $k$。如果在最大迭代次数 $K_{\\max}$ 内未满足该准则，则认为该方法未收敛，结果为 $-1$。该准则仅依赖于连续迭代向量之间的余弦值和容差 $\\tau$，符合要求。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef power_method(A, x0, tau, K_max):\n    \"\"\"\n    Approximates the dominant eigenvector of a matrix A using the Power Method.\n\n    Args:\n        A (np.ndarray): The square matrix.\n        x0 (np.ndarray): The initial non-zero vector.\n        tau (float): The tolerance for the stopping criterion.\n        K_max (int): The maximum number of iterations.\n\n    Returns:\n        int: The number of iterations until convergence, or -1 if not converged.\n    \"\"\"\n    # Normalize the initial vector x0 using the Euclidean (2-norm).\n    norm_x0 = np.linalg.norm(x0)\n    if norm_x0  1e-12:  # Treat as a zero vector\n        # The problem statement guarantees a non-zero initial vector.\n        # This check is for robustness.\n        return -1\n\n    x_prev = x0 / norm_x0\n\n    for k in range(1, K_max + 1):\n        # Apply the matrix A to the previous iterate\n        y = A @ x_prev\n\n        # Normalize the resulting vector to get the new iterate\n        norm_y = np.linalg.norm(y)\n        if norm_y  1e-12:\n            # If A*x_prev is the zero vector, this means we've converged to\n            # an eigenvector with eigenvalue 0. The next iterate is undefined.\n            # In this specific context, we'll consider this a form of convergence\n            # as the direction won't change further. The cosine would be undefined.\n            # Let's assume the cosine is 1.0, satisfying the criterion for any positive tau.\n            return k\n\n        x_curr = y / norm_y\n\n        # Compute the cosine of the angle between successive normalized iterates.\n        # Since they are unit vectors, this is their dot product.\n        # We take the absolute value to handle cases where the dominant eigenvalue is negative.\n        cosine_val = np.abs(np.dot(x_prev, x_curr))\n\n        # Check the stopping criterion: 1 - |cos(theta)|  tau\n        if (1.0 - cosine_val)  tau:\n            return k\n\n        # Prepare for the next iteration\n        x_prev = x_curr\n\n    # If the loop completes without convergence, return -1.\n    return -1\n\n\ndef solve():\n    \"\"\"\n    Defines the test cases and runs the power method for each, printing the results.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test case 1\n        (\n            np.array([[5.0, 0.0, 0.0], [0.0, 3.0, 0.0], [0.0, 0.0, 1.0]]),\n            np.array([1.0, 1.0, 1.0]),\n            1e-6,\n            1000\n        ),\n        # Test case 2\n        (\n            np.array([[2.0, 1.0, 0.0], [0.0, 4.0, 1.0], [0.0, 0.0, 1.0]]),\n            np.array([1.0, 2.0, 3.0]),\n            1e-9,\n            5000\n        ),\n        # Test case 3\n        (\n            np.array([[1.0, 0.0], [0.0, 0.99]]),\n            np.array([1.0, 0.1]),\n            1e-6,\n            5000\n        ),\n        # Test case 4\n        (\n            np.array([[2.0, 0.0], [0.0, -2.0]]),\n            np.array([1.0, 1.0]),\n            1e-6,\n            1000\n        )\n    ]\n\n    results = []\n    for A, x0, tau, K_max in test_cases:\n        iteration_count = power_method(A, x0, tau, K_max)\n        results.append(iteration_count)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\n\nsolve()\n```", "id": "3283230"}, {"introduction": "虽然幂迭代法通常表现出快速的几何收敛，但情况并非总是如此。本练习将探讨该方法应用于一种被称为剪切矩阵的特殊矩阵时的行为，该矩阵是不可对角化的。通过分析这种情况，您将更深入地理解决定收敛速度的条件，并观察到一个较慢的代数收敛的例子。[@problem_id:3283226]", "problem": "考虑一个由 $A = \\begin{pmatrix} 1  \\alpha \\\\ 0  1 \\end{pmatrix}$ 定义的 $2 \\times 2$ 剪切矩阵 $A$，其中 $\\alpha \\neq 0$ 为实数参数。用于逼近矩阵主特征对的幂法（PM）通过重复乘法和归一化生成迭代序列。设初始向量为 $x^{(0)} = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$，并定义未归一化的迭代向量为 $x^{(k)} = A^{k} x^{(0)}$，归一化的迭代向量为 $v^{(k)} = \\dfrac{x^{(k)}}{\\|x^{(k)}\\|_{2}}$，其中 $\\|\\cdot\\|_{2}$ 表示欧几里得范数。令 $e_{1} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$ 表示与特征值 $1$ 相关联的特征向量。从矩阵的幂和欧几里得范数的基本定义出发，推导 $x^{(k)}$ 和 $v^{(k)}$ 的闭式表达式，并用此来追踪归一化迭代向量 $v^{(k)}$ 在单位圆上的几何路径。然后，令 $\\theta_{k}$ 表示 $v^{(k)}$ 与 $e_{1}$ 之间的夹角（以弧度为单位），定义为 $\\tan(\\theta_{k}) = \\dfrac{v^{(k)} \\text{ 的第二个分量}}{v^{(k)} \\text{ 的第一个分量}}$。仅使用矩阵乘法和范数的基本性质，确定 $\\theta_{k}$ 的渐近衰减，并计算极限 $\\lim_{k \\to \\infty} k \\tan(\\theta_{k})$，将其表示为关于 $\\alpha$ 的闭式表达式。请将你的最终答案表示为单个解析表达式。无需四舍五入。", "solution": "该问题是适定的且科学上合理的，其基础是线性代数和数值分析的基本原理。我们可以按照问题陈述中概述的步骤进行求解。\n\n首先，我们确定矩阵 $A = \\begin{pmatrix} 1  \\alpha \\\\ 0  1 \\end{pmatrix}$ 的 $k$ 次幂。我们计算前几次幂以识别一个模式。\n$A^{1} = \\begin{pmatrix} 1  \\alpha \\\\ 0  1 \\end{pmatrix} = \\begin{pmatrix} 1  1\\alpha \\\\ 0  1 \\end{pmatrix}$\n$A^{2} = A \\cdot A = \\begin{pmatrix} 1  \\alpha \\\\ 0  1 \\end{pmatrix} \\begin{pmatrix} 1  \\alpha \\\\ 0  1 \\end{pmatrix} = \\begin{pmatrix} 1 \\cdot 1 + \\alpha \\cdot 0  1 \\cdot \\alpha + \\alpha \\cdot 1 \\\\ 0 \\cdot 1 + 1 \\cdot 0  0 \\cdot \\alpha + 1 \\cdot 1 \\end{pmatrix} = \\begin{pmatrix} 1  2\\alpha \\\\ 0  1 \\end{pmatrix}$\n$A^{3} = A^{2} \\cdot A = \\begin{pmatrix} 1  2\\alpha \\\\ 0  1 \\end{pmatrix} \\begin{pmatrix} 1  \\alpha \\\\ 0  1 \\end{pmatrix} = \\begin{pmatrix} 1  \\alpha + 2\\alpha \\\\ 0  1 \\end{pmatrix} = \\begin{pmatrix} 1  3\\alpha \\\\ 0  1 \\end{pmatrix}$\n模式很明显是 $A^{k} = \\begin{pmatrix} 1  k\\alpha \\\\ 0  1 \\end{pmatrix}$，对于任何整数 $k \\ge 1$。这可以通过数学归纳法严格证明。$k=1$ 的基本情况成立。假设它对某个整数 $k \\ge 1$ 成立，我们有 $A^{k+1} = A^k A = \\begin{pmatrix} 1  k\\alpha \\\\ 0  1 \\end{pmatrix} \\begin{pmatrix} 1  \\alpha \\\\ 0  1 \\end{pmatrix} = \\begin{pmatrix} 1  \\alpha + k\\alpha \\\\ 0  1 \\end{pmatrix} = \\begin{pmatrix} 1  (k+1)\\alpha \\\\ 0  1 \\end{pmatrix}$，这就完成了归纳证明。对于 $k=0$，$A^0=I$ 也符合该公式。\n\n接下来，我们推导未归一化迭代向量 $x^{(k)} = A^{k} x^{(0)}$ 的闭式表达式。初始向量为 $x^{(0)} = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$。\n$$x^{(k)} = A^{k} x^{(0)} = \\begin{pmatrix} 1  k\\alpha \\\\ 0  1 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 1 \\cdot 0 + k\\alpha \\cdot 1 \\\\ 0 \\cdot 0 + 1 \\cdot 1 \\end{pmatrix} = \\begin{pmatrix} k\\alpha \\\\ 1 \\end{pmatrix}$$\n这就是 $x^{(k)}$ 的闭式表达式。\n\n现在，我们求出归一化的迭代向量 $v^{(k)} = \\dfrac{x^{(k)}}{\\|x^{(k)}\\|_{2}}$。我们首先计算 $x^{(k)}$ 的欧几里得范数：\n$$\\|x^{(k)}\\|_{2} = \\left\\| \\begin{pmatrix} k\\alpha \\\\ 1 \\end{pmatrix} \\right\\|_{2} = \\sqrt{(k\\alpha)^2 + 1^2} = \\sqrt{k^2\\alpha^2 + 1}$$\n因此，归一化的迭代向量 $v^{(k)}$ 是：\n$$v^{(k)} = \\frac{1}{\\sqrt{k^2\\alpha^2 + 1}} \\begin{pmatrix} k\\alpha \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} \\frac{k\\alpha}{\\sqrt{k^2\\alpha^2 + 1}} \\\\ \\frac{1}{\\sqrt{k^2\\alpha^2 + 1}} \\end{pmatrix}$$\n这就是 $v^{(k)}$ 的闭式表达式。\n\n我们追踪 $v^{(k)}$ 的几何路径。根据定义，每个向量 $v^{(k)}$ 都是单位向量，因此其末端位于 $\\mathbb{R}^2$ 的单位圆上。$v^{(k)}$ 的第二个分量 $\\frac{1}{\\sqrt{k^2\\alpha^2 + 1}}$ 对于任何 $k \\ge 0$ 都恒为正。因此，所有迭代向量都位于上半圆。\n对于 $k=0$，我们有 $v^{(0)} = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$。\n当 $k \\to \\infty$ 时，我们分析 $v^{(k)}$ 的分量：\n第一个分量：$\\lim_{k \\to \\infty} \\frac{k\\alpha}{\\sqrt{k^2\\alpha^2 + 1}} = \\lim_{k \\to \\infty} \\frac{k\\alpha}{|k\\alpha|\\sqrt{1 + \\frac{1}{k^2\\alpha^2}}} = \\lim_{k \\to \\infty} \\frac{\\alpha}{|\\alpha|\\sqrt{1 + \\frac{1}{k^2\\alpha^2}}} = \\frac{\\alpha}{|\\alpha|} = \\text{sgn}(\\alpha)$。\n第二个分量：$\\lim_{k \\to \\infty} \\frac{1}{\\sqrt{k^2\\alpha^2 + 1}} = 0$。\n所以，$\\lim_{k \\to \\infty} v^{(k)} = \\begin{pmatrix} \\text{sgn}(\\alpha) \\\\ 0 \\end{pmatrix}$。\n如果 $\\alpha  0$，迭代向量 $v^{(k)}$ 在单位圆上描绘出一条从 $\\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$ 到 $\\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = e_1$ 的路径。\n如果 $\\alpha  0$，迭代向量描绘出一条从 $\\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$ 到 $\\begin{pmatrix} -1 \\\\ 0 \\end{pmatrix} = -e_1$ 的路径。\n\n接下来，我们使用问题中给出的 $v^{(k)}$ 与 $e_1$ 之间的角度 $\\theta_k$ 的定义：\n$$\\tan(\\theta_{k}) = \\frac{v^{(k)} \\text{ 的第二个分量}}{v^{(k)} \\text{ 的第一个分量}}$$\n代入 $v^{(k)}$ 的分量：\n$$\\tan(\\theta_{k}) = \\frac{\\frac{1}{\\sqrt{k^2\\alpha^2 + 1}}}{\\frac{k\\alpha}{\\sqrt{k^2\\alpha^2 + 1}}} = \\frac{1}{k\\alpha}$$\n注意，这对 $k \\ge 1$ 成立，因为当 $k=0$ 时，第一个分量为 $0$。\n\n根据这个表达式，我们确定 $\\theta_k$ 的渐近衰减。当 $k \\to \\infty$ 时，$\\tan(\\theta_k) = \\frac{1}{k\\alpha} \\to 0$。这意味着如果 $\\alpha  0$ 则 $\\theta_k \\to 0$，如果 $\\alpha  0$ 则 $\\theta_k \\to \\pi$（考虑到 $v^{(k)}$ 分量的符号）。对于大的 $k$，$\\tan(\\theta_k)$ 很小。使用对于 $x \\approx 0$ 的小角度近似 $\\tan(x) \\approx x$，我们发现对于 $\\alpha0$，$\\theta_k \\approx \\frac{1}{k\\alpha}$。角 $\\theta_k$ 以 $O(k^{-1})$ 的渐近速率衰减到 $0$。问题问的不是 $\\theta_k$ 本身，而是它的正切值。$\\tan(\\theta_k)$ 的渐近行为恰好是 $\\frac{1}{k\\alpha}$。\n\n最后，我们计算所要求的极限：\n$$\\lim_{k \\to \\infty} k \\tan(\\theta_{k})$$\n使用我们推导出的 $\\tan(\\theta_k)$ 表达式：\n$$\\lim_{k \\to \\infty} k \\left( \\frac{1}{k\\alpha} \\right) = \\lim_{k \\to \\infty} \\frac{k}{k\\alpha} = \\lim_{k \\to \\infty} \\frac{1}{\\alpha}$$\n由于 $\\alpha$ 是一个非零常数，极限就是 $\\frac{1}{\\alpha}$。这个结果对任何实数 $\\alpha \\neq 0$ 都成立。这表明，对于这个亏损矩阵，向量迭代方向的收敛是代数收敛，速率为 $O(k^{-1})$，而不是幂法中对于可对角化矩阵通常见到的几何收敛速率。", "answer": "$$\\boxed{\\frac{1}{\\alpha}}$$", "id": "3283226"}, {"introduction": "幂迭代法旨在寻找唯一的最大特征值，但如果我们想要求解其他特征值该怎么办？这个高级实践介绍了“降阶” (deflation) 的概念，这是一种在找到主特征值后，继续求解次大特征值的技术。您将实现该方法并分析其对误差的敏感性，从而深入了解多阶段特征值计算中的数值稳定性挑战。[@problem_id:3283273]", "problem": "考虑一个实对称矩阵 $A \\in \\mathbb{R}^{n \\times n}$，其特征对为 $\\{(\\lambda_i, v_i)\\}_{i=1}^n$，满足 $A v_i = \\lambda_i v_i$，其中特征向量 $\\{v_i\\}$ 构成一个标准正交基，且特征值按 $|\\lambda_1| \\ge |\\lambda_2| \\ge \\dots \\ge |\\lambda_n|$ 排序。幂法通过迭代一个非零向量 $x_k$ 来寻找主特征对，迭代规则为 $x_{k+1} = A x_k / \\|A x_k\\|_2$，并通过瑞利商 $x_k^\\top A x_k / (x_k^\\top x_k)$ 来估计特征值。降阶法从矩阵 $A$ 中移除主特征对的贡献，以揭示下一个特征值。如果 $(\\lambda_1, v_1)$ 已知，可以构造一个降阶矩阵 $A_1 = A - \\lambda_1 v_1 v_1^\\top$，对于该矩阵，$v_1$ 成为一个零向量，且 $A_1$ 的最大特征值理想情况下为 $\\lambda_2$。在实践中，$v_1$ 是近似得到的，因此降阶过程并不完美。\n\n从上述基本定义出发，实现幂法以获得一个近似的主特征对，然后执行秩一降阶法来估计第二大特征值。分析降阶过程对 $v_1$ 近似值中角度误差的敏感性。\n\n算法要求：\n\n- 实现一个函数，给定一个实对称矩阵 $A$、一个初始向量 $x_0 \\ne 0$、一个容差 $\\varepsilon  0$ 和一个最大迭代次数 $K_{\\max}$，该函数通过 $x_{k+1} = A x_k / \\|A x_k\\|_2$ 进行迭代，直到连续的瑞利商之差小于 $\\varepsilon$ 或达到 $K_{\\max}$ 次迭代。返回最终的瑞利商和归一化后的向量。\n- 实现秩一降阶法：给定一个标量 $\\mu$ 和一个单位向量 $u$，构造 $\\tilde{A} = A - \\mu\\, u u^\\top$。\n- 为进行敏感性分析，构造一个受扰动的主方向 $\\hat{v}_1(\\theta) = \\mathrm{normalize}(\\cos \\theta\\, v_1 + \\sin \\theta\\, v_2)$，其中角度 $\\theta$ 以弧度为单位。在降阶中使用 $\\mu(\\theta) = \\hat{v}_1(\\theta)^\\top A \\hat{v}_1(\\theta)$ 来模拟与近似向量相关的瑞利商的实际应用。\n\n测试套件：\n\n使用以下 $4 \\times 4$ 对称矩阵和角度（以弧度为单位），初始向量 $x_0$为全1向量，容差 $\\varepsilon = 10^{-12}$，以及 $K_{\\max} = 10000$。\n\n- 矩阵 $A_{\\mathrm{easy}}$：\n$$\nA_{\\mathrm{easy}} = \\begin{bmatrix}\n4  1  0  0 \\\\\n1  3  1  0 \\\\\n0  1  2  1 \\\\\n0  0  1  1\n\\end{bmatrix}.\n$$\n- 矩阵 $A_{\\mathrm{close}}$：\n$$\nA_{\\mathrm{close}} = \\begin{bmatrix}\n2  1  0  0 \\\\\n1  2  1  0 \\\\\n0  1  2  1 \\\\\n0  0  1  2\n\\end{bmatrix}.\n$$\n\n用于构造 $\\hat{v}_1(\\theta)$ 的角度：$\\theta \\in \\{0, 0.05, 0.2\\}$（以弧度为单位）。对于每个矩阵 $A \\in \\{A_{\\mathrm{easy}}, A_{\\mathrm{close}}\\}$ 和每个角度 $\\theta$，执行以下操作：\n\n1. 通过任何正确的数值方法计算 $A$ 的真实第二大特征值 $\\lambda_2$。\n2. 使用 $A$ 的真实 $v_1$ 和 $v_2$ 构造 $\\hat{v}_1(\\theta) = \\mathrm{normalize}(\\cos \\theta\\, v_1 + \\sin \\theta\\, v_2)$。\n3. 计算 $\\mu(\\theta) = \\hat{v}_1(\\theta)^\\top A \\hat{v}_1(\\theta)$。\n4. 降阶：$\\tilde{A}(\\theta) = A - \\mu(\\theta)\\, \\hat{v}_1(\\theta)\\hat{v}_1(\\theta)^\\top$。\n5. 从 $x_0$ 开始，对 $\\tilde{A}(\\theta)$ 应用幂法以估计其主特征值，记为 $\\tilde{\\lambda}_1(\\theta)$，该值在不完美降阶下作为 $A$ 的 $\\lambda_2$ 的估计值。\n6. 将绝对误差 $|\\tilde{\\lambda}_1(\\theta) - \\lambda_2|$ 报告为浮点数。\n\n最终输出格式：\n\n你的程序应生成一行输出，其中包含一个用方括号括起来的逗号分隔列表，顺序为\n$$\n\\big[ e(A_{\\mathrm{easy}}, 0), e(A_{\\mathrm{easy}}, 0.05), e(A_{\\mathrm{easy}}, 0.2), e(A_{\\mathrm{close}}, 0), e(A_{\\mathrm{close}}, 0.05), e(A_{\\mathrm{close}}, 0.2) \\big],\n$$\n其中 $e(A,\\theta) = |\\tilde{\\lambda}_1(\\theta) - \\lambda_2|$。将每个浮点数值四舍五入到 $10$ 位小数。角度以弧度为单位。不涉及物理单位。", "solution": "用户提供了一个关于幂法和秩一降阶法的数值线性代数问题。任务是实现这些方法，并分析降阶过程对主特征向量近似误差的敏感性。\n\n### 问题验证\n\n**第 1 步：提取给定信息**\n- **矩阵：** 两个大小为 $4 \\times 4$ 的实对称矩阵 $A_{\\mathrm{easy}}$ 和 $A_{\\mathrm{close}}$。\n$$\nA_{\\mathrm{easy}} = \\begin{bmatrix}\n4  1  0  0 \\\\\n1  3  1  0 \\\\\n0  1  2  1 \\\\\n0  0  1  1\n\\end{bmatrix}, \\quad\nA_{\\mathrm{close}} = \\begin{bmatrix}\n2  1  0  0 \\\\\n1  2  1  0 \\\\\n0  1  2  1 \\\\\n0  0  1  2\n\\end{bmatrix}\n$$\n- **特征对：** 一个实对称矩阵 $A \\in \\mathbb{R}^{n \\times n}$ 拥有特征对 $\\{(\\lambda_i, v_i)\\}_{i=1}^n$，其中 $A v_i = \\lambda_i v_i$。特征向量 $\\{v_i\\}$ 构成一个标准正交基，特征值按大小排序：$|\\lambda_1| \\ge |\\lambda_2| \\ge \\dots \\ge |\\lambda_n|$。\n- **幂法算法：** 一种寻找主特征对的迭代方法。迭代定义为 $x_{k+1} = A x_k / \\|A x_k\\|_2$。特征值通过瑞利商 $R(x_k) = x_k^\\top A x_k / (x_k^\\top x_k)$ 来估计。\n- **幂法实现参数：**\n    - 降阶矩阵的初始向量：$x_0 = [1, 1, 1, 1]^\\top$。\n    - 容差：$\\varepsilon = 10^{-12}$。\n    - 最大迭代次数：$K_{\\max} = 10000$。\n- **秩一降阶法：** 一种构造新矩阵 $A_1 = A - \\lambda_1 v_1 v_1^\\top$ 的过程，其主特征值为 $\\lambda_2$。\n- **敏感性分析：**\n    - 构造一个受扰动的主特征向量：$\\hat{v}_1(\\theta) = \\mathrm{normalize}(\\cos \\theta\\, v_1 + \\sin \\theta\\, v_2)$，其中 $v_1$ 和 $v_2$ 是 $A$ 的真实主特征向量。\n    - 相应的近似特征值为 $\\mu(\\theta) = \\hat{v}_1(\\theta)^\\top A \\hat{v}_1(\\theta)$。\n    - 降阶矩阵使用这些近似值：$\\tilde{A}(\\theta) = A - \\mu(\\theta)\\, \\hat{v}_1(\\theta)\\hat{v}_1(\\theta)^\\top$。\n    - 扰动角度指定为 $\\theta \\in \\{0, 0.05, 0.2\\}$ 弧度。\n- **任务：** 对于每个矩阵和每个角度，计算绝对误差 $e(A,\\theta) = |\\tilde{\\lambda}_1(\\theta) - \\lambda_2|$，其中 $\\tilde{\\lambda}_1(\\theta)$ 是使用幂法找到的 $\\tilde{A}(\\theta)$ 的主特征值。\n- **输出格式：** 一个包含 $(A_{\\mathrm{easy}}, \\theta=0)$、$(A_{\\mathrm{easy}}, \\theta=0.05)$、$(A_{\\mathrm{easy}}, \\theta=0.2)$、$(A_{\\mathrm{close}}, \\theta=0)$、$(A_{\\mathrm{close}}, \\theta=0.05)$ 和 $(A_{\\mathrm{close}}, \\theta=0.2)$ 误差的列表，每个值四舍五入到 $10$ 位小数。\n\n**第 2 步：使用提取的给定信息进行验证**\n- **科学基础：** 该问题牢固地植根于数值线性代数。幂法和 Hotelling 降阶法是标准的、有据可查的算法。敏感性分析是研究数值方法的合法且重要的部分。\n- **良置性：** 问题指定了所有必要的输入：矩阵、参数、初始向量以及分析过程。对于具有唯一最大模特征值（$|\\lambda_1|  |\\lambda_2|$）的实对称矩阵，幂法保证收敛，两个测试矩阵都满足此条件。所要求的输出是一组确定的数值。\n- **客观性：** 问题使用精确的数学语言陈述，没有歧义或主观论断。\n\n**第 3 步：结论与行动**\n该问题在科学上是合理的、良置的、客观的和完整的。满足有效问题的所有条件。我将继续构建并呈现解决方案。\n\n### 解决方案\n\n该问题要求对当主特征对仅为近似已知时秩一降阶法的稳定性进行数值研究。我们将首先描述幂法和降阶法的基本原理，然后概述执行指定敏感性分析的计算步骤。\n\n**1. 幂法**\n\n幂法是一种迭代算法，用于寻找矩阵 $A$ 对应于模最大特征值 $\\lambda_1$ 的特征对 $(\\lambda_1, v_1)$。从一个不与 $v_1$ 正交的非零向量 $x_0$ 开始，该方法按以下方式进行：\n$$\ny_{k+1} = A x_k, \\qquad x_{k+1} = \\frac{y_{k+1}}{\\|y_{k+1}\\|_2}\n$$\n当 $k \\to \\infty$ 时，向量 $x_k$ 收敛到主特征向量 $v_1$（或 $-v_1$），并且瑞利商 $R(x_k) = x_k^\\top A x_k$ 收敛到主特征值 $\\lambda_1$，因为 $x_k$ 是一个单位向量。\n\n通过连续迭代中瑞利商的变化来监控收敛。当 $|\\lambda^{(k+1)} - \\lambda^{(k)}|  \\varepsilon$ 或达到最大迭代次数 $K_{\\max}$ 时，算法终止，其中 $\\lambda^{(k)} = R(x_k)$。\n\n**2. 秩一降阶法**\n\n一旦找到了对称矩阵 $A$ 的主特征对 $(\\lambda_1, v_1)$，我们就可以构造一个新矩阵 $A_1$，其特征值为 $\\{0, \\lambda_2, \\lambda_3, \\dots, \\lambda_n\\}$。这通过秩一降阶法（也称为 Hotelling 降阶法）实现：\n$$\nA_1 = A - \\lambda_1 v_1 v_1^\\top\n$$\n$A_1$ 的性质如下：\n- 对于特征向量 $v_1$：\n$A_1 v_1 = A v_1 - \\lambda_1 v_1 (v_1^\\top v_1) = \\lambda_1 v_1 - \\lambda_1 v_1 (1) = 0$。因此，$v_1$ 是 $A_1$ 的一个特征向量，其特征值为 $0$。\n- 对于任何其他特征向量 $v_i$（其中 $i \\ne 1$）：\n由于 $A$ 是对称的，其特征向量是正交的，所以 $v_1^\\top v_i = 0$。\n$A_1 v_i = A v_i - \\lambda_1 v_1 (v_1^\\top v_i) = \\lambda_i v_i - \\lambda_1 v_1 (0) = \\lambda_i v_i$。\n对于 $i=2, \\dots, n$，特征对 $(\\lambda_i, v_i)$ 得以保留。$A_1$ 的主特征值现在是 $\\lambda_2$，可以通过对 $A_1$ 应用幂法来找到。\n\n**3. 对扰动的敏感性**\n\n在实践中，真实的特征对 $(\\lambda_1, v_1)$ 并非精确已知。而是获得一个近似值 $(\\mu, u)$。降阶后的矩阵则为 $\\tilde{A} = A - \\mu u u^\\top$。该问题通过在主特征向量中引入一个受控扰动来模拟这种情况。近似向量 $u$ 由前两个真实特征向量 $v_1$ 和 $v_2$ 的混合给出：\n$$\n\\hat{v}_1(\\theta) = \\cos\\theta\\, v_1 + \\sin\\theta\\, v_2\n$$\n由于 $v_1$ 和 $v_2$ 是标准正交的，$\\|\\hat{v}_1(\\theta)\\|_2 = \\sqrt{\\cos^2\\theta \\|v_1\\|_2^2 + \\sin^2\\theta \\|v_2\\|_2^2} = 1$，所以该向量已经是归一化的。角度 $\\theta$ 代表近似中的角度误差。近似特征值 $\\mu$ 是在该扰动向量处计算的瑞利商：\n$$\n\\mu(\\theta) = \\hat{v}_1(\\theta)^\\top A \\hat{v}_1(\\theta) = (\\cos\\theta v_1 + \\sin\\theta v_2)^\\top (\\lambda_1 \\cos\\theta v_1 + \\lambda_2 \\sin\\theta v_2) = \\lambda_1 \\cos^2\\theta + \\lambda_2 \\sin^2\\theta\n$$\n降阶后的矩阵为 $\\tilde{A}(\\theta) = A - \\mu(\\theta) \\hat{v}_1(\\theta) \\hat{v}_1(\\theta)^\\top$。当 $\\theta=0$ 时，我们得到完美的降阶，$\\tilde{A}(0)$ 的主特征值恰好是 $\\lambda_2$。对于 $\\theta  0$，$\\tilde{A}(\\theta)$ 的特征结构会受到扰动，其主特征值 $\\tilde{\\lambda}_1(\\theta)$ 将只是 $\\lambda_2$ 的一个近似值。我们预期误差 $|\\tilde{\\lambda}_1(\\theta) - \\lambda_2|$ 会随着 $\\theta$ 的增大而增大。\n\n**4. 执行计划**\n\n对于每个矩阵 $A \\in \\{A_{\\mathrm{easy}}, A_{\\mathrm{close}}\\}$ 和每个角度 $\\theta \\in \\{0, 0.05, 0.2\\}$，所需的计算将按以下步骤进行：\n1. 计算 $A$ 的完整特征系统，以获得真实的特征值 $\\{\\lambda_i\\}$ 和特征向量 $\\{v_i\\}$。我们将它们排序，使得 $|\\lambda_1| \\ge |\\lambda_2| \\ge \\dots$。\n2. 提取 $\\lambda_2$、$v_1$ 和 $v_2$。\n3. 构造受扰动的向量 $\\hat{v}_1(\\theta) = \\cos\\theta\\, v_1 + \\sin\\theta\\, v_2$。\n4. 计算相应的瑞利商 $\\mu(\\theta) = \\hat{v}_1(\\theta)^\\top A \\hat{v}_1(\\theta)$。\n5. 构造不完美降阶的矩阵 $\\tilde{A}(\\theta) = A - \\mu(\\theta) \\hat{v}_1(\\theta) \\hat{v}_1(\\theta)^\\top$。\n6. 对 $\\tilde{A}(\\theta)$ 应用幂法，初始向量为 $x_0 = [1, 1, 1, 1]^\\top$，容差为 $\\varepsilon = 10^{-12}$，最大迭代次数 $K_{\\max}=10000$，以找到其主特征值 $\\tilde{\\lambda}_1(\\theta)$。\n7. 计算绝对误差 $e(A,\\theta) = |\\tilde{\\lambda}_1(\\theta) - \\lambda_2|$。\n收集所有计算出的误差，并按指定格式呈现。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef power_method(A, x0, tol, max_iter):\n    \"\"\"\n    Implements the power method to find the dominant eigenvalue and eigenvector.\n\n    Args:\n        A (np.ndarray): The matrix.\n        x0 (np.ndarray): The initial vector.\n        tol (float): The tolerance for convergence.\n        max_iter (int): The maximum number of iterations.\n\n    Returns:\n        tuple: A tuple containing the dominant eigenvalue (float) and\n               the corresponding normalized eigenvector (np.ndarray).\n    \"\"\"\n    if np.linalg.norm(x0) == 0:\n        raise ValueError(\"Initial vector x0 cannot be the zero vector.\")\n    \n    x = x0 / np.linalg.norm(x0)\n    \n    # Calculate initial Rayleigh quotient. Since x is normalized, x.T @ x = 1.\n    lambda_prev = x.T @ A @ x\n    \n    for _ in range(max_iter):\n        Ax = A @ x\n        norm_Ax = np.linalg.norm(Ax)\n        if norm_Ax == 0:\n            # This can happen if the dominant eigenvalue is 0\n            return 0.0, x\n        \n        x = Ax / norm_Ax\n        \n        lambda_curr = x.T @ A @ x\n        \n        if np.abs(lambda_curr - lambda_prev)  tol:\n            return lambda_curr, x\n            \n        lambda_prev = lambda_curr\n        \n    return lambda_curr, x\n\ndef solve():\n    \"\"\"\n    Main function to execute the problem's test suite and print the results.\n    \"\"\"\n    # Define matrices from the problem statement\n    A_easy = np.array([\n        [4, 1, 0, 0],\n        [1, 3, 1, 0],\n        [0, 1, 2, 1],\n        [0, 0, 1, 1]\n    ], dtype=float)\n\n    A_close = np.array([\n        [2, 1, 0, 0],\n        [1, 2, 1, 0],\n        [0, 1, 2, 1],\n        [0, 0, 1, 2]\n    ], dtype=float)\n\n    # Define test parameters\n    test_cases = [\n        (A_easy, \"A_easy\"),\n        (A_close, \"A_close\")\n    ]\n    thetas = [0.0, 0.05, 0.2]  # Angles in radians\n    x0 = np.ones(4)\n    tolerance = 1e-12\n    max_iterations = 10000\n\n    results = []\n\n    for A, name in test_cases:\n        # 1. Compute true eigenpairs of A\n        # np.linalg.eigh returns eigenvalues in ascending order.\n        eigvals_asc, eigvecs_mat = np.linalg.eigh(A)\n        \n        # Reverse to get descending order of eigenvalues by magnitude\n        # For positive definite matrices, this is the same as descending order.\n        true_eigvals = eigvals_asc[::-1]\n        true_eigvecs = eigvecs_mat[:, ::-1]\n\n        # Extract dominant and sub-dominant pairs\n        lambda_1_true = true_eigvals[0]\n        lambda_2_true = true_eigvals[1]\n        v1_true = true_eigvecs[:, 0]\n        v2_true = true_eigvecs[:, 1]\n\n        for theta in thetas:\n            # 2. Form the perturbed vector v1_hat(theta)\n            # Since v1 and v2 are orthonormal, the result is already normalized.\n            v1_hat = np.cos(theta) * v1_true + np.sin(theta) * v2_true\n            \n            # 3. Compute the Rayleigh quotient mu(theta)\n            mu_theta = v1_hat.T @ A @ v1_hat\n            \n            # 4. Deflate the matrix A\n            A_tilde = A - mu_theta * np.outer(v1_hat, v1_hat)\n            \n            # 5. Apply power method to the deflated matrix A_tilde\n            lambda_tilde_1, _ = power_method(A_tilde, x0, tolerance, max_iterations)\n            \n            # 6. Report the absolute error\n            error = np.abs(lambda_tilde_1 - lambda_2_true)\n            results.append(error)\n\n    # Format and print the final output\n    formatted_results = [f\"{r:.10f}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3283273"}]}