{"hands_on_practices": [{"introduction": "为加权雅可比（Weighted Jacobi）等定常迭代法选择一个最优的固定松弛参数 $\\omega$，其关键在于估计迭代矩阵的谱范围。由于精确计算特征值成本高昂，我们通常依赖于估算。本实践 [@problem_id:3266561] 将引导你比较两种常用的策略：一种是基于格什戈林圆盘定理（Gershgorin Circle Theorem）的低成本静态界，另一种是基于瑞利商迭代（Rayleigh quotient iteration）的更高精度的动态估计。通过这个练习，你将亲身体会到不同估计方法在精度和计算开销之间的权衡。", "problem": "考虑一个线性系统，其系数矩阵 $A \\in \\mathbb{R}^{n \\times n}$ 是对称正定 (SPD) 矩阵，由带狄利克雷边界条件的泊松方程经标准有限差分法离散化得到。令 $D$ 表示 $A$ 的对角部分，并定义 $B = D^{-1} A$。对于加权雅可比（也称为预处理理查森）迭代法：\n$$\nx^{(k+1)} = x^{(k)} + \\omega D^{-1} \\left(b - A x^{(k)}\\right),\n$$\n其误差传播由迭代矩阵 $M_\\omega = I - \\omega B$ 控制，其中 $I$ 是单位矩阵，$\\omega > 0$ 是松弛参数。\n\n您将比较两种选择 $\\omega$ 的策略，并分析它们预测的收敛率中的误差：\n- 基于 Gershgorin 的策略：使用 $B$ 的谱的 Gershgorin 圆盘界来选择 $\\omega$。\n- 基于瑞利商的策略：使用迭代瑞利商来估计 $B$ 的极端特征值。\n\n仅使用以下基本假设和事实：\n- 在所提供的测试用例中，$A$ 是 SPD 且严格对角占优的，其对角线为常数，因此 $D = c I$（其中 $c > 0$），这使得 $B$ 也是对称正定的。\n- 迭代的收敛性由 $M_\\omega$ 的谱半径 $\\rho(M_\\omega)$ 决定。\n- Gershgorin 圆盘定理保证 $B$ 的所有特征值 $\\lambda$ 都位于圆盘的并集 $\\{ z \\in \\mathbb{C} : |z - b_{ii}| \\leq r_i \\}$ 内，其中 $b_{ii}$ 是 $B$ 在第 $i$ 行的对角元，$r_i = \\sum_{j \\neq i} |b_{ij}|$ 是对应的非对角元大小的行和。\n- 对于对称矩阵 $B$，一个非零向量 $x$ 的瑞利商是 $R_B(x) = \\dfrac{x^\\top B x}{x^\\top x}$，其在所有非零向量 $x$ 上的极值是 $B$ 的最小和最大特征值。\n\n每个测试用例的任务：\n1. 根据测试用例的规范构建 $A$ 并形成 $B = D^{-1} A$。\n2. 使用以下公式计算 $B$ 特征值的 Gershgorin 下界和上界：\n   $$\n   L_G = \\max\\left(0, \\min_i \\left(b_{ii} - r_i\\right)\\right), \\quad U_G = \\max_i \\left(b_{ii} + r_i\\right).\n   $$\n   使用这些界来选择一个松弛参数 $\\omega_G$，该参数能最小化 $\\rho(M_\\omega)$ 的一个上界，形式为 $\\max\\left(|1 - \\omega L_G|, |1 - \\omega U_G|\\right)$。\n3. 使用迭代瑞利商估计 $B$ 的最小和最大特征值：\n   - 使用幂迭代法估计 $\\lambda_{\\max}(B)$，并在最终向量处计算 $R_B(x)$。\n   - 使用反幂迭代法（求解以 $B$ 为系数矩阵的线性系统）估计 $\\lambda_{\\min}(B)$，并在最终向量处计算 $R_B(y)$。\n   选择一个松弛参数 $\\omega_R$，该参数能最小化代理项 $\\max\\left(|1 - \\omega \\widehat{\\lambda}_{\\min}|, |1 - \\omega \\widehat{\\lambda}_{\\max}|\\right)$。\n4. 对每种策略，计算预测的收敛率：\n   - Gershgorin 预测：$\\rho_{\\text{pred},G} = \\max\\left(|1 - \\omega_G L_G|, |1 - \\omega_G U_G|\\right)$。\n   - 瑞利商预测：$\\rho_{\\text{pred},R} = \\max\\left(|1 - \\omega_R \\widehat{\\lambda}_{\\min}|, |1 - \\omega_R \\widehat{\\lambda}_{\\max}|\\right)$。\n5. 使用 $B$ 的真实特征值 $\\{\\lambda_i\\}$ 计算每个选定 $\\omega$ 的实际收敛率：\n   $$\n   \\rho_{\\text{actual}}(\\omega) = \\max_i \\left|1 - \\omega \\lambda_i\\right|.\n   $$\n6. 计算每种策略的预测收敛率与实际收敛率之间的绝对误差：\n   $$\n   \\varepsilon_G = \\left|\\rho_{\\text{pred},G} - \\rho_{\\text{actual}}(\\omega_G)\\right|, \\quad \\varepsilon_R = \\left|\\rho_{\\text{pred},R} - \\rho_{\\text{actual}}(\\omega_R)\\right|.\n   $$\n7. 同时，使用 $B$ 的真实极端特征值计算最小化 $\\rho(M_\\omega)$ 的真实最优松弛参数 $\\omega_\\star$，并与 $\\omega_G$ 和 $\\omega_R$ 一同报告。\n\n测试套件：\n- 用例 1（理想情况）：一维泊松矩阵 $A \\in \\mathbb{R}^{10 \\times 10}$，主对角线元素为 $2$，第一副对角线和第一超对角线元素为 $-1$。\n- 用例 2（小尺寸情况）：一维泊松矩阵 $A \\in \\mathbb{R}^{3 \\times 3}$，定义如上。\n- 用例 3（二维离散化）：二维泊松矩阵 $A \\in \\mathbb{R}^{36 \\times 36}$，用于一个 $6 \\times 6$ 网格，采用五点差分格式，即主对角线元素为 $4$，每个相邻耦合项为 $-1$。\n\n最终输出格式：\n- 您的程序应产生单行输出，包含一个列表，每个测试用例对应其中一个条目。每个条目必须是列表\n  $[\\omega_G, \\omega_R, \\omega_\\star, \\varepsilon_G, \\varepsilon_R]$，\n  并按此顺序，使用无单位实数。整个输出必须是单个逗号分隔的列表，并用方括号括起来，例如：\n  $[[\\cdots],[\\cdots],[\\cdots]]$。", "solution": "用户提供了一个来自数值线性代数领域的有效且良定的问题陈述。任务是比较为加权雅可比迭代法选择松弛参数 $\\omega$ 的两种策略，该方法应用于从泊松方程导出的线性系统。\n\n### 问题表述\n\n我们考虑线性系统 $Ax=b$，其中 $A \\in \\mathbb{R}^{n \\times n}$ 是一个对称正定 (SPD) 矩阵。加权雅可比迭代法由下式给出：\n$$\nx^{(k+1)} = x^{(k)} + \\omega D^{-1} \\left(b - A x^{(k)}\\right)\n$$\n其中 $D$ 是 $A$ 的对角部分，$\\omega > 0$ 是松弛参数。此迭代的收敛性由迭代矩阵 $M_\\omega = I - \\omega B$ 的谱半径决定，其中 $B = D^{-1}A$，$I$ 是单位矩阵。目标是选择 $\\omega$ 以最小化 $\\rho(M_\\omega)$。\n\n$M_\\omega$ 的特征值为 $\\mu_i = 1 - \\omega \\lambda_i$，其中 $\\{\\lambda_i\\}_{i=1}^n$ 是 $B$ 的特征值。因此，谱半径为：\n$$\n\\rho(M_\\omega) = \\max_i |1 - \\omega \\lambda_i|\n$$\n由于 $A$ 是 SPD 矩阵且其对角元素为正，因此 $D$ 也是 SPD 矩阵。问题陈述指明 $D=cI$（对于标量 $c>0$），因此 $B = \\frac{1}{c}A$。由于 $A$ 是 SPD 矩阵，$B$ 也是 SPD 矩阵，其特征值 $\\lambda_i$ 均为实数且为正。设 $\\lambda_{\\min}$ 和 $\\lambda_{\\max}$ 分别为 $B$ 的最小和最大特征值。谱半径可以表示为：\n$$\n\\rho(M_\\omega) = \\max \\left( |1 - \\omega \\lambda_{\\min}|, |1 - \\omega \\lambda_{\\max}| \\right)\n$$\n为了最小化该量，我们令 `max` 函数的两个参数大小相等、符号相反：\n$$\n1 - \\omega \\lambda_{\\min} = -(1 - \\omega \\lambda_{\\max}) = -1 + \\omega \\lambda_{\\max}\n$$\n求解 $\\omega$ 可得最优松弛参数：\n$$\n\\omega_\\star = \\frac{2}{\\lambda_{\\min} + \\lambda_{\\max}}\n$$\n将其代回谱半径的表达式，得到最小可能谱半径，即最优收敛率：\n$$\n\\rho(M_{\\omega_\\star}) = \\frac{\\lambda_{\\max} - \\lambda_{\\min}}{\\lambda_{\\max} + \\lambda_{\\min}}\n$$\n问题的核心是比较两种估计 $\\lambda_{\\min}$ 和 $\\lambda_{\\max}$ 以找到一个合适的 $\\omega$ 的策略。\n\n### 策略 1：Gershgorin 圆盘界\n\nGershgorin 圆盘定理指出，$B$ 的每个特征值都位于圆盘 $G_i = \\{ z \\in \\mathbb{C} : |z - b_{ii}| \\leq r_i \\}$ 的并集内，其中 $r_i = \\sum_{j \\neq i} |b_{ij}|$。由于 $B$ 是对称的，其特征值为实数，因此它们位于区间 $[b_{ii} - r_i, b_{ii} + r_i]$ 的并集内。这为 $B$ 的谱提供了界：\n$$\n\\lambda_{\\min} \\geq \\min_i (b_{ii} - r_i) \\quad \\text{和} \\quad \\lambda_{\\max} \\leq \\max_i (b_{ii} + r_i)\n$$\n问题将基于 Gershgorin 的极端特征值估计值定义为 $L_G = \\max(0, \\min_i(b_{ii} - r_i))$ 和 $U_G = \\max_i(b_{ii} + r_i)$。这些估计值导出了松弛参数 $\\omega_G$ 的选择：\n$$\n\\omega_G = \\frac{2}{L_G + U_G}\n$$\n该策略的预测收敛率为：\n$$\n\\rho_{\\text{pred},G} = \\frac{U_G - L_G}{U_G + L_G}\n$$\n\n### 策略 2：瑞利商估计\n\n该策略使用迭代方法直接估计极端特征值。\n1.  **用于 $\\lambda_{\\max}$ 的幂迭代法**：对 $B$ 应用幂法。从一个随机向量 $x_0$ 开始，迭代 $x_{k+1} = Bx_k / \\|Bx_k\\|_2$ 产生一个向量序列，该序列收敛到对应于主特征值 $\\lambda_{\\max}$ 的特征向量。然后使用最终迭代向量 $x_f$ 的瑞利商计算估计值 $\\widehat{\\lambda}_{\\max}$：\n    $$\n    \\widehat{\\lambda}_{\\max} = R_B(x_f) = \\frac{x_f^\\top B x_f}{x_f^\\top x_f}\n    $$\n2.  **用于 $\\lambda_{\\min}$ 的反幂迭代法**：应用反幂法找到 $B$ 的最小特征值。这等价于对 $B^{-1}$ 应用幂法。从一个随机向量 $y_0$ 开始，迭代 $y_{k+1} = B^{-1}y_k / \\|B^{-1}y_k\\|_2$ 收敛到 $B$ 的对应于其最小特征值 $\\lambda_{\\min}$ 的特征向量。在每一步求解一个线性系统 $By_{k+1}' = y_k$，然后进行归一化。使用最终迭代向量 $y_f$ 的瑞利商计算估计值 $\\widehat{\\lambda}_{\\min}$：\n    $$\n    \\widehat{\\lambda}_{\\min} = R_B(y_f) = \\frac{y_f^\\top B y_f}{y_f^\\top y_f}\n    $$\n这些估计值产生松弛参数 $\\omega_R$ 和预测收敛率 $\\rho_{\\text{pred},R}$：\n$$\n\\omega_R = \\frac{2}{\\widehat{\\lambda}_{\\min} + \\widehat{\\lambda}_{\\max}}, \\quad \\rho_{\\text{pred},R} = \\frac{\\widehat{\\lambda}_{\\max} - \\widehat{\\lambda}_{\\min}}{\\widehat{\\lambda}_{\\max} + \\widehat{\\lambda}_{\\min}}\n$$\n\n### 分析与误差计算\n\n对于每个测试用例，我们执行以下计算：\n1.  构建矩阵 $A$ 并形成 $B = D^{-1}A$。\n2.  使用 Gershgorin 策略计算 $\\omega_G$ 和 $\\rho_{\\text{pred},G}$。\n3.  使用瑞利商策略计算 $\\omega_R$ 和 $\\rho_{\\text{pred},R}$。\n4.  使用数值特征值求解器计算 $B$ 的真实极端特征值，记作 $\\lambda_{\\min}^{\\text{true}}$ 和 $\\lambda_{\\max}^{\\text{true}}$。这些值决定了真实的最优参数 $\\omega_\\star = \\frac{2}{\\lambda_{\\min}^{\\text{true}} + \\lambda_{\\max}^{\\text{true}}}$。\n5.  使用 $B$ 的全部谱计算所选参数 $\\omega_G$ 和 $\\omega_R$ 的实际收敛率：\n    $$\n    \\rho_{\\text{actual}}(\\omega_G) = \\max_i |1 - \\omega_G \\lambda_i^{\\text{true}}|\n    $$\n    $$\n    \\rho_{\\text{actual}}(\\omega_R) = \\max_i |1 - \\omega_R \\lambda_i^{\\text{true}}|\n    $$\n6.  最后，计算预测率的绝对误差：\n    $$\n    \\varepsilon_G = |\\rho_{\\text{pred},G} - \\rho_{\\text{actual}}(\\omega_G)|\n    $$\n    $$\n    \\varepsilon_R = |\\rho_{\\text{pred},R} - \\rho_{\\text{actual}}(\\omega_R)|\n    $$\n\n此过程应用于三个测试用例，这些用例涉及来自一维和二维泊松方程有限差分法离散化的矩阵。结果提供了对两种估计策略准确性的定量比较。Gershgorin 界虽然易于计算，但通常是悲观的（即界限不紧），导致对收敛率的估计不太准确。相比之下，迭代瑞利商方法虽然计算密集，但通常能产生高度准确的极端特征值估计，因此能更精确地预测收敛行为。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef construct_1d_poisson(n):\n    \"\"\"Constructs the n x n 1D Poisson matrix.\"\"\"\n    A = np.zeros((n, n), dtype=float)\n    np.fill_diagonal(A, 2.0)\n    if n  1:\n        off_diag = np.full(n - 1, -1.0)\n        A += np.diag(off_diag, k=1)\n        A += np.diag(off_diag, k=-1)\n    return A\n\ndef construct_2d_poisson(m):\n    \"\"\"Constructs the n x n (n=m*m) 2D Poisson matrix for an m x m grid.\"\"\"\n    n = m * m\n    A_1d = construct_1d_poisson(m)\n    Im = np.eye(m)\n    # The standard 5-point stencil Laplacian is T_m tensor I_m + I_m tensor T_m,\n    # where T_m is the 1D Laplacian A_1d with 2 on diag and -1 on off-diag.\n    A = np.kron(A_1d, Im) + np.kron(Im, A_1d)\n    np.fill_diagonal(A, 4.0) # Ensure diagonal is exactly 4.\n    return A\n\ndef gershgorin_strategy(B):\n    \"\"\"Computes omega and predicted rate using Gershgorin bounds.\"\"\"\n    n = B.shape[0]\n    b_ii = np.diag(B)\n    r_i = np.sum(np.abs(B), axis=1) - np.abs(b_ii)\n    \n    L_g = max(0.0, np.min(b_ii - r_i))\n    U_g = np.max(b_ii + r_i)\n    \n    # Handle case where L_g + U_g is zero\n    if L_g + U_g == 0:\n        omega_g = 1.0 # Default value, though this case is unlikely for SPD matrices\n        rho_pred_g = 1.0\n    else:\n        omega_g = 2.0 / (L_g + U_g)\n        rho_pred_g = (U_g - L_g) / (L_g + U_g)\n        \n    return omega_g, rho_pred_g\n\ndef rayleigh_strategy(B, n, num_iter=100):\n    \"\"\"Computes omega and predicted rate using iterative Rayleigh quotients.\"\"\"\n    # Ensure a non-zero random start vector\n    np.random.seed(42) # for reproducibility\n    x = np.random.rand(n)\n    if np.linalg.norm(x) == 0:\n         x = np.ones(n)\n    x /= np.linalg.norm(x)\n\n    # Power iteration for lambda_max\n    for _ in range(num_iter):\n        Bx = B @ x\n        x = Bx / np.linalg.norm(Bx)\n    lambda_max_hat = (x.T @ B @ x) # x.T @ x is 1\n\n    # Inverse power iteration for lambda_min\n    y = np.random.rand(n)\n    if np.linalg.norm(y) == 0:\n         y = np.ones(n)\n    y /= np.linalg.norm(y)\n\n    for _ in range(num_iter):\n        try:\n            y_new = np.linalg.solve(B, y)\n        except np.linalg.LinAlgError:\n            # If B is singular, lambda_min is 0.\n            # This shouldn't happen for the given problem context (SPD matrices).\n            lambda_min_hat = 0.0\n            break\n        y = y_new / np.linalg.norm(y_new)\n    else: # Only runs if loop completes without break\n        lambda_min_hat = (y.T @ B @ y) # y.T @ y is 1\n\n    # Handle case where sum is zero\n    if lambda_min_hat + lambda_max_hat == 0:\n        omega_r = 1.0\n        rho_pred_r = 1.0\n    else:\n        omega_r = 2.0 / (lambda_min_hat + lambda_max_hat)\n        rho_pred_r = (lambda_max_hat - lambda_min_hat) / (lambda_min_hat + lambda_max_hat)\n        \n    return omega_r, rho_pred_r\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for the given test cases.\n    \"\"\"\n    test_cases = [\n        (10, None), # Case 1\n        (3, None),  # Case 2\n        (36, 6)     # Case 3\n    ]\n\n    all_results = []\n    \n    for n, grid_dim in test_cases:\n        # Step 1: Construct A and B\n        if grid_dim is None:  # 1D case\n            A = construct_1d_poisson(n)\n            c = 2.0\n        else:  # 2D case\n            A = construct_2d_poisson(grid_dim)\n            c = 4.0\n        B = A / c\n\n        # Step 2: Gershgorin Strategy\n        omega_g, rho_pred_g = gershgorin_strategy(B)\n\n        # Step 3: Rayleigh-Quotient Strategy\n        omega_r, rho_pred_r = rayleigh_strategy(B, n)\n        \n        # Step 4: True Optimal Parameter and Actual Rates\n        true_eigenvalues = np.linalg.eigvalsh(B)\n        lambda_min_true = np.min(true_eigenvalues)\n        lambda_max_true = np.max(true_eigenvalues)\n        \n        # Guard against division by zero if matrix is zero matrix\n        if lambda_min_true + lambda_max_true == 0:\n            omega_star = 1.0\n        else:\n            omega_star = 2.0 / (lambda_min_true + lambda_max_true)\n        \n        # Step 5: Actual Convergence Rates\n        rho_actual_g = np.max(np.abs(1.0 - omega_g * true_eigenvalues))\n        rho_actual_r = np.max(np.abs(1.0 - omega_r * true_eigenvalues))\n        \n        # Step 6: Absolute Errors\n        err_g = np.abs(rho_pred_g - rho_actual_g)\n        err_r = np.abs(rho_pred_r - rho_actual_r)\n        \n        # Step 7: Collate results\n        all_results.append([omega_g, omega_r, omega_star, err_g, err_r])\n\n    # Format and print the final output\n    formatted_results = [f\"[{','.join(f'{x:.8f}' for x in res)}]\" for res in all_results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3266561"}, {"introduction": "逐次超松弛（SOR）方法是高斯-赛德尔（Gauss-Seidel）迭代的一种高效加速技术，但其最优松弛参数 $\\omega_{\\text{opt}}$ 的选择比看起来要复杂。一个关键且并非显而易见的特性是，SOR的迭代矩阵结构及其谱半径都强烈依赖于线性方程组中方程的求解顺序。本实践 [@problem_id:3266531] 将通过对比经典的字典序（lexicographic ordering）与红黑排序（red-black ordering），让你清晰地看到仅仅改变方程顺序就能如何显著影响最优参数和收敛速度。", "problem": "考虑线性系统 $A x = b$，其中 $A \\in \\mathbb{R}^{n \\times n}$ 是一个对称正定 (SPD) 矩阵。求解 $A x = b$ 的迭代方法通常始于分解 $A = D - L - U$，其中 $D$ 是 $A$ 的对角部分，$L$ 是 $A$ 的严格下三角部分（选择符号以使 $A = D - L - U$ 成立），$U$ 是具有相同符号约定的严格上三角部分。逐次超松弛 (SOR) 方法通过使用松弛参数 $\\omega \\in (0,2)$ 对 Gauss–Seidel 步进行加权来更新迭代值，其线性误差传播算子依赖于 $\\omega$ 和分裂 $A = D - L - U$。通过一个置换矩阵 $P$ 对未知数进行置换，将系统重排为 $\\tilde{A} = P A P^{\\top}$，并根据新的排序引入一个新的分裂 $\\tilde{A} = \\tilde{D} - \\tilde{L} - \\tilde{U}$。\n\n从分解 $A = D - L - U$ 和 SOR 方法作为加权 Gauss–Seidel 迭代的定义出发，推导误差传播算子作为依赖于分裂和松弛参数的函数 $T(\\omega)$ 的表达式。然后，通过计算其在 $\\omega \\in (0,2)$ 上的谱半径 $\\rho(T(\\omega))$，确定一个数值上最优的松弛参数 $\\omega^\\star$，使得 $\\rho(T(\\omega))$ 最小。\n\n您必须构建并分析以下由矩阵和置换组成的测试套件。对于每种情况，您必须：\n1. 构造矩阵 $A$。\n2. 应用指定的置换向量 $p$，通过 $\\tilde{A}_{ij} = A_{p(i), p(j)}$ 获得 $\\tilde{A}$。\n3. 从 $\\tilde{A}$ 构造 $\\tilde{D}$、$\\tilde{L}$ 和 $\\tilde{U}$，使得 $\\tilde{A} = \\tilde{D} - \\tilde{L} - \\tilde{U}$。\n4. 从第一性原理推导 $T(\\omega)$，并在开区间 $(0,2)$ 上以步长 $0.01$ 的均匀网格上计算 $\\rho(T(\\omega))$，然后选择使 $\\rho(T(\\omega))$ 最小化的网格值作为 $\\omega^\\star$。\n5. 将 $\\omega^\\star$ 和最小谱半径 $\\min_{\\omega} \\rho(T(\\omega))$ 报告为四舍五入到六位小数的十进制浮点数。\n\n测试套件：\n- 情况 1（二维泊松问题，字典序）：$A$ 是在带有 Dirichlet 边界条件的 $N \\times N$ 内部网格上，通过五点有限差分拉普拉斯算子得到的 $N^2 \\times N^2$ SPD 矩阵，其中 $N = 6$。对角线元素为 $4$，每个内部点与其四个网格邻居以 $-1$ 耦合。置换是单位置换（行主字典序）。\n- 情况 2（二维泊松问题，红黑序）：$A$ 与情况 1 相同。置换首先列出所有 $(i+j)$ 为偶数的网格点，然后列出所有 $(i+j)$ 为奇数的网格点，其中 $i$ 和 $j$ 是从零开始的行和列索引。\n- 情况 3（一维泊松问题，字典序）：$A$ 是通过带有 Dirichlet 边界条件的一维有限差分拉普拉斯算子得到的 $n \\times n$ SPD 三对角矩阵，其中 $n = 20$，其对角线上元素为 $2$，第一亚对角线和第一超对角线上元素为 $-1$。置换是单位置换。\n- 情况 4（一维泊松问题，奇偶序）：$A$ 与情况 3 相同。置换首先列出所有偶数索引，然后列出所有奇数索引（从零开始）。\n- 情况 5（对角 SPD 矩阵，单位排序）：$A$ 是一个大小为 $10 \\times 10$ 的对角矩阵 $\\operatorname{diag}(1,2,\\dots,10)$。置换是单位置换。\n- 情况 6（对角 SPD 矩阵，随机排序）：$A$ 与情况 5 相同。置换是 $\\{0,1,\\dots,9\\}$ 的任意固定的非平凡置换。\n\n您的程序必须实现上述步骤，并生成单行输出，其中包含一个由方括号括起来的逗号分隔的浮点数列表，顺序如下：\n$[\\omega^\\star_{1}, \\rho_{\\min,1}, \\omega^\\star_{2}, \\rho_{\\min,2}, \\omega^\\star_{3}, \\rho_{\\min,3}, \\omega^\\star_{4}, \\rho_{\\min,4}, \\omega^\\star_{5}, \\rho_{\\min,5}, \\omega^\\star_{6}, \\rho_{\\min,6}]$。\n\n所有数字都必须报告为四舍五入到六位小数的十进制浮点数。此问题不涉及物理单位或角度；请勿在输出中包含任何单位。您的实现必须是一个完整的、可运行的程序，无需任何输入，并且仅使用指定的库。\n\n目标是构建一个例子，在该例子中，最优松弛参数强烈依赖于未知数的置换，并通过取严格下/上三角部分操作相对于重排序的非交换性来解释此行为，这种非交换性改变了 SOR 迭代算子及其谱。", "solution": "所提出的问题是数值线性代数中一个有效且适定的练习。它具有科学依据、客观，并包含为每个测试案例推导出唯一解所需的所有信息。目标是分析用于求解线性系统 $A x = b$ 的逐次超松弛 (SOR) 迭代法，重点关注最优松弛参数 $\\omega^\\star$ 的选择如何受未知数排序的影响。\n\n我们首先从第一性原理推导 SOR 迭代矩阵，或称误差传播算子 $T(\\omega)$。起点是线性系统 $A x = b$，其中 $A \\in \\mathbb{R}^{n \\times n}$ 是一个给定矩阵。SOR 方法基于将矩阵 $A$ 分裂为其对角、严格下三角和严格上三角部分。根据问题陈述，我们定义此分裂为 $A = D - L - U$，其中 $D$ 是包含 $A$ 对角线元素的对角矩阵，$-L$ 是 $A$ 的严格下三角部分，$-U$ 是 $A$ 的严格上三角部分。\n\n在第 $k+1$ 次迭代中，解向量的第 $i$ 个分量（表示为 $x_i^{(k+1)}$）的 SOR 更新规则是前一迭代值 $x_i^{(k)}$ 和 Gauss-Seidel 更新的加权平均：\n$$\nx_i^{(k+1)} = (1-\\omega) x_i^{(k)} + \\frac{\\omega}{a_{ii}} \\left( b_i - \\sum_{j=1}^{i-1} a_{ij} x_j^{(k+1)} - \\sum_{j=i+1}^{n} a_{ij} x_j^{(k)} \\right)\n$$\n其中 $\\omega \\in (0,2)$ 是松弛参数。为了以矩阵形式表示，我们可以为所有分量 $i=1, \\dots, n$ 重新排列方程：\n$$\na_{ii} x_i^{(k+1)} + \\omega \\sum_{j=1}^{i-1} a_{ij} x_j^{(k+1)} = (1-\\omega) a_{ii} x_i^{(k)} - \\omega \\sum_{j=i+1}^{n} a_{ij} x_j^{(k)} + \\omega b_i\n$$\n使用矩阵分裂的定义，求和可以用矩阵 $L$ 和 $U$ 来表示。左侧是 $(D - \\omega L) x^{(k+1)}$，右侧是 $((1-\\omega)D + \\omega U) x^{(k)} + \\omega b$。这给出了 SOR 迭代的矩阵形式：\n$$\n(D - \\omega L) x^{(k+1)} = ((1-\\omega)D + \\omega U) x^{(k)} + \\omega b\n$$\n因此，下一个迭代值 $x^{(k+1)}$ 是：\n$$\nx^{(k+1)} = (D - \\omega L)^{-1} ((1-\\omega)D + \\omega U) x^{(k)} + \\omega (D - \\omega L)^{-1} b\n$$\n这是一个形如 $x^{(k+1)} = T(\\omega) x^{(k)} + c$ 的定常迭代法。矩阵 $T(\\omega)$ 是误差传播算子，由下式给出：\n$$\nT(\\omega) = (D - \\omega L)^{-1} ((1-\\omega)D + \\omega U)\n$$\n该方法的收敛性由该算子的谱半径 $\\rho(T(\\omega)) = \\max_i |\\lambda_i(T(\\omega))|$ 决定，其中 $\\{\\lambda_i\\}$ 是 $T(\\omega)$ 的特征值。该方法收敛当且仅当 $\\rho(T(\\omega))  1$。最优松弛参数 $\\omega^\\star$ 是区间 $(0,2)$ 中使此谱半径最小化的 $\\omega$ 值。\n\n未知数的一个置换，由置换向量 $p$ 表示，将系统 $A x = b$ 转换为 $\\tilde{A} \\tilde{x} = \\tilde{b}$，其中 $\\tilde{A} = P A P^{\\top}$、$\\tilde{x} = Px$ 和 $\\tilde{b} = Pb$。问题使用向量 $p$ 来定义置换，使得新的矩阵元素为 $\\tilde{A}_{ij} = A_{p(i), p(j)}$。这种重排序为置换后的矩阵引入了一个新的分裂 $\\tilde{A} = \\tilde{D} - \\tilde{L} - \\tilde{U}$。至关重要的是，取矩阵的严格下（或上）三角部分的操作通常与置换不交换。也就是说，$\\tilde{L}$ 不一定等于 $P L P^{\\top}$。因此，置换后系统的 SOR 迭代算子\n$$\n\\tilde{T}(\\omega) = (\\tilde{D} - \\omega \\tilde{L})^{-1} ((1-\\omega)\\tilde{D} + \\omega \\tilde{U})\n$$\n与原始系统的算子不同，其谱 $\\rho(\\tilde{T}(\\omega))$ 也将不同。这意味着最优松弛参数 $\\omega^\\star$ 依赖于方程的排序。\n\n该问题要求对六个不同的情况进行数值搜索以找到 $\\omega^\\star$。对于每种情况，首先构造置换后的矩阵 $\\tilde{A}$。然后，构造其分裂 $\\tilde{A} = \\tilde{D} - \\tilde{L} - \\tilde{U}$。在 $(0,2)$ 的均匀网格上，以 $0.01$ 的步长计算 $\\omega$ 值的谱半径 $\\rho(\\tilde{T}(\\omega))$。选择产生最小谱半径的网格点作为 $\\omega^\\star$ 值。\n\n- **情况 1 和 2** 以及 **情况 3 和 4** 比较了标准的字典序与对于给定的拉普拉斯矩阵已知是“一致排序”的重排序（红黑序、奇偶序）。对于这类排序，存在一个 $\\omega^\\star$ 的解析公式，可用于检验数值结果。从任意排序变为一致排序会显著改变 $\\tilde{L}$ 和 $\\tilde{U}$ 的结构，从而改变最优参数。\n- **情况 5 和 6** 提供了一个反例。对于一个对角矩阵 $A$，任何置换 $P$ 都会得到另一个对角矩阵 $\\tilde{A} = P A P^{\\top}$。对于任何对角矩阵，其严格三角部分 $L$ 和 $U$ 都是零矩阵。因此，$\\tilde{L} = \\tilde{U} = 0$，正如 $L=U=0$ 一样。在这种特殊情况下，分裂操作与置换是可交换的。SOR 算子简化为 $T(\\omega) = (1-\\omega)I$，其谱半径为 $|1-\\omega|$。无论置换如何，这在 $\\omega=1$ 时达到最小值。这些情况展示了一种重排序对 $\\omega^\\star$ 没有影响的场景。\n\n数值实现将通过为每种情况构造矩阵和置换，执行对 $\\omega^\\star$ 的网格搜索，并报告所需的值来进行。", "answer": "```python\nimport numpy as np\n\ndef solve_case(A, p):\n    \"\"\"\n    Solves for the optimal SOR parameter for a given matrix and permutation.\n\n    Args:\n        A (np.ndarray): The original matrix.\n        p (np.ndarray): The permutation vector.\n\n    Returns:\n        tuple: A tuple containing the optimal omega and the minimum spectral radius.\n    \"\"\"\n    # Apply permutation: A_perm[i, j] = A[p[i], p[j]]\n    A_perm = A[np.ix_(p, p)]\n    \n    # Decompose the permuted matrix\n    D_tilde = np.diag(np.diag(A_perm))\n    # Note on sign convention: A = D - L - U, so -L is the strictly lower part.\n    L_tilde = -np.tril(A_perm, k=-1)\n    U_tilde = -np.triu(A_perm, k=1)\n\n    omegas = np.arange(0.01, 2.0, 0.01)\n    min_rho = float('inf')\n    omega_star = -1.0\n\n    for omega in omegas:\n        # Construct the SOR iteration matrix T_omega\n        # T_omega = inv(D - omega*L) * ((1-omega)*D + omega*U)\n        M = D_tilde - omega * L_tilde\n        N_mat = (1 - omega) * D_tilde + omega * U_tilde\n        \n        # Using linalg.solve is more stable than computing the inverse explicitly\n        try:\n            T_omega = np.linalg.solve(M, N_mat)\n        except np.linalg.LinAlgError:\n            continue # Matrix M is singular for this omega, skip\n\n        # Compute spectral radius\n        eigenvalues = np.linalg.eigvals(T_omega)\n        rho = np.max(np.abs(eigenvalues))\n\n        if rho  min_rho:\n            min_rho = rho\n            omega_star = omega\n            \n    return omega_star, min_rho\n\ndef make_laplacian_2d(N):\n    \"\"\"Creates the 2D finite-difference Laplacian matrix.\"\"\"\n    n = N * N\n    A = np.zeros((n, n))\n    for i in range(N):\n        for j in range(N):\n            k = i * N + j\n            A[k, k] = 4\n            if i > 0: A[k, (i-1)*N + j] = -1 # Up\n            if i  N-1: A[k, (i+1)*N + j] = -1 # Down\n            if j > 0: A[k, i*N + (j-1)] = -1 # Left\n            if j  N-1: A[k, i*N + (j+1)] = -1 # Right\n    return A\n\ndef make_laplacian_1d(n):\n    \"\"\"Creates the 1D finite-difference Laplacian matrix.\"\"\"\n    A = np.zeros((n, n))\n    for i in range(n):\n        A[i, i] = 2\n        if i > 0: A[i, i-1] = -1\n        if i  n-1: A[i, i+1] = -1\n    return A\n\ndef get_red_black_perm(N):\n    \"\"\"Generates the red-black ordering permutation for an N x N grid.\"\"\"\n    n = N * N\n    red_indices = []\n    black_indices = []\n    for i in range(N):\n        for j in range(N):\n            k = i * N + j\n            if (i + j) % 2 == 0:\n                red_indices.append(k)\n            else:\n                black_indices.append(k)\n    return np.array(red_indices + black_indices, dtype=int)\n\ndef get_even_odd_perm(n):\n    \"\"\"Generates the even-odd ordering permutation for a 1D vector of size n.\"\"\"\n    even_indices = [i for i in range(n) if i % 2 == 0]\n    odd_indices = [i for i in range(n) if i % 2 != 0]\n    return np.array(even_indices + odd_indices, dtype=int)\n\ndef solve():\n    \"\"\"Main function to run all test cases and print results.\"\"\"\n    \n    # Case 1: 2D Poisson, N=6, lexicographic\n    N1 = 6\n    n1 = N1 * N1\n    A1 = make_laplacian_2d(N1)\n    p1 = np.arange(n1)\n    \n    # Case 2: 2D Poisson, N=6, red-black\n    A2 = A1\n    p2 = get_red_black_perm(N1)\n    \n    # Case 3: 1D Poisson, n=20, lexicographic\n    n3 = 20\n    A3 = make_laplacian_1d(n3)\n    p3 = np.arange(n3)\n    \n    # Case 4: 1D Poisson, n=20, even-odd\n    A4 = A3\n    p4 = get_even_odd_perm(n3)\n    \n    # Case 5: Diagonal SPD, identity ordering\n    n5 = 10\n    A5 = np.diag(np.arange(1, n5 + 1))\n    p5 = np.arange(n5)\n    \n    # Case 6: Diagonal SPD, random (reversed) ordering\n    A6 = A5\n    p6 = np.arange(n5 - 1, -1, -1)\n\n    test_cases = [\n        (A1, p1),\n        (A2, p2),\n        (A3, p3),\n        (A4, p4),\n        (A5, p5),\n        (A6, p6),\n    ]\n\n    results = []\n    for A, p in test_cases:\n        omega_star, min_rho = solve_case(A, p)\n        results.append(round(omega_star, 6))\n        results.append(round(min_rho, 6))\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3266531"}, {"introduction": "在许多实际问题中，预先计算一个固定的最优松弛参数 $\\omega$ 可能非常困难或不切实际。一种更灵活的替代方案是在迭代过程中根据算法的实际表现动态调整参数。本练习 [@problem_id:3266497] 将指导你实现一种自适应的理查森迭代法（Richardson iteration），该方法通过监控残差范数的变化并采用回溯策略（将 $\\omega$ 减半）来保证收敛。这种方法展示了一种无需预先估计谱信息也能确保稳健性的实用技术。", "problem": "考虑使用自适应选择的松弛参数 $\\omega$ 通过稳态理查森迭代法求解线性系统 $A x = b$。设 $A \\in \\mathbb{R}^{n \\times n}$ 为对称正定 (SPD) 矩阵，且 $b \\in \\mathbb{R}^{n}$。将第 $k$ 次迭代的残差定义为 $r_k = b - A x_k$。带松弛的理查森更新公式为 $x_{k+1} = x_k + \\omega r_k$。您需要根据以下规则实现 $\\omega$ 的自适应选择。\n\n自适应松弛参数的算法规则：\n- 给定当前迭代值 $x_k$、其残差 $r_k$ 以及当前松弛参数 $\\omega_k > 0$，尝试进行试验性更新 $x_{k+1}^{\\mathrm{trial}} = x_k + \\omega_k r_k$ 并计算 $r_{k+1}^{\\mathrm{trial}} = b - A x_{k+1}^{\\mathrm{trial}}$。\n- 如果 $\\| r_{k+1}^{\\mathrm{trial}} \\|_2 \\le \\| r_k \\|_2$，则接受该步：设置 $x_{k+1} \\leftarrow x_{k+1}^{\\mathrm{trial}}$，$r_{k+1} \\leftarrow r_{k+1}^{\\mathrm{trial}}$，并为下一次迭代保留 $\\omega_{k+1} \\leftarrow \\omega_k$。\n- 如果 $\\| r_{k+1}^{\\mathrm{trial}} \\|_2 > \\| r_k \\|_2$，则拒绝该步并通过减半来回溯松弛参数：设置 $\\omega_k \\leftarrow \\omega_k / 2$ 并从相同的 $x_k$ 和 $r_k$ 重试；重复减半操作，直到某次试验步骤满足 $\\| r_{k+1}^{\\mathrm{trial}} \\|_2 \\le \\| r_k \\|_2$，然后如上所述接受该步。\n\n理论任务：\n- 仅从残差的定义、更新规则以及实对称矩阵的谱定理出发，推导出一个关于 $\\omega$ 的充分条件（用 $A$ 的特征值表示），在该条件下，对于任意残差向量 $r_k$，残差都是收缩的。解释为什么对于任何 SPD 矩阵 $A$ 和任何初始 $\\omega_k > 0$，回溯减半过程必须在有限次减半后终止，并证明被接受的步骤在欧几里得范数下是收缩的。根据 $A$ 的谱和当前的 $\\omega_k$，给出一个在任何迭代中所需的最大减半次数的界。\n- 解释为什么被接受的迭代会产生一个非增的残差范数序列 $\\{\\| r_k \\|_2\\}$，并讨论在序列 $\\{\\omega_k\\}$ 满足何种附加条件下可以获得线性收敛率。\n\n实现任务：\n- 实现上述自适应理查森方法。对于下方的每个测试用例，精确运行 $N$ 次被接受的迭代，其中 $N$ 由测试用例指定。当前的 $\\omega$ 在被接受的步骤之间保持不变，并且仅当试验步骤导致残差范数增加时，才通过回溯规则减小。如果在任何点 $\\| r_k \\|_2 = 0$，您可以提前终止，并为剩余的已接受步骤计数保留当前的 $x_k$ 和 $\\omega_k$。\n\n测试套件：\n- 测试用例 1：\n  - $A = \\mathrm{diag}([1, 10, 50])$\n  - $b = [1, 1, 1]^\\top$\n  - $x_0 = [0, 0, 0]^\\top$\n  - $\\omega_0 = 1.0$\n  - $N = 10$\n- 测试用例 2：\n  - $A = \\begin{bmatrix} 4  1  0 \\\\ 1  3  0.5 \\\\ 0  0.5  2 \\end{bmatrix}$\n  - $b = [1, 2, 3]^\\top$\n  - $x_0 = [0, 0, 0]^\\top$\n  - $\\omega_0 = 0.3$\n  - $N = 10$\n- 测试用例 3：\n  - $A = \\mathrm{diag}([0.01, 1, 2])$\n  - $b = [1, 1, 1]^\\top$\n  - $x_0 = [0, 0, 0]^\\top$\n  - $\\omega_0 = 1.2$\n  - $N = 12$\n- 测试用例 4：\n  - $A \\in \\mathbb{R}^{5 \\times 5}$ 是主对角线上为 $2$，第一副对角线和第一超对角线上为 $-1$ 的三对角矩阵：\n    $A = \\begin{bmatrix}\n    2  -1  0  0  0\\\\\n    -1  2  -1  0  0\\\\\n    0  -1  2  -1  0\\\\\n    0  0  -1  2  -1\\\\\n    0  0  0  -1  2\n    \\end{bmatrix}$\n  - $b = [1, 0, 0, 0, 1]^\\top$\n  - $x_0 = [0, 0, 0, 0, 0]^\\top$\n  - $\\omega_0 = 0.8$\n  - $N = 8$\n\n最终输出规格：\n- 对于每个测试用例，计算 $N$ 次被接受的迭代后的最终残差的欧几里得范数 $\\| r_{\\mathrm{final}} \\|_2$ 和最终松弛参数 $\\omega_{\\mathrm{final}}$。您的程序应生成单行输出，其中包含所有按顺序聚合的结果，形式为方括号括起来的逗号分隔列表：\n  - $[\\lVert r_{\\mathrm{final}}^{(1)} \\rVert_2, \\omega_{\\mathrm{final}}^{(1)}, \\lVert r_{\\mathrm{final}}^{(2)} \\rVert_2, \\omega_{\\mathrm{final}}^{(2)}, \\lVert r_{\\mathrm{final}}^{(3)} \\rVert_2, \\omega_{\\mathrm{final}}^{(3)}, \\lVert r_{\\mathrm{final}}^{(4)} \\rVert_2, \\omega_{\\mathrm{final}}^{(4)}]$\n- 将输出中的每个浮点数四舍五入到恰好 $6$ 位小数。\n- 不涉及物理单位。不使用角度。\n\n您提交的必须是一个完整的、可运行的程序，该程序完全按照规定执行这些计算，并且只打印所需的单行结果。不允许用户输入。", "solution": "该问题是有效的。它提出了一种定义明确的数值算法，即用于求解线性系统的自适应理查森迭代法，并基于数值线性代数的既定原则，提出了清晰的理论和实现任务。为测试用例提供了所有必要的数据和条件，问题具有科学合理性和客观性。\n\n### 理论分析\n\n该问题要求对求解线性系统 $A x = b$ 的自适应理查森方法进行理论分析，其中 $A \\in \\mathbb{R}^{n \\times n}$ 是对称正定 (SPD) 矩阵。迭代公式为 $x_{k+1} = x_k + \\omega r_k$，残差为 $r_k = b - A x_k$。\n\n#### 1. 收缩的充分条件与回溯的终止性\n\n首先，我们推导残差的更新规则。\n$$\nr_{k+1} = b - A x_{k+1} = b - A(x_k + \\omega r_k) = (b - A x_k) - \\omega A r_k = r_k - \\omega A r_k\n$$\n这可以写成 $r_{k+1} = (I - \\omega A) r_k$，其中 $I$ 是单位矩阵。矩阵 $G(\\omega) = I - \\omega A$ 是迭代矩阵。\n\n为使残差范数收缩，我们需要对于任何非零残差 $r_k$ 都有 $\\|r_{k+1}\\|_2  \\|r_k\\|_2$。如果迭代矩阵的诱导 2-范数小于 1，即 $\\|G(\\omega)\\|_2  1$，则可以保证这一点。\n\n由于 $A$ 是实对称矩阵，根据谱定理，它有一组完整的标准正交特征向量 $v_1, \\dots, v_n$，对应实特征值 $\\lambda_1, \\dots, \\lambda_n$。由于 $A$ 是正定的，其所有特征值都为正：对所有 $i$ 都有 $\\lambda_i > 0$。设 $\\lambda_{\\min} = \\min_i \\lambda_i$ 和 $\\lambda_{\\max} = \\max_i \\lambda_i$ 分别是 $A$ 的最小和最大特征值。\n\n迭代矩阵 $G(\\omega) = I - \\omega A$ 也是对称的。$G(\\omega)$ 的特征值为 $\\mu_i = 1 - \\omega \\lambda_i$。对于对称矩阵，其诱导 2-范数等于其谱半径（其特征值绝对值的最大值）：\n$$\n\\|G(\\omega)\\|_2 = \\rho(G(\\omega)) = \\max_{i} |1 - \\omega \\lambda_i|\n$$\n保证收缩的条件 $\\|G(\\omega)\\|_2  1$ 变为 $\\max_{i} |1 - \\omega \\lambda_i|  1$。这等价于要求对所有 $i=1, \\dots, n$ 都有 $-1  1 - \\omega \\lambda_i  1$。\n\n我们来分析这两个不等式：\n1.  $1 - \\omega \\lambda_i  1 \\implies -\\omega \\lambda_i  0$。由于 $\\omega > 0$ 且 $\\lambda_i > 0$，此不等式恒成立。\n2.  $-1  1 - \\omega \\lambda_i \\implies \\omega \\lambda_i  2 \\implies \\omega  2 / \\lambda_i$。这必须对所有特征值 $\\lambda_i$ 成立。最严格的条件是针对最大特征值 $\\lambda_{\\max}$。\n\n因此，对于任何非零残差，使得残差范数收缩的关于 $\\omega$ 的一个充分条件是 $0  \\omega  2 / \\lambda_{\\max}$。\n\n如果 $\\| r_{k+1}^{\\mathrm{trial}} \\|_2 \\le \\| r_k \\|_2$，回溯算法就接受该步骤。如果 $\\| G(\\omega) \\|_2 \\le 1$，这是可以保证的，它等价于 $\\max_i |1 - \\omega \\lambda_i| \\le 1$。根据上面的推导，如果 $\\omega \\le 2 / \\lambda_{\\max}$，这个条件成立。\n回溯过程从一个给定的 $\\omega_k > 0$ 开始，如果被拒绝，则将其替换为 $\\omega_k/2$。经过 $m$ 次减半后，新的参数是 $\\omega_k' = \\omega_k / 2^m$。一旦 $\\omega_k' \\le 2/\\lambda_{\\max}$，该步骤就会被接受。这个不等式可以写成 $\\omega_k / 2^m \\le 2/\\lambda_{\\max}$，这意味着 $2^m \\ge \\omega_k \\lambda_{\\max} / 2$。取以 2 为底的对数，我们得到 $m \\ge \\log_2(\\omega_k \\lambda_{\\max} / 2)$。由于 $\\omega_k > 0$ 且 $\\lambda_{\\max} > 0$，对数的参数是正的，因此满足此条件的有限整数 $m$ 总是存在的。这证明了减半过程必须终止。在任何迭代 $k$ 中所需的最大减半次数受 $\\max(0, \\lceil \\log_2(\\omega_k \\lambda_{\\max} / 2) \\rceil)$ 的限制，其中向上取整函数确保结果为整数，而 $\\max$ 处理初始 $\\omega_k$ 已满足条件的情况。被接受的步骤是一个非扩张，即 $\\| r_{k+1} \\|_2 \\le \\| r_k \\|_2$。除非 $\\| G(\\omega) \\|_2 = 1$，否则它是一个严格收缩，这种情况可能发生在 $\\omega = 2/\\lambda_i$（对于某个 $i$）且 $r_k$ 位于相应的特征空间中。减半操作确保 $\\omega$ 最终将进入开区间 $(0, 2/\\lambda_{\\max})$，从而保证对于任何非零残差都是严格收缩的。\n\n#### 2. 非增残差范数与线性收敛\n\n由接受准则的定义可知，由被接受的迭代产生的残差范数序列 $\\{\\| r_k \\|_2\\}$ 是非增的：$\\| r_{k+1} \\|_2 \\le \\| r_k \\|_2$。\n\n如果存在一个常数 $C \\in [0, 1)$，使得对于所有足够大的 $k$ 都有 $\\| r_{k+1} \\|_2 \\le C \\| r_k \\|_2$，则实现了线性收敛。在步骤 $k$ 使用被接受的参数 $\\omega_k$ 时的收缩因子是 $c_k = \\| I - \\omega_k A \\|_2$。为了实现线性收敛，我们需要收缩因子序列有界且远离 1，即存在一个常数 $C  1$，使得对所有 $k$ 都有 $c_k \\le C$。\n\n收缩因子是 $c_k = \\max(|1 - \\omega_k \\lambda_{\\min}|, |1 - \\omega_k \\lambda_{\\max}|)$。所述的自适应算法仅通过回溯来减小 $\\omega$。虽然这能确保如果初始 $\\omega_0$ 过大，它将被减小到稳定范围 $\\omega \\le 2/\\lambda_{\\max}$，但这并不能阻止 $\\omega$ 变得任意小。如果出于某种原因，算法必须反复回溯，那么被接受的参数序列 $\\{\\omega_k\\}$ 可能会收敛到 0。如果 $\\omega_k \\to 0$，则 $c_k \\to \\max(|1-0|, |1-0|) = 1$。这将导致收敛停滞，失去线性收敛率。\n\n因此，保证线性收敛率的一个附加条件是，被接受的松弛参数序列 $\\{\\omega_k\\}$ 必须有界且远离零。也就是说，必须存在某个 $\\omega_{\\min} > 0$ 使得对所有 $k$ 都有 $\\omega_k \\ge \\omega_{\\min}$。如果这个条件成立，并且我们还有一个上界 $\\omega_k \\le \\omega_{\\max}  2/\\lambda_{\\max}$，那么收敛因子 $c_k$ 就被 $C = \\max(|1 - \\omega_{\\min} \\lambda_{\\min}|, |1 - \\omega_{\\max} \\lambda_{\\max}|)  1$ 一致地界定，从而确保了线性收敛。\n\n### 实现\n\n实现将遵循所描述的算法。对于 $N$ 个要求的已接受步骤中的每一步，我们执行一个试错过程。一个内部循环尝试用当前的 $\\omega_k$ 进行更新。如果残差范数增加，则将 $\\omega_k$ 减半，并用相同的 $x_k$ 和 $r_k$ 重复试验。一旦试验步骤被接受，状态 $(x, r)$ 就会更新，成功的 $\\omega$ 会被带到下一次迭代中。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef adaptive_richardson(A, b, x0, omega0, N):\n    \"\"\"\n    Solves Ax = b using an adaptive Richardson iteration.\n\n    Args:\n        A (np.ndarray): The matrix A of the system.\n        b (np.ndarray): The vector b of the system.\n        x0 (np.ndarray): The initial guess for x.\n        omega0 (float): The initial relaxation parameter.\n        N (int): The number of accepted iterations to perform.\n\n    Returns:\n        tuple[float, float]: A tuple containing the final residual norm\n                             and the final relaxation parameter.\n    \"\"\"\n    x = np.copy(x0).astype(np.float64)\n    omega = float(omega0)\n    \n    # Initial residual\n    r = b - A @ x\n    \n    for _ in range(N):\n        norm_r_k = np.linalg.norm(r)\n        \n        # Terminate early if the solution is found\n        if np.isclose(norm_r_k, 0.0):\n            break\n\n        omega_trial = omega\n        \n        # Backtracking loop to find an acceptable omega\n        while True:\n            # Trial update using the current omega_trial\n            x_trial = x + omega_trial * r\n            r_trial = b - A @ x_trial\n            norm_r_trial = np.linalg.norm(r_trial)\n            \n            # Acceptance condition\n            if norm_r_trial = norm_r_k:\n                # Accept the step\n                x = x_trial\n                r = r_trial\n                omega = omega_trial  # Keep this omega for the next iteration\n                break  # Exit backtracking and move to the next of N steps\n            else:\n                # Reject the step, halve omega and retry\n                omega_trial /= 2.0\n                \n    final_norm_r = np.linalg.norm(r)\n    final_omega = omega\n    \n    return final_norm_r, final_omega\n\ndef solve():\n    \"\"\"\n    Defines test cases and runs the adaptive Richardson solver,\n    then prints the results in the specified format.\n    \"\"\"\n    \n    # Test case 1\n    A1 = np.diag([1.0, 10.0, 50.0])\n    b1 = np.array([1.0, 1.0, 1.0])\n    x01 = np.array([0.0, 0.0, 0.0])\n    omega01 = 1.0\n    N1 = 10\n    \n    # Test case 2\n    A2 = np.array([\n        [4.0, 1.0, 0.0],\n        [1.0, 3.0, 0.5],\n        [0.0, 0.5, 2.0]\n    ])\n    b2 = np.array([1.0, 2.0, 3.0])\n    x02 = np.array([0.0, 0.0, 0.0])\n    omega02 = 0.3\n    N2 = 10\n    \n    # Test case 3\n    A3 = np.diag([0.01, 1.0, 2.0])\n    b3 = np.array([1.0, 1.0, 1.0])\n    x03 = np.array([0.0, 0.0, 0.0])\n    omega03 = 1.2\n    N3 = 12\n\n    # Test case 4\n    diag4 = 2.0 * np.ones(5)\n    off_diag4 = -1.0 * np.ones(4)\n    A4 = np.diag(diag4) + np.diag(off_diag4, k=1) + np.diag(off_diag4, k=-1)\n    b4 = np.array([1.0, 0.0, 0.0, 0.0, 1.0])\n    x04 = np.array([0.0, 0.0, 0.0, 0.0, 0.0])\n    omega04 = 0.8\n    N4 = 8\n\n    test_cases = [\n        (A1, b1, x01, omega01, N1),\n        (A2, b2, x02, omega02, N2),\n        (A3, b3, x03, omega03, N3),\n        (A4, b4, x04, omega04, N4),\n    ]\n\n    results = []\n    for A, b, x0, omega0, N in test_cases:\n        # Note: The provided solution had a subtle bug where it recomputed A@r inside the while loop.\n        # This is inefficient. x_trial and r_trial can be computed more cheaply.\n        # x_trial = x + omega_trial * r\n        # r_trial = r - omega_trial * (A @ r)\n        # This correction is only to the logic, not the provided code.\n        # However, the provided user code did not follow this optimization and I am emulating it.\n        # Correcting the bug in my local `adaptive_richardson` for accuracy of results.\n        x = np.copy(x0).astype(np.float64)\n        omega = float(omega0)\n        r = b - A @ x\n        for _ in range(N):\n            norm_r_k = np.linalg.norm(r)\n            if np.isclose(norm_r_k, 0.0): break\n            omega_trial = omega\n            Ar = A @ r\n            while True:\n                r_trial = r - omega_trial * Ar\n                norm_r_trial = np.linalg.norm(r_trial)\n                if norm_r_trial = norm_r_k:\n                    x = x + omega_trial * r\n                    r = r_trial\n                    omega = omega_trial\n                    break\n                else:\n                    omega_trial /= 2.0\n        \n        final_norm_r = np.linalg.norm(r)\n        final_omega = omega\n\n        results.append(final_norm_r)\n        results.append(final_omega)\n\n    # Format output as a comma-separated list of numbers rounded to 6 decimal places.\n    output_str = \",\".join([f\"{val:.6f}\" for val in results])\n    print(f\"[{output_str}]\")\n\nsolve()\n```", "id": "3266497"}]}