{"hands_on_practices": [{"introduction": "在数值微分中，我们面临一个核心的权衡困境。当我们减小步长 $h$ 时，公式的截断误差会减小，理论上使近似结果更精确。然而，在有限精度的计算机世界里，过小的 $h$ 会导致灾难性的抵消效应，从而放大舍入误差。这个练习将引导你对总误差（截断误差与舍入误差之和）进行建模，以找到平衡这两者的“最佳点”，即那个能使总误差最小化的最优步长 $h$ [@problem_id:2391199]。", "problem": "考虑使用对称三点中心差分\n$$\nD(h)\\equiv \\frac{f(1+h)-f(1-h)}{2h}.\n$$\n来近似计算函数 $f(x)=\\exp(x)$ 在点 $x=1$ 处的一阶导数。\n假设函数求值在浮点运算中执行，该运算采用舍入到最近值的方式，单位舍入为 $u=2^{-53}$，因此每个计算出的函数值满足 $\\operatorname{fl}(f(t))=f(t)\\,(1+\\delta)$，其中 $|\\delta|\\le u$。为了本次分析的目的，假设 $D(h)$ 中的减法和除法是精确的。使用这些假设和基本原理，通过组合截断误差和舍入误差，为 $x=1$ 处的总绝对误差构建一个上界模型，并确定使该界最小化的步长 $h>0$ 的值。以一个实数的形式提供最优的 $h$。将您的答案四舍五入到 $3$ 位有效数字。", "solution": "该问题要求通过对总误差进行建模和最小化，来确定有限差分近似的最优步长 $h$。总误差是近似公式中固有的截断误差和有限精度运算产生的舍入误差的组合。\n\n首先，我们验证问题的陈述。\n已知条件是：\n- 函数：$f(x) = \\exp(x)$。\n- 求值点：$x=1$。\n- 近似公式：$D(h) = \\frac{f(1+h)-f(1-h)}{2h}$。\n- 浮点误差模型：$\\operatorname{fl}(f(t)) = f(t)(1+\\delta)$，其中 $|\\delta| \\le u$。\n- 单位舍入：$u=2^{-53}$。\n- 算术假设：减法和除法是精确的。\n- 目标：找到使总绝对误差的上界最小化的最优步长 $h_{\\text{opt}}$。\n\n该问题在数值分析方面有科学依据，是适定且客观的。没有可识别的缺陷。问题是有效的。我们继续进行求解。\n\n令 $\\tilde{D}(h)$ 表示在浮点运算中计算出的中心差分公式的值。总绝对误差 $E_{\\text{total}}$ 由精确导数 $f'(1)$ 与计算出的近似值 $\\tilde{D}(h)$ 之差的绝对值给出：\n$$ E_{\\text{total}} = |f'(1) - \\tilde{D}(h)| $$\n通过引入精确算术近似 $D(h)$，我们可以使用三角不等式将此误差分解为两部分：\n$$ E_{\\text{total}} = |f'(1) - D(h) + D(h) - \\tilde{D}(h)| \\le |f'(1) - D(h)| + |D(h) - \\tilde{D}(h)| $$\n项 $E_{\\text{trunc}}(h) = |f'(1) - D(h)|$ 是截断误差，其产生的原因是有限差分公式是导数的一个近似。项 $E_{\\text{round}}(h) = |D(h) - \\tilde{D}(h)|$ 是舍入误差，其产生于计算机算术的有限精度。\n\n现在我们将为每个误差分量推导一个上界，以构建我们的误差模型。\n\n**截断误差分析**\n我们使用泰勒定理将函数 $f(x)$ 在点 $x=1$ 附近展开。对于函数 $f(x) = \\exp(x)$，它的所有阶导数也都是 $\\exp(x)$。因此，对于所有 $n \\ge 0$，$f^{(n)}(1) = \\exp(1) = e$。\n$f(1+h)$ 和 $f(1-h)$ 的泰勒级数展开为：\n$$ f(1+h) = f(1) + f'(1)h + \\frac{f''(1)}{2!}h^2 + \\frac{f'''(1)}{3!}h^3 + O(h^4) $$\n$$ f(1-h) = f(1) - f'(1)h + \\frac{f''(1)}{2!}h^2 - \\frac{f'''(1)}{3!}h^3 + O(h^4) $$\n用第一个展开式减去第二个展开式：\n$$ f(1+h) - f(1-h) = 2f'(1)h + \\frac{2f'''(1)}{6}h^3 + O(h^5) $$\n将此结果代入 $D(h)$ 的公式中：\n$$ D(h) = \\frac{2f'(1)h + \\frac{1}{3}f'''(1)h^3 + O(h^5)}{2h} = f'(1) + \\frac{f'''(1)}{6}h^2 + O(h^4) $$\n因此，截断误差为：\n$$ E_{\\text{trunc}}(h) = |D(h) - f'(1)| = \\left| \\frac{f'''(1)}{6}h^2 + O(h^4) \\right| $$\n对于小的 $h$，主导项与 $h^2$ 成正比。为了建立我们模型的上界，我们使用首项。对于 $f(x) = \\exp(x)$ 在 $x=1$ 处，我们有 $f'''(1)=e$。因此，一个简单的截断误差界模型是：\n$$ E_{\\text{trunc}}(h) \\le \\frac{e}{6}h^2 $$\n\n**舍入误差分析**\n计算出的近似值 $\\tilde{D}(h)$ 使用函数求值的浮点数值：\n$$ \\tilde{D}(h) = \\frac{\\operatorname{fl}(f(1+h)) - \\operatorname{fl}(f(1-h))}{2h} $$\n使用给定的浮点误差模型 $\\operatorname{fl}(f(t)) = f(t)(1+\\delta)$，其中 $|\\delta| \\le u$，我们有：\n$$ \\tilde{D}(h) = \\frac{f(1+h)(1+\\delta_1) - f(1-h)(1+\\delta_2)}{2h} $$\n其中 $|\\delta_1| \\le u$ 且 $|\\delta_2| \\le u$。舍入误差为：\n$$ E_{\\text{round}}(h) = |D(h) - \\tilde{D}(h)| = \\left| \\frac{f(1+h)-f(1-h)}{2h} - \\frac{f(1+h)(1+\\delta_1) - f(1-h)(1+\\delta_2)}{2h} \\right| $$\n$$ E_{\\text{round}}(h) = \\left| \\frac{-f(1+h)\\delta_1 + f(1-h)\\delta_2}{2h} \\right| \\le \\frac{|f(1+h)||\\delta_1| + |f(1-h)||\\delta_2|}{2h} $$\n使用 $\\delta_i$ 的界：\n$$ E_{\\text{round}}(h) \\le \\frac{(|f(1+h)| + |f(1-h)|)u}{2h} $$\n为了构建模型，我们对小 $h$ 情况下的函数值进行近似。当 $h \\to 0$ 时，$f(1+h)$ 和 $f(1-h)$ 都趋近于 $f(1)=e$。由于 $\\exp(x)$ 始终为正，因此不需要绝对值。我们将分子建模为近似 $e+e=2e$。\n$$ E_{\\text{round}}(h) \\le \\frac{2eu}{2h} = \\frac{eu}{h} $$\n\n**总误差界的最小化**\n我们通过将截断误差和舍入误差的上界相加来构建总误差界模型 $E(h)$：\n$$ E(h) = \\frac{e}{6}h^2 + \\frac{eu}{h} $$\n为了找到使该函数最小化的 $h$ 值，我们计算它关于 $h$ 的导数并令其为零：\n$$ \\frac{dE}{dh} = \\frac{d}{dh} \\left( \\frac{e}{6}h^2 + eu h^{-1} \\right) = \\frac{2e}{6}h - eu h^{-2} = \\frac{e}{3}h - \\frac{eu}{h^2} $$\n令导数为零可得到最优的 $h$：\n$$ \\frac{e}{3}h = \\frac{eu}{h^2} \\implies h^3 = \\frac{3eu}{e} = 3u $$\n$$ h_{\\text{opt}} = (3u)^{1/3} $$\n二阶导数 $\\frac{d^2E}{dh^2} = \\frac{e}{3} + \\frac{2eu}{h^3}$ 对于 $h>0$ 始终为正，这证实了该 $h$ 值确实对应于一个最小值。\n\n**数值计算**\n给定 $u=2^{-53}$。将此代入我们的 $h_{\\text{opt}}$ 表达式中：\n$$ h_{\\text{opt}} = (3 \\times 2^{-53})^{1/3} $$\n数值为：\n$$ h_{\\text{opt}} \\approx (3 \\times 1.1102230246 \\times 10^{-16})^{1/3} \\approx (3.3306690738 \\times 10^{-16})^{1/3} \\approx 6.931833 \\times 10^{-6} $$\n根据要求将此结果四舍五入到 $3$ 位有效数字，得到 $6.93 \\times 10^{-6}$。", "answer": "$$\\boxed{6.93 \\times 10^{-6}}$$", "id": "2391199"}, {"introduction": "在理解了误差的基本原理之后，这项练习将我们从理论带入一个常见的实际挑战：处理带噪声的实验数据。当从位置测量值中估算速度和加速度等导数时，有限差分法会显著放大测量数据中固有的噪声。通过这个编程练习，你将亲身体验这一现象，从经验和理论两个层面量化噪声的放大效应，并理解为什么高阶导数对噪声更敏感 [@problem_id:2392343]。", "problem": "给定一个关于一维位置随时间变化的时间序列模型，该模型包含加性测量噪声。位置是一个已知的平滑时间函数，每个采样点都叠加了零均值、独立的噪声。您的任务是设计并实现一个程序，该程序针对一组测试用例，构建含噪的位置数据，使用有限差分法估计速度和加速度，并定量分析微分算子对测量噪声的放大效应。\n\n您的推理必须仅基于以下基本定义和事实：函数的导数是差商的极限，线性算子作用于独立随机变量时，其输出的方差根据算子权重的平方和进行叠加，以及泰勒级数展开可用于推导有限差分格式的局部截断误差阶。除这些原则外，请勿使用任何其他预封装公式。\n\n使用以下信号模型。无噪声位置为\n$$\nx(t) = A \\sin(2\\pi f_1 t) + C \\sin(2\\pi f_2 t) + D t^2,\n$$\n参数为\n$$\nA = 1.0\\ \\text{m},\\quad C = 0.5\\ \\text{m},\\quad f_1 = 1.0\\ \\text{Hz},\\quad f_2 = 3.0\\ \\text{Hz},\\quad D = 0.05\\ \\text{m/s}^2.\n$$\n三角函数中的角度以弧度为单位。精确的速度和加速度分别为\n$$\nv(t) = \\frac{dx}{dt} = 2\\pi f_1 A \\cos(2\\pi f_1 t) + 2\\pi f_2 C \\cos(2\\pi f_2 t) + 2 D t,\n$$\n$$\na(t) = \\frac{d^2 x}{dt^2} = - (2\\pi f_1)^2 A \\sin(2\\pi f_1 t) - (2\\pi f_2)^2 C \\sin(2\\pi f_2 t) + 2 D.\n$$\n\n采样与噪声模型。对于每个测试用例，在以下时间点进行均匀采样\n$$\nt_n = n \\,\\Delta t,\\quad n=0,1,\\dots,N-1,\\quad N = \\left\\lfloor \\frac{T}{\\Delta t}\\right\\rfloor + 1,\\quad T = 10\\ \\text{s},\n$$\n并形成含噪测量值\n$$\nx_n^{\\text{noisy}} = x(t_n) + \\eta_n,\\quad \\eta_n \\sim \\mathcal{N}(0,\\sigma_x^2)\\ \\text{i.i.d.},\n$$\n为了可复现性，使用一个固定的随机种子，其值为$12345$。所有距离单位为米，时间单位为秒。\n\n有限差分要求。从第一性原理出发，为一阶和二阶导数推导并实现二阶精度的有限差分格式如下：\n- 对于内部点，使用中心、二阶精度的格式。\n- 在两个边界处，使用单侧、二阶精度的格式。\n您的实现必须生成与输入$x_n$长度相同的数组$v_n^{\\text{FD}}$和$a_n^{\\text{FD}}$。\n\n噪声放大分析。设索引$i$处的有限差分导数为线性组合\n$$\ny_i = \\sum_{j} w_{i,j} x_j,\n$$\n其中$y_i$表示一阶或二阶导数估计值，$w_{i,j}$是有限差分权重除以$\\Delta t$的适当次幂。仅使用期望的线性性质和噪声样本的独立性，推导并计算：\n- 一阶导数的经验均方根（RMS）噪声放大，\n$$\ng_v^{\\text{emp}} = \\sqrt{\\frac{1}{N}\\sum_{i=0}^{N-1} \\left( v_i^{\\text{FD}}[x^{\\text{noisy}}] - v_i^{\\text{FD}}[x^{\\text{clean}}]\\right)^2 },\n$$\n和二阶导数的经验均方根（RMS）噪声放大，\n$$\ng_a^{\\text{emp}} = \\sqrt{\\frac{1}{N}\\sum_{i=0}^{N-1} \\left( a_i^{\\text{FD}}[x^{\\text{noisy}}] - a_i^{\\text{FD}}[x^{\\text{clean}}]\\right)^2 }.\n$$\n- 由权重预测的理论均方根（RMS）噪声放大，\n$$\ng^{\\text{theory}} = \\sigma_x \\sqrt{ \\frac{1}{N} \\sum_{i=0}^{N-1} \\left( \\sum_{j} w_{i,j}^2 \\right) }.\n$$\n使用每个索引（包括边界格式）实际使用的权重计算$g_v^{\\text{theory}}$和$g_a^{\\text{theory}}$。\n\n性能指标。对于每个测试用例，计算：\n- 速度估计值相对于精确速度的均方根误差，\n$$\nE_v = \\sqrt{\\frac{1}{N}\\sum_{i=0}^{N-1} \\left( v_i^{\\text{FD}}[x^{\\text{noisy}}] - v(t_i) \\right)^2 } \\ \\text{in m/s}.\n$$\n- 加速度估计值相对于精确加速度的均方根误差，\n$$\nE_a = \\sqrt{\\frac{1}{N}\\sum_{i=0}^{N-1} \\left( a_i^{\\text{FD}}[x^{\\text{noisy}}] - a(t_i) \\right)^2 } \\ \\text{in m/s}^2.\n$$\n- 比率\n$$\nR_v = \\frac{ g_v^{\\text{emp}} }{ g_v^{\\text{theory}} },\\qquad R_a = \\frac{ g_a^{\\text{emp}} }{ g_a^{\\text{theory}} },\n$$\n该比率表明经验噪声放大与理论预测的匹配程度。\n\n测试套件。在以下四个改变采样间隔和噪声水平的测试用例上运行您的程序：\n- 用例 $1$：$\\Delta t = 0.01\\ \\text{s}$，$\\sigma_x = 0.001\\ \\text{m}$。\n- 用例 $2$：$\\Delta t = 0.1\\ \\text{s}$，$\\sigma_x = 0.001\\ \\text{m}$。\n- 用例 $3$：$\\Delta t = 0.01\\ \\text{s}$，$\\sigma_x = 0.01\\ \\text{m}$。\n- 用例 $4$：$\\Delta t = 0.001\\ \\text{s}$，$\\sigma_x = 0.001\\ \\text{m}$。\n\n最终输出格式。您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，汇总所有结果。对于从$1$到$4$的每个用例，按以下顺序附加四个浮点数：$E_v$（单位 m/s）、$E_a$（单位 m/s$^2$）、$R_v$（无量纲）、$R_a$（无量纲）。每个数字必须以科学记数法打印，并精确到六位有效数字。例如，整体输出格式为\n$$\n[\\ E_{v,1},\\ E_{a,1},\\ R_{v,1},\\ R_{a,1},\\ E_{v,2},\\ E_{a,2},\\ R_{v,2},\\ R_{a,2},\\ E_{v,3},\\ E_{a,3},\\ R_{v,3},\\ R_{a,3},\\ E_{v,4},\\ E_{a,4},\\ R_{v,4},\\ R_{a,4}\\ ].\n$$", "solution": "我们从第一性原理出发。函数$x(t)$在时间$t$的一阶导数由以下极限定义\n$$\n\\frac{dx}{dt}(t) = \\lim_{\\Delta t \\to 0} \\frac{x(t+\\Delta t) - x(t-\\Delta t)}{2\\Delta t},\n$$\n这表明对于有限但较小的$\\Delta t$，可以使用对称（中心）差商。二阶导数定义为\n$$\n\\frac{d^2x}{dt^2}(t) = \\lim_{\\Delta t \\to 0} \\frac{x(t+\\Delta t) - 2x(t) + x(t-\\Delta t)}{\\Delta t^2}.\n$$\n使用围绕$t_i = i \\Delta t$的泰勒级数展开，\n$$\nx(t_{i\\pm 1}) = x(t_i) \\pm \\Delta t\\, x'(t_i) + \\frac{\\Delta t^2}{2} x''(t_i) \\pm \\frac{\\Delta t^3}{6} x^{(3)}(t_i) + \\frac{\\Delta t^4}{24} x^{(4)}(t_i) + \\mathcal{O}(\\Delta t^5),\n$$\n我们可以为内部点推导出二阶精度的中心格式：\n$$\nx'(t_i) = \\frac{x_{i+1} - x_{i-1}}{2\\Delta t} + \\mathcal{O}(\\Delta t^2), \\quad\nx''(t_i) = \\frac{x_{i+1} - 2 x_i + x_{i-1}}{\\Delta t^2} + \\mathcal{O}(\\Delta t^2),\n$$\n其中$x_i = x(t_i)$。在边界处，无法构建对称差分；作为替代，使用前向点的泰勒展开可以得到单侧、二阶精度的差分格式。对于左边界$i=0$处的一阶导数，\n$$\nx'(t_0) = \\frac{-3 x_0 + 4 x_1 - x_2}{2\\Delta t} + \\mathcal{O}(\\Delta t^2),\n$$\n类似地，对于右边界$i=N-1$，\n$$\nx'(t_{N-1}) = \\frac{3 x_{N-1} - 4 x_{N-2} + x_{N-3}}{2\\Delta t} + \\mathcal{O}(\\Delta t^2).\n$$\n对于二阶导数，前向和后向的二阶精度单侧格式为\n$$\nx''(t_0) = \\frac{2 x_0 - 5 x_1 + 4 x_2 - x_3}{\\Delta t^2} + \\mathcal{O}(\\Delta t^2), \\quad\nx''(t_{N-1}) = \\frac{2 x_{N-1} - 5 x_{N-2} + 4 x_{N-3} - x_{N-4}}{\\Delta t^2} + \\mathcal{O}(\\Delta t^2).\n$$\n这些公式是通过求解方程组得到的，该方程组通过逐项匹配泰勒展开来消除低阶误差项，从而确保二阶精度。\n\n噪声放大分析基于线性性质。设将$\\{x_j\\}$映射到导数估计值$\\{y_i\\}$的有限差分算子是线性的：\n$$\ny_i = \\sum_{j} w_{i,j} x_j,\n$$\n其中$w_{i,j}$是算子权重，包括根据导数阶数由$\\Delta t$的适当次幂进行的归一化。假设测量值包含加性零均值、独立噪声$\\eta_j$，其方差为$\\mathbb{V}[\\eta_j] = \\sigma_x^2$。由于线性和独立性，\n$$\n\\mathbb{E}[y_i] = \\sum_{j} w_{i,j} \\mathbb{E}[x_j], \\quad\n\\mathbb{V}[y_i] = \\sum_{j} w_{i,j}^2 \\,\\mathbb{V}[\\eta_j] = \\sigma_x^2 \\sum_{j} w_{i,j}^2.\n$$\n因此，索引$i$处噪声分量的均方根（RMS）等于\n$$\n\\sqrt{\\mathbb{V}[y_i]} = \\sigma_x \\sqrt{\\sum_{j} w_{i,j}^2}.\n$$\n一个在所有索引上计算的全局RMS，与实践中使用的经验RMS一致，是\n$$\ng^{\\text{theory}} = \\sigma_x \\sqrt{\\frac{1}{N}\\sum_{i=0}^{N-1} \\left(\\sum_{j} w_{i,j}^2\\right)}.\n$$\n该表达式通过与索引相关的权重，自然地包含了边界效应。\n\n由此得出两个推论：\n- 对于一阶导数，权重与$1/\\Delta t$成比例，因此$g_v^{\\text{theory}} \\propto \\sigma_x/\\Delta t$；也就是说，如果每个样本的位置噪声水平固定，减小$\\Delta t$会增加速度估计中的噪声放大。\n- 对于二阶导数，权重与$1/\\Delta t^2$成比例，因此$g_a^{\\text{theory}} \\propto \\sigma_x/\\Delta t^2$，这意味着噪声的放大效应更强。\n\n通过比较微分算子应用于含噪数据和纯净数据的结果，可以分离出经验均方根（RMS）噪声放大，这样做可以消除确定性的截断误差：\n$$\ng^{\\text{emp}} = \\sqrt{\\frac{1}{N}\\sum_{i=0}^{N-1} \\left( y_i[x^{\\text{noisy}}] - y_i[x^{\\text{clean}}] \\right)^2 }.\n$$\n因为$y_i[\\cdot]$是线性的，且$x^{\\text{noisy}} = x^{\\text{clean}} + \\eta$，所以差值等于$y_i[\\eta]$，在大量样本的极限下，其RMS与上面推导的理论表达式相匹配。因此，比率\n$$\nR_v = \\frac{g_v^{\\text{emp}}}{g_v^{\\text{theory}}},\\qquad R_a = \\frac{g_a^{\\text{emp}}}{g_a^{\\text{theory}}},\n$$\n在经验平均具有代表性时，应接近于$1$。\n\n算法设计：\n- 构建时间样本$t_i = i \\Delta t$，对于$i=0,\\dots,N-1$，$T=10$秒。\n- 根据给定的解析表达式计算纯净位置$x(t_i)$，以及精确速度$v(t_i)$和加速度$a(t_i)$。\n- 使用固定的种子生成加性噪声$\\eta_i \\sim \\mathcal{N}(0,\\sigma_x^2)$以确保可复现性，并构成$x^{\\text{noisy}}_i = x(t_i) + \\eta_i$。\n- 为一阶和二阶导数实现二阶精度的有限差分估计器，该估计器在内部应用中心格式，在边界应用单侧格式，从而为纯净和含噪输入都生成$v^{\\text{FD}}$和$a^{\\text{FD}}$。\n- 计算性能指标：$E_v$和$E_a$作为与精确导数相比的RMS误差。计算$g_v^{\\text{emp}}$和$g_a^{\\text{emp}}$作为对含噪和纯净数据应用有限差分输出之间的RMS差异。通过在每个索引处对权重平方求和并取平均值，然后乘以$\\sigma_x$并取平方根，来计算$g_v^{\\text{theory}}$和$g_a^{\\text{theory}}$。最后，计算$R_v$和$R_a$作为经验放大与理论放大的比率。\n- 对四个指定的测试用例重复此过程。将合并的结果以要求的单行、带括号、逗号分隔的列表形式打印，使用科学记数法和六位有效数字。\n\n预期趋势：\n- 将$\\sigma_x$增加10倍，应使$g^{\\text{emp}}$以及$E_v$和$E_a$中由噪声主导的分量增加10倍。\n- 增加$\\Delta t$应使$g_v^{\\text{theory}}$大致按$1/\\Delta t$减小，使$g_a^{\\text{theory}}$大致按$1/\\Delta t^2$减小；然而，截断误差以$\\mathcal{O}(\\Delta t^2)$增长，因此由于截断误差和噪声放大之间的权衡，$E_v$和$E_a$可能不会随$\\Delta t$单调减小。\n- 比率$R_v$和$R_a$应接近于$1$，从而验证线性噪声放大分析。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef position(t, A=1.0, C=0.5, f1=1.0, f2=3.0, D=0.05):\n    # x(t) in meters\n    return A * np.sin(2*np.pi*f1*t) + C * np.sin(2*np.pi*f2*t) + D * t**2\n\ndef velocity_true(t, A=1.0, C=0.5, f1=1.0, f2=3.0, D=0.05):\n    # v(t) in m/s\n    return 2*np.pi*f1*A * np.cos(2*np.pi*f1*t) + 2*np.pi*f2*C * np.cos(2*np.pi*f2*t) + 2*D*t\n\ndef acceleration_true(t, A=1.0, C=0.5, f1=1.0, f2=3.0, D=0.05):\n    # a(t) in m/s^2\n    return -(2*np.pi*f1)**2 * A * np.sin(2*np.pi*f1*t) - (2*np.pi*f2)**2 * C * np.sin(2*np.pi*f2*t) + 2*D\n\ndef fd_first_derivative(x, dt):\n    \"\"\"\n    Second-order accurate finite difference for first derivative.\n    - One-sided 3-point at boundaries (order 2).\n    - Centered 3-point in interior (order 2).\n    Returns y of same length as x.\n    \"\"\"\n    n = x.size\n    y = np.empty_like(x, dtype=float)\n    if n  3:\n        raise ValueError(\"Need at least 3 points for second-order first derivative\")\n    # Left boundary: (-3 x0 + 4 x1 - x2)/(2 dt)\n    y[0] = (-3.0*x[0] + 4.0*x[1] - 1.0*x[2]) / (2.0*dt)\n    # Interior\n    y[1:-1] = (x[2:] - x[:-2]) / (2.0*dt)\n    # Right boundary: (3 xN-1 - 4 xN-2 + xN-3)/(2 dt)\n    y[-1] = (3.0*x[-1] - 4.0*x[-2] + 1.0*x[-3]) / (2.0*dt)\n    return y\n\ndef fd_second_derivative(x, dt):\n    \"\"\"\n    Second-order accurate finite difference for second derivative.\n    - One-sided 4-point at boundaries (order 2).\n    - Centered 3-point in interior (order 2).\n    Returns y of same length as x.\n    \"\"\"\n    n = x.size\n    y = np.empty_like(x, dtype=float)\n    if n  4:\n        raise ValueError(\"Need at least 4 points for second derivative with second-order accuracy\")\n    # Left boundary: (2 x0 - 5 x1 + 4 x2 - x3)/dt^2\n    y[0] = (2.0*x[0] - 5.0*x[1] + 4.0*x[2] - 1.0*x[3]) / (dt*dt)\n    # Interior\n    y[1:-1] = (x[2:] - 2.0*x[1:-1] + x[:-2]) / (dt*dt)\n    # Right boundary: (2 xN-1 - 5 xN-2 + 4 xN-3 - xN-4)/dt^2\n    y[-1] = (2.0*x[-1] - 5.0*x[-2] + 4.0*x[-3] - 1.0*x[-4]) / (dt*dt)\n    return y\n\ndef weights_sqsum_first(n, dt):\n    \"\"\"\n    For each index i, compute sum_j w_{i,j}^2 for the first derivative operator.\n    Returns an array s of length n with s[i] = sum_j w_{i,j}^2.\n    \"\"\"\n    s = np.zeros(n, dtype=float)\n    inv = 1.0 / (2.0*dt)\n    # Left boundary: (-3, 4, -1)/(2 dt)\n    coeffs = np.array([-3.0, 4.0, -1.0]) * inv\n    s[0] = np.sum(coeffs**2)\n    # Interior: [-1, +1] at i-1 and i+1\n    c = np.array([-1.0, 1.0]) * inv\n    val = np.sum(c**2)\n    s[1:-1] = val\n    # Right boundary: (1, -4, 3)/(2 dt) applied to (i-2, i-1, i)\n    coeffs = np.array([1.0, -4.0, 3.0]) * inv\n    s[-1] = np.sum(coeffs**2)\n    return s\n\ndef weights_sqsum_second(n, dt):\n    \"\"\"\n    For each index i, compute sum_j w_{i,j}^2 for the second derivative operator.\n    Returns an array s of length n with s[i] = sum_j w_{i,j}^2.\n    \"\"\"\n    s = np.zeros(n, dtype=float)\n    inv2 = 1.0 / (dt*dt)\n    # Left boundary: (2, -5, 4, -1)/dt^2\n    coeffs = np.array([2.0, -5.0, 4.0, -1.0]) * inv2\n    s[0] = np.sum(coeffs**2)\n    # Interior: (1, -2, 1)/dt^2\n    c = np.array([1.0, -2.0, 1.0]) * inv2\n    val = np.sum(c**2)\n    s[1:-1] = val\n    # Right boundary: (-1, 4, -5, 2)/dt^2 applied to (i-3, i-2, i-1, i)\n    coeffs = np.array([-1.0, 4.0, -5.0, 2.0]) * inv2\n    s[-1] = np.sum(coeffs**2)\n    return s\n\ndef rms(x):\n    return np.sqrt(np.mean(np.square(x)))\n\ndef format_float(x):\n    # Scientific notation with exactly six significant figures\n    return f\"{x:.6e}\"\n\ndef run_case(dt, sigma_x, rng):\n    T = 10.0\n    N = int(np.floor(T/dt)) + 1\n    t = np.linspace(0.0, dt*(N-1), N)\n    x_clean = position(t)\n    # Generate noise with given sigma\n    noise = rng.normal(loc=0.0, scale=sigma_x, size=N)\n    x_noisy = x_clean + noise\n\n    # True derivatives\n    v_true = velocity_true(t)\n    a_true = acceleration_true(t)\n\n    # Finite difference estimates\n    v_fd_clean = fd_first_derivative(x_clean, dt)\n    a_fd_clean = fd_second_derivative(x_clean, dt)\n    v_fd_noisy = fd_first_derivative(x_noisy, dt)\n    a_fd_noisy = fd_second_derivative(x_noisy, dt)\n\n    # RMS errors against exact\n    E_v = rms(v_fd_noisy - v_true)\n    E_a = rms(a_fd_noisy - a_true)\n\n    # Empirical noise amplification (difference noisy-clean)\n    g_v_emp = rms(v_fd_noisy - v_fd_clean)\n    g_a_emp = rms(a_fd_noisy - a_fd_clean)\n\n    # Theoretical noise amplification from weights\n    s1 = weights_sqsum_first(N, dt)\n    s2 = weights_sqsum_second(N, dt)\n    g_v_theory = sigma_x * np.sqrt(np.mean(s1))\n    g_a_theory = sigma_x * np.sqrt(np.mean(s2))\n\n    # Ratios (avoid division by zero, though here not zero)\n    R_v = g_v_emp / g_v_theory if g_v_theory  0 else np.nan\n    R_a = g_a_emp / g_a_theory if g_a_theory  0 else np.nan\n\n    return E_v, E_a, R_v, R_a\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each tuple: (dt [s], sigma_x [m])\n    test_cases = [\n        (0.01, 0.001),   # Case 1\n        (0.1, 0.001),    # Case 2\n        (0.01, 0.01),    # Case 3\n        (0.001, 0.001),  # Case 4\n    ]\n\n    rng = np.random.default_rng(12345)\n\n    results = []\n    for dt, sigma_x in test_cases:\n        E_v, E_a, R_v, R_a = run_case(dt, sigma_x, rng)\n        results.extend([E_v, E_a, R_v, R_a])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(format_float(x) for x in results)}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "2392343"}, {"introduction": "现在，我们将运用有限差分法来解决一个更宏大且重要的问题：求解偏微分方程（PDE）。泊松方程是物理和工程领域的基石，描述了从静电学到热流等多种现象。在这项综合性实践中，你将使用拉普拉斯算子的五点差分格式，将这个偏微分方程转化为一个大型线性方程组，并进一步实现一个高效的迭代求解器（共轭梯度法）来求解它。这个练习展示了有限差分作为一种工具，在解决复杂科学问题中所蕴含的强大能力 [@problem_id:3227880]。", "problem": "考虑在单位正方形域上、带有狄利克雷边界条件的二维泊松方程，其形式如下：给定在 $\\Omega = [0,1] \\times [0,1]$ 上的一个二阶连续可微标量场 $\\phi(x,y)$ 及其拉普拉斯算子 $\\nabla^2 \\phi(x,y) = \\rho(x,y)$，使用有限差分法在均匀笛卡尔网格上近似 $\\phi$。出发的基本点是笛卡尔坐标系中拉普拉斯算子的定义 $\\nabla^2 \\phi = \\frac{\\partial^2 \\phi}{\\partial x^2} + \\frac{\\partial^2 \\phi}{\\partial y^2}$，以及围绕网格点的泰勒级数展开，以获得空间导数的二阶中心差分近似。你必须使用泰勒级数展开推导 $\\frac{\\partial^2 \\phi}{\\partial x^2}$ 和 $\\frac{\\partial^2 \\phi}{\\partial y^2}$ 的标准二阶中心差分格式，假设狄利克雷边界条件由已知的边界函数 $g(x,y)$ 定义，将离散算子组装成一个对应于负拉普拉斯算子 $- \\nabla^2$ 的大型稀疏矩阵（在狄利克雷条件下，该矩阵是对称正定的），并使用从第一性原理推导出的迭代求解器求解得到的线性系统。\n\n你的程序必须执行以下步骤：\n\n- 用一个在 $x$ 方向有 $N_x$ 个点、在 $y$ 方向有 $N_y$ 个点的均匀笛卡尔网格对 $\\Omega$ 进行离散化，间距分别为 $h_x = \\frac{1}{N_x - 1}$ 和 $h_y = \\frac{1}{N_y - 1}$。内部未知量是索引为 $i \\in \\{1,\\dots,N_x-2\\}$ 和 $j \\in \\{1,\\dots,N_y-2\\}$ 的网格点 $(x_i,y_j)$。\n- 仅使用网格点周围的基本泰勒级数展开，推导二阶导数的二阶中心差分公式，并结合它们以获得拉普拉斯算子的五点模板表示。由此，构建内部未知量上负拉普拉斯算子 $- \\nabla^2$ 的稀疏矩阵表示。你必须以压缩稀疏行（CSR）格式（压缩稀疏行（CSR）格式存储非零值及其列索引，以及行指针）显式表示此稀疏矩阵。\n- 通过右侧向量来合并狄利克雷边界条件。将内部未知向量 $\\mathbf{u}$ 的离散系统写成 $A \\mathbf{u} = \\mathbf{b}$ 的形式，其中 $A$ 是 $- \\nabla^2$ 的 CSR 矩阵，$\\mathbf{b}$ 集合了 $- \\rho(x_i,y_j)$ 以及来自边界值 $g(x,y)$ 的贡献。\n- 从最速下降法出发，通过使用 $A$-正交搜索方向进行增强，推导并实现用于对称正定矩阵的共轭梯度法。使用此方法求解 $A \\mathbf{u} = \\mathbf{b}$ 直至达到指定的容差。\n\n为了验证，使用解析解来一致地定义 $\\rho(x,y)$ 和 $g(x,y)$：\n\n- 对于三角函数情况：设 $\\phi(x,y) = \\sin(\\pi x)\\sin(\\pi y)$。则 $\\rho(x,y) = \\nabla^2 \\phi(x,y) = -2\\pi^2 \\sin(\\pi x)\\sin(\\pi y)$，边界函数为 $g(x,y) = \\phi(x,y)$，在 $\\partial \\Omega$ 上为 $0$。\n- 对于多项式情况：设 $\\phi(x,y) = x^2 + y^2 - x - y$。则 $\\rho(x,y) = \\nabla^2 \\phi(x,y) = 4$，边界函数为 $g(x,y) = \\phi(x,y)$，在 $\\partial \\Omega$ 上非零。\n\n测试套件规范。你的程序必须运行以下四个测试用例，并对每个用例计算内部网格近似相对于解析解 $\\phi(x,y)$ 的均方根误差：\n\n- 测试 1：$N_x = 32$, $N_y = 32$, $\\phi(x,y) = \\sin(\\pi x)\\sin(\\pi y)$, $\\rho(x,y) = -2\\pi^2 \\sin(\\pi x)\\sin(\\pi y)$。\n- 测试 2：$N_x = 8$, $N_y = 12$, $\\phi(x,y) = \\sin(\\pi x)\\sin(\\pi y)$, $\\rho(x,y) = -2\\pi^2 \\sin(\\pi x)\\sin(\\pi y)$。\n- 测试 3：$N_x = 3$, $N_y = 3$, $\\phi(x,y) = \\sin(\\pi x)\\sin(\\pi y)$, $\\rho(x,y) = -2\\pi^2 \\sin(\\pi x)\\sin(\\pi y)$。\n- 测试 4：$N_x = 17$, $N_y = 33$, $\\phi(x,y) = x^2 + y^2 - x - y$, $\\rho(x,y) = 4$。\n\n所有三角函数的角度单位均为弧度。\n\n数值输出规范：\n\n- 对每个测试用例，计算均方根误差 $E = \\sqrt{\\frac{1}{M}\\sum_{k=1}^{M} \\left( u_k - \\phi(x_k,y_k) \\right)^2}$，其中 $M = (N_x-2)(N_y-2)$，并将 $E$ 四舍五入到八位小数。\n- 你的程序应生成单行输出，其中包含一个逗号分隔的列表，用方括号括起来，例如 $\\left[ e_1, e_2, e_3, e_4 \\right]$，其中每个 $e_i$ 是对应测试用例的四舍五入后的浮点数。\n\n不允许外部输入。最终打印的行必须严格遵守上述格式。", "solution": "本任务要求在单位正方形域 $\\Omega = [0,1] \\times [0,1]$ 上，对带有指定狄利克雷边界条件的二维泊松方程 $\\nabla^2 \\phi = \\rho$ 进行数值求解。解将通过有限差分法获得，这将导出一个线性方程组，并使用共轭梯度法求解。从数值方案的推导到求解器的实现，整个过程详述如下。\n\n### 第一部分：有限差分离散化\n有限差分法的核心是使用离散网格点上的函数值来近似导数。我们首先推导二阶导数的二阶中心差分近似。\n\n考虑一个足够光滑的一维函数 $f(x)$。函数 $f(x)$ 在点 $x_i$ 附近的泰勒级数展开为：\n$$f(x_i + h) = f(x_i) + h f'(x_i) + \\frac{h^2}{2} f''(x_i) + \\frac{h^3}{6} f'''(x_i) + \\mathcal{O}(h^4)$$\n$$f(x_i - h) = f(x_i) - h f'(x_i) + \\frac{h^2}{2} f''(x_i) - \\frac{h^3}{6} f'''(x_i) + \\mathcal{O}(h^4)$$\n将这两个展开式相加，我们得到：\n$$f(x_i + h) + f(x_i - h) = 2 f(x_i) + h^2 f''(x_i) + \\mathcal{O}(h^4)$$\n求解 $f''(x_i)$ 可得二阶中心差分公式：\n$$f''(x_i) = \\frac{f(x_i - h) - 2f(x_i) + f(x_i + h)}{h^2} + \\mathcal{O}(h^2)$$\n此近似的误差为 $h^2$ 阶。\n\n我们将此原理应用于均匀笛卡尔网格上的二维函数 $\\phi(x,y)$。设网格点为 $(x_i, y_j)$，其中 $x_i = i h_x$ 且 $y_j = j h_y$。令 $\\phi_{i,j} = \\phi(x_i, y_j)$。在内部点 $(x_i, y_j)$ 处的二阶偏导数近似为：\n$$\\frac{\\partial^2 \\phi}{\\partial x^2}\\bigg|_{(i,j)} \\approx \\frac{\\phi_{i-1,j} - 2\\phi_{i,j} + \\phi_{i+1,j}}{h_x^2}$$\n$$\\frac{\\partial^2 \\phi}{\\partial y^2}\\bigg|_{(i,j)} \\approx \\frac{\\phi_{i,j-1} - 2\\phi_{i,j} + \\phi_{i,j+1}}{h_y^2}$$\n拉普拉斯算子 $\\nabla^2 = \\frac{\\partial^2}{\\partial x^2} + \\frac{\\partial^2}{\\partial y^2}$ 随之离散化为：\n$$\\nabla^2 \\phi \\bigg|_{(i,j)} \\approx \\frac{\\phi_{i-1,j} - 2\\phi_{i,j} + \\phi_{i+1,j}}{h_x^2} + \\frac{\\phi_{i,j-1} - 2\\phi_{i,j} + \\phi_{i,j+1}}{h_y^2}$$\n将此代入泊松方程 $\\nabla^2 \\phi(x_i, y_j) = \\rho(x_i, y_j)$ 可得：\n$$\\frac{\\phi_{i-1,j} - 2\\phi_{i,j} + \\phi_{i+1,j}}{h_x^2} + \\frac{\\phi_{i,j-1} - 2\\phi_{i,j} + \\phi_{i,j+1}}{h_y^2} = \\rho_{i,j}$$\n问题是根据负拉普拉斯算子 $- \\nabla^2$ 提出的，这导出了离散方程：\n$$ - \\left( \\frac{\\phi_{i-1,j} - 2\\phi_{i,j} + \\phi_{i+1,j}}{h_x^2} + \\frac{\\phi_{i,j-1} - 2\\phi_{i,j} + \\phi_{i,j+1}}{h_y^2} \\right) = -\\rho_{i,j}$$\n重新整理各项以组合 $\\phi$ 的系数，得到每个内部网格点 $(i,j)$ 的五点模板方程：\n$$ \\left(\\frac{2}{h_x^2} + \\frac{2}{h_y^2}\\right)\\phi_{i,j} - \\frac{1}{h_x^2}\\phi_{i-1,j} - \\frac{1}{h_x^2}\\phi_{i+1,j} - \\frac{1}{h_y^2}\\phi_{i,j-1} - \\frac{1}{h_y^2}\\phi_{i,j+1} = -\\rho_{i,j} $$\n\n### 第二部分：组装线性系统 $A \\mathbf{u} = \\mathbf{b}$\n所有 $M = (N_x-2)(N_y-2)$ 个内部点的方程组构成了一个大型稀疏线性系统 $A\\mathbf{u} = \\mathbf{b}$。\n\n**未知向量 $\\mathbf{u}$**：内部网格点处的未知 $\\phi$ 值被展平成一个大小为 $M$ 的单个向量 $\\mathbf{u}$。一种常见的映射是行主序，其中二维网格索引 $(i,j)$（$i \\in \\{1,\\dots,N_x-2\\}$ 和 $j \\in \\{1,\\dots,N_y-2\\}$）映射到一维向量索引 $k = (j-1)(N_x-2) + (i-1)$。\n\n**矩阵 $A$**：矩阵 $A$ 表示离散的负拉普拉斯算子。它是一个 $M \\times M$ 矩阵。对于对应于第 $k$ 个未知量（在网格点 $(i,j)$ 处）的方程：\n- 对角线元素 $A_{k,k}$ 是 $\\phi_{i,j}$ 的系数，即 $\\left(\\frac{2}{h_x^2} + \\frac{2}{h_y^2}\\right)$。\n- 非对角线元素对应于四个邻居。位于 $(i',j')$（映射到索引 $k'$）的邻居的系数是 $A_{k,k'}$。例如，对于邻居 $(i+1, j)$（映射到 $k+1$），系数是 $-\\frac{1}{h_x^2}$。\n矩阵 $A$ 是稀疏、对称的，并且在狄利克雷边界条件下是正定的。它具有五条非零对角线的带状结构。为了存储效率，它以压缩稀疏行（CSR）格式表示，该格式使用三个数组：`data` 用于非零值，`indices` 用于其列索引，`indptr` 用于标记每行的起始位置。\n\n**右侧向量 $\\mathbf{b}$**：大小为 $M$ 的向量 $\\mathbf{b}$ 包含源项以及来自边界条件的贡献。对于第 $k$ 个方程（在点 $(i,j)$ 处），对应的元素 $b_k$ 最初是 $-\\rho_{i,j}$。如果 $(i,j)$ 的四个邻居中的任何一个位于边界上，它们的 $\\phi$ 值由狄利克雷条件 $\\phi|_{\\partial\\Omega}=g(x,y)$ 给出。这些已知项被移到右侧。例如，如果点 $(i-1,j)$ 在边界上（即 $i=1$），其值为 $\\phi_{0,j} = g(x_0, y_j)$。左侧的项 $-\\frac{1}{h_x^2}\\phi_{0,j}$ 被移过去，因此我们将 $\\frac{1}{h_x^2}g(x_0, y_j)$ 加到 $b_k$ 上。\n\n### 第三部分：共轭梯度法\n共轭梯度（CG）法是一种迭代算法，旨在求解大型稀疏线性系统 $A\\mathbf{u} = \\mathbf{b}$，其中 $A$ 是对称正定的。它通过选择 $A$-正交的搜索方向来改进最速下降法。\n\n$A\\mathbf{u} = \\mathbf{b}$ 的解是二次型 $f(\\mathbf{u}) = \\frac{1}{2}\\mathbf{u}^T A \\mathbf{u} - \\mathbf{u}^T \\mathbf{b}$ 的唯一最小化子。该二次型的梯度为 $\\nabla f(\\mathbf{u}) = A\\mathbf{u} - \\mathbf{b}$。负梯度 $-\\nabla f(\\mathbf{u}) = \\mathbf{b} - A\\mathbf{u}$ 是残差 $\\mathbf{r}$，并指向最速下降方向。\n\nCG 方法生成一系列收敛到真实解的迭代 $\\mathbf{u}_k$。在每一步 $k$，它通过沿搜索方向 $\\mathbf{p}_k$ 从 $\\mathbf{u}_k$ 移动来计算新的解 $\\mathbf{u}_{k+1}$：$\\mathbf{u}_{k+1} = \\mathbf{u}_k + \\alpha_k \\mathbf{p}_k$。与最速下降法中 $\\mathbf{p}_k = \\mathbf{r}_k$ 不同，CG 构建的搜索方向是 $A$-正交的：当 $i \\neq j$ 时，$\\mathbf{p}_i^T A \\mathbf{p}_j = 0$。这是通过取当前残差 $\\mathbf{r}_{k+1}$ 并以 $A$-内积的意义上移除任何与前一个搜索方向 $\\mathbf{p}_k$ 平行的分量来实现的。\n\n标准的 CG 算法如下：\n1. 初始化：$\\mathbf{u}_0$（例如，$\\mathbf{0}$），计算残差 $\\mathbf{r}_0 = \\mathbf{b} - A\\mathbf{u}_0$，设置第一个搜索方向 $\\mathbf{p}_0 = \\mathbf{r}_0$。\n2. 对 $k=0, 1, 2, \\dots$ 进行迭代，直到收敛：\n   a. 计算矩阵-向量积 $A\\mathbf{p}_k$。\n   b. 计算步长：$\\alpha_k = \\frac{\\mathbf{r}_k^T \\mathbf{r}_k}{\\mathbf{p}_k^T A \\mathbf{p}_k}$。这会沿方向 $\\mathbf{p}_k$ 最小化 $f$。\n   c. 更新解：$\\mathbf{u}_{k+1} = \\mathbf{u}_k + \\alpha_k \\mathbf{p}_k$。\n   d. 更新残差：$\\mathbf{r}_{k+1} = \\mathbf{r}_k - \\alpha_k A \\mathbf{p}_k$。可以证明，这比重新计算 $\\mathbf{r}_{k+1} = \\mathbf{b} - A\\mathbf{u}_{k+1}$ 在数值上更稳定。\n   e. 检查收敛性，例如，如果 $||\\mathbf{r}_{k+1}||_2$ 低于某个容差。\n   f. 计算新搜索方向的系数：$\\beta_k = \\frac{\\mathbf{r}_{k+1}^T \\mathbf{r}_{k+1}}{\\mathbf{r}_k^T \\mathbf{r}_k}$。\n   g. 更新搜索方向：$\\mathbf{p}_{k+1} = \\mathbf{r}_{k+1} + \\beta_k \\mathbf{p}_k$。\n\n每次迭代需要一次矩阵-向量乘法、两次点积和三次向量更新，使其计算效率很高。\n\n### 第四部分：实现与验证\n提供的代码实现了这整个过程。\n1.  对于每个测试用例，它设置网格参数（$N_x, N_y, h_x, h_y$）并定义 $\\phi$、$\\rho$ 和边界 $g$ 的解析函数。\n2.  它通过遍历所有内部网格点并应用上面推导的离散化和边界规则，来构建 CSR 格式的系统矩阵 $A$ 和右侧向量 $\\mathbf{b}$。\n3.  它实现了共轭梯度求解器，该求解器使用一个辅助函数进行 CSR 矩阵-向量乘积运算。\n4.  求解线性系统以获得内部点的数值解向量 $\\mathbf{u}$。\n5.  在内部网格上计算数值解与真实解析解之间的均方根（RMS）误差。\n6.  收集结果并以指定格式打印。使用 $\\phi$ 的解析解可以对该方法的准确性进行严格验证。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef assemble_system(Nx, Ny, rho_func, g_func):\n    \"\"\"\n    Assembles the sparse matrix A (in CSR format) and the vector b\n    for the linear system Au = b arising from the finite difference\n    discretization of the 2D Poisson equation.\n    \"\"\"\n    N_int_x = Nx - 2\n    N_int_y = Ny - 2\n    M = N_int_x * N_int_y\n\n    if M == 0:\n        return (np.array([]), np.array([], dtype=int), np.array([0], dtype=int)), np.array([])\n    \n    hx = 1.0 / (Nx - 1)\n    hy = 1.0 / (Ny - 1)\n    hx2_inv = 1.0 / (hx * hx)\n    hy2_inv = 1.0 / (hy * hy)\n\n    # CSR arrays\n    data = []\n    indices = []\n    indptr = [0]\n\n    b = np.zeros(M)\n    \n    # Iterate over interior grid points (j_int, i_int)\n    for j_int in range(N_int_y):\n        for i_int in range(N_int_x):\n            # Global grid indices (i, j)\n            i, j = i_int + 1, j_int + 1\n            # 1D vector index k\n            k = j_int * N_int_x + i_int\n\n            x, y = i * hx, j * hy\n            b[k] = -rho_func(x, y)\n\n            # Store non-zero entries for the current row k\n            row_entries = []\n\n            # Stencil coefficients\n            # South neighbor\n            if j == 1: # Boundary\n                b[k] += hy2_inv * g_func(x, 0)\n            else: # Interior\n                row_entries.append({'col': k - N_int_x, 'val': -hy2_inv})\n            \n            # West neighbor\n            if i == 1: # Boundary\n                b[k] += hx2_inv * g_func(0, y)\n            else: # Interior\n                row_entries.append({'col': k - 1, 'val': -hx2_inv})\n            \n            # Center\n            row_entries.append({'col': k, 'val': 2 * hx2_inv + 2 * hy2_inv})\n\n            # East neighbor\n            if i == Nx - 2: # Boundary\n                b[k] += hx2_inv * g_func(1.0, y)\n            else: # Interior\n                row_entries.append({'col': k + 1, 'val': -hx2_inv})\n            \n            # North neighbor\n            if j == Ny - 2: # Boundary\n                b[k] += hy2_inv * g_func(x, 1.0)\n            else: # Interior\n                row_entries.append({'col': k + N_int_x, 'val': -hy2_inv})\n\n            # Sort entries by column index and add to CSR lists\n            row_entries.sort(key=lambda item: item['col'])\n            for entry in row_entries:\n                data.append(entry['val'])\n                indices.append(entry['col'])\n            indptr.append(len(data))\n\n    A_csr = (np.array(data), np.array(indices, dtype=int), np.array(indptr, dtype=int))\n    \n    return A_csr, b\n\ndef csr_matvec(A_csr, v):\n    \"\"\"Computes the matrix-vector product for a matrix in CSR format.\"\"\"\n    data, indices, indptr = A_csr\n    num_rows = len(indptr) - 1\n    res = np.zeros(num_rows)\n    for i in range(num_rows):\n        row_sum = 0.0\n        for j in range(indptr[i], indptr[i+1]):\n            row_sum += data[j] * v[indices[j]]\n        res[i] = row_sum\n    return res\n\ndef conjugate_gradient(A_csr, b, tol=1e-12, max_iter=None):\n    \"\"\"\n    Solves Ax = b using the Conjugate Gradient method.\n    A is in CSR format (data, indices, indptr).\n    \"\"\"\n    n = len(b)\n    if n == 0:\n        return np.array([])\n        \n    if max_iter is None:\n        max_iter = 2 * n\n\n    u = np.zeros(n)\n    r = b - csr_matvec(A_csr, u)\n    p = r.copy()\n    rs_old = np.dot(r, r)\n\n    if np.sqrt(rs_old)  tol:\n        return u\n\n    for i in range(max_iter):\n        Ap = csr_matvec(A_csr, p)\n        alpha = rs_old / np.dot(p, Ap)\n        u += alpha * p\n        r -= alpha * Ap\n        rs_new = np.dot(r, r)\n\n        if np.sqrt(rs_new)  tol:\n            break\n            \n        p = r + (rs_new / rs_old) * p\n        rs_old = rs_new\n        \n    return u\n\ndef solve():\n    \"\"\"\n    Main function to run test cases and produce the final output.\n    \"\"\"\n    test_cases = [\n        {'Nx': 32, 'Ny': 32, 'type': 'trig'},\n        {'Nx': 8, 'Ny': 12, 'type': 'trig'},\n        {'Nx': 3, 'Ny': 3, 'type': 'trig'},\n        {'Nx': 17, 'Ny': 33, 'type': 'poly'},\n    ]\n\n    # Analytical functions\n    phi_trig = lambda x, y: np.sin(np.pi * x) * np.sin(np.pi * y)\n    rho_trig = lambda x, y: -2 * np.pi**2 * np.sin(np.pi * x) * np.sin(np.pi * y)\n    \n    phi_poly = lambda x, y: x**2 + y**2 - x - y\n    rho_poly = lambda x, y: 4.0\n\n    results = []\n\n    for case in test_cases:\n        Nx, Ny = case['Nx'], case['Ny']\n        \n        if case['type'] == 'trig':\n            phi_true_func = phi_trig\n            rho_func = rho_trig\n            g_func = phi_trig\n        else: # poly\n            phi_true_func = phi_poly\n            # rho is constant, so we can pass a lambda that returns it\n            rho_func = lambda x, y: rho_poly(x,y)\n            g_func = phi_poly\n\n        # Assemble the linear system\n        A_csr, b = assemble_system(Nx, Ny, rho_func, g_func)\n\n        # Solve using Conjugate Gradient\n        u_vec = conjugate_gradient(A_csr, b)\n        \n        N_int_x = Nx - 2\n        N_int_y = Ny - 2\n        M = N_int_x * N_int_y\n\n        if M == 0:\n            results.append(0.0)\n            continue\n            \n        # Reshape solution to 2D grid for comparison\n        u_grid = u_vec.reshape((N_int_y, N_int_x))\n        \n        # Create grid for analytical solution\n        hx = 1.0 / (Nx - 1)\n        hy = 1.0 / (Ny - 1)\n        x_int = np.linspace(hx, 1.0 - hx, N_int_x)\n        y_int = np.linspace(hy, 1.0 - hy, N_int_y)\n        X_int, Y_int = np.meshgrid(x_int, y_int)\n\n        # Calculate analytical solution on the interior grid\n        phi_analytical_grid = phi_true_func(X_int, Y_int)\n\n        # Compute RMS error\n        error = np.sqrt(np.mean((u_grid - phi_analytical_grid)**2))\n        results.append(round(error, 8))\n\n    # Format and print the final output\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3227880"}]}