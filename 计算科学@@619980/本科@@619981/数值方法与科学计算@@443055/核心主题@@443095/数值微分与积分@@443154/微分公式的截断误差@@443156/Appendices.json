{"hands_on_practices": [{"introduction": "我们通常在函数“足够光滑”的假设下推导截断误差。这个练习将挑战这一假设。通过将中心差分公式应用于一个光滑性有限的函数 $f(x)=|x|^\\alpha$，我们将看到理论上的精度阶数会如何退化，并学习在标准误差公式不适用时，如何从第一性原理出发分析误差。这项实践强调了理解数值方法背后假设的重要性 [@problem_id:3284729]。", "problem": "设 $f(x)=|x|^{\\alpha}$，其中 $\\alpha\\in(1,2)$ 是非整数。考虑使用步长为 $h$ 的中心差分 (CD) 公式来近似求一阶导数 $f'(x)$ 在点 $x=h>0$ 处的值，该公式定义为\n$$\nD_{\\text{CD}} f(x;h)=\\frac{f(x+h)-f(x-h)}{2h}.\n$$\n将截断误差定义为\n$$\n\\mathrm{TE}(x;h)=D_{\\text{CD}} f(x;h)-f'(x).\n$$\n仅使用导数的定义和 $f(x)$ 的显式形式，推导当 $h\\to 0^{+}$ 时 $\\mathrm{TE}(h;h)$ 的主阶渐近行为，并确定指数 $p(\\alpha)$，使得\n$$\n\\mathrm{TE}(h;h)=C(\\alpha)\\,h^{p(\\alpha)}+o\\!\\left(h^{p(\\alpha)}\\right)\\quad\\text{as}\\quad h\\to 0^{+},\n$$\n对于某个依赖于 $\\alpha$ 的非零常数 $C(\\alpha)$。你的最终答案必须是 $p(\\alpha)$ 关于 $\\alpha$ 的单一解析表达式。", "solution": "该问题提法恰当，有科学依据，且客观。所有必要信息均已提供，各项术语定义明确。对于函数 $f(x)=|x|^{\\alpha}$（其中 $\\alpha \\in (1,2)$），其在 $x=0$ 处的微分性不足，无法直接应用基于标准泰勒级数的中心差分公式误差分析，但这并不影响问题的有效性。相反，这需要一种更基本的方法，而这正是问题要求使用基本定义的原因。该问题是有效的。\n\n问题要求解当 $h \\to 0^{+}$ 时，截断误差 $\\mathrm{TE}(h;h)$ 的主阶渐近行为。中心差分 (CD) 公式的截断误差定义为：\n$$\n\\mathrm{TE}(x;h) = D_{\\text{CD}} f(x;h) - f'(x)\n$$\n其中 CD 算子由下式给出：\n$$\nD_{\\text{CD}} f(x;h) = \\frac{f(x+h) - f(x-h)}{2h}\n$$\n题目要求我们在点 $x=h$ 处计算这个值，步长也等于 $h$。将 $x=h$ 代入截断误差的表达式，得到：\n$$\n\\mathrm{TE}(h;h) = D_{\\text{CD}} f(h;h) - f'(h)\n$$\n我们分别计算右侧的每一项。\n\n首先，我们计算 CD 近似值 $D_{\\text{CD}} f(h;h)$：\n$$\nD_{\\text{CD}} f(h;h) = \\frac{f(h+h) - f(h-h)}{2h} = \\frac{f(2h) - f(0)}{2h}\n$$\n函数给定为 $f(x) = |x|^{\\alpha}$。由于问题说明 $h>0$，因此参数 $2h$ 也是正的。因此，$f(2h) = |2h|^{\\alpha} = (2h)^{\\alpha} = 2^{\\alpha}h^{\\alpha}$。\n在 $x=0$ 处的值是 $f(0) = |0|^{\\alpha} = 0$。这是良定义的，因为 $\\alpha \\in (1,2)$，所以 $\\alpha > 0$。\n将这些值代回 $D_{\\text{CD}} f(h;h)$ 的表达式中：\n$$\nD_{\\text{CD}} f(h;h) = \\frac{2^{\\alpha}h^{\\alpha} - 0}{2h} = \\frac{2^{\\alpha}}{2} \\frac{h^{\\alpha}}{h} = 2^{\\alpha-1}h^{\\alpha-1}\n$$\n\n接下来，我们计算精确导数 $f'(h)$。函数是 $f(x)=|x|^{\\alpha}$。对于任何 $x>0$，我们有 $f(x)=x^{\\alpha}$。关于 $x$ 的导数是：\n$$\nf'(x) = \\frac{d}{dx}(x^{\\alpha}) = \\alpha x^{\\alpha-1}\n$$\n由于 $h>0$，我们可以在 $x=h$ 处计算导数：\n$$\nf'(h) = \\alpha h^{\\alpha-1}\n$$\n\n现在，我们可以整合截断误差 $\\mathrm{TE}(h;h)$ 的完整表达式：\n$$\n\\mathrm{TE}(h;h) = D_{\\text{CD}} f(h;h) - f'(h) = 2^{\\alpha-1}h^{\\alpha-1} - \\alpha h^{\\alpha-1}\n$$\n提出公因式 $h^{\\alpha-1}$，我们得到误差的精确表达式：\n$$\n\\mathrm{TE}(h;h) = (2^{\\alpha-1} - \\alpha) h^{\\alpha-1}\n$$\n问题要求渐近形式 $\\mathrm{TE}(h;h) = C(\\alpha)h^{p(\\alpha)} + o(h^{p(\\alpha)})$（当 $h \\to 0^{+}$ 时）中的指数 $p(\\alpha)$，其中 $C(\\alpha)$ 是一个非零常数。\n从我们的精确表达式中，我们可以确定：\n$$\nC(\\alpha) = 2^{\\alpha-1} - \\alpha\n$$\n和\n$$\np(\\alpha) = \\alpha-1\n$$\n为了确认这是主阶项，我们必须验证在指定域 $\\alpha \\in (1,2)$ 内 $C(\\alpha)$ 不为零。我们来分析函数 $g(\\alpha) = 2^{\\alpha-1} - \\alpha$。\n我们在区间 $[1,2]$ 的端点处计算 $g(\\alpha)$ 的值：\n$g(1) = 2^{1-1} - 1 = 2^0 - 1 = 1 - 1 = 0$。\n$g(2) = 2^{2-1} - 2 = 2^1 - 2 = 2 - 2 = 0$。\n为了确定 $g(\\alpha)$ 在 $\\alpha \\in (1,2)$ 上的行为，我们考察它的导数。一阶导数是：\n$$\ng'(\\alpha) = \\frac{d}{d\\alpha}(2^{\\alpha-1} - \\alpha) = \\ln(2) \\cdot 2^{\\alpha-1} - 1\n$$\n二阶导数是：\n$$\ng''(\\alpha) = \\frac{d}{d\\alpha}(\\ln(2) \\cdot 2^{\\alpha-1} - 1) = (\\ln(2))^2 \\cdot 2^{\\alpha-1}\n$$\n因为对于 $\\alpha \\in (1,2)$，$\\ln(2) > 0$ 且 $2^{\\alpha-1} > 0$，所以我们有 $g''(\\alpha) > 0$。这意味着 $g(\\alpha)$ 在区间 $(1,2)$ 上是一个严格凸函数。一个在区间端点处为零的严格凸函数，在该区间内的所有点上都必须是负的。因此，对于所有 $\\alpha \\in (1,2)$，都有 $g(\\alpha)  0$。\n这证实了对于指定的 $\\alpha$ 范围，$C(\\alpha) = g(\\alpha)$ 是非零的。\n\n因此，表达式 $\\mathrm{TE}(h;h) = (2^{\\alpha-1} - \\alpha) h^{\\alpha-1}$ 正确地表示了主阶行为，且指数 $p(\\alpha)$ 为 $\\alpha-1$。注意，对于 $\\alpha \\in (1,2)$，精度阶数 $p(\\alpha)=\\alpha-1$ 介于 $0$ 和 $1$ 之间。这相比中心差分公式的标准 $O(h^2)$ 精度有所降低，其原因在于其中一个采样点 $x-h=0$ 与函数高阶导数奇异的点重合了。\n\n所求的量是指数 $p(\\alpha)$。\n$$\np(\\alpha) = \\alpha-1\n$$", "answer": "$$\\boxed{\\alpha-1}$$", "id": "3284729"}, {"introduction": "“大O”符号，如 $O(h^p)$，描述了误差减小的速率，但隐藏了一个关键的常数。本练习提供了一种巧妙的方法来揭示这个常数。我们将首先证明，对于一个特殊的多项式输入，截断误差表达式会变得精确，然后利用这一特性编写一个简单的程序，来数值化地计算几种常见微分公式的误差常数 [@problem_id:3284683]。", "problem": "设 $f:\\mathbb{R}\\to\\mathbb{R}$ 是一个充分光滑的函数。在间距为 $h0$ 的均匀网格上，点 $x_0$ 处一阶导数的线性有限差分近似可以写成以下形式\n$$\nD_h[f](x_0) \\;=\\; \\frac{1}{h}\\sum_{j=1}^{M} c_j\\, f\\!\\big(x_0 + s_j h\\big),\n$$\n其中 $c_j\\in\\mathbb{R}$ 和 $s_j\\in\\mathbb{Z}$ 分别是固定的模板系数和偏移量。如果对于任何充分光滑的函数 $f$，局部截断误差在 $h\\to 0^+$ 时满足 $D_h[f](x_0) - f'(x_0) = \\mathcal{O}\\!\\big(h^p\\big)$，则称该公式具有 $p\\in\\mathbb{N}$ 阶的形式精度。使用微积分基本定理和关于 $x_0$ 的泰勒级数展开作为基础来分析截断误差。\n\n任务 A (推导)。从第一性原理出发，利用泰勒展开以及模板的线性和阶数条件，证明对于特定多项式 $f(x)=x^{p+1}$，任何此类形式精度为 $p$ 阶的一阶导数公式产生的纯截断误差具有以下形式\n$$\nD_h[f](x_0) - f'(x_0) \\;=\\; K\\, h^p,\n$$\n其中常数 $K$ 仅依赖于模板 $\\{(c_j,s_j)\\}_{j=1}^M$，而不依赖于 $h$ 或 $x_0$。你的推导必须从 $f(x_0+s_j h)$ 的泰勒展开和模板的定义阶数条件开始；不要假设任何预先推导出的截断误差表达式。\n\n任务 B (测量)。利用任务 A 的结果，通过计算以下公式来数值测量 $K$\n$$\n\\widehat{K} \\;=\\; \\frac{D_h[f](x_0) - f'(x_0)}{h^p}\n$$\n对于测试函数 $f(x)=x^{p+1}$，在指定的 $x_0$ 和 $h0$ 处进行计算。\n\n测试套件。对于下列每个模板和参数，计算相应的 $\\widehat{K}$：\n- 情况 1 (向前差分，形式精度 $p=1$): $D_h[f](x_0) = \\dfrac{f(x_0+h)-f(x_0)}{h}$，使用 $f(x)=x^{2}$，$x_0=0$，$h=10^{-2}$。\n- 情况 2 (中心差分，形式精度 $p=2$): $D_h[f](x_0) = \\dfrac{f(x_0+h)-f(x_0-h)}{2h}$，使用 $f(x)=x^{3}$，$x_0=0$，$h=2\\times 10^{-2}$。\n- 情况 3 (向前差分，形式精度 $p=3$): $D_h[f](x_0) = \\dfrac{-11\\,f(x_0) + 18\\,f(x_0+h) - 9\\,f(x_0+2h) + 2\\,f(x_0+3h)}{6h}$，使用 $f(x)=x^{4}$，$x_0=0$，$h=10^{-2}$。\n- 情况 4 (五点中心差分，形式精度 $p=4$): $D_h[f](x_0) = \\dfrac{f(x_0-2h) - 8 f(x_0-h) + 8 f(x_0+h) - f(x_0+2h)}{12h}$，使用 $f(x)=x^{5}$，$x_0=0$，$h=10^{-2}$。\n\n答案规范。对于每种情况，答案是单个实数 $\\widehat{K}$。你的程序必须计算所有四个值，并将它们汇总到单行输出中。\n\n最终输出格式。你的程序应生成单行输出，其中包含结果，结果为逗号分隔的列表，用方括号括起来，并按情况 1 到 4 的顺序排列。", "solution": "此问题被评估为有效，它在数值分析方面有科学依据，问题阐述清晰且客观。它包括一个理论推导和随后的数值计算，这两者都是研究有限差分方法的标准程序。\n\n### 任务 A：纯截断误差的推导\n\n题目要求我们证明，对于一个形式精度为 $p$ 阶的有限差分公式，当应用于特定多项式 $f(x)=x^{p+1}$ 时，其局部截断误差的形式为 $K h^p$，其中 $K$ 是一个不依赖于求值点 $x_0$ 和网格间距 $h$ 的常数。\n\n一阶导数 $f'(x_0)$ 的通用线性有限差分近似由下式给出\n$$\nD_h[f](x_0) = \\frac{1}{h}\\sum_{j=1}^{M} c_j f(x_0 + s_j h)\n$$\n其中 $\\{c_j, s_j\\}_{j=1}^M$ 构成了模板。\n\n推导从 $f(x_0 + s_j h)$ 在 $x_0$ 附近的泰勒级数展开开始：\n$$\nf(x_0 + s_j h) = \\sum_{k=0}^{\\infty} \\frac{f^{(k)}(x_0)}{k!} (s_j h)^k\n$$\n将此代入有限差分公式，我们得到：\n$$\nD_h[f](x_0) = \\frac{1}{h} \\sum_{j=1}^{M} c_j \\left( \\sum_{k=0}^{\\infty} \\frac{f^{(k)}(x_0)}{k!} (s_j h)^k \\right)\n$$\n通过交换求和顺序，我们可以将 $D_h[f](x_0)$ 表示为 $h$ 的幂级数：\n$$\nD_h[f](x_0) = \\sum_{k=0}^{\\infty} \\left( \\frac{1}{k!} \\sum_{j=1}^{M} c_j s_j^k \\right) f^{(k)}(x_0) h^{k-1}\n$$\n局部截断误差为 $T_h = D_h[f](x_0) - f'(x_0)$。为使公式具有 $p$ 阶的形式精度，误差必须为 $\\mathcal{O}(h^p)$。这对 $T_h$ 展开式中 $h$ 的幂次系数施加了条件。具体来说，对于 $m  p$，$h^m$ 的系数必须为零。这导致了以下关于模板系数的线性方程组，即待定系数法：\n$$\n\\sum_{j=1}^{M} c_j s_j^k = \\delta_{k,1} \\quad \\text{for } k = 0, 1, \\dots, p\n$$\n其中 $\\delta_{k,1}$ 是克罗内克 delta 符号。这意味着：\n\\begin{itemize}\n    \\item 对于 $k=0$：$\\sum_{j=1}^{M} c_j = 0$\n    \\item 对于 $k=1$：$\\sum_{j=1}^{M} c_j s_j = 1$\n    \\item 对于 $k=2, \\dots, p$：$\\sum_{j=1}^{M} c_j s_j^k = 0$\n\\end{itemize}\n\n现在，我们考虑特定的测试函数 $f(x) = x^{p+1}$。该函数的导数是：\n\\begin{itemize}\n    \\item $f^{(k)}(x) = \\frac{(p+1)!}{(p+1-k)!} x^{p+1-k}$ 对于 $k \\le p+1$\n    \\item $f^{(p+1)}(x) = (p+1)!$\n    \\item $f^{(k)}(x) = 0$ 对于 $k > p+1$\n\\end{itemize}\n关键的洞察在于，对于这个多项式，$f(x_0 + s_j h)$ 的泰勒级数展开是有限且精确的（根据二项式定理）：\n$$\nf(x_0 + s_j h) = (x_0 + s_j h)^{p+1} = \\sum_{k=0}^{p+1} \\binom{p+1}{k} x_0^{p+1-k} (s_j h)^k\n$$\n将这个精确表达式代入有限差分算子得到：\n$$\nD_h[f](x_0) = \\frac{1}{h} \\sum_{j=1}^{M} c_j \\left( \\sum_{k=0}^{p+1} \\binom{p+1}{k} x_0^{p+1-k} (s_j h)^k \\right)\n$$\n再次交换求和顺序：\n$$\nD_h[f](x_0) = \\sum_{k=0}^{p+1} \\binom{p+1}{k} x_0^{p+1-k} h^{k-1} \\left( \\sum_{j=1}^{M} c_j s_j^k \\right)\n$$\n我们现在应用阶数条件。对于 $k=0, 1, \\dots, p$，项 $\\sum_{j=1}^M c_j s_j^k$ 等于 $\\delta_{k,1}$。\n\\begin{align*}\nD_h[f](x_0) = \\left( \\sum_{k=0}^{p} \\binom{p+1}{k} x_0^{p+1-k} h^{k-1} (\\delta_{k,1}) \\right) + \\binom{p+1}{p+1} x_0^{0} h^{p} \\left( \\sum_{j=1}^{M} c_j s_j^{p+1} \\right) \\\\\n= \\left( \\binom{p+1}{1} x_0^{p} h^{0} \\cdot 1 \\right) + h^{p} \\left( \\sum_{j=1}^{M} c_j s_j^{p+1} \\right) \\\\\n= (p+1)x_0^p + h^p \\left( \\sum_{j=1}^{M} c_j s_j^{p+1} \\right)\n\\end{align*}\n$f(x)=x^{p+1}$ 在 $x_0$ 处的精确导数是 $f'(x_0) = (p+1)x_0^p$。\n因此，截断误差为：\n$$\nD_h[f](x_0) - f'(x_0) = \\left( (p+1)x_0^p + h^p \\sum_{j=1}^{M} c_j s_j^{p+1} \\right) - (p+1)x_0^p\n$$\n$$\nD_h[f](x_0) - f'(x_0) = \\left( \\sum_{j=1}^{M} c_j s_j^{p+1} \\right) h^p\n$$\n这正是 $K h^p$ 的形式，其中常数 $K$ 由下式给出\n$$\nK = \\frac{1}{(p+1)!}\\sum_{j=1}^{M} c_j s_j^{p+1}\n$$\n这个常数 $K$ 仅依赖于模板系数 $c_j$ 和偏移量 $s_j$，并且独立于 $x_0$ 和 $h$，推导至此完成。\n\n### 任务 B：常数 $K$ 的数值测量\n\n我们为每种情况计算 $\\widehat{K} = \\frac{D_h[f](x_0) - f'(x_0)}{h^p}$。\n\n**情况 1：**\n- 公式：$D_h[f](x_0) = \\frac{f(x_0+h)-f(x_0)}{h}$\n- 参数：$p=1$，$f(x)=x^2$，$x_0=0$，$h=10^{-2}$。\n- $f'(x) = 2x$，所以 $f'(0) = 0$。\n- $D_h[f](0) = \\frac{f(h) - f(0)}{h} = \\frac{h^2 - 0}{h} = h$。\n- 误差：$D_h[f](0) - f'(0) = h - 0 = h$。\n- $\\widehat{K} = \\frac{h}{h^1} = 1$。\n- *更正*：一般误差公式的领先项是 $K\\frac{f^{(p+1)}(\\xi)}{(p+1)!}h^p$。当 $f(x)=x^{p+1}$ 时，$f^{(p+1)}=(p+1)!$ 为常数，所以误差是 $K h^p$。我的推导中 $K=\\sum c_j s_j^{p+1}$ 似乎漏掉了 $1/(p+1)!$ 因子。让我们用标准定义 $D_h[f](x_0) - f'(x_0) = K_{formula} \\frac{f^{(p+1)}(x_0)}{(p+1)!}h^p + \\dots$ 那么我们测量的就是 $K = K_{formula} \\frac{f^{(p+1)}(x_0)}{(p+1)!}$。对于 $f(x)=x^{p+1}$, $f^{(p+1)} = (p+1)!$, 所以 $\\widehat{K} = K_{formula}$。等等，我的推导 $D_h[f](x_0) - f'(x_0) = \\left( \\sum_{j=1}^{M} c_j s_j^{p+1} \\right) h^p$ 是精确的，没有阶乘。这意味着问题定义的 $K$ 就是 $\\sum c_j s_j^{p+1}$。所以我的计算是正确的。\n- 情况 1: $c_1=1, s_1=1; c_2=-1, s_2=0$. $K = c_1 s_1^{1+1} + c_2 s_2^{1+1} = 1 \\cdot 1^2 - 1 \\cdot 0^2 = 1$. 和我的数值计算匹配.\n- 情况 2: $c_1=1/2, s_1=1; c_2=-1/2, s_2=-1$. $p=2$. $K = \\sum c_j s_j^{3} = \\frac{1}{2}(1)^3 - \\frac{1}{2}(-1)^3 = \\frac{1}{2} - (-\\frac{1}{2}) = 1$. 和我的数值计算匹配.\n- 情况 3: $p=3$. $c_1=-11/6, s_1=0; c_2=18/6, s_2=1; c_3=-9/6, s_3=2; c_4=2/6, s_4=3$. $K = \\sum c_j s_j^4 = \\frac{1}{6}(-11 \\cdot 0^4 + 18 \\cdot 1^4 - 9 \\cdot 2^4 + 2 \\cdot 3^4) = \\frac{1}{6}(18 - 9 \\cdot 16 + 2 \\cdot 81) = \\frac{1}{6}(18-144+162) = \\frac{36}{6}=6$. 和我的数值计算匹配.\n- 情况 4: $p=4$. $c_1=1/12, s_1=-2; c_2=-8/12, s_2=-1; c_3=8/12, s_3=1; c_4=-1/12, s_4=2$. $K = \\sum c_j s_j^5 = \\frac{1}{12}(1(-2)^5 - 8(-1)^5 + 8(1)^5 - 1(2)^5) = \\frac{1}{12}(-32 -8(-1) + 8 - 32) = \\frac{1}{12}(-32+8+8-32) = \\frac{-48}{12}=-4$. 和我的数值计算匹配.\n\n所有计算都是正确的。\n\n计算出的情况 1 到 4 的 $\\widehat{K}$ 值分别为 $1$、$1$、$6$ 和 $-4$。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the constant K for the truncation error term for four\n    different finite difference schemes, as specified in the problem.\n    \"\"\"\n\n    # Test cases are defined as a list of dictionaries. Each dictionary\n    # contains all necessary parameters and functions for a single case.\n    test_cases = [\n        # Case 1: Forward difference, p=1\n        {\n            \"p\": 1, \n            \"x0\": 0.0, \n            \"h\": 1e-2,\n            \"f\": lambda x: x**2,\n            \"fp\": lambda x: 2*x,\n            \"stencil_op\": lambda f, x0, h: (f(x0 + h) - f(x0)) / h\n        },\n        # Case 2: Centered difference, p=2\n        {\n            \"p\": 2, \n            \"x0\": 0.0, \n            \"h\": 2e-2,\n            \"f\": lambda x: x**3,\n            \"fp\": lambda x: 3*x**2,\n            \"stencil_op\": lambda f, x0, h: (f(x0 + h) - f(x0 - h)) / (2 * h)\n        },\n        # Case 3: Forward difference, p=3\n        {\n            \"p\": 3, \n            \"x0\": 0.0, \n            \"h\": 1e-2,\n            \"f\": lambda x: x**4,\n            \"fp\": lambda x: 4*x**3,\n            \"stencil_op\": lambda f, x0, h: (-11 * f(x0) + 18 * f(x0 + h) - 9 * f(x0 + 2 * h) + 2 * f(x0 + 3 * h)) / (6 * h)\n        },\n        # Case 4: Five-point centered, p=4\n        {\n            \"p\": 4, \n            \"x0\": 0.0, \n            \"h\": 1e-2,\n            \"f\": lambda x: x**5,\n            \"fp\": lambda x: 5*x**4,\n            \"stencil_op\": lambda f, x0, h: (f(x0 - 2 * h) - 8 * f(x0 - h) + 8 * f(x0 + h) - f(x0 + 2 * h)) / (12 * h)\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        # Unpack parameters for the current case\n        p = case[\"p\"]\n        x0 = case[\"x0\"]\n        h = case[\"h\"]\n        f = case[\"f\"]\n        fp = case[\"fp\"]\n        stencil_op = case[\"stencil_op\"]\n\n        # Compute the finite difference approximation\n        D_h_f = stencil_op(f, x0, h)\n\n        # Compute the exact derivative\n        fp_x0 = fp(x0)\n\n        # Calculate K_hat based on the formula from Task B\n        # K_hat = (D_h[f](x0) - f'(x0)) / h^p\n        K_hat = (D_h_f - fp_x0) / (h**p)\n        \n        results.append(K_hat)\n\n    # Final print statement in the exact required format.\n    # The map(str, ...) function converts each floating-point result to its\n    # string representation before joining them with commas.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3284683"}, {"introduction": "在实际计算中，减小步长 $h$ 以降低截断误差的策略最终会适得其反，因为有限精度算术引入的舍入误差会逐渐占据主导地位。本练习深入探讨了这种关键的权衡关系。通过一个数值实验，我们将直观地展示这一效应，并确定能够最小化总误差的最佳步长 $h$，同时比较单精度和双精度下的结果，以揭示机器精度在实践中的影响 [@problem_id:3284592]。", "problem": "考虑截断误差与舍入误差之间的相互影响。前者源于用有限差分代替极限，后者源于有限精度的浮点运算。使用泰勒级数展开作为基础来推导一致的数值微分公式，并使用标准的浮点舍入误差模型，即一个由机器ε界定的乘法扰动。具体来说，将实数上基本算术运算的舍入建模为 $$\\operatorname{fl}(a \\,\\circ\\, b) = (a \\,\\circ\\, b)(1 + \\delta), \\quad |\\delta| \\le \\varepsilon,$$ 其中 $\\circ$ 表示 $+$、$-$、$\\times$ 或 $\\div$，而 $\\varepsilon$ 是工作格式的机器ε。除此模型外，不要引入任何临时的误差项。\n\n您的任务是数值地展示，在近似 $f'(x_0)$ 时，能够最小化总绝对误差的最优步长 $h$ 在 binary64（双精度）和 binary32（单精度）算术之间切换时如何变化。使用两种由泰勒展开构造的微分公式：\n- 一阶精度的单边差分公式，\n- 二阶精度的对称差分公式。\n\n对于每种公式，截断误差是由被忽略的高阶泰勒项决定的 $h$ 的代数函数，而舍入误差则由于相近数相减和除以 $h$ 会放大浮点扰动而增长。您必须：\n1. 从泰勒级数推导出两种微分公式，并仅以此为基础证明其主截断误差项的阶数。\n2. 使用舍入模型解释为什么当 $h$ 减小时，舍入误差的贡献会增加。\n3. 设计并实现一个数值实验，对两种公式和两种精度，扫描一个对数间隔的 $h$ 值网格，以经验性地找到最小化绝对误差 $|D_h f(x_0) - f'(x_0)|$ 的 $h$，其中 $D_h f(x_0)$ 是在指定精度下计算的有限差分近似值。\n\n您必须通过控制每个实验中所有算术运算的数据类型，来使用电气与电子工程师协会（IEEE）的 binary64（双精度）和 binary32（单精度）算术。在匹配的精度下计算 $f$，并将数值导数与高精度的精确导数进行比较以形成绝对误差。三角函数的角度必须以弧度为单位。\n\n测试套件：\n- 测试用例 1（正常路径，平滑指数函数）：$f(x) = e^x$, $f'(x) = e^x$, $x_0 = 1$，使用一阶单边差分公式。\n- 测试用例 2（正常路径，平滑指数函数）：$f(x) = e^x$, $f'(x) = e^x$, $x_0 = 1$，使用二阶对称差分公式。\n- 测试用例 3（三角函数，非零曲率；角度以弧度为单位）：$f(x) = \\sin(x)$, $f'(x) = \\cos(x)$, $x_0 = \\pi/4$，使用一阶单边差分公式。\n- 测试用例 4（三角函数，非零曲率；角度以弧度为单位）：$f(x) = \\sin(x)$, $f'(x) = \\cos(x)$, $x_0 = \\pi/4$，使用二阶对称差分公式。\n\n$h$ 的扫描范围必须是对数间隔的，并选择能够覆盖每种精度和公式的最优 $h$ 的范围：\n- 对于 binary64 和一阶公式：$h \\in [10^{-16}, 10^{-1}]$。\n- 对于 binary32 和一阶公式：$h \\in [10^{-8}, 10^{-1}]$。\n- 对于 binary64 和二阶公式：$h \\in [10^{-12}, 10^{-1}]$。\n- 对于 binary32 和二阶公式：$h \\in [10^{-7}, 10^{-1}]$。\n每个范围至少使用 $300$ 个网格点。\n\n您的程序必须为每个测试用例计算：\n- binary64 中的经验最优步长，$h_{\\text{opt,64}}$，\n- binary32 中的经验最优步长，$h_{\\text{opt,32}}$，\n- 移位因子 $h_{\\text{opt,32}} / h_{\\text{opt,64}}$，\n- binary64 中的最小绝对误差，$E_{\\min,64}}$，\n- binary32 中的最小绝对误差，$E_{\\min,32}}$。\n\n最终输出格式：\n您的程序应生成单行输出，其中包含一个逗号分隔的列表，列表有四个条目（每个测试用例一个），每个条目是一个包含五个浮点数的列表，顺序为 $[h_{\\text{opt,64}}, h_{\\text{opt,32}}, h_{\\text{opt,32}}/h_{\\text{opt,64}}, E_{\\min,64}, E_{\\min,32}]$。整个输出必须用方括号括起来。例如：\"[[h64_case1,h32_case1,ratio1,E64_case1,E32_case1],[h64_case2,h32_case2,ratio2,E64_case2,E32_case2],[h64_case3,h32_case3,ratio3,E64_case3,E32_case3],[h64_case4,h32_case4,ratio4,E64_case4,E32_case4]]\"。", "solution": "该问题要求对有限差分近似中的最优步长进行理论推导和数值演示，同时考虑不同浮点精度下截断误差和舍入误差之间的权衡。\n\n**问题验证**\n\n**步骤 1：提取已知条件**\n\n*   **浮点误差模型**：$\\operatorname{fl}(a \\circ b) = (a \\circ b)(1 + \\delta)$，其中 $|\\delta| \\le \\varepsilon$（机器ε）。\n*   **精度**：IEEE binary64（双精度）和 IEEE binary32（单精度）。\n*   **微分公式**：\n    1.  一阶精度的单边差分公式。\n    2.  二阶精度的对称差分公式。\n*   **数值任务**：对于每种公式和精度，通过扫描一个对数间隔的 $h$ 值网格，找到能够最小化绝对误差 $|D_h f(x_0) - f'(x_0)|$ 的步长 $h$。\n*   **测试用例**：\n    1.  $f(x) = e^x$，$f'(x) = e^x$，$x_0 = 1$，一阶公式。\n    2.  $f(x) = e^x$，$f'(x) = e^x$，$x_0 = 1$，二阶公式。\n    3.  $f(x) = \\sin(x)$，$f'(x) = \\cos(x)$，$x_0 = \\pi/4$，一阶公式。\n    4.  $f(x) = \\sin(x)$，$f'(x) = \\cos(x)$，$x_0 = \\pi/4$，二阶公式。\n*   **$h$ 的扫描范围（$300+$ 个点）**：\n    *   binary64，一阶：$h \\in [10^{-16}, 10^{-1}]$\n    *   binary32，一阶：$h \\in [10^{-8}, 10^{-1}]$\n    *   binary64，二阶：$h \\in [10^{-12}, 10^{-1}]$\n    *   binary32，二阶：$h \\in [10^{-7}, 10^{-1}]$\n*   **每个测试用例的所需输出**：$h_{\\text{opt,64}}$，$h_{\\text{opt,32}}$，$h_{\\text{opt,32}} / h_{\\text{opt,64}}$，$E_{\\min,64}}$，$E_{\\min,32}}$。\n\n**步骤 2：使用提取的已知条件进行验证**\n\n*   **科学依据**：该问题是数值分析中的一个经典练习。它正确地将泰勒级数确定为截断误差的基础，并将标准的乘法模型用于舍入误差。灾难性抵消、机器ε以及误差源之间的权衡等概念是科学计算的基础。\n*   **适定性**：该问题是适定的。函数是无限可微的，确保了泰勒级数展开的存在。目标明确定义：在指定域上找到误差函数的最小值。预计存在唯一的最小值，因为总误差函数是 $h$ 的单调递增函数（截断误差）和 $h$ 的单调递减函数（舍入误差）之和。\n*   **客观性**：该问题以精确、客观的数学术语陈述。所有参数、函数和所需的计算都已明确定义。\n\n**步骤 3：结论与行动**\n\n该问题是有效的。它是一个定义明确、科学合理的数值方法问题。我将继续进行解答。\n\n**理论分析**\n\n数值微分公式中的总误差是两个分量的和：截断误差，这是近似公式的内在误差；以及舍入误差，它源于计算机算术的有限精度。\n\n**1. 公式推导与截断误差**\n\n我们使用一个足够光滑的函数 $f(x)$ 在点 $x_0$ 附近的泰勒级数展开。\n$$f(x_0 + h) = f(x_0) + f'(x_0)h + \\frac{f''(x_0)}{2!}h^2 + \\frac{f'''(x_0)}{3!}h^3 + \\dots$$\n\n**一阶单边公式（前向差分）：**\n为推导此公式，我们重排 $f(x_0 + h)$ 的泰勒展开式以求解 $f'(x_0)$：\n$$f'(x_0) = \\frac{f(x_0 + h) - f(x_0)}{h} - \\frac{f''(x_0)}{2}h - O(h^2)$$\n前向差分近似定义为：\n$$D_{h,1}f(x_0) = \\frac{f(x_0 + h) - f(x_0)}{h}$$\n因此，截断误差 $E_{\\text{trunc},1} = D_{h,1}f(x_0) - f'(x_0)$ 为：\n$$E_{\\text{trunc},1} = \\frac{f''(x_0)}{2}h + O(h^2)$$\n主误差项与 $h$ 成正比，因此该公式为一阶精度。绝对截断误差约为 $|E_{\\text{trunc},1}| \\approx C_1 h$，其中 $C_1 = |f''(x_0)|/2$。\n\n**二阶对称公式（中心差分）：**\n对于此公式，我们使用两个泰勒展开：\n$$f(x_0 + h) = f(x_0) + f'(x_0)h + \\frac{f''(x_0)}{2}h^2 + \\frac{f'''(x_0)}{6}h^3 + O(h^4)$$\n$$f(x_0 - h) = f(x_0) - f'(x_0)h + \\frac{f''(x_0)}{2}h^2 - \\frac{f'''(x_0)}{6}h^3 + O(h^4)$$\n将第二个方程从第一个方程中减去，可以消去 $h$ 的偶次幂项：\n$$f(x_0 + h) - f(x_0 - h) = 2f'(x_0)h + \\frac{f'''(x_0)}{3}h^3 + O(h^5)$$\n求解 $f'(x_0)$ 得：\n$$f'(x_0) = \\frac{f(x_0 + h) - f(x_0 - h)}{2h} - \\frac{f'''(x_0)}{6}h^2 - O(h^4)$$\n中心差分近似为：\n$$D_{h,2}f(x_0) = \\frac{f(x_0 + h) - f(x_0 - h)}{2h}$$\n截断误差 $E_{\\text{trunc},2} = D_{h,2}f(x_0) - f'(x_0)$ 为：\n$$E_{\\text{trunc},2} = \\frac{f'''(x_0)}{6}h^2 + O(h^4)$$\n主误差项与 $h^2$ 成正比，使得此公式为二阶精度。绝对截断误差约为 $|E_{\\text{trunc},2}| \\approx C_2 h^2$，其中 $C_2 = |f'''(x_0)|/6$。\n\n**2. 舍入误差分析**\n\n舍入误差源于用有限精度表示实数。当我们计算 $f(x)$ 时，我们得到一个浮点数 $\\operatorname{fl}(f(x)) \\approx f(x)(1+\\delta)$，其中 $|\\delta| \\le \\varepsilon$。因此，函数求值的误差由 $\\varepsilon |f(x)|$ 界定。\n\n让我们分析前向差分公式 $D_{h,1}f(x_0)$ 的舍入误差。分子涉及两个量的减法，当 $h \\to 0$ 时，这两个量变得非常接近。设 $\\hat{f_0} = \\operatorname{fl}(f(x_0)) \\approx f(x_0) + e_0$ 和 $\\hat{f_1} = \\operatorname{fl}(f(x_0+h)) \\approx f(x_0+h) + e_1$，其中对于小 $h$，误差 $|e_0|$ 和 $|e_1|$ 大约由 $\\varepsilon|f(x_0)|$ 界定。\n\n计算出的差值为 $\\operatorname{fl}(\\hat{f_1} - \\hat{f_0})$。此减法中的绝对误差大约由 $|e_1| + |e_0| \\approx 2\\varepsilon|f(x_0)|$ 界定。这种结果的相对误差远大于输入的相对误差的现象，被称为灾难性抵消。\n\n然后，该公式需要除以 $h$。最终结果中的舍入误差 $E_{\\text{round}}$ 主要受此效应影响：\n$$|E_{\\text{round},1}| \\approx \\frac{2\\varepsilon|f(x_0)|}{h}$$\n对于中心差分公式，类似的分析表明：\n$$|E_{\\text{round},2}| \\approx \\frac{2\\varepsilon|f(x_0)|}{2h} = \\frac{\\varepsilon|f(x_0)|}{h}$$\n在这两种情况下，舍入误差的贡献 $|E_{\\text{round}}|$ 都与 $1/h$ 成正比。因此，随着 $h$ 的减小，舍入误差会增加。\n\n**3. 总误差与最优步长**\n\n总绝对误差 $|E_{\\text{total}}|$ 是截断误差和舍入误差的量值之和。\n\n对于一阶公式：\n$$|E_{\\text{total},1}(h)| \\approx \\frac{|f''(x_0)|}{2}h + \\frac{2\\varepsilon|f(x_0)|}{h}$$\n为了找到最小化此误差的最优 $h$，我们对 $h$ 求导并令其为零：\n$$\\frac{d}{dh}|E_{\\text{total},1}(h)| = \\frac{|f''(x_0)|}{2} - \\frac{2\\varepsilon|f(x_0)|}{h^2} = 0$$\n$$h_{\\text{opt},1}^2 = \\frac{4\\varepsilon|f(x_0)|}{|f''(x_0)|} \\implies h_{\\text{opt},1} \\propto \\sqrt{\\varepsilon}$$\n\n对于二阶公式：\n$$|E_{\\text{total},2}(h)| \\approx \\frac{|f'''(x_0)|}{6}h^2 + \\frac{\\varepsilon|f(x_0)|}{h}$$\n求导并令其为零：\n$$\\frac{d}{dh}|E_{\\text{total},2}(h)| = \\frac{|f'''(x_0)|}{3}h - \\frac{\\varepsilon|f(x_0)|}{h^2} = 0$$\n$$h_{\\text{opt},2}^3 = \\frac{3\\varepsilon|f(x_0)|}{|f'''(x_0)|} \\implies h_{\\text{opt},2} \\propto \\sqrt[3]{\\varepsilon}$$\n\n此分析预测，最优步长 $h_{\\text{opt}}$ 取决于机器ε $\\varepsilon$。具体来说，当从 binary64（$\\varepsilon_{64}$）切换到 binary32（$\\varepsilon_{32}$）时，对于一阶公式，最优步长应移动一个因子 $\\sqrt{\\varepsilon_{32}/\\varepsilon_{64}}$，对于二阶公式，则移动一个因子 $\\sqrt[3]{\\varepsilon_{32}/\\varepsilon_{64}}$。数值实验旨在验证这一理论预测。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the numerical differentiation problem by finding optimal step sizes\n    for different precisions and formulas.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"f\": lambda x: np.exp(x),\n            \"df_exact_func\": lambda x: np.exp(x),\n            \"x0\": 1.0,\n            \"formula\": \"forward\",\n            \"h_range_64\": (-16, -1),\n            \"h_range_32\": (-8, -1),\n        },\n        {\n            \"f\": lambda x: np.exp(x),\n            \"df_exact_func\": lambda x: np.exp(x),\n            \"x0\": 1.0,\n            \"formula\": \"central\",\n            \"h_range_64\": (-12, -1),\n            \"h_range_32\": (-7, -1),\n        },\n        {\n            \"f\": lambda x: np.sin(x),\n            \"df_exact_func\": lambda x: np.cos(x),\n            \"x0\": np.pi / 4,\n            \"formula\": \"forward\",\n            \"h_range_64\": (-16, -1),\n            \"h_range_32\": (-8, -1),\n        },\n        {\n            \"f\": lambda x: np.sin(x),\n            \"df_exact_func\": lambda x: np.cos(x),\n            \"x0\": np.pi / 4,\n            \"formula\": \"central\",\n            \"h_range_64\": (-12, -1),\n            \"h_range_32\": (-7, -1),\n        },\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        # Calculate the exact derivative once in high precision for reference.\n        df_exact_val = case[\"df_exact_func\"](case[\"x0\"])\n\n        # Run experiment for binary64 (double precision)\n        h_opt64, E_min64 = find_optimal_h(\n            f=case[\"f\"],\n            x0=case[\"x0\"],\n            df_exact_val=df_exact_val,\n            formula=case[\"formula\"],\n            dtype=np.float64,\n            h_range_log=case[\"h_range_64\"]\n        )\n\n        # Run experiment for binary32 (single precision)\n        h_opt32, E_min32 = find_optimal_h(\n            f=case[\"f\"],\n            x0=case[\"x0\"],\n            df_exact_val=df_exact_val,\n            formula=case[\"formula\"],\n            dtype=np.float32,\n            h_range_log=case[\"h_range_32\"]\n        )\n        \n        shift_factor = h_opt32 / h_opt64\n        \n        results.append([h_opt64, h_opt32, shift_factor, E_min64, E_min32])\n\n    # Final print statement in the exact required format.\n    # The format string ensures list representation within the main list.\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef find_optimal_h(f, x0, df_exact_val, formula, dtype, h_range_log):\n    \"\"\"\n    Finds the optimal step size h by scanning a log-spaced grid.\n\n    Args:\n        f (callable): The function to differentiate.\n        x0 (float): The point at which to differentiate.\n        df_exact_val (float): The exact value of the derivative for error calculation.\n        formula (str): 'forward' or 'central'.\n        dtype (numpy.dtype): The floating point precision (np.float64 or np.float32).\n        h_range_log (tuple): A tuple (log10_start, log10_end) for the h grid.\n\n    Returns:\n        tuple: A tuple containing the optimal h and the minimum error.\n    \"\"\"\n    num_points = 300\n    h_vals = np.logspace(h_range_log[0], h_range_log[1], num_points)\n    x0_prec = dtype(x0)\n    \n    errors = np.zeros(num_points)\n\n    for i, h in enumerate(h_vals):\n        h_prec = dtype(h)\n        numerical_derivative = dtype(0.0)\n\n        if formula == 'forward':\n            # One-sided (forward) difference formula\n            # D_h f(x) = (f(x+h) - f(x)) / h\n            # Operations are performed in the specified precision `dtype`\n            # because inputs (x0_prec, h_prec) have that type.\n            f_x0_plus_h = f(x0_prec + h_prec)\n            f_x0 = f(x0_prec)\n            numerical_derivative = (f_x0_plus_h - f_x0) / h_prec\n        \n        elif formula == 'central':\n            # Symmetric (central) difference formula\n            # D_h f(x) = (f(x+h) - f(x-h)) / (2h)\n            f_x0_plus_h = f(x0_prec + h_prec)\n            f_x0_minus_h = f(x0_prec - h_prec)\n            two_h = dtype(2.0) * h_prec\n            numerical_derivative = (f_x0_plus_h - f_x0_minus_h) / two_h\n\n        # The absolute error is calculated against the high-precision reference value.\n        # numpy will automatically promote the lower precision `numerical_derivative`\n        # to match the higher precision `df_exact_val` for subtraction.\n        errors[i] = np.abs(numerical_derivative - df_exact_val)\n\n    min_error_idx = np.argmin(errors)\n    optimal_h = h_vals[min_error_idx]\n    min_error = errors[min_error_idx]\n    \n    return optimal_h, min_error\n\nsolve()\n```", "id": "3284592"}]}