{"hands_on_practices": [{"introduction": "理论知识通过实践得以升华。在数值计算中，理解算法的准确性、稳定性和效率之间的权衡至关重要。我们将从一个基本任务开始：数值微分。简单地减小步长 $h$ 并非提高精度的唯一途径。这个练习将向您展示如何通过更高阶的算法，即理查森外推法（Richardson extrapolation），系统性地提升计算结果的准确性。通过比较一阶方法和二阶方法的收敛速度，您将亲身体会到算法“阶数”在追求高精度解时的强大威力 [@problem_id:3204708]。", "problem": "考虑对一个光滑函数 $f$ 在点 $x_0$ 处的导数进行数值近似的任务。我们将比较两种算法的精度和效率：简单的前向有限差分法和由前向有限差分构造的 Richardson 外推法。比较将基于绝对误差对步长 $h$ 的经验依赖关系，该关系对于足够小的 $h$ 表示为幂律 $E(h) \\approx C h^p$，其中 $E(h)$ 是绝对误差，$C$ 是一个取决于 $f$ 和 $x_0$ 的常数，$p$ 是收敛阶。此研究将基于导数的定义和光滑函数的泰勒级数展开。\n\n从导数的基本定义 $f'(x_0) = \\lim_{h \\to 0} \\frac{f(x_0+h) - f(x_0)}{h}$ 和 $f$ 在 $x_0$ 附近的泰勒级数展开出发，推导以下方法的截断误差对 $h$ 的主阶依赖关系：\n- 前向有限差分 $D_{\\mathrm{fd}}(x_0,h) = \\frac{f(x_0+h) - f(x_0)}{h}$。\n- 由前向差分构造的 Richardson 外推法 $D_{\\mathrm{rich}}(x_0,h) = 2 D_{\\mathrm{fd}}(x_0,\\frac{h}{2}) - D_{\\mathrm{fd}}(x_0,h)$。\n\n然后，实现一个程序，对于一组给定的、具有已知精确导数的光滑测试函数，计算每种方法的经验收敛阶 $p$ 和一个局部误差比 $r$，定义为 $r(h) = \\frac{E(h)}{E(h/2)}$。在截断误差主导舍入误差的预定 $h$ 值范围内，使用对数据 $(\\log h, \\log E(h))$ 的线性最小二乘回归来估计 $p$。\n\n假设角度以弧度为单位。不涉及物理单位。所有数学量必须以无单位方式处理。\n\n测试套件包含以下函数-点对和步长方案：\n- $f_1(x) = \\sin(x)$ 于 $x_0 = 1.3$，步长 $h_k = 2^{-k}$，其中 $k \\in \\{3,4,5,\\dots,20\\}$。\n- $f_2(x) = e^{x}$ 于 $x_0 = 1.0$，步长 $h_k = 2^{-k}$，其中 $k \\in \\{3,4,5,\\dots,20\\}$。\n- $f_3(x) = \\frac{1}{1+x^2}$ 于 $x_0 = 0.5$，步长 $h_k = 2^{-k}$，其中 $k \\in \\{3,4,5,\\dots,20\\}$。\n\n对于每个测试用例：\n- 计算绝对误差 $E_{\\mathrm{fd}}(h_k) = \\left|D_{\\mathrm{fd}}(x_0,h_k) - f'(x_0)\\right|$ 和 $E_{\\mathrm{rich}}(h_k) = \\left|D_{\\mathrm{rich}}(x_0,h_k) - f'(x_0)\\right|$。\n- 使用子集 $k \\in \\{3,4,5,6,7,8,9,10\\}$，通过对 $\\log E$ 与 $\\log h$ 进行线性最小二乘回归，估计经验收敛阶 $p_{\\mathrm{fd}}$ 和 $p_{\\mathrm{rich}}$。\n- 计算局部误差比 $r_{\\mathrm{fd}} = \\frac{E_{\\mathrm{fd}}(h_8)}{E_{\\mathrm{fd}}(h_9)}$ 和 $r_{\\mathrm{rich}} = \\frac{E_{\\mathrm{rich}}(h_8)}{E_{\\mathrm{rich}}(h_9)}$，其中 $h_8 = 2^{-8}$，$h_9 = 2^{-9}$。\n\n你的程序应该生成一行输出，包含一个用方括号括起来的逗号分隔列表，且没有空格。每个测试用例的结果必须格式化为一个列表 $[p_{\\mathrm{fd}},p_{\\mathrm{rich}},r_{\\mathrm{fd}},r_{\\mathrm{rich}}]$，并按上面列出的测试用例的顺序排列。因此，最终输出的形式为 $[[p_{\\mathrm{fd},1},p_{\\mathrm{rich},1},r_{\\mathrm{fd},1},r_{\\mathrm{rich},1}],[p_{\\mathrm{fd},2},p_{\\mathrm{rich},2},r_{\\mathrm{fd},2},r_{\\mathrm{rich},2}],[p_{\\mathrm{fd},3},p_{\\mathrm{rich},3},r_{\\mathrm{fd},3},r_{\\mathrm{rich},3}]]$，其中所有条目都是十进制浮点数。", "solution": "用户提供的问题是一个定义明确的数值分析练习，涉及比较两种数值微分算法：前向有限差分法和 Richardson 外推方案。该问题在科学上是合理的、自洽的，并且所有数据和步骤都得到了明确的规定。因此，该问题是有效的，我将继续提供完整的解决方案。\n\n解决方案包括两个部分：首先，对两种方法的截断误差进行理论推导；其次，解释为验证这些理论发现而设计的数值实现。\n\n### 截断误差的理论分析\n\n分析的核心在于光滑函数 $f(x)$ 在点 $x_0$ 附近的泰勒级数展开。对于一个小的位移 $h$，级数为：\n$$\nf(x_0+h) = f(x_0) + hf'(x_0) + \\frac{h^2}{2!}f''(x_0) + \\frac{h^3}{3!}f'''(x_0) + \\dots + \\frac{h^n}{n!}f^{(n)}(x_0) + \\mathcal{O}(h^{n+1})\n$$\n其中 $f^{(n)}(x_0)$ 是 $f$ 在 $x_0$ 处计算的 $n$ 阶导数，$\\mathcal{O}(h^{n+1})$ 代表 $h^{n+1}$ 阶及更高阶的项。\n\n#### 1. 前向有限差分 ($D_{\\mathrm{fd}}$)\n\n前向有限差分公式定义为：\n$$\nD_{\\mathrm{fd}}(x_0,h) = \\frac{f(x_0+h) - f(x_0)}{h}\n$$\n为了分析其精度，我们代入 $f(x_0+h)$ 的泰勒级数：\n$$\nD_{\\mathrm{fd}}(x_0,h) = \\frac{1}{h} \\left[ \\left( f(x_0) + hf'(x_0) + \\frac{h^2}{2}f''(x_0) + \\mathcal{O}(h^3) \\right) - f(x_0) \\right]\n$$\n通过消去 $f(x_0)$ 并除以 $h$ 来简化表达式：\n$$\nD_{\\mathrm{fd}}(x_0,h) = \\frac{1}{h} \\left[ hf'(x_0) + \\frac{h^2}{2}f''(x_0) + \\mathcal{O}(h^3) \\right] = f'(x_0) + \\frac{h}{2}f''(x_0) + \\mathcal{O}(h^2)\n$$\n因此，截断误差 $E_{\\mathrm{trunc}}(h) = D_{\\mathrm{fd}}(x_0,h) - f'(x_0)$ 为：\n$$\nE_{\\mathrm{trunc}}(h) = \\frac{h}{2}f''(x_0) + \\mathcal{O}(h^2)\n$$\n误差的主阶项与 $h^1$ 成正比。因此，前向有限差分法是一阶精确的，其收敛阶为 $p=1$。对于小 $h$，绝对误差 $E(h)$ 近似为 $E(h) \\approx |\\frac{f''(x_0)}{2}| h$。\n\n#### 2. Richardson 外推法 ($D_{\\mathrm{rich}}$)\nRichardson 外推法是一种提高数值方法精度阶数的技术。所提供的特定公式旨在消除前向有限差分法的主阶误差项。\n$$\nD_{\\mathrm{rich}}(x_0,h) = 2 D_{\\mathrm{fd}}(x_0,\\frac{h}{2}) - D_{\\mathrm{fd}}(x_0,h)\n$$\n为了找到其截断误差，我们需要 $D_{\\mathrm{fd}}$ 的更高阶误差展开。设 $A(h)$ 为近似值 $D_{\\mathrm{fd}}(x_0, h)$。我们已经确定 $A(h)$ 的误差展开式具有以下形式：\n$$\nA(h) = f'(x_0) + C_1 h + C_2 h^2 + C_3 h^3 + \\dots\n$$\n其中 $C_k = \\frac{f^{(k+1)}(x_0)}{(k+1)!}$。具体来说，$C_1 = \\frac{f''(x_0)}{2}$ 和 $C_2 = \\frac{f'''(x_0)}{6}$。\n\n步长为 $h/2$ 的近似值为：\n$$\nA(\\frac{h}{2}) = f'(x_0) + C_1 \\left(\\frac{h}{2}\\right) + C_2 \\left(\\frac{h}{2}\\right)^2 + \\mathcal{O}(h^3) = f'(x_0) + \\frac{C_1}{2}h + \\frac{C_2}{4}h^2 + \\mathcal{O}(h^3)\n$$\n现在，将这些代入 $D_{\\mathrm{rich}}(x_0,h)$ 的公式中：\n$$\nD_{\\mathrm{rich}}(x_0,h) = 2 \\left( f'(x_0) + \\frac{C_1}{2}h + \\frac{C_2}{4}h^2 + \\mathcal{O}(h^3) \\right) - \\left( f'(x_0) + C_1 h + C_2 h^2 + \\mathcal{O}(h^3) \\right)\n$$\n展开各项可得：\n$$\nD_{\\mathrm{rich}}(x_0,h) = (2f'(x_0) - f'(x_0)) + (C_1 - C_1)h + \\left(\\frac{C_2}{2} - C_2\\right)h^2 + \\mathcal{O}(h^3)\n$$\n$$\nD_{\\mathrm{rich}}(x_0,h) = f'(x_0) - \\frac{C_2}{2}h^2 + \\mathcal{O}(h^3)\n$$\n代回 $C_2$ 的表达式：\n$$\nD_{\\mathrm{rich}}(x_0,h) = f'(x_0) - \\frac{f'''(x_0)}{12}h^2 + \\mathcal{O}(h^3)\n$$\nRichardson 外推法的截断误差为：\n$$\nE_{\\mathrm{trunc}}(h) = D_{\\mathrm{rich}}(x_0,h) - f'(x_0) = - \\frac{f'''(x_0)}{12}h^2 + \\mathcal{O}(h^3)\n$$\n主阶误差项与 $h^2$ 成正比。因此，该方法是二阶精确的，其收敛阶为 $p=2$。对于小 $h$，绝对误差 $E(h)$ 近似为 $E(h) \\approx |\\frac{f'''(x_0)}{12}| h^2$。\n\n### 局部误差比与数值验证\n\n理论关系 $E(h) \\approx C h^p$ 提供了一种通过经验验证收敛阶的方法。局部误差比 $r(h)$ 定义为：\n$$\nr(h) = \\frac{E(h)}{E(h/2)} \\approx \\frac{C h^p}{C (h/2)^p} = \\frac{h^p}{h^p / 2^p} = 2^p\n$$\n根据我们的推导，我们预期：\n- 对于前向差分法 ($p=1$)：$r(h) \\approx 2^1 = 2$。\n- 对于 Richardson 外推法 ($p=2$)：$r(h) \\approx 2^2 = 4$。\n\n经验收敛阶 $p$ 是从方程 $\\log E(h) \\approx \\log C + p \\log h$ 估计的。这表明 $\\log E$ 和 $\\log h$ 之间存在线性关系，其中斜率是收敛阶 $p$。对一组 $(\\log h, \\log E)$ 数据点进行线性最小二乘回归，可以得到该斜率的估计值。\n\n该实现将计算每个函数在指定步长范围 $h_k = 2^{-k}$ 内的绝对误差 $E_{\\mathrm{fd}}(h_k)$ 和 $E_{\\mathrm{rich}}(h_k)$。然后，它将对指定的数据子集（$k \\in \\{3, \\dots, 10\\}$）执行回归以找到 $p_{\\mathrm{fd}}$ 和 $p_{\\mathrm{rich}}$，并计算在 $h=h_8=2^{-8}$ 时的局部比率 $r_{\\mathrm{fd}}$ 和 $r_{\\mathrm{rich}}$。结果将被整理并以指定格式打印。数值结果预计将与前向差分法的理论预测 $p=1, r \\approx 2$ 以及 Richardson 外推法的 $p=2, r \\approx 4$ 非常吻合。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes theoretical and empirical convergence properties for numerical differentiation.\n    \"\"\"\n\n    # Define the test functions and their analytical derivatives\n    def f1(x):\n        return np.sin(x)\n\n    def df1(x):\n        return np.cos(x)\n\n    def f2(x):\n        return np.exp(x)\n\n    def df2(x):\n        return np.exp(x)\n\n    def f3(x):\n        return 1.0 / (1.0 + x**2)\n\n    def df3(x):\n        return -2.0 * x / (1.0 + x**2)**2\n\n    # Define the numerical differentiation algorithms\n    def forward_diff(f, x0, h):\n        \"\"\"Computes the forward finite difference approximation of the derivative.\"\"\"\n        return (f(x0 + h) - f(x0)) / h\n\n    def richardson_extrap(f, x0, h):\n        \"\"\"Computes the Richardson extrapolation from forward differences.\"\"\"\n        # This formula is equivalent to the central difference approximation\n        # (f(x0+h/2) - f(x0-h/2))/h but derived from Richardson's method\n        # D_rich = 2 * D_fd(h/2) - D_fd(h)\n        d_h = forward_diff(f, x0, h)\n        d_h_half = forward_diff(f, x0, h / 2.0)\n        return 2.0 * d_h_half - d_h\n\n    # Define the test cases as specified in the problem statement\n    test_cases = [\n        {'f': f1, 'df': df1, 'x0': 1.3},\n        {'f': f2, 'df': df2, 'x0': 1.0},\n        {'f': f3, 'df': df3, 'x0': 0.5}\n    ]\n\n    all_results = []\n\n    # Define the range of step sizes\n    k_values = np.arange(3, 21)\n    h_values = 2.0**(-k_values)\n\n    # Define the slice of data to be used for regression (k=3 to 10)\n    k_min_reg, k_max_reg = 3, 10\n    reg_start_index = k_min_reg - k_values[0]\n    reg_end_index = k_max_reg - k_values[0] + 1\n    reg_slice = slice(reg_start_index, reg_end_index)\n\n    # Define indices for calculating the local error ratio (k=8, k=9)\n    k8_index = 8 - k_values[0]\n    k9_index = 9 - k_values[0]\n\n\n    for case in test_cases:\n        f = case['f']\n        df = case['df']\n        x0 = case['x0']\n\n        exact_derivative = df(x0)\n\n        # Compute numerical approximations for all h values\n        d_fd = np.array([forward_diff(f, x0, h) for h in h_values])\n        d_rich = np.array([richardson_extrap(f, x0, h) for h in h_values])\n        \n        # Compute absolute errors\n        e_fd = np.abs(d_fd - exact_derivative)\n        e_rich = np.abs(d_rich - exact_derivative)\n\n        # To prevent log(0) errors, replace any zero errors with a very small number.\n        # This is a safeguard; for the given functions, non-zero error is expected.\n        e_fd[e_fd == 0] = np.finfo(float).tiny\n        e_rich[e_rich == 0] = np.finfo(float).tiny\n        \n        # --- Empirical Convergence Order (p) ---\n        # Select the data for the regression analysis\n        h_reg = h_values[reg_slice]\n        e_fd_reg = e_fd[reg_slice]\n        e_rich_reg = e_rich[reg_slice]\n\n        # Prepare data for log-log regression\n        log_h = np.log(h_reg)\n        log_e_fd = np.log(e_fd_reg)\n        log_e_rich = np.log(e_rich_reg)\n\n        # Perform linear least squares regression (polyfit of degree 1)\n        # The slope of the line in the log-log plot is the order of convergence p.\n        p_fd = np.polyfit(log_h, log_e_fd, 1)[0]\n        p_rich = np.polyfit(log_h, log_e_rich, 1)[0]\n\n        # --- Local Error Ratio (r) ---\n        # Compute the ratio of errors for h_8 and h_9\n        r_fd = e_fd[k8_index] / e_fd[k9_index]\n        r_rich = e_rich[k8_index] / e_rich[k9_index]\n\n        all_results.append([p_fd, p_rich, r_fd, r_rich])\n\n    # Format the final output string as a list of lists of floats\n    inner_lists_str = []\n    for sublist in all_results:\n        inner_lists_str.append(f\"[{','.join(map(str, sublist))}]\")\n    \n    final_output_str = f\"[{','.join(inner_lists_str)}]\"\n    \n    print(final_output_str)\n\nsolve()\n```", "id": "3204708"}, {"introduction": "在掌握了如何提升准确性的方法后，我们来探讨另一个关键维度：稳定性。在模拟动态系统（如求解常微分方程）时，为了提高计算效率，我们很自然地会想采用更大的时间步长。然而，这种做法可能暗藏风险。本练习将使用经典的 Lotka-Volterra 捕食者-被食者模型，来揭示前向欧拉法在步长过大时可能出现的数值不稳定性。您将通过编程实践发现，不恰当的步长选择不仅会降低精度，甚至会导致解出现违背物理现实的负值，这是关于算法稳定性一个生动而深刻的教训 [@problem_id:3204844]。", "problem": "您的任务是研究前向欧拉法在应用于经典的 Lotka–Volterra 捕食者-猎物模型时的数值精度和效率。该连续模型由两个耦合的常微分方程 (ODEs) 组成，它们表示猎物种群 $x(t)$ 和捕食者种群 $y(t)$ 随时间的演化：\n$$\n\\frac{dx}{dt} = \\alpha x - \\beta x y,\\quad\n\\frac{dy}{dt} = \\delta x y - \\gamma y,\n$$\n其中参数 $\\alpha > 0$，$\\beta > 0$，$\\gamma > 0$ 和 $\\delta > 0$，初始条件为 $x(0) = x_0 > 0$，$y(0) = y_0 > 0$。在这些条件下，精确的连续时间动态过程能保持种群的非负性。前向欧拉法是一种一阶显式时间步进格式，它通过以大小为 $h > 0$ 的步长从 $t_n = n h$ 推进到 $t_{n+1} = (n+1) h$ 来近似求解。\n\n您的任务是：\n- 从导数的极限定义和均匀步长 $h$ 的时间离散化概念出发，推导上述 ODE 系统的前向欧拉更新方程。解释为什么尽管真实解保持非负，但较大的步长 $h$ 会导致数值解中出现非物理的负值。\n- 实现一个程序，在给定 $(\\alpha,\\beta,\\gamma,\\delta)$、$(x_0,y_0)$、步长 $h$ 和最终时间 $T$ 的情况下，应用前向欧拉法进行 $N = T/h$ 步计算（假设对于所有测试用例，$T/h$ 均为整数）。在每次模拟中，检测任一种群是否在任何步骤变为负值，跟踪 $x_n$ 和 $y_n$ 达到的最小值，并返回最终值 $x_N$ 和 $y_N$ 以及作为计算量代理的步数 $N$。\n- 讨论 $h$ 的选择如何影响精度（通过截断误差）和效率（通过步数），并将其与观察到的非物理负种群值的出现与否联系起来。\n\n使用以下测试套件，每个套件由 $(\\alpha,\\beta,\\gamma,\\delta,x_0,y_0,h,T)$ 指定，所有量均为无量纲：\n- 测试 $1$：$(\\alpha,\\beta,\\gamma,\\delta) = (\\,1.1,\\,0.4,\\,0.4,\\,0.1\\,)$, $(x_0,y_0) = (\\,10.0,\\,5.0\\,)$, $h = 0.01$, $T = 10.0$。\n- 测试 $2$：$(\\alpha,\\beta,\\gamma,\\delta) = (\\,1.1,\\,0.4,\\,0.4,\\,0.1\\,)$, $(x_0,y_0) = (\\,10.0,\\,5.0\\,)$, $h = 0.1$, $T = 10.0$。\n- 测试 $3$：$(\\alpha,\\beta,\\gamma,\\delta) = (\\,1.1,\\,0.4,\\,0.4,\\,0.1\\,)$, $(x_0,y_0) = (\\,10.0,\\,5.0\\,)$, $h = 1.2$, $T = 6.0$。\n- 测试 $4$：$(\\alpha,\\beta,\\gamma,\\delta) = (\\,1.0,\\,0.5,\\,1.0,\\,0.05\\,)$, $(x_0,y_0) = (\\,5.0,\\,20.0\\,)$, $h = 0.12$, $T = 1.2$。\n- 边界测试 $5$：$(\\alpha,\\beta,\\gamma,\\delta) = (\\,1.0,\\,1.0,\\,1.0,\\,0.5\\,)$, $(x_0,y_0) = (\\,1.0,\\,2.0\\,)$, $h = 1.0$, $T = 3.0$。\n\n程序输出格式：\n- 对于每个测试，生成一个列表 $[\\mathrm{neg}, \\min x, \\min y, x_{\\mathrm{final}}, y_{\\mathrm{final}}, N]$，其中 $\\mathrm{neg}$ 是一个布尔值，指示在任何步骤（包括最后一步）是否出现了非物理的负种群值，$\\min x$ 和 $\\min y$ 是在整个轨迹（包括初始值和最终值）中观察到的 $x_n$ 和 $y_n$ 的最小值，$x_{\\mathrm{final}}$ 和 $y_{\\mathrm{final}}$ 是 $N$ 步后的最终值，而 $N$ 是所用的总步数。\n- 将所有测试的结果聚合到一行中，作为一个由方括号括起来的逗号分隔列表。也就是说，您的程序应该打印一行形式为 $[$result\\_1,result\\_2,result\\_3,result\\_4,result\\_5$]$ 的内容，其中每个 result 是上面描述的列表，布尔值、整数和浮点数均使用标准十进制表示法。", "solution": "所述问题在科学上是合理的，提法明确且客观。它基于数理生物学（Lotka-Volterra 模型）和数值分析（前向欧拉法）的基本原理，这些都是完善的研究领域。问题提供了所有必要的数据，并定义了一个清晰、可验证的任务。这是一个用于研究数值方法性质的典型例子，特别是精度、效率和稳定性之间的权衡。因此，该问题是有效的，我将提供完整的解答。\n\nLotka-Volterra 捕食者-猎物模型是一个由两个耦合的一阶非线性常微分方程 (ODEs) 组成的系统。设 $x(t)$ 为时间 $t$ 的猎物种群数量，$y(t)$ 为捕食者种群数量。该系统由以下方程给出：\n$$\n\\frac{dx}{dt} = \\alpha x - \\beta x y = f(x, y)\n$$\n$$\n\\frac{dy}{dt} = \\delta x y - \\gamma y = g(x, y)\n$$\n参数 $\\alpha, \\beta, \\gamma, \\delta$ 是代表相互作用率的正实数常数。$\\alpha x$ 项模拟了在没有捕食者的情况下猎物的指数增长。$-\\beta x y$ 项表示猎物被捕食者消耗的速率。$\\delta x y$ 项表示由于消耗猎物而导致的捕食者种群的增长。$-\\gamma y$ 项表示捕食者的自然死亡。初始条件为 $x(0) = x_0 > 0$ 和 $y(0) = y_0 > 0$。精确解的一个重要性质是，如果种群初始为正，那么它们在所有时间 $t > 0$ 内都将保持为正。\n\n### 前向欧拉法的推导\n\n前向欧拉法是一种用于近似求解初值问题的数值方法。它由导数的定义推导而来。对于函数 $x(t)$，其导数定义为：\n$$\n\\frac{dx}{dt} = \\lim_{h \\to 0} \\frac{x(t+h) - x(t)}{h}\n$$\n对于一个小的、有限的时间步长 $h > 0$，我们可以通过去掉极限来近似导数，这是一种一阶前向差分近似：\n$$\n\\frac{dx}{dt} \\approx \\frac{x(t+h) - x(t)}{h}\n$$\n我们将时间离散化为大小为 $h$ 的步长，使得 $t_n = n h$，$n = 0, 1, 2, \\ldots$。设 $x_n$ 和 $y_n$ 分别是真实解 $x(t_n)$ 和 $y(t_n)$ 的数值近似。因此，$x_n \\approx x(t_n)$ 且 $x_{n+1} \\approx x(t_{n+1}) = x(t_n + h)$。\n\n将这个近似代入时间 $t_n$ 处的 ODE 系统中，我们得到：\n$$\n\\frac{x_{n+1} - x_n}{h} \\approx \\left.\\frac{dx}{dt}\\right|_{t=t_n} = \\alpha x(t_n) - \\beta x(t_n) y(t_n)\n$$\n$$\n\\frac{y_{n+1} - y_n}{h} \\approx \\left.\\frac{dy}{dt}\\right|_{t=t_n} = \\delta x(t_n) y(t_n) - \\gamma y(t_n)\n$$\n前向欧拉法使用已知值 $x_n$ 和 $y_n$ 在当前时间步 $t_n$ 计算方程的右侧（即导数）。这会得到以下显式更新规则：\n$$\nx_{n+1} = x_n + h \\cdot f(x_n, y_n) = x_n + h(\\alpha x_n - \\beta x_n y_n)\n$$\n$$\ny_{n+1} = y_n + h \\cdot g(x_n, y_n) = y_n + h(\\delta x_n y_n - \\gamma y_n)\n$$\n这些方程使我们能够从初始条件 $(x_0, y_0)$ 开始，根据当前时间步的状态 $(x_n, y_n)$ 计算出下一个时间步的状态 $(x_{n+1}, y_{n+1})$。\n\n### 非物理负种群值问题\n\n尽管 Lotka-Volterra 方程的精确解保持非负，但前向欧拉法产生的数值解可能变为负值。这是数值不稳定的一个表现。我们可以分析更新方程来看这是如何发生的。\n\n考虑捕食者种群 $y$ 的更新方程：\n$$\ny_{n+1} = y_n + h(\\delta x_n y_n - \\gamma y_n) = y_n (1 + h(\\delta x_n - \\gamma))\n$$\n如果我们从一个正的种群 $y_n > 0$ 开始，更新后的种群 $y_{n+1}$ 将为负，当且仅当括号中的项为负：\n$$\n1 + h(\\delta x_n - \\gamma)  0 \\implies h(\\delta x_n - \\gamma)  -1 \\implies h(\\gamma - \\delta x_n) > 1\n$$\n当步长 $h$ 较大时，这个不等式更容易满足。它也取决于状态变量 $x_n$。如果猎物种群 $x_n$ 非常小，以至于 $\\delta x_n$ 远小于 $\\gamma$，捕食者种群的人均增长率 $(\\delta x_n - \\gamma)$ 就会成为一个很大的负数。一个大的步长 $h$ 可能导致更新“过冲”超过 $y=0$ 的值，从而产生非物理的负种群密度。\n\n对猎物种群 $x$ 的类似分析表明：\n$$\nx_{n+1} = x_n + h(\\alpha x_n - \\beta x_n y_n) = x_n(1 + h(\\alpha - \\beta y_n))\n$$\n从 $x_n > 0$ 开始，$x_{n+1}$ 可能变为负，如果：\n$$\n1 + h(\\alpha - \\beta y_n)  0 \\implies h(\\beta y_n - \\alpha) > 1\n$$\n如果步长 $h$ 较大且捕食者种群 $y_n$ 非常高，导致猎物的增长率为较大的负值，这种情况就可能发生。\n\n### 精度、效率和稳定性\n\n步长 $h$ 的选择涉及在精度、效率和稳定性之间的关键权衡。\n- **精度**：前向欧拉法是一种一阶方法，这意味着其全局截断误差（在固定时间 $T$ 后，数值解与真实解之间的差异）与步长成正比，即误差 $\\propto O(h)$。较小的 $h$ 会减小误差，并产生对真实动态过程更精确的近似。\n- **效率**：计算量由达到最终时间 $T$ 所需的总步数 $N$ 决定。由于 $N = T/h$，成本与 $h$ 成反比。较小的 $h$ 需要更多步数，增加了计算时间，使模拟效率降低。\n- **稳定性**：如前所述，大的步长 $h$ 可能导致数值不稳定，产生物理上无意义的结果，例如负的种群数量。对稳定且物理上合理的解的要求，对步长 $h$ 施加了一个上限。这个上限，称为稳定性条件，通常取决于系统参数和状态变量本身，这使得它对于非线性系统来说很复杂。\n\n总之，选择 $h$ 需要一种平衡。它必须足够小以确保解是稳定的（例如，保持非负）并达到期望的精度，但又必须足够大以使计算成本可控。所提供的测试用例旨在说明这种权衡：小的 $h$ 值（测试 1）应该在高的计算成本（$N=1000$）下是稳定和精确的，而大的 $h$ 值（测试 3, 4, 5）则为了高效率（低 $N$）而冒着不稳定和显著不精确的风险。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Applies the Forward Euler method to the Lotka-Volterra model for a suite of test cases\n    and analyzes the numerical behavior.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    # Each case is: (alpha, beta, gamma, delta, x0, y0, h, T)\n    test_cases = [\n        (1.1, 0.4, 0.4, 0.1, 10.0, 5.0, 0.01, 10.0),\n        (1.1, 0.4, 0.4, 0.1, 10.0, 5.0, 0.1, 10.0),\n        (1.1, 0.4, 0.4, 0.1, 10.0, 5.0, 1.2, 6.0),\n        (1.0, 0.5, 1.0, 0.05, 5.0, 20.0, 0.12, 1.2),\n        (1.0, 1.0, 1.0, 0.5, 1.0, 2.0, 1.0, 3.0),\n    ]\n\n    results = []\n    for case in test_cases:\n        alpha, beta, gamma, delta, x0, y0, h, T = case\n\n        # The problem statement guarantees T/h is an integer.\n        N = int(round(T / h))\n\n        # Initialize state variables\n        x = x0\n        y = y0\n\n        # Initialize tracking variables\n        min_x = x0\n        min_y = y0\n        neg_occurred = False\n\n        # Time-stepping loop\n        for _ in range(N):\n            # Calculate the next state using Forward Euler update rules\n            x_next = x + h * (alpha * x - beta * x * y)\n            y_next = y + h * (delta * x * y - gamma * y)\n            \n            # Update state\n            x = x_next\n            y = y_next\n\n            # Check for non-physical negative populations\n            if x  0.0 or y  0.0:\n                neg_occurred = True\n\n            # Track the minimum values encountered\n            if x  min_x:\n                min_x = x\n            if y  min_y:\n                min_y = y\n        \n        # In case the initial conditions were the minimums and a negative value\n        # occurred, we must ensure the minimum reflects the negative value.\n        # This is already handled by the loop logic correctly.\n\n        # Store the results for the current test case.\n        # Format: [neg_occurred, min_x, min_y, x_final, y_final, N]\n        result_for_case = [neg_occurred, min_x, min_y, x, y, N]\n        results.append(result_for_case)\n\n    # The final print statement must follow this exact format, with each sublist\n    # represented by its standard string form, joined by commas.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```", "id": "3204844"}, {"introduction": "最后，我们将挑战一个更复杂的场景，它体现了在高性能计算中常见的优化策略。求解大型线性方程组 $A u = b$ 是科学与工程计算的核心任务之一。当使用像共轭梯度法这样的迭代方法时，其收敛速度至关重要。本练习引入了“预条件”的概念，即通过一次性的“预处理”计算，将原问题转化为一个更易于求解的形式，从而加速迭代过程。您将比较两种不同的预条件技术：计算成本低廉的雅可比（Jacobi）预条件子和设置成本较高但可能更有效的不完全 Cholesky（IC(0)）分解。通过一个量化的成本模型，您将权衡初始设置开销与迭代次数节省之间的利弊，从而为特定问题选择最优的算法策略 [@problem_id:3204865]。", "problem": "你的任务是，在使用共轭梯度 (CG) 方法求解对称正定 (SPD) 矩阵 $A$ 的线性系统 $A u = b$ 时，评估两种不同预条件子——雅可比 (Jacobi) 和零填充不完全 Cholesky (IC(0))——在算法精度和效率之间的权衡。你的目标是实现一个程序，该程序针对一组指定的测试用例，计算达到目标绝对残差范数所需的 CG 迭代次数，并结合一个简单而有原则的运算计数模型来估算总工作量。然后，你必须为每个测试用例确定哪种预条件子能使总成本最小化。\n\n使用的基本原理：\n- 共轭梯度 (CG) 方法适用于 SPD 系统，并在一个 Krylov 子空间中生成迭代解 $u_k$，以减小残差 $r_k = b - A u_k$。\n- 预处理将系统转换为 $M^{-1} A u = M^{-1} b$，其中 $M$ 是一个 SPD 矩阵，其选择旨在使变换后的系统具有更好的条件数。雅可比预条件子使用 $M = \\operatorname{diag}(A)$，而零填充不完全 Cholesky (IC(0)) 使用 $M = L L^\\top$，其中 $L$ 是一个下三角矩阵，根据 $A$ 的稀疏模式构建，且不引入超出该模式的填充。\n- 此处的算法效率通过浮点运算 (flop) 计数来评估，该计数与非零元素数量等结构量相关。\n\n实现要求：\n1. 构建以下 SPD 测试矩阵 $A$ 和右端向量 $b$：\n   - 用例 #1：尺寸为 $n = 50$ 的一维泊松三对角矩阵，其模板项为对角线上是 $2$，第一亚对角线和第一超对角线上是 $-1$，$b = \\mathbf{1}$，目标绝对残差容差为 $\\varepsilon = 10^{-8}$。\n   - 用例 #2：$10 \\times 10$ 网格上的二维泊松五点模板（因此 $n = 100$），对角线上为 $4$，每个内部邻居对应位置为 $-1$，$b = \\mathbf{1}$，$\\varepsilon = 10^{-8}$。\n   - 用例 #3：尺寸为 $n = 80$ 的对角 SPD 矩阵，其对角线元素在 $10^{-2}$ 到 $10^{2}$ 之间呈几何分布，即 $A = \\operatorname{diag}\\left(10^{-2}, 10^{-2 + \\frac{4}{79}}, \\dots, 10^{2}\\right)$，$b = \\mathbf{1}$，$\\varepsilon = 10^{-10}$。\n   - 用例 #4：标量 SPD 矩阵 $A = [2]$ (因此 $n=1$)，$b = [1]$，$\\varepsilon = 10^{-12}$。\n   - 用例 #5：尺寸为 $n = 200$ 的一维泊松三对角矩阵，模板与用例 #1 相同，$b = \\mathbf{1}$，$\\varepsilon = 10^{-6}$。\n   在所有用例中，使用绝对 2-范数残差停止准则 $\\lVert r_k \\rVert_2 \\le \\varepsilon$，并以 $u_0 = 0$ 初始化。角度单位不适用。\n\n2. 实现预条件共轭梯度 (PCG) 方法，包含以下组件：\n   - 雅可比预条件子：$M = \\operatorname{diag}(A)$，因此 $M^{-1} r$ 通过逐元素除以 $A$ 的对角线来计算。\n   - 不完全 Cholesky IC(0) 预条件子：计算一个下三角矩阵 $L$，使得 $L L^\\top \\approx A$，使用 $A$ 的稀疏模式，且在严格下三角部分，除了 $A_{ij} \\ne 0$ 的位置外，不引入任何填充。也就是说，$L_{ij}$ 仅在 $i \\ge j$ 且 $A_{ij} \\ne 0$ 时才可能为非零。通过求解 $L y = r$ (前向代入) 和 $L^\\top z = y$ (后向代入) 来应用 $M^{-1}$。\n   - 通过处理 IC(0) 分解中过小或为负的对角主元，在需要时使用一个小的正则化项，以确保数值稳定性。\n\n3. 定义并使用以下运算计数模型来估算总计算成本：\n   - 令 $\\operatorname{nnz}(A)$ 表示 $A$ 中的非零元素数量，$\\operatorname{nnz}(L)$ 表示下三角因子 $L$ 中的非零元素数量 (包括其对角线)。\n   - 设置成本：\n     - 雅可比：$C_{\\text{setup}}^{\\text{Jac}} = n$。\n     - IC(0): $C_{\\text{setup}}^{\\text{IC}} = \\sum_{i=1}^{n} d_i^2$, 其中 $d_i$ 是 $L$ 的第 $i$ 行中结构上允许的非零元素数量（包括对角线），即满足 $j \\le i$ 且 $A_{ij} \\ne 0$ 的索引 $j$ 的数量。\n   - 每次迭代的成本：\n     - 每次 PCG 迭代的基准成本（两者通用）：$C_{\\text{base}} = 2 \\cdot \\operatorname{nnz}(A) + 12 n$，涵盖了一次稀疏矩阵-向量乘积、内积和向量更新。\n     - 应用预条件子的额外成本：\n       - 雅可比：$C_{\\text{prec}}^{\\text{Jac}} = n$。\n       - IC(0): $C_{\\text{prec}}^{\\text{IC}} = 2 \\cdot \\operatorname{nnz}(L)$ (前向和后向三角求解)。\n     - 因此，每次迭代的成本为 $C_{\\text{iter}}^{\\text{Jac}} = 2 \\cdot \\operatorname{nnz}(A) + 13 n$ 和 $C_{\\text{iter}}^{\\text{IC}} = 2 \\cdot \\operatorname{nnz}(A) + 12 n + 2 \\cdot \\operatorname{nnz}(L)$。\n   - 达到容差 $\\varepsilon$ 的总成本为 $C_{\\text{total}} = C_{\\text{setup}} + k \\cdot C_{\\text{iter}}$，其中 $k$ 是 PCG 达到 $\\lVert r_k \\rVert_2 \\le \\varepsilon$ 所需的迭代次数。\n\n4. 对每个测试用例，计算：\n   - $k_{\\text{Jac}}$: 使用雅可比预处理的迭代次数。\n   - $k_{\\text{IC}}$: 使用 IC(0) 预处理的迭代次数。\n   - 使用上述模型计算的 $C_{\\text{total}}^{\\text{Jac}}$ 和 $C_{\\text{total}}^{\\text{IC}}$。\n   - 一个优胜者代码 $w$，定义如下：如果雅可比预处理的总成本严格更小，则 $w = 0$；如果 IC(0) 预处理的总成本严格更小，则 $w = 1$；如果总成本相等，则 $w = -1$。\n\n5. 最终输出格式：\n   - 你的程序应生成单行输出，其中包含一个用方括号括起来的、以逗号分隔的列表。每个测试用例的结果本身必须是一个包含五个整数的列表，格式为 $[w, k_{\\text{Jac}}, k_{\\text{IC}}, C_{\\text{total}}^{\\text{Jac}}, C_{\\text{total}}^{\\text{IC}}]$。\n   - 例如，整体结构必须是单行的 $[[w_1, k_{\\text{Jac},1}, k_{\\text{IC},1}, C_{\\text{total},1}^{\\text{Jac}}, C_{\\text{total},1}^{\\text{IC}}], \\dots, [w_5, k_{\\text{Jac},5}, k_{\\text{IC},5}, C_{\\text{total},5}^{\\text{Jac}}, C_{\\text{total},5}^{\\text{IC}}]]$。\n\n需要精确实现的测试套件：\n- 用例 #1：$n = 50$，一维泊松，$\\varepsilon = 10^{-8}$。\n- 用例 #2：$10 \\times 10$ 二维泊松，$\\varepsilon = 10^{-8}$。\n- 用例 #3：$n = 80$，对角线元素呈几何分布，从 $10^{-2}$ 到 $10^{2}$，$\\varepsilon = 10^{-10}$。\n- 用例 #4：$n = 1$，$A = [2]$，$\\varepsilon = 10^{-12}$。\n- 用例 #5：$n = 200$，一维泊松，$\\varepsilon = 10^{-6}$。\n\n所有数值答案必须是指定的整数，本问题不涉及物理单位或角度单位。算法残差范数必须是绝对 2-范数，迭代次数必须计为整数。程序必须是完全自包含的，不得需要任何用户输入或外部数据。输出必须遵守上述指定的精确格式。", "solution": "问题要求对应用于对称正定 (SPD) 线性系统 $Au=b$ 的两种预条件共轭梯度 (PCG) 方法的算法精度与效率进行权衡评估。这两种预条件子是雅可比 (Jacobi) 和零填充不完全 Cholesky (IC($0$))。评估基于一个指定的浮点运算 (flop) 计数模型。解决方案分为四个主要部分：矩阵构建、预条件子实现、PCG 求解器和成本分析逻辑，每个部分都旨在满足问题的具体要求。\n\n### 1. 系统与算法实现\n\n**矩阵构建：**\n五个测试用例涉及三种类型的 SPD 矩阵，构建方法如下：\n- **一维泊松矩阵：** 对于一个 $n \\times n$ 矩阵，这是一个三对角矩阵，主对角线为 $2$，第一亚对角线和超对角线为 $-1$。使用 `numpy.diag` 构建。\n- **二维泊松矩阵：** 对于一个 $m \\times m$ 网格，这会产生一个 $n \\times n$ 的矩阵，其中 $n=m^2$。该矩阵主对角线为 $4$，对应于网格邻居的非对角线位置为 $-1$。这个结构使用 `numpy.diag` 来构建主对角线（值为 $4$）、第一亚/超对角线（值为 $-1$）以及第 $m$ 亚/超对角线（值为 $-1$）。对应于网格边界的连接（例如，第 $m-1$ 行和第 $m$ 行之间）随后被显式设置为 $0$，以强制正确的二维拓扑结构。\n- **对角矩阵：** 对于用例 #3，创建了一个尺寸为 $n=80$ 的对角矩阵，其对角线元素在 $10^{-2}$ 到 $10^{2}$ 之间呈几何分布。这是通过使用 `numpy.geomspace` 生成对角线元素来实现的。\n- **标量矩阵：** 用例 #4 是一个平凡的 $1 \\times 1$ 矩阵 $A=[2]$。\n\n右端向量 $b$ 在所有用例中都是全一向量 $\\mathbf{1}$，除了用例 #4 中 $b=[1]$。在所有用例中，解的初始猜测都是零向量 $u_0 = 0$。\n\n**预条件共轭梯度 (PCG) 求解器：**\n采用 PCG 算法的标准实现。该算法迭代地改进解 $u_k$ 以最小化误差的 A-范数。过程从 $u_0 = 0, r_0 = b$ 开始，对 $k=0, 1, 2, \\ldots$ 按如下步骤进行：\n1. 求解 $M z_k = r_k$ 得到 $z_k$。\n2. 计算 $\\rho_k = r_k^\\top z_k$。\n3. 更新搜索方向：如果 $k=0$，则 $p_k = z_k$；否则 $p_k = z_k + (\\rho_k / \\rho_{k-1}) p_{k-1}$。\n4. 计算 $q_k = A p_k$。\n5. 计算步长：$\\alpha_k = \\rho_k / (p_k^\\top q_k)$。\n6. 更新解：$u_{k+1} = u_k + \\alpha_k p_k$。\n7. 更新残差：$r_{k+1} = r_k - \\alpha_k q_k$。\n当残差的绝对 2-范数 $\\lVert r_{k+1} \\rVert_2$ 小于指定的容差 $\\varepsilon$ 时，循环终止。返回迭代次数 $k$。\n\n### 2. 预条件子实现\n\n预条件子 $M$ 是 $A$ 的一个近似，用于将系统转换为一个条件数更好的系统 $M^{-1} A u = M^{-1} b$，从而加速 CG 的收敛。在每次 PCG 迭代中都需要计算 $M^{-1}$ 对一个向量的作用。\n\n**雅可比预条件子：**\n雅可比预条件子定义为 $M = \\operatorname{diag}(A)$。由于 $A$ 是 SPD 矩阵，其对角线元素均为正。将 $M^{-1}$ 应用于残差向量 $r$ 是一个简单且计算成本低廉的逐元素除法：$(M^{-1}r)_i = r_i / A_{ii}$。其设置成本极小，仅涉及提取对角线。\n\n**不完全 Cholesky (IC(0)) 预条件子：**\nIC($0$) 预条件子构建一个近似 $M = L L^\\top \\approx A$，其中 $L$ 是一个下三角矩阵。IC($0$) 的关键约束是 $L$ 的稀疏模式被限制在 $A$ 的下三角部分的稀疏模式内。也就是说，$L_{ij}$ 仅当 $i \\ge j$ 且 $A_{ij}$ 非零时才可能为非零。\n\n该分解通过遍历 $i \\ge j$ 的矩阵元素 $(i, j)$ 并基于标准的 Cholesky 更新公式计算 $L_{ij}$ 来实现，但求和仅限于允许的稀疏模式内：\n$$ L_{ij} = \\frac{1}{L_{jj}} \\left( A_{ij} - \\sum_{k=0}^{j-1, L_{ik}\\ne 0, L_{jk}\\ne 0} L_{ik} L_{jk} \\right) \\quad (i  j) $$\n$$ L_{ii} = \\sqrt{A_{ii} - \\sum_{k=0}^{i-1, L_{ik}\\ne 0} L_{ik}^2} $$\n为了按要求确保数值稳定性，如果用于计算 $L_{ii}$ 的平方根的参数为非正数，则将其替换为一个小的正值 ($10^{-12}$)。\n应用 $M^{-1}$ 涉及求解 $L y = r$ (前向代入)，然后求解 $L^\\top z = y$ (后向代入)。这些三角求解是通过遵循 $L$ 的稀疏结构的循环来实现的。\n\n### 3. 成本分析与比较\n\n关于哪种预条件子更“高效”的决定是基于所提供的总计算成本模型。总成本是一次性设置成本与迭代累积成本之和。\n\n**成本模型计算：**\n对于每个测试用例和预条件子，计算以下量：\n- PCG 求解器返回的迭代次数 $k$。\n- **设置成本 ($C_{\\text{setup}}$):**\n  - 雅可比：$C_{\\text{setup}}^{\\text{Jac}} = n$。\n  - IC($0$): $C_{\\text{setup}}^{\\text{IC}} = \\sum_{i=1}^{n} d_i^2$, 其中 $d_i$ 是 $A$ 的第 $i$ 行下三角部分（包括对角线）的非零元素数量 (即 `np.count_nonzero(A[i, :i+1])`)。\n- **每次迭代成本 ($C_{\\text{iter}}$):**\n  - 雅可比：$C_{\\text{iter}}^{\\text{Jac}} = 2 \\cdot \\operatorname{nnz}(A) + 13 n$。\n  - IC($0$): $C_{\\text{iter}}^{\\text{IC}} = 2 \\cdot \\operatorname{nnz}(A) + 12 n + 2 \\cdot \\operatorname{nnz}(L)$。\n  此处，$\\operatorname{nnz}(A)$ 和 $\\operatorname{nnz}(L)$ 分别是矩阵 $A$ 和计算出的因子 $L$ 中的非零条目数。\n- **总成本 ($C_{\\text{total}}$):**\n  - $C_{\\text{total}} = C_{\\text{setup}} + k \\cdot C_{\\text{iter}}$.\n\n所有成本计算均使用整数算术执行，最终结果按要求转换为整数。\n\n**优胜者确定：**\n对每个测试用例，比较总成本 $C_{\\text{total}}^{\\text{Jac}}$ 和 $C_{\\text{total}}^{\\text{IC}}$。分配一个优胜者代码 $w$：如果雅可比预处理的成本严格更低，则 $w=0$；如果 IC($0$) 的成本严格更低，则 $w=1$；如果成本相同，则 $w=-1$。每个用例的最终输出是一个包含 $[w, k_{\\text{Jac}}, k_{\\text{IC}}, C_{\\text{total}}^{\\text{Jac}}, C_{\\text{total}}^{\\text{IC}}]$ 的列表。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem, encapsulating all logic.\n    \"\"\"\n\n    def build_poisson_1d(n):\n        \"\"\"Builds a 1D Poisson matrix of size n x n.\"\"\"\n        A = np.diag(np.full(n, 2.0))\n        A += np.diag(np.full(n - 1, -1.0), k=1)\n        A += np.diag(np.full(n - 1, -1.0), k=-1)\n        return A\n\n    def build_poisson_2d(m):\n        \"\"\"Builds a 2D Poisson matrix for an m x m grid.\"\"\"\n        n = m * m\n        A = np.diag(np.full(n, 4.0))\n        off_diag1 = np.full(n - 1, -1.0)\n        off_diag_m = np.full(n - m, -1.0)\n        \n        A += np.diag(off_diag1, k=1)\n        A += np.diag(off_diag1, k=-1)\n        A += np.diag(off_diag_m, k=m)\n        A += np.diag(off_diag_m, k=-m)\n        \n        # Remove connections at grid boundaries\n        for i in range(1, m):\n            idx = i * m\n            A[idx - 1, idx] = 0\n            A[idx, idx - 1] = 0\n            \n        return A\n\n    def build_diag_geometric(n, v_start, v_end):\n        \"\"\"Builds a diagonal matrix with geometrically spaced entries.\"\"\"\n        return np.diag(np.geomspace(v_start, v_end, n))\n\n    class JacobiPreconditioner:\n        \"\"\"Jacobi Preconditioner: M = diag(A).\"\"\"\n        def __init__(self, A):\n            self.inv_diag = 1.0 / np.diag(A)\n        \n        def apply(self, r):\n            return self.inv_diag * r\n\n    class IC0Preconditioner:\n        \"\"\"Incomplete Cholesky (0) Preconditioner.\"\"\"\n        def __init__(self, A):\n            self.L = self._factor(A)\n            self.Lt = self.L.T\n\n        def _factor(self, A):\n            n = A.shape[0]\n            L = np.zeros_like(A, dtype=float)\n            \n            for i in range(n):\n                for j in range(i + 1):\n                    if A[i, j] != 0 or i == j:\n                        s = A[i, j]\n                        # This vectorized version is faster than iterating k\n                        # and performs the same calculation based on sparsity\n                        s -= np.dot(L[i, :j], L[j, :j])\n                        \n                        if i == j:\n                            if s = 1e-12: s = 1e-12\n                            L[i, i] = np.sqrt(s)\n                        else:\n                            L[i, j] = s / L[j, j]\n            return L\n\n        def apply(self, r):\n            # Forward substitution: Solve Ly = r\n            y = np.linalg.solve(self.L, r)\n            # Backward substitution: Solve L^T z = y\n            z = np.linalg.solve(self.Lt, y)\n            return z\n\n    def pcg(A, b, precon, tol, max_iter=10000):\n        \"\"\"Preconditioned Conjugate Gradient solver.\"\"\"\n        n = A.shape[0]\n        u = np.zeros(n)\n        r = b - A @ u\n        \n        r_norm = np.linalg.norm(r)\n        if r_norm  tol:\n            return 0\n        \n        z = precon.apply(r)\n        p = z\n        rho_old = np.dot(r, z)\n        \n        for k in range(1, max_iter + 1):\n            q = A @ p\n            alpha = rho_old / np.dot(p, q)\n            u += alpha * p\n            r -= alpha * q\n            \n            r_norm = np.linalg.norm(r)\n            if r_norm  tol:\n                return k\n            \n            z = precon.apply(r)\n            rho_new = np.dot(r, z)\n            if rho_old == 0: return k # Should not happen for SPD A\n            beta = rho_new / rho_old\n            p = z + beta * p\n            rho_old = rho_new\n            \n        return max_iter\n\n    def calculate_total_cost(A, k, precon_name, L=None):\n        \"\"\"Calculates total cost based on the provided model.\"\"\"\n        n = A.shape[0]\n        nnz_A = np.count_nonzero(A)\n        \n        if precon_name == 'jacobi':\n            c_setup = n\n            c_iter = 2 * nnz_A + 13 * n\n        elif precon_name == 'ic0':\n            d_i_sq_sum = sum(np.count_nonzero(A[i, :i+1])**2 for i in range(n))\n            c_setup = d_i_sq_sum\n            nnz_L = np.count_nonzero(L)\n            c_iter = 2 * nnz_A + 12 * n + 2 * nnz_L\n        else:\n            raise ValueError(\"Unknown preconditioner name\")\n            \n        return c_setup + k * c_iter\n\n    test_cases = [\n        {'id': 1, 'type': 'poisson_1d', 'n': 50, 'tol': 1e-8},\n        {'id': 2, 'type': 'poisson_2d', 'm': 10, 'tol': 1e-8},\n        {'id': 3, 'type': 'diag_geom', 'n': 80, 'start': 1e-2, 'end': 1e2, 'tol': 1e-10},\n        {'id': 4, 'type': 'scalar', 'A': np.array([[2.0]]), 'b': np.array([1.0]), 'tol': 1e-12},\n        {'id': 5, 'type': 'poisson_1d', 'n': 200, 'tol': 1e-6},\n    ]\n\n    results = []\n    for case in test_cases:\n        if case['type'] == 'poisson_1d':\n            A = build_poisson_1d(case['n'])\n            b = np.ones(case['n'])\n        elif case['type'] == 'poisson_2d':\n            n = case['m']**2\n            A = build_poisson_2d(case['m'])\n            b = np.ones(n)\n        elif case['type'] == 'diag_geom':\n            A = build_diag_geometric(case['n'], case['start'], case['end'])\n            b = np.ones(case['n'])\n        elif case['type'] == 'scalar':\n            A = case['A']\n            b = case['b']\n        \n        tol = case['tol']\n        \n        # Jacobi\n        precon_jac = JacobiPreconditioner(A)\n        k_jac = pcg(A, b, precon_jac, tol)\n        cost_jac = calculate_total_cost(A, k_jac, 'jacobi')\n        \n        # IC(0)\n        precon_ic0 = IC0Preconditioner(A)\n        k_ic0 = pcg(A, b, precon_ic0, tol)\n        cost_ic0 = calculate_total_cost(A, k_ic0, 'ic0', L=precon_ic0.L)\n        \n        # Determine winner\n        if cost_jac  cost_ic0:\n            w = 0\n        elif cost_ic0  cost_jac:\n            w = 1\n        else:\n            w = -1\n        \n        results.append([w, k_jac, k_ic0, int(cost_jac), int(cost_ic0)])\n\n    # Final print statement in the exact required format.\n    formatted_results = [f\"[{','.join(map(str, res))}]\" for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3204865"}]}