{"hands_on_practices": [{"introduction": "本实践练习将有力地展示收敛加速技术。您将从求解 $x = \\cos(x)$ 的基本不动点迭代法开始，并观察其线性收敛性。然后，通过应用 Steffensen 方法——一种无需额外导数信息的巧妙技巧——您将见证收敛速度跃升至二次，效率得到显著提升。这个练习突出了算法上的改进如何能够对性能产生深远的影响。[@problem_id:3265247]", "problem": "编写一个完整的程序，研究并比较基本不动点迭代法及其 Steffensen 加速法对于函数 $\\cos(x)$ 的收敛速度，计算中使用弧度。您的推导必须基于收敛阶的核心定义：一个序列 $\\{x_k\\}$ 以 $p \\ge 1$ 阶收敛于 $x^\\star$，如果存在一个常数 $C \\in (0,\\infty)$ 使得\n$$\n\\lim_{k \\to \\infty} \\frac{|x_{k+1} - x^\\star|}{|x_k - x^\\star|^p} = C.\n$$\n问题是从该定义出发，不使用任何外部数据源，实现以下任务。\n\n任务：\n- 考虑不动点映射 $g(x) = \\cos(x)$ 和不动点迭代 $x_{k+1} = g(x_k)$。已知该迭代会线性收敛到 $g$ 的唯一不动点 $x^\\star$，即方程 $x = \\cos(x)$ 的解。\n- 将 Steffensen 方法应用于同一映射 $g$。该方法在当前迭代点 $x_k$ 处仅使用 $g$ 的函数值，其代数更新式如下：\n$$\nx_{k+1} = x_k - \\frac{\\left(g(x_k) - x_k\\right)^2}{g(g(x_k)) - 2 g(x_k) + x_k},\n$$\n该公式在分母不为零时使用。如果分母等于 $0$ 或在机器精度范围内与 $0$ 在数值上无法区分，则该步骤必须回退到普通的不动点更新 $x_{k+1} = g(x_k)$。\n- 使用根据相对于高精度参考解 $x^\\star$ 的三个连续误差计算出的观测收敛阶估计量。对于误差 $e_k = |x_k - x^\\star|$，定义：\n$$\np_k = \\frac{\\ln\\left(e_{k} / e_{k-1}\\right)}{\\ln\\left(e_{k-1} / e_{k-2}\\right)},\n$$\n该公式在对数有定义时使用。使用由不太接近浮点数下溢的误差计算出的最新可用 $p_k$；具体来说，优先选择满足 $e_{k-2} \\ge 10^{-10}$ 的最大索引 $k$；如果不存在这样的三元组，则使用可用的最大有效 $p_k$。\n- 使用基于相对于 $x^\\star$ 的绝对误差低于某个容差，或达到最大迭代次数上限的停止准则，以先到者为准。\n\n参考解 $x^\\star$：\n- 通过应用牛顿法，将 $f(x) = \\cos(x) - x = 0$ 的解作为 $x^\\star$ 的高精度参考值进行内部计算。牛顿法公式为：\n$$\nx_{n+1} = x_n - \\frac{f(x_n)}{f'(x_n)}, \\quad f'(x) = -\\sin(x) - 1,\n$$\n从 $x_0 = 0.7$ 开始迭代，直到牛顿更新步长的大小小于 $10^{-16}$ 或达到最大 100 次迭代。使用这个内部计算出的 $x^\\star$ 进行所有误差计算。角度以弧度为单位。\n\n实现与数值细节：\n- 实现两个生成序列 $\\{x_k\\}$ 的求解器：\n  1. 普通不动点迭代 $x_{k+1} = \\cos(x_k)$。\n  2. 上面定义的 Steffensen 加速迭代。\n- 对两个求解器，当 $|x_k - x^\\star|  10^{-14}$ 或达到 200 次迭代后停止，以先到者为准。\n- 对于 Steffensen 方法，如果分母 $g(g(x_k)) - 2 g(x_k) + x_k$ 在精确算术中等于 $0$，或在浮点运算中其绝对值小于 $10^{-14}$，则该步骤使用回退方案 $x_{k+1} = g(x_k)$。\n- 对于每个生成的序列，使用上面定义的 $p_k$ 估计量计算观测收敛阶，并按照之前的规定为每个序列报告一个单一的估计值。\n\n测试套件：\n- 对两种方法使用以下四个初始猜测值：$x_0 \\in \\{0.1, 1.0, 2.0, -1.0\\}$。\n- 对于每个 $x_0$，同时运行普通不动点迭代和 Steffensen 方法。对于每个序列，返回四舍五入到两位小数的观测收敛阶估计值。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含 8 个结果，形式为方括号内的逗号分隔列表，顺序如下：\n$$\n[\\text{普通法}(0.1),\\ \\text{Steffensen法}(0.1),\\ \\text{普通法}(1.0),\\ \\text{Steffensen法}(1.0),\\ \\text{普通法}(2.0),\\ \\text{Steffensen法}(2.0),\\ \\text{普通法}(-1.0),\\ \\text{Steffensen法}(-1.0)].\n$$\n- 每个条目是四舍五入到两位小数的观测收敛阶估计值，并且必须打印为浮点数。\n- 不需要用户输入。不涉及物理单位。角度以弧度为单位。单行输出必须严格遵循上述格式，例如：$[1.00,2.00,1.00,2.00,1.00,2.00,1.00,2.00]$。", "solution": "该问题要求对标准不动点迭代法及其 Steffensen 加速法的收敛速度进行比较研究。分析将在函数 $g(x) = \\cos(x)$ 上进行，所有计算均使用弧度。任务的核心是实现这两种方法，生成迭代序列，然后对每个序列的收敛阶进行数值估计。\n\n### 理论框架\n\n函数 $g(x)$ 的不动点是一个值 $x^\\star$，满足 $x^\\star = g(x^\\star)$。对于给定的函数 $g(x) = \\cos(x)$，其不动点是方程 $x = \\cos(x)$ 的唯一解，该解也被称为 Dottie 数。\n\n**1. 普通不动点迭代**\n\n基本不动点迭代由序列 $x_{k+1} = g(x_k)$ 定义，其中 $k=0, 1, 2, \\dots$。该方法的收敛性由函数 $g(x)$ 在不动点 $x^\\star$ 邻域内的性质决定。根据不动点定理，如果 $|g'(x^\\star)|  1$，那么对于任何足够接近 $x^\\star$ 的初始猜测值 $x_0$，迭代都将线性收敛。\n\n对于 $g(x) = \\cos(x)$，其导数为 $g'(x) = -\\sin(x)$。不动点 $x^\\star$ 约等于 $0.739085$。在该点，导数为 $g'(x^\\star) = -\\sin(x^\\star) \\approx -0.674$。由于 $|g'(x^\\star)| \\approx 0.674  1$，迭代保证收敛。\n\n收敛是线性的，这意味着误差 $e_k = |x_k - x^\\star|$ 在每一步都大致减少一个常数因子。形式上，如果存在渐进误差常数 $C \\in (0, 1)$，使得\n$$\n\\lim_{k \\to \\infty} \\frac{|x_{k+1} - x^\\star|}{|x_k - x^\\star|^1} = |g'(x^\\star)| = C,\n$$\n则序列 $\\{x_k\\}$ 以 $p=1$ 阶（线性）收敛于 $x^\\star$。对于此问题，我们预期估计的收敛阶将接近 $p=1$。\n\n**2. Steffensen 方法**\n\nSteffensen 方法是一种加速技术，它能将一个线性收敛的不动点迭代转化为二次收敛的迭代，而无需计算导数。对于给定的映射 $g(x)$，其更新规则为：\n$$\nx_{k+1} = x_k - \\frac{\\left(g(x_k) - x_k\\right)^2}{g(g(x_k)) - 2 g(x_k) + x_k}.\n$$\n这可以看作是对函数 $f(x) = g(x) - x = 0$ 应用牛顿法，其中导数 $f'(x) = g'(x) - 1$ 是用有限差分来近似的。在适当的条件下（具体来说，$g'(x^\\star) \\neq 1$），Steffensen 方法表现出二次收敛性。\n\n二次收敛（$p=2$ 阶）意味着每次迭代中正确的有效数字位数大约会翻倍。形式上，对于某个常数 $C \\in (0, \\infty)$，有\n$$\n\\lim_{k \\to \\infty} \\frac{|x_{k+1} - x^\\star|}{|x_k - x^\\star|^2} = C\n$$\n因此，我们预期 Steffensen 方法的估计收敛阶将接近 $p=2$。如果 Steffensen 更新式中的分母在数值上接近于零，则有必要回退到普通的不动点迭代步骤 $x_{k+1}=g(x_k)$。这种情况可能发生在 $g(x_k)-x_k$ 非常小的时候，即当我们已经非常接近解时。\n\n### 实现策略\n\n**1. 高精度参考解 $x^\\star$**\n\n为了计算误差 $e_k = |x_k - x^\\star|$，需要一个高精度的 $x^\\star$ 值。该值将通过对对方程 $f(x) = \\cos(x) - x = 0$ 应用牛顿法来计算。牛顿迭代式为：\n$$\nx_{n+1} = x_n - \\frac{f(x_n)}{f'(x_n)} = x_n - \\frac{\\cos(x_n) - x_n}{-\\sin(x_n) - 1}.\n$$\n从 $x_0 = 0.7$ 开始，我们进行迭代，直到更新步长的大小 $|x_{n+1} - x_n|$ 小于 $10^{-16}$ 这个严格的容差。\n\n**2. 迭代求解器**\n\n将实现两个求解器：一个用于普通不动点迭代 $x_{k+1} = \\cos(x_k)$，另一个用于 Steffensen 方法。两个求解器都将从给定的初始猜测值 $x_0$ 开始，并生成一个迭代序列 $\\{x_k\\}$。当绝对误差 $|x_k - x^\\star|$ 小于 $10^{-14}$ 或达到最大 200 次迭代时，迭代将终止。Steffensen 求解器将包含指定的后备机制，即如果 $|g(g(x_k)) - 2g(x_k) + x_k|  10^{-14}$，则使用更简单的更新式 $x_{k+1} = \\cos(x_k)$。\n\n**3. 观测收敛阶 $p_k$**\n\n收敛阶是从误差序列 $\\{e_k\\}$ 中数值估计出来的。提供的公式是：\n$$\np_k = \\frac{\\ln\\left(e_{k} / e_{k-1}\\right)}{\\ln\\left(e_{k-1} / e_{k-2}\\right)}.\n$$\n此公式由理论定义 $e_k \\approx C e_{k-1}^p$ 推导而来，该定义意味着 $\\ln(e_k) - \\ln(e_{k-1}) \\approx p (\\ln(e_{k-1}) - \\ln(e_{k-2}))$。生成序列后，我们为所有可能的索引 $k$ 计算 $p_k$。为了获得一个单一的代表性收敛阶估计，我们选择满足误差 $e_{k-2}$ 不过分小（具体来说是 $e_{k-2} \\ge 10^{-10}$）的最大索引 $k$ 对应的 $p_k$ 值。这确保了估计值取自收敛的渐近区域，但又在浮点精度限制误差比率的准确性之前。如果不存在这样的索引，则使用最后计算出的 $p_k$ 值。\n\n最终程序将对每个初始猜测值 $x_0 \\in \\{0.1, 1.0, 2.0, -1.0\\}$ 执行此过程，总共产生八个收敛阶估计值，并按要求进行格式化和打印。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to orchestrate the numerical experiment and print the final results.\n    \"\"\"\n\n    def compute_reference_solution():\n        \"\"\"\n        Computes a high-accuracy reference solution x* for x = cos(x) using Newton's method.\n        The equation is f(x) = cos(x) - x = 0.\n        The derivative is f'(x) = -sin(x) - 1.\n        \"\"\"\n        x = 0.7  # Initial guess\n        max_iter = 100\n        tolerance = 1e-16\n        \n        for _ in range(max_iter):\n            f_x = np.cos(x) - x\n            fp_x = -np.sin(x) - 1\n            if fp_x == 0:  # Avoid division by zero\n                break\n            update = -f_x / fp_x\n            x += update\n            if np.abs(update)  tolerance:\n                break\n        return x\n\n    def run_iteration(method, x0, x_star):\n        \"\"\"\n        Generates a sequence of iterates for a given method.\n\n        Args:\n            method (str): 'plain' for fixed-point, 'steffensen' for Steffensen's method.\n            x0 (float): The initial guess.\n            x_star (float): The high-accuracy reference solution.\n\n        Returns:\n            list: The history of iterates [x0, x1, ...].\n        \"\"\"\n        x_k = float(x0)\n        history = [x_k]\n        max_iter = 200\n        stop_tol = 1e-14\n        fallback_tol = 1e-14\n\n        for _ in range(max_iter):\n            if np.abs(x_k - x_star)  stop_tol:\n                break\n            \n            if method == 'plain':\n                x_k_plus_1 = np.cos(x_k)\n            elif method == 'steffensen':\n                g_xk = np.cos(x_k)\n                g_g_xk = np.cos(g_xk)\n                denominator = g_g_xk - 2 * g_xk + x_k\n                \n                if np.abs(denominator)  fallback_tol:\n                    # Fallback to plain fixed-point iteration\n                    x_k_plus_1 = g_xk\n                else:\n                    numerator = (g_xk - x_k)**2\n                    x_k_plus_1 = x_k - numerator / denominator\n            else:\n                raise ValueError(\"Unknown method specified.\")\n            \n            x_k = x_k_plus_1\n            history.append(x_k)\n            \n        return history\n\n    def estimate_order(x_history, x_star):\n        \"\"\"\n        Estimates the order of convergence from a sequence of iterates.\n\n        Args:\n            x_history (list): The sequence of iterates.\n            x_star (float): The high-accuracy reference solution.\n\n        Returns:\n            float: The estimated order of convergence.\n        \"\"\"\n        if len(x_history)  3:\n            # Not enough data points to compute even one p_k value.\n            # Based on the problem, this scenario is not expected.\n            return np.nan\n\n        errors = np.abs(np.array(x_history) - x_star)\n        \n        p_k_values = []\n        for k in range(2, len(errors)):\n            e_k2, e_k1, e_k = errors[k-2], errors[k-1], errors[k]\n            \n            # Avoid log(0) and division by zero from the ratios\n            if e_k1 == 0 or e_k2 == 0 or e_k1 == e_k2:\n                continue\n            \n            # Ratios for the logarithms\n            ratio1 = e_k / e_k1\n            ratio2 = e_k1 / e_k2\n\n            # Ensure arguments to log are positive\n            if ratio1 = 0 or ratio2 = 0:\n                continue\n            \n            log_numerator = np.log(ratio1)\n            log_denominator = np.log(ratio2)\n            \n            if log_denominator == 0:\n                continue\n            \n            p_k = log_numerator / log_denominator\n            p_k_values.append((k, p_k))\n\n        if not p_k_values:\n            # Could not compute any p_k.\n            return np.nan\n        \n        # Selection rule: Find the latest p_k where e_{k-2} is not too small.\n        for k, p_k in reversed(p_k_values):\n            if errors[k-2] = 1e-10:\n                return p_k\n        \n        # Fallback: if no such p_k exists, use the latest one available.\n        return p_k_values[-1][1]\n\n    # Compute the reference solution once\n    x_star = compute_reference_solution()\n\n    test_cases = [0.1, 1.0, 2.0, -1.0]\n    final_results = []\n\n    for x0_val in test_cases:\n        # Plain fixed-point iteration\n        history_plain = run_iteration('plain', x0_val, x_star)\n        order_plain = estimate_order(history_plain, x_star)\n        final_results.append(order_plain)\n\n        # Steffensen-accelerated iteration\n        history_steff = run_iteration('steffensen', x0_val, x_star)\n        order_steff = estimate_order(history_steff, x_star)\n        final_results.append(order_steff)\n\n    # Format the results to 2 decimal places and print\n    formatted_output = [f\"{res:.2f}\" for res in final_results]\n    print(f\"[{','.join(formatted_output)}]\")\n\nsolve()\n```", "id": "3265247"}, {"introduction": "牛顿法以其快速的二次收敛而闻名，但此性质仅对单根成立。本练习将探讨重根这一重要情况，在这种情况下，该方法的性能会下降。通过为一个具有三重根的函数实现牛顿法，您将通过数值验证其收敛变为线性的，并且渐近误差率会趋近于理论预测值 $2/3$。这一实践加深了对算法标榜性能背后的理论假设的理解。[@problem_id:3265302]", "problem": "您需要构建并分析一个牛顿法因根的重数而仅表现出线性收敛的案例。请从收敛速度和牛顿法的基础定义出发。您必须实现该算法，并使用精确的数值指标来量化观察到的行为。\n\n用作基本依据的定义：\n- 如果 $f(\\alpha)=0$, $f'(\\alpha)=0$, $\\dots$, $f^{(m-1)}(\\alpha)=0$ 且 $f^{(m)}(\\alpha)\\neq 0$，则点 $\\alpha \\in \\mathbb{R}$ 是函数 $f:\\mathbb{R}\\to\\mathbb{R}$ 的一个 $m \\in \\mathbb{N}$ 重根。\n- 对于可微函数 $f$ 的一个单根，牛顿法的近似迭代定义为 $x_{k+1} = x_k - \\dfrac{f(x_k)}{f'(x_k)}$，其中 $k \\ge 0$。\n- 给定一个收敛到 $\\alpha$ 的序列 $\\{x_k\\}$，定义误差为 $e_k = |x_k - \\alpha|$。如果存在一个常数 $c \\in (0,1)$ 使得 $\\displaystyle \\lim_{k\\to\\infty} \\dfrac{e_{k+1}}{e_k} = c$，则称该方法线性收敛。如果对于某个 $\\lambda \\in (0,\\infty)$，有 $\\displaystyle \\lim_{k\\to\\infty} \\dfrac{e_{k+1}}{e_k^p} = \\lambda$，则称其以 $p0$ 阶收敛。收敛阶数的一个经验估计量为\n$$\np_k = \\dfrac{\\ln\\!\\left(\\dfrac{e_{k+1}}{e_k}\\right)}{\\ln\\!\\left(\\dfrac{e_k}{e_{k-1}}\\right)} \\quad \\text{for } k \\ge 1,\n$$\n其中 $\\ln$ 表示自然对数。\n\n任务：\n1) 提供一个具有3重根的显式函数 $f$。根据上述定义证明其重数。\n2) 对函数 $f$ 实现牛顿法，并针对几个初始猜测值计算误差序列 $e_k$。使用这些误差来估计：\n   - 随着 $k$ 增大的渐近误差比 $r_k = \\dfrac{e_{k+1}}{e_k}$。\n   - 如上定义的经验收敛阶数 $p_k$。\n3) 通过证明 $r_k$ 稳定在 $(0, 1)$ 区间内的一个常数附近，且 $p_k$ 稳定在 1 附近，来数值上证明牛顿法对于三重根仅线性收敛。\n\n为了可复现性和评估，请使用以下参数集测试套件。在每种情况下，都给定了 $f$、$f'$、真根 $\\alpha$、初始猜测值 $x_0$ 和固定的迭代次数 $N$：\n- 测试 A (精确因式分解情况): $f(x) = (x-1)^3$, $f'(x) = 3(x-1)^2$, $\\alpha = 1$, $x_0 = 2$, $N = 6$.\n- 测试 B (非平凡因子情况): $f(x) = (x-1)^3 (x^2+1)$, $f'(x) = 3(x-1)^2 (x^2+1) + (x-1)^3 (2x)$, $\\alpha = 1$, $x_0 = 1.7$, $N = 10$.\n- 测试 C (解析非多项式因子情况): $f(x) = (x+0.5)^3 e^x$, $f'(x) = e^x\\left(3(x+0.5)^2 + (x+0.5)^3\\right)$, $\\alpha = -0.5$, $x_0 = 0$, $N = 12$.\n\n计算和输出要求：\n- 对于每个测试用例，从 $x_0$ 开始精确运行 $N$ 次牛顿法迭代，收集误差序列 $e_k = |x_k - \\alpha|$（其中 $k=0,1,\\dots,N$）。计算最后一个比率 $r_{N-1} = \\dfrac{e_N}{e_{N-1}}$ 和最后一个阶数估计 $p_{N-1} = \\dfrac{\\ln(e_N/e_{N-1})}{\\ln(e_{N-1}/e_{N-2})}$。\n- 您的程序应输出单行，包含一个逗号分隔的扁平列表，其值为 $[r_A, p_A, r_B, p_B, r_C, p_C]$，其中 $r_{\\cdot}$ 和 $p_{\\cdot}$ 分别是每个测试用例的最终比率和最终阶数估计。\n- 将所有数字作为浮点值输出。为了清晰和可复现性，在打印前将每个值四舍五入到6位小数。\n- 不涉及物理单位，也不使用角度。\n\n您的程序必须是一个完整、可运行的程序，能够使用指定的测试套件执行所有计算，并产生所需的单行输出格式：一个单行列表，如 $[r_A,p_A,r_B,p_B,r_C,p_C]$。", "solution": "该问题要求对牛顿法应用于具有大于1重数（特别是3重根）的函数时的情况进行理论和数值分析。我们将首先为预期的线性收敛建立理论基础，然后验证一个示例函数的重数，最后设计并执行一个数值实验，以量化所提供测试用例的收敛行为。\n\n**1. 收敛性的理论分析**\n\n牛顿法是一个由 $x_{k+1} = x_k - \\frac{f(x_k)}{f'(x_k)}$ 定义的迭代格式。这可以表示为不动点迭代 $x_{k+1} = g(x_k)$，其中迭代函数为 $g(x) = x - \\frac{f(x)}{f'(x)}$。在根 $\\alpha$ 附近的收敛行为由 $g(x)$ 在 $\\alpha$ 处的导数决定。如果 $|g'(\\alpha)|  1$，则迭代在局部收敛到 $\\alpha$。如果 $g'(\\alpha) \\neq 0$，收敛速度是线性的；如果 $g'(\\alpha) = 0$，收敛速度至少是二次的。\n\n设 $\\alpha$ 是一个重数为 $m \\in \\mathbb{N}$（其中 $m  1$）的根。根据定义，函数 $f(x)$ 可以写成 $f(x) = (x-\\alpha)^m h(x)$ 的形式，其中 $h(x)$ 是一个满足 $h(\\alpha) \\neq 0$ 的函数。对 $f(x)$ 求导可得：\n$$\nf'(x) = m(x-\\alpha)^{m-1}h(x) + (x-\\alpha)^m h'(x)\n$$\n将这些代入 $g(x)$ 的表达式中：\n$$\ng(x) = x - \\frac{(x-\\alpha)^m h(x)}{m(x-\\alpha)^{m-1}h(x) + (x-\\alpha)^m h'(x)} = x - \\frac{(x-\\alpha)h(x)}{m h(x) + (x-\\alpha)h'(x)}\n$$\n为了找到收敛速度，我们计算 $g'(x)$：\n$$\ng'(x) = 1 - \\frac{[h(x) + (x-\\alpha)h'(x)][m h(x) + (x-\\alpha)h'(x)] - [(x-\\alpha)h(x)][m h'(x) + h'(x) + (x-\\alpha)h''(x)]}{[m h(x) + (x-\\alpha)h'(x)]^2}\n$$\n计算当 $x \\to \\alpha$ 时的极限：\n$$\n\\lim_{x\\to\\alpha} g'(x) = g'(\\alpha) = 1 - \\frac{[h(\\alpha) + 0][m h(\\alpha) + 0] - [0]}{[m h(\\alpha) + 0]^2} = 1 - \\frac{m h(\\alpha)^2}{m^2 h(\\alpha)^2} = 1 - \\frac{1}{m}\n$$\n因此，对于重数 $m  1$ 的根，牛顿法线性收敛，其渐近误差比为 $c = |g'(\\alpha)| = |1 - 1/m|$。因为 $m  1$，所以有 $0  c  1$。\n\n在这个问题中，我们特别关注重数为 $m=3$ 的根。因此，理论上的渐近误差比为：\n$$\nc = 1 - \\frac{1}{3} = \\frac{2}{3}\n$$\n收敛阶数为 $p=1$，表明是线性收敛。\n\n**2. 根的重数证明**\n\n问题要求提供一个具有3重根的函数并加以证明。我们将使用测试 A 中的函数：$f(x) = (x-1)^3$。根为 $\\alpha=1$。根据所给定义，我们必须验证 $f(\\alpha)=f'(\\alpha)=f''(\\alpha)=0$ 且 $f'''(\\alpha)\\neq 0$。\n\n该函数及其逐次导数如下：\n- $f(x) = (x-1)^3$\n- $f'(x) = 3(x-1)^2$\n- $f''(x) = 6(x-1)$\n- $f'''(x) = 6$\n\n在根 $\\alpha=1$ 处计算这些导数的值：\n- $f(1) = (1-1)^3 = 0$\n- $f'(1) = 3(1-1)^2 = 0$\n- $f''(1) = 6(1-1) = 0$\n- $f'''(1) = 6 \\neq 0$\n\n由于在 $\\alpha=1$ 处前两个导数为零，而三阶导数非零，这证实了 $\\alpha=1$ 是 $f(x)$ 的一个3重根。其他给定的函数可以类似地进行验证，因为它们都构造成 $(x-\\alpha)^3 h(x)$ 的形式，且 $h(\\alpha) \\neq 0$。\n\n**3. 数值实验与预期结果**\n\n我们将为三个测试用例（A、B、C）中的每一个实现牛顿法。对于每个用例，我们从一个初始猜测值 $x_0$ 开始，迭代 $N$ 次。过程如下：\n1. 以 $x_k = x_0$ 初始化 $k=0$。用一个数组存储误差 $e_k = |x_k - \\alpha|$（其中 $k=0, 1, \\dots, N$）。\n2. 对于从 $0$ 到 $N-1$ 的 $k$，计算下一个迭代值：$x_{k+1} = x_k - \\frac{f(x_k)}{f'(x_k)}$。\n3. 计算并存储相应的误差 $e_{k+1} = |x_{k+1} - \\alpha|$。\n4. 经过 $N$ 次迭代后，可得到误差序列 $e_0, e_1, \\dots, e_N$。\n5. 计算最终的渐近误差比估计：$r_{N-1} = \\frac{e_N}{e_{N-1}}$。\n6. 计算最终的经验收敛阶数：$p_{N-1} = \\frac{\\ln(e_N/e_{N-1})}{\\ln(e_{N-1}/e_{N-2})}$。\n\n根据我们的理论分析，数值结果应显示，对于每个测试用例，$r_{N-1}$ 的值趋近于理论极限 $2/3 \\approx 0.666667$，而 $p_{N-1}$ 的值趋近于 $1$。这将按要求在数值上证明牛顿法对于三重根表现出线性收敛。最终的实现将执行这些计算并按规定格式化输出。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Constructs and analyzes cases where Newton’s method exhibits linear convergence\n    due to a root of multiplicity 3. The implementation runs three test cases\n    and computes the final asymptotic error ratio and empirical order of convergence.\n    \"\"\"\n    \n    # Test A (exact factorization case)\n    # f(x) = (x-1)^3\n    # f'(x) = 3(x-1)^2\n    # alpha = 1, x_0 = 2, N = 6\n    case_A = {\n        \"f\": lambda x: (x - 1.0)**3,\n        \"fp\": lambda x: 3.0 * (x - 1.0)**2,\n        \"alpha\": 1.0,\n        \"x0\": 2.0,\n        \"N\": 6,\n    }\n\n    # Test B (nontrivial factor case)\n    # f(x) = (x-1)^3 * (x^2+1)\n    # f'(x) = 3(x-1)^2 * (x^2+1) + (x-1)^3 * (2x)\n    # alpha = 1, x_0 = 1.7, N = 10\n    case_B = {\n        \"f\": lambda x: (x - 1.0)**3 * (x**2 + 1.0),\n        \"fp\": lambda x: 3.0 * (x - 1.0)**2 * (x**2 + 1.0) + (x - 1.0)**3 * (2.0 * x),\n        \"alpha\": 1.0,\n        \"x0\": 1.7,\n        \"N\": 10,\n    }\n\n    # Test C (analytic nonpolynomial factor case)\n    # f(x) = (x+0.5)^3 * e^x\n    # f'(x) = e^x * (3(x+0.5)^2 + (x+0.5)^3)\n    # alpha = -0.5, x_0 = 0, N = 12\n    case_C = {\n        \"f\": lambda x: (x + 0.5)**3 * np.exp(x),\n        \"fp\": lambda x: np.exp(x) * (3.0 * (x + 0.5)**2 + (x + 0.5)**3),\n        \"alpha\": -0.5,\n        \"x0\": 0.0,\n        \"N\": 12,\n    }\n\n    test_cases = [case_A, case_B, case_C]\n    results = []\n\n    for case in test_cases:\n        f, fp = case[\"f\"], case[\"fp\"]\n        alpha, x0, N = case[\"alpha\"], case[\"x0\"], case[\"N\"]\n        \n        # Array to store the sequence of errors e_k = |x_k - alpha|\n        # Size is N+1 for e_0, e_1, ..., e_N\n        errors = np.zeros(N + 1, dtype=float)\n        \n        x_k = float(x0)\n        errors[0] = np.abs(x_k - alpha)\n        \n        # Perform N iterations of Newton's method\n        for k in range(N):\n            f_val = f(x_k)\n            fp_val = fp(x_k)\n            \n            # Newton's iteration step\n            # Note: Given problem setup ensures fp_val is not zero away from the root.\n            x_k = x_k - f_val / fp_val\n            \n            # Store the error of the new iterate\n            errors[k + 1] = np.abs(x_k - alpha)\n            \n        # The last three errors needed for calculations are e_{N-2}, e_{N-1}, e_N\n        e_N = errors[N]\n        e_N_minus_1 = errors[N - 1]\n        e_N_minus_2 = errors[N - 2]\n\n        # Calculate the final asymptotic error ratio r_{N-1}\n        # r_{N-1} = e_N / e_{N-1}\n        r_final = e_N / e_N_minus_1\n        \n        # Calculate the final empirical order of convergence p_{N-1}\n        # p_{N-1} = ln(e_N / e_{N-1}) / ln(e_{N-1} / e_{N-2})\n        # numpy.log is the natural logarithm (ln)\n        p_final = np.log(e_N / e_N_minus_1) / np.log(e_N_minus_1 / e_N_minus_2)\n        \n        results.extend([r_final, p_final])\n\n    # Final print statement in the exact required format.\n    # Output is a flat list [r_A, p_A, r_B, p_B, r_C, p_C]\n    # with each value rounded to 6 decimal places.\n    print(f\"[{','.join([f'{val:.6f}' for val in results])}]\")\n\nsolve()\n```", "id": "3265302"}, {"introduction": "试位法（Regula Falsi）通常被认为是二分法的一种巧妙改进，因为它利用函数值来进行更智能的猜测。然而，该方法可能存在一个被称为“停滞”的严重缺陷。在本练习中，您将把该方法应用于一些特定形状的函数，由于这些函数的几何特性，其中一个区间端点几乎不动，导致收敛速度极其缓慢。通过估计收敛常数，您将量化这种停滞现象，并理解算法的性能如何高度依赖于问题的具体情况。[@problem_id:3265240]", "problem": "考虑试位法 (Regula Falsi)，该方法对于一个在区间 $[a,b]$ 上的连续函数 $f$（满足 $f(a)  0  f(b)$），在每次迭代中通过点 $(a,f(a))$ 和 $(b,f(b))$ 的割线与 $x$ 轴的交点来构造一个新的点 $c$，然后替换 $a$ 或 $b$ 中的一个，以保持根区间的条件。设真根为 $r$，并定义与所选近似序列 $\\{x_k\\}$ 相关联的误差序列 $e_k = |x_k - r|$。收敛速度由以下核心定义来刻画：\n- 线性收敛：存在一个常数 $C \\in (0,1)$，使得 $\\lim_{k \\to \\infty} \\frac{e_{k+1}}{e_k} = C$。\n- 超线性收敛：$\\lim_{k \\to \\infty} \\frac{e_{k+1}}{e_k} = 0$。\n- 二次收敛：存在一个常数 $M  0$，使得对于所有足够大的 $k$，$e_{k+1} \\le M e_k^2$。\n\n您的任务是实现试位法 (Regula Falsi)，并在方法因端点函数值高度不平衡而表现出停滞现象的场景下，经验性地估计其渐进线性收敛常数 $C$。具体来说，使用割线交点迭代序列 $\\{c_k\\}$ 作为近似值 $x_k$，使用解析已知的根 $r$ 计算误差序列 $e_k = |c_k - r|$，并通过对迭代历史最后一部分的比率 $\\frac{e_{k+1}}{e_k}$ 进行平均来估计 $C$。估算过程必须是确定性的和可复现的。\n\n使用以下科学上合理的测试套件，每个案例由一个具有已知根的指数函数和一个为引发不同程度停滞而选择的区间定义：\n\n- 测试案例 1 (极端停滞，端点函数值为大的正数):\n  - 函数：$f(x) = e^{x} - (1 + \\delta)$，其中 $\\delta = 10^{-6}$。\n  - 区间：$[a,b] = [0,20]$。\n  - 迭代次数：$N = 5000$。\n  - 真根：$r = \\ln(1+\\delta)$。\n  - 预期定性行为：线性收敛，常数 $C$ 极度接近于 $1$。\n\n- 测试案例 2 (中度停滞，端点函数值为较小的正数):\n  - 函数：$f(x) = e^{x} - (1 + \\delta)$，其中 $\\delta = 10^{-3}$。\n  - 区间：$[a,b] = [0,5]$。\n  - 迭代次数：$N = 50$。\n  - 真根：$r = \\ln(1+\\delta)$。\n  - 预期定性行为：线性收敛，常数 $C$ 明显小于 $1$。\n\n- 测试案例 3 (数值精度边界情况):\n  - 函数：$f(x) = e^{x} - (1 + \\delta)$，其中 $\\delta = 10^{-12}$。\n  - 区间：$[a,b] = [0,20]$。\n  - 迭代次数：$N = 5000$。\n  - 真根：$r = \\ln(1+\\delta)$。\n  - 预期定性行为：由于步长小于机器精度，在双精度浮点分辨率内，经验常数 $C$ 实际上等于 $1$。\n\n实现要求：\n- 使用标准的试位法更新规则。在每次迭代中，计算\n  $$ c = \\frac{a \\, f(b) - b \\, f(a)}{f(b) - f(a)}, $$\n  如果 $f(a) \\cdot f(c)  0$，则用 $c$ 替换 $a$，否则用 $c$ 替换 $b$。累积序列 $\\{c_k\\}$。\n- 对每个测试案例，计算误差序列 $e_k = |c_k - r|$，对于 $e_k  0$ 的 $k$ 形成比率 $\\rho_k = \\frac{e_{k+1}}{e_k}$，并估计 $C$ 为最后 $m$ 个比率的中位数，其中 $m$ 应选择为一个固定的正整数，该数相对于 $N$ 较小，但足够大以平滑波动。\n- 将所有最终答案表示为无量纲的浮点数。不涉及物理单位。\n\n最终输出规格：\n- 您的程序应生成一行输出，其中包含三个测试案例的估计常数，格式为方括号内以逗号分隔的列表，例如 $[C_1,C_2,C_3]$。每个 $C_i$ 必须是浮点数。\n\n覆盖性设计：\n- 该测试套件覆盖了正常路径情况（中度停滞）、极端停滞情况（线性收敛常数 $C$ 非常接近 $1$）以及停滞与浮点精度相互作用的数值边界情况。\n\n您的程序必须是自包含的，并且不得读取任何输入。它必须严格遵守指定的输出格式。", "solution": "试位法是一种迭代求根算法，用于处理在区间 $[a, b]$ 上 $f(a)$ 和 $f(b)$ 异号的连续函数 $f(x)$。在每次迭代 $k$ 中，通过连接点 $(a_k, f(a_k))$ 和 $(b_k, f(b_k))$ 的割线的 $x$ 轴截距来计算根的新近似值 $c_k$。$c_k$ 的公式为：\n$$\nc_k = \\frac{a_k f(b_k) - b_k f(a_k)}{f(b_k) - f(a_k)}\n$$\n然后通过保留 $c_k$ 和前一个端点之一 $a_k$ 或 $b_k$ 来选择下一次迭代的区间 $[a_{k+1}, b_{k+1}]$，以使新区间继续包含根。指定的更新规则是，如果 $f(a_k) \\cdot f(c_k)  0$（即它们同号），则用 $c_k$ 替换 $a_k$，否则用 $c_k$ 替换 $b_k$。\n\n对于函数 $f(x) = e^{x} - (1 + \\delta)$（其中 $\\delta  0$），其二阶导数为 $f''(x) = e^x  0$，这意味着该函数是严格凸的。对于给定的区间 $[a, b]$，其中 $a=0$ 且 $b  0$，我们有 $f(a)  0$ 和 $f(b)  0$。由于凸性，割线始终位于函数图像上方，其 $x$ 轴截距 $c_k$ 将始终位于真根 $r$ 的左侧。这意味着 $f(c_k)  0$。由于初始的 $f(a_0)=f(0)$ 也为负，条件 $f(a_k) \\cdot f(c_k)  0$ 将在每次迭代中都满足。因此，左端点总是被更新，$a_{k+1} = c_k$，而右端点 $b$ 在整个过程中保持不变。这种现象被称为停滞，它将该方法的收敛速度从超线性降级为线性。\n\n收敛速度由误差序列 $e_k = |x_k - r|$ 的行为来表征，其中 $r$ 是真根，$x_k$ 是近似序列。对于本问题，我们使用割线交点，因此 $x_k = c_k$。线性收敛意味着对于较大的 $k$：\n$$\ne_{k+1} \\approx C \\cdot e_k \\quad \\implies \\quad \\frac{e_{k+1}}{e_k} \\approx C\n$$\n其中 $C \\in (0,1)$ 是渐进线性收敛常数。\n\n为了对每个测试案例经验性地估计 $C$，实现了以下算法：\n$1$. 对每个测试案例配置 $(\\delta, [a, b], N)$，运行试位法达指定的迭代次数 $N$。存储迭代序列 $\\{c_k\\}_{k=0}^{N-1}$。\n$2$. 解析地计算真根 $r = \\ln(1 + \\delta)$。\n$3$. 计算误差序列 $e_k = |c_k - r|$，其中 $k = 0, 1, \\dots, N-1$。\n$4$. 对所有 $e_k  0$ 的 $k$ 生成比率序列 $\\rho_k = e_{k+1}/e_k$。这避免了除以零的错误。\n$5$. 常数 $C$ 被估计为最后 $m$ 个计算比率的中位数。使用中位数是为了提供一个稳健的估计，它对迭代序列进入完全渐进状态之前可能发生的瞬态波动不敏感。选择一个固定值 $m=10$，相对于测试案例中的迭代次数来说它很小，但足以平均掉噪声。\n\n此程序应用于三个不同的测试案例，旨在探究该方法行为的不同方面：\n- **案例 1：** 极端停滞 ($b=20, \\delta=10^{-6}$)。相对于 $|f(a)|$ 而言，$f(b)$ 的值很大，导致收敛非常缓慢，预期 $C$ 值极度接近 $1$。\n- **案例 2：** 中度停滞 ($b=5, \\delta=10^{-3}$)。端点不平衡性不太严重，导致更快的收敛（但仍是线性的），常数 $C$ 明显小于 $1$。\n- **案例 3：** 数值精度极限 ($b=20, \\delta=10^{-12}$)。真根 $r \\approx 10^{-12}$ 很小，且收敛常数 $C$ 理论上接近 $1$。迭代值 $a_k$ 的更新步长，近似为 $a_{k+1} \\approx a_k - \\frac{b (a_k-r) f'(r)}{f(b)}$，变得非常小，以至于当 $a_k$ 接近 $r$ 时，其变化量相对于 $a_k$ 本身来说小于机器精度。这导致 $a_k$ 在数值上停滞（$a_{k+1}$ 在计算上与 $a_k$ 无法区分），从而对于大的 $k$ 值，误差 $e_k$ 变为常数。因此，比率 $e_{k+1}/e_k$ 恰好变为 $1$，最终比率的中位数将为 $1$。\n\n实现将系统地为每个案例执行这些步骤，并报告估计的常数 $C$。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements the False Position Method to estimate the linear convergence\n    constant C for three test cases demonstrating stagnation.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test case 1 (extreme stagnation)\n        {'delta': 1e-6, 'interval': [0.0, 20.0], 'N': 5000},\n        # Test case 2 (moderate stagnation)\n        {'delta': 1e-3, 'interval': [0.0, 5.0], 'N': 50},\n        # Test case 3 (numerical precision edge case)\n        {'delta': 1e-12, 'interval': [0.0, 20.0], 'N': 5000},\n    ]\n\n    results = []\n    # m is the number of final ratios to use for the median calculation\n    m = 10 \n\n    for case in test_cases:\n        delta = case['delta']\n        a, b = case['interval']\n        N = case['N']\n\n        # Define the function f(x) for the current case\n        f = lambda x: np.exp(x) - (1.0 + delta)\n\n        # Calculate the true root\n        r = np.log(1.0 + delta)\n\n        c_k_sequence = []\n        \n        # Ensure initial interval is valid\n        f_a = f(a)\n        f_b = f(b)\n        \n        if f_a * f_b = 0:\n            # This should not happen for the given test cases\n            # but is good practice for a general implementation.\n            raise ValueError(\"Function has the same sign at interval endpoints.\")\n\n        # False Position Method iteration\n        for _ in range(N):\n            # The check for (f_b - f_a == 0) is omitted as it is highly\n            # unlikely for these continuous, non-constant functions.\n            c = (a * f_b - b * f_a) / (f_b - f_a)\n            c_k_sequence.append(c)\n            \n            f_c = f(c)\n\n            # Update the interval\n            if f_a * f_c  0:\n                a = c\n                f_a = f_c\n            else:\n                b = c\n                f_b = f_c\n\n        # Post-processing to estimate C\n        c_k = np.array(c_k_sequence, dtype=np.float64)\n        \n        # Compute the error sequence e_k = |c_k - r|\n        e_k = np.abs(c_k - r)\n        \n        # The sequence of ratios rho_k = e_{k+1} / e_k\n        # We must filter out cases where the denominator e_k is zero.\n        denominators = e_k[:-1]\n        numerators = e_k[1:]\n        \n        # Create a mask to select only elements where the denominator is  0\n        mask = denominators  0\n        \n        if np.any(mask):\n            valid_ratios = numerators[mask] / denominators[mask]\n\n            if len(valid_ratios) = m:\n                # Estimate C as the median of the last m ratios\n                C_estimate = np.median(valid_ratios[-m:])\n            elif len(valid_ratios)  0:\n                # If there are fewer than m ratios, use all available ones\n                C_estimate = np.median(valid_ratios)\n            else:\n                # No valid ratios could be computed\n                C_estimate = np.nan # Should not happen in these test cases\n        else:\n            C_estimate = np.nan\n\n        results.append(C_estimate)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```", "id": "3265240"}]}