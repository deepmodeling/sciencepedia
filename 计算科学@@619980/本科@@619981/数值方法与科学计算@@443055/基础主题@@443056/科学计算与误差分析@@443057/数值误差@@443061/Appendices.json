{"hands_on_practices": [{"introduction": "二次方程求根公式是代数学的基石之一，我们早已对其烂熟于心。然而，在有限精度的计算机上，这个我们信赖的公式有时会给出惊人地不准确的结果。本练习 [@problem_id:3258060] 将引导您探索当两个根非常接近时发生的“灾难性相消”现象，并学习如何利用韦达定理重新设计一个数值稳定的算法来获得精确的解。", "problem": "考虑二次方程 $a x^2 + b x + c = 0$，并分析在有限精度算术中计算其实数根时的数值误差。此分析的基础必须从两个事实出发：(i) 二次方程的根 $r_1$ 和 $r_2$ 的韦达定理，即 $r_1 + r_2 = -\\frac{b}{a}$ 和 $r_1 r_2 = \\frac{c}{a}$，以及 (ii) 电气和电子工程师协会 (IEEE) 754 二进制浮点算术中浮点舍入的标准模型，其中基本运算（加法、减法、乘法和除法）的舍入相对误差的绝对值以单位舍入误差 $u$ 为界，其中 $u$ 取决于格式的二进制精度。您的任务是基于这些基础进行推理，研究当 $b^2 \\approx 4 a c$（根几乎相等）时有效位的损失，并设计和实现一种数值稳定的算法来计算根，以在适当时避免分子中的减法相消。\n\n具体来说，请执行以下操作：\n\n1. 分析在有限精度下计算判别式 $\\Delta = b^2 - 4 a c$ 时，当 $b^2$ 和 $4 a c$ 几乎相等时如何产生较大的相对误差，并解释为什么减去几乎相等的量会放大 $\\Delta$ 的相对误差。\n\n2. 仅使用上述基本事实（韦达定理和浮点舍入模型），推导一种避免分子中减法相消的根计算策略。您的推导必须从恒等式 $r_1 + r_2 = -\\frac{b}{a}$ 和 $r_1 r_2 = \\frac{c}{a}$ 出发，不得引入问题陈述中的任何快捷公式。该策略应以避免相消的方式计算一个根，然后使用根之积恒等式获得第二个根。\n\n3. 在一个程序中实现两种方法：\n   - 直接法，使用教科书上的二次公式和双精度算术计算两个根：计算 $\\Delta$，然后从 $-\\frac{b \\pm \\sqrt{\\Delta}}{2 a}$ 计算 $r_1$ 和 $r_2$。\n   - 在步骤 2 中推导出的替代方法，使用双精度算术实现，该方法避免了一个根的减法相消，并使用根之积恒等式来恢复第二个根。\n\n4. 为进行验证，使用以 10 为基数的任意精度算术计算的高精度参考值来近似真实的实数根。为此，请使用 Python 标准库的 decimal 算术，并设置至少 80 位的精度。使用高精度参考值来量化每种双精度方法在两个根上的最大相对误差。对于一对计算出的根 $(\\tilde{r}_1, \\tilde{r}_2)$ 和参考根 $(r_1, r_2)$，将每个根的相对误差定义为 $\\frac{|\\tilde{r}-r|}{\\max(1, |r|)}$，并将该方法的得分定义为两个根误差中的最大值。因为不同方法之间的根排序可能不同，所以应以最小化最大相对误差的方式将计算出的根与参考根进行匹配。\n\n5. 使用以下参数值测试套件，其中涵盖了几乎相等的根、边界条件、大尺度系数和经典的相消场景。每个三元组为 $(a,b,c)$，所有量均为无量纲：\n   - 测试 1：$a = 1$, $b = 2$, $c = 1 - 10^{-15}$。\n   - 测试 2：$a = 1$, $b = 2$, $c = 1 - 10^{-30}$。\n   - 测试 3：$a = 1$, $b = 10^8$, $c = \\frac{b^2}{4}\\left(1 - 10^{-16}\\right)$。\n   - 测试 4：$a = 1$, $b = 10^8$, $c = 1$。\n   - 测试 5：$a = -1$, $b = 2$, $c = -1 + 10^{-15}$。\n\n6. 您的程序应生成单行输出，其中包含每个测试用例（按相同顺序）的一个布尔值，指示替代方法的最大相对误差是否严格小于直接方法。所需的最终输出格式为单行 Python 风格的布尔值列表，例如 `[{\\rm True},{\\rm False},\\dots]`。不得打印任何其他文本。\n\n不涉及物理单位或角度单位。所有误差量均为纯数字，必须以小数形式计算和报告（无百分号）。实现必须是自包含的，不得读取输入或访问任何外部资源。", "solution": "该问题被评估为有效。它在数值分析这一成熟领域具有科学依据，特别是关于浮点算术和算法稳定性。该问题是良构的，提供了所有必要的数据、定义和约束，以获得唯一且可验证的解。语言客观、正式。\n\n任务是分析求解二次方程 $a x^2 + b x + c = 0$ 时的数值误差，并实现和比较两种求其实数根的方法：标准的直接公式和一种从韦达定理推导出的数值稳定的替代方法。\n\n### 第 1 部分：判别式中的数值误差分析\n\nIEEE 754 浮点算术的标准模型指出，对于任何基本算术运算 $\\circ \\in \\{+, -, \\times, \\div\\}$，对两个实数 $x$ 和 $y$ 进行运算的浮点结果由 $fl(x \\circ y) = (x \\circ y)(1 + \\delta)$ 给出，其中相对误差 $\\delta$ 的绝对值以单位舍入误差 $u$ 为界（对于双精度，约等于 $1.11 \\times 10^{-16}$）。\n\n我们要分析当 $b^2 \\approx 4ac$ 时判别式 $\\Delta = b^2 - 4ac$ 的计算。令 $X = b^2$ 和 $Y = 4ac$。在有限精度下，这些量是带有一些误差计算的。令 $\\tilde{X}$ 和 $\\tilde{Y}$ 为它们的浮点表示。我们可以将其建模为：\n$$ \\tilde{X} = fl(b^2) = b^2(1 + \\delta_1) $$\n$$ \\tilde{Y} = fl(4ac) = 4ac(1 + \\delta_2) $$\n其中 $|\\delta_1|$ 和 $|\\delta_2|$ 的量级与 $u$ 相当。然后减法计算为：\n$$ \\tilde{\\Delta} = fl(\\tilde{X} - \\tilde{Y}) = (\\tilde{X} - \\tilde{Y})(1 + \\delta_3) \\quad \\text{其中 } |\\delta_3| \\le u $$\n代入 $\\tilde{X}$ 和 $\\tilde{Y}$ 的表达式：\n$$ \\tilde{\\Delta} = (b^2(1 + \\delta_1) - 4ac(1 + \\delta_2))(1 + \\delta_3) $$\n$$ \\tilde{\\Delta} = (b^2 - 4ac + b^2\\delta_1 - 4ac\\delta_2)(1 + \\delta_3) $$\n忽略高阶误差项，$\\Delta$ 的绝对误差为：\n$$ E_{abs} = \\tilde{\\Delta} - \\Delta \\approx b^2\\delta_1 - 4ac\\delta_2 $$\n相对误差为 $E_{rel} = \\frac{E_{abs}}{\\Delta}$：\n$$ E_{rel} \\approx \\frac{b^2\\delta_1 - 4ac\\delta_2}{b^2 - 4ac} $$\n当 $b^2 \\approx 4ac$ 时，分母 $b^2 - 4ac$ 变得非常小。然而，分子约为 $b^2(\\delta_1 - \\delta_2)$。由于 $\\delta_1$ 和 $\\delta_2$ 是独立的舍入误差，它们的差通常不为零。因此，我们是用一个非常小的量除一个典型大小的误差，这可能导致相对误差 $E_{rel}$ 变得任意大。这种现象被称为减法相消或有效位损失。如果 $b^2$ 和 $4ac$ 非常接近，以至于它们的差小于浮点格式的精度，计算出的判别式 $\\tilde{\\Delta}$ 甚至可能变为零，从而丢失所有关于真实的、小的、非零的差值的信息。\n\n### 第 2 部分：稳定求根算法的推导\n\n标准二次公式提供的根 $r_{1}, r_{2}$ 为：\n$$ r = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a} = \\frac{-b \\pm \\sqrt{\\Delta}}{2a} $$\n当 $\\sqrt{\\Delta}$ 接近 $|b|$ 时，分子 $-b \\pm \\sqrt{\\Delta}$ 中可能会出现另一种数值不稳定性。这种情况发生在 $\\Delta = b^2 - 4ac$ 接近 $b^2$ 时，这意味着 $|4ac| \\ll b^2$。\n\n假设 $b>0$。项 $\\sqrt{\\Delta} = \\sqrt{b^2 - 4ac} = b\\sqrt{1-4ac/b^2} \\approx b(1 - 2ac/b^2) = b - 2ac/b$。在这种情况下，$\\sqrt{\\Delta} \\approx b$。\n- 计算 $-b - \\sqrt{\\Delta}$ 涉及两个负数相加，这在数值上是稳定的。\n- 计算 $-b + \\sqrt{\\Delta}$ 涉及两个几乎相等的正数相减，导致灾难性的减法相消。\n\n如果 $b0$，则 $-b>0$，情况正好相反：$-b + \\sqrt{\\Delta}$ 是稳定的（正数之和），而 $-b - \\sqrt{\\Delta}$ 会遭受相消。\n\n要构建一个稳定的算法，我们必须始终选择分子中构成有效加法的运算（即避免减去符号相同且大小相近的量）。这可以通过选择根号的符号与 $-b$ 的符号相匹配来实现。一种紧凑的写法是使用符号函数 $\\text{sign}(b)$：\n$$ r_1 = \\frac{-b - \\text{sign}(b)\\sqrt{\\Delta}}{2a} $$\n该公式能高精度地计算一个根 $r_1$，而不管 $a$、$b$ 和 $c$ 的值如何（假设 $\\Delta \\ge 0$）。\n\n现在，我们使用韦达定理来找到第二个根 $r_2$。问题强制要求使用这种基础方法。韦达定理为：\n1. $r_1 + r_2 = -b/a$\n2. $r_1 r_2 = c/a$\n\n如果我们使用和关系，我们会得到 $r_2 = -b/a - r_1$。代入我们为 $r_1$ 推导的稳定表达式：\n$$ r_2 = -\\frac{b}{a} - \\frac{-b - \\text{sign}(b)\\sqrt{\\Delta}}{2a} = \\frac{-2b - (-b - \\text{sign}(b)\\sqrt{\\Delta})}{2a} = \\frac{-b + \\text{sign}(b)\\sqrt{\\Delta}}{2a} $$\n这个推导直接回到了我们试图避免的不稳定公式。\n\n因此，我们必须使用乘积关系 $r_1 r_2 = c/a$。这得出：\n$$ r_2 = \\frac{c/a}{r_1} = \\frac{c}{a r_1} $$\n这个计算涉及除以精确计算的根 $r_1$。由于除法在浮点算术中是数值稳定的运算，此方法将为 $r_2$ 得出准确的值。\n\n因此，稳定算法如下：\n1. 计算 $\\Delta = b^2 - 4ac$。\n2. 计算稳定根 $r_1 = \\frac{-b - \\text{sign}(b)\\sqrt{\\Delta}}{2a}$。\n3. 使用韦达乘积法则计算第二个根：$r_2 = \\frac{c}{ar_1}$。\n\n### 第 3 和 4 部分：实现与验证\n\n实现将包括针对每个测试用例 $(a,b,c)$ 的三个组件：\n1.  **高精度参考值**：使用 Python 的 `decimal` 库以 80 位精度近似计算真实根，为误差测量提供基准。\n2.  **直接法**：在标准双精度浮点算术中，使用教科书公式 $r = \\frac{-b \\pm \\sqrt{\\Delta}}{2a}$ 计算两个根。\n3.  **替代方法**：在双精度算术中实现上述推导的稳定算法。\n\n每种方法的性能通过其最大相对误差来评分。对于一个计算出的根 $\\tilde{r}$ 和参考根 $r$，相对误差为 $\\frac{|\\tilde{r}-r|}{\\max(1, |r|)}$。该度量标准能稳健地处理接近于零的根。由于根的排序 $(\\tilde{r}_1, \\tilde{r}_2)$ 可能与参考排序 $(r_1, r_2)$ 不匹配，我们测试两种可能的配对，并选择使该对的最大误差最小化的配对。最后，一个布尔值指示替代方法的分数（最大相对误差）是否严格小于直接方法的分数。程序将为给定的测试套件输出这些布尔值的列表。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom decimal import Decimal, getcontext\n\ndef solve():\n    \"\"\"\n    Computes and compares the numerical accuracy of two methods for solving\n    quadratic equations, producing a boolean list indicating if the alternative\n    method is more accurate than the direct method for a given test suite.\n    \"\"\"\n    # 1. Set up high precision context and test cases\n    getcontext().prec = 80\n\n    # Define test cases for double-precision calculations\n    b3_float = 1e8\n    c3_float = (b3_float**2 / 4.0) * (1.0 - 1e-16)\n    test_cases_float = [\n        (1.0, 2.0, 1.0 - 1e-15),\n        (1.0, 2.0, 1.0 - 1e-30),\n        (1.0, b3_float, c3_float),\n        (1.0, 1e8, 1.0),\n        (-1.0, 2.0, -1.0 + 1e-15),\n    ]\n\n    # Define corresponding test cases for high-precision reference calculations\n    b3_dec = Decimal('1e8')\n    c3_dec = (b3_dec**2 / Decimal(4)) * (Decimal(1) - Decimal('1e-16'))\n    test_cases_dec = [\n        (Decimal('1'), Decimal('2'), Decimal('1') - Decimal('1e-15')),\n        (Decimal('1'), Decimal('2'), Decimal('1') - Decimal('1e-30')),\n        (Decimal('1'), b3_dec, c3_dec),\n        (Decimal('1'), Decimal('1e8'), Decimal('1')),\n        (Decimal('-1'), Decimal('2'), Decimal('-1') + Decimal('1e-15')),\n    ]\n\n    results = []\n\n    for i in range(len(test_cases_float)):\n        a_f, b_f, c_f = test_cases_float[i]\n        a_d, b_d, c_d = test_cases_dec[i]\n\n        # 2. High-precision reference calculation\n        delta_d = b_d**2 - 4 * a_d * c_d\n        if delta_d  0:\n            # This should not occur for the given test cases, as they are\n            # designed to have real roots. If it did happen due to some\n            # unforeseen issue, we treat the roots as coalescing at -b/(2a).\n            sqrt_delta_d = Decimal(0)\n        else:\n            sqrt_delta_d = delta_d.sqrt()\n        \n        ref_r1 = (-b_d + sqrt_delta_d) / (2 * a_d)\n        ref_r2 = (-b_d - sqrt_delta_d) / (2 * a_d)\n        ref_roots = [ref_r1, ref_r2]\n\n        # 3. Direct method (double-precision)\n        delta_f = b_f**2 - 4 * a_f * c_f\n        # Prevent math domain error for sqrt from small negative delta due to rounding\n        if delta_f  0:\n            delta_f = 0.0\n        sqrt_delta_f = np.sqrt(delta_f)\n        \n        direct_r1 = (-b_f + sqrt_delta_f) / (2 * a_f)\n        direct_r2 = (-b_f - sqrt_delta_f) / (2 * a_f)\n        direct_roots = [direct_r1, direct_r2]\n\n        # 4. Alternative stabilized method (double-precision)\n        # The numerator term avoids cancellation by matching the sign of the radical\n        # to the sign of -b. np.copysign(1.0, b_f) gives a 1.0 with the sign of b_f.\n        q = -b_f - np.copysign(1.0, b_f) * sqrt_delta_f\n        \n        # Handle the case where q can be zero (e.g., if b=0 and c=0)\n        if q == 0.0:\n            # Roots are both 0.\n            alt_r1 = 0.0\n            alt_r2 = 0.0\n        else:\n            # First root is calculated from the stable numerator q\n            alt_r1 = q / (2 * a_f)\n            # Second root is from Vieta's product relation: r1*r2 = c/a\n            alt_r2 = c_f / (a_f * alt_r1)\n        alt_roots = [alt_r1, alt_r2]\n\n        # 5. Error calculation and comparison\n        def calculate_max_rel_error(computed_roots, reference_roots):\n            # Using str() when converting float to Decimal is crucial to avoid\n            # representing the float's binary approximation error in the Decimal.\n            r1_comp_d = Decimal(str(computed_roots[0]))\n            r2_comp_d = Decimal(str(computed_roots[1]))\n            r1_ref, r2_ref = reference_roots\n\n            def rel_err(comp, ref):\n                # Use max(1, |ref|) as the denominator to handle roots near zero gracefully\n                denominator = max(Decimal(1), ref.copy_abs())\n                if denominator == 0:\n                    return Decimal(0) if comp == ref else Decimal('inf')\n                return (comp - ref).copy_abs() / denominator\n            \n            # Since root order is arbitrary, check both pairings and take the minimum max error.\n            # Pairing 1: (r1_comp, r1_ref), (r2_comp, r2_ref)\n            err11 = rel_err(r1_comp_d, r1_ref)\n            err22 = rel_err(r2_comp_d, r2_ref)\n            match1_max_err = max(err11, err22)\n            \n            # Pairing 2: (r1_comp, r2_ref), (r2_comp, r1_ref)\n            err12 = rel_err(r1_comp_d, r2_ref)\n            err21 = rel_err(r2_comp_d, r1_ref)\n            match2_max_err = max(err12, err21)\n            \n            return min(match1_max_err, match2_max_err)\n\n        direct_error = calculate_max_rel_error(direct_roots, ref_roots)\n        alt_error = calculate_max_rel_error(alt_roots, ref_roots)\n        \n        results.append(alt_error  direct_error)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3258060"}, {"introduction": "使用泰勒级数是近似计算函数值的经典方法，但更多的项数总能带来更好的精度吗？本练习 [@problem_id:3258054] 将通过计算自然对数函数 $\\ln(1+x)$ 来揭示一个深刻的权衡。您将亲手比较截断误差（源于有限项数的泰勒级数）与舍入误差（尤其是在 $x$ 极小时计算 $1+x$ 导致的灾难性相消）之间的较量，从而理解数值近似中理论与实践的差异。", "problem": "您需要设计并实现一个数值实验，该实验旨在揭示在零附近近似自然对数时产生的截断误差和舍入误差。从泰勒定理和相对误差的定义出发，为函数 $x \\mapsto \\ln(1+x)$ 推导其在点 $x=0$ 处的有限和近似，并通过与高精度参考计算的比较，来量化该近似在浮点运算中的质量。推导过程必须从第一性原理开始，包括某一点的泰勒定理、复合函数导数的形式以及相对误差的定义。数值结果必须由一个如下文指定的完整程序生成。\n\n需要执行和实现的任务：\n- 对于一个给定的小 $|x|$ 实数输入，推导并实现一个以 $x=0$ 为中心、使用恰好 $N$ 项的截断泰勒展开来近似 $\\ln(1+x)$。截断项数 $N$ 由每个测试用例提供，并且必须用作级数中固定的项数；不要用自适应停止代替这个固定的 $N$。\n- 使用一个精确舍入的参考值作为真实值，该参考值基于一个直接计算 $\\ln(1+x)$ 而不是通过独立的浮点运算来构造 $1+x$ 的函数。\n- 对于每个测试用例，相对于相同的参考值计算两种误差度量：\n  1) 截断泰勒近似的相对误差，当 $f_{\\mathrm{ref}}(x) \\neq 0$ 时定义为 $E_{\\mathrm{series}} = \\frac{|\\widehat{f}(x) - f_{\\mathrm{ref}}(x)|}{|f_{\\mathrm{ref}}(x)|}$，当 $f_{\\mathrm{ref}}(x) = 0$ 时定义为绝对误差 $|\\widehat{f}(x) - f_{\\mathrm{ref}}(x)|$。\n  2) 朴素复合 $x \\mapsto \\ln(1+x)$ 的相对误差，其计算方法是先在浮点运算中构造 $1+x$，然后取 $\\ln(\\cdot)$，其定义方式与上述相同。\n- 实现级数求和时，应通过你从第一性原理推导出的代数递推关系，从第 $(k-1)$ 项更新到第 $k$ 项，而不是在每次迭代中独立地重新计算每个幂次和除法。使用一个长度为 $N$ 的固定循环来累加总和。\n\n不涉及角度单位。不涉及物理单位。所有数值输出必须使用标准的浮点表示。\n\n测试套件：\n- 使用以下测试用例列表，每个测试用例指定为一个序对 $(x,N)$，其中 $x \\in \\mathbb{R}$ 且 $N \\in \\mathbb{N}$：\n  1) $(x,N) = (0, 50)$\n  2) $(x,N) = (10^{-8}, 20)$\n  3) $(x,N) = (-10^{-8}, 20)$\n  4) $(x,N) = (10^{-4}, 50)$\n  5) $(x,N) = (-10^{-4}, 50)$\n  6) $(x,N) = (10^{-16}, 5)$\n  7) $(x,N) = (10^{-2}, 100)$\n\n最终输出格式：\n- 你的程序应生成单行输出，其中包含一个长度为 $2m$ 的浮点数列表， $m$ 是测试用例的数量。对于按上述顺序给出的每个测试用例，首先输出 $E_{\\mathrm{series}}$，然后输出上面描述的朴素复合的相对误差。该列表必须以逗号分隔，并用方括号括起来，不含任何额外文本。例如，使用 \"[...]\" 的格式。", "solution": "解决方案的构思如下：\n1. 验证步骤（非常简要，因为问题是有效的）。\n2. $\\ln(1+x)$ 的泰勒级数推导。\n3. 级数项的递推关系推导。\n4. 数值误差来源的讨论：截断误差和舍入误差（包括朴素方法中的灾难性抵消）。\n5. 计算方法的定义：\n    - 使用 `np.log1p` 的参考值。\n    - 使用递推关系的级数近似。\n    - 使用 `np.log(1+x)` 的朴素近似。\n    - 误差计算公式。\n6. 实现计划总结，该计划将理论与最终答案中的代码联系起来。\n\n这看起来是完整且严谨的。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the numerical problem of approximating ln(1+x) and quantifying errors.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (0.0, 50),\n        (1e-8, 20),\n        (-1e-8, 20),\n        (1e-4, 50),\n        (-1e-4, 50),\n        (1e-16, 5),\n        (1e-2, 100)\n    ]\n\n    results = []\n\n    def relative_error(approx, ref):\n        \"\"\"\n        Calculates the relative error, handling the case where the reference is zero.\n        \"\"\"\n        # Using np.abs for robustness with numpy floats.\n        if ref == 0.0:\n            return np.abs(approx - ref)\n        return np.abs(approx - ref) / np.abs(ref)\n\n    for x, N in test_cases:\n        # 1. Compute the high-accuracy reference value using np.log1p.\n        # This function is specifically designed for this case to avoid loss of significance.\n        ref_val = np.log1p(x)\n\n        # 2. Compute the naive approximation by first forming 1+x.\n        # This operation can cause catastrophic cancellation if x is small.\n        naive_approx = np.log(1.0 + x)\n\n        # 3. Compute the truncated Taylor series approximation.\n        # The series is ln(1+x) = sum_{k=1 to inf} [(-1)^(k-1) * x^k / k]\n        # We use a recurrence relation for the terms: T_k = T_{k-1} * (-x * (k-1)/k)\n        if x == 0.0:\n            # For x=0, all terms are zero.\n            series_approx = 0.0\n        else:\n            # First term (k=1): T_1 = x\n            term = float(x)\n            series_approx = term\n            # Loop to add terms from k=2 to N.\n            for k in range(2, N + 1):\n                # Update the term based on the previous term T_{k-1}.\n                term = term * (-x * (k - 1) / k)\n                series_approx += term\n\n        # 4. Calculate the relative errors for both approximations.\n        e_series = relative_error(series_approx, ref_val)\n        e_naive = relative_error(naive_approx, ref_val)\n\n        results.append(e_series)\n        results.append(e_naive)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3258054"}, {"introduction": "在科学计算中，许多序列是通过递推关系定义的，但这看似简单的迭代过程可能暗藏陷阱。本练习 [@problem_id:3258009] 聚焦于球贝塞尔函数的计算，它是一个绝佳的例子，用以说明数值不稳定性是如何在递推中产生的。您将实现一个正向递推算法并观察其结果如何被“寄生”的增长解所破坏，然后通过实现一个巧妙的反向递推算法（米勒算法）来获得稳定且精确的结果，从而深刻体会到计算方向对算法稳定性的决定性影响。", "problem": "考虑第一类球贝塞尔函数，记作 $j_n(x)$，它是球贝塞尔微分方程 $x^2 y'' + 2 x y' + \\left(x^2 - n(n+1)\\right) y = 0$ 的解。与之相关的三项递推关系的两个线性无关解构成一个数值计算上很敏感的组合：任何舍入误差在某个递推方向下都可能激发增长的解（这对于 $j_n$ 而言是非物理的）。根据基本性质，球贝塞尔函数满足三项递推关系 $j_{n+1}(x) = \\frac{2n+1}{x} j_n(x) - j_{n-1}(x)$（对 $n \\ge 1$ 成立），以及闭式表达式 $j_0(x) = \\frac{\\sin(x)}{x}$ 和 $j_1(x) = \\frac{\\sin(x)}{x^2} - \\frac{\\cos(x)}{x}$。角度应以弧度为单位进行解释。所有计算必须使用双精度浮点算术。\n\n你的任务是实现两种算法来计算序列 $\\{j_n(x)\\}_{n=0}^N$：\n- 一种数学上正确但数值不稳定的前向递推算法，从 $j_0(x)$ 和 $j_1(x)$ 开始，使用 $j_{n+1}(x) = \\frac{2n+1}{x} j_n(x) - j_{n-1}(x)$ 计算 $j_{n+1}(x)$，其中 $n = 1, 2, \\ldots, N-1$。\n- 一种数值稳定的逆序计算（Miller 算法），从一个远大于 $N$ 的大索引 $L$ 开始，使用反向递推关系 $j_{n-1}(x) = \\frac{2n+1}{x} j_n(x) - j_{n+1}(x)$ 向下计算。该算法以任意种子值 $j_{L+1}(x) = 0$ 和 $j_L(x) = 1$ 开始（注意总体尺度是任意的），一直计算到 $n = 0$，然后通过一个单一的缩放因子对整个序列进行重新缩放，使得计算出的 $j_0(x)$ 与已知的闭式解 $j_0(x) = \\frac{\\sin(x)}{x}$ 相匹配。对于 $n \\le N$，重新缩放后的值 $j_n(x)$ 构成了稳定序列。\n\n你的推理和实现应基于以下核心定义：\n- 球贝塞尔微分方程 $x^2 y'' + 2 x y' + \\left(x^2 - n(n+1)\\right) y = 0$ 将 $j_n(x)$ 定义为一个保持有界的物理​​解。\n- 三项递推关系 $j_{n+1}(x) = \\frac{2n+1}{x} j_n(x) - j_{n-1}(x)$ 是一个从该微分方程及其解结构推导出来的、经过充分检验的恒等式。\n\n不允许使用超出这些定义范围的任何捷径公式。分析递推关系在前向和后向传播中固有的数值误差，并利用这一点来设计你的稳定算法。通过使用合理的参数大小并明确处理以弧度为单位的角度，确保科学上的真实性。\n\n实现一个程序，对于以下测试套件，计算不稳定的前向递推序列和稳定的后向（Miller）序列在所有阶数 $n \\in \\{0, 1, \\ldots, N\\}$ 上的最大绝对偏差：\n- 测试用例 1：$x = 1.0$ (弧度)，$N = 50$。\n- 测试用例 2：$x = 10.0$ (弧度)，$N = 30$。\n- 测试用例 3：$x = 0.1$ (弧度)，$N = 20$。\n- 测试用例 4：$x = 50.0$ (弧度)，$N = 40$。\n\n对于后向（Miller）计算，在所有测试用例中均使用 $L = N + 60$。每个测试用例的结果必须是单个浮点数，等于 $\\max_{0 \\le n \\le N} \\left| j_n^{\\text{forward}}(x) - j_n^{\\text{backward}}(x) \\right|$，其中 $j_n^{\\text{forward}}(x)$ 是通过从 $j_0(x)$ 和 $j_1(x)$ 开始的前向递推计算的，而 $j_n^{\\text{backward}}(x)$ 是通过 Miller 算法计算并重新缩放以匹配 $j_0(x)$ 的。\n\n你的程序应该生成单行输出，其中包含一个由方括号括起来的、以逗号分隔的结果列表，并按上面列出的测试用例的顺序排列（例如 $[r_1,r_2,r_3,r_4]$）。角度必须以弧度处理，除此角度单位外不涉及其他物理单位。输出是浮点数。", "solution": "该问题陈述已经过验证，被认为是合理的。它的科学基础是关于特殊函数及其数值计算的成熟理论，特别是关于球贝塞尔函数的理论。这个问题是适定的，为两种指定的算法提供了所有必要的定义、递推关系、初始条件和参数。目标明确，语言精确。它代表了数值分析中一个关于递推关系稳定性的经典且非平凡的问题。\n\n问题的核心在于三项递推关系的数值性质：\n$$\nj_{n+1}(x) = \\frac{2n+1}{x} j_n(x) - j_{n-1}(x)\n$$\n这是一个线性二阶齐次差分方程。因此，其通解 $y_n(x)$ 是两个线性无关解的线性组合：$y_n(x) = A f_n(x) + B g_n(x)$。对于球贝塞尔方程，这两个解分别是第一类球贝塞尔函数 $j_n(x)$ 和第二类球贝塞尔函数 $y_n(x)$。\n\n理解数值稳定性的关键在于，当阶数 $n$ 变大时，它们对于固定参数 $x$ 的渐进行为。\n- 所需的解 $j_n(x)$ 是“最小”解或“隐性”解。对于任何固定的 $x$，当 $n \\to \\infty$ 时，它会迅速衰减至零。\n- 另一个解 $y_n(x)$ 是“主导”解。当 $n \\to \\infty$ 时，它会无界增长。\n\n任何数值计算都会引入微小的浮点舍入误差。一个计算出的序列，记作 $\\tilde{j}_n(x)$，将不可避免地成为两个解的组合：\n$$\n\\tilde{j}_n(x) \\approx j_n(x) + \\epsilon y_n(x)\n$$\n其中 $\\epsilon$ 是一个代表初始累积误差的小系数。\n\n**前向递推（不稳定方法）**\n前向递推算法根据前两项 $j_n(x)$ 和 $j_{n-1}(x)$ 来计算 $j_{n+1}(x)$。我们从已知的 $j_0(x)$ 和 $j_1(x)$ 的值开始。\n1.  初始化一个数组来存储序列 $\\{j_n(x)\\}_{n=0}^N$。\n2.  使用提供的闭式表达式计算初始值：\n    $$\n    j_0(x) = \\frac{\\sin(x)}{x}\n    $$\n    $$\n    j_1(x) = \\frac{\\sin(x)}{x^2} - \\frac{\\cos(x)}{x}\n    $$\n    即使是这些初始值也会有微小的舍入误差，这实际上引入了主导解 $y_n(x)$ 的一个微小分量。\n3.  对 $k = 1, 2, \\ldots, N-1$ 迭代应用递推关系 $j_{k+1}(x) = \\frac{2k+1}{x} j_k(x) - j_{k-1}(x)$。\n随着 $n$ 的增加，特别是当 $2n+1  x$ 时，因子 $\\frac{2n+1}{x}$ 会变大，从而放大任何现有的误差。最初很小的分量 $\\epsilon y_n(x)$（它也满足该递推关系）由于 $y_n(x)$ 的主导性质会迅速增长。最终，这个误差项会压倒真实的、衰减的解 $j_n(x)$，导致完全不正确且常常是发散的结果。因此，对于递增的 $n$，此方法在数值上是不稳定的。\n\n**后向递推（Miller 算法 - 稳定方法）**\n前向递推的不稳定性可以通过反向应用递推关系来克服。这就是 Miller 算法的基础。当按 $n$ 向下迭代时，递推关系会自然地抑制主导解 $y_n(x)$ 并增强所需的最小解 $j_n(x)$。\n1.  重写递推关系以求解索引最小的项：\n    $$\n    j_{n-1}(x) = \\frac{2n+1}{x} j_n(x) - j_{n+1}(x)\n    $$\n2.  选择一个远大于所需最大阶数 $N$ 的起始索引 $L$（此处为 $L=N+60$）。在如此大的索引处，$j_L(x)$ 的真实值非常接近于零。\n3.  我们通过设置 $f_{L+1}(x) = 0$ 和 $f_L(x) = 1$（或任何小的、任意的非零常数）来初始化一个未缩放的序列，我们称之为 $f_n$。这些初始条件与真实值不符，但这是该算法的关键。\n4.  对 $n = L, L-1, \\ldots, 1$ 向下迭代递推关系，以计算序列 $\\{f_k(x)\\}_{k=0}^L$。由于我们在稳定方向上进行递推，对于所有 $n \\ll L$，所得序列 $f_n(x)$ 将与真实的最小解 $j_n(x)$ 成比例。即，$f_n(x) \\approx C \\cdot j_n(x)$，其中 $C$ 是某个未知的比例常数。\n5.  为了找到 $C$，我们使用真实序列的一个已知值。最方便的是 $j_0(x)$。我们有计算出的 $f_0(x)$ 和已知的解析值 $j_0^{\\text{true}}(x) = \\frac{\\sin(x)}{x}$。因此，缩放因子为 $S = \\frac{j_0^{\\text{true}}(x)}{f_0(x)}$。\n6.  通过重新缩放所有计算值来获得稳定序列：对于 $n=0, 1, \\ldots, N$，有 $j_n^{\\text{backward}}(x) = S \\cdot f_n(x)$。由于 $j_0^{\\text{backward}}(x)$ 经过重新缩放以匹配真实值，因此在 $n=0$ 处的绝对误差将为零（在机器精度范围内）。\n\n**最终计算**\n目标是通过将前向方法与稳定的后向方法进行比较来量化前向方法的失败程度。对于每个测试用例，我们计算两个序列 $\\{j_n^{\\text{forward}}(x)\\}_{n=0}^N$ 和 $\\{j_n^{\\text{backward}}(x)\\}_{n=0}^N$，然后找出它们在所有计算阶数上的最大绝对偏差：\n$$\n\\text{偏差} = \\max_{0 \\le n \\le N} \\left| j_n^{\\text{forward}}(x) - j_n^{\\text{backward}}(x) \\right|\n$$\n这个值可作为衡量不稳定前向递推所累积误差的指标。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the maximum absolute deviation between unstable forward recurrence\n    and stable backward recurrence (Miller's algorithm) for spherical Bessel\n    functions j_n(x).\n    \"\"\"\n\n    def compute_forward(x, N):\n        \"\"\"\n        Computes the sequence {j_n(x)} for n=0 to N using forward recurrence.\n        This method is numerically unstable for n  x.\n        \"\"\"\n        # All computations use double precision (np.float64) by default.\n        j_fwd = np.zeros(N + 1, dtype=np.float64)\n\n        if x == 0.0:\n            j_fwd[0] = 1.0\n            return j_fwd\n\n        j_fwd[0] = np.sin(x) / x\n        if N  0:\n            j_fwd[1] = (np.sin(x) / x**2) - (np.cos(x) / x)\n\n        for n in range(1, N):\n            term1 = ((2.0 * n + 1.0) / x) * j_fwd[n]\n            term2 = j_fwd[n - 1]\n            # Check for potential overflow before assignment\n            if np.isinf(term1):\n                j_fwd[n + 1:] = np.inf\n                break\n            j_fwd[n + 1] = term1 - term2\n\n        return j_fwd\n\n    def compute_backward(x, N):\n        \"\"\"\n        Computes the sequence {j_n(x)} for n=0 to N using backward recurrence\n        (Miller's algorithm). This method is numerically stable.\n        \"\"\"\n        L = N + 60\n        j_bwd_unscaled = np.zeros(L + 2, dtype=np.float64)\n\n        if x == 0.0:\n            j_bwd = np.zeros(N + 1, dtype=np.float64)\n            j_bwd[0] = 1.0\n            return j_bwd\n\n        # Set initial arbitrary values at a large index L\n        j_bwd_unscaled[L + 1] = 0.0\n        j_bwd_unscaled[L] = 1.0  # An arbitrary small number; 1.0 is fine.\n\n        # Recur downwards from n=L to n=1\n        for n in range(L, 0, -1):\n            term1 = ((2.0 * n + 1.0) / x) * j_bwd_unscaled[n]\n            term2 = j_bwd_unscaled[n + 1]\n            j_bwd_unscaled[n - 1] = term1 - term2\n\n        # Calculate the true j_0(x) for normalization\n        j0_true = np.sin(x) / x\n\n        # The computed sequence is proportional to the true sequence.\n        # Find the scaling factor by comparing the computed j_0 with the true j_0.\n        f0_computed = j_bwd_unscaled[0]\n        \n        # This case is highly unlikely for the given x values.\n        if f0_computed == 0.0:\n            scale_factor = 0.0\n        else:\n            scale_factor = j0_true / f0_computed\n\n        # Rescale the sequence to get the final result\n        j_bwd = j_bwd_unscaled[:N + 1] * scale_factor\n        return j_bwd\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (1.0, 50),\n        (10.0, 30),\n        (0.1, 20),\n        (50.0, 40),\n    ]\n\n    results = []\n    for x_val, N_val in test_cases:\n        j_forward = compute_forward(x_val, N_val)\n        j_backward = compute_backward(x_val, N_val)\n\n        # Compute the maximum absolute deviation between the two sequences\n        deviation = np.max(np.abs(j_forward - j_backward))\n        results.append(deviation)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3258009"}]}