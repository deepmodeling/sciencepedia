{"hands_on_practices": [{"introduction": "在数值线性代数中，Givens旋转是一种用于在矩阵中引入零元素的基本工具，它在QR分解等算法中扮演着核心角色。通过直接从定义出发计算Givens旋转矩阵的范数，我们可以加深对诱导范数定义的理解，特别是其与矩阵列和、行和以及几何变换属性的联系。这个练习将展示，对于一个保持向量长度不变的旋转操作，其诱导2-范数直观地反映了这一几何特性。[@problem_id:3242259]", "problem": "在数值方法和科学计算中的许多数值线性代数算法中，会使用吉文斯旋转来引入零元素，同时保持向量长度不变。设 $n \\geq 2$，且 $G \\in \\mathbb{R}^{n \\times n}$ 是一个吉文斯旋转，它通过平面旋转角 $\\theta \\in \\mathbb{R}$ 作用于坐标 $i$ 和 $j$（其中 $1 \\leq i  j \\leq n$），因此，在由第 $i$ 行、第 $j$ 行和第 $i$ 列、第 $j$ 列构成的 $2 \\times 2$ 子矩阵中，$G$ 的元素为 $G_{ii} = \\cos \\theta$, $G_{ij} = \\sin \\theta$, $G_{ji} = -\\sin \\theta$, $G_{jj} = \\cos \\theta$，而在其他位置，$G$ 等于单位矩阵。仅使用诱导矩阵范数的定义\n$$\n\\|A\\|_{p} = \\sup_{x \\neq 0} \\frac{\\|A x\\|_{p}}{\\|x\\|_{p}},\n$$\n推导 $G$ 的诱导 $1$-范数、 $2$-范数和 $\\infty$-范数，并将其表示为关于 $\\cos \\theta$ 和 $\\sin \\theta$ 的显式表达式。请提供精确的符号表达式，无需进行取整。最终答案必须是一个复合表达式，按 $1$-范数、 $2$-范数、 $\\infty$-范数的顺序列出这三个范数。", "solution": "该问题要求仅使用诱导矩阵范数的定义，推导吉文斯旋转矩阵 $G \\in \\mathbb{R}^{n \\times n}$ 的诱导 $1$-范数、$2$-范数和 $\\infty$-范数。矩阵 $G$ 作用于坐标 $i$ 和 $j$，其中 $1 \\leq i  j \\leq n$。其非平凡元素构成一个位于第 $i$ 行、第 $j$ 行和第 $i$ 列、第 $j$ 列的 $2 \\times 2$ 子矩阵：\n$$\n\\begin{pmatrix} G_{ii}  G_{ij} \\\\ G_{ji}  G_{jj} \\end{pmatrix} = \\begin{pmatrix} \\cos \\theta  \\sin \\theta \\\\ -\\sin \\theta  \\cos \\theta \\end{pmatrix}\n$$\n其中 $\\theta \\in \\mathbb{R}$ 是旋转角。在其他位置，$G$ 是单位矩阵，即当 $k \\notin \\{i, j\\}$ 时 $G_{kk} = 1$，所有其他非对角元素均为 $0$。为方便起见，令 $c = \\cos \\theta$ 和 $s = \\sin \\theta$。\n\n矩阵 $A$ 的诱导 $p$-范数定义为\n$$\n\\|A\\|_{p} = \\sup_{x \\neq 0} \\frac{\\|A x\\|_{p}}{\\|x\\|_{p}} = \\sup_{\\|x\\|_{p}=1} \\|A x\\|_{p}\n$$\n我们将对 $p=1$、$p=2$ 和 $p=\\infty$ 应用此定义。\n\n设 $x \\in \\mathbb{R}^n$ 为任意向量，令 $y=Gx$。$y$ 的分量由下式给出：\n当 $k \\notin \\{i, j\\}$ 时，$y_k = x_k$\n$y_i = c x_i + s x_j$\n$y_j = -s x_i + c x_j$\n\n**诱导 $1$-范数 $\\|G\\|_{1}$ 的推导**\n\n诱导 $1$-范数定义为 $\\|G\\|_{1} = \\sup_{\\|x\\|_{1}=1} \\|Gx\\|_{1}$。\n设 $x \\in \\mathbb{R}^n$ 使得 $\\|x\\|_{1} = \\sum_{k=1}^n |x_k| = 1$。$y=Gx$ 的 $1$-范数是：\n$$\n\\|Gx\\|_{1} = \\sum_{k=1}^n |y_k| = \\sum_{k \\notin \\{i,j\\}} |y_k| + |y_i| + |y_j| = \\sum_{k \\notin \\{i,j\\}} |x_k| + |c x_i + s x_j| + |-s x_i + c x_j|\n$$\n对后两项使用三角不等式：\n$|c x_i + s x_j| \\leq |c||x_i| + |s||x_j|$\n$|-s x_i + c x_j| \\leq |-s||x_i| + |c||x_j| = |s||x_i| + |c||x_j|$\n将这些不等式相加，我们得到：\n$|c x_i + s x_j| + |-s x_i + c x_j| \\leq (|c|+|s|)|x_i| + (|s|+|c|)|x_j| = (|c|+|s|)(|x_i|+|x_j|)$。\n将此代入 $\\|Gx\\|_{1}$ 的表达式中：\n$$\n\\|Gx\\|_{1} \\leq \\sum_{k \\notin \\{i,j\\}} |x_k| + (|c|+|s|)(|x_i|+|x_j|)\n$$\n令 $M = |c|+|s| = |\\cos\\theta| + |\\sin\\theta|$。注意 $M \\geq 1$，因为 $M^2 = (|c|+|s|)^2 = |c|^2+|s|^2+2|c||s| = c^2+s^2+2|cs| = 1+2|\\cos\\theta \\sin\\theta| \\geq 1$。\n令 $\\alpha = |x_i| + |x_j|$。由于 $\\|x\\|_1=1$，我们有 $\\sum_{k \\notin \\{i,j\\}} |x_k| = 1-\\alpha$，其中 $0 \\leq \\alpha \\leq 1$。不等式变为：\n$$\n\\|Gx\\|_{1} \\leq (1-\\alpha) + M\\alpha = 1 + (M-1)\\alpha\n$$\n由于 $M \\geq 1$，当 $\\alpha$ 取最大值时，表达式 $1 + (M-1)\\alpha$ 达到最大值。$\\alpha$ 的最大值为 $1$，这在对于所有 $k \\notin \\{i,j\\}$ 都有 $x_k = 0$ 时发生，即当 $x$ 的支撑集局限于索引 $\\{i, j\\}$ 时。对于此类向量，该不等式意味着 $\\|Gx\\|_{1} \\leq M$。\n这就建立了一个上界：$\\|G\\|_{1} \\leq |\\cos\\theta| + |\\sin\\theta|$。\n\n为了证明能够达到这个上确界，我们必须找到一个向量 $x$，满足 $\\|x\\|_{1}=1$ 且 $\\|Gx\\|_{1} = |\\cos\\theta| + |\\sin\\theta|$。考虑标准基向量 $x = e_i$。那么 $\\|e_i\\|_{1}=1$。向量 $Ge_i$ 对应于 $G$ 的第 $i$ 列，即 $c e_i - s e_j$。\n这个向量的 $1$-范数是：\n$$\n\\|Ge_i\\|_{1} = \\|c e_i - s e_j\\|_{1} = |c| + |-s| = |\\cos\\theta| + |\\sin\\theta|\n$$\n因为我们找到了一个向量 $x$ 使得 $\\|Gx\\|_{1}$ 等于上界，所以上确界必定是这个值。\n$$\n\\|G\\|_{1} = |\\cos\\theta| + |\\sin\\theta|\n$$\n\n**诱导 $\\infty$-范数 $\\|G\\|_{\\infty}$ 的推导**\n\n诱导 $\\infty$-范数定义为 $\\|G\\|_{\\infty} = \\sup_{\\|x\\|_{\\infty}=1} \\|Gx\\|_{\\infty}$。\n设 $x \\in \\mathbb{R}^n$ 使得 $\\|x\\|_{\\infty} = \\max_k |x_k| = 1$。$y=Gx$ 的 $\\infty$-范数是：\n$$\n\\|Gx\\|_{\\infty} = \\max_{k} |y_k| = \\max\\left(\\max_{k \\notin \\{i,j\\}} |x_k|, |c x_i + s x_j|, |-s x_i + c x_j|\\right)\n$$\n由于 $\\|x\\|_{\\infty}=1$，我们知道对所有 $k$ 都有 $|x_k| \\leq 1$。因此，$\\max_{k \\notin \\{i,j\\}} |x_k| \\leq 1$。\n使用三角不等式以及 $|x_i| \\leq 1$ 和 $|x_j| \\leq 1$ 的事实：\n$|c x_i + s x_j| \\leq |c||x_i| + |s||x_j| \\leq |c|(1) + |s|(1) = |\\cos\\theta| + |\\sin\\theta|$。\n$|-s x_i + c x_j| \\leq |s||x_i| + |c||x_j| \\leq |s|(1) + |c|(1) = |\\cos\\theta| + |\\sin\\theta|$。\n因此，我们得到 $\\|Gx\\|_{\\infty}$ 的一个上界：\n$$\n\\|Gx\\|_{\\infty} \\leq \\max(1, |\\cos\\theta| + |\\sin\\theta|)\n$$\n如前所述， $|\\cos\\theta| + |\\sin\\theta| \\geq 1$。因此，上界为 $\\|G\\|_{\\infty} \\leq |\\cos\\theta| + |\\sin\\theta|$。\n\n为了证明能够达到这个界，我们构造一个向量 $x$ 满足 $\\|x\\|_{\\infty}=1$。令当 $k \\notin \\{i,j\\}$ 时 $x_k=0$。我们选择 $x_i$ 和 $x_j$ 来最大化 $|y_i| = |c x_i + s x_j|$。令 $x_i = \\mathrm{sgn}(c)$ 和 $x_j = \\mathrm{sgn}(s)$，其中 $\\mathrm{sgn}$ 是符号函数。由于 $c^2+s^2=1$，它们中至少有一个非零，从而确保 $\\|x\\|_\\infty = \\max(|\\mathrm{sgn}(c)|, |\\mathrm{sgn}(s)|, 0) = 1$。\n对于这个 $x$ 的选择，$y=Gx$ 的第 $i$ 个分量是：\n$$\ny_i = c x_i + s x_j = c(\\mathrm{sgn}(c)) + s(\\mathrm{sgn}(s)) = |c| + |s| = |\\cos\\theta| + |\\sin\\theta|\n$$\n结果向量 $y$ 的 $\\infty$-范数是 $\\|y\\|_{\\infty} = \\max_k |y_k|$。由于 $|y_i|$ 是其中一个分量，所以 $\\|y\\|_{\\infty} \\geq |y_i| = |\\cos\\theta| + |\\sin\\theta|$。\n将此与上界结合，我们必然得到等式。\n$$\n\\|G\\|_{\\infty} = |\\cos\\theta| + |\\sin\\theta|\n$$\n\n**诱导 $2$-范数 $\\|G\\|_{2}$ 的推导**\n\n诱导 $2$-范数定义为 $\\|G\\|_{2} = \\sup_{\\|x\\|_{2}=1} \\|Gx\\|_{2}$。\n设 $x \\in \\mathbb{R}^n$。我们计算 $y=Gx$ 的欧几里得范数的平方：\n$$\n\\|Gx\\|_{2}^2 = \\|y\\|_{2}^2 = \\sum_{k=1}^n y_k^2 = \\sum_{k \\notin \\{i,j\\}} y_k^2 + y_i^2 + y_j^2\n$$\n代入 $y$ 的分量表达式：\n$$\n\\|Gx\\|_{2}^2 = \\sum_{k \\notin \\{i,j\\}} x_k^2 + (c x_i + s x_j)^2 + (-s x_i + c x_j)^2\n$$\n展开平方项：\n$$\n(c x_i + s x_j)^2 = c^2 x_i^2 + 2cs x_i x_j + s^2 x_j^2\n$$\n$$\n(-s x_i + c x_j)^2 = s^2 x_i^2 - 2cs x_i x_j + c^2 x_j^2\n$$\n这两项的和是：\n$$\n(c^2 x_i^2 + 2cs x_i x_j + s^2 x_j^2) + (s^2 x_i^2 - 2cs x_i x_j + c^2 x_j^2) = (c^2+s^2)x_i^2 + (s^2+c^2)x_j^2\n$$\n由于 $c^2+s^2 = \\cos^2\\theta + \\sin^2\\theta = 1$，上式可简化为 $x_i^2+x_j^2$。\n将此代回 $\\|Gx\\|_{2}^2$ 的表达式：\n$$\n\\|Gx\\|_{2}^2 = \\sum_{k \\notin \\{i,j\\}} x_k^2 + (x_i^2 + x_j^2) = \\sum_{k=1}^n x_k^2 = \\|x\\|_{2}^2\n$$\n这表明对于任何向量 $x \\in \\mathbb{R}^n$ 都有 $\\|Gx\\|_{2} = \\|x\\|_{2}$。吉文斯旋转矩阵 $G$ 是关于 $2$-范数的等距变换；它保持向量的长度。\n在诱导 $2$-范数的定义中使用这个性质：\n$$\n\\|G\\|_{2} = \\sup_{x \\neq 0} \\frac{\\|Gx\\|_{2}}{\\|x\\|_{2}} = \\sup_{x \\neq 0} \\frac{\\|x\\|_{2}}{\\|x\\|_{2}} = \\sup_{x \\neq 0} 1 = 1\n$$\n因此，诱导 $2$-范数是 $1$。这也可以写作 $\\cos^2\\theta+\\sin^2\\theta$，但 $1$ 是规范的简化表达式。\n\n总之，诱导范数是：\n$\\|G\\|_{1} = |\\cos\\theta| + |\\sin\\theta|$\n$\\|G\\|_{2} = 1$\n$\\|G\\|_{\\infty} = |\\cos\\theta| + |\\sin\\theta|$", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n|\\cos\\theta| + |\\sin\\theta|  1  |\\cos\\theta| + |\\sin\\theta|\n\\end{pmatrix}\n}\n$$", "id": "3242259"}, {"introduction": "一个矩阵的诱导2-范数告诉我们它对向量的最大“拉伸”程度，但哪些向量被拉伸得最长呢？这个问题引导我们从计算范数的值转向探索实现这一最大拉伸的向量集合的结构。通过分析一个特定对角矩阵，我们将揭示这个集合是一个子空间，其维度与矩阵的最大奇异值（或在对称情况下，最大特征值的绝对值）的重数直接相关。这项练习深化了我们对诱导2-范数、特征值和奇异值之间几何联系的理解。[@problem_id:3242331]", "problem": "设 $A(\\alpha) \\in \\mathbb{R}^{3 \\times 3}$ 是实对角矩阵\n$$\nA(\\alpha) = \\begin{pmatrix}\n\\alpha  0  0 \\\\\n0  \\alpha  0 \\\\\n0  0  1\n\\end{pmatrix}.\n$$\n考虑其诱导算子 $2$-范数 $\\|A(\\alpha)\\|_2$，定义为\n$$\n\\|A(\\alpha)\\|_2 = \\sup_{x \\in \\mathbb{R}^3,\\, x \\neq 0} \\frac{\\|A(\\alpha)x\\|_2}{\\|x\\|_2}.\n$$\n定义集合\n$$\nS(\\alpha) = \\left\\{ x \\in \\mathbb{R}^3 : \\|A(\\alpha)x\\|_2 = \\|A(\\alpha)\\|_2\\,\\|x\\|_2 \\right\\}.\n$$\n从诱导算子 $2$-范数的定义和对称半正定矩阵的性质出发，确定子空间 $S(\\alpha)$ 的维数，作为实参数 $\\alpha$ 的函数。将你的最终答案表示为关于 $\\alpha$ 的单个闭式分段表达式。无需四舍五入。", "solution": "问题要求解给定矩阵 $A(\\alpha)$ 的子空间 $S(\\alpha)$ 的维数，作为实参数 $\\alpha$ 的函数。集合 $S(\\alpha)$ 定义为\n$$\nS(\\alpha) = \\left\\{ x \\in \\mathbb{R}^3 : \\|A(\\alpha)x\\|_2 = \\|A(\\alpha)\\|_2\\,\\|x\\|_2 \\right\\}.\n$$\n根据定义，矩阵 $M \\in \\mathbb{R}^{m \\times n}$ 的诱导算子 $2$-范数由下式给出\n$$\n\\|M\\|_2 = \\sup_{x \\in \\mathbb{R}^n,\\, x \\neq 0} \\frac{\\|Mx\\|_2}{\\|x\\|_2}.\n$$\n将此表达式平方，我们得到\n$$\n\\|M\\|_2^2 = \\sup_{x \\neq 0} \\frac{\\|Mx\\|_2^2}{\\|x\\|_2^2} = \\sup_{x \\neq 0} \\frac{x^T M^T M x}{x^T x}.\n$$\n项 $\\frac{x^T M^T M x}{x^T x}$ 是矩阵 $M^T M$ 的瑞利商。矩阵 $M^T M$ 总是对称半正定的。对称矩阵的瑞利商的一个基本性质是其上确界等于该矩阵的最大特征值。因此，$\\|M\\|_2^2$ 是 $M^T M$ 的最大特征值，而 $\\|M\\|_2$ 是 $M$ 的最大奇异值。\n\n集合 $S(\\alpha)$ 由零向量和所有使上确界达到的非零向量 $x$ 组成。这些恰好是矩阵 $M^T M$ 对应于其最大特征值的特征向量。这些特征向量与零向量的并集构成一个子空间，即对应于 $M^T M$ 最大特征值的特征空间。因此，$S(\\alpha)$ 的维数是矩阵 $A(\\alpha)^T A(\\alpha)$ 最大特征值的几何重数。\n\n给定的矩阵是\n$$\nA(\\alpha) = \\begin{pmatrix}\n\\alpha  0  0 \\\\\n0  \\alpha  0 \\\\\n0  0  1\n\\end{pmatrix}.\n$$\n由于 $A(\\alpha)$ 是一个实对角矩阵，它是对称的，所以 $A(\\alpha)^T = A(\\alpha)$。我们来计算矩阵 $A(\\alpha)^T A(\\alpha)$：\n$$\nA(\\alpha)^T A(\\alpha) = A(\\alpha)^2 = \\begin{pmatrix}\n\\alpha  0  0 \\\\\n0  \\alpha  0 \\\\\n0  0  1\n\\end{pmatrix}\n\\begin{pmatrix}\n\\alpha  0  0 \\\\\n0  \\alpha  0 \\\\\n0  0  1\n\\end{pmatrix}\n= \\begin{pmatrix}\n\\alpha^2  0  0 \\\\\n0  \\alpha^2  0 \\\\\n0  0  1\n\\end{pmatrix}.\n$$\n这个对角矩阵的特征值是其对角线上的元素：$\\alpha^2$（代数重数为 $2$）和 $1$（代数重数为 $1$）。设 $\\lambda_{\\max}$ 是 $A(\\alpha)^T A(\\alpha)$ 的最大特征值。我们有 $\\lambda_{\\max} = \\max(\\alpha^2, 1)$。$S(\\alpha)$ 的维数是与 $\\lambda_{\\max}$ 相关联的特征空间的维数。我们根据参数 $\\alpha$ 的值分三种情况进行分析。\n\n情况 1: $|\\alpha|  1$。\n在这种情况下，$\\alpha^2  1$。最大特征值是 $\\lambda_{\\max} = \\alpha^2$。我们需要求出矩阵 $A(\\alpha)^T A(\\alpha) - \\lambda_{\\max}I$ 的零空间的维数：\n$$\nA(\\alpha)^T A(\\alpha) - \\alpha^2 I = \\begin{pmatrix}\n\\alpha^2  0  0 \\\\\n0  \\alpha^2  0 \\\\\n0  0  1\n\\end{pmatrix} - \\begin{pmatrix}\n\\alpha^2  0  0 \\\\\n0  \\alpha^2  0 \\\\\n0  0  \\alpha^2\n\\end{pmatrix} = \\begin{pmatrix}\n0  0  0 \\\\\n0  0  0 \\\\\n0  0  1 - \\alpha^2\n\\end{pmatrix}.\n$$\n特征空间是满足 $(A(\\alpha)^T A(\\alpha) - \\alpha^2 I)x = 0$ 的向量 $x = (x_1, x_2, x_3)^T$ 的集合。这给出了方程 $(1 - \\alpha^2)x_3 = 0$。因为 $\\alpha^2 \\neq 1$，我们必须有 $x_3 = 0$。$x_1$ 和 $x_2$ 没有约束。因此，该特征空间由向量 $(1, 0, 0)^T$ 和 $(0, 1, 0)^T$ 张成。这个子空间的维数是 $2$。\n所以，对于 $|\\alpha|  1$，$\\dim(S(\\alpha)) = 2$。\n\n情况 2: $|\\alpha|  1$。\n在这种情况下，$\\alpha^2  1$。最大特征值是 $\\lambda_{\\max} = 1$。我们求出矩阵 $A(\\alpha)^T A(\\alpha) - \\lambda_{\\max}I$ 的零空间的维数：\n$$\nA(\\alpha)^T A(\\alpha) - 1 \\cdot I = \\begin{pmatrix}\n\\alpha^2  0  0 \\\\\n0  \\alpha^2  0 \\\\\n0  0  1\n\\end{pmatrix} - \\begin{pmatrix}\n1  0  0 \\\\\n0  1  0 \\\\\n0  0  1\n\\end{pmatrix} = \\begin{pmatrix}\n\\alpha^2 - 1  0  0 \\\\\n0  \\alpha^2 - 1  0 \\\\\n0  0  0\n\\end{pmatrix}.\n$$\n特征空间是满足 $(A(\\alpha)^T A(\\alpha) - I)x = 0$ 的向量 $x = (x_1, x_2, x_3)^T$ 的集合。这给出了方程 $(\\alpha^2 - 1)x_1 = 0$ 和 $(\\alpha^2 - 1)x_2 = 0$。因为 $\\alpha^2 \\neq 1$，我们必须有 $x_1 = 0$ 和 $x_2 = 0$。$x_3$ 没有约束。该特征空间由向量 $(0, 0, 1)^T$ 张成。这个子空间的维数是 $1$。\n所以，对于 $|\\alpha|  1$，$\\dim(S(\\alpha)) = 1$。\n\n情况 3: $|\\alpha| = 1$。\n在这种情况下，$\\alpha^2 = 1$。$A(\\alpha)^T A(\\alpha)$ 的特征值为 $1$ (重数为 $2$) 和 $1$。因此，只有一个特征值 $\\lambda = 1$，其代数重数为 $3$。最大特征值是 $\\lambda_{\\max} = 1$。我们求出矩阵 $A(\\alpha)^T A(\\alpha) - \\lambda_{\\max}I$ 的零空间的维数：\n$$\nA(\\alpha)^T A(\\alpha) - 1 \\cdot I = \\begin{pmatrix}\n1  0  0 \\\\\n0  1  0 \\\\\n0  0  1\n\\end{pmatrix} - \\begin{pmatrix}\n1  0  0 \\\\\n0  1  0 \\\\\n0  0  1\n\\end{pmatrix} = \\begin{pmatrix}\n0  0  0 \\\\\n0  0  0 \\\\\n0  0  0\n\\end{pmatrix}.\n$$\n方程 $(A(\\alpha)^T A(\\alpha) - I)x = 0$ 变为 $0 \\cdot x = 0$，这对向量 $x \\in \\mathbb{R}^3$ 不施加任何限制。因此，特征空间是整个空间 $\\mathbb{R}^3$。这个子空间的维数是 $3$。\n所以，对于 $|\\alpha| = 1$，$\\dim(S(\\alpha)) = 3$。\n\n综合所有情况，我们将 $S(\\alpha)$ 的维数表示为 $\\alpha$ 的分段函数：\n$$\n\\dim(S(\\alpha)) = \\begin{cases}\n1  \\text{若 } |\\alpha|  1 \\\\\n3  \\text{若 } |\\alpha| = 1 \\\\\n2  \\text{若 } |\\alpha|  1\n\\end{cases}\n$$\n该函数给出了对于任何实数 $\\alpha$ 值，子空间 $S(\\alpha)$ 的维数。", "answer": "$$\n\\boxed{\n\\begin{cases}\n1  \\text{若 } |\\alpha|  1 \\\\\n3  \\text{若 } |\\alpha| = 1 \\\\\n2  \\text{若 } |\\alpha|  1\n\\end{cases}\n}\n$$", "id": "3242331"}, {"introduction": "理论告诉我们，矩阵的诱导2-范数由 $A^\\top A$ 的最大特征值决定，而实现范数的向量是对应的特征向量。然而，在实际计算中，我们如何找到这个向量呢？这个实践任务将理论与算法联系起来，要求我们设计并实现幂迭代法，这是一种经典的数值算法，用于逼近矩阵的主特征向量。通过这个练习，你将亲手构建一个工具来近似计算诱导2-范数，并验证其在寻找“最大拉伸”方向上的有效性。[@problem_id:3242276]", "problem": "给定一个实数矩阵 $A \\in \\mathbb{R}^{m \\times n}$ 和一个容差参数 $\\epsilon \\in (0,1)$。诱导（算子）矩阵范数 $\\|A\\|_2$ 定义为在所有非零向量 $x \\in \\mathbb{R}^n$ 上比率 $\\|A x\\|_2 / \\|x\\|_2$ 的上确界。任务是设计并实现一个基于幂方法的算法，该算法为每个给定的测试用例计算一个向量 $x \\in \\mathbb{R}^n$，使得不等式\n$$\n\\|A x\\|_2 \\ge (1 - \\epsilon)\\, \\|A\\|_2 \\, \\|x\\|_2\n$$\n成立。您的算法必须从数值方法和科学计算中适用于诱导（算子）矩阵范数的一个基本原理出发：即诱导 $2$-范数的定义，以及 $\\|A\\|_2 = \\sqrt{\\lambda_{\\max}(A^\\top A)}$ 这个经过充分验证的事实，其中 $\\lambda_{\\max}(A^\\top A)$ 是对称半正定矩阵 $A^\\top A$ 的最大特征值。由此，推导为什么近似达到 $\\|A\\|_2$ 的向量 $x$ 可以通过 $A^\\top A$ 的主特征向量来近似，以及幂方法如何应用于 $A^\\top A$ 以产生这样的向量。\n\n您的实现必须遵守以下原则和要求：\n- 从诱导 $2$-范数的核心定义出发：$\\|A\\|_2 = \\sup_{x \\ne 0} \\|A x\\|_2 / \\|x\\|_2$，以及 $A^\\top A$ 是对称半正定矩阵的观察，因此其最大特征值 $\\lambda_{\\max}$ 满足 $\\|A\\|_2 = \\sqrt{\\lambda_{\\max}}$。\n- 对 $B = A^\\top A$ 使用幂方法来近似其主特征向量 $v \\in \\mathbb{R}^n$。初始化一个非零向量 $v_0$ 并迭代 $v_{k+1} = B v_k / \\|B v_k\\|_2$，直到一个有原则的停止准则成立。从第一性原理出发证明停止准则的合理性（例如，根据瑞利商 $q(v) = \\frac{v^\\top B v}{v^\\top v}$ 和残差 $\\|B v - q(v) v\\|_2$）。\n- 定义 $x$ 为幂方法最终的归一化迭代向量，并数值上验证不等式 $\\|A x\\|_2 \\ge (1 - \\epsilon)\\, \\|A\\|_2 \\, \\|x\\|_2$。仅为验证目的，您可以使用奇异值分解（SVD）来计算 $\\|A\\|_2$，奇异值分解（SVD）返回的奇异值中的最大值即等于 $\\|A\\|_2$。\n- 如果 $A$ 是零矩阵（此时 $\\|A\\|_2 = 0$），请显式处理此边界情况，并解释为什么任何向量 $x$ 都满足该不等式。\n\n需要包含并证明其合理性的算法设计细节：\n- 对 $v_0$ 进行确定性初始化或可复现的随机初始化。\n- 从对称半正定矩阵的性质推导出的基于残差的终止条件：当残差 $\\|B v - q(v) v\\|_2$ 低于根据 $B$ 的尺度指定的容差时，停止幂迭代。\n- 设置最大迭代次数以保证终止。\n- 处理秩亏矩阵和具有聚集主特征值的矩阵的策略。\n\n测试套件：\n使用以下测试用例，每个用例由 $(A, \\epsilon)$ 指定，包含明确的数值矩阵和容差。矩阵以标准的行主序给出，所有数值均为实数。\n\n- 测试用例 1（理想情况，矩形，混合条目）：\n  $$\n  A_1 = \\begin{bmatrix}\n  2.0  -1.0  0.0 \\\\\n  0.5  3.0  1.0 \\\\\n  -1.0  0.0  4.0 \\\\\n  3.0  -2.0  1.5\n  \\end{bmatrix},\\quad \\epsilon_1 = 10^{-6}.\n  $$\n- 测试用例 2（边界情况，缩放的单位矩阵，对任意 $x$ 都能精确达到范数）：\n  $$\n  A_2 = 2.5\\, I_4,\\quad \\epsilon_2 = 10^{-12}.\n  $$\n- 测试用例 3（秩亏，矩形）：\n  $$\n  A_3 = \\begin{bmatrix}\n  3.0  0.0  0.0 \\\\\n  0.0  0.0  0.0\n  \\end{bmatrix},\\quad \\epsilon_3 = 10^{-8}.\n  $$\n- 测试用例 4（病态，聚集的奇异值）：\n  $$\n  A_4 = \\operatorname{diag}\\!\\big(1.0,\\, 0.9999,\\, 0.9998,\\, 0.2,\\, 0.1\\big),\\quad \\epsilon_4 = 10^{-5}.\n  $$\n- 测试用例 5（零矩阵，边界情况）：\n  $$\n  A_5 = 0_{3 \\times 4},\\quad \\epsilon_5 = 0.25.\n  $$\n\n实现参数：\n- 对幂方法使用 $50000$ 的最大迭代次数和 $10^{-12}$ 的残差容差。\n- 在每一步对迭代向量进行归一化以避免数值溢出。\n- 使用固定的随机种子进行初始化以确保可复现性。\n\n最终输出格式：\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果。对于每个测试用例，输出一个布尔值，指示您的算法生成的向量 $x$ 是否满足不等式 $\\|A x\\|_2 \\ge (1 - \\epsilon)\\, \\|A\\|_2 \\, \\|x\\|_2$。最终输出必须是\n$$\n[\\text{result}_1,\\text{result}_2,\\text{result}_3,\\text{result}_4,\\text{result}_5],\n$$\n其中每个 $\\text{result}_i$ 是 $\\text{True}$ 或 $\\text{False}$。", "solution": "该问题要求设计并实现一个基于幂方法的算法，为给定的实数矩阵 $A \\in \\mathbb{R}^{m \\times n}$ 和容差 $\\epsilon \\in (0,1)$，找到一个向量 $x \\in \\mathbb{R}^n$ 满足不等式 $\\|A x\\|_2 \\ge (1 - \\epsilon)\\, \\|A\\|_2 \\, \\|x\\|_2$。解决方案必须从第一性原理推导得出。\n\n首先，我们分析问题陈述。矩阵 A 的诱导 $2$-范数定义为：\n$$\n\\|A\\|_2 = \\sup_{x \\in \\mathbb{R}^n, x \\neq 0} \\frac{\\|A x\\|_2}{\\|x\\|_2}\n$$\n由于该表达式对 $x$ 是齐次的，我们可以将搜索空间限制在单位范数向量上，即 $\\|x\\|_2=1$。\n$$\n\\|A\\|_2 = \\sup_{\\|x\\|_2=1} \\|A x\\|_2\n$$\n最大化此数量的向量 $x$ 就是我们试图近似的向量。为了便于分析，我们使用范数的平方：\n$$\n\\|A\\|_2^2 = \\sup_{\\|x\\|_2=1} \\|A x\\|_2^2\n$$\n欧几里得范数的平方可以表示为点积：$\\|v\\|_2^2 = v^\\top v$。因此，\n$$\n\\|A x\\|_2^2 = (A x)^\\top (A x) = x^\\top A^\\top A x\n$$\n让我们定义矩阵 $B = A^\\top A$。矩阵 $B$ 是一个大小为 $n \\times n$ 的方阵。它是对称的，因为 $B^\\top = (A^\\top A)^\\top = A^\\top (A^\\top)^\\top = A^\\top A = B$。此外，$B$ 是半正定的，因为对于任何向量 $x \\in \\mathbb{R}^n$，二次型 $x^\\top B x = x^\\top A^\\top A x = \\|A x\\|_2^2 \\ge 0$。\n\n寻找 $\\|A\\|_2^2$ 的问题现在转化为在单位球面上寻找二次型的最大值：\n$$\n\\|A\\|_2^2 = \\sup_{\\|x\\|_2=1} x^\\top B x\n$$\n对于对称矩阵 $B$，表达式 $x^\\top B x$ 被称为瑞利商（对于单位向量 $x$）。线性代数中的一个基本结果，即瑞利-里兹定理（Rayleigh-Ritz theorem），指出瑞利商的上确界是矩阵 $B$ 的最大特征值。设 $B$ 的特征值为 $\\lambda_1 \\ge \\lambda_2 \\ge \\dots \\ge \\lambda_n \\ge 0$。那么：\n$$\n\\lambda_{\\max}(B) = \\lambda_1 = \\sup_{\\|x\\|_2=1} x^\\top B x = \\|A\\|_2^2\n$$\n当 $x$ 是对应于最大特征值 $\\lambda_1$ 的特征向量时，达到此上确界。这个主特征向量正是使比率 $\\|Ax\\|_2 / \\|x\\|_2$ 最大的向量。因此，我们的目标是找到 $B = A^\\top A$ 的主特征向量的一个良好近似。\n\n幂方法是一种经典的迭代算法，用于近似主特征向量及其对应的特征值。对于矩阵 $B$，该算法过程如下：\n1. 选择一个初始非零向量 $v_0 \\in \\mathbb{R}^n$。为确保可复现性，将使用固定的随机种子。选择一个随机向量是为了最小化其与主特征空间正交的概率。\n2. 对 $k = 0, 1, 2, \\dots$ 进行迭代：\n   $$\n   v_{k+1} = \\frac{B v_k}{\\|B v_k\\|_2}\n   $$\n每次迭代的归一化步骤至关重要，以防止向量的幅度增长或消失，这可能导致数值上溢或下溢。如果 $B$ 有一个唯一的主特征值（即 $\\lambda_1  \\lambda_2$），则向量序列 $\\{v_k\\}$ 收敛到与 $\\lambda_1$ 相关联的归一化特征向量。\n\n需要一个有原则的停止准则来终止迭代。如果一个迭代向量 $v_k$ 近似满足特征向量方程 $B v_k = \\lambda v_k$，那么它就是特征向量的一个良好近似。近似的质量可以通过残差向量 $r_k = B v_k - q(v_k) v_k$ 的范数来衡量，其中 $q(v_k)$ 是瑞利商 $q(v_k) = \\frac{v_k^\\top B v_k}{v_k^\\top v_k}$。由于我们的迭代向量是归一化的（$v_k^\\top v_k = 1$），这简化为 $q(v_k) = v_k^\\top B v_k$。当残差范数 $\\|r_k\\|_2$ 低于指定的容差 $\\tau = 10^{-12}$ 时，迭代将停止。还设置了最大迭代次数 $50000$ 以保证终止，这对于收敛缓慢的序列（例如当主特征值聚集时，$\\lambda_1 \\approx \\lambda_2$）很重要。\n\n最终的迭代向量，我们称之为 $x$，就是我们对 $B$ 的主特征向量的近似。然后我们验证它是否满足所需的不等式：\n$$\n\\|A x\\|_2 \\ge (1 - \\epsilon)\\, \\|A\\|_2 \\, \\|x\\|_2\n$$\n为了验证，$\\|A\\|_2$ 的精确值是使用 $A$ 的奇异值分解 (SVD) 计算的，因为 $\\|A\\|_2$ 等于 $A$ 的最大奇异值 $\\sigma_{\\max}(A)$。请注意，$A$ 的奇异值是 $A^\\top A$ 特征值的平方根。\n\n一个特殊情况是当 $A$ 为零矩阵 $A = 0_{m \\times n}$ 时。在这种情况下，$\\|A\\|_2 = 0$。不等式变为 $\\|0 \\cdot x\\|_2 \\ge (1 - \\epsilon) \\cdot 0 \\cdot \\|x\\|_2$，简化为 $0 \\ge 0$。这对任何向量 $x \\in \\mathbb{R}^n$ 都成立。我们的算法必须显式处理这种情况，因为 $B = A^\\top A$ 也将是零矩阵，幂方法迭代 $B v_k / \\|B v_k\\|_2$ 将涉及除以零。对于这种情况，可以选择任何非零向量 $x$ 作为解。\n\n总而言之，算法如下：\n1. 给定 $A \\in \\mathbb{R}^{m \\times n}$。\n2. 如果 $A$ 是零矩阵，返回一个非零向量（例如，一个全一向量）作为 $x$。\n3. 否则，计算 $B = A^\\top A$。\n4. 初始化一个随机单位向量 $v_0$。\n5. 迭代 $v_{k+1} = B v_k / \\|B v_k\\|_2$ 直到残差范数 $\\|B v_{k+1} - (v_{k+1}^\\top B v_{k+1}) v_{k+1}\\|_2$ 小于 $10^{-12}$ 或达到最大迭代次数 $50000$ 次。\n6. 将 $x$ 设置为最终收敛的向量。\n7. 然后，实现将数值验证此 $x$ 是否对给定的 $\\epsilon$ 满足问题中的不等式。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    为给定的测试套件解决问题。\n    对于每个测试用例 (A, epsilon)，它在 A.T @ A 上使用幂方法计算向量 x，\n    并验证是否满足 ||A x||_2 = (1 - epsilon) * ||A||_2 * ||x||_2。\n    \"\"\"\n\n    def compute_x_vector(A: np.ndarray, resid_tol: float = 1e-12, max_iter: int = 50000) - np.ndarray:\n        \"\"\"\n        计算近似于 A 的主右奇异向量的向量 x。\n        这是通过对矩阵 B = A.T @ A 应用幂方法来实现的。\n\n        Args:\n            A: 输入矩阵，形状为 (m, n)。\n            resid_tol: 用于停止迭代的残差范数容差。\n            max_iter: 最大迭代次数。\n\n        Returns:\n            计算出的向量 x，形状为 (n,)。\n        \"\"\"\n        m, n = A.shape\n        if n == 0:\n            return np.array([])\n        \n        # Handle the boundary case where A is the zero matrix.\n        if np.allclose(A, 0):\n            # Any non-zero vector x satisfies the inequality 0 = 0.\n            # We return a vector of ones for consistency.\n            return np.ones(n)\n\n        B = A.T @ A\n\n        # Initialize with a reproducible random vector to avoid starting\n        # in a subspace orthogonal to the dominant eigenvector.\n        rng = np.random.default_rng(seed=42)\n        v = rng.standard_normal(n)\n        if np.linalg.norm(v) == 0: # Extremely unlikely but possible\n             v = np.ones(n)\n        v = v / np.linalg.norm(v)\n\n        for _ in range(max_iter):\n            Bv = B @ v\n            norm_Bv = np.linalg.norm(Bv)\n            \n            # If Bv is zero, it means v is in the null space of B.\n            # This can happen if A is rank-deficient and our random vector\n            # landed in the null space. We are looking for the dominant\n            # eigenvector, so we should stop or re-initialize.\n            # Here, we assume the initial vector is not in the null space.\n            if norm_Bv == 0:\n                break # Converged to an eigenvector with eigenvalue 0.\n            \n            v_next = Bv / norm_Bv\n\n            # Stopping criterion based on the residual norm.\n            # q is the Rayleigh quotient v_next.T @ B @ v_next\n            # Since B @ v_next is parallel to B @ (B @ v), we can simplify.\n            # q = v_next.T @ (B @ v_next)\n            q = np.dot(v_next, B @ v_next)\n            residual_norm = np.linalg.norm(B @ v_next - q * v_next)\n\n            v = v_next\n\n            if residual_norm  resid_tol:\n                break\n        \n        return v\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (np.array([\n            [2.0, -1.0, 0.0],\n            [0.5, 3.0, 1.0],\n            [-1.0, 0.0, 4.0],\n            [3.0, -2.0, 1.5]\n        ]), 1e-6),\n        (2.5 * np.eye(4), 1e-12),\n        (np.array([\n            [3.0, 0.0, 0.0],\n            [0.0, 0.0, 0.0]\n        ]), 1e-8),\n        (np.diag([1.0, 0.9999, 0.9998, 0.2, 0.1]), 1e-5),\n        (np.zeros((3, 4)), 0.25)\n    ]\n\n    results = []\n    for A, epsilon in test_cases:\n        # 1. Compute the vector x using the power method.\n        # This is the core algorithm being tested.\n        x = compute_x_vector(A)\n\n        # 2. Verify the inequality ||A x||_2 = (1 - epsilon) * ||A||_2 * ||x||_2.\n        # This part uses a reliable method (SVD) to get the true ||A||_2 for verification.\n        if x.size == 0: # Handle zero-column matrices\n            results.append(True) # Inequality typically 0 = 0\n            continue\n\n        lhs = np.linalg.norm(A @ x)\n\n        singular_values = np.linalg.svd(A, compute_uv=False)\n        # ||A||_2 is the largest singular value. Handle case where A has no singular values (e.g., empty).\n        norm_A2 = singular_values[0] if singular_values.size > 0 else 0.0\n        \n        norm_x = np.linalg.norm(x)\n        # Avoid division by zero if x is the zero vector, although compute_x_vector should not return it for non-zero A.\n        if norm_x == 0:\n            # If x is 0, lhs is 0. If norm_A2 is 0, rhs is 0. If norm_A2 is 0, rhs is 0.\n            # So 0 = 0 holds.\n            is_satisfied = True\n        else:\n            rhs = (1.0 - epsilon) * norm_A2 * norm_x\n            is_satisfied = lhs >= rhs\n\n        results.append(is_satisfied)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3242276"}]}