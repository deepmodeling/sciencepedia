{"hands_on_practices": [{"introduction": "要真正掌握像增广拉格朗日方法这样的迭代算法，最好的方法是亲手完整地走一遍计算流程。这个练习 [@problem_id:2208360] 为一个简单的一维问题提供了这样的机会，让你能够专注于算法的两个核心步骤：最小化增广拉格朗日函数和更新拉格朗日乘子。通过这个基础练习，你将对算法的运作机制建立起直观且扎实的理解。", "problem": "考虑在等式约束 $h(x) = x - 3 = 0$ 条件下最小化目标函数 $f(x) = x^2$ 的约束优化问题。\n\n增广拉格朗日方法是解决此类问题的一种迭代算法。增广拉格朗日函数定义为：\n$$L_A(x, \\lambda; \\rho) = f(x) - \\lambda h(x) + \\frac{\\rho}{2} [h(x)]^2$$\n其中 $\\lambda$ 是拉格朗日乘子估计值，$\\rho > 0$ 是惩罚参数。\n\n从估计值 $\\lambda_k$ 开始，该方法的一次迭代包括两个主要步骤：\n1.  通过求解无约束最小化问题来找到下一个迭代点 $x_{k+1}$：\n    $$x_{k+1} = \\arg\\min_{x} L_A(x, \\lambda_k; \\rho)$$\n2.  使用以下公式更新拉格朗日乘子：\n    $$\\lambda_{k+1} = \\lambda_k - \\rho h(x_{k+1})$$\n\n使用初始乘子估计值 $\\lambda_0 = 1$ 和惩罚参数 $\\rho = 2$，执行一次完整的增广拉格朗日方法迭代。确定新迭代点 $x_1$ 和更新后的乘子 $\\lambda_1$ 的值。\n\n答案以行矩阵 $\\begin{pmatrix} x_1  \\lambda_1 \\end{pmatrix}$ 的形式表示，并使用精确分数。", "solution": "给定 $f(x) = x^{2}$ 和 $h(x) = x - 3$，其增广拉格朗日函数为\n$$L_{A}(x,\\lambda;\\rho) = f(x) - \\lambda h(x) + \\frac{\\rho}{2}[h(x)]^{2}.$$\n当 $\\lambda_{0} = 1$ 且 $\\rho = 2$ 时，该函数变为\n$$L_{A}(x,1;2) = x^{2} - 1\\cdot(x-3) + \\frac{2}{2}(x-3)^{2} = x^{2} - x + 3 + (x-3)^{2}.$$\n为求得 $x_{1}$，求解无约束最小化问题：\n$$x_{1} = \\arg\\min_{x} L_{A}(x,1;2).$$\n对函数求导，并令其导数为零：\n$$\\frac{d}{dx}L_{A}(x,1;2) = 2x - 1 + 2(x-3) = 4x - 7,$$\n$$4x - 7 = 0 \\implies x_{1} = \\frac{7}{4}.$$\n二阶导数为\n$$\\frac{d^{2}}{dx^{2}}L_{A}(x,1;2) = 4 > 0,$$\n所以 $x_{1} = \\frac{7}{4}$ 是唯一的极小值点。\n\n使用 $\\lambda_{1} = \\lambda_{0} - \\rho h(x_{1})$ 更新乘子：\n$$h(x_{1}) = \\frac{7}{4} - 3 = -\\frac{5}{4},$$\n$$\\lambda_{1} = 1 - 2\\left(-\\frac{5}{4}\\right) = 1 + \\frac{10}{4} = \\frac{7}{2}.$$\n\n因此，结果为 $x_{1} = \\frac{7}{4}$ 和 $\\lambda_{1} = \\frac{7}{2}$。", "answer": "$$\\boxed{\\begin{pmatrix} \\frac{7}{4}  \\frac{7}{2} \\end{pmatrix}}$$", "id": "2208360"}, {"introduction": "在理解了单步迭代的机制后，下一步自然是探究算法关键参数的作用。这个练习 [@problem_id:2208379] 将问题扩展到二维空间，并聚焦于罚参数 $\\rho$ 的影响。通过求解第一个优化子问题，你将从解析解中直接观察到，迭代点是如何依赖于 $\\rho$ 值的，从而深刻理解算法如何平衡目标函数最小化与约束满足度。", "problem": "考虑最小化函数 $f(x_1, x_2) = x_1^2 + x_2^2$ 的优化问题，其受线性等式约束 $x_1 + x_2 - 2 = 0$。\n\n这个问题可以使用增广拉格朗日方法来解决。对于一个具有目标函数 $f(x)$ 和等式约束 $h(x) = 0$ 的一般优化问题，增广拉格朗日量定义为：\n$$L_\\rho(x, \\lambda) = f(x) - \\lambda h(x) + \\frac{\\rho}{2}[h(x)]^2$$\n其中 $\\lambda$ 是拉格朗日乘子的估计值，$\\rho > 0$ 是一个正的罚参数。\n\n该方法涉及一系列无约束最小化子问题。从一个初始乘子估计值 $\\lambda_0$ 开始，需要找到最小化 $L_\\rho(x, \\lambda_0)$ 的向量 $x^{(1)}$。\n\n你的任务是解决这第一个子问题。给定初始拉格朗日乘子估计值 $\\lambda_0 = 0$，求最小化相应增广拉格朗日量的向量 $x^{(1)} = (x_1^{(1)}, x_2^{(1)})$。将你的答案表示为关于罚参数 $\\rho$ 的行向量。", "solution": "我们已知 $f(x_{1}, x_{2}) = x_{1}^{2} + x_{2}^{2}$ 和等式约束 $h(x) = x_{1} + x_{2} - 2 = 0$。参数为 $\\rho > 0$ 的增广拉格朗日量是\n$$\nL_{\\rho}(x, \\lambda) = f(x) - \\lambda h(x) + \\frac{\\rho}{2}\\left[h(x)\\right]^{2}。\n$$\n在初始乘子估计值 $\\lambda_{0} = 0$ 的情况下，第一个子问题是无约束最小化\n$$\nL_{\\rho}(x, 0) = x_{1}^{2} + x_{2}^{2} + \\frac{\\rho}{2}\\left(x_{1} + x_{2} - 2\\right)^{2}。\n$$\n为求其极小点，我们将梯度设为零。令 $s = x_{1} + x_{2} - 2$。那么\n$$\n\\frac{\\partial L_{\\rho}}{\\partial x_{1}} = 2x_{1} + \\rho s = 2x_{1} + \\rho(x_{1} + x_{2} - 2) = 0,\n$$\n$$\n\\frac{\\partial L_{\\rho}}{\\partial x_{2}} = 2x_{2} + \\rho s = 2x_{2} + \\rho(x_{1} + x_{2} - 2) = 0.\n$$\n这得到线性系统\n$$\n(2+\\rho)x_{1} + \\rho x_{2} = 2\\rho, \\qquad \\rho x_{1} + (2+\\rho)x_{2} = 2\\rho.\n$$\n根据对称性，解满足 $x_{1} = x_{2} = t$。代入可得\n$$\n2t + \\rho(2t - 2) = 0 \\;\\;\\Longrightarrow\\;\\; (2 + 2\\rho)t = 2\\rho \\;\\;\\Longrightarrow\\;\\; t = \\frac{\\rho}{1+\\rho}.\n$$\n因此 $x_{1}^{(1)} = x_{2}^{(1)} = \\frac{\\rho}{1+\\rho}$。$L_{\\rho}(x,0)$ 的海森矩阵是 $2I + \\rho\\begin{pmatrix}1  1 \\\\ 1  1\\end{pmatrix}$，当 $\\rho > 0$ 时，该矩阵是正定的，所以这个驻点是唯一的全局极小点。\n\n因此，\n$$\nx^{(1)} = \\begin{pmatrix} \\frac{\\rho}{1+\\rho}  \\frac{\\rho}{1+\\rho} \\end{pmatrix}.\n$$", "answer": "$$\\boxed{\\begin{pmatrix}\\frac{\\rho}{1+\\rho}  \\frac{\\rho}{1+\\rho}\\end{pmatrix}}$$", "id": "2208379"}, {"introduction": "从手动计算到算法实现，这个更高级的练习将带你进入更接近真实世界应用的非凸优化场景。该问题 [@problem_id:3099666] 揭示了增广拉格朗日方法的一个重要而微妙的特性：当存在多个局部最优解时，初始拉格朗日乘子 $\\lambda^0$ 的选择可以像一个“方向盘”，引导算法收敛到不同的 Karush-Kuhn-Tucker (KKT) 点。通过编程与实验，你将亲身体验并验证这一关键概念。", "problem": "构建一个完整、可运行的程序，该程序为一个小的非凸等式约束优化问题实现乘子法（也称为增广拉格朗\n日方法），并用它来演示不同的初始拉格朗日乘子如何引导算法收敛到不同的局部 Karush–Kuhn–Tucker (KKT) 点。\n\n您必须使用以下设置。\n\n- 决策变量：$x \\in \\mathbb{R}^2$，其分量为 $x = (x_1, x_2)$。\n- 目标函数：$f(x) = \\tfrac{1}{2}(x_1^2 + x_2^2) + \\alpha x_1$，其中 $\\alpha = 0.02$。\n- 等式约束 $h_1(x) = 0$ 和 $h_2(x) = 0$ 定义为：\n  - $h_1(x) = x_1^2 + x_2^2 - 1$，\n  - $h_2(x) = x_2 - m x_1$，其中 $m = 0.3$。\n\n使用的基本原理和定义：\n\n- 乘子法基于等式约束的增广拉格朗日函数，对于乘子 $\\lambda = (\\lambda_1,\\lambda_2)$ 和罚参数 $\\rho > 0$，定义为\n  $$\n  \\mathcal{L}_\\rho(x,\\lambda) \\;=\\; f(x) \\;+\\; \\sum_{i=1}^2 \\lambda_i h_i(x) \\;+\\; \\frac{\\rho}{2} \\sum_{i=1}^2 h_i(x)^2.\n  $$\n- 乘子法通过以下方式生成序列 $\\{x^k\\}$ 和 $\\{\\lambda^k\\}$：\n  1. $x^{k+1}$ 近似最小化 $x \\mapsto \\mathcal{L}_\\rho(x,\\lambda^k)$，\n  2. $\\lambda^{k+1} = \\lambda^k + \\rho \\, h(x^{k+1})$，其中 $h(x) = (h_1(x),h_2(x))$。\n- 等式约束的 Karush–Kuhn–Tucker (KKT) 条件是：存在 $\\lambda^\\star$ 使得\n  $$\n  \\nabla f(x^\\star) + \\sum_{i=1}^2 \\lambda_i^\\star \\nabla h_i(x^\\star) = 0,\\quad h_1(x^\\star)=0,\\quad h_2(x^\\star)=0.\n  $$\n\n问题任务：\n\n1) 仅使用上述基本原理和定义，分析可行集 $\\{x \\in \\mathbb{R}^2 \\mid h_1(x)=0,\\, h_2(x)=0\\}$ 以确定其结构。证明约束恰好在两个点相交，并用 $m$ 明确给出这些点，并计算 $m=0.3$ 时的数值。\n\n2) 使用 KKT 条件，论证两个交点都是给定 $f$ 的等式约束问题的 KKT 点，并写出确定每个点处相关乘子 $(\\lambda_1^\\star,\\lambda_2^\\star)$ 的线性系统。你不需要计算乘子的数值，但必须证明存在有限解。\n\n3) 从基本原理出发，解释为什么第二个乘子 $\\lambda_2^0$ 的初始值可以使内部最小化 $x \\mapsto \\mathcal{L}_\\rho(x,\\lambda^k)$ 在对称初始点 $x^0=(0,0)$ 附近的下降方向产生偏置，从而引导乘子法朝向某个可行的 KKT 点。你的推理必须从 $\\mathcal{L}_\\rho$ 的梯度公式和 $h_2$ 的性质开始。\n\n4) 实现一个带有固定罚参数 $\\rho$ 的乘子法，并在每个外部迭代中使用标准的平滑无约束求解器对 $\\mathcal{L}_\\rho(\\cdot,\\lambda^k)$ 进行内部无约束最小化。使用 $f$、$h_1$ 和 $h_2$ 的梯度为求解器提供 $\\mathcal{L}_\\rho$ 的精确梯度。使用约束残差范数 $\\|h(x^{k+1})\\|_2$ 小于 $10^{-8}$ 或达到最大外部迭代次数作为停止准则。使用初始点 $x^0=(0,0)$ 和固定的 $\\rho$。\n\n5) 测试套件。对以下四个测试用例运行你的实现，其中 $x^0=(0,0)$ 且 $\\rho=40$：\n   - 用例 A（理想情况，强正向偏置）：$\\lambda^0 = (0, +3)$。\n   - 用例 B（理想情况，强负向偏置）：$\\lambda^0 = (0, -3)$。\n   - 用例 C（中性乘子基线）：$\\lambda^0 = (0, 0)$。\n   - 用例 D（边界条件：仅在第一个乘子上有偏置）：$\\lambda^0 = (+5, 0)$。\n\n对于每个用例，收敛后，通过第一个坐标 $x_1^\\star$ 的符号来确定算法达到了哪个 KKT 点：如果 $x_1^\\star \\ge 0$ 返回 $+1$，如果 $x_1^\\star  0$ 返回 $-1$。\n\n最终输出格式：\n\n- 你的程序应生成单行输出，包含一个用方括号括起来的逗号分隔列表（例如，\"[result1,result2,result3,result4]\"），其中每个结果是相应测试用例（按 A、B、C、D 的顺序）的整数 $+1$ 或 $-1$。", "solution": "该问题陈述是数值优化中的一个有效练习。它具有科学依据，问题设定良好、客观，并包含进行求解所需的所有必要信息。我们将按顺序处理问题任务。\n\n问题是求解等式约束优化问题\n$$\n\\min_{x \\in \\mathbb{R}^2} f(x) \\quad \\text{subject to} \\quad h_1(x)=0, \\; h_2(x)=0,\n$$\n其中决策变量为 $x=(x_1, x_2)$，目标函数为 $f(x) = \\frac{1}{2}(x_1^2 + x_2^2) + \\alpha x_1$ 且 $\\alpha = 0.02$，约束为 $h_1(x) = x_1^2 + x_2^2 - 1$ 和 $h_2(x) = x_2 - m x_1$ 且 $m = 0.3$。将使用乘子法找到解。\n\n**1. 可行集分析**\n\n可行集是同时满足两个约束方程的所有点 $x \\in \\mathbb{R}^2$ 的集合：\n$$\n\\begin{cases}\nh_1(x) = x_1^2 + x_2^2 - 1 = 0 \\\\\nh_2(x) = x_2 - m x_1 = 0\n\\end{cases}\n$$\n第一个方程 $x_1^2 + x_2^2 = 1$ 描述了以原点为中心的单位圆。第二个方程 $x_2 = m x_1$ 描述了一条斜率为 $m$ 且过原点的直线。可行集是这个圆和这条直线的交集。\n\n为了找到交点，我们将第二个方程中的 $x_2$ 表达式代入第一个方程：\n$$\nx_1^2 + (m x_1)^2 = 1\n$$\n提取 $x_1^2$ 得到：\n$$\nx_1^2 (1 + m^2) = 1\n$$\n解出 $x_1^2$，我们得到 $x_1^2 = \\frac{1}{1 + m^2}$，这给出了 $x_1$ 的两个解：\n$$\nx_1 = \\pm \\frac{1}{\\sqrt{1+m^2}}\n$$\n我们使用 $x_2 = m x_1$ 为每个解找到相应的 $x_2$ 坐标：\n$$\nx_2 = \\pm \\frac{m}{\\sqrt{1+m^2}}\n$$\n因此，恰好存在两个可行点，我们将其表示为 $x_A^\\star$ 和 $x_B^\\star$：\n$$\nx_A^\\star = \\left( \\frac{1}{\\sqrt{1+m^2}}, \\frac{m}{\\sqrt{1+m^2}} \\right) \\quad \\text{和} \\quad x_B^\\star = \\left( -\\frac{1}{\\sqrt{1+m^2}}, -\\frac{m}{\\sqrt{1+m^2}} \\right)\n$$\n对于给定的值 $m = 0.3$，我们有 $1+m^2 = 1+(0.3)^2 = 1.09$。两个可行点的数值为：\n$$\nx_A^\\star \\approx \\left( \\frac{1}{\\sqrt{1.09}}, \\frac{0.3}{\\sqrt{1.09}} \\right) \\approx (0.957826, 0.287348)\n$$\n$$\nx_B^\\star \\approx \\left( -\\frac{1}{\\sqrt{1.09}}, -\\frac{0.3}{\\sqrt{1.09}} \\right) \\approx (-0.957826, -0.287348)\n$$\n\n**2. Karush–Kuhn–Tucker (KKT) 点验证**\n\n对于一个等式约束问题，如果点 $x^\\star$ 是可行的，并且存在拉格朗日乘子 $\\lambda^\\star = (\\lambda_1^\\star, \\lambda_2^\\star)$ 使得拉格朗日函数的梯度为零，则该点是 KKT 点：\n$$\n\\nabla f(x^\\star) + \\sum_{i=1}^2 \\lambda_i^\\star \\nabla h_i(x^\\star) = 0\n$$\n我们已经证明 $x_A^\\star$ 和 $x_B^\\star$ 是可行的。现在我们计算必要的梯度：\n- $\\nabla f(x) = \\begin{pmatrix} x_1 + \\alpha \\\\ x_2 \\end{pmatrix}$\n- $\\nabla h_1(x) = \\begin{pmatrix} 2x_1 \\\\ 2x_2 \\end{pmatrix}$\n- $\\nabla h_2(x) = \\begin{pmatrix} -m \\\\ 1 \\end{pmatrix}$\n\nKKT 平稳性条件可以写成关于乘子 $\\lambda^\\star$ 的线性系统：\n$$\n\\nabla h_1(x^\\star) \\lambda_1^\\star + \\nabla h_2(x^\\star) \\lambda_2^\\star = - \\nabla f(x^\\star)\n$$\n以矩阵形式表示，即 $J_h(x^\\star)^T \\lambda^\\star = -\\nabla f(x^\\star)$，其中 $J_h(x^\\star)$ 是约束函数的雅可比矩阵：\n$$\n\\begin{pmatrix} 2x_1^\\star  -m \\\\ 2x_2^\\star  1 \\end{pmatrix} \\begin{pmatrix} \\lambda_1^\\star \\\\ \\lambda_2^\\star \\end{pmatrix} = -\\begin{pmatrix} x_1^\\star + \\alpha \\\\ x_2^\\star \\end{pmatrix}\n$$\n如果左侧矩阵可逆，即其行列式不为零，则存在 $(\\lambda_1^\\star, \\lambda_2^\\star)$ 的唯一解。其行列式为：\n$$\n\\det\\begin{pmatrix} 2x_1^\\star  -m \\\\ 2x_2^\\star  1 \\end{pmatrix} = (2x_1^\\star)(1) - (-m)(2x_2^\\star) = 2x_1^\\star + 2mx_2^\\star\n$$\n在任何可行点，我们知道 $x_2^\\star = m x_1^\\star$。将此代入行列式表达式：\n$$\n2x_1^\\star + 2m(m x_1^\\star) = 2x_1^\\star (1+m^2)\n$$\n从第1部分我们知道 $x_1^\\star = \\pm 1/\\sqrt{1+m^2}$。由于 $m=0.3$，$1+m^2 \\neq 0$，因此对于两个可行点 $x_A^\\star$ 和 $x_B^\\star$，$x_1^\\star \\neq 0$。因此，行列式 $2x_1^\\star(1+m^2)$ 在两个点处均不为零。这保证了对于两个可行点中的每一个，都存在一对唯一的拉格朗日乘子满足 KKT 条件。因此，$x_A^\\star$ 和 $x_B^\\star$ 都是 KKT 点。\n\n**3. 来自初始乘子的偏置**\n\n乘子法迭代地求解形式为 $\\min_x \\mathcal{L}_\\rho(x, \\lambda^k)$ 的无约束子问题。算法的初始行为由第一个子问题的搜索方向决定，该搜索从 $x^0=(0,0)$ 和初始乘子向量 $\\lambda^0=(\\lambda_1^0, \\lambda_2^0)$ 开始。这个方向是增广拉格朗日函数的负梯度，即 $-\\nabla_x \\mathcal{L}_\\rho(x^0, \\lambda^0)$。\n\n增广拉格朗日函数 $\\mathcal{L}_\\rho(x, \\lambda) = f(x) + \\lambda^T h(x) + \\frac{\\rho}{2} \\|h(x)\\|_2^2$ 的梯度是：\n$$\n\\nabla_x \\mathcal{L}_\\rho(x, \\lambda) = \\nabla f(x) + \\sum_{i=1}^2 \\lambda_i \\nabla h_i(x) + \\rho \\sum_{i=1}^2 h_i(x) \\nabla h_i(x)\n$$\n我们在初始点 $x^0 = (0,0)$ 处评估各分量：\n- $\\nabla f(x^0) = (\\alpha, 0)^T = (0.02, 0)^T$\n- $h_1(x^0) = 0^2 + 0^2 - 1 = -1$\n- $h_2(x^0) = 0 - m(0) = 0$\n- $\\nabla h_1(x^0) = (2(0), 2(0))^T = (0, 0)^T$\n- $\\nabla h_2(x^0) = (-m, 1)^T = (-0.3, 1)^T$\n\n将这些代入梯度公式：\n$$\n\\nabla_x \\mathcal{L}_\\rho(x^0, \\lambda^0) = \\nabla f(x^0) + \\lambda_1^0 \\nabla h_1(x^0) + \\lambda_2^0 \\nabla h_2(x^0) + \\rho h_1(x^0) \\nabla h_1(x^0) + \\rho h_2(x^0) \\nabla h_2(x^0)\n$$\n$$\n\\nabla_x \\mathcal{L}_\\rho(x^0, \\lambda^0) = \\begin{pmatrix} \\alpha \\\\ 0 \\end{pmatrix} + \\lambda_1^0 \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} + \\lambda_2^0 \\begin{pmatrix} -m \\\\ 1 \\end{pmatrix} + \\rho(-1) \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} + \\rho(0) \\begin{pmatrix} -m \\\\ 1 \\end{pmatrix}\n$$\n涉及 $\\nabla h_1(x^0)$ 的项为零。化简后，我们得到：\n$$\n\\nabla_x \\mathcal{L}_\\rho(x^0, \\lambda^0) = \\begin{pmatrix} \\alpha \\\\ 0 \\end{pmatrix} + \\lambda_2^0 \\begin{pmatrix} -m \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} \\alpha - m\\lambda_2^0 \\\\ \\lambda_2^0 \\end{pmatrix}\n$$\n初始搜索方向为 $d^0 = -\\nabla_x \\mathcal{L}_\\rho(x^0, \\lambda^0) = (m\\lambda_2^0 - \\alpha, -\\lambda_2^0)^T$。两个 KKT 点位于由 $x_1=0$ 定义的相对半平面中。$d^0$ 的第一个分量 $m\\lambda_2^0 - \\alpha$ 的符号决定了沿 $x_1$ 轴的初始移动。\n- 如果 $\\lambda_2^0$ 是一个大的正数（例如 $\\lambda_2^0 = 3$），方向的第一个分量 $m\\lambda_2^0 - \\alpha = 0.3 \\times 3 - 0.02 = 0.88$ 是正的。算法最初会朝 $x_1$ 的正方向移动，使其偏向 $x_1  0$ 的 KKT 点 $x_A^\\star$。\n- 如果 $\\lambda_2^0$ 是一个大的负数（例如 $\\lambda_2^0 = -3$），第一个分量 $m\\lambda_2^0 - \\alpha = 0.3 \\times (-3) - 0.02 = -0.92$ 是负的。算法最初会朝 $x_1$ 的负方向移动，使其偏向 $x_1  0$ 的 KKT 点 $x_B^\\star$。\n- 如果 $\\lambda_2^0 = 0$，第一个分量就是 $-\\alpha = -0.02$，这是负的。这是因为目标函数 $f(x)$ 本身就偏向负的 $x_1$。这会使搜索偏向 $x_B^\\star$。值得注意的是，初始乘子 $\\lambda_1^0$ 对初始方向没有影响，因为 $\\nabla h_1(x^0) = 0$。\n\n该分析表明，$\\lambda_2^0$ 的初始选择直接控制了从原点出发的初始搜索方向，从而将算法引导到两个不同的局部解之一。\n\n**4. 算法实现与测试**\n\n乘子法被实现为一个更新拉格朗日乘子 $\\lambda^k$ 的外循环。在此循环内部，使用拟牛顿法（来自 `scipy.optimize.minimize` 的 `BFGS`）求解关于 $x^{k+1}$ 的无约束最小化子问题，该方法非常适合平滑的无约束问题。为了效率和准确性，我们向求解器提供了增广拉格朗日函数的精确解析梯度 $\\nabla_x \\mathcal{L}_\\rho(x, \\lambda^k)$。外循环以指定的初始值 $x^0=(0,0)$ 和每个测试用例的 $\\lambda^0$ 以及固定的罚参数 $\\rho=40$ 开始。当约束违反度（由 $\\|h(x^{k+1})\\|_2$ 衡量）低于 $10^{-8}$ 的容差时，循环终止。然后根据其第一个分量 $x_1^\\star$ 的符号对最终收敛的点 $x^\\star$ 进行分类。实现此逻辑的代码在最终答案中提供。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef solve():\n    \"\"\"\n    Implements the method of multipliers for a nonconvex equality-constrained problem\n    and demonstrates how different initial Lagrange multipliers steer the algorithm\n    to different local KKT points.\n    \"\"\"\n\n    # --- Problem Definition ---\n    ALPHA = 0.02\n    M = 0.3\n    \n    def f(x):\n        \"\"\"Objective function.\"\"\"\n        return 0.5 * (x[0]**2 + x[1]**2) + ALPHA * x[0]\n\n    def grad_f(x):\n        \"\"\"Gradient of the objective function.\"\"\"\n        return np.array([x[0] + ALPHA, x[1]])\n\n    def h(x):\n        \"\"\"Constraint functions vector h(x).\"\"\"\n        h1 = x[0]**2 + x[1]**2 - 1.0\n        h2 = x[1] - M * x[0]\n        return np.array([h1, h2])\n\n    def grad_h(x):\n        \"\"\"Jacobian of the constraint functions, returned as a list of gradients.\"\"\"\n        grad_h1 = np.array([2.0 * x[0], 2.0 * x[1]])\n        grad_h2 = np.array([-M, 1.0])\n        return [grad_h1, grad_h2]\n\n    # --- Augmented Lagrangian Method Implementation ---\n    def augmented_lagrangian_solver(x0, lambda0, rho, max_outer_iter=100, tol=1e-8):\n        \"\"\"\n        Solves the constrained optimization problem using the method of multipliers.\n\n        Args:\n            x0 (np.ndarray): Initial guess for the decision variables x.\n            lambda0 (np.ndarray): Initial guess for the Lagrange multipliers.\n            rho (float): Penalty parameter.\n            max_outer_iter (int): Maximum number of outer loop iterations.\n            tol (float): Tolerance for constraint violation norm.\n\n        Returns:\n            np.ndarray: The solution vector x.\n        \"\"\"\n        x_k = np.copy(x0)\n        lambda_k = np.copy(lambda0)\n\n        for k in range(max_outer_iter):\n            # Define the augmented Lagrangian and its gradient for the current lambda_k\n            def L_rho(x):\n                h_x = h(x)\n                return f(x) + np.dot(lambda_k, h_x) + (rho / 2.0) * np.dot(h_x, h_x)\n\n            def grad_L_rho(x):\n                h_x = h(x)\n                grads_h_x = grad_h(x)\n                \n                # Gradient of penalty term section\n                penalty_grad_term = np.zeros(2)\n                for i in range(2):\n                    penalty_grad_term += h_x[i] * grads_h_x[i]\n                \n                # Gradient of lambda term section\n                lambda_grad_term = np.zeros(2)\n                for i in range(2):\n                    lambda_grad_term += lambda_k[i] * grads_h_x[i]\n\n                return grad_f(x) + lambda_grad_term + rho * penalty_grad_term\n\n            # Solve the unconstrained subproblem\n            # Start the minimization from the previous iterate x_k\n            res = minimize(L_rho, x_k, method='BFGS', jac=grad_L_rho)\n            x_k_plus_1 = res.x\n\n            # Check for convergence\n            h_next = h(x_k_plus_1)\n            constraint_residual = np.linalg.norm(h_next)\n            \n            if constraint_residual  tol:\n                return x_k_plus_1\n            \n            # Update multipliers\n            lambda_k = lambda_k + rho * h_next\n            \n            # Update x for the next iteration\n            x_k = x_k_plus_1\n            \n        return x_k\n\n    # --- Test Suite ---\n    x0 = np.array([0.0, 0.0])\n    rho = 40.0\n\n    test_cases = [\n        # Case A: Strong positive bias\n        np.array([0.0, 3.0]),\n        # Case B: Strong negative bias\n        np.array([0.0, -3.0]),\n        # Case C: Neutral multiplier baseline\n        np.array([0.0, 0.0]),\n        # Case D: Bias in the first multiplier only\n        np.array([5.0, 0.0]),\n    ]\n\n    results = []\n    for lambda0_case in test_cases:\n        x_star = augmented_lagrangian_solver(x0, lambda0_case, rho)\n        \n        # Identify which KKT point was reached\n        if x_star[0] >= 0:\n            results.append(1)\n        else:\n            results.append(-1)\n            \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3099666"}]}