{"hands_on_practices": [{"introduction": "我们从一个基本但极具代表性的练习开始：将一个点投影到由一组线性不等式定义的多面体上。这个练习将引导你运用Karush-Kuhn-Tucker (KKT) 条件来精确求解投影问题。通过解决这个问题 [@problem_id:3134385]，你将掌握将优化理论的核心原理应用于寻找最近可行点的基本技能，这是梯度投影法的关键步骤。", "problem": "考虑一个欧几里得投影算子，它将点投影到一个闭凸多面体 $C \\subset \\mathbb{R}^{n}$ 上，该多面体由 $C = \\{ x \\in \\mathbb{R}^{n} : A x \\le b \\}$ 定义，其中 $A \\in \\mathbb{R}^{m \\times n}$ 且 $b \\in \\mathbb{R}^{m}$。一个点 $z \\in \\mathbb{R}^{n}$ 到 $C$ 上的投影，是在线性不等式约束下，最小化到 $z$ 的欧几里得距离平方的严格凸二次优化问题的唯一最小化子。该投影及其一阶特征是约束优化中梯度投影法的基础。从欧几里得投影、凸集以及约束优化问题的拉格朗日函数的基本定义出发，推导刻画投影 $\\Pi_{C}(z)$ 的 Karush-Kuhn-Tucker (KKT) 条件。然后，使用这些条件计算在 $n = 2$，$m = 3$ 的具体实例下 $\\Pi_{C}(z)$ 的精确值，其中\n$$\nA = \\begin{pmatrix}\n1  1\\\\\n-1  0\\\\\n0  -1\n\\end{pmatrix}, \n\\qquad\nb = \\begin{pmatrix}\n1\\\\\n0\\\\\n0\n\\end{pmatrix},\n\\qquad\nz = \\begin{pmatrix}\n2\\\\\n-\\frac{1}{2}\n\\end{pmatrix}.\n$$\n将最终答案表示为投影点 $\\Pi_{C}(z)$，形式为一个 $1 \\times 2$ 的行向量。无需四舍五入，不涉及单位。", "solution": "问题陈述经过严格验证，被认为是有效的。这是一个适定的凸优化问题，基于成熟的数学原理，并为获得唯一解提供了所有必要信息。\n\n任务是找到一个点 $z \\in \\mathbb{R}^{n}$ 在闭凸多面体 $C = \\{ x \\in \\mathbb{R}^{n} : A x \\le b \\}$ 上的欧几里得投影。根据定义，投影 $\\Pi_{C}(z)$ 是 $C$ 中离 $z$ 最近的唯一一个点。这可以被表述为一个严格凸二次优化问题：\n$$\n\\begin{aligned}\n\\text{minimize}  \\quad f(x) = \\frac{1}{2} \\|x - z\\|_{2}^{2} \\\\\n\\text{subject to}  \\quad Ax \\le b\n\\end{aligned}\n$$\n包含因子 $\\frac{1}{2}$ 是为了方便，因为它在不改变最小化子 $x^* = \\Pi_{C}(z)$ 的情况下简化了梯度。\n\n约束 $Ax \\le b$ 可以分量形式写作 $g_i(x) = a_i^T x - b_i \\le 0$，其中 $i = 1, \\dots, m$，$a_i^T$ 是矩阵 $A$ 的第 $i$ 行，$b_i$ 是向量 $b$ 的第 $i$ 个分量。\n由于目标函数 $f(x)$ 是凸函数，且约束是线性的（因此也是凸的），这是一个凸优化问题。如果存在一个点 $x_0$ 使得 $A x_0 < b$（严格可行），则 Slater 条件成立。这样的点的存在性没有保证，但对于二次规划（QP）的强对偶性而言并非必需。在这种情况下，Karush-Kuhn-Tucker (KKT) 条件是最优性的充要条件。\n\n首先，我们推导一般的 KKT 条件。该问题的拉格朗日函数为：\n$$ L(x, \\lambda) = f(x) + \\sum_{i=1}^{m} \\lambda_i g_i(x) = \\frac{1}{2} (x-z)^T(x-z) + \\lambda^T(Ax - b) $$\n其中 $\\lambda \\in \\mathbb{R}^{m}$ 是拉格朗日乘子向量。\n\n一个点 $x^*$ 成为最优解的 KKT 条件如下：\n1.  **平稳性 (Stationarity)**：拉格朗日函数关于 $x$ 的梯度在最优点 $x^*$ 处必须为零：\n    $$ \\nabla_x L(x^*, \\lambda^*) = 0 $$\n    目标函数的梯度是 $\\nabla_x f(x) = x - z$。约束项的梯度是 $\\nabla_x(\\lambda^T(Ax-b)) = A^T \\lambda$。\n    因此，平稳性条件是：\n    $$ x^* - z + A^T \\lambda^* = 0 \\quad \\implies \\quad x^* = z - A^T \\lambda^* $$\n\n2.  **原始可行性 (Primal Feasibility)**：最优点 $x^*$ 必须满足问题的约束：\n    $$ A x^* \\le b $$\n\n3.  **对偶可行性 (Dual Feasibility)**：拉格朗日乘子必须为非负：\n    $$ \\lambda^* \\ge 0 \\quad (\\text{i.e., } \\lambda_i^* \\ge 0 \\text{ for all } i = 1, \\dots, m) $$\n\n4.  **互补松弛性 (Complementary Slackness)**：对于每个约束，要么该约束是激活的（即等式成立），要么相应的拉格朗日乘子为零：\n    $$ \\lambda_i^*(a_i^T x^* - b_i) = 0 \\quad \\text{for all } i = 1, \\dots, m $$\n\n这四个条件共同刻画了投影 $\\Pi_C(z) = x^*$。\n\n接下来，我们将这些条件应用于给定的具体实例：\n$n=2$, $m=3$，以及\n$$\nA = \\begin{pmatrix} 1  1\\\\ -1  0\\\\ 0  -1 \\end{pmatrix}, \\quad b = \\begin{pmatrix} 1\\\\ 0\\\\ 0 \\end{pmatrix}, \\quad z = \\begin{pmatrix} 2\\\\ -\\frac{1}{2} \\end{pmatrix}\n$$\n可行集 $C$ 由以下三个不等式定义：\n1. $x_1 + x_2 \\le 1$\n2. $-x_1 \\le 0 \\implies x_1 \\ge 0$\n3. $-x_2 \\le 0 \\implies x_2 \\ge 0$\n\n该集合是 $x_1x_2$-平面上的一个三角形，顶点为 $(0,0)$、$(1,0)$ 和 $(0,1)$。\n要投影的点是 $z = (2, -1/2)$。我们检查 $z$ 是否在 $C$ 中：\n$z_1 + z_2 = 2 - 1/2 = 3/2 > 1$。第一个约束被违反。\n$z_2 = -1/2 < 0$。第三个约束被违反。\n因此，$z \\notin C$，投影 $x^* = \\Pi_C(z)$ 必须位于 $C$ 的边界上。\n\n根据平稳性条件 $x^* = z - A^T \\lambda^*$，我们有：\n$$ \\begin{pmatrix} x_1^* \\\\ x_2^* \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ -1/2 \\end{pmatrix} - \\begin{pmatrix} 1  -1  0 \\\\ 1  0  -1 \\end{pmatrix} \\begin{pmatrix} \\lambda_1^* \\\\ \\lambda_2^* \\\\ \\lambda_3^* \\end{pmatrix} $$\n这给出了分量形式的方程：\n$$ x_1^* = 2 - \\lambda_1^* + \\lambda_2^* $$\n$$ x_2^* = -1/2 - \\lambda_1^* + \\lambda_3^* $$\n\n我们现在使用互补松弛性条件来找到解。我们必须确定在解 $x^*$ 处哪些约束是激活的（即等式成立）。我们可以测试激活约束的可能组合。\n\n让我们根据 $z$ 的位置假设哪些约束是激活的。由于 $z_1+z_2 > 1$ 且 $z_2 < 0$，约束 1 和 3 是激活的似乎是合理的。\n\n**假设：约束 1 和 3 是激活的。**\n这意味着 $x_1^* + x_2^* = 1$ 且 $x_2^* = 0$。解这个简单的方程组得到候选点 $x^* = (1, 0)$。\n根据互补松弛性，由于这些约束是激活的，我们可能有 $\\lambda_1^* > 0$ 和 $\\lambda_3^* > 0$。由于假设约束 2 不是激活的，我们必须有 $\\lambda_2^* = 0$。\n\n让我们检查是否存在 $\\lambda_1^* \\ge 0$ 和 $\\lambda_3^* \\ge 0$ 满足 $x^*=(1,0)$ 和 $\\lambda_2^*=0$ 的平稳性条件。\n将 $x_1^*=1$，$x_2^*=0$ 和 $\\lambda_2^*=0$ 代入平稳性方程：\n对于 $x_1^*$：\n$$ 1 = 2 - \\lambda_1^* + 0 \\implies \\lambda_1^* = 1 $$\n对于 $x_2^*$：\n$$ 0 = -1/2 - \\lambda_1^* + \\lambda_3^* \\implies 0 = -1/2 - 1 + \\lambda_3^* \\implies \\lambda_3^* = \\frac{3}{2} $$\n\n现在我们验证这个候选解，$x^* = (1, 0)$ 和 $\\lambda^* = (1, 0, 3/2)$，是否满足所有 KKT 条件。\n\n1.  **平稳性 (Stationarity)**：根据构造已满足。\n    $1 = 2 - 1 + 0 \\implies 1=1$。\n    $0 = -1/2 - 1 + 3/2 \\implies 0=0$。\n\n2.  **原始可行性 (Primal Feasibility, $x^* \\in C$)**: 我们对照约束检查 $x^*=(1,0)$。\n    - $x_1^* + x_2^* = 1+0 = 1 \\le 1$。（满足）\n    - $x_1^* = 1 \\ge 0$。（满足）\n    - $x_2^* = 0 \\ge 0$。（满足）\n    点 $x^*=(1,0)$ 在 $C$ 中。\n\n3.  **对偶可行性 (Dual Feasibility, $\\lambda^* \\ge 0$)**：\n    - $\\lambda_1^*=1 \\ge 0$。（满足）\n    - $\\lambda_2^*=0 \\ge 0$。（满足）\n    - $\\lambda_3^*=3/2 \\ge 0$。（满足）\n    向量 $\\lambda^*$ 是可行的。\n\n4.  **互补松弛性 (Complementary Slackness, $\\lambda_i^*(a_i^T x^* - b_i) = 0$)**：\n    - $\\lambda_1^*(x_1^*+x_2^*-1) = 1(1+0-1) = 0$。（满足）\n    - $\\lambda_2^*(-x_1^*) = 0(-1) = 0$。（满足，因为 $\\lambda_2^*=0$）\n    - $\\lambda_3^*(-x_2^*) = (3/2)(-0) = 0$。（满足）\n\n所有四个 KKT 条件都已满足。由于该优化问题是严格凸的且约束是仿射的，满足 KKT 条件是一个点成为唯一全局最小值的充要条件。因此，点 $z = (2, -1/2)$ 到集合 $C$ 上的投影是点 $x^* = (1, 0)$。\n\n最终答案是投影点 $\\Pi_C(z) = (1,0)$，表示为一个 $1 \\times 2$ 的行向量。", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n1  0\n\\end{pmatrix}\n}\n$$", "id": "3134385"}, {"introduction": "在掌握了通用方法后，我们将注意力转向一种特殊但至关重要的约束：$\\ell_1$ 范数球。在这个练习中，你将发现通用的 KKT 条件如何演化为一个出人意料的简洁高效算法——软阈值算子。这个练习 [@problem_id:3134290] 揭示了优化领域的一个核心思想：特殊结构的约束集往往能催生出高效的投影算法，这对于稀疏信号恢复和机器学习等大规模应用至关重要。", "problem": "考虑一个约束优化问题，目标是最小化一个可微函数 $f:\\mathbb{R}^{n}\\to\\mathbb{R}$，其约束条件为属于凸集 $C=\\{x\\in\\mathbb{R}^{n}:\\|x\\|_{1}\\leq \\tau\\}$，其中 $\\|x\\|_{1}=\\sum_{i=1}^{n}|x_{i}|$ 是 $\\ell_{1}$ 范数。点 $y\\in\\mathbb{R}^{n}$ 在集合 $C$ 上的投影被定义为以下严格凸问题的唯一最小化子\n$$\n\\min_{x\\in\\mathbb{R}^{n}}\\;\\frac{1}{2}\\|x-y\\|_{2}^{2}\\quad\\text{约束条件为}\\quad \\|x\\|_{1}\\leq \\tau,\n$$\n其中 $\\|x-y\\|_{2}$ 是欧几里得范数。从这个定义和 Karush–Kuhn–Tucker (KKT) 条件出发，推导投影到 $\\ell_{1}$ 球上的坐标形式结构，并展示其如何呈现出一种类似软阈值的形式。然后，将此投影集成到单个投影梯度步中，用于解决一个具体实例，如下所示。\n\n设 $n=4$，$\\tau=2$，且\n$$\nf(x)=\\frac{1}{2}\\|x-c\\|_{2}^{2},\\qquad c=\\begin{pmatrix}3\\\\ -1\\\\ 0.8\\\\ 0.1\\end{pmatrix}.\n$$\n取起始点 $x^{0}=\\begin{pmatrix}0\\\\ 0\\\\ 0\\\\ 0\\end{pmatrix}$ 和步长 $\\alpha=1$。执行一次投影梯度迭代：从 $x^{0}$ 开始走一个梯度步，然后使用您推导出的投影公式投影回集合 $C$。将得到的迭代点 $x^{1}$ 以行向量的形式报告。不需要四舍五入。", "solution": "用户希望解决一个涉及梯度投影法的两部分问题。我将首先验证问题陈述，然后进行求解。\n\n### 步骤1：提取已知条件\n- **要最小化的目标函数**：一个可微函数 $f:\\mathbb{R}^{n}\\to\\mathbb{R}$。\n- **约束集**：$C=\\{x\\in\\mathbb{R}^{n}:\\|x\\|_{1}\\leq \\tau\\}$，其中 $\\|x\\|_{1}=\\sum_{i=1}^{n}|x_{i}|$ 是 $\\ell_{1}$ 范数。\n- **投影定义**：点 $y\\in\\mathbb{R}^{n}$ 在集合 $C$ 上的投影，记作 $P_C(y)$，是 $\\min_{x\\in\\mathbb{R}^{n}}\\;\\frac{1}{2}\\|x-y\\|_{2}^{2}$ 在约束 $\\|x\\|_{1}\\leq \\tau$ 下的唯一最小化子。\n- **方法**：使用 Karush–Kuhn–Tucker (KKT) 条件推导投影公式。\n- **具体实例**：\n    - 维度：$n=4$。\n    - $\\ell_1$ 球半径：$\\tau=2$。\n    - 函数：$f(x)=\\frac{1}{2}\\|x-c\\|_{2}^{2}$，其中 $c=\\begin{pmatrix}3\\\\ -1\\\\ 0.8\\\\ 0.1\\end{pmatrix}$。\n    - 起始点：$x^{0}=\\begin{pmatrix}0\\\\ 0\\\\ 0\\\\ 0\\end{pmatrix}$。\n    - 步长：$\\alpha=1$。\n- **任务**：\n    1. 推导投影到 $\\ell_{1}$ 球上的坐标形式结构。\n    2. 执行一次投影梯度迭代以求得 $x^1$。\n    3. 以行向量形式报告 $x^1$。\n\n### 步骤2：使用提取的已知条件进行验证\n1.  **科学依据**：该问题植根于凸优化，这是数学和工程的核心领域。梯度投影、$\\ell_1$ 范数和 KKT 条件等概念是该领域中公认的基础概念。\n2.  **适定性**：投影到闭凸集（$\\ell_1$ 球）上的问题是一个经典的适定问题；唯一解总是存在的。具体实例提供了所有必要的参数，且函数 $f(x)$ 是凸且可微的，这保证了投影梯度法是适用的。\n3.  **客观性**：该问题使用精确的数学语言陈述，没有歧义或主观论断。\n4.  **无其他缺陷**：该问题是自包含的、一致的，并且没有表现出指令中列出的任何无效标准。\n\n### 步骤3：结论与行动\n问题有效。我将继续提供完整解答。\n\n### 第1部分：推导到 $\\ell_1$ 球上的投影\n\n一个点 $y \\in \\mathbb{R}^n$ 到凸集 $C = \\{x \\in \\mathbb{R}^n : \\|x\\|_1 \\le \\tau\\}$ 上的投影是以下严格凸优化问题的解：\n$$ \\min_{x \\in \\mathbb{R}^n} \\frac{1}{2} \\|x-y\\|_2^2 \\quad \\text{约束条件为} \\quad \\|x\\|_1 \\le \\tau $$\n目标函数是 $\\frac{1}{2}\\sum_{i=1}^n (x_i - y_i)^2$，约束是 $\\sum_{i=1}^n |x_i| \\le \\tau$。我们构造拉格朗日函数：\n$$ L(x, \\lambda) = \\frac{1}{2} \\|x-y\\|_2^2 + \\lambda (\\|x\\|_1 - \\tau) $$\n其中 $\\lambda \\ge 0$ 是不等式约束的拉格朗日乘子。\n\nKKT 最优性条件在最优点 $x^*$ 处必须成立。由于当任何 $x_i=0$ 时，$\\|x\\|_1$ 项是不可微的，我们使用次梯度的概念。平稳性条件表明，在最优点 $x^*$ 处，拉格朗日函数关于 $x$ 的次微分必须包含零：\n$$ 0 \\in \\partial_x L(x^*, \\lambda) = (x^* - y) + \\lambda \\partial \\|x^*\\|_1 $$\n这里，$\\partial \\|x^*\\|_1$ 是 $\\ell_1$ 范数在 $x^*$ 处的次微分。这意味着存在一个次梯度 $g \\in \\partial \\|x^*\\|_1$ 使得：\n$$ y - x^* = \\lambda g $$\n次梯度 $g$ 的分量 $g_i$ 由以下方式给出：如果 $x_i^* \\neq 0$，则 $g_i = \\mathrm{sign}(x_i^*)$；如果 $x_i^* = 0$，则 $g_i \\in [-1, 1]$。\n\n对 $x_i^*$ 逐分量分析：\n$$ y_i - x_i^* = \\lambda g_i $$\n1.  如果 $y_i > \\lambda$：假设 $x_i^* \\le 0$。那么 $y_i - x_i^* \\ge y_i > \\lambda$。另一方面，$\\lambda g_i \\le \\lambda \\cdot 1 = \\lambda$（因为 $g_i \\in [-1, 1]$）。这是一个矛盾。因此，$x_i^* > 0$。当 $x_i^* > 0$ 时，我们有 $g_i = 1$，这给出 $y_i - x_i^* = \\lambda$，即 $x_i^* = y_i - \\lambda > 0$。\n2.  如果 $y_i < -\\lambda$：假设 $x_i^* \\ge 0$。那么 $y_i - x_i^* \\le y_i < -\\lambda$。另一方面，$\\lambda g_i \\ge \\lambda \\cdot (-1) = -\\lambda$。这是一个矛盾。因此，$x_i^* < 0$。当 $x_i^* < 0$ 时，我们有 $g_i = -1$，这给出 $y_i - x_i^* = -\\lambda$，即 $x_i^* = y_i + \\lambda < 0$。\n3.  如果 $|y_i| \\le \\lambda$：假设 $x_i^* > 0$。那么 $x_i^* = y_i - \\lambda \\le \\lambda - \\lambda = 0$，矛盾。假设 $x_i^* < 0$。那么 $x_i^* = y_i + \\lambda \\ge -\\lambda + \\lambda = 0$，矛盾。因此，我们必须有 $x_i^* = 0$。在这种情况下，条件 $y_i - 0 = \\lambda g_i$ 变为 $y_i = \\lambda g_i$。我们可以选择 $g_i = y_i / \\lambda$，这满足 $g_i \\in [-1, 1]$，因为 $|y_i| \\le \\lambda$。\n\n这三种情况可以用软阈值算子统一表示，其定义为 $S_a(z) = \\mathrm{sign}(z) \\max(|z|-a, 0)$。解具有以下形式：\n$$ x_i^* = S_\\lambda(y_i) = \\mathrm{sign}(y_i) \\max(|y_i| - \\lambda, 0) $$\n$\\lambda \\ge 0$ 的值由 KKT 互补松弛性条件 $\\lambda(\\|x^*\\|_1 - \\tau) = 0$ 确定。\n\n情况 A：如果 $\\|y\\|_1 \\le \\tau$。\n我们可以选择 $\\lambda = 0$。那么 $x_i^* = S_0(y_i) = y_i$。解为 $x^*=y$。约束 $\\|x^*\\|_1 = \\|y\\|_1 \\le \\tau$ 得到满足。由于 $\\lambda=0$，互补松弛性也得到满足。因此，如果 $y$ 已经位于 $\\ell_1$ 球内，它就是自身的投影。\n\n情况 B：如果 $\\|y\\|_1 > \\tau$。\n解不能是 $y$，所以我们必须有 $\\lambda > 0$。根据互补松弛性，我们必须有 $\\|x^*\\|_1 = \\tau$。代入 $x_i^*$ 的形式：\n$$ \\sum_{i=1}^n |x_i^*| = \\sum_{i=1}^n |\\mathrm{sign}(y_i) \\max(|y_i| - \\lambda, 0)| = \\sum_{i=1}^n \\max(|y_i| - \\lambda, 0) = \\tau $$\n我们需要找到唯一的 $\\lambda > 0$ 来解这个方程。函数 $h(\\lambda) = \\sum_{i=1}^n \\max(|y_i| - \\lambda, 0)$ 是一个关于 $\\lambda$ 的连续非增函数。由于 $h(0) = \\|y\\|_1 > \\tau$ 且 $\\lim_{\\lambda \\to \\infty} h(\\lambda) = 0$，所以存在唯一解。找到这个 $\\lambda$ 的一个高效算法如下：\n1. 令 $u_i = |y_i|$，对于 $i=1, \\dots, n$。将这些值按降序排序：$u_{(1)} \\ge u_{(2)} \\ge \\dots \\ge u_{(n)}$。\n2. 找到索引 $\\rho = \\max \\left\\{ k \\in \\{1, \\dots, n\\} \\mid u_{(k)} - \\frac{1}{k}\\left(\\sum_{i=1}^k u_{(i)} - \\tau\\right) > 0 \\right\\}$。\n3. 阈值为 $\\lambda = \\frac{1}{\\rho}\\left(\\sum_{i=1}^\\rho u_{(i)} - \\tau\\right)$。\n然后投影 $x^*$ 通过逐分量计算得到 $x_i^* = S_\\lambda(y_i)$。\n\n### 第2部分：应用于具体实例\n\n我们被要求对给定问题执行一步投影梯度法。更新规则是：\n$$ x^{1} = P_C(x^0 - \\alpha \\nabla f(x^0)) $$\n给定的参数是：\n- $n=4$，$\\tau=2$，$C = \\{x \\in \\mathbb{R}^4 : \\|x\\|_1 \\le 2\\}$。\n- $f(x)=\\frac{1}{2}\\|x-c\\|_{2}^{2}$，其中 $c=\\begin{pmatrix}3\\\\ -1\\\\ 0.8\\\\ 0.1\\end{pmatrix}$。\n- $x^{0}=\\begin{pmatrix}0\\\\ 0\\\\ 0\\\\ 0\\end{pmatrix}$ 且 $\\alpha=1$。\n\n1.  **计算梯度**：\n    $f(x)$ 的梯度是 $\\nabla f(x) = x-c$。\n\n2.  **在 $x^0$ 处计算梯度**：\n    $\\nabla f(x^0) = x^0 - c = \\begin{pmatrix}0\\\\ 0\\\\ 0\\\\ 0\\end{pmatrix} - \\begin{pmatrix}3\\\\ -1\\\\ 0.8\\\\ 0.1\\end{pmatrix} = \\begin{pmatrix}-3\\\\ 1\\\\ -0.8\\\\ -0.1\\end{pmatrix}$。\n\n3.  **执行梯度步**：\n    令 $y = x^0 - \\alpha \\nabla f(x^0)$。\n    $y = \\begin{pmatrix}0\\\\ 0\\\\ 0\\\\ 0\\end{pmatrix} - 1 \\cdot \\begin{pmatrix}-3\\\\ 1\\\\ -0.8\\\\ -0.1\\end{pmatrix} = \\begin{pmatrix}3\\\\ -1\\\\ 0.8\\\\ 0.1\\end{pmatrix}$。\n    这就是我们需要投影到集合 $C$ 上的点。\n\n4.  **将 $y$ 投影到 $C$ 上**：\n    首先，我们通过计算其 $\\ell_1$ 范数来检查 $y$ 是否已经在 $C$ 中：\n    $\\|y\\|_1 = |3| + |-1| + |0.8| + |0.1| = 3 + 1 + 0.8 + 0.1 = 4.9$。\n    因为 $\\|y\\|_1 = 4.9 > \\tau=2$，点 $y$ 在集合 $C$ 之外。我们必须找到 $\\lambda > 0$。\n\n    我们使用第1部分中描述的算法。\n    - 各分量的绝对值为 $u = (|3|, |-1|, |0.8|, |0.1|) = (3, 1, 0.8, 0.1)$。\n    - 这些值已经按降序排好：$u_{(1)}=3$, $u_{(2)}=1$, $u_{(3)}=0.8$, $u_{(4)}=0.1$。\n    - 我们通过检查 $k=1, 2, 3, 4$ 来找到 $\\rho$。令 $\\theta_k = \\frac{1}{k}(\\sum_{i=1}^k u_{(i)} - \\tau)$。我们需要找到满足 $u_{(k)} > \\theta_k$ 的最大 $k$。\n        - 对于 $k=1$：$\\theta_1 = \\frac{1}{1}(3-2) = 1$。$u_{(1)} > \\theta_1$ 是否成立？$3 > 1$。是。\n        - 对于 $k=2$：$\\theta_2 = \\frac{1}{2}((3+1)-2) = \\frac{2}{2} = 1$。$u_{(2)} > \\theta_2$ 是否成立？$1 > 1$。否。\n    - 由于当 $k=2$ 时条件不成立，满足条件的最大 $k$ 是 $k=1$。因此，$\\rho=1$。\n\n    - 现在我们使用 $\\rho=1$ 来计算 $\\lambda$：\n    $\\lambda = \\theta_1 = \\frac{1}{1}(u_{(1)} - \\tau) = 3 - 2 = 1$。\n\n    - 最后，我们通过对 $y=(3, -1, 0.8, 0.1)^T$ 的每个分量应用带有 $\\lambda=1$ 的软阈值算子来计算投影 $x^1 = P_C(y)$：\n    $x_1^1 = S_1(y_1) = \\mathrm{sign}(3)\\max(|3|-1, 0) = 1 \\cdot \\max(2, 0) = 2$。\n    $x_2^1 = S_1(y_2) = \\mathrm{sign}(-1)\\max(|-1|-1, 0) = -1 \\cdot \\max(0, 0) = 0$。\n    $x_3^1 = S_1(y_3) = \\mathrm{sign}(0.8)\\max(|0.8|-1, 0) = 1 \\cdot \\max(-0.2, 0) = 0$。\n    $x_4^1 = S_1(y_4) = \\mathrm{sign}(0.1)\\max(|0.1|-1, 0) = 1 \\cdot \\max(-0.9, 0) = 0$。\n\n    得到的迭代点是 $x^1 = \\begin{pmatrix}2\\\\ 0\\\\ 0\\\\ 0\\end{pmatrix}$。作为健全性检查，$\\|x^1\\|_1 = |2|+|0|+|0|+|0| = 2 = \\tau$，这与我们的理论一致。\n\n最终结果，以行向量表示，是 $\\begin{pmatrix} 2  0  0  0 \\end{pmatrix}$。", "answer": "$$\n\\boxed{\\begin{pmatrix}\n2  0  0  0\n\\end{pmatrix}}\n$$", "id": "3134290"}, {"introduction": "在熟练掌握了手动计算之后，是时候通过编码将理论付诸实践了。这个练习处理的是箱式约束，这是最常见的约束类型之一，其投影操作是一个简单的“裁剪”过程。除了实现算法本身，你还将设计并验证一个准则，用以预测哪些变量会“粘附”在其边界上 [@problem_id:3134326]。这将使你对梯度投影法的动态行为以及“有效集”的概念有更深刻的理解。", "problem": "考虑在一个箱式约束的凸集上最小化一个连续可微的凸二次函数 $f:\\mathbb{R}^n \\to \\mathbb{R}$ 的任务。目标函数为 $f(x) = \\tfrac{1}{2} x^\\top Q x + c^\\top x$，其中 $Q \\in \\mathbb{R}^{n \\times n}$ 是对称正定矩阵，且 $c \\in \\mathbb{R}^n$。可行集为 $[l,u] = \\{x \\in \\mathbb{R}^n : l \\le x \\le u\\}$，其中给定的向量 $l,u \\in \\mathbb{R}^n$ 满足对所有索引 $i$ 都有 $l_i \\le u_i$。定义梯度为 $\\nabla f(x) = Q x + c$。梯度投影法通过 $x^{t+1} = P_{[l,u]}(x^t - \\alpha \\nabla f(x^t))$ 生成迭代点，其中 $P_{[l,u]}(\\cdot)$ 是到箱体 $[l,u]$ 上的欧几里得投影，通过分量裁剪的方式执行，且 $\\alpha > 0$ 是一个固定的步长。\n\n从凸集投影的核心定义和约束凸优化的一阶最优性条件出发，设计并论证一个有原则的准则，该准则使用梯度来预测：对于梯度投影法产生的下一个迭代点，当前处于其边界上的坐标是否将保持激活状态（即，将精确地停留在相同的边界上）。然后，实现一个实验，该实验：\n- 在迭代过程中检测坐标何时触及边界（变为激活状态）。\n- 在每次迭代 $t$ 中，对于在 $l_i$ 或 $u_i$ 处激活的每个坐标 $i$，使用您在 $x^t$ 处基于梯度的准则来预测该坐标在下一个迭代点 $x^{t+1}$ 是否将保持激活状态。\n- 执行梯度投影更新，并验证每个预测是否正确。\n- 汇总所有迭代和所有激活坐标的预测准确率，该准确率定义为正确预测数除以总预测数。\n\n使用以下测试套件，它涵盖了一般情况、耦合效应以及在激活坐标处梯度为零的边缘情况。在所有情况下，如果 $|x_i^t - l_i| \\le 10^{-12}$，则将坐标 $i$ 视为在下边界处激活；如果 $|x_i^t - u_i| \\le 10^{-12}$，则视为在上边界处激活。\n\n- 测试用例 1 (理想情况，对角 Hessian 矩阵):\n  - 维度 $n = 3$。\n  - 矩阵 $Q = \\mathrm{diag}(2,3,4)$。\n  - 向量 $c = \\begin{bmatrix} -2 \\\\ 1 \\\\ 0.5 \\end{bmatrix}$。\n  - 下边界 $l = \\begin{bmatrix} 0 \\\\ -1 \\\\ -0.5 \\end{bmatrix}$。\n  - 上边界 $u = \\begin{bmatrix} 1 \\\\ 2 \\\\ 1 \\end{bmatrix}$。\n  - 初始点 $x^0 = \\begin{bmatrix} 0 \\\\ 2 \\\\ -0.5 \\end{bmatrix}$。\n  - 步长 $\\alpha = 0.3$。\n  - 迭代次数 $T = 20$。\n\n- 测试用例 2 (含非对角元素的耦合 Hessian 矩阵):\n  - 维度 $n = 4$。\n  - 矩阵 $Q = \\begin{bmatrix} 4  1  0  0 \\\\ 1  3  1  0 \\\\ 0  1  2  1 \\\\ 0  0  1  1.5 \\end{bmatrix}$。\n  - 向量 $c = \\begin{bmatrix} 0.5 \\\\ -1 \\\\ 0 \\\\ 1 \\end{bmatrix}$。\n  - 下边界 $l = \\begin{bmatrix} -1 \\\\ -1 \\\\ -1 \\\\ -1 \\end{bmatrix}$。\n  - 上边界 $u = \\begin{bmatrix} 1 \\\\ 0.5 \\\\ 0.5 \\\\ 1 \\end{bmatrix}$。\n  - 初始点 $x^0 = \\begin{bmatrix} 1 \\\\ 0.5 \\\\ -1 \\\\ -0.5 \\end{bmatrix}$。\n  - 步长 $\\alpha = 0.25$。\n  - 迭代次数 $T = 30$。\n\n- 测试用例 3 (在激活坐标处梯度为零的边缘情况):\n  - 维度 $n = 3$。\n  - 矩阵 $Q = \\mathrm{diag}(1,1,1)$。\n  - 向量 $c = \\begin{bmatrix} -1 \\\\ 0 \\\\ 1 \\end{bmatrix}$。\n  - 下边界 $l = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\end{bmatrix}$。\n  - 上边界 $u = \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\end{bmatrix}$。\n  - 初始点 $x^0 = \\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\end{bmatrix}$。\n  - 步长 $\\alpha = 0.5$。\n  - 迭代次数 $T = 10$。\n\n您的程序必须：\n1. 对每个测试用例，为指定的 $f$、$[l,u]$ 和 $\\alpha$ 实现梯度投影迭代。\n2. 在每次迭代中，对每个当前激活的坐标，根据您基于梯度的准则，计算关于其在下一个迭代点是否会保持激活的预测，然后在执行更新后验证该预测。\n3. 计算每个测试用例的准确率，结果为 $[0,1]$ 区间内的一个实数。\n\n最终输出格式：\n- 您的程序应产生单行输出，其中包含用方括号括起来的逗号分隔列表形式的结果，例如 $[r_1,r_2,r_3]$，其中每个 $r_k$ 是测试用例 $k$ 的预测准确率，以小数值表示。不应打印任何其他文本。\n- 不涉及角度，也没有物理单位。所有报告的数字都必须是无单位的。", "solution": "该问题要求推导一个准则，用以预测在箱式约束优化任务中，一个激活的坐标在梯度投影法的下一次迭代中是否会保持激活状态。该准则必须有理有据，植根于该算法的机理和优化理论。\n\n该优化问题是在约束 $x \\in [l, u]$ 下最小化一个凸二次函数 $f(x) = \\frac{1}{2} x^\\top Q x + c^\\top x$，其中 $[l, u]$ 是由下边界和上边界向量 $l$ 和 $u$ 定义的箱体。矩阵 $Q$ 是对称正定的，这确保了 $f(x)$ 是严格凸的。\n\n目标函数的梯度为 $\\nabla f(x) = Qx + c$。\n\n梯度投影法通过以下更新规则生成一个迭代序列 $\\{x^t\\}$：\n$$\nx^{t+1} = P_{[l,u]}(x^t - \\alpha \\nabla f(x^t))\n$$\n其中 $\\alpha > 0$ 是一个固定的步长，而 $P_{[l,u]}(\\cdot)$ 是到可行集 $[l,u]$ 上的欧几里得投影。对于箱式约束，这个投影是可分离的，可以按分量计算。对每个坐标 $i \\in \\{1, \\dots, n\\}$，更新方式为：\n$$\nx_i^{t+1} = P_{[l_i, u_i]}(x_i^t - \\alpha g_i^t)\n$$\n其中 $g_i^t = (\\nabla f(x^t))_i$ 是在 $x^t$ 处梯度的第 $i$ 个分量。这个一维投影算子，也被称为裁剪（clipping），定义为：\n$$\nP_{[l_i, u_i]}(y_i) = \\max(l_i, \\min(u_i, y_i))\n$$\n\n如果一个坐标 $x_i^t$ 位于其某个边界上，即 $x_i^t = l_i$ 或 $x_i^t = u_i$，则它被认为是激活的。我们需要设计一个使用梯度 $g^t$ 的准则，来预测 $x_i^{t+1}$ 是否会等于相同的边界值。\n\n**预测准则的推导**\n\n让我们来分析一个激活坐标保持激活状态的条件。\n\n**情况1：坐标 $i$ 在下边界处激活。**\n假设在迭代 $t$ 时，坐标 $i$ 位于其下边界，即 $x_i^t = l_i$。该分量的更新规则变为：\n$$\nx_i^{t+1} = \\max(l_i, \\min(u_i, l_i - \\alpha g_i^t))\n$$\n为使该坐标在下边界保持激活，必须有 $x_i^{t+1} = l_i$。该等式成立的充要条件是，外层 $\\max$ 函数的参数小于或等于 $l_i$：\n$$\n\\min(u_i, l_i - \\alpha g_i^t) \\le l_i\n$$\n由于已知 $l_i \\le u_i$，上述条件若要满足，则需要 $\\min$ 函数的第二个参数小于或等于 $l_i$：\n$$\nl_i - \\alpha g_i^t \\le l_i\n$$\n两边减去 $l_i$ 得到：\n$$\n-\\alpha g_i^t \\le 0\n$$\n由于步长 $\\alpha$ 是严格正的（$\\alpha > 0$），我们可以两边除以 $-\\alpha$ 并反转不等号：\n$$\ng_i^t \\ge 0\n$$\n因此，在迭代 $t$ 时于其下边界 $l_i$ 处激活的坐标 $i$，将在迭代 $t+1$ 时保持在 $l_i$ 处激活的充要条件是，梯度的第 $i$ 个分量 $g_i^t = (\\nabla f(x^t))_i$ 是非负的。\n\n这个准则很直观。在 $x_i^t = l_i$ 处，一个非负的梯度分量 $g_i^t \\ge 0$ 意味着函数 $f$ 沿着坐标 $i$ 进入可行区域的方向是平稳或递增的。梯度下降步 $x_i^t - \\alpha g_i^t$ 试图移动到一个小于或等于 $l_i$ 的值。投影算子则确保该点保持在边界 $l_i$ 上。\n\n**情况2：坐标 $i$ 在上边界处激活。**\n现在，假设在迭代 $t$ 时，坐标 $i$ 位于其上边界，即 $x_i^t = u_i$。更新规则为：\n$$\nx_i^{t+1} = \\max(l_i, \\min(u_i, u_i - \\alpha g_i^t))\n$$\n为使该坐标在上边界保持激活，必须有 $x_i^{t+1} = u_i$。这种情况发生于内层项 $\\min(u_i, u_i - \\alpha g_i^t)$ 的值为 $u_i$。这成立的充要条件是：\n$$\nu_i \\le u_i - \\alpha g_i^t\n$$\n两边减去 $u_i$ 得到：\n$$\n0 \\le -\\alpha g_i^t\n$$\n同样，由于 $\\alpha > 0$，这可以简化为：\n$$\ng_i^t \\le 0\n$$\n因此，在迭代 $t$ 时于其上边界 $u_i$ 处激活的坐标 $i$，将在迭代 $t+1$ 时保持在 $u_i$ 处激活的充要条件是，梯度的第 $i$ 个分量 $g_i^t$ 是非正的。\n\n这个准则也与优化原理一致。在 $x_i^t = u_i$ 处，一个非正的梯度分量 $g_i^t \\le 0$ 表明，当我们从边界 $u_i$ 向可行集外部移动时，函数是平稳或递增的。因此，梯度下降步试图移动到一个大于或等于 $u_i$ 的值，而投影操作会将其裁剪回 $u_i$。\n\n**最终准则**\n\n用于预测一个激活坐标是否保持激活的基于梯度的准则如下：设 $x^t$ 为当前迭代点，$g^t = \\nabla f(x^t)$ 为梯度。对任意坐标 $i$：\n1.  如果 $x_i^t$ 在下边界 $l_i$ 处激活，则当且仅当 $g_i^t \\ge 0$ 时，预测其将 **保持激活**。\n2.  如果 $x_i^t$ 在上边界 $u_i$ 处激活，则当且仅当 $g_i^t \\le 0$ 时，预测其将 **保持激活**。\n\n这组条件与该问题的一阶最优性必要条件（Karush-Kuhn-Tucker 条件）直接相关。一个点 $x^*$ 是局部最小值的必要条件是，对每个坐标 $i$：\n- 如果 $l_i < x_i^* < u_i$，则 $(\\nabla f(x^*))_i = 0$。\n- 如果 $x_i^* = l_i$，则 $(\\nabla f(x^*))_i \\ge 0$。\n- 如果 $x_i^* = u_i$，则 $(\\nabla f(x^*))_i \\le 0$。\n我们的准则本质上是检查迭代点 $x^t$ 是否为其激活集满足了一部分最优性条件。如果满足，则该坐标的梯度投影步是静止的。\n\n该实现将模拟梯度投影法固定次数的迭代。在每次迭代中，对于每个被识别为激活的坐标（在 $10^{-12}$ 的数值容差内），它将应用上述准则进行预测。然后，它将计算下一个迭代点并验证预测是否正确，汇总这些结果以计算总体准确率。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef run_experiment(Q, c, l, u, x0, alpha, T):\n    \"\"\"\n    Runs the gradient projection experiment for a single test case.\n\n    Args:\n        Q (np.ndarray): The Hessian matrix of the quadratic term.\n        c (np.ndarray): The linear term vector.\n        l (np.ndarray): The lower bounds vector.\n        u (np.ndarray): The upper bounds vector.\n        x0 (np.ndarray): The initial point.\n        alpha (float): The stepsize.\n        T (int): The number of iterations.\n\n    Returns:\n        float: The prediction accuracy.\n    \"\"\"\n    n = len(x0)\n    x = np.array(x0, dtype=float)\n    l = np.array(l, dtype=float)\n    u = np.array(u, dtype=float)\n    Q = np.array(Q, dtype=float)\n    c = np.array(c, dtype=float)\n\n    total_predictions = 0\n    correct_predictions = 0\n    tolerance = 1e-12\n\n    for _ in range(T):\n        # 1. Calculate gradient at the current iterate x\n        grad = Q @ x + c\n\n        # 2. Identify active coordinates and make predictions\n        # A prediction is a dictionary storing info about an active coordinate\n        predictions_this_step = []\n        for i in range(n):\n            is_active_lower = np.abs(x[i] - l[i]) <= tolerance\n            is_active_upper = np.abs(x[i] - u[i]) <= tolerance\n            \n            predicted_will_remain = None\n            bound_type = None\n\n            if is_active_lower:\n                # Criterion for remaining at lower bound: grad_i >= 0\n                predicted_will_remain = (grad[i] >= 0)\n                bound_type = 'lower'\n            elif is_active_upper:\n                # Criterion for remaining at upper bound: grad_i <= 0\n                predicted_will_remain = (grad[i] <= 0)\n                bound_type = 'upper'\n\n            if predicted_will_remain is not None:\n                total_predictions += 1\n                predictions_this_step.append({\n                    'index': i,\n                    'predicted_will_remain': predicted_will_remain,\n                    'bound_type': bound_type\n                })\n\n        # 3. Perform the gradient projection update\n        x_unprojected = x - alpha * grad\n        x_next = np.clip(x_unprojected, l, u)\n\n        # 4. Verify predictions against the actual outcome\n        for pred_info in predictions_this_step:\n            i = pred_info['index']\n            predicted_will_remain = pred_info['predicted_will_remain']\n            bound_type = pred_info['bound_type']\n            \n            actual_remained = False\n            if bound_type == 'lower':\n                if np.abs(x_next[i] - l[i]) <= tolerance:\n                    actual_remained = True\n            elif bound_type == 'upper':\n                if np.abs(x_next[i] - u[i]) <= tolerance:\n                    actual_remained = True\n\n            if predicted_will_remain == actual_remained:\n                correct_predictions += 1\n                \n        # 5. Update x for the next iteration\n        x = x_next\n        \n    if total_predictions == 0:\n        # If no predictions were ever made, accuracy is vacuously 100%.\n        return 1.0\n    \n    return correct_predictions / total_predictions\n\ndef solve():\n    \"\"\"\n    Defines the test cases, runs the experiments, and prints the results.\n    \"\"\"\n    test_cases = [\n        # Test Case 1\n        {\n            \"Q\": np.diag([2.0, 3.0, 4.0]),\n            \"c\": np.array([-2.0, 1.0, 0.5]),\n            \"l\": np.array([0.0, -1.0, -0.5]),\n            \"u\": np.array([1.0, 2.0, 1.0]),\n            \"x0\": np.array([0.0, 2.0, -0.5]),\n            \"alpha\": 0.3,\n            \"T\": 20\n        },\n        # Test Case 2\n        {\n            \"Q\": np.array([\n                [4.0, 1.0, 0.0, 0.0],\n                [1.0, 3.0, 1.0, 0.0],\n                [0.0, 1.0, 2.0, 1.0],\n                [0.0, 0.0, 1.0, 1.5]\n            ]),\n            \"c\": np.array([0.5, -1.0, 0.0, 1.0]),\n            \"l\": np.array([-1.0, -1.0, -1.0, -1.0]),\n            \"u\": np.array([1.0, 0.5, 0.5, 1.0]),\n            \"x0\": np.array([1.0, 0.5, -1.0, -0.5]),\n            \"alpha\": 0.25,\n            \"T\": 30\n        },\n        # Test Case 3\n        {\n            \"Q\": np.diag([1.0, 1.0, 1.0]),\n            \"c\": np.array([-1.0, 0.0, 1.0]),\n            \"l\": np.array([0.0, 0.0, 0.0]),\n            \"u\": np.array([1.0, 1.0, 1.0]),\n            \"x0\": np.array([0.0, 0.0, 1.0]),\n            \"alpha\": 0.5,\n            \"T\": 10\n        }\n    ]\n\n    results = []\n    for params in test_cases:\n        accuracy = run_experiment(**params)\n        results.append(accuracy)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3134326"}]}