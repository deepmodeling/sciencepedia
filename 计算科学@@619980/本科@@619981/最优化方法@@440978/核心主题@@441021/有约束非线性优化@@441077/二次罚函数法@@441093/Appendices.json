{"hands_on_practices": [{"introduction": "要真正理解二次罚函数法，一个有效的方法是观察无约束问题的解如何随着罚参数 $\\rho$ 的变化而改变。这个练习为一个基础但重要的问题类别——线性约束二次规划（LCQP）——提供了对这条“解路径”或“轨迹”的具体分析性观察。通过显式推导这条轨迹 [@problem_id:495741]，我们可以直接洞察罚项如何在 $\\rho$ 增大时将解拉向可行域。", "problem": "考虑一般的线性约束二次规划 (LCQP) 问题：\n$$\n\\begin{aligned}\n\\text{minimize} \\quad  f(\\mathbf{x}) = \\frac{1}{2}\\mathbf{x}^T Q \\mathbf{x} + \\mathbf{c}^T \\mathbf{x} \\\\\n\\text{subject to} \\quad  A\\mathbf{x} = \\mathbf{b}\n\\end{aligned}\n$$\n其中，$\\mathbf{x} \\in \\mathbb{R}^n$ 是优化变量。问题数据包括一个对称正定矩阵 $Q \\in \\mathbb{R}^{n \\times n}$，一个向量 $\\mathbf{c} \\in \\mathbb{R}^n$，一个行满秩 ($m  n$) 的矩阵 $A \\in \\mathbb{R}^{m \\times n}$，以及一个向量 $\\mathbf{b} \\in \\mathbb{R}^m$。\n\n为了解决这个约束问题，我们采用二次惩罚方法。该方法通过向目标函数添加惩罚项，将问题转化为一系列无约束优化问题。由此产生的无约束目标函数，称为增广目标函数，由下式给出：\n$$\nL_\\rho(\\mathbf{x}) = f(\\mathbf{x}) + \\frac{\\rho}{2} \\|A\\mathbf{x} - \\mathbf{b}\\|_2^2\n$$\n其中 $\\rho  0$ 是一个标量惩罚参数。对于任意给定的 $\\rho  0$，函数 $L_\\rho(\\mathbf{x})$ 有一个唯一的最小化子，我们将其记为 $\\mathbf{x}^*(\\rho)$。当 $\\rho$ 变化时，这个最小化子所描绘的路径称为最小化子的轨迹。\n\n请推导该轨迹 $\\mathbf{x}^*(\\rho)$ 的显式解析表达式，用惩罚参数 $\\rho$ 和问题数据矩阵与向量 $(Q, \\mathbf{c}, A, \\mathbf{b})$ 表示。", "solution": "二次惩罚方法旨在对一系列递增的惩罚参数 $\\rho$ 求解增广目标函数 $L_\\rho(\\mathbf{x})$ 的最小值。对于给定的 $\\rho$，唯一的最小化子记为 $\\mathbf{x}^*(\\rho)$。\n\n增广目标函数为：\n$$\nL_\\rho(\\mathbf{x}) = \\frac{1}{2}\\mathbf{x}^T Q \\mathbf{x} + \\mathbf{c}^T \\mathbf{x} + \\frac{\\rho}{2} \\|A\\mathbf{x} - \\mathbf{b}\\|_2^2\n$$\n我们可以展开惩罚项，即欧几里得范数的平方：\n$$\n\\|A\\mathbf{x} - \\mathbf{b}\\|_2^2 = (A\\mathbf{x} - \\mathbf{b})^T(A\\mathbf{x} - \\mathbf{b}) = (\\mathbf{x}^T A^T - \\mathbf{b}^T)(A\\mathbf{x} - \\mathbf{b}) = \\mathbf{x}^T A^T A \\mathbf{x} - \\mathbf{x}^T A^T \\mathbf{b} - \\mathbf{b}^T A \\mathbf{x} + \\mathbf{b}^T \\mathbf{b}\n$$\n由于 $\\mathbf{b}^T A \\mathbf{x}$ 是一个标量，它等于其转置 $(\\mathbf{b}^T A \\mathbf{x})^T = \\mathbf{x}^T A^T \\mathbf{b}$。因此，惩罚项变为：\n$$\n\\|A\\mathbf{x} - \\mathbf{b}\\|_2^2 = \\mathbf{x}^T A^T A \\mathbf{x} - 2\\mathbf{b}^T A \\mathbf{x} + \\mathbf{b}^T \\mathbf{b}\n$$\n将其代回 $L_\\rho(\\mathbf{x})$ 得：\n$$\nL_\\rho(\\mathbf{x}) = \\frac{1}{2}\\mathbf{x}^T Q \\mathbf{x} + \\mathbf{c}^T \\mathbf{x} + \\frac{\\rho}{2}(\\mathbf{x}^T A^T A \\mathbf{x} - 2\\mathbf{b}^T A \\mathbf{x} + \\mathbf{b}^T \\mathbf{b})\n$$\n为了找到最小化子 $\\mathbf{x}^*(\\rho)$，我们必须找到使 $L_\\rho(\\mathbf{x})$ 相对于 $\\mathbf{x}$ 的梯度为零的点。我们计算梯度 $\\nabla_{\\mathbf{x}} L_\\rho(\\mathbf{x})$：\n$$\n\\nabla_{\\mathbf{x}} L_\\rho(\\mathbf{x}) = \\nabla_{\\mathbf{x}}\\left(\\frac{1}{2}\\mathbf{x}^T Q \\mathbf{x}\\right) + \\nabla_{\\mathbf{x}}(\\mathbf{c}^T \\mathbf{x}) + \\frac{\\rho}{2} \\nabla_{\\mathbf{x}}(\\mathbf{x}^T A^T A \\mathbf{x} - 2\\mathbf{b}^T A \\mathbf{x} + \\mathbf{b}^T \\mathbf{b})\n$$\n使用标准的矩阵微积分法则（对于对称的 $Q$ 和 $A^T A$）：\n- 对于对称矩阵 $M$，$\\nabla_{\\mathbf{x}}(\\frac{1}{2}\\mathbf{x}^T M \\mathbf{x}) = M\\mathbf{x}$。\n- $\\nabla_{\\mathbf{x}}(\\mathbf{v}^T \\mathbf{x}) = \\mathbf{v}$。\n\n梯度为：\n$$\n\\nabla_{\\mathbf{x}} L_\\rho(\\mathbf{x}) = Q\\mathbf{x} + \\mathbf{c} + \\frac{\\rho}{2}(2A^T A \\mathbf{x} - 2A^T\\mathbf{b})\n$$\n$$\n\\nabla_{\\mathbf{x}} L_\\rho(\\mathbf{x}) = Q\\mathbf{x} + \\mathbf{c} + \\rho(A^T A \\mathbf{x} - A^T\\mathbf{b})\n$$\n将梯度设为零向量以找到驻点 $\\mathbf{x}^*(\\rho)$：\n$$\nQ\\mathbf{x}^*(\\rho) + \\mathbf{c} + \\rho A^T A \\mathbf{x}^*(\\rho) - \\rho A^T\\mathbf{b} = \\mathbf{0}\n$$\n现在，我们求解 $\\mathbf{x}^*(\\rho)$。将包含 $\\mathbf{x}^*(\\rho)$ 的项组合在一起：\n$$\n(Q + \\rho A^T A)\\mathbf{x}^*(\\rho) = \\rho A^T\\mathbf{b} - \\mathbf{c}\n$$\n为了确认这个驻点是一个最小值点，我们考察 $L_\\rho(\\mathbf{x})$ 的海森矩阵，即 $\\nabla^2_{\\mathbf{x}} L_\\rho(\\mathbf{x})$：\n$$\n\\nabla^2_{\\mathbf{x}} L_\\rho(\\mathbf{x}) = Q + \\rho A^T A\n$$\n问题陈述中指出 $Q$ 是对称正定矩阵 ($Q \\succ 0$)。对于任何矩阵 $A$，矩阵 $A^T A$ 总是半正定的。由于 $\\rho  0$，海森矩阵 $Q + \\rho A^T A$ 是一个正定矩阵与一个半正定矩阵之和，结果是一个正定矩阵。因此，$L_\\rho(\\mathbf{x})$ 是一个严格凸函数，该驻点是其唯一的全局最小值点。\n\n为了求出 $\\mathbf{x}^*(\\rho)$ 的表达式，我们可以对矩阵 $(Q + \\rho A^T A)$ 求逆：\n$$\n\\mathbf{x}^*(\\rho) = (Q + \\rho A^T A)^{-1} (\\rho A^T\\mathbf{b} - \\mathbf{c})\n$$\n这就是最小化子轨迹作为 $\\rho$ 的函数的显式表达式。", "answer": "$$\n\\boxed{(Q + \\rho A^T A)^{-1} (\\rho A^T\\mathbf{b} - \\mathbf{c})}\n$$", "id": "495741"}, {"introduction": "虽然理论看似简单直接，但在实践中应用罚函数法时，问题的构建需要非常谨慎。这个练习探讨了一个常见的陷阱：冗余或缩放不当的约束。通过一个简单的计算 [@problem_id:3169165]，我们将看到重复一个约束（即使在数学上是等价的）会如何扭曲罚函数的目标曲面并导致不同的解，这突显了恰当的约束加权的重要性。", "problem": "考虑约束优化问题：在约束 $c(x)=0$ 下最小化 $f(x)$，其中 $f(x)=(x-1)^{2}$ 且 $c(x)=x$。假设该约束被冗余复制和缩放，形成向量值约束 $\\tilde{c}(x) = \\begin{pmatrix} c_{1}(x) \\\\ c_{2}(x) \\end{pmatrix} = \\begin{pmatrix} x \\\\ \\alpha x \\end{pmatrix}$，其中 $\\alpha0$ 是一个固定的标量。使用二次罚函数法，定义加权惩罚目标函数\n$$\nP_{\\mu}(x;M) \\;=\\; f(x) \\;+\\; \\frac{1}{2\\mu}\\,\\tilde{c}(x)^{\\top} M \\,\\tilde{c}(x),\n$$\n其中 $\\mu0$ 是罚参数，$M\\in\\mathbb{R}^{2\\times 2}$ 是一个对称正定（SPD）权重矩阵。设 $\\mu=1$ 且 $\\alpha=3$。\n\n考虑以下两种情况：\n- 情况 A（无补偿）：$M=I_{2}$。\n- 情况 B（对重复缩放进行补偿）：选择一个对角矩阵 $M=\\mathrm{diag}(m_{1},m_{2})$，使得对所有实数 $x$ 都有 $\\tilde{c}(x)^{\\top}M\\,\\tilde{c}(x)=2x^{2}$，从而使对 $x$ 的惩罚项与 $\\alpha$ 无关。然后在惩罚目标函数中使用任何一个这样的对角矩阵 $M$。\n\n令 $x_{\\mathrm{A}}$ 和 $x_{\\mathrm{B}}$ 分别表示情况 A 和情况 B 中 $P_{\\mu}(x;M)$ 的唯一最小化子。计算比率 $R = x_{\\mathrm{A}}/x_{\\mathrm{B}}$。最终答案请给出精确实数值（不要四舍五入）。", "solution": "该问题要求使用二次罚函数法，在两种不同的约束权重方案下分析一个约束优化问题。目标是计算相应惩罚函数的最小化子之比。\n\n问题是在约束 $c(x) = x = 0$ 下最小化函数 $f(x) = (x-1)^2$。该约束被重复和缩放，得到向量值约束函数 $\\tilde{c}(x) = \\begin{pmatrix} x \\\\ \\alpha x \\end{pmatrix}$。给定标量 $\\alpha=3$。\n\n二次罚函数法通过最小化一个无约束的目标函数（即惩罚函数）来近似求解约束问题，该惩罚函数定义为：\n$$\nP_{\\mu}(x;M) = f(x) + \\frac{1}{2\\mu}\\,\\tilde{c}(x)^{\\top} M \\,\\tilde{c}(x)\n$$\n此处，$\\mu  0$ 是罚参数，$M$ 是对称正定（SPD）权重矩阵。给定 $\\mu=1$。\n\n我们将分析这两种指定的情况，以找到各自的最小化子 $x_{\\mathrm{A}}$ 和 $x_{\\mathrm{B}}$。\n\n**情况 A：无补偿**\n\n在此情况下，权重矩阵是单位矩阵 $M = I_2 = \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix}$。\n向量值约束为 $\\tilde{c}(x) = \\begin{pmatrix} x \\\\ 3x \\end{pmatrix}$。\n二次惩罚项为 $\\tilde{c}(x)^{\\top} M \\,\\tilde{c}(x)$。我们来计算此项：\n$$\n\\tilde{c}(x)^{\\top} M \\,\\tilde{c}(x) = \\begin{pmatrix} x  3x \\end{pmatrix} \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix} \\begin{pmatrix} x \\\\ 3x \\end{pmatrix} = \\begin{pmatrix} x  3x \\end{pmatrix} \\begin{pmatrix} x \\\\ 3x \\end{pmatrix} = x^2 + (3x)^2 = x^2 + 9x^2 = 10x^2\n$$\n因此，情况 A 的惩罚目标函数（记为 $P_{\\mathrm{A}}(x)$）为：\n$$\nP_{\\mathrm{A}}(x) = f(x) + \\frac{1}{2\\mu} \\left( 10x^2 \\right) = (x-1)^2 + \\frac{1}{2(1)} (10x^2) = (x-1)^2 + 5x^2\n$$\n展开此表达式，得到一个关于 $x$ 的二次函数：\n$$\nP_{\\mathrm{A}}(x) = (x^2 - 2x + 1) + 5x^2 = 6x^2 - 2x + 1\n$$\n为了找到最小化子 $x_{\\mathrm{A}}$，我们通过将 $P_{\\mathrm{A}}(x)$ 对 $x$ 的一阶导数设为零来求其临界点。这是取最小值的的一阶必要条件。\n$$\n\\frac{d}{dx}P_{\\mathrm{A}}(x) = \\frac{d}{dx}(6x^2 - 2x + 1) = 12x - 2\n$$\n将导数设为零：\n$$\n12x - 2 = 0 \\implies 12x = 2 \\implies x_{\\mathrm{A}} = \\frac{2}{12} = \\frac{1}{6}\n$$\n为确认这是一个最小值点，我们检查二阶充分条件。二阶导数为：\n$$\n\\frac{d^2}{dx^2}P_{\\mathrm{A}}(x) = 12\n$$\n由于 $\\frac{d^2}{dx^2}P_{\\mathrm{A}}(x) = 12  0$，函数 $P_{\\mathrm{A}}(x)$ 是严格凸函数，因此 $x_{\\mathrm{A}} = \\frac{1}{6}$ 是唯一的全局最小化子。\n\n**情况 B：对重复缩放进行补偿**\n\n在此情况下，权重矩阵是一个对角矩阵 $M = \\mathrm{diag}(m_1, m_2)$，其选择是为了满足对所有实数 $x$ 都有 $\\tilde{c}(x)^{\\top}M\\,\\tilde{c}(x) = 2x^2$ 的条件。此条件旨在使对 $x$ 的惩罚项与缩放因子 $\\alpha$ 无关。\n\n我们来分析对 $M$ 的条件：\n$$\n\\tilde{c}(x)^{\\top} M \\,\\tilde{c}(x) = \\begin{pmatrix} x  3x \\end{pmatrix} \\begin{pmatrix} m_1  0 \\\\ 0  m_2 \\end{pmatrix} \\begin{pmatrix} x \\\\ 3x \\end{pmatrix} = m_1 x^2 + m_2 (3x)^2 = (m_1 + 9m_2)x^2\n$$\n为使该式对所有 $x$ 都等于 $2x^2$，我们必须有 $m_1 + 9m_2 = 2$。问题规定 $M$ 必须是对称正定矩阵，对于对角矩阵而言，这要求 $m_1  0$ 且 $m_2  0$。满足这些条件的 $(m_1, m_2)$ 有无穷多解（例如，$m_1=1, m_2=1/9$）。然而，根据问题的提法，我们无需选择一个特定的 $M$。惩罚目标函数由二次型 $\\tilde{c}(x)^{\\top}M\\,\\tilde{c}(x)$ 的规定值决定。\n\n情况 B 的惩罚目标函数（记为 $P_{\\mathrm{B}}(x)$）为：\n$$\nP_{\\mathrm{B}}(x) = f(x) + \\frac{1}{2\\mu} \\left( \\tilde{c}(x)^{\\top}M\\,\\tilde{c}(x) \\right) = (x-1)^2 + \\frac{1}{2(1)} (2x^2) = (x-1)^2 + x^2\n$$\n展开此表达式：\n$$\nP_{\\mathrm{B}}(x) = (x^2 - 2x + 1) + x^2 = 2x^2 - 2x + 1\n$$\n为了找到最小化子 $x_{\\mathrm{B}}$，我们再次应用一阶必要条件：\n$$\n\\frac{d}{dx}P_{\\mathrm{B}}(x) = \\frac{d}{dx}(2x^2 - 2x + 1) = 4x - 2\n$$\n将导数设为零：\n$$\n4x - 2 = 0 \\implies 4x = 2 \\implies x_{\\mathrm{B}} = \\frac{2}{4} = \\frac{1}{2}\n$$\n二阶导数为 $\\frac{d^2}{dx^2}P_{\\mathrm{B}}(x) = 4  0$，这确认了 $P_{\\mathrm{B}}(x)$ 是严格凸函数，且 $x_{\\mathrm{B}} = \\frac{1}{2}$ 是唯一的全局最小化子。\n\n**计算比率**\n\n最后一步是计算比率 $R = x_{\\mathrm{A}}/x_{\\mathrm{B}}$。\n使用上面推导出的值：\n$$\nx_{\\mathrm{A}} = \\frac{1}{6}\n$$\n$$\nx_{\\mathrm{B}} = \\frac{1}{2}\n$$\n比率为：\n$$\nR = \\frac{x_{\\mathrm{A}}}{x_{\\mathrm{B}}} = \\frac{1/6}{1/2} = \\frac{1}{6} \\cdot \\frac{2}{1} = \\frac{2}{6} = \\frac{1}{3}\n$$", "answer": "$$\n\\boxed{\\frac{1}{3}}\n$$", "id": "3169165"}, {"introduction": "二次罚函数法的威力在于通过增大罚参数 $\\rho$ 来更严格地强制执行约束。然而，这会带来巨大的数值计算代价。这个动手实践将指导你实现牛顿法，并直接观察当 $\\rho$ 增大时，Hessian矩阵病态化的关键问题 [@problem_id:3169152]。这一现象使得优化子问题越来越难以求解，也是推动更高级方法发展的一个关键动因。", "problem": "考虑在有限维欧几里得空间中，通过二次罚函数法对等式约束问题进行无约束重构。设 $\\mathbf{x} \\in \\mathbb{R}^n$，目标函数 $f:\\mathbb{R}^n \\to \\mathbb{R}$ 和约束映射 $c:\\mathbb{R}^n \\to \\mathbb{R}^m$ 均为二次连续可微函数。定义二次罚函数目标 $\\Phi(\\mathbf{x};\\rho) = f(\\mathbf{x}) + \\tfrac{\\rho}{2}\\lVert c(\\mathbf{x})\\rVert_2^2$，其中罚参数 $\\rho  0$。应用于 $\\Phi(\\mathbf{x};\\rho)$ 的 Newton 法通过求解由二阶泰勒展开式导出的线性系统，在基点 $\\mathbf{x}$ 处寻找步长 $\\mathbf{p}$。Hessian 矩阵的条件数影响数值稳定性，步长大小影响每次迭代的进展。\n\n从梯度 $\\nabla$、Hessian 矩阵 $\\nabla^2$、链式法则以及通过求解包含 $\\Phi(\\mathbf{x};\\rho)$ 的 Hessian 矩阵和梯度的系统所确定的 Newton 步长的基本定义出发，推导计算以下量所需的表达式：\n1. 对于给定的 $\\rho$，在固定基点 $\\mathbf{x}$ 处的 Newton 步长 $\\mathbf{p}$。\n2. 该步长的欧几里得范数 $\\lVert \\mathbf{p} \\rVert_2$。\n3. $\\Phi(\\mathbf{x};\\rho)$ 的 Hessian 矩阵的条件数，其定义为最大奇异值与最小奇异值之比，使用奇异值分解（SVD）计算。如果最小奇异值为零，则条件数定义为 $+\\infty$。\n\n然后，实现一个仿真程序，研究对于几个测试用例，逐步增加罚参数如何影响 Hessian 条件数和 Newton 步长的大小。使用更新规则 $\\rho_{k+1} = 10 \\rho_k$，初始值 $\\rho_0 = 10^{-2}$，并对 $K=6$ 个值 $\\{\\rho_0,\\rho_1,\\dots,\\rho_5\\}$ 进行仿真。\n\n仿真协议：\n- 对每个测试用例，固定一个基点 $\\mathbf{x}_0$，并为每个 $\\rho_k$ 计算在 $\\mathbf{x}_0$ 处的 Newton 步长 $\\mathbf{p}_k$，在不同的 $\\rho_k$ 值之间不更新 $\\mathbf{x}_0$。这可以分离出 $\\rho$ 对 Hessian 条件数和步长大小的影响。\n- 对每个 $\\rho_k$，通过 SVD 计算 $\\nabla^2 \\Phi(\\mathbf{x}_0;\\rho_k)$ 的 Hessian 条件数，并通过求解 Newton 系统得到欧几里得步长范数 $\\lVert \\mathbf{p}_k \\rVert_2$。如果 Hessian 矩阵是奇异或病态的，则使用伪逆计算最小范数步长。\n\n测试套件（均在 $\\mathbb{R}^2$ 中）：\n- 用例 A（对称正定，线性约束）：$f(\\mathbf{x}) = \\tfrac{1}{2}\\mathbf{x}^\\top Q \\mathbf{x}$，其中 $Q = \\begin{bmatrix}1  0 \\\\ 0  2\\end{bmatrix}$；$c(\\mathbf{x}) = [x_1 + x_2 - 1]$；基点 $\\mathbf{x}_0 = [0,0]^\\top$。\n- 用例 B（可能导致不定性的非线性约束）：$f(\\mathbf{x}) = \\tfrac{1}{2}\\mathbf{x}^\\top Q \\mathbf{x}$，其中 $Q = \\begin{bmatrix}2  0 \\\\ 0  1\\end{bmatrix}$；$c(\\mathbf{x}) = [x_1^2 + x_2 - 1]$；基点 $\\mathbf{x}_0 = [0.5,-0.2]^\\top$。\n- 用例 C（病态目标函数，线性约束改善条件数）：$f(\\mathbf{x}) = \\tfrac{1}{2}\\mathbf{x}^\\top Q \\mathbf{x}$，其中 $Q = \\begin{bmatrix}10^{-3}  0 \\\\ 0  1\\end{bmatrix}$；$c(\\mathbf{x}) = [x_1 - 0.5]$；基点 $\\mathbf{x}_0 = [1.0,1.0]^\\top$。\n\n使用的科学基础：\n- 二次可微目标函数的 Newton 法定义：步长 $\\mathbf{p}$ 求解 $\\nabla^2 \\Phi(\\mathbf{x};\\rho)\\,\\mathbf{p} = -\\nabla \\Phi(\\mathbf{x};\\rho)$。\n- 复合函数与和函数的梯度和 Hessian 矩阵的链式法则。\n- $c(\\mathbf{x})$ 的雅可比矩阵 $J(\\mathbf{x})$ 和分量 Hessian 矩阵 $\\nabla^2 c_i(\\mathbf{x})$ 的定义。\n- 用于可能不定的对称矩阵条件数的奇异值分解。\n\n不涉及角度单位。不涉及物理单位。\n\n对于每个测试用例，您的程序必须生成两个包含 $K$ 个浮点数的列表：\n- 对应于 $\\rho_k, k=0,\\dots,5$ 的 Hessian 条件数列表。\n- 对应于 $\\rho_k, k=0,\\dots,5$ 的 Newton 步长欧几里得范数列表。\n\n最终输出格式：\n您的程序应生成单行输出，包含一个由三个测试用例摘要组成的列表，每个摘要是如上所述的两个列表的列表。具体来说，输出格式必须为 $[\\,[\\,[\\text{cond}_{A,0},\\dots,\\text{cond}_{A,5}],\\,[\\lVert \\mathbf{p}_{A,0}\\rVert_2,\\dots,\\lVert \\mathbf{p}_{A,5}\\rVert_2]\\,],\\,[\\,[\\text{cond}_{B,0},\\dots,\\text{cond}_{B,5}],\\,[\\lVert \\mathbf{p}_{B,0}\\rVert_2,\\dots,\\lVert \\mathbf{p}_{B,5}\\rVert_2]\\,],\\,[\\,[\\text{cond}_{C,0},\\dots,\\text{cond}_{C,5}],\\,[\\lVert \\mathbf{p}_{C,0}\\rVert_2,\\dots,\\lVert \\mathbf{p}_{C,5}\\rVert_2]\\,]\\,]$。\n\n输出值必须是浮点数。不得打印任何额外文本。", "solution": "该问题要求推导分析二次罚函数法的关键量，并对其行为进行数值仿真。该问题是自洽的，科学上基于数值优化的原理，并且是适定的。因此，我们可以着手求解。\n\n首先，我们推导二次罚函数目标函数的梯度和 Hessian 矩阵、Newton 步长以及 Hessian 条件数的必要表达式。\n\n二次罚函数目标函数定义为：\n$$\n\\Phi(\\mathbf{x};\\rho) = f(\\mathbf{x}) + \\frac{\\rho}{2}\\lVert c(\\mathbf{x})\\rVert_2^2\n$$\n其中 $f:\\mathbb{R}^n \\to \\mathbb{R}$ 是目标函数，$c:\\mathbb{R}^n \\to \\mathbb{R}^m$ 是约束映射，$\\rho  0$ 是罚参数。约束向量的范数平方可以写为其分量 $c_i(\\mathbf{x})$ 的平方和：\n$$\n\\lVert c(\\mathbf{x})\\rVert_2^2 = c(\\mathbf{x})^\\top c(\\mathbf{x}) = \\sum_{i=1}^{m} c_i(\\mathbf{x})^2\n$$\n\n**1. 罚函数目标的梯度 $\\nabla \\Phi(\\mathbf{x};\\rho)$**\n\n为了求 $\\Phi(\\mathbf{x};\\rho)$ 关于 $\\mathbf{x}$ 的梯度，我们逐项微分。$f(\\mathbf{x})$ 的梯度就是 $\\nabla f(\\mathbf{x})$。对于罚项，我们应用链式法则。设 $g(\\mathbf{x}) = \\frac{\\rho}{2} \\sum_{i=1}^{m} c_i(\\mathbf{x})^2$。其梯度的第 $j$ 个分量是：\n$$\n\\frac{\\partial g}{\\partial x_j} = \\frac{\\rho}{2} \\sum_{i=1}^{m} 2 c_i(\\mathbf{x}) \\frac{\\partial c_i(\\mathbf{x})}{\\partial x_j} = \\rho \\sum_{i=1}^{m} c_i(\\mathbf{x}) \\frac{\\partial c_i(\\mathbf{x})}{\\partial x_j}\n$$\n这个表达式可以写成向量形式。约束映射 $c(\\mathbf{x})$ 的雅可比矩阵是一个 $m \\times n$ 矩阵 $J(\\mathbf{x})$，其元素为 $(J(\\mathbf{x}))_{ij} = \\frac{\\partial c_i(\\mathbf{x})}{\\partial x_j}$。表达式 $\\sum_{i=1}^{m} c_i(\\mathbf{x}) \\frac{\\partial c_i(\\mathbf{x})}{\\partial x_j}$ 是向量 $J(\\mathbf{x})^\\top c(\\mathbf{x})$ 的第 $j$ 个分量。因此，罚项的梯度是 $\\rho J(\\mathbf{x})^\\top c(\\mathbf{x})$。\n\n合并各项，罚函数目标的完整梯度为：\n$$\n\\nabla \\Phi(\\mathbf{x};\\rho) = \\nabla f(\\mathbf{x}) + \\rho J(\\mathbf{x})^\\top c(\\mathbf{x})\n$$\n\n**2. 罚函数目标的 Hessian 矩阵 $\\nabla^2 \\Phi(\\mathbf{x};\\rho)$**\n\n为了求 Hessian 矩阵，我们将梯度表达式对 $\\mathbf{x}^\\top$ 微分。$f(\\mathbf{x})$ 的 Hessian 矩阵是 $\\nabla^2 f(\\mathbf{x})$。我们需要计算项 $\\rho J(\\mathbf{x})^\\top c(\\mathbf{x})$ 的 Hessian 矩阵。\n该项可以看作是 $\\frac{\\rho}{2}c(\\mathbf{x})^\\top c(\\mathbf{x})$ 的梯度。所以我们需要求 $\\frac{\\rho}{2}c(\\mathbf{x})^\\top c(\\mathbf{x})$ 的 Hessian 矩阵。\n$$\n\\nabla^2 \\left( \\frac{\\rho}{2} \\sum_{i=1}^m c_i(\\mathbf{x})^2 \\right) = \\frac{\\rho}{2} \\sum_{i=1}^m \\nabla^2(c_i(\\mathbf{x})^2)\n$$\n使用 Hessian 矩阵的乘法法则，$\\nabla^2(u^2) = \\nabla(2u\\nabla u) = 2(\\nabla u)(\\nabla u)^\\top + 2u\\nabla^2 u$，我们得到：\n$$\n\\nabla^2(c_i(\\mathbf{x})^2) = 2(\\nabla c_i(\\mathbf{x}))(\\nabla c_i(\\mathbf{x}))^\\top + 2c_i(\\mathbf{x})\\nabla^2 c_i(\\mathbf{x})\n$$\n其中 $\\nabla c_i(\\mathbf{x})$ 是一个列向量，$\\nabla^2 c_i(\\mathbf{x})$ 是第 $i$ 个约束函数的 Hessian 矩阵。对所有 $i$ 求和：\n$$\n\\frac{\\rho}{2} \\sum_{i=1}^m \\left( 2(\\nabla c_i(\\mathbf{x}))(\\nabla c_i(\\mathbf{x}))^\\top + 2c_i(\\mathbf{x})\\nabla^2 c_i(\\mathbf{x}) \\right) = \\rho \\left( \\sum_{i=1}^m (\\nabla c_i(\\mathbf{x}))(\\nabla c_i(\\mathbf{x}))^\\top + \\sum_{i=1}^m c_i(\\mathbf{x})\\nabla^2 c_i(\\mathbf{x}) \\right)\n$$\n第一个求和项 $\\sum_{i=1}^m (\\nabla c_i(\\mathbf{x}))(\\nabla c_i(\\mathbf{x}))^\\top$ 正是矩阵乘积 $J(\\mathbf{x})^\\top J(\\mathbf{x})$ 的定义。\n因此，罚函数目标的 Hessian 矩阵为：\n$$\n\\nabla^2 \\Phi(\\mathbf{x};\\rho) = \\nabla^2 f(\\mathbf{x}) + \\rho \\left( J(\\mathbf{x})^\\top J(\\mathbf{x}) + \\sum_{i=1}^m c_i(\\mathbf{x}) \\nabla^2 c_i(\\mathbf{x}) \\right)\n$$\n\n**3. Newton 步长 $\\mathbf{p}$ 及其范数**\n\n最小化 $\\Phi(\\mathbf{x};\\rho)$ 的 Newton 法涉及通过求解围绕当前点 $\\mathbf{x}$ 的二阶泰勒展开式导出的线性系统来找到搜索方向（步长）$\\mathbf{p}$。该系统是：\n$$\n\\nabla^2 \\Phi(\\mathbf{x};\\rho) \\mathbf{p} = -\\nabla \\Phi(\\mathbf{x};\\rho)\n$$\nNewton 步长 $\\mathbf{p}$ 是这个 $n \\times n$ 线性方程组的解。当 Hessian 矩阵 $\\nabla^2 \\Phi(\\mathbf{x};\\rho)$ 是奇异或病态时，我们使用伪逆（表示为 $(\\cdot)^+$）计算一个最小范数解：\n$$\n\\mathbf{p} = -(\\nabla^2 \\Phi(\\mathbf{x};\\rho))^+ \\nabla \\Phi(\\mathbf{x};\\rho)\n$$\n然后，该步长的欧几里得范数计算为 $\\lVert \\mathbf{p} \\rVert_2 = \\sqrt{\\mathbf{p}^\\top\\mathbf{p}}$。\n\n**4. Hessian 条件数**\n\nHessian 矩阵 $H_\\Phi = \\nabla^2 \\Phi(\\mathbf{x};\\rho)$ 的条件数衡量了其对扰动的敏感性。对于对称矩阵，其奇异值是其特征值的绝对值。条件数定义为最大奇异值 $\\sigma_{\\max}$ 与最小奇异值 $\\sigma_{\\min}$ 之比：\n$$\n\\text{cond}(H_\\Phi) = \\frac{\\sigma_{\\max}(H_\\Phi)}{\\sigma_{\\min}(H_\\Phi)}\n$$\n这些奇异值通过 $H_\\Phi$ 的奇异值分解（SVD）计算。如果 $\\sigma_{\\min} = 0$，则矩阵是奇异的，条件数定义为 $+\\infty$。\n\n**仿真步骤**\n\n该仿真将为三个测试用例中的每一个实现这些推导出的公式。对于每个用例，在固定的基点 $\\mathbf{x}_0$ 处，我们将遍历一系列罚参数 $\\rho_k = 10^{-2} \\times 10^k$，其中 $k \\in \\{0, 1, 2, 3, 4, 5\\}$。在每次迭代中，我们将：\n1.  在 $\\mathbf{x}_0$ 处计算必要的分量：$\\nabla f(\\mathbf{x}_0)$、$\\nabla^2 f(\\mathbf{x}_0)$、$c(\\mathbf{x}_0)$、$J(\\mathbf{x}_0)$ 和 $\\nabla^2 c_i(\\mathbf{x}_0)$。\n2.  构建梯度 $\\nabla \\Phi(\\mathbf{x}_0;\\rho_k)$ 和 Hessian 矩阵 $H_\\Phi = \\nabla^2 \\Phi(\\mathbf{x}_0;\\rho_k)$。\n3.  计算 $H_\\Phi$ 的 SVD 以找到其奇异值并计算条件数。\n4.  使用 $H_\\Phi$ 的伪逆求解 Newton 步长 $\\mathbf{p}_k$。\n5.  计算欧几里得范数 $\\lVert \\mathbf{p}_k \\rVert_2$。\n6.  为每个测试用例收集条件数和步长范数的列表。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the quadratic penalty method simulation problem.\n    Derives and computes Newton steps and Hessian condition numbers for three test cases\n    as the penalty parameter rho increases.\n    \"\"\"\n\n    def run_simulation_case(case_params):\n        \"\"\"\n        Runs the simulation for a single test case.\n\n        Args:\n            case_params (dict): A dictionary containing the functions and data for a case.\n                - 'grad_f': Gradient of the objective function.\n                - 'hess_f': Hessian of the objective function.\n                - 'c_func': Constraint function(s).\n                - 'jac_c': Jacobian of the constraint function(s).\n                - 'hess_c': List of Hessians of each constraint component.\n                - 'x0': The base point for the simulation.\n\n        Returns:\n            list: A list containing two lists:\n                  - The list of Hessian condition numbers for each rho.\n                  - The list of Newton step norms for each rho.\n        \"\"\"\n        grad_f_func = case_params['grad_f']\n        hess_f_func = case_params['hess_f']\n        c_func = case_params['c_func']\n        jac_c_func = case_params['jac_c']\n        hess_c_funcs = case_params['hess_c']\n        x0 = case_params['x0']\n\n        rho_values = [10**(-2 + k) for k in range(6)]\n        \n        condition_numbers = []\n        step_norms = []\n\n        # Evaluate problem functions at the fixed base point x0\n        grad_f_x0 = grad_f_func(x0)\n        hess_f_x0 = hess_f_func(x0)\n        c_x0 = c_func(x0)\n        jac_c_x0 = jac_c_func(x0)\n        \n        # Calculate the sum term in the Hessian formula: sum(c_i * H_ci)\n        sum_c_hess_c = np.zeros_like(hess_f_x0)\n        for i in range(len(c_x0)):\n            sum_c_hess_c += c_x0[i] * hess_c_funcs[i](x0)\n\n        for rho in rho_values:\n            # 1. Assemble the gradient and Hessian of the penalty function\n            # Gradient: grad_Phi = grad_f + rho * J^T * c\n            grad_phi = grad_f_x0 + rho * jac_c_x0.T @ c_x0\n\n            # Hessian: hess_Phi = hess_f + rho * (J^T * J + sum(c_i * H_ci))\n            hess_phi = hess_f_x0 + rho * (jac_c_x0.T @ jac_c_x0 + sum_c_hess_c)\n\n            # 2. Compute the condition number from SVD\n            try:\n                singular_values = np.linalg.svd(hess_phi, compute_uv=False)\n                s_min = singular_values[-1]\n                s_max = singular_values[0]\n                \n                # Check for singularity\n                if s_min  1e-16:\n                    cond_num = np.inf\n                else:\n                    cond_num = s_max / s_min\n            except np.linalg.LinAlgError:\n                cond_num = np.inf\n            condition_numbers.append(cond_num)\n\n            # 3. Compute the Newton step p using pseudoinverse\n            # H_phi * p = -g_phi  => p = -pinv(H_phi) * g_phi\n            try:\n                p = -np.linalg.pinv(hess_phi) @ grad_phi\n            except np.linalg.LinAlgError:\n                p = np.full_like(x0, np.nan) # Should not happen with pinv, but for robustness\n            \n            # 4. Compute the Euclidean norm of the step\n            p_norm = np.linalg.norm(p)\n            step_norms.append(p_norm)\n            \n        return [condition_numbers, step_norms]\n\n    # Definition of test cases\n    test_cases = [\n        { # Case A\n            'grad_f': lambda x: np.array([x[0], 2 * x[1]]),\n            'hess_f': lambda x: np.array([[1.0, 0.0], [0.0, 2.0]]),\n            'c_func': lambda x: np.array([x[0] + x[1] - 1.0]),\n            'jac_c': lambda x: np.array([[1.0, 1.0]]),\n            'hess_c': [lambda x: np.zeros((2, 2))],\n            'x0': np.array([0.0, 0.0])\n        },\n        { # Case B\n            'grad_f': lambda x: np.array([2 * x[0], x[1]]),\n            'hess_f': lambda x: np.array([[2.0, 0.0], [0.0, 1.0]]),\n            'c_func': lambda x: np.array([x[0]**2 + x[1] - 1.0]),\n            'jac_c': lambda x: np.array([[2 * x[0], 1.0]]),\n            'hess_c': [lambda x: np.array([[2.0, 0.0], [0.0, 0.0]])],\n            'x0': np.array([0.5, -0.2])\n        },\n        { # Case C\n            'grad_f': lambda x: np.array([1e-3 * x[0], x[1]]),\n            'hess_f': lambda x: np.array([[1e-3, 0.0], [0.0, 1.0]]),\n            'c_func': lambda x: np.array([x[0] - 0.5]),\n            'jac_c': lambda x: np.array([[1.0, 0.0]]),\n            'hess_c': [lambda x: np.zeros((2, 2))],\n            'x0': np.array([1.0, 1.0])\n        }\n    ]\n\n    all_results = []\n    for case in test_cases:\n        result = run_simulation_case(case)\n        all_results.append(result)\n\n    # Format output to match [[case_A_res], [case_B_res], ...]\n    # where each case_res is [[conds],[norms]].\n    # The map(str, ...) and join will create a string like \"[[...]],[[...]]\"\n    # which is then wrapped in \"[\" and \"]\".\n    results_str = ','.join(map(str, all_results))\n    print(f\"[{results_str}]\")\n\n\nsolve()\n```", "id": "3169152"}]}