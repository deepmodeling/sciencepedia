{"hands_on_practices": [{"introduction": "要想真正掌握一个算法，最好的方法就是亲自动手实践。第一个练习将引导你手动完成共轭梯度法的每一个计算步骤。通过处理一个简单的 $3 \\times 3$ 小型系统，你将巩固对算法各组成部分——如残差、搜索方向和步长——如何计算以及它们如何协同作用，从而在至多 $n$ 步内达到解的理解。[@problem_id:3111640]", "problem": "考虑无约束二次最小化问题，其目标函数为 $f(x) = \\tfrac{1}{2} x^{\\top} A x - b^{\\top} x$，其中 $A \\in \\mathbb{R}^{3 \\times 3}$ 是一个具有不同特征值的对称正定 (SPD) 矩阵。设\n$$\nA = \\begin{pmatrix}\n2  0  0 \\\\\n0  3  0 \\\\\n0  0  5\n\\end{pmatrix}, \\quad\nb = \\begin{pmatrix}\n1 \\\\ 1 \\\\ 1\n\\end{pmatrix}, \\quad\nx_{0} = \\begin{pmatrix}\n0 \\\\ 0 \\\\ 0\n\\end{pmatrix}.\n$$\n注意 $A$ 是对称正定矩阵，且具有不同的特征值 $2$、$3$ 和 $5$。应用共轭梯度法 (Conjugate Gradient, CG) 从 $x_{0}$ 开始最小化 $f(x)$，方法是构造两两 $A$-共轭的搜索方向，并在每一步沿当前搜索方向执行精确线搜索。按以下第一性原理进行：\n- 使用 $f(x)$ 的定义和欧几里得内积，通过最小化关于 $\\alpha$ 的 $f(x_{k} + \\alpha p_{k})$ 来推导每次迭代的步长。\n- 施加连续方向的 $A$-共轭性来确定新搜索方向的递推关系，并证明残差与搜索方向之间出现的正交关系。\n- 手动执行恰好 $3$ 次共轭梯度迭代（$k = 0, 1, 2$），计算 $x_{1}$、$x_{2}$ 和 $x_{3}$，并通过证明 $x_{3}$ 处的残差为零向量来验证终止。\n\n$f(x_{3})$ 的精确值是多少？请以单一简化分数形式给出你的答案。无需四舍五入。", "solution": "问题是使用共轭梯度法 (CG) 最小化二次函数 $f(x) = \\frac{1}{2} x^{\\top} A x - b^{\\top} x$。该函数的梯度为 $\\nabla f(x) = A x - b$。当 $\\nabla f(x) = 0$ 时达到最小值，这对应于求解线性系统 $A x = b$。在迭代点 $x_k$ 处的残差定义为 $r_k = b - A x_k = -\\nabla f(x_k)$。CG 方法生成一个迭代序列 $x_{k+1} = x_k + \\alpha_k p_k$，其中 $p_k$ 是搜索方向，$\\alpha_k$ 是步长。\n\n首先，我们通过执行精确线搜索来推导步长 $\\alpha_k$。我们将 $\\phi(\\alpha) = f(x_k + \\alpha p_k)$ 对 $\\alpha$ 进行最小化：\n$$\n\\phi(\\alpha) = \\frac{1}{2} (x_k + \\alpha p_k)^{\\top} A (x_k + \\alpha p_k) - b^{\\top} (x_k + \\alpha p_k)\n$$\n展开此表达式并利用 $A$ 的对称性 ($A=A^{\\top}$)，我们得到：\n$$\n\\phi(\\alpha) = \\frac{1}{2} x_k^{\\top} A x_k - b^{\\top} x_k + \\alpha (p_k^{\\top} A x_k - p_k^{\\top} b) + \\frac{1}{2} \\alpha^2 p_k^{\\top} A p_k\n$$\n为求最小值，我们将关于 $\\alpha$ 的导数设为零：\n$$\n\\frac{d\\phi}{d\\alpha} = p_k^{\\top} A x_k - p_k^{\\top} b + \\alpha p_k^{\\top} A p_k = p_k^{\\top} (A x_k - b) + \\alpha p_k^{\\top} A p_k = -p_k^{\\top} r_k + \\alpha p_k^{\\top} A p_k = 0\n$$\n求解 $\\alpha$，我们得到步长：\n$$\n\\alpha_k = \\frac{p_k^{\\top} r_k}{p_k^{\\top} A p_k}\n$$\n\n接下来，我们确定搜索方向 $p_k$ 的递推关系。初始搜索方向是最速下降方向，$p_0 = r_0$。对于后续步骤，新的搜索方向 $p_{k+1}$ 被构造为新残差 $r_{k+1}$ 和前一搜索方向 $p_k$ 的线性组合：\n$$\np_{k+1} = r_{k+1} + \\beta_k p_k\n$$\n选择系数 $\\beta_k$ 以强制实现连续搜索方向之间的 $A$-共轭性，即 $p_{k+1}^{\\top} A p_k = 0$。\n$$\n(r_{k+1} + \\beta_k p_k)^{\\top} A p_k = 0 \\implies r_{k+1}^{\\top} A p_k + \\beta_k p_k^{\\top} A p_k = 0\n$$\n这给出了 $\\beta_k$ 的 Hestenes-Stiefel 公式：\n$$\n\\beta_k = -\\frac{r_{k+1}^{\\top} A p_k}{p_k^{\\top} A p_k}\n$$\n残差更新为 $r_{k+1} = r_k - \\alpha_k A p_k$，由此我们得到 $A p_k = \\frac{1}{\\alpha_k} (r_k - r_{k+1})$。将此代入 $\\beta_k$ 的分子中：\n$$\nr_{k+1}^{\\top} A p_k = \\frac{1}{\\alpha_k} r_{k+1}^{\\top} (r_k - r_{k+1}) = -\\frac{1}{\\alpha_k} r_{k+1}^{\\top} r_{k+1}\n$$\n这里我们使用了连续残差是正交的这一事实，$r_{k+1}^{\\top} r_k = 0$。此外，搜索方向更新 $p_k = r_k + \\beta_{k-1} p_{k-1}$ 和残差的正交性 $r_k^{\\top} r_j=0$（对于 $j  k$）意味着 $p_k^{\\top} r_k = (r_k + \\beta_{k-1} p_{k-1})^{\\top} r_k = r_k^{\\top} r_k$。这将步长公式简化为 $\\alpha_k = \\frac{r_k^{\\top} r_k}{p_k^{\\top} A p_k}$。\n将此代入 $\\beta_k$ 的表达式中：\n$$\n\\beta_k = -\\frac{- (r_{k+1}^{\\top} r_{k+1}) / \\alpha_k}{p_k^{\\top} A p_k} = \\frac{r_{k+1}^{\\top} r_{k+1}}{\\alpha_k (p_k^{\\top} A p_k)} = \\frac{r_{k+1}^{\\top} r_{k+1}}{(\\frac{r_k^{\\top} r_k}{p_k^{\\top} A p_k}) (p_k^{\\top} A p_k)} = \\frac{r_{k+1}^{\\top} r_{k+1}}{r_k^{\\top} r_k}\n$$\n这是 $\\beta_k$ 的 Fletcher-Reeves 公式，我们将用它进行计算。\n\n给定 $A = \\begin{pmatrix} 2  0  0 \\\\ 0  3  0 \\\\ 0  0  5 \\end{pmatrix}$，$b = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix}$，以及 $x_0 = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}$。\n\n**迭代 $k=0$:**\n1.  初始残差：$r_0 = b - A x_0 = b = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix}$。\n2.  初始搜索方向：$p_0 = r_0 = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix}$。\n3.  计算 $r_0^{\\top} r_0 = 1^2 + 1^2 + 1^2 = 3$。\n4.  计算 $A p_0 = \\begin{pmatrix} 2 \\\\ 3 \\\\ 5 \\end{pmatrix}$。\n5.  计算 $p_0^{\\top} A p_0 = \\begin{pmatrix} 1  1  1 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ 3 \\\\ 5 \\end{pmatrix} = 2+3+5 = 10$。\n6.  步长：$\\alpha_0 = \\frac{r_0^{\\top} r_0}{p_0^{\\top} A p_0} = \\frac{3}{10}$。\n7.  更新解：$x_1 = x_0 + \\alpha_0 p_0 = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix} + \\frac{3}{10} \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 3/10 \\\\ 3/10 \\\\ 3/10 \\end{pmatrix}$。\n8.  更新残差：$r_1 = r_0 - \\alpha_0 A p_0 = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix} - \\frac{3}{10} \\begin{pmatrix} 2 \\\\ 3 \\\\ 5 \\end{pmatrix} = \\begin{pmatrix} 1 - 6/10 \\\\ 1 - 9/10 \\\\ 1 - 15/10 \\end{pmatrix} = \\begin{pmatrix} 4/10 \\\\ 1/10 \\\\ -5/10 \\end{pmatrix} = \\frac{1}{10} \\begin{pmatrix} 4 \\\\ 1 \\\\ -5 \\end{pmatrix}$。\n\n**迭代 $k=1$:**\n1.  计算 $r_1^{\\top} r_1 = (\\frac{1}{10})^2 (4^2 + 1^2 + (-5)^2) = \\frac{1}{100}(16+1+25) = \\frac{42}{100} = \\frac{21}{50}$。\n2.  计算 $\\beta_0 = \\frac{r_1^{\\top} r_1}{r_0^{\\top} r_0} = \\frac{21/50}{3} = \\frac{7}{50}$。\n3.  更新搜索方向：$p_1 = r_1 + \\beta_0 p_0 = \\frac{1}{10} \\begin{pmatrix} 4 \\\\ 1 \\\\ -5 \\end{pmatrix} + \\frac{7}{50} \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix} = \\frac{1}{50} \\begin{pmatrix} 20 \\\\ 5 \\\\ -25 \\end{pmatrix} + \\frac{1}{50} \\begin{pmatrix} 7 \\\\ 7 \\\\ 7 \\end{pmatrix} = \\frac{1}{50} \\begin{pmatrix} 27 \\\\ 12 \\\\ -18 \\end{pmatrix}$。\n4.  计算 $A p_1 = \\frac{1}{50} \\begin{pmatrix} 2  0  0 \\\\ 0  3  0 \\\\ 0  0  5 \\end{pmatrix} \\begin{pmatrix} 27 \\\\ 12 \\\\ -18 \\end{pmatrix} = \\frac{1}{50} \\begin{pmatrix} 54 \\\\ 36 \\\\ -90 \\end{pmatrix}$。\n5.  计算 $p_1^{\\top} A p_1 = \\frac{1}{50} \\begin{pmatrix} 27  12  -18 \\end{pmatrix} \\frac{1}{50} \\begin{pmatrix} 54 \\\\ 36 \\\\ -90 \\end{pmatrix} = \\frac{1}{2500}(27 \\cdot 54 + 12 \\cdot 36 - 18 \\cdot (-90)) = \\frac{1458+432+1620}{2500} = \\frac{3510}{2500} = \\frac{351}{250}$。\n6.  步长：$\\alpha_1 = \\frac{r_1^{\\top} r_1}{p_1^{\\top} A p_1} = \\frac{21/50}{351/250} = \\frac{21}{50} \\cdot \\frac{250}{351} = \\frac{21 \\cdot 5}{351} = \\frac{105}{351} = \\frac{35}{117}$。\n7.  更新解：$x_2 = x_1 + \\alpha_1 p_1 = \\begin{pmatrix} 3/10 \\\\ 3/10 \\\\ 3/10 \\end{pmatrix} + \\frac{35}{117} \\frac{1}{50} \\begin{pmatrix} 27 \\\\ 12 \\\\ -18 \\end{pmatrix} = \\frac{1}{10}\\begin{pmatrix} 3 \\\\ 3 \\\\ 3 \\end{pmatrix} + \\frac{7}{1170} \\begin{pmatrix} 27 \\\\ 12 \\\\ -18 \\end{pmatrix} = \\frac{117}{1170} \\begin{pmatrix} 3 \\\\ 3 \\\\ 3 \\end{pmatrix} + \\frac{1}{1170} \\begin{pmatrix} 189 \\\\ 84 \\\\ -126 \\end{pmatrix} = \\frac{1}{1170} \\begin{pmatrix} 351+189 \\\\ 351+84 \\\\ 351-126 \\end{pmatrix} = \\frac{1}{1170} \\begin{pmatrix} 540 \\\\ 435 \\\\ 225 \\end{pmatrix} = \\frac{15}{1170} \\begin{pmatrix} 36 \\\\ 29 \\\\ 15 \\end{pmatrix} = \\frac{1}{78} \\begin{pmatrix} 36 \\\\ 29 \\\\ 15 \\end{pmatrix}$。\n8.  更新残差：$r_2 = r_1 - \\alpha_1 A p_1 = \\frac{1}{10} \\begin{pmatrix} 4 \\\\ 1 \\\\ -5 \\end{pmatrix} - \\frac{35}{117} \\frac{1}{50} \\begin{pmatrix} 54 \\\\ 36 \\\\ -90 \\end{pmatrix} = \\frac{1}{10} \\begin{pmatrix} 4 \\\\ 1 \\\\ -5 \\end{pmatrix} - \\frac{7}{65} \\begin{pmatrix} 3 \\\\ 2 \\\\ -5 \\end{pmatrix} = \\frac{1}{130} \\begin{pmatrix} 52-42 \\\\ 13-28 \\\\ -65+70 \\end{pmatrix} = \\frac{1}{130} \\begin{pmatrix} 10 \\\\ -15 \\\\ 5 \\end{pmatrix} = \\frac{1}{26} \\begin{pmatrix} 2 \\\\ -3 \\\\ 1 \\end{pmatrix}$。\n\n**迭代 $k=2$:**\n1.  计算 $r_2^{\\top} r_2 = (\\frac{1}{26})^2 (2^2 + (-3)^2 + 1^2) = \\frac{1}{676}(4+9+1) = \\frac{14}{676} = \\frac{7}{338}$。\n2.  计算 $\\beta_1 = \\frac{r_2^{\\top} r_2}{r_1^{\\top} r_1} = \\frac{7/338}{21/50} = \\frac{7}{338} \\cdot \\frac{50}{21} = \\frac{1}{338} \\frac{50}{3} = \\frac{25}{507}$。\n3.  更新搜索方向：$p_2 = r_2 + \\beta_1 p_1 = \\frac{1}{26} \\begin{pmatrix} 2 \\\\ -3 \\\\ 1 \\end{pmatrix} + \\frac{25}{507} \\frac{1}{50} \\begin{pmatrix} 27 \\\\ 12 \\\\ -18 \\end{pmatrix} = \\frac{1}{26} \\begin{pmatrix} 2 \\\\ -3 \\\\ 1 \\end{pmatrix} + \\frac{1}{1014} \\begin{pmatrix} 27 \\\\ 12 \\\\ -18 \\end{pmatrix} = \\frac{39}{1014} \\begin{pmatrix} 2 \\\\ -3 \\\\ 1 \\end{pmatrix} + \\frac{1}{1014} \\begin{pmatrix} 27 \\\\ 12 \\\\ -18 \\end{pmatrix} = \\frac{1}{1014} \\begin{pmatrix} 78+27 \\\\ -117+12 \\\\ 39-18 \\end{pmatrix} = \\frac{1}{1014} \\begin{pmatrix} 105 \\\\ -105 \\\\ 21 \\end{pmatrix} = \\frac{21}{1014} \\begin{pmatrix} 5 \\\\ -5 \\\\ 1 \\end{pmatrix} = \\frac{7}{338} \\begin{pmatrix} 5 \\\\ -5 \\\\ 1 \\end{pmatrix}$。\n4.  计算 $A p_2 = \\frac{7}{338} \\begin{pmatrix} 2  0  0 \\\\ 0  3  0 \\\\ 0  0  5 \\end{pmatrix} \\begin{pmatrix} 5 \\\\ -5 \\\\ 1 \\end{pmatrix} = \\frac{7}{338} \\begin{pmatrix} 10 \\\\ -15 \\\\ 5 \\end{pmatrix} = \\frac{35}{338} \\begin{pmatrix} 2 \\\\ -3 \\\\ 1 \\end{pmatrix}$。\n5.  计算 $p_2^{\\top} A p_2 = \\frac{7}{338} \\begin{pmatrix} 5  -5  1 \\end{pmatrix} \\frac{35}{338} \\begin{pmatrix} 2 \\\\ -3 \\\\ 1 \\end{pmatrix} = \\frac{245}{338^2}(10+15+1) = \\frac{245 \\cdot 26}{338^2} = \\frac{245}{338 \\cdot 13} = \\frac{245}{4394}$。\n6.  步长：$\\alpha_2 = \\frac{r_2^{\\top} r_2}{p_2^{\\top} A p_2} = \\frac{7/338}{245/4394} = \\frac{7}{338} \\frac{4394}{245} = \\frac{7 \\cdot 13}{245} = \\frac{91}{245} = \\frac{13}{35}$。\n7.  更新解：$x_3 = x_2 + \\alpha_2 p_2 = \\frac{1}{78} \\begin{pmatrix} 36 \\\\ 29 \\\\ 15 \\end{pmatrix} + \\frac{13}{35} \\frac{7}{338} \\begin{pmatrix} 5 \\\\ -5 \\\\ 1 \\end{pmatrix} = \\frac{1}{78} \\begin{pmatrix} 36 \\\\ 29 \\\\ 15 \\end{pmatrix} + \\frac{1}{5 \\cdot 26} \\begin{pmatrix} 5 \\\\ -5 \\\\ 1 \\end{pmatrix} = \\frac{1}{78} \\begin{pmatrix} 36 \\\\ 29 \\\\ 15 \\end{pmatrix} + \\frac{1}{130} \\begin{pmatrix} 5 \\\\ -5 \\\\ 1 \\end{pmatrix} = \\frac{5}{390} \\begin{pmatrix} 36 \\\\ 29 \\\\ 15 \\end{pmatrix} + \\frac{3}{390} \\begin{pmatrix} 5 \\\\ -5 \\\\ 1 \\end{pmatrix} = \\frac{1}{390} \\begin{pmatrix} 180+15 \\\\ 145-15 \\\\ 75+3 \\end{pmatrix} = \\frac{1}{390} \\begin{pmatrix} 195 \\\\ 130 \\\\ 78 \\end{pmatrix} = \\begin{pmatrix} 195/390 \\\\ 130/390 \\\\ 78/390 \\end{pmatrix} = \\begin{pmatrix} 1/2 \\\\ 1/3 \\\\ 1/5 \\end{pmatrix}$。\n8.  更新残差：$r_3 = r_2 - \\alpha_2 A p_2 = \\frac{1}{26} \\begin{pmatrix} 2 \\\\ -3 \\\\ 1 \\end{pmatrix} - \\frac{13}{35} \\frac{35}{338} \\begin{pmatrix} 2 \\\\ -3 \\\\ 1 \\end{pmatrix} = \\frac{1}{26} \\begin{pmatrix} 2 \\\\ -3 \\\\ 1 \\end{pmatrix} - \\frac{13}{338} \\begin{pmatrix} 2 \\\\ -3 \\\\ 1 \\end{pmatrix} = \\frac{1}{26} \\begin{pmatrix} 2 \\\\ -3 \\\\ 1 \\end{pmatrix} - \\frac{1}{26} \\begin{pmatrix} 2 \\\\ -3 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}$。\n\n残差 $r_3$ 是零向量，这验证了该方法已在 3 次迭代中收敛到精确解，正如对一个 $3 \\times 3$ 系统所预期的那样。\n最终解是 $x_3 = x^* = A^{-1} b = \\begin{pmatrix} 1/2 \\\\ 1/3 \\\\ 1/5 \\end{pmatrix}$。\n\n我们被要求计算 $f(x_3)$。\n$$\nf(x_3) = \\frac{1}{2} x_3^{\\top} A x_3 - b^{\\top} x_3\n$$\n由于 $r_3 = b - Ax_3 = 0$，我们有 $Ax_3 = b$。将此代入 $f(x_3)$ 的表达式中：\n$$\nf(x_3) = \\frac{1}{2} x_3^{\\top} b - b^{\\top} x_3 = -\\frac{1}{2} b^{\\top} x_3\n$$\n现在我们计算其值：\n$$\nb^{\\top} x_3 = \\begin{pmatrix} 1  1  1 \\end{pmatrix} \\begin{pmatrix} 1/2 \\\\ 1/3 \\\\ 1/5 \\end{pmatrix} = \\frac{1}{2} + \\frac{1}{3} + \\frac{1}{5} = \\frac{15}{30} + \\frac{10}{30} + \\frac{6}{30} = \\frac{31}{30}\n$$\n因此，函数的最小值为：\n$$\nf(x_3) = -\\frac{1}{2} \\left( \\frac{31}{30} \\right) = -\\frac{31}{60}\n$$", "answer": "$$\\boxed{-\\frac{31}{60}}$$", "id": "3111640"}, {"introduction": "在观察了共轭梯度法的迭代过程之后，让我们来思考一个特殊的理想情境。本练习探讨当二次目标函数的等值面是完美的球面时（即其Hessian矩阵是单位矩阵的倍数），会发生什么。理解这个简单的案例，能够揭示为何共轭梯度法如此强大，以及为什么系统矩阵的特征值分布（或条件数）是其性能的关键决定因素。[@problem_id:2211314]", "problem": "考虑多元二次函数 $f(\\mathbf{x})$ 的最小化问题，其定义为：\n$$f(\\mathbf{x}) = \\frac{1}{2} c \\mathbf{x}^T \\mathbf{x} - \\mathbf{b}^T \\mathbf{x}$$\n其中，对于某个整数 $n \\ge 1$，$\\mathbf{x}$ 和 $\\mathbf{b}$ 是 $\\mathbb{R}^n$ 中的列向量，而 $c$ 是一个正实值常数。\n\n从一个梯度非零的任意初始点 $\\mathbf{x}_0$ 开始，执行一次共轭梯度（CG）方法的迭代来寻找下一个点 $\\mathbf{x}_1$。\n\n确定 $\\mathbf{x}_1$ 的表达式。请用常数 $c$ 以及向量 $\\mathbf{b}$ 和 $\\mathbf{x}_0$ 来表示你的答案。", "solution": "我们考虑二次函数 $f(\\mathbf{x})=\\frac{1}{2}c\\,\\mathbf{x}^{T}\\mathbf{x}-\\mathbf{b}^{T}\\mathbf{x}$，其中 $c0$。其梯度和Hessian矩阵为\n$$\n\\nabla f(\\mathbf{x})=c\\,\\mathbf{x}-\\mathbf{b}, \\qquad \\nabla^{2}f(\\mathbf{x})=c\\,\\mathbf{I},\n$$\n因此，最小化 $f$ 等价于求解线性系统\n$$\n\\mathbf{A}\\mathbf{x}=\\mathbf{b}, \\quad \\text{with} \\quad \\mathbf{A}=c\\,\\mathbf{I},\n$$\n该系统是对称正定的。\n\n在共轭梯度法中，从梯度非零的 $\\mathbf{x}_{0}$ 开始，定义第一次迭代的残差和搜索方向为\n$$\n\\mathbf{r}_{0}=\\mathbf{b}-\\mathbf{A}\\mathbf{x}_{0}=\\mathbf{b}-c\\,\\mathbf{x}_{0}, \\qquad \\mathbf{p}_{0}=\\mathbf{r}_{0}.\n$$\n步长 $\\alpha_{0}$ 由下式给出\n$$\n\\alpha_{0}=\\frac{\\mathbf{r}_{0}^{T}\\mathbf{r}_{0}}{\\mathbf{p}_{0}^{T}\\mathbf{A}\\mathbf{p}_{0}}=\\frac{\\mathbf{r}_{0}^{T}\\mathbf{r}_{0}}{\\mathbf{r}_{0}^{T}(c\\,\\mathbf{I})\\mathbf{r}_{0}}=\\frac{\\mathbf{r}_{0}^{T}\\mathbf{r}_{0}}{c\\,\\mathbf{r}_{0}^{T}\\mathbf{r}_{0}}=\\frac{1}{c}.\n$$\n因此，经过一次CG迭代后更新的点是\n$$\n\\mathbf{x}_{1}=\\mathbf{x}_{0}+\\alpha_{0}\\mathbf{p}_{0}=\\mathbf{x}_{0}+\\frac{1}{c}\\left(\\mathbf{b}-c\\,\\mathbf{x}_{0}\\right)=\\frac{1}{c}\\,\\mathbf{b}.\n$$\n因此，由于 $\\mathbf{A}=c\\,\\mathbf{I}$ 只有一个特征值，单次CG步骤即可达到精确的最小值点。", "answer": "$$\\boxed{\\frac{1}{c}\\,\\mathbf{b}}$$", "id": "2211314"}, {"introduction": "现在，我们将从手动计算转向计算探索，以获得更深刻的洞察力。本练习通过在系统矩阵 $A$ 的特征基中检视共轭梯度法的行为，来探究其收敛方式。你将编写一个程序来观察该算法如何优先快速地消除与极大和极小特征值相关的误差分量，这为该方法的收敛理论提供了强有力的视觉和定量证据。[@problem_id:3111694]", "problem": "考虑严格凸二次函数 $f(\\mathbf{x}) = \\tfrac{1}{2}\\,\\mathbf{x}^{\\top} A \\mathbf{x} - \\mathbf{b}^{\\top} \\mathbf{x}$ 的无约束最小化问题，其中 $A \\in \\mathbb{R}^{n \\times n}$ 是对称正定 (SPD) 矩阵，$\\mathbf{b} \\in \\mathbb{R}^{n}$。共轭梯度 (CG) 法通过使用 $A$-共轭的搜索方向迭代更新 $\\mathbf{x}_{k}$ 来解决此问题，而无需构造 $A^{-1}$。从第一性原理出发，可以通过误差 $\\mathbf{e}_{k} = \\mathbf{x}_{k} - \\mathbf{x}^{\\star}$ 来刻画 CG 的迭代量 $\\mathbf{x}_{k}$，其中 $\\mathbf{x}^{\\star}$ 是满足 $A \\mathbf{x}^{\\star} = \\mathbf{b}$ 的唯一最小化子。当 $A$ 具有特征分解 $A = U \\Lambda U^{\\top}$，其中 $U = [\\mathbf{u}_{1}, \\mathbf{u}_{2}, \\ldots, \\mathbf{u}_{n}]$ 是标准正交特征向量，$\\Lambda = \\operatorname{diag}(\\lambda_{1}, \\ldots, \\lambda_{n})$ 是特征值时，误差沿特征基的分量由投影 $\\alpha_{i}^{(k)} = \\mathbf{u}_{i}^{\\top} \\mathbf{e}_{k}$ 给出。根据经验和理论观察，CG 迭代会以特有的模式逐步抑制与 $A$ 的极端特征值（非常大和非常小）相关的误差分量。\n\n你的任务是编写一个完整、可运行的程序，该程序：\n- 为 SPD 矩阵 $A$ 实现共轭梯度 (CG) 方法，以最小化 $\\mathbb{R}^{3}$ 中的 $f(\\mathbf{x})$。\n- 对于给定的测试用例，构造具有已知特征向量和特征值的 $A$，设置 $\\mathbf{x}^{\\star}$ 和 $\\mathbf{x}_{0}$，运行 CG 恰好 $n$ 次迭代（$n=3$），并记录在每次迭代 $k \\in \\{0,1,2,3\\}$ 时，每个特征向量 $\\mathbf{u}_{i}$ 对应的误差投影 $\\alpha_{i}^{(k)} = \\mathbf{u}_{i}^{\\top} (\\mathbf{x}_{k} - \\mathbf{x}^{\\star})$。\n- 对于每个特征分量 $i$，将“湮灭迭代” $t_{i}$ 定义为满足 $\\left|\\alpha_{i}^{(k)}\\right| \\leq \\tau \\left|\\alpha_{i}^{(0)}\\right|$ 的最小 $k \\in \\{0,1,2,3\\}$，其中 $\\tau \\in (0,1)$ 是一个给定的阈值分数。\n- 确定 CG 是否表现出以下湮灭时间顺序：与最大特征值对齐的分量比与中间特征值对齐的分量更早湮灭，而后者又比与最小特征值对齐的分量更早湮灭。形式上，设特征值排序为 $\\lambda_{\\min} \\leq \\lambda_{\\text{mid}} \\leq \\lambda_{\\max}$，验证 $t_{\\max}  t_{\\text{mid}}  t_{\\min}$ 是否成立。\n\n使用以下覆盖不同行为的测试套件：\n- 测试用例 1 (具有强谱分离的理想情况): $A = \\operatorname{diag}([1, 10, 100])$, $\\mathbf{x}^{\\star} = [1, 1, 1]^{\\top}$, $\\mathbf{x}_{0} = [0, 0, 0]^{\\top}$, 阈值 $\\tau = 0.05$。\n- 测试用例 2 (具有非平凡特征基的相同谱): 令 $U = R_{z}(\\theta) R_{x}(\\phi)$，其中 $R_{z}(\\theta) = \\begin{bmatrix}\\cos \\theta  -\\sin \\theta  0 \\\\ \\sin \\theta  \\cos \\theta  0 \\\\ 0  0  1 \\end{bmatrix}$ 且 $R_{x}(\\phi) = \\begin{bmatrix} 1  0  0 \\\\ 0  \\cos \\phi  -\\sin \\phi \\\\ 0  \\sin \\phi  \\cos \\phi \\end{bmatrix}$，角度以弧度为单位指定。取 $\\theta = \\pi/4$, $\\phi = \\pi/3$，设 $\\Lambda = \\operatorname{diag}([1, 10, 100])$，且 $A = U \\Lambda U^{\\top}$。使用 $\\mathbf{x}^{\\star} = [1, 1, 1]^{\\top}$, $\\mathbf{x}_{0} = [0, 0, 0]^{\\top}$，阈值 $\\tau = 0.05$。\n- 测试用例 3 (特征值相近): $A = \\operatorname{diag}([10, 11, 12])$, $\\mathbf{x}^{\\star} = [1, 1, 1]^{\\top}$, $\\mathbf{x}_{0} = [0, 0, 0]^{\\top}$, 阈值 $\\tau = 0.05$。\n- 测试用例 4 (病态谱): $A = \\operatorname{diag}([10^{-3}, 1, 10^{3}])$, $\\mathbf{x}^{\\star} = [1, 1, 1]^{\\top}$, $\\mathbf{x}_{0} = [0, 0, 0]^{\\top}$, 阈值 $\\tau = 0.05$。\n\n对于每个测试用例：\n1. 计算特征分解 $A = U \\Lambda U^{\\top}$ 并对特征值进行排序，使得 $\\lambda_{\\min} \\leq \\lambda_{\\text{mid}} \\leq \\lambda_{\\max}$，对应的特征向量为 $\\mathbf{u}_{\\min}$、$\\mathbf{u}_{\\text{mid}}$ 和 $\\mathbf{u}_{\\max}$。\n2. 从 $\\mathbf{x}_{0}$ 开始，运行共轭梯度法恰好 $n = 3$ 次迭代，其中 $\\mathbf{b} = A \\mathbf{x}^{\\star}$，并记录 $k = 0, 1, 2, 3$ 时的 $\\mathbf{x}_{k}$。\n3. 使用阈值分数 $\\tau$ 计算如上定义的 $t_{\\min}$、$t_{\\text{mid}}$ 和 $t_{\\max}$。\n4. 输出一个布尔值，指示 $t_{\\max}  t_{\\text{mid}}  t_{\\min}$ 是否满足。\n\n你的程序应该生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果，例如 $[r_{1}, r_{2}, r_{3}, r_{4}]$，其中每个 $r_{i}$ 是测试用例 $i$ 的布尔值。角度只允许使用弧度。不涉及物理单位。最终输出必须是布尔值。该单行打印是你的程序应该产生的唯一输出。", "solution": "用户提供了一个有效的问题。\n该问题要求研究共轭梯度 (CG) 法在最小化严格凸二次函数 $f(\\mathbf{x}) = \\tfrac{1}{2}\\mathbf{x}^{\\top} A \\mathbf{x} - \\mathbf{b}^{\\top} \\mathbf{x}$ 时的收敛行为。矩阵 $A \\in \\mathbb{R}^{n \\times n}$ 是对称正定的 (SPD)，维度指定为 $n=3$。$f(\\mathbf{x})$ 的唯一最小化子 $\\mathbf{x}^{\\star}$ 是线性系统 $A \\mathbf{x}^{\\star} = \\mathbf{b}$ 的解。问题的核心是验证一个关于 CG 减小与 $A$ 的特征向量对齐的误差分量的速率的假设。具体来说，我们测试与最大特征值对应的误差分量是否比与中间特征值对应的误差分量被抑制得更快，而后者又比与最小特征值对应的误差分量被抑制得更快。\n\n**共轭梯度 (CG) 算法**\nCG 方法是一种迭代算法，从初始猜测 $\\mathbf{x}_{0}$ 开始，生成一系列收敛到 $\\mathbf{x}^{\\star}$ 的近似值 $\\{\\mathbf{x}_{k}\\}$。给定迭代 $k$ 的更新规则如下：\n\n初始化：\n$\\mathbf{r}_{0} = \\mathbf{b} - A \\mathbf{x}_{0}$\n$\\mathbf{p}_{0} = \\mathbf{r}_{0}$\n\n对于 $k = 0, 1, 2, \\ldots$：\n步长：$\\alpha_{k} = \\frac{\\mathbf{r}_{k}^{\\top} \\mathbf{r}_{k}}{\\mathbf{p}_{k}^{\\top} A \\mathbf{p}_{k}}$\n更新解：$\\mathbf{x}_{k+1} = \\mathbf{x}_{k} + \\alpha_{k} \\mathbf{p}_{k}$\n更新残差：$\\mathbf{r}_{k+1} = \\mathbf{r}_{k} - \\alpha_{k} A \\mathbf{p}_{k}$\n更新方向：$\\beta_{k} = \\frac{\\mathbf{r}_{k+1}^{\\top} \\mathbf{r}_{k+1}}{\\mathbf{r}_{k}^{\\top} \\mathbf{r}_{k}}$，然后 $\\mathbf{p}_{k+1} = \\mathbf{r}_{k+1} + \\beta_{k} \\mathbf{p}_{k}$\n\n搜索方向 $\\{\\mathbf{p}_k\\}$ 被构造成 $A$-共轭的，即对于 $i \\neq j$ 有 $\\mathbf{p}_i^\\top A \\mathbf{p}_j = 0$。\n\n**特征基中的误差分析**\n第 $k$ 次迭代的误差是向量 $\\mathbf{e}_{k} = \\mathbf{x}_{k} - \\mathbf{x}^{\\star}$。由于 $A$ 是 SPD 矩阵，它有一套完备的标准正交特征向量 $\\mathbf{u}_{1}, \\ldots, \\mathbf{u}_{n}$，对应正特征值 $\\lambda_{1}, \\ldots, \\lambda_{n}$。我们可以在这个特征基中表示误差 $\\mathbf{e}_{k}$。误差沿着每个特征向量 $\\mathbf{u}_{i}$ 的分量由投影 $\\alpha_{i}^{(k)} = \\mathbf{u}_{i}^{\\top} \\mathbf{e}_{k}$ 给出。该问题研究了当 $k$ 增加时量值 $|\\alpha_{i}^{(k)}|$ 的行为。\n\n**验证过程**\n该解决方案以一个程序实现，该程序对每个测试用例执行以下步骤：\n1.  **系统设置**：按规定构造矩阵 $A \\in \\mathbb{R}^{3 \\times 3}$。对于给定的最小化子 $\\mathbf{x}^{\\star} = [1, 1, 1]^{\\top}$，向量 $\\mathbf{b}$ 计算为 $\\mathbf{b} = A \\mathbf{x}^{\\star}$。\n2.  **特征分解**：程序计算 $A$ 的特征分解以找到其特征值和特征向量。特征值按升序排序，$\\lambda_{\\min} \\leq \\lambda_{\\text{mid}} \\leq \\lambda_{\\max}$，它们对应的特征向量被识别为 $\\mathbf{u}_{\\min}$、$\\mathbf{u}_{\\text{mid}}$ 和 $\\mathbf{u}_{\\max}$。\n3.  **CG 迭代**：从初始猜测 $\\mathbf{x}_{0} = [0, 0, 0]^{\\top}$ 开始，CG 算法精确执行 $n=3$ 次迭代。这将生成迭代序列 $\\mathbf{x}_{0}, \\mathbf{x}_{1}, \\mathbf{x}_{2}, \\mathbf{x}_{3}$。\n4.  **湮灭时间计算**：对于每个特征方向 $i \\in \\{\\min, \\text{mid}, \\max\\}$，确定“湮灭迭代” $t_{i}$。它被定义为最小的迭代索引 $k \\in \\{0, 1, 2, 3\\}$，使得相对误差投影的量值降至给定阈值 $\\tau = 0.05$ 以下。形式上，$t_{i} = \\min \\left\\{ k \\in \\{0,1,2,3\\} \\, \\Big| \\, \\left|\\alpha_{i}^{(k)}\\right| \\leq \\tau \\left|\\alpha_{i}^{(0)}\\right| \\right\\}$。在精确算术中，CG 最多在 $n$ 步内收敛，因此 $\\mathbf{e}_{3} = \\mathbf{0}$，保证了 $\\alpha_{i}^{(3)} = 0$。这确保了湮灭条件最迟在 $k=3$ 时总能满足，因此 $t_i$ 在集合 $\\{0, 1, 2, 3\\}$ 内总是有明确定义的。\n5.  **假设检验**：程序评估布尔条件 $t_{\\max}  t_{\\text{mid}}  t_{\\min}$。此关系形式化了与较大特征值相关的误差分量被更快抑制的假设。如果条件满足，程序输出 `True`，否则输出 `False`。对所有四个提供的测试用例重复此过程。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef conjugate_gradient(A, b, x0, n_iter):\n    \"\"\"\n    Implements the Conjugate Gradient method for solving Ax=b.\n    Runs for exactly n_iter iterations and returns all intermediate solutions.\n    \"\"\"\n    x = x0.copy()\n    r = b - A @ x\n    p = r.copy()\n    \n    # Check for the trivial case where the initial guess is already the solution.\n    if np.linalg.norm(r)  1e-15:\n        return [x0.copy()] * (n_iter + 1)\n        \n    rs_old = r.T @ r\n    iterates = [x0.copy()]\n    \n    for _ in range(n_iter):\n        Ap = A @ p\n        \n        # Denominator of alpha can be zero if p is in the null space of A.\n        # For an SPD matrix A, this only happens if p=0.\n        # p=0 implies r=0, so the algorithm would have converged.\n        denom = p.T @ Ap\n        if np.isclose(denom, 0):\n            # This indicates convergence has been reached.\n            # Continue filling iterates list with the current solution.\n            for _ in range(n_iter - len(iterates) + 1):\n                iterates.append(x.copy())\n            return iterates\n\n        alpha = rs_old / denom\n        \n        x = x + alpha * p\n        r = r - alpha * Ap\n        \n        rs_new = r.T @ r\n        \n        # In exact arithmetic, p cannot be zero unless r is, so rs_old won't be zero.\n        # Floating point arithmetic can lead to issues, though unlikely in this controlled problem.\n        beta = rs_new / rs_old\n        p = r + beta * p\n        rs_old = rs_new\n        \n        iterates.append(x.copy())\n        \n    return iterates\n\ndef analyze_case(A, x_star, x0, tau):\n    \"\"\"\n    Analyzes a single test case according to the problem description.\n    \"\"\"\n    n = A.shape[0]\n    b = A @ x_star\n    \n    # Step 1: Compute eigendecomposition.\n    # np.linalg.eigh returns eigenvalues in ascending order, which is required.\n    lambdas, U = np.linalg.eigh(A)\n    \n    # Eigenvectors corresponding to lambda_min, lambda_mid, lambda_max.\n    eigenvectors = {'min': U[:, 0], 'mid': U[:, 1], 'max': U[:, 2]}\n\n    # Step 2: Run Conjugate Gradient for n=3 iterations.\n    # The list will contain x_0, x_1, x_2, x_3.\n    x_k_list = conjugate_gradient(A, b, x0, n)\n    \n    # Step 3: Compute error projections and annihilation times.\n    annihilation_times = {}\n    \n    # Compute initial error and its projections.\n    e0 = x_k_list[0] - x_star\n    alpha_i_0 = {\n        'min': eigenvectors['min'].T @ e0,\n        'mid': eigenvectors['mid'].T @ e0,\n        'max': eigenvectors['max'].T @ e0\n    }\n    \n    for i_key in ['min', 'mid', 'max']:\n        initial_proj_mag = abs(alpha_i_0[i_key])\n        \n        # Handle case where initial projection is already zero.\n        if np.isclose(initial_proj_mag, 0):\n            annihilation_times[i_key] = 0\n            continue\n\n        # Find the smallest k where the annihilation condition is met.\n        found = False\n        for k in range(n + 1):  # k from 0 to 3\n            ek = x_k_list[k] - x_star\n            alpha_i_k = eigenvectors[i_key].T @ ek\n            \n            if abs(alpha_i_k) = tau * initial_proj_mag:\n                annihilation_times[i_key] = k\n                found = True\n                break\n        \n        # This fallback is for safety but should not be reached in exact arithmetic,\n        # as CG converges in n steps, making e_n = 0.\n        if not found:\n            annihilation_times[i_key] = n + 1 \n\n    t_min = annihilation_times['min']\n    t_mid = annihilation_times['mid']\n    t_max = annihilation_times['max']\n\n    # Step 4: Output boolean indicating if t_max  t_mid  t_min is satisfied.\n    return t_max  t_mid  t_min\n\ndef solve():\n    # Define common parameters for all test cases.\n    x_star = np.array([1.0, 1.0, 1.0])\n    x0 = np.array([0.0, 0.0, 0.0])\n    tau = 0.05\n\n    # Test Case 1: Happy path with strong spectral separation\n    A1 = np.diag([1.0, 10.0, 100.0])\n    \n    # Test Case 2: Same spectrum with a nontrivial eigenbasis\n    theta = np.pi / 4.0\n    phi = np.pi / 3.0\n    cos_t, sin_t = np.cos(theta), np.sin(theta)\n    cos_p, sin_p = np.cos(phi), np.sin(phi)\n    \n    Rz = np.array([[cos_t, -sin_t, 0.0], \n                   [sin_t, cos_t, 0.0], \n                   [0.0, 0.0, 1.0]])\n    Rx = np.array([[1.0, 0.0, 0.0], \n                   [0.0, cos_p, -sin_p], \n                   [0.0, sin_p, cos_p]])\n                   \n    U = Rz @ Rx\n    Lambda = np.diag([1.0, 10.0, 100.0])\n    A2 = U @ Lambda @ U.T\n\n    # Test Case 3: Near-equal eigenvalues\n    A3 = np.diag([10.0, 11.0, 12.0])\n\n    # Test Case 4: Ill-conditioned spectrum\n    A4 = np.diag([1e-3, 1.0, 1e3])\n    \n    test_cases_matrices = [A1, A2, A3, A4]\n    \n    results = []\n    for A in test_cases_matrices:\n        result = analyze_case(A, x_star, x0, tau)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3111694"}]}