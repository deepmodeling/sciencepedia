{"hands_on_practices": [{"introduction": "在我们开始寻找合适的步长 $\\alpha$ 之前，必须确保我们选择的搜索方向 $p$ 是一个有效的“下降方向”。这个练习将通过一个简单但有力的反例，揭示当违反基本规则 $\\nabla f(x)^\\top p \\lt 0$ 时会发生什么。你将发现，对于一个上升方向，Armijo 条件是永远无法满足的，从而深刻理解为什么下降方向是线搜索算法的根本前提。[@problem_id:3189981]", "problem": "考虑二次连续可微函数 $f:\\mathbb{R}^{2}\\to\\mathbb{R}$，定义为 $f(x)=\\tfrac{1}{2}\\|x\\|^{2}$，在点 $x_{0}=\\begin{pmatrix}1\\\\0\\end{pmatrix}$ 处进行评估，搜索方向为 $p=\\begin{pmatrix}1\\\\0\\end{pmatrix}$。充分下降（Armijo）条件规定，对于给定的常数 $c_{1}\\in(0,1)$，一个可接受的步长 $\\alpha0$ 必须满足\n$$\nf(x_{0}+\\alpha p)\\le f(x_{0})+c_{1}\\,\\alpha\\,\\nabla f(x_{0})^{\\top}p.\n$$\n仅使用 Armijo 条件的定义和基本微积分，分析函数 $f$ 沿着直线 $x(\\alpha)=x_{0}+\\alpha p$ 的变化以及由 $p^{\\top}\\nabla^{2}f(x_{0})p$ 量化的沿 $p$ 方向的曲率的作用。证明沿着该方向 $p$ 曲率是非负的，并且 $p$ 不是一个下降方向。然后，计算阈值\n$$\nc_{1}^{\\star}=\\inf_{\\alpha0}\\frac{f(x_{0}+\\alpha p)-f(x_{0})}{\\alpha\\,\\nabla f(x_{0})^{\\top}p},\n$$\n该值是在原则上能使 Armijo 不等式对某个 $\\alpha0$ 可行的最小常数。判断在此设置下，对于任何 $c_{1}\\in(0,1)$ 是否存在 $\\alpha0$ 满足 Armijo 条件，并报告 $c_{1}^{\\star}$ 的单个数值。你的最终答案必须是一个无单位的实数。无需四舍五入。", "solution": "该问题是有效的，因为它科学地基于数值优化的原理，问题设定良好，目标明确，并为唯一解提供了所有必要信息。\n\n首先，我们确定函数 $f(x)=\\frac{1}{2}\\|x\\|^{2}$ 在点 $x_{0}=\\begin{pmatrix}1\\\\0\\end{pmatrix}$ 处的性质。该函数为 $f(x_1, x_2) = \\frac{1}{2}(x_1^2 + x_2^2)$。\n$f$ 的梯度由 $\\nabla f(x) = \\begin{pmatrix} \\frac{\\partial f}{\\partial x_1} \\\\ \\frac{\\partial f}{\\partial x_2} \\end{pmatrix} = \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} = x$ 给出。\n$f$ 的海森矩阵是 $\\nabla^{2}f(x) = \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix} = I$，其中 $I$ 是 $2 \\times 2$ 的单位矩阵。对于所有 $x \\in \\mathbb{R}^2$，海森矩阵是常数。\n\n在特定点 $x_{0}=\\begin{pmatrix}1\\\\0\\end{pmatrix}$ 处，我们有：\n函数值为 $f(x_{0}) = \\frac{1}{2}(1^2 + 0^2) = \\frac{1}{2}$。\n梯度为 $\\nabla f(x_{0}) = x_{0} = \\begin{pmatrix}1\\\\0\\end{pmatrix}$。\n海森矩阵为 $\\nabla^{2}f(x_{0}) = I = \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix}$。\n\n问题要求我们考虑搜索方向 $p=\\begin{pmatrix}1\\\\0\\end{pmatrix}$。如果一个方向 $p$ 满足 $\\nabla f(x_{0})^{\\top}p  0$，则它是一个下降方向。我们来计算这个量：\n$$\n\\nabla f(x_{0})^{\\top}p = \\begin{pmatrix}1  0\\end{pmatrix} \\begin{pmatrix}1\\\\0\\end{pmatrix} = (1)(1) + (0)(0) = 1.\n$$\n由于 $\\nabla f(x_{0})^{\\top}p = 1  0$，方向 $p$ 不是一个下降方向。它是一个上升方向，意味着函数值从 $x_{0}$ 开始沿此方向初始增加。\n\n接下来，我们分析 $f$ 在 $x_{0}$ 处沿方向 $p$ 的曲率。曲率由二次型 $p^{\\top}\\nabla^{2}f(x_{0})p$ 给出。\n$$\np^{\\top}\\nabla^{2}f(x_{0})p = \\begin{pmatrix}1  0\\end{pmatrix} \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix} \\begin{pmatrix}1\\\\0\\end{pmatrix} = \\begin{pmatrix}1  0\\end{pmatrix} \\begin{pmatrix}1\\\\0\\end{pmatrix} = 1.\n$$\n曲率为 1，这是一个正值，因此证实了曲率是非负的。\n\n现在，我们计算阈值 $c_{1}^{\\star}$，其定义为：\n$$\nc_{1}^{\\star}=\\inf_{\\alpha0}\\frac{f(x_{0}+\\alpha p)-f(x_{0})}{\\alpha\\,\\nabla f(x_{0})^{\\top}p}.\n$$\n让我们计算这个表达式中的各项。沿搜索方向的点是 $x(\\alpha) = x_{0}+\\alpha p = \\begin{pmatrix}1\\\\0\\end{pmatrix} + \\alpha\\begin{pmatrix}1\\\\0\\end{pmatrix} = \\begin{pmatrix}1+\\alpha\\\\0\\end{pmatrix}$。\n在该点处的函数值为：\n$$\nf(x_{0}+\\alpha p) = \\frac{1}{2}\\|x_{0}+\\alpha p\\|^2 = \\frac{1}{2}((1+\\alpha)^2 + 0^2) = \\frac{1}{2}(1+\\alpha)^2 = \\frac{1}{2}(1+2\\alpha+\\alpha^2).\n$$\n定义 $c_{1}^{\\star}$ 的分数中的分子是：\n$$\nf(x_{0}+\\alpha p)-f(x_{0}) = \\frac{1}{2}(1+2\\alpha+\\alpha^2) - \\frac{1}{2} = \\alpha + \\frac{1}{2}\\alpha^2.\n$$\n分母是：\n$$\n\\alpha\\,\\nabla f(x_{0})^{\\top}p = \\alpha(1) = \\alpha.\n$$\n将这些代入 $c_{1}^{\\star}$ 的表达式中，我们得到下确界内的比率为：\n$$\n\\frac{\\alpha + \\frac{1}{2}\\alpha^2}{\\alpha}.\n$$\n因为我们考虑的是 $\\alpha  0$，我们可以通过用 $\\alpha$ 除分子和分母来简化这个分数：\n$$\n\\frac{\\alpha(1 + \\frac{1}{2}\\alpha)}{\\alpha} = 1 + \\frac{1}{2}\\alpha.\n$$\n现在我们必须找到这个表达式在所有 $\\alpha  0$ 上的下确界：\n$$\nc_{1}^{\\star} = \\inf_{\\alpha0} \\left(1 + \\frac{1}{2}\\alpha\\right).\n$$\n函数 $g(\\alpha) = 1 + \\frac{1}{2}\\alpha$ 是关于 $\\alpha$ 的严格递增函数。它在区间 $(0, \\infty)$ 上的下确界是当 $\\alpha$ 从右侧趋近于 0 时的极限：\n$$\nc_{1}^{\\star} = \\lim_{\\alpha \\to 0^+} \\left(1 + \\frac{1}{2}\\alpha\\right) = 1.\n$$\n阈值为 $c_{1}^{\\star}=1$。\n\n最后，我们必须判断对于任何 $c_{1} \\in (0,1)$ 是否存在 $\\alpha  0$ 满足 Armijo 条件。Armijo 条件是：\n$$\nf(x_{0}+\\alpha p) \\le f(x_{0})+c_{1}\\,\\alpha\\,\\nabla f(x_{0})^{\\top}p.\n$$\n对于 $\\alpha  0$ 且 $\\nabla f(x_0)^\\top p \\ne 0$，我们可以将其重新整理为：\n$$\n\\frac{f(x_{0}+\\alpha p)-f(x_{0})}{\\alpha\\,\\nabla f(x_{0})^{\\top}p} \\le c_1.\n$$\n我们已经计算出左侧为 $1 + \\frac{1}{2}\\alpha$。所以条件变为：\n$$\n1 + \\frac{1}{2}\\alpha \\le c_1.\n$$\n我们已知 $\\alpha  0$，这意味着 $\\frac{1}{2}\\alpha  0$，因此 $1 + \\frac{1}{2}\\alpha  1$。\nArmijo 条件要求参数 $c_1$ 在区间 $(0,1)$ 内，即 $c_1  1$。\n因此，不等式 $1 + \\frac{1}{2}\\alpha \\le c_1$ 要求一个严格大于 1 的数小于或等于一个严格小于 1 的数。这是一个矛盾。因此，对于任何 $c_1 \\in (0,1)$，不存在步长 $\\alpha  0$ 能够满足 Armijo 条件。这是 $p$ 不是下降方向的直接结果。所要求的数值是 $c_{1}^{\\star}$。", "answer": "$$\\boxed{1}$$", "id": "3189981"}, {"introduction": "既然我们知道了需要一个下降方向，一个自然的问题是：我们总能找到一个满足 Armijo 条件的步长 $\\alpha$ 吗？这个练习将 Armijo 条件与函数的光滑性（通过梯度的 Lipschitz 常数 $L$ 来量化）联系起来。你将推导出一个理论上的、在最坏情况下也保证有效的步长，从而建立“解是存在的”信心，同时也能体会到理论界限与实际应用之间的差异。[@problem_id:3189999]", "problem": "设 $f:\\mathbb{R}^{n}\\to\\mathbb{R}$ 是一个连续可微的函数，其梯度是 $L$-Lipschitz 连续的，这意味着存在 $L0$，使得对于所有 $x,y\\in\\mathbb{R}^{n}$，都有 $\\|\\nabla f(x)-\\nabla f(y)\\|\\le L\\|x-y\\|$。考虑单步最速下降 $x^{+}=x-\\alpha \\nabla f(x)$，其中步长 $\\alpha0$。参数为 $c_{1}\\in(0,1)$ 的 Armijo 充分下降条件要求，沿着下降方向 $d$ 必须满足 $f(x+\\alpha d)\\le f(x)+c_{1}\\alpha \\nabla f(x)^{\\top}d$。假设方向为最速下降方向 $d=-\\nabla f(x)$，并回想一下 $\\nabla f$ 的 $L$-Lipschitz 连续性意味着所谓的下降引理。\n\n仅使用这些基本事实，推导出一个关于 $\\alpha$ 的显式上界，该上界保证 Armijo 充分下降条件对于最速下降步成立。然后，特化这个界限，得出一个仅依赖于 $L$ 的 $\\alpha$ 的单一选择，该选择能保证对于每一个 $c_{1}\\in(0,\\tfrac{1}{2}]$，无论 $x$ 和 $\\nabla f(x)$ 为何，Armijo 条件都成立。将你的最终答案表示为以 $L$ 为变量的封闭形式解析表达式。最后，简要解释这个固定选择相对于实践中 Armijo 条件可能允许的步长是否保守，并将你的解释与 Wolfe 和 Goldstein 条件等曲率条件的作用联系起来。最终答案不需要进行数值取整。", "solution": "题目要求在给定一个梯度为 $L$-Lipschitz 连续的函数 $f$ 的情况下，为最速下降法推导一个步长 $\\alpha$ 的上界，以保证 Armijo 充分下降条件成立。然后，题目要求基于此界限给出一个固定的步长选择，并简要讨论其实际意义。\n\n首先，我们将给定信息形式化。\n函数 $f:\\mathbb{R}^{n}\\to\\mathbb{R}$ 是连续可微的。\n其梯度 $\\nabla f$ 是 $L$-Lipschitz 连续的，这意味着存在一个常数 $L0$，使得对于所有 $x, y \\in \\mathbb{R}^{n}$：\n$$\n\\|\\nabla f(x) - \\nabla f(y)\\| \\le L \\|x-y\\|\n$$\n这个性质引出下降引理 (Descent Lemma)（也称为二次上界引理），其表述为：\n$$\nf(y) \\le f(x) + \\nabla f(x)^{\\top}(y-x) + \\frac{L}{2} \\|y-x\\|^2\n$$\n优化步骤是最速下降步：\n$$\nx^{+} = x - \\alpha \\nabla f(x)\n$$\n其中 $\\alpha  0$ 是步长。这对应于在方向 $d = -\\nabla f(x)$ 上的步长 $\\alpha d$。\n\n对于沿下降方向 $d$ 的步长 $\\alpha$，Armijo 充分下降条件是：\n$$\nf(x+\\alpha d) \\le f(x) + c_1 \\alpha \\nabla f(x)^{\\top} d\n$$\n其中 $c_1 \\in (0, 1)$。将最速下降方向 $d = -\\nabla f(x)$ 代入此条件，我们得到：\n$$\nf(x - \\alpha \\nabla f(x)) \\le f(x) + c_1 \\alpha \\nabla f(x)^{\\top} (-\\nabla f(x))\n$$\n$$\nf(x - \\alpha \\nabla f(x)) \\le f(x) - c_1 \\alpha \\|\\nabla f(x)\\|^2\n$$\n我们的目标是找到 $\\alpha$ 的一个上界，以保证这个不等式成立。我们使用下降引理。令 $y = x^{+} = x - \\alpha \\nabla f(x)$。应用该引理：\n$$\nf(x - \\alpha \\nabla f(x)) \\le f(x) + \\nabla f(x)^{\\top}((x - \\alpha \\nabla f(x)) - x) + \\frac{L}{2} \\|(x - \\alpha \\nabla f(x)) - x\\|^2\n$$\n$$\nf(x - \\alpha \\nabla f(x)) \\le f(x) + \\nabla f(x)^{\\top}(-\\alpha \\nabla f(x)) + \\frac{L}{2} \\|-\\alpha \\nabla f(x)\\|^2\n$$\n$$\nf(x - \\alpha \\nabla f(x)) \\le f(x) - \\alpha \\|\\nabla f(x)\\|^2 + \\frac{L\\alpha^2}{2} \\|\\nabla f(x)\\|^2\n$$\n这个不等式给出了下一次迭代中 $f$ 值的上界。如果这个上界小于或等于 Armijo 不等式的右侧，那么 Armijo 条件就得到满足。也就是说，我们需要：\n$$\nf(x) - \\alpha \\|\\nabla f(x)\\|^2 + \\frac{L\\alpha^2}{2} \\|\\nabla f(x)\\|^2 \\le f(x) - c_1 \\alpha \\|\\nabla f(x)\\|^2\n$$\n假设我们不在驻点（即 $\\nabla f(x) \\neq 0$），我们可以从两边减去 $f(x)$，然后除以正标量 $\\|\\nabla f(x)\\|^2$：\n$$\n-\\alpha + \\frac{L\\alpha^2}{2} \\le -c_1 \\alpha\n$$\n由于根据定义 $\\alpha  0$，我们可以除以 $\\alpha$：\n$$\n-1 + \\frac{L\\alpha}{2} \\le -c_1\n$$\n现在，我们求解 $\\alpha$：\n$$\n\\frac{L\\alpha}{2} \\le 1 - c_1\n$$\n$$\n\\alpha \\le \\frac{2(1-c_1)}{L}\n$$\n这就是保证对于给定的 $c_1 \\in (0, 1)$，Armijo 条件成立的 $\\alpha$ 的显式上界。\n\n接下来，我们必须特化这个界限，以找到一个仅依赖于 $L$ 的 $\\alpha$ 的单一选择，该选择能保证对于每一个 $c_1 \\in (0, \\frac{1}{2}]$，条件都成立。为确保不等式 $\\alpha \\le \\frac{2(1-c_1)}{L}$ 对该区间内所有的 $c_1$ 都成立，$\\alpha$ 必须小于或等于右侧表达式在该区间上的最小值。令 $g(c_1) = \\frac{2(1-c_1)}{L}$。关于 $c_1$ 的导数是 $g'(c_1) = -\\frac{2}{L}$，因为 $L0$，所以该导数为负。因此，$g(c_1)$ 是一个关于 $c_1$ 的递减函数。$g(c_1)$ 在区间 $(0, \\frac{1}{2}]$ 上的最小值将在 $c_1$ 的最大值处取得，即 $c_1 = \\frac{1}{2}$。\n上界的最小值为：\n$$\n\\min_{c_1 \\in (0, 1/2]} \\frac{2(1-c_1)}{L} = \\frac{2(1 - \\frac{1}{2})}{L} = \\frac{2(\\frac{1}{2})}{L} = \\frac{1}{L}\n$$\n因此，任何满足 $0  \\alpha \\le \\frac{1}{L}$ 的 $\\alpha$ 都将对任意选择的 $c_1 \\in (0, \\frac{1}{2}]$ 满足 Armijo 条件。题目要求一个单一的 $\\alpha$ 选择；对于任何函数 $f$（具有 $L$-Lipschitz 梯度）和任何 $x$ 都有效的最通用且限制最少的固定选择是这个范围的上限，即 $\\alpha = \\frac{1}{L}$。\n\n最后，我们被要求评论这个固定选择是否保守。步长 $\\alpha = \\frac{1}{L}$ 确实非常保守。这个值是根据全局 Lipschitz 常数 $L$ 推导出的最坏情况保证，它反映了函数 $f$ 在其整个定义域上的最大曲率。在许多区域，局部曲率可能远小于 $L$，这意味着 Armijo 条件下可能允许大得多的步长，从而实现更快的收敛。诸如回溯之类的实用线搜索方法利用了这一点，它们从一个较大的试探步长开始，然后迭代地减小它，直到满足 Armijo 条件，从而使步长适应每次迭代时函数的局部性质。\n\n这与曲率条件的作用有关，例如 Wolfe 或 Goldstein 条件。Armijo 条件（一个充分下降条件）本身只提供了可接受步长的上界。它不能防止步长 $\\alpha$ 过小。一个算法可能用一个极小的 $\\alpha$ 来满足 Armijo 条件，而这个 $\\alpha$ 几乎无法向最小值取得进展。为确保有意义的进展，Armijo 条件通常与一个曲率条件配对。第二个 Wolfe 条件，$\\nabla f(x+\\alpha d)^{\\top}d \\ge c_2 \\nabla f(x)^{\\top}d$ 对于 $c_2 \\in (c_1, 1)$，强制要求新点的斜率比初始斜率更缓，这有效地为可接受的步长设置了下界，并排除了病态的小步长。Goldstein 条件同样从上方和下方限制了可接受的步长。因此，虽然我们的固定步长 $\\alpha=\\frac{1}{L}$ 保证了函数值的下降，但它可能是一个过于谨慎的步长，一个采用曲率条件的更复杂的线搜索方法会拒绝它，而选择一个更大、更有效的步长。", "answer": "$$\n\\boxed{\\frac{1}{L}}\n$$", "id": "3189999"}, {"introduction": "理论保证了步长的存在，但在实践中高效地找到一个“好”的步长则是一个挑战。这个编码练习将理论付诸实践，要求你实现一个包含强 Wolfe 条件的线搜索算法。你将通过具体例子发现一个常见的陷阱——因曲率参数 $c_2$ 选择不当而导致的步长停滞——并学习一种实用的自适应策略来解决它，从而弥合教科书算法与稳健的现实世界代码之间的差距。[@problem_id:3143424]", "problem": "考虑一个二次连续可微函数 $f:\\mathbb{R}^n\\rightarrow\\mathbb{R}$ 和在点 $x\\in\\mathbb{R}^n$ 处的一个下降方向 $p\\in\\mathbb{R}^n$。线搜索选择一个步长 $\\alpha0$ 以沿着直线 $x+\\alpha p$ 减小 $f$ 的值。称为强 Wolfe 条件的非精确线搜索由两个不等式组成，这两个不等式由常数 $c_1$ 和 $c_2$ 参数化，其中 $0  c_1  c_2  1$：\n1.  充分下降条件：$f(x+\\alpha p)\\le f(x)+c_1\\,\\alpha\\,\\nabla f(x)^\\top p$\n2.  曲率条件：$\\lvert \\nabla f(x+\\alpha p)^\\top p\\rvert \\le c_2\\,\\lvert \\nabla f(x)^\\top p\\rvert$\n\n你的任务是编写一个 Python 程序来执行以下操作：\n1.  实现一个回溯线搜索算法，该算法从初始步长 $\\alpha_0$ 开始，并通过一个因子 $\\beta\\in(0,1)$ 进行迭代收缩，直到同时满足强 Wolfe 条件。如果 $\\alpha$ 小于一个阈值 $\\alpha_{\\min}$，算法应终止。\n2.  将你的算法应用于具体例子：函数 $f(x)=\\tfrac{1}{2}\\,x^\\top Q x$，其中 $Q=\\mathrm{diag}(100, 1)$，起始点 $x_0=(1,1)^\\top$，方向为最速下降方向 $p_0=-\\nabla f(x_0)$。\n3.  使用以下参数：$c_1=10^{-4}, \\alpha_0=1, \\beta=0.5, \\alpha_{\\min}=10^{-6}$。\n4.  实现一个针对 $c_2$ 的自动调整策略。如果充分下降条件满足但曲率条件不满足，则应将 $c_2$ 调整为 $c_2^{\\mathrm{new}} = \\min\\{0.99, \\max\\{c_1+10^{-12}, r(\\alpha)+0.05\\}\\}$，其中 $r(\\alpha)=|\\nabla f(x+\\alpha p)^\\top p|/|\\nabla f(x)^\\top p|$。如果使用这个新的 $c_2$ 满足曲率条件，则算法应接受当前的 $\\alpha$ 并返回。\n5.  对于二次问题，精确线搜索步长为 $\\alpha_{\\text{exact}} = -(p_0^\\top \\nabla f(x_0))/(p_0^\\top Q p_0)$。计算此值。\n6.  按顺序评估以下五个测试用例，并报告每个用例返回的 $\\alpha$：\n    -   用例 1 (c2=0.9, auto_tune=False): 标准情况，预期会成功。\n    -   用例 2 (c2=0.1, auto_tune=False): $c_2$ 值较小，可能导致停滞。\n    -   用例 3 (c2=0.1, auto_tune=True): 自动调整应能从停滞中恢复。\n    -   用例 4 (c2=0.001, auto_tune=False): 更严重的停滞。\n    -   用例 5 (c2=0.9, auto_tune=True): 自动调整应不会干扰成功的用例。\n7.  将所有五个结果以及精确的线搜索步长 $\\alpha_{\\text{exact}}$ 收集到一个列表中，并以格式 `[result1,result2,result3,result4,result5,alpha_exact]` 打印该列表。", "solution": "该问题要求实现并分析一种基于强 Wolfe 条件的回溯线搜索算法，展示一种常见的失效模式及其修正策略。解决方案是基于多元微积分和数值优化的基本原理制定的。\n\n### 线搜索原理与强 Wolfe 条件\n\n在诸如梯度下降或拟牛顿法等迭代优化方法中，从当前点 $x$ 沿下降方向 $p$（即函数 $f$ 初始下降的方向，$\\nabla f(x)^\\top p  0$）进行更新。新点为 $x_{\\text{new}} = x + \\alpha p$，其中 $\\alpha  0$ 是步长。线搜索的目标是找到一个合适的 $\\alpha$，使 $f$ 的值得到充分的减小。\n\n**强 Wolfe 条件**为可接受的步长 $\\alpha$ 提供了一套标准准则。它们由两个不等式组成：\n1.  **充分下降 (Armijo) 条件**：这确保了步长 $\\alpha$ 能使函数值实现明显的减小，该减小量与步长和方向导数成正比。\n    $$f(x+\\alpha p)\\le f(x)+c_1\\,\\alpha\\,\\nabla f(x)^\\top p$$\n    这里，$c_1 \\in (0, 1)$ 是一个常数，通常很小（例如，$10^{-4}$）。由于 $p$ 是下降方向，$\\nabla f(x)^\\top p  0$，因此右侧构成了对 $f(x+\\alpha p)$ 的一个上界，该上界比 $f(x+\\alpha p)  f(x)$ 更为严格。\n\n2.  **曲率条件**：这确保了步长不会太短。它要求函数在新点 $x+\\alpha p$ 处沿方向 $p$ 的斜率的绝对值小于原点 $x$ 处斜率的绝对值，从而确保取得进展。强 Wolfe 版本的形式为：\n    $$\\lvert \\nabla f(x+\\alpha p)^\\top p\\rvert \\le c_2\\,\\lvert \\nabla f(x)^\\top p\\rvert$$\n    这里，$c_2 \\in (c_1, 1)$ 是一个常数。该条件防止算法在梯度仍然很大的区域采取过小的步长。\n\n### 二次模型问题\n\n为分析该算法，我们使用一个标准的测试函数：正定二次型 $f(x)=\\tfrac{1}{2}\\,x^\\top Q x$。对于此类函数，其梯度是线性的：$\\nabla f(x) = Qx$。这种解析上的简单性使得所有相关量都可以精确计算。\n具体参数为：\n-   $Q = \\mathrm{diag}(100, 1)$，一个 $2 \\times 2$ 对称正定矩阵。\n-   $x_0 = (1, 1)^\\top$。\n-   方向为最速下降方向，$p_0 = -\\nabla f(x_0) = -Qx_0$。\n\n我们来计算初始值：\n-   $\\nabla f(x_0) = Qx_0 = \\begin{pmatrix} 100  0 \\\\ 0  1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 100 \\\\ 1 \\end{pmatrix}$。\n-   $p_0 = -\\nabla f(x_0) = \\begin{pmatrix} -100 \\\\ -1 \\end{pmatrix}$。\n-   在 $x_0$ 处的方向导数为 $\\nabla f(x_0)^\\top p_0 = \\begin{pmatrix} 100  1 \\end{pmatrix} \\begin{pmatrix} -100 \\\\ -1 \\end{pmatrix} = -10000 - 1 = -10001$。\n\n在新点 $x_0+\\alpha p_0$ 处的梯度为 $\\nabla f(x_0+\\alpha p_0) = Q(x_0+\\alpha p_0)$。此新点处的方向导数为 $\\nabla f(x_0+\\alpha p_0)^\\top p_0 = (Q(x_0+\\alpha p_0))^\\top p_0 = (Qx_0 + \\alpha Qp_0)^\\top p_0 = \\nabla f(x_0)^\\top p_0 + \\alpha(Qp_0)^\\top p_0$。\n使用 $p_0 = -Qx_0$，这可以简化为 $\\nabla f(x_0)^\\top p_0 - \\alpha(Q(Qx_0))^\\top p_0 = \\nabla f(x_0)^\\top p_0 - \\alpha(Q^2x_0)^\\top p_0$。表达式 $\\nabla f(x_0)^\\top p_0 + \\alpha p_0^\\top Q p_0$ 更为直接。\n数值上：\n$p_0^\\top Q p_0 = \\begin{pmatrix} -100  -1 \\end{pmatrix} \\begin{pmatrix} 100  0 \\\\ 0  1 \\end{pmatrix} \\begin{pmatrix} -100 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} -10000  -1 \\end{pmatrix} \\begin{pmatrix} -100 \\\\ -1 \\end{pmatrix} = 1000000 + 1 = 1000001$。\n所以，$\\nabla f(x_0+\\alpha p_0)^\\top p_0 = -10001 + \\alpha (1000001)$。\n\n### 精确线搜索（基准）\n\n对于二次函数，最小化 $\\phi(\\alpha) = f(x_0 + \\alpha p_0)$ 的精确步长 $\\alpha_{\\text{exact}}$ 可以通过设置 $\\frac{d\\phi}{d\\alpha}=0$ 来找到。\n$\\frac{d\\phi}{d\\alpha} = \\nabla f(x_0+\\alpha p_0)^\\top p_0 = 0$。\n使用上面的表达式：$-10001 + \\alpha_{\\text{exact}} (1000001) = 0$。\n这得出 $\\alpha_{\\text{exact}} = \\frac{10001}{1000001} \\approx 0.01$。该值为理论最优值，并作为我们的参考。\n\n### 回溯算法与停滞\n\n回溯线搜索从一个初始猜测值 $\\alpha_0$ 开始，并通过一个因子 $\\beta \\in (0, 1)$ 对其进行迭代收缩，直到满足 Wolfe 条件。如果 $\\alpha$ 小于一个阈值 $\\alpha_{\\min}$，算法将终止。\n\n曲率条件是 $|\\nabla f(x_0+\\alpha p_0)^\\top p_0| \\le c_2 |\\nabla f(x_0)^\\top p_0|$，这可以转换为：\n$$|-10001 + \\alpha (1000001)| \\le c_2 |-10001|$$\n$$|1 - \\alpha \\frac{1000001}{10001}| \\le c_2 \\implies |1 - \\alpha/\\alpha_{\\text{exact}}| \\le c_2$$\n这个不等式定义了 $\\alpha_{\\text{exact}}$ 周围一个可接受的 $\\alpha$ 值区间：\n$$1 - c_2 \\le \\alpha/\\alpha_{\\text{exact}} \\le 1 + c_2$$\n$$\\alpha \\in [(1-c_2)\\alpha_{\\text{exact}}, (1+c_2)\\alpha_{\\text{exact}}]$$\n\n采用从 $\\alpha_0=1$ 开始、$\\beta=0.5$ 的纯回溯法，试验步长序列为 $1, 0.5, 0.25, ..., \\alpha_k = (\\frac{1}{2})^k$。如果选择的 $c_2$ 太小，可接受的 $\\alpha$ 值区间可能会变得非常窄，以至于没有一个试验步长 $\\alpha_k$ 落入其中。\n例如，当 $c_2=0.1$ 且 $\\alpha_{\\text{exact}} \\approx 0.01$ 时，可接受区间约为 $[0.009, 0.011]$。回溯序列包括 $\\alpha=0.015625$（太大）和下一步 $\\alpha=0.0078125$（太小）。回溯法会“跳过”有效区域。当 $\\alpha \\to 0$ 时，我们有 $|1-\\alpha/\\alpha_{\\text{exact}}| \\to 1$。曲率条件要求 $1 \\le c_2$，但这是不可能的，因为 $c_2  1$。因此，一旦 $\\alpha$ 足够小，曲率条件就永远无法满足。算法将继续回溯，直到 $\\alpha  \\alpha_{\\min}$，从而导致停滞。这正是测试用例 2 和 4 所演示的。\n\n### 用于恢复的自动调整策略\n\n为防止这种停滞，引入了一种针对 $c_2$ 的自适应策略。如果充分下降条件满足但曲率条件不满足，这意味着步长处于一个“好”的区域，但曲率约束过于严格。自动调整机制基于观测到的曲率比 $r(\\alpha) = |\\nabla f(x+\\alpha p)^\\top p| / |\\nabla f(x)^\\top p|$ 来放宽 $c_2$。\n更新规则是：\n$$c_2^{\\mathrm{new}} = \\min\\{0.99, \\max\\{c_1+10^{-12}, r(\\alpha)+0.05\\}\\}$$\n新的 $c_2$ 保证在 $(c_1, 1)$ 区间内，并且比当前观测到的比率 $r(\\alpha)$ 稍大。当使用 $c_2^{\\mathrm{new}}$ 重新检查曲率条件时，测试 $r(\\alpha) \\le c_2^{\\mathrm{new}}$ 极有可能通过，因为 $r(\\alpha) \\le r(\\alpha)+0.05$。这使得算法能够接受当前的步长 $\\alpha$ 并继续前进，从而有效地克服了停滞。测试用例 3 演示了这种恢复。测试用例 5 表明该机制在不需要时不会产生干扰。\n\n下面的 Python 程序实现了这整个逻辑，计算了所有测试用例的步长，并计算了精确步长以供比较。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and evaluates a backtracking line search with strong Wolfe conditions,\n    demonstrating stagnation due to a poor c2 choice and recovery via auto-tuning.\n    \"\"\"\n    \n    # 2. Construct the concrete example\n    Q = np.diag([100.0, 1.0])\n    x0 = np.array([1.0, 1.0])\n    \n    c1 = 1e-4\n    alpha0 = 1.0\n    beta = 0.5\n    alpha_min = 1e-6\n\n    # Define the function and its gradient\n    def f(x):\n        return 0.5 * x.T @ Q @ x\n\n    def grad_f(x):\n        return Q @ x\n\n    # Calculate initial direction and constant terms\n    p0 = -grad_f(x0)\n    f_x0 = f(x0)\n    grad_f_x0_T_p0 = grad_f(x0).T @ p0\n    abs_grad_p_term = np.abs(grad_f_x0_T_p0)\n\n    # 1. Implement backtracking line search with strong Wolfe conditions\n    def backtracking_wolfe(c2_initial, auto_tune):\n        \"\"\"\n        Performs backtracking line search to find a step size alpha\n        satisfying the strong Wolfe conditions.\n        \"\"\"\n        alpha = alpha0\n        c2 = c2_initial\n\n        while alpha >= alpha_min:\n            # Test sufficient decrease (Armijo) condition\n            armijo_cond_satisfied = f(x0 + alpha * p0) = f_x0 + c1 * alpha * grad_f_x0_T_p0\n            \n            if armijo_cond_satisfied:\n                # Test curvature condition\n                grad_f_x_alpha_p_T_p = grad_f(x0 + alpha * p0).T @ p0\n                abs_grad_p_alpha_term = np.abs(grad_f_x_alpha_p_T_p)\n                \n                curvature_cond_satisfied = abs_grad_p_alpha_term = c2 * abs_grad_p_term\n                \n                if curvature_cond_satisfied:\n                    return alpha # Both conditions met\n\n                # 4. Implement auto-tuning for c2\n                elif auto_tune:\n                    r_alpha = abs_grad_p_alpha_term / abs_grad_p_term\n                    c2_new = min(0.99, max(c1 + 1e-12, r_alpha + 0.05))\n                    \n                    # Re-check with the relaxed c2\n                    if abs_grad_p_alpha_term = c2_new * abs_grad_p_term:\n                        return alpha # Recovered with new c2\n\n            # If conditions fail, backtrack\n            alpha *= beta\n\n        # If loop finishes, alpha has dropped below alpha_min\n        return alpha_min\n\n    # 5. Compute the exact line search step size\n    def exact_line_search():\n        \"\"\"\n        Computes the exact optimal step size for the quadratic function.\n        \"\"\"\n        # For this specific problem: alpha = - (p0' * grad_f(x0)) / (p0' * Q * p0)\n        numerator = -grad_f_x0_T_p0\n        denominator = p0.T @ Q @ p0\n        return numerator / denominator\n\n    # 6. Evaluate all test cases\n    test_cases = [\n        # (c2, auto_tune_enabled)\n        (0.9, False),    # Case 1: Happy path\n        (0.1, False),    # Case 2: Stagnation\n        (0.1, True),     # Case 3: Auto-tuned recovery\n        (0.001, False),  # Case 4: Severe stagnation\n        (0.9, True),     # Case 5: Redundant tuning\n    ]\n\n    results = []\n    for c2_val, tune_flag in test_cases:\n        result = backtracking_wolfe(c2_val, tune_flag)\n        results.append(result)\n\n    # Add the exact line search result\n    results.append(exact_line_search())\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3143424"}]}