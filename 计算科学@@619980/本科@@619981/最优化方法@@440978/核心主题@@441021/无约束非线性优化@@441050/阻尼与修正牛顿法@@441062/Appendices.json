{"hands_on_practices": [{"introduction": "理论知识需要通过实践来巩固。在第一个练习中，我们将深入研究阻尼牛顿法的核心——回溯线搜索。即使我们找到了一个有效的下降方向，步长（即沿该方向前进的距离）的选择也至关重要。本练习将通过在一个经典的非凸函数上实现和比较两种不同的阻尼策略（“积极”与“保守”），让你亲身体验线搜索参数如何影响算法的收敛路径和效率 [@problem_id:3115942]。通过量化收敛到全局最小值所需的迭代次数，你将直观地理解在算法设计中，对进步的渴望与对稳定性的需求之间存在的权衡。", "problem": "我们要求您在一个非凸一维测试函数上，实现并比较牛顿法框架下的两种阻尼策略，用于无约束最小化问题。比较必须是定量的：对于每种策略和每个指定的起始点，报告在规定容差内达到全局最小值所需的外部迭代次数。\n\n请从以下基本框架开始：\n- 目标函数是一个二次连续可微函数 $f:\\mathbb{R}\\to\\mathbb{R}$，其梯度为 $\\nabla f(x)$，Hessian矩阵（二阶导数）为 $f''(x)$。\n- 牛顿法通过最小化关于 $p$ 的二阶泰勒模型 $m_x(p) = f(x) + \\nabla f(x)\\,p + \\frac{1}{2} f''(x)\\,p^2$ 而产生，这给出了求解 $f''(x)\\,p_N(x) = -\\nabla f(x)$ 的无阻尼牛顿步长 $p_N(x)$。\n- 步长阻尼使用回溯线搜索，以满足 Armijo（充分下降）条件 $f(x+\\alpha p) \\le f(x) + c_1\\,\\alpha\\,\\nabla f(x)\\,p$。其中 $c_1 \\in (0,1)$，步长 $\\alpha \\in (0,1]$ 通过使用收缩因子 $\\rho \\in (0,1)$ 进行连续缩减 $\\alpha \\leftarrow \\rho \\alpha$ 来获得。\n\n目标函数及已知性质：\n- 考虑非凸双阱势函数 $f(x) = x^4 - 2x^2$。其梯度和Hessian矩阵分别定义为 $\\nabla f(x) = 4x^3 - 4x$ 和 $f''(x) = 12x^2 - 4$。全局最小值在 $x^\\star \\in \\{-1,+1\\}$ 处取得，最优值为 $f(x^\\star) = -1$。\n\n用于处理非凸性的修正牛顿方向：\n- 当 $f''(x) \\le 0$（非正曲率）时，无阻尼牛顿步长 $p_N(x) = -\\nabla f(x)/f''(x)$ 并不构成一个下降方向。在这种情况下，使用最速下降方向 $p(x) = -\\nabla f(x)$，它保证只要 $\\nabla f(x) \\ne 0$，就有 $\\nabla f(x)\\,p(x) = -\\|\\nabla f(x)\\|^2 < 0$。\n- 鞍点逃逸约定：如果 $|\\nabla f(x)| \\le \\varepsilon_g$ 且 $f''(x) \\le 0$（例如，$x=0$），则通过选择一个单位方向 $p(x) = +1$ 来打破对称性，并依靠线搜索阻尼来确保函数值下降。\n\n待比较的阻尼策略：\n- 激进阻尼策略：Armijo 参数 $c_1 = 10^{-4}$，收缩因子 $\\rho = 0.7$，初始步长 $\\alpha_0 = 1$，最大回溯步数 $B = 50$。\n- 保守阻尼策略：Armijo 参数 $c_1 = 10^{-2}$，收缩因子 $\\rho = 0.3$，初始步长 $\\alpha_0 = 1$，最大回溯步数 $B = 50$。\n\n停止准则与迭代计数：\n- 设 $\\varepsilon_f = 10^{-10}$。当 $f(x_k) \\le -1 + \\varepsilon_f$ 时，宣布成功。只计算外部迭代次数 $k$（即接受的步数），不计回溯尝试的次数。如果在 $K_{\\max} = 200$ 次外部迭代内未能成功，则该次运行返回 $K_{\\max}$。\n- 在上述鞍点逃逸规则中使用梯度范数阈值 $\\varepsilon_g = 10^{-12}$。\n\n在当前点 $x_k$ 的每次外部迭代 $k$ 中需实现的算法摘要如下：\n1. 计算梯度 $\\nabla f(x_k)$ 和Hessian矩阵 $f''(x_k)$。\n2. 按如下方式选择方向 $p_k$：\n   - 如果 $f''(x_k) > 0$，设 $p_k = -\\nabla f(x_k)/f''(x_k)$。\n   - 否则，如果 $|\\nabla f(x_k)| > \\varepsilon_g$，设 $p_k = -\\nabla f(x_k)$。\n   - 否则，设 $p_k = +1$。\n3. 从 $\\alpha_0 = 1$ 开始，对 $\\alpha$ 进行回溯，通过缩减 $\\alpha \\leftarrow \\rho \\alpha$ 直到 Armijo 条件 $f(x_k + \\alpha p_k) \\le f(x_k) + c_1 \\alpha \\nabla f(x_k)p_k$ 成立，或已尝试 $B$ 次缩减。使用最终得到的 $\\alpha$。\n4. 更新 $x_{k+1} = x_k + \\alpha p_k$。如果 $f(x_{k+1}) \\le -1 + \\varepsilon_f$，则停止并报告 $k+1$；否则继续迭代，直到达到上限 $K_{\\max}$。\n\n测试套件：\n- 对以下起始点 $x_0$ 评估两种阻尼策略：\n  1. $x_0 = 2.0$（理想路径，在远离右侧阱的位置）。\n  2. $x_0 = -2.0$（理想路径，在远离左侧阱的位置）。\n  3. $x_0 = 0.2$（靠近具有负曲率的中心区域）。\n  4. $x_0 = 0.0$（精确鞍点，梯度为零）。\n  5. $x_0 = 0.57735$（靠近拐点，此处 $f''(x) \\approx 0$）。\n  6. $x_0 = -0.57735$（与前一情况对称）。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含结果，格式为逗号分隔的数对列表，每个测试用例对应一个数对，并用方括号括起来。每个数对的形式为 $[n_{\\text{aggressive}},n_{\\text{conservative}}]$，不含空格，其中 $n_{\\text{aggressive}}$ 和 $n_{\\text{conservative}}$ 是整数。例如：$[[a_1,b_1],[a_2,b_2],\\dots,[a_6,b_6]]$。\n\n角度、物理单位和百分比不适用于此纯数学任务；无需进行单位转换。", "solution": "用户要求实现并比较一种改进的牛顿法，该方法带有两种不同的阻尼策略，用于最小化一个一维非凸函数。\n\n### 步骤1：提取已知信息\n\n- **目标函数**：$f(x) = x^4 - 2x^2$，其中 $x \\in \\mathbb{R}$。\n- **梯度**：$\\nabla f(x) = 4x^3 - 4x$。\n- **Hessian矩阵**：$f''(x) = 12x^2 - 4$。\n- **全局最小值**：$x^\\star \\in \\{-1, +1\\}$，其中 $f(x^\\star) = -1$。\n- **在 $x_k$ 处的修正搜索方向 $p_k$**：\n    - 如果 $f''(x_k) > 0$：$p_k = -\\nabla f(x_k)/f''(x_k)$（牛顿步长）。\n    - 否则如果 $|\\nabla f(x_k)| > \\varepsilon_g$：$p_k = -\\nabla f(x_k)$（最速下降）。\n    - 否则（即 $f''(x_k) \\le 0$ 且 $|\\nabla f(x_k)| \\le \\varepsilon_g$）：$p_k = +1$（鞍点逃逸）。\n- **线搜索**：满足 Armijo 条件 $f(x_k + \\alpha p_k) \\le f(x_k) + c_1 \\alpha \\nabla f(x_k) p_k$ 的回溯。\n    - 初始步长：$\\alpha_0 = 1$。\n    - 收缩：$\\alpha \\leftarrow \\rho \\alpha$。\n    - 最大回溯步数：$B=50$。\n- **阻尼策略**：\n    - 激进策略：Armijo 参数 $c_1 = 10^{-4}$，收缩因子 $\\rho = 0.7$。\n    - 保守策略：Armijo 参数 $c_1 = 10^{-2}$，收缩因子 $\\rho = 0.3$。\n- **容差和限制**：\n    - 成功收敛的函数值容差：$\\varepsilon_f = 10^{-10}$。如果 $f(x_k) \\le -1 + \\varepsilon_f$，则宣布成功。\n    - 鞍点逃逸的梯度范数阈值：$\\varepsilon_g = 10^{-12}$。\n    - 最大外部迭代次数：$K_{\\max} = 200$。\n- **测试套件（起始点 $x_0$）**：$2.0, -2.0, 0.2, 0.0, 0.57735, -0.57735$。\n\n### 步骤2：使用提取的已知信息进行验证\n\n- **科学依据**：该问题基于数值优化的标准、成熟的原理，特别是牛顿法、线搜索技术（Armijo 条件）以及针对非凸函数的修正。函数及其导数的指定是正确的。\n- **良构性**：该问题清晰地定义了所有必要参数（$c_1, \\rho, \\varepsilon_f, \\varepsilon_g, K_{\\max}, B$）、一个特定的算法和一组测试用例。期望得到一个唯一且有意义的解（每种情况下的迭代次数）。\n- **客观性**：该问题使用精确的数学语言，并为性能评估和终止设定了客观、定量的标准。它不含任何主观论断。\n\n该问题自成体系、一致且科学合理。它没有违反任何无效性标准。\n\n### 步骤3：结论与行动\n\n问题陈述是**有效的**。将提供完整解决方案。\n\n### 解题思路\n\n该问题的核心是实现一种稳健的线搜索牛顿法，能够处理非凸性。这是通过在 Hessian 矩阵非正定时修正搜索方向，确保所选方向始终是下降方向（或在鞍点处的特殊逃逸方向），并且步长能保证目标函数充分下降来实现的。\n\n**1. 牛顿法基础**\n用于优化的标准牛顿法通过一个二次模型来近似目标函数 $f(x)$ 在点 $x_k$ 处的值：\n$$m_{x_k}(p) = f(x_k) + \\nabla f(x_k) p + \\frac{1}{2} f''(x_k) p^2$$\n最小化该模型的步长 $p$ 即为牛顿步长 $p_N(x_k)$，通过求解 $\\nabla m_{x_k}(p) = 0$ 得到，这导出了线性系统 $f''(x_k) p = -\\nabla f(x_k)$。对于我们的一维情况，这简化为 $p_k = -\\nabla f(x_k) / f''(x_k)$。仅当Hessian矩阵 $f''(x_k)$ 为正时，才能保证此步长是一个下降方向（即 $\\nabla f(x_k) p_k < 0$）。\n\n**2. 处理非凸性**\n目标函数 $f(x) = x^4 - 2x^2$ 是非凸的。其Hessian矩阵 $f''(x) = 12x^2 - 4$ 在区间 $|x| \\le 1/\\sqrt{3} \\approx 0.57735$ 内为非正（$f''(x) \\le 0$）。在此区域内，纯牛顿步长不是下降方向，可能会指向一个最大值。指定的算法通过一个标准修正来解决此问题：\n- 如果 $f''(x_k) > 0$，该区域是局部凸的，使用牛顿步长 $p_k = -\\nabla f(x_k)/f''(x_k)$。\n- 如果 $f''(x_k) \\le 0$，纯牛顿步长不可靠。算法默认采用最速下降法，使用方向 $p_k = -\\nabla f(x_k)$。只要梯度非零，该方向始终是下降方向，因为方向导数为 $\\nabla f(x_k) p_k = -\\|\\nabla f(x_k)\\|^2 < 0$。\n- 在像 $x_k=0$ 这样的鞍点处会出现一个特殊情况，此时 $\\nabla f(0) = 0$ 且 $f''(0) = -4 \\le 0$。在这里，牛顿和最速下降方向都为零，会使算法停滞。该问题指定了一个鞍点逃逸机制：如果在非正曲率区域内梯度小于容差 $\\varepsilon_g = 10^{-12}$，则选择一个固定方向 $p_k=+1$ 来打破对称性并将迭代点移离鞍点。\n\n**3. 使用回溯线搜索的阻尼步长**\n即使有了一个有效的下降方向，一个完整的步长 $x_{k+1} = x_k + p_k$ 也可能会增加函数值。通过线搜索寻找一个步长 $\\alpha \\in (0, 1]$ 进行阻尼，以确保取得进展。该算法采用回溯线搜索来满足 Armijo（或充分下降）条件：\n$$f(x_k + \\alpha p_k) \\le f(x_k) + c_1 \\alpha \\nabla f(x_k) p_k$$\n这里，$c_1 \\in (0,1)$ 是一个参数。项 $\\nabla f(x_k) p_k$ 是 $f$ 在 $x_k$ 沿 $p_k$ 方向的方向导数。该条件要求实际的函数减少量至少是函数线性近似所预测减少量的一个分数 $c_1$。算法从一个完整步长 $\\alpha = 1$ 开始，并以一个因子 $\\rho \\in (0,1)$ 连续减小它（即 $\\alpha \\leftarrow \\rho \\alpha$），直到满足条件。\n\n**4. 阻尼策略的比较**\n这两种策略由其参数 $c_1$ 和 $\\rho$ 区分：\n- **激进策略（$c_1 = 10^{-4}, \\rho = 0.7$）**：较小的 $c_1$ 值使得 Armijo 条件更容易满足，从而鼓励采用更大的步长。较大的收缩因子 $\\rho$ 意味着在回溯过程中步长减小得更慢。该策略试图快速取得进展，但有超调的风险。\n- **保守策略（$c_1 = 10^{-2}, \\rho = 0.3$）**：较大的 $c_1$ 值对函数下降提出了更严格的要求，导致步长更小、更谨慎。较小的收缩因子 $\\rho$ 在初始步长不佳时会迅速减小步长。该策略通常更稳健，但收敛速度可能更慢。\n\n**5. 实现算法**\n对于每个起始点 $x_0$ 和每种策略，执行以下迭代过程：\n1. 初始化 $k=0$ 和 $x_0$。\n2. 对于 $k$ 从 $0$到 $K_{\\max}-1$：\n    a. 计算 $f(x_k)$、$\\nabla f(x_k)$ 和 $f''(x_k)$。\n    b. 使用修正的牛顿逻辑选择搜索方向 $p_k$。\n    c. 执行回溯线搜索：\n        i. 初始化 $\\alpha = 1$。\n        ii. 对于 $j=0$ 到 $B-1$：\n            如果 $f(x_k + \\alpha p_k) \\le f(x_k) + c_1 \\alpha \\nabla f(x_k) p_k$，则跳出内循环。\n            否则，$\\alpha \\leftarrow \\rho \\alpha$。\n    d. 更新迭代点：$x_{k+1} = x_k + \\alpha p_k$。\n    e. 检查收敛性：如果 $f(x_{k+1}) \\le -1 + \\varepsilon_f$，停止并报告外部迭代总次数 $k+1$。\n3. 如果循环完成但未收敛，则报告 $K_{\\max}$。\n\n此过程被系统地应用于所有测试用例，以生成所需的比较数据。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and compares two damping strategies for a modified Newton method\n    on a nonconvex 1D function.\n    \"\"\"\n\n    def f(x):\n        \"\"\"Objective function.\"\"\"\n        return x**4 - 2 * x**2\n\n    def grad_f(x):\n        \"\"\"Gradient of the objective function.\"\"\"\n        return 4 * x**3 - 4 * x\n\n    def hess_f(x):\n        \"\"\"Hessian of the objective function.\"\"\"\n        return 12 * x**2 - 4\n\n    def run_newton_method(x0, c1, rho):\n        \"\"\"\n        Executes the modified Newton method for a given starting point and damping policy.\n\n        Args:\n            x0 (float): The starting point.\n            c1 (float): The Armijo condition parameter.\n            rho (float): The backtracking line search contraction factor.\n\n        Returns:\n            int: The number of outer iterations required for convergence, or K_max if not converged.\n        \"\"\"\n        x = float(x0)\n        \n        # Constants and tolerances\n        K_max = 200\n        B_max = 50\n        eps_f = 1e-10\n        eps_g = 1e-12\n        f_target = -1.0 + eps_f\n\n        for k in range(K_max):\n            fx = f(x)\n            grad_fx = grad_f(x)\n            hess_fx = hess_f(x)\n\n            # Step 2: Choose the search direction p_k\n            if hess_fx > 0:\n                p = -grad_fx / hess_fx\n            elif abs(grad_fx) > eps_g:\n                p = -grad_fx\n            else: # Saddle point escape\n                p = 1.0\n\n            # Step 3: Backtracking line search for step size alpha\n            alpha = 1.0\n            \n            # Pre-calculate the directional derivative for the Armijo check\n            directional_deriv = grad_fx * p\n            \n            for _ in range(B_max):\n                x_new = x + alpha * p\n                fx_new = f(x_new)\n                \n                # Armijo condition\n                if fx_new <= fx + c1 * alpha * directional_deriv:\n                    break\n                \n                alpha *= rho\n            else:\n                # If backtracking fails to find a step after B_max attempts,\n                # we proceed with the last computed (and very small) alpha.\n                # In many cases, this may stall the algorithm, leading to K_max.\n                x_new = x + alpha * p\n\n            # Step 4: Update and check for convergence\n            x = x_new\n            \n            if f(x) <= f_target:\n                return k + 1\n\n        return K_max\n\n    # Define the test suite and damping policies\n    test_cases = [\n        2.0,\n        -2.0,\n        0.2,\n        0.0,\n        0.57735,\n        -0.57735,\n    ]\n    \n    aggressive_policy = {'c1': 1e-4, 'rho': 0.7}\n    conservative_policy = {'c1': 1e-2, 'rho': 0.3}\n\n    results = []\n    for x0_val in test_cases:\n        # Run with aggressive policy\n        n_aggressive = run_newton_method(x0_val, **aggressive_policy)\n        \n        # Run with conservative policy\n        n_conservative = run_newton_method(x0_val, **conservative_policy)\n        \n        results.append(f\"[{n_aggressive},{n_conservative}]\")\n\n    # Print the final output in the specified format\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n\n```", "id": "3115942"}, {"introduction": "上一个练习展示了即使有好的下降方向，步长也需要小心选择。然而，当Hessian矩阵非正定时（即存在负曲率），牛顿方向本身甚至可能不再是下降方向。本练习将引导你解决这一根本性问题 [@problem_id:3115922]。你将对比一个朴素的阻尼牛顿法和一个经过修正的牛顿法，后者通过“裁剪”Hessian矩阵的负特征值来强制其为正定。通过在一个具有对角Hessian矩阵的多维函数上进行实验，你将清晰地看到为何需要修正Hessian，并掌握一种确保算法在非凸区域稳健收敛的强大技术。", "problem": "考虑光滑函数 $f:\\mathbb{R}^n\\to\\mathbb{R}$ 的无约束最小化问题，其分量形式定义为\n$$\nf(x) \\;=\\; \\sum_{i=1}^n \\big(x_i^2 - 1\\big)^2.\n$$\n该函数有多个全局最小点，这些点的坐标全为 $+1$ 或 $-1$。此外，它在原点处有一个鞍点。对于此 $f(x)$，其梯度和海森矩阵由众所周知的定义给出\n$$\n\\nabla f(x) \\;=\\; \\left[\\frac{\\partial f}{\\partial x_1}(x),\\dots,\\frac{\\partial f}{\\partial x_n}(x)\\right]^\\top,\\qquad \\nabla^2 f(x) \\;=\\; \\left[\\frac{\\partial^2 f}{\\partial x_i \\partial x_j}(x)\\right]_{i,j=1}^n,\n$$\n并且可以从第一性原理验证\n$$\n\\nabla f(x)_i \\;=\\; 4\\,x_i\\,(x_i^2-1),\\qquad \\nabla^2 f(x) \\;=\\; \\mathrm{diag}\\big(12 x_1^2 - 4,\\,\\dots,\\,12 x_n^2 - 4\\big).\n$$\n在坐标 $x_i\\approx 0$ 附近，曲率 $12 x_i^2 - 4$ 为负，因此海森矩阵是不定的，经典的牛顿方向不一定是下降方向。\n\n你的任务是编写一个完整的程序，为该函数 $f(x)$ 实现并比较两种二阶优化方法：\n\n1) 带回溯线搜索的阻尼牛顿法：\n- 在迭代点 $x_k$ 处，通过求解 $\\nabla^2 f(x_k)\\,p_k = -\\nabla f(x_k)$ 来构建牛顿步长 $p_k$。\n- 如果 $p_k$ 不是下降方向，即 $\\nabla f(x_k)^\\top p_k \\ge 0$，则声明此测试用例失败（对于非下降方向，任何步长都无法满足充分下降条件）。\n- 否则，使用回溯 Armijo 线搜索找到一个步长 $\\alpha_k \\in (0,1]$，使得\n$$\nf(x_k + \\alpha_k p_k) \\;\\le\\; f(x_k) + c_1\\,\\alpha_k\\,\\nabla f(x_k)^\\top p_k,\n$$\n其中 Armijo 常数 $c_1\\in(0,1)$ 和收缩因子 $\\beta\\in(0,1)$ 为固定值（使用 $c_1 = 10^{-4}$ 和 $\\beta = 1/2$）。\n- 更新 $x_{k+1} = x_k + \\alpha_k p_k$ 并重复，直到达到最大迭代次数或梯度范数足够小。\n\n2) 带特征值裁剪和相同回溯线搜索的修正牛顿法：\n- 在迭代点 $x_k$ 处，进行对称特征分解 $\\nabla^2 f(x_k) = Q \\Lambda Q^\\top$，其中 $Q$ 是正交矩阵，$\\Lambda = \\mathrm{diag}(\\lambda_1,\\dots,\\lambda_n)$ 是实数对角矩阵。\n- 定义一个裁剪后的谱 $\\Lambda' = \\mathrm{diag}(\\lambda'_1,\\dots,\\lambda'_n)$，其中\n$$\n\\lambda'_i \\;=\\; \\max\\{\\lambda_i,\\;\\delta\\},\\qquad \\delta \\;=\\; 10^{-3}\\,\\max\\{1,\\;\\max_j|\\lambda_j|\\}.\n$$\n- 定义修正后的海森矩阵 $H_k' = Q \\Lambda' Q^\\top$（其根据构造是对称正定的），并通过求解 $H_k' p_k = -\\nabla f(x_k)$ 来计算 $p_k$。\n- 由于 $H_k'$ 是对称正定的，只要 $\\nabla f(x_k)\\neq 0$，$p_k$ 就保证是一个严格下降方向。使用与上述相同的 Armijo 回溯法来获得 $\\alpha_k$ 并更新 $x_{k+1} = x_k + \\alpha_k p_k$。\n\n对于这两种方法，使用以下实现参数，使问题完全确定且可测试：\n- 对于以下所有测试用例，维度 $n=3$。\n- 最大迭代次数 $k_{\\max} = 100$。\n- 梯度范数容差 $\\|\\nabla f(x_k)\\|_2 \\le 10^{-8}$ 仅作为迭代停止条件；不要将任何稳定点本身视为成功。\n- 当且仅当最终目标函数值满足 $f(x_{\\mathrm{final}}) \\le 10^{-10}$ 时，才声明并记录测试用例成功，这对应于在数值精度范围内达到了全局最小点的盆地。否则声明失败。\n\n测试套件。对以下初始点列表运行这两种方法：\n- 用例 T1（原点附近的负曲率）：$x_0 = (0.1,\\,0.1,\\,0.1)$。\n- 用例 T2（混合曲率，两个点远在负曲率区域之外）：$x_0 = (2.0,\\,-2.0,\\,0.5)$。\n- 用例 T3（精确鞍点）：$x_0 = (0.0,\\,0.0,\\,0.0)$。\n- 用例 T4（远离最小点，但具有正曲率）：$x_0 = (10.0,\\,10.0,\\,10.0)$。\n\n对于每个测试用例，运行这两种方法，并按顺序记录每个用例的两个布尔值：\n- $b_{\\mathrm{DN},\\mathrm{T}i}$ 表示阻尼牛顿法在用例 $\\mathrm{T}i$ 上的结果，\n- $b_{\\mathrm{MN},\\mathrm{T}i}$ 表示带特征值裁剪的修正牛顿法在用例 $\\mathrm{T}i$ 上的结果，\n其中每个布尔值在方法根据 $f(x_{\\mathrm{final}}) \\le 10^{-10}$ 成功时为真，否则为假。\n\n最终输出格式。你的程序应生成单行输出，其中包含一个逗号分隔的 Python 风格列表，该列表包含八个布尔值，顺序完全如下：\n$$\n\\big[ b_{\\mathrm{DN},\\mathrm{T}1},\\; b_{\\mathrm{MN},\\mathrm{T}1},\\; b_{\\mathrm{DN},\\mathrm{T}2},\\; b_{\\mathrm{MN},\\mathrm{T}2},\\; b_{\\mathrm{DN},\\mathrm{T}3},\\; b_{\\mathrm{MN},\\mathrm{T}3},\\; b_{\\mathrm{DN},\\mathrm{T}4},\\; b_{\\mathrm{MN},\\mathrm{T}4} \\big].\n$$\n此问题不涉及物理单位。角度不适用。每个测试的最终答案是指定的布尔值。", "solution": "用户提供的问题被评估为有效。\n\n### 第 1 步：提取已知信息\n- **要最小化的函数**：$f(x) = \\sum_{i=1}^n (x_i^2 - 1)^2$，其中 $f:\\mathbb{R}^n\\to\\mathbb{R}$。\n- **梯度**：$\\nabla f(x)_i = 4x_i(x_i^2-1)$。\n- **海森矩阵**：$\\nabla^2 f(x) = \\mathrm{diag}(12 x_1^2 - 4, \\dots, 12 x_n^2 - 4)$。\n- **维度**：$n=3$。\n- **方法 1（阻尼牛顿法）**：\n    - 牛顿步长：求解 $\\nabla^2 f(x_k) p_k = -\\nabla f(x_k)$。\n    - 失败条件：如果 $p_k$ 不是下降方向，即 $\\nabla f(x_k)^\\top p_k \\ge 0$，则声明失败。\n    - 线搜索：回溯 Armijo 法则，$f(x_k + \\alpha_k p_k) \\le f(x_k) + c_1 \\alpha_k \\nabla f(x_k)^\\top p_k$。\n    - 线搜索参数：Armijo 常数 $c_1 = 10^{-4}$，收缩因子 $\\beta = 1/2$。\n- **方法 2（修正牛顿法）**：\n    - 特征分解：$\\nabla^2 f(x_k) = Q \\Lambda Q^\\top$。\n    - 特征值裁剪：$\\lambda'_i = \\max\\{\\lambda_i, \\delta\\}$。\n    - 裁剪阈值：$\\delta = 10^{-3} \\max\\{1, \\max_j|\\lambda_j|\\}$。\n    - 修正海森矩阵：$H_k' = Q \\Lambda' Q^\\top$。\n    - 修正步长：求解 $H_k' p_k = -\\nabla f(x_k)$。\n    - 线搜索：与方法 1 相同。\n- **通用参数**：\n    - 最大迭代次数：$k_{\\max} = 100$。\n    - 迭代停止条件：$\\|\\nabla f(x_k)\\|_2 \\le 10^{-8}$。\n    - 成功标准：最终目标函数值 $f(x_{\\mathrm{final}}) \\le 10^{-10}$。\n- **测试套件（初始点 $x_0$）**：\n    - T1: $(0.1, 0.1, 0.1)$。\n    - T2: $(2.0, -2.0, 0.5)$。\n    - T3: $(0.0, 0.0, 0.0)$。\n    - T4: $(10.0, 10.0, 10.0)$。\n- **输出**：一个包含 8 个布尔值的列表，指示每种方法在每个测试用例上的成功（$True$）或失败（$False$）。\n\n### 第 2 步：使用提取的已知信息进行验证\n该问题在科学上和数学上是合理的。\n- **科学依据**：该问题是数值优化领域的标准练习，该领域是应用数学和计算机科学的一个子领域。函数、其导数以及优化算法（阻尼牛顿法和修正牛顿法）在该领域都是典型的。\n- **适定性**：该问题是适定的。所有参数、条件以及成功/失败的标准都已明确给出，使得每个测试用例都有唯一、可验证的结果。算法是确定性的。\n- **客观性**：语言精确且量化。成功和失败由客观的数值阈值定义。\n\n该问题没有任何无效性缺陷。它是完整的、一致的且可形式化的。\n\n### 第 3 步：结论与行动\n问题是**有效**的。将提供一个解决方案。\n\n***\n\n### 解题推导\n任务是实现并比较两种二阶优化方法，以最小化函数 $f(x) = \\sum_{i=1}^3 (x_i^2 - 1)^2$。问题的核心在于每种方法如何处理非正定的海森矩阵 $\\nabla^2 f(x)$。\n\n$f(x)$ 的梯度和海森矩阵如下：\n$$\n\\nabla f(x)_i = 4x_i(x_i^2 - 1)\n$$\n$$\n\\nabla^2 f(x) = \\mathrm{diag}(12x_1^2 - 4, 12x_2^2 - 4, 12x_3^2 - 4)\n$$\n海森矩阵是一个对角矩阵。这极大地简化了计算。对于任意分量 $i$，海森矩阵对应的特征值为 $\\lambda_i = 12x_i^2 - 4$。海森矩阵是正定的当且仅当其所有特征值都为正，即对所有 $i=1, 2, 3$ 都有 $12x_i^2 - 4 > 0$。这等价于对所有 $i$ 都有 $|x_i| > 1/\\sqrt{3} \\approx 0.577$。例如，在原点附近，如果 $|x_i| < 1/\\sqrt{3}$，则曲率为负，海森矩阵非正定。\n\n**1. 阻尼牛顿法**\n\n在每次迭代 $k$ 中，该方法从点 $x_k$ 开始，尝试找到一个搜索方向 $p_k$ 和一个步长 $\\alpha_k$。\n\n- **搜索方向 $p_k$**：方向 $p_k$ 是牛顿系统 $\\nabla^2 f(x_k) p_k = -\\nabla f(x_k)$ 的解。由于 $\\nabla^2 f(x_k)$ 是对角矩阵，解可按元素计算：\n$$\np_{k,i} = -\\frac{\\nabla f(x_k)_i}{(\\nabla^2 f(x_k))_{ii}} = -\\frac{4x_{k,i}(x_{k,i}^2-1)}{12x_{k,i}^2-4}\n$$\n- **下降条件**：如果搜索方向 $p_k$ 与梯度向量成钝角，即 $\\nabla f(x_k)^\\top p_k < 0$，则它是一个下降方向。如果此条件成立，则保证沿 $p_k$ 方向移动一个足够小的步长会减小函数值。\n$$\n\\nabla f(x_k)^\\top p_k = \\sum_{i=1}^3 \\nabla f(x_k)_i \\, p_{k,i} = -\\sum_{i=1}^3 \\frac{(\\nabla f(x_k)_i)^2}{12x_{k,i}^2-4}\n$$\n如果海森矩阵是正定的（对所有 $i$ 都有 $12x_{k,i}^2 - 4 > 0$），则 $\\nabla f(x_k)^\\top p_k < 0$（假设 $\\nabla f(x_k) \\neq 0$），且 $p_k$ 是一个下降方向。然而，如果海森矩阵的某些分量为负，则求和中的相应项为正。如果这些项足够大，整个和可能变为正，即 $\\nabla f(x_k)^\\top p_k \\ge 0$。在这种情况下，$p_k$ 不是下降方向，问题中指定的方法将声明该测试用例失败。\n\n- **步长 $\\alpha_k$**：如果 $p_k$ 是下降方向，则执行回溯线搜索以找到步长 $\\alpha_k \\in (0,1]$。它从 $\\alpha_k = 1$ 开始，并重复乘以收缩因子 $\\beta=1/2$，直到满足 Armijo 充分下降条件：\n$$\nf(x_k + \\alpha_k p_k) \\le f(x_k) + c_1 \\alpha_k \\nabla f(x_k)^\\top p_k, \\quad \\text{其中 } c_1 = 10^{-4}\n$$\n- **更新**：下一个迭代点是 $x_{k+1} = x_k + \\alpha_k p_k$。重复该过程，直到达到最大迭代次数 $k_{\\max}=100$ 或满足停止条件 $\\|\\nabla f(x_k)\\|_2 \\le 10^{-8}$。\n\n**2. 带特征值裁剪的修正牛顿法**\n\n此方法修正海森矩阵以确保其始终是正定的，从而保证得到一个下降方向。\n\n- **海森矩阵修正**：在每次迭代 $k$ 中，计算 $\\nabla^2 f(x_k)$ 的特征值。由于海森矩阵是对角矩阵，其特征值就是其对角线元素，$\\lambda_i = 12x_{k,i}^2 - 4$。计算裁剪阈值 $\\delta$：\n$$\n\\delta = 10^{-3} \\max\\{1, \\max_j|\\lambda_j|\\}\n$$\n任何小于此阈值的特征值都会被“裁剪”为 $\\delta$：\n$$\n\\lambda'_i = \\max\\{\\lambda_i, \\delta\\}\n$$\n根据构造，所有 $\\lambda'_i \\ge \\delta > 0$。修正后的海森矩阵 $H'_k$ 是一个对角矩阵，其对角线上的元素是这些裁剪后的特征值。由于其所有特征值都为正，$H'_k$ 是对称正定的。\n\n- **搜索方向 $p_k$**：通过求解修正系统 $H'_k p_k = -\\nabla f(x_k)$ 来找到搜索方向。按元素计算如下：\n$$\np_{k,i} = -\\frac{\\nabla f(x_k)_i}{\\lambda'_i}\n$$\n- **下降条件**：与梯度的内积为：\n$$\n\\nabla f(x_k)^\\top p_k = -\\sum_{i=1}^3 \\frac{(\\nabla f(x_k)_i)^2}{\\lambda'_i}\n$$\n由于对所有 $i$ 都有 $\\lambda'_i > 0$，只要 $\\nabla f(x_k) \\neq 0$，该和就严格为负。因此，$p_k$ 始终是一个严格下降方向，该方法在此阶段不会失败。\n\n- **步长 $\\alpha_k$ 和更新**：使用与阻尼牛顿法相同的回溯线搜索和更新规则。\n\n**测试用例分析**\n\n- **T1: $x_0 = (0.1, 0.1, 0.1)$**：此处， $|x_i| < 1/\\sqrt{3}$，因此所有海森特征值 $\\lambda_i = 12(0.1)^2 - 4 = -3.88$ 均为负。阻尼牛顿法将计算出一个指向原点的方向 $p_k$，而原点是一个鞍点和局部最大值点。该方向是一个上升方向（$\\nabla f(x_k)^\\top p_k > 0$），因此该方法将失败。修正牛顿法会将负特征值裁剪为一个小的正值 $\\delta$，有效地将搜索方向转变为一个缩放的负梯度方向，这将使迭代点远离原点，朝向一个最小点移动。\n\n- **T2: $x_0 = (2.0, -2.0, 0.5)$**：海森矩阵有两个正特征值（$\\lambda_{1,2} = 12(2)^2 - 4 = 44$）和一个负特征值（$\\lambda_3 = 12(0.5)^2 - 4 = -1$）。海森矩阵是不定的。对于阻尼牛顿法，下降条件取决于正曲率分量和负曲率分量之间的平衡。计算表明 $\\nabla f(x_k)^\\top p_k < 0$，因此它是一个下降方向，方法可以继续。修正牛顿法将裁剪掉唯一的负特征值，确保一个稳健的下降方向。预计两种方法都会成功。\n\n- **T3: $x_0 = (0.0, 0.0, 0.0)$**：原点是一个稳定点，$\\nabla f(0) = 0$。两种算法都会在开始时检查梯度范数，发现其为零，并立即终止。最终点是 $x_{\\mathrm{final}}=(0,0,0)$，其中 $f(0)=3$。该值不满足成功条件 $f(x_{\\mathrm{final}}) \\le 10^{-10}$。因此，两种方法都将被声明为该测试用例的失败。\n\n- **T4: $x_0 = (10.0, 10.0, 10.0)$**：远离原点， $|x_i| > 1/\\sqrt{3}$，所有海森特征值 $\\lambda_i = 12(10)^2-4=1196$ 都很大且为正。海森矩阵是强正定的。在这个区域，阻尼牛顿法等同于标准牛顿法，会非常快地收敛。修正牛顿法的裁剪机制不会被触发，因为所有特征值都已经很大且为正（$\\lambda_i > \\delta$）。因此，两种方法的行为将完全相同，预计都会成功。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef f(x: np.ndarray) -> float:\n    \"\"\"Objective function f(x) = sum((x_i^2 - 1)^2).\"\"\"\n    return np.sum((x**2 - 1)**2)\n\ndef grad_f(x: np.ndarray) -> np.ndarray:\n    \"\"\"Gradient of f(x).\"\"\"\n    return 4 * x * (x**2 - 1)\n\ndef hess_f_diag(x: np.ndarray) -> np.ndarray:\n    \"\"\"Diagonal of the Hessian of f(x).\"\"\"\n    return 12 * x**2 - 4\n\ndef damped_newton(x0: np.ndarray) -> tuple[bool, np.ndarray]:\n    \"\"\"\n    Implements the damped Newton method with backtracking line search.\n\n    Returns:\n        A tuple (success, x_final), where success is a boolean indicating\n        if the method converged to a global minimum, and x_final is the\n        final iterate.\n    \"\"\"\n    x = np.copy(x0)\n    k_max = 100\n    grad_tol = 1e-8\n    success_tol_f = 1e-10\n    c1 = 1e-4\n    beta = 0.5\n\n    for _ in range(k_max):\n        g = grad_f(x)\n        if np.linalg.norm(g) <= grad_tol:\n            break\n\n        H_diag = hess_f_diag(x)\n        \n        # Avoid division by zero, although the descent check should handle this.\n        if np.any(np.abs(H_diag) < 1e-15):\n            return f(x) <= success_tol_f, x\n\n        # Newton step\n        p = -g / H_diag\n\n        # Check for descent direction as per problem statement\n        gTp = g @ p\n        if gTp >= 0:\n            return False, x  # Declare failure for the test case\n\n        # Backtracking line search\n        alpha = 1.0\n        fx = f(x)\n        while f(x + alpha * p) > fx + c1 * alpha * gTp:\n            alpha *= beta\n            # Failsafe for extremely small step sizes\n            if alpha < 1e-16:\n                return f(x) <= success_tol_f, x\n        \n        x = x + alpha * p\n        \n    final_f = f(x)\n    return final_f <= success_tol_f, x\n\ndef modified_newton(x0: np.ndarray) -> tuple[bool, np.ndarray]:\n    \"\"\"\n    Implements the modified Newton method with eigenvalue clipping.\n\n    Returns:\n        A tuple (success, x_final) as in damped_newton.\n    \"\"\"\n    x = np.copy(x0)\n    k_max = 100\n    grad_tol = 1e-8\n    success_tol_f = 1e-10\n    c1 = 1e-4\n    beta = 0.5\n    delta_factor = 1e-3\n\n    for _ in range(k_max):\n        g = grad_f(x)\n        if np.linalg.norm(g) <= grad_tol:\n            break\n\n        # For a diagonal Hessian, eigenvalues are the diagonal entries\n        lambda_vals = hess_f_diag(x)\n\n        # Calculate clipping threshold\n        delta = delta_factor * np.max([1.0, np.max(np.abs(lambda_vals))])\n        \n        # Clip eigenvalues\n        lambda_prime = np.maximum(lambda_vals, delta)\n        \n        # Modified Newton step\n        p = -g / lambda_prime\n        \n        # Descent is guaranteed, no explicit check needed\n        gTp = g @ p\n\n        # Backtracking line search\n        alpha = 1.0\n        fx = f(x)\n        while f(x + alpha * p) > fx + c1 * alpha * gTp:\n            alpha *= beta\n            # Failsafe for extremely small step sizes\n            if alpha < 1e-16:\n                return f(x) <= success_tol_f, x\n        \n        x = x + alpha * p\n        \n    final_f = f(x)\n    return final_f <= success_tol_f, x\n\ndef solve():\n    \"\"\"\n    Runs the defined test suite and prints the results in the required format.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        np.array([0.1, 0.1, 0.1]),   # T1\n        np.array([2.0, -2.0, 0.5]),  # T2\n        np.array([0.0, 0.0, 0.0]),   # T3\n        np.array([10.0, 10.0, 10.0]), # T4\n    ]\n\n    results = []\n    for x0 in test_cases:\n        # Run Damped Newton method\n        success_dn, _ = damped_newton(x0)\n        results.append(success_dn)\n        \n        # Run Modified Newton method\n        success_mn, _ = modified_newton(x0)\n        results.append(success_mn)\n\n    # Convert boolean True/False to string \"True\"/\"False\" for printing\n    # Example format: [False,True,True,True,False,False,True,True]\n    # The problem skeleton used ','.join, which has no space.\n    # Python's list-to-string conversion adds spaces. \n    # Sticking to the skeleton's join method.\n    str_results = [str(r) for r in results]\n    print(f\"[{','.join(str_results)}]\")\n\nsolve()\n```", "id": "3115922"}, {"introduction": "我们现在知道，当面对非凸函数时，修正Hessian矩阵是确保牛顿法稳健性的关键。但是，如何修正呢？本练习将带你探索并比较两种主流的Hessian修正策略 [@problem_id:3115918]。第一种是谱校正，直接将Hessian矩阵的负特征值“翻转”为其绝对值；第二种是谱位移，通过给Hessian矩阵加上一个单位矩阵的倍数 $\\lambda I$ 来改变其谱特性。通过在一个具有非对角Hessian矩阵的更复杂的二维非凸函数上实施这两种方法，你将深入了解它们在实践中的表现差异，并为解决更广泛的优化问题打下坚实的基础。", "problem": "考虑在 $\\mathbb{R}^n$ 中对一个二次连续可微函数进行无约束最小化。经典牛顿法在迭代点 $\\mathbf{x}_k$ 处使用局部二次模型，通过求解线性系统 $H_k \\mathbf{p}_k = -\\nabla f(\\mathbf{x}_k)$ 来提出一个步 $\\mathbf{p}_k$，其中 $H_k$ 表示 $f$ 在 $\\mathbf{x}_k$ 处的Hessian矩阵。当 $H_k$ 是不定的或近乎奇异时，纯牛顿方向可能不是一个下降方向，该方法可能会停滞或发散。两种标准的修正是：(i) 谱校正，将 $H_k$ 的负特征值翻转为其绝对值以强制实现正定性；(ii) 阻尼牛顿法，通过添加 $\\lambda I$（其中 $\\lambda \\ge 0$，$I$ 是单位矩阵）来移动 $H_k$ 的谱。\n\n从二阶泰勒展开的基本原理\n$$\nf(\\mathbf{x}_k + \\mathbf{p}) \\approx f(\\mathbf{x}_k) + \\nabla f(\\mathbf{x}_k)^\\top \\mathbf{p} + \\tfrac{1}{2} \\mathbf{p}^\\top H_k \\mathbf{p},\n$$\n以及当 $H_k$ 是正定时牛顿步能最小化二次模型的原则出发，设计并实现一个实验，比较以下方法的收敛行为：\n- 通过特征值校正的修正牛顿法：计算对称特征值分解 $H_k = Q_k \\Lambda_k Q_k^\\top$；构造 $\\tilde{\\Lambda}_k$，其对角线元素为 $\\tilde{\\lambda}_{k,i} = |\\lambda_{k,i}| + \\delta$，其中 $0 < \\delta \\ll 1$ 是一个小的正则化参数；设置 $\\tilde{H}_k = Q_k \\tilde{\\Lambda}_k Q_k^\\top$ 并计算步 $\\mathbf{p}_k = -\\tilde{H}_k^{-1} \\nabla f(\\mathbf{x}_k)$。\n- 通过位移的阻尼牛顿法：对于给定的 $\\lambda \\ge 0$，使用 $H_k^\\lambda = H_k + \\lambda I$ 并计算 $\\mathbf{p}_k = -(H_k^\\lambda)^{-1} \\nabla f(\\mathbf{x}_k)$。\n\n在两种方法中，通过应用满足 Armijo 充分下降条件的回溯线搜索来强制实现全局收敛。具体来说，从步长 $\\alpha_k = 1$ 开始，用 $\\beta \\in (0,1)$ 重复替换 $\\alpha_k \\leftarrow \\beta \\alpha_k$，直到\n$$\nf(\\mathbf{x}_k + \\alpha_k \\mathbf{p}_k) \\le f(\\mathbf{x}_k) + c_1 \\alpha_k \\nabla f(\\mathbf{x}_k)^\\top \\mathbf{p}_k,\n$$\n其中 $c_1 \\in (0,1)$ 是一个小常数。如果在任何迭代中 $\\nabla f(\\mathbf{x}_k)^\\top \\mathbf{p}_k \\ge 0$，则使用最速下降方向 $\\mathbf{p}_k = -\\nabla f(\\mathbf{x}_k)$ 以保证一个下降方向。\n\n在 $\\mathbb{R}^2$ 中使用非凸四次测试函数：\n$$\nf(x,y) = x^4 + y^4 - 3 x^2 - 3 y^2 + 0.5\\, x y + 0.8\\, x^2 y^2,\n$$\n其梯度为\n$$\n\\nabla f(x,y) = \\begin{bmatrix}\n4x^3 - 6x + 0.5\\, y + 1.6\\, x y^2 \\\\\n4y^3 - 6y + 0.5\\, x + 1.6\\, y x^2\n\\end{bmatrix},\n$$\n且Hessian矩阵为\n$$\nH(x,y) =\n\\begin{bmatrix}\n12 x^2 - 6 + 1.6\\, y^2 & 0.5 + 3.2\\, x y \\\\\n0.5 + 3.2\\, x y & 12 y^2 - 6 + 1.6\\, x^2\n\\end{bmatrix}.\n$$\n\n使用基于梯度范数的相同停止准则来实现这两种方法：当 $\\|\\nabla f(\\mathbf{x}_k)\\|_2 \\le \\varepsilon$ 或迭代次数达到 $k_{\\max}$ 时停止。使用以下固定的算法参数：$\\varepsilon = 10^{-6}$，$k_{\\max} = 200$，$c_1 = 10^{-4}$，$\\beta = 0.5$ 以及 $\\delta = 10^{-6}$。\n\n测试套件：\n- 情况1：初始点 $(x_0,y_0) = (1.5,-1.5)$，阻尼牛顿位移 $\\lambda = 1.0$。\n- 情况2：初始点 $(x_0,y_0) = (0.1,0.1)$，阻尼牛顿位移 $\\lambda = 1.0$。\n- 情况3：初始点 $(x_0,y_0) = (3.0,3.0)$，阻尼牛顿位移 $\\lambda = 1.0$。\n- 情况4：初始点 $(x_0,y_0) = (-2.0,0.5)$，阻尼牛顿位移 $\\lambda = 7.0$。\n\n对于每种情况，运行这两种方法并记录达到停止准则 $\\|\\nabla f(\\mathbf{x}_k)\\|_2 \\le \\varepsilon$ 所需的迭代次数（如果未达到，则记录为 $k_{\\max}$）。你的程序应生成单行输出，其中包含这八个结果，这些结果按\n$[\\text{flip\\_iters\\_case1}, \\text{shift\\_iters\\_case1}, \\text{flip\\_iters\\_case2}, \\text{shift\\_iters\\_case2}, \\text{flip\\_iters\\_case3}, \\text{shift\\_iters\\_case3}, \\text{flip\\_iters\\_case4}, \\text{shift\\_iters\\_case4}]$\n的顺序排列在一个扁平列表中，形式为用方括号括起来的逗号分隔列表（例如，$[3,5,7,8,2,4,3,6]$）。所有值必须是整数。", "solution": "该问题提出了一个定义明确的数值实验，用于比较两种修正牛顿法在非凸函数无约束优化中的表现。验证确认了该问题陈述在科学上是合理的、内容是自洽的，并且在算法上是精确的。\n\n牛顿法的核心在于使用从其二阶泰勒展开式导出的二次模型来近似一个函数 $f: \\mathbb{R}^n \\to \\mathbb{R}$：\n$$\nm_k(\\mathbf{p}) = f(\\mathbf{x}_k) + \\nabla f(\\mathbf{x}_k)^\\top \\mathbf{p} + \\frac{1}{2} \\mathbf{p}^\\top H_k \\mathbf{p}\n$$\n其中 $\\mathbf{p}$ 是待求的步，$\\nabla f(\\mathbf{x}_k)$ 是 $f$ 在 $\\mathbf{x}_k$ 处的梯度，而 $H_k$ 是 $f$ 在 $\\mathbf{x}_k$ 处的Hessian矩阵。当Hessian矩阵 $H_k$ 是对称正定时，该二次模型有一个唯一的最小化子，可通过将其关于 $\\mathbf{p}$ 的梯度设为零来找到：$\\nabla_p m_k(\\mathbf{p}) = \\nabla f(\\mathbf{x}_k) + H_k \\mathbf{p} = \\mathbf{0}$。这就产生了纯牛顿步 $\\mathbf{p}_k = -H_k^{-1} \\nabla f(\\mathbf{x}_k)$，通常通过求解线性系统 $H_k \\mathbf{p}_k = -\\nabla f(\\mathbf{x}_k)$ 来计算。\n\n对于非凸函数，Hessian矩阵 $H_k$ 可能是不定的（同时具有正负特征值）或奇异的。在这种情况下，二次模型 $m_k(\\mathbf{p})$ 没有唯一的最小化子，纯牛顿步要么没有良定义，要么可能不是一个下降方向（即，它可能不满足 $\\nabla f(\\mathbf{x}_k)^\\top \\mathbf{p}_k < 0$）。这可能导致优化算法发散或收敛到鞍点或局部最大值。所提出的两种方法是通过确保使用正定矩阵计算步长来解决这一缺陷的标准策略。\n\n**方法一：通过特征值校正的修正牛顿法**\n\n该方法直接修改Hessian矩阵的谱性质。在第 $k$ 次迭代的步骤如下：\n$1$. 计算Hessian矩阵的对称特征值分解：$H_k = Q_k \\Lambda_k Q_k^\\top$，其中 $Q_k$ 是一个正交矩阵，其列是 $H_k$ 的特征向量，$\\Lambda_k$ 是一个由相应特征值 $\\lambda_{k,i}$ 组成的对角矩阵。\n$2$. 构造一个修正的特征值对角矩阵 $\\tilde{\\Lambda}_k$。对于 $H_k$ 的每个特征值 $\\lambda_{k,i}$，相应的修正特征值被设置为 $\\tilde{\\lambda}_{k,i} = |\\lambda_{k,i}| + \\delta$。$|\\lambda_{k,i}|$ 项将任何负特征值“翻转”为正值，而小的正则化参数 $\\delta > 0$（给定为 $10^{-6}$）确保所有结果特征值都是严格正的，从而防止奇异性。\n$3$. 重构一个修正的Hessian矩阵 $\\tilde{H}_k = Q_k \\tilde{\\Lambda}_k Q_k^\\top$。根据构造，$\\tilde{H}_k$ 是对称正定的。\n$4$. 通过求解线性系统 $\\tilde{H}_k \\mathbf{p}_k = -\\nabla f(\\mathbf{x}_k)$ 来计算搜索方向 $\\mathbf{p}_k$。由于 $\\tilde{H}_k$ 是正定的，对于任何非零梯度，所得步长 $\\mathbf{p}_k$ 保证是一个下降方向，因为 $\\nabla f(\\mathbf{x}_k)^\\top \\mathbf{p}_k = - \\nabla f(\\mathbf{x}_k)^\\top \\tilde{H}_k^{-1} \\nabla f(\\mathbf{x}_k) < 0$。\n\n**方法二：通过位移的阻尼牛顿法**\n\n该方法通常与Levenberg-Marquardt算法相关联，它通过添加单位矩阵的倍数来修正Hessian矩阵。\n$1$. 形成一个修正的Hessian矩阵 $H_k^\\lambda = H_k + \\lambda I$，其中 $\\lambda \\ge 0$ 是阻尼参数，$I$ 是单位矩阵。\n$2$. 这个修正将 $H_k$ 的每个特征值都移动了 $\\lambda$。如果 $\\lambda_{k,\\min}$ 是 $H_k$ 的最小特征值，那么 $H_k^\\lambda$ 的特征值是 $\\lambda_{k,i} + \\lambda$。为了保证 $H_k^\\lambda$ 是正定的，必须选择 $\\lambda$ 使得 $\\lambda_{k,\\min} + \\lambda > 0$，即 $\\lambda > -\\lambda_{k,\\min}$。问题为每个测试案例指定了固定的 $\\lambda$ 值。\n$3$. 通过求解系统 $(H_k + \\lambda I) \\mathbf{p}_k = -\\nabla f(\\mathbf{x}_k)$ 来计算搜索方向。如果选择的 $\\lambda$ 足够大以使 $H_k + \\lambda I$ 成为正定矩阵，则保证得到下降方向。\n\n**通过线搜索和保障措施实现全局化**\n\n即使有保证的下降方向，完整步长 $\\mathbf{p}_k$ 可能过长，导致函数值增加。需要一种全局化策略来确保从远离解的初始点收敛。问题指定了一种强制执行 Armijo 充分下降条件的回溯线搜索。\n在每次迭代中，我们寻找一个步长 $\\alpha_k > 0$ 使得：\n$$\nf(\\mathbf{x}_k + \\alpha_k \\mathbf{p}_k) \\le f(\\mathbf{x}_k) + c_1 \\alpha_k \\nabla f(\\mathbf{x}_k)^\\top \\mathbf{p}_k\n$$\n其中给定的常数 $c_1 \\in (0,1)$，此处 $c_1=10^{-4}$。该过程从完整步长（$\\alpha_k=1$）开始，并用一个因子 $\\beta \\in (0,1)$（此处 $\\beta=0.5$）重复减小它，直到满足条件。然后新的迭代点是 $\\mathbf{x}_{k+1} = \\mathbf{x}_k + \\alpha_k \\mathbf{p}_k$。\n\n还包括一个最终的保障措施：如果计算出的搜索方向 $\\mathbf{p}_k$ 由于某种原因未能成为下降方向（即 $\\nabla f(\\mathbf{x}_k)^\\top \\mathbf{p}_k \\ge 0$），算法必须退回到最速下降方向 $\\mathbf{p}_k = -\\nabla f(\\mathbf{x}_k)$。这确保了方向导数 $\\nabla f(\\mathbf{x}_k)^\\top \\mathbf{p}_k = -\\|\\nabla f(\\mathbf{x}_k)\\|_2^2$ 总是负的（除非在驻点），从而保证线搜索最终会找到一个有效的步长 $\\alpha_k > 0$。\n\n实现将遵循这些原则，对每个测试案例进行迭代，直到梯度范数 $\\|\\nabla f(\\mathbf{x}_k)\\|_2$ 低于容差 $\\varepsilon=10^{-6}$ 或达到最大迭代次数 $k_{\\max}=200$。", "answer": "```python\nimport numpy as np\n\ndef f(x_vec):\n    \"\"\"The nonconvex quartic test function.\"\"\"\n    x, y = x_vec[0], x_vec[1]\n    return x**4 + y**4 - 3*x**2 - 3*y**2 + 0.5*x*y + 0.8*x**2*y**2\n\ndef grad_f(x_vec):\n    \"\"\"The gradient of the test function.\"\"\"\n    x, y = x_vec[0], x_vec[1]\n    df_dx = 4*x**3 - 6*x + 0.5*y + 1.6*x*y**2\n    df_dy = 4*y**3 - 6*y + 0.5*x + 1.6*y*x**2\n    return np.array([df_dx, df_dy])\n\ndef hess_f(x_vec):\n    \"\"\"The Hessian of the test function.\"\"\"\n    x, y = x_vec[0], x_vec[1]\n    d2f_dx2 = 12*x**2 - 6 + 1.6*y**2\n    d2f_dy2 = 12*y**2 - 6 + 1.6*x**2\n    d2f_dxdy = 0.5 + 3.2*x*y\n    return np.array([[d2f_dx2, d2f_dxdy], [d2f_dxdy, d2f_dy2]])\n\ndef run_optimization(x0, method, params):\n    \"\"\"\n    Executes a modified Newton method.\n    \n    Args:\n        x0 (np.ndarray): Initial point.\n        method (str): 'flip' or 'shift'.\n        params (dict): Dictionary of algorithmic parameters.\n    \n    Returns:\n        int: Number of iterations taken.\n    \"\"\"\n    x_k = np.copy(x0)\n    \n    k_max = params['k_max']\n    eps = params['eps']\n    delta = params['delta']\n    lambda_val = params.get('lambda', 0.0) # Use get for lambda to avoid error for 'flip'\n    c1 = params['c1']\n    beta = params['beta']\n    \n    for k in range(k_max):\n        g = grad_f(x_k)\n        \n        # Check stopping criterion\n        if np.linalg.norm(g) <= eps:\n            return k\n            \n        H = hess_f(x_k)\n        \n        # Compute search direction p_k\n        if method == 'flip':\n            eigenvalues, Q = np.linalg.eigh(H)\n            Lambda_tilde = np.diag(np.abs(eigenvalues) + delta)\n            H_tilde = Q @ Lambda_tilde @ Q.T\n            try:\n                p_k = np.linalg.solve(H_tilde, -g)\n            except np.linalg.LinAlgError:\n                p_k = -g\n        \n        elif method == 'shift':\n            H_lambda = H + lambda_val * np.eye(len(x0))\n            try:\n                p_k = np.linalg.solve(H_lambda, -g)\n            except np.linalg.LinAlgError:\n                p_k = -g\n        \n        else:\n            raise ValueError(\"Unknown method specified\")\n            \n        # Safeguard: ensure descent direction\n        if g.T @ p_k >= 0:\n            p_k = -g\n            \n        # Backtracking line search (Armijo condition)\n        alpha = 1.0\n        f_k = f(x_k)\n        g_dot_p = g.T @ p_k\n        \n        # A safeguard for the line search loop to prevent extremely small steps\n        ls_max_iter = 50 \n        for _ in range(ls_max_iter):\n            if f(x_k + alpha * p_k) <= f_k + c1 * alpha * g_dot_p:\n                break\n            alpha *= beta\n        else: # if loop finishes without break\n            alpha = 0 # Failed to find a step, stay at the same point\n        \n        # Update iterate\n        x_k = x_k + alpha * p_k\n        \n    return k_max\n\ndef solve():\n    \"\"\"Main function to run the experiment and print results.\"\"\"\n    params = {\n        'eps': 1e-6,\n        'k_max': 200,\n        'c1': 1e-4,\n        'beta': 0.5,\n        'delta': 1e-6\n    }\n    \n    test_cases = [\n        {'x0': np.array([1.5, -1.5]), 'lambda': 1.0},\n        {'x0': np.array([0.1, 0.1]), 'lambda': 1.0},\n        {'x0': np.array([3.0, 3.0]), 'lambda': 1.0},\n        {'x0': np.array([-2.0, 0.5]), 'lambda': 7.0}\n    ]\n    \n    results = []\n    \n    for case in test_cases:\n        x0 = case['x0']\n        \n        # Run eigenvalue correction method\n        flip_iters = run_optimization(x0, 'flip', params)\n        results.append(flip_iters)\n        \n        # Run damped Newton method\n        shift_params = params.copy()\n        shift_params['lambda'] = case['lambda']\n        shift_iters = run_optimization(x0, 'shift', shift_params)\n        results.append(shift_iters)\n        \n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3115918"}]}