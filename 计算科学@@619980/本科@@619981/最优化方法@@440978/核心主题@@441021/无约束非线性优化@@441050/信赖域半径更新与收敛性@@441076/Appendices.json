{"hands_on_practices": [{"introduction": "要掌握一个复杂算法的精髓，最好的方法往往是从一个简化的理想情况入手。在这个练习中，我们将对一个简单的二次函数应用信赖域方法，其中我们的模型是完美的（即 $\\rho_k = 1$）。这种理想化的设定可以排除模型不准确性带来的干扰，使我们能够通过手动计算，清晰地追踪算法的每一步，并观察信赖域半径 $\\Delta_k$ 是如何与梯度范数相互作用，最终促使算法收敛的 [@problem_id:3193997]。", "problem": "考虑一个信赖域方法，用于最小化严格凸二次目标函数 $f(x) = \\frac{1}{2} x^{\\top} Q x + b^{\\top} x$，其中 $Q \\in \\mathbb{R}^{2 \\times 2}$ 是一个已知的对称正定矩阵。假设 $Q = 2 I$，其中 $I$ 是单位矩阵，初始点 $x_0$ 处的初始梯度为 $g_0 = \\nabla f(x_0) = \\begin{pmatrix}1 \\\\ 0\\end{pmatrix}$，因此 $\\|g_0\\| = 1$。在第 $k$ 次迭代中，信赖域子问题是最小化二次模型 $m_k(s) = f(x_k) + g_k^{\\top} s + \\frac{1}{2} s^{\\top} Q s$，约束条件为 $\\|s\\| \\leq \\Delta_k$。假设在理想情况下，对于所有 $k$，实际减少量与预测减少量之比 $\\rho_k = 1$ 成立，并且在每次迭代中都计算出信赖域子问题的精确全局最小化子 $s_k$。\n\n使用以下半径更新规则：\n- 如果计算出的步长满足 $\\|s_k\\| = \\Delta_k$（即步长位于信赖域的边界上），则设置 $\\Delta_{k+1} = \\tau \\Delta_k$，其中 $\\tau = 1.5$。\n- 如果计算出的步长满足 $\\|s_k\\| < \\Delta_k$（即步长严格位于信赖域内部），则保持半径不变：$\\Delta_{k+1} = \\Delta_k$。\n\n设初始信赖域半径为 $\\Delta_0 = 0.1$。在这些假设和更新规则下：\n1. 对于二次模型 $Q = 2 I$，从第一性原理推导出信赖域子问题的精确解 $s_k$，区分边界和内部两种情况，并展示梯度 $\\|g_k\\|$ 是如何演变的。\n2. 使用推导出的关系，计算精确序列 $\\Delta_0, \\Delta_1, \\Delta_2, \\ldots$，直到并包括第一个使得步长为内部解（即 $\\|s_K\\| < \\Delta_K$）的迭代索引 $K$，该步长将导致在下一次迭代中收敛。\n3. 在二次函数信赖域方法的背景下，简要解释激进的信赖域半径增长（较大的 $\\tau$）与数值稳定性之间的权衡。\n\n将你的最终答案以包含 $(\\Delta_0, \\Delta_1, \\ldots, \\Delta_K, K)$ 的行向量形式给出。无需四舍五入。", "solution": "该问题是有效的，因为它在数值优化理论中有科学依据，是适定的，并且表述客观。它要求分析在严格凸二次函数上应用的信赖域方法，这是优化文献中的一个标准说明性案例。所有必需的参数和规则都已提供。\n\n该问题将按要求分三部分解决。\n\n第1部分：子问题解和梯度演变的推导。\n\n第 $k$ 次迭代的信赖域子问题是找到一个步长 $s_k$ 来求解：\n$$ \\min_{s} \\quad m_k(s) = f(x_k) + g_k^{\\top} s + \\frac{1}{2} s^{\\top} Q s $$\n$$ \\text{subject to} \\quad \\|s\\| \\leq \\Delta_k $$\n其中 $g_k = \\nabla f(x_k)$ 且 $Q = 2I$。模型为 $m_k(s) = f(x_k) + g_k^{\\top} s + s^{\\top}s$。\n\n这是一个凸优化问题，因此 Karush-Kuhn-Tucker (KKT) 条件是其最优性的充要条件。拉格朗日函数为：\n$$ \\mathcal{L}(s, \\lambda) = m_k(s) + \\frac{\\lambda}{2} (s^{\\top}s - \\Delta_k^2) = f(x_k) + g_k^{\\top} s + s^{\\top}s + \\frac{\\lambda}{2} (s^{\\top}s - \\Delta_k^2) $$\nKKT 条件是：\n1. 稳定性条件：$\\nabla_s \\mathcal{L}(s, \\lambda) = g_k + 2s + \\lambda s = g_k + (2+\\lambda)s = 0$\n2. 原始可行性：$\\|s\\|^2 \\leq \\Delta_k^2$\n3. 对偶可行性：$\\lambda \\geq 0$\n4. 互补松弛性：$\\lambda (\\|s\\|^2 - \\Delta_k^2) = 0$\n\n从稳定性条件，我们得到 $(2+\\lambda)s = -g_k$。因为 $Q=2I$ 是正定的，对于任何 $\\lambda \\ge 0$ 都有 $2+\\lambda > 0$。因此，我们可以将解 $s$ 写为：\n$$ s_k(\\lambda) = -\\frac{1}{2+\\lambda} g_k $$\n\n我们根据互补松弛性条件分析两种情况。\n\n情况 A：内部解 ($\\|s_k\\| < \\Delta_k$)。\n约束不活跃，这意味着 $\\lambda=0$。步长是模型 $m_k(s)$ 的无约束最小化子：\n$$ s_k = -\\frac{1}{2} g_k $$\n这种情况发生在无约束步长的范数在信赖域半径内时：\n$$ \\|s_k\\| = \\|-\\frac{1}{2} g_k\\| = \\frac{1}{2} \\|g_k\\| < \\Delta_k $$\n这等价于条件 $\\|g_k\\| < 2\\Delta_k$。\n\n情况 B：边界解 ($\\|s_k\\| = \\Delta_k$)。\n约束是活跃的，这意味着 $\\lambda \\ge 0$。我们必须求解 $\\lambda$ 使得 $\\|s_k(\\lambda)\\| = \\Delta_k$。\n$$ \\|s_k(\\lambda)\\| = \\left\\|-\\frac{1}{2+\\lambda} g_k\\right\\| = \\frac{1}{2+\\lambda} \\|g_k\\| = \\Delta_k $$\n求解 $2+\\lambda$ 得到 $2+\\lambda = \\frac{\\|g_k\\|}{\\Delta_k}$。\n将此代回 $s_k$ 的表达式中：\n$$ s_k = -\\frac{1}{(\\|g_k\\|/\\Delta_k)} g_k = -\\Delta_k \\frac{g_k}{\\|g_k\\|} $$\n这是一个沿最速下降方向的步长，其长度等于信赖域半径 $\\Delta_k$。这种情况发生在无约束步长会位于信赖域外部或边界上时，即 $\\frac{1}{2}\\|g_k\\| \\ge \\Delta_k$，或 $\\|g_k\\| \\ge 2\\Delta_k$。\n\n接下来，我们推导梯度的演变。目标函数是 $f(x) = \\frac{1}{2}x^\\top Q x + b^\\top x$，所以它的梯度是 $\\nabla f(x) = Qx+b$。新的迭代点是 $x_{k+1} = x_k + s_k$。新的梯度是：\n$$ g_{k+1} = \\nabla f(x_{k+1}) = Q(x_k + s_k) + b = (Qx_k+b) + Qs_k = g_k + Qs_k $$\n当 $Q=2I$ 时，这变成 $g_{k+1} = g_k + 2s_k$。\n\n让我们检查每种情况下的梯度更新：\n- 对于内部解 ($s_k = -\\frac{1}{2} g_k$):\n  $$ g_{k+1} = g_k + 2(-\\frac{1}{2} g_k) = g_k - g_k = 0 $$\n  算法在这一步找到了精确的最小值。\n- 对于边界解 ($s_k = -\\Delta_k \\frac{g_k}{\\|g_k\\|}$):\n  $$ g_{k+1} = g_k + 2\\left(-\\Delta_k \\frac{g_k}{\\|g_k\\|}\\right) = \\left(1 - \\frac{2\\Delta_k}{\\|g_k\\|}\\right) g_k $$\n  新梯度 $g_{k+1}$ 与 $g_k$ 共线。新梯度的范数是：\n  $$ \\|g_{k+1}\\| = \\left\\| \\left(1 - \\frac{2\\Delta_k}{\\|g_k\\|}\\right) g_k \\right\\| = \\left|1 - \\frac{2\\Delta_k}{\\|g_k\\|}\\right| \\|g_k\\| $$\n  因为这种情况发生在 $\\|g_k\\| \\ge 2\\Delta_k$ 时，所以项 $1 - \\frac{2\\Delta_k}{\\|g_k\\|}$ 是非负的。\n  $$ \\|g_{k+1}\\| = \\left(1 - \\frac{2\\Delta_k}{\\|g_k\\|}\\right) \\|g_k\\| = \\|g_k\\| - 2\\Delta_k $$\n\n第2部分：序列 $\\Delta_0, \\Delta_1, \\ldots, \\Delta_K$ 的计算。\n\n给定 $\\|g_0\\| = 1$ 和 $\\Delta_0 = 0.1$。半径更新规则是：对于边界步，$\\Delta_{k+1} = 1.5 \\Delta_k$；对于内部步，$\\Delta_{k+1} = \\Delta_k$。我们寻找第一个使步长成为内部步的索引 $K$。\n\n迭代 $k=0$:\n- 我们有 $\\|g_0\\| = 1$ 和 $\\Delta_0 = 0.1$。\n- 我们检查条件：$\\|g_0\\| = 1$ 与 $2\\Delta_0 = 2(0.1) = 0.2$。\n- 因为 $1 \\ge 0.2$，我们处于边界情况。步长为 $\\|s_0\\|=\\Delta_0$。\n- 新的梯度范数是 $\\|g_1\\| = \\|g_0\\| - 2\\Delta_0 = 1 - 0.2 = 0.8$。\n- 新的半径是 $\\Delta_1 = 1.5 \\Delta_0 = 1.5 \\times 0.1 = 0.15$。\n\n迭代 $k=1$:\n- 我们有 $\\|g_1\\| = 0.8$ 和 $\\Delta_1 = 0.15$。\n- 我们检查条件：$\\|g_1\\| = 0.8$ 与 $2\\Delta_1 = 2(0.15) = 0.3$。\n- 因为 $0.8 \\ge 0.3$，我们处于边界情况。步长为 $\\|s_1\\|=\\Delta_1$。\n- 新的梯度范数是 $\\|g_2\\| = \\|g_1\\| - 2\\Delta_1 = 0.8 - 0.3 = 0.5$。\n- 新的半径是 $\\Delta_2 = 1.5 \\Delta_1 = 1.5 \\times 0.15 = 0.225$。\n\n迭代 $k=2$:\n- 我们有 $\\|g_2\\| = 0.5$ 和 $\\Delta_2 = 0.225$。\n- 我们检查条件：$\\|g_2\\| = 0.5$ 与 $2\\Delta_2 = 2(0.225) = 0.45$。\n- 因为 $0.5 \\ge 0.45$，我们处于边界情况。步长为 $\\|s_2\\|=\\Delta_2$。\n- 新的梯度范数是 $\\|g_3\\| = \\|g_2\\| - 2\\Delta_2 = 0.5 - 0.45 = 0.05$。\n- 新的半径是 $\\Delta_3 = 1.5 \\Delta_2 = 1.5 \\times 0.225 = 0.3375$。\n\n迭代 $k=3$:\n- 我们有 $\\|g_3\\| = 0.05$ 和 $\\Delta_3 = 0.3375$。\n- 我们检查条件：$\\|g_3\\| = 0.05$ 与 $2\\Delta_3 = 2(0.3375) = 0.675$。\n- 因为 $0.05 < 0.675$，我们处于内部情况。这是第一个内部步。\n- 因此，索引为 $K=3$。\n- 所取的步长为 $s_3 = -\\frac{1}{2}g_3$，其范数为 $\\|s_3\\| = \\frac{1}{2}\\|g_3\\| = \\frac{0.05}{2} = 0.025$。这确实小于 $\\Delta_3=0.3375$。根据规则，半径不会改变，所以 $\\Delta_4 = \\Delta_3$。\n- 算法在下一步终止，因为 $g_4=0$。\n\n直到 $\\Delta_K$ 的半径序列是：\n$\\Delta_0 = 0.1$\n$\\Delta_1 = 0.15$\n$\\Delta_2 = 0.225$\n$\\Delta_3 = 0.3375$\n索引为 $K=3$。\n所求的行向量是 $(\\Delta_0, \\Delta_1, \\Delta_2, \\Delta_3, K) = (0.1, 0.15, 0.225, 0.3375, 3)$。\n\n第3部分：权衡解释。\n\n信赖域半径更新因子 $\\tau$ 控制信赖域大小调整的速度。$\\tau$ 的选择代表了收敛速度和稳健性之间的基本权衡，尤其对于一般的非线性函数。\n\n一种激进的增长策略（大的 $\\tau$，例如 $\\tau > 2$）允许在模型能够很好地预测目标函数时（即实际减少量与预测减少量之比 $\\rho_k$ 很高时），信赖域半径 $\\Delta_k$ 迅速增加。对于像本问题中的二次函数，模型是该函数的完美表示。因此，采取最优牛顿步（$s = -H^{-1}g$）的唯一障碍是信赖域约束。大的 $\\tau$ 允许信赖域快速扩张到一个可以包含牛顿步的大小，从而用更少的迭代次数实现收敛。在这种理想化的情况下，更激进的 $\\tau$ 对性能而言是严格更优的。\n\n然而，在更一般的非二次目标函数的情况下，二次模型 $m_k(s)$ 只是一个局部近似。过度激进的半径扩张可能导致信赖域变得过大，使得模型在区域边界上不再是目标函数的忠实表示。这可能导致计算出的步长 $s_k$ 效果很差，从而使得 $\\rho_k$ 的值很小甚至是负值。结果是，该步长被拒绝，信赖域必须收缩。这可能导致算法在激进的半径扩张和急剧的半径收缩之间振荡，这是低效且数值不稳定的。\n\n一种保守的增长策略（较小的 $\\tau$，例如 $\\tau \\in (1, 2]$）可以促进数值稳定性和稳健性。通过更谨慎地扩张信赖域，模型更有可能在信赖域内保持良好的近似。这导致接受步长并取得稳步进展的概率更高，这在处理高度非线性函数或远离局部最小值时尤其重要。这种稳定性的代价是可能收敛得更慢，因为可能需要更多次迭代才能使信赖域变得足够大，以允许更长、更富成效的步长。\n\n总之，这种权衡是在表现良好（近二次）的函数上实现快速收敛，与在更困难、高度非线性的问题上确保稳定、可靠的进展之间做出的选择。", "answer": "$$ \\boxed{ \\begin{pmatrix} 0.1 & 0.15 & 0.225 & 0.3375 & 3 \\end{pmatrix} } $$", "id": "3193997"}, {"introduction": "现实世界中的优化问题很少像我们第一个例子那样清晰简单。这个练习将带你从理论走向实践，要求你为一个以其狭窄弯曲山谷而闻名的罗森布罗克（Rosenbrock）函数实现一个完整的信赖域求解器 [@problem_id:3193957]。你将通过亲手实践，了解“天真”的参数选择为何会导致算法振荡或信赖域半径过早崩溃，并学会如何调整这些参数以获得稳健的性能，这是任何优化实践者都需具备的关键技能。", "problem": "本题要求您实现并研究一种带有显式信赖域半径更新的信赖域方法，该方法应用于一个具有狭窄弯曲谷的非凸函数。目标是诊断何时朴素的信赖域半径更新规则会导致信赖域半径的振荡或过早崩溃，并展示能够保持进展且仅在驻点附近才将信赖域半径驱动至零的参数选择。\n\n考虑具有狭窄弯曲谷的双变量 Rosenbrock 函数，\n$$\nf(x,y) \\;=\\; 100\\,(y - x^2)^2 + (1 - x)^2,\n$$\n其定义域为所有 $(x,y)\\in \\mathbb{R}^2$。令 $x_k = (x_k^{(1)}, x_k^{(2)})^\\top \\in \\mathbb{R}^2$，并定义梯度 $g_k = \\nabla f(x_k)$ 和 Hessian 矩阵 $B_k = \\nabla^2 f(x_k)$。在第 $k$ 次迭代时，信赖域模型是二阶泰勒近似\n$$\nm_k(s) \\;=\\; f(x_k) + g_k^\\top s + \\tfrac{1}{2} s^\\top B_k s,\n$$\n带有一个信赖域约束 $\\|s\\|_2 \\le \\Delta_k$，其中 $\\Delta_k > 0$ 是信赖域半径，$\\|\\cdot\\|_2$ 表示欧几里得范数。在每次迭代中，您需要求解信赖域子问题的近似解，即在 $\\|s\\|_2 \\le \\Delta_k$ 的约束下最小化 $m_k(s)$。\n\n使用以下基本定义和事实作为您推导和实现的基础：\n- 一个二次连续可微函数的梯度和 Hessian 矩阵被定义为表征其一阶和二阶泰勒模型的一阶和二阶导数。\n- 信赖域比率定义为\n$$\n\\rho_k \\;=\\; \\frac{f(x_k) - f(x_k + s_k)}{m_k(0) - m_k(s_k)} \\;=\\; \\frac{\\text{actual reduction}}{\\text{predicted reduction}}.\n$$\n- 一个经过充分检验的基本接受规则是：如果 $\\rho_k \\ge \\eta$ (其中 $\\eta \\in (0,1)$)，则接受步长 $s_k$。\n- 一个标准的信赖域半径更新规则使用参数 $\\gamma_{\\text{dec}} \\in (0,1)$ 和 $\\gamma_{\\text{inc}} > 1$，并可选地使用第二个阈值 $\\eta_{\\text{inc}} \\in (\\eta,1)$ 来决定何时扩大信赖域。\n\n您的任务：\n1) 使用截断共轭梯度法（Steihaug 型）实现一个信赖域方法，以近似求解子问题。该方法利用局部曲率信息构建搜索方向，并在遇到负曲率或达到信赖域边界时停止，无需显式地对 $B_k$ 求逆。\n2) 使用参数 $\\eta$、$\\eta_{\\text{inc}}$、$\\gamma_{\\text{dec}}$、$\\gamma_{\\text{inc}}$ 实现步长接受和信赖域半径更新规则，其逻辑如下：\n   - 如果 $\\rho_k < \\eta$，则拒绝该步长，并设置 $\\Delta_{k+1} = \\max(\\gamma_{\\text{dec}} \\Delta_k, \\Delta_{\\min})$。\n   - 如果 $\\rho_k \\ge \\eta$，则接受该步长。如果此外还有 $\\rho_k > \\eta_{\\text{inc}}$ 且 $\\|s_k\\|_2$ 位于边界上（即 $\\|s_k\\|_2$ 在数值公差范围内等于 $\\Delta_k$），则设置 $\\Delta_{k+1} = \\min(\\gamma_{\\text{inc}} \\Delta_k, \\Delta_{\\max})$。否则，保持 $\\Delta_{k+1} = \\Delta_k$。\n   - 使用固定的界限 $\\Delta_{\\min}$ 和 $\\Delta_{\\max}$，满足 $0 < \\Delta_{\\min} \\ll 1 \\ll \\Delta_{\\max}$。\n3) 通过追踪 $\\Delta_k$ 是否在梯度范数仍然很大的情况下连续多次迭代变得非常小，来诊断 $\\Delta_k$ 的振荡和过早崩溃。具体来说，定义：\n   - 一个小半径阈值 $\\Delta_{\\text{small}}$，\n   - 一个关于梯度的“远离驻点”阈值 $\\tau_g$，\n   - 一个连续迭代窗口长度 $M$，\n   并且，如果存在一个长度为 $M$ 的索引窗口（忽略最初的 $W$ 次预热迭代），在该窗口的每次迭代中同时满足 $\\Delta_k < \\Delta_{\\text{small}}$ 和 $\\|g_k\\|_2 > \\tau_g$，则断定发生了“远离驻点的过早崩溃”。\n4) 对于数值评估，使用初始点 $x_0 = (-1.2, 1.0)^\\top$，最大迭代次数上限 $K_{\\max}$，以及以下停止和评估阈值：\n   - 用于通过 $\\|g_k\\|_2 \\le \\varepsilon_g$ 判断收敛的驻点容差 $\\varepsilon_g$，\n   - 用于断定函数值有明显下降的目标函数阈值 $f_\\text{target}$，\n   - 参数 $\\Delta_{\\text{small}}$、$\\tau_g$、$M$、$W$ 如上所述。\n\n测试套件：\n提供以下五个测试用例，每个用例都是参数 $(\\eta, \\eta_{\\text{inc}}, \\gamma_{\\text{dec}}, \\gamma_{\\text{inc}}, \\Delta_0, \\Delta_{\\max}, K_{\\max})$ 的元组：\n- 用例 A（朴素、收缩剧烈、接受条件严苛）：$(0.95, 0.98, 0.1, 4.0, 1.0, 100.0, 1000)$。\n- 用例 B（调优、接受条件宽松、更新适中）：$(0.1, 0.75, 0.5, 2.0, 1.0, 100.0, 1000)$。\n- 用例 C（调优、初始半径大）：$(0.1, 0.75, 0.5, 2.0, 10.0, 100.0, 1000)$。\n- 用例 D（调优、初始半径非常小）：$(0.1, 0.75, 0.5, 2.0, 10^{-4}, 100.0, 1000)$。\n- 用例 E（朴素、收缩略缓和但接受条件仍严苛）：$(0.9, 0.9, 0.2, 3.0, 1.0, 100.0, 1000)$。\n\n在所有用例中使用的固定评估阈值：\n- $\\Delta_{\\min} = 10^{-12}$，\n- $\\Delta_{\\text{small}} = 10^{-8}$，\n- $\\tau_g = 10^{-2}$，\n- $M = 15$，\n- $W = 20$，\n- $\\varepsilon_g = 10^{-5}$，\n- $f_\\text{target} = 10^{-4}$。\n\n对于每个测试用例，您的程序应运行信赖域方法，并返回一个包含三个整数的列表，其中整数 1 代表布尔值 true，0 代表 false：\n- $b_1$：是否取得了显著进展，定义为 $f(x_{\\text{final}}) \\le f_\\text{target}$，\n- $b_2$：是否发生了远离驻点的过早崩溃（如第 3 项中所定义），\n- $b_3$：是否收敛到驻点，定义为 $\\|g_{\\text{final}}\\|_2 \\le \\varepsilon_g$。\n\n最终输出格式：\n您的程序应生成单行输出，其中包含所有五个测试用例的结果，格式为一个用方括号括起来的逗号分隔列表，其中每个元素本身是一个列表 $[b_1,b_2,b_3]$。例如：$[[1,0,1],[\\dots],\\dots]$。不应打印任何额外文本。由于不涉及物理单位、角度或百分比，因此无需进行单位转换。以自包含的方式实现所有内容，无需外部输入文件或用户交互。", "solution": "该问题要求实现并分析一个应用于非凸 Rosenbrock 函数的信赖域优化算法。分析的重点是信赖域半径的行为，特别是诊断导致其过早崩溃的条件。解决方案将首先定义目标函数及其导数，然后详细说明信赖域方法的组成部分，包括子问题求解器和半径更新逻辑，最后解释评估标准。\n\n目标函数是双变量 Rosenbrock 函数，定义为：\n$$\nf(x, y) = 100(y - x^2)^2 + (1 - x)^2\n$$\n令变量向量为 $x = (x^{(1)}, x^{(2)})^\\top$。则函数为 $f(x^{(1)}, x^{(2)}) = 100(x^{(2)} - (x^{(1)})^2)^2 + (1 - x^{(1)})^2$。该函数是优化算法的标准基准测试，因为它具有一个狭窄的弯曲谷，通向位于 $(1, 1)^\\top$ 的全局最小值，在该点处 $f(1, 1) = 0$。\n\n对于二阶信赖域方法，我们需要梯度向量 $g(x) = \\nabla f(x)$ 和 Hessian 矩阵 $B(x) = \\nabla^2 f(x)$。\n偏导数如下：\n$$\n\\frac{\\partial f}{\\partial x^{(1)}} = 200(y - x^2)(-2x) - 2(1 - x) = -400x(y - x^2) - 2(1 - x)\n$$\n$$\n\\frac{\\partial f}{\\partial x^{(2)}} = 200(y - x^2)\n$$\n因此，梯度为：\n$$\ng(x, y) = \\begin{pmatrix} -400x(y - x^2) - 2(1 - x) \\\\ 200(y - x^2) \\end{pmatrix}\n$$\n二阶偏导数如下：\n$$\n\\frac{\\partial^2 f}{(\\partial x^{(1)})^2} = -400(y - x^2) - 400x(-2x) + 2 = -400y + 1200x^2 + 2\n$$\n$$\n\\frac{\\partial^2 f}{(\\partial x^{(2)})^2} = 200\n$$\n$$\n\\frac{\\partial^2 f}{\\partial x^{(2)} \\partial x^{(1)}} = \\frac{\\partial^2 f}{\\partial x^{(1)} \\partial x^{(2)}} = -400x\n$$\n这给出了 Hessian 矩阵：\n$$\nB(x, y) = \\begin{pmatrix} 1200x^2 - 400y + 2 & -400x \\\\ -400x & 200 \\end{pmatrix}\n$$\n信赖域方法通过迭代来寻求最小化目标函数。在每次迭代 $k$ 中，从点 $x_k$ 开始，我们构建函数 $f$ 在 $x_k$ 周围的二次模型 $m_k(s)$：\n$$\nm_k(s) = f(x_k) + g_k^\\top s + \\frac{1}{2} s^\\top B_k s\n$$\n其中 $g_k = g(x_k)$ 且 $B_k = B(x_k)$。然后我们找到一个步长 $s_k$，它近似求解信赖域子问题：\n$$\n\\min_{s \\in \\mathbb{R}^2} m_k(s) \\quad \\text{subject to} \\quad \\|s\\|_2 \\le \\Delta_k\n$$\n这里，$\\Delta_k > 0$ 是信赖域半径，它定义了 $x_k$ 周围的一个区域，我们“相信”在该区域内模型是 $f$ 的一个良好近似。\n\n为了解决这个子问题，我们使用截断共轭梯度 (CG) 方法，通常称为 Steihaug-Toint 算法。这种迭代方法非常适用，因为它可以处理不定的 Hessian 矩阵 $B_k$，并自然地包含了信赖域边界。该方法应用 CG 来求解线性系统 $B_k s = -g_k$，但有两个关键的修改：\n1.  **负曲率：** 如果遇到一个方向 $d_j$ 使得 $d_j^\\top B_k d_j \\le 0$，则二次模型 $m_k(s)$ 在该方向上不是凸的。CG 过程被终止，步长 $s_k$ 通过从当前的 CG 迭代点沿 $d_j$ 方向移动直到达到信赖域边界 $\\|s\\|_2 = \\Delta_k$ 来确定。\n2.  **边界相交：** 如果一个 CG 步会导致迭代点 $s_{j+1}$ 超出信赖域（即 $\\|s_{j+1}\\|_2 > \\Delta_k$），则该步长被截断，使其正好落在边界上。该过程被终止。\n\n在这两种触及边界的情况下，我们通过求解二次方程 $\\|s_j + \\tau d_j\\|_2^2 = \\Delta_k^2$ 来找到一个标量 $\\tau > 0$，并取其正根。最终的步长为 $s_k = s_j + \\tau d_j$。\n\n一旦找到子问题的近似解 $s_k$，我们通过比较目标函数的实际减少量与模型预测的减少量来评估其质量。比率 $\\rho_k$ 定义为：\n$$\n\\rho_k = \\frac{\\text{actual reduction}}{\\text{predicted reduction}} = \\frac{f(x_k) - f(x_k + s_k)}{m_k(0) - m_k(s_k)} = \\frac{f(x_k) - f(x_k + s_k)}{-g_k^\\top s_k - \\frac{1}{2} s_k^\\top B_k s_k}\n$$\n对于一个有效的步长，预测减少量应为正。如果不是，则模型很差，我们通过将 $\\rho_k$ 设置为一个能确保拒绝步长的值（例如 $\\rho_k=0$）来处理这种情况。根据 $\\rho_k$ 的值，我们接受或拒绝步长，并更新信赖域半径：\n- 如果 $\\rho_k < \\eta$：模型拟合效果差。拒绝该步长（$x_{k+1} = x_k$），并缩小信赖域：$\\Delta_{k+1} = \\max(\\gamma_{\\text{dec}} \\Delta_k, \\Delta_{\\min})$。\n- 如果 $\\rho_k \\ge \\eta$：模型足够好。接受该步长：$x_{k+1} = x_k + s_k$。然后调整半径。如果模型一致性非常好（$\\rho_k > \\eta_{\\text{inc}}$）并且步长受边界约束（$\\|s_k\\|_2$ 接近 $\\Delta_k$），我们扩大信赖域：$\\Delta_{k+1} = \\min(\\gamma_{\\text{inc}} \\Delta_k, \\Delta_{\\max})$。否则，半径保持不变：$\\Delta_{k+1} = \\Delta_k$。\n\n整个算法通过迭代这些步骤进行，直到梯度范数 $\\|g_k\\|_2$ 低于容差 $\\varepsilon_g$，或达到最大迭代次数 $K_{\\max}$。\n\n分析涉及诊断“远离驻点的过早崩溃”。当信赖域半径 $\\Delta_k$ 变得非常小，阻碍了算法的进展，而迭代点仍远离驻点（即 $\\|g_k\\|_2$ 很大）时，就会发生这种现象。如果在初始 $W$ 次预热迭代后，连续 $M$ 次迭代中，条件 $\\Delta_k < \\Delta_{\\text{small}}$ 和 $\\|g_k\\|_2 > \\tau_g$ 同时成立，则检测到此现象。\n\n最后，对于每个测试用例，我们基于三个标准来评估其性能：\n1.  $b_1$：是否取得了显著进展（$f(x_{\\text{final}}) \\le f_\\text{target}$）。\n2.  $b_2$：是否发生了如上所定义的过早崩溃。\n3.  $b_3$：是否收敛到驻点（$\\|g_{\\text{final}}\\|_2 \\le \\varepsilon_g$）。\n\n参数 $(\\eta, \\eta_{\\text{inc}}, \\gamma_{\\text{dec}}, \\gamma_{\\text{inc}})$ 的选择对性能有关键影响。严格的接受标准（高 $\\eta$）与剧烈的半径收缩（低 $\\gamma_{\\text{dec}}$）相结合，很容易导致过早崩溃，因为算法可能无法在 Rosenbrock 函数的狭窄山谷中导航。相反，更宽松和保守的参数通常会带来稳健的收敛。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef rosenbrock(x):\n    \"\"\"Computes the Rosenbrock function value.\"\"\"\n    return 100.0 * (x[1] - x[0]**2)**2 + (1.0 - x[0])**2\n\ndef rosenbrock_grad(x):\n    \"\"\"Computes the gradient of the Rosenbrock function.\"\"\"\n    grad = np.zeros(2)\n    grad[0] = -400.0 * x[0] * (x[1] - x[0]**2) - 2.0 * (1.0 - x[0])\n    grad[1] = 200.0 * (x[1] - x[0]**2)\n    return grad\n\ndef rosenbrock_hessian(x):\n    \"\"\"Computes the Hessian of the Rosenbrock function.\"\"\"\n    hess = np.zeros((2, 2))\n    hess[0, 0] = 1200.0 * x[0]**2 - 400.0 * x[1] + 2.0\n    hess[0, 1] = -400.0 * x[0]\n    hess[1, 0] = -400.0 * x[0]\n    hess[1, 1] = 200.0\n    return hess\n\ndef truncated_cg(g, B, delta):\n    \"\"\"\n    Solves the trust-region subproblem using the truncated conjugate-gradient\n    (Steihaug-Toint) method.\n    \"\"\"\n    s = np.zeros_like(g)\n    r = g.copy()\n    d = -r.copy()\n    hit_boundary = False\n\n    if np.linalg.norm(r) == 0:\n        return s, hit_boundary\n\n    max_cg_iter = len(g)\n    for j in range(max_cg_iter):\n        dBd = d.T @ B @ d\n\n        if dBd = 0:\n            # Negative curvature detected. Find tau to hit the boundary.\n            a = d.T @ d\n            b = 2 * (s.T @ d)\n            c = s.T @ s - delta**2\n            # We want the positive root of a*tau^2 + b*tau + c = 0\n            tau = (-b + np.sqrt(b**2 - 4 * a * c)) / (2 * a)\n            s += tau * d\n            hit_boundary = True\n            break\n        \n        alpha = (r.T @ r) / dBd\n        s_new = s + alpha * d\n\n        if np.linalg.norm(s_new) >= delta:\n            # Step hits or exceeds boundary. Find tau to be exactly on boundary.\n            a = d.T @ d\n            b = 2 * (s.T @ d)\n            c = s.T @ s - delta**2\n            tau = (-b + np.sqrt(b**2 - 4 * a * c)) / (2 * a)\n            s += tau * d\n            hit_boundary = True\n            break\n\n        s = s_new\n        r_new = r + alpha * (B @ d)\n        \n        # CG convergence check\n        if np.linalg.norm(r_new)  1e-6 * np.linalg.norm(g):\n            break\n\n        beta = (r_new.T @ r_new) / (r.T @ r)\n        r = r_new\n        d = -r + beta * d\n        \n    return s, hit_boundary\n\ndef run_trust_region(params, fixed_params):\n    \"\"\"Runs the trust-region algorithm for a given set of parameters.\"\"\"\n    eta, eta_inc, gamma_dec, gamma_inc, delta_0, delta_max, k_max = params\n    delta_min = fixed_params['delta_min']\n    delta_small = fixed_params['delta_small']\n    tau_g = fixed_params['tau_g']\n    M = fixed_params['M']\n    W = fixed_params['W']\n    eps_g = fixed_params['eps_g']\n    f_target = fixed_params['f_target']\n\n    x = np.array([-1.2, 1.0])\n    delta = delta_0\n\n    delta_history = []\n    gnorm_history = []\n\n    for k in range(k_max):\n        f_k = rosenbrock(x)\n        g_k = rosenbrock_grad(x)\n        B_k = rosenbrock_hessian(x)\n        \n        g_norm = np.linalg.norm(g_k)\n        delta_history.append(delta)\n        gnorm_history.append(g_norm)\n\n        if g_norm = eps_g:\n            break\n\n        s_k, hit_boundary = truncated_cg(g_k, B_k, delta)\n\n        pred_reduction = -(g_k.T @ s_k + 0.5 * s_k.T @ B_k @ s_k)\n        \n        x_new = x + s_k\n        actual_reduction = f_k - rosenbrock(x_new)\n\n        if pred_reduction = 0:\n            rho_k = -1.0 # Will force rejection\n        else:\n            rho_k = actual_reduction / pred_reduction\n\n        if rho_k  eta:\n            # Reject step, shrink radius\n            delta = max(gamma_dec * delta, delta_min)\n        else:\n            # Accept step\n            x = x_new\n            # Update radius\n            if rho_k > eta_inc and hit_boundary:\n                delta = min(gamma_inc * delta, delta_max)\n            # else delta remains the same\n\n    x_final = x\n    f_final = rosenbrock(x_final)\n    g_final_norm = np.linalg.norm(rosenbrock_grad(x_final))\n\n    # Assessment\n    b1 = 1 if f_final = f_target else 0\n    b3 = 1 if g_final_norm = eps_g else 0\n\n    premature_collapse = False\n    collapse_counter = 0\n    if len(delta_history) > W + M:\n        for i in range(W, len(delta_history)):\n            if delta_history[i]  delta_small and gnorm_history[i] > tau_g:\n                collapse_counter += 1\n            else:\n                collapse_counter = 0\n            \n            if collapse_counter >= M:\n                premature_collapse = True\n                break\n    b2 = 1 if premature_collapse else 0\n    \n    return [b1, b2, b3]\n\ndef solve():\n    \"\"\"\n    Main function to run test cases and print results.\n    \"\"\"\n    test_cases = [\n        # (eta, eta_inc, gamma_dec, gamma_inc, delta_0, delta_max, k_max)\n        (0.95, 0.98, 0.1, 4.0, 1.0, 100.0, 1000), # Case A\n        (0.1, 0.75, 0.5, 2.0, 1.0, 100.0, 1000),  # Case B\n        (0.1, 0.75, 0.5, 2.0, 10.0, 100.0, 1000), # Case C\n        (0.1, 0.75, 0.5, 2.0, 1e-4, 100.0, 1000), # Case D\n        (0.9, 0.9, 0.2, 3.0, 1.0, 100.0, 1000),   # Case E\n    ]\n\n    fixed_params = {\n        'delta_min': 1e-12,\n        'delta_small': 1e-8,\n        'tau_g': 1e-2,\n        'M': 15,\n        'W': 20,\n        'eps_g': 1e-5,\n        'f_target': 1e-4,\n    }\n\n    results = []\n    for case in test_cases:\n        result = run_trust_region(case, fixed_params)\n        results.append(result)\n\n    # Format the final output string exactly as specified.\n    formatted_results = [f\"[{r[0]},{r[1]},{r[2]}]\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3193957"}, {"introduction": "标准算法在某些特定的“病态”情况下可能会失效。这项高级练习将让你扮演算法设计者的角色，去诊断并解决一个具体的问题 [@problem_id:3193978]。你将分析一个具有几乎平坦山脊的函数，在这种情况下，标准的半径更新规则会因步长过小而停滞不前。你的任务是分析其失效的原因，并实现一个更“智能”的更新规则来摆脱这个陷阱，从而展示出对收敛性质更深层次的理解。", "problem": "要求您设计并实现一个程序，用于检验一个二次连续可微且带有近乎平坦山脊的目标函数的信赖域半径更新规则，并提出一个有原则的修改方案，使得即使在模型下降量很小的情况下，只要模型高度可靠，也能增大信赖域半径。核心关注点在于如何、为何以及何时更新信赖域半径，以确保算法能够从平坦区域收敛，在这些区域中，即使模型与真实目标函数之间的一致性很好，步长也仍然非常小。\n\n使用的基本方法是无约束优化的标准信赖域方法。在迭代点 $x_k \\in \\mathbb{R}^n$，二次模型为\n$$\nm_k(p) = f(x_k) + g_k^\\top p + \\tfrac{1}{2} p^\\top B_k p,\n$$\n其中 $g_k = \\nabla f(x_k)$，$B_k = \\nabla^2 f(x_k)$ 是Hessian矩阵。信赖域子问题寻求\n$$\n\\min_{p \\in \\mathbb{R}^n} \\; m_k(p) \\quad \\text{subject to} \\quad \\|p\\| \\le \\Delta_k,\n$$\n其中 $\\Delta_k  0$ 是当前的信赖域半径。候选步长 $p_k$ 通过以下比率来评判\n$$\n\\rho_k = \\frac{f(x_k) - f(x_k + p_k)}{m_k(0) - m_k(p_k)},\n$$\n该比率比较了实际减少量与预测减少量。标准更新规则在 $\\rho_k$ 很小时缩小 $\\Delta_k$，在 $\\rho_k$ 很大且步长位于边界上或边界附近时增大 $\\Delta_k$。\n\n构造一个带有近乎平坦山脊的光滑目标函数：\n$$\nf(x_1, x_2) = \\sqrt{\\varepsilon + x_1^2} + \\tfrac{\\kappa}{2} x_2^2,\n$$\n其中 $\\varepsilon  0$ 和 $\\kappa  0$ 是固定的正常量。其梯度和Hessian矩阵为\n$$\ng(x) = \\begin{bmatrix} \\dfrac{x_1}{\\sqrt{\\varepsilon + x_1^2}} \\\\ \\kappa x_2 \\end{bmatrix}, \\qquad\nB(x) = \\begin{bmatrix} \\dfrac{\\varepsilon}{(\\varepsilon + x_1^2)^{3/2}}  0 \\\\ 0  \\kappa \\end{bmatrix}.\n$$\n注意，在 $x_1 \\approx 0$ 的山脊附近，$x_1$ 方向的曲率很大，而函数值沿 $x_1$ 变化非常缓慢，且分量 $g_1$ 可能非常小；这可能产生 $\\rho_k \\approx 1$ 但 $\\|p_k\\|$ 非常小的步长 $p_k$，因此标准规则可能不会增大 $\\Delta_k$。\n\n您的任务：\n- 在维度 $n=2$ 上，使用拉格朗日乘子最优性条件为信赖域子问题实现一个精确求解器：找到 $\\lambda \\ge 0$ 使得\n$$\n(B(x_k) + \\lambda I) p_k = -g(x_k), \\quad \\text{with} \\quad \\|p_k\\| \\le \\Delta_k,\n$$\n如果对于 $\\lambda=0$ 有 $\\|p_k\\|  \\Delta_k$，则增加 $\\lambda$ 直到 $\\|p_k\\| = \\Delta_k$（对 $\\lambda$ 使用稳健的二分法）。\n- 计算实际减少量 $f(x_k) - f(x_k + p_k)$ 和预测减少量 $m_k(0) - m_k(p_k) = -g_k^\\top p_k - \\tfrac{1}{2} p_k^\\top B_k p_k$，然后计算 $\\rho_k$。\n- 实现标准的信赖域半径更新决策规则：\n  - 如果 $\\rho_k  0.25$，将决策标记为“缩小”（-1）。\n  - 否则，如果 $\\rho_k  0.75$ 且 $\\|p_k\\| \\ge 0.8 \\Delta_k$，标记为“增大”（+1）。\n  - 否则，标记为“不变”（0）。\n- 提出并实现一个能感知停滞的扩展规则，以逃离平坦区域，即使模型下降量很小：\n  - 如果 $\\rho_k \\ge \\eta_{\\text{high}}$ 且 $\\|p_k\\| \\le \\chi \\Delta_k$ 且 $m_k(0) - m_k(p_k) \\le c_{\\text{pred}} \\|g_k\\| \\Delta_k$ 且 $\\|g_k\\|  \\tau_g$，标记为“增大”（+1）。\n  - 否则，如果 $\\rho_k  0.25$，标记为“缩小”（-1）。\n  - 否则，如果 $\\rho_k  0.75$ 且 $\\|p_k\\| \\ge 0.8 \\Delta_k$，标记为“增大”（+1）。\n  - 否则，标记为“不变”（0）。\n此处 $\\eta_{\\text{high}}$、$\\chi$、$c_{\\text{pred}}$ 和 $\\tau_g$ 是固定阈值，您必须选择在此尺度下合理的阈值；使用 $\\eta_{\\text{high}} = 0.9$、$\\chi = 0.2$、$c_{\\text{pred}} = 0.2$ 和 $\\tau_g = 10^{-8}$。\n\n测试套件规范：\n使用 $\\varepsilon = 10^{-6}$ 和 $\\kappa = 20$。对于以下每种情况，程序必须计算 $p_k$、$\\rho_k$，并返回决策对 $[d_{\\text{std}}, d_{\\text{prop}}]$，其中 $d_{\\text{std}}, d_{\\text{prop}} \\in \\{-1, 0, +1\\}$：\n1. $x_k = (10^{-8}, 10^{-3})$，$\\Delta_k = 10^{-2}$：近乎平坦的山脊，伴随微小的内部步长和 $\\rho_k \\approx 1$；感知停滞的规则应增大半径，而标准规则不应增大。\n2. $x_k = (10^{-1}, 0)$，$\\Delta_k = 5 \\cdot 10^{-2}$：边界附近模型一致性良好；两个规则都应增大半径。\n3. $x_k = (2, 0)$，$\\Delta_k = 10$：模型一致性差，候选步长非常大；两个规则都应缩小半径。\n4. $x_k = (0, 0)$，$\\Delta_k = 10^{-1}$：零梯度边缘情况；两个规则都应返回不变。\n\n最终输出格式：\n您的程序应生成一行输出，其中包含测试套件的结果，格式为逗号分隔的Python风格列表的列表，按顺序包含每种情况的决策，例如，`[[d_1^{\\text{std}}, d_1^{\\text{prop}}],[d_2^{\\text{std}}, d_2^{\\text{prop}}],\\dots]`。此问题不涉及任何物理单位或角度单位。输出中的所有数字答案均为 $\\{-1,0,1\\}$ 中的整数，且聚合输出是这些整数对的列表。", "solution": "用户提出了一个数值优化领域的有效问题。任务是分析和比较两种不同的信赖域半径更新策略，针对一个已知会导致缓慢收敛的特定目标函数。该问题具有科学依据，是适定的，并包含了获得唯一、可验证解所需的所有信息。\n\n该问题的核心是使用信赖域方法对一个二次连续可微的目标函数 $f: \\mathbb{R}^2 \\to \\mathbb{R}$ 进行无约束优化。该特定函数被设计为具有近乎平坦的山脊，这对标准算法构成了挑战。该函数由下式给出：\n$$\nf(x_1, x_2) = \\sqrt{\\varepsilon + x_1^2} + \\frac{\\kappa}{2} x_2^2\n$$\n其中 $\\varepsilon  0$ 和 $\\kappa  0$ 是固定的正常量。在我们的测试中，$\\varepsilon = 10^{-6}$ 且 $\\kappa = 20$。梯度 $g(x) = \\nabla f(x)$ 和Hessian矩阵 $B(x) = \\nabla^2 f(x)$ 分别是：\n$$\ng(x) = \\begin{bmatrix} \\frac{x_1}{\\sqrt{\\varepsilon + x_1^2}} \\\\ \\kappa x_2 \\end{bmatrix}, \\qquad\nB(x) = \\begin{bmatrix} \\frac{\\varepsilon}{(\\varepsilon + x_1^2)^{3/2}}  0 \\\\ 0  \\kappa \\end{bmatrix}\n$$\n在每次迭代 $k$，给定一个点 $x_k$，我们构建目标函数的一个二次模型：\n$$\nm_k(p) = f(x_k) + g_k^\\top p + \\frac{1}{2} p^\\top B_k p\n$$\n其中 $g_k = g(x_k)$ 且 $B_k = B(x_k)$。信赖域方法的核心是找到一个步长 $p_k$，它能近似求解信赖域子问题：\n$$\n\\min_{p \\in \\mathbb{R}^2} \\; m_k(p) \\quad \\text{subject to} \\quad \\|p\\|_2 \\le \\Delta_k\n$$\n其中 $\\Delta_k  0$ 是信赖域半径。\n\n为了解决这个子问题，我们使用 Karush-Kuhn-Tucker (KKT) 条件。一个解 $p_k$ 必须满足：\n$$\n(B_k + \\lambda I) p_k = -g_k\n$$\n对于某个拉格朗日乘子 $\\lambda \\ge 0$。此处，$I$ 是 $2 \\times 2$ 的单位矩阵。这些条件还要求 $\\lambda(\\Delta_k - \\|p_k\\|_2) = 0$。由于 $\\varepsilon  0$ 和 $\\kappa  0$，Hessian矩阵 $B_k$ 是一个具有正对角元素的对角矩阵，这意味着它是正定的。这极大地简化了子问题的求解。寻找 $p_k$ 的算法如下：\n1. 通过设置 $\\lambda=0$ 来计算完整的牛顿步（$m_k(p)$ 的无约束最小化子）：$p_k(0) = -B_k^{-1} g_k$。\n2. 如果 $\\|p_k(0)\\|_2 \\le \\Delta_k$，牛顿步位于信赖域内，是该子问题的最优解。因此，$p_k = p_k(0)$。这是内部解。\n3. 如果 $\\|p_k(0)\\|_2  \\Delta_k$，解必须位于信赖域的边界上，即 $\\|p_k\\|_2 = \\Delta_k$。我们必须找到一个满足此条件的 $\\lambda  0$。步长作为 $\\lambda$ 的函数是 $p_k(\\lambda) = -(B_k + \\lambda I)^{-1} g_k$。我们必须求解关于 $\\lambda$ 的标量非线性长期方程 $\\|p_k(\\lambda)\\|_2 - \\Delta_k = 0$。由于 $B_k$ 是对角矩阵，其对角元素为 $b_{11}$ 和 $b_{22}$，方程变为：\n$$\n\\sqrt{\\left(\\frac{-g_1}{b_{11} + \\lambda}\\right)^2 + \\left(\\frac{-g_2}{b_{22} + \\lambda}\\right)^2} = \\Delta_k\n$$\n使用数值求根方法，特别是二分法，求解此方程以得到 $\\lambda  0$。由于函数 $\\|p_k(\\lambda)\\|_2$ 对于 $\\lambda  0$ 是单调递减的，因此二分法是稳健的。\n\n一旦计算出步长 $p_k$，其质量通过实际减少量与模型预测的减少量之比 $\\rho_k$ 来评估：\n$$\n\\rho_k = \\frac{\\text{Ared}}{\\text{Pred}} = \\frac{f(x_k) - f(x_k + p_k)}{m_k(0) - m_k(p_k)}\n$$\n预测减少量由 $m_k(0) - m_k(p_k) = -g_k^\\top p_k - \\frac{1}{2} p_k^\\top B_k p_k$ 给出。如果 $Pred = 0$，这种情况当且仅当 $g_k = 0$ 时发生（因为 $B_k$ 是正定的），此时 $p_k=0$ 且 $Ared=0$。在这种情况下，$\\rho_k$ 未定义，信赖域半径应保持不变。\n\n研究的核心在于为下一次迭代更新 $\\Delta_k$ 为 $\\Delta_{k+1}$ 的规则。\n标准更新规则基于 $\\rho_k$ 和步长相对于半径的长度：\n- 如果 $\\rho_k  0.25$：模型预测效果差。缩小信赖域。决策为“缩小”（-1）。\n- 否则，如果 $\\rho_k  0.75$ 且 $\\|p_k\\|_2 \\ge 0.8 \\Delta_k$：模型预测效果好，且步长受边界约束。增大信赖域。决策为“增大”（+1）。\n- 否则：步长可接受，但没有充分理由改变半径。决策为“不变”（0）。\n\n所提出的感知停滞的扩展规则旨在处理模型准确（$\\rho_k$ 很高）但步长 $p_k$ 和预测减少量非常小的情况。这发生在平坦区域。该规则在这些条件下优先进行扩展，以更快地逃离平坦区域。其逻辑是顺序的：\n1. 如果 $\\rho_k \\ge \\eta_{\\text{high}}$ ($0.9$)、$\\|p_k\\|_2 \\le \\chi \\Delta_k$ ($0.2$)、$Pred \\le c_{\\text{pred}} \\|g_k\\|_2 \\Delta_k$ ($0.2$) 且 $\\|g_k\\|_2  \\tau_g$ ($10^{-8}$): 这些条件识别出一个非驻点区域中的小而高度精确的步长，其中线性项预测的下降量很小。增大信赖域。决策为“增大”（+1）。\n2. 否则，如果 $\\rho_k  0.25$：缩小信赖域。决策为“缩小”（-1）。\n3. 否则，如果 $\\rho_k  0.75$ 且 $\\|p_k\\|_2 \\ge 0.8 \\Delta_k$：标准增大条件。决策为“增大”（+1）。\n4. 否则：决策为“不变”（0）。\n\n将此框架应用于测试用例，得出以下分析：\n\n情况 1：$x_k = (10^{-8}, 10^{-3})$，$\\Delta_k = 10^{-2}$。\n此时，$x_1$ 非常接近 $0$。梯度分量 $g_1$ 非常小（$\\approx 10^{-5}$），而Hessian分量 $b_{11}$ 很大（$\\approx 1000$）。得到的牛顿步是一个内部解（$p_k \\approx (-10^{-8}, -10^{-3})$），其范数 $\\|p_k\\|_2 \\approx 10^{-3}$ 非常小。模型高度精确，导致 $\\rho_k \\approx 1$。\n- 标准规则：$\\rho_k  0.75$，但 $\\|p_k\\|_2 \\approx 10^{-3}$ 远小于 $0.8 \\Delta_k = 8 \\cdot 10^{-3}$。因此，决策为“不变”（0）。\n- 所提规则：停滞条件被触发。$\\rho_k \\approx 1 \\ge 0.9$，$\\|p_k\\|_2 \\approx 10^{-3} \\le 0.2 \\Delta_k = 2 \\cdot 10^{-3}$，预测减少量很小，满足 $c_{pred}$ 准则，且 $\\|g_k\\|_2 \\approx 2 \\cdot 10^{-2}  10^{-8}$。所有条件都满足，导致做出“增大”（+1）的决策。这展示了所提规则的优势。\n\n情况 2：$x_k = (10^{-1}, 0)$，$\\Delta_k = 5 \\cdot 10^{-2}$。\n牛顿步非常大，因此解 $p_k$ 位于信赖域边界上，且 $\\|p_k\\|_2 = \\Delta_k$。模型一致性极好，$\\rho_k \\approx 1$。\n- 两个规则：条件 $\\rho_k  0.75$ 和 $\\|p_k\\|_2 \\ge 0.8 \\Delta_k$ 得到满足。所提规则的停滞条件不满足，因为 $\\|p_k\\|_2 = \\Delta_k$ 违反了 $\\|p_k\\|_2 \\le \\chi \\Delta_k$。两个规则都正确地决定“增大”（+1）。\n\n情况 3：$x_k = (2, 0)$，$\\Delta_k = 10$。\n远离原点时，函数在 $x_1$ 方向上几乎是线性的，因此二阶模型在长距离上的近似效果很差。步长 $p_k$ 在边界上，且 $\\|p_k\\|_2 = 10$。步长应用后，实际函数值增加，导致实际减少量为负（$Ared  0$），因此 $\\rho_k  0$。\n- 两个规则：由于 $\\rho_k  0.25$，两个规则都正确地决定“缩小”（-1）。\n\n情况 4：$x_k = (0, 0)$，$\\Delta_k = 10^{-1}$。\n这是一个驻点，因为 $g_k = (0,0)$。最优步长为 $p_k = 0$。因此，$Ared=0$ 且 $Pred=0$。在这种情况下，$\\rho_k$ 未定义。没有理由改变信赖域半径。\n- 两个规则：逻辑通过默认“不变”（0）来处理这种情况。所提规则中的特定条件 $\\|g_k\\|_2  \\tau_g$ 明确防止了在驻点处进行扩展。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import root_scalar\n\n# --- Problem Definition ---\n\n# Fixed parameters for the objective function\nEPSILON = 1e-6\nKAPPA = 20.0\n\n# Thresholds for the proposed stagnation-aware rule\nETA_HIGH = 0.9\nCHI = 0.2\nC_PRED = 0.2\nTAU_G = 1e-8\n\ndef f_obj(x, eps=EPSILON, kap=KAPPA):\n    \"\"\"Objective function f(x_1, x_2).\"\"\"\n    return np.sqrt(eps + x[0]**2) + (kap / 2.0) * x[1]**2\n\ndef g_grad(x, eps=EPSILON, kap=KAPPA):\n    \"\"\"Gradient of the objective function.\"\"\"\n    g1 = x[0] / np.sqrt(eps + x[0]**2)\n    g2 = kap * x[1]\n    return np.array([g1, g2])\n\ndef B_hess(x, eps=EPSILON, kap=KAPPA):\n    \"\"\"Hessian of the objective function.\"\"\"\n    b11 = eps / (eps + x[0]**2)**1.5\n    b22 = kap\n    # The Hessian is diagonal\n    return np.diag([b11, b22])\n\ndef solve_tr_subproblem(gk, Bk, delta_k):\n    \"\"\"\n    Solves the 2D trust-region subproblem min m(p) s.t. ||p|| = delta_k\n    using the KKT conditions and bisection for the boundary case.\n    \"\"\"\n    if np.linalg.norm(gk) == 0:\n        return np.zeros(2)\n\n    # Since B_k is positive definite, we can compute the Newton step.\n    try:\n        p_newton = -np.linalg.solve(Bk, gk)\n    except np.linalg.LinAlgError:\n        # This case is not expected here as Bk is positive definite\n        p_newton = -np.linalg.pinv(Bk) @ gk\n    \n    if np.linalg.norm(p_newton) = delta_k:\n        # Interior solution\n        return p_newton\n\n    # Boundary solution: find lambda > 0 such that ||p(lambda)|| = delta_k\n    b_diag = np.diag(Bk)\n    \n    def secular_eq(lam):\n        # Using the diagonal structure of Bk for p(lambda)\n        p_lam = -gk / (b_diag + lam)\n        return np.linalg.norm(p_lam) - delta_k\n    \n    # Establish a safe search bracket [lambda_low, lambda_high] for bisection.\n    lambda_low = 0.0\n    # An upper bound can be derived from ||g||/delta_k. Start there and increase if needed.\n    lambda_high = np.linalg.norm(gk) / delta_k\n    while secular_eq(lambda_high) > 0:\n        lambda_high *= 2.0\n\n    # Use a robust root-finding method (bisection) to find lambda\n    sol = root_scalar(secular_eq, bracket=[lambda_low, lambda_high], method='bisect')\n    lam_star = sol.root\n    \n    p_star = -gk / (b_diag + lam_star)\n    return p_star\n\ndef compute_decisions(xk, pk, delta_k):\n    \"\"\"\n    Computes rho_k and returns the decisions for both standard and proposed rules.\n    \"\"\"\n    gk = g_grad(xk)\n    Bk = B_hess(xk)\n\n    actual_reduction = f_obj(xk) - f_obj(xk + pk)\n    predicted_reduction = -gk.T @ pk - 0.5 * pk.T @ Bk @ pk\n\n    if abs(predicted_reduction)  1e-15:\n        # If predicted reduction is zero (e.g. g_k=0), p_k=0, no change.\n        return [0, 0]\n\n    rho_k = actual_reduction / predicted_reduction\n    norm_pk = np.linalg.norm(pk)\n    norm_gk = np.linalg.norm(gk)\n\n    # Standard rule\n    d_std = 0\n    if rho_k  0.25:\n        d_std = -1\n    elif rho_k > 0.75 and norm_pk >= 0.8 * delta_k:\n        d_std = 1\n\n    # Proposed stagnation-aware rule\n    d_prop = 0\n    # The conditions must be checked in the specified order (if-elif-else)\n    stagnation_increase = (\n        rho_k >= ETA_HIGH and\n        norm_pk = CHI * delta_k and\n        predicted_reduction = C_PRED * norm_gk * delta_k and\n        norm_gk > TAU_G\n    )\n    \n    if stagnation_increase:\n        d_prop = 1\n    elif rho_k  0.25:\n        d_prop = -1\n    elif rho_k > 0.75 and norm_pk >= 0.8 * delta_k:\n        d_prop = 1\n    \n    return [d_std, d_prop]\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and compute decisions.\n    \"\"\"\n    # Test cases: (x_k_tuple, delta_k)\n    test_cases = [\n        ((1e-8, 1e-3), 1e-2),\n        ((1e-1, 0.0), 5e-2),\n        ((2.0, 0.0), 10.0),\n        ((0.0, 0.0), 1e-1),\n    ]\n\n    results = []\n    for case in test_cases:\n        x_k_tuple, delta_k = case\n        x_k = np.array(x_k_tuple)\n        \n        # Calculate g_k and B_k at the current point\n        g_k = g_grad(x_k)\n        B_k = B_hess(x_k)\n        \n        # Solve the trust-region subproblem to get the step p_k\n        p_k = solve_tr_subproblem(g_k, B_k, delta_k)\n\n        # Compute the decisions based on the two different update rules\n        decisions = compute_decisions(x_k, p_k, delta_k)\n        results.append(decisions)\n\n    # Format the final output string exactly as specified\n    result_strs = [f\"[{r[0]},{r[1]}]\" for r in results]\n    final_output = f\"[{','.join(result_strs)}]\"\n    print(final_output)\n\nif __name__ == '__main__':\n    solve()\n```", "id": "3193978"}]}