{"hands_on_practices": [{"introduction": "在实践中，我们如何判断一个优化算法已经收敛？仅依赖单个指标可能并不可靠。例如，在平坦区域，梯度范数可能很小，但迭代点离最优点仍很远。本练习将指导你设计并实现一个“字典序”停止规则，它以明确的优先级组合了三个核心收敛指标：梯度范数、步长范数和函数值下降量。通过在非凸测试函数上评估此规则，你将亲身体验如何构建一个更全面、更鲁棒的停止策略 [@problem_id:3187944]。", "problem": "要求您为一种迭代优化方法设计并实现一个字典序停止准则，该准则结合了梯度范数、步长范数和每次迭代的函数值下降量，并在非凸基准测试函数上对其进行评估。实现必须是一个完整的、可运行的程序。\n\n使用的基本原理是无约束可微优化的一阶必要最优性条件：对于一个可微函数 $f:\\mathbb{R}^n\\to\\mathbb{R}$ 的局部极小值点 $x^\\star$，其梯度满足 $\\nabla f(x^\\star)=\\mathbf{0}$。在数值计算中，这一原理促使我们在梯度范数很小时停止迭代。然而，数值停滞也可以通过小步长或可忽略的函数值下降来检测。您的任务是推导一个对这些标准进行优先级排序的字典序停止准则，并在带回溯线搜索的梯度下降法中实现它。\n\n将迭代方法定义如下。给定初始点 $x_0\\in\\mathbb{R}^n$，在第 $k$ 次迭代时，当前点为 $x_k$，计算梯度 $g_k=\\nabla f(x_k)$ 及其范数 $G_k=\\lVert g_k\\rVert_2$。从初始步长 $t_0>0$ 开始，使用收缩因子 $\\beta\\in(0,1)$ 和 Armijo 参数 $c\\in(0,1)$ 执行回溯线搜索，以找到一个步长 $t_k$，使得 Armijo 条件成立：\n$$\nf(x_k - t_k g_k) \\le f(x_k) - c\\,t_k\\,\\lVert g_k\\rVert_2^2.\n$$\n更新迭代点 $x_{k+1}=x_k - t_k g_k$。定义步长范数 $S_k=\\lVert x_{k+1}-x_k\\rVert_2$ 和单步函数下降量 $D_k=f(x_k)-f(x_{k+1})$。\n\n使用容差 $\\tau_g>0$、$\\tau_s>0$ 和 $\\tau_f>0$ 设计字典序停止准则如下，在每次迭代 $k$ 时按所述顺序进行评估：\n- 门槛 1（梯度范数优先）：若 $G_k\\le \\tau_g$，则停止。\n- 门槛 2（步长范数优先）：若 $k\\ge 0$ 且 $S_k\\le \\tau_s$，则停止。\n- 门槛 3（函数下降量优先）：若 $k\\ge 0$ 且 $D_k\\le \\tau_f$，则停止。\n如果所有门槛都未触发，并且达到了最大迭代次数 $N_{\\max}$，则终止。\n\n您必须在带回溯的梯度下降法中为以下非凸基准测试函数实现此规则：\n\n- 维度为 $2$ 的 Himmelblau 函数，定义为\n$$\nf_H(x,y)=\\left(x^2+y-11\\right)^2+\\left(x+y^2-7\\right)^2.\n$$\n\n- 维度为 $2$ 的 Rastrigin 函数，$A=10$，定义为\n$$\nf_R(x,y)=2A+\\left(x^2 - A\\cos(2\\pi x)\\right)+\\left(y^2 - A\\cos(2\\pi y)\\right).\n$$\n\n您的程序必须使用上述的字典序停止准则和回溯线搜索。对于每个测试用例，返回一个整数代码，指示是哪个门槛停止了迭代：如果门槛 1 触发，返回 $1$；如果门槛 2 触发，返回 $2$；如果门槛 3 触发，返回 $3$；或者如果因达到 $N_{\\max}$ 而终止，则返回 $4$。\n\n在以下测试集上实现并评估该规则。每个测试用例是一个参数元组 $(f,\\ x_0,\\ \\tau_g,\\ \\tau_s,\\ \\tau_f,\\ t_0,\\ \\beta,\\ c,\\ N_{\\max})$：\n\n- 用例 A（理想情况，预期梯度门槛起主导作用）：$f=f_H$，$x_0=(0,0)$，$\\tau_g=10^{-4}$，$\\tau_s=10^{-12}$，$\\tau_f=10^{-12}$，$t_0=1$，$\\beta=0.5$，$c=10^{-4}$，$N_{\\max}=10^4$。\n\n- 用例 B（边界情况，通过极小的初始步长强制触发步长门槛）：$f=f_R$，$x_0=(3,3)$，$\\tau_g=10^{-100}$，$\\tau_s=10^{-10}$，$\\tau_f=10^{-100}$，$t_0=10^{-12}$，$\\beta=0.5$，$c=10^{-4}$，$N_{\\max}=10^3$。\n\n- 用例 C（边界情况，使用严格的梯度和步长容差的函数下降门槛）：$f=f_R$，$x_0=(0.3,-0.3)$，$\\tau_g=10^{-20}$，$\\tau_s=10^{-50}$，$\\tau_f=10^{-8}$，$t_0=0.5$，$\\beta=0.5$，$c=10^{-4}$，$N_{\\max}=10^4$。\n\n- 用例 D（边界情况，起始点为驻点）：$f=f_H$，$x_0=(3,2)$，$\\tau_g=10^{-8}$，$\\tau_s=10^{-12}$，$\\tau_f=10^{-12}$，$t_0=1$，$\\beta=0.5$，$c=10^{-4}$，$N_{\\max}=10^3$。\n\n您的程序应生成单行输出，其中包含用例 A、B、C 和 D 的停止原因代码，按此顺序，以逗号分隔的列表形式包含在方括号中，例如 $\\left[1,2,3,1\\right]$。", "solution": "用户要求设计并实现一个配备了特定字典序停止准则的梯度下降算法。本解答将形式化该算法，定义必要的数学组件，并概述实现逻辑。\n\n### 问题验证\n\n**步骤 1：提取给定条件**\n\n- **迭代方法**：带回溯线搜索的梯度下降法。\n- **更新规则**：$x_{k+1} = x_k - t_k g_k$，其中 $g_k = \\nabla f(x_k)$。\n- **回溯线搜索**：\n    - **Armijo 条件**：$f(x_k - t_k g_k) \\le f(x_k) - c\\,t_k\\,\\lVert g_k\\rVert_2^2$。\n    - **参数**：初始步长 $t_0 > 0$，收缩因子 $\\beta \\in (0,1)$，Armijo 参数 $c \\in (0,1)$。\n- **用于停止的量**：\n    - 梯度范数：$G_k = \\lVert g_k \\rVert_2$。\n    - 步长范数：$S_k = \\lVert x_{k+1} - x_k \\rVert_2$。\n    - 函数下降量：$D_k = f(x_k) - f(x_{k+1})$。\n- **字典序停止准则**：\n    - **容差**：$\\tau_g > 0$, $\\tau_s > 0$, $\\tau_f > 0$。\n    - **最大迭代次数**：$N_{\\max}$。\n    - **评估顺序**：\n        1.  **门槛 1**：若 $G_k \\le \\tau_g$，停止（返回代码 $1$）。\n        2.  **门槛 2**：若 $k \\ge 0$ 且 $S_k \\le \\tau_s$，停止（返回代码 $2$）。\n        3.  **门槛 3**：若 $k \\ge 0$ 且 $D_k \\le \\tau_f$，停止（返回代码 $3$）。\n        4.  **最大迭代次数**：若循环完成 $N_{\\max}$ 次迭代，停止（返回代码 $4$）。\n- **基准测试函数**：\n    - **Himmelblau 函数 ($f_H$)**：$f_H(x,y) = \\left(x^2+y-11\\right)^2+\\left(x+y^2-7\\right)^2$。\n    - **Rastrigin 函数 ($f_R$)**：$f_R(x,y) = 2A+\\left(x^2 - A\\cos(2\\pi x)\\right)+\\left(y^2 - A\\cos(2\\pi y)\\right)$，其中 $A=10$。\n- **测试集**：\n    - **用例 A**：$(f_H, x_0=(0,0), \\tau_g=10^{-4}, \\tau_s=10^{-12}, \\tau_f=10^{-12}, t_0=1, \\beta=0.5, c=10^{-4}, N_{\\max}=10^4)$。\n    - **用例 B**：$(f_R, x_0=(3,3), \\tau_g=10^{-100}, \\tau_s=10^{-10}, \\tau_f=10^{-100}, t_0=10^{-12}, \\beta=0.5, c=10^{-4}, N_{\\max}=10^3)$。\n    - **用例 C**：$(f_R, x_0=(0.3,-0.3), \\tau_g=10^{-20}, \\tau_s=10^{-50}, \\tau_f=10^{-8}, t_0=0.5, \\beta=0.5, c=10^{-4}, N_{\\max}=10^4)$。\n    - **用例 D**：$(f_H, x_0=(3,2), \\tau_g=10^{-8}, \\tau_s=10^{-12}, \\tau_f=10^{-12}, t_0=1, \\beta=0.5, c=10^{-4}, N_{\\max}=10^3)$。\n- **输出格式**：一个逗号分隔的整数停止原因代码列表，例如 $[1,2,3,1]$。\n\n**步骤 2：基于给定条件进行验证**\n\n- **科学基础扎实**：该问题基于数值优化中的基本和标准概念：梯度下降、一阶最优性条件、回溯线搜索（Armijo 准则）和停止准则。基准测试函数是测试非凸优化程序的标准函数。该问题在科学上是合理的。\n- **适定的**：该问题是一个计算任务，具有明确定义的算法、特定的输入和确定性的输出格式。存在一个唯一且有意义的解（停止代码序列），并且可以计算得出。\n- **客观的**：所有定义和参数都以精确的数学语言给出。没有主观或含糊的陈述。\n- **完整且一致**：提供了实现算法和运行测试用例所需的所有必要信息。参数已指定，函数已定义，停止准则的逻辑清晰明确。条件“若 $k \\ge 0$”（对于门槛 2 和 3）在标准的从零开始索引的迭代循环中是自然满足的，但正确地暗示了这些检查从第一次迭代开始就有效。\n- **未检测到其他缺陷**：该问题并非不切实际、不适定、微不足道或无法验证。这是一个结构良好的计算数学练习。\n\n**步骤 3：结论与行动**\n\n该问题有效。将提供完整的解决方案。\n\n### 基于原理的方案设计\n\n问题的核心是实现一个梯度下降算法。该算法通过沿负梯度方向采取步长，迭代地走向函数 $f: \\mathbb{R}^n \\to \\mathbb{R}$ 的局部最小值。\n\n**1. 梯度下降迭代**\n在每次迭代 $k$ 中，从点 $x_k$ 开始，通过以下方式找到下一个点 $x_{k+1}$：\n$$\nx_{k+1} = x_k - t_k g_k\n$$\n其中 $g_k = \\nabla f(x_k)$ 是 $f$ 在 $x_k$ 处的梯度，而 $t_k > 0$ 是步长。\n\n**2. 步长选择：回溯线搜索**\n步长 $t_k$ 由回溯线搜索过程确定，以确保函数值有足够的下降。从一个初始猜测 $t = t_0$ 开始，步长被反复乘以一个因子 $\\beta \\in (0,1)$，直到满足 Armijo-Goldstein 条件：\n$$\nf(x_k - t g_k) \\le f(x_k) - c t \\lVert g_k \\rVert_2^2\n$$\n其中 $c \\in (0,1)$ 是一个常数。第一个满足此不等式的 $t$ 值被选为 $t_k$。这保证了步长提供的目标函数下降量与步长和梯度范数的平方成正比。\n\n**3. 基准测试函数及其梯度**\n该算法将在两个非凸函数上进行测试。为了实现，需要它们的梯度。\n\n- **Himmelblau 函数**：$f_H(x,y) = (x^2+y-11)^2 + (x+y^2-7)^2$。\n  其梯度 $\\nabla f_H = (\\frac{\\partial f_H}{\\partial x}, \\frac{\\partial f_H}{\\partial y})$ 为：\n  $$\n  \\frac{\\partial f_H}{\\partial x} = 2(x^2+y-11)(2x) + 2(x+y^2-7)(1) = 4x(x^2+y-11) + 2(x+y^2-7)\n  $$\n  $$\n  \\frac{\\partial f_H}{\\partial y} = 2(x^2+y-11)(1) + 2(x+y^2-7)(2y) = 2(x^2+y-11) + 4y(x+y^2-7)\n  $$\n\n- **Rastrigin 函数**：$f_R(x,y) = 2A + (x^2 - A\\cos(2\\pi x)) + (y^2 - A\\cos(2\\pi y))$，其中 $A=10$。\n  其梯度 $\\nabla f_R = (\\frac{\\partial f_R}{\\partial x}, \\frac{\\partial f_R}{\\partial y})$ 为：\n  $$\n  \\frac{\\partial f_R}{\\partial x} = 2x - A(-\\sin(2\\pi x))(2\\pi) = 2x + 2\\pi A \\sin(2\\pi x)\n  $$\n  $$\n  \\frac{\\partial f_R}{\\partial y} = 2y - A(-\\sin(2\\pi y))(2\\pi) = 2y + 2\\pi A \\sin(2\\pi y)\n  $$\n\n**4. 字典序停止准则及实现逻辑**\n算法的主循环最多进行 $N_{\\max}$ 次迭代。在每次迭代 $k$ 中，按精确的字典序检查停止准则。\n\n设 $x_k$ 为当前迭代点。\n1.  计算函数值 $f_k = f(x_k)$ 和梯度 $g_k = \\nabla f(x_k)$。\n2.  计算梯度范数 $G_k = \\lVert g_k \\rVert_2$。\n3.  **检查门槛 1**：如果 $G_k \\le \\tau_g$，算法终止并返回停止代码 $1$。此检查基于一阶必要最优性条件 $\\nabla f(x^\\star) = \\mathbf{0}$。\n4.  如果门槛 1 未满足，则执行回溯线搜索以找到步长 $t_k$。\n5.  计算下一个迭代点：$x_{k+1} = x_k - t_k g_k$。\n6.  计算步长范数 $S_k = \\lVert x_{k+1} - x_k \\rVert_2 = t_k \\lVert g_k \\rVert_2$。\n7.  计算新的函数值 $f_{k+1} = f(x_{k+1})$ 和函数下降量 $D_k = f_k - f_{k+1}$。\n8.  **检查门槛 2**：如果 $S_k \\le \\tau_s$，算法终止并返回停止代码 $2$。这表明迭代点不再有显著移动，暗示着数值停滞或收敛。\n9.  **检查门槛 3**：如果 $D_k \\le \\tau_f$，算法终止并返回停止代码 $3$。这表明每次迭代中目标函数的改善可以忽略不计。\n10. 如果没有门槛被触发，则循环进入下一次迭代 $k+1$，以 $x_{k+1}$ 为新点。\n11. **检查最大迭代次数**：如果循环在没有任何门槛被触发的情况下完成了 $N_{\\max}$ 次迭代，算法终止并返回停止代码 $4$。\n\n对每个测试用例实施此操作序列，以确定相应的停止代码。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to define test cases and run the optimization algorithm.\n    \"\"\"\n\n    # Define benchmark functions and their gradients\n    def himmelblau(x):\n        \"\"\"Himmelblau's function.\"\"\"\n        return (x[0]**2 + x[1] - 11)**2 + (x[0] + x[1]**2 - 7)**2\n\n    def grad_himmelblau(x):\n        \"\"\"Gradient of Himmelblau's function.\"\"\"\n        dx = 4 * x[0] * (x[0]**2 + x[1] - 11) + 2 * (x[0] + x[1]**2 - 7)\n        dy = 2 * (x[0]**2 + x[1] - 11) + 4 * x[1] * (x[0] + x[1]**2 - 7)\n        return np.array([dx, dy])\n\n    def rastrigin(x):\n        \"\"\"Rastrigin's function.\"\"\"\n        A = 10\n        return 2 * A + (x[0]**2 - A * np.cos(2 * np.pi * x[0])) + \\\n               (x[1]**2 - A * np.cos(2 * np.pi * x[1]))\n\n    def grad_rastrigin(x):\n        \"\"\"Gradient of Rastrigin's function.\"\"\"\n        A = 10\n        dx = 2 * x[0] + 2 * np.pi * A * np.sin(2 * np.pi * x[0])\n        dy = 2 * x[1] + 2 * np.pi * A * np.sin(2 * np.pi * x[1])\n        return np.array([dx, dy])\n\n    def lexicographic_gradient_descent(f, grad_f, x0, tau_g, tau_s, tau_f, t0, beta, c, n_max):\n        \"\"\"\n        Performs gradient descent with backtracking and a lexicographic stopping rule.\n        \n        Returns an integer stop code:\n        1: Gradient norm tolerance met (Gate 1)\n        2: Step norm tolerance met (Gate 2)\n        3: Function decrease tolerance met (Gate 3)\n        4: Maximum iterations reached\n        \"\"\"\n        x_k = np.array(x0, dtype=float)\n\n        for k in range(n_max):\n            f_k = f(x_k)\n            g_k = grad_f(x_k)\n            G_k = np.linalg.norm(g_k)\n\n            # Gate 1: Gradient norm priority\n            if G_k = tau_g:\n                return 1\n\n            # Backtracking line search\n            t_k = t0\n            while f(x_k - t_k * g_k) > f_k - c * t_k * (G_k**2):\n                t_k *= beta\n                # Safety break for excessively small step sizes\n                if t_k  1e-20: \n                    t_k = 0\n                    break\n\n            x_k_plus_1 = x_k - t_k * g_k\n            \n            S_k = np.linalg.norm(x_k_plus_1 - x_k)\n            f_k_plus_1 = f(x_k_plus_1)\n            D_k = f_k - f_k_plus_1\n\n            # Gate 2: Step norm priority\n            if S_k = tau_s:\n                return 2\n\n            # Gate 3: Function decrease priority\n            if D_k = tau_f:\n                return 3\n\n            x_k = x_k_plus_1\n        \n        # Termination due to maximum iterations\n        return 4\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case A\n        {'f': himmelblau, 'grad_f': grad_himmelblau, 'x0': (0, 0), 'tau_g': 1e-4, \n         'tau_s': 1e-12, 'tau_f': 1e-12, 't0': 1, 'beta': 0.5, 'c': 1e-4, 'n_max': 10000},\n        # Case B\n        {'f': rastrigin, 'grad_f': grad_rastrigin, 'x0': (3, 3), 'tau_g': 1e-100, \n         'tau_s': 1e-10, 'tau_f': 1e-100, 't0': 1e-12, 'beta': 0.5, 'c': 1e-4, 'n_max': 1000},\n        # Case C\n        {'f': rastrigin, 'grad_f': grad_rastrigin, 'x0': (0.3, -0.3), 'tau_g': 1e-20, \n         'tau_s': 1e-50, 'tau_f': 1e-8, 't0': 0.5, 'beta': 0.5, 'c': 1e-4, 'n_max': 10000},\n        # Case D\n        {'f': himmelblau, 'grad_f': grad_himmelblau, 'x0': (3, 2), 'tau_g': 1e-8, \n         'tau_s': 1e-12, 'tau_f': 1e-12, 't0': 1, 'beta': 0.5, 'c': 1e-4, 'n_max': 1000}\n    ]\n\n    results = []\n    for case in test_cases:\n        result = lexicographic_gradient_descent(**case)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3187944"}, {"introduction": "简单地检查单步进展可能具有误导性，因为算法可能会经历暂时的停滞或振荡。为了克服这个问题，我们可以采用一种更稳健的策略，即在一段时间窗口内评估算法的进展。本练习将引导你实现一个“窗口停滞”准则，该准则同时监控函数值在过去 $m$ 次迭代中的总降幅和迭代点的移动距离。通过实现和调整这个更复杂的准则，你将学会如何设计出能有效识别真正收敛而非暂时减速的停止策略 [@problem_id:3187906]。", "problem": "考虑通过固定步长的梯度下降（GD）方法来最小化一个可微目标函数 $f:\\mathbb{R}^n \\to \\mathbb{R}$，该方法由迭代格式 $x_{k+1} = x_k - \\alpha \\nabla f(x_k)$ 定义，其中 $\\alpha  0$ 是一个选定的步长，$x_k \\in \\mathbb{R}^n$ 是第 $k \\in \\mathbb{N}$ 次迭代的迭代点。停止准则必须根据有原则的容差条件来决定迭代过程何时终止。在本问题中，您将实现并评估一个带有两个可调参数的窗口化停滞准则：函数下降容差 $\\tau  0$ 和窗口长度 $m \\in \\mathbb{N}$。对于一个固定的 $\\varepsilon  0$，该准则在第一个满足以下两个条件的索引 $k \\ge m$ 处终止迭代：\n$$\nf(x_{k-m}) - f(x_k)  \\tau\n\\quad\\text{and}\\quad\n\\|x_k - x_{k-m}\\|_2 \\le \\varepsilon.\n$$\n如果在给定的最大迭代次数 $K$ 内不存在这样的 $k$，则该准则不被触发，我们设置 $k_{\\mathrm{stop}} = K$。\n\n您的任务是实现带有上述停止准则的 GD，并对每个测试用例，通过在给定的候选有限集中搜索来调整 $(\\tau, m)$。对于每个候选对 $(\\tau, m)$，从指定的初始点开始运行 GD，直到停止准则触发或达到最大迭代次数 $K$，并记录 $k_{\\mathrm{stop}}$ 和相应的迭代点 $x_{k_{\\mathrm{stop}}}$。仅当在停止时满足质量要求，候选 $(\\tau, m)$ 才被认为是可接受的。在可接受的候选中，选择 $k_{\\mathrm{stop}}$ 最小的对。如果 $k_{\\mathrm{stop}}$ 出现平局，选择 $m$ 较大的那个；如果仍然平局，选择 $\\tau$ 较小的那个。如果没有可接受的候选，则选择使下述质量要求违规度最小化的对，并使用相同的平局决胜规则。\n\n质量要求和违规度度量。对于每个测试用例，都指定了一个基于范数的解邻近度阈值 $\\rho  0$。质量要求为\n$$\n\\|x_{k_{\\mathrm{stop}}}\\|_2 \\le \\rho.\n$$\n如果不满足此要求，则定义违规度度量 $v = \\max\\{0, \\|x_{k_{\\mathrm{stop}}}\\|_2 - \\rho\\}$，并选择使 $v$ 最小化的候选（使用上述相同的平局决胜规则）。\n\n为以下包含三个用例的测试套件实现上述过程。在所有用例中，使用欧几里得范数 $\\|\\cdot\\|_2$，且不涉及物理单位。\n\n测试用例 1（理想情况，各向异性二次型）：\n- 目标函数：$f(x) = \\tfrac{1}{2}(x_1^2 + 10 x_2^2)$ 在 $\\mathbb{R}^2$ 上。\n- 梯度：$\\nabla f(x) = (x_1, 10 x_2)$。\n- 初始点：$x_0 = (5, -5)$。\n- 步长：$\\alpha = 0.15$。\n- 最大迭代次数：$K = 5000$。\n- 停滞移动容差：$\\varepsilon = 10^{-6}$。\n- 候选集：$\\{\\tau\\} = \\{10^{-7}, 10^{-6}, 10^{-5}\\}$ 和 $\\{m\\} = \\{3, 5, 10\\}$。\n- 质量阈值：$\\rho = 10^{-3}$。\n\n测试用例 2（边界条件，小步长各向同性二次型）：\n- 目标函数：$f(x) = \\tfrac{1}{2}(x_1^2 + x_2^2)$ 在 $\\mathbb{R}^2$ 上。\n- 梯度：$\\nabla f(x) = (x_1, x_2)$。\n- 初始点：$x_0 = (2, 2)$。\n- 步长：$\\alpha = 10^{-3}$。\n- 最大迭代次数：$K = 20000$。\n- 停滞移动容差：$\\varepsilon = 10^{-7}$。\n- 候选集：$\\{\\tau\\} = \\{10^{-7}, 10^{-6}, 10^{-5}\\}$ 和 $\\{m\\} = \\{3, 10, 50\\}$。\n- 质量阈值：$\\rho = 5 \\times 10^{-2}$。\n\n测试用例 3（边缘情况，平坦平台几何）：\n- 目标函数：$f(x) = \\arctan(\\|x\\|_2)$ 在 $\\mathbb{R}^2$ 上。\n- 梯度：对于 $x \\neq 0$，$\\nabla f(x) = \\dfrac{1}{1+\\|x\\|_2^2} \\dfrac{x}{\\|x\\|_2}$；对于 $x = 0$，$\\nabla f(0) = (0, 0)$。\n- 初始点：$x_0 = (100, 0)$。\n- 步长：$\\alpha = 10^{-1}$。\n- 最大迭代次数：$K = 200000$。\n- 停滞移动容差：$\\varepsilon = 10^{-4}$。\n- 候选集：$\\{\\tau\\} = \\{10^{-8}, 10^{-7}, 10^{-6}\\}$ 和 $\\{m\\} = \\{20, 50\\}$。\n- 质量阈值：$\\rho = 10^{-1}$。\n\n您的程序应生成单行输出，其中包含一个用逗号分隔并用方括号括起来的结果列表。每个结果必须是对应测试用例所选的对，形式为一个二元列表 $[\\tau, m]$，并按上述用例的顺序排列。例如，输出格式必须为 $[[\\tau_1,m_1],[\\tau_2,m_2],[\\tau_3,m_3]]$ 的形式，不含空格。", "solution": "我们从梯度下降（GD）的核心定义开始。给定一个可微目标函数 $f:\\mathbb{R}^n \\to \\mathbb{R}$ 和一个固定的步长 $\\alpha  0$，GD 通过 $x_{k+1} = x_k - \\alpha \\nabla f(x_k)$ 生成迭代点。在温和的正则性条件下且 $\\alpha$ 足够小时，序列 $(f(x_k))_{k \\in \\mathbb{N}}$ 是不增的，并且 $(x_k)$ 会趋近于一个一阶平稳点，在凸设定中这通常是一个最小化点。\n\n一个有原则的停止准则应能检测到停滞的开始。像 $\\|\\nabla f(x_k)\\| \\le \\text{容差}$ 或 $\\|x_{k+1}-x_k\\| \\le \\text{容差}$ 这样的单次迭代检查可能对瞬态噪声或方向变化敏感。窗口化准则通过聚合多次迭代的信息来缓解此问题。定义一个窗口长度 $m \\in \\mathbb{N}$ 以及函数下降容差 $\\tau  0$ 和决策空间移动容差 $\\varepsilon  0$。窗口化停滞准则在第一个满足以下条件的 $k \\ge m$ 处宣告停止：\n$$\nf(x_{k-m}) - f(x_k)  \\tau,\\quad \\|x_k - x_{k-m}\\|_2 \\le \\varepsilon.\n$$\n这将两个信号耦合在一起：窗口内目标函数的垂直下降和参数空间中的水平移动。如果两者都很小，说明算法在最近的 $m$ 次迭代中进展甚微，这表明要么接近了平稳点，要么步长太小无法产生有意义的进展。在这两种情况下，如果附带质量检查，则停止是合理的。\n\n质量要求。对于每个测试用例，我们施加一个解邻近度度量 $\\|x_{k_{\\mathrm{stop}}}\\|_2 \\le \\rho$，其中 $\\rho  0$ 是选定的。对于测试用例 1 和 2 中的二次型目标函数，唯一的最小化点在 $x^\\star = 0$ 处，因此限制 $\\|x_{k_{\\mathrm{stop}}}\\|_2$ 直接度量了与最小化点的接近程度。对于测试用例 3 中的 $\\arctan$ 目标函数，最小化点同样是 $x^\\star = 0$，因为 $\\arctan(\\|x\\|_2)$ 在 $\\|x\\|_2 \\ge 0$ 上是严格递增的，所以同样的准则适用。\n\n调整 $(\\tau, m)$。参数对 $(\\tau, m)$ 控制着准则的灵敏度：\n- 较大的 $m$ 在更长的时间范围内跟踪进展，降低了由瞬态行为引起的过早停止的风险。\n- 较小的 $\\tau$ 要求在窗口内有更显著的函数值下降才能继续，这会延迟停止，直到算法真正进入平坦区。\n- 移动容差 $\\varepsilon$ 确保了即使函数几乎是平的，除非迭代点本身已经稳定，否则我们不会停止。\n\n为调整 $(\\tau, m)$，我们评估所有候选，并选择在满足质量要求 $\\|x_{k_{\\mathrm{stop}}}\\|_2 \\le \\rho$ 的前提下，使停止迭代索引 $k_{\\mathrm{stop}}$ 最小的对。这是一个合理的优化目标：如果能保证足够的质量，则更早停止是更可取的。在没有候选满足质量要求的情况下，我们选择使违规度 $v = \\max\\{0, \\|x_{k_{\\mathrm{stop}}}\\|_2 - \\rho\\}$ 最小的对，这个值量化了结果与期望容差的差距。平局通过偏好更大的窗口 $m$（更鲁棒的聚合）和然后更小的 $\\tau$（更严格的下降要求）来打破，这与保守的停止策略设计相一致。\n\n算法设计。对于每个测试用例：\n1. 初始化 $x_0$ 并计算 $f(x_0)$。\n2. 对于每个候选对 $(\\tau, m)$：\n   a. 使用 $x_{k+1} = x_k - \\alpha \\nabla f(x_k)$ 进行最多 $K$ 步的 GD 迭代。\n   b. 维护历史记录 $(x_j)$ 和 $(f(x_j))$。对于每个 $k \\ge m$，计算\n      $$\n      \\Delta f_k = f(x_{k-m}) - f(x_k),\\quad d_k = \\|x_k - x_{k-m}\\|_2.\n      $$\n      如果 $\\Delta f_k  \\tau$ 且 $d_k \\le \\varepsilon$，则声明 $k_{\\mathrm{stop}} = k$ 并停止循环。\n   c. 如果循环完成而未触发停止，则设置 $k_{\\mathrm{stop}} = K$。\n   d. 评估质量 $\\|x_{k_{\\mathrm{stop}}}\\|_2 \\le \\rho$ 并记录 $k_{\\mathrm{stop}}$ 和必要的违规度 $v$。\n3. 在可接受的候选中选择使 $k_{\\mathrm{stop}}$ 最小的对；否则，选择使违规度 $v$ 最小的对，并采用指定的平局决胜规则。\n\n从基本原理证明窗口化准则的合理性。在梯度满足利普希茨连续条件（参数为 $L  0$，即 $\\|\\nabla f(x) - \\nabla f(y)\\|_2 \\le L \\|x-y\\|_2$）的 GD 中，对于 $\\alpha \\in (0, 2/L)$，存在经典的下降性质\n$$\nf(x_{k+1}) \\le f(x_k) - \\tfrac{1}{2}\\alpha (2 - \\alpha L) \\|\\nabla f(x_k)\\|_2^2,\n$$\n这确保了 $f(x_k)$ 的单调递减。当一个窗口内的下降量小于 $\\tau$ 时，该窗口内每次迭代的 $f$ 平均变化量被 $\\tau/m$ 所界定，表明梯度范数平均而言很小。同时，如果 $\\|x_k - x_{k-m}\\|_2 \\le \\varepsilon$，则迭代点构成一个近似柯西段，意味着在决策空间中趋于稳定。总而言之，这些反映了接近平稳性：在梯度小且移动小的情况下，GD 的进一步进展将是微不足道的。设置容差 $(\\tau, \\varepsilon)$ 和时间范围 $m$ 是在校准停止前可以容忍多少边际进展，而调整过程则选择能尽早达到目标质量的值。\n\n三个测试用例涵盖了：\n- 一个良态的各向异性二次型，其中 GD 收敛迅速，代表理想情况。\n- 一个小步长的各向同性二次型，说明当每次迭代进展缓慢时，对 $\\tau$ 和 $m$ 选择的敏感性。\n- 一个平坦平台目标函数 $\\arctan(\\|x\\|_2)$，在远离最小化点处梯度极小，暴露了这样一种边缘情况：如果不对容差进行调整以避免假阳性，朴素的容差可能导致过早停止；基于违规度的后备方案确保了即使没有候选达到目标质量，也能做出有原则的选择。\n\n程序枚举了候选的 $(\\tau, m)$ 对，使用窗口化停滞准则运行 GD，应用选择标准，并以指定的单行格式输出所选的对。", "answer": "```python\nimport numpy as np\n\ndef gd_run(f, grad, x0, alpha, K, eps_move, tau, m):\n    \"\"\"\n    Run Gradient Descent with a windowed stagnation stopping rule.\n    Stop at first k >= m such that:\n        f(x_{k-m}) - f(x_k)  tau and ||x_k - x_{k-m}|| = eps_move\n    If not triggered by K, set k_stop = K.\n    Returns (k_stop, x_stop_norm).\n    \"\"\"\n    x_hist = [np.array(x0, dtype=float)]\n    f_hist = [f(x_hist[0])]\n    x = x_hist[0]\n    k_stop = None\n\n    for k in range(1, K + 1):\n        g = grad(x)\n        x = x - alpha * g\n        fx = f(x)\n        x_hist.append(x)\n        f_hist.append(fx)\n\n        if k >= m:\n            df = f_hist[k - m] - fx\n            move = np.linalg.norm(x - x_hist[k - m])\n            if df  tau and move = eps_move:\n                k_stop = k\n                break\n\n    if k_stop is None:\n        k_stop = K\n        x_stop = x_hist[-1]\n    else:\n        x_stop = x_hist[k_stop]\n\n    return k_stop, np.linalg.norm(x_stop)\n\n\ndef select_tau_m(f, grad, x0, alpha, K, eps_move, tau_list, m_list, rho):\n    \"\"\"\n    Enumerate candidates (tau, m), run GD, and select:\n      - among acceptable candidates (||x_stop|| = rho), the one with minimal k_stop;\n      - ties broken by larger m, then smaller tau;\n      - if none acceptable, minimize violation v = max(0, ||x_stop|| - rho) with same tie-breakers.\n    Returns the selected [tau, m].\n    \"\"\"\n    candidates = []\n    for tau in tau_list:\n        for m in m_list:\n            k_stop, xnorm = gd_run(f, grad, x0, alpha, K, eps_move, tau, m)\n            acceptable = xnorm = rho\n            violation = max(0.0, xnorm - rho)\n            candidates.append({\n                \"tau\": tau, \"m\": m, \"k\": k_stop,\n                \"acceptable\": acceptable, \"violation\": violation\n            })\n\n    # Filter acceptable candidates\n    acceptable_cands = [c for c in candidates if c[\"acceptable\"]]\n    if acceptable_cands:\n        # Sort by k ascending, m descending, tau ascending\n        acceptable_cands.sort(key=lambda c: (c[\"k\"], -c[\"m\"], c[\"tau\"]))\n        best = acceptable_cands[0]\n    else:\n        # Sort by violation ascending, k ascending, m descending, tau ascending\n        candidates.sort(key=lambda c: (c[\"violation\"], c[\"k\"], -c[\"m\"], c[\"tau\"]))\n        best = candidates[0]\n\n    return [best[\"tau\"], best[\"m\"]]\n\n\ndef solve():\n    # Define test cases\n    # Test Case 1\n    def f1(x):\n        return 0.5 * (x[0]**2 + 10.0 * x[1]**2)\n    def g1(x):\n        return np.array([x[0], 10.0 * x[1]])\n    x0_1 = np.array([5.0, -5.0])\n    alpha_1 = 0.15\n    K_1 = 5000\n    eps_1 = 1e-6\n    tau_list_1 = [1e-7, 1e-6, 1e-5]\n    m_list_1 = [3, 5, 10]\n    rho_1 = 1e-3\n\n    # Test Case 2\n    def f2(x):\n        return 0.5 * (x[0]**2 + x[1]**2)\n    def g2(x):\n        return np.array([x[0], x[1]])\n    x0_2 = np.array([2.0, 2.0])\n    alpha_2 = 1e-3\n    K_2 = 20000\n    eps_2 = 1e-7\n    tau_list_2 = [1e-7, 1e-6, 1e-5]\n    m_list_2 = [3, 10, 50]\n    rho_2 = 5e-2\n\n    # Test Case 3\n    def f3(x):\n        r = np.linalg.norm(x)\n        return np.arctan(r)\n    def g3(x):\n        r = np.linalg.norm(x)\n        if r == 0.0:\n            return np.array([0.0, 0.0])\n        return (1.0 / (1.0 + r*r)) * (x / r)\n    x0_3 = np.array([100.0, 0.0])\n    alpha_3 = 1e-1\n    K_3 = 200000\n    eps_3 = 1e-4\n    tau_list_3 = [1e-8, 1e-7, 1e-6]\n    m_list_3 = [20, 50]\n    rho_3 = 1e-1\n\n    results = []\n    # Case 1\n    sel1 = select_tau_m(f1, g1, x0_1, alpha_1, K_1, eps_1, tau_list_1, m_list_1, rho_1)\n    results.append(sel1)\n    # Case 2\n    sel2 = select_tau_m(f2, g2, x0_2, alpha_2, K_2, eps_2, tau_list_2, m_list_2, rho_2)\n    results.append(sel2)\n    # Case 3\n    sel3 = select_tau_m(f3, g3, x0_3, alpha_3, K_3, eps_3, tau_list_3, m_list_3, rho_3)\n    results.append(sel3)\n\n    # Format output without spaces\n    def format_pair(p):\n        return \"[\" + f\"{p[0]},{p[1]}\" + \"]\"\n    out = \"[\" + \",\".join(format_pair(p) for p in results) + \"]\"\n    print(out)\n\nsolve()\n```", "id": "3187906"}, {"introduction": "梯度范数趋近于零是算法收敛到平稳点的核心标志，但它是否总是意味着我们找到了一个局部最优解？在非凸优化的复杂世界里，答案并非总是肯定的。本练习将揭示一个重要的警示：当与非单调线搜索等高级策略结合时，仅依赖梯度范数的停止准则可能会被“欺骗”。你将构建一个场景，其中算法利用非单调性“跳”到一个梯度很小但远离真正最优解的平坦区域，从而导致过早终止。这个实践突出了理解算法行为与停止准则之间相互作用的极端重要性 [@problem_id:3187963]。", "problem": "考虑一个针对连续可微目标函数 $f:\\mathbb{R}^n \\to \\mathbb{R}$ 的无约束最小化问题。最速下降迭代定义为 $\\mathbf{x}_{k+1} = \\mathbf{x}_k + \\alpha_k \\mathbf{d}_k$，其中 $\\mathbf{d}_k = -\\nabla f(\\mathbf{x}_k)$，步长 $\\alpha_k$ 通过强制执行充分下降条件的回溯线搜索来选择。在一个窗口大小为 $M \\in \\mathbb{N}$ 的非单调 Armijo 回溯线搜索中，第 $k$ 次迭代的接受条件是\n$$\nf(\\mathbf{x}_k + \\alpha_k \\mathbf{d}_k) \\le C_k + c\\,\\alpha_k\\,\\nabla f(\\mathbf{x}_k)^\\top \\mathbf{d}_k,\n$$\n其中 $c \\in (0,1)$ 是一个固定常数，而 $C_k = \\max\\{f(\\mathbf{x}_j) \\mid j \\in \\{k-\\min(M-1,k), \\ldots, k\\}\\}$ 是包括当前值在内的最近 $M$ 个观测函数值的最大值。当 $M=1$ 时，此条件简化为经典的单调 Armijo 准则。停止准则是基于梯度范数容差：如果 $\\|\\nabla f(\\mathbf{x}_k)\\|_2 \\le \\tau_g$，则终止，其中 $\\tau_g  0$ 是给定的。\n\n您的任务是实现一个完整、可运行的程序，该程序：\n- 使用最速下降方向 $\\mathbf{d}_k = -\\nabla f(\\mathbf{x}_k)$。\n- 使用如上所述的窗口大小为 $M$ 的非单调 Armijo 回溯法，参数为 $c \\in (0,1)$、初始试探步长 $\\alpha_0  0$ 和收缩因子 $\\rho \\in (0,1)$。为了突出非单调性，当 $M1$ 时，在回溯前允许步长以因子 $\\gamma1$ 进行有控制的扩展，直至上限 $\\alpha_{\\max}0$：当充分下降条件成立时，通过乘以 $\\gamma$ 来增加 $\\alpha$，直到条件不成立或达到 $\\alpha_{\\max}$；然后通过乘以 $\\rho$ 进行回溯，直到充分下降条件成立，并接受该步长。当 $M=1$ 时，不使用扩展（即，设置 $\\gamma=1$）。\n- 当满足梯度范数容差 $\\|\\nabla f(\\mathbf{x}_k)\\|_2 \\le \\tau_g$、达到最大迭代次数 $k_{\\max}$，或者线搜索在数值安全范围内未能找到正步长时，程序停止。\n\n对于每个测试用例，报告：\n- 执行的迭代次数（整数）。\n- 最终梯度范数（浮点数）。\n- 最终目标值（浮点数）。\n- 一个布尔值，指示算法是否因满足梯度范数容差而停止。\n- 一个布尔值，指示最终点是否接近最小化点（每个案例中具体定义）。\n- 一个布尔值，指示一种失败模式，定义为因梯度范数容差而停止，但并未接近最小化点。\n\n从以下基本原理开始：\n- 最速下降原理使用 $\\mathbf{d}_k = -\\nabla f(\\mathbf{x}_k)$，这是欧几里得范数下瞬时下降最快的方向。\n- Armijo 型充分下降强制要求 $f(\\mathbf{x}_k + \\alpha \\mathbf{d}_k) \\le f(\\mathbf{x}_k) + c\\,\\alpha\\,\\nabla f(\\mathbf{x}_k)^\\top \\mathbf{d}_k$，其中 $c \\in (0,1)$；非单调变体将 $f(\\mathbf{x}_k)$ 替换为 $C_k$，允许 $f$ 相对于当前值暂时增加。\n- 梯度范数容差衡量一阶平稳性，小的 $\\|\\nabla f\\|_2$ 表示接近平稳点，但在非凸设置中不一定接近最小化点。\n\n实现程序以运行以下测试套件。每个测试用例指定了目标函数 $f$、其梯度 $\\nabla f$、起始点 $\\mathbf{x}_0$、线搜索参数以及最小化点邻近性测试。对于所有向量输入，将 $\\mathbb{R}^n$ 中的 $\\mathbf{x}$ 表示为数组。\n\n测试套件：\n1. 凸二次函数，单调搜索（理想路径）：\n   - $\\mathbb{R}^2$ 中的 $f(\\mathbf{x}) = \\tfrac{1}{2}\\,\\mathbf{x}^\\top Q\\,\\mathbf{x}$，其中 $Q = \\mathrm{diag}(1,10)$。\n   - $\\nabla f(\\mathbf{x}) = Q\\,\\mathbf{x}$。\n   - 从 $\\mathbf{x}_0 = (3,-4)$ 开始。\n   - 窗口大小 $M = 1$。\n   - 参数: $c = 10^{-4}$, $\\alpha_0 = 1$, $\\rho = 0.5$, $\\gamma = 1$, $\\alpha_{\\max} = 1$。\n   - 容差和限制: $\\tau_g = 10^{-6}$, $k_{\\max} = 10000$。\n   - 接近最小化点测试：如果 $\\|\\mathbf{x}\\|_2 \\le \\delta$，其中 $\\delta = 10^{-6}$，则报告 true。\n\n2. 凸二次函数，非单调搜索（理想路径，仍应成功）：\n   - 与案例1相同的 $f$、$\\nabla f$ 和 $\\mathbf{x}_0$。\n   - 窗口大小 $M = 5$。\n   - 参数: $c = 10^{-4}$, $\\alpha_0 = 1$, $\\rho = 0.5$, $\\gamma = 2$, $\\alpha_{\\max} = 10$。\n   - 容差和限制: $\\tau_g = 10^{-6}$, $k_{\\max} = 10000$。\n   - 接近最小化点测试：$\\|\\mathbf{x}\\|_2 \\le \\delta$，其中 $\\delta = 10^{-6}$。\n\n3. $\\mathbb{R}$ 中的非凸双阱函数，非单调搜索（由非单调性和扩展引起的失败模式）：\n   - $f(x) = (x^2 - 1)^4$。\n   - $\\nabla f(x) = 8x(x^2 - 1)^3$。\n   - 从 $x_0 = -1.5$ 开始。\n   - 窗口大小 $M = 5$。\n   - 参数: $c = 10^{-4}$, $\\alpha_0 = 0.5$, $\\rho = 0.5$, $\\gamma = 2$, $\\alpha_{\\max} = 50$。\n   - 容差和限制: $\\tau_g = 10^{-1}$, $k_{\\max} = 50$。\n   - 接近最小化点测试：如果 $\\min\\{|x-1|,|x+1|\\} \\le \\delta$，其中 $\\delta = 10^{-1}$，则报告 true。\n   - 此案例旨在允许一个可接受的步长，该步长将 $f$（相对于当前值）增加到一个平坦区域，即 $x \\approx 0$ 附近，该区域的梯度范数很小，从而在远离真实最小化点时触发基于梯度的停止条件。\n\n4. $\\mathbb{R}$ 中的非凸双阱函数，单调搜索（对比案例）：\n   - 与案例3相同的 $f$、$\\nabla f$ 和 $x_0$。\n   - 窗口大小 $M = 1$。\n   - 参数: $c = 10^{-4}$, $\\alpha_0 = 0.5$, $\\rho = 0.5$, $\\gamma = 1$, $\\alpha_{\\max} = 0.5$。\n   - 容差和限制: $\\tau_g = 10^{-1}$, $k_{\\max} = 50$。\n   - 接近最小化点测试：$\\min\\{|x-1|,|x+1|\\} \\le \\delta$，其中 $\\delta = 10^{-1}$。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，列表中的每个元素是按指定顺序排列的六个值的列表，代表一个测试用例的结果：$[\\text{iterations}, \\text{final\\_grad\\_norm}, \\text{final\\_f\\_value}, \\text{stopped\\_on\\_grad\\_tol}, \\text{near\\_minimizer}, \\text{failure}]$。例如，包含两个案例的输出将如下所示：$[[i_1,g_1,f_1,b_1,n_1,h_1],[i_2,g_2,f_2,b_2,n_2,h_2]]$。", "solution": "该问题被评估为**有效**。它提出了一个在数值优化领域中定义明确的任务，并以成熟的理论为基础。该问题要求实现一个包含非单调线搜索策略的最速下降算法，并在一组指定的测试用例上进行评估。所有参数、函数和算法逻辑都以足够的精度提供，以实现唯一且可验证的实现。测试套件旨在展示算法的关键行为，包括一种场景，其中标准的基于梯度的停止准则可能导致在远离真实最小化点时过早终止，这是优化中一个概念上很重要的主题。\n\n解决方案涉及实现一个最速下降优化例程 $\\mathbf{x}_{k+1} = \\mathbf{x}_k + \\alpha_k \\mathbf{d}_k$，其中搜索方向为最速下降方向 $\\mathbf{d}_k = -\\nabla f(\\mathbf{x}_k)$。关键部分是通过非单调回溯线搜索确定步长 $\\alpha_k$。\n\n线搜索必须满足非单调充分下降条件，也称为非单调 Armijo 条件：\n$$\nf(\\mathbf{x}_k + \\alpha_k \\mathbf{d}_k) \\le C_k + c\\,\\alpha_k\\,\\nabla f(\\mathbf{x}_k)^\\top \\mathbf{d}_k\n$$\n其中 $c \\in (0,1)$ 是一个常数，而 $C_k$ 是在最近一个迭代窗口内观察到的最大函数值：\n$$\nC_k = \\max\\{f(\\mathbf{x}_j) \\mid j \\in \\{k-\\min(M-1,k), \\ldots, k\\}\\}\n$$\n参数 $M \\in \\mathbb{N}$ 是窗口大小。当 $M=1$ 时，$C_k = f(\\mathbf{x}_k)$，该条件简化为标准的单调 Armijo 准则。非单调策略（$M  1$）允许目标函数值相对于当前值 $f(\\mathbf{x}_k)$ 暂时增加，只要它相对于历史窗口中的最大值 $C_k$ 是下降的。这可以帮助算法逃离狭窄的山谷，并更有效地在复杂的非凸地形中导航。\n\n根据问题陈述，特定的线搜索过程在非单调情况（$M  1$）下遵循一个两阶段序列：\n1.  **扩展阶段**：从试探步长 $\\alpha = \\alpha_0$ 开始，只要非单调 Armijo 条件成立，步长就重复乘以一个扩展因子 $\\gamma  1$。这个过程持续到条件不成立或步长超过最大值 $\\alpha_{\\max}$。该阶段终止时的步长成为后续回溯阶段的起始点。\n2.  **回溯阶段**：从扩展阶段确定的步长（如果 $M=1$，则直接为 $\\alpha_0$）开始，步长重复乘以一个收缩因子 $\\rho \\in (0,1)$，直到满足非单调 Armijo 条件。\n\n整个算法在以下三个条件之一满足时终止：\n1.  梯度的欧几里得范数低于指定容差：$\\|\\nabla f(\\mathbf{x}_k)\\|_2 \\le \\tau_g$。这表明迭代点接近一个平稳点。\n2.  达到最大迭代次数 $k_{\\max}$。\n3.  线搜索未能找到有效的正步长，表明存在数值问题或算法崩溃。\n\n实现将包括一个管理迭代过程的主函数 `steepest_descent` 和一个封装了寻找 $\\alpha_k$ 逻辑的辅助函数 `line_search`。测试用例旨在探究算法的行为：\n-   **案例1和2** 将单调（$M=1$）和非单调（$M=5$）方法应用于一个简单的凸二次函数。两者都有望成功收敛到原点的最小化点。\n-   **案例3** 在一个非凸函数上使用非单调策略。它专门设计用于展示非单调方法与梯度范数停止准则相结合时的一个潜在陷阱。非单调准则通过引用过去较高的 $f$ 值，可以接受一个大步长，将迭代点移动到一个 $f$ 值比当前值高但梯度范数很小（例如，在 $x=0$ 处的局部最大值附近）的区域。这可能导致算法因梯度小而终止，尽管它远离位于 $x=\\pm 1$ 的真实最小化点。\n-   **案例4** 作为对照，将单调搜索应用于同一个非凸问题。更严格的下降要求预计会阻止跳跃到 $x=0$ 附近的平坦区域，并引导迭代点朝向一个局部最小化点。\n\n程序将执行这些测试用例并报告性能指标，包括迭代次数、最终梯度范数、最终目标值，以及指示终止原因和是否接近真实最小化点的标志。", "answer": "```python\nimport numpy as np\n\ndef line_search(f, xk, dk, grad_fk_T_dk, Ck, M, c, alpha0, rho, gamma, alpha_max):\n    \"\"\"\n    Performs the nonmonotone Armijo line search with a sequential expansion-then-backtrack\n    logic as described in the problem statement.\n    \n    Args:\n        f: The objective function.\n        xk: The current iterate.\n        dk: The current search direction.\n        grad_fk_T_dk: The precomputed dot product of the gradient and direction.\n        Ck: The reference value for the nonmonotone Armijo condition.\n        M: The window size for nonmonotonicity.\n        c: The Armijo condition constant.\n        alpha0: The initial trial step length.\n        rho: The backtracking contraction factor.\n        gamma: The expansion factor.\n        alpha_max: The maximum allowable step length.\n\n    Returns:\n        The accepted step length, or 0.0 on failure.\n    \"\"\"\n    \n    alpha_bt_start = alpha0\n    \n    # Phase 1: Expansion (only if M > 1, as per problem spec)\n    # This phase determines the starting alpha for the backtracking phase.\n    if M > 1:\n        alpha_exp = alpha0\n        # The loop continues as long as the Armijo condition holds.\n        # It sets `alpha_bt_start` to the first value that FAILS the condition\n        # or to alpha_max if the boundary is reached.\n        while f(xk + alpha_exp * dk) = Ck + c * alpha_exp * grad_fk_T_dk:\n            next_alpha = alpha_exp * gamma\n            if next_alpha > alpha_max:\n                alpha_bt_start = alpha_max\n                break\n            alpha_exp = next_alpha\n        else:  # Loop completed without break, meaning alpha_exp failed\n            alpha_bt_start = alpha_exp\n            \n    # Phase 2: Backtracking\n    alpha = alpha_bt_start\n    # This loop runs until the Armijo condition is satisfied.\n    num_bt_steps = 0\n    max_bt_steps = 100 # Safeguard against an infinite loop\n    while f(xk + alpha * dk) > Ck + c * alpha * grad_fk_T_dk:\n        alpha *= rho\n        num_bt_steps += 1\n        if num_bt_steps > max_bt_steps:\n            return 0.0  # Line search failure\n\n    return alpha\n\n\ndef steepest_descent(f, grad_f, x0, M, c, alpha0, rho, gamma, alpha_max, tau_g, k_max, near_minimizer_test):\n    \"\"\"\n    Implements the steepest descent algorithm with nonmonotone line search.\n    \"\"\"\n    x = np.array(x0, dtype=float)\n    f_history = [f(x)]\n    \n    k = 0\n    stopped_on_grad_tol = False\n\n    while k  k_max:\n        grad_fx = grad_f(x)\n        grad_norm = np.linalg.norm(grad_fx)\n\n        if grad_norm = tau_g:\n            stopped_on_grad_tol = True\n            break\n        \n        dk = -grad_fx\n        \n        # Determine the window for C_k. f_history has k+1 elements.\n        window_start_idx = max(0, len(f_history) - M)\n        Ck = max(f_history[window_start_idx:])\n        \n        grad_fk_T_dk = np.dot(grad_fx, dk)\n        \n        alpha_k = line_search(f, x, dk, grad_fk_T_dk, Ck, M, c, alpha0, rho, gamma, alpha_max)\n        \n        if alpha_k = 0.0: # Line search failure\n            # If line search fails, we cannot proceed. Terminate.\n            break\n            \n        x = x + alpha_k * dk\n        f_history.append(f(x))\n        k += 1\n\n    final_grad_norm = np.linalg.norm(grad_f(x))\n    final_f_val = f(x)\n    near_minimizer = near_minimizer_test(x)\n    failure = stopped_on_grad_tol and not near_minimizer\n    \n    return [k, final_grad_norm, final_f_val, stopped_on_grad_tol, near_minimizer, failure]\n\n\ndef solve():\n    \"\"\"\n    Sets up and runs the test suite for the steepest descent algorithm.\n    \"\"\"\n\n    # Test Case 1: Convex quadratic, monotone\n    Q1 = np.diag([1.0, 10.0])\n    f1 = lambda x: 0.5 * x.T @ Q1 @ x\n    grad_f1 = lambda x: Q1 @ x\n    near_min_test1 = lambda x: np.linalg.norm(x) = 1e-6\n\n    # Test Case 3: Nonconvex double-well\n    f3 = lambda x: (x[0]**2 - 1)**4\n    grad_f3 = lambda x: np.array([8 * x[0] * (x[0]**2 - 1)**3])\n    near_min_test3 = lambda x: min(abs(x[0] - 1), abs(x[0] + 1)) = 1e-1\n\n    test_cases = [\n        # Case 1\n        {\n            \"f\": f1, \"grad_f\": grad_f1, \"x0\": [3.0, -4.0], \"M\": 1,\n            \"c\": 1e-4, \"alpha0\": 1.0, \"rho\": 0.5, \"gamma\": 1.0, \"alpha_max\": 1.0,\n            \"tau_g\": 1e-6, \"k_max\": 10000, \"near_minimizer_test\": near_min_test1\n        },\n        # Case 2\n        {\n            \"f\": f1, \"grad_f\": grad_f1, \"x0\": [3.0, -4.0], \"M\": 5,\n            \"c\": 1e-4, \"alpha0\": 1.0, \"rho\": 0.5, \"gamma\": 2.0, \"alpha_max\": 10.0,\n            \"tau_g\": 1e-6, \"k_max\": 10000, \"near_minimizer_test\": near_min_test1\n        },\n        # Case 3\n        {\n            \"f\": f3, \"grad_f\": grad_f3, \"x0\": [-1.5], \"M\": 5,\n            \"c\": 1e-4, \"alpha0\": 0.5, \"rho\": 0.5, \"gamma\": 2.0, \"alpha_max\": 50.0,\n            \"tau_g\": 1e-1, \"k_max\": 50, \"near_minimizer_test\": near_min_test3\n        },\n        # Case 4\n        {\n            \"f\": f3, \"grad_f\": grad_f3, \"x0\": [-1.5], \"M\": 1,\n            \"c\": 1e-4, \"alpha0\": 0.5, \"rho\": 0.5, \"gamma\": 1.0, \"alpha_max\": 0.5,\n            \"tau_g\": 1e-1, \"k_max\": 50, \"near_minimizer_test\": near_min_test3\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        result = steepest_descent(**case)\n        results.append(result)\n\n    # Format output as a list of lists string\n    result_strings = []\n    for r in results:\n        # Convert each item in the sublist to its string representation\n        # Python's str(True) is 'True', which is acceptable.\n        str_r = [str(item) for item in r]\n        result_strings.append(f\"[{','.join(str_r)}]\")\n        \n    print(f\"[{','.join(result_strings)}]\")\n\nsolve()\n```", "id": "3187963"}]}