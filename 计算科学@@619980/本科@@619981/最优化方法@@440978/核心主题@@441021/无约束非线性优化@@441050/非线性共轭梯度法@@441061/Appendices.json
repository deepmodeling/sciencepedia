{"hands_on_practices": [{"introduction": "本次实践的核心任务是将非线性共轭梯度（NCG）算法从数学公式转化为可执行的程序。我们将使用稳健的 Polak–Ribière–Polyak+ (PRP+) 更新规则来实现一个求解器，并在包括著名的 Rosenbrock 函数在内的一系列标准优化问题上进行测试。这个练习旨在构建你在数值优化方面的基础编程能力，并展示该方法在应对不同挑战时的有效性。[@problem_id:2418452]", "problem": "给定几个定义在欧几里得空间上的可微目标函数及初始点。您的任务是编写一个完整的程序，在每种情况下，仅使用函数值和精确的一阶导数来计算近似极小值点。您的程序所使用的梯度对于指定的目标函数必须在机器精度内是精确的；请勿使用任何有限差分近似。请勿使用一阶导数之外的任何信息。使用基于梯度欧几里得范数的终止准则。\n\n数学设定：\n\n- 设 $f:\\mathbb{R}^n \\to \\mathbb{R}$ 是一个连续可微函数，$\\nabla f(\\mathbf{x})$ 表示其梯度。从给定的初始点 $\\mathbf{x}_0 \\in \\mathbb{R}^n$ 开始，仅使用 $f$ 和 $\\nabla f$ 的求值来计算一个序列 $\\{\\mathbf{x}_k\\}$，以尝试最小化 $f$。\n- 当 $\\|\\nabla f(\\mathbf{x}_k)\\|_2 \\le \\varepsilon$ 或迭代次数达到指定的最大值时终止。对 $\\|\\cdot\\|_2$ 使用欧几里得范数。\n- 算法不得使用任何二阶信息（无Hessian矩阵或Hessian向量积），也不得使用有限差分导数近似。程序必须在机器精度内计算指定函数的精确梯度。\n\n测试套件：\n\n对于以下每种情况，都给定了函数 $f(\\mathbf{x})$、维度 $n$ 和初始点 $\\mathbf{x}_0$。\n\n- 情况A（非凸、狭窄山谷、双变量）：\n  - 维度：$n=2$。\n  - 目标函数：\n    $$f(\\mathbf{x}) = 100\\,(x_2 - x_1^2)^2 + (1 - x_1)^2.$$\n  - 初始点：$\\mathbf{x}_0 = (-1.2,\\; 1.0)$。\n\n- 情况B（病态可分二次函数、五变量）：\n  - 维度：$n=5$。\n  - 目标函数：\n    $$f(\\mathbf{x}) = \\tfrac{1}{2}\\sum_{i=1}^{5} \\lambda_i x_i^2,\\quad \\lambda_i = 10^{\\,i-1}.$$\n  - 初始点：$\\mathbf{x}_0 = (1,\\; -1,\\; 1,\\; -1,\\; 1)$。\n\n- 情况C（光滑、耦合、非凸、三变量）：\n  - 维度：$n=3$。\n  - 目标函数：\n    $$f(\\mathbf{x}) = 0.1\\,(x_1^2 + x_2^2 + x_3^2) + \\sin(x_1)\\cos(x_2) + e^{x_3} - x_3.$$\n  - 初始点：$\\mathbf{x}_0 = (0.5,\\; -0.5,\\; 0.0)$。\n\n- 情况D（已在极小值点、四变量）：\n  - 维度：$n=4$。\n  - 目标函数：\n    $$f(\\mathbf{x}) = \\sum_{i=1}^{4} (x_i - 1)^2.$$\n  - 初始点：$\\mathbf{x}_0 = (1,\\; 1,\\; 1,\\; 1).$\n\n要求：\n\n- 停止容差：对梯度范数使用 $\\varepsilon = 10^{-6}$。\n- 最大迭代次数：每种情况使用 $N_{\\max} = 10000$。\n- 您的程序所使用的梯度对于指定的目标函数必须在机器精度内是精确的。\n- 程序不得读取任何输入，也不得写入除下述最终行之外的任何输出。\n\n输出规格：\n\n- 对于每种情况，报告终止时的最终目标函数值 $f(\\mathbf{x}_\\star)$，四舍五入到小数点后六位。\n- 您的程序应生成单行输出，其中包含用方括号括起来的逗号分隔列表。该列表必须按顺序包含情况A、情况B、情况C和情况D的四个四舍五入后的目标函数值。例如，一个有效的输出格式是：\n  - \"[0.000123,0.000000,1.234567,0.000000]\"\n\n不涉及物理单位。三角函数中使用的角度，按照数学惯例，以弧度为单位。输出必须是实数，并且必须遵循上述确切格式。", "solution": "所提出的问题是一个标准的数值优化任务，要求最小化几个定义明确的可微函数。方法被限制为一阶方法，这意味着它只能使用函数值 $f(\\mathbf{x})$ 和梯度值 $\\nabla f(\\mathbf{x})$。禁止使用二阶信息，如Hessian矩阵。在这些约束条件下，一个非常合适且高效的算法是非线性共轭梯度（CG）法。虽然更简单的最速下降法也满足这些约束，但对于具有高曲率或病态性质的问题，如Rosenbrock函数（情况A）和给定的具有悬殊特征值的二次函数（情况B），其收敛速度是出了名的慢。CG方法通过构建作为梯度共轭式扩展的搜索方向来加速收敛，从而有效地结合了先前步骤的信息。\n\n非线性CG算法的迭代过程，从初始点 $\\mathbf{x}_0$ 开始，对 $k=0, 1, 2, \\dots$ 定义如下：\n1. 计算当前迭代点的梯度：$\\mathbf{g}_k = \\nabla f(\\mathbf{x}_k)$。\n2. 检查收敛性：如果梯度的欧几里得范数 $\\|\\mathbf{g}_k\\|_2$ 小于指定的容差 $\\varepsilon$，算法终止。\n3. 计算搜索方向 $\\mathbf{p}_k$。对于第一次迭代（$k=0$），方向是最速下降方向，$\\mathbf{p}_0 = -\\mathbf{g}_0$。对于后续迭代（$k > 0$），方向是当前负梯度与前一个搜索方向的线性组合：\n   $$\n   \\mathbf{p}_k = -\\mathbf{g}_k + \\beta_k \\mathbf{p}_{k-1}\n   $$\n   标量 $\\beta_k$ 决定了CG方法的具体变体。\n4. 进行线搜索，以确定沿方向 $\\mathbf{p}_k$ 的合适步长 $\\alpha_k > 0$。\n5. 更新迭代点：$\\mathbf{x}_{k+1} = \\mathbf{x}_k + \\alpha_k \\mathbf{p}_k$。\n\n对于此实现，由于其经验性能通常优于Fletcher-Reeves等其他公式，因此选择Polak–Ribière–Polyak (PRP)公式来计算 $\\beta_k$。PRP公式为：\n$$\n\\beta_k^{\\text{PRP}} = \\frac{\\mathbf{g}_k^T (\\mathbf{g}_k - \\mathbf{g}_{k-1})}{\\mathbf{g}_{k-1}^T \\mathbf{g}_{k-1}}\n$$\n为提高鲁棒性并保证全局收敛性，将其增强为PRP+方法，其中 $\\beta_k = \\max(0, \\beta_k^{\\text{PRP}})$。如果 $\\beta_k^{\\text{PRP}}$ 变为负值（这可能在远离局部最小值时发生），此修改可防止算法采取不良步骤。此外，作为一种保障措施，如果搜索方向 $\\mathbf{p}_k$ 不再是下降方向（即，如果 $\\mathbf{p}_k^T \\mathbf{g}_k \\ge 0$），则强制将其重置为最速下降方向 $-\\mathbf{g}_k$。\n\n步长 $\\alpha_k$ 是通过满足强Wolfe条件的线搜索找到的：\n1. 充分下降（Armijo）条件：$f(\\mathbf{x}_k + \\alpha_k \\mathbf{p}_k) \\le f(\\mathbf{x}_k) + c_1 \\alpha_k \\mathbf{g}_k^T \\mathbf{p}_k$\n2. 曲率条件：$|\\nabla f(\\mathbf{x}_k + \\alpha_k \\mathbf{p}_k)^T \\mathbf{p}_k| \\le c_2 |\\mathbf{g}_k^T \\mathbf{p}_k|$\n常数选择为标准值 $c_1 = 10^{-4}$ 和 $c_2 = 0.9$。这些条件确保每一步都能在目标函数值上实现有意义的减少。使用 `scipy.optimize.line_search` 函数来实现这一步。\n\n解析梯度要求是精确的。四个测试用例的梯度推导如下：\n\n情况A：Rosenbrock函数，$f(\\mathbf{x}) = 100(x_2 - x_1^2)^2 + (1 - x_1)^2$，其中 $\\mathbf{x} \\in \\mathbb{R}^2$。\n$$\n\\nabla f(\\mathbf{x}) = \\begin{pmatrix} \\frac{\\partial f}{\\partial x_1} \\\\ \\frac{\\partial f}{\\partial x_2} \\end{pmatrix} = \\begin{pmatrix} -400x_1(x_2 - x_1^2) - 2(1 - x_1) \\\\ 200(x_2 - x_1^2) \\end{pmatrix}\n$$\n\n情况B：病态可分二次函数，$f(\\mathbf{x}) = \\frac{1}{2}\\sum_{i=1}^{5} \\lambda_i x_i^2$，其中 $\\lambda_i = 10^{i-1}$，$\\mathbf{x} \\in \\mathbb{R}^5$。\n每个 $x_j$ 的梯度分量为 $\\frac{\\partial f}{\\partial x_j} = \\lambda_j x_j$。\n$$\n\\nabla f(\\mathbf{x})_j = 10^{j-1} x_j, \\quad \\text{对于 } j=1, \\dots, 5\n$$\n\n情况C：光滑、耦合、非凸函数，$f(\\mathbf{x}) = 0.1(x_1^2 + x_2^2 + x_3^2) + \\sin(x_1)\\cos(x_2) + e^{x_3} - x_3$，其中 $\\mathbf{x} \\in \\mathbb{R}^3$。\n$$\n\\nabla f(\\mathbf{x}) = \\begin{pmatrix} 0.2x_1 + \\cos(x_1)\\cos(x_2) \\\\ 0.2x_2 - \\sin(x_1)\\sin(x_2) \\\\ 0.2x_3 + e^{x_3} - 1 \\end{pmatrix}\n$$\n\n情况D：简单二次函数，$f(\\mathbf{x}) = \\sum_{i=1}^{4} (x_i - 1)^2$，其中 $\\mathbf{x} \\in \\mathbb{R}^4$。\n每个 $x_j$ 的梯度分量为 $\\frac{\\partial f}{\\partial x_j} = 2(x_j-1)$。\n$$\n\\nabla f(\\mathbf{x})_j = 2(x_j - 1), \\quad \\text{对于 } j=1, \\dots, 4\n$$\n对于这种情况，初始点 $\\mathbf{x}_0 = (1, 1, 1, 1)$ 是该函数的唯一全局最小值点。因此，$\\nabla f(\\mathbf{x}_0) = \\mathbf{0}$，算法在第 $k=0$ 次迭代时立即终止，目标值为 $f(\\mathbf{x}_0)=0$。\n\n该实现将这些元素组合成一个单一的程序。一个通用的求解器函数封装了CG逻辑，并针对每个测试用例，使用各自的目标函数、梯度和初始点进行调用。当 $\\|\\nabla f(\\mathbf{x}_k)\\|_2 \\le 10^{-6}$ 或达到 $10000$ 次迭代后，程序终止。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import optimize\n\ndef conjugate_gradient_solver(f, grad_f, x0, tol=1e-6, max_iter=10000):\n    \"\"\"\n    Minimizes a function using the Nonlinear Conjugate Gradient method (Polak-Ribière-Polyak+).\n    \"\"\"\n    x_k = np.array(x0, dtype=float)\n    f_k = f(x_k)\n    g_k = grad_f(x_k)\n    grad_norm = np.linalg.norm(g_k)\n\n    if grad_norm = tol:\n        return f(x_k)\n\n    p_k = -g_k\n    \n    k = 0\n    while k  max_iter and grad_norm > tol:\n        # Perform line search to find alpha_k satisfying strong Wolfe conditions.\n        # c1=1e-4 and c2=0.9 are standard for CG.\n        try:\n            line_search_result = optimize.line_search(f, grad_f, x_k, p_k, gfk=g_k, old_fval=f_k, c1=1e-4, c2=0.9)\n            alpha_k = line_search_result[0]\n        except Exception:\n            # line_search can sometimes raise errors for extreme values\n            alpha_k = None\n\n        # If line search fails, restart with steepest descent.\n        if alpha_k is None:\n            p_k = -g_k\n            try:\n                line_search_result = optimize.line_search(f, grad_f, x_k, p_k, gfk=g_k, old_fval=f_k, c1=1e-4, c2=0.9)\n                alpha_k = line_search_result[0]\n            except Exception:\n                alpha_k = None\n            \n            if alpha_k is None:\n                # If it still fails, terminate. Could be due to precision limits.\n                break\n\n        x_k_plus_1 = x_k + alpha_k * p_k\n        g_k_plus_1 = grad_f(x_k_plus_1)\n\n        # Polak-Ribière-Polyak+ update for beta\n        g_k_dot_g_k = np.dot(g_k, g_k)\n        if g_k_dot_g_k == 0:\n            beta_k_plus_1 = 0.0\n        else:\n            beta_numerator = np.dot(g_k_plus_1, g_k_plus_1 - g_k)\n            beta_k_plus_1 = max(0, beta_numerator / g_k_dot_g_k)\n        \n        # New search direction\n        p_k_plus_1 = -g_k_plus_1 + beta_k_plus_1 * p_k\n\n        # Check for descent direction. If not, reset to steepest descent.\n        if np.dot(p_k_plus_1, g_k_plus_1) >= 0:\n            p_k_plus_1 = -g_k_plus_1\n\n        # Update variables for the next iteration\n        x_k = x_k_plus_1\n        g_k = g_k_plus_1\n        p_k = p_k_plus_1\n        f_k = f(x_k) # Can be taken from line_search output, but re-evaluating is simple.\n        \n        grad_norm = np.linalg.norm(g_k)\n        k += 1\n\n    return f(x_k)\n\ndef solve():\n    # Final print statement in the exact required format.\n    \n    # Case A: Rosenbrock function\n    def f_A(x):\n        return 100.0 * (x[1] - x[0]**2)**2 + (1.0 - x[0])**2\n\n    def grad_f_A(x):\n        df_dx1 = -400.0 * x[0] * (x[1] - x[0]**2) - 2.0 * (1.0 - x[0])\n        df_dx2 = 200.0 * (x[1] - x[0]**2)\n        return np.array([df_dx1, df_dx2])\n\n    # Case B: Ill-conditioned separable quadratic\n    def f_B(x):\n        lambdas = 10.**np.arange(len(x))\n        return 0.5 * np.sum(lambdas * x**2)\n\n    def grad_f_B(x):\n        lambdas = 10.**np.arange(len(x))\n        return lambdas * x\n\n    # Case C: Smooth, coupled, nonconvex\n    def f_C(x):\n        term1 = 0.1 * np.sum(x**2)\n        term2 = np.sin(x[0]) * np.cos(x[1])\n        term3 = np.exp(x[2]) - x[2]\n        return term1 + term2 + term3\n\n    def grad_f_C(x):\n        df_dx1 = 0.2 * x[0] + np.cos(x[0]) * np.cos(x[1])\n        df_dx2 = 0.2 * x[1] - np.sin(x[0]) * np.sin(x[1])\n        df_dx3 = 0.2 * x[2] + np.exp(x[2]) - 1.0\n        return np.array([df_dx1, df_dx2, df_dx3])\n    \n    # Case D: Simple quadratic\n    def f_D(x):\n        return np.sum((x - 1.0)**2)\n\n    def grad_f_D(x):\n        return 2.0 * (x - 1.0)\n    \n    test_cases = [\n        {'f': f_A, 'grad_f': grad_f_A, 'x0': [-1.2, 1.0]},\n        {'f': f_B, 'grad_f': grad_f_B, 'x0': [1.0, -1.0, 1.0, -1.0, 1.0]},\n        {'f': f_C, 'grad_f': grad_f_C, 'x0': [0.5, -0.5, 0.0]},\n        {'f': f_D, 'grad_f': grad_f_D, 'x0': [1.0, 1.0, 1.0, 1.0]}\n    ]\n\n    results = []\n    for case in test_cases:\n        final_f_val = conjugate_gradient_solver(\n            f=case['f'],\n            grad_f=case['grad_f'],\n            x0=case['x0'],\n            tol=1e-6,\n            max_iter=10000\n        )\n        results.append(final_f_val)\n\n    # Format output as specified\n    formatted_results = [\"{:.6f}\".format(r) for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "2418452"}, {"introduction": "一个稳健的 NCG 求解器其核心组成部分之一是线搜索，它能确保每一步都取得足够的进展。在本练习中，我们将特意创建一个使用固定步长的“有缺陷”的求解器，并将其与一个强制执行 Armijo 充分下降条件的标准求解器进行比较。这种对比将清晰地揭示，为何严谨的线搜索不仅是一种优化，更是保证算法收敛性的必要条件，尤其是在处理具有挑战性的非凸或病态问题时。[@problem_id:2418455]", "problem": "要求您实现并比较非线性共轭梯度法的两种变体，用于求解连续可微函数的无约束最小化问题。第一种变体是标准的非线性共轭梯度算法，它通过回溯线搜索强制满足第一个 Wolfe 条件（Armijo 充分下降）。第二种变体是故意设计的错误版本：它不强制满足 Armijo 条件，而是在每次迭代中使用固定的单位步长。您的任务是通过精心选择的测试函数来证明，即使标准方法能够收敛，省略 Armijo 条件也可能导致算法不收敛或发散。\n\n从以下基础概念开始：\n- 对于一个连续可微的目标函数 $f:\\mathbb{R}^n \\to \\mathbb{R}$，下降方法生成迭代点 $x_{k+1} = x_k + \\alpha_k d_k$，其中搜索方向 $d_k$ 满足 $g_k^\\top d_k  0$，$g_k = \\nabla f(x_k)$ 且 $\\alpha_k > 0$ 是步长。\n- 第一个 Wolfe (Armijo) 充分下降条件要求，对于常数 $c_1 \\in (0,1)$，步长需满足\n$$\nf(x_k + \\alpha_k d_k) \\le f(x_k) + c_1 \\alpha_k g_k^\\top d_k.\n$$\n- 在非线性共轭梯度法中，搜索方向通过以下方式构造\n$$\nd_0 = -g_0,\\quad d_k = -g_k + \\beta_k d_{k-1}\\ \\text{for}\\ k \\ge 1,\n$$\n其中 $\\beta_k$ 的经典选择如 Polak–Ribière–Plus，并带有一个安全措施：如果 $g_k^\\top d_k \\ge 0$，则重置 $d_k=-g_k$ 以保持下降性。\n- 对于形如 $f(x) = \\tfrac{1}{2} x^\\top Q x$ 的光滑凸二次目标函数，其中 $Q$ 是对称正定矩阵，采用固定步长 $\\alpha$ 的梯度下降法会产生线性迭代 $x_{k+1} = (I - \\alpha Q) x_k$。当且仅当谱半径满足 $\\rho(I - \\alpha Q)  1$ 时，算法收敛到极小值点 $x^\\star=0$，这等价于 $0  \\alpha  2/\\lambda_{\\max}(Q)$。\n\n您的程序必须实现：\n- 一个标准的非线性共轭梯度求解器，该求解器使用回溯线搜索来强制满足 Armijo 条件，并采用用户选择的常数 $c_1 \\in (0,1)$ 和回溯率 $\\tau \\in (0,1)$。\n- 一个有缺陷的非线性共轭梯度求解器，它对所有 $k$ 都使用 $\\alpha_k \\equiv 1$（无充分下降检查）。\n\n您必须遵循的设计细节：\n- 对共轭参数 $\\beta_k$ 使用 Polak–Ribière–Plus 选择，并在计算出的方向不是下降方向时采用重置安全措施。\n- 当 $\\|\\nabla f(x_k)\\|_2 \\le \\varepsilon$（给定容差 $\\varepsilon > 0$）或当固定的迭代预算耗尽时终止。\n- 如果在耗尽迭代预算时仍未满足梯度容差，则判定为不收敛。如果目标函数值变为非数值、超过一个大阈值，或迭代点范数超过一个大阈值，则判定为发散。\n\n构建一个测试套件以展示不同的行为：\n- 测试 A（发散见证）：一个凸二次函数 $f(x) = \\tfrac{1}{2} x^\\top Q x$，其中 $Q = \\mathrm{diag}(10.0, 0.1)$，初始点 $x_0 = [1.0, 1.0]$。根据谱半径判据，固定步长 $\\alpha = 1$ 违反了 $0  \\alpha  2/\\lambda_{\\max}(Q)$，因为 $\\lambda_{\\max}(Q) = 10.0$，因此错误方法预期会发散，而强制执行 Armijo 条件的方法应该会收敛。\n- 测试 B（理想路径）：一个良态的凸二次函数，其中 $Q = \\mathrm{diag}(0.5, 0.25)$，初始点 $x_0 = [2.0, -3.0]$。这里 $\\lambda_{\\max}(Q) = 0.5$，所以固定步长 $\\alpha = 1$ 满足 $0  \\alpha  2/\\lambda_{\\max}(Q)$，两种方法都应该收敛。\n- 测试 C（边界条件）：同样的凸二次函数，其中 $Q = \\mathrm{diag}(1.0, 1.0)$，初始点 $x_0 = [0.0, 0.0]$，这已经是一个极小值点。两种方法都应立即检测到收敛。\n- 测试 D（非凸压力测试）：一个缩放的 Rosenbrock 函数 $f(x_1,x_2) = 10\\,(x_2 - x_1^2)^2 + (1 - x_1)^2$，初始点 $x_0 = [-1.2, 1.0]$。错误方法的单位步长可能会在这个弯曲的峡谷中导致数值爆炸，而带有 Armijo 回溯的标准方法应该能收敛到 $[1,1]$ 附近的极小值点。\n\n在您的程序中使用的数值参数：\n- 梯度范数容差 $\\varepsilon = 10^{-6}$，最大迭代次数 $N_{\\max} = 5000$，Armijo 常数 $c_1 = 10^{-4}$，回溯率 $\\tau = 0.5$。\n- 发散阈值：如果 $\\|x_k\\|_2 > 10^{12}$ 或 $f(x_k) > 10^{50}$ 或 $f(x_k)$ 为非数值，则判定为发散。\n\n您的程序必须：\n- 实现两种求解器，在所有四个测试上运行它们，并根据观察到的行为为每个测试确定一个整数代码：\n    - 如果标准方法收敛而错误方法不收敛（不收敛或发散），则输出 $1$。\n    - 如果两者都收敛，则输出 $0$。\n    - 如果两者都不收敛，则输出 $-1$。\n    - 如果错误方法收敛而标准方法不收敛，则输出 $2$。\n- 生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果。例如，输出格式必须类似于单个 Python 风格的列表字面量，如 [r1,r2,r3,r4]，其中每个条目是指定的整数代码之一。\n\n不涉及角度单位。此问题中没有物理单位。所有量都是无量纲的。输出必须严格遵循指定的单行格式。", "solution": "所提出的问题是实现并对比分析用于无约束优化的非线性共轭梯度 (NCG) 法的两种变体。一种变体被正确实现，它遵循线搜索方法的基本原则，强制满足 Armijo 充分下降条件。第二种变体则被故意设计为有缺陷的，它采用固定的单位步长，从而省略了线搜索这一关键的保障措施。目标是通过计算证明，在正确实现的算法能够成功的问题上，省略 Armijo 条件可能导致失败，具体表现为不收敛或发散。该问题是适定的、科学上合理的，并为算法实现和验证提供了清晰的基础。\n\n考虑一个一般的无约束最小化问题，旨在寻找一个连续可微目标函数 $f: \\mathbb{R}^n \\to \\mathbb{R}$ 的局部极小值点。NCG 方法是一种迭代算法，它使用更新规则生成一个点序列 $\\{x_k\\}_{k \\ge 0}$：\n$$\nx_{k+1} = x_k + \\alpha_k d_k\n$$\n这里，$x_k \\in \\mathbb{R}^n$ 是当前迭代点，$d_k \\in \\mathbb{R}^n$ 是搜索方向，$\\alpha_k > 0$ 是步长。目标函数在 $x_k$ 处的梯度记为 $g_k = \\nabla f(x_k)$。\n\n搜索方向 $d_k$ 被构造为下降方向，即 $g_k^\\top d_k  0$。NCG 方向是递归定义的。初始方向是最速下降方向 $d_0 = -g_0$。对于后续的迭代 $k \\ge 1$，方向是当前负梯度与前一个方向的线性组合：\n$$\nd_k = -g_k + \\beta_k d_{k-1}\n$$\n标量 $\\beta_k$ 是共轭参数。问题指定了 Polak–Ribière–Plus 变体，该变体以其强大的数值性能而闻名。其定义为：\n$$\n\\beta_k = \\max \\left\\{ 0, \\frac{g_k^\\top(g_k - g_{k-1})}{\\|g_{k-1}\\|_2^2} \\right\\}\n$$\n这个选择包含了一个非负约束，这在某些条件下有助于确保全局收敛。一个至关重要的安全措施是重置条件：如果计算出的方向 $d_k$ 未能成为下降方向（即，如果 $g_k^\\top d_k \\ge 0$），则通过将搜索方向设为最速下降方向 $d_k = -g_k$ 来重置该方法。\n\n本次研究的核心在于步长 $\\alpha_k$ 的确定。\n\n**标准的 NCG 方法**采用回溯线搜索来寻找一个满足 Armijo 充分下降条件的步长 $\\alpha_k$。对于给定的常数 $c_1 \\in (0, 1)$，该条件为：\n$$\nf(x_k + \\alpha_k d_k) \\le f(x_k) + c_1 \\alpha_k g_k^\\top d_k\n$$\n该不等式确保目标函数的下降量至少是在 $x_k$ 处 $f$ 的线性近似所预测的下降量的一个分数。回溯过程从一个初始试探步长（通常为 $\\alpha = 1$）开始，并以一个因子 $\\tau \\in (0, 1)$（例如 $\\alpha \\leftarrow \\tau \\alpha$）迭代地减小步长，直到满足该条件。指定的参数为 $c_1 = 10^{-4}$ 和 $\\tau = 0.5$。\n\n**有缺陷的 NCG 方法**绕过了这一关键检查，并简单地对所有迭代 $k \\ge 0$ 设置 $\\alpha_k = 1$。虽然对于某些良态函数，或者当初始迭代点接近解时，这样做可能是可以接受的，但它通常是一种可能导致失败的不可靠策略。\n\n算法的终止由梯度的范数决定。如果 $\\|\\nabla f(x_k)\\|_2 \\le \\varepsilon$（对于容差 $\\varepsilon = 10^{-6}$），则认为迭代收敛。如果迭代次数超过预算 $N_{\\max} = 5000$，则中止过程，这被归类为不收敛。如果迭代点范数 $\\|x_k\\|_2$ 超过 $10^{12}$，函数值 $f(x_k)$ 超过 $10^{50}$，或者 $f(x_k)$ 变为非数值 (NaN)，则判定为发散。\n\n分析是在一套包含四个测试用例的测试套件上进行的，这些测试用例旨在揭示两种方法的不同行为。\n\n**测试 A**：一个凸二次函数 $f(x) = \\frac{1}{2} x^\\top Q x$，其 Hessian 矩阵 $Q = \\mathrm{diag}(10.0, 0.1)$ 是病态的。对于二次函数，采用固定步长 $\\alpha$ 的 NCG 方法等价于线性迭代系统 $x_{k+1} = (I - \\alpha Q) x_k$。该系统收敛当且仅当迭代矩阵的谱半径 $\\rho(I - \\alpha Q)$ 小于 1。当 $\\alpha=1$ 时，$I-Q$ 的特征值为 $1-10.0 = -9.0$ 和 $1-0.1 = 0.9$。谱半径为 $\\rho(I - Q) = \\max\\{|-9.0|, |0.9|\\} = 9.0$，大于 1。因此，有缺陷的方法保证会发散。而标准方法由于 Armijo 条件带来的自适应步长，预期会收敛。\n\n**测试 B**：一个良态的凸二次函数，其中 $Q = \\mathrm{diag}(0.5, 0.25)$。在这里，对于采用 $\\alpha=1$ 的有缺陷方法，$I-Q$ 的特征值为 $1-0.5 = 0.5$ 和 $1-0.25=0.75$。谱半径为 $\\rho(I - Q) = 0.75  1$，满足收敛条件。因此，有缺陷的方法和标准方法都预期会收敛。\n\n**测试 C**：一个凸二次函数，其初始点 $x_0 = [0.0, 0.0]$ 是全局极小值点。初始梯度为 $\\nabla f(x_0) = 0$。两个算法都必须在第一次迭代前检查终止条件，并立即判定收敛。\n\n**测试 D**：非凸的 Rosenbrock 函数 $f(x_1,x_2) = 10(x_2 - x_1^2)^2 + (1-x_1)^2$，从起始点 $x_0 = [-1.2, 1.0]$ 开始。这个函数是一个经典的基准测试，其特点是存在一个狭窄、弯曲的峡谷。有缺陷的方法所采用的固定单位步长很可能导致迭代点“跳过”峡谷，从而引起函数值增加和不稳定的行为，可能导致发散或不收敛。相比之下，标准方法的回溯线搜索将系统性地减小步长以确保充分下降，使得迭代点能够沿着峡谷走向位于 $[1,1]$ 附近的极小值点。\n\n每个测试的结果是一个整数代码：如果标准方法收敛而有缺陷的方法不收敛，则为 $1$；如果两者都收敛，则为 $0$；如果两者都不收敛，则为 $-1$；如果是有缺陷的方法收敛而标准方法不收敛，则为 $2$。这种系统性的比较为线搜索机制在确保基于下降的优化算法的鲁棒性方面所起的不可或缺的作用提供了清晰的证据。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and compares a proper and a faulty nonlinear conjugate gradient method.\n    \"\"\"\n\n    # --- Numerical Parameters ---\n    EPSILON = 1e-6\n    MAX_ITER = 5000\n    C1 = 1e-4\n    TAU = 0.5\n    DIV_NORM_THRESHOLD = 1e12\n    DIV_F_THRESHOLD = 1e50\n\n    def nonlinear_cg(f, grad_f, x0, use_armijo):\n        \"\"\"\n        Nonlinear Conjugate Gradient (NCG) solver.\n\n        Args:\n            f: Objective function.\n            grad_f: Gradient of the objective function.\n            x0: Initial point.\n            use_armijo: Boolean flag to use Armijo line search.\n\n        Returns:\n            A string indicating the outcome: \"converged\", \"nonconverged\", \"diverged\".\n        \"\"\"\n        x_k = np.copy(x0).astype(np.float64)\n        g_k = grad_f(x_k)\n        \n        # Initial check for convergence at x0\n        norm_g_k = np.linalg.norm(g_k)\n        if norm_g_k = EPSILON:\n            return \"converged\"\n\n        d_k = -g_k\n        k = 0\n\n        while k  MAX_ITER:\n            # Line Search\n            if use_armijo:\n                alpha_k = 1.0\n                descent_condition_val = C1 * np.dot(g_k, d_k)\n                # The dot product g_k.T @ d_k should be  0 due to safeguard\n                try:\n                    f_k = f(x_k)\n                    while f(x_k + alpha_k * d_k) > f_k + alpha_k * descent_condition_val:\n                        alpha_k *= TAU\n                        if alpha_k  1e-15: # Prevent infinite loop if step size becomes too small\n                           return \"nonconverged\"\n                except (OverflowError, ValueError):\n                    return \"diverged\" # f() evaluation might fail\n            else:\n                alpha_k = 1.0\n\n            # Update position\n            x_k_plus_1 = x_k + alpha_k * d_k\n\n            # Check for divergence\n            try:\n                f_next = f(x_k_plus_1)\n                if np.linalg.norm(x_k_plus_1) > DIV_NORM_THRESHOLD or f_next > DIV_F_THRESHOLD or np.isnan(f_next):\n                    return \"diverged\"\n            except (OverflowError, ValueError):\n                return \"diverged\"\n\n            g_k_plus_1 = grad_f(x_k_plus_1)\n            norm_g_k_plus_1 = np.linalg.norm(g_k_plus_1)\n\n            # Check for convergence\n            if norm_g_k_plus_1 = EPSILON:\n                return \"converged\"\n\n            # Polak-Ribiere-Plus (PR+) for beta\n            norm_g_k_sq = norm_g_k**2\n            if norm_g_k_sq > 1e-14: # Safeguard against division by zero\n                beta_k_plus_1 = max(0, np.dot(g_k_plus_1, g_k_plus_1 - g_k) / norm_g_k_sq)\n            else:\n                beta_k_plus_1 = 0.0\n\n            # Update search direction\n            d_k_plus_1 = -g_k_plus_1 + beta_k_plus_1 * d_k\n\n            # Restart if not a descent direction\n            if np.dot(g_k_plus_1, d_k_plus_1) >= 0:\n                d_k_plus_1 = -g_k_plus_1\n\n            # Prepare for next iteration\n            x_k = x_k_plus_1\n            g_k = g_k_plus_1\n            norm_g_k = norm_g_k_plus_1\n            d_k = d_k_plus_1\n            k += 1\n\n        return \"nonconverged\"\n    \n    # --- Test Case Definitions ---\n\n    # Test A: Divergence Witness\n    Q_A = np.diag([10.0, 0.1])\n    def f_A(x): return 0.5 * x.T @ Q_A @ x\n    def grad_f_A(x): return Q_A @ x\n    x0_A = np.array([1.0, 1.0])\n\n    # Test B: Happy Path\n    Q_B = np.diag([0.5, 0.25])\n    def f_B(x): return 0.5 * x.T @ Q_B @ x\n    def grad_f_B(x): return Q_B @ x\n    x0_B = np.array([2.0, -3.0])\n\n    # Test C: Boundary Condition\n    Q_C = np.diag([1.0, 1.0])\n    def f_C(x): return 0.5 * x.T @ Q_C @ x\n    def grad_f_C(x): return Q_C @ x\n    x0_C = np.array([0.0, 0.0])\n\n    # Test D: Nonconvex Stress Test (Scaled Rosenbrock)\n    def f_D(x): return 10.0 * (x[1] - x[0]**2)**2 + (1.0 - x[0])**2\n    def grad_f_D(x):\n        df_dx1 = -40.0 * x[0] * (x[1] - x[0]**2) - 2.0 * (1.0 - x[0])\n        df_dx2 = 20.0 * (x[1] - x[0]**2)\n        return np.array([df_dx1, df_dx2])\n    x0_D = np.array([-1.2, 1.0])\n\n    test_cases = [\n        (f_A, grad_f_A, x0_A),\n        (f_B, grad_f_B, x0_B),\n        (f_C, grad_f_C, x0_C),\n        (f_D, grad_f_D, x0_D),\n    ]\n\n    results = []\n    for f, grad_f, x0 in test_cases:\n        proper_status = nonlinear_cg(f, grad_f, x0, use_armijo=True)\n        faulty_status = nonlinear_cg(f, grad_f, x0, use_armijo=False)\n\n        proper_converged = (proper_status == \"converged\")\n        faulty_converged = (faulty_status == \"converged\")\n\n        if proper_converged and not faulty_converged:\n            results.append(1)\n        elif proper_converged and faulty_converged:\n            results.append(0)\n        elif not proper_converged and not faulty_converged:\n            results.append(-1)\n        elif not proper_converged and faulty_converged:\n            results.append(2)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2418455"}, {"introduction": "参数 $\\beta_k$ 更新公式的选择是区分不同 NCG 变体的关键。本次实践聚焦于流行的 Polak–Ribière–Polyak (PRP) 更新方法及其“plus”修正版（PRP+）。通过在一个精心挑选的非凸函数上测试这两种方法，你将亲眼观察到标准 PRP 有时如何无法产生下降方向，以及 PRP+ 中简单的非负约束如何提供关键的保障，从而增强算法的稳健性。[@problem_id:2418475]", "problem": "您将编写一个完整、可运行的程序，在一个非凸、二次连续可微的目标函数上，比较使用 Polak–Ribière–Polyak (PRP) 更新的非线性共轭梯度法与其修改后的变体 PRP-plus (PRP+)。在这个目标函数上，标准的 PRP 方法可能会无法生成下降方向。您必须仅使用无约束平滑最小化的基本定义以及 PRP 和 PRP-plus 更新的定义来构建算法。\n\n无约束最小化问题是最小化函数\n$$\nf(\\mathbf{x}) = \\tfrac{1}{4} x_1^4 - \\tfrac{1}{2} x_1^2 + \\tfrac{1}{2} x_2^2,\n$$\n其梯度为\n$$\n\\nabla f(\\mathbf{x}) = \\begin{bmatrix} x_1^3 - x_1 \\\\ x_2 \\end{bmatrix}.\n$$\n该函数是非凸的，因为其 Hessian 矩阵的特征值为 $3 x_1^2 - 1$ 和 $1$，因此对于 $|x_1|  1/\\sqrt{3}$，在 $x_1$ 方向上存在负曲率。\n\n实现两个非线性共轭梯度求解器，它们共享除更新系数外的所有组件：\n- 搜索方向初始化：$\\mathbf{d}_0 = -\\nabla f(\\mathbf{x}_0)$。\n- 在迭代 $k \\ge 1$ 时，计算\n$$\n\\beta_k^{PRP} = \\frac{\\nabla f(\\mathbf{x}_k)^\\top\\big(\\nabla f(\\mathbf{x}_k) - \\nabla f(\\mathbf{x}_{k-1})\\big)}{\\|\\nabla f(\\mathbf{x}_{k-1})\\|_2^2},\n$$\n并令方向更新为 $\\mathbf{d}_k = -\\nabla f(\\mathbf{x}_k) + \\beta_k \\mathbf{d}_{k-1}$，其中 $\\beta_k$ 的选择如下：\n- PRP：$\\beta_k = \\beta_k^{PRP}$，\n- PRP-plus (PRP+)：$\\beta_k = \\max\\{0, \\beta_k^{PRP}\\}$。\n\n使用回溯 Armijo 线搜索来选择沿 $\\mathbf{d}_k$ 的步长 $\\alpha_k$。从一个初始试验步长 $\\alpha_0$ 开始，通过一个固定因子重复减小，直到满足 Armijo 充分下降条件。具体来说，对于给定的参数 $c_1 \\in (0, 1)$ 和 $\\rho \\in (0, 1)$，找到最小的整数 $m \\ge 0$，使得当 $\\alpha = \\alpha_0 \\rho^m$ 时，\n$$\nf(\\mathbf{x}_k + \\alpha \\mathbf{d}_k) \\le f(\\mathbf{x}_k) + c_1 \\alpha \\nabla f(\\mathbf{x}_k)^\\top \\mathbf{d}_k.\n$$\n\n停止准则：当 $\\|\\nabla f(\\mathbf{x}_k)\\|_2 \\le \\varepsilon$ 或达到最大迭代次数时停止。\n\n下降测试：在每次迭代 $k$ 迈出一步之前，记录 $\\nabla f(\\mathbf{x}_k)^\\top \\mathbf{d}_k  0$ 是否成立（这表示一个下降方向）。对于每次运行，生成一个布尔标志，如果运行期间至少有一次出现非下降方向（即，对于任何 $k$，有 $\\nabla f(\\mathbf{x}_k)^\\top \\mathbf{d}_k \\ge 0$），则该标志为真。\n\n在所有运行中使用的参数：\n- 初始试验步长 $ \\alpha_0 = 5.0$，\n- 充分下降常数 $ c_1 = 10^{-4}$，\n- 回溯因子 $ \\rho = 0.5$，\n- 容差 $ \\varepsilon = 10^{-8}$，\n- 最大迭代次数 $ N_{\\max} = 50$，\n- 每次线搜索的最大回溯次数 $ M_{\\max} = 40$。\n\n您必须从以下初始点开始运行 PRP 和 PRP+（这是测试套件）：\n1. $\\mathbf{x}_0^{(1)} = \\begin{bmatrix} 0.2 \\\\ 0.0 \\end{bmatrix}$，这是一个靠近 $x_1$ 方向上鞍点的点，该处的负曲率可能导致 PRP 方法在后续迭代中无法生成下降方向。\n2. $\\mathbf{x}_0^{(2)} = \\begin{bmatrix} 0.0 \\\\ 2.0 \\end{bmatrix}$，这是一个在 $x_2$ 方向上有大梯度但 $x_1$ 分量为零的点，用于探测可分离性以及沿 $x_2$ 方向的快速下降。\n3. $\\mathbf{x}_0^{(3)} = \\begin{bmatrix} -1.5 \\\\ 0.5 \\end{bmatrix}$，这是一个位于左侧极小值点左侧且 $x_2$ 分量非零的点。\n4. $\\mathbf{x}_0^{(4)} = \\begin{bmatrix} 1.2 \\\\ -1.0 \\end{bmatrix}$，这是一个位于右侧极小值点右侧且 $x_2$ 分量非零的点。\n\n对于每个初始点，运行 PRP 和 PRP+，并返回以下四个值的列表：\n- 终止后 PRP 的最终目标值，四舍五入到六位小数，\n- 终止后 PRP+ 的最终目标值，四舍五入到六位小数，\n- 一个布尔值，指示 PRP 是否至少生成过一次非下降方向，\n- 一个布尔值，指示 PRP+ 是否至少生成过一次非下降方向。\n\n最终输出格式：您的程序应生成单行输出，其中包含一个含四个列表的列表，每个列表对应一个初始点，顺序如上所列。每个内部列表必须是 $[f_{\\text{PRP}}, f_{\\text{PRP+}}, b_{\\text{PRP}}, b_{\\text{PRP+}}]$ 的形式，其中 $f_{\\text{PRP}}$ 和 $f_{\\text{PRP+}}$ 是四舍五入到六位小数的浮点数，$b_{\\text{PRP}}$ 和 $b_{\\text{PRP+}}$ 是布尔值。例如，一个语法正确的输出形状是\n$$\n[[0.0,0.0,False,True],[\\dots],[\\dots],[\\dots]].\n$$\n作为一个重要的边缘情况，请注意，在 $\\mathbf{x}_0^{(1)} = [0.2, 0.0]^\\top$ 处，使用 $\\alpha_0 = 5.0$ 和所描述的 Armijo 规则，第一步会落在大约 $\\mathbf{x}_1 \\approx [1.16, 0.0]^\\top$ 的位置，对此 PRP 更新会产生一个正的 $\\beta_1^{PRP}$，因此下一个搜索方向 $\\mathbf{d}_1$ 满足 $\\nabla f(\\mathbf{x}_1)^\\top \\mathbf{d}_1 > 0$，即一个非下降方向。在这种情况下，PRP+ 更新与 PRP 一致，因为 $\\beta_1^{PRP} > 0$，因此在该迭代中也表现出非下降方向。您的程序必须通过下降测试来检测此类事件。", "solution": "用户提供了一个在计算工程领域内有效且定义明确的问题，具体涉及无约束非线性优化。任务是实现并比较非线性共轭梯度（NCG）方法的两种变体——Polak–Ribière–Polyak (PRP) 及其修改版 PRP-plus (PRP+)——在一个特定的非凸目标函数上。所有参数、初始条件和评估标准都已明确定义。该问题在科学上是合理的，并且需要实现已有的数值算法。因此，将开发一个完整的解决方案。\n\n问题的核心是迭代最小化目标函数 $f(\\mathbf{x}): \\mathbb{R}^2 \\to \\mathbb{R}$，其定义为：\n$$\nf(\\mathbf{x}) = \\frac{1}{4} x_1^4 - \\frac{1}{2} x_1^2 + \\frac{1}{2} x_2^2\n$$\n该函数的梯度 $\\nabla f(\\mathbf{x})$ 对 NCG 方法至关重要，其表达式为：\n$$\n\\nabla f(\\mathbf{x}) = \\begin{bmatrix} x_1^3 - x_1 \\\\ x_2 \\end{bmatrix}\n$$\nNCG 方法是一种迭代算法，它生成一系列点 $\\mathbf{x}_k$，旨在收敛到 $f(\\mathbf{x})$ 的一个局部最小值。从 $\\mathbf{x}_k$ 到 $\\mathbf{x}_{k+1}$ 的更新遵循以下规则：\n$$\n\\mathbf{x}_{k+1} = \\mathbf{x}_k + \\alpha_k \\mathbf{d}_k\n$$\n其中 $\\mathbf{d}_k$ 是搜索方向，$\\alpha_k > 0$ 是步长。\n\n对于每次迭代 $k = 0, 1, 2, \\dots$，算法按以下步骤进行：\n\n1.  **搜索方向计算**：初始搜索方向是最速下降方向：$\\mathbf{d}_0 = -\\nabla f(\\mathbf{x}_0)$。对于后续迭代（$k \\ge 1$），搜索方向是当前负梯度和前一个搜索方向的线性组合：\n    $$\n    \\mathbf{d}_k = -\\nabla f(\\mathbf{x}_k) + \\beta_k \\mathbf{d}_{k-1}\n    $$\n    系数 $\\beta_k$ 区分了不同的 NCG 方法。本问题考虑 PRP 和 PRP+ 变体。标准的 PRP 更新公式为：\n    $$\n    \\beta_k^{\\text{PRP}} = \\frac{\\nabla f(\\mathbf{x}_k)^\\top \\left( \\nabla f(\\mathbf{x}_k) - \\nabla f(\\mathbf{x}_{k-1}) \\right)}{\\|\\nabla f(\\mathbf{x}_{k-1})\\|_2^2}\n    $$\n    这两种方法由它们对 $\\beta_k$ 的选择来定义：\n    -   **PRP 方法**：$\\beta_k = \\beta_k^{\\text{PRP}}$。该方法可能表现出良好的性能，但在非凸问题上可能无法生成下降方向，从而可能导致算法失败。\n    -   **PRP+ 方法**：$\\beta_k = \\max\\{0, \\beta_k^{\\text{PRP}}\\}$。此修改确保 $\\beta_k$ 是非负的。如果 $\\beta_k^{\\text{PRP}}  0$，$\\beta_k$ 将被重置为 $0$，搜索方向变为最速下降方向 $\\mathbf{d}_k = -\\nabla f(\\mathbf{x}_k)$，这保证了下降（前提是 $\\nabla f(\\mathbf{x}_k) \\neq \\mathbf{0}$）。这使得 PRP+ 方法更加稳健。\n\n2.  **下降方向测试**：在确定步长之前，验证 $\\mathbf{d}_k$ 是否为下降方向至关重要。如果 $\\nabla f(\\mathbf{x}_k)^\\top \\mathbf{d}_k  0$，则该条件满足。我们将为每次运行记录是否曾生成过非下降方向（即 $\\nabla f(\\mathbf{x}_k)^\\top \\mathbf{d}_k \\ge 0$）。\n\n3.  **步长计算**：步长 $\\alpha_k$ 通过满足 Armijo 充分下降条件的回溯线搜索来确定。从一个初始试验步长 $\\alpha = \\alpha_0$ 开始，我们通过一个因子 $\\rho$ 迭代地减小它，直到对于最小的整数 $m \\ge 0$ 满足以下条件：\n    $$\n    f(\\mathbf{x}_k + \\alpha \\mathbf{d}_k) \\le f(\\mathbf{x}_k) + c_1 \\alpha \\nabla f(\\mathbf{x}_k)^\\top \\mathbf{d}_k\n    $$\n    其中 $\\alpha = \\alpha_0 \\rho^m$。给定的参数为 $c_1 = 10^{-4}$，$\\rho = 0.5$，初始试验步长为 $\\alpha_0 = 5.0$。线搜索将在最多 $M_{\\max} = 40$ 次缩减后终止。\n\n4.  **终止**：迭代过程持续进行，直到梯度的范数低于指定的容差 $\\varepsilon = 10^{-8}$，或达到最大迭代次数 $N_{\\max} = 50$。\n\n实现将包含一个封装了此逻辑的主求解器函数。该函数将针对两种方法（PRP 和 PRP+）在四个指定的初始点 $\\mathbf{x}_0$ 上分别调用。对于每次运行，将记录最终的目标函数值和一个指示是否遇到非下降方向的布尔标志。最终结果将按规定格式进行汇总和格式化。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the NCG comparison and print results.\n    \"\"\"\n    params = {\n        'alpha0_init': 5.0,\n        'c1': 1e-4,\n        'rho': 0.5,\n        'epsilon': 1e-8,\n        'N_max': 50,\n        'M_max': 40,\n    }\n\n    test_cases = [\n        np.array([0.2, 0.0]),\n        np.array([0.0, 2.0]),\n        np.array([-1.5, 0.5]),\n        np.array([1.2, -1.0])\n    ]\n\n    all_results = []\n    for x0 in test_cases:\n        f_prp, b_prp = run_cg_solver(x0, 'prp', params)\n        f_prp_plus, b_prp_plus = run_cg_solver(x0, 'prp+', params)\n        \n        inner_result = [\n            round(f_prp, 6),\n            round(f_prp_plus, 6),\n            b_prp,\n            b_prp_plus\n        ]\n        all_results.append(inner_result)\n\n    # Format the final output string as specified, without spaces in lists.\n    string_inner_lists = []\n    for res_list in all_results:\n        parts = [str(item) for item in res_list]\n        string_inner_lists.append(f\"[{','.join(parts)}]\")\n    \n    final_output = f\"[{','.join(string_inner_lists)}]\"\n    print(final_output)\n\ndef f(x):\n    \"\"\"Objective function.\"\"\"\n    return 0.25 * x[0]**4 - 0.5 * x[0]**2 + 0.5 * x[1]**2\n\ndef grad_f(x):\n    \"\"\"Gradient of the objective function.\"\"\"\n    return np.array([x[0]**3 - x[0], x[1]])\n\ndef line_search(xk, dk, fk, grad_fk_dot_dk, params):\n    \"\"\"Backtracking Armijo line search.\"\"\"\n    alpha = params['alpha0_init']\n    c1 = params['c1']\n    rho = params['rho']\n    M_max = params['M_max']\n\n    for _ in range(M_max):\n        # A very small step might result from a non-descent direction.\n        # This is expected behavior demonstrating the algorithm's failure.\n        if grad_fk_dot_dk >= 0:\n            pass # Armijo condition check below will handle this\n\n        if f(xk + alpha * dk) = fk + c1 * alpha * grad_fk_dot_dk:\n            return alpha\n        alpha *= rho\n    \n    return alpha # Return the last (smallest) alpha if M_max is reached.\n\ndef run_cg_solver(x0, method_type, params):\n    \"\"\"\n    Runs the Nonlinear Conjugate Gradient solver.\n    \"\"\"\n    x_k = np.copy(x0).astype(np.float64)\n    k = 0\n    non_descent_occurred = False\n\n    N_max = params['N_max']\n    epsilon = params['epsilon']\n\n    grad_k = grad_f(x_k)\n    d_k = -grad_k\n    \n    while k  N_max:\n        grad_norm_k = np.linalg.norm(grad_k)\n        if grad_norm_k = epsilon:\n            break\n        \n        # Descent direction test for the current direction d_k\n        grad_fk_dot_dk = np.dot(grad_k, d_k)\n        if grad_fk_dot_dk >= 0:\n            non_descent_occurred = True\n\n        # Perform line search to find an appropriate step length alpha_k\n        fk = f(x_k)\n        alpha_k = line_search(x_k, d_k, fk, grad_fk_dot_dk, params)\n        \n        # Update iterate\n        x_k_plus_1 = x_k + alpha_k * d_k\n        \n        # Compute gradient for the next iteration\n        grad_k_plus_1 = grad_f(x_k_plus_1)\n        \n        # Compute beta for the next direction\n        # Denominator for beta calculation\n        grad_k_norm_sq = np.dot(grad_k, grad_k)\n        \n        if grad_k_norm_sq  1e-16: # Safety for division by zero\n            beta_k_plus_1 = 0.0\n        else:\n            # Numerator for PRP beta\n            beta_prp_num = np.dot(grad_k_plus_1, grad_k_plus_1 - grad_k)\n            beta_prp = beta_prp_num / grad_k_norm_sq\n            \n            if method_type == 'prp':\n                beta_k_plus_1 = beta_prp\n            elif method_type == 'prp+':\n                beta_k_plus_1 = max(0.0, beta_prp)\n            else:\n                raise ValueError(\"Unknown method type\")\n\n        # Update direction for the next iteration\n        d_k_plus_1 = -grad_k_plus_1 + beta_k_plus_1 * d_k\n        \n        # Prepare for next iteration\n        x_k = x_k_plus_1\n        grad_k = grad_k_plus_1\n        d_k = d_k_plus_1\n        k += 1\n\n    final_f = f(x_k)\n    return final_f, non_descent_occurred\n\nsolve()\n```", "id": "2418475"}]}