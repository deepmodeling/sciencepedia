{"hands_on_practices": [{"introduction": "L-BFGS 算法的“记忆”由最近的若干步位移向量 $s_k$ 和梯度变化向量 $y_k$ 构成。这个练习将带你计算这些基本构件，它们是算法后续所有计算的原始数据，也是理解拟牛顿法如何捕捉函数曲率信息的关键第一步。通过这个基础计算，你将为深入理解算法的核心机制打下坚实的基础。[@problem_id:2184596]", "problem": "您正在分析有限内存Broyden–Fletcher–Goldfarb–Shanno (L-BFGS) 算法的行为，这是一种用于无约束优化的流行拟牛顿法。该算法通过存储最近的 $m$ 对向量 $(s_k, y_k)$ 来构建逆Hessian矩阵的近似。其中，$x_k$ 是第 $k$ 步的迭代点，$s_k = x_{k+1} - x_k$ 是位移向量，$y_k = \\nabla f(x_{k+1}) - \\nabla f(x_k)$ 是某个目标函数 $f(x)$ 的梯度向量的变化。\n\n考虑对二维凸二次函数 $f(x) = f(x_1, x_2) = (x_1 - 2)^2 + 3(x_2 + 1)^2$ 的优化。一个优化程序产生了以下三个迭代点（位置向量）的序列：\n$$\nx_0 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}, \\quad x_1 = \\begin{pmatrix} 1 \\\\ -2 \\end{pmatrix}, \\quad x_2 = \\begin{pmatrix} 2 \\\\ -1.5 \\end{pmatrix}\n$$\n计算L-BFGS算法会基于此迭代序列存储的两对历史向量 $(s_0, y_0)$ 和 $(s_1, y_1)$。\n\n将您的答案表示为一个 $2 \\times 4$ 矩阵，其中各列按特定顺序分别代表向量 $s_0$、$y_0$、$s_1$ 和 $y_1$。对任何非整数值，请使用分数表示。", "solution": "目标是计算当 $k=0$ 和 $k=1$ 时的位移向量 $s_k = x_{k+1} - x_k$ 和梯度差向量 $y_k = \\nabla f(x_{k+1}) - \\nabla f(x_k)$。\n\n首先，我们需要求出目标函数 $f(x_1, x_2) = (x_1 - 2)^2 + 3(x_2 + 1)^2$ 的梯度。其偏导数为：\n$$\n\\frac{\\partial f}{\\partial x_1} = 2(x_1 - 2)\n$$\n$$\n\\frac{\\partial f}{\\partial x_2} = 3 \\cdot 2(x_2 + 1) = 6(x_2 + 1)\n$$\n所以，梯度向量为：\n$$\n\\nabla f(x_1, x_2) = \\begin{pmatrix} 2(x_1 - 2) \\\\ 6(x_2 + 1) \\end{pmatrix}\n$$\n\n接下来，我们在给定的每个迭代点 $x_0$、$x_1$ 和 $x_2$ 处计算梯度。我们将这些梯度记为 $g_0$、$g_1$ 和 $g_2$。\n\n对于 $x_0 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$：\n$$\ng_0 = \\nabla f(0, 0) = \\begin{pmatrix} 2(0 - 2) \\\\ 6(0 + 1) \\end{pmatrix} = \\begin{pmatrix} -4 \\\\ 6 \\end{pmatrix}\n$$\n\n对于 $x_1 = \\begin{pmatrix} 1 \\\\ -2 \\end{pmatrix}$：\n$$\ng_1 = \\nabla f(1, -2) = \\begin{pmatrix} 2(1 - 2) \\\\ 6(-2 + 1) \\end{pmatrix} = \\begin{pmatrix} 2(-1) \\\\ 6(-1) \\end{pmatrix} = \\begin{pmatrix} -2 \\\\ -6 \\end{pmatrix}\n$$\n\n对于 $x_2 = \\begin{pmatrix} 2 \\\\ -1.5 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ -\\frac{3}{2} \\end{pmatrix}$：\n$$\ng_2 = \\nabla f(2, -1.5) = \\begin{pmatrix} 2(2 - 2) \\\\ 6(-1.5 + 1) \\end{pmatrix} = \\begin{pmatrix} 2(0) \\\\ 6(-0.5) \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ -3 \\end{pmatrix}\n$$\n\n现在我们可以计算位移向量 $s_0$ 和 $s_1$。\n\n当 $k=0$ 时：\n$$\ns_0 = x_1 - x_0 = \\begin{pmatrix} 1 \\\\ -2 \\end{pmatrix} - \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ -2 \\end{pmatrix}\n$$\n\n当 $k=1$ 时：\n$$\ns_1 = x_2 - x_1 = \\begin{pmatrix} 2 \\\\ -1.5 \\end{pmatrix} - \\begin{pmatrix} 1 \\\\ -2 \\end{pmatrix} = \\begin{pmatrix} 2 - 1 \\\\ -1.5 - (-2) \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 0.5 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ \\frac{1}{2} \\end{pmatrix}\n$$\n\n接下来，我们计算梯度差向量 $y_0$ 和 $y_1$。\n\n当 $k=0$ 时：\n$$\ny_0 = g_1 - g_0 = \\begin{pmatrix} -2 \\\\ -6 \\end{pmatrix} - \\begin{pmatrix} -4 \\\\ 6 \\end{pmatrix} = \\begin{pmatrix} -2 - (-4) \\\\ -6 - 6 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ -12 \\end{pmatrix}\n$$\n\n当 $k=1$ 时：\n$$\ny_1 = g_2 - g_1 = \\begin{pmatrix} 0 \\\\ -3 \\end{pmatrix} - \\begin{pmatrix} -2 \\\\ -6 \\end{pmatrix} = \\begin{pmatrix} 0 - (-2) \\\\ -3 - (-6) \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 3 \\end{pmatrix}\n$$\n\n最后，我们将结果组合成一个 $2 \\times 4$ 矩阵，其中各列分别为 $s_0, y_0, s_1, y_1$。\n$$\ns_0 = \\begin{pmatrix} 1 \\\\ -2 \\end{pmatrix}, \\quad y_0 = \\begin{pmatrix} 2 \\\\ -12 \\end{pmatrix}, \\quad s_1 = \\begin{pmatrix} 1 \\\\ \\frac{1}{2} \\end{pmatrix}, \\quad y_1 = \\begin{pmatrix} 2 \\\\ 3 \\end{pmatrix}\n$$\n最终的矩阵是：\n$$\n\\begin{pmatrix} 1  2  1  2 \\\\ -2  -12  \\frac{1}{2}  3 \\end{pmatrix}\n$$", "answer": "$$\n\\boxed{\\begin{pmatrix} 1  2  1  2 \\\\ -2  -12  \\frac{1}{2}  3 \\end{pmatrix}}\n$$", "id": "2184596"}, {"introduction": "在存储了历史信息对 $(s_k, y_k)$ 之后，L-BFGS 算法利用一种称为“双循环递归”的高效技巧来计算搜索方向，从而巧妙地避免了构造和存储庞大的逆 Hessian 矩阵。本练习将引导你逐步完成这一关键计算过程，亲身体验 L-BFGS 算法在内存效率上的精妙之处。这是掌握 L-BFGS 核心操作的必经之路。[@problem_id:2184578]", "problem": "有限内存Broyden–Fletcher–Goldfarb–Shanno (L-BFGS) 算法是一种用于无约束优化的常用拟牛顿法。在每次迭代 $k$ 中，该算法通过将逆Hessian矩阵的近似应用于当前梯度 $g_k = \\nabla f(x_k)$ 的负值来计算搜索方向 $p_k$。这个近似是使用最近 $m$ 步的有限历史记录隐式构建的。\n\n历史记录以向量对 $(s_i, y_i)$ 的形式存储，其中 $i=k-m, \\dots, k-1$，$s_i = x_{i+1} - x_i$ 是位置的变化量，$y_i = g_{i+1} - g_i$ 是梯度的变化量。然后通过一个称为 L-BFGS 双循环递归的过程来找到搜索方向 $p_k$。\n\n考虑在步骤 $k$ 进行 L-BFGS 更新，内存大小为 $m=2$。从先前步骤中获得的相关数据如下：\n- 当前梯度：$g_k = \\begin{pmatrix} 1 \\\\ -2 \\end{pmatrix}$\n- 步骤 $k-1$ 的历史记录：$s_{k-1} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$，$y_{k-1} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$\n- 步骤 $k-2$ 的历史记录：$s_{k-2} = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$，$y_{k-2} = \\begin{pmatrix} -1 \\\\ 2 \\end{pmatrix}$\n\n您的任务是计算本次迭代的搜索方向向量 $p_k$。请将您的答案表示为一个 $2 \\times 1$ 的列向量，其分量为精确的有理数。", "solution": "L-BFGS 搜索方向 $p_k$ 是通过近似计算乘积 $-H_k g_k$ 得到的，其中 $H_k$ 是逆Hessian矩阵的近似。这可以通过双循环递归算法高效地实现。我们已知 $m=2$，梯度 $g_k$，以及历史向量 $(s_{k-1}, y_{k-1})$ 和 $(s_{k-2}, y_{k-2})$。\n\n该算法如下：\n\n1.  用当前梯度初始化向量 $q$：\n    $q = g_k = \\begin{pmatrix} 1 \\\\ -2 \\end{pmatrix}$。\n\n2.  **第一个循环（反向传递）：** 此循环从 $i = k-1$ 向下迭代到 $i = k-m$。在我们的例子中，$i$ 从 $k-1$ 到 $k-2$。\n    我们首先预先计算标量 $\\rho_i = \\frac{1}{y_i^T s_i}$。\n    对于 $i = k-1$：\n    $y_{k-1}^T s_{k-1} = \\begin{pmatrix} 1  1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = (1)(1) + (1)(0) = 1$。\n    所以，$\\rho_{k-1} = \\frac{1}{1} = 1$。\n\n    对于 $i = k-2$：\n    $y_{k-2}^T s_{k-2} = \\begin{pmatrix} -1  2 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} = (-1)(0) + (2)(1) = 2$。\n    所以，$\\rho_{k-2} = \\frac{1}{2}$。\n\n    现在，我们执行循环更新。我们还将存储计算出的 $\\alpha_i$ 值，因为第二个循环需要它们。\n    -   **对于 $i = k-1$**：\n        $\\alpha_{k-1} = \\rho_{k-1} s_{k-1}^T q = (1) \\begin{pmatrix} 1  0 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ -2 \\end{pmatrix} = (1)((1)(1) + (0)(-2)) = 1$。\n        $q \\leftarrow q - \\alpha_{k-1} y_{k-1} = \\begin{pmatrix} 1 \\\\ -2 \\end{pmatrix} - (1) \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 1-1 \\\\ -2-1 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ -3 \\end{pmatrix}$。\n\n    -   **对于 $i = k-2$**：\n        $\\alpha_{k-2} = \\rho_{k-2} s_{k-2}^T q = \\left(\\frac{1}{2}\\right) \\begin{pmatrix} 0  1 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ -3 \\end{pmatrix} = \\left(\\frac{1}{2}\\right)((0)(0) + (1)(-3)) = -\\frac{3}{2}$。\n        $q \\leftarrow q - \\alpha_{k-2} y_{k-2} = \\begin{pmatrix} 0 \\\\ -3 \\end{pmatrix} - \\left(-\\frac{3}{2}\\right) \\begin{pmatrix} -1 \\\\ 2 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ -3 \\end{pmatrix} + \\begin{pmatrix} -\\frac{3}{2} \\\\ 3 \\end{pmatrix} = \\begin{pmatrix} 0 - \\frac{3}{2} \\\\ -3 + 3 \\end{pmatrix} = \\begin{pmatrix} -\\frac{3}{2} \\\\ 0 \\end{pmatrix}$。\n\n3.  **初始Hessian缩放：** 初始逆Hessian近似 $H_k^0$ 是一个对角矩阵 $\\gamma_k I$，其中 $\\gamma_k = \\frac{s_{k-1}^T y_{k-1}}{y_{k-1}^T y_{k-1}}$。我们通过将此缩放后的单位矩阵与当前 $q$ 相乘来初始化结果向量 $r$。\n    $s_{k-1}^T y_{k-1} = (1)(1) + (0)(1) = 1$。\n    $y_{k-1}^T y_{k-1} = (1)^2 + (1)^2 = 2$。\n    $\\gamma_k = \\frac{1}{2}$。\n    $r = \\gamma_k q = \\frac{1}{2} \\begin{pmatrix} -\\frac{3}{2} \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} -\\frac{3}{4} \\\\ 0 \\end{pmatrix}$。\n\n4.  **第二个循环（正向传递）：** 此循环从 $i = k-m$ 向上迭代到 $i = k-1$。在我们的例子中，$i$ 从 $k-2$ 到 $k-1$。\n    -   **对于 $i = k-2$**：\n        $\\beta = \\rho_{k-2} y_{k-2}^T r = \\left(\\frac{1}{2}\\right) \\begin{pmatrix} -1  2 \\end{pmatrix} \\begin{pmatrix} -\\frac{3}{4} \\\\ 0 \\end{pmatrix} = \\left(\\frac{1}{2}\\right)((-1)(-\\frac{3}{4}) + (2)(0)) = \\frac{3}{8}$。\n        $r \\leftarrow r + s_{k-2} (\\alpha_{k-2} - \\beta) = \\begin{pmatrix} -\\frac{3}{4} \\\\ 0 \\end{pmatrix} + \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} \\left(-\\frac{3}{2} - \\frac{3}{8}\\right) = \\begin{pmatrix} -\\frac{3}{4} \\\\ 0 \\end{pmatrix} + \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} \\left(-\\frac{12}{8} - \\frac{3}{8}\\right) = \\begin{pmatrix} -\\frac{3}{4} \\\\ 0 \\end{pmatrix} + \\begin{pmatrix} 0 \\\\ -\\frac{15}{8} \\end{pmatrix} = \\begin{pmatrix} -\\frac{6}{8} \\\\ -\\frac{15}{8} \\end{pmatrix}$。\n\n    -   **对于 $i = k-1$**：\n        $\\beta = \\rho_{k-1} y_{k-1}^T r = (1) \\begin{pmatrix} 1  1 \\end{pmatrix} \\begin{pmatrix} -\\frac{6}{8} \\\\ -\\frac{15}{8} \\end{pmatrix} = (1)(-\\frac{6}{8}) + (1)(-\\frac{15}{8}) = -\\frac{21}{8}$。\n        $r \\leftarrow r + s_{k-1} (\\alpha_{k-1} - \\beta) = \\begin{pmatrix} -\\frac{6}{8} \\\\ -\\frac{15}{8} \\end{pmatrix} + \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} \\left(1 - \\left(-\\frac{21}{8}\\right)\\right) = \\begin{pmatrix} -\\frac{6}{8} \\\\ -\\frac{15}{8} \\end{pmatrix} + \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} \\left(\\frac{8}{8} + \\frac{21}{8}\\right) = \\begin{pmatrix} -\\frac{6}{8} \\\\ -\\frac{15}{8} \\end{pmatrix} + \\begin{pmatrix} \\frac{29}{8} \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} \\frac{23}{8} \\\\ -\\frac{15}{8} \\end{pmatrix}$。\n\n5.  双循环递归的最终结果是向量 $r = H_k g_k$。搜索方向为 $p_k = -r$。\n    $p_k = - \\begin{pmatrix} \\frac{23}{8} \\\\ -\\frac{15}{8} \\end{pmatrix} = \\begin{pmatrix} -\\frac{23}{8} \\\\ \\frac{15}{8} \\end{pmatrix}$。", "answer": "$$\\boxed{\\begin{pmatrix} -\\frac{23}{8} \\\\ \\frac{15}{8} \\end{pmatrix}}$$", "id": "2184578"}, {"introduction": "掌握了算法的机械步骤后，理解保证其稳定性和有效性的内在条件也至关重要。本题是一个思想实验，旨在探讨当作为 BFGS 更新基石的“曲率条件” $y_k^T s_k \\gt 0$ 仅仅勉强满足时，会带来怎样的数值后果。通过分析这种临界情况，你可以将抽象的数学条件与算法的具体行为联系起来，深化对 L-BFGS 鲁棒性的理解。[@problem_id:2184583]", "problem": "在数值优化领域，限制内存的 Broyden–Fletcher–Goldfarb–Shanno (L-BFGS) 算法是一种流行的拟牛顿法，用于解决形式为 $\\min_{x \\in \\mathbb{R}^n} f(x)$ 的无约束优化问题。该算法迭代地构建 Hessian 矩阵逆的近似。在每次迭代 $k$ 中，从位置 $x_k$ 移动到 $x_{k+1}$，步长向量定义为 $s_k = x_{k+1} - x_k$。这导致目标函数梯度的变化量为 $y_k = \\nabla f(x_{k+1}) - \\nabla f(x_k)$。\n\n该算法的一个关键部分是“曲率条件” $y_k^T s_k > 0$，必须满足该条件以确保更新后的 Hessian 矩阵逆近似保持正定。这个条件通常由一个满足 Wolfe 条件的线搜索过程来强制执行。\n\n考虑一个场景，L-BFGS 算法正在最小化函数 $f(x)$，在某次迭代 $k$ 中，线搜索找到了一个步长 $s_k$，使得曲率条件仅被勉强满足。具体来说，标量积 $\\delta_k = y_k^T s_k$ 的值是一个非常小的正数（例如，数量级为 $10^{-20}$），但算法继续使用这个 $(s_k, y_k)$ 对进行更新。\n\n下列哪个陈述最准确地描述了这对后续 L-BFGS 迭代最直接和最重要的后果？\n\nA. 算法将断定已达到最小值并终止，因为梯度的微小变化意味着一个平坦区域。\nB. 更新将无法保持 Hessian 矩阵逆近似的正定性，可能导致下一步增加目标函数值。\nC. 新的 Hessian 矩阵逆近似将把函数建模为具有极低的曲率，导致下一步的搜索方向向量过长。\nD. Hessian 矩阵逆近似将变得近乎奇异，导致计算出的下一个搜索方向接近于零向量。\nE. 算法将自动减小其内存参数 $m$ 以提高数值稳定性。", "solution": "我们考虑 L-BFGS 使用的 Hessian 逆的 BFGS 更新。给定 $s_{k} = x_{k+1} - x_{k}$ 和 $y_{k} = \\nabla f(x_{k+1}) - \\nabla f(x_{k})$，定义 $\\delta_{k} = y_{k}^{T} s_{k}$。曲率条件要求 $\\delta_{k} > 0$。Hessian 逆的更新公式为\n$$\nH_{k+1} = \\left(I - \\rho_{k} s_{k} y_{k}^{T}\\right) H_{k} \\left(I - \\rho_{k} y_{k} s_{k}^{T}\\right) + \\rho_{k} s_{k} s_{k}^{T},\n$$\n其中 $\\rho_{k} = \\frac{1}{y_{k}^{T} s_{k}} = \\frac{1}{\\delta_{k}}$。\n\n如果 $\\delta_{k}$ 是一个非常小的正数，那么 $\\rho_{k}$ 就会非常大。检查这个更新公式：\n- 秩一项 $\\rho_{k} s_{k} s_{k}^{T}$ 贡献了一个大小与 $\\rho_{k}$ 成正比的分量，在 $s_{k}$ 方向上产生巨大的放大效应。\n- 乘法校正项 $(I - \\rho_{k} s_{k} y_{k}^{T})$ 和 $(I - \\rho_{k} y_{k} s_{k}^{T})$ 也包含 $\\rho_{k}$，因此可以显著改变 $H_{k}$，但关键的是，当 $\\delta_{k} > 0$ 时，BFGS 的构造能够保持正定性。\n\n下一次迭代的搜索方向是\n$p_{k+1} = - H_{k+1} \\nabla f(x_{k+1})$.\n因为 $H_{k+1}$ 包含大项 $\\rho_{k} s_{k} s_{k}^{T}$，$\\nabla f(x_{k+1})$ 沿着 $s_{k}$ 的任何非零分量都将在 $p_{k+1}$ 中被极大地放大。等价地，该更新强制满足割线条件 $H_{k+1} y_{k} = s_{k}$；当 $\\delta_{k}$ 极小时，这将一个小的曲率观测值映射为相关子空间上的一个大的有效逆曲率，从而导致一个非常大的步长。因此，该模型隐含地表示了极低的曲率，并产生一个过长的搜索方向。\n\n我们现在排除不正确的选项：\n- A 是不正确的，因为终止不是由一个小的 $\\delta_{k}$ 触发的；标准的停止准则基于范数（如 $|\\!| \\nabla f(x_{k}) |\\!|$）或步长，而不是直接基于 $y_{k}^{T} s_{k}$。\n- B 是不正确的，因为只要 $y_{k}^{T} s_{k} > 0$，BFGS 就会保持正定性。\n- D 是不正确的，因为大的 $\\rho_{k}$ 会使 $H_{k+1}$ 在某些方向上变得很大，而不是近乎奇异；其后果是产生大幅度的方向，而不是近乎零的方向。\n- E 是不正确的，因为 L-BFGS 不会自动减小其内存参数 $m$ 来应对小的 $\\delta_{k}$；至多，一些实现可能会跳过这个 $(s_k, y_k)$ 对，但这并非对 $m$ 的强制性自动缩减。\n\n因此，最直接和最重要的后果是，新的 Hessian 矩阵逆近似将函数建模为具有极低的曲率，并产生一个过长的搜索方向。", "answer": "$$\\boxed{C}$$", "id": "2184583"}]}