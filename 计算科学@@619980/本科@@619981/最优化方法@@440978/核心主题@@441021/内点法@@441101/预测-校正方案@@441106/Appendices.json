{"hands_on_practices": [{"introduction": "预测校正思想不仅能用于设计新算法，也能为我们理解现有算法提供一个新视角。例如，广泛应用的动量法就可以被看作一个预测校正过程：速度更新 $v_{k+1}=\\beta v_{k}-\\alpha \\nabla f(x_{k})$ 是“预测”步骤，而位置更新 $x_{k+1}=x_{k}+v_{k+1}$ 则是“校正”步骤。通过在一个简单二次函数上分析其“过冲”行为 ([@problem_id:3163746])，我们可以更深刻地理解学习率 $\\alpha$ 和动量系数 $\\beta$ 等超参数如何影响算法的收敛动态。", "problem": "考虑一个用于最小化光滑函数 $f(x)$ 的基于动量的预测-校正方案，该方案由预测步 $v_{k+1}=\\beta v_{k}-\\alpha \\nabla f(x_{k})$ 和校正步 $x_{k+1}=x_{k}+v_{k+1}$ 定义，其中 $\\alpha>0$ 是步长，$\\beta \\geq 0$ 是动量系数。假设目标函数为严格凸二次函数 $f(x)=\\frac{1}{2}\\lambda x^{2}$，其曲率参数为 $\\lambda>0$，因此唯一最小值点位于 $x^{\\star}=0$，梯度为 $\\nabla f(x)=\\lambda x$。从 $x_{0}>0$ 和 $v_{0}=0$ 开始。将校正步的过冲定义为迭代点穿过最小值点，即当 $x_{k}>0$ 时有 $x_{k+1}<0$。仅使用二次目标函数的基本性质和给定的更新规则，推导出最大步长 $\\alpha^{\\star}(\\beta,\\lambda)$，使得第一次校正 $x_{1}$ 和第二次校正 $x_{2}$ 都不会过冲（即 $x_{1}\\geq 0$ 且 $x_{2}\\geq 0$）。将最终答案表示为关于 $\\beta$ 和 $\\lambda$ 的闭式解析表达式。无需进行四舍五入。", "solution": "首先对问题进行验证，以确保其具有科学依据、是良定的且信息完整。\n\n### 步骤 1：提取已知条件\n-   **预测-校正方案**：一种用于最小化函数 $f(x)$ 的基于动量的方法。\n-   **预测步**：$v_{k+1}=\\beta v_{k}-\\alpha \\nabla f(x_{k})$。\n-   **校正步**：$x_{k+1}=x_{k}+v_{k+1}$。\n-   **步长**：$\\alpha>0$。\n-   **动量系数**：$\\beta \\geq 0$。\n-   **目标函数**：一个严格凸二次函数，$f(x)=\\frac{1}{2}\\lambda x^{2}$。\n-   **曲率参数**：$\\lambda>0$。\n-   **唯一最小值点**：$x^{\\star}=0$。\n-   **梯度**：$\\nabla f(x)=\\lambda x$。\n-   **初始条件**：$x_{0}>0$ 且 $v_{0}=0$。\n-   **过冲定义**：迭代点穿过最小值点，即当 $x_{k}>0$ 时有 $x_{k+1}<0$。\n-   **约束**：找到最大步长 $\\alpha^{\\star}(\\beta, \\lambda)$，使得第一次校正 ($x_1$) 和第二次校正 ($x_2$) 都不会过冲。这转化为条件 $x_{1}\\geq 0$ 和 $x_{2}\\geq 0$。\n\n### 步骤 2：使用提取的已知条件进行验证\n该问题描述了一种应用于简单强凸二次函数的基于动量的优化方法（重球法的一种变体）。这是数值优化领域中的一个标准和基础的分析。所有术语都定义明确，初始条件和约束也已清晰陈述。该问题是自洽的、数学上一致且客观的。它在科学上是合理的并且是良定的，要求找出满足一组推导出的不等式的最大参数值。\n\n### 步骤 3：结论与行动\n该问题是有效的。将推导出一个完整的解。\n\n### 解题推导\n\n给定的预测-校正方案的更新规则是：\n$$v_{k+1} = \\beta v_{k} - \\alpha \\nabla f(x_{k})$$\n$$x_{k+1} = x_{k} + v_{k+1}$$\n\n对于指定的目标函数 $f(x)=\\frac{1}{2}\\lambda x^{2}$，其梯度为 $\\nabla f(x) = \\lambda x$。将此代入预测步，得到：\n$$v_{k+1} = \\beta v_{k} - \\alpha \\lambda x_{k}$$\n\n我们可以将状态更新表示为一个耦合线性递推关系。我们将从初始条件 $x_0 > 0$ 和 $v_0 = 0$ 开始，计算前两次迭代的结果 $x_1$ 和 $x_2$。\n\n**第一次校正步 ($k=0$):**\n我们计算 $v_1$，然后计算 $x_1$。\n$$v_1 = \\beta v_0 - \\alpha \\lambda x_0 = \\beta(0) - \\alpha \\lambda x_0 = -\\alpha \\lambda x_0$$\n$$x_1 = x_0 + v_1 = x_0 - \\alpha \\lambda x_0 = (1 - \\alpha \\lambda) x_0$$\n\n第一个无过冲条件是 $x_1 \\geq 0$。由于给定 $x_0 > 0$，这要求：\n$$1 - \\alpha \\lambda \\geq 0$$\n$$\\alpha \\lambda \\leq 1$$\n由于 $\\lambda > 0$，这给出了我们对步长 $\\alpha$ 的第一个约束：\n$$\\alpha \\leq \\frac{1}{\\lambda}$$\n\n**第二次校正步 ($k=1$):**\n接下来，我们使用刚刚求得的 $x_1$ 和 $v_1$ 的值来计算 $v_2$ 和 $x_2$。\n$$v_2 = \\beta v_1 - \\alpha \\lambda x_1 = \\beta(-\\alpha \\lambda x_0) - \\alpha \\lambda ((1 - \\alpha \\lambda) x_0)$$\n$$v_2 = [-\\alpha \\lambda \\beta - \\alpha \\lambda (1 - \\alpha \\lambda)] x_0$$\n$$v_2 = [-\\alpha \\lambda \\beta - \\alpha \\lambda + (\\alpha \\lambda)^2] x_0$$\n$$v_2 = [(\\alpha \\lambda)^2 - \\alpha \\lambda (\\beta+1)] x_0$$\n\n现在，我们计算 $x_2$：\n$$x_2 = x_1 + v_2 = (1 - \\alpha \\lambda) x_0 + [(\\alpha \\lambda)^2 - \\alpha \\lambda (\\beta+1)] x_0$$\n$$x_2 = [1 - \\alpha \\lambda + (\\alpha \\lambda)^2 - \\alpha \\lambda \\beta - \\alpha \\lambda] x_0$$\n$$x_2 = [(\\alpha \\lambda)^2 - (2+\\beta) \\alpha \\lambda + 1] x_0$$\n\n第二个无过冲条件是 $x_2 \\geq 0$。由于 $x_0 > 0$，这要求关于 $\\alpha\\lambda$ 的多项式为非负：\n$$(\\alpha \\lambda)^2 - (2+\\beta) \\alpha \\lambda + 1 \\geq 0$$\n\n让我们通过定义变量 $u = \\alpha \\lambda$ 来分析这个二次不等式。不等式变为：\n$$u^2 - (2+\\beta) u + 1 \\geq 0$$\n\n这是一个关于 $u$ 的二次函数，代表一个开口向上的抛物线。当 $u$ 在相应方程 $u^2 - (2+\\beta) u + 1 = 0$ 的根之外时，该不等式成立。我们使用求根公式来找到根：\n$$u = \\frac{-(-(2+\\beta)) \\pm \\sqrt{(-(2+\\beta))^2 - 4(1)(1)}}{2(1)}$$\n$$u = \\frac{2+\\beta \\pm \\sqrt{(2+\\beta)^2 - 4}}{2}$$\n$$u = \\frac{2+\\beta \\pm \\sqrt{4 + 4\\beta + \\beta^2 - 4}}{2}$$\n$$u = \\frac{2+\\beta \\pm \\sqrt{\\beta^2 + 4\\beta}}{2}$$\n\n设两个根为 $u_1$ 和 $u_2$，且 $u_1 \\leq u_2$：\n$$u_1 = \\frac{2+\\beta - \\sqrt{\\beta^2 + 4\\beta}}{2}$$\n$$u_2 = \\frac{2+\\beta + \\sqrt{\\beta^2 + 4\\beta}}{2}$$\n不等式 $u^2 - (2+\\beta) u + 1 \\geq 0$ 在 $u \\leq u_1$ 或 $u \\geq u_2$ 时成立。\n\n**合并约束条件：**\n我们需要找到满足两个无过冲条件的最大 $\\alpha > 0$。用 $u = \\alpha \\lambda$ 表示：\n1.  从 $x_1 \\geq 0$ 得：$u \\leq 1$。\n2.  从 $x_2 \\geq 0$ 得：$u \\leq u_1$ 或 $u \\geq u_2$。\n\n我们需要同时满足这两个条件。让我们比较一下边界。\n首先，我们检查是否 $u_1 \\leq 1$：\n$$\\frac{2+\\beta - \\sqrt{\\beta^2 + 4\\beta}}{2} \\leq 1$$\n$$2+\\beta - \\sqrt{\\beta^2 + 4\\beta} \\leq 2$$\n$$\\beta \\leq \\sqrt{\\beta^2 + 4\\beta}$$\n由于 $\\beta \\geq 0$，两边都是非负的，所以我们可以对两边进行平方：\n$$\\beta^2 \\leq \\beta^2 + 4\\beta$$\n$$0 \\leq 4\\beta$$\n这对所有 $\\beta \\geq 0$ 都成立。因此，$u_1 \\leq 1$。\n\n其次，我们检查是否 $u_2 \\geq 1$：\n$$\\frac{2+\\beta + \\sqrt{\\beta^2 + 4\\beta}}{2} \\geq 1$$\n$$2+\\beta + \\sqrt{\\beta^2 + 4\\beta} \\geq 2$$\n$$\\beta + \\sqrt{\\beta^2 + 4\\beta} \\geq 0$$\n这对所有 $\\beta \\geq 0$ 也都成立，因为两项都是非负的。因此，$u_2 \\geq 1$。\n\n我们要寻找最大的 $u$，使得 ($u \\leq 1$) 且 ($u \\leq u_1$ 或 $u \\geq u_2$)。\n由于 $u_1 \\leq 1 \\leq u_2$，这些条件的交集是 $u \\leq u_1$。区域 $u \\geq u_2$ 与 $u \\leq 1$ 不相容（除了当 $\\beta=0$ 时 $u_2=1$ 时的单点 $u=1$）。\n对于任何 $\\beta \\geq 0$，合并后的条件简化为：\n$$u \\leq u_1$$\n\n为了找到最大允许步长 $\\alpha^{\\star}$，我们取 $u = \\alpha \\lambda$ 的最大可能值，即 $u_1$：\n$$\\alpha^{\\star} \\lambda = u_1 = \\frac{2+\\beta - \\sqrt{\\beta^2 + 4\\beta}}{2}$$\n\n解出 $\\alpha^{\\star}$，得到最终表达式：\n$$\\alpha^{\\star}(\\beta, \\lambda) = \\frac{2+\\beta - \\sqrt{\\beta^2 + 4\\beta}}{2\\lambda}$$\n这可以写成：\n$$\\alpha^{\\star}(\\beta, \\lambda) = \\frac{2+\\beta - \\sqrt{\\beta(\\beta+4)}}{2\\lambda}$$\n这是保证第一次和第二次校正步都不会越过最小值点 $x^{\\star}=0$ 的最大步长。", "answer": "$$\n\\boxed{\\frac{2+\\beta - \\sqrt{\\beta(\\beta+4)}}{2\\lambda}}\n$$", "id": "3163746"}, {"introduction": "掌握了基本结构后，我们来探讨预测校正格式在处理约束优化问题中的应用。罚函数法提供了一个非常自然的框架：首先，“预测”一个忽略约束的解；然后，通过一个惩罚项进行“校正”，将解“拉回”到可行域附近。这个练习 ([@problem_id:3163753]) 通过一个简洁的一维问题，让学生可以从解析层面推导校正的强度与惩罚参数 $\\rho$ 之间的关系，从而直观地理解校正过程的本质。", "problem": "考虑一个一维约束优化问题，其目标函数为 $f(x) = \\frac{1}{2}(x - 1)^{2}$，非线性不等式约束为 $g(x) = x - \\frac{1}{2} \\leq 0$。采用如下的预测-校正方案：预测步通过求解无约束问题（忽略任何非绑定约束）获得，校正步通过最小化罚函数目标 $F_{\\rho}(x) = f(x) + \\rho \\,\\max\\!\\big(0, g(x)\\big)^{2}$ 获得，其中 $\\rho > 0$ 是罚参数。\n\n从无约束最小化的基本定义（目标函数的一阶平稳性）和罚函数的定义（其中 max 算子定义了一个分段光滑的校正）出发，推导由罚函数目标产生的校正后极小值点 $x^{c}(\\rho)$。将校正强度 $I(\\rho)$ 定义为从预测值到校正值的绝对位移，即 $I(\\rho) = \\big|x^{p} - x^{c}(\\rho)\\big|$，其中 $x^{p}$ 表示预测值。请给出 $I(\\rho)$ 作为 $\\rho$ 的函数的闭式解析表达式。\n\n你的最终答案必须是 $I(\\rho)$ 的单个解析表达式，且不得包含任何单位或额外评论。", "solution": "该问题要求推导应用于一个一维约束优化问题的特定预测-校正方案的校正强度 $I(\\rho)$。\n\n目标函数为 $f(x) = \\frac{1}{2}(x - 1)^{2}$，不等式约束为 $g(x) = x - \\frac{1}{2} \\leq 0$。\n\n首先，我们确定预测解 $x^{p}$。问题陈述，预测步是通过求解无约束问题获得的。这意味着我们必须找到使 $f(x)$ 最小化的 $x$ 值，而不考虑约束 $g(x) \\leq 0$。\n\n函数 $f(x)$ 是一个严格凸二次函数。其最小值可以通过找到其一阶导数为零的点来求得。\n$$f'(x) = \\frac{d}{dx} \\left[ \\frac{1}{2}(x - 1)^{2} \\right] = x - 1$$\n将导数设为零，得到：\n$$x - 1 = 0 \\implies x = 1$$\n二阶导数为 $f''(x) = 1 > 0$，这证实了 $x=1$ 是一个极小值点。\n因此，预测解为 $x^{p} = 1$。\n我们可以检查这个预测值是否满足约束：$g(x^p) = g(1) = 1 - \\frac{1}{2} = \\frac{1}{2}$。由于 $\\frac{1}{2} \\not\\leq 0$，预测值违反了约束。\n\n接下来，我们确定校正解 $x^{c}(\\rho)$。这是通过最小化罚函数 $F_{\\rho}(x)$ 获得的，其定义为：\n$$F_{\\rho}(x) = f(x) + \\rho \\,\\max\\!\\big(0, g(x)\\big)^{2}$$\n代入 $f(x)$ 和 $g(x)$ 的表达式：\n$$F_{\\rho}(x) = \\frac{1}{2}(x - 1)^{2} + \\rho \\,\\max\\!\\big(0, x - \\frac{1}{2}\\big)^{2}$$\n$\\max$ 算子的存在使其成为一个分段函数。我们根据 $g(x) = x - \\frac{1}{2}$ 的符号来分析它。\n\n情况 1：$x \\leq \\frac{1}{2}$。\n在这个区域，$x - \\frac{1}{2} \\leq 0$，因此 $\\max(0, x - \\frac{1}{2}) = 0$。罚函数简化为：\n$$F_{\\rho}(x) = \\frac{1}{2}(x - 1)^{2} \\quad \\text{对于 } x \\leq \\frac{1}{2}$$\n我们需要找到该函数在区间 $(-\\infty, \\frac{1}{2}]$ 上的最小值。无约束极小值点在 $x=1$处，该点在此区间之外。由于函数 $f(x)$ 在 $x  1$ 时是严格递减的，因此它在区间 $(-\\infty, \\frac{1}{2}]$ 上的最小值出现在边界点 $x = \\frac{1}{2}$ 处。\n\n情况 2：$x  \\frac{1}{2}$。\n在这个区域，$x - \\frac{1}{2}  0$，因此 $\\max(0, x - \\frac{1}{2}) = x - \\frac{1}{2}$。罚函数为：\n$$F_{\\rho}(x) = \\frac{1}{2}(x - 1)^{2} + \\rho \\left(x - \\frac{1}{2}\\right)^{2} \\quad \\text{对于 } x  \\frac{1}{2}$$\n这是一个可微函数。为了找到其最小值，我们计算其一阶导数并将其设为零。\n$$F'_{\\rho}(x) = \\frac{d}{dx} \\left[ \\frac{1}{2}(x - 1)^{2} + \\rho \\left(x - \\frac{1}{2}\\right)^{2} \\right]$$\n$$F'_{\\rho}(x) = (x - 1) + 2\\rho \\left(x - \\frac{1}{2}\\right) = x - 1 + 2\\rho x - \\rho$$\n$$F'_{\\rho}(x) = (1 + 2\\rho)x - (1 + \\rho)$$\n将导数设为零以找到驻点：\n$$(1 + 2\\rho)x - (1 + \\rho) = 0$$\n$$(1 + 2\\rho)x = 1 + \\rho$$\n$$x = \\frac{1 + \\rho}{1 + 2\\rho}$$\n我们必须验证这个驻点位于本情况的区域内，即 $x  \\frac{1}{2}$。我们检验该不等式：\n$$\\frac{1 + \\rho}{1 + 2\\rho}  \\frac{1}{2}$$\n由于 $\\rho  0$，分母 $1 + 2\\rho$ 是正数，所以我们可以在不等式两边同乘以 $2(1+2\\rho)$ 而不改变不等号方向：\n$$2(1 + \\rho)  1(1 + 2\\rho)$$\n$$2 + 2\\rho  1 + 2\\rho$$\n$$2  1$$\n这个不等式对任何 $\\rho$ 都成立。因此，该驻点总是在区域 $x  \\frac{1}{2}$ 内。二阶导数为 $F''_{\\rho}(x) = 1 + 2\\rho$，对于 $\\rho  0$ 恒为正，证实了这是一个极小值点。\n\n函数 $F_{\\rho}(x)$ 在 $\\mathbb{R}$ 上是连续且可微的。在边界 $x = \\frac{1}{2}$ 处，左右导数相等。因此，$F_{\\rho}(x)$ 的全局最小值是其唯一的驻点。该极小值点即为校正解：\n$$x^{c}(\\rho) = \\frac{1 + \\rho}{1 + 2\\rho}$$\n\n最后，我们计算校正强度 $I(\\rho)$，它被定义为从预测值到校正值的绝对位移：\n$$I(\\rho) = |x^{p} - x^{c}(\\rho)|$$\n代入 $x^p$ 和 $x^c(\\rho)$ 的表达式：\n$$I(\\rho) = \\left| 1 - \\frac{1 + \\rho}{1 + 2\\rho} \\right|$$\n为了简化绝对值内的表达式，我们通分：\n$$I(\\rho) = \\left| \\frac{1 + 2\\rho}{1 + 2\\rho} - \\frac{1 + \\rho}{1 + 2\\rho} \\right|$$\n$$I(\\rho) = \\left| \\frac{(1 + 2\\rho) - (1 + \\rho)}{1 + 2\\rho} \\right|$$\n$$I(\\rho) = \\left| \\frac{1 + 2\\rho - 1 - \\rho}{1 + 2\\rho} \\right|$$\n$$I(\\rho) = \\left| \\frac{\\rho}{1 + 2\\rho} \\right|$$\n由于问题指定 $\\rho  0$，分子 $\\rho$ 和分母 $1 + 2\\rho$ 都严格为正。因此，绝对值是多余的。\n$$I(\\rho) = \\frac{\\rho}{1 + 2\\rho}$$\n这就是校正强度作为罚参数 $\\rho$ 的函数的闭式解析表达式。", "answer": "$$\n\\boxed{\\frac{\\rho}{1 + 2\\rho}}\n$$", "id": "3163753"}, {"introduction": "从解析练习走向动手实践。当面临如概率单纯形这类复杂约束时，“校正”步骤的设计本身就成为一个核心挑战。这个问题 ([@problem_id:3163727]) 要求学生动手实现并比较两种截然不同的校正策略：一种基于欧几里得几何的投影校正，另一种则基于信息几何的熵正则化校正。通过这项实践，你将深刻体会到为校正步骤选择不同的几何结构会如何根本性地影响算法的行为与性能。", "problem": "考虑在概率单纯形上最小化一个光滑凸函数。设单纯形定义为 $\\Delta = \\{x \\in \\mathbb{R}^n \\mid \\sum_{i=1}^n x_i = 1, \\; x_i \\ge 0\\}$。设目标函数为 $f(x) = \\frac{1}{2} x^\\top Q x + c^\\top x$，其中 $Q \\in \\mathbb{R}^{n \\times n}$ 是对称半正定矩阵，而 $c \\in \\mathbb{R}^n$。梯度为 $\\nabla f(x) = Q x + c$。从投影梯度法和使用熵邻近函数的镜像下降法的基本定义出发，为此约束最小化问题构建一个预测-校正方案的两种变体：\n\n- 变体 A (欧几里得投影): 使用梯度步长进行预测，并通过投影到定义 $\\Delta$ 的仿射和不等式约束上来进行校正。\n- 变体 B (熵校正): 使用梯度步长进行预测，并使用熵邻近度（Kullback-Leibler 散度）进行校正，以保持在 $\\Delta$ 上。\n\n你的任务是：\n- 基于无约束梯度下降原理，构建预测步骤。\n- 使用必要的最优性条件，从第一性原理推导到 $\\Delta$ 上的欧几里得投影校正器。\n- 通过最小化由 Kullback-Leibler 散度正则化的线性化目标，从第一性原理推导熵校正更新。\n- 将两种变体实现为具有固定步长 $\\alpha$ 和固定迭代次数 $T$ 的迭代方案。\n- 对于每个测试用例，从 $\\Delta$ 中的同一起始点运行两种变体 $T$ 步，并比较它们的最终目标值。\n\n最终程序必须：\n- 根据推导忠实地实现两种方案。\n- 对于每个测试用例，计算一个布尔值输出，指示变体 B 是否取得了比变体 A 严格更低的最终目标值。\n\n使用以下测试套件，其中每个案例指定 $(n, Q, c, \\alpha, T, x^{(0)})$：\n1. 正常路径：$n=3$, $Q=\\begin{bmatrix}2  -1  0 \\\\ -1  2  -1 \\\\ 0  -1  2\\end{bmatrix}$, $c=\\begin{bmatrix}0.5 \\\\ -0.2 \\\\ 0.1\\end{bmatrix}$, $\\alpha=0.2$, $T=200$, $x^{(0)}=\\begin{bmatrix}\\frac{1}{3} \\\\ \\frac{1}{3} \\\\ \\frac{1}{3}\\end{bmatrix}$。\n2. 边界起始：$n=2$, $Q=\\begin{bmatrix}1  0.2 \\\\ 0.2  1\\end{bmatrix}$, $c=\\begin{bmatrix}0.0 \\\\ 0.4\\end{bmatrix}$, $\\alpha=0.5$, $T=100$, $x^{(0)}=\\begin{bmatrix}1.0 \\\\ 0.0\\end{bmatrix}$。\n3. 大步长：$n=4$, $Q=\\operatorname{diag}(1.5, 2.0, 0.7, 0.3)$, $c=\\begin{bmatrix}-0.3 \\\\ 0.1 \\\\ 0.0 \\\\ 0.2\\end{bmatrix}$, $\\alpha=1.0$, $T=60$, $x^{(0)}=\\begin{bmatrix}0.25 \\\\ 0.25 \\\\ 0.25 \\\\ 0.25\\end{bmatrix}$。\n4. 平坦二次型（线性目标）：$n=3$, $Q=\\begin{bmatrix}0  0  0 \\\\ 0  0  0 \\\\ 0  0  0\\end{bmatrix}$, $c=\\begin{bmatrix}0.1 \\\\ -0.05 \\\\ 0.0\\end{bmatrix}$, $\\alpha=0.3$, $T=100$, $x^{(0)}=\\begin{bmatrix}\\frac{1}{3} \\\\ \\frac{1}{3} \\\\ \\frac{1}{3}\\end{bmatrix}$。\n5. 更高维度：$n=5$, $Q=\\operatorname{diag}(0.5, 1.0, 1.5, 0.8, 1.2)$, $c=\\begin{bmatrix}0.2 \\\\ -0.1 \\\\ 0.05 \\\\ 0.0 \\\\ -0.2\\end{bmatrix}$, $\\alpha=0.15$, $T=150$, $x^{(0)}=\\begin{bmatrix}0.5 \\\\ 0.2 \\\\ 0.1 \\\\ 0.1 \\\\ 0.1\\end{bmatrix}$。\n\n你的程序应生成一行输出，其中包含一个用方括号括起来的逗号分隔列表的结果（例如，$[result_1,result_2,result_3,result_4,result_5]$），其中每个 $result_i$ 是测试用例 $i$ 的布尔值，指示变体 B 是否严格优于变体 A（即，其最终目标值是否严格更低）。", "solution": "该问题要求制定、实现和比较两种预测-校正方案，用于在概率单纯形 $\\Delta = \\{x \\in \\mathbb{R}^n \\mid \\sum_{i=1}^n x_i = 1, \\; x_i \\ge 0\\}$ 上最小化一个光滑凸二次函数 $f(x) = \\frac{1}{2} x^\\top Q x + c^\\top x$。矩阵 $Q$ 是对称半正定的，这确保了 $f(x)$ 的凸性。\n\n用于约束优化的预测-校正方法的通用迭代结构是：\n1.  **预测步骤**：从当前迭代点 $x^{(k)}$ 出发，基于目标函数的一些局部信息（通常忽略约束）走一步。这将产生一个候选点 $y^{(k+1)}$。\n2.  **校正步骤**：调整候选点 $y^{(k+1)}$ 以产生满足问题约束的下一个迭代点 $x^{(k+1)}$。\n\n我们将推导此方案的两种变体，它们的不同之处在于其校正步骤。预测步骤对两者是共同的，并且基于标准的梯度下降更新。\n\n**预测步骤**\n\n给定当前迭代点 $x^{(k)} \\in \\Delta$，预测步骤执行一个无约束的梯度下降更新。目标函数的梯度为 $\\nabla f(x) = Qx + c$。预测步骤以固定步长 $\\alpha  0$ 沿负梯度方向移动：\n$$\ny^{(k+1)} = x^{(k)} - \\alpha \\nabla f(x^{(k)})\n$$\n点 $y^{(k+1)}$ 通常在单纯形 $\\Delta$ 之外，因为它不一定满足 $\\sum_{i=1}^n y_i^{(k+1)} = 1$ 或 $y_i^{(k+1)} \\ge 0$。\n\n**变体 A：通过欧几里得投影进行校正**\n\n这个变体，被称为投影梯度下降法，通过找到预测点 $y^{(k+1)}$ 在 $\\Delta$ 中关于欧几里得距离最近的点来校正它。此校正是以下二次规划子问题的解：\n$$\nx^{(k+1)} = \\operatorname{proj}_{\\Delta}(y^{(k+1)}) = \\arg\\min_{z \\in \\Delta} \\frac{1}{2} \\|z - y^{(k+1)}\\|_2^2\n$$\n为了解决这个问题，我们为具有约束 $z \\ge 0$ 和 $\\mathbf{1}^\\top z = 1$ 的最小化问题构建拉格朗日函数：\n$$\nL(z, \\lambda, \\nu) = \\frac{1}{2} \\sum_{i=1}^n (z_i - y_i^{(k+1)})^2 - \\sum_{i=1}^n \\lambda_i z_i + \\nu \\left(\\sum_{i=1}^n z_i - 1\\right)\n$$\n其中 $\\lambda_i \\ge 0$ 是非负约束的拉格朗日乘子，$\\nu$ 是等式约束的乘子。卡罗需-库恩-塔克（KKT）最优性条件是：\n1.  **平稳性**：$\\frac{\\partial L}{\\partial z_i} = z_i - y_i^{(k+1)} - \\lambda_i + \\nu = 0$，对于 $i=1, \\dots, n$。\n2.  **原始可行性**：$\\sum_{i=1}^n z_i = 1$ 且 $z_i \\ge 0$。\n3.  **对偶可行性**：$\\lambda_i \\ge 0$。\n4.  **互补松弛性**：$\\lambda_i z_i = 0$。\n\n根据平稳性条件，$z_i = y_i^{(k+1)} + \\lambda_i - \\nu$。根据互补松弛性，如果 $z_i  0$，则 $\\lambda_i = 0$，这意味着 $z_i = y_i^{(k+1)} - \\nu$。如果 $z_i = 0$，则 $\\lambda_i \\ge 0$。根据平稳性，$\\lambda_i = \\nu - y_i^{(k+1)} \\ge 0$，这意味着 $y_i^{(k+1)} \\le \\nu$。结合这些观察， $z_i$ 的解可以用单个乘子 $\\nu$ 紧凑地表示为：\n$$\nz_i = \\max(0, y_i^{(k+1)} - \\nu)\n$$\n$\\nu$ 的值由等式约束 $\\sum_{i=1}^n z_i = 1$ 确定：\n$$\n\\sum_{i=1}^n \\max(0, y_i^{(k+1)} - \\nu) = 1\n$$\n函数 $g(\\nu) = \\sum_{i=1}^n \\max(0, y_i^{(k+1)} - \\nu)$ 是一个关于 $\\nu$ 的连续、分段线性和非增函数。求解 $g(\\nu)=1$ 可以高效完成。一个标准算法包括将 $y^{(k+1)}$ 的分量按降序排序，即 $y_{(1)} \\ge y_{(2)} \\ge \\dots \\ge y_{(n)}$。然后找到一个索引 $\\rho \\in \\{1, \\dots, n\\}$，使得最优的 $\\nu$ 位于区间 $[y_{(\\rho+1)}, y_{(\\rho)}]$ 内。这会导出一个基于排序后向量的前 $\\rho$ 个分量的 $\\nu$ 的闭式解。一旦找到 $\\nu$，就可以计算出投影 $x^{(k+1)}$。\n\n**变体 B：通过熵正则化进行校正**\n\n这个变体是镜像下降算法的一个实例，它将预测和校正步骤合并为一个单一的更新。它通过最小化在 $x^{(k)}$ 处的函数的线性近似，并由 Kullback-Leibler (KL) 散度进行正则化，来找到下一个迭代点 $x^{(k+1)}$。KL 散度在单纯形上作为一种邻近度度量（Bregman 散度）。KL 散度定义为 $D_{KL}(z \\| x) = \\sum_{i=1}^n z_i \\log\\left(\\frac{z_i}{x_i^{(k)}}\\right)$。更新规则由下式给出：\n$$\nx^{(k+1)} = \\arg\\min_{z \\in \\Delta} \\left\\{ \\langle \\nabla f(x^{(k)}), z \\rangle + \\frac{1}{\\alpha} D_{KL}(z \\| x^{(k)}) \\right\\}\n$$\n该子问题的目标可以写为：\n$$\nJ(z) = \\alpha \\sum_{i=1}^n (\\nabla_i f(x^{(k)})) z_i + \\sum_{i=1}^n z_i \\log z_i - \\sum_{i=1}^n z_i \\log x_i^{(k)}\n$$\n我们在约束 $\\sum_{i=1}^n z_i = 1$ 和 $z_i \\ge 0$ 下最小化 $J(z)$。假设搜索空间是单纯形的内部（为了可微性），我们建立拉格朗日函数：\n$$\nL(z, \\mu) = J(z) + \\mu \\left(\\sum_{i=1}^n z_i - 1\\right)\n$$\n对 $z_i$求导并令其为零，得到：\n$$\n\\frac{\\partial L}{\\partial z_i} = \\alpha \\nabla_i f(x^{(k)}) + (\\log z_i + 1) - \\log x_i^{(k)} + \\mu = 0\n$$\n求解 $z_i$：\n$$\n\\log z_i = \\log x_i^{(k)} - \\alpha \\nabla_i f(x^{(k)}) - \\mu - 1\n$$\n$$\nz_i = \\exp(\\log x_i^{(k)} - \\alpha \\nabla_i f(x^{(k)}) - \\mu - 1) = x_i^{(k)} \\exp(-\\alpha \\nabla_i f(x^{(k)})) \\exp(-\\mu - 1)\n$$\n项 $\\exp(-\\mu - 1)$ 是一个归一化常数，我们称之为 $C$。所以，$z_i = C \\cdot x_i^{(k)} \\exp(-\\alpha \\nabla_i f(x^{(k)}))$。我们使用约束 $\\sum_i z_i = 1$ 来找到 $C$：\n$$\n1 = \\sum_{i=1}^n z_i = C \\sum_{i=1}^n x_i^{(k)} \\exp(-\\alpha \\nabla_i f(x^{(k)}))\n$$\n这给出了 $x^{(k+1)} = z$ 的更新规则：\n$$\nx_i^{(k+1)} = \\frac{x_i^{(k)} \\exp(-\\alpha \\nabla_i f(x^{(k)}))}{\\sum_{j=1}^n x_j^{(k)} \\exp(-\\alpha \\nabla_j f(x^{(k)}))}\n$$\n这就是指数化梯度更新。它巧妙地确保了如果 $x^{(k)} \\in \\Delta$，那么 $x^{(k+1)}$ 会自动属于 $\\Delta$（所有分量都是非负的并且它们的和为 $1$）。如果某个分量 $x_i^{(k)}$ 为 $0$，它将在所有后续迭代中保持为 $0$。\n\n**总结与比较**\n\n-   **变体 A (投影梯度下降)**：在每一步中，它执行一个标准的欧几里得梯度步骤，然后将结果投影回单纯形上。投影操作可能计算量很大，但能确保迭代点是可行集中距离无约束更新点最近的点。\n-   **变体 B (镜像下降/指数化梯度)**：它使用由熵函数导出的非欧几里得几何。更新是乘法形式的，并且比欧几里得投影计算更简单。它内在地尊重单纯形的边界，并且特别适用于概率分布。\n\n实现将执行两种算法固定的迭代次数 $T$，并比较最终的目标函数值，以确定在每个测试用例中哪个变体表现更好。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the test cases and print the results.\n    \"\"\"\n    test_cases = [\n        # Case 1: Happy path\n        {'n': 3, 'Q': np.array([[2, -1, 0], [-1, 2, -1], [0, -1, 2]]), \n         'c': np.array([0.5, -0.2, 0.1]), 'alpha': 0.2, 'T': 200, \n         'x0': np.array([1/3, 1/3, 1/3])},\n        # Case 2: Boundary start\n        {'n': 2, 'Q': np.array([[1, 0.2], [0.2, 1]]), \n         'c': np.array([0.0, 0.4]), 'alpha': 0.5, 'T': 100, \n         'x0': np.array([1.0, 0.0])},\n        # Case 3: Large step size\n        {'n': 4, 'Q': np.diag([1.5, 2.0, 0.7, 0.3]), \n         'c': np.array([-0.3, 0.1, 0.0, 0.2]), 'alpha': 1.0, 'T': 60, \n         'x0': np.array([0.25, 0.25, 0.25, 0.25])},\n        # Case 4: Flat quadratic (linear objective)\n        {'n': 3, 'Q': np.zeros((3, 3)), \n         'c': np.array([0.1, -0.05, 0.0]), 'alpha': 0.3, 'T': 100, \n         'x0': np.array([1/3, 1/3, 1/3])},\n        # Case 5: Higher dimension\n        {'n': 5, 'Q': np.diag([0.5, 1.0, 1.5, 0.8, 1.2]), \n         'c': np.array([0.2, -0.1, 0.05, 0.0, -0.2]), 'alpha': 0.15, 'T': 150, \n         'x0': np.array([0.5, 0.2, 0.1, 0.1, 0.1])}\n    ]\n\n    results = []\n    for case in test_cases:\n        Q, c, alpha, T, x0 = case['Q'], case['c'], case['alpha'], case['T'], case['x0']\n\n        # Run Variant A\n        x_final_A = run_variant_A(Q, c, alpha, T, x0)\n        \n        # Run Variant B\n        x_final_B = run_variant_B(Q, c, alpha, T, x0)\n\n        # Calculate final objective values\n        f_A = objective_function(x_final_A, Q, c)\n        f_B = objective_function(x_final_B, Q, c)\n        \n        # Compare and store the boolean result\n        results.append(f_B  f_A)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, [r.item() for r in results]))}]\")\n\ndef objective_function(x, Q, c):\n    \"\"\"Computes the objective function f(x) = 0.5 * x.T @ Q @ x + c.T @ x.\"\"\"\n    return 0.5 * x.T @ Q @ x + c.T @ x\n\ndef gradient(x, Q, c):\n    \"\"\"Computes the gradient of the objective function.\"\"\"\n    return Q @ x + c\n\ndef project_onto_simplex(y):\n    \"\"\"Projects a vector y onto the probability simplex.\"\"\"\n    n = len(y)\n    y_sorted = np.sort(y)[::-1]\n    y_cumsum = np.cumsum(y_sorted)\n    \n    # Find rho: the number of positive components in the projection\n    candidates = y_sorted - (y_cumsum - 1) / np.arange(1, n + 1)\n    rho_candidates = np.where(candidates > 0)[0]\n    \n    if len(rho_candidates) == 0:\n        # This case implies all y_i are very small, and the projection will be\n        # a standard basis vector. The logic below correctly handles this\n        # by setting rho=1. For clarity and robustness, we can ensure rho >= 1.\n        rho = 1\n    else:    \n        rho = rho_candidates[-1] + 1\n    \n    theta = (y_cumsum[rho - 1] - 1) / rho\n    \n    x_proj = np.maximum(0, y - theta)\n    return x_proj\n\ndef run_variant_A(Q, c, alpha, T, x0):\n    \"\"\"Implements Variant A: Predictor-corrector with Euclidean projection.\"\"\"\n    x = np.copy(x0)\n    for _ in range(T):\n        grad = gradient(x, Q, c)\n        y = x - alpha * grad\n        x = project_onto_simplex(y)\n    return x\n\ndef run_variant_B(Q, c, alpha, T, x0):\n    \"\"\"Implements Variant B: Predictor-corrector with entropic correction.\"\"\"\n    x = np.copy(x0)\n    for _ in range(T):\n        grad = gradient(x, Q, c)\n        # Using a guard for exp to prevent overflow/underflow on extreme grad values.\n        # Clip the argument of exp to a reasonable range.\n        exp_arg = -alpha * grad\n        # A large positive value in exp_arg can cause overflow.\n        # A large negative value can cause underflow to zero.\n        # Clipping helps maintain numerical stability.\n        exp_arg = np.clip(exp_arg, -700, 700)\n        x_unnormalized = x * np.exp(exp_arg)\n        # Normalize to stay on the simplex\n        # Add a small epsilon to prevent division by zero if all components underflow to zero\n        sum_unnormalized = np.sum(x_unnormalized)\n        if sum_unnormalized > 0:\n            x = x_unnormalized / sum_unnormalized\n        else:\n            # If all components are zero (due to underflow or x starting at zero),\n            # we can either stop or distribute the probability mass.\n            # A simple robust choice is to reset to a uniform distribution,\n            # but for this problem, we'll just let it stay at zero-vector,\n            # though this is not on the simplex. In practice, x starts positive,\n            # so this is unlikely.\n            pass\n            \n    return x\n\n# A version of solve that outputs JSON-compatible booleans (lowercase)\ndef solve_json_bool():\n    test_cases = [\n        {'n': 3, 'Q': np.array([[2, -1, 0], [-1, 2, -1], [0, -1, 2]]), 'c': np.array([0.5, -0.2, 0.1]), 'alpha': 0.2, 'T': 200, 'x0': np.array([1/3, 1/3, 1/3])},\n        {'n': 2, 'Q': np.array([[1, 0.2], [0.2, 1]]), 'c': np.array([0.0, 0.4]), 'alpha': 0.5, 'T': 100, 'x0': np.array([1.0, 0.0])},\n        {'n': 4, 'Q': np.diag([1.5, 2.0, 0.7, 0.3]), 'c': np.array([-0.3, 0.1, 0.0, 0.2]), 'alpha': 1.0, 'T': 60, 'x0': np.array([0.25, 0.25, 0.25, 0.25])},\n        {'n': 3, 'Q': np.zeros((3, 3)), 'c': np.array([0.1, -0.05, 0.0]), 'alpha': 0.3, 'T': 100, 'x0': np.array([1/3, 1/3, 1/3])},\n        {'n': 5, 'Q': np.diag([0.5, 1.0, 1.5, 0.8, 1.2]), 'c': np.array([0.2, -0.1, 0.05, 0.0, -0.2]), 'alpha': 0.15, 'T': 150, 'x0': np.array([0.5, 0.2, 0.1, 0.1, 0.1])}\n    ]\n\n    results = []\n    for case in test_cases:\n        Q, c, alpha, T, x0 = case['Q'], case['c'], case['alpha'], case['T'], case['x0']\n        x_final_A = run_variant_A(Q, c, alpha, T, x0.copy())\n        x_final_B = run_variant_B(Q, c, alpha, T, x0.copy())\n        f_A = objective_function(x_final_A, Q, c)\n        f_B = objective_function(x_final_B, Q, c)\n        results.append(str(f_B  f_A).lower())\n\n    print(f\"[{','.join(results)}]\")\n\n# For the final answer, let's stick to the requested format if it is strict.\n# The original code produced Python's `True`/`False`.\nsolve()\n```", "id": "3163727"}]}