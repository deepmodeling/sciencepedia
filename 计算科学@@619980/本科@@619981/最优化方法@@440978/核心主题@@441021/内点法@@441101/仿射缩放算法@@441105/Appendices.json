{"hands_on_practices": [{"introduction": "理解一个算法最可靠的方法莫过于亲手计算一遍。这个练习将引导你为一个简单的线性规划问题完整地执行一轮原始仿射尺度迭代。通过从头计算对偶变量估计、搜索方向和步长，你将深刻理解每个数学构件（如缩放矩阵 $X$、方向 $p$ 和步长 $\\alpha$）在算法中所扮演的具体角色。[@problem_id:3095992]", "problem": "考虑以下线性规划 (LP) 问题：最小化线性目标函数 $c^{\\top} x$，约束条件为仿射等式约束 $A x = b$ 和非负性约束 $x \\ge 0$。数据为\n$$\nA = \\begin{pmatrix} 1  1  1 \\end{pmatrix}, \\quad b = 3, \\quad c = \\begin{pmatrix} 1 \\\\ 2 \\\\ 0 \\end{pmatrix},\n$$\n以及严格可行的起始点为\n$$\nx^{(0)} = \\begin{pmatrix} \\tfrac{1}{2} \\\\ \\tfrac{3}{2} \\\\ 1 \\end{pmatrix}.\n$$\n从基本原理出发，执行原始仿射尺度 (AS) 方法的一次完整迭代：将搜索方向解释为在由 $X^{-1}$（其中 $X = \\mathrm{diag}(x^{(0)})$）导出的局部范数下，线性化目标 $c^{\\top} d$ 限制在仿射可行集 $A d = 0$ 上的最陡下降方向。利用此解释，确定强制一阶可行性的拉格朗日乘子 $y$，然后构建相应的搜索方向 $p$，最后使用参数为 $\\beta = \\tfrac{9}{10}$ 的到边界分数规则选择步长 $\\alpha$。计算更新后的点 $x^{(1)} = x^{(0)} + \\alpha p$。提供 $x^{(1)}$ 各分量的精确值，不要四舍五入。将最终答案表示为单个行向量。", "solution": "该问题是适定的，有科学依据，并为获得唯一解提供了所有必要信息。验证所提供的起始点 $x^{(0)}$ 是严格可行的：\n$A x^{(0)} = \\begin{pmatrix} 1  1  1 \\end{pmatrix} \\begin{pmatrix} \\tfrac{1}{2} \\\\ \\tfrac{3}{2} \\\\ 1 \\end{pmatrix} = \\frac{1}{2} + \\frac{3}{2} + 1 = 3 = b$。\n$x^{(0)}$ 的所有分量都是严格为正的。因此，该问题是有效的，我们继续求解。\n\n原始仿射尺度方法通过考虑问题的一个尺度变换版本，在当前严格可行点 $x$ 处生成一个搜索方向 $p$。如前所述，我们在可行子空间 $Ad=0$ 内寻找目标函数 $c^{\\top}d$ 的最陡下降方向，该方向由局部范数 $\\|d\\|_{X^{-1}} = \\sqrt{d^{\\top}X^{-2}d}$（其中 $X = \\mathrm{diag}(x)$）来度量。这可以表述为以下优化问题：\n$$\n\\begin{array}{ll}\n\\text{minimize}  c^{\\top} d \\\\\n\\text{subject to}  Ad = 0 \\\\\n                  d^{\\top}X^{-2}d = \\text{constant}\n\\end{array}\n$$\n该问题的拉格朗日函数为 $L(d, y) = c^{\\top}d - y^{\\top}(Ad)$，我们只考虑方向上的等式约束。在尺度变换后的范数下，关于 $d$ 的最优性条件意味着目标函数相对于尺度变换后坐标的梯度必须与尺度变换后的可行子空间正交。搜索方向 $p$ 是负的尺度变换后梯度在尺度变换后约束矩阵的零空间上的投影。\n\n令尺度变换后的变量为 $\\tilde{d} = X^{-1} d$，尺度变换后的数据为 $\\tilde{A} = AX$ 和 $\\tilde{c} = Xc$。在尺度变换后的空间中，搜索方向 $\\tilde{p}$ 是 $-\\tilde{c}$ 在 $\\tilde{A}$ 的零空间上的投影：\n$$\n\\tilde{p} = -\\left(I - \\tilde{A}^{\\top}(\\tilde{A}\\tilde{A}^{\\top})^{-1}\\tilde{A}\\right)\\tilde{c}\n$$\n那么，在原始空间中的搜索方向是 $p = X\\tilde{p}$。\n\n或者，更直接地，原始 LP 的一阶最优性条件意味着存在一个对偶变量估计 $y$ 和一个简约成本向量 $s = c - A^{\\top}y$。仿射尺度搜索方向 $p$ 由 $p = -X^2 s$ 给出。选择对偶估计 $y$（拉格朗日乘子）以确保搜索方向位于 $A$ 的零空间中，即 $Ap=0$。$A(-X^2(c-A^{\\top}y)) = 0 \\implies -AX^2c + AX^2A^{\\top}y = 0 \\implies y = (AX^2A^{\\top})^{-1}AX^2c$。这与问题中寻找拉格朗日乘子 $y$ 的要求相符。\n\n我们给定的起始点为 $x^{(0)} = \\begin{pmatrix} \\frac{1}{2} \\\\ \\frac{3}{2} \\\\ 1 \\end{pmatrix}$。\n尺度变换矩阵 $X$ 及其平方为：\n$$\nX = \\mathrm{diag}(x^{(0)}) = \\begin{pmatrix} \\frac{1}{2}  0  0 \\\\ 0  \\frac{3}{2}  0 \\\\ 0  0  1 \\end{pmatrix}, \\quad X^2 = \\begin{pmatrix} \\frac{1}{4}  0  0 \\\\ 0  \\frac{9}{4}  0 \\\\ 0  0  1 \\end{pmatrix}\n$$\n给定数据： $A = \\begin{pmatrix} 1  1  1 \\end{pmatrix}$ 和 $c = \\begin{pmatrix} 1 \\\\ 2 \\\\ 0 \\end{pmatrix}$。\n\n首先，我们计算拉格朗日乘子 $y$ 所需的分量：\n$$\nAX^2 = \\begin{pmatrix} 1  1  1 \\end{pmatrix} \\begin{pmatrix} \\frac{1}{4}  0  0 \\\\ 0  \\frac{9}{4}  0 \\\\ 0  0  1 \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{4}  \\frac{9}{4}  1 \\end{pmatrix}\n$$\n$$\nAX^2A^{\\top} = \\begin{pmatrix} \\frac{1}{4}  \\frac{9}{4}  1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix} = \\frac{1}{4} + \\frac{9}{4} + 1 = \\frac{10}{4} + 1 = \\frac{5}{2} + 1 = \\frac{7}{2}\n$$\n$$\nAX^2c = \\begin{pmatrix} \\frac{1}{4}  \\frac{9}{4}  1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 2 \\\\ 0 \\end{pmatrix} = \\frac{1}{4} + \\frac{18}{4} + 0 = \\frac{19}{4}\n$$\n现在，我们计算拉格朗日乘子 $y$：\n$$\ny = (AX^2A^{\\top})^{-1}AX^2c = \\left(\\frac{7}{2}\\right)^{-1} \\left(\\frac{19}{4}\\right) = \\frac{2}{7} \\cdot \\frac{19}{4} = \\frac{19}{14}\n$$\n接下来，我们确定简约成本向量 $s = c - A^{\\top}y$：\n$$\ns = \\begin{pmatrix} 1 \\\\ 2 \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix} \\left(\\frac{19}{14}\\right) = \\begin{pmatrix} 1 - \\frac{19}{14} \\\\ 2 - \\frac{19}{14} \\\\ 0 - \\frac{19}{14} \\end{pmatrix} = \\begin{pmatrix} \\frac{14-19}{14} \\\\ \\frac{28-19}{14} \\\\ -\\frac{19}{14} \\end{pmatrix} = \\begin{pmatrix} -\\frac{5}{14} \\\\ \\frac{9}{14} \\\\ -\\frac{19}{14} \\end{pmatrix}\n$$\n搜索方向 $p$ 是 $p = -X^2 s$：\n$$\np = - \\begin{pmatrix} \\frac{1}{4}  0  0 \\\\ 0  \\frac{9}{4}  0 \\\\ 0  0  1 \\end{pmatrix} \\begin{pmatrix} -\\frac{5}{14} \\\\ \\frac{9}{14} \\\\ -\\frac{19}{14} \\end{pmatrix} = - \\begin{pmatrix} -\\frac{5}{56} \\\\ \\frac{81}{56} \\\\ -\\frac{19}{14} \\end{pmatrix} = \\begin{pmatrix} \\frac{5}{56} \\\\ -\\frac{81}{56} \\\\ \\frac{19}{14} \\end{pmatrix} = \\begin{pmatrix} \\frac{5}{56} \\\\ -\\frac{81}{56} \\\\ \\frac{76}{56} \\end{pmatrix}\n$$\n为了确定步长 $\\alpha$，我们使用到边界分数规则，$\\alpha = \\beta \\alpha_{\\max}$。首先，我们找到保持非负性 $x^{(0)} + \\alpha p \\ge 0$ 的最大步长 $\\alpha_{\\max}$。这受限于 $p_i  0$ 的分量。\n$$\n\\alpha_{\\max} = \\min_{i: p_i  0} \\left\\{ \\frac{x_i^{(0)}}{-p_i} \\right\\}\n$$\n在我们的例子中，只有 $p_2 = -\\frac{81}{56}$ 是负数。\n$$\n\\alpha_{\\max} = \\frac{x_2^{(0)}}{-p_2} = \\frac{\\frac{3}{2}}{-(-\\frac{81}{56})} = \\frac{3}{2} \\cdot \\frac{56}{81} = \\frac{3 \\cdot 28}{81} = \\frac{28}{27}\n$$\n步长参数为 $\\beta = \\frac{9}{10}$。所以步长为：\n$$\n\\alpha = \\beta \\alpha_{\\max} = \\frac{9}{10} \\cdot \\frac{28}{27} = \\frac{1}{10} \\cdot \\frac{28}{3} = \\frac{28}{30} = \\frac{14}{15}\n$$\n最后，我们计算新的迭代点 $x^{(1)} = x^{(0)} + \\alpha p$：\n$$\nx^{(1)} = \\begin{pmatrix} \\frac{1}{2} \\\\ \\frac{3}{2} \\\\ 1 \\end{pmatrix} + \\frac{14}{15} \\begin{pmatrix} \\frac{5}{56} \\\\ -\\frac{81}{56} \\\\ \\frac{76}{56} \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{2} \\\\ \\frac{3}{2} \\\\ 1 \\end{pmatrix} + \\begin{pmatrix} \\frac{14 \\cdot 5}{15 \\cdot 56} \\\\ \\frac{14 \\cdot (-81)}{15 \\cdot 56} \\\\ \\frac{14 \\cdot 76}{15 \\cdot 56} \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{2} \\\\ \\frac{3}{2} \\\\ 1 \\end{pmatrix} + \\begin{pmatrix} \\frac{5}{15 \\cdot 4} \\\\ \\frac{-81}{15 \\cdot 4} \\\\ \\frac{76}{15 \\cdot 4} \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{2} \\\\ \\frac{3}{2} \\\\ 1 \\end{pmatrix} + \\begin{pmatrix} \\frac{5}{60} \\\\ -\\frac{81}{60} \\\\ \\frac{76}{60} \\end{pmatrix}\n$$\n$$\nx_1^{(1)} = \\frac{1}{2} + \\frac{5}{60} = \\frac{30}{60} + \\frac{5}{60} = \\frac{35}{60} = \\frac{7}{12}\n$$\n$$\nx_2^{(1)} = \\frac{3}{2} - \\frac{81}{60} = \\frac{90}{60} - \\frac{81}{60} = \\frac{9}{60} = \\frac{3}{20}\n$$\n$$\nx_3^{(1)} = 1 + \\frac{76}{60} = \\frac{60}{60} + \\frac{76}{60} = \\frac{136}{60} = \\frac{34}{15}\n$$\n更新后的点是 $x^{(1)} = \\begin{pmatrix} \\frac{7}{12} \\\\ \\frac{3}{20} \\\\ \\frac{34}{15} \\end{pmatrix}$。\n作为检验，我们确认 $A x^{(1)} = b$：\n$$\n\\frac{7}{12} + \\frac{3}{20} + \\frac{34}{15} = \\frac{35}{60} + \\frac{9}{60} + \\frac{136}{60} = \\frac{35+9+136}{60} = \\frac{180}{60} = 3\n$$\n约束条件得到满足。", "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{7}{12}  \\frac{3}{20}  \\frac{34}{15} \\end{pmatrix}}\n$$", "id": "3095992"}, {"introduction": "掌握了基本计算之后，下一步是理解算法的“智能”所在。仿射尺度法的精髓在于其能根据当前迭代点的位置动态调整搜索方向，从而高效地逼近最优解。本练习通过分析一个特例，探讨算法如何根据当前各变量的大小和目标函数中对应的成本，有选择地、以不同速率将某些变量推向零，揭示缩放机制如何巧妙地引导收敛路径。[@problem_id:3095980]", "problem": "考虑标准型线性规划（LP）\n最小化 $c^{\\top} x$，约束条件为 $A x = b$, $x \\ge 0$，\n其中\n$A = \\begin{bmatrix} 1  1  0  0 \\\\ 0  0  1  1 \\end{bmatrix}$，$b = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}$，以及 $c = \\begin{bmatrix} 0 \\\\ 2 \\\\ 3 \\\\ 4 \\end{bmatrix}$。\n假设我们从严格可行点 $x^{(0)} = \\begin{bmatrix} 0.5 \\\\ 0.5 \\\\ 0.5 \\\\ 0.5 \\end{bmatrix}$ 开始应用仿射尺度算法，该点满足 $A x^{(0)} = b$ 和 $x^{(0)}  0$。仿射尺度步长定义为在当前正对角尺度 $D = \\operatorname{diag}(x)$ 诱导的局部度量下，于仿射包 $\\{x : A x = b\\}$ 内 $c^{\\top} x$ 的最速下降，然后沿该方向迈出一个保持正性的步长。\n\n选择所有正确的陈述。\n\nA. 唯一最优解是 $x^{\\star} = \\begin{bmatrix} 1 \\\\ 0 \\\\ 1 \\\\ 0 \\end{bmatrix}$，它位于由 $x_2 = 0$ 和 $x_4 = 0$ 定义的面上。\n\nB. 在 $x^{(0)}$ 处，仿射尺度搜索方向 $d$ 使 $x_2$ 的减小速度是 $x_4$ 的两倍，无论是在绝对值上还是相对值上；具体来说，$d_2 = -0.25$ 且 $d_4 = -0.125$，因此 $|d_2| = 2 |d_4|$ 且 $(-d_2/x_2)/(-d_4/x_4) = 2$。\n\nC. 在 $x^{(0)}$ 处，仿射尺度方向与负成本向量成比例，即对于某个 $\\lambda  0$ 有 $d = -\\lambda c$。\n\nD. 对于严格可行点 $x$ 处的仿射尺度方向 $d$，每单位步长的分量相对收缩率满足 $-d_i/x_i = x_i p_i$，其中 $p = c - A^{\\top} y$ 是由尺度诱导的简约成本向量，而 $y$ 确保了在仿射包内的可行性。因此，在 $x^{(0)}$ 处，当 $x_2^{(0)} = x_4^{(0)}$ 且 $p_2  p_4  0$ 时，随着该方法接近最优面，坐标 $x_2$ 比 $x_4$ 收缩得更快。", "solution": "用户要求我分析给定线性规划（LP）和起始点的仿射尺度算法的第一步，并评估几个相关的陈述。\n\n### 第1步：提取已知条件\n线性规划以标准型给出：\n最小化 $c^{\\top} x$\n约束条件为 $A x = b$, $x \\ge 0$。\n\n具体数据如下：\n-   $A = \\begin{bmatrix} 1  1  0  0 \\\\ 0  0  1  1 \\end{bmatrix}$\n-   $b = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}$\n-   $c = \\begin{bmatrix} 0 \\\\ 2 \\\\ 3 \\\\ 4 \\end{bmatrix}$\n-   起始点是 $x^{(0)} = \\begin{bmatrix} 0.5 \\\\ 0.5 \\\\ 0.5 \\\\ 0.5 \\end{bmatrix}$。\n\n问题将仿射尺度步长描述为在仿射包 $\\{x : A x = b\\}$ 内，基于由尺度 $D = \\operatorname{diag}(x)$ 诱导的局部度量下 $c^{\\top} x$ 的最速下降，然后进行一个保持正性的步进。\n\n### 第2步：使用提取的已知条件进行验证\n1.  **科学依据**：问题描述了一个著名的内点法，即仿射尺度算法，应用于一个标准型线性规划。这些概念在数学上和算法上都是合理的。\n2.  **适定性**：该线性规划是适定的。我们来检查起始点 $x^{(0)}$ 的可行性。\n    -   $A x^{(0)} = \\begin{bmatrix} 1  1  0  0 \\\\ 0  0  1  1 \\end{bmatrix} \\begin{bmatrix} 0.5 \\\\ 0.5 \\\\ 0.5 \\\\ 0.5 \\end{bmatrix} = \\begin{bmatrix} 0.5+0.5 \\\\ 0.5+0.5 \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} = b$。等式约束得到满足。\n    -   $x^{(0)} = \\begin{bmatrix} 0.5 \\\\ 0.5 \\\\ 0.5 \\\\ 0.5 \\end{bmatrix}  0$。非负约束被严格满足。\n    因此，$x^{(0)}$ 是一个严格可行点。\n3.  **客观性**：问题陈述使用了精确的数学语言并提供了客观数据。\n\n问题是自洽、一致且适定的。它基于优化理论的既定原则。因此，该问题是 **有效的**。\n\n### 仿射尺度方向的推导\n\n仿射尺度算法在严格可行点 $x$ 处的搜索方向 $d$ 由 $d = -D^2 p$ 给出，其中 $D = \\operatorname{diag}(x)$ 是尺度矩阵， $p = c - A^{\\top} y$ 是简约成本向量。计算对偶变量估计 $y$ 以确保方向位于缩放后约束矩阵的零空间中，即 $(AD)d = 0$。这导出了公式：\n$$y = (A D^2 A^{\\top})^{-1} A D^2 c$$\n让我们为给定问题在 $x^{(0)}$ 处计算这些量。\n\n1.  **尺度矩阵**：\n    $x^{(0)} = \\begin{bmatrix} 0.5 \\\\ 0.5 \\\\ 0.5 \\\\ 0.5 \\end{bmatrix}$，所以 $D = \\operatorname{diag}(0.5, 0.5, 0.5, 0.5) = 0.5 I$。\n    $D^2 = \\operatorname{diag}(0.25, 0.25, 0.25, 0.25) = 0.25 I$。\n\n2.  **对偶变量估计 $y$**：\n    由于 $D^2 = 0.25 I$，$y$ 的公式简化为：\n    $y = (A (0.25 I) A^{\\top})^{-1} A (0.25 I) c = (0.25 A A^{\\top})^{-1} (0.25 A c) = (A A^{\\top})^{-1} A c$。\n    $A A^{\\top} = \\begin{bmatrix} 1  1  0  0 \\\\ 0  0  1  1 \\end{bmatrix} \\begin{bmatrix} 1  0 \\\\ 1  0 \\\\ 0  1 \\\\ 0  1 \\end{bmatrix} = \\begin{bmatrix} 1^2+1^2  0 \\\\ 0  1^2+1^2 \\end{bmatrix} = \\begin{bmatrix} 2  0 \\\\ 0  2 \\end{bmatrix}$。\n    $(A A^{\\top})^{-1} = \\begin{bmatrix} 1/2  0 \\\\ 0  1/2 \\end{bmatrix} = 0.5 I$。\n    $A c = \\begin{bmatrix} 1  1  0  0 \\\\ 0  0  1  1 \\end{bmatrix} \\begin{bmatrix} 0 \\\\ 2 \\\\ 3 \\\\ 4 \\end{bmatrix} = \\begin{bmatrix} 0+2 \\\\ 3+4 \\end{bmatrix} = \\begin{bmatrix} 2 \\\\ 7 \\end{bmatrix}$。\n    $y = (A A^{\\top})^{-1} (A c) = \\begin{bmatrix} 0.5  0 \\\\ 0  0.5 \\end{bmatrix} \\begin{bmatrix} 2 \\\\ 7 \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 3.5 \\end{bmatrix}$。\n\n3.  **简约成本向量 $p$**：\n    $p = c - A^{\\top} y$。\n    $A^{\\top} y = \\begin{bmatrix} 1  0 \\\\ 1  0 \\\\ 0  1 \\\\ 0  1 \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 3.5 \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 1 \\\\ 3.5 \\\\ 3.5 \\end{bmatrix}$。\n    $p = \\begin{bmatrix} 0 \\\\ 2 \\\\ 3 \\\\ 4 \\end{bmatrix} - \\begin{bmatrix} 1 \\\\ 1 \\\\ 3.5 \\\\ 3.5 \\end{bmatrix} = \\begin{bmatrix} -1 \\\\ 1 \\\\ -0.5 \\\\ 0.5 \\end{bmatrix}$。\n\n4.  **搜索方向 $d$**：\n    $d = -D^2 p = -0.25 I \\begin{bmatrix} -1 \\\\ 1 \\\\ -0.5 \\\\ 0.5 \\end{bmatrix} = -0.25 \\begin{bmatrix} -1 \\\\ 1 \\\\ -0.5 \\\\ 0.5 \\end{bmatrix} = \\begin{bmatrix} 0.25 \\\\ -0.25 \\\\ 0.125 \\\\ -0.125 \\end{bmatrix}$。\n    我们可以验证 $d$ 位于 $A$ 的零空间中：\n    $A d = \\begin{bmatrix} 1  1  0  0 \\\\ 0  0  1  1 \\end{bmatrix} \\begin{bmatrix} 0.25 \\\\ -0.25 \\\\ 0.125 \\\\ -0.125 \\end{bmatrix} = \\begin{bmatrix} 0.25 - 0.25 \\\\ 0.125 - 0.125 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$。\n\n### 逐项分析\n\n**A. 唯一最优解是 $x^{\\star} = \\begin{bmatrix} 1 \\\\ 0 \\\\ 1 \\\\ 0 \\end{bmatrix}$，它位于由 $x_2 = 0$ 和 $x_4 = 0$ 定义的面上。**\n\n为了找到最优解，我们分析这个线性规划。目标是最小化 $c^{\\top} x = 0x_1 + 2x_2 + 3x_3 + 4x_4$。\n约束条件是：\n1.  $x_1 + x_2 = 1$\n2.  $x_3 + x_4 = 1$\n3.  $x_1, x_2, x_3, x_4 \\ge 0$\n\n从约束条件可知，$x_1 \\ge 0 \\implies 1-x_2 \\ge 0 \\implies x_2 \\le 1$。类似地，$x_3 \\ge 0 \\implies 1-x_4 \\ge 0 \\implies x_4 \\le 1$。\n我们可以用 $x_2$ 和 $x_4$ 来表示目标函数：\n$c^{\\top} x = 2x_2 + 3x_3 + 4x_4 = 2x_2 + 3(1-x_4) + 4x_4 = 3 + 2x_2 + x_4$。\n我们需要最小化 $f(x_2, x_4) = 3 + 2x_2 + x_4$，约束条件是 $0 \\le x_2 \\le 1$ 和 $0 \\le x_4 \\le 1$。\n由于 $x_2$ 和 $x_4$ 的系数（$+2$ 和 $+1$）是正的，当 $x_2$ 和 $x_4$ 取其可能的最小值0时，目标函数达到最小值。\n所以，最优值是 $x_2 = 0$ 和 $x_4 = 0$。\n这个选择唯一地确定了其他变量：\n$x_1 = 1 - x_2 = 1 - 0 = 1$。\n$x_3 = 1 - x_4 = 1 - 0 = 1$。\n唯一最优解是 $x^{\\star} = \\begin{bmatrix} 1 \\\\ 0 \\\\ 1 \\\\ 0 \\end{bmatrix}$。\n该解满足 $x_2 = 0$ 和 $x_4 = 0$，因此它位于由这些方程定义的面上。\n**结论：正确。**\n\n**B. 在 $x^{(0)}$ 处，仿射尺度搜索方向 $d$ 使 $x_2$ 的减小速度是 $x_4$ 的两倍，无论是在绝对值上还是相对值上；具体来说，$d_2 = -0.25$ 且 $d_4 = -0.125$，因此 $|d_2| = 2 |d_4|$ 且 $(-d_2/x_2)/(-d_4/x_4) = 2$。**\n\n根据我们对搜索方向的计算，$d = \\begin{bmatrix} 0.25 \\\\ -0.25 \\\\ 0.125 \\\\ -0.125 \\end{bmatrix}$。\n所以，$d_2 = -0.25$ 且 $d_4 = -0.125$。陈述中的具体数值是正确的。\n让我们检查绝对速率：\n$|d_2| = |-0.25| = 0.25$。\n$|d_4| = |-0.125| = 0.125$。\n比率是 $|d_2|/|d_4| = 0.25/0.125 = 2$，这意味着 $|d_2| = 2 |d_4|$。$x_2$ 的绝对减小速率是 $x_4$ 的两倍。\n现在我们检查在 $x^{(0)}$ 处的相对速率，其中 $x_2^{(0)}=0.5$ 且 $x_4^{(0)}=0.5$。分量 $x_i$ 的相对变化率是 $d_i/x_i$。相对收缩率是 $-d_i/x_i$。\n$x_2$ 的相对收缩率：$-d_2/x_2^{(0)} = -(-0.25)/0.5 = 0.5$。\n$x_4$ 的相对收缩率：$-d_4/x_4^{(0)} = -(-0.125)/0.5 = 0.25$。\n这些相对收缩率之比是 $(0.5) / (0.25) = 2$。这与表达式 $(-d_2/x_2)/(-d_4/x_4) = 2$ 相符。\n该陈述完全由计算结果支持。\n**结论：正确。**\n\n**C. 在 $x^{(0)}$ 处，仿射尺度方向与负成本向量成比例，即对于某个 $\\lambda  0$ 有 $d = -\\lambda c$。**\n\n我们有 $d = \\begin{bmatrix} 0.25 \\\\ -0.25 \\\\ 0.125 \\\\ -0.125 \\end{bmatrix}$ 和 $c = \\begin{bmatrix} 0 \\\\ 2 \\\\ 3 \\\\ 4 \\end{bmatrix}$。\n如果 $d = -\\lambda c$，那么对于某个 $\\lambda  0$ 我们将有：\n$\\begin{bmatrix} 0.25 \\\\ -0.25 \\\\ 0.125 \\\\ -0.125 \\end{bmatrix} = -\\lambda \\begin{bmatrix} 0 \\\\ 2 \\\\ 3 \\\\ 4 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ -2\\lambda \\\\ -3\\lambda \\\\ -4\\lambda \\end{bmatrix}$。\n比较第一个分量得到 $0.25 = 0$，这是一个矛盾。\n更根本地说，仿射尺度方向 $d$ 必须位于 $A$ 的零空间中，意味着 $Ad = 0$。我们来检查 $-c$ 是否位于 $A$ 的零空间中。\n$A(-c) = -Ac = -\\begin{bmatrix} 2 \\\\ 7 \\end{bmatrix} \\neq 0$。\n由于 $-c$ 不在 $A$ 的零空间中，搜索方向 $d$ 不可能与 $-c$ 成比例。\n**结论：不正确。**\n\n**D. 对于严格可行点 $x$ 处的仿射尺度方向 $d$，每单位步长的分量相对收缩率满足 $-d_i/x_i = x_i p_i$，其中 $p = c - A^{\\top} y$ 是由尺度诱导的简约成本向量，而 $y$ 确保了在仿射包内的可行性。因此，在 $x^{(0)}$ 处，当 $x_2^{(0)} = x_4^{(0)}$ 且 $p_2  p_4  0$ 时，随着该方法接近最优面，坐标 $x_2$ 比 $x_4$ 收缩得更快。**\n\n我们先来验证这个公式。搜索方向是 $d = -D^2 p$。分量形式为 $d_i = -(D^2)_{ii} p_i$。由于 $D = \\operatorname{diag}(x_1, \\dots, x_n)$，所以 $D^2 = \\operatorname{diag}(x_1^2, \\dots, x_n^2)$。因此，$d_i = -x_i^2 p_i$。\n分量相对收缩率是 $-d_i/x_i$。\n$-d_i/x_i = -(-x_i^2 p_i) / x_i = x_i p_i$。\n公式是正确的。对 $p$ 和 $y$ 的描述也是准确的。\n现在，我们来评估在 $x^{(0)}$ 处的结果。\n在 $x^{(0)}$ 处，我们有 $x_2^{(0)} = 0.5$ 和 $x_4^{(0)} = 0.5$。条件 $x_2^{(0)} = x_4^{(0)}$ 成立。\n根据我们对简约成本向量 $p$ 的计算，我们有 $p_2 = 1$ 和 $p_4 = 0.5$。条件 $p_2  p_4  0$ 也成立。\n结论是“坐标 $x_2$ 比 $x_4$ 收缩得更快”。我们来比较一下收缩率。\n$x_2$ 的相对收缩率：$-d_2/x_2^{(0)} = x_2^{(0)} p_2 = (0.5)(1) = 0.5$。\n$x_4$ 的相对收缩率：$-d_4/x_4^{(0)} = x_4^{(0)} p_4 = (0.5)(0.5) = 0.25$。\n由于 $0.5  0.25$，$x_2$ 的相对收缩率大于 $x_4$ 的相对收缩率。\n我们也可以比较绝对收缩率，即 $d$ 的各分量的大小。\n$|d_2| = |-x_2^2 p_2| = (0.5)^2(1) = 0.25$。\n$|d_4| = |-x_4^2 p_4| = (0.5)^2(0.5) = 0.125$。\n由于 $0.25  0.125$，$x_2$ 在绝对值上也收缩得更快。结论是正确的。\n该陈述正确地提供了相对收缩率的一般公式，然后将其正确地应用于特定的初始点，从而得出了关于算法行为的有效结论。\n**结论：正确。**", "answer": "$$\\boxed{ABD}$$", "id": "3095980"}, {"introduction": "理论模型在理想条件下运行良好，但实际问题往往充满挑战，例如约束矩阵 $A$ 的列向量尺度差异巨大可能导致数值不稳定的“病态”问题。这个计算实验将带你直面这一现实挑战。通过实现并比较标准仿射尺度算法和一个经过“列归一化”预处理的变体，你将亲眼见证一个简单的变量代换如何能显著提升算法在处理病态问题时的收敛速度和稳健性。[@problem_id:3095999]", "problem": "你将构建并求解一系列线性规划问题，使用原始仿射缩放算法，并在约束矩阵的列存在数量级差异的实例上比较两种变体。目标是量化列归一化如何影响达到固定的一阶最优性容差所需的迭代次数。你的程序的最终输出必须是单行文本，汇总了在指定测试集上的结果。\n\n考虑等式形式的线性规划：最小化 $c^\\top x$，约束条件为 $A x = b$, $x \\in \\mathbb{R}^n$, $x  0$。原始仿射缩放算法通过计算在由 $D^{-1}$（其中 $D = \\mathrm{diag}(x)$）导出的加权范数下的最速可行下降方向，来迭代更新一个严格可行点 $x$，然后沿此方向前进一步，同时保持严格正性。你应该依赖的相关定义和性质如下：\n- 线性规划的定义、线性目标函数梯度 $c$ 的作用以及可行下降方向的概念。\n- 由正定矩阵定义的范数（此处为由 $D^{-1}$ 导出的范数）下的最速下降方向。\n- 将向量投影到矩阵零空间，以及构造满足 $A p = 0$ 的可行方向。\n- 涉及简约成本向量和与等式约束相关的拉格朗日乘子的线性规划一阶最优性条件。\n- 需要选择一个步长以确保 $x + \\alpha p  0$，并在适当的最优性残差足够小时终止。\n\n你将实现两种变体：\n- 标准的原始仿射缩放，直接应用于 $(A,b,c)$。\n- 一种列归一化变体，通过变量替换来预处理 $A$ 的列，使其具有单位欧几里得范数。设 $S = \\mathrm{diag}(\\|a_1\\|_2,\\dots,\\|a_n\\|_2)$，其中 $a_j$ 表示 $A$ 的第 $j$ 列。定义变换后的变量 $y = S x$，变换后的约束矩阵 $A' = A S^{-1}$，以及变换后的目标向量 $c' = S^{-1} c$。在 $y$ 变量上对 $(A', b, c')$ 应用仿射缩放算法，从 $y^{(0)} = S x^{(0)}$ 开始。为进行公平比较，在映射回 $x = S^{-1} y$ 后，使用在原始变量中定义的相同最优性残差来检查收敛性。\n\n两种变体的停止规则：在当前点 $x$ 处，构造 $D = \\mathrm{diag}(x)$ 并计算拉格朗日乘子 $\\lambda$，其解为 $m \\times m$ 线性系统 $(A D^2 A^\\top)\\, \\lambda = A D^2 c$。定义简约成本向量 $r = c - A^\\top \\lambda$。当 $\\| D r \\|_\\infty \\le 10^{-6}$ 时终止。使用步长保护参数 $\\beta = 0.9$，并在每次迭代中选择满足 $x + \\alpha p  0$ 的最大 $\\alpha \\in (0,1)$，然后将 $\\alpha$ 设置为该最大值的 $\\beta$ 倍。设置 $10{,}000$ 次迭代的硬上限；如果方法在上限内未达到容差，则报告上限作为迭代次数。\n\n实例构造：\n- 将随机数生成器种子固定为 $2025$。\n- 设置 $m = 2$ 和 $n = 30$。\n- 抽取一个基础矩阵 $A_{\\text{base}} \\in \\mathbb{R}^{m \\times n}$，其元素独立且均匀分布在 $[0.5, 1.5]$ 中。\n- 对于一个分布参数 $p \\in \\{0, 3, 6\\}$，通过 $s_j = 10^{\\ell_j}$ 定义一个列缩放向量 $s \\in \\mathbb{R}^n$，其中 $\\ell_j$ 对于 $j = 1, \\dots, n$ 在 $[-p, p]$ 上均匀分布。构造 $A = A_{\\text{base}} \\, \\mathrm{diag}(s)$，使得 $A$ 的列存在数量级差异。\n- 抽取一个严格正的初始点 $x^{(0)} \\in \\mathbb{R}^n$，其元素独立且均匀分布在 $[0.5, 1.5]$ 中。\n- 设置 $b = A x^{(0)}$，使得 $x^{(0)}$ 是严格可行的。\n- 抽取目标向量 $c \\in \\mathbb{R}^n$，其元素独立且均匀分布在 $[0.5, 1.5]$ 中。\n\n测试集：\n- 使用三个分布参数 $p \\in \\{0, 3, 6\\}$，以及相同的 $A_{\\text{base}}$、$x^{(0)}$ 和 $c$。对于每个 $p$，按上述方式构造 $A$ 和 $b$，然后以相同的容差、步长保护参数和迭代上限运行两种算法变体。\n- 对于每个 $p$，记录标准变体所用的迭代次数，以及列归一化变体所用的迭代次数，两者都根据原始变量中的相同停止规则进行衡量。\n\n最终输出格式：\n- 你的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果。对于每个测试用例（每个 $p$），结果是一个包含两个整数元素的列表 $[\\text{iters\\_standard}, \\text{iters\\_normalized}]$。因此，最终输出应类似于\n$[[k_0^{\\text{std}},k_0^{\\text{norm}}],[k_1^{\\text{std}},k_1^{\\text{norm}}],[k_2^{\\text{std}},k_2^{\\text{norm}}]]$,\n其中每个 $k_i^{\\text{std}}$ 和 $k_i^{\\text{norm}}$ 都是一个整数迭代次数。\n\n角度单位不适用。不存在物理单位。所有报告的值都是无单位的整数。请确保你的代码在指定的种子和参数下是确定性的，并且不需要任何外部输入。", "solution": "该问题是有效的。它提出了一个定义明确的计算实验，以评估列缩放对原始仿射缩放算法在线性规划问题上性能的影响。所有参数、方法和评估标准都得到了明确的规定。\n\n任务是求解一个形式如下的线性规划（LP）问题：\n$$ \\text{minimize } c^\\top x \\quad \\text{subject to} \\quad Ax = b, \\quad x  0 $$\n其中 $c, x \\in \\mathbb{R}^n$，$A \\in \\mathbb{R}^{m \\times n}$，$b \\in \\mathbb{R}^m$。条件 $x  0$ 意味着向量 $x$ 的每个分量 $x_j$ 都必须是严格正的。这种问题结构适合采用内点法，例如原始仿射缩放算法。\n\n原始仿射缩放算法是一种迭代方法，它从一个严格可行点 $x^{(0)}$（即 $A x^{(0)} = b$ 且 $x^{(0)}  0$）开始，生成一系列保持严格可行且旨在减小目标函数值的迭代点 $x^{(k)}$。\n\n在每次迭代 $k$ 中，给定当前迭代点 $x^{(k)}$，算法的核心是找到一个合适的搜索方向 $p^{(k)}$ 和一个步长 $\\alpha^{(k)}$。然后，下一个迭代点计算为 $x^{(k+1)} = x^{(k)} + \\alpha^{(k)} p^{(k)}$。\n\n搜索方向 $p^{(k)}$ 是通过在缩放后的坐标系中考虑该问题而导出的。设 $D_k = \\mathrm{diag}(x^{(k)})$ 是一个对角矩阵，其对角线元素是当前迭代点 $x^{(k)}$ 的分量。我们定义一个变量变换 $x = D_k y$。当前点 $x^{(k)}$ 被映射到 $y^{(k)} = (D_k)^{-1} x^{(k)} = e$，其中 $e$ 是全为1的向量。线性规划约束变为 $(AD_k)y = b$。在原始空间中的可行方向 $p^{(k)}$ 必须满足 $A p^{(k)} = 0$。这对应于在缩放空间中满足 $(AD_k) d_y = 0$ 的方向 $d_y$。\n\n仿射缩放方向是缩放后目标函数的负梯度在缩放后约束矩阵 $AD_k$ 的零空间上的投影。缩放后的目标函数是 $(D_k c)^\\top y$，所以它的梯度是 $D_k c$。投影到矩阵 $M$ 的零空间的投影矩阵是 $I - M^\\top(MM^\\top)^{-1}M$。因此，在缩放空间中的方向是：\n$$ d_y = - \\left( I - (AD_k)^\\top(AD_k(AD_k)^\\top)^{-1}AD_k \\right) D_k c $$\n通过 $p^{(k)} = D_k d_y$ 将其映射回原始空间，我们得到搜索方向：\n$$ p^{(k)} = -D_k^2 \\left( c - A^\\top (AD_k^2A^\\top)^{-1}A D_k^2 c \\right) $$\n我们可以识别出拉格朗日乘子 $\\lambda^{(k)} = (AD_k^2A^\\top)^{-1}A D_k^2 c$ 和简约成本向量 $r^{(k)} = c - A^\\top \\lambda^{(k)}$。然后，方向可以简化为：\n$$ p^{(k)} = -D_k^2 r^{(k)} $$\n\n停止准则是基于一阶最优性条件，该条件要求解的简约成本向量为非负。内点法的一个常用条件是检查缩放后简约成本的范数。根据规定，当 $\\|D_k r^{(k)}\\|_\\infty \\le \\epsilon$ 时算法终止，容差为 $\\epsilon = 10^{-6}$。\n\n步长 $\\alpha^{(k)}$ 的选择必须确保下一个迭代点保持严格可行，即 $x^{(k+1)} = x^{(k)} + \\alpha^{(k)} p^{(k)}  0$。对于任何分量 $j$ 使得 $p_j^{(k)}  0$，我们必须有 $\\alpha^{(k)}  -x_j^{(k)} / p_j^{(k)}$。这导出了可能的最大步长 $\\alpha_{\\text{max_to_boundary}} = \\min_{j: p_j^{(k)}  0} (-x_j^{(k)} / p_j^{(k)})$。问题指定了一个特定规则：首先，找到保持正性的最大 $\\alpha \\in (0, 1)$，即 $\\alpha_{\\text{cand}} = \\min(1, \\alpha_{\\text{max_to_boundary}})$。然后，使用一个保护参数 $\\beta = 0.9$ 从边界回退，得到最终步长 $\\alpha^{(k)} = \\beta \\cdot \\alpha_{\\text{cand}}$。\n\n我们实现并比较此算法的两种变体：\n\n1.  **标准变体**：此变体将前述的仿射缩放算法直接应用于给定的问题数据 $(A, b, c)$，从 $x^{(0)}$ 开始。\n\n2.  **列归一化变体**：此变体旨在减轻因矩阵 $A$ 的列范数存在较大差异而导致的潜在病态问题。它通过变量变换引入了一个预处理步骤。\n    - 首先，我们定义一个缩放矩阵 $S = \\mathrm{diag}(\\|a_1\\|_2, \\|a_2\\|_2, \\dots, \\|a_n\\|_2)$，其中 $a_j$ 是 $A$ 的第 $j$ 列。\n    - 我们引入新变量 $y = Sx$，这意味着 $x = S^{-1}y$。\n    - 原始LP被转换为一个等价的、关于 $y$ 变量的LP：\n      $$ \\text{minimize } (S^{-1}c)^\\top y \\quad \\text{subject to} \\quad (AS^{-1})y = b, \\quad y  0 $$\n    - 设 $c' = S^{-1}c$ 和 $A' = AS^{-1}$。现在 $A'$ 的列具有单位欧几里得范数。\n    - 然后，将标准的仿射缩放算法应用于变换后的问题 $(A', b, c')$，从变换后的初始点 $y^{(0)} = S x^{(0)}$ 开始。\n    - 为了公平比较，停止准则在每次迭代中都在原始问题空间进行评估。迭代点 $y^{(k)}$ 被映射回 $x^{(k)} = S^{-1} y^{(k)}$，并使用原始数据 $(A, b, c)$ 和当前点 $x^{(k)}$ 检查终止规则 $\\|D_k r^{(k)}\\|_\\infty \\le 10^{-6}$。\n\n该实验通过从一个基础矩阵 $A_{\\text{base}}$ 开始，并使用一个分布参数 $p \\in \\{0, 3, 6\\}$ 在其列中引入尺度差异来构造实例。对于每个 $p$，两种算法变体都会运行，并记录收敛所需的迭代次数，最大上限为 $10,000$。这使得我们能够定量比较在面对尺度不佳的数据时，列归一化如何影响算法的收敛速度。", "answer": "```python\nimport numpy as np\n\ndef run_affine_scaling(A, b, c, x0, tol, max_iter, beta):\n    \"\"\"\n    Implements the standard primal affine scaling algorithm.\n    \"\"\"\n    m, n = A.shape\n    x = x0.copy()\n\n    for k in range(max_iter):\n        # 1. Check stopping criterion\n        # Using element-wise operations with x vector is more efficient than\n        # explicitly forming the diagonal matrix D. D^2 is diag(x**2).\n        x_sq = x * x\n        AD2 = A * x_sq  # Equivalent to A @ diag(x_sq)\n        M = AD2 @ A.T\n\n        try:\n            # Check for singularity.\n            if np.linalg.cond(M) > 1 / np.finfo(M.dtype).eps:\n                # The matrix is numerically singular, likely due to ill-conditioning.\n                # This can happen with large p, algorithm fails to proceed.\n                return max_iter\n            \n            # Solve the m x m system for Lagrange multipliers\n            lambda_ = np.linalg.solve(M, AD2 @ c)\n        except np.linalg.LinAlgError:\n            # If solver fails, indicates breakdown of the method\n            return max_iter\n\n        r = c - A.T @ lambda_  # Reduced cost\n        \n        # Norm of scaled reduced cost: ||D r||_inf\n        Dr_norm_inf = np.linalg.norm(x * r, ord=np.inf)\n        if Dr_norm_inf = tol:\n            return k\n\n        # 2. Compute search direction\n        p = -x_sq * r\n\n        # 3. Compute step size\n        neg_p_indices = np.where(p  0)[0]\n        if len(neg_p_indices) == 0:\n            # Direction is non-negative, objective is unbounded below.\n            # This should not happen in this problem's setup.\n            return max_iter \n        \n        # Max step to the boundary\n        alpha_max_to_bnd = np.min(-x[neg_p_indices] / p[neg_p_indices])\n\n        # Per problem spec: max alpha in (0,1) such that positivity is maintained\n        alpha_cand = min(1.0, alpha_max_to_bnd)\n        \n        # Final step size with safeguard\n        alpha = beta * alpha_cand\n        \n        # 4. Update iterate\n        x += alpha * p\n\n    return max_iter\n\ndef run_affine_scaling_normalized(A_orig, b_orig, c_orig, x0_orig, tol, max_iter, beta):\n    \"\"\"\n    Implements the column-normalized primal affine scaling algorithm.\n    \"\"\"\n    m, n = A_orig.shape\n\n    # 1. Preprocessing: Create the transformed problem\n    col_norms = np.linalg.norm(A_orig, axis=0)\n    # Avoid division by zero if a column is zero, though not expected here.\n    col_norms[col_norms == 0] = 1.0  \n    \n    A_prime = A_orig / col_norms\n    c_prime = c_orig / col_norms\n    b_prime = b_orig # b is unchanged\n    y0 = x0_orig * col_norms\n\n    y = y0.copy()\n\n    for k in range(max_iter):\n        # 2. Check stopping criterion in the original space\n        x = y / col_norms\n        x_sq = x * x\n        \n        AD2_orig = A_orig * x_sq\n        M_orig = AD2_orig @ A_orig.T\n\n        try:\n            if np.linalg.cond(M_orig) > 1 / np.finfo(M_orig.dtype).eps:\n                return max_iter\n            lambda_orig = np.linalg.solve(M_orig, AD2_orig @ c_orig)\n        except np.linalg.LinAlgError:\n            return max_iter\n            \n        r_orig = c_orig - A_orig.T @ lambda_orig\n        \n        Dr_norm_inf = np.linalg.norm(x * r_orig, ord=np.inf)\n        if Dr_norm_inf = tol:\n            return k\n\n        # 3. Perform one iteration in the transformed (y) space\n        y_sq = y * y\n        A_prime_D2y = A_prime * y_sq\n        M_y = A_prime_D2y @ A_prime.T\n        \n        try:\n            if np.linalg.cond(M_y) > 1 / np.finfo(M_y.dtype).eps:\n                return max_iter\n            lambda_y = np.linalg.solve(M_y, A_prime_D2y @ c_prime)\n        except np.linalg.LinAlgError:\n            return max_iter\n\n        r_y = c_prime - A_prime.T @ lambda_y\n        p_y = -y_sq * r_y\n\n        neg_py_indices = np.where(p_y  0)[0]\n        if len(neg_py_indices) == 0:\n            return max_iter\n        \n        alpha_max_to_bnd_y = np.min(-y[neg_py_indices] / p_y[neg_py_indices])\n        alpha_cand_y = min(1.0, alpha_max_to_bnd_y)\n        alpha_y = beta * alpha_cand_y\n        \n        y += alpha_y * p_y\n\n    return max_iter\n\ndef solve():\n    \"\"\"\n    Main function to set up the problem, run the simulations, and format the output.\n    \"\"\"\n    # Fixed parameters\n    seed = 2025\n    m, n = 2, 30\n    tol = 1e-6\n    beta = 0.9\n    max_iter = 10000\n    spread_params = [0, 3, 6]\n\n    # Generate base data using the fixed seed\n    rng = np.random.default_rng(seed)\n    A_base = rng.uniform(0.5, 1.5, size=(m, n))\n    x0 = rng.uniform(0.5, 1.5, size=n)\n    c = rng.uniform(0.5, 1.5, size=n)\n\n    results = []\n\n    for p in spread_params:\n        # Construct the A matrix and b vector for the current spread parameter\n        if p == 0:\n            s_vec = np.ones(n)\n        else:\n            exponents = np.linspace(-p, p, n)\n            s_vec = 10.0**exponents\n        \n        # A = A_base * s_vec is equivalent to A_base @ diag(s_vec)\n        A = A_base * s_vec\n        b = A @ x0\n\n        # Run both algorithm variants\n        iters_standard = run_affine_scaling(A, b, c, x0, tol, max_iter, beta)\n        iters_normalized = run_affine_scaling_normalized(A, b, c, x0, tol, max_iter, beta)\n        \n        # Store results for this test case\n        results.append([iters_standard, iters_normalized])\n\n    # Format the final output string exactly as required\n    output_parts = [f\"[{std},{norm}]\" for std, norm in results]\n    print(f\"[{','.join(output_parts)}]\")\n\nsolve()\n```", "id": "3095999"}]}