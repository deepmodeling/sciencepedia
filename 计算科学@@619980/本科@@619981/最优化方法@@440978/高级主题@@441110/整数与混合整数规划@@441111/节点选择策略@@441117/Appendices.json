{"hands_on_practices": [{"introduction": "为了客观地比较不同节点选择策略的效率，我们可以引入一个量化指标。本练习引入“遗憾值”（Regret）的概念，用以衡量一个策略相比于理想化的“先知”策略多付出的搜索代价。通过在具体算例中追踪深度优先搜索（DFS）和最佳优先搜索（Best-First）的扩展过程，你将亲手计算它们的遗憾值，从而深入理解它们在不同树结构下的行为差异和性能权衡。[@problem_id:3157396]", "problem": "考虑一个确定性树搜索设置，其中搜索策略在每一步选择一个前沿节点进行扩展。节点扩展会揭示其子节点及其相关的评估值，并产生 $1$ 的单位成本。叶节点带有一个目标值 $\\mathrm{obj}(\\ell)$，该值是叶节点 $\\ell$ 处解的真实质量。最优叶节点是具有最小目标值的叶节点，记为 $v^{\\star} = \\min_{\\ell} \\mathrm{obj}(\\ell)$。当最优叶节点被扩展时，搜索即告终止。对于任何策略 $\\pi$ 和树 $T$，将扩展计数 $E_{\\pi}(T)$ 定义为直到最优叶节点被扩展为止执行的节点扩展总数。将神谕策略定义为理想策略，该策略预先知道最优叶节点的身份，并且只扩展从根到该叶节点的唯一路径上的节点，因此 $E_{\\mathrm{oracle}}(T)$ 等于该路径上的节点数。策略 $\\pi$ 在树 $T$ 上的遗憾值为\n$$\nR_{\\pi}(T) = E_{\\pi}(T) - E_{\\mathrm{oracle}}(T).\n$$\n我们将比较两种节点选择策略：深度优先选择和最佳优先选择。深度优先选择使用后进先出 (LIFO) 的栈规则，并且在每个内部节点都有固定的子节点生成顺序。最佳优先选择选择具有最小启发式值 $h(n)$ 的前沿节点；平局则由较早的生成时间打破（先生成，先选择）。对于叶节点 $\\ell$，设置 $h(\\ell) = \\mathrm{obj}(\\ell)$。\n\n给定两个基准树。\n\n树 $T_1$：根节点 $r_1$ 有两个子节点，按 $a$ 然后 $b$ 的顺序生成。启发式值为 $h(a) = 5$ 和 $h(b) = 6$。节点 $a$ 有两个叶子节点 $a_1$ 和 $a_2$，其目标值为 $\\mathrm{obj}(a_1) = 5$ 和 $\\mathrm{obj}(a_2) = 9$；因此 $h(a_1) = 5$ 和 $h(a_2) = 9$。节点 $b$ 有两个叶子节点 $b_1$ 和 $b_2$，其目标值为 $\\mathrm{obj}(b_1) = 4$ 和 $\\mathrm{obj}(b_2) = 10$；因此 $h(b_1) = 4$ 和 $h(b_2) = 10$。$T_1$ 中的最优叶节点是 $b_1$，其目标值为 $4$。\n\n树 $T_2$：根节点 $r_2$ 有两个子节点，按 $d$ 然后 $c$ 的顺序生成。启发式值为 $h(d) = 2.0$ 和 $h(c) = 1.6$。节点 $d$ 有一个叶子节点 $d_1$，其目标值为 $\\mathrm{obj}(d_1) = 2.0$，所以 $h(d_1) = 2.0$。节点 $c$ 有一个内部子节点 $c_A$，其 $h(c_A) = 1.5$，和一个叶子节点 $c_B$，其 $\\mathrm{obj}(c_B) = 5$，所以 $h(c_B) = 5$。节点 $c_A$ 有两个叶子节点 $c_{A1}$ 和 $c_{A2}$，其目标值为 $\\mathrm{obj}(c_{A1}) = 3.5$ 和 $\\mathrm{obj}(c_{A2}) = 4.1$，所以 $h(c_{A1}) = 3.5$ 和 $h(c_{A2}) = 4.1$。$T_2$ 中的最优叶节点是 $d_1$，其目标值为 $2.0$。\n\n仅使用上述定义和数据，计算比率\n$$\n\\rho = \\frac{R_{\\mathrm{DFS}}(T_1) + R_{\\mathrm{DFS}}(T_2)}{R_{\\mathrm{BF}}(T_1) + R_{\\mathrm{BF}}(T_2)},\n$$\n其中 $\\mathrm{DFS}$ 表示深度优先选择策略，$\\mathrm{BF}$ 表示最佳优先选择策略。请将最终答案表示为精确分数。无需四舍五入。无需单位。", "solution": "该问题是有效的，因为它是自洽的，科学上基于算法分析的原理，并且是客观陈述的。所有必要的数据、定义和条件都已提供，并且没有内部矛盾。我们可以开始求解。\n\n目标是计算比率 $\\rho = \\frac{R_{\\mathrm{DFS}}(T_1) + R_{\\mathrm{DFS}}(T_2)}{R_{\\mathrm{BF}}(T_1) + R_{\\mathrm{BF}}(T_2)}$。策略 $\\pi$ 在树 $T$ 上的遗憾值定义为 $R_{\\pi}(T) = E_{\\pi}(T) - E_{\\mathrm{oracle}}(T)$，其中 $E_{\\pi}(T)$ 是节点扩展的总数。\n\n“节点扩展”是一个产生 $1$ 成本的动作。当最优叶节点被扩展时，搜索终止。问题陈述，神谕策略的扩展计数 $E_{\\mathrm{oracle}}(T)$ 等于从根到最优叶节点的路径上的节点数。这意味着叶节点是以从前沿被选择的方式被“扩展”的，这会产生 $1$ 的成本，并且如果该叶节点是最优的，搜索就会结束。我们将一致地应用此解释。\n\n首先，我们确定每棵树的神谕策略的扩展计数 $E_{\\mathrm{oracle}}(T)$。\n对于树 $T_1$，最优叶节点是 $b_1$。从根节点 $r_1$ 到 $b_1$ 的唯一路径是 $r_1 \\to b \\to b_1$。这条路径包含 $3$ 个节点。因此，$E_{\\mathrm{oracle}}(T_1) = 3$。被扩展的节点是 $r_1$、$b$ 和 $b_1$。\n对于树 $T_2$，最优叶节点是 $d_1$。从根节点 $r_2$ 到 $d_1$ 的唯一路径是 $r_2 \\to d \\to d_1$。这条路径包含 $3$ 个节点。因此，$E_{\\mathrm{oracle}}(T_2) = 3$。被扩展的节点是 $r_2$、$d$ 和 $d_1$。\n\n接下来，我们计算每棵树上深度优先搜索 ($\\mathrm{DFS}$) 和最佳优先搜索 ($\\mathrm{BF}$) 策略的扩展计数 $E_{\\pi}(T)$。\n\n**树 $T_1$ 的分析**\n最优叶节点是 $b_1$，其 $\\mathrm{obj}(b_1) = 4$。\n\n**1. 对 $T_1$ 进行 DFS：**\n$\\mathrm{DFS}$ 使用 LIFO 栈。在 $r_1$ 处的子节点生成顺序是 $a$ 然后 $b$。我们假设其他节点的子节点有固定的、一致的顺序（例如，对于 $a$ 是 $a_1$ 然后 $a_2$，对于 $b$ 是 $b_1$ 然后 $b_2$）。\n- 步骤 1：扩展 $r_1$。已扩展节点：{$r_1$}。成本：$1$。生成子节点 $a, b$。栈（顶端在前）：[$a, b$]。\n- 步骤 2：扩展 $a$。已扩展节点：{$r_1, a$}。成本：$2$。生成子节点 $a_1, a_2$。栈：[$a_1, a_2, b$]。\n- 步骤 3：扩展 $a_1$。已扩展节点：{$r_1, a, a_1$}。成本：$3$。这是一个叶节点，但不是最优的（$\\mathrm{obj}(a_1)=5 \\neq 4$）。栈：[$a_2, b$]。\n- 步骤 4：扩展 $a_2$。已扩展节点：{$r_1, a, a_1, a_2$}。成本：$4$。这是一个叶节点，但不是最优的（$\\mathrm{obj}(a_2)=9 \\neq 4$）。栈：[$b$]。\n- 步骤 5：扩展 $b$。已扩展节点：{$r_1, a, a_1, a_2, b$}。成本：$5$。生成子节点 $b_1, b_2$。栈：[$b_1, b_2$]。\n- 步骤 6：扩展 $b_1$。已扩展节点：{$r_1, a, a_1, a_2, b, b_1$}。成本：$6$。这是最优叶节点。搜索终止。\n扩展总数为 $E_{\\mathrm{DFS}}(T_1) = 6$。\n遗憾值为 $R_{\\mathrm{DFS}}(T_1) = E_{\\mathrm{DFS}}(T_1) - E_{\\mathrm{oracle}}(T_1) = 6 - 3 = 3$。\n\n**2. 对 $T_1$ 进行 BF：**\n$\\mathrm{BF}$ 使用按启发式值 $h(n)$ 排序的优先队列。平局由较早的生成时间打破。\n- 步骤 1：扩展 $r_1$。已扩展节点：{$r_1$}。成本：$1$。前沿（优先队列）：{($a, h=5$), ($b, h=6$)}。节点 $a$ 的优先级更高。\n- 步骤 2：扩展 $a$（因为 $h(a)  h(b)$）。已扩展节点：{$r_1, a$}。成本：$2$。子节点 $a_1, a_2$ 被添加到前沿。前沿：{($a_1, h=5$), ($b, h=6$), ($a_2, h=9$)}。节点 $a_1$ 的 $h$ 值最小。\n- 步骤 3：扩展 $a_1$。已扩展节点：{$r_1, a, a_1$}。成本：$3$。此叶节点不是最优的。前沿：{($b, h=6$), ($a_2, h=9$)}。节点 $b$ 的优先级更高。\n- 步骤 4：扩展 $b$。已扩展节点：{$r_1, a, a_1, b$}。成本：$4$。子节点 $b_1, b_2$ 被添加。前沿：{($b_1, h=4$), ($a_2, h=9$), ($b_2, h=10$)}。节点 $b_1$ 现在具有最小的 $h$ 值。\n- 步骤 5：扩展 $b_1$。已扩展节点：{$r_1, a, a_1, b, b_1$}。成本：$5$。这是最优叶节点。搜索终止。\n扩展总数为 $E_{\\mathrm{BF}}(T_1) = 5$。\n遗憾值为 $R_{\\mathrm{BF}}(T_1) = E_{\\mathrm{BF}}(T_1) - E_{\\mathrm{oracle}}(T_1) = 5 - 3 = 2$。\n\n**树 $T_2$ 的分析**\n最优叶节点是 $d_1$，其 $\\mathrm{obj}(d_1) = 2.0$。\n\n**1. 对 $T_2$ 进行 DFS：**\n在 $r_2$ 处的子节点生成顺序是 $d$ 然后 $c$。\n- 步骤 1：扩展 $r_2$。已扩展节点：{$r_2$}。成本：$1$。生成子节点 $d, c$。栈：[$d, c$]。\n- 步骤 2：扩展 $d$。已扩展节点：{$r_2, d$}。成本：$2$。生成子节点 $d_1$。栈：[$d_1, c$]。\n- 步骤 3：扩展 $d_1$。已扩展节点：{$r_2, d, d_1$}。成本：$3$。这是最优叶节点。搜索终止。\n扩展总数为 $E_{\\mathrm{DFS}}(T_2) = 3$。\n遗憾值为 $R_{\\mathrm{DFS}}(T_2) = E_{\\mathrm{DFS}}(T_2) - E_{\\mathrm{oracle}}(T_2) = 3 - 3 = 0$。\n\n**2. 对 $T_2$ 进行 BF：**\n- 步骤 1：扩展 $r_2$。已扩展节点：{$r_2$}。成本：$1$。前沿：{($c, h=1.6$), ($d, h=2.0$)}。节点 $c$ 的优先级更高。\n- 步骤 2：扩展 $c$（因为 $h(c)  h(d)$）。已扩展节点：{$r_2, c$}。成本：$2$。子节点 $c_A, c_B$ 被添加。前沿：{($c_A, h=1.5$), ($d, h=2.0$), ($c_B, h=5.0$)}。节点 $c_A$ 的 $h$ 值最小。\n- 步骤 3：扩展 $c_A$。已扩展节点：{$r_2, c, c_A$}。成本：$3$。子节点 $c_{A1}, c_{A2}$ 被添加。前沿：{($d, h=2.0$), ($c_{A1}, h=3.5$), ($c_{A2}, h=4.1$), ($c_B, h=5.0$)}。节点 $d$ 现在具有最小的 $h$ 值。\n- 步骤 4：扩展 $d$。已扩展节点：{$r_2, c, c_A, d$}。成本：$4$。子节点 $d_1$ 被添加。前沿：{($d_1, h=2.0$), ($c_{A1}, h=3.5$), ($c_{A2}, h=4.1$), ($c_B, h=5.0$)}。尽管 $d$ 和 $d_1$ 有相同的启发式值（$h=2.0$），但 $d$ 是根节点扩展时产生的前沿节点，而 $d_1$ 是新添加的。在 $d$ 被扩展并移除后，$d_1$ 是新的具有最小 $h$ 值的前沿节点。\n- 步骤 5：扩展 $d_1$。已扩展节点：{$r_2, c, c_A, d, d_1$}。成本：$5$。这是最优叶节点。搜索终止。\n扩展总数为 $E_{\\mathrm{BF}}(T_2) = 5$。\n遗憾值为 $R_{\\mathrm{BF}}(T_2) = E_{\\mathrm{BF}}(T_2) - E_{\\mathrm{oracle}}(T_2) = 5 - 3 = 2$。\n\n**最终计算**\n我们得到以下遗憾值：\n$R_{\\mathrm{DFS}}(T_1) = 3$\n$R_{\\mathrm{DFS}}(T_2) = 0$\n$R_{\\mathrm{BF}}(T_1) = 2$\n$R_{\\mathrm{BF}}(T_2) = 2$\n\n现在我们计算比率 $\\rho$：\n$$\n\\rho = \\frac{R_{\\mathrm{DFS}}(T_1) + R_{\\mathrm{DFS}}(T_2)}{R_{\\mathrm{BF}}(T_1) + R_{\\mathrm{BF}}(T_2)} = \\frac{3 + 0}{2 + 2} = \\frac{3}{4}\n$$", "answer": "$$\\boxed{\\frac{3}{4}}$$", "id": "3157396"}, {"introduction": "纯粹的最佳优先搜索有时会陷入局部“陷阱”，反复探索一个看似最优但实际并非全局最优的区域。本练习将你置于一个混合整数线性规划（MILP）求解器的情景中，通过应用一种带惩罚的最佳优先规则，你将体验现代求解器如何通过动态调整节点优先级来平衡“强化”（intensification）与“多样化”（diversification），从而提升全局搜索能力。[@problem_id:3157443]", "problem": "一个求解器对一个混合整数线性规划（MILP）问题应用分支定界法，并使用最优优先的节点选择规则。在最优优先选择中，下一个要扩展的节点是所有开放节点中线性规划（LP）松弛界最大的那个节点。为了鼓励多样化，如果节点的预期下一个分支变量与最近扩展的节点中使用的任何变量相匹配，求解器会通过引入一个惩罚来修改选择。\n\n从以下核心定义开始：在用于 MILP 的分支定界法中，每个开放节点都有一个 LP 松弛目标界，最优优先选择会选择界最大的节点。如果节点的预期下一个分支变量属于先前扩展节点中使用的分支变量的最近历史记录，可以通过从选择分数中减去一个固定惩罚来引入多样化机制。\n\n在对一个具有二元变量 $x_1, x_2, x_3$ 的小型 MILP 进行搜索的某个阶段，前沿由 $4$ 个开放节点组成，索引为 $i \\in \\{1,2,3,4\\}$。对于每个节点 $i$，用 $U_i$ 表示其 LP 松弛上界，用 $v_i \\in \\{x_1,x_2,x_3\\}$ 表示其预期的下一个分支变量（例如，该节点处分数部分最大的变量）。求解器维护一个历史集合 $H$，其中包含最近扩展的两个节点中使用的最后 $L=2$ 个分支变量；这里 $H=\\{x_1,x_2\\}$。用于节点选择的带惩罚的最优优先分数为\n$$\nS_i \\;=\\; \\begin{cases}\nU_i - \\lambda,  \\text{若 } v_i \\in H,\\\\\nU_i,  \\text{若 } v_i \\notin H,\n\\end{cases}\n$$\n其中惩罚参数 $\\lambda0$。\n\n对于当前的前沿，数据如下：\n- $U_1 = 10.2$，$v_1 = x_1$。\n- $U_2 = 10.2$，$v_2 = x_2$。\n- $U_3 = 10.1$，$v_3 = x_1$。\n- $U_4 = 10.0$，$v_4 = x_3$。\n\n设 $\\lambda = 0.25$ 且 $H=\\{x_1,x_2\\}$。求解器选择 $S_i$ 最大的节点；如果出现平局，则选择索引最小的节点。\n\n计算下一个将被选择的节点的索引 $i^\\star \\in \\{1,2,3,4\\}$。你的答案应为一个整数。无需四舍五入。", "solution": "该问题要求我们确定在一个混合整数线性规划（MILP）问题的分支定界搜索中，下一个将被选择的节点是哪一个。选择是基于一个带惩罚的最优优先规则。\n\n问题的核心在于为搜索前沿上的每个开放节点 $i \\in \\{1, 2, 3, 4\\}$ 计算一个分数 $S_i$。分数最高的节点将被选中。问题指定了一个平局打破规则：如果多个节点共享最高分，则选择索引最小的那个。\n\n节点 $i$ 的分数 $S_i$ 由以下分段函数定义：\n$$\nS_i \\;=\\; \\begin{cases}\nU_i - \\lambda,  \\text{若 } v_i \\in H,\\\\\nU_i,  \\text{若 } v_i \\notin H,\n\\end{cases}\n$$\n其中 $U_i$ 是节点 $i$ 的线性规划（LP）松弛上界，$v_i$ 是其预期的下一个分支变量，$H$ 是最近使用的分支变量的历史集合，$\\lambda$ 是一个正的惩罚参数。\n\n给定的数据是：\n- 开放节点集合的索引为 $i \\in \\{1, 2, 3, 4\\}$。\n- LP 松弛上界为 $U_1 = 10.2$，$U_2 = 10.2$，$U_3 = 10.1$ 和 $U_4 = 10.0$。\n- 预期的下一个分支变量为 $v_1 = x_1$，$v_2 = x_2$，$v_3 = x_1$ 和 $v_4 = x_3$。\n- 最后 $L=2$ 个分支变量的历史集合为 $H = \\{x_1, x_2\\}$。\n- 惩罚参数为 $\\lambda = 0.25$。\n\n我们现在将为四个节点中的每一个计算分数 $S_i$。\n\n对于节点 $i=1$：\n预期的分支变量是 $v_1 = x_1$。我们检查 $v_1$ 是否在历史集合 $H = \\{x_1, x_2\\}$ 中。由于 $x_1 \\in H$，分数会受到惩罚。\n$$S_1 = U_1 - \\lambda = 10.2 - 0.25 = 9.95$$\n\n对于节点 $i=2$：\n预期的分支变量是 $v_2 = x_2$。我们检查 $v_2$ 是否在历史集合 $H = \\{x_1, x_2\\}$ 中。由于 $x_2 \\in H$，分数会受到惩罚。\n$$S_2 = U_2 - \\lambda = 10.2 - 0.25 = 9.95$$\n\n对于节点 $i=3$：\n预期的分支变量是 $v_3 = x_1$。我们检查 $v_3$ 是否在历史集合 $H = \\{x_1, x_2\\}$ 中。由于 $x_1 \\in H$，分数会受到惩罚。\n$$S_3 = U_3 - \\lambda = 10.1 - 0.25 = 9.85$$\n\n对于节点 $i=4$：\n预期的分支变量是 $v_4 = x_3$。我们检查 $v_4$ 是否在历史集合 $H = \\{x_1, x_2\\}$ 中。由于 $x_3 \\notin H$，分数不会受到惩罚。\n$$S_4 = U_4 = 10.0$$\n\n现在，我们收集所有节点的计算得分：\n- $S_1 = 9.95$\n- $S_2 = 9.95$\n- $S_3 = 9.85$\n- $S_4 = 10.0$\n\n选择规则是选择具有最大分数 $S_i$ 的节点。我们比较这些分数：\n$$\\max(S_1, S_2, S_3, S_4) = \\max(9.95, 9.95, 9.85, 10.0) = 10.0$$\n最高分是 $10.0$，仅对应于节点 $4$。由于最高分没有平局，因此不需要打破平局的规则。\n\n因此，求解器将选择索引为 $i^\\star = 4$ 的节点。", "answer": "$$\n\\boxed{4}\n$$", "id": "3157443"}, {"introduction": "深度优先和最佳优先策略各有优劣，我们能否将它们结合起来，创造出更强大的混合策略？本练习将这一想法付诸实践，通过一个混合参数 $\\lambda$ 将寻求深度（类DFS）和追求效用（类Best-First）的目标融合进一个统一的优先级函数中。你的任务是实现一个确定性的搜索算法，并进行元优化，以找到在不同特征的训练集上使总扩展节点数最小化的最佳 $\\lambda$ 值，从而体验算法设计与调优的核心过程。[@problem_id:3157413]", "problem": "给你一个确定性的树搜索设置，要求你对一个控制节点选择的混合参数执行元优化。其基础是节点选择策略的定义：深度优先搜索选择深度较大的节点，最佳优先搜索选择启发式效用较大的节点。我们定义一个混合了这两种策略的单一参数化优先级策略，然后寻找在训练集上最小化总节点扩展数的参数。目标是根据这些核心定义，推导出如何以纯算法的方式实现和评估元优化，并编写一个程序，为几个不同的训练集计算最优参数。\n\n用作基础的定义：\n- 一个有限有根搜索树是一个带有指定根节点的有向无环图。每个节点都有一个非负整数深度，定义为从根节点到该节点的路径上的边数。\n- 开放列表是已生成但尚未扩展的前沿节点的集合。当一个节点从开放列表中移除时，它被扩展；扩展会将其子节点生成到开放列表中。\n- 目标节点是当其被选择扩展时终止搜索的节点。\n- 最佳优先节点选择由效用函数 $U(n)$ 指导，其中较大的 $U(n)$ 表示更有希望。深度优先节点选择由较大的 $\\text{depth}(n)$ 指导。\n- 我们将节点 $n$ 的混合优先级 $p(n)$ 定义为\n$$\np(n) \\;=\\; \\lambda\\, U(n) \\;+\\; (1-\\lambda)\\,\\text{depth}(n),\n$$\n其中 $\\lambda \\in [0,1]$ 是混合参数。通过最大化 $p(n)$ 来选择要扩展的节点，确定性的平局打破规则如下所述。\n\n你的任务：\n- 实现一个确定性的树搜索，在每一步从开放列表中选择使 $p(n)$ 最大化的节点 $n$。如果出现平局，首先选择 $\\text{depth}(n)$ 较大的节点；如果 $p(n)$ 和 $\\text{depth}(n)$ 都相等，则选择节点标识符较小的节点。初始开放列表仅包含根节点。当一个节点从开放列表中移除时，将扩展计数器加 1，如果该节点是目标节点，则终止该任务的搜索。否则，将其子节点及其正确的深度生成到开放列表中。不要重新插入已扩展的节点（无重复扩展）。如果在未达到目标的情况下开放列表变空，则扩展计数为已扩展节点的数量。\n- 对于元优化，您必须评估混合参数的离散网格 $\\lambda \\in \\{0.00, 0.25, 0.50, 0.75, 1.00\\}$，并选择使训练集中所有任务的扩展总数最小化的 $\\lambda$。如果不同 $\\lambda$ 值的总扩展数出现平局，请选择最小的 $\\lambda$。\n\n训练集和任务：\n- 训练集 1 (两个任务):\n  - 任务 A1:\n    - 节点：从 $0$ 到 $8$ 的整数，根节点为 $0$。\n    - 边：$\\{(0,1),(0,2),(1,3),(1,4),(2,5),(2,6),(3,7),(4,8)\\}$。\n    - 目标节点：$\\{5\\}$。\n    - 效用 $U(n)$:\n      $U(0)=0.20$, $U(1)=0.10$, $U(2)=0.90$, $U(3)=0.05$, $U(4)=0.05$, $U(5)=1.00$, $U(6)=0.20$, $U(7)=0.01$, $U(8)=0.01$。\n  - 任务 A2:\n    - 节点：从 $0$ 到 $7$ 的整数，根节点为 $0$。\n    - 边：$\\{(0,1),(0,2),(1,3),(2,4),(2,5),(4,6),(5,7)\\}$。\n    - 目标节点：$\\{6\\}$。\n    - 效用 $U(n)$:\n      $U(0)=0.20$, $U(1)=0.10$, $U(2)=0.80$, $U(3)=0.05$, $U(4)=0.90$, $U(5)=0.40$, $U(6)=1.00$, $U(7)=0.30$。\n- 训练集 2 (两个任务，启发式具有误导性且目标节点较深):\n  - 任务 M1:\n    - 节点：从 $0$ 到 $10$ 的整数，根节点为 $0$。\n    - 边：$\\{(0,1),(0,2),(1,3),(3,4),(4,5),(2,6),(2,7),(6,8),(7,9),(7,10)\\}$。\n    - 目标节点：$\\{5\\}$。\n    - 效用 $U(n)$:\n      $U(0)=0.20$, $U(1)=0.10$, $U(2)=0.95$, $U(3)=0.20$, $U(4)=0.30$, $U(5)=1.00$, $U(6)=0.90$, $U(7)=0.85$, $U(8)=0.80$, $U(9)=0.75$, $U(10)=0.70$。\n  - 任务 M2:\n    - 节点：从 $0$ 到 $8$ 的整数，根节点为 $0$。\n    - 边：$\\{(0,1),(0,2),(1,3),(3,4),(2,5),(2,6),(5,7),(6,8)\\}$。\n    - 目标节点：$\\{4\\}$。\n    - 效用 $U(n)$:\n      $U(0)=0.10$, $U(1)=0.10$, $U(2)=0.90$, $U(3)=0.20$, $U(4)=1.00$, $U(5)=0.85$, $U(6)=0.80$, $U(7)=0.70$, $U(8)=0.60$。\n- 训练集 3 (效用一致，目标节点较深):\n  - 任务 U1:\n    - 节点：从 $0$ 到 $6$ 的整数，根节点为 $0$。\n    - 边：$\\{(0,1),(0,2),(1,3),(2,4),(4,5),(5,6)\\}$。\n    - 目标节点：$\\{6\\}$。\n    - 效用 $U(n)$:\n      对于所有节点 $n \\in \\{0,1,2,3,4,5,6\\}$，$U(n)=0.50$。\n\n不涉及角度单位。不涉及物理单位。所有结果均为纯数值。上述训练集作为测试套件，涵盖：\n- 启发式信息有效的一般情况（训练集 1）。\n- 启发式具有误导性且目标节点较深的情况（训练集 2）。\n- 具有一致效用的边界情况（训练集 3），用于检验平局打破规则以及 $\\lambda=0$ 和 $\\lambda=1$ 的极端情况。\n\n要求输出：\n- 您的程序必须使用所述的确定性搜索过程，为每个训练集分别计算最优的 $\\lambda$，从集合 $\\{0.00, 0.25, 0.50, 0.75, 1.00\\}$ 中选择。\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，按训练集 1、训练集 2、训练集 3 的顺序排列结果，每个 $\\lambda$ 值四舍五入到两位小数（例如，$[0.75,0.00,0.25]$）。", "solution": "该问题要求我们为一种混合树搜索策略找到一个最优的混合参数 $\\lambda$。该策略在最佳优先搜索（优先考虑高效用值 $U(n)$ 的节点）和深度优先搜索（优先考虑更深层的节点）之间进行插值。优化是在一组离散的 $\\lambda$ 值上进行的，目标是最小化给定训练集中一系列搜索任务的总节点扩展数。\n\n我们的第一步是根据所提供的定义，形式化地描述环境和搜索算法。\n\n一个搜索任务由树结构、一个根节点、一组目标节点以及每个节点 $n$ 的效用值 $U(n)$ 定义。树由邻接表表示，将每个父节点映射到其子节点。一个节点 $n$ 的深度 $\\text{depth}(n)$ 是从根节点到该节点的路径上的边数。我们可以使用从根节点（其 $\\text{depth}(0) = 0$）开始的广度优先搜索 (BFS) 预先计算树中所有节点的深度。\n\n问题的核心是参数化的搜索算法。在每一步，算法必须从 `open list`（已生成但尚未扩展的节点集）中选择一个节点进行扩展。选择由混合优先级函数决定：\n$$\np(n) = \\lambda\\, U(n) + (1-\\lambda)\\,\\text{depth}(n)\n$$\n其中 $\\lambda \\in [0, 1]$ 是混合参数。选择具有最大优先级 $p(n)$ 的节点。为确保确定性，指定了严格的平局打破规则：\n1. 如果多个节点共享相同的最大优先级 $p(n)$，则选择 $\\text{depth}(n)$ 较大的那个。\n2. 如果仍然存在平局，则选择数值标识符较小的那个节点。\n\n这个分层选择规则可以通过对开放列表中的节点进行排序来实现。对于任意两个节点 $n_1$ 和 $n_2$，如果元组 $(p(n_1), \\text{depth}(n_1), -n_1\\text{.id})$ 在字典序上大于 $(p(n_2), \\text{depth}(n_2), -n_2\\text{.id})$，则 $n_1$ 优先于 $n_2$。节点标识符上的负号反转了比较，从而在平局时有效选择了较小的 ID。\n\n对于单个任务和给定的 $\\lambda$，搜索过程如下：\n1. 初始化一个 `open_list`，其中只包含根节点。\n2. 初始化一个空集 `expanded_nodes` 来跟踪已扩展的节点，以防止重复扩展。\n3. 将扩展计数器初始化为 $0$。\n4. 循环直到 `open_list` 为空或找到目标：\n    a. 根据优先级函数 $p(n)$ 和指定的平局打破规则，从 `open_list` 中选择最佳节点 $n_{best}$。\n    b. 从 `open_list` 中移除 $n_{best}$，并将其添加到 `expanded_nodes` 集合中。将扩展计数器加一。\n    c. 如果 $n_{best}$ 是一个目标节点，则终止此任务的搜索并返回当前的扩展计数。\n    d. 否则，对于 $n_{best}$ 的每个先前未被扩展的子节点，将其添加到 `open_list` 中。\n5. 如果循环因 `open_list` 为空而终止，则返回总扩展计数。\n\n元优化层封装了此搜索过程。对于三个训练集中的每一个，我们必须从离散集合 $\\{0.00, 0.25, 0.50, 0.75, 1.00\\}$ 中找到最优的 $\\lambda$。\n单个训练集的处理过程是：\n1. 初始化一个列表，用于存储每个 $\\lambda$ 的结果。\n2. 对于测试集中的每个 $\\lambda$：\n    a. 为此 $\\lambda$ 初始化一个 `total_expansions` 计数器为 $0$。\n    b. 对于训练集中的每个任务：\n        i. 使用当前的 $\\lambda$ 运行搜索算法。\n        ii. 将得到的扩展数加到 `total_expansions` 中。\n    c. 存储配对 (`total_expansions`, $\\lambda$)。\n3. 在评估完所有 $\\lambda$ 值之后，选择具有最小 `total_expansions` 的配对。如果 `total_expansions` 出现平局，问题规定选择 $\\lambda$ 最小的那个。这可以通过先按总扩展数（升序）再按 $\\lambda$（升序）对结果对进行排序，然后选择第一个元素来实现。\n\n整个过程是确定性的，并且在计算上是明确定义的。实现将涉及创建数据结构来表示任务，一个类或函数来封装搜索逻辑，以及一个顶层循环来为每个训练集执行元优化。最终输出将是每个训练集的最优 $\\lambda$ 值的列表，并按要求格式化。\n\n例如，在训练集 3 中，所有节点的效用 $U(n)$ 都是一个常数 0.5。优先级函数变为 $p(n) = \\lambda \\cdot 0.5 + (1-\\lambda)\\text{depth}(n)$。对于任何 $\\lambda \\in [0, 1)$，由于 $1-\\lambda > 0$，最大化 $p(n)$ 等同于最大化 $\\text{depth}(n)$。如果 $\\lambda=1$，$p(n)$ 变为常数，平局打破规则规定我们最大化 $\\text{depth}(n)$。因此，对于测试集中的任何 $\\lambda$，搜索行为都是相同的，导致相同的扩展数。根据元优化的平局打破规则，必须选择最小的 $\\lambda$，即 $0.00$。这为我们的实现提供了一个有用的​​一致性检查。", "answer": "```python\nimport numpy as np\n\n# Note: The problem statement lists scipy as a library, but it is not necessary\n# for this particular problem, so it is not imported.\n\ndef solve():\n    \"\"\"\n    Main solver function that orchestrates the meta-optimization process\n    for each training set.\n    \"\"\"\n\n    # --- Data Definition ---\n    TRAINING_SET_1 = [\n        {\n            \"id\": \"A1\",\n            \"root\": 0,\n            \"edges\": [(0,1),(0,2),(1,3),(1,4),(2,5),(2,6),(3,7),(4,8)],\n            \"goals\": {5},\n            \"utilities\": {0:0.20, 1:0.10, 2:0.90, 3:0.05, 4:0.05, 5:1.00, 6:0.20, 7:0.01, 8:0.01},\n        },\n        {\n            \"id\": \"A2\",\n            \"root\": 0,\n            \"edges\": [(0,1),(0,2),(1,3),(2,4),(2,5),(4,6),(5,7)],\n            \"goals\": {6},\n            \"utilities\": {0:0.20, 1:0.10, 2:0.80, 3:0.05, 4:0.90, 5:0.40, 6:1.00, 7:0.30},\n        },\n    ]\n\n    TRAINING_SET_2 = [\n        {\n            \"id\": \"M1\",\n            \"root\": 0,\n            \"edges\": [(0,1),(0,2),(1,3),(3,4),(4,5),(2,6),(2,7),(6,8),(7,9),(7,10)],\n            \"goals\": {5},\n            \"utilities\": {0:0.20, 1:0.10, 2:0.95, 3:0.20, 4:0.30, 5:1.00, 6:0.90, 7:0.85, 8:0.80, 9:0.75, 10:0.70},\n        },\n        {\n            \"id\": \"M2\",\n            \"root\": 0,\n            \"edges\": [(0,1),(0,2),(1,3),(3,4),(2,5),(2,6),(5,7),(6,8)],\n            \"goals\": {4},\n            \"utilities\": {0:0.10, 1:0.10, 2:0.90, 3:0.20, 4:1.00, 5:0.85, 6:0.80, 7:0.70, 8:0.60},\n        },\n    ]\n\n    TRAINING_SET_3 = [\n        {\n            \"id\": \"U1\",\n            \"root\": 0,\n            \"edges\": [(0,1),(0,2),(1,3),(2,4),(4,5),(5,6)],\n            \"goals\": {6},\n            \"utilities\": {n: 0.50 for n in range(7)},\n        },\n    ]\n\n    all_training_sets = [TRAINING_SET_1, TRAINING_SET_2, TRAINING_SET_3]\n    \n    # --- Class for Search Task ---\n    class SearchTask:\n        \"\"\"\n        Represents a single tree search task, including its structure,\n        goal, utilities, and pre-computed depths.\n        \"\"\"\n        def __init__(self, task_data):\n            self.root = task_data[\"root\"]\n            self.goals = task_data[\"goals\"]\n            self.utilities = task_data[\"utilities\"]\n            \n            # Build adjacency list\n            max_node = 0\n            if task_data[\"edges\"]:\n                max_node = max(max(p, c) for p, c in task_data[\"edges\"])\n            if self.utilities:\n                 max_node = max(max_node, max(self.utilities.keys()))\n\n            self.adj = {i: [] for i in range(max_node + 1)}\n            for parent, child in task_data[\"edges\"]:\n                self.adj[parent].append(child)\n            \n            # Pre-compute depths using BFS\n            self.depths = {}\n            if self.root not in self.adj and self.root not in self.utilities:\n                return # Empty tree\n            \n            queue = [(self.root, 0)]\n            visited = {self.root}\n            self.depths[self.root] = 0\n            head = 0\n            while head  len(queue):\n                curr, d = queue[head]\n                head += 1\n                for child in self.adj.get(curr, []):\n                    if child not in visited:\n                        visited.add(child)\n                        self.depths[child] = d + 1\n                        queue.append((child, d + 1))\n\n        def run_search(self, lambda_val):\n            \"\"\"\n            Performs the deterministic tree search for a given lambda.\n            Returns the number of nodes expanded.\n            \"\"\"\n            open_list = [(self.root)]\n            expanded_nodes = set()\n            expansion_count = 0\n            \n            while open_list:\n                # Sort open_list to find the best node to expand.\n                # The key is a tuple that implements the specified priority and tie-breaking rules.\n                # 1. Maximize p(n)\n                # 2. Maximize depth(n)\n                # 3. Minimize node ID\n                open_list.sort(\n                    key=lambda node_id: (\n                        lambda_val * self.utilities[node_id] + (1 - lambda_val) * self.depths[node_id],\n                        self.depths[node_id],\n                        -node_id\n                    ),\n                    reverse=True\n                )\n                \n                current_node = open_list.pop(0)\n\n                if current_node in expanded_nodes:\n                    continue\n                \n                expanded_nodes.add(current_node)\n                expansion_count += 1\n                \n                if current_node in self.goals:\n                    return expansion_count\n                \n                # Add children to open_list\n                for child in self.adj.get(current_node, []):\n                    if child not in expanded_nodes:\n                        open_list.append(child)\n            \n            return expansion_count\n\n    # --- Meta-optimization logic ---\n    def find_optimal_lambda(task_list_data):\n        \"\"\"\n        Finds the optimal lambda for a given training set.\n        \"\"\"\n        tasks = [SearchTask(data) for data in task_list_data]\n        lambdas_to_test = [0.00, 0.25, 0.50, 0.75, 1.00]\n        \n        results = []\n        for lam in lambdas_to_test:\n            total_expansions = 0\n            for task in tasks:\n                total_expansions += task.run_search(lam)\n            results.append((total_expansions, lam))\n            \n        # Tie-break by smallest lambda if expansions are equal\n        results.sort(key=lambda x: (x[0], x[1]))\n        \n        return results[0][1]\n\n    # --- Main Execution Loop ---\n    optimal_lambdas = []\n    for training_set in all_training_sets:\n        best_lambda = find_optimal_lambda(training_set)\n        optimal_lambdas.append(f\"{best_lambda:.2f}\")\n\n    print(f\"[{','.join(optimal_lambdas)}]\")\n\nsolve()\n```", "id": "3157413"}]}