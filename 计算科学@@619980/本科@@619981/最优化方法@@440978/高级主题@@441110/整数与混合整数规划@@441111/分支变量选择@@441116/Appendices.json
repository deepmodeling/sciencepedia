{"hands_on_practices": [{"introduction": "在选择分支变量时，一个自然的直觉是优先考虑那些在目标函数中具有最大系数的变量，因为它们似乎对解的质量影响最大。然而，这种简单的启发式方法可能具有误导性。这个练习 [@problem_id:3104763] 通过一个精心设计的例子，展示了一个目标函数系数很小的变量如何由于其在约束结构中的关键耦合作用，在分支时产生意想不到的巨大边界改进。通过这个计算，你将深刻理解到，有效的变量选择必须超越目标函数，审视变量在整个问题约束系统中的全局角色。", "problem": "考虑以下针对一个最大化问题的混合整数线性规划 (MILP)，其二元决策变量为 $x_1$、$x_2$ 和 $x_3$：\n最大化 $8 x_1 + 7 x_2 + 0.1 x_3$\n约束条件为\n$2 x_1 + 2 x_2 + 3 x_3 \\le 4$，\n$x_1 + x_2 - x_3 \\le 1$，\n$0 \\le x_i \\le 1$ 对于 $i \\in \\{1,2,3\\}$，\n$x_i \\in \\{0,1\\}$ 对于 $i \\in \\{1,2,3\\}$。\n\n在一个分支定界 (B&B) 树的根节点，使用线性规划 (LP) 松弛，即去掉整数性要求 $x_i \\in \\{0,1\\}$，只保留 $0 \\le x_i \\le 1$。令 $z_{\\text{parent}}$ 表示根节点处的最优 LP 目标值。考虑在根节点对变量 $x_3$ 进行分支，分别创建两个子节点，对应 $x_3 = 0$ 和 $x_3 = 1$。令 $z_0$ 和 $z_1$ 表示这两个子节点的最优 LP 目标值。\n\n定义对 $x_3$ 分支所带来的平均界限改进量为\n$\\Delta_{\\text{avg}} = \\dfrac{\\left(z_{\\text{parent}} - z_0\\right) + \\left(z_{\\text{parent}} - z_1\\right)}{2}$。\n计算此实例的 $\\Delta_{\\text{avg}}$。然后，在你的解答中，通过引用旁约束如何与 $x_3$ 相互作用，解释为什么对 $x_3$（其目标系数 0.1 很小）进行分支仍然能产生较大的平均界限改进量。\n\n将 $\\Delta_{\\text{avg}}$ 的最终数值四舍五入到 3 位有效数字。最终答案中不需要也不允许使用单位。", "solution": "我们从混合整数优化的基本定义开始：混合整数线性规划 (MILP) 的线性规划 (LP) 松弛为最大化问题中的真实最优值提供了一个上界，而分支定界 (B&B) 中对变量进行分支会将可行域分割成不相交的子问题，从而可以收紧这个界限。分支处的界限改进量是通过 LP 上界相对于父节点的减少量来衡量的。我们将计算父节点的 LP 界 $z_{\\text{parent}}$、子节点的 LP 界 $z_0$ 和 $z_1$，然后计算平均界限改进量 $\\Delta_{\\text{avg}}$。\n\n步骤 $1$：求解根节点的 LP 松弛。根节点的 LP 松弛为\n最大化 $8 x_1 + 7 x_2 + 0.1 x_3$\n约束条件为\n$2 x_1 + 2 x_2 + 3 x_3 \\le 4$，\n$x_1 + x_2 - x_3 \\le 1$，\n$0 \\le x_i \\le 1$ 对于 $i \\in \\{1,2,3\\}$。\n\n引入聚合变量 $s = x_1 + x_2$。约束条件变为\n$2 s + 3 x_3 \\le 4$，\n$s \\le 1 + x_3$，\n$0 \\le s \\le 2$，\n$0 \\le x_3 \\le 1$，\n目标函数变为 $8 x_1 + 7 x_2 + 0.1 x_3$。对于固定的 $s$，通过将尽可能多的 $s$ 分配给系数较大的变量，可以最大化表达式 $8 x_1 + 7 x_2$。因此，最优分配是：\n若 $s \\le 1$，则令 $x_1 = s, x_2 = 0$，得到 $8 x_1 + 7 x_2 = 8 s$，\n若 $1 \\le s \\le 2$，则令 $x_1 = 1, x_2 = s - 1$，得到 $8 x_1 + 7 x_2 = 7 s + 1$。\n\n因此，目标函数变为\n$O(s,x_3) = \\begin{cases}\n8 s + 0.1 x_3, & 0 \\le s \\le 1, \\\\\n7 s + 1 + 0.1 x_3, & 1 \\le s \\le 2,\n\\end{cases}$\n约束条件为\n$s \\le \\min\\{2 - 1.5 x_3, 1 + x_3\\}$，\n$0 \\le x_3 \\le 1$。\n\n对于每个 $x_3$，最优的 $s$ 是最大可行 $s$，因为 $O(s,x_3)$ 在两种情况下都随 $s$ 的增加而增加。比较两个上界：\n$2 - 1.5 x_3 \\le 1 + x_3$ 在 $1 - 2.5 x_3 \\le 0$ 时成立，即 $x_3 \\ge 0.4$。\n因此，\n对于 $0 \\le x_3 \\le 0.4$，更紧的约束是 $s \\le 1 + x_3$，所以选择 $s = 1 + x_3$，\n对于 $0.4 \\le x_3 \\le 1$，更紧的约束是 $s \\le 2 - 1.5 x_3$，所以选择 $s = 2 - 1.5 x_3$。\n\n情况 1：$0 \\le x_3 \\le 0.4$。此时 $s = 1 + x_3 \\in [1,1.4]$，所以我们处于 $1 \\le s \\le 2$ 的情况，并且\n$O(x_3) = 7 s + 1 + 0.1 x_3 = 7(1 + x_3) + 1 + 0.1 x_3 = 8 + 7.1 x_3$，\n该函数随 $x_3$ 递增，因此在这种情况下，最大值出现在 $x_3 = 0.4$。\n\n情况 2：$0.4 \\le x_3 \\le 1$。此时 $s = 2 - 1.5 x_3$。根据 $s \\ge 1$ 是否成立，存在两种子情况。\n子情况 2a：$0.4 \\le x_3 \\le \\tfrac{2}{3}$。此时 $s \\ge 1$，所以\n$O(x_3) = 7 s + 1 + 0.1 x_3 = 7(2 - 1.5 x_3) + 1 + 0.1 x_3 = 15 - 10.4 x_3$，\n该函数随 $x_3$ 递减，因此在此子区间上的最大值出现在 $x_3 = 0.4$。\n子情况 2b：$\\tfrac{2}{3} \\le x_3 \\le 1$。此时 $s \\le 1$，所以\n$O(x_3) = 8 s + 0.1 x_3 = 8(2 - 1.5 x_3) + 0.1 x_3 = 16 - 11.9 x_3$，\n该函数也随 $x_3$ 递减，其最大值出现在 $x_3 = \\tfrac{2}{3}$，但该值严格低于接下来计算的 $x_3 = 0.4$ 处的值。\n\n因此，全局最大值点在边界 $x_3 = 0.4$ 处，对应的 $s = 1.4$。在 $1 \\le s \\le 2$ 的情况下，分配为 $x_1 = 1, x_2 = s - 1 = 0.4$。因此，根节点 LP 的最优目标值为\n$z_{\\text{parent}} = 8 \\cdot 1 + 7 \\cdot 0.4 + 0.1 \\cdot 0.4 = 8 + 2.8 + 0.04 = 10.84$。\n\n步骤 2：求解对 $x_3$ 分支得到的两个子节点的 LP 松弛。\n\n$x_3 = 0$ 的子节点：\n约束变为 $2 x_1 + 2 x_2 \\le 4$ 和 $x_1 + x_2 \\le 1$。合并得到 $x_1 + x_2 \\le 1$。为最大化 $8 x_1 + 7 x_2$，令 $x_1 = 1, x_2 = 0$。该子节点的 LP 界为\n$z_0 = 8 \\cdot 1 + 7 \\cdot 0 + 0.1 \\cdot 0 = 8$。\n\n$x_3 = 1$ 的子节点：\n约束变为 $2 x_1 + 2 x_2 + 3 \\le 4$，即 $x_1 + x_2 \\le 0.5$，以及 $x_1 + x_2 \\le 2$（此处非紧）。为最大化 $8 x_1 + 7 x_2$，将所有的 0.5 分配给 $x_1$：$x_1 = 0.5, x_2 = 0$。该子节点的 LP 界为\n$z_1 = 8 \\cdot 0.5 + 7 \\cdot 0 + 0.1 \\cdot 1 = 4 + 0.1 = 4.1$。\n\n步骤 3：计算平均界限改进量。根据定义，\n$\\Delta_{\\text{avg}} = \\dfrac{\\left(z_{\\text{parent}} - z_0\\right) + \\left(z_{\\text{parent}} - z_1\\right)}{2}\n= \\dfrac{\\left(10.84 - 8\\right) + \\left(10.84 - 4.1\\right)}{2}\n= \\dfrac{2.84 + 6.74}{2}\n= \\dfrac{9.58}{2}\n= 4.79$。\n\n现象的解释和说明：尽管 $x_3$ 的目标系数很小（即 0.1），但旁约束 $x_1 + x_2 - x_3 \\le 1$ 将 $x_3$ 与高利润变量 $x_1$ 和 $x_2$ 紧密地耦合在一起。在根节点的 LP 松弛中，将 $x_3$ 增加 $\\Delta$ 允许和 $x_1 + x_2$ 增加最多 $\\Delta$，由于 $x_1$ 和 $x_2$ 的系数较大，这可以被用来将目标函数值增加大约 $7 \\Delta$ 到 $8 \\Delta$。这就是为什么 LP 将 $x_3$ 设为 0.4，即使其直接贡献仅为 0.04。当我们对 $x_3$ 进行分支并强制 $x_3 = 0$ 或 $x_3 = 1$ 时，我们破坏了这种小数的“促成”效应：当 $x_3 = 0$ 时，耦合约束将 $x_1 + x_2$ 的上限限制为 1；而当 $x_3 = 1$ 时，背包约束 $2 x_1 + 2 x_2 + 3 x_3 \\le 4$ 将 $x_1 + x_2$ 的上限限制为 0.5。两个分支都严重降低了 LP 上界，从而产生了较大的平均界限改进量 $\\Delta_{\\text{avg}} = 4.79$。这说明，当紧密的旁约束将一个目标系数很小的变量与高利润变量强力联系在一起时，对该变量进行分支可以带来出乎意料的强劲界限改进。", "answer": "$$\\boxed{4.79}$$", "id": "3104763"}, {"introduction": "认识到简单启发式方法的局限性后，我们转向一种更强大、更可靠的策略：强分支（strong branching）。该方法通过对每个候选分数变量进行试探性分支，并计算其子节点产生的实际边界改进来评估其潜力。虽然效果显著，但强分支的计算成本高昂，且常常出现多个变量得分相似的情况，此时需要一个有效的决胜局规则。这个编程练习 [@problem_id:3104701] 要求你实现一个两级选择策略，它不仅使用强分支作为主要标准，还引入了一个新颖的“隐含约束收紧”评分作为决胜局规则，从而在多个优秀候选中做出最明智的选择。", "problem": "给定一组二元决策变量 $x \\in \\{0,1\\}^n$ 以及一个具有线性目标和线性约束的混合整数线性规划问题。分支定界 (B&B) 方法依赖于在当前线性规划 (LP) 松弛解存在分数项时选择一个分支变量。在强分支中，变量选择的依据是评估将候选变量 $x_i$ 分别固定为 $0$ 和 $1$ 所产生的两个子节点的 LP 松弛界，并对预期的界限改善进行评分。在实践中，多个分数变量可能通过一个或多个共享约束紧密耦合，这可能导致这些变量的强分支得分相似。在这种情况下，需要一个次要标准（决胜标准）来选择能最大化紧约束隐含紧致化的变量。\n\n从以下基本事实出发：\n- 混合整数线性规划的 LP 松弛将整数约束 $x_i \\in \\{0,1\\}$ 替换为 $0 \\le x_i \\le 1$，并在最大化问题中产生一个上界。\n- 对变量 $x_i$ 进行分支会通过添加 $x_i = 0$ 或 $x_i = 1$（通过界限实现）创建两个子问题，这在最大化设置中只会降低或维持 LP 松弛界。\n- 如果 $A x^* = b$，则约束 $A x \\le b$ 在解 $x^*$ 处是紧约束；等式约束 $A x = b$ 在任何可行解处始终是紧约束。\n\n您的任务是构建一个程序，为下面提供的每个测试用例执行以下步骤：\n1. 求解一个最大化问题的父 LP 松弛\n   $$\n   \\max_{x \\in \\mathbb{R}^n} \\; p^\\top x \\quad \\text{subject to} \\quad A_{\\text{ub}} x \\le b_{\\text{ub}}, \\; A_{\\text{eq}} x = b_{\\text{eq}}, \\; 0 \\le x \\le 1,\n   $$\n   其中 $p \\in \\mathbb{R}^n$ 是利润向量，$A_{\\text{ub}} \\in \\mathbb{R}^{m_{\\text{ub}} \\times n}$ 和 $b_{\\text{ub}} \\in \\mathbb{R}^{m_{\\text{ub}}}$ 定义了不等式约束，$A_{\\text{eq}} \\in \\mathbb{R}^{m_{\\text{eq}} \\times n}$ 和 $b_{\\text{eq}} \\in \\mathbb{R}^{m_{\\text{eq}}}$ 定义了等式约束。这些约束可以代表（例如）预算限制和基数要求。令最优 LP 松弛解为 $x^*$，目标值为 $z^* = p^\\top x^*$。\n2. 对于一个小的容差 $\\epsilon$，识别分数变量集合 $\\mathcal{F} = \\{ i \\in \\{1,\\dots,n\\} \\mid \\epsilon < x^*_i < 1 - \\epsilon \\}$。\n3. 对于每个 $i \\in \\mathcal{F}$，通过求解两个子 LP 松弛来进行强分支：\n   - “向下”分支添加 $x_i = 0$，\n   - “向上”分支添加 $x_i = 1$。\n   令 $z^{\\downarrow}_i$ 和 $z^{\\uparrow}_i$ 表示子 LP 的最优值。通过以下公式定义变量 $i$ 的强分支得分\n   $$\n   S_i = \\max(0, z^* - z^{\\downarrow}_i) + \\max(0, z^* - z^{\\uparrow}_i).\n   $$\n   这量化了因对 $x_i$ 进行分支而导致的 LP 上界（对于最大化问题）的总减少量。\n4. 通过检查是否满足\n   $$\n   \\max_{i \\in \\mathcal{F}} S_i - \\min_{i \\in \\mathcal{F}} S_i \\le \\tau,\n   $$\n   来分析对任何分数变量进行分支是否产生相似的界，其中 $\\tau$ 是一个小的非负容差。\n5. 提出并应用一个决胜标准，该标准旨在最大化紧约束中的隐含约束紧致化。具体来说，令 $\\mathcal{B}_{\\text{ub}} = \\{ j \\mid b_{\\text{ub},j} - (A_{\\text{ub}} x^*)_j \\le \\delta \\}$ 为在解 $x^*$ 处的紧不等式约束集合（对于一个小的容差 $\\delta$），并将所有等式约束视为紧约束。对于一个分数变量 $i$，定义其四舍五入目标 $\\mathrm{round}(x^*_i) = 1$（如果 $x^*_i \\ge \\tfrac{1}{2}$）和 $0$（否则），并令 $\\Delta_i = |\\mathrm{round}(x^*_i) - x^*_i|$。然后定义隐含紧致化得分\n   $$\n   T_i = \\sum_{j \\in \\mathcal{B}_{\\text{ub}}} \\max(0, A_{\\text{ub},j i}) \\, \\Delta_i \\;+\\; \\sum_{j=1}^{m_{\\text{eq}}} |A_{\\text{eq},j i}| \\, \\Delta_i.\n   $$\n   这衡量了将 $x_i$ 固定到其最近的整数会迫使紧约束通过调整其他变量而收紧的程度。当强分支得分在 $\\tau$ 范围内相似时，选择具有最大 $T_i$ 的变量；否则，选择具有最大 $S_i$ 的变量。如果出现平局，优先选择较大的 $T_i$，如果仍然平局，则选择最小的索引 $i$。\n\n实现上述逻辑，并将其应用于以下测试套件。每个测试用例由 $(p, A_{\\text{ub}}, b_{\\text{ub}}, A_{\\text{eq}}, b_{\\text{eq}})$ 指定：\n- 测试用例 1（理想情况，多个分数变量由一个等式和一个不等式耦合）：\n  - $n = 5$\n  - $p = [5, 5, 5, 1, 1]$\n  - $A_{\\text{ub}} = \\begin{bmatrix} 4 & 4 & 4 & 1 & 1 \\end{bmatrix}$, $b_{\\text{ub}} = [10]$\n  - $A_{\\text{eq}} = \\begin{bmatrix} 1 & 1 & 1 & 1 & 1 \\end{bmatrix}$, $b_{\\text{eq}} = [3]$\n- 测试用例 2（边界情况，一个分数变量可能主导强分支的改进）：\n  - $n = 6$\n  - $p = [6, 6, 6, 2, 2, 2]$\n  - $A_{\\text{ub}} = \\begin{bmatrix} 5 & 5 & 5 & 1 & 1 & 1 \\end{bmatrix}$, $b_{\\text{ub}} = [11]$\n  - $A_{\\text{eq}} = \\begin{bmatrix} 1 & 1 & 1 & 1 & 1 & 1 \\end{bmatrix}$, $b_{\\text{eq}} = [4]$\n- 测试用例 3（边缘情况，两个不等式约束通过对偶耦合产生分数变量）：\n  - $n = 5$\n  - $p = [3, 3, 3, 3, 3]$\n  - $A_{\\text{ub}} = \\begin{bmatrix} 4 & 4 & 4 & 1 & 1 \\\\ 1 & 1 & 1 & 1 & 1 \\end{bmatrix}$, $b_{\\text{ub}} = [9, 3.2]$\n  - $A_{\\text{eq}}$ 缺失, $b_{\\text{eq}}$ 缺失\n\n为保证数值鲁棒性，使用 $\\epsilon = 10^{-8}$ 来检测分数变量，使用 $\\tau = 10^{-6}$ 进行强分支相似性检查，并使用 $\\delta = 10^{-8}$ 来检测紧不等式。\n\n最终输出格式：您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，列表中的每个元素都是一个双项列表 $[\\text{similar}, \\text{index}]$，对应一个测试用例。第一项 $\\text{similar}$ 是一个布尔值，指示分数变量的强分支得分是否在容差 $\\tau$ 之内，第二项 $\\text{index}$ 是根据所述规则选择的分支变量的从零开始的索引（如果没有分数变量，则使用 $-1$）。例如：$[[\\text{True}, 2],[\\text{False}, 1],[\\text{True}, 3]]$。\n\n此问题不涉及物理单位、角度或百分比。程序必须是一个完整的、可运行的程序。", "solution": "该问题要求为应用于最大化混合整数线性规划 (MILP) 的分支定界算法实现一个特定的两级变量选择策略。主要选择标准是强分支，并使用一种新颖的“隐含紧致化得分”作为决胜标准。该过程将应用于一组测试用例。\n\n求解方法严格遵循问题陈述中详述的步骤。\n\n1.  **父 LP 松弛解**：对于每个测试用例，我们首先求解给定 MILP 的父线性规划 (LP) 松弛。整数约束 $x_i \\in \\{0,1\\}$ 被松弛为 $0 \\le x_i \\le 1$。问题被表述为：\n    $$\n    \\max_{x \\in \\mathbb{R}^n} \\; p^\\top x \\quad \\text{subject to} \\quad A_{\\text{ub}} x \\le b_{\\text{ub}}, \\; A_{\\text{eq}} x = b_{\\text{eq}}, \\; 0 \\le x \\le 1\n    $$\n    我们利用 `scipy.optimize.linprog` 函数来完成此任务。由于此函数执行最小化，因此通过提供 $c = -p$ 来反转目标。最优解向量表示为 $x^*$，最优目标值（上界）表示为 $z^* = -(\\text{result.fun})$。\n\n    一个关键的微妙之处在于，一个 LP 可能拥有多个最优解（即，最优解集是可行多胞体的一个面，不一定是一个顶点）。返回的具体向量 $x^*$ 可能取决于求解器的算法。因此，分数变量集 $\\mathcal{F}$ 和紧约束集 $\\mathcal{B}_{\\text{ub}}$ 可能会因 $x^*$ 的选择而异。然而，该问题在计算上是明确定义的，因为像 SciPy 中提供的确定性 LP 求解器将返回一个单一、特定的解，使得算法的后续步骤明确无误。\n\n2.  **分数变量的识别**：根据父 LP 解 $x^*$，我们使用指定的容差 $\\epsilon = 10^{-8}$ 识别分数变量集 $\\mathcal{F} = \\{ i \\mid \\epsilon < x^*_i < 1 - \\epsilon \\}$。如果该集合为空，则无需分支，该测试用例的处理过程终止。\n\n3.  **强分支得分 ($S_i$) 计算**：对于每个分数变量 $x_i$（其中 $i \\in \\mathcal{F}$），我们通过求解两个子 LP 来模拟分支：\n    -   **向下分支**：通过将变量 $i$ 的界限设置为 $(0, 0)$，将约束 $x_i = 0$ 添加到父 LP 中。得到的最优值为 $z^{\\downarrow}_i$。\n    -   **向上分支**：通过将变量 $i$ 的界限设置为 $(1, 1)$，将约束 $x_i = 1$ 添加到父 LP 中。得到的最优值为 $z^{\\uparrow}_i$。\n\n    变量 $i$ 的强分支得分随后计算为两个分支中目标退化的总和：\n    $$\n    S_i = \\max(0, z^* - z^{\\downarrow}_i) + \\max(0, z^* - z^{\\uparrow}_i)\n    $$\n    $\\max(0, \\cdot)$ 项确保了非负性，这是预期的，因为对最大化问题进行分支不会改善 LP 松弛界。\n\n4.  **相似性检查**：我们评估强分支得分是否聚集在一起。如果至少有两个分数变量，我们计算得分的范围。如果 $\\max_{i \\in \\mathcal{F}} S_i - \\min_{i \\in \\mathcal{F}} S_i \\le \\tau$（其中 $\\tau = 10^{-6}$），则认为得分“相似”。这个布尔结果决定了使用哪种选择标准。\n\n5.  **隐含紧致化得分 ($T_i$) 计算**：该得分用作决胜标准。它为每个分数变量 $i \\in \\mathcal{F}$ 计算：\n    - 首先，我们在父解 $x^*$ 处识别紧约束。所有等式约束都是紧的。如果不等式约束 $j$ 的松弛量 $b_{\\text{ub},j} - (A_{\\text{ub}} x^*)_j$ 小于或等于容差 $\\delta = 10^{-8}$，则该约束被认为是紧的。\n    - 对于每个变量 $i \\in \\mathcal{F}$，我们确定其四舍五入目标 $\\mathrm{round}(x^*_i)$ 和所需的变化量 $\\Delta_i = |\\mathrm{round}(x^*_i) - x^*_i|$。\n    - 得分 $T_i$ 是所有紧约束中加权系数量级的总和，并按 $\\Delta_i$ 缩放：\n      $$\n      T_i = \\Delta_i \\left( \\sum_{j \\in \\mathcal{B}_{\\text{ub}}} \\max(0, A_{\\text{ub},j i}) + \\sum_{j=1}^{m_{\\text{eq}}} |A_{\\text{eq},j i}| \\right)\n      $$\n      其中 $\\mathcal{B}_{\\text{ub}}$ 是紧不等式约束的索引集。对于不等式，只有正系数有贡献，因为当变量值改变时，这些系数会强制产生紧致化。\n\n6.  **最终变量选择**：分支变量的选择遵循严格的层次化规则：\n    -   如果强分支得分相似，主要标准是选择具有最大紧致化得分 $T_i$ 的变量。\n    -   如果得分不相似，主要标准是选择具有最大强分支得分 $S_i$ 的变量。\n    -   主要标准中的任何平局都通过选择具有较大 $T_i$ 得分的变量来打破。\n    -   如果仍然存在平局，则通过选择具有最小索引 $i$ 的变量来解决。\n\n这一完整逻辑被封装在提供的 Python 程序中，该程序处理每个测试用例并报告选择过程的结果。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import linprog\n\ndef solve():\n    \"\"\"\n    Main function to orchestrate the variable selection process for all test cases.\n    \"\"\"\n    # Define the test cases as specified in the problem statement.\n    test_cases = [\n        # Test Case 1\n        {\n            \"p\": np.array([5, 5, 5, 1, 1], dtype=float),\n            \"A_ub\": np.array([[4, 4, 4, 1, 1]], dtype=float),\n            \"b_ub\": np.array([10], dtype=float),\n            \"A_eq\": np.array([[1, 1, 1, 1, 1]], dtype=float),\n            \"b_eq\": np.array([3], dtype=float),\n        },\n        # Test Case 2\n        {\n            \"p\": np.array([6, 6, 6, 2, 2, 2], dtype=float),\n            \"A_ub\": np.array([[5, 5, 5, 1, 1, 1]], dtype=float),\n            \"b_ub\": np.array([11], dtype=float),\n            \"A_eq\": np.array([[1, 1, 1, 1, 1, 1]], dtype=float),\n            \"b_eq\": np.array([4], dtype=float),\n        },\n        # Test Case 3\n        {\n            \"p\": np.array([3, 3, 3, 3, 3], dtype=float),\n            \"A_ub\": np.array([[4, 4, 4, 1, 1], [1, 1, 1, 1, 1]], dtype=float),\n            \"b_ub\": np.array([9, 3.2], dtype=float),\n            \"A_eq\": None,\n            \"b_eq\": None,\n        }\n    ]\n\n    # Tolerances\n    epsilon = 1e-8\n    tau = 1e-6\n    delta = 1e-8\n\n    results = []\n    for case in test_cases:\n        result = process_case(case, epsilon, tau, delta)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    # The default string representation of a list of lists matches the required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\n\ndef process_case(case_data, epsilon, tau, delta):\n    \"\"\"\n    Processes a single test case according to the specified logic.\n    \"\"\"\n    p = case_data[\"p\"]\n    A_ub = case_data[\"A_ub\"]\n    b_ub = case_data[\"b_ub\"]\n    A_eq = case_data[\"A_eq\"]\n    b_eq = case_data[\"b_eq\"]\n    \n    n = len(p)\n    c = -p\n    bounds = [(0, 1)] * n\n\n    # 1. Solve the parent LP relaxation.\n    res_parent = linprog(c, A_ub=A_ub, b_ub=b_ub, A_eq=A_eq, b_eq=b_eq, bounds=bounds, method='highs')\n    \n    if not res_parent.success:\n        # This case is not expected for the given problems.\n        # Fallback for ill-defined problems.\n        return [False, -1]\n        \n    x_star = res_parent.x\n    z_star = -res_parent.fun\n\n    # 2. Identify the set of fractional variables.\n    F_indices = [i for i, x_val in enumerate(x_star) if epsilon  x_val  1 - epsilon]\n    \n    if not F_indices:\n        return [False, -1]\n    \n    s_scores = {}\n    \n    # 3. For each fractional variable, perform strong branching.\n    for i in F_indices:\n        # Down branch (x_i = 0)\n        bounds_down = bounds[:]\n        bounds_down[i] = (0, 0)\n        res_down = linprog(c, A_ub=A_ub, b_ub=b_ub, A_eq=A_eq, b_eq=b_eq, bounds=bounds_down, method='highs')\n        z_down_i = -res_down.fun if res_down.success else -np.inf\n        \n        # Up branch (x_i = 1)\n        bounds_up = bounds[:]\n        bounds_up[i] = (1, 1)\n        res_up = linprog(c, A_ub=A_ub, b_ub=b_ub, A_eq=A_eq, b_eq=b_eq, bounds=bounds_up, method='highs')\n        z_up_i = -res_up.fun if res_up.success else -np.inf\n        \n        s_scores[i] = max(0, z_star - z_down_i) + max(0, z_star - z_up_i)\n\n    # 4. Analyze strong branching score similarity.\n    similar = False\n    if len(F_indices) > 1:\n        max_s = max(s_scores.values())\n        min_s = min(s_scores.values())\n        if max_s - min_s = tau:\n            similar = True\n\n    # 5. Propose and apply the tiebreaker score T_i.\n    t_scores = {}\n    \n    # Identify binding inequality constraints.\n    binding_ub_indices = []\n    if A_ub is not None:\n        slacks = b_ub - (A_ub @ x_star)\n        binding_ub_indices = [j for j, slack in enumerate(slacks) if slack = delta]\n    \n    for i in F_indices:\n        round_xi = 1.0 if x_star[i] >= 0.5 else 0.0\n        delta_i = abs(round_xi - x_star[i])\n        \n        tightening_sum = 0.0\n        # Sum over binding inequality constraints.\n        if A_ub is not None:\n             for j in binding_ub_indices:\n                tightening_sum += max(0, A_ub[j, i])\n        \n        # Sum over all equality constraints (always binding).\n        if A_eq is not None:\n            for j in range(A_eq.shape[0]):\n                tightening_sum += abs(A_eq[j, i])\n                \n        t_scores[i] = delta_i * tightening_sum\n        \n    # 6. Select the branching variable based on the hierarchical rule.\n    candidates = [(s_scores[i], t_scores[i], i) for i in F_indices]\n    \n    if similar:\n        # Primary sort: T_i (desc), Secondary sort: index (asc)\n        candidates.sort(key=lambda x: (-x[1], x[2]))\n    else:\n        # Primary sort: S_i (desc), Secondary: T_i (desc), Tertiary: index (asc)\n        candidates.sort(key=lambda x: (-x[0], -x[1], x[2]))\n        \n    best_index = candidates[0][2]\n    \n    return [similar, best_index]\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3104701"}, {"introduction": "除了像强分支那样通过直接模拟来评估变量，我们还可以构建更轻量级的启发式评分函数来指导决策。这类方法旨在用更少的计算量捕捉变量的重要性。这个练习 [@problem_id:3104685] 将指导你开发一个创新的加权分数性评分规则。该规则巧妙地结合了两个关键因素：变量的LP松弛解 $x_i^*$ 与半整数 $0.5$ 的接近程度，以及一个反映该变量在“紧”约束（即接近饱和的约束）中参与度的权重 $w_i$。通过实现这个结合了变量数值特性和问题结构特性的启发式规则，你将学会一种在实践中平衡决策质量与计算效率的常用技巧。", "problem": "要求您设计并实现一个程序，该程序基于线性规划 (LP) 松弛的输出，为混合整数线性规划 (MILP) 选择一个分支变量。该程序必须基于优化的基本原理，并且必须计算一个加权度量来衡量每个变量作为分支变量的适合程度。从线性规划和混合整数线性规划的核心定义出发，推导一个基于约束重要性对变量进行加权的规则，并将其与一个衡量小数部分与半整数接近程度的度量相结合。最终的选择必须是使该加权差异最小化的变量索引（使用从零开始的索引）。\n\n使用的基本定义：\n- 混合整数线性规划 (MILP) 问题的线性规划 (LP) 松弛由一组线性约束和一个连续决策向量定义。考虑形式为 $A x \\le b$ 的约束，其中 $A \\in \\mathbb{R}^{m \\times n}$，$b \\in \\mathbb{R}^{m}$，而 $x \\in \\mathbb{R}^{n}$ 是 LP 松弛解。第 $j$ 个约束的松弛量为 $s_j = b_j - (A x)_j$，对于一个可行的 LP 解，有 $s_j \\ge 0$。\n- 变量的小数部分定义为 $\\operatorname{frac}(x_i) = x_i - \\lfloor x_i \\rfloor$。\n- 分支变量选择旨在选择一个预期分支后能显著收紧松弛的变量。根据经验，小数部分最接近 $0.5$ 的变量影响较大，而深度参与紧约束的变量应被赋予更高的权重。\n\n您的任务：\n1. 对于每个模型，仅使用在 LP 解处可用的量，为每个约束 $j$ 计算一个约束重要性得分。设约束重要性定义为 $I_j = \\dfrac{1}{1 + s_j}$，其中 $s_j$ 是约束 $j$ 的松弛量。该公式通过为紧约束（即 $s_j$ 较小）分配较大的 $I_j$ 来强调它们。\n2. 对于每个变量 $i$，计算一个权重 $w_i$，该权重聚合了其在各约束中的参与度（相对于约束的重要性）。使用系数的绝对值来衡量参与度：$w_i = \\sum_{j=1}^{m} I_j \\, |a_{j,i}|$，其中 $a_{j,i}$ 是 $A$ 的 $(j,i)$ 项。\n3. 对于每个变量 $i$，计算其小数部分与 $0.5$ 的绝对偏差，即 $d_i = \\left| \\operatorname{frac}(x_i^*) - 0.5 \\right|$，其中 $x_i^*$ 是变量 $i$ 的 LP 松弛值。\n4. 通过结合权重和偏差来定义选择得分 $S_i$，即 $S_i = w_i \\cdot d_i$。如果 $w_i = 0$，则将 $S_i$ 视为 $+\\infty$，以将不参与任何约束的变量排除在考虑范围之外。\n5. 选择具有最小得分 $S_i$ 的变量索引 $i^\\star$。如果在小的容差范围（使用绝对容差 $\\epsilon = 10^{-9}$）内存在平局，则通过选择具有最大 $w_i$ 的候选者来打破平局，如果仍然平局，则选择最小的索引。\n\n实现上述过程，并将其应用于以下模型测试集。在每个模型中，所有变量都被视为整数约束，且 LP 松弛解 $x^*$ 是可行的。\n\n模型 $\\mathbf{1}$ (理想情况，具有混合小数部分):\n- 约束矩阵:\n$$\nA^{(1)} = \\begin{bmatrix}\n1  2  0  1 \\\\\n0  1  3  1 \\\\\n2  0  1  0\n\\end{bmatrix}\n$$\n- 右端项:\n$$\nb^{(1)} = \\begin{bmatrix} 5 \\\\ 6 \\\\ 3 \\end{bmatrix}\n$$\n- LP 松弛解:\n$$\nx^{*(1)} = \\begin{bmatrix} 0.8 \\\\ 1.5 \\\\ 0.2 \\\\ 1.1 \\end{bmatrix}\n$$\n\n模型 $\\mathbf{2}$ (边界情况，具有一个整数和一个半整数小数部分):\n- 约束矩阵:\n$$\nA^{(2)} = \\begin{bmatrix}\n1  1  1 \\\\\n2  0  1\n\\end{bmatrix}\n$$\n- 右端项:\n$$\nb^{(2)} = \\begin{bmatrix} 4.0 \\\\ 5.0 \\end{bmatrix}\n$$\n- LP 松弛解:\n$$\nx^{*(2)} = \\begin{bmatrix} 2.0 \\\\ 1.49 \\\\ 0.5 \\end{bmatrix}\n$$\n\n模型 $\\mathbf{3}$ (平局情况：两个变量的小数部分均为半整数):\n- 约束矩阵:\n$$\nA^{(3)} = \\begin{bmatrix}\n1  0  1  2 \\\\\n0  2  1  0\n\\end{bmatrix}\n$$\n- 右端项:\n$$\nb^{(3)} = \\begin{bmatrix} 4.0 \\\\ 4.0 \\end{bmatrix}\n$$\n- LP 松弛解:\n$$\nx^{*(3)} = \\begin{bmatrix} 0.5 \\\\ 1.5 \\\\ 0.75 \\\\ 1.25 \\end{bmatrix}\n$$\n\n模型 $\\mathbf{4}$ (边缘情况，一个零权重变量被排除在选择之外):\n- 约束矩阵:\n$$\nA^{(4)} = \\begin{bmatrix}\n1  1  0\n\\end{bmatrix}\n$$\n- 右端项:\n$$\nb^{(4)} = \\begin{bmatrix} 1.4 \\end{bmatrix}\n$$\n- LP 松弛解:\n$$\nx^{*(4)} = \\begin{bmatrix} 0.3 \\\\ 0.7 \\\\ 0.49 \\end{bmatrix}\n$$\n\n您的程序必须：\n- 为四个模型中的每一个实现上述五步流程。\n- 生成一行输出，其中包含四个模型选定索引的逗号分隔列表，并用方括号括起来，使用从零开始的索引。例如，输出格式必须严格为 $[i_1,i_2,i_3,i_4]$ 的形式，其中每个 $i_k$ 都是一个整数。\n\n不涉及物理单位或角度单位。所有计算出的数值输出都必须是无单位的纯数字。最终输出必须严格遵循指定的单行格式，不得包含任何额外文本。", "solution": "问题陈述指导我们设计并实现一种特定的启发式方法，用于在混合整数线性规划 (MILP) 的背景下选择分支变量。该选择基于线性规划 (LP) 松弛的解。问题定义明确，数学上合理，并提供了所有必要的数据和公式。因此，我将对四个指定模型中的每一个进行系统的推导和计算。\n\n该方法的核心是为每个变量 $x_i$ 计算一个选择得分 $S_i$，并选择得分最小的变量。得分 $S_i$ 是一个权重 $w_i$（衡量变量在约束系统中的结构重要性）和一个偏差 $d_i$（衡量变量的小数值与 $0.5$ 的距离）的乘积。\n\n流程如下：\n1.  对于每个约束 $j$，计算松弛量 $s_j = b_j - (A x^*)_j$，其中 $x^*$ 是 LP 松弛解。然后，约束重要性为 $I_j = \\frac{1}{1 + s_j}$。\n2.  对于每个变量 $i$，计算其权重 $w_i = \\sum_{j=1}^{m} I_j \\, |a_{j,i}|$，其中 $a_{j,i}$ 是约束矩阵 $A$ 的元素。\n3.  对于每个变量 $i$，找到其小数部分 $\\operatorname{frac}(x_i^*) = x_i^* - \\lfloor x_i^* \\rfloor$ 并计算偏差 $d_i = |\\operatorname{frac}(x_i^*) - 0.5|$。\n4.  计算选择得分 $S_i = w_i \\cdot d_i$。一个特殊条件适用：如果 $w_i = 0$，则将 $S_i$ 设为 $+\\infty$。\n5.  选择得分 $S_i$ 最小的变量。平局首先通过选择权重 $w_i$ 最大的变量来解决，然后通过选择索引最小的变量来解决。得分比较时使用 $10^{-9}$ 的容差 $\\epsilon$。\n\n我们现在将此流程应用于每个模型。\n\n### 模型 1\n给定：\n- $A^{(1)} = \\begin{bmatrix} 1  2  0  1 \\\\ 0  1  3  1 \\\\ 2  0  1  0 \\end{bmatrix}$\n- $b^{(1)} = \\begin{bmatrix} 5 \\\\ 6 \\\\ 3 \\end{bmatrix}$\n- $x^{*(1)} = \\begin{bmatrix} 0.8 \\\\ 1.5 \\\\ 0.2 \\\\ 1.1 \\end{bmatrix}$\n\n**步骤 1：约束松弛量和重要性**\n首先，我们计算乘积 $A^{(1)}x^{*(1)}$：\n$$ A^{(1)}x^{*(1)} = \\begin{bmatrix} 1(0.8) + 2(1.5) + 0(0.2) + 1(1.1) \\\\ 0(0.8) + 1(1.5) + 3(0.2) + 1(1.1) \\\\ 2(0.8) + 0(1.5) + 1(0.2) + 0(1.1) \\end{bmatrix} = \\begin{bmatrix} 4.9 \\\\ 3.2 \\\\ 1.8 \\end{bmatrix} $$\n松弛量 $s_j = b_j - (A x^*)_j$ 是：\n- $s_1 = 5 - 4.9 = 0.1$\n- $s_2 = 6 - 3.2 = 2.8$\n- $s_3 = 3 - 1.8 = 1.2$\n约束重要性 $I_j = 1/(1+s_j)$ 是：\n- $I_1 = 1 / (1 + 0.1) = 1 / 1.1 \\approx 0.90909$\n- $I_2 = 1 / (1 + 2.8) = 1 / 3.8 \\approx 0.26316$\n- $I_3 = 1 / (1 + 1.2) = 1 / 2.2 \\approx 0.45455$\n\n**步骤 2：变量权重**\n权重 $w_i = \\sum_{j} I_j |a_{j,i}|$ 是：\n- $w_0 = I_1|1| + I_2|0| + I_3|2| \\approx 0.90909(1) + 0.45455(2) \\approx 1.81818$\n- $w_1 = I_1|2| + I_2|1| + I_3|0| \\approx 0.90909(2) + 0.26316(1) \\approx 2.08134$\n- $w_2 = I_1|0| + I_2|3| + I_3|1| \\approx 0.26316(3) + 0.45455(1) \\approx 1.24401$\n- $w_3 = I_1|1| + I_2|1| + I_3|0| \\approx 0.90909(1) + 0.26316(1) \\approx 1.17225$\n\n**步骤 3：偏差**\n小数部分 $\\operatorname{frac}(x_i^*)$ 和偏差 $d_i$ 是：\n- $x_0^* = 0.8 \\implies \\operatorname{frac}(x_0^*) = 0.8 \\implies d_0 = |0.8 - 0.5| = 0.3$\n- $x_1^* = 1.5 \\implies \\operatorname{frac}(x_1^*) = 0.5 \\implies d_1 = |0.5 - 0.5| = 0.0$\n- $x_2^* = 0.2 \\implies \\operatorname{frac}(x_2^*) = 0.2 \\implies d_2 = |0.2 - 0.5| = 0.3$\n- $x_3^* = 1.1 \\implies \\operatorname{frac}(x_3^*) = 0.1 \\implies d_3 = |0.1 - 0.5| = 0.4$\n\n**步骤 4  5：得分和选择**\n得分 $S_i = w_i \\cdot d_i$ 是：\n- $S_0 \\approx 1.81818 \\cdot 0.3 \\approx 0.54545$\n- $S_1 \\approx 2.08134 \\cdot 0.0 = 0.0$\n- $S_2 \\approx 1.24401 \\cdot 0.3 \\approx 0.37320$\n- $S_3 \\approx 1.17225 \\cdot 0.4 \\approx 0.46890$\n最小得分是 $S_1 = 0.0$。因此，选择的变量索引为 $1$。\n\n### 模型 2\n给定：\n- $A^{(2)} = \\begin{bmatrix} 1  1  1 \\\\ 2  0  1 \\end{bmatrix}$\n- $b^{(2)} = \\begin{bmatrix} 4.0 \\\\ 5.0 \\end{bmatrix}$\n- $x^{*(2)} = \\begin{bmatrix} 2.0 \\\\ 1.49 \\\\ 0.5 \\end{bmatrix}$\n\n**步骤 1：约束松弛量和重要性**\n$A^{(2)}x^{*(2)} = \\begin{bmatrix} 1(2.0) + 1(1.49) + 1(0.5) \\\\ 2(2.0) + 0(1.49) + 1(0.5) \\end{bmatrix} = \\begin{bmatrix} 3.99 \\\\ 4.5 \\end{bmatrix}$\n- $s_1 = 4.0 - 3.99 = 0.01$\n- $s_2 = 5.0 - 4.5 = 0.5$\n- $I_1 = 1 / (1 + 0.01) = 1 / 1.01 \\approx 0.99010$\n- $I_2 = 1 / (1 + 0.5) = 1 / 1.5 \\approx 0.66667$\n\n**步骤 2：变量权重**\n- $w_0 = I_1|1| + I_2|2| \\approx 0.99010(1) + 0.66667(2) \\approx 2.32343$\n- $w_1 = I_1|1| + I_2|0| \\approx 0.99010(1) \\approx 0.99010$\n- $w_2 = I_1|1| + I_2|1| \\approx 0.99010(1) + 0.66667(1) \\approx 1.65677$\n\n**步骤 3：偏差**\n- $x_0^* = 2.0 \\implies \\operatorname{frac}(x_0^*) = 0.0 \\implies d_0 = |0.0 - 0.5| = 0.5$\n- $x_1^* = 1.49 \\implies \\operatorname{frac}(x_1^*) = 0.49 \\implies d_1 = |0.49 - 0.5| = 0.01$\n- $x_2^* = 0.5 \\implies \\operatorname{frac}(x_2^*) = 0.5 \\implies d_2 = |0.5 - 0.5| = 0.0$\n\n**步骤 4  5：得分和选择**\n- $S_0 \\approx 2.32343 \\cdot 0.5 \\approx 1.16172$\n- $S_1 \\approx 0.99010 \\cdot 0.01 \\approx 0.00990$\n- $S_2 \\approx 1.65677 \\cdot 0.0 = 0.0$\n最小得分是 $S_2 = 0.0$。选择的变量索引为 $2$。\n\n### 模型 3\n给定：\n- $A^{(3)} = \\begin{bmatrix} 1  0  1  2 \\\\ 0  2  1  0 \\end{bmatrix}$\n- $b^{(3)} = \\begin{bmatrix} 4.0 \\\\ 4.0 \\end{bmatrix}$\n- $x^{*(3)} = \\begin{bmatrix} 0.5 \\\\ 1.5 \\\\ 0.75 \\\\ 1.25 \\end{bmatrix}$\n\n**步骤 1：约束松弛量和重要性**\n$A^{(3)}x^{*(3)} = \\begin{bmatrix} 1(0.5) + 0(1.5) + 1(0.75) + 2(1.25) \\\\ 0(0.5) + 2(1.5) + 1(0.75) + 0(1.25) \\end{bmatrix} = \\begin{bmatrix} 3.75 \\\\ 3.75 \\end{bmatrix}$\n- $s_1 = 4.0 - 3.75 = 0.25$\n- $s_2 = 4.0 - 3.75 = 0.25$\n- $I_1 = 1 / (1 + 0.25) = 1 / 1.25 = 0.8$\n- $I_2 = 1 / (1 + 0.25) = 1 / 1.25 = 0.8$\n\n**步骤 2：变量权重**\n- $w_0 = I_1|1| + I_2|0| = 0.8(1) = 0.8$\n- $w_1 = I_1|0| + I_2|2| = 0.8(2) = 1.6$\n- $w_2 = I_1|1| + I_2|1| = 0.8(1) + 0.8(1) = 1.6$\n- $w_3 = I_1|2| + I_2|0| = 0.8(2) = 1.6$\n\n**步骤 3：偏差**\n- $x_0^* = 0.5 \\implies \\operatorname{frac}(x_0^*) = 0.5 \\implies d_0 = |0.5 - 0.5| = 0.0$\n- $x_1^* = 1.5 \\implies \\operatorname{frac}(x_1^*) = 0.5 \\implies d_1 = |0.5 - 0.5| = 0.0$\n- $x_2^* = 0.75 \\implies \\operatorname{frac}(x_2^*) = 0.75 \\implies d_2 = |0.75 - 0.5| = 0.25$\n- $x_3^* = 1.25 \\implies \\operatorname{frac}(x_3^*) = 0.25 \\implies d_3 = |0.25 - 0.5| = 0.25$\n\n**步骤 4  5：得分和选择**\n- $S_0 = 0.8 \\cdot 0.0 = 0.0$\n- $S_1 = 1.6 \\cdot 0.0 = 0.0$\n- $S_2 = 1.6 \\cdot 0.25 = 0.4$\n- $S_3 = 1.6 \\cdot 0.25 = 0.4$\n最小得分是 $0.0$，由索引为 $0$ 和 $1$ 的变量共享。我们应用平局决胜规则：选择权重 $w_i$ 最大的候选者。\n- 候选者 $0$：$w_0 = 0.8$\n- 候选者 $1$：$w_1 = 1.6$\n由于 $w_1  w_0$，我们选择索引为 $1$ 的变量。\n\n### 模型 4\n给定：\n- $A^{(4)} = \\begin{bmatrix} 1  1  0 \\end{bmatrix}$\n- $b^{(4)} = \\begin{bmatrix} 1.4 \\end{bmatrix}$\n- $x^{*(4)} = \\begin{bmatrix} 0.3 \\\\ 0.7 \\\\ 0.49 \\end{bmatrix}$\n\n**步骤 1：约束松弛量和重要性**\n$A^{(4)}x^{*(4)} = \\begin{bmatrix} 1(0.3) + 1(0.7) + 0(0.49) \\end{bmatrix} = \\begin{bmatrix} 1.0 \\end{bmatrix}$\n- $s_1 = 1.4 - 1.0 = 0.4$\n- $I_1 = 1 / (1 + 0.4) = 1 / 1.4 \\approx 0.71429$\n\n**步骤 2：变量权重**\n- $w_0 = I_1|1| = 1/1.4 \\approx 0.71429$\n- $w_1 = I_1|1| = 1/1.4 \\approx 0.71429$\n- $w_2 = I_1|0| = 0$\n\n**步骤 3：偏差**\n- $x_0^* = 0.3 \\implies \\operatorname{frac}(x_0^*) = 0.3 \\implies d_0 = |0.3 - 0.5| = 0.2$\n- $x_1^* = 0.7 \\implies \\operatorname{frac}(x_1^*) = 0.7 \\implies d_1 = |0.7 - 0.5| = 0.2$\n- $x_2^* = 0.49 \\implies \\operatorname{frac}(x_2^*) = 0.49 \\implies d_2 = |0.49 - 0.5| = 0.01$\n\n**步骤 4  5：得分和选择**\n- $S_0 = (1/1.4) \\cdot 0.2 = 0.2 / 1.4 = 1/7 \\approx 0.14286$\n- $S_1 = (1/1.4) \\cdot 0.2 = 0.2 / 1.4 = 1/7 \\approx 0.14286$\n- 根据规则，$w_2=0$，所以 $S_2 = +\\infty$。\n最小得分是 $1/7$，由索引为 $0$ 和 $1$ 的变量共享。我们应用第一个平局决胜规则（最大权重）。\n- 候选者 $0$：$w_0 = 1/1.4$\n- 候选者 $1$：$w_1 = 1/1.4$\n权重相同，因此我们应用第二个平局决胜规则：选择最小的索引。索引为 $0$ 和 $1$。最小的是 $0$。因此，我们选择索引为 $0$ 的变量。\n\n### 结果总结\n- 模型 1：索引 $1$\n- 模型 2：索引 $2$\n- 模型 3：索引 $1$\n- 模型 4：索引 $0$\n\n最终输出将是这些索引的列表。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to orchestrate the solution process for all test cases.\n    \"\"\"\n\n    def select_branching_variable(A, b, x_star, tol=1e-9):\n        \"\"\"\n        Implements the 5-step process to select a branching variable.\n        \n        Args:\n            A (np.ndarray): Constraint matrix.\n            b (np.ndarray): Right-hand side vector.\n            x_star (np.ndarray): LP relaxation solution vector.\n            tol (float): Tolerance for score comparison.\n\n        Returns:\n            int: The zero-based index of the selected variable.\n        \"\"\"\n        A = np.atleast_2d(A)\n        m, n = A.shape\n        b = np.atleast_1d(b)\n        x_star = np.atleast_1d(x_star)\n\n        # Step 1: Compute slacks and constraint importances\n        # The problem is stated as Ax = b, so slack is s = b - Ax\n        s = b - A @ x_star\n        # Ensure positivity of slack per definition s_j >= 0.\n        # This can be slightly negative due to float precision, clamp at 0.\n        s[s  0] = 0\n        I = 1.0 / (1.0 + s)\n\n        # Step 2: Compute variable weights\n        # w_i = sum_j I_j * |a_ji|\n        # This is equivalent to the dot product of I and each column of |A|.\n        # np.dot(I, np.abs(A)) computes this efficiently.\n        w = np.dot(I, np.abs(A))\n\n        # Step 3: Compute deviations from 0.5\n        # frac(x_i) = x_i - floor(x_i)\n        frac_x = x_star - np.floor(x_star)\n        d = np.abs(frac_x - 0.5)\n\n        # Step 4: Compute selection scores\n        # S_i = w_i * d_i\n        S = w * d\n        # If w_i = 0, treat S_i as +inf\n        zeros_w_indices = np.where(np.isclose(w, 0))[0]\n        S[zeros_w_indices] = np.inf\n\n        # Step 5: Select variable index\n        if np.all(np.isinf(S)):\n            # This case should not happen with the given data but is a safe guard.\n            # If all scores are infinity, just pick the first variable by index.\n            return 0\n            \n        min_score = np.min(S)\n        \n        # Find all candidates with score close to the minimum\n        candidate_indices = np.where(np.abs(S - min_score) = tol)[0]\n\n        if len(candidate_indices) == 1:\n            return candidate_indices[0]\n        \n        # Tie-breaking rule 1: largest w_i\n        candidate_weights = w[candidate_indices]\n        max_w = np.max(candidate_weights)\n        \n        # Use np.isclose for robust float comparison\n        best_candidates_by_weight = candidate_indices[np.isclose(candidate_weights, max_w)]\n\n        if len(best_candidates_by_weight) == 1:\n            return best_candidates_by_weight[0]\n\n        # Tie-breaking rule 2: smallest index\n        return np.min(best_candidates_by_weight)\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"A\": np.array([[1, 2, 0, 1], [0, 1, 3, 1], [2, 0, 1, 0]]),\n            \"b\": np.array([5, 6, 3]),\n            \"x_star\": np.array([0.8, 1.5, 0.2, 1.1])\n        },\n        {\n            \"A\": np.array([[1, 1, 1], [2, 0, 1]]),\n            \"b\": np.array([4.0, 5.0]),\n            \"x_star\": np.array([2.0, 1.49, 0.5])\n        },\n        {\n            \"A\": np.array([[1, 0, 1, 2], [0, 2, 1, 0]]),\n            \"b\": np.array([4.0, 4.0]),\n            \"x_star\": np.array([0.5, 1.5, 0.75, 1.25])\n        },\n        {\n            \"A\": np.array([[1, 1, 0]]),\n            \"b\": np.array([1.4]),\n            \"x_star\": np.array([0.3, 0.7, 0.49])\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        result = select_branching_variable(case[\"A\"], case[\"b\"], case[\"x_star\"])\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3104685"}]}