{"hands_on_practices": [{"introduction": "这项基础练习将带你从头开始，将凸凹过程（CCP）应用于一个简单的一维非凸函数。通过这个练习，你将掌握 CCP 的两个关键步骤：构建差分凸（DC）分解以及推导迭代更新规则。通过亲手实现并运行该算法，你将具体地理解 CCP 如何在复杂的能量景观中导航以寻找驻点。[@problem_id:3114687]", "problem": "您被要求为一个单变量凸函数差(DC)最小化问题实现凸凹过程(CCP)，其目标函数是一个玩具示例 $f(x) = \\sin(x) + \\alpha x^2$，其中 $\\alpha \\ge 0$ 是一个参数。此任务包含三个部分：构建一个有效的DC分解，推导CCP更新规则，以及从不同的初始点运行算法以研究收敛行为与初始曲率的关系。三角函数中使用的所有角度都必须是弧度制。\n\n请从以下基础知识开始：\n- 二阶导数检验：一个二次可微函数 $q(x)$是凸函数的充要条件是，对于其定义域中的所有$x$，都有$q''(x) \\ge 0$。\n- $\\sin(x)$的梯度是$\\cos(x)$，其二阶导数是$-\\sin(x)$。\n- 如果一个可微函数$r(x)$的梯度是$L$-利普希茨连续的，即对于所有$x,y$都满足$\\|\\nabla r(x) - \\nabla r(y)\\| \\le L \\|x-y\\|$，那么函数 $x \\mapsto \\frac{L}{2}x^2 - r(x)$是凸函数，因为其二阶导数的下界为$0$。\n\n您的程序需要完成的任务：\n1) 构建一个DC分解 $f(x) = g(x) - h(x)$，其中$g(x)$和$h(x)$都是凸函数。仅使用上述基础事实来证明您的构造的合理性。\n2) 仅使用CCP最小化DC函数的定义——即在第$k$次迭代时，通过线性化凹部来最小化所得到的凸代理函数——推导出单变量情况下的显式迭代更新规则并实现它。您的实现必须是数值稳定的，并基于决策变量的容差设置停止准则，同时必须设定最大迭代次数以避免算法不终止。\n3) 对于下述每个测试用例，运行CCP并输出一份总结，该总结将观察到的收敛性与起始点的初始曲率$f$联系起来。具体来说，对于每个案例，计算：\n   - CCP找到的计算驻点$x^\\star$。\n   - 目标函数值$f(x^\\star)$。\n   - 一个布尔值，指示二阶导数$f''(x^\\star) > 0$是否成立（此处解释为局部最小值凭证）。\n   - 一个布尔值，指示算法是否在停止准则下收敛。\n   - 起始点$x_0$处的初始曲率$f''(x_0)$。\n   - 一个布尔值，指示初始曲率$f''(x_0) > 0$是否成立。\n   - 使用的迭代次数。\n\n您的程序必须使用的量定义：\n- 函数为 $f(x) = \\sin(x) + \\alpha x^2$。\n- 初始曲率为 $f''(x_0) = -\\sin(x_0) + 2\\alpha$。\n- 局部最小值凭证是 $f''(x^\\star) > 0$的布尔值。\n- CCP迭代解是通过求解每次迭代中凹部线性化产生的凸子问题得到的。\n\n角度单位要求：\n- 所有三角函数计算必须使用弧度。\n\n测试套件：\n使用以下测试用例，每个用例由$(\\alpha, x_0)$指定，并按上述描述计算与曲率相关的量和CCP运行结果。\n- 案例 1：$\\alpha = 0.1$, $x_0 = 0.1$。\n- 案例 2：$\\alpha = 0.1$, $x_0 = 2.5$。\n- 案例 3：$\\alpha = 0.6$, $x_0 = -10.0$。\n- 案例 4：$\\alpha = 0.3$, $x_0 = 4.0$。\n- 案例 5：$\\alpha = 0.5$, $x_0 = \\frac{\\pi}{2}$。\n\n需要实现的算法细节：\n- 使用使您的DC分解对所有$x \\in \\mathbb{R}$都有效的最小有效曲率移动常数$L$。\n- 对两次迭代之间$x$的绝对变化使用停止容差$\\varepsilon = 10^{-10}$。\n- 使用最大迭代次数上限为$10000$。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果。列表中的每个元素对应于上面列出的一个测试用例，并按顺序排列。每个元素本身必须是一个包含7个量的列表，顺序为：$x^\\star$、$f(x^\\star)$、局部最小值凭证、收敛布尔值、$f''(x_0^{(1)})$、初始曲率正布尔值、迭代次数。例如，整体输出必须具有以下形式：\n$[ [x^\\star_1,f(x^\\star_1),\\text{min}_1,\\text{conv}_1,f''(x_0^{(1)}),\\text{icurvpos}_1,\\text{iters}_1], \\ldots, [x^\\star_5,f(x^\\star_5),\\text{min}_5,\\text{conv}_5,f''(x_0^{(5)}),\\text{icurvpos}_5,\\text{iters}_5] ]$。", "solution": "用户提供的问题是有效的，因为它科学地基于优化理论的原理，问题陈述清晰，具有明确的目标和约束，并且没有任何矛盾或歧义。我们将继续提供完整的解决方案。\n\n该问题要求实现并分析凸凹过程(Convex-Concave Procedure, CCP)，以最小化给定参数$\\alpha \\ge 0$下的单变量目标函数$f(x) = \\sin(x) + \\alpha x^2$。该过程涉及三个主要任务：构建一个有效的凸函数差(Difference-of-Convex, DC)分解，推导CCP迭代更新规则，并实现该算法以研究其在指定测试用例上的收敛行为。\n\n### 第1部分：构建一个有效的DC分解\n\n目标函数为$f(x) = \\sin(x) + \\alpha x^2$。DC分解将$f(x)$表示为两个凸函数的差，即$f(x) = g(x) - h(x)$。\n\n对于$\\alpha \\ge 0$，项$\\alpha x^2$是凸的，因为它的二阶导数是$2\\alpha \\ge 0$。项$\\sin(x)$不是凸的，因为它的二阶导数$-\\sin(x)$可以为负。我们必须分解函数以处理这个非凸部分。\n\n创建DC分解的一种标准方法是加上并减去一个足够大的二次项$\\frac{L}{2}x^2$，以使各分量变为凸函数。我们可以将$f(x)$分解为：\n$$f(x) = \\left( \\alpha x^2 + \\frac{L}{2}x^2 \\right) - \\left( \\frac{L}{2}x^2 - \\sin(x) \\right)$$\n我们定义：\n$g(x) = (\\alpha + \\frac{L}{2})x^2$\n$h(x) = \\frac{L}{2}x^2 - \\sin(x)$\n\n要使其成为一个有效的DC分解，$g(x)$和$h(x)$都必须是凸函数。\n\n1.  **$g(x)$的凸性：**\n    $g(x)$的二阶导数是$g''(x) = 2(\\alpha + \\frac{L}{2}) = 2\\alpha + L$。要使$g(x)$是凸的，我们需要$g''(x) \\ge 0$。由于$\\alpha \\ge 0$且我们将选择一个非负常数$L$，这个条件将得到满足。\n\n2.  **$h(x)$的凸性：**\n    $h(x)$的二阶导数是$h''(x) = L + \\sin(x)$。根据二阶导数检验，$h(x)$是凸函数的充要条件是，对于所有$x \\in \\mathbb{R}$，都有$h''(x) \\ge 0$。这意味着我们必须有$L + \\sin(x) \\ge 0$对所有$x$成立。由于$\\sin(x)$的最小值是$-1$，该条件变为$L - 1 \\ge 0$，即$L \\ge 1$。\n\n问题要求使用最小的有效常数$L$。因此，我们选择$L=1$。\n\n当$L=1$时：\n- $g(x) = (\\alpha + \\frac{1}{2})x^2$是凸的，因为对于$\\alpha \\ge 0$，其二阶导数$2\\alpha + 1 \\ge 1 > 0$。\n- $h(x) = \\frac{1}{2}x^2 - \\sin(x)$是凸的，因为对于所有$x$，其二阶导数$1 + \\sin(x) \\ge 1 - 1 = 0$。\n\n因此，一个有效的DC分解是$f(x) = g(x) - h(x)$，其中函数如上定义且$L=1$。\n\n### 第2部分：推导CCP更新规则\n\nCCP算法通过迭代求解一系列凸子问题来解决非凸问题$\\min_x \\{g(x) - h(x)\\}$。在每次迭代$k$中，给定当前迭代点$x_k$，目标函数的凹部$-h(x)$被其在$x_k$处的一阶泰勒近似所取代。这产生了一个待最小化的凸代理函数。\n\n$h(x)$在$x_k$处的线性化是$h(x_k) + \\nabla h(x_k)(x - x_k)$。\n在第$k+1$次迭代时的CCP子问题是：\n$$x_{k+1} = \\arg\\min_x \\left\\{ g(x) - \\left[ h(x_k) + \\nabla h(x_k)(x - x_k) \\right] \\right\\}$$\n项$h(x_k)$和$-\\nabla h(x_k)(-x_k)$相对于优化变量$x$是常数，因此我们可以将问题简化为：\n$$x_{k+1} = \\arg\\min_x \\left\\{ g(x) - x \\nabla h(x_k) \\right\\}$$\n我们需要$h(x)$的梯度：\n$\\nabla h(x) = \\frac{d}{dx} \\left(\\frac{1}{2}x^2 - \\sin(x)\\right) = x - \\cos(x)$。\n所以，$\\nabla h(x_k) = x_k - \\cos(x_k)$。\n\n子问题的目标函数是：\n$$J(x) = g(x) - x \\nabla h(x_k) = \\left(\\alpha + \\frac{1}{2}\\right)x^2 - (x_k - \\cos(x_k))x$$\n这是一个关于$x$的凸二次函数。为了找到它的最小值点，我们将其关于$x$的导数设为零：\n$$\\nabla J(x) = 2\\left(\\alpha + \\frac{1}{2}\\right)x - (x_k - \\cos(x_k)) = 0$$\n$$(2\\alpha + 1)x = x_k - \\cos(x_k)$$\n解出$x$得到$x_{k+1}$的显式迭代更新规则：\n$$x_{k+1} = \\frac{x_k - \\cos(x_k)}{2\\alpha + 1}$$\n由于$\\alpha \\ge 0$，分母$2\\alpha+1$总是大于或等于$1$，因此该更新是良定义的。\n\n### 第3部分：实现与分析\n\n派生出的更新规则被实现为一个迭代算法。从一个初始点$x_0$开始，生成序列$\\{x_k\\}$，直到连续迭代点之间的绝对差$|x_{k+1} - x_k|$小于指定的容差$\\varepsilon = 10^{-10}$，或者达到最大迭代次数$10000$次。\n\n对于每个由$(\\alpha, x_0)$对定义的测试用例，运行该算法以找到一个驻点$x^\\star$。CCP迭代的驻点是一个不动点，必须满足$x = \\frac{x - \\cos(x)}{2\\alpha+1}$。这可以简化为$(2\\alpha+1)x = x-\\cos(x)$，或$2\\alpha x + \\cos(x) = 0$。这个条件与将原始函数$f(x)$的梯度$f'(x) = 2\\alpha x + \\cos(x)$设为零是相同的。因此，CCP算法正确地收敛到$f(x)$的驻点。\n\n收敛后，为每个测试用例计算并报告以下量：\n- 计算出的驻点$x^\\star$。\n- 驻点处的目标函数值$f(x^\\star) = \\sin(x^\\star) + \\alpha(x^\\star)^2$。\n- 一个指示是否为局部最小值的布尔凭证，评估为$f''(x^\\star) > 0$。二阶导数为$f''(x) = -\\sin(x) + 2\\alpha$。\n- 一个布尔标志，指示算法是否在最大迭代次数内收敛。\n- 起始点处的初始曲率$f''(x_0) = -\\sin(x_0) + 2\\alpha$。\n- 一个布尔标志，指示$x_0$处的初始曲率是否为正。\n- 执行的迭代次数。\n\n该实现将对所有指定的测试用例应用此过程，并按要求格式化结果。", "answer": "```python\nimport numpy as np\n\ndef run_ccp(alpha, x0, tol=1e-10, max_iter=10000):\n    \"\"\"\n    Implements the Convex-Concave Procedure (CCP) for the given objective.\n    \n    Args:\n        alpha (float): The parameter for the objective function.\n        x0 (float): The initial starting point.\n        tol (float): The tolerance for the stopping criterion.\n        max_iter (int): The maximum number of iterations.\n        \n    Returns:\n        list: A list containing the 7 required output quantities.\n    \"\"\"\n    x_k = x0\n    converged = False\n    iters = 0\n\n    for i in range(max_iter):\n        x_k_plus_1 = (x_k - np.cos(x_k)) / (2 * alpha + 1)\n        iters = i + 1\n\n        if np.abs(x_k_plus_1 - x_k)  tol:\n            converged = True\n            x_k = x_k_plus_1\n            break\n        \n        x_k = x_k_plus_1\n    \n    # In case of non-convergence, x_k is the last computed value.\n    if not converged:\n        iters = max_iter\n\n    x_star = x_k\n\n    # Calculate required metrics\n    f_x_star = np.sin(x_star) + alpha * x_star**2\n    f_double_prime_x_star = -np.sin(x_star) + 2 * alpha\n    is_local_min = bool(f_double_prime_x_star > 0)\n\n    f_double_prime_x0 = -np.sin(x0) + 2 * alpha\n    is_initial_curvature_pos = bool(f_double_prime_x0 > 0)\n\n    return [\n        x_star,\n        f_x_star,\n        is_local_min,\n        converged,\n        f_double_prime_x0,\n        is_initial_curvature_pos,\n        iters\n    ]\n\ndef solve():\n    \"\"\"\n    Runs the CCP solver for all test cases and prints the formatted output.\n    \"\"\"\n    test_cases = [\n        (0.1, 0.1),\n        (0.1, 2.5),\n        (0.6, -10.0),\n        (0.3, 4.0),\n        (0.5, np.pi / 2.0)\n    ]\n\n    results = []\n    for alpha, x0 in test_cases:\n        result = run_ccp(alpha, x0)\n        # Ensure booleans are lowercase 'true'/'false' for perfect format matching if needed,\n        # but Python's default `str(bool)` is 'True'/'False' and is acceptable.\n        # Example: list_str = f\"[{','.join(str(item).lower() if isinstance(item, bool) else str(item) for item in result)}]\"\n        results.append(result)\n\n    # Format the final output string according to the strict specification.\n    # The format requires a string representation of a list of lists with no spaces.\n    formatted_results = []\n    for case_result in results:\n        # Convert each item in the sublist to a string\n        str_items = [str(item) for item in case_result]\n        # Join them with commas\n        list_str = ','.join(str_items)\n        # Enclose in brackets\n        formatted_results.append(f\"[{list_str}]\")\n    \n    # Join the formatted sublists with commas\n    main_content = ','.join(formatted_results)\n    \n    # Enclose the entire string in brackets\n    final_output = f\"[{main_content}]\"\n    \n    print(final_output)\n\n# Run the solver\nsolve()\n\n```", "id": "3114687"}, {"introduction": "理解了基础知识后，让我们将 CCP 应用于一个在机器学习和统计学中具有重要实践意义的问题：促进稀疏性。这个练习将展示如何使用 CCP 处理像负 $\\ell_1$ 范数这样的非凸惩罚项，这些惩罚项旨在鼓励稀疏解。你将看到 CCP 的子问题如何巧妙地转化为著名的 LASSO 问题的一个变体，从而深入了解该算法的行为。[@problem_id:3114720]", "problem": "考虑非凸目标函数 $f(x) = \\|A x\\|_{2}^{2} - \\mu \\|x\\|_{1}$，其中 $A \\in \\mathbb{R}^{m \\times n}$ 且 $\\mu  0$。您将使用凸差规划（Difference-of-Convex programming, DC programming）和凸凹过程（Convex-Concave Procedure, CCP）的框架，来构造一个在每次迭代中求解的凸代理子问题。\n\n从 DC 规划的核心定义和 $\\ell_{1}$ 范数的次梯度出发，首先通过引入一个辅助参数 $\\lambda  0$ 将 $f$ 表示为两个凸函数的差，从而构造一个有效的 DC 分解。然后，利用凸凹过程（CCP）的定义，在当前迭代点 $x^{k}$ 处通过次梯度对凹部进行线性化，以推导出用于最小化生成 $x^{k+1}$ 的凸代理问题的显式形式。证明该代理子问题具有类似最小绝对收缩和选择算子（Least Absolute Shrinkage and Selection Operator, LASSO）的目标结构（二次损失加上一个 $\\ell_{1}$ 惩罚项），并带有一个依赖于所选次梯度的附加线性项。\n\n最后，对于以下具体实例：\n- $$A = \\begin{pmatrix} 1   0 \\\\ 0   2 \\end{pmatrix}$$\n- $\\mu  0$，$\\lambda  0$\n- 初始点 $$x^{0} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$$\n- 且选择的次梯度 $s^{0} \\in \\partial \\|x^{0}\\|_{1}$ 为 $$s^{0} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$$\n\n以闭式解的形式计算第一个 CCP 代理问题的精确最小化子 $x^{1}$。将您的最终答案表示为一个包含两个元素的行矩阵，用 $\\mu$ 表示。无需四舍五入，也不涉及单位。\n\n除了计算之外，请根据推导出的代理问题，解释与标准 LASSO 目标（不含该线性项）相比，额外的线性项如何影响迭代解中的稀疏模式。您的解释必须基于第一性原理推导，并且不应依赖于任何未引入的快捷公式。", "solution": "用户希望使用凸凹过程（CCP）分析非凸目标函数 $f(x) = \\|A x\\|_{2}^{2} - \\mu \\|x\\|_{1}$。这主要包括三个部分：\n1.  推导 CCP 代理子问题的一般形式。\n2.  在给定特定初始点 $x^0$ 和次梯度选择的情况下，求解第一个迭代解 $x^1$。\n3.  解释代理问题中附加线性项对解的稀疏性的作用。\n\n### 问题验证\n首先分析问题陈述。\n\n**步骤1：提取已知条件**\n- 目标函数：$f(x) = \\|A x\\|_{2}^{2} - \\mu \\|x\\|_{1}$\n- 矩阵 $A \\in \\mathbb{R}^{m \\times n}$\n- 常数 $\\mu  0$\n- 辅助参数 $\\lambda  0$\n- 迭代解：$x^k$ (当前)，$x^{k+1}$ (下一个)\n- 具体实例：\n    - $A = \\begin{pmatrix} 1   0 \\\\ 0   2 \\end{pmatrix}$\n    - $\\mu  0$，$\\lambda  0$\n    - 初始点 $x^{0} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$\n    - 次梯度选择 $s^{0} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} \\in \\partial \\|x^{0}\\|_{1}$\n\n**步骤2：使用提取的已知条件进行验证**\n- **科学依据：** 该问题基于凸分析和优化的既定原则，特别是凸差（DC）规划和凸凹过程（CCP）。这些都是标准技术。所涉及的函数，即平方 $\\ell_2$ 范数和 $\\ell_1$ 范数，是数学及其应用中的基本概念。\n- **适定性：** 问题陈述清晰。它要求推导一个代理函数，并为一个具体的、完全定义的实例计算下一个迭代解。待推导的子问题是凸的，这确保了最小化子的存在。\n- **客观性：** 问题以精确的数学语言表述，没有歧义或主观论断。\n\n必须验证次梯度的选择。$\\|x\\|_1$ 在点 $x$ 处的次梯度是向量集合 $s$，其分量为 $s_i = \\text{sign}(x_i)$（如果 $x_i \\neq 0$）和 $s_i \\in [-1, 1]$（如果 $x_i = 0$）。对于 $x^0 = (1, 0)^T$，我们有 $x^0_1 = 1$ 和 $x^0_2 = 0$。一个有效的次梯度 $s^0$ 必须满足 $s^0_1 = \\text{sign}(1) = 1$ 和 $s^0_2 \\in [-1, 1]$。所提供的选择 $s^0 = (1, 0)^T$ 满足这些条件，因为 $s^0_1=1$ 且 $s^0_2=0 \\in [-1, 1]$。\n\n**步骤3：结论和行动**\n该问题被判定为**有效**。我们可以继续进行求解。\n\n### 第1部分：CCP 代理子问题的推导\n\n问题的核心是在 DC 规划框架内构建 $f(x)$ 的优化问题，这需要将 $f(x)$ 表示为两个凸函数之差，即 $f(x) = g(x) - h(x)$。问题指定通过引入一个辅助参数 $\\lambda  0$ 来构造一个这样的分解，并要求得到的子问题是一个带有附加线性项的类 LASSO 目标。\n\n标准分解 $g(x) = \\|Ax\\|_2^2$ 和 $h(x) = \\mu\\|x\\|_1$ 无法产生所需的结构。我们必须找到一种分解，其中 $\\ell_1$ 范数保留在不被线性化的凸部分 $g(x)$ 中。\n\n我们通过在目标函数中加上和减去项 $\\lambda\\|x\\|_1$ 来引入 $\\lambda  0$：\n$$f(x) = \\|A x\\|_{2}^{2} - \\mu \\|x\\|_{1} = (\\|A x\\|_{2}^{2} + \\lambda \\|x\\|_{1}) - (\\lambda + \\mu)\\|x\\|_{1}$$\n我们定义两个函数 $g(x)$ 和 $h(x)$：\n- $g(x) = \\|A x\\|_{2}^{2} + \\lambda \\|x\\|_{1}$\n- $h(x) = (\\lambda + \\mu)\\|x\\|_{1}$\n\n要使这是一个有效的 DC 分解，$g(x)$ 和 $h(x)$ 都必须是凸函数。\n- 函数 $\\|A x\\|_{2}^{2}$ 是凸的。因为 $\\lambda  0$，函数 $\\lambda \\|x\\|_{1}$ 也是凸的。两个凸函数的和是凸的，所以 $g(x)$ 是凸的。\n- 因为 $\\lambda  0$ 且 $\\mu  0$，系数 $(\\lambda + \\mu)$ 是正的。一个凸函数（$\\|x\\|_1$）的正数倍是凸的，所以 $h(x)$ 是凸的。\n\n我们成功地将 $f(x) = g(x) - h(x)$ 表示为两个凸函数的差。\n\n凸凹过程（CCP）通过在每一步最小化 $f(x)$ 的一个凸代理函数来生成一个迭代序列 $\\{x^k\\}$。该代理函数是通过在当前迭代点 $x^k$ 周围对第二个凸函数 $h(x)$ 进行线性化来构造的。根据凸性的定义，对于任何 $x$ 和一个次梯度 $s_h(x^k) \\in \\partial h(x^k)$，我们有以下不等式：\n$$h(x) \\ge h(x^k) + \\langle s_h(x^k), x - x^k \\rangle$$\n这意味着 $-h(x) \\le -h(x^k) - \\langle s_h(x^k), x - x^k \\rangle$。\n将此代入 $f(x)$ 的表达式，得到一个上界（即代理函数）：\n$$f(x) = g(x) - h(x) \\le g(x) - (h(x^k) + \\langle s_h(x^k), x - x^k \\rangle) =: \\hat{f}(x; x^k)$$\n下一个迭代解 $x^{k+1}$ 是通过最小化这个代理函数找到的：\n$$x^{k+1} = \\arg\\min_x \\hat{f}(x; x^k)$$\n项 $h(x^k)$ 和 $\\langle s_h(x^k), x^k \\rangle$ 相对于 $x$ 是常数，因此可以从最小化问题中去掉。问题变为：\n$$x^{k+1} = \\arg\\min_x \\{ g(x) - \\langle s_h(x^k), x \\rangle \\}$$\n我们需要 $h(x) = (\\lambda+\\mu)\\|x\\|_1$ 的次梯度。根据次梯度的缩放法则：\n$$\\partial h(x) = (\\lambda+\\mu) \\partial\\|x\\|_1$$\n设 $s(x^k)$ 是在 $x^k$ 处从 $\\partial\\|x\\|_1$ 中选定的一个次梯度。那么我们可以设置 $s_h(x^k) = (\\lambda+\\mu)s(x^k)$。\n将 $g(x)$ 和 $s_h(x^k)$ 的表达式代入最小化问题，我们得到待最小化的代理目标函数的显式形式：\n$$x^{k+1} = \\arg\\min_x \\left\\{ \\|Ax\\|_2^2 + \\lambda \\|x\\|_1 - \\langle (\\lambda+\\mu)s(x^k), x \\rangle \\right\\}$$\n该目标具有 LASSO 问题的结构（一个二次损失 $\\|Ax\\|_2^2$ 加上一个 $\\ell_1$ 惩罚项 $\\lambda\\|x\\|_1$），并带有一个附加线性项 $-\\left((\\lambda+\\mu)s(x^k)\\right)^T x$。\n\n### 第2部分：$x^1$ 的计算\n\n给定具体实例：\n- $A = \\begin{pmatrix} 1   0 \\\\ 0   2 \\end{pmatrix}$，$x^0 = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$，$s^0 = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$。\n- 参数 $\\mu$ 和 $\\lambda$ 是正常数。\n\n我们需要计算 $x^1 = \\arg\\min_x \\{ \\|Ax\\|_2^2 + \\lambda \\|x\\|_1 - ((\\lambda+\\mu)s^0)^T x \\}$。设 $x = (x_1, x_2)^T$。\n\n待最小化的目标函数 $L(x_1, x_2)$ 是：\n$$L(x_1, x_2) = \\left\\| \\begin{pmatrix} 1   0 \\\\ 0   2 \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} \\right\\|_2^2 + \\lambda \\left\\| \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} \\right\\|_1 - (\\lambda+\\mu) \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}^T \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix}$$\n展开各项：\n- $\\|Ax\\|_2^2 = x_1^2 + (2x_2)^2 = x_1^2 + 4x_2^2$。\n- $\\lambda\\|x\\|_1 = \\lambda(|x_1| + |x_2|)$。\n- $((\\lambda+\\mu)s^0)^T x = (\\lambda+\\mu)(1 \\cdot x_1 + 0 \\cdot x_2) = (\\lambda+\\mu)x_1$。\n\n目标函数在 $x_1$ 和 $x_2$ 上是可分的：\n$$L(x_1, x_2) = \\left( x_1^2 + \\lambda|x_1| - (\\lambda+\\mu)x_1 \\right) + \\left( 4x_2^2 + \\lambda|x_2| \\right)$$\n我们可以独立地最小化这两个部分。\n\n**对 $x_1$ 的最小化**：\n设 $F_1(x_1) = x_1^2 - (\\lambda+\\mu)x_1 + \\lambda|x_1|$。我们使用次梯度最优性条件 $0 \\in \\partial F_1(x_1)$ 来找到最小值。\n次梯度为 $\\partial F_1(x_1) = 2x_1 - (\\lambda+\\mu) + \\lambda \\partial|x_1|$。\n- 如果 $x_1  0$，则 $\\partial|x_1| = \\{1\\}$。导数为 $2x_1 - (\\lambda+\\mu) + \\lambda = 2x_1 - \\mu$。令其为零得到 $x_1 = \\frac{\\mu}{2}$。由于 $\\mu  0$，此解在定义域 $x_1  0$ 内，是一个候选解。\n- 如果 $x_1  0$，则 $\\partial|x_1| = \\{-1\\}$。导数为 $2x_1 - (\\lambda+\\mu) - \\lambda = 2x_1 - (2\\lambda+\\mu)$。令其为零得到 $x_1 = \\frac{2\\lambda+\\mu}{2}$。由于 $\\lambda, \\mu  0$，该值为正，与假设 $x_1  0$ 矛盾。在此定义域内没有解。\n- 如果 $x_1 = 0$，则 $\\partial|x_1| = [-1, 1]$。次梯度集合为 $\\partial F_1(0) = -(\\lambda+\\mu) + \\lambda[-1, 1] = [-(\\lambda+\\mu)-\\lambda, -(\\lambda+\\mu)+\\lambda] = [-(2\\lambda+\\mu), -\\mu]$。由于 $\\mu  0$，此区间不包含 $0$。因此 $x_1=0$ 不是最小化子。\n唯一的最小化子是 $x_1 = \\frac{\\mu}{2}$。\n\n**对 $x_2$ 的最小化**：\n设 $F_2(x_2) = 4x_2^2 + \\lambda|x_2|$。我们在 $0 \\in \\partial F_2(x_2)$ 处找到最小值。\n次梯度为 $\\partial F_2(x_2) = 8x_2 + \\lambda \\partial|x_2|$。\n- 如果 $x_2  0$，导数为 $8x_2 + \\lambda$。令其为零得到 $x_2 = -\\frac{\\lambda}{8}$，这与 $x_2  0$ 矛盾（因为 $\\lambda0$）。\n- 如果 $x_2  0$，导数为 $8x_2 - \\lambda$。令其为零得到 $x_2 = \\frac{\\lambda}{8}$，这与 $x_2  0$ 矛盾。\n- 如果 $x_2 = 0$，次梯度集合为 $\\partial F_2(0) = 8(0) + \\lambda[-1, 1] = [-\\lambda, \\lambda]$。由于 $\\lambda  0$，此区间包含 $0$。\n因此，唯一的最小化子是 $x_2 = 0$。\n\n结合结果，最小化子为 $x^1 = \\begin{pmatrix} \\frac{\\mu}{2} \\\\ 0 \\end{pmatrix}$。\n\n### 第3部分：线性项对稀疏性的影响\n\n代理子问题是最小化 $J(x) = \\|Ax\\|_2^2 + \\lambda\\|x\\|_1 - c^T x$，其中 $c = (\\lambda+\\mu)s(x^k)$。一个标准的 LASSO 目标函数没有线性项 $-c^T x$。我们分析该项如何影响稀疏性。\n\n最小化子 $x$ 的一阶最优性条件是 $0 \\in \\partial J(x)$：\n$$0 \\in 2A^TAx + \\lambda \\partial\\|x\\|_1 - c$$\n这可以重写为 $c \\in 2A^TAx + \\lambda \\partial\\|x\\|_1$。\n我们来考察解的某个分量 $x_i$ 为零的条件。如果 $x_i=0$，那么 $\\partial\\|x\\|_1$ 的第 $i$ 个分量是区间 $[-1, 1]$。最优性条件的第 $i$ 个分量变为：\n$$c_i \\in (2A^TAx)_i + \\lambda[-1, 1] \\quad \\text{其中 } x_i=0$$\n这等价于 $|(2A^TAx)_i - c_i| \\le \\lambda$。在此表达式中，项 $(2A^TAx)_i$ 代表解的其他非零分量的影响。作为对比，标准 LASSO（其中 $c=0$）的稀疏性条件是 $|(2A^TAx)_i| \\le \\lambda$。\n\n向量 $c$ 定义为 $c = (\\lambda+\\mu)s(x^k)$，其中 $s(x^k) \\in \\partial\\|x\\|_1|_{x^k}$。\n- 对于在前一次迭代中非零的分量 $i$（$x_i^k \\ne 0$），我们有 $s_i(x^k) = \\text{sign}(x_i^k)$。因此，$c_i = (\\lambda+\\mu)\\text{sign}(x_i^k)$。这是一个大小为 $\\lambda+\\mu$ 的非零值。$x_i$ 在下一步变为零的条件是 $|(2A^TAx)_i - (\\lambda+\\mu)\\text{sign}(x_i^k)| \\le \\lambda$。由于 $|\\lambda+\\mu|  \\lambda$，项 $(2A^TAx)_i$ 必须取一个特定的非零值才能满足该条件，这使得与标准 LASSO 情况相比，该分量变为零的可能性要小得多。这引入了一种“惯性”或“记忆”效应，即鼓励非零分量保持非零。\n\n- 对于在前一次迭代中为零的分量 $j$（$x_j^k = 0$），我们可以选择任何 $s_j(x^k) \\in [-1, 1]$。一个常见的选择是 $s_j(x^k)=0$。在这种情况下，$c_j=0$，且 $x_j$ 保持为零的条件是 $|(2A^TAx)_j| \\le \\lambda$，这与标准 LASSO 的条件相同。\n\n总而言之，线性项 $-c^T x$ 使得先前活跃（非零）的变量更难变得不活跃（零）。它激励新的迭代解 $x^{k+1}$ 保持前一个次梯度 $s(x^k)$ 的符号模式。这种“惯性”可能导致迭代解的稀疏性低于通过求解标准 LASSO 问题所能获得的稀疏性，因为它促进了活跃集在迭代间的持续存在。", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{\\mu}{2}   0\n\\end{pmatrix}\n}\n$$", "id": "3114720"}, {"introduction": "真正掌握一个工具，不仅要知道如何使用它，还要了解它在何时可能失效。这最后一个练习将探讨 CCP 的一个关键理论问题：它对所涉及函数的可微性的依赖。我们将研究一个带有非利普希茨（non-Lipschitz）惩罚项的场景，其中标准 CCP 公式会失效，然后学习如何使用一种称为平滑（smoothing）的技术来解决这个问题，这是优化领域中一个强大的概念。[@problem_id:3114699]", "problem": "考虑一个常用于诱导稀疏性的一维差分凸优化模型：最小化目标函数 $F(x) = f(x) - g(x)$，其中 $f$ 是凸函数，$g$ 也是凸函数，因此 $-g$ 是凹函数。凸凹过程 (CCP) 在第 $k$ 次迭代时，通过将凹部线性化来构造一个凸代理函数：用其在 $x^{(k)}$ 处的仿射上界替换 $-g(x)$，即使用 $-g(x^{(k)}) - s^{(k)}(x - x^{(k)})$，其中 $s^{(k)} \\in \\partial g(x^{(k)})$ (次微分)。最小化这个代理函数以产生下一个迭代点 $x^{(k+1)}$。\n\n您将分析当模型中引入非 Lipschitz 罚项时 CCP 的行为。设 $p \\in (0,1)$，$b  0$ 且 $\\lambda  0$。考虑非负实数轴上的非凸惩罚回归目标函数：\n$$\n\\min_{x \\geq 0} \\; F(x) = \\frac{1}{2}(x - b)^{2} + \\lambda h(x), \\quad \\text{其中 } h(x) = x^{p}.\n$$\n一种常见的差分凸分解是通过引入一个线性项 $c x$（其中 $c  0$）并将 $h(x)$ 写成 $h(x) = c x - \\big(c x - x^{p}\\big)$ 来获得的，因此\n$$\nF(x) = \\underbrace{\\frac{1}{2}(x - b)^{2} + \\lambda c x}_{\\text{凸}} \\;-\\; \\underbrace{\\lambda\\big(c x - x^{p}\\big)}_{\\text{凸}}.\n$$\n回答下列问题：\n\n1) 仅使用凸分析的核心定义，证明尽管 $g(x) = \\lambda(c x - x^{p})$ 在 $(0,\\infty)$ 上是凸的，但从 $x^{(0)} = 0$ 开始的 CCP 步骤无法执行，因为凹部 $-g$ 在 $x^{(0)} = 0$ 处的仿射上界没有有限的斜率。请精确刻画当 $x \\downarrow 0$ 时右导数 $g^{\\prime}(x)$ 的特征，并解释为什么这会阻碍在 $x^{(0)} = 0$ 处的 CCP 线性化。\n\n2) 提出一种平滑化方法，以抑制在 $x=0$ 处的非 Lipschitz 行为，同时保持罚项的凹性。定义\n$h_{\\epsilon}(x) = (x + \\epsilon)^{p} - \\epsilon^{p}, \\quad \\epsilon  0,$\n并设 $g_{\\epsilon}(x) = \\lambda\\big(c x - h_{\\epsilon}(x)\\big)$。证明对于任何固定的 $\\epsilon  0$，$g_{\\epsilon}$ 在 $[0,\\infty)$ 上是凸的，并且 $g_{\\epsilon}$ 在 $x=0$ 处的所有单边导数都是有限的，从而使得在 $x^{(0)} = 0$ 处的 CCP 线性化是良定义的。\n\n3) 为了量化平滑化对优化的影响，将注意力限制在域\n$$\n\\mathcal{D} = \\left[ r, \\infty \\right), \\quad \\text{其中 } r = \\big(\\lambda p(1 - p)\\big)^{\\frac{1}{2 - p}},\n$$\n上，并考虑平滑化后的目标函数\n$$\nF_{\\epsilon}(x) = \\frac{1}{2}(x - b)^{2} + \\lambda h_{\\epsilon}(x), \\quad x \\in \\mathcal{D}.\n$$\n设 $x^{\\star} \\in \\mathcal{D}$ 表示 $F$ 在 $\\mathcal{D}$ 上的唯一最小化子，$x_{\\epsilon} \\in \\mathcal{D}$ 表示 $F_{\\epsilon}$ 在 $\\mathcal{D}$ 上的唯一最小化子。仅使用基本事实（在 $\\mathcal{D}$ 上的凸性、一阶最优性和连续性论证），推导当 $\\epsilon \\to 0$ 时偏差大小\n$$\nB(\\epsilon) = |x_{\\epsilon} - x^{\\star}|\n$$\n的极限。\n\n您的最终答案必须是当 $\\epsilon \\to 0$ 时 $B(\\epsilon)$ 极限的单个值。无需四舍五入，此问题中没有物理单位。请以精确值的形式表示最终答案。", "solution": "该问题是有效的。这是优化理论领域中一组适定的问题，具体涉及凸凹过程 (CCP) 和非 Lipschitz 正则化子的性质。问题的前提在数学上是合理的，问题是客观的，并且可以通过严格的推导来回答。\n\n该问题分为三个部分。我们将依次处理每一部分。\n\n1) 第一部分要求我们证明，对于目标函数 $F(x)$ 的指定差分凸 (DC) 分解，从 $x^{(0)} = 0$ 开始的 CCP 迭代无法执行。给定的 DC 分解是 $F(x) = f(x) - g(x)$，其中 $g(x) = \\lambda(c x - x^{p})$。CCP 算法要求在当前迭代点 $x^{(k)}$ 处，通过将凹部 $-g(x)$ 线性化来为 $F(x)$ 构造一个凸代理函数。这涉及用其一阶泰勒近似替换 $-g(x)$，由于 $g(x)$ 的凸性，该近似可作为一个全局上界。该近似由 $-g(x^{(k)}) - s^{(k)}(x - x^{(k)})$ 给出，其中 $s^{(k)}$ 必须是 $g$ 在 $x^{(k)}$ 处的一个次梯度，即 $s^{(k)} \\in \\partial g(x^{(k)})$。\n\n为使此步骤在 $x^{(0)} = 0$ 处是良定义的，次微分 $\\partial g(0)$ 必须非空。函数为 $g(x) = \\lambda(c x - x^{p})$，定义在 $[0, \\infty)$ 上，参数为 $\\lambda  0$，$c  0$ 和 $p \\in (0, 1)$。问题指出 $g(x)$ 在 $(0, \\infty)$ 上是凸的。我们可以通过检查其二阶导数来验证这一点：\n$$\ng'(x) = \\lambda(c - p x^{p-1})\n$$\n$$\ng''(x) = \\lambda(-p(p-1)x^{p-2}) = \\lambda p(1-p)x^{p-2}\n$$\n对于 $x  0$，并且给定 $\\lambda  0$ 和 $p \\in (0, 1)$，我们有 $\\lambda p(1-p)  0$ 和 $x^{p-2}  0$。因此，对于所有 $x \\in (0, \\infty)$，$g''(x)  0$，这证实了 $g(x)$ 在 $(0, \\infty)$ 上是严格凸的。\n\n现在，我们必须分析在边界点 $x=0$ 处的行为。对于一个定义在 $[0, \\infty)$ 上的凸函数，次梯度 $s \\in \\partial g(0)$ 必须是一个有限实数。我们可以通过其单边方向导数来刻画边界点处的次微分。$g(x)$ 在 $x=0$ 处的右导数定义为：\n$$\ng'_{+}(0) = \\lim_{h \\downarrow 0} \\frac{g(0+h) - g(0)}{h}\n$$\n由于 $g(0) = \\lambda(c \\cdot 0 - 0^p) = 0$，我们有：\n$$\ng'_{+}(0) = \\lim_{h \\downarrow 0} \\frac{\\lambda(ch - h^p)}{h} = \\lim_{h \\downarrow 0} \\lambda(c - h^{p-1})\n$$\n因为 $p \\in (0,1)$，所以指数 $p-1$ 是负的。令 $q = 1-p \\in (0,1)$。那么 $h^{p-1} = h^{-q} = \\frac{1}{h^q}$。当 $h \\downarrow 0$ 时，$h^q \\to 0$，因此 $\\frac{1}{h^q} \\to +\\infty$。极限变为：\n$$\ng'_{+}(0) = \\lambda(c - \\lim_{h \\downarrow 0} h^{p-1}) = -\\infty\n$$\n如果方向导数是无穷大，则不存在有限的次梯度。次微分 $\\partial g(0)$ 是所有直线 $y=sx$ 斜率 $s$ 的集合，这些直线是 $g(x)$ 的全局下估计量，即对于所有 $x \\ge 0$ 都有 $\\lambda(cx - x^p) \\ge sx$。对于 $x0$，这个不等式等价于 $\\lambda c - \\lambda x^{p-1} \\ge s$。当 $x \\downarrow 0$ 时，左边趋于 $-\\infty$。没有有限的 $s$ 能够对 $0$ 的任何右邻域中的所有 $x$ 满足这个不等式。因此，$\\partial g(0)$ 是空集。\n由于 $\\partial g(0)$ 为空，无法选择次梯度 $s^{(0)} \\in \\partial g(0)$。$-g(x)$ 在 $x=0$ 处的仿射上界的斜率（即 $-s^{(0)}$）不是有限的。因此，从 $x^{(0)} = 0$ 开始的 CCP 步骤无法执行。\n\n2) 第二部分引入了一个平滑化罚项来解决在 $x=0$ 处的问题。平滑化罚项为 $h_{\\epsilon}(x) = (x + \\epsilon)^{p} - \\epsilon^{p}$，其中 $\\epsilon  0$。DC 分解中对应的凸部分是 $g_{\\epsilon}(x) = \\lambda\\big(c x - h_{\\epsilon}(x)\\big) = \\lambda\\big(c x - (x + \\epsilon)^{p} + \\epsilon^{p}\\big)$。我们需要证明 $g_{\\epsilon}(x)$ 在 $[0, \\infty)$ 上是凸的，并且它在 $x=0$ 处的导数是有限的。\n\n为了证明凸性，我们检查 $g_{\\epsilon}(x)$ 在 $x \\in [0, \\infty)$ 上的二阶导数：\n$$\ng'_{\\epsilon}(x) = \\frac{d}{dx} \\lambda\\big(c x - (x + \\epsilon)^{p} + \\epsilon^{p}\\big) = \\lambda\\big(c - p(x + \\epsilon)^{p-1}\\big)\n$$\n$$\ng''_{\\epsilon}(x) = \\frac{d}{dx} \\lambda\\big(c - p(x + \\epsilon)^{p-1}\\big) = \\lambda\\big(-p(p-1)(x + \\epsilon)^{p-2}\\big) = \\lambda p(1-p)(x+\\epsilon)^{p-2}\n$$\n对于任何 $x \\ge 0$ 和任何固定的 $\\epsilon  0$，我们有 $x+\\epsilon \\ge \\epsilon  0$。由于 $\\lambda  0$ 和 $p \\in (0, 1)$，因此 $\\lambda p(1-p)  0$。项 $(x+\\epsilon)^{p-2}$ 也是正的并且是良定义的。因此，对于所有 $x \\in [0, \\infty)$，$g''_{\\epsilon}(x)  0$。在此区间上的正二阶导数意味着 $g_{\\epsilon}(x)$ 在 $[0, \\infty)$ 上是严格凸的。\n\n接下来，我们必须证明 $g_{\\epsilon}(x)$ 在 $x=0$ 处的单边导数是有限的。由于 $g_{\\epsilon}(x)$ 在一个包含 $[0, \\infty)$ 的开区间（例如 $(-\\epsilon, \\infty)$）上是可微的，它在 $x=0$ 处的左导数和右导数都存在、相等，并由导数 $g'_{\\epsilon}(0)$ 的值给出。\n$$\ng'_{\\epsilon}(0) = \\lambda\\big(c - p(0 + \\epsilon)^{p-1}\\big) = \\lambda(c - p\\epsilon^{p-1})\n$$\n对于任何固定的 $\\epsilon  0$，$\\epsilon^{p-1}$ 是一个有限的正数。因此，$g'_{\\epsilon}(0)$ 是一个有限实数。所以，在 $x=0$ 处的所有单边导数都是有限的。这确保了次微分 $\\partial g_{\\epsilon}(0)$ 是非空单点集 $\\{g'_{\\epsilon}(0)\\}$。因此，对于平滑化后的问题，在 $x^{(0)} = 0$ 处的 CCP 线性化是良定义的。\n\n3) 第三部分要求计算当 $\\epsilon \\to 0$ 时偏差大小 $B(\\epsilon) = |x_{\\epsilon} - x^{\\star}|$ 的极限。这里，$x^{\\star}$ 是 $F(x) = \\frac{1}{2}(x - b)^{2} + \\lambda x^{p}$ 在域 $\\mathcal{D} = [r, \\infty)$ 上的唯一最小化子，$x_{\\epsilon}$ 是平滑化目标函数 $F_{\\epsilon}(x) = \\frac{1}{2}(x - b)^{2} + \\lambda h_{\\epsilon}(x)$ 在同一域 $\\mathcal{D}$ 上的唯一最小化子。常数 $r$ 定义为 $r = \\big(\\lambda p(1 - p)\\big)^{\\frac{1}{2 - p}}$。\n\n首先，我们注意到域 $\\mathcal{D}$ 的选择使得 $F(x)$ 和 $F_{\\epsilon}(x)$ 对于 $x \\in \\mathcal{D}$ 都是凸的。对于 $F(x)$，其二阶导数为 $F''(x) = 1 + \\lambda p(p-1)x^{p-2} = 1 - \\lambda p(1-p)x^{p-2}$。对于 $x \\in [r, \\infty)$，我们有 $x \\ge r$，这意味着 $x^{2-p} \\ge r^{2-p} = \\lambda p(1-p)$。因此，$x^{p-2} = \\frac{1}{x^{2-p}} \\le \\frac{1}{\\lambda p(1-p)}$。这得出 $F''(x) = 1 - \\lambda p(1-p)x^{p-2} \\ge 1 - \\lambda p(1-p)\\frac{1}{\\lambda p(1-p)} = 0$。所以 $F(x)$ 在 $\\mathcal{D}$ 上是凸的。类似的论证表明 $F_\\epsilon(x)$ 在 $\\mathcal{D}$ 上是严格凸的。两个函数都是连续的和强制的（即当 $x \\to \\infty$ 时，$F(x) \\to \\infty$ 且 $F_\\epsilon(x) \\to \\infty$），这保证了在闭集 $\\mathcal{D}$ 中最小化子 $x^{\\star}$ 和 $x_{\\epsilon}$ 的存在性和唯一性。\n\n为了找到 $|x_{\\epsilon} - x^{\\star}|$ 的极限，我们依赖于一个关于最小化子的连续性论证。让我们分析当 $\\epsilon \\to 0$ 时最小化子序列 $\\{x_{\\epsilon}\\}_{\\epsilon  0}$。该序列包含在 $\\mathcal{D}$ 中。可以证明它是有界的。根据 Bolzano-Weierstrass 定理，它必须包含一个收敛子序列。设 $\\{x_{\\epsilon_k}\\}_{k=1}^{\\infty}$ 是这样一个子序列，其中当 $k \\to \\infty$ 时 $\\epsilon_k \\to 0$，并设其极限为 $\\hat{x} = \\lim_{k\\to\\infty} x_{\\epsilon_k}$。由于对于所有 $k$ 都有 $x_{\\epsilon_k} \\in \\mathcal{D}$ 并且 $\\mathcal{D}$ 是一个闭集，极限点 $\\hat{x}$ 也必须属于 $\\mathcal{D}$。\n\n根据 $x_{\\epsilon_k}$ 作为最小化子的定义，对于任何 $x \\in \\mathcal{D}$，我们有：\n$$\nF_{\\epsilon_k}(x_{\\epsilon_k}) \\le F_{\\epsilon_k}(x)\n$$\n我们对这个不等式在 $k \\to \\infty$ 时取极限。函数 $F_{\\epsilon}(y)$ 对 $\\epsilon$ 和 $y$（对于 $y0$）都是连续的。当 $k \\to \\infty$ 时，我们有 $\\epsilon_k \\to 0$ 和 $x_{\\epsilon_k} \\to \\hat{x}$。因此，左侧收敛：\n$$\n\\lim_{k\\to\\infty} F_{\\epsilon_k}(x_{\\epsilon_k}) = \\lim_{k\\to\\infty} \\left( \\frac{1}{2}(x_{\\epsilon_k} - b)^2 + \\lambda((x_{\\epsilon_k} + \\epsilon_k)^p - \\epsilon_k^p) \\right) = \\frac{1}{2}(\\hat{x} - b)^2 + \\lambda \\hat{x}^p = F(\\hat{x})\n$$\n对于任何固定的 $x \\in \\mathcal{D}$，右侧也收敛：\n$$\n\\lim_{k\\to\\infty} F_{\\epsilon_k}(x) = \\lim_{k\\to\\infty} \\left( \\frac{1}{2}(x - b)^2 + \\lambda((x + \\epsilon_k)^p - \\epsilon_k^p) \\right) = \\frac{1}{2}(x - b)^2 + \\lambda x^p = F(x)\n$$\n对不等式取极限得到：\n$$\nF(\\hat{x}) \\le F(x) \\quad \\text{对于所有 } x \\in \\mathcal{D}\n$$\n这就是 $F(x)$ 在 $\\mathcal{D}$ 上最小化子的定义。由于 $x^{\\star}$ 是 $F(x)$ 在 $\\mathcal{D}$ 上的唯一最小化子，我们必须有 $\\hat{x} = x^{\\star}$。\n\n这表明 $\\{x_{\\epsilon}\\}$ 的任何收敛子序列都必须收敛到 $x^{\\star}$。分析学中的一个标准结果指出，如果一个有界序列的每个子序列都有一个收敛到相同极限的更深一层子序列，那么原始序列也收敛到该极限。因此，整个序列 $\\{x_{\\epsilon}\\}$ 收敛到 $x^{\\star}$：\n$$\n\\lim_{\\epsilon \\to 0} x_{\\epsilon} = x^{\\star}\n$$\n那么，通过对 $B(\\epsilon)$ 的定义应用极限，可以找到偏差大小的极限：\n$$\n\\lim_{\\epsilon \\to 0} B(\\epsilon) = \\lim_{\\epsilon \\to 0} |x_{\\epsilon} - x^{\\star}|\n$$\n根据绝对值函数的连续性：\n$$\n\\lim_{\\epsilon \\to 0} |x_{\\epsilon} - x^{\\star}| = \\left| \\lim_{\\epsilon \\to 0} (x_{\\epsilon} - x^{\\star}) \\right| = \\left| \\left(\\lim_{\\epsilon \\to 0} x_{\\epsilon}\\right) - x^{\\star} \\right| = |x^{\\star} - x^{\\star}| = 0\n$$\n偏差大小的极限是 $0$。", "answer": "$$\n\\boxed{0}\n$$", "id": "3114699"}]}