{"hands_on_practices": [{"introduction": "我们的第一个实践探讨了在线优化中的一个核心问题：如何选择合适的算法。我们将比较两种基本但功能强大的算法——在线梯度下降（OGD）和在线牛顿步（ONS）——在一个二次损失场景下的表现。通过这个练习，你将亲手推导出它们的“遗憾”上界，并量化地理解利用损失函数曲率（二阶信息）如何能够带来从 $O(\\sqrt{T})$ 到 $O(\\log T)$ 的性能巨大提升。[@problem_id:3159405]", "problem": "考虑一个在线凸优化问题，其决策集为 $\\mathcal{X} = \\{x \\in \\mathbb{R}^{d} : \\|x\\|_{2} \\leq B\\}$，其中 $d \\in \\mathbb{N}$ 和 $B > 0$ 是给定的常数。在每一轮 $t = 1, 2, \\ldots, T$，学习者选择一个 $x_t \\in \\mathcal{X}$，然后观察到一个形如 $f_t(x) = \\frac{1}{2}\\|A_t x - b_t\\|_{2}^{2}$ 的凸损失函数，其中数据满足一致有界 $\\|A_t\\|_{2} \\leq L$ 和 $\\|b_t\\|_{2} \\leq M$，对于已知的常数 $L > 0$ 和 $M \\geq 0$。性能由累积悔值来衡量\n$$\nR_T \\triangleq \\sum_{t=1}^{T} f_t(x_t) - \\min_{x \\in \\mathcal{X}} \\sum_{t=1}^{T} f_t(x).\n$$\n假设学习者或者使用初始化矩阵为 $\\lambda_0 I_d$（对于某个 $\\lambda_0 > 0$）的 Online Newton Step (ONS)，或者使用 Online Gradient Descent (OGD)，其步长为标准选择的常数步长，该步长能在有界域上为凸 Lipschitz 损失产生最优阶的悔值。仅使用基本定义（凸性、悔值）和经过充分检验的事实（范数不等式、柯西-施瓦茨不等式、谱范数的次可乘性、指数凹性的矩阵不等式刻画，以及 ONS 在指数凹损失上和 OGD 在凸 Lipschitz 损失上的标准悔值保证），完成以下任务：\n- 在 $\\mathcal{X}$ 上给出残差大小 $\\|A_t x - b_t\\|_{2}$ 的显式界，推导在 $\\mathcal{X}$ 上梯度范数 $\\|\\nabla f_t(x)\\|_{2}$ 的一致界，并确定 $f_t$ 在 $\\mathcal{X}$ 上的指数凹性参数 $\\eta$。\n- 使用这些推导出的量，将标准的 ONS 悔值保证具体化到本问题设置中，以获得 $R_T$ 关于 $d$、$B$、$L$、$M$、$\\lambda_0$ 和 $T$ 的显式上界表达式（不含隐藏常数）。\n- 将标准的 OGD 对直径为 $D$ 的集合上的凸 Lipschitz 函数的悔值保证具体化到本问题设置中，以获得 $R_T$ 关于 $B$、$L$、$M$ 和 $T$ 的显式上界表达式（不含隐藏常数）。\n将你的最终答案以单行矩阵的形式给出，该矩阵按顺序包含这两个表达式：首先是 ONS 悔值上界，然后是 OGD 悔值上界。最终答案中不应出现不等式或方程；只写出两个简化的封闭形式解析表达式。无需四舍五入。", "solution": "用户希望我通过按预期格式提供所需信息来解决该问题。\n\n### 问题验证\n\n首先，对问题陈述进行严格的验证。\n\n**步骤1：提取已知条件**\n- 决策集：$\\mathcal{X} = \\{x \\in \\mathbb{R}^{d} : \\|x\\|_{2} \\leq B\\}$。\n- 参数：$d \\in \\mathbb{N}$，$B > 0$，$L > 0$，$M \\geq 0$，$\\lambda_0 > 0$。\n- 时间范围：$t = 1, 2, \\ldots, T$。\n- 损失函数：$f_t(x) = \\frac{1}{2}\\|A_t x - b_t\\|_{2}^{2}$。\n- 数据边界：$\\|A_t\\|_{2} \\leq L$ 和 $\\|b_t\\|_{2} \\leq M$。\n- 悔值定义：$R_T = \\sum_{t=1}^{T} f_t(x_t) - \\min_{x \\in \\mathcal{X}} \\sum_{t=1}^{T} f_t(x)$。\n- 考虑的算法：使用初始化矩阵 $\\lambda_0 I_d$ 的 Online Newton Step (ONS) 和使用最优选择的常数步长的 Online Gradient Descent (OGD)。\n- 允许的假设：使用在线优化理论中的基本定义和标准、经过充分检验的结论。\n\n**步骤2：使用提取的已知条件进行验证**\n- **科学依据：** 该问题是在线凸优化领域的一个标准练习。该设置涉及范数球上的二次损失函数（最小二乘法），是经典且被广泛研究的。所有提到的概念和工具（悔值、ONS、OGD、指数凹性）都是该领域的基础。该问题在科学上和数学上是合理的。\n- **适定性：** 问题要求为两种标准算法推导显式的悔值上界。鉴于这些算法的标准保证，该任务是明确定义的，并且可以根据所提供的参数得到唯一的解析表达式。\n- **客观性：** 问题以精确、形式化的数学语言陈述，没有歧义或主观性。\n- **完整性与一致性：** 提供了所有必要的参数（$d$, $B$, $L$, $M$, $\\lambda_0$, $T$）。问题明确允许使用标准结论，这使得推导所需界限的过程是自洽的。设置中没有矛盾。\n\n**步骤3：结论与行动**\n问题有效。我们可以继续进行求解。\n\n### 所需量的推导\n\n解决方案需要将 ONS 和 OGD 的标准悔值界具体化。为此，我们必须首先推导损失函数 $f_t(x)$ 在域 $\\mathcal{X}$ 上的关键性质。\n\n**1. 梯度范数的一致界**\n\n在第 $t$ 轮的损失函数是 $f_t(x) = \\frac{1}{2}\\|A_t x - b_t\\|_{2}^{2}$。它关于 $x$ 的梯度是：\n$$ \\nabla f_t(x) = A_t^T (A_t x - b_t) $$\n我们寻求其欧几里得范数 $\\|\\nabla f_t(x)\\|_{2}$ 在任意 $x \\in \\mathcal{X}$ 和任意 $t \\in \\{1, \\ldots, T\\}$ 下的一致上界。使用谱范数（$L_2$ 算子范数）的次可乘性和三角不等式：\n$$ \\|\\nabla f_t(x)\\|_{2} = \\|A_t^T (A_t x - b_t)\\|_{2} \\leq \\|A_t^T\\|_{2} \\|A_t x - b_t\\|_{2} $$\n因为 $\\|A_t^T\\|_{2} = \\|A_t\\|_{2} \\leq L$，这变为：\n$$ \\|\\nabla f_t(x)\\|_{2} \\leq L \\left( \\|A_t x\\|_{2} + \\|b_t\\|_{2} \\right) $$\n再次使用次可乘性以及对于 $x \\in \\mathcal{X}$，我们有 $\\|x\\|_{2} \\leq B$ 的事实：\n$$ \\|\\nabla f_t(x)\\|_{2} \\leq L \\left( \\|A_t\\|_{2} \\|x\\|_{2} + \\|b_t\\|_{2} \\right) \\leq L (L B + M) $$\n这提供了一个一致的 Lipschitz 常数，我们记为 $G$：\n$$ G = L(LB+M) $$\n\n**2. 指数凹性参数**\n\n如果一个可微函数 $f$ 的海森矩阵满足矩阵不等式 $\\nabla^2 f(x) \\succeq \\eta (\\nabla f(x))(\\nabla f(x))^T$，则该函数是 $\\eta$-指数凹的。$f_t(x)$ 的海森矩阵是：\n$$ \\nabla^2 f_t(x) = A_t^T A_t $$\n因此，指数凹性的条件是 $A_t^T A_t \\succeq \\eta (\\nabla f_t(x))(\\nabla f_t(x))^T$。对于任意向量 $z \\in \\mathbb{R}^d$，这意味着 $z^T (A_t^T A_t) z \\geq \\eta (z^T \\nabla f_t(x))^2$。左边是 $\\|A_t z\\|_2^2$。右边涉及 $(z^T \\nabla f_t(x))^2 = ((A_t z)^T(A_t x - b_t))^2$。根据柯西-施瓦茨不等式，它的上界是 $\\|A_t z\\|_2^2 \\|A_t x - b_t\\|_2^2$。\n因此，如果 $\\|A_t z\\|_2^2 \\geq \\eta \\|A_t z\\|_2^2 \\|A_t x - b_t\\|_2^2$，即 $1 \\geq \\eta \\|A_t x - b_t\\|_2^2$，则条件 $A_t^T A_t \\succeq \\eta (\\nabla f_t(x))(\\nabla f_t(x))^T$ 成立。\n为了找到对所有 $t$ 和所有 $x \\in \\mathcal{X}$ 都有效的一致参数 $\\eta$，我们需要 $\\|A_t x - b_t\\|_2$ 的一个上界。如在梯度界推导中所示，$\\|A_t x - b_t\\|_{2} \\leq LB + M$。\n因此，如果 $1 \\geq \\eta (LB+M)^2$，则条件成立。我们可以选择满足此条件的最大 $\\eta$，即：\n$$ \\eta = \\frac{1}{(LB+M)^2} $$\n\n### Online Newton Step (ONS) 悔值界\n\n对于一系列 $\\eta$-指数凹且梯度一致有界于 $G$ 的函数，使用初始化矩阵 $\\Lambda_0 = \\lambda_0 I_d$ 的 ONS 的标准悔值保证是：\n$$ R_T \\leq \\frac{d}{2\\eta} \\ln\\left(1 + \\frac{T G^2}{d \\lambda_0}\\right) $$\n代入推导出的量 $G = L(LB+M)$ 和 $\\eta = \\frac{1}{(LB+M)^2}$：\n$$ R_T \\leq \\frac{d}{2 \\left(\\frac{1}{(LB+M)^2}\\right)} \\ln\\left(1 + \\frac{T (L(LB+M))^2}{d \\lambda_0}\\right) $$\n简化此表达式得到 ONS 的显式上界：\n$$ R_T \\leq \\frac{d(LB+M)^2}{2} \\ln\\left(1 + \\frac{TL^2(LB+M)^2}{d\\lambda_0}\\right) $$\n\n### Online Gradient Descent (OGD) 悔值界\n\n对于在直径为 $D$ 的凸集 $\\mathcal{X}$ 上的一系列凸 $G$-Lipschitz 函数，使用最优常数步长的 OGD 的标准悔值保证是：\n$$ R_T \\leq D G \\sqrt{T} $$\n我们需要找到集合 $\\mathcal{X} = \\{x \\in \\mathbb{R}^{d} : \\|x\\|_{2} \\leq B\\}$ 的直径 $D$。直径定义为 $D = \\sup_{x,y \\in \\mathcal{X}} \\|x-y\\|_2$。根据三角不等式，$\\|x-y\\|_2 \\leq \\|x\\|_2 + \\|y\\|_2 \\leq B + B = 2B$。当选择任意 $\\|x\\|_2=B$ 的 $x$ 并令 $y=-x$ 时，可以达到这个最大值，此时 $\\|x-(-x)\\|_2 = \\|2x\\|_2 = 2\\|x\\|_2 = 2B$。因此，直径是：\n$$ D = 2B $$\n将 $D = 2B$ 和梯度界 $G = L(LB+M)$ 代入标准的 OGD 悔值保证中：\n$$ R_T \\leq (2B) (L(LB+M)) \\sqrt{T} $$\n简化此表达式得到 OGD 的显式上界：\n$$ R_T \\leq 2BL(LB+M)\\sqrt{T} $$\n\n### 最终答案的构建\n\n最终答案要求将两个推导出的悔值界——ONS 的和 OGD 的，按此顺序——以单行矩阵的形式呈现。\n- ONS 界: $\\frac{d(LB+M)^2}{2} \\ln\\left(1 + \\frac{TL^2(LB+M)^2}{d\\lambda_0}\\right)$\n- OGD 界: $2BL(LB+M)\\sqrt{T}$", "answer": "$$\n\\boxed{\\pmatrix{ \\frac{d(LB+M)^2}{2} \\ln\\left(1 + \\frac{TL^2(LB+M)^2}{d\\lambda_0}\\right) & 2BL(LB+M)\\sqrt{T} }}\n$$", "id": "3159405"}, {"introduction": "上一个练习主要在欧几里得空间中进行，然而许多决策问题的内在几何结构并非如此。这个实践将我们带入一个不同的领域，介绍“跟随正则化领导者”（FTRL）框架以及熵正则化，这是一种特别适用于在概率单纯形上做决策的算法（例如，资源分配或专家建议问题）。你将通过一个具体的例子计算FTRL算法的决策更新，从而深入理解非欧几里得正则化项如何塑造学习过程。[@problem_id:3159427]", "problem": "考虑一个在$3$维概率单纯形$\\Delta_{3}=\\{x\\in\\mathbb{R}^{3}: x_{i}\\geq 0,\\ \\sum_{i=1}^{3}x_{i}=1\\}$上的在线凸优化（Online Convex Optimization, OCO）问题。在每一轮$t\\in\\{1,2,3,4,5\\}$，学习者选择一个决策$x_{t}\\in\\Delta_{3}$，然后对手揭示一个线性损失函数$f_{t}(x)=\\ell_{t}\\cdot x$，其中$\\ell_{t}\\in\\mathbb{R}^{3}$。学习者使用带有熵正则化的“跟随正则化领导者”（Follow-The-Regularized-Leader, FTRL）算法，具体来说是负香农熵正则化项$R(x)=\\sum_{i=1}^{3} x_{i}\\ln x_{i}$，以及学习率参数$\\eta>0$。在第$t+1$轮的FTRL决策是以下凸规划问题的解\n$$\nx_{t+1}\\in\\arg\\min_{x\\in\\Delta_{3}}\\left\\{\\eta\\sum_{s=1}^{t}(\\ell_{s}\\cdot x)+R(x)\\right\\}.\n$$\n假设对手选择一系列与定义域角点对齐的损失，方式如下：对于一个固定的量值$M>0$，\n$$\n\\ell_{1}=(M,0,0),\\quad \\ell_{2}=(M,0,0),\\quad \\ell_{3}=(0,M,0),\\quad \\ell_{4}=(0,M,0),\\quad \\ell_{5}=(0,0,M).\n$$\n从没有先前损失开始，使用上述OCO和FTRL的定义以及凸优化的原理，推导经过这五轮后，由熵FTRL更新产生的学习者决策$x_{6}$的显式解析表达式。你的最终答案必须是$x_{6}$作为$\\Delta_{3}$中一个点的单一闭式表达式，用$\\eta$和$M$表示。无需四舍五入。", "solution": "用户希望找到一个在线凸优化问题在$5$轮之后学习者的决策向量$x_{6}$。学习者使用带有负香农熵正则化项的“跟随正则化领导者”（FTRL）算法。\n\n决策$x_{t+1}$通过求解以下优化问题来确定：\n$$\nx_{t+1} = \\arg\\min_{x\\in\\Delta_{3}}\\left\\{\\eta\\sum_{s=1}^{t}(\\ell_{s}\\cdot x)+R(x)\\right\\}\n$$\n其中$\\Delta_{3}=\\{x\\in\\mathbb{R}^{3}: x_{i}\\geq 0,\\ \\sum_{i=1}^{3}x_{i}=1\\}$是$3$维概率单纯形，$R(x)=\\sum_{i=1}^{3} x_{i}\\ln x_{i}$是正则化项，$\\eta > 0$是学习率。\n\n我们需要计算$x_{6}$，这意味着我们需要将从$t=1$到$t=5$的损失相加。令$L_{5}$为累积损失向量，$L_{5} = \\sum_{s=1}^{5} \\ell_{s}$。给定的损失向量是：\n$$\n\\ell_{1}=(M,0,0) \\\\\n\\ell_{2}=(M,0,0) \\\\\n\\ell_{3}=(0,M,0) \\\\\n\\ell_{4}=(0,M,0) \\\\\n\\ell_{5}=(0,0,M)\n$$\n按分量将这些向量相加，我们得到：\n$$\nL_{5} = (M+M, M+M, M) = (2M, 2M, M)\n$$\n$x_{6}$的优化问题是：\n$$\nx_{6} = \\arg\\min_{x\\in\\Delta_{3}}\\left\\{\\eta (L_{5} \\cdot x) + R(x)\\right\\}\n$$\n我们写出目标函数，记为$F(x)$：\n$$\nF(x) = \\eta (2Mx_{1} + 2Mx_{2} + Mx_{3}) + x_{1}\\ln x_{1} + x_{2}\\ln x_{2} + x_{3}\\ln x_{3}\n$$\n这是一个带有约束$x_{1}+x_{2}+x_{3}=1$和$x_{i} \\geq 0$（对于$i \\in \\{1, 2, 3\\}$）的凸优化问题。我们使用拉格朗日乘数法来找到解。拉格朗日函数$\\mathcal{L}$是：\n$$\n\\mathcal{L}(x, \\nu, \\lambda_{1}, \\lambda_{2}, \\lambda_{3}) = F(x) + \\nu\\left(\\sum_{i=1}^{3}x_{i} - 1\\right) - \\sum_{i=1}^{3}\\lambda_{i}x_{i}\n$$\n其中$\\nu$是等式约束的拉格朗日乘数，$\\lambda_i$是非负约束$x_i \\geq 0$的乘数。\n\n在最小值点必须满足卡鲁什-库恩-塔克（KKT）条件。平稳性条件要求将拉格朗日函数关于$x$的梯度设为零。对于每个分量$x_{i}$：\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial x_{i}} = \\eta (L_{5})_{i} + (\\ln x_{i} + 1) + \\nu - \\lambda_{i} = 0\n$$\n其中$(L_{5})_{i}$是向量$L_{5}$的第$i$个分量。\n\n正则化项$R(x)$是严格凸的，并且当任意$x_i \\to 0^+$时，其值趋近于$+\\infty$（因为$\\lim_{z\\to 0^+} z\\ln z = 0$，但其导数$\\ln z + 1 \\to -\\infty$）。这起到了一个屏障的作用，确保对于有限的损失值，最优解将位于单纯形的内部，即对于所有$i$，$x_{i} > 0$。\n根据KKT互补松弛条件，$\\lambda_{i}x_{i} = 0$。由于$x_{i} > 0$，我们必须有$\\lambda_{i} = 0$，对于$i=1, 2, 3$。\n\n因此，平稳性条件简化为：\n$$\n\\eta (L_{5})_{i} + \\ln x_{i} + 1 + \\nu = 0\n$$\n求解$x_{i}$：\n$$\n\\ln x_{i} = -\\eta (L_{5})_{i} - 1 - \\nu\n$$\n$$\nx_{i} = \\exp(-\\eta (L_{5})_{i} - 1 - \\nu) = \\exp(-\\eta (L_{5})_{i}) \\exp(-1-\\nu)\n$$\n项$\\exp(-1-\\nu)$对所有$i$都是一个常数。我们可以使用单纯形约束$\\sum_{i=1}^{3}x_{i} = 1$来找到它的值：\n$$\n\\sum_{i=1}^{3} \\exp(-\\eta (L_{5})_{i}) \\exp(-1-\\nu) = 1\n$$\n$$\n\\exp(-1-\\nu) \\left( \\sum_{j=1}^{3} \\exp(-\\eta (L_{5})_{j}) \\right) = 1\n$$\n$$\n\\exp(-1-\\nu) = \\frac{1}{\\sum_{j=1}^{3} \\exp(-\\eta (L_{5})_{j})}\n$$\n将其代回到$x_{i}$的表达式中，我们得到通解，这对应于将softmax函数应用于负的累积损失：\n$$\nx_{i} = \\frac{\\exp(-\\eta (L_{5})_{i})}{\\sum_{j=1}^{3} \\exp(-\\eta (L_{5})_{j})}\n$$\n现在，我们代入累积损失向量$L_{5} = (2M, 2M, M)$的具体分量，来求$x_{6} = (x_{6,1}, x_{6,2}, x_{6,3})$的分量。分母是：\n$$\nD = \\sum_{j=1}^{3} \\exp(-\\eta (L_{5})_{j}) = \\exp(-\\eta (2M)) + \\exp(-\\eta (2M)) + \\exp(-\\eta M) = 2\\exp(-2\\eta M) + \\exp(-\\eta M)\n$$\n$x_{6}$的分量是：\n$$\nx_{6,1} = \\frac{\\exp(-\\eta (L_{5})_{1})}{D} = \\frac{\\exp(-2\\eta M)}{2\\exp(-2\\eta M) + \\exp(-\\eta M)}\n$$\n$$\nx_{6,2} = \\frac{\\exp(-\\eta (L_{5})_{2})}{D} = \\frac{\\exp(-2\\eta M)}{2\\exp(-2\\eta M) + \\exp(-\\eta M)}\n$$\n$$\nx_{6,3} = \\frac{\\exp(-\\eta (L_{5})_{3})}{D} = \\frac{\\exp(-\\eta M)}{2\\exp(-2\\eta M) + \\exp(-\\eta M)}\n$$\n为了简化这些表达式，我们可以将每个分数的分子和分母同乘以$\\exp(2\\eta M)$：\n$$\nx_{6,1} = \\frac{\\exp(-2\\eta M) \\cdot \\exp(2\\eta M)}{(2\\exp(-2\\eta M) + \\exp(-\\eta M)) \\cdot \\exp(2\\eta M)} = \\frac{1}{2 + \\exp(\\eta M)}\n$$\n$$\nx_{6,2} = \\frac{\\exp(-2\\eta M) \\cdot \\exp(2\\eta M)}{(2\\exp(-2\\eta M) + \\exp(-\\eta M)) \\cdot \\exp(2\\eta M)} = \\frac{1}{2 + \\exp(\\eta M)}\n$$\n$$\nx_{6,3} = \\frac{\\exp(-\\eta M) \\cdot \\exp(2\\eta M)}{(2\\exp(-2\\eta M) + \\exp(-\\eta M)) \\cdot \\exp(2\\eta M)} = \\frac{\\exp(\\eta M)}{2 + \\exp(\\eta M)}\n$$\n因此，学习者的决策向量$x_{6}$是：\n$$\nx_{6} = \\left( \\frac{1}{2 + \\exp(\\eta M)}, \\frac{1}{2 + \\exp(\\eta M)}, \\frac{\\exp(\\eta M)}{2 + \\exp(\\eta M)} \\right)\n$$", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{1}{2 + \\exp(\\eta M)} & \\frac{1}{2 + \\exp(\\eta M)} & \\frac{\\exp(\\eta M)}{2 + \\exp(\\eta M)}\n\\end{pmatrix}\n}\n$$", "id": "3159427"}, {"introduction": "理论分析常常依赖于理想化的假设，比如梯度有界。但在实践中，梯度可能会出乎意料地大，从而破坏算法的稳定性。最后的这个实践直面这一挑战，分析了“梯度裁剪”这一在现代机器学习中无处不在的技术。我们将严格推导带有梯度裁剪的OGD算法的遗憾界，并揭示其在控制梯度范数和引入偏差之间的根本性权衡。[@problem_id:3159395]", "problem": "考虑在欧几里得直径为 $D$ 的闭凸集 $\\mathcal{K} \\subset \\mathbb{R}^{d}$ 上的在线凸优化问题，使用在线梯度下降（OGD）算法。直径为 $D$ 意味着对于所有 $\\mathbf{x}, \\mathbf{y} \\in \\mathcal{K}$，都有 $\\|\\mathbf{x} - \\mathbf{y}\\| \\le D$。在每一轮 $t \\in \\{1,2,\\dots,T\\}$，在选择 $\\mathbf{x}_{t} \\in \\mathcal{K}$ 之后，一个凸损失函数 $f_{t} : \\mathcal{K} \\to \\mathbb{R}$ 会被揭示。我们定义相对于一个固定比较点 $\\mathbf{x}^{\\star} \\in \\mathcal{K}$ 的悔憾为\n$$\nR_{T} \\triangleq \\sum_{t=1}^{T} \\big( f_{t}(\\mathbf{x}_{t}) - f_{t}(\\mathbf{x}^{\\star}) \\big).\n$$\n假设可以获取梯度 $\\mathbf{g}_{t} \\triangleq \\nabla f_{t}(\\mathbf{x}_{t})$，但为了强制执行悔憾证明中使用的有界梯度假设，您在阈值 $C > 0$ 处应用梯度裁剪，定义裁剪后的梯度为\n$$\n\\mathbf{u}_{t} \\triangleq \\begin{cases}\n\\mathbf{g}_{t},  \\text{若 } \\|\\mathbf{g}_{t}\\| \\le C, \\\\\nC \\, \\dfrac{\\mathbf{g}_{t}}{\\|\\mathbf{g}_{t}\\|},  \\text{若 } \\|\\mathbf{g}_{t}\\| > C.\n\\end{cases}\n$$\nOGD 更新使用裁剪后的梯度：\n$$\n\\mathbf{x}_{t+1} \\triangleq \\Pi_{\\mathcal{K}}\\big( \\mathbf{x}_{t} - \\eta \\mathbf{u}_{t} \\big),\n$$\n其中 $\\Pi_{\\mathcal{K}}$ 表示到集合 $\\mathcal{K}$ 上的欧几里得投影，$\\eta > 0$ 是步长。\n\n从基本定义和性质（凸性：$f_{t}(\\mathbf{x}_{t}) - f_{t}(\\mathbf{x}^{\\star}) \\le \\langle \\nabla f_{t}(\\mathbf{x}_{t}), \\mathbf{x}_{t} - \\mathbf{x}^{\\star} \\rangle$，欧几里得投影的非扩张性，以及柯西-施瓦茨不等式）出发，推导 $R_{T}$ 的一个上界，该上界应明确体现由裁剪强制执行的有界梯度项与裁剪引起的偏差之间的权衡。为了量化偏差，假设梯度遵循以下数据模型：恰好有 $k$ 轮，其中 $\\|\\mathbf{g}_{t}\\| = G$ 且 $G > C$；在所有其他轮次中，$\\|\\mathbf{g}_{t}\\| \\le C$。对于那 $k$ 轮，裁剪导致的差异为 $\\|\\mathbf{g}_{t} - \\mathbf{u}_{t}\\| = G - C$，并且你可以使用 $\\mathcal{K}$ 的直径，将每轮裁剪偏差的内积贡献的上界设为 $D (G - C)$。\n\n选择 $\\eta$ 以最小化您推导出的关于 $\\eta$ 的上界，并以 $D, C, T, k$ 和 $G$ 的单一闭式解析表达式的形式报告 $R_{T}$ 的最小化上界。不需要进行数值评估，也不涉及任何单位。最终答案必须仅为该单一表达式。", "solution": "该问题要求为带梯度裁剪的在线梯度下降（OGD）的悔憾 $R_{T}$ 推导一个上界。推导必须从基本原理出发，并要考虑到裁剪引入的偏差。最终的上界必须相对于步长 $\\eta$ 进行最小化。\n\n悔憾定义为 $R_{T} \\triangleq \\sum_{t=1}^{T} \\big( f_{t}(\\mathbf{x}_{t}) - f_{t}(\\mathbf{x}^{\\star}) \\big)$。\n由于每个损失函数 $f_t$ 都是凸的，我们可以应用凸性的一阶条件，该条件指出对于定义域中的任何 $\\mathbf{x}, \\mathbf{y}$，都有 $f_t(\\mathbf{x}) - f_t(\\mathbf{y}) \\le \\langle \\nabla f_t(\\mathbf{x}), \\mathbf{x} - \\mathbf{y} \\rangle$。将此应用于我们的上下文，其中 $\\mathbf{g}_{t} \\triangleq \\nabla f_{t}(\\mathbf{x}_{t})$：\n$$\nf_{t}(\\mathbf{x}_{t}) - f_{t}(\\mathbf{x}^{\\star}) \\le \\langle \\mathbf{g}_{t}, \\mathbf{x}_{t} - \\mathbf{x}^{\\star} \\rangle.\n$$\n对从 $t=1$ 到 $T$ 的所有轮次求和：\n$$\nR_{T} \\le \\sum_{t=1}^{T} \\langle \\mathbf{g}_{t}, \\mathbf{x}_{t} - \\mathbf{x}^{\\star} \\rangle.\n$$\nOGD 更新规则使用的是裁剪后的梯度 $\\mathbf{u}_t$，而不是真实的梯度 $\\mathbf{g}_t$。为了分析算法的性能，我们必须将 $\\mathbf{u}_t$ 引入表达式中。我们将内积分解如下：\n$$\n\\langle \\mathbf{g}_{t}, \\mathbf{x}_{t} - \\mathbf{x}^{\\star} \\rangle = \\langle \\mathbf{u}_{t} + (\\mathbf{g}_{t} - \\mathbf{u}_{t}), \\mathbf{x}_{t} - \\mathbf{x}^{\\star} \\rangle = \\langle \\mathbf{u}_{t}, \\mathbf{x}_{t} - \\mathbf{x}^{\\star} \\rangle + \\langle \\mathbf{g}_{t} - \\mathbf{u}_{t}, \\mathbf{x}_{t} - \\mathbf{x}^{\\star} \\rangle.\n$$\n第一项与算法的更新有关，而第二项是裁剪引入的偏差。因此，悔憾的上界是两个分量之和：\n$$\nR_{T} \\le \\sum_{t=1}^{T} \\langle \\mathbf{u}_{t}, \\mathbf{x}_{t} - \\mathbf{x}^{\\star} \\rangle + \\sum_{t=1}^{T} \\langle \\mathbf{g}_{t} - \\mathbf{u}_{t}, \\mathbf{x}_{t} - \\mathbf{x}^{\\star} \\rangle.\n$$\n让我们分别分析每个分量。\n\n首先，考虑涉及偏差的项 $\\sum_{t=1}^{T} \\langle \\mathbf{g}_{t} - \\mathbf{u}_{t}, \\mathbf{x}_{t} - \\mathbf{x}^{\\star} \\rangle$。\n根据问题的数据模型，有两种类型的轮次。对于 $\\|\\mathbf{g}_{t}\\| \\le C$ 的 $T-k$ 轮，裁剪未被激活，所以 $\\mathbf{u}_{t} = \\mathbf{g}_{t}$，这意味着 $\\mathbf{g}_{t} - \\mathbf{u}_{t} = \\mathbf{0}$。这些轮次的贡献为零。\n对于 $\\|\\mathbf{g}_{t}\\| = G > C$ 的 $k$ 轮，问题给出了内积的明确上界：$\\langle \\mathbf{g}_{t} - \\mathbf{u}_{t}, \\mathbf{x}_{t} - \\mathbf{x}^{\\star} \\rangle \\le D (G - C)$。\n对所有 $T$ 轮求和，偏差项的总贡献上界为：\n$$\n\\sum_{t=1}^{T} \\langle \\mathbf{g}_{t} - \\mathbf{u}_{t}, \\mathbf{x}_{t} - \\mathbf{x}^{\\star} \\rangle \\le k D (G - C).\n$$\n\n接下来，我们分析标准的 OGD 项 $\\sum_{t=1}^{T} \\langle \\mathbf{u}_{t}, \\mathbf{x}_{t} - \\mathbf{x}^{\\star} \\rangle$。我们使用基于到比较点的平方欧几里得距离 $\\|\\mathbf{x}_{t} - \\mathbf{x}^{\\star}\\|^{2}$ 的势函数论证。更新规则是 $\\mathbf{x}_{t+1} = \\Pi_{\\mathcal{K}}(\\mathbf{x}_{t} - \\eta \\mathbf{u}_{t})$。\n让我们分析第 $t+1$ 步的距离：\n$$\n\\|\\mathbf{x}_{t+1} - \\mathbf{x}^{\\star}\\|^{2} = \\|\\Pi_{\\mathcal{K}}(\\mathbf{x}_{t} - \\eta \\mathbf{u}_{t}) - \\mathbf{x}^{\\star}\\|^{2}.\n$$\n由于 $\\mathbf{x}^{\\star} \\in \\mathcal{K}$，我们可以使用欧几里得投影 $\\Pi_{\\mathcal{K}}$ 的非扩张性质，该性质指出对于任何向量 $\\mathbf{z}$ 和任何点 $\\mathbf{y} \\in \\mathcal{K}$，都有 $\\|\\Pi_{\\mathcal{K}}(\\mathbf{z}) - \\mathbf{y}\\| \\le \\|\\mathbf{z} - \\mathbf{y}\\|$。将此应用于 $\\mathbf{z} = \\mathbf{x}_{t} - \\eta \\mathbf{u}_{t}$ 和 $\\mathbf{y} = \\mathbf{x}^{\\star}$：\n$$\n\\|\\mathbf{x}_{t+1} - \\mathbf{x}^{\\star}\\|^{2} \\le \\|\\mathbf{x}_{t} - \\eta \\mathbf{u}_{t} - \\mathbf{x}^{\\star}\\|^{2} = \\|(\\mathbf{x}_{t} - \\mathbf{x}^{\\star}) - \\eta \\mathbf{u}_{t}\\|^{2}.\n$$\n展开平方范数：\n$$\n\\|\\mathbf{x}_{t+1} - \\mathbf{x}^{\\star}\\|^{2} \\le \\|\\mathbf{x}_{t} - \\mathbf{x}^{\\star}\\|^{2} - 2\\eta \\langle \\mathbf{u}_{t}, \\mathbf{x}_{t} - \\mathbf{x}^{\\star} \\rangle + \\eta^{2} \\|\\mathbf{u}_{t}\\|^{2}.\n$$\n重新整理这个不等式以分离出内积项，我们得到：\n$$\n\\langle \\mathbf{u}_{t}, \\mathbf{x}_{t} - \\mathbf{x}^{\\star} \\rangle \\le \\frac{1}{2\\eta} \\left( \\|\\mathbf{x}_{t} - \\mathbf{x}^{\\star}\\|^{2} - \\|\\mathbf{x}_{t+1} - \\mathbf{x}^{\\star}\\|^{2} \\right) + \\frac{\\eta}{2} \\|\\mathbf{u}_{t}\\|^{2}.\n$$\n将此不等式从 $t=1$ 到 $T$ 求和：\n$$\n\\sum_{t=1}^{T} \\langle \\mathbf{u}_{t}, \\mathbf{x}_{t} - \\mathbf{x}^{\\star} \\rangle \\le \\frac{1}{2\\eta} \\sum_{t=1}^{T} \\left( \\|\\mathbf{x}_{t} - \\mathbf{x}^{\\star}\\|^{2} - \\|\\mathbf{x}_{t+1} - \\mathbf{x}^{\\star}\\|^{2} \\right) + \\frac{\\eta}{2} \\sum_{t=1}^{T} \\|\\mathbf{u}_{t}\\|^{2}.\n$$\n右边的第一个和是一个伸缩级数：\n$$\n\\sum_{t=1}^{T} \\left( \\|\\mathbf{x}_{t} - \\mathbf{x}^{\\star}\\|^{2} - \\|\\mathbf{x}_{t+1} - \\mathbf{x}^{\\star}\\|^{2} \\right) = \\|\\mathbf{x}_{1} - \\mathbf{x}^{\\star}\\|^{2} - \\|\\mathbf{x}_{T+1} - \\mathbf{x}^{\\star}\\|^{2}.\n$$\n由于 $\\mathbf{x}_{1}, \\mathbf{x}^{\\star} \\in \\mathcal{K}$，距离 $\\|\\mathbf{x}_{1} - \\mathbf{x}^{\\star}\\|$ 受集合 $\\mathcal{K}$ 的直径 $D$ 的限制，因此 $\\|\\mathbf{x}_{1} - \\mathbf{x}^{\\star}\\|^{2} \\le D^{2}$。同时，$\\|\\mathbf{x}_{T+1} - \\mathbf{x}^{\\star}\\|^{2} \\ge 0$。因此，这个伸缩和的上界为 $D^{2}$。\n对于第二个和，我们使用裁剪梯度的性质：根据定义，对所有 $t$ 都有 $\\|\\mathbf{u}_{t}\\| \\le C$。因此，$\\|\\mathbf{u}_{t}\\|^{2} \\le C^{2}$，并且\n$$\n\\sum_{t=1}^{T} \\|\\mathbf{u}_{t}\\|^{2} \\le \\sum_{t=1}^{T} C^{2} = T C^{2}.\n$$\n代入这些界限，我们得到：\n$$\n\\sum_{t=1}^{T} \\langle \\mathbf{u}_{t}, \\mathbf{x}_{t} - \\mathbf{x}^{\\star} \\rangle \\le \\frac{D^{2}}{2\\eta} + \\frac{\\eta T C^{2}}{2}.\n$$\n结合悔憾的两个分量的界限：\n$$\nR_{T} \\le \\left( \\frac{D^{2}}{2\\eta} + \\frac{\\eta T C^{2}}{2} \\right) + k D (G - C).\n$$\n这个表达式是我们对悔憾的上界，它是步长 $\\eta$ 的函数。为了找到最紧的上界，我们必须选择 $\\eta$ 来最小化这个表达式。项 $k D (G-C)$ 关于 $\\eta$ 是一个常数。我们需要最小化 $f(\\eta) = \\frac{D^{2}}{2\\eta} + \\frac{\\eta T C^{2}}{2}$。\n我们对 $\\eta$ 求导数并令其为0：\n$$\n\\frac{d f}{d \\eta} = -\\frac{D^{2}}{2\\eta^{2}} + \\frac{T C^{2}}{2} = 0.\n$$\n解出 $\\eta$：\n$$\n\\frac{T C^{2}}{2} = \\frac{D^{2}}{2\\eta^{2}} \\implies \\eta^{2} = \\frac{D^{2}}{T C^{2}} \\implies \\eta^{\\star} = \\frac{D}{C\\sqrt{T}}.\n$$\n我们选择正根，因为 $\\eta > 0$。二阶导数 $\\frac{d^{2}f}{d\\eta^{2}} = \\frac{D^{2}}{\\eta^{3}}$ 对于 $\\eta>0$ 是正的，这证实了这是一个最小值点。\n最后，我们将最优步长 $\\eta^{\\star}$ 代回悔憾上界：\n$$\nR_{T} \\le \\frac{D^{2}}{2 \\left(\\frac{D}{C\\sqrt{T}}\\right)} + \\frac{\\left(\\frac{D}{C\\sqrt{T}}\\right) T C^{2}}{2} + k D (G - C)\n$$\n$$\nR_{T} \\le \\frac{D^{2} C \\sqrt{T}}{2D} + \\frac{D T C^{2}}{2 C \\sqrt{T}} + k D (G - C)\n$$\n$$\nR_{T} \\le \\frac{D C \\sqrt{T}}{2} + \\frac{D C \\sqrt{T}}{2} + k D (G - C)\n$$\n$$\nR_{T} \\le D C \\sqrt{T} + k D (G - C).\n$$\n这就是悔憾 $R_T$ 的最小化上界。", "answer": "$$\\boxed{D C \\sqrt{T} + k D (G - C)}$$", "id": "3159395"}]}