{"hands_on_practices": [{"introduction": "现实世界中的物理系统普遍存在时间延迟，例如执行器响应延迟或信号传输延迟，这给控制器设计带来了挑战。本练习介绍一种处理输入延迟的强大技术：状态增广法。通过将过去的控制输入视为系统状态的一部分，我们可以将一个带有时滞的系统转化为一个等效的、无时滞的标准状态空间形式，从而能够应用我们熟悉的线性二次调节器（LQR）框架来求解。[@problem_id:3121144] 这项实践不仅能帮助你掌握处理时滞系统的关键技能，还能加深你对离散时间Riccati递归在增广系统上应用的理解。", "problem": "考虑一个由 $x_{k+1} = a x_{k} + b u_{k-1}$ 给出的带有一步输入延迟的离散时间线性系统，其中 $x_{k} \\in \\mathbb{R}$ 是状态，而 $u_{k} \\in \\mathbb{R}$ 是控制输入。延迟为 $d=1$。请您将这个延迟系统转换为一个等效的无延迟增广系统，并通过 Riccati 递归推导出最优控制。\n\n设有限时域性能指标为\n$$J = \\sum_{k=0}^{N-1} \\left( x_{k}^{2} q + u_{k}^{2} r \\right) + x_{N}^{2} q_{f},$$\n其中 $q$、$r$ 和 $q_{f}$ 为非负标量，时域长度 $N \\in \\mathbb{N}$。假设最优控制是使用线性二次调节器（LQR）计算的，该调节器由动态规划原理推导得出。\n\n任务：\n1. 引入增广状态 $z_{k} \\triangleq \\begin{pmatrix} x_{k} \\\\ u_{k-1} \\end{pmatrix}$，并以 $z_{k+1} = \\bar{A} z_{k} + \\bar{B} u_{k}$ 的形式写出无延迟的增广动力学方程，用 $a$ 和 $b$ 识别 $\\bar{A}$ 和 $\\bar{B}$。\n2. 从动态规划原理出发，证明最优未来成本是增广状态的二次型 $J_{k}^{\\star}(z_{k}) = z_{k}^{\\top} P_{k} z_{k}$，并推导出 $P_{k}$ 的 Riccati 递归，以及相应的最优状态反馈控制律 $u_{k}^{\\star} = -K_{k} z_{k}$（用 $\\bar{A}$、$\\bar{B}$、$Q_{z}$ 和 $R$ 表示），其中 $Q_{z}$ 是与给定性能指标一致的增广状态惩罚矩阵。\n3. 对于特定的系统参数 $a = 2$、$b = 1$、$q = 1$、$r = 1$、$q_{f} = 1$，时域 $N = 2$，以及初始条件 $x_{0} = 2$、$u_{-1} = 1$，计算最优控制输入 $u_{0}^{\\star}$。您的最终答案必须是一个实数。无需四舍五入。", "solution": "### 任务1：增广系统构建\n\n给定的带输入延迟的离散时间线性系统是：\n$$x_{k+1} = a x_{k} + b u_{k-1}$$\n其中 $x_k \\in \\mathbb{R}$ 是状态，$u_k \\in \\mathbb{R}$ 是在时刻 $k$ 的控制输入。增广状态向量定义为 $z_{k} \\triangleq \\begin{pmatrix} x_{k} \\\\ u_{k-1} \\end{pmatrix}$。\n\n我们希望以 $z_{k+1} = \\bar{A} z_{k} + \\bar{B} u_{k}$ 的形式找到增广系统动力学方程。让我们构建在时刻 $k+1$ 的状态向量：\n$$z_{k+1} = \\begin{pmatrix} x_{k+1} \\\\ u_{(k+1)-1} \\end{pmatrix} = \\begin{pmatrix} x_{k+1} \\\\ u_{k} \\end{pmatrix}$$\n第一个分量 $x_{k+1}$ 由系统动力学给出：$x_{k+1} = a x_{k} + b u_{k-1}$。用 $z_k$ 的分量表示，即 $x_{k+1} = a x_{k} + b u_{k-1} + 0 \\cdot u_k$。\n第二个分量 $u_k$ 可以写成一个平凡的恒等式：$u_k = 0 \\cdot x_k + 0 \\cdot u_{k-1} + 1 \\cdot u_k$。\n\n将这两个方程组合成矩阵形式，我们得到：\n$$z_{k+1} = \\begin{pmatrix} x_{k+1} \\\\ u_{k} \\end{pmatrix} = \\begin{pmatrix} a  b \\\\ 0  0 \\end{pmatrix} \\begin{pmatrix} x_{k} \\\\ u_{k-1} \\end{pmatrix} + \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} u_{k}$$\n这与期望的形式 $z_{k+1} = \\bar{A} z_{k} + \\bar{B} u_{k}$ 相匹配，其中增广系统矩阵为：\n$$\\bar{A} = \\begin{pmatrix} a  b \\\\ 0  0 \\end{pmatrix}, \\quad \\bar{B} = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$$\n\n### 任务2：Riccati 递归推导\n\n性能指标由下式给出：\n$$J = \\sum_{k=0}^{N-1} \\left( x_{k}^{2} q + u_{k}^{2} r \\right) + x_{N}^{2} q_{f}$$\n我们用增广状态 $z_k$ 重写这个代价函数。\n阶段代价是 $L(x_k, u_k) = x_k^2 q + u_k^2 r$。\n由于 $x_k = \\begin{pmatrix} 1  0 \\end{pmatrix} z_k$，我们有 $x_k^2 q = \\left(\\begin{pmatrix} 1  0 \\end{pmatrix} z_k\\right)^2 q = z_k^\\top \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} q \\begin{pmatrix} 1  0 \\end{pmatrix} z_k = z_k^\\top \\begin{pmatrix} q  0 \\\\ 0  0 \\end{pmatrix} z_k$。\n因此，阶段代价可以写成 $L(z_k, u_k) = z_k^\\top Q_z z_k + u_k^\\top R u_k$，其中 $Q_z = \\begin{pmatrix} q  0 \\\\ 0  0 \\end{pmatrix}$ 和 $R = r$。\n\n终端代价是 $x_N^2 q_f$。类似地，这变为 $z_N^\\top P_N z_N$，其中 $P_N = \\begin{pmatrix} q_f  0 \\\\ 0  0 \\end{pmatrix}$。\n增广 LQR 问题是在约束 $z_{k+1} = \\bar{A} z_k + \\bar{B} u_k$ 下最小化 $J = \\sum_{k=0}^{N-1} (z_k^\\top Q_z z_k + u_k^\\top R u_k) + z_N^\\top P_N z_N$。\n\n我们使用动态规划。令 $J_k^\\star(z_k)$ 为在时刻 $k$ 从状态 $z_k$ 出发的最优未来成本。Bellman 方程为：\n$$J_k^\\star(z_k) = \\min_{u_k} \\left[ z_k^\\top Q_z z_k + u_k^\\top R u_k + J_{k+1}^\\star(z_{k+1}) \\right]$$\n我们假设未来成本具有二次型形式：$J_k^\\star(z_k) = z_k^\\top P_k z_k$，其中 $P_k$ 是一个对称半正定矩阵。终端条件为 $J_N^\\star(z_N) = z_N^\\top P_N z_N$，因此我们有 $P_N = \\begin{pmatrix} q_f  0 \\\\ 0  0 \\end{pmatrix}$。\n\n代入 Bellman 方程：\n$$z_k^\\top P_k z_k = \\min_{u_k} \\left[ z_k^\\top Q_z z_k + u_k^\\top R u_k + (\\bar{A} z_k + \\bar{B} u_k)^\\top P_{k+1} (\\bar{A} z_k + \\bar{B} u_k) \\right]$$\n让我们展开最小化项内部的表达式，我们称之为 $\\mathcal{L}_k(z_k, u_k)$：\n$$\\mathcal{L}_k(z_k, u_k) = z_k^\\top Q_z z_k + u_k^\\top R u_k + z_k^\\top \\bar{A}^\\top P_{k+1} \\bar{A} z_k + 2 u_k^\\top \\bar{B}^\\top P_{k+1} \\bar{A} z_k + u_k^\\top \\bar{B}^\\top P_{k+1} \\bar{B} u_k$$\n为了找到最优控制 $u_k^\\star$，我们对 $\\mathcal{L}_k$ 关于 $u_k$ 求梯度并令其为零：\n$$\\frac{\\partial \\mathcal{L}_k}{\\partial u_k} = 2 R u_k + 2 \\bar{B}^\\top P_{k+1} \\bar{A} z_k + 2 \\bar{B}^\\top P_{k+1} \\bar{B} u_k = 0$$\n解出 $u_k$：\n$$(R + \\bar{B}^\\top P_{k+1} \\bar{B}) u_k = - \\bar{B}^\\top P_{k+1} \\bar{A} z_k$$\n$$u_k^\\star = - (R + \\bar{B}^\\top P_{k+1} \\bar{B})^{-1} (\\bar{B}^\\top P_{k+1} \\bar{A}) z_k$$\n这是一个状态反馈控制律 $u_k^\\star = -K_k z_k$，其中时变增益矩阵为：\n$$K_k = (R + \\bar{B}^\\top P_{k+1} \\bar{B})^{-1} \\bar{B}^\\top P_{k+1} \\bar{A}$$\n将 $u_k^\\star$ 代回 Bellman 方程，得到 $P_k$ 的 Riccati 递归：\n$$z_k^\\top P_k z_k = z_k^\\top Q_z z_k + (u_k^\\star)^\\top R u_k^\\star + (\\bar{A} z_k + \\bar{B} u_k^\\star)^\\top P_{k+1} (\\bar{A} z_k + \\bar{B} u_k^\\star)$$\n经过代数整理，这简化为：\n$$P_k = Q_z + \\bar{A}^\\top P_{k+1} \\bar{A} - (\\bar{A}^\\top P_{k+1} \\bar{B}) (R + \\bar{B}^\\top P_{k+1} \\bar{B})^{-1} (\\bar{B}^\\top P_{k+1} \\bar{A})$$\n这个方程从终端条件 $P_N$ 开始逆向时间求解。一个等价的形式是 $P_k = Q_z + \\bar{A}^\\top P_{k+1} \\bar{A} - K_k^\\top (R + \\bar{B}^\\top P_{k+1} \\bar{B}) K_k$。\n\n### 任务3：数值计算\n\n给定的参数是 $a = 2$、$b = 1$、$q = 1$、$r = 1$、$q_f = 1$ 和 $N = 2$。\n增广系统矩阵为：\n$$\\bar{A} = \\begin{pmatrix} 2  1 \\\\ 0  0 \\end{pmatrix}, \\quad \\bar{B} = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$$\n代价矩阵为：\n$$Q_z = \\begin{pmatrix} 1  0 \\\\ 0  0 \\end{pmatrix}, \\quad R = 1$$\n我们需要找到 $u_0^\\star$，这需要计算 $K_0$。这又需要 $P_1$，而 $P_1$ 是从 $P_2$ 计算得到的。\n\n**第1步：计算 $P_2$ (在 $k=N=2$ 时)**\n终端代价矩阵为：\n$$P_2 = P_N = \\begin{pmatrix} q_f  0 \\\\ 0  0 \\end{pmatrix} = \\begin{pmatrix} 1  0 \\\\ 0  0 \\end{pmatrix}$$\n\n**第2步：计算 $P_1$ (在 $k=N-1=1$ 时)**\n我们使用 $k=1$ 时的 Riccati 递归：\n$$P_1 = Q_z + \\bar{A}^\\top P_2 \\bar{A} - (\\bar{A}^\\top P_2 \\bar{B}) (R + \\bar{B}^\\top P_2 \\bar{B})^{-1} (\\bar{B}^\\top P_2 \\bar{A})$$\n让我们计算各项：\n$$\\bar{A}^\\top P_2 \\bar{A} = \\begin{pmatrix} 2  0 \\\\ 1  0 \\end{pmatrix} \\begin{pmatrix} 1  0 \\\\ 0  0 \\end{pmatrix} \\begin{pmatrix} 2  1 \\\\ 0  0 \\end{pmatrix} = \\begin{pmatrix} 2  0 \\\\ 1  0 \\end{pmatrix} \\begin{pmatrix} 2  1 \\\\ 0  0 \\end{pmatrix} = \\begin{pmatrix} 4  2 \\\\ 2  1 \\end{pmatrix}$$\n$$\\bar{B}^\\top P_2 \\bar{B} = \\begin{pmatrix} 0  1 \\end{pmatrix} \\begin{pmatrix} 1  0 \\\\ 0  0 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 0  0 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} = 0$$\n$$\\bar{A}^\\top P_2 \\bar{B} = \\begin{pmatrix} 2  0 \\\\ 1  0 \\end{pmatrix} \\begin{pmatrix} 1  0 \\\\ 0  0 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 2  0 \\\\ 1  0 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$$\n修正项为零。因此：\n$$P_1 = Q_z + \\bar{A}^\\top P_2 \\bar{A} = \\begin{pmatrix} 1  0 \\\\ 0  0 \\end{pmatrix} + \\begin{pmatrix} 4  2 \\\\ 2  1 \\end{pmatrix} = \\begin{pmatrix} 5  2 \\\\ 2  1 \\end{pmatrix}$$\n\n**第3步：计算 $K_0$ (在 $k=0$ 时)**\n增益 $K_0$ 由下式给出：\n$$K_0 = (R + \\bar{B}^\\top P_1 \\bar{B})^{-1} \\bar{B}^\\top P_1 \\bar{A}$$\n让我们计算各项：\n$$\\bar{B}^\\top P_1 \\bar{B} = \\begin{pmatrix} 0  1 \\end{pmatrix} \\begin{pmatrix} 5  2 \\\\ 2  1 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 2  1 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} = 1$$\n$$R + \\bar{B}^\\top P_1 \\bar{B} = 1 + 1 = 2$$\n$$\\bar{B}^\\top P_1 \\bar{A} = \\begin{pmatrix} 0  1 \\end{pmatrix} \\begin{pmatrix} 5  2 \\\\ 2  1 \\end{pmatrix} \\begin{pmatrix} 2  1 \\\\ 0  0 \\end{pmatrix} = \\begin{pmatrix} 2  1 \\end{pmatrix} \\begin{pmatrix} 2  1 \\\\ 0  0 \\end{pmatrix} = \\begin{pmatrix} 4  2 \\end{pmatrix}$$\n现在，我们可以计算 $K_0$：\n$$K_0 = (2)^{-1} \\begin{pmatrix} 4  2 \\end{pmatrix} = \\frac{1}{2} \\begin{pmatrix} 4  2 \\end{pmatrix} = \\begin{pmatrix} 2  1 \\end{pmatrix}$$\n\n**第4步：计算最优控制 $u_0^\\star$**\n最优控制为 $u_0^\\star = -K_0 z_0$。\n初始条件是 $x_0 = 2$ 和 $u_{-1} = 1$。初始增广状态是：\n$$z_0 = \\begin{pmatrix} x_0 \\\\ u_{-1} \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix}$$\n最后，我们计算 $u_0^\\star$：\n$$u_0^\\star = -K_0 z_0 = -\\begin{pmatrix} 2  1 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix} = -(2 \\cdot 2 + 1 \\cdot 1) = -(4+1) = -5$$\n在 $k=0$ 时的最优控制输入是 -5。", "answer": "$$\\boxed{-5}$$", "id": "3121144"}, {"introduction": "虽然线性二次调节器（LQR）在其二次型性能指标下被证明是“最优”的，但在工程实践中，我们常常面临多种设计选择。本练习将LQR与另一种经典控制方法——极点配置（或称特征值分配）——进行直接比较。通过为一个随机系统设计这两种控制器，你将探索不同设计哲学之间的权衡。[@problem_id:3121205] 这个过程将引导你不仅评估预设的成本函数，还要分析如鲁棒稳定性裕度、状态方差等其他关键性能指标，从而深刻理解“最优性”始终是相对于特定目标而言的。", "problem": "考虑具有过程噪声的标量离散时间线性系统\n$$x_{k+1} = a\\,x_k + b\\,u_k + w_k,$$\n其中 $a=1.2$, $b=1$，且 $\\{w_k\\}$ 是一个零均值独立同分布过程，其方差为 $\\sigma_w^2=0.04$。状态 $x_k$ 被完全测量。你将比较两种形式为 $u_k=-K\\,x_k$ 的静态状态反馈设计。\n\n控制器 EA (特征值配置)：选择 $K$ 将标称闭环特征值置于 $\\lambda_d=0.3$。\n\n控制器 LQR (线性二次调节器)：选择 $K$ 来最小化无限时域平均二次代价\n$$J=\\lim_{N\\to\\infty}\\frac{1}{N}\\,\\mathbb{E}\\left[\\sum_{k=0}^{N-1}\\left(Q\\,x_k^2 + R\\,u_k^2\\right)\\right]$$\n对于标称系统，其中 $Q=1$ 且 $R=2$。\n\n此外，考虑 $a$ 中存在形式为 $\\tilde a = a + \\delta$ 的加性参数不确定性，其中 $|\\delta|\\le \\Delta$。在离散时间中，闭环稳定性意味着闭环特征值的模严格小于 $1$。\n\n对于该系统和数据，以下哪个陈述是正确的？\n\nA. 对于给定的数据，线性二次调节器 (LQR) 反馈比特征值配置控制器实现了更低的稳态平均代价，但在保持闭环稳定性的同时，它能容忍的 $a$ 中的最大加性不确定性 $|\\delta|\\le \\Delta$ 更小。\n\nB. 因为两种设计都稳定了系统，所以对于给定的 $Q$ 和 $R$，它们的稳态平均代价相等。\n\nC. 在所述噪声下，对于特征值配置，状态 $x_k$ 的稳态方差比线性二次调节器 (LQR) 更大。\n\nD. 与线性二次调节器 (LQR) 相比，特征值配置控制器保证了对 $a$ 中加性不确定性的鲁棒稳定裕度更小。", "solution": "### 解题推导\n\n首先，我们确定闭环系统动力学。将控制律 $u_k = -K x_k$ 代入系统方程得到：\n$$x_{k+1} = a x_k + b(-K x_k) + w_k = (a - bK) x_k + w_k$$\n标称闭环特征值为 $\\lambda_{cl} = a - bK$。使用给定参数 $a=1.2$ 和 $b=1$，这变为 $\\lambda_{cl} = 1.2 - K$。\n\n**控制器 EA (特征值配置)**\n\n目标是将闭环特征值置于 $\\lambda_d = 0.3$。\n$$\\lambda_{EA} = a - b K_{EA} = 1.2 - (1)K_{EA} = 0.3$$\n求解反馈增益 $K_{EA}$：\n$$K_{EA} = 1.2 - 0.3 = 0.9$$\n\n**控制器 LQR (线性二次调节器)**\n\nLQR 增益 $K_{LQR}$ 最小化给定的代价函数。对于标量离散时间系统，增益通过离散代数Riccati方程 (DARE) 的解 $P$ 求得。DARE方程为：\n$$P = a^T P a - (a^T P b)(R + b^T P b)^{-1}(b^T P a) + Q$$\n代入标量值 $a=1.2$, $b=1$, $Q=1$, $R=2$：\n$$P = (1.2)^2 P - ((1.2)P(1))(2 + (1)^2 P)^{-1}((1)P(1.2)) + 1$$\n$$P = 1.44 P - \\frac{1.44 P^2}{2+P} + 1$$\n我们必须求解 $P$。由于对 $(a, \\sqrt{Q})$ 是可检测的（这里 $a=1.2 \\ne 0$）且对 $(a, b)$ 是可镇定的（这里 $a=1.2, b=1$ 是可控的，这是一个更强的条件），存在唯一的正定解 $P > 0$。\n$$P - 1 = 1.44 P - \\frac{1.44 P^2}{2+P}$$\n$$(P - 1)(2+P) = 1.44 P (2+P) - 1.44 P^2$$\n$$2P + P^2 - 2 - P = 2.88 P + 1.44 P^2 - 1.44 P^2$$\n$$P^2 + P - 2 = 2.88 P$$\n$$P^2 - 1.88 P - 2 = 0$$\n使用二次公式，$P = \\frac{-(-1.88) \\pm \\sqrt{(-1.88)^2 - 4(1)(-2)}}{2(1)}$：\n$$P = \\frac{1.88 \\pm \\sqrt{3.5344 + 8}}{2} = \\frac{1.88 \\pm \\sqrt{11.5344}}{2}$$\n由于 $P$ 必须为正，我们取正根：\n$$P \\approx \\frac{1.88 + 3.39623}{2} = \\frac{5.27623}{2} \\approx 2.6381$$\nLQR增益 $K_{LQR}$ 由以下公式给出：\n$$K_{LQR} = (R + b^T P b)^{-1}(b^T P a) = \\frac{abP}{R+b^2P}$$\n$$K_{LQR} \\approx \\frac{(1.2)(1)(2.6381)}{2+(1)^2(2.6381)} = \\frac{3.1657}{4.6381} \\approx 0.6826$$\nLQR控制器对应的闭环特征值为：\n$$\\lambda_{LQR} = a - b K_{LQR} \\approx 1.2 - (1)(0.6826) = 0.5174$$\n\n现在我们可以分析这两种设计的属性。\n\n**1. 稳态平均代价 ($J$) 和状态方差 ($\\text{Var}(x_k)$)**\n\n闭环系统为 $x_{k+1} = \\lambda_{cl} x_k + w_k$。在稳态下，设状态方差为 $V_x = \\mathbb{E}[x_k^2]$（因为 $\\mathbb{E}[x_k]=0$）。\n$$V_x = \\text{Var}(x_{k+1}) = \\text{Var}(\\lambda_{cl} x_k + w_k)$$\n由于 $x_k$ 依赖于过去的噪声项 $w_{k-1}, w_{k-2}, \\dots$，它与当前的噪声 $w_k$ 不相关。因此：\n$$V_x = \\lambda_{cl}^2 \\text{Var}(x_k) + \\text{Var}(w_k) = \\lambda_{cl}^2 V_x + \\sigma_w^2$$\n$$V_x(1 - \\lambda_{cl}^2) = \\sigma_w^2 \\implies V_x = \\frac{\\sigma_w^2}{1 - \\lambda_{cl}^2}$$\n稳态平均代价为 $J = \\mathbb{E}[Q x_k^2 + R u_k^2] = \\mathbb{E}[Q x_k^2 + R(-K x_k)^2]$。\n$$J = (Q + R K^2) \\mathbb{E}[x_k^2] = (Q + R K^2) V_x = (Q + R K^2) \\frac{\\sigma_w^2}{1 - \\lambda_{cl}^2}$$\n\n*   **对于控制器 EA：**\n    *   $K_{EA} = 0.9$, $\\lambda_{EA} = 0.3$。\n    *   状态方差：$V_{x,EA} = \\frac{0.04}{1 - (0.3)^2} = \\frac{0.04}{0.91} \\approx 0.04396$。\n    *   平均代价：$J_{EA} = (1 + 2(0.9)^2) V_{x,EA} = (1 + 1.62) (0.04396) = 2.62 \\times 0.04396 \\approx 0.1152$。\n\n*   **对于控制器 LQR：**\n    *   $K_{LQR} \\approx 0.6826$, $\\lambda_{LQR} \\approx 0.5174$。\n    *   状态方差：$V_{x,LQR} = \\frac{0.04}{1 - (0.5174)^2} \\approx \\frac{0.04}{1 - 0.2677} = \\frac{0.04}{0.7323} \\approx 0.05462$。\n    *   平均代价：$J_{LQR} = (1 + 2(0.6826)^2) V_{x,LQR} \\approx (1 + 2(0.466)) (0.05462) = 1.932 \\times 0.05462 \\approx 0.1055$。\n\n**比较：**\n*   代价：$J_{LQR} \\approx 0.1055  J_{EA} \\approx 0.1152$。LQR 控制器产生更低的代价，正如其定义所预期的。\n*   方差：$V_{x,LQR} \\approx 0.05462  V_{x,EA} \\approx 0.04396$。EA 控制器导致更小的稳态状态方差。\n\n**2. 鲁棒稳定裕度 ($\\Delta$)**\n\n受扰动的闭环特征值为 $\\lambda_{pert} = \\tilde{a} - bK = (a+\\delta) - bK = (a-bK) + \\delta = \\lambda_{cl} + \\delta$。\n为了保证稳定性，我们需要 $|\\lambda_{pert}|  1$，即 $|\\lambda_{cl} + \\delta|  1$。\n这展开为 $-1  \\lambda_{cl} + \\delta  1$，或 $-1 - \\lambda_{cl}  \\delta  1 - \\lambda_{cl}$。\n对于所有的 $|\\delta| \\le \\Delta$，如果区间 $[-\\Delta, \\Delta]$ 包含在 $(-1 - \\lambda_{cl}, 1 - \\lambda_{cl})$ 内，则控制器是鲁棒稳定的。这要求：\n$$ \\Delta  1 - \\lambda_{cl} \\quad \\text{和} \\quad \\Delta  1 + \\lambda_{cl}$$\n因此，最大可容忍的不确定性是 $\\Delta_{max} = \\min(1-\\lambda_{cl}, 1+\\lambda_{cl})$。\n由于 $\\lambda_{EA}$ 和 $\\lambda_{LQR}$ 都是正的，所以极限由 $1 - \\lambda_{cl}$ 决定。\n*   **对于控制器 EA：** $\\Delta_{max,EA} = 1 - \\lambda_{EA} = 1 - 0.3 = 0.7$。\n*   **对于控制器 LQR：** $\\Delta_{max,LQR} \\approx 1 - \\lambda_{LQR} = 1 - 0.5174 = 0.4826$。\n\n**比较：**\n*   稳定裕度：$\\Delta_{max,LQR} \\approx 0.4826  \\Delta_{max,EA} = 0.7$。LQR 控制器在参数 $a$ 中能容忍更小的不确定性范围。\n\n### 逐项分析\n\n**A. 对于给定的数据，线性二次调节器 (LQR) 反馈比特征值配置控制器实现了更低的稳态平均代价，但在保持闭环稳定性的同时，它能容忍的 $a$ 中的最大加性不确定性 $|\\delta|\\le \\Delta$ 更小。**\n*   我们的计算表明 $J_{LQR} \\approx 0.1055$ 且 $J_{EA} \\approx 0.1152$，所以 $J_{LQR}  J_{EA}$。这部分是正确的。\n*   我们的计算表明 $\\Delta_{max,LQR} \\approx 0.4826$ 且 $\\Delta_{max,EA} = 0.7$，所以 LQR 控制器能容忍更小的最大不确定性。这部分也是正确的。\n*   因此整个陈述是正确的。\n**结论：正确**\n\n**B. 因为两种设计都稳定了系统，所以对于给定的 $Q$ 和 $R$，它们的稳态平均代价相等。**\n*   我们的计算表明 $J_{LQR} \\approx 0.1055$ 且 $J_{EA} \\approx 0.1152$。这些代价不相等。代价不仅取决于稳定性，还取决于具体的闭环动力学和控制努力。\n**结论：不正确**\n\n**C. 在所述噪声下，对于特征值配置，状态 $x_k$ 的稳态方差比线性二次调节器 (LQR) 更大。**\n*   我们的计算表明 $V_{x,EA} \\approx 0.04396$ 且 $V_{x,LQR} \\approx 0.05462$。\n*   特征值配置的方差比 LQR *小*（$V_{x,EA}  V_{x,LQR}$）。该陈述的主张正好相反。\n**结论：不正确**\n\n**D. 与线性二次调节器 (LQR) 相比，特征值配置控制器保证了对 $a$ 中加性不确定性的鲁棒稳定裕度更小。**\n*   我们的计算表明 $\\Delta_{max,EA} = 0.7$ 且 $\\Delta_{max,LQR} \\approx 0.4826$。\n*   特征值配置控制器的裕度比 LQR 控制器 *大*。该陈述的主张正好相反。\n**结论：不正确**", "answer": "$$\\boxed{A}$$", "id": "3121205"}, {"introduction": "线性二次调节器（LQR）优美的解析解严格依赖于系统的线性动力学和二次型成本函数。当成本函数变为非凸（例如，控制成本在多个值附近较低）或系统约束变得复杂时，解析方法便不再适用。本练习将引导你进入数值求解的世界，通过在离散化的状态空间上实施动态规划（DP）算法来解决一个具有非凸控制成本的优化问题。[@problem_id:3121214] 这项实践不仅让你掌握了解决更广泛优化问题的基本数值工具，还通过与“短视”的贪心策略对比，直观地揭示了全局最优规划如何避免局部最优陷阱。", "problem": "考虑一个确定性的、有限时域的、离散时间的最优控制问题，其状态和控制均为标量。系统根据动态方程 $x_{k+1} = x_k + u_k$ 演化，其中时间指标 $k$ 为整数，$k = 0, 1, \\dots, N-1$。给定初始状态 $x_0 \\in \\mathbb{R}$，控制 $u_k$ 被约束在一个固定的、有限的容许值集合 $\\mathcal{U} \\subset \\mathbb{R}$ 中。阶段成本为 $c(x_k,u_k) = q \\, x_k^2 + \\ell(u_k)$，其中非凸的控制成本为\n$$\n\\ell(u) = \\min\\{(u - a)^2,\\,(u + b)^2\\},\n$$\n参数 $a  0$ 且 $b  0$。终端成本为 $\\phi(x_N) = p \\, x_N^2$，其中 $p  0$。需要最小化的总成本是时域内所有阶段成本与终端成本之和。\n\n仅从该控制问题的定义和动态规划的最优性原理出发，推导如何计算最优未来成本和最优策略。实现一个完整的算法，该算法能够：\n- 在状态空间上构建一个均匀网格 $\\mathcal{X}$，该网格需足以包含在时域 $N$ 内从 $x_0$ 出发、在控制集 $\\mathcal{U}$ 下所有可达的状态。\n- 通过反向动态规划计算状态网格上的最优策略，并从 $x_0$ 开始正向应用该策略，以生成实现的最优控制序列 $(u_0^{\\star},\\dots,u_{N-1}^{\\star})$ 和状态轨迹 $(x_0,\\dots,x_N)$。\n- 沿着此轨迹计算实现的最优总成本 $J^{\\star}$。\n- 计算一个贪心短视策略，该策略在每个时间步选择仅使 $\\ell(u_k)$ 最小化的控制 $u_k$（当出现平局时，必须选择绝对值最小的控制；若仍为平局，则选择数值最小的控制），并从 $x_0$ 开始正向应用该策略，以得出贪心策略下的总成本 $J^{\\mathrm{greedy}}$。\n- 通过计算在实现的最优控制序列中，$\\ell(u)$ 的选定激活分支变化的次数来识别策略切换。对于一个控制值 $u$，如果 $(u-a)^2 \\le (u+b)^2$，则定义其激活分支索引为 $0$，否则为 $1$。切换事件的数量是分支索引从 $k-1$ 到 $k$ 发生变化的索引 $k$ 的计数。\n- 根据以下规则检测局部最小值陷阱指示器：当且仅当贪心策略在所有时间步上都使用单一分支（无切换）、动态规划策略至少有一次切换，且 $J^{\\mathrm{greedy}}  J^{\\star}$ 时，返回一个为真的布尔值。\n\n您的实现必须通过将 $x_{k+1}$ 映射到最近的状态网格点来处理网格上的状态演化。如果 $x_{k+1}$ 超出网格范围，则必须将其裁剪到最近的边界网格点。您必须为动态规划定义一个平局打破规则：当多个控制达到相同的最小值时，选择绝对值最小的控制；若仍为平局，则选择数值最小的控制。\n\n测试套件。对于以下每个参数集，运行算法并收集该测试用例的四元组结果：$[J^{\\star},\\,S^{\\star},\\,\\text{is\\_greedy\\_suboptimal},\\,\\text{is\\_trap}]$，其中 $J^{\\star}$ 是作为浮点数表示的实现的最优总成本，$S^{\\star}$ 是沿实现的最优策略的整数分支切换次数，$\\text{is\\_greedy\\_suboptimal}$ 是一个布尔值，指示是否 $J^{\\mathrm{greedy}}  J^{\\star}$，而 $\\text{is\\_trap}$ 是上文定义的布尔局部最小值陷阱指示器。使用状态网格间距 $\\Delta x = 0.25$。\n\n- 案例1（一般情况）：$N = 6$, $q = 1.0$, $p = 5.0$, $a = 1.0$, $b = 1.0$, $x_0 = 2.5$, $\\mathcal{U} = \\{-2.0,-1.5,-1.0,-0.5,0.0,0.5,1.0,1.5,2.0\\}$。\n- 案例2（边界时域）：$N = 1$, $q = 1.0$, $p = 2.0$, $a = 1.0$, $b = 1.0$, $x_0 = -0.75$, $\\mathcal{U} = \\{-2.0,-1.0,-0.5,0.0,0.5,1.0,2.0\\}$。\n- 案例3（非对称势阱）：$N = 5$, $q = 0.5$, $p = 10.0$, $a = 2.0$, $b = 0.5$, $x_0 = -3.0$, $\\mathcal{U} = \\{-3.0,-2.5,-2.0,-1.5,-1.0,-0.5,0.0,0.5,1.0,1.5,2.0,2.5,3.0\\}$。\n- 案例4（强终端惩罚）：$N = 8$, $q = 0.1$, $p = 20.0$, $a = 1.5$, $b = 1.5$, $x_0 = 3.0$, $\\mathcal{U} = \\{-2.0,-1.5,-1.0,-0.5,0.0,0.5,1.0,1.5,2.0\\}$。\n\n最终输出格式。您的程序应生成单行输出，其中包含所有测试用例的结果。这些结果聚合为一个由方括号括起来的逗号分隔列表，其中每个元素本身是该测试用例的四元组，也由方括号括起来并以逗号分隔。例如：\n$[ [J_1^{\\star},S_1^{\\star},\\text{flag}_{1},\\text{flag}_{1}], [J_2^{\\star},S_2^{\\star},\\text{flag}_{2},\\text{flag}_{2}], \\dots ]$。\n不涉及物理单位或角度单位。所有布尔值必须打印为大写的Python布尔值（$\\text{True}$ 或 $\\text{False}$）。所有浮点数和整数必须以标准Python表示法打印。", "solution": "**1. 问题表述与最优性原理**\n\n目标是最小化由下式给出的总成本函数 $J$：\n$$J(x_0, u_0, \\dots, u_{N-1}) = \\sum_{k=0}^{N-1} c(x_k, u_k) + \\phi(x_N)$$\n其中阶段成本为 $c(x_k, u_k) = qx_k^2 + \\ell(u_k)$，且 $\\ell(u) = \\min\\{(u - a)^2, (u + b)^2\\}$，终端成本为 $\\phi(x_N) = px_N^2$。系统动态为 $x_{k+1} = x_k + u_k$，初始状态 $x_0$ 给定。控制 $u_k$ 必须从一个有限的容许控制集合 $\\mathcal{U}$ 中选择。\n\n根据 Bellman 的最优性原理，一个最优策略具有这样的性质：无论初始状态和初始决策是什么，余下的决策对于由第一个决策导致的状态而言，也必须构成一个最优策略。该原理引出了动态规划的递推关系。\n\n令 $J_k^{\\star}(x)$ 为从时间 $k$ 的状态 $x$ 到时域末端的最优未来成本。递推从终端时间 $N$ 开始：\n$$J_N^{\\star}(x) = \\phi(x) = p x^2$$\n对于之前的时间步 $k = N-1, N-2, \\dots, 0$，Bellman 方程将时间 $k$ 的未来成本与时间 $k+1$ 的未来成本关联起来：\n$$J_k^{\\star}(x) = \\min_{u \\in \\mathcal{U}} \\left\\{ c(x, u) + J_{k+1}^{\\star}(x + u) \\right\\}$$\n最小值内的项代表在时间 $k$、状态 $x$ 时选择控制 $u$ 所产生的成本，以及此后按最优策略继续执行的成本之和。\n\n**2. 通过离散化进行数值求解**\n\n由于状态 $x$ 是连续的，我们无法直接将 $J_k^{\\star}(x)$ 制成表格。问题指定了使用状态空间离散化的数值方法。\n\n首先，我们构建一个均匀的状态网格 $\\mathcal{X}$，保证其能包含所有可能的轨迹。最小和最大可达状态分别为 $x_{\\min} = x_0 + N \\cdot \\min(\\mathcal{U})$ 和 $x_{\\max} = x_0 + N \\cdot \\max(\\mathcal{U})$。网格 $\\mathcal{X}$ 在区间 $[x_{\\min}, x_{\\max}]$ 上构建，具有指定的间距 $\\Delta x$。设网格点为 $\\{x_i\\}$。\n\nDP 算法随后在此网格上按时间反向进行：\n\n**步骤1：反向过程（策略和价值函数计算）**\n- **初始化 ($k=N$)：**对于每个状态 $x_i \\in \\mathcal{X}$，计算并存储终端成本：$J_N(x_i) = p x_i^2$。\n- **反向迭代 ($k=N-1, \\dots, 0$)：**对于每个状态 $x_i \\in \\mathcal{X}$：\n  - 对于每个容许控制 $u_j \\in \\mathcal{U}$，计算该特定选择的未来成本：\n    1. 计算下一状态：$x_{\\text{next}} = x_i + u_j$。\n    2. 将此下一状态映射到网格。根据问题要求，这包括将 $x_{\\text{next}}$ 裁剪到网格边界，然后找到最近的网格点，我们称之为 $x'_{\\text{next}}$。\n    3. 从该下一网格状态检索已计算出的最优未来成本：$J_{k+1}(x'_{\\text{next}})$。\n    4. 对于这个 $(x_i, u_j)$ 对，总成本为 $V(u_j) = q x_i^2 + \\ell(u_j) + J_{k+1}(x'_{\\text{next}})$。\n  - 找到所有控制中的最小成本：$J_k(x_i) = \\min_{u_j \\in \\mathcal{U}} V(u_j)$。\n  - 识别达到该最小值的控制集合。应用指定的平局打破规则（最小绝对值，然后是最小数值）来选择唯一的最优控制 $\\mu_k^{\\star}(x_i)$。\n  - 存储最优未来成本 $J_k(x_i)$ 和最优控制 $\\mu_k^{\\star}(x_i)$。\n\n此过程结束后，我们便得到了为所有网格状态 $x_i$ 和时间步 $k$ 制表的最优策略 $\\mu_k^{\\star}(x_i)$ 和最优价值函数 $J_k(x_i)$。\n\n**步骤2：正向过程（轨迹实现）**\n在计算出最优策略 $\\mu_k^{\\star}$ 后，我们从初始状态 $x_0$ 开始正向模拟系统，以找到实现的轨迹并计算相关的成本和指标。\n\n- 用 $x_0^{\\star} = x_0$ 初始化状态轨迹。\n- 对于 $k=0, \\dots, N-1$：\n  1. 获取当前真实状态 $x_k^{\\star}$。\n  2. 将 $x_k^{\\star}$ 映射到其最近的网格点 $x'_k = \\text{nearest}(x_k^{\\star}, \\mathcal{X})$。\n  3. 从策略表中查找最优控制：$u_k^{\\star} = \\mu_k^{\\star}(x'_k)$。\n  4. 演化真实状态：$x_{k+1}^{\\star} = x_k^{\\star} + u_k^{\\star}$。\n  5. 存储 $u_k^{\\star}$ 和 $x_{k+1}^{\\star}$。\n\n**3. 所需指标的计算**\n\n- **最优成本 ($J^{\\star}$)：** 使用实现的最优状态轨迹 $(x_0^{\\star}, \\dots, x_N^{\\star})$ 和控制序列 $(u_0^{\\star}, \\dots, u_{N-1}^{\\star})$，总成本计算如下：\n  $$J^{\\star} = \\sum_{k=0}^{N-1} (q(x_k^{\\star})^2 + \\ell(u_k^{\\star})) + p(x_N^{\\star})^2$$\n\n- **贪心策略成本 ($J^{\\mathrm{greedy}}$)：** 短视贪心策略是在每一步都最小化即时控制成本 $\\ell(u)$ 的策略，而忽略状态成本和未来后果。贪心控制 $u^{\\mathrm{greedy}}$ 通过 $\\arg\\min_{u \\in \\mathcal{U}} \\ell(u)$ 找到，并按规定打破平局。由于 $\\ell(u)$ 与状态和时间无关，因此该控制对于所有 $k$ 都是恒定的。从 $x_0$ 开始模拟贪心轨迹，并以类似方式计算 $J^{\\mathrm{greedy}}$。\n\n- **策略切换 ($S^{\\star}$)：** 控制成本 $\\ell(u) = \\min\\{(u-a)^2, (u+b)^2\\}$ 有两个分支。控制 $u$ 的激活分支由 $(u-a)^2 \\le (u+b)^2$ 是否成立来决定。这可以简化为检查是否 $u \\ge (a-b)/2$。我们定义一个分支索引函数：\n  $$B(u) = \\begin{cases} 0  \\text{if } u \\ge (a-b)/2 \\\\ 1  \\text{if } u  (a-b)/2 \\end{cases}$$\n  我们将此函数应用于最优控制序列 $u_0^{\\star}, \\dots, u_{N-1}^{\\star}$ 以获得一个分支索引序列。切换次数 $S^{\\star}$ 是连续时间步之间分支索引发生变化的次数。\n\n- **陷阱指示器：** 布尔标志 `is_greedy_suboptimal` ($J^{\\mathrm{greedy}}  J^{\\star}$) 和 `is_trap`（贪心策略无切换，DP策略至少有一次切换，且贪心策略是次优的）是基于这些结果计算的。“陷阱”表示这样一种情况：简单的贪心方法陷入了一个局部吸引的控制模式，而最优的DP策略能够正确地进行前瞻规划并切换模式，以实现更低的总成本。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the algorithm for all test cases and print the results.\n    \"\"\"\n    test_cases = [\n        {'N': 6, 'q': 1.0, 'p': 5.0, 'a': 1.0, 'b': 1.0, 'x0': 2.5, 'U': [-2.0, -1.5, -1.0, -0.5, 0.0, 0.5, 1.0, 1.5, 2.0]},\n        {'N': 1, 'q': 1.0, 'p': 2.0, 'a': 1.0, 'b': 1.0, 'x0': -0.75, 'U': [-2.0, -1.0, -0.5, 0.0, 0.5, 1.0, 2.0]},\n        {'N': 5, 'q': 0.5, 'p': 10.0, 'a': 2.0, 'b': 0.5, 'x0': -3.0, 'U': [-3.0, -2.5, -2.0, -1.5, -1.0, -0.5, 0.0, 0.5, 1.0, 1.5, 2.0, 2.5, 3.0]},\n        {'N': 8, 'q': 0.1, 'p': 20.0, 'a': 1.5, 'b': 1.5, 'x0': 3.0, 'U': [-2.0, -1.5, -1.0, -0.5, 0.0, 0.5, 1.0, 1.5, 2.0]},\n    ]\n    dx = 0.25\n\n    results = []\n    for params in test_cases:\n        result = _solve_case(_dx=dx, **params)\n        results.append(result)\n    \n    # Format the final output string\n    s_results = [f\"[{r[0]},{r[1]},{'True' if r[2] else 'False'},{'True' if r[3] else 'False'}]\" for r in results]\n    print(f\"[{','.join(s_results)}]\")\n\ndef _solve_case(N, q, p, a, b, x0, U, _dx):\n    \"\"\"\n    Solves a single optimal control problem instance.\n    \"\"\"\n\n    # --- Helper Functions ---\n    def l_cost(u, a_param, b_param):\n        return min((u - a_param)**2, (u + b_param)**2)\n\n    def branch_index(u, a_param, b_param):\n        # branch 0 if (u-a)^2 = (u+b)^2, else 1. This simplifies to u >= (a-b)/2.\n        return 0 if u >= (a_param - b_param) / 2.0 else 1\n\n    def solve_tiebreak(controls):\n        # Sort by |u|, then by u\n        return sorted(controls, key=lambda u_val: (abs(u_val), u_val))[0]\n\n    # --- 1. State Grid Construction ---\n    u_min_val, u_max_val = min(U), max(U)\n    x_min_reach = x0 + N * u_min_val\n    x_max_reach = x0 + N * u_max_val\n    # Use arange; add a small tolerance to include the endpoint\n    X = np.arange(x_min_reach, x_max_reach + _dx / 2.0, _dx)\n    num_states = len(X)\n    x_grid_min_val, x_grid_max_val = X[0], X[-1]\n    \n    def map_to_index(x):\n        clipped_x = np.clip(x, x_grid_min_val, x_grid_max_val)\n        return np.argmin(np.abs(X - clipped_x))\n\n    # --- 2. Dynamic Programming Backward Pass ---\n    J = np.zeros((N + 1, num_states))\n    policy = np.zeros((N, num_states))\n\n    # Terminal cost\n    J[N, :] = p * X**2\n\n    # Iterate backwards from k = N-1 to 0\n    for k in range(N - 1, -1, -1):\n        for i in range(num_states):\n            x = X[i]\n            stage_cost_x = q * x**2\n            \n            costs_for_u = []\n            for u in U:\n                control_cost_l = l_cost(u, a, b)\n                x_next_raw = x + u\n                next_idx = map_to_index(x_next_raw)\n                cost_to_go_next = J[k + 1, next_idx]\n                total_cost_for_u = stage_cost_x + control_cost_l + cost_to_go_next\n                costs_for_u.append(total_cost_for_u)\n                \n            min_cost = min(costs_for_u)\n            J[k, i] = min_cost\n            \n            candidate_us = [u for u, cost in zip(U, costs_for_u) if np.isclose(cost, min_cost)]\n            policy[k, i] = solve_tiebreak(candidate_us)\n\n    # --- 3. Forward Pass (Optimal Trajectory) ---\n    x_traj_opt = np.zeros(N + 1)\n    u_traj_opt = np.zeros(N)\n    x_traj_opt[0] = x0\n\n    for k in range(N):\n        current_x = x_traj_opt[k]\n        current_idx = map_to_index(current_x)\n        u_star_k = policy[k, current_idx]\n        u_traj_opt[k] = u_star_k\n        x_traj_opt[k + 1] = current_x + u_star_k\n    \n    # --- 4. Calculate Optimal Cost (J_star) ---\n    J_star = 0.0\n    for k in range(N):\n        J_star += q * x_traj_opt[k]**2 + l_cost(u_traj_opt[k], a, b)\n    J_star += p * x_traj_opt[N]**2\n\n    # --- 5. Calculate Policy Switches (S_star) ---\n    branch_indices_opt = [branch_index(u, a, b) for u in u_traj_opt]\n    S_star = 0\n    if N > 0:\n        for k in range(1, N):\n            if branch_indices_opt[k] != branch_indices_opt[k-1]:\n                S_star += 1\n\n    # --- 6. Greedy Policy Simulation ---\n    l_vals = [l_cost(u, a, b) for u in U]\n    min_l = min(l_vals)\n    candidate_greedy_us = [u for u, l_val in zip(U, l_vals) if np.isclose(l_val, min_l)]\n    u_greedy = solve_tiebreak(candidate_greedy_us)\n\n    x_traj_greedy = np.zeros(N + 1)\n    x_traj_greedy[0] = x0\n    for k in range(N):\n        x_traj_greedy[k + 1] = x_traj_greedy[k] + u_greedy\n    \n    J_greedy = 0.0\n    for k in range(N):\n        J_greedy += q * x_traj_greedy[k]**2 + l_cost(u_greedy, a, b)\n    J_greedy += p * x_traj_greedy[N]**2\n        \n    # --- 7. Final Indicators ---\n    is_greedy_suboptimal = J_greedy > J_star\n    \n    # Greedy policy uses constant control, so it has 0 switches.\n    greedy_switches = 0\n    \n    # A trap occurs if greedy stays on one branch, DP switches, and DP is better.\n    is_trap = (greedy_switches == 0) and (S_star >= 1) and is_greedy_suboptimal\n\n    return [J_star, S_star, is_greedy_suboptimal, is_trap]\n\nif __name__ == '__main__':\n    solve()\n\n```", "id": "3121214"}]}