{"hands_on_practices": [{"introduction": "在我们深入研究复杂的优化算法之前，理解正则化为何如此有效至关重要。本练习将通过一个简化的岭回归（Ridge Regression）模型，引导您分析正则化中最核心的概念之一：偏差-方差权衡（Bias-Variance Tradeoff）[@problem_id:3172142]。通过从第一性原理推导均方误差（$MSE$）的表达式，您将亲眼看到正则化参数 $\\lambda$ 如何在降低模型方差的同时引入少量偏差，从而揭示正则化提高预测性能的根本原因。", "problem": "考虑带高斯噪声的线性模型：$y = X \\beta + \\varepsilon$，其中 $X \\in \\mathbb{R}^{5 \\times 5}$ 是一个正交矩阵，使得 $X^{\\top} X = I_5$ 且 $X X^{\\top} = I_5$，噪声向量 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2} I_5)$，其中 $\\sigma^{2} = 1$，真实参数向量 $\\beta \\in \\mathbb{R}^{5}$ 满足 $\\|\\beta\\|_{2}^{2} = 100$。考虑岭正则化估计量 $\\hat{\\beta}_{\\lambda}$，其定义为目标函数 $\\|y - X b\\|_{2}^{2} + \\lambda \\|b\\|_{2}^{2}$ 在 $b \\in \\mathbb{R}^{5}$ 上的最小化子，其中 $\\lambda \\ge 0$。设性能标准为期望参数均方误差（mean squared error (MSE)），定义为 $\\mathbb{E}\\left[\\|\\hat{\\beta}_{\\lambda} - \\beta\\|_{2}^{2}\\right]$。\n\n仅从岭估计量的定义、正交矩阵的性质以及多维正态随机向量的期望和方差的基本性质出发，推导 $\\mathbb{E}\\left[\\|\\hat{\\beta}_{\\lambda} - \\beta\\|_{2}^{2}\\right]$ 作为 $\\lambda$ 的函数的显式表达式，然后确定使该期望均方误差等于非正则化估计量（普通最小二乘法 (OLS)，对应于 $\\lambda = 0$）的期望均方误差的最小正 $\\lambda$ 值。这个值量化了在这种 $\\|\\beta\\|_{2}$ 较大的情况下，由岭回归引入的偏差何时开始超过其方差的减小量。\n\n将最小的正 $\\lambda$ 作为你的最终答案。将答案四舍五入到四位有效数字。", "solution": "我们从岭估计量的定义开始，即它是严格凸函数 $b \\mapsto \\|y - X b\\|_{2}^{2} + \\lambda \\|b\\|_{2}^{2}$ 的最小化子。一阶最优性条件给出了闭式解\n$$\n\\hat{\\beta}_{\\lambda} = (X^{\\top} X + \\lambda I)^{-1} X^{\\top} y.\n$$\n代入线性模型 $y = X \\beta + \\varepsilon$ 可得\n$$\n\\hat{\\beta}_{\\lambda} = (X^{\\top} X + \\lambda I)^{-1} X^{\\top} (X \\beta + \\varepsilon) = (X^{\\top} X + \\lambda I)^{-1} (X^{\\top} X \\beta + X^{\\top} \\varepsilon).\n$$\n使用 $X^{\\top} X = I_5$，我们得到\n$$\n\\hat{\\beta}_{\\lambda} = (I_5 + \\lambda I_5)^{-1} (\\beta + X^{\\top} \\varepsilon) = \\frac{1}{1+\\lambda} \\beta + \\frac{1}{1+\\lambda} X^{\\top} \\varepsilon.\n$$\n因此，估计误差分解为\n$$\n\\hat{\\beta}_{\\lambda} - \\beta = \\left(\\frac{1}{1+\\lambda} - 1\\right) \\beta + \\frac{1}{1+\\lambda} X^{\\top} \\varepsilon = -\\frac{\\lambda}{1+\\lambda}\\,\\beta + \\frac{1}{1+\\lambda} X^{\\top} \\varepsilon.\n$$\n我们计算 $\\hat{\\beta}_{\\lambda} - \\beta$ 的均值和协方差。因为 $\\mathbb{E}[\\varepsilon] = 0$ 且 $X$ 是确定性的，\n$$\n\\mathbb{E}[\\hat{\\beta}_{\\lambda} - \\beta] = -\\frac{\\lambda}{1+\\lambda}\\,\\beta,\n$$\n所以偏差向量为 $-\\frac{\\lambda}{1+\\lambda}\\,\\beta$。$\\hat{\\beta}_{\\lambda}$ 的协方差是\n$$\n\\operatorname{Var}(\\hat{\\beta}_{\\lambda}) = \\operatorname{Var}\\!\\left(\\frac{1}{1+\\lambda} X^{\\top} \\varepsilon\\right) = \\frac{1}{(1+\\lambda)^{2}} \\operatorname{Var}(X^{\\top} \\varepsilon).\n$$\n因为 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2} I_5)$ 且 $X$ 是正交的，所以 $X^{\\top} \\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2} X^{\\top} X) = \\mathcal{N}(0, \\sigma^{2} I_5)$。因此，\n$$\n\\operatorname{Var}(\\hat{\\beta}_{\\lambda}) = \\frac{\\sigma^{2}}{(1+\\lambda)^{2}} I_5.\n$$\n根据均方误差的偏差-方差分解，\n$$\n\\mathbb{E}\\left[\\|\\hat{\\beta}_{\\lambda} - \\beta\\|_{2}^{2}\\right] = \\|\\mathbb{E}[\\hat{\\beta}_{\\lambda}] - \\beta\\|_{2}^{2} + \\operatorname{tr}\\left(\\operatorname{Var}(\\hat{\\beta}_{\\lambda})\\right).\n$$\n使用上述表达式，\n$$\n\\|\\mathbb{E}[\\hat{\\beta}_{\\lambda}] - \\beta\\|_{2}^{2} = \\left\\|-\\frac{\\lambda}{1+\\lambda}\\,\\beta\\right\\|_{2}^{2} = \\frac{\\lambda^{2}}{(1+\\lambda)^{2}} \\|\\beta\\|_{2}^{2},\n$$\n且\n$$\n\\operatorname{tr}\\left(\\operatorname{Var}(\\hat{\\beta}_{\\lambda})\\right) = \\operatorname{tr}\\left(\\frac{\\sigma^{2}}{(1+\\lambda)^{2}} I_5\\right) = \\frac{5 \\sigma^{2}}{(1+\\lambda)^{2}}.\n$$\n因此，作为 $\\lambda$ 的函数的期望参数均方误差是\n$$\n\\mathbb{E}\\left[\\|\\hat{\\beta}_{\\lambda} - \\beta\\|_{2}^{2}\\right] = \\frac{\\lambda^{2} \\|\\beta\\|_{2}^{2} + 5 \\sigma^{2}}{(1+\\lambda)^{2}}.\n$$\n对于非正则化估计量（普通最小二乘法 (OLS)），$\\lambda = 0$，所以\n$$\n\\mathbb{E}\\left[\\|\\hat{\\beta}_{0} - \\beta\\|_{2}^{2}\\right] = 5 \\sigma^{2}.\n$$\n我们需要求得满足以下条件的最小正 $\\lambda$\n$$\n\\frac{\\lambda^{2} \\|\\beta\\|_{2}^{2} + 5 \\sigma^{2}}{(1+\\lambda)^{2}} = 5 \\sigma^{2}.\n$$\n两边去分母，\n$$\n\\lambda^{2} \\|\\beta\\|_{2}^{2} + 5 \\sigma^{2} = 5 \\sigma^{2} (1 + 2 \\lambda + \\lambda^{2}) = 5 \\sigma^{2} + 10 \\sigma^{2} \\lambda + 5 \\sigma^{2} \\lambda^{2}.\n$$\n消去两边的 $5 \\sigma^{2}$ 并重新整理，\n$$\n\\lambda^{2} \\|\\beta\\|_{2}^{2} = 10 \\sigma^{2} \\lambda + 5 \\sigma^{2} \\lambda^{2},\n$$\n$$\n\\lambda^{2} \\left(\\|\\beta\\|_{2}^{2} - 5 \\sigma^{2}\\right) = 10 \\sigma^{2} \\lambda.\n$$\n假设 $\\|\\beta\\|_{2}^{2} > 5 \\sigma^{2}$（给定数据满足此条件），两边除以 $\\lambda > 0$ 可得\n$$\n\\lambda \\left(\\|\\beta\\|_{2}^{2} - 5 \\sigma^{2}\\right) = 10 \\sigma^{2},\n$$\n所以最小的正数解是\n$$\n\\lambda^{\\star} = \\frac{10 \\sigma^{2}}{\\|\\beta\\|_{2}^{2} - 5 \\sigma^{2}}.\n$$\n当 $\\sigma^{2} = 1$ 且 $\\|\\beta\\|_{2}^{2} = 100$ 时，上式变为\n$$\n\\lambda^{\\star} = \\frac{10 \\cdot 1}{100 - 5 \\cdot 1} = \\frac{10}{95}.\n$$\n数值上，$\\frac{10}{95} \\approx 0.105263\\dots$。四舍五入到四位有效数字得到 $0.1053$。\n\n这个值标志着岭回归的期望均方误差回到非正则化水平的点；对于任何更大的 $\\lambda$，期望均方误差将超过 OLS 的期望均方误差，这表明当信号 $\\|\\beta\\|_{2}$ 很大时，由岭回归引入的偏差可能超过其方差的减小量。", "answer": "$$\\boxed{0.1053}$$", "id": "3172142"}, {"introduction": "在理解了正则化的基本原理后，我们将注意力转向能够诱导稀疏性的LASSO（Least Absolute Shrinkage and Selection Operator）及其求解算法。本练习将介绍坐标下降法（Coordinate Descent），这是一种非常直观且高效的优化策略[@problem_id:3172091]。您将通过推导其单坐标更新规则，发现著名的软阈值算子（soft-thresholding operator）是如何自然出现的，并通过一个具体的数值算例，加深对LASSO如何逐个“筛选”变量以实现稀疏解的理解。", "problem": "考虑最小绝对收缩和选择算子 (LASSO) 问题，该问题最小化目标函数\n$$\nF(w) \\;=\\; \\frac{1}{2}\\,\\|y - X w\\|_{2}^{2} \\;+\\; \\lambda\\,\\|w\\|_{1},\n$$\n其中 $X \\in \\mathbb{R}^{n \\times p}$ 是一个固定的数据矩阵，$y \\in \\mathbb{R}^{n}$ 是一个固定的响应向量，$w \\in \\mathbb{R}^{p}$ 是决策变量，$\\lambda > 0$ 是一个正则化参数。令 $X_{j} \\in \\mathbb{R}^{n}$ 表示 $X$ 的第 $j$ 列，$w_{j}$ 表示 $w$ 的第 $j$ 个分量。定义软阈值算子 $S_{\\alpha}(z)$ 为\n$$\nS_{\\alpha}(z) \\;=\\; \\operatorname{sign}(z)\\,\\max\\big(|z| - \\alpha,\\,0\\big),\n$$\n对于 $\\alpha \\ge 0$ 和 $z \\in \\mathbb{R}$。\n\n从 $F(w)$ 的定义和绝对值函数的次梯度出发，推导 $w_{j}$ 的精确坐标级最小化器，同时保持所有其他坐标固定。您的推导必须从分离出坐标 $j$ 的贡献得到的一维简化问题开始，并且只使用基本原理（凸性以及 $\\ell_{1}$ 范数的次梯度最优性）。然后，使用您推导出的更新公式，为以下实例计算一个具体的坐标下降更新：\n$$\nX \\;=\\; \\begin{pmatrix}\n2  -1  0 \\\\\n0  1  3\n\\end{pmatrix},\\qquad\ny \\;=\\; \\begin{pmatrix} 1 \\\\ 4 \\end{pmatrix},\\qquad\nw \\;=\\; \\begin{pmatrix} 0.5 \\\\ -0.2 \\\\ 0.1 \\end{pmatrix},\\qquad\n\\lambda \\;=\\; 1.2,\n$$\n针对坐标 $j = 2$。提供经过一次精确坐标级最小化步骤后 $w_{2}$ 的更新数值。\n\n最后，根据 $F(w)$ 的结构和坐标下降的性质，简要解释为什么循环坐标下降对此凸问题具有全局收敛性，以及为什么在实践中 $X$ 的稀疏性可以加速收敛。您的解释应基于基本性质（凸性、$\\ell_{1}$ 惩罚项的可分性以及光滑部分的坐标级 Lipschitz 连续性），不要引用未经证明的论断或捷径公式。\n\n将更新后的 $w_{2}$ 的最终数值答案表示为单个实数。如果需要四舍五入，请保留四位有效数字；否则，提供精确值。", "solution": "所述问题是有效的。它在凸优化理论中有科学依据，是良定的，对于指定的任务有唯一解，并且所有定义和数据都是完整和一致的。因此，我们可以着手求解。\n\n该问题需要三部分回答：首先，推导 LASSO 目标函数的坐标级最小化器；其次，对该结果进行数值应用；第三，解释坐标下降法对此问题的收敛性质。\n\n**第 1 部分：坐标级最小化器的推导**\n\nLASSO 目标函数由下式给出\n$$\nF(w) = \\frac{1}{2}\\|y - X w\\|_{2}^{2} + \\lambda\\|w\\|_{1}\n$$\n其中 $w \\in \\mathbb{R}^{p}$。我们希望找到 $F(w)$ 关于单个坐标 $w_j$ 的最小化器，同时保持所有其他坐标 $w_k$（其中 $k \\neq j$）固定。\n\n我们可以将函数 $F(w)$ 分解为依赖于 $w_j$ 的项和相对于 $w_j$ 为常数的项。光滑项是 $\\|y - X w\\|_{2}^{2}$。我们可以如下分离出 $X$ 的第 $j$ 列的贡献：\n$$\nXw = \\sum_{k=1}^{p} X_k w_k = X_j w_j + \\sum_{k \\neq j} X_k w_k\n$$\n我们定义部分残差向量 $r_{(-j)} \\in \\mathbb{R}^{n}$ 为\n$$\nr_{(-j)} = y - \\sum_{k \\neq j} X_k w_k\n$$\n将此代入目标函数，我们得到\n$$\nF(w) = \\frac{1}{2}\\|r_{(-j)} - X_j w_j\\|_{2}^{2} + \\lambda \\sum_{k=1}^{p} |w_k| = \\frac{1}{2}\\|r_{(-j)} - X_j w_j\\|_{2}^{2} + \\lambda |w_j| + \\lambda \\sum_{k \\neq j} |w_k|\n$$\n为了关于 $w_j$ 最小化 $F(w)$，我们可以忽略所有不依赖于 $w_j$ 的项。这给了我们一个一维优化问题：\n$$\n\\min_{w_j \\in \\mathbb{R}} f_j(w_j) \\quad \\text{其中} \\quad f_j(w_j) = \\frac{1}{2}\\|r_{(-j)} - X_j w_j\\|_{2}^{2} + \\lambda|w_j|\n$$\n让我们展开平方 $\\ell_2$-范数：\n$$\n\\|r_{(-j)} - X_j w_j\\|_{2}^{2} = (r_{(-j)} - X_j w_j)^T(r_{(-j)} - X_j w_j) = \\|r_{(-j)}\\|_2^2 - 2 w_j X_j^T r_{(-j)} + w_j^2 \\|X_j\\|_2^2\n$$\n函数 $f_j(w_j)$ 在相差一个加性常数的情况下可以写成\n$$\nf_j(w_j) = \\frac{1}{2}\\|X_j\\|_2^2 w_j^2 - (X_j^T r_{(-j)}) w_j + \\lambda|w_j|\n$$\n这个函数是凸函数，因为它是一个二次函数（假设 $X_j \\neq 0$ 时是凸的）和一个乘以 $\\lambda > 0$ 的绝对值函数（也是凸的）的和。一个点 $w_j^*$ 是 $f_j(w_j)$ 的全局最小化点，当且仅当 $0$ 属于 $f_j$ 在 $w_j^*$ 处的次微分，记为 $\\partial f_j(w_j^*)$。\n\n$f_j(w_j)$ 的次微分由下式给出：\n$$\n\\partial f_j(w_j) = \\|X_j\\|_2^2 w_j - X_j^T r_{(-j)} + \\lambda \\partial|w_j|\n$$\n其中绝对值函数的次微分是\n$$\n\\partial|z| = \\begin{cases} \\{\\operatorname{sign}(z)\\}  \\text{if } z \\neq 0 \\\\ [-1, 1]  \\text{if } z = 0 \\end{cases}\n$$\n因此，最优性条件 $0 \\in \\partial f_j(w_j^*)$ 变为：\n$$\nX_j^T r_{(-j)} - \\|X_j\\|_2^2 w_j^* \\in \\lambda \\partial|w_j^*|\n$$\n我们根据 $w_j^*$ 的值分情况分析此条件：\n1.  如果 $w_j^* > 0$，那么 $\\partial|w_j^*| = \\{1\\}$。该条件变为 $X_j^T r_{(-j)} - \\|X_j\\|_2^2 w_j^* = \\lambda$，这给出 $w_j^* = \\frac{X_j^T r_{(-j)} - \\lambda}{\\|X_j\\|_2^2}$。为了使该解与初始假设 $w_j^* > 0$ 一致，我们必须有 $X_j^T r_{(-j)} > \\lambda$。\n2.  如果 $w_j^*  0$，那么 $\\partial|w_j^*| = \\{-1\\}$。该条件变为 $X_j^T r_{(-j)} - \\|X_j\\|_2^2 w_j^* = -\\lambda$，这给出 $w_j^* = \\frac{X_j^T r_{(-j)} + \\lambda}{\\|X_j\\|_2^2}$。为了与 $w_j^*  0$ 保持一致，我们必须有 $X_j^T r_{(-j)}  -\\lambda$。\n3.  如果 $w_j^* = 0$，那么 $\\partial|w_j^*| = [-1, 1]$。该条件变为 $X_j^T r_{(-j)} \\in [-\\lambda, \\lambda]$，或者 $|X_j^T r_{(-j)}| \\le \\lambda$。\n\n让我们整合这些结果。令 $z = X_j^T r_{(-j)}$ 和 $\\alpha = \\|X_j\\|_2^2$。最优的 $w_j^*$ 是：\n$$\nw_j^* = \\begin{cases} (z - \\lambda)/\\alpha  \\text{if } z > \\lambda \\\\ (z + \\lambda)/\\alpha  \\text{if } z  -\\lambda \\\\ 0  \\text{if } |z| \\le \\lambda \\end{cases}\n$$\n这个表达式正是软阈值算子 $S_{\\lambda/\\alpha}(z/\\alpha)$。让我们来验证一下：\n$$\nS_{\\lambda/\\alpha}\\left(\\frac{z}{\\alpha}\\right) = \\operatorname{sign}\\left(\\frac{z}{\\alpha}\\right) \\max\\left(\\left|\\frac{z}{\\alpha}\\right| - \\frac{\\lambda}{\\alpha}, 0\\right) = \\frac{1}{\\alpha} \\operatorname{sign}(z) \\max\\left(|z| - \\lambda, 0\\right)\n$$\n- 如果 $z > \\lambda$，表达式给出 $\\frac{1}{\\alpha}(1)(z - \\lambda) = (z-\\lambda)/\\alpha$。\n- 如果 $z  -\\lambda$，表达式给出 $\\frac{1}{\\alpha}(-1)(-z - \\lambda) = (z+\\lambda)/\\alpha$。\n- 如果 $|z| \\le \\lambda$，表达式给出 $\\frac{1}{\\alpha}\\operatorname{sign}(z)(0) = 0$。\n推导完成。$w_j$ 的精确坐标级最小化器由 $w_j^* = S_{\\lambda/\\|X_j\\|_2^2}\\left(\\frac{X_j^T r_{(-j)}}{\\|X_j\\|_2^2}\\right)$ 给出。\n\n**第 2 部分：数值计算**\n\n给定以下实例：\n$$\nX = \\begin{pmatrix} 2  -1  0 \\\\ 0  1  3 \\end{pmatrix}, \\quad y = \\begin{pmatrix} 1 \\\\ 4 \\end{pmatrix}, \\quad w_{\\text{initial}} = \\begin{pmatrix} 0.5 \\\\ -0.2 \\\\ 0.1 \\end{pmatrix}, \\quad \\lambda = 1.2\n$$\n我们需要更新坐标 $j=2$，它对应于 $w_2$。其他坐标固定在它们的当前值：$w_1 = 0.5$ 和 $w_3 = 0.1$。\n\n首先，我们确定 $X$ 的相关列：$X_2 = \\begin{pmatrix} -1 \\\\ 1 \\end{pmatrix}$。\n该列的平方范数为 $\\|X_2\\|_2^2 = (-1)^2 + 1^2 = 2$。\n\n接下来，我们计算部分残差 $r_{(-2)}$：\n$$\nr_{(-2)} = y - X_1 w_1 - X_3 w_3 = \\begin{pmatrix} 1 \\\\ 4 \\end{pmatrix} - \\begin{pmatrix} 2 \\\\ 0 \\end{pmatrix}(0.5) - \\begin{pmatrix} 0 \\\\ 3 \\end{pmatrix}(0.1)\n$$\n$$\nr_{(-2)} = \\begin{pmatrix} 1 \\\\ 4 \\end{pmatrix} - \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} 0 \\\\ 0.3 \\end{pmatrix} = \\begin{pmatrix} 1 - 1 - 0 \\\\ 4 - 0 - 0.3 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 3.7 \\end{pmatrix}\n$$\n现在，我们计算项 $X_2^T r_{(-2)}$：\n$$\nX_2^T r_{(-2)} = \\begin{pmatrix} -1  1 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 3.7 \\end{pmatrix} = (-1)(0) + (1)(3.7) = 3.7\n$$\n现在我们可以应用软阈值算子。更新后的值 $w_2^{\\text{new}}$ 为\n$$\nw_2^{\\text{new}} = S_{\\lambda/\\|X_2\\|_2^2}\\left(\\frac{X_2^T r_{(-2)}}{\\|X_2\\|_2^2}\\right)\n$$\n阈值参数为 $\\lambda/\\|X_2\\|_2^2 = 1.2/2 = 0.6$。算子的自变量为 $X_2^T r_{(-2)}/\\|X_2\\|_2^2 = 3.7/2 = 1.85$。\n因此，我们计算：\n$$\nw_2^{\\text{new}} = S_{0.6}(1.85)\n$$\n使用定义 $S_{\\alpha}(z) = \\operatorname{sign}(z) \\max(|z|-\\alpha, 0)$：\n$$\nw_2^{\\text{new}} = \\operatorname{sign}(1.85) \\max(|1.85| - 0.6, 0) = 1 \\cdot \\max(1.85 - 0.6, 0) = \\max(1.25, 0) = 1.25\n$$\n$w_2$ 的更新值恰好是 $1.25$。\n\n**第 3 部分：收敛性解释**\n\n循环坐标下降法对于 LASSO 问题的全局收敛性取决于目标函数 $F(w)$ 的特定结构。\n函数 $F(w) = \\frac{1}{2}\\|y - Xw\\|_2^2 + \\lambda \\|w\\|_1$ 是两个分量的和：\n1.  一个光滑、可微的凸函数 $f(w) = \\frac{1}{2}\\|y - Xw\\|_2^2$。其梯度为 $\\nabla f(w) = X^T(Xw - y)$。关于单个坐标的梯度 $\\nabla_j f(w) = X_j^T(Xw - y)$ 是 Lipschitz 连续的，其 Lipschitz 常数为 $L_j = \\|X_j\\|_2^2$。\n2.  一个非光滑的凸函数 $g(w) = \\lambda \\|w\\|_1$。关键在于，这个函数是**可分的**，意味着它可以写成关于各个坐标的函数之和：$g(w) = \\sum_{j=1}^p \\lambda|w_j|$。\n\n这些性质的组合——一个由具有坐标级 Lipschitz 梯度的光滑部分和一个可分的非光滑部分组成的凸目标函数——足以保证循环坐标下降法收敛到全局最小值。在每一步中，该算法都对一个坐标执行精确最小化。因为函数是凸的，任何局部最小值都是全局最小值。由坐标下降法生成的迭代序列 $\\{w^{(k)}\\}$ 保证有极限点，并且任何这样的极限点都是 $F(w)$ 的全局最小化点。这是非光滑凸优化理论中的一个经典结果。\n\n数据矩阵 $X$ 的稀疏性可以显著地从计算时间的角度加速收敛。对 $w_j$ 的每次坐标更新中的核心计算是项 $X_j^T r_{(-j)}$。这可以计算为 $X_j^T y - \\sum_{k \\neq j} (X_j^T X_k) w_k$。如果 $X$ 是稀疏的，它的许多列 $X_k$ 将是稀疏向量。内积 $X_j^T X_k$ 的计算可以快得多，因为所需乘法的次数与两个向量都具有非零元素的位置数成正比，而不是它们的完整维度 $n$。在许多情况下，这个内积将恰好为零。这降低了求和的成本。另外，如果维护完整的残差 $r=y-Xw$，那么更新 $w_j \\to w_j^{\\text{new}}$ 需要对残差进行更新 $r \\to r - X_j (w_j^{\\text{new}} - w_j^{\\text{old}})$。如果向量 $X_j$ 是稀疏的（只有少量非零项），这个更新操作的计算成本就很低。因此，坐标下降的每一步完成得更快，从而导致更快的整体收敛时间。", "answer": "$$\n\\boxed{1.25}\n$$", "id": "3172091"}, {"introduction": "作为本章的收尾，我们将探索一个更高级且通用的优化框架：交替方向乘子法（Alternating Direction Method of Multipliers, ADMM）。ADMM通过“分裂”变量将原问题分解为一系列更简单的子问题，是解决大规模机器学习问题的强大技术[@problem_id:3172064]。在本练习中，您将为LASSO问题推导ADMM的完整迭代步骤并编程实现，从而获得解决复杂优化问题的宝贵实践经验，并体会到现代优化算法的精妙之处。", "problem": "要求您使用分裂变量 $z$ 和等式约束 $w=z$，推导并实现用于最小绝对收缩和选择算子 (Lasso) 的交替方向乘子法 (ADMM)。从约束公式以及凸优化和增广拉格朗日方法的基本定义出发。仅基于第一性原理，即等式约束凸问题的增广拉格朗日定义、可微子问题的平稳性条件以及 $\\ell_{1}$-范数的邻近算子定义，推导出变量 $w$、$z$ 和缩放对偶变量 $u$ 的显式更新公式。然后，实现您推导的算法，并在提供的测试集上进行评估。\n\n该优化问题是最小化目标函数\n$$\n\\frac{1}{2}\\lVert y - X w \\rVert_{2}^{2} + \\lambda \\lVert z \\rVert_{1}\n$$\n约束条件为\n$$\nw = z,\n$$\n其中 $X \\in \\mathbb{R}^{n \\times p}$ 是一个数据矩阵，$y \\in \\mathbb{R}^{n}$ 是一个响应向量，$w \\in \\mathbb{R}^{p}$ 是系数向量，$\\lambda \\in \\mathbb{R}_{+}$ 是正则化参数，$\\lVert \\cdot \\rVert_{1}$ 和 $\\lVert \\cdot \\rVert_{2}$ 分别表示 $\\ell_{1}$-范数和欧几里得范数。\n\n要求：\n- 从增广拉格朗日量出发，推导 $w$、$z$ 和缩放对偶变量 $u$ 的 ADMM 更新公式。您的推导必须仅使用等式约束的增广拉格朗日量的核心定义以及凸函数和邻近算子的性质。不要使用预先记下的快捷公式。\n- 实现该算法，并使用基于原始和对偶残差范数的停止准则，该准则与绝对和相对容差进行比较。\n\n数值实现细节：\n- 使用带有惩罚参数 $\\rho \\in \\mathbb{R}_{+}$ 的 ADMM 缩放形式。\n- 对原始残差 $r^{k} = w^{k} - z^{k}$ 和对偶残差 $s^{k} = \\rho (z^{k} - z^{k-1})$ 使用停止准则：\n  - 当 $\\lVert r^{k} \\rVert_{2} \\le \\varepsilon_{\\mathrm{pri}}$ 且 $\\lVert s^{k} \\rVert_{2} \\le \\varepsilon_{\\mathrm{dual}}$ 时停止。\n  - 使用 $\\varepsilon_{\\mathrm{pri}} = \\sqrt{p}\\,\\varepsilon_{\\mathrm{abs}} + \\varepsilon_{\\mathrm{rel}} \\max\\{\\lVert w^{k} \\rVert_{2}, \\lVert z^{k} \\rVert_{2}\\}$ 和 $\\varepsilon_{\\mathrm{dual}} = \\sqrt{p}\\,\\varepsilon_{\\mathrm{abs}} + \\varepsilon_{\\mathrm{rel}} \\rho \\lVert u^{k} \\rVert_{2}$，其中 $p$ 是 $X$ 的列数。\n- 使用 $\\varepsilon_{\\mathrm{abs}} = 10^{-6}$、$\\varepsilon_{\\mathrm{rel}} = 10^{-4}$，以及最大迭代次数 $K_{\\max} = 2000$，除非提前收敛。\n- 初始化 $w^{0} = 0$, $z^{0} = 0$, $u^{0} = 0$。\n\n测试集：\n在以下四种情况下运行您的实现。对于每种情况，计算收敛解 $w$ 处的最终目标值\n$$\nF(w) = \\frac{1}{2}\\lVert y - X w \\rVert_{2}^{2} + \\lambda \\lVert w \\rVert_{1}\n$$\n并将其四舍五入到 $6$ 位小数。\n\n- 情况 A (正交设计，检查软阈值行为)：\n  - $X = I_{3}$，$3 \\times 3$ 单位矩阵。\n  - $y = [3.0, -1.0, 0.2]^{\\top}$。\n  - $\\lambda = 1.0$。\n  - $\\rho = 1.0$。\n- 情况 B (非常大的正则化，将 $w$ 推向零的边界情况)：\n  - $X = \\begin{bmatrix} 1  0  2 \\\\ 0  1  -1 \\\\ 1  1  1 \\\\ 2  -1  0 \\end{bmatrix}$。\n  - $y = [1.0, -2.0, 0.5, 3.0]^{\\top}$。\n  - $\\lambda = 100.0$。\n  - $\\rho = 1.0$。\n- 情况 C (秩亏设计，通过 $\\rho I$ 项测试数值稳定性)：\n  - $X = \\begin{bmatrix} 1  2 \\\\ 2  4 \\\\ 3  6 \\end{bmatrix}$。\n  - $y = [1.0, 0.0, 1.0]^{\\top}$。\n  - $\\lambda = 0.1$。\n  - $\\rho = 1.0$。\n- 情况 D (微小正则化，接近最小二乘)：\n  - $X = \\begin{bmatrix} 1  0 \\\\ 0  1 \\\\ 1  1 \\\\ 2  1 \\end{bmatrix}$。\n  - $y = [1.0, 2.0, 2.5, 4.0]^{\\top}$。\n  - $\\lambda = 10^{-6}$。\n  - $\\rho = 1.0$。\n\n最终输出格式：\n您的程序应生成单行输出，其中包含按 A、B、C、D 顺序排列的 4 个四舍五入后的目标值，形式为逗号分隔的列表，并用方括号括起来（例如 $[v_{A},v_{B},v_{C},v_{D}]$）。不应打印任何额外文本。不涉及物理单位或角度单位，所有数值答案均为无单位实数。唯一可接受的输出是四舍五入到 $6$ 位小数的实数。", "solution": "用户提供的问题是推导并实现用于Lasso优化问题的交替方向乘子法 (ADMM) 的一个有效请求。所有定义、约束和数据都具有科学依据、内部一致且是适定的。该任务完全属于凸优化领域。\n\n使用分裂变量 $z$，Lasso 优化问题可以公式化为：\n$$\n\\text{minimize} \\quad f(w) + g(z) \\quad \\text{subject to} \\quad w - z = 0\n$$\n其中 $f(w) = \\frac{1}{2}\\lVert y - X w \\rVert_{2}^{2}$ 且 $g(z) = \\lambda \\lVert z \\rVert_{1}$。这是一个等式约束的凸优化问题。\n\nADMM 算法通过最小化增广拉格朗日量来解决这个问题，该拉格朗日量包含了目标函数和等式约束。对于一个形如 $\\min_{w,z} f(w) + g(z)$ 且满足 $Aw + Bz = c$ 的一般问题，其增广拉格朗日量为：\n$$\nL_{\\rho}(w, z, \\nu) = f(w) + g(z) + \\nu^\\top(Aw + Bz - c) + \\frac{\\rho}{2}\\lVert Aw + Bz - c \\rVert_{2}^{2}\n$$\n其中 $\\nu$ 是对偶变量，$\\rho  0$ 是惩罚参数。\n\n对于我们的特定问题，$A = I$，$B = -I$（其中 $I$ 是适当维度的单位矩阵），且 $c = 0$。将这些代入一般形式，得到：\n$$\nL_{\\rho}(w, z, \\nu) = \\frac{1}{2}\\lVert y - Xw \\rVert_{2}^{2} + \\lambda \\lVert z \\rVert_{1} + \\nu^\\top(w - z) + \\frac{\\rho}{2}\\lVert w - z \\rVert_{2}^{2}\n$$\n\nADMM 算法以一种缩放形式运行，其中对偶变量被 $\\rho$ 缩放。令 $u = (1/\\rho)\\nu$，因此 $\\nu = \\rho u$。项 $\\nu^\\top(w-z) + \\frac{\\rho}{2}\\lVert w-z \\rVert_2^2$ 可以通过配方法重写：\n$$\n\\rho u^\\top(w-z) + \\frac{\\rho}{2}\\lVert w-z \\rVert_2^2 = \\frac{\\rho}{2} (2u^\\top(w-z) + \\lVert w-z \\rVert_2^2) = \\frac{\\rho}{2}(\\lVert w - z + u \\rVert_{2}^{2} - \\lVert u \\rVert_{2}^{2})\n$$\n忽略相对于 $w$ 和 $z$ 的常数项，缩放的增广拉格朗日量为：\n$$\nL_{\\rho}(w, z, u) = \\frac{1}{2}\\lVert y - Xw \\rVert_{2}^{2} + \\lambda \\lVert z \\rVert_{1} + \\frac{\\rho}{2}\\lVert w - z + u \\rVert_{2}^{2}\n$$\n\nADMM 是一种迭代方法，在每次迭代 $k+1$ 中，它按顺序执行三个更新步骤：\n1. 相对于 $w$ 最小化 $L_{\\rho}$，保持 $z$ 和 $u$ 在第 $k$ 次迭代的值不变。\n2. 相对于 $z$ 最小化 $L_{\\rho}$，使用新计算的 $w^{k+1}$ 和固定的 $u^k$。\n3. 更新对偶变量 $u$。\n\n每个更新步骤的显式推导如下：\n\n**1. $w$-更新**\n\n$w$ 的更新是通过求解以下无约束最小化问题找到的：\n$$\nw^{k+1} = \\arg\\min_{w} \\left( \\frac{1}{2}\\lVert y - Xw \\rVert_{2}^{2} + \\frac{\\rho}{2}\\lVert w - z^{k} + u^{k} \\rVert_{2}^{2} \\right)\n$$\n目标函数是关于 $w$ 的二次可微函数。通过将其关于 $w$ 的梯度设为零来找到最小值。令目标函数为 $J(w)$。\n$$\n\\nabla_{w} J(w) = \\nabla_{w} \\left( \\frac{1}{2}(y - Xw)^\\top(y - Xw) + \\frac{\\rho}{2}(w - (z^k-u^k))^\\top(w - (z^k-u^k)) \\right)\n$$\n$$\n\\nabla_{w} J(w) = -X^\\top(y - Xw) + \\rho(w - (z^k - u^k))\n$$\n令 $\\nabla_{w} J(w) = 0$：\n$$\n-X^\\top y + X^\\top X w + \\rho w - \\rho(z^k - u^k) = 0\n$$\n$$\n(X^\\top X + \\rho I) w = X^\\top y + \\rho(z^k - u^k)\n$$\n矩阵 $(X^\\top X + \\rho I)$ 是正定的，因此对于任何 $\\rho  0$ 都是可逆的，因为 $X^\\top X$ 是半正定的。$w$ 的更新是这个线性方程组的解：\n$$\nw^{k+1} = (X^\\top X + \\rho I)^{-1} (X^\\top y + \\rho(z^k - u^k))\n$$\n\n**2. $z$-更新**\n\n$z$ 的更新是通过求解以下问题找到的：\n$$\nz^{k+1} = \\arg\\min_{z} \\left( \\lambda \\lVert z \\rVert_{1} + \\frac{\\rho}{2}\\lVert w^{k+1} - z + u^{k} \\rVert_{2}^{2} \\right)\n$$\n我们可以将二次项重写为 $\\frac{\\rho}{2}\\lVert z - (w^{k+1} + u^{k}) \\rVert_{2}^{2}$。问题变为：\n$$\nz^{k+1} = \\arg\\min_{z} \\left( \\lambda \\lVert z \\rVert_{1} + \\frac{\\rho}{2}\\lVert z - (w^{k+1} + u^{k}) \\rVert_{2}^{2} \\right)\n$$\n这是缩放的 $\\ell_1$-范数函数 $h(z) = \\lambda \\lVert z \\rVert_{1}$ 在点 $v = w^{k+1} + u^{k}$ 处的邻近算子的定义，缩放参数为 $1/\\rho$：\n$$\nz^{k+1} = \\text{prox}_{\\lambda\\lVert \\cdot \\rVert_1, 1/\\rho}(w^{k+1} + u^k)\n$$\n这个问题的解由软阈值算子 $S_{\\kappa}(a)$ 给出，其逐元素定义为 $S_{\\kappa}(a_i) = \\text{sign}(a_i)\\max(|a_i|-\\kappa, 0)$。在我们的情况下，阈值为 $\\kappa = \\lambda/\\rho$。更新公式为：\n$$\nz^{k+1} = S_{\\lambda/\\rho}(w^{k+1} + u^k)\n$$\n\n**3. $u$-更新**\n\n缩放对偶变量的更新公式为：\n$$\nu^{k+1} = u^k + r^{k+1}\n$$\n其中 $r^{k+1} = w^{k+1} - z^{k+1}$ 是第 $k+1$ 次迭代时的原始残差。这个更新规则确保如果原始残差不为零，对偶变量会被调整以在后续迭代中更强地施加约束。\n\n**停止准则**\n\n当原始和对偶残差的范数低于指定的容差时，算法终止。在第 $k$ 次迭代时：\n- 原始残差为 $r^k = w^k - z^k$。\n- 对偶残差为 $s^k = \\rho (z^k - z^{k-1})$。\n- 容差为 $\\varepsilon_{\\mathrm{pri}} = \\sqrt{p}\\,\\varepsilon_{\\mathrm{abs}} + \\varepsilon_{\\mathrm{rel}} \\max\\{\\lVert w^{k} \\rVert_{2}, \\lVert z^{k} \\rVert_{2}\\}$ 和 $\\varepsilon_{\\mathrm{dual}} = \\sqrt{p}\\,\\varepsilon_{\\mathrm{abs}} + \\varepsilon_{\\mathrm{rel}} \\rho \\lVert u^{k} \\rVert_{2}$。\n- 当同时满足 $\\lVert r^{k} \\rVert_{2} \\le \\varepsilon_{\\mathrm{pri}}$ 和 $\\lVert s^{k} \\rVert_{2} \\le \\varepsilon_{\\mathrm{dual}}$ 时，循环终止。\n\n数值实现将使用这些推导出的更新公式和停止准则。对于 $w$ 的更新，可以通过预先计算 $(X^\\top X + \\rho I)$ 的 Cholesky 分解来高效地求解线性系统。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import linalg\n\ndef solve():\n    \"\"\"\n    Sets up and runs the ADMM for Lasso on the four test cases specified\n    in the problem statement.\n    \"\"\"\n\n    def solve_admm(X, y, lambda_, rho, eps_abs=1e-6, eps_rel=1e-4, max_iter=2000):\n        \"\"\"\n        Solves the Lasso problem using ADMM.\n\n        The problem is to minimize:\n        0.5 * ||y - Xw||_2^2 + lambda * ||w||_1\n\n        Args:\n            X (np.ndarray): Data matrix (n x p).\n            y (np.ndarray): Response vector (n,).\n            lambda_ (float): L1 regularization parameter.\n            rho (float): ADMM penalty parameter.\n            eps_abs (float): Absolute tolerance for stopping criteria.\n            eps_rel (float): Relative tolerance for stopping criteria.\n            max_iter (int): Maximum number of iterations.\n\n        Returns:\n            np.ndarray: The converged coefficient vector w.\n        \"\"\"\n        n, p = X.shape\n\n        # Initialize variables\n        w = np.zeros(p)\n        z = np.zeros(p)\n        u = np.zeros(p)\n\n        # Pre-computation for w-update\n        X_T_X = X.T @ X\n        X_T_y = X.T @ y\n        # Use Cholesky factorization to solve the linear system in the w-update efficiently\n        # (X^T X + rho * I)w = X^T y + rho * (z - u)\n        L_and_lower = linalg.cho_factor(X_T_X + rho * np.eye(p))\n\n        for k in range(max_iter):\n            # Store z from previous iteration for dual residual calculation\n            z_prev = np.copy(z)\n\n            # 1. w-update\n            rhs = X_T_y + rho * (z - u)\n            w = linalg.cho_solve(L_and_lower, rhs)\n\n            # 2. z-update (soft-thresholding)\n            z_tilde = w + u\n            threshold = lambda_ / rho\n            z = np.sign(z_tilde) * np.maximum(np.abs(z_tilde) - threshold, 0)\n            \n            # 3. u-update\n            u = u + w - z\n\n            # Stopping criteria check\n            # Primal residual\n            r_k = w - z\n            norm_r = np.linalg.norm(r_k)\n            \n            # Dual residual\n            s_k = rho * (z - z_prev)\n            norm_s = np.linalg.norm(s_k)\n\n            # Tolerances\n            eps_pri = np.sqrt(p) * eps_abs + eps_rel * np.maximum(np.linalg.norm(w), np.linalg.norm(z))\n            eps_dual = np.sqrt(p) * eps_abs + eps_rel * rho * np.linalg.norm(u)\n\n            if norm_r = eps_pri and norm_s = eps_dual:\n                break\n        \n        return w\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case A: Orthonormal design\n        {\n            'X': np.eye(3),\n            'y': np.array([3.0, -1.0, 0.2]),\n            'lambda': 1.0,\n            'rho': 1.0\n        },\n        # Case B: Very large regularization\n        {\n            'X': np.array([[1, 0, 2], [0, 1, -1], [1, 1, 1], [2, -1, 0]]),\n            'y': np.array([1.0, -2.0, 0.5, 3.0]),\n            'lambda': 100.0,\n            'rho': 1.0\n        },\n        # Case C: Rank-deficient design\n        {\n            'X': np.array([[1, 2], [2, 4], [3, 6]]),\n            'y': np.array([1.0, 0.0, 1.0]),\n            'lambda': 0.1,\n            'rho': 1.0\n        },\n        # Case D: Tiny regularization\n        {\n            'X': np.array([[1, 0], [0, 1], [1, 1], [2, 1]]),\n            'y': np.array([1.0, 2.0, 2.5, 4.0]),\n            'lambda': 1e-6,\n            'rho': 1.0\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        X, y, lambda_, rho = case['X'], case['y'], case['lambda'], case['rho']\n        \n        w_final = solve_admm(X, y, lambda_, rho)\n        \n        # Calculate the final objective value F(w)\n        objective_value = 0.5 * np.linalg.norm(y - X @ w_final)**2 + lambda_ * np.linalg.norm(w_final, 1)\n        \n        results.append(round(objective_value, 6))\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3172064"}]}