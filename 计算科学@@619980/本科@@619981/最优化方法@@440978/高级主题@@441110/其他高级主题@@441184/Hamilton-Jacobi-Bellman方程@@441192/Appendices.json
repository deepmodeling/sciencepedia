{"hands_on_practices": [{"introduction": "在应用哈密顿-雅可比-贝尔曼（HJB）方程解决最优控制问题时，第一步是构建哈密顿量。这个函数将系统的动态特性、运行成本和价值函数梯度联系在一起，构成了HJB方程的核心。本练习将引导你为一个随机线性二次调节器（LQR）问题推导最小化的哈密顿量和最优反馈控制策略，这是掌握HJB方法的基石 [@problem_id:3080765]。", "problem": "考虑一维受控伊藤随机微分方程\n$$\n\\mathrm{d}X_{t} = b(X_{t},u_{t})\\,\\mathrm{d}t + \\sigma\\,\\mathrm{d}W_{t},\n$$\n其漂移为 $b(x,u) = u$，扩散系数为常数 $\\sigma > 0$，控制过程 $u_{t}$ 在 $\\mathbb{R}$ 中取值。设运行成本为\n$$\nf(x,u) = \\frac{1}{2} r\\,u^{2} + q\\,x^{2},\n$$\n其中 $r>0$ 且 $q>0$。对于一个具有有界连续终端成本 $g(x)$ 的有限时间域问题，将价值函数 $V(t,x)$ 定义为\n$$\nV(t,x) = \\inf_{u} \\,\\mathbb{E}\\!\\left[ \\int_{t}^{T} f(X_{s},u_{s})\\,\\mathrm{d}s + g(X_{T}) \\,\\middle|\\, X_{t}=x \\right],\n$$\n其中下确界取遍所有在 $\\mathbb{R}$ 中取值的循序可测控制。使用动态规划原理以及出现在哈密顿-雅可比-贝尔曼 (HJB) 方程中的哈密顿量的定义，点 $(x,p,M)$ 处的哈密顿量由施加于一个 $C^{2}$ 检验函数的受控生成元与运行成本之和，对所有控制 $u \\in \\mathbb{R}$ 取下确界得到。特别地，如果 $p$ 表示一阶空间导数，$M$ 表示二阶空间导数，请计算：\n- 最小化的哈密顿量，作为 $x$, $p$, $M$, $\\sigma$, $r$ 和 $q$ 的显式函数；\n- 相应的最小化状态反馈控制，作为 $p$ 和 $r$ 的显式函数。\n\n请将最终答案表示为一个双元素行矩阵，第一个元素是最小化的哈密顿量，第二个元素是最小化反馈控制。不需要数值近似，也不涉及单位。[@problem_id:294]", "solution": "用户要求我解决一个在随机最优控制背景下，与哈密顿-雅可比-贝尔曼 (HJB) 方程相关的问题。\n\n### 步骤1：提取已知条件\n- 受控伊藤随机微分方程 (SDE)：$\\mathrm{d}X_{t} = b(X_{t},u_{t})\\,\\mathrm{d}t + \\sigma\\,\\mathrm{d}W_{t}$。\n- 漂移函数：$b(x,u) = u$。\n- 扩散系数：$\\sigma > 0$ 是一个常数。\n- 控制过程 $u_{t}$ 在 $\\mathbb{R}$ 中取值。\n- 运行成本函数：$f(x,u) = \\frac{1}{2} r\\,u^{2} + q\\,x^{2}$。\n- 成本函数中的常数满足：$r>0$ 和 $q>0$。\n- 价值函数针对有限时间域 $[t, T]$ 定义为：$V(t,x) = \\inf_{u} \\,\\mathbb{E}\\!\\left[ \\int_{t}^{T} f(X_{s},u_{s})\\,\\mathrm{d}s + g(X_{T}) \\,\\middle|\\, X_{t}=x \\right]$。\n- 终端成本 $g(x)$ 是一个有界连续函数。\n- 点 $(x,p,M)$ 处的哈密顿量被定义为，对所有控制 $u \\in \\mathbb{R}$，施加于一个 $C^{2}$ 检验函数的受控生成元与运行成本之和的下确界。\n- 变量 $p$ 表示检验函数的一阶空间导数。\n- 变量 $M$ 表示检验函数的二阶空间导数。\n\n### 步骤2：使用提取的已知条件进行验证\n对问题进行验证。\n\n- **科学依据**：该问题是随机线性二次调节器 (LQR) 问题的标准形式，这是最优控制理论的基石。哈密顿-雅可比-贝尔曼 (HJB) 方程是通过动态规划解决此类问题的核心工具。所有概念在随机微分方程和控制理论领域都是标准且成熟的。\n- **适定性**：该问题是适定的。它要求计算两个具体且良定义的量：最小化的哈密顿量和最优反馈控制。条件 $r>0$ 确保了成本函数在控制变量 $u$ 上是严格凸的，这保证了最小值的存在性和唯一性。\n- **客观性**：该问题以精确、客观的数学语言陈述，没有任何主观性或歧义。\n\n该问题没有说明中列出的任何缺陷。它在科学上是合理的、适定的、客观的、完整的且可形式化的。\n\n### 步骤3：结论与行动\n该问题是**有效的**。将提供完整解答。\n\n### 解答\n该问题要求计算给定随机控制问题的最小化哈密顿量和相应的最优控制。在哈密顿-雅可比-贝尔曼 (HJB) 方程的背景下，哈密顿量定义为：\n$$\nH(x,p,M) = \\inf_{u \\in \\mathbb{R}} \\left\\{ \\mathcal{L}^{u} + f(x,u) \\right\\}\n$$\n其中 $\\mathcal{L}^{u}$ 是受控随机过程的生成元，作用于一个检验函数（其一阶和二阶空间导数分别用 $p$ 和 $M$ 表示），而 $f(x,u)$ 是运行成本。\n\n首先，我们确定生成元 $\\mathcal{L}^{u}$。对于由 $\\mathrm{d}X_{t} = \\mu(t, X_{t})\\,\\mathrm{d}t + \\Sigma(t, X_{t})\\,\\mathrm{d}W_{t}$ 给出的一般一维伊藤过程，作用于 $C^{2}$ 函数 $\\phi(x)$ 的生成元是：\n$$\n\\mathcal{L}\\phi(x) = \\mu(t,x) \\frac{\\partial \\phi}{\\partial x} + \\frac{1}{2} \\Sigma(t,x)^{2} \\frac{\\partial^{2} \\phi}{\\partial x^{2}}\n$$\n在我们的具体问题中，SDE 是 $\\mathrm{d}X_{t} = u_{t}\\,\\mathrm{d}t + \\sigma\\,\\mathrm{d}W_{t}$。漂移是 $\\mu(t,x,u) = u$，扩散是 $\\Sigma(t,x,u) = \\sigma$。设 $\\phi(x)$ 是一个通用的 $C^2$ 检验函数。根据问题陈述，我们将其导数表示为 $ p = \\frac{\\mathrm{d}\\phi}{\\mathrm{d}x}$ 和 $M = \\frac{\\mathrm{d}^2\\phi}{\\mathrm{d}x^2}$。\n因此，受控过程的生成元 $\\mathcal{L}^{u}$ 为：\n$$\n\\mathcal{L}^{u} = u \\cdot p + \\frac{1}{2}\\sigma^{2} \\cdot M\n$$\n接下来，我们加上运行成本 $f(x,u) = \\frac{1}{2} r\\,u^{2} + q\\,x^{2}$。需要对控制 $u \\in \\mathbb{R}$ 最小化的表达式是：\n$$\nJ(u) = \\mathcal{L}^{u} + f(x,u) = \\left( u p + \\frac{1}{2}\\sigma^{2}M \\right) + \\left( \\frac{1}{2} r\\,u^{2} + q\\,x^{2} \\right)\n$$\n我们可以将此表达式重新排列为 $u$ 的函数：\n$$\nJ(u) = \\frac{1}{2} r\\,u^{2} + p\\,u + \\left( q\\,x^{2} + \\frac{1}{2}\\sigma^{2}M \\right)\n$$\n这是一个关于 $u$ 的二次函数。由于问题陈述 $r>0$，抛物线开口向上，该函数具有唯一的全局最小值。为了找到最小化 $J(u)$ 的 $u$ 值，我们计算其关于 $u$ 的一阶导数并令其为零。\n$$\n\\frac{\\mathrm{d}J}{\\mathrm{d}u} = \\frac{\\mathrm{d}}{\\mathrm{d}u} \\left( \\frac{1}{2} r\\,u^{2} + p\\,u + q\\,x^{2} + \\frac{1}{2}\\sigma^{2}M \\right) = r\\,u + p\n$$\n令导数为零，得到最优控制，我们记作 $u^{\\star}$：\n$$\nr\\,u^{\\star} + p = 0 \\implies u^{\\star} = -\\frac{p}{r}\n$$\n这就是作为 $p$ 和 $r$ 函数的最小化状态反馈控制。这是要求答案的第二部分。\n\n为了找到最小化的哈密顿量 $H(x,p,M)$，我们将 $u^{\\star}$ 代回 $J(u)$ 的表达式中：\n$$\nH(x,p,M) = J(u^{\\star}) = \\frac{1}{2} r\\left(-\\frac{p}{r}\\right)^{2} + p\\left(-\\frac{p}{r}\\right) + q\\,x^{2} + \\frac{1}{2}\\sigma^{2}M\n$$\n$$\nH(x,p,M) = \\frac{1}{2} r\\left(\\frac{p^{2}}{r^{2}}\\right) - \\frac{p^{2}}{r} + q\\,x^{2} + \\frac{1}{2}\\sigma^{2}M\n$$\n$$\nH(x,p,M) = \\frac{p^{2}}{2r} - \\frac{p^{2}}{r} + q\\,x^{2} + \\frac{1}{2}\\sigma^{2}M\n$$\n合并包含 $p^{2}$ 的项：\n$$\nH(x,p,M) = -\\frac{p^{2}}{2r} + q\\,x^{2} + \\frac{1}{2}\\sigma^{2}M\n$$\n这就是作为 $x$, $p$, $M$, $\\sigma$, $r$ 和 $q$ 的显式函数的最小化哈密顿量。这是要求答案的第一部分。\n\n最终答案以一个双元素行矩阵的形式呈现。\n第一个元素：最小化的哈密顿量 $H(x,p,M) = q\\,x^{2} - \\frac{p^{2}}{2r} + \\frac{1}{2}\\sigma^{2}M$。\n第二个元素：最小化控制 $u^{\\star} = -\\frac{p}{r}$。", "answer": "$$\n\\boxed{\\begin{pmatrix} q\\,x^{2} - \\frac{p^{2}}{2r} + \\frac{1}{2}\\sigma^{2}M  -\\frac{p}{r} \\end{pmatrix}}\n$$", "id": "3080765"}, {"introduction": "掌握了如何构建哈密顿量之后，下一步自然是求解完整的HJB方程。本练习将带你解决一个经典的无限时域、折扣成本的LQR问题 [@problem_id:3080759]。你将看到，通过一个二次价值函数的巧妙假设，复杂的HJB偏微分方程可以被转化为一个简单的代数方程——著名的代数Riccati方程（ARE），这展示了一种求解一大类控制问题的强大解析方法。", "problem": "考虑一维受控随机微分方程 (SDE)\n$$\n\\mathrm{d}x_{t}=\\big(a\\,x_{t}+b\\,u_{t}\\big)\\,\\mathrm{d}t+\\sigma\\,\\mathrm{d}W_{t},\n$$\n其中 $a=1$、$b=2$ 且 $\\sigma=1$，并且 $\\{W_{t}\\}_{t\\ge 0}$ 是一个标准维纳过程。令控制 $\\{u_{t}\\}_{t\\ge 0}$ 关于由 $\\{W_{t}\\}$ 生成的 filtration 是循序可测的，并满足确保唯一强解存在的标准可积性条件。无穷时域折扣成本为\n$$\nJ(u)=\\mathbb{E}\\left[\\int_{0}^{\\infty}\\exp(-\\rho t)\\,\\big(q\\,x_{t}^{2}+r\\,u_{t}^{2}\\big)\\,\\mathrm{d}t\\right],\n$$\n其中 $\\rho=3$、$q=1$ 且 $r=1$。假设时间同质性，并考虑稳态马尔可夫反馈控制 $u_{t}=u(x_{t})$。\n\n从动态规划原理和扩散过程的无穷小生成元出发，推导在无穷时域折扣设置下值函数 $V(x)$ 的 Hamilton-Jacobi-Bellman (HJB) 方程。在二次拟设 $V(x)=P\\,x^{2}+v_{0}$（其中常数 $P0$ 且 $v_{0}\\in\\mathbb{R}$）下，证明 $P$ 必须满足一个代数 Riccati 方程，显式求解 $P$，并确定稳态最优反馈 $u^{\\star}(x)$ 和相应的值函数 $V(x)$。\n\n将您的最终答案以单行矩阵的形式给出三个表达式，顺序为 $(P,\\;u^{\\star}(x),\\;V(x))$。最终答案中不应出现推导过程。最终答案必须是精确的闭式表达式。不要四舍五入或近似。不需要单位。", "solution": "该问题是有效的。这是一个适定的、有科学依据的随机最优控制问题，具体来说是一个连续时间线性二次调节器问题。所有必要的数据和条件都已给出，没有矛盾或模糊之处。\n\n无穷时域折扣最优控制问题的值函数定义为\n$$\nV(x) = \\inf_{u} J(u) = \\inf_{u} \\mathbb{E}\\left[\\int_{0}^{\\infty}\\exp(-\\rho t)\\,\\big(q\\,x_{t}^{2}+r\\,u_{t}^{2}\\big)\\,\\mathrm{d}t \\, \\bigg| \\, x_0=x \\right]\n$$\n其中下确界取遍所有容许控制 $\\{u_t\\}$。动态规划原理导出 Hamilton-Jacobi-Bellman (HJB) 方程。对于一个时间同质的无穷时域折扣问题，HJB 方程由下式给出\n$$\n\\rho V(x) = \\inf_{u \\in \\mathbb{R}} \\left\\{ L(x, u) + \\mathcal{A}^u V(x) \\right\\}\n$$\n其中 $\\rho$ 是折扣率，$L(x, u) = q\\,x^{2}+r\\,u^{2}$ 是运行成本，$\\mathcal{A}^u$ 是受控扩散过程 $x_t$ 的无穷小生成元。\n\n受控随机微分方程 (SDE) 为\n$$\n\\mathrm{d}x_{t}=\\big(a\\,x_{t}+b\\,u_{t}\\big)\\,\\mathrm{d}t+\\sigma\\,\\mathrm{d}W_{t}\n$$\n漂移系数为 $\\mu(x, u) = a\\,x+b\\,u$，扩散系数为 $\\eta(x, u) = \\sigma$。作用于二次可微函数 $V(x)$ 的无穷小生成元 $\\mathcal{A}^u$ 是\n$$\n\\mathcal{A}^u V(x) = \\mu(x,u) \\frac{\\mathrm{d}V}{\\mathrm{d}x} + \\frac{1}{2} \\eta(x,u)^2 \\frac{\\mathrm{d}^2V}{\\mathrm{d}x^2} = (a\\,x+b\\,u)V'(x) + \\frac{1}{2}\\sigma^2 V''(x)\n$$\n将 $L(x, u)$ 和 $\\mathcal{A}^u V(x)$ 代入 HJB 方程得到\n$$\n\\rho V(x) = \\inf_{u \\in \\mathbb{R}} \\left\\{ q\\,x^2 + r\\,u^2 + (a\\,x+b\\,u)V'(x) + \\frac{1}{2}\\sigma^2 V''(x) \\right\\}\n$$\n为了找到最优控制 $u^{\\star}$，我们对大括号内的表达式关于 $u$ 求最小值。该表达式是 $u$ 的二次函数。最小化的一阶条件是通过将关于 $u$ 的偏导数设为零来找到的：\n$$\n\\frac{\\partial}{\\partial u} \\left[ q\\,x^2 + r\\,u^2 + (a\\,x+b\\,u)V'(x) + \\frac{1}{2}\\sigma^2 V''(x) \\right] = 2\\,r\\,u + b\\,V'(x) = 0\n$$\n求解 $u$ 得到作为值函数导数的函数的稳态最优反馈控制：\n$$\nu^{\\star}(x) = -\\frac{b}{2r}V'(x)\n$$\n二阶导数为 $2r$，因为 $r=1  0$ 所以为正，这证实了这确实是一个最小值点。\n\n将 $u^{\\star}(x)$ 代回 HJB 方程，消去下确界：\n$$\n\\rho V(x) = q\\,x^2 + r\\left(-\\frac{b}{2r}V'(x)\\right)^2 + \\left(a\\,x+b\\left(-\\frac{b}{2r}V'(x)\\right)\\right)V'(x) + \\frac{1}{2}\\sigma^2 V''(x)\n$$\n简化各项：\n$$\n\\rho V(x) = q\\,x^2 + r\\frac{b^2}{4r^2}(V'(x))^2 + a\\,xV'(x) - \\frac{b^2}{2r}(V'(x))^2 + \\frac{1}{2}\\sigma^2 V''(x)\n$$\n$$\n\\rho V(x) = q\\,x^2 + a\\,xV'(x) - \\frac{b^2}{4r}(V'(x))^2 + \\frac{1}{2}\\sigma^2 V''(x)\n$$\n这是一个关于值函数 $V(x)$ 的非线性常微分方程。\n\n我们现在使用二次拟设 $V(x) = P\\,x^2 + v_0$，其中 $P0$ 和 $v_0$ 是待定常数。其导数为：\n$$\nV'(x) = 2\\,P\\,x\n$$\n$$\nV''(x) = 2\\,P\n$$\n将这些代入 HJB 方程：\n$$\n\\rho(P\\,x^2+v_0) = q\\,x^2 + a\\,x(2\\,P\\,x) - \\frac{b^2}{4r}(2\\,P\\,x)^2 + \\frac{1}{2}\\sigma^2(2\\,P)\n$$\n$$\n\\rho\\,P\\,x^2 + \\rho\\,v_0 = q\\,x^2 + 2\\,a\\,P\\,x^2 - \\frac{b^2}{4r}(4\\,P^2\\,x^2) + \\sigma^2\\,P\n$$\n$$\n\\rho\\,P\\,x^2 + \\rho\\,v_0 = \\left( q + 2\\,a\\,P - \\frac{b^2}{r}P^2 \\right)x^2 + \\sigma^2\\,P\n$$\n为了使该方程对所有 $x \\in \\mathbb{R}$ 成立，我们必须令等式两边 $x$ 的各次幂的系数相等。\n令 $x^2$ 的系数相等：\n$$\n\\rho\\,P = q + 2\\,a\\,P - \\frac{b^2}{r}P^2\n$$\n整理后得到关于 $P$ 的代数 Riccati 方程 (ARE)：\n$$\n\\frac{b^2}{r}P^2 + (\\rho - 2\\,a)P - q = 0\n$$\n令常数项（$x^0$ 的系数）相等：\n$$\n\\rho\\,v_0 = \\sigma^2\\,P \\quad \\implies \\quad v_0 = \\frac{\\sigma^2\\,P}{\\rho}\n$$\n我们给定的参数值为：$a=1$、$b=2$、$\\sigma=1$、$\\rho=3$、$q=1$、$r=1$。将这些值代入 ARE：\n$$\n\\frac{2^2}{1}P^2 + (3 - 2 \\cdot 1)P - 1 = 0\n$$\n$$\n4P^2 + P - 1 = 0\n$$\n我们使用二次公式求解这个关于 $P$ 的二次方程：\n$$\nP = \\frac{-1 \\pm \\sqrt{1^2 - 4(4)(-1)}}{2(4)} = \\frac{-1 \\pm \\sqrt{1 + 16}}{8} = \\frac{-1 \\pm \\sqrt{17}}{8}\n$$\n值函数必须是凸的，这对应于 $P  0$。因此，我们必须选择正根：\n$$\nP = \\frac{-1 + \\sqrt{17}}{8}\n$$\n现在我们求常数项 $v_0$：\n$$\nv_0 = \\frac{\\sigma^2\\,P}{\\rho} = \\frac{1^2}{3}P = \\frac{1}{3} \\left( \\frac{\\sqrt{17}-1}{8} \\right) = \\frac{\\sqrt{17}-1}{24}\n$$\n因此，值函数为：\n$$\nV(x) = P\\,x^2 + v_0 = \\left(\\frac{\\sqrt{17}-1}{8}\\right)x^2 + \\frac{\\sqrt{17}-1}{24}\n$$\n最后，我们确定最优反馈控制 $u^{\\star}(x)$：\n$$\nu^{\\star}(x) = -\\frac{b}{2r}V'(x) = -\\frac{b}{2r}(2\\,P\\,x) = -\\frac{b}{r}Px\n$$\n代入 $b$、$r$ 和已解出的 $P$ 的值：\n$$\nu^{\\star}(x) = -\\frac{2}{1} \\left( \\frac{\\sqrt{17}-1}{8} \\right) x = -\\frac{\\sqrt{17}-1}{4}x\n$$\n所求的三个量是：\n$P = \\frac{\\sqrt{17}-1}{8}$\n$u^{\\star}(x) = -\\frac{\\sqrt{17}-1}{4}x$\n$V(x) = \\frac{\\sqrt{17}-1}{8}x^2 + \\frac{\\sqrt{17}-1}{24}$", "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{\\sqrt{17}-1}{8}  -\\frac{\\sqrt{17}-1}{4}x  \\frac{\\sqrt{17}-1}{8}x^2 + \\frac{\\sqrt{17}-1}{24} \\end{pmatrix}}\n$$", "id": "3080759"}, {"introduction": "虽然解析解能提供深刻的洞见，但许多现实世界中的问题过于复杂，无法手动求解。最后的这个练习将理论与实践联系起来，指导你通过策略迭代法来数值求解HJB方程 [@problem_id:3001638]。你将学习如何对状态和控制空间进行离散化，并迭代地逼近最优策略和价值函数，从而对HJB方程在实际中的应用有更深入的理解。", "problem": "从第一性原理出发，实现一个完整的策略迭代方案，用于求解一个由受控随机微分方程引起的一维平稳折扣 Hamilton-Jacobi-Bellman 方程。受控状态过程由随机微分方程 $dX_t = \\left(a X_t + b u_t\\right) dt + \\sigma dW_t$ 给出，折扣率为 $\\rho  0$，其中 $a$、$b$、$\\sigma$ 和 $\\rho$ 是固定的实数参数，$u_t \\in \\mathbb{R}$ 是一个控制，$W_t$ 是一个标准维纳过程。瞬时成本为 $\\ell(x,u) = q x^2 + r u^2$，其中 $q  0$ 且 $r  0$。从初始状态 $x$ 出发，目标是在所有容许控制下，最小化折扣成本 $J^u(x) = \\mathbb{E}\\left[\\int_0^\\infty e^{-\\rho t} \\ell\\left(X_t^u, u_t\\right) dt\\right]$。令 $V(x)$ 表示值函数。\n\n您的任务是：\n- 推导 $V(x)$ 在有界区间 $[-x_{\\max}, x_{\\max}]$ 上满足的平稳折扣 Hamilton-Jacobi-Bellman 方程，其狄利克雷边界条件选择为与该问题的精确无限时域折扣线性二次解 $V^{\\mathrm{ref}}(x)$ 相匹配。已知参考解是 $V^{\\mathrm{ref}}(x) = P x^2 + C$ 形式的二次函数，其中系数 $P$ 由 Hamilton-Jacobi-Bellman 方程所蕴含的代数 Riccati 关系确定，而 $C$ 的选择是为了满足常数扩散项的平稳折扣平衡。\n- 在 $[-x_{\\max}, x_{\\max}]$ 上用 $N$ 个网格点对状态空间进行均匀离散化，网格间距为 $h = 2 x_{\\max}/(N-1)$。在内部节点处，用中心差分算子近似一阶导数，用标准三点中心差分算子近似二阶导数。施加狄利克雷边界条件 $V(-x_{\\max}) = V^{\\mathrm{ref}}(-x_{\\max})$ 和 $V(x_{\\max}) = V^{\\mathrm{ref}}(x_{\\max})$。\n- 将动作空间离散化为一个有限、对称的集合 $\\mathcal{U}_M = \\left\\{u_j\\right\\}_{j=1}^M$，包含 $M$ 个在 $[-u_{\\max}, u_{\\max}]$ 中均匀分布的值，其中 $u_{\\max}$ 选择为 $u_{\\max} = \\kappa | b P x_{\\max} / r |$ (其中 $\\kappa = 1.25$)，以确保离散动作网格覆盖整个计算域上的连续最优解。\n- 实现策略迭代：\n  - 策略评估：对于每个内部网格点 $x_i$ 上的固定离散策略 $u(x_i) \\in \\mathcal{U}_M$，求解离散化的平稳折扣 Hamilton-Jacobi-Bellman 方程产生的线性系统，以获得网格上的值向量。\n  - 策略改进：在每个内部网格点 $x_i$ 处，使用当前的离散梯度近似 $\\left(D_x V\\right)(x_i)$ 计算离散哈密顿量的最小化子 $\\arg\\min_{u \\in \\mathcal{U}_M} \\left\\{ r u^2 + b u \\left(D_x V\\right)(x_i) \\right\\}$，并更新策略。\n  - 当值向量和策略向量的上确界范数变化均小于容差 $\\varepsilon$，或当策略迭代次数达到最大值 $K_{\\max}$ 时，终止迭代。\n- 对下面的每个测试用例，计算网格上的均匀误差 $\\| V_{\\mathrm{num}} - V^{\\mathrm{ref}} \\|_{\\infty}$，其中 $V_{\\mathrm{num}}$ 是收敛的数值值函数，$V^{\\mathrm{ref}}$ 是精确的二次参考解。\n\n使用以下参数集测试套件，每个参数集以元组 $\\left(a, b, q, r, \\sigma, \\rho, x_{\\max}, N, M\\right)$ 的形式提供：\n- 测试 1：$\\left(-0.5, 1.0, 1.0, 1.0, 0.5, 1.0, 3.0, 161, 61\\right)$。\n- 测试 2：$\\left(0.2, 1.0, 1.0, 0.5, 0.1, 1.0, 2.0, 161, 61\\right)$。\n- 测试 3：$\\left(0.3, 1.0, 2.0, 1.0, 1.0, 2.0, 3.0, 161, 61\\right)$。\n\n实现细节和要求：\n- 在内部节点上从零策略 $u \\equiv 0$ 开始进行策略迭代。\n- 使用容差 $\\varepsilon = 10^{-6}$ 和最大迭代次数 $K_{\\max} = 100$。\n- 所有计算都是无量纲的；不需要物理单位。\n- 您的程序必须是一个单一、完整的脚本，无需用户输入即可完成所有三个测试的全部计算，并打印包含结果的单行输出。\n\n最终输出格式：\n- 您的程序应产生单行输出，其中包含一个用方括号括起来的逗号分隔列表，按测试顺序排列，例如 $\\left[\\text{result}_1,\\text{result}_2,\\text{result}_3\\right]$，其中每个条目是对应测试在状态网格上计算出的 $\\| V_{\\mathrm{num}} - V^{\\mathrm{ref}} \\|_{\\infty}$ 的浮点值。", "solution": "用户提供的问题是随机最优控制领域中一个适定且科学严谨的任务。它要求使用策略迭代方案对一个平稳折扣 Hamilton-Jacobi-Bellman (HJB) 方程进行数值求解。该问题是自洽的，提供了所有必要的参数和方法论规范。它基于成熟的数学理论，是计算金融和控制工程中的一个标准问题。因此，该问题被认为是有效的，下面提供了完整的解决方案。\n\n### 1. Hamilton-Jacobi-Bellman 方程和精确解\n\n状态过程由线性随机微分方程控制：\n$$\ndX_t = \\left(a X_t + b u_t\\right) dt + \\sigma dW_t\n$$\n其中 $a$、$b$ 和 $\\sigma$ 是实常数，$u_t \\in \\mathbb{R}$ 是控制，$W_t$ 是一个维纳过程。目标是最小化折扣成本泛函，折扣率为 $\\rho  0$，瞬时成本为二次形式 $\\ell(x, u) = q x^2 + r u^2$：\n$$\nJ^u(x) = \\mathbb{E}\\left[\\int_0^\\infty e^{-\\rho t} \\ell\\left(X_t^u, u_t\\right) dt \\;\\middle|\\; X_0 = x \\right]\n$$\n值函数 $V(x) = \\inf_u J^u(x)$ 满足平稳 Hamilton-Jacobi-Bellman (HJB) 方程：\n$$\n\\rho V(x) = \\inf_{u \\in \\mathbb{R}} \\left\\{ \\mathcal{L}^u V(x) + \\ell(x, u) \\right\\}\n$$\n其中 $\\mathcal{L}^u$ 是过程 $X_t$ 的无穷小生成元，由 $\\mathcal{L}^u V(x) = (a x + b u) V'(x) + \\frac{1}{2} \\sigma^2 V''(x)$ 给出。代入生成元和瞬时成本的表达式，HJB 方程变为：\n$$\n\\rho V(x) = \\inf_{u \\in \\mathbb{R}} \\left\\{ (a x + b u) V'(x) + \\frac{1}{2} \\sigma^2 V''(x) + q x^2 + r u^2 \\right\\}\n$$\n通过将大括号中项关于 $u$ 的偏导数设为零，可以找到关于 $u$ 的下确界：\n$$\nb V'(x) + 2 r u = 0 \\implies u^*(x) = -\\frac{b}{2r} V'(x)\n$$\n将此最优控制 $u^*(x)$ 代回 HJB 方程，得到一个关于 $V(x)$ 的非线性常微分方程：\n$$\n\\rho V(x) = a x V'(x) - \\frac{b^2}{4r} (V'(x))^2 + \\frac{1}{2} \\sigma^2 V''(x) + q x^2\n$$\n对于这个线性二次问题，已知值函数是状态的二次函数，$V(x) = P x^2 + C$。其导数为 $V'(x) = 2 P x$ 和 $V''(x) = 2 P$。将这些代入 HJB 方程得到：\n$$\n\\rho (P x^2 + C) = a x (2 P x) - \\frac{b^2}{4r} (2 P x)^2 + \\frac{1}{2} \\sigma^2 (2 P) + q x^2\n$$\n$$\n\\rho P x^2 + \\rho C = (2aP - \\frac{b^2}{r}P^2 + q) x^2 + \\sigma^2 P\n$$\n令 $x$ 的同次幂系数相等，我们得到两个关于未知常数 $P$ 和 $C$ 的代数方程。$x^2$ 的系数产生了连续时间代数 Riccati 方程 (ARE)：\n$$\n\\frac{b^2}{r} P^2 - (2a - \\rho) P - q = 0\n$$\n由于成本必须是正定的，我们要求 $P  0$。ARE 是一个关于 $P$ 的二次方程，它有一个唯一的正解，由以下公式给出：\n$$\nP = \\frac{(2a - \\rho) + \\sqrt{(2a - \\rho)^2 + 4(b^2/r)q}}{2(b^2/r)}\n$$\n令常数项相等，得到关于 $C$ 的方程：\n$$\n\\rho C = \\sigma^2 P \\implies C = \\frac{\\sigma^2 P}{\\rho}\n$$\n这就定义了在无限域上的精确参考解 $V^{\\mathrm{ref}}(x) = Px^2 + C$。\n\n### 2. 离散化\n\n该问题在有界域 $x \\in [-x_{\\max}, x_{\\max}]$ 上求解，该域被离散化为一个包含 $N$ 个点的均匀网格 $\\{x_i\\}_{i=0}^{N-1}$，间距为 $h = 2x_{\\max}/(N-1)$。令 $V_i$ 为 $V(x_i)$ 的数值近似。在内部网格点 $x_i$ (其中 $i \\in \\{1, \\dots, N-2\\}$)，导数使用中心有限差分进行近似：\n$$\nV'(x_i) \\approx \\frac{V_{i+1} - V_{i-1}}{2h}, \\qquad V''(x_i) \\approx \\frac{V_{i+1} - 2V_i + V_{i-1}}{h^2}\n$$\n对于一个固定的策略 $u(x)$（离散表示为 $u_i = u(x_i)$），HJB 方程在 $V$ 上是线性的。在每个内部点 $x_i$ 的离散形式是：\n$$\n\\rho V_i = (a x_i + b u_i) \\left(\\frac{V_{i+1} - V_{i-1}}{2h}\\right) + \\frac{1}{2}\\sigma^2 \\left(\\frac{V_{i+1} - 2V_i + V_{i-1}}{h^2}\\right) + q x_i^2 + r u_i^2\n$$\n重新整理各项，我们得到一个关于 $V_{i-1}, V_i, V_{i+1}$ 的线性方程：\n$$\n\\left( \\frac{a x_i + b u_i}{2h} - \\frac{\\sigma^2}{2h^2} \\right) V_{i-1} + \\left( \\rho + \\frac{\\sigma^2}{h^2} \\right) V_i - \\left( \\frac{a x_i + b u_i}{2h} + \\frac{\\sigma^2}{2h^2} \\right) V_{i+1} = q x_i^2 + r u_i^2\n$$\n边界条件是狄利克雷型的，固定为参考解的值：$V_0 = V^{\\mathrm{ref}}(-x_{\\max})$ 和 $V_{N-1} = V^{\\mathrm{ref}}(x_{\\max})$。\n\n### 3. 策略迭代算法\n\n策略迭代是一种在策略评估和策略改进两个步骤之间交替进行的迭代方法。\n\n**初始化**：算法从一个初始策略开始，该策略被设为零策略，$u^{(0)}(x_i) = 0$（对于所有内部网格点 $x_i$）。初始值函数 $V^{(0)}$ 被初始化为零，其边界值由参考解设定。\n\n**策略评估**：对于一个给定的策略 $u^{(k)}$，我们求解相应的值函数 $V^{(k+1)}$。所有内部点 $i \\in \\{1, \\dots, N-2\\}$ 的离散 HJB 方程集构成一个关于未知值 $\\{V_i^{(k+1)}\\}_{i=1}^{N-2}$ 的三对角线性方程组。这个形如 $\\mathbf{A}\\mathbf{V} = \\mathbf{d}$ 的系统使用标准的线性代数求解器来求解。矩阵 $\\mathbf{A}$ 和向量 $\\mathbf{d}$ 依赖于当前策略 $u^{(k)}$。\n\n**策略改进**：在计算出新的值函数 $V^{(k+1)}$ 后，通过在每个内部网格点 $x_i$ 最小化离散哈密顿量来将策略更新为 $u^{(k+1)}$：\n$$\nu_i^{(k+1)} = \\arg\\min_{u \\in \\mathcal{U}_M} \\left\\{ (a x_i + b u) \\frac{V_{i+1}^{(k+1)} - V_{i-1}^{(k+1)}}{2h} + \\frac{1}{2}\\sigma^2 (\\dots) + q x_i^2 + r u^2 \\right\\}\n$$\n这等价于在离散动作集 $\\mathcal{U}_M$ 上最小化 $r u^2 + b u \\frac{V_{i+1}^{(k+1)} - V_{i-1}^{(k+1)}}{2h}$。无约束的最小化子是 $u_i^* = -\\frac{b}{2r} \\frac{V_{i+1}^{(k+1)} - V_{i-1}^{(k+1)}}{2h}$。然后，新的策略动作 $u_i^{(k+1)}$ 被选为离散动作集 $\\mathcal{U}_M$ 中最接近 $u_i^*$ 的元素。\n\n**终止**：评估和改进之间的迭代持续进行，直到策略和值函数收敛。当值向量的最大绝对变化 $\\|V^{(k+1)} - V^{(k)}\\|_{\\infty}$ 和策略向量的最大绝对变化 $\\|u^{(k+1)} - u^{(k)}\\|_{\\infty}$ 都低于指定的容差 $\\varepsilon = 10^{-6}$，或者达到最大迭代次数 $K_{\\max} = 100$ 时，过程终止。\n\n### 4. 误差计算\n\n在策略迭代收敛到最终数值解 $V_{\\mathrm{num}}$ 后，通过计算其与计算网格上精确参考解 $V^{\\mathrm{ref}}$ 之间的误差的均匀范数来评估其准确性：\n$$\n\\text{Error} = \\| V_{\\mathrm{num}} - V^{\\mathrm{ref}} \\|_{\\infty} = \\max_{i \\in \\{0, \\dots, N-1\\}} | V_{\\mathrm{num}}(x_i) - V^{\\mathrm{ref}}(x_i) |\n$$\n对每个提供的测试用例都会计算这个量。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the policy iteration for all test cases and print results.\n    \"\"\"\n    test_cases = [\n        # (a, b, q, r, sigma, rho, x_max, N, M)\n        (-0.5, 1.0, 1.0, 1.0, 0.5, 1.0, 3.0, 161, 61),\n        (0.2, 1.0, 1.0, 0.5, 0.1, 1.0, 2.0, 161, 61),\n        (0.3, 1.0, 2.0, 1.0, 1.0, 2.0, 3.0, 161, 61),\n    ]\n\n    results = []\n    for params in test_cases:\n        error = run_policy_iteration(params)\n        results.append(error)\n\n    # Format and print the final output as a single line\n    print(f\"[{','.join(f'{r:.8f}' for r in results)}]\")\n\ndef run_policy_iteration(params):\n    \"\"\"\n    Solves the HJB equation for a single set of parameters using policy iteration.\n    \"\"\"\n    a, b, q, r, sigma, rho, x_max, N, M = params\n    \n    # --- Step A: Pre-computation and Setup ---\n\n    # 1. Solve the Algebraic Riccati Equation for P\n    # The ARE is (b^2/r)P^2 - (2a - rho)P - q = 0\n    A_ric = b**2 / r\n    B_ric = -(2.0*a - rho)\n    C_ric = -q\n    discriminant = B_ric**2 - 4.0 * A_ric * C_ric\n    # We need the positive root for P\n    P = (-B_ric + np.sqrt(discriminant)) / (2.0 * A_ric)\n\n    # 2. Calculate the constant C\n    C = (sigma**2 * P) / rho\n\n    # 3. Define the reference (exact) value function\n    def v_ref(x):\n        return P * x**2 + C\n\n    # 4. Set up the state and action grids\n    x_grid = np.linspace(-x_max, x_max, N)\n    h = x_grid[1] - x_grid[0]\n    v_ref_grid = v_ref(x_grid)\n\n    kappa = 1.25\n    u_max_val = kappa * abs(b * P * x_max / r)\n    \n    if M  1 and u_max_val  0:\n        u_grid = np.linspace(-u_max_val, u_max_val, M)\n    else:\n        u_grid = np.zeros(M)\n    u_step = u_grid[1] - u_grid[0] if M  1 else 0\n\n    # 5. Set up iteration parameters\n    tol = 1e-6\n    max_iter = 100\n    \n    # --- Step B: Policy Iteration Loop ---\n\n    # Initialization\n    # Policy for interior points (size N-2)\n    current_policy = np.zeros(N - 2)\n    # Value function on the full grid (size N)\n    current_V = np.zeros(N)\n    # Set Dirichlet boundary conditions from the exact solution\n    current_V[0] = v_ref_grid[0]\n    current_V[-1] = v_ref_grid[-1]\n    \n    x_interior = x_grid[1:-1]\n    \n    for k in range(max_iter):\n        \n        # --- 1. Policy Evaluation ---\n        # Solve the linear system A * V_interior = d for V\n        \n        drift_coeff = a * x_interior + b * current_policy\n        \n        # Coefficients of the tridiagonal system\n        L = drift_coeff / (2.0 * h) - sigma**2 / (2.0 * h**2)\n        D = rho + sigma**2 / h**2\n        U = -drift_coeff / (2.0 * h) - sigma**2 / (2.0 * h**2)\n        \n        # Construct the (N-2) x (N-2) system matrix A\n        A = np.diag(D * np.ones(N-2)) + np.diag(U[:-1], k=1) + np.diag(L[1:], k=-1)\n        \n        # Construct the right-hand side vector d\n        d = q * x_interior**2 + r * current_policy**2\n        \n        # Adjust d for boundary conditions\n        d[0] -= L[0] * v_ref_grid[0]\n        d[-1] -= U[-1] * v_ref_grid[-1]\n        \n        # Solve for the new interior values of V\n        try:\n            V_interior_new = np.linalg.solve(A, d)\n        except np.linalg.LinAlgError:\n            # Fallback for singular matrix, although not expected here\n            return np.inf\n\n        # Form the full new value function vector\n        V_new = np.concatenate(([v_ref_grid[0]], V_interior_new, [v_ref_grid[-1]]))\n        \n        # --- 2. Policy Improvement ---\n        # Update the policy by minimizing the Hamiltonian\n        \n        # Approximate V'(x) at interior points\n        V_prime_interior = (V_new[2:] - V_new[:-2]) / (2.0 * h)\n        \n        # Compute the unconstrained optimal control\n        u_star_interior = -b / (2.0 * r) * V_prime_interior\n        \n        # Find the closest control in the discrete action space\n        new_policy = np.zeros_like(current_policy)\n        if M  1 and u_step  0:\n            indices = np.round((u_star_interior - u_grid[0]) / u_step)\n            indices = np.clip(indices, 0, M - 1).astype(int)\n            new_policy = u_grid[indices]\n            \n        # --- 3. Termination Check ---\n        \n        val_change = np.max(np.abs(V_new - current_V))\n        pol_change = np.max(np.abs(new_policy - current_policy))\n        \n        # Update for the next iteration\n        current_V = V_new\n        current_policy = new_policy\n        \n        if val_change  tol and pol_change  tol:\n            break\n            \n    # --- Step C: Final Error Calculation ---\n    \n    # The converged numerical solution is the last computed value function\n    V_num = current_V\n    error = np.max(np.abs(V_num - v_ref_grid))\n    \n    return error\n\nif __name__ == '__main__':\n    solve()\n```", "id": "3001638"}]}