{"hands_on_practices": [{"introduction": "在应用差分凸函数算法（DCA）之前，我们必须首先能够识别并为一个给定的非凸函数构建一个差分凸（DC）分解。第一个练习侧重于这些基本技能：识别凸分量 $g(x)$ 和 $h(x)$，然后计算凸部分的次梯度，这是 DCA 更新步骤的关键要素。这项练习将加深您在 DC 规划背景下对凸分析基础知识的理解。[@problem_id:3119906]", "problem": "考虑由下式定义的函数 $f:\\mathbb{R}^3 \\to \\mathbb{R}$\n$$\nf(x) \\;=\\; \\max_{i \\in \\{1,2,3\\}} \\, a_i^\\top x \\;-\\; \\mu \\,\\|x\\|_2^2,\n$$\n其中 $a_1=\\begin{pmatrix}1\\\\-1\\\\0\\end{pmatrix}$，$a_2=\\begin{pmatrix}0\\\\2\\\\-1\\end{pmatrix}$，$a_3=\\begin{pmatrix}-1\\\\0\\\\2\\end{pmatrix}$，并且 $\\mu0$ 是一个给定的标量 (取 $\\mu=2$)。目标是从凸函数差(DC)规划的角度分析该目标函数。\n\n任务：\n1. 仅使用凸分析和优化中的基本定义和事实，推导出一个凸函数差(DC)分解 $f(x)=g(x)-h(x)$，其中 $g$ 和 $h$ 都是 $\\mathbb{R}^3$ 上的凸函数。解释为什么每个分量都是凸的。\n2. 令 $x_0=\\begin{pmatrix}1\\\\1\\\\1\\end{pmatrix}$。刻画凸函数 $g$ 在 $x_0$ 处的次微分，并找出在 $x_0$ 处的所有活动仿射函数。然后，在 $\\partial g(x_0)$ 中的所有次梯度中，确定具有最小欧几里得范数的唯一子梯度，并将其以行向量的形式报告。\n\n你最终报告的答案应该是 $g$ 在 $x_0$ 处的最小欧几里得范数次梯度，以行向量形式表示，使用精确值。不需要四舍五入。", "solution": "该问题被评估为有效。它在凸优化领域，特别是凸函数差(DC)规划方面，具有科学依据。该问题是适定的、客观的和自洽的，提供了推导指定任务唯一解所需的所有数据和定义。\n\n要分析的函数是 $f:\\mathbb{R}^3 \\to \\mathbb{R}$，定义为\n$$\nf(x) \\;=\\; \\max_{i \\in \\{1,2,3\\}} \\, a_i^\\top x \\;-\\; \\mu \\,\\|x\\|_2^2\n$$\n其中 $a_1=\\begin{pmatrix}1\\\\-1\\\\0\\end{pmatrix}$，$a_2=\\begin{pmatrix}0\\\\2\\\\-1\\end{pmatrix}$，$a_3=\\begin{pmatrix}-1\\\\0\\\\2\\end{pmatrix}$，且 $\\mu=2$。\n\n任务1：推导一个凸函数差(DC)分解 $f(x)=g(x)-h(x)$。\n\n我们可以通过定义两个分量函数 $g(x)$ 和 $h(x)$ 来分解函数 $f(x)$。一个自然的选择是：\n$$\ng(x) = \\max_{i \\in \\{1,2,3\\}} \\, a_i^\\top x\n$$\n$$\nh(x) = \\mu \\,\\|x\\|_2^2 = 2 \\,\\|x\\|_2^2\n$$\n如此选择，显然有 $f(x) = g(x) - h(x)$。我们现在必须证明 $g(x)$ 和 $h(x)$ 都是 $\\mathbb{R}^3$ 上的凸函数。\n\n对于 $g(x)$：\n对每个索引 $i \\in \\{1, 2, 3\\}$，函数 $g_i(x) = a_i^\\top x$ 是 $\\mathbb{R}^3$ 上的一个线性泛函。任何线性函数也是仿射函数，且所有仿射函数都是凸的（也是凹的）。函数 $g(x)$ 被定义为这三个凸函数的逐点最大值，$g(x) = \\max\\{g_1(x), g_2(x), g_3(x)\\}$。凸分析中的一个基本性质指出，一组凸函数的逐点最大值本身也是一个凸函数。因此，$g(x)$ 是 $\\mathbb{R}^3$ 上的一个凸函数。\n\n对于 $h(x)$：\n函数 $h(x)$ 由 $h(x) = 2\\|x\\|_2^2$ 给出。在坐标中，对于 $x = \\begin{pmatrix}x_1  x_2  x_3\\end{pmatrix}^\\top \\in \\mathbb{R}^3$，这表示为 $h(x) = 2(x_1^2 + x_2^2 + x_3^2)$。这是一个关于 $x$ 的二次函数。一个二阶可微函数是凸函数的充要条件是其黑塞矩阵在其整个定义域上是半正定的。$h(x)$ 的梯度是：\n$$\n\\nabla h(x) = \\nabla \\left( 2(x_1^2 + x_2^2 + x_3^2) \\right) = 2 \\begin{pmatrix} 2x_1 \\\\ 2x_2 \\\\ 2x_3 \\end{pmatrix} = 4x\n$$\n$h(x)$ 的黑塞矩阵是其梯度的雅可比矩阵：\n$$\n\\nabla^2 h(x) = \\nabla(4x) = 4I_3 = \\begin{pmatrix} 4  0  0 \\\\ 0  4  0 \\\\ 0  0  4 \\end{pmatrix}\n$$\n其中 $I_3$ 是 $3 \\times 3$ 单位矩阵。该矩阵的特征值都等于 $4$，是严格正的。因此，黑塞矩阵 $\\nabla^2 h(x)$ 对于所有 $x \\in \\mathbb{R}^3$ 都是正定的。这证明了 $h(x)$ 是 $\\mathbb{R}^3$ 上的一个严格凸函数。\n\n因为 $g(x)$ 和 $h(x)$ 都是凸函数，所以分解 $f(x) = g(x) - h(x)$ 是一个有效的 DC 分解。\n\n任务2：刻画在 $x_0 = \\begin{pmatrix}1\\\\1\\\\1\\end{pmatrix}$ 处的 $\\partial g(x_0)$ 并找到最小范数次梯度。\n\n首先，我们必须确定在 $x_0$ 处的活动仿射函数。我们对每个 $i \\in \\{1, 2, 3\\}$ 计算 $a_i^\\top x_0$：\n$$\na_1^\\top x_0 = \\begin{pmatrix}1  -1  0\\end{pmatrix} \\begin{pmatrix}1\\\\1\\\\1\\end{pmatrix} = 1(1) - 1(1) + 0(1) = 0\n$$\n$$\na_2^\\top x_0 = \\begin{pmatrix}0  2  -1\\end{pmatrix} \\begin{pmatrix}1\\\\1\\\\1\\end{pmatrix} = 0(1) + 2(1) - 1(1) = 1\n$$\n$$\na_3^\\top x_0 = \\begin{pmatrix}-1  0  2\\end{pmatrix} \\begin{pmatrix}1\\\\1\\\\1\\end{pmatrix} = -1(1) + 0(1) + 2(1) = 1\n$$\n$g(x_0)$ 的值是这些值的最大值：$g(x_0) = \\max\\{0, 1, 1\\} = 1$。\n在 $x_0$ 处的活动索引集，记为 $I(x_0)$，由满足 $a_i^\\top x_0 = g(x_0)$ 的索引组成。在本例中，$I(x_0) = \\{2, 3\\}$。活动仿射函数是 $a_2^\\top x$ 和 $a_3^\\top x$。\n\n$g(x)$ 在点 $x_0$ 处的次微分由该点处活动函数的梯度（或次梯度）的凸包给出。由于函数 $g_i(x)=a_i^\\top x$ 是可微的，它们的次微分是包含其梯度的单点集，即 $\\partial g_i(x_0) = \\{\\nabla g_i(x_0)\\} = \\{a_i\\}$。因此，$g$ 在 $x_0$ 处的次微分是：\n$$\n\\partial g(x_0) = \\text{conv} \\left( \\bigcup_{i \\in I(x_0)} \\{a_i\\} \\right) = \\text{conv}\\{a_2, a_3\\}\n$$\n这是 $\\mathbb{R}^3$ 中连接向量 $a_2$ 和 $a_3$ 的线段。任何次梯度 $v \\in \\partial g(x_0)$ 都可以表示为 $a_2$ 和 $a_3$ 的凸组合：\n$$\nv = \\lambda a_2 + (1-\\lambda) a_3, \\quad \\text{for } \\lambda \\in [0, 1]\n$$\n代入向量 $a_2$ 和 $a_3$：\n$$\nv(\\lambda) = \\lambda \\begin{pmatrix}0\\\\2\\\\-1\\end{pmatrix} + (1-\\lambda) \\begin{pmatrix}-1\\\\0\\\\2\\end{pmatrix} = \\begin{pmatrix} -(1-\\lambda) \\\\ 2\\lambda \\\\ -\\lambda + 2(1-\\lambda) \\end{pmatrix} = \\begin{pmatrix} \\lambda-1 \\\\ 2\\lambda \\\\ 2-3\\lambda \\end{pmatrix}\n$$\n我们需要找到具有最小欧几里得范数的次梯度。这等价于在线段 $[a_2, a_3]$ 上找到离原点最近的向量 $v$。我们可以通过最小化关于 $\\lambda \\in [0, 1]$ 的欧几里得范数的平方 $\\|v(\\lambda)\\|_2^2$ 来实现这一点。\n$$\n\\|v(\\lambda)\\|_2^2 = (\\lambda-1)^2 + (2\\lambda)^2 + (2-3\\lambda)^2\n$$\n展开各项：\n$$\n\\|v(\\lambda)\\|_2^2 = (\\lambda^2 - 2\\lambda + 1) + 4\\lambda^2 + (4 - 12\\lambda + 9\\lambda^2) = 14\\lambda^2 - 14\\lambda + 5\n$$\n这是一个关于 $\\lambda$ 的二次函数，表示一个开口向上的抛物线。其最小值位于顶点处。我们通过将导数设为零来找到使之最小的 $\\lambda$：\n$$\n\\frac{d}{d\\lambda} (14\\lambda^2 - 14\\lambda + 5) = 28\\lambda - 14 = 0\n$$\n解出 $\\lambda$ 得到 $\\lambda = \\frac{14}{28} = \\frac{1}{2}$。\n由于该值 $\\lambda = \\frac{1}{2}$ 位于区间 $[0, 1]$ 内，因此它是真正的最小值点。\n\n我们将 $\\lambda = \\frac{1}{2}$ 代回 $v(\\lambda)$ 的表达式，以找到具有最小欧几里得范数的唯一子梯度：\n$$\nv_{\\min} = v\\left(\\frac{1}{2}\\right) = \\begin{pmatrix} \\frac{1}{2}-1 \\\\ 2\\left(\\frac{1}{2}\\right) \\\\ 2-3\\left(\\frac{1}{2}\\right) \\end{pmatrix} = \\begin{pmatrix} -\\frac{1}{2} \\\\ 1 \\\\ \\frac{4}{2}-\\frac{3}{2} \\end{pmatrix} = \\begin{pmatrix} -\\frac{1}{2} \\\\ 1 \\\\ \\frac{1}{2} \\end{pmatrix}\n$$\n题目要求将此向量以行向量的形式报告。", "answer": "$$\n\\boxed{\\begin{pmatrix} -\\frac{1}{2}  1  \\frac{1}{2} \\end{pmatrix}}\n$$", "id": "3119906"}, {"introduction": "在学会如何分解一个函数之后，下一步是理解 DCA 算法的实际执行过程。这个练习将引导您推导并实现 DCA 的单次迭代。通过对函数的凹部分进行线性化，您将构建一个凸子问题，并了解如何将其转化为一个标准的线性规划（LP）问题，从而在优化理论与计算实践之间架起一座桥梁。[@problem_id:3119896]", "problem": "考虑函数 $f(x)=\\max_{i\\in\\{1,\\dots,m\\}} a_i^\\top x - \\alpha\\|x\\|_2^2$，其中 $x\\in\\mathbb{R}^n$，$\\alpha0$，且对于 $i\\in\\{1,\\dots,m\\}$ 有 $a_i\\in\\mathbb{R}^n$。从凸性 (convexity) 和差分凸 (difference-of-convex, DC) 分解的基本定义出发，完成以下任务。\n\n1. 仅使用“仿射函数的逐点最大值是凸的”以及“范数平方的正标量倍数是凸的”这两个核心定义，推导 $f(x)$ 的一个显式 DC 分解，形式为 $f(x)=g(x)-h(x)$，其中 $g$ 和 $h$ 均为凸函数。\n\n2. 使用差分凸算法 (Difference-of-Convex Algorithm, DCA) 的定义，从第一性原理出发，推导单次迭代的更新规则。该规则在给定当前迭代点 $x^{(0)}\\in\\mathbb{R}^n$ 的情况下，通过在 $x^{(0)}$ 处线性化凸部分 $h(x)$ 并在一个紧凸集上最小化所得到的凸模型来计算 $x^{(1)}$。为确保子问题是适定的 (well-posed)，将最小化限制在轴对齐的盒子 $C=\\{x\\in\\mathbb{R}^n:\\ -R\\le x_j\\le R\\ \\text{for all}\\ j\\in\\{1,\\dots,n\\}\\}$ 内，其中给定 $R0$。证明 DCA 子问题\n$$x^{(1)}\\in\\arg\\min_{y\\in C}\\ \\ g(y)-\\langle\\nabla h(x^{(0)}),y\\rangle$$\n等价于以下变量为 $(y,t)\\in\\mathbb{R}^n\\times\\mathbb{R}$ 的线性规划问题：\n$$\\min_{y\\in\\mathbb{R}^n,\\ t\\in\\mathbb{R}}\\ \\ t\\quad\\text{subject to}\\quad (a_i-2\\alpha x^{(0)})^\\top y \\le t\\ \\ \\text{for all}\\ i\\in\\{1,\\dots,m\\},\\ \\ -R\\le y_j\\le R\\ \\ \\text{for all}\\ j\\in\\{1,\\dots,n\\}.$$\n\n3. 实现一个完整的、可运行的程序，该程序根据第 2 项中的推导执行恰好一次 DCA 迭代，求解相应的线性规划问题，并为下面指定的每个测试用例返回 $x^{(1)}$。该程序必须确定性地求解线性规划，并且不得需要任何用户输入。所有答案必须纯粹表示为无量纲的数字。您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果，每个案例的 $x^{(1)}$ 向量呈现为用方括号括起来的逗号分隔列表，并且每个标量都四舍五入到六位小数（例如，$[[0.123456,0.000000],[1.000000]]$）。\n\n测试套件：\n- 用例 1：$n=2$, $m=3$, $a_1=[1,0]$, $a_2=[0,1]$, $a_3=[-1,-1]$, $\\alpha=0.5$, $R=1.0$, $x^{(0)}=[0.5,-0.3]$。\n- 用例 2：$n=3$, $m=4$, $a_1=[1,2,0]$, $a_2=[-2,1,1]$, $a_3=[0,-1,1]$, $a_4=[1,1,1]$, $\\alpha=0.3$, $R=2.0$, $x^{(0)}=[0,0,0]$。\n- 用例 3：$n=1$, $m=2$, $a_1=[2]$, $a_2=[-3]$, $\\alpha=1.0$, $R=0.5$, $x^{(0)}=[0.4]$。\n- 用例 4：$n=2$, $m=2$, $a_1=[0.1,0.1]$, $a_2=[-0.1,0.2]$, $\\alpha=0.01$, $R=0.1$, $x^{(0)}=[0.0,0.0]$。\n\n您的程序必须输出单行，包含 $[x^{(1)}_{\\text{用例 1}},x^{(1)}_{\\text{用例 2}},x^{(1)}_{\\text{用例 3}},x^{(1)}_{\\text{用例 4}}]$，其中每个 $x^{(1)}_{\\text{用例 k}}$ 是一个浮点数列表，四舍五入到六位小数，逗号后没有空格。", "solution": "该问题要求分三部分作答：推导给定函数的差分凸 (DC) 分解，推导相应的差分凸算法 (DCA) 子问题及其与线性规划 (LP) 的等价性，以及实现一个为一组测试用例求解此 LP 的程序。\n\n### 第 1 部分：DC 分解的推导\n\n待分析的函数是 $f(x) = \\max_{i \\in \\{1, \\dots, m\\}} a_i^\\top x - \\alpha\\|x\\|_2^2$，其中 $x \\in \\mathbb{R}^n$，$a_i \\in \\mathbb{R}^n$ 且 $\\alpha  0$。DC 分解将 $f(x)$ 表示为两个凸函数的差，即 $f(x) = g(x) - h(x)$。\n\n我们遵循凸分析的两个基本原则：\n1.  一组凸函数的逐点最大值本身也是一个凸函数。\n2.  一个非负标量与一个凸函数的乘积是一个凸函数。\n\n我们将函数 $f(x)$ 分解为两部分。\n第一部分是 $\\max_{i \\in \\{1, \\dots, m\\}} a_i^\\top x$。对于每个 $i \\in \\{1, \\dots, m\\}$，函数 $l_i(x) = a_i^\\top x$ 是一个关于 $x$ 的仿射（因此也是凸）函数。根据第一个原则，这些仿射函数的逐点最大值是凸的。我们将这部分定义为 $g(x)$：\n$$g(x) = \\max_{i \\in \\{1, \\dots, m\\}} a_i^\\top x$$\n根据定义，$g(x)$ 是一个凸函数。\n\n$f(x)$ 的第二部分是 $-\\alpha\\|x\\|_2^2$。这是函数 $\\alpha\\|x\\|_2^2$ 的负值。平方欧几里得范数 $\\|x\\|_2^2$ 是一个众所周知的凸函数。根据第二个原则，由于 $\\alpha  0$，函数 $h(x) = \\alpha\\|x\\|_2^2$ 也是一个凸函数。\n\n有了这些定义，原始函数 $f(x)$ 可以写成：\n$$f(x) = g(x) - h(x) = \\left( \\max_{i \\in \\{1, \\dots, m\\}} a_i^\\top x \\right) - \\left( \\alpha\\|x\\|_2^2 \\right)$$\n这为 $f(x)$ 提供了一个显式的 DC 分解，其中 $g(x) = \\max_{i \\in \\{1, \\dots, m\\}} a_i^\\top x$ 和 $h(x) = \\alpha\\|x\\|_2^2$ 均为凸函数。\n\n### 第 2 部分：DCA 子问题及 LP 等价性的推导\n\nDCA 是一种用于最小化 DC 函数 $f(x) = g(x) - h(x)$ 的迭代方法。在每次迭代 $k$ 中，给定当前点 $x^{(k)}$，该算法通过用函数 $h(x)$ 在 $x^{(k)}$ 附近的一阶泰勒近似来替换它，从而构造一个 $f(x)$ 的凸模型。下一个迭代点 $x^{(k+1)}$ 通过最小化这个凸模型得到。\n\nDCA 的一般单步更新规则由下式给出：\n$$x^{(k+1)} \\in \\arg\\min_{y} \\left( g(y) - (h(x^{(k)}) + \\langle \\nabla h(x^{(k)}), y - x^{(k)} \\rangle) \\right)$$\n由于 $h(x^{(k)})$ 和 $\\langle \\nabla h(x^{(k)}), x^{(k)} \\rangle$ 是相对于优化变量 $y$ 的常数项，最小化上述表达式等价于最小化：\n$$x^{(k+1)} \\in \\arg\\min_{y} \\left( g(y) - \\langle \\nabla h(x^{(k)}), y \\rangle \\right)$$\n问题规定，此最小化应在紧凸集 $C = \\{x \\in \\mathbb{R}^n : -R \\le x_j \\le R \\text{ for all } j \\in \\{1, \\dots, n\\}\\}$ 上进行。从初始点 $x^{(0)}$ 开始，第一次迭代就是通过求解以下问题来找到 $x^{(1)}$：\n$$x^{(1)} \\in \\arg\\min_{y \\in C} \\left( g(y) - \\langle \\nabla h(x^{(0)}), y \\rangle \\right)$$\n从第 1 部分可知，$g(y) = \\max_{i \\in \\{1, \\dots, m\\}} a_i^\\top y$ 且 $h(x) = \\alpha\\|x\\|_2^2 = \\alpha \\sum_{j=1}^n x_j^2$。$h(x)$ 的梯度是 $\\nabla h(x) = 2\\alpha x$。在点 $x^{(0)}$ 处，梯度为 $\\nabla h(x^{(0)}) = 2\\alpha x^{(0)}$。\n\n将这些代入子问题的目标函数中得到：\n$$ g(y) - \\langle \\nabla h(x^{(0)}), y \\rangle = \\max_{i \\in \\{1, \\dots, m\\}} a_i^\\top y - \\langle 2\\alpha x^{(0)}, y \\rangle $$\n利用内积的线性性质，我们可以将最大值内的项合并：\n$$ \\max_{i \\in \\{1, \\dots, m\\}} a_i^\\top y - (2\\alpha x^{(0)})^\\top y = \\max_{i \\in \\{1, \\dots, m\\}} (a_i - 2\\alpha x^{(0)})^\\top y $$\n因此，DCA 子问题是：\n$$ x^{(1)} \\in \\arg\\min_{y \\in C} \\left( \\max_{i \\in \\{1, \\dots, m\\}} (a_i - 2\\alpha x^{(0)})^\\top y \\right) $$\n这是一个极小化极大问题 (minimax problem)，可以重构为一个线性规划 (LP)。我们引入一个辅助标量变量 $t \\in \\mathbb{R}$。如果我们最小化 $t$ 并满足条件 $t \\ge \\max_i (\\dots)$，就可以最小化目标 $\\max_i (\\dots)$。这等价于以下线性不等式组：\n$$ t \\ge (a_i - 2\\alpha x^{(0)})^\\top y \\quad \\text{for all } i \\in \\{1, \\dots, m\\} $$\n将定义域约束 $y \\in C$ 包含进来，完整的优化问题就变成了一个关于变量 $(y, t) \\in \\mathbb{R}^n \\times \\mathbb{R}$ 的 LP：\n$$ \\min_{y \\in \\mathbb{R}^n, t \\in \\mathbb{R}} \\quad t $$\n$$ \\text{subject to:} $$\n$$ (a_i - 2\\alpha x^{(0)})^\\top y \\le t \\quad \\text{for all } i \\in \\{1, \\dots, m\\} $$\n$$ -R \\le y_j \\le R \\quad \\text{for all } j \\in \\{1, \\dots, n\\} $$\n这个 LP 公式与问题陈述中提供的公式完全相同。从此 LP 得到的最优向量 $y$ 就是解 $x^{(1)}$。\n\n### 第 3 部分：实现\n\n所提供的 Python 代码为每个测试用例实现了上述推导出的 LP 的求解。它使用了 `scipy.optimize.linprog` 函数，这是一个用于线性规划的标准数值求解器。对于每个测试用例，代码首先构造 LP 的参数：成本向量 `c`、不等式约束矩阵 `A_ub`、不等式约束上界 `b_ub` 以及变量边界。然后调用 `linprog` 并提取最优向量 $y$，它对应于 $x^{(1)}$。最终结果按要求格式化并打印。", "answer": "```python\nimport numpy as np\nfrom scipy.optimize import linprog\n\ndef solve():\n    \"\"\"\n    Solves for the first DCA iterate x^(1) for a series of test cases.\n    Each subproblem is a linear program derived from the DCA framework.\n    \"\"\"\n    \n    # Test Suite:\n    # Each tuple contains (n, m, a_vectors, alpha, R, x0)\n    test_cases = [\n        (2, 3, np.array([[1, 0], [0, 1], [-1, -1]]), 0.5, 1.0, np.array([0.5, -0.3])),\n        (3, 4, np.array([[1, 2, 0], [-2, 1, 1], [0, -1, 1], [1, 1, 1]]), 0.3, 2.0, np.array([0, 0, 0])),\n        (1, 2, np.array([[2], [-3]]), 1.0, 0.5, np.array([0.4])),\n        (2, 2, np.array([[0.1, 0.1], [-0.1, 0.2]]), 0.01, 0.1, np.array([0.0, 0.0]))\n    ]\n\n    results_vectors = []\n\n    for case in test_cases:\n        n, m, a_vectors, alpha, R, x0 = case\n\n        # The optimization variable for linprog is z = [y_1, ..., y_n, t].\n        # The size of z is n + 1.\n\n        # 1. Define the objective function vector 'c'.\n        # We want to minimize t, so c is [0, ..., 0, 1].\n        c = np.zeros(n + 1)\n        c[n] = 1.0\n\n        # 2. Define the inequality constraints A_ub * z = b_ub.\n        # These correspond to (a_i - 2*alpha*x0)^T * y - t = 0.\n        \n        # Calculate the vectors b_i = a_i - 2*alpha*x0\n        grad_h_x0 = 2 * alpha * x0\n        B = a_vectors - grad_h_x0\n        \n        # A_ub is an m x (n+1) matrix.\n        # The i-th row is [b_i^T, -1].\n        A_ub = np.hstack([B, -np.ones((m, 1))])\n        \n        # b_ub is a vector of zeros of size m.\n        b_ub = np.zeros(m)\n\n        # 3. Define the bounds for each variable in z.\n        # -R = y_j = R for j=1..n\n        # t is unbounded.\n        bounds = [(-R, R)] * n + [(None, None)]\n\n        # 4. Solve the linear program.\n        # The 'highs' method is deterministic and robust.\n        res = linprog(c, A_ub=A_ub, b_ub=b_ub, bounds=bounds, method='highs')\n        \n        # 5. Extract the solution for y (which is x^(1)).\n        # The solution vector res.x contains [y_1, ..., y_n, t].\n        # We need the first n components.\n        x1 = res.x[:n]\n        \n        results_vectors.append(x1)\n\n    # Format the output as specified: [[r1_1,r1_2,...],[r2_1,r2_2,...],...]\n    # with each number rounded to six decimal places.\n    formatted_results = []\n    for vec in results_vectors:\n        formatted_vec = \",\".join([f\"{v:.6f}\" for v in vec])\n        formatted_results.append(f\"[{formatted_vec}]\")\n    \n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3119896"}, {"introduction": "现在，我们将把所学知识应用于一个来自机器学习领域的完整且实际的问题：稀疏图学习。这个综合性练习涉及将带有截断 $\\ell_1$ 惩罚项的问题表述为一个 DC 规划，实现完整的 DCA 循环直至收敛，并处理如图连通性这样的实际约束。这项实践展示了 DC 规划在解决现代数据科学中出现的复杂非凸问题方面的强大能力。[@problem_id:3119825]", "problem": "您的任务是使用差分凸 (Difference-of-Convex, DC) 框架和差分凸算法 (Difference-of-Convex Algorithm, DCA) 来构建并求解一个带约束 $\\ell_1$ 惩罚的图学习问题。目标是在学习到的边权重中引入稀疏性，同时保持图的连通性。您必须编写一个完整的、可运行的程序，该程序能在指定的测试用例上执行 DCA，并按如下所述的精确格式输出聚合指标。\n\n使用的基本原理：\n- 如果一个函数 $F$ 可以写成 $F(\\mathbf{x}) = G(\\mathbf{x}) - H(\\mathbf{x})$ 的形式，其中 $G$ 和 $H$ 都是凸函数，则称该函数为差分凸 (DC) 函数。\n- 差分凸算法 (DCA) 通过在 $\\mathbf{w}^k$ 处对凹部进行线性化，迭代求解一个凸子问题来构建序列 $\\{\\mathbf{w}^k\\}$，即 $\\mathbf{w}^{k+1} \\in \\arg\\min_{\\mathbf{w} \\ge \\mathbf{l}} \\left\\{ G(\\mathbf{w}) - \\langle \\mathbf{s}^k, \\mathbf{w} \\rangle \\right\\}$，其中 $\\mathbf{s}^k \\in \\partial H(\\mathbf{w}^k)$ 且 $\\partial H$ 表示 $H$ 的次微分。\n- 对于标量 $x$，参数为 $\\lambda  0$ 和 $\\theta  0$ 的带约束 $\\ell_1$ 惩罚定义为 $p_{\\lambda,\\theta}(x) = \\min\\{\\lambda |x|, \\theta\\}$，它是一个非凸函数。\n- 通过对一个固定的生成树（例如，最小生成树）中所有边的权重强制施加严格为正的下界，可以保证一个包含 $n$ 个节点的连通无向图保持连通，因为一个权重严格为正的生成树本身构成一个连通子图。\n\n问题设置：\n- 考虑一个无向图，它有 $n$ 个节点，索引为 $i \\in \\{1,\\dots,n\\}$，嵌入在给定坐标的 $\\mathbb{R}^2$ 中。设无向边的集合为所有满足 $1 \\le i  j \\le n$ 的节点对 $(i,j)$，边权重向量为 $\\mathbf{w} \\in \\mathbb{R}^m$，其中 $m = n(n-1)/2$，且对所有边满足 $w_{ij} \\ge 0$。\n- 设节点 $i$ 和 $j$ 之间的基准亲和度为 $w^0_{ij} = \\exp\\left(-\\frac{\\| \\mathbf{x}_i - \\mathbf{x}_j \\|_2^2}{\\sigma^2}\\right)$，其中给定了 $\\sigma  0$，$\\mathbf{x}_i \\in \\mathbb{R}^2$ 是节点 $i$ 的坐标。\n- 学习目标是估计一个与 $\\mathbf{w}^0$ 相近的 $\\mathbf{w}$，同时通过带约束 $\\ell_1$ 正则化来促进稀疏性。该优化问题为：\n$$\n\\min_{\\mathbf{w} \\in \\mathbb{R}^m} \\; F(\\mathbf{w}) = \\frac{1}{2} \\| \\mathbf{w} - \\mathbf{w}^0 \\|_2^2 + \\sum_{e=1}^m p_{\\lambda,\\theta}(w_e)\n\\quad \\text{subject to} \\quad \\mathbf{w} \\ge \\mathbf{l},\n$$\n其中不等式是逐元素的，$\\mathbf{l} \\in \\mathbb{R}^m_{\\ge 0}$ 是一个下界向量。为保持连通性，对于一个固定的生成树 $T$ 中的所有边 $e$，$\\mathbf{l}$ 必须强制 $w_e \\ge \\tau$（其中 $\\tau  0$ 已指定），而对于所有其他边，则强制 $w_e \\ge 0$。\n- 您必须将带约束 $\\ell_1$ 惩罚表示为一个 DC 函数（即，确定凸函数 $g$ 和 $h$ 使得 $p_{\\lambda,\\theta}(x) = g(x) - h(x)$），并应用 DCA。在每次 DCA 迭代中，您需要：\n    1. 计算一个次梯度 $\\mathbf{s}^k \\in \\partial H(\\mathbf{w}^k)$。\n    2. 求解凸子问题以生成 $\\mathbf{w}^{k+1}$：\n    $$\n    \\mathbf{w}^{k+1} \\in \\arg\\min_{\\mathbf{w} \\ge \\mathbf{l}} \\left\\{ \\frac{1}{2} \\| \\mathbf{w} - \\mathbf{w}^0 \\|_2^2 + \\sum_{e=1}^m g(w_e) - \\langle \\mathbf{s}^k, \\mathbf{w} \\rangle \\right\\}.\n    $$\n- 在所述的目标函数和箱式约束下，该凸子问题在各个坐标上是可分的。您必须为这个凸子问题实现一个精确的逐坐标求解器，同时满足非负性和生成树下界约束。\n\n实现要求：\n- 实现 DCA，并设置一个基于 $\\mathbf{w}$ 相对变化的停止准则，即当 $\\frac{\\| \\mathbf{w}^{k+1} - \\mathbf{w}^{k} \\|_2}{\\max\\{1, \\| \\mathbf{w}^{k} \\|_2\\}} \\le \\varepsilon$ 或达到最大迭代次数时停止，其中 $\\varepsilon  0$ 已指定。\n- 将生成树 $T$ 构建为完全图的最小生成树，使用节点间的欧几里得距离作为边成本。对所有 $e \\in T$ 强制 $w_e \\ge \\tau$，对其他边强制 $w_e \\ge 0$。\n- 收敛后，构建学习到的图的无向邻接矩阵，并使用图遍历算法验证其连通性，其中边的权重严格大于 $0$ 时才认为该边存在。\n\n测试套件：\n对于所有测试用例，参数包括节点坐标、$\\sigma$、$\\lambda$、$\\theta$、$\\tau$、最大迭代次数 $K_{\\max}$ 和容差 $\\varepsilon$。您的程序必须执行以下三个测试用例，并按规定输出结果。\n\n- 测试用例 1 (理想路径):\n    - 节点数 $n = 5$，坐标为：\n      $\\mathbf{x}_1 = (0,0)$, $\\mathbf{x}_2 = (1,0)$, $\\mathbf{x}_3 = (1,1)$, $\\mathbf{x}_4 = (0,1)$, $\\mathbf{x}_5 = (0.5,0.2)$。\n    - 参数：$\\sigma = 0.8$, $\\lambda = 0.3$, $\\theta = 0.15$, $\\tau = 0.05$, $K_{\\max} = 50$, $\\varepsilon = 10^{-8}$。\n- 测试用例 2 (强稀疏性压力和共线几何):\n    - 节点数 $n = 4$，坐标为：\n      $\\mathbf{x}_1 = (0,0)$, $\\mathbf{x}_2 = (0.5,0)$, $\\mathbf{x}_3 = (1.0,0)$, $\\mathbf{x}_4 = (1.5,0)$。\n    - 参数：$\\sigma = 0.5$, $\\lambda = 0.5$, $\\theta = 0.05$, $\\tau = 0.05$, $K_{\\max} = 50$, $\\varepsilon = 10^{-8}$。\n- 测试用例 3 (因上限值较大导致的弱稀疏性):\n    - 节点数 $n = 6$，坐标为：\n      $\\mathbf{x}_1 = (0,0)$, $\\mathbf{x}_2 = (0.3,0.1)$, $\\mathbf{x}_3 = (0.6,0)$, $\\mathbf{x}_4 = (0.0,0.6)$, $\\mathbf{x}_5 = (0.6,0.6)$, $\\mathbf{x}_6 = (0.3,0.3)$。\n    - 参数：$\\sigma = 1.0$, $\\lambda = 0.2$, $\\theta = 0.5$, $\\tau = 0.03$, $K_{\\max} = 50$, $\\varepsilon = 10^{-8}$。\n\n输出规范：\n- 对于每个测试用例，计算并返回一个包含三个条目的列表：$[c, s, b]$，其中 $c$ 是学习到的权重严格大于 $0$ 的边的整数数量，$s$ 是所有学习到的边权重的浮点数总和，$b$ 是一个布尔值，表示学习到的图是否连通（连通则为 $\\text{True}$；否则为 $\\text{False}$）。\n- 您的程序应生成单行输出，其中包含测试用例 1、2 和 3 的三个结果，格式为用方括号括起来的逗号分隔列表，即 $[[c_1,s_1,b_1],[c_2,s_2,b_2],[c_3,s_3,b_3]]$。", "solution": "用户提供的问题是图学习领域一个适定的优化任务，适合使用差分凸算法 (DCA) 解决。该问题具有科学依据，内容自洽，且所有参数都已明确指定。所要求的方法是非凸优化领域的标准方法。下面我将提供一个完整的解决方案。\n\n问题的核心是求解以下优化问题：\n$$\n\\min_{\\mathbf{w} \\in \\mathbb{R}^m} \\; F(\\mathbf{w}) = \\frac{1}{2} \\| \\mathbf{w} - \\mathbf{w}^0 \\|_2^2 + \\sum_{e=1}^m p_{\\lambda,\\theta}(w_e)\n\\quad \\text{subject to} \\quad \\mathbf{w} \\ge \\mathbf{l}\n$$\n其中 $p_{\\lambda,\\theta}(x) = \\min\\{\\lambda |x|, \\theta\\}$ 是带约束 $\\ell_1$ 惩罚，约束 $\\mathbf{w} \\ge \\mathbf{l}$ 确保了非负性并强制保持连通性，对于预先计算的生成树中的边 $e$，有 $l_e \\ge \\tau  0$，而对于其他边，有 $l_e \\ge 0$。由于约束强制 $w_e \\ge 0$，惩罚项可简化为 $p_{\\lambda,\\theta}(w_e) = \\min\\{\\lambda w_e, \\theta\\}$。\n\n### 1. 差分凸 (DC) 公式\n\nDCA 要求目标函数表示为两个凸函数的差。目标函数 $F(\\mathbf{w})$ 是一个凸二次项和一系列非凸带约束 $\\ell_1$ 惩罚项之和。我们可以将整个目标函数构建为一个 DC 函数。\n\n对于非负变量 $x \\ge 0$，带约束 $\\ell_1$ 惩罚可以写成：\n$$\np_{\\lambda,\\theta}(x) = \\min\\{\\lambda x, \\theta\\}\n$$\n这是一个标准的非凸函数，可以分解为两个凸函数的差。一个常见的分解是：\n$$\np_{\\lambda,\\theta}(x) = \\lambda x - \\max\\{0, \\lambda x - \\theta\\}\n$$\n我们定义 $g(x) = \\lambda x$ 和 $h(x) = \\max\\{0, \\lambda x - \\theta\\}$。\n- $g(x)$ 是一个线性函数，因此是凸函数。\n- $h(x)$ 是两个凸函数（$0$ 和 $\\lambda x - \\theta$）的最大值，因此也是凸函数。\n因此，$p_{\\lambda,\\theta}(x) = g(x) - h(x)$ 是一个有效的 DC 分解。\n\n现在，我们可以将整个目标函数 $F(\\mathbf{w})$ 改写为 $G(\\mathbf{w}) - H(\\mathbf{w})$ 的形式：\n$$\nF(\\mathbf{w}) = \\left( \\frac{1}{2} \\| \\mathbf{w} - \\mathbf{w}^0 \\|_2^2 + \\sum_{e=1}^m g(w_e) \\right) - \\sum_{e=1}^m h(w_e)\n$$\n我们定义两个凸分量：\n1.  $G(\\mathbf{w}) = \\frac{1}{2} \\| \\mathbf{w} - \\mathbf{w}^0 \\|_2^2 + \\sum_{e=1}^m \\lambda w_e = \\frac{1}{2} \\| \\mathbf{w} - \\mathbf{w}^0 \\|_2^2 + \\lambda \\mathbf{1}^T \\mathbf{w}$。该函数是一个严格凸二次函数和一个线性函数之和，因此是严格凸的。\n2.  $H(\\mathbf{w}) = \\sum_{e=1}^m \\max\\{0, \\lambda w_e - \\theta\\}$。该函数是凸函数之和，因此是凸的。\n\n### 2. 差分凸算法 (DCA)\n\nDCA 是一种迭代算法，在每一步 $k$，它围绕当前迭代点 $\\mathbf{w}^k$ 对凹部 $-H(\\mathbf{w})$ 进行线性化，并求解得到的凸子问题。更新规则为：\n$$\n\\mathbf{w}^{k+1} \\in \\arg\\min_{\\mathbf{w} \\ge \\mathbf{l}} \\left\\{ G(\\mathbf{w}) - \\langle \\mathbf{s}^k, \\mathbf{w} \\rangle \\right\\}\n$$\n其中 $\\mathbf{s}^k$ 是 $H$ 在 $\\mathbf{w}^k$ 处的一个次梯度，即 $\\mathbf{s}^k \\in \\partial H(\\mathbf{w}^k)$。\n\n**次梯度计算：**\n由于 $H(\\mathbf{w}) = \\sum_{e=1}^m h(w_e)$，其此微分是其各分量次微分的笛卡尔积。我们逐分量计算次梯度 $\\mathbf{s}^k$：$s_e^k \\in \\partial h(w_e^k)$。$h(x) = \\max\\{0, \\lambda x - \\theta\\}$ 的次微分为：\n$$\n\\partial h(x) = \\begin{cases} \\{0\\}  \\text{if } \\lambda x - \\theta  0 \\iff x  \\theta/\\lambda \\\\ [0, \\lambda]  \\text{if } \\lambda x - \\theta = 0 \\iff x = \\theta/\\lambda \\\\ \\{\\lambda\\}  \\text{if } \\lambda x - \\theta > 0 \\iff x > \\theta/\\lambda \\end{cases}\n$$\n为了获得一个特定的次梯度向量 $\\mathbf{s}^k$，我们可以在不可微点处做出确定性的选择。一个常见的选择是取区间的端点之一，例如 $\\lambda$。因此我们设定：\n$$\ns_e^k = \\begin{cases} 0  \\text{if } w_e^k  \\theta/\\lambda \\\\ \\lambda  \\text{if } w_e^k \\ge \\theta/\\lambda \\end{cases}\n$$\n\n**求解凸子问题：**\n关于 $\\mathbf{w}^{k+1}$ 的子问题是：\n$$\n\\mathbf{w}^{k+1} = \\arg\\min_{\\mathbf{w} \\ge \\mathbf{l}} \\left\\{ \\frac{1}{2} \\| \\mathbf{w} - \\mathbf{w}^0 \\|_2^2 + \\lambda \\sum_{e=1}^m w_e - \\sum_{e=1}^m s_e^k w_e \\right\\}\n$$\n这个优化问题是可分的，意味着我们可以独立求解每个分量 $w_e$：\n$$\nw_e^{k+1} = \\arg\\min_{w_e \\ge l_e} \\left\\{ \\frac{1}{2} (w_e - w_e^0)^2 + (\\lambda - s_e^k) w_e \\right\\}\n$$\n这是一个带下界约束的一维二次最小化问题。无约束最小化解 $w_e^*$ 可通过将目标函数对 $w_e$ 的导数设为零来求得：\n$$\n(w_e - w_e^0) + (\\lambda - s_e^k) = 0 \\implies w_e^* = w_e^0 - (\\lambda - s_e^k)\n$$\n约束问题的解是 $w_e^*$ 在可行集 $[l_e, \\infty)$ 上的投影：\n$$\nw_e^{k+1} = \\max\\{l_e, w_e^*\\} = \\max\\{l_e, w_e^0 - (\\lambda - s_e^k)\\}\n$$\n代入 $s_e^k$ 的表达式，我们得到每个分量的闭式更新规则：\n- 如果 $w_e^k  \\theta/\\lambda$，则 $s_e^k=0$，且 $w_e^{k+1} = \\max\\{l_e, w_e^0 - \\lambda\\}$。\n- 如果 $w_e^k \\ge \\theta/\\lambda$，则 $s_e^k=\\lambda$，且 $w_e^{k+1} = \\max\\{l_e, w_e^0\\}$。\n\n### 3. 算法流程\n\n每个测试用例的完整算法如下：\n\n1.  **初始化**：\n    a. 给定节点坐标 $\\{\\mathbf{x}_i\\}$，计算成对欧几里得距离矩阵。\n    b. 构建完全图，并使用距离作为边成本计算其最小生成树 (MST)。\n    c. 定义下界向量 $\\mathbf{l} \\in \\mathbb{R}^m$：对于每个边 $e$，如果 $e$ 在 MST 中，则设 $l_e = \\tau$，否则设 $l_e = 0$。\n    d. 计算基准亲和度向量 $\\mathbf{w}^0$，其中 $w^0_{ij} = \\exp(-\\|\\mathbf{x}_i - \\mathbf{x}_j\\|_2^2 / \\sigma^2)$。\n    e. 初始化权重向量 $\\mathbf{w}^0_{\\text{iter}} = \\max\\{\\mathbf{w}^0, \\mathbf{l}\\}$ 以确保起始点可行。为简单起见，我们将迭代序列表示为 $\\mathbf{w}^k$。\n\n2.  **DCA 迭代**：对于 $k=0, 1, ..., K_{\\max}-1$：\n    a. 使用上面推导的逐分量更新规则计算下一个迭代点 $\\mathbf{w}^{k+1}$。\n    b. 检查停止准则：如果 $\\frac{\\|\\mathbf{w}^{k+1} - \\mathbf{w}^k\\|_2}{\\max\\{1, \\|\\mathbf{w}^k\\|_2\\}} \\le \\varepsilon$，则终止循环。\n\n3.  **结果分析**：\n    a. 设最终权重向量为 $\\mathbf{w}^*$。\n    b. 统计权重严格为正的边的数量，$c = |\\{e \\mid w^*_e > 0\\}|$。\n    c. 计算所有学习到的权重的总和，$s = \\sum_e w^*_e$。\n    d. 验证连通性：为图构建一个邻接矩阵，其中当边的权重 $w^*_e > 0$ 时，边存在。使用图遍历算法（如 BFS 或 DFS）或连通分量算法来确定图是否连通。这将得到布尔值 $b$。由于 MST 边的权重至少为 $\\tau>0$，该构造保证了 $b=\\text{True}$。\n\n该流程已为提供的测试用例实现，以生成所需的输出。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.spatial.distance import pdist, squareform\nfrom scipy.sparse.csgraph import minimum_spanning_tree, connected_components\n\ndef run_dca_for_case(node_coords, sigma, lam, theta, tau, K_max, eps):\n    \"\"\"\n    Solves the graph learning problem for a single test case using DCA.\n    \"\"\"\n    n = len(node_coords)\n    m = n * (n - 1) // 2\n\n    # Generate an ordered list of edges (i, j) with i  j\n    edges = []\n    edge_map = {}\n    idx = 0\n    for i in range(n):\n        for j in range(i + 1, n):\n            edges.append((i, j))\n            edge_map[(i, j)] = idx\n            idx += 1\n\n    # 1. Pre-computation\n    # 1a. Compute baseline affinity vector w0\n    coords = np.array(node_coords)\n    w0 = np.zeros(m)\n    for i in range(m):\n        u, v = edges[i]\n        dist_sq = np.sum((coords[u] - coords[v])**2)\n        w0[i] = np.exp(-dist_sq / sigma**2)\n\n    # 1b. Compute MST to define lower bounds l\n    dist_matrix = squareform(pdist(coords))\n    mst_sparse = minimum_spanning_tree(dist_matrix)\n    mst_rows, mst_cols = mst_sparse.nonzero()\n    mst_edges = set(zip(np.minimum(mst_rows, mst_cols), np.maximum(mst_rows, mst_cols)))\n\n    l_vec = np.zeros(m)\n    for i in range(m):\n        u, v = edges[i]\n        if (u, v) in mst_edges:\n            l_vec[i] = tau\n\n    # 2. DCA Iteration\n    # 2a. Initialize w\n    w_k = np.maximum(w0, l_vec) # Start with a feasible point\n    \n    threshold = theta / lam\n\n    for k in range(K_max):\n        w_prev = w_k.copy()\n\n        # Compute subgradient s_k based on w_prev\n        # s_k = np.where(w_prev  threshold, 0.0, lam)\n\n        # Solve subproblem for w_{k+1}\n        # w_unc = w0 - (lam - s_k)\n        # w_k = np.maximum(l_vec, w_unc)\n        \n        # A more direct way to write the update:\n        # if w_prev[e]  threshold -> s_k[e]=0 -> w_k[e] = max(l_vec[e], w0[e] - lam)\n        # if w_prev[e] >= threshold -> s_k[e]=lam -> w_k[e] = max(l_vec[e], w0[e])\n        \n        w_k_case1 = np.maximum(l_vec, w0 - lam)\n        w_k_case2 = np.maximum(l_vec, w0)\n        \n        w_k = np.where(w_prev  threshold, w_k_case1, w_k_case2)\n\n        # Check stopping criterion\n        norm_w_prev = np.linalg.norm(w_prev)\n        rel_change = np.linalg.norm(w_k - w_prev) / max(1.0, norm_w_prev)\n        \n        if rel_change = eps:\n            break\n\n    w_final = w_k\n\n    # 3. Post-processing and Result Analysis\n    # 3a. Count edges with weight > 0\n    c = np.sum(w_final > 1e-9) # Use a small tolerance for floating point comparison\n\n    # 3b. Sum of all weights\n    s = np.sum(w_final)\n\n    # 3c. Check connectivity\n    adj_matrix = np.zeros((n, n))\n    for i in range(m):\n        if w_final[i] > 1e-9:\n            u, v = edges[i]\n            adj_matrix[u, v] = 1\n            adj_matrix[v, u] = 1\n    \n    n_components, _ = connected_components(adj_matrix, directed=False)\n    b = n_components == 1\n\n    return [int(c), float(s), bool(b)]\n\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the final result.\n    \"\"\"\n    test_cases = [\n        {\n            \"node_coords\": [(0, 0), (1, 0), (1, 1), (0, 1), (0.5, 0.2)],\n            \"sigma\": 0.8, \"lam\": 0.3, \"theta\": 0.15, \"tau\": 0.05,\n            \"K_max\": 50, \"eps\": 1e-8\n        },\n        {\n            \"node_coords\": [(0, 0), (0.5, 0), (1.0, 0), (1.5, 0)],\n            \"sigma\": 0.5, \"lam\": 0.5, \"theta\": 0.05, \"tau\": 0.05,\n            \"K_max\": 50, \"eps\": 1e-8\n        },\n        {\n            \"node_coords\": [(0, 0), (0.3, 0.1), (0.6, 0), (0.0, 0.6), (0.6, 0.6), (0.3, 0.3)],\n            \"sigma\": 1.0, \"lam\": 0.2, \"theta\": 0.5, \"tau\": 0.03,\n            \"K_max\": 50, \"eps\": 1e-8\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        result = run_dca_for_case(**case)\n        results.append(result)\n\n    # Convert results to the specified string format\n    results_str = []\n    for res in results:\n        res_str = f\"[{res[0]},{res[1]:.6f},{str(res[2])}]\"\n        results_str.append(res_str)\n        \n    print(f\"[{','.join(results_str)}]\")\n\nsolve()\n```", "id": "3119825"}]}