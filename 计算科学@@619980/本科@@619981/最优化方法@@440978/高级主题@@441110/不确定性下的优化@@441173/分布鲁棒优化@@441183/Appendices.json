{"hands_on_practices": [{"introduction": "机器学习中的一个核心挑战是过拟合：模型在训练数据上表现完美，但在未见过的新数据上表现不佳。分布鲁棒优化（Distributionally Robust Optimization, DRO）为提高模型的泛化能力提供了一种强大的范式。本练习将指导您通过一个具体的编码实例，直观地比较易于过拟合的样本均值近似（Sample Average Approximation, SAA）方法与基于Wasserstein距离的DRO方法 [@problem_id:3121634]。通过这个实践，您将亲眼见证DRO如何通过考虑经验数据周围的一个分布“球”，生成更鲁棒的解，从而在存在异常值时获得更好的样本外性能。", "problem": "我们提供一个玩具实例，以研究样本平均近似 (SAA) 中的过拟合现象，以及通过 Wasserstein 分布鲁棒优化 (DRO) 获得的泛化改进。考虑一个标量决策变量 $x \\in \\mathbb{R}$ 和一个标量不确定参数 $\\xi \\in \\mathbb{R}$。损失定义为 $f(x,\\xi) = \\tfrac{1}{2} x^2 - x \\xi$，对于所有 $\\xi$，该函数关于 $x$ 都是凸的。$\\xi$ 的真实数据生成分布是一个混合分布：以 0.9 的概率从正态分布 $\\mathcal{N}(0,1)$ 中抽取 $\\xi$，并以 0.1 的概率将 $\\xi$ 设为常数 $10$。构建的训练数据集包含 $20$ 个样本：16 个样本大致来自 $\\mathcal{N}(0,1)$ 分量，以及 $4$ 个位于 $10$ 的离群值。为了可复现性，请使用以下固定列表\n$$\n\\Xi_{\\text{train}} = [-1.5, 1.5, -1.0, 1.0, -0.7, 0.7, -0.3, 0.3, -2.0, 2.0, -0.2, 0.2, -1.2, 1.2, -0.4, 0.4, 10, 10, 10, 10].\n$$\n采用以基础度量 $d(\\xi,\\xi') = |\\xi - \\xi'|$ 定义的 1-Wasserstein 距离，将模糊集 $\\mathcal{W}_{\\epsilon}$ 定义为以训练数据构建的经验分布 $\\hat{P}_N$ 为中心、半径为 $\\epsilon \\ge 0$ 的球。从期望风险和模糊集的基本定义出发，在不假设任何预先推导的鲁棒公式的情况下，推导 SAA 估计量 $x_{\\text{SAA}}$ 和 Wasserstein-DRO 估计量 $x_{\\text{DRO}}(\\epsilon)$，它们分别最小化经验期望损失和在 $\\mathcal{W}_{\\epsilon}$ 上的最坏情况期望损失。然后，在真实分布下评估它们的样本外风险。在此设置中，决策 $x$ 的样本外风险是关于上述混合分布计算的真实期望 $\\mathbb{E}[f(x,\\xi)]$。\n\n你的程序必须：\n- 完全按照给定的方式构建 $\\Xi_{\\text{train}}$。\n- 计算 $\\Xi_{\\text{train}}$ 上的经验均值 $\\bar{\\xi}$。\n- 通过最小化 $\\tfrac{1}{2} x^2 - x \\bar{\\xi}$ 来计算 SAA 决策 $x_{\\text{SAA}}$。\n- 使用 1-Wasserstein 球 $\\mathcal{W}_{\\epsilon}$ 和基础度量 $|\\cdot|$，基于原理性推理为每个给定的 $\\epsilon$ 推导 Wasserstein-DRO 决策 $x_{\\text{DRO}}(\\epsilon)$，并实现它。\n- 对于每个决策 $x_{\\text{DRO}}(\\epsilon)$，在真实混合分布下评估样本外风险 $\\mathbb{E}\\left[\\tfrac{1}{2} x^2 - x \\xi\\right]$。\n\n使用以下半径值的测试套件：\n$$\n[\\;0.0,\\;0.2,\\;1.0,\\;2.0,\\;2.5\\;].\n$$\n这些值涵盖了一般“理想路径”情况 ($0.2$)、基线 SAA 情况 ($0.0$)、适度鲁棒化 ($1.0$)、$\\epsilon$ 等于 $|\\bar{\\xi}|$ 的边界阈值情况 ($2.0$)，以及极端鲁棒化 ($2.5$) 。对于列表中的每个 $\\epsilon$，输出在真实混合分布下评估的 $x_{\\text{DRO}}(\\epsilon)$ 对应的样本外风险。真实分布由混合权重和分量参数完全指定；你必须计算混合分布的精确均值并一致地使用它。\n\n你的程序应生成单行输出，其中包含用方括号括起来的逗号分隔的结果列表，例如 $[\\text{result}_1,\\text{result}_2,\\dots]$。所有答案必须是实数（浮点数），不带任何物理单位。较小的值表示更好的泛化能力。", "solution": "该问题要求针对一个特定的二次损失函数，推导并比较样本平均近似 (SAA) 估计量和 Wasserstein 分布鲁棒优化 (DRO) 估计量。这些估计量的性能将根据其样本外风险进行评估，该风险是相对于一个真实的、已知的数据生成分布计算的。\n\n首先，我们定义问题的核心组成部分。决策变量是一个标量 $x \\in \\mathbb{R}$，不确定参数是一个标量 $\\xi \\in \\mathbb{R}$。损失函数由 $f(x, \\xi) = \\frac{1}{2} x^2 - x \\xi$ 给出。训练数据包含 $N=20$ 个样本，以固定列表 $\\Xi_{\\text{train}}$ 的形式提供。经验分布 $\\hat{P}_N$ 是这 $N$ 个样本上的离散均匀分布。\n\nSAA 估计量，记为 $x_{\\text{SAA}}$，是经验风险最小化问题的解：\n$$\nx_{\\text{SAA}} = \\arg\\min_{x \\in \\mathbb{R}} \\mathbb{E}_{\\hat{P}_N}[f(x, \\xi)]\n$$\n经验风险计算为训练样本上的平均损失：\n$$\n\\mathbb{E}_{\\hat{P}_N}[f(x, \\xi)] = \\frac{1}{N} \\sum_{i=1}^{N} f(x, \\xi_i) = \\frac{1}{N} \\sum_{i=1}^{N} \\left(\\frac{1}{2} x^2 - x \\xi_i\\right) = \\frac{1}{2} x^2 - x \\left(\\frac{1}{N} \\sum_{i=1}^{N} \\xi_i\\right)\n$$\n令 $\\bar{\\xi} = \\frac{1}{N} \\sum_{i=1}^{N} \\xi_i$ 为训练样本的经验均值。SAA 目标函数简化为 $\\frac{1}{2} x^2 - x \\bar{\\xi}$。这是一个关于 $x$ 的严格凸二次函数。为了找到最小值点，我们将关于 $x$ 的一阶导数设为零：\n$$\n\\frac{d}{dx} \\left(\\frac{1}{2} x^2 - x \\bar{\\xi}\\right) = x - \\bar{\\xi} = 0\n$$\n这得到 SAA 估计量：\n$$\nx_{\\text{SAA}} = \\bar{\\xi}\n$$\n\n接下来，我们讨论 Wasserstein-DRO 估计量 $x_{\\text{DRO}}(\\epsilon)$。它是以下极小化极大问题的解：\n$$\nx_{\\text{DRO}}(\\epsilon) = \\arg\\min_{x \\in \\mathbb{R}} \\sup_{P \\in \\mathcal{W}_{\\epsilon}} \\mathbb{E}_{P}[f(x, \\xi)]\n$$\n此处，$\\mathcal{W}_{\\epsilon}$ 是围绕经验分布 $\\hat{P}_N$、半径为 $\\epsilon \\geq 0$ 的 1-Wasserstein 球，定义为 $\\mathcal{W}_{\\epsilon} = \\{P \\mid W_1(P, \\hat{P}_N) \\le \\epsilon \\}$，其基础度量为绝对差 $d(\\xi, \\xi') = |\\xi - \\xi'|$。\n\n为了解决这个极小化极大问题，我们首先分析对于一个固定的 $x$ 的内部最大化问题：\n$$\n\\sup_{P \\in \\mathcal{W}_{\\epsilon}} \\mathbb{E}_{P}[f(x, \\xi)] = \\sup_{P \\in \\mathcal{W}_{\\epsilon}} \\mathbb{E}_{P}\\left[\\frac{1}{2} x^2 - x \\xi\\right] = \\frac{1}{2} x^2 + \\sup_{P \\in \\mathcal{W}_{\\epsilon}} \\mathbb{E}_{P}[-x \\xi]\n$$\nKantorovich-Rubinstein 对偶定理指出，1-Wasserstein 距离可以表示为：\n$$\nW_1(P, \\hat{P}_N) = \\sup_{\\phi: \\|\\phi\\|_{\\text{Lip}} \\le 1} \\left( \\mathbb{E}_P[\\phi(\\xi)] - \\mathbb{E}_{\\hat{P}_N}[\\phi(\\xi)] \\right)\n$$\n其中上确界是取遍所有 1-Lipschitz 函数 $\\phi$。由此，对于任何函数 $g(\\xi)$，我们有不等式 $\\mathbb{E}_P[g(\\xi)] - \\mathbb{E}_{\\hat{P}_N}[g(\\xi)] \\le W_1(P, \\hat{P}_N) \\cdot L(g)$，其中 $L(g)$ 是 $g$ 的 Lipschitz 常数。对于任何 $P \\in \\mathcal{W}_{\\epsilon}$，这意味着 $\\mathbb{E}_P[g(\\xi)] \\le \\mathbb{E}_{\\hat{P}_N}[g(\\xi)] + \\epsilon L(g)$。这个上界是已知的紧界。\n在我们的情况下，我们感兴趣的函数是 $g(\\xi) = -x\\xi$。其 Lipschitz 常数为 $L(g) = \\sup_{\\xi_1\\neq\\xi_2}\\frac{|-x\\xi_1 - (-x\\xi_2)|}{|\\xi_1-\\xi_2|} = |-x| = |x|$。\n因此，最坏情况期望为：\n$$\n\\sup_{P \\in \\mathcal{W}_{\\epsilon}} \\mathbb{E}_{P}[-x \\xi] = \\mathbb{E}_{\\hat{P}_N}[-x \\xi] + \\epsilon |x| = -x \\bar{\\xi} + \\epsilon |x|\n$$\n将此代回 DRO 目标函数，我们需要求解：\n$$\nx_{\\text{DRO}}(\\epsilon) = \\arg\\min_{x \\in \\mathbb{R}} \\left( \\frac{1}{2} x^2 - x \\bar{\\xi} + \\epsilon |x| \\right)\n$$\n目标函数 $J(x) = \\frac{1}{2} x^2 - x \\bar{\\xi} + \\epsilon |x|$ 是凸函数，但在 $x=0$ 处不可微。我们可以使用次梯度微积分。其最小值点 $x^*$ 必须满足 $0 \\in \\partial J(x^*)$。$|x|$ 的次梯度在 $x \\neq 0$ 时为 $\\text{sgn}(x)$，在 $x=0$ 时为 $[-1, 1]$。\n$J(x)$ 的次梯度为 $\\partial J(x) = x - \\bar{\\xi} + \\epsilon \\cdot \\partial |x|$。\n如果 $x > 0$，我们需要 $x - \\bar{\\xi} + \\epsilon = 0 \\implies x = \\bar{\\xi} - \\epsilon$。这仅在 $\\bar{\\xi} - \\epsilon > 0$（即 $\\bar{\\xi} > \\epsilon$）时成立。\n如果 $x  0$，我们需要 $x - \\bar{\\xi} - \\epsilon = 0 \\implies x = \\bar{\\xi} + \\epsilon$。这仅在 $\\bar{\\xi} + \\epsilon  0$（即 $\\bar{\\xi}  -\\epsilon$）时成立。\n如果 $x=0$，我们需要 $0 \\in 0 - \\bar{\\xi} + \\epsilon[-1, 1] = [-\\bar{\\xi}-\\epsilon, -\\bar{\\xi}+\\epsilon]$。如果 $-\\bar{\\xi}-\\epsilon \\le 0 \\le -\\bar{\\xi}+\\epsilon$，该条件成立，可简化为 $|\\bar{\\xi}| \\le \\epsilon$。\n\n综合这些条件，解是应用于 $\\bar{\\xi}$ 且阈值为 $\\epsilon$ 的软阈值算子：\n$$\nx_{\\text{DRO}}(\\epsilon) = \\text{sign}(\\bar{\\xi}) \\max(0, |\\bar{\\xi}| - \\epsilon)\n$$\n注意，当 $\\epsilon=0$ 时，我们有 $x_{\\text{DRO}}(0) = \\text{sign}(\\bar{\\xi})\\max(0, |\\bar{\\xi}|) = \\bar{\\xi}$，这正确地恢复了 SAA 估计量 $x_{\\text{SAA}}$。\n\n最后，我们计算给定决策 $x$ 的样本外风险 $R(x)$。这是损失函数在真实数据生成分布 $P_{\\text{true}}$ 下的期望：\n$$\nR(x) = \\mathbb{E}_{P_{\\text{true}}}[f(x, \\xi)] = \\mathbb{E}_{P_{\\text{true}}}\\left[\\frac{1}{2} x^2 - x \\xi\\right] = \\frac{1}{2} x^2 - x \\mathbb{E}_{P_{\\text{true}}}[\\xi]\n$$\n$\\xi$ 的真实分布是一个混合分布：以 0.9 的概率，$\\xi \\sim \\mathcal{N}(0,1)$；以 0.1 的概率，$\\xi=10$。真实均值 $\\mu_{\\text{true}} = \\mathbb{E}_{P_{\\text{true}}}[\\xi]$ 为：\n$$\n\\mu_{\\text{true}} = 0.9 \\cdot \\mathbb{E}[\\mathcal{N}(0,1)] + 0.1 \\cdot 10 = 0.9 \\cdot 0 + 0.1 \\cdot 10 = 1.0\n$$\n因此，样本外风险为 $R(x) = \\frac{1}{2} x^2 - x$。\n\n我们现在进行数值计算。\n训练数据为 $\\Xi_{\\text{train}} = [-1.5, 1.5, -1.0, 1.0, -0.7, 0.7, -0.3, 0.3, -2.0, 2.0, -0.2, 0.2, -1.2, 1.2, -0.4, 0.4, 10, 10, 10, 10]$。\n前 16 个对称样本的总和为 0。四个离群值的总和为 $4 \\times 10 = 40$。样本总数为 $N=20$。\n经验均值为 $\\bar{\\xi} = \\frac{0 + 40}{20} = 2.0$。\n\n我们为每个给定的 $\\epsilon$ 评估 $x_{\\text{DRO}}(\\epsilon) = \\text{soft}(2.0, \\epsilon)$ 及其风险 $R(x_{\\text{DRO}}(\\epsilon))$：\n1.  $\\epsilon=0.0$: $x_{\\text{DRO}}(0.0) = \\text{soft}(2.0, 0.0) = 2.0$。风险 $R(2.0) = \\frac{1}{2}(2.0)^2 - 2.0 = 2.0 - 2.0 = 0.0$。\n2.  $\\epsilon=0.2$: $x_{\\text{DRO}}(0.2) = \\text{soft}(2.0, 0.2) = 1.8$。风险 $R(1.8) = \\frac{1}{2}(1.8)^2 - 1.8 = 1.62 - 1.8 = -0.18$。\n3.  $\\epsilon=1.0$: $x_{\\text{DRO}}(1.0) = \\text{soft}(2.0, 1.0) = 1.0$。风险 $R(1.0) = \\frac{1}{2}(1.0)^2 - 1.0 = 0.5 - 1.0 = -0.5$。\n4.  $\\epsilon=2.0$: $x_{\\text{DRO}}(2.0) = \\text{soft}(2.0, 2.0) = 0.0$。风险 $R(0.0) = \\frac{1}{2}(0.0)^2 - 0.0 = 0.0$。\n5.  $\\epsilon=2.5$: $x_{\\text{DRO}}(2.5) = \\text{soft}(2.0, 2.5) = 0.0$。风险 $R(0.0) = \\frac{1}{2}(0.0)^2 - 0.0 = 0.0$。\n\nSAA 解 $x_{\\text{SAA}}=2.0$ 对训练数据中的离群值产生了过拟合，因为它远大于真实的最优决策 $x^* = \\arg\\min_x R(x) = \\arg\\min_x(\\frac{1}{2}x^2-x) = 1.0$。$\\epsilon=1.0$ 的 DRO 解完美地修正了经验均值，从而获得了最佳的样本外性能。较小的 $\\epsilon$ 值提供的修正不足，而较大的 $\\epsilon$ 值会过度正则化，使解收缩得太多，从而导致性能下降。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes SAA and Wasserstein-DRO solutions and their out-of-sample risks.\n    \"\"\"\n    \n    # Define the training dataset as specified in the problem statement.\n    Xi_train = np.array([\n        -1.5, 1.5, -1.0, 1.0, -0.7, 0.7, -0.3, 0.3, -2.0, 2.0, \n        -0.2, 0.2, -1.2, 1.2, -0.4, 0.4, 10.0, 10.0, 10.0, 10.0\n    ])\n\n    # Define the test suite of Wasserstein radii.\n    epsilon_values = [0.0, 0.2, 1.0, 2.0, 2.5]\n\n    # Compute the empirical mean of the training data.\n    # This corresponds to the SAA decision x_SAA.\n    xi_bar = np.mean(Xi_train)\n\n    # The true mean of the data-generating distribution for xi.\n    # P(xi) = 0.9 * N(0, 1) + 0.1 * delta_10\n    # E[xi] = 0.9 * E[N(0,1)] + 0.1 * E[delta_10] = 0.9 * 0 + 0.1 * 10 = 1.0\n    mu_true = 1.0\n\n    def soft_threshold(y, T):\n        \"\"\"\n        The soft-thresholding operator, which gives the Wasserstein-DRO solution.\n        soft(y, T) = sign(y) * max(0, |y| - T)\n        \"\"\"\n        if T  0:\n            raise ValueError(\"Threshold T must be non-negative.\")\n        return np.sign(y) * np.maximum(0, np.abs(y) - T)\n\n    def out_of_sample_risk(x):\n        \"\"\"\n        Computes the out-of-sample risk R(x) = E_true[0.5*x^2 - x*xi].\n        R(x) = 0.5*x^2 - x * E_true[xi]\n        \"\"\"\n        return 0.5 * x**2 - x * mu_true\n\n    results = []\n    for epsilon in epsilon_values:\n        # Compute the Wasserstein-DRO decision x_DRO(epsilon).\n        # For epsilon = 0, this is the SAA decision x_SAA.\n        x_dro = soft_threshold(xi_bar, epsilon)\n        \n        # Evaluate the out-of-sample risk for this decision.\n        risk = out_of_sample_risk(x_dro)\n        results.append(risk)\n\n    # Format the output as a comma-separated list in brackets.\n    # The problem specifies that smaller values indicate better generalization.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3121634"}, {"introduction": "在理解了DRO能够提升模型泛化能力后，我们自然会问：Wasserstein模糊集在机制上是如何运作的？本练习旨在为此建立一个清晰的几何直观。它将揭示对于一个简单的线性损失函数，其在Wasserstein球内的最坏情况分布可以通过在给定半径 $\\epsilon$ 内对每个数据点进行“对抗性”移位来找到 [@problem_id:3121643]。通过动手实现并可视化这一过程，您将把“模糊集”这一抽象概念变得具体而清晰，深刻理解不确定性半径 $\\epsilon$ 是如何直接控制数据点扰动范围的。", "problem": "要求您实现一个小型、独立的程序，用于计算在Wasserstein球下定义的二维数据的分布鲁棒优化问题的最坏情况样本位置。考虑以下设置。设有一有限点集 $\\{x_i\\}_{i=1}^n \\subset \\mathbb{R}^2$，其经验分布在每个 $x_i$ 上赋予 $1/n$ 的质量。设损失为线性泛函 $f_w(x) = w^\\top x$，其中 $w \\in \\mathbb{R}^2$ 是给定的参数向量。考虑一个由无穷阶Wasserstein球定义的不确定性分布集，其基度量为欧几里得范数，即所有满足 $W_\\infty(Q, P_n) \\le \\epsilon$ 的分布 $Q$ 的集合，其中 $P_n$ 是 $\\{x_i\\}$ 上的经验分布。在此设置下，最坏情况期望损失定义为在半径为 $\\epsilon$ 的Wasserstein球内的所有分布上，期望损失的上确界，受Wasserstein距离约束。仅使用基本定义和凸分析原理，推导当每个原始样本 $x_i$ 被允许在以 $x_i$ 为中心的半径为 $\\epsilon$ 的欧几里得球内移动时，最大化平均损失的最坏情况扰动样本位置 $\\{x_i^\\star\\}$ 的优化器。您的程序必须为指定的测试套件计算这些最坏情况样本位置，并生成确切所需的输出格式。\n\n您必须使用的基本基础和定义：\n- 在 $\\mathbb{R}^2$ 上，具有欧几里得成本的两个分布之间的无穷阶Wasserstein距离 $W_\\infty$ 是使用耦合和欧几里得距离的本质上确界来定义的；它意味着每个质量点最多可以移动 $\\epsilon$。\n- 欧几里得范数定义为 $\\|v\\|_2 = \\sqrt{v_1^2 + v_2^2}$，对于 $v \\in \\mathbb{R}^2$。\n- 对于线性泛函 $f_w(x) = w^\\top x$，当梯度非零时，其在闭合欧几里得球上的最大化器在梯度的方向上于边界处取得。\n- 柯西-施瓦茨不等式：对于任何 $a, b \\in \\mathbb{R}^2$，有 $a^\\top b \\le \\|a\\|_2 \\|b\\|_2$，当且仅当 $a$ 和 $b$ 正向共线时等号成立。\n\n您的任务：\n1. 从基本基础出发，推导在给定 $w$ 和约束 $\\|x - x_i\\|_2 \\le \\epsilon$ 下，$f_w(x)$ 的每个样本的最大化器 $x_i^\\star$，包括 $\\|w\\|_2 = 0$ 的边缘情况。\n2. 实现一个程序，给定点集 $\\{x_i\\}$、向量 $w$ 和半径 $\\epsilon$，计算所有最坏情况扰动样本位置 $\\{x_i^\\star\\}$。\n3. 将您的实现应用于以下测试套件。为保证输出的数值稳定性，报告的坐标应四舍五入到六位小数。\n\n测试套件（每个案例的形式为 $(X, w, \\epsilon)$，其中 $X$ 是一个二维点列表）：\n- 案例1: $X = \\{[1.0, 0.0], [0.0, 1.0]\\}$, $w = [2.0, 1.0]$, $\\epsilon = 0.0$.\n- 案例2: $X = \\{[1.0, 0.0], [0.0, 1.0]\\}$, $w = [2.0, 1.0]$, $\\epsilon = 0.5$.\n- 案例3: $X = \\{[1.0, 0.0], [0.0, 1.0]\\}$, $w = [2.0, 1.0]$, $\\epsilon = 1.0$.\n- 案例4: $X = \\{[-1.0, -1.0], [2.0, 0.0], [0.0, -2.0]\\}$, $w = [0.0, 0.0]$, $\\epsilon = 0.75$.\n- 案例5: $X = \\{[0.5, -0.5], [1.5, 2.0]\\}$, $w = [-3.0, 4.0]$, $\\epsilon = 0.25$.\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，列表中的每个元素对应一个测试案例，并且是该案例计算出的最坏情况位置的列表。每个位置必须是一个双元素列表 $[x_1^\\star, x_2^\\star]$，两个条目都四舍五入到六位小数。例如，格式应如下所示：\n  - 对于两个案例，每个案例有两个点：$[[[x_{11}^\\star, x_{12}^\\star], [x_{21}^\\star, x_{22}^\\star]], [[y_{11}^\\star, y_{12}^\\star], [y_{21}^\\star, y_{22}^\\star]]]$。\n- 此问题不涉及任何物理单位。", "solution": "用户想要解决一个在分布不确定性下的最坏情况优化问题。\n\n### **问题陈述验证**\n\n**步骤1：提取已知条件**\n- **数据**：一个包含 $n$ 个点的集合 $\\{x_i\\}_{i=1}^n \\subset \\mathbb{R}^2$。\n- **经验分布**：$P_n = \\frac{1}{n} \\sum_{i=1}^n \\delta_{x_i}$，其中 $\\delta_{x_i}$ 是点 $x_i$ 上的狄拉克质量。\n- **损失函数**：一个线性泛函 $f_w(x) = w^\\top x$，对于给定的向量 $w \\in \\mathbb{R}^2$。\n- **不确定性集**：一个无穷阶Wasserstein球，定义为 $\\mathbb{R}^2$ 上所有满足Wasserstein距离 $W_\\infty(Q, P_n) \\le \\epsilon$ 的分布 $Q$ 的集合。基度量是欧几里得范数 $\\| \\cdot \\|_2$。\n- **目标**：找到在不确定性集上实现期望损失上确界的最坏情况样本位置 $\\{x_i^\\star\\}_{i=1}^n$。这在数学上表示为找到解决 $\\sup_{Q: W_\\infty(Q, P_n) \\le \\epsilon} \\mathbb{E}_Q[w^\\top x]$ 的位置。\n- **基本定义**：\n    - $W_\\infty$ 距离约束意味着每个质量点 $x_i$ 可以移动到一个新位置 $x'$，该位置在半径为 $\\epsilon$ 的欧几里得球内，即 $\\|x' - x_i\\|_2 \\le \\epsilon$。\n    - 欧几里得范数为 $\\|v\\|_2 = \\sqrt{v_1^2 + v_2^2}$。\n    - 线性泛函 $w^\\top x$ 在闭合欧几里得球上的最大化器位于梯度 $w$ 方向的边界上，前提是 $w \\neq 0$。\n    - 柯西-施瓦茨不等式：对于 $a, b \\in \\mathbb{R}^2$，有 $a^\\top b \\le \\|a\\|_2 \\|b\\|_2$。\n- **任务**：\n    1. 从基本基础出发，推导在给定 $w$ 和约束 $\\|x - x_i\\|_2 \\le \\epsilon$ 下，$f_w(x)$ 的每个样本的最大化器 $x_i^\\star$，包括 $\\|w\\|_2 = 0$ 的边缘情况。\n    2. 实现一个程序，给定点集 $\\{x_i\\}$、向量 $w$ 和半径 $\\epsilon$，计算所有最坏情况扰动样本位置 $\\{x_i^\\star\\}$。\n    3. 在给定的测试套件上运行并格式化输出。\n\n**步骤2：使用提取的已知条件进行验证**\n根据验证标准对问题进行评估。\n- **科学依据**：是。该问题是分布鲁棒优化（DRO）中的一个标准表述，基于Wasserstein距离、凸优化和对偶理论等已建立的数学概念。\n- **适定性**：是。目标是在一个紧集（一族闭球）上最大化一个连续（线性）函数。根据极值定理，最大值存在。推导过程将表明当 $w \\ne 0$ 时，该最大值是唯一的。\n- **目标明确**：是。问题使用精确、无歧义的数学术语进行陈述。\n- **缺陷检查**：\n    1.  **科学上不健全**：无。该表述是DRO中一个有效且被广泛研究的问题。\n    2.  **无法形式化/不相关**：无。问题被形式化陈述，并且是分布鲁棒优化主题的核心。\n    3.  **不完整/矛盾**：无。每个测试案例所需的所有数据（$X, w, \\epsilon$）都已提供。\n    4.  **不切实际/不可行**：不适用。该问题纯属数学问题，不涉及物理约束。\n    5.  **不适定**：无。如上所述，存在一个明确定义的解。\n    6.  **伪深刻/琐碎**：无。该问题要求从第一性原理进行正确推导并精确实现，这构成了一个有效的挑战。\n    7.  **超出科学可验证范围**：无。解决方案在数学上是可推导的，在计算上是可验证的。\n\n**步骤3：结论与行动**\n问题是**有效的**。继续进行求解。\n\n### **最坏情况样本位置的推导**\n\n目标是计算最坏情况期望损失：\n$$ \\sup_{Q: W_\\infty(Q, P_n) \\le \\epsilon} \\mathbb{E}_Q[w^\\top x] $$\n分布鲁棒优化中的一个基本结果（通常由Kantorovich对偶性推导）指出，对于无穷阶Wasserstein球，函数的最坏情况期望可以通过对每个样本的独立最坏情况实现取上确界来找到。鉴于经验分布 $P_n$ 在每个点 $x_i$ 上的质量为 $1/n$，该问题可以分解为 $n$ 个独立的子问题：\n$$ \\sup_{Q: W_\\infty(Q, P_n) \\le \\epsilon} \\mathbb{E}_Q[w^\\top x] = \\frac{1}{n} \\sum_{i=1}^n \\sup_{x'} \\{ w^\\top x' \\mid \\|x' - x_i\\|_2 \\le \\epsilon \\} $$\n我们的任务是为这 $n$ 个相同的最大化问题中的每一个找到优化器 $x_i^\\star$。让我们分析一个针对通用样本 $x_i$ 的子问题：\n$$ \\max_{x'} w^\\top x' \\quad \\text{subject to} \\quad \\|x' - x_i\\|_2 \\le \\epsilon $$\n设变量变换为 $x' = x_i + \\delta$，其中 $\\delta \\in \\mathbb{R}^2$。对 $x'$ 的约束转化为对扰动 $\\delta$ 的约束：\n$$ \\|(x_i + \\delta) - x_i\\|_2 \\le \\epsilon \\implies \\|\\delta\\|_2 \\le \\epsilon $$\n目标函数变为：\n$$ w^\\top x' = w^\\top (x_i + \\delta) = w^\\top x_i + w^\\top \\delta $$\n由于 $w^\\top x_i$ 相对于优化变量 $\\delta$ 是一个常数，最大化 $w^\\top x'$ 等价于最大化 $w^\\top \\delta$：\n$$ \\max_{\\delta} w^\\top \\delta \\quad \\text{subject to} \\quad \\|\\delta\\|_2 \\le \\epsilon $$\n我们可以使用柯西-施瓦茨不等式（$a^\\top b \\le \\|a\\|_2 \\|b\\|_2$）来确定最优扰动 $\\delta^\\star$。应用该不等式，我们得到：\n$$ w^\\top \\delta \\le \\|w\\|_2 \\|\\delta\\|_2 $$\n给定约束 $\\|\\delta\\|_2 \\le \\epsilon$，我们可以进一步限定该表达式的界：\n$$ w^\\top \\delta \\le \\|w\\|_2 \\epsilon $$\n当 $\\delta$ 与 $w$ 正向共线且具有最大可能模长 $\\epsilon$ 时，等号成立（从而达到最大值）。\n\n我们现在分析向量 $w$ 的两种情况：\n\n**情况1：$w \\ne 0$**\n如果 $w$ 不是零向量，则 $\\|w\\|_2 > 0$。最大化 $w^\\top \\delta$ 的唯一最优扰动 $\\delta^\\star$ 是在 $w$ 方向上长度为 $\\epsilon$ 的向量：\n$$ \\delta^\\star = \\epsilon \\frac{w}{\\|w\\|_2} $$\n然后，通过将此最优扰动加到原始样本位置 $x_i$ 上，可以找到最坏情况样本位置 $x_i^\\star$：\n$$ x_i^\\star = x_i + \\delta^\\star = x_i + \\epsilon \\frac{w}{\\|w\\|_2} $$\n这表明，为了最大化线性损失，每个数据点都应沿着损失向量的梯度 $w$ 的方向移动，移动到不确定性集所允许的最大距离 $\\epsilon$。这对每个样本 $i=1, \\dots, n$ 都成立。\n\n**情况2：$w = 0$**\n如果 $w$ 是零向量，则 $\\|w\\|_2 = 0$。目标函数变为：\n$$ w^\\top \\delta = 0^\\top \\delta = 0 $$\n在这种情况下，只要满足约束 $\\|\\delta\\|_2 \\le \\epsilon$，无论如何选择 $\\delta$，目标值都为 $0$。以 $x_i$ 为中心、半径为 $\\epsilon$ 的球内的任何点都是优化器。在这种情况下，一个自然且标准的选择是选择具有最小范数的解，这对应于做最小的改变。这导致选择 $\\delta^\\star = 0$。因此，最坏情况位置就是原始位置本身：\n$$ x_i^\\star = x_i $$\n请注意，如果 $\\|w\\|_2=0$，情况1中的公式是未定义的，因此必须单独处理此情况。\n\n结合这两种情况，最坏情况位置 $x_i^\\star$ 的公式为：\n$$ x_i^\\star = \\begin{cases} x_i + \\epsilon \\frac{w}{\\|w\\|_2}  \\text{if } \\|w\\|_2 > 0 \\\\ x_i  \\text{if } \\|w\\|_2 = 0 \\end{cases} $$\n此公式将用于实现，以解决给定测试套件的问题。", "answer": "```python\nimport numpy as np\n\ndef compute_worst_case_locations(X, w, epsilon):\n    \"\"\"\n    Computes the worst-case sample locations for a linear loss.\n\n    Args:\n        X (list of lists): The original sample locations, [[x1, y1], [x2, y2], ...].\n        w (list): The parameter vector [w1, w2] of the linear loss.\n        epsilon (float): The radius of the Wasserstein ball (and per-sample uncertainty sets).\n\n    Returns:\n        list of lists: The worst-case sample locations.\n    \"\"\"\n    X_np = np.array(X, dtype=float)\n    w_np = np.array(w, dtype=float)\n\n    # Handle the edge case where epsilon is zero. No perturbation is possible.\n    if epsilon == 0.0:\n        return X_np.tolist()\n\n    # Calculate the Euclidean norm of the vector w.\n    norm_w = np.linalg.norm(w_np)\n\n    # If the norm of w is zero (or numerically close to it), the loss is constant.\n    # The optimal perturbation is zero (no change to the points).\n    if norm_w  1e-9:\n        return X_np.tolist()\n    \n    # Calculate the unit vector in the direction of w.\n    u_w = w_np / norm_w\n    \n    # The optimal perturbation is epsilon times the unit vector u_w.\n    # This vector is the same for all points.\n    delta_star = epsilon * u_w\n    \n    # Add the perturbation to all original sample locations.\n    # Numpy's broadcasting handles this efficiently.\n    X_star_np = X_np + delta_star\n    \n    return X_star_np.tolist()\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print the results.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1\n        ({\"X\": [[1.0, 0.0], [0.0, 1.0]], \"w\": [2.0, 1.0], \"epsilon\": 0.0}),\n        # Case 2\n        ({\"X\": [[1.0, 0.0], [0.0, 1.0]], \"w\": [2.0, 1.0], \"epsilon\": 0.5}),\n        # Case 3\n        ({\"X\": [[1.0, 0.0], [0.0, 1.0]], \"w\": [2.0, 1.0], \"epsilon\": 1.0}),\n        # Case 4\n        ({\"X\": [[-1.0, -1.0], [2.0, 0.0], [0.0, -2.0]], \"w\": [0.0, 0.0], \"epsilon\": 0.75}),\n        # Case 5\n        ({\"X\": [[0.5, -0.5], [1.5, 2.0]], \"w\": [-3.0, 4.0], \"epsilon\": 0.25}),\n    ]\n\n    # Store formatted string results for each test case\n    results_str_list = []\n\n    for case in test_cases:\n        worst_case_locs = compute_worst_case_locations(case[\"X\"], case[\"w\"], case[\"epsilon\"])\n        \n        # Format the result for a single case according to the required output format.\n        # Each point is formatted as [x,y] with 6 decimal places.\n        points_str = [f\"[{p[0]:.6f},{p[1]:.6f}]\" for p in worst_case_locs]\n        case_result_str = f\"[{','.join(points_str)}]\"\n        results_str_list.append(case_result_str)\n\n    # Join the results of all cases into a single string.\n    final_output = f\"[{','.join(results_str_list)}]\"\n    \n    # Final print statement must produce only the specified output format.\n    print(final_output)\n\n# Execute the solver.\nsolve()\n\n```", "id": "3121643"}, {"introduction": "我们已经了解了DRO为何有用（“Why”），并建立了其工作机制的直观理解（“What”）。现在，是时候学习如何从零开始解决一个DRO问题了（“How”）。本练习将引导您实现一个完整的数值优化算法来求解一个分布鲁棒模型 [@problem_id:3121614]。您将首先推导DRO目标的闭式解及其次梯度，这是优化算法的核心要素。随后，您将实现投影次梯度法，这是一种解决此类带约束的非光滑凸问题的基本算法，从而将理论转化为可执行的代码。", "problem": "给定一个分布鲁棒优化问题，其中模糊集是一个围绕经验分布的 Wasserstein-1 球。考虑决策向量 $x \\in \\mathbb{R}^d$、损失函数 $\\ell:\\mathbb{R}^d \\times \\mathbb{R}^d \\to \\mathbb{R}$ 以及数据生成随机向量 $\\xi \\in \\mathbb{R}^d$。分布鲁棒目标为\n$$\nF(x) \\;=\\; \\sup_{P \\in \\mathcal{W}_\\epsilon(\\widehat P)} \\;\\mathbb{E}_P\\big[\\ell(x,\\xi)\\big],\n$$\n其中 $\\widehat P$ 是由 $n$ 个样本 $\\{\\xi_i\\}_{i=1}^n$ 支持的经验分布，$\\mathcal{W}_\\epsilon(\\widehat P)$ 表示在欧几里得基本成本下，与 $\\widehat P$ 的 Wasserstein-1 距离至多为 $\\epsilon$ 的所有概率分布的集合。损失函数为\n$$\n\\ell(x,\\xi) \\;=\\; \\tfrac{1}{2}\\,\\|x\\|_2^2 \\;+\\; \\xi^\\top x,\n$$\n该函数关于 $x$ 是凸的。可行集是 $\\ell_\\infty$ 方盒\n$$\n\\mathcal{X} \\;=\\; \\{\\,x\\in\\mathbb{R}^d : \\|x\\|_\\infty \\le B\\,\\}.\n$$\n\n任务：\n- 对于任意 $x \\in \\mathcal{X}$，仅使用以下基本依据推导 $F(x)$ 的一个次梯度：\n  - 次梯度的定义：对于一个凸函数 $f:\\mathbb{R}^d\\to\\mathbb{R}$，如果对于所有 $y$ 都有 $f(y)\\ge f(x)+g^\\top(y-x)$，则向量 $g$ 是 $f$ 在 $x$ 处的一个次梯度。\n  - 一个凸函数族的逐点上确界是凸的，并且该上确界在点 $x$ 处的次梯度可以从 $x$ 处的任意一个激活函数的次梯度中获得。\n  - 对于带有欧几里得基本成本的 Wasserstein-1 距离的 Kantorovich–Rubinstein 对偶表示及其对 Lipschitz 函数的推论：如果函数 $f$ 关于欧几里得范数的 Lipschitz 常数 $L$ 是有限的，则 $\\sup_{P:W_1(P,\\widehat P)\\le \\epsilon}\\mathbb{E}_P[f(\\xi)]=\\mathbb{E}_{\\widehat P}[f(\\xi)] + \\epsilon L$。\n  - 欧几里得范数的次微分：当 $x\\neq 0$ 时，$\\partial \\|x\\|_2 = \\{\\,x/\\|x\\|_2\\,\\}$；当 $x=0$ 时，$\\partial \\|0\\|_2 = \\{\\,s\\in\\mathbb{R}^d : \\|s\\|_2 \\le 1\\,\\}$。\n- 设计一种投影次梯度法，用以在 $\\mathcal{X}$ 上最小化 $F(x)$，步长递减，$\\alpha_t = \\alpha_0/\\sqrt{t}$，其中 $t$ 是迭代索引，$\\alpha_0>0$ 是初始步长。到 $\\mathcal{X}$ 上的投影是逐坐标裁剪到 $\\left[-B,B\\right]$ 的算子。\n- 为上述简单设定实现此方法，并报告下面每个测试用例在 $T$ 次迭代后 $F(x)$ 的最终值。在您的实现中，在当前迭代点 $x_t$ 处取一个次梯度，并通过将点 $x_t - \\alpha_t g_t$ 投影到 $\\mathcal{X}$ 上来更新 $x_{t+1}$，其中 $g_t$ 是 $F$ 在 $x_t$ 处的任意有效次梯度。当 $x_t=0$ 时，选择欧几里得范数的任意一个模至多为 $1$ 的次梯度（例如，选择零向量）。\n\n定义和数据：\n- 经验分布为 $\\widehat P = \\frac{1}{n}\\sum_{i=1}^n \\delta_{\\xi_i}$，其中样本为 $\\xi_i \\in \\mathbb{R}^d$。\n- 维度、样本集、模糊半径和算法参数在下面的每个测试用例中指定。\n- 鲁棒目标 $F$ 必须使用您从次梯度推导中得到的闭式表达式进行精确评估，而不是通过蒙特卡洛方法。\n\n测试套件（每个用例指定维度 $d$、样本数 $n$、样本 $\\{\\xi_i\\}_{i=1}^n$、半径 $\\epsilon$、方盒边界 $B$、初始步长 $\\alpha_0$ 和迭代次数 $T$）：\n- 用例 1：$d=2$, $n=3$, 样本 $\\xi_1=\\begin{bmatrix}1\\\\-1\\end{bmatrix}$, $\\xi_2=\\begin{bmatrix}0\\\\2\\end{bmatrix}$, $\\xi_3=\\begin{bmatrix}-2\\\\1\\end{bmatrix}$, $\\epsilon=0.4$, $B=10$, $\\alpha_0=1.0$, $T=2000$。\n- 用例 2：$d=2$, $n=2$, 样本 $\\xi_1=\\begin{bmatrix}2\\\\2\\end{bmatrix}$, $\\xi_2=\\begin{bmatrix}2\\\\2\\end{bmatrix}$, $\\epsilon=0.0$, $B=10$, $\\alpha_0=1.0$, $T=2000$。\n- 用例 3：$d=2$, $n=2$, 样本 $\\xi_1=\\begin{bmatrix}1\\\\0\\end{bmatrix}$, $\\xi_2=\\begin{bmatrix}-1\\\\0\\end{bmatrix}$, $\\epsilon=1.5$, $B=10$, $\\alpha_0=1.0$, $T=2000$。\n- 用例 4：$d=2$, $n=1$, 样本 $\\xi_1=\\begin{bmatrix}5\\\\-5\\end{bmatrix}$, $\\epsilon=1.0$, $B=0.1$, $\\alpha_0=1.0$, $T=2000$。\n\n实现要求：\n- 使用您的推导所隐含的 $F(x)$ 的精确闭式，在任意 $x$ 处对 $F(x)$ 进行数值评估。\n- 使用您推导出的次梯度实现投影次梯度法。\n- 将 $x_1$ 初始化为零向量。\n- 使用递减的步长 $\\alpha_t=\\alpha_0/\\sqrt{t}$。\n- 到 $\\mathcal{X}$ 上的投影必须是逐坐标裁剪到 $\\left[-B,B\\right]$ 的算子。\n- 对于 $x_t=0$，选择零向量作为 $\\partial\\|x_t\\|_2$ 的一个有效元素。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含 4 个用例的鲁棒目标值，每个值四舍五入到 6 位小数，以逗号分隔的列表形式用方括号括起来，顺序为用例 1、用例 2、用例 3、用例 4（例如，`\"[v_1,v_2,v_3,v_4]\"`）。", "solution": "用户提供的问题被评估为有效。它在分布鲁棒优化领域具有科学依据，对于最小化是适定的，并使用客观、形式化的数学语言进行描述。为获得唯一解所需的所有数据和参数均已提供，且不存在内部矛盾。\n\n问题是在可行集 $\\mathcal{X}$ 上最小化分布鲁棒目标函数 $F(x)$，其中\n$$\nF(x) \\;=\\; \\sup_{P \\in \\mathcal{W}_\\epsilon(\\widehat P)} \\;\\mathbb{E}_P\\big[\\ell(x,\\xi)\\big]\n$$\n损失函数是 $\\ell(x,\\xi) = \\frac{1}{2}\\|x\\|_2^2 + \\xi^\\top x$，可行集是 $\\ell_\\infty$ 方盒 $\\mathcal{X} = \\{x \\in \\mathbb{R}^d : \\|x\\|_\\infty \\le B\\}$。\n\n首先，我们推导目标函数 $F(x)$ 的闭式表达式。对于一个固定的决策向量 $x \\in \\mathbb{R}^d$，损失 $\\ell(x, \\xi)$ 是随机向量 $\\xi \\in \\mathbb{R}^d$ 的函数。我们确定该函数关于 $\\xi$ 上的欧几里得范数的 Lipschitz 常数。$\\ell(x, \\xi)$ 关于 $\\xi$ 的梯度是：\n$$\n\\nabla_\\xi \\ell(x, \\xi) = \\nabla_\\xi \\left( \\tfrac{1}{2}\\|x\\|_2^2 + \\xi^\\top x \\right) = x\n$$\n$\\ell(x,\\xi)$ 关于 $\\xi$ 的 Lipschitz 常数，记为 $L_x$，是该梯度的欧几里得范数：\n$$\nL_x = \\|\\nabla_\\xi \\ell(x, \\xi)\\|_2 = \\|x\\|_2\n$$\n问题提供了 Kantorovich-Rubinstein 对偶表示对具有 Lipschitz 常数 $L$ 的函数 $f$ 的推论：\n$$\n\\sup_{P:W_1(P,\\widehat P)\\le \\epsilon}\\mathbb{E}_P[f(\\xi)]=\\mathbb{E}_{\\widehat P}[f(\\xi)] + \\epsilon L\n$$\n通过设 $f(\\xi) = \\ell(x,\\xi)$ 和 $L = L_x = \\|x\\|_2$，将此结果应用于我们的问题，我们得到鲁棒目标 $F(x)$ 的闭式：\n$$\nF(x) = \\mathbb{E}_{\\widehat P}[\\ell(x,\\xi)] + \\epsilon \\|x\\|_2\n$$\n在经验分布 $\\widehat P = \\frac{1}{n}\\sum_{i=1}^n \\delta_{\\xi_i}$ 上的期望计算如下：\n$$\n\\mathbb{E}_{\\widehat P}[\\ell(x,\\xi)] = \\frac{1}{n} \\sum_{i=1}^n \\left( \\tfrac{1}{2}\\|x\\|_2^2 + \\xi_i^\\top x \\right) = \\tfrac{1}{2}\\|x\\|_2^2 + \\left(\\frac{1}{n} \\sum_{i=1}^n \\xi_i\\right)^\\top x\n$$\n令 $\\bar{\\xi} = \\frac{1}{n}\\sum_{i=1}^n \\xi_i$ 为数据的经验均值。目标函数简化为：\n$$\nF(x) = \\tfrac{1}{2}\\|x\\|_2^2 + \\bar{\\xi}^\\top x + \\epsilon \\|x\\|_2\n$$\n此表达式将用于评估最终迭代点的目标函数值。\n\n接下来，我们推导 $F(x)$ 的一个次梯度。函数 $F(x)$ 是凸的，因为它是三个凸函数之和：$f_1(x) = \\frac{1}{2}\\|x\\|_2^2$，$f_2(x) = \\bar{\\xi}^\\top x$ 和 $f_3(x) = \\epsilon \\|x\\|_2$。和的次微分是次微分的和。\n$$\n\\partial F(x) = \\nabla f_1(x) + \\nabla f_2(x) + \\partial f_3(x)\n$$\n可微项的梯度为 $\\nabla f_1(x) = x$ 和 $\\nabla f_2(x) = \\bar{\\xi}$。不可微项的次微分是 $\\partial f_3(x) = \\epsilon \\cdot \\partial \\|x\\|_2$。如前所述，欧几里得范数的次微分在 $x \\neq 0$ 时为 $\\partial \\|x\\|_2 = \\{x/\\|x\\|_2\\}$，在 $x=0$ 时为 $\\partial \\|0\\|_2 = \\{s \\in \\mathbb{R}^d : \\|s\\|_2 \\le 1\\}$。\n综合这些， $F(x)$ 的次微分为：\n$$\n\\partial F(x) = \\{ x + \\bar{\\xi} + \\epsilon s \\mid s \\in \\partial \\|x\\|_2 \\}\n$$\n对于投影次梯度算法，我们需要在每个迭代点 $x_t$ 选择一个次梯度 $g_t$。遵循问题的指示：\n\\begin{itemize}\n    \\item 如果 $x_t \\neq 0$，我们选择唯一的次梯度 $s = x_t/\\|x_t\\|_2$，这得到：\n    $$\n    g_t = x_t + \\bar{\\xi} + \\epsilon \\frac{x_t}{\\|x_t\\|_2}\n    $$\n    \\item 如果 $x_t = 0$，我们被指示选择零向量作为范数部分的次梯度。这对应于从次微分 $\\partial \\|0\\|_2$ 中选择 $s=0$，这得到：\n    $$\n    g_t = 0 + \\bar{\\xi} + \\epsilon \\cdot 0 = \\bar{\\xi}\n    $$\n\\end{itemize}\n\n最后，我们设计投影次梯度法来求解 $\\min_{x \\in \\mathcal{X}} F(x)$。该算法按以下步骤进行：\n\\begin{enumerate}\n    \\item \\textbf{初始化}：设置迭代计数器 $t=1$ 并初始化决策向量 $x_1 = 0 \\in \\mathbb{R}^d$。\n    \\item \\textbf{迭代}：对于 $t = 1, 2, \\ldots, T$：\n    \\begin{enumerate}\n        \\item 使用上面推导的规则计算一个次梯度 $g_t \\in \\partial F(x_t)$。\n        \\item 计算步长 $\\alpha_t = \\alpha_0 / \\sqrt{t}$。\n        \\item 执行更新步骤：$y_{t+1} = x_t - \\alpha_t g_t$。\n        \\item 将结果投影到可行集 $\\mathcal{X}$ 上：$x_{t+1} = \\Pi_{\\mathcal{X}}(y_{t+1})$。到 $\\ell_\\infty$ 方盒上的投影是一个逐坐标裁剪操作：对于每个分量 $j$，$(x_{t+1})_j = \\max(-B, \\min(B, (y_{t+1})_j))$。\n    \\end{enumerate}\n    \\item \\textbf{终止}：在 $T$ 次迭代后，算法终止。最终结果是使用其闭式评估的目标函数值 $F(x_{T+1})$。\n\\end{enumerate}\n该过程被实现以找到给定测试用例的解。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef run_projected_subgradient_method(d, samples, epsilon, B, alpha0, T):\n    \"\"\"\n    Implements the projected subgradient method for the given problem.\n\n    Args:\n        d (int): Dimension of the space.\n        samples (np.ndarray): Data samples, shape (n, d).\n        epsilon (float): Radius of the Wasserstein ambiguity set.\n        B (float): Bound for the l-infinity box constraint.\n        alpha0 (float): Initial step size.\n        T (int): Number of iterations.\n\n    Returns:\n        float: The final objective value F(x) after T iterations.\n    \"\"\"\n    # Pre-computation of the empirical mean\n    xi_bar = np.mean(samples, axis=0)\n\n    # Initialization\n    x = np.zeros(d)\n    \n    # Projected subgradient method iterations\n    for t in range(1, T + 1):\n        # 1. Compute subgradient g_t at x_t\n        norm_x = np.linalg.norm(x)\n        \n        # Following the problem's rule for computing the subgradient\n        if norm_x  1e-12: # Use a small tolerance for numerical stability\n            # For x = 0, the subgradient of the norm is taken as the zero vector.\n            # g = x + xi_bar + epsilon * 0 = xi_bar\n            g = xi_bar\n        else:\n            # For x != 0, g = x + xi_bar + epsilon * (x / ||x||_2)\n            g = x + xi_bar + epsilon * x / norm_x\n\n        # 2. Compute step size alpha_t\n        alpha = alpha0 / np.sqrt(t)\n\n        # 3. Update step\n        x_unprojected = x - alpha * g\n\n        # 4. Projection step\n        x = np.clip(x_unprojected, -B, B)\n\n    # After T iterations, calculate the final objective value F(x)\n    # F(x) = 0.5 * ||x||_2^2 + xi_bar^T * x + epsilon * ||x||_2\n    final_norm_x = np.linalg.norm(x)\n    final_objective_value = 0.5 * final_norm_x**2 + np.dot(xi_bar, x) + epsilon * final_norm_x\n    \n    return final_objective_value\n\ndef solve():\n    \"\"\"\n    Defines test cases and runs the solver for each case.\n    \"\"\"\n    test_cases = [\n        # Case 1: d, n, samples, epsilon, B, alpha0, T\n        {'d': 2, 'n': 3, 'samples': np.array([[1.0, -1.0], [0.0, 2.0], [-2.0, 1.0]]), 'epsilon': 0.4, 'B': 10.0, 'alpha0': 1.0, 'T': 2000},\n        # Case 2\n        {'d': 2, 'n': 2, 'samples': np.array([[2.0, 2.0], [2.0, 2.0]]), 'epsilon': 0.0, 'B': 10.0, 'alpha0': 1.0, 'T': 2000},\n        # Case 3\n        {'d': 2, 'n': 2, 'samples': np.array([[1.0, 0.0], [-1.0, 0.0]]), 'epsilon': 1.5, 'B': 10.0, 'alpha0': 1.0, 'T': 2000},\n        # Case 4\n        {'d': 2, 'n': 1, 'samples': np.array([[5.0, -5.0]]), 'epsilon': 1.0, 'B': 0.1, 'alpha0': 1.0, 'T': 2000},\n    ]\n\n    results = []\n    for case in test_cases:\n        result = run_projected_subgradient_method(\n            d=case['d'],\n            samples=case['samples'],\n            epsilon=case['epsilon'],\n            B=case['B'],\n            alpha0=case['alpha0'],\n            T=case['T']\n        )\n        results.append(f\"{result:.6f}\")\n\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3121614"}]}