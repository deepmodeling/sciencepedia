{"hands_on_practices": [{"introduction": "随机优化问题可能看起来令人望而生畏，因为它们涉及不确定性。样本平均近似 (Sample Average Approximation, SAA) 方法通过用观测数据的平均值替代未知的期望值，提供了一个直观的切入点。在这个练习中 [@problem_id:2182114]，您将把 SAA 应用于一个经典的物流问题：在交通状况不确定的城市中寻找最快路线，从而将一个复杂的随机问题转化为一个可解的最短路径问题。", "problem": "一家物流公司正在使用模拟来优化一个拥堵的小城区的送货路线。该地区的道路网络可以建模为一组四个交叉口，分别标记为 1、2、3 和 4，由五条双向路段连接。这些路段是 (1,2)、(1,3)、(2,3)、(2,4) 和 (3,4)。\n\n为了考虑交通的可变性，该公司运行了五次模拟，每次模拟代表一种可能发生的交通情景。在每种情景下，通过每个路段所需的行驶时间（单位：分钟）如下表所示。\n\n| 路段    | 情景 1 | 情景 2 | 情景 3 | 情景 4 | 情景 5 |\n|--------------|------------|------------|------------|------------|------------|\n| (1,2)        | 15         | 25         | 18         | 22         | 20         |\n| (1,3)        | 30         | 12         | 15         | 18         | 25         |\n| (2,3)        | 10         | 8          | 12         | 15         | 5          |\n| (2,4)        | 35         | 40         | 28         | 30         | 32         |\n| (3,4)        | 20         | 25         | 35         | 30         | 20         |\n\n你的任务是找到从交叉口 1 到交叉口 4 的最佳路线。你需要使用样本均值近似 (SAA) 方法。在 SAA 方法中，每个路段的真实（但未知）的期望行驶时间由其样本均值来近似，该样本均值是根据模拟数据计算得出的。问题就变成了找到总近似行驶时间最短的路径。\n\n基于此 SAA 模型，从交叉口 1 到交叉口 4 的行程的估计最小平均行驶时间是多少？用分钟表示你的答案，并四舍五入到三位有效数字。", "solution": "我们应用样本均值近似 (SAA) 方法：对于每个路段 $e$，我们用 5 个情景下的样本均值来近似其期望行驶时间，\n$$\n\\hat{t}_{e}=\\frac{1}{5}\\sum_{s=1}^{5}t_{e}^{(s)}.\n$$\n计算所有路段的样本均值：\n- 对于 (1,2)：\n$$\n\\hat{t}_{12}=\\frac{1}{5}(15+25+18+22+20)=\\frac{100}{5}=20.\n$$\n- 对于 (1,3)：\n$$\n\\hat{t}_{13}=\\frac{1}{5}(30+12+15+18+25)=\\frac{100}{5}=20.\n$$\n- 对于 (2,3)：\n$$\n\\hat{t}_{23}=\\frac{1}{5}(10+8+12+15+5)=\\frac{50}{5}=10.\n$$\n- 对于 (2,4)：\n$$\n\\hat{t}_{24}=\\frac{1}{5}(35+40+28+30+32)=\\frac{165}{5}=33.\n$$\n- 对于 (3,4)：\n$$\n\\hat{t}_{34}=\\frac{1}{5}(20+25+35+30+20)=\\frac{130}{5}=26.\n$$\n\n使用这些边权重，计算从 $1$ 到 $4$ 的简单路径的总时间：\n- 路径 $1\\to 2\\to 4$：$20+33=53$。\n- 路径 $1\\to 3\\to 4$：$20+26=46$。\n- 路径 $1\\to 2\\to 3\\to 4$：$20+10+26=56$。\n- 路径 $1\\to 3\\to 2\\to 4$：$20+10+33=63$。\n\n最小总时间是 $46$，路径为 $1\\to 3\\to 4$。四舍五入到三位有效数字得到 $46.0$。", "answer": "$$\\boxed{46.0}$$", "id": "2182114"}, {"introduction": "随机梯度下降 (Stochastic Gradient Descent, SGD) 是许多现代机器学习应用的核心引擎，但其性能取决于精心选择的超参数。小批量大小 $b$ 体现了一个典型的权衡：较大的批量能提供更准确的梯度估计，但计算成本也更高。这个练习 [@problem_id:3187488] 要求您对这种权衡进行数学建模，并编写一个程序来寻找能够在单位时间内最大化学习进度的最佳小批量大小。", "problem": "考虑在强凸二次目标函数 $f(x) = \\frac{\\lambda}{2} x^{2}$ 上进行单参数随机梯度下降（SGD），其中 $\\lambda > 0$ 是曲率。在第 $k$ 次迭代时，真实梯度为 $\\nabla f(x_{k}) = \\lambda x_{k}$。假设使用带加性噪声的无偏随机梯度估计器，因此单样本梯度观测值的形式为 $g(x_{k}) = \\lambda x_{k} + \\varepsilon$，其中 $\\mathbb{E}[\\varepsilon] = 0$ 且 $\\mathrm{Var}(\\varepsilon) = \\sigma^{2}$。对于大小为 $b$ 的小批量（mini-batch），由于独立性，平均噪声的方差为 $\\sigma^{2} / b$，小批量估计器为 $g_{b}(x_{k}) = \\lambda x_{k} + \\bar{\\varepsilon}_{b}$，其中 $\\mathrm{Var}(\\bar{\\varepsilon}_{b}) = \\sigma^{2} / b$。\n\n使用固定步长 $\\alpha > 0$ 的 SGD 更新规则为 $x_{k+1} = x_{k} - \\alpha g_{b}(x_{k})$。定义每次更新的计算时间为 $T(b) = t_{0} + c b$，其中 $t_{0} > 0$ 是每次更新的固定开销时间， $c > 0$ 是每个样本的计算成本。$t_{0}$ 和 $c$ 的单位都是秒（s），而小批量大小 $b$ 是无单位的。\n\n定义预期单步进展为目标值的预期减少量，\n$$\n\\Delta(b) = \\mathbb{E}\\left[f(x_{k}) - f(x_{k+1})\\right],\n$$\n以及单位时间的预期进展为\n$$\nR(b) = \\frac{\\Delta(b)}{T(b)}.\n$$\n\n从上述核心定义出发——凸二次目标函数 $f(x)$、带独立噪声的无偏梯度估计器、小批量平均以及 SGD 更新规则——推导出 $R(b)$ 关于参数 $(\\lambda, \\alpha, x_{k}, \\sigma^{2}, t_{0}, c, b)$ 的表达式，不使用任何快捷公式或外部提示。然后，设计一个程序，针对下面列出的每个测试用例，在整数小批量大小 $b \\in \\{1,2,\\dots,B_{\\max}\\}$ 上进行搜索，并返回使 $R(b)$ 最大化的 $b$ 的整数值。在所有测试用例中，使用固定的常数 $\\lambda = 1$、$\\alpha = 0.5$、$x_{k} = 1$、$t_{0} = 0.02$ (s)以及 $B_{\\max} = 512$。在每个测试用例中，$c$ 的单位必须是秒/样本（s/sample）。不涉及角度单位，也不需要百分比。\n\n测试套件（每个测试用例是一对 $(\\sigma^{2}, c)$，其中 $\\sigma^{2}$ 无单位，$c$ 的单位是秒/样本）：\n- 用例 1: $(\\sigma^{2}, c) = (1.0, 0.002)$\n- 用例 2: $(\\sigma^{2}, c) = (10.0, 0.002)$\n- 用例 3: $(\\sigma^{2}, c) = (0.01, 0.002)$\n- 用例 4: $(\\sigma^{2}, c) = (1.0, 0.02)$\n- 用例 5: $(\\sigma^{2}, c) = (5.0, 0.0005)$\n\n您的程序应生成单行输出，其中包含用方括号括起来的逗号分隔列表形式的结果（例如，$[b_{1},b_{2},b_{3},b_{4},b_{5}]$），其中 $b_{i}$ 是用例 $i$ 的最优小批量大小。输出值必须是整数。", "solution": "该问题是有效的，因为它科学地基于随机优化的原理，问题定义明确、客观且内部一致。提供了唯一解所需的所有数据和常数。我们现在将进行推导和求解。\n\n主要目标是找到一个整数小批量大小 $b$，以最大化单位时间的预期进展 $R(b)$。该量定义为预期单步进展 $\\Delta(b)$ 与每次更新的计算时间 $T(b)$ 之比。我们必须首先根据给定的定义推导出 $R(b)$ 的显式表达式。\n\n所讨论的目标函数是一个强凸二次函数：\n$$f(x) = \\frac{\\lambda}{2} x^{2}$$\n其中曲率 $\\lambda > 0$。在点 $x_k$ 处的真实梯度为 $\\nabla f(x_k) = \\lambda x_k$。\n\n大小为 $b$ 的小批量随机梯度估计器由下式给出：\n$$g_{b}(x_{k}) = \\lambda x_{k} + \\bar{\\varepsilon}_{b}$$\n其中平均噪声项 $\\bar{\\varepsilon}_{b}$ 的均值为 $\\mathbb{E}[\\bar{\\varepsilon}_{b}] = 0$，方差为 $\\mathrm{Var}(\\bar{\\varepsilon}_{b}) = \\frac{\\sigma^{2}}{b}$。此处，$\\sigma^2$ 是来自单个样本的噪声方差。\n\n使用固定步长 $\\alpha > 0$ 的随机梯度下降（SGD）更新规则为：\n$$x_{k+1} = x_{k} - \\alpha g_{b}(x_{k})$$\n\n预期单步进展定义为 $\\Delta(b) = \\mathbb{E}\\left[f(x_{k}) - f(x_{k+1})\\right]$。我们首先用 $x_k$ 和随机梯度来表示 $f(x_{k+1})$：\n$$f(x_{k+1}) = \\frac{\\lambda}{2} x_{k+1}^{2} = \\frac{\\lambda}{2} \\left(x_{k} - \\alpha g_{b}(x_{k})\\right)^{2}$$\n展开平方项得到：\n$$f(x_{k+1}) = \\frac{\\lambda}{2} \\left( x_{k}^{2} - 2\\alpha x_{k} g_{b}(x_{k}) + \\alpha^{2} g_{b}(x_{k})^{2} \\right)$$\n为了求出 $f(x_{k+1})$ 的期望值，我们对梯度估计器中的噪声取期望。在第 $k$ 次迭代中，$x_k$ 的值被视为固定的：\n$$\\mathbb{E}[f(x_{k+1})] = \\frac{\\lambda}{2} \\mathbb{E}\\left[ x_{k}^{2} - 2\\alpha x_{k} g_{b}(x_{k}) + \\alpha^{2} g_{b}(x_{k})^{2} \\right]$$\n根据期望的线性性质：\n$$\\mathbb{E}[f(x_{k+1})] = \\frac{\\lambda}{2} \\left( x_{k}^{2} - 2\\alpha x_{k} \\mathbb{E}[g_{b}(x_{k})] + \\alpha^{2} \\mathbb{E}[g_{b}(x_{k})^{2}] \\right)$$\n我们现在需要计算 $g_{b}(x_{k})$ 的一阶矩和二阶矩。\n一阶矩（期望）为：\n$$\\mathbb{E}[g_{b}(x_{k})] = \\mathbb{E}[\\lambda x_{k} + \\bar{\\varepsilon}_{b}] = \\lambda x_{k} + \\mathbb{E}[\\bar{\\varepsilon}_{b}] = \\lambda x_{k}$$\n二阶矩使用关系式 $\\mathbb{E}[X^2] = \\mathrm{Var}(X) + (\\mathbb{E}[X])^2$ 计算：\n$$\\mathbb{E}[g_{b}(x_{k})^{2}] = \\mathrm{Var}(g_{b}(x_{k})) + (\\mathbb{E}[g_{b}(x_{k})])^{2}$$\n估计器的方差为：\n$$\\mathrm{Var}(g_{b}(x_{k})) = \\mathrm{Var}(\\lambda x_{k} + \\bar{\\varepsilon}_{b}) = \\mathrm{Var}(\\bar{\\varepsilon}_{b}) = \\frac{\\sigma^{2}}{b}$$\n因此，二阶矩为：\n$$\\mathbb{E}[g_{b}(x_{k})^{2}] = \\frac{\\sigma^{2}}{b} + (\\lambda x_{k})^{2} = \\lambda^{2} x_{k}^{2} + \\frac{\\sigma^{2}}{b}$$\n将这些矩代回到 $\\mathbb{E}[f(x_{k+1})]$ 的表达式中：\n$$\\mathbb{E}[f(x_{k+1})] = \\frac{\\lambda}{2} \\left( x_{k}^{2} - 2\\alpha x_{k} (\\lambda x_{k}) + \\alpha^{2} \\left(\\lambda^{2} x_{k}^{2} + \\frac{\\sigma^{2}}{b}\\right) \\right)$$\n$$\\mathbb{E}[f(x_{k+1})] = \\frac{\\lambda}{2} \\left( x_{k}^{2} - 2\\alpha\\lambda x_{k}^{2} + \\alpha^{2}\\lambda^{2} x_{k}^{2} + \\frac{\\alpha^{2}\\sigma^{2}}{b} \\right)$$\n将与 $x_k^2$ 相关的项分组：\n$$\\mathbb{E}[f(x_{k+1})] = \\frac{\\lambda}{2} x_{k}^{2} (1 - 2\\alpha\\lambda + \\alpha^{2}\\lambda^{2}) + \\frac{\\lambda\\alpha^{2}\\sigma^{2}}{2b}$$\n括号中的项是一个完全平方，$(1 - \\alpha\\lambda)^{2}$：\n$$\\mathbb{E}[f(x_{k+1})] = \\frac{\\lambda}{2} x_{k}^{2} (1 - \\alpha\\lambda)^{2} + \\frac{\\lambda\\alpha^{2}\\sigma^{2}}{2b}$$\n现在我们可以计算预期进展 $\\Delta(b) = \\mathbb{E}[f(x_k)] - \\mathbb{E}[f(x_{k+1})]$。由于 $x_k$ 是固定的，$\\mathbb{E}[f(x_k)]=f(x_k)=\\frac{\\lambda}{2}x_k^2$。\n$$\\Delta(b) = \\frac{\\lambda}{2}x_k^2 - \\left( \\frac{\\lambda}{2} x_{k}^{2} (1 - \\alpha\\lambda)^{2} + \\frac{\\lambda\\alpha^{2}\\sigma^{2}}{2b} \\right)$$\n$$\\Delta(b) = \\frac{\\lambda}{2}x_k^2 \\left[ 1 - (1 - \\alpha\\lambda)^{2} \\right] - \\frac{\\lambda\\alpha^{2}\\sigma^{2}}{2b}$$\n展开项 $1 - (1 - \\alpha\\lambda)^{2} = 1 - (1 - 2\\alpha\\lambda + \\alpha^2\\lambda^2) = 2\\alpha\\lambda - \\alpha^2\\lambda^2 = \\alpha\\lambda(2 - \\alpha\\lambda)$。\n$$\\Delta(b) = \\frac{\\lambda}{2}x_k^2 \\left[ \\alpha\\lambda(2 - \\alpha\\lambda) \\right] - \\frac{\\lambda\\alpha^{2}\\sigma^{2}}{2b}$$\n简化后得到预期单步进展的最终表达式：\n$$\\Delta(b) = \\alpha\\lambda^2 x_k^2 \\left(1 - \\frac{\\alpha\\lambda}{2}\\right) - \\frac{\\lambda\\alpha^{2}\\sigma^{2}}{2b}$$\n这个表达式清楚地显示了两个组成部分：一个与批量大小无关的正进展项（对应于确定性梯度步长），以及一个由梯度噪声引起的负项，该项随着批量大小 $b$ 的增加而减小。\n\n每次更新的计算时间由线性模型 $T(b) = t_{0} + c b$ 给出。\n\n进展速率 $R(b)$ 是这两个量的比值：\n$$R(b) = \\frac{\\Delta(b)}{T(b)} = \\frac{\\alpha\\lambda^2 x_k^2 \\left(1 - \\frac{\\alpha\\lambda}{2}\\right) - \\frac{\\lambda\\alpha^{2}\\sigma^{2}}{2b}}{t_{0} + c b}$$\n问题要求找到整数 $b \\in \\{1, 2, \\dots, B_{\\max}\\}$，对于给定的参数集，该整数可以最大化函数 $R(b)$。常数为 $\\lambda = 1$、$\\alpha = 0.5$、$x_{k} = 1$、$t_{0} = 0.02$ 以及 $B_{\\max} = 512$。参数 $(\\sigma^2, c)$ 因测试用例而异。\n\n让我们将固定常数代入 $R(b)$ 的表达式中。\n项 $\\alpha\\lambda^2 x_k^2 \\left(1 - \\frac{\\alpha\\lambda}{2}\\right)$ 变为：\n$$C_1 = (0.5)(1)^2(1)^2 \\left(1 - \\frac{(0.5)(1)}{2}\\right) = 0.5 \\left(1 - 0.25\\right) = 0.5(0.75) = 0.375$$\n噪声项的系数 $\\frac{\\lambda\\alpha^{2}}{2}$ 变为：\n$$\\frac{(1)(0.5)^2}{2} = \\frac{0.25}{2} = 0.125$$\n因此，对于给定参数，$R(b)$ 的完整表达式为：\n$$R(b) = \\frac{0.375 - \\frac{0.125 \\sigma^2}{b}}{0.02 + c b}$$\n找到最优整数 $b$ 的算法方法是执行直接搜索。我们将遍历从 $1$ 到 $B_{\\max} = 512$ 的所有可能的整数值 $b$，为每个值计算 $R(b)$，并找出产生最大 $R(b)$ 的 $b$ 值。对于一个小的搜索空间来说，这是一种简单而稳健的方法。对于给定的测试用例 $(\\sigma^2, c)$，程序将计算 $b=1, 2, ..., 512$ 时的 $R(b)$，并记录导致最高值的 $b$。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves for the optimal mini-batch size b for a series of test cases.\n    The optimization goal is to maximize the expected progress per unit time, R(b).\n    \"\"\"\n\n    # Define the fixed constants from the problem statement.\n    LAMBDA = 1.0  # Curvature\n    ALPHA = 0.5   # Step-size\n    XK = 1.0      # Current position\n    T0 = 0.02     # Fixed overhead time per update (s)\n    B_MAX = 512   # Maximum mini-batch size to search\n\n    # Test suite with pairs of (sigma^2, c)\n    # sigma^2 is the single-sample noise variance (unitless)\n    # c is the compute cost per sample (s/sample)\n    test_cases = [\n        (1.0, 0.002),\n        (10.0, 0.002),\n        (0.01, 0.002),\n        (1.0, 0.02),\n        (5.0, 0.0005),\n    ]\n\n    results = []\n\n    # Pre-calculate the constant part of the numerator of R(b), which is independent of sigma^2 and c.\n    # This term corresponds to alpha * lambda^2 * x_k^2 * (1 - (alpha * lambda) / 2)\n    progress_term_constant = ALPHA * LAMBDA**2 * XK**2 * (1 - (ALPHA * LAMBDA) / 2.)\n\n    for sigma_sq, c in test_cases:\n        best_b = -1\n        max_R = -np.inf\n\n        # Pre-calculate the coefficient of the noise term in the numerator.\n        # This term corresponds to (lambda * alpha^2 * sigma^2) / 2\n        noise_term_coeff = (LAMBDA * ALPHA**2 * sigma_sq) / 2.\n\n        # Search over all allowed integer mini-batch sizes\n        for b in range(1, B_MAX + 1):\n            # Calculate the expected one-step progress, Delta(b)\n            # Delta(b) = progress_term_constant - noise_term_coeff / b\n            delta_b = progress_term_constant - noise_term_coeff / b\n            \n            # The progress rate R(b) can only be maximal if progress Delta(b) is positive.\n            # While we could skip b values where delta_b = 0, the max search handles this naturally.\n\n            # Calculate the computational time per update, T(b)\n            T_b = T0 + c * b\n\n            # Calculate the expected progress per unit time, R(b)\n            # Avoid division by zero, although T_b > 0 is guaranteed by problem constraints.\n            if T_b > 0:\n                R_b = delta_b / T_b\n            else:\n                R_b = -np.inf\n\n            # Update the best batch size if the current one is better\n            if R_b > max_R:\n                max_R = R_b\n                best_b = b\n        \n        results.append(best_b)\n\n    # Format the final output as a comma-separated list in brackets\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3187488"}, {"introduction": "像 Adam 这样的自适应优化器以其快速收敛而闻名，但它们的行为并非总是万无一失。这个高级练习 [@problem_id:3187493] 让您亲手设计一个实验，揭示 Adam 优化器在特定情况下的一个潜在不稳定性。通过从零开始实现 Adam 及其更稳定的变体 AMSGrad，并在一个特殊构造的梯度序列上进行测试，您将对这些前沿算法的内部机制和鲁棒性有更深刻的理解。", "problem": "您需要设计一个确定性随机优化实验，以对比自适应矩估计（Adam）方法和带最大值的自适应矩估计（AMSGrad）方法在不同超参数选择下的行为。重点是构建一个受控的梯度流，在该梯度流中，由于数值稳定性项和指数平均参数选择不当，Adam会发散，而AMSGrad在相同数据上保持稳定。\n\n基本原理必须如下，不能简化：随机优化使用从一系列噪声梯度中计算出的基于梯度的更新。自适应矩方法是基于观测梯度的一阶矩和二阶矩的指数移动平均构建的，并结合了因零初始化而产生的偏差校正。稳定性受到指数平均的衰减参数、学习率以及分母中的加性稳定项之间相互作用的影响。\n\n在一维空间中完整构建该实验。使用 $T$ 次迭代的时间范围，由 $C$ 个长度为 $L$ 的相同周期生成，因此 $T = C L$。在每个周期内，使用单个正向脉冲后跟多个小的负值来定义梯度流 $\\{g_t\\}_{t=1}^T$：\n- 设脉冲幅度为 $S  0$。\n- 对于每个周期，在周期的第一步设置 $g_{t} = S$。\n- 对于该周期余下的 $L-1$ 步，设置 $g_{t} = -s$，其中 $s = \\dfrac{S}{L-1}$，这样每个周期内的梯度总和恰好为 $0$。\n\n这种构造产生了一个非平稳的梯度二阶矩，其具有间歇性的尖峰和长时间的小幅值平静期，用以测试自适应方法的敏感性。在 $x_0 = 0$ 处初始化参数，并使用所选的自适应方法执行参数更新。两种自适应方法都必须根据指数移动平均和偏差校正的定义来实现，并使用以下超参数：\n- 学习率 $\\,\\alpha  0\\,$。\n- 一阶矩衰减率 $\\,\\beta_1 \\in (0,1)\\,$。\n- 二阶矩衰减率 $\\,\\beta_2 \\in (0,1)\\,$。\n- 加性稳定项 $\\,\\varepsilon  0\\,$。\n\n如果最终迭代的幅值超过一个固定界限或变为非有限数，则认为一次运行已经发散。具体来说，定义发散阈值 $D$，如果 $|x_T|  D$ 或 $x_T$ 不是一个有限实数，则称该方法发散。\n\n您的程序必须：\n- 从指数移动平均和偏差校正的第一性原理出发，实现Adam和AMSGrad。\n- 按照描述生成梯度流，其中 $L = 200$，$S = 1$，$C = 8$，因此 $T = 1600$。\n- 对每个测试用例，在完全相同的梯度流上运行这两种方法。\n- 使用初始迭代值 $x_0 = 0$ 和发散阈值 $D = 10$。\n\n使用以下超参数集 $\\left(\\alpha, \\beta_1, \\beta_2, \\varepsilon\\right)$ 的测试套件：\n1. $\\left(10^{-2},\\, 0.99,\\, 0.1,\\, 10^{-20}\\right)$，这是一个 $\\,\\varepsilon\\,$ 非常小且 $\\,\\beta_2\\,$ 相对较小的案例，当二阶矩估计值迅速下降时，应会引起Adam的不稳定性。\n2. $\\left(10^{-2},\\, 0.9,\\, 0.999,\\, 10^{-8}\\right)$，这是一个典型的选择，预计对Adam和AMSGrad都稳定。\n3. $\\left(10^{-2},\\, 0.9,\\, 10^{-6},\\, 10^{-32}\\right)$，这是一个 $\\,\\beta_2\\,$ 和 $\\,\\varepsilon\\,$ 极小的案例，旨在展示Adam的敏感性。\n4. $\\left(2 \\times 10^{-4},\\, 0.99,\\, 0.1,\\, 10^{-20}\\right)$，这是一个学习率非常小的边界情况，即使在 $\\,\\beta_2\\,$ 和 $\\,\\varepsilon\\,$ 不利的情况下，也有望避免发散。\n\n对于每个测试用例，按固定顺序返回两个布尔值结果：\n- 第一个布尔值：Adam在该案例中是否发散。\n- 第二个布尔值：AMSGrad在该案例中是否发散。\n\n最终输出格式：您的程序应生成单行输出，其中包含所有八个布尔值，以逗号分隔列表的形式包含在方括号内，按测试用例排序为 $[ \\text{Adam 案例 1}, \\text{AMSGrad 案例 1}, \\text{Adam 案例 2}, \\text{AMSGrad 案例 2}, \\text{Adam 案例 3}, \\text{AMSGrad 案例 3}, \\text{Adam 案例 4}, \\text{AMSGrad 案例 4} ]$。", "solution": "问题陈述被评估为有效。它提出了一个在随机优化领域中定义明确、有科学依据且客观的计算实验。所有必要的参数和定义都已提供，任务是在旨在突显Adam已知潜在失败模式的特定条件下，实现并比较两种标准算法：Adam和AMSGrad。\n\n本实验的目标是展示一个场景，在该场景中，自适应矩估计（Adam）优化器会发散，而其变体AMSGrad则保持稳定。这是通过构建一个特定的非平稳梯度流并选择能够暴露Adam对梯度二阶矩估计快速变化的敏感性的超参数来实现的。\n\n首先，我们定义梯度流 $\\{g_t\\}_{t=1}^T$。该流构建在总共 $T$ 个时间步上，由 $C$ 个长度为 $L$ 的相同周期组成。问题指定了 $C=8$ 个周期和周期长度 $L=200$，总时间范围为 $T = C \\times L = 8 \\times 200 = 1600$ 步。在每个周期内，梯度是一个大的正向脉冲 $g_t = S$，后跟 $L-1$ 个小的负梯度。问题将脉冲幅度设置为 $S=1$。为确保单个周期内的梯度总和为零，小的负梯度值 $s$ 必须是 $s = \\frac{S}{L-1} = \\frac{1}{200-1} = \\frac{1}{199}$。\n因此，在任何时间步 $t \\in \\{1, 2, \\dots, 1600\\}$ 的梯度由下式给出：\n$$\ng_t =\n\\begin{cases}\n    S = 1  \\text{if } (t-1) \\pmod{L} = 0 \\\\\n    -s = -\\frac{1}{199}  \\text{if } (t-1) \\pmod{L} \\neq 0\n\\end{cases}\n$$\n这种结构确保了梯度的二阶矩 $g_t^2$ 会经历大的尖峰 ($S^2=1$)，随后是长时间的非常小的值 ($s^2 \\approx 2.5 \\times 10^{-5}$)。\n\n优化过程从初始参数值 $x_0 = 0$ 开始。我们将实现并比较两种优化算法：Adam和AMSGrad。两者都依赖于梯度一阶矩和二阶矩的指数移动平均。超参数包括学习率 $\\alpha$、一阶矩衰减率 $\\beta_1$、二阶矩衰减率 $\\beta_2$ 和数值稳定性项 $\\varepsilon$。\n\nAdam算法的更新如下。我们将一阶矩和二阶矩向量初始化为 $m_0 = 0$ 和 $v_0 = 0$。对于每个时间步 $t = 1, \\dots, T$：\n$1$. 更新有偏一阶矩估计：$m_t = \\beta_1 m_{t-1} + (1 - \\beta_1) g_t$。\n$2$. 更新有偏二阶矩估计：$v_t = \\beta_2 v_{t-1} + (1 - \\beta_2) g_t^2$。\n$3$. 计算偏差校正后的一阶矩估计：$\\hat{m}_t = \\frac{m_t}{1 - \\beta_1^t}$。\n$4$. 计算偏差校正后的二阶矩估计：$\\hat{v}_t = \\frac{v_t}{1 - \\beta_2^t}$。\n$5$. 更新参数：$x_t = x_{t-1} - \\alpha \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\varepsilon}$。\n\nAMSGrad算法修改了Adam的更新规则，以确保有效学习率不会增加。我们初始化 $m_0 = 0$，$v_0 = 0$，并额外初始化 $v_{\\max, 0} = 0$。对于每个时间步 $t = 1, \\dots, T$：\n$1$. 更新有偏一阶矩估计：$m_t = \\beta_1 m_{t-1} + (1 - \\beta_1) g_t$。\n$2$. 更新有偏二阶矩估计：$v_t = \\beta_2 v_{t-1} + (1 - \\beta_2) g_t^2$。\n$3$. 计算偏差校正后的一阶矩估计：$\\hat{m}_t = \\frac{m_t}{1 - \\beta_1^t}$。\n$4$. 维持迄今为止所见二阶矩估计的最大值：$v_{\\max, t} = \\max(v_{\\max, t-1}, v_t)$。\n$5$. 使用此最大值更新参数：$x_t = x_{t-1} - \\alpha \\frac{\\hat{m}_t}{\\sqrt{v_{\\max, t}} + \\varepsilon}$。\n\n关键区别在于更新规则的分母。在Adam中，如果最近的梯度很小，特别是如果 $\\beta_2$ 很小（即短时记忆），$\\hat{v}_t$ 项可能会减小。如果 $\\hat{v}_t$ 变得非常小，有效学习率 $\\alpha / (\\sqrt{\\hat{v}_t} + \\varepsilon)$ 可能会爆炸，导致发散。AMSGrad通过使用 $v_{\\max, t}$ 来防止这种情况，该值是非递减的，从而防止有效学习率不受控制地增长。\n\n如果最终参数值 $x_T$ 的幅值大于阈值 $D=10$，或者 $x_T$ 是一个非有限数（例如，无穷大或NaN），则认为一次运行已经发散。\n\n实验针对四个不同的超参数集 $(\\alpha, \\beta_1, \\beta_2, \\varepsilon)$ 运行：\n$1$. $(10^{-2}, 0.99, 0.1, 10^{-20})$：小的 $\\beta_2$ 和极小的 $\\varepsilon$ 预计会导致Adam发散，因为大梯度尖峰的记忆会迅速消失，分母会急剧缩小。AMSGrad应保持稳定。\n$2$. $(10^{-2}, 0.9, 0.999, 10^{-8})$：大的 $\\beta_2$ 使二阶矩估计具有长时记忆，这应该能保持Adam的稳定。预计两种方法都将保持稳定。\n$3$. $(10^{-2}, 0.9, 10^{-6}, 10^{-32})$：极小的 $\\beta_2$ 和 $\\varepsilon$ 提供了一个更严峻的测试案例，强烈有利于AMSGrad的稳定性机制。Adam极有可能发散。\n$4$. $(2 \\times 10^{-4}, 0.99, 0.1, 10^{-20})$：非常小的学习率 $\\alpha$ 应该足以抑制更新步骤，从而防止Adam发散，即使 $\\beta_2$ 和 $\\varepsilon$ 的值存在问题。\n\n对于每种情况，我们将在相同的梯度流上模拟Adam和AMSGrad，并为每种方法报告一个指示是否发散的布尔值。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Designs and runs a deterministic stochastic optimization experiment\n    to contrast Adam and AMSGrad, implementing both from first principles.\n    \"\"\"\n    \n    # Define problem constants\n    L = 200  # Cycle length\n    S = 1.0  # Burst magnitude\n    C = 8    # Number of cycles\n    T = L * C  # Total iterations\n    D = 10.0 # Divergence threshold\n    \n    # Generate the gradient stream\n    s = S / (L - 1)\n    gradients = np.full(T, -s)\n    cycle_starts = np.arange(0, T, L)\n    gradients[cycle_starts] = S\n\n    # Define the test suite of hyperparameters (alpha, beta1, beta2, epsilon)\n    test_cases = [\n        (1e-2, 0.99, 0.1, 1e-20),\n        (1e-2, 0.9, 0.999, 1e-8),\n        (1e-2, 0.9, 1e-6, 1e-32),\n        (2e-4, 0.99, 0.1, 1e-20),\n    ]\n\n    results = []\n    \n    for params in test_cases:\n        alpha, beta1, beta2, epsilon = params\n\n        # --- Adam Simulation ---\n        x_adam = 0.0\n        m_adam = 0.0\n        v_adam = 0.0\n        beta1_power_t = 1.0\n        beta2_power_t = 1.0\n        \n        for i in range(T):\n            grad = gradients[i]\n            \n            # Update powers for bias correction\n            beta1_power_t *= beta1\n            beta2_power_t *= beta2\n            \n            # Update biased moment estimates\n            m_adam = beta1 * m_adam + (1.0 - beta1) * grad\n            v_adam = beta2 * v_adam + (1.0 - beta2) * (grad**2)\n            \n            # Compute bias-corrected estimates\n            m_hat = m_adam / (1.0 - beta1_power_t)\n            v_hat = v_adam / (1.0 - beta2_power_t)\n            \n            # Parameter update\n            # Precaution: ensure v_hat is non-negative before sqrt\n            if v_hat  0: v_hat = 0\n            x_adam -= alpha * m_hat / (np.sqrt(v_hat) + epsilon)\n            \n            # Early exit on non-finite value\n            if not np.isfinite(x_adam):\n                break\n\n        adam_diverged = np.abs(x_adam) > D or not np.isfinite(x_adam)\n        results.append(adam_diverged)\n\n        # --- AMSGrad Simulation ---\n        x_ams = 0.0\n        m_ams = 0.0\n        v_ams = 0.0\n        v_max_ams = 0.0\n        beta1_power_t_ams = 1.0\n        \n        for i in range(T):\n            grad = gradients[i]\n            \n            # Update powers for bias correction\n            beta1_power_t_ams *= beta1\n\n            # Update biased moment estimates\n            m_ams = beta1 * m_ams + (1.0 - beta1) * grad\n            v_ams = beta2 * v_ams + (1.0 - beta2) * (grad**2)\n            \n            # Maintain the maximum of the second moment estimate\n            v_max_ams = max(v_max_ams, v_ams)\n\n            # Compute bias-corrected first moment estimate\n            m_hat = m_ams / (1.0 - beta1_power_t_ams)\n            \n            # Parameter update\n            # No need to check for negative v_max_ams as it's non-decreasing from 0.\n            x_ams -= alpha * m_hat / (np.sqrt(v_max_ams) + epsilon)\n            \n            # Early exit on non-finite value\n            if not np.isfinite(x_ams):\n                break\n\n        amsgrad_diverged = np.abs(x_ams) > D or not np.isfinite(x_ams)\n        results.append(amsgrad_diverged)\n\n    # Format the final output as specified\n    print(f\"[{','.join(str(r).lower() for r in results)}]\")\n\nsolve()\n```", "id": "3187493"}]}