{"hands_on_practices": [{"introduction": "小批量随机方法的核心是在计算效率与梯度估计的准确性之间取得平衡。然而，当一个小批量中的样本恰好具有异常或极端的值时，梯度估计的质量可能会严重下降。在本练习中 ([@problem_id:3150590])，你将通过扮演“对抗者”的角色，主动挑选样本以最大化梯度估计的误差，从而直观地理解梯度方差带来的挑战。此外，你还将探索梯度裁剪作为一种有效的防御策略，并量化其对降低最坏情况误差的影响。", "problem": "考虑一个一维的逐样本梯度数据集，由多重集 $\\{-4,-1,0,0,1,5\\}$ 给出，并假设使用大小为 $b=3$ 的小批量来构建一个小批量梯度估计器。令全数据梯度为所有逐样本梯度的算术平均值 $\\bar{g}$，小批量梯度估计器为所选小批量 $B$ 中梯度的算术平均值 $\\hat{g}(B)$。在标准的小批量随机方法中，当 $B$ 是随机采样时，$\\hat{g}(B)$ 旨在成为 $\\bar{g}$ 的一个低方差估计。在对抗性小批量构建中，对手选择一个固定大小为 $b=3$ 的小批量 $B$，以最大化估计器与真实梯度之间的最坏情况平方偏差，该偏差由均方误差 (MSE) 度量 $R(B)=(\\hat{g}(B)-\\bar{g})^{2}$ 定义。\n\n从算术平均值的基本定义和平均值的基本性质出发，推导出最大化 $R(B)$ 的对抗性选择形式，并计算精确的最坏情况值\n$$\nR^{\\star}=\\max_{\\substack{B\\subset\\{-4,-1,0,0,1,5\\}\\\\|B|=3}}(\\hat{g}(B)-\\bar{g})^{2}.\n$$\n\n作为一种缓解策略，考虑对每个逐样本梯度进行标量梯度裁剪，裁剪到区间 $[-\\tau,\\tau]$，阈值为 $\\tau=3$。定义裁剪后的梯度映射为 $\\phi(g)=\\max\\{-\\tau,\\min\\{g,\\tau\\}\\}$，裁剪后的数据集为 $\\{\\phi(g):g\\in\\{-4,-1,0,0,1,5\\}\\}$。令 $\\bar{g}_{\\tau}$ 为裁剪后梯度的均值，$\\hat{g}_{\\tau}(B)$ 为所选小批量中裁剪后梯度的均值。计算精确的最坏情况裁剪值\n$$\nR^{\\star}_{\\tau}=\\max_{\\substack{B\\subset\\{\\phi(g):g\\in\\{-4,-1,0,0,1,5\\}\\}\\\\|B|=3}}(\\hat{g}_{\\tau}(B)-\\bar{g}_{\\tau})^{2}.\n$$\n\n将您的最终答案以包含 $R^{\\star}$ 和 $R^{\\star}_{\\tau}$ 的单行矩阵形式报告，使用精确形式。无需四舍五入。", "solution": "该问题要求计算两个量，它们表示小批量梯度估计器与真实全数据梯度之间的最坏情况平方偏差。第一个量 $R^{\\star}$ 适用于原始数据集，第二个量 $R^{\\star}_{\\tau}$ 适用于每个梯度都经过标量裁剪的数据集。\n\n首先，我们分析未裁剪的情况以确定 $R^{\\star}$。\n给定的逐样本梯度数据集是多重集 $S = \\{-4, -1, 0, 0, 1, 5\\}$。样本总数为 $N=6$。\n全数据梯度 $\\bar{g}$ 是这些值的算术平均值：\n$$\n\\bar{g} = \\frac{1}{N} \\sum_{g \\in S} g = \\frac{1}{6} (-4 + (-1) + 0 + 0 + 1 + 5) = \\frac{1}{6}\n$$\n对于大小为 $b=3$ 的小批量 $B$，其小批量梯度估计器 $\\hat{g}(B)$ 是该小批量中梯度的均值：\n$$\n\\hat{g}(B) = \\frac{1}{b} \\sum_{g \\in B} g = \\frac{1}{3} \\sum_{g \\in B} g\n$$\n我们的任务是找到在所有可能的大小为 3 的小批量 $B$ 上，平方偏差 $R(B) = (\\hat{g}(B) - \\bar{g})^2$ 的最大可能值。该值记为 $R^{\\star}$：\n$$\nR^{\\star} = \\max_{B \\subset S, |B|=3} (\\hat{g}(B) - \\bar{g})^2\n$$\n为了最大化这个表达式，对手必须选择一个能最大化绝对差 $|\\hat{g}(B) - \\bar{g}|$ 的小批量 $B$。当小批量均值 $\\hat{g}(B)$ 尽可能大或尽可能小时，就会发生这种情况。$\\hat{g}(B)$ 的值与 $B$ 中元素的总和成正比。\n\n为了找到最大可能均值 $\\hat{g}_{max}$，我们从 $S$ 中选择 $b=3$ 个最大的梯度值。它们是 $\\{0, 1, 5\\}$。设这个批量为 $B_{max}$。其元素的总和是 $0 + 1 + 5 = 6$。对应的均值是：\n$$\n\\hat{g}(B_{max}) = \\frac{1}{3}(6) = 2\n$$\n为了找到最小可能均值 $\\hat{g}_{min}$，我们从 $S$ 中选择 $b=3$ 个最小的梯度值。它们是 $\\{-4, -1, 0\\}$。设这个批量为 $B_{min}$。其元素的总和是 $-4 + (-1) + 0 = -5$。对应的均值是：\n$$\n\\hat{g}(B_{min}) = \\frac{1}{3}(-5) = -\\frac{5}{3}\n$$\n我们现在评估这两种极端情况下与真实均值 $\\bar{g} = \\frac{1}{6}$ 的偏差：\n$\\hat{g}(B_{max})$ 的偏差是 $2 - \\frac{1}{6} = \\frac{12}{6} - \\frac{1}{6} = \\frac{11}{6}$。\n$\\hat{g}(B_{min})$ 的偏差是 $-\\frac{5}{3} - \\frac{1}{6} = -\\frac{10}{6} - \\frac{1}{6} = -\\frac{11}{6}$。\n因此，最大绝对偏差是 $\\left|\\frac{11}{6}\\right| = \\frac{11}{6}$。\n最坏情况的平方偏差是这个值的平方：\n$$\nR^{\\star} = \\left(\\frac{11}{6}\\right)^2 = \\frac{121}{36}\n$$\n\n接下来，我们分析裁剪后的情况以确定 $R^{\\star}_{\\tau}$。\n梯度裁剪使用阈值 $\\tau=3$ 和函数 $\\phi(g)=\\max\\{-\\tau,\\min\\{g,\\tau\\}\\}$ 进行。我们将此函数应用于原始数据集 $S$ 中的每个梯度：\n$\\phi(-4) = \\max\\{-3, \\min\\{-4, 3\\}\\} = -3$\n$\\phi(-1) = \\max\\{-3, \\min\\{-1, 3\\}\\} = -1$\n$\\phi(0) = 0$\n$\\phi(1) = 1$\n$\\phi(5) = \\max\\{-3, \\min\\{5, 3\\}\\} = 3$\n裁剪后的数据集是多重集 $S_{\\tau} = \\{-3, -1, 0, 0, 1, 3\\}$。\n裁剪后梯度的均值 $\\bar{g}_{\\tau}$ 是：\n$$\n\\bar{g}_{\\tau} = \\frac{1}{N} \\sum_{g' \\in S_{\\tau}} g' = \\frac{1}{6}(-3 - 1 + 0 + 0 + 1 + 3) = \\frac{0}{6} = 0\n$$\n最坏情况的裁剪后平方偏差 $R^{\\star}_{\\tau}$ 是在所有来自裁剪后集合 $S_{\\tau}$ 的大小为 3 的小批量 $B$ 上，$(\\hat{g}_{\\tau}(B) - \\bar{g}_{\\tau})^2$ 的最大值。由于 $\\bar{g}_{\\tau} = 0$，这简化为最大化 $(\\hat{g}_{\\tau}(B))^2$。这等价于最大化 $|\\hat{g}_{\\tau}(B)|$。\n和之前一样，我们从 $S_{\\tau}$ 中找到具有最大和最小总和的小批量。\n对于最大总和，我们从 $S_{\\tau}$ 中选择 $b=3$ 个最大的元素：$\\{0, 1, 3\\}$。总和为 $0 + 1 + 3 = 4$。均值为 $\\hat{g}_{\\tau,max} = \\frac{4}{3}$。\n对于最小总和，我们从 $S_{\\tau}$ 中选择 $b=3$ 个最小的元素：$\\{-3, -1, 0\\}$。总和为 $-3 - 1 + 0 = -4$。均值为 $\\hat{g}_{\\tau,min} = -\\frac{4}{3}$。\n极值均值为 $\\frac{4}{3}$ 和 $-\\frac{4}{3}$。最大绝对值为 $\\left|\\frac{4}{3}\\right| = \\frac{4}{3}$。\n最坏情况的裁剪后平方偏差是这个值的平方：\n$$\nR^{\\star}_{\\tau} = \\left(\\frac{4}{3}\\right)^2 = \\frac{16}{9}\n$$\n问题要求将 $R^{\\star}$ 和 $R^{\\star}_{\\tau}$ 作为行矩阵报告。\n我们得到 $R^{\\star} = \\frac{121}{36}$ 和 $R^{\\star}_{\\tau} = \\frac{16}{9}$。", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{121}{36}  \\frac{16}{9}\n\\end{pmatrix}\n}\n$$", "id": "3150590"}, {"introduction": "简单的算术平均意味着每个样本在梯度估计中具有同等的重要性，但这并非总是最佳策略。如果某些样本的梯度估计本身就比其他样本更“嘈杂”（即方差更大），那么赋予它们相同的权重就不尽合理。这个练习 ([@problem_id:3150573]) 将引导你推导出一个最优的加权方案，即“逆方差加权”，它通过为方差较小的（更可靠的）样本分配更高的权重，来最小化最终梯度估计器的方差。这揭示了通过智能地结合来自不同样本的信息来构造更优估计器的基本原理。", "problem": "考虑在小批量设置下，用于单参数模型的随机梯度下降 (SGD)。设一个小批量由索引集 $B=\\{1,2,3\\}$ 表示，对于每个 $i \\in B$，令 $X_i$ 为一个标量随机梯度估计，该估计满足 $\\mathbb{E}[X_i]=\\mu$ (无偏性) 和 $\\operatorname{Var}(X_i)=\\sigma_i^{2}$。假设随机变量 $X_i$ 是相互独立的。一个加权小批量梯度估计量形式为 $\\widehat{g}=\\sum_{i \\in B} w_i X_i$，其中每个样本的权重满足约束 $\\sum_{i \\in B} w_i=1$ 以保持无偏性，即 $\\mathbb{E}[\\widehat{g}]=\\mu$。 \n\n从期望和方差的定义出发，仅使用独立性和线性的标准性质，推导在约束 $\\sum_{i \\in B} w_i=1$ 下能最小化方差 $\\operatorname{Var}(\\widehat{g})$ 的权重。然后，对于一个方差分别为 $\\sigma_1^2=1$, $\\sigma_2^2=4$ 和 $\\sigma_3^2=9$ 的小批量，计算最优权重。 \n\n将你的最终答案表示为按 $(w_1, w_2, w_3)$ 顺序排列的三个最优权重的行向量。无需四舍五入。", "solution": "我们的目标是找到一组权重 $(w_1, w_2, w_3)$，以最小化加权小批量梯度估计量 $\\widehat{g} = \\sum_{i=1}^3 w_i X_i$ 的方差，同时满足无偏性约束 $\\sum_{i=1}^3 w_i = 1$。\n\n首先，我们推导 $\\widehat{g}$ 的方差表达式。由于随机变量 $X_i$ 是相互独立的，加权和的方差是方差的加权和：\n$$\n\\operatorname{Var}(\\widehat{g}) = \\operatorname{Var}\\left(\\sum_{i=1}^3 w_i X_i\\right) = \\sum_{i=1}^3 \\operatorname{Var}(w_i X_i)\n$$\n利用方差的性质 $\\operatorname{Var}(aY) = a^2 \\operatorname{Var}(Y)$ 和给定的 $\\operatorname{Var}(X_i) = \\sigma_i^2$，我们得到：\n$$\n\\operatorname{Var}(\\widehat{g}) = \\sum_{i=1}^3 w_i^2 \\sigma_i^2 = w_1^2 \\sigma_1^2 + w_2^2 \\sigma_2^2 + w_3^2 \\sigma_3^2\n$$\n这是一个约束优化问题。我们希望在约束 $g(w_1, w_2, w_3) = w_1 + w_2 + w_3 - 1 = 0$ 下最小化上述方差函数。我们使用拉格朗日乘数法来解决这个问题。拉格朗日函数 $\\mathcal{L}$ 定义为：\n$$\n\\mathcal{L}(w_1, w_2, w_3, \\lambda) = (w_1^2 \\sigma_1^2 + w_2^2 \\sigma_2^2 + w_3^2 \\sigma_3^2) - \\lambda(w_1 + w_2 + w_3 - 1)\n$$\n为了找到最优权重，我们将 $\\mathcal{L}$ 对每个 $w_i$ 的偏导数设为零：\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial w_i} = 2 w_i \\sigma_i^2 - \\lambda = 0\n$$\n从上式解出 $w_i$：\n$$\nw_i = \\frac{\\lambda}{2 \\sigma_i^2}\n$$\n这个结果表明，最优权重与对应梯度估计的方差成反比。接下来，我们将这个关系代入约束方程 $\\sum w_i = 1$ 中，以求解拉格朗日乘数 $\\lambda$：\n$$\n\\sum_{i=1}^3 \\frac{\\lambda}{2 \\sigma_i^2} = 1 \\implies \\frac{\\lambda}{2} \\left( \\frac{1}{\\sigma_1^2} + \\frac{1}{\\sigma_2^2} + \\frac{1}{\\sigma_3^2} \\right) = 1\n$$\n解出 $\\lambda$：\n$$\n\\lambda = \\frac{2}{\\sum_{j=1}^3 1/\\sigma_j^2}\n$$\n将 $\\lambda$ 的表达式代回 $w_i$ 的方程中，我们得到最优权重的一般公式：\n$$\nw_i = \\frac{1}{2 \\sigma_i^2} \\cdot \\frac{2}{\\sum_{j=1}^3 1/\\sigma_j^2} = \\frac{1/\\sigma_i^2}{\\sum_{j=1}^3 1/\\sigma_j^2}\n$$\n现在，我们使用给定的方差值 $\\sigma_1^2=1$, $\\sigma_2^2=4$ 和 $\\sigma_3^2=9$ 来计算具体的权重。首先，计算逆方差之和：\n$$\n\\sum_{j=1}^3 \\frac{1}{\\sigma_j^2} = \\frac{1}{1} + \\frac{1}{4} + \\frac{1}{9} = \\frac{36}{36} + \\frac{9}{36} + \\frac{4}{36} = \\frac{49}{36}\n$$\n然后，我们计算每个权重：\n$$\nw_1 = \\frac{1/1}{49/36} = \\frac{36}{49}\n$$\n$$\nw_2 = \\frac{1/4}{49/36} = \\frac{1}{4} \\cdot \\frac{36}{49} = \\frac{9}{49}\n$$\n$$\nw_3 = \\frac{1/9}{49/36} = \\frac{1}{9} \\cdot \\frac{36}{49} = \\frac{4}{49}\n$$\n因此，最优权重向量为 $(\\frac{36}{49}, \\frac{9}{49}, \\frac{4}{49})$。我们可以验证这些权重的和为 $\\frac{36+9+4}{49} = \\frac{49}{49} = 1$，满足约束条件。", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{36}{49}  \\frac{9}{49}  \\frac{4}{49}\n\\end{pmatrix}\n}\n$$", "id": "3150573"}, {"introduction": "经典优化理论通常假设随机梯度中存在不会消失的噪声，这使得学习率需要衰减才能收敛。然而，在现代深度学习中，模型常常在“插值区域”（interpolating regime）工作，即它们有能力完美拟合所有训练数据。这个高级练习 ([@problem_id:3150601]) 将带你探索小批量SGD在这一特殊制度下的独特动态。你将通过精确的数学推导发现，在这种情况下，随机梯度噪声在最优解处会完全消失，从而使得算法的行为与经典理论的预测有所不同。", "problem": "考虑标量线性回归，数据为 $\\{(a_{i}, b_{i})\\}_{i=1}^{n}$，其中 $a_{i} \\in \\mathbb{R}$ 且 $b_{i} \\in \\mathbb{R}$。定义单样本损失为 $f_{i}(x) = \\frac{1}{2}\\left(a_{i} x - b_{i}\\right)^{2}$，经验风险为 $f(x) = \\frac{1}{n}\\sum_{i=1}^{n} f_{i}(x)$。假设插值设定成立：存在 $x^{\\ast} \\in \\mathbb{R}$，使得对于所有 $i \\in \\{1,\\dots,n\\}$ 都有 $a_{i} x^{\\ast} = b_{i}$。\n\n考虑小批量随机梯度下降 (SGD)，批量大小为 $m \\in \\{1,\\dots,n\\}$，学习率为 $\\eta > 0$。在每次迭代 $k$ 中，从 $\\{1,\\dots,n\\}$ 中无放回均匀采样一个大小为 $m$ 的子集 $B_{k}$，更新方式为\n$$\nx_{k+1} \\;=\\; x_{k} \\;-\\; \\eta \\cdot \\frac{1}{m}\\sum_{i \\in B_{k}} \\nabla f_{i}(x_{k}).\n$$\n令误差为 $e_{k} = x_{k} - x^{\\ast}$。定义 $H = \\frac{1}{n}\\sum_{i=1}^{n} a_{i}^{2}$ 以及集合 $\\{a_{i}^{2}\\}_{i=1}^{n}$ 的有限总体方差 $\\sigma^{2} = \\frac{1}{n}\\sum_{i=1}^{n}\\left(a_{i}^{2} - H\\right)^{2}$。\n\n任务：\n1) 推导 $e_{k+1}$ 关于 $e_{k}$ 和一个依赖于批量的标量的精确小批量动态，并明确证明为何在此插值设定下，随机性在 $x^{\\ast}$ 处消失。\n\n2) 仅使用基本定义和关于无放回采样下样本均值的期望和方差的公认事实，推导 $\\mathbb{E}\\!\\left[e_{k+1}^{2}\\,\\middle|\\, e_{k}\\right]$ 作为 $e_{k}$, $\\eta$, $H$, $\\sigma^{2}$, $n$ 和 $m$ 的函数的显式表达式。\n\n3) 对于固定的 $e_{k} \\neq 0$，确定能最小化 $\\mathbb{E}\\!\\left[e_{k+1}^{2}\\,\\middle|\\, e_{k}\\right]$ 的学习率 $\\eta$，并将其表示为 $H$, $\\sigma^{2}$, $n$ 和 $m$ 的闭式解析表达式。\n\n将任务3中得到的最小化学习率的表达式作为你的最终答案。无需进行数值评估。如果得到多种代数等价形式，请以最简闭式形式给出其中任意一种。不要包含单位。", "solution": "该问题提出了一个插值设定下的标量线性回归问题，并要求对采用无放回采样的小批量随机梯度下降（SGD）进行分析。任务涉及推导误差动态、条件期望平方误差以及最优学习率。\n\n**任务1：推导误差动态并分析在 $x^{\\ast}$ 处的随机性**\n\n首先，我们计算单样本损失函数 $f_{i}(x)$ 的梯度：\n$$\n\\nabla f_{i}(x) = \\frac{d}{dx} \\left[ \\frac{1}{2}(a_{i} x - b_{i})^{2} \\right] = (a_{i} x - b_{i}) \\cdot a_{i} = a_{i}^{2}x - a_{i}b_{i}.\n$$\n利用插值条件 $b_{i} = a_{i}x^{\\ast}$，我们可以用误差 $e_{k} = x_{k} - x^{\\ast}$ 来表示梯度：\n$$\n\\nabla f_{i}(x_{k}) = a_{i}^{2}x_{k} - a_{i}(a_{i}x^{\\ast}) = a_{i}^{2}(x_{k} - x^{\\ast}) = a_{i}^{2}e_{k}.\n$$\n现在，将此代入小批量SGD更新规则中：\n$$\nx_{k+1} = x_{k} - \\eta \\cdot \\frac{1}{m}\\sum_{i \\in B_{k}} \\nabla f_{i}(x_{k}) = x_{k} - \\frac{\\eta}{m}\\sum_{i \\in B_{k}} (a_{i}^{2}e_{k}).\n$$\n为了找到误差 $e_{k+1} = x_{k+1} - x^{\\ast}$ 的动态，我们从等式两边减去 $x^{\\ast}$：\n$$\nx_{k+1} - x^{\\ast} = (x_{k} - x^{\\ast}) - \\frac{\\eta}{m}e_{k}\\sum_{i \\in B_{k}} a_{i}^{2}.\n$$\n这就得到了误差的精确小批量动态：\n$$\ne_{k+1} = e_{k} - \\left(\\frac{\\eta}{m}\\sum_{i \\in B_{k}} a_{i}^{2}\\right) e_{k} = \\left(1 - \\eta \\cdot \\frac{1}{m}\\sum_{i \\in B_{k}} a_{i}^{2}\\right) e_{k}.\n$$\n这个表达式表明 $e_{k+1}$ 与 $e_{k}$ 成正比，比例常数依赖于随机采样的批量 $B_{k}$。这个依赖于批量的标量是 $\\left(1 - \\eta \\cdot \\frac{1}{m}\\sum_{i \\in B_{k}} a_{i}^{2}\\right)$。\n\n为了证明随机性在 $x^{\\ast}$ 处消失，我们计算在 $x_{k} = x^{\\ast}$ 处的随机梯度。此时误差为 $e_{k} = x^{\\ast} - x^{\\ast} = 0$。\n在 $x^{\\ast}$ 处的单样本梯度为：\n$$\n\\nabla f_{i}(x^{\\ast}) = a_{i}^{2}(x^{\\ast} - x^{\\ast}) = 0 \\quad \\text{for all } i \\in \\{1,\\dots,n\\}.\n$$\n因此，小批量梯度为：\n$$\n\\frac{1}{m}\\sum_{i \\in B_{k}} \\nabla f_{i}(x^{\\ast}) = \\frac{1}{m}\\sum_{i \\in B_{k}} 0 = 0.\n$$\n这个结果对任何批量 $B_{k}$ 都成立。由于随机梯度恒等于零，不依赖于批量的随机选择，因此在解 $x^{\\ast}$ 处随机性消失。在 $x_k = x^*$ 处的更新是 $x_{k+1} = x^* - \\eta \\cdot 0 = x^*$，这是确定性的。\n\n**任务2：推导条件期望平方误差 $\\mathbb{E}[e_{k+1}^{2} | e_{k}]$**\n\n从任务1的误差动态开始：\n$$\ne_{k+1} = \\left(1 - \\eta H_{B_{k}}\\right)e_{k}, \\quad \\text{其中} \\quad H_{B_{k}} = \\frac{1}{m}\\sum_{i \\in B_{k}} a_{i}^{2}.\n$$\n两边平方得到：\n$$\ne_{k+1}^{2} = \\left(1 - \\eta H_{B_{k}}\\right)^{2} e_{k}^{2}.\n$$\n我们对 $e_{k}$ 取条件期望。由于在第 $k$ 次迭代时 $e_{k}$ 是已知的，它被视为一个常数。随机性来自于批量 $B_{k}$。\n$$\n\\mathbb{E}[e_{k+1}^{2} | e_{k}] = \\mathbb{E}\\left[\\left(1 - \\eta H_{B_{k}}\\right)^{2} \\mid e_{k}\\right] e_{k}^{2}.\n$$\n展开平方项并利用期望的线性性质：\n$$\n\\mathbb{E}\\left[\\left(1 - \\eta H_{B_{k}}\\right)^{2}\\right] = \\mathbb{E}\\left[1 - 2\\eta H_{B_{k}} + \\eta^{2} H_{B_{k}}^{2}\\right] = 1 - 2\\eta\\mathbb{E}[H_{B_{k}}] + \\eta^{2}\\mathbb{E}[H_{B_{k}}^{2}].\n$$\n我们需要计算样本均值 $H_{B_{k}}$ 的前两阶矩。令值的总体为 $\\{Z_{i} = a_{i}^{2}\\}_{i=1}^{n}$。总体均值为 $\\mu_{Z} = \\frac{1}{n}\\sum_{i=1}^{n} Z_{i} = H$。总体方差为 $\\frac{1}{n}\\sum_{i=1}^{n}(Z_{i} - \\mu_{Z})^{2} = \\sigma^{2}$，如定义所示。\n\n对于无放回采样，样本均值 $H_{B_{k}}$ 的期望是总体均值：\n$$\n\\mathbb{E}[H_{B_{k}}] = H.\n$$\n二阶矩与方差相关：$\\mathbb{E}[H_{B_{k}}^{2}] = \\text{Var}(H_{B_{k}}) + (\\mathbb{E}[H_{B_{k}}])^{2} = \\text{Var}(H_{B_{k}}) + H^{2}$。\n\n从大小为 $n$、方差为 $\\sigma^{2}$ 的总体中无放回抽取大小为 $m$ 的样本，其样本均值的方差的标准公式是：\n$$\n\\text{Var}(H_{B_{k}}) = \\frac{\\sigma^{2}}{m}\\left(\\frac{n-m}{n-1}\\right).\n$$\n其中 $\\sigma^2$ 是用因子 $1/n$ 定义的总体方差。此公式包含了有限总体校正因子 $\\frac{n-m}{n-1}$。\n\n现在我们可以计算 $\\mathbb{E}[H_{B_{k}}^{2}]$：\n$$\n\\mathbb{E}[H_{B_{k}}^{2}] = \\text{Var}(H_{B_{k}}) + H^{2} = \\frac{\\sigma^{2}}{m}\\left(\\frac{n-m}{n-1}\\right) + H^{2}.\n$$\n将这些矩代回到期望平方误差的表达式中：\n$$\n\\mathbb{E}[e_{k+1}^{2} | e_{k}] = \\left[1 - 2\\eta\\mathbb{E}[H_{B_{k}}] + \\eta^{2}\\mathbb{E}[H_{B_{k}}^{2}]\\right]e_{k}^{2}\n$$\n$$\n\\mathbb{E}[e_{k+1}^{2} | e_{k}] = \\left[1 - 2\\eta H + \\eta^{2}\\left(H^{2} + \\frac{\\sigma^{2}}{m}\\frac{n-m}{n-1}\\right)\\right]e_{k}^{2}.\n$$\n这就是条件期望平方误差的最终表达式。\n\n**任务3：求最优学习率 $\\eta$**\n\n为了找到对于固定的 $e_{k} \\neq 0$ 能最小化 $\\mathbb{E}[e_{k+1}^{2} | e_{k}]$ 的学习率 $\\eta$，我们需要最小化以下关于 $\\eta$ 的函数：\n$$\ng(\\eta) = 1 - 2\\eta H + \\eta^{2}\\left(H^{2} + \\frac{\\sigma^{2}(n-m)}{m(n-1)}\\right).\n$$\n这是关于 $\\eta$ 的一个二次函数。由于 $H^{2} \\ge 0$, $\\sigma^{2} \\ge 0$, $n \\ge m \\ge 1$，$\\eta^{2}$ 项的系数为正，因此该抛物线开口向上，存在唯一的最小值。我们通过令其关于 $\\eta$ 的导数为零来求最小值：\n$$\n\\frac{dg}{d\\eta} = -2H + 2\\eta\\left(H^{2} + \\frac{\\sigma^{2}(n-m)}{m(n-1)}\\right) = 0.\n$$\n求解 $\\eta$：\n$$\n2\\eta\\left(H^{2} + \\frac{\\sigma^{2}(n-m)}{m(n-1)}\\right) = 2H,\n$$\n$$\n\\eta_{\\text{opt}} = \\frac{H}{H^{2} + \\frac{\\sigma^{2}(n-m)}{m(n-1)}}.\n$$\n为了将其表示为分母中不含繁分数的单个分数形式，我们可以将分子和分母同乘以 $m(n-1)$：\n$$\n\\eta_{\\text{opt}} = \\frac{m(n-1)H}{m(n-1)H^{2} + \\sigma^{2}(n-m)}.\n$$\n这就是最小化单步期望平方误差的学习率的闭式表达式。", "answer": "$$\\boxed{\\frac{m(n-1)H}{m(n-1)H^{2} + (n-m)\\sigma^{2}}}$$", "id": "3150601"}]}