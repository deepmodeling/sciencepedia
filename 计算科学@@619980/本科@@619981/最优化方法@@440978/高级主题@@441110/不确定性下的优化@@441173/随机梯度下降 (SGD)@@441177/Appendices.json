{"hands_on_practices": [{"introduction": "随机梯度下降（SGD）的核心思想是，在每一步中仅使用一小部分数据（甚至单个数据点）来估计梯度的方向，从而实现快速迭代。这个练习将带你完成一次最基本的SGD更新步骤，让你亲手体验该算法的计算过程。通过这个简单的二次函数示例[@problem_id:2206637]，你将掌握SGD的核心机制，为理解其更复杂的行为打下坚实的基础。", "problem": "一个迭代优化算法用于寻找能最小化一个成本函数的参数 $x$。总成本函数是几个分量函数的平均值：$F(x) = \\frac{1}{N}\\sum_{i=1}^{N} f_i(x)$。在此特定情况下，分量函数是二次函数，由 $f_i(x) = (x - c_i)^2$ 给出，其中常数 $c_i = i$，$i = 1, 2, \\dots, 10$，因此 $N=10$。\n\n优化过程从参数的初始猜测值 $x_0$ 开始。在每一步中，通过仅使用一个随机选择的分量函数 $f_j(x)$，从当前估计值 $x_k$ 计算出新的估计值 $x_{k+1}$。更新规则定义为：\n$$x_{k+1} = x_k - \\eta \\left( \\frac{d f_j(x)}{dx} \\bigg|_{x=x_k} \\right)$$\n其中 $\\eta$ 是一个常数，称为学习率。\n\n给定初始参数值 $x_0 = 10.0$ 和学习率 $\\eta = 0.1$，计算一次更新步骤后参数 $x_1$ 的值。对于这第一步，使用的分量函数是索引为 $j=5$ 的 $f_j(x)$。", "solution": "我们已知分量函数的形式为 $f_{i}(x) = (x - c_{i})^{2}$，其中 $c_{i} = i$。对于第一次更新，选择的索引是 $j=5$，所以 $f_{5}(x) = (x - 5)^{2}$。\n\n更新规则是\n$$\nx_{k+1} = x_{k} - \\eta \\left.\\frac{d f_{j}(x)}{dx}\\right|_{x=x_{k}}.\n$$\n使用幂法则和链式法则，所选分量函数的导数是\n$$\n\\frac{d f_{5}(x)}{dx} = \\frac{d}{dx}\\left[(x - 5)^{2}\\right] = 2(x - 5).\n$$\n在当前迭代值 $x_{0} = 10$ 处求值，得到\n$$\n\\left.\\frac{d f_{5}(x)}{dx}\\right|_{x=10} = 2(10 - 5) = 10.\n$$\n学习率为 $\\eta = 0.1$，更新变为\n$$\nx_{1} = x_{0} - \\eta \\cdot 10 = 10 - 0.1 \\times 10 = 10 - 1 = 9.\n$$\n因此，在使用 $f_{5}$ 进行一次更新步骤后，参数值为 $x_{1} = 9$。", "answer": "$$\\boxed{9}$$", "id": "2206637"}, {"introduction": "我们已经知道如何执行一次SGD更新。一个自然的问题是：这个步骤是否总能使我们更接近最优解，即总损失函数的值是否总会下降？这个练习[@problem_id:2206653]通过一个具体的例子揭示了一个SGD的关键特性。你会发现，单步SGD更新有时甚至会使总损失函数值增加，这与全批量梯度下降的行为截然不同，并直观地展示了SGD优化路径的“随机”性。", "problem": "在机器学习领域，优化算法通过最小化损失函数来训练模型参数。考虑一个具有单个标量参数 $w$ 的简单模型。目标是最小化总损失函数 $F(w)$，该函数定义为包含两个数据点的小型数据集上各个损失函数的平均值。总损失函数由下式给出：\n\n$$F(w) = \\frac{1}{2}\\left[f_1(w) + f_2(w)\\right]$$\n\n与这两个数据点相关的各个损失函数为：\n\n$$f_1(w) = \\frac{1}{2}(w - 2)^2$$\n$$f_2(w) = \\frac{1}{2}(w - 10)^2$$\n\n训练过程从初始参数值 $w_0 = 3$ 开始。使用学习率为 $\\eta = 2$ 的随机梯度下降 (SGD) 算法执行单个更新步骤。对于本次特定更新，仅使用第一个数据点的损失函数 $f_1(w)$ 来计算梯度。\n\n计算由这次 SGD 单步更新导致的总损失函数值的变化量 $F(w_1) - F(w_0)$。将您的最终答案四舍五入到三位有效数字。", "solution": "我们已知 $F(w)=\\frac{1}{2}\\left[f_{1}(w)+f_{2}(w)\\right]$，其中 $f_{1}(w)=\\frac{1}{2}(w-2)^{2}$ 且 $f_{2}(w)=\\frac{1}{2}(w-10)^{2}$。初始参数为 $w_{0}=3$。学习率为 $\\eta=2$ 的单步 SGD 仅使用 $f_{1}$ 的梯度。\n\n一维的 SGD 更新规则为\n$$\nw_{1}=w_{0}-\\eta\\,\\frac{d f_{1}}{d w}\\bigg|_{w=w_{0}}.\n$$\n计算导数：\n$$\n\\frac{d f_{1}}{d w}=\\frac{d}{d w}\\left[\\frac{1}{2}(w-2)^{2}\\right]=(w-2).\n$$\n在 $w_{0}=3$ 处求值：\n$$\n\\frac{d f_{1}}{d w}\\bigg|_{w=3}=3-2=1.\n$$\n因此更新后的参数为\n$$\nw_{1}=3-2\\cdot 1=1.\n$$\n\n现在计算 $F(w_{0})$：\n$$\nf_{1}(3)=\\frac{1}{2}(3-2)^{2}=\\frac{1}{2},\\quad f_{2}(3)=\\frac{1}{2}(3-10)^{2}=\\frac{1}{2}\\cdot 49=\\frac{49}{2},\n$$\n$$\nF(3)=\\frac{1}{2}\\left(\\frac{1}{2}+\\frac{49}{2}\\right)=\\frac{1}{2}\\cdot\\frac{50}{2}=\\frac{1}{2}\\cdot 25=\\frac{25}{2}.\n$$\n\n计算 $F(w_{1})$：\n$$\nf_{1}(1)=\\frac{1}{2}(1-2)^{2}=\\frac{1}{2},\\quad f_{2}(1)=\\frac{1}{2}(1-10)^{2}=\\frac{1}{2}\\cdot 81=\\frac{81}{2},\n$$\n$$\nF(1)=\\frac{1}{2}\\left(\\frac{1}{2}+\\frac{81}{2}\\right)=\\frac{1}{2}\\cdot\\frac{82}{2}=\\frac{1}{2}\\cdot 41=\\frac{41}{2}.\n$$\n\n因此，总损失的变化量为\n$$\nF(w_{1})-F(w_{0})=\\frac{41}{2}-\\frac{25}{2}=\\frac{16}{2}=8.\n$$\n四舍五入到三位有效数字，结果为 $8.00$。", "answer": "$$\\boxed{8.00}$$", "id": "2206653"}, {"introduction": "在前面的练习中，我们观察到SGD的单步更新不一定会降低总损失。这种行为的根源在于随机梯度估计的不确定性。这个练习[@problem_id:2206620]将引导你量化这种不确定性，通过计算随机梯度的方差。理解方差是理解SGD为何具有“噪声”以及这种噪声如何影响优化过程的关键，它解释了算法探索参数空间的能力。", "problem": "在许多机器学习问题中，目标是最小化一个损失函数 $F(x)$，该函数结构化为在一个数据集上的平均值。一种常见的形式是 $F(x) = \\frac{1}{N} \\sum_{i=1}^{N} f_i(x)$，其中 $f_i(x)$ 是与第 $i$ 个数据点相关的损失，$x$ 是一个模型参数。\n\n考虑一个简化的一维问题，我们希望找到最小化损失函数的参数 $x$。该数据集仅包含两个数据点 ($N=2$)，导出以下分量损失函数：\n$$f_1(x) = (x-2)^2$$\n$$f_2(x) = (x+2)^2$$\n因此，总损失函数为 $F(x) = \\frac{1}{2} (f_1(x) + f_2(x))$。\n\n随机梯度下降 (SGD) 是一种迭代优化算法，它在每一步都近似 $F(x)$ 的真实梯度。在其最简单的形式中，一个随机梯度估计量（我们记为 $g(x)$）的计算方法是：首先从 $\\{1, 2\\}$ 中均匀随机地选择一个索引 $i$，然后计算相应分量函数的梯度，$g(x) = \\nabla f_i(x)$。在这个一维情况下，梯度算子 $\\nabla$ 就是关于 $x$ 的导数，即 $\\frac{d}{dx}$。\n\n计算随机梯度估计量 $g(x)$ 在特定点 $x = 1$ 处的方差。", "solution": "给定 $f_{1}(x)=(x-2)^{2}$ 和 $f_{2}(x)=(x+2)^{2}$，随机梯度估计量 $g(x)$ 的定义是：从 $\\{1,2\\}$ 中均匀选择 $i$ 并设 $g(x)=\\frac{d}{dx}f_{i}(x)$。首先计算分量梯度：\n$$\n\\frac{d}{dx}f_{1}(x)=2(x-2), \\quad \\frac{d}{dx}f_{2}(x)=2(x+2).\n$$\n在 $x=1$ 处，$g(1)$ 取值为\n$$\ng(1)=2(1-2)=-2 \\quad \\text{概率为 } \\frac{1}{2}, \\quad g(1)=2(1+2)=6 \\quad \\text{概率为 } \\frac{1}{2}.\n$$\n计算 $g(1)$ 的均值：\n$$\n\\mathbb{E}[g(1)]=\\frac{1}{2}(-2)+\\frac{1}{2}(6)=2.\n$$\n这等于 $F$ 在 $x=1$ 处的真实梯度，因为\n$$\nF'(x)=\\frac{1}{2}\\left(2(x-2)+2(x+2)\\right)=2x \\implies F'(1)=2.\n$$\n计算二阶矩：\n$$\n\\mathbb{E}[g(1)^{2}]=\\frac{1}{2}\\left((-2)^{2}+6^{2}\\right)=\\frac{1}{2}(4+36)=20.\n$$\n因此，方差为\n$$\n\\operatorname{Var}(g(1))=\\mathbb{E}[g(1)^{2}]-\\left(\\mathbb{E}[g(1)]\\right)^{2}=20-2^{2}=16.\n$$\n因此，随机梯度估计量在 $x=1$ 处的方差是 $16$。", "answer": "$$\\boxed{16}$$", "id": "2206620"}]}