{"hands_on_practices": [{"introduction": "在无导数优化中，局部代理模型的质量，特别是其梯度近似的准确性，是算法性能的关键。这个实践将引导你通过一个数值实验，亲手验证采样点的几何布局与疏密程度如何直接影响线性模型梯度的估计精度。通过在一个已知真实梯度的二次函数上进行测试，你将直观地理解良好几何构型（“模型适定性”）对构建可靠模型的重要性。[@problem_id:3153344]", "problem": "考虑一个用于基于模型的无导数梯度近似的确定性实验。设维度为 $n=2$。通过以下二次模型定义一个二阶连续可微的目标函数 $f:\\mathbb{R}^2\\to\\mathbb{R}$\n$$\nf(\\mathbf{x})=\\tfrac{1}{2}\\,\\mathbf{x}^{\\top}Q\\,\\mathbf{x}+ \\mathbf{c}^{\\top}\\mathbf{x},\n$$\n其中\n$$\nQ=\\begin{bmatrix} 3 & 1 \\\\ 1 & 2 \\end{bmatrix},\\quad \\mathbf{c}=\\begin{bmatrix}1\\\\-2\\end{bmatrix}.\n$$\n设建模中心为\n$$\n\\mathbf{x}_c=\\begin{bmatrix}0.5\\\\-0.3\\end{bmatrix}.\n$$\n根据多元微积分的基本原理，$\\mathbf{x}_c$ 处的真实梯度即为雅可比矩阵（对于标量值函数，即为梯度）\n$$\n\\nabla f(\\mathbf{x}_c)=Q\\,\\mathbf{x}_c+\\mathbf{c}.\n$$\n一种标准的基于模型的无导数方法通过仅使用函数值来构建一个仿射模型 $m(\\mathbf{x})=a+\\mathbf{g}^{\\top}(\\mathbf{x}-\\mathbf{x}_c)$，方法是在 $\\mathbf{x}_c$ 附近的样本点 $\\{\\mathbf{y}_i\\}_{i=1}^m$ 上求解最小二乘拟合，其中 $\\mathbf{s}_i=\\mathbf{y}_i-\\mathbf{x}_c$ 是小位移。估计梯度是从拟合中得到的向量 $\\widehat{\\mathbf{g}}$。科学上的真实性要求样本集的几何形状影响可辨識度和准确性，并且采样半径控制因在线性模型中忽略曲率而引入的截断误差。\n\n从最小二乘法和 $f$ 在 $\\mathbf{x}_c$ 周围的一阶泰勒展开的基本定义出发，设计一个程序，该程序为每个指定的测试案例构建样本集，使用线性最小二乘法拟合仿射模型，并返回梯度估计误差的欧几里得范数\n$$\n\\left\\|\\widehat{\\mathbf{g}}-\\nabla f(\\mathbf{x}_c)\\right\\|_2.\n$$\n用于在圆上放置点的角度必须以弧度表示。在这个纯数学背景下，不涉及任何物理单位。\n\n实现以下测试套件，该套件通过采样半径改变样本密度，通过点的排列改变几何质量。在所有情况下，将中心设置在 $\\mathbf{x}_c$ 处，并在每个采样点上精确评估 $f$：\n\n- 情况A（分布良好的几何形状，较大半径）：在 $\\mathbf{x}_c$ 周围半径为 $r=0.5$ 的圆上取 $m=12$ 个等距点。角度为 $2\\pi i/m$，其中 $i=0,1,\\dots,m-1$，单位为弧度。\n- 情况B（分布良好的几何形状，较小半径）：在 $\\mathbf{x}_c$ 周围半径为 $r=0.01$ 的圆上取 $m=12$ 个如上所述的等距点。\n- 情况C（近共线的几何形状，较大半径）：沿直线方向 $\\mathbf{v}=\\begin{bmatrix}1\\\\ \\varepsilon\\end{bmatrix}$（$\\varepsilon=10^{-3}$）上的点，缩放至半径 $r=0.5$，有 $m=12$ 个偏移量 $t_i$ 在区间 $[-1,1]$ 内均匀分布；$\\mathbf{s}_i=r\\,t_i\\,\\mathbf{v}/\\|\\mathbf{v}\\|_2$。\n- 情况D（近共线的几何形状，较小半径）：与情况C相同，但半径为 $r=0.01$。\n- 情况E（退化的几何形状）：$m=12$ 个点全部位于 $\\mathbf{x}_c$（即，对于所有 $i$，$\\mathbf{s}_i=\\mathbf{0}$）。\n- 情况F（最小的分布良好的集合）：在半径为 $r=0.1$ 的圆上取 $m=3$ 个点，角度分别为 $0$、$2\\pi/3$ 和 $4\\pi/3$ 弧度。\n- 情况G（最小的近共线集合）：$m=3$ 个点，其中 $\\mathbf{v}=\\begin{bmatrix}1\\\\ \\varepsilon\\end{bmatrix}$（$\\varepsilon=10^{-6}$），偏移量 $t\\in\\{-1,0,1\\}$，半径为 $r=0.1$；$\\mathbf{s}_i=r\\,t_i\\,\\mathbf{v}/\\|\\mathbf{v}\\|_2$。\n\n对于每种情况，构建一个设计矩阵，其中包含一列全为1的列和 $\\mathbf{s}_i$ 的分量，求解关于 $(a,\\widehat{\\mathbf{g}})$ 的线性最小二乘问题，并报告标量误差 $\\left\\|\\widehat{\\mathbf{g}}-\\nabla f(\\mathbf{x}_c)\\right\\|_2$。\n\n您的程序应生成单行输出，其中包含一个逗号分隔的列表，用方括号括起来的结果（例如，“[result1,result2,result3]”），并按情况A到G的顺序排列。", "solution": "该问题要求设计一个数值实验来评估一种基于模型的无导数梯度估计方法的准确性。我们将首先验证问题陈述，然后从基本原理出发制定解决方案。\n\n### 问题验证\n\n**第1步：提取已知条件**\n- **目标函数：** $f(\\mathbf{x}) = \\tfrac{1}{2}\\mathbf{x}^{\\top}Q\\mathbf{x} + \\mathbf{c}^{\\top}\\mathbf{x}$，其中 $\\mathbf{x} \\in \\mathbb{R}^2$。\n- **矩阵 Q：** $Q=\\begin{bmatrix} 3 & 1 \\\\ 1 & 2 \\end{bmatrix}$。\n- **向量 c：** $\\mathbf{c}=\\begin{bmatrix}1\\\\-2\\end{bmatrix}$。\n- **建模中心：** $\\mathbf{x}_c=\\begin{bmatrix}0.5\\\\-0.3\\end{bmatrix}$。\n- **真实梯度：** $\\nabla f(\\mathbf{x}_c) = Q\\mathbf{x}_c + \\mathbf{c}$。\n- **仿射模型：** $m(\\mathbf{x}) = a + \\mathbf{g}^{\\top}(\\mathbf{x}-\\mathbf{x}_c)$。待估计的参数是标量截距 $a$ 和梯度向量 $\\widehat{\\mathbf{g}}$。\n- **样本点：** 使用位移 $\\mathbf{s}_i = \\mathbf{y}_i - \\mathbf{x}_c$ 来生成样本点 $\\mathbf{y}_i$。\n- **方法：** 在样本集 $\\{\\mathbf{y}_i\\}_{i=1}^m$ 上求解一个线性最小二乘问题来找到 $(a, \\widehat{\\mathbf{g}})$。\n- **输出度量：** 梯度估计误差的欧几里得范数，$\\|\\widehat{\\mathbf{g}} - \\nabla f(\\mathbf{x}_c)\\|_2$。\n- **测试案例：**\n    - **A:** $r=0.5, m=12$，圆形几何。\n    - **B:** $r=0.01, m=12$，圆形几何。\n    - **C:** $r=0.5, m=12$，近共线几何，$\\mathbf{v}=\\begin{bmatrix}1\\\\10^{-3}\\end{bmatrix}$。\n    - **D:** $r=0.01, m=12$，与C相同的近共线几何。\n    - **E:** $r$ 未定义（所有点均在中心），$m=12$，退化几何，$\\mathbf{s}_i=\\mathbf{0}$。\n    - **F:** $r=0.1, m=3$，最小的分布良好的圆形几何。\n    - **G:** $r=0.1, m=3$，最小的近共线几何，$\\mathbf{v}=\\begin{bmatrix}1\\\\10^{-6}\\end{bmatrix}$ 且偏移量 $t\\in\\{-1,0,1\\}$。\n\n**第2步：使用提取的已知条件进行验证**\n- **科学依据：** 该问题牢固地植根于数值优化领域，特别是在无导数方法的分析中。使用二次测试函数和线性模型是分析算法性质的标准技术。最小二乘拟合、模型适定性以及样本几何形状的影响等概念是该领域的核心。所有数学公式都是正确的。\n- **适定性：** 该问题是适定的。只要设计矩阵具有满列秩，最小二乘问题就有唯一解。测试案例旨在探索此条件在不同稳定性程度上成立的情景（情况A、B、C、D、F），以及它不成立的情景（情况E、G）。失效情况本身是分析的重要组成部分，并会导致标准数值线性代数程序产生可预测的结果。\n- **客观性：** 问题陈述精确、量化，并且没有主观因素。\n\n**第3步：结论与行动**\n该问题在科学上是合理的、适定的和客观的。这是一个有效且具有指导意义的数值分析练习。我们将继续进行完整的解决方案。\n\n### 基于原理的解决方案设计\n\n任务的核心是，通过拟合一个局部仿射模型 $m(\\mathbf{x}) = a + \\mathbf{g}^{\\top}(\\mathbf{x}-\\mathbf{x}_c)$，仅使用在一组样本点 $\\{\\mathbf{y}_i\\}_{i=1}^m$ 上评估的函数值，来近似函数 $f:\\mathbb{R}^n \\to \\mathbb{R}$ 在点 $\\mathbf{x}_c$ 处的梯度。\n\n**1. 理论公式化**\n\n我们寻求找到参数 $a \\in \\mathbb{R}$ 和 $\\widehat{\\mathbf{g}} \\in \\mathbb{R}^n$，以最小化模型预测值与样本点处真实函数值之间的平方差之和。设位移向量为 $\\mathbf{s}_i = \\mathbf{y}_i - \\mathbf{x}_c$。在 $\\mathbf{y}_i$ 处评估的模型为 $m(\\mathbf{y}_i) = a + \\widehat{\\mathbf{g}}^{\\top}\\mathbf{s}_i$。最小二乘问题是：\n$$ \\min_{a, \\widehat{\\mathbf{g}}} \\sum_{i=1}^m \\left( (a + \\widehat{\\mathbf{g}}^{\\top}\\mathbf{s}_i) - f(\\mathbf{y}_i) \\right)^2 $$\n这是一个标准的线性最小二乘问题。设参数向量为 $\\boldsymbol{\\theta} = \\begin{bmatrix} a \\\\ \\widehat{\\mathbf{g}} \\end{bmatrix} \\in \\mathbb{R}^{1+n}$。对于我们的问题，$n=2$，所以 $\\boldsymbol{\\theta} \\in \\mathbb{R}^3$。我们可以将该系统表示为矩阵形式 $M\\boldsymbol{\\theta} \\approx \\mathbf{f}_{vals}$，其中：\n- $M$ 是 $m \\times (1+n)$ 的设计矩阵。每一行对应一个样本点：\n    $$ M = \\begin{bmatrix} 1 & \\mathbf{s}_1^{\\top} \\\\ 1 & \\mathbf{s}_2^{\\top} \\\\ \\vdots & \\vdots \\\\ 1 & \\mathbf{s}_m^{\\top} \\end{bmatrix} = \\begin{bmatrix} 1 & s_{1,1} & s_{1,2} \\\\ 1 & s_{2,1} & s_{2,2} \\\\ \\vdots & \\vdots & \\vdots \\\\ 1 & s_{m,1} & s_{m,2} \\end{bmatrix} $$\n- $\\mathbf{f}_{vals}$ 是函数评估值的 $m \\times 1$ 向量：\n    $$ \\mathbf{f}_{vals} = \\begin{bmatrix} f(\\mathbf{y}_1) \\\\ f(\\mathbf{y}_2) \\\\ \\vdots \\\\ f(\\mathbf{y}_m) \\end{bmatrix} $$\n最小化 $\\|M\\boldsymbol{\\theta} - \\mathbf{f}_{vals}\\|_2^2$ 的最小二乘解 $\\boldsymbol{\\theta}^*$ 可通过求解正规方程 $M^{\\top}M\\boldsymbol{\\theta} = M^{\\top}\\mathbf{f}_{vals}$ 找到。在实践中，使用数值稳定的算法，例如基于QR分解或奇异值分解（SVD）的算法。估计的梯度 $\\widehat{\\mathbf{g}}$ 由解向量 $\\boldsymbol{\\theta}^*$ 的最后 $n=2$ 个分量组成。\n\n**2. 二次函数的误差分析**\n\n给定的目标函数 $f(\\mathbf{x})$ 是一个二次函数。其在 $\\mathbf{x}_c$ 周围的泰勒级数展开是精确的，并在二阶项处终止：\n$$ f(\\mathbf{x}_c + \\mathbf{s}) = f(\\mathbf{x}_c) + \\nabla f(\\mathbf{x}_c)^{\\top}\\mathbf{s} + \\frac{1}{2}\\mathbf{s}^{\\top}\\nabla^2 f(\\mathbf{x}_c)\\mathbf{s} $$\n对于给定的 $f$，梯度为 $\\nabla f(\\mathbf{x}) = Q\\mathbf{x} + \\mathbf{c}$，Hessian矩阵为常数 $\\nabla^2 f(\\mathbf{x}) = Q$。因此，函数值的向量可以写成：\n$$ \\mathbf{f}_{vals, i} = f(\\mathbf{x}_c) + \\nabla f(\\mathbf{x}_c)^{\\top}\\mathbf{s}_i + \\frac{1}{2}\\mathbf{s}_i^{\\top}Q\\mathbf{s}_i $$\n最小二乘求解器实际上是试图将一个线性模型拟合到这个二次数据上。差异来自于二次项 $\\frac{1}{2}\\mathbf{s}_i^{\\top}Q\\mathbf{s}_i$，这是仿射模型的截断误差。估计梯度的误差 $\\Delta\\mathbf{g} = \\widehat{\\mathbf{g}} - \\nabla f(\\mathbf{x}_c)$ 直接归因于样本集的几何形状（编码在 $M$ 中）如何将这些二次项映射到解中。可以证明，参数向量中的误差为 $\\Delta\\boldsymbol{\\theta} = (M^{\\top}M)^{-1}M^{\\top}\\mathbf{b}$，其中 $b_i = \\frac{1}{2}\\mathbf{s}_i^{\\top}Q\\mathbf{s}_i$。因此，梯度估计 $\\widehat{\\mathbf{g}}$ 的质量关键取决于 $M$ 的性质，这个概念被称为样本集的“适定性”。\n\n**3. 实现步骤**\n\n我们将为每个测试案例实施遵循以下步骤的程序。\n\n**步骤 A：初始化**\n首先，我们定义恒定的问题参数 $Q$、$\\mathbf{c}$ 和 $\\mathbf{x}_c$。然后我们计算作为基准的真实梯度：\n$$ \\nabla f(\\mathbf{x}_c) = Q\\mathbf{x}_c + \\mathbf{c} = \\begin{bmatrix}3 & 1\\\\1 & 2\\end{bmatrix}\\begin{bmatrix}0.5\\\\-0.3\\end{bmatrix} + \\begin{bmatrix}1\\\\-2\\end{bmatrix} = \\begin{bmatrix}1.2\\\\-0.1\\end{bmatrix} + \\begin{bmatrix}1\\\\-2\\end{bmatrix} = \\begin{bmatrix}2.2\\\\-2.1\\end{bmatrix} $$\n\n**步骤 B：分情况计算循环**\n对于从 A 到 G 的每个案例，我们执行以下操作：\n1.  **生成样本位移：** 创建一个 $m \\times 2$ 矩阵 $S$，其中每一行是一个由案例指定的位移向量 $\\mathbf{s}_i^{\\top}$（例如，圆上的点，直线上的点）。\n2.  **评估函数：** 通过 $\\mathbf{y}_i = \\mathbf{x}_c + \\mathbf{s}_i$ 形成样本点。对每个 $\\mathbf{y}_i$ 评估函数 $f(\\mathbf{y}_i) = \\tfrac{1}{2}\\mathbf{y}_i^{\\top}Q\\mathbf{y}_i + \\mathbf{c}^{\\top}\\mathbf{y}_i$，以创建向量 $\\mathbf{f}_{vals}$。\n3.  **构建设计矩阵：** 通过在位移矩阵 $S$ 前面添加一列全为1的列来构建 $m \\times 3$ 的设计矩阵 $M$。\n4.  **求解最小二乘问题：** 使用数值线性最小二乘求解器找到最小化 $\\|M\\boldsymbol{\\theta} - \\mathbf{f}_{vals}\\|_2$ 的 $\\boldsymbol{\\theta}^* = [a, \\widehat{g}_1, \\widehat{g}_2]^{\\top}$。这可以稳健地处理秩亏情况（E和G），通过提供一个最小范数解。\n5.  **提取梯度并计算误差：** 从 $\\boldsymbol{\\theta}^*$ 中提取估计梯度 $\\widehat{\\mathbf{g}} = [\\widehat{g}_1, \\widehat{g}_2]^{\\top}$。最终误差计算为差异的欧几里得范数：$\\|\\widehat{\\mathbf{g}} - \\nabla f(\\mathbf{x}_c)\\|_2$。\n\n这个系统化的过程允许直接比较采样半径和几何排列如何影响梯度近似的准确性。例如，我们预期案例A和B中的对称几何形状会因误差项的抵消而产生高度准确的结果（误差接近机器精度），而案例C、D和G中条件不良的几何形状将导致更大的误差。案例E代表了可辨识性的完全失败。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves for the gradient estimation error in a model-based derivative-free\n    method for a series of test cases.\n    \"\"\"\n    \n    # Define problem constants\n    Q = np.array([[3.0, 1.0], [1.0, 2.0]])\n    c = np.array([1.0, -2.0])\n    xc = np.array([0.5, -0.3])\n    \n    # Define the quadratic objective function\n    def f(x):\n        return 0.5 * x.T @ Q @ x + c.T @ x\n        \n    # Calculate the true gradient at xc\n    grad_true = Q @ xc + c\n    \n    # Define the test suite\n    test_cases = [\n        # Case A: well-spread, larger radius\n        {'name': 'A', 'r': 0.5, 'm': 12, 'type': 'circle'},\n        # Case B: well-spread, smaller radius\n        {'name': 'B', 'r': 0.01, 'm': 12, 'type': 'circle'},\n        # Case C: nearly collinear, larger radius\n        {'name': 'C', 'r': 0.5, 'm': 12, 'type': 'line', 'eps': 1e-3},\n        # Case D: nearly collinear, smaller radius\n        {'name': 'D', 'r': 0.01, 'm': 12, 'type': 'line', 'eps': 1e-3},\n        # Case E: degenerate geometry\n        {'name': 'E', 'm': 12, 'type': 'degenerate'},\n        # Case F: minimal well-spread set\n        {'name': 'F', 'r': 0.1, 'm': 3, 'type': 'circle'},\n        # Case G: minimal nearly collinear set\n        {'name': 'G', 'r': 0.1, 'm': 3, 'type': 'line', 'eps': 1e-6, 't_offsets': [-1, 0, 1]},\n    ]\n    \n    results = []\n    \n    for case in test_cases:\n        m = case['m']\n        \n        # 1. Generate sample displacements S\n        S = np.zeros((m, 2))\n        \n        if case['type'] == 'circle':\n            r = case['r']\n            if case['name'] == 'F':\n                angles = np.array([0, 2 * np.pi / 3, 4 * np.pi / 3])\n            else: # Cases A, B\n                angles = np.linspace(0, 2 * np.pi, m, endpoint=False)\n            S[:, 0] = r * np.cos(angles)\n            S[:, 1] = r * np.sin(angles)\n            \n        elif case['type'] == 'line':\n            r = case['r']\n            eps = case['eps']\n            v = np.array([1.0, eps])\n            v_norm = v / np.linalg.norm(v)\n            if 't_offsets' in case: # Case G\n                t = np.array(case['t_offsets'])\n            else: # Cases C, D\n                t = np.linspace(-1.0, 1.0, m)\n            S = t[:, np.newaxis] * v_norm\n            \n        elif case['type'] == 'degenerate': # Case E\n            # S is already initialized to zeros\n            pass\n\n        # 2. Evaluate function\n        Y = xc + S\n        f_vals = np.array([f(y) for y in Y])\n        \n        # 3. Construct design matrix M\n        M = np.hstack([np.ones((m, 1)), S])\n        \n        # 4. Solve least-squares problem\n        # theta = [a, g_hat_1, g_hat_2]\n        # Use rcond=None to use machine precision for rank detection\n        theta, _, _, _ = np.linalg.lstsq(M, f_vals, rcond=None)\n        \n        # 5. Extract gradient and compute error\n        g_hat = theta[1:]\n        error = np.linalg.norm(g_hat - grad_true)\n        results.append(error)\n\n    # Final print statement in the exact required format\n    print(f\"[{','.join(f'{res:.12e}' for res in results)}]\")\n\nsolve()\n```", "id": "3153344"}, {"introduction": "现实世界中的许多优化问题，其目标函数在不同坐标方向上尺度差异巨大，即呈现出“各向异性”的特点。本练习将带你构建一个完整的二次代理模型，并将其应用于一个更高级的信任域框架中。你将对比标准的球形信任域和能够自适应函数几何特性的椭球信任域，从而理解如何通过选择合适的范数来“预处理”问题，并获得更优的优化步长。[@problem_id:3153334]", "problem": "给定一个场景，其中目标函数的曲率在不同坐标轴上差异巨大。您需要比较在一个基于模型的无导数优化框架中，球形信赖域与由椭球范数定义的各向异性信赖域的有效性。核心任务是实现一个程序，该程序针对几个测试案例，仅使用函数求值来构建局部二次模型，然后通过求解在两种不同范数下的信赖域子问题来提出步长。该程序必须报告与欧几里得范数相比，椭球范数是否能产生严格更优或相等的目标函数下降。\n\n该问题的基本构成包括以下元素：\n- 目标函数在点 $\\mathbf{x}_k$ 附近的局部二次模型，表示为 $m(\\mathbf{s}) = c + \\mathbf{g}^\\top \\mathbf{s} + \\frac{1}{2}\\mathbf{s}^\\top H \\mathbf{s}$，其中 $\\mathbf{s} = \\mathbf{x} - \\mathbf{x}_k$，$c \\in \\mathbb{R}$，$\\mathbf{g} \\in \\mathbb{R}^n$，且 $H \\in \\mathbb{R}^{n \\times n}$ 是对称矩阵。\n- 信赖域子问题，旨在找到近似最小化模型 $m(\\mathbf{s})$ 的 $\\mathbf{s}$，并满足范数约束 $\\|\\mathbf{s}\\| \\leq \\Delta$，其中 $\\Delta > 0$ 是信赖域半径。\n- 由椭球范数 $\\|\\mathbf{s}\\|_M = \\sqrt{\\mathbf{s}^\\top M \\mathbf{s}}$ 定义的各向异性信赖域，其中 $M \\in \\mathbb{R}^{n \\times n}$ 是对称正定矩阵，因此约束变为 $\\|\\mathbf{s}\\|_M \\leq \\Delta$。\n- 变量变换 $\\mathbf{y} = L\\mathbf{s}$，其中 $L \\in \\mathbb{R}^{n \\times n}$ 满足 $L^\\top L = M$，这将 $\\|\\mathbf{s}\\|_M \\leq \\Delta$ 转换为欧几里得约束 $\\|\\mathbf{y}\\| \\leq \\Delta$，并将模型转换为 $m(\\mathbf{s}) = c + (L^{-\\top}\\mathbf{g})^\\top \\mathbf{y} + \\frac{1}{2}\\mathbf{y}^\\top (L^{-\\top}HL^{-1}) \\mathbf{y}$。\n\n您的程序必须实现以下功能：\n- 仅使用在附近点集上的目标函数求值，围绕 $\\mathbf{x}_k$ 构建一个局部二次模型。使用最小二乘拟合来确定模型 $m(\\mathbf{s})$ 的 $c$、$\\mathbf{g}$ 和对称矩阵 $H$。设计矩阵应使用特征 $\\left[1, s_1, \\dots, s_n, \\frac{1}{2}s_1^2, \\dots, \\frac{1}{2}s_n^2, s_1 s_2, \\dots\\right]$，并通过对二次项仅使用唯一的 $\\{i \\leq j\\}$ 对来确保 $H$ 的对称性。\n- 实现一个截断共轭梯度求解器，用于求解在欧几里得范数 $\\|\\cdot\\|$ 下和通过变量变换 $\\mathbf{y} = L\\mathbf{s}$ 在椭球范数 $\\|\\cdot\\|_M$ 下的信赖域子问题。使用 Steihaug 的方法来处理潜在的边界命中或负曲率方向，而无需显式线搜索。\n- 使用实际目标函数评估真实的目标函数下降，并比较这两个步长。为每个测试案例报告一个布尔值，指示椭球范数是否获得了小于或等于欧几里得范数所获得的目标值。\n\n目标函数是严格凸二次型，形式为\n$$\nf(\\mathbf{x}) = \\frac{1}{2}\\mathbf{x}^\\top Q \\mathbf{x} + \\mathbf{q}^\\top \\mathbf{x},\n$$\n其中 $Q \\in \\mathbb{R}^{n \\times n}$ 是对称正定矩阵，$\\mathbf{q} \\in \\mathbb{R}^n$。您不得直接使用梯度或 Hessian 矩阵；所有模型信息必须从函数求值中推断。\n\n在以下测试套件下，为维度 $n = 3$ 实现该程序。每个测试案例指定了 $Q$、$\\mathbf{q}$、起始点 $\\mathbf{x}_k$、信赖域半径 $\\Delta$、用于模型构建的采样半径 $h$ 以及用于椭球范数的度量矩阵 $M$。请使用以下测试套件。\n- 测试案例 1（理想各向异性路径）：$Q = \\operatorname{diag}(1000, 1, 0.01)$，其中 $Q_{12} = Q_{21} = 20$，所有其他非对角元素为 $0$；$\\mathbf{q} = [0, 0, -1]^\\top$；$\\mathbf{x}_k = [0, 0, 0]^\\top$；$\\Delta = 0.1$；$h = 0.05$；$M = Q$。\n- 测试案例 2（各向同性曲率）：$Q = \\operatorname{diag}(5, 5, 5)$，所有非对角元素为 $0$；$\\mathbf{q} = [1, -1, 0.5]^\\top$；$\\mathbf{x}_k = [0, 0, 0]^\\top$；$\\Delta = 0.1$；$h = 0.05$；$M = I$。\n- 测试案例 3（近乎平坦的轴）：$Q = \\operatorname{diag}(100, 0.001, 100)$，其中 $Q_{13} = Q_{31} = 5$，所有其他非对角元素为 $0$；$\\mathbf{q} = [0, -1, 0]^\\top$；$\\mathbf{x}_k = [0, 0, 0]^\\top$；$\\Delta = 0.05$；$h = 0.025$；$M = Q$。\n- 测试案例 4（极小信赖半径边界）：$Q = \\operatorname{diag}(50, 2, 0.5)$，所有非对角元素为 $0$；$\\mathbf{q} = [0.1, -0.2, 0.3]^\\top$；$\\mathbf{x}_k = [0, 0, 0]^\\top$；$\\Delta = 0.001$；$h = 0.0005$；$M = Q$。\n\n对于每个测试案例，从形如 $\\mathbf{x}_k$、$\\mathbf{x}_k \\pm h \\mathbf{e}_i$（$i \\in \\{1, 2, 3\\}$）和 $\\mathbf{x}_k \\pm h (\\mathbf{e}_i + \\mathbf{e}_j)$（$i  j$）的点构建模型，其中 $\\mathbf{e}_i$ 是 $\\mathbb{R}^3$ 中的标准基向量。这将产生一个包含 $13$ 个点的集合。在这些点上通过最小二乘法拟合模型参数 $(c, \\mathbf{g}, H)$。\n\n对于每种范数（欧几里得范数和椭球范数），使用 Steihaug 的截断共轭梯度法在适当的变量空间中求解信赖域子问题。使用 $\\mathbf{s}_E$ 表示您的求解器返回的欧几里得步长，$\\mathbf{s}_M$ 表示椭球步长。计算实际目标值 $f(\\mathbf{x}_k + \\mathbf{s}_E)$ 和 $f(\\mathbf{x}_k + \\mathbf{s}_M)$，并为每个测试案例输出一个布尔值，指示是否满足 $f(\\mathbf{x}_k + \\mathbf{s}_M) \\leq f(\\mathbf{x}_k + \\mathbf{s}_E)$。\n\n您的程序应生成单行输出，其中包含用方括号括起来的逗号分隔的结果列表（例如，$\\left[\\text{result}_1, \\text{result}_2, \\text{result}_3, \\text{result}_4\\right]$）。每个 $\\text{result}_i$ 必须是一个布尔值（$\\text{True}$ 或 $\\text{False}$）。不允许使用外部输入或文件。", "solution": "该问题要求比较球形（欧几里得范数）信赖域与椭球信赖域对于一个基于模型的无导数优化算法的有效性。该比较在一组涉及表现出各向异性曲率的严格凸二次目标函数的测试案例上进行。解决方案的核心涉及三个主要步骤：构建目标函数的局部二次模型，为每种范数求解信赖域子问题，以及比较所得步长实现的真实目标函数下降。\n\n### 1. 二次模型构建\n\n优化器无法知晓目标函数，只能通过函数求值来访问它。我们在点 $\\mathbf{x}_k$ 附近构建目标函数 $f(\\mathbf{x})$ 的局部二次模型 $m(\\mathbf{s})$。该模型由下式给出：\n$$\nm(\\mathbf{s}) = c + \\mathbf{g}^\\top \\mathbf{s} + \\frac{1}{2}\\mathbf{s}^\\top H \\mathbf{s}\n$$\n其中 $\\mathbf{s} = \\mathbf{x} - \\mathbf{x}_k$，$c \\in \\mathbb{R}$ 是一个常数偏移，$\\mathbf{g} \\in \\mathbb{R}^n$ 是模型梯度，而 $H \\in \\mathbb{R}^{n \\times n}$ 是对称的模型 Hessian 矩阵。对于此问题，维度为 $n=3$。\n\n模型的系数，即 $c$、$\\mathbf{g}$ 以及 $H$ 的唯一元素，是通过对一组样本点上求得的函数值进行最小二乘拟合来确定的。我们有 $1+3+6=10$ 个未知系数：$c, g_1, g_2, g_3, H_{11}, H_{22}, H_{33}, H_{12}, H_{13}, H_{23}$。问题指定使用围绕 $\\mathbf{x}_k$ 的 $13$ 个样本点 $\\mathbf{x}_j$，这些点是通过大小为 $h$ 的步长 $\\mathbf{s}_j$ 生成的。位移向量 $\\mathbf{s}_j$ 的集合是 $\\{\\mathbf{0}\\} \\cup \\{\\pm h \\mathbf{e}_i\\}_{i=1}^3 \\cup \\{\\pm h (\\mathbf{e}_i + \\mathbf{e}_j)\\}_{1 \\le i  j \\le 3}$。\n\n令 $\\boldsymbol{\\beta}$ 为模型系数向量：\n$$\n\\boldsymbol{\\beta} = [c, g_1, g_2, g_3, H_{11}, H_{22}, H_{33}, H_{12}, H_{13}, H_{23}]^\\top\n$$\n模型可以表示为基函数 $\\phi_k(\\mathbf{s})$ 的线性组合：\n$$\nm(\\mathbf{s}) = \\beta_0 \\cdot 1 + \\sum_{i=1}^3 \\beta_i s_i + \\sum_{i=1}^3 \\beta_{3+i} \\left(\\frac{1}{2}s_i^2\\right) + \\beta_7 s_1s_2 + \\beta_8 s_1s_3 + \\beta_9 s_2s_3\n$$\n对于 13 个样本位移中的每一个 $\\mathbf{s}_j$，我们构建一个特征向量 $\\boldsymbol{\\phi}(\\mathbf{s}_j) = [1, s_{j,1}, s_{j,2}, s_{j,3}, \\frac{1}{2}s_{j,1}^2, \\frac{1}{2}s_{j,2}^2, \\frac{1}{2}s_{j,3}^2, s_{j,1}s_{j,2}, s_{j,1}s_{j,3}, s_{j,2}s_{j,3}]^\\top$。这些向量构成了设计矩阵 $\\Phi \\in \\mathbb{R}^{13 \\times 10}$ 的行。我们还将对应的目标函数值 $y_j = f(\\mathbf{x}_k + \\mathbf{s}_j)$ 收集到一个向量 $\\mathbf{y} \\in \\mathbb{R}^{13}$ 中。系数向量 $\\boldsymbol{\\beta}$ 通过求解线性最小二乘问题找到：\n$$\n\\min_{\\boldsymbol{\\beta}} \\|\\Phi \\boldsymbol{\\beta} - \\mathbf{y}\\|_2^2\n$$\n解由 $\\boldsymbol{\\beta} = (\\Phi^\\top \\Phi)^{-1} \\Phi^\\top \\mathbf{y}$ 给出。从 $\\boldsymbol{\\beta}$ 中，我们组装模型参数 $c, \\mathbf{g}, H$。\n\n### 2. 信赖域子问题\n\n一旦模型 $m(\\mathbf{s})$ 构建完成，我们寻找一个在半径为 $\\Delta > 0$ 的信赖域内最小化该模型的步长 $\\mathbf{s}$。\n\n#### 2.1. 球形信赖域（欧几里得范数）\n标准的信赖域子问题是求解：\n$$\n\\min_{\\mathbf{s} \\in \\mathbb{R}^n} m(\\mathbf{s}) \\quad \\text{subject to} \\quad \\|\\mathbf{s}\\|_2 \\leq \\Delta\n$$\n其中 $\\|\\mathbf{s}\\|_2 = \\sqrt{\\mathbf{s}^\\top \\mathbf{s}}$ 是欧几里得范数。\n\n#### 2.2. 椭球信赖域（椭球范数）\n可以使用对称正定矩阵 $M$ 定义一个各向异性信赖域：\n$$\n\\min_{\\mathbf{s} \\in \\mathbb{R}^n} m(\\mathbf{s}) \\quad \\text{subject to} \\quad \\|\\mathbf{s}\\|_M \\leq \\Delta\n$$\n其中 $\\|\\mathbf{s}\\|_M = \\sqrt{\\mathbf{s}^\\top M \\mathbf{s}}$。此问题通过变量变换来解决。令 $M = L^\\top L$ 为 $M$ 的 Cholesky 分解。我们定义一个新变量 $\\mathbf{y} = L\\mathbf{s}$，这意味着 $\\mathbf{s} = L^{-1}\\mathbf{y}$。约束变换为标准的欧几里得范数约束：\n$$\n\\|\\mathbf{s}\\|_M^2 = \\mathbf{s}^\\top M \\mathbf{s} = \\mathbf{s}^\\top L^\\top L \\mathbf{s} = (L\\mathbf{s})^\\top (L\\mathbf{s}) = \\mathbf{y}^\\top \\mathbf{y} = \\|\\mathbf{y}\\|_2^2 \\leq \\Delta^2\n$$\n目标模型用 $\\mathbf{y}$ 重写：\n$$\nm(\\mathbf{s}(\\mathbf{y})) = c + \\mathbf{g}^\\top (L^{-1}\\mathbf{y}) + \\frac{1}{2}(L^{-1}\\mathbf{y})^\\top H (L^{-1}\\mathbf{y}) = c + (L^{-\\top}\\mathbf{g})^\\top \\mathbf{y} + \\frac{1}{2}\\mathbf{y}^\\top (L^{-\\top}HL^{-1}) \\mathbf{y}\n$$\n这定义了一个新的二次模型 $m'(\\mathbf{y}) = c + \\mathbf{g}'^\\top \\mathbf{y} + \\frac{1}{2}\\mathbf{y}^\\top H' \\mathbf{y}$，其中 $\\mathbf{g}' = L^{-\\top}\\mathbf{g}$ 且 $H' = L^{-\\top}HL^{-1}$。子问题变成了在 $\\mathbf{y}$ 空间中的标准球形信赖域问题：\n$$\n\\min_{\\mathbf{y} \\in \\mathbb{R}^n} m'(\\mathbf{y}) \\quad \\text{subject to} \\quad \\|\\mathbf{y}\\|_2 \\leq \\Delta\n$$\n在求得最优步长 $\\mathbf{y}^*$ 后，我们变换回原始空间以获得步长：$\\mathbf{s}_M = L^{-1}\\mathbf{y}^*$。在问题是各向异性的情况下，选择 $M$ 来近似 $f$ 的真实 Hessian 矩阵可以产生更好的步长，因为它对子问题进行了预处理。\n\n### 3. Steihaug 的截断共轭梯度法\n\n信赖域子问题使用 Steihaug 方法近似求解。该方法将共轭梯度（CG）算法应用于二次型 $q(\\mathbf{s}) = \\mathbf{g}^\\top \\mathbf{s} + \\frac{1}{2}\\mathbf{s}^\\top H \\mathbf{s}$，并在步长离开信赖域或遇到非正曲率方向时终止。\n\n该算法过程如下：\n1.  初始化步长 $\\mathbf{s}_0 = \\mathbf{0}$，残差 $\\mathbf{r}_0 = \\mathbf{g}$，以及方向 $\\mathbf{d}_0 = -\\mathbf{r}_0$。\n2.  对于 $k=0, 1, 2, \\dots$:\n    a. 检查非正曲率：如果 $\\mathbf{d}_k^\\top H \\mathbf{d}_k \\leq 0$，计算一个步长 $\\tau > 0$ 使得 $\\|\\mathbf{s}_k + \\tau \\mathbf{d}_k\\|_2 = \\Delta$。最终解为 $\\mathbf{s}_k + \\tau \\mathbf{d}_k$。终止。\n    b. 计算 CG 步长大小：$\\alpha_k = (\\mathbf{r}_k^\\top \\mathbf{r}_k) / (\\mathbf{d}_k^\\top H \\mathbf{d}_k)$。\n    c. 计算一个预期的下一步长：$\\mathbf{s}_{k+1} = \\mathbf{s}_k + \\alpha_k \\mathbf{d}_k$。\n    d. 检查信赖域边界：如果 $\\|\\mathbf{s}_{k+1}\\|_2 > \\Delta$，计算一个步长 $\\tau > 0$ 使得 $\\|\\mathbf{s}_k + \\tau \\mathbf{d}_k\\|_2 = \\Delta$。最终解为 $\\mathbf{s}_k + \\tau \\mathbf{d}_k$。终止。\n    e. 更新残差：$\\mathbf{r}_{k+1} = \\mathbf{r}_k + \\alpha_k H \\mathbf{d}_k$。\n    f. 检查收敛性：如果 $\\|\\mathbf{r}_{k+1}\\|_2$ 低于某个容差，则以 $\\mathbf{s}_{k+1}$ 终止。\n    g. 更新搜索方向：$\\beta_k = (\\mathbf{r}_{k+1}^\\top \\mathbf{r}_{k+1}) / (\\mathbf{r}_k^\\top \\mathbf{r}_k)$，以及 $\\mathbf{d}_{k+1} = -\\mathbf{r}_{k+1} + \\beta_k \\mathbf{d}_k$。\n\n步骤 2a 和 2d 中的 $\\tau$ 值通过求解二次方程 $(\\mathbf{d}_k^\\top \\mathbf{d}_k)\\tau^2 + (2\\mathbf{s}_k^\\top \\mathbf{d}_k)\\tau + (\\mathbf{s}_k^\\top \\mathbf{s}_k - \\Delta^2) = 0$ 的正根得到。\n\n### 4. 比较与最终输出\n\n对于每个测试案例，我们执行以下过程：\n1.  从函数求值中构建二次模型 $(c, \\mathbf{g}, H)$。\n2.  使用 Steihaug 方法求解欧几里得范数下的子问题，以获得 $\\mathbf{s}_E$。\n3.  通过将模型变换为 $(\\mathbf{g}', H')$，使用 Steihaug 方法找到 $\\mathbf{y}^*$，并变换回原始空间以得到 $\\mathbf{s}_M = L^{-1}\\mathbf{y}^*$，从而求解椭球范数下的子问题。\n4.  在新点处评估真实目标函数值：$f(\\mathbf{x}_k+\\mathbf{s}_E)$ 和 $f(\\mathbf{x}_k+\\mathbf{s}_M)$。\n5.  该测试案例的结果是表达式 $f(\\mathbf{x}_k+\\mathbf{s}_M) \\leq f(\\mathbf{x}_k+\\mathbf{s}_E)$ 的布尔值。", "answer": "```python\nimport numpy as np\nfrom scipy.linalg import cholesky, solve_triangular\n\ndef solve():\n    \"\"\"\n    Main solver function that iterates through test cases and performs the comparison.\n    \"\"\"\n\n    test_cases = [\n        {\n            \"Q\": np.array([[1000, 20, 0], [20, 1, 0], [0, 0, 0.01]]),\n            \"q\": np.array([0, 0, -1]),\n            \"xk\": np.array([0, 0, 0]),\n            \"Delta\": 0.1, \"h\": 0.05,\n            \"M\": np.array([[1000, 20, 0], [20, 1, 0], [0, 0, 0.01]])\n        },\n        {\n            \"Q\": np.array([[5, 0, 0], [0, 5, 0], [0, 0, 5]]),\n            \"q\": np.array([1, -1, 0.5]),\n            \"xk\": np.array([0, 0, 0]),\n            \"Delta\": 0.1, \"h\": 0.05,\n            \"M\": np.eye(3)\n        },\n        {\n            \"Q\": np.array([[100, 0, 5], [0, 0.001, 0], [5, 0, 100]]),\n            \"q\": np.array([0, -1, 0]),\n            \"xk\": np.array([0, 0, 0]),\n            \"Delta\": 0.05, \"h\": 0.025,\n            \"M\": np.array([[100, 0, 5], [0, 0.001, 0], [5, 0, 100]])\n        },\n        {\n            \"Q\": np.array([[50, 0, 0], [0, 2, 0], [0, 0, 0.5]]),\n            \"q\": np.array([0.1, -0.2, 0.3]),\n            \"xk\": np.array([0, 0, 0]),\n            \"Delta\": 0.001, \"h\": 0.0005,\n            \"M\": np.array([[50, 0, 0], [0, 2, 0], [0, 0, 0.5]])\n        }\n    ]\n\n    results = []\n    for params in test_cases:\n        result = run_comparison(params)\n        results.append(result)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef objective_function(x, Q, q):\n    \"\"\" The true objective function f(x) = 0.5*x'Qx + q'x. \"\"\"\n    return 0.5 * x.T @ Q @ x + q.T @ x\n\ndef build_model(f_obj, xk, h):\n    \"\"\" Constructs a quadratic model via least-squares fit. \"\"\"\n    n = len(xk)\n    e = np.eye(n)\n    \n    sample_s = [np.zeros(n)]\n    # Points on axes\n    for i in range(n):\n        sample_s.append(h * e[:, i])\n        sample_s.append(-h * e[:, i])\n    # Points on planes\n    for i in range(n):\n        for j in range(i + 1, n):\n            sample_s.append(h * (e[:, i] + e[:, j]))\n            sample_s.append(-h * (e[:, i] + e[:, j]))\n\n    sample_s = np.array(sample_s)\n    y = np.array([f_obj(xk + s) for s in sample_s])\n\n    # Design matrix Phi\n    Phi = np.zeros((len(sample_s), 10))\n    Phi[:, 0] = 1.0  # c\n    Phi[:, 1:4] = sample_s # g\n    Phi[:, 4:7] = 0.5 * sample_s**2 # H_ii\n    Phi[:, 7] = sample_s[:, 0] * sample_s[:, 1] # H_12\n    Phi[:, 8] = sample_s[:, 0] * sample_s[:, 2] # H_13\n    Phi[:, 9] = sample_s[:, 1] * sample_s[:, 2] # H_23\n\n    beta, _, _, _ = np.linalg.lstsq(Phi, y, rcond=None)\n    \n    c = beta[0]\n    g = beta[1:4]\n    H = np.zeros((n,n))\n    H[0, 0], H[1, 1], H[2, 2] = beta[4], beta[5], beta[6]\n    H[0, 1] = H[1, 0] = beta[7]\n    H[0, 2] = H[2, 0] = beta[8]\n    H[1, 2] = H[2, 1] = beta[9]\n    \n    # We need to model the change m(s) = f(xk+s)-f(xk), so g is correct,\n    # H is correct, but c should be m(0)=0. The solver doesn't use c.\n    # The gradient g and Hessian H are for the full model m(s) approx f(xk+s).\n    # Since the TRS solver minimizes m(s) starting from s=0, the constant part of the\n    # model (c) does not affect the optimal step s.\n    return g, H\n\n\ndef steihaug_cg(g, H, Delta, tol=1e-9):\n    \"\"\"\n    Solves the trust-region subproblem using Steihaug's truncated CG method.\n    min g's + 0.5 s'Hs  s.t. ||s|| = Delta\n    \"\"\"\n    n = len(g)\n    s = np.zeros(n)\n    r = g\n    d = -r\n\n    if np.linalg.norm(r)  tol:\n        return s\n\n    for _ in range(n):\n        dTHd = d.T @ H @ d\n        \n        if dTHd = 0:\n            # Negative curvature direction. Find intersection with boundary.\n            tau = solve_tau_quadratic(s, d, Delta)\n            return s + tau * d\n\n        alpha = (r.T @ r) / dTHd\n        s_new = s + alpha * d\n\n        if np.linalg.norm(s_new) > Delta:\n            # Step exits trust region. Find intersection.\n            tau = solve_tau_quadratic(s, d, Delta)\n            return s + tau * d\n        \n        s = s_new\n        r_new = r + alpha * (H @ d)\n\n        if np.linalg.norm(r_new)  tol:\n            return s\n            \n        beta = (r_new.T @ r_new) / (r.T @ r)\n        d = -r_new + beta * d\n        r = r_new\n    \n    return s\n\ndef solve_tau_quadratic(s, d, Delta):\n    \"\"\"\n    Solves for tau > 0 in ||s + tau*d||^2 = Delta^2\n    \"\"\"\n    a = d.T @ d\n    b = 2 * (s.T @ d)\n    c = s.T @ s - Delta**2\n    discriminant = b**2 - 4*a*c\n    # We seek the positive root, which corresponds to moving \"forward\" along d.\n    # Since a > 0 and c = 0, the product ac is negative, discriminant is always positive\n    # and sqrt(discriminant) >= |b|, so -b + sqrt >= 0.\n    tau = (-b + np.sqrt(discriminant)) / (2 * a)\n    return tau\n\ndef run_comparison(params):\n    \"\"\"\n    Runs a single comparison for a given set of parameters.\n    \"\"\"\n    Q, q, xk, Delta, h, M = params['Q'], params['q'], params['xk'], params['Delta'], params['h'], params['M']\n    \n    f_obj = lambda x: objective_function(x, Q, q)\n\n    # 1. Build quadratic model\n    g, H = build_model(f_obj, xk, h)\n\n    # 2. Solve for Euclidean step s_E\n    s_E = steihaug_cg(g, H, Delta)\n\n    # 3. Solve for Ellipsoidal step s_M\n    L = cholesky(M, lower=True)\n    # L_inv = np.linalg.inv(L) # Slower, less stable\n    L_inv_T = solve_triangular(L, np.eye(L.shape[0]), lower=True, trans='T')\n    L_inv = L_inv_T.T\n\n    g_prime = L_inv.T @ g\n    H_prime = L_inv.T @ H @ L_inv\n\n    y_star = steihaug_cg(g_prime, H_prime, Delta)\n    s_M = L_inv @ y_star\n\n    # 4. Compare true objective values\n    f_E = f_obj(xk + s_E)\n    f_M = f_obj(xk + s_M)\n    \n    return f_M = f_E\n\nif __name__ == '__main__':\n    solve()\n```", "id": "3153334"}, {"introduction": "二次多项式模型是无导数优化的基石，但当面对高度非线性或多峰值的复杂函数时，它们可能难以胜任。本实践将引入一种更灵活的替代方案——径向基函数（RBF）模型，并展示一种基于交叉验证的原则性方法，用以判断何时值得采用更复杂的RBF模型。你将开发一个程序化测试，根据数据驱动的预测误差，为具体问题自动选择更优的模型。[@problem_id:3153247]", "problem": "您的任务是构建一个有原则的、程序化的检验，用以决定在基于模型的无导数优化中，何时应优先选择径向基函数（RBF）模型而非二次多项式模型来构建代理模型。该决策基于交叉验证的预测误差，适用于可能为多峰值的目标函数曲面。此问题必须使用基本原理和定义来解决：基于模型的无导数优化依赖代理建模来近似一个目标函数而无需使用导数；交叉验证通过留出数据点来估计泛化误差；RBF核岭回归和二次多项式岭回归是线性平滑器，其留一法预测可通过平滑矩阵恒等式计算，而无需重复拟合。\n\n定义以下模型和原理：\n- 代理模型是一个函数 $\\hat{f}(x)$，旨在使用仅在 $x$ 处的函数评估值来近似一个未知目标 $f(x)$。在基于模型的无导数优化中，代理模型拟合于采样数据 $\\{x_i, y_i\\}_{i=1}^m$，其中 $y_i = f(x_i)$。\n- 在使用高斯径向基函数核的核岭回归中，核函数为 $$K(x, x') = \\exp\\left(-(\\varepsilon \\lVert x - x' \\rVert_2)^2\\right),$$ 其中 $\\varepsilon > 0$ 是形状参数。给定核矩阵 $K \\in \\mathbb{R}^{m \\times m}$，其元素为 $K_{ij} = K(x_i, x_j)$，以及正则化参数 $\\lambda_r > 0$，训练点上的拟合值为 $$\\hat{\\mathbf{y}} = S_r \\mathbf{y}, \\quad S_r = K (K + \\lambda_r I)^{-1},$$ 其中 $I$ 是单位矩阵，$\\mathbf{y} \\in \\mathbb{R}^m$ 是 $y_i$ 的向量。\n- 在二次多项式岭回归中，设计矩阵 $X \\in \\mathbb{R}^{m \\times p}$ 收集了 $d$ 维中最高二次的基函数：常数项、线性项 $x_k$、纯二次项 $x_k^2$ 以及成对交叉项 $x_k x_\\ell$（其中 $1 \\le k  \\ell \\le d$）。给定正则化参数 $\\lambda_q > 0$，训练点上的拟合值为 $$\\hat{\\mathbf{y}} = S_q \\mathbf{y}, \\quad S_q = X \\left(X^\\top X + \\lambda_q I_p\\right)^{-1} X^\\top,$$ 其中 $I_p$ 是 $p \\times p$ 的单位矩阵。\n- 对于任何具有 $\\hat{\\mathbf{y}} = S \\mathbf{y}$ 形式的线性平滑器，在 $x_i$ 处的留一法交叉验证（LOOCV）预测可以无需重新拟合而通过以下公式计算：$$\\hat{y}^{(-i)} = \\frac{\\hat{y}_i - S_{ii} y_i}{1 - S_{ii}},$$ 前提是 $1 - S_{ii} \\neq 0$。LOOCV均方根误差（RMSE）为 $$\\mathrm{RMSE}_{\\mathrm{LOO}} = \\sqrt{\\frac{1}{m} \\sum_{i=1}^m \\left(\\hat{y}^{(-i)} - y_i\\right)^2}.$$\n- 提出以下基于交叉验证误差和受抽样变异性启发的裕度的切换规则：计算RBF核岭模型的 $\\mathrm{RMSE}_{r}$ 和二次岭模型的 $\\mathrm{RMSE}_{q}$，并设 $$\\gamma = \\alpha \\frac{s_y}{\\sqrt{m}},$$ 其中 $s_y$ 是 $\\{y_i\\}$ 的样本标准差（使用贝塞尔校正，即除以 $m-1$），$\\alpha$ 是一个固定常数。如果 $$\\mathrm{RMSE}_{r} + \\gamma  \\mathrm{RMSE}_{q},$$ 则选择RBF模型，否则选择二次模型。此规则仅在更灵活的RBF模型以一个随样本量增大而缩小的裕度足够优越时，才偏好它。\n\n您的程序必须：\n1. 根据上述规范实现两种模型，使用岭正则化以确保稳定性。\n2. 使用平滑矩阵恒等式计算两种模型的LOOCV RMSE，而不是通过重复拟合 $m$ 次。\n3. 使用上面定义的裕度 $\\gamma$ 和每个测试用例指定的 $\\alpha$ 来实现切换规则。\n4. 对于RBF核，将形状参数 $\\varepsilon$ 设置为采样点之间中值成对距离的倒数，即 $$\\varepsilon = \\frac{1}{\\mathrm{median}\\left\\{\\lVert x_i - x_j \\rVert_2 : 1 \\le i  j \\le m\\right\\}}.$$ 如果中值距离为零，则回退到一个小的正默认值。所有三角函数均使用弧度。\n\n构建并评估以下测试套件。在每种情况下，使用给定的随机种子从指定的超矩形中独立均匀地采样 $m$ 个点以保证可复现性，并计算 $y_i = f(x_i)$，不添加噪声。使用提供的正则化参数和裕度常数 $\\alpha$。\n\n- 测试用例1（多峰值二维Rastrigin函数）：\n    - 维度：$d = 2$\n    - 目标函数：$$f(x) = A d + \\sum_{k=1}^d \\left(x_k^2 - A \\cos(2 \\pi x_k)\\right), \\quad A = 10$$\n    - 定义域：$x \\in [-5.12, 5.12]^2$\n    - 样本量：$m = 60$\n    - 随机种子：$42$\n    - 正则化：$\\lambda_r = 10^{-3}$，$\\lambda_q = 10^{-6}$\n    - 裕度参数：$\\alpha = 0.1$\n\n- 测试用例2（二维单峰值二次碗形函数）：\n    - 维度：$d = 2$\n    - 目标函数：$$f(x) = (x_1 - 1)^2 + 2(x_2 + 0.5)^2 + 3$$\n    - 定义域：$x \\in [-2, 2]^2$\n    - 样本量：$m = 40$\n    - 随机种子：$123$\n    - 正则化：$\\lambda_r = 10^{-3}$，$\\lambda_q = 10^{-6}$\n    - 裕度参数：$\\alpha = 0.1$\n\n- 测试用例3（三维多峰值Ackley函数）：\n    - 维度：$d = 3$\n    - 目标函数：$$f(x) = -20 \\exp\\left(-0.2 \\sqrt{\\frac{1}{d} \\sum_{k=1}^d x_k^2}\\right) - \\exp\\left(\\frac{1}{d} \\sum_{k=1}^d \\cos(2 \\pi x_k)\\right) + 20 + e$$\n    - 定义域：$x \\in [-2, 2]^3$\n    - 样本量：$m = 80$\n    - 随机种子：$2024$\n    - 正则化：$\\lambda_r = 10^{-3}$，$\\lambda_q = 10^{-6}$\n    - 裕度参数：$\\alpha = 0.1$\n\n- 测试用例4（二维带轻微正弦扰动的二次函数）：\n    - 维度：$d = 2$\n    - 目标函数：$$f(x) = x_1^2 + x_2^2 + 0.1 \\sin(3 x_1) \\sin(3 x_2)$$\n    - 定义域：$x \\in [-1.5, 1.5]^2$\n    - 样本量：$m = 25$\n    - 随机种子：$7$\n    - 正则化：$\\lambda_r = 10^{-3}$，$\\lambda_q = 10^{-6}$\n    - 裕度参数：$\\alpha = 0.1$\n\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果，每个测试用例生成列表 $[b, \\mathrm{RMSE}_r, \\mathrm{RMSE}_q]$，其中 $b$ 是一个整数指示符，如果切换规则选择了RBF模型，则为 $1$，否则为 $0$。因此，最终输出是一个包含四个长度为三的列表的列表，对应四个测试用例，例如：\"[[b1,rmse_r1,rmse_q1],[b2,rmse_r2,rmse_q2],[b3,rmse_r3,rmse_q3],[b4,rmse_r4,rmse_q4]]\"。", "solution": "该问题要求在基于模型的无导数优化背景下，创建并实现一个定量检验，用以在两种代理模型之间做出选择：一种是更灵活的径向基函数（RBF）模型，另一种是更简单的二次多项式模型。该决策基于比较它们各自通过留一法交叉验证（LOOCV）估计的预测误差，并对更复杂的RBF模型施加惩罚。\n\n对于每个定义的测试用例，总体流程如下。首先，通过从指定的 $d$ 维定义域中均匀抽样 $m$ 个点 $x_i$，并评估给定的目标函数 $f$ 以使 $y_i = f(x_i)$，来生成一个由 $m$ 个输入-输出对 $\\{x_i, y_i\\}_{i=1}^m$ 组成的数据集。该数据集作为训练和验证两种模型的基础。\n\n第一个模型是二次多项式岭回归模型。对于此模型，我们构建一个设计矩阵 $X \\in \\mathbb{R}^{m \\times p}$。该矩阵的每一行对应一个样本点 $x_i$，并填充了最高二次的多项式基的值。对于一个点 $x = (x_1, \\dots, x_d)^\\top \\in \\mathbb{R}^d$，该基包括一个常数项（$1$）、线性项（$x_k$，其中 $k=1, \\dots, d$）、纯二次项（$x_k^2$，其中 $k=1, \\dots, d$）以及交叉乘积项（$x_k x_\\ell$，其中 $1 \\le k  \\ell \\le d$）。基函数的总数为 $p = \\frac{(d+1)(d+2)}{2}$。拟合值向量 $\\hat{\\mathbf{y}}_q$ 是通过一个线性平滑器得到的，即 $\\hat{\\mathbf{y}}_q = S_q \\mathbf{y}$，其中 $\\mathbf{y} = (y_1, \\dots, y_m)^\\top$ 是观测到的函数值向量。平滑矩阵 $S_q$ 定义为：\n$$S_q = X (X^\\top X + \\lambda_q I_p)^{-1} X^\\top$$\n在此表达式中，$\\lambda_q > 0$ 表示正则化参数，$I_p$ 是 $p \\times p$ 的单位矩阵。\n\n第二个模型是使用高斯径向基函数（RBF）核的核岭回归模型。RBF核由以下函数给出：\n$$K(x, x') = \\exp\\left(-(\\varepsilon \\lVert x - x' \\rVert_2)^2\\right)$$\n其中 $\\varepsilon > 0$ 是一个控制基函数“宽度”的形状参数。$\\varepsilon$ 的值是根据数据使用一种常用启发式方法确定的：它被设置为所有样本点 $\\{x_i\\}$ 之间成对欧几里得距离中位数的倒数。\n$$\\varepsilon = \\frac{1}{\\mathrm{median}\\left\\{\\lVert x_i - x_j \\rVert_2 : 1 \\le i  j \\le m\\right\\}}$$\n如果该中位数距离为零（在连续采样中是罕见事件），则使用一个小的正默认值。然后组装 $m \\times m$ 的核矩阵 $K$，其元素为 $K_{ij} = K(x_i, x_j)$。拟合值 $\\hat{\\mathbf{y}}_r$ 也是一个线性平滑器的结果，$\\hat{\\mathbf{y}}_r = S_r \\mathbf{y}$，其中平滑矩阵 $S_r$ 为：\n$$S_r = K (K + \\lambda_r I)^{-1}$$\n这里，$\\lambda_r > 0$ 是RBF模型的正则化参数，$I$ 是 $m \\times m$ 的单位矩阵。\n\n为了比较这两种模型的泛化性能，我们采用留一法交叉验证（LOOCV）。对于形如 $\\hat{\\mathbf{y}} = S\\mathbf{y}$ 的线性平滑器，一个关键的见解是，无需重复拟合模型 $m$ 次即可高效计算LOOCV残差。第 $i$ 个数据点的LOOCV预测 $\\hat{y}^{(-i)}$ 的误差由以下公式给出：\n$$y_i - \\hat{y}^{(-i)} = \\frac{y_i - \\hat{y}_i}{1 - S_{ii}}$$\n其中 $\\hat{y}_i$ 是点 $i$ 的原始拟合值，$S_{ii}$ 是相应平滑矩阵 $S$ 的第 $i$ 个对角元素。这使得可以快速计算每个模型的LOOCV均方根误差（RMSE）：\n$$\\mathrm{RMSE}_{\\mathrm{LOO}} = \\sqrt{\\frac{1}{m} \\sum_{i=1}^m \\left(\\frac{y_i - \\hat{y}_i}{1 - S_{ii}}\\right)^2}$$\n此公式用于计算二次模型的 $\\mathrm{RMSE}_{q}$ 和RBF模型的 $\\mathrm{RMSE}_{r}$。\n\n最后一步是应用指定的决策规则。为避免使用更灵活的RBF模型时出现过拟合，引入了一个惩罚项。二次模型被视为默认模型，只有当RBF模型的性能优于一个足够的裕度时，才选择RBF模型。规则是：如果\n$$\\mathrm{RMSE}_{r} + \\gamma  \\mathrm{RMSE}_{q}$$\n则选择RBF模型，否则选择二次模型。裕度项 $\\gamma$ 定义为：\n$$\\gamma = \\alpha \\frac{s_y}{\\sqrt{m}}$$\n其中 $\\alpha$ 是一个指定的常数，$s_y$ 是观测值 $\\{y_i\\}$ 的样本标准差（使用贝塞尔校正计算，即除以 $m-1$），$m$ 是样本量。这个裕度惩罚了RBF模型的复杂性，并随着可用数据的增多而缩小，反映了对估计误差的置信度增加。\n\n程序化实现将对每个测试用例执行这整个序列，报告所选模型的二元指示符（$1$ 表示RBF，$0$ 表示二次模型），以及计算出的 $\\mathrm{RMSE}_{r}$ 和 $\\mathrm{RMSE}_{q}$。", "answer": "```python\nimport numpy as np\nfrom scipy.spatial.distance import pdist, squareform\n\ndef solve():\n    \"\"\"\n    Main solver function to run all test cases and print the final result.\n    \"\"\"\n\n    def rastrigin_fn(x, A=10):\n        d = len(x)\n        return A * d + np.sum(x**2 - A * np.cos(2 * np.pi * x))\n\n    def quadratic_bowl_fn(x):\n        return (x[0] - 1)**2 + 2 * (x[1] + 0.5)**2 + 3\n\n    def ackley_fn(x):\n        d = len(x)\n        sum_sq = np.sum(x**2)\n        sum_cos = np.sum(np.cos(2 * np.pi * x))\n        term1 = -20 * np.exp(-0.2 * np.sqrt(sum_sq / d))\n        term2 = -np.exp(sum_cos / d)\n        return term1 + term2 + 20 + np.e\n\n    def perturbed_quadratic_fn(x):\n        return x[0]**2 + x[1]**2 + 0.1 * np.sin(3 * x[0]) * np.sin(3 * x[1])\n\n    test_cases = [\n        {\n            \"name\": \"Rastrigin_2D\",\n            \"d\": 2,\n            \"f\": rastrigin_fn,\n            \"domain\": [-5.12, 5.12],\n            \"m\": 60,\n            \"seed\": 42,\n            \"lambda_r\": 1e-3,\n            \"lambda_q\": 1e-6,\n            \"alpha\": 0.1\n        },\n        {\n            \"name\": \"Quadratic_Bowl_2D\",\n            \"d\": 2,\n            \"f\": quadratic_bowl_fn,\n            \"domain\": [-2, 2],\n            \"m\": 40,\n            \"seed\": 123,\n            \"lambda_r\": 1e-3,\n            \"lambda_q\": 1e-6,\n            \"alpha\": 0.1\n        },\n        {\n            \"name\": \"Ackley_3D\",\n            \"d\": 3,\n            \"f\": ackley_fn,\n            \"domain\": [-2, 2],\n            \"m\": 80,\n            \"seed\": 2024,\n            \"lambda_r\": 1e-3,\n            \"lambda_q\": 1e-6,\n            \"alpha\": 0.1\n        },\n        {\n            \"name\": \"Perturbed_Quadratic_2D\",\n            \"d\": 2,\n            \"f\": perturbed_quadratic_fn,\n            \"domain\": [-1.5, 1.5],\n            \"m\": 25,\n            \"seed\": 7,\n            \"lambda_r\": 1e-3,\n            \"lambda_q\": 1e-6,\n            \"alpha\": 0.1,\n        }\n    ]\n\n    def _compute_loocv_rmse(y, y_hat, S_diag):\n        \"\"\"Computes LOOCV RMSE using the GCV trick.\"\"\"\n        # Handle cases where 1 - S_ii is very close to zero\n        # to prevent division by zero or large numerical errors.\n        # For ridge-type problems, 0  S_ii  1, so this is a safeguard.\n        denominator = 1 - S_diag\n        safe_denominator = np.where(np.abs(denominator)  1e-12, 1e-12, denominator)\n        loocv_errors = (y - y_hat) / safe_denominator\n        return np.sqrt(np.mean(loocv_errors**2))\n\n    def evaluate_rbf(X_pts, y, lambda_r):\n        \"\"\"Evaluates the RBF model and computes its LOOCV RMSE.\"\"\"\n        pairwise_dists = pdist(X_pts, 'euclidean')\n        median_dist = np.median(pairwise_dists)\n        \n        if median_dist  1e-8:\n            epsilon = 1.0 / 1e-8\n        else:\n            epsilon = 1.0 / median_dist\n        \n        sq_dists = squareform(pairwise_dists**2)\n        K = np.exp(-(epsilon**2) * sq_dists)\n        \n        m = X_pts.shape[0]\n        Im = np.eye(m)\n        \n        # S_r = K @ inv(K + lambda_r * I)\n        A = K + lambda_r * Im\n        A_inv = np.linalg.inv(A)\n        S_r = K @ A_inv\n        \n        y_hat_r = S_r @ y\n        S_r_diag = np.diag(S_r)\n        \n        return _compute_loocv_rmse(y, y_hat_r, S_r_diag)\n\n    def evaluate_quadratic(X_pts, y, d, lambda_q):\n        \"\"\"Evaluates the quadratic model and computes its LOOCV RMSE.\"\"\"\n        m = X_pts.shape[0]\n        \n        # Build design matrix X\n        num_basis_funcs = (d + 1) * (d + 2) // 2\n        X_design = np.zeros((m, num_basis_funcs))\n        \n        col_idx = 0\n        for i in range(m):\n            pt = X_pts[i]\n            # Constant\n            X_design[i, 0] = 1.0\n            col_idx = 1\n            # Linear\n            X_design[i, col_idx:col_idx+d] = pt\n            col_idx += d\n            # Pure quadratic\n            X_design[i, col_idx:col_idx+d] = pt**2\n            col_idx += d\n            # Cross terms\n            for j in range(d):\n                for k in range(j + 1, d):\n                    X_design[i, col_idx] = pt[j] * pt[k]\n                    col_idx += 1\n\n        p = X_design.shape[1]\n        Ip = np.eye(p)\n        \n        # S_q = X @ inv(X.T @ X + lambda_q * I) @ X.T\n        B = X_design.T @ X_design + lambda_q * Ip\n        B_inv = np.linalg.inv(B)\n        \n        # Fitted values y_hat_q\n        y_hat_q = X_design @ (B_inv @ (X_design.T @ y))\n        \n        # Diagonal of S_q, S_q_ii = X_i @ inv(B) @ X_i.T\n        S_q_diag = np.sum((X_design @ B_inv) * X_design, axis=1)\n        \n        return _compute_loocv_rmse(y, y_hat_q, S_q_diag)\n    \n    results = []\n    for case in test_cases:\n        d = case[\"d\"]\n        m = case[\"m\"]\n        domain = case[\"domain\"]\n        \n        # Generate sample points\n        rng = np.random.default_rng(case[\"seed\"])\n        X_pts = rng.uniform(domain[0], domain[1], size=(m, d))\n        y = np.array([case[\"f\"](x) for x in X_pts])\n        \n        # Evaluate models\n        rmse_r = evaluate_rbf(X_pts, y, case[\"lambda_r\"])\n        rmse_q = evaluate_quadratic(X_pts, y, d, case[\"lambda_q\"])\n        \n        # Apply switching rule\n        s_y = np.std(y, ddof=1) if m > 1 else 0\n        gamma = case[\"alpha\"] * s_y / np.sqrt(m)\n        \n        b = 1 if rmse_r + gamma  rmse_q else 0\n        \n        results.append([b, rmse_r, rmse_q])\n\n    # Format the final output string exactly as required\n    result_strings = [f\"[{res[0]},{res[1]},{res[2]}]\" for res in results]\n    print(f\"[{','.join(result_strings)}]\")\n\nsolve()\n```", "id": "3153247"}]}