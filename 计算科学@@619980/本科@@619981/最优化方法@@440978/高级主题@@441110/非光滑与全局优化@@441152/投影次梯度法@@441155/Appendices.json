{"hands_on_practices": [{"introduction": "这项首次实践是一项基础练习。我们将解决一个经典的凸优化问题：在概率单纯形上最小化无穷范数。该问题 [@problem_id:3165079] 的精妙之处在于其最优解是已知的，这为评估我们算法的性能提供了一个清晰的基准。通过这个练习，你将实现投影次梯度法，并实证地比较几种基本步长策略的收敛行为，包括恒定步长、递减步长以及最优的 Polyak 步长。这将帮助你建立关于步长选择如何关键性地影响算法效率和收敛性的直观理解。", "problem": "考虑在概率单纯形 $C=\\{x\\in\\mathbb{R}^n:\\ x_i\\ge 0\\ \\text{for all}\\ i,\\ \\sum_{i=1}^n x_i=1\\}$ 上最小化函数 $f(x)=\\lVert x\\rVert_{\\infty}$ 的凸优化问题。仅从凸性、次梯度、范数和欧几里得投影的基本定义出发。具体来说，使用无穷范数的定义 $f(x)=\\max_{i\\in\\{1,\\dots,n\\}} x_i$，凸函数在一点的次梯度的定义（即满足 $f(y)\\ge f(x)+g^{\\top}(y-x)$ 对所有 $y$ 成立的任意向量 $g$），以及点到闭凸集上的欧几里得投影的定义（即集合中使欧几里得距离最小化的唯一一点）。\n\n任务：\n1. 推导 $f(x)$ 在 $C$ 上的一个显式极小点 $x^\\star$，并计算相应的最优值 $f^\\star=f(x^\\star)$。您的推导必须从上述基本定义开始，并且对任意的 $n\\in\\mathbb{N}$ 都有效。\n2. 对于由迭代公式\n$$\nx^{k+1}=\\Pi_C\\!\\left(x^k - t_k\\,g^k\\right),\n$$\n定义的投影次梯度法 (PSGM)，其中 $\\Pi_C$ 表示到 $C$ 上的欧几里得投影，$g^k\\in\\partial f(x^k)$ 是 $f$ 在 $x^k$ 处的一个次梯度，$\\{t_k\\}_{k\\ge 0}$ 是一个正步长序列，实现一个程序，该程序：\n   - 初始化于 $x^0=e_1$（$\\mathbb{R}^n$ 中的第一个标准基向量）。\n   - 在每次迭代中，选择一个次梯度 $g^k$，该次梯度是对应于 $x^k$ 当前最大分量索引的标准基向量（若有多个最大分量，则选择最小的索引）。\n   - 应用到单纯形 $C$ 上的欧几里得投影。\n   - 在第一个满足 $f(x^k)-f^\\star\\le \\varepsilon$ 的迭代索引 $k$ 处停止，其中 $\\varepsilon0$ 由每个测试用例给出。\n   - 如果在固定的最大迭代次数 $N_{\\max}$ 内没有找到这样的 $k$，则该测试用例返回 $-1$。\n\n您必须比较在四种不同的步长序列 $\\{t_k\\}$ 下，达到 $f(x^k)-f^\\star\\le \\varepsilon$ 所需的迭代次数：\n- 固定步长：$t_k=c$，对于给定的 $c0$。\n- 递减平方根步长：$t_k=\\alpha/\\sqrt{k+1}$，对于给定的 $\\alpha0$。\n- 调和步长：$t_k=\\beta/(k+1)$，对于给定的 $\\beta0$。\n- Polyak 步长（使用已知最优值）：$t_k=\\dfrac{f(x^k)-f^\\star}{\\lVert g^k\\rVert_2^2}$。\n\n使用以下测试套件，所有步骤均使用上述 PSGM 和指定参数执行：\n- 测试用例 1：$n=5$，$\\varepsilon=10^{-3}$，固定步长 $t_k$，$c=0.1$，$N_{\\max}=20000$。\n- 测试用例 2：$n=5$，$\\varepsilon=10^{-3}$，固定步长 $t_k$，$c=0.5$，$N_{\\max}=20000$。\n- 测试用例 3：$n=10$，$\\varepsilon=10^{-4}$，递减平方根步长 $t_k$，$α=0.3$，$N_{\\max}=20000$。\n- 测试用例 4：$n=2$，$\\varepsilon=10^{-6}$，调和步长 $t_k$，$β=0.7$，$N_{\\max}=50000$。\n- 测试用例 5：$n=7$，$\\varepsilon=0$，如上定义的 Polyak 步长，$N_{\\max}=10000$。\n\n对于每个测试用例，程序必须输出达到 $f(x^k)-f^\\star\\le \\varepsilon$ 所需的迭代次数，如果未在 $N_{\\max}$ 内满足此标准，则输出 $-1$。\n\n最终输出格式：\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表（例如，$[r_1,r_2,r_3,r_4,r_5]$），其中每个 $r_i$ 是相应测试用例的迭代次数的整数值。不应打印任何其他文本。", "solution": "问题陈述经评估有效。这是一个凸优化领域中定义明确的问题，基于成熟的数学原理。其定义、条件和测试用例是完整、一致且可形式化的。\n\n按要求，解决方案分为两部分。首先，是对极小点和最优值的解析推导。其次，是对实现投影次梯度法 (PSGM) 所需组件的详细描述。\n\n### 第 1 部分：优化问题的解析解\n\n该优化问题是在 $x$ 位于 $\\mathbb{R}^n$ 中的概率单纯形这一约束条件下，最小化函数 $f(x)=\\lVert x\\rVert_{\\infty}$。这可以写成：\n$$\n\\min_{x \\in \\mathbb{R}^n} \\lVert x \\rVert_\\infty \\quad \\text{subject to} \\quad x \\in C\n$$\n其中约束集是概率单纯形 $C = \\{x \\in \\mathbb{R}^n \\mid \\sum_{i=1}^n x_i = 1, x_i \\ge 0 \\text{ for } i=1, \\dots, n\\}$。\n\n要最小化的函数是 $f(x) = \\lVert x\\rVert_{\\infty}$。由于向量 $x \\in C$ 的所有分量 $x_i$ 都是非负的，无穷范数简化为 $f(x) = \\max_{i \\in \\{1, \\dots, n\\}} x_i$。\n\n令 $\\mu = f(x) = \\max_{i} x_i$。根据最大值的定义，对于任何 $x \\in C$，我们有 $x_i \\le \\mu$ 对所有 $i = 1, \\dots, n$ 成立。将此不等式对所有 $i$ 求和：\n$$\n\\sum_{i=1}^n x_i \\le \\sum_{i=1}^n \\mu = n\\mu\n$$\n单纯形约束要求 $\\sum_{i=1}^n x_i = 1$。将此代入不等式得到：\n$$\n1 \\le n\\mu\n$$\n这意味着对于任何可行点 $x \\in C$，目标函数的值 $\\mu = f(x)$ 必须满足：\n$$\nf(x) \\ge \\frac{1}{n}\n$$\n这为问题的最优值建立了一个下界 $1/n$。\n\n为了证明这个下界就是最小值，我们必须证明存在一个点 $x^\\star \\in C$ 使得 $f(x^\\star) = 1/n$。考虑由下式定义的均匀向量 $x^\\star$：\n$$\nx^\\star = \\left(\\frac{1}{n}, \\frac{1}{n}, \\dots, \\frac{1}{n}\\right)^{\\top}\n$$\n我们验证该点是否属于单纯形 $C$：\n1.  对于每个分量 $i$，$x^\\star_i = 1/n$。由于 $n \\in \\mathbb{N}$，我们有 $n \\ge 1$，所以 $x^\\star_i  0$。非负性约束 $x_i \\ge 0$ 得到满足。\n2.  分量之和为 $\\sum_{i=1}^n x^\\star_i = \\sum_{i=1}^n \\frac{1}{n} = n \\cdot \\frac{1}{n} = 1$。求和约束得到满足。\n\n因此，$x^\\star$ 是可行集 $C$ 中的一个点。目标函数在该点的值为：\n$$\nf^\\star = f(x^\\star) = \\max_{i \\in \\{1, \\dots, n\\}} \\left(\\frac{1}{n}\\right) = \\frac{1}{n}\n$$\n因为我们找到了一个达到下界 $1/n$ 的可行点 $x^\\star$，所以该点是 $f(x)$ 在 $C$ 上的一个极小点，且最优值为 $f^\\star = 1/n$。此外，这个极小点是唯一的。如果存在另一个点 $y \\in C$ 满足 $f(y) = 1/n$，则意味着 $\\max_i y_i = 1/n$。如果任何分量 $y_j$ 小于 $1/n$，那么为了满足 $\\sum_i y_i = 1$，至少必须有另一个分量 $y_k$ 大于 $1/n$，这与 $\\max_i y_i = 1/n$ 相矛盾。因此，任何极小点的所有分量都必须恰好是 $1/n$，这证明了唯一性。\n\n总之，唯一的极小点是 $x^\\star = (1/n, \\dots, 1/n)^{\\top}$，最优值是 $f^\\star = 1/n$。\n\n### 第 2 部分：投影次梯度法的设计\n\n投影次梯度法 (PSGM) 是一种用于约束凸优化的迭代算法。其迭代公式为：\n$$\nx^{k+1} = \\Pi_C(x^k - t_k g^k)\n$$\n其中 $x^k$ 是第 $k$ 步的迭代点，$g^k$ 是 $f$ 在 $x^k$ 处的一个次梯度，$t_k  0$ 是步长，$\\Pi_C$ 是到集合 $C$ 上的欧几里得投影。\n\n**$f(x) = \\lVert x \\rVert_\\infty$ 的次梯度**\n对于一个通用的凸函数 $\\phi$，如果对所有 $y$ 都有 $\\phi(y) \\ge \\phi(x) + g^{\\top}(y-x)$ 成立，则向量 $g$ 是在点 $x$ 处的一个次梯度。在点 $x$ 处的所有次梯度的集合称为次微分，记为 $\\partial \\phi(x)$。对于 $f(x) = \\lVert x \\rVert_\\infty$，在点 $x$ 处的次微分是对应于最大绝对值分量的所有索引 $i$ 的向量 $e_i \\cdot \\text{sign}(x_i)$ 的凸包。由于我们在单纯形 $C$ 上操作，所有 $x_i \\ge 0$，因此 $\\lVert x \\rVert_\\infty = \\max_i x_i$ 且 $\\text{sign}(x_i)$ 是非负的。次微分简化为：\n$$\n\\partial f(x) = \\text{conv}\\{e_i \\mid i \\in I(x)\\}\n$$\n其中 $I(x) = \\{i \\mid x_i = \\max_{j=1,\\dots,n} x_j\\}$，$e_i$ 是第 $i$ 个标准基向量。问题指定了一个确定性规则来选择一个次梯度 $g^k \\in \\partial f(x^k)$：令 $j$ 为 $I(x^k)$ 中的最小索引，并设 $g^k = e_j$。这是一个有效的次梯度，因为它是定义次微分的凸包的顶点之一。\n\n**到概率单纯形上的欧几里得投影 $(\\Pi_C)$**\n一个点 $z \\in \\mathbb{R}^n$ 到 $C$ 上的投影是以下二次规划问题的唯一解：\n$$\n\\Pi_C(z) = \\arg\\min_{x \\in C} \\frac{1}{2} \\lVert x - z \\rVert_2^2\n$$\n该问题的 Karush-Kuhn-Tucker (KKT) 条件导出一个形式为 $x_i = \\max(0, z_i - \\theta)$（对所有 $i$）的解。参数 $\\theta$ 是与等式约束 $\\sum_i x_i = 1$ 相关联的拉格朗日乘子，必须选择它以满足此约束。这意味着 $\\theta$ 是方程 $\\sum_{i=1}^n \\max(0, z_i - \\theta) = 1$ 的根。这可以被高效地求解。首先，将 $z$ 的分量按降序排序：$u_1 \\ge u_2 \\ge \\dots \\ge u_n$。最优的 $\\theta$ 通过确定投影后将保持为正的分量数量 $\\rho$ 来找到。值 $\\rho$ 是满足 $u_j - \\frac{1}{j}(\\sum_{i=1}^j u_i - 1)  0$ 的最大整数 $j$。一旦找到 $\\rho$，$\\theta$ 就被计算为 $\\theta = \\frac{1}{\\rho}(\\sum_{i=1}^\\rho u_i - 1)$。然后投影向量 $x$ 由 $x_i = \\max(0, z_i - \\theta)$ 给出。\n\n**步长规则**\n该算法比较了四种步长序列 $\\{t_k\\}_{k\\ge0}$：\n1.  **固定步长：** $t_k = c$，对于给定的常数 $c  0$。\n2.  **递减平方根步长：** $t_k = \\alpha/\\sqrt{k+1}$，对于给定的 $\\alpha  0$。\n3.  **调和步长：** $t_k = \\beta/(k+1)$，对于给定的 $\\beta  0$。\n4.  **Polyak 步长：** $t_k = \\frac{f(x^k) - f^\\star}{\\lVert g^k \\rVert_2^2}$。由于我们选择的次梯度是 $g^k=e_j$，其欧几里得范数的平方为 $\\lVert g^k \\rVert_2^2 = \\lVert e_j \\rVert_2^2 = 1$。该规则简化为 $t_k = f(x^k) - f^\\star = \\max_i x^k_i - 1/n$。\n\n**算法摘要**\n总体算法如下：\n1.  **初始化：** 对于给定的测试用例（$n$, $\\varepsilon$, $N_{\\max}$, 步长规则），设置 $f^\\star = 1/n$。初始化迭代计数器 $k = 0$ 和向量 $x^0 = e_1 = (1, 0, \\dots, 0)^{\\top}$。\n2.  **迭代循环：** 对于 $k = 0, 1, 2, \\dots, N_{\\max}$：\n    a. 计算当前目标值 $f(x^k) = \\max_i x^k_i$。\n    b. 检查停止准则：如果 $f(x^k) - f^\\star \\le \\varepsilon$，则终止并返回当前迭代次数 $k$。\n    c. 如果 $k=N_{\\max}$，已达到最大迭代次数但未收敛；跳出循环。\n    d. 选择次梯度 $g^k = e_j$，其中 $j = \\arg\\max_i x^k_i$（通过最小索引来解决平局）。\n    e. 使用测试用例指定的规则计算步长 $t_k$。\n    f. 执行更新步骤：$z^k = x^k - t_k g^k$。\n    g. 投影回单纯形以获得下一次迭代：$x^{k+1} = \\Pi_C(z^k)$。\n3.  **终止：** 如果循环完成而未满足停止准则，则返回 $-1$。\n\n将实施这种结构化方法来解决给定的测试用例。$\\varepsilon = 0$ 的测试用例要求迭代点精确达到最优值，这在使用浮点运算的情况下可能无法在有限步数内实现。在这种情况下，预计算法将运行最大迭代次数并返回 $-1$。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef project_simplex(z: np.ndarray) - np.ndarray:\n    \"\"\"\n    Projects a vector z onto the probability simplex using an efficient algorithm.\n\n    The probability simplex is the set C = {x | sum(x) = 1, x_i = 0}.\n    The projection is the solution to argmin_{x in C} ||x - z||_2.\n\n    Args:\n        z: A numpy array representing the vector to be projected.\n\n    Returns:\n        A numpy array representing the projected vector.\n    \"\"\"\n    n = len(z)\n    # The algorithm requires sorting the vector z in descending order.\n    u = np.sort(z)[::-1]\n    \n    # Compute the cumulative sum of the sorted vector.\n    cssv = np.cumsum(u)\n    \n    # Find rho, the number of positive components in the projected vector.\n    # This is done by finding the largest j such that u_j - (1/j)(sum_{i=1 to j} u_i - 1)  0.\n    # The condition is vectorized for efficiency.\n    indices = np.arange(1, n + 1)\n    condition = u - (cssv - 1) / indices  0\n    \n    # np.where returns non-zero indices. rho is the 1-based index.\n    rho_idx = np.where(condition)[0][-1]\n    rho = rho_idx + 1\n\n    # Compute the thresholding parameter theta.\n    theta = (cssv[rho_idx] - 1) / rho\n    \n    # The projection is obtained by shifting z by theta and taking the positive part.\n    x = np.maximum(0, z - theta)\n    \n    return x\n\ndef psgm_solver(n: int, epsilon: float, N_max: int, step_rule: str, params: dict) - int:\n    \"\"\"\n    Solves the optimization problem using the Projected Subgradient Method.\n    \n    Minimizes f(x) = ||x||_inf over the probability simplex.\n\n    Args:\n        n: Dimension of the space.\n        epsilon: Tolerance for the stopping criterion f(x^k) - f* = epsilon.\n        N_max: Maximum number of iterations.\n        step_rule: The name of the step-size rule to use.\n        params: A dictionary containing parameters for the step-size rule.\n\n    Returns:\n        The number of iterations k to reach the stopping criterion, or -1 if not met.\n    \"\"\"\n    f_star = 1.0 / n\n    x = np.zeros(n)\n    x[0] = 1.0  # Initialize at x^0 = e_1\n\n    for k in range(N_max + 1):\n        # We are at iteration k, with iterate x (representing x^k).\n        f_xk = np.max(x)\n\n        # Check stopping criterion.\n        if f_xk - f_star = epsilon:\n            return k\n\n        # If k has reached N_max, we've checked x^N_max and failed. Stop.\n        if k == N_max:\n            break\n\n        # --- Prepare for next iteration k+1 ---\n        \n        # 1. Subgradient g^k at x^k.\n        # np.argmax breaks ties by returning the first (smallest) index.\n        j = np.argmax(x)\n        g = np.zeros(n)\n        g[j] = 1.0\n\n        # 2. Step size t_k.\n        tk = 0.0\n        if step_rule == 'constant':\n            tk = params['c']\n        elif step_rule == 'sqrt':\n            tk = params['alpha'] / np.sqrt(k + 1)\n        elif step_rule == 'harmonic':\n            tk = params['beta'] / (k + 1)\n        elif step_rule == 'polyak':\n            # ||g^k||_2^2 is 1 since g^k is a standard basis vector.\n            tk = f_xk - f_star\n        \n        # 3. Update from x^k to x^{k+1}.\n        z = x - tk * g  # Subgradient descent step\n        x = project_simplex(z)  # Projection step\n\n    return -1\n\ndef solve():\n    \"\"\"\n    Defines and runs the test cases, then prints the results.\n    \"\"\"\n    # (n, epsilon, N_max, step_rule, {params})\n    test_cases = [\n        (5, 1e-3, 20000, 'constant', {'c': 0.1}),\n        (5, 1e-3, 20000, 'constant', {'c': 0.5}),\n        (10, 1e-4, 20000, 'sqrt', {'alpha': 0.3}),\n        (2, 1e-6, 50000, 'harmonic', {'beta': 0.7}),\n        (7, 0.0, 10000, 'polyak', {}),\n    ]\n\n    results = []\n    for case in test_cases:\n        n, epsilon, N_max, rule, params = case\n        result = psgm_solver(n, epsilon, N_max, rule, params)\n        results.append(result)\n\n    # Format and print the final output as a single line.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3165079"}, {"introduction": "在探索了步长策略之后，我们的第二次实践将深入探讨该算法的“投影”方面。我们将研究投影步骤如何改变基于次梯度的下降方向，这是该方法的一个关键行为。在这个练习中 [@problem_id:3165045]，你将在一个简单的盒子约束上最小化一个线性残差的无穷范数，其中投影是一个直观的坐标裁剪操作。通过追踪裁剪频率以及预期更新方向与实际更新方向之间的一致性等指标，你将对约束如何主动塑造优化路径，以及为何该方法尽管存在这些改变仍能收敛获得定量的理解。", "problem": "要求您设计并实现一个程序，用于评估在凸优化问题的投影次梯度法中，使用逐坐标裁剪作为到箱式约束的投影的效果。目标是分析在优化函数时，投影步骤多大频率上改变了预期的更新方向，并研究这种改变如何可能在有效的下降方向上引入偏差。该分析应基于凸优化的第一性原理，并通过在多个不同箱体大小的测试用例上运行该方法来进行经验验证。\n\n从以下基本定义和事实开始：\n- 如果对于所有 $x,y \\in \\mathbb{R}^n$ 和所有 $\\lambda \\in [0,1]$，都有 $f(\\lambda x + (1-\\lambda)y) \\le \\lambda f(x) + (1-\\lambda) f(y)$，则函数 $f:\\mathbb{R}^n \\to \\mathbb{R}$ 是凸函数。\n- 凸函数 $f$ 在点 $x$ 处的次梯度（记为 $\\partial f(x)$）是所有向量 $g \\in \\mathbb{R}^n$ 的集合，这些向量满足对于所有 $y \\in \\mathbb{R}^n$ 都有 $f(y) \\ge f(x) + g^\\top (y-x)$。$\\partial f(x)$ 中的任何元素 $g$ 都称为一个次梯度。\n- 点 $y \\in \\mathbb{R}^n$ 到非空闭凸集 $C \\subset \\mathbb{R}^n$ 上的投影（记为 $\\Pi_C(y)$）是集合 $C$ 中唯一的点 $z$，它最小化了欧几里得距离的平方 $\\|z - y\\|_2^2$。对于一个箱式约束 $C = \\{x \\in \\mathbb{R}^n \\mid \\ell \\le x \\le u\\}$，其下界 $\\ell \\in \\mathbb{R}^n$ 和上界 $u \\in \\mathbb{R}^n$ 是逐坐标定义的，投影 $\\Pi_C(y)$ 等于逐坐标裁剪：\n$$\n\\bigl(\\Pi_C(y)\\bigr)_i = \\min\\bigl\\{\\max\\{y_i, \\ell_i\\}, u_i\\bigr\\} \\quad \\text{for all } i \\in \\{1,\\dots,n\\}.\n$$\n- 投影次梯度法通过以下方式更新迭代点\n$$\nx_{k+1} = \\Pi_C\\bigl(x_k - \\alpha_k g_k\\bigr),\n$$\n其中 $g_k \\in \\partial f(x_k)$ 是一个次梯度，$\\alpha_k  0$ 是步长。\n\n考虑凸目标函数\n$$\nf(x) = \\|A x - b\\|_\\infty = \\max_{i \\in \\{1,\\dots,m\\}} \\bigl| (A x - b)_i \\bigr|,\n$$\n其中 $A \\in \\mathbb{R}^{m \\times n}$ 且 $b \\in \\mathbb{R}^m$。您的实现必须从第一性原理出发，在每次迭代 $k$ 中计算一个有效的次梯度 $g_k \\in \\partial f(x_k)$，并将逐坐标裁剪投影 $\\Pi_C$ 应用于箱体 $C$。\n\n算法要求：\n1. 初始化 $x_0 \\in C$。\n2. 在每次迭代 $k \\in \\{0,1,\\dots,T-1\\}$ 时，仅使用上述定义以及逐点最大值和绝对值函数次梯度的基本性质，为函数 $f(x) = \\|A x - b\\|_\\infty$ 计算一个次梯度 $g_k \\in \\partial f(x_k)$。\n3. 选择步长序列 $\\alpha_k = \\alpha_0/\\sqrt{k+1}$，其中 $\\alpha_0  0$ 是一个常数。\n4. 使用到箱体 $C$ 上的逐坐标裁剪来更新 $x_{k+1} = \\Pi_C(x_k - \\alpha_k g_k)$。\n5. 在迭代过程中收集以下定量指标：\n   - 经过 $T$ 次迭代后的最终目标值 $f(x_T)$。\n   - 平均每次迭代的裁剪比例，定义为在所有迭代 $k$ 上，坐标满足 $(\\Pi_C(x_k - \\alpha_k g_k))_i \\ne (x_k - \\alpha_k g_k)_i$ 的比例的均值。\n   - 预期步进方向 $-g_k$ 与实际位移 $d_k = x_{k+1} - x_k$ 之间的平均余弦相似度，定义于迭代中 $\\|g_k\\|_2$ 和 $\\|d_k\\|_2$ 均不为零的情况\n   $$\n   \\cos_k = \\frac{(-g_k)^\\top d_k}{\\|g_k\\|_2 \\, \\|d_k\\|_2}.\n   $$\n   平均值应在有定义 $\\cos_k$ 的迭代上计算。\n这些指标量化了裁剪对有效更新方向的影响及其潜在偏差。\n\n实现约束：\n- 使用 $m = 60$ 和 $n = 25$。\n- 使用固定种子为 0 的伪随机数生成器，确定性地构造 $A \\in \\mathbb{R}^{60 \\times 25}$ 和 $b \\in \\mathbb{R}^{60}$，其中 $A$ 的元素独立地从均值为 0、方差按 $1/n$ 缩放的正态分布中抽取，$b$ 的元素独立地从均值为 0、方差为 1 的标准正态分布中抽取。明确地，使用固定种子 0 设置 $A_{ij} \\sim \\mathcal{N}(0,1)/\\sqrt{n}$ 和 $b_i \\sim \\mathcal{N}(0,1)$。\n- 初始点 $x_0$ 必须选择为箱体的中点，即 $x_0 = (\\ell + u)/2$。\n\n测试套件：\n在以下四个测试用例上运行您的程序，以探索一系列裁剪机制。在每种情况下，使用 $T = 200$ 次迭代，并将上面列出的指标报告为浮点数。\n- 用例 1：箱体 $C = [-5,5]^n$，对所有 $i$ 有 $\\ell_i = -5$ 和 $u_i = 5$，且 $\\alpha_0 = 1.0$。\n- 用例 2：箱体 $C = [-0.05,0.05]^n$，对所有 $i$ 有 $\\ell_i = -0.05$ 和 $u_i = 0.05$，且 $\\alpha_0 = 1.0$。\n- 用例 3：各向异性箱体 $C$，对于 $i \\in \\{1,\\dots,10\\}$ 有 $\\ell_i = -0.02$ 和 $u_i = 0.02$，对于 $i \\in \\{11,\\dots,25\\}$ 有 $\\ell_i = -0.2$ 和 $u_i = 0.2$，且 $\\alpha_0 = 1.0$。\n- 用例 4：箱体 $C = [-0.05,0.05]^n$，对所有 $i$ 有 $\\ell_i = -0.05$ 和 $u_i = 0.05$，以及较小的步长尺度 $\\alpha_0 = 0.1$。\n\n最终输出格式：\n您的程序应生成单行输出，其中包含结果，格式为逗号分隔的列表的列表，每个内部列表对应一个测试用例，并按顺序包含三个浮点数 $\\bigl[f(x_T), \\text{平均裁剪比例}, \\text{平均余弦相似度}\\bigr]$。值必须以定点小数格式打印。例如：\n$$\n\\bigl[ [v_{1,1}, v_{1,2}, v_{1,3}], [v_{2,1}, v_{2,2}, v_{2,3}], [v_{3,1}, v_{3,2}, v_{3,3}], [v_{4,1}, v_{4,2}, v_{4,3}] \\bigr].\n$$\n不涉及物理单位。不直接报告角度；只报告无量纲的余弦相似度。程序必须硬编码上述测试用例，并以确切要求的格式生成单行最终输出。", "solution": "该问题要求分析用于在箱式约束下最小化凸函数的投影次梯度法。分析的重点是量化投影步骤对算法轨迹的影响。该问题是适定的，科学上基于凸优化的原理，并且所有参数都已明确指定。因此，我们将着手提供一个完整的解决方案。\n\n优化问题定义为：\n$$\n\\min_{x \\in C} f(x)\n$$\n其中目标函数为 $f(x) = \\|A x - b\\|_\\infty$（对于 $A \\in \\mathbb{R}^{m \\times n}$ 和 $b \\in \\mathbb{R}^m$），约束集是箱体 $C = \\{x \\in \\mathbb{R}^n \\mid \\ell \\le x \\le u\\}$。向量 $\\ell, u \\in \\mathbb{R}^n$ 分别定义了 $x$ 每个坐标的下界和上界。\n\n**1. 投影次梯度法**\n\n该算法通过以下更新规则生成一个迭代点序列 $\\{x_k\\}$：\n$$\nx_{k+1} = \\Pi_C(x_k - \\alpha_k g_k)\n$$\n此处，$x_k$ 是当前估计值，$g_k$ 是 $f$ 在 $x_k$ 处的一个次梯度，$\\alpha_k  0$ 是步长，$\\Pi_C$ 是到集合 $C$ 上的欧几里得投影。\n\n**2. 目标函数的次梯度**\n\n目标函数为 $f(x) = \\max_{i \\in \\{1,\\dots,m\\}} |(A x - b)_i|$。令 $a_i^\\top$ 为矩阵 $A$ 的第 $i$ 行。该函数可以写成 $m$ 个凸函数 $h_i(x) = |a_i^\\top x - b_i|$ 的逐点最大值。\n逐点最大值函数 $f(x) = \\max_{i} h_i(x)$ 的次微分是“激活”函数（即那些满足 $h_i(x)=f(x)$ 的函数）的次微分的并集的凸包。令 $I(x) = \\{i \\mid |a_i^\\top x - b_i| = \\|Ax - b\\|_\\infty\\}$ 为在 $x$ 处的激活索引集。那么，\n$$\n\\partial f(x) = \\text{conv} \\left( \\bigcup_{i \\in I(x)} \\partial h_i(x) \\right)\n$$\n为了找到单个次梯度 $g_k \\in \\partial f(x_k)$，我们可以选择任意激活索引 $j_k \\in I(x_k)$ 并从 $\\partial h_{j_k}(x_k)$ 中选择任意次梯度。一个方便的激活索引选择是 $j_k = \\text{argmax}_{i \\in \\{1, \\dots, m\\}} |a_i^\\top x_k - b_i|$。\n函数 $h_{j_k}(x) = |a_{j_k}^\\top x - b_{j_k}|$ 是绝对值函数与一个仿射函数的复合。根据次微分的链式法则，$h_{j_k}$ 在 $x_k$ 处的一个次梯度由 $s \\cdot \\nabla_x(a_{j_k}^\\top x - b_{j_k}) = s \\cdot a_{j_k}$ 给出，其中 $s \\in \\partial |\\cdot|$ 是在 $z = a_{j_k}^\\top x_k - b_{j_k}$ 处求值。绝对值函数的次微分在 $z \\neq 0$ 时为 $\\partial|z| = \\{\\text{sign}(z)\\}$，在 $z = 0$ 时为 $\\partial|z| = [-1, 1]$。\n因此，次梯度 $g_k$ 的一个有效且计算上简单的选择是：\n$$\ng_k = \\text{sign}(a_{j_k}^\\top x_k - b_{j_k}) a_{j_k}\n$$\n其中 $j_k$ 是使 $\\|Ax_k - b\\|_\\infty$ 达到最大值的索引。我们将使用 $\\text{sign}(0) = 0$ 的标准定义，这对应于从区间 $[-1, 1]$ 中选择 0，是一个有效的次梯度选择。\n\n**3. 到箱式约束的投影**\n\n点 $y \\in \\mathbb{R}^n$ 到箱体 $C = [\\ell, u]$ 上的投影是可分的，可以逐坐标计算。对于每个坐标 $i=1,\\dots,n$，投影为：\n$$\n(\\Pi_C(y))_i = \\min\\{\\max\\{y_i, \\ell_i\\}, u_i\\}\n$$\n这个操作也被称为裁剪。\n\n**4. 算法实现与分析**\n\n完整的算法按以下步骤进行，对于 $k=0, 1, \\dots, T-1$：\n- 初始化 $x_0 = (\\ell + u)/2$。\n- 对于每次迭代 $k$：\n    1. 计算残差 $r_k = A x_k - b$。\n    2. 找到激活索引 $j_k = \\text{argmax}_{i} |r_{k,i}|$。\n    3. 计算次梯度 $g_k = \\text{sign}(r_{k, j_k}) a_{j_k}^\\top$。\n    4. 设置步长 $\\alpha_k = \\alpha_0 / \\sqrt{k+1}$。\n    5. 执行暂定更新：$y_k = x_k - \\alpha_k g_k$。\n    6. 投影回可行集：$x_{k+1} = \\Pi_C(y_k)$。\n\n为了分析投影的影响，我们收集三个指标：\n1.  **最终目标值 $f(x_T)$**：衡量 $T$ 次迭代后解的质量。\n2.  **平均裁剪比例**：对于每次迭代 $k$，被投影改变的坐标比例为 $p_k = \\frac{1}{n} \\sum_{i=1}^n \\mathbb{I}\\left((x_{k+1})_i \\neq (y_k)_i\\right)$，其中 $\\mathbb{I}$ 是指示函数。平均值 $\\frac{1}{T}\\sum_{k=0}^{T-1} p_k$ 量化了投影被激活的频率。\n3.  **平均余弦相似度**：预期的更新方向是 $s_k = -g_k$。实际位移是 $d_k = x_{k+1} - x_k$。余弦相似度 $\\cos(\\theta_k) = \\frac{s_k^\\top d_k}{\\|s_k\\|_2 \\|d_k\\|_2}$ 衡量了这两个向量之间的对齐程度。值为 1 表示完美对齐（投影没有有效改变方向），而小于 1 的值表示投影使步进发生了偏转。负值则意味着步进方向与下降方向相反，这对于非锐角是可能的。平均值在所有 $s_k$ 和 $d_k$ 均不为零的迭代上计算。\n\n程序将使用给定的参数 $n=25$、$m=60$、$T=200$ 以及为 $A$ 和 $b$ 确定性生成的数据来执行。将评估四个不同的测试用例，这些用例改变了箱体 $C$ 和初始步长尺度 $\\alpha_0$，以观察在不同约束活跃程度下的行为。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef run_psm(A, b, ell, u, alpha_0, T):\n    \"\"\"\n    Runs the Projected Subgradient Method for T iterations.\n    \n    Args:\n        A (np.ndarray): The matrix A in the objective function.\n        b (np.ndarray): The vector b in the objective function.\n        ell (np.ndarray): The lower bounds of the box constraint.\n        u (np.ndarray): The upper bounds of the box constraint.\n        alpha_0 (float): The initial step size scale.\n        T (int): The number of iterations.\n        \n    Returns:\n        list: A list containing [final_objective, avg_clipping_prop, avg_cos_sim].\n    \"\"\"\n    m, n = A.shape\n    x = (ell + u) / 2.0\n    \n    clipping_proportions = []\n    cos_similarities = []\n\n    for k in range(T):\n        # 1. Compute a subgradient g_k\n        residual = A @ x - b\n        j_k = np.argmax(np.abs(residual))\n        \n        s_jk = np.sign(residual[j_k])\n        g_k = s_jk * A[j_k, :]\n        \n        # 2. Compute step size alpha_k\n        alpha_k = alpha_0 / np.sqrt(k + 1)\n        \n        # 3. Tentative update y_k\n        y_k = x - alpha_k * g_k\n        \n        # 4. Projection to get x_{k+1}\n        x_next = np.clip(y_k, ell, u)\n        \n        # 5. Collect metrics for analysis\n        # Clipping proportion\n        num_clipped = np.sum(x_next != y_k)\n        clipping_proportions.append(num_clipped / n)\n        \n        # Cosine similarity\n        d_k = x_next - x\n        \n        norm_g = np.linalg.norm(g_k)\n        norm_d = np.linalg.norm(d_k)\n        \n        # Average over iterations where both norms are non-zero.\n        if norm_g  1e-12 and norm_d  1e-12:\n            # Intended step direction is -g_k\n            cos_sim = (-g_k @ d_k) / (norm_g * norm_d)\n            cos_similarities.append(cos_sim)\n            \n        x = x_next\n    \n    # After T iterations, x is x_T\n    final_obj = np.max(np.abs(A @ x - b))\n    \n    avg_clip_prop = np.mean(clipping_proportions) if clipping_proportions else 0.0\n    avg_cos_sim = np.mean(cos_similarities) if cos_similarities else 0.0\n    \n    # The problem asks for floats to be reported.\n    return [float(final_obj), float(avg_clip_prop), float(avg_cos_sim)]\n\ndef solve():\n    \"\"\"\n    Main function to define test cases, run the optimizer, and print results.\n    \"\"\"\n    # Define problem dimensions and seed for deterministic data generation\n    m, n = 60, 25\n    seed = 0\n    T = 200\n\n    # Generate problem data (A, b)\n    rng = np.random.default_rng(seed)\n    A = rng.normal(loc=0.0, scale=1.0, size=(m, n)) / np.sqrt(n)\n    b = rng.normal(loc=0.0, scale=1.0, size=m)\n    \n    # Define the test suite\n    test_cases = [\n        # Case 1: Wide box, standard step size scale\n        {'ell': np.full(n, -5.0), 'u': np.full(n, 5.0), 'alpha_0': 1.0},\n        # Case 2: Narrow box, standard step size scale\n        {'ell': np.full(n, -0.05), 'u': np.full(n, 0.05), 'alpha_0': 1.0},\n        # Case 3: Anisotropic box, standard step size scale\n        {'ell': np.concatenate([np.full(10, -0.02), np.full(n-10, -0.2)]),\n         'u': np.concatenate([np.full(10, 0.02), np.full(n-10, 0.2)]),\n         'alpha_0': 1.0},\n        # Case 4: Narrow box, small step size scale\n        {'ell': np.full(n, -0.05), 'u': np.full(n, 0.05), 'alpha_0': 0.1},\n    ]\n\n    results = []\n    for case in test_cases:\n        result = run_psm(A, b, case['ell'], case['u'], case['alpha_0'], T)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    # The problem description contains an ambiguity between the textual requirement for\n    # \"fixed-point decimal format\" and the provided generic print statement template.\n    # This implementation adheres strictly to the print statement template provided in\n    # the OUTPUT STRUCTURE section, which relies on the default string representation\n    # of Python lists and floats.\n    print(str(results).replace(\"], [\", \"],[\"))\n    \nsolve()\n```", "id": "3165045"}, {"introduction": "我们的最后一次实践通过关注一个更复杂的可行集——欧几里得球体和盒子的交集，来提升挑战的难度。在许多实际应用中，投影到约束集上并非一个简单的操作。这个练习 [@problem_id:3164956] 要求你首先推导并实现一个针对这种复杂投影的高效算法，该过程本身涉及到求解一个一维求根问题。然后，你将把这个自定义的投影算子集成到投影次梯度法中，以解决一个 $\\ell_1$ 范数最小化问题，从而展示如何处理那些投影步骤本身就是一个重要子问题的优化问题。", "problem": "考虑闭凸集 $C \\subset \\mathbb{R}^n$，它由一个欧几里得（二范数）球和一个坐标级上限（箱型）的交集定义：\n$$\nC \\;=\\; \\left\\{ x \\in \\mathbb{R}^n \\;:\\; \\|x\\|_2 \\le R,\\;\\; |x_i| \\le u_i \\text{ for all } i=1,\\dots,n \\right\\},\n$$\n其中 $R \\ge 0$ 是给定的半径，向量 $u \\in \\mathbb{R}^n$ 的分量为非负值 $u_i \\ge 0$（每个 $u_i$ 可以为零，这将对应的坐标固定为零）。对于任意 $y \\in \\mathbb{R}^n$，将 $y$ 投影到 $C$ 上的欧几里得投影，记作 $\\Pi_C(y)$，是以下问题的唯一极小值点：\n$$\n\\min_{x \\in \\mathbb{R}^n} \\;\\; \\frac{1}{2}\\|x - y\\|_2^2 \\quad \\text{subject to} \\quad \\|x\\|_2 \\le R, \\;\\; |x_i| \\le u_i \\text{ for all } i.\n$$\n从投影到闭凸集的基本定义和 Karush-Kuhn-Tucker (KKT) 最优性条件（Karush-Kuhn-Tucker (KKT) conditions）出发，推导一个用于计算 $\\Pi_C(y)$ 的算法，该算法结合了排序和裁剪。您的算法必须：\n- 使用任何最优解 $x^\\star$ 在坐标级上与 $y$ 的符号模式对齐的特性。\n- 识别一个标量收缩参数如何与二范数约束耦合，并通过裁剪与箱型约束相互作用，从而产生一个可以通过一维搜索求解的单调关系。\n- 利用与箱型上限相关的断点的排序来为一维搜索建立一个可靠的区间。\n\n在推导并实现 $\\Pi_C(y)$ 后，将其集成到投影次梯度法中，以在可行集 $C$ 上最小化凸函数 $f(x) = \\|x\\|_1$。使用以下投影次梯度迭代：\n$$\nx^{k+1} \\;=\\; \\Pi_C\\Big(x^k \\;-\\; \\alpha_k\\,g^k \\Big),\n$$\n其中 $g^k \\in \\partial \\|x^k\\|_1$ 是 $\\ell_1$-范数在 $x^k$ 处的任意次梯度，$\\alpha_k$ 是一个递减的步长。对于此问题，逐分量选择次梯度为\n$$\ng^k_i \\;=\\; \n\\begin{cases}\n\\operatorname{sign}(x^k_i),  \\text{if } x^k_i \\neq 0, \\\\\n0,  \\text{if } x^k_i = 0,\n\\end{cases}\n$$\n并将步长设置为 $\\alpha_k = \\alpha_0 / \\sqrt{k+1}$，其中 $\\alpha_0  0$ 是一个给定的值。\n\n您的程序必须实现：\n1. 一个数值稳健的例程，用于为任何给定的 $y$、$R$ 和 $u$ 计算 $\\Pi_C(y)$，该例程使用断点排序和裁剪，并结合在标量收缩参数上的一维单调二分法。\n2. 具有指定次梯度和步长的投影次梯度法，以近似求解\n$$\n\\min_{x \\in C} \\|x\\|_1.\n$$\n\n测试套件：\n在以下五个测试案例上运行您的实现。对于每个案例，从给定的初始点 $x^0$ 开始执行投影次梯度法 $N$ 次迭代，并报告 $f(x^N) = \\|x^N\\|_1$ 的最终值（作为实数）：\n- 案例 1（两个约束均活跃）：\n  - 维度：$n=7$\n  - 半径：$R = 2.0$\n  - 上限：$u = [0.7,\\;0.9,\\;0.4,\\;1.2,\\;0.5,\\;0.8,\\;1.1]$\n  - 初始点：$x^0 = [2.5,\\;-1.7,\\;0.9,\\;3.0,\\;-2.2,\\;0.0,\\;1.5]$\n  - 步长参数：$\\alpha_0 = 0.6$\n  - 迭代次数：$N = 200$\n- 案例 2（仅二范数活跃）：\n  - 维度：$n=5$\n  - 半径：$R = 0.8$\n  - 上限：$u = [10.0,\\;10.0,\\;10.0,\\;10.0,\\;10.0]$\n  - 初始点：$x^0 = [1.0,\\;-2.0,\\;0.5,\\;0.8,\\;-0.3]$\n  - 步长参数：$\\alpha_0 = 0.5$\n  - 迭代次数：$N = 150$\n- 案例 3（仅箱型约束活跃）：\n  - 维度：$n=4$\n  - 半径：$R = 100.0$\n  - 上限：$u = [0.2,\\;0.3,\\;0.25,\\;0.15]$\n  - 初始点：$x^0 = [5.0,\\;-4.0,\\;3.0,\\;-2.0]$\n  - 步长参数：$\\alpha_0 = 3.0$\n  - 迭代次数：$N = 80$\n- 案例 4（退化半径）：\n  - 维度：$n=3$\n  - 半径：$R = 0.0$\n  - 上限：$u = [1.0,\\;1.0,\\;1.0]$\n  - 初始点：$x^0 = [0.5,\\;-0.5,\\;2.0]$\n  - 步长参数：$\\alpha_0 = 1.0$\n  - 迭代次数：$N = 50$\n- 案例 5（包含零的混合上限）：\n  - 维度：$n=6$\n  - 半径：$R = 1.5$\n  - 上限：$u = [0.0,\\;0.6,\\;0.0,\\;1.0,\\;0.3,\\;0.9]$\n  - 初始点：$x^0 = [-1.2,\\;2.0,\\;3.0,\\;-0.7,\\;0.4,\\;-2.5]$\n  - 步长参数：$\\alpha_0 = 0.7$\n  - 迭代次数：$N = 180$\n\n最终输出格式：\n您的程序应生成单行输出，其中包含五个案例的结果，以逗号分隔的列表形式，并用方括号括起来，顺序与上面列出的案例相同，每个条目等于该案例的最终目标值 $\\|x^N\\|_1$。例如，输出必须采用以下形式：\n$$\n[\\text{value}_1,\\text{value}_2,\\text{value}_3,\\text{value}_4,\\text{value}_5].\n$$\n不应打印任何额外文本。", "solution": "任务是推导并实现一个算法，用于将点欧几里得投影到一个凸集 $C$ 上，该凸集定义为一个中心在原点的欧几里得球和一个中心在原点的箱型的交集，然后使用这个投影在投影次梯度法中最小化 $C$ 上的 $\\ell_1$-范数。\n\n### 第 1 部分：投影算子 $\\Pi_C(y)$ 的推导\n\n点 $y \\in \\mathbb{R}^n$ 在闭凸集 $C$ 上的投影被定义为以下优化问题的唯一解：\n$$\n\\Pi_C(y) = \\arg\\min_{x \\in \\mathbb{R}^n} \\frac{1}{2}\\|x - y\\|_2^2\n$$\n其约束条件定义了集合 $C$：\n$$\n\\begin{align*}\n \\|x\\|_2^2 - R^2 \\le 0 \\\\\n x_i - u_i \\le 0, \\quad \\text{for } i=1, \\dots, n \\\\\n -x_i - u_i \\le 0, \\quad \\text{for } i=1, \\dots, n\n\\end{align*}\n$$\n其中 $R \\ge 0$ 且 $u_i \\ge 0$。这是一个具有严格凸目标函数的凸优化问题，因此存在唯一解 $x^\\star$。我们使用 Karush-Kuhn-Tucker (KKT) 条件来刻画这个解。\n\n该问题的拉格朗日函数是：\n$$\nL(x, \\lambda, \\boldsymbol{\\mu}^+, \\boldsymbol{\\mu}^-) = \\frac{1}{2}\\|x - y\\|_2^2 + \\frac{\\lambda}{2}(\\|x\\|_2^2 - R^2) + \\sum_{i=1}^n \\mu_i^+(x_i - u_i) + \\sum_{i=1}^n \\mu_i^-(-x_i - u_i)\n$$\n其中 $\\lambda \\ge 0$, $\\mu_i^+ \\ge 0$ 和 $\\mu_i^- \\ge 0$ 分别是球约束和箱型约束的拉格朗日乘子。\n\nKKT 的定常性条件要求拉格朗日函数关于 $x$ 的梯度在最优点 $x^\\star$ 处为零：\n$$\n\\nabla_x L(x^\\star, \\lambda, \\boldsymbol{\\mu}^+, \\boldsymbol{\\mu}^-) = (x^\\star - y) + \\lambda x^\\star + \\boldsymbol{\\mu}^+ - \\boldsymbol{\\mu}^- = 0\n$$\n对每个 $i=1, \\dots, n$ 逐分量写出：\n$$\nx_i^\\star - y_i + \\lambda x_i^\\star + \\mu_i^+ - \\mu_i^- = 0 \\quad \\implies \\quad (1+\\lambda)x_i^\\star = y_i - \\mu_i^+ + \\mu_i^-\n$$\n由于 $\\lambda \\ge 0$，我们有 $1+\\lambda \\ge 1$，所以可以写成：\n$$\nx_i^\\star = \\frac{y_i - \\mu_i^+ + \\mu_i^-}{1+\\lambda}\n$$\n互补松弛条件表明：\n$$\n\\begin{align*}\n \\lambda (\\|x^\\star\\|_2^2 - R^2) = 0 \\\\\n \\mu_i^+(x_i^\\star - u_i) = 0 \\quad \\text{for all } i \\\\\n \\mu_i^-(-x_i^\\star - u_i) = 0 \\quad \\text{for all } i\n\\end{align*}\n我们分析 $x_i^\\star$ 与乘子 $\\mu_i^+$ 和 $\\mu_i^-$ 之间的关系：\n1.  如果分量 $i$ 的箱型约束不活跃，即 $-u_i  x_i^\\star  u_i$，互补松弛性意味着 $\\mu_i^+ = 0$ 和 $\\mu_i^- = 0$。在这种情况下，$x_i^\\star = \\frac{y_i}{1+\\lambda}$。\n2.  如果 $x_i^\\star = u_i$（并假设 $u_i  0$），则必然有 $-x_i^\\star  u_i$，所以 $\\mu_i^- = 0$。定常性条件给出 $\\mu_i^+ = y_i - (1+\\lambda)u_i$。由于 $\\mu_i^+ \\ge 0$，这种情况在 $y_i - (1+\\lambda)u_i \\ge 0$ 时成立，即 $\\frac{y_i}{1+\\lambda} \\ge u_i$。\n3.  如果 $x_i^\\star = -u_i$（并假设 $u_i  0$），则必然有 $x_i^\\star  u_i$，所以 $\\mu_i^+ = 0$。定常性条件给出 $\\mu_i^- = -y_i - (1+\\lambda)u_i$。由于 $\\mu_i^- \\ge 0$，这种情况在 $-y_i - (1+\\lambda)u_i \\ge 0$ 时成立，即 $\\frac{y_i}{1+\\lambda} \\le -u_i$。\n\n结合这三种情况，我们可以用一个裁剪（或中值）操作将 $x_i^\\star$ 表示为单一参数 $\\lambda$ 的函数：\n$$\nx_i^\\star(\\lambda) = \\text{median}\\left\\{-u_i, \\frac{y_i}{1+\\lambda}, u_i\\right\\} = \\text{clip}\\left(\\frac{y_i}{1+\\lambda}, -u_i, u_i\\right)\n$$\n这可以用符号函数重写，证实了 $x_i^\\star$ 的符号与 $y_i$ 的符号一致（或者如果 $y_i=0$ 则为零）：\n$$\nx_i^\\star(\\lambda) = \\text{sign}(y_i) \\min\\left\\{\\frac{|y_i|}{1+\\lambda}, u_i\\right\\}\n$$\n现在我们必须找到满足所有 KKT 条件的正确的 $\\lambda \\ge 0$ 值。箱型约束通过构造已经满足。剩下的条件是原始可行性 $\\|x^\\star\\|_2 \\le R$ 和互补松弛性 $\\lambda (\\|x^\\star\\|_2^2 - R^2) = 0$。\n\n让我们测试 $\\lambda = 0$ 的情况。候选解是 $x^\\star(0) = \\text{clip}(y, -u, u)$。这只是 $y$ 在箱型约束上的投影。如果这个点也满足球约束，即 $\\|\\text{clip}(y, -u, u)\\|_2 \\le R$，那么所有 KKT 条件都以 $\\lambda=0$ 满足，我们就找到了解。\n\n如果 $\\|\\text{clip}(y, -u, u)\\|_2  R$，那么选择 $\\lambda=0$ 违反了原始可行性。根据互补松弛性，我们必须有 $\\lambda  0$，这反过来意味着球约束在解处必须是活跃的：$\\|x^\\star(\\lambda)\\|_2^2 = R^2$。因此，我们需要找到方程的根 $\\lambda  0$：\n$$\nf(\\lambda) = \\|x^\\star(\\lambda)\\|_2^2 - R^2 = \\sum_{i=1}^n \\left(\\text{sign}(y_i) \\min\\left\\{\\frac{|y_i|}{1+\\lambda}, u_i\\right\\}\\right)^2 - R^2 = 0\n$$\n我们引入变量替换 $v = 1 + \\lambda$。由于 $\\lambda  0$，我们在 $v  1$ 的范围内搜索。方程变为找到 $h(v)=0$ 的根：\n$$\nh(v) = \\sum_{i=1}^n \\left( \\min\\left\\{\\frac{|y_i|}{v}, u_i\\right\\} \\right)^2 - R^2 = 0\n$$\n函数 $h(v)$ 对于 $v  0$ 是连续且单调递减的。在 $v=1$ 时，$h(1) = \\|\\text{clip}(y, -u, u)\\|_2^2 - R^2  0$（根据我们的情况假设）。当 $v \\to \\infty$ 时，$h(v) \\to -R^2 \\le 0$（对于 $R0$）。因此，存在唯一的根 $v^\\star \\in (1, \\infty)$。\n\n这个根可以使用一维搜索算法（如二分法）高效地找到。为了建立一个可靠的搜索区间，我们注意到 $ \\min\\{\\frac{|y_i|}{v}, u_i\\} \\le \\frac{|y_i|}{v}$。因此，对于根 $v^\\star$：\n$$\nR^2 = \\sum_{i=1}^n \\left( \\min\\left\\{\\frac{|y_i|}{v^\\star}, u_i\\right\\} \\right)^2 \\le \\sum_{i=1}^n \\left(\\frac{|y_i|}{v^\\star}\\right)^2 = \\frac{\\|y\\|_2^2}{(v^\\star)^2}\n$$\n这意味着 $(v^\\star)^2 \\le \\frac{\\|y\\|_2^2}{R^2}$，所以 $v^\\star \\le \\frac{\\|y\\|_2}{R}$。由于我们知道 $v^\\star  1$，我们得到了一个紧凑的搜索区间 $[1, \\|y\\|_2/R]$。\n\n最终的投影算法如下：\n1.  如果 $R=0$，集合 $C$ 只是 $\\{0\\}$，所以 $\\Pi_C(y)=0$。\n2.  计算到箱型上的投影：$x_{\\text{box}} = \\text{clip}(y, -u, u)$。\n3.  如果 $\\|x_{\\text{box}}\\|_2 \\le R$，则解为 $x^\\star = x_{\\text{box}}$。\n4.  否则，使用二分法在区间 $[1, \\|y\\|_2/R]$ 上找到 $h(v)=0$ 的根 $v^\\star$。\n5.  解是 $x^\\star = \\text{sign}(y) \\min\\{|y|/v^\\star, u\\}$，其中操作是逐元素的。\n\n### 第 2 部分：投影次梯度法\n\n我们旨在解决 $\\min_{x \\in C} f(x)$，其中 $f(x) = \\|x\\|_1$。投影次梯度法通过以下更新规则生成一系列迭代点 $x^k$：\n$$\nx^{k+1} = \\Pi_C\\left(x^k - \\alpha_k g^k\\right)\n$$\n- $x^k$ 是当前迭代点。\n- $g^k$ 是 $f(x)$ 在 $x^k$ 处的次梯度。$\\ell_1$-范数在除了分量 $x_i^k = 0$ 的地方都可微。次微分是 $\\partial \\|x^k\\|_1 = \\{g \\in \\mathbb{R}^n \\mid g_i = \\text{sign}(x_i^k) \\text{ if } x_i^k \\neq 0, g_i \\in [-1, 1] \\text{ if } x_i^k = 0\\}$。问题指定使用特定的次梯度，即当 $x_i^k=0$ 时 $g_i=0$。这可以紧凑地写为 $g^k = \\text{sign}(x^k)$。\n- $\\alpha_k$ 是步长。问题指定了递减步长规则：$\\alpha_k = \\frac{\\alpha_0}{\\sqrt{k+1}}$，对于 $k=0, 1, 2, \\dots$。\n- $\\Pi_C$ 是在第 1 部分中推导出的投影算子。\n\n算法从一个初始点 $x^0$ 开始，对于固定的迭代次数 $N$，重复计算次梯度，沿负次梯度方向迈出一步，然后将结果投影回可行集 $C$。最终的迭代点 $x^N$ 提供了最小化问题的一个近似解，我们报告其目标值 $\\|x^N\\|_1$。", "answer": "```python\nimport numpy as np\n\ndef _projection(y: np.ndarray, R: float, u: np.ndarray, tol: float = 1e-12, max_iter: int = 100) - np.ndarray:\n    \"\"\"\n    Computes the Euclidean projection of a point y onto the set C.\n    C = {x | ||x||_2 = R and |x_i| = u_i}\n    \n    This is a helper function for the main solve() function.\n    \"\"\"\n    if R  0:\n        raise ValueError(\"Radius R must be non-negative.\")\n    \n    # Trivial case: projection onto {0}\n    if R == 0:\n        return np.zeros_like(y)\n\n    # Step 1: Project onto the box constraint |x_i| = u_i\n    y_abs = np.abs(y)\n    x_box = np.sign(y) * np.minimum(y_abs, u)\n    \n    # Step 2: Check if this point is already within the ball\n    norm_x_box = np.linalg.norm(x_box)\n    \n    if norm_x_box = R:\n        return x_box\n        \n    # Step 3: If not, the solution must lie on the boundary of the ball.\n    # We need to find v  1 such that ||clip(y/v, -u, u)||_2 = R.\n    # This is equivalent to finding the root of h(v) = ||clip(y/v, ...)||_2^2 - R^2 = 0.\n    # We use bisection search for v.\n    \n    norm_y = np.linalg.norm(y)\n    # If y is zero vector, norm_x_box would be 0, so we wouldn't reach this branch unless R>0.\n    if norm_y == 0:\n        return np.zeros_like(y)\n\n    # Establish search bracket [v_low, v_high] for v = 1 + lambda\n    v_low = 1.0\n    # From KKT analysis, the root v* must be = ||y||_2 / R\n    v_high = norm_y / R if R  0 else norm_y # Avoid division by zero, though R=0 is handled\n    \n    # Bisection search to find the root v*\n    for _ in range(max_iter):\n        v_mid = (v_low + v_high) / 2.0\n        # Check for convergence\n        if v_mid == v_low or v_mid == v_high:\n            break\n            \n        x_cand_abs = np.minimum(y_abs / v_mid, u)\n        norm_x_cand_sq = np.dot(x_cand_abs, x_cand_abs) # More efficient than norm\n        \n        if norm_x_cand_sq  R**2:\n            v_low = v_mid\n        else:\n            v_high = v_mid\n            \n    v_star = (v_low + v_high) / 2.0\n    \n    # Compute the final projection using the found v*\n    x_star = np.sign(y) * np.minimum(y_abs / v_star, u)\n    \n    return x_star\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite for the projected subgradient method.\n    \"\"\"\n    test_cases = [\n        # Case 1: both constraints active\n        {\n            \"R\": 2.0, \"u\": np.array([0.7, 0.9, 0.4, 1.2, 0.5, 0.8, 1.1]),\n            \"x0\": np.array([2.5, -1.7, 0.9, 3.0, -2.2, 0.0, 1.5]),\n            \"alpha0\": 0.6, \"N\": 200\n        },\n        # Case 2: only two-norm active\n        {\n            \"R\": 0.8, \"u\": np.array([10.0, 10.0, 10.0, 10.0, 10.0]),\n            \"x0\": np.array([1.0, -2.0, 0.5, 0.8, -0.3]),\n            \"alpha0\": 0.5, \"N\": 150\n        },\n        # Case 3: only box active\n        {\n            \"R\": 100.0, \"u\": np.array([0.2, 0.3, 0.25, 0.15]),\n            \"x0\": np.array([5.0, -4.0, 3.0, -2.0]),\n            \"alpha0\": 3.0, \"N\": 80\n        },\n        # Case 4: degenerate radius\n        {\n            \"R\": 0.0, \"u\": np.array([1.0, 1.0, 1.0]),\n            \"x0\": np.array([0.5, -0.5, 2.0]),\n            \"alpha0\": 1.0, \"N\": 50\n        },\n        # Case 5: mixed caps including zeros\n        {\n            \"R\": 1.5, \"u\": np.array([0.0, 0.6, 0.0, 1.0, 0.3, 0.9]),\n            \"x0\": np.array([-1.2, 2.0, 3.0, -0.7, 0.4, -2.5]),\n            \"alpha0\": 0.7, \"N\": 180\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        R, u, x0, alpha0, N = case[\"R\"], case[\"u\"], case[\"x0\"], case[\"alpha0\"], case[\"N\"]\n        \n        x = np.copy(x0)\n        \n        for k in range(N):\n            # Compute subgradient of ||x||_1\n            # np.sign() correctly returns 0 for input 0\n            g = np.sign(x)\n            \n            # Compute step size\n            alpha = alpha0 / np.sqrt(k + 1)\n            \n            # Subgradient step\n            y_update = x - alpha * g\n            \n            # Projection step\n            x = _projection(y_update, R, u)\n            \n        final_l1_norm = np.linalg.norm(x, 1)\n        results.append(final_l1_norm)\n        \n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3164956"}]}