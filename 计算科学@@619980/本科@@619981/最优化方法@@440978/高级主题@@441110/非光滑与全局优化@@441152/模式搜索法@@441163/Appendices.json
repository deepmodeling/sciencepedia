{"hands_on_practices": [{"introduction": "掌握任何算法的第一步都是亲手实现其核心引擎。本练习 [@problem_id:3161487] 将指导您实现经典的 Hooke-Jeeves (HJ) 算法，并将其应用于著名的 Rosenbrock 函数，这是一个形似狭长弯曲山谷的挑战性测试问题。通过这个实践，您将深入理解探索性移动和模式移动如何协同工作，以及步长自适应调整在引导搜索穿越复杂地形时的关键作用。", "problem": "实现一个确定性的 Hooke–Jeeves (HJ) 模式搜索算法，以最小化双变量 Rosenbrock 函数，从多个初始点开始，并强调步长如何自适应地遍历弯曲的峡谷。Rosenbrock 函数定义为 $f(x,y) = (1 - x)^2 + 100\\,(y - x^2)^2$。您的程序必须从直接搜索的基本定义开始实现以下内容。\n\n定义和要求：\n- Hooke–Jeeves (HJ) 方法交替进行探索性移动和（如果成功）模式移动。探索性移动沿着坐标方向进行轮询以寻找局部改进；模式移动则沿着从前一个基点到新的改进点的方向进行外推。该方法全程不使用梯度或导数。\n- 使用以下确定性探索性移动：对于当前点 $\\mathbf{x} \\in \\mathbb{R}^2$ 和步长 $\\Delta > 0$，按 $x$ 然后 $y$ 的顺序依次探测坐标，并在每个坐标内先探测正方向，然后是负方向。对于坐标 $i \\in \\{1,2\\}$，尝试 $\\mathbf{x} + \\Delta \\mathbf{e}_i$；如果这严格改进了 $f$，则接受该点并从这个新点继续探测下一个坐标。否则，尝试 $\\mathbf{x} - \\Delta \\mathbf{e}_i$；如果这严格改进了 $f$，则接受该点并继续；否则保持该坐标不变并继续。\n- 从一个基点 $\\mathbf{x}^{B}$ 进行一次完整的探索性移动，得到一个严格改进的点 $\\mathbf{x}^{E}$（即 $f(\\mathbf{x}^{E}) < f(\\mathbf{x}^{B})$）之后，尝试一个由 $\\mathbf{x}^{P} = \\mathbf{x}^{E} + p\\,(\\mathbf{x}^{E} - \\mathbf{x}^{B})$ 定义的模式移动，其中 $p > 0$ 是一个固定的模式因子。然后从 $\\mathbf{x}^{P}$ 开始，以相同的步长 $\\Delta$ 进行一次探索性移动，得到 $\\tilde{\\mathbf{x}}$。如果 $f(\\tilde{\\mathbf{x}}) < f(\\mathbf{x}^{E})$，则接受模式移动并将新基点设置为 $\\mathbf{x}^{B} \\leftarrow \\tilde{\\mathbf{x}}$；否则，放弃模式移动并将新基点设置为 $\\mathbf{x}^{B} \\leftarrow \\mathbf{x}^{E}$。\n- 如果从 $\\mathbf{x}^{B}$ 开始的探索性移动在步长 $\\Delta$ 下未能找到改进（即 $\\mathbf{x}^{E} = \\mathbf{x}^{B}$），则将步长乘以一个因子 $c \\in (0,1)$ 进行缩减，即 $\\Delta \\leftarrow c\\,\\Delta$。\n- 当 $\\Delta < \\Delta_{\\min}$ 或达到函数求值次数上限时终止。\n- 所有运行均使用以下固定参数：模式因子 $p = 1$，步长缩减因子 $c = 0.5$，最小步长 $\\Delta_{\\min} = 10^{-5}$，以及最大函数求值次数 $N_{\\max} = 200000$。所有改进的比较都应是严格的（使用 $$ 而不是 $\\leq$）。本问题不涉及角度。\n\n测试套件是以下初始条件列表，每个条件由一个初始点 $(x_0,y_0)$ 和一个初始步长 $\\Delta_0$ 指定：\n- 情况 1：$(x_0,y_0) = (-1.2, 1.0)$，$\\Delta_0 = 0.5$。\n- 情况 2：$(x_0,y_0) = (0.0, 0.0)$，$\\Delta_0 = 0.5$。\n- 情况 3：$(x_0,y_0) = (1.2, 1.2)$，$\\Delta_0 = 0.25$。\n- 情况 4：$(x_0,y_0) = (1.0, 1.0)$，$\\Delta_0 = 0.5$。\n- 情况 5：$(x_0,y_0) = (-1.5, 2.0)$，$\\Delta_0 = 1.0$。\n- 情况 6（用于立即终止的边缘情况）：$(x_0,y_0) = (0.0, 0.0)$，$\\Delta_0 = 10^{-7}$。\n\n对于每种情况，您的程序必须使用上述确切规则运行 Hooke–Jeeves 方法，并按顺序返回以下六个量：\n- 终止时的最终目标值 $f^\\star$，四舍五入保留六位小数。\n- 最终的 $x$ 坐标 $x^\\star$，四舍五入保留六位小数。\n- 最终的 $y$ 坐标 $y^\\star$，四舍五入保留六位小数。\n- 使用的函数求值总次数，为整数。\n- 执行的步长缩减总次数，为整数。\n- 接受的模式移动总次数，为整数。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含所有六种情况的结果，格式为列表的列表，并严格按照上述情况的顺序排列。\n- 每个内部列表必须是 $[f^\\star, x^\\star, y^\\star, N_{\\text{eval}}, N_{\\text{shrink}}, N_{\\text{pattern}}]$ 的形式，其中前三项打印为四舍五入到六位的小数，后三项打印为整数。\n- 整体输出必须打印为不含空格的单行，例如：$[[\\cdots],[\\cdots],\\ldots]$。\n\n关于科学真实性和基本原理的说明：\n- Rosenbrock 峡谷在最小值点 $(x,y) = (1,1)$ 附近狭窄且弯曲，形成了各向异性的曲率，这对基于步长的方法构成了挑战。Hooke–Jeeves 的步长自适应（当没有改进时 $\\Delta \\leftarrow c\\,\\Delta$）至关重要，它能充分减小 $\\Delta$ 以使探索性移动与峡谷的曲率对齐，同时模式移动能加速沿已发现的下降方向的进展。", "solution": "用户希望实现 Hooke-Jeeves (HJ) 确定性模式搜索算法，以找到双变量 Rosenbrock 函数 $f(x,y) = (1 - x)^2 + 100(y - x^2)^2$ 的最小值。\n\n### 问题验证\n\n**步骤 1：提取给定条件**\n- **目标函数**：$f(x,y) = (1 - x)^2 + 100(y - x^2)^2$。\n- **算法**：Hooke-Jeeves (HJ) 模式搜索，包含两个主要部分：探索性移动和模式移动。\n- **探索性移动定义**：从当前点 $\\mathbf{x}$ 开始，沿坐标轴 $x$ 然后 $y$ 进行探测。对于每个坐标，首先探测正方向（$\\mathbf{x} + \\Delta\\mathbf{e}_i$）。如果发现 $f$ 有严格改进，则接受新点，并*从这个新点*开始下一个坐标的探索。否则，探测负方向（$\\mathbf{x} - \\Delta\\mathbf{e}_i$）。如果这能改进 $f$，则接受并继续。如果两个方向都不能改进 $f$，则该坐标的点保持不变。\n- **模式移动定义**：如果从基点 $\\mathbf{x}^{B}$ 进行的探索性移动找到了一个更好的点 $\\mathbf{x}^{E}$（即 $f(\\mathbf{x}^{E})  f(\\mathbf{x}^{B})$），则定义一个模式点为 $\\mathbf{x}^{P} = \\mathbf{x}^{E} + p(\\mathbf{x}^{E} - \\mathbf{x}^{B})$。然后从 $\\mathbf{x}^{P}$ 进行第二次探索性移动，得到点 $\\tilde{\\mathbf{x}}$。\n- **更新规则**：\n    - 如果 $f(\\tilde{\\mathbf{x}})  f(\\mathbf{x}^{E})$，则接受模式移动，新基点变为 $\\mathbf{x}^{B} \\leftarrow \\tilde{\\mathbf{x}}$。\n    - 否则，拒绝模式移动，新基点为 $\\mathbf{x}^{B} \\leftarrow \\mathbf{x}^{E}$。\n- **步长缩减**：如果从 $\\mathbf{x}^{B}$ 开始的探索性移动未能找到任何改进，则步长 $\\Delta$ 缩减为 $\\Delta \\leftarrow c\\Delta$。\n- **终止条件**：当 $\\Delta  \\Delta_{\\min}$ 或函数求值次数超过 $N_{\\max}$ 时，算法停止。\n- **固定参数**：\n    - 模式因子 $p = 1$。\n    - 步长缩减因子 $c = 0.5$。\n    - 最小步长 $\\Delta_{\\min} = 10^{-5}$。\n    - 最大函数求值次数 $N_{\\max} = 200000$。\n- **测试用例（初始点 $(x_0, y_0)$，初始步长 $\\Delta_0$**）：\n    1. $((-1.2, 1.0), 0.5)$\n    2. $((0.0, 0.0), 0.5)$\n    3. $((1.2, 1.2), 0.25)$\n    4. $((1.0, 1.0), 0.5)$\n    5. $((-1.5, 2.0), 1.0)$\n    6. $((0.0, 0.0), 10^{-7})$\n- **要求输出**：对于每种情况，一个列表 $[f^\\star, x^\\star, y^\\star, N_{\\text{eval}}, N_{\\text{shrink}}, N_{\\text{pattern}}]$，其中前三个值四舍五入到六位小数。\n\n**步骤 2：使用提取的给定条件进行验证**\n- **科学依据**：该问题基于数值优化这一成熟的领域。Hooke-Jeeves 方法和 Rosenbrock 函数是该领域的经典主题。\n- **适定性**：该问题是适定的。算法是确定性的，所有参数和初始条件都已明确定义，终止标准也毫不含糊。这确保了每个测试用例都有唯一、可计算的结果。\n- **客观性**：该问题使用精确、客观的数学和算法语言陈述，没有主观解释的余地。\n- **结论**：问题陈述有效，因为它科学合理、完全指定且客观。它没有违反任何无效性标准。\n\n**步骤 3：判定与行动**\n问题有效。将提供完整解决方案。\n\n### 算法实现\n\n解决方案涉及按规定直接实现 Hooke-Jeeves 算法。核心逻辑分为三个主要部分：控制整体流程的主函数 `hooke_jeeves`，用于 `exploratory_move` 的辅助函数，以及一个管理函数求值的类。\n\n1.  **函数求值计数器**：为目标函数 $f(x,y)$ 使用一个包装类 `FunctionEvaluator`。每次通过该类实例调用函数时，内部计数器都会递增。这种方法集中了函数求值次数的计数，这是一个关键的终止条件。\n\n2.  **探索性移动函数 (`exploratory_move`)**：此函数实现沿坐标轴的搜索。它接受一个起始点 $\\mathbf{x}_{\\text{start}}$、其函数值 $f(\\mathbf{x}_{\\text{start}})$、步长 $\\Delta$ 和函数求值器。实现的关键在于遵循指定的贪婪、顺序更新规则：一旦沿某个坐标（例如 $x$ 轴正方向）找到改进，该新点立即成为后续沿下一个坐标（例如 $y$ 轴）搜索的基点。这确保了探索期间的路径是严格下降的。\n\n3.  **主循环 (`hooke_jeeves`)**：此函数协调整个过程。\n    - **初始化**：它从初始点 $\\mathbf{x}_0$ 和初始步长 $\\Delta_0$ 开始。初始化步长缩减次数（$N_{\\text{shrink}}$）和接受的模式移动次数（$N_{\\text{pattern}}$）的计数器。维护一个状态变量 `x_best_ever`，用于追踪整个搜索过程中找到的最佳点，这在因达到 $N_{\\max}$ 而终止时至关重要。\n    - **迭代**：主 `while` 循环持续进行，直到不满足终止条件（$\\Delta \\ge \\Delta_{\\min}$ 和 $N_{\\text{eval}}  N_{\\max}$）。\n    - **每次迭代的主要逻辑**：\n        a. 从当前基点 $\\mathbf{x}_{\\text{base}}$ 开始执行一次探索性移动。\n        b. **如果找到改进**：如果探索性移动产生一个点 $\\mathbf{x}_{\\text{explore}}$ 且 $f(\\mathbf{x}_{\\text{explore}})  f(\\mathbf{x}_{\\text{base}})$，则尝试进行模式移动。模式点计算为 $\\mathbf{x}_{P} = \\mathbf{x}_{\\text{explore}} + p(\\mathbf{x}_{\\text{explore}} - \\mathbf{x}_{\\text{base}})$。从 $\\mathbf{x}_{P}$ 开始进行第二次探索性移动，以找到 $\\tilde{\\mathbf{x}}$。如果 $f(\\tilde{\\mathbf{x}})  f(\\mathbf{x}_{\\text{explore}})$，则下一次迭代的新基点设置为 $\\tilde{\\mathbf{x}}$（接受模式），否则设置为 $\\mathbf{x}_{\\text{explore}}$（拒绝模式）。\n        c. **如果没有改进**：如果初始的探索性移动未能找到更好的点，则步长 $\\Delta$ 按缩减因子 $c$ 减小，下一次迭代的基点保持不变。\n    - **终止**：一旦循环终止，函数返回找到的最佳目标值、相应的坐标 $(x^\\star, y^\\star)$，以及 $N_{\\text{eval}}$、$N_{\\text{shrink}}$ 和 $N_{\\text{pattern}}$ 的最终计数。\n\n程序将对六个提供的测试用例分别执行此逻辑，并将输出格式化为单个列表的列表。", "answer": "```python\nimport numpy as np\n\ndef rosenbrock_func(x, y):\n    \"\"\"The Rosenbrock function.\"\"\"\n    return (1.0 - x)**2 + 100.0 * (y - x**2)**2\n\nclass FunctionEvaluator:\n    \"\"\"A wrapper class to count function evaluations.\"\"\"\n    def __init__(self, func):\n        self.func = func\n        self.count = 0\n    \n    def __call__(self, p):\n        self.count += 1\n        return self.func(p[0], p[1])\n\ndef exploratory_move(x_start, f_start, delta, f_evaluator):\n    \"\"\"\n    Performs an exploratory move from a starting point, as per the problem description.\n\n    Args:\n        x_start: The starting point for the exploration.\n        f_start: The function value at x_start, to avoid re-evaluation.\n        delta: The current step size.\n        f_evaluator: The callable function-counting wrapper.\n\n    Returns:\n        A tuple (x_current, f_current) representing the best point found and its function value.\n    \"\"\"\n    x_current = np.copy(x_start)\n    f_current = f_start\n    \n    # Sequentially probe each coordinate\n    for i in range(len(x_start)):\n        # Probe in the positive direction\n        x_test = np.copy(x_current)\n        x_test[i] += delta\n        f_test = f_evaluator(x_test)\n        \n        if f_test  f_current:\n            x_current = x_test\n            f_current = f_test\n            continue  # Accepted, move to next coordinate from this new point\n        \n        # If positive failed, probe in the negative direction\n        x_test[i] -= 2 * delta  # From x_current[i] + delta to x_current[i] - delta\n        f_test = f_evaluator(x_test)\n        \n        if f_test  f_current:\n            x_current = x_test\n            f_current = f_test\n    \n    return x_current, f_current\n\ndef hooke_jeeves(x0, delta0, p, c, delta_min, n_max):\n    \"\"\"\n    Implements the Hooke-Jeeves pattern search algorithm.\n    \"\"\"\n    f_evaluator = FunctionEvaluator(rosenbrock_func)\n    \n    x_base = np.array(x0, dtype=float)\n    delta = float(delta0)\n    \n    n_shrinks = 0\n    n_patterns = 0\n    \n    # Initialize with the starting point. This is the first function evaluation.\n    x_best_ever = np.copy(x_base)\n    f_best_ever = f_evaluator(x_best_ever)\n    f_base = f_best_ever\n    \n    while True:\n        # Check termination conditions before starting the iteration\n        if delta  delta_min:\n            break\n        if n_max is not None and f_evaluator.count >= n_max:\n            break\n            \n        x_base_iter_start = np.copy(x_base)\n        \n        # Perform an exploratory move from the current base point\n        x_explore, f_explore = exploratory_move(x_base, f_base, delta, f_evaluator)\n        \n        if f_explore  f_base:\n            # Improvement found after exploration.\n            if f_explore  f_best_ever:\n                f_best_ever = f_explore\n                x_best_ever = np.copy(x_explore)\n\n            # Attempt a pattern move\n            x_pattern_start = x_explore + p * (x_explore - x_base_iter_start)\n            \n            if n_max is not None and f_evaluator.count >= n_max:\n                 x_base, f_base = x_explore, f_explore\n                 break\n\n            f_pattern_start = f_evaluator(x_pattern_start)\n            \n            # Perform a second exploratory move from the pattern point\n            x_tilde, f_tilde = exploratory_move(x_pattern_start, f_pattern_start, delta, f_evaluator)\n\n            # Decide whether to accept the pattern move\n            if f_tilde  f_explore:\n                x_base = np.copy(x_tilde)\n                f_base = f_tilde\n                n_patterns += 1\n            else:\n                x_base = np.copy(x_explore)\n                f_base = f_explore\n                \n            if f_base  f_best_ever:\n                 f_best_ever = f_base\n                 x_best_ever = np.copy(x_base)\n\n        else:\n            # No improvement, so reduce the step size.\n            delta *= c\n            n_shrinks += 1\n            \n    return f_best_ever, x_best_ever[0], x_best_ever[1], f_evaluator.count, n_shrinks, n_patterns\n\ndef solve():\n    \"\"\"\n    Runs the Hooke-Jeeves algorithm for a suite of test cases and prints the results.\n    \"\"\"\n    # Fixed parameters for all runs\n    p = 1.0\n    c = 0.5\n    delta_min = 1e-5\n    n_max = 200000\n\n    # Test suite: (initial_point, initial_delta)\n    test_cases = [\n        # Case 1\n        {'x0': (-1.2, 1.0), 'delta0': 0.5},\n        # Case 2\n        {'x0': (0.0, 0.0), 'delta0': 0.5},\n        # Case 3\n        {'x0': (1.2, 1.2), 'delta0': 0.25},\n        # Case 4\n        {'x0': (1.0, 1.0), 'delta0': 0.5},\n        # Case 5\n        {'x0': (-1.5, 2.0), 'delta0': 1.0},\n        # Case 6\n        {'x0': (0.0, 0.0), 'delta0': 1e-7},\n    ]\n\n    all_results = []\n    for case in test_cases:\n        f_star, x_star, y_star, n_eval, n_shrink, n_pattern = hooke_jeeves(\n            x0=case['x0'],\n            delta0=case['delta0'],\n            p=p,\n            c=c,\n            delta_min=delta_min,\n            n_max=n_max\n        )\n        \n        # A special check for Case 4 (start at optimum). The algorithm as written\n        # may produce -0.0 due to floating point nuances. Correct to 0.0.\n        formatted_result = [\n            round(f_star, 6),\n            round(x_star, 6),\n            round(y_star, 6),\n            int(n_eval),\n            int(n_shrink),\n            int(n_pattern)\n        ]\n        if formatted_result[0] == -0.0: formatted_result[0] = 0.0\n        \n        all_results.append(str(formatted_result))\n\n    # The required output format is a single line, list of lists.\n    # The string representation of a list of lists is needed.\n    # We join the string representation of each inner list.\n    print(f\"[{','.join(all_results)}]\")\n\nsolve()\n```", "id": "3161487"}, {"introduction": "模式搜索方法的一大优势在于其无需导数，这使其成为处理非光滑（non-smooth）优化问题的有力工具。本练习 [@problem_id:3161536] 探讨了如何应用 HJ 算法最小化一个复合目标函数，该函数结合了最小二乘数据拟合项和稀疏性驱动的 $\\ell_1$ 范数正则化项。这个实践将帮助您理解如何利用直接搜索处理在机器学习和信号处理中常见的非微分问题，并体会正则化在塑造解的结构中的作用。", "problem": "实现一个无导数模式搜索，使用经典的 Hooke–Jeeves (HJ) 方法来最小化一个复合目标函数，该函数结合了最小二乘数据失配项和稀疏性促进正则化项。您必须构建一个形式如下的目标函数\n$$\nf(\\mathbf{x}) \\;=\\; \\left\\|A\\mathbf{x} - \\mathbf{b}\\right\\|_2 \\;+\\; \\tau \\left\\|\\mathbf{x}\\right\\|_1,\n$$\n其中 $A \\in \\mathbb{R}^{m \\times n}$，$\\mathbf{b} \\in \\mathbb{R}^m$，$\\mathbf{x} \\in \\mathbb{R}^n$，符号 $\\left\\| \\cdot \\right\\|_2$ 表示欧几里得范数，符号 $\\left\\| \\cdot \\right\\|_1$ 表示绝对值之和。该目标函数是凸的，但由于 $\\ell_1$ 项的存在，通常是不可微的，这促使我们使用直接搜索算法。\n\n从以下基础概念开始：\n- 无约束最小化是寻找 $\\mathbf{x}^\\star \\in \\mathbb{R}^n$ 以最小化一个实值函数 $f:\\mathbb{R}^n \\to \\mathbb{R}$ 的任务。\n- 欧几里得范数定义为 $\\left\\|\\mathbf{y}\\right\\|_2 = \\sqrt{\\sum_{i=1}^m y_i^2}$，对于任意 $\\mathbf{y}\\in\\mathbb{R}^m$。\n- $\\ell_1$ 范数定义为 $\\left\\|\\mathbf{x}\\right\\|_1 = \\sum_{i=1}^n |x_i|$。\n- 直接搜索模式法仅使用函数求值和步长控制来在搜索空间中导航。\n\n使用以下结构实现经典的 Hooke–Jeeves (HJ) 方法：\n- 探查移动：给定一个基点 $\\mathbf{x}_B$ 和一个步长 $\\Delta0$，对于 $i \\in \\{1,\\dots,n\\}$，依次沿坐标方向 $\\pm \\Delta \\mathbf{e}_i$ 进行探查，其中 $\\mathbf{e}_i$ 是 $\\mathbb{R}^n$ 中的第 $i$ 个标准基向量。当且仅当试验点严格减小了目标值时，才接受该点。\n- 模式移动：如果探查移动找到了一个改进点 $\\mathbf{x}_E$ 满足 $f(\\mathbf{x}_E)  f(\\mathbf{x}_B)$，则尝试沿方向 $\\mathbf{p} = \\mathbf{x}_E - \\mathbf{x}_B$ 进行模式移动，通过在 $\\mathbf{x}_P = \\mathbf{x}_E + \\mathbf{p}$ 处求值。如果这相对于 $\\mathbf{x}_E$ 严格改进了目标值，则接受 $\\mathbf{x}_P$ 并随后从该点开始执行新的探查移动；否则将新基点设为 $\\mathbf{x}_E$。\n- 步长缩减：如果从 $\\mathbf{x}_B$ 开始的探查移动没有产生改进，则通过 $\\Delta \\leftarrow \\alpha \\Delta$（其中 $0  \\alpha  1$）来缩减步长。\n- 终止条件：当 $\\Delta  \\text{tol}$ 或迭代次数达到预设的最大值时停止。\n\n您的程序必须从基本原理出发实现上述方法，并在以下测试套件上进行评估。对于每个案例，返回终止时达到的最小化目标值 $f^\\star$，其形式为浮点数。\n\n测试套件：\n- 案例 $1$ (光滑基线，方形良态系统):\n  - $A = \\begin{bmatrix} 3  0 \\\\ 0  1 \\end{bmatrix}$，$\\mathbf{b} = \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix}$，$\\tau = 0$。\n  - 初始点 $\\mathbf{x}_0 = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$。\n  - 初始步长 $\\Delta_0 = 1.0$，缩减因子 $\\alpha = 0.5$，容差 $\\text{tol} = 10^{-6}$，最大迭代次数 $10000$。\n- 案例 $2$ (非光滑复合，方形系统):\n  - $A = \\begin{bmatrix} 2  -1  0 \\\\ 0  1  2 \\\\ 1  0  1 \\end{bmatrix}$，$\\mathbf{b} = \\begin{bmatrix} 1 \\\\ 2 \\\\ 0 \\end{bmatrix}$，$\\tau = 0.1$。\n  - 初始点 $\\mathbf{x}_0 = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\end{bmatrix}$。\n  - $\\Delta_0 = 1.0$，$\\alpha = 0.5$，$\\text{tol} = 10^{-6}$，最大迭代次数 $10000$。\n- 案例 $3$ (非光滑复合，欠定系统以鼓励稀疏性):\n  - $A = \\begin{bmatrix} 1  2  3 \\\\ 0  1  1 \\end{bmatrix}$，$\\mathbf{b} = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$，$\\tau = 0.5$。\n  - 初始点 $\\mathbf{x}_0 = \\begin{bmatrix} 1 \\\\ -1 \\\\ 2 \\end{bmatrix}$。\n  - $\\Delta_0 = 0.8$，$\\alpha = 0.5$，$\\text{tol} = 10^{-6}$，最大迭代次数 $10000$。\n- 案例 $4$ (退化数据项，纯 $\\ell_1$ 项与一个常数的权衡):\n  - $A = \\begin{bmatrix} 0  0 \\\\ 0  0 \\end{bmatrix}$，$\\mathbf{b} = \\begin{bmatrix} 3 \\\\ 4 \\end{bmatrix}$，$\\tau = 1.0$。\n  - 初始点 $\\mathbf{x}_0 = \\begin{bmatrix} 5 \\\\ -5 \\end{bmatrix}$。\n  - $\\Delta_0 = 1.0$，$\\alpha = 0.5$，$\\text{tol} = 10^{-6}$，最大迭代次数 $10000$。\n\n最终输出规范：\n- 您的程序必须生成单行输出，其中包含一个用方括号括起来的逗号分隔列表。该列表必须按顺序包含案例 1 到 4 的 $f^\\star$ 值。每个值都必须是浮点数。此任务不涉及单位。不使用角度。不要打印任何额外的文本或行。", "solution": "该问题是有效的。它提出了数值优化领域一个明确定义的任务：实现 Hooke-Jeeves (HJ) 模式搜索算法，以最小化一个凸的、不可微的复合目标函数，并在指定的测试套件上评估其性能。所有必要的参数、初始条件和终止标准都已提供，并且该问题在科学上和数学上都是合理的。\n\n待最小化的目标函数由下式给出：\n$$\nf(\\mathbf{x}) \\;=\\; \\left\\|A\\mathbf{x} - \\mathbf{b}\\right\\|_2 \\;+\\; \\tau \\left\\|\\mathbf{x}\\right\\|_1\n$$\n其中 $A \\in \\mathbb{R}^{m \\times n}$ 是一个矩阵，$\\mathbf{x} \\in \\mathbb{R}^n$ 是变量向量，$\\mathbf{b} \\in \\mathbb{R}^m$ 是一个向量，$\\tau \\ge 0$ 是标量正则化参数。该函数由两项组成：\n1. 数据失配项 $\\left\\|A\\mathbf{x} - \\mathbf{b}\\right\\|_2$，即残差的欧几里得 ($\\ell_2$) 范数。该项促使解在最小二乘意义上满足线性系统 $A\\mathbf{x} \\approx \\mathbf{b}$。\n2. 正则化项 $\\tau \\left\\|\\mathbf{x}\\right\\|_1$，即解向量 $\\mathbf{x}$ 的加权 $\\ell_1$ 范数。该项促进稀疏性，意味着它鼓励 $\\mathbf{x}$ 的分量恰好为零。\n\n$\\ell_1$ 范数项 $\\left\\|\\mathbf{x}\\right\\|_1 = \\sum_{i=1}^n |x_i|$ 的存在，使得目标函数 $f(\\mathbf{x})$ 在任何一个或多个分量 $x_i$ 为零的点 $\\mathbf{x}$ 处不可微。这排除了使用基于梯度的优化方法，并促使使用像 Hooke-Jeeves 算法这样的无导数方法。\n\nHooke-Jeeves 方法是一种直接搜索算法，它通过迭代地探索搜索空间，使用两种类型的移动：探查移动和模式移动。\n\n**算法实现**\n\n该实现将遵循 Hooke-Jeeves 方法的经典结构。设 $\\mathbf{x}_B$ 为当前迭代的基点，$\\Delta$ 为当前步长。\n\n**1. 探查移动**\n探查移动沿坐标轴探查给定点附近的区域。给定一个点 $\\mathbf{x}_{c}$ 和一个步长 $\\Delta$，过程如下：\n- 初始化一个临时最佳点 $\\mathbf{x}_{new} \\leftarrow \\mathbf{x}_{c}$。\n- 对于每个坐标方向 $i = 1, \\dots, n$：\n  - 在 $\\mathbf{x}_{trial} = \\mathbf{x}_{new} + \\Delta \\mathbf{e}_i$ 处计算函数值，其中 $\\mathbf{e}_i$ 是第 $i$ 个标准基向量。如果 $f(\\mathbf{x}_{trial})  f(\\mathbf{x}_{new})$，则接受该移动：$\\mathbf{x}_{new} \\leftarrow \\mathbf{x}_{trial}$。\n  - 否则，在 $\\mathbf{x}_{trial} = \\mathbf{x}_{new} - \\Delta \\mathbf{e}_i$ 处计算函数值。如果 $f(\\mathbf{x}_{trial})  f(\\mathbf{x}_{new})$，则接受该移动：$\\mathbf{x}_{new} \\leftarrow \\mathbf{x}_{trial}$。\n- 返回最终点 $\\mathbf{x}_{new}$。\n\n**2. HJ 主算法**\n主循环协调探查移动和模式移动以在搜索空间中导航。\n\n- **初始化**：\n  - 设置初始基点 $\\mathbf{x}_B \\leftarrow \\mathbf{x}_0$。\n  - 设置前一个基点 $\\mathbf{x}_{B, \\text{prev}} \\leftarrow \\mathbf{x}_B$。\n  - 设置初始步长 $\\Delta \\leftarrow \\Delta_0$。\n  - 初始化迭代计数器 $k \\leftarrow 0$。\n\n- **迭代循环**：算法持续进行，直到步长 $\\Delta$ 小于容差 $\\text{tol}$ 或迭代次数 $k$ 超过最大值 $k_{\\text{max}}$。\n\n  - **a. 模式移动与探查**：根据前一次迭代的改进方向，定义一个临时模式点 $\\mathbf{x}_P$：\n    $$\n    \\mathbf{x}_P = \\mathbf{x}_B + (\\mathbf{x}_B - \\mathbf{x}_{B, \\text{prev}})\n    $$\n    然后从这个模式点开始执行探查移动，以找到一个新的候选点 $\\mathbf{x}_E = \\text{Explore}(\\mathbf{x}_P, \\Delta)$。\n\n  - **b. 模式移动的接受**：将新点 $\\mathbf{x}_E$ 与当前基点 $\\mathbf{x}_B$ 进行比较。\n    - 如果 $f(\\mathbf{x}_E)  f(\\mathbf{x}_B)$，则模式移动成功。为下一次迭代更新点：\n      $$\n      \\mathbf{x}_{B, \\text{prev}} \\leftarrow \\mathbf{x}_B\n      $$\n      $$\n      \\mathbf{x}_B \\leftarrow \\mathbf{x}_E\n      $$\n    - 如果 $f(\\mathbf{x}_E) \\ge f(\\mathbf{x}_B)$，则模式移动未能产生更好的点。算法放弃模式移动的结果，并尝试一个更简单的移动。\n\n  - **c. 精化（如果模式移动失败）**：从当前基点 $\\mathbf{x}_B$ 执行一次探查移动：\n    $$\n    \\mathbf{x}_E = \\text{Explore}(\\mathbf{x}_B, \\Delta)\n    $$\n    - 如果这次精化搜索成功，即 $f(\\mathbf{x}_E)  f(\\mathbf{x}_B)$，则更新点：\n      $$\n      \\mathbf{x}_{B, \\text{prev}} \\leftarrow \\mathbf{x}_B\n      $$\n      $$\n      \\mathbf{x}_B \\leftarrow \\mathbf{x}_E\n      $$\n    - 如果这第二次探查移动也失败了，则算法在当前步长下陷入停滞。\n\n  - **d. 步长缩减**：如果模式移动和随后的精化探查移动都未能改进目标函数值，则缩减步长：\n    $$\n    \\Delta \\leftarrow \\alpha \\Delta\n    $$\n    其中 $0  \\alpha  1$ 是缩减因子。重置基点以防止在下一次迭代中出现过于激进的模式移动：\n    $$\n    \\mathbf{x}_{B, \\text{prev}} \\leftarrow \\mathbf{x}_B\n    $$\n\n- **终止**：当 $\\Delta  \\text{tol}$ 或 $k \\ge k_{\\text{max}}$ 时，循环终止。最终解是最后计算出的基点 $\\mathbf{x}_B$，最小化的目标值是 $f(\\mathbf{x}_B)$。这个逻辑确保了系统性的搜索，并会收敛到一个局部最小值附近。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements the Hooke-Jeeves algorithm to solve the given test cases.\n    \"\"\"\n    \n    test_cases = [\n        {\n            \"A\": np.array([[3, 0], [0, 1]]),\n            \"b\": np.array([1, 2]),\n            \"tau\": 0.0,\n            \"x0\": np.array([0, 0]),\n            \"delta0\": 1.0,\n            \"alpha\": 0.5,\n            \"tol\": 1e-6,\n            \"max_iter\": 10000,\n        },\n        {\n            \"A\": np.array([[2, -1, 0], [0, 1, 2], [1, 0, 1]]),\n            \"b\": np.array([1, 2, 0]),\n            \"tau\": 0.1,\n            \"x0\": np.array([0, 0, 0]),\n            \"delta0\": 1.0,\n            \"alpha\": 0.5,\n            \"tol\": 1e-6,\n            \"max_iter\": 10000,\n        },\n        {\n            \"A\": np.array([[1, 2, 3], [0, 1, 1]]),\n            \"b\": np.array([1, 0]),\n            \"tau\": 0.5,\n            \"x0\": np.array([1, -1, 2]),\n            \"delta0\": 0.8,\n            \"alpha\": 0.5,\n            \"tol\": 1e-6,\n            \"max_iter\": 10000,\n        },\n        {\n            \"A\": np.array([[0, 0], [0, 0]]),\n            \"b\": np.array([3, 4]),\n            \"tau\": 1.0,\n            \"x0\": np.array([5, -5]),\n            \"delta0\": 1.0,\n            \"alpha\": 0.5,\n            \"tol\": 1e-6,\n            \"max_iter\": 10000,\n        },\n    ]\n\n    results = []\n\n    for case in test_cases:\n        A, b, tau = case[\"A\"], case[\"b\"], case[\"tau\"]\n        x0, delta, alpha, tol, max_iter = case[\"x0\"], case[\"delta0\"], case[\"alpha\"], case[\"tol\"], case[\"max_iter\"]\n        \n        def objective_function(x):\n            \"\"\"Calculates the composite objective function value.\"\"\"\n            l2_term = np.linalg.norm(A @ x - b)\n            l1_term = tau * np.linalg.norm(x, 1)\n            return l2_term + l1_term\n\n        def exploratory_move(x_c, delta_val):\n            \"\"\"Performs an exploratory move from a given point.\"\"\"\n            x_new = x_c.copy()\n            f_new = objective_function(x_new)\n            n_dims = len(x_c)\n\n            for i in range(n_dims):\n                # Probe in the positive direction\n                x_trial = x_new.copy()\n                x_trial[i] += delta_val\n                f_trial = objective_function(x_trial)\n                if f_trial  f_new:\n                    x_new = x_trial\n                    f_new = f_trial\n                    continue\n                \n                # Probe in the negative direction\n                x_trial = x_new.copy()\n                x_trial[i] -= delta_val\n                f_trial = objective_function(x_trial)\n                if f_trial  f_new:\n                    x_new = x_trial\n                    f_new = f_trial\n            return x_new\n\n        # Hooke-Jeeves main algorithm\n        x_base = x0.astype(float)\n        x_prev_base = x0.astype(float)\n        \n        for k in range(max_iter):\n            if delta  tol:\n                break\n            \n            # 1. Start with an exploratory move from the current base.\n            x_explored = exploratory_move(x_base, delta)\n            \n            if objective_function(x_explored)  objective_function(x_base):\n                # Exploration was successful. Set this as the new base and attempt a pattern move.\n                x_prev_base = x_base\n                x_base = x_explored\n                \n                # 2. Pattern move\n                x_pattern = x_base + (x_base - x_prev_base)\n                x_pattern_explored = exploratory_move(x_pattern, delta)\n                \n                if objective_function(x_pattern_explored)  objective_function(x_base):\n                    # Pattern move was successful.\n                    x_base = x_pattern_explored\n            else:\n                # Exploration from base failed, reduce step size.\n                delta *= alpha\n        \n        final_f_val = objective_function(x_base)\n        results.append(final_f_val)\n\n    print(f\"[{','.join(f'{r:.12f}' for r in results)}]\")\n\nsolve()\n```", "id": "3161536"}, {"introduction": "现实世界中的优化问题往往伴随着各种约束条件。本练习 [@problem_id:3161458] 挑战您将 HJ 算法扩展到带约束的场景，在一个香蕉形的非线性可行域内最小化 Rosenbrock 函数。您将实现并比较两种主流的约束处理策略：投影截断法和二次惩罚函数法，通过对比它们的行为，深入理解在模式搜索框架下处理复杂边界的不同思路及其权衡。", "problem": "实现一个完整的、可运行的程序，用于在一个带约束的最小化问题上比较 Hooke–Jeeves (HJ) 模式搜索方法的两种约束处理策略。目标是最小化 Rosenbrock 函数 $f(x,y) = (1 - x)^2 + 100(y - x^2)^2$，其约束条件为曲线不等式约束 $g(x,y) = y - x^2 \\le 0$。你必须实现 Hooke–Jeeves 方法的两种变体：\n\n- 变体 A (投影截断可行性)：在每一步接受的移动中强制保持可行性。对于在探索或模式步骤中从一个可行基点 $b = (x_0,y_0)$ 生成的任何试验点 $p = (x_{\\mathrm{trial}}, y_{\\mathrm{trial}})$，如果 $g(p) \\le 0$ 则按原样接受该试验点；否则，用该不可行试验点替换为从 $b$ 到 $p$ 的线段与曲线边界 $g(x,y)=0$ 的唯一交点，即点 $b + t^\\star (p-b)$，其中 $t^\\star \\in (0,1]$ 满足 $y_0 + t^\\star \\,\\mathrm{d}y = \\left(x_0 + t^\\star \\,\\mathrm{d}x\\right)^2$，其中 $\\mathrm{d}x = x_{\\mathrm{trial}} - x_0$ 且 $\\mathrm{d}y = y_{\\mathrm{trial}} - y_0$。你必须通过求解二次方程来确定 $t^\\star$，该方程是通过将线的参数化代入 $y - x^2 = 0$ 得到的，即\n$$\n(\\mathrm{d}x)^2 \\, t^2 + \\left(2 x_0 \\mathrm{d}x - \\mathrm{d}y \\right)\\, t + \\left(x_0^2 - y_0\\right) = 0\n$$\n并选择在 $[0,1]$ 区间内且位于从 $b$ 到 $p$ 线上第一个交点的根。如果 $\\mathrm{d}x=0$ 使方程变为线性，则求解该线性方程。通过处理小系数来确保数值鲁棒性。对于初始点，如果它是不可行的，则通过沿垂直方向裁剪将其投影到可行集上：将 $y$ 替换为 $\\min(y, x^2)$ 以获得一个可行的起始基点。\n\n- 变体 B (二次罚函数)：允许不可行的试验点，但评估罚函数目标\n$$\nF_\\rho(x,y) = f(x,y) + \\rho \\, \\max\\{0, g(x,y)\\}^2\n$$\n并且当且仅当一个试验点能使 $F_\\rho$ 减小时才接受它。在此变体的搜索过程中不应用投影。\n\n两种变体都必须使用在 $\\mathbb{R}^2$ 中的 Hooke–Jeeves 模式搜索框架：从一个基点和一个初始步长 $\\Delta  0$ 开始，在 $\\pm \\Delta \\, e_1$ 和 $\\pm \\Delta \\, e_2$ 方向上（其中 $e_1=(1,0)$ 和 $e_2=(0,1)$）执行逐坐标的探索性移动，接受遇到的第一个能改进相关目标函数（变体 A 使用 $f$；变体 B 使用 $F_\\rho$）的移动。在一个成功的探索阶段将基点从旧基点 $x_0$ 改进到一个新点 $x_1$ 之后，尝试一次模式移动 $x_p = x_1 + (x_1 - x_0)$。在变体 A 中，在评估之前，对从 $x_1$ 到 $x_p$ 的线段通过投影截断来强制保持可行性；在变体 B 中，直接评估 $x_p$ 处的 $F_\\rho$ 值。如果模式移动带来了进一步的改进，则接受它作为新的基点；否则，保留探索性移动的改进结果。如果在探索性移动中没有找到改进，则将步长乘以因子 $\\tfrac{1}{2}$ 来减小步长。当 $\\Delta  \\Delta_{\\min}$ 或在达到预设的最大迭代次数后终止。\n\n你必须依赖的基本依据和原则：\n- 直接搜索方法在不使用导数的情况下评估目标函数，使用探索性移动来定位下降方向，以及使用模式移动来利用已发现的下降趋势。\n- 在 $g(x,y) \\le 0$ 约束下的可行性等价于 $y \\le x^2$，其边界是曲线 $y = x^2$。\n- 沿当前试验线段到边界上的投影截断相当于求解上面给出的一维交点条件。\n- 二次罚函数方法将约束问题替换为无约束问题，该方法通过在目标函数中增加一个与约束违反量的平方成正比的项来实现。\n\n你必须实现的算法规范：\n- 在每个变体中，使用带有微小数值容差的严格递减作为接受标准。\n- 探索性移动按坐标顺序执行，接受第一个改进的移动，然后从更新后的点继续对下一个坐标进行操作。\n- 在一次成功的探索性改进 $x_1 \\ne x_0$ 后，尝试一次单一的模式移动 $x_p = x_1 + (x_1 - x_0)$。\n- 如果未能改进，则设置 $\\Delta \\leftarrow \\tfrac{1}{2}\\Delta$。\n- 终止条件：当 $\\Delta  \\Delta_{\\min}$ 或迭代计数器达到指定的最大值时停止。\n\n测试套件和输出：\n实现该程序以运行以下四个测试用例，每个用例由初始点、初始步长、罚函数参数、最小步长和最大迭代次数指定：\n- 用例 $1$：起始点 $(-1.2, 1.0)$，$\\Delta_0 = 0.5$，$\\rho = 10.0$，$\\Delta_{\\min} = 10^{-5}$，最大迭代次数 $= 10000$。\n- 用例 $2$：起始点 $(0.0, 0.0)$，$\\Delta_0 = 0.25$，$\\rho = 10.0$，$\\Delta_{\\min} = 10^{-5}$，最大迭代次数 $= 10000$。\n- 用例 $3$：起始点 $(0.5, 0.5)$，$\\Delta_0 = 0.5$，$\\rho = 100.0$，$\\Delta_{\\min} = 10^{-5}$，最大迭代次数 $= 10000$。\n- 用例 $4$：起始点 $(2.0, 3.0)$，$\\Delta_0 = 1.0$，$\\rho = 1.0$，$\\Delta_{\\min} = 10^{-5}$，最大迭代次数 $= 10000$。\n\n对每个用例，使用相同的参数运行两种变体。对于变体 A，报告在返回的（可行）点上原始目标函数 $f(x,y)$ 的最终值。对于变体 B，报告在返回的点上罚函数目标 $F_\\rho(x,y)$ 的最终值。你的程序应产生单行输出，其中包含按以下顺序排列、用逗号分隔并括在方括号内的结果：\n$[\\, f_A^{(1)},\\, F_{B,\\rho}^{(1)},\\, f_A^{(2)},\\, F_{B,\\rho}^{(2)},\\, f_A^{(3)},\\, F_{B,\\rho}^{(3)},\\, f_A^{(4)},\\, F_{B,\\rho}^{(4)} \\,].$\n\n所有角度（如果出现）必须以弧度为单位。此问题中没有带单位的物理量。所有数值输出必须是标准十进制表示的实数。代码必须是完全自包含的，不接受任何输入，并严格按照指定格式打印一行输出。实现必须使用现代编程语言，并遵守稍后描述的最终答案代码环境约束。", "solution": "用户提供了一个在数值优化领域定义明确的计算问题。经过严格验证，确认该问题陈述是自包含的，其科学基础植根于直接搜索方法和约束处理的原理，并且没有矛盾或含糊不清之处。该问题被认定为有效。\n\n任务是实现 Hooke-Jeeves 模式搜索算法，以最小化二维 Rosenbrock 函数 $f(x,y) = (1 - x)^2 + 100(y - x^2)^2$，其约束条件为不等式约束 $g(x,y) = y - x^2 \\le 0$。这将通过使用两种不同的约束处理策略来完成：投影截断（变体 A）和二次罚函数（变体 B）。\n\n解决方案的结构是一个 Python 程序，该程序实现了核心算法，然后将其应用于四个指定的测试用例。\n\n### 数学和算法公式化\n\n#### 1. 目标函数和约束函数\n无约束目标函数是 Rosenbrock 函数，这是优化算法的一个标准基准测试，以其狭窄的抛物线形山谷而闻名。\n$$\nf(x,y) = (1 - x)^2 + 100(y - x^2)^2\n$$\n可行域由不等式约束 $g(x,y) = y - x^2 \\le 0$ 定义，它描述了抛物线 $y = x^2$ 上及其下方的区域。无约束 Rosenbrock 函数的全局最小值位于 $(1,1)$，其值为 $f(1,1)=0$。该点是不可行的，因为 $g(1,1) = 1 - 1^2 = 0$。它位于边界上，因此它也是有约束条件下的最小值。\n\n#### 2. Hooke-Jeeves 模式搜索框架\nHooke-Jeeves 方法是一种迭代的、无导数的优化算法。每次迭代包括两个主要阶段：探索性移动和模式移动。\n\n- **初始化**：算法从一个基点 $x_{\\mathrm{base}}$、一个初始步长 $\\Delta$、一个步长缩减因子（此处为 $\\frac{1}{2}$）、一个最小步长 $\\Delta_{\\min}$ 和一个最大迭代次数开始。\n\n- **探索性移动**：从当前基点开始，算法探测局部邻域。它按顺序将每个坐标扰动 $\\pm \\Delta$。对于 $\\mathbb{R}^2$ 中的一个点，搜索方向为 $\\pm \\Delta e_1$ 和 $\\pm \\Delta e_2$，其中 $e_1=(1,0)$ 和 $e_2=(0,1)$。按照规定，第一个能使目标函数严格减小的移动被接受，并且对下一个坐标的搜索将从这个新更新的点开始。如果沿任何坐标方向的移动都不能带来改进，则探索阶段失败。\n\n- **模式移动**：如果探索阶段成功，会找到一个新点 $x_{\\mathrm{new}}$，它优于前一个基点 $x_{\\mathrm{old}}$。这表明一个有希望的下降方向，$d = x_{\\mathrm{new}} - x_{\\mathrm{old}}$。算法尝试通过沿此方向进行“模式移动”来加速搜索：\n$$\nx_p = x_{\\mathrm{new}} + d = x_{\\mathrm{new}} + (x_{\\mathrm{new}} - x_{\\mathrm{old}})\n$$\n如果 $x_p$ 处的目标值比 $x_{\\mathrm{new}}$ 处的值有所改进，则 $x_p$ 成为下一次迭代的新基点。否则，算法退回到 $x_{\\mathrm{new}}$，该点成为新的基点。\n\n- **步长缩减**：如果探索阶段未能找到任何改进，步长 $\\Delta$ 将被减小（例如，减半），算法从相同的基点继续下一次迭代。这使得算法在接近最小值时可以进行更精细的搜索。\n\n- **终止条件**：当步长 $\\Delta$ 小于最小阈值 $\\Delta_{\\min}$ 或达到最大迭代次数时，算法终止。\n\n#### 3. 约束处理策略\n\n**变体 A：投影截断可行性**\n此变体在每一步都强制保持可行性。被最小化的目标函数是原始的 $f(x,y)$。\n\n- **初始点**：如果用户提供的起始点是不可行的，通过将其 $y$-坐标设置为 $\\min(y, x^2)$，将其投影到可行集上。\n- **移动接受**：每当从当前可行点 $b$ 生成一个试验点 $p$（来自探索性或模式移动），都会检查其可行性。\n    - 如果 $g(p) \\le 0$，该点是可行的，并按原样进行评估。\n    - 如果 $g(p)  0$，该点是不可行的。它必须被替换为沿连接 $b$ 和 $p$ 的线段到边界 $y=x^2$ 上的投影点。这个投影点是 $b + t^\\star (p-b)$，其中 $t^\\star \\in [0,1]$ 是通过求解参数化直线和抛物线的交点来找到的。将线的参数化 $x(t) = x_0 + t\\,\\mathrm{d}x$, $y(t) = y_0 + t\\,\\mathrm{d}y$ 代入边界方程 $y=x^2$，得到指定的关于 $t$ 的二次方程：\n    $$\n    (\\mathrm{d}x)^2 t^2 + (2 x_0 \\mathrm{d}x - \\mathrm{d}y) t + (x_0^2 - y_0) = 0\n    $$\n    其中 $(\\mathrm{d}x, \\mathrm{d}y) = p - b$。由于 $b$ 是可行的 ($x_0^2-y_0 \\ge 0$) 而 $p$ 是不可行的，保证存在一个唯一的根 $t^\\star \\in [0,1)$。使用标准的二次公式找到这个根，并对 $\\mathrm{d}x \\approx 0$ 的线性情况进行特殊处理。\n\n**变体 B：二次罚函数方法**\n此变体通过向目标函数添加一个惩罚项，将约束问题转化为无约束问题。这允许搜索探索不可行区域。\n\n- **罚函数目标**：算法最小化一个罚函数目标函数 $F_\\rho(x,y)$：\n    $$\n    F_\\rho(x,y) = f(x,y) + \\rho \\cdot \\max\\{0, g(x,y)\\}^2\n    $$\n    这里，$\\rho  0$ 是罚函数参数。对于可行点（$g(x,y) \\le 0$），惩罚项为零；对于不可行点，惩罚项随约束违反的幅度呈二次方增长。\n- **移动接受**：试验点的接受或拒绝纯粹基于它们是否减小了 $F_\\rho$ 的值。不执行任何投影。最终返回的点可能是轻微不可行的，其不可行程度由 $\\rho$ 的大小控制。\n\n该实现将包括用于目标函数和约束的辅助函数、一个用于解决变体 A 中投影步骤的鲁棒函数，以及一个封装了两种变体核心逻辑的主 `hooke_jeeves_solver` 函数。一个 `solve` 函数将协调执行四个指定的测试用例并格式化最终输出。", "answer": "```python\nimport numpy as np\nimport math\n\ndef solve():\n    \"\"\"\n    Main function to run the Hooke-Jeeves comparison for all test cases\n    and print the formatted results.\n    \"\"\"\n\n    # Define a small tolerance for floating-point comparisons in acceptance criteria.\n    TOL = 1e-9\n\n    def f_rosenbrock(p):\n        \"\"\"Rosenbrock function f(x,y) = (1-x)^2 + 100(y-x^2)^2.\"\"\"\n        x, y = p\n        return (1.0 - x)**2 + 100.0 * (y - x**2)**2\n\n    def g_constraint(p):\n        \"\"\"Constraint function g(x,y) = y - x^2. Feasible if = 0.\"\"\"\n        x, y = p\n        return y - x**2\n\n    def F_penalized(p, rho):\n        \"\"\"Penalized objective function F_rho(x,y).\"\"\"\n        return f_rosenbrock(p) + rho * max(0.0, g_constraint(p))**2\n\n    def solve_projection_t(base, trial):\n        \"\"\"\n        Solves for the parameter t in [0, 1] for the intersection of the\n        line segment from a feasible 'base' to an infeasible 'trial' point\n        with the boundary g(x,y) = 0.\n        \"\"\"\n        x0, y0 = base\n        xt, yt = trial\n        \n        dx = xt - x0\n        dy = yt - y0\n        \n        # We solve At^2 + Bt + C = 0 for t.\n        # A = (dx)^2, B = 2*x0*dx - dy, C = x0^2 - y0\n        a = dx**2\n        b = 2.0 * x0 * dx - dy\n        c = x0**2 - y0\n        \n        # Since base is feasible (y0 = x0^2) and trial is infeasible (yt > xt^2),\n        # quadratic at t=0 is C >= 0 and at t=1 is (xt^2 - yt)  0.\n        # This guarantees a unique root in [0, 1).\n        \n        if abs(a)  TOL:  # Linear equation: Bt + C = 0\n            if abs(b)  TOL:\n                # This case is unreachable if base is feasible and trial is infeasible.\n                return 0.0\n            t = -c / b\n        else:  # Quadratic equation\n            discriminant = b**2 - 4.0 * a * c\n            if discriminant  0.0: # Clamp due to potential floating point errors\n                discriminant = 0.0\n            \n            sqrt_d = math.sqrt(discriminant)\n            \n            # Standard quadratic formula roots\n            t1 = (-b + sqrt_d) / (2.0 * a)\n            t2 = (-b - sqrt_d) / (2.0 * a)\n            \n            # Select the unique root within the [0, 1] interval.\n            t1_valid = (0.0 - TOL) = t1 = (1.0 + TOL)\n            t2_valid = (0.0 - TOL) = t2 = (1.0 + TOL)\n\n            if t1_valid and t2_valid:\n                t = min(t1, t2) # First intersection\n            elif t1_valid:\n                t = t1\n            else:\n                t = t2\n                \n        # Clamp to [0, 1] for robustness against floating-point inaccuracies.\n        return max(0.0, min(1.0, t))\n\n    def project(base, trial):\n        \"\"\"Projects an infeasible trial point back onto the feasible set boundary.\"\"\"\n        t_star = solve_projection_t(base, trial)\n        return base + t_star * (trial - base)\n\n    def hooke_jeeves_solver(start_point, delta0, delta_min, max_iter, variant, rho=None):\n        \"\"\"\n        Implementation of the Hooke-Jeeves pattern search method with two\n        constraint-handling variants.\n        \"\"\"\n        x_base = np.array(start_point, dtype=float)\n        delta = float(delta0)\n        \n        if variant == 'A':\n            objective_func = f_rosenbrock\n            # Ensure initial point is feasible for Variant A\n            if g_constraint(x_base) > 0:\n                x_base[1] = min(x_base[1], x_base[0]**2)\n        elif variant == 'B':\n            def objective_func(p): return F_penalized(p, rho)\n        else:\n            raise ValueError(\"Unknown variant specified.\")\n\n        for _ in range(max_iter):\n            if delta  delta_min:\n                break\n                \n            x_iter_start_base = x_base.copy()\n            f_iter_start_base = objective_func(x_iter_start_base)\n\n            # --- 1. Exploratory Moves ---\n            x_explore = x_iter_start_base.copy()\n            \n            for i in range(len(x_explore)):\n                f_before_coord_move = objective_func(x_explore)\n                \n                # Try positive step\n                p_plus_trial = x_explore.copy()\n                p_plus_trial[i] += delta\n                \n                p_plus_final = p_plus_trial\n                if variant == 'A' and g_constraint(p_plus_trial) > 0:\n                    p_plus_final = project(x_explore, p_plus_trial)\n                \n                f_plus = objective_func(p_plus_final)\n                \n                if f_plus  f_before_coord_move - TOL:\n                    x_explore = p_plus_final\n                    continue  # Accepted, on to next dimension\n\n                # Try negative step\n                p_minus_trial = x_explore.copy()\n                p_minus_trial[i] -= delta\n                \n                p_minus_final = p_minus_trial\n                if variant == 'A' and g_constraint(p_minus_trial) > 0:\n                    p_minus_final = project(x_explore, p_minus_trial)\n                \n                f_minus = objective_func(p_minus_final)\n\n                if f_minus  f_before_coord_move - TOL:\n                    x_explore = p_minus_final\n            \n            f_explore = objective_func(x_explore)\n            \n            if f_explore  f_iter_start_base - TOL:  # Improvement found\n                # --- 2. Pattern Move ---\n                pattern_dir = x_explore - x_iter_start_base\n                p_pattern_trial = x_explore + pattern_dir\n                \n                p_pattern_final = p_pattern_trial\n                if variant == 'A' and g_constraint(p_pattern_trial) > 0:\n                    p_pattern_final = project(x_explore, p_pattern_trial)\n\n                f_pattern = objective_func(p_pattern_final)\n                \n                if f_pattern  f_explore - TOL:\n                    x_base = p_pattern_final\n                else:\n                    x_base = x_explore\n            else:  # No improvement from exploratory moves\n                delta /= 2.0\n        \n        # Return the final objective value as required by the problem.\n        return objective_func(x_base)\n\n    test_cases = [\n        {'start': (-1.2, 1.0), 'delta0': 0.5, 'rho': 10.0, 'delta_min': 1e-5, 'max_iter': 10000},\n        {'start': (0.0, 0.0), 'delta0': 0.25, 'rho': 10.0, 'delta_min': 1e-5, 'max_iter': 10000},\n        {'start': (0.5, 0.5), 'delta0': 0.5, 'rho': 100.0, 'delta_min': 1e-5, 'max_iter': 10000},\n        {'start': (2.0, 3.0), 'delta0': 1.0, 'rho': 1.0, 'delta_min': 1e-5, 'max_iter': 10000}\n    ]\n\n    results = []\n    for case in test_cases:\n        # Run Variant A (Projection-Truncation)\n        f_A = hooke_jeeves_solver(\n            start_point=case['start'],\n            delta0=case['delta0'],\n            delta_min=case['delta_min'],\n            max_iter=case['max_iter'],\n            variant='A'\n        )\n        results.append(f_A)\n\n        # Run Variant B (Quadratic Penalty)\n        F_B = hooke_jeeves_solver(\n            start_point=case['start'],\n            delta0=case['delta0'],\n            delta_min=case['delta_min'],\n            max_iter=case['max_iter'],\n            variant='B',\n            rho=case['rho']\n        )\n        results.append(F_B)\n\n    print(f\"[{','.join(f'{r:.8f}' for r in results)}]\")\n\nsolve()\n```", "id": "3161458"}]}