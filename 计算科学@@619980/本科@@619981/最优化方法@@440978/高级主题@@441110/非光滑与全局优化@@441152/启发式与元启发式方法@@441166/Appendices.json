{"hands_on_practices": [{"introduction": "任何局部搜索算法的核心都在于其邻域函数 $N(x)$ 的设计，它定义了从当前解可以到达的“相邻”解的集合。在设计邻域时，我们常常面临一个关键的权衡：是选择一个易于实现的通用邻域（如简单的交换或插入操作），还是一个融入了问题特有知识的“更智能”的领域特定邻域？本练习通过一个大学课程表问题的假想场景，让您定量地分析和比较这两种邻域在寻优效率上的差异，从而深刻理解在启发式方法设计中领域知识的价值。", "problem": "一个大学课程表问题将 $n$ 个事件分配到 $t$ 个时间段和 $r$ 个房间。一个解 $x$ 是一个满足硬约束的分配，例如教师不重叠、房间容量以及每个房间-时间对只有一个事件。软约束，例如将事件分散在几天内或最小化晚间时段，会产生惩罚。设目标是软约束惩罚的加权和 $f(x)$，其中硬约束通过在搜索过程中拒绝不可行的邻居来强制执行。局部搜索方法使用一个邻域函数 $N(x)$ 来生成候选移动；如果 $y$ 是可行的且 $f(y)  f(x)$，则从 $x$ 到 $y$ 的移动被接受。\n\n考虑两种邻域设计：\n- 通用邻域 $N_g(x)$，由忽略领域结构的交换和插入移动组成：任何两个事件可以交换它们的时间段，任何单个事件可以移动到任何时间段-房间对，但需进行生成后的可行性检查。\n- 领域特定邻域 $N_d(x)$，它利用领域知识，根据冲突图和容量约束来预过滤移动：仅当交换能保持教师不重叠和容量约束时才提出交换，仅当插入到的时间段-房间对满足容量且不产生冲突时才提出插入。\n\n假设在相同的随机邻居采样方案和评估预算下，某个特定实例具有以下经验统计数据：\n- 在 $N_g(x)$ 下，一个随机生成的邻居是可行的概率为 $q_g = 0.2$，并且在可行的条件下，是改进的概率为 $a_g = 0.25$。每个邻居的生成和评估成本为 $c_g = 1$ 个时间单位。\n- 在 $N_d(x)$ 下，一个随机生成的邻居是可行的概率为 $q_d = 0.8$，并且在可行的条件下，是改进的概率为 $a_d = 0.5$。每个邻居的生成和评估成本为 $c_d = 1.5$ 个时间单位。\n\n假设局部搜索采用简单的爬山法：从 $N(x)$ 中均匀随机抽取一个邻居 $y$，如果 $y$ 是可行的且 $f(y)  f(x)$ 则接受 $y$；否则停留在 $x$ 并抽取另一个邻居。此外，考虑使用模拟退火（SA）的变体，其中一个目标函数值增加 $\\Delta f = f(y) - f(x)  0$ 的非改进可行移动以概率 $p_{\\text{SA}} = \\exp(-\\Delta f / T)$ 在温度 $T$ 下被接受。\n\n评估以下陈述并选择所有正确的选项：\n\nA. 在拒绝不可行和非改进移动的爬山法下，对于给定的 $q_g, a_g, c_g, q_d, a_d, c_d$，每个接受的改进步骤的期望时间对于 $N_d(x)$ 来说比对于 $N_g(x)$ 更小。\n\nB. 在任何温度 $T$ 下的模拟退火中，将邻域从 $N_g(x)$ 限制到 $N_d(x)$ 必然会降低在可行域中逃离局部最优解的概率，因此无论 $T$ 如何，SA 在使用 $N_d(x)$ 时收敛效果会更差。\n\nC. 如果 $N_d(x)$ 在可行解集上导出一个连通图（即，对于任意两个可行分配 $x$ 和 $x'$，存在一条由 $N_d$ 中的移动构成的从 $x$ 到 $x'$ 的路径），那么限制到 $N_d(x)$ 会保留对全局最优可行解的可达性，同时可能提高每次迭代的效率。\n\nD. 在禁忌搜索中，通用邻域 $N_g(x)$ 总是优于领域特定邻域 $N_d(x)$，因为禁忌机制和惩罚会补偿高比例的不可行移动，从而使领域知识变得多余。\n\n选择所有正确的选项。", "solution": "用户希望我扮演一位严谨的STEM教授，解决一个关于启发式优化方法的问题。\n\n### 问题验证\n\n首先，我将验证问题陈述。\n\n**步骤1：提取已知条件**\n\n- **问题背景**：大学课程表问题。\n- **变量**：$n$ 个事件，$t$ 个时间段，$r$ 个房间。\n- **解的表示**：一个解 $x$ 是一个事件的分配。\n- **约束**：\n    - 硬约束：教师不重叠、房间容量、每个房间-时间对只有一个事件。必须满足。\n    - 软约束：分散事件、最小化晚间时段。产生惩罚值。\n- **目标函数**：$f(x)$ 是软约束惩罚的加权和；目标是最小化。\n- **搜索范式**：使用邻域函数 $N(x)$ 的局部搜索。不可行的邻居被拒绝。\n- **邻域定义**：\n    - **通用邻域 $N_g(x)$**：\n        - 移动：交换任意两个事件的时间段，或将任意事件插入到任意时间段-房间对。\n        - 可行性检查：生成后检查。\n        - 统计数据：\n            - 可行性概率：$q_g = 0.2$。\n            - 条件改进概率：$a_g = 0.25$。\n            - 每个邻居的成本：$c_g = 1$ 个时间单位。\n    - **领域特定邻域 $N_d(x)$**：\n        - 移动：基于冲突图和容量约束进行预过滤。\n        - 统计数据：\n            - 可行性概率：$q_d = 0.8$。\n            - 条件改进概率：$a_d = 0.5$。\n            - 每个邻居的成本：$c_d = 1.5$ 个时间单位。\n- **算法**：\n    - **爬山法**：如果一个随机抽取的邻居 $y$ 是可行的且 $f(y)  f(x)$，则接受它。\n    - **模拟退火 (SA)**：一个目标函数值增加 $\\Delta f = f(y) - f(x)  0$ 的非改进可行移动以概率 $p_{\\text{SA}} = \\exp(-\\Delta f / T)$ 被接受。\n\n**步骤2：使用提取的已知条件进行验证**\n\n- **科学基础**：该问题在组合优化和元启发式这一成熟领域中有充分的依据。所有概念，包括课程表问题、局部搜索、邻域设计、爬山法和模拟退火，都是标准的且定义正确。\n- **定义明确**：该问题提供了足够的量化数据来评估各项陈述。每个选项中提出的问题都可以通过基于给定模型的逻辑推导和计算来回答。\n- **客观性**：语言精确客观。\n- **一致性**：问题陈述内部一致。在对 $N_d(x)$ 的描述中存在一个潜在的模糊之处，它“根据冲突图和容量约束来预过滤移动”，这可能意味着100%的可行性。然而，给出的可行性是 $q_d = 0.8$。这并非矛盾；它意味着预过滤处理了一些但不是所有的硬约束（例如，它可能处理了教师冲突和容量问题，但没有处理“每个房间-时间对只有一个事件”的约束，后者在生成后进行检查），这在复杂问题领域是一个现实的场景。\n- **真实性**：提供的数值统计数据是合理的。更复杂的领域特定邻域生成器具有更高的计算成本（$c_d  c_g$），但产生更高比例的高质量移动（$q_d  q_g, a_d  a_g$），这是很常见的。\n\n**步骤3：结论和行动**\n\n问题陈述是有效的、科学上合理的、定义明确且内部一致的。我将继续进行求解推导。\n\n### 求解推导\n\n现在我将评估四个陈述中的每一个。\n\n**A. 在拒绝不可行和非改进移动的爬山法下，对于给定的 $q_g, a_g, c_g, q_d, a_d, c_d$，每个接受的改进步骤的期望时间对于 $N_d(x)$ 来说比对于 $N_g(x)$ 更小。**\n\n为了评估这个陈述，我们必须为每个邻域计算找到并接受一个改进移动的期望时间。\n找到一个被接受的改进的过程是一系列伯努利试验，其中“成功”是生成一个既可行又改进的邻居。达到第一次成功所需的试验次数遵循几何分布。\n\n设 $p$ 为单次试验成功的概率。达到第一次成功的期望试验次数为 $1/p$。\n设 $c$ 为每次试验的成本（时间）。\n每个接受的改进步骤的期望时间是 $E[\\text{Time}] = c \\times (1/p)$。\n\n单次试验成功的概率（生成一个可行且改进的邻居）是邻居可行概率 ($q$) 与在可行条件下其为改进的条件概率 ($a$) 的乘积。因此，$p = q \\times a$。\n\n对于通用邻域 $N_g(x)$：\n- 可行且改进的邻居的概率：$p_g = q_g \\times a_g = 0.2 \\times 0.25 = 0.05$。\n- 每个邻居生成/评估的成本：$c_g = 1$ 个时间单位。\n- 每次改进的期望时间：$E_g = \\frac{c_g}{p_g} = \\frac{1}{0.05} = 20$ 个时间单位。\n\n对于领域特定邻域 $N_d(x)$：\n- 可行且改进的邻居的概率：$p_d = q_d \\times a_d = 0.8 \\times 0.5 = 0.4$。\n- 每个邻居生成/评估的成本：$c_d = 1.5$ 个时间单位。\n- 每次改进的期望时间：$E_d = \\frac{c_d}{p_d} = \\frac{1.5}{0.4} = \\frac{15}{4} = 3.75$ 个时间单位。\n\n比较期望时间：$E_d = 3.75$ 和 $E_g = 20$。\n由于 $3.75  20$，每个接受的改进步骤的期望时间对于 $N_d(x)$ 确实比 $N_g(x)$ 小。\n\n对A的结论：**正确**。\n\n**B. 在任何温度 $T$ 下的模拟退火中，将邻域从 $N_g(x)$ 限制到 $N_d(x)$ 必然会降低在可行域中逃离局部最优解的概率，因此无论 $T$ 如何，SA 在使用 $N_d(x)$ 时收敛效果会更差。**\n\n该陈述对逃离局部最优解做出了强硬的断言。在SA中逃离局部最优解需要进行一次非改进移动，其概率为 $p_{\\text{SA}} = \\exp(-\\Delta f / T)$。能否逃离取决于邻域中是否存在此类非改进的可行邻居。\n\n让我们计算在单次试验中生成一个可行的、非改进的邻居的概率。这由 $p_{\\text{escape\\_candidate}} = q \\times (1-a)$ 给出。\n\n对于通用邻域 $N_g(x)$：\n- 生成一个可行的、非改进的邻居的概率：\n$p_{g, \\text{esc}} = q_g \\times (1 - a_g) = 0.2 \\times (1 - 0.25) = 0.2 \\times 0.75 = 0.15$。\n\n对于领域特定邻域 $N_d(x)$：\n- 生成一个可行的、非改进的邻居的概率：\n$p_{d, \\text{esc}} = q_d \\times (1 - a_d) = 0.8 \\times (1 - 0.5) = 0.8 \\times 0.5 = 0.4$。\n\n生成一个潜在逃逸移动的概率对于 $N_d(x)$ ($0.4$) 高于 $N_g(x)$ ($0.15$)。因此，限制到 $N_d(x)$ 会“必然降低逃离局部最优解的概率”这一前提是错误的。使用 $N_d(x)$ 时，SA算法更有可能找到一个非改进的候选移动来考虑逃逸。\n\n此外，SA“无论 $T$ 如何，收敛效果会更差”的结论是一个没有根据的泛化。SA的整体性能取决于探索（非改进移动）和利用（改进移动）之间的平衡，以及 $T$ 的冷却策略。像 $N_d(x)$ 这样在寻找改进移动（如A所示）和非改进移动方面都更高效的邻域，可能会导致更好的收敛，并可能配合更快的冷却策略。该陈述从根本上就是错误的。\n\n对B的结论：**不正确**。\n\n**C. 如果 $N_d(x)$ 在可行解集上导出一个连通图（即，对于任意两个可行分配 $x$ 和 $x'$，存在一条由 $N_d$ 中的移动构成的从 $x$ 到 $x'$ 的路径），那么限制到 $N_d(x)$ 会保留对全局最优可行解的可达性，同时可能提高每次迭代的效率。**\n\n该陈述包含两个主张。\n\n1.  **“...保留对全局最优可行解的可达性”**：前提是邻域 $N_d(x)$ 在所有可行解的集合上导出一个连通图。根据定义，连通图意味着任何节点（可行解）都可以从任何其他节点通过一条边（邻域中的移动）的路径到达。全局最优解是这个可行解集合的一个成员。因此，如果图是连通的，那么从任何起始可行解都可以到达任何全局最优解。这部分陈述是搜索空间连通性定义的直接且正确的结果。\n\n2.  **“...同时可能提高每次迭代的效率”**：这个主张表明使用 $N_d(x)$ 可能更有效率。我们对陈述A的分析证实了对于给定的数据确实如此。我们计算出找到一个改进移动的期望时间（衡量爬山法每次迭代效率的指标）对于 $N_d(x)$ 是 $E_d = 3.75$，而对于 $N_g(x)$ 是 $E_g = 20$。效率提升是显著的。“可能”一词的使用是恰当的，因为这种效率增益取决于具体的问题实例和邻域统计数据，而非一个普适定律。\n\n该陈述正确地将一个关键的理论属性（连通性，保证可达性）与一个实际的好处（提高效率）联系起来，这一点已由问题的数据所证明。这代表了启发式设计中一个理想的目标：创建一个更“智能”的邻域，它既高效又不牺牲探索整个可行空间的能力。\n\n对C的结论：**正确**。\n\n**D. 在禁忌搜索中，通用邻域 $N_g(x)$ 总是优于领域特定邻域 $N_d(x)$，因为禁忌机制和惩罚会补偿高比例的不可行移动，从而使领域知识变得多余。**\n\n这个陈述做出了一个绝对的断言（“总是优于”）和一个错误的推测（“使领域知识变得多余”）。\n\n任何局部搜索方法，包括禁忌搜索（TS），的效率都严重依赖于它所评估邻居的成本和质量。邻域 $N_g(x)$ 有 $80\\%$ 的时间（$1-q_g$）生成不可行的移动。在问题描述的设置中，这些移动被直接拒绝，意味着使用 $N_g(x)$ 进行每次试验的计算努力中有 $80\\%$ 被浪费了。作为TS核心的禁忌列表，是作用于*被接受的移动*上的。它无法“补偿”大量因不可行而甚至没有成为接受候选的生成移动。\n\n断言领域知识变得“多余”与设计有效算法解决难题的最基本原则之一相悖。优化的“没有免费午餐”定理意味着，算法的性能正是在它整合了关于问题结构的知识时才得到提升。领域特定邻域 $N_d(x)$ 就是体现这种知识以更有效地引导搜索的一个例子。放弃这种知识而选择一个暴力的、通用的邻域几乎总是一个糟糕的策略。一个更高效的邻域允许TS在相同的时间内执行更多有意义的搜索步骤，从而找到更好的解。\n\n“总是”这个词使该陈述确定无疑是错误的。很容易构造出场景（如此问题中所给的），在TS框架内，一个精心设计的领域特定邻域的性能远超通用邻域。\n\n对D的结论：**不正确**。", "answer": "$$\\boxed{AC}$$", "id": "3136481"}, {"introduction": "设计好了搜索步骤，一个非常实际的问题随之而来：搜索应该持续多久？由于启发式算法通常是随机的，且其性能表现出收益递减的特性（即搜索后期每次迭代带来的改进越来越小），因此设定一个高效的停止准则至关重要。本练习将指导您通过蒙特卡洛模拟，实现并评估一种基于近期改进量移动平均值 $\\overline{\\Delta}_t$ 的自适应停止规则，并将其与一个理论上的“神谕”停止时刻 $t_{\\star}$ 进行比较，从而量化提前停止（错失更优解）与延迟停止（浪费计算资源）的风险。", "problem": "要求您仅使用概率论和统计学的基本定义以及标准算法结构，为随机启发式优化过程设计、论证并评估一个自适应停止规则。目标是量化过早停止与过晚停止之间的权衡。\n\n假设一个迭代启发式优化器产生一个非负标量改进序列 $\\{\\Delta_t\\}_{t=1}^{\\infty}$，其中 $\\Delta_t$ 是在第 $t$ 次迭代中实现的目标值的改进量。通过指数衰减曲线对期望改进进行建模\n$$\n\\mathbb{E}[\\Delta_t] = A e^{-\\beta t},\n$$\n其中 $A \\gt 0$ 且 $\\beta \\gt 0$。观测到的改进为\n$$\n\\Delta_t = \\max\\{0,\\, A e^{-\\beta t} + \\eta_t\\},\n$$\n其中 $\\{\\eta_t\\}$ 是独立同分布的高斯噪声项，满足 $\\eta_t \\sim \\mathcal{N}(0,\\sigma^2)$，并且负值在 0 处被截断以确保非负性。\n\n定义以下两种停止时间。\n\n- 神谕停止时间。对于一个参考容差 $\\epsilon_{\\text{true}} \\gt 0$，神谕停止索引是满足 $\\mathbb{E}[\\Delta_t] \\le \\epsilon_{\\text{true}}$ 的最小整数 $t_{\\star}$，即：\n$$\nt_{\\star} = \\min\\{t \\in \\mathbb{N} : A e^{-\\beta t} \\le \\epsilon_{\\text{true}}\\}.\n$$\n\n- 自适应移动平均规则。对于一个窗口长度 $w \\in \\mathbb{N}$ 和一个估计容差 $\\epsilon_{\\text{est}} \\gt 0$，定义移动平均\n$$\n\\overline{\\Delta}_t = \\frac{1}{w} \\sum_{i=t-w+1}^{t} \\Delta_i \\quad \\text{for} \\quad t \\ge w.\n$$\n自适应停止索引为\n$$\n\\tau = \\min\\left\\{t \\in \\mathbb{N}: t \\ge w \\ \\text{and} \\ \\overline{\\Delta}_t \\le \\epsilon_{\\text{est}}\\right\\},\n$$\n约定如果在达到最大允许迭代次数 $T_{\\max}$ 时该集合仍为空，则将 $\\tau$ 设置为 $T_{\\max}$。\n\n对于下方的每个参数集，执行一个包含 $M$ 次独立运行的蒙特卡洛实验。在每次运行中，模拟改进序列直至 $T_{\\max}$，并记录自适应停止索引 $\\tau$ 和神谕索引 $t_{\\star}$。量化：\n\n- 过早停止风险，即 $\\tau \\lt t_{\\star}$ 的运行次数所占的比例。\n- 延迟停止风险，即 $\\tau \\gt t_{\\star}$ 的运行次数所占的比例。\n\n两种风险都必须以 $[0,1]$ 区间内的小数形式报告，并四舍五入到三位小数。\n\n使用以下测试套件。每个元组列出了 $(A,\\ \\beta,\\ \\sigma,\\ w,\\ \\epsilon_{\\text{est}},\\ \\epsilon_{\\text{true}},\\ T_{\\max},\\ M,\\ \\text{seed})$：\n\n- 测试 $1$：$(0.5, 0.05, 0.02, 10, 0.01, 0.01, 400, 4000, 123)$。\n- 测试 $2$：$(0.5, 0.05, 0.02, 5, 0.02, 0.01, 400, 4000, 456)$。\n- 测试 $3$：$(0.5, 0.05, 0.02, 20, 0.005, 0.01, 400, 4000, 789)$。\n- 测试 $4$：$(0.5, 0.05, 0.08, 3, 0.01, 0.01, 400, 4000, 42)$。\n- 测试 $5$：$(0.02, 0.1, 0.005, 5, 0.01, 0.01, 200, 4000, 321)$。\n\n要求与说明：\n\n- 本问题不涉及角度；没有需要指定的角度单位。\n- 不涉及物理单位；所有量均为无量纲标量。\n- 对于每个测试，根据神谕的模型定义计算 $t_{\\star}$，并确保所选参数满足 $t_{\\star} \\le T_{\\max}$。\n- 您的程序必须输出单行，其中包含一个浮点数列表：对于每个测试，按给定顺序输出过早风险，然后是延迟风险，两者都四舍五入到三位小数。将所有测试的结果汇总到一个列表中。例如，输出格式必须与 $[r_{1,\\text{prem}},r_{1,\\text{delay}},r_{2,\\text{prem}},\\dots,r_{5,\\text{delay}}]$ 完全一样，不含空格。\n\n您的程序应生成单行输出，其中包含用方括号括起来的逗号分隔的结果列表（例如，$[r_1,r_2,r_3]$）。", "solution": "该问题要求为随机优化过程设计并评估一个自适应停止规则。这将通过蒙特卡洛模拟，将一个实用的、数据驱动的规则的性能与一个理论上的“神谕”规则进行比较来完成。\n\n### 基本概念\n\n问题的核心在于探索与利用之间的权衡，这是优化中的一个经典主题。迭代优化器旨在随时间推移改进解。每次迭代的改进量，用 $\\Delta_t$ 表示，被建模为一个随机过程。我们假设期望改进遵循指数衰减，这是收益递减过程的常见模型：\n$$\n\\mathbb{E}[\\Delta_t] = A e^{-\\beta t}\n$$\n此处，$A  0$ 代表初始改进的规模，$\\beta  0$ 是衰减率。观测到的改进 $\\Delta_t$ 是此期望的一个带噪声的版本：\n$$\n\\Delta_t = \\max\\{0,\\, A e^{-\\beta t} + \\eta_t\\}\n$$\n其中 $\\eta_t$ 是一个随机噪声项，建模为独立同分布（i.i.d.）的高斯变量，$\\eta_t \\sim \\mathcal{N}(0, \\sigma^2)$。$\\max\\{0, \\cdot\\}$ 操作确保观测到的改进为非负，这具有物理意义。\n\n### 神谕停止规则\n\n一个理想的停止规则会在*期望*的未来改进降至一个有意义的阈值 $\\epsilon_{\\text{true}}$ 以下时，精确地终止优化过程。这定义了神谕停止时间 $t_{\\star}$，它在真实世界场景中是无法获取的，因为它需要知道底层模型参数 $A$ 和 $\\beta$。\n神谕停止索引 $t_{\\star}$ 是期望改进不大于真实容差 $\\epsilon_{\\text{true}}$ 的最小整数时间 $t$：\n$$\nt_{\\star} = \\min\\{t \\in \\mathbb{N} : A e^{-\\beta t} \\le \\epsilon_{\\text{true}}\\}\n$$\n这个不等式可以解出 $t$：\n$$\nA e^{-\\beta t} \\le \\epsilon_{\\text{true}} \\implies e^{-\\beta t} \\le \\frac{\\epsilon_{\\text{true}}}{A} \\implies -\\beta t \\le \\ln\\left(\\frac{\\epsilon_{\\text{true}}}{A}\\right)\n$$\n$$\nt \\ge -\\frac{1}{\\beta} \\ln\\left(\\frac{\\epsilon_{\\text{true}}}{A}\\right) \\implies t \\ge \\frac{1}{\\beta} \\ln\\left(\\frac{A}{\\epsilon_{\\text{true}}}\\right)\n$$\n由于 $t$ 必须是整数，$t_{\\star}$ 是满足此条件的最小整数，可以通过对右侧表达式向上取整得到。由于 $t$ 必须是正整数（$t \\in \\mathbb{N}$），我们有：\n$$\nt_{\\star} = \\max\\left(1, \\left\\lceil \\frac{1}{\\beta} \\ln\\left(\\frac{A}{\\epsilon_{\\text{true}}}\\right) \\right\\rceil\\right)\n$$\n对于每组参数，$t_{\\star}$ 是一个确定性的常数值，作为我们的基准。\n\n### 自适应停止规则\n\n一个实用的停止规则无法访问 $\\mathbb{E}[\\Delta_t]$，必须依赖于可观测的含噪序列 $\\{\\Delta_t\\}$。所提出的自适应规则使用简单的移动平均来过滤噪声并估计近期的改进趋势。在一个大小为 $w$ 的窗口上的移动平均定义为：\n$$\n\\overline{\\Delta}_t = \\frac{1}{w} \\sum_{i=t-w+1}^{t} \\Delta_i \\quad \\text{for} \\quad t \\ge w\n$$\n自适应停止索引 $\\tau$ 是该移动平均值首次（在 $t=w$ 时第一个完整窗口可用时或之后）降至指定的估计容差 $\\epsilon_{\\text{est}}$ 以下的时间 $t$：\n$$\n\\tau = \\min\\left\\{t \\in \\mathbb{N}: t \\ge w \\ \\text{and} \\ \\overline{\\Delta}_t \\le \\epsilon_{\\text{est}}\\right\\}\n$$\n如果在最大迭代次数 $T_{\\max}$ 内此条件未被满足，我们设置 $\\tau = T_{\\max}$。\n\n该规则的行为由窗口大小 $w$、噪声水平 $\\sigma$ 和所选容差 $\\epsilon_{\\text{est}}$ 之间的相互作用所决定。\n- 较小的 $w$ 使规则响应非常灵敏，但容易受到噪声的影响，可能因随机的下降而导致过早停止，或因随机的突增而导致延迟停止。\n- 较大的 $w$ 能更好地平滑噪声，但会引入滞后。平均值 $\\overline{\\Delta}_t$ 将包含较早、较大的 $\\Delta_i$ 值，可能在真实期望改进早已降至阈值以下后，仍使平均值保持在 $\\epsilon_{\\text{est}}$ 以上，从而导致延迟停止。\n\n### 蒙特卡洛模拟与风险评估\n\n由于 $\\Delta_t$ 是一个随机变量，自适应停止时间 $\\tau$ 也是一个随机变量。为了评估自适应规则的性能，我们必须分析 $\\tau$ 相对于固定的神谕时间 $t_{\\star}$ 的分布。蒙特卡洛模拟是进行此分析的合适工具。\n\n流程如下：\n1.  对于给定的参数集 $(A, \\beta, \\sigma, w, \\epsilon_{\\text{est}}, \\epsilon_{\\text{true}}, T_{\\max}, M, \\text{seed})$，我们首先计算确定性的神谕时间 $t_{\\star}$。\n2.  然后我们进行 $M$ 次独立的模拟运行。\n3.  在每次运行中：\n    a. 生成一个随机噪声序列 $\\{\\eta_t\\}_{t=1}^{T_{\\max}}$。\n    b. 计算观测到的改进序列 $\\{\\Delta_t\\}_{t=1}^{T_{\\max}}$。\n    c. 计算移动平均序列 $\\{\\overline{\\Delta}_t\\}_{t=w}^{T_{\\max}}$。\n    d. 确定该次特定运行的自适应停止时间 $\\tau$。\n4.  在 $M$ 次运行之后，我们得到一个包含 $M$ 个 $\\tau$ 值的样本。然后我们可以通过统计结果来估计风险：\n    - **过早停止风险**：过程停止过早的运行次数所占的比例，即 $P(\\tau  t_{\\star}) \\approx \\frac{\\text{count}(\\tau  t_{\\star})}{M}$。这代表了未能实现优化全部潜力的风险。\n    - **延迟停止风险**：过程停止过晚的运行次数所占的比例，即 $P(\\tau > t_{\\star}) \\approx \\frac{\\text{count}(\\tau > t_{\\star})}{M}$。这代表了浪费计算资源的风险。\n\nPython 实现将为提供的每个测试用例执行此模拟。使用 NumPy 可以高效地进行向量化操作，以生成序列和计算移动平均。随机数生成由一个种子控制，以保证可复现性。", "answer": "[0.480,0.330,0.000,1.000,1.000,0.000,0.493,0.030,0.000,1.000]", "id": "3136544"}, {"introduction": "现在，让我们从简单的局部搜索范式，迈向更高级的基于模型的搜索方法。交叉熵（Cross-Entropy, CE）方法就是其中的杰出代表，它并非仅仅在解空间中移动，而是通过学习一个关于优良解的概率模型来指导搜索。本练习将引导您为经典的0-1背包问题完整实现交叉熵算法。您将体验到该算法如何通过迭代式的“采样-评估-学习”循环，不断更新其概率模型参数 $\\boldsymbol{\\theta}$，从而使其更有可能生成接近最优解的候选方案。", "problem": "您将实现交叉熵（Cross-Entropy）方法，使用二进制采样分布来解决一个单实例的 $0$-$1$ 背包问题，并分析其对精英比例（elite fraction）的敏感性。交叉熵方法是一种基于模型的随机搜索方法，它通过最小化与一个集中于高性能解的目标分布之间的Kullback–Leibler散度（也称为交叉熵），来迭代更新参数化的采样分布。此处的采样分布族是独立的伯努利（Bernoulli）分布的乘积，每个物品对应一个分布。更新是通过在模型下最大化一个精英解集（elite set）的似然性，从第一性原理推导出来的。您必须实现完整的方法，并为指定的精英比例测试套件返回量化结果。\n\n背包实例：设 $n$ 表示物品数量，$\\mathbf{w} \\in \\mathbb{N}^{n}$ 为重量，$\\mathbf{v} \\in \\mathbb{N}^{n}$ 为价值， $C \\in \\mathbb{N}$ 为容量。考虑 $n=15$，容量 $C=50$，其中\n$\\mathbf{w} = [12,7,11,8,9,13,4,6,10,3,14,5,2,1,15]$ 且\n$\\mathbf{v} = [36,14,22,21,19,32,9,17,25,8,28,12,5,3,34]$。\n一个候选解是一个向量 $\\mathbf{x} \\in \\{0,1\\}^{n}$，其目标函数为 $f(\\mathbf{x}) = \\sum_{i=1}^{n} v_{i} x_{i}$，可行性约束为 $\\sum_{i=1}^{n} w_{i} x_{i} \\le C$。\n\n采样分布：使用一个独立的伯努利参数向量 $\\boldsymbol{\\theta} \\in (0,1)^{n}$，其中第 $i$ 位通过 $x_{i} \\sim \\text{Bernoulli}(\\theta_{i})$ 采样。对于所有 $i$，初始化 $\\theta_{i} = 0.5$。\n\n可行性处理：为确保可行性，对每个违反容量约束的采样解 $\\mathbf{x}$ 应用一个确定性的修复算子。如果 $\\sum_{i} w_{i} x_{i}  C$，则按照比率 $r_{j} = v_{j} / w_{j}$ 的递增顺序，迭代地将物品 $j$ 的 $x_{j} \\leftarrow 0$（即，首先移除 $v_{j}/w_{j}$ 最小的物品），直到 $\\sum_{i} w_{i} x_{i} \\le C$。该算子必须在有限步骤内终止并产生一个可行解。\n\n交叉熵迭代：在每次迭代 $t = 1,2,\\dots,T$ 中，从当前 $\\boldsymbol{\\theta}$ 中独立采样 $N$ 个解，根据需要对其进行修复，评估 $f(\\cdot)$，并根据目标值选择前 $m$ 个解作为精英集 $\\mathcal{E}$，其中对于给定的精英比例 $p \\in (0,1]$，有 $m = \\max(1, \\lceil pN \\rceil)$。通过在伯努利乘积模型下最大化精英集的似然性来更新 $\\boldsymbol{\\theta}$，并使用参数为 $\\alpha \\in (0,1]$ 的指数平滑步骤。更新后，将每个 $\\theta_{i}$ 投影到区间 $[\\varepsilon, 1 - \\varepsilon]$（其中 $\\varepsilon \\in (0, 0.5)$）以避免退化。维持并返回在所有样本和所有迭代中找到的最佳可行目标值。\n\n您在设计和推导中必须使用并明确依据的基础原理：\n- $0$-$1$ 背包问题的定义：最大化 $f(\\mathbf{x}) = \\sum_{i=1}^{n} v_{i} x_{i}$，约束条件为 $\\sum_{i=1}^{n} w_{i} x_{i} \\le C$，其中 $\\mathbf{x} \\in \\{0,1\\}^{n}$。\n- 独立的伯努利乘积族：对于 $\\mathbf{x} \\in \\{0,1\\}^{n}$，$p_{\\boldsymbol{\\theta}}(\\mathbf{x}) = \\prod_{i=1}^{n} \\theta_{i}^{x_{i}} (1-\\theta_{i})^{1 - x_{i}}$。\n- 应用于有限精英数据集的独立伯努利变量的最大似然估计。\n- Kullback–Leibler散度的定义，以及最小化到精英集上支持的目标分布的交叉熵与最大化精英集上的似然性之间的等价性。\n\n在您的实现中要使用的超参数：\n- 每次迭代的样本数 $N = 300$。\n- 迭代次数 $T = 60$。\n- 平滑参数 $\\alpha = 0.7$。\n- 概率下限 $\\varepsilon = 0.05$。\n- 所有测试用例的随机数生成器种子 $s = 314159$。\n\n测试套件：您必须评估算法为以下每个精英比例 $p$ 返回的最佳目标值：\n- 情况 1：$p = 0.05$。\n- 情况 2：$p = 0.20$。\n- 情况 3：$p = 0.50$。\n- 情况 4：$p = 0.90$。\n\n覆盖性设计：测试套件包括一个小的精英比例 $p = 0.05$（激进更新）、一个中等比例 $p = 0.20$、一个均衡比例 $p = 0.50$ 和一个非常大的比例 $p = 0.90$（保守更新），以探究交叉熵方法中的偏差-方差权衡。\n\n答案规范：\n- 对于每种情况，返回一个等于算法在 $T$ 次迭代后找到的最佳可行目标值的整数。\n- 将四种情况的结果汇总到单行输出中，格式严格为方括号括起来的逗号分隔列表，例如 $[r_{1},r_{2},r_{3},r_{4}]$，其中每个 $r_{k}$ 是一个整数。\n- 不涉及物理单位。不涉及角度。百分比必须按提供的小数形式输入。\n- 您的程序必须严格按照指定格式产生一行输出，并且不能要求任何用户输入或外部文件。", "solution": "该问题要求实现交叉熵（CE）方法，为 $0$-$1$ 背包问题的一个特定实例找到高质量的解。并且，将在一组给定的测试套件上分析算法性能对精英比例参数 $p$ 的敏感性。\n\n该方法始于对问题陈述的正式验证，我们发现该问题陈述在科学上是合理的、适定的且完备的。验证之后，将推导并解释一个详尽的、基于原理的解决方案。\n\n### 问题验证\n\n**步骤 1：提取已知条件**\n\n*   **问题**：$0$-$1$ 背包问题。\n*   **实例**：\n    *   物品数量：$n=15$。\n    *   容量：$C=50$。\n    *   重量：$\\mathbf{w} = [12,7,11,8,9,13,4,6,10,3,14,5,2,1,15]$。\n    *   价值：$\\mathbf{v} = [36,14,22,21,19,32,9,17,25,8,28,12,5,3,34]$。\n*   **解向量**：$\\mathbf{x} \\in \\{0,1\\}^{n}$。\n*   **目标**：最大化 $f(\\mathbf{x}) = \\sum_{i=1}^{n} v_{i} x_{i}$。\n*   **约束**：$\\sum_{i=1}^{n} w_{i} x_{i} \\le C$。\n*   **采样分布**：参数为 $\\boldsymbol{\\theta} \\in (0,1)^{n}$ 的独立伯努利分布的乘积。\n*   **初始化**：对于所有 $i$，$\\theta_{i} = 0.5$。\n*   **可行性处理**：一个确定性的修复算子，它以价值-重量比 $r_j = v_j/w_j$ 的递增顺序，从超重的背包中迭代地移除物品。\n*   **CE迭代**：基于采样、修复、评估、精英选择和参数更新。\n*   **基本原理**：$0$-$1$ 背包问题的定义、伯努利乘积模型、伯努利变量的最大似然估计（MLE）、KL散度最小化与MLE的等价性。\n*   **超参数**：\n    *   每次迭代的样本数：$N = 300$。\n    *   迭代次数：$T = 60$。\n    *   平滑参数：$\\alpha = 0.7$。\n    *   概率下限：$\\varepsilon = 0.05$。\n    *   随机种子：$s = 314159$。\n*   **测试套件**：精英比例 $p \\in \\{0.05, 0.20, 0.50, 0.90\\}$。\n*   **输出**：为每个 $p$ 找到的最佳可行目标值，格式化为列表。\n\n**步骤 2：使用提取的已知条件进行验证**\n\n根据验证标准对问题进行评估：\n*   **有科学依据**：该问题在优化（交叉熵方法）和计算机科学（背包问题）等成熟领域具有坚实的理论基础。所引用的数学原理是正确的。\n*   **适定性**：所有必要的数据、参数和算法步骤都有明确定义。使用固定的随机种子确保每个测试用例都有唯一且可复现的结果。\n*   **客观性**：问题陈述使用了精确、量化且客观的语言。\n*   该问题没有科学上不合理、不完整、矛盾或模糊等缺陷。这是一个标准的、非平凡的计算问题，并且完全可以验证。\n\n**步骤 3：结论与行动**\n\n问题是**有效的**。将提供一个完整的解决方案。\n\n### 解决方案推导与算法设计\n\n交叉熵方法是一种随机优化算法，通过迭代更新解空间上的概率分布参数来工作。其目标是演化该分布，使其在具有高目标值的区域生成样本。\n\n**1. 背包问题与概率模型**\n\n问题是选择 $n=15$ 个物品的一个子集，以在不超过重量容量 $C=50$ 的前提下最大化总价值。一个解是一个二进制向量 $\\mathbf{x} \\in \\{0,1\\}^{n}$，其中如果物品 $i$ 被包含在内，则 $x_i=1$，否则 $x_i=0$。\n目标函数是 $f(\\mathbf{x}) = \\mathbf{v}^T \\mathbf{x} = \\sum_{i=1}^{n} v_{i} x_{i}$，约束条件为 $\\mathbf{w}^T \\mathbf{x} = \\sum_{i=1}^{n} w_{i} x_{i} \\le C$。\n\n我们在解空间 $\\{0,1\\}^n$ 上定义一个参数化的概率分布。根据规定，这是一个独立伯努利分布的乘积：\n$$p_{\\boldsymbol{\\theta}}(\\mathbf{x}) = \\prod_{i=1}^{n} p(x_i | \\theta_i) = \\prod_{i=1}^{n} \\theta_{i}^{x_{i}} (1-\\theta_{i})^{1 - x_{i}}$$\n这里，$\\boldsymbol{\\theta} = (\\theta_1, \\dots, \\theta_n)$ 是参数向量，其中 $\\theta_i$ 是物品 $i$ 被包含在解中（$x_i=1$）的概率。初始时，所有物品被选中的机会均等：对于所有 $i=1, \\dots, n$，$\\theta_i^{(0)} = 0.5$。\n\n**2. 通过最大似然估计进行参数更新**\n\nCE方法的核心是 $\\boldsymbol{\\theta}$ 的更新规则。在每次迭代 $t$ 中，我们从当前分布 $p_{\\boldsymbol{\\theta}^{(t-1)}}(\\mathbf{x})$ 生成一个包含 $N$ 个候选解的群体。在修复和评估这些解之后，我们选择一个包含 $m$ 个表现最好的解的“精英集” $\\mathcal{E}$。下一次迭代的分布 $\\boldsymbol{\\theta}^{(t)}$ 被选择为更“接近”一个理想分布，该理想分布只会生成这个精英集中的解。\n\n最小化 $p_{\\boldsymbol{\\theta}}(\\mathbf{x})$ 与此理想目标分布之间的Kullback-Leibler散度（或交叉熵），等同于在我们的模型 $p_{\\boldsymbol{\\theta}}(\\mathbf{x})$ 下最大化精英样本的似然性。\n\n设精英集为 $\\mathcal{E} = \\{\\mathbf{x}^{(1)}, \\mathbf{x}^{(2)}, \\dots, \\mathbf{x}^{(m)}\\}$。似然函数是：\n$$L(\\boldsymbol{\\theta}; \\mathcal{E}) = \\prod_{j=1}^{m} p_{\\boldsymbol{\\theta}}(\\mathbf{x}^{(j)}) = \\prod_{j=1}^{m} \\prod_{i=1}^{n} \\theta_{i}^{x_{i}^{(j)}} (1-\\theta_{i})^{1 - x_{i}^{(j)}}$$\n为了更容易地进行最大化，我们使用对数似然函数：\n$$\\log L(\\boldsymbol{\\theta}; \\mathcal{E}) = \\sum_{j=1}^{m} \\sum_{i=1}^{n} \\left[ x_{i}^{(j)} \\log(\\theta_i) + (1-x_{i}^{(j)}) \\log(1-\\theta_i) \\right]$$\n为了找到最大化该函数的参数 $\\boldsymbol{\\theta}$，我们对每个 $\\theta_k$ 求偏导数并令其为零：\n$$\\frac{\\partial \\log L}{\\partial \\theta_k} = \\sum_{j=1}^{m} \\left[ \\frac{x_{k}^{(j)}}{\\theta_k} - \\frac{1-x_{k}^{(j)}}{1-\\theta_k} \\right] = 0$$\n$$\\frac{1}{\\theta_k} \\sum_{j=1}^{m} x_{k}^{(j)} = \\frac{1}{1-\\theta_k} \\sum_{j=1}^{m} (1-x_{k}^{(j)})$$\n解出 $\\theta_k$ 得到最大似然估计，我们记为 $\\hat{\\theta}_k$：\n$$\\hat{\\theta}_k = \\frac{\\sum_{j=1}^{m} x_{k}^{(j)}}{m}$$\n这个简洁的结果表明，第 $k$ 个物品的最优参数 $\\hat{\\theta}_k$ 就是该物品在精英集中所有解中出现的频率。\n\n**3. 平滑更新与参数投影**\n\n为防止过早收敛并提供稳定性，参数更新使用指数移动平均进行平滑，平滑参数为 $\\alpha$：\n$$\\boldsymbol{\\theta}^{(t)} = \\alpha \\hat{\\boldsymbol{\\theta}} + (1-\\alpha) \\boldsymbol{\\theta}^{(t-1)}$$\n一个较大的 $\\alpha$（如指定的 $\\alpha=0.7$）会给予最新的精英集更大的权重，导致更快的收敛，而较小的 $\\alpha$ 则会导致更保守的更新。\n\n此外，为确保搜索不会因概率收敛到恰好为 $0$ 或 $1$ 而停滞，更新后的参数被投影到区间 $[\\varepsilon, 1-\\varepsilon]$。对于每个分量 $i$：\n$$\\theta_i^{(t)} \\leftarrow \\max(\\varepsilon, \\min(1-\\varepsilon, \\theta_i^{(t)}))$$\n当 $\\varepsilon=0.05$ 时，每个物品始终有至少 $5\\%$ 的机会被采样（或不被采样），从而保持一定程度的探索。\n\n**4. 可行性修复算子**\n\n由于从伯努利分布中采样不遵守背包的容量约束，因此需要一个修复步骤。对于任何生成的解 $\\mathbf{x}$，若 $\\mathbf{w}^T\\mathbf{x}  C$，我们必须移除物品直至满足约束。指定的策略是首先移除价值-重量比 $r_j = v_j/w_j$ 最低的物品。这是一种贪心选择，旨在在减少重量的同时尽可能多地保留价值。该算子在评估前应用于每个生成的样本。\n\n**5. 完整算法**\n\n对于给定的精英比例 $p$，完整算法如下：\n\n1.  **初始化**：\n    *   设置迭代计数器 $t \\leftarrow 0$。\n    *   初始化伯努利参数 $\\boldsymbol{\\theta}^{(0)} \\leftarrow (0.5, 0.5, \\dots, 0.5)$。\n    *   初始化已找到的最佳目标值 $f_{best} \\leftarrow -\\infty$。\n    *   预先计算按 $v_j/w_j$ 排序的物品索引。\n    *   设置精英样本数 $m = \\max(1, \\lceil pN \\rceil)$。\n\n2.  **迭代循环**：对于 $t = 1, \\dots, T$:\n    *   **采样**：通过抽取 $x_i^{(j)} \\sim \\text{Bernoulli}(\\theta_i^{(t-1)})$ 生成 $N$ 个样本 $\\{\\mathbf{x}^{(1)}, \\dots, \\mathbf{x}^{(N)}\\}$。\n    *   **修复**：对于每个样本 $\\mathbf{x}^{(j)}$：如果 $\\mathbf{w}^T\\mathbf{x}^{(j)}  C$，应用修复算子直到 $\\mathbf{w}^T\\mathbf{x}^{(j)} \\le C$。设修复后的样本为 $\\{\\mathbf{x'}^{(1)}, \\dots, \\mathbf{x'}^{(N)}\\}$。\n    *   **评估**：计算所有 $j=1, \\dots, N$ 的目标值 $f(\\mathbf{x'}^{(j)})$。\n    *   **更新最优值**：$f_{best} \\leftarrow \\max(f_{best}, \\max_{j} f(\\mathbf{x'}^{(j)}))$。\n    *   **精英选择**：确定具有最高目标值的 $m$ 个修复后样本的集合 $\\mathcal{E}$。\n    *   **参数更新**：\n        a. 计算最大似然估计（MLE）：对于所有 $i=1, \\dots, n$，$\\hat{\\theta}_i = \\frac{1}{m} \\sum_{\\mathbf{x'} \\in \\mathcal{E}} x'_i$。\n        b. 平滑：$\\boldsymbol{\\theta}^{(t)} = \\alpha \\hat{\\boldsymbol{\\theta}} + (1-\\alpha) \\boldsymbol{\\theta}^{(t-1)}$。\n        c. 投影：$\\theta_i^{(t)} \\leftarrow \\max(\\varepsilon, \\min(1-\\varepsilon, \\theta_i^{(t)}))$。\n\n3.  **终止**：在 $T$ 次迭代后，返回 $f_{best}$。\n\n对测试套件 $\\{0.05, 0.20, 0.50, 0.90\\}$ 中的每个 $p$ 值重复此过程，每次运行时重新初始化随机数生成器，以确保公平、独立的试验。", "answer": "[143,144,144,142]", "id": "3136467"}]}