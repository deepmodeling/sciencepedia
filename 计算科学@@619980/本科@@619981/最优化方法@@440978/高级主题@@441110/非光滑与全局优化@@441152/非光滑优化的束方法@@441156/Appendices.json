{"hands_on_practices": [{"introduction": "这个练习将引导您从零开始实现近端束方法。您将编写区分“有效步”（serious step）和“无效步”（null step）的核心逻辑，前者能够改进解，后者则用于优化函数模型。通过观察不同起始点 $x_0$ 如何影响算法的初始行为，您将对模型精度与优化进程之间的相互作用建立直观理解 [@problem_id:3105171]。", "problem": "考虑一个凸非光滑优化问题，其目标函数 $f:\\mathbb{R}^n\\to\\mathbb{R}$ 由 $f(x)=\\lVert Ax-b\\rVert_\\infty$ 给出，其中 $A\\in\\mathbb{R}^{m\\times n}$ 且 $b\\in\\mathbb{R}^m$。无穷范数定义为 $\\lVert y\\rVert_\\infty=\\max_{1\\leq i\\leq m}|y_i|$，函数 $f(x)$ 可写作 $f(x)=\\max_{1\\leq i\\leq m}|a_i^\\top x-b_i|$，其中 $a_i^\\top$ 表示 $A$ 的第 $i$ 行。该函数是凸函数且是分段线性的，因此可能是非光滑的。\n\n您的任务是实现一种邻近束方法，以研究不同初始化点 $x_0$ 对首次成功步出现前的无效步次数的影响。邻近束算法应根据凸分析和非光滑优化的基本原理构建如下：\n\n1. 从凸性、次梯度和切平面模型的定义开始。对于给定的点 $x$，可以通过选择一个索引 $i^\\star\\in\\operatorname{argmax}_{1\\leq i\\leq m}|a_i^\\top x-b_i|$ 来构造 $f$ 的一个有效次梯度 $g(x)$，并定义\n$$\ng(x)=\\operatorname{sign}(a_{i^\\star}^\\top x-b_{i^\\star})\\,a_{i^\\star},\n$$\n其中，如果 $t>0$，则 $\\operatorname{sign}(t)=1$；如果 $t<0$，则 $\\operatorname{sign}(t)=-1$；如果 $t=0$，则 $\\operatorname{sign}(0)=0$。\n\n2. 维护一个切平面束 $\\{(y_j,f_j,g_j)\\}_{j=1}^J$，其中 $y_j\\in\\mathbb{R}^n$ 是一个先前评估过的点，$f_j=f(y_j)$ 是函数值，$g_j\\in\\mathbb{R}^n$ 是在 $y_j$ 处的一个次梯度。定义切平面模型\n$$\nm_J(x)=\\max_{1\\leq j\\leq J}\\left\\{f_j+g_j^\\top(x-y_j)\\right\\}。\n$$\n\n3. 在迭代 $k$ 次时，使用当前邻近中心 $x_k$，求解邻近子问题\n$$\n\\min_{x\\in\\mathbb{R}^n,\\;z\\in\\mathbb{R}} \\quad z+\\frac{t}{2}\\lVert x-x_k\\rVert_2^2\n\\quad\\text{subject to}\\quad\nz\\geq f_j+g_j^\\top(x-y_j)\\quad\\text{for all }j\\in\\{1,\\dots,J\\},\n$$\n其中 $t>0$ 是一个固定的邻近参数，以获得候选解 $(\\hat{x},\\hat{z})$。请注意，在解处，有 $\\hat{z}=m_J(\\hat{x})$。\n\n4. 评估实际下降量和预测下降量。令 $\\Delta_k^{\\text{m}}=f(x_k)-\\hat{z}$ 表示预测下降量，令 $\\Delta_k^{\\text{a}}=f(x_k)-f(\\hat{x})$ 表示实际下降量。使用带有参数 $\\alpha\\in(0,1)$ 的标准束接受性测试：\n$$\n\\text{accept as a serious step if}\\quad \\Delta_k^{\\text{a}}\\geq \\alpha\\,\\Delta_k^{\\text{m}}.\n$$\n如果测试通过，记录在这次首次成功步之前所执行的无效步次数，然后停止。否则，声明为无效步，将新的切平面 $(y_{J+1},f_{J+1},g_{J+1})=(\\hat{x},f(\\hat{x}),g(\\hat{x}))$ 添加到束中，保持 $x_k$ 不变，并重复该过程。\n\n5. 为确保在边缘情况下能够终止，设置最大无效步数 $K_{\\max}$；如果在 $K_{\\max}$ 次无效步内没有出现成功步，则返回 $K_{\\max}$。\n\n为以下固定的问题数据实现此过程：\n- 维度 $n=2$ 和 $m=2$。\n- 矩阵 $A=\\begin{bmatrix}2 & 0\\\\0 & 3\\end{bmatrix}$。\n- 向量 $b=\\begin{bmatrix}2\\\\-6\\end{bmatrix}$。\n- 邻近参数 $t=1$。\n- 接受参数 $\\alpha=0.3$。\n- 最大无效步数 $K_{\\max}=50$。\n- 欧几里得范数 $\\lVert\\cdot\\rVert_2$ 用于邻近项。\n\n使用以下初始点 $x_0\\in\\mathbb{R}^2$ 测试套件：\n- 测试 1：$x_0=\\begin{bmatrix}1\\\\-2\\end{bmatrix}$。\n- 测试 2：$x_0=\\begin{bmatrix}0\\\\0\\end{bmatrix}$。\n- 测试 3：$x_0=\\begin{bmatrix}10\\\\10\\end{bmatrix}$。\n- 测试 4：$x_0=\\begin{bmatrix}-100\\\\50\\end{bmatrix}$。\n- 测试 5：$x_0=\\begin{bmatrix}10^{-8}\\\\-2\\cdot 10^{-8}\\end{bmatrix}$。\n\n每个测试用例需要计算从指定的 $x_0$ 开始，算法在首次成功步出现前所执行的无效步次数。如果成功步立即出现（在第一个候选点处），则无效步的次数为 $0$。如果在 $K_{\\max}$ 次无效步内没有出现成功步，则报告 $K_{\\max}$。\n\n最终输出格式：您的程序应生成单行输出，其中包含五个测试用例的结果，格式为方括号内以逗号分隔的列表，例如 $[r_1,r_2,r_3,r_4,r_5]$，其中每个 $r_i$ 是为测试 $i$ 观察到的无效步次数的整数。", "solution": "用户希望我实现一种邻近束方法来解决一个非光滑凸优化问题，并确定对于几个不同的初始点，在采取第一个成功步之前所执行的无效步次数。\n\n### 1. 问题说明\n目标函数是 $f(x) = \\lVert Ax - b \\rVert_\\infty$，其中 $x \\in \\mathbb{R}^n$，$A \\in \\mathbb{R}^{m \\times n}$ 且 $b \\in \\mathbb{R}^m$。\n该函数可写作 $f(x) = \\max_{1 \\le i \\le m} |a_i^\\top x - b_i|$，其中 $a_i^\\top$ 是 $A$ 的第 $i$ 行。\n问题提供以下数据：\n- $n=2$, $m=2$\n- $A = \\begin{bmatrix} 2  0 \\\\ 0  3 \\end{bmatrix}$，因此 $a_1 = [2, 0]^\\top$ 且 $a_2 = [0, 3]^\\top$。\n- $b = \\begin{bmatrix} 2 \\\\ -6 \\end{bmatrix}$，因此 $b_1 = 2$ 且 $b_2 = -6$。\n因此，目标函数为 $f(x) = \\max \\left( |2x_1 - 2|, |3x_2 + 6| \\right)$。\n该函数的最小值点是 $x^* = [1, -2]^\\top$，在该点处 $f(x^*) = 0$。\n\n### 2. 邻近束算法\n算法流程如下：\n1.  **初始化**：从一个初始点 $x_0$ 开始。该点作为第一个邻近中心 $x_k$（对于 $k=0$）。信息束 $B$ 用该点本身、其函数值和一个次梯度进行初始化：$B = \\{(y_1, f_1, g_1)\\}$，其中 $y_1=x_k$，$f_1=f(x_k)$ 且 $g_1 \\in \\partial f(x_k)$。问题指定了次梯度的计算方法：\n    $g(x) = \\operatorname{sign}(a_{i^\\star}^\\top x - b_{i^\\star}) a_{i^\\star}$ 对于 $i^\\star \\in \\operatorname{argmax}_{1 \\le i \\le m} |a_i^\\top x - b_i|$。符号函数定义为：当 $t>0$ 时 $\\operatorname{sign}(t)=1$，当 $t<0$ 时为 $-1$，当 $t=0$ 时为 $0$。\n\n2.  **子问题构建**：在每次迭代中，通过求解函数切平面模型的正则化版本来找到一个候选点 $\\hat{x}$。原始子问题是：\n    $$ \\min_{x \\in \\mathbb{R}^n, z \\in \\mathbb{R}} \\quad z + \\frac{t}{2}\\lVert x - x_k \\rVert_2^2 \\quad \\text{s.t.} \\quad z \\ge f_j + g_j^\\top(x - y_j), \\quad \\forall (y_j, f_j, g_j) \\in B $$\n    该问题通常通过其对偶形式求解，这是一个凸二次规划（QP）问题。设束中有 $J$ 个元素。对偶 QP 是：\n    $$ \\min_{\\lambda \\in \\mathbb{R}^J} \\quad \\frac{1}{2t} \\left\\lVert \\sum_{j=1}^J \\lambda_j g_j \\right\\rVert_2^2 + \\sum_{j=1}^J \\lambda_j \\epsilon_j \\quad \\text{s.t.} \\quad \\sum_{j=1}^J \\lambda_j = 1, \\quad \\lambda_j \\ge 0 $$\n    其中 $\\epsilon_j = f(x_k) - (f_j + g_j^\\top(x_k - y_j))$ 是第 $j$ 个切平面在当前中心 $x_k$ 处的线性化误差。\n\n3.  **候选点与步评估**：一旦找到最优对偶变量 $\\lambda^*$，候选点 $\\hat{x}$ 计算如下：\n    $$ \\hat{x} = x_k - \\frac{1}{t} \\sum_{j=1}^J \\lambda_j^* g_j $$\n    这一步的质量通过比较函数值的实际下降量 $\\Delta^a = f(x_k) - f(\\hat{x})$ 与模型预测的下降量 $\\Delta^m$ 来评估。预测下降量由对偶 QP 目标函数的最优值给出（乘以 $2t$）：\n    $$ \\Delta^m = \\sum_{j=1}^J \\lambda_j^* \\epsilon_j + \\frac{1}{t} \\left\\lVert \\sum_{j=1}^J \\lambda_j^* g_j \\right\\rVert_2^2 $$\n\n4.  **接受性测试**：从 $x_k$ 到 $\\hat{x}$ 的一步是**成功步**（serious step），如果实际下降量是预测下降量的足够大的一个比例：\n    $$ \\Delta^a \\ge \\alpha \\Delta^m $$\n    其中 $\\alpha \\in (0, 1)$ 是一个给定参数。\n    - 如果是成功步，则算法成功找到了一个改进。对于本问题，我们停止并记录到目前为止所执行的无效步次数。\n    - 如果条件不满足，则该步是**无效步**（null step）。邻近中心 $x_k$ 不更新。取而代之的是，利用候选点的信息通过向束中添加新的切平面来改进模型：$B \\leftarrow B \\cup \\{(\\hat{x}, f(\\hat{x}), g(\\hat{x}))\\}$。无效步计数器加一，然后从 QP 子问题重新开始该过程。\n\n5.  **终止**：每个测试用例的程序在第一次成功步时终止，或者在执行了最大 $K_{\\max}$ 次无效步后终止。\n\n### 3. 实现计划\n对于每个提供的初始点 $x_0$，执行所述算法。\n- 每次迭代的核心是求解一个 QP。这将使用 `scipy.optimize.minimize` 和 'SLSQP' 方法来处理，该方法适用于约束二次规划。\n- 束将作为一个元组列表来管理。在每个无效步中，该列表会增长，待解 QP 的规模会增加一。\n- 过程开始时无效步计数为 $0$。如果第一个候选点导致了成功步，则结果为 $0$。否则，每次无效步都会使计数器递增。\n- 提供的参数是 $t=1$，$\\alpha=0.3$ 和 $K_{\\max}=50$。\n- 实现将循环遍历五个测试用例，为每个用例执行束方法，并存储由此产生的无效步计数。\n\n### 4. 代码结构\n实现被封装在一个 `solve` 函数中。\n- 辅助函数 `f_obj(x, A, b)` 和 `g_subgrad(x, A, b)` 将分别计算目标函数值和一个次梯度。\n- 单个测试用例的主要逻辑被分离到一个函数 `solve_bundle_case` 中。该函数从起始点 $x_0$ 初始化束，并进入一个代表搜索成功步的循环。\n- 在循环内部，它为当前的束构造并求解对偶 QP，计算候选点 $\\hat{x}$，并执行接受性测试。\n- 如果测试通过，循环终止，并返回当前的无效步计数。如果测试失败，则向束中添加一个新的切平面，循环继续。\n- 如果循环完成（即，在 $K_{\\max}$ 次迭代中没有成功步），则返回 $K_{\\max}$。\n- 最后，`solve` 函数收集所有测试用例的结果，并以指定的格式打印它们。", "answer": "```python\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef solve():\n    \"\"\"\n    Main solver function to run the proximal bundle method for all test cases.\n    \"\"\"\n\n    # --- Problem Definition ---\n    A = np.array([[2.0, 0.0], [0.0, 3.0]])\n    b = np.array([2.0, -6.0])\n    \n    # --- Algorithm Parameters ---\n    t = 1.0\n    alpha = 0.3\n    K_max = 50\n\n    # --- Test Cases ---\n    test_cases = [\n        np.array([1.0, -2.0]),\n        np.array([0.0, 0.0]),\n        np.array([10.0, 10.0]),\n        np.array([-100.0, 50.0]),\n        np.array([1e-8, -2e-8]),\n    ]\n    \n    # --- Helper Functions ---\n    def f_obj(x, A_mat, b_vec):\n        \"\"\"Computes the objective function f(x) = ||Ax - b||_inf.\"\"\"\n        return np.linalg.norm(A_mat @ x - b_vec, ord=np.inf)\n\n    def g_subgrad(x, A_mat, b_vec):\n        \"\"\"Computes a subgradient g(x) of f at x.\"\"\"\n        y = A_mat @ x - b_vec\n        istar = np.argmax(np.abs(y))\n        sign_val = np.sign(y[istar])\n        # The problem specifies sign(0) = 0. np.sign abides by this.\n        return sign_val * A_mat[istar, :].T\n\n    def run_bundle_for_case(x_start):\n        \"\"\"\n        Executes the proximal bundle method for a single starting point x_start.\n        Returns the number of null steps before the first serious step.\n        \"\"\"\n        x_k = np.array(x_start, dtype=float)\n        f_k = f_obj(x_k, A, b)\n        \n        # The bundle is a list of tuples: (y_j, f_j, g_j)\n        bundle = [(x_k, f_k, g_subgrad(x_k, A, b))]\n\n        for s in range(K_max):\n            J = len(bundle)\n            \n            # --- Form and Solve the Dual QP ---\n            # 1. Collect subgradients into a matrix G and compute linearization errors\n            G_matrix = np.array([item[2] for item in bundle]).T\n            epsilons = np.zeros(J)\n            for j in range(J):\n                y_j, f_j, g_j = bundle[j]\n                epsilons[j] = f_k - (f_j + g_j.T @ (x_k - y_j))\n            \n            # 2. Define the QP objective and its Jacobian\n            H = (1.0 / t) * (G_matrix.T @ G_matrix)\n            \n            def qp_obj(lmbda):\n                return 0.5 * lmbda.T @ H @ lmbda + epsilons.T @ lmbda\n\n            def qp_jac(lmbda):\n                return H @ lmbda + epsilons\n\n            # 3. Define QP constraints: sum(lambda_j) = 1 and lambda_j >= 0\n            constraints = [{'type': 'eq', 'fun': lambda lmbda: np.sum(lmbda) - 1.0}]\n            bounds = [(0, None) for _ in range(J)]\n            \n            # 4. Solve the QP\n            lmbda_0 = np.ones(J) / J\n            res = minimize(qp_obj, lmbda_0, jac=qp_jac, bounds=bounds, constraints=constraints, method='SLSQP')\n            lmbda_star = res.x\n            \n            # --- Evaluate Candidate Step ---\n            g_agg = G_matrix @ lmbda_star\n            x_hat = x_k - (1.0 / t) * g_agg\n            \n            # Predicted decrease from the model\n            eps_agg = epsilons.T @ lmbda_star\n            delta_m = eps_agg + (1.0 / t) * np.linalg.norm(g_agg)**2\n            \n            # Actual decrease in objective function\n            f_hat = f_obj(x_hat, A, b)\n            delta_a = f_k - f_hat\n            \n            # --- Acceptance Test ---\n            if delta_a >= alpha * delta_m:\n                # Serious step: found an improvement.\n                return s\n            else:\n                # Null step: improve the model by adding the new information.\n                # The proximal center x_k is NOT updated.\n                bundle.append((x_hat, f_hat, g_subgrad(x_hat, A, b)))\n        \n        # If the loop finishes, K_max null steps were taken without a serious step.\n        return K_max\n\n    # --- Main Execution Loop ---\n    results = []\n    for x0 in test_cases:\n        num_null_steps = run_bundle_for_case(x0)\n        results.append(num_null_steps)\n        \n    # --- Format and Print Output ---\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```", "id": "3105171"}, {"introduction": "在实现了基本算法之后，本练习将挑战您思考所用方向信息的质量。您将比较理论上更优的“聚合次梯度” $s_k$ 与一个更简单的“平均次梯度”启发式方法 $\\bar{g}$ 的预测能力。这项实践突显了束方法如何综合历史信息以在非光滑函数构成的复杂“地形”中做出更优决策的重要性 [@problem_id:3105153]。", "problem": "给定一个由线性函数的逐点最大值定义的非光滑目标函数。设 $A \\in \\mathbb{R}^{m \\times n}$ 的行向量为 $\\{a_i^\\top\\}_{i=1}^m$，并定义函数 $f : \\mathbb{R}^n \\to \\mathbb{R}$ 为 $f(x) = \\max_{1 \\le i \\le m} a_i^\\top x$。函数 $f$ 在点 $x$ 处的次微分 $\\partial f(x)$ 是激活行集合 $\\{a_i : i \\in I(x)\\}$ 的凸包，其中 $I(x)$ 是在 $x$ 处达到最大值的索引集合。束方法中使用的聚合次梯度 $s_k$ 取为 $\\partial f(x_k)$ 中欧几里得范数最小的元素，即在 $x_k$ 处的激活行的凸组合中，在 $\\mathbb{R}^n$ 中范数最小的那个。形式上，如果 $I(x_k) = \\{i_1,\\dots,i_p\\}$，那么 $s_k$ 是通过在满足 $\\alpha_j \\ge 0$ 和 $\\sum_{j=1}^p \\alpha_j = 1$ 的条件下，最小化 $s = \\sum_{j=1}^p \\alpha_j a_{i_j}$ 的欧几里得范数平方 $\\|s\\|_2^2$ 得到的。另外，定义一个平均近期窗口预测子 $\\bar{g}$，作为在近期迭代点 $\\{x_j\\}$ 处计算的近期次梯度 $g_j$ 的算术平均值，其中每个 $g_j$ 是 $x_j$ 处的任意有效次梯度（在本练习中，为确保确定性，取 $g_j$ 为 $x_j$ 处的一个激活行 $a_i$，选择 $I(x_j)$ 中的最小索引）。\n\n您的任务是实现一个程序，对于每个指定的测试用例，在当前迭代点 $x_k$ 执行以下步骤：\n1. 计算聚合次梯度 $s_k$，作为 $x_k$ 处激活行的最小欧几里得范数凸组合（如果 $I(x_k)$ 是单元素集，则 $s_k$ 就是那个单一向量）。\n2. 使用上述确定性选择规则，从提供的近期迭代点计算平均近期窗口次梯度 $\\bar{g}$。\n3. 对于每个预测子 $g_{\\text{hat}} \\in \\{s_k, \\bar{g}\\}$，形成最速下降方向 $d = - g_{\\text{hat}}$，取一个小的步长 $t$（如下所示），并计算：\n   - 实际下降量 $\\Delta f = f(x_k) - f(x_k + t d)$，其中 $f$ 通过最大值定义精确计算。\n   - 线性模型预测下降量 $v_{\\text{hat}} = t \\, \\|g_{\\text{hat}}\\|_2^2$，这来自于使用 $g_{\\text{hat}}$ 作为 $x_k$ 处次梯度的预测子对 $f$ 进行的局部线性化。\n4. 定义对齐误差 $e(g_{\\text{hat}}) = |\\Delta f - v_{\\text{hat}}|$，如果 $e(s_k) < e(\\bar{g})$，则声明聚合次梯度对齐得更好；否则，平均近期窗口预测子对齐得更好或两者持平。\n\n您的程序必须为下面的一组测试用例输出一行内容，该行包含一个布尔值列表，指示在每个用例中聚合次梯度 $s_k$ 是否比 $\\bar{g}$ 对齐得更好。\n\n此问题不涉及物理单位。所有角度（如有）均默认为弧度，但此问题不使用角度。所有分数和十进制数均已按此格式指定；请勿使用百分号。\n\n使用以下测试套件。在每个案例中，使用步长 $t = 0.01$。对于每个测试用例，$A$ 由其行向量给出，$x_k$ 是当前迭代点，近期窗口是用于计算近期次梯度 $g_j$ 并将其平均以形成 $\\bar{g}$ 的点列表。在近期点 $x_j$ 选择次梯度时，使用 $I(x_j)$ 中的最小索引来确定性地选择 $a_i$。\n\n- 测试用例 1（在 $x_k$ 处有两个激活行）：\n  - $A = \\begin{bmatrix} 1.0  0.0 \\\\ 0.7  0.6 \\\\ 1.0  -0.5 \\end{bmatrix}$，\n  - $x_k = \\begin{bmatrix} 1.0 \\\\ 0.0 \\end{bmatrix}$，\n  - 近期窗口点：$\\left\\{ \\begin{bmatrix} 1.0 \\\\ 0.0 \\end{bmatrix}, \\begin{bmatrix} 0.8 \\\\ 0.2 \\end{bmatrix}, \\begin{bmatrix} 1.2 \\\\ -0.1 \\end{bmatrix} \\right\\}$。\n\n- 测试用例 2（在 $x_k$ 处有唯一的激活行且包含负项）：\n  - $A = \\begin{bmatrix} -1.0  0.0 \\\\ 0.0  -1.0 \\\\ -0.5  -0.2 \\end{bmatrix}$，\n  - $x_k = \\begin{bmatrix} -1.0 \\\\ -2.0 \\end{bmatrix}$，\n  - 近期窗口点：$\\left\\{ \\begin{bmatrix} -1.0 \\\\ -2.0 \\end{bmatrix}, \\begin{bmatrix} -0.8 \\\\ -2.2 \\end{bmatrix}, \\begin{bmatrix} -1.2 \\\\ -1.8 \\end{bmatrix}, \\begin{bmatrix} -1.1 \\\\ -2.5 \\end{bmatrix} \\right\\}$。\n\n- 测试用例 3（窗口大小为一）：\n  - $A = \\begin{bmatrix} 0.0  1.0 \\\\ 1.0  0.1 \\\\ 0.6  0.7 \\end{bmatrix}$，\n  - $x_k = \\begin{bmatrix} 0.2 \\\\ 0.5 \\end{bmatrix}$，\n  - 近期窗口点：$\\left\\{ \\begin{bmatrix} 0.1 \\\\ 0.4 \\end{bmatrix} \\right\\}$。\n\n- 测试用例 4（三维空间，重复项之间存在平局）：\n  - $A = \\begin{bmatrix} 1.0  -0.5  0.2 \\\\ 0.8  0.9  -0.3 \\\\ 0.6  0.0  1.2 \\\\ 1.0  -0.5  0.2 \\end{bmatrix}$，\n  - $x_k = \\begin{bmatrix} 0.2 \\\\ -1.0 \\\\ 0.0 \\end{bmatrix}$，\n  - 近期窗口点：$\\left\\{ \\begin{bmatrix} 0.2 \\\\ -1.0 \\\\ 0.0 \\end{bmatrix}, \\begin{bmatrix} 0.0 \\\\ -1.0 \\\\ 0.5 \\end{bmatrix}, \\begin{bmatrix} 0.6 \\\\ -0.5 \\\\ 0.8 \\end{bmatrix} \\right\\}$。\n\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表（例如 `[result1,result2,result3,result4]`）作为结果，其中每个结果都是一个布尔值，指示对应测试用例的 $e(s_k) < e(\\bar{g})$ 是否成立。当出现平局时，通过在任何需要单个次梯度的点选择 $I(x)$ 中的最小索引来确保确定性选择。\n\n注意：聚合次梯度的计算需要解决一个凸二次规划（QP）问题，即在概率单纯形上最小化一个凸二次目标。您必须实现一个数值稳定且确定性的过程，为所有给定的测试用例生成有效解。", "solution": "所提出的问题是非光滑优化领域的一个有效计算任务。它要求为一个定义为仿射函数逐点最大值的函数，实现并比较两种不同的次梯度预测子。该问题有科学依据，定义明确，所有术语和数据都已明确提供。下面将逐步进行求解。\n\n问题的核心是为一个函数 $f(x) = \\max_{1 \\le i \\le m} a_i^\\top x$，在点 $x_k$ 比较两种次梯度估计的局部预测能力。这两种预测子是聚合次梯度 $s_k$ 和平均近期窗口次梯度 $\\bar{g}$。比较基于对齐误差，该误差衡量实际函数下降量与基于次梯度预测子的线性模型所预测的下降量之间的差异。较小的对齐误差表示预测子更好。\n\n让我们概述一下对于一个给定的测试用例（包括矩阵 $A$、当前迭代点 $x_k$ 和一组近期窗口点）所需的计算步骤。下降步长给定为 $t = 0.01$。\n\n首先，我们定义两个辅助函数：一个用于计算 $f(x)$，另一个用于识别在点 $x$ 处的激活次梯度集。\n函数 $f(x)$ 计算为 $f(x) = \\max_{i} (A x)_i$。\n在 $x$ 处的激活索引集，表示为 $I(x)$，是 $I(x) = \\{i \\mid a_i^\\top x = f(x)\\}$。由于浮点运算，我们将 $f(x) - a_i^\\top x < \\epsilon$（对于一个小的容差 $\\epsilon$，例如 $\\epsilon=10^{-9}$）的索引识别为激活索引。相应的激活次梯度是行向量 $\\{a_i\\}_{i \\in I(x)}$。\n\n**第 1 步：计算聚合次梯度 $s_k$**\n\n聚合次梯度 $s_k$ 是次微分 $\\partial f(x_k) = \\text{conv}\\{a_i \\mid i \\in I(x_k)\\}$ 中欧几里得范数最小的元素。这等价于将原点投影到 $x_k$ 处激活次梯度的凸包上。\n\n设在 $x_k$ 处的激活次梯度集为 $G_{act} = \\{g_1, g_2, \\dots, g_p\\}$，其中每个 $g_j$ 是来自 $A$ 的某个行向量 $a_i^\\top$，其中 $i \\in I(x_k)$。我们需要找到 $s_k = \\sum_{j=1}^p \\alpha_j g_j$，它最小化 $\\|s_k\\|_2^2$，并满足约束条件 $\\sum_{j=1}^p \\alpha_j = 1$ 和 $\\alpha_j \\ge 0$ 对所有 $j$ 成立。\n\n这是一个凸二次规划（QP）问题。设 $G$ 是一个矩阵，其列是向量 $g_j \\in G_{act}$。我们寻求找到向量 $\\alpha = [\\alpha_1, \\dots, \\alpha_p]^\\top$ 来解决：\n$$\n\\min_{\\alpha} \\frac{1}{2} \\|G \\alpha\\|_2^2 = \\frac{1}{2} \\alpha^\\top (G^\\top G) \\alpha\n$$\n约束条件为：\n$$\n\\sum_{j=1}^p \\alpha_j = 1, \\quad \\alpha_j \\ge 0, \\quad j=1, \\dots, p\n$$\n这个标准的二次规划问题可以使用数值优化库来解决，例如使用 'SLSQP' 方法的 `scipy.optimize.minimize`。目标函数是二次的，约束是线性的。\n\n如果 $|I(x_k)|=1$，意味着只有一个激活次梯度 $a_i$，那么凸包就是一个单点，因此 $s_k = a_i$。\n\n**第 2 步：计算平均近期窗口次梯度 $\\bar{g}$**\n\n预测子 $\\bar{g}$ 是在“近期窗口”中提供的一系列近期点上计算的次梯度的算术平均值。对于窗口中的每个点 $x_j$，我们必须首先确定一个次梯度 $g_j \\in \\partial f(x_j)$。问题指定了一个确定性规则：识别激活索引集 $I(x_j)$，并选择与 $I(x_j)$ 中最小索引 $i$ 对应的次梯度 $a_i$。\n在为窗口中的每个 $x_j$ 计算出 $g_j$ 后，$\\bar{g}$ 按如下方式计算：\n$$\n\\bar{g} = \\frac{1}{|W|} \\sum_{x_j \\in W} g_j\n$$\n其中 $W$ 是近期窗口点的集合。\n\n**第 3 步：计算对齐误差**\n\n对于每个预测子 $g_{\\text{hat}} \\in \\{s_k, \\bar{g}\\}$，我们评估其质量。质量通过将实际函数下降量与从函数线性模型导出的预测下降量进行比较来衡量。\n\n下降方向取为 $d = -g_{\\text{hat}}$。从 $x_k$ 沿该方向走一个步长 $t$，到达新点 $x_{\\text{new}} = x_k + t d$。\n\n实际函数下降量为：\n$$\n\\Delta f = f(x_k) - f(x_{\\text{new}}) = f(x_k) - f(x_k - t g_{\\text{hat}})\n$$\n\n基于一阶近似 $f(x_k+d) \\approx f(x_k) + g_{\\text{hat}}^\\top d$ 的预测下降量为：\n$$\nv_{\\text{hat}} = -g_{\\text{hat}}^\\top d = -g_{\\text{hat}}^\\top (-t g_{\\text{hat}}) = t \\, \\|g_{\\text{hat}}\\|_2^2\n$$\n\n预测子 $g_{\\text{hat}}$ 的对齐误差定义为实际下降量与预测下降量之间的绝对差：\n$$\ne(g_{\\text{hat}}) = |\\Delta f - v_{\\text{hat}}|\n$$\n我们为 $s_k$ 和 $\\bar{g}$ 计算这个误差，得到 $e(s_k)$ 和 $e(\\bar{g})$。\n\n**第 4 步：比较预测子**\n\n最后一步是比较这两个误差。如果聚合次梯度 $s_k$ 的误差严格小于平均预测子 $\\bar{g}$ 的误差，则认为它对齐得更好。也就是说，如果 $e(s_k) < e(\\bar{g})$，则该测试用例的结果为 `True`，否则为 `False`。对每个提供的测试用例执行此比较。最终输出是这些布尔结果的列表。", "answer": "```python\nimport numpy as np\nfrom scipy.optimize import minimize, Bounds\n\ndef solve():\n    \"\"\"\n    Solves the nonsmooth optimization predictor comparison problem for a suite of test cases.\n    \"\"\"\n    \n    # Test cases defined in the problem statement.\n    test_cases = [\n        {\n            \"A\": np.array([[1.0, 0.0], [0.7, 0.6], [1.0, -0.5]]),\n            \"x_k\": np.array([1.0, 0.0]),\n            \"window_points\": [np.array([1.0, 0.0]), np.array([0.8, 0.2]), np.array([1.2, -0.1])]\n        },\n        {\n            \"A\": np.array([[-1.0, 0.0], [0.0, -1.0], [-0.5, -0.2]]),\n            \"x_k\": np.array([-1.0, -2.0]),\n            \"window_points\": [np.array([-1.0, -2.0]), np.array([-0.8, -2.2]), np.array([-1.2, -1.8]), np.array([-1.1, -2.5])]\n        },\n        {\n            \"A\": np.array([[0.0, 1.0], [1.0, 0.1], [0.6, 0.7]]),\n            \"x_k\": np.array([0.2, 0.5]),\n            \"window_points\": [np.array([0.1, 0.4])]\n        },\n        {\n            \"A\": np.array([[1.0, -0.5, 0.2], [0.8, 0.9, -0.3], [0.6, 0.0, 1.2], [1.0, -0.5, 0.2]]),\n            \"x_k\": np.array([0.2, -1.0, 0.0]),\n            \"window_points\": [np.array([0.2, -1.0, 0.0]), np.array([0.0, -1.0, 0.5]), np.array([0.6, -0.5, 0.8])]\n        }\n    ]\n\n    t = 0.01  # Step size\n    epsilon = 1e-9 # Tolerance for floating point comparisons of active subgradients\n    results = []\n    \n    def f(x_vec, A_mat):\n        \"\"\"Computes the function value f(x) = max(A @ x).\"\"\"\n        return np.max(A_mat @ x_vec)\n\n    def get_active_indices(x_vec, A_mat):\n        \"\"\"Finds the indices of active rows in A at point x.\"\"\"\n        vals = A_mat @ x_vec\n        max_val = np.max(vals)\n        return np.where(max_val - vals = epsilon)[0]\n\n    for case in test_cases:\n        A, x_k, window_points = case[\"A\"], case[\"x_k\"], case[\"window_points\"]\n        \n        # 1. Compute the aggregate subgradient s_k\n        active_indices_k = get_active_indices(x_k, A)\n        \n        # We need to handle duplicate active subgradients carefully.\n        # The QP must be over unique vectors to form a valid basis.\n        unique_active_subgrads, unique_indices = np.unique(A[active_indices_k], axis=0, return_index=True)\n\n        if len(unique_active_subgrads) == 1:\n            s_k = unique_active_subgrads[0]\n        else:\n            # G has unique active subgradients as columns.\n            G = unique_active_subgrads.T\n            p = G.shape[1]\n            Q = G.T @ G\n            \n            def qp_objective(alpha, Q_mat):\n                return 0.5 * alpha.T @ Q_mat @ alpha\n\n            constraints = ({'type': 'eq', 'fun': lambda alpha: np.sum(alpha) - 1})\n            bounds = Bounds([0.] * p, [np.inf] * p)\n            alpha_0 = np.ones(p) / p\n            \n            res = minimize(fun=qp_objective, x0=alpha_0, args=(Q,), method='SLSQP', bounds=bounds, constraints=constraints)\n            optimal_alpha = res.x\n            s_k = G @ optimal_alpha\n\n        # 2. Compute the averaged recent-window subgradient g_bar\n        recent_subgrads = []\n        for x_j in window_points:\n            active_indices_j = get_active_indices(x_j, A)\n            smallest_idx = np.min(active_indices_j)\n            g_j = A[smallest_idx]\n            recent_subgrads.append(g_j)\n        g_bar = np.mean(recent_subgrads, axis=0)\n\n        # 3. Calculate alignment errors\n        f_k = f(x_k, A)\n        \n        # For s_k\n        d_s = -s_k\n        x_new_s = x_k + t * d_s\n        delta_f_s = f_k - f(x_new_s, A)\n        v_hat_s = t * np.linalg.norm(s_k)**2\n        e_s = np.abs(delta_f_s - v_hat_s)\n        \n        # For g_bar\n        d_g = -g_bar\n        x_new_g = x_k + t * d_g\n        delta_f_g = f_k - f(x_new_g, A)\n        v_hat_g = t * np.linalg.norm(g_bar)**2\n        e_g = np.abs(delta_f_g - v_hat_g)\n\n        # 4. Compare and append result\n        results.append(e_s  e_g)\n        \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3105153"}, {"introduction": "一个稳健的算法必须能处理不完美或冗余的数据。这最后一个练习将展示束方法的一个关键优势：其在面对退化信息时的稳定性。您将通过编程验证，无论束中包含多少重复的切平面，聚合过程的结果 $x^\\star$ 和 $g_{\\text{agg}}$ 都是不变的，从而亲手确认该方法的理论可靠性和数值稳定性 [@problem_id:3105175]。", "problem": "考虑一个非光滑凸函数，它被定义为有限个仿射函数（切分）的逐点最大值。设该函数由一组形式为 $\\ell_i(x) = g_i^\\top x + c_i$ 的切分 $\\ell_i(x)$ 表示，其中 $g_i \\in \\mathbb{R}^n$ 且 $c_i \\in \\mathbb{R}$。在第 $k$ 次迭代时的切平面模型为 $m_k(x) = \\max_i \\ell_i(x)$。丛方法中的正则化主问题是通过求解以下凸二次规划（QP）问题来计算邻近中心更新 $x^\\star$：\n$$\n\\min_{x \\in \\mathbb{R}^n, \\, s \\in \\mathbb{R}} \\; s + \\frac{t}{2}\\|x - x_k\\|_2^2 \\quad \\text{约束条件为} \\quad s \\ge \\ell_i(x) \\;\\; \\text{对所有}\\;\\; i,\n$$\n其中 $t  0$ 是正则化参数，$x_k \\in \\mathbb{R}^n$ 是当前的邻近中心。\n\n当多个切分 $\\ell_i$ 重合（即完全相同）但源自不同的点 $x_i$ 和可能不同的次梯度评估 $g_i$ 时（尽管它们定义了相同的仿射函数 $x \\mapsto g^\\top x + c$），丛方法中会出现一种退化现象。在这种情况下，切分的聚合（通过与主问题的活动约束相关联的对偶乘子）应能处理冗余，而不会使步长 $x^\\star$ 或模型预测不稳定，并且聚合的切分在相同切分的重复下应保持不变。\n\n您的任务是实现一个程序，针对下面描述的每种情况，使用给定的切分求解正则化主问题，计算聚合次梯度 $g_{\\text{agg}}$ 和相应的更新 $x^\\star$，然后评估重复相同切分对解和模型值的影响。聚合的切分可以表示为 $\\ell_{\\text{agg}}(x) = g_{\\text{agg}}^\\top x + c_{\\text{agg}}$，其中 $c_{\\text{agg}}$ 是偏移量 $c_i$ 的凸组合，其系数自然地从主问题的最优性条件中产生。您的实现应基于凸优化的基本原理，从主问题出发，仅使用公认的定义和事实；不要使用任何跳过基本推导路径的快捷公式或预先推导的表达式。\n\n使用以下场景（维度为 $n=2$）。在每个场景中，构建指定的切分集，求解主问题两次（一次包含重复切分，一次将每个相同切分的重复项合并为单个代表），并计算所要求的输出。下面提到的所有数值公差均为绝对公差。\n\n场景 $\\mathsf{S1}$（一般情况）：\n- 正则化参数：$t = 2.0$。\n- 邻近中心：$x_k = [1.3, -2.1]$。\n- 相同切分族 $\\mathsf{A}$：$g_{\\mathsf{A}} = [2, -1]$，$c_{\\mathsf{A}} = 0.0$，在三个不同点重复（因此有三个相同的切分）。\n- 额外的不同切分 $\\mathsf{B}$：$g_{\\mathsf{B}} = [-1, 2]$，$c_{\\mathsf{B}} = -0.5$，包含一次。\n- 构建“重复”集，包含三个 $\\mathsf{A}$ 的副本和一个 $\\mathsf{B}$ 的副本；以及“唯一”集，包含一个 $\\mathsf{A}$ 的副本和一个 $\\mathsf{B}$ 的副本。\n- 计算：\n  1. $b_1$：一个布尔值，指示 $\\|x^\\star_{\\text{dup}} - x^\\star_{\\text{uniq}}\\|_2 \\le 10^{-9}$ 是否成立。\n  2. $b_2$：一个布尔值，指示 $\\|g_{\\text{agg,dup}} - g_{\\text{agg,uniq}}\\|_2 \\le 10^{-9}$ 是否成立。\n  3. $d_1$：一个浮点数，等于 $\\left| \\left( s_{\\text{dup}} + \\frac{t}{2}\\|x^\\star_{\\text{dup}} - x_k\\|_2^2 \\right) - \\left( s_{\\text{uniq}} + \\frac{t}{2}\\|x^\\star_{\\text{uniq}} - x_k\\|_2^2 \\right) \\right|$，其中 $s$ 是主问题中的最优上镜图变量。\n\n场景 $\\mathsf{S2}$（边缘情况：只有相同的切分）：\n- 正则化参数：$t = 1.5$。\n- 邻近中心：$x_k = [-3.7, 4.2]$。\n- 仅有相同切分族 $\\mathsf{A}$：$g_{\\mathsf{A}} = [2, -1]$，$c_{\\mathsf{A}} = 0.0$，重复五次。\n- 构建“重复”集，包含五个 $\\mathsf{A}$ 的副本；以及“唯一”集，包含一个 $\\mathsf{A}$ 的副本。\n- 计算：\n  1. $b_3$：一个布尔值，指示 $\\|x^\\star_{\\text{dup}} - x^\\star_{\\text{uniq}}\\|_2 \\le 10^{-9}$ 是否成立。\n  2. $b_4$：一个布尔值，指示 $\\|g_{\\text{agg,dup}} - g_{\\text{agg,uniq}}\\|_2 \\le 10^{-9}$ 是否成立。\n  3. $d_2$：一个浮点数，等于 $\\left| \\left( s_{\\text{dup}} + \\frac{t}{2}\\|x^\\star_{\\text{dup}} - x_k\\|_2^2 \\right) - \\left( s_{\\text{uniq}} + \\frac{t}{2}\\|x^\\star_{\\text{uniq}} - x_k\\|_2^2 \\right) \\right|$。\n\n场景 $\\mathsf{S3}$（边界情况：非常强的正则化）：\n- 正则化参数：$t = 1000.0$。\n- 邻近中心：$x_k = [10.0, -10.0]$。\n- 相同切分族 $\\mathsf{A}$：$g_{\\mathsf{A}} = [2, -1]$，$c_{\\mathsf{A}} = 0.0$，重复四次。\n- 相同切分族 $\\mathsf{B}$：$g_{\\mathsf{B}} = [-1, 2]$，$c_{\\mathsf{B}} = -0.5$，重复四次。\n- 构建“重复”集，包含四个 $\\mathsf{A}$ 的副本和四个 $\\mathsf{B}$ 的副本；以及“唯一”集，包含一个 $\\mathsf{A}$ 的副本和一个 $\\mathsf{B}$ 的副本。\n- 计算：\n  1. $b_5$：一个布尔值，指示 $\\|x^\\star_{\\text{dup}} - x^\\star_{\\text{uniq}}\\|_2 \\le 10^{-9}$ 是否成立。\n  2. $b_6$：一个布尔值，指示 $\\|g_{\\text{agg,dup}} - g_{\\text{agg,uniq}}\\|_2 \\le 10^{-9}$ 是否成立。\n  3. $d_3$：一个浮点数，等于 $\\left| \\left( s_{\\text{dup}} + \\frac{t}{2}\\|x^\\star_{\\text{dup}} - x_k\\|_2^2 \\right) - \\left( s_{\\text{uniq}} + \\frac{t}{2}\\|x^\\star_{\\text{uniq}} - x_k\\|_2^2 \\right) \\right|$。\n\n实现要求：\n- 从主问题和基本的凸优化事实出发，推导并使用一个有原则的算法。您可以使用对应于活动切分凸组合系数（对偶乘子）的变量的等价公式，前提是您的推导基于第一性原理。\n- 对于每个场景，您必须为“重复”集和“唯一”集计算 $x^\\star$、$s$ 和 $g_{\\text{agg}}$。\n- 最终输出必须以 $10^{-9}$ 的绝对公差计算布尔值，并使用为浮点数实现的精确算术进行计算。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含按顺序聚合的九个结果，形式为逗号分隔的列表，并用方括号括起来：$[b_1,b_2,d_1,b_3,b_4,d_2,b_5,b_6,d_3]$。\n- 布尔值必须打印为 $\\texttt{True}$ 或 $\\texttt{False}$，浮点数必须打印为标准十进制数。\n\n本问题不涉及物理单位或角度单位。", "solution": "该问题要求分析丛方法主问题中冗余切分的影响。我们将通过推导正则化主问题的对偶问题，证明其对重复切分的不变性，并针对三种指定场景进行数值验证来实现这一目标。\n\n### 步骤 1：问题验证\n\n所提供的问题陈述是一个在凸优化领域，特别是关于非光滑函数丛方法理论的，形式良好且具有科学依据的练习。\n\n**1. 提取给定条件：**\n为三种不同的场景（$\\mathsf{S1}$, $\\mathsf{S2}$, $\\mathsf{S3}$）提供了所有必要的参数。对于每个场景，给定条件包括：\n- 正则化参数 $t$。\n- 邻近中心 $x_k$。\n- 一组由梯度 $g_i$ 和偏移量 $c_i$ 定义的仿射切分，包括相同切分的族。\n- “重复”和“唯一”切分集的定义。\n- 需要精确计算的量：解向量（$x^\\star$, $g_{\\text{agg}}$）的布尔比较 $b_1, \\dots, b_6$ 和主问题目标值的浮点差异 $d_1, d_2, d_3$。\n- 向量比较的绝对公差为 $10^{-9}$。\n\n**2. 使用提取的给定条件进行验证：**\n- **科学依据：** 该问题基于近端丛方法的标准公式，这是一种在非光滑凸优化中成熟的技术。主问题是一个凸二次规划（QP），其分析使用凸分析和对偶理论的基本原理。所有概念都是标准的且事实正确。\n- **形式良好：** 主问题是一个严格凸的QP（由于项 $\\frac{t}{2}\\|x - x_k\\|_2^2$ 中 $t0$），这保证了唯一解 $(x^\\star, s^\\star)$ 的存在。问题是自包含的，并提供了找到此解所需的所有数据。\n- **客观性：** 问题陈述使用精确、无歧义的数学语言进行表述。任务是客观的，并且可以通过计算进行验证。\n\n**3. 结论与行动：**\n问题是有效的。它在科学上是合理的，形式良好且客观。将按要求基于第一性原理来开发解决方案。\n\n### 步骤 2：推导与求解\n\n任务的核心是求解正则化主问题，这是一个凸二次规划（QP）：\n$$\n\\min_{x \\in \\mathbb{R}^n, \\, s \\in \\mathbb{R}} \\; s + \\frac{t}{2}\\|x - x_k\\|_2^2 \\quad \\text{约束条件为} \\quad g_i^\\top x + c_i - s \\le 0 \\;\\; \\text{对所有切分}\\;\\; i=1, \\dots, m.\n$$\n为了从第一性原理出发求解此问题，我们推导并求解其对偶问题。\n\n**1. 对偶问题的推导**\n设 $\\lambda_i \\ge 0$ 是与第 $i$ 个约束相关联的拉格朗日乘子。拉格朗日函数为：\n$$\nL(x, s, \\lambda) = s + \\frac{t}{2}\\|x - x_k\\|_2^2 + \\sum_{i=1}^m \\lambda_i (g_i^\\top x + c_i - s)\n$$\n对偶函数 $q(\\lambda)$ 是拉格朗日函数关于原始变量 $x$ 和 $s$ 的下确界。我们通过将 $L$ 对 $x$ 和 $s$ 的偏导数设为零（平稳性条件）来找到此下确界。\n\n对 $s$ 的导数：\n$$\n\\frac{\\partial L}{\\partial s} = 1 - \\sum_{i=1}^m \\lambda_i = 0 \\implies \\sum_{i=1}^m \\lambda_i = 1\n$$\n此条件表明，最优乘子 $\\lambda_i^\\star$ 必须构成一个凸组合。\n\n对 $x$ 的导数：\n$$\n\\nabla_x L = t(x - x_k) + \\sum_{i=1}^m \\lambda_i g_i = 0\n$$\n这给出了最优原始解 $x^\\star$ 关于最优乘子 $\\lambda^\\star$ 的表达式：\n$$\nx^\\star = x_k - \\frac{1}{t} \\sum_{i=1}^m \\lambda_i g_i\n$$\n将这些条件代入拉格朗日函数以消去 $x$ 和 $s$：\n$$\n\\begin{align*}\nq(\\lambda) = s\\left(1 - \\sum_{i=1}^m \\lambda_i\\right) + \\frac{t}{2}\\left\\|-\\frac{1}{t}\\sum_{i=1}^m \\lambda_i g_i\\right\\|_2^2 + \\sum_{i=1}^m \\lambda_i c_i + \\sum_{i=1}^m \\lambda_i g_i^\\top \\left(x_k - \\frac{1}{t}\\sum_{j=1}^m \\lambda_j g_j\\right) \\\\\n= 0 + \\frac{1}{2t}\\left\\|\\sum_{i=1}^m \\lambda_i g_i\\right\\|_2^2 + \\sum_{i=1}^m \\lambda_i c_i + \\left(\\sum_{i=1}^m \\lambda_i g_i\\right)^\\top x_k - \\frac{1}{t}\\left\\|\\sum_{i=1}^m \\lambda_i g_i\\right\\|_2^2 \\\\\n= -\\frac{1}{2t}\\left\\|\\sum_{i=1}^m \\lambda_i g_i\\right\\|_2^2 + \\sum_{i=1}^m \\lambda_i (g_i^\\top x_k + c_i)\n\\end{align*}\n$$\n对偶问题是最大化 $q(\\lambda)$，受限于对 $\\lambda$ 的约束：\n$$\n\\max_{\\lambda \\in \\mathbb{R}^m} \\quad -\\frac{1}{2t}\\left\\|\\sum_{i=1}^m \\lambda_i g_i\\right\\|_2^2 + \\sum_{i=1}^m \\lambda_i (g_i^\\top x_k + c_i) \\quad \\text{s.t.} \\quad \\sum_{i=1}^m \\lambda_i = 1, \\;\\; \\lambda_i \\ge 0.\n$$\n这等价于最小化 $-q(\\lambda)$，这是一个标准形式的凸 QP。令 $G$ 为一个 $n \\times m$ 矩阵，其列为次梯度 $g_i$，令 $\\alpha$ 为一个 $m$ 维向量，其中 $\\alpha_i = g_i^\\top x_k + c_i$。对偶问题变为：\n$$\n\\min_{\\lambda \\in \\mathbb{R}^m} \\quad \\frac{1}{2t}\\|G\\lambda\\|_2^2 - \\alpha^\\top \\lambda \\quad \\text{s.t.} \\quad \\mathbf{1}^\\top \\lambda = 1, \\;\\; \\lambda \\ge 0.\n$$\n这可以写成 $\\min_{\\lambda} \\frac{1}{2}\\lambda^\\top H \\lambda + f^\\top \\lambda$，其中 $H = \\frac{1}{t}G^\\top G$，$f = -\\alpha$。\n\n**2. 原始解的恢复与不变性**\n通过求解此 QP 找到最优对偶变量 $\\lambda^\\star$ 后，我们计算所需的量：\n- **聚合次梯度：** $g_{\\text{agg}} = \\sum_{i=1}^m \\lambda_i^\\star g_i = G\\lambda^\\star$。\n- **邻近中心更新：** $x^\\star = x_k - \\frac{1}{t} g_{\\text{agg}}$。\n- **上镜图变量：** 在最优解处，$s$ 被迫等于仿射函数的上包络，因此 $s^\\star = \\max_i(g_i^\\top x^\\star + c_i)$。\n\n现在，考虑重复切分的情况。设唯一切分的集合由索引 $u \\in U$ 标记。设 $I_u$ 是原始（重复）列表中对应于唯一切分 $u$ 的索引集合。那么对于任何 $i \\in I_u$，我们有 $g_i = g_u$ 和 $c_i = c_u$，这意味着 $\\alpha_i = \\alpha_u$。\n令 $\\hat{\\lambda}_u = \\sum_{i \\in I_u} \\lambda_i$。对偶目标中的项可以分组：\n$$\n\\sum_{i=1}^m \\lambda_i g_i = \\sum_{u \\in U} \\sum_{i \\in I_u} \\lambda_i g_u = \\sum_{u \\in U} \\left(\\sum_{i \\in I_u} \\lambda_i\\right) g_u = \\sum_{u \\in U} \\hat{\\lambda}_u g_u\n$$\n$$\n\\sum_{i=1}^m \\lambda_i \\alpha_i = \\sum_{u \\in U} \\sum_{i \\in I_u} \\lambda_i \\alpha_u = \\sum_{u \\in U} \\left(\\sum_{i \\in I_u} \\lambda_i\\right) \\alpha_u = \\sum_{u \\in U} \\hat{\\lambda}_u \\alpha_u\n$$\n对 $\\lambda$ 的约束转化为 $\\sum_{u \\in U} \\hat{\\lambda}_u = 1$ 和 $\\hat{\\lambda}_u \\ge 0$。用 $\\hat{\\lambda}_u$ 表示的对偶问题与唯一切分集的对偶问题是相同的。\n这证明了 $\\hat{\\lambda}_u^\\star$ 的最优值是唯一的，并且与切分 $u$ 的重复次数无关。因此：\n- $g_{\\text{agg}} = \\sum_{u \\in U} \\hat{\\lambda}_u^\\star g_u$ 是不变的。\n- $x^\\star = x_k - \\frac{1}{t} g_{\\text{agg}}$ 是不变的。\n- $s^\\star = \\max_{u \\in U} (g_u^\\top x^\\star + c_u)$ 是不变的。\n- 最优目标值 $s^\\star + \\frac{t}{2}\\|x^\\star - x_k\\|_2^2$ 是不变的。\n\n因此，对于每个场景，我们预期 $\\|x^\\star_{\\text{dup}} - x^\\star_{\\text{uniq}}\\|_2$、$\\|g_{\\text{agg,dup}} - g_{\\text{agg,uniq}}\\|_2$ 以及目标值的差异都应为零，直至数值精度。\n\n**3. 算法实现**\n对于每个场景以及“重复”和“唯一”切分集，都实现了以下过程：\n1. 构建矩阵 $G$ 和向量 $\\alpha$。\n2. 构造对偶 QP 目标函数和约束。\n3. 使用数值优化例程（`scipy.optimize.minimize`）求解对偶 QP 以得到 $\\lambda^\\star$。\n4. 从 $\\lambda^\\star$ 计算 $g_{\\text{agg}}$、$x^\\star$ 和 $s^\\star$。\n5. 计算原始目标值。\n6. 比较“重复”和“唯一”运行的结果，以计算所需的布尔值和浮点数差异。", "answer": "```python\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef solve_master_problem(cuts, t, x_k):\n    \"\"\"\n    Solves the regularized bundle method master problem via its dual.\n\n    Args:\n        cuts (list of tuples): A list where each tuple is (g, c)\n                               defining a cut l(x) = g.T @ x + c.\n        t (float): The regularization parameter.\n        x_k (np.ndarray): The current proximal center.\n\n    Returns:\n        tuple: A tuple containing (x_star, g_agg, s_star).\n    \"\"\"\n    m = len(cuts)\n    n = len(x_k)\n\n    if m == 0:\n        # No cuts, trivial solution.\n        g_agg = np.zeros(n)\n        x_star = x_k.copy()\n        # s is unconstrained from below, but in context of min s, it implies\n        # s would go to -inf. The problem context implies a non-empty bundle.\n        # However, the scenarios provided always have at least one cut.\n        # This case is for robustness and won't be hit.\n        s_star = -np.inf \n        return x_star, g_agg, s_star\n\n    G = np.array([cut[0] for cut in cuts]).T  # Shape (n, m)\n    c = np.array([cut[1] for cut in cuts])    # Shape (m,)\n    \n    alpha = G.T @ x_k + c\n\n    # Dual QP objective: min 0.5 * lambda.T @ H @ lambda + f.T @ lambda\n    # H = (1/t) * G.T @ G\n    H = (1.0 / t) * (G.T @ G)\n    # f = -alpha\n    f = -alpha\n\n    def dual_objective(lambda_vec):\n        return 0.5 * lambda_vec.T @ H @ lambda_vec + f.T @ lambda_vec\n\n    # Constraints: sum(lambda) = 1, lambda_i >= 0\n    constraints = ({'type': 'eq', 'fun': lambda l: np.sum(l) - 1.0})\n    bounds = tuple((0, None) for _ in range(m))\n\n    # Initial guess for the optimizer\n    lambda0 = np.ones(m) / m\n\n    res = minimize(dual_objective, lambda0, method='SLSQP', bounds=bounds, constraints=constraints)\n    \n    lambda_star = res.x\n\n    # Numerical stability: project lambda_star onto the simplex\n    lambda_star[lambda_star  0] = 0\n    if np.sum(lambda_star) > 0:\n        lambda_star /= np.sum(lambda_star)\n\n    # Recover primal solution\n    g_agg = G @ lambda_star\n    x_star = x_k - (1.0 / t) * g_agg\n    s_star = np.max(G.T @ x_star + c)\n    \n    return x_star, g_agg, s_star\n\ndef solve_scenario(t, x_k, cuts_dup, cuts_uniq):\n    \"\"\"\n    Solves a scenario for both duplicate and unique cuts and computes metrics.\n    \"\"\"\n    # Solve for duplicate cuts\n    x_star_dup, g_agg_dup, s_star_dup = solve_master_problem(cuts_dup, t, x_k)\n    obj_val_dup = s_star_dup + (t / 2.0) * np.linalg.norm(x_star_dup - x_k)**2\n\n    # Solve for unique cuts\n    x_star_uniq, g_agg_uniq, s_star_uniq = solve_master_problem(cuts_uniq, t, x_k)\n    obj_val_uniq = s_star_uniq + (t / 2.0) * np.linalg.norm(x_star_uniq - x_k)**2\n    \n    # Compute metrics\n    tol = 1e-9\n    b_x = np.linalg.norm(x_star_dup - x_star_uniq) = tol\n    b_g = np.linalg.norm(g_agg_dup - g_agg_uniq) = tol\n    d_obj = np.abs(obj_val_dup - obj_val_uniq)\n\n    return b_x, b_g, d_obj\n\ndef solve():\n    results = []\n\n    # Scenario S1\n    t1 = 2.0\n    x_k1 = np.array([1.3, -2.1])\n    g_A1, c_A1 = np.array([2.0, -1.0]), 0.0\n    g_B1, c_B1 = np.array([-1.0, 2.0]), -0.5\n    cuts_dup1 = [(g_A1, c_A1)] * 3 + [(g_B1, c_B1)]\n    cuts_uniq1 = [(g_A1, c_A1), (g_B1, c_B1)]\n    b1, b2, d1 = solve_scenario(t1, x_k1, cuts_dup1, cuts_uniq1)\n    results.extend([b1, b2, d1])\n\n    # Scenario S2\n    t2 = 1.5\n    x_k2 = np.array([-3.7, 4.2])\n    g_A2, c_A2 = np.array([2.0, -1.0]), 0.0\n    cuts_dup2 = [(g_A2, c_A2)] * 5\n    cuts_uniq2 = [(g_A2, c_A2)]\n    b3, b4, d2 = solve_scenario(t2, x_k2, cuts_dup2, cuts_uniq2)\n    results.extend([b3, b4, d2])\n    \n    # Scenario S3\n    t3 = 1000.0\n    x_k3 = np.array([10.0, -10.0])\n    g_A3, c_A3 = np.array([2.0, -1.0]), 0.0\n    g_B3, c_B3 = np.array([-1.0, 2.0]), -0.5\n    cuts_dup3 = [(g_A3, c_A3)] * 4 + [(g_B3, c_B3)] * 4\n    cuts_uniq3 = [(g_A3, c_A3), (g_B3, c_B3)]\n    b5, b6, d3 = solve_scenario(t3, x_k3, cuts_dup3, cuts_uniq3)\n    results.extend([b5, b6, d3])\n\n    # Format and print the final output\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3105175"}]}