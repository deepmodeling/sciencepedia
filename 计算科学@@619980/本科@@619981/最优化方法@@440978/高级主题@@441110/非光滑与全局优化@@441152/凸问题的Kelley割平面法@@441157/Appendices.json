{"hands_on_practices": [{"introduction": "理论学习之后，最佳的消化方式莫过于亲手实践。本练习旨在引导您完整地经历凯利切平面法的前三次迭代，从零开始构建一个逐步逼近最优解的下界模型。通过手动计算每个切平面并求解相应的线性规划主问题，您将对算法的核心机制——“查询-切割-求解”的循环——建立起具体而深刻的理解。[@problem_id:3141056]", "problem": "考虑一个凸最小化问题，其目标函数为 $f:\\mathbb{R}^{2}\\to\\mathbb{R}$，定义为 $f(x)=\\max\\{x_{1}+x_{2},\\,2x_{1}-x_{2},\\,1-x_{1}\\}$，定义域为方盒 $X=\\{x\\in\\mathbb{R}^{2}:\\|x\\|_{\\infty}\\le 2\\}$，其中 $\\|x\\|_{\\infty}=\\max\\{|x_{1}|,|x_{2}|\\}$。仿射函数的逐点最大值是凸的，对于一个凸函数 $f$，在点 $y$ 处的任意次梯度 $g\\in\\partial f(y)$ 对所有 $x$ 都满足次梯度不等式 $f(x)\\ge f(y)+g^{\\top}(x-y)$。凯利切平面法利用此不等式构造线性下估计量，并求解一系列在 $f$ 的上图 (epigraph) 上的线性规划 (LP) 主问题。\n\n使用凯利切平面法，从 $y^{0}=(0,0)$ 开始。在每次迭代 $k$ 中，在 $y^{k}$ 处查询预言机 (oracle)，它会返回一个 $f$ 在 $y^{k}$ 处的次梯度，该次梯度来自于在 $y^{k}$ 处取得最大值的某个活跃仿射函数。在变量 $(x,t)$ 中构建凯利切平面 $t\\ge f(y^{k})+g^{k\\top}(x-y^{k})$，并定义主 LP，其目标是最小化 $t$，约束条件为所有已收集的切平面和 $x\\in X$。令 $y^{k+1}$ 为主 LP 最优解的 $x$ 分量。如果存在多个最优解，则选择欧几里得范数 $\\|x\\|_{2}$ 最小的解作为 $y^{k+1}$。\n\n任务：\n- 显式计算在 $y^{0}$、$y^{1}$ 和 $y^{2}$ 处生成的前三个凯利切平面。\n- 写下相应的三个 LP 主问题。\n- 求解每个主问题以获得 $y^{1}$、$y^{2}$、$y^{3}$ 及其最优目标值 $t^{1}$、$t^{2}$、$t^{3}$。\n- 作为你的最终答案，提供 $t^{3}$ 的精确值。\n\n无需四舍五入；请用精确数表示你的最终答案。", "solution": "该问题要求将凯利切平面法应用于一个凸最小化问题。我们的任务是从一个给定的初始点开始，执行算法的三次迭代，并确定第三个主问题的最优目标值。\n\n问题是在可行集 $X = \\{x \\in \\mathbb{R}^2 : \\|x\\|_\\infty \\le 2\\}$ 上最小化 $f(x) = \\max\\{x_1+x_2, 2x_1-x_2, 1-x_1\\}$。集合 $X$ 可以明确地写为 $X = \\{(x_1, x_2) \\in \\mathbb{R}^2 : -2 \\le x_1 \\le 2, -2 \\le x_2 \\le 2\\}$。\n\n凯利切平面法生成一个点序列 $\\{y^k\\}$ 和一个最优值的下界序列 $\\{t^k\\}$。在每次迭代 $k$ 中，我们求解一个线性规划 (LP) 主问题来找到下一个迭代点 $y^{k+1}$ 和更新后的下界 $t^{k+1}$。主问题最小化变量 $t$，其约束条件是一组称为“切平面”的线性不等式，这些不等式由次梯度不等式导出。\n\n第 $k$ 个切平面由 $t \\ge f(y^k) + g^{k\\top}(x-y^k)$ 给出，其中 $g^k$ 是 $f$ 在 $y^k$ 处的一个次梯度。对于一个定义为若干可微函数最大值的函数，其在某一点的次梯度可以取为在该点处于活跃状态（即达到最大值）的任意一个函数的梯度。\n\n设三个仿射函数为 $f_1(x) = x_1+x_2$，$f_2(x) = 2x_1-x_2$ 和 $f_3(x) = 1-x_1$。它们各自的梯度分别为 $g_1 = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$，$g_2 = \\begin{pmatrix} 2 \\\\ -1 \\end{pmatrix}$ 和 $g_3 = \\begin{pmatrix} -1 \\\\ 0 \\end{pmatrix}$。\n\n**迭代 0 ($k=0$)**\n\n我们从初始点 $y^0 = (0,0)$ 开始。\n\n1.  **预言机查询**：我们首先计算目标函数在 $y^0$ 处的值：\n    $$f(y^0) = f(0,0) = \\max\\{0+0, 2(0)-0, 1-0\\} = \\max\\{0, 0, 1\\} = 1$$\n    活跃函数是 $f_3(x) = 1-x_1$。我们选择其对应的梯度作为次梯度：\n    $$g^0 = g_3 = \\begin{pmatrix} -1 \\\\ 0 \\end{pmatrix}$$\n\n2.  **第一个凯利切平面**：第一个切平面（切割 1）是利用次梯度不等式构建的：\n    $$t \\ge f(y^0) + g^{0\\top}(x-y^0)$$\n    $$t \\ge 1 + \\begin{pmatrix} -1  & 0 \\end{pmatrix} \\begin{pmatrix} x_1 - 0 \\\\ x_2 - 0 \\end{pmatrix}$$\n    $$t \\ge 1 - x_1$$\n\n3.  **第一个主问题 (LP1)**：我们最小化 $t$，约束条件是第一个切平面和约束 $x \\in X$：\n    $$\n    \\begin{aligned}\n    (LP1) \\quad \\min_{x, t} \\quad  & t \\\\\n    \\text{s.t.} \\quad & t \\ge 1 - x_1 \\\\\n     & -2 \\le x_1 \\le 2 \\\\\n     & -2 \\le x_2 \\le 2\n    \\end{aligned}\n    $$\n\n4.  **LP1 的解**：为了最小化 $t$，我们必须最小化不等式的右侧 $1-x_1$。这可以通过在可行集 $X$ 上最大化 $x_1$ 来实现。最大值为 $x_1=2$。\n    因此 $t$ 的最优值为 $t^1 = 1 - 2 = -1$。\n    $x$ 的最优解集为 $\\{(x_1, x_2) : x_1=2, -2 \\le x_2 \\le 2\\}$。\n    根据平局决胜规则，我们必须选择欧几里得范数 $\\|x\\|_2 = \\sqrt{x_1^2 + x_2^2}$ 最小的解。对于 $x_1=2$，这需要最小化 $\\sqrt{2^2 + x_2^2}$，这在 $x_2=0$ 时发生。\n    因此，下一个迭代点是 $y^1 = (2,0)$，主问题的目标值为 $t^1=-1$。\n\n**迭代 1 ($k=1$)**\n\n我们继续处理点 $y^1 = (2,0)$。\n\n1.  **预言机查询**：我们计算 $f$ 在 $y^1$ 处的值：\n    $$f(y^1) = f(2,0) = \\max\\{2+0, 2(2)-0, 1-2\\} = \\max\\{2, 4, -1\\} = 4$$\n    活跃函数是 $f_2(x) = 2x_1-x_2$。次梯度是：\n    $$g^1 = g_2 = \\begin{pmatrix} 2 \\\\ -1 \\end{pmatrix}$$\n\n2.  **第二个凯利切平面**：第二个切平面（切割 2）是：\n    $$t \\ge f(y^1) + g^{1\\top}(x-y^1)$$\n    $$t \\ge 4 + \\begin{pmatrix} 2  & -1 \\end{pmatrix} \\begin{pmatrix} x_1 - 2 \\\\ x_2 - 0 \\end{pmatrix}$$\n    $$t \\ge 4 + 2(x_1-2) - x_2 = 4 + 2x_1 - 4 - x_2$$\n    $$t \\ge 2x_1 - x_2$$\n\n3.  **第二个主问题 (LP2)**：我们将新的切平面添加到主问题中：\n    $$\n    \\begin{aligned}\n    (LP2) \\quad \\min_{x, t} \\quad  & t \\\\\n    \\text{s.t.} \\quad & t \\ge 1 - x_1 \\quad (\\text{切割 1}) \\\\\n     & t \\ge 2x_1 - x_2 \\quad (\\text{切割 2}) \\\\\n     & -2 \\le x_1 \\le 2, \\quad -2 \\le x_2 \\le 2\n    \\end{aligned}\n    $$\n\n4.  **LP2 的解**：这等价于求解 $\\min_{x \\in X} \\max\\{1-x_1, 2x_1-x_2\\}$。这个逐点最大值函数的最小值将出现在定义域 $X$ 的某个顶点上，或者在 $1-x_1=2x_1-x_2$ 的点上。\n    - 检查 $X = [-2,2]^2$ 的顶点：\n      - $x=(2,2): \\max\\{1-2, 4-2\\} = \\max\\{-1, 2\\}=2$。\n      - $x=(2,-2): \\max\\{1-2, 4-(-2)\\} = \\max\\{-1, 6\\}=6$。\n      - $x=(-2,2): \\max\\{1-(-2), -4-2\\} = \\max\\{3, -6\\}=3$。\n      - $x=(-2,-2): \\max\\{1-(-2), -4-(-2)\\} = \\max\\{3, -2\\}=3$。\n    - 检查 $1-x_1=2x_1-x_2$ 的点，化简为 $x_2=3x_1-1$。在这条线上，最大值函数的值为 $1-x_1$。我们需要找到位于 $X$ 内的点 $(x_1, 3x_1-1)$ 上 $1-x_1$ 的最小值。这意味着要最大化 $x_1$。\n      $X$ 内 $x_2 = 3x_1-1$ 的线段由其与方盒边界的交点界定。\n      - 当 $x_2=2$: $2 = 3x_1-1 \\implies 3x_1=3 \\implies x_1=1$。点是 $(1,2)$。\n      - 当 $x_2=-2$: $-2 = 3x_1-1 \\implies 3x_1=-1 \\implies x_1=-1/3$。点是 $(-1/3, -2)$。\n      该线段上最大的 $x_1$ 是 $x_1=1$。在点 $(1,2)$，目标值为 $1-1=0$。\n    比较所有候选值 $\\{2,6,3,0,1-(-1/3)=4/3\\}$，最小值为 $0$。\n    这个最小值在唯一的点 $(1,2)$ 处取得。\n    因此，解为 $y^2 = (1,2)$ 且 $t^2=0$。\n\n**迭代 2 ($k=2$)**\n\n我们现在使用点 $y^2 = (1,2)$。\n\n1.  **预言机查询**：计算 $f$ 在 $y^2$ 处的值：\n    $$f(y^2) = f(1,2) = \\max\\{1+2, 2(1)-2, 1-1\\} = \\max\\{3, 0, 0\\} = 3$$\n    活跃函数是 $f_1(x) = x_1+x_2$。次梯度是：\n    $$g^2 = g_1 = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$$\n\n2.  **第三个凯利切平面**：第三个切平面（切割 3）是：\n    $$t \\ge f(y^2) + g^{2\\top}(x-y^2)$$\n    $$t \\ge 3 + \\begin{pmatrix} 1  & 1 \\end{pmatrix} \\begin{pmatrix} x_1 - 1 \\\\ x_2 - 2 \\end{pmatrix}$$\n    $$t \\ge 3 + (x_1-1) + (x_2-2) = 3 + x_1 - 1 + x_2 - 2$$\n    $$t \\ge x_1 + x_2$$\n\n3.  **第三个主问题 (LP3)**：添加新的切平面以构成 LP3：\n    $$\n    \\begin{aligned}\n    (LP3) \\quad \\min_{x, t} \\quad  & t \\\\\n    \\text{s.t.} \\quad & t \\ge 1 - x_1 \\quad (\\text{切割 1}) \\\\\n     & t \\ge 2x_1 - x_2 \\quad (\\text{切割 2}) \\\\\n     & t \\ge x_1 + x_2 \\quad (\\text{切割 3}) \\\\\n     & -2 \\le x_1 \\le 2, \\quad -2 \\le x_2 \\le 2\n    \\end{aligned}\n    $$\n\n4.  **LP3 的解**：这等价于求解 $t^3 = \\min_{x \\in X} \\max\\{1 - x_1, 2x_1 - x_2, x_1 + x_2\\}$。\n    函数 $h(x) = \\max\\{1 - x_1, 2x_1 - x_2, x_1 + x_2\\}$ 是凸函数。它在 $\\mathbb{R}^2$ 上的无约束最小值出现在次微分包含零向量的点。这样的一个候选点是三个仿射函数都相等的点：\n    $$1 - x_1 = 2x_1 - x_2 = x_1 + x_2$$\n    从 $2x_1 - x_2 = x_1 + x_2$，我们得到 $x_1 = 2x_2$。\n    将此代入 $1 - x_1 = x_1 + x_2$：\n    $$1 - 2x_2 = 2x_2 + x_2$$\n    $$1 = 5x_2 \\implies x_2 = \\frac{1}{5}$$\n    然后 $x_1 = 2x_2 = 2(\\frac{1}{5}) = \\frac{2}{5}$。\n    我们用第三个等式 $1 - x_1 = 2x_1 - x_2$ 来检查：\n    $1 - \\frac{2}{5} = \\frac{3}{5}$ 且 $2(\\frac{2}{5}) - \\frac{1}{5} = \\frac{4}{5} - \\frac{1}{5} = \\frac{3}{5}$。等式成立。\n    三个函数相等的点是 $x^* = (\\frac{2}{5}, \\frac{1}{5})$。\n    我们检查这个点是否在可行集 $X$ 中：\n    $|x_1^*| = \\frac{2}{5} \\le 2$ 且 $|x_2^*| = \\frac{1}{5} \\le 2$。该点在 $X$ 内部。\n    由于凸函数 $h(x)$ 的无约束最小化子位于可行集 $X$ 内，它也是约束问题的最小化子。\n    x-变量的最优解是 $y^3 = (\\frac{2}{5}, \\frac{1}{5})$。\n    $t$ 的最优值是函数在该点的值：\n    $$t^3 = 1 - x_1 = 1 - \\frac{2}{5} = \\frac{3}{5}$$\n    第三个主问题的最优值是 $t^3 = \\frac{3}{5}$。\n\n结果总结：\n- 切平面 1: $t \\ge 1-x_1$。LP1 解: $(y^1, t^1) = ((2,0), -1)$。\n- 切平面 2: $t \\ge 2x_1-x_2$。LP2 解: $(y^2, t^2) = ((1,2), 0)$。\n- 切平面 3: $t \\ge x_1+x_2$。LP3 解: $(y^3, t^3) = ((\\frac{2}{5}, \\frac{1}{5}), \\frac{3}{5})$。\n问题要求 $t^3$ 的精确值。", "answer": "$$\\boxed{\\frac{3}{5}}$$", "id": "3141056"}, {"introduction": "掌握了基本流程后，我们来探讨一个更深入的问题：切平面是如何生成的，以及该方法的效率瓶颈在哪里。本练习将要求您为一类在机器学习和统计学中极为常见的函数 $f(x) = \\|A x - b\\|_{2}$ 推导一个具体的切平面。更重要的是，它将引导您思考为何对于具有光滑曲面“上图”(epigraph)的函数，纯粹的线性逼近会导致收敛缓慢，从而深化您对一阶方法局限性的认识。[@problem_id:3141057]", "problem": "考虑凸函数 $f(x) = \\|A x - b\\|_{2}$，其中 $A \\in \\mathbb{R}^{2 \\times 2}$，$b \\in \\mathbb{R}^{2}$，$x \\in \\mathbb{R}^{2}$。$f$ 的上图 (epigraph) 是集合 $\\{(x,t) \\in \\mathbb{R}^{2} \\times \\mathbb{R} \\mid t \\ge f(x)\\}$。凯利切平面法构造线性下估计量（切分），这些下估计量在先前的迭代点处支撑 $f$ 的上图。在点 $x_{k}$ 处的一个有效切分是根据 $f$ 在 $x_{k}$ 处的次梯度构建的，并且必须是上图在点 $(x_{k}, f(x_{k}))$ 处的支撑超平面。设\n$$\nA = \\begin{bmatrix} 2  -1 \\\\ 0  1 \\end{bmatrix}, \\quad b = \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix}, \\quad x_{k} = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}.\n$$\n假设残差 $r_{k} = A x_{k} - b$ 非零，并且给定向量 $s_{k} = A^{\\top}(A x_{k} - b)$。请选择为函数 $f(x) = \\|A x - b\\|_{2}$ 在 $x_{k}$ 处提供有效凯利切平面，并正确解释为什么当 $f$ 的上图是光滑曲线时，仅使用线性切分会收敛缓慢的选项。\n\nA. $t \\ge \\sqrt{5} + \\frac{-4 x_{1} + x_{2} - 1}{\\sqrt{5}}$。线性切分在光滑弯曲的上图上可能收敛缓慢，因为它们只捕捉到一阶（切线）信息；一个分段线性的下估计量需要许多切分才能很好地近似曲率，导致每次迭代的改进很小。\n\nB. $t \\ge \\sqrt{5} + (-4 x_{1} + x_{2} - 1)$。线性切分在光滑上图上收敛很快，因为高曲率保证了每增加一个平面都能迈出很大一步。\n\nC. $t \\ge -\\frac{4}{\\sqrt{5}} x_{1} + \\frac{1}{\\sqrt{5}} x_{2}$。此处的线性切分收敛缓慢，因为 $\\|A x - b\\|_{2}$ 不是凸函数，所以平面无法支撑其上图。\n\nD. $t \\ge \\sqrt{5} - \\frac{-4 x_{1} + x_{2} - 1}{\\sqrt{5}}$。线性切分收敛缓慢，因为范数处处不可微，从而妨碍了有效的切线近似。", "solution": "问题要求针对给定的凸函数和点，推导出一个有效的凯利切平面，并评估该方法的性能特点。\n\n首先，我们建立凯利切平面的理论基础。对于一个凸函数 $f(x)$，其上图是 $\\mathbb{R}^{n} \\times \\mathbb{R}$ 中满足 $t \\ge f(x)$ 的点 $(x, t)$ 的集合。在迭代点 $x_k$ 处的一个凯利切平面是一个线性不等式，它定义了上图在点 $(x_k, f(x_k))$ 处的支撑超平面。这由函数的一阶近似给出：\n$$\nt \\ge f(x_k) + g_k^\\top (x - x_k)\n$$\n其中 $g_k$ 是 $f$ 在 $x_k$ 处的任意次梯度，即 $g_k \\in \\partial f(x_k)$。\n\n问题给出了函数 $f(x) = \\|A x - b\\|_{2}$ 和具体数值：\n$$\nA = \\begin{bmatrix} 2  -1 \\\\ 0  1 \\end{bmatrix}, \\quad b = \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix}, \\quad x_{k} = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}.\n$$\n我们来计算凯利切平面不等式所需的各个组成部分。\n\n1.  **计算 $f(x_k)$：**\n    在 $x_k$ 处的残差是 $r_k = A x_k - b$。\n    $$\n    A x_k = \\begin{bmatrix} 2  -1 \\\\ 0  1 \\end{bmatrix} \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} (2)(0) + (-1)(1) \\\\ (0)(0) + (1)(1) \\end{bmatrix} = \\begin{bmatrix} -1 \\\\ 1 \\end{bmatrix}.\n    $$\n    $$\n    r_k = A x_k - b = \\begin{bmatrix} -1 \\\\ 1 \\end{bmatrix} - \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix} = \\begin{bmatrix} -2 \\\\ -1 \\end{bmatrix}.\n    $$\n    题目陈述假设此残差非零，事实也确实如此。现在我们可以计算函数值：\n    $$\n    f(x_k) = \\|A x_k - b\\|_{2} = \\left\\| \\begin{bmatrix} -2 \\\\ -1 \\end{bmatrix} \\right\\|_{2} = \\sqrt{(-2)^2 + (-1)^2} = \\sqrt{4 + 1} = \\sqrt{5}.\n    $$\n\n2.  **计算一个次梯度 $g_k \\in \\partial f(x_k)$：**\n    函数 $f(x) = \\|A x - b\\|_{2}$ 是欧几里得范数函数 $g(z) = \\|z\\|_2$ 和仿射函数 $h(x) = Ax-b$ 的复合。次微分的链式法则表明 $\\partial f(x) = A^\\top \\partial g(h(x))$。\n    欧几里得范数 $\\|z\\|_2$ 的次微分在 $z \\neq 0$ 时是 $\\{\\frac{z}{\\|z\\|_2}\\}$，在 $z = 0$ 时是闭单位球 $\\{u \\mid \\|u\\|_2 \\le 1\\}$。\n    由于 $A x_k - b = \\begin{bmatrix} -2 \\\\ -1 \\end{bmatrix} \\neq 0$，函数 $f$ 在 $x_k$ 处是可微的。其次微分 $\\partial f(x_k)$ 只包含一个元素，即梯度 $\\nabla f(x_k)$。\n    $$\n    g_k = \\nabla f(x_k) = A^\\top \\frac{A x_k - b}{\\|A x_k - b\\|_{2}}.\n    $$\n    我们有 $A x_k - b = \\begin{bmatrix} -2 \\\\ -1 \\end{bmatrix}$ 和 $\\|A x_k - b\\|_{2} = \\sqrt{5}$。$A$ 的转置是：\n    $$\n    A^\\top = \\begin{bmatrix} 2  0 \\\\ -1  1 \\end{bmatrix}.\n    $$\n    所以，次梯度是：\n    $$\n    g_k = \\begin{bmatrix} 2  0 \\\\ -1  1 \\end{bmatrix} \\frac{1}{\\sqrt{5}} \\begin{bmatrix} -2 \\\\ -1 \\end{bmatrix} = \\frac{1}{\\sqrt{5}} \\begin{bmatrix} (2)(-2) + (0)(-1) \\\\ (-1)(-2) + (1)(-1) \\end{bmatrix} = \\frac{1}{\\sqrt{5}} \\begin{bmatrix} -4 \\\\ 1 \\end{bmatrix}.\n    $$\n    注意，题目给出了向量 $s_k = A^\\top(A x_k - b)$，即 $\\begin{bmatrix} -4 \\\\ 1 \\end{bmatrix}$。因此，次梯度是 $g_k = s_k / f(x_k)$。\n\n3.  **构建凯利切平面不等式：**\n    将计算出的值代入一般形式 $t \\ge f(x_k) + g_k^\\top (x - x_k)$：\n    $$\n    t \\ge \\sqrt{5} + \\left( \\frac{1}{\\sqrt{5}} \\begin{bmatrix} -4 \\\\ 1 \\end{bmatrix} \\right)^\\top \\left( \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix} - \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} \\right)\n    $$\n    $$\n    t \\ge \\sqrt{5} + \\frac{1}{\\sqrt{5}} \\begin{bmatrix} -4  & 1 \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 - 1 \\end{bmatrix}\n    $$\n    $$\n    t \\ge \\sqrt{5} + \\frac{1}{\\sqrt{5}} (-4x_1 + (x_2 - 1))\n    $$\n    $$\n    t \\ge \\sqrt{5} + \\frac{-4x_1 + x_2 - 1}{\\sqrt{5}}\n    $$\n    这就是在 $x_k$ 处的有效凯利切平面。\n\n现在，我们评估每个选项。\n\n**A. $t \\ge \\sqrt{5} + \\frac{-4 x_{1} + x_{2} - 1}{\\sqrt{5}}$。线性切分在光滑弯曲的上图上可能收敛缓慢，因为它们只捕捉到一阶（切线）信息；一个分段线性的下估计量需要许多切分才能很好地近似曲率，导致每次迭代的改进很小。**\n-   **切分公式**：该不等式与我们推导的结果完全匹配。\n-   **解释**：这个解释是正确的。凯利切平面法用一个由半空间（切分）交集形成的多面体集从下方近似凸上图。对于具有光滑弯曲上图的函数，例如 $f(x) = \\|Ax-b\\|_2$，每个线性切分仅在一个点上与上图相切。线性下估计量与实际函数之间的差距在远离切点时呈二次方增长。因此，需要许多切分来创建对曲面的合理精确近似，这意味着算法通常步长很小，收敛缓慢。\n-   **结论**：**正确**。\n\n**B. $t \\ge \\sqrt{5} + (-4 x_{1} + x_{2} - 1)$。线性切分在光滑上图上收敛很快，因为高曲率保证了每增加一个平面都能迈出很大一步。**\n-   **切分公式**：这不正确。包含 $x_1$ 和 $x_2$ 的项应该除以 $\\sqrt{5}$。这个公式错误地使用了 $s_k = A^\\top(A x_k - b)$ 作为次梯度，而不是正确归一化的 $g_k = s_k / f(x_k)$。\n-   **解释**：这个解释不正确。高曲率恰恰是导致线性近似效果差、并使得像凯利法这样的一阶方法收敛缓慢的原因。\n-   **结论**：**不正确**。\n\n**C. $t \\ge -\\frac{4}{\\sqrt{5}} x_{1} + \\frac{1}{\\sqrt{5}} x_{2}$。此处的线性切分收敛缓慢，因为 $\\|A x - b\\|_{2}$ 不是凸函数，所以平面无法支撑其上图。**\n-   **切分公式**：这不正确。它缺少常数项 $f(x_k) - g_k^\\top x_k$。完整的不等式是 $t \\ge f(x_k) + g_k^\\top(x-x_k) = (f(x_k) - g_k^\\top x_k) + g_k^\\top x$。常数项计算为 $\\sqrt{5} - \\frac{1}{\\sqrt{5}}(-4(0) + 1(1)) = \\sqrt{5} - \\frac{1}{\\sqrt{5}} = \\frac{4}{\\sqrt{5}}$。所以完整的不等式应该是 $t \\ge \\frac{4}{\\sqrt{5}} + \\frac{-4x_1+x_2}{\\sqrt{5}}$。所提供的公式不完整。\n-   **解释**：这个解释是根本错误的。函数 $f(x) = \\|A x - b\\|_{2}$ 是一个众所周知的凸函数例子。它是一个凸函数（欧几里得范数）与一个仿射函数的复合。凯利方法的整个理论基础都建立在函数是凸函数这一事实上，这保证了基于次梯度的切平面是有效的全局下估计量。\n-   **结论**：**不正确**。\n\n**D. $t \\ge \\sqrt{5} - \\frac{-4 x_{1} + x_{2} - 1}{\\sqrt{5}}$。线性切分收敛缓慢，因为范数处处不可微，从而妨碍了有效的切线近似。**\n-   **切分公式**：这不正确。$f(x_k)$ 项和次梯度项之间的符号是错误的。支撑超平面不等式是 $t \\ge f(x_k) + g_k^\\top(x-x_k)$，而不是 $t \\ge f(x_k) - g_k^\\top(x-x_k)$。\n-   **解释**：这个解释不正确。欧几里得范数 $\\|z\\|_2$ 除了在 $z=0$ 处，在其他任何地方都是可微的。“范数处处不可微”的说法是错误的。在点 $x_k$ 处，函数是可微的，因为 $A x_k - b \\neq 0$。收敛缓慢是由于线性近似对于曲线函数质量不佳，而不是普遍缺乏可微性。\n-   **结论**：**不正确**。\n\n只有选项 A 同时提供了凯利切平面的正确数学公式和对该方法性能的正确概念性解释。", "answer": "$$\\boxed{A}$$", "id": "3141057"}, {"introduction": "将理论付诸代码是检验和巩固学习成果的最终环节。这个综合性练习要求您设计并实现一个完整的计算实验，用以比较凯利切平面法和投影梯度下降法在求解一个光滑凸优化问题时的表现。通过编程实现算法、追踪关键性能指标（如模型包络误差和与最优解的距离），您将不再仅仅是理论的旁观者，而是能够通过经验数据来量化和评判算法性能的实践者。[@problem_id:3141092]", "problem": "设计并实现一个实验，以比较在有界凸集上解决凸最小化问题的两种迭代方法：凯利切平面法和投影梯度下降法，两者作用于同一个可微凸目标函数。目标函数为 $f:\\mathbb{R}^n\\to\\mathbb{R}$，定义如下\n$$\nf(x) \\equiv \\sqrt{1+\\lVert x\\rVert_2^2},\n$$\n该函数是凸且平滑的，并在原点 $x^\\star = 0$ 处取得其唯一最小值，值为 $f(x^\\star)=1$。可行集是一个超立方体（盒子）\n$$\n\\mathcal{X} \\equiv \\{x\\in\\mathbb{R}^n : \\lVert x\\rVert_\\infty \\le R\\}.\n$$\n\n您的程序必须实现这两种方法，并在每次迭代中测量源于凸优化基本原理的两个量：(i) 基于支撑超平面下模型的模型包络误差，以及 (ii) 迭代点到最小化点的欧氏距离。\n\n使用的基本原理：\n- 凸性的一阶特征表明，对于一个可微凸函数，对于任意 $x,y\\in\\mathbb{R}^n$，支撑超平面不等式成立：\n$$\nf(y) \\ge f(x) + \\nabla f(x)^\\top (y-x).\n$$\n- 因此，过去所有支撑超平面的逐点上确界是一个全局下界（也称为切平面模型或多面体下估计器）。如果 $\\{x_i\\}_{i=0}^{k-1}$ 是过去的查询点，其梯度为 $g_i \\equiv \\nabla f(x_i)$，则定义模型\n$$\nm_{k-1}(x) \\equiv \\max_{0\\le i\\le k-1} \\left\\{ f(x_i) + g_i^\\top (x-x_i) \\right\\}.\n$$\n对于任意 $x\\in\\mathbb{R}^n$，我们有 $m_{k-1}(x) \\le f(x)$，并且点 $x$ 相对于模型 $m_{k-1}$ 的包络误差是 $f(x) - m_{k-1}(x) \\ge 0$。\n\n需要实现的算法规范：\n1) 在 $\\mathcal{X}$ 上的凯利切平面法：\n- 初始化：选择 $x_0 \\in \\mathcal{X}$ 并计算 $g_0 \\equiv \\nabla f(x_0)$。初始化切平面集 $\\mathcal{C}_0 \\equiv \\{(x_0,g_0)\\}$。\n- 对于迭代 $t=1,2,\\dots,K$：\n  - 求解线性主问题\n    最小化 $t$，约束条件为\n    $$\n    t \\ge f(x_i) + g_i^\\top (x - x_i) \\quad \\text{对于所有} \\ i\\in\\{0,\\dots,t-1\\}, \\quad x\\in\\mathcal{X}.\n    $$\n    为确保当模型平坦时能够进行确定性的字典序破除平局，在目标函数上增加一个关于 $x$ 的无穷小 $\\ell_1$-正则化项：最小化 $t + \\varepsilon \\lVert x\\rVert_1$，其中 $\\varepsilon = 10^{-9}$。通过引入辅助变量 $u\\in\\mathbb{R}^n$ 并满足 $u_j \\ge |x_j|$（对于 $j=1,\\dots,n$）将其实现为线性规划，因此目标函数为 $t + \\varepsilon \\sum_{j=1}^n u_j$，并受切平面约束和盒子约束 $-R \\le x_j \\le R$ 的限制。\n  - 令最优点为 $x_t$。计算包络误差 $e_t \\equiv f(x_t) - m_{t-1}(x_t)$。计算梯度 $g_t \\equiv \\nabla f(x_t)$ 并将新的切平面 $(x_t,g_t)$ 添加到模型中。\n\n2) 在 $\\mathcal{X}$ 上的投影梯度下降法：\n- 初始化：选择相同或指定的 $x_0 \\in \\mathcal{X}$。初始化切平面集 $\\mathcal{C}_0^{\\text{GD}} \\equiv \\{(x_0,\\nabla f(x_0))\\}$。\n- 对于迭代 $t=1,2,\\dots,K$：\n  - 计算梯度 $g_{t-1} \\equiv \\nabla f(x_{t-1})$，使用步长 $\\alpha>0$ 进行一步梯度下降，并投影回盒子中：\n    $$\n    \\tilde{x}_t \\equiv x_{t-1} - \\alpha g_{t-1}, \\quad\n    x_t \\equiv \\Pi_{\\mathcal{X}}(\\tilde{x}_t),\n    $$\n    其中 $\\Pi_{\\mathcal{X}}$ 表示到 $\\mathcal{X}$ 上的欧氏投影，即在 $[-R,R]$ 上的逐坐标裁剪。\n  - 仅使用来自梯度下降迭代的过去梯度信息，计算包络误差 $e_t^{\\text{GD}} \\equiv f(x_t) - \\max_{0\\le i\\le t-1}\\{ f(x_i) + (\\nabla f(x_i))^\\top (x_t - x_i)\\}$。\n  - 将新的切平面 $(x_t,\\nabla f(x_t))$ 添加到 $\\mathcal{C}_t^{\\text{GD}}$ 中。\n\n对于这两种方法，在每次迭代 $t$ 之后，还要计算到最优点的欧氏距离 $d_t \\equiv \\| x_t - x^\\star\\|_2 = \\| x_t\\|_2$。\n\n您必须遵守的实现细节：\n- 使用精确的目标函数 $f(x)=\\sqrt{1+\\|x\\|_2^2}$ 及其梯度\n$$\n\\nabla f(x) \\equiv \\frac{x}{\\sqrt{1+\\lVert x\\rVert_2^2}}.\n$$\n- 对于凯利法的主问题，按照描述实现带有变量 $(x,t,u)$ 的线性规划，通过辅助变量 $u$ 和线性约束 $x_j \\le u_j$ 及 $-x_j \\le u_j$（对于每个坐标 $j$）来实现目标 $t + \\varepsilon \\|x\\|_1$。\n- 对于梯度下降，使用给定的恒定步长 $\\alpha$ 和由逐坐标裁剪给出的投影 $\\Pi_{\\mathcal{X}}$。\n- 为保证数值稳定性，在计算包络误差时，如果由于数值舍入得到一个小的负值，请将其替换为 $0$。\n\n每个测试用例要报告的指标：\n- 令 $K$ 为迭代次数。对于每种方法，计算累积包络误差 $\\sum_{t=1}^K e_t$ 和最终距离 $d_K$。将这四个数字作为一个列表报告 $[S_{\\text{Kelley}}, S_{\\text{GD}}, D_{\\text{Kelley}}, D_{\\text{GD}}]$，其中 $S$ 表示累积包络误差，D 表示到 $x^\\star$ 的最终距离。将每个报告的数字四舍五入到六位小数。\n\n测试套件：\n在以下四个测试用例上运行您的程序。对于每个案例，维度 $n$、盒子半径 $R$、迭代次数 $K$、梯度下降步长 $\\alpha$ 和初始点 $x_0$ 都已指定。\n\n- 案例 1 (通用二维)：$n=2$, $R=2.0$, $K=10$, $\\alpha=0.9$, $x_0 = (1.0,-1.0)$。\n- 案例 2 (中等维度)：$n=5$, $R=1.0$, $K=12$, $\\alpha=0.8$, $x_0 = (0.5,-0.4,0.3,-0.2,0.1)$。\n- 案例 3 (边界起始，激进步长)：$n=3$, $R=0.5$, $K=8$, $\\alpha=1.0$, $x_0 = (0.5,-0.5,0.5)$。\n- 案例 4 (从最优点起始)：$n=4$, $R=1.5$, $K=5$, $\\alpha=0.9$, $x_0 = (0.0,0.0,0.0,0.0)$。\n\n最终输出格式：\n您的程序应生成单行输出，其中包含一个 Python 风格的列表，该列表按上述案例顺序列出了四个案例各自的结果列表。也就是说，输出必须是以下形式的单行\n\"[ [S_Kelley1,S_GD1,D_Kelley1,D_GD1], [S_Kelley2,S_GD2,D_Kelley2,D_GD2], [S_Kelley3,S_GD3,D_Kelley3,D_GD3], [S_Kelley4,S_GD4,D_Kelley4,D_GD4] ]\"\n每个数字按要求四舍五入到六位小数。", "solution": "用户要求进行一个计算实验，以比较两种用于约束凸优化的基本一阶方法：凯利切平面法和投影梯度下降法。该问题定义明确、科学上合理，且实现所需的所有参数都已提供。任务的核心是实现两种算法，在每次迭代中跟踪指定的性能指标，并报告一系列测试用例的汇总结果。\n\n### 数学公式和原理\n\n问题是在紧凸集 $\\mathcal{X}$ 上最小化一个可微凸函数 $f(x)$。\n$$\n\\min_{x \\in \\mathcal{X}} f(x)\n$$\n具体的目标函数是 $f(x) \\equiv \\sqrt{1+\\|x\\|_2^2}$，可行集是一个超立方体 $\\mathcal{X} \\equiv \\{x\\in\\mathbb{R}^n : \\|x\\|_\\infty \\le R\\}$。函数 $f(x)$ 在 $x^\\star=0$ 处取得最小值，由于 $R>0$，对于所有测试用例，该点都包含在 $\\mathcal{X}$ 中。\n\n目标函数的梯度对两种方法都至关重要，其表达式为：\n$$\n\\nabla f(x) = \\frac{x}{\\sqrt{1+\\|x\\|_2^2}} = \\frac{x}{f(x)}\n$$\n对于 $x=0$，梯度为 $\\nabla f(0) = 0$。\n\n凯利方法和其分析的基石都是凸性的一阶特征，该特征指出，函数在任意点的线性近似提供了一个全局下估计器：\n$$\nf(y) \\ge f(x) + \\nabla f(x)^\\top (y-x) \\quad \\forall x, y \\in \\mathbb{R}^n\n$$\n这个不等式定义了函数 $f$ 在点 $(x, f(x))$ 的图像的支撑超平面。\n\n凯利方法利用这一点，通过在过去的迭代点 $\\{x_i\\}_{i=0}^{t-1}$ 处的支撑超平面，迭代地构建 $f(x)$ 的多面体下模型。模型 $m_{t-1}(x)$ 是这些超平面的逐点最大值：\n$$\nm_{t-1}(x) \\equiv \\max_{0\\le i\\le t-1} \\left\\{ f(x_i) + \\nabla f(x_i)^\\top (x-x_i) \\right\\}\n$$\n下一个迭代点 $x_t$ 被选择为在 $\\mathcal{X}$ 上最小化该模型的点。这等价于求解一个线性规划 (LP)。\n\n另一方面，投影梯度下降法沿着负梯度方向走一步，然后将结果投影回可行集 $\\mathcal{X}$。到超立方体上的投影是一个简单的逐坐标裁剪操作。\n\n两种方法都基于两个指标进行评估：\n1.  **累积包络误差**：$\\sum_{t=1}^K e_t$，其中 $e_t = f(x_t) - m_{t-1}(x_t)$。这衡量了新迭代点 $x_t$ 处的真实函数值超出当前多面体模型预测值的程度。对于凯利方法，$m_{t-1}(x_t)$ 是主问题的最优值，而对于 PGD，我们必须使用迭代历史显式地计算它。较小的误差表明模型是函数的良好近似。\n2.  **到最优点的最终距离**：$d_K = \\| x_K - x^\\star \\|_2 = \\| x_K \\|_2$。这衡量了最终迭代点离真实最小化点的距离。\n\n### 算法实现\n\n**凯利切平面法**\n\n在每次迭代 t 中，我们必须找到 $x_t$，它在 $\\mathcal{X}$ 上最小化模型 $m_{t-1}(x)$，并附加一个正则化项 $\\varepsilon \\|x\\|_1$ 用于破除平局。这被表述为以下线性规划：\n$$\n\\begin{aligned}\n\\min_{x, t_{\\text{var}}, u} \\quad  & t_{\\text{var}} + \\varepsilon \\sum_{j=1}^n u_j \\\\\n\\text{s.t.} \\quad & t_{\\text{var}} \\ge f(x_i) + \\nabla f(x_i)^\\top (x - x_i),  \\forall i \\in \\{0, \\dots, t-1\\} \\\\\n& -R \\le x_j \\le R,  \\forall j \\in \\{1, \\dots, n\\} \\\\\n& -u_j \\le x_j \\le u_j,  \\forall j \\in \\{1, \\dots, n\\} \\\\\n& u_j \\ge 0,  \\forall j \\in \\{1, \\dots, n\\}\n\\end{aligned}\n$$\nLP 求解器的优化变量是 $z = [x_1, \\dots, x_n, t_{\\text{var}}, u_1, \\dots, u_n]$。约束被重排为求解器所需的标准形式 $A_{ub} z \\le b_{ub}$。实现为此目的使用了 `scipy.optimize.linprog`。迭代点 $x_t$ 处的包络误差是 $e_t = f(x_t) - t_{\\text{var}}^*$，其中 $t_{\\text{var}}^*$ 是来自 LP 的上图变量的最优值。\n\n**投影梯度下降 (PGD)**\n\nPGD 的实现更直接。在每次迭代 t，更新规则是：\n1.  $g_{t-1} = \\nabla f(x_{t-1})$\n2.  $\\tilde{x}_t = x_{t-1} - \\alpha g_{t-1}$\n3.  $x_t = \\Pi_{\\mathcal{X}}(\\tilde{x}_t) = \\text{numpy.clip}(\\tilde{x}_t, -R, R)$\n\n对于 PGD，维护迭代 $\\{x_i\\}_{i=0}^{t-1}$ 和梯度 $\\{\\nabla f(x_i)\\}_{i=0}^{t-1}$ 的历史记录，其唯一目的是按规定计算包络误差 $e_t = f(x_t) - m_{t-1}(x_t)$。\n\n该实现将这两种算法封装在不同的函数中。一个主驱动函数遍历指定的测试用例，调用算法实现，收集结果，并将其格式化为所需的单行字符串输出。通过确保计算出的包络误差（可能由于浮点运算而略为负值）被限制在 0，来处理数值稳定性问题。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import linprog\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem, implementing and comparing Kelley's method and PGD.\n    \"\"\"\n\n    def f(x: np.ndarray) -> float:\n        \"\"\"Objective function f(x) = sqrt(1 + ||x||_2^2).\"\"\"\n        return np.sqrt(1.0 + np.dot(x, x))\n\n    def grad_f(x: np.ndarray) -> np.ndarray:\n        \"\"\"Gradient of f(x).\"\"\"\n        val_f = f(x)\n        if val_f == 1.0:  # This happens only if x is a zero vector\n            return np.zeros_like(x, dtype=float)\n        return x / val_f\n\n    def run_kelley(n: int, R: float, K: int, x0: np.ndarray, epsilon: float) -> tuple[float, float]:\n        \"\"\"\n        Implements Kelley's cutting-plane method.\n        Returns: (cumulative_envelope_error, final_distance_to_optimum)\n        \"\"\"\n        x_current = np.array(x0, dtype=float)\n        \n        past_iterates = [x_current]\n        past_gradients = [grad_f(x_current)]\n\n        cumulative_envelope_error = 0.0\n\n        for t_iter in range(1, K + 1):\n            # Build and solve the master LP\n            # Variables: z = [x_1, ..., x_n, t_var, u_1, ..., u_n]\n            # Total variables: n (for x) + 1 (for t_var) + n (for u) = 2n + 1\n            \n            # Objective: minimize t_var + epsilon * sum(u_j)\n            c = np.zeros(2 * n + 1)\n            c[n] = 1.0\n            c[n + 1:] = epsilon\n            \n            # Inequality constraints: A_ub @ z <= b_ub\n            num_cuts = len(past_iterates)\n            num_l1_constraints = 2 * n\n            A_ub = np.zeros((num_cuts + num_l1_constraints, 2 * n + 1))\n            b_ub = np.zeros(num_cuts + num_l1_constraints)\n            \n            # Cutting plane constraints: g_i^T * x - t_var <= g_i^T * x_i - f(x_i)\n            for i in range(num_cuts):\n                x_i, g_i = past_iterates[i], past_gradients[i]\n                f_i = f(x_i)\n                A_ub[i, :n] = g_i\n                A_ub[i, n] = -1.0\n                b_ub[i] = np.dot(g_i, x_i) - f_i\n                \n            # L1 norm constraints: x_j - u_j <= 0 and -x_j - u_j <= 0\n            for j in range(n):\n                idx_x, idx_u = j, n + 1 + j\n                # x_j - u_j <= 0\n                A_ub[num_cuts + j, idx_x] = 1.0\n                A_ub[num_cuts + j, idx_u] = -1.0\n                # -x_j - u_j <= 0\n                A_ub[num_cuts + n + j, idx_x] = -1.0\n                A_ub[num_cuts + n + j, idx_u] = -1.0\n\n            # Bounds for variables\n            bounds = [(-R, R)] * n + [(None, None)] + [(0, None)] * n\n            \n            res = linprog(c, A_ub=A_ub, b_ub=b_ub, bounds=bounds, method='highs')\n            \n            if not res.success:\n                res = linprog(c, A_ub=A_ub, b_ub=b_ub, bounds=bounds, method='interior-point')\n                if not res.success:\n                     raise RuntimeError(f\"Kelley LP failed at iteration {t_iter} with message: {res.message}\")\n\n            x_current = res.x[:n]\n            t_val = res.x[n]\n            \n            # Envelope error e_t = f(x_t) - m_{t-1}(x_t), where m_{t-1}(x_t) is t_val from LP\n            envelope_error = f(x_current) - t_val\n            cumulative_envelope_error += max(0.0, envelope_error)\n\n            # Add new cut for the next iteration\n            past_iterates.append(x_current)\n            past_gradients.append(grad_f(x_current))\n\n        final_distance = np.linalg.norm(x_current)\n        return cumulative_envelope_error, final_distance\n\n    def run_pgd(n: int, R: float, K: int, alpha: float, x0: np.ndarray) -> tuple[float, float]:\n        \"\"\"\n        Implements projected gradient descent.\n        Returns: (cumulative_envelope_error, final_distance_to_optimum)\n        \"\"\"\n        x_current = np.array(x0, dtype=float)\n        \n        past_iterates = [x_current]\n        past_gradients = [grad_f(x_current)]\n\n        cumulative_envelope_error = 0.0\n\n        for t_iter in range(1, K + 1):\n            g_prev = past_gradients[-1]\n            x_prev = past_iterates[-1]\n            \n            x_tilde = x_prev - alpha * g_prev\n            x_current = np.clip(x_tilde, -R, R)\n            \n            # Compute envelope error e_t = f(x_t) - m_{t-1}(x_t)\n            model_val = -np.inf\n            for i in range(len(past_iterates)):\n                x_i, g_i = past_iterates[i], past_gradients[i]\n                f_i = f(x_i)\n                plane_val = f_i + np.dot(g_i, x_current - x_i)\n                if plane_val > model_val:\n                    model_val = plane_val\n            \n            envelope_error = f(x_current) - model_val\n            cumulative_envelope_error += max(0.0, envelope_error)\n\n            # Store new iterate and its gradient for the model\n            past_iterates.append(x_current)\n            past_gradients.append(grad_f(x_current))\n            \n        final_distance = np.linalg.norm(x_current)\n        return cumulative_envelope_error, final_distance\n\n    test_cases = [\n        {'n': 2, 'R': 2.0, 'K': 10, 'alpha': 0.9, 'x0': np.array([1.0, -1.0])},\n        {'n': 5, 'R': 1.0, 'K': 12, 'alpha': 0.8, 'x0': np.array([0.5, -0.4, 0.3, -0.2, 0.1])},\n        {'n': 3, 'R': 0.5, 'K': 8, 'alpha': 1.0, 'x0': np.array([0.5, -0.5, 0.5])},\n        {'n': 4, 'R': 1.5, 'K': 5, 'alpha': 0.9, 'x0': np.array([0.0, 0.0, 0.0, 0.0])},\n    ]\n\n    epsilon = 1e-9\n    all_results = []\n\n    for case in test_cases:\n        s_kelley, d_kelley = run_kelley(case['n'], case['R'], case['K'], case['x0'], epsilon)\n        s_gd, d_gd = run_pgd(case['n'], case['R'], case['K'], case['alpha'], case['x0'])\n        all_results.append([s_kelley, s_gd, d_kelley, d_gd])\n\n    # Format the output string as per requirements\n    formatted_case_results = []\n    for res_list in all_results:\n        num_strs = [f\"{val:.6f}\" for val in res_list]\n        formatted_case_results.append(f\"[{','.join(num_strs)}]\")\n    \n    final_output = f\"[[{','.join(formatted_case_results)}]]\"\n    print(final_output.replace(\" \", \"\"))\n\nsolve()\n```", "id": "3141092"}]}