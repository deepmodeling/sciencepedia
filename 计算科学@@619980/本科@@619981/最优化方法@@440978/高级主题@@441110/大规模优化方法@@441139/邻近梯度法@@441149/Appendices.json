{"hands_on_practices": [{"introduction": "邻近算子是邻近梯度法的核心构件。为了熟练使用该算法，我们首先需要掌握如何计算这些算子。本练习将通过一个基本的二次函数，引导你推导其邻近算子的闭式解，从而帮助你深入理解邻近算子的定义及其计算过程。[@problem_id:2195112]", "problem": "在数值优化领域，近端算子（proximal operator）是用于求解不可微或带约束问题的算法中的一个基本工具。对于一个给定的标量函数 $g(x)$ 和一个正的缩放参数 $\\lambda > 0$，$\\lambda g$ 的近端算子作用于点 $v$ 的结果，被定义为能够最小化一个复合目标函数的 $x$ 的值。\n\n其形式化定义如下：\n$$\n\\text{prox}_{\\lambda g}(v) = \\arg\\min_{x \\in \\mathbb{R}} \\left( g(x) + \\frac{1}{2\\lambda} (x-v)^2 \\right)\n$$\n你的任务是求出一般二次函数的近端算子。考虑函数 $g(x) = \\frac{1}{2}ax^2 + bx$，其中 $a$ 和 $b$ 是实值常数且 $a > 0$。\n\n推导 $\\text{prox}_{\\lambda g}(v)$ 关于参数 $a$、$b$、$v$ 和 $\\lambda$ 的闭式解析表达式。", "solution": "我们需要计算 $g(x)=\\frac{1}{2}a x^{2}+b x$（其中 $a0$ 且 $\\lambda0$）的 $\\text{prox}_{\\lambda g}(v)$。根据定义，\n$$\n\\text{prox}_{\\lambda g}(v)=\\arg\\min_{x\\in\\mathbb{R}}\\left(\\frac{1}{2}a x^{2}+b x+\\frac{1}{2\\lambda}(x-v)^{2}\\right).\n$$\n定义目标函数\n$$\nJ(x)=\\frac{1}{2}a x^{2}+b x+\\frac{1}{2\\lambda}(x-v)^{2}.\n$$\n由于 $a0$ 且 $\\lambda0$，函数 $J$ 是严格凸的，因为它的二阶导数为\n$$\nJ''(x)=a+\\frac{1}{\\lambda}0,\n$$\n所以它有一个由一阶最优性条件 $J'(x)=0$ 所刻画的唯一最小化子。计算其导数：\n$$\nJ'(x)=a x+b+\\frac{1}{\\lambda}(x-v)=(a+\\frac{1}{\\lambda})x+b-\\frac{v}{\\lambda}.\n$$\n令 $J'(x)=0$，解出 $x$：\n$$\n(a+\\frac{1}{\\lambda})x+b-\\frac{v}{\\lambda}=0\n\\;\\;\\Longrightarrow\\;\\;\nx=\\frac{\\frac{v}{\\lambda}-b}{a+\\frac{1}{\\lambda}}.\n$$\n分子和分母同乘以 $\\lambda$ 可得\n$$\nx=\\frac{v-b\\lambda}{1+a\\lambda}.\n$$\n因此，\n$$\n\\text{prox}_{\\lambda g}(v)=\\frac{v-b\\lambda}{1+a\\lambda}.\n$$", "answer": "$$\\boxed{\\frac{v-b\\lambda}{1+a\\lambda}}$$", "id": "2195112"}, {"introduction": "理解了邻近算子后，下一步是将其与梯度下降步骤结合起来，观察邻近梯度法的一次完整迭代。本练习将带你处理一个带非负约束的最小二乘问题，手动执行算法的第一步。通过这个具体例子，你将清楚地看到算法如何通过梯度步骤寻找下降方向，并通过邻近步骤将解投影回可行域。[@problem_id:2195110]", "problem": "考虑一个优化问题，旨在寻找一个点 $x = (x_1, x_2) \\in \\mathbb{R}^2$，以最小化函数 $F(x)$，同时满足其分量的非负约束，即 $x_1 \\ge 0$ 且 $x_2 \\ge 0$。需要最小化的函数是从 $x$ 到目标点 $a$ 的欧几里得距离的平方，由 $F(x) = \\frac{1}{2}\\|x - a\\|_2^2$ 给出。\n\n通过将光滑部分定义为 $f(x) = \\frac{1}{2}\\|x - a\\|_2^2$，非光滑部分 $g(x)$ 定义为非负象限的指示函数，这个问题可以被转化为近端算法的标准形式 $\\min_{x} f(x) + g(x)$。当 $x_1 \\ge 0$ 且 $x_2 \\ge 0$ 时，指示函数 $g(x)$ 的值为零，否则为无穷大。\n\n你的任务是应用近端梯度法来解决这个问题。近端梯度法的迭代更新规则由下式给出：\n$$ x_{k+1} = \\text{prox}_{\\gamma g}(x_k - \\gamma \\nabla f(x_k)) $$\n其中 $\\gamma$ 是步长，$\\text{prox}_{\\gamma g}$ 是与函数 $g$ 相关联的近端算子。\n\n给定目标点 $a = (5, -4)$，初始点 $x_0 = (1, 1)$，以及步长 $\\gamma = 0.2$，请计算下一个迭代点 $x_1$。将你的答案表示为行向量 $(x_{1,1}, x_{1,2})$，其中 $x_{1,1}$ 和 $x_{1,2}$ 是向量 $x_1$ 的分量。", "solution": "我们要优化的目标是在非负象限上最小化 $F(x)=\\frac{1}{2}\\|x-a\\|_{2}^{2}$。在近端梯度分解中，设 $f(x)=\\frac{1}{2}\\|x-a\\|_{2}^{2}$ 且 $g(x)=\\iota_{\\mathbb{R}_{+}^{2}}(x)$，其中 $\\iota_{\\mathbb{R}_{+}^{2}}(x)$ 是可行集 $\\mathbb{R}_{+}^{2}=\\{x\\in\\mathbb{R}^{2}:x_{1}\\ge 0,\\ x_{2}\\ge 0\\}$ 的指示函数。\n\n$f$ 的梯度由下式给出\n$$\n\\nabla f(x)=x-a.\n$$\n$\\gamma g$ 在点 $z$ 处的近端算子是到 $\\mathbb{R}_{+}^{2}$ 上的欧几里得投影：\n$$\n\\text{prox}_{\\gamma g}(z)=\\operatorname*{arg\\,min}_{y\\in\\mathbb{R}^{2}}\\left\\{\\iota_{\\mathbb{R}_{+}^{2}}(y)+\\frac{1}{2\\gamma}\\|y-z\\|_{2}^{2}\\right\\}=P_{\\mathbb{R}_{+}^{2}}(z),\n$$\n也就是按分量在零处截断：\n$$\nP_{\\mathbb{R}_{+}^{2}}(z)=\\big(\\max\\{z_{1},0\\},\\ \\max\\{z_{2},0\\}\\big).\n$$\n\n当 $a=(5,-4)$，$x_{0}=(1,1)$ 且 $\\gamma=0.2$ 时，计算在 $x_{0}$ 处的梯度：\n$$\n\\nabla f(x_{0})=x_{0}-a=(1,1)-(5,-4)=(-4,5).\n$$\n执行梯度步：\n$$\nx_{0}-\\gamma \\nabla f(x_{0})=(1,1)-0.2\\,(-4,5)=(1+0.8,\\ 1-1)=\\left(\\frac{9}{5},\\ 0\\right).\n$$\n应用近端映射，即到 $\\mathbb{R}_{+}^{2}$ 上的投影：\n$$\nx_{1}=\\text{prox}_{\\gamma g}\\big(x_{0}-\\gamma \\nabla f(x_{0})\\big)=P_{\\mathbb{R}_{+}^{2}}\\left(\\frac{9}{5},0\\right)=\\left(\\frac{9}{5},0\\right),\n$$\n因为两个分量都已是非负的。", "answer": "$$\\boxed{\\begin{pmatrix} \\frac{9}{5}  0 \\end{pmatrix}}$$", "id": "2195110"}, {"introduction": "邻近梯度法的收敛速度在很大程度上取决于光滑项梯度的Lipschitz常数 $L$，因为它决定了算法的最大允许步长。本练习旨在探讨一个在实践中至关重要的问题：数据矩阵的列相关性如何影响LASSO问题的Lipschitz常数。通过计算不同相关性下的Lipschitz常数，你将量化地理解为什么高度相关的特征会减慢算法的收敛速度。[@problem_id:2195111]", "problem": "在机器学习中，最小绝对收缩和选择算子 (LASSO) 是一种同时执行变量选择和正则化的回归分析方法。LASSO 优化问题被表述为找到一个系数向量 $x \\in \\mathbb{R}^n$ 来最小化目标函数：\n$$ F(x) = f(x) + g(x) = \\frac{1}{2} \\|Ax - b\\|_2^2 + \\lambda \\|x\\|_1 $$\n其中 $A \\in \\mathbb{R}^{m \\times n}$ 是数据矩阵，$b \\in \\mathbb{R}^m$ 是观测向量，$\\lambda  0$ 是一个正则化参数。\n\n这个问题通常使用近端梯度法来求解，这是一种具有以下更新规则的迭代算法：\n$$ x_{k+1} = \\text{prox}_{\\gamma g}(x_k - \\gamma \\nabla f(x_k)) $$\n其中 $\\gamma  0$ 是步长，$\\text{prox}_{\\gamma g}$ 是函数 $g$ 的近端算子。\n\n近端梯度法的理论收敛速率关键取决于平滑部分梯度 $\\nabla f(x)$ 的利普希茨常数 $L$。为保证收敛，步长 $\\gamma$ 必须满足 $\\gamma  2/L$。一种保守且常见的选择是将步长设置为与 $1/L$ 成正比。因此，较大的 $L$ 值会迫使使用较小的步长，这通常会导致收敛速度变慢。\n\n考虑一个简化的情景，其中数据矩阵 $A \\in \\mathbb{R}^{m \\times 2}$ (对于 $m \\ge 2$) 由两个列向量 $a_1$ 和 $a_2$ 组成。这些列被归一化，使得 $\\|a_1\\|_2 = 1$ 且 $\\|a_2\\|_2 = 1$。列之间的皮尔逊相关系数由 $\\rho = a_1^T a_2$ 给出，我们考虑 $\\rho \\in [0, 1)$ 的情况。\n\n您的任务是量化列相关性 $\\rho$ 如何影响利普希茨常数 $L$。具体来说，计算比率 $L_{0.8} / L_{0.2}$，其中 $L_{\\rho}$ 表示当列相关性为 $\\rho$ 时 $\\nabla f(x)$ 的利普希茨常数。该比率表示当列相关性从 $0.2$ 增加到 $0.8$ 时，最大允许步长必须减小的因子。\n\n请用最简精确分数形式表示您的答案。", "solution": "我们想要求解平滑部分 $f(x) = \\frac{1}{2}\\|Ax - b\\|_{2}^{2}$ 梯度的利普希茨常数。其梯度为\n$$\n\\nabla f(x) = A^{T}(Ax - b) = A^{T}A\\,x - A^{T}b.\n$$\n对于具有海森矩阵 $H = A^{T}A$ 的二次函数 $f$，其梯度是 $L$-利普希茨的，其中\n$$\nL = \\|A^{T}A\\|_{2},\n$$\n由于 $A^{T}A$ 是对称半正定的，该值等于其最大特征值。\n\n当 $A = [a_{1}\\; a_{2}]$，其中 $\\|a_{1}\\|_{2} = \\|a_{2}\\|_{2} = 1$ 且 $\\rho = a_{1}^{T}a_{2} \\in [0,1)$ 时，格拉姆矩阵为\n$$\nA^{T}A = \\begin{pmatrix} 1  \\rho \\\\ \\rho  1 \\end{pmatrix}.\n$$\n特征值满足\n$$\n\\det\\!\\left(\\begin{pmatrix} 1  \\rho \\\\ \\rho  1 \\end{pmatrix} - \\lambda I\\right) = (1 - \\lambda)^{2} - \\rho^{2} = 0,\n$$\n所以\n$$\n\\lambda_{\\pm} = 1 \\pm \\rho.\n$$\n因此，利普希茨常数为\n$$\nL_{\\rho} = \\lambda_{\\max}(A^{T}A) = 1 + \\rho.\n$$\n所要求的比率为\n$$\n\\frac{L_{0.8}}{L_{0.2}} = \\frac{1 + 0.8}{1 + 0.2} = \\frac{1 + \\frac{4}{5}}{1 + \\frac{1}{5}} = \\frac{\\frac{9}{5}}{\\frac{6}{5}} = \\frac{9}{6} = \\frac{3}{2}.\n$$\n因此，当相关性从 $0.2$ 增加到 $0.8$ 时，最大允许步长（与 $1/L$ 成正比）必须按因子 $\\frac{3}{2}$ 减小。", "answer": "$$\\boxed{\\frac{3}{2}}$$", "id": "2195111"}]}