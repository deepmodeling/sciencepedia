{"hands_on_practices": [{"introduction": "为了掌握交替最小化（AM）的核心机制，我们从一个基础的计算练习开始。这个练习将通过一个简单的二次函数，揭示梯度下降步骤与精确块最小化之间的深刻联系。通过亲手推导坐标级 Lipschitz 常数并执行一步“非精确”更新，您会发现对于二次函数，这种更新实际上是精确的，从而为理解更复杂的 AM 行为奠定坚实的数学直觉 [@problem_id:3097264]。", "problem": "考虑一个二阶连续可微函数\n$$\nf(x,y) \\;=\\; \\frac{3}{2}\\,x^{2} \\;+\\; 2\\,x\\,y \\;+\\; \\frac{5}{2}\\,y^{2} \\;-\\; 4\\,x \\;-\\; 6\\,y,\n$$\n其中决策变量为 $x \\in \\mathbb{R}$ 和 $y \\in \\mathbb{R}$。根据定义，如果存在一个常数 $L_{x} \\ge 0$，使得对于所有的 $x, x' \\in \\mathbb{R}$ 和任意固定的 $y \\in \\mathbb{R}$，偏梯度是坐标级利普希茨连续的，\n$$\n\\big\\|\\nabla_{x} f(x,y) - \\nabla_{x} f(x',y)\\big\\| \\;\\le\\; L_{x}\\,|x-x'|,\n$$\n对于固定的 $x$ 和 $y,y'$， $L_{y}$ 的定义类似。\n\n任务：\n1) 从第一性原理出发，推导 $f(x,y)$ 的最小有效坐标级利普希茨常数 $L_{x}$ 和 $L_{y}$。\n2) 使用这些常数，构建一个单步非精确交替最小化（AM）方案，其中 $x$ 的更新是在 $f(\\cdot,y^{k})$ 上进行步长为 $1/L_{x}$ 的一次梯度步，接着 $y$ 的更新是在 $f(x^{k+1},\\cdot)$ 上进行步长为 $1/L_{y}$ 的一次梯度步。从 $(x^{0},y^{0})=(1,0)$ 开始，计算该方案产生的 $(x^{1},y^{1})$。\n3) 对于这个二次函数 $f$，推导在给定 $y$ 时 $x$ 的精确块最小化子（exact block minimizer）以及在给定 $x$ 时 $y$ 的精确块最小化子，并将其与步骤2中获得的非精确AM更新进行简要比较。\n\n将你的最终答案以包含 $(x^{1},y^{1})$ 的行矩阵形式报告。无需四舍五入。", "solution": "该问题提法恰当，具有科学依据且客观。它包含连续优化中的标准任务，涉及一个凸二次函数。所有必要信息均已提供，问题没有矛盾或含糊之处。因此，该问题是有效的，将提供完整的解答。\n\n所考虑的函数为\n$$\nf(x,y) \\;=\\; \\frac{3}{2}\\,x^{2} \\;+\\; 2\\,x\\,y \\;+\\; \\frac{5}{2}\\,y^{2} \\;-\\; 4\\,x \\;-\\; 6\\,y.\n$$\n\n**1) 坐标级利普希茨常数的推导**\n\n坐标级利普希茨连续偏梯度的定义要求，对于一个常数 $L_x \\ge 0$，不等式 $|\\nabla_{x} f(x,y) - \\nabla_{x} f(x',y)| \\le L_{x}\\,|x-x'|$ 对所有 $x, x' \\in \\mathbb{R}$ 和任意固定的 $y \\in \\mathbb{R}$ 成立。由于 $f$ 是二阶连续可微的，最小的此类常数 $L_x$ 由关于 $x$ 的二阶偏导数的绝对值的上确界（supremum）给出。\n\n首先，我们计算关于 $x$ 的偏梯度：\n$$\n\\nabla_{x} f(x,y) = \\frac{\\partial f}{\\partial x} = 3\\,x + 2\\,y - 4.\n$$\n然后，我们计算关于 $x$ 的二阶偏导数：\n$$\n\\frac{\\partial^2 f}{\\partial x^2} = \\frac{\\partial}{\\partial x} (3\\,x + 2\\,y - 4) = 3.\n$$\n最小利普希茨常数 $L_x$ 是该导数在所有 $x$ 和 $y$ 上的绝对值的上确界：\n$$\nL_x = \\sup_{x, y \\in \\mathbb{R}} \\left| \\frac{\\partial^2 f}{\\partial x^2}(x,y) \\right| = |3| = 3.\n$$\n类似地，对于 $y$ 坐标，我们首先计算关于 $y$ 的偏梯度：\n$$\n\\nabla_{y} f(x,y) = \\frac{\\partial f}{\\partial y} = 2\\,x + 5\\,y - 6.\n$$\n然后，我们计算关于 $y$ 的二阶偏导数：\n$$\n\\frac{\\partial^2 f}{\\partial y^2} = \\frac{\\partial}{\\partial y} (2\\,x + 5\\,y - 6) = 5.\n$$\n最小利普希茨常数 $L_y$ 是该导数的绝对值的上确界：\n$$\nL_y = \\sup_{x, y \\in \\mathbb{R}} \\left| \\frac{\\partial^2 f}{\\partial y^2}(x,y) \\right| = |5| = 5.\n$$\n因此，最小的有效坐标级利普希茨常数是 $L_x = 3$ 和 $L_y = 5$。\n\n**2) 非精确交替最小化（AM）步骤**\n\n非精确AM方案由以下更新定义，从 $(x^0, y^0) = (1, 0)$ 开始：\n$$\nx^{k+1} = x^{k} - \\frac{1}{L_x} \\nabla_x f(x^k, y^k)\n$$\n$$\ny^{k+1} = y^{k} - \\frac{1}{L_y} \\nabla_y f(x^{k+1}, y^k)\n$$\n我们计算第一次迭代，即当 $k=0$ 时，以求得 $(x^1, y^1)$。\n\n首先，我们执行 $x$ 的更新。我们需要偏梯度 $\\nabla_x f(x^0, y^0)$：\n$$\n\\nabla_x f(x^0, y^0) = \\nabla_x f(1, 0) = 3(1) + 2(0) - 4 = -1.\n$$\n使用 $L_x = 3$，对 $x^1$ 的更新为：\n$$\nx^{1} = x^{0} - \\frac{1}{L_x} \\nabla_x f(x^0, y^0) = 1 - \\frac{1}{3}(-1) = 1 + \\frac{1}{3} = \\frac{4}{3}.\n$$\n接下来，我们执行 $y$ 的更新。此更新使用新计算出的值 $x^1 = 4/3$ 和之前的值 $y^0 = 0$。我们需要偏梯度 $\\nabla_y f(x^1, y^0)$：\n$$\n\\nabla_y f(x^1, y^0) = \\nabla_y f\\left(\\frac{4}{3}, 0\\right) = 2\\left(\\frac{4}{3}\\right) + 5(0) - 6 = \\frac{8}{3} - 6 = \\frac{8}{3} - \\frac{18}{3} = -\\frac{10}{3}.\n$$\n使用 $L_y = 5$，对 $y^1$ 的更新为：\n$$\ny^{1} = y^{0} - \\frac{1}{L_y} \\nabla_y f(x^1, y^0) = 0 - \\frac{1}{5}\\left(-\\frac{10}{3}\\right) = \\frac{10}{15} = \\frac{2}{3}.\n$$\n因此，该方案一步的结果是 $(x^1, y^1) = \\left(\\frac{4}{3}, \\frac{2}{3}\\right)$。\n\n**3) 精确块最小化子与比较**\n\n为了找到给定固定 $y$ 时 $x$ 的精确块最小化子，记作 $x^*(y)$，我们对 $x$ 求解一阶最优性条件 $\\nabla_x f(x,y) = 0$：\n$$\n3\\,x + 2\\,y - 4 = 0 \\implies 3\\,x = 4 - 2\\,y \\implies x^*(y) = \\frac{4 - 2\\,y}{3}.\n$$\n类似地，为了找到给定固定 $x$ 时 $y$ 的精确块最小化子，记作 $y^*(x)$，我们对 $y$ 求解 $\\nabla_y f(x,y) = 0$：\n$$\n2\\,x + 5\\,y - 6 = 0 \\implies 5\\,y = 6 - 2\\,x \\implies y^*(x) = \\frac{6 - 2\\,x}{5}.\n$$\n现在我们将这些精确最小化子与步骤2中的“非精确”AM更新进行比较。让我们代数地检查 $x$ 更新规则：\n$$\nx^{k+1} = x^k - \\frac{1}{L_x}\\nabla_x f(x^k, y^k) = x^k - \\frac{1}{3}(3x^k + 2y^k - 4) = x^k - x^k - \\frac{2}{3}y^k + \\frac{4}{3} = \\frac{4 - 2y^k}{3}.\n$$\n这个表达式与 $x^*(y^k)$ 完全相同。\n\n让我们检查 $y$ 更新规则：\n$$\ny^{k+1} = y^k - \\frac{1}{L_y}\\nabla_y f(x^{k+1}, y^k) = y^k - \\frac{1}{5}(2x^{k+1} + 5y^k - 6) = y^k - \\frac{2}{5}x^{k+1} - y^k + \\frac{6}{5} = \\frac{6 - 2x^{k+1}}{5}.\n$$\n这个表达式与 $y^*(x^{k+1})$ 完全相同。\n\n比较表明，对于这个特定的二次函数，所规定的“非精确”AM方案等同于精确的交替最小化方法，也称为块坐标下降（Block Coordinate Descent, BCD）。这是因为对于形式为 $g(z) = \\frac{1}{2}Az^2 + Bz + C$（其中 $A > 0$）的一维二次子问题，其梯度 $g'(z)$ 的利普希茨常数是 $L=A$。一个步长为 $1/L$ 的单次梯度下降步骤就能找到精确的最小化子：$z_{k+1} = z_k - \\frac{1}{A}(Az_k + B) = -B/A$。在我们的问题中，$x$ 和 $y$ 的子问题都是二次的，并且所选的步长（$1/L_x$ 和 $1/L_y$）恰好是二阶导数的倒数，这使得每个更新步骤都是对其各自子问题的精确最小化。", "answer": "$$\\boxed{\\begin{pmatrix} \\frac{4}{3}  \\frac{2}{3} \\end{pmatrix}}$$", "id": "3097264"}, {"introduction": "理论上的算法在实践中常常会遇到意想不到的动态行为。本练习通过一个编程任务，让您直面交替最小化方法的一个常见挑战：在目标函数存在狭窄“山谷”时产生的振荡（或称“Z”字形）收敛。您将实现带有阻尼因子的交替最小化算法，通过量化振荡行为和收敛速度，来探索如何通过欠松弛技术有效抑制这种振荡，从而提高算法的实际性能 [@problem_id:3097286]。", "problem": "考虑一个双凸（biconvex）的双变量目标函数：对于其中任一变量的任意固定值，该函数在另一个变量上是凸的。设函数定义为 $$f(x,y) = p x^2 + q y^2 + r x y$$ 其中 $p > 0$，$q > 0$ 且 $r \\in \\mathbb{R}$。对于任意固定的 $y$，该函数在 $x$ 上是凸的，因为 $x^2$ 的系数 $p$ 是严格为正的，并且固定 $y$ 所产生的任何关于 $x$ 的线性项都不会影响其在 $x$ 上的凸性。同样地，对于任意固定的 $x$，该函数在 $y$ 上是凸的，因为 $y^2$ 的系数 $q$ 是严格为正的。\n\n交替最小化（Alternating Minimization, AM）算法定义如下：给定当前迭代点 $(x_k, y_k)$，通过首先对 $f(x,y_k)$ 关于 $x$ 进行最小化以获得 $x_{k+1}$，然后对 $f(x_{k+1}, y)$ 关于 $y$ 进行最小化以获得 $y_{k+1}$，来计算下一个迭代点。为研究导致“之”字形（zig-zagging）行为的窄谷效应，实现带有可选阻尼（欠松弛）因子 AM 算法。该因子由一个标量 $\\lambda \\in (0,1]$ 指定，并应用于每个精确最小化器，具体如下：在计算当前坐标上的精确最小化器后，使用当前值和该最小化器的凸组合进行更新，其中权重为 $\\lambda$。具体而言，使用以下规则：$$x_{k+1} = (1 - \\lambda) x_k + \\lambda \\,\\widehat{x}(y_k), \\quad y_{k+1} = (1 - \\lambda) y_k + \\lambda \\,\\widehat{y}(x_{k+1}),$$ 其中 $\\widehat{x}(y_k)$ 表示 $f(x,y_k)$ 关于 $x$ 的精确最小化器，而 $\\widehat{y}(x_{k+1})$ 表示 $f(x_{k+1},y)$ 关于 $y$ 的精确最小化器。$\\lambda = 1$ 的情况对应于无阻尼的 AM 算法。\n\n您的任务：\n- 从凸性和最小化的基本定义出发，推导函数 $f(x,y)$ 的精确最小化器 $\\widehat{x}(y)$ 和 $\\widehat{y}(x)$。\n- 实现上述带阻尼的 AM 迭代，并对固定步数的迭代过程进行仿真。\n- 通过以下在 $T$ 次迭代中的指标来量化“之”字形行为和收敛性：\n    1. 由带阻尼的 AM 更新所产生的线性迭代矩阵 $M$ 的谱半径 $\\rho(M)$，它控制着此二次模型的线性收敛性。一个较大且接近 1 的 $\\rho(M)$ 表示收敛速度较慢且振荡行为增加，而 $\\rho(M)  1$ 表示收缩。\n    2. 序列 $\\{x_k\\}_{k=0}^{T}$ 中符号变化的次数，这捕捉了跨越窄谷的“之”字形行为。\n    3. 最终欧几里得范数与初始欧几里得范数之比，定义为 $$\\text{ratio} = \\frac{\\sqrt{x_T^2 + y_T^2}}{\\sqrt{x_0^2 + y_0^2}}.$$ 较小的比率表示更强的收缩。\n\n对于每个测试用例，计算无阻尼算法（$\\lambda = 1$）和带阻尼算法（$\\lambda = 0.5$）的这些指标。\n\n使用以下测试套件，每个由 $(p,q,r,x_0,y_0,T)$ 指定：\n- 用例 1（窄谷，对称尺度）：$(p,q,r,x_0,y_0,T) = (1,1,1.9,1,1,30)$。\n- 用例 2（宽谷，对称尺度）：$(p,q,r,x_0,y_0,T) = (1,1,0.5,1,1,30)$。\n- 用例 3（窄谷，不对称尺度）：$(p,q,r,x_0,y_0,T) = (10,0.1,1.9,1,1,30)$。\n\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果。每个测试用例应按 $[\\rho_{\\lambda=1},\\rho_{\\lambda=0.5},\\text{signChanges}_{\\lambda=1},\\text{signChanges}_{\\lambda=0.5},\\text{finalRatio}_{\\lambda=1},\\text{finalRatio}_{\\lambda=0.5}]$ 的顺序贡献一个列表，最终输出应将这三个列表聚合为一个列表，例如：$$[\\,[\\rho_{1},\\rho_{0.5},s_{1},s_{0.5},u_{1},u_{0.5}],\\,[\\dots],\\,[\\dots]\\,].$$ 不涉及物理单位，并且从符号变化中可能推断出的所有角度都纯粹是代数的，不需要单位说明。确保输出中的所有数值都以标准十进制格式表示。", "solution": "该问题要求对一个双凸二次目标函数，分析带有可选阻尼因子的交替最小化（AM）算法。分析内容包括推导迭代的线性算子，计算其谱半径，以及通过仿真算法来衡量收敛速度和振荡行为等性能指标。\n\n目标函数由下式给出\n$$f(x,y) = p x^2 + q y^2 + r x y$$\n其中参数 $p  0$，$q  0$ 且 $r \\in \\mathbb{R}$。此函数是双凸的，意味着当一个变量保持恒定时，它在另一个变量上是凸的。如果该函数是联合凸的（jointly convex），则其全局最小化点为 $(x,y)=(0,0)$，这要求其海森矩阵（Hessian matrix）是半正定的。海森矩阵为 $H = \\begin{pmatrix} 2p  r \\\\ r  2q \\end{pmatrix}$，如果 $2p  0$，$2q  0$ 且 $\\det(H) = 4pq - r^2 \\ge 0$，则其半正定性得到保证。给定的所有测试用例都满足这个更严格的条件，从而确保了唯一的全局最小值。\n\n**1. 精确最小化器的推导**\n\nAM 算法通过每次沿一个坐标轴最小化函数来进行。为了找到固定 $y$ 时的精确最小化器 $\\widehat{x}(y)$，我们求解 $f(x,y)$ 关于 $x$ 的驻点：\n$$ \\frac{\\partial f(x,y)}{\\partial x} = \\frac{\\partial}{\\partial x} (p x^2 + q y^2 + r x y) = 2px + ry = 0 $$\n由于 $f$ 在 $x$ 上是严格凸的（因为 $p  0$），这个驻点是唯一的最小化器：\n$$ \\widehat{x}(y) = -\\frac{r}{2p} y $$\n同样地，对于固定的 $x$，我们通过求解关于 $y$ 的驻点来找到最小化器 $\\widehat{y}(x)$：\n$$ \\frac{\\partial f(x,y)}{\\partial y} = \\frac{\\partial}{\\partial y} (p x^2 + q y^2 + r x y) = 2qy + rx = 0 $$\n由于 $f$ 在 $y$ 上是严格凸的（因为 $q  0$），唯一的最小化器是：\n$$ \\widehat{y}(x) = -\\frac{r}{2q} x $$\n\n**2. 带阻尼的交替最小化迭代**\n\n从迭代点 $(x_k, y_k)$ 到 $(x_{k+1}, y_{k+1})$ 的带阻尼 AM 更新是使用阻尼因子 $\\lambda \\in (0,1]$ 顺序定义的。\n\n首先，$x$ 的更新是当前值 $x_k$ 和精确最小化器 $\\widehat{x}(y_k)$ 的凸组合：\n$$ x_{k+1} = (1 - \\lambda) x_k + \\lambda \\,\\widehat{x}(y_k) = (1 - \\lambda) x_k + \\lambda \\left(-\\frac{r}{2p} y_k\\right) = (1 - \\lambda) x_k - \\frac{\\lambda r}{2p} y_k $$\n\n接下来，$y$ 的更新使用新计算出的 $x_{k+1}$：\n$$ y_{k+1} = (1 - \\lambda) y_k + \\lambda \\,\\widehat{y}(x_{k+1}) = (1 - \\lambda) y_k + \\lambda \\left(-\\frac{r}{2q} x_{k+1}\\right) $$\n为了将其表示为对向量 $z_k = \\begin{pmatrix} x_k \\\\ y_k \\end{pmatrix}$ 的单次线性更新，我们将 $x_{k+1}$ 的表达式代入 $y_{k+1}$ 的方程中：\n$$ y_{k+1} = (1 - \\lambda) y_k - \\frac{\\lambda r}{2q} \\left( (1 - \\lambda) x_k - \\frac{\\lambda r}{2p} y_k \\right) $$\n$$ y_{k+1} = -\\frac{\\lambda r(1 - \\lambda)}{2q} x_k + \\left( (1 - \\lambda) + \\frac{\\lambda^2 r^2}{4pq} \\right) y_k $$\n完整的迭代可以写成矩阵形式 $z_{k+1} = M z_k$：\n$$ \\begin{pmatrix} x_{k+1} \\\\ y_{k+1} \\end{pmatrix} = \\begin{pmatrix} 1-\\lambda  -\\frac{\\lambda r}{2p} \\\\ -\\frac{\\lambda r(1-\\lambda)}{2q}  1-\\lambda + \\frac{\\lambda^2 r^2}{4pq} \\end{pmatrix} \\begin{pmatrix} x_k \\\\ y_k \\end{pmatrix} $$\n因此，迭代矩阵 $M$ 为：\n$$ M = \\begin{pmatrix} 1-\\lambda  -\\frac{\\lambda r}{2p} \\\\ -\\frac{\\lambda r(1-\\lambda)}{2q}  1-\\lambda + \\frac{\\lambda^2 r^2}{4pq} \\end{pmatrix} $$\n\n**3. 迭代矩阵的谱半径**\n\n线性迭代的收敛速度由谱半径 $\\rho(M) = \\max_i |\\mu_i|$ 决定，其中 $\\mu_i$ 是 $M$ 的特征值。特征值是特征方程 $\\det(M - \\mu I) = 0$ 的根。设 $\\kappa = \\frac{r^2}{4pq}$。特征方程为：\n$$ \\mu^2 - \\text{Tr}(M)\\mu + \\det(M) = 0 $$\n矩阵的迹为 $\\text{Tr}(M) = (1-\\lambda) + (1-\\lambda + \\lambda^2 \\kappa) = 2(1-\\lambda) + \\lambda^2 \\kappa$。\n行列式为 $\\det(M) = (1-\\lambda)(1-\\lambda + \\lambda^2 \\kappa) - \\left(-\\frac{\\lambda r}{2p}\\right)\\left(-\\frac{\\lambda r(1-\\lambda)}{2q}\\right) = (1-\\lambda)^2 + \\lambda^2\\kappa(1-\\lambda) - \\lambda^2\\kappa(1-\\lambda) = (1-\\lambda)^2$。\n因此，特征方程是：\n$$ \\mu^2 - \\left( 2(1-\\lambda) + \\lambda^2 \\kappa \\right)\\mu + (1-\\lambda)^2 = 0 $$\n这个关于 $\\mu$ 的二次方程的判别式是 $\\Delta = (2(1-\\lambda) + \\lambda^2 \\kappa)^2 - 4(1-\\lambda)^2 = (\\lambda^2 \\kappa)(4(1-\\lambda) + \\lambda^2 \\kappa)$。由于 $\\lambda \\in (0, 1]$，$1-\\lambda \\ge 0$。又因为 $p,q  0$，所以 $\\kappa \\ge 0$。因此，$\\Delta \\ge 0$，特征值总是实数。\n由于根的乘积 $\\mu_1 \\mu_2 = (1-\\lambda)^2 \\ge 0$ 且根的和 $\\mu_1+\\mu_2 = 2(1-\\lambda) + \\lambda^2 \\kappa \\ge 0$，因此特征值是非负的。\n因此，谱半径是两个特征值中较大的一个：\n$$ \\rho(M) = \\mu_{\\text{max}} = \\frac{ \\left( 2(1-\\lambda) + \\lambda^2 \\kappa \\right) + \\sqrt{ (\\lambda^2 \\kappa)(4(1-\\lambda) + \\lambda^2 \\kappa) } }{2} $$\n对于无阻尼 AM 的特殊情况（$\\lambda=1$），方程得到简化。矩阵变为 $M = \\begin{pmatrix} 0  -r/(2p) \\\\ 0  r^2/(4pq) \\end{pmatrix}$。由于这是一个三角矩阵，其特征值是其对角线元素，即 $\\mu_1 = 0$ 和 $\\mu_2 = \\kappa = \\frac{r^2}{4pq}$。谱半径为 $\\rho(M_{\\lambda=1}) = \\kappa$。\n\n**4. 仿真与性能指标**\n\n该算法将通过从 $(x_0, y_0)$ 开始，对推导出的更新方程进行 $T$ 步迭代来实现。对于每组参数 $(p, q, r)$ 和每个 $\\lambda \\in \\{1, 0.5\\}$ 的选择，计算以下指标：\n\n1.  **谱半径** $\\rho(M)$：使用上面推导的解析公式计算。接近 1 的值表示收敛缓慢。\n2.  **符号变化次数**：生成 x 坐标序列 $\\{x_k\\}_{k=0}^T$。统计在 $k \\in \\{1, \\dots, T\\}$ 范围内，乘积 $x_k x_{k-1}$ 为负的次数。这量化了跨越 $y$ 轴的“之”字形行为。\n3.  **最终与初始范数之比**：计算为 $\\frac{\\sqrt{x_T^2 + y_T^2}}{\\sqrt{x_0^2 + y_0^2}}$，它提供了在 $T$ 次迭代中总收缩程度的度量。\n\n现在将基于这些推导和定义进行实现。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the damped Alternating Minimization problem for the given test cases.\n    \"\"\"\n    test_cases = [\n        # Case 1 (narrow valley, symmetric scales)\n        (1, 1, 1.9, 1, 1, 30),\n        # Case 2 (wide valley, symmetric scales)\n        (1, 1, 0.5, 1, 1, 30),\n        # Case 3 (narrow valley, asymmetric scales)\n        (10, 0.1, 1.9, 1, 1, 30),\n    ]\n\n    lambda_vals = [1.0, 0.5]\n    all_results = []\n\n    for p, q, r, x0, y0, T in test_cases:\n        case_results_interleaved = []\n        \n        # Lists to hold results for lambda=1 and lambda=0.5 to be interleaved later.\n        rhos = []\n        sign_changes_list = []\n        ratios = []\n\n        for lmbda in lambda_vals:\n            # 1. Calculate the spectral radius rho(M)\n            # kappa is the convergence factor for the undamped case\n            kappa = r**2 / (4 * p * q)\n            \n            if lmbda == 1.0:\n                rho = kappa\n            else:\n                one_minus_lmbda = 1.0 - lmbda\n                # Numerator of the eigenvalue formula\n                trace_M = 2 * one_minus_lmbda + lmbda**2 * kappa\n                # Term inside the square root\n                discriminant_term = lmbda**2 * kappa * (4 * one_minus_lmbda + lmbda**2 * kappa)\n                sqrt_term = np.sqrt(discriminant_term)\n                \n                # The spectral radius is the larger, positive eigenvalue\n                rho = (trace_M + sqrt_term) / 2.0\n            \n            rhos.append(rho)\n\n            # 2. Run the simulation\n            x_k, y_k = float(x0), float(y0)\n            x_hist = [x_k]\n            \n            # The simulation uses the direct update rules derived from the problem description\n            for _ in range(T):\n                # First, update x\n                x_hat = -r / (2 * p) * y_k\n                x_kp1 = (1 - lmbda) * x_k + lmbda * x_hat\n                \n                # Then, update y using the new x_kp1\n                y_hat = -r / (2 * q) * x_kp1\n                y_kp1 = (1 - lmbda) * y_k + lmbda * y_hat\n                \n                x_k, y_k = x_kp1, y_kp1\n                x_hist.append(x_k)\n\n            # 3. Quantify metrics\n            # 3a. Count sign changes in the x-coordinate\n            sign_changes = 0\n            for i in range(1, len(x_hist)):\n                if x_hist[i] * x_hist[i-1]  0:\n                    sign_changes += 1\n            sign_changes_list.append(sign_changes)\n\n            # 3b. Calculate final-to-initial norm ratio\n            norm0 = np.sqrt(x0**2 + y0**2)\n            normT = np.sqrt(x_hist[-1]**2 + y_k**2) # y_k is final y\n            \n            ratio = normT / norm0 if norm0 > 0 else 0.0\n            ratios.append(ratio)\n\n        # Interleave results: [rho_1, rho_0.5, sc_1, sc_0.5, r_1, r_0.5]\n        case_results_interleaved = [\n            rhos[0], rhos[1],\n            sign_changes_list[0], sign_changes_list[1],\n            ratios[0], ratios[1]\n        ]\n        all_results.append(case_results_interleaved)\n\n    # Format the final output string as a list of lists.\n    # e.g., [[1,2],[3,4]]\n    outer_list_str = []\n    for res_list in all_results:\n        inner_list_str = f\"[{','.join(map(str, res_list))}]\"\n        outer_list_str.append(inner_list_str)\n    final_output = f\"[{','.join(outer_list_str)}]\"\n    \n    print(final_output)\n\nsolve()\n\n```", "id": "3097286"}, {"introduction": "许多现实世界中的优化问题都带有约束条件，例如非负性或归一化。这个高级编程练习将带您进入一个更贴近实际应用的场景：约束双线性逆问题。您将实现一个交替最小化算法，其中每个子问题都是一个线性最小二乘问题，并通过投影步骤来强制执行概率单纯形约束。通过对比有投影和无投影两种方案，您将深刻体会到在迭代优化中处理约束的关键作用，这是在机器学习和信号处理等领域应用交替最小化的必备技能 [@problem_id:3097289]。", "problem": "给定一个双线性逆问题，其定义如下。设 $A \\in \\mathbb{R}^{m \\times n}$ 是一个已知矩阵，$b \\in \\mathbb{R}^{m}$ 是一个根据双线性传感模型生成的观测向量\n$$\nb \\;=\\; A\\,(x \\odot y),\n$$\n其中 $x \\in \\mathbb{R}^{n}$ 和 $y \\in \\mathbb{R}^{n}$ 是未知向量，满足正性与归一化约束 $x \\ge 0$, $y \\ge 0$, $\\sum_{i=1}^{n} x_i = 1$ 和 $\\sum_{i=1}^{n} y_i = 1$。这里，$\\odot$ 表示逐元素（Hadamard）乘积。目标是通过最小化平方残差来估计 $x$ 和 $y$\n$$\nf(x,y) \\;=\\; \\tfrac{1}{2}\\,\\left\\|A\\big(\\operatorname{diag}(y)\\,x\\big) - b\\right\\|_2^2 \\;=\\; \\tfrac{1}{2}\\,\\left\\|A\\big(x \\odot y\\big) - b\\right\\|_2^2,\n$$\n约束条件为 $x \\ge 0$, $y \\ge 0$, $\\sum_{i=1}^{n} x_i = 1$ 和 $\\sum_{i=1}^{n} y_i = 1$。\n\n从最小二乘法的基本定义和到闭凸集上的欧几里得投影出发，实现交替最小化 (Alternating Minimization) 算法：在固定 $y$ 时对 $x$ 最小化 $f(x,y)$，以及在固定 $x$ 时对 $y$ 最小化 $f(x,y)$，如此交替进行。在每次块更新时，将双线性项线性化以获得一个最小二乘子问题，然后通过到概率单纯形上的欧几里得投影来施加约束\n$$\n\\Delta^n \\;=\\; \\left\\{z \\in \\mathbb{R}^n \\;:\\; z_i \\ge 0 \\text{ for all } i, \\;\\sum_{i=1}^{n} z_i = 1 \\right\\}。\n$$\n为了研究投影的作用，实现并比较两种变体：\n- 变体 $\\mathsf{proj}$：在每次最小二乘块更新后，将更新后的向量投影到 $\\Delta^n$ 上。\n- 变体 $\\mathsf{unproj}$：执行最小二乘块更新，不进行任何投影。\n\n对于每种变体，在固定次数的迭代之后，报告：\n- 最终残差 $r \\;=\\; \\left\\|A\\,(x \\odot y) - b\\right\\|_2$。\n- 总约束违反度 $v$，对于一对 $(x,y)$ 定义为\n$$\nv \\;=\\; \\left(\\sum_{i=1}^{n} \\max\\{0, -x_i\\} \\right) \\;+\\; \\left|\\sum_{i=1}^{n} x_i - 1\\right| \\;+\\; \\left(\\sum_{i=1}^{n} \\max\\{0, -y_i\\} \\right) \\;+\\; \\left|\\sum_{i=1}^{n} y_i - 1\\right|。\n$$\n\n您的程序必须实现这两种变体，并为下述每个测试用例计算指标 $(r, v)$。不涉及物理单位。此问题不涉及角度。\n\n测试套件：\n- 用例 $1$ (理想情况)：$m = 7$, $n = 5$。使用随机种子 $s_A = 0$ 生成具有独立标准正态分布元素的 $A$。使用随机种子 $s_\\star = 1$，通过采样率为 1 的独立指数分布条目来生成 $x^\\star$ 和 $y^\\star$，并将每个向量归一化以使其位于 $\\Delta^n$ 中。设置 $b = A\\,(x^\\star \\odot y^\\star)$。使用 $x^{(0)} = \\tfrac{1}{n}\\,\\mathbf{1}$ 和 $y^{(0)} = \\tfrac{1}{n}\\,\\mathbf{1}$ 初始化算法，并在每种变体中执行 $T = 50$ 次交替更新。\n- 用例 $2$ (边界分量接近零)：$m = 7$, $n = 5$。使用随机种子 $s_A = 1$ 生成具有独立标准正态分布元素的 $A$。选择 $\\epsilon = 10^{-6}$ 并设置\n$$\nx^\\star \\;=\\; \\operatorname{normalize}\\big([\\epsilon,\\, 0.7,\\, 0.299999,\\, \\epsilon,\\, \\epsilon]\\big), \\quad\ny^\\star \\;=\\; \\operatorname{normalize}\\big([0.6,\\, \\epsilon,\\, \\epsilon,\\, 0.4,\\, \\epsilon]\\big),\n$$\n其中 $\\operatorname{normalize}(z)$ 表示 $z/\\sum_i z_i$，以确保其属于 $\\Delta^n$。设置 $b = A\\,(x^\\star \\odot y^\\star)$，使用 $x^{(0)} = \\tfrac{1}{n}\\,\\mathbf{1}$ 和 $y^{(0)} = \\tfrac{1}{n}\\,\\mathbf{1}$ 初始化，并执行 $T = 50$ 次更新。\n- 用例 $3$ (病态设计)：$m = 7$, $n = 5$。使用随机种子 $s_A = 2$ 生成具有独立标准正态分布元素的 $A$，并用对角矩阵 $D = \\operatorname{diag}([1,\\, 10^{-2},\\, 10^{-3},\\, 1,\\, 10^{-4}])$ 缩放其列。使用随机种子 $s_\\star = 3$，通过采样率为 1 的独立指数分布条目来生成 $x^\\star$ 和 $y^\\star$，并将每个向量归一化以使其位于 $\\Delta^n$ 中。设置 $b = A\\,(x^\\star \\odot y^\\star)$，使用 $x^{(0)} = \\tfrac{1}{n}\\,\\mathbf{1}$ 和 $y^{(0)} = \\tfrac{1}{n}\\,\\mathbf{1}$ 初始化，并执行 $T = 50$ 次更新。\n\n实现要求：\n- 在每次固定 $y$ 的 $x$ 更新中，将 $f(x,y)$ 简化为形式为 $\\min_x \\tfrac{1}{2}\\|B\\,x - b\\|_2^2$ 的最小二乘问题，其中矩阵 $B$ 被适当地定义，然后通过最小二乘解更新 $x$；在变体 $\\mathsf{proj}$ 中，接着投影到 $\\Delta^n$ 上，而在变体 $\\mathsf{unproj}$ 中不进行投影。\n- 在每次固定 $x$ 的 $y$ 更新中，类似地简化为 $\\min_y \\tfrac{1}{2}\\|C\\,y - b\\|_2^2$ 的问题，其中矩阵 $C$ 被适当地定义；在变体 $\\mathsf{proj}$ 中，接着进行投影，而在变体 $\\mathsf{unproj}$ 中不进行投影。\n- 使用与上述规定完全一致的确定性随机种子。\n\n最终输出格式：\n您的程序应生成单行输出，按顺序包含所有十二个结果\n$$\n\\big[r^{\\mathrm{proj}}_1,\\, r^{\\mathrm{unproj}}_1,\\, v^{\\mathrm{proj}}_1,\\, v^{\\mathrm{unproj}}_1,\\, r^{\\mathrm{proj}}_2,\\, r^{\\mathrm{unproj}}_2,\\, v^{\\mathrm{proj}}_2,\\, v^{\\mathrm{unproj}}_2,\\, r^{\\mathrm{proj}}_3,\\, r^{\\mathrm{unproj}}_3,\\, v^{\\mathrm{proj}}_3,\\, v^{\\mathrm{unproj}}_3\\big],\n$$\n打印为逗号分隔的列表，并用方括号括起来（例如，$[a,b,c,d,e,f,g,h,i,j,k,l]$），其中每个条目都是一个浮点数。", "solution": "当前的问题是，从通过双线性传感模型 $b = A(x \\odot y)$ 获取的观测向量 $b \\in \\mathbb{R}^m$ 中估计两个未知向量 $x \\in \\mathbb{R}^n$ 和 $y \\in \\mathbb{R}^n$，其中 $A \\in \\mathbb{R}^{m \\times n}$ 是一个已知矩阵，$\\odot$ 表示逐元素乘积。该估计被表述为一个约束优化问题，旨在最小化平方残差，同时对 $x$ 和 $y$ 施加正性和归一化约束。\n\n目标函数由下式给出：\n$$\nf(x,y) = \\tfrac{1}{2}\\,\\left\\|A\\big(x \\odot y\\big) - b\\right\\|_2^2\n$$\n约束要求 $x$ 和 $y$ 都属于概率单纯形 $\\Delta^n$：\n$$\n\\Delta^n = \\left\\{z \\in \\mathbb{R}^n \\mid z_i \\ge 0 \\text{ for all } i, \\text{ and } \\sum_{i=1}^{n} z_i = 1 \\right\\}\n$$\n由于双线性项 $x \\odot y$，目标函数 $f(x,y)$ 是非凸的，这使得寻找全局最小值具有挑战性。该问题指定使用交替最小化 (AM) 算法，这是一种用于解决具有多个变量块问题的常用迭代方法。AM 的过程是固定一组变量，对另一组变量最小化目标函数，然后交换它们的角色。\n\n用于此问题的 AM 算法通过两个主要步骤进行迭代：\n1.  **$x$-最小化**：将 $y$ 固定为其当前估计值 $y^{(k)}$，并求解 $x$。\n2.  **$y$-最小化**：将 $x$ 固定为其新计算的估计值 $x^{(k+1)}$，并求解 $y$。\n\n让我们详细说明每个子问题。\n\n**1. $x$-最小化子问题：**\n当 $y=y^{(k)}$ 保持不变时，目标函数仅是 $x$ 的函数：\n$$\n\\min_x \\tfrac{1}{2}\\,\\left\\|A\\big(x \\odot y^{(k)}\\big) - b\\right\\|_2^2\n$$\n我们可以通过定义一个对角矩阵 $D_y = \\operatorname{diag}(y^{(k)})$，将逐元素乘积表示为矩阵-向量乘积。这样，$x \\odot y^{(k)} = D_y x$。于是该子问题是一个标准的线性最小二乘问题：\n$$\n\\min_x \\tfrac{1}{2}\\,\\left\\| (A D_y) x - b \\right\\|_2^2\n$$\n令 $B_k = A \\operatorname{diag}(y^{(k)})$。无约束最小化解 $x_{LS}$ 是正规方程 $(B_k^T B_k) x = B_k^T b$ 的解。该解的形式由 $x_{LS} = B_k^\\dagger b$ 给出，其中 $B_k^\\dagger$ 是 $B_k$ 的 Moore-Penrose 伪逆。\n\n**2. $y$-最小化子问题：**\n类似地，在获得 $x$ 的更新估计值（我们记为 $x^{(k+1)}$）后，我们将其固定并求解 $y$。该子问题是：\n$$\n\\min_y \\tfrac{1}{2}\\,\\left\\|A\\big(x^{(k+1)} \\odot y\\big) - b\\right\\|_2^2\n$$\n使用恒等式 $x^{(k+1)} \\odot y = \\operatorname{diag}(x^{(k+1)}) y$，我们定义 $D_x = \\operatorname{diag}(x^{(k+1)})$ 并获得另一个线性最小二乘问题：\n$$\n\\min_y \\tfrac{1}{2}\\,\\left\\| (A D_x) y - b \\right\\|_2^2\n$$\n令 $C_{k+1} = A \\operatorname{diag}(x^{(k+1)})$。无约束最小化解是 $y_{LS} = C_{k+1}^\\dagger b$。\n\n该问题要求比较两种处理约束的变体。\n\n**变体 $\\mathsf{unproj}$**：在此变体中，迭代更新期间忽略约束。更新步骤就是无约束的最小二乘解：\n$$\nx^{(k+1)} = \\left(A \\operatorname{diag}(y^{(k)})\\right)^\\dagger b\n$$\n$$\ny^{(k+1)} = \\left(A \\operatorname{diag}(x^{(k+1)})\\right)^\\dagger b\n$$\n约束没有被强制执行，因此最终的迭代结果 $x$ 和 $y$ 不保证位于 $\\Delta^n$ 中。\n\n**变体 $\\mathsf{proj}$**：此变体在每次最小二乘更新后，通过将中间解投影到 $\\Delta^n$ 上来强制执行单纯形约束。步骤如下：\n1.  计算 $x$ 的无约束更新：$x_{LS} = \\left(A \\operatorname{diag}(y^{(k)})\\right)^\\dagger b$。\n2.  投影到单纯形上：$x^{(k+1)} = \\operatorname{proj}_{\\Delta^n}(x_{LS})$。\n3.  计算 $y$ 的无约束更新：$y_{LS} = \\left(A \\operatorname{diag}(x^{(k+1)})\\right)^\\dagger b$。\n4.  投影到单纯形上：$y^{(k+1)} = \\operatorname{proj}_{\\Delta^n}(y_{LS})$。\n\n**到概率单纯形上的欧几里得投影**\n向量 $z \\in \\mathbb{R}^n$ 到 $\\Delta^n$ 上的投影是以下凸优化问题的解：\n$$\n\\operatorname{proj}_{\\Delta^n}(z) = \\arg\\min_{p \\in \\Delta^n} \\tfrac{1}{2} \\|p-z\\|_2^2\n$$\n该问题的解可以被高效地找到。Karush-Kuhn-Tucker (KKT) 条件表明，投影后的向量 $p$ 具有 $p_i = \\max(0, z_i - \\mu)$ 的形式（对于所有 $i=1, \\dots, n$），其中 $\\mu$ 是为满足约束 $\\sum_i p_i = 1$ 而选择的拉格朗日乘子。这意味着我们必须找到一个 $\\mu$ 使得 $\\sum_{i=1}^n \\max(0, z_i - \\mu) = 1$。函数 $g(\\mu) = \\sum_{i=1}^n \\max(0, z_i - \\mu)$ 是连续、分段线性和单调递减的。找到 $g(\\mu) - 1 = 0$ 的根可以得到正确的 $\\mu$。一种有效的算法将 $z$ 的元素按降序排序，即 $z_{(1)} \\ge z_{(2)} \\ge \\dots \\ge z_{(n)}$，并找到一个索引 $\\rho$，使得 $\\mu$ 由前 $\\rho$ 个元素确定。算法如下：\n1. 将 $z$ 排序为 $u$：$u_1 \\ge u_2 \\ge \\dots \\ge u_n$。\n2. 找到 $\\rho = \\max\\left\\{j \\in \\{1, \\dots, n\\} \\mid u_j - \\frac{1}{j}\\left(\\sum_{i=1}^j u_i - 1\\right)  0\\right\\}$。\n3. 计算阈值 $\\theta = \\frac{1}{\\rho}\\left(\\sum_{i=1}^\\rho u_i - 1\\right)$。\n4. 投影的分量是 $p_i = \\max(0, z_i - \\theta)$。\n\n这个过程使我们能够正确地实现变体 $\\mathsf{proj}$。数值实现将使用 `numpy.linalg.lstsq` 来稳定地求解最小二乘子问题，随后是用于变体 $\\mathsf{proj}$ 的投影算法。最后，我们按照规定为每个变体和测试用例计算残差 $r$ 和约束违反度 $v$。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef project_simplex(v: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Projects a vector v onto the probability simplex.\n    \"\"\"\n    n_features = v.shape[0]\n    u = np.sort(v)[::-1]\n    cssv = np.cumsum(u)\n    rho = np.nonzero(u * np.arange(1, n_features + 1) > cssv - 1)[0][-1]\n    theta = (cssv[rho] - 1) / (rho + 1.0)\n    return np.maximum(v - theta, 0)\n\ndef calculate_metrics(A: np.ndarray, b: np.ndarray, x: np.ndarray, y: np.ndarray):\n    \"\"\"\n    Calculates the final residual and constraint violation.\n    \"\"\"\n    # Residual\n    residual_vec = A @ (x * y) - b\n    r = np.linalg.norm(residual_vec)\n\n    # Constraint violation\n    v_x = np.sum(np.maximum(0, -x)) + np.abs(np.sum(x) - 1)\n    v_y = np.sum(np.maximum(0, -y)) + np.abs(np.sum(y) - 1)\n    v = v_x + v_y\n    \n    return r, v\n\ndef run_am_variant(A: np.ndarray, b: np.ndarray, x0: np.ndarray, y0: np.ndarray, T: int, variant: str):\n    \"\"\"\n    Runs an Alternating Minimization variant ('proj' or 'unproj').\n    \"\"\"\n    x, y = x0.copy(), y0.copy()\n    n = A.shape[1]\n\n    for _ in range(T):\n        # x-update\n        # Reformulate as || (A * diag(y)) * x - b ||^2\n        if np.all(np.abs(y)  1e-9): # handle case where y is near zero\n            B = np.zeros_like(A)\n        else:\n            B = A * y.reshape(1, n)\n        \n        x_ls = np.linalg.lstsq(B, b, rcond=None)[0]\n        \n        if variant == 'proj':\n            x = project_simplex(x_ls)\n        else: # unproj\n            x = x_ls\n\n        # y-update\n        # Reformulate as || (A * diag(x)) * y - b ||^2\n        if np.all(np.abs(x)  1e-9):\n            C = np.zeros_like(A)\n        else:\n            C = A * x.reshape(1, n)\n            \n        y_ls = np.linalg.lstsq(C, b, rcond=None)[0]\n        \n        if variant == 'proj':\n            y = project_simplex(y_ls)\n        else: # unproj\n            y = y_ls\n\n    return calculate_metrics(A, b, x, y)\n\n\ndef solve():\n    \"\"\"\n    Main function to run test cases and generate the final output.\n    \"\"\"\n    test_cases = []\n    \n    # Case 1 Setup\n    m, n, s_A, s_star, T = 7, 5, 0, 1, 50\n    rng_A = np.random.default_rng(s_A)\n    A1 = rng_A.standard_normal((m, n))\n    rng_star = np.random.default_rng(s_star)\n    x_star_raw = rng_star.exponential(1.0, size=n)\n    x_star1 = x_star_raw / np.sum(x_star_raw)\n    y_star_raw = rng_star.exponential(1.0, size=n)\n    y_star1 = y_star_raw / np.sum(y_star_raw)\n    b1 = A1 @ (x_star1 * y_star1)\n    x0_1 = np.ones(n) / n\n    y0_1 = np.ones(n) / n\n    test_cases.append({'A': A1, 'b': b1, 'x0': x0_1, 'y0': y0_1, 'T': T})\n\n    # Case 2 Setup\n    m, n, s_A, T = 7, 5, 1, 50\n    rng_A = np.random.default_rng(s_A)\n    A2 = rng_A.standard_normal((m, n))\n    eps = 1e-6\n    x_star_raw = np.array([eps, 0.7, 0.299999, eps, eps])\n    x_star2 = x_star_raw / np.sum(x_star_raw)\n    y_star_raw = np.array([0.6, eps, eps, 0.4, eps])\n    y_star2 = y_star_raw / np.sum(y_star_raw)\n    b2 = A2 @ (x_star2 * y_star2)\n    x0_2 = np.ones(n) / n\n    y0_2 = np.ones(n) / n\n    test_cases.append({'A': A2, 'b': b2, 'x0': x0_2, 'y0': y0_2, 'T': T})\n\n    # Case 3 Setup\n    m, n, s_A, s_star, T = 7, 5, 2, 3, 50\n    rng_A = np.random.default_rng(s_A)\n    A_raw = rng_A.standard_normal((m, n))\n    D = np.diag([1.0, 1e-2, 1e-3, 1.0, 1e-4])\n    A3 = A_raw @ D\n    rng_star = np.random.default_rng(s_star)\n    x_star_raw = rng_star.exponential(1.0, size=n)\n    x_star3 = x_star_raw / np.sum(x_star_raw)\n    y_star_raw = rng_star.exponential(1.0, size=n)\n    y_star3 = y_star_raw / np.sum(y_star_raw)\n    b3 = A3 @ (x_star3 * y_star3)\n    x0_3 = np.ones(n) / n\n    y0_3 = np.ones(n) / n\n    test_cases.append({'A': A3, 'b': b3, 'x0': x0_3, 'y0': y0_3, 'T': T})\n\n    results = []\n    for case in test_cases:\n        A, b, x0, y0, T_iter = case['A'], case['b'], case['x0'], case['y0'], case['T']\n        \n        # Run proj variant\n        r_proj, v_proj = run_am_variant(A, b, x0, y0, T_iter, 'proj')\n        \n        # Run unproj variant\n        r_unproj, v_unproj = run_am_variant(A, b, x0, y0, T_iter, 'unproj')\n        \n        results.extend([r_proj, r_unproj, v_proj, v_unproj])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{x:.6f}' for x in results)}]\")\n\nsolve()\n```", "id": "3097289"}]}