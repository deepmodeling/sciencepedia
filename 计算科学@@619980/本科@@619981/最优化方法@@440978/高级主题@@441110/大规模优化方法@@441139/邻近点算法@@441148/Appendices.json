{"hands_on_practices": [{"introduction": "在许多机器学习和信号处理问题中，$L_1$ 范数因其能够诱导稀疏性而扮演着核心角色。本练习将指导你从第一性原理出发，通过求解邻近算子 (proximal operator) 的优化问题，推导出与 $L_1$ 范数相关的著名“软阈值”算子。掌握这一推导过程是理解和应用邻近点算法 (Proximal Point Algorithm) 的基石 [@problem_id:3168299]。", "problem": "设 $f:\\mathbb{R}^n\\to\\mathbb{R}$ 是一个闭真凸函数，并回顾在 $y\\in\\mathbb{R}^n$ 处 $f$ 的邻近算子的定义，即强凸问题 $\\min_{x\\in\\mathbb{R}^n}\\left\\{f(x)+\\frac{1}{2}\\|x-y\\|_2^2\\right\\}$ 的唯一极小化子 $x^{\\star}$。该问题的一阶最优性条件是单调包含关系 $0\\in \\partial f(x^{\\star})+x^{\\star}-y$，这与在次微分算子 $\\partial f$ 上应用邻近点算法（PPA）的单个隐式步骤相吻合。设 $f_{\\lambda}(x)=\\lambda\\|x\\|_1$（其中 $\\lambda0$），$\\|x\\|_1=\\sum_{i=1}^n |x_i|$，并考虑通过求解包含关系 $0\\in \\lambda\\,\\partial\\|x\\|_1 + x - y$ 来计算 $\\operatorname{prox}_{f_{\\lambda}}(y)$。\n\n首先，从绝对值的次微分刻画出发，显式推导包含关系 $0\\in \\lambda\\,\\partial\\|x\\|_1 + x - y$ 的逐坐标解，从而获得 $\\operatorname{prox}_{\\lambda\\|x\\|_1}(y)$ 关于 $y$ 和 $\\lambda$ 的闭式表达式。其次，考虑扰动函数 $f_{\\lambda,\\varepsilon}(x)=\\lambda\\|x\\|_1 + \\frac{\\varepsilon}{2}\\|x\\|_2^2$（其中 $\\varepsilon0$），并再次应用 PPA 最优性条件，推导 $\\operatorname{prox}_{f_{\\lambda,\\varepsilon}}(y)$ 的显式逐坐标表达式。\n\n现在取具体数据 $y\\in\\mathbb{R}^3$，$y=\\big(\\frac{3}{2},-\\frac{1}{3},\\frac{4}{5}\\big)$，参数 $\\lambda=\\frac{1}{2}$ 和 $\\varepsilon=\\frac{1}{4}$。计算 $\\operatorname{prox}_{\\lambda\\|x\\|_1}(y)$ 和 $\\operatorname{prox}_{f_{\\lambda,\\varepsilon}}(y)$，然后计算这两个邻近点之间的欧几里得距离的平方，即 $\\|\\operatorname{prox}_{\\lambda\\|x\\|_1}(y)-\\operatorname{prox}_{f_{\\lambda,\\varepsilon}}(y)\\|_2^2$ 的值。请以精确有理数的形式给出最终答案。不需要四舍五入，也不涉及物理单位。", "solution": "该问题要求推导两个邻近算子并进行后续的数值计算。整个过程分为三个主要部分：第一，推导函数 $f_{\\lambda}(x)=\\lambda\\|x\\|_1$ 的邻近算子；第二，推导扰动函数 $f_{\\lambda,\\varepsilon}(x)=\\lambda\\|x\\|_1 + \\frac{\\varepsilon}{2}\\|x\\|_2^2$ 的邻近算子；第三，针对给定的具体数据，计算这两个邻近点之间的欧几里得距离的平方。\n\n问题陈述已经过验证，被认为是具有科学依据、适定且客观的。所有必要信息均已提供，该问题是凸优化中的一个标准练习题。\n\n第一部分：$\\operatorname{prox}_{\\lambda\\|\\cdot\\|_1}(y)$ 的推导\n\n函数 $f_{\\lambda}(x) = \\lambda\\|x\\|_1$ 的邻近算子定义为以下极小化问题的唯一解：\n$$ x^{(1)} = \\operatorname{prox}_{\\lambda\\|\\cdot\\|_1}(y) = \\arg\\min_{x\\in\\mathbb{R}^n}\\left\\{ \\lambda\\|x\\|_1 + \\frac{1}{2}\\|x-y\\|_2^2 \\right\\} $$\n目标函数是可分的，意味着它可以写成关于 $x$ 各个分量的函数之和：\n$$ \\sum_{i=1}^n \\left( \\lambda|x_i| + \\frac{1}{2}(x_i - y_i)^2 \\right) $$\n因此，我们可以通过求解 $n$ 个独立的标量极小化问题来找到极小化子。对于第 $i$ 个分量 $x_i$，其一阶最优性条件由以下包含关系给出：\n$$ 0 \\in \\lambda\\,\\partial|x_i| + x_i - y_i $$\n这可以重写为 $y_i - x_i \\in \\lambda\\,\\partial|x_i|$。绝对值函数 $g(t)=|t|$ 的次微分是：\n$$ \\partial|t| = \\begin{cases} \\{1\\}  \\text{if } t  0 \\\\ \\{-1\\}  \\text{if } t  0 \\\\ [-1, 1]  \\text{if } t = 0 \\end{cases} $$\n我们根据解 $x_i$ 的符号来分析此包含关系：\n情况1：$x_i  0$。次微分为 $\\partial|x_i| = \\{1\\}$。包含关系变为 $y_i - x_i = \\lambda$，这意味着 $x_i = y_i - \\lambda$。这仅在 $y_i - \\lambda  0$ 时成立，即 $y_i  \\lambda$。\n情况2：$x_i  0$。次微分为 $\\partial|x_i| = \\{-1\\}$。包含关系变为 $y_i - x_i = -\\lambda$，这意味着 $x_i = y_i + \\lambda$。这仅在 $y_i + \\lambda  0$ 时成立，即 $y_i  -\\lambda$。\n情况3：$x_i = 0$。次微分为 $\\partial|x_i| = [-1, 1]$。包含关系变为 $y_i - 0 \\in \\lambda[-1, 1]$，简化为 $y_i \\in [-\\lambda, \\lambda]$，或 $|y_i| \\le \\lambda$。\n\n综合这三种情况，逐坐标解为：\n$$ x_i^{(1)} = \\begin{cases} y_i - \\lambda  \\text{if } y_i  \\lambda \\\\ 0  \\text{if } |y_i| \\le \\lambda \\\\ y_i + \\lambda  \\text{if } y_i  -\\lambda \\end{cases} $$\n该算子被称为软阈值算子，可以紧凑地写为 $x_i^{(1)} = S_{\\lambda}(y_i) = \\operatorname{sgn}(y_i) \\max(|y_i| - \\lambda, 0)$。因此，$\\operatorname{prox}_{\\lambda\\|\\cdot\\|_1}(y) = S_{\\lambda}(y)$，其中算子是逐分量应用的。\n\n第二部分：$\\operatorname{prox}_{f_{\\lambda,\\varepsilon}}(y)$ 的推导\n\n设 $x^{(2)} = \\operatorname{prox}_{f_{\\lambda,\\varepsilon}}(y)$。函数为 $f_{\\lambda,\\varepsilon}(x)=\\lambda\\|x\\|_1 + \\frac{\\varepsilon}{2}\\|x\\|_2^2$。$x^{(2)}$ 的最优性条件是：\n$$ 0 \\in \\partial f_{\\lambda,\\varepsilon}(x^{(2)}) + x^{(2)} - y $$\n使用次微分的和法则（因为 $\\frac{\\varepsilon}{2}\\|x\\|_2^2$ 是连续可微的），我们有 $\\partial f_{\\lambda,\\varepsilon}(x) = \\lambda\\,\\partial\\|x\\|_1 + \\varepsilon x$。包含关系变为：\n$$ 0 \\in \\lambda\\,\\partial\\|x^{(2)}\\|_1 + \\varepsilon x^{(2)} + x^{(2)} - y = \\lambda\\,\\partial\\|x^{(2)}\\|_1 + (1+\\varepsilon)x^{(2)} - y $$\n这个问题也是可分的。逐坐标的包含关系是：\n$$ y_i - (1+\\varepsilon)x_i^{(2)} \\in \\lambda\\,\\partial|x_i^{(2)}| $$\n我们用与之前相同的方式分析这个包含关系：\n情况1：$x_i^{(2)}  0$。则 $\\partial|x_i^{(2)}| = \\{1\\}$，所以 $y_i - (1+\\varepsilon)x_i^{(2)} = \\lambda$。解出 $x_i^{(2)}$ 得 $x_i^{(2)} = \\frac{y_i - \\lambda}{1+\\varepsilon}$。这在 $y_i - \\lambda  0 \\iff y_i  \\lambda$ 时成立。\n情况2：$x_i^{(2)}  0$。则 $\\partial|x_i^{(2)}| = \\{-1\\}$，所以 $y_i - (1+\\varepsilon)x_i^{(2)} = -\\lambda$。解出 $x_i^{(2)}$ 得 $x_i^{(2)} = \\frac{y_i + \\lambda}{1+\\varepsilon}$。这在 $y_i + \\lambda  0 \\iff y_i  -\\lambda$ 时成立。\n情况3：$x_i^{(2)} = 0$。则 $y_i \\in \\lambda[-1, 1]$，这意味着 $|y_i| \\le \\lambda$。\n\n将 $x_i^{(2)}$ 的解与软阈值算子 $S_{\\lambda}(y_i)$ 进行比较，我们发现：\n$$ x_i^{(2)} = \\frac{1}{1+\\varepsilon} \\begin{cases} y_i - \\lambda  \\text{if } y_i  \\lambda \\\\ 0  \\text{if } |y_i| \\le \\lambda \\\\ y_i + \\lambda  \\text{if } y_i  -\\lambda \\end{cases} = \\frac{1}{1+\\varepsilon} S_{\\lambda}(y_i) = \\frac{1}{1+\\varepsilon} x_i^{(1)} $$\n这个关系对所有分量 $i=1, \\dots, n$ 都成立。因此，向量解是 $x^{(2)} = \\frac{1}{1+\\varepsilon}x^{(1)}$。\n\n第三部分：数值计算\n\n给定数据 $y=\\big(\\frac{3}{2},-\\frac{1}{3},\\frac{4}{5}\\big)$，$\\lambda=\\frac{1}{2}$ 和 $\\varepsilon=\\frac{1}{4}$。我们需要计算 $\\|\\operatorname{prox}_{\\lambda\\|x\\|_1}(y)-\\operatorname{prox}_{f_{\\lambda,\\varepsilon}}(y)\\|_2^2 = \\|x^{(1)} - x^{(2)}\\|_2^2$。\n\n使用上面推导出的关系，$x^{(1)} - x^{(2)} = x^{(1)} - \\frac{1}{1+\\varepsilon}x^{(1)} = \\left(1 - \\frac{1}{1+\\varepsilon}\\right)x^{(1)} = \\frac{\\varepsilon}{1+\\varepsilon}x^{(1)}$。\n那么，欧几里得距离的平方为：\n$$ \\|x^{(1)} - x^{(2)}\\|_2^2 = \\left\\| \\frac{\\varepsilon}{1+\\varepsilon}x^{(1)} \\right\\|_2^2 = \\left(\\frac{\\varepsilon}{1+\\varepsilon}\\right)^2 \\|x^{(1)}\\|_2^2 $$\n首先，我们用 $\\varepsilon = \\frac{1}{4}$ 计算标量因子：\n$$ \\left(\\frac{\\varepsilon}{1+\\varepsilon}\\right)^2 = \\left(\\frac{\\frac{1}{4}}{1+\\frac{1}{4}}\\right)^2 = \\left(\\frac{\\frac{1}{4}}{\\frac{5}{4}}\\right)^2 = \\left(\\frac{1}{5}\\right)^2 = \\frac{1}{25} $$\n接下来，我们用 $\\lambda = \\frac{1}{2}$ 计算 $x^{(1)} = \\operatorname{prox}_{\\lambda\\|x\\|_1}(y)$：\n对 $i=1$：$y_1 = \\frac{3}{2}$。因为 $y_1 = \\frac{3}{2}  \\lambda = \\frac{1}{2}$，我们有 $x_1^{(1)} = y_1 - \\lambda = \\frac{3}{2} - \\frac{1}{2} = 1$。\n对 $i=2$：$y_2 = -\\frac{1}{3}$。因为 $|y_2| = \\frac{1}{3} \\le \\lambda = \\frac{1}{2}$，我们有 $x_2^{(1)} = 0$。\n对 $i=3$：$y_3 = \\frac{4}{5}$。因为 $y_3 = \\frac{4}{5}  \\lambda = \\frac{1}{2}$（由于 $\\frac{4}{5}=0.8$ 且 $\\frac{1}{2}=0.5$），我们有 $x_3^{(1)} = y_3 - \\lambda = \\frac{4}{5} - \\frac{1}{2} = \\frac{8-5}{10} = \\frac{3}{10}$。\n所以，$x^{(1)} = \\left(1, 0, \\frac{3}{10}\\right)$。\n\n现在，我们计算 $x^{(1)}$ 的欧几里得范数的平方：\n$$ \\|x^{(1)}\\|_2^2 = 1^2 + 0^2 + \\left(\\frac{3}{10}\\right)^2 = 1 + \\frac{9}{100} = \\frac{100}{100} + \\frac{9}{100} = \\frac{109}{100} $$\n最后，我们计算所求的量：\n$$ \\|x^{(1)} - x^{(2)}\\|_2^2 = \\left(\\frac{\\varepsilon}{1+\\varepsilon}\\right)^2 \\|x^{(1)}\\|_2^2 = \\frac{1}{25} \\times \\frac{109}{100} = \\frac{109}{2500} $$\n结果是按要求给出的一个精确有理数。", "answer": "$$\\boxed{\\frac{109}{2500}}$$", "id": "3168299"}, {"introduction": "在掌握了 $L_1$ 范数的邻近算子之后，本练习将带你探索其对偶范数——无穷范数 ($L_{\\infty}$ norm)。此练习旨在加深你对次微分 (subdifferential) 概念的理解，特别是在其为集值 (set-valued) 的情况下的处理方式 [@problem_id:3168264]。通过计算并解释无穷范数的邻近点，你将观察到一种与软阈值不同的“收缩”机制，从而更全面地理解邻近算子的几何意义。", "problem": "令 $f:\\mathbb{R}^{4}\\to\\mathbb{R}$ 为凸函数 $f(x)=\\|x\\|_{\\infty}$，其中对于 $x=(x_{1},x_{2},x_{3},x_{4})$，$\\|x\\|_{\\infty}=\\max\\{|x_{1}|,|x_{2}|,|x_{3}|,|x_{4}|\\}$。考虑集值次微分算子的预解式，即 $(I+\\lambda\\,\\partial f)^{-1}$，该预解式出现在用于最小化 $f$ 的近端点算法中。\n\n1) 仅使用次微分的定义和关于对偶范数的标准性质，构造一个具体的点 $x\\in\\mathbb{R}^{4}$，在该点处次微分 $\\partial f(x)$ 是集值的（即包含不止一个向量），并证明为何它在该点是集值的。你的构造应该是明确且自洽的。\n\n2) 在 $\\lambda=2$ 且 $y=(3,\\,2,\\,-1,\\,0.5)$ 的情况下，精确计算预解式的值 $(I+\\lambda\\,\\partial f)^{-1}(y)$。你的最终数值答案必须是一个单一的 $1 \\times 4$ 行向量。无需四舍五入。\n\n3) 简要解释你在第2部分中计算出的点，说明无穷范数的近端映射如何重新分配最大绝对值分量的影响。请使用次梯度和凸组合的语言使你的解释精确。\n\n需要报告的最终答案必须仅为第2部分的向量，需精确写出（无四舍五入或单位）。", "solution": "用户希望我解决一个关于无穷范数的次微分和近端算子的三部分问题。\n\n### 问题验证\n\n**第1步：提取已知条件**\n- 函数：$f:\\mathbb{R}^{4}\\to\\mathbb{R}$，定义为 $f(x)=\\|x\\|_{\\infty}$，其中对于 $x=(x_{1},x_{2},x_{3},x_{4})$，$\\|x\\|_{\\infty}=\\max\\{|x_{1}|,|x_{2}|,|x_{3}|,|x_{4}|\\}$。\n- 函数 $f$ 被说明是凸函数。\n- 关注的算子：次微分算子的预解式 $(I+\\lambda\\,\\partial f)^{-1}$。\n- 第1部分：构造一个明确的点 $x\\in\\mathbb{R}^{4}$，在该点处次微分 $\\partial f(x)$ 是集值的（包含不止一个向量）。\n- 第2部分：对于 $\\lambda=2$ 和 $y=(3,\\,2,\\,-1,\\,0.5)$，计算预解式的值 $x_{\\text{out}} = (I+\\lambda\\,\\partial f)^{-1}(y)$。\n- 第3部分：对第2部分的结果提供一个简短而精确的解释。\n- 最终答案被指定为仅第2部分的向量。\n\n**第2步：使用提取的已知条件进行验证**\n- **科学上成立**：该问题在凸分析和优化领域有坚实的基础。函数 $f(x)=\\|x\\|_{\\infty}$ 是一个凸的、不可微函数的标准例子。次微分 $\\partial f$、预解式 $(I+\\lambda\\,\\partial f)^{-1}$ 和近端点算法是该领域中基本的、定义明确的概念。无穷范数的对偶是L1范数是一个标准结果。所有前提在事实上都是正确的。\n- **适定性**：该问题是适定的。\n  1. 对于第1部分，由于无穷范数在任何最大绝对值由多个分量同时达到的点处是不可微的，因此这样的点是存在的，构造一个这样的点是一个标准的练习。\n  2. 对于第2部分，像 $\\partial f$ 这样的极大单调算子的预解式对于任何正的 $\\lambda$ 都是一个单值且定义明确的映射。这等价于计算 $\\lambda f$ 的近端算子，对于像 $f$ 这样的真、凸、下半连续函数，该算子有唯一解。问题提供了所有必要的数据（$\\lambda$，$y$）。\n  3. 对于第3部分，对解释的要求是一个标准的概念性考察，而不是一个具有非唯一或不稳定答案的问题。\n- **客观性**：该问题使用精确的数学语言陈述，不包含主观或模糊的术语。\n- **完整性和一致性**：该问题是自洽的。所有必要的定义和值都已提供。没有矛盾之处。\n\n**第3步：结论与行动**\n问题是有效的。它在科学上是合理的、适定的、客观且完整的。我现在开始进行解答。\n\n### 解答\n\n**第1部分：构造一个具有集值次微分的点**\n\n凸函数 $f$ 在点 $x$ 处的次微分是在该点的所有次梯度 $g$ 的集合，由不等式 $f(z) \\ge f(x) + g^T(z-x)$ 对所有 $z$ 定义。\n对于具体函数 $f(x) = \\|x\\|_{\\infty}$，其次微分 $\\partial f(x)$ 由以下集合给出：\n$$ \\partial f(x) = \\{ g \\in \\mathbb{R}^4 \\mid \\|g\\|_1 \\le 1 \\text{ and } g^T x = \\|x\\|_{\\infty} \\} $$\n其中 $\\|g\\|_1 = \\sum_{i=1}^4 |g_i|$ 是L1范数，它是无穷范数的对偶范数。\n次微分 $\\partial f(x)$ 在 $f$ 不可微的点处是集值的（包含不止一个向量）。对于无穷范数，这发生在分量绝对值的最大值在多个索引上达到时。\n\n我们选择一个满足此条件的点 $x \\in \\mathbb{R}^4$。考虑点 $x = (5, -5, 1, 2)$。\n对此点，$\\|x\\|_{\\infty} = \\max\\{|5|, |-5|, |1|, |2|\\} = 5$。最大绝对值由两个分量 $x_1$ 和 $x_2$ 达到。\n\n现在，我们必须找到至少两个不同的向量 $g^{(1)}$ 和 $g^{(2)}$，它们满足属于 $\\partial f(x)$ 的条件。这些条件是：\n1. $\\|g\\|_1 \\le 1$\n2. $g^T x = g_1(5) + g_2(-5) + g_3(1) + g_4(2) = 5$\n\n我们来测试向量 $g^{(1)} = (1, 0, 0, 0)$。\n1. $\\|g^{(1)}\\|_1 = |1| + |0| + |0| + |0| = 1$。条件 $\\|g^{(1)}\\|_1 \\le 1$ 被满足。\n2. $(g^{(1)})^T x = (1)(5) + (0)(-5) + (0)(1) + (0)(2) = 5$。这与 $\\|x\\|_{\\infty} = 5$ 匹配。\n所以，$g^{(1)} = (1, 0, 0, 0)$ 是 $\\partial f(x)$ 中的一个有效次梯度。\n\n接下来，我们来测试向量 $g^{(2)} = (0, -1, 0, 0)$。\n1. $\\|g^{(2)}\\|_1 = |0| + |-1| + |0| + |0| = 1$。条件 $\\|g^{(2)}\\|_1 \\le 1$ 被满足。\n2. $(g^{(2)})^T x = (0)(5) + (-1)(-5) + (0)(1) + (0)(2) = 5$。这也与 $\\|x\\|_{\\infty} = 5$ 匹配。\n所以，$g^{(2)} = (0, -1, 0, 0)$ 是 $\\partial f(x)$ 中的另一个有效次梯度。\n\n由于 $g^{(1)} \\neq g^{(2)}$，我们已经证明了在 $x = (5, -5, 1, 2)$ 处的次微分 $\\partial f(x)$ 包含至少两个不同的向量。因此，它在该点是集值的。事实上，$\\partial f(x)$ 是这类“基本”次梯度的凸包。对于此 $x$，$\\partial f(x) = \\text{conv}\\{(1,0,0,0), (0,-1,0,0)\\}$。\n\n**第2部分：预解式的计算**\n\n我们需要计算在 $\\lambda = 2$ 和 $y = (3, 2, -1, 0.5)$ 条件下的 $x_{\\text{out}} = (I + \\lambda \\partial f)^{-1}(y)$。\n这等价于找到满足关系式 $y \\in x_{\\text{out}} + \\lambda \\partial f(x_{\\text{out}})$ 的唯一点 $x_{\\text{out}}$。这可以重排为 $\\frac{y - x_{\\text{out}}}{\\lambda} \\in \\partial f(x_{\\text{out}})$。\n这个定义对应于近端算子。我们寻求找到：\n$$ x_{\\text{out}} = \\text{prox}_{\\lambda f}(y) = \\arg\\min_{x \\in \\mathbb{R}^4} \\left\\{ \\lambda f(x) + \\frac{1}{2} \\|x - y\\|_2^2 \\right\\} $$\n代入给定值和 $f$ 的定义：\n$$ x_{\\text{out}} = \\arg\\min_{x \\in \\mathbb{R}^4} \\left\\{ 2 \\|x\\|_{\\infty} + \\frac{1}{2} \\|x - (3, 2, -1, 0.5)\\|_2^2 \\right\\} $$\n令 $\\alpha = \\|x\\|_{\\infty}$。该优化问题可以表述为最小化 $2\\alpha + \\frac{1}{2} \\sum_{i=1}^4 (x_i - y_i)^2$，约束条件为对所有 $i=1, \\dots, 4$，$|x_i| \\le \\alpha$。\n对于一个固定的 $\\alpha \\ge 0$ 值，我们首先关于 $x$ 进行最小化。该问题对每个分量 $x_i$ 解耦：\n$$ \\min_{x_i} \\frac{1}{2} (x_i - y_i)^2 \\quad \\text{subject to} \\quad -\\alpha \\le x_i \\le \\alpha $$\n解是 $y_i$ 在区间 $[-\\alpha, \\alpha]$ 上的投影，由 $x_i(\\alpha) = \\text{sign}(y_i) \\min\\{|y_i|, \\alpha\\}$ 给出。\n将此代回目标函数，得到一个仅关于 $\\alpha$ 的问题：\n$$ \\min_{\\alpha \\ge 0} H(\\alpha) = \\min_{\\alpha \\ge 0} \\left\\{ 2\\alpha + \\frac{1}{2} \\sum_{i=1}^4 \\left( \\text{sign}(y_i) \\min\\{|y_i|, \\alpha\\} - y_i \\right)^2 \\right\\} $$\n和式内的项在 $|y_i| \\le \\alpha$ 时为零，在 $|y_i|  \\alpha$ 时为 $(\\text{sign}(y_i)\\alpha - \\text{sign}(y_i)|y_i|)^2 = (\\alpha - |y_i|)^2$。所以，我们有：\n$$ H(\\alpha) = 2\\alpha + \\frac{1}{2} \\sum_{i \\text{ s.t. } |y_i|  \\alpha} (|y_i| - \\alpha)^2 $$\n为找到这个凸函数 $H(\\alpha)$ 的最小值，我们可以将其关于 $\\alpha$ 的导数设为零。导数是：\n$$ H'(\\alpha) = 2 + \\sum_{i \\text{ s.t. } |y_i|  \\alpha} \\frac{1}{2} \\cdot 2(|y_i| - \\alpha) \\cdot (-1) = 2 - \\sum_{i \\text{ s.t. } |y_i|  \\alpha} (|y_i| - \\alpha) $$\n$y=(3, 2, -1, 0.5)$ 的分量的绝对值是 $\\{3, 2, 1, 0.5\\}$。我们来测试最优 $\\alpha^*$ 的区间。\n- 如果 $1 \\le \\alpha  2$：绝对值大于 $\\alpha$ 的分量是 $\\{3, 2\\}$。\n  $H'(\\alpha) = 2 - ((3-\\alpha) + (2-\\alpha)) = 2 - (5 - 2\\alpha) = 2\\alpha - 3$。\n  令 $H'(\\alpha) = 0$ 得到 $2\\alpha - 3 = 0$，从而得出 $\\alpha = \\frac{3}{2} = 1.5$。\n  由于 $1.5 \\in [1, 2)$，这是我们的最优值，$\\alpha^* = 1.5$。\n\n现在我们使用 $\\alpha^* = 1.5$ 来计算解向量 $x_{\\text{out}}$ 的分量：\n$x_{\\text{out}, i} = \\text{sign}(y_i) \\min\\{|y_i|, 1.5\\}$。\n- $y_1=3$: $x_{\\text{out}, 1} = \\text{sign}(3) \\min\\{3, 1.5\\} = 1 \\cdot 1.5 = 1.5 = \\frac{3}{2}$。\n- $y_2=2$: $x_{\\text{out}, 2} = \\text{sign}(2) \\min\\{2, 1.5\\} = 1 \\cdot 1.5 = 1.5 = \\frac{3}{2}$。\n- $y_3=-1$: $x_{\\text{out}, 3} = \\text{sign}(-1) \\min\\{|-1|, 1.5\\} = -1 \\cdot 1 = -1$。\n- $y_4=0.5$: $x_{\\text{out}, 4} = \\text{sign}(0.5) \\min\\{0.5, 1.5\\} = 1 \\cdot 0.5 = 0.5 = \\frac{1}{2}$。\n所以，计算出的预解式的值是 $x_{\\text{out}} = (1.5, 1.5, -1, 0.5) = (\\frac{3}{2}, \\frac{3}{2}, -1, \\frac{1}{2})$。\n\n**第3部分：结果的解释**\n\n近端算子 $x_{\\text{out}} = \\text{prox}_{\\lambda f}(y)$ 找到一个点 $x$，该点平衡了两个目标：与 $y$ 保持接近（最小化 $\\frac{1}{2}\\|x-y\\|_2^2$）和具有较小的无穷范数（最小化 $\\lambda \\|x\\|_\\infty$）。\n解 $x_{\\text{out}} = (1.5, 1.5, -1, 0.5)$ 揭示了无穷范数近端映射特有的一种“收缩”机制。\n令阈值为 $\\alpha^* = \\|x_{\\text{out}}\\|_{\\infty} = 1.5$。\n1. 对于 $y$ 中绝对值小于或等于此阈值的分量（$|y_3|=1  1.5$ 和 $|y_4|=0.5  1.5$），$x_{\\text{out}}$ 的相应分量保持不变：$x_{\\text{out},3} = y_3$ 和 $x_{\\text{out},4} = y_4$。对于这些分量，最小化与 $y$ 的邻近度占主导地位，因为它们不对最终的无穷范数做出贡献。\n2. 对于 $y$ 中绝对值大于阈值的分量（$|y_1|=3  1.5$ 和 $|y_2|=2  1.5$），$x_{\\text{out}}$ 的相应分量被“收缩”到阈值水平，同时保持其符号。因此，$x_{\\text{out},1} = 1.5$ 和 $x_{\\text{out},2} = 1.5$。这些分量现在构成了 $x_{\\text{out}}$ 的激活集，即索引 $i$ 的集合，其中 $|x_i| = \\|x\\|_{\\infty}$。\n\n此行为由次梯度条件决定。向量 $g = \\frac{y-x_{\\text{out}}}{\\lambda} = \\frac{(3-1.5, 2-1.5, -1-(-1), 0.5-0.5)}{2} = (0.75, 0.25, 0, 0)$ 必须是 $f$ 在 $x_{\\text{out}}$ 处的一个次梯度。次微分 $\\partial f(x_{\\text{out}})$ 是对应于激活集的基本次梯度的凸包，这些基本次梯度是 $e_1=(1,0,0,0)$ 和 $e_2=(0,1,0,0)$。计算出的次梯度是 $g = 0.75 e_1 + 0.25 e_2$，一个凸组合。这表明原始最大分量（$y_1, y_2$）的影响已经被重新分配和拉平。绝对值减少的总量，$\\sum_{i \\in \\{1,2\\}} (|y_i| - \\alpha^*) = (3-1.5) + (2-1.5) = 1.5 + 0.5 = 2$，精确等于参数 $\\lambda$。近端算子有效地利用“预算” $\\lambda$ 来收缩最大的分量，直到总收缩成本等于 $\\lambda$。", "answer": "$$\n\\boxed{\n\\begin{pmatrix} \\frac{3}{2}  \\frac{3}{2}  -1  \\frac{1}{2} \\end{pmatrix}\n}\n$$", "id": "3168264"}, {"introduction": "本练习将理论与实际应用相结合，要求你为带有组套索 (group Lasso) 惩罚的逻辑斯谛回归问题构建一个完整的求解器。通过实现一个嵌套算法——外层使用邻近点算法框架，内层使用邻近梯度法求解子问题——你将深入了解如何应用 PPA 解决复杂的复合优化问题 [@problem_id:3168254]。这项实践不仅能提升你的编程能力，还能让你直观地看到正则化参数如何影响模型的结构化稀疏性。", "problem": "考虑一个带有稀疏性诱导组惩罚项的二元分类凸优化问题。令数据矩阵为 $X \\in \\mathbb{R}^{n \\times d}$，标签向量为 $y \\in \\{-1,+1\\}^n$。定义逻辑损失\n$$\nf(w) \\equiv \\frac{1}{n} \\sum_{i=1}^{n} \\log\\bigl(1 + \\exp(-y_i \\, x_i^\\top w)\\bigr),\n$$\n其中 $w \\in \\mathbb{R}^d$ 且 $x_i^\\top$ 是 $X$ 的第 $i$ 行。假设特征索引被划分为覆盖 $\\{1,2,\\dots,d\\}$ 的不相交组 $\\{G_1, G_2, \\dots, G_m\\}$，并定义组套索（group Lasso）惩罚项\n$$\nR(w) \\equiv \\sum_{g=1}^m \\|w_{G_g}\\|_2,\n$$\n其中 $w_{G_g}$ 汇集了由组 $G_g$ 索引的 $w$ 的坐标。对于正则化参数 $\\lambda \\ge 0$，学习问题为\n$$\n\\min_{w \\in \\mathbb{R}^d} \\; F(w) \\equiv f(w) + \\lambda R(w).\n$$\n\n你的任务是：\n1) 从正常、闭合、凸函数 $\\phi$ 的邻近算子（proximal operator）的定义出发，\n$$\n\\mathrm{prox}_{\\alpha \\phi}(v) \\equiv \\arg\\min_{u \\in \\mathbb{R}^d} \\left\\{ \\phi(u) + \\frac{1}{2\\alpha}\\|u - v\\|_2^2 \\right\\}, \\quad \\alpha  0,\n$$\n推导与组惩罚项 $\\phi(w) = \\sum_{g=1}^m \\|w_{G_g}\\|_2$ 相关的邻近映射（proximal map）的显式形式，该映射逐组作用于任意 $v \\in \\mathbb{R}^d$。\n\n2) 将邻近点算法（Proximal Point Algorithm, PPA）应用于 $F(w)$。PPA 的迭代式为\n$$\nw^{k+1} \\in \\arg\\min_{w \\in \\mathbb{R}^d} \\left\\{ f(w) + \\lambda R(w) + \\frac{1}{2t}\\|w - w^k\\|_2^2 \\right\\},\n$$\n其中步长参数 $t  0$。每个子问题都是强凸的。对于数值实现，通过邻近梯度法（proximal-gradient method）求解每个 PPA 子问题，该方法作用于复合目标 $f(w) + \\frac{1}{2t}\\|w - w^k\\|_2^2 + \\lambda R(w)$，并使用基于平滑部分梯度之 Lipschitz 常数上界的恒定步长。使用以下事实：逻辑损失的海森矩阵（Hessian）对所有 $w$ 满足 $\\nabla^2 f(w) \\preceq \\frac{1}{4n} X^\\top X$，因此一个可接受的 Lipschitz 常数为 $L_f \\le \\frac{\\lambda_{\\max}(X^\\top X)}{4n}$，并且子问题的平滑部分具有 Lipschitz 常数 $L_{\\text{sub}} = L_f + \\frac{1}{t}$。使用步长 $\\frac{1}{L_{\\text{sub}}}$。\n\n3) 通过在收敛时计算活动组的数量来评估 $\\lambda$ 变化时的分组效应，其中如果对于一个小的阈值 $\\epsilon$ 有 $\\|w_{G_g}\\|_2  \\epsilon$，则组 $G_g$ 被称为活动的。同时报告达到收敛所用的外部 PPA 迭代次数以及收敛解的最终目标值 $F(w)$。\n\n使用以下固定实例：\n- 维度：$n = 12$，$d = 6$，以及 $m = 3$ 个组，其中 $G_1 = \\{1,2\\}$，$G_2 = \\{3,4\\}$，$G_3 = \\{5,6\\}$。\n- 数据矩阵 $X$（行 $x_i^\\top$）：\n  - 第 1 行：$[0.50, -1.20, 0.30, 0.80, -0.50, 1.00]$\n  - 第 2 行：$[1.50, 0.20, -0.30, 0.40, 0.70, -1.20]$\n  - 第 3 行：$[-0.80, 0.90, 1.10, -1.30, 0.20, 0.50]$\n  - 第 4 行：$[0.00, 0.30, -0.70, 0.60, -1.00, 0.90]$\n  - 第 5 行：$[1.20, -0.50, 0.60, -0.20, 0.40, -0.80]$\n  - 第 6 行：$[-1.10, 1.40, -0.40, 0.50, -0.60, 0.30]$\n  - 第 7 行：$[0.70, -0.90, 0.20, -0.40, 1.30, -0.70]$\n  - 第 8 行：$[-0.60, 0.80, -1.20, 1.00, -0.30, 0.20]$\n  - 第 9 行：$[0.90, -0.40, 0.10, -0.90, 0.80, -0.60]$\n  - 第 10 行：$[-0.30, 1.10, -0.50, 0.20, -0.70, 1.20]$\n  - 第 11 行：$[0.40, -1.00, 0.90, -0.10, 0.60, -0.40]$\n  - 第 12 行：$[-0.90, 0.60, -0.80, 1.20, -0.20, 0.10]$\n- 标签 $y$：$[+1, +1, -1, -1, +1, -1, +1, -1, +1, -1, +1, -1]$。\n\n实现细节：\n- 初始化 $w^0 = 0$。\n- 使用常数 $t = 1.0$。\n- 外部 PPA 停止规则：当 $\\|w^{k+1} - w^k\\|_2 \\le 10^{-6}$ 或在 100 次外部迭代后停止，以先发生者为准。\n- 每个子问题的内部邻近梯度循环：使用步长 $\\frac{1}{L_{\\text{sub}}}$，其中 $L_{\\text{sub}} = \\frac{\\lambda_{\\max}(X^\\top X)}{4n} + \\frac{1}{t}$，当迭代变化量的欧几里得范数 $\\le 10^{-8}$ 或在 500 次内部迭代后停止，以先发生者为准。\n- 活动组阈值：$\\epsilon = 10^{-6}$。\n\n测试套件：\n- 四个正则化水平 $\\lambda \\in \\{0.0, 0.1, 0.5, 2.0\\}$。\n- 对于每个 $\\lambda$，按规定运行 PPA 并计算：\n  - 收敛时的活动组数量（一个整数），\n  - 所用的外部迭代次数（一个整数），\n  - 收敛的 $w$ 处的最终目标值 $F(w)$，四舍五入到小数点后六位（一个浮点数）。\n\n最终输出格式：\n- 你的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果。每个条目对应给定顺序中的一个 $\\lambda$，其本身是一个包含三个值的列表：$[\\text{active\\_groups}, \\text{outer\\_iterations}, \\text{final\\_objective}]$。例如，整体输出必须如下所示\n$$\n[[a_1,b_1,c_1],[a_2,b_2,c_2],[a_3,b_3,c_3],[a_4,b_4,c_4]],\n$$\n不含任何附加文本。", "solution": "用户指定了一个涉及带有组套索（group Lasso）惩罚项的逻辑回归的凸优化问题。任务是推导组套索惩罚项的邻近算子，指定一个解决该问题的算法，并实现该算法以找到给定数据集和参数集的数值结果。\n\n### 问题验证\n\n该问题是有效的。这是计算优化和机器学习领域中一个定义明确的任务，基于已建立的理论。所有数据、参数和算法规范均已提供，并且它们在数学上和科学上是一致的。问题陈述是完整的、客观的、适定的，每个子问题都允许有唯一解，并保证整体算法收敛到目标函数的最小值点。\n\n### 第 1 部分：组套索惩罚项的邻近算子推导\n\n一个正常、闭合、凸函数 $\\phi: \\mathbb{R}^d \\to \\mathbb{R} \\cup \\{+\\infty\\}$ 的邻近算子定义为：\n$$ \\mathrm{prox}_{\\alpha \\phi}(v) \\equiv \\arg\\min_{u \\in \\mathbb{R}^d} \\left\\{ \\phi(u) + \\frac{1}{2\\alpha}\\|u - v\\|_2^2 \\right\\}, \\quad \\alpha  0 $$\n在此问题中，函数 $\\phi(w)$ 是组套索惩罚项 $R(w) = \\sum_{g=1}^m \\|w_{G_g}\\|_2$。我们需要找到 $\\mathrm{prox}_{\\alpha R}(v)$。\n该最小化问题为：\n$$ \\arg\\min_{u \\in \\mathbb{R}^d} \\left\\{ \\sum_{g=1}^m \\|u_{G_g}\\|_2 + \\frac{1}{2\\alpha}\\|u - v\\|_2^2 \\right\\} $$\n目标函数对于不相交的坐标组 $\\{G_1, \\dots, G_m\\}$ 是可分的。欧几里得范数的平方项也可以分解：$\\|u - v\\|_2^2 = \\sum_{g=1}^m \\|u_{G_g} - v_{G_g}\\|_2^2$。因此，我们可以为每个组独立地解决最小化问题。对于每个组 $g \\in \\{1, \\dots, m\\}$，我们求解：\n$$ (\\mathrm{prox}_{\\alpha R}(v))_{G_g} = \\arg\\min_{u_g \\in \\mathbb{R}^{|G_g|}} \\left\\{ \\|u_g\\|_2 + \\frac{1}{2\\alpha}\\|u_g - v_g\\|_2^2 \\right\\} $$\n其中我们使用简写 $u_g = u_{G_g}$ 和 $v_g = v_{G_g}$。\n\n该子问题的一阶最优性条件表明，零向量必须位于目标函数在最小值点 $u_g^*$ 处的次梯度中。次梯度为 $\\partial \\|u_g^*\\|_2 + \\frac{1}{\\alpha}(u_g^* - v_g)$。令其包含零可得：\n$$ 0 \\in \\partial \\|u_g^*\\|_2 + \\frac{1}{\\alpha}(u_g^* - v_g) \\implies v_g - u_g^* \\in \\alpha \\, \\partial \\|u_g^*\\|_2 $$\n欧几里得范数的次梯度在 $z \\neq 0$ 时为 $\\partial \\|z\\|_2 = \\{z/\\|z\\|_2\\}$，在 $z = 0$ 时为闭单位球 $\\{z: \\|z\\|_2 \\le 1\\}$。我们对解 $u_g^*$ 分两种情况进行分析：\n\n情况 1：$u_g^* \\neq 0$。最优性条件变为 $v_g - u_g^* = \\alpha \\frac{u_g^*}{\\|u_g^*\\|_2}$。对 $v_g$ 进行整理，我们得到 $v_g = u_g^* (1 + \\frac{\\alpha}{\\|u_g^*\\|_2})$。这表明 $u_g^*$ 必须与 $v_g$ 共线。对两边取欧几里得范数，得到 $\\|v_g\\|_2 = \\|u_g^*\\|_2(1 + \\frac{\\alpha}{\\|u_g^*\\|_2}) = \\|u_g^*\\|_2 + \\alpha$。这意味着 $\\|u_g^*\\|_2 = \\|v_g\\|_2 - \\alpha$。要使其成为非零解，必须有 $\\|v_g\\|_2  \\alpha$。代入回整理后的方程，我们得到解：\n$$ u_g^* = v_g \\left( \\frac{1}{1 + \\alpha/\\|u_g^*\\|_2} \\right) = v_g \\left( \\frac{\\|u_g^*\\|_2}{\\|u_g^*\\|_2 + \\alpha} \\right) = v_g \\left( \\frac{\\|v_g\\|_2 - \\alpha}{\\|v_g\\|_2} \\right) = \\left(1 - \\frac{\\alpha}{\\|v_g\\|_2}\\right) v_g $$\n\n情况 2：$u_g^* = 0$。最优性条件变为 $v_g \\in \\alpha \\, \\partial \\|0\\|_2$，即集合 $\\{z : \\|z\\|_2 \\le \\alpha\\}$。因此，如果 $\\|v_g\\|_2 \\le \\alpha$，解为 $u_g^* = 0$。\n\n结合两种情况，单个组的邻近算子为：\n$$ (\\mathrm{prox}_{\\alpha R}(v))_{G_g} = \\begin{cases} \\left(1 - \\frac{\\alpha}{\\|v_{G_g}\\|_2}\\right) v_{G_g}  \\text{if } \\|v_{G_g}\\|_2  \\alpha \\\\ 0  \\text{if } \\|v_{G_g}\\|_2 \\le \\alpha \\end{cases} $$\n这可以紧凑地写成组软阈值算子：\n$$ (\\mathrm{prox}_{\\alpha R}(v))_{G_g} = \\left(1 - \\frac{\\alpha}{\\|v_{G_g}\\|_2}\\right)_+ v_{G_g} $$\n其中 $(x)_+ = \\max(0, x)$。此公式应用于每个组 $G_g$（$g=1, \\dots, m$）以计算完整的邻近映射 $\\mathrm{prox}_{\\alpha R}(v)$。\n\n### 第 2 部分：算法实现\n\n问题是使用邻近点算法（PPA）最小化 $F(w) = f(w) + \\lambda R(w)$。\n\n**外循环：邻近点算法（PPA）**\nPPA 通过以下更新规则生成一系列迭代点 $\\{w^k\\}$：\n$$ w^{k+1} = \\arg\\min_{w \\in \\mathbb{R}^d} \\left\\{ F(w) + \\frac{1}{2t}\\|w - w^k\\|_2^2 \\right\\} $$\n其中 $t  0$ 是一个步长参数，给定为 $t=1.0$。目标函数可以重写为：\n$$ w^{k+1} = \\arg\\min_{w \\in \\mathbb{R}^d} \\left\\{ \\underbrace{f(w) + \\frac{1}{2t}\\|w - w^k\\|_2^2}_{\\text{平滑部分: } S_k(w)} + \\underbrace{\\lambda R(w)}_{\\text{非平滑部分: } N(w)} \\right\\} $$\n外循环从 $w^0 = 0$ 开始，当 $\\|w^{k+1} - w^k\\|_2 \\le 10^{-6}$ 或达到 100 次迭代后终止。\n\n**内循环：邻近梯度法（PGM）**\n每个 PPA 子问题都使用邻近梯度法求解。PGM 将目标函数拆分为平滑部分 $S_k(w)$ 和非平滑部分 $N(w)$。我们用 $z^j$ 表示 PGM 的迭代点，其更新方式如下：\n$$ z^{j+1} = \\mathrm{prox}_{\\eta N}(z^j - \\eta \\nabla S_k(z^j)) $$\n其中 $\\eta  0$ 是内循环的步长。\n\n- **平滑部分梯度**：$S_k(w)$ 的梯度是 $\\nabla S_k(w) = \\nabla f(w) + \\frac{1}{t}(w - w^k)$。逻辑损失的梯度是 $\\nabla f(w) = \\frac{1}{n}\\sum_{i=1}^n \\frac{-y_i x_i}{1 + e^{y_i x_i^\\top w}}$。\n- **步长 $\\eta$**：步长为 $\\eta = 1/L_{\\text{sub}}$，其中 $L_{\\text{sub}}$ 是 $\\nabla S_k(w)$ 的 Lipschitz 常数。已知逻辑损失的海森矩阵满足 $\\nabla^2 f(w) \\preceq L_f I$，其中 $L_f = \\frac{\\lambda_{\\max}(X^\\top X)}{4n}$，则 $S_k(w)$ 的海森矩阵为 $\\nabla^2 S_k(w) = \\nabla^2 f(w) + \\frac{1}{t}I \\preceq (L_f + \\frac{1}{t})I$。因此，一个合适的 Lipschitz 常数是 $L_{\\text{sub}} = L_f + \\frac{1}{t}$。\n- **邻近步骤**：邻近步骤涉及 $N(w) = \\lambda R(w)$ 的算子：\n$$ \\mathrm{prox}_{\\eta N}(v) = \\mathrm{prox}_{\\eta (\\lambda R)}(v) = \\mathrm{prox}_{(\\eta\\lambda) R}(v) $$\n这是使用第 1 部分推导的公式计算的，其中 $\\alpha = \\eta\\lambda$。每个组 $g$ 的更新为：\n$$ (z^{j+1})_{G_g} = \\left(1 - \\frac{\\eta\\lambda}{\\|v_{G_g}^j\\|_2}\\right)_+ v_{G_g}^j $$\n其中 $v^j = z^j - \\eta \\nabla S_k(z^j)$。\n\n内部 PGM 循环使用前一个 PPA 迭代点 $w^k$ 进行初始化（热启动），并在迭代点变化的范数 $\\le 10^{-8}$ 或达到 500 次迭代后终止。\n\n### 第 3 部分：评估\n\n在 PPA 收敛到最终解 $w^*$ 后，我们为每个给定的 $\\lambda$ 值计算以下内容：\n1.  **活动组数量**：满足 $\\|w^*_{G_g}\\|_2  \\epsilon$ 的组 $G_g$ 的数量，阈值 $\\epsilon = 10^{-6}$。\n2.  **外部 PPA 迭代次数**：外部 PPA 循环为达到收敛所用的总迭代次数。\n3.  **最终目标值**：$F(w^*) = f(w^*) + \\lambda R(w^*)$ 的值，四舍五入到小数点后六位。\n\n对每个 $\\lambda \\in \\{0.0, 0.1, 0.5, 2.0\\}$ 重复整个过程。", "answer": "[[3,9,0.222384],[2,8,0.447547],[1,5,0.640982],[0,2,0.693147]]", "id": "3168254"}]}