{"hands_on_practices": [{"introduction": "理论知识是基础，但真正的理解来自于实践。我们将从一个精心设计的单变量非凸函数开始，对比两种最经典的优化算法：梯度下降法和牛顿法。通过这个练习[@problem_id:3145146]，你将亲手实现并观察到，二阶方法（牛顿法）虽然在局部收敛速度上具有优势，但也可能因其对曲率的敏感性而陷入错误的吸引盆或在非凸区域失效，而一阶方法（梯度下降法）则更为稳健但效率较低。", "problem": "考虑使用两种方法来最小化一个非凸标量函数：带回溯线搜索的梯度下降法和带回溯线搜索的牛顿法。您必须使用的基本概念和定义是：局部最小值和全局最小值的定义，可微函数的梯度和Hessian矩阵的概念，以及一阶和二阶泰勒近似。如果在点 $x^{\\star}$ 周围存在一个邻域，对于该邻域中的所有 $x$ 都满足 $f(x^{\\star}) \\leq f(x)$，则称 $x^{\\star}$ 为一个局部最小值点。一个全局最小值点 $x^{\\mathrm{glob}}$ 满足对于定义域中所有的 $x$，都有 $f(x^{\\mathrm{glob}}) \\leq f(x)$。梯度下降法源于一阶泰勒近似，而牛顿法源于二阶泰勒近似。\n\n您必须在标量函数\n$$\nf(x) = (x^2 - 1)^2 + 0.3\\,x,\n$$\n上实现这两种方法，其梯度为\n$$\n\\nabla f(x) = 4x^3 - 4x + 0.3,\n$$\nHessian矩阵为\n$$\n\\nabla^2 f(x) = 12x^2 - 4.\n$$\n回溯线搜索必须强制执行Armijo充分下降条件：对于一个试探步长 $x_{k+1} = x_k + \\alpha p_k$，如果满足\n$$\nf(x_{k+1}) \\leq f(x_k) + c\\,\\alpha\\,\\nabla f(x_k)\\,p_k,\n$$\n则接受 $x_{k+1}$，其中 $c \\in (0,1)$ 是固定值。回溯线搜索请使用 $c = 10^{-4}$ 和收缩因子 $\\rho = 0.5$。对于梯度下降法，使用搜索方向 $p_k = -\\nabla f(x_k)$。对于牛顿法，当 $\\nabla^2 f(x_k) \\neq 0$ 时，使用一维牛顿方向 $p_k = -\\nabla f(x_k)/\\nabla^2 f(x_k)$。如果牛顿方向不是下降方向，即 $\\nabla f(x_k)\\,p_k \\geq 0$，则声明牛顿法在 $x_k$ 处因局部曲率引导方法进入错误吸引盆而失败，并且不从此初始条件继续进行。\n\n您的程序必须：\n- 严格按照规定实现这两种方法，最大迭代次数为 $100$ 次，梯度范数的终止容差为 $\\varepsilon = 10^{-8}$，即当 $|\\nabla f(x_k)| \\leq \\varepsilon$ 时停止。\n- 通过对三次方程 $4x^3 - 4x + 0.3 = 0$ 进行多项式求根，精确求解 $\\nabla f(x) = 0$ 来确定驻点集合，使用 $\\nabla^2 f(x)$ 的符号对最小值点进行分类，并通过比较它们的函数值来确定最小值点中的全局最小值点。\n- 对于每次运行，确定方法是否收敛、使用的迭代次数、最终点 $x_{\\mathrm{final}}$，以及最终值 $f(x_{\\mathrm{final}})$ 是否在 $10^{-8}$ 的容差内与全局最小值匹配。\n\n测试套件：\n从以下初始点运行这两种方法：\n- $x_0 = -0.2$ (测试原点附近的非凸曲率)，\n- $x_0 = 0.8$ (测试向右侧较浅的局部吸引盆的吸引力)，\n- $x_0 = -0.8$ (测试向左侧更深的全局吸引盆的吸引力)。\n\n输出规格：\n对于每个初始点，生成一个包含以下五个条目的列表：\n- 一个布尔值，表示牛顿法是否达到全局最小值，\n- 一个布尔值，表示梯度下降法是否达到全局最小值，\n- 一个布尔值，表示牛顿法所需的迭代次数是否严格少于梯度下降法（如果牛顿法未能产生一个步长，则将此布尔值声明为false），\n- 一个整数，表示牛顿法使用的迭代次数（如果牛顿法立即失败，则使用 $0$），\n- 一个整数，表示梯度下降法使用的迭代次数。\n\n您的程序应生成单行输出，其中包含三个初始点的结果，格式为一个用方括号括起来的逗号分隔列表，不含空格。其中每个元素是如上所述的列表，例如 $[\\,[\\cdots],\\,[\\cdots],\\,[\\cdots]\\,]$，格式化时无空格。", "solution": "问题是有效的。这是一个适定的、有科学依据的数值优化练习，旨在比较梯度下降法和牛顿法在一维非凸函数上的行为。所有参数和条件都已明确指定。\n\n问题的核心是分析一阶方法（梯度下降法）与二阶方法（牛顿法）在最小化函数 $f(x) = (x^2 - 1)^2 + 0.3x$ 时的性能。该函数的非凸性，其特点是同时存在正曲率和负曲率区域，旨在突显这两种方法之间的根本差异。\n\n首先，我们必须通过识别其驻点并对其进行分类来刻画函数 $f(x)$。驻点是梯度 $\\nabla f(x) = 4x^3 - 4x + 0.3 = 0$ 的根。对这个三次多项式使用数值求根算法，我们找到三个实根：\n$x_1^* \\approx -1.03653191$\n$x_2^* \\approx 0.07530320$\n$x_3^* \\approx 0.96122871$\n\n为了对这些驻点进行分类，我们使用二阶充分条件，即评估Hessian矩阵 $\\nabla^2 f(x) = 12x^2 - 4$ 在每个点上的符号。\n- 对于 $x_1^* \\approx -1.0365$：$\\nabla^2 f(x_1^*) = 12(-1.0365)^2 - 4 \\approx 8.89 > 0$。这表明 $x_1^*$ 是一个局部最小值点。\n- 对于 $x_2^* \\approx 0.0753$：$\\nabla^2 f(x_2^*) = 12(0.0753)^2 - 4 \\approx -3.93  0$。这表明 $x_2^*$ 是一个局部最大值点。在这个负曲率区域，牛顿法预计会失败。\n- 对于 $x_3^* \\approx 0.9612$：$\\nabla^2 f(x_3^*) = 12(0.9612)^2 - 4 \\approx 7.09 > 0$。这表明 $x_3^*$ 是另一个局部最小值点。\n\n为了找到全局最小值，我们比较两个局部最小值点的函数值：\n- $f(x_1^*) \\approx f(-1.0365) = ((-1.0365)^2 - 1)^2 + 0.3(-1.0365) \\approx -0.30546$\n- $f(x_3^*) \\approx f(0.9612) = ((0.9612)^2 - 1)^2 + 0.3(0.9612) \\approx 0.29416$\n由于 $f(x_1^*)  f(x_3^*)$，点 $x_1^*$ 是全局最小值点，全局最小值为 $f_{\\mathrm{glob}} \\approx -0.30546193$。如果任何算法运行后其终点的函数值与此值的差在 $10^{-8}$ 的容差范围内，则认为该算法找到了全局最小值。\n\n优化算法实现如下。两种方法都利用回溯线搜索来确保每一步都有充分的下降，满足Armijo条件 $f(x_{k+1}) \\leq f(x_k) + c\\,\\alpha\\,\\nabla f(x_k)\\,p_k$，其中 $c = 10^{-4}$，步长缩减因子 $\\rho = 0.5$。初始步长取为 $\\alpha=1.0$。\n\n**梯度下降法（GD）：** 这种一阶方法使用负梯度作为其搜索方向，$p_k = -\\nabla f(x_k)$。只要 $\\nabla f(x_k) \\neq 0$，这就保证了是一个下降方向，因为 $\\nabla f(x_k) p_k = -(\\nabla f(x_k))^2  0$。GD是稳健的，并且总是会向局部最小值点前进，但其收敛速度可能很慢。\n\n**牛顿法（NM）：** 这种二阶方法使用搜索方向 $p_k = -(\\nabla^2 f(x_k))^{-1} \\nabla f(x_k)$。它源于函数的二次模型。当Hessian矩阵 $\\nabla^2 f(x_k)$ 是正定时，牛顿方向是一个下降方向，该方法表现出快速的（二次）局部收敛性。然而，如果Hessian矩阵不是正定的（即，在此一维情况下 $\\nabla^2 f(x_k) \\leq 0$），该方向可能不是下降方向，并且该方法可能被吸引到鞍点或最大值点。根据问题规范，如果在任何迭代 $k$ 中 $\\nabla^2 f(x_k) \\leq 0$，则声明该方法对于该初始条件失败，并终止该次运行。报告的迭代次数为 $k$。\n\n程序将从每个指定的初始点（$x_0 \\in \\{-0.2, 0.8, -0.8\\}$）执行这两种算法，跟踪它们的性能指标（收敛状态、迭代次数、最终点），并根据指定的输出格式报告结果。这包括将结果与预先计算的全局最小值进行比较，以及比较两种方法之间的迭代次数。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the optimization problem by implementing and comparing\n    gradient descent and Newton's method on a given nonconvex function.\n    \"\"\"\n\n    # --- Problem Definition ---\n    C = 1e-4\n    RHO = 0.5\n    TOL = 1e-8\n    MAX_ITER = 100\n\n    def f(x):\n        return (x**2 - 1)**2 + 0.3 * x\n\n    def grad_f(x):\n        return 4 * x**3 - 4 * x + 0.3\n\n    def hess_f(x):\n        return 12 * x**2 - 4\n\n    # --- Stationary Points and Global Minimum Analysis ---\n    # Solve grad_f(x) = 4x^3 - 4x + 0.3 = 0\n    roots = np.roots([4, 0, -4, 0.3])\n    \n    # Classify roots and find local minima\n    local_minima_x = []\n    for r in roots:\n        # We are only interested in real roots for this 1D problem.\n        if np.isreal(r):\n            if hess_f(np.real(r))  0:\n                local_minima_x.append(np.real(r))\n\n    # Find global minimum value by comparing function values at local minima\n    f_values_at_minima = [f(x) for x in local_minima_x]\n    global_min_val = min(f_values_at_minima)\n\n    # --- Core Algorithm Implementations ---\n    def backtracking_line_search(x, p, f, grad_f):\n        alpha = 1.0\n        fx = f(x)\n        grad_fx_p = grad_f(x) * p\n        while f(x + alpha * p)  fx + C * alpha * grad_fx_p:\n            alpha *= RHO\n            if alpha  1e-16: # Prevent infinite loop\n                break\n        return alpha\n\n    def gradient_descent(x0):\n        x = float(x0)\n        for k in range(MAX_ITER):\n            grad = grad_f(x)\n            if abs(grad) = TOL:\n                return x, k, 'converged'\n            \n            p = -grad\n            alpha = backtracking_line_search(x, p, f, grad_f)\n            x = x + alpha * p\n        \n        # Final check after max iterations\n        status = 'converged' if abs(grad_f(x)) = TOL else 'max_iter_reached'\n        return x, MAX_ITER, status\n\n    def newton_method(x0):\n        x = float(x0)\n        for k in range(MAX_ITER):\n            # Check for failure condition (non-positive definite Hessian)\n            hess = hess_f(x)\n            if hess = 0:\n                return x, k, 'failed'\n            \n            grad = grad_f(x)\n            if abs(grad) = TOL:\n                return x, k, 'converged'\n            \n            p = -grad / hess\n            alpha = backtracking_line_search(x, p, f, grad_f)\n            x = x + alpha * p\n            \n        # Final check after max iterations\n        status = 'converged' if abs(grad_f(x)) = TOL else 'max_iter_reached'\n        return x, MAX_ITER, status\n\n    # --- Test Suite Execution ---\n    test_cases = [-0.2, 0.8, -0.8]\n    results = []\n\n    for x0 in test_cases:\n        # Run Gradient Descent\n        x_final_gd, iters_gd, status_gd = gradient_descent(x0)\n        \n        # Run Newton's Method\n        x_final_nm, iters_nm, status_nm = newton_method(x0)\n\n        # --- Evaluate Results ---\n        # 1. Did Newton reach the global minimum?\n        nm_reaches_glob_min = False\n        if status_nm == 'converged' or status_nm == 'max_iter_reached':\n             if abs(f(x_final_nm) - global_min_val) = TOL:\n                 nm_reaches_glob_min = True\n\n        # 2. Did Gradient Descent reach the global minimum?\n        gd_reaches_glob_min = False\n        if status_gd == 'converged' or status_gd == 'max_iter_reached':\n            if abs(f(x_final_gd) - global_min_val) = TOL:\n                gd_reaches_glob_min = True\n        \n        # 3. Did Newton use strictly fewer iterations?\n        nm_fewer_iters = False\n        if status_nm != 'failed':\n            nm_fewer_iters = iters_nm  iters_gd\n        \n        # 4. Number of iterations for Newton\n        iters_nm_out = iters_nm\n        \n        # 5. Number of iterations for Gradient Descent\n        iters_gd_out = iters_gd\n        \n        case_result = [\n            str(nm_reaches_glob_min),\n            str(gd_reaches_glob_min),\n            str(nm_fewer_iters),\n            str(iters_nm_out),\n            str(iters_gd_out)\n        ]\n        results.append(f\"[{','.join(case_result)}]\")\n\n    # Final print statement in the exact required format\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3145146"}, {"introduction": "梯度下降法在许多情况下表现良好，但在某些特定地形上会遇到严重困难。这个实践[@problem_id:3145074]将带你构建一个反例，即一个具有狭窄、弯曲“山谷”的二维函数，以揭示梯度下降法低效的“之字形”下降行为。通过将它的表现与一种考虑了曲率信息的改进方法（阻尼牛顿法）进行比较，你将深刻理解为什么二阶信息对于高效导航复杂优化地形至关重要。", "problem": "您的任务是构建并分析一个具体的、光滑的二维函数，该函数在非凸曲面中穿越狭窄山谷时，对于使用 Armijo 回溯线搜索的梯度下降法会表现出“Z”字形行为。重点在于识别局部和全局最优解，展示一个朴素的一阶方法如何在山谷两侧之间循环（即第一个坐标的符号反复交替），并提出能够缓解此行为的、感知曲率的更新方法。推导和实现必须基于优化方法的核心定义。\n\n基本原理：\n- 如果存在一个半径 $r0$，使得对于所有满足 $\\lVert x-x^\\star\\rVert\\le r$ 的点 $x$，都有 $f(x^\\star)\\le f(x)$，那么点 $x^\\star$ 是可微函数 $f:\\mathbb{R}^n\\to\\mathbb{R}$ 的一个局部极小值点。如果对于所有 $x\\in\\mathbb{R}^n$ 都有 $f(x^\\star)\\le f(x)$，那么点 $x^\\star$ 是一个全局极小值点。\n- 对于一个可微函数 $f$，局部最优性的一阶必要条件是 $\\nabla f(x^\\star)=0$。\n- 使用 Armijo 回溯的梯度下降法选择方向 $d_k=-\\nabla f(x_k)$ 和步长 $t_k$，使得 $f(x_k+t_k d_k)\\le f(x_k)+c\\,t_k\\,\\nabla f(x_k)^\\top d_k$，其中 $c\\in(0,1)$ 是一个固定常数。标准的做法是使用一个回溯收缩因子 $\\tau\\in(0,1)$ 不断缩减步长 $t_k\\leftarrow \\tau t_k$，直到满足 Armijo 条件。\n- Hessian 矩阵 $H(x)=\\nabla^2 f(x)$ 捕捉了局部曲率。感知曲率的更新方法通过 $H(x)^{-1}$ 的一个近似来对梯度进行预处理，通常会加入正则化以确保其正定性。\n\n问题设定：\n1. 考虑由下式定义的光滑函数 $f:\\mathbb{R}^2\\to\\mathbb{R}$：\n$$\nf(x,y)=\\left(x^2-1\\right)^2+\\alpha\\,y^2+\\beta\\,\\sin(3y)\\,x+\\gamma\\,\\sin^2(3y),\n$$\n其中 $\\alpha0$，$\\beta\\in\\mathbb{R}$，$\\gamma\\ge 0$ 是固定参数。该函数在 $x$ 方向上有一个四次截面，在 $x=\\pm 1$ 附近形成了山谷的谷壁；在 $y$ 方向上有一个浅的二次项；还有一个在 $y$ 方向上的正弦耦合项，它会调制山谷，并能在 $y$ 演变时产生交替的横向力，将 $x$ 推向相对的谷壁。\n\n2. 仅使用微分法则，从上述定义中推导出梯度 $\\nabla f(x,y)$ 和 Hessian 矩阵 $H(x,y)$：\n- 梯度分量为\n$$\n\\frac{\\partial f}{\\partial x}(x,y)=4x\\left(x^2-1\\right)+\\beta\\,\\sin(3y),\\quad\n\\frac{\\partial f}{\\partial y}(x,y)=2\\alpha\\,y+3\\beta\\,\\cos(3y)\\,x+6\\gamma\\,\\sin(3y)\\cos(3y).\n$$\n- Hessian 矩阵的元素为\n$$\n\\frac{\\partial^2 f}{\\partial x^2}(x,y)=12x^2-4,\\quad\n\\frac{\\partial^2 f}{\\partial y^2}(x,y)=2\\alpha-9\\beta\\,\\sin(3y)\\,x+18\\gamma\\,\\cos(6y),\n$$\n$$\n\\frac{\\partial^2 f}{\\partial x\\partial y}(x,y)=\\frac{\\partial^2 f}{\\partial y\\partial x}(x,y)=3\\beta\\,\\cos(3y).\n$$\n\n3. 仅使用上述定义，实现两种下降方法：\n- 方法 A（一阶）：使用 Armijo 回溯的梯度下降法，方向为 $d_k=-\\nabla f(x_k,y_k)$，Armijo 参数 $c$ 固定为 $10^{-4}$，回溯因子 $\\tau$ 固定为 $0.5$，初始步长 $t_0=1$。\n- 方法 B（感知曲率）：一种阻尼牛顿预处理步，使用 $d_k=-\\left(H(x_k,y_k)+\\lambda_k I\\right)^{-1}\\nabla f(x_k,y_k)$，其中 $\\lambda_k$ 选择为 $\\lambda_k=\\max\\left\\{0,-\\lambda_{\\min}(H(x_k,y_k))+\\varepsilon\\right\\}$，$\\lambda_{\\min}(H)$ 是 $H$ 的最小特征值，$I$ 是单位矩阵，$\\varepsilon$ 是一个小的正数常量（取 $\\varepsilon=10^{-3}$）。对此方向使用与之前相同的 Armijo 参数进行回溯。\n\n4. 检测“Z”字形循环：将连续迭代点之间 $x$ 坐标的符号变化定义为一次“谷壁翻转”，即 $\\operatorname{sign}(x_{k+1})\\neq \\operatorname{sign}(x_k)$ 且 $x_{k+1}$ 和 $x_k$ 均不为零。如果谷壁翻转次数至少为 $K$（其中 $K=8$），则声明出现“循环”。在固定的迭代预算内，为每种方法统计谷壁翻转的次数。\n\n5. 收敛性与最优解：对于测试中使用的参数范围，确定 $f$ 的全局极小值点。报告最终迭代点是否接近一个全局极小值点，该判断需同时满足两个标准：与最近的候选全局极小值点 $(1,0)$ 或 $(-1,0)$ 的距离小于距离阈值 $\\delta=10^{-2}$，并且函数值小于绝对阈值 $\\eta=10^{-6}$。\n\n6. 测试套件与输出规范：\n使用以下测试用例，每个用例由元组 $(\\alpha,\\beta,\\gamma,x_0,y_0,\\text{max\\_iters})$ 指定，所有数字均明确写出：\n- 用例 1：$(0.005,4.0,0.1,0.0,3.0,400)$，预期方法 A 会表现出明显的“Z”字形行为，而方法 B 的翻转次数较少。\n- 用例 2：$(0.005,4.0,0.1,0.0,0.2,200)$，从靠近谷底的位置开始，预期会表现出极少的“Z”字形行为。\n- 用例 3：$(0.08,1.0,0.05,0.0,2.4,400)$，耦合较弱，预期“Z”字形行为会减少。\n- 用例 4：$(0.5,4.0,0.1,0.0,3.0,400)$，$y$ 方向曲率更陡，预期会抑制振荡。\n\n对于每个用例，从相同的初始点运行方法 A 和方法 B，并按顺序记录以下六个值：\n- 方法 A 的谷壁翻转整数次数。\n- 方法 B 的谷壁翻转整数次数。\n- 方法 A 的最终函数值（浮点数）。\n- 方法 B 的最终函数值（浮点数）。\n- 一个布尔值，指示方法 A 是否根据上述标准位于全局最小值的邻域内。\n- 一个布尔值，指示方法 B 是否根据上述标准位于全局最小值的邻域内。\n\n最终输出格式：\n您的程序应生成单行输出，其中包含一个用方括号括起来的、以逗号分隔的结果列表。列表中的每个元素对应一个测试用例，其本身又是按上述顺序包含六个值的列表。例如，输出应如下所示：\n$[[\\text{case1\\_vals}],[\\text{case2\\_vals}],[\\text{case3\\_vals}],[\\text{case4\\_vals}]]$.\n不涉及物理单位，也不需要角度或百分比。所有计算都必须在 $\\mathbb{R}^2$ 中使用实数算术执行。", "solution": "该问题要求构建并分析一个光滑的二维函数，该函数在一阶和感知曲率的优化方法下会表现出特定的行为。目标是实现并比较两种算法——带 Armijo 回溯的梯度下降法和一种阻尼牛顿预处理方法，并量化它们在穿越狭窄非凸山谷时的性能。此分析涉及跟踪一个“Z”字形度量、评估目标函数以及检查是否收敛到预定义的目标区域。这个问题定义明确，数学上合理，并为完整的解决方案提供了所有必要的参数和规范。\n\n所考虑的函数是 $f:\\mathbb{R}^2\\to\\mathbb{R}$，定义如下：\n$$\nf(x,y)=\\left(x^2-1\\right)^2+\\alpha\\,y^2+\\beta\\,\\sin(3y)\\,x+\\gamma\\,\\sin^2(3y)\n$$\n参数为 $\\alpha  0$，$\\beta \\in \\mathbb{R}$ 和 $\\gamma \\ge 0$。这个函数被设计成具有类似山谷的结构。主导项 $\\left(x^2-1\\right)^2$ 创建了一个在 $x=\\pm 1$ 处有极小值的四次势，形成了与 y 轴对齐的山谷的谷壁。项 $\\alpha y^2$ 为谷底引入了一个简单的二次斜率。项 $\\beta \\sin(3y)x$ 提供了两个坐标之间的关键耦合，引入了一个依赖于位置 $y$ 的 $x$ 方向上的振荡力。最后，项 $\\gamma \\sin^2(3y)$ 为谷底增加了进一步的起伏。对于 $\\beta \\ne 0$ 的情况，真正的全局极小值点并不位于 $(\\pm 1, 0)$，因为在这些点梯度不为零。然而，这些点可作为主山谷结构底部的参考位置，并且问题基于它们定义了特定的收敛准则。\n\n该分析基于两种优化算法：\n\n**方法 A：带 Armijo 回溯的梯度下降法**\n\n这是一种一阶方法，其中每次迭代 $k$ 的搜索方向是负梯度，$d_k = -\\nabla f(x_k, y_k)$。梯度由下式给出：\n$$\n\\nabla f(x,y) = \\begin{pmatrix} 4x(x^2-1) + \\beta\\sin(3y) \\\\ 2\\alpha y + 3\\beta x\\cos(3y) + 3\\gamma\\sin(6y) \\end{pmatrix}\n$$\n步长 $t_k$ 通过 Armijo 回溯线搜索确定，从初始猜测值 $t_{k,0}=1$ 开始，并以因子 $\\tau=0.5$ 不断减小，直到满足条件 $f(x_k+t_k d_k) \\le f(x_k) + c\\,t_k\\,\\nabla f(x_k)^\\top d_k$，其中常数 $c=10^{-4}$。在一个狭窄的山谷中，水平集是细长且弯曲的，梯度向量通常几乎与谷底方向正交。因此，梯度下降法倾向于在山谷间迈出大步，在其谷壁之间反弹，而沿着山谷的进展却很缓慢。这表现为“Z”字形行为，通过计算“谷壁翻转”（定义为连续迭代点 $x_k$ 和 $x_{k+1}$ 具有相反符号）来量化。\n\n**方法 B：阻尼牛顿预处理下降法**\n\n这是一种感知曲率的方法，它使用来自 Hessian 矩阵 $H(x,y) = \\nabla^2 f(x,y)$ 的信息来找到更有效的搜索方向。Hessian 矩阵由下式给出：\n$$\nH(x,y) = \\begin{pmatrix}\n12x^2-4  3\\beta\\cos(3y) \\\\\n3\\beta\\cos(3y)  2\\alpha - 9\\beta x\\sin(3y) + 18\\gamma\\cos(6y)\n\\end{pmatrix}\n$$\n搜索方向计算为 $d_k = - (H(x_k,y_k) + \\lambda_k I)^{-1} \\nabla f(x_k,y_k)$，其中 $I$ 是 $2 \\times 2$ 的单位矩阵。项 $\\lambda_k$ 是一个阻尼参数，用于确保修正后的 Hessian 矩阵 $H(x_k,y_k) + \\lambda_k I$ 是正定的，从而保证 $d_k$ 是一个下降方向。它被选择为 $\\lambda_k = \\max\\{0, -\\lambda_{\\min}(H(x_k,y_k)) + \\varepsilon\\}$，其中 $\\lambda_{\\min}(H)$ 是 Hessian 矩阵的最小特征值，$\\varepsilon=10^{-3}$ 是一个小的正数常量。通过融合曲率信息，该方法有效地重塑了问题的几何结构，使搜索方向能更直接地指向沿着谷底的极小值点，从而缓解“Z”字形效应。步长使用与方法 A 中相同的 Armijo 回溯过程确定。\n\n该实现将对四个不同的测试用例模拟这两种方法，每个用例由一组参数 $(\\alpha, \\beta, \\gamma)$ 和一个初始点 $(x_0, y_0)$ 定义。对于每次运行，我们将记录总的谷壁翻转次数。在指定迭代次数结束时，我们将评估最终的函数值并检查收敛性。收敛性定义为同时满足两个条件：最终迭代点到两个点 $(1,0)$ 和 $(-1,0)$ 中较近者的欧几里得距离必须小于 $\\delta=10^{-2}$，并且最终函数值必须小于 $\\eta=10^{-6}$。最终输出将是一个结构化列表，其中包含每个测试用例的这六个度量指标。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the optimization problem for all test cases.\n    Implements and compares gradient descent and a damped Newton method\n    on a specified non-convex function.\n    \"\"\"\n\n    test_cases = [\n        # (alpha, beta, gamma, x0, y0, max_iters)\n        (0.005, 4.0, 0.1, 0.0, 3.0, 400),\n        (0.005, 4.0, 0.1, 0.0, 0.2, 200),\n        (0.08, 1.0, 0.05, 0.0, 2.4, 400),\n        (0.5, 4.0, 0.1, 0.0, 3.0, 400),\n    ]\n\n    # Armijo and convergence parameters\n    C_ARMIJO = 1e-4\n    TAU = 0.5\n    EPSILON = 1e-3\n    DIST_THRESHOLD = 1e-2\n    FUNC_THRESHOLD = 1e-6\n    MINIMIZER_CANDIDATES = [np.array([1.0, 0.0]), np.array([-1.0, 0.0])]\n\n    def f(p, alpha, beta, gamma):\n        x, y = p\n        return (x**2 - 1)**2 + alpha * y**2 + beta * np.sin(3*y) * x + gamma * np.sin(3*y)**2\n\n    def grad_f(p, alpha, beta, gamma):\n        x, y = p\n        df_dx = 4 * x * (x**2 - 1) + beta * np.sin(3*y)\n        df_dy = 2 * alpha * y + 3 * beta * np.cos(3*y) * x + 6 * gamma * np.sin(3*y) * np.cos(3*y)\n        return np.array([df_dx, df_dy])\n\n    def hess_f(p, alpha, beta, gamma):\n        x, y = p\n        d2f_dx2 = 12 * x**2 - 4\n        d2f_dy2 = 2 * alpha - 9 * beta * np.sin(3*y) * x + 18 * gamma * np.cos(6*y)\n        d2f_dxdy = 3 * beta * np.cos(3*y)\n        return np.array([[d2f_dx2, d2f_dxdy], [d2f_dxdy, d2f_dy2]])\n\n    def armijo_backtracking(p, d, grad_p, f_p, alpha, beta, gamma):\n        t = 1.0\n        while True:\n            p_new = p + t * d\n            f_new = f(p_new, alpha, beta, gamma)\n            if f_new = f_p + C_ARMIJO * t * np.dot(grad_p, d):\n                return t\n            t = t * TAU\n\n    def run_method_a(alpha, beta, gamma, x0, y0, max_iters):\n        p = np.array([x0, y0])\n        flips = 0\n        for k in range(max_iters):\n            grad_p = grad_f(p, alpha, beta, gamma)\n            d = -grad_p\n            f_p = f(p, alpha, beta, gamma)\n            \n            t = armijo_backtracking(p, d, grad_p, f_p, alpha, beta, gamma)\n            \n            p_prev = p\n            p = p + t * d\n\n            if np.sign(p_prev[0]) != np.sign(p[0]) and p_prev[0] != 0 and p[0] != 0:\n                flips += 1\n        \n        return p, flips\n\n    def run_method_b(alpha, beta, gamma, x0, y0, max_iters):\n        p = np.array([x0, y0])\n        flips = 0\n        for k in range(max_iters):\n            grad_p = grad_f(p, alpha, beta, gamma)\n            hess_p = hess_f(p, alpha, beta, gamma)\n\n            try:\n                min_eigval = np.linalg.eigvalsh(hess_p).min()\n            except np.linalg.LinAlgError:\n                min_eigval = -1.0 # If something fails, assume non-PD\n\n            lambda_k = max(0, -min_eigval + EPSILON)\n            \n            H_reg = hess_p + lambda_k * np.eye(2)\n            \n            try:\n                d = np.linalg.solve(H_reg, -grad_p)\n            except np.linalg.LinAlgError:\n                # Fallback to gradient descent if regularized Hessian is singular\n                d = -grad_p\n            \n            f_p = f(p, alpha, beta, gamma)\n            t = armijo_backtracking(p, d, grad_p, f_p, alpha, beta, gamma)\n            \n            p_prev = p\n            p = p + t * d\n\n            if np.sign(p_prev[0]) != np.sign(p[0]) and p_prev[0] != 0 and p[0] != 0:\n                flips += 1\n        \n        return p, flips\n\n    results = []\n    for case in test_cases:\n        alpha, beta, gamma, x0, y0, max_iters = case\n        \n        # Method A\n        p_final_a, flips_a = run_method_a(alpha, beta, gamma, x0, y0, max_iters)\n        f_val_a = f(p_final_a, alpha, beta, gamma)\n        dist_a = min(np.linalg.norm(p_final_a - m) for m in MINIMIZER_CANDIDATES)\n        converged_a = dist_a  DIST_THRESHOLD and f_val_a  FUNC_THRESHOLD\n        \n        # Method B\n        p_final_b, flips_b = run_method_b(alpha, beta, gamma, x0, y0, max_iters)\n        f_val_b = f(p_final_b, alpha, beta, gamma)\n        dist_b = min(np.linalg.norm(p_final_b - m) for m in MINIMIZER_CANDIDATES)\n        converged_b = dist_b  DIST_THRESHOLD and f_val_b  FUNC_THRESHOLD\n        \n        case_results = [\n            flips_a, flips_b, f_val_a, f_val_b, converged_a, converged_b\n        ]\n        results.append(case_results)\n\n    # Format output as specified\n    results_str = [\n        f\"[{r[0]},{r[1]},{r[2]},{r[3]},{str(r[4]).lower()},{str(r[5]).lower()}]\"\n        for r in results\n    ]\n    print(f\"[{','.join(results_str)}]\")\n\nsolve()\n```", "id": "3145074"}, {"introduction": "前面的练习展示了局部优化算法如何致力于寻找单个最小值，但它们无法保证找到的是全局最优解。那么，我们如何系统地探索整个搜索空间呢？这个练习[@problem_id:3145095]介绍了一种广泛使用的全局优化启发式策略：多起点法。你将使用著名的Himmelblau函数作为测试平台，通过从不同随机点启动局部优化器，来经验性地量化找到全局最优解的概率。", "problem": "考虑由Himmelblau函数定义的函数 $f:\\mathbb{R}^2\\to\\mathbb{R}$，$f(x,y) = (x^2 + y - 11)^2 + (x + y^2 - 7)^2$。您将设计并分析一个多起点局部优化方案，以通过经验量化从随机初始化到达全局最小值的概率。\n\n基本原理和定义：\n- 如果存在点 $(x^\\star,y^\\star)$ 的一个邻域 $\\mathcal{N}$，使得对于所有 $(x,y)\\in\\mathcal{N}$ 都有 $f(x^\\star,y^\\star)\\le f(x,y)$，则该点是一个局部极小值点。\n- 如果对于所有 $(x,y)\\in\\mathbb{R}^2$ 都有 $f(x^\\star,y^\\star)\\le f(x,y)$，则点 $(x^\\star,y^\\star)$ 是一个全局极小值点。\n- 由于 $f(x,y)$ 是平方和，它满足对于所有 $(x,y)\\in\\mathbb{R}^2$ 都有 $f(x,y)\\ge 0$，并且任何使两个平方项同时为零的点都会得到值为 $0$ 的全局最小值。\n\n任务：\n- 实现一个执行以下步骤的多起点方案：\n  1. 从给定的轴对齐矩形 $[a_x,b_x]\\times[a_y,b_y]$ 上的均匀分布中抽取 $N$ 个独立的起始点 $(x_0,y_0)$。\n  2. 从每个起始点 $(x_0,y_0)$ 开始，使用基于一阶微积分的确定性局部求解器（例如，Broyden–Fletcher–Goldfarb–Shanno (BFGS) 方法），并利用由微分的乘法法则和链式法则推导出的 $f(x,y)$ 的解析梯度。\n  3. 对于一个数值公差 $\\varepsilon_f0$，如果局部求解器产生的终点 $(\\hat x,\\hat y)$ 满足 $f(\\hat x,\\hat y)\\le \\varepsilon_f$，则声明为“成功”。否则，声明为“失败”。\n  4. 通过蒙特卡洛估计量 $\\hat p = \\frac{1}{N}\\sum_{i=1}^{N} I_i$ 来估计在指定的随机初始化下达到全局最小值的概率 $p$，其中 $I_i$ 是第 $i$ 次运行成功的指示函数。\n- 在一个测试用例的所有运行中使用相同的求解器超参数，包括最大迭代次数。\n\n不涉及角度单位。不涉及物理单位。所有概率必须以 $[0,1]$ 范围内的十进制数报告。\n\n测试套件（所有随机抽样必须是均匀的，并可通过给定的种子复现）：\n- 案例1（理想情况，宽定义域，充足的迭代次数）：$N=200$，$\\text{seed}=7$，$[a_x,b_x]=[-6,6]$，$[a_y,b_y]=[-6,6]$，$\\varepsilon_f=10^{-6}$，最大迭代次数 $=200$。\n- 案例2（小样本边界）：$N=10$，$\\text{seed}=11$，$[a_x,b_x]=[-6,6]$，$[a_y,b_y]=[-6,6]$，$\\varepsilon_f=10^{-6}$，最大迭代次数 $=200$。\n- 案例3（定义域集中于单个盆地区域）：$N=100$，$\\text{seed}=23$，$[a_x,b_x]=[2.5,3.5]$，$[a_y,b_y]=[1.5,2.5]$，$\\varepsilon_f=10^{-6}$，最大迭代次数 $=50$。\n- 案例4（极少迭代次数的压力测试）：$N=200$，$\\text{seed}=31$, $[a_x,b_x]=[-6,6]$，$[a_y,b_y]=[-6,6]$，$\\varepsilon_f=10^{-6}$，最大迭代次数 $=5$。\n\n要求输出：\n- 您的程序必须使用 $f(x,y)$ 的解析梯度、Broyden–Fletcher–Goldfarb–Shanno 方法以及为每个案例指定的参数来实现上述方案。\n- 对于每个案例，将估计的概率 $\\hat p$ 输出为浮点数。最终输出必须是单行，包含一个列表，按案例1到4的顺序列出四个概率，格式为方括号内用逗号分隔的列表（例如，$[\\hat p_1,\\hat p_2,\\hat p_3,\\hat p_4]$）。这些值可以用任何合理的固定精度打印。", "solution": "该问题要求实现并分析一个多起点优化方案，以估计找到Himmelblau函数 $f(x,y)$ 全局最小值的概率。问题陈述的验证证实了其科学性、适定性、客观性，并包含了计算求解所需的所有必要信息。该问题是有效的。\n\n需要最小化的函数是Himmelblau函数，定义为 $f:\\mathbb{R}^2 \\to \\mathbb{R}$：\n$$f(x,y) = (x^2 + y - 11)^2 + (x + y^2 - 7)^2$$\n该函数是两个平方项的和。其值始终为非负，即对于所有 $(x,y) \\in \\mathbb{R}^2$，$f(x,y) \\ge 0$。$f(x,y)$ 的全局极小值点是满足 $f(x^\\star, y^\\star) = 0$ 的点 $(x^\\star, y^\\star)$。这当且仅当两个平方项同时为零时发生：\n\\begin{align*}\nx^2 + y - 11 = 0 \\\\\nx + y^2 - 7 = 0\n\\end{align*}\n解这个非线性方程组可以得到四个不同的解，它们是该函数的四个全局极小值点：\n\\begin{itemize}\n    \\item $(3, 2)$\n    \\item $(-2.805118..., 3.131312...)$\n    \\item $(-3.779310..., -3.283186...)$\n    \\item $(3.584428..., -1.848126...)$\n\\end{itemize}\n该多起点方案采用了一种局部优化算法，具体是Broyden–Fletcher–Goldfarb–Shanno (BFGS) 方法。BFGS 是一种拟牛顿法，它需要目标函数的梯度来迭代搜索局部最小值。问题要求使用解析梯度，我们将在下面进行推导。\n\n令 $u(x,y) = x^2 + y - 11$ 且 $v(x,y) = x + y^2 - 7$，这样 $f = u^2 + v^2$。$f$ 的梯度，记作 $\\nabla f(x,y)$，是其偏导数组成的向量 $(\\frac{\\partial f}{\\partial x}, \\frac{\\partial f}{\\partial y})$。使用微分的链式法则和加法法则：\n$$ \\frac{\\partial f}{\\partial x} = \\frac{\\partial}{\\partial x}(u^2 + v^2) = 2u \\frac{\\partial u}{\\partial x} + 2v \\frac{\\partial v}{\\partial x} $$\n$$ \\frac{\\partial f}{\\partial y} = \\frac{\\partial}{\\partial y}(u^2 + v^2) = 2u \\frac{\\partial u}{\\partial y} + 2v \\frac{\\partial v}{\\partial y} $$\n$u(x,y)$ 和 $v(x,y)$ 的偏导数是：\n\\begin{align*}\n\\frac{\\partial u}{\\partial x} = 2x,  \\quad \\frac{\\partial u}{\\partial y} = 1 \\\\\n\\frac{\\partial v}{\\partial x} = 1,  \\quad \\frac{\\partial v}{\\partial y} = 2y\n\\end{align*}\n将这些代入 $f(x,y)$ 偏导数的表达式中：\n\\begin{align*}\n\\frac{\\partial f}{\\partial x} = 2(x^2 + y - 11)(2x) + 2(x + y^2 - 7)(1) \\\\\n= 4x(x^2 + y - 11) + 2(x + y^2 - 7) \\\\\n\\frac{\\partial f}{\\partial y} = 2(x^2 + y - 11)(1) + 2(x + y^2 - 7)(2y) \\\\\n= 2(x^2 + y - 11) + 4y(x + y^2 - 7)\n\\end{align*}\n因此，解析梯度向量为：\n$$ \\nabla f(x,y) = \\begin{pmatrix} 4x(x^2 + y - 11) + 2(x + y^2 - 7) \\\\ 2(x^2 + y - 11) + 4y(x + y^2 - 7) \\end{pmatrix} $$\n这个梯度函数将被提供给 BFGS 求解器以实现高效收敛。\n\n任务的核心是实现一个蒙特卡洛模拟。每个测试用例的步骤如下：\n1. 为了可复现性，设置随机数生成器种子。指定运行的参数：试验次数 $N$、采样域 $[a_x,b_x]\\times[a_y,b_y]$、成功公差 $\\varepsilon_f$ 和求解器的最大迭代次数。\n2. 初始化成功计数器 $S$ 为 $0$。\n3. 对于从 $1$ 到 $N$ 的 $i$：\n    a. 从指定的矩形域上的均匀分布中抽取一个随机起始点 $(x_0, y_0)$，即 $x_0 \\sim U(a_x, b_x)$ 和 $y_0 \\sim U(a_y, b_y)$。\n    b. 从 $(x_0, y_0)$ 开始运行 BFGS 算法，使用解析推导的梯度 $\\nabla f$。求解器在收敛或达到最大迭代次数后终止。\n    c. 设求解器找到的终点为 $(\\hat{x}, \\hat{y})$。计算函数值 $f(\\hat{x}, \\hat{y})$。\n    d. 如果 $f(\\hat{x}, \\hat{y}) \\le \\varepsilon_f$，则该次运行被视为“成功”，计数器 $S$ 增加 1。公差 $\\varepsilon_f = 10^{-6}$ 足够小，可以确保成功的运行终止在四个全局最小值（其中 $f=0$）之一的非常近的位置。\n4. 在所有 $N$ 次试验完成后，成功概率 $p$ 通过样本均值 $\\hat{p} = S/N$ 进行估计。\n\n对提供的四个测试用例中的每一个都执行此过程，从而得出四个概率估计值：$\\hat{p}_1, \\hat{p}_2, \\hat{p}_3, \\hat{p}_4$。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef solve():\n    \"\"\"\n    Implements a multistart optimization scheme to estimate the probability of\n    finding a global minimum of the Himmelblau function for four test cases.\n    \"\"\"\n\n    # Define the Himmelblau function f(p) where p = [x, y]\n    def himmelblau_func(p):\n        x, y = p\n        term1 = x**2 + y - 11\n        term2 = x + y**2 - 7\n        return term1**2 + term2**2\n\n    # Define the analytic gradient of the Himmelblau function\n    def himmelblau_grad(p):\n        x, y = p\n        # Partial derivative with respect to x\n        df_dx = 4 * x * (x**2 + y - 11) + 2 * (x + y**2 - 7)\n        # Partial derivative with respect to y\n        df_dy = 2 * (x**2 + y - 11) + 4 * y * (x + y**2 - 7)\n        return np.array([df_dx, df_dy])\n\n    # Test suite parameters\n    test_cases = [\n        # Case 1: N=200, seed=7, [-6,6]x[-6,6], eps=1e-6, max_iter=200\n        {'N': 200, 'seed': 7, 'domain_x': [-6, 6], 'domain_y': [-6, 6], 'eps_f': 1e-6, 'max_iter': 200},\n        # Case 2: N=10, seed=11, [-6,6]x[-6,6], eps=1e-6, max_iter=200\n        {'N': 10, 'seed': 11, 'domain_x': [-6, 6], 'domain_y': [-6, 6], 'eps_f': 1e-6, 'max_iter': 200},\n        # Case 3: N=100, seed=23, [2.5,3.5]x[1.5,2.5], eps=1e-6, max_iter=50\n        {'N': 100, 'seed': 23, 'domain_x': [2.5, 3.5], 'domain_y': [1.5, 2.5], 'eps_f': 1e-6, 'max_iter': 50},\n        # Case 4: N=200, seed=31, [-6,6]x[-6,6], eps=1e-6, max_iter=5\n        {'N': 200, 'seed': 31, 'domain_x': [-6, 6], 'domain_y': [-6, 6], 'eps_f': 1e-6, 'max_iter': 5},\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        N = case['N']\n        seed = case['seed']\n        ax, bx = case['domain_x']\n        ay, by = case['domain_y']\n        eps_f = case['eps_f']\n        max_iter = case['max_iter']\n\n        # Set up a random number generator with the specified seed for reproducibility\n        rng = np.random.default_rng(seed)\n\n        success_count = 0\n        for _ in range(N):\n            # 1. Draw a random starting point from the uniform distribution\n            x0 = rng.uniform(ax, bx)\n            y0 = rng.uniform(ay, by)\n            initial_guess = np.array([x0, y0])\n\n            # 2. Run the BFGS local solver\n            res = minimize(\n                himmelblau_func,\n                initial_guess,\n                method='BFGS',\n                jac=himmelblau_grad,\n                options={'maxiter': max_iter}\n            )\n\n            # 3. Check for success\n            if res.fun = eps_f:\n                success_count += 1\n        \n        # 4. Estimate the probability\n        p_hat = success_count / N\n        results.append(p_hat)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3145095"}]}