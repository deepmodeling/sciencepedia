{"hands_on_practices": [{"introduction": "要真正理解优化算法，最好的方法莫过于亲手实践。本章提供了一系列动手练习，旨在加深你对无约束和有约束优化核心概念的理解。我们将从一个经典的无约束优化场景开始：比较最速下降法和共轭梯度法。最速下降法虽然直观，但在处理某些几何形状（所谓的“狭长山谷”）的问题时会表现出收敛缓慢的“之字形”行为。通过这个练习，你将亲手构建这样一个场景，并将其与更高效的共轭梯度法进行对比，从而深刻体会到高级优化算法的威力。[@problem_id:3195724]", "problem": "考虑一个严格凸二次函数 $f:\\mathbb{R}^n\\to\\mathbb{R}$ 的无约束最小化问题，该函数定义为 $f(x)=\\tfrac{1}{2}x^\\top Q x - b^\\top x$，其中 $Q\\in\\mathbb{R}^{n\\times n}$ 是对称正定矩阵，且 $b\\in\\mathbb{R}^n$。其梯度为 $\\nabla f(x)=Qx-b$，唯一最小化点 $x^\\star$ 满足 $Qx^\\star=b$。现有两种迭代方法将在此类问题上进行比较：\n\n- 使用精确线搜索的最速下降法，该方法通过迭代更新 $x_{k+1}=x_k+\\alpha_k d_k$（其中 $d_k=-\\nabla f(x_k)$），并通过在 $\\alpha\\in\\mathbb{R}$ 上最小化 $\\phi(\\alpha)=f(x_k+\\alpha d_k)$ 来选择步长 $\\alpha_k$。\n- 共轭梯度法 (CG)，该方法通过迭代生成 $Q$-共轭方向来求解线性系统 $Qx=b$。\n\n设计并实现一个数值实验，以揭示一个反例场景：在该场景中，使用精确线搜索的最速下降法在狭窄的山谷中产生“之”字形（zig-zag）路径。同时，将该方法在具有细长特征结构的矩阵上的行为与共轭梯度法进行比较。使用二维情况 ($n=2$)，其中矩阵 $Q$ 由旋转和对角特征值构造，以模拟狭窄且旋转的山谷，即 $Q=R(\\theta)^\\top \\operatorname{diag}(\\lambda_1,\\lambda_2) R(\\theta)$，其中 $R(\\theta)$ 是旋转角度为 $\\theta$ 的旋转矩阵。\n\n使用的基本原理：\n- 可微函数的梯度定义 $\\nabla f(x)$，以及通过 $Qx^\\star=b$ 对严格凸二次函数最小化点的刻画。\n- 定义为沿当前搜索方向进行最小化的精确线搜索。\n- $Q$ 的对称正定性概念及其特征结构。\n\n实现这两种方法，并针对使用精确线搜索的最速下降法，测量其前两个步进方向 $d_0$ 和 $d_1$ 之间的夹角（以度为单位）。如果该方法在少于两次迭代内收敛，则报告夹角为 $0.0$ 度。使用欧几里得内积和范数。夹角必须以度为单位报告。\n\n测试套件规范（每个案例都会产生一个具有细长特征结构的独特矩阵）：\n- 案例1（理想路径，旋转的狭窄山谷）：$\\lambda_1=1$，$\\lambda_2=1000$，$\\theta=30$ 度，$b=\\begin{bmatrix}1\\\\-1\\end{bmatrix}$，$x_0=\\begin{bmatrix}2\\\\2\\end{bmatrix}$，容差 $\\varepsilon=10^{-10}$，最速下降法最大迭代次数 $10^4$，共轭梯度法最大迭代次数 $10^2$。\n- 案例2（边界对齐，与坐标轴对齐的狭窄山谷）：$\\lambda_1=1$，$\\lambda_2=1000$，$\\theta=0$ 度，$b=\\begin{bmatrix}1\\\\0.5\\end{bmatrix}$，$x_0=\\begin{bmatrix}-3\\\\3\\end{bmatrix}$，容差 $\\varepsilon=10^{-10}$，最速下降法最大迭代次数 $10^4$，共轭梯度法最大迭代次数 $10^2$。\n- 案例3（边缘案例，极度细长且旋转）：$\\lambda_1=1$，$\\lambda_2=10^6$，$\\theta=45$ 度，$b=\\begin{bmatrix}0.5\\\\-0.5\\end{bmatrix}$，$x_0=\\begin{bmatrix}1\\\\-1\\end{bmatrix}$，容差 $\\varepsilon=10^{-10}$，最速下降法最大迭代次数 $10^4$，共轭梯度法最大迭代次数 $10^2$。\n\n对于每个测试案例，计算并报告：\n1. 使用精确线搜索的最速下降法达到 $\\lVert \\nabla f(x_k)\\rVert_2\\le\\varepsilon$ 所需的整数迭代次数，若未达到则报告最大迭代次数。\n2. 共轭梯度法达到 $\\lVert b-Qx_k\\rVert_2\\le\\varepsilon$ 所需的整数迭代次数，若未达到则报告最大迭代次数。\n3. 前两个最速下降步骤 $d_0$ 和 $d_1$ 之间的夹角（以度为单位的浮点数）；如果发生的步骤少于两步，则报告 $0.0$ 度。\n4. 一个布尔值，指示最速下降法所需的迭代次数是否严格多于共轭梯度法。\n\n你的程序应生成单行输出，其中包含三个测试案例的汇总结果，格式为方括号内用逗号分隔的列表。顺序必须是：\n$[k_{\\text{SD},1},k_{\\text{CG},1},\\alpha_{1},\\text{bool}_1,k_{\\text{SD},2},k_{\\text{CG},2},\\alpha_{2},\\text{bool}_2,k_{\\text{SD},3},k_{\\text{CG},3},\\alpha_{3},\\text{bool}_3]$,\n其中 $k_{\\text{SD},i}$ 和 $k_{\\text{CG},i}$ 分别是案例 $i$ 中最速下降法和共轭梯度法的迭代次数，$\\alpha_i$ 是案例 $i$ 的夹角（以度为单位），而 $\\text{bool}_i$ 是相应的布尔值。夹角必须以度表示。输出中不应出现百分比或其他单位。", "solution": "用户提供了一个有效的问题陈述。\n\n### 问题验证\n\n**第一步：提取已知条件**\n- **目标函数**：无约束最小化 $f:\\mathbb{R}^n\\to\\mathbb{R}$，定义为 $f(x)=\\tfrac{1}{2}x^\\top Q x - b^\\top x$。\n- **矩阵 Q**：$Q\\in\\mathbb{R}^{n\\times n}$ 是对称正定（SPD）矩阵。\n- **梯度**：$\\nabla f(x)=Qx-b$。\n- **最小化点条件**：唯一的最小化点 $x^\\star$ 满足 $Qx^\\star=b$。\n- **方法一：使用精确线搜索的最速下降法（SD）**：\n    - 更新规则：$x_{k+1}=x_k+\\alpha_k d_k$。\n    - 搜索方向：$d_k=-\\nabla f(x_k)$。\n    - 步长 $\\alpha_k$：通过在 $\\alpha\\in\\mathbb{R}$ 上最小化 $\\phi(\\alpha)=f(x_k+\\alpha d_k)$ 来选择。\n- **方法二：共轭梯度法（CG）**：\n    - 一种求解线性系统 $Qx=b$ 的迭代方法。\n- **实验设置**：\n    - 维度：$n=2$。\n    - 矩阵构造：$Q=R(\\theta)^\\top \\operatorname{diag}(\\lambda_1,\\lambda_2) R(\\theta)$，其中 $R(\\theta)$ 是旋转矩阵。\n- **需报告的指标**：\n    1. SD 达到 $\\lVert \\nabla f(x_k)\\rVert_2\\le\\varepsilon$ 所需的整数迭代次数 ($k_{SD}$)。\n    2. CG 达到 $\\lVert b-Qx_k\\rVert_2\\le\\varepsilon$ 所需的整数迭代次数 ($k_{CG}$)。\n    3. 前两个 SD 步进方向 $d_0$ 和 $d_1$ 之间的夹角，以度为单位（浮点数）。若迭代次数少于两次，则报告 $0.0$。\n    4. 一个布尔值，指示是否 $k_{SD}  k_{CG}$。\n- **测试套件**：\n    - **案例1**：$\\lambda_1=1$, $\\lambda_2=1000$, $\\theta=30^\\circ$, $b=\\begin{bmatrix}1\\\\-1\\end{bmatrix}$, $x_0=\\begin{bmatrix}2\\\\2\\end{bmatrix}$, $\\varepsilon=10^{-10}$, $max\\_iter_{SD}=10^4$, $max\\_iter_{CG}=10^2$。\n    - **案例2**：$\\lambda_1=1$, $\\lambda_2=1000$, $\\theta=0^\\circ$, $b=\\begin{bmatrix}1\\\\0.5\\end{bmatrix}$, $x_0=\\begin{bmatrix}-3\\\\3\\end{bmatrix}$, $\\varepsilon=10^{-10}$, $max\\_iter_{SD}=10^4$, $max\\_iter_{CG}=10^2$。\n    - **案例3**：$\\lambda_1=1$, $\\lambda_2=10^6$, $\\theta=45^\\circ$, $b=\\begin{bmatrix}0.5\\\\-0.5\\end{bmatrix}$, $x_0=\\begin{bmatrix}1\\\\-1\\end{bmatrix}$, $\\varepsilon=10^{-10}$, $max\\_iter_{SD}=10^4$, $max\\_iter_{CG}=10^2$。\n- **输出格式**：单行 `[k_SD,1,k_CG,1,angle_1,bool_1,k_SD,2,k_CG,2,angle_2,bool_2,k_SD,3,k_CG,3,angle_3,bool_3]`。\n\n**第二步：使用提取的已知条件进行验证**\n- **科学依据**：该问题在根本上是合理的。它涉及将数值优化中标准的、成熟的算法（最速下降法、共轭梯度法）应用于经典的二次最小化问题。其数学框架完全正确。\n- **适定性**：由于 $Q$ 是对称正定矩阵，函数 $f(x)$ 是严格凸的，这保证了唯一最小化点的存在。算法是确定性的且定义明确。所述数值实验的解保证存在。\n- **客观性**：问题陈述使用了精确、客观的数学语言。所有参数都已数值化指定，没有主观解释的余地。\n- **完整性**：提供了所有必要的组成部分：目标函数、算法定义、矩阵构造、终止准则、初始条件和具体的测试参数。\n- **可行性**：数值在标准浮点运算的范围内。该实验被设计为计算上是可行的。\n- **结构**：该问题被清晰地构造为一个比较性的数值实验。\n\n**第三步：结论与行动**\n问题有效。我们继续进行求解。\n\n### 算法设计与原理\n\n该问题要求实现并比较两种基本优化算法，用于最小化一个严格凸二次函数 $f(x) = \\frac{1}{2}x^\\top Q x - b^\\top x$。该函数的梯度为 $\\nabla f(x) = Qx - b$，全局最小值 $x^\\star$ 通过求解线性系统 $\\nabla f(x^\\star) = 0$ 得到，这等价于 $Qx^\\star = b$。两种方法的停止条件，$\\lVert Qx_k - b \\rVert_2 \\le \\varepsilon$ 或 $\\lVert b - Qx_k \\rVert_2 \\le \\varepsilon$，都基于迭代点 $x_k$ 处的梯度范数。\n\n**1. 使用精确线搜索的最速下降法**\n\n最速下降法沿负梯度方向进行迭代移动，该方向是函数局部最速下降的方向。\n更新规则由下式给出：\n$$x_{k+1} = x_k + \\alpha_k d_k$$\n其中搜索方向为 $d_k = -\\nabla f(x_k) = b - Qx_k$。\n\n步长 $\\alpha_k$ 通过精确线搜索选择，即我们找到一个 $\\alpha \\ge 0$ 来最小化一维函数 $\\phi(\\alpha) = f(x_k + \\alpha d_k)$。\n$$ \\phi(\\alpha) = \\frac{1}{2}(x_k + \\alpha d_k)^\\top Q (x_k + \\alpha d_k) - b^\\top(x_k + \\alpha d_k) $$\n为了找到最小值，我们将关于 $\\alpha$ 的导数设为零：\n$$ \\phi'(\\alpha) = (Q(x_k + \\alpha d_k) - b)^\\top d_k = (Qx_k - b + \\alpha Q d_k)^\\top d_k = 0 $$\n$$ (\\nabla f(x_k) + \\alpha Q d_k)^\\top d_k = 0 $$\n代入 $d_k = -\\nabla f(x_k)$：\n$$ (\\nabla f(x_k) - \\alpha Q \\nabla f(x_k))^\\top (-\\nabla f(x_k)) = 0 $$\n这可以简化为 $-\\nabla f(x_k)^\\top \\nabla f(x_k) + \\alpha \\nabla f(x_k)^\\top Q \\nabla f(x_k) = 0$。解出 $\\alpha_k$ 可得：\n$$ \\alpha_k = \\frac{\\nabla f(x_k)^\\top \\nabla f(x_k)}{\\nabla f(x_k)^\\top Q \\nabla f(x_k)} $$\n由于 $Q$ 是对称正定的，分母为正，从而保证 $\\alpha_k  0$，除非 $\\nabla f(x_k) = 0$。\n\n对于任何可微函数，精确线搜索的一个关键性质是，新点的梯度与到达该点所用的搜索方向正交，即 $\\nabla f(x_{k+1})^\\top d_k = 0$。对于最速下降法，这意味着 $\\nabla f(x_{k+1})^\\top \\nabla f(x_k) = 0$。连续梯度之间的这种正交性导致了在狭窄山谷中（对应于病态矩阵 $Q$）特有的“之”字形行为，从而导致收敛缓慢。\n\n**2. 共轭梯度法（CG）**\n\nCG 法是求解 $Qx=b$ 的一种更复杂的算法。它不是重复使用最速下降方向，而是构造一系列 $Q$-正交（或“共轭”）的搜索方向 $p_0, p_1, \\dots$。此性质确保在精确算术下，一个 $n$ 维二次函数的最值能在至多 $n$ 次迭代内找到。\n\n算法如下：\n1. 初始化: $x_0$, $r_0 = b - Qx_0$, $p_0 = r_0$, $k=0$。\n2. 迭代直至收敛：\n   a. 计算步长：$\\alpha_k = \\frac{r_k^\\top r_k}{p_k^\\top Q p_k}$。\n   b. 更新解：$x_{k+1} = x_k + \\alpha_k p_k$。\n   c. 更新残差：$r_{k+1} = r_k - \\alpha_k Q p_k$。\n   d. 计算下一个方向的改进因子：$\\beta_{k+1} = \\frac{r_{k+1}^\\top r_{k+1}}{r_k^\\top r_k}$。\n   e. 更新搜索方向：$p_{k+1} = r_{k+1} + \\beta_{k+1} p_k$。\n   f. 递增 $k$。\n\nCG 通过确保每个新方向 $p_{k+1}$ 都校正了先前步骤引入的不利分量，从而避免了最速下降法的“之”字形行为，使得在病态问题上收敛速度快得多。\n\n**3. 实验测量**\n\n该数值实验旨在凸显这种性能差异。\n- **矩阵构造**: 矩阵 $Q = R(\\theta)^\\top D R(\\theta)$（其中 $D=\\operatorname{diag}(\\lambda_1, \\lambda_2)$）定义了二次函数的Hessian矩阵。$Q$ 的特征值为 $\\lambda_1, \\lambda_2$。条件数 $\\kappa(Q) = \\lambda_{max}/\\lambda_{min}$ 量化了 $f(x)$ 等值线的“拉伸”程度。大的条件数意味着一个狭窄的山谷，最速下降法在此会遇到困难。旋转矩阵 $R(\\theta)$ 决定了该山谷在平面中的朝向。\n- **角度测量**: 我们测量前两个最速下降方向 $d_0 = -\\nabla f(x_0)$ 和 $d_1 = -\\nabla f(x_1)$ 之间的夹角 $\\psi$。一个接近 $90^\\circ$ 的值直接证实了“之”字形行为。夹角通过点积计算：$$ \\psi = \\arccos\\left(\\frac{d_0^\\top d_1}{\\lVert d_0 \\rVert_2 \\lVert d_1 \\rVert_2}\\right) $$ 结果从弧度转换为度。如果算法在少于两步内收敛（即在 $k=0$ 或 $k=1$ 时），意味着 $d_1$ 未被计算，此时夹角报告为 $0.0$。", "answer": "```python\nimport numpy as np\n\ndef steepest_descent(Q, b, x0, tol, max_iter):\n    \"\"\"\n    Solves min(0.5*x.T*Q*x - b.T*x) using steepest descent with exact line search.\n\n    Returns:\n        k (int): Number of iterations.\n        d_hist (list): History of the first two search directions.\n    \"\"\"\n    x = x0.copy()\n    d_hist = []\n\n    for k in range(max_iter + 1):\n        grad = Q @ x - b\n        grad_norm = np.linalg.norm(grad)\n\n        if grad_norm = tol:\n            return k, d_hist\n\n        if k == max_iter:\n            break\n\n        # Store the first two non-zero search directions\n        if k  2 and grad_norm > 1e-15: # Avoid storing near-zero vectors\n            d_hist.append(-grad)\n\n        # Exact line search for quadratic function\n        alpha = (grad.T @ grad) / (grad.T @ Q @ grad)\n        \n        # Update step\n        x = x - alpha * grad\n        \n    return max_iter, d_hist\n\ndef conjugate_gradient(Q, b, x0, tol, max_iter):\n    \"\"\"\n    Solves Qx=b using the Conjugate Gradient method.\n\n    Returns:\n        k (int): Number of iterations.\n    \"\"\"\n    x = x0.copy()\n    r = b - Q @ x\n    \n    # Check initial guess\n    if np.linalg.norm(r) = tol:\n        return 0\n    \n    p = r.copy()\n    rs_old = r.T @ r\n\n    # CG algorithm allows at most n iterations for an n x n system\n    # We loop for max_iter, which might be less than n=2.\n    for k in range(max_iter):\n        Ap = Q @ p\n        alpha = rs_old / (p.T @ Ap)\n        \n        x = x + alpha * p\n        r = r - alpha * Ap\n\n        if np.linalg.norm(r) = tol:\n            return k + 1\n            \n        rs_new = r.T @ r\n        # Fletcher-Reeves update for beta\n        beta = rs_new / rs_old\n        p = r + beta * p\n        rs_old = rs_new\n        \n    return max_iter\n\ndef solve():\n    \"\"\"\n    Main function to run the numerical experiment and print results.\n    \"\"\"\n    test_cases = [\n        # Case 1: Rotated narrow valley\n        {'lambda1': 1.0, 'lambda2': 1000.0, 'theta_deg': 30.0, \n         'b': np.array([1.0, -1.0]), 'x0': np.array([2.0, 2.0]), \n         'tol': 1e-10, 'max_iter_sd': 10000, 'max_iter_cg': 100},\n        # Case 2: Axis-aligned narrow valley\n        {'lambda1': 1.0, 'lambda2': 1000.0, 'theta_deg': 0.0, \n         'b': np.array([1.0, 0.5]), 'x0': np.array([-3.0, 3.0]), \n         'tol': 1e-10, 'max_iter_sd': 10000, 'max_iter_cg': 100},\n        # Case 3: Extremely elongated and rotated\n        {'lambda1': 1.0, 'lambda2': 1e6, 'theta_deg': 45.0, \n         'b': np.array([0.5, -0.5]), 'x0': np.array([1.0, -1.0]), \n         'tol': 1e-10, 'max_iter_sd': 10000, 'max_iter_cg': 100},\n    ]\n\n    all_results = []\n\n    for case in test_cases:\n        # Unpack case parameters\n        l1, l2 = case['lambda1'], case['lambda2']\n        theta_rad = np.deg2rad(case['theta_deg'])\n        b, x0 = case['b'], case['x0']\n        tol = case['tol']\n        max_iter_sd, max_iter_cg = case['max_iter_sd'], case['max_iter_cg']\n\n        # Construct matrix Q\n        c, s = np.cos(theta_rad), np.sin(theta_rad)\n        R = np.array([[c, -s], [s, c]])\n        D = np.diag([l1, l2])\n        Q = R.T @ D @ R\n\n        # Run Steepest Descent\n        k_sd, d_hist = steepest_descent(Q, b, x0, tol, max_iter_sd)\n        \n        # Run Conjugate Gradient\n        k_cg = conjugate_gradient(Q, b, x0, tol, max_iter_cg)\n\n        # Calculate angle between first two SD steps\n        angle_deg = 0.0\n        if len(d_hist) >= 2:\n            d0, d1 = d_hist[0], d_hist[1]\n            cos_angle = (d0.T @ d1) / (np.linalg.norm(d0) * np.linalg.norm(d1))\n            # Clip to handle potential floating point inaccuracies\n            cos_angle = np.clip(cos_angle, -1.0, 1.0)\n            angle_rad = np.arccos(cos_angle)\n            angle_deg = np.rad2deg(angle_rad)\n\n        # Compare iteration counts\n        sd_slower = k_sd > k_cg\n\n        # Aggregate results for this case\n        all_results.extend([k_sd, k_cg, angle_deg, sd_slower])\n\n    # Format the final output string\n    # Booleans are automatically converted to 'True'/'False' by str()\n    # Floats and ints are converted as expected.\n    output_str = f\"[{','.join(map(str, all_results))}]\"\n    print(output_str)\n\nsolve()\n```", "id": "3195724"}, {"introduction": "在掌握了无约束优化的基本方法后，我们自然会问：如何处理带有约束的优化问题？许多现实世界的问题都包含变量的边界限制，例如物理量不能为负。这个练习将介绍一种处理此类“箱型约束”的简洁而直观的算法——投影梯度法。其核心思想非常简单：像解决无约束问题一样沿梯度方向走一步，如果超出了可行域，就将结果投影回最近的可行点。通过实现该算法，你不仅能掌握一种基本的约束优化技术，还能学会识别和追踪在优化过程中哪些约束变成了“有效约束”，这是约束优化中的一个关键概念。[@problem_id:3195673]", "problem": "你需要实现并分析用于求解带箱式约束的凸二次目标函数的投影梯度法。考虑由 $f(x)=\\tfrac{1}{2}\\lVert A x-b\\rVert_2^2$ 定义的无约束最小二乘目标函数 $f:\\mathbb{R}^n\\to\\mathbb{R}$，其中 $A\\in\\mathbb{R}^{m\\times n}$ 且 $b\\in\\mathbb{R}^m$。对于给定的 $l,u\\in\\mathbb{R}^n$（其中对所有索引 $i$ 都有 $l_i\\le u_i$），施加逐分量的箱式约束 $l\\le x\\le u$。可行集是超矩形（箱体）$[l,u]=\\{x\\in\\mathbb{R}^n\\mid l\\le x\\le u\\}$。请使用下面列出的基本事实和定义作为唯一的出发点。\n\n要使用的基本定义：\n- $f(x)=\\tfrac{1}{2}\\lVert A x-b\\rVert_2^2$ 的梯度为 $\\nabla f(x)=A^\\top(Ax-b)$。\n- 到箱体 $[l,u]$ 上的欧几里得投影是逐分量裁剪算子 $P_{[l,u]}(y)$，其分量为 $[P_{[l,u]}(y)]_i=\\min\\{\\max\\{y_i,l_i\\},u_i\\}$。\n- 在点 $x$ 处的投影梯度向量为 $g_P(x)=x-P_{[l,u]}(x-\\nabla f(x))$。\n- 对于上述凸二次函数，其梯度的一个标准利普希茨常数为 $L=\\lVert A\\rVert_2^2$，即 $A$ 的谱范数的平方。\n\n任务：\n1. 从一个可行点 $x^{(0)}$ 开始，使用固定步长 $\\alpha=\\tfrac{1}{L}$（其中 $L=\\lVert A\\rVert_2^2$）实现投影梯度法，以最小化 $[l,u]$ 上的 $f(x)$。在第 $k$ 次迭代时，执行更新\n   $$x^{(k+1)}=P_{[l,u]}\\bigl(x^{(k)}-\\alpha\\,\\nabla f(x^{(k)})\\bigr).$$\n   当投影梯度的无穷范数满足 $\\lVert g_P(x^{(k)})\\rVert_\\infty\\le \\varepsilon$（其中 $\\varepsilon0$ 是给定的容差），或达到最大迭代次数时停止。使用 $\\varepsilon=10^{-8}$ 和最大 $10{,}000$ 次迭代。在比较 $x_i$ 与 $l_i$ 或 $u_i$ 时，使用有效集容差 $\\varepsilon_{\\mathrm{act}}=10^{-8}$。\n\n2. 在每次迭代 $k$ 时，定义有效集\n   - 下界有效集 $\\,\\mathcal{A}_\\ell^{(k)}=\\{\\,i\\mid x_i^{(k)}\\le l_i+\\varepsilon_{\\mathrm{act}}\\,\\}$，\n   - 上界有效集 $\\,\\mathcal{A}_u^{(k)}=\\{\\,i\\mid x_i^{(k)}\\ge u_i-\\varepsilon_{\\mathrm{act}}\\,\\}$。\n   统计在所有迭代中有效集并集发生变化的次数，即，计算满足 $(\\mathcal{A}_\\ell^{(k)}\\cup\\mathcal{A}_u^{(k)})\\neq(\\mathcal{A}_\\ell^{(k-1)}\\cup\\mathcal{A}_u^{(k-1)})$ 的 $k\\ge 1$ 的数量。\n\n3. 对于下方的每个测试用例，返回包含以下结果的元组：\n   - 最终目标函数值 $f(x^{(k)})$，四舍五入到 $6$ 位小数，\n   - 最终投影梯度无穷范数 $\\lVert g_P(x^{(k)})\\rVert_\\infty$，四舍五入到 $8$ 位小数，\n   - 执行的总迭代次数（整数），\n   - 最终下界有效索引列表（按升序排序），\n   - 最终上界有效索引列表（按升序排序），\n   - 有效集变化次数（整数）。\n   索引是从零开始的。\n\n测试和覆盖范围：\n使用以下五个测试用例，它们共同覆盖了常规内部解、有下界和上界约束起作用的解、存在 $l_i=u_i$ 的固定变量以及常数目标函数的情况。\n\n- 测试 $1$（预期为内部解）：\n  $$A=\\begin{bmatrix}2  0\\\\0  1\\end{bmatrix},\\quad b=\\begin{bmatrix}1\\\\2\\end{bmatrix},\\quad l=\\begin{bmatrix}-10\\\\-10\\end{bmatrix},\\quad u=\\begin{bmatrix}10\\\\10\\end{bmatrix},\\quad x^{(0)}=\\begin{bmatrix}0\\\\0\\end{bmatrix}.$$\n\n- 测试 $2$（预期下界和上界均有效）：\n  $$A=\\begin{bmatrix}1  0\\\\0  1\\end{bmatrix},\\quad b=\\begin{bmatrix}-1\\\\3\\end{bmatrix},\\quad l=\\begin{bmatrix}0\\\\0\\end{bmatrix},\\quad u=\\begin{bmatrix}2\\\\2\\end{bmatrix},\\quad x^{(0)}=\\begin{bmatrix}1\\\\1\\end{bmatrix}.$$\n\n- 测试 $3$（至少一个坐标上界有效）：\n  $$A=\\begin{bmatrix}1  0\\\\0  1\\end{bmatrix},\\quad b=\\begin{bmatrix}10\\\\1\\end{bmatrix},\\quad l=\\begin{bmatrix}-5\\\\-1\\end{bmatrix},\\quad u=\\begin{bmatrix}1\\\\2\\end{bmatrix},\\quad x^{(0)}=\\begin{bmatrix}0\\\\0\\end{bmatrix}.$$\n\n- 测试 $4$（固定变量，$l_1=u_1$）：\n  $$A=\\begin{bmatrix}1  1\\\\0  1\\end{bmatrix},\\quad b=\\begin{bmatrix}1\\\\0\\end{bmatrix},\\quad l=\\begin{bmatrix}0\\\\-1\\end{bmatrix},\\quad u=\\begin{bmatrix}0\\\\1\\end{bmatrix},\\quad x^{(0)}=\\begin{bmatrix}0\\\\0\\end{bmatrix}.$$\n\n- 测试 $5$（常数目标函数；零矩阵）：\n  $$A=\\begin{bmatrix}0  0\\\\0  0\\end{bmatrix},\\quad b=\\begin{bmatrix}1\\\\-2\\end{bmatrix},\\quad l=\\begin{bmatrix}-1\\\\-1\\end{bmatrix},\\quad u=\\begin{bmatrix}1\\\\1\\end{bmatrix},\\quad x^{(0)}=\\begin{bmatrix}0.25\\\\-0.5\\end{bmatrix}.$$\n\n最终输出格式：\n你的程序应生成单行输出，其中包含五个测试的结果，格式为用方括号括起来的逗号分隔列表，每个测试的结果都是一个列表，其顺序如上所述。确切格式必须是\n$$\\bigl[\\,[f_1,\\;p_1,\\;k_1,\\;L_1,\\;U_1,\\;c_1],\\;[f_2,\\;p_2,\\;k_2,\\;L_2,\\;U_2,\\;c_2],\\;[f_3,\\;p_3,\\;k_3,\\;L_3,\\;U_3,\\;c_3],\\;[f_4,\\;p_4,\\;k_4,\\;L_4,\\;U_4,\\;c_4],\\;[f_5,\\;p_5,\\;k_5,\\;L_5,\\;U_5,\\;c_5]\\bigr],$$\n其中 $f_i$ 和 $p_i$ 是按规定四舍五入的浮点数，$k_i$ 和 $c_i$ 是整数，$L_i,U_i$ 是整数列表。程序不得读取任何输入，并且必须在内部使用上述测试套件。", "solution": "用户要求实现并分析一种用于求解带箱式约束的凸二次目标函数的投影梯度法。在提供解决方案之前，需要对问题的正确性和可解性进行验证。\n\n### 问题验证\n\n**步骤 1：提取已知条件**\n\n- **目标函数：** $f(x) = \\frac{1}{2} \\lVert A x - b \\rVert_2^2$，其中 $A \\in \\mathbb{R}^{m \\times n}$ 且 $b \\in \\mathbb{R}^m$。\n- **约束：** 由给定的 $l, u \\in \\mathbb{R}^n$（其中 $l_i \\le u_i$）定义的箱式约束 $l \\le x \\le u$。可行集是超矩形 $[l, u]$。\n- **梯度：** $\\nabla f(x) = A^\\top(A x - b)$。\n- **投影算子：** 到 $[l,u]$ 上的欧几里得投影是 $P_{[l,u]}(y)$，其分量定义为 $[P_{[l,u]}(y)]_i = \\min\\{\\max\\{y_i, l_i\\}, u_i\\}$。\n- **投影梯度向量：** $g_P(x) = x - P_{[l,u]}(x - \\nabla f(x))$。\n- **利普希茨常数：** 梯度 $\\nabla f(x)$ 的利普希茨常数被指定为 $L = \\lVert A \\rVert_2^2$，其中 $\\lVert \\cdot \\rVert_2$ 是谱范数。\n- **算法：** 投影梯度法，更新规则为 $x^{(k+1)} = P_{[l,u]}(x^{(k)} - \\alpha \\nabla f(x^{(k)}))$，固定步长为 $\\alpha = \\frac{1}{L}$。\n- **初始点：** 每个测试用例都提供了一个可行点 $x^{(0)}$。\n- **停止准则：**\n    1.  投影梯度的无穷范数：$\\lVert g_P(x^{(k)}) \\rVert_\\infty \\le \\varepsilon$，容差 $\\varepsilon = 10^{-8}$。\n    2.  最大迭代次数：$10,000$。\n- **有效集定义：** 对于迭代点 $x^{(k)}$ 和有效集容差 $\\varepsilon_{\\mathrm{act}} = 10^{-8}$：\n    - 下界有效集：$\\mathcal{A}_\\ell^{(k)} = \\{\\,i \\mid x_i^{(k)} \\le l_i + \\varepsilon_{\\mathrm{act}}\\,\\}$。\n    - 上界有效集：$\\mathcal{A}_u^{(k)} = \\{\\,i \\mid x_i^{(k)} \\ge u_i - \\varepsilon_{\\mathrm{act}}\\,\\}$。\n- **有效集变化计数：** 有效集并集发生变化的迭代次数 $k \\ge 1$，即 $(\\mathcal{A}_\\ell^{(k)} \\cup \\mathcal{A}_u^{(k)}) \\neq (\\mathcal{A}_\\ell^{(k-1)} \\cup \\mathcal{A}_u^{(k-1)})$。\n- **测试用例：** 提供了五个特定的 $(A, b, l, u, x^{(0)})$ 实例。\n- **要求输出：** 对于每个测试用例，要求返回一个包含六个特定值的元组，其中浮点数需按规定进行四舍五入。\n\n**步骤 2：使用提取的已知条件进行验证**\n\n- **科学依据：** 该问题是带箱式约束的二次规划的标准实例，这是数学优化中的一个基本课题。目标函数 $f(x)$ 是凸函数，因为其海森矩阵 $\\nabla^2 f(x) = A^\\top A$ 是半正定的。可行集 $[l,u]$ 是紧致且凸的。投影梯度法是解决此类问题的一种成熟的一阶算法。所提供的梯度、投影和利普希茨常数的定义都是标准且正确的。\n- **适定性：** 由于是在一个非空紧集 ($[l,u]$) 上最小化一个连续凸函数 ($f(x)$)，因此保证存在最小值点。指定的算法使用固定步长 $\\alpha = 1/L$，保证收敛到问题的一个驻点。因此，一个有意义的解是存在的，并且可以通过所提出的方法找到。\n- **客观性：** 该问题使用精确、无歧义的数学语言和定义进行陈述。\n- **完整性与一致性：** 每个测试用例都提供了所有必要的参数（$A, b, l, u, x^{(0)}$）、算法常数（$\\varepsilon, \\varepsilon_{\\mathrm{act}}$，最大迭代次数）和定义。条件 $l_i \\le u_i$ 已被陈述，并对所有测试数据成立。初始点 $x^{(0)}$ 被确认为可行点（即 $l \\le x^{(0)} \\le u$），满足了算法的一个先决条件。该问题是自洽且内部一致的。\n\n**步骤 3：结论与行动**\n\n该问题是 **有效的**。它在数值优化领域是一个定义明确、科学合理且完整的问题。我们可以着手构建解决方案。\n\n### 解法\n\n该问题要求实现投影梯度法来解决以下约束优化问题：\n$$ \\min_{x \\in \\mathbb{R}^n} f(x) = \\frac{1}{2} \\lVert Ax - b \\rVert_2^2 \\quad \\text{subject to} \\quad l \\le x \\le u $$\n该方法是一个迭代过程，它将标准的梯度下降算法推广到处理约束。每次迭代包括两个主要部分：一个梯度下降步和一个投影步。\n\n**1. 算法步骤**\n迭代更新由下式给出：\n$$ x^{(k+1)} = P_{[l,u]}\\left(x^{(k)} - \\alpha \\nabla f(x^{(k)})\\right) $$\n其中 $k$ 是迭代索引，$x^{(k)}$ 是当前迭代点，$\\nabla f(x^{(k)})$ 是目标函数在 $x^{(k)}$ 处的梯度，$\\alpha$ 是步长，$P_{[l,u]}$ 是到可行集上的投影算子。\n\n**2. 梯度计算**\n目标函数是 $f(x) = \\frac{1}{2}(Ax-b)^\\top(Ax-b)$。它关于 $x$ 的梯度被正确地提供为：\n$$ \\nabla f(x) = A^\\top(Ax-b) $$\n\n**3. 步长选择**\n为了使投影梯度法在固定步长下收敛，$\\alpha$ 必须在范围 $(0, 2/L)$ 内，其中 $L$ 是梯度 $\\nabla f(x)$ 的一个利普希茨常数。问题指定使用步长 $\\alpha = 1/L$，这是一个常用且能保证收敛的安全选择。利普希茨常数由 $A$ 的谱范数的平方给出，$L = \\lVert A \\rVert_2^2$。谱范数 $\\lVert A \\rVert_2$ 是 $A$ 的最大奇异值。在 $A=0$ 的特殊情况下，梯度恒为零，$L=0$，算法应立即终止，因为初始点已是最优解。为处理除以零的情况，我们可以检查 $L=0$ 并相应地处理。\n\n**4. 投影操作**\n投影 $P_{[l,u]}(y)$ 将点 $y \\in \\mathbb{R}^n$ 映射到箱体 $[l,u]$ 中在欧几里得范数意义下最近的点。此操作可以逐分量执行：\n$$ [P_{[l,u]}(y)]_i = \\text{clip}(y_i, l_i, u_i) = \\min\\{\\max\\{y_i, l_i\\}, u_i\\} $$\n\n**5. 停止准则**\n一个点 $x^*$ 是驻点（也是这个凸问题的解）当且仅当对于任意 $\\gamma  0$，它都满足一阶最优性条件 $x^* = P_{[l,u]}(x^* - \\gamma \\nabla f(x^*))$。问题定义了一个投影梯度向量 $g_P(x) = x - P_{[l,u]}(x - \\nabla f(x))$，这对应于步长 $\\gamma=1$ 时最优性条件的残差。当该向量的无穷范数 $\\lVert g_P(x^{(k)}) \\rVert_\\infty$ 小于容差 $\\varepsilon = 10^{-8}$ 时，算法终止。条件 $g_P(x^*) = 0$ 正确地识别了驻点。\n\n**6. 有效集跟踪**\n在每次迭代 $k$ 中，我们识别出那些处于或接近其下界或上界的变量所对应的索引集。这些集合的并集 $\\mathcal{S}^{(k)} = \\mathcal{A}_\\ell^{(k)} \\cup \\mathcal{A}_u^{(k)}$ 表示在第 $k$ 次迭代时的有效集。从一次迭代到下一次迭代，这个集合发生变化的次数将被计数。实现时将维护上一次迭代的有效集以进行此比较。\n\n**7. 实现计划**\n总体算法如下：\n对于每个测试用例：\n1.  初始化 $k=0$，$x = x^{(0)}$，以及 `active_set_changes = 0`。\n2.  计算 $L = \\lVert A \\rVert_2^2$。如果 $L0$，则设置 $\\alpha = 1/L$；否则，$\\alpha$ 可以设置为任何值，因为梯度将为零。\n3.  根据 $x$ 计算初始的有效集并集 $\\mathcal{S}_{\\text{prev}}$。\n4.  对 $k$ 从 $0$ 循环到 `max_iterations - 1`：\n    a. 计算梯度 $\\nabla f(x) = A^\\top(Ax-b)$。\n    b. 计算投影梯度向量 $g_P(x) = x - P_{[l,u]}(x - \\nabla f(x))$。\n    c. 如果 $\\lVert g_P(x) \\rVert_\\infty \\le \\varepsilon$，则算法已收敛。跳出循环。\n    d. 执行更新：$x_{\\text{next}} = P_{[l,u]}(x - \\alpha \\nabla f(x))$。\n    e. 计算新点的有效集并集 $\\mathcal{S}_{\\text{curr}}$。\n    f. 如果 $\\mathcal{S}_{\\text{curr}} \\neq \\mathcal{S}_{\\text{prev}}$，则增加 `active_set_changes`。\n    g. 为下一次迭代更新：$x = x_{\\text{next}}$ 且 $\\mathcal{S}_{\\text{prev}} = \\mathcal{S}_{\\text{curr}}$。\n    h. 增加 $k$。\n5.  循环终止后，使用最终的迭代点 $x$ 计算最终的目标函数值、投影梯度范数、有效集和总变化次数。\n6.  按指定格式格式化结果并存储它们。\n7.  处理完所有测试用例后，以要求的格式打印汇总结果。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and analyzes the projected gradient method for a convex quadratic\n    objective with box constraints, based on the provided problem description.\n    \"\"\"\n    \n    test_cases = [\n        # Test 1 (interior solution expected)\n        {\n            \"A\": np.array([[2.0, 0.0], [0.0, 1.0]]),\n            \"b\": np.array([1.0, 2.0]),\n            \"l\": np.array([-10.0, -10.0]),\n            \"u\": np.array([10.0, 10.0]),\n            \"x0\": np.array([0.0, 0.0]),\n        },\n        # Test 2 (both lower and upper activity expected)\n        {\n            \"A\": np.array([[1.0, 0.0], [0.0, 1.0]]),\n            \"b\": np.array([-1.0, 3.0]),\n            \"l\": np.array([0.0, 0.0]),\n            \"u\": np.array([2.0, 2.0]),\n            \"x0\": np.array([1.0, 1.0]),\n        },\n        # Test 3 (upper activity on at least one coordinate)\n        {\n            \"A\": np.array([[1.0, 0.0], [0.0, 1.0]]),\n            \"b\": np.array([10.0, 1.0]),\n            \"l\": np.array([-5.0, -1.0]),\n            \"u\": np.array([1.0, 2.0]),\n            \"x0\": np.array([0.0, 0.0]),\n        },\n        # Test 4 (fixed variable with l_1=u_1)\n        {\n            \"A\": np.array([[1.0, 1.0], [0.0, 1.0]]),\n            \"b\": np.array([1.0, 0.0]),\n            \"l\": np.array([0.0, -1.0]),\n            \"u\": np.array([0.0, 1.0]),\n            \"x0\": np.array([0.0, 0.0]),\n        },\n        # Test 5 (constant objective; zero matrix)\n        {\n            \"A\": np.array([[0.0, 0.0], [0.0, 0.0]]),\n            \"b\": np.array([1.0, -2.0]),\n            \"l\": np.array([-1.0, -1.0]),\n            \"u\": np.array([1.0, 1.0]),\n            \"x0\": np.array([0.25, -0.5]),\n        },\n    ]\n\n    eps = 1e-8\n    eps_act = 1e-8\n    max_iter = 10000\n    \n    all_results = []\n\n    def get_active_sets(x, l, u, tol):\n        lower_active_indices = np.where(x = l + tol)[0]\n        upper_active_indices = np.where(x >= u - tol)[0]\n        return frozenset(lower_active_indices), frozenset(upper_active_indices)\n\n    for case in test_cases:\n        A, b, l, u, x = case[\"A\"], case[\"b\"], case[\"l\"], case[\"u\"], case[\"x0\"].copy()\n        \n        # Calculate Lipschitz constant and step size\n        L = np.linalg.norm(A, 2)**2\n        if L > 0:\n            alpha = 1.0 / L\n        else:\n            # For A=0, L=0. Gradient is always 0. Any alpha works.\n            alpha = 1.0\n\n        active_set_changes = 0\n        \n        lower_prev, upper_prev = get_active_sets(x, l, u, eps_act)\n        union_prev = lower_prev.union(upper_prev)\n        \n        num_iter = 0\n        for k in range(max_iter):\n            # Gradient of f(x) = 1/2 ||Ax-b||^2 is A^T(Ax-b)\n            grad = A.T @ (A @ x - b)\n            \n            # Stopping criterion: ||g_P(x)||_inf = eps\n            # g_P(x) = x - P_{[l,u]}(x - grad)\n            # Projection uses a step of 1, as per problem definition\n            g_p = x - np.clip(x - grad, l, u)\n            norm_g_p = np.linalg.norm(g_p, np.inf)\n            \n            if norm_g_p = eps:\n                break\n\n            # Projected gradient update step\n            x_next = np.clip(x - alpha * grad, l, u)\n            \n            # Check for active set change\n            lower_curr, upper_curr = get_active_sets(x_next, l, u, eps_act)\n            union_curr = lower_curr.union(upper_curr)\n            \n            if union_curr != union_prev:\n                active_set_changes += 1\n                \n            x = x_next\n            union_prev = union_curr\n            num_iter = k + 1\n\n        # Final calculations\n        final_f = 0.5 * np.linalg.norm(A @ x - b)**2\n        \n        # Recalculate final projected gradient norm for consistency\n        final_grad = A.T @ (A @ x - b)\n        final_g_p = x - np.clip(x - final_grad, l, u)\n        final_norm_g_p = np.linalg.norm(final_g_p, np.inf)\n\n        final_lower_active, final_upper_active = get_active_sets(x, l, u, eps_act)\n\n        result_tuple = (\n            round(final_f, 6),\n            round(final_norm_g_p, 8),\n            int(num_iter),\n            sorted(list(final_lower_active)),\n            sorted(list(final_upper_active)),\n            int(active_set_changes)\n        )\n        all_results.append(result_tuple)\n\n    # Format output\n    result_strings = []\n    for res in all_results:\n        f_val, p_norm, k_iter, l_list, u_list, c_changes = res\n        # Python's default list-to-string conversion matches the required format\n        result_strings.append(\n            f\"[{f_val}, {p_norm}, {k_iter}, {l_list}, {u_list}, {c_changes}]\"\n        )\n    \n    print(f\"[{','.join(result_strings)}]\")\n\nsolve()\n```", "id": "3195673"}, {"introduction": "对于更一般的等式和不等式约束，简单的投影梯度法可能不再适用。此时，一种强大的策略是将有约束问题转化为一系列无约束子问题来求解，这便是罚函数法和障碍函数法的核心思想。本练习将引导你实现这两种方法：用于处理等式约束的二次罚函数法，以及用于处理不等式约束的对数障碍法。你将使用牛顿法求解这些子问题，并数值化地追踪解的轨迹（即“中心路径”），从而直观地观察算法如何逐步逼近真实的约束最优解。[@problem_id:3195614]", "problem": "考虑一个包含两个变量的约束优化问题，该问题定义为在满足一个等式约束和一个不等式约束的条件下，最小化一个二次目标函数。设目标函数为 $f:\\mathbb{R}^2\\to\\mathbb{R}$，由 $f(\\mathbf{x})=\\tfrac{1}{2}\\|\\mathbf{x}\\|_2^2$ 给出，其中 $\\mathbf{x}=(x_1,x_2)$。同时，问题包含等式约束 $h(\\mathbf{x})=x_1+x_2-1=0$ 和不等式约束 $g(\\mathbf{x})=x_1-0.2\\le 0$。该问题是严格凸的，并拥有唯一的最优解。\n\n推导的基本依据：\n- 针对含等式和不等式约束的优化问题，拉格朗日函数 $L(\\mathbf{x},\\lambda,\\nu)=f(\\mathbf{x})+\\lambda h(\\mathbf{x})+\\nu g(\\mathbf{x})$ 的定义。\n- Karush–Kuhn–Tucker (KKT) 条件：平稳性条件 $\\nabla f(\\mathbf{x}^\\star)+\\lambda^\\star \\nabla h(\\mathbf{x}^\\star)+\\nu^\\star \\nabla g(\\mathbf{x}^\\star)=\\mathbf{0}$、原始可行性条件 $h(\\mathbf{x}^\\star)=0$, $g(\\mathbf{x}^\\star)\\le 0$、对偶可行性条件 $\\nu^\\star\\ge 0$ 以及互补松弛性条件 $\\nu^\\star g(\\mathbf{x}^\\star)=0$。\n- 针对等式约束的二次罚函数法：对于参数 $\\mu0$，考虑最小化 $F_{\\mu}(\\mathbf{x})=f(\\mathbf{x})+\\tfrac{\\mu}{2}\\|h(\\mathbf{x})\\|_2^2$。当 $\\mu\\to\\infty$ 时，该问题逼近原等式约束问题。\n- 针对不等式约束的对数障碍法：对于参数 $t0$，考虑在严格可行集（其中 $g_i(\\mathbf{x})0$）上最小化 $F_{t}(\\mathbf{x})=f(\\mathbf{x})-\\tfrac{1}{t}\\sum_i \\log(-g_i(\\mathbf{x}))$。当 $t\\to\\infty$ 时，其最小化点构成的轨迹（中心路径）会逼近由有效约束定义的边界。\n\n任务：\n- 您必须实现一个程序，对于上述固定形式的 $f$、$h$ 和 $g$，在定义域 $g(\\mathbf{x})0$ 上，针对惩罚参数 $\\mu$ 和障碍参数 $t$ 的多个值，最小化组合的无约束惩罚目标函数\n$$\nF_{\\mu,t}(\\mathbf{x})=f(\\mathbf{x})+\\tfrac{\\mu}{2}\\big(h(\\mathbf{x})\\big)^2-\\tfrac{1}{t}\\log\\big(-g(\\mathbf{x})\\big)\n$$\n。包含 $\\mu$ 的项近似地强制执行等式约束；对数障碍项则强制执行不等式约束的严格可行性，并在 $t\\to\\infty$ 时逼近有效边界。\n\n- 基于上述基本依据，推导 $F_{\\mu,t}$ 的平稳性方程，并指定一个稳健的数值方法来求解这些方程，同时要确保解位于定义域 $g(\\mathbf{x})0$ 内。\n\n- 使用第一性原理计算原始约束问题的唯一最优解 $\\mathbf{x}^\\star$。\n\n- 比较两条轨迹：\n  1. 二次罚函数轨迹：固定 $t$ 于一个中等值，同时增加 $\\mu$；记录每个 $\\mu$ 值下 $F_{\\mu,t}$ 的最小化点。\n  2. 对数障碍中心路径轨迹：固定 $\\mu$ 于一个较大值，同时增加 $t$；记录每个 $t$ 值下 $F_{\\mu,t}$ 的最小化点。\n\n- 对每个测试用例，报告：\n  1. 计算得到的最小化点与真实约束解之间的欧几里得距离 $d=\\|\\mathbf{x}_{\\mu,t}-\\mathbf{x}^\\star\\|_2$。\n  2. 等式残差 $r=|h(\\mathbf{x}_{\\mu,t})|$。\n  3. 不等式违规量 $v=\\max\\{g(\\mathbf{x}_{\\mu,t}),0\\}$。\n  4. 计算得到的最小化点的第一个坐标 $x_1$，以数值方式显示当 $t\\to\\infty$ 时向不等式边界 $x_1=0.2$ 的趋近过程。\n\n测试套件：\n- 二次罚函数轨迹，固定障碍参数 $t=50.0$，惩罚参数 $\\mu\\in\\{0.0,1.0,10.0,100.0\\}$。\n- 对数障碍轨迹，固定惩罚参数 $\\mu=100.0$，障碍参数 $t\\in\\{0.5,2.0,10.0,1000.0\\}$。\n\n算法要求：\n- 使用带回溯线搜索的阻尼牛顿法，该方法需确保迭代点保持严格可行（即 $x_10.2$），并保证凸目标函数 $F_{\\mu,t}$ 的值下降。使用基于梯度范数小于一个很小容差的停止准则。\n- 从一个严格可行点开始初始化，例如 $\\mathbf{x}^{(0)}=(0,1)$，该点满足 $g(\\mathbf{x}^{(0)})=-0.20$。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含一个由逗号分隔的列表组成的列表，并用方括号括起。每个内部列表必须按相应测试用例的 $[d,r,v,x_1]$ 顺序排列。示例格式为 $[[d_1,r_1,v_1,x_{1,1}],[d_2,r_2,v_2,x_{1,2}],\\dots]$。\n- 所有值必须是浮点数。输出中不涉及角度或物理单位，也不出现百分比。", "solution": "用户希望使用二次罚函数和对数障碍的组合方法来解决一个约束优化问题。解决方案需要三个部分：首先，求出原始约束问题的解析解，作为基准；其次，为数值方法推导必要的方程；第三，实现该数值方法以计算指定的轨迹并报告结果。\n\n问题是在等式约束 $h(\\mathbf{x})=x_1+x_2-1=0$ 和不等式约束 $g(\\mathbf{x})=x_1-0.2\\le 0$ 的条件下，最小化目标函数 $f(\\mathbf{x})=\\tfrac{1}{2}\\|\\mathbf{x}\\|_2^2 = \\tfrac{1}{2}(x_1^2+x_2^2)$，其中 $\\mathbf{x}=(x_1, x_2) \\in \\mathbb{R}^2$。\n\n**1. 通过 KKT 条件求解析解**\n\n该问题是凸问题（严格凸的目标函数，仿射约束），因此 Karush-Kuhn-Tucker (KKT) 条件是其最优性的充分必要条件。拉格朗日函数为：\n$$L(\\mathbf{x},\\lambda,\\nu) = f(\\mathbf{x}) + \\lambda h(\\mathbf{x}) + \\nu g(\\mathbf{x}) = \\tfrac{1}{2}(x_1^2+x_2^2) + \\lambda(x_1+x_2-1) + \\nu(x_1-0.2)$$\n各函数的梯度为 $\\nabla f(\\mathbf{x}) = \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix}$，$\\nabla h(\\mathbf{x}) = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$，以及 $\\nabla g(\\mathbf{x}) = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$。\n\n对于最优解 $(\\mathbf{x}^\\star, \\lambda^\\star, \\nu^\\star)$ 的 KKT 条件如下：\n1.  **平稳性**：$\\nabla f(\\mathbf{x}^\\star) + \\lambda^\\star \\nabla h(\\mathbf{x}^\\star) + \\nu^\\star \\nabla g(\\mathbf{x}^\\star) = \\mathbf{0}$\n    $$x_1^\\star + \\lambda^\\star + \\nu^\\star = 0$$\n    $$x_2^\\star + \\lambda^\\star = 0$$\n2.  **原始可行性**：\n    $$x_1^\\star + x_2^\\star - 1 = 0$$\n    $$x_1^\\star - 0.2 \\le 0$$\n3.  **对偶可行性**：$\\nu^\\star \\ge 0$\n4.  **互补松弛性**：$\\nu^\\star (x_1^\\star - 0.2) = 0$\n\n我们根据互补松弛性条件分析两种情况。\n\n情况 1：不等式约束未激活，即 $x_1^\\star - 0.2  0$。\n互补松弛性条件意味着 $\\nu^\\star = 0$。平稳性方程简化为：\n$x_1^\\star + \\lambda^\\star = 0 \\implies x_1^\\star = -\\lambda^\\star$\n$x_2^\\star + \\lambda^\\star = 0 \\implies x_2^\\star = -\\lambda^\\star$\n因此，$x_1^\\star = x_2^\\star$。将此代入等式约束 $x_1^\\star + x_2^\\star - 1 = 0$ 得到 $2x_1^\\star = 1$，所以 $x_1^\\star = 0.5$。\n这意味着 $\\mathbf{x}^\\star = (0.5, 0.5)$。然而，这违反了不等式约束：$g(0.5, 0.5) = 0.5 - 0.2 = 0.3 \\not\\le 0$。因此，这种情况不会得到可行解。\n\n情况 2：不等式约束被激活，即 $x_1^\\star - 0.2 = 0$。\n这得到 $x_1^\\star = 0.2$。原始可行性条件 $x_1^\\star - 0.2 \\le 0$ 得到满足。根据等式约束 $x_1^\\star+x_2^\\star-1=0$，我们发现 $0.2 + x_2^\\star - 1 = 0$，从而得到 $x_2^\\star = 0.8$。候选解为 $\\mathbf{x}^\\star = (0.2, 0.8)$。\n现在我们求解对偶变量 $\\lambda^\\star$ 和 $\\nu^\\star$ 并检查其一致性。\n根据第二个平稳性方程：$x_2^\\star + \\lambda^\\star = 0 \\implies 0.8 + \\lambda^\\star = 0 \\implies \\lambda^\\star = -0.8$。\n根据第一个平稳性方程：$x_1^\\star + \\lambda^\\star + \\nu^\\star = 0 \\implies 0.2 + (-0.8) + \\nu^\\star = 0 \\implies \\nu^\\star = 0.6$。\n对偶可行性条件 $\\nu^\\star = 0.6 \\ge 0$ 得到满足。\n所有 KKT 条件均已满足。因此，唯一的最优解是 $\\mathbf{x}^\\star = (0.2, 0.8)$。\n\n**2. 惩罚目标函数及平稳性方程**\n\n该问题通过最小化以下无约束目标函数来近似求解：\n$$F_{\\mu,t}(\\mathbf{x}) = f(\\mathbf{x}) + \\tfrac{\\mu}{2}\\big(h(\\mathbf{x})\\big)^2 - \\tfrac{1}{t}\\log\\big(-g(\\mathbf{x})\\big)$$\n代入给定的函数：\n$$F_{\\mu,t}(x_1, x_2) = \\tfrac{1}{2}(x_1^2+x_2^2) + \\tfrac{\\mu}{2}(x_1+x_2-1)^2 - \\tfrac{1}{t}\\log(0.2-x_1)$$\n该函数的定义域为 $g(\\mathbf{x})  0$，即 $x_1  0.2$。函数 $F_{\\mu,t}$ 是严格凸的，因为它是一个严格凸函数 ($f$)、一个凸函数（二次罚函数）和另一个凸函数（对数障碍函数）的和。其最小化点是唯一的，可通过将其梯度设为零来找到。\n\n梯度 $\\nabla F_{\\mu,t}(\\mathbf{x})$ 为：\n$$\\frac{\\partial F_{\\mu,t}}{\\partial x_1} = x_1 + \\mu(x_1+x_2-1) \\cdot (1) - \\frac{1}{t} \\frac{-1}{0.2-x_1} = x_1 + \\mu(x_1+x_2-1) + \\frac{1}{t(0.2-x_1)}$$\n$$\\frac{\\partial F_{\\mu,t}}{\\partial x_2} = x_2 + \\mu(x_1+x_2-1) \\cdot (1) = x_2 + \\mu(x_1+x_2-1)$$\n平稳性条件由非线性方程组 $\\nabla F_{\\mu,t}(\\mathbf{x}) = \\mathbf{0}$ 给出。\n\n**3. 通过阻尼牛顿法进行数值求解**\n\n为求解 $\\nabla F_{\\mu,t}(\\mathbf{x}) = \\mathbf{0}$，我们采用牛顿法。这需要 $F_{\\mu,t}$ 的海森矩阵，即梯度系统的雅可比矩阵。\n海森矩阵 $\\nabla^2 F_{\\mu,t}(\\mathbf{x})$ 为：\n$$\\frac{\\partial^2 F_{\\mu,t}}{\\partial x_1^2} = 1 + \\mu + \\frac{1}{t(0.2-x_1)^2}$$\n$$\\frac{\\partial^2 F_{\\mu,t}}{\\partial x_2 \\partial x_1} = \\frac{\\partial^2 F_{\\mu,t}}{\\partial x_1 \\partial x_2} = \\mu$$\n$$\\frac{\\partial^2 F_{\\mu,t}}{\\partial x_2^2} = 1 + \\mu$$\n海森矩阵是：\n$$\\mathbf{H}_F(\\mathbf{x}) = \\nabla^2 F_{\\mu,t}(\\mathbf{x}) = \\begin{pmatrix} 1 + \\mu + \\frac{1}{t(0.2-x_1)^2}  \\mu \\\\ \\mu  1+\\mu \\end{pmatrix}$$\n牛顿迭代为 $\\mathbf{x}^{(k+1)} = \\mathbf{x}^{(k)} + s \\Delta \\mathbf{x}^{(k)}$，其中牛顿步长 $\\Delta \\mathbf{x}^{(k)}$ 是线性方程组 $\\mathbf{H}_F(\\mathbf{x}^{(k)}) \\Delta \\mathbf{x}^{(k)} = -\\nabla F_{\\mu,t}(\\mathbf{x}^{(k)})$ 的解。\n\n使用带回溯线搜索的阻尼牛顿法来确保全局收敛和严格可行性。在每次迭代 $k$ 中：\n1.  计算梯度 $\\mathbf{g}^{(k)} = \\nabla F_{\\mu,t}(\\mathbf{x}^{(k)})$ 和海森矩阵 $\\mathbf{H}^{(k)} = \\mathbf{H}_F(\\mathbf{x}^{(k)})$。\n2.  求解 $\\mathbf{H}^{(k)} \\Delta \\mathbf{x}^{(k)} = -\\mathbf{g}^{(k)}$ 以获得牛顿步长 $\\Delta \\mathbf{x}^{(k)}$。\n3.  执行回溯线搜索以找到步长 $s \\in (0,1]$。从 $s=1$ 开始。\n    a.  首先，确保严格可行性：当 $x_1^{(k)} + s\\Delta x_1^{(k)} \\ge 0.2$ 时，将 $s$ 乘以一个因子 $\\beta \\in (0,1)$ 来减小 $s$，例如 $s \\leftarrow \\beta s$。\n    b.  接着，确保充分下降（Armijo 条件）：对于某个 $\\alpha \\in (0,0.5)$，当 $F_{\\mu,t}(\\mathbf{x}^{(k)} + s\\Delta\\mathbf{x}^{(k)})  F_{\\mu,t}(\\mathbf{x}^{(k)}) + \\alpha s (\\mathbf{g}^{(k)})^T \\Delta\\mathbf{x}^{(k)}$ 时，将 $s$ 乘以因子 $\\beta$ 来减小 $s$，即 $s \\leftarrow \\beta s$。\n4.  更新迭代点：$\\mathbf{x}^{(k+1)} = \\mathbf{x}^{(k)} + s \\Delta \\mathbf{x}^{(k)}$。\n该过程从严格可行点 $\\mathbf{x}^{(0)}=(0,1)$ 开始初始化，并在梯度范数 $\\|\\mathbf{g}^{(k)}\\|_2$ 小于一个很小的容差时终止。\n\n对测试套件中的每对参数 $(\\mu, t)$ 执行此程序。对于每个计算出的最小化点 $\\mathbf{x}_{\\mu,t}$，我们计算其与真实解的欧几里得距离 $d=\\|\\mathbf{x}_{\\mu,t}-\\mathbf{x}^\\star\\|_2$、等式约束残差 $r=|h(\\mathbf{x}_{\\mu,t})|$、不等式约束违规量 $v=\\max\\{g(\\mathbf{x}_{\\mu,t}),0\\}$（由于障碍法，该值将为 0）以及第一个坐标 $x_1$。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the constrained optimization problem using a combined penalty and barrier method.\n    Derives the analytical solution, sets up the numerical solver, and computes results for test cases.\n    \"\"\"\n    \n    # The true optimal solution derived from KKT conditions.\n    x_star = np.array([0.2, 0.8])\n\n    # ===== Define Problem Functions =====\n    \n    def h_func(x):\n        \"\"\"Equality constraint h(x) = x1 + x2 - 1.\"\"\"\n        return x[0] + x[1] - 1.0\n    \n    def g_func(x):\n        \"\"\"Inequality constraint g(x) = x1 - 0.2.\"\"\"\n        return x[0] - 0.2\n\n    # ===== Penalized/Barrier Objective and its Derivatives =====\n\n    def F_obj(x, mu, t):\n        \"\"\"Combined objective F(x; mu, t).\"\"\"\n        if g_func(x) >= 0:\n            return np.inf  # Enforce strict feasibility\n        \n        f_val = 0.5 * np.dot(x, x)\n        h_val = h_func(x)\n        g_val = g_func(x)\n        \n        # log(-g(x)) = log(0.2 - x1)\n        barrier_term = (1.0 / t) * np.log(-g_val)\n        penalty_term = (mu / 2.0) * h_val**2\n        \n        return f_val + penalty_term - barrier_term\n\n    def F_grad(x, mu, t):\n        \"\"\"Gradient of the combined objective F.\"\"\"\n        grad = np.zeros(2)\n        h_val = h_func(x)\n        \n        # grad(f) = x\n        # grad(penalty) = mu*h(x)*grad(h) = mu*(x1+x2-1)*[1, 1]\n        # grad(-log barrier) = - (1/t) * (1/(-g(x))) * (-grad(g)) = 1/(t*(-g(x))) * [1, 0]\n        \n        grad[0] = x[0] + mu * h_val + (1.0 / t) * (1.0 / (0.2 - x[0]))\n        grad[1] = x[1] + mu * h_val\n        return grad\n\n    def F_hess(x, mu, t):\n        \"\"\"Hessian of the combined objective F.\"\"\"\n        hess = np.zeros((2, 2))\n        g_val = g_func(x)\n        \n        # H(f) = I\n        # H(penalty) = mu * grad(h) * grad(h)^T = mu * [[1, 1], [1, 1]]\n        # H(-log barrier) = d/dx(1/(t*(0.2-x1))) = [[1/(t*(0.2-x1)^2), 0], [0, 0]]\n        \n        hess[0, 0] = 1.0 + mu + (1.0 / t) / ((-g_val)**2)\n        hess[0, 1] = mu\n        hess[1, 0] = mu\n        hess[1, 1] = 1.0 + mu\n        return hess\n\n    # ===== Numerical Solver =====\n\n    def newton_solver(mu, t):\n        \"\"\"\n        Damped Newton method with backtracking line search to minimize F(x; mu, t).\n        \"\"\"\n        x = np.array([0.0, 1.0])  # Initial point is strictly feasible\n        \n        # Solver parameters\n        tol = 1e-9\n        alpha = 0.25 # for Armijo condition\n        beta = 0.5   # step size reduction factor\n        max_iter = 100\n\n        for _ in range(max_iter):\n            grad = F_grad(x, mu, t)\n            \n            if np.linalg.norm(grad) = tol:\n                break\n            \n            hess = F_hess(x, mu, t)\n            \n            # Newton step: H * p = -g\n            p = np.linalg.solve(hess, -grad)\n            \n            # Backtracking line search\n            s = 1.0\n            \n            # 1. Ensure strict feasibility (stay within domain x1  0.2)\n            while x[0] + s * p[0] >= 0.2:\n                s *= beta\n                if s = 1e-15:\n                    s = 0.0\n                    break\n            \n            # 2. Ensure sufficient descent (Armijo-Goldstein condition)\n            if s > 0.0:\n              f_current = F_obj(x, mu, t)\n              grad_dot_p = np.dot(grad, p)\n              while F_obj(x + s * p, mu, t) > f_current + alpha * s * grad_dot_p:\n                  s *= beta\n                  if s = 1e-15:\n                      s = 0.0\n                      break\n            \n            x = x + s * p\n            \n        return x\n\n    # ===== Test Suite Execution =====\n    \n    test_cases = [\n        # Quadratic penalty trajectory with fixed t=50.0\n        (0.0, 50.0),\n        (1.0, 50.0),\n        (10.0, 50.0),\n        (100.0, 50.0),\n        # Log barrier trajectory with fixed mu=100.0\n        (100.0, 0.5),\n        (100.0, 2.0),\n        (100.0, 10.0),\n        (100.0, 1000.0),\n    ]\n\n    results = []\n    for mu, t in test_cases:\n        x_mt = newton_solver(mu, t)\n        \n        # Calculate metrics\n        d = np.linalg.norm(x_mt - x_star)\n        r = np.abs(h_func(x_mt))\n        g_val = g_func(x_mt)\n        v = np.max([g_val, 0.0]) # Will be 0.0 due to barrier\n        x1 = x_mt[0]\n        \n        results.append([d, r, v, x1])\n\n    # Final print statement in the exact required format.\n    result_str = \",\".join([f\"[{d},{r},{v},{x1}]\" for d, r, v, x1 in results])\n    print(f\"[{result_str}]\")\n\nsolve()\n```", "id": "3195614"}]}