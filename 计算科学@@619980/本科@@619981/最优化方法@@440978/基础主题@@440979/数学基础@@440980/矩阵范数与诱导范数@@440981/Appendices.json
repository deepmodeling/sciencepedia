{"hands_on_practices": [{"introduction": "我们经常直接使用矩阵 $1$-范数的计算公式，即“最大绝对列和”。但你是否想过这个公式是如何从诱导范数的基本定义 $\\|A\\|_1 = \\sup_{x \\neq 0} \\frac{\\|A x\\|_{1}}{\\|x\\|_{1}}$ 推导出来的？通过本练习 [@problem_id:3148401]，你将亲手完成这个推导过程，从而深刻理解诱导范数的本质。此外，我们还将通过一个模拟机器学习特征矩阵的例子，来揭示这个抽象的范数值在现实世界的数据分析中，是如何反映特征尺度的敏感性的。", "problem": "考虑一个用于机器学习优化的线性模型，其中数据集由一个矩阵 $A \\in \\mathbb{R}^{m \\times n}$ 表示。$A$ 的每一列对应一个特征，每一行对应一个样本。假设该模型使用坐标级更新，其稳定性受线性映射 $x \\mapsto A x$ 如何放大以向量1-范数度量的扰动的影响。仅使用以下基本定义：对于向量 $x \\in \\mathbb{R}^{n}$，1-范数定义为 $\\|x\\|_{1} = \\sum_{j=1}^{n} |x_{j}|$，且 $A$ 的诱导矩阵1-范数定义为\n$$\n\\|A\\|_{1} = \\sup_{x \\neq 0} \\frac{\\|A x\\|_{1}}{\\|x\\|_{1}}.\n$$\n从这些定义以及绝对值和不等式的标准性质出发，推导 $\\|A\\|_{1}$ 关于 $A$ 中元素的闭式表达式，然后对以下数据集矩阵进行求值（该矩阵的构建旨在反映一个特征的尺度远大于其他特征的情况）：\n$$\nA = \\begin{pmatrix}\n7.5  0.2  -0.1 \\\\\n8.3  -0.15  0.05 \\\\\n6.9  0.1  0.2 \\\\\n9.1  -0.05  -0.15 \\\\\n8.7  0.25  0.05\n\\end{pmatrix}.\n$$\n用文字解释所推导的表达式如何与优化中特征尺度化的敏感性相关联，特别是为什么不平衡的特征尺度会主导诱导矩阵1-范数，从而主导映射 $x \\mapsto A x$ 下扰动的最坏情况放大。最后，报告给定矩阵 $A$ 的 $\\|A\\|_{1}$ 的确切数值。无需四舍五入；提供精确值。", "solution": "问题要求推导诱导矩阵1-范数的闭式表达式，对给定矩阵 $A$ 进行求值，并从概念上解释其与机器学习模型中特征缩放的相关性。该问题提法清晰，有科学依据，并为完整解答提供了所有必要信息。\n\n首先，我们将推导矩阵 $A \\in \\mathbb{R}^{m \\times n}$ 的诱导矩阵1-范数（记为 $\\|A\\|_1$）的公式。给出的定义是：\n$$\n\\|A\\|_{1} = \\sup_{x \\neq 0} \\frac{\\|A x\\|_{1}}{\\|x\\|_{1}}\n$$\n其中 $x \\in \\mathbb{R}^{n}$ 且向量1-范数为 $\\|x\\|_1 = \\sum_{j=1}^{n} |x_j|$。\n\n令 $y = Ax$。向量 $y \\in \\mathbb{R}^m$ 的第 $i$ 个分量由 $y_i = \\sum_{j=1}^{n} a_{ij} x_j$ 给出，其中 $a_{ij}$ 是 $A$ 的元素。那么 $Ax$ 的1-范数是：\n$$\n\\|Ax\\|_1 = \\|y\\|_1 = \\sum_{i=1}^{m} |y_i| = \\sum_{i=1}^{m} \\left| \\sum_{j=1}^{n} a_{ij} x_j \\right|\n$$\n对内层求和应用绝对值的三角不等式，即 $|\\sum_k z_k| \\le \\sum_k |z_k|$，我们得到：\n$$\n\\left| \\sum_{j=1}^{n} a_{ij} x_j \\right| \\le \\sum_{j=1}^{n} |a_{ij} x_j| = \\sum_{j=1}^{n} |a_{ij}| |x_j|\n$$\n将此不等式代回 $\\|Ax\\|_1$ 的表达式中：\n$$\n\\|Ax\\|_1 \\le \\sum_{i=1}^{m} \\left( \\sum_{j=1}^{n} |a_{ij}| |x_j| \\right)\n$$\n由于所有项都是非负的，我们可以交换求和顺序：\n$$\n\\|Ax\\|_1 \\le \\sum_{j=1}^{n} \\sum_{i=1}^{m} |a_{ij}| |x_j| = \\sum_{j=1}^{n} \\left( |x_j| \\sum_{i=1}^{m} |a_{ij}| \\right)\n$$\n我们定义 $C_j = \\sum_{i=1}^{m} |a_{ij}|$ 为 $A$ 的第 $j$ 列元素的绝对值之和。该不等式变为：\n$$\n\\|Ax\\|_1 \\le \\sum_{j=1}^{n} C_j |x_j|\n$$\n现在，令 $C_{\\max} = \\max_{1 \\le j \\le n} C_j = \\max_{1 \\le j \\le n} \\sum_{i=1}^{m} |a_{ij}|$。因为对所有 $j$ 都有 $C_j \\le C_{\\max}$，我们可以写出：\n$$\n\\sum_{j=1}^{n} C_j |x_j| \\le \\sum_{j=1}^{n} C_{\\max} |x_j| = C_{\\max} \\sum_{j=1}^{n} |x_j| = C_{\\max} \\|x\\|_1\n$$\n结合这些不等式，我们为比率 $\\frac{\\|Ax\\|_1}{\\|x\\|_1}$ 建立了一个上界：\n$$\n\\|Ax\\|_1 \\le C_{\\max} \\|x\\|_1 \\implies \\frac{\\|Ax\\|_1}{\\|x\\|_1} \\le C_{\\max} \\quad \\text{for all } x \\neq 0\n$$\n这意味着该比率的上确界也小于或等于 $C_{\\max}$：\n$$\n\\|A\\|_1 = \\sup_{x \\neq 0} \\frac{\\|Ax\\|_1}{\\|x\\|_1} \\le C_{\\max}\n$$\n为完成推导，我们必须证明这个上界是可以达到的。也就是说，我们必须找到一个特定的非零向量 $x_0$，使得等式 $\\frac{\\|Ax_0\\|_1}{\\|x_0\\|_1} = C_{\\max}$ 成立。\n\n设 $k$ 是达到最大绝对列和的列的索引，使得 $C_k = C_{\\max}$。考虑向量 $x_0 = e_k$，其中 $e_k$ 是 $\\mathbb{R}^n$ 中的第 $k$ 个标准基向量。该向量在第 $k$ 个位置为1，其余位置为0。\n这个向量的1-范数是 $\\|x_0\\|_1 = \\|e_k\\|_1 = 1$。\n乘积 $Ax_0 = Ae_k$ 是矩阵 $A$ 的第 $k$ 列。我们将此列向量记为 $a_k$。\n因此，$Ax_0$ 的1-范数是：\n$$\n\\|Ax_0\\|_1 = \\|a_k\\|_1 = \\sum_{i=1}^{m} |a_{ik}|\n$$\n根据我们对索引 $k$ 的定义，这个和恰好是 $C_k = C_{\\max}$。\n对于这个特定的选择 $x_0 = e_k$，比率变为：\n$$\n\\frac{\\|Ax_0\\|_1}{\\|x_0\\|_1} = \\frac{C_{\\max}}{1} = C_{\\max}\n$$\n因为我们找到了一个向量 $x_0$ 使得比率等于 $C_{\\max}$，并且我们也证明了该比率永远不会超过 $C_{\\max}$，所以我们得出结论，上确界必须恰好是 $C_{\\max}$。因此，诱导矩阵1-范数的闭式表达式是最大绝对列和：\n$$\n\\|A\\|_{1} = \\max_{1 \\le j \\le n} \\sum_{i=1}^{m} |a_{ij}|\n$$\n\n接下来，我们对给定的矩阵求此表达式的值：\n$$\nA = \\begin{pmatrix}\n7.5  0.2  -0.1 \\\\\n8.3  -0.15  0.05 \\\\\n6.9  0.1  0.2 \\\\\n9.1  -0.05  -0.15 \\\\\n8.7  0.25  0.05\n\\end{pmatrix}\n$$\n这里，$m=5$ 且 $n=3$。我们计算绝对列和 $C_1$、$C_2$ 和 $C_3$。\n对第一列（$j=1$）：\n$$\nC_1 = |7.5| + |8.3| + |6.9| + |9.1| + |8.7| = 7.5 + 8.3 + 6.9 + 9.1 + 8.7 = 40.5\n$$\n对第二列（$j=2$）：\n$$\nC_2 = |0.2| + |-0.15| + |0.1| + |-0.05| + |0.25| = 0.2 + 0.15 + 0.1 + 0.05 + 0.25 = 0.75\n$$\n对第三列（$j=3$）：\n$$\nC_3 = |-0.1| + |0.05| + |0.2| + |-0.15| + |0.05| = 0.1 + 0.05 + 0.2 + 0.15 + 0.05 = 0.55\n$$\n诱导矩阵1-范数是这些和的最大值：\n$$\n\\|A\\|_{1} = \\max(C_1, C_2, C_3) = \\max(40.5, 0.75, 0.55) = 40.5\n$$\n\n最后，我们解释与特征缩放的联系。推导出的公式 $\\|A\\|_1 = \\max_j \\sum_i |a_{ij}|$ 表明，诱导1-范数完全由绝对值之和最大的那一列决定。在给定的情境中，$A$ 的每一列代表一个特征，其元素是该特征在不同样本上的值。和 $\\sum_i |a_{ij}|$ 可以被解释为数据集中第 $j$ 个特征的总量级或尺度的度量。\n\n诱导范数 $\\|A\\|_1$ 量化了在线性变换 $x \\mapsto Ax$ 下向量1-范数的最大放大倍数。它代表了一个最坏情况下的灵敏度度量：对向量 $x$ 的任何输入扰动 $\\delta x$ 将在输出中产生一个扰动 $\\delta y = A\\delta x$，其上界为 $\\|\\delta y\\|_1 \\le \\|A\\|_1 \\|\\delta x\\|_1$。\n\n给定的矩阵 $A$ 被构建成使其一个特征（第一列）的尺度远大于其他特征。正如我们的计算所示，这种不平衡的尺度导致第一列的绝对值和（$C_1=40.5$）完全主导了其他列的和（$C_2=0.75$，$C_3=0.55$）。因此，总范数 $\\|A\\|_1$ 完全由这个单一的、大尺度的特征决定。这意味着任何对此范数敏感的优化算法（例如某些坐标级方法）的稳定性和行为将由尺度最差的特征所决定。与此特征对齐的扰动（即，集中在输入向量 $x$ 的第一个分量中）被放大了 40.5 倍，而沿其他特征方向的扰动将被放大得小得多。如此大的、不平衡的放大因子会导致数值方法中的病态条件和不稳定性，这就是为什么特征缩放（例如，标准化列以使其具有相似的尺度）是许多优化和机器学习算法的关键预处理步骤。\n\n$\\|A\\|_1$ 的确切数值是 $40.5$。", "answer": "$$\n\\boxed{40.5}\n$$", "id": "3148401"}, {"introduction": "在掌握了单一范数的计算后，一个自然的问题是：为什么我们需要多种不同的范数，例如 $2$-范数和 $\\infty$-范数？本练习 [@problem_id:3148424] 将通过一个精巧的例子回答这个问题。我们将构建一个特殊的矩阵，它的 $2$-范数很小，但 $\\infty$-范数却很大。通过分析梯度裁剪这一现代优化技术在该矩阵上的表现，你将直观地看到，不同范数的选择如何导致算法行为的巨大差异，从而理解在实际应用中选择合适范数的重要性。", "problem": "考虑诱导矩阵范数 $\\|A\\|_{p} = \\sup_{\\|x\\|_{p} = 1} \\|A x\\|_{p}$（其中 $p \\in \\{2, \\infty\\}$）以及梯度裁剪的梯度下降法，其定义如下：给定一个阈值 $\\tau  0$ 和一个范数索引 $p$，如果梯度向量 $g$ 满足 $\\|g\\|_{p}  \\tau$，则将其缩放为一个共线向量 $\\tilde{g}_{p}$，使得 $\\|\\tilde{g}_{p}\\|_{p} = \\tau$；否则 $\\tilde{g}_{p} = g$。您将分析在凸二次目标函数中，不同的诱导范数如何与梯度裁剪相互作用。\n\n定义矩阵 $A \\in \\mathbb{R}^{100 \\times 100}$ 仅有其第一行为非零行。设该第一行为 $r^{\\top}$，其中 $r \\in \\mathbb{R}^{100}$ 的所有元素 $r_{i} = 0.1$（对于所有 $i \\in \\{1, \\dots, 100\\}$），且 $A$ 的其余所有行均为零。考虑凸二次函数 $f(x) = \\frac{1}{2} x^{\\top} A^{\\top} A x$ 和起始点 $x_{0} = r$。设步长为 $\\alpha = 1$，裁剪阈值为 $\\tau = 0.2$。\n\n任务：\n- 根据诱导矩阵范数的定义，验证 $A$ 满足 $\\|A\\|_{2}$ 小而 $\\|A\\|_{\\infty}$ 大。\n- 使用上述裁剪规则，计算从 $x_{0}$ 出发，在 $p = \\infty$ 和 $p = 2$ 两种情况下进行一步梯度下降的结果，并简要解释这两种裁剪选择如何影响最小化 $f$ 的单步进展。\n- 最后，计算 $f(x_{1}^{(2)})$ 的精确数值，其中 $x_{1}^{(2)}$ 表示如上所述在 $2$-范数下进行一步梯度裁剪后的点。最终答案以单个数字形式给出，不带单位。无需四舍五入。", "solution": "用户提供了一个优化方法领域的问题。我将首先根据指定标准验证问题陈述，如果问题有效，则继续提供完整解答。\n\n### 第一步：提取给定条件\n- 诱导矩阵范数定义：$\\|A\\|_{p} = \\sup_{\\|x\\|_{p} = 1} \\|A x\\|_{p}$，其中 $p \\in \\{2, \\infty\\}$。\n- 梯度裁剪规则：给定一个阈值 $\\tau  0$ 和一个范数索引 $p$，如果梯度向量 $g$ 满足 $\\|g\\|_{p}  \\tau$，则将其缩放为 $\\tilde{g}_{p} = \\frac{\\tau}{\\|g\\|_{p}} g$。否则，$\\tilde{g}_{p} = g$。\n- 矩阵 $A \\in \\mathbb{R}^{100 \\times 100}$ 的第一行为 $r^{\\top}$，其余所有行均为零。\n- 向量 $r \\in \\mathbb{R}^{100}$ 的所有元素 $r_{i} = 0.1$（对于所有 $i \\in \\{1, \\dots, 100\\}$）。\n- 目标函数为 $f(x) = \\frac{1}{2} x^{\\top} A^{\\top} A x$。\n- 起始点为 $x_{0} = r$。\n- 梯度下降步长为 $\\alpha = 1$。\n- 裁剪阈值为 $\\tau = 0.2$。\n\n### 第二步：使用提取的给定条件进行验证\n该问题在线性代数和数值优化的数学领域内是良定义的。\n- **科学基础：** 诱导矩阵范数、凸二次函数和带裁剪的梯度下降等概念是优化和机器学习中的标准和基础概念。该问题是一个标准的数学练习。\n- **良构性：** 提供了所有必要的定义、常数和初始条件，可以为任务的每个部分计算出唯一解。函数 $f(x)$ 是凸函数，因为其海森矩阵 $A^{\\top}A$ 是半正定的，这是分析梯度下降的标准设置。\n- **客观性：** 问题以精确、客观的数学语言陈述。\n- **缺陷分析：**\n    - 问题不违反任何科学或数学原理。\n    - 这是一个可形式化的问题，与所述主题直接相关。\n    - 设置是完整的且内部一致。\n    - 所有条件都是数学性的，不涉及物理现实，因此并非不可行。\n    - 问题结构良好，能够为所需的计算导出唯一且有意义的解。\n    - 问题并非微不足道；它需要仔细应用定义来说明 $L_2$ 和 $L_\\infty$ 范数在高维空间中的一个关键区别。\n    - 所有结果均可通过数学方法验证。\n\n### 第三步：结论与行动\n问题有效。将提供详细解答。\n\n### 解\n\n该问题要求针对一个特定的二次目标函数，分析在两种不同范数下的梯度裁剪。我们将按顺序处理每个任务。\n\n首先，我们来描述给定的矩阵 $A$ 和向量 $r$。向量 $r \\in \\mathbb{R}^{100}$ 的分量定义为 $r_i = 0.1$（对于 $i=1, \\dots, 100$）。矩阵 $A \\in \\mathbb{R}^{100 \\times 100}$ 的第一行等于 $r^{\\top}$，其余所有行均为零。这可以紧凑地写成 $A = e_1 r^{\\top}$，其中 $e_1 = (1, 0, \\dots, 0)^{\\top}$ 是 $\\mathbb{R}^{100}$ 中的第一个标准基向量。\n\n**任务1：验证 $\\|A\\|_{2}$ 小而 $\\|A\\|_{\\infty}$ 大。**\n\n诱导 $\\infty$-范数 $\\|A\\|_{\\infty}$ 定义为最大绝对行和。\n$$ \\|A\\|_{\\infty} = \\max_{i} \\sum_{j=1}^{100} |A_{ij}| $$\n对于矩阵 $A$，第一行是 $r^{\\top}$，因此其元素的绝对值之和为 $\\sum_{j=1}^{100} |r_j| = \\sum_{j=1}^{100} 0.1 = 100 \\times 0.1 = 10$。$A$ 的所有其他行都是零，所以它们的绝对行和为 $0$。这些值中的最大值为 $10$。\n$$ \\|A\\|_{\\infty} = 10 $$\n诱导 $2$-范数 $\\|A\\|_{2}$ 是 $A$ 的最大奇异值，也就是矩阵 $A^{\\top}A$ 的最大特征值的平方根。\n$$ A^{\\top}A = (e_1 r^{\\top})^{\\top} (e_1 r^{\\top}) = (r e_1^{\\top}) (e_1 r^{\\top}) = r (e_1^{\\top} e_1) r^{\\top} $$\n因为 $e_1^{\\top}e_1 = \\|e_1\\|_2^2 = 1$，我们有：\n$$ A^{\\top}A = r r^{\\top} $$\n这是一个秩为 1 的矩阵。矩阵 $uv^{\\top}$ 的特征值为 $v^{\\top}u$（对应特征向量 $u$）和 $0$（重数为 $n-1$）。在这里，$u=v=r$。唯一的非零特征值为 $\\lambda = r^{\\top}r$。\n$$ \\lambda = r^{\\top}r = \\sum_{i=1}^{100} r_i^2 = \\sum_{i=1}^{100} (0.1)^2 = 100 \\times 0.01 = 1 $$\n$A^{\\top}A$ 的特征值为 $\\{1, 0, \\dots, 0\\}$。最大特征值为 $\\lambda_{\\max} = 1$。因此，诱导 $2$-范数为：\n$$ \\|A\\|_{2} = \\sqrt{\\lambda_{\\max}(A^{\\top}A)} = \\sqrt{1} = 1 $$\n比较这两个范数，$\\|A\\|_{\\infty} = 10$ 确实远大于 $\\|A\\|_{2} = 1$。这验证了前提。\n\n**任务2：计算一步梯度下降并解释裁剪的效果。**\n\n目标函数为 $f(x) = \\frac{1}{2} x^{\\top} A^{\\top} A x$。其梯度为 $\\nabla f(x) = A^{\\top} A x$。我们计算在起始点 $x_0 = r$ 处的梯度 $g$。\n$$ g = \\nabla f(x_0) = A^{\\top} A x_0 = (r r^{\\top}) r = r(r^{\\top}r) $$\n因为我们已经算出 $r^{\\top}r = 1$，所以梯度就是：\n$$ g = r $$\n梯度下降的更新规则是 $x_1 = x_0 - \\alpha \\tilde{g}$，其中 $\\tilde{g}$ 是裁剪后的梯度，$\\alpha=1$。在这里，$x_1 = x_0 - \\tilde{g} = r - \\tilde{g}$。我们分析在阈值 $\\tau=0.2$ 下，$p=\\infty$ 和 $p=2$ 时的裁剪情况。\n\n情况 $p=\\infty$：我们计算梯度 $g=r$ 的 $\\infty$-范数。\n$$ \\|g\\|_{\\infty} = \\|r\\|_{\\infty} = \\max_{i} |r_i| = 0.1 $$\n由于 $\\|g\\|_{\\infty} = 0.1 \\le \\tau = 0.2$，不发生裁剪。因此，$\\tilde{g}_{\\infty} = g = r$。更新后的点是：\n$$ x_{1}^{(\\infty)} = x_0 - \\tilde{g}_{\\infty} = r - r = 0 $$\n凸函数 $f(x) = \\frac{1}{2} x^{\\top} (A^{\\top}A) x$ 的最小值在 $x=0$ 处取得。使用 $\\infty$-范数裁剪，梯度下降步骤在一次迭代中直接移动到全局最小值。这是因为 $\\infty$-范数很小，反映了梯度的任何单个分量都不大的事实。\n\n情况 $p=2$：我们计算梯度 $g=r$ 的 $2$-范数。\n$$ \\|g\\|_{2} = \\|r\\|_{2} = \\sqrt{r^{\\top}r} = \\sqrt{1} = 1 $$\n由于 $\\|g\\|_{2} = 1  \\tau = 0.2$，应用梯度裁剪。裁剪后的梯度是：\n$$ \\tilde{g}_{2} = \\frac{\\tau}{\\|g\\|_{2}} g = \\frac{0.2}{1} r = 0.2 r $$\n更新后的点是：\n$$ x_{1}^{(2)} = x_0 - \\tilde{g}_{2} = r - 0.2 r = (1 - 0.2) r = 0.8 r $$\n使用 $2$-范数裁剪，梯度的模长从 $1$ 缩减到 $0.2$，导致朝向最小值的步长大大减小。$2$-范数对所有分量的总和大小敏感，尽管每个分量都很小，但在这种高维情况下，这个总和很大。这种敏感性导致步长急剧减小，与未裁剪的步骤或使用 $\\infty$-范数裁剪的步骤相比，收敛速度减慢。\n\n**任务3：计算 $f(x_{1}^{(2)})$ 的精确数值。**\n\n我们需要在点 $x_{1}^{(2)} = 0.8 r$ 处计算函数 $f$ 的值。\n$$ f(x_{1}^{(2)}) = \\frac{1}{2} (x_{1}^{(2)})^{\\top} A^{\\top}A x_{1}^{(2)} $$\n代入 $x_{1}^{(2)} = 0.8 r$ 和 $A^{\\top}A = r r^{\\top}$：\n$$ f(0.8r) = \\frac{1}{2} (0.8r)^{\\top} (rr^{\\top}) (0.8r) $$\n$$ f(0.8r) = \\frac{1}{2} (0.8)^2 r^{\\top} r r^{\\top} r $$\n利用 $r^{\\top}r = 1$ 这一事实：\n$$ f(0.8r) = \\frac{1}{2} (0.8)^2 (1) (1) = \\frac{1}{2} (0.64) = 0.32 $$\n经过一步 $2$-范数裁剪后，目标函数的值为 $0.32$。作为比较，起始点的值为 $f(x_0)=f(r)=\\frac{1}{2}r^{\\top}(rr^{\\top})r = \\frac{1}{2}(r^{\\top}r)^2 = \\frac{1}{2}(1)^2=0.5$。而使用 $\\infty$-范数裁剪得到的点的值为 $f(x_1^{(\\infty)}) = f(0) = 0$。这证实了使用 $2$-范数裁剪时进展较慢。", "answer": "$$ \\boxed{0.32} $$", "id": "3148424"}, {"introduction": "矩阵范数最重要的应用之一是分析线性方程组 $A x = b$ 求解的稳定性。一个微小的输入扰动 $\\delta b$ 会在解 $x$ 中造成多大的误差？这个问题的答案由矩阵的条件数 $\\kappa(A)$ 决定，而条件数本身就是用矩阵范数定义的。本练习 [@problem_id:3148445] 将引导你为一个特殊构造的矩阵计算其在 $1$-范数和 $\\infty$-范数下的条件数。你将会发现，对于同一个矩阵，不同范数下的条件数可能天差地别，这直接影响了我们对系统解的稳定性的评估。", "problem": "考虑一个维度为 $n \\geq 2$ 的方阵。定义矩阵 $S \\in \\mathbb{R}^{n \\times n}$，其元素为 $s_{1j} = 1$（对于 $j = 2, 3, \\dots, n$）及 $s_{ij} = 0$（其他情况）。构造矩阵 $A = I - S$，其中 $I$ 是单位矩阵。使用这种显式构造来分析求解线性系统 $A x = b$ 在数据向量 $b$ 存在扰动时的敏感性。\n\n任务：\n- 仅使用矩阵和幂零算子的基本性质，证明 $A$ 是可逆的并求出 $A^{-1}$。\n- 计算此 $A$ 的 $\\|A^{-1}\\|_{1}$ 和 $\\|A^{-1}\\|_{\\infty}$。\n- 计算 $\\|A\\|_{1}$ 和 $\\|A\\|_{\\infty}$，然后计算相应的条件数 $\\kappa_{1}(A)$ 和 $\\kappa_{\\infty}(A)$，其中 $\\kappa_{p}(A) = \\|A\\|_{p} \\|A^{-1}\\|_{p}$ 是对于一个诱导矩阵范数。\n- 对于一个扰动 $b \\mapsto b + \\delta b$，仅使用诱导范数的定义和次可乘性，推导相对解误差 $\\|x - \\hat{x}\\|_{p} / \\|x\\|_{p}$ 在 $p \\in \\{1, \\infty\\}$ 时的最坏情况上界。此处，$x$ 是 $A x = b$ 的精确解，$\\hat{x}$ 是 $A \\hat{x} = b + \\delta b$ 的解。\n- 在相对数据误差在两种范数下相同的附加假设下，即 $\\|\\delta b\\|_{1} / \\|b\\|_{1} = \\|\\delta b\\|_{\\infty} / \\|b\\|_{\\infty}$，确定两种最坏情况相对误差界（$\\infty$-范数与 $1$-范数之比）作为 $n$ 的函数。\n\n以关于 $n$ 的单个解析表达式的形式给出你的最终答案。", "solution": "该问题陈述经评估具有科学依据、问题适定、客观且完整。它代表了数值线性代数中的一个标准练习。该问题有效，将提供解答。\n\n该问题要求对一个特定矩阵 $A = I - S$ 进行多步分析。让我们逐一完成每个任务。\n\n首先，我们定义矩阵 $S \\in \\mathbb{R}^{n \\times n}$ (对于 $n \\ge 2$)。其元素由 $s_{1j} = 1$ (对于 $j \\in \\{2, 3, \\dots, n\\}$) 和 $s_{ij} = 0$ (对于所有其他对 $(i, j)$) 给出。这意味着 $S$ 的唯一非零项位于第一行，从第二列开始。\n矩阵 $S$ 的形式如下：\n$$S = \\begin{pmatrix}\n0  1  1  \\dots  1 \\\\\n0  0  0  \\dots  0 \\\\\n0  0  0  \\dots  0 \\\\\n\\vdots  \\vdots  \\vdots  \\ddots  \\vdots \\\\\n0  0  0  \\dots  0\n\\end{pmatrix}$$\n\n**任务1：证明 $A$ 是可逆的并求出 $A^{-1}$。**\n\n矩阵 $A$ 定义为 $A = I - S$。为了证明其可逆性并求出其逆矩阵，我们首先分析 $S$ 的性质。我们来计算 $S^2$。$S^2$ 的第 $(i, k)$ 个元素由 $(S^2)_{ik} = \\sum_{j=1}^{n} s_{ij} s_{jk}$ 给出。\n- 如果 $i  1$，则对所有 $j$ 都有 $s_{ij} = 0$，所以 $(S^2)_{ik} = 0$。\n- 如果 $i = 1$，则 $(S^2)_{1k} = \\sum_{j=1}^{n} s_{1j} s_{jk}$。唯一的非零项 $s_{1j}$ 是对于 $j \\in \\{2, 3, \\dots, n\\}$。因此，求和变为 $(S^2)_{1k} = \\sum_{j=2}^{n} s_{1j} s_{jk} = \\sum_{j=2}^{n} (1) s_{jk}$。\n要使 $s_{jk}$ 不为零，必须有 $j=1$。然而，求和的范围是从 $j=2$ 到 $n$。因此，对于求和中的每一个 $j$，$s_{jk} = 0$。\n结果，对所有 $k$ 都有 $(S^2)_{1k} = \\sum_{j=2}^{n} 0 = 0$。\n由于 $S^2$ 的所有元素都为零，所以 $S^2 = O$，其中 $O$ 是 $n \\times n$ 的零矩阵。这表明 $S$ 是一个2阶幂零矩阵。\n\n矩阵 $A$ 由 $A = I - S$ 给出。因为 $S$ 是幂零的，所以 $A$ 的逆的几何级数（诺伊曼级数）是有限的：\n$$(I-S)^{-1} = I + S + S^2 + S^3 + \\dots$$\n由于对于所有 $k \\ge 2$ 都有 $S^k = O$，该级数终止。\n$$A^{-1} = (I - S)^{-1} = I + S$$\n逆矩阵 $A^{-1}$ 的存在证明了 $A$ 是可逆的。\n$A$ 和 $A^{-1}$ 的显式形式为：\n$$A = I - S = \\begin{pmatrix}\n1  -1  -1  \\dots  -1 \\\\\n0  1  0  \\dots  0 \\\\\n0  0  1  \\dots  0 \\\\\n\\vdots  \\vdots  \\vdots  \\ddots  \\vdots \\\\\n0  0  0  \\dots  1\n\\end{pmatrix}$$\n$$A^{-1} = I + S = \\begin{pmatrix}\n1  1  1  \\dots  1 \\\\\n0  1  0  \\dots  0 \\\\\n0  0  1  \\dots  0 \\\\\n\\vdots  \\vdots  \\vdots  \\ddots  \\vdots \\\\\n0  0  0  \\dots  1\n\\end{pmatrix}$$\n\n**任务2：计算 $\\|A^{-1}\\|_{1}$ 和 $\\|A^{-1}\\|_{\\infty}$。**\n\n矩阵的诱导$1$-范数是最大绝对列和，$\\|M\\|_1 = \\max_{1 \\le j \\le n} \\sum_{i=1}^{n} |m_{ij}|$。\n对于 $A^{-1} = I + S$:\n- 第一列 ($j=1$) 的绝对值之和为 $|1| = 1$。\n- 对于任何其他列 $j \\in \\{2, \\dots, n\\}$，非零项是 $(A^{-1})_{1j}=1$ 和 $(A^{-1})_{jj}=1$。绝对值之和为 $|1| + |1| = 2$。\n因此，$\\|A^{-1}\\|_1 = \\max(1, 2, \\dots, 2) = 2$。\n\n矩阵的诱导$\\infty$-范数是最大绝对行和，$\\|M\\|_{\\infty} = \\max_{1 \\le i \\le n} \\sum_{j=1}^{n} |m_{ij}|$。\n对于 $A^{-1} = I + S$:\n- 第一行 ($i=1$) 有 $n$ 个等于 $1$ 的项。绝对值之和为 $\\sum_{j=1}^{n} |1| = n$。\n- 对于任何其他行 $i \\in \\{2, \\dots, n\\}$，唯一的非零项是 $(A^{-1})_{ii}=1$。绝对值之和为 $|1| = 1$。\n因此，$\\|A^{-1}\\|_{\\infty} = \\max(n, 1, \\dots, 1) = n$。\n\n**任务3：计算 $\\|A\\|_{1}$、$\\|A\\|_{\\infty}$ 及条件数。**\n\n我们首先计算 $A = I - S$ 的范数。\n对于 $A$ 的 $1$-范数：\n- 第一列 ($j=1$) 的绝对值之和为 $|1| = 1$。\n- 对于任何其他列 $j \\in \\{2, \\dots, n\\}$，非零项是 $a_{1j}=-1$ 和 $a_{jj}=1$。绝对值之和为 $|-1| + |1| = 2$。\n因此，$\\|A\\|_1 = \\max(1, 2, \\dots, 2) = 2$。\n\n对于 $A$ 的 $\\infty$-范数：\n- 第一行 ($i=1$) 的项为 $1, -1, \\dots, -1$。绝对值之和为 $|1| + \\sum_{j=2}^{n} |-1| = 1 + (n-1) = n$。\n- 对于任何其他行 $i \\in \\{2, \\dots, n\\}$，唯一的非零项是 $a_{ii}=1$。绝对值之和为 $|1|=1$。\n因此，$\\|A\\|_{\\infty} = \\max(n, 1, \\dots, 1) = n$。\n\n条件数 $\\kappa_p(A)$ 定义为 $\\kappa_p(A) = \\|A\\|_p \\|A^{-1}\\|_p$。\n- 对于 $p=1$：$\\kappa_1(A) = \\|A\\|_1 \\|A^{-1}\\|_1 = 2 \\cdot 2 = 4$。\n- 对于 $p=\\infty$：$\\kappa_{\\infty}(A) = \\|A\\|_{\\infty} \\|A^{-1}\\|_{\\infty} = n \\cdot n = n^2$。\n\n**任务4：推导相对解误差的最坏情况上界。**\n\n设 $x$ 是 $Ax=b$ 的解，$\\hat{x}$ 是扰动系统 $A\\hat{x} = b + \\delta b$ 的解。\n解的误差为 $\\delta x = \\hat{x} - x$。\n将第一个方程从第二个方程中减去，得到：\n$A\\hat{x} - Ax = (b + \\delta b) - b \\implies A(\\hat{x} - x) = \\delta b \\implies A \\delta x = \\delta b$。\n由于 $A$ 可逆，我们可以写成 $\\delta x = A^{-1} \\delta b$。\n对两边取 $p$-范数，并使用诱导范数的性质 $\\|Mv\\|_p \\le \\|M\\|_p \\|v\\|_p$，我们得到：\n$\\|\\delta x\\|_p = \\|A^{-1} \\delta b\\|_p \\le \\|A^{-1}\\|_p \\|\\delta b\\|_p$。\n\n为了找到相对误差 $\\|\\delta x\\|_p / \\|x\\|_p$，我们需要一个涉及 $\\|x\\|_p$ 的界。从原始方程 $Ax=b$，我们有 $\\|b\\|_p = \\|Ax\\|_p \\le \\|A\\|_p \\|x\\|_p$。假设 $b \\ne 0$（因此 $x \\ne 0$），我们可以写成 $1/\\|x\\|_p \\le \\|A\\|_p/\\|b\\|_p$。\n\n结合这两个不等式：\n$$\\frac{\\|\\delta x\\|_p}{\\|x\\|_p} \\le \\frac{\\|A^{-1}\\|_p \\|\\delta b\\|_p}{\\|x\\|_p} \\le \\|A^{-1}\\|_p \\|\\delta b\\|_p \\left( \\frac{\\|A\\|_p}{\\|b\\|_p} \\right)$$\n重新整理各项，我们得到标准的最坏情况相对误差界：\n$$\\frac{\\|\\hat{x} - x\\|_p}{\\|x\\|_p} \\le \\|A\\|_p \\|A^{-1}\\|_p \\frac{\\|\\delta b\\|_p}{\\|b\\|_p} = \\kappa_p(A) \\frac{\\|\\delta b\\|_p}{\\|b\\|_p}$$\n这个界被认为是“最坏情况”的，因为对于任何矩阵 $A$，都存在向量 $b$ 和 $\\delta b$ 使得这个不等式变为等式。\n\n**任务5：确定两种最坏情况相对误差界之比。**\n\n在$\\infty$-范数下的相对误差最坏情况上界为：\n$$E_{\\infty} = \\kappa_{\\infty}(A) \\frac{\\|\\delta b\\|_{\\infty}}{\\|b\\|_{\\infty}} = n^2 \\frac{\\|\\delta b\\|_{\\infty}}{\\|b\\|_{\\infty}}$$\n在$1$-范数下的相对误差最坏情况上界为：\n$$E_{1} = \\kappa_{1}(A) \\frac{\\|\\delta b\\|_{1}}{\\|b\\|_{1}} = 4 \\frac{\\|\\delta b\\|_{1}}{\\|b\\|_{1}}$$\n我们被给予一个附加假设，即相对数据误差在两种范数下是相同的：\n$$\\frac{\\|\\delta b\\|_{1}}{\\|b\\|_{1}} = \\frac{\\|\\delta b\\|_{\\infty}}{\\|b\\|_{\\infty}}$$\n设这个共同的值为 $\\epsilon$。误差界变为 $E_{\\infty} = n^2 \\epsilon$ 和 $E_1 = 4 \\epsilon$。\n问题要求的是两种最坏情况相对误差界之比，即 $\\infty$-范数与 $1$-范数之比。这个比值为：\n$$\\frac{E_{\\infty}}{E_{1}} = \\frac{n^2 \\epsilon}{4 \\epsilon} = \\frac{n^2}{4}$$\n这个结果对于 $n \\ge 2$ 成立。", "answer": "$$\\boxed{\\frac{n^2}{4}}$$", "id": "3148445"}]}