{"hands_on_practices": [{"introduction": "在机器学习中，我们优化的目标函数通常是大量数据点损失函数的平均值。这个练习通过一个简洁的二次函数例子，揭示了一个深刻的原理：即使单个函数在某些方向上曲率很大（即不够光滑），将它们平均后也可能得到一个整体上更光滑的函数。通过解决这个问题 [@problem_id:3144694]，你将亲身体会到，问题结构（例如平均化）是如何改善优化问题的几何特性，从而为分布式训练或大批量梯度下降等方法的有效性提供理论支撑。", "problem": "考虑 $L \\in \\mathbb{R}$ 且 $L>0$，定义函数 $\\phi_1,\\phi_2:\\mathbb{R}^2 \\to \\mathbb{R}$ 为\n$$\n\\phi_1(x) \\;=\\; \\tfrac{1}{2}\\, x^{\\top} A_1 x\n\\quad\\text{和}\\quad\n\\phi_2(x) \\;=\\; \\tfrac{1}{2}\\, x^{\\top} A_2 x,\n$$\n其中\n$$\nA_1 \\;=\\; \\begin{pmatrix} L  0 \\\\ 0  0 \\end{pmatrix}\n\\quad\\text{和}\\quad\nA_2 \\;=\\; \\begin{pmatrix} 0  0 \\\\ 0  L \\end{pmatrix}.\n$$\n令 $f:\\mathbb{R}^2 \\to \\mathbb{R}$ 为经验平均值\n$$\nf(x) \\;=\\; \\tfrac{1}{2}\\big(\\phi_1(x)+\\phi_2(x)\\big).\n$$\n仅从梯度利普希茨连续性的定义和关于对称矩阵的基本线性代数知识出发，首先证明每个 $\\phi_i$ 的梯度相对于欧几里得范数是 $L$-利普希茨的，然后确定最小的常数 $L_f$ 使得\n$$\n\\|\\nabla f(x)-\\nabla f(y)\\|_2 \\;\\le\\; L_f \\,\\|x-y\\|_2\n\\quad\\text{对所有}\\quad x,y \\in \\mathbb{R}^2.\n$$\n请用 $L$ 的封闭式表达式给出你的最终答案。不需要四舍五入，也不涉及单位。答案必须是单个表达式。", "solution": "该问题陈述清晰，具有科学依据，并包含确定唯一解所需的所有信息。我们可以开始求解。\n\n问题要求做两件事：首先，证明 $\\phi_1$ 和 $\\phi_2$ 的梯度是 $L$-利普希茨连续的；其次，求出它们的经验平均值 $f$ 的梯度的最小利普希茨常数。\n\n设 $x = \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} \\in \\mathbb{R}^2$ 和 $y = \\begin{pmatrix} y_1 \\\\ y_2 \\end{pmatrix} \\in \\mathbb{R}^2$ 为任意向量。范数为欧几里得范数 $\\|\\cdot\\|_2$。\n\n如果存在一个常数 $K \\ge 0$ 使得对于所有 $x, y \\in \\mathbb{R}^n$ 都有 $\\|g(x) - g(y)\\|_2 \\le K \\|x-y\\|_2$，则函数 $g: \\mathbb{R}^n \\to \\mathbb{R}^m$ 是 $K$-利普希茨连续的。满足此条件的最小 $K$ 值即为利普希茨常数。\n\n对于一个定义域为全空间 $\\mathbb{R}^n$ 的可微函数，如果其黑塞矩阵有界，即对所有 $x$ 都有 $\\|\\nabla^2 f(x)\\|_2 \\le L$，则其梯度是 $L$-利普希茨连续的。对于形式为 $\\phi(x) = \\frac{1}{2} x^\\top A x$ 的二次函数（其中 $A$ 是对称矩阵），梯度为 $\\nabla \\phi(x) = Ax$，黑塞矩阵为 $\\nabla^2 \\phi(x) = A$。梯度的利普希茨常数是常数黑塞矩阵的谱范数，即 $\\|A\\|_2$。对于对称矩阵，谱范数是其最大特征值的绝对值，即 $\\lambda_{\\max}(|A|)$。\n\n**第一部分：$\\phi_1$ 和 $\\phi_2$ 梯度的利普希茨连续性**\n\n首先，考虑函数 $\\phi_1(x) = \\frac{1}{2} x^\\top A_1 x$，其中 $A_1 = \\begin{pmatrix} L  0 \\\\ 0  0 \\end{pmatrix}$。矩阵 $A_1$ 是对称的。\n$\\phi_1$ 的梯度由下式给出：\n$$\n\\nabla \\phi_1(x) = A_1 x = \\begin{pmatrix} L  0 \\\\ 0  0 \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} = \\begin{pmatrix} Lx_1 \\\\ 0 \\end{pmatrix}.\n$$\n为了检查梯度的利普希茨连续性，我们考察其差值：\n$$\n\\nabla \\phi_1(x) - \\nabla \\phi_1(y) = A_1 x - A_1 y = A_1(x-y).\n$$\n取欧几里得范数：\n$$\n\\|\\nabla \\phi_1(x) - \\nabla \\phi_1(y)\\|_2 = \\|A_1(x-y)\\|_2.\n$$\n根据诱导矩阵范数（谱范数）的定义，我们有：\n$$\n\\|A_1(x-y)\\|_2 \\le \\|A_1\\|_2 \\|x-y\\|_2.\n$$\n满足此不等式的最小常数是 $\\|A_1\\|_2$。对于对称矩阵，该范数是其特征值绝对值的最大值。$A_1$ 的特征值是其对角线元素，即 $\\lambda_1 = L$ 和 $\\lambda_2 = 0$。由于给定 $L>0$，特征值是非负的。\n$$\n\\|A_1\\|_2 = \\max(|L|, |0|) = L.\n$$\n因此，对所有 $x, y \\in \\mathbb{R}^2$ 都有 $\\|\\nabla \\phi_1(x) - \\nabla \\phi_1(y)\\|_2 \\le L \\|x-y\\|_2$。这证明了 $\\nabla \\phi_1(x)$ 是 $L$-利普希茨连续的。\n\n同理，对于函数 $\\phi_2(x) = \\frac{1}{2} x^\\top A_2 x$，其中 $A_2 = \\begin{pmatrix} 0  0 \\\\ 0  L \\end{pmatrix}$。矩阵 $A_2$ 是对称的。\n$\\phi_2$ 的梯度为：\n$$\n\\nabla \\phi_2(x) = A_2 x = \\begin{pmatrix} 0  0 \\\\ 0  L \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ Lx_2 \\end{pmatrix}.\n$$\n梯度的差为 $\\nabla \\phi_2(x) - \\nabla \\phi_2(y) = A_2(x-y)$。$\\nabla \\phi_2$ 的利普希茨常数是 $\\|A_2\\|_2$。$A_2$ 的特征值为 $\\lambda_1=0$ 和 $\\lambda_2=L$。谱范数为：\n$$\n\\|A_2\\|_2 = \\max(|0|, |L|) = L.\n$$\n因此，对所有 $x, y \\in \\mathbb{R}^2$ 都有 $\\|\\nabla \\phi_2(x) - \\nabla \\phi_2(y)\\|_2 \\le L \\|x-y\\|_2$。这证明了 $\\nabla \\phi_2(x)$ 也是 $L$-利普希茨连续的。\n\n**第二部分：$f$ 的梯度的最小利普希茨常数 $L_f$**\n\n函数 $f(x)$ 定义为 $\\phi_1(x)$ 和 $\\phi_2(x)$ 的经验平均值：\n$$\nf(x) = \\frac{1}{2} (\\phi_1(x) + \\phi_2(x)).\n$$\n代入 $\\phi_1$ 和 $\\phi_2$ 的定义：\n$$\nf(x) = \\frac{1}{2} \\left( \\frac{1}{2} x^\\top A_1 x + \\frac{1}{2} x^\\top A_2 x \\right) = \\frac{1}{4} x^\\top (A_1 + A_2) x.\n$$\n梯度是线性的，所以我们可以写出：\n$$\n\\nabla f(x) = \\nabla \\left( \\frac{1}{2} (\\phi_1(x) + \\phi_2(x)) \\right) = \\frac{1}{2} (\\nabla \\phi_1(x) + \\nabla \\phi_2(x)).\n$$\n代入梯度的表达式：\n$$\n\\nabla f(x) = \\frac{1}{2} (A_1 x + A_2 x) = \\frac{1}{2} (A_1 + A_2) x.\n$$\n我们来计算矩阵和 $A_1+A_2$：\n$$\nA_1 + A_2 = \\begin{pmatrix} L  0 \\\\ 0  0 \\end{pmatrix} + \\begin{pmatrix} 0  0 \\\\ 0  L \\end{pmatrix} = \\begin{pmatrix} L  0 \\\\ 0  L \\end{pmatrix} = L I,\n$$\n其中 $I$ 是 $2 \\times 2$ 的单位矩阵。\n所以，$f$ 的梯度是：\n$$\n\\nabla f(x) = \\frac{1}{2} (L I) x = \\frac{L}{2} x.\n$$\n我们想要求解满足 $\\|\\nabla f(x) - \\nabla f(y)\\|_2 \\le L_f \\|x-y\\|_2$ 的最小常数 $L_f$。\n我们来计算不等式的左边：\n$$\n\\|\\nabla f(x) - \\nabla f(y)\\|_2 = \\left\\| \\frac{L}{2} x - \\frac{L}{2} y \\right\\|_2 = \\left\\| \\frac{L}{2} (x - y) \\right\\|_2.\n$$\n利用范数的性质，并且已知 $L>0$：\n$$\n\\left\\| \\frac{L}{2} (x - y) \\right\\|_2 = \\left| \\frac{L}{2} \\right| \\|x-y\\|_2 = \\frac{L}{2} \\|x-y\\|_2.\n$$\n在这种情况下，不等式变成了一个等式：\n$$\n\\|\\nabla f(x) - \\nabla f(y)\\|_2 = \\frac{L}{2} \\|x-y\\|_2.\n$$\n我们需要找到满足 $\\frac{L}{2} \\|x-y\\|_2 \\le L_f \\|x-y\\|_2$ 对所有 $x, y \\in \\mathbb{R}^2$ 成立的最小 $L_f$。对于任意 $x \\ne y$，我们可以除以 $\\|x-y\\|_2 > 0$ 得到：\n$$\n\\frac{L}{2} \\le L_f.\n$$\n因此，$L_f$ 能取的最小值为 $\\frac{L}{2}$。\n\n或者，我们可以求 $f(x)$ 的黑塞矩阵。$f(x)$ 的黑塞矩阵是 $\\nabla f(x) = \\frac{L}{2}x$ 的梯度：\n$$\n\\nabla^2 f(x) = \\frac{d}{dx} \\left(\\frac{L}{2} x\\right) = \\frac{L}{2} I = \\begin{pmatrix} L/2  0 \\\\ 0  L/2 \\end{pmatrix}.\n$$\n梯度的利普希茨常数 $L_f$ 是黑塞矩阵的谱范数。\n$$\nL_f = \\|\\nabla^2 f(x)\\|_2 = \\left\\| \\frac{L}{2} I \\right\\|_2.\n$$\n这个对角矩阵的特征值均为 $\\frac{L}{2}$。谱范数是最大绝对值特征值。\n$$\nL_f = \\max\\left(\\left|\\frac{L}{2}\\right|, \\left|\\frac{L}{2}\\right|\\right) = \\frac{L}{2}.\n$$\n两种方法得到相同的结果。最小常数 $L_f$ 是 $\\frac{L}{2}$。", "answer": "$$\n\\boxed{\\frac{L}{2}}\n$$", "id": "3144694"}, {"introduction": "在掌握了二次函数的基础之上，让我们来分析一个在深度学习等领域中常见的非线性“饱和”函数。本练习要求你通过计算并分析函数的Hessian矩阵来确定一个全局利普希茨常数。完成这个挑战 [@problem_id:3144663] 将加深你对函数曲率如何随位置变化的理解，特别是函数在输入值很大时趋于平坦的“饱和”行为如何使其光滑性常数由原点附近的局部行为决定。", "problem": "考虑函数 $f:\\mathbb{R}^{n}\\to\\mathbb{R}$，其定义为 $f(x)=\\frac{\\|x\\|_{2}^{2}}{1+\\|x\\|_{2}^{2}}$，其中 $\\|x\\|_{2}$ 表示欧几里得范数。从关于欧几里得范数的利普希茨连续梯度的定义出发，使用多元微积分的核心知识来确定一个全局利普希茨常数 $L$，使得梯度 $\\nabla f$ 对所有 $x,y\\in\\mathbb{R}^{n}$ 满足 $ \\|\\nabla f(x)-\\nabla f(y)\\|_{2}\\leq L\\,\\|x-y\\|_{2}$。你的推导必须基于第一性原理，并且在科学上保持一致。作为推理的一部分，解释当 $\\|x\\|_{2}$ 很大时，$f$ 的饱和行为如何影响你获得的 $L$ 的界。将 $L$ 的最终值以一个精确的数字形式给出。不需要四舍五入，也不涉及单位。", "solution": "我们从定义开始：如果存在一个常数 $L\\geq 0$，使得对于所有 $x,y\\in\\mathbb{R}^{n}$，一个函数具有关于欧几里得范数的利普希茨连续梯度，则\n$$\n\\|\\nabla f(x)-\\nabla f(y)\\|_{2}\\leq L\\,\\|x-y\\|_{2}.\n$$\n根据多元微积分和向量值函数的中值定理，一个经过充分验证的事实是，当 $f$ 是二次连续可微且其黑塞矩阵 $\\nabla^{2}f(x)$ 对所有 $x$ 都存在时，一个充分的（且在许多光滑情况下是紧的）全局利普希茨常数由其黑塞矩阵谱范数的一致上界给出：\n$$\nL=\\sup_{x\\in\\mathbb{R}^{n}}\\|\\nabla^{2}f(x)\\|_{2},\n$$\n其中 $\\|\\cdot\\|_{2}$ 表示由欧几里得范数诱导的算子范数。为了计算这个界，我们将推导 $f$ 的梯度和黑塞矩阵，然后一致地对所有 $x$ 计算 $\\nabla^{2}f(x)$ 的最大绝对特征值。\n\n令 $s=\\|x\\|_{2}^{2}=x^{\\top}x$。定义 $g(s)=\\frac{s}{1+s}$。那么 $f(x)=g(s)$ 是 $g$ 与 $s(x)$ 的复合函数。根据链式法则和已知的梯度 $\\nabla s(x)=2x$，我们有\n$$\ng'(s)=\\frac{(1+s)-s}{(1+s)^{2}}=\\frac{1}{(1+s)^{2}},\n\\quad\\text{所以}\\quad\n\\nabla f(x)=g'(s)\\,\\nabla s(x)=\\frac{2x}{(1+s)^{2}}.\n$$\n为了求黑塞矩阵，将 $\\nabla f(x)$ 写成 $a(x)\\,x$ 的形式，其中 $a(x)=\\frac{2}{(1+s)^{2}}$。$a(x)\\,x$ 的雅可比矩阵是\n$$\n\\nabla^{2}f(x)=a(x)\\,I + x\\,(\\nabla a(x))^{\\top},\n$$\n其中 $I$ 是单位矩阵，$\\nabla a(x)$ 是标量场 $a(x)$ 的梯度。通过 $s$ 对 $a(x)$ 关于 $x$ 求导：\n$$\na(x)=2(1+s)^{-2},\\quad \\frac{\\mathrm{d}a}{\\mathrm{d}s} = -4(1+s)^{-3},\n\\quad \\nabla a(x)=\\frac{\\mathrm{d}a}{\\mathrm{d}s}\\,\\nabla s(x) = -4(1+s)^{-3}\\cdot 2x = -\\frac{8x}{(1+s)^{3}}.\n$$\n因此，\n$$\n\\nabla^{2}f(x)=\\frac{2}{(1+s)^{2}}\\,I \\;-\\; \\frac{8}{(1+s)^{3}}\\,x x^{\\top}.\n$$\n这是一个对称矩阵，形式为 $\\alpha\\,I - \\beta\\,u u^{\\top}$，其中 $\\alpha=\\frac{2}{(1+s)^{2}}$，$\\beta=\\frac{8\\|x\\|_{2}^{2}}{(1+s)^{3}}=\\frac{8s}{(1+s)^{3}}$，并且当 $x\\neq 0$ 时 $u$ 是 $x$ 方向的任意单位向量（如果 $x=0$，则第二项为 $0$）。其谱可以使用秩一更新的特征结构来表征。具体来说：\n- 在与 $u$ 正交的任何方向上，特征值为 $\\lambda_{\\perp}=\\alpha=\\frac{2}{(1+s)^{2}}$（重数为 $n-1$）。\n- 在 $u$ 方向上，由于 $x x^{\\top}$ 的作用相当于 $s\\,u u^{\\top}$，特征值为\n$$\n\\lambda_{\\parallel}=\\alpha - \\beta = \\frac{2}{(1+s)^{2}} - \\frac{8s}{(1+s)^{3}} = \\frac{2 - 6s}{(1+s)^{3}}.\n$$\n对称矩阵的谱范数 $\\|\\nabla^{2}f(x)\\|_{2}$ 等于其特征值中的最大绝对值。因此，\n$$\n\\|\\nabla^{2}f(x)\\|_{2}=\\max\\!\\left\\{\\left|\\lambda_{\\perp}\\right|,\\left|\\lambda_{\\parallel}\\right|\\right\\}=\\max\\!\\left\\{\\frac{2}{(1+s)^{2}},\\;\\frac{|2-6s|}{(1+s)^{3}}\\right\\}, \\quad s=\\|x\\|_{2}^{2}\\ge 0.\n$$\n我们现在在 $s\\ge 0$ 的范围内最大化这个表达式。\n\n首先，观察到 $\\lambda_{\\perp}(s)=\\frac{2}{(1+s)^{2}}$ 是 $s$ 的严格递减函数，并在 $s=0$ 时达到其最大值：\n$$\n\\max_{s\\ge 0}\\lambda_{\\perp}(s)=\\lambda_{\\perp}(0)=\\frac{2}{(1+0)^{2}}=2.\n$$\n接下来，考虑 $\\left|\\lambda_{\\parallel}(s)\\right|=\\frac{|2-6s|}{(1+s)^{3}}$。分两个区域讨论：\n- 对于 $0\\le s\\le \\frac{1}{3}$，我们有 $\\left|\\lambda_{\\parallel}(s)\\right|=\\frac{2-6s}{(1+s)^{3}}$。求导\n$$\nh(s)=\\frac{2-6s}{(1+s)^{3}},\\quad h'(s)=\\frac{12(s-1)}{(1+s)^{4}},\n$$\n该导数在 $[0,\\frac{1}{3}]$ 上为负。因此 $h$ 在该区间上递减，其在此区间的最大值在 $s=0$ 处取得：\n$$\n\\max_{0\\le s\\le \\frac{1}{3}}\\left|\\lambda_{\\parallel}(s)\\right|=h(0)=\\frac{2}{1^{3}}=2.\n$$\n- 对于 $s\\ge \\frac{1}{3}$，我们有 $\\left|\\lambda_{\\parallel}(s)\\right|=\\frac{6s-2}{(1+s)^{3}}$。求导\n$$\nk(s)=\\frac{6s-2}{(1+s)^{3}},\\quad k'(s)=\\frac{12(1-s)}{(1+s)^{4}},\n$$\n该导数在 $\\left[\\frac{1}{3},1\\right]$ 上为正，在 $s>1$ 时为负。因此，在 $s\\ge \\frac{1}{3}$ 上的最大值出现在 $s=1$ 处：\n$$\n\\max_{s\\ge \\frac{1}{3}}\\left|\\lambda_{\\parallel}(s)\\right|=k(1)=\\frac{6\\cdot 1-2}{(1+1)^{3}}=\\frac{4}{8}=\\frac{1}{2}.\n$$\n结合两个区域，$\\left|\\lambda_{\\parallel}(s)\\right|$ 的全局最大值为 $\\max\\{2,\\frac{1}{2}\\}=2$。因此，\n$$\n\\sup_{x\\in\\mathbb{R}^{n}}\\|\\nabla^{2}f(x)\\|_{2}=\\sup_{s\\ge 0}\\max\\!\\left\\{\\frac{2}{(1+s)^{2}},\\;\\frac{|2-6s|}{(1+s)^{3}}\\right\\}=2.\n$$\n根据前面的原理，这个上确界为梯度提供了一个有效的全局利普希茨常数：\n$$\nL=2.\n$$\n\n最后，我们讨论饱和行为的影响。当 $\\|x\\|_{2}\\to\\infty$ 时，函数 $f(x)=\\frac{\\|x\\|_{2}^{2}}{1+\\|x\\|_{2}^{2}}$ 趋近于饱和值 $1$，梯度 $\\nabla f(x)=\\frac{2x}{(1+\\|x\\|_{2}^{2})^{2}}$ 趋于 $0$。黑塞矩阵 $\\nabla^{2}f(x)=\\frac{2}{(1+s)^{2}}I-\\frac{8}{(1+s)^{3}}x x^{\\top}$ 的算子范数也随着 $s\\to\\infty$ 而衰减到零矩阵。这意味着曲率在原点附近最大，并随着函数的饱和而减小，因此全局利普希茨常数 $L$ 由 $\\|x\\|_{2}$ 较小时的行为决定，特别是在 $x=0$ 处达到其最大值，此时 $\\nabla^{2}f(0)=2I$ 且 $\\|\\nabla^{2}f(0)\\|_{2}=2$。", "answer": "$$\\boxed{2}$$", "id": "3144663"}, {"introduction": "前面的练习表明，函数的曲率（即局部光滑度）在不同区域可能存在巨大差异，因此使用单一的全局$L$常数可能过于保守。这个最终的编程实践将理论洞察转化为算法优势，你将实现一种能够利用局部光滑度估计的自适应梯度下降算法。通过将该自适应方法与使用固定步长的基准方法进行比较 [@problem_id:3144675]，你将直观地看到根据局部信息调整步长是如何有效加速收敛过程的，这是现代优化算法设计的核心思想之一。", "problem": "考虑二次连续可微函数 $f : \\mathbb{R}^n \\to \\mathbb{R}$，其定义为\n$$\nf(\\mathbf{x}) \\;=\\; \\sum_{i=1}^n \\left( \\sin(x_i) + 0.1\\,x_i^2 \\right),\n$$\n其中 $\\mathbf{x} = (x_1,\\dots,x_n) \\in \\mathbb{R}^n$。所有三角量必须以弧度为单位进行处理。梯度 $\\nabla f$ 和黑塞矩阵 $\\nabla^2 f$ 处处存在，并且梯度在 $\\mathbb{R}^n$ 上是全局 Lipschitz 连续的。\n\n你的任务是，对于非凸函数 $f$，经验性地比较一个全局 Lipschitz 常数 $L_{\\mathrm{global}}$ 与沿着梯度下降（GD）轨迹评估的局部 Lipschitz 常数 $L_{\\mathrm{local}}(\\mathbf{x}, r)$，并实现一个随时间递增同时保持局部安全的步长策略。\n\n该比较和策略必须基于以下具有数学依据的构造：\n\n- 全局 Lipschitz 常数 $L_{\\mathrm{global}}$ 是任何可证明的上界，确保对于所有 $\\mathbf{x}, \\mathbf{y} \\in \\mathbb{R}^n$，都有 $\\|\\nabla f(\\mathbf{x}) - \\nabla f(\\mathbf{y})\\|_2 \\le L_{\\mathrm{global}} \\|\\mathbf{x} - \\mathbf{y}\\|_2$。\n- 对于给定的中心 $\\mathbf{x} \\in \\mathbb{R}^n$ 和半径 $r > 0$，局部 Lipschitz 常数 $L_{\\mathrm{local}}(\\mathbf{x}, r)$ 是任何可证明的上界，确保对于所有满足 $\\|\\mathbf{y} - \\mathbf{x}\\|_2 \\le r$ 的 $\\mathbf{y}$，都有 $\\|\\nabla f(\\mathbf{y}) - \\nabla f(\\mathbf{x})\\|_2 \\le L_{\\mathrm{local}}(\\mathbf{x}, r) \\|\\mathbf{y} - \\mathbf{x}\\|_2$。在本问题中，$f$ 的黑塞矩阵是对角矩阵，这允许你通过在轴对齐区间 $[x_i - r, x_i + r]$（对于 $i = 1,\\dots,n$）上取 $\\nabla^2 f$ 的对角元素绝对值的上确界来高效地计算 $L_{\\mathrm{local}}(\\mathbf{x}, r)$。你必须通过在每个坐标区间内使用 $m$ 个点进行均匀采样来数值近似这个上确界，然后取所有坐标上的最大值。\n\n你必须实现两个从相同初始点开始并运行指定迭代次数 $T$ 的 GD 过程：\n\n1. 基线恒定步长规则：\n   - 使用 $\\alpha_{\\mathrm{base}} = \\theta / L_{\\mathrm{global}}$，其中 $\\theta \\in (0,1)$ 是一个给定的标量。\n   - 更新规则：$\\mathbf{x}_{t+1} = \\mathbf{x}_t - \\alpha_{\\mathrm{base}} \\nabla f(\\mathbf{x}_t)$，对于 $t = 0,1,\\dots,T-1$。\n\n2. 提议的保持局部安全的递增策略：\n   - 定义一个递增因子 $\\eta_t = 1 - e^{-\\kappa t}$（其中 $\\kappa > 0$）和一个半径策略 $r_t = r_0 \\,\\rho^t$（其中 $r_0 > 0$ 且 $\\rho \\in (0,1]$）。\n   - 在第 $t$ 次迭代时，如上所述计算 $L_{\\mathrm{local}}(\\mathbf{x}_t, r_t)$ 的数值近似值。\n   - 选择步长\n     $$\n     \\alpha_t \\;=\\; \\min\\!\\left( \\frac{\\eta_t}{L_{\\mathrm{local}}(\\mathbf{x}_t, r_t)} \\,,\\, \\frac{r_t}{\\|\\nabla f(\\mathbf{x}_t)\\|_2} \\right),\n     $$\n     其中第二项强制施加一个信赖域上限，以将步长保持在半径为 $r_t$ 的球内，从而使局部 Lipschitz 界适用。\n   - 更新规则：$\\mathbf{x}_{t+1} = \\mathbf{x}_t - \\alpha_t \\nabla f(\\mathbf{x}_t)$，对于 $t = 0,1,\\dots,T-1$。\n\n对于每次运行，记录最终的函数值 $f(\\mathbf{x}_T)$。对于提议的策略，还需检查序列 $(\\alpha_t)_{t=0}^{T-1}$ 是否单调非递减。\n\n角度单位要求：所有三角函数求值和区间都必须以弧度为单位。\n\n为以下测试套件实现上述内容。每个测试用例由一个元组 $(n, \\mathbf{x}_0, T, r_0, \\rho, \\kappa, \\theta, m)$ 指定：\n\n- 用例 A（一般情况）：$(3, (2.0, -1.0, 0.5), 60, 0.8, 0.95, 0.12, 0.8, 300)$。\n- 用例 B（靠近高曲率区域）：$(3, (-\\tfrac{\\pi}{2}, -\\tfrac{\\pi}{2}, -\\tfrac{\\pi}{2}), 60, 0.4, 0.90, 0.10, 0.8, 300)$。\n- 用例 C（大半径，局部等于全局行为）：$(3, (10.0, -10.0, 3.0), 60, 5.0, 1.00, 0.15, 0.8, 300)$。\n\n你的程序必须：\n- 为给定的 $f$ 计算一个具有数学依据的 $L_{\\mathrm{global}}$。\n- 对每个用例，运行两种 GD 变体并计算三个输出：\n  1. 提议的递增策略下的最终函数值 $f(\\mathbf{x}_T^{\\mathrm{inc}})$，作为浮点数。\n  2. 基线恒定步长下的最终函数值 $f(\\mathbf{x}_T^{\\mathrm{base}})$，作为浮点数。\n  3. 一个布尔值，指示对于提议的策略，$(\\alpha_t)$ 是否是单调非递减的。\n- 生成单行输出，其中包含结果，格式为由逗号分隔的三个子列表，每个用例一个子列表，每个子列表的形式为 $[f(\\mathbf{x}_T^{\\mathrm{inc}}), f(\\mathbf{x}_T^{\\mathrm{base}}), \\text{is\\_monotone}]$，并用方括号括起来，例如：$[[a_1,b_1,c_1],[a_2,b_2,c_2],[a_3,b_3,c_3]]$。\n\n不允许外部输入。程序必须是自包含的并确定性地运行。", "solution": "问题陈述已经过验证，被认为是合理的。这是一个在数值优化领域中定义明确、有科学依据的问题，没有矛盾、歧义或故作高深的主张。我们可以开始进行正式的求解。\n\n核心任务是实现并比较两种用于最小化给定非凸函数 $f(\\mathbf{x})$ 的梯度下降（GD）算法。第一种算法使用基于梯度全局 Lipschitz 常数的保守恒定步长。第二种算法采用一种更激进的、局部自适应的步长策略，该策略旨在随时间递增，同时通过遵循局部曲率估计来保持安全。\n\n首先，我们将问题的数学组成部分形式化。\n\n目标函数是 $f : \\mathbb{R}^n \\to \\mathbb{R}$，由下式给出：\n$$f(\\mathbf{x}) = \\sum_{i=1}^n \\left( \\sin(x_i) + 0.1\\,x_i^2 \\right)$$\n由于此函数是关于每个坐标 $x_i$ 的可分离函数的总和，其梯度 $\\nabla f(\\mathbf{x})$ 和黑塞矩阵 $\\nabla^2 f(\\mathbf{x})$ 的计算非常直接。梯度的第 $j$ 个分量是：\n$$(\\nabla f(\\mathbf{x}))_j = \\frac{\\partial f}{\\partial x_j} = \\cos(x_j) + 0.2\\,x_j$$\n黑塞矩阵是对角矩阵，因为当 $i \\neq j$ 时，混合偏导数 $\\frac{\\partial^2 f}{\\partial x_i \\partial x_j} = 0$。对角线元素为：\n$$(\\nabla^2 f(\\mathbf{x}))_{ii} = \\frac{\\partial^2 f}{\\partial x_i^2} = -\\sin(x_i) + 0.2$$\n因此，黑塞矩阵为：\n$$\\nabla^2 f(\\mathbf{x}) = \\mathrm{diag}(-\\sin(x_1) + 0.2, \\dots, -\\sin(x_n) + 0.2)$$\n\n为了实现基线 GD 算法，我们需要梯度 $\\nabla f$ 的一个全局 Lipschitz 常数 $L_{\\mathrm{global}}$。对于一个二次连续可微函数，可以通过在整个定义域 $\\mathbb{R}^n$ 上取黑塞矩阵谱范数的上确界来找到这样一个常数。对角矩阵的谱范数（或诱导 $2$-范数）是其对角元素绝对值的最大值。\n$$L_{\\mathrm{global}} = \\sup_{\\mathbf{x} \\in \\mathbb{R}^n} \\|\\nabla^2 f(\\mathbf{x})\\|_2 = \\sup_{\\mathbf{x} \\in \\mathbb{R}^n} \\max_{i=1, \\dots, n} |-\\sin(x_i) + 0.2|$$\n表达式 $|-\\sin(x_i) + 0.2|$ 在 $\\sin(x_i)$ 取其极值 $-1$ 和 $1$ 时达到最大。当 $\\sin(x_i) = -1$ 时，值为 $|-(-1) + 0.2| = |1.2| = 1.2$。当 $\\sin(x_i) = 1$ 时，值为 $|-1 + 0.2| = |-0.8| = 0.8$。这些值中的最大值是 $1.2$。因此，一个有效的全局 Lipschitz 常数是 $L_{\\mathrm{global}} = 1.2$。\n\n接下来，我们定义局部 Lipschitz 常数 $L_{\\mathrm{local}}(\\mathbf{x}, r)$。这个常数必须对于 $\\mathbf{x}$ 邻域中的所有 $\\mathbf{y}$ 满足 $\\|\\nabla f(\\mathbf{y}) - \\nabla f(\\mathbf{x})\\|_2 \\le L_{\\mathrm{local}}(\\mathbf{x}, r) \\|\\mathbf{y} - \\mathbf{x}\\|_2$。问题指定使用一个可证明的上界。一个合适的界是包含该邻域的区域上黑塞矩阵谱范数的上确界。按照指示，我们使用由区间 $[x_i - r, x_i + r]$ 定义的轴对齐超矩形。\n$$L_{\\mathrm{local}}(\\mathbf{x}, r) = \\max_{i=1,\\dots,n} \\left( \\sup_{y_i \\in [x_i - r, x_i + r]} |-\\sin(y_i) + 0.2| \\right)$$\n问题要求通过在每个区间 $[x_i - r, x_i + r]$ 内均匀采样 $m$ 个点，在这些点上计算 $|-\\sin(y_i) + 0.2|$，找到每个坐标 $i$ 的最大值，然后取所有坐标上的最大值来数值近似这个值。\n\n定义了这些组件后，我们指定两种 GD 算法。两者都从一个初始点 $\\mathbf{x}_0$ 开始，并运行 $T$ 次迭代。\n\n1.  **基线恒定步长 GD：**\n    步长是恒定的：$\\alpha_{\\mathrm{base}} = \\theta / L_{\\mathrm{global}}$，其中 $\\theta \\in (0,1)$ 是一个给定的参数。\n    对于 $t = 0, 1, \\dots, T-1$ 的更新规则是：\n    $$\\mathbf{x}_{t+1}^{\\mathrm{base}} = \\mathbf{x}_t^{\\mathrm{base}} - \\alpha_{\\mathrm{base}} \\nabla f(\\mathbf{x}_t^{\\mathrm{base}})$$\n\n2.  **提议的递增步长 GD：**\n    该算法使用一个时变步长 $\\alpha_t$。在每次迭代 $t$：\n    - 计算一个递增因子 $\\eta_t = 1 - e^{-\\kappa t}$，其中 $\\kappa > 0$。当 $t \\to \\infty$ 时，这个因子趋近于 1。\n    - 计算一个局部半径 $r_t = r_0 \\rho^t$，其中 $r_0 > 0$ 且 $\\rho \\in (0,1]$。这个半径定义了进行局部分析的区域。\n    - 局部 Lipschitz 常数 $L_{\\mathrm{local}}(\\mathbf{x}_t, r_t)$ 按照前面描述的方式进行数值近似。\n    - 步长 $\\alpha_t$ 被选择为：\n      $$\\alpha_t = \\min\\!\\left( \\frac{\\eta_t}{L_{\\mathrm{local}}(\\mathbf{x}_t, r_t)} \\,,\\, \\frac{r_t}{\\|\\nabla f(\\mathbf{x}_t)\\|_2} \\right)$$\n      第一项根据局部曲率调整步长，并由递增因子 $\\eta_t$ 进行缩放。第二项作为一个信赖域约束，确保更新步长 $\\alpha_t \\nabla f(\\mathbf{x}_t)$ 的范数不大于 $r_t$。这将下一次迭代保持在当前迭代点周围半径为 $r_t$ 的球内，在该球内局部 Lipschitz 估计是适用的。如果 $\\|\\nabla f(\\mathbf{x}_t)\\|_2=0$，这也避免了除以零的情况。\n    - 对于 $t = 0, 1, \\dots, T-1$ 的更新规则是：\n      $$\\mathbf{x}_{t+1}^{\\mathrm{inc}} = \\mathbf{x}_t^{\\mathrm{inc}} - \\alpha_t \\nabla f(\\mathbf{x}_t^{\\mathrm{inc}})$$\n\n该实现将为提供的每个测试用例执行这两种算法。对于每个用例，它将报告最终函数值 $f(\\mathbf{x}_T^{\\mathrm{inc}})$、最终函数值 $f(\\mathbf{x}_T^{\\mathrm{base}})$，以及一个指示提议策略的步长序列 $(\\alpha_t)_{t=0}^{T-1}$ 是否单调非递减的布尔值。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the optimization problem by comparing two gradient descent schedules.\n    \"\"\"\n\n    # --- Mathematical function definitions ---\n\n    def f(x: np.ndarray) -> float:\n        \"\"\"Computes the value of the objective function f(x).\"\"\"\n        return np.sum(np.sin(x) + 0.1 * x**2)\n\n    def grad_f(x: np.ndarray) -> np.ndarray:\n        \"\"\"Computes the gradient of the objective function f(x).\"\"\"\n        return np.cos(x) + 0.2 * x\n\n    def hessian_diag_abs(y_coords: np.ndarray) -> np.ndarray:\n        \"\"\"Computes the absolute values of the diagonal entries of the Hessian.\"\"\"\n        return np.abs(-np.sin(y_coords) + 0.2)\n\n    # --- Lipschitz constant calculations ---\n\n    L_global = 1.2  # Derived analytically as sup ||nabla^2 f(x)||_2\n\n    def compute_L_local(x: np.ndarray, r: float, m: int) -> float:\n        \"\"\"\n        Numerically approximates the local Lipschitz constant L_local(x, r).\n        \"\"\"\n        n = x.shape[0]\n        max_eigenvalues = np.zeros(n)\n        for i in range(n):\n            # Sample m points in the interval [x_i - r, x_i + r]\n            sample_points = np.linspace(x[i] - r, x[i] + r, m)\n            # Evaluate the absolute Hessian diagonal entry at these points\n            hessian_values = hessian_diag_abs(sample_points)\n            # Find the max over the samples for this coordinate\n            max_eigenvalues[i] = np.max(hessian_values)\n        \n        # L_local is the max over all coordinates\n        return np.max(max_eigenvalues)\n\n    # --- Gradient Descent algorithms ---\n\n    def run_gd_baseline(x0: np.ndarray, T: int, theta: float) -> float:\n        \"\"\"Runs the baseline GD with a constant step-size.\"\"\"\n        alpha_base = theta / L_global\n        x = np.copy(x0)\n        for _ in range(T):\n            grad = grad_f(x)\n            x = x - alpha_base * grad\n        return f(x)\n\n    def run_gd_proposed(x0: np.ndarray, T: int, r0: float, rho: float, kappa: float, m: int):\n        \"\"\"Runs the proposed GD with an increasing, locally safe step-size.\"\"\"\n        x = np.copy(x0)\n        alpha_history = []\n        \n        for t in range(T):\n            eta_t = 1.0 - np.exp(-kappa * t)\n            r_t = r0 * (rho ** t)\n            \n            L_local_t = compute_L_local(x, r_t, m)\n            \n            grad = grad_f(x)\n            grad_norm = np.linalg.norm(grad)\n\n            # Term 1 of the min expression for alpha_t\n            alpha_term1 = eta_t / L_local_t\n            \n            # Term 2 (trust-region cap), with a safeguard for grad_norm == 0\n            if grad_norm  1e-12:\n                # If gradient is zero, the step is zero. alpha's value is irrelevant for the update.\n                # However, to avoid division by zero, we take the other term.\n                alpha_term2 = np.inf\n            else:\n                alpha_term2 = r_t / grad_norm\n            \n            alpha_t = min(alpha_term1, alpha_term2)\n            alpha_history.append(alpha_t)\n            \n            x = x - alpha_t * grad\n            \n        final_f_val = f(x)\n        \n        # Check for monotonic non-decreasing property of the alpha sequence\n        is_monotone = all(alpha_history[i] >= alpha_history[i-1] for i in range(1, len(alpha_history)))\n        \n        return final_f_val, is_monotone\n\n    # --- Test suite execution ---\n\n    test_cases = [\n        # Case A: (n, x0, T, r0, rho, kappa, theta, m)\n        (3, np.array([2.0, -1.0, 0.5]), 60, 0.8, 0.95, 0.12, 0.8, 300),\n        # Case B:\n        (3, np.array([-np.pi/2, -np.pi/2, -np.pi/2]), 60, 0.4, 0.90, 0.10, 0.8, 300),\n        # Case C:\n        (3, np.array([10.0, -10.0, 3.0]), 60, 5.0, 1.00, 0.15, 0.8, 300),\n    ]\n\n    results = []\n    for case in test_cases:\n        n, x0, T, r0, rho, kappa, theta, m = case\n        \n        # Run baseline GD\n        f_final_base = run_gd_baseline(x0, T, theta)\n        \n        # Run proposed GD\n        f_final_inc, is_monotone = run_gd_proposed(x0, T, r0, rho, kappa, m)\n        \n        # The boolean needs to be lowercase for the final string representation\n        is_monotone_str = str(is_monotone).lower()\n        \n        results.append(f\"[{f_final_inc},{f_final_base},{is_monotone_str}]\")\n\n    # Format the final output string as specified\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3144675"}]}