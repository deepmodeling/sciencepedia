{"hands_on_practices": [{"introduction": "要真正理解变分自编码器（VAE），最好的方法就是亲手搭建一个。这项实践将指导您从零开始实现一个简单的线性 VAE，并在一个合成的单细胞 RNA 测序（scRNA-seq）数据集上进行训练。通过完成这项练习，您将对 VAE 的损失函数、重参数化技巧，以及最重要的——如何通过将学习到的潜空间与已知的生物过程（如细胞周期）关联起来进行解释，有一个具体而深刻的理解。[@problem_id:2439780] 这是在基因组学中应用 VAE 的一项基本技能。", "problem": "您将实现一个完整、可运行的程序，该程序在一个合成的单细胞 RNA 测序 (scRNA-seq) 数据集上训练一个一维变分自编码器 (VAE)，该数据集代表一个主要的类细胞周期因子。然后，您将通过分析两个典型细胞周期标记基因的重建轨迹，来证明遍历该单一潜在维度对应于可解释的细胞周期变化。\n\n基本依据。仅使用以下基本定义和事实来推导和设计您的解决方案：\n- 变分自编码器 (VAE) 基于最大化证据下界 (ELBO)。对于数据 $x$、潜变量 $z$、先验 $p(z)$、似然 $p_\\theta(x \\mid z)$ 和变分后验 $q_\\phi(z \\mid x)$，每个样本的 ELBO 为\n$$\n\\mathcal{L}(\\theta,\\phi;x) \\;=\\; \\mathbb{E}_{q_\\phi(z \\mid x)}\\left[\\log p_\\theta(x \\mid z)\\right] \\;-\\; \\mathrm{KL}\\!\\left(q_\\phi(z \\mid x)\\,\\|\\,p(z)\\right),\n$$\n其中 $\\mathrm{KL}$ 表示 Kullback–Leibler 散度。\n- 重参数化技巧将 $z$ 写为 $z = \\mu_\\phi(x) + \\sigma_\\phi(x) \\,\\epsilon$，其中 $\\epsilon \\sim \\mathcal{N}(0,1)$，从而能够对 $\\mathbb{E}_{q_\\phi}$ 进行基于梯度的优化。\n- 对于高斯先验 $p(z)=\\mathcal{N}(0,1)$ 和高斯变分后验 $q_\\phi(z \\mid x)=\\mathcal{N}(\\mu_\\phi(x), \\sigma_\\phi^2(x))$，每个样本的 Kullback–Leibler 散度等于\n$$\n\\mathrm{KL}\\!\\left(q_\\phi(z \\mid x)\\,\\|\\,p(z)\\right) \\;=\\; \\tfrac{1}{2}\\left(\\mu_\\phi(x)^2 + \\sigma_\\phi(x)^2 - \\log \\sigma_\\phi(x)^2 - 1\\right).\n$$\n- 对于高斯解码器 $p_\\theta(x \\mid z) = \\mathcal{N}(f_\\theta(z), \\sigma_x^2 I)$，最大化 $\\mathbb{E}_{q_\\phi}[\\log p_\\theta(x \\mid z)]$（在相差一个加性常数的情况下）等价于最小化期望平方重建误差 $\\tfrac{1}{2\\sigma_x^2}\\,\\mathbb{E}_{q_\\phi}\\!\\left[\\lVert x - f_\\theta(z)\\rVert_2^2\\right]$。\n\n要实现的模型和数据集规范。您必须构建一个具有线性编码器和线性解码器的一维 VAE：\n- 先验：$p(z)=\\mathcal{N}(0,1)$。\n- 解码器：$p_\\theta(x\\mid z)=\\mathcal{N}(W z + b,\\; \\sigma_x^2 I)$，其中 $\\sigma_x^2 = 1$ 为固定值，而 $W \\in \\mathbb{R}^{D \\times 1}$ 和 $b \\in \\mathbb{R}^{D}$ 是可训练的。\n- 编码器：$q_\\phi(z \\mid x)=\\mathcal{N}(a^\\top x + c,\\; \\exp(\\ell))$，其中 $a \\in \\mathbb{R}^{D}$、$c \\in \\mathbb{R}$ 和 $\\ell \\in \\mathbb{R}$（一个标量对数方差）是可训练的。\n- 训练目标：最小化负 ELBO，Kullback–Leibler 散度项上可带有可选的 $\\beta$ 权重（即 $\\beta$-VAE 目标），即\n$$\n\\mathcal{J}(\\theta,\\phi) \\;=\\; \\frac{1}{N}\\sum_{i=1}^{N}\\left\\{\\tfrac{1}{2}\\left\\lVert x_i - (W z_i + b)\\right\\rVert_2^2 \\;+\\; \\beta \\cdot \\tfrac{1}{2}\\left(\\mu_i^2 + \\sigma^2 - \\ell - 1\\right)\\right\\},\n$$\n其中 $z_i = \\mu_i + \\sigma \\epsilon_i$，$\\mu_i = a^\\top x_i + c$，$\\sigma = \\exp(\\tfrac{1}{2}\\ell)$，以及 $\\epsilon_i \\sim \\mathcal{N}(0,1)$。\n- 优化：使用随机梯度下降或全批量梯度下降及重参数化技巧。要优化的参数是 $W$、$b$、$a$、$c$ 和 $\\ell$。\n\n要实现的合成 scRNA-seq 数据生成器。对于每个测试用例，生成 $N$ 个细胞和 $D$ 个基因，这些数据遵循一个代表细胞周期评分的主要线性因子：\n- 选择 $D = 5$。令基因索引 $0$ 表示一个典型的 S 期标记物（例如，增殖细胞核抗原），基因索引 $1$ 表示一个典型的 G2/M 期标记物（例如，细胞周期蛋白 B1）。为简单起见，我们将它们称为 “PCNA”（索引 $0$）和 “CCNB1”（索引 $1$）。\n- 为 $i = 1,\\dots,N$ 采样潜在得分 $s_i \\sim \\mathcal{N}(0,1)$。\n- 设真实载荷向量为 $u \\in \\mathbb{R}^D$，其中 $u_0 = 1.0$，$u_1 = -0.8$，$u_2 = 0.25$，$u_3 = 0.1$，$u_4 = -0.05$。\n- 设基线向量为 $m \\in \\mathbb{R}^D$，其中 $m_0 = 2.0$，$m_1 = 1.5$，$m_2 = 0.5$，$m_3 = 0.1$，$m_4 = -0.2$。\n- 观测值生成如下\n$$\nx_i \\;=\\; m \\;+\\; u\\, s_i \\;+\\; \\varepsilon_i,\n\\quad\\text{with}\\quad\n\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma_\\mathrm{noise}^2 I_D),\n$$\n其中 $\\sigma_\\mathrm{noise}$ 按每个测试用例指定。这种构造确保了主要变异是一个单一因子，该因子会增加“PCNA”并减少“CCNB1”，从而捕获一个类细胞周期的轴。\n\n潜在空间遍历和对齐标准。训练后，定义一个潜在遍历网格 $z_j$，其中 $j=1,\\dots,K$，$K=41$ 个点在 $-2$ 到 $2$ 之间等距分布。对于每个 $z_j$，解码得到 $\\hat{x}(z_j) = W z_j + b$。因为潜在轴的方向是任意的，所以通过翻转 $W$ 的符号（如果内部维护编码器方向，则相应地翻转）来强制执行一个标准方向，以使重建的“PCNA”随 $z$ 增加。将细胞周期对齐标准定义为：\n- $z$ 与重建的“PCNA”轨迹之间的斯皮尔曼等级相关性的非负值，且其绝对值至少为 $\\tau$。\n- $z$ 与重建的“CCNB1”轨迹之间的斯皮尔曼等级相关性的非正值，且其绝对值至少为 $\\tau$。\n使用 $\\tau = 0.95$，并使用标准的斯皮尔曼等级相关性定义计算相关性。\n\n测试套件。您的程序必须运行以下三个测试用例，并为每个用例返回一个布尔值，指示对齐标准是否成立：\n- 用例 A（理想情况）：$N=300$，$\\sigma_\\mathrm{noise}=0.30$，$\\beta=1.0$，seed $=42$。\n- 用例 B（中度噪声，正则化不足）：$N=120$，$\\sigma_\\mathrm{noise}=0.60$，$\\beta=0.5$，seed $=123$。\n- 用例 C（小样本，过度正则化）：$N=40$，$\\sigma_\\mathrm{noise}=0.80$，$\\beta=4.0$，seed $=2024$。\n\n最终输出格式。您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表的结果（例如，“[resultA,resultB,resultC]”），每个结果是分别对应于用例 A、B 和 C 的布尔字面量 True 或 False。不应打印任何其他文本。", "solution": "问题陈述已通过验证。\n\n### 步骤 1：提取给定信息\n\n**变分自编码器 (VAE) 公式：**\n- 证据下界 (ELBO): $\\mathcal{L}(\\theta,\\phi;x) \\;=\\; \\mathbb{E}_{q_\\phi(z \\mid x)}\\left[\\log p_\\theta(x \\mid z)\\right] \\;-\\; \\mathrm{KL}\\!\\left(q_\\phi(z \\mid x)\\,\\|\\,p(z)\\right)$\n- 重参数化技巧: $z = \\mu_\\phi(x) + \\sigma_\\phi(x) \\,\\epsilon$ ，其中 $\\epsilon \\sim \\mathcal{N}(0,1)$\n- 高斯后验和先验的 Kullback–Leibler (KL) 散度: $\\mathrm{KL}\\!\\left(q_\\phi(z \\mid x)\\,\\|\\,p(z)\\right) \\;=\\; \\tfrac{1}{2}\\left(\\mu_\\phi(x)^2 + \\sigma_\\phi(x)^2 - \\log \\sigma_\\phi(x)^2 - 1\\right)$\n- 高斯解码器重建项: 最小化 $\\tfrac{1}{2\\sigma_x^2}\\,\\mathbb{E}_{q_\\phi}\\!\\left[\\lVert x - f_\\theta(z)\\rVert_2^2\\right]$\n\n**具体模型和数据集实现：**\n- **模型架构（一维线性 VAE）：**\n    - 先验: $p(z)=\\mathcal{N}(0,1)$。\n    - 解码器: $p_\\theta(x\\mid z)=\\mathcal{N}(W z + b,\\; \\sigma_x^2 I)$ ，固定 $\\sigma_x^2 = 1$。可训练参数为 $W \\in \\mathbb{R}^{D \\times 1}$ 和 $b \\in \\mathbb{R}^{D}$。\n    - 编码器: $q_\\phi(z \\mid x)=\\mathcal{N}(a^\\top x + c,\\; \\exp(\\ell))$。可训练参数为 $a \\in \\mathbb{R}^{D}$，$c \\in \\mathbb{R}$ 和 $\\ell \\in \\mathbb{R}$。\n- **训练目标 ($\\beta$-VAE):**\n    - 最小化: $\\mathcal{J}(\\theta,\\phi) \\;=\\; \\frac{1}{N}\\sum_{i=1}^{N}\\left\\{\\tfrac{1}{2}\\left\\lVert x_i - (W z_i + b)\\right\\rVert_2^2 \\;+\\; \\beta \\cdot \\tfrac{1}{2}\\left(\\mu_i^2 + \\sigma^2 - \\ell - 1\\right)\\right\\}$\n    - 定义: $z_i = \\mu_i + \\sigma \\epsilon_i$, $\\mu_i = a^\\top x_i + c$, $\\sigma = \\exp(\\tfrac{1}{2}\\ell)$, $\\epsilon_i \\sim \\mathcal{N}(0,1)$。\n- **合成数据生成器：**\n    - 基因数量: $D = 5$。\n    - 基因身份: 索引 0 为 \"PCNA\", 索引 1 为 \"CCNB1\"。\n    - 潜在得分: $s_i \\sim \\mathcal{N}(0,1)$ ，对于 $i=1,\\dots,N$。\n    - 真实载荷向量: $u = [1.0, -0.8, 0.25, 0.1, -0.05]^\\top$。\n    - 基线向量: $m = [2.0, 1.5, 0.5, 0.1, -0.2]^\\top$。\n    - 观测模型: $x_i \\;=\\; m \\;+\\; u\\, s_i \\;+\\; \\varepsilon_i$，其中 $\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma_\\mathrm{noise}^2 I_D)$。\n- **潜在空间遍历和对齐标准：**\n    - 遍历网格: 从 $-2$ 到 $2$ 的 $K=41$ 个等距点 $z_j$。\n    - 重建轨迹: $\\hat{x}(z_j) = W z_j + b$。\n    - 方向: 如有必要，翻转 $W$ 的符号以确保重建的 \"PCNA\" 随 $z$ 增加。\n    - 对齐阈值: $\\tau = 0.95$。\n    - 条件: 斯皮尔曼等级相关性 $\\rho(z, \\text{\"PCNA\"}) \\ge \\tau$ 并且 $\\rho(z, \\text{\"CCNB1\"}) \\le -\\tau$。\n\n**测试套件：**\n- 用例 A: $N=300$, $\\sigma_\\mathrm{noise}=0.30$, $\\beta=1.0$, seed $=42$。\n- 用例 B: $N=120$, $\\sigma_\\mathrm{noise}=0.60$, $\\beta=0.5$, seed $=123$。\n- 用例 C: $N=40$, $\\sigma_\\mathrm{noise}=0.80$, $\\beta=4.0$, seed $=2024$。\n\n### 步骤 2：使用提取的给定信息进行验证\n\n根据验证标准对问题进行评估。\n- **科学依据：** 问题是合理的。它使用标准的机器学习模型（变分自编码器）和经典的统计模型（线性因子分析）来解决计算生物学中的一个问题（识别单细胞数据中的潜在结构）。合成数据生成过程是一个明确定义的模拟，这是测试算法的常用且有效的方法。\n- **适定性：** 问题是适定的。所有模型组件、目标函数、数据生成参数和评估标准都得到了明确的数学定义。没有缺失或矛盾的规范。\n- **客观性：** 问题以客观、正式的语言陈述。所有标准都是定量的和可验证的。\n\n### 步骤 3：结论与行动\n\n该问题是**有效**的。它是一个定义明确、具有科学依据的计算任务。现在将构建一个完整的解决方案。\n\n### 解决方案\n\n任务是构建并训练一个一维线性变分自编码器（$\\beta$-VAE），用于处理合成的单细胞数据，并验证学习到的潜在维度是否正确捕获了潜在的生物过程，在本例中是一个类细胞周期因子。\n\n**1. 合成数据生成**\n我们首先生成一个代表 $N$ 个细胞和 $D=5$ 个基因的合成数据集。细胞 $i$ 的表达谱 $x_i \\in \\mathbb{R}^D$ 由线性因子模型生成：\n$$\nx_i = m + u s_i + \\varepsilon_i\n$$\n这里，$s_i \\sim \\mathcal{N}(0,1)$ 是一个潜在得分，代表细胞在细胞周期轴上的位置。向量 $m \\in \\mathbb{R}^D$ 是基线平均表达水平，向量 $u \\in \\mathbb{R}^D$ 是定义变异方向的载荷向量。项 $\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma_{\\text{noise}}^2 I_D)$ 代表每个基因独立的生物和技术噪声。指定的载荷，$u_0 = 1.0$ (\"PCNA\") 和 $u_1 = -0.8$ (\"CCNB1\")，建立了从 S 期到 G2/M 期过渡的特征性反相关关系。我们的 VAE 必须从数据 $x_i$ 中恢复这个变异轴。\n\n**2. VAE模型和目标函数**\n我们采用一个具有一维潜在空间 $z \\in \\mathbb{R}$ 的线性 VAE。\n- **编码器** $q_\\phi(z|x)$，将数据点 $x \\in \\mathbb{R}^D$ 映射到潜在空间上的一个分布。我们使用一个高斯后验，其均值采用线性映射：\n$$\nq_\\phi(z \\mid x) = \\mathcal{N}(z \\mid \\mu(x), \\sigma^2)\n\\quad \\text{其中} \\quad\n\\mu(x) = a^\\top x + c \\quad \\text{和} \\quad \\sigma^2 = \\exp(\\ell)\n$$\n需要学习的参数是权重向量 $a \\in \\mathbb{R}^D$、偏置 $c \\in \\mathbb{R}$ 和对数方差 $\\ell \\in \\mathbb{R}$。\n\n- **解码器** $p_\\theta(x|z)$，将潜空间中的一个点 $z$ 映射回数据空间中的一个分布。我们使用一个具有固定单位方差的线性高斯解码器：\n$$\np_\\theta(x \\mid z) = \\mathcal{N}(x \\mid W z + b, I_D)\n$$\n需要学习的参数是权重矩阵（这里是一个向量）$W \\in \\mathbb{R}^{D \\times 1}$ 和偏置向量 $b \\in \\mathbb{R}^D$。\n\n训练目标是最小化负证据下界 (ELBO)，其中 KL 项由参数 $\\beta$ 加权（$\\beta$-VAE 目标）。对于一批 $N$ 个样本，目标函数为：\n$$\n\\mathcal{J} = \\frac{1}{N} \\sum_{i=1}^N \\left( \\underbrace{ \\frac{1}{2} \\|x_i - \\hat{x}_i\\|^2_2 }_{\\text{重建误差}} + \\beta \\cdot \\underbrace{ \\frac{1}{2} \\left( \\mu_i^2 + \\sigma^2 - \\log(\\sigma^2) - 1 \\right) }_{\\text{KL 散度}} \\right)\n$$\n其中 $\\hat{x}_i = W z_i + b$，而 $z_i$ 是使用重参数化技巧从 $q_\\phi(z|x_i)$ 中采样的：$z_i = \\mu_i + \\sigma \\epsilon_i$，其中 $\\epsilon_i \\sim \\mathcal{N}(0,1)$。代入 $\\log(\\sigma^2) = \\ell$，我们得到问题陈述中给出的精确目标函数。\n\n**3. 通过梯度下降进行优化**\n我们使用全批量梯度下降来最小化 $\\mathcal{J}$，优化参数 $\\phi = \\{a, c, \\ell\\}$ 和 $\\theta = \\{W, b\\}$。梯度是针对数据集上的总损失计算然后平均的。对于单个样本 $i$，设残差为 $r_i = x_i - (W z_i + b)$。损失 $L_i$ 的梯度使用链式法则导出。\n\n- **解码器参数梯度：**\n$$ \\nabla_W L_i = -r_i z_i^\\top \\quad\\quad \\nabla_b L_i = -r_i $$\n- **编码器参数梯度：**\n$$ \\nabla_a L_i = x_i(-W^\\top r_i + \\beta \\mu_i) $$\n$$ \\nabla_c L_i = -W^\\top r_i + \\beta \\mu_i $$\n$$ \\nabla_\\ell L_i = \\frac{1}{2} \\left[ (-W^\\top r_i) \\sigma \\epsilon_i + \\beta (\\sigma^2 - 1) \\right] $$\n其中 $\\sigma = \\exp(\\ell/2)$。参数通过迭代更新： $p \\leftarrow p - \\eta \\nabla_p \\mathcal{J}$，其中 $\\eta$ 是学习率。\n\n**4. 潜在空间遍历和验证**\n训练收敛后，我们评估潜在变量 $z$ 是否捕获了主要的变异轴。\n首先，我们建立一个规范方向。学习到的潜在轴的方向是任意的；模型可能会学到 $z$ 的增加对应于 \"PCNA\" 表达的减少，这与真实的生成因子 $s$ 相反。我们通过检查潜在变量遍历与重建的 \"PCNA\" 表达之间的相关性来强制执行一个标准方向。创建一个包含从 $-2$ 到 $2$ 的 $K=41$ 个值的网格 $z_j$。将这些值解码以获得轨迹 $\\hat{x}(z_j) = W z_j + b$。我们计算斯皮尔曼等级相关性 $\\rho(z, \\hat{x}_0(z))$。如果 $\\rho < 0$，我们通过翻转解码器权重 $W$ 的符号来反转潜在轴：$W \\leftarrow -W$。此操作确保 $z$ 的增加对应于 \"PCNA\" 表达的增加，从而使我们的潜在空间与生物学惯例对齐。如果我们要继续使用编码器，其参数也需要翻转（$a \\leftarrow -a$, $c \\leftarrow -c$），但对于解码器的分析，只有 $W$ 是重要的。\n\n最后，我们测试对齐标准。使用朝向正确的 $W$，我们计算 \"PCNA\"（基因索引 0）和 \"CCNB1\"（基因索引 1）轨迹的斯皮尔曼相关性。如果满足以下条件，则标准成立：\n$$\n\\rho(z, \\hat{x}_0(z)) \\ge 0.95 \\quad \\text{和} \\quad \\rho(z, \\hat{x}_1(z)) \\le -0.95\n$$\n这证实了模型在其单一潜在维度上学习到了两个关键标记基因的反相关行为。对所有三个测试用例重复此过程，以得出最终的布尔结果。", "answer": "```python\nimport numpy as np\nfrom scipy.stats import spearmanr\n\ndef solve():\n    \"\"\"\n    Main function to run the VAE experiment for all test cases.\n    \"\"\"\n\n    test_cases = [\n        # Case A: Happy path\n        {'N': 300, 'sigma_noise': 0.30, 'beta': 1.0, 'seed': 42},\n        # Case B: Moderate noise, under-regularized\n        {'N': 120, 'sigma_noise': 0.60, 'beta': 0.5, 'seed': 123},\n        # Case C: Small sample, over-regularized\n        {'N': 40, 'sigma_noise': 0.80, 'beta': 4.0, 'seed': 2024},\n    ]\n\n    results = []\n    for params in test_cases:\n        result = run_single_case(**params)\n        results.append(result)\n\n    # Format the final output as specified.\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef generate_data(N, sigma_noise, seed):\n    \"\"\"\n    Generates synthetic scRNA-seq data based on a linear factor model.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    D = 5\n    \n    # True generating parameters\n    u = np.array([1.0, -0.8, 0.25, 0.1, -0.05]).reshape(D, 1)\n    m = np.array([2.0, 1.5, 0.5, 0.1, -0.2])\n\n    # Latent scores and noise\n    s = rng.normal(loc=0.0, scale=1.0, size=(N, 1))\n    noise = rng.normal(loc=0.0, scale=sigma_noise, size=(N, D))\n\n    # Generate data\n    X = m + s @ u.T + noise\n    return X\n\ndef train_vae(X, beta, seed, learning_rate=0.01, epochs=1000):\n    \"\"\"\n    Trains a 1D linear Variational Autoencoder.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    N, D = X.shape\n\n    # Initialize VAE parameters\n    # Decoder params\n    W = rng.normal(loc=0.0, scale=0.1, size=(D, 1))\n    b = np.zeros(D)\n    # Encoder params\n    a = rng.normal(loc=0.0, scale=0.1, size=(D, 1))\n    c = 0.0\n    ell = 0.0 # log-variance\n\n    for epoch in range(epochs):\n        # --- Forward pass ---\n        # Encoder\n        mu = X @ a + c  # Shape (N, 1)\n        sigma = np.exp(0.5 * ell)\n        \n        # Reparameterization trick\n        epsilon = rng.normal(loc=0.0, scale=1.0, size=(N, 1))\n        z = mu + sigma * epsilon  # Shape (N, 1)\n        \n        # Decoder\n        X_recon = z @ W.T + b  # Shape (N, D)\n        \n        # --- Loss calculation (for monitoring, not strictly needed for gradients) ---\n        # recon_loss = 0.5 * np.mean(np.sum((X - X_recon)**2, axis=1))\n        # kl_loss = 0.5 * np.mean(mu**2 + sigma**2 - ell - 1)\n        # loss = recon_loss + beta * kl_loss\n        \n        # --- Gradient calculation (full-batch) ---\n        # Residual term\n        res = X - X_recon  # Shape (N, D)\n\n        # Gradients for decoder parameters (W, b)\n        grad_W = - (res.T @ z) / N\n        grad_b = - np.mean(res, axis=0)\n\n        # Gradients for encoder parameters (a, c, ell)\n        # Chain rule term from reconstruction loss back to z\n        d_recon_loss_d_z = - (res @ W) # Shape (N, 1)\n        \n        # Gradient for 'a'\n        d_kl_loss_d_mu = beta * mu\n        d_loss_d_mu = d_recon_loss_d_z + d_kl_loss_d_mu # Shape (N, 1)\n        grad_a = (X.T @ d_loss_d_mu) / N\n        \n        # Gradient for 'c'\n        grad_c = np.mean(d_loss_d_mu)\n\n        # Gradient for 'ell'\n        d_loss_d_z = d_recon_loss_d_z\n        d_z_d_ell = 0.5 * sigma * epsilon\n        d_kl_loss_d_ell = 0.5 * beta * (sigma**2 - 1)\n        grad_ell = np.mean(d_loss_d_z * d_z_d_ell) + d_kl_loss_d_ell\n\n        # --- Parameter update ---\n        W -= learning_rate * grad_W\n        b -= learning_rate * grad_b\n        a -= learning_rate * grad_a\n        c -= learning_rate * grad_c\n        ell -= learning_rate * grad_ell\n\n    return W, b\n\ndef analyze_model(W, b):\n    \"\"\"\n    Performs latent traversal and checks the alignment criterion.\n    \"\"\"\n    tau = 0.95\n    K = 41\n    z_grid = np.linspace(-2.0, 2.0, K)\n\n    # Decode the latent traversal grid\n    X_recon_traj = z_grid.reshape(-1, 1) @ W.T + b.reshape(1, -1)\n\n    # Extract trajectories for \"PCNA\" (gene 0) and \"CCNB1\" (gene 1)\n    pcna_traj = X_recon_traj[:, 0]\n    \n    # Check orientation and flip if necessary\n    rho_pcna, _ = spearmanr(z_grid, pcna_traj)\n\n    if rho_pcna  0:\n        W = -W\n        # Recalculate trajectories with flipped W\n        X_recon_traj = z_grid.reshape(-1, 1) @ W.T + b.reshape(1, -1)\n        pcna_traj = X_recon_traj[:, 0]\n        # Recalculate correlation for the check\n        rho_pcna, _ = spearmanr(z_grid, pcna_traj)\n        \n    ccnb1_traj = X_recon_traj[:, 1]\n    rho_ccnb1, _ = spearmanr(z_grid, ccnb1_traj)\n\n    # Verify the alignment criterion\n    is_aligned = (rho_pcna >= tau) and (rho_ccnb1 = -tau)\n    \n    return is_aligned\n\ndef run_single_case(N, sigma_noise, beta, seed):\n    \"\"\"\n    Executes one full test case from data generation to analysis.\n    \"\"\"\n    # Generate data with the case-specific seed for the entire process.\n    X = generate_data(N=N, sigma_noise=sigma_noise, seed=seed)\n    \n    # Train the VAE. The same seed ensures reproducible parameter initialization.\n    W, b = train_vae(X=X, beta=beta, seed=seed)\n\n    # Analyze the trained model\n    is_aligned = analyze_model(W, b)\n    \n    return is_aligned\n\nif __name__ == '__main__':\n    solve()\n```", "id": "2439780"}, {"introduction": "尽管 VAE 功能强大，但要有效使用它们，就必须了解其局限性。一个使用简单损失函数训练的标准 VAE 有时会产生在物理上或生物学上不合理的结果。这个思想实验探讨了一个关键的失败模式：一个在蛋白质结构上训练的 VAE 尽管重建误差很低，却生成了化学上不可能存在的键角。[@problem_id:2439813] 通过分析潜在原因，您将学会批判性地评估模型的目标函数（例如，笛卡尔坐标下的均方误差）与您希望捕捉的真实科学属性（例如，有效的立体化学）之间的一致性，这项实践磨练了您的模型诊断技能。", "problem": "一个变分自编码器（VAE）被训练用于根据高分辨率晶体学数据对蛋白质骨架结构进行建模。输入 $x$ 包含归一化坐标系中每个残基的骨架原子的笛卡尔坐标。该VAE使用一个编码器生成潜变量 $z$ 的近似后验，一个关于 $z$ 的标准正态先验，以及一个将 $z$ 映射到预测坐标的解码器。尽管在训练期间实现了较低的重构误差，但该模型频繁生成具有化学上不可能的键角（例如，远超出已知立体化学范围的C–N–C$\\alpha$角）和系统性扭曲的键长的结构。\n\n从第一性原理出发，VAE的目标函数结合了一个由解码器似然模型决定的数据拟合项和一个鼓励近似后验接近先验的正则化项。假设训练已经收敛，并且数据集确实包含物理上有效的结构。目标函数或解码器设计的哪个方面最可能是产生化学上不可能的键角的罪魁祸首？\n\n选择一个选项。\n\nA. Kullback–Leibler (KL) 散度项的权重太小，导致潜空间过拟合，从而违反了化学原理。\n\nB. 重构似然被建模为关于笛卡尔坐标的独立高斯分布，使用均方误差，并且解码器输出无约束的坐标；损失函数或解码器参数化中没有明确的立体化学感知约束。\n\nC. 用于梯度的随机重参数化技巧引入了偏差，这种偏差在解码过程中破坏了角度分布。\n\nD. 对潜变量选择标准正态先验，而不是学习先验或混合先验，迫使模型进入无效的几何区域。", "solution": "必须首先验证问题陈述的科学性和逻辑完整性。\n\n**第1步：提取已知条件**\n- **模型**：一个变分自编码器（VAE）。\n- **应用**：蛋白质骨架结构建模。\n- **输入数据 ($x$)**：来自高分辨率晶体学的骨架原子的笛卡尔坐标，表示在归一化坐标系中。\n- **VAE 架构**：\n    - 编码器：$q_{\\phi}(z|x)$，近似潜变量 $z$ 的后验分布。\n    - 先验：$p(z)$，一个标准正态分布，即 $p(z) = \\mathcal{N}(z | 0, I)$。\n    - 解码器：$p_{\\theta}(x|z)$，将潜变量 $z$ 映射到预测坐标。\n- **训练结果**：模型达到了低的重构误差。\n- **生成结果**：模型频繁生成具有化学上不可能的键角（例如C–N–C$\\alpha$角）和扭曲的键长的结构。\n- **假设**：训练过程已经收敛，并且源数据集只包含物理上有效的结构。\n- **目标函数**：VAE的目标函数结合了一个数据拟合项（重构似然）和一个正则化项（KL散度）。\n\n**第2步：使用提取的已知条件进行验证**\n问题陈述是有效的。\n- **科学上合理**：该问题描述了在将VAE等生成模型应用于分子结构时一个标准且有据可查的挑战。VAE、笛卡尔坐标与内坐标（键角、键长）、重构损失、KL散度以及蛋白质立体化学等概念在计算生物学和机器学习中都是标准的。尽管基于坐标的误差很低，但生成化学上无效的结构的问题是一个真实且关键的研究课题。\n- **问题明确**：这个问题提得很好，它要求在一组看似合理的选项中，找出导致某一特定、可观察到的失败模式的最可能原因。它旨在测试对VAE框架各组成部分及其物理含义的深入理解。\n- **客观性**：语言技术性强、精确且无主观性。\n\n**第3步：结论与行动**\n问题是有效的。将推导解决方案。\n\n**解决方案的推导**\n\n变分自编码器的目标函数，即在训练过程中最大化的证据下界（ELBO），公式如下：\n$$\n\\mathcal{L}(\\theta, \\phi; x) = \\mathbb{E}_{q_{\\phi}(z|x)}[\\log p_{\\theta}(x|z)] - D_{KL}(q_{\\phi}(z|x) || p(z))\n$$\n第一项 $\\mathbb{E}_{q_{\\phi}(z|x)}[\\log p_{\\theta}(x|z)]$ 代表重构似然。对于像笛卡尔坐标这样的连续数据，该项通常由一个具有固定协方差 $\\sigma^2 I$ 的多元高斯分布来建模。在此模型下最大化对数似然等同于最小化输入坐标 $x$ 和解码后坐标 $\\hat{x} = \\text{decoder}(z)$ 之间的均方误差（MSE）：\n$$\n\\text{重构损失} \\propto \\sum_{i=1}^{N} ||x_i - \\hat{x}_i||^2\n$$\n其中 $x_i$ 和 $\\hat{x}_i$ 是第 $i$ 个原子的坐标向量。\n\n问题陈述指出模型取得了低的重构误差。这意味着模型成功地最小化了原始坐标和重构坐标之间的均方误差。然而，它未能保持化学有效性，特别是键角和键长。\n\n键长和键角是内坐标，它们是笛卡尔坐标的函数。例如，由位置向量为 $r_A$、$r_B$ 和 $r_C$ 的原子 $A$、$B$ 和 $C$ 形成的键角 $\\alpha$ 由下式给出：\n$$\n\\alpha = \\arccos\\left(\\frac{(r_A - r_B) \\cdot (r_C - r_B)}{||r_A - r_B|| \\cdot ||r_C - r_B||}\\right)\n$$\nMSE损失函数直接作用于笛卡尔坐标（$r_A, r_B, r_C$），并且对定义这些内坐标的高度非线性和敏感的关系是无知的。解码器作为一个通用函数逼近器（神经网络），没有关于共价几何的内在知识。它仅仅被训练来最小化MSE。虽然训练数据由有效结构组成，但模型学习到的从潜空间到笛卡尔空间的映射只受这个几何上很朴素的损失函数约束。笛卡尔坐标中微小、难以察觉的误差会累积，并在像键角这样的刚性自由度上导致显著的、物理上不现实的偏差。例如，三个原子位置上 $0.1$ Å 的误差可能会使一个键角从其理想值（例如，$109.5^\\circ$）急剧变为一个化学上不可能的值，而对总MSE损失的贡献却很小。\n\n因此，问题的核心是优化目标（笛卡尔空间中的MSE）和期望属性（立体化学有效性）之间的不匹配。模型没有因为生成错误的几何结构而受到明确的惩罚，所以它没有学会避免这种情况，尤其是在通过解码非训练样本直接编码的潜空间点来生成新样本时。\n\n**逐项分析**\n\n**A. Kullback–Leibler (KL) 散度项的权重太小，导致潜空间过拟合，从而违反了化学原理。**\nKL散度对潜空间进行正则化。KL项的权重较小（例如，在标准VAE中，权重默认为$1$）会导致较弱的正则化。较弱的正则化允许近似后验 $q_{\\phi}(z|x)$ 更多地偏离先验 $p(z)$，这可能导致在训练集上过拟合。如果模型过拟合，它将能出色地重构训练数据。由于训练数据是化学上有效的，这就无法解释为什么会生成无效结构。事实上，一个非常大的KL权重（例如，在一个$\\beta \\gg 1$的$\\beta$-VAE中）更可能为了得到一个解耦的潜空间而牺牲重构质量，这反而可能损害化学正确性。因此，这个推理是有缺陷的。**不正确**。\n\n**B. 重构似然被建模为关于笛卡尔坐标的独立高斯分布，使用均方误差，并且解码器输出无约束的坐标；损失函数或解码器参数化中没有明确的立体化学感知约束。**\n这与从第一性原理进行的推导完全一致。在笛卡尔坐标上选择基于MSE的重构损失是问题的直接原因。这个损失函数对分子图及其所蕴含的共价几何是无知的。解码器是“无约束”的，因为它是一个标准的多层感知机，输出一个数值向量，没有任何结构上的特性来强制执行正确的键长或键角。观察到的失败是这种建模选择的直接后果。模型只做它被告知的事情：最小化坐标误差，而不是保持化学规则。这是最根本、最直接的解释。**正确**。\n\n**C. 用于梯度的随机重参数化技巧引入了偏差，这种偏差在解码过程中破坏了角度分布。**\n重参数化技巧（$z = \\mu + \\sigma \\odot \\epsilon$）是一种能够让梯度通过随机采样节点进行反向传播的技术。它被设计为期望梯度的无偏估计量。虽然梯度估计是有噪声的（具有高方差），但该方法本身没有固有的*偏差*会导致系统性地破坏角度分布。引入的噪声影响解码输出的所有方面，而不仅仅是角度。与目标函数本身更直接的缺陷相比，这是一个微小且极不可能的解释，无法说明为何会出现如此系统性和严重的化学原理违规。**不正确**。\n\n**D. 对潜变量选择标准正态先验，而不是学习先验或混合先验，迫使模型进入无效的几何区域。**\n先验 $p(z)$ 决定了潜空间的期望结构。标准正态先验鼓励编码器将数据点映射到一个连续、居中的分布。这通常是生成模型的一个理想属性，因为它允许平滑插值。从这个潜空间到输出（笛卡尔坐标）空间的映射是解码器的责任。如果解码器产生无效的结构，那是解码器学习到的映射以及指导它的目标函数的失败，而不是先验形状的失败。一个不同的先验（例如，高斯混合模型）可能能更好地捕捉多模态数据分布，但它不能解决解码器没有被教授化学规则这个根本问题。问题出在*解码器和损失函数*系统中，而不是*先验*中。**不正确**。", "answer": "$$\\boxed{B}$$", "id": "2439813"}, {"introduction": "在生物学中，机器学习的真正威力通常通过将领域知识直接整合到模型中来释放。这项高级练习展示了如何修改 VAE 的目标函数以融入先验的生物学信息。[@problem_id:2439822] 您将推导并实现一个改进的重建损失，该损失利用蛋白质-蛋白质相互作用（PPI）网络，对已知相互作用的基因对的重建误差施加更重的惩罚。这项实践超越了标准 VAE 的范畴，教您如何创建更具生物学依据的模型，这是从通用模型迈向定制化研究工具的关键一步。", "problem": "给定一个场景，其中一个变分自编码器（Variational Autoencoder, VAE）在基因表达谱上进行训练，每个样本是一个代表 $G$ 个基因表达水平的实值向量。对于单个样本，我们将真实表达向量表示为 $\\mathbf{x} \\in \\mathbb{R}^G$，解码器重构结果表示为 $\\hat{\\mathbf{x}} \\in \\mathbb{R}^G$。对于单位方差的高斯似然，其对证据下界（Evidence Lower Bound, ELBO）的规范重构贡献与平方误差 $\\sum_{i=1}^{G} (x_i - \\hat{x}_i)^2$ 成正比。此外，还给定一个蛋白质-蛋白质相互作用（Protein-Protein Interaction, PPI）网络，该网络由一个对称、非负且对角线元素为零的邻接矩阵 $\\mathbf{A} \\in \\mathbb{R}_{\\ge 0}^{G \\times G}$ 表示，该矩阵编码了相互作用的基因对。\n\n你的任务是修改重构项，以便对相互作用的基因对的重构误差施加更重的惩罚。从以下单个样本的成对增强重构目标函数开始：\n$$\n\\mathcal{L}_{\\text{mod}}(\\mathbf{x}, \\hat{\\mathbf{x}}; \\mathbf{A}, \\alpha) \\;=\\; \\sum_{i=1}^{G} \\big(x_i - \\hat{x}_i\\big)^2 \\;+\\; \\alpha \\sum_{1 \\le i  j \\le G} A_{ij} \\Big(\\big(x_i - \\hat{x}_i\\big)^2 + \\big(x_j - \\hat{x}_j\\big)^2\\Big),\n$$\n其中 $\\alpha \\in \\mathbb{R}_{\\ge 0}$ 是一个可调标量，用于控制成对惩罚的强度。仅根据此定义和图上求和的标准性质，推导出一个 $\\mathcal{L}_{\\text{mod}}$ 的等价表达式，该表达式仅使用每个基因的平方误差和 PPI 图的节点度，即可在 $\\mathcal{O}(G + \\lvert E \\rvert)$ 时间内计算（其中 $\\lvert E \\rvert$ 是 $\\mathbf{A}$ 中非零非对角线元素的数量，每条无向边只计数一次）。使用代数操作和图论恒等式来证明你推导的每一步。\n\n然后，实现一个程序，为几个指定的测试用例计算 $\\mathcal{L}_{\\text{mod}}$。在每个测试用例中，都给定了 $\\mathbf{x}$、$\\hat{\\mathbf{x}}$、$\\mathbf{A}$ 和 $\\alpha$。所有数组和矩阵都很小，并在下文明确提供。不涉及随机性。无需报告物理单位。\n\n实现约束：\n- 使用实值算术。\n- 假设 $\\mathbf{A}$ 意为对称且对角线为零的矩阵；如果存在轻微的不对称，你可以通过 $\\tfrac{1}{2}\\big(\\mathbf{A} + \\mathbf{A}^\\top\\big)$ 对其进行对称化，然后将对角线元素置零。\n- 你的程序必须为每个测试用例计算一个标量 $\\mathcal{L}_{\\text{mod}}$。\n\n测试套件（每个用例相互独立）：\n- 用例 1（正常路径，混合连通性）：\n  - $G = 4$,\n  - $\\mathbf{x} = [\\,10.0,\\, 7.5,\\, 3.0,\\, 0.0\\,]$,\n  - $\\hat{\\mathbf{x}} = [\\,9.5,\\, 7.0,\\, 2.5,\\, 0.5\\,]$,\n  - $\\mathbf{A}$ 具有边 $(0,1)$, $(1,2)$, $(2,3)$，权重均为 $1.0$，其余为零；明确表示为：\n    $$\n    \\mathbf{A} \\;=\\; \\begin{bmatrix}\n    0  1  0  0 \\\\\n    1  0  1  0 \\\\\n    0  1  0  1 \\\\\n    0  0  1  0\n    \\end{bmatrix},\n    $$\n  - $\\alpha = 0.3$。\n- 用例 2（边界情况，$\\alpha = 0$）：\n  - $G = 3$,\n  - $\\mathbf{x} = [\\,1.0,\\, 2.0,\\, 3.0\\,]$,\n  - $\\hat{\\mathbf{x}} = [\\,1.0,\\, 1.0,\\, 3.0\\,]$,\n  - $\\mathbf{A}$ 完全连接，无自环；明确表示为：\n    $$\n    \\mathbf{A} \\;=\\; \\begin{bmatrix}\n    0  1  1 \\\\\n    1  0  1 \\\\\n    1  1  0\n    \\end{bmatrix},\n    $$\n  - $\\alpha = 0.0$。\n- 用例 3（无边，$\\mathbf{A}=\\mathbf{0}$）：\n  - $G = 3$,\n  - $\\mathbf{x} = [\\,4.0,\\, 0.0,\\, 5.0\\,]$,\n  - $\\hat{\\mathbf{x}} = [\\,2.0,\\, 1.0,\\, 7.0\\,]$,\n  - $\\mathbf{A} = \\mathbf{0}_{3 \\times 3}$,\n  - $\\alpha = 0.8$。\n- 用例 4（完全连接，更强放大）：\n  - $G = 3$,\n  - $\\mathbf{x} = [\\,5.0,\\, 5.0,\\, 5.0\\,]$,\n  - $\\hat{\\mathbf{x}} = [\\,4.0,\\, 7.0,\\, 2.0\\,]$,\n  - $\\mathbf{A}$ 完全连接，无自环；明确表示为：\n    $$\n    \\mathbf{A} \\;=\\; \\begin{bmatrix}\n    0  1  1 \\\\\n    1  0  1 \\\\\n    1  1  0\n    \\end{bmatrix},\n    $$\n  - $\\alpha = 0.5$。\n- 用例 5（精确重构）：\n  - $G = 2$,\n  - $\\mathbf{x} = [\\,1.2,\\, 3.4\\,]$,\n  - $\\hat{\\mathbf{x}} = [\\,1.2,\\, 3.4\\,]$,\n  - $\\mathbf{A}$ 是一条单边；明确表示为：\n    $$\n    \\mathbf{A} \\;=\\; \\begin{bmatrix}\n    0  1 \\\\\n    1  0\n    \\end{bmatrix},\n    $$\n  - $\\alpha = 2.0$。\n- 用例 6（带权重的 PPI 边）：\n  - $G = 3$,\n  - $\\mathbf{x} = [\\,2.0,\\, 0.0,\\, 1.0\\,]$,\n  - $\\hat{\\mathbf{x}} = [\\,1.0,\\, 1.0,\\, 0.0\\,]$,\n  - $\\mathbf{A}$ 的边权重为 $A_{01} = A_{10} = 0.5$, $A_{12} = A_{21} = 0.2$，其余为零；明确表示为：\n    $$\n    \\mathbf{A} \\;=\\; \\begin{bmatrix}\n    0  0.5  0 \\\\\n    0.5  0  0.2 \\\\\n    0  0.2  0\n    \\end{bmatrix},\n    $$\n  - $\\alpha = 1.25$。\n\n最终输出格式：\n- 你的程序应生成单行输出，其中包含一个方括号括起来的逗号分隔列表，列表中的每个标量 $\\mathcal{L}_{\\text{mod}}$ 按顺序对应于用例 1 到 6，每个值需四舍五入到恰好 6 位小数（例如，$[1.234000,2.000000]$）。", "solution": "所述问题在科学上是合理的、在数学上是适定的，并且是完整的。它提出了计算生物学中的一个标准任务：修改机器学习目标函数以融入先验生物学知识——在本例中是蛋白质-蛋白质相互作用网络——然后推导一种高效的计算方法。问题的前提基于变分自编码器和图论的既定原理。获得唯一解所需的所有数据均已提供。因此，我们可以继续进行推导和实现。\n\n任务是为修改后的重构损失 $\\mathcal{L}_{\\text{mod}}$ 推导出一个等价但计算效率更高的表达式。提供的定义是：\n$$\n\\mathcal{L}_{\\text{mod}}(\\mathbf{x}, \\hat{\\mathbf{x}}; \\mathbf{A}, \\alpha) \\;=\\; \\sum_{i=1}^{G} \\big(x_i - \\hat{x}_i\\big)^2 \\;+\\; \\alpha \\sum_{1 \\le i  j \\le G} A_{ij} \\Big(\\big(x_i - \\hat{x}_i\\big)^2 + \\big(x_j - \\hat{x}_j\\big)^2\\Big)\n$$\n此处，$G$是基因数量，$\\mathbf{x}$是真实表达向量，$\\hat{\\mathbf{x}}$是其重构结果，$\\mathbf{A}$是PPI网络的对称邻接矩阵且$A_{ii}=0$，而$\\alpha$是一个非负标量权重。\n\n为简化分析，我们定义每个基因的平方误差为 $e_i = (x_i - \\hat{x}_i)^2$。$\\mathcal{L}_{\\text{mod}}$的表达式变为：\n$$\n\\mathcal{L}_{\\text{mod}} = \\sum_{i=1}^{G} e_i \\;+\\; \\alpha \\sum_{1 \\le i  j \\le G} A_{ij} (e_i + e_j)\n$$\n第一项 $\\sum_{i=1}^{G} e_i$ 是标准的平方误差和，可在 $\\mathcal{O}(G)$ 时间内计算。第二项，我们将其表示为 $S_{_P}$，代表成对惩罚。$S_{_P}$的朴素计算涉及对所有配对 $(i, j)$ 的双重循环，导致 $\\mathcal{O}(G^2)$ 的复杂度。我们的目标是为此项找到一个可以更高效计算的表达式。\n$$\nS_{_P} = \\sum_{1 \\le i  j \\le G} A_{ij} (e_i + e_j)\n$$\n我们可以将这个和分解为两部分：\n$$\nS_{_P} = \\sum_{1 \\le i  j \\le G} A_{ij} e_i + \\sum_{1 \\le i  j \\le G} A_{ij} e_j\n$$\n这种形式没有直接的帮助。一种更有效的方法是重新排列求和，关注每个独立节点误差 $e_k$ 对总和 $S_{_P}$ 的贡献。求和 $\\sum_{1 \\le i  j \\le G}$ 遍历了由 $\\mathbf{A}$ 代表的图中所有唯一的无向边。\n\n对于一个特定的节点 $k$，当满足以下条件时，项 $e_k$ 会被包含在内：\n1.  $i = k$ 时，对于所有的 $j  k$。对总和的贡献是 $\\sum_{j  k} A_{kj} e_k$。\n2.  $j = k$ 时，对于所有的 $i  k$。对总和的贡献是 $\\sum_{i  k} A_{ik} e_k$。\n\n因此，在 $S_{_P}$ 表达式中乘以 $e_k$ 的总系数是这两部分之和：\n$$\n\\text{Coefficient of } e_k = \\sum_{j  k} A_{kj} + \\sum_{i  k} A_{ik}\n$$\n问题陈述邻接矩阵 $\\mathbf{A}$ 是对称的（$A_{ij} = A_{ji}$）且对角线元素为零（$A_{ii} = 0$）。利用对称性，我们可以重写第二个求和：$\\sum_{i  k} A_{ik} = \\sum_{i  k} A_{ki}$。\n$e_k$ 的系数则为：\n$$\n\\sum_{j  k} A_{kj} + \\sum_{i  k} A_{ki} = \\sum_{j \\neq k} A_{kj}\n$$\n这个和 $\\sum_{j \\neq k} A_{kj}$ 恰好是节点 $k$ 的加权度，我们表示为 $d_k$。由于 $A_{kk} = 0$，加权度就是邻接矩阵第 $k$ 行（或列）的和：$d_k = \\sum_{j=1}^{G} A_{kj}$。\n\n通过对所有节点 $k = 1, \\dots, G$ 的贡献求和，我们重构了整个成对和 $S_{_P}$：\n$$\nS_{_P} = \\sum_{k=1}^{G} e_k d_k\n$$\n此推导是正确的，因为原始和中的每一项 $A_{ij}(e_i+e_j)$ 都被完全计算在内：$A_{ij}e_i$ 被计入 $e_i d_i$ 项，而 $A_{ij}e_j$（即 $A_{ji}e_j$）被计入 $e_j d_j$ 项。\n\n将这个 $S_{_P}$ 的简化表达式代回到 $\\mathcal{L}_{\\text{mod}}$ 的公式中，我们得到：\n$$\n\\mathcal{L}_{\\text{mod}} = \\sum_{i=1}^{G} e_i + \\alpha \\sum_{i=1}^{G} e_i d_i\n$$\n我们可以提出因子 $e_i$，得到最终的、计算上高效的表达式：\n$$\n\\mathcal{L}_{\\text{mod}} = \\sum_{i=1}^{G} e_i (1 + \\alpha d_i)\n$$\n让我们分析这个最终形式的计算复杂度。\n1.  对所有 $i=1, \\dots, G$，计算每个基因的平方误差 $e_i = (x_i - \\hat{x}_i)^2$ 需要 $\\mathcal{O}(G)$ 时间。\n2.  计算所有 $i=1, \\dots, G$ 的加权度 $d_i = \\sum_{j=1}^{G} A_{ij}$。如果 $\\mathbf{A}$ 是一个稠密矩阵，这需要 $\\mathcal{O}(G^2)$ 时间。然而，对于一个有 $|E|$ 条边的稀疏图（其中 $|E|$ 是上三角矩阵中非零元素的数量），我们可以通过遍历邻接表表示在 $\\mathcal{O}(G+|E|)$ 时间内计算出所有度。\n3.  最终的求和 $\\sum_{i=1}^{G} e_i (1 + \\alpha d_i)$ 需要对 $G$ 个基因进行单次遍历，耗时 $\\mathcal{O}(G)$。\n\n因此，使用此推导公式的总时间复杂度为 $\\mathcal{O}(G+|E|)$（如果从稀疏表示计算度）或 $\\mathcal{O}(G^2)$（如果从稠密矩阵计算）。由于问题要求一个*可以*在 $\\mathcal{O}(G+|E|)$ 时间内计算的公式，我们推导出的表达式满足了这一要求。这种形式避免了对基因对的显式 $\\mathcal{O}(G^2)$ 迭代，这对于大的 $G$ 值是原始定义的主要瓶颈。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the modified reconstruction loss L_mod for a VAE\n    using an efficient, derived formula for several test cases.\n    \"\"\"\n    \n    test_cases = [\n        # Case 1\n        {\n            \"x\": np.array([10.0, 7.5, 3.0, 0.0]),\n            \"x_hat\": np.array([9.5, 7.0, 2.5, 0.5]),\n            \"A\": np.array([\n                [0.0, 1.0, 0.0, 0.0],\n                [1.0, 0.0, 1.0, 0.0],\n                [0.0, 1.0, 0.0, 1.0],\n                [0.0, 0.0, 1.0, 0.0]\n            ]),\n            \"alpha\": 0.3\n        },\n        # Case 2\n        {\n            \"x\": np.array([1.0, 2.0, 3.0]),\n            \"x_hat\": np.array([1.0, 1.0, 3.0]),\n            \"A\": np.array([\n                [0.0, 1.0, 1.0],\n                [1.0, 0.0, 1.0],\n                [1.0, 1.0, 0.0]\n            ]),\n            \"alpha\": 0.0\n        },\n        # Case 3\n        {\n            \"x\": np.array([4.0, 0.0, 5.0]),\n            \"x_hat\": np.array([2.0, 1.0, 7.0]),\n            \"A\": np.zeros((3, 3)),\n            \"alpha\": 0.8\n        },\n        # Case 4\n        {\n            \"x\": np.array([5.0, 5.0, 5.0]),\n            \"x_hat\": np.array([4.0, 7.0, 2.0]),\n            \"A\": np.array([\n                [0.0, 1.0, 1.0],\n                [1.0, 0.0, 1.0],\n                [1.0, 1.0, 0.0]\n            ]),\n            \"alpha\": 0.5\n        },\n        # Case 5\n        {\n            \"x\": np.array([1.2, 3.4]),\n            \"x_hat\": np.array([1.2, 3.4]),\n            \"A\": np.array([\n                [0.0, 1.0],\n                [1.0, 0.0]\n            ]),\n            \"alpha\": 2.0\n        },\n        # Case 6\n        {\n            \"x\": np.array([2.0, 0.0, 1.0]),\n            \"x_hat\": np.array([1.0, 1.0, 0.0]),\n            \"A\": np.array([\n                [0.0, 0.5, 0.0],\n                [0.5, 0.0, 0.2],\n                [0.0, 0.2, 0.0]\n            ]),\n            \"alpha\": 1.25\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        x = case[\"x\"]\n        x_hat = case[\"x_hat\"]\n        A = case[\"A\"]\n        alpha = case[\"alpha\"]\n\n        # Per problem specification, ensure A is symmetric with a zero diagonal.\n        # This is a robust preprocessing step.\n        A_sym = 0.5 * (A + A.T)\n        np.fill_diagonal(A_sym, 0)\n\n        # Calculate per-gene squared errors: e_i = (x_i - x_hat_i)^2\n        e = (x - x_hat)**2\n\n        # Calculate weighted node degrees: d_i = sum_j A_ij\n        d = A_sym.sum(axis=1)\n\n        # Compute L_mod using the derived efficient formula:\n        # L_mod = sum_i e_i * (1 + alpha * d_i)\n        l_mod = np.sum(e * (1.0 + alpha * d))\n        \n        results.append(l_mod)\n\n    # Format the output as a comma-separated list with 6 decimal places.\n    output_str = \"[\" + \",\".join([f\"{res:.6f}\" for res in results]) + \"]\"\n    print(output_str)\n\nsolve()\n```", "id": "2439822"}]}