{"hands_on_practices": [{"introduction": "无监督学习的核心在于无需预先标记就能从数据中发现固有模式。本练习将引导您应用最经典的无监督算法之一——k-均值聚类，对一组代表病毒基因组的数据点进行分组。您将学习如何通过计算“纯度”这一外部评估指标来衡量聚类结果的质量，从而判断算法发现的结构是否与已知的生物学分类相符。[@problem_id:2432796]", "problem": "您会得到一个未标注病毒基因组的小型、基于组成的表示，形式为二维实数空间中的点。每个点对应一个由现实的组成特征（如鸟嘌呤-胞嘧啶比例和二核苷酸偏向指数）总结的基因组。一组相应的参考病毒科标签仅用于评估，不得用于形成簇。您必须确定通过最小化簇内离散度获得的无监督簇是否能在某个量化标准下对应于已知的病毒科。\n\n设数据集为 $n=6$ 个点 $\\{\\mathbf{x}_i\\}_{i=1}^{6}\\subset\\mathbb{R}^2$，其中\n$\\mathbf{x}_1=(0.35,0.10)$，$\\mathbf{x}_2=(0.36,0.12)$，$\\mathbf{x}_3=(0.60,0.20)$，$\\mathbf{x}_4=(0.62,0.22)$，$\\mathbf{x}_5=(0.40,0.35)$，$\\mathbf{x}_6=(0.42,0.36)$。设对应的病毒科标签为 $\\{y_i\\}_{i=1}^{6}$，其中 $y_1=0$，$y_2=0$，$y_3=1$，$y_4=1$，$y_5=2$，$y_6=2$。设 $k\\in\\mathbb{N}$ 表示所要求的簇数。\n\n定义一个划分 $\\mathcal{P}=\\{C_1,\\dots,C_k\\}$，它是索引集 $\\{1,\\dots,6\\}$ 的一个划分，由 $k$ 个非空的、两两不相交的子集组成，其并集为 $\\{1,\\dots,6\\}$。对于给定的划分 $\\mathcal{P}$ 及其导出的簇均值 $\\boldsymbol{\\mu}_j=\\frac{1}{|C_j|}\\sum_{i\\in C_j}\\mathbf{x}_i$（对于 $j\\in\\{1,\\dots,k\\}$），误差平方和 (SSE) 为\n$$\n\\mathrm{SSE}(\\mathcal{P})=\\sum_{j=1}^{k}\\sum_{i\\in C_j}\\left\\|\\mathbf{x}_i-\\boldsymbol{\\mu}_j\\right\\|_2^2.\n$$\n在所有有效的 $k$ 簇划分中，考虑任何使 $\\mathrm{SSE}(\\mathcal{P})$ 最小化的划分 $\\mathcal{P}^\\star$。定义划分 $\\mathcal{P}$ 相对于标签 $\\{y_i\\}$ 的纯度为\n$$\n\\mathrm{purity}(\\mathcal{P},\\{y_i\\})=\\frac{1}{n}\\sum_{j=1}^{k}\\max_{\\ell\\in\\{0,1,2\\}}\\left|\\{i\\in C_j: y_i=\\ell\\}\\right|.\n$$\n\n对于下方的每个测试用例，您必须：\n- 为指定的 $k$ 确定一个使 $\\mathrm{SSE}(\\mathcal{P})$ 最小化的划分 $\\mathcal{P}^\\star$。\n- 计算 $\\mathrm{purity}(\\mathcal{P}^\\star,\\{y_i\\})$。\n- 将纯度与阈值 $\\tau$ 进行比较，如果 $\\mathrm{purity}(\\mathcal{P}^\\star,\\{y_i\\})\\ge \\tau$ 则输出布尔值 $b$ 为 $\\mathrm{True}$，否则为 $\\mathrm{False}$。该布尔值回答了这个问题：“在阈值 $\\tau$ 下，所得到的无监督簇是否能对应于已知的病毒科？”\n\n测试套件（所有用例均使用上述共享的数据集和标签）：\n- 用例 1：$k=3$，$\\tau=0.95$。\n- 用例 2：$k=2$，$\\tau=0.8$。\n- 用例 3：$k=1$，$\\tau=0.4$。\n- 用例 4：$k=6$，$\\tau=1.0$。\n\n最终输出格式：您的程序应生成单行输出，其中包含按顺序排列的四个布尔结果，形式为一个用方括号括起来的逗号分隔列表（例如，$[\\mathrm{True},\\mathrm{False},\\mathrm{True},\\mathrm{False}]$）。", "solution": "对问题陈述进行验证。\n\n**步骤 1：提取给定信息**\n- $\\mathbb{R}^2$中的 $n=6$ 个点的数据集：$\\{\\mathbf{x}_i\\}_{i=1}^{6}$。\n  $\\mathbf{x}_1=(0.35, 0.10)$，$\\mathbf{x}_2=(0.36, 0.12)$，$\\mathbf{x}_3=(0.60, 0.20)$，$\\mathbf{x}_4=(0.62, 0.22)$，$\\mathbf{x}_5=(0.40, 0.35)$ 和 $\\mathbf{x}_6=(0.42, 0.36)$。\n- 用于评估的对应标签：$\\{y_i\\}_{i=1}^{6}$。\n  $y_1=0$，$y_2=0$，$y_3=1$，$y_4=1$，$y_5=2$ 和 $y_6=2$。\n- 簇数：$k\\in\\mathbb{N}$。\n- 划分的定义：$\\mathcal{P}=\\{C_1,\\dots,C_k\\}$，索引集 $\\{1,\\dots,6\\}$ 的一个划分为 $k$ 个非空、不相交的子集。\n- 簇均值的定义：$\\boldsymbol{\\mu}_j=\\frac{1}{|C_j|}\\sum_{i\\in C_j}\\mathbf{x}_i$。\n- 要最小化的目标函数：误差平方和，$\\mathrm{SSE}(\\mathcal{P})=\\sum_{j=1}^{k}\\sum_{i\\in C_j}\\left\\|\\mathbf{x}_i-\\boldsymbol{\\mu}_j\\right\\|_2^2$。\n- 最优划分的定义：$\\mathcal{P}^\\star$ 是任何使 $\\mathrm{SSE}(\\mathcal{P})$ 最小化的划分。\n- 评估指标：纯度，$\\mathrm{purity}(\\mathcal{P},\\{y_i\\})=\\frac{1}{n}\\sum_{j=1}^{k}\\max_{\\ell\\in\\{0,1,2\\}}\\left|\\{i\\in C_j: y_i=\\ell\\}\\right|$，其中 $n=6$。\n- 任务：对于每对 $(k, \\tau)$，找到 $\\mathcal{P}^\\star$，计算其纯度，并确定该纯度是否大于或等于 $\\tau$。\n- 测试套件：\n  - 用例 1：$k=3, \\tau=0.95$。\n  - 用例 2：$k=2, \\tau=0.8$。\n  - 用例 3：$k=1, \\tau=0.4$。\n  - 用例 4：$k=6, \\tau=1.0$。\n\n**步骤 2：使用提取的给定信息进行验证**\n该问题具有科学依据，在现实的生物信息学背景下使用了标准的无监督学习概念（SSE 最小化，等同于 k-means 目标）和评估指标（纯度）。该问题是适定的；对于一个小的有限数据集（$n=6$），可能划分的数量是有限的，并且小到足以进行穷举搜索，这保证了可以找到 SSE 的全局最小值。该问题是客观的，所有术语和数据都有精确定义。没有矛盾、缺失信息或伪科学论断。\n\n**结论：**问题有效。\n\n**解题思路**\n任务是对一组 $n=6$ 个数据点 $\\{\\mathbf{x}_i\\}_{i=1}^6$ 进行聚类，给定簇数 $k$。最优聚类被定义为使误差平方和（SSE）最小化的数据点索引 $\\{1, \\dots, 6\\}$ 的划分 $\\mathcal{P}^\\star$。由于点的数量 $n=6$ 很小，通过枚举将 6 个索引组成的集合划分为 $k$ 个非空子集的所有可能划分，在计算上是可行的，可以找到 SSE 的全局最小值。这种划分的数量由第二类斯特林数 $S(n,k)$ 给出。\n\n对于每个测试用例，我们首先通过计算每个可能划分的 SSE 并选择 SSE 最小的那个来确定最优划分 $\\mathcal{P}^\\star$。一旦确定了 $\\mathcal{P}^\\star$，我们就计算其相对于给定标签 $\\{y_i\\}_{i=1}^6$ 的纯度。最后，我们将此纯度值与指定的阈值 $\\tau$ 进行比较，以产生一个布尔结果。点的坐标为 $\\mathbf{x}_1=(0.35, 0.10)$，$\\mathbf{x}_2=(0.36, 0.12)$，$\\mathbf{x}_3=(0.60, 0.20)$，$\\mathbf{x}_4=(0.62, 0.22)$，$\\mathbf{x}_5=(0.40, 0.35)$ 和 $\\mathbf{x}_6=(0.42, 0.36)$。标签为 $y_1=0$，$y_2=0$，$y_3=1$，$y_4=1$，$y_5=2$ 和 $y_6=2$。\n\n**用例 1：$k=3$，$\\tau=0.95$**\n将 6 个项划分为 3 个簇的划分数量为 $S(6,3)=90$。执行穷举搜索。数据点在视觉上形成三个不同的对：$(\\mathbf{x}_1, \\mathbf{x}_2)$、$(\\mathbf{x}_3, \\mathbf{x}_4)$ 和 $(\\mathbf{x}_5, \\mathbf{x}_6)$。与这些点对相对应的划分是 $\\mathcal{P}_1 = \\{\\{1,2\\}, \\{3,4\\}, \\{5,6\\}\\}$。此划分的 SSE 计算如下：\n- $C_1=\\{1,2\\}$: $\\boldsymbol{\\mu}_1=(0.355, 0.11)$。 $\\sum_{i \\in C_1} ||\\mathbf{x}_i - \\boldsymbol{\\mu}_1||^2 = 0.00025$。\n- $C_2=\\{3,4\\}$: $\\boldsymbol{\\mu}_2=(0.61, 0.21)$。 $\\sum_{i \\in C_2} ||\\mathbf{x}_i - \\boldsymbol{\\mu}_2||^2 = 0.0004$。\n- $C_3=\\{5,6\\}$: $\\boldsymbol{\\mu}_3=(0.41, 0.355)$。 $\\sum_{i \\in C_3} ||\\mathbf{x}_i - \\boldsymbol{\\mu}_3||^2 = 0.00025$。\n总 $\\mathrm{SSE}(\\mathcal{P}_1) = 0.00025 + 0.0004 + 0.00025 = 0.0009$。穷举评估证实这是最小的 SSE，因此 $\\mathcal{P}^\\star = \\mathcal{P}_1$。\n\n接下来，我们计算 $\\mathcal{P}^\\star$ 的纯度：\n- 对于簇 $C_1=\\{1,2\\}$，其标签为 $\\{y_1,y_2\\}=\\{0,0\\}$。主要标签是 $0$，计数为 $2$。\n- 对于簇 $C_2=\\{3,4\\}$，其标签为 $\\{y_3,y_4\\}=\\{1,1\\}$。主要标签是 $1$，计数为 $2$。\n- 对于簇 $C_3=\\{5,6\\}$，其标签为 $\\{y_5,y_6\\}=\\{2,2\\}$。主要标签是 $2$，计数为 $2$。\n纯度为 $\\frac{1}{6}(2+2+2) = \\frac{6}{6} = 1.0$。\n与阈值 $\\tau=0.95$ 比较：$1.0 \\ge 0.95$，为 $\\mathrm{True}$。\n\n**用例 2：$k=2$，$\\tau=0.8$**\n划分数量为 $S(6,2)=31$。经过穷举搜索，发现使 SSE 最小化的划分是 $\\mathcal{P}^\\star = \\{\\{1,2\\}, \\{3,4,5,6\\}\\}$。这对应于将两个最远的点对与第三个点对分组。此划分的最小 SSE 约为 $0.061925$。\n\n我们计算此划分的纯度：\n- 对于簇 $C_1=\\{1,2\\}$，其标签为 $\\{y_1,y_2\\}=\\{0,0\\}$。最大计数为 $2$。\n- 对于簇 $C_2=\\{3,4,5,6\\}$，其标签为 $\\{y_3,y_4,y_5,y_6\\}=\\{1,1,2,2\\}$。标签的计数为 $\\{1:2, 2:2\\}$。最大计数为 $2$。\n纯度为 $\\frac{1}{6}(2+2) = \\frac{4}{6} \\approx 0.6667$。\n与阈值 $\\tau=0.8$ 比较：$0.6667 < 0.8$，为 $\\mathrm{False}$。\n\n**用例 3：$k=1$，$\\tau=0.4$**\n只有 $S(6,1)=1$ 种可能的划分：$\\mathcal{P}^\\star = \\{\\{1,2,3,4,5,6\\}\\}$。\n这个单簇包含所有点。标签为 $\\{0,0,1,1,2,2\\}$。标签的计数为 $\\{0:2, 1:2, 2:2\\}$。最大计数为 $2$。\n纯度为 $\\frac{1}{6}(2) = \\frac{2}{6} \\approx 0.3333$。\n与阈值 $\\tau=0.4$ 比较：$0.3333 < 0.4$，为 $\\mathrm{False}$。\n\n**用例 4：$k=6$，$\\tau=1.0$**\n只有 $S(6,6)=1$ 种可能的划分：$\\mathcal{P}^\\star = \\{\\{1\\},\\{2\\},\\{3\\},\\{4\\},\\{5\\},\\{6\\}\\}$。每个点自成一簇。SSE 显然为 $0$。\n\n我们计算纯度：\n每个簇 $C_j=\\{j\\}$ 包含一个单点。每个簇的最大标签计数必然为 $1$。\n纯度为 $\\frac{1}{6}(1+1+1+1+1+1) = \\frac{6}{6} = 1.0$。\n与阈值 $\\tau=1.0$ 比较：$1.0 \\ge 1.0$，为 $\\mathrm{True}$。\n\n最终结果总结：\n- 用例 1：$\\mathrm{True}$\n- 用例 2：$\\mathrm{False}$\n- 用例 3：$\\mathrm{False}$\n- 用例 4：$\\mathrm{True}$", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the clustering validation problem for the given dataset and test cases.\n    \"\"\"\n    # Define the dataset and labels as specified in the problem.\n    # Using 0-based indexing for arrays.\n    points = np.array([\n        [0.35, 0.10],  # index 0, point x_1\n        [0.36, 0.12],  # index 1, point x_2\n        [0.60, 0.20],  # index 2, point x_3\n        [0.62, 0.22],  # index 3, point x_4\n        [0.40, 0.35],  # index 4, point x_5\n        [0.42, 0.36]   # index 5, point x_6\n    ])\n    labels = np.array([0, 0, 1, 1, 2, 2])\n    n = len(points)\n\n    # Test suite\n    test_cases = [\n        {'k': 3, 'tau': 0.95},\n        {'k': 2, 'tau': 0.8},\n        {'k': 1, 'tau': 0.4},\n        {'k': 6, 'tau': 1.0},\n    ]\n\n    # Memoization for the partition generator\n    partition_memo = {}\n\n    def generate_partitions(elements, k):\n        \"\"\"\n        Generates all partitions of a set of elements into k non-empty subsets.\n        Uses a recursive algorithm based on Stirling numbers of the second kind.\n        \"\"\"\n        # Use a tuple of elements as a key for memoization\n        elements_tuple = tuple(sorted(elements))\n        if (elements_tuple, k) in partition_memo:\n            return partition_memo[(elements_tuple, k)]\n\n        n_elements = len(elements)\n        if k  1 or k > n_elements:\n            return []\n        if k == 1:\n            return [[elements]]\n        if k == n_elements:\n            return [[[e] for e in elements]]\n\n        first = elements[0]\n        rest = elements[1:]\n        \n        partitions = []\n        # Case 1: 'first' is in a new cluster by itself.\n        # We need partitions of 'rest' into k-1 clusters.\n        partitions_k_minus_1 = generate_partitions(rest, k - 1)\n        for p in partitions_k_minus_1:\n            partitions.append([[first]] + p)\n            \n        # Case 2: 'first' is added to one of the k existing clusters.\n        # We need partitions of 'rest' into k clusters.\n        partitions_k = generate_partitions(rest, k)\n        for p in partitions_k:\n            for i in range(len(p)):\n                new_p = [list(s) for s in p]\n                new_p[i].append(first)\n                partitions.append(new_p)\n        \n        partition_memo[(elements_tuple, k)] = partitions\n        return partitions\n\n    results = []\n    \n    initial_indices = list(range(n))\n\n    for case in test_cases:\n        k = case['k']\n        tau = case['tau']\n        \n        min_sse = float('inf')\n        best_partition = None\n        \n        # Generate all partitions of point indices into k clusters\n        all_partitions = generate_partitions(initial_indices, k)\n\n        for partition in all_partitions:\n            current_sse = 0.0\n            for cluster_indices in partition:\n                # Get points belonging to the current cluster\n                cluster_points = points[cluster_indices]\n                # Calculate centroid\n                centroid = np.mean(cluster_points, axis=0)\n                # Calculate SSE for this cluster\n                sse_cluster = np.sum((cluster_points - centroid)**2)\n                current_sse += sse_cluster\n            \n            if current_sse  min_sse:\n                min_sse = current_sse\n                best_partition = partition\n        \n        # Calculate purity for the best partition\n        sum_max_counts = 0\n        for cluster_indices in best_partition:\n            if not cluster_indices:\n                continue\n            # Get labels for the points in the cluster\n            cluster_labels = labels[cluster_indices]\n            # Find the count of the most frequent label\n            if len(cluster_labels) > 0:\n                _, counts = np.unique(cluster_labels, return_counts=True)\n                max_count = np.max(counts)\n            else:\n                max_count = 0\n            sum_max_counts += max_count\n            \n        purity = sum_max_counts / n\n        \n        # Compare purity with threshold and record the boolean result\n        results.append(purity >= tau)\n\n    # Format and print the final output\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```", "id": "2432796"}, {"introduction": "当拥有标记数据时，有监督学习通常能提供强大的预测能力。但与无监督方法相比，它的优势究竟有多大？本练习将通过一个核心的生物信息学问题——区分DNA编码序列与非编码序列——来直接对比有监督方法（支持向量机, SVM）与无监督方法（k-均值聚类）。您将使用 $k$-mer 频率作为特征，亲身体验标记数据在解决特定预测任务时所带来的巨大价值。[@problem_id:2432827]", "problem": "给定一个有限字母表 $\\mathcal{A} = \\{\\mathrm{A}, \\mathrm{C}, \\mathrm{G}, \\mathrm{T}\\}$ 以及一个特征映射的定义，该映射将长度为 $L$ 的脱氧核糖核酸（DNA）序列 $s$ 转换为一个归一化的 $k$-mer 频率向量 $\\phi_k(s) \\in \\mathbb{R}^{4^k}$。对于一个给定的正整数 $k$，令 $\\mathcal{M}_k$ 为字母表 $\\mathcal{A}$ 上所有长度为 $k$ 的字符串的集合，按字典序枚举。对于每个 $m \\in \\mathcal{M}_k$，其分量 $\\left[\\phi_k(s)\\right]_m$ 定义为使用步长为 $1$ 的滑动窗口在 $s$ 中对 $m$ 的出现次数进行计数，然后除以 $s$ 中 $k$-mer 的总数，即 $L - k + 1$（假设 $L \\ge k$ 且不出现模糊字符）。形式上，\n$$\n\\left[\\phi_k(s)\\right]_m = \\frac{1}{L - k + 1} \\sum_{i=1}^{L-k+1} \\mathbf{1}\\left\\{ s_i s_{i+1} \\cdots s_{i+k-1} = m \\right\\},\n$$\n其中 $\\mathbf{1}\\{\\cdot\\}$ 是指示函数，索引是基于 $1$ 的。\n\n一个二元标签 $y \\in \\{-1, +1\\}$ 指示一个 DNA 序列是蛋白质编码（$+1$）还是非编码（$-1$）。考虑一个监督线性分类器，它由一个权重向量 $w \\in \\mathbb{R}^{4^k}$和偏置 $b \\in \\mathbb{R}$定义，作用于特征 $\\phi_k(s)$，通过得分 $f(s) = w^\\top \\phi_k(s) + b$ 和预测规则 $\\hat{y}(s) = \\mathrm{sign}(f(s))$ 进行预测，其中如果 $z \\ge 0$，则 $\\mathrm{sign}(z) = +1$，否则为 $-1$。模型参数 $(w,b)$ 通过最小化带有平方合页损失（squared hinge loss）的正则化经验风险来获得：\n$$\nJ(w,b) = \\frac{1}{2} \\lVert w \\rVert_2^2 + C \\sum_{i=1}^{N} \\left( \\max\\left(0, 1 - y_i (w^\\top x_i + b) \\right) \\right)^2,\n$$\n其中 $C  0$ 是一个给定的正则化参数，$\\{(x_i, y_i)\\}_{i=1}^N$ 是训练样本，其中 $x_i = \\phi_k(s_i)$ 且 $y_i \\in \\{-1, +1\\}$，$\\lVert \\cdot \\rVert_2$ 表示欧几里得范数。\n\n对于一个无监督基线，考虑将一组给定的特征向量 $\\{x_j\\}_{j=1}^M$（其中 $x_j = \\phi_k(s_j)$，对应 $M$ 个序列）划分为恰好两个非空簇 $\\mathcal{C}_0$ 和 $\\mathcal{C}_1$，以最小化簇内平方和：\n$$\nW(\\mathcal{C}_0, \\mathcal{C}_1) = \\sum_{j \\in \\mathcal{C}_0} \\lVert x_j - \\mu_0 \\rVert_2^2 + \\sum_{j \\in \\mathcal{C}_1} \\lVert x_j - \\mu_1 \\rVert_2^2,\n$$\n其中 $\\mu_0$ 和 $\\mu_1$ 分别是它们各自簇的均值。在获得最小化划分后，为每个项目定义一个基于簇的预测 $\\tilde{y}_j \\in \\{-1, +1\\}$，方法是将两个簇映射到 $\\{-1, +1\\}$，使得与同一批项目的真实标签匹配的比例最大化；将该最大比例作为无监督准确率（以小数形式表示）报告。\n\n您将获得一个带标签的 DNA 序列训练集 $\\mathcal{D}_{\\mathrm{train}}$ 和一个带标签的测试集 $\\mathcal{D}_{\\mathrm{test}}$。所有序列仅由字母表 $\\mathcal{A}$ 中的符号组成。\n\n训练序列（标签 $y$ 在括号中显示，其中 $+1$ 表示编码，$-1$ 表示非编码）：\n- `s_1 = `ATGGCGGCCGCGGGCGCCGCGGGCGACGGCTGA`` $(+1)$\n- `s_2 = `ATGCGCGCGCGGGCCGCGGCTGCGGCGTAG`` $(+1)$\n- `s_3 = `ATGGGCGACGGCGGCGACGGCGGCGACTAA`` $(+1)$\n- `s_4 = `ATGGCCGCTGCGGCTGGCGCTGCGGCTTGA`` $(+1)$\n- `s_5 = `ATGGCGGCGGCGGCGGCGGCGGCGGCGGCGTAA`` $(+1)$\n- `s_6 = `ATGGGCGCCGCGGGCGCCGCGGGCGCCTGA`` $(+1)$\n- `s_7 = `TATATAAATAATATATATTTATATAATAATA`` $(-1)$\n- `s_8 = `AAATATATATTTAAATATATATATATAAAA`` $(-1)$\n- `s_9 = `TTTATATATAAATATAATATATTTATAAAT`` $(-1)$\n- `s_{10} = `AATAATAATATATTTATAAATAATATATAT`` $(-1)$\n- `s_{11} = `ATATATAAATATATAATATATAAATATATA`` $(-1)$\n- `s_{12} = `TATATATAAATAAATATATATATAAATATA`` $(-1)$\n\n测试序列（带标签）：\n- `t_1 = `ATGGCGGGCGGGCGACGGCTAA`` $(+1)$\n- `t_2 = `ATGGCCGCGGCTGGCGCTGCGTAG`` $(+1)$\n- `t_3 = `ATGGCGGCGGCGGCGGCGTGA`` $(+1)$\n- `t_4 = `AATATATATATAAATATATATAAATAATA`` $(-1)$\n- `t_5 = `TATATTTATAAATATATATAAATATTTAT`` $(-1)$\n- `t_6 = `AAATAATATATATATAAATAATATATATA`` $(-1)$\n\n您的任务如下。\n\n1. 监督分类。对于每个指定的配对 $(k, C)$，为所有 $s \\in \\mathcal{D}_{\\mathrm{train}}$ 计算特征向量 $\\phi_k(s)$，通过最小化 $J(w,b)$ 来学习 $(w,b)$，然后使用 $\\hat{y}(s) = \\mathrm{sign}(w^\\top \\phi_k(s) + b)$ 计算在 $\\mathcal{D}_{\\mathrm{test}}$ 上的测试准确率（以小数形式表示）。\n2. 无监督聚类基线。对于每个指定的 $k$，为所有 $s \\in \\mathcal{D}_{\\mathrm{test}}$ 计算 $\\phi_k(s)$，并找到一个划分为恰好两个非空簇的分区，该分区能最小化 $W(\\mathcal{C}_0,\\mathcal{C}_1)$。将簇映射到标签 $\\{-1,+1\\}$ 以最大化与 $\\mathcal{D}_{\\mathrm{test}}$ 上真实标签的一致性，并将该最大一致性作为准确率（以小数形式表示）报告。\n\n测试套件。按以下确切顺序运行五个案例：\n- 案例1：监督学习，其中 $k = 3, C = 1$。\n- 案例2：监督学习，其中 $k = 2, C = 1$。\n- 案例3：监督学习，其中 $k = 3, C = 0.01$。\n- 案例4：无监督学习，其中 $k = 3$ 仅在测试集上。\n- 案例5：无监督学习，其中 $k = 2$ 仅在测试集上。\n\n答案规格和输出格式。\n- 对于每个案例，答案是一个实数，等于在 $\\mathcal{D}_{\\mathrm{test}}$ 上的准确率，以小数形式表示，并四舍五入到小数点后恰好三位。\n- 您的程序应生成单行输出，其中包含五个结果，以逗号分隔的列表形式包含在方括号内，顺序与上述案例一致。例如，一个包含五个占位符值的输出应看起来像 $[\\alpha_1,\\alpha_2,\\alpha_3,\\alpha_4,\\alpha_5]$，其中每个 $\\alpha_i$ 是一个四舍五入到小数点后三位的实数。", "solution": "所提出的问题要求实现和评估两种不同的机器学习范式——监督分类和无监督聚类——用于区分蛋白质编码和非编码的脱氧核糖核酸（DNA）序列的任务。该问题具有科学依据，定义明确，且形式上是规范的。它为完整的计算解决方案提供了所有必要的数据和定义。因此，这是一个有效的问题。解决方案的步骤是首先构建特征表示，然后为两种范式实现指定的算法。\n\n两种方法的基本步骤都是将符号化的 DNA 序列转换为适合算法处理的定量、数值格式。这是通过一个特征映射 $\\phi_k(s)$ 实现的，它将一个长度为 $L$ 的序列 $s$ 投影到 $\\mathbb{R}^{4^k}$ 中的一个实值向量。该向量的每个维度对应于字母表 $\\mathcal{A} = \\{\\mathrm{A}, \\mathrm{C}, \\mathrm{G}, \\mathrm{T}\\}$ 上 $4^k$ 个可能的长度为 $k$ 的字符串（称为 $k$-mer）之一。每个分量的值是相应 $k$-mer 在序列中的归一化频率。具体而言，对于一个 $k$-mer $m$，其频率是在大小为 $k$、步长为 $1$ 的滑动窗口内的计数，除以窗口总数 $L-k+1$。这建立了一个向量空间模型，其中每个序列都是一个点，学习算法可以利用这些点之间的几何关系。所有 $k$-mer 的集合 $\\mathcal{M}_k$ 按字典序排列，这定义了一个从 $k$-mer 到特征向量中索引的一致映射。这可以通过将字符 A、C、G、T 视为四进制数系统中的数字（例如，$A=0, C=1, G=2, T=3$）来实现。\n\n首先，我们处理监督分类任务。给定一个包含 $N$ 个序列及其已知标签 $y_i \\in \\{-1, +1\\}$ 的训练数据集 $\\mathcal{D}_{\\mathrm{train}}$。目标是学习一个线性决策函数 $f(s) = w^\\top \\phi_k(s) + b$，该函数可以预测新序列的标签。参数，即一个权重向量 $w \\in \\mathbb{R}^{4^k}$ 和一个标量偏置 $b \\in \\mathbb{R}$，是通过最小化一个正则化经验风险函数来确定的。指定的目标函数是：\n$$\nJ(w,b) = \\frac{1}{2} \\lVert w \\rVert_2^2 + C \\sum_{i=1}^{N} \\left( \\max\\left(0, 1 - y_i (w^\\top x_i + b) \\right) \\right)^2\n$$\n在这里，$x_i = \\phi_k(s_i)$ 是第 $i$ 个训练序列的特征向量。第一项 $\\frac{1}{2} \\lVert w \\rVert_2^2$ 是一个 $\\ell_2$-正则化项，用于惩罚过大的权重以防止过拟合。第二项是训练集上的平方合页损失之和，它惩罚错误分类以及边界不足的正确分类。参数 $C  0$ 控制正则化与拟合训练数据之间的权衡。这个目标函数 $J(w,b)$ 是凸函数且处处可微，确保存在唯一的全局最小值。我们可以使用基于梯度的优化算法（如 L-BFGS-B，即带边界的有限内存 Broyden–Fletcher–Goldfarb–Shanno 算法）来找到这个最小值。为此，我们计算 $J(w,b)$ 关于 $w$ 和 $b$ 的偏导数。令 $\\mathcal{S}$ 为满足 $y_i(w^\\top x_i + b)  1$ 的索引集合。梯度为：\n$$\n\\nabla_w J = w - 2C \\sum_{i \\in \\mathcal{S}} y_i (1 - y_i(w^\\top x_i + b)) x_i\n$$\n$$\n\\nabla_b J = -2C \\sum_{i \\in \\mathcal{S}} y_i (1 - y_i(w^\\top x_i + b))\n$$\n从一个初始猜测（例如 $w=0, b=0$）开始，优化器迭代更新参数以找到最优解 $(w^*, b^*)$。模型训练完成后，通过计算其在测试集 $\\mathcal{D}_{\\mathrm{test}}$ 上的准确率来评估其性能：即预测标签 $\\hat{y}(s) = \\mathrm{sign}(w^{*\\top} \\phi_k(s) + b^*)$ 与真实标签相匹配的测试序列所占的比例。\n\n其次，我们处理无监督聚类基线。这种方法在学习阶段不使用标签。任务是将从测试集 $\\mathcal{D}_{\\mathrm{test}}$ 导出的特征向量 $\\{x_j\\}_{j=1}^M$ 划分为恰好两个非空簇 $\\mathcal{C}_0$ 和 $\\mathcal{C}_1$。该划分必须最小化簇内平方和（WCSS），这是 k-means 聚类的一个标准目标：\n$$\nW(\\mathcal{C}_0, \\mathcal{C}_1) = \\sum_{j \\in \\mathcal{C}_0} \\lVert x_j - \\mu_0 \\rVert_2^2 + \\sum_{j \\in \\mathcal{C}_1} \\lVert x_j - \\mu_1 \\rVert_2^2\n$$\n其中 $\\mu_0$ 和 $\\mu_1$ 是簇的质心（均值）。尽管对于一般的 k-means 问题找到最优划分是 NP-难的，但测试集中的数据点数量很少（$M=6$）。这允许通过枚举所有可能的非平凡划分来获得精确解。将一个包含 $M$ 个项目的集合划分为两个非空子集的方法数由第二类斯特林数 $S(M, 2) = 2^{M-1} - 1$ 给出。对于 $M=6$，存在 $2^5 - 1 = 31$ 种唯一的划分。我们可以遍历这些划分中的每一种，计算相应的 WCSS，并找出产生全局最小值的划分。在找到最优簇 $\\mathcal{C}_0^*$ 和 $\\mathcal{C}_1^*$ 后，我们通过将它们与真实标签进行比较来评估其质量。由于簇的标识（$0$ 和 $1$）是任意的，我们必须测试两种可能的到真实标签 $\\{-1, +1\\}$ 的映射：（$\\mathcal{C}_0^* \\to -1, \\mathcal{C}_1^* \\to +1$）和（$\\mathcal{C}_0^* \\to +1, \\mathcal{C}_1^* \\to -1$）。无监督准确率定义为这两种映射中实现的最大准确率。\n\n最终的程序包括针对指定的参数（$k, C$）执行这两个算法框架，并报告所得的测试准确率，四舍五入到小数点后三位。", "answer": "完整且可运行的 Python 3 代码如下。导入的库必须符合指定的执行环境。\n```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef solve():\n    \"\"\"\n    Solves the complete problem, including both supervised and unsupervised tasks.\n    \"\"\"\n    # Define the problem data\n    train_seqs = [\n        \"ATGGCGGCCGCGGGCGCCGCGGGCGACGGCTGA\", \"ATGCGCGCGCGGGCCGCGGCTGCGGCGTAG\",\n        \"ATGGGCGACGGCGGCGACGGCGGCGACTAA\", \"ATGGCCGCTGCGGCTGGCGCTGCGGCTTGA\",\n        \"ATGGCGGCGGCGGCGGCGGCGGCGGCGGCGTAA\", \"ATGGGCGCCGCGGGCGCCGCGGGCGCCTGA\",\n        \"TATATAAATAATATATATTTATATAATAATA\", \"AAATATATATTTAAATATATATATATAAAA\",\n        \"TTTATATATAAATATAATATATTTATAAAT\", \"AATAATAATATATTTATAAATAATATATAT\",\n        \"ATATATAAATATATAATATATAAATATATA\", \"TATATATAAATAAATATATATATAAATATA\"\n    ]\n    train_labels = np.array([1, 1, 1, 1, 1, 1, -1, -1, -1, -1, -1, -1])\n\n    test_seqs = [\n        \"ATGGCGGGCGGGCGACGGCTAA\", \"ATGGCCGCGGCTGGCGCTGCGTAG\",\n        \"ATGGCGGCGGCGGCGGCGTGA\", \"AATATATATATAAATATATATAAATAATA\",\n        \"TATATTTATAAATATATATAAATATTTAT\", \"AAATAATATATATATAAATAATATATATA\"\n    ]\n    test_labels = np.array([1, 1, 1, -1, -1, -1])\n\n    alphabet_map = {'A': 0, 'C': 1, 'G': 2, 'T': 3}\n\n    def get_kmer_index(kmer):\n        \"\"\"Calculates the lexicographical index of a k-mer.\"\"\"\n        index = 0\n        for char in kmer:\n            index = index * 4 + alphabet_map[char]\n        return index\n\n    def phi_k(s, k):\n        \"\"\"Computes the k-mer frequency vector phi_k(s).\"\"\"\n        L = len(s)\n        dim = 4**k\n        if L  k:\n            return np.zeros(dim)\n        \n        counts = np.zeros(dim)\n        num_kmers = L - k + 1\n        \n        for i in range(num_kmers):\n            kmer = s[i:i+k]\n            idx = get_kmer_index(kmer)\n            counts[idx] += 1\n            \n        return counts / num_kmers\n\n    def objective_function(theta, X, y, C):\n        \"\"\"Computes J(w, b) and its gradient for L2-SVM.\"\"\"\n        N, D = X.shape\n        w = theta[:-1]\n        b = theta[-1]\n        \n        margins = y * (X.dot(w) + b)\n        loss_terms = 1 - margins\n        violations_mask = loss_terms > 0\n        \n        squared_hinge_loss = np.sum(loss_terms[violations_mask]**2)\n        objective_value = 0.5 * np.dot(w, w) + C * squared_hinge_loss\n        \n        grad = np.zeros_like(theta)\n        grad[:-1] = w\n        \n        if np.any(violations_mask):\n            common_grad_factor = -2 * C * loss_terms[violations_mask] * y[violations_mask]\n            grad[:-1] += np.dot(common_grad_factor, X[violations_mask, :])\n            grad[-1] += np.sum(common_grad_factor)\n        \n        return objective_value, grad\n\n    def solve_supervised(k, C):\n        \"\"\"Trains the SVM and computes test accuracy.\"\"\"\n        D = 4**k\n        X_train = np.array([phi_k(s, k) for s in train_seqs])\n        \n        initial_theta = np.zeros(D + 1)\n        res = minimize(\n            objective_function,\n            initial_theta,\n            args=(X_train, train_labels, C),\n            jac=True,\n            method='L-BFGS-B'\n        )\n        \n        w_opt, b_opt = res.x[:-1], res.x[-1]\n        \n        X_test = np.array([phi_k(s, k) for s in test_seqs])\n        \n        scores = X_test.dot(w_opt) + b_opt\n        predictions = np.sign(scores)\n        predictions[predictions == 0] = 1 # Per problem: sign(z>=0) = +1\n        \n        accuracy = np.mean(predictions == test_labels)\n        return accuracy\n\n    def solve_unsupervised(k):\n        \"\"\"Performs clustering and computes best-match accuracy.\"\"\"\n        M = len(test_seqs)\n        X_test = np.array([phi_k(s, k) for s in test_seqs])\n        \n        min_wcss = np.inf\n        best_partition = None\n        \n        # Enumerate all non-trivial partitions of M items into 2 clusters.\n        # Fix the first item in cluster 0 and iterate through assignments for the rest.\n        # This gives 2^(M-1) possibilities. The case where all items are in cluster 0\n        # (i=0) is excluded to ensure non-empty clusters.\n        num_items_to_partition = M - 1\n        for i in range(1, 2**num_items_to_partition):\n            c0_indices = [0]\n            c1_indices = []\n            \n            for j in range(num_items_to_partition):\n                if (i >> j)  1:\n                    c1_indices.append(j + 1)\n                else:\n                    c0_indices.append(j + 1)\n\n            C0 = X_test[c0_indices, :]\n            C1 = X_test[c1_indices, :]\n            \n            wcss = np.sum((C0 - np.mean(C0, axis=0))**2) + \\\n                   np.sum((C1 - np.mean(C1, axis=0))**2)\n\n            if wcss  min_wcss:\n                min_wcss = wcss\n                best_partition = (c0_indices, c1_indices)\n\n        c0_indices, c1_indices = best_partition\n        \n        # Mapping 1: C0 -> -1, C1 -> +1\n        pred1 = np.ones(M)\n        pred1[c0_indices] = -1\n        acc1 = np.mean(pred1 == test_labels)\n        \n        # Mapping 2: C0 -> +1, C1 -> -1\n        pred2 = np.ones(M)\n        pred2[c1_indices] = -1\n        acc2 = np.mean(pred2 == test_labels)\n        \n        return max(acc1, acc2)\n\n    # Execute all five test cases\n    results = [\n        solve_supervised(k=3, C=1.0),\n        solve_supervised(k=2, C=1.0),\n        solve_supervised(k=3, C=0.01),\n        solve_unsupervised(k=3),\n        solve_unsupervised(k=2)\n    ]\n    \n    # Format and print the final output\n    formatted_results = [f\"{r:.3f}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "2432827"}, {"introduction": "有监督和无监督学习并非相互排斥，它们的结合往往能产生更强大的分析能力。在这个高级实践中，您将构建一个在现代生物信息学中非常常见的两阶段学习流程。首先，您将使用一种无监督方法（主成分分析，PCA）从高维的基因表达数据中学习到一个紧凑且有意义的低维表示；然后，您将这个新学到的表示作为特征，输入到一个有监督模型中来预测临床结果。[@problem_id:2432878]", "problem": "您需要编写一个完整的、可运行的程序，该程序对模拟的基因表达数据执行一个两阶段学习流程，该流程适用于多个测试用例，并结合了无监督表示学习阶段和有监督预测阶段。目标和数据生成过程在下面用数学方式指定，您的程序必须严格实现这些定义。\n\n数据生成过程。对于每个测试用例，按如下方式生成一个训练集和一个独立的测试集。设训练患者数量为 $n_{\\mathrm{train}}$，测试患者数量为 $n_{\\mathrm{test}}$，基因数量为 $p$，真实潜维为 $r^\\star$。设学习者使用的无监督潜维为 $k$，其中 $k$ 是一个整数，范围为 $0 \\le k \\le \\min\\{p,n_{\\mathrm{train}}-1\\}$。为每个测试用例固定一个基础随机种子 $s$，以确保可复现性。\n\n- 使用基础种子 $s$，抽取一个基因载荷矩阵 $W \\in \\mathbb{R}^{p \\times r^\\star}$，其条目为独立的标准正态分布。\n- 使用相同的基础种子 $s$ 并继续生成的数值序列，抽取一个生存系数向量 $b \\in \\mathbb{R}^{r^\\star}$，其条目为独立的标准正态分布。\n- 使用相同的基础种子 $s$ 并继续生成的数值序列，抽取训练潜因子 $Z_{\\mathrm{train}} \\in \\mathbb{R}^{n_{\\mathrm{train}} \\times r^\\star}$，其条目为独立的标准正态分布。\n- 使用相同的基础种子 $s$ 并继续生成的数值序列，抽取训练基因表达噪声 $E^{(X)}_{\\mathrm{train}} \\in \\mathbb{R}^{n_{\\mathrm{train}} \\times p}$，其条目独立地服从分布 $\\mathcal{N}(0,\\sigma_X^2)$。\n- 使用相同的基础种子 $s$ 并继续生成的数值序列，抽取训练生存噪声 $e^{(y)}_{\\mathrm{train}} \\in \\mathbb{R}^{n_{\\mathrm{train}}}$，其条目独立地服从分布 $\\mathcal{N}(0,\\sigma_y^2)$。\n- 定义训练基因表达为 $X_{\\mathrm{train}} = Z_{\\mathrm{train}} W^\\top + E^{(X)}_{\\mathrm{train}} \\in \\mathbb{R}^{n_{\\mathrm{train}} \\times p}$。\n- 定义训练生存时间为 $y_{\\mathrm{train}} = Z_{\\mathrm{train}} b + e^{(y)}_{\\mathrm{train}} \\in \\mathbb{R}^{n_{\\mathrm{train}}}$。\n\n对于测试集，使用与上述相同的 $W$ 和 $b$，但使用独立的潜因子和噪声：\n- 使用测试种子 $s+1$ 抽取 $Z_{\\mathrm{test}} \\in \\mathbb{R}^{n_{\\mathrm{test}} \\times r^\\star}$（条目为独立的标准正态分布），$E^{(X)}_{\\mathrm{test}} \\in \\mathbb{R}^{n_{\\mathrm{test}} \\times p}$（条目独立地服从分布 $\\mathcal{N}(0,\\sigma_X^2)$），以及 $e^{(y)}_{\\mathrm{test}} \\in \\mathbb{R}^{n_{\\mathrm{test}}}$（条目独立地服从分布 $\\mathcal{N}(0,\\sigma_y^2)$）。\n- 定义测试基因表达和生存为 $X_{\\mathrm{test}} = Z_{\\mathrm{test}} W^\\top + E^{(X)}_{\\mathrm{test}}$ 和 $y_{\\mathrm{test}} = Z_{\\mathrm{test}} b + e^{(y)}_{\\mathrm{test}}$。\n\n两阶段学习流程。您的程序必须在训练数据上执行以下两个优化问题，然后在测试数据上进行评估。\n\n1. 无监督表示学习。通过求解以下问题，学习一个线性编码器 $E: \\mathbb{R}^p \\to \\mathbb{R}^k$ 和一个线性解码器 $D: \\mathbb{R}^k \\to \\mathbb{R}^p$，以及一个中心化向量 $\\mu \\in \\mathbb{R}^p$：\n$$\n\\min_{E,D,\\mu} \\ \\frac{1}{n_{\\mathrm{train}}} \\sum_{i=1}^{n_{\\mathrm{train}}} \\left\\| x_i - D\\!\\left(E\\!\\left(x_i - \\mu\\right)\\right) \\right\\|_2^2,\n$$\n其中 $x_i \\in \\mathbb{R}^p$ 表示 $X_{\\mathrm{train}}$ 的第 $i$ 行。所学的编码器 $E$ 必须是线性的，解码器 $D$ 也必须是线性的。使用所学的编码器和中心化向量为每个训练样本 $i \\in \\{1,\\dots,n_{\\mathrm{train}}\\}$ 生成潜表示 $z_i^{(\\mathrm{lat})} = E(x_i - \\mu) \\in \\mathbb{R}^k$，并对每个测试样本 $j \\in \\{1,\\dots,n_{\\mathrm{test}}\\}$，使用从训练数据中估计的相同 $E$ 和 $\\mu$ 生成 $z_j^{(\\mathrm{lat,test})} = E(x^{(\\mathrm{test})}_j - \\mu)$。\n\n2. 有监督预测。通过求解以下问题，学习一个形式为 $g(z) = c^\\top z + d$ 的仿射预测器 $g: \\mathbb{R}^k \\to \\mathbb{R}$：\n$$\n\\min_{c \\in \\mathbb{R}^k, \\ d \\in \\mathbb{R}} \\ \\frac{1}{n_{\\mathrm{train}}} \\sum_{i=1}^{n_{\\mathrm{train}}} \\left( y_i - \\left(c^\\top z_i^{(\\mathrm{lat})} + d\\right) \\right)^2,\n$$\n其中 $y_i$ 表示 $y_{\\mathrm{train}}$ 的第 $i$ 个条目。使用所学的 $(c,d)$，为每个测试样本 $j$ 计算预测值 $\\hat{y}_j = c^\\top z_j^{(\\mathrm{lat,test})} + d$，并评估测试均方误差：\n$$\n\\mathrm{MSE}_{\\mathrm{test}} = \\frac{1}{n_{\\mathrm{test}}} \\sum_{j=1}^{n_{\\mathrm{test}}} \\left( \\hat{y}_j - y^{(\\mathrm{test})}_j \\right)^2.\n$$\n\n测试套件。您的程序必须实现上述过程，并为以下每个参数设置输出测试均方误差，使用所描述的精确生成和训练程序。每个测试用例是一个元组 $(s, n_{\\mathrm{train}}, n_{\\mathrm{test}}, p, r^\\star, k, \\sigma_X, \\sigma_y)$:\n\n- 测试用例 1: (7, 120, 80, 60, 5, 5, 0.3, 0.5).\n- 测试用例 2: (13, 150, 150, 80, 6, 6, 2.0, 2.0).\n- 测试用例 3: (21, 50, 50, 200, 3, 3, 0.5, 0.5).\n- 测试用例 4: (1, 100, 100, 70, 8, 2, 0.4, 0.4).\n- 测试用例 5: (99, 100, 100, 50, 4, 0, 0.5, 0.5).\n\n答案规格和输出格式。您的程序必须生成单行输出，其中包含按上述顺序列出的测试用例的测试均方误差列表。每个值必须四舍五入到恰好 $6$ 位小数。输出必须是方括号括起来的逗号分隔列表，例如 $[\\mathrm{mse}_1,\\mathrm{mse}_2,\\dots]$，不含多余的空格或文本。", "solution": "该问题陈述是有效的。它具有科学依据，问题设定良好、客观，并为计算实验提供了完整且一致的规范。该任务涉及将标准的两阶段机器学习流程应用于模拟数据，这是生物信息学研究中评估方法的常见做法。数据生成过程是一个线性因子模型，学习算法是主成分分析和普通最小二乘回归，这两者都是基础且易于理解的。所有测试用例的参数都在有效范围内。因此，我们可以着手解决。\n\n该问题要求实现一个两阶段学习流程。第一阶段是无监督的，学习基因表达数据的低维表示。第二阶段是有监督的，使用这个学习到的表示来预测临床结果（生存时间）。\n\n**阶段1：无监督表示学习**\n\n第一个优化问题是：\n$$\n\\min_{E,D,\\mu} \\ \\frac{1}{n_{\\mathrm{train}}} \\sum_{i=1}^{n_{\\mathrm{train}}} \\left\\| x_i - D\\!\\left(E\\!\\left(x_i - \\mu\\right)\\right) \\right\\|_2^2\n$$\n其中 $E: \\mathbb{R}^p \\to \\mathbb{R}^k$ 和 $D: \\mathbb{R}^k \\to \\mathbb{R}^p$ 是线性变换，$\\mu \\in \\mathbb{R}^p$ 是一个中心化向量。这个目标函数旨在最小化线性自编码器的重构误差。该问题的解由主成分分析（PCA）给出。\n\n首先，最优的中心化向量 $\\mu$ 是训练数据向量 $x_i$ 的经验均值：\n$$\n\\mu = \\frac{1}{n_{\\mathrm{train}}} \\sum_{i=1}^{n_{\\mathrm{train}}} x_i = \\frac{1}{n_{\\mathrm{train}}} X_{\\mathrm{train}}^\\top \\mathbf{1}_{n_{\\mathrm{train}}}\n$$\n令 $\\bar{X}_{\\mathrm{train}} = X_{\\mathrm{train}} - \\mathbf{1}_{n_{\\mathrm{train}}} \\mu^\\top$ 为中心化的训练数据矩阵。优化问题简化为寻找 $\\bar{X}_{\\mathrm{train}}$ 的最佳秩-k 近似。根据 Eckart-Young-Mirsky 定理，这可以通过对 $\\bar{X}_{\\mathrm{train}}$ 进行奇异值分解（SVD）来实现。\n\n令中心化数据矩阵的 SVD 为 $\\bar{X}_{\\mathrm{train}} = U S V^\\top$，其中 $V \\in \\mathbb{R}^{p \\times p}$ 的列是主成分（样本协方差矩阵的特征向量）。令 $V_k \\in \\mathbb{R}^{p \\times k}$ 是一个矩阵，其列是前 $k$ 个主成分，对应于 $k$ 个最大的奇异值。\n\n线性编码器 $E$ 将中心化数据投影到由这 $k$ 个分量定义的基上。其矩阵表示为 $V_k^\\top$。训练样本的学习潜表示为：\n$$\nZ^{(\\mathrm{lat})} = \\bar{X}_{\\mathrm{train}} V_k \\in \\mathbb{R}^{n_{\\mathrm{train}} \\times k}\n$$\n对于测试数据 $X_{\\mathrm{test}}$，潜表示通过首先使用训练集的均值 $\\mu$ 对数据进行中心化，然后应用相同的投影来计算：\n$$\nZ^{(\\mathrm{lat,test})} = (X_{\\mathrm{test}} - \\mathbf{1}_{n_{\\mathrm{test}}} \\mu^\\top) V_k\n$$\n\n**阶段2：有监督预测**\n\n第二阶段涉及通过在训练数据上求解以下普通最小二乘法（OLS）问题来学习一个仿射预测器 $g(z) = c^\\top z + d$：\n$$\n\\min_{c \\in \\mathbb{R}^k, \\ d \\in \\mathbb{R}} \\ \\frac{1}{n_{\\mathrm{train}}} \\sum_{i=1}^{n_{\\mathrm{train}}} \\left( y_i - \\left(c^\\top z_i^{(\\mathrm{lat})} + d\\right) \\right)^2\n$$\n为了解决这个问题，我们可以用一列全为1的向量来增广潜表示矩阵 $Z^{(\\mathrm{lat})}$，以考虑截距项 $d$。令 $\\tilde{Z}^{(\\mathrm{lat})} = [Z^{(\\mathrm{lat})} \\ \\mathbf{1}_{n_{\\mathrm{train}}}] \\in \\mathbb{R}^{n_{\\mathrm{train}} \\times (k+1)}$ 和 $\\tilde{c} = [c^\\top, d]^\\top \\in \\mathbb{R}^{k+1}$。目标变为最小化 $\\|\\mathbf{y}_{\\mathrm{train}} - \\tilde{Z}^{(\\mathrm{lat})} \\tilde{c}\\|_2^2$。\n\n通过求解正规方程找到解，得到：\n$$\n\\tilde{c}_{\\mathrm{opt}} = (\\tilde{Z}^{(\\mathrm{lat})\\top} \\tilde{Z}^{(\\mathrm{lat})})^\\dagger \\tilde{Z}^{(\\mathrm{lat})\\top} \\mathbf{y}_{\\mathrm{train}}\n$$\n其中 $\\dagger$ 表示 Moore-Penrose 伪逆，即使矩阵不可逆，它也能提供一个稳定的解。\n\n当 $k=0$ 时会出现一个特殊情况。此时，潜空间是平凡的（零维），所以 $Z^{(\\mathrm{lat})}$ 是一个空矩阵。模型简化为 $g(z)=d$。优化问题变为 $\\min_d \\sum_i (y_i - d)^2$，其解是训练标签的均值：$d = \\bar{y}_{\\mathrm{train}} = \\frac{1}{n_{\\mathrm{train}}} \\sum_i y_i$。\n\n**评估**\n\n最后，使用学习到的系数 $\\tilde{c}_{\\mathrm{opt}} = [c_{\\mathrm{opt}}^\\top, d_{\\mathrm{opt}}]^\\top$ 和测试潜表示 $Z^{(\\mathrm{lat,test})}$ 来生成对测试集的预测。\n对于 $k  0$，我们形成一个增广的测试矩阵 $\\tilde{Z}^{(\\mathrm{lat,test})} = [Z^{(\\mathrm{lat,test})} \\ \\mathbf{1}_{n_{\\mathrm{test}}}]$，预测值为 $\\hat{\\mathbf{y}}_{\\mathrm{test}} = \\tilde{Z}^{(\\mathrm{lat,test})} \\tilde{c}_{\\mathrm{opt}}$。\n对于 $k=0$，每个测试样本的预测值就是常数值 $\\hat{y}_j = d_{\\mathrm{opt}} = \\bar{y}_{\\mathrm{train}}$。\n\n性能通过测试均方误差（MSE）进行评估：\n$$\n\\mathrm{MSE}_{\\mathrm{test}} = \\frac{1}{n_{\\mathrm{test}}} \\sum_{j=1}^{n_{\\mathrm{test}}} \\left( \\hat{y}_j - y^{(\\mathrm{test})}_j \\right)^2\n$$\n以下程序为每个指定的测试用例实现了这整个过程。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve_pipeline(s, n_train, n_test, p, r_star, k, sigma_X, sigma_y):\n    \"\"\"\n    Implements the full data generation and two-stage learning pipeline for a single test case.\n    \n    Args:\n        s (int): Base random seed.\n        n_train (int): Number of training patients.\n        n_test (int): Number of test patients.\n        p (int): Number of genes.\n        r_star (int): True latent dimension.\n        k (int): Unsupervised latent dimension for the learner.\n        sigma_X (float): Standard deviation of gene expression noise.\n        sigma_y (float): Standard deviation of survival noise.\n        \n    Returns:\n        float: The test mean squared error.\n    \"\"\"\n    \n    # --- Data Generation ---\n    # Training data generation\n    rng_train = np.random.default_rng(s)\n    W = rng_train.standard_normal(size=(p, r_star))\n    b = rng_train.standard_normal(size=r_star)\n    Z_train = rng_train.standard_normal(size=(n_train, r_star))\n    E_X_train = rng_train.normal(scale=sigma_X, size=(n_train, p))\n    e_y_train = rng_train.normal(scale=sigma_y, size=n_train)\n    \n    X_train = Z_train @ W.T + E_X_train\n    y_train = Z_train @ b + e_y_train\n    \n    # Test data generation\n    rng_test = np.random.default_rng(s + 1)\n    Z_test = rng_test.standard_normal(size=(n_test, r_star))\n    E_X_test = rng_test.normal(scale=sigma_X, size=(n_test, p))\n    e_y_test = rng_test.normal(scale=sigma_y, size=n_test)\n    \n    X_test = Z_test @ W.T + E_X_test\n    y_test = Z_test @ b + e_y_test\n\n    # --- Two-Stage Learning Pipeline ---\n\n    # Stage 1: Unsupervised Representation Learning (PCA)\n    mu = np.mean(X_train, axis=0)\n    X_train_centered = X_train - mu\n    \n    if k > 0:\n        # SVD on centered training data\n        # full_matrices=False is more efficient\n        _, _, Vt = np.linalg.svd(X_train_centered, full_matrices=False)\n        \n        # Encoder is based on the top k principal components (right singular vectors)\n        # Vt is V.T, so we take the first k rows and transpose\n        Vk = Vt[:k, :].T  # Shape: (p, k)\n        \n        # Get latent representations for training and test sets\n        Z_lat_train = X_train_centered @ Vk\n        X_test_centered = X_test - mu\n        Z_lat_test = X_test_centered @ Vk\n    \n    # Stage 2: Supervised Prediction (OLS)\n    if k == 0:\n        # If k=0, the model is y = d. The best predictor is the mean of y_train.\n        d = np.mean(y_train)\n        y_hat_test = np.full(n_test, d)\n    else:\n        # Augment latent features with a column of ones for the intercept\n        Z_train_aug = np.c_[Z_lat_train, np.ones(n_train)]\n        \n        # Solve for coefficients (c and d) using least squares\n        # coeffs will contain [c_1, ..., c_k, d]\n        coeffs, _, _, _ = np.linalg.lstsq(Z_train_aug, y_train, rcond=None)\n        \n        # Predict on the test set\n        Z_test_aug = np.c_[Z_lat_test, np.ones(n_test)]\n        y_hat_test = Z_test_aug @ coeffs\n\n    # --- Evaluation ---\n    mse_test = np.mean((y_hat_test - y_test)**2)\n    return mse_test\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print results.\n    \"\"\"\n    # Test cases: (s, n_train, n_test, p, r_star, k, sigma_X, sigma_y)\n    test_cases = [\n        (7, 120, 80, 60, 5, 5, 0.3, 0.5),\n        (13, 150, 150, 80, 6, 6, 2.0, 2.0),\n        (21, 50, 50, 200, 3, 3, 0.5, 0.5),\n        (1, 100, 100, 70, 8, 2, 0.4, 0.4),\n        (99, 100, 100, 50, 4, 0, 0.5, 0.5),\n    ]\n\n    results = []\n    for params in test_cases:\n        mse = solve_pipeline(*params)\n        results.append(mse)\n\n    # Format the output as specified: a comma-separated list in brackets,\n    # with each value rounded to 6 decimal places.\n    formatted_results = [f\"{res:.6f}\" for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "2432878"}]}