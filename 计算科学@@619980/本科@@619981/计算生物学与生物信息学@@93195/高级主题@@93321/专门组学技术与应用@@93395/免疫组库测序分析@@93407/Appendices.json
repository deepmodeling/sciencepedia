{"hands_on_practices": [{"introduction": "在免疫组库测序中，一个基本问题是：我们测序的深度足够了吗？本练习将指导你通过构建稀疏曲线（rarefaction curve）来回答这个问题，这是一个评估样本克隆型丰富度是否被充分捕获的关键质量控制步骤。你将应用超几何分布的原理，通过编程实现一个决策规则，以确定是否需要进一步测序，从而在研究成本和数据完整性之间做出量化权衡。[@problem_id:2399358]", "problem": "一个实验室对一个淋巴细胞群体的免疫受体库进行了测序，并获得了一个观测到的克隆型及其读取计数的列表。将测序读取建模为从一个大小为 $N$ 的有限多重集合中进行无放回抽样，$N$ 是观测到的总读取数，每个克隆型 $i$ 出现 $c_i$ 次，其中 $c_i \\ge 1$ 且 $\\sum_i c_i = N$。定义稀疏化函数为无放回地对 $n$ 个读取进行二次抽样时，观测到的不同克隆型数量的期望值，记为 $E[S(n)]$。\n\n您的任务是编写一个程序，对于每个指定的测试用例，根据当前测序深度附近稀疏化曲线的斜率，来决定是否建议进行额外的测序。使用以下有科学依据的基础：\n\n- 从有限总体中进行无放回抽样遵循超几何分布定律。\n- 对于每个计数为 $c_i$ 的克隆型 $i$，在大小为 $n$ 的子样本中未观测到其 $c_i$ 个分子中任何一个的概率是二项式系数之比 $\\dfrac{\\binom{N - c_i}{n}}{\\binom{N}{n}}$，其中 $0 \\le n \\le N$。\n- 根据期望的线性性质，$E[S(n)]$ 等于所有克隆型在子样本中至少被观测到一次的概率之和。\n\n决策规则：给定一个正整数窗口大小 $m$（$1 \\le m \\le N$）和一个非负阈值 $\\alpha$（单位为每个额外读取的预期新克隆型数），计算在最后 $m$ 个读取上预期的克隆丰富度的平均边际增益，\n$$\ng = \\frac{E[S(N)] - E[S(N - m)]}{m}。\n$$\n如果 $g  \\alpha$，则返回布尔值 True（建议进行更深度的测序）；否则返回 False。\n\n仅使用上述假设和标准数学函数计算 $E[S(n)]$。为确保大 $N$ 值的数值稳定性，请通过阶乘的对数（例如，使用伽马函数的对数）来计算二项式系数的比率。\n\n每个测试用例的输入是：\n- 一个正整数列表 $[c_1,c_2,\\dots,c_R]$，满足 $\\sum_i c_i = N$，\n- 一个整数窗口大小 $m$，满足 $1 \\le m \\le N$，\n- 一个非负实数阈值 $\\alpha$。\n\n您的程序必须处理以下测试套件：\n\n- 测试用例 1：计数 $[25,20,15,10,8,7,5,5,3,2]$，$m = 10$，$\\alpha = 0.05$。\n- 测试用例 2：计数 $[50,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1]$（一个有 $50$ 个读取的克隆型和 $50$ 个单例克隆型），$m = 10$，$\\alpha = 0.15$。\n- 测试用例 3：计数由 $20$ 个克隆型组成，每个克隆型有 $5$ 个读取，即 $[5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5]$，$m = 10$，$\\alpha = 0.05$。\n- 测试用例 4：计数 $[100]$，$m = 10$，$\\alpha = 0.001$。\n- 测试用例 5：计数由 $100$ 个单例克隆型组成，即 $[1,1,1,\\dots,1]$（有 $100$ 个条目），$m = 10$，$\\alpha = 0.5$。\n\n您的程序应生成一行输出，其中包含测试用例的布尔决策，形式为方括号括起来的逗号分隔列表（例如，`[True,False,...]`）。不应打印任何额外文本。", "solution": "问题陈述已经过严格验证，并被认为是有效的。它在科学上基于概率论的既定原则及其在计算生物学中的标准应用，特别是在免疫受体库测序数据的分析中。该问题被妥善地提出，所有术语、条件和数据都以清晰、一致和客观的方式提供。其中没有逻辑矛盾、科学不准确之处或含糊不清的地方。因此，我们可以进行形式化的求解。\n\n核心任务是基于克隆型稀疏化曲线的局部斜率，评估是否需要进一步测序的决策规则。如果平均边际增益 $g$ 超过阈值 $\\alpha$，则决策为 `True`，否则为 `False`。边际增益定义为：\n$$\ng = \\frac{E[S(N)] - E[S(N - m)]}{m}\n$$\n其中 $N$ 是总读取数，$m$ 是指定的窗口大小，$E[S(n)]$ 是在无放回随机抽取的 $n$ 个读取子样本中观测到的不同克隆型数量的期望值。输入是一个克隆型计数列表 $[c_1, c_2, \\dots, c_R]$、窗口大小 $m$ 和阈值 $\\alpha$。\n\n我们的推导分为两部分：首先，评估 $E[S(N)]$，其次，评估 $E[S(N-m)]$。\n\n**1. 评估 $E[S(N)]$**\n\n项 $E[S(N)]$ 表示从群体中抽样所有 $N$ 个读取时观测到的不同克隆型数量的期望值。根据定义，如果抽样了所有读取，必然会观测到原始库中存在的每一个克隆型。因此，观测到的克隆型数量确定性地等于 $R$，即不同克隆型的总数。\n$$\nE[S(N)] = R\n$$\n其中 $R$ 只是输入计数列表的长度。这简化了 $g$ 的表达式：\n$$\ng = \\frac{R - E[S(N - m)]}{m}\n$$\n\n**2. 对于一般子样本大小 $n$ 评估 $E[S(n)]$**\n\n不同克隆型数量的期望值 $E[S(n)]$ 是根据期望的线性性质推导出来的。设 $I_i(n)$ 为一个指示随机变量，如果克隆型 $i$ 在大小为 $n$ 的子样本中被观测到，则 $I_i(n) = 1$，否则 $I_i(n) = 0$。观测到的克隆型总数为 $S(n) = \\sum_{i=1}^{R} I_i(n)$。其期望为：\n$$\nE[S(n)] = E\\left[\\sum_{i=1}^{R} I_i(n)\\right] = \\sum_{i=1}^{R} E[I_i(n)]\n$$\n指示变量的期望是它所指示事件的概率。\n$$\nE[I_i(n)] = P(\\text{clonotype } i \\text{ is observed}) = 1 - P(\\text{clonotype } i \\text{ is not observed})\n$$\n该问题正确地指出抽样是无放回的，这遵循超几何分布。在一个大小为 $n$ 的子样本中未观测到克隆型 $i$（其有 $c_i$ 个读取）的概率，等于从不属于克隆型 $i$ 的 $N-c_i$ 个读取中选出全部 $n$ 个读取的概率。从 $N$ 个读取中选取 $n$ 个的总方式数为 $\\binom{N}{n}$。从不属于克隆型 $i$ 的读取中选取 $n$ 个的方式数为 $\\binom{N-c_i}{n}$。因此，\n$$\nP(\\text{clonotype } i \\text{ is not observed}) = \\frac{\\binom{N - c_i}{n}}{\\binom{N}{n}}\n$$\n这在 $0 \\le n \\le N-c_i$ 时有效。如果 $n  N-c_i$，则不可能避免选中至少一个来自克隆型 $i$ 的读取，因此未观测到它的概率为 $0$。\n\n结合这些结果，我们得到稀疏化曲线的公式：\n$$\nE[S(n)] = \\sum_{i=1}^{R} \\left(1 - \\frac{\\binom{N-c_i}{n}}{\\binom{N}{n}}\\right) = R - \\sum_{i=1}^{R} \\frac{\\binom{N-c_i}{n}}{\\binom{N}{n}}\n$$\n\n**3. 数值计算与最终算法**\n\n为了计算 $g$，我们必须计算 $E[S(N-m)]$。设 $n' = N-m$。\n$$\nE[S(n')] = R - \\sum_{i=1}^{R} \\frac{\\binom{N-c_i}{n'}}{\\binom{N}{n'}}\n$$\n直接计算具有大参数的二项式系数会导致数值溢出。按规定，我们必须使用伽马函数的对数 $\\log \\Gamma(z+1) = \\log(z!)$ 来计算该比率。未命中克隆型 $i$ 的概率的对数为：\n$$\n\\log P_i(\\text{miss}) = \\log\\left(\\frac{\\binom{N-c_i}{n'}}{\\binom{N}{n'}}\\right) = \\log\\left(\\frac{(N-c_i)!(N-n')!}{N!(N-c_i-n')!}\\right)\n$$\n使用对数伽马函数，这变为：\n$$\n\\log P_i(\\text{miss}) = \\log\\Gamma(N-c_i+1) + \\log\\Gamma(N-n'+1) - \\log\\Gamma(N+1) - \\log\\Gamma(N-c_i-n'+1)\n$$\n那么概率就是 $P_i(\\text{miss}) = \\exp(\\log P_i(\\text{miss}))$。`scipy.special.gammaln` 函数通过返回无穷大来正确处理参数为非正整数的情况，这在取指数后，当 $n'  N-c_i$ 时，能正确地得到概率为 $0$。\n\n完整的算法如下：\n1.  给定计数列表 $[c_1, \\dots, c_R]$，$m$ 和 $\\alpha$。\n2.  计算总读取数 $N = \\sum_{i=1}^{R} c_i$ 和不同克隆型的数量 $R$。\n3.  将用于计算的子样本大小设置为 $n' = N-m$。\n4.  初始化未命中概率之和，$\\Sigma_P = 0$。\n5.  对于列表中的每个计数 $c_i$：\n    a. 如果 $n' \\le N-c_i$，使用上面的对数伽马公式计算 $\\log P_i(\\text{miss})$。\n    b. 将 $\\exp(\\log P_i(\\text{miss}))$ 加到 $\\Sigma_P$ 中。\n6.  计算在深度 $n'$ 时的预期克隆型数量：$E[S(n')] = R - \\Sigma_P$。\n7.  计算边际增益：$g = (R - E[S(n')]) / m$。\n8.  返回比较 $g  \\alpha$ 的布尔结果。\n此过程是稳健的，并直接实现了经过验证的科学模型。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import gammaln\n\ndef solve():\n    \"\"\"\n    Solves the immune repertoire sequencing analysis problem for a suite of test cases.\n    \"\"\"\n\n    def compute_decision(counts, m, alpha):\n        \"\"\"\n        Computes the decision to continue sequencing based on the slope of the rarefaction curve.\n\n        Args:\n            counts (list): A list of positive integers representing clonotype read counts.\n            m (int): A positive integer window size.\n            alpha (float): A non-negative real threshold for the marginal gain.\n\n        Returns:\n            bool: True if further sequencing is advisable, False otherwise.\n        \"\"\"\n        # Total number of reads (N) and distinct clonotypes (R)\n        N = sum(counts)\n        R = len(counts)\n\n        # The expected number of clonotypes at full depth E[S(N)] is exactly R.\n        E_S_N = float(R)\n\n        # We need to compute E[S(N-m)], the expected number of clonotypes when\n        # subsampling n_prime = N - m reads.\n        n_prime = N - m\n\n        # E[S(n')] = sum_{i=1 to R} (1 - P(clonotype i is missed))\n        # This is equivalent to R - sum_{i=1 to R} P(clonotype i is missed).\n        # We calculate the sum of miss probabilities.\n        sum_prob_miss = 0.0\n\n        for c_i in counts:\n            # The probability of missing a clonotype is non-zero only if\n            # it's possible to draw n_prime reads without picking any of its c_i members.\n            # This requires the number of other reads (N - c_i) to be at least n_prime.\n            if N - c_i >= n_prime:\n                # To avoid numerical overflow with large factorials, we compute the\n                # log of the ratio of binomial coefficients C(N-c_i, n') / C(N, n').\n                # log(P_miss) = log( (N-c_i)! * (N-n')! / (N! * (N-c_i-n')!) )\n                # This is computed using the log-gamma function, where lgamma(x+1) = log(x!).\n                log_prob_miss = (gammaln(N - c_i + 1) +\n                                 gammaln(N - n_prime + 1) -\n                                 gammaln(N + 1) -\n                                 gammaln(N - c_i - n_prime + 1))\n                \n                sum_prob_miss += np.exp(log_prob_miss)\n\n        # The expected number of clonotypes at depth n_prime\n        E_S_N_minus_m = R - sum_prob_miss\n\n        # The average marginal gain 'g' is the slope of the rarefaction curve\n        # approximated over the last m reads.\n        # The problem statement guarantees 1 = m = N, so m is never zero.\n        g = (E_S_N - E_S_N_minus_m) / m\n\n        return g > alpha\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # counts, m, alpha\n        ([25, 20, 15, 10, 8, 7, 5, 5, 3, 2], 10, 0.05),\n        (([50] + [1] * 50), 10, 0.15),\n        ([5] * 20, 10, 0.05),\n        ([100], 10, 0.001),\n        ([1] * 100, 10, 0.5),\n    ]\n\n    results = []\n    for counts, m, alpha in test_cases:\n        decision = compute_decision(counts, m, alpha)\n        # Format boolean as specified (True, False)\n        results.append(str(decision))\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "2399358"}, {"introduction": "测序数据中充满了噪声，区分真实的低频克隆型和高频克隆型产生的测序错误是免疫组库分析的核心挑战。这个练习将让你扮演一个生物信息学家的角色，使用统计假设检验来解决这个问题。你将建立一个基于二项分布的错误模型，并计算$p$值，以判断一个观测到的候选克隆型是真实的生物信号还是随机的技术假象。[@problem_id:2399399]", "problem": "给定一个免疫组库测序实验的独立读段抽样模型，其中样本中存在一个已知的克隆型，其相对频率为 $f$。设总读段数为 $N$。在测序数据中，一个候选克隆型被观察到 $k$ 次。来自已知克隆型的每个读段都有可能因测序错误而被独立地误读为候选克隆型，其概率为 $p_{error}$，其中 $p_{error}$ 是一个有效的、针对该特定错误序列的单读段错误识别概率。假设读段是独立的，并且与其他克隆型通过测序错误对候选克隆型产生的贡献相比，来自已知克隆型的贡献是微不足道的。在原假设（即候选克隆型完全是已知克隆型错误衍生的产物）下，候选克隆型观察到的读段数 $K$ 服从参数为 $N$ 和 $f \\cdot p_{error}$ 的二项分布。在备择假设下，除了来自已知克隆型的任何错误衍生贡献外，该候选克隆型还有一个真实的潜在频率 $\\theta$。对于每种情况，使用显著性水平为 $\\alpha$ 的右尾准则来判断观察结果是否与原假设不一致。\n\n您的任务是编写一个完整的程序，针对每个测试用例，输出一个布尔值，该布尔值基于在原假设模型下观察到至少 $k$ 次的概率以及阈值 $\\alpha$，来指示是否应将候选克隆型判定为真实克隆型（即拒绝原假设）。程序必须硬编码以下测试套件，并按要求格式生成结果。\n\n测试套件（每个用例是一个 $(N, f, p_{error}, k, \\alpha)$ 元组）：\n- 用例 1：$(N=100000, f=0.01, p_{error}=0.001, k=6, \\alpha=0.01)$\n- 用例 2：$(N=100000, f=0.01, p_{error}=0.001, k=1, \\alpha=0.05)$\n- 用例 3：$(N=20000, f=0.005, p_{error}=0.001, k=1, \\alpha=0.1)$\n- 用例 4：$(N=50000, f=0.0, p_{error}=0.001, k=1, \\alpha=0.05)$\n- 用例 5：$(N=1000000, f=0.01, p_{error}=0.001, k=8, \\alpha=0.01)$\n\n最终输出格式：\n您的程序应生成单行输出，其中包含一个由方括号括起来的逗号分隔列表，每个条目按测试用例的顺序为 True 或 False。例如，输出必须如下所示：`[result1,result2,result3,result4,result5]`。", "solution": "所提出的问题是统计假设检验中的一个标准练习，应用于计算生物学领域，特别是免疫组库测序数据的分析。该问题定义明确，具有科学依据，并包含了得出唯一解所需的所有信息。我们将着手进行形式化分析。\n\n核心任务是确定观察到的候选克隆型是一个真实的生物实体，还是仅仅是一个已知的、高丰度克隆型测序错误的产物。这被表述为一个假设检验。\n\n原假设 $H_0$ 是：候选克隆型完全是一个错误产物。在此假设下，我们用随机变量 $K$ 表示其观察到的读段数，该读段数源于属于已知克隆型的读段的错误测序事件。在一个总共测序了 $N$ 个读段的样本中，已知克隆型的真实相对频率为 $f$。已知克隆型的任何单个读段被错误测序为候选克隆型的概率为 $p_{error}$。由于我们假设来自任何其他来源的贡献可以忽略不计，那么在整个 $N$ 个读段的测序运行中，任何一个给定的读段因这一特定错误过程而被观察为候选克隆型的概率是“抽样到已知克隆型”的概率与“随后发生错误”的概率的乘积。这给出了单次读段的成功概率 $p = f \\cdot p_{error}$。\n\n正如问题所述，读段是独立抽样的。因此，候选克隆型的观察读段数 $K$ 服从具有 $N$ 次试验和成功概率 $p$ 的二项分布。\n$$K \\sim \\text{Binomial}(n, p)$$\n其中 $n = N$ 且 $p = f \\cdot p_{error}$。\n\n我们获得了一个候选克隆型的观察计数值为 $k$。为了检验 $H_0$ 的有效性，我们采用右尾检验。我们计算在 $H_0$ 为真的前提下，观察到至少与我们测量结果一样极端的结果的概率。这个概率就是 p 值。对于右尾检验，p 值的计算公式为：\n$$p\\text{-value} = P(K \\ge k | H_0)$$\n\n该概率可根据二项分布的概率质量函数 (PMF) $P(K=i) = \\binom{n}{i} p^i (1-p)^{n-i}$ 计算如下：\n$$p\\text{-value} = \\sum_{i=k}^{n} \\binom{n}{i} p^i (1-p)^{n-i}$$\n\n出于计算目的，使用累积分布函数 (CDF) $F(k) = P(K \\le k)$ 的补集可以更高效、更准确地计算此和。它们的关系是：\n$$P(K \\ge k) = 1 - P(K  k) = 1 - P(K \\le k-1) = 1 - F(k-1; n, p)$$\n这等价于在 $k-1$ 处求值的生存函数 (SF)。\n\n决策规则是将计算出的 p 值与预先设定的显著性水平 $\\alpha$ 进行比较。如果 p 值小于或等于 $\\alpha$，则认为观察到的结果在原假设下因偶然发生的可能性足够小。\n$$\\text{若 } p\\text{-value} \\le \\alpha \\text{，则拒绝 } H_0$$\n\n拒绝 $H_0$ 意味着我们有统计证据断定，该候选克隆型不仅仅是一个错误产物，应被视为一个“真实”的克隆型。如果 $p\\text{-value}  \\alpha$，我们未能拒绝 $H_0$，这意味着观察结果与错误模型一致。\n\n对于每个具有参数 $(N, f, p_{error}, k, \\alpha)$ 的测试用例，我们将：\n1.  计算二项分布的概率参数 $p = f \\cdot p_{error}$。\n2.  对于 $K \\sim \\text{Binomial}(N, p)$，计算 p 值 $P(K \\ge k)$。\n3.  将 p 值与 $\\alpha$ 进行比较，并断定是否拒绝 $H_0$。拒绝对应于 `True` 结果，未能拒绝则对应于 `False` 结果。\n\n让我们简要分析一下 $f=0$ 的特殊情况，如用例 $4$ 所示。此时，概率参数 $p = 0 \\cdot p_{error} = 0$。分布为 $\\text{Binomial}(N, 0)$，这意味着 $P(K=0)=1$ 且 $P(K0)=0$。如果观察到任何非零计数值 $k \\ge 1$，则 p 值 $P(K \\ge k | p=0)$ 恰好为 $0$。由于对于任何常规的显著性水平，都有 $0 \\le \\alpha$，因此在原假设模型下不可能发生的情况下，即使观察到一个读段，也会立即导致拒绝 $H_0$。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import binom\n\ndef solve():\n    \"\"\"\n    Solves the immune repertoire clonotype calling problem for a suite of test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # Each case is a tuple of (N, f, p_error, k, alpha).\n    test_cases = [\n        (100000, 0.01, 0.001, 6, 0.01),\n        (100000, 0.01, 0.001, 1, 0.05),\n        (20000, 0.005, 0.001, 1, 0.1),\n        (50000, 0.0, 0.001, 1, 0.05),\n        (1000000, 0.01, 0.001, 8, 0.01),\n    ]\n\n    results = []\n    for case in test_cases:\n        # Unpack parameters for the current test case\n        N, f, p_error, k, alpha = case\n\n        # Number of trials for the Binomial distribution\n        n = int(N)\n        \n        # Probability of success for each trial under the null hypothesis\n        p = f * p_error\n\n        # Under the null hypothesis, the number of observed reads K for the candidate\n        # clonotype follows a Binomial distribution B(n, p).\n        # We perform a right-tail test. The p-value is the probability of\n        # observing a count of at least k.\n        # p-value = P(K >= k)\n\n        # Handle the edge case where k=0. P(K>=0) is always 1.\n        # The problem implies k>=1 since the clonotype is \"observed k times\".\n        # However, for completeness:\n        if k == 0:\n            p_value = 1.0\n        else:\n            # P(K >= k) is calculated using the survival function (SF), which is 1 - CDF.\n            # sf(x) = P(X > x). Therefore, P(K >= k) = P(K > k-1) = sf(k-1).\n            # This is numerically more stable than 1 - cdf(k-1) or summing the pmf.\n            p_value = binom.sf(k - 1, n, p)\n\n        # Reject the null hypothesis if the p-value is less than or equal to the\n        # significance level alpha. Rejecting H0 means we call it a true clonotype.\n        is_true_clonotype = p_value = alpha\n        results.append(is_true_clonotype)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2399399"}, {"introduction": "在确认了一组真实的克隆型之后，下一步是挖掘它们之间的生物学关系。针对同一抗原的T细胞常常具有相似的CDR3序列，形成功能相关的“克隆家族”。本练习将引导你使用网络科学的方法来识别这些结构，你将首先基于序列编辑距离（Levenshtein distance）构建一个克隆型相似性网络，然后通过最大化模块度（modularity）来应用社区发现算法，从而揭示免疫组库中潜在的高阶组织模式。[@problem_id:2399318]", "problem": "给定一个有限的氨基酸序列集合，代表了T细胞受体的互补决定区3 (CDR3) 的氨基酸序列。设该序列集合被索引为 $\\{s_1,\\dots,s_n\\}$。将任意两个序列 $s_i$ 和 $s_j$ 之间的编辑距离（Levenshtein距离） $d(s_i,s_j)$ 定义为将 $s_i$ 转换为 $s_j$ 所需的最小单字符插入、删除或替换次数。对于一个固定的非负整数阈值 $\\tau$，构建一个无向简单图 $G=(V,E)$，其中 $V=\\{1,\\dots,n\\}$，且当且仅当 $d(s_i,s_j)\\le \\tau$ 时，边 $\\{i,j\\}\\in E$ 存在。设 $A\\in\\{0,1\\}^{n\\times n}$ 为 $G$ 的邻接矩阵，其中对所有 $i$ 都有 $A_{ii}=0$。设 $k_i=\\sum_{j=1}^n A_{ij}$ 表示节点 $i$ 的度。设 $m=\\frac{1}{2}\\sum_{i=1}^n\\sum_{j=1}^n A_{ij}$ 表示总边数。\n\n一个划分（社群分配）$c$ 为每个节点 $i$ 分配一个社群标签 $c_i\\in\\{1,\\dots,C\\}$，其中 $C$ 是某个正整数。图 $G$ 上一个划分 $c$ 的 Newman–Girvan 模块度 $Q(c)$ 定义为\n$$\nQ(c)=\\frac{1}{2m}\\sum_{i=1}^n\\sum_{j=1}^n\\left(A_{ij}-\\frac{k_i k_j}{2m}\\right)\\,\\mathbf{1}\\{c_i=c_j\\},\n$$\n其中 $\\mathbf{1}\\{\\cdot\\}$ 是指示函数。按照惯例，当 $m=0$（没有边）时，对所有划分 $c$ 定义 $Q(c)=0$。\n\n你的任务是，对于每个指定的测试用例，计算一个能使 $Q(c)$ 在所有可能的划分 $c$ 中达到最大值的划分 $c^\\star$。如果存在多个使模块度最大化的划分，按顺序应用以下确定性的平局打破规则：\n- 如果 $m=0$，选择每个节点自成一个社群的划分（即，$C=n$ 且每个社群的大小为 $1$）。\n- 否则，在模块度最大的划分中，选择社群数量 $C$ 最少的那一个。\n- 如果仍然平局，在余下的划分中，选择其社群大小的多重集（按非递增顺序排序后）字典序最小的那一个。\n- 如果仍然平局，选择其社群标签向量 $(c_1,\\dots,c_n)$（按照首次出现的顺序重排为连续标签后）字典序最小的那一个。\n\n对于每个测试用例，报告 $c^\\star$ 的社群大小列表，按非递增顺序排序。最终的程序输出必须将所有测试用例的结果汇总到单行中，该行包含一个列表，其元素是每个测试用例的社群大小列表，且不含空格。\n\n测试套件：\n- 测试用例 1：$\\tau=1$，序列\n  - $s_1=$ \"CASSLGQETQYF\"\n  - $s_2=$ \"CASSLGQETQFF\"\n  - $s_3=$ \"CASSLGQDTQYF\"\n  - $s_4=$ \"CASSIRSSYEQYF\"\n  - $s_5=$ \"CASSIGSSYEQYF\"\n  - $s_6=$ \"CASSIRSSYEQFF\"\n- 测试用例 2：$\\tau=0$，序列\n  - $s_1=$ \"CASSLAPGNTIYF\"\n  - $s_2=$ \"CATSQRGQLNTQF\"\n  - $s_3=$ \"CATSASGQGNNEQF\"\n  - $s_4=$ \"CASSYNEGYTF\"\n- 测试用例 3：$\\tau=2$，序列\n  - $s_1=$ \"CASSLGQETQYF\"\n  - $s_2=$ \"CASSLGQETQFF\"\n  - $s_3=$ \"CASSLGQDTQYF\"\n  - $s_4=$ \"CASSLGRETQYF\"\n- 测试用例 4：$\\tau=1$，序列\n  - $s_1=$ \"CASSVGQETQYF\"\n  - $s_2=$ \"CASSVGQETQFF\"\n  - $s_3=$ \"CASSVGRETQYF\"\n  - $s_4=$ \"CASSIGQETQYF\"\n  - $s_5=$ \"CASSIRSSYEQYF\"\n  - $s_6=$ \"CASSIRSSYEQFF\"\n\n最终输出格式：\n- 你的程序应生成单行输出，其中包含按给定顺序排列的测试用例结果列表。每个结果必须是该测试用例的社群大小列表，按非递增顺序排序。外层列表和所有内层列表都必须在没有空格的情况下呈现，例如 `[[a,b],[c],[d,e,f]]`，其中 $a$、$b$、$c$、$d$、$e$ 和 $f$ 是整数。", "solution": "该问题陈述经过了严格的验证，被确定为具有科学依据、定义明确且客观。它提出了一个植根于网络科学和生物信息学既定原则的形式化计算任务。因此，该问题是有效的，下面提供了解决方案。\n\n问题的核心是找到节点集 $V=\\{1, \\dots, n\\}$ 的一个划分 $c^\\star$，以最大化 Newman-Girvan 模块度函数 $Q(c)$。节点代表T细胞受体CDR3序列，图结构由它们之间的相似性决定，该相似性使用 Levenshtein 距离进行量化。\n\n一个划分 $c$ 的模块度由下式给出\n$$\nQ(c)=\\frac{1}{2m}\\sum_{i=1}^n\\sum_{j=1}^n\\left(A_{ij}-\\frac{k_i k_j}{2m}\\right)\\,\\mathbf{1}\\{c_i=c_j\\},\n$$\n其中 $A$ 是邻接矩阵，$k_i$ 是节点 $i$ 的度，$m$ 是总边数，$\\mathbf{1}\\{c_i=c_j\\}$ 是指示函数。对于 $m=0$ 的退化情况，$Q(c)$ 被定义为 $0$。模块度 $Q$ 通过比较社群内部的边密度与具有相同度分布的随机图中预期的边密度，来量化一个划分的质量。更高的 $Q$ 值表示更显著的社群结构。\n\n对于一般图，最大化 $Q(c)$ 的任务是NP难问题。然而，在所提供的测试用例中，节点数量 $n$ 很小（最多为 $n=6$）。一个 $n$ 元集合的不同划分总数由贝尔数 $B_n$ 给出。对于 $n=6$，$B_6=203$，这是一个计算上可行的数字。因此，该问题可以通过对节点集的所有可能划分进行穷举搜索来解决。\n\n总体算法如下：\n\n1.  **图的构建**：对于每个测试用例，由一组序列 $\\{s_1, \\dots, s_n\\}$ 和一个阈值 $\\tau$ 构成，构建一个无向图 $G=(V, E)$。当且仅当 Levenshtein 距离 $d(s_i, s_j) \\le \\tau$ 时，边 $\\{i,j\\}$ 存在。由此，计算出邻接矩阵 $A$、节点度 $k_i$ 和总边数 $m$。Levenshtein 距离使用标准的动态规划算法计算。\n\n2.  **划分生成**：生成顶点集 $V=\\{1, \\dots, n\\}$ 的所有可能划分。这可以通过递归算法来完成。每个划分都是 $V$ 的一组非空、不相交的子集，这些子集的并集为 $V$。\n\n3.  **模块度计算与优化**：对于每个生成的划分 $c$，计算其模块度 $Q(c)$。该公式可以更方便地表示为对社群求和的形式以进行计算：\n    $$\n    Q(c) = \\sum_{l=1}^C \\left( \\frac{e_l}{m} - \\left(\\frac{d_l}{2m}\\right)^2 \\right),\n    $$\n    其中，求和遍历划分中的 $C$ 个社群，$e_l$ 是社群 $l$ 内部的边数，$d_l$ 是社群 $l$ 中节点度的总和。\n\n4.  **平局打破**：问题指定了一套严格、确定性的平局打破规则。为了找到唯一的最优划分 $c^\\star$，我们寻求在所有可能划分的集合中找到字典序最小的元组，其中一个划分 $c$ 的元组构造如下：\n    $$\n    \\left( -Q(c), C(c), S(c), V(c) \\right)\n    $$\n    - $-Q(c)$: 模块度的负值。最小化此值即最大化 $Q(c)$。\n    - $C(c)$: 划分 $c$ 中的社群数量。\n    - $S(c)$: 社群大小的元组，按非递增顺序排序。\n    - $V(c)$: 规范化的社群标签向量 $(c_1, \\dots, c_n)$，按照首次出现的顺序重排为连续整数（例如，包含节点 $1$ 的社群标记为 $1$，扫描节点 $2, 3, \\dots$ 时遇到的第一个新社群标记为 $2$，以此类推）。这为每个划分提供了唯一的表示。\n\n    搜索算法会遍历所有划分，为每个划分计算此元组，并保留与找到的字典序最小的元组相对应的划分。$m=0$ 的特殊情况按照问题陈述中的定义单独处理。\n\n5.  **非连通图的分解**：一个关键的观察是，一个最优划分绝不会将来自图的不同连通分量的节点分在同一组。可以证明，合并来自不同连通分量的社群会严格降低模块度。这意味着问题可以分解：可以为每个连通分量独立地找到最优划分，这些划分的并集构成了整个图的最优划分。全局划分的模块度不是各分量模块度的简单求和，但最优性分解原则是成立的。对于测试套件中的小 $n$ 值，这种简化并非绝对必要，但它是模块度的一个基本属性。对于给定的测试用例，这一洞察简化了手动分析。例如，在测试用例1中，图分解为两个不相交的星形图，从而可以进行单独分析。\n\n通过对每个测试用例系统地应用此过程，可以确定唯一的最优划分 $c^\\star$。每个用例的最终输出是 $c^\\star$ 中社群大小的列表，按非递增顺序排序。", "answer": "```python\nimport numpy as np\n\ndef levenshtein_distance(s1: str, s2: str) -> int:\n    \"\"\"Calculates the Levenshtein distance between two strings.\"\"\"\n    m, n = len(s1), len(s2)\n    if m  n:\n        s1, s2 = s2, s1\n        m, n = n, m\n    \n    if n == 0:\n        return m\n\n    dp_row = np.arange(n + 1, dtype=int)\n\n    for i in range(1, m + 1):\n        prev_row_val = dp_row[0]\n        dp_row[0] = i\n        for j in range(1, n + 1):\n            temp = dp_row[j]\n            cost = 0 if s1[i - 1] == s2[j - 1] else 1\n            dp_row[j] = min(dp_row[j] + 1,          # Deletion\n                            dp_row[j - 1] + 1,      # Insertion\n                            prev_row_val + cost) # Substitution\n            prev_row_val = temp\n            \n    return dp_row[n]\n\ndef generate_partitions(n: int):\n    \"\"\"Generates all partitions of the set {0, 1, ..., n-1}.\"\"\"\n    elements = list(range(n))\n    def _generate(index, partition):\n        if index == n:\n            yield [list(p) for p in partition]\n            return\n        \n        # Add to an existing community\n        for p_set in partition:\n            p_set.add(elements[index])\n            yield from _generate(index + 1, partition)\n            p_set.remove(elements[index])\n            \n        # Add to a new community\n        partition.append({elements[index]})\n        yield from _generate(index + 1, partition)\n        partition.pop()\n\n    if n == 0:\n        yield []\n        return\n        \n    yield from _generate(1, [{elements[0]}])\n\ndef get_canonical_vector(partition, n: int) -> tuple[int, ...]:\n    \"\"\"Computes the canonical label vector for a partition.\"\"\"\n    # Create an arbitrary labeling first\n    temp_c = [0] * n\n    label = 1\n    for community in partition:\n        for node_idx in community:\n            temp_c[node_idx] = label\n        label += 1\n    \n    # Relabel to be consecutive in order of first appearance\n    c_canon = [0] * n\n    mapping = {}\n    next_canon_label = 1\n    for i in range(n):\n        original_label = temp_c[i]\n        if original_label not in mapping:\n            mapping[original_label] = next_canon_label\n            next_canon_label += 1\n        c_canon[i] = mapping[original_label]\n    \n    return tuple(c_canon)\n\n\ndef solve():\n    \"\"\"\n    Solves the modularity maximization problem for all test cases.\n    \"\"\"\n    test_cases = [\n        {\n            \"tau\": 1,\n            \"sequences\": [\n                \"CASSLGQETQYF\", \"CASSLGQETQFF\", \"CASSLGQDTQYF\",\n                \"CASSIRSSYEQYF\", \"CASSIGSSYEQYF\", \"CASSIRSSYEQFF\"\n            ]\n        },\n        {\n            \"tau\": 0,\n            \"sequences\": [\n                \"CASSLAPGNTIYF\", \"CATSQRGQLNTQF\",\n                \"CATSASGQGNNEQF\", \"CASSYNEGYTF\"\n            ]\n        },\n        {\n            \"tau\": 2,\n            \"sequences\": [\n                \"CASSLGQETQYF\", \"CASSLGQETQFF\",\n                \"CASSLGQDTQYF\", \"CASSLGRETQYF\"\n            ]\n        },\n        {\n            \"tau\": 1,\n            \"sequences\": [\n                \"CASSVGQETQYF\", \"CASSVGQETQFF\", \"CASSVGRETQYF\",\n                \"CASSIGQETQYF\", \"CASSIRSSYEQYF\", \"CASSIRSSYEQFF\"\n            ]\n        }\n    ]\n\n    results = []\n\n    for case in test_cases:\n        tau, sequences = case[\"tau\"], case[\"sequences\"]\n        n = len(sequences)\n\n        # Step 1: Construct graph\n        adj = np.zeros((n, n), dtype=int)\n        for i in range(n):\n            for j in range(i + 1, n):\n                if levenshtein_distance(sequences[i], sequences[j]) = tau:\n                    adj[i, j] = adj[j, i] = 1\n\n        degrees = np.sum(adj, axis=1)\n        m_total_edges = int(np.sum(degrees) / 2)\n\n        # Handle m=0 case as per tie-breaking rule 1\n        if m_total_edges == 0:\n            results.append([1] * n)\n            continue\n\n        best_key = (float('inf'),)\n        best_partition_sizes = []\n\n        two_m = 2 * m_total_edges\n\n        # Iterate through all partitions of the nodes\n        for partition in generate_partitions(n):\n            \n            # Calculate modularity\n            q_val = 0.0\n            for community in partition:\n                d_l = np.sum(degrees[community])\n                e_l = 0\n                community_nodes = list(community)\n                for i in range(len(community_nodes)):\n                    for j in range(i + 1, len(community_nodes)):\n                        u, v = community_nodes[i], community_nodes[j]\n                        e_l += adj[u, v]\n                \n                term_1 = e_l / m_total_edges if m_total_edges > 0 else 0\n                term_2 = (d_l / two_m)**2 if two_m > 0 else 0\n                q_val += (term_1 - term_2)\n            \n            # Get tie-breaker values\n            num_communities = len(partition)\n            sorted_sizes = tuple(sorted([len(c) for c in partition], reverse=True))\n            canonical_vec = get_canonical_vector(partition, n)\n            \n            current_key = (-q_val, num_communities, sorted_sizes, canonical_vec)\n\n            if best_key[0] == float('inf') or current_key  best_key:\n                best_key = current_key\n                best_partition_sizes = list(sorted_sizes)\n        \n        results.append(best_partition_sizes)\n\n    # Final output formatting\n    output_str = str(results).replace(\" \", \"\")\n    print(output_str)\n\nsolve()\n```", "id": "2399318"}]}