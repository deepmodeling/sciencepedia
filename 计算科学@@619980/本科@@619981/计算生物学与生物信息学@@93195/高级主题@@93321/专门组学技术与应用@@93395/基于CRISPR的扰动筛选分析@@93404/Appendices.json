{"hands_on_practices": [{"introduction": "CRISPR筛选分析的核心是从原始测序读数中识别出哪些基因的扰动会导致显著的表型变化。本练习将指导您从头开始构建一个简化但功能强大的统计分析流程，其灵感源自$DESeq2$等业内标准工具，用于将原始的向导RNA（gRNA）计数数据转化为“命中”基因的排序列表。通过亲手实现这个流程，您将巩固对计数数据的归一化、离散度估计和广义线性模型等核心概念的理解[@problem_id:2371981]。", "problem": "你的任务是借鉴序列计数数据差异表达分析（DESeq2）的思路，从基本原理出发，实现一个完整的差异分析流程，用于比较两个混合的成簇规律间隔短回文重复序列（CRISPR）扰动筛选。目标是识别出条件特异性必需基因，此处定义为那些在一个条件下，经过适当的归一化和统计建模后，其向导RNA丰度显著低于另一条件的基因。\n\n请从以下基本依据和经过充分检验的事实出发：\n- 来自混合筛选的计数数据是非负整数，并且相对于泊松模型表现出过度离散。一个广泛使用的模型是负二项（NB）分布，其方差函数为 $V(\\mu) = \\mu + \\alpha \\mu^{2}$，其中 $\\mu$ 是均值，$\\alpha \\ge 0$ 是离散度。\n- 带有对数连接函数的广义线性模型（GLM）是建模跨条件计数的标准方法。对于样本 $j$，其库大小因子为 $s_{j}$，设计矩阵 $X$ 包含一个截距项和一个二元条件指示变量 $x_{j} \\in \\{0,1\\}$，则对于基因 $g$ 的模型为 $\\log \\mu_{gj} = \\log s_{j} + \\beta_{g0} + \\beta_{g1} x_{j}$。\n- 比率中位数法（median-of-ratios method）提供了稳健的文库大小归一化。对于每个样本 $j$，库大小因子 $s_{j}$ 是 $k_{gj}/G_{g}$ 在所有基因中的中位数，其中 $k_{gj}$ 是原始计数值，$G_{g}$ 是 $k_{g\\cdot}$ 在所有样本中的几何平均数，该平均数仅在所有样本中计数值均为严格正数的基因上计算。\n- 使用来自费雪信息（Fisher Information）的方差并通过渐近正态近似的Wald检验，提供了一种检验系数是否等于零的方法。\n- Benjamini–Hochberg (BH) 程序可以控制假发现率（FDR）。\n\n你必须实现以下步骤：\n1) 通过比率中位数法进行库大小因子归一化：\n   - 对每个基因 $g$，仅当对所有 $j \\in \\{1,\\dots,m\\}$ 都有 $k_{gj} > 0$ 时，计算其几何平均数 $G_{g} = \\exp\\left( \\frac{1}{m} \\sum_{j=1}^{m} \\log k_{gj} \\right)$；否则，在该步骤中排除基因 $g$。\n   - 对每个样本 $j$，计算所有具有有效 $G_{g}$ 的基因 $g$ 的比率 $r_{gj} = k_{gj} / G_{g}$，并将 $s_{j}$ 设为 $\\{ r_{gj} \\}$ 的中位数。\n   - 中心化库大小因子，使 $\\exp\\left( \\frac{1}{m} \\sum_{j=1}^{m} \\log s_{j} \\right) = 1$。\n2) 通过矩量法估计公共离散度：\n   - 对每个基因 $g$，计算归一化后的计数值 $y_{gj} = k_{gj} / s_{j}$ 及其在所有 $m$ 个样本中的均值 $\\bar{y}_{g}$ 和样本方差 $S^{2}_{g}$。\n   - 对于 $\\bar{y}_{g} > 0$ 的基因，定义一个单基因离散度估计值 $\\hat{\\alpha}_{g} = \\max\\left(0,\\; \\frac{S^{2}_{g} - \\bar{y}_{g}}{\\bar{y}_{g}^{2}} \\right)$。\n   - 将所有 $\\bar{y}_{g} > 0$ 的基因的 $\\{ \\hat{\\alpha}_{g} \\}$ 的中位数定义为单一的公共离散度 $\\hat{\\alpha}$。\n3) 对每个基因 $g$，使用Fisher评分（迭代重加权最小二乘法）拟合一个带有对数连接函数的负二项GLM，其中使用偏移量 $\\log s_{j}$，公共离散度 $\\hat{\\alpha}$，以及包含截距项和条件指示变量 $x_{j} \\in \\{0,1\\}$ 的设计矩阵：\n   - 初始化 $\\beta_{g0}$ 和 $\\beta_{g1}$（例如，$\\beta_{g1}=0$，$\\beta_{g0}$ 为平均归一化计数的对数）。\n   - 在每次迭代中，计算线性预测器 $\\eta_{gj} = \\log s_{j} + \\beta_{g0} + \\beta_{g1} x_{j}$，均值 $\\mu_{gj} = \\exp(\\eta_{gj})$，权重 $w_{gj} = \\frac{\\mu_{gj}}{1 + \\hat{\\alpha}\\mu_{gj}}$，以及工作响应 $z_{gj} = \\eta_{gj} + \\frac{k_{gj} - \\mu_{gj}}{\\mu_{gj}}$。\n   - 更新 $\\beta_{g} = (\\mathbf{X}^{\\top} \\mathbf{W} \\mathbf{X})^{-1} \\mathbf{X}^{\\top} \\mathbf{W} \\mathbf{z}$ 直到收敛或达到固定的迭代上限，其中 $\\mathbf{W}$ 是对角矩阵，其对角线元素为 $w_{gj}$。\n4) 对于基因 $g$，收敛后，从 $(\\mathbf{X}^{\\top} \\mathbf{W} \\mathbf{X})$ 的逆矩阵中提取估计的系数 $\\hat{\\beta}_{g1}$ 及其标准误 $\\operatorname{se}(\\hat{\\beta}_{g1})$。构建Wald统计量 $Z_{g} = \\hat{\\beta}_{g1} / \\operatorname{se}(\\hat{\\beta}_{g1})$，并使用标准正态分布计算双边 $p$ 值。\n5) 对所有基因应用 Benjamini–Hochberg 程序，以获得校正后的 $q$ 值。\n6) 如果一个基因同时满足以下两个标准，则定义其为“相对于条件0，在条件1中是条件特异性必需的”：\n   - 校正后的 $q$ 值小于 $\\alpha_{\\mathrm{FDR}}$。\n   - 估计的 log$_{2}$ 倍数变化 $\\widehat{\\mathrm{LFC}}_{g} = \\hat{\\beta}_{g1} / \\log 2$ 小于或等于 $- \\tau$，其中 $\\tau$ 是用户指定的非负阈值。\n\n在所有样本中计数值均为零的基因，或 $\\bar{y}_{g} = 0$ 的基因，应从检验中排除。\n\n你的程序必须实现上述流程，并在以下测试集上运行。对于每个测试用例，输入包括一个原始计数矩阵（基因为行，样本为列），以及一个长度等于样本数的二元条件分配向量。使用 $\\alpha_{\\mathrm{FDR}} = 0.1$ 和 $\\tau = 1.0$，并报告被判定为相对于条件0在条件1中是条件特异性必需的基因的从零开始的索引。\n\n测试集：\n- 案例1（均衡的重复，明显的差异性耗尽）：\n  - 计数矩阵 $K$，包含6个基因和6个样本（前3个样本为条件0，后3个样本为条件1）：\n    - 基因0：$[100, 90, 110, 120, 80, 100]$\n    - 基因1：$[50, 45, 55, 60, 40, 50]$\n    - 基因2：$[30, 27, 33, 9, 6, 8]$\n    - 基因3：$[20, 18, 22, 24, 16, 20]$\n    - 基因4：$[10, 9, 11, 3, 2, 3]$\n    - 基因5：$[60, 54, 66, 72, 48, 60]$\n  - 条件向量 $x = [0, 0, 0, 1, 1, 1]$。\n- 案例2（严重的文库大小不平衡，单个差异基因）：\n  - 计数矩阵 $K$，包含5个基因和4个样本（前2个样本为条件0，后2个样本为条件1）：\n    - 基因0：$[160, 40, 120, 56]$\n    - 基因1：$[80, 20, 12, 6]$\n    - 基因2：$[40, 10, 30, 14]$\n    - 基因3：$[30, 7, 23, 10]$\n    - 基因4：$[10, 3, 8, 4]$\n  - 条件向量 $x = [0, 0, 1, 1]$。\n- 案例3（含有零的稀疏计数，一个强差异基因）：\n  - 计数矩阵 $K$，包含6个基因和4个样本（前2个样本为条件0，后2个样本为条件1）：\n    - 基因0：$[2, 1, 2, 1]$\n    - 基因1：$[0, 0, 0, 0]$\n    - 基因2：$[3, 2, 3, 2]$\n    - 基因3：$[1, 0, 1, 0]$\n    - 基因4：$[4, 4, 4, 4]$\n    - 基因5：$[6, 5, 1, 1]$\n  - 条件向量 $x = [0, 0, 1, 1]$。\n\n要求和输出：\n- 严格按照描述实现完整的流程。\n- 对每个案例，返回满足显著性和效应量标准的、从零开始的基因索引的排序列表。\n- 你的程序应生成一行输出，其中包含一个用方括号括起来的、以逗号分隔的结果列表（例如，$[result_{1},result_{2},result_{3}]$），其中每个 $result_{i}$ 是一个整数列表，表示案例 $i$ 中被检出基因的索引。对于所提供的测试集，程序应输出一行类似 $[[i_{1},i_{2}], [j_{1}], [k_{1}]]$ 的内容，其中包含发现的索引。", "solution": "所提出的问题是计算生物学中一个定义明确且科学合理的任务。它要求实现一个标准化的统计流程，用以从计数数据中识别差异丰度的特征，特别是在CRISPR筛选的背景下。该方法论是像DESeq2这类成熟算法的简化变体，它建立在广义线性模型和既定统计实践的坚实基础之上。该问题是有效的，并且可以通过遵循所述步骤构建一个严谨的解决方案。\n\n分析从基本原理出发，如下所述。\n\n**1. 统计基础：负二项模型**\n来自混合CRISPR筛选的原始数据是计数，$k_{gj}$，表示样本 $j$ 中向导RNA $g$ 的测序读数数量。这类数据是非负整数，并且总是表现出过度离散，即方差大于均值。因此，方差等于均值的泊松分布是不够的。我们采用负二项（NB）分布，它增加了一个离散度参数 $\\alpha$ 来模拟这种额外的方差。方差-均值关系由下式给出：\n$$V(\\mu_{g}) = \\mu_{g} + \\alpha \\mu_{g}^{2}$$\n其中 $\\mu_{g}$ 是向导 $g$ 的平均丰度，$\\alpha \\ge 0$ 是离散度参数。当 $\\alpha=0$ 时，NB分布简化为泊松分布。\n\n**2. 文库大小归一化**\n不同样本的测序深度不同，导致总读数（文库大小）存在系统性差异。为了使计数在样本间具有可比性，我们必须进行归一化。问题指定了比率中位数法，该方法对大部分差异丰度基因具有稳健性。\n\n首先，对于在所有 $m$ 个样本中都具有非零计数 $k_{gj} > 0$ 的基因子集，我们通过计算每个基因 $g$ 计数的几何平均数来构建一个伪参考样本：\n$$G_{g} = \\left( \\prod_{j=1}^{m} k_{gj} \\right)^{1/m} = \\exp\\left( \\frac{1}{m} \\sum_{j=1}^{m} \\log k_{gj} \\right)$$\n其次，对于每个样本 $j$，我们计算其计数值 $k_{gj}$ 与参考子集中所有基因的伪参考值 $G_{g}$ 的比率。样本 $j$ 的库大小因子 $s_{j}$ 是这些比率的中位数：\n$$s_{j} = \\underset{g}{\\text{median}} \\left\\{ \\frac{k_{gj}}{G_{g}} \\right\\}$$\n最后，将这些库大小因子中心化，使其几何平均数为1，确保归一化不会改变计数数据的总体尺度。这通过将每个 $s_j$ 除以所有库大小因子的几何平均数来实现。\n\n**3. 公共离散度估计**\n离散度参数 $\\alpha$ 必须从数据中估计。虽然可以进行单基因离散度估计，但在重复次数少时不稳定。一个常见的做法是估计一个单一的、在所有基因间共享的全局离散度值 $\\hat{\\alpha}$。这是一种汇集信息的稳健方法。这里使用矩量法。\n\n对于每个基因 $g$，归一化计数计算为 $y_{gj} = k_{gj} / s_{j}$。我们计算这些归一化计数在所有样本中的均值 $\\bar{y}_{g}$ 和样本方差 $S^{2}_{g}$。根据NB方差函数，我们有 $S_g^2 \\approx \\bar{y}_g + \\alpha_g \\bar{y}_g^2$。整理后得到单基因离散度估计值：\n$$\\hat{\\alpha}_{g} = \\frac{S^{2}_{g} - \\bar{y}_{g}}{\\bar{y}_{g}^{2}}$$\n为确保非负性，我们取 $\\max(0, \\hat{\\alpha}_{g})$。最终的公共离散度 $\\hat{\\alpha}$ 是这些单基因估计值的中位数，仅在 $\\bar{y}_{g} > 0$ 的基因上计算。中位数提供了对具有极端方差的离群基因的稳健性。\n\n**4. 广义线性模型（GLM）拟合**\n为了建模实验条件对基因丰度的影响，我们为每个基因的计数拟合一个负二项GLM。该模型通过一个对数连接函数将期望计数 $\\mu_{gj}$ 与实验变量联系起来：\n$$\\log(\\mu_{gj}) = \\log(s_{j}) + \\beta_{g0} + \\beta_{g1} x_{j}$$\n这里，$\\log(s_j)$ 是一个偏移项，用于解释文库大小归一化。$\\mathbf{X}$ 是设计矩阵，其中一列是代表截距项的全1列，另一列是二元条件指示变量 $x_j \\in \\{0, 1\\}$。系数 $\\beta_{g0}$ 和 $\\beta_{g1}$ 分别代表对数基线丰度和条件间的对数倍数变化。\n\n我们使用迭代重加权最小二乘法（IRLS）来估计系数 $\\beta_g = [\\beta_{g0}, \\beta_{g1}]^T$，对于此GLM，这等同于Fisher评分法。迭代更新公式为：\n$$\\beta_g^{(t+1)} = (\\mathbf{X}^{\\top} \\mathbf{W}^{(t)} \\mathbf{X})^{-1} \\mathbf{X}^{\\top} \\mathbf{W}^{(t)} \\mathbf{z}^{(t)}$$\n其中，在第 $t$ 次迭代时：\n- $\\eta_{gj} = \\log(s_j) + \\beta_{g0}^{(t)} + \\beta_{g1}^{(t)} x_{j}$ 是线性预测器。\n- $\\mu_{gj} = \\exp(\\eta_{gj})$ 是估计的均值。\n- $w_{gj} = \\frac{\\mu_{gj}}{1 + \\hat{\\alpha}\\mu_{gj}}$ 是权重矩阵 $\\mathbf{W}$ 的对角元素。\n- $z_{gj} = \\eta_{gj} + \\frac{k_{gj} - \\mu_{gj}}{\\mu_{gj}}$ 是工作响应。\n迭代过程持续进行，直到 $\\beta_g$ 的变化小于容忍阈值或达到最大迭代次数。\n\n**5. 假设检验与多重检验校正**\n我们的主要兴趣是条件是否具有显著影响，即 $\\beta_{g1} \\neq 0$。为此，我们使用Wald检验。检验统计量为：\n$$Z_{g} = \\frac{\\hat{\\beta}_{g1}}{\\operatorname{se}(\\hat{\\beta}_{g1})}$$\n其中 $\\hat{\\beta}_{g1}$ 是从收敛的IRLS得到的系数估计值，其标准误 $\\operatorname{se}(\\hat{\\beta}_{g1})$ 由估计量的渐近协方差矩阵 $\\widehat{\\text{Cov}}(\\hat{\\beta}_g) = (\\mathbf{X}^{\\top} \\mathbf{W}_{\\text{final}} \\mathbf{X})^{-1}$ 导出。具体来说，$\\operatorname{se}(\\hat{\\beta}_{g1})$ 是该矩阵中对应于 $\\beta_{g1}$ 的对角元素的平方根。在零假设 $H_0: \\beta_{g1} = 0$ 下，统计量 $Z_g$ 服从标准正态分布。这使得可以计算双边 $p$ 值。\n\n由于我们同时对数千个基因执行此检验，我们面临多重检验问题。为了控制假发现的比例，我们使用Benjamini-Hochberg (BH) 程序调整 $p$ 值，从而得到 $q$ 值（FDR校正后的 $p$ 值）。\n\n**6. 条件特异性必需基因的识别**\n一个基因被识别为“相对于条件0，在条件1中是条件特异性必需的”，如果它满足两个标准：\n1.  **统计显著性**：校正后的 $q$ 值必须低于指定阈值，$q_g < \\alpha_{\\mathrm{FDR}}$（例如0.1）。\n2.  **效应量**：基因必须在条件1中被耗尽。我们使用log2倍数变化 $\\widehat{\\mathrm{LFC}}_{g} = \\hat{\\beta}_{g1} / \\log 2$ 来量化这一点。标准是 $\\widehat{\\mathrm{LFC}}_{g} \\le -\\tau$，其中 $\\tau$ 是一个非负的幅度阈值（例如1.0，对应于丰度至少减半）。\n\n在所有样本中计数为零或无法可靠拟合模型的基因将被排除在分析之外。最终输出是满足这两个标准的基因索引的排序列表。", "answer": "```python\nimport numpy as np\nfrom scipy.stats import norm\n\ndef solve():\n    \"\"\"\n    Main function to run the differential analysis pipeline on all test cases.\n    \"\"\"\n\n    class DifferentialAnalysis:\n        \"\"\"\n        Implements the complete differential analysis pipeline for CRISPR screen count data.\n        \"\"\"\n        \n        def __init__(self, counts, conditions, alpha_fdr, tau):\n            self.raw_counts = np.asarray(counts, dtype=float)\n            self.conditions = np.asarray(conditions, dtype=float)\n            self.alpha_fdr = alpha_fdr\n            self.tau_lfc = tau\n            self.num_genes, self.num_samples = self.raw_counts.shape\n            self.log2 = np.log(2)\n            self.irls_iterations = 25\n            self.irls_tol = 1e-6\n\n        def run(self):\n            \"\"\"\n            Executes the full analysis pipeline.\n            \"\"\"\n            # Step 0: Filter genes that are untestable from the start\n            genes_with_counts = self.raw_counts.sum(axis=1) > 0\n            if not np.any(genes_with_counts):\n                return []\n\n            # Step 1: Size-factor normalization\n            size_factors = self._calculate_size_factors()\n            if size_factors is None: # Happens if no genes for geo mean calc\n                return []\n            \n            normalized_counts = self.raw_counts / size_factors\n            mean_normalized_counts = np.mean(normalized_counts, axis=1)\n            \n            # Further filter genes where mean normalized count is zero\n            testable_genes_mask = genes_with_counts  (mean_normalized_counts > 0)\n            testable_gene_indices = np.where(testable_genes_mask)[0]\n            if len(testable_gene_indices) == 0:\n                return []\n\n            # Step 2: Common dispersion estimation\n            common_dispersion = self._estimate_common_dispersion(size_factors, testable_genes_mask)\n\n            # Step 3  4: Per-gene GLM fit and Wald test\n            design_matrix = np.vstack([np.ones(self.num_samples), self.conditions]).T\n            \n            p_values = []\n            betas = []\n            final_testable_indices = []\n\n            for i in testable_gene_indices:\n                counts_g = self.raw_counts[i, :]\n                try:\n                    beta, cov_matrix = self._fit_nb_glm(counts_g, design_matrix, size_factors, common_dispersion)\n                    \n                    beta1 = beta[1]\n                    se_beta1 = np.sqrt(cov_matrix[1, 1])\n                    \n                    if np.isinf(se_beta1) or se_beta1  1e-8:\n                        p_val = 1.0\n                    else:\n                        wald_stat = beta1 / se_beta1\n                        p_val = 2 * (1 - norm.cdf(np.abs(wald_stat)))\n                    \n                    p_values.append(p_val)\n                    betas.append(beta1)\n                    final_testable_indices.append(i)\n                except (np.linalg.LinAlgError, ValueError):\n                    continue\n\n            # Step 5: Benjamini-Hochberg FDR correction\n            if not p_values:\n                return []\n            q_values = self._benjamini_hochberg(p_values)\n\n            # Step 6: Identify condition-specific essential genes\n            significant_genes = []\n            for gene_idx, beta1, q_val in zip(final_testable_indices, betas, q_values):\n                lfc = beta1 / self.log2\n                if q_val  self.alpha_fdr and lfc = -self.tau_lfc:\n                    significant_genes.append(gene_idx)\n            \n            return sorted(significant_genes)\n\n        def _calculate_size_factors(self):\n            genes_for_geo_mean_mask = np.all(self.raw_counts > 0, axis=1)\n            \n            if not np.any(genes_for_geo_mean_mask):\n                return np.ones(self.num_samples) # Fallback if no gene is viable\n\n            counts_subset = self.raw_counts[genes_for_geo_mean_mask, :]\n            geo_means = np.exp(np.mean(np.log(counts_subset), axis=1))\n            \n            ratios = counts_subset / geo_means[:, np.newaxis]\n            size_factors_uncentered = np.median(ratios, axis=0)\n            \n            if np.any(size_factors_uncentered = 0): # Avoid log(0) or log(-)\n                return size_factors_uncentered / np.mean(size_factors_uncentered)\n\n            geo_mean_sf = np.exp(np.mean(np.log(size_factors_uncentered)))\n            size_factors = size_factors_uncentered / geo_mean_sf\n            return size_factors\n\n        def _estimate_common_dispersion(self, size_factors, valid_genes_mask):\n            counts_subset = self.raw_counts[valid_genes_mask, :]\n            if counts_subset.shape[0] == 0:\n                return 0.0\n\n            normalized_counts = counts_subset / size_factors\n            mean_y = np.mean(normalized_counts, axis=1)\n            var_y = np.var(normalized_counts, axis=1, ddof=1)\n            \n            mean_y_sq = mean_y**2\n            # Add a small epsilon to avoid division by zero\n            mean_y_sq[mean_y_sq == 0] = 1e-8\n\n            dispersions_g = (var_y - mean_y) / mean_y_sq\n            dispersions_g[dispersions_g  0] = 0\n            \n            return np.median(dispersions_g)\n\n        def _fit_nb_glm(self, counts_g, X, s, alpha):\n            mean_norm_count = np.mean(counts_g / s)\n            if mean_norm_count = 0:\n                raise ValueError(\"Mean normalized count is zero or negative.\")\n            \n            beta = np.array([np.log(mean_norm_count), 0.0])\n\n            for _ in range(self.irls_iterations):\n                beta_old = beta.copy()\n                \n                eta = X @ beta + np.log(s)\n                mu = np.exp(eta)\n                \n                weights = mu / (1.0 + alpha * mu)\n                if np.sum(weights)  1e-8:\n                    raise np.linalg.LinAlgError(\"All weights are near zero.\")\n                \n                W = np.diag(weights)\n                z = eta + (counts_g - mu) / mu\n                \n                XT_W_X = X.T @ W @ X\n                XT_W_X_inv = np.linalg.inv(XT_W_X)\n                beta = XT_W_X_inv @ X.T @ W @ z\n                \n                if np.sum(np.abs(beta - beta_old)) / (np.sum(np.abs(beta_old)) + 1e-8)  self.irls_tol:\n                    break\n            \n            # Recalculate final covariance matrix with converged beta\n            eta = X @ beta + np.log(s)\n            mu = np.exp(eta)\n            weights = mu / (1.0 + alpha * mu)\n            W = np.diag(weights)\n            XT_W_X = X.T @ W @ X\n            cov_matrix = np.linalg.inv(XT_W_X)\n            \n            return beta, cov_matrix\n\n        def _benjamini_hochberg(self, p_values):\n            p_values = np.asarray(p_values)\n            num_tests = len(p_values)\n            \n            sorted_indices = np.argsort(p_values)\n            sorted_p_values = p_values[sorted_indices]\n            \n            ranks = np.arange(1, num_tests + 1)\n            q_values_sorted = sorted_p_values * num_tests / ranks\n            \n            q_values_sorted = np.minimum.accumulate(q_values_sorted[::-1])[::-1]\n            \n            q_values = np.empty_like(p_values)\n            q_values[sorted_indices] = q_values_sorted\n            return q_values\n\n    # Test suite from the problem statement\n    test_cases = [\n        # Case 1\n        (\n            [[100, 90, 110, 120, 80, 100], [50, 45, 55, 60, 40, 50], [30, 27, 33, 9, 6, 8],\n             [20, 18, 22, 24, 16, 20], [10, 9, 11, 3, 2, 3], [60, 54, 66, 72, 48, 60]],\n            [0, 0, 0, 1, 1, 1]\n        ),\n        # Case 2\n        (\n            [[160, 40, 120, 56], [80, 20, 12, 6], [40, 10, 30, 14],\n             [30, 7, 23, 10], [10, 3, 8, 4]],\n            [0, 0, 1, 1]\n        ),\n        # Case 3\n        (\n            [[2, 1, 2, 1], [0, 0, 0, 0], [3, 2, 3, 2], [1, 0, 1, 0],\n             [4, 4, 4, 4], [6, 5, 1, 1]],\n            [0, 0, 1, 1]\n        )\n    ]\n    \n    alpha_fdr = 0.1\n    tau = 1.0\n    all_results = []\n\n    for counts, conditions in test_cases:\n        analyzer = DifferentialAnalysis(counts, conditions, alpha_fdr, tau)\n        result = analyzer.run()\n        all_results.append(result)\n\n    # Format the final output exactly as required, handling spaces in list string representation.\n    # The default str() includes spaces, which is consistent with the problem's example format: [[i_1, i_2], [j_1], [k_1]]\n    output_str = ','.join(map(str, all_results))\n    print(f\"[{output_str}]\")\n\nsolve()\n```", "id": "2371981"}, {"introduction": "虽然标准的分析流程在理想情况下表现良好，但真实的生物学过程往往更为复杂。本练习将探讨一个具有挑战性的场景：一个基因的多个亚型（isoforms）具有相反的功能，这可能会使简单的分析方法失效。通过构建一个模拟环境并比较两种不同的“命中”基因识别算法，您将深入了解不同算法的稳健性，并体会到在非理想条件下评估分析方法的重要性[@problem_id:2372047]。", "problem": "您的任务是构建一个完全指定的模拟，用于分析基于成簇规律间隔短回文重复序列 (CRISPR) 的混合扰动筛选，以评估当单个基因具有两种功能相反的异构体并被不同向导核糖核酸 (gRNA) 靶向时，两种不同的命中基因识别算法的表现。您必须实现一个遵循以下数学规范的程序，运行一组指定的测试用例，并输出一个单行整数列表，该列表编码了两种算法在每个测试用例上的相对正确性。\n\n考虑一个包含 $G$ 个基因和每个基因 $K$ 个向导的混合筛选。有一个特殊的基因，表示为 $g^\\star$，它具有两种功能相反的异构体。每个向导在选择前（时间 $0$）和选择后（时间 $1$）进行测序。对于基因 $j$ 中的每个向导 $i$，其选择前计数 $X_{0,ji}$ 和选择后计数 $X_{1,ji}$ 是从负二项分布中独立抽取的样本，该分布具有指定的均值和共同的离散度参数。所有随机抽样都必须通过使用每个测试用例提供的种子来确定性地执行，以初始化随机数生成器。\n\n定义以下参数，除非另有说明，这些参数在所有测试用例中都是固定的：\n\n- 基因总数 $G = 200$。\n- 每个基因的向导数 $K = 6$。\n- 特殊基因索引 $g^\\star = 0$（零基索引）。\n- 每个向导的基线选择前平均计数 $\\mu_0 = 500$。\n- 共同的负二项离散度（大小参数）$r = 50$。\n- 算法M的单边显著性水平 $\\alpha = 0.05$。\n- 算法B的贝叶斯因子阈值 $T = 10$。\n- 算法B的备择效应均值幅度参数 $\\beta = 0.6$。\n- 所有对数运算必须使用自然对数。\n\n负二项分布的参数化和抽样：对于期望的均值 $\\mu$ 和离散度（大小）$r$，令 $p = \\frac{r}{r+\\mu}$。那么，一次抽样 $Y \\sim \\mathrm{NB}(r,p)$ 的均值为 $\\mu$，方差为 $\\mu + \\frac{\\mu^2}{r}$。所有计数抽样均使用此参数化。\n\n真实的向导级别效应：令 $\\ell_{ji}$ 表示基因 $j$ 中向导 $i$ 的真实对数倍数变化。\n\n- 对于所有基因 $j \\ne g^\\star$ 及其所有向导，设置 $\\ell_{ji} = 0$。\n- 对于特殊基因 $g^\\star$，有三类向导：\n  1. 靶向异构体A的向导，真实效应为 $+\\mu$（富集）。\n  2. 靶向异构体B的向导，真实效应为 $-\\mu$（耗竭）。\n  3. 共享外显子的向导，真实效应为 $0$。\n  分配给这三类的 $K$ 个向导的比例分别为 $p_A$、 $p_B$ 和 $p_S$，且 $p_A + p_B + p_S = 1$。要将比例转换为总和为 $K$ 的整数 $(K_A, K_B, K_S)$，首先计算 $(\\lfloor p_A K \\rfloor, \\lfloor p_B K \\rfloor, \\lfloor p_S K \\rfloor)$ 和余数 $(\\{p_A K\\}, \\{p_B K\\}, \\{p_S K\\})$。令 $R = K - (\\lfloor p_A K \\rfloor + \\lfloor p_B K \\rfloor + \\lfloor p_S K \\rfloor)$。通过将剩余的 $R$ 个向导按其余数降序分配给相应的类别（每个 $+1$）来分配；任何平局都按固定的顺序 A、B、S 来打破。将前 $K_A$ 个向导分配给 $+\\mu$，接下来的 $K_B$ 个向导分配给 $-\\mu$，剩余的 $K_S$ 个向导分配给 $0$。\n\n计数生成和观测效应：对于基因 $j$ 中的每个向导 $i$，抽取\n- $X_{0,ji} \\sim \\mathrm{NB}(r, \\frac{r}{r+\\mu_0})$，\n- $X_{1,ji} \\sim \\mathrm{NB}(r, \\frac{r}{r+\\mu_0 \\exp(\\ell_{ji})})$。\n定义观测到的向导级别对数倍数变化\n$$\nL_{ji} = \\log\\left(\\frac{X_{1,ji} + 1}{X_{0,ji} + 1}\\right).\n$$\n\n根据所有向导级别的 $L_{ji}$ 值，通过中位数绝对偏差定义一个稳健的尺度估计 $\\hat{\\sigma}$：令 $m = \\mathrm{median}(\\{L_{ji}\\})$ 和 $\\mathrm{MAD} = \\mathrm{median}(|L_{ji} - m|)$。设置 $\\hat{\\sigma} = 1.4826 \\cdot \\mathrm{MAD}$。如果 $\\hat{\\sigma} = 0$，则将其替换为 $\\{L_{ji}\\}$ 的样本标准差；如果样本标准差也为 $0$，则替换为一个小的常数 $10^{-8}$。\n\n算法M（一种带有Bonferroni校正的特定方向秩最小 $p$ 值）：\n1. 对于每个向导 $L_{ji}$，定义标准化值 $Z_{ji} = \\frac{L_{ji}}{\\hat{\\sigma}}$。\n2. 令 $\\Phi(\\cdot)$ 表示标准正态累积分布函数。将耗竭和富集的单边 $p$ 值分别定义为 $p^-_{ji} = \\Phi(Z_{ji})$ 和 $p^+_{ji} = 1 - \\Phi(Z_{ji})$。\n3. 对于每个基因 $j$，定义经Bonferroni校正的基因级别值 $P^-_j = \\min\\{1, K \\cdot \\min_i p^-_{ji}\\}$ 和 $P^+_j = \\min\\{1, K \\cdot \\min_i p^+_{ji}\\}$。\n4. 定义 $P_j = \\min(P^-_j, P^+_j)$，如果 $P^+_j  P^-_j$，则预测方向 $\\mathrm{dir}_M(j) = +1$，否则 $\\mathrm{dir}_M(j) = -1$。如果 $P^+_j = P^-_j$，则通过设置 $\\mathrm{dir}_M(j) = +1$ 来打破平局。\n5. 如果 $P_j \\le \\alpha$，算法M将基因 $j$ 判定为命中基因；否则，不判定为命中。\n\n算法B（使用固定备择假设的高斯似然比的双边贝叶斯因子）：\n1. 设 $L_{ji}$ 的零分布为 $\\mathcal{N}(0, \\tau^2)$，其中 $\\tau = \\hat{\\sigma}$。\n2. 定义两个备择分布：耗竭 $\\mathcal{N}(-\\beta, \\tau^2)$ 和富集 $\\mathcal{N}(+\\beta, \\tau^2)$。\n3. 对于基因 $j$，计算对数贝叶斯因子\n$$\n\\log \\mathrm{BF}^-(j) = \\sum_{i=1}^K \\left[ \\log \\phi(L_{ji}; -\\beta, \\tau) - \\log \\phi(L_{ji}; 0, \\tau) \\right],\n$$\n$$\n\\log \\mathrm{BF}^+(j) = \\sum_{i=1}^K \\left[ \\log \\phi(L_{ji}; +\\beta, \\tau) - \\log \\phi(L_{ji}; 0, \\tau) \\right],\n$$\n其中 $\\phi(x; \\mu, \\tau)$ 是均值为 $\\mu$、标准差为 $\\tau$ 的高斯概率密度函数。令 $\\log \\mathrm{BF}_{\\max}(j) = \\max(\\log \\mathrm{BF}^-(j), \\log \\mathrm{BF}^+(j))$。如果 $\\log \\mathrm{BF}^+(j)  \\log \\mathrm{BF}^-(j)$，则预测方向为 $\\mathrm{dir}_B(j) = +1$，否则为 $\\mathrm{dir}_B(j) = -1$。如果相等，则通过设置 $\\mathrm{dir}_B(j) = +1$ 来打破平局。\n4. 如果 $\\log \\mathrm{BF}_{\\max}(j) \\ge \\log T$，算法B将基因 $j$ 判定为命中基因；否则，不判定为命中。\n\n特殊基因 $g^\\star$ 的真实方向仅由设计比例决定：如果 $p_A  p_B$，定义 $L^\\star = +1$；如果 $p_B  p_A$，定义 $L^\\star = -1$；如果 $p_A = p_B$，定义 $L^\\star = 0$。一个算法对 $g^\\star$ 的判定被认为是正确的，其标准如下：如果 $L^\\star \\in \\{+1, -1\\}$，当且仅当该算法判定其为命中基因且其预测方向等于 $L^\\star$ 时，该算法才是正确的；如果 $L^\\star = 0$，当且仅当该算法不判定其为命中基因时，该算法才是正确的。\n\n对于每个测试用例，计算算法M和算法B在特殊基因 $g^\\star$ 上的两个正确性指标 $C_M$ 和 $C_B$（每个指标为 $0$ 或 $1$）。将它们映射为单个整数结果 $R = 2 \\cdot C_M + C_B$，其取值范围为 $\\{0,1,2,3\\}$，分别对应于两者都不正确、仅算法B正确、仅算法M正确或两者都正确。\n\n测试套件：对于每个用例，您将获得一个元组 $(\\mathrm{seed}, \\mu, p_A, p_B, p_S)$，其中 $p_A + p_B + p_S = 1$。使用以下四个测试用例：\n1. $(42, 0.8, 0.8, 0.2, 0.0)$，\n2. $(43, 0.8, 0.25, 0.75, 0.0)$，\n3. $(44, 0.8, 0.5, 0.5, 0.0)$，\n4. $(45, 1.0, 0.1, 0.1, 0.8)$。\n\n最终输出格式：您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表（例如，$[r_1,r_2,r_3,r_4]$），其中每个 $r_i$ 是对应测试用例的整数 $R$，顺序与上面列出的一致。不应打印任何其他文本。不涉及物理单位或角度，任何分数必须在输入参数中表示为小数。所有计算都必须按规定执行，并能根据给定的种子进行复现。", "solution": "该问题要求为基于成簇规律间隔短回文重复序列 (CRISPR) 的混合扰动筛选构建并执行一个详细的模拟。其目标是评估两种不同的命中基因识别算法（指定为算法M和算法B）在一个特定情景下的性能。该情景涉及一个单一的基因 $g^\\star$，它拥有两个具有拮抗生物学效应（富集和耗竭）的异构体。该问题被判定为有效，因为它在科学上植根于计算生物学和生物信息学的原理，在数学和算法上是适定的，并为模拟提供了完整且明确的规范。\n\n模拟过程的结构如下。我们首先定义筛选的参数和数据生成的统计模型。然后，我们描述两种分析算法的实现。最后，我们建立评估每种算法对特殊基因 $g^\\star$ 进行分类的正确性的标准。\n\n**1. 模拟框架**\n\n模拟的筛选包含 $G = 200$ 个基因，每个基因由 $K = 6$ 个向导RNA (gRNA) 靶向。一个索引为 $g^\\star = 0$ 的特定基因，被建模为具有两个效应相反的异构体。对于基因 $j$ 中的每个向导 $i$，我们模拟来自筛选前文库 ($T_0$) 和筛选后文库 ($T_1$) 的读数。这些计数表示为 $X_{0,ji}$ 和 $X_{1,ji}$，是从负二项分布 $\\mathrm{NB}(r, p)$ 中生成的，这是为高通量测序中过度离散的计数数据建模的标准选择。该分布由其离散度（大小）参数 $r = 50$ 和一个概率参数 $p$ 来参数化。对于期望的平均计数 $\\mu$，概率设置为 $p = \\frac{r}{r+\\mu}$。筛选前文库中一个向导的基线平均计数为 $\\mu_0 = 500$。\n\n向导的真实生物学效应由其对数倍数变化 $\\ell_{ji}$ 表示。对于所有靶向非特殊基因 ($j \\ne g^\\star$) 的向导，其效应为空，即 $\\ell_{ji} = 0$。对于特殊基因 $g^\\star$，向导被分为三类：\n- $K_A$ 个靶向异构体A的向导，真实效应为 $+\\mu$（富集）。\n- $K_B$ 个靶向异构体B的向导，真实效应为 $-\\mu$（耗竭）。\n- $K_S$ 个靶向共享区域的向导，真实效应为 $0$。\n\n计数 $K_A$、$K_B$ 和 $K_S$ 由指定的比例 $(p_A, p_B, p_S)$ 决定，使得 $p_A + p_B + p_S = 1$。整数计数通过先对每个类别的 $p \\cdot K$ 取底，然后将剩余的 $R = K - (\\lfloor p_A K \\rfloor + \\lfloor p_B K \\rfloor + \\lfloor p_S K \\rfloor)$ 个向导逐一分配给具有最大小数部分的类别来确定性地计算，平局按A、B、S的顺序打破。\n\n在定义了真实效应 $\\ell_{ji}$ 后，筛选后计数的均值为 $\\mu_0 \\exp(\\ell_{ji})$。因此，计数生成如下：\n$$\nX_{0,ji} \\sim \\mathrm{NB}\\left(r, \\frac{r}{r+\\mu_0}\\right)\n$$\n$$\nX_{1,ji} \\sim \\mathrm{NB}\\left(r, \\frac{r}{r+\\mu_0 \\exp(\\ell_{ji})}\\right)\n$$\n所有随机抽样均使用每个用例的种子确定性地为随机数生成器进行初始化。根据这些计数，计算观测到的向导级别对数倍数变化 $L_{ji}$，其中包含一个伪计数 $1$ 以处理零计数：\n$$\nL_{ji} = \\log\\left(\\frac{X_{1,ji} + 1}{X_{0,ji} + 1}\\right)\n$$\n其中 $\\log$ 表示自然对数。\n\n使用中位数绝对偏差 (MAD) 从所有观测到的 $L_{ji}$ 值集合中计算出零对数倍数变化的稳健标准差估计值 $\\hat{\\sigma}$： $\\hat{\\sigma} = 1.4826 \\cdot \\mathrm{median}(|L_{ji} - \\mathrm{median}(\\{L\\cdot\\})|)$。如果 $\\hat{\\sigma}$ 为零，则使用指定的备用值。\n\n**2. 命中基因识别算法**\n\n应用两种算法到数据上以识别命中基因。\n\n**算法M（修正的秩最小P值）：**\n该算法基于识别每个基因的最极端的单个向导效应，并应用Bonferroni校正。\n1. 对每个向导，计算一个标准化的Z分数：$Z_{ji} = L_{ji} / \\hat{\\sigma}$。\n2. 使用标准正态累积分布函数 $\\Phi(\\cdot)$ 计算耗竭和富集的单边p值：\n   $$ p^-_{ji} = \\Phi(Z_{ji}) \\quad \\text{和} \\quad p^+_{ji} = 1 - \\Phi(Z_{ji}) $$\n3. 通过取每个方向上最小的向导级别p值，并对 $K$ 个向导应用Bonferroni校正，来获得基因级别的p值：\n   $$ P^-_j = \\min\\left(1, K \\cdot \\min_i p^-_{ji}\\right) \\quad \\text{和} \\quad P^+_j = \\min\\left(1, K \\cdot \\min_i p^+_{ji}\\right) $$\n4. 最终的基因p值为 $P_j = \\min(P^-_j, P^+_j)$。如果 $P^+_j  P^-_j$，预测方向 $\\mathrm{dir}_M(j)$ 为 $+1$，否则为 $-1$（平局规则为 $+1$）。\n5. 如果 $P_j \\le \\alpha$，则判定一个基因为命中基因，其中显著性水平为 $\\alpha = 0.05$。\n\n**算法B（基于贝叶斯因子）：**\n该算法使用贝叶斯框架来比较一个零假设与两个具有固定效应大小的备择假设。\n1. 对于非命中基因，观测到的对数倍数变化 $L_{ji}$ 假定遵循一个零分布 $\\mathcal{N}(0, \\hat{\\sigma}^2)$。\n2. 定义两个备择分布：耗竭为 $\\mathcal{N}(-\\beta, \\hat{\\sigma}^2)$，富集为 $\\mathcal{N}(+\\beta, \\hat{\\sigma}^2)$，具有固定的效应幅度 $\\beta = 0.6$。\n3. 对于每个基因 $j$，计算对数贝叶斯因子以比较备择假设与零假设。对于单个向导 $L_{ji}$，在正向备择假设与零假设下的对数似然比简化为 $\\frac{L_{ji}\\beta}{\\hat{\\sigma}^2} - \\frac{\\beta^2}{2\\hat{\\sigma}^2}$。对基因 $j$ 的所有向导求和：\n   $$ \\log \\mathrm{BF}^+(j) = \\sum_{i=1}^K \\left( \\frac{L_{ji}\\beta}{\\hat{\\sigma}^2} - \\frac{\\beta^2}{2\\hat{\\sigma}^2} \\right) = \\frac{\\beta}{\\hat{\\sigma}^2} \\sum_{i=1}^K L_{ji} - \\frac{K\\beta^2}{2\\hat{\\sigma}^2} $$\n   同样，对于负向备择假设：\n   $$ \\log \\mathrm{BF}^-(j) = \\sum_{i=1}^K \\left( -\\frac{L_{ji}\\beta}{\\hat{\\sigma}^2} - \\frac{\\beta^2}{2\\hat{\\sigma}^2} \\right) = -\\frac{\\beta}{\\hat{\\sigma}^2} \\sum_{i=1}^K L_{ji} - \\frac{K\\beta^2}{2\\hat{\\sigma}^2} $$\n4. 最大对数贝叶斯因子为 $\\log \\mathrm{BF}_{\\max}(j) = \\max(\\log \\mathrm{BF}^-(j), \\log \\mathrm{BF}^+(j))$。如果 $\\log \\mathrm{BF}^+(j)  \\log \\mathrm{BF}^-(j)$，预测方向 $\\mathrm{dir}_B(j)$ 为 $+1$，否则为 $-1$（平局规则为 $+1$）。\n5. 如果证据足够强，即 $\\log \\mathrm{BF}_{\\max}(j) \\ge \\log T$，则判定一个基因为命中基因，其中贝叶斯因子阈值为 $T = 10$。\n\n**3. 评估与输出**\n\n每种算法的性能仅在特殊基因 $g^\\star$ 上进行评估。该基因的真实方向 $L^\\star$ 由输入设计定义：如果 $p_A  p_B$，则 $L^\\star = +1$；如果 $p_B  p_A$，则 $L^\\star = -1$；如果 $p_A = p_B$，则 $L^\\star = 0$。\n- 如果 $L^\\star \\in \\{+1, -1\\}$，一个算法当且仅当它将 $g^\\star$ 判定为命中基因且其预测方向与 $L^\\star$ 匹配时，才是正确的。\n- 如果 $L^\\star = 0$，一个算法当且仅当它不将 $g^\\star$ 判定为命中基因时，才是正确的。\n\n对于每个测试用例，我们分别为算法M和B计算二元正确性指标 $C_M$ 和 $C_B$。这些指标被组合成一个单一的整数结果 $R = 2 \\cdot C_M + C_B$，该结果唯一地编码了两种算法的结果。程序将为每个提供的测试用例执行模拟，并输出这些整数结果的列表。", "answer": "```python\nimport numpy as np\nfrom scipy.stats import nbinom, norm\n\ndef solve():\n    \"\"\"\n    Runs the full CRISPR screen simulation and analysis for a suite of test cases.\n    \"\"\"\n\n    # --- Fixed Parameters ---\n    G = 200  # Total number of genes\n    K = 6  # Guides per gene\n    g_star_idx = 0  # Special gene index\n    mu_0 = 500.0  # Baseline pre-selection mean count\n    r = 50.0  # Common Negative Binomial dispersion (size)\n    alpha = 0.05  # Significance level for Algorithm M\n    T = 10.0  # Bayes factor threshold for Algorithm B\n    beta = 0.6  # Alternative effect size for Algorithm B\n\n    # --- Test Cases (seed, mu, p_A, p_B, p_S) ---\n    test_cases = [\n        (42, 0.8, 0.8, 0.2, 0.0),\n        (43, 0.8, 0.25, 0.75, 0.0),\n        (44, 0.8, 0.5, 0.5, 0.0),\n        (45, 1.0, 0.1, 0.1, 0.8),\n    ]\n\n    results = []\n\n    for seed, mu_effect, p_A, p_B, p_S in test_cases:\n        rng = np.random.default_rng(seed)\n\n        # --- 1. Determine Guide Counts for Special Gene ---\n        fractions = {'A': p_A, 'B': p_B, 'S': p_S}\n        base_counts = {cat: int(frac * K) for cat, frac in fractions.items()}\n        remainders = {cat: frac * K - base_counts[cat] for cat, frac in fractions.items()}\n        \n        # Sort categories by remainder descending, with tie-break A, B, S\n        sorted_cats = sorted(fractions.keys(), key=lambda c: (-remainders[c], ['A', 'B', 'S'].index(c)))\n        \n        R = K - sum(base_counts.values())\n        final_counts = base_counts.copy()\n        for i in range(R):\n            final_counts[sorted_cats[i]] += 1\n        \n        K_A, K_B, K_S = final_counts['A'], final_counts['B'], final_counts['S']\n\n        # --- 2. Generate True Log Fold Changes (l_ji) ---\n        l_ji = np.zeros((G, K))\n        effects = np.concatenate([\n            np.full(K_A, mu_effect),\n            np.full(K_B, -mu_effect),\n            np.full(K_S, 0.0)\n        ])\n        l_ji[g_star_idx, :] = effects\n\n        # --- 3. Generate Counts and Observed LFCs ---\n        p0 = r / (r + mu_0)\n        X0 = nbinom.rvs(n=r, p=p0, size=(G, K), random_state=rng)\n        \n        mu1 = mu_0 * np.exp(l_ji)\n        p1 = r / (r + mu1)\n        X1 = nbinom.rvs(n=r, p=p1, size=(G, K), random_state=rng)\n\n        L_ji = np.log((X1 + 1) / (X0 + 1))\n\n        # --- 4. Estimate Robust Scale (sigma_hat) ---\n        median_lfc = np.median(L_ji)\n        mad = np.median(np.abs(L_ji - median_lfc))\n        sigma_hat = 1.4826 * mad\n        if sigma_hat == 0.0:\n            sigma_hat = np.std(L_ji)\n            if sigma_hat == 0.0:\n                sigma_hat = 1e-8\n\n        # --- 5. Run Algorithms on Special Gene g* ---\n        L_star = L_ji[g_star_idx, :]\n        \n        # --- Algorithm M ---\n        Z_star = L_star / sigma_hat\n        p_minus_guides = norm.cdf(Z_star)\n        p_plus_guides = 1.0 - p_minus_guides\n        \n        P_minus_star = min(1.0, K * np.min(p_minus_guides))\n        P_plus_star = min(1.0, K * np.min(p_plus_guides))\n        \n        P_star_M = min(P_minus_star, P_plus_star)\n        \n        # Tie-break: dir=+1 if P+ = P-\n        dir_M = 1 if P_plus_star = P_minus_star else -1\n        hit_M = P_star_M = alpha\n\n        # --- Algorithm B ---\n        sum_L_star = np.sum(L_star)\n        log_BF_plus = (beta / sigma_hat**2) * sum_L_star - (K * beta**2) / (2 * sigma_hat**2)\n        log_BF_minus = (-beta / sigma_hat**2) * sum_L_star - (K * beta**2) / (2 * sigma_hat**2)\n        \n        log_BF_max = max(log_BF_plus, log_BF_minus)\n\n        # Tie-break: dir=+1 if BF+ >= BF- (or equal)\n        dir_B = 1 if log_BF_plus >= log_BF_minus else -1\n        hit_B = log_BF_max >= np.log(T)\n\n        # --- 6. Evaluate Correctness ---\n        if p_A > p_B:\n            L_star_truth = 1\n        elif p_B > p_A:\n            L_star_truth = -1\n        else:\n            L_star_truth = 0\n\n        C_M = 0\n        if L_star_truth in [1, -1]:\n            if hit_M and dir_M == L_star_truth:\n                C_M = 1\n        elif L_star_truth == 0:\n            if not hit_M:\n                C_M = 1\n\n        C_B = 0\n        if L_star_truth in [1, -1]:\n            if hit_B and dir_B == L_star_truth:\n                C_B = 1\n        elif L_star_truth == 0:\n            if not hit_B:\n                C_B = 1\n        \n        # --- 7. Compute Final Result ---\n        R = 2 * C_M + C_B\n        results.append(R)\n\n    # --- Print Final Output ---\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2372047"}, {"introduction": "任何CRISPR筛选分析的关键一步是确保观察到的表型是由预期的“靶向”效应引起的。本练习着重解决“脱靶”效应问题，即gRNA可能通过结合到基因组中的非预期位点来影响细胞行为。您将开发一种算法，系统性地搜索在具有相似意外表型的gRNA群体中富集的序列基序（motif），这是识别潜在脱靶信号的有力方法[@problem_id:2372025]。", "problem": "给定一个玩具数据集，该数据集代表一个基于聚类定期间隔短回文重复序列 (Clustered Regularly Interspaced Short Palindromic Repeats, CRISPR) 的扰动筛选。该数据集由指导序列以及每个指导序列关联的一个实值表型组成。目标是定义并实现一个决策程序，用以检测表现出异常相似表型的指导序列之间是否存在共享的序列基序。该程序必须纯粹用以下数学术语来定义。\n\n设有 $N$ 个指导序列，索引为 $i \\in \\{1,\\dots,N\\}$。每个指导序列具有：\n- 一个核苷酸序列 $s_i$，其字母表为 $\\{ \\text{A}, \\text{C}, \\text{G}, \\text{T} \\}$。\n- 一个实值表型 $y_i \\in \\mathbb{R}$。\n\n给定一个相似性阈值 $\\tau  0$ 和一个邻居度阈值 $t \\in \\mathbb{Z}_{\\ge 0}$，定义异常相似的指导序列集合\n$$\nS \\;=\\; \\left\\{ i \\in \\{1,\\dots,N\\} \\;:\\; \\left|\\left\\{ j \\in \\{1,\\dots,N\\}\\setminus\\{i\\} \\;:\\; |y_i - y_j| \\le \\tau \\right\\}\\right| \\;\\ge\\; t \\right\\}.\n$$\n令 $n_S = |S|$。对于一个基序长度 $k \\in \\mathbb{Z}_{\\ge 1}$，一个 $k$-聚体基序 $m$ 是字母表 $\\{\\text{A},\\text{C},\\text{G},\\text{T}\\}$ 上任意一个长度为 $k$ 的字符串。如果 $m$ 作为连续子串出现在 $s_i$ 中（出现可以重叠），则称指导序列 $i$ 包含基序 $m$。对于一个基序 $m$，定义指示符\n$$\nI_i(m) \\;=\\; \\begin{cases}\n1  \\text{if $m$ is a substring of $s_i$,}\\\\\n0  \\text{otherwise.}\n\\end{cases}\n$$\n定义\n$$\nK(m) \\;=\\; \\sum_{i=1}^{N} I_i(m), \\qquad x(m) \\;=\\; \\sum_{i \\in S} I_i(m).\n$$\n将 $x(m)$ 解释为集合 $S$ 中包含 $m$ 的指导序列数量，将 $K(m)$ 解释为所有指导序列中包含 $m$ 的总数量。考虑一个无效模型，其中集合 $S$ 中的 $n_S$ 个指导序列是从全部 $N$ 个指导序列中无放回均匀抽取的。在此无效模型下，$X \\sim \\text{Hypergeometric}(N, K(m), n_S)$ 是一个大小为 $n_S$ 的随机子集中包含该基序的指导序列数量。定义单侧富集 $p$ 值为\n$$\np(m) \\;=\\; \\mathbb{P}\\left[ X \\ge x(m) \\right] \\;=\\; \\sum_{r = x(m)}^{\\min\\{K(m),\\, n_S\\}} \\frac{\\binom{K(m)}{r} \\binom{N - K(m)}{n_S - r}}{\\binom{N}{n_S}}.\n$$\n令 $M$ 为一个非空的、由允许的基序长度组成的有限集合。令 $\\mathcal{C}$ 为候选基序集合，定义为\n$$\n\\mathcal{C} \\;=\\; \\bigcup_{k \\in M} \\left\\{ m \\in \\{\\text{A},\\text{C},\\text{G},\\text{T}\\}^k \\;:\\; K(m) \\ge 1 \\right\\}.\n$$\n选择一个唯一的“最佳”基序\n$$\nm^\\star \\;\\in\\; \\operatorname*{arg\\,min}_{m \\in \\mathcal{C}} \\; p(m),\n$$\n并遵循以下确定性平局打破规则：如果多个基序达到相同的最小 $p(m)$ 值（在标准实数比较的精度内），则根据排序 $\\text{A}  \\text{C}  \\text{G}  \\text{T}$ 和标准字符串字典序比较，选择字典序最小的基序。\n\n使用数字映射 $\\text{A}\\mapsto 0$, $\\text{C}\\mapsto 1$, $\\text{G}\\mapsto 2$, $\\text{T}\\mapsto 3$，将任意基序 $m = m_1 m_2 \\dots m_k$ 映射为一个以 4 为基数的整数秩，如下所示\n$$\n\\mathrm{rank}(m) \\;=\\; \\sum_{\\ell=1}^{k} d(m_\\ell)\\, 4^{k-\\ell},\n$$\n其中 $d(\\text{A})=0$, $d(\\text{C})=1$, $d(\\text{G})=2$, $d(\\text{T})=3$。\n\n给定一个显著性水平 $\\alpha \\in (0,1)$，为每个测试用例报告一个三元组\n$$\n\\left[ \\;\\mathrm{rank}(m^\\star),\\; \\text{round}\\!\\left(p(m^\\star),\\, 6\\right),\\; \\mathbf{1}\\{ p(m^\\star)  \\alpha \\} \\; \\right],\n$$\n其中 $\\text{round}(\\cdot,6)$ 表示四舍五入到 6 位小数，$\\mathbf{1}\\{\\cdot\\}$ 是指示函数。\n\n您的程序必须生成单行输出，其中包含所有测试用例的结果。结果应为一个由方括号括起来的逗号分隔列表，每个测试用例的结果都表示为上述的三元组列表。\n\n测试套件规范。使用以下三个测试用例，每个用例由元组 $\\left( \\{s_i\\}_{i=1}^{N}, \\{y_i\\}_{i=1}^{N}, \\tau, t, M, \\alpha \\right)$ 定义：\n\n- 测试用例 A (正常路径):\n  - $N = 8$。\n  - 序列 $\\{s_i\\}$:\n    - $s_1 = \\text{ACGTAAACGT}$,\n    - $s_2 = \\text{TTTTCGTAAG}$,\n    - $s_3 = \\text{GGGGCGTCCC}$,\n    - $s_4 = \\text{AACGTTTTTT}$,\n    - $s_5 = \\text{AAAATTTTGG}$,\n    - $s_6 = \\text{CCCCCAAAAA}$,\n    - $s_7 = \\text{GACGTGAAAA}$,\n    - $s_8 = \\text{TAAAAAACGT}$.\n  - 表型 $\\{y_i\\}$ (无物理单位): $[\\,1.02,\\;0.98,\\;1.05,\\;1.00,\\;2.00,\\;2.10,\\;0.97,\\;1.01\\,]$。\n  - 阈值: $\\tau = 0.08$, $t = 3$。\n  - 允许的基序长度: $M = \\{3,4\\}$。\n  - 显著性水平: $\\alpha = 0.05$。\n\n- 测试用例 B (平局打破):\n  - $N = 4$。\n  - 序列 $\\{s_i\\}$:\n    - $s_1 = \\text{AAACCCGGGT}$,\n    - $s_2 = \\text{AAACCCGGGA}$,\n    - $s_3 = \\text{TTTGGGCCCA}$,\n    - $s_4 = \\text{TTTGGGCCCT}$.\n  - 表型 $\\{y_i\\}$ (无物理单位): $[\\,0.50,\\;0.49,\\;1.50,\\;1.60\\,]$。\n  - 阈值: $\\tau = 0.02$, $t = 1$。\n  - 允许的基序长度: $M = \\{3\\}$。\n  - 显著性水平: $\\alpha = 0.05$。\n\n- 测试用例 C ($S$ 为空的边缘情况):\n  - $N = 4$。\n  - 序列 $\\{s_i\\}$:\n    - $s_1 = \\text{AGTCAGTCAG}$,\n    - $s_2 = \\text{CAGTCAGTCA}$,\n    - $s_3 = \\text{GTCAGTCAGT}$,\n    - $s_4 = \\text{TCAGTCAGTC}$.\n  - 表型 $\\{y_i\\}$ (无物理单位): $[\\,0.00,\\;1.00,\\;2.00,\\;3.00\\,]$。\n  - 阈值: $\\tau = 0.05$, $t = 1$。\n  - 允许的基序长度: $M = \\{4\\}$。\n  - 显著性水平: $\\alpha = 0.05$。\n\n最终输出格式。您的程序应生成单行输出，其中包含所有结果，格式为一个由方括号括起来的逗号分隔列表。其中每一项是对应测试用例的列表 $[\\,\\mathrm{rank}(m^\\star),\\; \\text{round}(p(m^\\star),6),\\; \\mathbf{1}\\{ p(m^\\star)  \\alpha \\}\\,]$，并按 A、B、C 的顺序排列。", "solution": "该问题要求实现一个确定性程序，用于从 CRISPR 扰动筛选中识别在特定指导 RNA 子集中具有统计学显著富集的序列基序。该问题在计算上定义明确，其科学基础植根于生物信息学和统计学原理，并且所有术语和程序都经过了精确的数学定义。我的验证确认了该问题的有效性，并且每个测试用例都存在唯一解。我现在将详细阐述解决过程。\n\n对于每个测试用例，整个过程可以分解为五个主要步骤：\n1.  识别“异常相似的指导序列”集合，记为 $S$。\n2.  基于所提供的序列和允许的基序长度 $M$，生成所有候选基序的集合 $\\mathcal{C}$。\n3.  对于每个候选基序 $m \\in \\mathcal{C}$，使用超几何分布计算其富集 $p$ 值 $p(m)$。\n4.  通过找到最小化 $p(m)$ 的基序来确定“最佳”基序 $m^\\star$，并应用指定的字典序平局打破规则。\n5.  计算最终输出的三元组：$[\\mathrm{rank}(m^\\star), \\mathrm{round}(p(m^\\star), 6), \\mathbf{1}\\{p(m^\\star)  \\alpha\\}]$。\n\n让我们用所需的数学严谨性来剖析每个步骤。\n\n步骤 1：确定集合 S\n集合 $S$ 由所有至少有 $t$ 个“邻居”的指导序列 $i$ 组成，其中邻居 $j$ 是指其表型 $y_j$ 与 $y_i$“接近”的另一个指导序列。形式上，对于每个指导序列 $i \\in \\{1, \\dots, N\\}$，我们计算其邻域的大小，$c_i = |\\left\\{ j \\in \\{1,\\dots,N\\}\\setminus\\{i\\} \\;:\\; |y_i - y_j| \\le \\tau \\right\\}\\right|$。然后将集合 $S$ 定义为 $S = \\{i : c_i \\ge t\\}$。该集合的大小为 $n_S = |S|$。这是对所提供定义的直接实现。\n\n步骤 2：生成候选基序 $\\mathcal{C}$\n候选基序集合 $\\mathcal{C}$ 是所有出现在至少一个指导序列 $\\{s_i\\}_{i=1}^N$ 中、且长度在集合 $M$ 中指定的唯一子串的集合。为构建此集合，我们遍历每个允许的长度 $k \\in M$。对于每个 $k$，我们遍历所有指导序列 $s_i$。然后，我们从每个 $s_i$ 中提取所有长度为 $k$ 的子串，并将它们添加到一个主集合数据结构中以确保唯一性。接着，将得到的集合 $\\mathcal{C}$ 按字典序排序。此排序对于正确且高效地实现为选择 $m^\\star$ 而规定的平局打破规则至关重要。\n\n步骤 3：计算富集 $p$ 值\n对于每个基序 $m \\in \\mathcal{C}$，我们必须首先计算两个量：\n-   $K(m)$: 数据集中包含基序 $m$ 作为子串的指导序列总数。这通过遍历所有 $N$ 个序列并检查 $m$ 是否存在来计算。\n-   $x(m)$: 集合 $S$ 中包含基序 $m$ 的指导序列数量。这通过遍历由 $S$ 索引的指导序列并检查 $m$ 是否存在来计算。\n\n基序 $m$ 在集合 $S$ 中富集的统计显著性通过从超几何分布导出的 $p$ 值来量化。零假设是，集合 $S$ 中的 $n_S$ 个指导序列是从 $N$ 个指导序列的总群体中无放回随机抽样的。随机变量 $X$ 代表在此类随机样本中包含基序 $m$ 的指导序列数量。$X$ 服从超几何分布，$X \\sim \\text{Hypergeometric}(N, K(m), n_S)$，其参数为：\n-   总体大小：$N$ (指导序列总数)\n-   总体中成功的次数：$K(m)$ (含有基序 $m$ 的指导序列总数)\n-   样本大小：$n_S$ (集合 $S$ 的大小)\n\n单侧 $p$ 值是在样本中观察到至少 $x(m)$ 次成功的概率，由 $p(m) = \\mathbb{P}[X \\ge x(m)]$ 给出。这可以使用超几何分布的生存函数 (SF) 来计算，该函数通常定义为 $\\text{sf}(q) = \\mathbb{P}[X  q]$。因此，我们有 $p(m) = \\mathbb{P}[X \\ge x(m)] = \\text{sf}(x(m)-1)$。在计算上，这可以由 Python 中像 `scipy.stats` 这样的科学计算库中可用的函数可靠地处理。\n\n如测试用例 C 所示，一个重要的边缘情况是当集合 $S$ 为空时，即 $n_S=0$。在这种情况下，我们抽取一个大小为 0 的样本。观察到的成功次数 $x(m)$ 必须为 0。根据定义，在大小为 0 的样本中观察到至少 0 次成功的概率是 1。因此，对于任何基序 $m$，如果 $n_S=0$，则 $p(m)=1$。\n\n步骤 4：选择最佳基序 $m^\\star$\n最佳基序 $m^\\star$ 是 $\\mathcal{C}$ 中所有基序中具有最小 $p$ 值的那个。问题指定了一个严格的平局打破规则：如果多个基序产生相同的最小 $p$ 值，则选择字典序最小的那个。通过遍历步骤 2 中按字典序排序的基序列表，遇到的第一个达到最小 $p$ 值的基序保证是正确的 $m^\\star$。我们维护一个变量来记录目前找到的最小 $p$ 值 $p_{min}$ 以及相应的基序 $m^\\star$。每当找到一个 $p$ 值严格小于当前 $p_{min}$ 的基序时，就更新这些变量。\n\n步骤 5：计算和格式化最终输出\n一旦确定了 $m^\\star$ 及其关联的 $p$ 值 $p(m^\\star)$，就可以构建最终结果。\n-   $m^\\star = m_1 m_2 \\dots m_k$ 的秩被计算为一个以 4 为基数的整数。使用提供的映射 $d(\\text{A})=0, d(\\text{C})=1, d(\\text{G})=2, d(\\text{T})=3$，秩由 $\\mathrm{rank}(m^\\star) = \\sum_{\\ell=1}^{k} d(m_\\ell) 4^{k-\\ell}$ 给出。这可以使用霍纳 (Horner) 方法高效实现。\n-   $p$ 值 $p(m^\\star)$ 四舍五入到 6 位小数。\n-   计算显著性指示符 $\\mathbf{1}\\{ p(m^\\star)  \\alpha \\}$，如果条件为真，其值为 1，否则为 0。\n\n这三个值构成了该测试用例的最终输出列表。然后，按照指定格式，将所有测试用例的结果汇总到一个列表中。", "answer": "```python\nimport numpy as np\nfrom scipy.stats import hypergeom\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the final results.\n    \"\"\"\n\n    test_cases = [\n        # Test case A (happy path)\n        (\n            [ # sequences {s_i}\n                \"ACGTAAACGT\", \"TTTTCGTAAG\", \"GGGGCGTCCC\", \"AACGTTTTTT\",\n                \"AAAATTTTGG\", \"CCCCCAAAAA\", \"GACGTGAAAA\", \"TAAAAAACGT\"\n            ],\n            [1.02, 0.98, 1.05, 1.00, 2.00, 2.10, 0.97, 1.01], # phenotypes {y_i}\n            0.08, # tau\n            3,    # t\n            {3, 4}, # M\n            0.05  # alpha\n        ),\n        # Test case B (tie-breaking)\n        (\n            [ # sequences {s_i}\n                \"AAACCCGGGT\", \"AAACCCGGGA\", \"TTTGGGCCCA\", \"TTTGGGCCCT\"\n            ],\n            [0.50, 0.49, 1.50, 1.60], # phenotypes {y_i}\n            0.02, # tau\n            1,    # t\n            {3},  # M\n            0.05  # alpha\n        ),\n        # Test case C (edge case with empty S)\n        (\n            [ # sequences {s_i}\n                \"AGTCAGTCAG\", \"CAGTCAGTCA\", \"GTCAGTCAGT\", \"TCAGTCAGTC\"\n            ],\n            [0.00, 1.00, 2.00, 3.00], # phenotypes {y_i}\n            0.05, # tau\n            1,    # t\n            {4},  # M\n            0.05  # alpha\n        )\n    ]\n\n    results = []\n    for case in test_cases:\n        result = process_case(*case)\n        # Format the result list into a string representation for the final output.\n        # round() can return int if result is .0. str() will format it correctly.\n        results.append(str(result).replace(\" \", \"\"))\n\n\n    print(f\"[{','.join(results)}]\")\n\ndef rank_motif(motif: str) -> int:\n    \"\"\"\n    Maps a motif to its base-4 integer rank.\n    \"\"\"\n    d_map = {'A': 0, 'C': 1, 'G': 2, 'T': 3}\n    rank = 0\n    for char in motif:\n        rank = rank * 4 + d_map[char]\n    return rank\n\ndef process_case(sequences: list[str], phenotypes: list[float], tau: float, t: int, M: set[int], alpha: float):\n    \"\"\"\n    Processes a single test case according to the problem specification.\n    \"\"\"\n    N = len(sequences)\n\n    # Step 1: Determine the set S of unexpectedly similar guides\n    S = set()\n    for i in range(N):\n        neighbor_count = 0\n        for j in range(N):\n            if i == j:\n                continue\n            if abs(phenotypes[i] - phenotypes[j]) = tau:\n                neighbor_count += 1\n        if neighbor_count >= t:\n            S.add(i)\n    nS = len(S)\n\n    # Step 2: Generate the set of candidate motifs C\n    candidate_motifs = set()\n    for k in M:\n        if k = 0:\n            continue\n        for seq in sequences:\n            if len(seq) >= k:\n                for i in range(len(seq) - k + 1):\n                    candidate_motifs.add(seq[i:i+k])\n    \n    # Sort for deterministic tie-breaking\n    sorted_motifs = sorted(list(candidate_motifs))\n\n    if not sorted_motifs:\n        # According to the spec, C is a union over k in M of motifs that appear at least once.\n        # If no motifs are found, C is empty. The argmin over an empty set is undefined.\n        # For the given tests this path is not taken. A robust implementation needs a default.\n        # Given the edge case of empty S, a reasonable default for empty C would be p=1.\n        # Let's assume C is never empty for the given test cases.\n        # A plausible default for m_star would be the lexicographically smallest possible motif, e.g. 'AAA...'.\n        # However, the problem definition of C implies it's never empty if there are sequences and valid M.\n        # Let's proceed under this assumption.\n        pass\n\n    best_motif = \"\"\n    min_p_value = float('inf')\n\n    # Handle the empty set S edge case.\n    if nS == 0:\n        # If S is empty, x(m) is always 0. P(X >= 0) is always 1.\n        # The best motif is the lexicographically first one.\n        best_motif = sorted_motifs[0] if sorted_motifs else \"\"\n        min_p_value = 1.0\n    else:\n        # Step 3  4: For each motif, calculate p-value and find the best motif\n        for m in sorted_motifs:\n            K_m = sum(1 for s in sequences if m in s)\n            x_m = sum(1 for i in S if m in sequences[i])\n\n            # Calculate the one-sided enrichment p-value using the hypergeometric survival function.\n            # P[X >= x_m] is computed as sf(x_m-1).\n            # M_pop -> N (total guides)\n            # n_success -> K_m (guides with motif)\n            # N_sample -> nS (guides in S)\n            p_val = hypergeom.sf(x_m - 1, N, K_m, nS)\n\n            if p_val  min_p_value:\n                min_p_value = p_val\n                best_motif = m\n                # Because the motifs are pre-sorted lexicographically, the first time we\n                # find a minimum p-value, we are guaranteed to satisfy the tie-breaking rule.\n\n    # Step 5: Calculate the final result triple\n    m_star_rank = rank_motif(best_motif)\n    p_m_star_rounded = round(min_p_value, 6)\n    is_significant = 1 if min_p_value  alpha else 0\n\n    return [m_star_rank, p_m_star_rounded, is_significant]\n\nif __name__ == '__main__':\n    solve()\n```", "id": "2372025"}]}