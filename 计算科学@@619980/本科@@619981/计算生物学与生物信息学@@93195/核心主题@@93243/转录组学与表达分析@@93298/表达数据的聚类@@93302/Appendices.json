{"hands_on_practices": [{"introduction": "原始的基因表达数据，尤其是来自测序技术的数据，其数值本身可能具有误导性。不同样本的总读数（即文库大小）可能因技术原因而非生物学差异而有巨大差别。这个练习 [@problem_id:2379247] 将让你通过对转换前后的数据进行聚类，亲眼见证数据标准化的关键重要性，并理解为何它是表达数据分析中不可或缺的第一步。", "problem": "给定您几个小型的基因表达矩阵，旨在为多个样本的核糖核酸测序（RNA-seq）读数计数建模。每个矩阵以基因为行，样本为列。对于每个矩阵，您必须在两种数据体系下计算样本的两种聚类：一种是未经处理的原始计数，另一种是应用文库大小归一化后进行对数变换的转换版本。然后，您必须使用一个数学上定义的指数来量化原始聚类和转换后聚类的相似程度。\n\n定义和要求：\n\n- 令原始计数矩阵表示为 $X \\in \\mathbb{N}_{0}^{g \\times s}$，其中 $g$ 是基因数量，$s$ 是样本数量。条目 $X_{ij}$ 是基因 $i$ 在样本 $j$ 中的非负整数计数。\n\n- 将每个样本的文库大小定义为 $L_j = \\sum_{i=1}^{g} X_{ij}$，适用于每个样本 $j \\in \\{1,\\dots,s\\}$。定义每百万计数（CPM）矩阵 $C \\in \\mathbb{R}^{g \\times s}$ 为\n  $$C_{ij} = \\begin{cases}\n  10^6 \\cdot \\frac{X_{ij}}{L_j}, & \\text{若 } L_j > 0, \\\\\n  0, & \\text{若 } L_j = 0.\n  \\end{cases}$$\n  定义对数变换后的归一化矩阵 $Y \\in \\mathbb{R}^{g \\times s}$ 为\n  $Y_{ij} = \\log_2(C_{ij} + 1)$。\n\n- 您必须使用两种方法对 $X$ 和 $Y$ 中的样本（即列）进行聚类：\n  1. 在 $\\mathbb{R}^{g}$ 空间中使用标准欧几里得距离 $d(\\mathbf{u},\\mathbf{v}) = \\left\\|\\mathbf{u} - \\mathbf{v}\\right\\|_2$ 进行 $k$-均值聚类，其中 $k$ 值由每个测试用例指定。初始化必须是确定性的：使用前 $k$ 个样本（按列顺序）作为初始中心点。通过交替进行分配和更新步骤进行迭代，直到分配稳定或达到最多 $100$ 次迭代。如果在任何迭代中某个簇变为空，则立即将其中心重新初始化为距离任何现有中心当前最小距离最大的样本。在分配过程中，如果距离完全相等，则将样本分配给索引最小的簇。\n  2. 使用相同欧几里得距离和平均连锁的凝聚式层次聚类，通过切割树状图以获得 $k$ 个簇，从而产生正好 $k$ 个扁平簇，切割时使用标准的基于基数的准则。聚类必须在样本向量（列）上执行。\n\n- 对于每种方法，使用调整兰德指数（ARI）比较在 $X$ 上获得的聚类标签和在 $Y$ 上获得的聚类标签。给定 $s$ 个项目的两个聚类 $\\mathcal{U}$ 和 $\\mathcal{V}$，令 $n_{ij}$ 表示列联计数，$a_i = \\sum_{j} n_{ij}$，$b_j = \\sum_{i} n_{ij}$，并令\n  $$\\binom{n}{2} = \\frac{n(n-1)}{2}。$$\n  调整兰德指数为\n  $$\\mathrm{ARI}(\\mathcal{U},\\mathcal{V}) = \\frac{\\sum_{i,j} \\binom{n_{ij}}{2} - \\left[\\sum_{i} \\binom{a_i}{2} \\sum_{j} \\binom{b_j}{2}\\right] \\Big/ \\binom{s}{2}}{\\frac{1}{2}\\left[\\sum_{i} \\binom{a_i}{2} + \\sum_{j} \\binom{b_j}{2}\\right] - \\left[\\sum_{i} \\binom{a_i}{2} \\sum_{j} \\binom{b_j}{2}\\right] \\Big/ \\binom{s}{2}}.$$\n  如果分母为零，若两个分区相同，则定义 $\\mathrm{ARI} = 1$，否则定义 $\\mathrm{ARI} = 0$。\n\n- 所有计算均无单位。不涉及角度。将所有报告的实数四舍五入至 $6$ 位小数。\n\n测试套件：\n\n对于以下每个测试用例，将提供的矩阵视为 $X$，并将指定的 $k$ 视为目标簇数。在每种情况下，计算两个 ARI 值：第一个是 $k$-均值（原始与转换后），第二个是层次聚类（原始与转换后）。\n\n- 测试用例 $1$ （强烈的文库大小效应，掩盖了原始计数中的相对组成）：\n  - $X^{(1)}$ 具有 $g=2$ 和 $s=4$，其列对应于样本 $1$ 到 $4$：\n    - 样本 $1$：$\\begin{bmatrix} 100 \\\\ 10 \\end{bmatrix}$，\n    - 样本 $2$：$\\begin{bmatrix} 1000 \\\\ 100 \\end{bmatrix}$，\n    - 样本 $3$：$\\begin{bmatrix} 10 \\\\ 100 \\end{bmatrix}$，\n    - 样本 $4$：$\\begin{bmatrix} 100 \\\\ 1000 \\end{bmatrix}$。\n  - 因此 $X^{(1)} = \\begin{bmatrix} 100 & 1000 & 10 & 100 \\\\ 10 & 100 & 100 & 1000 \\end{bmatrix}$，且 $k=2$。\n\n- 测试用例 $2$ （零文库大小的边缘情况，避免距离相等）：\n  - $X^{(2)}$ 具有 $g=2$ 和 $s=3$，其列为：\n    - 样本 $1$：$\\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$，\n    - 样本 $2$：$\\begin{bmatrix} 12 \\\\ 0 \\end{bmatrix}$，\n    - 样本 $3$：$\\begin{bmatrix} 0 \\\\ 9 \\end{bmatrix}$。\n  - 因此 $X^{(2)} = \\begin{bmatrix} 0 & 12 & 0 \\\\ 0 & 0 & 9 \\end{bmatrix}$，且 $k=2$。\n\n- 测试用例 $3$ （总数均衡且具有清晰的相对表达结构）：\n  - $X^{(3)}$ 具有 $g=3$ 和 $s=6$，其列为：\n    - 样本 $1$：$\\begin{bmatrix} 100 \\\\ 50 \\\\ 10 \\end{bmatrix}$，\n    - 样本 $2$：$\\begin{bmatrix} 96 \\\\ 48 \\\\ 16 \\end{bmatrix}$，\n    - 样本 $3$：$\\begin{bmatrix} 104 \\\\ 52 \\\\ 4 \\end{bmatrix}$，\n    - 样本 $4$：$\\begin{bmatrix} 10 \\\\ 50 \\\\ 100 \\end{bmatrix}$，\n    - 样本 $5$：$\\begin{bmatrix} 16 \\\\ 48 \\\\ 96 \\end{bmatrix}$，\n    - 样本 $6$：$\\begin{bmatrix} 4 \\\\ 52 \\\\ 104 \\end{bmatrix}$。\n  - 因此 $X^{(3)} = \\begin{bmatrix} 100 & 96 & 104 & 10 & 16 & 4 \\\\ 50 & 48 & 52 & 50 & 48 & 52 \\\\ 10 & 16 & 4 & 100 & 96 & 104 \\end{bmatrix}$，且 $k=2$。\n\n要求的程序输出：\n\n- 您的程序必须生成单行输出，其中包含一个用方括号括起来的、以逗号分隔的列表。该列表必须按以下顺序包含 $6$ 个实数：\n  - 测试用例 $1$ 的 $k$-均值 ARI，测试用例 $1$ 的层次聚类 ARI，测试用例 $2$ 的 $k$-均值 ARI，测试用例 $2$ 的层次聚类 ARI，测试用例 $3$ 的 $k$-均值 ARI，测试用例 $3$ 的层次聚类 ARI。\n- 每个实数必须四舍五入到 $6$ 位小数。\n- 例如，一个语法上有效的输出应如下所示 $[0.123456,0.000000,1.000000,1.000000,0.876543,0.876543]$，但其值需根据上述定义计算。", "solution": "所提出的问题是生物信息学中一个明确定义的计算任务，要求在标准归一化程序前后，比较基因表达数据的聚类结果。所有定义、算法和参数都已明确规定。该问题具有科学依据、客观且定义明确。因此，我将进行完整解答。\n\n任务是取一个原始基因表达计数矩阵 $X$，计算一个归一化并变换后的矩阵 $Y$，使用两种不同的方法（$k$-均值和层次聚类）对 $X$ 和 $Y$ 的样本（列）进行聚类，然后使用调整兰德指数（ARI）量化原始数据聚类与变换后数据聚类之间的相似性。这个过程必须对三个不同的测试用例重复进行。\n\n首先，我们来形式化数据变换过程。给定一个原始计数矩阵 $X \\in \\mathbb{N}_{0}^{g \\times s}$，其中 $g$ 是基因数，$s$ 是样本数，变换到矩阵 $Y$ 的过程包括两个步骤：\n1.  **每百万计数（CPM）归一化**：此步骤校正了样本之间测序深度（文库大小）的差异。样本 $j$ 的文库大小为 $L_j = \\sum_{i=1}^{g} X_{ij}$。CPM 矩阵 $C$ 的计算公式为：若 $L_j > 0$，则 $C_{ij} = 10^6 \\cdot X_{ij} / L_j$；若 $L_j=0$，则 $C_{ij} = 0$。这将每个样本中的计数重新缩放到一个共同的总数 $10^6$。\n2.  **对数变换**：为了稳定方差并使数据更具对称性，应用了 $\\log_2$ 变换。为了避免对零取对数，会加上一个伪计数 $1$。最终的矩阵是 $Y_{ij} = \\log_2(C_{ij} + 1)$。\n\n接下来，我们必须为样本实现指定的两种聚类算法，样本由矩阵 $X$ 和 $Y$ 的列向量表示。\n\n**$k$-均值聚类**：\n这是一种迭代划分方法。对于一个 $g$ 维空间中的 $s$ 个样本向量集合，算法按以下步骤进行：\n1.  **初始化**：选择前 $k$ 个样本作为初始的 $k$ 个质心。\n2.  **分配步骤**：根据欧几里得距离 $d(\\mathbf{u},\\mathbf{v}) = \\sqrt{\\sum_{i=1}^{g}(u_i-v_i)^2}$，将每个样本分配给距离其最近的质心所在的簇。如果距离出现平局，则将样本分配给索引最小的簇。\n3.  **更新步骤**：将每个簇的质心重新计算为分配给该簇的所有样本向量的均值。\n4.  **空簇处理**：如果更新步骤导致某个簇没有任何分配的样本，则需重新初始化其质心。新的质心选择为距离当前所有存在的（非空）质心具有最大最小欧几里得距离的样本向量。这有助于将新的质心放置在数据空间的稀疏区域。\n5.  **终止**：重复步骤 2-4，直到两次迭代之间的簇分配不再改变，或达到最大 100 次迭代。\n\n**凝聚式层次聚类**：\n这是一种自底向上的方法，用于构建簇的层次结构。\n1.  **初始化**：每个样本最初都自成一簇。\n2.  **迭代**：在每一步中，将两个最接近的簇合并成一个新的、更大的簇。此过程持续进行，直到只剩下一个包含所有样本的簇。两个簇之间的距离由一个连锁标准定义。问题指定了**平均连锁**（也称为 UPGMA），其中两个簇之间的距离是所有跨簇样本对（每对中一个样本来自一个簇）之间欧几里得距离的平均值。\n3.  **扁平簇提取**：为了得到恰好 $k$ 个簇，合并过程在执行了 $s-k$ 次合并后停止。这等同于在能够产生 $k$ 个不同分支的水平上切割所产生的树状图。\n\n最后，对于每种聚类方法，我们必须比较从原始矩阵 $X$ 得到的划分（称之为 $\\mathcal{U}$）与从变换后矩阵 $Y$ 得到的划分（称之为 $\\mathcal{V}$）。比较度量是**调整兰德指数（ARI）**。ARI 衡量两个数据聚类之间的相似性，并对偶然性进行了校正。其值范围在 -1 到 1 之间，其中 1 表示完全一致，接近 0 的值表示随机一致，负值表示一致性低于偶然预期的水平。\n提供的公式是：\n$$\n\\mathrm{ARI}(\\mathcal{U},\\mathcal{V}) = \\frac{\\sum_{i,j} \\binom{n_{ij}}{2} - \\left[\\sum_{i} \\binom{a_i}{2} \\sum_{j} \\binom{b_j}{2}\\right] \\Big/ \\binom{s}{2}}{\\frac{1}{2}\\left[\\sum_{i} \\binom{a_i}{2} + \\sum_{j} \\binom{b_j}{2}\\right] - \\left[\\sum_{i} \\binom{a_i}{2} \\sum_{j} \\binom{b_j}{2}\\right] \\Big/ \\binom{s}{2}}\n$$\n其中 $n_{ij}$ 是在 $\\mathcal{U}$ 中簇 $i$ 和 $\\mathcal{V}$ 中簇 $j$ 的样本数，$a_i$ 是 $\\mathcal{U}$ 中簇 $i$ 的总大小，$b_j$ 是 $\\mathcal{V}$ 中簇 $j$ 的总大小，$s$ 是总样本数。项 $\\binom{n}{2}$ 计算元素对的数量。对于分母为零的情况，给出了一个特殊条件：如果分区相同，则 $\\mathrm{ARI}=1$，否则为 $0$。\n\n实现过程将通过应用这些步骤来处理每个测试用例，计算所需的两个 ARI 值（一个用于 $k$-均值，一个用于层次聚类），并将它们整理成一个单一的输出列表。所有数值计算将使用指定的库进行，`numpy` 用于线性代数，`scipy` 用于层次聚类和组合等专门函数。", "answer": "```python\nimport numpy as np\nfrom scipy.cluster.hierarchy import linkage, fcluster\nfrom scipy.special import comb\n\ndef transform_data(X):\n    \"\"\"\n    Applies library-size normalization and log2 transform to a raw count matrix.\n    X: Raw count matrix (genes x samples).\n    \"\"\"\n    g, s = X.shape\n    lib_sizes = np.sum(X, axis=0)\n    \n    C = np.zeros_like(X, dtype=np.float64)\n    # Using a loop to avoid division by zero without a mask\n    for j in range(s):\n        if lib_sizes[j] > 0:\n            C[:, j] = 1e6 * X[:, j] / lib_sizes[j]\n    \n    Y = np.log2(C + 1)\n    return Y\n\ndef custom_kmeans(data, k, max_iter=100):\n    \"\"\"\n    Performs k-means clustering with deterministic initialization and specific rules.\n    data: Data matrix (samples x features).\n    \"\"\"\n    n_samples, n_features = data.shape\n    \n    # Initialization: first k samples are initial centers\n    centroids = data[:k].copy().astype(np.float64)\n    \n    labels = -np.ones(n_samples, dtype=int)\n    \n    for iteration in range(max_iter):\n        # Assignment step\n        new_labels = np.zeros(n_samples, dtype=int)\n        for i in range(n_samples):\n            # Using squared Euclidean distance to avoid sqrt\n            distances_sq = np.sum((data[i] - centroids)**2, axis=1)\n            min_dist_sq = np.min(distances_sq)\n            # Tie-breaking: assign to cluster with smallest index\n            min_dist_indices = np.where(distances_sq == min_dist_sq)[0]\n            new_labels[i] = np.min(min_dist_indices)\n        \n        if np.array_equal(labels, new_labels):\n            break\n        \n        labels = new_labels\n        \n        # Update step\n        new_centroids = np.zeros((k, n_features), dtype=np.float64)\n        cluster_counts = np.bincount(labels, minlength=k)\n        \n        # Calculate new centroids for non-empty clusters\n        non_empty_mask = cluster_counts > 0\n        np.add.at(new_centroids, labels, data)\n        new_centroids[non_empty_mask] /= cluster_counts[non_empty_mask][:, np.newaxis]\n        \n        # Handle empty clusters\n        empty_clusters_indices = np.where(~non_empty_mask)[0]\n        if len(empty_clusters_indices) > 0:\n            existing_centroids = new_centroids[non_empty_mask]\n            \n            # Find sample(s) with largest minimum distance to any existing center\n            if existing_centroids.shape[0] > 0:\n                dists_to_centers_sq = np.array([np.sum((sample - existing_centroids)**2, axis=1) for sample in data])\n                min_dists_sq = np.min(dists_to_centers_sq, axis=1)\n                \n                # Sort samples by descending minimum distance to find candidates\n                furthest_samples_indices = np.argsort(-min_dists_sq)\n                \n                # Assign distinct furthest samples to empty clusters\n                candidate_idx = 0\n                for cluster_idx in empty_clusters_indices:\n                    new_centroids[cluster_idx] = data[furthest_samples_indices[candidate_idx]]\n                    candidate_idx += 1\n            else: # All clusters became empty, re-initialize from start\n                new_centroids[:len(empty_clusters_indices)] = data[:len(empty_clusters_indices)]\n        \n        centroids = new_centroids\n        \n    return labels\n\ndef hierarchical_clustering(data, k):\n    \"\"\"\n    Performs agglomerative hierarchical clustering with average linkage.\n    data: Data matrix (samples x features).\n    \"\"\"\n    if data.shape[0] < 2:\n        return np.zeros(data.shape[0], dtype=int)\n    Z = linkage(data, method='average', metric='euclidean')\n    labels = fcluster(Z, t=k, criterion='maxclust')\n    return labels - 1  # Convert to 0-based labels\n\ndef _canonicalize_labels(labels):\n    \"\"\"Converts labels to a canonical 0-indexed form for comparison.\"\"\"\n    mapping = {}\n    next_label = 0\n    new_labels = np.empty_like(labels)\n    for i, label in enumerate(labels):\n        if label not in mapping:\n            mapping[label] = next_label\n            next_label += 1\n        new_labels[i] = mapping[label]\n    return new_labels\n\ndef adjusted_rand_index(labels_true, labels_pred):\n    \"\"\"Computes the Adjusted Rand Index.\"\"\"\n    n_samples = len(labels_true)\n    if n_samples <= 1:\n        return 1.0\n\n    # Create contingency table\n    classes = np.unique(labels_true)\n    clusters = np.unique(labels_pred)\n    contingency = np.empty((classes.shape[0], clusters.shape[0]), dtype=int)\n    for i, class_val in enumerate(classes):\n        for j, cluster_val in enumerate(clusters):\n            contingency[i, j] = np.sum((labels_true == class_val) & (labels_pred == cluster_val))\n\n    sum_comb_nij = sum(comb(n, 2, exact=True) for n in contingency.flatten())\n    sum_comb_a = sum(comb(n, 2, exact=True) for n in np.sum(contingency, axis=1))\n    sum_comb_b = sum(comb(n, 2, exact=True) for n in np.sum(contingency, axis=0))\n    \n    comb_s = comb(n_samples, 2, exact=True)\n    if comb_s == 0:\n        return 1.0\n\n    expected_index = (sum_comb_a * sum_comb_b) / comb_s\n    numerator = sum_comb_nij - expected_index\n    denominator = 0.5 * (sum_comb_a + sum_comb_b) - expected_index\n    \n    if denominator == 0:\n        labels_true_canon = _canonicalize_labels(labels_true)\n        labels_pred_canon = _canonicalize_labels(labels_pred)\n        return 1.0 if np.array_equal(labels_true_canon, labels_pred_canon) else 0.0\n    \n    return numerator / denominator\n\ndef solve():\n    \"\"\"Main function to run all test cases and produce the final output.\"\"\"\n    test_cases = [\n        (np.array([[100, 1000, 10, 100], [10, 100, 100, 1000]]), 2),\n        (np.array([[0, 12, 0], [0, 0, 9]]), 2),\n        (np.array([[100, 96, 104, 10, 16, 4], \n                   [50, 48, 52, 50, 48, 52], \n                   [10, 16, 4, 100, 96, 104]]), 2)\n    ]\n\n    results = []\n    for X, k in test_cases:\n        Y = transform_data(X)\n        \n        # Samples are columns in X, so we transpose for standard clustering input (samples x features)\n        X_T = X.T\n        Y_T = Y.T\n\n        # k-means clustering\n        labels_x_kmeans = custom_kmeans(X_T, k)\n        labels_y_kmeans = custom_kmeans(Y_T, k)\n        ari_kmeans = adjusted_rand_index(labels_x_kmeans, labels_y_kmeans)\n        results.append(round(ari_kmeans, 6))\n\n        # Hierarchical clustering\n        labels_x_hclust = hierarchical_clustering(X_T, k)\n        labels_y_hclust = hierarchical_clustering(Y_T, k)\n        ari_hclust = adjusted_rand_index(labels_x_hclust, labels_y_hclust)\n        results.append(round(ari_hclust, 6))\n\n    formatted_results = [f\"{r:.6f}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "2379247"}, {"introduction": "在现实世界的数据分析中，结合不同方法的优点是一种常见且强大的策略。这个高级练习 [@problem_id:2379272] 介绍了一种混合方法：利用层次聚类发现的结构来为 $k$-means 算法提供智能的初始质心。通过这种方式，你可以克服 $k$-means 算法对初始随机质心选择的敏感性，从而获得更稳健、更有意义的聚类结果。", "problem": "给定一个有限的基因表达谱集合，由一个实值矩阵 $X \\in \\mathbb{R}^{n \\times m}$ 表示，其中每一行对应一个基因，每一列对应一个实验条件。设基因由 $0,1,\\ldots,n-1$ 进行索引，第 $i$ 行记为 $x_i \\in \\mathbb{R}^m$。考虑欧几里得距离 $d(x_i,x_j) = \\lVert x_i - x_j \\rVert_2$。\n\n请用纯数学术语定义以下对象和约定。\n\n- 通过迭代合并子集的方式，在基因集合上构建一个凝聚层次结构。在任何阶段，设 $\\mathcal{C}$ 为当前非空、两两不相交的子集族，其并集为 $\\{0,1,\\ldots,n-1\\}$。对于任意两个不同的子集 $A,B \\in \\mathcal{C}$，定义平均链接（average-linkage）相异度\n$$\n\\delta(A,B) = \\frac{1}{|A|\\,|B|} \\sum_{i \\in A}\\sum_{j \\in B} d(x_i,x_j).\n$$\n在每个合并步骤中，选择一对 $(A^\\star,B^\\star)$，使其在所有 $A \\neq B$ 的无序对 $\\{A,B\\} \\subset \\mathcal{C}$ 中最小化 $\\delta(A,B)$。如果 $\\delta(A,B)$ 的值出现平局，则通过以下规则确定性地打破平局：在所有平局的对中，选择其有序对 $(\\min A, \\min B)$ 字典序最小的对，其中对于任意集合 $A$ 和 $B$ 我们假设 $\\min A < \\min B$。合并高度定义为 $h(A^\\star,B^\\star) = \\delta(A^\\star,B^\\star)$。在 $\\mathcal{C}$ 中用 $A^\\star \\cup B^\\star$ 替换 $A^\\star$ 和 $B^\\star$，并继续此过程直到只剩下一个子集。结果会产生一个具有明确定义的合并高度和合并全序的唯一树状图。\n\n- 对于任意阈值 $t \\in \\mathbb{R}_{\\ge 0}$，将在高度 $t$ 处切割树状图所产生的划分定义如下。从单元集开始，按其高度 $h$ 的非递减顺序应用所有满足 $h \\le t$ 的合并；任何高度严格大于 $t$ 的合并都不应用。得到的子集族是 $\\{0,1,\\ldots,n-1\\}$ 的一个划分 $\\mathcal{P}_t = \\{C_1,\\ldots,C_k\\}$，其中 $k \\in \\{1,2,\\ldots,n\\}$。为确保这 $k$ 个部分的规范化标记，对这些部分进行排序，使得如果 $a_\\ell = \\min C_\\ell$，则 $a_1 < a_2 < \\cdots < a_k$。定义 $k = |\\mathcal{P}_t|$。\n\n- 对于任意非空子集 $C \\subseteq \\{0,1,\\ldots,n-1\\}$，将其质心定义为\n$$\n\\mu_C = \\frac{1}{|C|} \\sum_{i \\in C} x_i \\in \\mathbb{R}^m.\n$$\n\n- 对于 $\\{0,1,\\ldots,n-1\\}$ 的任意划分 $\\mathcal{A} = \\{A_1,\\ldots,A_k\\}$（$k \\ge 1$），以及任意选择的中心点 $\\{\\mu_1,\\ldots,\\mu_k\\} \\subset \\mathbb{R}^m$，定义簇内平方和（WCSS）目标函数\n$$\nW(\\mathcal{A}, \\{\\mu_\\ell\\}_{\\ell=1}^k) = \\sum_{\\ell=1}^k \\sum_{i \\in A_\\ell} \\lVert x_i - \\mu_\\ell \\rVert_2^2.\n$$\n\n- 一个对 $(\\mathcal{A}, \\{\\mu_\\ell\\})$，其中 $\\mathcal{A} = \\{A_1,\\ldots,A_k\\}$，如果满足以下两个属性，则称其为自洽的：\n    1. 对于每个 $i \\in \\{0,1,\\ldots,n-1\\}$，如果 $i \\in A_\\ell$，那么对于所有的 $r \\in \\{1,\\ldots,k\\}$，都有 $\\lVert x_i - \\mu_\\ell \\rVert_2 \\le \\lVert x_i - \\mu_r \\rVert_2$，平局通过选择索引最小的簇来打破（即，如果存在多个 $r$ 使距离最小化，则将 $i$ 分配给其中最小的 $r$）。\n    2. 对于每个 $\\ell \\in \\{1,\\ldots,k\\}$，$\\mu_\\ell$ 是 $A_\\ell$ 的质心，即 $\\mu_\\ell = \\frac{1}{|A_\\ell|}\\sum_{i \\in A_\\ell} x_i$。\n如果在这些条件下某些 $A_\\ell$ 会变为空，则相应的 $\\mu_\\ell$ 将保持其前一个值不变，以确保更新的良定义性。\n\n- 考虑以下从给定阈值 $t$ 获得自洽对的确定性方案：令 $\\mathcal{P}_t = \\{C_1,\\ldots,C_k\\}$ 为在高度 $t$ 切割得到的划分，其中 $C_\\ell$ 按上文所述的 $\\min C_\\ell$ 递增顺序排序。初始化 $\\mu_\\ell^{(0)} = \\mu_{C_\\ell}$，对 $\\ell \\in \\{1,\\ldots,k\\}$。通过在每次迭代 $r \\in \\{0,1,2,\\ldots\\}$ 中，将每个 $i \\in \\{0,\\ldots,n-1\\}$ 分配到最小化 $\\lVert x_i - \\mu_\\ell^{(r)} \\rVert_2$ 的索引 $\\ell$（平局由最小的 $\\ell$ 打破），从而得到 $\\mathcal{A}^{(r+1)} = \\{A_1^{(r+1)},\\ldots,A_k^{(r+1)}\\}$，来定义一个序列。然后，当 $A_\\ell^{(r+1)}$ 非空时，将 $\\mu_\\ell^{(r+1)}$ 更新为 $A_\\ell^{(r+1)}$ 的质心；如果 $A_\\ell^{(r+1)}$ 为空，则设 $\\mu_\\ell^{(r+1)} = \\mu_\\ell^{(r)}$。当 $\\mathcal{A}^{(r+1)} = \\mathcal{A}^{(r)}$（无变化）时，或最多在 $1000$ 次迭代后停止，以先发生者为准。输出是包含 $k$ 个部分的 $(\\mathcal{A}^\\star, \\{\\mu_\\ell^\\star\\})$，以及 $W^\\star = W(\\mathcal{A}^\\star, \\{\\mu_\\ell^\\star\\})$。\n\n使用上述定义，实现一个程序，对以下具有 $n=8$ 个基因和 $m=3$ 个条件的基因表达矩阵 $X \\in \\mathbb{R}^{8 \\times 3}$ 进行操作，行按基因索引排序：\n$$\nX = \\begin{bmatrix}\n10.0 & 11.2 & 9.5 \\\\\n9.5 & 10.8 & 9.7 \\\\\n10.8 & 11.0 & 10.2 \\\\\n49.5 & 50.1 & 51.2 \\\\\n51.2 & 49.7 & 50.4 \\\\\n49.9 & 51.0 & 49.8 \\\\\n100.2 & 98.9 & 101.5 \\\\\n101.1 & 100.3 & 98.8\n\\end{bmatrix}.\n$$\n\n你的程序必须为下面测试套件中的每个阈值 $t$ 计算：\n- $\\mathcal{P}_t$ 中的簇数量 $k$，\n- 自洽分配，为一个包含 $n$ 个整数的列表，这些整数在 $\\{0,1,\\ldots,k-1\\}$ 范围内，表示每个基因索引 $i$ 在 $\\mathcal{A}^\\star$ 中被分配到的簇的索引，其中簇索引按指定的 $\\min C_\\ell$ 递增顺序排序，\n- 最终质心 $\\{\\mu_\\ell^\\star\\}_{\\ell=1}^k$，为一个包含 $k$ 个列表的列表，每个列表长度为 $m$，\n- 最终的簇内平方和 $W^\\star$。\n\n所有实值输出必须四舍五入到恰好 $6$ 位小数。测试套件的阈值是：\n- $t_1 = 0.01$，\n- $t_2 = 5.0$，\n- $t_3 = 1000.0$。\n\n程序的最终输出必须将所有测试用例的结果聚合成单行，格式如下。对于每个测试用例 $t_j$，生成一个列表\n$$\n\\left[ k,\\, W^\\star,\\, [a_0,a_1,\\ldots,a_{n-1}],\\, [\\,[\\mu_{1,1},\\ldots,\\mu_{1,m}],\\,\\ldots,\\,[\\mu_{k,1},\\ldots,\\mu_{k,m}]\\,] \\right],\n$$\n其中 $a_i$ 是从零开始的簇分配索引，$\\mu_{\\ell,r}$ 是四舍五入到 $6$ 位小数的质心分量。程序应生成单行输出，包含所有测试用例的结果，形式为用方括号括起来的逗号分隔列表，例如：\n$[ t_1\\text{的结果}, t_2\\text{的结果}, t_3\\text{的结果} ]$。\n\n你的实现必须在给定的平局打破约定下是确定性的，并且不得需要任何外部输入。\n\n该测试套件旨在覆盖：当 $t=t_1$ 时 $k=n$ 的边界条件（无合并），当 $t=t_2$ 时具有中间值 $k$ 的典型情况，以及当 $t=t_3$ 时 $k=1$ 的边界条件。", "solution": "该问题要求对给定的基因表达矩阵 $X \\in \\mathbb{R}^{8 \\times 3}$ 实现一个确定性的两阶段聚类分析。第一阶段涉及凝聚层次聚类，以生成数据的初始划分。第二阶段使用一个迭代的类 k-means 算法来精炼此划分，以获得一个自洽的结果。整个过程通过在每个阶段明确的平局打破规则而变得确定性。\n\n首先，我们必须为这 $n=8$ 个基因构建完整的凝聚层次结构。这个过程从每个基因作为一个单元簇开始，即 $\\mathcal{C}^{(0)} = \\{\\{0\\}, \\{1\\}, \\ldots, \\{7\\}\\}$。在随后的每一步中，我们合并展现出最小平均链接相异度的两个簇 $A,B \\in \\mathcal{C}$，其定义为：\n$$\n\\delta(A,B) = \\frac{1}{|A|\\,|B|} \\sum_{i \\in A}\\sum_{j \\in B} d(x_i,x_j)\n$$\n其中 $d(x_i, x_j) = \\lVert x_i - x_j \\rVert_2$ 是基因 $i$ 和基因 $j$ 表达谱之间的欧几里得距离。此步骤的一个关键方面是平局打破规则：如果多对簇产生相同的最小相异度 $\\delta$，则选择满足有序对 $(\\min A, \\min B)$ 字典序最小的那一对 $\\{A, B\\}$（其中 $\\min A < \\min B$）。这个规则确保了最终的层次结构是唯一的，该结构由带有合并高度 $h(A, B) = \\delta(A, B)$ 的树状图表示。这个过程执行 $n-1=7$ 步，直到所有基因都属于同一个簇。必须存储完整的合并序列及其对应的高度。\n\n接下来，对于每个给定的阈值 $t \\in \\{0.01, 5.0, 1000.0\\}$，我们推导出一个初始划分 $\\mathcal{P}_t$。这个划分是通过执行层次结构中所有高度 $h$ 小于或等于阈值 $t$ 的合并来获得的。得到的划分 $\\mathcal{P}_t = \\{C_1, C_2, \\ldots, C_k\\}$ 由 $k$ 个不相交的簇组成。为了规范表示，这些簇按照其最小基因索引严格递增的顺序排列：$\\min C_1 < \\min C_2 < \\cdots < \\min C_k$。簇的数量 $k = |\\mathcal{P}_t|$ 由阈值 $t$ 决定。\n\n第三阶段是一个迭代精炼过程，类似于 k-means 算法，旨在达到一个自洽状态。该过程使用划分 $\\mathcal{P}_t$ 进行初始化。具体来说，簇的数量是 $k$，初始质心是 $\\mathcal{P}_t$ 中各簇的均值：\n$$\n\\mu_\\ell^{(0)} = \\frac{1}{|C_\\ell|} \\sum_{i \\in C_\\ell} x_i \\quad \\text{for } \\ell = 1, \\ldots, k.\n$$\n迭代过程随后在两个步骤之间交替进行：\n1.  **分配步骤**：将每个基因 $i$ 分配给其质心 $\\mu_\\ell^{(r)}$ 与 $x_i$ 最近的簇 $\\ell$。也就是说，我们找到 $\\ell^\\star = \\arg\\min_{\\ell \\in \\{1,\\ldots,k\\}} \\lVert x_i - \\mu_\\ell^{(r)} \\rVert_2$。再次地，一个平局打破规则被指定：如果多个质心等距，则将基因分配给索引 $\\ell$ 最小的簇。这一步产生一个新的划分 $\\mathcal{A}^{(r+1)}$。\n2.  **更新步骤**：根据新的分配重新计算质心。对于每个簇 $A_\\ell^{(r+1)}$，新的质心是其均值：$\\mu_\\ell^{(r+1)} = \\frac{1}{|A_\\ell^{(r+1)}|} \\sum_{i \\in A_\\ell^{(r+1)}} x_i$。如果任何簇 $A_\\ell^{(r+1)}$ 变为空，则其质心不被更新；即 $\\mu_\\ell^{(r+1)} = \\mu_\\ell^{(r)}$。\n\n这个过程持续进行，直到簇分配在一次迭代到下一次迭代之间不再改变（$\\mathcal{A}^{(r+1)} = \\mathcal{A}^{(r)}$），或最多进行 $1000$ 次迭代。设最终的自洽划分为 $\\mathcal{A}^\\star$，最终质心为 $\\{\\mu_\\ell^\\star\\}$。\n\n最后，我们为最终的配置计算簇内平方和（WCSS）：\n$$\nW^\\star = W(\\mathcal{A}^\\star, \\{\\mu_\\ell^\\star\\}) = \\sum_{\\ell=1}^k \\sum_{i \\in A_\\ell^\\star} \\lVert x_i - \\mu_\\ell^\\star \\rVert_2^2.\n$$\n对于每个测试阈值 $t$，收集结果——$k$、$W^\\star$、每个基因的最终簇分配列表，以及最终质心列表。所有实值输出都四舍五入到 6 位小数，并按规定格式化为单行。实现必须一丝不苟地遵循所有定义和平局打破规则，以确保一个正确且确定性的结果。", "answer": "```python\nimport numpy as np\nimport sys\n\ndef solve():\n    \"\"\"\n    Main function to solve the clustering problem for the given test cases.\n    \"\"\"\n    X = np.array([\n        [10.0, 11.2, 9.5],\n        [9.5, 10.8, 9.7],\n        [10.8, 11.0, 10.2],\n        [49.5, 50.1, 51.2],\n        [51.2, 49.7, 50.4],\n        [49.9, 51.0, 49.8],\n        [100.2, 98.9, 101.5],\n        [101.1, 100.3, 98.8]\n    ])\n\n    ts = [0.01, 5.0, 1000.0]\n\n    # Perform hierarchical clustering once.\n    hc = HierarchicalClustering(X)\n    linkage_matrix = hc.get_linkage_matrix()\n\n    all_results = []\n    for t in ts:\n        result = solve_for_t(X, t, linkage_matrix)\n        all_results.append(result)\n\n    # Format output\n    results_strs = [format_result(r) for r in all_results]\n    print(f\"[{','.join(results_strs)}]\")\n\nclass HierarchicalClustering:\n    \"\"\"\n    Implements agglomerative hierarchical clustering with average linkage and\n    a specific tie-breaking rule.\n    \"\"\"\n    def __init__(self, X):\n        self.X = X\n        self.n = X.shape[0]\n        self.dist_matrix = self._compute_pairwise_distances()\n        self.linkage_matrix = self._build_linkage()\n\n    def get_linkage_matrix(self):\n        return self.linkage_matrix\n\n    def _compute_pairwise_distances(self):\n        dists = np.zeros((self.n, self.n))\n        for i in range(self.n):\n            for j in range(i + 1, self.n):\n                d = np.linalg.norm(self.X[i] - self.X[j])\n                dists[i, j] = dists[j, i] = d\n        return dists\n\n    def _get_cluster_dist(self, c1_indices, c2_indices):\n        total_dist = np.sum(self.dist_matrix[np.ix_(list(c1_indices), list(c2_indices))])\n        return total_dist / (len(c1_indices) * len(c2_indices))\n\n    def _build_linkage(self):\n        active_clusters = {i: {i} for i in range(self.n)}\n        linkage = []\n        next_cluster_id = self.n\n\n        for _ in range(self.n - 1):\n            best_pair_info = (sys.float_info.max,)\n\n            active_ids = sorted(list(active_clusters.keys()))\n            for i in range(len(active_ids)):\n                for j in range(i + 1, len(active_ids)):\n                    id1, id2 = active_ids[i], active_ids[j]\n                    c1, c2 = active_clusters[id1], active_clusters[id2]\n                    dist = self._get_cluster_dist(c1, c2)\n\n                    m1, m2 = min(c1), min(c2)\n                    if m1 > m2:\n                        m1, m2 = m2, m1\n                    \n                    current_pair_info = (dist, m1, m2, id1, id2)\n\n                    if current_pair_info < best_pair_info:\n                        best_pair_info = current_pair_info\n            \n            dist, _, _, id1, id2 = best_pair_info\n            id1, id2 = sorted((id1, id2))\n\n            c1 = active_clusters.pop(id1)\n            c2 = active_clusters.pop(id2)\n            \n            new_cluster = c1.union(c2)\n            linkage.append([float(id1), float(id2), dist, float(len(new_cluster))])\n            \n            active_clusters[next_cluster_id] = new_cluster\n            next_cluster_id += 1\n            \n        return np.array(linkage)\n\ndef solve_for_t(X, t, linkage_matrix):\n    \"\"\"\n    Solves the problem for a single threshold t.\n    \"\"\"\n    n = X.shape[0]\n    \n    # 1. Cut dendrogram\n    cluster_points = {i: {i} for i in range(n)}\n    dsu_parent = list(range(n))\n\n    def find_set(v):\n        if v == dsu_parent[v]:\n            return v\n        dsu_parent[v] = find_set(dsu_parent[v])\n        return dsu_parent[v]\n\n    def unite_sets(a, b):\n        a = find_set(a)\n        b = find_set(b)\n        if a != b:\n            dsu_parent[max(a, b)] = min(a,b)\n\n    next_cluster_id = n\n    for i in range(linkage_matrix.shape[0]):\n        h = linkage_matrix[i, 2]\n        if h > t:\n            break\n        id1, id2 = int(linkage_matrix[i, 0]), int(linkage_matrix[i, 1])\n        c1, c2 = cluster_points[id1], cluster_points[id2]\n        \n        rep1, rep2 = next(iter(c1)), next(iter(c2))\n        unite_sets(rep1, rep2)\n        \n        cluster_points[next_cluster_id] = c1.union(c2)\n        next_cluster_id += 1\n    \n    # Extract partition from DSU\n    initial_partition_map = {}\n    for i in range(n):\n        root = find_set(i)\n        if root not in initial_partition_map:\n            initial_partition_map[root] = set()\n        initial_partition_map[root].add(i)\n    \n    initial_clusters_list = sorted(list(initial_partition_map.values()), key=min)\n    k = len(initial_clusters_list)\n    \n    # 2. Initialize centroids\n    centroids = np.array([X[list(c)].mean(axis=0) for c in initial_clusters_list])\n    \n    # 3. K-means refinement\n    assignments = -np.ones(n, dtype=int)\n    \n    for iteration in range(1001):\n        new_assignments = np.zeros(n, dtype=int)\n        for i in range(n):\n            distances = np.linalg.norm(X[i] - centroids, axis=1)\n            min_dist = np.min(distances)\n            tied_indices = np.where(np.isclose(distances, min_dist))[0]\n            new_assignments[i] = np.min(tied_indices)\n            \n        if np.array_equal(assignments, new_assignments):\n            break\n        \n        assignments = new_assignments\n        if iteration == 1000:\n            break\n\n        new_centroids = np.copy(centroids)\n        for j in range(k):\n            cluster_indices = np.where(assignments == j)[0]\n            if len(cluster_indices) > 0:\n                new_centroids[j] = X[cluster_indices].mean(axis=0)\n        centroids = new_centroids\n\n    # 4. Final calculations\n    wcss = 0.0\n    for j in range(k):\n        cluster_indices = np.where(assignments == j)[0]\n        if len(cluster_indices) > 0:\n            wcss += np.sum(np.linalg.norm(X[cluster_indices] - centroids[j], axis=1)**2)\n            \n    return [k, wcss, assignments.tolist(), centroids.tolist()]\n\ndef format_result(res):\n    \"\"\"Formats a single test case result into the required string format.\"\"\"\n    k, W, assignments, centroids = res\n    k_str = str(k)\n    W_str = f\"{W:.6f}\"\n    assignments_str = str(assignments).replace(\" \", \"\")\n\n    centroid_strs = []\n    for cent_vec in centroids:\n        vec_strs = [f\"{val:.6f}\" for val in cent_vec]\n        centroid_strs.append(f\"[{','.join(vec_strs)}]\")\n    centroids_str = f\"[{','.join(centroid_strs)}]\"\n    \n    return f\"[{k_str},{W_str},{assignments_str},{centroids_str}]\"\n\nsolve()\n```", "id": "2379235"}, {"introduction": "数据经过适当的预处理后，下一步是选择聚类算法及其参数。本实践 [@problem_id:2379235] 旨在探讨在层次聚类中，不同的“连接”准则（linkage criteria）如何导致对数据结构产生截然不同的解读。你将构建一个包含由中间点连接的两个簇的合成数据集，亲身体验“单连接”（single linkage）如何通过“链式效应”将远距离的群组连接起来，而“全连接”（complete linkage）则倾向于保持簇的紧凑和球状。", "problem": "给定一个二维点集族，其旨在模拟两个表达样本组，并可选地包含一个由中间体构成的连接桥。设环境空间为 $\\mathbb{R}^2$，其欧几里得距离为 $d((x_1,y_1),(x_2,y_2))=\\sqrt{(x_1-x_2)^2+(y_1-y_2)^2}$。对于固定的参数 $L>0$、奇数 $m\\ge 3$、垂直间距 $\\Delta_y>0$、桥端点 $a<0<b$ 以及整数 $k\\ge 0$，定义以下子集：\n- 左侧集合 $S_L=\\{(-L,\\, j\\,\\Delta_y)\\ |\\ j\\in\\{-\\frac{m-1}{2},\\ldots,-1,0,1,\\ldots,\\frac{m-1}{2}\\}\\}$，\n- 右侧集合 $S_R=\\{(L,\\, j\\,\\Delta_y)\\ |\\ j\\in\\{-\\frac{m-1}{2},\\ldots,-1,0,1,\\ldots,\\frac{m-1}{2}\\}\\}$，\n- 桥集合 $S_B$ 在 $k=0$ 时为空集，否则 $S_B=\\{(x_j,\\,0)\\ |\\ x_j=a+j\\cdot\\frac{(b-a)}{(k-1)},\\ j\\in\\{0,1,\\ldots,k-1\\}\\}$。\n\n完整数据集为 $S=S_L\\cup S_R\\cup S_B$，包含 $N=|S|$ 个点。考虑凝聚式层次聚类法，其两种连接变体纯粹由其簇间相异度定义：\n- 对于非空不相交集合 $A$ 和 $B$，单连接距离为 $\\min\\{d(p,q)\\ |\\ p\\in A,\\,q\\in B\\}$，\n- 对于非空不相交集合 $A$ 和 $B$，全连接距离为 $\\max\\{d(p,q)\\ |\\ p\\in A,\\,q\\in B\\}$。\n\n从 $N$ 个单点集开始，在选定的连接方式下，重复合并集合间距离最小的一对不同集合，会产生一个合并序列以及一个由合并高度（相异度）组成的非递减列表 $h_1\\le h_2\\le\\cdots\\le h_{N-1}$。对于给定的连接方式，定义 $t_{\\min,2}=h_{N-2}$，它是满足以下条件的最小非负实数：在任意满足 $h_{N-2}\\le t<h_{N-1}$ 的阈值 $t$ 处切割该层次结构，均能得到恰好 $2$ 个簇。将双簇标签定义为当合并过程进行到恰好剩下 $2$ 个簇时停止所得到的分区。\n\n定义真实标签 $g:S\\to\\{0,1\\}$ 如下：如果 $x<0$，则 $g((x,y))=0$；如果 $x\\ge 0$，则 $g((x,y))=1$。对于任何将点集划分为 $2$ 个非空簇 $C_1$ 和 $C_2$ 的聚类结果，定义其相对于 $g$ 的纯度为\n$$\n\\mathrm{Purity}(C_1,C_2;g)=\\frac{\\max\\{|\\{p\\in C_1\\ |\\ g(p)=0\\}|,\\,|\\{p\\in C_1\\ |\\ g(p)=1\\}|\\}+\\max\\{|\\{p\\in C_2\\ |\\ g(p)=0\\}|,\\,|\\{p\\in C_2\\ |\\ g(p)=1\\}|\\}}{N}.\n$$\n\n你的任务是，对下方指定的每一组参数，构建 $S$，并分别计算单连接和全连接下的以下指标：\n- 双簇标签的纯度，记为 $P_{\\text{single}}$ 和 $P_{\\text{complete}}$，以及\n- 阈值 $t_{\\min,2}$，记为 $T_{\\text{single}}$ 和 $T_{\\text{complete}}$。\n\n测试套件（每个元组列出 $(L,m,\\Delta_y,a,b,k)$）：\n- 案例 1（两个分离的组，无桥）： $(10,\\,5,\\,3.1,\\,0,\\,0,\\,0)$，\n- 案例 2（中等大小的桥，非对称锚点）： $(10,\\,5,\\,3.1,\\,-4.8,\\,5.2,\\,9)$，\n- 案例 3（密集的、更长的桥，非对称锚点）： $(10,\\,5,\\,3.1,\\,-6.7,\\,7.1,\\,17)$。\n\n最终输出格式：你的程序应输出单行结果，该行包含一个由方括号括起来的、以逗号分隔的列表，其中每个元素对应一个案例，并且其本身是包含四个实数 $[P_{\\text{single}},P_{\\text{complete}},T_{\\text{single}},T_{\\text{complete}}]$ 的列表，四舍五入到小数点后恰好四位。例如，一个包含两个案例的有效输出如下所示：$[[0.9500,1.0000,3.1000,3.1000],[0.8000,0.9500,4.8000,9.2000]]$。", "solution": "该问题陈述是有效且可解的，但存在一个需要注意的小歧义。桥集合 $S_B = \\{(x_j,\\,0)\\ |\\ x_j=a+j\\cdot\\frac{(b-a)}{(k-1)},\\ j\\in\\{0,1,\\ldots,k-1\\}\\}$ 的定义在 $k=1$ 的情况下因除以零而定义不当。然而，所提供的测试案例使用的 $k$ 值为 $k \\in \\{0, 9, 17\\}$，在这些情况下定义都是明确的。此外，参数约束 $a<0<b$ 在案例 1 中未被满足，其中 $(a,b)=(0,0)$。这不被视为一个问题，因为当 $k=0$ 时，参数 $a$ 和 $b$ 并未用于构建点集 $S$。因此，对于指定的测试案例，该问题是可处理的。\n\n任务是使用两种不同的连接标准（单连接和全连接）对三个定义的点集执行凝聚式层次聚类，并评估结果。对于每种情况，我们必须计算双簇分区下的纯度（$P$）以及形成该分区的阈值 $t_{\\min,2}$（$T$）。\n\n解决方案的核心是实现凝聚式层次聚类算法。算法从 $N$ 个单点簇（$S$ 中的每个点构成一个簇）开始，迭代地合并两个最近的簇，直到只剩下一个簇。簇之间的距离由所选的连接方法定义。发生合并时对应的距离序列构成一个非递减的高度列表，$h_1 \\le h_2 \\le \\dots \\le h_{N-1}$。\n\n双簇标签是当系统中恰好剩下两个簇时的状态。该状态在 $N-2$ 次合并后达到。阈值 $t_{\\min,2}$ 定义为 $h_{N-2}$，即第 $(N-2)$ 次合并的高度——这次合并将簇的数量从三个减少到两个。\n\n一个分区 $\\{C_1, C_2\\}$ 的纯度是相对于真实标签 $g$ 计算的，其中如果 $x<0$ 则 $g((x,y))=0$，如果 $x\\ge 0$ 则 $g((x,y))=1$。如果将每个簇的标签指定为其内部成员多数所属的标签，则纯度是能够被正确分类的点的比例。\n\n以下是针对每个测试案例的理论分析。\n\n**案例 1: $(L,m,\\Delta_y,a,b,k) = (10, 5, 3.1, 0, 0, 0)$**\n数据集 $S$ 由两组点构成，每组 $m=5$ 个点，$S_L$ 位于 $x=-10$ 处，$S_R$ 位于 $x=10$ 处。桥集合 $S_B$ 为空 ($k=0$)。总点数为 $N=10$。\n- 组内距离：在 $S_L$ 或 $S_R$ 内部，点之间的最小距离为 $\\Delta_y=3.1$。最大距离为 $4\\Delta_y=12.4$。\n- 组间距离：$S_L$ 中的任意点与 $S_R$ 中的任意点之间的最小距离为 $d((-10,0), (10,0)) = 20$。\n- 真实分区为 $\\{S_L, S_R\\}$，这是完美分离的。\n\n_单连接_:\n数据集中最小的距离都是组内距离，等于 $\\Delta_y=3.1$。凝聚过程将首先在 $S_L$ 和 $S_R$ 内部形成簇。单连接表现出链式行为；$S_L$ 中的所有点将通过一系列高度为 $3.1$ 的合并形成一个簇，$S_R$ 中的所有点也将形成另一个簇。经过 $2 \\times (m-1) = 8$ 次合并后，将剩下两个簇，$S_L$ 和 $S_R$。因此，双簇分区为 $\\{S_L, S_R\\}$，得到完美纯度 $P_{\\text{single}}=1.0$。第 $(N-2)$ 次合并即第 8 次合并。由于前 8 次合并都发生在高度 $3.1$ 处，因此 $T_{\\text{single}} = h_8 = 3.1$。\n\n_全连接_:\n该连接方法对簇的直径敏感。$S_L$ 内部的最大距离（其直径）为 $4\\Delta_y=12.4$，小于 $S_L$ 和 $S_R$ 之间的最小距离（$20$）。因此，与单连接一样，所有组内合并都将发生在任何组间合并之前。双簇分区同样为 $\\{S_L, S_R\\}$，纯度 $P_{\\text{complete}}=1.0$。阈值 $T_{\\text{complete}}=h_8$ 是总共第 8 次合并的高度。形成完整 $S_L$ 簇（$S_R$ 也类似）的最后一次合并将涉及连接 $S_L$ 的两个子集。这次合并的高度可以大到其直径，即 $12.4$。前 8 次合并中两个最大的合并高度将是形成 $S_L$ 和 $S_R$ 的最后一次合并的高度。因此，$h_7$ 和 $h_8$ 都将是 $12.4$。所以，$T_{\\text{complete}}=12.4$。\n\n**案例 2: $(L,m,\\Delta_y,a,b,k) = (10, 5, 3.1, -4.8, 5.2, 9)$**\n在这里，一个由 $k=9$ 个点组成的桥连接着两个组。$N=19$。\n- $S_B$ 中的点在 x 轴上等距分布，间距为 $1.25$，范围从 $x=-4.8$ 到 $x=5.2$。\n- $S_L$ 和 $S_B$ 之间的最小距离是 $d((-10,0),(-4.8,0)) = 5.2$。\n- $S_R$ 和 $S_B$ 之间的最小距离是 $d((10,0),(5.2,0)) = 4.8$。\n- 最小距离的层级关系是：$1.25$（$S_B$ 内部）< $3.1$（$S_L,S_R$ 内部）< $4.8$（$S_R - S_B$ 之间）< $5.2$（$S_L - S_B$ 之间）。\n\n_单连接_:\n由于链式效应， $S_B$ 中的所有点将首先在高度 $1.25$ 处合并成一个单一的簇 $C_B$。然后，$S_L$ 和 $S_R$ 内部将在高度 $3.1$ 处发生组内合并。这将产生三个簇：$S_L, S_R, C_B$。下一次合并将发生在这三者中最近的一对之间，即 $S_R$ 和 $C_B$，距离为 $4.8$。这会剩下两个簇：$\\{S_L, S_R \\cup C_B\\}$。这就是双簇分区。合并发生在高度 $h_{17} = 4.8$ 处，所以 $T_{\\text{single}}=4.8$。为此分区计算纯度：$C_1=S_L$ 包含 $5$ 个 $g=0$ 的点。$C_2=S_R \\cup S_B$ 包含来自 $S_B$ 的 $4$ 个 $g=0$ 的点和 $5+5=10$ 个 $g=1$ 的点。纯度为 $(5 + \\max(4,10))/19 = 15/19 \\approx 0.7895$。\n\n_全连接_:\n这种连接方法偏好紧凑的簇，并能抵抗链式效应。预期它会根据真实标签对数据进行分区，将 $S_L$ 与附近 x 坐标为负的桥接点（$C_0$）分为一组，将 $S_R$ 与 x 坐标为正的桥接点（$C_1$）分为一组。此分区得出 $P_{\\text{complete}}=1.0$。阈值 $T_{\\text{complete}}=h_{17}$ 是在 $C_0$ 或 $C_1$ 簇 *内部* 发生的最后一次合并的高度。这由任一簇的“瓶颈”分区决定。形成 $C_1 = S_R \\cup \\{p\\in S_B|g(p)=1\\}$ 的最终合并高度受其直径限制，具体为 $d((10, 6.2), (0.2, 0)) \\approx 11.5966$。$C_0$ 的对应值更小。因此，$T_{\\text{complete}} \\approx 11.5966$。\n\n**案例 3: $(L,m,\\Delta_y,a,b,k) = (10, 5, 3.1, -6.7, 7.1, 17)$**\n现在桥更密集（$k=17$），并且更靠近两侧的组。$N=27$。\n- $S_B$ 中的点间距为 $0.8625$，范围从 $x=-6.7$ 到 $x=7.1$。\n- 最小距离：$0.8625$（$S_B$ 内部）< $2.9$（$S_R - S_B$ 之间）< $3.1$（$S_L,S_R$ 内部）< $3.3$（$S_L - S_B$ 之间）。\n\n_单连接_:\n逻辑与案例 2 类似。桥的邻近性更强将再次导致链式效应。距离的层级关系决定了特定的合并顺序：$S_B$ 形成一个簇，然后与 $S_R$ 合并（高度 $2.9$），然后吸收它（高度 $3.1$）。与此同时，$S_L$ 也形成一个簇。最终得到的两个簇是 $S_L$ 和 $S_R \\cup S_B$。当所有高度直到 $3.1$ 的合并都完成后，此分区形成。连接这两个最终簇的合并发生在高度 $3.3$ 处。因此，$T_{\\text{single}} = h_{25}$ 是形成这两个组所需要的最后一次合并的高度，即 $3.1$。对于分区 $\\{S_L, S_R \\cup S_B\\}$，$C_1=S_L$ 有 $5$ 个 $g=0$ 的点。$C_2=S_R \\cup S_B$ 有 $8$ 个 $g=0$ 的点和 $5+9=14$ 个 $g=1$ 的点。纯度为 $(5 + 14)/27 = 19/27 \\approx 0.7037$。\n\n_全连接_:\n与案例 2 类似，全连接将产生真实标签分区 $\\{C_0, C_1\\}$，纯度 $P_{\\text{complete}}=1.0$。阈值 $T_{\\text{complete}}=h_{25}$ 由形成 $C_0$ 和 $C_1$ 的最终合并高度中较大的一个决定。簇 $C_1$ 的几何形状由 $S_R$ 和最接近原点的正值桥接点决定，该点 x 坐标为 $0.2$（与案例 2 相同）。因此，其瓶颈合并高度的计算是相同的，得出 $T_{\\text{complete}} \\approx 11.5966$。$C_0$ 对应的合并高度比案例 2 中略大，但仍小于 $C_1$ 的。因此，$T_{\\text{complete}}$ 保持不变。\n\n以下程序实现了该逻辑，以计算所需的值。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef construct_point_set(L, m, delta_y, a, b, k):\n    \"\"\"Constructs the point set S and ground truth labels.\"\"\"\n    # S_L: Left set\n    j_indices = np.arange(-(m - 1) / 2, (m - 1) / 2 + 1)\n    s_l = np.array([[-L, j * delta_y] for j in j_indices])\n\n    # S_R: Right set\n    s_r = np.array([[L, j * delta_y] for j in j_indices])\n\n    # S_B: Bridge set\n    if k == 0:\n        s_b = np.empty((0, 2))\n    elif k == 1:\n        # The problem statement is ill-defined for k=1.\n        # Based on practical interpretation for a single point, place it at 'a'.\n        s_b = np.array([[a, 0.0]])\n    else:\n        s_b = np.array([[a + j * (b - a) / (k - 1), 0.0] for j in range(k)])\n\n    points = np.vstack([s_l, s_r, s_b])\n    \n    # Ground truth labels g(p)\n    labels = (points[:, 0] >= 0).astype(int)\n    \n    return points, labels\n\ndef pairwise_distance_matrix(points):\n    \"\"\"Computes the Euclidean distance matrix for all pairs of points.\"\"\"\n    # Using broadcasting for efficiency\n    return np.sqrt(np.sum((points[:, np.newaxis, :] - points[np.newaxis, :, :])**2, axis=-1))\n\ndef agglomerative_hierarchical_clustering(points, linkage):\n    \"\"\"\n    Performs agglomerative hierarchical clustering from scratch.\n    \n    Args:\n        points (np.ndarray): An N-by-2 array of point coordinates.\n        linkage (str): 'single' or 'complete'.\n\n    Returns:\n        tuple: A list of merge heights and the two-cluster partition.\n    \"\"\"\n    n = len(points)\n    if n <= 1:\n        return [], [[i for i in range(n)]]\n\n    # Pre-compute all point-to-point distances\n    dist_matrix = pairwise_distance_matrix(points)\n    \n    # Initialize each point as a cluster of indices\n    clusters = [{i} for i in range(n)]\n    merge_heights = []\n    \n    two_cluster_partition = None\n\n    for _ in range(n - 1):\n        min_dist = np.inf\n        best_pair = (-1, -1)\n        \n        # Find the two closest clusters\n        for i in range(len(clusters)):\n            for j in range(i + 1, len(clusters)):\n                cluster1_indices = list(clusters[i])\n                cluster2_indices = list(clusters[j])\n                \n                # Extract the submatrix of distances between points in the two clusters\n                sub_dist = dist_matrix[np.ix_(cluster1_indices, cluster2_indices)]\n                \n                if linkage == 'single':\n                    d = np.min(sub_dist)\n                elif linkage == 'complete':\n                    d = np.max(sub_dist)\n                else:\n                    raise ValueError(\"Invalid linkage method specified.\")\n                \n                if d < min_dist:\n                    min_dist = d\n                    best_pair = (i, j)\n        \n        merge_heights.append(min_dist)\n        \n        # Merge the closest pair\n        i, j = best_pair\n        if i > j: i, j = j, i  # Ensure i < j for correct popping\n        \n        clusters[i].update(clusters[j])\n        clusters.pop(j)\n\n        # Capture the partition when exactly two clusters remain\n        if len(clusters) == 2:\n            two_cluster_partition = [list(c) for c in clusters]\n\n    return merge_heights, two_cluster_partition\n\ndef calculate_purity(partition, labels):\n    \"\"\"Calculates the purity of a two-cluster partition.\"\"\"\n    n = len(labels)\n    if n == 0: return 1.0\n\n    c1_indices, c2_indices = partition\n    \n    c1_labels = labels[c1_indices]\n    c2_labels = labels[c2_indices]\n    \n    # Count members of each ground-truth class in the first cluster\n    num_c1_g0 = np.sum(c1_labels == 0)\n    num_c1_g1 = len(c1_labels) - num_c1_g0\n    \n    # Count members of each ground-truth class in the second cluster\n    num_c2_g0 = np.sum(c2_labels == 0)\n    num_c2_g1 = len(c2_labels) - num_c2_g0\n    \n    # Sum of majority class sizes\n    correctly_clustered = max(num_c1_g0, num_c1_g1) + max(num_c2_g0, num_c2_g1)\n    \n    purity = correctly_clustered / n\n    return purity\n\n\ndef solve():\n    \"\"\"Main function to solve the problem for all test cases.\"\"\"\n    test_cases = [\n        # (L, m, Delta_y, a, b, k)\n        (10, 5, 3.1, 0, 0, 0),        # Case 1\n        (10, 5, 3.1, -4.8, 5.2, 9),   # Case 2\n        (10, 5, 3.1, -6.7, 7.1, 17),  # Case 3\n    ]\n\n    all_results = []\n    for case in test_cases:\n        L, m, delta_y, a, b, k = case\n        points, labels = construct_point_set(L, m, delta_y, a, b, k)\n        n = len(points)\n\n        # Single linkage\n        heights_s, partition_s = agglomerative_hierarchical_clustering(points, 'single')\n        p_single = calculate_purity(partition_s, labels)\n        # h_{N-2} is the (N-2)-th element in a 1-indexed list of N-1 heights.\n        # This corresponds to index N-3 in a 0-indexed list.\n        t_single = heights_s[n - 3] if n > 2 else (heights_s[0] if n==2 else 0)\n\n        # Complete linkage\n        heights_c, partition_c = agglomerative_hierarchical_clustering(points, 'complete')\n        p_complete = calculate_purity(partition_c, labels)\n        t_complete = heights_c[n - 3] if n > 2 else (heights_c[0] if n==2 else 0)\n        \n        all_results.append([p_single, p_complete, t_single, t_complete])\n    \n    # Format the final output string\n    formatted_results = []\n    for res in all_results:\n        formatted_list = f\"[{res[0]:.4f},{res[1]:.4f},{res[2]:.4f},{res[3]:.4f}]\"\n        formatted_results.append(formatted_list)\n        \n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "2379272"}]}