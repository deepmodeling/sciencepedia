{"hands_on_practices": [{"introduction": "贝叶斯推断的核心在于它如何将先验信念与数据提供的证据相结合，以更新我们对假设的看法。这个实践练习将引导你通过编程，亲手探索先验概率 $p(T_i)$ 和似然函数 $p(D|T_i)$ 之间的动态平衡。通过计算不同情景下最大似然（ML）拓扑的后验概率，你将直观地理解强烈的先验信念如何影响最终结论，以及强大的数据证据又是如何能够超越先验的。[@problem_id:2375013]", "problem": "给定一个包含 $4$ 个分类单元的离散无根树拓扑结构集合，以及它们对应的对数似然和在这些拓扑结构上的先验概率质量函数。您的任务是通过原则性推导和计算，展示一个强的先验如何导致最大似然（ML）树具有非常低的后验概率。\n\n使用的起点和定义：\n- 从应用于有限假设空间的贝叶斯定理以及最大似然（ML）和最大后验（MAP）的标准定义出发。将每种拓扑结构视为一个离散假设。\n- 令 $T_i$ 表示第 $i$ 种拓扑结构，令 $D$ 表示观测到的序列数据，令 $L_i$ 表示在固定的、具有独立位点的时间可逆替换模型下的对数似然 $\\log p(D \\mid T_i)$，令 $\\pi_i$ 表示拓扑结构 $T_i$ 的先验概率 $p(T_i)$。假设 $L_i$ 已通过标准算法（例如，剪枝算法）在固定模型下计算得出，并且 $\\sum_i \\pi_i = 1$，其中每个 $\\pi_i \\in (0,1)$。\n\n您必须完成以下任务：\n1. 从第一性原理出发，推导后验概率 $p(T_i \\mid D)$ 关于 $\\{L_i\\}$ 和 $\\{\\pi_i\\}$ 的表达式，然后将其特化为 ML 拓扑结构 $T_m$ 的后验概率，其中 $m = \\arg\\max_i L_i$。\n2. 为保证数值稳定性，请论述当 $L_i$ 为绝对值很大的负数时，如何计算归一化常数以避免下溢。您的实现必须在对数空间中操作，并且对于给定的值必须是稳定的。\n3. 实现一个程序，对于下方的每个测试用例，计算赋予 ML 拓扑结构 $T_m$ 的后验概率，并以浮点数形式返回，四舍五入到六位小数。\n\n测试套件 (每个用例列出了 $\\{L_1,L_2,L_3\\}$ 和 $\\{\\pi_1,\\pi_2,\\pi_3\\})$:\n- 用例 $1$ (先验强烈偏好一个非 ML 拓扑结构；中等强度数据):\n  - $\\{L_i\\} = \\{-100.0, -101.0, -102.0\\}$\n  - $\\{\\pi_i\\} = \\{0.05, 0.90, 0.05\\}$\n- 用例 $2$ (均匀先验；数据与用例 1 相同):\n  - $\\{L_i\\} = \\{-100.0, -101.0, -102.0\\}$\n  - $\\{\\pi_i\\} = \\{\\tfrac{1}{3}, \\tfrac{1}{3}, \\tfrac{1}{3}\\}$\n- 用例 $3$ (数据信号极弱；先验极力反对 ML 拓扑结构):\n  - $\\{L_i\\} = \\{-100.0, -100.001, -100.002\\}$\n  - $\\{\\pi_i\\} = \\{0.01, 0.98, 0.01\\}$\n- 用例 $4$ (极强的数据信号压倒了强的先验):\n  - $\\{L_i\\} = \\{-90.0, -100.0, -110.0\\}$\n  - $\\{\\pi_i\\} = \\{0.05, 0.90, 0.05\\}$\n\n输出规范：\n- 对于每个用例，计算 ML 拓扑结构的后验概率 $p(T_m \\mid D)$，结果为浮点数，四舍五入到六位小数。\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表（例如，$[0.123456,0.234567,0.345678,0.456789]$）。", "solution": "所述问题是有效的。它具有科学依据，是良定的，并包含推导唯一且有意义解所需的所有信息。它代表了贝叶斯推断在计算系统发育学领域的标准应用。我们将继续进行推导和求解。\n\n问题的核心在于将贝叶斯定理应用于一组离散的假设，在本情境中，这些假设是给定分类单元的可能无根树拓扑结构。对于 $4$ 个分类单元，存在 $3$ 种这样的唯一拓扑结构。让我们用集合 $\\{T_1, T_2, T_3\\}$ 来表示这些拓扑结构。给定我们观测到的数据 $D$，它通常由对齐的分子序列组成。\n\n**1. 后验概率的推导**\n\n我们从贝叶斯定理开始，对于一个特定的假设 $T_i$ 和数据 $D$，其表述如下：\n$$ p(T_i \\mid D) = \\frac{p(D \\mid T_i) p(T_i)}{p(D)} $$\n此处，各项分别为：\n- $p(T_i \\mid D)$ 是给定数据 $D$ 时拓扑结构 $T_i$ 的后验概率。这是我们希望计算的量。\n- $p(D \\mid T_i)$ 是在拓扑结构 $T_i$ 为真实拓扑结构的情况下，观测到数据 $D$ 的似然。\n- $p(T_i)$ 是拓扑结构 $T_i$ 的先验概率，反映了我们在观测数据之前对该假设的信念。\n- $p(D)$ 是数据的边际似然，或称为证据。它作为一个归一化常数。\n\n边际似然 $p(D)$ 根据全概率定律，通过对所有可能的假设求和来计算：\n$$ p(D) = \\sum_{j=1}^{3} p(D, T_j) = \\sum_{j=1}^{3} p(D \\mid T_j) p(T_j) $$\n将此代回贝叶斯定理，我们得到拓扑结构 $T_i$ 的后验概率的完整表达式：\n$$ p(T_i \\mid D) = \\frac{p(D \\mid T_i) p(T_i)}{\\sum_{j=1}^{3} p(D \\mid T_j) p(T_j)} $$\n问题提供了对数似然 $L_i = \\log p(D \\mid T_i)$ 和先验 $\\pi_i = p(T_i)$。为了使用这些值，我们必须通过取指数将对数似然转换回似然：$p(D \\mid T_i) = \\exp(L_i)$。将这些代入我们的公式可得：\n$$ p(T_i \\mid D) = \\frac{\\exp(L_i) \\pi_i}{\\sum_{j=1}^{3} \\exp(L_j) \\pi_j} $$\n这是任何拓扑结构 $T_i$ 的后验概率的通用表达式。\n\n最大似然（ML）拓扑结构，我们记为 $T_m$，是使似然函数最大化的那一个。由于对数是严格单调函数，最大化似然等价于最大化对数似然。因此，ML 拓扑结构的索引 $m$ 由下式给出：\n$$ m = \\arg\\max_i L_i $$\n这个特定的 ML 拓扑结构 $T_m$ 的后验概率则为：\n$$ p(T_m \\mid D) = \\frac{\\exp(L_m) \\pi_m}{\\sum_{j=1}^{3} \\exp(L_j) \\pi_j} $$\n\n**2. 数值稳定的计算**\n\n所提供的对数似然值 $L_i$ 是很大的负数（例如,$-100.0$）。直接计算 $\\exp(L_i)$ 会得到一个非常接近于零的值，标准的浮点运算会将其舍入为 $0.0$，这种现象称为算术下溢。这将导致分子和分母中的所有项都变为零，从而产生 $0/0$ 的不定式。\n\n为避免此问题，我们采用一种标准的数值稳定技术，通常称为“log-sum-exp”技巧。我们找出最大的对数似然，根据定义即为 $L_m$：$L_m = \\max_j\\{L_j\\}$。然后，我们从后验概率表达式的分子和分母中提出公因子 $\\exp(L_m)$。这不会改变分式的值。\n$$ p(T_i \\mid D) = \\frac{\\exp(L_i) \\pi_i}{\\sum_{j} \\exp(L_j) \\pi_j} = \\frac{\\exp(L_m) \\cdot \\exp(L_i - L_m) \\pi_i}{\\exp(L_m) \\cdot \\sum_{j} \\exp(L_j - L_m) \\pi_j} $$\n消去 $\\exp(L_m)$ 项，我们得到一个数值稳定的公式：\n$$ p(T_i \\mid D) = \\frac{\\exp(L_i - L_m) \\pi_i}{\\sum_{j=1}^{3} \\exp(L_j - L_m) \\pi_j} $$\n在此表达式中，每一项的指数为 $L_j - L_m$。由于 $L_m$ 是最大值，这个差值总是小于或等于零。对于 ML 拓扑结构本身，其指数为 $L_m - L_m = 0$，结果为 $\\exp(0) = 1$。所有其他项 $\\exp(L_j - L_m)$（其中 $j \\neq m$）的计算结果将是一个介于 $0$ 和 $1$ 之间的数，从而防止了下溢。\n\n特别地，对于 ML 拓扑结构 $T_m$ 的后验概率，分子项变为 $\\exp(L_m - L_m) \\pi_m = \\exp(0) \\pi_m = \\pi_m$。我们所求量的最终稳定公式是：\n$$ p(T_m \\mid D) = \\frac{\\pi_m}{\\sum_{j=1}^{3} \\exp(L_j - L_m) \\pi_j} $$\n该表达式构成了我们计算的基础。\n\n**3. 实现与最终计算**\n\n我们现在将实现一个程序，将此公式应用于给定的四个测试用例。目标是计算每个用例的 $p(T_m \\mid D)$。该程序将首先确定最大对数似然 $L_m$ 及其对应的索引 $m$。然后，它将通过对所有三种拓扑结构的项 $\\exp(L_j - L_m) \\pi_j$ 求和来计算分母（归一化常数）。最后，它将使用上面推导的稳定公式计算 ML 拓扑结构的后验概率。结果将展示数据中的似然信号与先验信念之间的相互作用如何决定最终的后验概率，以及强的先验如何导致 ML 拓扑结构在贝叶斯分析后被认为是不太可能的。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the Bayesian phylogenetics problem for the given test cases.\n    It calculates the posterior probability of the Maximum Likelihood (ML) topology.\n    \"\"\"\n\n    # Test cases defined in the problem statement.\n    # Each case is a tuple of (log_likelihoods, priors).\n    test_cases = [\n        # Case 1: Prior strongly favors a non-ML topology; moderate data\n        (np.array([-100.0, -101.0, -102.0]), np.array([0.05, 0.90, 0.05])),\n        # Case 2: Uniform prior; same data as Case 1\n        (np.array([-100.0, -101.0, -102.0]), np.array([1/3, 1/3, 1/3])),\n        # Case 3: Extremely weak data; very strong prior against the ML topology\n        (np.array([-100.0, -100.001, -100.002]), np.array([0.01, 0.98, 0.01])),\n        # Case 4: Very strong data overriding a strong prior\n        (np.array([-90.0, -100.0, -110.0]), np.array([0.05, 0.90, 0.05])),\n    ]\n\n    results = []\n    for log_likelihoods, priors in test_cases:\n        result = calculate_ml_posterior(log_likelihoods, priors)\n        results.append(result)\n\n    # Format the final output as specified.\n    # The map function converts each rounded float to a string for joining.\n    formatted_results = [f\"{r:.6f}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\ndef calculate_ml_posterior(log_likelihoods: np.ndarray, priors: np.ndarray) -> float:\n    \"\"\"\n    Calculates the posterior probability of the Maximum Likelihood (ML) topology\n    using a numerically stable method.\n\n    Args:\n        log_likelihoods: A numpy array of log-likelihoods for each topology.\n        priors: A numpy array of prior probabilities for each topology.\n\n    Returns:\n        The posterior probability of the ML topology as a float.\n    \"\"\"\n    # 1. Identify the Maximum Likelihood (ML) topology and its properties.\n    # The ML topology has the maximum log-likelihood.\n    # np.argmax returns the index of the maximum value.\n    ml_index = np.argmax(log_likelihoods)\n    \n    # Get the maximum log-likelihood value. This is L_m.\n    L_m = log_likelihoods[ml_index]\n    \n    # Get the prior probability for the ML topology. This is pi_m.\n    pi_m = priors[ml_index]\n\n    # 2. Compute the normalization constant (denominator) in a numerically stable way.\n    # The formula for the normalization constant C is:\n    # C = sum_j(exp(L_j - L_m) * pi_j)\n    \n    # Calculate the log-likelihood differences (L_j - L_m).\n    # This prevents underflow when exponentiating.\n    log_likelihood_diffs = log_likelihoods - L_m\n    \n    # Exponentiate the differences and multiply by the priors.\n    terms = np.exp(log_likelihood_diffs) * priors\n    \n    # Sum the terms to get the normalization constant.\n    normalization_constant = np.sum(terms)\n\n    # 3. Calculate the posterior probability of the ML topology.\n    # The stable formula for p(T_m | D) is:\n    # p(T_m | D) = (exp(L_m - L_m) * pi_m) / C = pi_m / C\n    # The numerator of the ML topology's posterior after stabilization is simply its prior.\n    numerator_ml = np.exp(L_m - L_m) * pi_m # which simplifies to pi_m\n    \n    posterior_ml = numerator_ml / normalization_constant\n    \n    return posterior_ml\n    \nsolve()\n```", "id": "2375013"}, {"introduction": "理解了先验和似然的相互作用后，我们来深入探究系统发育中似然计算的内部机制。在贝叶斯分析中，我们关心的是拓扑结构本身，而像枝长和祖先状态这样的参数是我们需要处理的“滋扰参数”。这个练习要求你为一个简单的三分类单元树手工计算边际似然，通过对所有未知的枝长进行积分并对所有可能的根节点状态求和，你将揭开贝叶斯系统发育软件核心计算的神秘面纱。[@problem_id:2375048]", "problem": "考虑一个有根三分类单元树，其叶节点 $A$、$B$ 和 $C$ 通过三条长度分别为 $\\ell_A$、$\\ell_B$ 和 $\\ell_C$ 的未知分支直接连接到一个根节点上。在叶节点上观察到一个二态性状，其状态为 $x_A=0$、$x_B=0$ 和 $x_C=1$。未观察到的根节点状态 $r \\in \\{0,1\\}$ 具有稳态先验 $\\pi(0)=\\pi(1)=\\frac{1}{2}$。沿着长度为 $\\ell \\ge 0$ 的分支，演化遵循由以下转移概率定义的对称二态替换模型：\n$$\np_{\\text{same}}(\\ell) \\;=\\; \\frac{1}{2} + \\frac{1}{2}\\exp(-2\\ell), \n\\qquad\np_{\\text{diff}}(\\ell) \\;=\\; \\frac{1}{2} - \\frac{1}{2}\\exp(-2\\ell),\n$$\n其中 $p_{\\text{same}}(\\ell)$ 是分支末端状态与其起始状态相同的概率，而 $p_{\\text{diff}}(\\ell)$ 是两者不同的概率。分支长度是先验独立的，分别服从 $\\ell_A \\sim \\text{Exponential}(\\beta)$、$\\ell_B \\sim \\text{Exponential}(\\beta)$ 和 $\\ell_C \\sim \\text{Exponential}(\\beta)$，其中 $\\beta>0$ 是已知的，其密度函数为 $p(\\ell \\mid \\beta)=\\beta \\exp(-\\beta \\ell)$（对于 $\\ell \\ge 0$）。\n\n计算在此模型下观测数据的边际似然，\n$$\np(x_A=0, x_B=0, x_C=1 \\mid \\beta),\n$$\n其通过对未知分支长度 $\\ell_A$、$\\ell_B$、$\\ell_C$ 进行积分并对未观察到的根状态 $r$ 进行求和得到。将您的最终答案表示为关于 $\\beta$ 的单个闭式解析表达式。不要对答案进行取整。", "solution": "该问题是良构的，并且在计算生物学领域有其科学基础。所有必要信息均已提供，目标是进行一次清晰的数学计算。我们着手求解。\n\n目标是计算给定超参数 $\\beta$ 时观测数据 $D = \\{x_A=0, x_B=0, x_C=1\\}$ 的边际似然。这通过对未知分支长度 $\\boldsymbol{\\ell} = (\\ell_A, \\ell_B, \\ell_C)$ 进行积分，并对未观察到的根状态 $r \\in \\{0, 1\\}$ 进行求和得到。边际似然由以下公式给出：\n$$\np(D \\mid \\beta) = \\int_0^\\infty \\int_0^\\infty \\int_0^\\infty p(D \\mid \\ell_A, \\ell_B, \\ell_C) \\, p(\\ell_A, \\ell_B, \\ell_C \\mid \\beta) \\, d\\ell_A \\, d\\ell_B \\, d\\ell_C\n$$\n分支长度的先验分布由独立的指数分布给出：\n$$\np(\\ell_A, \\ell_B, \\ell_C \\mid \\beta) = p(\\ell_A \\mid \\beta) p(\\ell_B \\mid \\beta) p(\\ell_C \\mid \\beta) = \\beta^3 \\exp(-\\beta(\\ell_A+\\ell_B+\\ell_C))\n$$\n给定分支长度时数据的似然 $p(D \\mid \\boldsymbol{\\ell})$ 通过对根状态 $r$ 进行边缘化求得：\n$$\np(D \\mid \\boldsymbol{\\ell}) = \\sum_{r \\in \\{0, 1\\}} p(D \\mid r, \\boldsymbol{\\ell}) \\, p(r)\n$$\n给定根状态 $r$ 和分支长度，叶节点上的性状状态是条件独立的。因此，以 $r$ 为条件的似然为：\n$$\np(D \\mid r, \\boldsymbol{\\ell}) = p(x_A \\mid r, \\ell_A) \\, p(x_B \\mid r, \\ell_B) \\, p(x_C \\mid r, \\ell_C)\n$$\n我们考虑两种可能的根状态，使用转移概率 $p_{\\text{same}}(\\ell) = \\frac{1}{2} + \\frac{1}{2}\\exp(-2\\ell)$ 和 $p_{\\text{diff}}(\\ell) = \\frac{1}{2} - \\frac{1}{2}\\exp(-2\\ell)$。\n\n情况1：根状态 $r=0$。\n数据为 $x_A=0$、$x_B=0$ 和 $x_C=1$。\n- 对于叶节点 $A$，其状态与根节点相同 ($0 \\to 0$)。概率为 $p_{\\text{same}}(\\ell_A)$。\n- 对于叶节点 $B$，其状态与根节点相同 ($0 \\to 0$)。概率为 $p_{\\text{same}}(\\ell_B)$。\n- 对于叶节点 $C$，其状态与根节点不同 ($0 \\to 1$)。概率为 $p_{\\text{diff}}(\\ell_C)$。\n此情况下的联合概率为 $p(D \\mid r=0, \\boldsymbol{\\ell}) = p_{\\text{same}}(\\ell_A)p_{\\text{same}}(\\ell_B)p_{\\text{diff}}(\\ell_C)$。\n\n情况2：根状态 $r=1$。\n- 对于叶节点 $A$，其状态与根节点不同 ($1 \\to 0$)。概率为 $p_{\\text{diff}}(\\ell_A)$。\n- 对于叶节点 $B$，其状态与根节点不同 ($1 \\to 0$)。概率为 $p_{\\text{diff}}(\\ell_B)$。\n- 对于叶节点 $C$，其状态与根节点相同 ($1 \\to 1$)。概率为 $p_{\\text{same}}(\\ell_C)$。\n此情况下的联合概率为 $p(D \\mid r=1, \\boldsymbol{\\ell}) = p_{\\text{diff}}(\\ell_A)p_{\\text{diff}}(\\ell_B)p_{\\text{same}}(\\ell_C)$。\n\n总似然 $p(D \\mid \\boldsymbol{\\ell})$ 是这些可能性的和，按根状态的先验概率 $p(r=0) = p(r=1) = \\frac{1}{2}$ 加权：\n$$\np(D \\mid \\boldsymbol{\\ell}) = \\frac{1}{2} p_{\\text{same}}(\\ell_A)p_{\\text{same}}(\\ell_B)p_{\\text{diff}}(\\ell_C) + \\frac{1}{2} p_{\\text{diff}}(\\ell_A)p_{\\text{diff}}(\\ell_B)p_{\\text{same}}(\\ell_C)\n$$\n代入转移概率的定义：\n$$\np(D \\mid \\boldsymbol{\\ell}) = \\frac{1}{2} \\left[ \\left(\\frac{1}{2} + \\frac{1}{2}e^{-2\\ell_A}\\right) \\left(\\frac{1}{2} + \\frac{1}{2}e^{-2\\ell_B}\\right) \\left(\\frac{1}{2} - \\frac{1}{2}e^{-2\\ell_C}\\right) + \\left(\\frac{1}{2} - \\frac{1}{2}e^{-2\\ell_A}\\right) \\left(\\frac{1}{2} - \\frac{1}{2}e^{-2\\ell_B}\\right) \\left(\\frac{1}{2} + \\frac{1}{2}e^{-2\\ell_C}\\right) \\right]\n$$\n从每一项中提出因子 $\\frac{1}{2}$：\n$$\np(D \\mid \\boldsymbol{\\ell}) = \\frac{1}{2} \\cdot \\left(\\frac{1}{8}\\right) \\left[ (1+e^{-2\\ell_A})(1+e^{-2\\ell_B})(1-e^{-2\\ell_C}) + (1-e^{-2\\ell_A})(1-e^{-2\\ell_B})(1+e^{-2\\ell_C}) \\right]\n$$\n令 $u_i = \\exp(-2\\ell_i)$。方括号中的表达式为 $(1+u_A)(1+u_B)(1-u_C) + (1-u_A)(1-u_B)(1+u_C)$。\n展开第一项：$(1+u_A+u_B+u_Au_B)(1-u_C) = 1+u_A+u_B-u_C+u_Au_B-u_Au_C-u_Bu_C-u_Au_Bu_C$。\n展开第二项：$(1-u_A-u_B+u_Au_B)(1+u_C) = 1-u_A-u_B+u_C+u_Au_B-u_Au_C-u_Bu_C+u_Au_Bu_C$。\n将这两个展开式相加得到 $2+2u_Au_B-2u_Au_C-2u_Bu_C$。\n代回到似然表达式中：\n$$\np(D \\mid \\boldsymbol{\\ell}) = \\frac{1}{16} \\left( 2 + 2e^{-2\\ell_A}e^{-2\\ell_B} - 2e^{-2\\ell_A}e^{-2\\ell_C} - 2e^{-2\\ell_B}e^{-2\\ell_C} \\right)\n$$\n$$\np(D \\mid \\boldsymbol{\\ell}) = \\frac{1}{8} \\left( 1 + e^{-2(\\ell_A+\\ell_B)} - e^{-2(\\ell_A+\\ell_C)} - e^{-2(\\ell_B+\\ell_C)} \\right)\n$$\n现在，我们通过用先验 $p(\\boldsymbol{\\ell} \\mid \\beta)$ 对 $p(D \\mid \\boldsymbol{\\ell})$ 进行积分来计算边际似然：\n$$\np(D \\mid \\beta) = \\int_0^\\infty \\int_0^\\infty \\int_0^\\infty \\frac{1}{8} \\left( 1 + e^{-2(\\ell_A+\\ell_B)} - e^{-2(\\ell_A+\\ell_C)} - e^{-2(\\ell_B+\\ell_C)} \\right) \\beta^3 e^{-\\beta(\\ell_A+\\ell_B+\\ell_C)} \\,d\\ell_A \\,d\\ell_B \\,d\\ell_C\n$$\n该积分可分解为四项。我们使用以下通用定积分：\n$$\nI(\\alpha) = \\int_0^\\infty e^{-\\alpha\\ell} \\beta e^{-\\beta\\ell} \\,d\\ell = \\beta \\int_0^\\infty e^{-(\\alpha+\\beta)\\ell} \\,d\\ell = \\beta \\left[ \\frac{-1}{\\alpha+\\beta}e^{-(\\alpha+\\beta)\\ell} \\right]_0^\\infty = \\frac{\\beta}{\\alpha+\\beta}\n$$\n对于没有额外指数项的分支，其积分对应于 $\\alpha=0$，因此 $I(0) = \\frac{\\beta}{\\beta} = 1$。对于带有 $e^{-2\\ell}$ 项的分支，其积分对应于 $\\alpha=2$，因此 $I(2)=\\frac{\\beta}{\\beta+2}$。\n总边际似然为：\n$$\np(D \\mid \\beta) = \\frac{1}{8} \\left[ \\iiint_V 1 \\cdot p(\\boldsymbol{\\ell} \\mid \\beta) dV + \\iiint_V e^{-2(\\ell_A+\\ell_B)} p(\\boldsymbol{\\ell} \\mid \\beta) dV - \\iiint_V e^{-2(\\ell_A+\\ell_C)} p(\\boldsymbol{\\ell} \\mid \\beta) dV - \\iiint_V e^{-2(\\ell_B+\\ell_C)} p(\\boldsymbol{\\ell} \\mid \\beta) dV \\right]\n$$\n其中 $dV = d\\ell_A d\\ell_B d\\ell_C$ 且 $p(\\boldsymbol{\\ell} \\mid \\beta) = p(\\ell_A|\\beta)p(\\ell_B|\\beta)p(\\ell_C|\\beta)$。\n每个三重积分都是三个一维积分的乘积：\n1.  第一项：$\\frac{1}{8} [I(0) \\cdot I(0) \\cdot I(0)] = \\frac{1}{8} [1 \\cdot 1 \\cdot 1] = \\frac{1}{8}$。\n2.  第二项：$\\frac{1}{8} [I(2) \\cdot I(2) \\cdot I(0)] = \\frac{1}{8} \\left[\\frac{\\beta}{\\beta+2} \\cdot \\frac{\\beta}{\\beta+2} \\cdot 1\\right] = \\frac{1}{8} \\left(\\frac{\\beta}{\\beta+2}\\right)^2$。\n3.  第三项：$-\\frac{1}{8} [I(2) \\cdot I(0) \\cdot I(2)] = -\\frac{1}{8} \\left[\\frac{\\beta}{\\beta+2} \\cdot 1 \\cdot \\frac{\\beta}{\\beta+2}\\right] = -\\frac{1}{8} \\left(\\frac{\\beta}{\\beta+2}\\right)^2$。\n4.  第四项：$-\\frac{1}{8} [I(0) \\cdot I(2) \\cdot I(2)] = -\\frac{1}{8} \\left[1 \\cdot \\frac{\\beta}{\\beta+2} \\cdot \\frac{\\beta}{\\beta+2}\\right] = -\\frac{1}{8} \\left(\\frac{\\beta}{\\beta+2}\\right)^2$。\n\n将这些项相加：\n$$\np(D \\mid \\beta) = \\frac{1}{8} + \\frac{1}{8}\\left(\\frac{\\beta}{\\beta+2}\\right)^2 - \\frac{1}{8}\\left(\\frac{\\beta}{\\beta+2}\\right)^2 - \\frac{1}{8}\\left(\\frac{\\beta}{\\beta+2}\\right)^2 = \\frac{1}{8} \\left[1 - \\left(\\frac{\\beta}{\\beta+2}\\right)^2\\right]\n$$\n我们简化此表达式：\n$$\np(D \\mid \\beta) = \\frac{1}{8} \\left[ \\frac{(\\beta+2)^2 - \\beta^2}{(\\beta+2)^2} \\right] = \\frac{1}{8} \\left[ \\frac{\\beta^2+4\\beta+4 - \\beta^2}{(\\beta+2)^2} \\right] = \\frac{1}{8} \\left[ \\frac{4\\beta+4}{(\\beta+2)^2} \\right]\n$$\n$$\np(D \\mid \\beta) = \\frac{4(\\beta+1)}{8(\\beta+2)^2} = \\frac{\\beta+1}{2(\\beta+2)^2}\n$$\n这就是边际似然的最终闭式表达式。", "answer": "$$\\boxed{\\frac{\\beta+1}{2(\\beta+2)^{2}}}$$", "id": "2375048"}, {"introduction": "掌握了贝叶斯推断的基本方程和似然计算的理论后，让我们将所有知识整合到一个模拟研究中，来验证贝叶斯方法的强大之处。这个综合练习将引导你模拟DNA序列数据，并展示贝叶斯推断的一个关键特性：一致性。你将亲眼见证，随着数据量（即序列长度）的增加，后验概率如何更加集中于生成数据的“真实”拓扑结构，从而更有力地证明统计系统发育学的威力。[@problem_id:2375068]", "problem": "给定一个包含四个标记为 $A$、$B$、$C$和$D$的分类单元的二元无根系统发育树。真实的无根拓扑结构为 $T_{\\text{true}} = ((A,B),(C,D))$。所有分支长度均在核苷酸的连续时间马尔可夫链 (CTMC) 和 Jukes–Cantor 模型下，以每个位点的预期替换数来衡量。设叶分支长度为 $l_A = 0.2$、$l_B = 0.2$、$l_C = 0.2$、$l_D = 0.2$，唯一的内部边长度为 $l_{\\text{int}} = 0.1$。考虑一种有根表示，通过将根置于内部边的中点形成，使得从根到两个内部节点的长度各为 $l_{\\text{int}}/2$。\n\n替换过程是 Jukes–Cantor 模型，速率为每单位分支长度 $\\mu = 1$。状态空间为 $\\{A,C,G,T\\}$。沿着长度为 $t$ 的分支，从状态 $i$ 转移到状态 $j$ 的概率为\n$$\nP_{ij}(t) =\n\\begin{cases}\n\\frac{1}{4} + \\frac{3}{4} e^{- \\frac{4}{3} t} & \\text{如果 } i = j, \\\\\n\\frac{1}{4} - \\frac{1}{4} e^{- \\frac{4}{3} t} & \\text{如果 } i \\ne j,\n\\end{cases}\n$$\n且平稳分布为均匀分布，即对所有 $i$，$\\pi_i = \\frac{1}{4}$。\n\n数据生成：对于每个位点 $k \\in \\{1,\\dots,L\\}$，从平稳分布中抽取根状态 $R_k$。状态沿树向下传播，位点间相互独立，子状态根据其父状态，沿着长度为 $t$ 的分支使用 $P(t)$ 进行采样。观测数据是分类单元 $A$、$B$、$C$ 和 $D$ 在 $k = 1,\\dots,L$ 时的叶状态 $(X_{A,k}, X_{B,k}, X_{C,k}, X_{D,k})$。\n\n推断：考虑这四个分类单元上的三个无根二元拓扑结构集合，\n$$\n\\mathcal{T} = \\{T_1=((A,B),(C,D)),\\; T_2=((A,C),(B,D)),\\; T_3=((A,D),(B,C))\\}.\n$$\n为这三个拓扑结构分配一个均匀先验，使得对每个 $i \\in \\{1,2,3\\}$，$\\Pr(T_i)=\\frac{1}{3}$。对于任何拓扑结构 $T \\in \\mathcal{T}$，其在给定长度为 $L$ 的序列比对下的似然定义为，在 Jukes–Cantor 模型下，使用相同的分支长度 $l_A, l_B, l_C, l_D, l_{\\text{int}}$ 且根位于内部边中点（因此与根相连的两条边的长度均为 $l_{\\text{int}}/2$）时，所有位点似然的乘积。对于一个观测到叶状态为 $(x_A, x_B, x_C, x_D)$ 的单位点，其位点似然是根处的平稳权重与沿所有分支到叶的转移概率的乘积，再对所有内部节点和根状态求和的结果。在 $\\mathcal{T}$ 上的后验与先验乘以似然成正比，并且必须进行归一化，以使三个拓扑结构上的后验概率之和为 $1$。\n\n任务：在 $T_{\\text{true}}$ 下模拟序列数据，并为以下每个测试用例计算分配给 $T_{\\text{true}}$ 的后验概率。为保证可复现性，在模拟每个数据集之前，使用该测试用例指定的整数种子 $s$ 初始化伪随机数生成器。测试套件是数对 $(L,s)$ 的集合：\n$$\n\\{(1,7),\\; (25,11),\\; (100,13),\\; (400,17),\\; (1200,19)\\}.\n$$\n\n要求：\n- 在模拟和似然计算中均使用上述指定的 Jukes–Cantor 模型。\n- 对所有拓扑结构，均使用相同的分支长度 $l_A = l_B = l_C = l_D = 0.2$ 和 $l_{\\text{int}} = 0.1$，并将根置于内部边的中点。\n- 假设各位点是独立同分布的。\n- 后验必须根据似然和三个拓扑结构上的均匀先验精确计算。\n\n你的程序应生成单行输出，其中包含一个由方括号括起来的逗号分隔列表，列表中的每个值是按上述顺序列出的每个测试用例分配给 $T_{\\text{true}}$ 的后验概率，并四舍五入到六位小数。例如，输出格式必须是\n$$\n[\\text{p}_1,\\text{p}_2,\\text{p}_3,\\text{p}_4,\\text{p}_5],\n$$\n其中 $\\text{p}_i$ 是第 $i$ 个测试用例中 $T_{\\text{true}}$ 的后验概率，四舍五入到六位小数。本问题无需报告物理单位。不涉及角度。所有最终数值答案必须为小数。", "solution": "所提供的问题陈述已经过严格验证，被认为是科学上合理、定义明确且客观的。它提出了计算系统发育学中的一个标准任务：在给定序列数据的情况下，使用贝叶斯推断来评估一组竞争性树拓扑结构的后验概率。该问题指定了一个完整的数据生成模型（在固定的真实拓扑结构上的 Jukes-Cantor 模型）和一个精确的推断框架（在拓扑结构上使用均匀先验和固定分支长度的贝叶斯模型选择）。所有参数都已明确提供。因此，我们将着手进行形式化的求解。\n\n问题的核心是，在给定一个从真实拓扑结构 $T_{\\text{true}} = ((A,B),(C,D))$ 模拟出的 DNA 序列比对的情况下，计算该真实拓扑结构的后验概率。该后验概率使用贝叶斯定理计算。四个分类单元（$A, B, C, D$）的可能无根拓扑结构集合为 $\\mathcal{T} = \\{T_1, T_2, T_3\\}$，其中 $T_1 = ((A,B),(C,D))$，$T_2 = ((A,C),(B,D))$，以及 $T_3 = ((A,D),(B,C))$。\n\n在给定观测序列比对数据 $D$ 的情况下，拓扑结构 $T_i \\in \\mathcal{T}$ 的后验概率由以下公式给出：\n$$\n\\Pr(T_i | D) = \\frac{\\Pr(D | T_i) \\Pr(T_i)}{\\sum_{j=1}^{3} \\Pr(D | T_j) \\Pr(T_j)}\n$$\n问题指定了拓扑结构上的均匀先验，$\\Pr(T_i) = 1/3$ 对于 $i \\in \\{1, 2, 3\\}$。这使得后验概率简化为与似然 $\\Pr(D|T_i)$ 成正比，我们将其表示为 $L(T_i|D)$：\n$$\n\\Pr(T_i | D) = \\frac{L(T_i | D)}{\\sum_{j=1}^{3} L(T_j | D)}\n$$\n比对中的位点被假定为独立同分布（i.i.d.）。因此，长度为 $L$ 的比对 $D$ 的总似然是每个位点 $D_k$ 的似然的乘积：\n$$\nL(T_i | D) = \\prod_{k=1}^{L} L(T_i | D_k)\n$$\n其中 $D_k = (x_{A,k}, x_{B,k}, x_{C,k}, x_{D,k})$ 表示在位点 $k$ 处四个分类单元上观测到的核苷酸。\n\n替换过程遵循 Jukes-Cantor 模型。沿着长度为 $t$ 的分支，从状态 $i$ 转移到状态 $j$ 的概率由矩阵 $P(t)$ 给出，其元素为：\n$$\nP_{ij}(t) =\n\\begin{cases}\n\\frac{1}{4} + \\frac{3}{4} e^{- \\frac{4}{3} t} & \\text{如果 } i = j \\\\\n\\frac{1}{4} - \\frac{1}{4} e^{- \\frac{4}{3} t} & \\text{如果 } i \\ne j\n\\end{cases}\n$$\n该模型假设核苷酸的平稳分布是均匀的，$\\pi = (\\frac{1}{4}, \\frac{1}{4}, \\frac{1}{4}, \\frac{1}{4})$。无论是模拟还是推断，都需要一棵有根树。这通过将根置于长度为 $l_{\\text{int}} = 0.1$ 的唯一内部边的中点来形成。这导致从根发出两条长度为 $l_{\\text{int}}/2 = 0.05$ 的内部分支。通向叶节点的叶分支长度均为 $0.2$。\n\n位点似然 $L(T_i | D_k)$ 使用 Felsenstein 的剪枝算法计算。该算法能有效地对树中所有内部节点的可能状态进行求和。对于给定的拓扑结构 $T_i$ 和位点 $k$，该算法计算每个节点的部分似然，从叶节点向根节点移动。\n设 $L_u(s)$ 为节点 $u$ 在状态 $s$ 时的部分似然。\n1.  **叶节点：** 在叶节点 $u \\in \\{A, B, C, D\\}$，部分似然向量根据观测数据进行初始化。例如，对于叶节点 $A$，如果观测到的状态是 $x_{A,k}$，那么 $L_A(s) = \\delta_{s, x_{A,k}}$，其中 $\\delta$ 是克罗内克 δ。这可以表示为一个独热向量。\n2.  **内部节点：** 对于一个内部节点 $u$，其子节点为 $v$ 和 $w$，通过长度为 $t_v$ 和 $t_w$ 的分支相连，其部分似然为：\n    $$\n    L_u(s) = \\left( \\sum_{s_v} P_{s,s_v}(t_v) L_v(s_v) \\right) \\left( \\sum_{s_w} P_{s,s_w}(t_w) L_w(s_w) \\right)\n    $$\n    在矩阵-向量表示法中，如果 $L_v$ 和 $L_w$ 是子节点的部分似然列向量，这可以表示为：\n    $$\n    L_u = (P(t_v)^T L_v) \\odot (P(t_w)^T L_w)\n    $$\n    其中 `T` 表示转置，$\\odot$ 表示逐元素乘积。\n3.  **根节点：** 这个过程一直持续到根节点 $R$。然后，该位点的总似然是根节点处的部分似然与平稳分布 $\\pi$ 加权后的总和：\n    $$\n    L(T_i | D_k) = \\sum_{s} \\pi_s L_R(s) = \\pi^T L_R\n    $$\n拓扑结构 $T_i$ 决定了哪些叶节点是姐妹节点。例如，在 $T_1 = ((A,B),(C,D))$ 下，内部节点分别是 $(A,B)$ 和 $(C,D)$ 的父节点。在 $T_2 = ((A,C),(B,D))$ 下，它们是 $(A,C)$ 和 $(B,D)$ 的父节点。所有拓扑结构的分支长度都是相同的：叶分支长度为 $0.2$，从根向下的两条分支长度为 $0.05$。\n\n总体流程如下：\n对于每个测试用例 $(L, s)$：\n1.  用种子 $s$ 初始化伪随机数生成器。\n2.  模拟一个长度为 $L$ 的序列比对。对于每个位点，从 $\\pi$ 中抽取一个根状态，然后使用转移概率 $P(t)$ 沿真实的有根树 $T_1 = ((A,B),(C,D))$ 向下传播状态。\n3.  计算三种拓扑结构中每一种的总对数似然，$\\log L(T_i|D) = \\sum_{k=1}^{L} \\log L(T_i | D_k)$。当 $L$ 很大时，使用对数对于避免数值下溢至关重要。\n4.  根据对数似然计算后验概率。为保持数值稳定性，我们使用 log-sum-exp 技巧。设 $\\ell_i = \\log L(T_i|D)$ 且 $\\ell_{\\max} = \\max(\\ell_1, \\ell_2, \\ell_3)$。真实拓扑结构 $T_1$ 的后验概率为：\n    $$\n    \\Pr(T_1 | D) = \\frac{e^{\\ell_1}}{e^{\\ell_1} + e^{\\ell_2} + e^{\\ell_3}} = \\frac{e^{\\ell_1 - \\ell_{\\max}}}{e^{\\ell_1 - \\ell_{\\max}} + e^{\\ell_2 - \\ell_{\\max}} + e^{\\ell_3 - \\ell_{\\max}}}\n    $$\n最终实现的程序对每个指定的测试用例执行此流程，并报告 $T_1$ 的后验概率，四舍五入到六位小数。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the Bayesian phylogenetics problem by simulating sequence data and\n    computing the posterior probability of the true tree topology.\n    \"\"\"\n    # Define problem constants\n    N_STATES = 4\n    STATES = np.arange(N_STATES)  # 0:A, 1:C, 2:G, 3:T\n    STATIONARY_PI = np.full(N_STATES, 1.0 / N_STATES)\n\n    # Branch lengths\n    PENDANT_T = 0.2\n    INTERNAL_T = 0.1 / 2.0\n\n    # Define the three unrooted topologies for 4 taxa {A, B, C, D}\n    # These are mapped to indices {0, 1, 2, 3}\n    # T1 = ((A,B),(C,D)), T2 = ((A,C),(B,D)), T3 = ((A,D),(B,C))\n    TOPOLOGIES = [\n        (((0, 1), (2, 3))),  # T1, the true topology\n        (((0, 2), (1, 3))),  # T2\n        (((0, 3), (1, 2))),  # T3\n    ]\n\n    def get_jc_p_matrix(t):\n        \"\"\"\n        Calculates the Jukes-Cantor transition probability matrix for a branch of length t.\n        \"\"\"\n        p_ii = 0.25 + 0.75 * np.exp(-4.0 / 3.0 * t)\n        p_ij = 0.25 - 0.25 * np.exp(-4.0 / 3.0 * t)\n        P = np.full((N_STATES, N_STATES), p_ij)\n        np.fill_diagonal(P, p_ii)\n        return P\n\n    # Pre-calculate transition matrices for the given branch lengths\n    P_PENDANT = get_jc_p_matrix(PENDANT_T)\n    P_INTERNAL = get_jc_p_matrix(INTERNAL_T)\n\n    def simulate_site():\n        \"\"\"\n        Simulates a single site down the true tree topology T1=((A,B),(C,D)).\n        \"\"\"\n        # 1. Sample root state from the stationary distribution\n        root_state = np.random.choice(STATES, p=STATIONARY_PI)\n\n        # 2. Propagate states to internal nodes N1 (ancestor of A,B) and N2 (ancestor of C,D)\n        n1_state = np.random.choice(STATES, p=P_INTERNAL[root_state, :])\n        n2_state = np.random.choice(STATES, p=P_INTERNAL[root_state, :])\n\n        # 3. Propagate states to leaf nodes A, B, C, D\n        a_state = np.random.choice(STATES, p=P_PENDANT[n1_state, :])\n        b_state = np.random.choice(STATES, p=P_PENDANT[n1_state, :])\n        c_state = np.random.choice(STATES, p=P_PENDANT[n2_state, :])\n        d_state = np.random.choice(STATES, p=P_PENDANT[n2_state, :])\n\n        return [a_state, b_state, c_state, d_state]\n\n    def calculate_site_likelihood(site_data, topology):\n        \"\"\"\n        Calculates the likelihood of a single site for a given topology using \n        Felsenstein's pruning algorithm.\n        \"\"\"\n        # Initialize partial likelihoods at the leaves (one-hot vectors)\n        leaf_likelihoods = np.zeros((4, N_STATES))\n        leaf_likelihoods[0, site_data[0]] = 1.0  # Taxon A\n        leaf_likelihoods[1, site_data[1]] = 1.0  # Taxon B\n        leaf_likelihoods[2, site_data[2]] = 1.0  # Taxon C\n        leaf_likelihoods[3, site_data[3]] = 1.0  # Taxon D\n\n        # Get leaf pairings for the two internal nodes from the topology definition\n        (p1_left_idx, p1_right_idx), (p2_left_idx, p2_right_idx) = topology\n\n        # Calculate partial likelihoods for internal node 1\n        L_n1_from_left = P_PENDANT.T @ leaf_likelihoods[p1_left_idx]\n        L_n1_from_right = P_PENDANT.T @ leaf_likelihoods[p1_right_idx]\n        L_n1 = L_n1_from_left * L_n1_from_right\n\n        # Calculate partial likelihoods for internal node 2\n        L_n2_from_left = P_PENDANT.T @ leaf_likelihoods[p2_left_idx]\n        L_n2_from_right = P_PENDANT.T @ leaf_likelihoods[p2_right_idx]\n        L_n2 = L_n2_from_left * L_n2_from_right\n\n        # Propagate likelihoods to the root\n        L_root_from_n1 = P_INTERNAL.T @ L_n1\n        L_root_from_n2 = P_INTERNAL.T @ L_n2\n        L_root = L_root_from_n1 * L_root_from_n2\n\n        # Final site likelihood is the dot product with the stationary distribution\n        site_likelihood = STATIONARY_PI @ L_root\n        return site_likelihood\n\n    test_cases = [\n        (1, 7),\n        (25, 11),\n        (100, 13),\n        (400, 17),\n        (1200, 19),\n    ]\n\n    results = []\n    for L, seed in test_cases:\n        # Set the random seed for reproducibility\n        np.random.seed(seed)\n\n        # 1. Simulate sequence alignment of length L\n        alignment = [simulate_site() for _ in range(L)]\n\n        # 2. Calculate log-likelihoods for each of the three topologies\n        log_likelihoods = np.zeros(len(TOPOLOGIES))\n        for i, topo in enumerate(TOPOLOGIES):\n            total_log_lik = 0.0\n            for site in alignment:\n                site_lik = calculate_site_likelihood(site, topo)\n                if site_lik > 0:\n                    total_log_lik += np.log(site_lik)\n                else:\n                    total_log_lik = -np.inf # Should not happen in practice\n            log_likelihoods[i] = total_log_lik\n\n        # 3. Compute posterior probabilities from log-likelihoods\n        # Use log-sum-exp trick for numerical stability\n        max_log_lik = np.max(log_likelihoods)\n        shifted_log_liks = log_likelihoods - max_log_lik\n        likelihoods = np.exp(shifted_log_liks)\n        \n        # With a uniform prior, posterior is proportional to likelihood\n        sum_likelihoods = np.sum(likelihoods)\n        posteriors = likelihoods / sum_likelihoods\n\n        # The posterior for T_true is the first one in the list\n        posterior_T1 = posteriors[0]\n        results.append(posterior_T1)\n\n    # Format the results to exactly six decimal places and print\n    formatted_results = [\"{:.6f}\".format(res) for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "2375068"}]}