{"hands_on_practices": [{"introduction": "有时，一个直观上看似合理的算法在特定情况下可能会表现得非常糟糕。本练习将挑战你分析一个针对0-1背包问题的“反向贪心”策略，并通过构建一个反例来揭示其最坏情况下的性能。这个实践将锻炼你找出算法“致命弱点”的能力，这是算法分析中至关重要的一环。", "problem": "考虑标准的$0$-$1$背包问题：给定$n$个物品，其中物品$i$具有正价值$v_i$和正重量$w_i$，以及一个背包容量$C>0$，目标是选择一个子集$S \\subseteq \\{1,\\dots,n\\}$，使得在满足约束条件$\\sum_{i \\in S} w_i \\le C$的情况下，最大化$\\sum_{i \\in S} v_i$。令物品$i$的价值重量比为$r_i = \\frac{v_i}{w_i}$。\n\n分析以下用于$0$-$1$背包问题的“逆向贪心”算法：从包含所有物品的集合$S$开始，每次迭代地移除一个物品，总是移除$S$中剩余物品里当前比率$r_i$最小的那个，直到总重量$\\sum_{i \\in S} w_i \\le C$。然后算法返回剩余的集合$S$。\n\n使用$0$-$1$背包问题、价值重量比和近似比的核心定义，推导该算法的最坏情况近似比。算法的近似比定义为$\\inf_{I} \\frac{\\mathrm{ALG}(I)}{\\mathrm{OPT}(I)}$，其中$\\mathrm{ALG}(I)$是算法在实例$I$上产生的值，而$\\mathrm{OPT}(I)$是实例$I$的最优值。你的推导必须从这些定义出发，并构造一个科学上合理的实例族来证明这个界限。将你的最终答案表示为一个实数。无需四舍五入。", "solution": "对问题陈述进行验证。\n\n### 步骤1：提取已知条件\n- **问题类型**：$0$-$1$背包问题。\n- **输入**：一个包含$n$个物品的集合，其中物品$i \\in \\{1, \\dots, n\\}$具有正价值$v_i > 0$和正重量$w_i > 0$。一个背包容量$C > 0$。\n- **目标**：对于物品子集$S$，在满足约束条件$\\sum_{i \\in S} w_i \\le C$的情况下，最大化总价值$\\sum_{i \\in S} v_i$。\n- **定义**：物品$i$的价值重量比为$r_i = \\frac{v_i}{w_i}$。\n- **待分析算法（“逆向贪心”）**：\n    1. 初始化解集$S$为包含所有物品，$S = \\{1, \\dots, n\\}$。\n    2. 当$S$中物品的总重量超过容量时，即$\\sum_{i \\in S} w_i > C$：\n        a. 找出$S$中当前所有物品中价值重量比最小的物品$j$，使得$r_j = \\min_{k \\in S} \\{r_k\\}$。\n        b. 从$S$中移除物品$j$。\n    3. 算法返回最终集合$S$。\n- **评估指标**：最坏情况近似比，定义为$\\inf_{I} \\frac{\\mathrm{ALG}(I)}{\\mathrm{OPT}(I)}$，其中$I$代表问题的一个实例，$\\mathrm{ALG}(I)$是逆向贪心算法得到的值，而$\\mathrm{OPT}(I)$是最优值。\n\n### 步骤2：使用提取的已知条件进行验证\n- **科学依据**：该问题位于理论计算机科学这一成熟领域，具体来说是近似算法分析。$0$-$1$背包问题是一个经典的NP难优化问题。所使用的所有概念——价值、重量、容量、近似比——都是标准的且有严格定义。所提出的“逆向贪心”算法是一个貌似可行的启发式算法，其性能可以进行数学分析。该问题是科学上合理的。\n- **良定性**：问题陈述清晰。它明确了算法逻辑、目标函数、约束条件以及待推导量（最坏情况近似比）的精确定义。期望得到一个唯一的、有意义的数值答案。\n- **客观性**：问题使用精确、形式化的语言描述，没有主观或含糊的术语。\n- **结论**：该问题是自洽的、一致的且结构形式化的。它没有违反任何无效性标准。\n\n### 步骤3：裁定与行动\n该问题是**有效的**。将提供完整解答。\n\n### 解答推导\n设$\\mathrm{ALG}(I)$为逆向贪心算法针对给定问题实例$I$所选物品的总价值，设$\\mathrm{OPT}(I)$为该实例可能的最大价值（最优解）。算法的近似比是在所有可能的实例$I$上，比率$\\frac{\\mathrm{ALG}(I)}{\\mathrm{OPT}(I)}$的下确界。\n\n为了确定最坏情况近似比，我们试图构造一个使该比率最小化的实例族。逆向贪心算法的策略是通过首先移除低比率的物品来保留高价值重量比（$r_i = v_i/w_i$）的物品。该算法的一个失败案例可能会涉及这样一种情景：一个比率较低的物品本身却非常有价值，并且是最优解的关键部分。该算法根据其设计，可能会丢弃这个物品。\n\n考虑以下由一个小的正实数$\\epsilon$参数化的问题实例族。设背包容量$C$为任何大于$1$的值。实例$I_\\epsilon$包含两个物品：\n\n- **物品1**：$v_1 = 1$，$w_1 = C$。\n- **物品2**：$v_2 = \\epsilon$，$w_2 = \\epsilon$。\n\n我们要求$v_i > 0$，$w_i > 0$和$C > 0$。通过选择$C > 1$和$0 < \\epsilon < 1$，这些条件得到满足。\n\n让我们分析逆向贪心算法的行为，并确定此实例$I_\\epsilon$的最优解。\n\n**1. 算法的性能（$\\mathrm{ALG}(I_\\epsilon)$）：**\n首先，我们计算这两个物品的价值重量比：\n- $r_1 = \\frac{v_1}{w_1} = \\frac{1}{C}$\n- $r_2 = \\frac{v_2}{w_2} = \\frac{\\epsilon}{\\epsilon} = 1$\n\n因为我们选择了$C > 1$，所以$\\frac{1}{C} < 1$，因此$r_1 < r_2$。\n\n算法从集合$S = \\{1, 2\\}$开始。总重量为$W_{total} = w_1 + w_2 = C + \\epsilon$。\n由于$W_{total} > C$，算法必须移除一个物品。根据其定义，它移除比率最小的物品。因为$r_1 < r_2$，所以物品1被移除。\n\n得到的集合是$S_{ALG} = \\{2\\}$。该集合的总重量是$w_2 = \\epsilon$。由于我们选择了$\\epsilon < 1$和$C > 1$，我们有$w_2 < C$，所以重量约束被满足，算法终止。\n算法获得的值是物品2的价值：\n$$ \\mathrm{ALG}(I_\\epsilon) = v_2 = \\epsilon $$\n\n**2. 最优解（$\\mathrm{OPT}(I_\\epsilon)$）：**\n我们现在通过考虑所有可行的物品子集来找到最优解。\n- **子集{1}**：重量为$w_1 = C$，满足容量约束$w_1 \\le C$。价值为$v_1 = 1$。这是一个可行解。\n- **子集{2}**：重量为$w_2 = \\epsilon$。由于$\\epsilon < C$，这也是一个可行解。价值为$v_2 = \\epsilon$。\n- **子集{1, 2}**：重量为$w_1 + w_2 = C + \\epsilon$，大于$C$。该子集不可行。\n- **子集$\\emptyset$**：价值为$0$。\n\n最优值是所有可行子集中的最大价值：\n$$ \\mathrm{OPT}(I_\\epsilon) = \\max(\\{1, \\epsilon\\}) $$\n因为我们选择了$0 < \\epsilon < 1$，所以最大值是$1$。\n$$ \\mathrm{OPT}(I_\\epsilon) = 1 $$\n\n**3. 近似比计算：**\n对于实例$I_\\epsilon$，比率为：\n$$ \\frac{\\mathrm{ALG}(I_\\epsilon)}{\\mathrm{OPT}(I_\\epsilon)} = \\frac{\\epsilon}{1} = \\epsilon $$\n\n我们要找的是最坏情况近似比，即在所有可能实例上该比率的下确界。我们构造的实例族$I_\\epsilon$表明，对于任何任意小的正数$\\delta$，我们可以选择$\\epsilon = \\delta$（只要$\\delta < 1$）来创建一个实例，使得其近似比为$\\delta$。\n\n形式上，我们的实例族可能产生的比率集合是$\\{ \\epsilon \\mid 0 < \\epsilon < 1 \\}$。这个集合的下确界是：\n$$ \\inf_{\\epsilon \\in (0,1)} \\{\\epsilon\\} = 0 $$\n由于任何实例的近似比都必须是非负的（因为所有价值$v_i$都是正的），并且我们已经证明该比率可以任意接近$0$，所以该算法的最坏情况近似比是$0$。\n$$ \\inf_I \\frac{\\mathrm{ALG}(I)}{\\mathrm{OPT}(I)} = 0 $$\n这证明了不存在常数$c > 0$使得$\\mathrm{ALG}(I) \\ge c \\cdot \\mathrm{OPT}(I)$对所有实例$I$都成立。与最优解相比，该算法的性能可以任意差。", "answer": "$$\\boxed{0}$$", "id": "3207612"}, {"introduction": "在看到一个算法如何失效后，我们现在转向分析一个虽然不是最优但却很成功的算法。集合覆盖问题的贪心算法是有效近似算法的经典范例。本练习将引导你完成其紧密近似比 $H_m$ 的经典证明，展示一个精心构造的实例如何证明一个理论界限是无法被改进的。", "problem": "您将分析加权集合覆盖（Weighted Set Cover）问题的贪心算法，以构造一个实例族，在该实例族上，其近似比在极限情况下等于第 $m$ 个调和数。从基本定义开始。\n\n定义：\n- 在加权集合覆盖问题中，给定一个包含 $m$ 个元素的有限全集 $U$，$U$ 的一个子集族 $\\mathcal{S}$，以及一个成本函数 $c : \\mathcal{S} \\to \\mathbb{R}_{>0}$。目标是找到一个子集族 $\\mathcal{C} \\subseteq \\mathcal{S}$，它能覆盖 $U$ (即 $\\bigcup_{S \\in \\mathcal{C}} S = U$)，且总成本 $\\sum_{S \\in \\mathcal{C}} c(S)$ 最小。\n- 贪心算法重复选择一个集合 $S \\in \\mathcal{S}$，该集合使得比率 $c(S) / |S \\setminus C|$ 最小，其中 $C$ 表示已覆盖元素的集合，并任意处理相等情况，直到 $U$ 中的所有元素都被覆盖。\n- 一个实例上的近似比是该实例上贪心算法的总成本与最优总成本的比值。\n- 第 $m$ 个调和数是 $H_m \\triangleq \\sum_{i=1}^{m} \\frac{1}{i}$。\n\n对于给定的整数 $m \\geq 2$ 和参数 $\\delta$（$0 < \\delta < 1$），考虑以下实例 $(U,\\mathcal{S},c)$：\n- 全集 $U = \\{u_1, u_2, \\ldots, u_m\\}$。\n- 集族 $\\mathcal{S}$ 由两类集合组成：\n  1. 对每个 $i \\in \\{1,2,\\ldots,m\\}$，一个单元素集 $T_i \\triangleq \\{u_i\\}$，其成本为 $c(T_i) \\triangleq \\frac{1}{i}$。\n  2. 对每个 $j \\in \\{1,2,\\ldots,m\\}$，一个前缀集 $P_j \\triangleq \\{u_1, u_2, \\ldots, u_j\\}$，其成本为 $c(P_j) \\triangleq 1 + \\delta$。\n\n任务：\n1. 根据上述定义，确定并证明在此实例上贪心算法所选择的集合的精确序列，并计算其总成本，表示为 $m$ 和 $\\delta$ 的函数。\n2. 计算最优总成本，表示为 $m$ 和 $\\delta$ 的函数，并从第一性原理证明其最优性。\n3. 设 $R(m,\\delta)$ 表示此实例上贪心成本与最优成本的比值。计算极限 $\\lim_{\\delta \\to 0^{+}} R(m,\\delta)$ 并将你的最终答案表示为关于 $m$ 和 $H_m$ 的单个闭式解析表达式。\n\n你的最终答案必须是单个闭式表达式。无需四舍五-入。", "solution": "该问题提供了一个加权集合覆盖问题的具体实例，并要求分析贪心算法在该实例上的性能。验证过程确认了该问题是适定的、数学上合理的，并且所有定义都是完整和一致的。我们按要求分三部分进行解答。\n\n第1部分：贪心算法分析\n\n加权集合覆盖的贪心算法迭代地选择集合 $S$，以最小化其成本与新覆盖元素数量的比值。设 $C_{k-1}$ 是第 $k$ 次迭代前已覆盖的元素集合。算法选择 $S \\in \\mathcal{S}$ 来最小化 $\\frac{c(S)}{|S \\setminus C_{k-1}|}$ 的值。\n\n让我们追踪算法在给定实例上的执行过程。\n设 $\\mathcal{C}_{greedy}$ 是算法选择的集合族。\n\n**迭代 $k=1$**：\n最初，已覆盖元素的集合为空，即 $C_0 = \\emptyset$。任何集合 $S$ 覆盖的新元素数量就是其基数 $|S|$。我们为 $\\mathcal{S}$ 中的每个集合计算成本效益比。\n对于单元素集 $T_i = \\{u_i\\}$，其中 $i \\in \\{1, 2, \\ldots, m\\}$：\n比率为 $\\frac{c(T_i)}{|T_i \\setminus C_0|} = \\frac{c(T_i)}{|T_i|} = \\frac{1/i}{1} = \\frac{1}{i}$。\n对于前缀集 $P_j = \\{u_1, u_2, \\ldots, u_j\\}$，其中 $j \\in \\{1, 2, \\ldots, m\\}$：\n比率为 $\\frac{c(P_j)}{|P_j \\setminus C_0|} = \\frac{c(P_j)}{|P_j|} = \\frac{1+\\delta}{j}$。\n\n算法选择具有最小比率的集合。我们需要在所有可能的比率集合中找到最小值：$\\{\\frac{1}{1}, \\frac{1}{2}, \\ldots, \\frac{1}{m}\\} \\cup \\{\\frac{1+\\delta}{1}, \\frac{1+\\delta}{2}, \\ldots, \\frac{1+\\delta}{m}\\}$。\n比较有效大小相同的集合的比率，对于任何 $k \\in \\{1, \\ldots, m\\}$，$T_k$ 的比率是 $\\frac{1}{k}$，$P_k$ 的比率是 $\\frac{1+\\delta}{k}$。因为 $\\delta > 0$，我们有 $1 < 1+\\delta$，这意味着 $\\frac{1}{k} < \\frac{1+\\delta}{k}$。这对所有 $k$ 都成立。\n所有单元素集中的最小比率是 $\\frac{1}{m}$（对于 $T_m$）。所有前缀集中的最小比率是 $\\frac{1+\\delta}{m}$（对于 $P_m$）。因此，总体的最小比率是 $\\frac{1}{m}$，由集合 $S_1 = T_m$ 唯一达到。\n因此，第一个被选择的集合是 $T_m$。已覆盖元素的集合变为 $C_1 = \\{u_m\\}$。\n\n**关于选择序列的归纳证明**：\n我们声称贪心算法按序列 $T_m, T_{m-1}, \\ldots, T_1$ 选择单元素集。我们用归纳法证明这一点。\n归纳基础：对于 $k=1$，我们已证明算法选择 $T_m$。这建立了我们对应于元素 $u_m$ 的归纳法的基础。\n归纳假设：假设在前 $k$ 步中（$1 \\le k < m$），算法已选择了集合 $T_m, T_{m-1}, \\ldots, T_{m-k+1}$。$k$ 步后已覆盖元素的集合是 $C_k = \\{u_m, u_{m-1}, \\ldots, u_{m-k+1}\\}$。未覆盖元素的集合是 $U \\setminus C_k = \\{u_1, u_2, \\ldots, u_{m-k}\\}$。\n\n归纳步骤（第 $k+1$ 次迭代）：\n我们现在确定要选择的集合。任何集合 $S$ 的成本效益比为 $\\frac{c(S)}{|S \\setminus C_k|}$。\n对于任何剩余的单元素集 $T_i$，$i \\in \\{1, 2, \\ldots, m-k\\}$，元素 $u_i$ 未被覆盖。因此，$|T_i \\setminus C_k| = 1$。比率为 $\\frac{c(T_i)}{1} = \\frac{1}{i}$。这些比率中的最小值是 $\\frac{1}{m-k}$，由 $T_{m-k}$ 达到。\n对于任何前缀集 $P_j$，$j \\in \\{1, 2, \\ldots, m\\}$，它覆盖的新元素数量是 $|P_j \\setminus C_k| = |P_j \\cap (U \\setminus C_k)| = |\\{u_1, \\ldots, u_j\\} \\cap \\{u_1, \\ldots, u_{m-k}\\}|$。这个基数是 $\\min(j, m-k)$。\n$P_j$ 的比率是 $\\frac{c(P_j)}{|P_j \\setminus C_k|} = \\frac{1+\\delta}{\\min(j, m-k)}$。\n为了找到所有前缀集中的最小比率，我们必须最大化分母 $\\min(j, m-k)$。$\\min(j, m-k)$ 的最大值是 $m-k$，对于任何 $j \\ge m-k$ 都可以达到。所以，任何前缀集的最小比率是 $\\frac{1+\\delta}{m-k}$。\n现在我们比较来自单元素集的最小比率 $\\frac{1}{m-k}$（来自 $T_{m-k}$），与来自前缀集的最小比率 $\\frac{1+\\delta}{m-k}$。\n因为 $\\delta > 0$，我们有 $1 < 1+\\delta$，因此 $\\frac{1}{m-k} < \\frac{1+\\delta}{m-k}$。\n唯一最小比率是 $\\frac{1}{m-k}$，由集合 $T_{m-k}$ 达到。因此，在第 $k+1$ 步，算法选择集合 $S_{k+1}=T_{m-k}$。\n\n根据数学归纳法原理，贪心算法将按此顺序选择集合 $T_m, T_{m-1}, \\ldots, T_1$。这覆盖了 $U$ 的所有元素。\n贪心算法找到的覆盖的总成本 $\\text{Cost}_{greedy}$ 是这些集合的成本之和：\n$$ \\text{Cost}_{greedy} = c(T_m) + c(T_{m-1}) + \\cdots + c(T_1) = \\sum_{i=1}^{m} c(T_i) = \\sum_{i=1}^{m} \\frac{1}{i} $$\n根据定义，这个和是第 $m$ 个调和数 $H_m$。\n$$ \\text{Cost}_{greedy} = H_m $$\n\n第2部分：最优成本计算\n\n我们必须找到一个有效的覆盖 $\\mathcal{C} \\subseteq \\mathcal{S}$，其总成本 $\\text{Cost}_{opt}$ 最小。\n最优覆盖有两个主要候选者：\n1. 一个仅由单元素集组成的覆盖。为了覆盖整个全集 $U=\\{u_1, \\ldots, u_m\\}$，这个覆盖必须是 $\\mathcal{C}_1 = \\{T_1, T_2, \\ldots, T_m\\}$。其成本为 $\\text{Cost}(\\mathcal{C}_1) = \\sum_{i=1}^m c(T_i) = H_m$。\n2. 一个使用一个或多个前缀集的覆盖。如果一个覆盖包含任何前缀集 $P_j$，其成本至少为 $c(P_j)=1+\\delta$。这种类型中最有效的覆盖是单个集合 $P_m = \\{u_1, \\ldots, u_m\\}$，它本身就覆盖了所有的 $U$。这个覆盖 $\\mathcal{C}_2 = \\{P_m\\}$ 的成本是 $\\text{Cost}(\\mathcal{C}_2) = c(P_m) = 1+\\delta$。任何其他包含前缀集的覆盖的成本都至少为 $1+\\delta$，所以 $\\{P_m\\}$ 是在使用前缀集的覆盖中最好的。\n\n最优成本必须是所有可能覆盖的成本中的最小值。根据上面的分析，任何覆盖要么是类型1（仅单元素集），要么包含至少一个前缀集（导致成本至少为 $1+\\delta$）。因此，最优成本是 $\\mathcal{C}_1$ 和 $\\mathcal{C}_2$ 成本的最小值：\n$$ \\text{Cost}_{opt}(m, \\delta) = \\min(H_m, 1+\\delta) $$\n问题要求在 $\\delta \\to 0^+$ 的极限情况下进行分析。对于 $m \\ge 2$，调和数 $H_m = 1 + \\frac{1}{2} + \\cdots + \\frac{1}{m} > 1$。因此，$H_m-1 > 0$。我们可以选择 $\\delta$ 使得 $0 < \\delta < H_m-1$，这意味着 $1+\\delta < H_m$。由于极限考虑任意小的正 $\\delta$，这个条件将会成立。\n因此，为了本次分析的目的，最优解是覆盖 $\\{P_m\\}$，其成本为：\n$$ \\text{Cost}_{opt}(m, \\delta) = 1+\\delta $$\n\n第3部分：近似比极限的计算\n\n近似比 $R(m,\\delta)$ 是此实例上贪心算法的成本与最优成本的比值。\n$$ R(m,\\delta) = \\frac{\\text{Cost}_{greedy}}{\\text{Cost}_{opt}} = \\frac{H_m}{1+\\delta} $$\n我们被要求计算当 $\\delta$ 从正方向趋近于 $0$ 时此比率的极限。\n$$ \\lim_{\\delta \\to 0^+} R(m,\\delta) = \\lim_{\\delta \\to 0^+} \\frac{H_m}{1+\\delta} $$\n由于 $H_m$ 是关于 $\\delta$ 的一个常数，且函数 $f(\\delta) = \\frac{H_m}{1+\\delta}$ 在 $\\delta=0$ 处是连续的，我们可以通过直接代入来计算极限：\n$$ \\lim_{\\delta \\to 0^+} \\frac{H_m}{1+\\delta} = \\frac{H_m}{1+0} = H_m $$\n所求的极限是第 $m$ 个调和数 $H_m$。", "answer": "$$ \\boxed{H_m} $$", "id": "3207647"}, {"introduction": "最坏情况下的性能保证非常强大，但它们并不能完全反映全貌。本练习将从纯理论转向计算实验。你将实现著名的顶点覆盖问题的2-近似算法，并在随机图上将其性能与真实最优解进行比较，从而估算其在“平均情况”下的性能，并观察它在实践中的表现。这项练习有助于理解理论保证与实际性能之间的关系。", "problem": "要求您实现一个实验，以估计一个经典的顶点覆盖（Vertex Cover）问题二近似算法在 Erdős–Rényi 随机图上的平均情况近似比。目标是设计一个完整的、可运行的程序，该程序能够生成随机图，应用该近似算法，计算精确的最优顶点覆盖以测量近似比，并在多次试验中汇总结果。\n\n您必须使用以下基本概念：\n- 无向图的顶点覆盖是一个顶点的子集，该子集使得图中的每条边至少有一个端点在该子集中。形式上，对于无向图 $G = (V, E)$，一个集合 $C \\subseteq V$ 是顶点覆盖，如果对于每条边 $(u, v) \\in E$，都有 $u \\in C$ 或 $v \\in C$ 成立。\n- Erdős–Rényi 随机图模型 $G(n, p)$ 在一个大小为 $n$ 的顶点集上，以概率 $p$ 独立地抽取每条可能的边。\n- 一个优化问题的近似算法通过其近似比进行评估。对于一个最小化问题的实例 $I$，其最优值为 $\\operatorname{OPT}(I)$，算法输出值为 $\\operatorname{ALG}(I)$，其在该实例上的近似比定义为 $\\rho(I) = \\frac{\\operatorname{ALG}(I)}{\\operatorname{OPT}(I)}$。约定当 $\\operatorname{OPT}(I) = 0$ 时，近似比定义为 $1$（这适用于没有边的图，此时最优和算法所得的顶点覆盖大小均为 $0$）。\n- 基于极大匹配的经典顶点覆盖二近似算法，构造任意一个极大匹配 $M$，并返回 $M$ 中所有边的端点集合。该集合的大小为 $2 \\lvert M \\rvert$，并且是一个有效的顶点覆盖。\n\n您的程序必须：\n- 实现上述基于极大匹配的二近似算法，其中匹配通过以固定的确定性顺序扫描边来贪心构造。\n- 为每个生成的图实例计算精确的最优顶点覆盖大小。您必须精确地（而不是近似地）完成此计算，可使用任何正确的精确方法。禁止使用提示；但是，只要是正确的，精确的回溯或分支定界搜索都是可以接受的。\n- 对于每个随机图实例 $G \\sim G(n, p)$，计算如上指定的近似比 $\\rho(G)$。\n- 对于每个参数元组，通过蒙特卡洛估计量来估计平均情况近似比，即对从 $G(n, p)$ 中抽取的 $s$ 个独立样本 $G$ 计算其 $\\rho(G)$ 的算术平均值。使用固定的随机种子以确保可复现性。\n- 如果生成的图没有边（因此最优顶点覆盖大小为 $0$），则定义 $\\rho(G) = 1$。\n- 所有计算都是无单位且纯组合的；不涉及物理单位。\n\n需要从基本定义实现的设计细节：\n- 基于极大匹配的二近似算法的正确性源于这样一个事实：任何匹配 $M$ 的大小都是任何顶点覆盖大小的一个下界。由于匹配中的每条边都需要一个不同的顶点来覆盖它，因此有 $\\lvert M \\rvert \\le \\operatorname{OPT}(G)$。该算法返回一个大小为 $2 \\lvert M \\rvert$ 的顶点覆盖，因此其大小最多是最优解的两倍。\n- 根据大数定律，样本均值随着样本数量的增加会收敛于期望值。因此，蒙特卡洛估计量使用算术平均值作为在 $G(n, p)$ 模型下期望近似比的估计。\n\n输入是隐式的；您必须硬编码并运行以下参数值的测试套件。每个测试用例是一个元组 $(n, p, s, \\text{seed})$，其中 $n$ 是顶点数，$p$ 是边概率，$s$ 是独立图样本的数量，$\\text{seed}$ 是随机数生成器的种子：\n- 测试 1：$(n, p, s, \\text{seed}) = (10, 0.2, 80, 101)$。\n- 测试 2：$(n, p, s, \\text{seed}) = (12, 0.5, 30, 202)$。\n- 测试 3：$(n, p, s, \\text{seed}) = (8, 0.0, 60, 303)$。\n- 测试 4：$(n, p, s, \\text{seed}) = (9, 0.9, 40, 404)$。\n- 测试 5：$(n, p, s, \\text{seed}) = (1, 0.7, 10, 505)$。\n\n算法要求和约束：\n- $G(n, p)$ 的图生成必须独立地以概率 $p$ 包含每个无序对 $\\{i, j\\}$（其中 $0 \\le i < j < n$），且不得包含自环或重边。\n- 精确的最优顶点覆盖必须通过一个正确的精确算法计算。一个可接受的方法是分支定界搜索，该方法重复地选择一条未被覆盖的边 $(u, v)$，并分支为将 $u$ 或 $v$ 加入覆盖集，同时使用记忆化或剪枝来使计算在本次使用的小图上是可行的。\n- 极大匹配必须通过按字典序扫描边来确定性地构造，如果某条边的两个端点当前都未匹配，则添加该边，直到无法再添加任何边为止。\n\n输出：\n- 对于每个测试用例，输出一个浮点数，等于在 $s$ 个图上 $\\rho(G)$ 的样本均值，四舍五入到 $6$ 位小数。\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，列表中的结果按上述测试套件的顺序排列，例如 $[r_1, r_2, r_3, r_4, r_5]$，其中每个 $r_i$ 是测试 $i$ 的四舍五入后的浮点结果。\n\n没有用户输入。最终输出必须是指定格式的单行文本。所有随机化操作都必须使用提供的种子。通过为每个测试用例独立使用给定的种子来为随机数生成器播种，以确保可复现性。答案是无单位的实数。不涉及角度或物理单位。", "solution": "我们从图论和近似分析的核心定义出发，推导出完整的解决方案，然后设计实验所需的算法组件。\n\n1. 定义与目标量\n- 设 $G = (V, E)$ 是一个无向简单图，其中 $\\lvert V \\rvert = n$，边集为 $E$。顶点覆盖 $C \\subseteq V$ 满足对于每条边 $(u, v) \\in E$，都有 $u \\in C$ 或 $v \\in C$。\n- 在 Erdős–Rényi 模型 $G(n, p)$ 中，对于 $0 \\le i < j < n$ 的每条可能的边 $\\{i, j\\}$，都以概率 $p$ 独立存在。\n- 对于一个最小化问题的实例 $I$，其最优值为 $\\operatorname{OPT}(I)$，算法输出为 $\\operatorname{ALG}(I)$，近似比为 $\\rho(I) = \\frac{\\operatorname{ALG}(I)}{\\operatorname{OPT}(I)}$。对于我们的实验，当 $\\operatorname{OPT}(G) = 0$ 时（即没有边），我们定义 $\\rho(G) = 1$，因为此时最优解和算法输出均为 $0$，我们赋予一个中性的比率 $1$。\n- 在 $G(n, p)$ 模型下的平均情况近似比为 $\\mathbb{E}_{G \\sim G(n, p)}[\\rho(G)]$。我们通过蒙特卡洛估计量 $\\hat{\\rho} = \\frac{1}{s} \\sum_{i=1}^{s} \\rho(G_i)$ 来估计它，其中 $G_i$ 是从 $G(n, p)$ 中抽取的独立样本。\n\n2. 二近似算法及其保证\n- 匹配 $M \\subseteq E$ 是一组不相交的边。任何顶点覆盖都必须包含 $M$ 中每条边的至少一个端点，因此 $\\lvert M \\rvert \\le \\operatorname{OPT}(G)$。\n- 经典的二近似算法构造任意一个极大匹配 $M$，并返回 $M$ 中所有边的端点集合 $C$。此时 $\\lvert C \\rvert = 2 \\lvert M \\rvert$。由于 $\\lvert M \\rvert \\le \\operatorname{OPT}(G)$，我们有 $\\lvert C \\rvert \\le 2 \\operatorname{OPT}(G)$，因此该算法实现了最坏情况下的 2 倍近似因子。\n- 我们实现一个确定性的贪心极大匹配算法：按字典序迭代所有边，如果某条边的两个端点都未被匹配，则将其加入匹配。返回的覆盖大小为 $2 \\lvert M \\rvert$。\n\n3. 精确最优顶点覆盖计算\n- 为了精确测量 $\\rho(G)$，需要进行精确计算。对于测试套件中的小图，分支定界搜索就足够了。\n- 预处理步骤：将边索引为 $e_0, e_1, \\dots, e_{m-1}$，其中 $m = \\lvert E \\rvert$。对于每个顶点 $v \\in \\{0, 1, \\dots, n-1\\}$，预计算一个位掩码 $B_v \\in \\{0, 1\\}^m$（存储为整数），指示哪些边与 $v$ 邻接。用位掩码 $X \\in \\{0, 1\\}^m$ 表示当前未覆盖的边集，其中如果边 $e_i$ 未被覆盖，则第 $i$ 位为 $1$。\n- 递推关系：如果 $X = 0$，则所有边都已被覆盖，所需额外大小为 $0$。否则，选择任意一条未覆盖的边 $e_i = (u, v)$，其在 $X$ 中的对应位为 $1$。为了覆盖 $e_i$，我们必须包含 $u$ 或 $v$。进行分支：\n  - 包含 $u$：新的未覆盖边掩码为 $X' = X \\land \\neg B_u$，成本为 $1$ 加上对 $X'$ 的最小覆盖成本。\n  - 包含 $v$：新的未覆盖边掩码为 $X'' = X \\land \\neg B_v$，成本为 $1$ 加上对 $X''$ 的最小覆盖成本。\n  取两个结果中的最小值。\n- 剪枝与记忆化：\n  - 上界初始化：使用二近似算法计算一个可行的覆盖大小，以初始化上界 $U$。\n  - 维护至今找到的最优解值，并放弃任何已选顶点数不少于该值的递归分支。\n  - 记忆化：对于给定的未覆盖边掩码 $X$，存储遇到的最小选择数；如果一个递归调用以更多或相等的选择数到达相同的 $X$，则剪枝该调用，因为它不可能导出更优的解。\n  - 为改进剪枝效果，在对 $(u, v)$ 进行分支时，首先探索 $\\{u, v\\}$ 中能覆盖更多当前未覆盖边的顶点，即比较 $X \\land B_u$ 和 $X \\land B_v$ 的置位数（popcount），并首先对较大的那个进行递归。\n- 正确性可以通过对未覆盖边数进行归纳来证明：在每一步中，必须选择一条未覆盖边的至少一个端点，而算法探索了两种选择，从而确保找到最小解。剪枝和记忆化不会移除任何潜在的最优解，因为它们只避免探索那些不可能比当前最优解更好或被先前见过的状态所支配的分支。\n\n4. 蒙特卡洛平均情况估计\n- 对于每个测试用例 $(n, p, s, \\text{seed})$，使用给定的种子初始化一个伪随机数生成器。通过以概率 $p$ 包含每条可能的边，从 $G(n, p)$ 中独立抽样 $s$ 个图。\n- 对于每个抽样图 $G$，通过二近似算法计算 $\\operatorname{ALG}(G)$，通过精确方法计算 $\\operatorname{OPT}(G)$，如果 $\\operatorname{OPT}(G) = 0$ 则定义 $\\rho(G)$ 为 $1$，否则定义 $\\rho(G) = \\frac{\\operatorname{ALG}(G)}{\\operatorname{OPT}(G)}$。\n- 估计的平均情况近似比为 $\\hat{\\rho} = \\frac{1}{s} \\sum_{i=1}^{s} \\rho(G_i)$。\n\n5. 测试套件与输出\n- 实现五个测试用例：\n  - $(n, p, s, \\text{seed}) = (10, 0.2, 80, 101)$。\n  - $(n, p, s, \\text{seed}) = (12, 0.5, 30, 202)$。\n  - $(n, p, s, \\text{seed}) = (8, 0.0, 60, 303)$。\n  - $(n, p, s, \\text{seed}) = (9, 0.9, 40, 404)$。\n  - $(n, p, s, \\text{seed}) = (1, 0.7, 10, 505)$。\n- 对于每个测试用例，输出一个浮点数，等于 $\\rho(G)$ 的样本均值，四舍五入到 $6$ 位小数。\n- 最终程序必须打印单行，内容为包含五个结果的列表，按顺序排列，以逗号分隔，并用方括号括起来，例如 $[1.234567,1.111111,1.000000,1.050000,1.000000]$。\n\n此设计基于图论和近似分析的核心定义，对小规模实例采用严格正确的精确计算来求解最优解，利用可证明的二近似方法作为实验对象，并使用蒙特卡洛方法来估计在 Erdős–Rényi 模型下的期望近似比。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom typing import List, Tuple, Dict\n\ndef generate_er_graph(n: int, p: float, rng: np.random.RandomState) -> List[Tuple[int, int]]:\n    \"\"\"\n    Generate an undirected simple graph G(n, p) as a list of edges (u, v) with u < v.\n    \"\"\"\n    edges = []\n    if n <= 1 or p == 0.0:\n        return edges\n    # Iterate lexicographically over pairs (i, j), i < j\n    for i in range(n - 1):\n        # Generate a vector of randoms for edges (i, j) for j > i\n        # However, generating one by one is simpler and avoids large arrays.\n        for j in range(i + 1, n):\n            if rng.rand() < p:\n                edges.append((i, j))\n    return edges\n\ndef approx_vertex_cover_size(edges: List[Tuple[int, int]], n: int) -> int:\n    \"\"\"\n    Two-approximation via maximal matching: greedily add edges in lex order if both endpoints free,\n    then return the number of endpoints in the matching (i.e., 2*|M|).\n    \"\"\"\n    matched = [False] * n\n    cover_size = 0\n    # Edges already lexicographically ordered by construction in generate_er_graph\n    for u, v in edges:\n        if not matched[u] and not matched[v]:\n            matched[u] = True\n            matched[v] = True\n            cover_size += 2\n    return cover_size\n\ndef exact_min_vertex_cover_size(edges: List[Tuple[int, int]], n: int, approx_upper_bound: int) -> int:\n    \"\"\"\n    Exact minimum vertex cover using branch-and-bound over uncovered-edge bitmasks.\n    - edges: list of (u, v) with u < v\n    - n: number of vertices\n    - approx_upper_bound: an initial upper bound for pruning (e.g., 2-approx size)\n    Returns the exact size of a minimum vertex cover.\n    \"\"\"\n    m = len(edges)\n    if m == 0:\n        return 0\n\n    # Map each vertex to the bitmask of incident edges\n    incident_masks = [0] * n\n    for idx, (u, v) in enumerate(edges):\n        incident_masks[u] |= (1 << idx)\n        incident_masks[v] |= (1 << idx)\n\n    # Initial uncovered edges mask: all edges uncovered\n    full_mask = (1 << m) - 1\n\n    # Best found so far (upper bound). Initialize with approx_upper_bound, but it might be 0 if no edges.\n    best = [approx_upper_bound if approx_upper_bound > 0 else m]  # wrap in list to allow mutation in closure\n\n    # Memoization: for a given uncovered-edge mask X, store the minimum number of selections (taken) encountered.\n    seen: Dict[int, int] = {}\n\n    # Precompute an order of edges to select the first uncovered edge quickly by index.\n    # We simply keep the list and will find the first set bit when needed.\n\n    def dfs(uncovered_mask: int, taken: int) -> None:\n        # Branch-and-bound pruning by current best\n        if taken >= best[0]:\n            return\n\n        if uncovered_mask == 0:\n            # All edges covered\n            if taken < best[0]:\n                best[0] = taken\n            return\n\n        # Memoization pruning: if we've seen this mask with <= taken, no need to proceed\n        prev = seen.get(uncovered_mask)\n        if prev is not None and taken >= prev:\n            return\n        seen[uncovered_mask] = taken\n\n        # Pick one uncovered edge: get index of a set bit in uncovered_mask\n        # Use least significant set bit\n        lsb = uncovered_mask & -uncovered_mask\n        edge_index = (lsb.bit_length() - 1)\n        u, v = edges[edge_index]\n\n        # Compute degrees with respect to uncovered edges to decide branch order\n        deg_u = (uncovered_mask & incident_masks[u]).bit_count()\n        deg_v = (uncovered_mask & incident_masks[v]).bit_count()\n\n        # Explore the vertex that covers more uncovered edges first to prune faster\n        if deg_u >= deg_v:\n            # Include u\n            dfs(uncovered_mask & ~incident_masks[u], taken + 1)\n            # Include v\n            dfs(uncovered_mask & ~incident_masks[v], taken + 1)\n        else:\n            # Include v\n            dfs(uncovered_mask & ~incident_masks[v], taken + 1)\n            # Include u\n            dfs(uncovered_mask & ~incident_masks[u], taken + 1)\n\n    dfs(full_mask, 0)\n    return best[0]\n\ndef estimate_average_ratio(n: int, p: float, samples: int, seed: int) -> float:\n    \"\"\"\n    For given (n, p), estimate the average approximation ratio over 'samples' graphs,\n    using the provided seed for reproducibility.\n    \"\"\"\n    rng = np.random.RandomState(seed)\n    ratios_sum = 0.0\n    for _ in range(samples):\n        edges = generate_er_graph(n, p, rng)\n        # Approximate cover size\n        approx_size = approx_vertex_cover_size(edges, n)\n        # Exact optimal cover size\n        opt_size = exact_min_vertex_cover_size(edges, n, approx_size)\n        if opt_size == 0:\n            ratio = 1.0  # by convention for edgeless graphs\n        else:\n            ratio = approx_size / opt_size\n        ratios_sum += ratio\n    return ratios_sum / samples if samples > 0 else float('nan')\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each tuple is (n, p, samples, seed)\n    test_cases = [\n        (10, 0.2, 80, 101),\n        (12, 0.5, 30, 202),\n        (8, 0.0, 60, 303),\n        (9, 0.9, 40, 404),\n        (1, 0.7, 10, 505),\n    ]\n\n    results = []\n    for n, p, s, seed in test_cases:\n        avg_ratio = estimate_average_ratio(n, p, s, seed)\n        results.append(f\"{avg_ratio:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3207616"}]}