{"hands_on_practices": [{"introduction": "在深入研究怀纳-齐夫编码的速率-失真性能之前，我们必须首先掌握其最基本的数学工具。任何分布式信源编码问题的分析都始于对信源 $X$ 与边信息 $Y$ 之间统计依赖关系的精确描述。这项基础练习将引导你通过已知的联合概率分布来计算条件概率 $p(X|Y)$，这是计算怀纳-齐夫理论中所有关键量（如条件熵和互信息）的必经之路。[@problem_id:1668845]", "problem": "在一个分布式传感器网络中，两个共址二元传感器 $X$ 和 $Y$ 用于监测同一环境状态。传感器的输出是相关的，其联合行为由一个概率质量函数 (PMF) $p(x, y)$ 描述，其中 $x, y \\in \\{0, 1\\}$。已知的联合 PMF 如下：\n$p(X=0, Y=0) = \\frac{3}{8}$\n$p(X=0, Y=1) = \\frac{1}{8}$\n$p(X=1, Y=0) = \\frac{1}{8}$\n$p(X=1, Y=1) = \\frac{3}{8}$\n\n在设计 Wyner-Ziv 压缩方案的背景下，传感器读数 $X$ 的编码无需知晓 $Y$，而解码器则可以利用 $Y$ 作为边信息。确定可达压缩率的一个基本前提是描述 $X$ 对 $Y$ 的统计依赖性。\n\n确定构成在给定 $Y$ 的情况下 $X$ 的条件概率分布的四个值。以 $[p(X=0|Y=0), p(X=1|Y=0), p(X=0|Y=1), p(X=1|Y=1)]$ 的特定顺序，将您的答案表示为一个包含四个分数的单行矩阵。", "solution": "我们已知联合 PMF 的值：\n$$\np(X=0, Y=0) = \\frac{3}{8}, \\quad p(X=0, Y=1) = \\frac{1}{8}, \\quad p(X=1, Y=0) = \\frac{1}{8}, \\quad p(X=1, Y=1) = \\frac{3}{8}.\n$$\n为了计算在给定 $Y$ 的情况下 $X$ 的条件概率，我们使用条件概率的定义：\n$$\np(X=x \\mid Y=y) = \\frac{p(X=x, Y=y)}{p_{Y}(y)},\n$$\n其中，$Y$ 的边缘分布是通过对 $x$ 求和得到的：\n$$\np_{Y}(y) = \\sum_{x \\in \\{0,1\\}} p(X=x, Y=y).\n$$\n首先，计算 $Y$ 的边缘概率：\n$$\np_{Y}(0) = p(0,0) + p(1,0) = \\frac{3}{8} + \\frac{1}{8} = \\frac{1}{2},\n$$\n$$\np_{Y}(1) = p(0,1) + p(1,1) = \\frac{1}{8} + \\frac{3}{8} = \\frac{1}{2}.\n$$\n现在计算条件概率：\n对于 $Y=0$：\n$$\np(X=0 \\mid Y=0) = \\frac{p(0,0)}{p_{Y}(0)} = \\frac{\\frac{3}{8}}{\\frac{1}{2}} = \\frac{3}{4},\n$$\n$$\np(X=1 \\mid Y=0) = \\frac{p(1,0)}{p_{Y}(0)} = \\frac{\\frac{1}{8}}{\\frac{1}{2}} = \\frac{1}{4}.\n$$\n对于 $Y=1$：\n$$\np(X=0 \\mid Y=1) = \\frac{p(0,1)}{p_{Y}(1)} = \\frac{\\frac{1}{8}}{\\frac{1}{2}} = \\frac{1}{4},\n$$\n$$\np(X=1 \\mid Y=1) = \\frac{p(1,1)}{p_{Y}(1)} = \\frac{\\frac{3}{8}}{\\frac{1}{2}} = \\frac{3}{4}.\n$$\n因此，按 $[p(X=0 \\mid Y=0), p(X=1 \\mid Y=0), p(X=0 \\mid Y=1), p(X=1 \\mid Y=1)]$ 顺序所求的行矩阵是\n$$\n\\begin{pmatrix}\n\\frac{3}{4} & \\frac{1}{4} & \\frac{1}{4} & \\frac{3}{4}\n\\end{pmatrix}.\n$$", "answer": "$$\\boxed{\\begin{pmatrix}\\frac{3}{4} & \\frac{1}{4} & \\frac{1}{4} & \\frac{3}{4}\\end{pmatrix}}$$", "id": "1668845"}, {"introduction": "维纳-齐夫定理是处理有损压缩的通用框架，但当失真要求为零（$D=0$）时，它便简化为著名的斯理潘-伍夫（Slepian-Wolf）无损编码定理。本练习探讨了这一重要的特殊情况，你需要计算实现完美重构信源 $X$ 所需的最小码率。这个码率由条件熵 $H(X|Y)$ 决定，它量化了在已知边信息 $Y$ 的情况下，信源 $X$ 剩余的不确定性。[@problem_id:1668812]", "problem": "在一个远程环境监测系统中，一个主传感器测量系统的状态，该状态由随机变量 $X$ 表示，它可以取三个值之一：$X \\in \\{0, 1, 2\\}$。观测到这些状态以相等的概率出现。一个次级的、可靠性较低的传感器提供相关的旁路信息，由随机变量 $Y$ 表示，它也取值于 $\\{0, 1, 2\\}$。次级传感器的观测数据在中央数据中心可用。\n\n两个传感器之间的相关性由以下条件概率来表征：当主传感器测量到状态 $x$ 时，次级传感器也测量到 $x$ 的概率是 $P(Y=x | X=x) = 0.8$。当主传感器测量到状态 $x$ 时，次级传感器测量到任何其他特定状态 $y \\neq x$ 的概率对于所有其他状态都是相同的，即对于 $y \\neq x$，有 $P(Y=y | X=x) = 0.1$。\n\n主传感器必须对其测量序列进行编码，并将其传输到中央数据中心。主传感器的编码器无法获取次级传感器的数据。根据分布式信源编码的原理（特别是 Slepian-Wolf 定理，它是 Wyner-Ziv 理论在无损压缩情况下的一个特例），为了让中央数据中心能够无损地完美重构状态序列 $X$，主传感器传输所需的理论最小平均数据率是多少？\n\n请用“比特/测量”作为单位，将你的答案四舍五入到四位有效数字。", "solution": "题目要求我们计算在解码器端有相关旁路信息 $Y$ 可用时，无损重构 $X$ 所需的理论最小平均数据率。根据 Slepian-Wolf 定理，仅观测 $X$ 的编码器可实现的最小码率是条件熵\n$$\nR_{\\min}=H(X|Y).\n$$\n\n给定 $X \\in \\{0,1,2\\}$，且对所有 $x$ 都有 $P(X=x)=\\frac{1}{3}$，以及一个由下式指定的对称信道 $P(Y=y|X=x)$\n$$\nP(Y=x|X=x)=0.8,\\quad P(Y=y|X=x)=0.1\\ \\text{for }y\\neq x,\n$$\n我们首先计算 $Y$ 的边缘概率分布：\n$$\nP(Y=y)=\\sum_{x}P(Y=y|X=x)P(X=x)=\\frac{1}{3}\\left(0.8+0.1+0.1\\right)=\\frac{1}{3},\n$$\n因此 $Y$ 也是均匀分布的。\n\n利用贝叶斯法则和对称性，对于任意的 $y$，我们有\n$$\nP(X=y|Y=y)=\\frac{P(Y=y|X=y)P(X=y)}{P(Y=y)}=\\frac{0.8\\cdot \\frac{1}{3}}{\\frac{1}{3}}=0.8,\n$$\n并且对于 $x\\neq y$，\n$$\nP(X=x|Y=y)=\\frac{P(Y=y|X=x)P(X=x)}{P(Y=y)}=\\frac{0.1\\cdot \\frac{1}{3}}{\\frac{1}{3}}=0.1.\n$$\n因此，后验概率分布 $P(X|Y=y)$ 对每个 $y$ 都是相同的，等于 $\\{0.8,0.1,0.1\\}$。因此\n$$\nH(X|Y)=\\sum_{y}P(Y=y)\\,H\\big(P(X|Y=y)\\big)=H\\big(0.8,0.1,0.1\\big).\n$$\n以比特为单位计算熵，\n$$\nH(X|Y)=-\\left[0.8\\log_{2}(0.8)+0.1\\log_{2}(0.1)+0.1\\log_{2}(0.1)\\right]\n=-0.8\\log_{2}(0.8)-0.2\\log_{2}(0.1).\n$$\n使用 $\\log_{2}(0.8)\\approx -0.3219280949$ 和 $\\log_{2}(0.1)\\approx -3.3219280949$，我们得到\n$$\nH(X|Y)\\approx 0.8\\times 0.3219280949+0.2\\times 3.3219280949\\approx 0.9219280949\\ \\text{比特/测量}.\n$$\n四舍五入到四位有效数字，得到 $0.9219$ 比特/测量。\n\n因此，所需的理论最小平均数据率是 $H(X|Y)\\approx 0.9219$ 比特/测量。", "answer": "$$\\boxed{0.9219}$$", "id": "1668812"}, {"introduction": "在探索了零失真下的码率极限后，我们转向另一个极端情况：零码率下的最小失真。当编码器不发送任何信息（$R=0$）时，解码器只能完全依赖边信息 $Y$ 来猜测信源 $X$。此场景下的问题转化为一个经典的贝叶斯估计问题，其目标是找到一个最优的估计策略，以最小化平均失真。通过本练习，你将计算出这个最小可能失真 $D_{\\min}$，它代表了仅利用信源间相关性所能达到的重构质量极限。[@problem_id:1668798]", "problem": "一个二进制数据源产生一个随机变量 $X \\in \\{0, 1\\}$，其概率分布已知为 $P(X=1) = p_X$。该信源需被编码以进行传输，但解码器可以访问到相关的边信息 $Y$。边信息由信源的带噪版本生成，具体来说，是通过将 $X$ 经过一个交叉概率为 $\\epsilon$ 的二进制对称信道 (BSC) 产生的。解码器不从编码器接收任何信息（零速率编码场景），并且必须仅根据观测到的边信息 $Y$ 生成对信源符号 $X$ 的估计 $\\hat{X}$。\n\n估计器的性能由平均错误概率来衡量，该概率对应于平均汉明失真 $E[d(X, \\hat{X})]$，其中当 $x \\ne \\hat{x}$ 时 $d(x, \\hat{x})=1$，当 $x = \\hat{x}$ 时 $d(x, \\hat{x})=0$。一个最优解码器将实现一个估计规则 $\\hat{x}(y)$，以最小化该平均错误概率。这个最小可达错误表示为 $D_{\\min}$。\n\n给定信源概率 $p_X = 0.800$ 和信道交叉概率 $\\epsilon = 0.300$，计算 $D_{\\min}$ 的值。请将您的答案表示为保留三位有效数字的小数。", "solution": "该问题要求在仅使用相关边信息 $Y$ 来估计二进制信源 $X$ 时的最小平均错误概率 $D_{\\min}$。这个场景对应于贝叶斯估计，其目标是找到一个能最小化平均失真的估计器 $\\hat{x}(y)$。对于汉明失真，平均失真就是错误概率，$D = P(X \\ne \\hat{X})$。\n\n一个能最小化错误概率的最优估计器是最大后验 (MAP) 估计器。该估计器选择在给定观测值 $y$ 的条件下具有最高后验概率的符号 $x$：\n$$ \\hat{x}(y) = \\arg\\max_{x \\in \\{0,1\\}} P(X=x|Y=y) $$\n总的最小错误概率 $D_{\\min}$ 是对每个可能的观测值 $y$，其错误概率乘以该观测值出现的概率的加权和：\n$$ D_{\\min} = P(Y=0)P(\\text{错误}|Y=0) + P(Y=1)P(\\text{错误}|Y=1) $$\n给定观测值 $y$，发生错误的概率是 MAP 规则*未*选择的那个符号的后验概率。这是两个后验概率中较小的一个。\n$$ P(\\text{错误}|Y=y) = \\min_{x \\in \\{0,1\\}} P(X=x|Y=y) $$\n将此代入 $D_{\\min}$ 的表达式中：\n$$ D_{\\min} = P(Y=0) \\left( \\min_{x \\in \\{0,1\\}} P(X=x|Y=0) \\right) + P(Y=1) \\left( \\min_{x \\in \\{0,1\\}} P(X=x|Y=1) \\right) $$\n使用恒等式 $P(A,B) = P(A)P(B|A)$，我们可以通过将边缘概率 $P(Y=y)$ 移入最小值算子内部来简化表达式：\n$$ D_{\\min} = \\min_{x \\in \\{0,1\\}} P(X=x, Y=0) + \\min_{x \\in \\{0,1\\}} P(X=x, Y=1) $$\n这个简洁的公式允许我们通过计算四个联合概率 $P(X=x, Y=y)$ 来计算 $D_{\\min}$。\n\n问题提供了以下参数：\n- 信源概率：$P(X=1) = p_X = 0.800$。这意味着 $P(X=0) = 1 - p_X = 0.200$。\n- 信道交叉概率：$\\epsilon = 0.300$。这是一个比特被翻转的概率，即 $P(Y \\neq X|X)$。正确传输的概率是 $1-\\epsilon = 0.700$。\n\n现在，我们使用公式 $P(X=x, Y=y) = P(Y=y|X=x)P(X=x)$ 计算四个联合概率：\n1.  $P(X=0, Y=0) = P(Y=0|X=0)P(X=0) = (1-\\epsilon)(1-p_X) = (0.700)(0.200) = 0.140$。\n2.  $P(X=1, Y=0) = P(Y=0|X=1)P(X=1) = \\epsilon \\cdot p_X = (0.300)(0.800) = 0.240$。\n3.  $P(X=0, Y=1) = P(Y=1|X=0)P(X=0) = \\epsilon(1-p_X) = (0.300)(0.200) = 0.060$。\n4.  $P(X=1, Y=1) = P(Y=1|X=1)P(X=1) = (1-\\epsilon)p_X = (0.700)(0.800) = 0.560$。\n\n我们可以将这些值代入 $D_{\\min}$ 的表达式中：\n$$ D_{\\min} = \\min(P(X=0, Y=0), P(X=1, Y=0)) + \\min(P(X=0, Y=1), P(X=1, Y=1)) $$\n$$ D_{\\min} = \\min(0.140, 0.240) + \\min(0.060, 0.560) $$\n从每对中取最小值：\n$$ D_{\\min} = 0.140 + 0.060 = 0.200 $$\n\n为了获得更深入的理解，我们来研究一下具体的 MAP 决策规则：\n- 如果观测到 $Y=0$：解码器比较 $P(X=0, Y=0)=0.140$ 和 $P(X=1, Y=0)=0.240$。由于 $0.240 > 0.140$，解码器断定发送 $X=1$ 的可能性更大。因此，$\\hat{x}(0) = 1$。\n- 如果观测到 $Y=1$：解码器比较 $P(X=0, Y=1)=0.060$ 和 $P(X=1, Y=1)=0.560$。由于 $0.560 > 0.060$，解码器断定 $\\hat{x}(1) = 1$。\n\n在两种情况下，最优决策都是猜测 $\\hat{X}=1$，而不管观测到的边信息 $Y$ 是什么。发生这种情况的原因是信源高度偏向于 $X=1$（一个强先验），而信道噪声足够大，以至于观测到 $Y=0$ 提供的证据不足以克服这个先验信念。最优策略是始终猜测最可能的信源符号。\n该策略的错误概率是猜测错误的概率，即 $P(X \\ne 1) = P(X=0) = 1 - p_X = 1 - 0.800 = 0.200$。这证实了我们的计算结果。\n\n最终答案，保留三位有效数字，是 0.200。", "answer": "$$\\boxed{0.200}$$", "id": "1668798"}]}