{"hands_on_practices": [{"introduction": "本节的第一个练习将引导你计算假设检验中的一个核心工具——Kullback-Leibler (KL) 散度。KL散度衡量了两个概率分布之间的差异，是斯坦因引理中错误概率指数的理论基础。通过这个具体的计算，你将为理解数据源的可区分性以及评估检验性能打下坚实的数学基础。[@problem_id:1630521]", "problem": "在一家工厂里，一台机器生产一种特定类型的电子元件。每个元件都经过测试，并被分为功能正常（状态 1）或有缺陷（状态 0）。机器的性能可以用伯努利分布来建模。\n\n关于机器的运行状态，有两个假设：\n- 假设 $H_0$：机器在正常条件下运行。生产功能正常元件的概率为 $p_0 = 1/3$。设此分布为 $P_0$。\n- 假设 $H_1$：机器需要维护。生产功能正常元件的概率为 $p_1 = 2/3$。设此分布为 $P_1$。\n\n为了量化“正常”状态与“需要维护”状态之间的统计可区分性，一位工程师决定计算 Kullback-Leibler (KL) 散度，也称为相对熵。\n\n计算 KL 散度 $D(P_0 || P_1)$。将您的答案以数值形式给出，并四舍五入到三位有效数字。", "solution": "我们将机器的输出建模为一个伯努利随机变量。对于伯努利分布，从参数为 $p_{0}$ 的分布 $P_{0}$ 到参数为 $p_{1}$ 的分布 $P_{1}$ 的 Kullback-Leibler 散度为\n$$\nD(P_{0}\\,\\|\\,P_{1}) \\;=\\; p_{0}\\,\\ln\\!\\left(\\frac{p_{0}}{p_{1}}\\right) \\;+\\; \\left(1-p_{0}\\right)\\,\\ln\\!\\left(\\frac{1-p_{0}}{1-p_{1}}\\right).\n$$\n代入 $p_{0}=\\frac{1}{3}$ 和 $p_{1}=\\frac{2}{3}$：\n$$\nD(P_{0}\\,\\|\\,P_{1}) \\;=\\; \\frac{1}{3}\\,\\ln\\!\\left(\\frac{\\frac{1}{3}}{\\frac{2}{3}}\\right) \\;+\\; \\frac{2}{3}\\,\\ln\\!\\left(\\frac{\\frac{2}{3}}{\\frac{1}{3}}\\right).\n$$\n简化比率：\n$$\n\\frac{\\frac{1}{3}}{\\frac{2}{3}} \\;=\\; \\frac{1}{2}, \\qquad \\frac{\\frac{2}{3}}{\\frac{1}{3}} \\;=\\; 2,\n$$\n所以\n$$\nD(P_{0}\\,\\|\\,P_{1}) \\;=\\; \\frac{1}{3}\\,\\ln\\!\\left(\\frac{1}{2}\\right) \\;+\\; \\frac{2}{3}\\,\\ln(2).\n$$\n使用 $\\ln\\!\\left(\\frac{1}{2}\\right)=-\\ln(2)$：\n$$\nD(P_{0}\\,\\|\\,P_{1}) \\;=\\; -\\frac{1}{3}\\,\\ln(2) \\;+\\; \\frac{2}{3}\\,\\ln(2) \\;=\\; \\frac{1}{3}\\,\\ln(2).\n$$\n数值上，使用 $\\ln(2)\\approx 0.693147$，我们得到\n$$\nD(P_{0}\\,\\|\\,P_{1}) \\approx \\frac{1}{3}\\times 0.693147 \\approx 0.231049,\n$$\n四舍五入到三位有效数字为 $0.231$。", "answer": "$$\\boxed{0.231}$$", "id": "1630521"}, {"introduction": "在理解了如何计算KL散度之后，下一个实践将带你进入一个非常实际的应用场景。这个练习演示了如何利用斯坦因引理来指导实验设计，特别是估算达到特定检验精度所需的样本数量。这让你看到信息论概念如何直接转化为解决“需要多少数据才足够？”这类关键问题的强大工具。[@problem_id:1630537]", "problem": "一名数据科学家正在进行一项二元假设检验，以区分两种数据来源。这些数据被建模为独立同分布的伯努利随机变量序列。原假设 $H_0$ 假定数据来自参数为 $p_0 = 1/2$ 的伯努利分布。备择假设 $H_1$ 假定数据来源是参数为 $p_1 \\neq p_0$ 的伯努利分布。\n\n对于一个包含 $n$ 个样本序列的假设检验，我们将第一类错误概率定义为 $\\alpha_n = P(\\text{decide } H_1 | H_0 \\text{ is true})$，第二类错误概率定义为 $\\beta_n = P(\\text{decide } H_0 | H_1 \\text{ is true})$。斯坦因引理 (Stein's Lemma) 指出，对于一个固定的第一类错误概率约束 $\\alpha_n \\leq \\epsilon$（其中 $\\epsilon \\in (0, 1/2)$），可实现的最小第二类错误概率 $\\beta_n^*$ 会随着样本数量 $n$ 的增加而呈指数级衰减。这个衰减的速率由两个分布之间的库尔贝克-莱布勒（Kullback-Leibler）散度给出。\n\n该数据科学家正在使用一种检验程序，其中使用自然对数计算得出的最优错误指数为 $C = 0.0872$。公司的规程要求将第一类错误概率固定在 $\\epsilon = 0.05$。目标是确定为达到一个非常低的第二类错误概率所需的样本量。\n\n使用从斯坦因引理导出的大样本近似法，计算为确保第二类错误概率至多为 $\\beta^* = 1 \\times 10^{-5}$ 所需的最小整数样本量 $n$。", "solution": "问题要求在给定二元假设检验的最优错误指数 $C$ 的情况下，计算达到特定第二类错误概率 $\\beta^*$ 所需的最小样本量 $n$。\n\n斯坦因引理 (Stein's Lemma) 提供了在固定第一类错误概率 $\\epsilon$ 的条件下，最小第二类错误概率 $\\beta_n^*$ 的渐进行为。对于大样本量 $n$，该关系由下式给出：\n$$ \\beta_n^* \\approx \\exp(-n \\cdot D(P_1 || P_0)) $$\n其中 $D(P_1 || P_0)$ 是假设 $H_1$ 下的概率分布（记为 $P_1$）与假设 $H_0$ 下的概率分布（记为 $P_0$）之间的库尔贝克-莱布勒（Kullback-Leibler, KL）散度。这个KL散度就是第二类错误的最优错误指数。\n\n题目说明最优错误指数为 $C = 0.0872$。因此，我们有：\n$$ C = D(P_1 || P_0) = 0.0872 $$\n需要注意的是，根据斯坦因引理，在大样本 $n$ 的极限情况下，这个结果对任何固定的第一类错误约束 $\\epsilon \\in (0, 1/2)$ 都成立。因此，具体值 $\\epsilon = 0.05$ 是问题设定的一部分，但在基于此近似法计算样本量 $n$ 时并不需要用到。\n\n我们给定的第二类错误概率目标为 $\\beta^* = 1 \\times 10^{-5}$。我们需要找到最小的整数 $n$ 使得 $\\beta_n^* \\leq \\beta^*$。使用近似法，我们设定：\n$$ \\beta^* = \\exp(-nC) $$\n现在，我们求解该方程以得到 $n$：\n$$ \\ln(\\beta^*) = \\ln(\\exp(-nC)) $$\n$$ \\ln(\\beta^*) = -nC $$\n$$ n = -\\frac{\\ln(\\beta^*)}{C} $$\n这也可以写成：\n$$ n = \\frac{\\ln(1/\\beta^*)}{C} $$\n\n现在，我们将给定的数值代入这个表达式：\n- $C = 0.0872$\n- $\\beta^* = 1 \\times 10^{-5}$\n\n因此，\n$$ n = \\frac{\\ln(1/(1 \\times 10^{-5}))}{0.0872} $$\n$$ n = \\frac{\\ln(10^5)}{0.0872} $$\n使用对数性质 $\\ln(x^y) = y \\ln(x)$，我们得到：\n$$ n = \\frac{5 \\ln(10)}{0.0872} $$\n使用 10 的自然对数的近似值 $\\ln(10) \\approx 2.302585$：\n$$ n \\approx \\frac{5 \\times 2.302585}{0.0872} $$\n$$ n \\approx \\frac{11.512925}{0.0872} $$\n$$ n \\approx 132.028956 $$\n\n样本数量 $n$ 必须是一个整数。题目要求的是确保第二类错误*至多为* $\\beta^*$ 的最小整数样本量。关系式 $\\beta_n^* \\approx \\exp(-nC)$ 表明 $\\beta_n^*$ 是 $n$ 的递减函数。因此，为了确保错误概率小于或等于目标值 $\\beta^*$，我们必须将计算出的 $n$ 值向上取整到下一个整数。这个数学运算被称为向上取整函数（ceiling function）。\n\n$$ n_{\\text{min}} = \\lceil 132.028956 \\rceil = 133 $$\n因此，需要最少 133 个样本才能满足指定的错误概率要求。", "answer": "$$\\boxed{133}$$", "id": "1630537"}, {"introduction": "最后一个练习将挑战你的逆向思维，从而加深你对斯坦因引理的理解。在这个思想实验中，我们不再从已知的分布出发计算性能，而是根据一个已知的检验性能（即错误指数）来反推数据源的可能特性。这个过程将揭示分布的内在属性（如此处的偏倚程度）与它们在统计上可区分性之间的深刻联系。[@problem_id:1630535]", "problem": "考虑一个二元假设检验，该检验旨在区分一个由 $n$ 个独立同分布的二元结果 $X_1, \\dots, X_n$ 组成的序列的两种模型，其中每个结果可以是 0 或 1。\n\n零假设 $H_0$ 将结果建模为来自一枚均匀的硬币，即成功概率（P(X=1)）为 $1/2$ 的伯努利分布。\n\n备择假设 $H_1$ 将结果建模为来自一枚有偏的硬币，即成功概率为 $p$ 的伯努利分布，其中 $p$ 是一个常数，满足 $p \\in [0, 1]$ 且 $p \\neq 1/2$。\n\n我们感兴趣的是这样一种决策规则：在样本数量 $n$ 很大的渐近极限下，将第一类错误概率（错误地拒绝 $H_0$）保持在任意小的常数 $\\epsilon > 0$ 以下，同时最小化第二类错误概率 $\\beta_n$（错误地接受 $H_0$）。这个最小第二类错误概率 $\\beta_n^*$ 的最优指数衰减率由错误指数给出，定义为 $\\lim_{n \\to \\infty} \\left(-\\frac{1}{n} \\log_2 \\beta_n^*\\right)$。该指数以比特为单位（对数以 2 为底），量化了随着样本数量的增加，第二类错误概率消失的速度。\n\n如果实验测得该错误指数恰好为 1 比特，那么以下哪个选项正确地指出了参数 $p$ 的所有可能值？\n\nA. 仅 $p=0$\n\nB. 仅 $p=1$\n\nC. $p=0$ 和 $p=1$\n\nD. $p=1/4$ 和 $p=3/4$\n\nE. 该问题不适定，因为不存在这样的 $p$ 值。", "solution": "我们考虑在 $H_{0}$ 下的独立同分布二元样本：$\\text{Bernoulli}(1/2)$，以及在 $H_{1}$ 下的独立同分布二元样本：$\\text{Bernoulli}(p)$，其中 $p$ 为固定值，满足 $p \\in [0,1]$ 且 $p \\neq 1/2$。在第一类错误被约束在任意固定的 $\\epsilon > 0$ 以下的 Neyman-Pearson 设定中，Chernoff-Stein 引理指出，最优的第二类错误指数满足\n$$\n\\lim_{n \\to \\infty} \\left(-\\frac{1}{n} \\ln \\beta_{n}^{*}\\right) \\;=\\; D(P_{1}\\|P_{0}),\n$$\n其中 $D(P_{1}\\|P_{0})$ 是使用自然对数的 Kullback-Leibler 散度。问题中使用 $\\log_{2}$ 定义指数，因此通过换底公式，\n$$\n\\lim_{n \\to \\infty} \\left(-\\frac{1}{n} \\log_{2} \\beta_{n}^{*}\\right) \\;=\\; \\frac{1}{\\ln 2}\\, D(P_{1}\\|P_{0}).\n$$\n因此，观测到的指数恰好为 $1$ 比特意味着\n$$\n\\frac{1}{\\ln 2}\\, D\\big(\\text{Bernoulli}(p)\\,\\|\\,\\text{Bernoulli}(1/2)\\big) \\;=\\; 1,\n$$\n或等价地\n$$\nD\\big(\\text{Bernoulli}(p)\\,\\|\\,\\text{Bernoulli}(1/2)\\big) \\;=\\; \\ln 2.\n$$\n\n计算散度：\n$$\nD\\big(\\text{Bernoulli}(p)\\,\\|\\,\\text{Bernoulli}(1/2)\\big)\n= p \\ln\\!\\left(\\frac{p}{1/2}\\right) + (1-p)\\ln\\!\\left(\\frac{1-p}{1/2}\\right)\n= p \\ln p + (1-p)\\ln(1-p) - \\ln\\!\\left(\\frac{1}{2}\\right).\n$$\n使用 $\\ln(1/2) = -\\ln 2$，上式可简化为\n$$\nD\\big(\\text{Bernoulli}(p)\\,\\|\\,\\text{Bernoulli}(1/2)\\big)\n= p \\ln p + (1-p)\\ln(1-p) + \\ln 2.\n$$\n引入以奈特为单位的伯努利熵，$h(p) = -p \\ln p - (1-p)\\ln(1-p)$。那么\n$$\nD\\big(\\text{Bernoulli}(p)\\,\\|\\,\\text{Bernoulli}(1/2)\\big)\n= \\ln 2 - h(p).\n$$\n令其等于 $\\ln 2$ 可得\n$$\n\\ln 2 - h(p) = \\ln 2 \\;\\;\\Longrightarrow\\;\\; h(p) = 0.\n$$\n对于伯努利分布，$h(p) = 0$ 当且仅当 $p \\in \\{0, 1\\}$，因为仅当分布是退化的（degenerate）时，熵才为零。\n\n因此，能使错误指数恰好为 1 比特的 $p$ 值只有 $p=0$ 和 $p=1$，对应于选项 C。", "answer": "$$\\boxed{C}$$", "id": "1630535"}]}