{"hands_on_practices": [{"introduction": "理论学习之后，让我们通过动手实践来深化理解。第一个练习将探讨信息与不确定性之间的基本反比关系。我们将以一个简单的伯努利系统（如抛硬币）为模型，直观地揭示当系统最不可预测（熵最大）时，我们能从观测中获取的关于其内在参数的信息（费雪信息）反而最少。这个练习将为你建立一个核心直觉。[@problem_id:1653764]", "problem": "考虑一个简单的二元系统，例如一个内存位或一个数字信号，它可以处于两种状态之一：“开”（用 1 表示）或“关”（用 0 表示）。设随机变量 $X$ 描述该系统的状态，遵循参数为 $p$ 的伯努利分布 (Bernoulli distribution)。即，系统处于“开”状态的概率为 $P(X=1) = p$，而处于“关”状态的概率为 $P(X=0) = 1-p$，其中 $p \\in (0, 1)$。\n\n系统状态的不确定性由 Shannon 熵来衡量，其函数为 $H(p) = -p \\ln(p) - (1-p) \\ln(1-p)$，其中 $\\ln$ 表示自然对数。\n\nFisher 信息 (Fisher information) 用于量化对 $X$ 的一次观测能提供多少关于参数 $p$ 的信息，对于伯努利分布，其定义为 $I(p) = \\frac{1}{p(1-p)}$。\n\n您的任务是分析最大不确定性点。首先，找到使熵 $H(p)$ 最大化的 $p$ 值。其次，计算在该特定 $p$ 值下的 Fisher 信息 $I(p)$。\n\n请提供两个数字作为您的最终答案：使熵最大化的 $p$ 值，以及相应的 Fisher 信息值。请将这两个值表示为精确分数或整数。", "solution": "给定伯努利熵 $H(p) = -p \\ln(p) - (1-p) \\ln(1-p)$（其中 $p \\in (0,1)$）和 Fisher 信息 $I(p) = \\frac{1}{p(1-p)}$。为了找到使 $H(p)$ 最大化的 $p$ 值，我们对 $p$ 求导，并令导数等于零。\n\n使用标准求导法则计算一阶导数：\n$$\n\\frac{d}{dp}\\big[-p \\ln p\\big] = -(\\ln p + 1), \\quad \\frac{d}{dp}\\big[-(1-p)\\ln(1-p)\\big] = \\ln(1-p) + 1.\n$$\n因此，\n$$\nH'(p) = -(\\ln p + 1) + (\\ln(1-p) + 1) = \\ln(1-p) - \\ln p = \\ln\\!\\left(\\frac{1-p}{p}\\right).\n$$\n令 $H'(p) = 0$ 以寻找临界点：\n$$\n\\ln\\!\\left(\\frac{1-p}{p}\\right) = 0 \\quad \\Longrightarrow \\quad \\frac{1-p}{p} = 1 \\quad \\Longrightarrow \\quad 1 - p = p \\quad \\Longrightarrow \\quad p = \\frac{1}{2}.\n$$\n为验证该临界点是极大值点，计算二阶导数：\n$$\nH''(p) = \\frac{d}{dp}\\big[\\ln(1-p) - \\ln p\\big] = -\\frac{1}{1-p} - \\frac{1}{p} = -\\left(\\frac{1}{1-p} + \\frac{1}{p}\\right).\n$$\n对于 $p \\in (0,1)$，括号内的两项均为正数，因此 $H''(p) < 0$，这证实了 $p = \\frac{1}{2}$ 是一个最大熵点。\n\n接下来，计算在该 $p$ 值下的 Fisher 信息：\n$$\nI\\!\\left(\\frac{1}{2}\\right) = \\frac{1}{\\left(\\frac{1}{2}\\right)\\left(1 - \\frac{1}{2}\\right)} = \\frac{1}{\\frac{1}{4}} = 4.\n$$\n因此，使熵最大化的 $p$ 值为 $\\frac{1}{2}$，相应的 Fisher 信息为 $4$。", "answer": "$$\\boxed{\\begin{pmatrix} \\frac{1}{2} & 4 \\end{pmatrix}}$$", "id": "1653764"}, {"introduction": "接下来，我们将技能提升到连续型随机变量。这个练习模拟了物理学中一个常见的情景——粒子衰变，它通常用指数分布来描述。通过计算指数分布中衰变率参数 $ \\lambda $ 的费雪信息，你将掌握在连续参数估计问题中量化信息内容的关键计算技能，这对于评估实验设计的精度至关重要。[@problem_id:1653754]", "problem": "在粒子物理学中，一类特定不稳定粒子的衰变时间可以被建模为一个随机变量。这种衰变时间的一个常用模型是指数分布。单个衰变时间 $X$ 的概率密度函数 (PDF) 由下式给出：\n$$ f(x; \\lambda) = \\lambda \\exp(-\\lambda x) $$\n其中 $x \\ge 0$，$\\lambda > 0$ 是衰变率参数。较大的 $\\lambda$ 意味着粒子平均衰变得更快。\n\n现进行一项实验来估计一种新发现粒子的衰变率 $\\lambda$。在这项实验中，一位物理学家观测到一组 $n$ 个独立同分布 (i.i.d.) 的衰变时间，记为 $X_1, X_2, \\ldots, X_n$。\n\nFisher 信息提供了一种方法，用于衡量一个可观测随机变量所带有的、关于其模型分布中未知参数的信息量。对于一组 $n$ 个独立同分布的观测值，Fisher 信息 $I_n(\\lambda)$ 量化了从数据中估计参数 $\\lambda$ 的精度。\n\n计算这组 $n$ 个衰变时间测量值关于衰变率参数 $\\lambda$ 的 Fisher 信息 $I_n(\\lambda)$。用 $n$ 和 $\\lambda$ 表示你的答案。", "solution": "我们将每个衰变时间 $X_{i}$ 建模为独立同分布，其密度函数为 $f(x;\\lambda)=\\lambda \\exp(-\\lambda x)$，$x\\ge 0$，其中 $\\lambda>0$。对于观测值 $x_{1},\\ldots,x_{n}$，似然函数为\n$$\nL(\\lambda;x_{1:n})=\\prod_{i=1}^{n}\\lambda \\exp(-\\lambda x_{i})=\\lambda^{n}\\exp\\!\\Big(-\\lambda\\sum_{i=1}^{n}x_{i}\\Big).\n$$\n对数似然函数为\n$$\n\\ell(\\lambda)=\\ln L(\\lambda;x_{1:n})=\\sum_{i=1}^{n}\\big(\\ln \\lambda-\\lambda x_{i}\\big)=n\\ln \\lambda-\\lambda\\sum_{i=1}^{n}x_{i}.\n$$\n得分函数（一阶导数）为\n$$\n\\frac{\\partial \\ell(\\lambda)}{\\partial \\lambda}=\\frac{n}{\\lambda}-\\sum_{i=1}^{n}x_{i},\n$$\n二阶导数为\n$$\n\\frac{\\partial^{2} \\ell(\\lambda)}{\\partial \\lambda^{2}}=-\\frac{n}{\\lambda^{2}}.\n$$\n根据独立同分布样本在标准正则性条件下的 Fisher 信息定义，\n$$\nI_{n}(\\lambda)=-\\mathbb{E}\\!\\left[\\frac{\\partial^{2} \\ell(\\lambda)}{\\partial \\lambda^{2}}\\right].\n$$\n由于 $\\partial^{2}\\ell/\\partial \\lambda^{2}=-n/\\lambda^{2}$ 不依赖于数据，其期望值等于其自身，因此得到\n$$\nI_{n}(\\lambda)=-\\Big(-\\frac{n}{\\lambda^{2}}\\Big)=\\frac{n}{\\lambda^{2}}.\n$$\n等价地，单个观测值的 Fisher 信息为 $I_{1}(\\lambda)=\\frac{1}{\\lambda^{2}}$，并且根据对 $n$ 个独立同分布观测值的可加性，可得 $I_{n}(\\lambda)=n I_{1}(\\lambda)=\\frac{n}{\\lambda^{2}}$。", "answer": "$$\\boxed{\\frac{n}{\\lambda^{2}}}$$", "id": "1653754"}, {"introduction": "最后一个练习是本章的点睛之笔，它将费雪信息和微分熵这两个核心概念在最重要的概率分布——高斯分布中完美地结合起来。你将分别计算高斯分布的费雪信息和微分熵，并探索它们之间的深刻联系。最终结果将揭示一个不依赖于分布具体参数（均值 $ \\mu $ 和方差 $ \\sigma^2 $）的优美普适常量，这有力地证明了参数估计的精度与系统的内在不确定性之间存在着深刻的数学关系。[@problem_id:1653753]", "problem": "一个物理量的测量被建模为一个服从高斯（正态）分布的随机变量 $X$。该分布的概率密度函数 (PDF) 由下式给出：\n$$p(x; \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right)$$\n在此，$\\mu$ 是分布的均值，代表物理量的真值，而 $\\sigma^2$ 是方差，代表测量的不确定度。方差 $\\sigma^2$ 是一个已知的正常数，而均值 $\\mu$ 是我们希望获得其信息的参数。\n\n对于这样的分布，信息论中有两个重要的分析量，即费雪信息和微分熵。\n\n费雪信息 $I(\\theta)$ 量化了可观测随机变量 $X$ 所携带的关于模型 $X$ 的分布中未知参数 $\\theta$ 的信息量。对于参数 $\\theta$，它被定义为得分平方的期望：\n$$I(\\theta) = \\mathbb{E}\\left[\\left(\\frac{\\partial}{\\partial \\theta} \\ln p(x; \\theta)\\right)^2\\right]$$\n其中期望 $\\mathbb{E}[\\cdot]$ 是相对于分布 $p(x; \\theta)$ 计算的。\n\n微分熵 $h(X)$ 是衡量连续随机变量平均不确定性的指标。其定义为：\n$$h(X) = -\\int_{-\\infty}^{\\infty} p(x) \\ln p(x) \\, dx$$\n它也可以用期望算子表示为 $h(X) = -\\mathbb{E}[\\ln p(X)]$。\n\n你的任务是计算乘积 $I(\\mu) \\cdot \\exp(2h(X))$ 的值，其中 $I(\\mu)$ 是高斯分布关于其均值 $\\mu$ 的费雪信息，而 $h(X)$ 是其微分熵。最终结果应为一个用基本数学常数表示的符号表达式。", "solution": "我们从高斯密度函数开始\n$$\np(x;\\mu,\\sigma^{2})=\\frac{1}{\\sqrt{2\\pi\\sigma^{2}}}\\exp\\!\\left(-\\frac{(x-\\mu)^{2}}{2\\sigma^{2}}\\right),\n$$\n其中 $\\sigma^{2}$ 是已知的，$\\mu$ 是我们感兴趣的参数。\n\n关于 $\\mu$ 的费雪信息：\n对数密度为\n$$\n\\ln p(x;\\mu,\\sigma^{2})=-\\frac{1}{2}\\ln(2\\pi\\sigma^{2})-\\frac{(x-\\mu)^{2}}{2\\sigma^{2}}.\n$$\n关于 $\\mu$ 的得分为\n$$\n\\frac{\\partial}{\\partial \\mu}\\ln p(x;\\mu,\\sigma^{2})=\\frac{x-\\mu}{\\sigma^{2}}.\n$$\n因此，使用 $\\mathbb{E}[(X-\\mu)^{2}]=\\sigma^{2}$，\n$$\nI(\\mu)=\\mathbb{E}\\!\\left[\\left(\\frac{\\partial}{\\partial \\mu}\\ln p(X;\\mu,\\sigma^{2})\\right)^{2}\\right]\n=\\mathbb{E}\\!\\left[\\left(\\frac{X-\\mu}{\\sigma^{2}}\\right)^{2}\\right]\n=\\frac{1}{\\sigma^{4}}\\mathbb{E}[(X-\\mu)^{2}]=\\frac{1}{\\sigma^{2}}.\n$$\n\n微分熵：\n根据定义，\n$$\nh(X)=-\\mathbb{E}[\\ln p(X;\\mu,\\sigma^{2})]\n=\\mathbb{E}\\!\\left[\\frac{1}{2}\\ln(2\\pi\\sigma^{2})+\\frac{(X-\\mu)^{2}}{2\\sigma^{2}}\\right]\n=\\frac{1}{2}\\ln(2\\pi\\sigma^{2})+\\frac{1}{2},\n$$\n这里我们使用了 $\\mathbb{E}[(X-\\mu)^{2}]=\\sigma^{2}$。等价地，\n$$\nh(X)=\\frac{1}{2}\\ln\\!\\big(2\\pi\\,\\exp(1)\\,\\sigma^{2}\\big).\n$$\n因此，\n$$\n\\exp\\big(2h(X)\\big)=\\exp\\!\\left(\\ln\\!\\big(2\\pi\\,\\exp(1)\\,\\sigma^{2}\\big)\\right)=2\\pi\\,\\exp(1)\\,\\sigma^{2}.\n$$\n\n乘积：\n$$\nI(\\mu)\\cdot \\exp\\big(2h(X)\\big)=\\frac{1}{\\sigma^{2}}\\cdot\\big(2\\pi\\,\\exp(1)\\,\\sigma^{2}\\big)=2\\pi\\,\\exp(1).\n$$\n这个最终表达式只依赖于基本常数。", "answer": "$$\\boxed{2 \\pi \\exp(1)}$$", "id": "1653753"}]}