{"hands_on_practices": [{"introduction": "率失真理论的一个核心性质是率失真函数 $R(D)$ 的凸性。这个性质听起来可能有些抽象，但通过一个称为“时分复用”的实用策略，我们可以直观地理解它。这个练习模拟了一个混合系统，它在两个不同的压缩算法之间切换，通过这种方式，我们能揭示出组合不同编码策略如何影响整体的率和失真，并从实践中验证率失真理论的凸性。[@problem_id:1650279]", "problem": "一个数字通信系统被设计用于压缩和传输来自一个无记忆信源的数据。对于该信源，任何压缩方案的性能都由一个率失真对 $(R, D)$ 来表征，其中 $R$ 是用于表示每个信源符号的平均比特数，而 $D$ 是原始数据与重建数据之间的相应平均失真。\n\n现有两种压缩算法可用于该信源，即算法A和算法B。\n- 算法A以速率 $R_A$ 运行，并达到平均失真 $D_A$。\n- 算法B以速率 $R_B$ 运行，并达到平均失真 $D_B$。\n\n现创建一个新的混合系统，用于处理一个长信源符号序列。该系统对比例为 $\\alpha$ 的符号采用算法A，对剩下比例为 $(1-\\alpha)$ 的符号采用算法B，其中 $0 \\le \\alpha \\le 1$。该系统在大的、独立的数据块上切换算法，因此切换带来的任何开销都可以忽略不计。\n\n确定该混合系统的有效速率 $R_{eff}$ 和有效平均失真 $D_{eff}$。您的答案应该是一个行矩阵，其中依次包含有效速率和有效失真，并用 $R_A, D_A, R_B, D_B,$ 和 $\\alpha$ 表示。", "solution": "假设一个长数据块包含 $N$ 个信源符号。根据设计，算法A用于处理比例为 $\\alpha$ 的符号，算法B用于处理剩下比例为 $(1-\\alpha)$ 的符号，且切换开销可以忽略。\n\n速率推导：\n- 算法A使用的总比特数是 $(\\alpha N) R_{A}$，算法B使用的总比特数是 $\\bigl((1-\\alpha) N\\bigr) R_{B}$。\n- 因此，处理 $N$ 个符号的总比特数是\n$$\n(\\alpha N) R_{A} + \\bigl((1-\\alpha) N\\bigr) R_{B}.\n$$\n- 有效速率定义为每个信源符号的平均比特数，即总比特数除以 $N$：\n$$\nR_{eff} = \\frac{(\\alpha N) R_{A} + \\bigl((1-\\alpha) N\\bigr) R_{B}}{N} = \\alpha R_{A} + (1-\\alpha) R_{B}.\n$$\n等价地，将每个符号的速率看作一个由所使用算法决定的随机变量，并应用期望的线性性质（全期望定律），可得 $R_{eff} = \\alpha R_{A} + (1-\\alpha) R_{B}$。\n\n失真推导：\n- 设 $D_{A}$ 和 $D_{B}$ 分别是算法A和算法B在给定信源和失真度量下实现的每个符号的平均失真。\n- 对于由算法A编码的 $\\alpha N$ 个符号，总失真是 $(\\alpha N) D_{A}$；对于由算法B编码的 $(1-\\alpha) N$ 个符号，总失真是 $\\bigl((1-\\alpha) N\\bigr) D_{B}$。\n- 处理 $N$ 个符号的总失真是\n$$\n(\\alpha N) D_{A} + \\bigl((1-\\alpha) N\\bigr) D_{B},\n$$\n因此，每个符号的有效平均失真是\n$$\nD_{eff} = \\frac{(\\alpha N) D_{A} + \\bigl((1-\\alpha) N\\bigr) D_{B}}{N} = \\alpha D_{A} + (1-\\alpha) D_{B}.\n$$\n这同样可以根据全期望定律，通过对给定符号使用哪种算法进行条件化来得出。\n\n因此，该混合系统的有效速率和有效失真是各独立算法的速率和失真的凸组合，权重分别为 $\\alpha$ 和 $(1-\\alpha)$。\n\n所要求的行矩阵，其第一项为有效速率，第二项为有效失真，是 $\\begin{pmatrix} R_{eff} & D_{eff} \\end{pmatrix}$，其中\n$$\nR_{eff} = \\alpha R_{A} + (1-\\alpha) R_{B}, \\quad D_{eff} = \\alpha D_{A} + (1-\\alpha) D_{B}.\n$$", "answer": "$$\\boxed{\\begin{pmatrix}\\alpha R_{A}+(1-\\alpha) R_{B} & \\alpha D_{A}+(1-\\alpha) D_{B}\\end{pmatrix}}$$", "id": "1650279"}, {"introduction": "在探索率失真曲线的边界时，一个关键点是当速率 $R=0$ 时所对应的最大可容忍失真 $D_{max}$。这代表了在没有任何信息传输的情况下，我们通过选择一个固定的最佳重构值可以达到的最小失真水平。这个练习通过引入一个非对称失真度量，挑战我们去寻找这个最佳的“猜测值”，它深刻地揭示了最优策略是如何依赖于我们定义“误差”的方式的。[@problem_id:1650343]", "problem": "在一个简单标量量化器的设计中，一个信源 $X$ 被建模为在区间 $[-1, 1]$ 上均匀分布的连续随机变量。对于零速率码，信源的所有可能输出都被映射到一个单一的、恒定的重建值 $\\hat{x}$。\n\n量化的质量由一个失真函数 $d(x, \\hat{x})$ 来衡量。对于这个特定应用，使用了一种非对称平方误差失真度量，它对信源值的低估的惩罚比高估更严重。该失真定义为：\n$$\nd(x, \\hat{x}) =\n\\begin{cases}\n4 (x - \\hat{x})^2 & \\text{若 } x < \\hat{x} \\\\\n(x - \\hat{x})^2 & \\text{若 } x \\ge \\hat{x}\n\\end{cases}\n$$\n您的任务是找到最优重建点 $\\hat{x}$，以最小化此信源和失真度量下的平均失真。\n\n将您的答案表示为一个单一的闭式解析表达式。", "solution": "设恒定重建值为 $\\hat{x} \\in \\mathbb{R}$。信源 $X$ 在 $[-1,1]$ 上均匀分布，所以其密度函数为 $f_{X}(x) = \\frac{1}{2}$（对于 $x \\in [-1,1]$），其他情况下为零。作为 $\\hat{x}$ 的函数，平均失真为\n$$\nD(\\hat{x}) = \\mathbb{E}\\!\\left[d(X,\\hat{x})\\right] = \\int_{-1}^{1} d(x,\\hat{x}) \\cdot \\frac{1}{2} \\, dx.\n$$\n使用给定的非对称平方误差失真，\n$$\nd(x,\\hat{x}) =\n\\begin{cases}\n4(x-\\hat{x})^{2}, & x<\\hat{x},\\\\\n(x-\\hat{x})^{2}, & x \\ge \\hat{x},\n\\end{cases}\n$$\n我们在 $x=\\hat{x}$ 处分割积分，但必须考虑 $\\hat{x}$ 相对于 $[-1,1]$ 的位置。考虑三种情况。\n\n情况1：$\\hat{x} \\le -1$。那么对于所有 $x \\in [-1,1]$，都有 $x \\ge \\hat{x}$，所以\n$$\nD(\\hat{x}) = \\frac{1}{2} \\int_{-1}^{1} (x-\\hat{x})^{2} \\, dx.\n$$\n计算\n$$\n\\int_{-1}^{1} (x-\\hat{x})^{2} \\, dx = \\left[\\frac{x^{3}}{3} - \\hat{x} x^{2} + \\hat{x}^{2} x\\right]_{-1}^{1} = \\frac{2}{3} + 2 \\hat{x}^{2},\n$$\n因此\n$$\nD(\\hat{x}) = \\hat{x}^{2} + \\frac{1}{3}.\n$$\n该式（在 $\\hat{x} \\le -1$ 的范围内）在边界 $\\hat{x}=-1$ 处取得最小值，得到 $D(-1)=\\frac{4}{3}$。\n\n情况2：$-1 \\le \\hat{x} \\le 1$。那么\n$$\nD(\\hat{x}) = \\frac{1}{2} \\left[ \\int_{-1}^{\\hat{x}} 4(x-\\hat{x})^{2} \\, dx + \\int_{\\hat{x}}^{1} (x-\\hat{x})^{2} \\, dx \\right].\n$$\n计算这两个积分。首先，\n$$\n\\int_{-1}^{\\hat{x}} (x-\\hat{x})^{2} \\, dx = \\left[\\frac{x^{3}}{3} - \\hat{x} x^{2} + \\hat{x}^{2} x\\right]_{-1}^{\\hat{x}} = \\frac{\\hat{x}^{3}}{3} + \\frac{1}{3} + \\hat{x} + \\hat{x}^{2}.\n$$\n其次，\n$$\n\\int_{\\hat{x}}^{1} (x-\\hat{x})^{2} \\, dx = \\left[\\frac{x^{3}}{3} - \\hat{x} x^{2} + \\hat{x}^{2} x\\right]_{\\hat{x}}^{1} = \\frac{1}{3} - \\hat{x} + \\hat{x}^{2} - \\frac{\\hat{x}^{3}}{3}.\n$$\n因此，\n$$\nD(\\hat{x}) = \\frac{1}{2} \\left[ 4\\!\\left(\\frac{\\hat{x}^{3}}{3} + \\frac{1}{3} + \\hat{x} + \\hat{x}^{2}\\right) + \\left(\\frac{1}{3} - \\hat{x} + \\hat{x}^{2} - \\frac{\\hat{x}^{3}}{3}\\right) \\right]\n= \\frac{1}{2} \\left( \\hat{x}^{3} + 5 \\hat{x}^{2} + 3 \\hat{x} + \\frac{5}{3} \\right).\n$$\n求导并令其为零：\n$$\nD'(\\hat{x}) = \\frac{3}{2} \\hat{x}^{2} + 5 \\hat{x} + \\frac{3}{2} = 0 \\quad \\Longleftrightarrow \\quad 3 \\hat{x}^{2} + 10 \\hat{x} + 3 = 0.\n$$\n解这个一元二次方程：\n$$\n\\hat{x} = \\frac{-10 \\pm \\sqrt{100 - 36}}{6} = \\frac{-10 \\pm 8}{6} \\in \\left\\{ -\\frac{1}{3}, -3 \\right\\}.\n$$\n只有 $\\hat{x} = -\\frac{1}{3}$ 位于区间 $[-1,1]$ 内。二阶导数是 $D''(\\hat{x}) = 3 \\hat{x} + 5$，在 $[-1,1]$ 上满足 $D''(\\hat{x}) \\ge 2 > 0$，所以 $D$ 在该区间上是严格凸函数，因此 $\\hat{x}=-\\frac{1}{3}$ 是此情况下的唯一最小值点。\n\n情况3：$\\hat{x} \\ge 1$。那么对于所有 $x \\in [-1,1]$，都有 $x < \\hat{x}$，所以\n$$\nD(\\hat{x}) = \\frac{1}{2} \\int_{-1}^{1} 4(x-\\hat{x})^{2} \\, dx = 2 \\left(\\frac{2}{3} + 2 \\hat{x}^{2}\\right) = \\frac{4}{3} + 4 \\hat{x}^{2},\n$$\n该式（在 $\\hat{x} \\ge 1$ 的范围内）在边界 $\\hat{x}=1$ 处取得最小值，得到 $D(1)=\\frac{4}{3} + 4$。\n\n综合比较所有情况，最小值在情况2中的 $\\hat{x} = -\\frac{1}{3}$ 处取得。因此，使平均失真最小化的最优重建点是 $\\hat{x} = -\\frac{1}{3}$。", "answer": "$$\\boxed{-\\frac{1}{3}}$$", "id": "1650343"}, {"introduction": "在理解了率失真函数的一般性质和边界点之后，是时候亲手推导一个完整的率失真函数了。本练习将引导我们处理信息论中最重要的连续信源之一——高斯信源。通过分析一个二维独立同分布的高斯信源，我们将学习如何将单个信源的率失真理论扩展到多维情况，并得出其在均方误差失真下的率失真函数表达式。[@problem_id:1652536]", "problem": "一个二维无记忆信源由随机向量 $X = (X_1, X_2)$ 定义，其中分量 $X_1$ 和 $X_2$ 是独立同分布 (i.i.d.) 的随机变量。每个分量服从均值为零、方差为 $\\sigma^2$ 的高斯分布，记为 $\\mathcal{N}(0, \\sigma^2)$。\n\n该信源将被压缩，重构 $\\hat{X} = (\\hat{X}_1, \\hat{X}_2)$ 的质量通过平均均方误差失真度量进行评估，定义为：\n$$D = \\frac{1}{2} E\\left[ \\|X - \\hat{X}\\|^2 \\right] = \\frac{1}{2} E\\left[ (X_1 - \\hat{X}_1)^2 + (X_2 - \\hat{X}_2)^2 \\right]$$\n\n确定此二维信源的率失真函数 $R(D)$。函数 $R(D)$ 表示为实现不大于 $D$ 的平均失真所需的每个信源向量的最小比特数。请给出在失真范围 $0 \\le D \\le \\sigma^2$ 内 $R(D)$ 的表达式。您的答案应该是一个关于 $\\sigma^2$ 和 $D$ 的符号表达式。", "solution": "信源向量为 $X=(X_{1},X_{2})$，其分量独立且 $X_{i}\\sim\\mathcal{N}(0,\\sigma^{2})$。失真度量是每个分量的平均均方误差，\n$$\nD=\\frac{1}{2}E\\!\\left[(X_{1}-\\hat{X}_{1})^{2}+(X_{2}-\\hat{X}_{2})^{2}\\right].\n$$\n对于一个均方误差约束为 $E[(X-\\hat{X})^{2}]\\leq d$ 的标量高斯信源 $X\\sim\\mathcal{N}(0,\\sigma^{2})$，其率失真函数（单位为比特）为\n$$\nr(d)=\\frac{1}{2}\\log_{2}\\!\\left(\\frac{\\sigma^{2}}{d}\\right),\\quad 0<d\\leq\\sigma^{2},\n$$\n且当 $d\\geq\\sigma^{2}$ 时，$r(d)=0$。这是经典的高斯率失真函数（RDF），它可以通过高斯测试信道实现，并可通过 Shannon 下界推导得出。\n\n对于具有独立分量和平均分量失真约束 $D$ 的二维无记忆信源，其向量率失真函数通过对独立的高斯模式进行反向注水得到。设分量失真为 $d_{1}$ 和 $d_{2}$，约束条件为\n$$\n\\frac{1}{2}(d_{1}+d_{2})\\leq D,\\quad 0\\leq d_{i}\\leq\\sigma^{2}.\n$$\n由于分量是独立同分布的，并且标量率失真函数 $r(d)$ 在 $(0,\\sigma^{2}]$ 上是凸的且严格递减的，最优分配将使失真均等化：对于 $0\\leq D\\leq\\sigma^{2}$，有 $d_{1}=d_{2}=D$。这也可以从 KKT 条件，或等价地，从反向注水解中得出，其中选择水位 $\\theta$ 使得\n$$\n\\frac{1}{2}\\sum_{i=1}^{2}\\min\\{\\lambda_{i},\\theta\\}=D,\n$$\n其中 $\\lambda_{1}=\\lambda_{2}=\\sigma^{2}$。对于 $0\\leq D\\leq\\sigma^{2}$，有 $\\theta=D\\leq\\sigma^{2}$，因此对于两个分量都有 $d_{i}=\\theta=D$。\n\n每个信源向量的总速率（单位为比特）是标量速率之和：\n$$\nR(D)=r(d_{1})+r(d_{2})=\\frac{1}{2}\\log_{2}\\!\\left(\\frac{\\sigma^{2}}{D}\\right)+\\frac{1}{2}\\log_{2}\\!\\left(\\frac{\\sigma^{2}}{D}\\right)\n=\\log_{2}\\!\\left(\\frac{\\sigma^{2}}{D}\\right),\n$$\n在 $0<D\\leq\\sigma^{2}$ 范围内有效，其中 $R(\\sigma^{2})=0$ 且当 $D\\to 0^{+}$ 时 $R(D)\\to\\infty$。", "answer": "$$\\boxed{\\log_{2}\\!\\left(\\frac{\\sigma^{2}}{D}\\right)}$$", "id": "1652536"}]}