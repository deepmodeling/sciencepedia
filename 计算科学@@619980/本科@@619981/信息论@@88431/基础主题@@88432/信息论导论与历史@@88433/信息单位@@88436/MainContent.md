## 引言
“信息”无处不在，但它究竟是什么？我们能否像测量长度或重量一样，精确地量化它？这个看似简单的问题，是现代通信、计算乃至我们理解宇宙的基础。长期以来，信息一直是一个模糊的哲学概念，直到 Claude Shannon 的开创性工作，才将其置于坚实的数学基础之上，解决了如何度量不确定性以及通信效率极限的根本问题。本文将带领您踏上一场发现之旅，从核心概念出发，探索信息的度量单位。在第一部分，我们将揭示信息量的基本原理，学习如何用“比特”来量化选择，并理解“熵”如何衡量一个系统的整体不确定性。接着，在第二部分，我们将跨越学科的边界，见证这些度量单位如何在工程、生物学和物理学等领域发挥其强大的解释力，将从DNA序列到[黑洞](@article_id:318975)奥秘的现象联系在一起。学完本文，您将不仅掌握信息的基本“度量衡”，更能体会到信息作为宇宙基本构成要素的深刻内涵。

## 原理与机制

在上一章中，我们开启了信息世界的大门。但“信息”这个词，我们每天都在使用，它究竟是什么？我们能像测量长度或重量一样，精确地测量它吗？答案是肯定的。这趟旅程，我们将一起探索如何量化信息，揭示其背后简单而深刻的原理。这不仅仅是关于计算机和01代码，更是一场触及物理学基本定律的发现之旅。

### 提问的艺术：信息的本质是消除不确定性

想象一个经典的游戏：“二十个问题”。你的朋友心里想好一个东西，你可以问最多二十个“是”或“否”的问题来猜出它是什么。这个游戏的核心是什么？是信息。每一个问题的答案，都帮你排除掉一些可能性，减少你的“不确定性”。

一个好的问题，应该能让你一下子排除掉一半的可能性。比如，如果东西可能成千上万，你问：“它是活的吗？”无论答案是“是”还是“否”，都瞬间将你的搜索范围缩小了一半。这就是信息论的奠基人 Claude Shannon 思考的起点。他提出，信息的[基本单位](@article_id:309297)，就应该与这种“[二分法](@article_id:301259)”选择相关。

这个单位，我们称之为**比特（bit）**，是“binary digit”的缩写。1 比特的信息，就是回答一个能将可能性减半的是非题所得到的信息量。

现在，我们来看一个更具体的场景。假设我们有一副洗得非常均匀的标准52张扑克牌，我随机抽出一张，然后告诉你这张牌是什么。为了让你唯一确定这张牌，我需要传递给你多少信息呢？ [@problem_id:1666579]

如果用“是或否”的问题来猜，你需要问多少个问题？第一次，你可以问：“这张牌是红色的吗？” 答“是”，剩下26张。第二次问：“是方片吗？” 答“否”，剩下13张黑桃。继续这样问下去，你会发现需要问好几个问题。具体是多少呢？

信息论给出了一个优美的公式。对于 $N$ 个等可能性的事件，当我们得知其中一个特定事件发生时，所获得的信息量 $I$ 为：

$$ I = \log_2(N) $$

这里的 $\log_2$ 是以2为底的对数，它恰好代表了“需要问多少个‘是或否’的问题”。对于52张牌，[信息量](@article_id:333051)就是 $\log_2(52)$。你可以在计算器上算一下，这大约是 $5.7$ 比特。是的，[信息量](@article_id:333051)可以是小数！这说明，确定一张牌所需要的信息，比5个“是或否”的问题多一点，但比6个要少。这正是信息论的精妙之处——它将我们直观的感受，用数学精确地表达了出来。

### 意外之喜：信息与概率息息相关

上面的例子有一个前提：每张牌被抽到的可能性是完全相同的。但现实世界充满了不均衡。你觉得，听到“明天太阳照常升起”和听到“明天有日全食”，哪个消息包含的信息更多？显然是后者，因为它更罕见，更出乎意料。

信息论完美地捕捉到了这一点：**一个事件发生的概率越低，当它真的发生时，我们所获得的信息量就越大。**

让我们来看一个现代科技中的例子。一个计算机的内存单元，本应稳定地存储“0”状态。但由于热扰动，它有 $0.15$ 的微小概率会自发翻转成“1”。那么，它保持“0”状态的概率就是 $1 - 0.15 = 0.85$ 。[@problem_id:1666601]

现在，我们进行一次检测。如果发现它仍然是“0”，这个结果有多大的[信息量](@article_id:333051)？如果发现它居然翻转成了“1”，这个结果又有多大的[信息量](@article_id:333051)呢？

为了处理不等可能性事件，我们需要一个更普适的公式。对于一个发生概率为 $p(x)$ 的事件 $x$，其信息量（或称为“[自信息](@article_id:325761)”、“惊奇度”）为：

$$ I(x) = -\log_2\big(p(x)\big) $$

让我们来验证一下。当 $p(x)$ 很小时（比如 $0.01$），$-\log_2(p(x))$ 是一个较大的正数，[信息量](@article_id:333051)大。当 $p(x)$ 很大时（比如 $0.99$），$-\log_2(p(x))$ 是一个接近于零的正数，[信息量](@article_id:333051)小。这与我们的直觉完全吻合！

回到内存单元的例子：
- 观测到它保持‘0’（概率 $p=0.85$）的信息量是：$-\log_2(0.85) \approx 0.23$ 比特。正如所料，这是一个很小的量。
- 观测到它翻转成‘1’（概率 $p=0.15$）的[信息量](@article_id:333051)是：$-\log_2(0.15) \approx 2.74$ 比特。这个[信息量](@article_id:333051)就大得多了！

这个简单的公式蕴含着深刻的哲理：信息是对“意外”的量度。常规之事波澜不惊，意外之变信息万千。

### 从个体到整体：用“熵”衡量系统的平均不确定性

我们已经知道如何计算**单个**事件的[信息量](@article_id:333051)。但如果我们想描述一个**系统**整体的不确定性呢？比如，一位神经科学家正在研究一个简化的大脑[神经元模型](@article_id:326522)，该[神经元](@article_id:324093)在任何时刻都可能处于三种状态之一：静息（概率 $0.65$）、放电（概率 $0.05$）或[不应期](@article_id:312604)（概率 $0.30$）。[@problem_id:1666599] 在我们观测之前，这个[神经元](@article_id:324093)的“状态”有多么不确定？

为了回答这个问题，我们需要一个衡量系统平均不确定性的指标。这个指标就是大名鼎鼎的**熵（Entropy）**，通常用 $H$ 表示。

熵的定义非常直观：它就是系统所有可能状态的“[自信息](@article_id:325761)”的[期望值](@article_id:313620)（或[加权平均](@article_id:304268)值）。也就是说，我们把每个状态可能发生带来的[信息量](@article_id:333051)，乘以它发生的概率，然后加总起来：

$$ H(X) = -\sum_{i=1}^{n} p_i \log_2(p_i) $$

其中 $p_i$ 是第 $i$ 个状态发生的概率。

对于那个[神经元模型](@article_id:326522)，它的熵就是：
$$ H = - \big( 0.65 \log_2(0.65) + 0.05 \log_2(0.05) + 0.30 \log_2(0.30) \big) $$
计算结果约为 $1.14$ 比特。这个值告诉我们，平均而言，每观测一次这个[神经元](@article_id:324093)的状态，我们[期望](@article_id:311378)能获得 $1.14$ 比特的信息。熵越高，代表系统的不确定性越大，状态越难以预测。如果一个系统只有一个确定状态（概率为1），它的熵就是0，因为没有任何不确定性。

### 信息的“度量衡”：不只是比特

到目前为止，我们一直使用“比特”作为单位，因为它与计算机的二进制天生一对。但正如我们可以用米、英尺、光年来测量距离一样，信息的单位也不是唯一的。选择不同的对数底，就对应着不同的[信息单位](@article_id:326136)。

- **比特 (Bit, base 2)**：我们已经很熟悉了，它是信息论的“米制单位”，与二元选择相关。

- **奈特 (Nat, base $e$)**：当科学家们，特别是物理学家和数学家，追求数学上的“自然”与和谐时，他们倾向于使用自然对数（以 $e \approx 2.718$ 为底）。这样计算出的[信息单位](@article_id:326136)就是奈特。[@problem_id:1666602]

- **哈特雷 (Hartley, base 10)**：使用以10为底的对数，则得到了以我们日常使用的十进制命名的单位——哈特雷。例如，从0到9这10个数字中随机猜中一个，获得的[信息量](@article_id:333051)正好是 $\log_{10}(10) = 1$ 哈特雷。[@problem_id:1666610]

- **特里特 (Trit, base 3)**：如果未来的计算机是基于三进制（0, 1, 2）的，那么用以3为底的对数来定义的“特里特”将成为最自然的单位。1 特里特的信息量等于 $\log_2(3) \approx 1.585$ 比特。[@problem_id:1666573]

这些单位之间的转换非常简单，只需要应用对数的换底公式 $\log_b(x) = \frac{\log_a(x)}{\log_a(b)}$。例如，从奈特换算到比特，我们有：
$$ H_{\text{bits}} = \frac{H_{\text{nats}}}{\ln(2)} $$
这说明，不同的单位测量的都是同一个内在的、物理的量——“不确定性”，只是使用了不同的“刻度尺”而已。[@problem_id:1666597]

让我们来看一个实际的比较。一位物理学家测得一个量子系统的熵为 $15$ 哈特雷，而一位计算机科学家测得一个数据流的熵为 $45$ 比特。哪个系统更不确定呢？[@problem_id:1666612] 为了比较，我们必须将它们换算到同一个单位。1 哈特雷等于 $\log_2(10) \approx 3.32$ 比特。所以，物理学家的熵大约是 $15 \times 3.32 = 49.8$ 比特。很显然，这个量子系统比数据流具有更高的不确定性。

### 最深刻的联结：信息即物理

谈到这里，信息似乎还只是一个抽象的数学概念。但现在，我们要揭示它最令人震撼的一面：**[信息是物理的](@article_id:339966)**。

让我们把目光转向物理学的另一大支柱——[热力学](@article_id:359663)和[统计力](@article_id:373880)学。19世纪，物理学家们也定义了一个“熵”，用来描述一个宏观系统的“无序程度”，或者说，对应于该宏观态的微观状态总数。玻尔兹曼（Boltzmann）的墓碑上就刻着这个著名的公式：

$$ S = k \log W $$

这里 $S$ 是[热力学熵](@article_id:316293)，$W$ 是微观状态数，而 $k$ 是一个常数，被称为玻尔兹曼常数。这个公式的形式是不是和我们的信息公式 $I = \log(N)$ 惊人地相似？

这绝非巧合。它们本质上是同一回事。

想象一个纳米尺度的存储设备，它通过将一个粒子局限在20个可能的[量子态](@article_id:306563)之一来“写入”信息。这个过程使得设备的[热力学熵](@article_id:316293)减少了 $\Delta S = k_B \ln(20)$，其中 $k_B$ 就是玻尔兹曼常数。那么，这个操作获得了多少信息呢？[@problem_id:1666616]

根据信息与物理的深刻联系，[信息增益](@article_id:325719) $I$ 和[热力学熵](@article_id:316293)变 $\Delta S$ 的关系为：
$$ \Delta S = k_B \cdot I_{\text{nats}} $$
当我们用“奈特”（自然对数）作为[信息单位](@article_id:326136)时，玻尔兹曼常数 $k_B$ 恰好就是连接物理世界（单位：[焦耳](@article_id:308101)/[开尔文](@article_id:297450)）和信息世界（单位：奈特）的桥梁！

因此，获得的信息量就是：
$$ I_{\text{nats}} = \frac{\Delta S}{k_B} = \frac{k_B \ln(20)}{k_B} = \ln(20) \text{ 奈特} $$
如果换算成比特，就是 $\log_2(20)$ 比特；换算成哈特雷，就是 $\log_{10}(20)$ 哈特雷。

这一联系，由后来的“兰道尔原理”（Landauer's principle）进一步证实，该原理指出，擦除1比特的信息，在物理上必然会向环境中释放至少 $k_B T \ln(2)$ 的热量。信息不再是数学家的玩具，它有质量，有能量，它的增减遵循宇宙的基本物理定律。

从一个游戏开始，我们最终抵达了现代物理学的基石。我们发现，无论是从一副扑克牌中识别一张牌，还是理解[神经元](@article_id:324093)的活动，亦或是操控一个粒子的[量子态](@article_id:306563)，我们所谈论的“信息”都是同一个东西。比特、奈特和哈特雷，只是我们为衡量这个宇宙基本属性所发明的不同尺子而已。信息，深深地编织在宇宙的物理实在之中。