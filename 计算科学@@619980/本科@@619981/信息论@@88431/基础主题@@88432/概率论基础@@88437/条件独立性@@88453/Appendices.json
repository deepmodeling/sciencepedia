{"hands_on_practices": [{"introduction": "这个练习探讨了一个典型的“共因”结构，其中一个随机事件 $Z$（抛硬币）决定了两个后续事件中哪一个会发生。通过计算给定 $Z$ 时 $X$ 和 $Y$ 之间的条件互信息 $I(X;Y|Z)$，你将亲手验证信息论中的一个核心原则：对共因进行条件化可以使其各个结果相互独立。这个练习旨在帮助你理解条件独立是如何通过阻断变量间的因果路径而产生的。[@problem_id:1612649]", "problem": "在一个概率游戏中，掷哪个骰子由一次公正的硬币投掷决定。设随机变量 $Z$ 代表硬币投掷的结果，其中 $Z=1$ 代表正面，$Z=0$ 代表反面。正面的概率为 $P(Z=1) = 0.5$，反面的概率为 $P(Z=0) = 0.5$。\n\n如果硬币为正面 ($Z=1$)，则掷一个公正的六面红色骰子。该骰子的结果由随机变量 $X$ 表示，它可以等概率地取 1 到 6 之间的任意整数值。\n如果硬币为反面 ($Z=0$)，则掷一个公正的四面蓝色骰子。该骰子的结果由随机变量 $Y$ 表示，它可以等概率地取 1 到 4 之间的任意整数值。\n\n为确保随机变量 $X$ 和 $Y$ 在每次硬币投掷结果下都有明确定义，我们采用以下约定：未被投掷的骰子其结果视为 0。也就是说：\n- 如果 $Z=1$，观测到 $X \\in \\{1, 2, 3, 4, 5, 6\\}$ 的一个结果，并且 $Y$ 被赋值为 0。\n- 如果 $Z=0$，观测到 $Y \\in \\{1, 2, 3, 4\\}$ 的一个结果，并且 $X$ 被赋值为 0。\n\n基于此设定，计算在给定硬币投掷结果的条件下，红色和蓝色骰子结果之间的条件互信息，记作 $I(X;Y|Z)$。答案以比特（bits）为单位表示，这意味着所有熵的计算都使用以 2 为底的对数。", "solution": "我们使用以 2 为底的对数的条件互信息定义：\n$$\nI(X;Y|Z) \\equiv \\sum_{z} P(Z=z)\\, I(X;Y \\mid Z=z).\n$$\n我们只需计算 $I(X;Y \\mid Z=1)$ 和 $I(X;Y \\mid Z=0)$，然后以 $P(Z=1)=\\frac{1}{2}$ 和 $P(Z=0)=\\frac{1}{2}$ 为权重取平均值。\n\n对于 $Z=1$：\n- 条件分布为 $P(Y=0 \\mid Z=1)=1$（因此 $Y$ 是确定性的）以及对于 $i \\in \\{1,2,3,4,5,6\\}$ 有 $P(X=i \\mid Z=1)=\\frac{1}{6}$。\n- 使用 $I(X;Y \\mid Z=1)=H(Y \\mid Z=1)-H(Y \\mid X,Z=1)$，我们有 $H(Y \\mid Z=1)=0$ 因为在给定 $Z=1$ 的条件下 $Y$ 是一个常数，同理 $H(Y \\mid X,Z=1)=0$。因此，\n$$\nI(X;Y \\mid Z=1)=0-0=0.\n$$\n\n对于 $Z=0$：\n- 条件分布为 $P(X=0 \\mid Z=0)=1$（因此 $X$ 是确定性的）以及对于 $j \\in \\{1,2,3,4\\}$ 有 $P(Y=j \\mid Z=0)=\\frac{1}{4}$。\n- 使用 $I(X;Y \\mid Z=0)=H(X \\mid Z=0)-H(X \\mid Y,Z=0)$，我们有 $H(X \\mid Z=0)=0$ 和 $H(X \\mid Y,Z=0)=0$。因此，\n$$\nI(X;Y \\mid Z=0)=0-0=0.\n$$\n\n对 $Z$ 取平均值，\n$$\nI(X;Y \\mid Z)=\\sum_{z \\in \\{0,1\\}} P(Z=z)\\, I(X;Y \\mid Z=z)=\\tfrac{1}{2}\\cdot 0+\\tfrac{1}{2}\\cdot 0=0.\n$$\n因此条件互信息为零比特。", "answer": "$$\\boxed{0}$$", "id": "1612649"}, {"introduction": "与上一个练习形成对比，本练习将介绍“对撞”结构或“解释消除”效应，这是条件独立中一个非常违反直觉但至关重要的概念。我们从两个完全独立的变量 $X$ 和 $Y$ 开始，然后观察它们的组合效应 $Z = X \\oplus Y$。你的任务是计算条件互信息 $I(X;Y|Z)$，从而量化地证明：对一个共同效应进行条件化，反而会在两个原本独立的变量之间产生依赖关系。 [@problem_id:1612630]", "problem": "在许多现代数据系统中，信息是以并行流的方式进行处理的。考虑一个此类系统的简化模型，它具有两个独立的二进制数据流，由随机变量 $X$ 和 $Y$ 表示。两个流都被设计为统计上无偏的，这意味着一个比特为0或1的概率相等。因此，$P(X=0) = P(X=1) = 1/2$，$P(Y=0) = P(Y=1) = 1/2$。一个监控过程并不直接观测 $X$ 和 $Y$，而是观测第三个二进制变量 $Z$，它是由两个流的相应比特进行异或（XOR）运算生成的：$Z = X \\oplus Y$。\n\n一位工程师正在分析该系统中的信息流。一个关键问题是，在给定观测输出的条件下，这两个初始流共享了多少信息。具体来说，请计算在观测到组合流 $Z$ 的条件下，两个流 $X$ 和 $Y$ 之间的条件互信息 $I(X;Y \\mid Z)$。\n\n答案以比特为单位表示。", "solution": "给定两个独立的无偏二进制随机变量 $X$ 和 $Y$，满足 $P(X=0)=P(X=1)=\\frac{1}{2}$ 和 $P(Y=0)=P(Y=1)=\\frac{1}{2}$，以及 $Z=X\\oplus Y$。我们要计算条件互信息 $I(X;Y\\mid Z)$（以比特为单位）。\n\n根据定义，\n$$\nI(X;Y\\mid Z)=H(X\\mid Z)-H(X\\mid Y,Z).\n$$\n首先计算 $H(X\\mid Z)$。我们有\n$$\nP(Z=0)=P(X=0,Y=0)+P(X=1,Y=1)=\\frac{1}{4}+\\frac{1}{4}=\\frac{1}{2},\n$$\n$$\nP(Z=1)=P(X=0,Y=1)+P(X=1,Y=0)=\\frac{1}{4}+\\frac{1}{4}=\\frac{1}{2}.\n$$\n对于 $z=0$，\n$$\nP(X=0\\mid Z=0)=\\frac{P(X=0,Z=0)}{P(Z=0)}=\\frac{P(X=0,Y=0)}{\\frac{1}{2}}=\\frac{\\frac{1}{4}}{\\frac{1}{2}}=\\frac{1}{2},\n$$\n类似地，$P(X=1\\mid Z=0)=\\frac{1}{2}$。因此 $H(X\\mid Z=0)=1$ 比特。\n\n对于 $z=1$，\n$$\nP(X=0\\mid Z=1)=\\frac{P(X=0,Z=1)}{P(Z=1)}=\\frac{P(X=0,Y=1)}{\\frac{1}{2}}=\\frac{\\frac{1}{4}}{\\frac{1}{2}}=\\frac{1}{2},\n$$\n类似地，$P(X=1\\mid Z=1)=\\frac{1}{2}$。因此 $H(X\\mid Z=1)=1$ 比特。\n\n因此，\n$$\nH(X\\mid Z)=\\sum_{z\\in\\{0,1\\}}P(Z=z)\\,H(X\\mid Z=z)=\\frac{1}{2}\\cdot 1+\\frac{1}{2}\\cdot 1=1.\n$$\n\n接下来计算 $H(X\\mid Y,Z)$。由于 $Z=X\\oplus Y$，所以 $X$ 的值由 $Y$ 和 $Z$ 通过 $X=Y\\oplus Z$ 唯一确定。因此\n$$\nH(X\\mid Y,Z)=0.\n$$\n\n综合以上结果，\n$$\nI(X;Y\\mid Z)=H(X\\mid Z)-H(X\\mid Y,Z)=1-0=1.\n$$\n因此，条件互信息为 $1$ 比特。", "answer": "$$\\boxed{1}$$", "id": "1612630"}, {"introduction": "在掌握了“共因”和“对撞”这两种基本结构后，这个练习将引导你进入一个更微妙的分析场景。问题不再局限于简单的图模型，而是要求你基于第一性原理来判断条件独立性。你将研究在给定三个独立伯努利试验之和为偶数的条件下，其中两个试验是否条件独立。通过这个练习，你会发现条件独立性有时不仅取决于变量间的结构关系，还可能依赖于其底层概率分布的具体参数，这将加深你对条件独立概念的理解。[@problem_id:1612639]", "problem": "考虑三个随机变量 $X, Y, Z$，它们代表了三次独立同分布的伯努利试验的结果。每次试验成功的概率（即变量取值为1）为 $p$，其中 $P(\\text{trial}=1) = p$，失败的概率（取值为0）为 $P(\\text{trial}=0) = 1-p$。参数 $p$ 被限制在开区间 $(0, 1)$ 内。\n\n令 $E$ 为结果之和为偶数的事件，即 $E$ 是 $X+Y+Z$ 为偶整数的事件。\n\n在什么条件下，随机变量 $X$ 和 $Y$ 在给定事件 $E$ 的情况下是条件独立的？\n\nA. 对于任何 $p \\in (0,1)$ 的值，给定 $E$，$X$ 和 $Y$ 总是条件独立的。\n\nB. 对于任何 $p \\in (0,1)$ 的值，给定 $E$，$X$ 和 $Y$ 永远不是条件独立的。\n\nC. 给定 $E$，$X$ 和 $Y$ 是条件独立的，当且仅当 $p = 1/2$。\n\nD. 给定 $E$，$X$ 和 $Y$ 是条件独立的，当且仅当 $p$ 是一个有理数。\n\nE. 在不知道 $Z$ 的具体结果的情况下，无法确定给定 $E$ 时 $X$ 和 $Y$ 的条件独立性。", "solution": "令 $X,Y,Z$ 是独立的 $\\operatorname{Bernoulli}(p)$ 随机变量，其中 $p \\in (0,1)$。定义 $E=\\{X+Y+Z \\text{ 是偶数}\\}$。事件 $E$ 发生，当且仅当变量中恰好有 $0$ 个或恰好有 $2$ 个为 $1$，所以\n$$\nP(E)=P(X=0,Y=0,Z=0)+P(\\text{exactly two of }X,Y,Z\\text{ are }1)\n=(1-p)^{3}+3p^{2}(1-p).\n$$\n等价地，\n$$\nP(E)=(1-p)\\big((1-p)^{2}+3p^{2}\\big).\n$$\n\n计算以 $E$ 为条件的联合概率：\n- 如果 $X=1$ 且 $Y=1$，那么为了使 $E$ 发生，$Z$ 必须为 $0$，所以\n$$\nP(X=1,Y=1,E)=P(X=1,Y=1,Z=0)=p^{2}(1-p).\n$$\n- 如果 $X=1$ 且 $Y=0$，那么 $Z$ 必须为 $1$，所以\n$$\nP(X=1,Y=0,E)=P(X=1,Y=0,Z=1)=p(1-p)p=p^{2}(1-p).\n$$\n- 根据对称性，\n$$\nP(X=0,Y=1,E)=p^{2}(1-p), \\quad P(X=0,Y=0,E)=(1-p)^{3}.\n$$\n因此，\n$$\nP(X=1,Y=1 \\mid E)=\\frac{p^{2}(1-p)}{P(E)}, \\quad\nP(X=1,Y=0 \\mid E)=\\frac{p^{2}(1-p)}{P(E)},\n$$\n$$\nP(X=0,Y=1 \\mid E)=\\frac{p^{2}(1-p)}{P(E)}, \\quad\nP(X=0,Y=0 \\mid E)=\\frac{(1-p)^{3}}{P(E)}.\n$$\n\n在 $E$ 条件下的边缘概率是\n$$\nP(X=1 \\mid E)=P(X=1,Y=1 \\mid E)+P(X=1,Y=0 \\mid E)=\\frac{2p^{2}(1-p)}{P(E)},\n$$\n并且根据对称性，$P(Y=1 \\mid E)=\\frac{2p^{2}(1-p)}{P(E)}$。\n\n对于条件独立性，我们需要例如\n$$\nP(X=1,Y=1 \\mid E)=P(X=1 \\mid E)\\,P(Y=1 \\mid E).\n$$\n代入上面的表达式得到\n$$\n\\frac{p^{2}(1-p)}{P(E)}=\\left(\\frac{2p^{2}(1-p)}{P(E)}\\right)^{2}.\n$$\n两边同乘以 $P(E)^{2}$，并注意到 $p \\in (0,1)$，所以我们可以除以 $p^{2}(1-p)>0$，我们得到\n$$\nP(E)=4p^{2}(1-p).\n$$\n使用 $P(E)=(1-p)\\big((1-p)^{2}+3p^{2}\\big)$ 并除以 $(1-p)>0$ 得出\n$$\n(1-p)^{2}+3p^{2}=4p^{2} \\quad \\Longrightarrow \\quad 1-2p+4p^{2}=4p^{2} \\quad \\Longrightarrow \\quad 1-2p=0,\n$$\n所以\n$$\np=\\frac{1}{2}.\n$$\n\n充分性成立：当 $p=\\frac{1}{2}$ 时，上述四个联合条件概率相等，均为 $\\frac{1}{4}$，边缘概率为 $\\frac{1}{2}$，因此对于所有 $a,b \\in \\{0,1\\}$，都有 $P(X=a,Y=b \\mid E)=P(X=a \\mid E)P(Y=b \\mid E)$。\n\n因此，给定 $E$，$X$ 和 $Y$ 是条件独立的，当且仅当 $p=\\frac{1}{2}$，这对应于选项 C。", "answer": "$$\\boxed{C}$$", "id": "1612639"}]}