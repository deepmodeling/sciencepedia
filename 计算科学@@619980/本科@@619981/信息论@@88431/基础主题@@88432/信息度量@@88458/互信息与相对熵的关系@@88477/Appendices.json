{"hands_on_practices": [{"introduction": "互信息的核心思想是衡量一个变量中包含的关于另一个变量的信息量，而它与相对熵（KL 散度）的深刻联系为我们提供了一种强大的计算工具。这个练习将带你通过一个具体的天气预报模型，直接应用互信息作为联合分布与边际分布乘积之间 KL 散度的定义，即 $I(X;Y) = D_{KL}(p(x,y) || p(x)p(y))$。通过这个计算，你将掌握从给定的联合概率分布出发，一步步求得互信息的基本方法。[@problem_id:1654581]", "problem": "考虑一个简化的天气预报系统模型。设随机变量 $X$ 代表每日的天气预报，它可以取三种状态之一：$x_1$（“预报：晴”）、$x_2$（“预报：多云”）或 $x_3$（“预报：雨”）。设随机变量 $Y$ 代表当天的实际天气状况，它可以取两种状态之一：$y_1$（“实际：晴”）或 $y_2$（“实际：非晴”）。\n\n在对该系统进行长期观测后，预报与实际状况的联合概率分布 $p(x, y)$ 被确定如下：\n- $p(X=x_1, Y=y_1) = 5/16$\n- $p(X=x_1, Y=y_2) = 1/16$\n- $p(X=x_2, Y=y_1) = 1/8$\n- $p(X=x_2, Y=y_2) = 3/16$\n- $p(X=x_3, Y=y_1) = 1/16$\n- $p(X=x_3, Y=y_2) = 1/4$\n\n计算预报 $X$ 与实际天气 $Y$ 之间的互信息 $I(X; Y)$。将您的答案以比特为单位表示，并四舍五入到四位有效数字。所有对数均应解释为以 2 为底。", "solution": "我们已知 $(X,Y)$ 在 $\\{x_{1},x_{2},x_{3}\\}\\times\\{y_{1},y_{2}\\}$ 上的联合分布：\n$$\np(x_{1},y_{1})=\\frac{5}{16},\\quad p(x_{1},y_{2})=\\frac{1}{16},\\quad\np(x_{2},y_{1})=\\frac{1}{8},\\quad p(x_{2},y_{2})=\\frac{3}{16},\\quad\np(x_{3},y_{1})=\\frac{1}{16},\\quad p(x_{3},y_{2})=\\frac{1}{4}.\n$$\n首先，通过对另一个变量的联合概率求和来计算边缘概率：\n$$\np(x_{1})=\\frac{5}{16}+\\frac{1}{16}=\\frac{3}{8},\\quad\np(x_{2})=\\frac{1}{8}+\\frac{3}{16}=\\frac{5}{16},\\quad\np(x_{3})=\\frac{1}{16}+\\frac{1}{4}=\\frac{5}{16},\n$$\n$$\np(y_{1})=\\frac{5}{16}+\\frac{1}{8}+\\frac{1}{16}=\\frac{8}{16}=\\frac{1}{2},\\quad\np(y_{2})=\\frac{1}{16}+\\frac{3}{16}+\\frac{1}{4}=\\frac{8}{16}=\\frac{1}{2}.\n$$\n互信息为\n$$\nI(X;Y)=\\sum_{x}\\sum_{y}p(x,y)\\,\\log_{2}\\!\\left(\\frac{p(x,y)}{p(x)\\,p(y)}\\right).\n$$\n由于 $p(y_{1})=p(y_{2})=\\frac{1}{2}$，该比率简化为 $\\frac{p(x,y)}{p(x)\\,p(y)}=\\frac{2\\,p(x,y)}{p(x)}$。计算每一项：\n$$\n\\begin{aligned}\n(x_{1},y_{1}):&\\quad \\frac{2\\,p}{p(x_{1})}=\\frac{2\\cdot\\frac{5}{16}}{\\frac{3}{8}}=\\frac{5}{3},&&\\text{贡献为 } \\frac{5}{16}\\log_{2}\\!\\left(\\frac{5}{3}\\right),\\\\\n(x_{1},y_{2}):&\\quad \\frac{2\\,p}{p(x_{1})}=\\frac{2\\cdot\\frac{1}{16}}{\\frac{3}{8}}=\\frac{1}{3},&&\\text{贡献为 } \\frac{1}{16}\\log_{2}\\!\\left(\\frac{1}{3}\\right),\\\\\n(x_{2},y_{1}):&\\quad \\frac{2\\,p}{p(x_{2})}=\\frac{2\\cdot\\frac{1}{8}}{\\frac{5}{16}}=\\frac{4}{5},&&\\text{贡献为 } \\frac{1}{8}\\log_{2}\\!\\left(\\frac{4}{5}\\right),\\\\\n(x_{2},y_{2}):&\\quad \\frac{2\\,p}{p(x_{2})}=\\frac{2\\cdot\\frac{3}{16}}{\\frac{5}{16}}=\\frac{6}{5},&&\\text{贡献为 } \\frac{3}{16}\\log_{2}\\!\\left(\\frac{6}{5}\\right),\\\\\n(x_{3},y_{1}):&\\quad \\frac{2\\,p}{p(x_{3})}=\\frac{2\\cdot\\frac{1}{16}}{\\frac{5}{16}}=\\frac{2}{5},&&\\text{贡献为 } \\frac{1}{16}\\log_{2}\\!\\left(\\frac{2}{5}\\right),\\\\\n(x_{3},y_{2}):&\\quad \\frac{2\\,p}{p(x_{3})}=\\frac{2\\cdot\\frac{1}{4}}{\\frac{5}{16}}=\\frac{8}{5},&&\\text{贡献为 } \\frac{1}{4}\\log_{2}\\!\\left(\\frac{8}{5}\\right).\n\\end{aligned}\n$$\n因此\n$$\n\\begin{aligned}\nI(X;Y)\n&=\\frac{5}{16}\\log_{2}\\!\\left(\\frac{5}{3}\\right)+\\frac{1}{16}\\log_{2}\\!\\left(\\frac{1}{3}\\right)+\\frac{1}{8}\\log_{2}\\!\\left(\\frac{4}{5}\\right)+\\frac{3}{16}\\log_{2}\\!\\left(\\frac{6}{5}\\right)+\\frac{1}{16}\\log_{2}\\!\\left(\\frac{2}{5}\\right)+\\frac{1}{4}\\log_{2}\\!\\left(\\frac{8}{5}\\right)\\\\\n&=\\frac{5}{16}(\\log_{2}5-\\log_{2}3)+\\frac{1}{16}(-\\log_{2}3)+\\frac{1}{8}(2-\\log_{2}5)+\\frac{3}{16}(\\log_{2}3+1-\\log_{2}5)+\\frac{1}{16}(1-\\log_{2}5)+\\frac{1}{4}(3-\\log_{2}5)\\\\\n&=\\left(\\frac{1}{4}+\\frac{3}{16}+\\frac{1}{16}+\\frac{3}{4}\\right)+\\left(-\\frac{5}{16}-\\frac{1}{16}+\\frac{3}{16}\\right)\\log_{2}3+\\left(\\frac{5}{16}-\\frac{1}{8}-\\frac{3}{16}-\\frac{1}{16}-\\frac{1}{4}\\right)\\log_{2}5\\\\\n&=\\frac{5}{4}-\\frac{3}{16}\\log_{2}3-\\frac{5}{16}\\log_{2}5.\n\\end{aligned}\n$$\n使用以 2 为底的对数进行数值计算，并四舍五入到四位有效数字，可得\n$$\nI(X;Y)\\approx 0.2272\\ \\text{比特}.\n$$", "answer": "$$\\boxed{0.2272}$$", "id": "1654581"}, {"introduction": "在实际的系统中，某些事件组合可能因为物理限制或系统设计而永远不会发生，这在概率模型中表现为零概率项。这个练习探讨了在这种情况下如何计算互信息，特别是如何处理 $p(x,y)=0$ 的情况。通过分析一个简化的数字通信系统模型，你将加深对 KL 散度公式中 $0 \\log 0 = 0$ 约定的理解，并体会系统性约束如何影响变量间的信息共享。[@problem_id:1654642]", "problem": "考虑一个简化的数字通信系统模型，该模型涉及一个发射的二进制信号 $X$ 和一个接收的二进制信号 $Y$。$X$ 和 $Y$ 均可取值于集合 $\\{0, 1\\}$。由于信道中一种特定类型的系统性故障，某些发射和接收信号的组合比其他组合或多或少地更可能出现，且有一种组合是不可能的。描述该系统行为的联合概率质量函数 $p(x,y) = P(X=x, Y=y)$ 由下式给出：\n\n$p(0,0) = 0$\n$p(0,1) = \\frac{1}{4}$\n$p(1,0) = \\frac{1}{2}$\n$p(1,1) = \\frac{1}{4}$\n\n计算发射信号和接收信号之间的互信息 $I(X;Y)$。请以比特为单位，给出闭式解析表达式形式的答案。", "solution": "互信息 $I(X;Y)$ 量化了因了解随机变量 $Y$ 而导致关于随机变量 $X$ 的不确定性的减少量。它可以表示为联合概率分布 $p(x,y)$ 与边缘概率分布的乘积 $p(x)p(y)$ 之间的 Kullback-Leibler (KL) 散度（或称相对熵）。其公式为：\n$$I(X;Y) = D_{KL}(p(x,y) || p(x)p(y)) = \\sum_{x \\in \\{0,1\\}} \\sum_{y \\in \\{0,1\\}} p(x,y) \\log_{2}\\left(\\frac{p(x,y)}{p(x)p(y)}\\right)$$\n由于答案要求的单位是比特，此处的对数以2为底。\n\n首先，我们必须计算 $X$ 和 $Y$ 的边缘概率分布。\n\n$X$ 的边缘分布，记为 $p(x)$，通过对所有可能的 $Y$ 值求和联合概率得到：\n对于 $x=0$：\n$p(X=0) = p(0,0) + p(0,1) = 0 + \\frac{1}{4} = \\frac{1}{4}$\n对于 $x=1$：\n$p(X=1) = p(1,0) + p(1,1) = \\frac{1}{2} + \\frac{1}{4} = \\frac{3}{4}$\n\n$Y$ 的边缘分布，记为 $p(y)$，通过对所有可能的 $X$ 值求和联合概率得到：\n对于 $y=0$：\n$p(Y=0) = p(0,0) + p(1,0) = 0 + \\frac{1}{2} = \\frac{1}{2}$\n对于 $y=1$：\n$p(Y=1) = p(0,1) + p(1,1) = \\frac{1}{4} + \\frac{1}{4} = \\frac{1}{2}$\n\n现在我们可以计算 $I(X;Y)$ 求和式中的各项。求和遍及 $(x,y)$ 的所有四种组合。\n\n对于 $(x,y) = (0,0)$ 的项：\n由于 $p(0,0) = 0$，整个项是 $0 \\times \\log_{2}(\\dots)$。按照惯例，$0 \\log 0 = 0$，因此该项对总和的贡献为0。\n\n对于 $(x,y) = (0,1)$ 的项：\n边缘概率的乘积为 $p(X=0)p(Y=1) = (\\frac{1}{4})(\\frac{1}{2}) = \\frac{1}{8}$。\n该项为：\n$$p(0,1) \\log_{2}\\left(\\frac{p(0,1)}{p(X=0)p(Y=1)}\\right) = \\frac{1}{4} \\log_{2}\\left(\\frac{1/4}{1/8}\\right) = \\frac{1}{4} \\log_{2}(2) = \\frac{1}{4} \\times 1 = \\frac{1}{4}$$\n\n对于 $(x,y) = (1,0)$ 的项：\n边缘概率的乘积为 $p(X=1)p(Y=0) = (\\frac{3}{4})(\\frac{1}{2}) = \\frac{3}{8}$。\n该项为：\n$$p(1,0) \\log_{2}\\left(\\frac{p(1,0)}{p(X=1)p(Y=0)}\\right) = \\frac{1}{2} \\log_{2}\\left(\\frac{1/2}{3/8}\\right) = \\frac{1}{2} \\log_{2}\\left(\\frac{4}{3}\\right)$$\n使用对数性质 $\\log(a/b) = \\log(a) - \\log(b)$：\n$$\\frac{1}{2} (\\log_{2}(4) - \\log_{2}(3)) = \\frac{1}{2} (2 - \\log_{2}(3)) = 1 - \\frac{1}{2}\\log_{2}(3)$$\n\n对于 $(x,y) = (1,1)$ 的项：\n边缘概率的乘积为 $p(X=1)p(Y=1) = (\\frac{3}{4})(\\frac{1}{2}) = \\frac{3}{8}$。\n该项为：\n$$p(1,1) \\log_{2}\\left(\\frac{p(1,1)}{p(X=1)p(Y=1)}\\right) = \\frac{1}{4} \\log_{2}\\left(\\frac{1/4}{3/8}\\right) = \\frac{1}{4} \\log_{2}\\left(\\frac{2}{3}\\right)$$\n使用对数性质 $\\log(a/b) = \\log(a) - \\log(b)$：\n$$\\frac{1}{4} (\\log_{2}(2) - \\log_{2}(3)) = \\frac{1}{4} (1 - \\log_{2}(3)) = \\frac{1}{4} - \\frac{1}{4}\\log_{2}(3)$$\n\n最后，我们将所有项相加得到互信息：\n$$I(X;Y) = 0 + \\frac{1}{4} + \\left(1 - \\frac{1}{2}\\log_{2}(3)\\right) + \\left(\\frac{1}{4} - \\frac{1}{4}\\log_{2}(3)\\right)$$\n$$I(X;Y) = \\left(\\frac{1}{4} + 1 + \\frac{1}{4}\\right) - \\left(\\frac{1}{2}\\log_{2}(3) + \\frac{1}{4}\\log_{2}(3)\\right)$$\n$$I(X;Y) = \\frac{3}{2} - \\frac{3}{4}\\log_{2}(3)$$\n这就是以比特为单位的互信息的最终闭式解析表达式。", "answer": "$$\\boxed{\\frac{3}{2} - \\frac{3}{4}\\log_{2}(3)}$$", "id": "1654642"}, {"introduction": "将互信息理解为相对熵不仅有助于计算，更能揭示信息论中的基本原理。此练习要求你证明一个核心推论：对于形成马尔可夫链 $X \\to Y \\to Z$ 的任意随机变量，在已知中间变量 $Y$ 的条件下，$X$ 和 $Z$ 之间的条件互信息 $I(X;Z|Y)$ 为零。这个证明过程直接利用了条件互信息的 KL 散度定义，它不仅是一个计算练习，更是一次对数据处理不等式背后逻辑的深刻洞察。[@problem_id:1654632]", "problem": "考虑一个由三个离散随机变量 $X, Y, Z$ 组成的序列，它们构成一个马尔可夫链，记作 $X \\to Y \\to Z$。这种关系意味着在给定当前状态 $Y$ 的条件下，未来状态 $Z$ 与过去状态 $X$ 是条件独立的。在数学上，这个条件被定义为 $p(z | x, y) = p(z | y)$，适用于所有使得联合概率 $p(x, y) > 0$ 的 $x, y, z$ 值。\n\n在信息论中，条件互信息 $I(X; Z | Y)$ 量化了在已知 $Y$ 的情况下，$X$ 和 $Z$ 共享的信息量。它可以根据真实条件联合分布 $p(x, z | y)$ 与条件边缘分布的乘积 $p(x|y)p(z|y)$ 之间的 Kullback-Leibler (KL) 散度来定义。公式如下：\n\n$$I(X; Z | Y) = \\sum_{x, y, z} p(x, y, z) \\log_{2} \\left( \\frac{p(x, z | y)}{p(x|y)p(z|y)} \\right)$$\n\n其中，求和遍及随机变量所有可能的值。\n\n根据这些定义，计算对于任何形成马尔可夫链 $X \\to Y \\to Z$ 的变量集，其条件互信息 $I(X; Z | Y)$ 的值。请用比特（bits）为单位，将答案表示为一个数字。", "solution": "该问题要求计算一组构成马尔可夫链 $X \\to Y \\to Z$ 的随机变量 $X, Y, Z$ 的条件互信息 $I(X; Z | Y)$。我们已知 $I(X; Z | Y)$ 以 Kullback-Leibler 散度表示的定义：\n$$I(X; Z | Y) = \\sum_{x, y, z} p(x, y, z) \\log_{2} \\left( \\frac{p(x, z | y)}{p(x|y)p(z|y)} \\right)$$\n我们的目标是利用马尔可夫链的性质来简化对数里的项。\n\n解决这个问题的关键在于证明对于马尔可夫链，对数内的分子 $p(x, z | y)$ 等于分母 $p(x|y)p(z|y)$。让我们来证明这个因式分解。\n\n根据条件概率的定义，$p(x, z | y)$ 这一项可以写成：\n$$p(x, z | y) = \\frac{p(x, y, z)}{p(y)}$$\n现在，我们将概率的链式法则应用于联合分布 $p(x, y, z)$：\n$$p(x, y, z) = p(z | x, y) p(x, y)$$\n将此代入 $p(x, z | y)$ 的表达式中：\n$$p(x, z | y) = \\frac{p(z | x, y) p(x, y)}{p(y)}$$\n我们已知 $X, Y, Z$ 构成一个马尔可夫链 $X \\to Y \\to Z$。该链的定义属性是 $p(z | x, y) = p(z | y)$。我们可以将这个属性代入我们的方程中：\n$$p(x, z | y) = \\frac{p(z | y) p(x, y)}{p(y)}$$\n接下来，我们可以重新排列右侧的项：\n$$p(x, z | y) = p(z | y) \\left( \\frac{p(x, y)}{p(y)} \\right)$$\n我们识别出括号中的项是条件概率 $p(x|y)$ 的定义：\n$$p(x|y) = \\frac{p(x, y)}{p(y)}$$\n将此代回，我们得到所需的因式分解：\n$$p(x, z | y) = p(z | y) p(x | y)$$\n这个结果表明，对于马尔可夫链，在给定 $Y$ 的条件下，变量 $X$ 和 $Z$ 是条件独立的。\n\n现在我们可以将这个结果代回到条件互信息的原始公式中：\n$$I(X; Z | Y) = \\sum_{x, y, z} p(x, y, z) \\log_{2} \\left( \\frac{p(x|y)p(z|y)}{p(x|y)p(z|y)} \\right)$$\n对数内的分数简化为 1：\n$$I(X; Z | Y) = \\sum_{x, y, z} p(x, y, z) \\log_{2}(1)$$\n因为 $\\log_{2}(1) = 0$，所以表达式变为：\n$$I(X; Z | Y) = \\sum_{x, y, z} p(x, y, z) \\cdot 0$$\n这简化为：\n$$I(X; Z | Y) = 0$$\n因此，对于任何马尔可夫链 $X \\to Y \\to Z$，给定 $Y$ 时 $X$ 和 $Z$ 之间的条件互信息为 0。这是一个重要的结果，被称为数据处理不等式，本例是其一个特例。其值为 0 比特。", "answer": "$$\\boxed{0}$$", "id": "1654632"}]}