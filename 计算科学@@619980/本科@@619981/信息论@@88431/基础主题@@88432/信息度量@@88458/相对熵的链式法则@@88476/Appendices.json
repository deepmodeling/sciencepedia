{"hands_on_practices": [{"introduction": "在处理联合分布的复杂性之前，我们首先需要掌握其基本组成部分。本练习聚焦于计算边际相对熵，这是相对熵链式法则中的一个关键组成部分。通过这个练习，你将从一个联合概率分布中提取边际分布，并量化在只考虑单个变量时，真实模型与近似模型之间的信息差异。[@problem_id:1609351]", "problem": "考虑两个二元随机变量 $X_1$ 和 $X_2$，每个变量都可以从集合 $\\{0, 1\\}$ 中取值。一位工程师正在评估两个不同的概率模型，记为 $p$ 和 $q$，用于描述这些变量的联合行为。\n\n第一个模型是一个特定的数据驱动模型，由联合概率质量函数 $p(x_1, x_2)$ 描述，其值如下：\n- $p(X_1=0, X_2=0) = \\frac{1}{2}$\n- $p(X_1=0, X_2=1) = \\frac{1}{4}$\n- $p(X_1=1, X_2=0) = \\frac{1}{8}$\n- $p(X_1=1, X_2=1) = \\frac{1}{8}$\n\n第二个模型是一个更简单的基线模型，它假设结果是均匀分布的。它由联合概率质量函数 $q(x_1, x_2)$ 描述：\n- $q(X_1=0, X_2=0) = \\frac{1}{4}$\n- $q(X_1=0, X_2=1) = \\frac{1}{4}$\n- $q(X_1=1, X_2=0) = \\frac{1}{4}$\n- $q(X_1=1, X_2=1) = \\frac{1}{4}$\n\n您的任务是，在边际上考虑每个变量时，量化使用模型 $p$ 而非模型 $q$ 所带来的信息增益，或称“意外”程度。计算 $X_1$ 的边际分布之间的 Kullback-Leibler 散度（也称为相对熵），记为 $D(p(x_1) || q(x_1))$，以及 $X_2$ 的边际分布之间的 Kullback-Leibler 散度，记为 $D(p(x_2) || q(x_2))$。\n\n请以一对使用以 2 为底的对数的精确解析表达式的形式提供您的最终答案。第一个表达式应为 $D(p(x_1) || q(x_1))$，第二个表达式应为 $D(p(x_2) || q(x_2))$。", "solution": "我们已知两个二元随机变量在模型 $p$ 和 $q$ 下的联合分布。在相同支撑集上的两个离散分布 $r$ 和 $s$ 之间的 Kullback-Leibler 散度（使用以 2 为底的对数）定义为\n$$\nD(r||s)=\\sum_{x} r(x)\\log_{2}\\left(\\frac{r(x)}{s(x)}\\right).\n$$\n我们需要 $D(p(x_{1})||q(x_{1}))$ 和 $D(p(x_{2})||q(x_{2}))$，所以我们首先计算边际分布。\n\n根据给定的 $p(x_{1},x_{2})$，\n$$\np(x_{1}=0)=p(0,0)+p(0,1)=\\frac{1}{2}+\\frac{1}{4}=\\frac{3}{4},\\quad\np(x_{1}=1)=p(1,0)+p(1,1)=\\frac{1}{8}+\\frac{1}{8}=\\frac{1}{4},\n$$\n$$\np(x_{2}=0)=p(0,0)+p(1,0)=\\frac{1}{2}+\\frac{1}{8}=\\frac{5}{8},\\quad\np(x_{2}=1)=p(0,1)+p(1,1)=\\frac{1}{4}+\\frac{1}{8}=\\frac{3}{8}.\n$$\n根据均匀分布的 $q(x_{1},x_{2})$，边际分布为\n$$\nq(x_{1}=0)=q(0,0)+q(0,1)=\\frac{1}{4}+\\frac{1}{4}=\\frac{1}{2},\\quad\nq(x_{1}=1)=\\frac{1}{2},\n$$\n$$\nq(x_{2}=0)=q(0,0)+q(1,0)=\\frac{1}{4}+\\frac{1}{4}=\\frac{1}{2},\\quad\nq(x_{2}=1)=\\frac{1}{2}.\n$$\n现在计算散度。对于 $X_{1}$，\n$$\nD(p(x_{1})||q(x_{1}))=\\sum_{x_{1}\\in\\{0,1\\}} p(x_{1})\\log_{2}\\left(\\frac{p(x_{1})}{q(x_{1})}\\right)\n=\\frac{3}{4}\\log_{2}\\left(\\frac{\\frac{3}{4}}{\\frac{1}{2}}\\right)+\\frac{1}{4}\\log_{2}\\left(\\frac{\\frac{1}{4}}{\\frac{1}{2}}\\right)\n=\\frac{3}{4}\\log_{2}\\left(\\frac{3}{2}\\right)+\\frac{1}{4}\\log_{2}\\left(\\frac{1}{2}\\right).\n$$\n因为 $\\log_{2}\\left(\\frac{1}{2}\\right)=-1$，这也可以写成\n$$\nD(p(x_{1})||q(x_{1}))=\\frac{3}{4}\\log_{2}\\left(\\frac{3}{2}\\right)-\\frac{1}{4}.\n$$\n对于 $X_{2}$，\n$$\nD(p(x_{2})||q(x_{2}))=\\sum_{x_{2}\\in\\{0,1\\}} p(x_{2})\\log_{2}\\left(\\frac{p(x_{2})}{q(x_{2})}\\right)\n=\\frac{5}{8}\\log_{2}\\left(\\frac{\\frac{5}{8}}{\\frac{1}{2}}\\right)+\\frac{3}{8}\\log_{2}\\left(\\frac{\\frac{3}{8}}{\\frac{1}{2}}\\right)\n=\\frac{5}{8}\\log_{2}\\left(\\frac{5}{4}\\right)+\\frac{3}{8}\\log_{2}\\left(\\frac{3}{4}\\right).\n$$\n这些是按要求给出的使用以 2 为底的对数的精确解析表达式。", "answer": "$$\\boxed{\\begin{pmatrix}\\frac{3}{4}\\log_{2}\\left(\\frac{3}{2}\\right)-\\frac{1}{4} & \\frac{5}{8}\\log_{2}\\left(\\frac{5}{4}\\right)+\\frac{3}{8}\\log_{2}\\left(\\frac{3}{4}\\right)\\end{pmatrix}}$$", "id": "1609351"}, {"introduction": "掌握了边际散度的计算后，我们可以进一步探索链式法则如何将联合分布的总散度进行分解。这个练习构建了一个巧妙的场景：真实模型 $p(x,y)$ 与近似模型 $q(x,y)$ 具有完全相同的条件概率分布。通过分析这个特殊案例，你将见证链式法则如何揭示总信息差异完全来源于边际分布的差异，从而深刻理解条件相对熵项的含义。[@problem_id:1609354]", "problem": "考虑两个不同的联合概率分布 $p(x,y)$ 和 $q(x,y)$，它们定义在一对二元随机变量 $X$ 和 $Y$ 上，两个变量的样本空间均为 $\\{0, 1\\}$。这两个分布是基于以下边缘概率和条件概率构建的：\n\n1.  在分布 $p$ 下，$X$ 的边缘概率分布由 $p(X=0) = \\frac{1}{2}$ 和 $p(X=1) = \\frac{1}{2}$ 给出。\n2.  在分布 $q$ 下，$X$ 的边缘概率分布由 $q(X=0) = \\frac{1}{3}$ 和 $q(X=1) = \\frac{2}{3}$ 给出。\n3.  对于 $p$ 和 $q$ 两个分布，给定 $X$ 时 $Y$ 的条件概率分布是相同的，具体规定如下：\n    *   $p(Y=0|X=0) = q(Y=0|X=0) = \\frac{1}{4}$\n    *   $p(Y=0|X=1) = q(Y=0|X=1) = \\frac{3}{4}$\n\n剩余的条件概率 $p(Y=1|X=x)$ 和 $q(Y=1|X=x)$ 由概率之和必须为一的公理确定。\n\n计算 Kullback-Leibler (KL) 散度，也称为相对熵，$D(p(x,y) || q(x,y))$。请用以 2 为底的对数，将您的答案表示为一个闭式解析表达式。", "solution": "题目要求计算两个联合概率分布 $p(x,y)$ 和 $q(x,y)$ 之间的 Kullback-Leibler (KL) 散度 $D(p(x,y) || q(x,y))$。\n\nKL 散度的链式法则指出，联合分布的散度可以分解为一个边缘分布的散度与一个条件散度之和：\n$$D(p(x,y) || q(x,y)) = D(p(x) || q(x)) + D(p(y|x) || q(y|x) | p(x))$$\n\n让我们分析第二项，即条件相对熵，其定义为：\n$$D(p(y|x) || q(y|x) | p(x)) = \\sum_{x \\in \\{0,1\\}} p(x) D(p(y|X=x) || q(y|X=x))$$\n内项 $D(p(y|X=x) || q(y|X=x))$ 是对于一个特定的 $X$ 值，$Y$ 的条件分布之间的 KL 散度。我们知道，对于任意两个概率分布 $P$ 和 $Q$，$D(P||Q) \\ge 0$，等号成立当且仅当 $P=Q$。\n\n让我们检查题目中给出的条件分布。\n当 $X=0$ 时：\n给定 $p(Y=0|X=0) = \\frac{1}{4}$ 和 $q(Y=0|X=0) = \\frac{1}{4}$。\n由于对于给定的 $x$，条件概率之和必须为 1：\n$p(Y=1|X=0) = 1 - p(Y=0|X=0) = 1 - \\frac{1}{4} = \\frac{3}{4}$。\n$q(Y=1|X=0) = 1 - q(Y=0|X=0) = 1 - \\frac{1}{4} = \\frac{3}{4}$。\n因此，对于所有 $y \\in \\{0,1\\}$，我们有 $p(y|X=0) = q(y|X=0)$。\n\n当 $X=1$ 时：\n给定 $p(Y=0|X=1) = \\frac{3}{4}$ 和 $q(Y=0|X=1) = \\frac{3}{4}$。\n同样地：\n$p(Y=1|X=1) = 1 - p(Y=0|X=1) = 1 - \\frac{3}{4} = \\frac{1}{4}$。\n$q(Y=1|X=1) = 1 - q(Y=0|X=1) = 1 - \\frac{3}{4} = \\frac{1}{4}$。\n因此，对于所有 $y \\in \\{0,1\\}$，我们有 $p(y|X=1) = q(y|X=1)$。\n\n由于对于 $x=0$ 和 $x=1$ 两种情况，条件分布 $p(y|X=x)$ 和 $q(y|X=x)$ 都是相同的，因此它们之间的 KL 散度在每种情况下都为零：\n$D(p(y|X=0) || q(y|X=0)) = 0$。\n$D(p(y|X=1) || q(y|X=1)) = 0$。\n\n现在我们可以计算总的条件相对熵：\n$$D(p(y|x) || q(y|x) | p(x)) = p(X=0) D(p(y|X=0) || q(y|X=0)) + p(X=1) D(p(y|X=1) || q(y|X=1))$$\n$$D(p(y|x) || q(y|x) | p(x)) = p(X=0) \\cdot 0 + p(X=1) \\cdot 0 = 0$$\n\n将这个结果代回链式法则方程，可以极大地简化问题：\n$$D(p(x,y) || q(x,y)) = D(p(x) || q(x)) + 0 = D(p(x) || q(x))$$\n问题现在简化为计算边缘分布 $p(x)$ 和 $q(x)$ 之间的 KL 散度。\n\n边缘分布由下式给出：\n$p(x)$: $p(X=0) = \\frac{1}{2}$，$p(X=1) = \\frac{1}{2}$。\n$q(x)$: $q(X=0) = \\frac{1}{3}$，$q(X=1) = \\frac{2}{3}$。\n\n$D(p(x) || q(x))$ 的计算公式为：\n$$D(p(x) || q(x)) = \\sum_{x \\in \\{0,1\\}} p(x) \\log_2 \\frac{p(x)}{q(x)}$$\n$$D(p(x) || q(x)) = p(X=0) \\log_2 \\left(\\frac{p(X=0)}{q(X=0)}\\right) + p(X=1) \\log_2 \\left(\\frac{p(X=1)}{q(X=1)}\\right)$$\n代入数值：\n$$D(p(x) || q(x)) = \\frac{1}{2} \\log_2 \\left(\\frac{1/2}{1/3}\\right) + \\frac{1}{2} \\log_2 \\left(\\frac{1/2}{2/3}\\right)$$\n$$D(p(x) || q(x)) = \\frac{1}{2} \\log_2 \\left(\\frac{3}{2}\\right) + \\frac{1}{2} \\log_2 \\left(\\frac{3}{4}\\right)$$\n使用对数性质 $\\log_2(a) + \\log_2(b) = \\log_2(ab)$：\n$$D(p(x) || q(x)) = \\frac{1}{2} \\log_2 \\left(\\frac{3}{2} \\cdot \\frac{3}{4}\\right) = \\frac{1}{2} \\log_2 \\left(\\frac{9}{8}\\right)$$\n使用性质 $\\log_2(a/b) = \\log_2(a) - \\log_2(b)$：\n$$D(p(x) || q(x)) = \\frac{1}{2} (\\log_2(9) - \\log_2(8))$$\n因为 $\\log_2(9) = \\log_2(3^2) = 2\\log_2(3)$ 且 $\\log_2(8) = \\log_2(2^3) = 3$：\n$$D(p(x) || q(x)) = \\frac{1}{2} (2\\log_2(3) - 3)$$\n$$D(p(x) || q(x)) = \\log_2(3) - \\frac{3}{2}$$\n因此，KL 散度 $D(p(x,y) || q(x,y))$ 是 $\\log_2(3) - \\frac{3}{2}$。", "answer": "$$\\boxed{\\log_{2}(3) - \\frac{3}{2}}$$", "id": "1609354"}, {"introduction": "这个最终练习将我们对信息分解的理解应用于一个更复杂的混合模型场景。通过分析一个因混合概率估计错误而产生的近似模型，你将发现 $D_{KL}(p || q)$ 如何能够精确地分离出与特定模型参数（在此为混合系数）相关的信息损失。此问题挑战你超越公式的直接套用，运用链式法则背后的概念框架，优雅地解决一个看似棘手的问题。[@problem_id:1609364]", "problem": "在一个数据合成过程中，一个系统从两种潜在机制之一生成随机变量对 $(X, Y)$，这两种机制由概率分布 $p_1(x,y)$ 和 $p_2(x,y)$ 描述。在每一步中，系统以 $\\alpha$ 的真实概率选择机制1，以 $1-\\alpha$ 的真实概率选择机制2，其中 $0 < \\alpha < 1$。所生成数据的最终真实分布是一个混合模型，由 $p(x,y) = \\alpha p_1(x,y) + (1-\\alpha) p_2(x,y)$ 给出。\n\n已知这两种潜在机制具有以下两个性质：\n1.  对于两种机制，变量 $Y$ 的边缘概率分布是相同的，即对于所有可能的输出 $y$，都有 $p_1(y) = p_2(y)$。\n2.  对于任意给定的输出 $y$，在机制1下 $X$ 的可能输出集合与在机制2下 $X$ 的可能输出集合完全不相交。换句话说，它们的支撑集是不相交的，因此如果 $p_1(x|y) > 0$，则 $p_2(x|y) = 0$，反之亦然。\n\n一位工程师创建了一个近似模型 $q(x,y)$ 来描述该系统。该模型正确地假设了混合形式以及相同的分量分布 $p_1$ 和 $p_2$，但它基于一个不正确的混合概率估计，使用了值 $\\beta$ 而不是 $\\alpha$，其中 $0 < \\beta < 1$。因此，该工程师的模型为 $q(x,y) = \\beta p_1(x,y) + (1-\\beta) p_2(x,y)$。\n\n计算使用近似模型 $q(x,y)$ 代替真实模型 $p(x,y)$ 所导致的信息损失。这个量被定义为概率对数比的期望值 $\\mathbb{E}_{p}\\left[\\ln \\frac{p(x,y)}{q(x,y)}\\right]$，也称为 Kullback-Leibler (KL) 散度 $D_{KL}(p || q)$。将最终答案表示为用 $\\alpha$ 和 $\\beta$ 表示的符号表达式。", "solution": "我们需要计算 Kullback-Leibler 散度\n$$\nD_{KL}(p\\|q)=\\mathbb{E}_{p}\\left[\\ln\\frac{p(X,Y)}{q(X,Y)}\\right]=\\iint p(x,y)\\,\\ln\\!\\left(\\frac{p(x,y)}{q(x,y)}\\right)\\,\\mathrm{d}x\\,\\mathrm{d}y,\n$$\n对于 $p(x,y)=\\alpha p_{1}(x,y)+(1-\\alpha)p_{2}(x,y)$ 和 $q(x,y)=\\beta p_{1}(x,y)+(1-\\beta)p_{2}(x,y)$，在以下假设下：\n1) 对所有 $y$，都有 $p_{1}(y)=p_{2}(y)$，\n2) 对每个 $y$，$p_{1}(x\\mid y)$ 和 $p_{2}(x\\mid y)$ 的支撑集是不相交的；即，如果 $p_{1}(x\\mid y)>0$ 则 $p_{2}(x\\mid y)=0$，反之亦然。\n\n定义集合\n$$\nA=\\{(x,y):p_{1}(x\\mid y)>0\\},\\qquad B=\\{(x,y):p_{2}(x\\mid y)>0\\}。\n$$\n根据假设2，$A$ 和 $B$ 是不相交的，并且在 $p$-测度为零的集合之外覆盖了 $p$ 的支撑集。在 $A$ 上，我们有 $p_{2}(x\\mid y)=0$，因此 $p_{2}(x,y)=p_{2}(x\\mid y)p_{2}(y)=0$，这意味着\n$$\np(x,y)=\\alpha p_{1}(x,y),\\qquad q(x,y)=\\beta p_{1}(x,y),\\qquad (x,y)\\in A.\n$$\n因此，对于 $(x,y)\\in A$，\n$$\n\\ln\\!\\left(\\frac{p(x,y)}{q(x,y)}\\right)=\\ln\\!\\left(\\frac{\\alpha p_{1}(x,y)}{\\beta p_{1}(x,y)}\\right)=\\ln\\!\\left(\\frac{\\alpha}{\\beta}\\right).\n$$\n类似地，在 $B$ 上我们有 $p_{1}(x\\mid y)=0$，因此 $p_{1}(x,y)=0$，从而\n$$\np(x,y)=(1-\\alpha)p_{2}(x,y),\\qquad q(x,y)=(1-\\beta)p_{2}(x,y),\\qquad (x,y)\\in B,\n$$\n这得到\n$$\n\\ln\\!\\left(\\frac{p(x,y)}{q(x,y)}\\right)=\\ln\\!\\left(\\frac{1-\\alpha}{1-\\beta}\\right),\\qquad (x,y)\\in B.\n$$\n\n在期望的计算中利用这些分段常数值，我们将积分进行划分并将常数项提出：\n$$\nD_{KL}(p\\|q)=\\iint_{A} p(x,y)\\,\\ln\\!\\left(\\frac{\\alpha}{\\beta}\\right)\\,\\mathrm{d}x\\,\\mathrm{d}y+\\iint_{B} p(x,y)\\,\\ln\\!\\left(\\frac{1-\\alpha}{1-\\beta}\\right)\\,\\mathrm{d}x\\,\\mathrm{d}y.\n$$\n这可以简化为\n$$\nD_{KL}(p\\|q)=\\ln\\!\\left(\\frac{\\alpha}{\\beta}\\right)\\iint_{A} p(x,y)\\,\\mathrm{d}x\\,\\mathrm{d}y+\\ln\\!\\left(\\frac{1-\\alpha}{1-\\beta}\\right)\\iint_{B} p(x,y)\\,\\mathrm{d}x\\,\\mathrm{d}y.\n$$\n我们计算概率 $\\iint_{A} p(x,y)\\,\\mathrm{d}x\\,\\mathrm{d}y$ 和 $\\iint_{B} p(x,y)\\,\\mathrm{d}x\\,\\mathrm{d}y$。在 $A$ 上，$p(x,y)=\\alpha p_{1}(x,y)$，并且由于支撑集不相交的假设，$p_{1}$ 的全部质量都集中在 $A$ 上，所以\n$$\n\\iint_{A} p(x,y)\\,\\mathrm{d}x\\,\\mathrm{d}y=\\alpha\\iint_{A} p_{1}(x,y)\\,\\mathrm{d}x\\,\\mathrm{d}y=\\alpha\\cdot 1=\\alpha.\n$$\n类似地，在 $B$ 上，$p(x,y)=(1-\\alpha)p_{2}(x,y)$，并且 $p_{2}$ 的全部质量都集中在 $B$ 上，所以\n$$\n\\iint_{B} p(x,y)\\,\\mathrm{d}x\\,\\mathrm{d}y=(1-\\alpha)\\iint_{B} p_{2}(x,y)\\,\\mathrm{d}x\\,\\mathrm{d}y=(1-\\alpha)\\cdot 1=(1-\\alpha).\n$$\n将这些结果代回，得到\n$$\nD_{KL}(p\\|q)=\\alpha\\,\\ln\\!\\left(\\frac{\\alpha}{\\beta}\\right)+(1-\\alpha)\\,\\ln\\!\\left(\\frac{1-\\alpha}{1-\\beta}\\right).\n$$\n这就是所求的用 $\\alpha$ 和 $\\beta$ 表示的符号表达式。", "answer": "$$\\boxed{\\alpha\\ln\\left(\\frac{\\alpha}{\\beta}\\right)+\\left(1-\\alpha\\right)\\ln\\left(\\frac{1-\\alpha}{1-\\beta}\\right)}$$", "id": "1609364"}]}