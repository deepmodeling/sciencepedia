{"hands_on_practices": [{"introduction": "本练习探讨了“反公地悲剧”（Tragedy of the Anti-commons）这一重要的经济学概念，其中多个主体对同一资源拥有排他权，反而可能导致资源未被充分利用。智能体作为理性的收益最大化者，在设定自己的准入价格时，并未考虑其决策对其他智能体收益造成的负外部性。通过模拟智能体基于“最优反应”动态调整价格的迭代过程，你将亲手揭示个体理性如何导致集体非理性的纳什均衡，并最终造成社会福利的损失[@problem_id:2370580]。这是一个理解策略互动和市场失灵的经典范例。", "problem": "考虑一个捕获“反公地悲剧”的基于智能体的环境，其中多个智能体对单一资源拥有排他权。环境中有 $N$ 个排他权持有者（智能体），索引为 $i \\in \\{1,\\dots,N\\}$。在每个名义时期，一个代表性用户对资源的估值为一个随机量 $V$，该值独立地从 $[0,1]$ 上的均匀分布中抽取。只有当用户能够支付所有智能体的准入价格总和以及每个智能体的交易摩擦成本时，她才能访问该资源。摩擦成本是非负的，用 $\\kappa \\ge 0$ 表示；它由用户承担，不归智能体所有。\n\n基本原理：\n- 预期需求等于支付意愿超过总负担的概率。在 $V \\sim \\mathrm{Uniform}[0,1]$ 的条件下，对于总负担 $P \\in [0,1]$，需求函数为 $D(P) = \\mathbb{P}(V \\ge P) = \\max(1 - P, 0)$，而对于 $P \\ge 1$，则 $D(P)=0$。\n- 每个智能体设定一个非负的准入价格 $p_i \\ge 0$。令 $P_{\\text{prices}} \\equiv \\sum_{i=1}^N p_i$ 且 $P_{\\text{total}} \\equiv P_{\\text{prices}} + N\\kappa$。\n\n基于智能体的价格更新规则：\n- 智能体是短视的收益最大化者，他们将其他智能体的当前价格视为给定，並通过松弛最优反应进行更新。给定其他智能体的价格，智能体 $i$ 选择 $p_i$ 以最大化其预期收益 $p_i \\cdot D\\big(P_{\\text{total}}\\big)$，其中 $P_{\\text{total}} = N\\kappa + p_i + \\sum_{j \\ne i} p_j$。设松弛参数为 $\\beta \\in (0,1]$，则更新规则为 $p_i \\leftarrow (1-\\beta)\\,p_i + \\beta \\cdot \\arg\\max_{p \\ge 0}\\; p \\cdot D\\!\\left(N\\kappa + p + \\sum_{j \\ne i} p_j\\right)$。\n- 该过程在智能体之间顺序迭代，直至收敛。\n\n您的任务：\n1. 实现上述基于智能体的学习动态模拟。所有 $i$ 的初始价格 $p_i = 0$。\n2. 在每次迭代中（对所有智能体进行一次遍历），使用松弛最优反应规则更新每个智能体的价格一次。\n3. 当一次遍历中所有 $p_i$ 的最大绝对变化小于给定容差 $\\varepsilon$ 时，或达到最大迭代次数时停止。\n4. 停止后，计算：\n   - 长期利用率 $u \\equiv D(P_{\\text{total}}) = \\max(1 - P_{\\text{total}}, 0)$。以小数形式（而非百分比）报告 $u$。\n   - 无条件平均实现收益 $R \\equiv u \\cdot P_{\\text{prices}}$（这是智能体在所有时期内收到的平均总付款，当没有发生准入时计为零）。\n\n需遵守的数值和算法细节：\n- 使用 $D(P) = \\max(1 - P, 0)$。\n- 在更新规则中，对标量凹问题使用精确最优反应。不要离散化价格空间。\n- 使用收敛容差 $\\varepsilon$ 和最大迭代次数上限 $\\text{max\\_iter}$。\n- 在最终报告的输出中，将 $u$ 和 $R$ 四舍五入到 $6$ 位小数。\n- 如果 $P_{\\text{total}} \\ge 1$，则根据定义 $u = 0$ 且 $R = 0$。\n\n测试套件：\n在以下参数集 $(N,\\kappa,\\beta,\\varepsilon,\\text{max\\_iter})$ 上运行您的程序：\n- 案例 $1$：$(N=\\;1,\\;\\kappa=\\;0.0,\\;\\beta=\\;1.0,\\;\\varepsilon=\\;10^{-12},\\;\\text{max\\_iter}=\\;10000)$。\n- 案例 $2$：$(N=\\;2,\\;\\kappa=\\;0.0,\\;\\beta=\\;1.0,\\;\\varepsilon=\\;10^{-12},\\;\\text{max\\_iter}=\\;10000)$。\n- 案例 $3$：$(N=\\;5,\\;\\kappa=\\;0.05,\\;\\beta=\\;1.0,\\;\\varepsilon=\\;10^{-12},\\;\\text{max\\_iter}=\\;10000)$。\n- 案例 $4$：$(N=\\;10,\\;\\kappa=\\;0.10,\\;\\beta=\\;1.0,\\;\\varepsilon=\\;10^{-12},\\;\\text{max\\_iter}=\\;10000)$。\n- 案例 $5$：$(N=\\;12,\\;\\kappa=\\;0.10,\\;\\beta=\\;1.0,\\;\\varepsilon=\\;10^{-12},\\;\\text{max\\_iter}=\\;10000)$。\n\n要求的最终输出格式：\n- 您的程序应生成单行输出，其中包含五个案例的结果，格式为一个由方括号括起来的逗号分隔列表，每个案例的结果本身是一个双元素列表 $[u,R]$，两个条目都四舍五入到 $6$ 位小数。例如，整体格式必须如下所示：\n\"[[u_1,R_1],[u_2,R_2],[u_3,R_3],[u_4,R_4],[u_5,R_5]]\"\n前后不带任何额外文本。", "solution": "所呈现的问题是基于智能体的计算经济学中一个定义明确的练习，具体模拟了“反公地悲剧”。该模型具有科学依据且内部一致，允许直接的计算求解。我们将首先推导智能体的最优行为，然后描述用以寻找系统均衡点的模拟算法。\n\n问题的核心在于确定每个智能体的最优反应函数。智能体 $i$ 试图在给定所有其他智能体价格 $\\mathbf{p}_{-i} = \\{p_j\\}_{j \\ne i}$ 的情况下，选择其价格 $p_i \\ge 0$ 以最大化自身预期收益 $\\pi_i$。智能体是短视的，意味着它不考虑其他智能体对其当前价格选择的未来反应。\n\n智能体的收益是其价格与用户购买准入权的概率的乘积。用户的总成本，即负担，是所有价格的总和加上总交易摩擦：$P_{\\text{total}} = \\sum_{j=1}^N p_j + N\\kappa$。令 $S_{-i} = \\sum_{j \\ne i} p_j$ 为所有其他智能体设定的价格之和。那么从智能体 $i$ 的角度来看，总负担可以写作 $P_{\\text{total}} = p_i + S_{-i} + N\\kappa$。\n\n需求函数由用户的估值 $V$（从 $\\mathrm{Uniform}[0,1]$ 分布中抽取）超过总负担的概率给出：$D(P_{\\text{total}}) = \\mathbb{P}(V \\ge P_{\\text{total}}) = \\max(0, 1 - P_{\\text{total}})$。\n\n因此，智能体 $i$ 的优化问题是：\n$$ \\max_{p_i \\ge 0} \\pi_i(p_i) = p_i \\cdot D(p_i + S_{-i} + N\\kappa) $$\n$$ \\max_{p_i \\ge 0} \\pi_i(p_i) = p_i \\cdot \\max(0, 1 - (p_i + S_{-i} + N\\kappa)) $$\n\n我们来分析这个目标函数。如果成本的固定部分 $S_{-i} + N\\kappa$ 大于或等于 $1$，那么对于任何 $p_i > 0$，总负担 $P_{\\text{total}}$ 将严格大于 $1$。这将导致需求为零，$D(P_{\\text{total}}) = 0$，从而收益为零，$\\pi_i = 0$。如果 $p_i = 0$，收益也为 $0$。因此，如果 $S_{-i} + N\\kappa \\ge 1$，任何非负价格 $p_i$ 产生的收益都为零，最优反应就是 $p_i = 0$。\n\n现在，考虑 $S_{-i} + N\\kappa < 1$ 的情况。为了使需求为正，我们必须有 $p_i + S_{-i} + N\\kappa < 1$，这意味着 $p_i < 1 - S_{-i} - N\\kappa$。在此范围内，目标函数为：\n$$ \\pi_i(p_i) = p_i (1 - S_{-i} - N\\kappa - p_i) $$\n这是关于 $p_i$ 的二次函数，代表一个开口向下的抛物线：$\\pi_i(p_i) = -p_i^2 + (1 - S_{-i} - N\\kappa)p_i$。该函数的最大值点可以通过将其关于 $p_i$ 的导数设为零来找到：\n$$ \\frac{\\partial \\pi_i}{\\partial p_i} = 1 - S_{-i} - N\\kappa - 2p_i = 0 $$\n求解 $p_i$ 得到无约束最优价格：\n$$ p_i^* = \\frac{1 - S_{-i} - N\\kappa}{2} $$\n由于我们正在考虑 $1 - S_{-i} - N\\kappa > 0$ 的情况，这个价格 $p_i^*$ 是正的。只要这个无约束最大值满足非负约束 $p_i \\ge 0$ (它确实满足)，它就是解。\n\n结合两种情况，给出智能体 $i$ 在给定其他智能体价格下的最优价格的最优反应函数 $\\mathrm{BR}_i(\\mathbf{p}_{-i})$ 是：\n$$ \\mathrm{BR}_i(\\mathbf{p}_{-i}) = \\max\\left(0, \\frac{1 - N\\kappa - \\sum_{j \\ne i} p_j}{2}\\right) $$\n这个表达式提供了问题陈述所要求的精确最优反应。\n\n该模拟实现了一个迭代过程，以找到这场定价博弈的纳什均衡。系统在任何时候的状态是价格向量 $\\mathbf{p} = (p_1, \\dots, p_N)$。过程从所有 $i \\in \\{1, \\dots, N\\}$ 的初始价格 $p_i = 0$ 开始。\n\n在每次迭代或遍历中，智能体顺序更新他们的价格。对于智能体 $i$，其新价格 $p_i'$ 使用松弛最优反应规则计算：\n$$ p_i' = (1-\\beta)p_i + \\beta \\cdot \\mathrm{BR}_i(\\mathbf{p}_{-i}) $$\n这里，$p_i$ 是智能体在前一次迭代中的价格，而 $\\mathbf{p}_{-i}$ 包含智能体 $j<i$ 的最新更新价格和智能体 $j>i$ 的前一次迭代价格。这是一种高斯-赛德尔 (Gauss-Seidel) 类型的更新方案。松弛参数 $\\beta \\in (0,1]$ 控制了向最优反应方向移动的步长。当 $\\beta = 1$ 时，智能体完全采用新的最优反应价格。\n\n模拟最多进行 $\\text{max\\_iter}$ 次迭代。如果价格收敛，它会提前终止。当一次完整遍历中任何智能体价格的最大绝对变化小于指定的容差 $\\varepsilon$ 时，即达到收敛：\n$$ \\max_{i \\in \\{1,\\dots,N\\}} |p_i' - p_i| < \\varepsilon $$\n\n终止时，设最终价格向量为 $\\mathbf{p}^{\\text{final}}$。聚合指标计算如下：\n1.  最终的价格总和是 $P_{\\text{prices}} = \\sum_{i=1}^N p_i^{\\text{final}}$。\n2.  最终的总负担是 $P_{\\text{total}} = P_{\\text{prices}} + N\\kappa$。\n3.  长期利用率是 $u = D(P_{\\text{total}}) = \\max(0, 1 - P_{\\text{total}})$。\n4.  无条件平均实现收益是 $R = u \\cdot P_{\\text{prices}}$。\n\n然后按要求将 $u$ 和 $R$ 的结果四舍五入到 $6$ 位小数。所提供的代码为每个指定的测试案例实现了这一逻辑。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef simulate(N, kappa, beta, epsilon, max_iter):\n    \"\"\"\n    Simulates the agent-based price setting in an anti-commons environment.\n\n    Args:\n        N (int): Number of agents.\n        kappa (float): Per-agent transaction friction cost.\n        beta (float): Relaxation parameter for price updates.\n        epsilon (float): Convergence tolerance.\n        max_iter (int): Maximum number of iterations.\n\n    Returns:\n        tuple: A tuple containing the utilization rate (u) and total revenue (R).\n    \"\"\"\n    # Initialize prices p_i = 0 for all i.\n    prices = np.zeros(N, dtype=np.float64)\n    \n    for iteration in range(max_iter):\n        old_prices = prices.copy()\n        max_change = 0.0\n        \n        # Sequentially update each agent's price.\n        for i in range(N):\n            # Sum of other agents' prices. This uses updated prices for j < i\n            # and old prices for j > i, which is a Gauss-Seidel update.\n            S_minus_i = np.sum(prices) - prices[i]\n            \n            # Calculate the best response for agent i.\n            numerator = 1.0 - N * kappa - S_minus_i\n            best_response = max(0.0, numerator / 2.0)\n            \n            # Apply the relaxed best-response update rule.\n            new_price = (1.0 - beta) * prices[i] + beta * best_response\n            \n            # Update the agent's price in the vector for the current iteration.\n            prices[i] = new_price\n            \n        # Check for convergence after a full sweep over all agents.\n        # The problem asks for checking convergence based on changes within a sweep.\n        # old_prices store the state before the sweep, prices store state after.\n        max_change = np.max(np.abs(prices - old_prices))\n        \n        if max_change < epsilon:\n            break\n            \n    # Calculate final metrics after convergence or max iterations.\n    P_prices = np.sum(prices)\n    P_total = P_prices + N * kappa\n    \n    # Utilization rate u\n    u = max(0.0, 1.0 - P_total)\n    \n    # Mean realized revenue R\n    R = u * P_prices\n    \n    return u, R\n\ndef solve():\n    \"\"\"\n    Runs the simulation for all test cases and prints the results in the required format.\n    \"\"\"\n    # Test suite: (N, kappa, beta, epsilon, max_iter)\n    test_cases = [\n        (1, 0.0, 1.0, 1e-12, 10000),   # Case 1\n        (2, 0.0, 1.0, 1e-12, 10000),   # Case 2\n        (5, 0.05, 1.0, 1e-12, 10000),  # Case 3\n        (10, 0.10, 1.0, 1e-12, 10000), # Case 4\n        (12, 0.10, 1.0, 1e-12, 10000)  # Case 5\n    ]\n\n    all_results = []\n    for case in test_cases:\n        N, kappa, beta, epsilon, max_iter = case\n        u, R = simulate(N, kappa, beta, epsilon, max_iter)\n        \n        # Round final results to 6 decimal places.\n        u_rounded = round(u, 6)\n        R_rounded = round(R, 6)\n        \n        all_results.append(f\"[{u_rounded},{R_rounded}]\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(all_results)}]\")\n\nsolve()\n\n```", "id": "2370580"}, {"introduction": "这个实践项目将视角从市场定价转向更宏观的资源配置问题，即在生产性活动与“寻租”（rent-seeking）活动之间的权衡。每个智能体都面临一个核心决策：投入多少精力去创造价值（生产），多少用于再分配现有价值（寻租）。此模型采用基于收益梯度的学习规则，模拟了智能体如何根据边际回报 $\\frac{\\partial \\pi_{i,t}}{\\partial x_{i,t}}$ 调整其努力分配，从而逐步适应环境[@problem_id:2370544]。通过实现这一模型，你将深入理解微观层面的寻租动机如何汇聚成宏观经济后果，并学会应用基于梯度的适应性行为规则。", "problem": "您的任务是设计并实现一个基于主体的计算经济学（ACE）模拟。该模拟的对象是一群主体，他们重复选择如何将单位努力分配给生产活动和寻租活动。模型必须由以下基本组件和规则构成。\n\n基本基础和核心定义：\n- 共有 $N$ 个主体，索引为 $i \\in \\{1,\\dots,N\\}$。时间是离散的，索引为 $t \\in \\{0,1,2,\\dots,T-1\\}$。\n- 主体 $i$ 选择其单位努力的一部分比例 $x_{i,t} \\in [0,1]$ 分配给生产活动（标记为 $K$），并将剩余部分 $r_{i,t} = 1 - x_{i,t}$ 分配给寻租活动。\n- 每个主体在周期 $t$ 的产出为 $y_{i,t} = A \\cdot x_{i,t}^{\\alpha}$，其中 $A > 0$ 且 $\\alpha \\in (0,1)$；这是一个标准的凹生产函数。总产出为 $Y_t = \\sum_{j=1}^{N} y_{j,t}$。\n- 总产出的一部分比例 $\\tau \\in [0,1]$ 被征税并重新分配给寻租者。周期 $t$ 的寻租池为 $\\tau \\cdot Y_t$。令 $R_t = \\sum_{j=1}^{N} r_{j,t}$ 表示总寻租努力。如果 $R_t > 0$，每个主体 $i$ 获得的寻租转移支付等于 $\\tau \\cdot Y_t \\cdot \\frac{r_{i,t}}{R_t}$。如果 $R_t = 0$，则所有主体的寻租转移支付为零。\n- 主体 $i$ 的总周期回报为\n$$\n\\pi_{i,t} \\;=\\; (1 - \\tau)\\, y_{i,t} \\;+\\; \\tau \\, Y_t \\,\\frac{r_{i,t}}{\\max(R_t, \\varepsilon)},\n$$\n其中 $\\varepsilon > 0$ 是一个数值正则化常数，仅用于在计算中避免除以零（见下文）。\n\n行为调整规则（短视回报梯度学习）：\n- 主体通过一个遵循边界的回报单调调整来更新其生产分配：\n$$\nx_{i,t+1} \\;=\\; \\mathrm{proj}_{[0,1]}\\!\\Big(x_{i,t} \\;+\\; \\eta \\cdot \\frac{\\partial \\pi_{i,t}}{\\partial x_{i,t}} \\cdot x_{i,t}\\,\\big(1-x_{i,t}\\big)\\Big),\n$$\n其中 $\\eta > 0$ 是一个学习率参数，$\\mathrm{proj}_{[0,1]}(\\cdot)$ 表示在 $[0,1]$ 上的投影，而 $\\frac{\\partial \\pi_{i,t}}{\\partial x_{i,t}}$ 是 $x_{i,t}$ 对 $\\pi_{i,t}$ 的周期内边际效应。乘法因子 $x_{i,t}(1-x_{i,t})$ 确保更新在边界处消失。第二个正则化常数 $\\delta_x > 0$ 必须仅在导数计算内部使用，通过将 $x_{i,t}$ 替换为 $\\max(x_{i,t}, \\delta_x)$ 来避免当 $x_{i,t}=0$ 时出现的奇异点，这适用于所有 $x_{i,t}$ 出现在指数小于1的幂次中的情况。\n\n推导目标和实现细节：\n- 从上述定义出发，计算导数 $\\frac{\\partial \\pi_{i,t}}{\\partial x_{i,t}}$，将 $x_{i,t}$ 视为在时间 $t$ 主体 $i$ 的唯一选择变量（保持其他主体的选择不变）。使用恒等式 $y_{i,t} = A \\cdot x_{i,t}^{\\alpha}$，$Y_t = \\sum_{j=1}^{N} y_{j,t}$，$r_{i,t} = 1 - x_{i,t}$，以及 $R_t = \\sum_{j=1}^{N} r_{j,t}$。您的实现必须在所有除以 $R_t$ 的地方使用正则化分母 $\\max(R_t,\\varepsilon)$，并且在涉及 $x_{i,t}^{\\alpha - 1}$ 的项中必须将 $x_{i,t}$ 替换为 $\\max(x_{i,t},\\delta_x)$ 以避免未定义的值。\n- 对所有主体 $i$，使用相同的起始分配 $x_{i,0} = x_0 \\in [0,1]$ 进行初始化。\n\n数值单位和报告：\n- 所有作为分数的参数（如 $\\tau$、$x_{i,t}$ 和 $x_0$）必须表示为 $[0,1]$ 范围内的小数，而不是百分比。\n- 角度不在此问题中出现。\n- 物理单位不在此问题中出现。\n\n任务：\n- 为下面的测试套件中的每个参数集，实现一个完整的、可运行的程序，模拟系统 $T$ 个周期。对于每个参数集，返回一个浮点数，该数等于 $T$ 次更新后的人口平均值 $\\frac{1}{N}\\sum_{i=1}^{N} x_{i,T}$（即，最终周期的平均生产分配）。每个报告的数字必须精确到小数点后 $6$ 位。\n\n测试套件（五个参数集）：\n- 测试 1 (理想路径): $(N, A, \\alpha, \\tau, \\eta, T, x_0, \\varepsilon, \\delta_x) = (200, 1.0, 0.5, 0.3, 0.05, 1000, 0.5, 10^{-8}, 10^{-8})$。\n- 测试 2 (无再分配边界): $(N, A, \\alpha, \\tau, \\eta, T, x_0, \\varepsilon, \\delta_x) = (200, 1.0, 0.5, 0.0, 0.05, 500, 0.2, 10^{-8}, 10^{-8})$。\n- 测试 3 (高再分配边缘): $(N, A, \\alpha, \\tau, \\eta, T, x_0, \\varepsilon, \\delta_x) = (200, 1.0, 0.5, 0.9, 0.02, 1000, 0.8, 10^{-8}, 10^{-8})$。\n- 测试 4 (低生产率情况): $(N, A, \\alpha, \\tau, \\eta, T, x_0, \\varepsilon, \\delta_x) = (200, 0.2, 0.5, 0.5, 0.05, 1000, 0.5, 10^{-8}, 10^{-8})$。\n- 测试 5 (近线性生产): $(N, A, \\alpha, \\tau, \\eta, T, x_0, \\varepsilon, \\delta_x) = (200, 1.0, 0.95, 0.4, 0.02, 1500, 0.5, 10^{-8}, 10^{-8})$。\n\n要求的最终输出格式：\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔的结果列表，每个数字的格式精确到小数点后 $6$ 位，顺序与上述测试一致。例如：$[0.123456,0.654321,0.500000,0.750000,0.333333]$。", "solution": "用户提供了一个问题陈述，描述了一个关于生产和寻租的基于主体的计算经济学（ACE）模型。任务是验证此模型，如果有效，则推导模拟所需的方程，实现模拟，并报告给定测试用例集的结果。\n\n### 第一步：提取已知条件\n\n-   **主体与时间**：$N$ 个主体，索引为 $i \\in \\{1,\\dots,N\\}$。离散时间 $t \\in \\{0,1,2,\\dots,T-1\\}$。\n-   **选择变量**：主体 $i$ 选择 $x_{i,t} \\in [0,1]$（投入生产的努力）。\n-   **寻租努力**：$r_{i,t} = 1 - x_{i,t}$。\n-   **生产函数**：$y_{i,t} = A \\cdot x_{i,t}^{\\alpha}$，参数为 $A > 0$ 和 $\\alpha \\in (0,1)$。\n-   **总产出**：$Y_t = \\sum_{j=1}^{N} y_{j,t}$。\n-   **寻租池**：$\\tau \\cdot Y_t$，其中 $\\tau \\in [0,1]$ 是税收/再分配率。\n-   **总寻租努力**：$R_t = \\sum_{j=1}^{N} r_{j,t}$。\n-   **寻租回报**：如果 $R_t > 0$，则为 $\\tau \\cdot Y_t \\cdot \\frac{r_{i,t}}{R_t}$；如果 $R_t = 0$，则为 $0$。\n-   **总回报函数**：$\\pi_{i,t} = (1 - \\tau)\\, y_{i,t} \\;+\\; \\tau \\, Y_t \\,\\frac{r_{i,t}}{\\max(R_t, \\varepsilon)}$，其中正则化常数 $\\varepsilon > 0$。\n-   **行为更新规则**：$x_{i,t+1} = \\mathrm{proj}_{[0,1]}\\!\\Big(x_{i,t} \\;+\\; \\eta \\cdot \\frac{\\partial \\pi_{i,t}}{\\partial x_{i,t}} \\cdot x_{i,t}\\,\\big(1-x_{i,t}\\big)\\Big)$，其中 $\\eta > 0$ 是学习率。\n-   **导数正则化**：为计算导数，在出现 $x_{i,t}^{\\alpha-1}$ 的地方，将 $x_{i,t}$ 替换为 $\\max(x_{i,t}, \\delta_x)$，使用 $\\delta_x > 0$。\n-   **初始条件**：对所有 $i \\in \\{1, \\dots, N\\}$，$x_{i,0} = x_0$。\n-   **任务**：对于每个参数集，模拟 $T$ 个周期，并报告最终的平均生产分配 $\\frac{1}{N}\\sum_{i=1}^{N} x_{i,T}$，四舍五入到小数点后 $6$ 位。\n-   **测试套件**：提供了五个参数集 $(N, A, \\alpha, \\tau, \\eta, T, x_0, \\varepsilon, \\delta_x)$。\n\n### 第二步：使用提取的已知条件进行验证\n\n根据验证标准对问题陈述进行严格评估。\n\n-   **科学依据**：该模型是竞争或寻租博弈的标准表述，在公共选择理论和计算经济学中被广泛使用。生产函数是标准的柯布-道格拉斯形式。学习规则是回报单调动态（类复制动态）的一种常见形式。作为经济权衡的风格化表述，该模型在科学上是合理的。\n-   **适定性**：该问题对于模拟是适定的。所有参数、函数和初始条件都已明确定义。更新规则是确定性的。使用投影 $\\mathrm{proj}_{[0,1]}(\\cdot)$ 和正则化常数 $\\varepsilon$、$\\delta_x$ 确保了模拟保持在有效的状态空间内，并避免了诸如除以零或非实数之类的数值奇异点。\n-   **客观性**：语言是正式、精确的，没有主观或模糊的术语。\n\n该问题没有表现出任何诸如科学上不合理、不可形式化、不完整、矛盾或不可行等缺陷。它是在指定领域内一个定义明确的计算任务。\n\n### 第三步：结论与行动\n\n问题是**有效的**。将开发一个解决方案。\n\n### 回报梯度的推导\n\n为了实现行为更新规则，我们必须首先计算主体 $i$ 的回报 $\\pi_{i,t}$ 对其自身选择 $x_{i,t}$ 的偏导数，同时保持所有其他主体（$j \\neq i$ 的 $x_{j,t}$）的选择不变。\n\n主体 $i$ 在时间 $t$ 的回报函数是：\n$$\n\\pi_{i,t} = (1 - \\tau)\\, y_{i,t} + \\tau \\, Y_t \\,\\frac{r_{i,t}}{R_t}\n$$\n我们将逐项对此函数关于 $x_{i,t}$ 求导。我们有以下关系及其关于 $x_{i,t}$ 的导数：\n-   $y_{i,t} = A x_{i,t}^{\\alpha} \\implies \\frac{\\partial y_{i,t}}{\\partial x_{i,t}} = A \\alpha x_{i,t}^{\\alpha-1}$\n-   $r_{i,t} = 1 - x_{i,t} \\implies \\frac{\\partial r_{i,t}}{\\partial x_{i,t}} = -1$\n-   $Y_t = y_{i,t} + \\sum_{j \\neq i} y_{j,t} \\implies \\frac{\\partial Y_t}{\\partial x_{i,t}} = \\frac{\\partial y_{i,t}}{\\partial x_{i,t}} = A \\alpha x_{i,t}^{\\alpha-1}$\n-   $R_t = r_{i,t} + \\sum_{j \\neq i} r_{j,t} \\implies \\frac{\\partial R_t}{\\partial x_{i,t}} = \\frac{\\partial r_{i,t}}{\\partial x_{i,t}} = -1$\n\n回报函数第一项的导数是直接的：\n$$\n\\frac{\\partial}{\\partial x_{i,t}} \\left [ (1 - \\tau) y_{i,t} \\right ] = (1 - \\tau) \\frac{\\partial y_{i,t}}{\\partial x_{i,t}} = (1 - \\tau) A \\alpha x_{i,t}^{\\alpha-1}\n$$\n\n对于第二项，我们对 $\\tau \\left( Y_t \\cdot \\frac{r_{i,t}}{R_t} \\right)$ 使用乘积法则求导：\n$$\n\\frac{\\partial}{\\partial x_{i,t}} \\left( Y_t \\frac{r_{i,t}}{R_t} \\right) = \\left( \\frac{\\partial Y_t}{\\partial x_{i,t}} \\right) \\left( \\frac{r_{i,t}}{R_t} \\right) + Y_t \\left( \\frac{\\partial}{\\partial x_{i,t}} \\frac{r_{i,t}}{R_t} \\right)\n$$\n我们需要份额项 $\\frac{r_{i,t}}{R_t}$ 的导数，为此我们使用商法则：\n$$\n\\frac{\\partial}{\\partial x_{i,t}} \\left( \\frac{r_{i,t}}{R_t} \\right) = \\frac{(\\frac{\\partial r_{i,t}}{\\partial x_{i,t}})R_t - r_{i,t}(\\frac{\\partial R_t}{\\partial x_{i,t}})}{R_t^2} = \\frac{(-1)R_t - r_{i,t}(-1)}{R_t^2} = \\frac{-R_t + r_{i,t}}{R_t^2} = -\\frac{R_t - r_{i,t}}{R_t^2}\n$$\n令 $R_{t,-i} = R_t - r_{i,t} = \\sum_{j \\neq i} r_{j,t}$ 为所有其他主体的总寻租努力。则份额项的导数为 $-\\frac{R_{t,-i}}{R_t^2}$。\n\n将此代回乘积法则表达式中：\n$$\n\\frac{\\partial}{\\partial x_{i,t}} \\left( Y_t \\frac{r_{i,t}}{R_t} \\right) = \\left( A \\alpha x_{i,t}^{\\alpha-1} \\right) \\left( \\frac{r_{i,t}}{R_t} \\right) + Y_t \\left( -\\frac{R_{t,-i}}{R_t^2} \\right)\n$$\n\n结合所有部分，回报函数的完整导数为：\n$$\n\\frac{\\partial \\pi_{i,t}}{\\partial x_{i,t}} = (1 - \\tau) A \\alpha x_{i,t}^{\\alpha-1} + \\tau \\left[ A \\alpha x_{i,t}^{\\alpha-1} \\frac{r_{i,t}}{R_t} - \\frac{Y_t R_{t,-i}}{R_t^2} \\right]\n$$\n该表达式提供了主体 $i$ 将单位努力从寻租转向生产的边际回报。\n\n### 实现计划\n\n模拟将使用 Python 语言和 `numpy` 库实现，以进行高效的向量计算，这对于处理 $N=200$ 个主体在多个时间步长上的情况至关重要。\n\n1.  **初始化**：对于每个测试用例，设置参数 $(N, A, \\alpha, \\tau, \\eta, T, x_0, \\varepsilon, \\delta_x)$。初始化一个大小为 $N$ 的 `numpy` 数组 `x`，所有元素设置为 $x_0$。\n\n2.  **模拟循环**：模拟进行 $T$ 个时间步。在每个步骤 $t$ 中：\n    a.  计算当前周期值：寻租努力向量 `r` 计算为 $1-x$。总寻租努力 `R_t` 和总产出 `Y_t` 通过对相应的主体级向量求和来计算。\n    b.  应用正则化：根据问题陈述，计算正则化分母 $R_{t,\\text{reg}} = \\max(R_t, \\varepsilon)$。对于导数计算，选择变量被正则化为 $x_{\\text{reg}} = \\max(x, \\delta_x)$，以防止在 $x=0$ 时 $x^{\\alpha-1}$ 出现未定义的值。\n    c.  计算梯度：使用向量化操作实现 $\\frac{\\partial \\pi_{i,t}}{\\partial x_{i,t}}$ 的推导公式。所有项，包括边际生产率 $A \\alpha x_{\\text{reg}}^{\\alpha-1}$ 和外部性项 $R_{t,-i} = R_t - r_i$，都作为 `numpy` 数组计算，从而得到一个梯度向量 `grad_pi`。\n    d.  更新状态：根据更新规则计算生产分配的变化量 $\\Delta x_t$：$\\Delta x_t = \\eta \\cdot \\text{grad\\_pi} \\cdot x \\cdot (1-x)$。更新状态向量：$x_{t+1} = x_t + \\Delta x_t$。\n    e.  投影：使用 `numpy.clip(x, 0.0, 1.0)` 将新的状态向量 $x_{t+1}$ 投影到有效区间 $[0,1]$ 上，确保对所有主体都有 $x_{i,t+1} \\in [0,1]$。\n\n3.  **结果提取**：经过 $T$ 次迭代后，计算最终的群体平均生产努力 $\\frac{1}{N}\\sum_{i=1}^{N} x_{i,T}$。\n\n4.  **格式化与输出**：每个测试用例的结果格式化为小数点后 $6$ 位。收集所有结果并按指定格式打印：`[result1,result2,...]`。\n\n此设计直接实现了理论模型，并遵守所有指定的计算和格式要求。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the ACE simulation for all test cases and print results.\n    \"\"\"\n\n    # Test suite (N, A, alpha, tau, eta, T, x0, epsilon, delta_x)\n    test_cases = [\n        (200, 1.0, 0.5, 0.3, 0.05, 1000, 0.5, 1e-8, 1e-8),\n        (200, 1.0, 0.5, 0.0, 0.05, 500, 0.2, 1e-8, 1e-8),\n        (200, 1.0, 0.5, 0.9, 0.02, 1000, 0.8, 1e-8, 1e-8),\n        (200, 0.2, 0.5, 0.5, 0.05, 1000, 0.5, 1e-8, 1e-8),\n        (200, 1.0, 0.95, 0.4, 0.02, 1500, 0.5, 1e-8, 1e-8),\n    ]\n\n    results = []\n    for case in test_cases:\n        N, A, alpha, tau, eta, T, x0, epsilon, delta_x = case\n\n        # Initialize the state vector for productive allocation\n        x = np.full(N, x0, dtype=np.float64)\n\n        # Main simulation loop for T periods\n        for _ in range(T):\n            # 1. Calculate current period aggregate variables\n            r = 1.0 - x\n            R_t = np.sum(r)\n            \n            # Using x directly for production as per y = A*x^alpha\n            y = A * np.power(x, alpha)\n            Y_t = np.sum(y)\n\n            # 2. Implement regularization for derivative calculation\n            R_t_reg = np.maximum(R_t, epsilon)\n            x_reg = np.maximum(x, delta_x)\n\n            # 3. Calculate the payoff gradient for each agent (vectorized)\n            # Marginal productivity: d(y_i)/d(x_i)\n            marginal_y = A * alpha * np.power(x_reg, alpha - 1)\n            \n            # Rent-seeking effort of all other agents: R_t - r_i\n            R_t_minus_i = R_t - r\n\n            # Term 1: Marginal gain from private production\n            grad_term1 = (1.0 - tau) * marginal_y\n\n            # Term 2: Marginal effect through rent-seeking payoff\n            # Sub-term 2a: Effect of increased total production on the rent pool\n            grad_term2a = tau * marginal_y * (r / R_t_reg)\n            # Sub-term 2b: Effect of changing own rent-seeking share\n            grad_term2b = -tau * Y_t * R_t_minus_i / (R_t_reg**2)\n\n            grad_pi = grad_term1 + grad_term2a + grad_term2b\n\n            # 4. Update the state vector based on the learning rule\n            update_term = eta * grad_pi * x * (1.0 - x)\n            x = x + update_term\n\n            # 5. Project the state vector back onto [0, 1]\n            x = np.clip(x, 0.0, 1.0)\n        \n        # After T periods, calculate the average productive allocation\n        final_avg_x = np.mean(x)\n        results.append(f\"{final_avg_x:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "2370544"}, {"introduction": "本练习将带你进入一个经典的协调博弈场景——“El Farol Bar”问题，这是一个基于智能体的计算经济学擅长的领域。在这个场景中，智能体需要在不确定他人选择的情况下，决定是否去一个容量有限的酒吧。该练习在经典模型的基础上增加了联盟（coalitions）和通信机制，这使得模型更贴近复杂的现实社会系统[@problem_id:2370498]。智能体通过强化学习来更新其行为倾向，你将通过模拟探索社会结构和通信在解决协调问题中的作用，并练习分析模拟生成的时间序列数据，以理解涌现出的集体行为。", "problem": "要求您实现并分析一个 El Farol Bar 问题的主体-基模型，在该模型中，主体可以被组织成联盟，这些联盟能以给定的成功内部沟通概率来协调其行动。该模型必须在固定数量的离散时期内进行模拟，并具有明确定义的主体行为、联盟激活和基于收益的学习。所有随机性必须通过对伪随机数生成器进行显式播种来确保可复现。\n\n模型定义：\n- 存在 $N$ 个主体，索引为 $i \\in \\{0,1,\\dots,N-1\\}$，以及一个容量为 $C \\in \\mathbb{N}$ 的酒吧。时间以离散时期 $t=0,1,\\dots,T-1$ 进行。\n- 每个主体 $i$ 在每个时期选择一个行动 $a_i(t) \\in \\{0,1\\}$，其中 $a_i(t)=1$ 表示去酒吧，$a_i(t)=0$ 表示待在家里。令总参与人数为 $A(t)=\\sum_{i=0}^{N-1} a_i(t)$。\n- 主体被外生地划分成不相交的联盟 $g \\in \\mathcal{G}$，这些联盟构成了主体集合的一个完整划分。一个联盟 $g$ 的大小为 $|g| \\ge 1$。大小为 $|g|=1$ 的联盟是单元联盟。对于每个大小为 $|g| \\ge 2$ 的联盟 $g$，定义其配额为 $q_g = \\left\\lfloor C \\cdot \\frac{|g|}{N} \\right\\rfloor$。对于大小为 $|g|=1$ 的单元联盟，不执行任何协调，也不应用任何配额。\n- 每个主体 $i$ 维持两个吸引力（倾向），$A_i^{\\text{att}}(t)$ 代表去酒吧，$A_i^{\\text{stay}}(t)$ 代表待在家里，初始值为 $A_i^{\\text{att}}(0)=0$ 和 $A_i^{\\text{stay}}(0)=0$。定义参数 $\\rho \\in (0,1)$（新近度）和 $\\lambda > 0$（学习步长）。\n- 私人意图阶段：在每个时期 $t$ 的开始，每个主体基于吸引力形成一个私人意图 $b_i(t) \\in \\{0,1\\}$：如果 $A_i^{\\text{att}}(t) > A_i^{\\text{stay}}(t)$，则 $b_i(t)=1$；如果 $A_i^{\\text{att}}(t) < A_i^{\\text{stay}}(t)$，则 $b_i(t)=0$；如果 $A_i^{\\text{att}}(t)=A_i^{\\text{stay}}(t)$，则通过公平的伯努利抛硬币来选择 $b_i(t)$。平局和所有抛硬币必须使用指定的伪随机数生成器和种子以确保可复现性。\n- 联盟激活与协调：对于每个大小为 $|g|\\ge 2$ 的联盟 $g$，通过一个成功概率为 $p_{\\text{comm}} \\in [0,1]$ 的独立伯努利试验来确定 $g$ 在时期 $t$ 是否被激活。如果未被激活，所有成员将他们的私人意图作为最终行动。如果在时期 $t$ 被激活，联盟会观察其成员的私人意图向量 $\\{b_i(t): i \\in g\\}$，并应用以下向下调整规则来产生最终行动 $\\{a_i(t): i \\in g\\}$：\n  1. 令 $S_g(t)=\\sum_{i \\in g} b_i(t)$ 为意图去酒吧的成员数量。\n  2. 如果 $S_g(t) \\le q_g$，则对所有 $i \\in g$ 都有 $a_i(t)=b_i(t)$。\n  3. 如果 $S_g(t) > q_g$，则在子集 $\\{i \\in g: b_i(t)=1\\}$ 中，精确选择 $q_g$ 名成员去酒吧，选择那些具有最大 $\\Delta_i(t)=A_i^{\\text{att}}(t)-A_i^{\\text{stay}}(t)$ 值的成员；$\\Delta_i(t)$ 的平局通过优先选择较小的主体索引来打破。为选中的 $q_g$ 名成员设置 $a_i(t)=1$，为该子集中的其余成员设置 $a_i(t)=0$。对于 $b_i(t)=0$ 的主体，设置 $a_i(t)=0$。\n- 对于任何单元联盟 $g$（其大小 $|g|=1$），其唯一成员 $i$ 的最终行动是 $a_i(t)=b_i(t)$。\n- 收益与学习：在所有联盟决策解决后，总参与人数 $A(t)=\\sum_i a_i(t)$ 得以实现。每个主体的时期收益 $u_i(t)$ 为\n  $$u_i(t)=\\begin{cases}\n  1 & \\text{如果 } a_i(t)=1 \\text{ 且 } A(t) \\le C,\\\\\n  1 & \\text{如果 } a_i(t)=0 \\text{ 且 } A(t) > C,\\\\\n  0 & \\text{其他情况。}\n  \\end{cases}$$\n  吸引力更新如下：\n  $$A_i^{\\text{att}}(t+1)=\\begin{cases}\n  (1-\\rho)\\,A_i^{\\text{att}}(t) + \\lambda\\,u_i(t) & \\text{如果 } a_i(t)=1,\\\\\n  (1-\\rho)\\,A_i^{\\text{att}}(t) & \\text{如果 } a_i(t)=0,\n  \\end{cases}$$\n  $$A_i^{\\text{stay}}(t+1)=\\begin{cases}\n  (1-\\rho)\\,A_i^{\\text{stay}}(t) + \\lambda\\,u_i(t) & \\text{如果 } a_i(t)=0,\\\\\n  (1-\\rho)\\,A_i^{\\text{stay}}(t) & \\text{如果 } a_i(t)=1.\n  \\end{cases}$$\n\n度量定义：\n- 对于给定的时间范围 $T$ 和尾部窗口长度 $H$（其中 $1 \\le H \\le T$），定义在时期 $t \\in \\{T-H, T-H+1, \\dots, T-1\\}$ 上的尾随平均值：\n  1. 平均参与人数差距 $\\bar{G} = \\frac{1}{H}\\sum_{t=T-H}^{T-1} \\left( A(t) - C \\right)$。\n  2. 过度拥挤频率 $\\bar{O} = \\frac{1}{H}\\sum_{t=T-H}^{T-1} \\mathbf{1}\\{A(t) > C\\}$，以 $[0,1]$ 范围内的小数表示。\n  3. 平均每时期协调反转份额 $\\bar{F} = \\frac{1}{H}\\sum_{t=T-H}^{T-1} \\left( \\frac{1}{N} \\sum_{i=0}^{N-1} \\mathbf{1}\\{a_i(t) \\ne b_i(t)\\} \\right)$，即，因联盟协调导致最终行动与其私人意图不同的主体的平均比例。\n\n随机性与可复现性：\n- 所有随机性，包括意图决策中的平局打破和联盟激活，都必须由一个根据每个测试用例指定的种子进行播种的伪随机数生成器生成。\n- 在每个测试用例中，都应精确使用给定的种子作为随机数生成器的初始化种子。\n\n测试套件：\n实现该模拟，并为以下四个参数集中的每一个计算 $(\\bar{G},\\bar{O},\\bar{F})$。在每种情况下，按指定顺序报告在最后 $H$ 个时期计算出的结果。\n\n- 情况 1：$N=51$，$C=25$，$T=2000$，$H=500$，$\\rho=0.01$，$\\lambda=0.2$，联盟大小由一个包含 51 个均为 1 的条目的列表给出（每个主体都是一个单元联盟），$p_{\\text{comm}}=0.5$，种子 $=42$。\n- 情况 2：$N=51$，$C=25$，$T=2000$，$H=500$，$\\rho=0.01$，$\\lambda=0.2$，联盟大小由列表 $[3,3,3,3,3,3,3,30]$ 给出（按主体索引递增的顺序），$p_{\\text{comm}}=1.0$，种子 $=43$。\n- 情况 3：$N=51$，$C=25$，$T=2000$，$H=500$，$\\rho=0.01$，$\\lambda=0.2$，联盟大小由列表 $[3,3,3,3,3,3,3,30]$ 给出，$p_{\\text{comm}}=0.0$，种子 $=44$。\n- 情况 4：$N=51$，$C=25$，$T=2000$，$H=500$，$\\rho=0.01$，$\\lambda=0.2$，联盟大小由一个包含十个 5 和一个 1 的列表给出（即 $[5,5,5,5,5,5,5,5,5,5,1]$），$p_{\\text{comm}}=0.8$，种子 $=45$。\n\n最终输出格式：\n您的程序应生成单行输出，其中包含用方括号括起来的逗号分隔列表形式的结果。每个测试用例的结果本身必须是一个包含三个浮点数 $[\\bar{G},\\bar{O},\\bar{F}]$ 的列表，顺序与上述测试用例相同。例如，整体输出格式必须是\n$[[g_1,o_1,f_1],[g_2,o_2,f_2],[g_3,o_3,f_3],[g_4,o_4,f_4]]$，\n其中每个 $g_k$、$o_k$ 和 $f_k$ 都以十进制数报告。", "solution": "问题陈述经过了严格验证，被确定为是界定良好的、在基于主体的计算经济学领域内具有科学依据且内部一致的。所有参数、行为规则以及客观评估标准都得到了明确的规定。因此，我将着手提供一个完整的解决方案。\n\n该问题要求实现一个离散时间的基于主体的模拟模型。系统状态由 $N$ 个主体的吸引力定义，这些吸引力根据它们的行动和由此产生的收益在 $T$ 个时期内演变。解决方案的核心是一个精确执行模型动态的计算算法。设计分为三个主要阶段：初始化、主模拟循环和最终测量。\n\n首先，初始化阶段建立模拟环境。这包括设置 $N$ 个主体，并根据指定的联盟大小列表将它们划分为不相交的联盟 $\\mathcal{G}$。对于大小为 $|g| \\ge 2$ 的每个联盟 $g \\in \\mathcal{G}$，预先计算一个协调配额 $q_g = \\lfloor C \\cdot \\frac{|g|}{N} \\rfloor$，其中 $C$ 是酒吧容量。每个主体 $i$ 初始化时，其去酒吧和待在家里的吸引力均为零，$A_i^{\\text{att}}(0)=0$ 和 $A_i^{\\text{stay}}(0)=0$。一个伪随机数生成器根据每个测试用例的指定进行播种，以确保所有随机事件的可复现性。\n\n其次，主模拟循环从时间 $t=0$ 到 $t=T-1$ 进行迭代。每个时期 $t$ 包含一系列直接源自问题定义的步骤：\n\n1.  **私人意图形成**：每个主体 $i$ 形成一个私人意图 $b_i(t) \\in \\{0,1\\}$，决定是去酒吧（1）还是待在家里（0）。这个决定基于对其当前吸引力的比较。如果 $A_i^{\\text{att}}(t) > A_i^{\\text{stay}}(t)$，意图为 $b_i(t)=1$。如果 $A_i^{\\text{att}}(t) < A_i^{\\text{stay}}(t)$，意图为 $b_i(t)=0$。在平局的情况下，$A_i^{\\text{att}}(t) = A_i^{\\text{stay}}(t)$，意图 $b_i(t)$ 通过使用已播种的随机数生成器进行公平抛硬币来确定。\n\n2.  **联盟协调**：确定最终行动 $a_i(t)$。对于单元联盟（$|g|=1$）中的主体，最终行动总是其私人意图，即 $a_i(t) = b_i(t)$。对于每个非单元联盟 $g$（$|g| \\ge 2$），通过一个成功概率为 $p_{\\text{comm}}$ 的伯努利试验来决定该联盟是否激活以进行协调。如果未激活，所有成员 $i \\in g$ 设置 $a_i(t) = b_i(t)$。如果激活，则将意图总和 $S_g(t) = \\sum_{i \\in g} b_i(t)$ 与联盟的配额 $q_g$ 进行比较。如果 $S_g(t) \\le q_g$，则无需协调，$a_i(t)=b_i(t)$ 对所有 $i \\in g$ 成立。然而，如果 $S_g(t) > q_g$，则应用一个向下调整规则。在 $S_g(t)$ 名意图去酒吧的成员中，精确选择 $q_g$ 名去酒吧。选择标准基于首先根据吸引力差异 $\\Delta_i(t) = A_i^{\\text{att}}(t) - A_i^{\\text{stay}}(t)$ 的降序，然后根据主体索引 $i$ 的升序对这些成员进行排序以打破平局。这 $q_g$ 名主体的最终行动被设为 $a_i(t)=1$。其余 $S_g(t) - q_g$ 名意图去酒吧的成员被“反转”，其最终行动变为 $a_i(t)=0$。最初意图待在家里（$b_i(t)=0$）的主体维持其行动 $a_i(t)=0$。\n\n3.  **收益与学习**：一旦所有最终行动被确定，就计算总参与人数 $A(t) = \\sum_{i=0}^{N-1} a_i(t)$。每个主体 $i$ 在做出“正确”决策（去一个不拥挤的酒吧，或者因酒吧拥挤而待在家里）时获得收益 $u_i(t)=1$，否则收益为 $u_i(t)=0$。随后，使用指定的 Roth-Erev 学习规则更新吸引力。对于选择行动 $a_i(t) \\in \\{0, 1\\}$ 的主体 $i$，其所选行动的吸引力通过 $A_i^{\\text{chosen}}(t+1) = (1-\\rho)A_i^{\\text{chosen}}(t) + \\lambda u_i(t)$ 更新，而未选行动的吸引力通过 $A_i^{\\text{unchosen}}(t+1) = (1-\\rho)A_i^{\\text{unchosen}}(t)$ 更新。在这里，$\\rho$ 是新近度参数，$\\lambda$ 是学习步长。\n\n第三，模拟完成后，进行最终分析。所需的度量指标——平均参与人数差距 $\\bar{G}$、过度拥挤频率 $\\bar{O}$ 和平均协调反转份额 $\\bar{F}$——是通过对模拟最后 $H$ 个时期（从 $t=T-H$ 到 $t=T-1$）的各自时间序列求平均值来计算的。该实现使用 NumPy 库提供的向量化操作来提高涉及主体状态数组的计算效率。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef run_simulation(N, C, T, H, rho, lambda_param, p_comm, coalition_sizes, seed):\n    \"\"\"\n    Implements and simulates the described agent-based El Farol Bar model with coalitions.\n    \"\"\"\n    # Initialize the pseudo-random number generator for reproducibility.\n    rng = np.random.default_rng(seed)\n\n    # 1. Initialization\n    # Construct coalition structure: a list of lists of agent indices.\n    coalitions = []\n    # Store quotas for non-singleton coalitions, indexed by coalition index.\n    quotas = {}\n    agent_idx_counter = 0\n    for g_idx, size in enumerate(coalition_sizes):\n        members = list(range(agent_idx_counter, agent_idx_counter + size))\n        coalitions.append(members)\n        if size >= 2:\n            # Per the problem, quota is floor(C * |g| / N).\n            quotas[g_idx] = int(C * size / N)\n        agent_idx_counter += size\n\n    # Agent state variables initialized to 0.\n    attractions = np.zeros(N, dtype=float)\n    stays = np.zeros(N, dtype=float)\n\n    # Data structures for storing tail-end history for final metrics.\n    attendance_history = []\n    flips_history = []\n\n    # 2. Main Simulation Loop\n    for t in range(T):\n        # 2a. Private Intention Phase\n        deltas = attractions - stays\n        intentions = np.zeros(N, dtype=int)\n        intentions[deltas > 0] = 1\n        \n        # Resolve ties with a fair coin flip.\n        tie_indices = np.where(deltas == 0)[0]\n        if len(tie_indices) > 0:\n            intentions[tie_indices] = rng.integers(0, 2, size=len(tie_indices))\n        \n        # Final actions start as intentions; may be modified by coordination.\n        actions = intentions.copy()\n\n        # 2b. Coalition Coordination Phase\n        for g_idx, g_members in enumerate(coalitions):\n            # Singleton coalitions (|g|=1) do not coordinate.\n            if len(g_members) < 2:\n                continue\n\n            # Bernoulli trial for coalition activation.\n            is_active = rng.random() < p_comm\n            if not is_active:\n                continue\n\n            # Active coalition coordination logic.\n            member_intentions = intentions[g_members]\n            S_g = np.sum(member_intentions)\n            q_g = quotas[g_idx]\n\n            if S_g > q_g:\n                # Identify members who intended to attend.\n                intending_member_local_indices = np.where(member_intentions == 1)[0]\n                global_intending_indices = [g_members[i] for i in intending_member_local_indices]\n                \n                # Create a list of candidates to be sorted.\n                candidates = []\n                for agent_idx in global_intending_indices:\n                    candidates.append((deltas[agent_idx], agent_idx))\n                \n                # Sort: 1st key descending delta, 2nd key ascending agent index.\n                candidates.sort(key=lambda x: (-x[0], x[1]))\n                \n                # The agents beyond the quota are 'flipped' to stay home.\n                for _, agent_idx in candidates[q_g:]:\n                    actions[agent_idx] = 0\n\n        # 2c. Payoff and Learning\n        A_t = np.sum(actions)\n        \n        payoffs = np.zeros(N, dtype=float)\n        actions_mask = (actions == 1) # Boolean mask for agents who attended.\n        \n        if A_t <= C:  # Bar not crowded: attenders get payoff 1.\n            payoffs[actions_mask] = 1.0\n        else:  # Bar crowded: stayers get payoff 1.\n            payoffs[~actions_mask] = 1.0\n\n        # Update attractions for t+1 using vectorized operations.\n        # First, apply the recency discount to all.\n        attractions = (1 - rho) * attractions\n        stays = (1 - rho) * stays\n\n        # Then, add the reinforcement term for the chosen action.\n        attractions[actions_mask] += lambda_param * payoffs[actions_mask]\n        stays[~actions_mask] += lambda_param * payoffs[~actions_mask]\n\n        # 2d. Data Recording for Tail Averages\n        if t >= T - H:\n            attendance_history.append(A_t)\n            flips_history.append(np.sum(actions != intentions))\n\n    # 3. Final Metrics Calculation\n    attendance_history = np.array(attendance_history, dtype=float)\n    flips_history = np.array(flips_history, dtype=float)\n\n    mean_gap = np.mean(attendance_history - C)\n    overcrowding_freq = np.mean(attendance_history > C)\n    mean_flip_share = np.mean(flips_history / N)\n\n    return [mean_gap, overcrowding_freq, mean_flip_share]\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1: All agents are singletons.\n        {'N': 51, 'C': 25, 'T': 2000, 'H': 500, 'rho': 0.01, 'lambda_param': 0.2, \n         'coalition_sizes': [1] * 51, 'p_comm': 0.5, 'seed': 42},\n        # Case 2: Coalitions, full communication.\n        {'N': 51, 'C': 25, 'T': 2000, 'H': 500, 'rho': 0.01, 'lambda_param': 0.2,\n         'coalition_sizes': [3,3,3,3,3,3,3,30], 'p_comm': 1.0, 'seed': 43},\n        # Case 3: Coalitions, no communication.\n        {'N': 51, 'C': 25, 'T': 2000, 'H': 500, 'rho': 0.01, 'lambda_param': 0.2,\n         'coalition_sizes': [3,3,3,3,3,3,3,30], 'p_comm': 0.0, 'seed': 44},\n        # Case 4: Another coalition structure, partial communication.\n        {'N': 51, 'C': 25, 'T': 2000, 'H': 500, 'rho': 0.01, 'lambda_param': 0.2,\n         'coalition_sizes': [5]*10 + [1], 'p_comm': 0.8, 'seed': 45},\n    ]\n\n    results = []\n    for params in test_cases:\n        result = run_simulation(**params)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(str(r) for r in results)}]\")\n\nsolve()\n```", "id": "2370498"}]}