{"hands_on_practices": [{"introduction": "本练习将引导你分析一个最简洁的均值场博弈模型。通过一个单周期的金融市场实例，我们将探索当个体决策汇总后必须与初始假设的群体行为（即均值场）一致时，如何寻找均衡解。这个问题通过引入一个代表“市场崩溃”的非连续成本函数，直观地展示了均值场如何作为一个约束条件影响每个个体的最优策略，是理解均值场博弈核心思想的绝佳起点 [@problem_id:2409409]。", "problem": "考虑一个金融市场中的单周期平均场博弈 (MFG)，其中有连续统的同质代理人。每个代理人拥有初始财富 $x_0 \\in \\mathbb{R}$，并选择一个控制 $\\,\\alpha \\in [\\underline{\\alpha}, \\overline{\\alpha}] \\subset \\mathbb{R}\\,$。总体中的聚合平均控制为 $m \\in \\mathbb{R}$。代理人的下一期财富由以下确定性仿射法则给出\n$$\nx_1 \\;=\\; x_0 \\,+\\, \\alpha \\,+\\, \\gamma\\, m,\n$$\n其中 $\\gamma \\in \\mathbb{R}$ 是一个给定的耦合参数，并假设 $1+\\gamma>0$。运行成本是不连续的，定义为\n$$\nL(x_0,\\alpha;m)\\;=\\;\n\\begin{cases}\n\\dfrac{1}{2}\\,\\alpha^2, & \\text{if } x_1 \\;\\ge\\; x_{\\min}, \\\\\n+\\infty, & \\text{if } x_1 \\;<\\; x_{\\min},\n\\end{cases}\n$$\n其中 $x_{\\min}\\in\\mathbb{R}$ 是一个给定的财富阈值，用于模拟市场崩盘：财富低于 $x_{\\min}$ 将产生无限成本。一个平均场博弈均衡是一个控制 $\\,\\alpha^\\star\\,$，它在约束 $\\alpha \\in [\\underline{\\alpha}, \\overline{\\alpha}]$ 和 $x_1 \\ge x_{\\min}$ 下最小化 $L(x_0,\\alpha;m)$，同时满足一致性条件 $\\,m=\\alpha^\\star\\,$。\n\n对于每个参数集 $(x_0, x_{\\min}, \\gamma, \\underline{\\alpha}, \\overline{\\alpha})$，将均衡值定义为\n$$\nV^\\star(x_0, x_{\\min}, \\gamma, \\underline{\\alpha}, \\overline{\\alpha}) \\;=\\;\n\\inf\\left\\{\\tfrac{1}{2}\\,\\alpha^2 \\;:\\; \\alpha \\in [\\underline{\\alpha}, \\overline{\\alpha}],\\; x_0 + \\alpha + \\gamma\\,\\alpha \\;\\ge\\; x_{\\min}\\right\\},\n$$\n按照惯例，如果可行集为空，则 $V^\\star=+\\infty$。\n\n你的任务是编写一个程序，为以下每个测试用例计算 $V^\\star$，结果为一个实数，如果不可行则为 $+\\infty$：\n\n- 测试用例 A (正常路径): $(x_0, x_{\\min}, \\gamma, \\underline{\\alpha}, \\overline{\\alpha}) = (\\,10,\\,5,\\,0.5,\\,-2,\\,2\\,)$.\n- 测试用例 B (防崩盘约束起作用，可行): $(x_0, x_{\\min}, \\gamma, \\underline{\\alpha}, \\overline{\\alpha}) = (\\,4,\\,5,\\,1,\\,-1,\\,3\\,)$.\n- 测试用例 C (防崩盘约束导致不可行): $(x_0, x_{\\min}, \\gamma, \\underline{\\alpha}, \\overline{\\alpha}) = (\\,0,\\,5,\\,0,\\,-1,\\,1\\,)$.\n- 测试用例 D (控制边界迫使采取非零行动): $(x_0, x_{\\min}, \\gamma, \\underline{\\alpha}, \\overline{\\alpha}) = (\\,10,\\,5,\\,0.2,\\,0.1,\\,0.5\\,)$.\n- 测试用例 E (恰好在边界上): $(x_0, x_{\\min}, \\gamma, \\underline{\\alpha}, \\overline{\\alpha}) = (\\,2,\\,5,\\,0,\\,-10,\\,3\\,)$.\n\n最终输出格式：你的程序应生成单行输出，其中包含按 A, B, C, D, E 顺序排列的结果，形式为逗号分隔的列表并用方括号括起来（例如，$[v_A,v_B,v_C,v_D,v_E]$），其中每个 $v_\\cdot$ 是一个实数或表示为 IEEE 浮点无穷大的 $+\\infty$。", "solution": "所提出的问题是一个源于单周期平均场博弈的约束优化问题。该问题是良定义的、数学上一致的，并且基于平均场博弈的标准理论。因此，该问题是有效的，并将提供一个解决方案。\n\n目标是计算均衡值 $V^\\star$，其定义为：\n$$\nV^\\star(x_0, x_{\\min}, \\gamma, \\underline{\\alpha}, \\overline{\\alpha}) = \\inf\\left\\{\\tfrac{1}{2}\\,\\alpha^2 \\;:\\; \\alpha \\in [\\underline{\\alpha}, \\overline{\\alpha}],\\; x_0 + \\alpha + \\gamma\\,\\alpha \\;\\ge\\; x_{\\min}\\right\\}\n$$\n这是一个在控制 $\\alpha$ 的可行集上寻找成本函数 $J(\\alpha) = \\frac{1}{2}\\alpha^2$ 最小值的优化问题。可行集（我们将其记为 $\\mathcal{A}$）由两个约束条件决定：\n1. 控制边界：$\\alpha \\in [\\underline{\\alpha}, \\overline{\\alpha}]$。\n2. 防崩盘约束：$x_0 + (1+\\gamma)\\alpha \\ge x_{\\min}$。\n\n问题陈述提供了一个关键假设 $1+\\gamma > 0$。这使得我们可以直接重排防崩盘约束以分离出 $\\alpha$：\n$$\n(1+\\gamma)\\alpha \\ge x_{\\min} - x_0\n$$\n$$\n\\alpha \\ge \\frac{x_{\\min} - x_0}{1+\\gamma}\n$$\n我们将此财富约束对控制施加的临界阈值定义为 $\\alpha_{\\text{crash}}$：\n$$\n\\alpha_{\\text{crash}} = \\frac{x_{\\min} - x_0}{1+\\gamma}\n$$\n可行集 $\\mathcal{A}$ 是 $\\alpha$ 的两个约束集的交集：\n$$\n\\mathcal{A} = \\{\\alpha \\in \\mathbb{R} \\mid \\underline{\\alpha} \\le \\alpha \\le \\overline{\\alpha} \\quad \\text{and} \\quad \\alpha \\ge \\alpha_{\\text{crash}}\\}\n$$\n这可以表示为一个单一的紧凑区间。令 $\\alpha_L$ 为控制的有效下界，$\\alpha_U$ 为控制的有效上界：\n$$\n\\alpha_L = \\max(\\underline{\\alpha}, \\alpha_{\\text{crash}})\n$$\n$$\n\\alpha_U = \\overline{\\alpha}\n$$\n因此，可行集是区间 $\\mathcal{A} = [\\alpha_L, \\alpha_U]$。\n\n该问题是可行的，当且仅当此区间非空，这要求 $\\alpha_L \\le \\alpha_U$。如果 $\\alpha_L > \\alpha_U$，则可行集为空，根据问题的惯例，其值为 $V^\\star = +\\infty$。\n\n如果问题是可行的，我们必须找到在区间 $[\\alpha_L, \\alpha_U]$上最小化目标函数 $J(\\alpha) = \\frac{1}{2}\\alpha^2$ 的最优控制 $\\alpha^\\star$。函数 $J(\\alpha)$ 是凸函数，在 $\\alpha=0$ 处有全局最小值。可行区间 $[\\alpha_L, \\alpha_U]$ 内最接近 0 的 $\\alpha$ 值即为最优控制 $\\alpha^\\star$。这等价于将点 $0$ 投影到区间 $[\\alpha_L, \\alpha_U]$上。我们基于区间相对于 $0$ 的位置，分三种情况来分析此投影：\n\n1.  如果区间 $[\\alpha_L, \\alpha_U]$ 包含 $0$（即 $\\alpha_L \\le 0 \\le \\alpha_U$），那么无约束最小化子是可行的，最优控制为 $\\alpha^\\star = 0$。\n\n2.  如果区间是严格为正的（即 $0 < \\alpha_L \\le \\alpha_U$），函数 $J(\\alpha)$ 在该区间上单调递增。因此，在左端点达到最小值，$\\alpha^\\star = \\alpha_L$。\n\n3.  如果区间是严格为负的（即 $\\alpha_L \\le \\alpha_U < 0$），函数 $J(\\alpha)$ 在该区间上单调递减。因此，在右端点达到最小值，$\\alpha^\\star = \\alpha_U$。\n\n一旦确定了最优控制 $\\alpha^\\star$，均衡值即计算为 $V^\\star = \\frac{1}{2}(\\alpha^\\star)^2$。这样就完成了解析解。计算的算法如下：\n\n对于每个参数集 $(x_0, x_{\\min}, \\gamma, \\underline{\\alpha}, \\overline{\\alpha})$：\n1.  计算 $\\alpha_{\\text{crash}} = (x_{\\min} - x_0) / (1 + \\gamma)$。\n2.  确定有效可行区间的边界：$\\alpha_L = \\max(\\underline{\\alpha}, \\alpha_{\\text{crash}})$ 和 $\\alpha_U = \\overline{\\alpha}$。\n3.  如果 $\\alpha_L > \\alpha_U$，结果为 $+\\infty$。\n4.  否则，确定 $\\alpha^\\star$：\n    - 如果 $\\alpha_L > 0$，则 $\\alpha^\\star = \\alpha_L$。\n    - 如果 $\\alpha_U < 0$，则 $\\alpha^\\star = \\alpha_U$。\n    - 否则，$\\alpha^\\star = 0$。\n5.  计算最终值 $V^\\star = \\frac{1}{2}(\\alpha^\\star)^2$。\n\n现在将此步骤应用于具体的测试用例。\n- **测试用例 A**: $(10, 5, 0.5, -2, 2)$。$\\alpha_{\\text{crash}} = (5-10)/1.5 \\approx -3.33$。$\\alpha_L = \\max(-2, -3.33) = -2$。$\\alpha_U = 2$。可行区间为 $[-2, 2]$。由于 $0 \\in [-2, 2]$，因此 $\\alpha^\\star = 0$。$V^\\star = \\frac{1}{2}(0)^2 = 0$。\n- **测试用例 B**: $(4, 5, 1, -1, 3)$。$\\alpha_{\\text{crash}} = (5-4)/2 = 0.5$。$\\alpha_L = \\max(-1, 0.5) = 0.5$。$\\alpha_U = 3$。可行区间为 $[0.5, 3]$。由于 $0.5 > 0$，因此 $\\alpha^\\star = 0.5$。$V^\\star = \\frac{1}{2}(0.5)^2 = 0.125$。\n- **测试用例 C**: $(0, 5, 0, -1, 1)$。$\\alpha_{\\text{crash}} = (5-0)/1 = 5$。$\\alpha_L = \\max(-1, 5) = 5$。$\\alpha_U = 1$。由于 $5 > 1$，该问题不可行。$V^\\star = +\\infty$。\n- **测试用例 D**: $(10, 5, 0.2, 0.1, 0.5)$。$\\alpha_{\\text{crash}} = (5-10)/1.2 \\approx -4.17$。$\\alpha_L = \\max(0.1, -4.17) = 0.1$。$\\alpha_U = 0.5$。可行区间为 $[0.1, 0.5]$。由于 $0.1 > 0$，因此 $\\alpha^\\star = 0.1$。$V^\\star = \\frac{1}{2}(0.1)^2 = 0.005$。\n- **测试用例 E**: $(2, 5, 0, -10, 3)$。$\\alpha_{\\text{crash}} = (5-2)/1 = 3$。$\\alpha_L = \\max(-10, 3) = 3$。$\\alpha_U = 3$。可行集是单点集 $\\{3\\}$。$\\alpha^\\star = 3$。$V^\\star = \\frac{1}{2}(3)^2 = 4.5$。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the equilibrium value V* for a single-period Mean Field Game.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # Each case is a tuple: (x0, x_min, gamma, alpha_underline, alpha_overline)\n    test_cases = [\n        (10.0, 5.0, 0.5, -2.0, 2.0),   # Test Case A\n        (4.0, 5.0, 1.0, -1.0, 3.0),    # Test Case B\n        (0.0, 5.0, 0.0, -1.0, 1.0),    # Test Case C\n        (10.0, 5.0, 0.2, 0.1, 0.5),    # Test Case D\n        (2.0, 5.0, 0.0, -10.0, 3.0),   # Test Case E\n    ]\n\n    def calculate_equilibrium_value(params):\n        \"\"\"\n        Calculates V* for a single parameter set.\n        \n        The problem is to minimize (1/2)*alpha^2 subject to:\n        1. alpha_underline <= alpha <= alpha_overline\n        2. x0 + (1 + gamma)*alpha >= x_min\n        \n        This leads to a feasible interval [alpha_L, alpha_U] for alpha.\n        \"\"\"\n        x0, x_min, gamma, alpha_underline, alpha_overline = params\n        \n        # The problem statement guarantees 1 + gamma > 0.\n        # Calculate the crash-avoidance threshold for alpha.\n        # This is derived from x0 + (1+gamma)*alpha >= x_min.\n        alpha_crash = (x_min - x0) / (1.0 + gamma)\n        \n        # Determine the effective feasible interval for alpha.\n        # The lower bound is the maximum of the given lower bound and the crash-avoidance threshold.\n        alpha_L = max(alpha_underline, alpha_crash)\n        alpha_U = alpha_overline\n        \n        # Check for feasibility. If the lower bound exceeds the upper bound, the feasible set is empty.\n        if alpha_L > alpha_U:\n            return np.inf\n\n        # The objective is to minimize (1/2)*alpha^2, which is equivalent to minimizing |alpha|.\n        # We need to find the point in the interval [alpha_L, alpha_U] that is closest to 0.\n        \n        alpha_star = 0.0\n        # If the interval is entirely to the right of 0, the closest point is the left endpoint.\n        if alpha_L > 0:\n            alpha_star = alpha_L\n        # If the interval is entirely to the left of 0, the closest point is the right endpoint.\n        elif alpha_U < 0:\n            alpha_star = alpha_U\n        # Otherwise, the interval contains 0, which is the unconstrained minimizer.\n        else: # alpha_L <= 0 <= alpha_U\n            alpha_star = 0.0\n            \n        # The equilibrium value is (1/2) * (alpha_star)^2.\n        v_star = 0.5 * alpha_star**2\n        return v_star\n\n    results = []\n    for case in test_cases:\n        result = calculate_equilibrium_value(case)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    # map(str, ...) correctly handles floating point numbers and 'inf'.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2409409"}, {"introduction": "在掌握了单周期模型后，我们进入一个动态的多周期环境。这个练习介绍了一类在经济学和金融学中极为重要的模型——线性二次（LQ）均值场博弈。你将学习到一个看似复杂的群体博弈问题如何可以被巧妙地分解：通过求解一个倒向的Riccati方程来确定个体的最优反馈策略，再利用该策略正向推演群体分布的演化。这个以城市增长为背景的例子 [@problem_id:2409391] 是掌握这一核心分析工具的理想实践。", "problem": "考虑一个由 $i$ 索引的城市连续统，时间为离散的 $t \\in \\{0,1,\\dots,T\\}$。每个城市 $i$ 都有一个对数规模 $x_t^i \\in \\mathbb{R}$。令 $m_t \\in \\mathbb{R}$ 表示在时刻 $t$ 时 $\\{x_t^i\\}_{i}$ 的横截面均值。城市 $i$ 通过选择一个控制量 $u_t^i \\in \\mathbb{R}$ 来控制其增长，该控制量通过以下确定性运动定律影响其下一时期的对数规模\n$$\nx_{t+1}^i \\;=\\; (1-\\beta)\\,x_t^i \\;+\\; \\beta\\,m_t \\;+\\; u_t^i,\n$$\n其中 $\\beta \\in [0,1]$ 是一个给定参数。对数规模的初始横截面分布使得 $x_0^i$ 的均值为 $m_0 \\in \\mathbb{R}$，方差为 $s_0^2 \\ge 0$。对于给定的确定性平均场路径 $\\{m_t\\}_{t=0}^T$，每个城市 $i$ 最小化以下二次目标函数\n$$\nJ^i \\;=\\; \\sum_{t=0}^{T-1} \\left(\\tfrac{1}{2}\\,q\\,\\big(x_t^i - m_t\\big)^2 \\;+\\; \\tfrac{1}{2}\\,r\\,\\big(u_t^i\\big)^2\\right) \\;+\\; \\tfrac{1}{2}\\,q_T\\,\\big(x_T^i - m_T\\big)^2,\n$$\n其中 $q \\ge 0$，$r > 0$ 和 $q_T \\ge 0$ 是给定的常数。平均场均衡是一个序列 $\\{m_t\\}_{t=0}^T$ 和一个可测反馈定律 $u_t^i = \\pi_t(x_t^i,m_t)$，使得对于每个城市 $i$，在给定 $\\{m_t\\}_{t=0}^T$ 的情况下，控制量 $u_t^i$ 能最小化 $J^i$，并且当所有城市都采用最优策略且 $x_0^i$ 的均值为 $m_0$、方差为 $s_0^2$ 时，所引出的横截面均值对所有 $t \\in \\{0,1,\\dots,T\\}$ 都满足不动点条件 $m_t = \\mathbb{E}[x_t^i]$。\n\n您的任务是，对于下面测试套件中的每一组参数，计算表征上述模型下唯一平均场均衡的以下两个量：\n- 在均衡平均场路径下，由时刻 $t=0$ 时的线性关系 $u_0^i = -K_0\\,(x_0^i - m_0)$ 定义的最优初始反馈系数 $K_0 \\in \\mathbb{R}$。\n- 终端时刻对数规模的横截面方差 $\\mathrm{Var}(x_T^i)$。\n\n假设除了上述给定的确定性动态过程外，没有其他外生冲击。将报告的每个数字表示为四舍五入到六位小数的实数。所有角度都是无量纲的。不涉及物理单位。\n\n用于评估的参数值测试套件：\n- 情况 1：$T = 10$, $\\beta = 0.3$, $q = 1.0$, $r = 0.5$, $q_T = 1.0$, $m_0 = 2.0$, $s_0^2 = 1.0$。\n- 情况 2：$T = 5$, $\\beta = 0.0$, $q = 1.0$, $r = 1.0$, $q_T = 1.0$, $m_0 = 0.0$, $s_0^2 = 0.25$。\n- 情况 3：$T = 20$, $\\beta = 0.9$, $q = 0.5$, $r = 1.0$, $q_T = 0.5$, $m_0 = -1.0$, $s_0^2 = 2.0$。\n- 情况 4：$T = 8$, $\\beta = 0.2$, $q = 1.0$, $r = 0.01$, $q_T = 1.0$, $m_0 = 1.0$, $s_0^2 = 3.0$。\n- 情况 5：$T = 12$, $\\beta = 0.4$, $q = 0.01$, $r = 1.0$, $q_T = 0.01$, $m_0 = 0.5$, $s_0^2 = 1.5$。\n- 情况 6：$T = 7$, $\\beta = 0.5$, $q = 1.0$, $r = 1.0$, $q_T = 1.0$, $m_0 = 3.0$, $s_0^2 = 0.0$。\n\n最终输出格式：\n- 对于每种情况，输出一个双元素列表 $[K_0,\\mathrm{Var}(x_T^i)]$，其中两个条目都四舍五入到六位小数。\n- 将所有情况的结果汇总到单行中，形式为用方括号括起来的逗号分隔列表，例如 $[[a_1,b_1],[a_2,b_2],\\dots]$，不含任何附加文本。", "solution": "对提供的问题进行验证。\n\n按原文提取给定条件：\n- 由 $i$ 索引的城市连续统。\n- 离散时间 $t \\in \\{0, 1, \\dots, T\\}$。\n- 城市 $i$ 的对数规模：$x_t^i \\in \\mathbb{R}$。\n- 横截面平均对数规模：$m_t = \\mathbb{E}[x_t^i]$。\n- 城市 $i$ 的控制量：$u_t^i \\in \\mathbb{R}$。\n- 运动定律：$x_{t+1}^i = (1-\\beta)x_t^i + \\beta m_t + u_t^i$，其中 $\\beta \\in [0,1]$。\n- $x_0^i$ 的初始分布：均值为 $m_0 \\in \\mathbb{R}$，方差为 $s_0^2 \\ge 0$。\n- 城市 $i$ 的目标函数：$J^i = \\sum_{t=0}^{T-1} (\\frac{1}{2}q(x_t^i - m_t)^2 + \\frac{1}{2}r(u_t^i)^2) + \\frac{1}{2}q_T(x_T^i - m_T)^2$，其中 $q \\ge 0$, $r > 0$, $q_T \\ge 0$。\n- 平均场均衡定义：一条路径 $\\{m_t\\}_{t=0}^T$ 和一个反馈定律 $u_t^i = \\pi_t(x_t^i, m_t)$，使得对于给定的 $\\{m_t\\}$，每个城市 $i$ 的 $u_t^i$ 是最优的，并且一致性条件 $m_t = \\mathbb{E}[x_t^i]$ 对所有 $t$ 成立。\n- 任务：从 $u_0^i = -K_0(x_0^i - m_0)$ 计算初始反馈系数 $K_0$ 和终端方差 $\\mathrm{Var}(x_T^i)$。\n\n该问题是平均场博弈理论中一个定义明确的标准问题，具体来说是一个离散时间的线性二次模型。该问题具有科学依据，是适定的，并以客观的数学语言表述。求解所需的所有参数均已提供。因此，该问题被视为有效。\n\n求解过程如下。首先，我们刻画平均场均衡。单个城市 $i$ 在给定平均场路径 $\\{m_t\\}_{t=0}^T$ 的情况下，求解一个线性二次最优控制问题。均衡要求遵循其最优策略的城市的聚合行为能够再现给定的平均场路径。\n\n令 $\\tilde{x}_t^i = x_t^i - m_t$ 为城市 $i$ 的对数规模与均值的偏差。该偏差的运动定律为：\n$$\n\\tilde{x}_{t+1}^i = x_{t+1}^i - m_{t+1} = \\big((1-\\beta)x_t^i + \\beta m_t + u_t^i\\big) - m_{t+1}\n$$\n$$\n\\tilde{x}_{t+1}^i = (1-\\beta)(\\tilde{x}_t^i + m_t) + \\beta m_t + u_t^i - m_{t+1} = (1-\\beta)\\tilde{x}_t^i + u_t^i + m_t - m_{t+1}\n$$\n成本函数变为：\n$$\nJ^i = \\sum_{t=0}^{T-1} \\left(\\tfrac{1}{2}q(\\tilde{x}_t^i)^2 + \\tfrac{1}{2}r(u_t^i)^2\\right) + \\tfrac{1}{2}q_T(\\tilde{x}_T^i)^2\n$$\n这是一个标准的线性二次调节器问题，其中 $(m_t - m_{t+1})$ 充当外生扰动。值函数是二次的，$V_t(\\tilde{x}_t^i) = \\frac{1}{2}P_t(\\tilde{x}_t^i)^2 + \\dots$，其中与 $\\tilde{x}_t^i$ 无关的项被省略。时刻 $t$ 的 Hamilton-Jacobi-Bellman 方程是：\n$$\nV_t(\\tilde{x}_t^i) = \\min_{u_t^i} \\left\\{ \\tfrac{1}{2}q(\\tilde{x}_t^i)^2 + \\tfrac{1}{2}r(u_t^i)^2 + V_{t+1}(\\tilde{x}_{t+1}^i) \\right\\}\n$$\n对 $u_t^i$ 最小化括号内的项，得到一阶条件：\n$$\nr u_t^i + \\frac{\\partial V_{t+1}}{\\partial \\tilde{x}_{t+1}^i} \\frac{\\partial \\tilde{x}_{t+1}^i}{\\partial u_t^i} = r u_t^i + P_{t+1}\\tilde{x}_{t+1}^i(1) = r u_t^i + P_{t+1}((1-\\beta)\\tilde{x}_t^i + u_t^i + m_t - m_{t+1}) = 0\n$$\n求解最优控制 $u_t^i$：\n$$\n(r+P_{t+1})u_t^i = -P_{t+1}((1-\\beta)\\tilde{x}_t^i + m_t - m_{t+1})\n$$\n$$\nu_t^i = -\\frac{P_{t+1}}{r+P_{t+1}}((1-\\beta)\\tilde{x}_t^i + m_t - m_{t+1})\n$$\n平均场一致性条件是 $\\mathbb{E}[x_t^i] = m_t$，这意味着对所有 $t$ 都有 $\\mathbb{E}[\\tilde{x}_t^i] = 0$。对原始状态动态取期望，得到 $m_{t+1} = (1-\\beta)m_t + \\beta m_t + \\mathbb{E}[u_t^i] = m_t + \\mathbb{E}[u_t^i]$，这意味着 $\\mathbb{E}[u_t^i] = m_{t+1} - m_t$。\n对最优控制律取期望：\n$$\n\\mathbb{E}[u_t^i] = -\\frac{P_{t+1}}{r+P_{t+1}}((1-\\beta)\\mathbb{E}[\\tilde{x}_t^i] + m_t - m_{t+1}) = -\\frac{P_{t+1}}{r+P_{t+1}}(m_t - m_{t+1})\n$$\n令 $\\mathbb{E}[u_t^i]$ 的两个表达式相等：\n$$\nm_{t+1} - m_t = -\\frac{P_{t+1}}{r+P_{t+1}}(m_t - m_{t+1}) \\implies \\left(1 + \\frac{P_{t+1}}{r+P_{t+1}}\\right)(m_{t+1} - m_t) = 0\n$$\n由于 $r > 0$ 且 $P_{t+1} \\ge 0$（因为它代表成本），括号中的项是严格为正的。因此，对于所有 $t$ 必然有 $m_{t+1} - m_t = 0$。这证明均衡平均场路径必须是恒定的：对于所有 $t = 0, \\dots, T$，$m_t = m_0$。\n\n当 $m_t$ 为常数时，问题显著简化。偏差的动态变为 $\\tilde{x}_{t+1}^i = (1-\\beta)\\tilde{x}_t^i + u_t^i$。问题现在变为关于偏差状态 $\\tilde{x}_t^i$ 的标准确定性 LQR 问题。最优控制简化为线性反馈定律：\n$$\nu_t^i = -\\frac{P_{t+1}(1-\\beta)}{r+P_{t+1}}\\tilde{x}_t^i = -K_t \\tilde{x}_t^i\n$$\n其中 $K_t = \\frac{P_{t+1}(1-\\beta)}{r+P_{t+1}}$ 是反馈增益。系数 $P_t$ 由离散时间 Riccati 方程确定，从 $t=T-1$ 向后求解至 $t=0$：\n$$\nP_t = q + (1-\\beta)^2 P_{t+1} - \\frac{((1-\\beta)P_{t+1})^2}{r+P_{t+1}} = q + \\frac{r P_{t+1}(1-\\beta)^2}{r+P_{t+1}}\n$$\n递归从终端成本导出的终端条件 $P_T = q_T$ 开始。然后，感兴趣的量 $K_0$ 计算为 $K_0 = \\frac{P_1(1-\\beta)}{r+P_1}$，这需要计算序列 $\\{P_t\\}_{t=1}^T$。\n\n接下来，我们推导横截面方差 $s_t^2 = \\mathrm{Var}(x_t^i) = \\mathrm{Var}(\\tilde{x}_t^i)$ 的演化。将最优控制代入偏差的动态方程：\n$$\n\\tilde{x}_{t+1}^i = (1-\\beta)\\tilde{x}_t^i - K_t \\tilde{x}_t^i = \\left((1-\\beta) - \\frac{P_{t+1}(1-\\beta)}{r+P_{t+1}}\\right)\\tilde{x}_t^i\n$$\n$$\n\\tilde{x}_{t+1}^i = (1-\\beta)\\left(1 - \\frac{P_{t+1}}{r+P_{t+1}}\\right)\\tilde{x}_t^i = \\left(\\frac{r(1-\\beta)}{r+P_{t+1}}\\right)\\tilde{x}_t^i\n$$\n方差根据前向递归演化。令 $v_t = s_t^2$：\n$$\nv_{t+1} = \\mathrm{Var}(\\tilde{x}_{t+1}^i) = \\mathbb{E}\\left[\\left(\\frac{r(1-\\beta)}{r+P_{t+1}}\\tilde{x}_t^i\\right)^2\\right] = \\left(\\frac{r(1-\\beta)}{r+P_{t+1}}\\right)^2 \\mathbb{E}[(\\tilde{x}_t^i)^2] = \\left(\\frac{r(1-\\beta)}{r+P_{t+1}}\\right)^2 v_t\n$$\n从给定的初始方差 $v_0 = s_0^2$ 开始，我们可以使用先前计算的序列 $\\{P_t\\}$，通过前向传递计算序列 $\\{v_t\\}_{t=1}^T$。终端方差是 $\\mathrm{Var}(x_T^i) = v_T$。\n\n算法如下：\n1. 初始化一个大小为 $T+1$ 的数组 $P$，并令 $P_T = q_T$。\n2. 从 $t = T-1$ 向下迭代到 $t=0$，通过求解 Riccati 方程来填充数组 $P$。\n3. 计算初始反馈增益 $K_0 = \\frac{P_1(1-\\beta)}{r+P_1}$。对于 $T=0$ 的情况，该值没有定义，但所有测试用例的 $T \\ge 1$。\n4. 初始化方差 $v_0 = s_0^2$。\n5. 使用存储的 $P_{t+1}$ 值，通过从 $t=0$ 到 $t=T-1$ 的前向迭代来计算方差的演化。\n6. 每种情况的最终结果是 $[K_0, v_T]$。\n\n下面为提供的测试用例实现了此过程。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves a discrete-time linear-quadratic mean-field game for a series of test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (T, beta, q, r, q_T, m_0, s0_sq)\n        (10, 0.3, 1.0, 0.5, 1.0, 2.0, 1.0),\n        (5, 0.0, 1.0, 1.0, 1.0, 0.0, 0.25),\n        (20, 0.9, 0.5, 1.0, 0.5, -1.0, 2.0),\n        (8, 0.2, 1.0, 0.01, 1.0, 1.0, 3.0),\n        (12, 0.4, 0.01, 1.0, 0.01, 0.5, 1.5),\n        (7, 0.5, 1.0, 1.0, 1.0, 3.0, 0.0),\n    ]\n\n    results = []\n    for case in test_cases:\n        T, beta, q, r, q_T, m_0, s0_sq = case\n        \n        # 1. Backward pass: Solve the Riccati equation for P_t\n        # P is an array of size T+1 for P_0, ..., P_T\n        P = np.zeros(T + 1)\n        P[T] = q_T\n        \n        # The equation for P_t depends on P_{t+1}. We iterate t from T-1 down to 0.\n        for t in range(T - 1, -1, -1):\n            P_next = P[t + 1]\n            term = (r * P_next * (1 - beta)**2) / (r + P_next)\n            P[t] = q + term\n\n        # 2. Calculate the initial feedback coefficient K_0\n        # K_0 depends on P_1. If T=0, there is no P_1 and no control u_0.\n        # Problem statement implies T >= 1 for control to be meaningful.\n        if T > 0:\n            K0 = (P[1] * (1 - beta)) / (r + P[1])\n        else:\n            # If T=0, there is no control phase, so K_0 is not applicable.\n            # Based on the problem structure, K_0=0 is a reasonable assignment.\n            K0 = 0.0\n\n        # 3. Forward pass: Evolve the cross-sectional variance\n        var = s0_sq\n        \n        # The evolution of var_t to var_{t+1} depends on P_{t+1}\n        for t in range(T):\n            P_next = P[t + 1]\n            factor = (r * (1 - beta)) / (r + P_next)\n            var = (factor**2) * var\n            \n        var_T = var\n\n        # Append rounded results for the current case\n        results.append([round(K0, 6), round(var_T, 6)])\n\n    # Format the final output string as specified.\n    # e.g., [[val1, val2], [val3, val4], ...]\n    final_output = \"[\" + \",\".join([f\"[{res[0]:.6f},{res[1]:.6f}]\" for res in results]) + \"]\"\n    print(final_output)\n\nsolve()\n```", "id": "2409391"}, {"introduction": "许多现实世界的问题并不具备线性二次模型那样的整洁结构，因此需要更通用的数值方法。本练习将指导你实现求解均值场博弈的“主力”算法——不动点迭代法。你将通过在一个离散状态的交易模型中反复执行两个步骤（给定均值场求解个体最优控制，再根据个体行为更新均值场），直至系统收敛到均衡，从而获得解决复杂均值场博弈问题的实践计算能力 [@problem_id:2409414]。", "problem": "考虑一个有限期界为 $T \\in \\mathbb{N}$ 的离散时间交易平均场博弈 (MFG)。时间由 $t \\in \\{0,1,\\dots,T-1\\}$ 索引。在时间 $t$，每个代理人拥有一个库存状态 $i_t \\in \\{-1,0,1\\}$。在每个时间 $t$，代理人选择一个控制（行为）$a_t \\in \\{-1,0,1\\}$，分别解释为“卖出”（-1）、“持有”（0）或“买入”（+1），并受到库存约束，即下一时刻的库存为\n$$\ni_{t+1} = \\min\\{1,\\max\\{-1,\\, i_t + a_t\\}\\}.\n$$\n令 $p_t(i)$ 表示在时间 $t$ 时库存状态 $i \\in \\{-1,0,1\\}$ 上的群体分布，其中 $p_t(-1)+p_t(0)+p_t(1)=1$。在时间 $t$ 的群体平均行为是\n$$\nm_t \\equiv \\sum_{i \\in \\{-1,0,1\\}} p_t(i)\\, a_t(i),\n$$\n其中 $a_t(i)$ 是最优反馈控制为时间 $t$ 的状态 $i$ 所指定的行为。市场漂移信号由以下公式给出\n$$\n\\mu_t = \\beta\\, m_t + \\xi_t,\n$$\n其中 $\\beta \\in \\mathbb{R}$ 是一个耦合参数，$\\xi_t \\in \\mathbb{R}$ 是一个外生漂移输入。在时间 $t$，处于状态 $i_t$ 的代理人采取行为 $a_t$ 的单步收益为\n$$\nr_t(i_t,a_t;\\mu_t) = \\mu_t\\, i_{t+1} - \\lambda\\, i_{t+1}^2 - c\\, |a_t|,\n$$\n其中库存惩罚为 $\\lambda \\ge 0$，交易成本为 $c \\ge 0$，而 $i_{t+1}$ 是由 $(i_t,a_t)$ 产生的交易后库存。代理人的目标是在整个时间范围内最大化单步收益的折现和，折现因子为 $\\gamma \\in (0,1]$，在时间 $T$ 的终值为 $0$。也就是说，给定一个序列 $(\\mu_t)_{t=0}^{T-1}$，最优控制是任意一个能够最大化以下表达式的反馈行为序列 $a_t(i) \\in \\{-1,0,1\\}$\n$$\n\\mathbb{E}\\left[\\sum_{t=0}^{T-1} \\gamma^t\\, r_t(i_t,a_t;\\mu_t)\\right],\n$$\n并受库存动态和约束集的限制。如果在给定的状态-时间对 $(i,t)$ 上，多个行为获得相同的最大值，则通过选择 $a_t(i)=0$ 来打破平局；如果 $a_t(i)=0$ 不在最大化行为之列，则选择 $a_t(i)=1$；否则选择 $a_t(i)=-1$。\n\n一个 MFG 均衡是一个序列 $(m_t)_{t=0}^{T-1}$，使得当每个代理人在给定 $\\mu_t=\\beta m_t + \\xi_t$ 的序列 $(\\mu_t)_{t=0}^{T-1}$ 下进行最优控制时，在每个时间点所引出的平均行为都等于 $m_t$。初始群体分布 $p_0$ (在 $\\{-1,0,1\\}$ 上) 是给定的，并且群体运动定律由最优反馈控制通过确定性的库存转移引出。\n\n给定此模型，编写一个完整的程序，为下面测试套件中的每个参数集计算一个满足上述不动点定义的均衡平均行为序列 $(m_t)_{t=0}^{T-1}$。对于每个测试用例，输出四舍五入到 $6$ 位小数的序列 $(m_0,\\dots,m_{T-1})$。\n\n使用以下参数集测试套件，每个参数集由 $(T,\\beta,\\gamma,c,\\lambda,\\xi,p_0)$ 指定，其中 $\\xi=(\\xi_0,\\xi_1,\\xi_2)$ 且 $p_0=(p_0(-1),p_0(0),p_0(1))$:\n- 测试 $1$：$(T,\\beta,\\gamma,c,\\lambda,\\xi,p_0) = (\\,3,\\,0.2,\\,0.95,\\,0.02,\\,0.01,\\,(0.0,0.0,0.0),\\,(0.3,0.4,0.3)\\,)$。\n- 测试 $2$：$(T,\\beta,\\gamma,c,\\lambda,\\xi,p_0) = (\\,3,\\,0.0,\\,0.95,\\,0.02,\\,0.01,\\,(0.0,0.0,0.0),\\,(0.3,0.4,0.3)\\,)$。\n- 测试 $3$：$(T,\\beta,\\gamma,c,\\lambda,\\xi,p_0) = (\\,3,\\,0.3,\\,0.95,\\,1.0,\\,0.01,\\,(0.0,0.0,0.0),\\,(0.3,0.4,0.3)\\,)$。\n- 测试 $4$：$(T,\\beta,\\gamma,c,\\lambda,\\xi,p_0) = (\\,3,\\,0.2,\\,0.95,\\,0.02,\\,0.01,\\;(-0.08,-0.02,0.0),\\,(0.2,0.5,0.3)\\,)$。\n- 测试 $5$：$(T,\\beta,\\gamma,c,\\lambda,\\xi,p_0) = (\\,3,\\,0.02,\\,0.95,\\,0.02,\\,0.01,\\,(0.0,0.0,0.0),\\,(0.3,0.4,0.3)\\,)$。\n\n您的程序应生成单行输出，其中包含一个 Python 风格的列表嵌套列表形式的结果。每个内部列表对于相应的测试用例等于 $[m_0,m_1,m_2]$，并且所有数字都四舍五入到 $6$ 位小数。格式必须严格如下\n$$\n[[m_0^{(1)},m_1^{(1)},m_2^{(1)}],[m_0^{(2)},m_1^{(2)},m_2^{(2)}],\\dots,[m_0^{(5)},m_1^{(5)},m_2^{(5)}]]\n$$\n且逗号后无空格。", "solution": "所呈现的问题是一个定义明确的、离散时间、有限状态的最优交易平均场博弈 (MFG)。它在科学上基于成熟的 MFG 理论，在数学上是一致的，并为计算唯一解提供了所有必要的参数和条件。因此，该问题被认为是有效的。任务是找到 MFG 均衡，该均衡被定义为在平均行为轨迹空间上的一个映射的不动点。我们将基于不动点迭代法（在此背景下也称为虚拟博弈）构建一个数值解。\n\n均衡是平均行为序列 $(m_t)_{t=0}^{T-1}$，使得如果所有代理人都预期此序列并相应地优化其个体交易策略，则他们行为的聚合结果将重现完全相同的序列。我们定义一个映射 $\\mathcal{F}$，它以一个推测的平均行为序列 $m = (m_t)_{t=0}^{T-1}$ 为输入，并返回由代理人优化和群体动态产生的平均行为序列 $\\hat{m} = (\\hat{m}_t)_{t=0}^{T-1}$。均衡是该映射的一个不动点，即 $m = \\mathcal{F}(m)$。\n\n该算法通过以下步骤进行，并迭代直至收敛：\n\n1.  **解决代表性代理人的最优控制问题**：对于一个给定的平均行为序列 $(m_t)_{t=0}^{T-1}$，代表性代理人的市场漂移信号被确定为 $\\mu_t = \\beta m_t + \\xi_t$，适用于每个时间 $t \\in \\{0, \\dots, T-1\\}$。代理人的问题是选择一个行为序列 $(a_t)_{t=0}^{T-1}$ 以最大化总折现收益。这是一个标准的有限期界动态规划问题。令 $V_t(i)$ 为值函数，表示在时间 $t$ 从状态 $i \\in \\{-1, 0, 1\\}$ 开始可实现的最大收益。该问题通过使用贝尔曼方程进行反向归纳来求解。终值被指定为零，因此对于所有 $i$，$V_T(i) = 0$。对于 $t = T-1, \\dots, 0$，值函数计算如下：\n    $$ V_t(i) = \\max_{a \\in \\{-1, 0, 1\\}} \\left\\{ r_t(i, a; \\mu_t) + \\gamma V_{t+1}(i') \\right\\} $$\n    其中 $i' = \\min\\{1, \\max\\{-1, i+a\\}\\}$ 是后续的库存状态。单步收益由 $r_t(i, a; \\mu_t) = \\mu_t i' - \\lambda (i')^2 - c|a|$ 给出。\n    对于每个状态-时间对 $(i, t)$，我们首先计算每个可能行为 $a \\in \\{-1, 0, 1\\}$ 的“Q值”：\n    $$ Q_t(i, a) = \\mu_t i' - \\lambda (i')^2 - c|a| + \\gamma V_{t+1}(i') $$\n    然后从 $Q_t(i, a)$ 的最大化子集合中选择最优行为 $a_t(i)$。问题指定了一个严格的平局打破规则以确保最优行为的唯一性：首先，如果 $a=0$ 是最大化者，则选择它；如果不是，如果 $a=1$ 是最大化者，则选择它；否则，选择 $a=-1$。此过程为所有的 $i$ 和 $t$ 定义了一个唯一的最优策略（反馈控制）$a_t(i)$。\n\n2.  **模拟群体动态**：在确定了最优策略 $a_t(i)$ 后，我们模拟库存状态上群体分布的演化。从 $t=0$ 时的初始分布 $p_0$ 开始，我们随时间向前演化系统。在每个步骤 $t$，通过对当前群体分布 $p_t$ 上的最优行为进行平均，计算出新的平均行为 $\\hat{m}_t$：\n    $$ \\hat{m}_t = \\sum_{i \\in \\{-1, 0, 1\\}} p_t(i) a_t(i) $$\n    然后计算下一时间步的群体分布 $p_{t+1}$。由于状态转移是确定性的，处于状态 $i$ 的每个子群体 $p_t(i)$ 会完全移动到下一个状态 $i' = \\min\\{1, \\max\\{-1, i+a_t(i)\\}\\}$。因此，通过将所有转移到每个状态的群体的概率相加，可以找到新的分布：\n    $$ p_{t+1}(j) = \\sum_{i \\in \\{-1,0,1\\} \\text{ s.t. } \\min\\{1,\\max\\{-1,i+a_t(i)\\}\\}=j} p_t(i) $$\n    这个前向传播过程产生一个新的平均行为序列 $\\hat{m} = (\\hat{m}_t)_{t=0}^{T-1}$。\n\n3.  **迭代至收敛**：计算出的序列 $\\hat{m}$ 是映射 $\\mathcal{F}$ 的输出，因此我们设置 $m^{(k+1)} = \\hat{m}$，其中 $k$ 是迭代索引。然后我们重复此过程，将 $m^{(k+1)}$ 反馈回步骤1。迭代从一个初始猜测开始，例如 $m^{(0)} = (0, \\dots, 0)$。当平均行为序列收敛时，即连续迭代之间的最大绝对差值小于一个小的容差 $\\epsilon$ 时，过程终止：$\\|_m^{(k+1)} - m^{(k)}\\|_{\\infty} < \\epsilon$。得到的序列就是所求的 MFG 均衡。\n\n在实现上，库存状态 $\\{-1, 0, 1\\}$ 可以方便地映射到数组索引 $\\{0, 1, 2\\}$。然后将这整个过程应用于测试套件中提供的每个参数集。", "answer": "```python\nimport numpy as np\n\ndef solve_mfg_equilibrium(T, beta, gamma, c, lambda_val, xi, p0):\n    \"\"\"\n    Computes the Mean Field Game equilibrium for a discrete-time trading model.\n\n    Args:\n        T (int): Time horizon.\n        beta (float): Coupling parameter for market drift.\n        gamma (float): Discount factor.\n        c (float): Transaction cost.\n        lambda_val (float): Inventory holding penalty.\n        xi (tuple): Exogenous drift sequence.\n        p0 (tuple): Initial population distribution over states {-1, 0, 1}.\n\n    Returns:\n        list: The equilibrium mean-action sequence [m_0, m_1, ..., m_{T-1}].\n    \"\"\"\n    states = np.array([-1, 0, 1])\n    actions = np.array([-1, 0, 1])\n\n    # Fixed-point iteration\n    m = np.zeros(T)\n    max_iter = 1000\n    tolerance = 1e-12\n\n    for _ in range(max_iter):\n        m_old = m.copy()\n\n        # 1. Calculate market drift given the mean-field m\n        mu = beta * m + np.array(xi)\n\n        # 2. Solve agent's optimal control problem (backward induction)\n        V = np.zeros(3)  # V_T(i) = 0 for states {-1, 0, 1}\n        policy = np.zeros((T, 3), dtype=int)  # policy[t, s] where s is state index\n\n        for t in range(T - 1, -1, -1):\n            V_next = V.copy()\n            V_current = np.zeros(3)\n            for s, i in enumerate(states):  # s is state index {0,1,2}, i is state value {-1,0,1}\n                q_values = np.zeros(3)\n                for a_idx, a in enumerate(actions):\n                    i_next = min(1, max(-1, i + a))\n                    s_next = i_next + 1\n                    \n                    reward = mu[t] * i_next - lambda_val * (i_next**2) - c * abs(a)\n                    q = reward + gamma * V_next[s_next]\n                    q_values[a_idx] = q\n\n                # Find optimal action with the specified tie-breaking rule\n                max_q = np.max(q_values)\n                optimal_actions_mask = np.isclose(q_values, max_q, atol=1e-12)\n                \n                if optimal_actions_mask[1]:  # Action a=0 is optimal\n                    best_action = 0\n                elif optimal_actions_mask[2]:  # Action a=1 is optimal\n                    best_action = 1\n                else:  # Action a=-1 is optimal\n                    best_action = -1\n                \n                policy[t, s] = best_action\n                # Index in q_values for action `a` is `a+1`\n                V_current[s] = q_values[best_action + 1]\n            V = V_current\n\n        # 3. Simulate population dynamics (forward pass)\n        p = np.array(p0)\n        m_new = np.zeros(T)\n        for t in range(T):\n            # Calculate mean action at time t\n            # policy[t, s] gives action for state_index s\n            actions_t = policy[t, :]\n            m_new[t] = np.dot(p, actions_t)\n\n            # Evolve population distribution\n            p_next = np.zeros(3)\n            for s, i in enumerate(states): # s is current state index\n                action = policy[t, s]\n                i_next = min(1, max(-1, i + action))\n                s_next = i_next + 1 # next state index\n                p_next[s_next] += p[s]\n            p = p_next\n\n        m = m_new\n\n        # 4. Check for convergence\n        if np.max(np.abs(m - m_old)) < tolerance:\n            break\n            \n    return m.tolist()\n\n\ndef solve():\n    \"\"\"\n    Main function to run test cases and print results in the required format.\n    \"\"\"\n    test_cases = [\n        # (T, beta, gamma, c, lambda_val, xi, p0)\n        (3, 0.2, 0.95, 0.02, 0.01, (0.0, 0.0, 0.0), (0.3, 0.4, 0.3)),\n        (3, 0.0, 0.95, 0.02, 0.01, (0.0, 0.0, 0.0), (0.3, 0.4, 0.3)),\n        (3, 0.3, 0.95, 1.0, 0.01, (0.0, 0.0, 0.0), (0.3, 0.4, 0.3)),\n        (3, 0.2, 0.95, 0.02, 0.01, (-0.08, -0.02, 0.0), (0.2, 0.5, 0.3)),\n        (3, 0.02, 0.95, 0.02, 0.01, (0.0, 0.0, 0.0), (0.3, 0.4, 0.3)),\n    ]\n    \n    results_str_list = []\n    for params in test_cases:\n        m_equilibrium = solve_mfg_equilibrium(*params)\n        m_rounded = [round(val, 6) for val in m_equilibrium]\n        \n        # Format list to string '[v1,v2,...]' without spaces\n        case_result_str = \"[\" + \",\".join(map(str, m_rounded)) + \"]\"\n        results_str_list.append(case_result_str)\n\n    # Format final output string '[[...],[...],...]'\n    final_output = \"[\" + \",\".join(results_str_list) + \"]\"\n    print(final_output)\n\nsolve()\n```", "id": "2409414"}]}