{"hands_on_practices": [{"introduction": "预测违约或其他不良后果的概率是信用风险管理的基础。在第一个练习中，我们将从零开始构建一个逻辑回归模型，用以预测一位金融顾问是否会面临监管处罚——这可视为其职业失败的一种代表。通过运用牛顿法实现优化并加入岭回归，你将深入实践，理解这些核心分类模型是如何构建和训练的，而不仅仅是调用现成的程序库。[@problem_id:2407536]", "problem": "您的任务是使用逻辑回归实现一个二元分类器，根据金融顾问的客户投诉历史和就业不稳定性，预测其在下一年是否会受到监管机构的制裁。目标是在计算经济学和金融学的背景下，从基本原理出发构建模型，使用针对二元结果的最大似然框架和用于正则化的惩罚概念。您的程序必须是一个完整的、可运行的脚本，使用所提供的数据和测试套件执行以下步骤，并以指定格式输出聚合结果。\n\n您必须从基本原理开始构建分类器：将二元结果建模为一个伯努利随机变量，其概率取决于由逻辑链接函数转换的线性指数，参数通过最大化观测数据的似然来估计。适用金融风险建模中常见的可解释性约束：包括一个截距项，仅使用训练集统计数据将特征标准化为零均值和单位方差，并对斜率系数使用各向同性 L2 范数惩罚（岭回归）以防止过拟合。截距项不得被惩罚。不得使用任何捷径公式或现成的模型拟合程序；推导参数的更新方程，并使用牛顿法实现一个数值稳定的求解器。\n\n数据描述：\n- 每位顾问的特征：\n  - $c$：客户投诉次数（计数），\n  - $r$：年均投诉次数（非负实数），\n  - $s$：投诉平均严重程度（在 $[0,1]$ 上归一化），\n  - $f$：过去 $5$ 年内更换公司的次数（计数），\n  - $t$：在当前公司的任职年限（非负实数）。\n- 目标：$y \\in \\{0,1\\}$，指示顾问在接下来的一年内是否受到制裁。\n\n训练数据集（每个项目为 $(c,r,s,f,t; y)$）：\n- $(0,\\, 0.0,\\, 0.1,\\, 0,\\, 15;\\, 0)$\n- $(1,\\, 0.1,\\, 0.2,\\, 0,\\, 12;\\, 0)$\n- $(2,\\, 0.2,\\, 0.3,\\, 1,\\, 10;\\, 0)$\n- $(0,\\, 0.0,\\, 0.2,\\, 1,\\, 18;\\, 0)$\n- $(1,\\, 0.1,\\, 0.1,\\, 0,\\, 20;\\, 0)$\n- $(2,\\, 0.2,\\, 0.15,\\, 1,\\, 16;\\, 0)$\n- $(1,\\, 0.0,\\, 0.25,\\, 0,\\, 14;\\, 0)$\n- $(0,\\, 0.05,\\, 0.05,\\, 0,\\, 22;\\, 0)$\n- $(3,\\, 0.5,\\, 0.4,\\, 1,\\, 9;\\, 0)$\n- $(5,\\, 0.8,\\, 0.7,\\, 2,\\, 5;\\, 1)$\n- $(7,\\, 1.2,\\, 0.9,\\, 3,\\, 3;\\, 1)$\n- $(4,\\, 0.6,\\, 0.6,\\, 2,\\, 6;\\, 1)$\n- $(8,\\, 1.5,\\, 0.85,\\, 4,\\, 2;\\, 1)$\n- $(6,\\, 1.0,\\, 0.8,\\, 3,\\, 4;\\, 1)$\n- $(9,\\, 1.8,\\, 0.95,\\, 5,\\, 1;\\, 1)$\n- $(3,\\, 0.4,\\, 0.55,\\, 2,\\, 8;\\, 1)$\n\n建模和训练要求：\n- 仅使用训练集将每个特征 $c$、$r$、$s$、$f$ 和 $t$ 标准化为零均值和单位方差。对于每个特征 $x$，标准化值的计算公式为 $(x - \\mu_x)/\\sigma_x$，其中 $\\mu_x$ 和 $\\sigma_x$ 是该特征的训练集均值和标准差。如果任何 $\\sigma_x$ 等于 $0$，则将其替换为 $1$ 以避免除以零。\n- 用全为 1 的截距列来增广标准化特征矩阵。\n- 使用牛顿法，通过最小化带惩罚的负对数似然来拟合逻辑回归参数，停止准则为参数更新的欧几里得范数小于 $10^{-8}$ 或达到 $50$ 次迭代，以先到者为准。\n- 使用强度为 $\\lambda \\ge 0$ 的岭惩罚，且仅应用于斜率系数（不包括截距）。\n\n分类规则：\n- 对于具有特征 $(c,r,s,f,t)$ 的给定顾问，计算预测的制裁概率，以 $[0,1]$ 区间内的小数表示。如果概率至少为 $0.5$，则分类为受制裁；否则分类为未受制裁。所有报告的概率必须是小数；不要使用百分号。\n\n测试套件：\n您的程序必须按规定多次训练模型，并返回以下四种情况的分类结果。每种情况提供一个惩罚水平 $\\lambda$ 和一个测试顾问的特征 $(c,r,s,f,t)$。对于每种情况，使用提供的 $\\lambda$ 在训练数据上拟合模型，然后使用 $0.5$ 的阈值对给定的测试顾问进行分类。\n\n- 情况 1：$\\lambda = 1.0$，$(c,r,s,f,t) = (6,\\, 1.0,\\, 0.8,\\, 3,\\, 3)$\n- 情况 2：$\\lambda = 0.0$，$(c,r,s,f,t) = (0,\\, 0.0,\\, 0.1,\\, 0,\\, 15)$\n- 情况 3：$\\lambda = 10.0$，$(c,r,s,f,t) = (10,\\, 1.5,\\, 0.9,\\, 5,\\, 2)$\n- 情况 4：$\\lambda = 0.5$，$(c,r,s,f,t) = (1,\\, 0.1,\\, 0.2,\\, 0,\\, 12)$\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含分类结果，格式为方括号括起来的逗号分隔的整数列表，按上述情况的顺序排列，例如 `\"[1,0,1,0]\"`。", "solution": "所述问题是有效的。它在科学上基于统计学习的原理，特别是带岭正则化的逻辑回归。其设定是适定的，提供了所有必要的数据、一个明确的目标函数以及一个指定的优化数值方法。这是计算金融学中一个客观且可形式化的问题。我们将从基本原理出发推导解决方案。\n\n设训练数据集为 $\\{(\\mathbf{x}_i, y_i)\\}_{i=1}^N$，其中 $\\mathbf{x}_i \\in \\mathbb{R}^D$ 是第 $i$ 位顾问的特征向量，而 $y_i \\in \\{0, 1\\}$ 是相应的二元结果（受制裁或未受制裁）。此处，$N=16$ 是训练观测值的数量，$D=5$ 是特征的数量。\n\n逻辑回归模型假定，正向结果（$y_i=1$）的概率由应用于特征线性组合的逻辑（sigmoid）函数给出：\n$$\nP(y_i=1 | \\mathbf{x}_i^*; \\boldsymbol{\\beta}) = p_i = \\sigma(\\mathbf{x}_i^{*T} \\boldsymbol{\\beta}) = \\frac{1}{1 + e^{-\\mathbf{x}_i^{*T} \\boldsymbol{\\beta}}}\n$$\n此处，$\\mathbf{x}_i^*$ 是第 $i$ 个特征向量，其前导增加了一个 $1$ 以容纳截距项，因此 $\\mathbf{x}_i^* \\in \\mathbb{R}^{D+1}$。向量 $\\boldsymbol{\\beta} \\in \\mathbb{R}^{D+1}$ 包含模型参数，其中 $\\beta_0$ 是截距，$\\beta_1, \\dots, \\beta_D$ 是斜率系数。\n\n参数 $\\boldsymbol{\\beta}$ 是通过最大化观测数据的对数似然来估计的，并附加一个正则化惩罚项。单个观测值的似然由伯努利概率质量函数给出：$L_i(\\boldsymbol{\\beta}) = p_i^{y_i} (1-p_i)^{1-y_i}$。$N$ 个独立观测值的总对数似然为：\n$$\n\\ell(\\boldsymbol{\\beta}) = \\sum_{i=1}^N \\ln(L_i(\\boldsymbol{\\beta})) = \\sum_{i=1}^N [y_i \\ln(p_i) + (1-y_i) \\ln(1-p_i)]\n$$\n代入 $p_i$ 的表达式，以及对于 $z_i=\\mathbf{x}_i^{*T} \\boldsymbol{\\beta}$，有 $\\ln(1-p_i) = \\ln(1-\\sigma(z_i)) = -z_i - \\ln(1+e^{z_i})$ 和 $\\ln(p_i) = \\ln(\\sigma(z_i)) = -\\ln(1+e^{-z_i}) = z_i - \\ln(1+e^{z_i})$，我们可以将对数似然简化为更方便的形式：\n$$\n\\ell(\\boldsymbol{\\beta}) = \\sum_{i=1}^N [y_i (\\mathbf{x}_i^{*T} \\boldsymbol{\\beta}) - \\ln(1 + e^{\\mathbf{x}_i^{*T} \\boldsymbol{\\beta}})]\n$$\n我们的目标是最小化带惩罚的*负*对数似然。惩罚项是斜率系数的 L2 范数平方（岭正则化），这会抑制过大的参数值，并有助于防止过拟合。截距 $\\beta_0$ 不被惩罚。要最小化的目标函数是：\n$$\nJ(\\boldsymbol{\\beta}) = -\\ell(\\boldsymbol{\\beta}) + \\frac{\\lambda}{2} \\sum_{j=1}^D \\beta_j^2 = \\sum_{i=1}^N [\\ln(1 + e^{\\mathbf{x}_i^{*T} \\boldsymbol{\\beta}}) - y_i (\\mathbf{x}_i^{*T} \\boldsymbol{\\beta})] + \\frac{\\lambda}{2} \\boldsymbol{\\beta}^T \\mathbf{I}' \\boldsymbol{\\beta}\n$$\n其中 $\\lambda \\ge 0$ 是正则化强度，$\\mathbf{I}'$ 是一个 $(D+1) \\times (D+1)$ 对角矩阵，其 $I'_{00} = 0$ 且 $I'_{jj} = 1$（对于 $j=1, \\dots, D$）。\n\n为了最小化 $J(\\boldsymbol{\\beta})$，我们使用牛顿法，这是一个具有以下更新规则的迭代算法：\n$$\n\\boldsymbol{\\beta}^{(t+1)} = \\boldsymbol{\\beta}^{(t)} - [\\mathbf{H}(\\boldsymbol{\\beta}^{(t)})]^{-1} \\nabla J(\\boldsymbol{\\beta}^{(t)})\n$$\n其中 $\\nabla J$ 是目标函数的梯度，$\\mathbf{H}$ 是其海森矩阵。\n\n首先，我们推导梯度 $\\nabla J(\\boldsymbol{\\beta})$。无惩罚部分梯度的第 $j$ 个分量是：\n$$\n\\frac{\\partial}{\\partial \\beta_j} \\left( \\sum_{i=1}^N [\\ln(1 + e^{\\mathbf{x}_i^{*T} \\boldsymbol{\\beta}}) - y_i (\\mathbf{x}_i^{*T} \\boldsymbol{\\beta})] \\right) = \\sum_{i=1}^N \\left[ \\frac{e^{\\mathbf{x}_i^{*T} \\boldsymbol{\\beta}}}{1 + e^{\\mathbf{x}_i^{*T} \\boldsymbol{\\beta}}} x_{ij}^* - y_i x_{ij}^* \\right] = \\sum_{i=1}^N (p_i - y_i) x_{ij}^*\n$$\n在矩阵表示法中，$\\mathbf{X}^*$ 是 $N \\times (D+1)$ 的增广设计矩阵，$\\mathbf{p}$ 是概率向量，$\\mathbf{y}$ 是结果向量，则梯度为 $\\mathbf{X}^{*T}(\\mathbf{p} - \\mathbf{y})$。\n惩罚项的梯度是 $\\lambda \\mathbf{I}' \\boldsymbol{\\beta}$。\n因此，完整的梯度是：\n$$\n\\nabla J(\\boldsymbol{\\beta}) = \\mathbf{X}^{*T}(\\mathbf{p} - \\mathbf{y}) + \\lambda \\mathbf{I}' \\boldsymbol{\\beta}\n$$\n接下来，我们推导海森矩阵 $\\mathbf{H}(\\boldsymbol{\\beta})$。元素 $H_{jk}$ 是 $\\frac{\\partial^2 J(\\boldsymbol{\\beta})}{\\partial \\beta_j \\partial \\beta_k}$。对于无惩罚部分：\n$$\n\\frac{\\partial^2}{\\partial \\beta_j \\partial \\beta_k} \\left( \\sum_{i=1}^N \\dots \\right) = \\frac{\\partial}{\\partial \\beta_k} \\left( \\sum_{i=1}^N p_i x_{ij}^* \\right) = \\sum_{i=1}^N x_{ij}^* \\frac{\\partial p_i}{\\partial \\beta_k}\n$$\n由于 $\\frac{\\partial p_i}{\\partial \\beta_k} = \\sigma(\\mathbf{x}_i^{*T} \\boldsymbol{\\beta})(1 - \\sigma(\\mathbf{x}_i^{*T} \\boldsymbol{\\beta})) x_{ik}^* = p_i(1-p_i)x_{ik}^*$，无惩罚部分的海森矩阵是：\n$$\nH_{jk} = \\sum_{i=1}^N x_{ij}^* p_i(1-p_i) x_{ik}^*\n$$\n以矩阵形式，这是 $\\mathbf{X}^{*T} \\mathbf{W} \\mathbf{X}^*$，其中 $\\mathbf{W}$ 是一个 $N \\times N$ 的对角矩阵，其对角线元素为 $W_{ii} = p_i(1-p_i)$。\n惩罚项的海森矩阵就是 $\\lambda \\mathbf{I}'$。\n完整的海森矩阵是：\n$$\n\\mathbf{H}(\\boldsymbol{\\beta}) = \\mathbf{X}^{*T} \\mathbf{W} \\mathbf{X}^* + \\lambda \\mathbf{I}'\n$$\n当 $\\lambda  0$ 时，该海森矩阵是正定的，确保了牛顿法存在唯一最小值和数值稳定性。\n\n实现将按以下步骤进行：\n1.  从训练数据中计算 $D=5$ 个特征中每个特征的均值 $\\boldsymbol{\\mu}$ 和标准差 $\\boldsymbol{\\sigma}$。如果任何 $\\sigma_j = 0$，则将其设为 $1$。\n2.  将训练数据矩阵 $\\mathbf{X}$ 标准化得到 $\\mathbf{X}_{std}$，其中每一列都具有零均值和单位方差。\n3.  用全为 1 的列增广 $\\mathbf{X}_{std}$ 以形成设计矩阵 $\\mathbf{X}^*$。\n4.  对每个测试用例，初始化参数向量 $\\boldsymbol{\\beta}$（例如，初始化为零）。\n5.  迭代应用牛顿法更新：\n    a. 计算概率 $\\mathbf{p} = \\sigma(\\mathbf{X}^* \\boldsymbol{\\beta}^{(t)})$ 和权重矩阵 $\\mathbf{W}^{(t)}$。\n    b. 计算梯度 $\\nabla J(\\boldsymbol{\\beta}^{(t)})$ 和海森矩阵 $\\mathbf{H}(\\boldsymbol{\\beta}^{(t)})$。\n    c. 求解线性系统 $\\mathbf{H}(\\boldsymbol{\\beta}^{(t)}) \\Delta \\boldsymbol{\\beta} = - \\nabla J(\\boldsymbol{\\beta}^{(t)})$ 以获得更新步长 $\\Delta \\boldsymbol{\\beta}$。\n    d. 更新参数：$\\boldsymbol{\\beta}^{(t+1)} = \\boldsymbol{\\beta}^{(t)} + \\Delta \\boldsymbol{\\beta}$。\n    e. 如果更新的欧几里得范数 $\\|\\Delta \\boldsymbol{\\beta}\\|_2$ 小于 $10^{-8}$ 或达到 $50$ 次迭代，则终止。\n6.  对于每个测试向量 $\\mathbf{x}_{\\text{test}}$，使用训练集的 $\\boldsymbol{\\mu}$ 和 $\\boldsymbol{\\sigma}$ 对其进行标准化，将其增广为 $\\mathbf{x}_{\\text{test}}^*$，计算预测概率 $p_{\\text{test}} = \\sigma(\\mathbf{x}_{\\text{test}}^{*T} \\boldsymbol{\\beta}_{\\text{final}})$，如果 $p_{\\text{test}} \\ge 0.5$ 则分类为 $1$，否则分类为 $0$。\n最终输出将是这些分类的列表。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and solves the logistic regression problem as specified.\n    \"\"\"\n    # Training dataset from the problem statement\n    # (c, r, s, f, t; y)\n    training_data = np.array([\n        [0.0, 0.0, 0.1, 0.0, 15.0, 0.0],\n        [1.0, 0.1, 0.2, 0.0, 12.0, 0.0],\n        [2.0, 0.2, 0.3, 1.0, 10.0, 0.0],\n        [0.0, 0.0, 0.2, 1.0, 18.0, 0.0],\n        [1.0, 0.1, 0.1, 0.0, 20.0, 0.0],\n        [2.0, 0.2, 0.15, 1.0, 16.0, 0.0],\n        [1.0, 0.0, 0.25, 0.0, 14.0, 0.0],\n        [0.0, 0.05, 0.05, 0.0, 22.0, 0.0],\n        [3.0, 0.5, 0.4, 1.0, 9.0, 0.0],\n        [5.0, 0.8, 0.7, 2.0, 5.0, 1.0],\n        [7.0, 1.2, 0.9, 3.0, 3.0, 1.0],\n        [4.0, 0.6, 0.6, 2.0, 6.0, 1.0],\n        [8.0, 1.5, 0.85, 4.0, 2.0, 1.0],\n        [6.0, 1.0, 0.8, 3.0, 4.0, 1.0],\n        [9.0, 1.8, 0.95, 5.0, 1.0, 1.0],\n        [3.0, 0.4, 0.55, 2.0, 8.0, 1.0],\n    ])\n\n    X_train_raw = training_data[:, :-1]\n    y_train = training_data[:, -1].reshape(-1, 1)\n\n    # Test cases from the problem statement\n    # Each is (lambda, (c, r, s, f, t))\n    test_cases = [\n        (1.0, (6.0, 1.0, 0.8, 3.0, 3.0)),\n        (0.0, (0.0, 0.0, 0.1, 0.0, 15.0)),\n        (10.0, (10.0, 1.5, 0.9, 5.0, 2.0)),\n        (0.5, (1.0, 0.1, 0.2, 0.0, 12.0)),\n    ]\n\n    # Step 1: Standardize features using training data statistics\n    mean_X = np.mean(X_train_raw, axis=0)\n    std_X = np.std(X_train_raw, axis=0)\n    # As per instructions, replace std=0 with 1 to avoid division by zero\n    std_X[std_X == 0] = 1.0\n    \n    X_train_std = (X_train_raw - mean_X) / std_X\n\n    # Step 2: Augment the standardized feature matrix with an intercept column\n    intercept_col = np.ones((X_train_std.shape[0], 1))\n    X_train_aug = np.hstack((intercept_col, X_train_std))\n\n    def numerically_stable_sigmoid(z):\n        \"\"\"Computes the sigmoid function in a numerically stable way.\"\"\"\n        # Using a vectorized implementation for efficiency\n        # This prevents overflow with large positive z and underflow with large negative z\n        pos_mask = (z = 0)\n        neg_mask = (z  0)\n        p = np.zeros_like(z, dtype=float)\n        p[pos_mask] = 1.0 / (1.0 + np.exp(-z[pos_mask]))\n        p[neg_mask] = np.exp(z[neg_mask]) / (1.0 + np.exp(z[neg_mask]))\n        return p\n\n    results = []\n    \n    num_features = X_train_aug.shape[1]\n\n    for lambda_val, test_features_raw in test_cases:\n        # Step 3: Fit the logistic regression model using Newton's method\n        \n        # Initialize parameters (beta) to zeros\n        beta = np.zeros((num_features, 1))\n        \n        # Regularization matrix I'\n        # Diagonal matrix with 0 for intercept and lambda for slopes\n        penalty_matrix = lambda_val * np.eye(num_features)\n        penalty_matrix[0, 0] = 0.0\n        \n        max_iter = 50\n        tolerance = 1e-8\n        \n        for i in range(max_iter):\n            # Calculate linear predictors and probabilities\n            z = X_train_aug @ beta\n            p = numerically_stable_sigmoid(z)\n            \n            # W is a diagonal matrix of weights p_i * (1 - p_i)\n            # Implemented with a 1D array for efficiency\n            weights = p * (1 - p)\n            W = np.diag(weights.flatten())\n            \n            # Calculate gradient of the penalized negative log-likelihood\n            # grad = X^T * (p - y) + lambda * I' * beta\n            gradient = X_train_aug.T @ (p - y_train) + penalty_matrix @ beta\n            \n            # Calculate Hessian of the penalized negative log-likelihood\n            # H = X^T * W * X + lambda * I'\n            hessian = X_train_aug.T @ W @ X_train_aug + penalty_matrix\n            \n            # Solve the linear system H * delta_beta = -gradient\n            # This is the Newton-Raphson update step\n            try:\n                # np.linalg.solve is more stable and efficient than inverting the matrix\n                delta_beta = np.linalg.solve(hessian, -gradient)\n            except np.linalg.LinAlgError:\n                # Use pseudo-inverse if Hessian is singular, for robustness\n                delta_beta = np.linalg.pinv(hessian) @ -gradient\n            \n            # Update parameters\n            beta += delta_beta\n            \n            # Check for convergence\n            if np.linalg.norm(delta_beta)  tolerance:\n                break\n\n        # Step 4: Classify the test advisor\n        x_test_std = (np.array(test_features_raw) - mean_X) / std_X\n        x_test_aug = np.hstack(([1.0], x_test_std))\n        \n        # Calculate predicted probability for the test case\n        prob_test = numerically_stable_sigmoid(x_test_aug @ beta).item()\n        \n        # Classify based on the 0.5 threshold\n        classification = 1 if prob_test = 0.5 else 0\n        results.append(classification)\n\n    # Final print statement in the exact required format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2407536"}, {"introduction": "信用风险不仅关乎孤立的违约事件，更关乎风险如何在相互关联的金融体系中传播。在理解了单个实体失败的基础上，这个练习将要求你模拟一个金融传染级联。你将对一个初始的外部违约事件如何通过公司网络传播、耗尽它们的股本并引发一连串后续失败进行建模，这个现象是理解和管理系统性风险的核心。[@problem_id:2385774]", "problem": "要求您形式化并实现一个贷款组合的压力测试程序，该程序需包含由外生交易对手违约所产生的二阶传染效应。该设计必须从核心信用风险定义出发，并遵循明确的逻辑规则进行。考虑一个包含 $N$ 个债务人的系统，其索引为 $i \\in \\{0,1,\\dots,N-1\\}$。相关基本要素如下：\n- 银行对这些债务人的违约风险暴露向量 $b \\in \\mathbb{R}_{\\ge 0}^{N}$，其中 $b_i$ 是银行对债务人 $i$ 的风险暴露。\n- 初始股本缓冲向量 $K^{(0)} \\in \\mathbb{R}_{ 0}^{N}$，其中 $K_i^{(0)}$ 是债务人 $i$ 在传染发生前的股本。\n- 债务人间风险暴露矩阵 $X \\in \\mathbb{R}_{\\ge 0}^{N \\times N}$，对所有 $i$ 满足 $X_{ii} = 0$，其中 $X_{i,j}$ 是债务人 $i$ 对债务人 $j$ 的风险暴露。\n- 银行贷款的违约损失率 $L^{\\text{bank}} \\in [0,1]$ 以及债务人之间债权的违约损失率 $L^{\\text{inter}} \\in [0,1]$。\n- 一个系统性损失乘数 $m \\ge 1$，用于在压力期间放大债务人之间的损失。\n- 一个初始违约集合 $D^{(0)} \\subseteq \\{0,1,\\dots,N-1\\}$，代表在时间 $t=0$ 时的外生违约。\n\n基本定义与规则：\n- 违约风险暴露 (EAD) 是指当交易对手违约时面临损失的未偿付风险暴露；此处，银行对债务人 $i$ 的 EAD 为 $b_i$，债务人 $i$ 对 $j$ 的债务人之间 EAD 为 $X_{i,j}$。\n- 违约损失率 (LGD) 是指违约时 EAD 未能收回的部分；此处，银行贷款的 LGD 为 $L^{\\text{bank}}$，债务人之间债权的 LGD 为 $L^{\\text{inter}}$。\n- 股本会因已实现的信用损失而减少。在每次传染迭代 $t \\in \\{1,2,\\dots\\}$ 中，通过扣除仅由在迭代 $t-1$ 时新增违约的交易对手所造成的损失来定义 $K^{(t)}$。对于尚未违约的债务人 $i$，\n$$\nK_i^{(t)} \\;=\\; K_i^{(t-1)} \\;-\\; \\sum_{j \\in \\Delta D^{(t-1)}} X_{i,j} \\cdot \\big(m \\cdot L^{\\text{inter}}\\big),\n$$\n其中 $\\Delta D^{(t-1)} = D^{(t-1)} \\setminus D^{(t-2)}$ 是在迭代 $t-1$ 时变为违约的债务人集合（并定义 $D^{(-1)} := \\emptyset$）。违约规则是：债务人 $i$ 在迭代 $t$ 时违约，当且仅当 $K_i^{(t)} \\le 0$。使用数值容差 $\\varepsilon = 10^{-12}$，并将 $K_i^{(t)} \\le \\varepsilon$ 视为违约，以避免浮点误差。\n- 当首次出现 $\\Delta D^{(T)} = \\emptyset$ 的迭代 $T$ 时，传染过程终止。最终违约集合为 $D^{(\\infty)} = D^{(T)}$。\n- 银行的组合损失为\n$$\n\\text{Loss} \\;=\\; \\sum_{i \\in D^{(\\infty)}} b_i \\cdot L^{\\text{bank}}.\n$$\n\n您的任务是实现一个程序，为下述每个测试用例计算：\n- 银行的总损失，以实数表示（结果以小数表示，而非百分比），以及\n- 违约债务人的总数（一个整数），包括外生初始违约和任何由传染引发的违约。\n\n所有输入中的索引均从零开始。不涉及物理单位。不使用角度。百分比必须表示为小数（例如，写成 $0.4$ 而不是 $40\\%$）。\n\n测试套件：\n- 案例 A（二阶传染，中度放大）：\n  - $N = 4$。\n  - $b = [10.0,\\, 5.0,\\, 8.0,\\, 6.0]$。\n  - $K^{(0)} = [2.0,\\, 2.0,\\, 1.0,\\, 1.1]$。\n  - $X$ 的非零项为 $X_{1,0} = 1.6$, $X_{2,0} = 1.8$, $X_{3,0} = 0.6$, $X_{1,2} = 0.9$, $X_{3,2} = 1.2$；所有其他项均为 $0$。\n  - $L^{\\text{bank}} = 0.45$, $L^{\\text{inter}} = 0.5$, $m = 1.2$。\n  - $D^{(0)} = \\{0\\}$。\n- 案例 B（无传染网络）：\n  - $N = 3$。\n  - $b = [5.0,\\, 5.0,\\, 5.0]$。\n  - $K^{(0)} = [1.0,\\, 1.0,\\, 1.0]$。\n  - $X$ 是 $3 \\times 3$ 的零矩阵。\n  - $L^{\\text{bank}} = 0.4$, $L^{\\text{inter}} = 0.5$, $m = 1.2$。\n  - $D^{(0)} = \\{1\\}$。\n- 案例 C（违约阈值的边界条件，完全级联）：\n  - $N = 3$。\n  - $b = [1.0,\\, 1.0,\\, 1.0]$。\n  - $K^{(0)} = [1.0,\\, 0.5,\\, 0.5]$。\n  - $X$ 的非零项为 $X_{1,0} = 1.0$, $X_{2,1} = 1.0$；所有其他项均为 $0$。\n  - $L^{\\text{bank}} = 0.5$, $L^{\\text{inter}} = 0.5$, $m = 1.0$。\n  - $D^{(0)} = \\{0\\}$。\n- 案例 D（放大效应在 $m=1$ 时不会发生的情况下引发了传染）：\n  - $N = 4$。\n  - $b = [2.0,\\, 2.0,\\, 2.0,\\, 2.0]$。\n  - $K^{(0)} = [1.2,\\, 0.9,\\, 0.15,\\, 0.5]$。\n  - $X$ 的非零项为 $X_{0,3} = 0.3$, $X_{1,3} = 0.3$, $X_{2,3} = 0.2$, $X_{0,2} = 0.7$, $X_{1,2} = 0.6$；所有其他项均为 $0$。\n  - $L^{\\text{bank}} = 0.5$, $L^{\\text{inter}} = 0.4$, $m = 2.0$。\n  - $D^{(0)} = \\{3\\}$。\n\n编程任务：\n- 精确地按照所述实现上述传染动态。\n- 对每个案例，返回一个列表 $[\\text{Loss},\\, \\text{Count}]$，其中 $\\text{Loss}$ 是一个浮点数，$\\text{Count}$ 是一个整数。您可以根据需要对中间计算进行四舍五入，但最终的 $\\text{Loss}$ 应使用标准浮点算术计算，并要求其绝对误差至少精确到 $10^{-6}$。\n- 最终输出格式：您的程序应生成单行输出，其中包含一个由逗号分隔并用方括号括起来的结果列表，每个案例的结果本身是一个双元素列表。例如，您的输出必须看起来像\n$[\\,[\\text{Loss}_A,\\text{Count}_A],[\\text{Loss}_B,\\text{Count}_B],[\\text{Loss}_C,\\text{Count}_C],[\\text{Loss}_D,\\text{Count}_D]\\,]$\n打印行中不含空格。", "solution": "对问题陈述进行验证。\n\n步骤1：提取已知信息。\n- 债务人数量为 $N$。\n- 银行的违约风险暴露向量为 $b \\in \\mathbb{R}_{\\ge 0}^{N}$。\n- 初始股本缓冲向量为 $K^{(0)} \\in \\mathbb{R}_{ 0}^{N}$。\n- 债务人间风险暴露矩阵为 $X \\in \\mathbb{R}_{\\ge 0}^{N \\times N}$，其中 $X_{i,j}$ 是债务人 $i$ 对债务人 $j$ 的风险暴露，且 $X_{ii} = 0$。\n- 银行的违约损失率为 $L^{\\text{bank}} \\in [0,1]$。\n- 债务人间的违约损失率为 $L^{\\text{inter}} \\in [0,1]$。\n- 系统性损失乘数为 $m \\ge 1$。\n- 外生违约的初始债务人集合为 $D^{(0)} \\subseteq \\{0,1,\\dots,N-1\\}$。\n- 违约的数值容差为 $\\varepsilon = 10^{-12}$。如果债务人 $i$ 的股本 $K_i^{(t)} \\le \\varepsilon$，则该债务人违约。\n- 传染动态由以下股本更新的迭代规则控制：\n$$K_i^{(t)} = K_i^{(t-1)} - \\sum_{j \\in \\Delta D^{(t-1)}} X_{i,j} \\cdot \\big(m \\cdot L^{\\text{inter}}\\big)$$\n其中 $\\Delta D^{(t-1)} = D^{(t-1)} \\setminus D^{(t-2)}$ 且 $D^{(-1)} := \\emptyset$。\n- 传染在第一次出现 $\\Delta D^{(T)} = \\emptyset$ 的迭代 $T$ 处终止。最终违约集合为 $D^{(\\infty)} = D^{(T)}$。\n- 银行的组合损失为 $\\text{Loss} = \\sum_{i \\in D^{(\\infty)}} b_i \\cdot L^{\\text{bank}}$。\n- 提供了四个具体的测试用例（A, B, C, D），包含了上述参数所有必要的数值。\n\n步骤2：使用提取的已知信息进行验证。\n- **科学依据**：该问题描述了一个金融传染的离散时间网络模型。风险暴露、股本、违约损失率和违约级联等概念是金融风险管理和计算经济学领域的标准概念。该模型虽然是简化的，但其基础是公认的原则，即资产（银行间风险暴露）损失会消耗公司的股本，导致其自身倒闭。因此，该问题具有科学合理性。\n- **适定性**：该过程是一个迭代算法。系统的状态是违約债务人的集合。该集合是单调不减的。由于债务人总数 $N$ 是有限的，传染过程必须在最多 $N$ 次迭代内终止。最终状态（所有违约债务人的集合）由初始条件和系统规则唯一确定。因此，对于总损失和违约数量，存在一个唯一的、稳定的解。该问题是适定的。\n- **客观性**：该问题使用精确的数学符号和明确的规则进行定义。所有参数均已给出，并且所需的输出也已明确规定。没有主观性语言。\n\n步骤3：结论与行动。\n该问题是有效的，因为它具有科学依据、适定性和客观性。将提供一个解决方案。\n\n传染过程被建模为一系列离散时间步，索引为 $t \\in \\{0, 1, 2, \\dots\\}$。我们必须跟踪每个债务人的状态，包括其股本资本和违约状态。\n\n迭代 $t$ 时的状态变量为：\n- $K^{(t)} \\in \\mathbb{R}^N$：所有债务人的股本向量。\n- $D^{(t)} \\subseteq \\{0, 1, \\dots, N-1\\}$：截至并包括迭代 $t$ 时所有已违约债务人的累积集合。\n- $\\Delta D^{(t)} = D^{(t)} \\setminus D^{(t-1)}$：在迭代 $t$ 时新增违约的债务人集合。\n\n算法流程如下：\n1.  **初始化 ($t=0$)**：\n    - 股本向量为初始股本 $K^{(0)}$。\n    - 累积违约集合 $D^{(-1)}$ 定义为空集 $\\emptyset$。\n    - 初始违约集合 $D^{(0)}$ 是外生给定的。\n    - 在 $t=0$ 时新增违约的债务人集合为 $\\Delta D^{(0)} = D^{(0)} \\setminus D^{(-1)} = D^{(0)}$。\n\n2.  **迭代传染 ($t=1, 2, \\dots, T$)**：模拟的核心是一个循环，只要有新一波违约发生，该循环就会继续。让我们将迭代 $t$ 开始时的状态定义为 $(K^{(t-1)}, D^{(t-1)})$，其中 $\\Delta D^{(t-1)}$ 是刚刚违约的公司集合。\n    - 如果 $\\Delta D^{(t-1)} = \\emptyset$，则传染已停止。过程终止。达到最终状态。\n    - 否则，我们必须计算对剩余有偿付能力债务人的影响。对于每个尚未违约的债务人 $i$（即 $i \\notin D^{(t-1)}$），我们计算其因 $\\Delta D^{(t-1)}$ 中的交易对手而遭受的损失：\n    $$\n    L_{i, t} = \\sum_{j \\in \\Delta D^{(t-1)}} X_{i,j} \\cdot \\big(m \\cdot L^{\\text{inter}}\\big)\n    $$\n    - 然后更新这些有偿付能力债务人的股本：\n    $$\n    K_i^{(t)} = K_i^{(t-1)} - L_{i, t}\n    $$\n    对于已经违约的债务人或在此步骤中股本未被更新的债务人，其股本值保持不变，即 $K_i^{(t)} = K_i^{(t-1)}$。\n    - 识别出本次迭代的新违约集合。一个先前有偿付能力的债务人 $i$（$i \\notin D^{(t-1)}$）如果其股本被耗尽，即 $K_i^{(t)} \\le \\varepsilon$，则现在违约。将此集合记为 $\\Delta D_{\\text{new}}^{(t)}$。\n    - 更新累积违约集合：$D^{(t)} = D^{(t-1)} \\cup \\Delta D_{\\text{new}}^{(t)}$。\n    - *下一次*迭代的新增违约债务人集合是 $\\Delta D^{(t)} = \\Delta D_{\\text{new}}^{(t)}$。然后该过程对 $t+1$ 重复。\n\n3.  **终止与最终计算**：当 $\\Delta D^{(T-1)} = \\emptyset$ 时，循环在迭代 $T$ 处终止。\n    - 所有违约债务人的最终集合是 $D^{(\\infty)} = D^{(T-1)}$。\n    - 违约总数是该集合的基数 $|D^{(\\infty)}|$。\n    - 银行的总损失根据其对最终违约集合中债务人的风险暴露计算得出：\n    $$\n    \\text{Loss} = \\sum_{i \\in D^{(\\infty)}} b_i \\cdot L^{\\text{bank}}\n    $$\n\n该算法为所提供的四个测试用例中的每一个实现。每个案例的参数用于初始化模拟，然后运行模拟直至完成，以找出最终损失和违约债务人的数量。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the simulation for all test cases and print results.\n    \"\"\"\n\n    # Numerical tolerance for default check\n    EPSILON = 1e-12\n\n    test_cases = [\n        # Case A: second-order contagion, moderate amplification\n        {\n            \"N\": 4,\n            \"b\": np.array([10.0, 5.0, 8.0, 6.0]),\n            \"K0\": np.array([2.0, 2.0, 1.0, 1.1]),\n            \"X_sparse\": {(1, 0): 1.6, (2, 0): 1.8, (3, 0): 0.6, (1, 2): 0.9, (3, 2): 1.2},\n            \"L_bank\": 0.45,\n            \"L_inter\": 0.5,\n            \"m\": 1.2,\n            \"D0\": {0},\n        },\n        # Case B: no contagion network\n        {\n            \"N\": 3,\n            \"b\": np.array([5.0, 5.0, 5.0]),\n            \"K0\": np.array([1.0, 1.0, 1.0]),\n            \"X_sparse\": {},\n            \"L_bank\": 0.4,\n            \"L_inter\": 0.5,\n            \"m\": 1.2,\n            \"D0\": {1},\n        },\n        # Case C: boundary condition at default threshold, full cascade\n        {\n            \"N\": 3,\n            \"b\": np.array([1.0, 1.0, 1.0]),\n            \"K0\": np.array([1.0, 0.5, 0.5]),\n            \"X_sparse\": {(1, 0): 1.0, (2, 1): 1.0},\n            \"L_bank\": 0.5,\n            \"L_inter\": 0.5,\n            \"m\": 1.0,\n            \"D0\": {0},\n        },\n        # Case D: amplification creates contagion that would not occur at m=1\n        {\n            \"N\": 4,\n            \"b\": np.array([2.0, 2.0, 2.0, 2.0]),\n            \"K0\": np.array([1.2, 0.9, 0.15, 0.5]),\n            \"X_sparse\": {(0, 3): 0.3, (1, 3): 0.3, (2, 3): 0.2, (0, 2): 0.7, (1, 2): 0.6},\n            \"L_bank\": 0.5,\n            \"L_inter\": 0.4,\n            \"m\": 2.0,\n            \"D0\": {3},\n        },\n    ]\n\n    def run_simulation(case):\n        \"\"\"\n        Runs the contagion simulation for a single test case.\n        \"\"\"\n        N = case[\"N\"]\n        b = case[\"b\"]\n        K = case[\"K0\"].copy()\n        X_sparse = case[\"X_sparse\"]\n        L_bank = case[\"L_bank\"]\n        L_inter = case[\"L_inter\"]\n        m = case[\"m\"]\n        D0 = case[\"D0\"]\n\n        # Construct the dense exposure matrix X\n        X = np.zeros((N, N))\n        for (i, j), val in X_sparse.items():\n            X[i, j] = val\n\n        # State variables for the simulation\n        D_final = set(D0)\n        newly_defaulted = set(D0)\n        \n        effective_lgd = m * L_inter\n\n        while newly_defaulted:\n            last_wave_defaults = list(newly_defaulted)\n            newly_defaulted = set()\n            \n            solvent_obligors = [i for i in range(N) if i not in D_final]\n            \n            if not solvent_obligors:\n                break\n                \n            # Calculate losses for solvent obligors from the last wave of defaults\n            losses = X[solvent_obligors, :][:, last_wave_defaults].sum(axis=1) * effective_lgd\n\n            # Update equity and check for new defaults\n            for idx, obligor_idx in enumerate(solvent_obligors):\n                K[obligor_idx] -= losses[idx]\n                if K[obligor_idx] = EPSILON:\n                    newly_defaulted.add(obligor_idx)\n\n            D_final.update(newly_defaulted)\n\n        # Calculate final results\n        final_loss = b[list(D_final)].sum() * L_bank\n        num_defaults = len(D_final)\n        \n        return [final_loss, num_defaults]\n\n    results = []\n    for case in test_cases:\n        result = run_simulation(case)\n        results.append(f\"[{result[0]},{result[1]}]\")\n\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "2385774"}, {"introduction": "随着机器学习模型成为信贷决策中不可或缺的一部分，理解其影响并提供可行的反馈变得至关重要。最后一个练习将探讨“算法纠正”这一前沿挑战：确定一个被拒的申请人需要对其个人资料做出哪些最小且最可行的改变才能获得批准。你将把这个问题构建为一个约束优化问题，应用凸优化的原理来寻找一条“通往批准的路径”，这是一个处于模型可解释性、公平性和计算金融交叉领域的重要概念。[@problem_id:2385810]", "problem": "给定一个使用机器学习 (ML) 训练的二元信用决策模型，该模型将一个特征向量 $\\mathbf{x} \\in \\mathbb{R}^{d}$ 映射为批准或拒绝。该模型是一个逻辑回归模型，其概率为 $p(\\mathbf{x}) = \\left(1 + \\exp\\left(-(\\mathbf{w}^{\\top}\\mathbf{x} + b)\\right)\\right)^{-1}$。决策规则是：如果 $p(\\mathbf{x}) \\geq 0.5$ 则批准，否则拒绝。这等价于当且仅当 $\\mathbf{w}^{\\top}\\mathbf{x} + b \\geq 0$ 时批准。考虑一个当前特征为 $\\mathbf{x}_{0}$ 且被拒绝的申请人。您需要计算出在可以和不可以更改的现实约束下，为达到批准状态所需的成本最小的对抗性修改（也称为补救措施）。\n\n从 $\\mathbf{x}_{0}$ 修改到 $\\mathbf{z}$ 的成本由一个加权二次范数衡量\n$$\n\\|\\mathbf{z} - \\mathbf{x}_{0}\\|_{\\mathbf{C}} \\equiv \\sqrt{(\\mathbf{z} - \\mathbf{x}_{0})^{\\top}\\mathbf{C}(\\mathbf{z} - \\mathbf{x}_{0})},\n$$\n其中 $\\mathbf{C}$ 是一个正定对角矩阵，表示改变每个特征的异构难度。由于监管或事实原因，某些特征是不可变的；不可变性由一个掩码 $\\mathbf{m} \\in \\{0,1\\}^{d}$ 概括，其中 $m_{i} = 0$ 意味着特征 $i$ 不能被改变，因此必须满足 $z_{i} = (x_{0})_{i}$。所有特征也必须保持在给定的上下界 $\\boldsymbol{\\ell} \\le \\mathbf{z} \\le \\mathbf{u}$ 之内，此不等式按分量解释。\n\n您的任务是编写一个程序，为每个指定的测试用例，从基本原理出发解决以下约束优化问题：\n- 最小化 $\\tfrac{1}{2}(\\mathbf{z} - \\mathbf{x}_{0})^{\\top}\\mathbf{C}(\\mathbf{z} - \\mathbf{x}_{0})$，\n- 约束条件为 $\\mathbf{w}^{\\top}\\mathbf{z} + b \\ge 0$，\n- 并且对于所有索引 $i$ 且 $m_{i} = 0$, $z_{i} = (x_{0})_{i}$，\n- 并且对于所有索引 $i$, $\\ell_{i} \\le z_{i} \\le u_{i}$。\n\n如果当前点 $\\mathbf{x}_{0}$ 已经满足 $\\mathbf{w}^{\\top}\\mathbf{x}_{0} + b \\ge 0$，则该案例的最小成本为 $0$，因为不需要任何改变。如果由于不可变性和箱形约束，无法获得批准（即，没有 $\\mathbf{z}$ 满足约束条件和 $\\mathbf{w}^{\\top}\\mathbf{z} + b \\ge 0$），则该案例输出表示为字符串 $inf$ 的 $+\\infty$。\n\n仅使用下面提供的模型和约束。所有数值数据都没有单位，必须作为纯数字处理。\n\n模型和度量参数（所有测试用例共享）：\n- 维度: $d = 4$。\n- 权重向量: $\\mathbf{w} = [\\,1.2,\\,-0.7,\\,0.3,\\,1.5\\,]$。\n- 截距: $b = -0.2$。\n- 成本矩阵: $\\mathbf{C} = \\mathrm{diag}([\\,1.0,\\,1.0,\\,4.0,\\,0.25\\,])$。\n- 下界: $\\boldsymbol{\\ell} = [\\,0.0,\\,0.0,\\,0.0,\\,0.0\\,]$。\n- 上界: $\\mathbf{u} = [\\,1.0,\\,1.0,\\,1.0,\\,1.0\\,]$。\n\n测试套件（每个案例提供 $\\mathbf{x}_{0}$ 和掩码 $\\mathbf{m}$）：\n- 案例 1 (一般可行情况): $\\mathbf{x}_{0} = [\\,0.2,\\,0.8,\\,0.1,\\,0.1\\,]$, $\\mathbf{m} = [\\,1,\\,1,\\,1,\\,1\\,]$。\n- 案例 2 (已获批准，零修改): $\\mathbf{x}_{0} = [\\,0.9,\\,0.1,\\,0.1,\\,0.1\\,]$, $\\mathbf{m} = [\\,1,\\,1,\\,1,\\,1\\,]$。\n- 案例 3 (不可变性增加难度但仍可行): $\\mathbf{x}_{0} = [\\,0.2,\\,0.8,\\,0.1,\\,0.1\\,]$, $\\mathbf{m} = [\\,1,\\,1,\\,1,\\,0\\,]$。\n- 案例 4 (在边界和不可变性约束下不可行): $\\mathbf{x}_{0} = [\\,0.0,\\,1.0,\\,0.0,\\,0.0\\,]$, $\\mathbf{m} = [\\,0,\\,0,\\,1,\\,0\\,]$。\n\n程序要求：\n- 对于每个案例，计算在满足约束条件下实现批准的最小成本 $\\|\\mathbf{z}^{\\star} - \\mathbf{x}_{0}\\|_{\\mathbf{C}}$，或确定其不可行。\n- 结果必须以浮点数形式返回，并四舍五入到 $6$ 位小数。如果不可行，则返回 $inf$。\n- 您的程序应生成单行输出，其中包含按案例 1 到 4 的顺序排列的结果，格式为方括号内的逗号分隔列表（例如，$\\texttt{[0.123456,0.000000,0.234567,inf]}$）。\n\n注意：\n- 如果出现任何角度或百分比，请将其表示为纯数字；此处未出现。\n- 您不得要求任何用户输入；请使用上述指定的参数。\n- 请基于逻辑回归分类和凸优化原理的基本定义来设计您的算法，并实现一个适用于带线性约束的凸二次规划问题的精确或数值求解器。", "solution": "问题陈述经审查后被认为是有效的。它提出了一个带线性约束的标准凸二次规划问题，这是算法补救措施（algorithmic recourse）领域一个适定且科学合理的问题。我们现在开始推导和实现解决方案。\n\n问题在于找到一个修改向量 $\\mathbf{z} \\in \\mathbb{R}^{d}$，为初始申请人特征向量 $\\mathbf{x}_{0}$ 找到满足一系列约束的最优解，同时最小化二次成本。该优化问题被表述为：\n$$\n\\begin{aligned}\n\\text{最小化} \\quad  f(\\mathbf{z}) = \\frac{1}{2}(\\mathbf{z} - \\mathbf{x}_{0})^{\\top}\\mathbf{C}(\\mathbf{z} - \\mathbf{x}_{0}) \\\\\n\\text{约束条件为} \\quad  \\mathbf{w}^{\\top}\\mathbf{z} + b \\ge 0 \\\\\n z_{i} = (x_0)_{i} \\quad \\forall i \\text{ 使得 } m_i = 0 \\\\\n \\boldsymbol{\\ell} \\le \\mathbf{z} \\le \\mathbf{u}\n\\end{aligned}\n$$\n目标函数 $f(\\mathbf{z})$ 是严格凸的，因为成本矩阵 $\\mathbf{C}$ 是正定的。约束都是线性的，定义了一个凸可行集。这种结构确保了如果解存在，它就是唯一的。最终要报告的成本是 $\\|\\mathbf{z}^{\\star} - \\mathbf{x}_{0}\\|_{\\mathbf{C}} = \\sqrt{2f(\\mathbf{z}^{\\star})}$。\n\n首先，我们处理初步情况。如果初始点 $\\mathbf{x}_{0}$ 已经满足批准约束 $\\mathbf{w}^{\\top}\\mathbf{x}_{0} + b \\ge 0$，则无需修改。最优解是 $\\mathbf{z}^{\\star} = \\mathbf{x}_{0}$，成本为 $0$。\n\n如果 $\\mathbf{w}^{\\top}\\mathbf{x}_{0} + b  0$，我们必须找到一个新的向量 $\\mathbf{z}$。在继续之前，我们必须检查可行性。一个解是可行的，当且仅当存在至少一个向量 $\\mathbf{z}$ 同时满足所有约束。为了检查这一点，我们找到分数函数 $\\mathbf{w}^{\\top}\\mathbf{z} + b$ 在不可变性和箱形约束下的最大可能值。设此最大分数为 $S_{max}$。该分数是通过为每个可变特征 $i$（其中 $m_i = 1$）选择 $z_i$ 来实现的：如果 $w_i  0$，则 $z_i$ 为 $u_i$；如果 $w_i  0$，则 $z_i$ 为 $\\ell_i$。如果 $S_{max}  0$，则没有任何可能的修改可以达到批准状态，问题是不可行的，导致成本为无穷大。\n\n如果问题可行且需要修改，我们解决完整的约束优化问题。我们使用拉格朗日乘子法。该问题的拉格朗日量为：\n$$\nL(\\mathbf{z}, \\lambda, \\boldsymbol{\\mu}_L, \\boldsymbol{\\mu}_U) = \\frac{1}{2}(\\mathbf{z} - \\mathbf{x}_{0})^{\\top}\\mathbf{C}(\\mathbf{z} - \\mathbf{x}_{0}) - \\lambda(\\mathbf{w}^{\\top}\\mathbf{z} + b) - \\boldsymbol{\\mu}_L^{\\top}(\\mathbf{z} - \\boldsymbol{\\ell}) - \\boldsymbol{\\mu}_U^{\\top}(\\mathbf{u} - \\mathbf{z})\n$$\n这个公式仅针对可变特征建立，因为不可变特征是固定的。设 $I_{mut} = \\{i \\mid m_i = 1\\}$。在最优点，拉格朗日量对于 $\\mathbf{z}$ 的可变分量的梯度必须为零：\n$$\n\\nabla_{\\mathbf{z}_{mut}} L = \\mathbf{C}_{mut}(\\mathbf{z}_{mut} - \\mathbf{x}_{0,mut}) - \\lambda \\mathbf{w}_{mut} - \\boldsymbol{\\mu}_{L,mut} + \\boldsymbol{\\mu}_{U,mut} = \\mathbf{0}\n$$\n这里，$\\lambda \\ge 0$, $\\boldsymbol{\\mu}_L \\ge 0$, 和 $\\boldsymbol{\\mu}_U \\ge 0$ 分别是对应于批准、下界和上界约束的拉格朗日乘子。由于 $\\mathbf{C}$ 是对角矩阵，我们可以求解出其中 $i \\in I_{mut}$ 的每个分量 $z_i$：\n$$\nz_i = (x_0)_i + \\frac{1}{C_{ii}}(\\lambda w_i + (\\mu_L)_i - (\\mu_U)_i)\n$$\nKarush-Kuhn-Tucker (KKT) 互补松弛条件指出，一个乘子非零，当且仅当其对应的不等式约束是活跃的（即等式成立）。对于箱形约束：\n- 如果 $\\ell_i  z_i  u_i$，则 $(\\mu_L)_i = 0$ 且 $(\\mu_U)_i = 0$，这给出 $z_i = (x_0)_i + \\lambda \\frac{w_i}{C_{ii}}$。\n- 如果 $z_i = \\ell_i$，则 $(\\mu_U)_i = 0$。\n- 如果 $z_i = u_i$，则 $(\\mu_L)_i = 0$。\n这使我们能够将最优的 $z_i$ 表示为仅关于 $\\lambda$ 的函数。$z_i$ 的解是无约束最优值在区间 $[\\ell_i, u_i]$ 上的投影：\n$$\nz_i^{\\star}(\\lambda) = \\mathrm{clip}\\left((x_0)_i + \\lambda \\frac{w_i}{C_{ii}}, \\ell_i, u_i\\right) = \\max\\left(\\ell_i, \\min\\left(u_i, (x_0)_i + \\lambda \\frac{w_i}{C_{ii}}\\right)\\right)\n$$\n对于不可变特征 $j \\notin I_{mut}$，我们有 $z_j^{\\star} = (x_0)_j$。\n\n由于初始点被拒绝，批准约束 $\\mathbf{w}^{\\top}\\mathbf{z} + b \\ge 0$ 在解处必须是活跃的，即 $\\mathbf{w}^{\\top}\\mathbf{z}^{\\star} + b = 0$。这给了我们一个找到最优 $\\lambda^{\\star}$ 的方程：\n$$\ng(\\lambda) = \\mathbf{w}^{\\top}\\mathbf{z}^{\\star}(\\lambda) + b = \\sum_{i=0}^{d-1} w_i z_i^{\\star}(\\lambda) + b = 0\n$$\n函数 $g(\\lambda)$ 对于 $\\lambda \\ge 0$ 是连续且单调非递减的。我们知道 $g(0) = \\mathbf{w}^{\\top}\\mathbf{x}_{0} + b  0$，且对于一个可行问题，存在某个大的 $\\lambda$ 使得 $g(\\lambda) \\ge 0$。因此，存在一个唯一的根 $\\lambda^{\\star} \\ge 0$，并且可以使用数值求根算法（如二分法）高效地找到它。\n\n完整的算法如下：\n1. 对于一个给定的测试用例 $(\\mathbf{x}_{0}, \\mathbf{m})$，计算初始分数 $S_0 = \\mathbf{w}^{\\top}\\mathbf{x}_{0} + b$。如果 $S_0 \\ge 0$，成本为 $0$。\n2. 如果 $S_0  0$，通过找到在箱形和不可变性约束下的最大可能分数 $S_{max}$ 来检查可行性。如果 $S_{max}  0$，问题不可行；成本为 $\\infty$。\n3. 如果问题可行，使用二分法在区间 $[\\lambda_{low}, \\lambda_{high}]$（例如 $[0, 1000]$）上找到 $g(\\lambda) = 0$ 的根 $\\lambda^{\\star}$。\n4. 使用最优的 $\\lambda^{\\star}$，通过对每个可变特征应用裁剪公式来计算最优解向量 $\\mathbf{z}^{\\star}$。\n5. 计算最终成本 $\\|\\mathbf{z}^{\\star} - \\mathbf{x}_{0}\\|_{\\mathbf{C}} = \\sqrt{\\sum_{i=0}^{d-1} C_{ii}(z_i^{\\star} - (x_0)_i)^2}$。\n此过程为该凸优化问题提供了一个精确解，其精度仅受限于二分法的数值精度。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the constrained optimization problem for multiple test cases.\n    \"\"\"\n    # Model and metric parameters (shared by all test cases)\n    w = np.array([1.2, -0.7, 0.3, 1.5])\n    b = -0.2\n    C_diag = np.array([1.0, 1.0, 4.0, 0.25])\n    ell = np.array([0.0, 0.0, 0.0, 0.0])\n    u = np.array([1.0, 1.0, 1.0, 1.0])\n\n    # Test suite\n    test_cases = [\n        # Case 1: general feasible case\n        {'x0': np.array([0.2, 0.8, 0.1, 0.1]), 'm': np.array([1, 1, 1, 1])},\n        # Case 2: already approved, zero modification\n        {'x0': np.array([0.9, 0.1, 0.1, 0.1]), 'm': np.array([1, 1, 1, 1])},\n        # Case 3: immutability increases difficulty but feasible\n        {'x0': np.array([0.2, 0.8, 0.1, 0.1]), 'm': np.array([1, 1, 1, 0])},\n        # Case 4: infeasible under bounds and immutability\n        {'x0': np.array([0.0, 1.0, 0.0, 0.0]), 'm': np.array([0, 0, 1, 0])},\n    ]\n\n    results = [\n        _solve_single_case(\n            case['x0'], case['m'], w, b, C_diag, ell, u\n        ) for case in test_cases\n    ]\n\n    # Format output according to problem specification\n    formatted_results = []\n    for r in results:\n        if r == np.inf:\n            formatted_results.append(\"inf\")\n        else:\n            formatted_results.append(f\"{r:.6f}\")\n\n    print(f\"[{','.join(formatted_results)}]\")\n\ndef _solve_single_case(x0, m, w, b, C_diag, ell, u):\n    \"\"\"\n    Solves the recourse optimization problem for a single applicant.\n    \"\"\"\n    # Step 1: Check if already approved\n    score_x0 = w.T @ x0 + b\n    if score_x0 = 0:\n        return 0.0\n\n    mutable_idx = np.where(m == 1)[0]\n    \n    # Step 2: Check for feasibility\n    # Find the maximum possible score under immutability and box constraints.\n    z_best = np.copy(x0)\n    for i in mutable_idx:\n        if w[i]  0:\n            z_best[i] = u[i]\n        elif w[i]  0:\n            z_best[i] = ell[i]\n    \n    max_score = w.T @ z_best + b\n    if max_score  0:\n        return np.inf\n\n    # Step 3: Find the optimal Lagrange multiplier lambda using bisection\n    \n    # Define the score function g(lambda) = w^T z(lambda) + b = 0\n    def g(lam):\n        z_lam = np.copy(x0)\n        for i in mutable_idx:\n            z_i_unconstrained = x0[i] + lam * w[i] / C_diag[i]\n            z_lam[i] = np.clip(z_i_unconstrained, ell[i], u[i])\n        return w.T @ z_lam + b\n\n    # Bisection search for lambda\n    lam_low = 0.0\n    lam_high = 1.0\n    # Establish a safe upper bound for lambda\n    while g(lam_high)  0:\n        lam_high *= 2.0\n        if lam_high  1e12: # Safety break against infinite loop\n            return np.inf\n\n    # Perform bisection for a fixed number of iterations for high precision\n    for _ in range(100):\n        lam_mid = (lam_low + lam_high) / 2\n        if g(lam_mid)  0:\n            lam_low = lam_mid\n        else:\n            lam_high = lam_mid\n    \n    lambda_star = (lam_low + lam_high) / 2\n\n    # Step 4: Compute the optimal solution z_star and its cost\n    z_star = np.copy(x0)\n    for i in mutable_idx:\n        z_i_unconstrained = x0[i] + lambda_star * w[i] / C_diag[i]\n        z_star[i] = np.clip(z_i_unconstrained, ell[i], u[i])\n\n    delta_z = z_star - x0\n    cost_squared = np.sum(C_diag * (delta_z**2))\n    cost = np.sqrt(cost_squared)\n\n    return cost\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "2385810"}]}