{"hands_on_practices": [{"introduction": "理论学习之后，最核心的第一步是理解一个训练好的神经网络如何进行预测。这个过程被称为“前向传播”（forward propagation），即数据从输入层开始，逐层流经网络，最终在输出层得到预测结果。本练习 [@problem_id:2387288] 将引导你亲手完成一个完整的前向传播计算，通过具体的数值和步骤，揭示模型“黑箱”内部的运作机制，让你对网络的决策过程有更直观的认识。", "problem": "您将处理一个二元分类任务，其背景源于计算经济学和金融学中的公司治理分析。每个样本代表公司行为准则的一个章节，该章节被编码为一个实值特征向量，该向量总结了每个章节中概念类别的归一化频率。任务目标是分类判断某一章节是否表明内部控制存在潜在弱点。\n\n模型定义：\n- 设输入为一个向量 $x \\in \\mathbb{R}^{8}$，其分量是以下八个概念类别的归一化频率（无单位，取值范围为 $[0,1]$），顺序如下：举报人报告清晰度、礼品政策严格性、利益冲突控制、审计委员会独立性、职责分离、关联方交易监督、豁免或例外频率、模糊语言频率。\n- 考虑一个具有整流线性单元（ReLU）激活函数和逻辑输出的单隐藏层前馈模型。定义如下：\n$$\nh(x) = \\phi\\!\\left(W_1 x + b_1\\right) \\in \\mathbb{R}^{4}, \\quad \\phi(z)_i = \\max\\{0, z_i\\},\n$$\n$$\nz_2(x) = W_2^\\top h(x) + b_2 \\in \\mathbb{R}, \\quad p(x) = \\sigma\\!\\left(z_2(x)\\right) = \\frac{1}{1 + e^{-z_2(x)}} \\in (0,1).\n$$\n类别预测为 $\\hat{y}(x) = 1$（如果 $p(x) \\geq 0.5$）或 $\\hat{y}(x) = 0$（其他情况）。\n- 参数是固定的，由下式给出：\n$$\nW_1 =\n\\begin{bmatrix}\n-0.1 & -0.1 & -0.1 & -0.1 & -0.1 & -0.1 & 0.6 & 0.6 \\\\\n0.4 & 0.4 & 0.3 & 0.3 & 0.3 & 0.3 & -0.2 & -0.2 \\\\\n0 & 0 & 0 & 0 & 0 & 0 & 0 & 0.8 \\\\\n0 & 0 & 0 & 0 & 0 & 0 & 0.8 & 0\n\\end{bmatrix},\n\\quad\nb_1 =\n\\begin{bmatrix}\n-0.1 \\\\ -0.05 \\\\ -0.2 \\\\ -0.2\n\\end{bmatrix},\n$$\n$$\nW_2 =\n\\begin{bmatrix}\n0.8 \\\\ -0.6 \\\\ 0.5 \\\\ 0.5\n\\end{bmatrix},\n\\quad\nb_2 = 0.\n$$\n\n任务：\n- 对于下面测试集中的每个测试输入 $x$，使用上述模型计算 $p(x)$ 和类别预测 $\\hat{y}(x)$。您必须仅输出整数形式的类别预测。\n\n测试集（每个 $x$ 是 $\\mathbb{R}^{8}$ 中的一个元素，其各分量取值在 $[0,1]$ 范围内）：\n- $x^{(1)} = [\\, 0.9,\\, 0.9,\\, 0.9,\\, 0.9,\\, 0.9,\\, 0.9,\\, 0.0,\\, 0.0 \\,]$\n- $x^{(2)} = [\\, 0.1,\\, 0.1,\\, 0.1,\\, 0.1,\\, 0.1,\\, 0.1,\\, 0.9,\\, 0.9 \\,]$\n- $x^{(3)} = [\\, 0.0,\\, 0.0,\\, 0.0,\\, 0.0,\\, 0.0,\\, 0.0,\\, 0.0,\\, 0.0 \\,]$\n- $x^{(4)} = [\\, 0.5,\\, 0.5,\\, 0.5,\\, 0.5,\\, 0.5,\\, 0.5,\\, 0.5,\\, 0.5 \\,]$\n- $x^{(5)} = [\\, 0.2,\\, 0.2,\\, 0.2,\\, 0.2,\\, 0.2,\\, 0.2,\\, 1.0,\\, 0.0 \\,]$\n\n测试覆盖设计：\n- 该测试集包括一个强控制信号且无风险信号的案例、一个弱控制且强风险信号的案例、一个输入为零向量的边界案例、一个混合的中等强度案例，以及一个低控制且单一风险因素被激活的案例。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的、由逗号分隔且不含空格的结果列表。例如，如果五个预测值为 $\\hat{y}^{(1)},\\ldots,\\hat{y}^{(5)} \\in \\{0,1\\}$，则应精确打印“[y1,y2,y3,y4,y5]”。", "solution": "问题陈述经过验证。\n\n**步骤1：提取给定信息**\n\n- **输入**：一个特征向量 $x \\in \\mathbb{R}^{8}$，其分量在 $[0,1]$ 范围内。\n- **模型架构**：一个单隐藏层前馈网络。\n- **隐藏层**：\n  $$h(x) = \\phi\\!\\left(W_1 x + b_1\\right) \\in \\mathbb{R}^{4}$$\n  其中 $\\phi(z)_i = \\max\\{0, z_i\\}$ 是整流线性单元（ReLU）激活函数。\n- **输出层**：\n  $$z_2(x) = W_2^\\top h(x) + b_2 \\in \\mathbb{R}$$\n  $$p(x) = \\sigma\\!\\left(z_2(x)\\right) = \\frac{1}{1 + e^{-z_2(x)}} \\in (0,1)$$\n- **分类规则**：\n  $$\\hat{y}(x) = 1 \\text{ 如果 } p(x) \\geq 0.5$$\n  $$\\hat{y}(x) = 0 \\text{ 如果 } p(x) < 0.5$$\n- **参数**：\n  $$\n  W_1 =\n  \\begin{bmatrix}\n  -0.1 & -0.1 & -0.1 & -0.1 & -0.1 & -0.1 & 0.6 & 0.6 \\\\\n  0.4 & 0.4 & 0.3 & 0.3 & 0.3 & 0.3 & -0.2 & -0.2 \\\\\n  0 & 0 & 0 & 0 & 0 & 0 & 0 & 0.8 \\\\\n  0 & 0 & 0 & 0 & 0 & 0 & 0.8 & 0\n  \\end{bmatrix},\n  \\quad\n  b_1 =\n  \\begin{bmatrix}\n  -0.1 \\\\ -0.05 \\\\ -0.2 \\\\ -0.2\n  \\end{bmatrix}\n  $$\n  $$\n  W_2 =\n  \\begin{bmatrix}\n  0.8 \\\\ -0.6 \\\\ 0.5 \\\\ 0.5\n  \\end{bmatrix},\n  \\quad\n  b_2 = 0\n  $$\n- **测试集**：\n  - $x^{(1)} = [\\, 0.9,\\, 0.9,\\, 0.9,\\, 0.9,\\, 0.9,\\, 0.9,\\, 0.0,\\, 0.0 \\,]^\\top$\n  - $x^{(2)} = [\\, 0.1,\\, 0.1,\\, 0.1,\\, 0.1,\\, 0.1,\\, 0.1,\\, 0.9,\\, 0.9 \\,]^\\top$\n  - $x^{(3)} = [\\, 0.0,\\, 0.0,\\, 0.0,\\, 0.0,\\, 0.0,\\, 0.0,\\, 0.0,\\, 0.0 \\,]^\\top$\n  - $x^{(4)} = [\\, 0.5,\\, 0.5,\\, 0.5,\\, 0.5,\\, 0.5,\\, 0.5,\\, 0.5,\\, 0.5 \\,]^\\top$\n  - $x^{(5)} = [\\, 0.2,\\, 0.2,\\, 0.2,\\, 0.2,\\, 0.2,\\, 0.2,\\, 1.0,\\, 0.0 \\,]^\\top$\n- **任务**：为测试集中的每个输入向量计算 $\\hat{y}(x)$。\n\n**步骤2：使用提取的给定信息进行验证**\n\n根据既定标准对该问题进行评估。\n- **科学依据**：该问题描述了一个标准的前馈神经网络，这是机器学习和深度学习中的一个基本模型。所涉及的数学运算（矩阵乘法、ReLU激活、logistic sigmoid函数）都是标准且正确的。其应用背景是合理的。\n- **适定性**：该问题是完全指定的。所有参数（$W_1, b_1, W_2, b_2$）、输入向量（$x^{(i)}$）和函数（$\\phi, \\sigma$）都有明确定义。所有矩阵和向量的维度都是一致的。例如，$W_1$ 是 $4 \\times 8$，$x$ 是 $8 \\times 1$，相乘产生一个 $4 \\times 1$ 的结果，与 $b_1$ 的维度相匹配。对于每个测试用例，都存在唯一、稳定的解。\n- **客观性**：该问题以精确、定量的术语陈述，没有歧义或主观论断。\n\n**步骤3：结论与行动**\n\n该问题是**有效的**。它在科学上是合理的、适定的、客观的，并且不包含矛盾或缺失信息。可以推导出严谨的解。\n\n**解题推导**\n\n分类规则 $\\hat{y}(x) = 1$（如果 $p(x) \\geq 0.5$）等价于基于 logit 值 $z_2(x)$ 的符号进行分类。由于 logistic 函数 $\\sigma(z)$ 是单调递增的，且 $\\sigma(0) = 0.5$，因此条件 $p(x) \\geq 0.5$ 等价于 $z_2(x) \\geq 0$。我们将为每个测试用例计算 $z_2(x)$。\n\n计算步骤如下：\n1. 计算隐藏层的激活前向量：$z_1 = W_1 x + b_1$。\n2. 逐分量应用ReLU激活函数：$h = \\phi(z_1) = \\max\\{0, z_1\\}$。\n3. 计算logit值（输出的激活前值）：$z_2 = W_2^\\top h + b_2$。\n4. 确定类别：如果 $z_2 \\geq 0$，则 $\\hat{y} = 1$；如果 $z_2 < 0$，则 $\\hat{y} = 0$。\n\n**案例1：$x^{(1)} = [\\, 0.9,\\, 0.9,\\, 0.9,\\, 0.9,\\, 0.9,\\, 0.9,\\, 0.0,\\, 0.0 \\,]^\\top$**\n$z_1^{(1)} = W_1 x^{(1)} + b_1 = \\begin{bmatrix} (-0.1 \\times 6 \\times 0.9) + 0 - 0.1 \\\\ (0.4 \\times 2 + 0.3 \\times 4) \\times 0.9 - 0.05 \\\\ 0 - 0.2 \\\\ 0 - 0.2 \\end{bmatrix} = \\begin{bmatrix} -0.54 - 0.1 \\\\ 1.8 - 0.05 \\\\ -0.2 \\\\ -0.2 \\end{bmatrix} = \\begin{bmatrix} -0.64 \\\\ 1.75 \\\\ -0.2 \\\\ -0.2 \\end{bmatrix}$\n$h^{(1)} = \\phi(z_1^{(1)}) = [\\, 0, 1.75, 0, 0 \\,]^\\top$\n$z_2^{(1)} = W_2^\\top h^{(1)} + b_2 = 0.8(0) - 0.6(1.75) + 0.5(0) + 0.5(0) + 0 = -1.05$\n由于 $z_2^{(1)} = -1.05 < 0$，预测值为 $\\hat{y}^{(1)} = 0$。\n\n**案例2：$x^{(2)} = [\\, 0.1,\\, 0.1,\\, 0.1,\\, 0.1,\\, 0.1,\\, 0.1,\\, 0.9,\\, 0.9 \\,]^\\top$**\n$z_1^{(2)} = W_1 x^{(2)} + b_1 = \\begin{bmatrix} (-0.1 \\times 6 \\times 0.1) + (0.6 \\times 2 \\times 0.9) - 0.1 \\\\ (0.4 \\times 2 + 0.3 \\times 4) \\times 0.1 + (-0.2 \\times 2 \\times 0.9) - 0.05 \\\\ 0.8(0.9) - 0.2 \\\\ 0.8(0.9) - 0.2 \\end{bmatrix} = \\begin{bmatrix} -0.06 + 1.08 - 0.1 \\\\ 0.2 - 0.36 - 0.05 \\\\ 0.72 - 0.2 \\\\ 0.72 - 0.2 \\end{bmatrix} = \\begin{bmatrix} 0.92 \\\\ -0.21 \\\\ 0.52 \\\\ 0.52 \\end{bmatrix}$\n$h^{(2)} = \\phi(z_1^{(2)}) = [\\, 0.92, 0, 0.52, 0.52 \\,]^\\top$\n$z_2^{(2)} = W_2^\\top h^{(2)} + b_2 = 0.8(0.92) - 0.6(0) + 0.5(0.52) + 0.5(0.52) + 0 = 0.736 + 0.26 + 0.26 = 1.256$\n由于 $z_2^{(2)} = 1.256 > 0$，预测值为 $\\hat{y}^{(2)} = 1$。\n\n**案例3：$x^{(3)} = [\\, 0.0,\\, 0.0,\\, 0.0,\\, 0.0,\\, 0.0,\\, 0.0,\\, 0.0,\\, 0.0 \\,]^\\top$**\n$z_1^{(3)} = W_1 x^{(3)} + b_1 = W_1 \\cdot 0 + b_1 = b_1 = [\\, -0.1, -0.05, -0.2, -0.2 \\,]^\\top$\n$h^{(3)} = \\phi(z_1^{(3)}) = [\\, 0, 0, 0, 0 \\,]^\\top$\n$z_2^{(3)} = W_2^\\top h^{(3)} + b_2 = W_2^\\top \\cdot 0 + 0 = 0$\n由于 $z_2^{(3)} = 0$，满足 $z_2 \\geq 0$ 条件。预测值为 $\\hat{y}^{(3)} = 1$。\n\n**案例4：$x^{(4)} = [\\, 0.5,\\, 0.5,\\, 0.5,\\, 0.5,\\, 0.5,\\, 0.5,\\, 0.5,\\, 0.5 \\,]^\\top$**\n$z_1^{(4)} = W_1 x^{(4)} + b_1 = \\begin{bmatrix} (-0.1 \\times 6 + 0.6 \\times 2) \\times 0.5 - 0.1 \\\\ (0.4 \\times 2 + 0.3 \\times 4 - 0.2 \\times 2) \\times 0.5 - 0.05 \\\\ 0.8 \\times 0.5 - 0.2 \\\\ 0.8 \\times 0.5 - 0.2 \\end{bmatrix} = \\begin{bmatrix} (0.6) \\times 0.5 - 0.1 \\\\ (1.6) \\times 0.5 - 0.05 \\\\ 0.4 - 0.2 \\\\ 0.4 - 0.2 \\end{bmatrix} = \\begin{bmatrix} 0.3 - 0.1 \\\\ 0.8 - 0.05 \\\\ 0.2 \\\\ 0.2 \\end{bmatrix} = \\begin{bmatrix} 0.2 \\\\ 0.75 \\\\ 0.2 \\\\ 0.2 \\end{bmatrix}$\n$h^{(4)} = \\phi(z_1^{(4)}) = [\\, 0.2, 0.75, 0.2, 0.2 \\,]^\\top$\n$z_2^{(4)} = W_2^\\top h^{(4)} + b_2 = 0.8(0.2) - 0.6(0.75) + 0.5(0.2) + 0.5(0.2) + 0 = 0.16 - 0.45 + 0.1 + 0.1 = -0.09$\n由于 $z_2^{(4)} = -0.09 < 0$，预测值为 $\\hat{y}^{(4)} = 0$。\n\n**案例5：$x^{(5)} = [\\, 0.2,\\, 0.2,\\, 0.2,\\, 0.2,\\, 0.2,\\, 0.2,\\, 1.0,\\, 0.0 \\,]^\\top$**\n$z_1^{(5)} = W_1 x^{(5)} + b_1 = \\begin{bmatrix} (-0.1 \\times 6 \\times 0.2) + 0.6(1.0) - 0.1 \\\\ (0.4 \\times 2 \\times 0.2 + 0.3 \\times 4 \\times 0.2) - 0.2(1.0) - 0.05 \\\\ -0.2 \\\\ 0.8(1.0) - 0.2 \\end{bmatrix} = \\begin{bmatrix} -0.12 + 0.6 - 0.1 \\\\ 0.4 - 0.2 - 0.05 \\\\ -0.2 \\\\ 0.8 - 0.2 \\end{bmatrix} = \\begin{bmatrix} 0.38 \\\\ 0.15 \\\\ -0.2 \\\\ 0.6 \\end{bmatrix}$\n$h^{(5)} = \\phi(z_1^{(5)}) = [\\, 0.38, 0.15, 0, 0.6 \\,]^\\top$\n$z_2^{(5)} = W_2^\\top h^{(5)} + b_2 = 0.8(0.38) - 0.6(0.15) + 0.5(0) + 0.5(0.6) + 0 = 0.304 - 0.09 + 0.3 = 0.514$\n由于 $z_2^{(5)} = 0.514 > 0$，预测值为 $\\hat{y}^{(5)} = 1$。\n\n最终的预测列表是 $[0, 1, 1, 0, 1]$。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the class predictions for a given set of input vectors\n    using a predefined one-hidden-layer neural network.\n    \"\"\"\n    \n    # Define the model parameters as numpy arrays.\n    W1 = np.array([\n        [-0.1, -0.1, -0.1, -0.1, -0.1, -0.1, 0.6, 0.6],\n        [0.4, 0.4, 0.3, 0.3, 0.3, 0.3, -0.2, -0.2],\n        [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8],\n        [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8, 0.0]\n    ])\n\n    b1 = np.array([-0.1, -0.05, -0.2, -0.2])\n\n    W2 = np.array([0.8, -0.6, 0.5, 0.5])\n    \n    # b2 is given as 0.0\n    b2 = 0.0\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        np.array([0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.0, 0.0]),\n        np.array([0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.9, 0.9]),\n        np.array([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]),\n        np.array([0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5]),\n        np.array([0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 1.0, 0.0]),\n    ]\n\n    results = []\n    for x in test_cases:\n        # Step 1: Calculate the pre-activation of the hidden layer.\n        # W1 is (4, 8), x is (8,). The result z1 is (4,).\n        z1 = W1 @ x + b1\n\n        # Step 2: Apply the ReLU activation function.\n        # np.maximum performs an element-wise maximum.\n        h = np.maximum(0, z1)\n\n        # Step 3: Calculate the pre-activation of the output layer (logit).\n        # W2.T is (4,), h is (4,). The result z2 is a scalar.\n        z2 = W2.T @ h + b2\n\n        # Step 4: Determine the class prediction.\n        # The classification rule is y_hat = 1 if p(x) >= 0.5, which is equivalent\n        # to z2(x) >= 0.\n        y_hat = 1 if z2 >= 0 else 0\n        \n        results.append(y_hat)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2387288"}, {"introduction": "一个未经训练的模型是毫无用处的。在掌握了前向传播之后，我们自然会问：模型中的参数（权重和偏置）是如何确定的？本练习 [@problem_id:2387257] 将带你深入了解模型训练的核心——梯度下降法。你将从零开始，为一项分类任务推导并实施梯度下降算法，亲身体验驱动深度学习的优化引擎是如何工作的。", "problem": "您将获得一个简化的、完全指定的列表式分类问题，该问题旨在模拟以下问题：在给定时间，在一组有限的候选国家中，仅根据经济和政治一致性的数值指标，哪个国家最有可能下一个加入某个特定的贸易集团。每个候选国家由一个包含归一化一致性度量指标的固定长度特征向量表示。您将把此问题作为一个概率分类任务来处理，使用一个单层模型，该模型为每个候选者分配一个标量分数，将分数转换为候选集上的概率分布，并通过最大化观测到的过去结果的联合似然来进行训练。\n\n从以下基本原理出发：对于 $n$ 个互斥结果的一组分数 $\\{s_k\\}_{k=1}^{n}$，分类概率分布由归一化指数映射 (softmax) 定义，参数通过最大化似然（等价于最小化负对数似然）来估计。目标函数应包含对参数的标准 $\\ell_2$ 惩罚项。训练算法应使用从第一性原理推导出的基于梯度的优化方法。\n\n数学设定：\n\n- 对于每个具有 $n_t$ 个候选国家的训练事件 $t$，存在一个特征矩阵 $X_t \\in \\mathbb{R}^{n_t \\times d}$，其中每一行 $x_{t,k} \\in \\mathbb{R}^{d}$ 代表候选者 $k$ 的 $d$ 维特征向量；以及一个观测索引 $y_t \\in \\{0,1,\\dots,n_t-1\\}$，表示在该事件中实际下一个加入的国家。\n- 使用参数为 $w \\in \\mathbb{R}^{d}$ 和 $b \\in \\mathbb{R}$ 的单层线性评分模型 $s_{t,k} = w^\\top x_{t,k} + b$，并通过对 $n_t$ 个候选者应用 softmax 映射来定义事件 $t$ 的模型隐含概率。\n- 所有训练事件的总目标函数是负对数似然与对 $w$ 的 $\\ell_2$ 惩罚项之和，惩罚项系数为 $\\lambda > 0$（不对 $b$ 进行正则化）。\n\n您的任务：\n\n1) 基于以上原理，从似然定义和链式法则出发，明确推导实现参数 $w$ 和 $b$ 的基于梯度的学习规则所需的表达式，不使用捷径。\n\n2) 使用批量梯度下降和固定步长实现一个训练程序。对 softmax 使用数值稳定的计算方法。训练过程必须是确定性的，不得依赖任何随机性。\n\n3) 在下方指定的训练集上进行训练后，通过计算每个测试用例中具有最高预测概率的候选者的索引（从零开始），在提供的测试套件（训练中未使用的候选集）上评估训练好的模型。\n\n数据规格：\n\n- 每个特征向量的维度：$d = 5$。\n- 特征语义和范围（仅用于解释；程序会将其视为纯数字）：\n  - $x_1$：与该集团的归一化贸易份额，范围在 $[0,1]$，\n  - $x_2$：归一化的关税政策一致性，范围在 $[0,1]$，\n  - $x_3$：归一化的人均国内生产总值一致性，范围在 $[0,1]$，\n  - $x_4$：归一化的民主与治理相似度，范围在 $[0,1]$，\n  - $x_5$：归一化的外交政策投票相似度，范围在 $[0,1]$。\n\n训练事件（每个事件是一个候选者矩阵，后跟观测到的加入索引）：\n- 事件 $1$ ($n_1 = 3$)：\n  - 候选者：\n    - $[0.60, 0.70, 0.50, 0.60, 0.65]$\n    - $[0.40, 0.50, 0.60, 0.50, 0.45]$\n    - $[0.80, 0.80, 0.70, 0.70, 0.80]$\n  - 观测索引 $y_1 = 2$。\n- 事件 $2$ ($n_2 = 2$)：\n  - 候选者：\n    - $[0.30, 0.40, 0.50, 0.40, 0.40]$\n    - $[0.70, 0.60, 0.60, 0.80, 0.70]$\n  - 观测索引 $y_2 = 1$。\n- 事件 $3$ ($n_3 = 4$)：\n  - 候选者：\n    - $[0.20, 0.30, 0.40, 0.40, 0.30]$\n    - $[0.50, 0.60, 0.50, 0.50, 0.60]$\n    - $[0.90, 0.90, 0.90, 0.80, 0.85]$\n    - $[0.70, 0.50, 0.70, 0.60, 0.65]$\n  - 观测索引 $y_3 = 2$。\n\n超参数：\n- 学习率 $\\eta = 0.10$，\n- 正则化系数 $\\lambda = 10^{-3}$，\n- 全批量迭代次数 (轮次) $T = 2000$。\n\n测试套件（用于评估的候选集；每个测试用例输出一个整数）：\n- 测试用例 $1$ ($n = 3$)：\n  - $[0.65, 0.70, 0.55, 0.60, 0.60]$\n  - $[0.50, 0.50, 0.55, 0.45, 0.50]$\n  - $[0.75, 0.80, 0.75, 0.75, 0.82]$\n- 测试用例 $2$ ($n = 2$)：\n  - $[0.35, 0.45, 0.55, 0.50, 0.48]$\n  - $[0.60, 0.55, 0.50, 0.70, 0.60]$\n- 测试用例 $3$ ($n = 4$)：\n  - $[0.55, 0.60, 0.60, 0.60, 0.60]$\n  - $[0.60, 0.55, 0.55, 0.55, 0.55]$\n  - $[0.65, 0.65, 0.65, 0.65, 0.65]$\n  - $[0.50, 0.50, 0.50, 0.50, 0.50]$\n- 测试用例 $4$ ($n = 3$)：\n  - $[0.00, 0.00, 0.00, 0.00, 0.00]$\n  - $[0.20, 0.20, 0.20, 0.20, 0.20]$\n  - $[0.10, 0.10, 0.10, 0.10, 0.10]$\n\n最终输出规格：\n- 您的程序应使用指定的超参数在提供的训练事件上进行训练，然后生成一行输出，其中包含四个测试用例的预测索引（从零开始），格式为用方括号括起来的逗号分隔列表，例如 $[a,b,c,d]$，其中 $a$、$b$、$c$、$d$ 均为整数，取值范围在 $\\{0,1,2,3\\}$ 内，具体取决于测试用例的大小。", "solution": "所呈现的问题是一个适定的、有科学依据的分类任务，可以使用标准的多项逻辑回归模型来解决。其有效性已得到确认，并且解决方案是从第一性原理推导出来的。\n\n核心任务是训练一个带参数 $w \\in \\mathbb{R}^{d}$ 和 $b \\in \\mathbb{R}$ 的线性模型，用以预测一个集合中的哪个候选者最有可能加入某个贸易集团。训练通过使用批量梯度下降法最小化一个带正则化的负对数似然目标函数来执行。\n\n假设有 $N$ 个训练事件，由 $t \\in \\{1, 2, \\dots, N\\}$ 索引。对于每个事件 $t$，我们有一组 $n_t$ 个候选国家，其特征由矩阵 $X_t \\in \\mathbb{R}^{n_t \\times d}$ 表示。每一行 $x_{t,k} \\in \\mathbb{R}^{d}$ 是候选者 $k \\in \\{0, 1, \\dots, n_t-1\\}$ 的特征向量。观测结果是加入国家的索引 $y_t$。\n\n事件 $t$ 中候选者 $k$ 的分数由一个线性函数给出：\n$$s_{t,k} = w^\\top x_{t,k} + b$$\n\n候选者 $k$ 被选中的概率由 softmax 函数建模，该函数将分数归一化为在 $n_t$ 个候选者上的概率分布：\n$$P_{t,k} \\equiv P(y=k | X_t, w, b) = \\frac{\\exp(s_{t,k})}{\\sum_{j=0}^{n_t-1} \\exp(s_{t,j})}$$\n\n目标是通过最大化观测到训练数据 $\\{y_t\\}_{t=1}^N$ 的联合似然来找到参数 $w$ 和 $b$。这等价于最小化总负对数似然 $L_{NLL}$。我们为权重向量 $w$ 添加一个 $\\ell_2$ 正则化项以防止过拟合。总目标函数 $L(w,b)$ 为：\n$$L(w, b) = L_{NLL} + L_{REG} = \\left( \\sum_{t=1}^{N} L_t(w, b) \\right) + \\frac{\\lambda}{2} \\|w\\|^2_2$$\n其中 $\\lambda > 0$ 是正则化系数， $L_t$ 是单个事件 $t$ 的负对数似然：\n$$L_t(w,b) = -\\log(P_{t, y_t})$$\n代入分数和概率的表达式，我们可以将 $L_t$ 写为：\n$$L_t = -\\log\\left(\\frac{\\exp(s_{t,y_t})}{\\sum_{j=0}^{n_t-1} \\exp(s_{t,j})}\\right) = -s_{t,y_t} + \\log\\left(\\sum_{j=0}^{n_t-1} \\exp(s_{t,j})\\right)$$\n\n为了使用梯度下降法训练模型，我们必须计算总目标函数 $L$ 对于参数 $w$ 和 $b$ 的偏导数。参数 $\\theta$ 的梯度更新规则是 $\\theta \\leftarrow \\theta - \\eta \\nabla_{\\theta} L$，其中 $\\eta$ 是学习率。\n\n首先，我们推导关于权重向量 $w$ 的梯度。正则化项的梯度是直观的：\n$$\\frac{\\partial}{\\partial w} \\left(\\frac{\\lambda}{2} w^\\top w\\right) = \\lambda w$$\n负对数似然的梯度是每个事件梯度的总和：$\\frac{\\partial L_{NLL}}{\\partial w} = \\sum_{t=1}^N \\frac{\\partial L_t}{\\partial w}$。我们使用链式法则来求单个事件 $t$ 的梯度。对于 $w$ 的第 $i$ 个分量，记为 $w_i$：\n$$\\frac{\\partial L_t}{\\partial w_i} = \\sum_{k=0}^{n_t-1} \\frac{\\partial L_t}{\\partial s_{t,k}} \\frac{\\partial s_{t,k}}{\\partial w_i}$$\n分数 $s_{t,k}$ 对 $w_i$ 的导数是：\n$$\\frac{\\partial s_{t,k}}{\\partial w_i} = \\frac{\\partial}{\\partial w_i} \\left(\\sum_{l=1}^{d} w_l (x_{t,k})_l + b\\right) = (x_{t,k})_i$$\n损失 $L_t$ 对分数 $s_{t,k}$ 的导数是：\n$$\\frac{\\partial L_t}{\\partial s_{t,k}} = \\frac{\\partial}{\\partial s_{t,k}} \\left(-s_{t,y_t} + \\log\\left(\\sum_{j=0}^{n_t-1} \\exp(s_{t,j})\\right)\\right) = -\\delta_{k, y_t} + \\frac{\\exp(s_{t,k})}{\\sum_{j=0}^{n_t-1} \\exp(s_{t,j})} = P_{t,k} - \\delta_{k, y_t}$$\n其中 $\\delta_{k, y_t}$ 是克罗内克 δ (Kronecker delta)，当 $k=y_t$ 时为 $1$，否则为 $0$。\n\n综合这些部分，$L_t$ 对 $w_i$ 的梯度是：\n$$\\frac{\\partial L_t}{\\partial w_i} = \\sum_{k=0}^{n_t-1} (P_{t,k} - \\delta_{k, y_t}) (x_{t,k})_i$$\n以向量形式表示，事件 $t$ 的梯度是：\n$$\\frac{\\partial L_t}{\\partial w} = \\sum_{k=0}^{n_t-1} (P_{t,k} - \\delta_{k, y_t}) x_{t,k} = X_t^\\top (p_t - e_{y_t})$$\n其中 $p_t$ 是概率向量 $\\{P_{t,k}\\}_{k=0}^{n_t-1}$，$e_{y_t}$ 是真实标签 $y_t$ 的独热编码向量。\n\n$w$ 的总梯度是所有训练事件的梯度之和加上正则化梯度：\n$$\\nabla_w L(w,b) = \\left( \\sum_{t=1}^{N} \\sum_{k=0}^{n_t-1} (P_{t,k} - \\delta_{k, y_t}) x_{t,k} \\right) + \\lambda w$$\n\n接下来，我们推导关于偏置项 $b$ 的梯度。正则化项不依赖于 $b$。我们再次使用链式法则：\n$$\\frac{\\partial L_t}{\\partial b} = \\sum_{k=0}^{n_t-1} \\frac{\\partial L_t}{\\partial s_{t,k}} \\frac{\\partial s_{t,k}}{\\partial b}$$\n分数对 $b$ 的导数很简单：\n$$\\frac{\\partial s_{t,k}}{\\partial b} = \\frac{\\partial}{\\partial b} (w^\\top x_{t,k} + b) = 1$$\n因此，$L_t$ 对 $b$ 的梯度是：\n$$\\frac{\\partial L_t}{\\partial b} = \\sum_{k=0}^{n_t-1} (P_{t,k} - \\delta_{k, y_t}) (1) = \\left(\\sum_{k=0}^{n_t-1} P_{t,k}\\right) - \\left(\\sum_{k=0}^{n_t-1} \\delta_{k, y_t}\\right)$$\n由于 softmax 的概率之和为 1 ($\\sum P_{t,k} = 1$)，且真实标签的独热编码向量各元素之和也为 1 ($\\sum \\delta_{k, y_t} = 1$)，我们得到：\n$$\\frac{\\partial L_t}{\\partial b} = 1 - 1 = 0$$\n这意味着对于任何单个训练事件 $t$，损失对偏置项 $b$ 的梯度都为零。\n\n因此，所有事件关于 $b$ 的总梯度是：\n$$\\nabla_b L(w,b) = \\sum_{t=1}^{N} \\frac{\\partial L_t}{\\partial b} = \\sum_{t=1}^{N} 0 = 0$$\n这个结果意味着，在使用批量（或小批量）梯度下降时，如果偏置项 $b$ 从零开始初始化，它将永远不会被更新。在实践中，这意味着偏置项对于这个特定的模型结构是多余的，但我们仍将其包含在推导和实现中以保持完整性。\n\n批量梯度下降算法初始化参数 $w$ 和 $b$（例如，为零），并在 $T$ 个轮次中迭代更新它们：\n$$w^{(i+1)} = w^{(i)} - \\eta \\nabla_w L(w^{(i)}, b^{(i)})$$\n$$b^{(i+1)} = b^{(i)} - \\eta \\nabla_b L(w^{(i)}, b^{(i)})$$\n为了数值稳定性，softmax 函数通过平移分数来计算：$s_k \\to s_k - \\max_j(s_j)$。这可以在对大分数进行指数运算时防止溢出，同时保持最终的概率不变。\n\n在使用学习率 $\\eta=0.10$ 和正则化系数 $\\lambda=10^{-3}$ 进行 $T=2000$ 次迭代后，最终的参数 $(w,b)$ 被用来对测试用例进行预测。对于每个测试用例，我们计算所有候选者的分数，并选择具有最大分数的候选者的索引。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the listwise classification problem using batch gradient descent.\n    \"\"\"\n    # 1. Define problem data and hyperparameters\n    d = 5\n    eta = 0.10\n    lambda_reg = 1e-3\n    T = 2000\n\n    train_events = [\n        (np.array([\n            [0.60, 0.70, 0.50, 0.60, 0.65],\n            [0.40, 0.50, 0.60, 0.50, 0.45],\n            [0.80, 0.80, 0.70, 0.70, 0.80]\n        ]), 2),\n        (np.array([\n            [0.30, 0.40, 0.50, 0.40, 0.40],\n            [0.70, 0.60, 0.60, 0.80, 0.70]\n        ]), 1),\n        (np.array([\n            [0.20, 0.30, 0.40, 0.40, 0.30],\n            [0.50, 0.60, 0.50, 0.50, 0.60],\n            [0.90, 0.90, 0.90, 0.80, 0.85],\n            [0.70, 0.50, 0.70, 0.60, 0.65]\n        ]), 2)\n    ]\n\n    test_cases = [\n        np.array([\n            [0.65, 0.70, 0.55, 0.60, 0.60],\n            [0.50, 0.50, 0.55, 0.45, 0.50],\n            [0.75, 0.80, 0.75, 0.75, 0.82]\n        ]),\n        np.array([\n            [0.35, 0.45, 0.55, 0.50, 0.48],\n            [0.60, 0.55, 0.50, 0.70, 0.60]\n        ]),\n        np.array([\n            [0.55, 0.60, 0.60, 0.60, 0.60],\n            [0.60, 0.55, 0.55, 0.55, 0.55],\n            [0.65, 0.65, 0.65, 0.65, 0.65],\n            [0.50, 0.50, 0.50, 0.50, 0.50]\n        ]),\n        np.array([\n            [0.00, 0.00, 0.00, 0.00, 0.00],\n            [0.20, 0.20, 0.20, 0.20, 0.20],\n            [0.10, 0.10, 0.10, 0.10, 0.10]\n        ])\n    ]\n\n    # 2. Initialize model parameters\n    w = np.zeros(d)\n    b = 0.0\n\n    # 3. Training with Batch Gradient Descent\n    for _ in range(T):\n        grad_w_total = np.zeros(d)\n        grad_b_total = 0.0\n\n        # Loop over all training events to compute the full batch gradient\n        for X_t, y_t in train_events:\n            n_t = X_t.shape[0]\n\n            # Calculate scores: s = Xw + b\n            scores = X_t @ w + b\n            \n            # Numerically stable softmax to compute probabilities\n            scores_stable = scores - np.max(scores)\n            exp_scores = np.exp(scores_stable)\n            probs = exp_scores / np.sum(exp_scores)\n\n            # Create one-hot encoded target vector\n            target = np.zeros(n_t)\n            target[y_t] = 1.0\n            \n            # Calculate the error term (p - y_onehot)\n            error = probs - target\n            \n            # Calculate gradient contribution from this event\n            grad_w_t = X_t.T @ error\n            grad_b_t = np.sum(error) # This will be (close to) zero\n            \n            # Accumulate gradients for the batch\n            grad_w_total += grad_w_t\n            grad_b_total += grad_b_t\n            \n        # Add regularization gradient for w (L2 penalty)\n        grad_w_total += lambda_reg * w\n        \n        # Update parameters using the batch gradient\n        w -= eta * grad_w_total\n        b -= eta * grad_b_total\n\n    # 4. Evaluation on test suite\n    results = []\n    for X_test in test_cases:\n        # Calculate scores for the test case\n        scores = X_test @ w + b\n        # Prediction is the index of the highest score\n        predicted_index = np.argmax(scores)\n        results.append(predicted_index)\n\n    # 5. Print the final results in the specified format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2387257"}, {"introduction": "在金融等高风险领域，模型的预测准确率并非全部，其在压力下的可靠性（即鲁棒性）同样至关重要。本项高级练习 [@problem_id:2387277] 将探讨如何通过模拟“对抗性攻击”（adversarial attacks）来评估模型的鲁棒性。你将计算一个微小且有针对性的输入扰动 $x$ 如何能够欺骗模型，从而获得对现代机器学习研究中这一前沿且重要领域的实践理解。", "problem": "一个自动化的信用评分系统使用前馈神经网络将归一化的申请人特征向量映射到违约概率。您将通过计算每个申请人在沿着损失函数关于输入的梯度的符号方向上进行扰动时，导致模型预测类别发生改变的最小扰动幅度，来评估模型对于限制在容许特征域内的小型对抗性扰动的鲁棒性。所有特征都经过预归一化，位于闭合的超立方体域 $[0,1]^d$ 内。\n\n模型定义。设输入维度为 $d=5$。定义一个具有双曲正切激活函数和一个逻辑输出的单隐藏层神经网络：\n- 隐藏层预激活：$z_1 = W_1 x + b_1 \\in \\mathbb{R}^h$，其中隐藏层大小为 $h=4$。\n- 隐藏层激活：$a_1 = \\tanh(z_1)$，逐元素应用。\n- Logit：$z_2 = w_2^\\top a_1 + b_2 \\in \\mathbb{R}$。\n- 违约概率：$p_\\theta(x) = \\sigma(z_2) = \\dfrac{1}{1 + e^{-z_2}} \\in (0,1)$。\n- 预测类别：$\\hat{y}(x) = \\mathbb{1}\\{p_\\theta(x) \\ge 0.5\\}$。\n\n参数是固定且已知的：\n$$\nW_1 =\n\\begin{bmatrix}\n0.6 & -0.4 & 0.2 & 0.1 & -0.3\\\\\n-0.2 & 0.5 & -0.1 & 0.4 & 0.2\\\\\n0.3 & 0.1 & 0.4 & -0.5 & 0.2\\\\\n-0.1 & 0.3 & 0.2 & 0.2 & -0.4\n\\end{bmatrix},\\quad\nb_1 =\n\\begin{bmatrix}\n0.0\\\\\n0.1\\\\\n-0.05\\\\\n0.05\n\\end{bmatrix},\\quad\nw_2 =\n\\begin{bmatrix}\n0.7\\\\\n-0.6\\\\\n0.5\\\\\n-0.4\n\\end{bmatrix},\\quad\nb_2 = 0.1.\n$$\n\n损失与对抗方向。对于给定的真实标签 $y \\in \\{0,1\\}$，使用二元交叉熵损失函数\n$$\n\\mathcal{L}(x,y) = -\\left[ y \\log p_\\theta(x) + (1-y)\\log\\left(1 - p_\\theta(x)\\right)\\right].\n$$\n令 $g(x,y) = \\nabla_x \\mathcal{L}(x,y)$ 为损失函数关于输入的梯度。在 $x$ 处的快速梯度符号法 (FGSM) 对抗方向是逐元素的符号向量 $\\mathrm{sign}\\!\\left(g(x,y)\\right) \\in \\{-1,0,1\\}^d$。对于一个扰动预算 $\\varepsilon \\ge 0$，定义扰动后的输入\n$$\nx_{\\mathrm{adv}}(\\varepsilon) = \\Pi_{[0,1]^d}\\!\\left(x + \\varepsilon \\cdot \\mathrm{sign}\\!\\left(g(x,y)\\right)\\right),\n$$\n其中 $\\Pi_{[0,1]^d}(\\cdot)$ 表示逐元素地投影（裁剪）到盒子 $[0,1]^d$ 上。\n\n沿 FGSM 方向的最小对抗预算。对于下方的每个测试用例 $(x,y)$，定义初始预测类别 $\\hat{y}_0 = \\hat{y}(x)$。在固定边界 $\\varepsilon_{\\max}$ 内，沿 FGSM 方向的最小对抗预算为\n$$\n\\varepsilon^\\star(x,y) = \n\\begin{cases}\n0.0,& \\text{如果 } \\hat{y}_0 \\ne y,\\\\\n\\inf\\left\\{\\varepsilon \\in [0,\\varepsilon_{\\max}] : \\hat{y}\\!\\left(x_{\\mathrm{adv}}(\\varepsilon)\\right) \\ne \\hat{y}_0 \\right\\}, & \\text{否则},\n\\end{cases}\n$$\n我们约定，如果不存在这样的 $\\varepsilon \\in [0,\\varepsilon_{\\max}]$，则 $\\varepsilon^\\star(x,y) = \\varepsilon_{\\max}$。\n\n您的任务：\n- 仅从链式法则和以上定义出发，推导出一个关于 $W_1$、$b_1$、$w_2$、$b_2$、$x$ 和 $y$ 的 $\\nabla_x \\mathcal{L}(x,y)$ 的显式可实现表达式。\n- 实现一个数值程序，为每个测试用例找到 $\\varepsilon^\\star(x,y)$。该程序使用从粗到细的搜索方法：首先通过在 $[0,\\varepsilon_{\\max}]$ 上进行均匀网格搜索来定位一个包围区间，然后通过二分法进行细化，直到区间宽度低于指定的容差。假设映射 $\\varepsilon \\mapsto \\hat{y}(x_{\\mathrm{adv}}(\\varepsilon))$ 是具有有限数量跳跃点的分段常数函数。对于每个被扰动的候选点，通过投影来严格执行盒子约束 $[0,1]^d$。\n- 使用 $\\varepsilon_{\\max} = 0.5$（无量纲）和 $10^{-6}$ 的搜索容差（无量纲）。粗网格必须包含 $[0,\\varepsilon_{\\max}]$ 中的 $N=512$ 个均匀间隔的点。\n\n测试套件。使用以下6个申请人特征向量和标签，其中每个 $x \\in [0,1]^5$ 按 $(\\text{收入}, \\text{债务比率}, \\text{年龄}, \\text{历史长度}, \\text{拖欠记录})$ 的顺序给出作为归一化特征，而 $y \\in \\{0,1\\}$ 是给定的真实违约指示符：\n$$\nx^{(1)} = \\begin{bmatrix}0.7\\\\ 0.2\\\\ 0.4\\\\ 0.6\\\\ 0.3\\end{bmatrix},\\ y^{(1)} = 1;\\quad\nx^{(2)} = \\begin{bmatrix}0.1\\\\ 0.9\\\\ 0.2\\\\ 0.1\\\\ 0.8\\end{bmatrix},\\ y^{(2)} = 0;\\\\\nx^{(3)} = \\begin{bmatrix}0.5\\\\ 0.5\\\\ 0.5\\\\ 0.5\\\\ 0.5\\end{bmatrix},\\ y^{(3)} = 1;\\quad\nx^{(4)} = \\begin{bmatrix}0.0\\\\ 0.0\\\\ 0.0\\\\ 0.0\\\\ 0.0\\end{bmatrix},\\ y^{(4)} = 0;\\\\\nx^{(5)} = \\begin{bmatrix}1.0\\\\ 1.0\\\\ 1.0\\\\ 1.0\\\\ 1.0\\end{bmatrix},\\ y^{(5)} = 0;\\quad\nx^{(6)} = \\begin{bmatrix}0.9\\\\ 0.1\\\\ 0.8\\\\ 0.2\\\\ 0.1\\end{bmatrix},\\ y^{(6)} = 1.\n$$\n\n输出规格。您的程序应生成单行输出，其中包含六个值 $\\left[\\varepsilon^\\star\\!\\left(x^{(i)},y^{(i)}\\right)\\right]_{i=1}^6$，以逗号分隔列表的形式用方括号括起来，每个值四舍五入到4位小数（使用十进制表示法，而不是百分比）。例如，一个有效的输出格式是 $[0.0312,0.0000,0.5000,0.0125,0.2210,0.0984]$（这些仅为示例；您的程序必须计算给定模型和测试套件的实际值）。", "solution": "所提出的问题被验证为自包含的、有科学依据的、且是良定的。它构成了神经网络对抗鲁棒性研究中的一个标准任务。我们现在开始求解，这需要两个组成部分：首先，损失梯度的解析推导；其次，指定数值搜索过程的实现。\n\n**1. 损失的输入梯度推导**\n\n目标是计算二元交叉熵损失 $\\mathcal{L}(x,y)$ 相对于输入特征向量 $x \\in \\mathbb{R}^d$ 的梯度 $g(x,y) = \\nabla_x \\mathcal{L}(x,y)$。这通过系统地应用链式法则，将导数从标量损失反向传播到输入向量来实现。\n\n模型的前向传播定义了以下函数依赖关系：\n$1.$ $z_1(x) = W_1 x + b_1$\n$2.$ $a_1(z_1) = \\tanh(z_1)$\n$3.$ $z_2(a_1) = w_2^\\top a_1 + b_2$\n$4.$ $p_\\theta(x) = \\sigma(z_2) = (1 + e^{-z_2})^{-1}$\n$5.$ $\\mathcal{L}(x,y) = -\\left[ y \\log p_\\theta(x) + (1-y)\\log\\left(1 - p_\\theta(x)\\right)\\right]$\n\n我们按相反的顺序计算所需的偏导数。\n\n首先，损失 $\\mathcal{L}$ 相对于模型输出概率 $p_\\theta(x)$ 的导数：\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial p_\\theta(x)} = -\\frac{y}{p_\\theta(x)} + \\frac{1-y}{1-p_\\theta(x)} = \\frac{p_\\theta(x) - y}{p_\\theta(x)(1-p_\\theta(x))}\n$$\n\n其次，sigmoid 激活函数 $\\sigma(\\cdot)$ 相对于其输入 logit $z_2$ 的导数：\n$$\n\\frac{\\partial p_\\theta(x)}{\\partial z_2} = \\frac{d\\sigma(z_2)}{dz_2} = \\sigma(z_2)(1-\\sigma(z_2)) = p_\\theta(x)(1-p_\\theta(x))\n$$\n结合这两个结果，得到了一个众所周知的、关于 logits 的交叉熵损失梯度的关键简化：\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial z_2} = \\frac{\\partial \\mathcal{L}}{\\partial p_\\theta(x)} \\frac{\\partial p_\\theta(x)}{\\partial z_2} = \\left(\\frac{p_\\theta(x) - y}{p_\\theta(x)(1-p_\\theta(x))}\\right) \\left(p_\\theta(x)(1-p_\\theta(x))\\right) = p_\\theta(x) - y\n$$\n\n第三，logit $z_2$ 相对于隐藏激活向量 $a_1$ 的梯度：\n$$\nz_2 = w_2^\\top a_1 + b_2 = \\sum_{j=1}^{h} (w_2)_j (a_1)_j + b_2 \\implies \\nabla_{a_1} z_2 = w_2\n$$\n\n第四，隐藏激活向量 $a_1$ 相对于隐藏预激活向量 $z_1$ 的梯度。由于 $a_1 = \\tanh(z_1)$ 是逐元素操作，其雅可比矩阵是对角矩阵：\n$$\n\\frac{\\partial a_1}{\\partial z_1} = \\mathrm{diag}\\left(\\frac{d}{dz_{1,j}} \\tanh(z_{1,j})\\right)_{j=1}^h = \\mathrm{diag}\\left(1 - \\tanh^2(z_{1,j})\\right)_{j=1}^h\n$$\n\n第五，隐藏预激活向量 $z_1$ 相对于输入向量 $x$ 的梯度：\n$$\nz_1 = W_1 x + b_1 \\implies \\frac{\\partial z_1}{\\partial x} = W_1\n$$\n\n最后，我们使用链式法则组合出完整的梯度 $\\nabla_x \\mathcal{L}(x,y)$：\n$$\n\\nabla_x \\mathcal{L}(x,y)^\\top = \\frac{\\partial \\mathcal{L}}{\\partial x} = \\frac{\\partial \\mathcal{L}}{\\partial z_2} \\frac{\\partial z_2}{\\partial a_1} \\frac{\\partial a_1}{\\partial z_1} \\frac{\\partial z_1}{\\partial x}\n$$\n在向量-矩阵表示法中，对于一个列梯度向量：\n$$\n\\nabla_x \\mathcal{L}(x,y) = \\left(\\frac{\\partial z_1}{\\partial x}\\right)^\\top \\left(\\frac{\\partial a_1}{\\partial z_1}\\right)^\\top \\left(\\frac{\\partial z_2}{\\partial a_1}\\right)^\\top \\frac{\\partial \\mathcal{L}}{\\partial z_2} = W_1^\\top \\mathrm{diag}\\left(1 - \\tanh^2(z_1)\\right) w_2 \\left(p_\\theta(x) - y\\right)\n$$\n为了计算实现，使用逐元素（哈达玛）积 $\\odot$ 可以更高效地表达：\n$$\ng(x,y) = \\nabla_x \\mathcal{L}(x,y) = \\left(p_\\theta(x) - y\\right) W_1^\\top \\left( w_2 \\odot \\left(1 - a_1 \\odot a_1\\right) \\right)\n$$\n其中 $a_1 = \\tanh(W_1 x + b_1)$。这就是梯度的显式可实现表达式。\n\n**2. 最小对抗预算的数值程序**\n\n任务是为每个给定的对 $(x,y)$ 找到 $\\varepsilon^\\star(x,y)$。程序如下：\n\n1.  **初始预测与检查**：对于一个给定的测试用例 $(x,y)$，我们首先计算模型的初始预测 $\\hat{y}_0 = \\hat{y}(x) = \\mathbb{1}\\{p_\\theta(x) \\ge 0.5\\}$。问题定义，如果模型初始预测不正确，即 $\\hat{y}_0 \\ne y$，则所需的最小扰动预算为 $\\varepsilon^\\star(x,y) = 0.0$。\n\n2.  **对抗方向**：如果模型预测正确（$\\hat{y}_0 = y$），我们基于快速梯度符号法 (FGSM) 计算对抗方向。这涉及使用上面推导的公式计算梯度 $g(x,y) = \\nabla_x \\mathcal{L}(x,y)$，并取其逐元素的符号：$v = \\mathrm{sign}(g(x,y)) \\in \\{-1, 0, 1\\}^d$。\n\n3.  **搜索最小扰动**：目标是找到导致分类翻转的最小 $\\varepsilon \\in [0, \\varepsilon_{\\max}]$。扰动后的输入为 $x_{\\mathrm{adv}}(\\varepsilon) = \\Pi_{[0,1]^d}(x + \\varepsilon v)$，其中 $\\Pi_{[0,1]^d}$ 是逐元素的裁剪操作，以将特征值保持在 $[0,1]$ 超立方体内。我们寻求 $\\varepsilon^\\star = \\inf\\{\\varepsilon \\in [0, \\varepsilon_{\\max}] : \\hat{y}(x_{\\mathrm{adv}}(\\varepsilon)) \\ne \\hat{y}_0 \\}$。采用两阶段的数值搜索。\n\n    a.  **粗网格搜索**：我们首先搜索一个包围区间。在 $[0, \\varepsilon_{\\max}]$ 上定义一个包含 $N=512$ 个点的均匀网格。设这些点为 $\\{\\varepsilon_i\\}_{i=0}^{N-1}$。我们从 $i=1$ 开始，为每个 $\\varepsilon_i$ 评估模型的预测 $\\hat{y}(x_{\\mathrm{adv}}(\\varepsilon_i))$。第一个出现 $\\hat{y}(x_{\\mathrm{adv}}(\\varepsilon_i)) \\ne \\hat{y}_0$ 的实例确定了一个包含阈值的包围区间 $[a, b] = [\\varepsilon_{i-1}, \\varepsilon_i]$。如果在 $[0, \\varepsilon_{\\max}]$ 内的任何 $\\varepsilon_i$ 都没有观察到这种翻转，则根据问题定义，最小预算取为 $\\varepsilon^\\star(x,y) = \\varepsilon_{\\max}$。\n\n    b.  **二分法细化**：一旦找到一个区间 $[a, b]$，使得 $\\hat{y}(x_{\\mathrm{adv}}(a)) = \\hat{y}_0$ 且 $\\hat{y}(x_{\\mathrm{adv}}(b)) \\ne \\hat{y}_0$，我们使用二分法来细化阈值的位置。该算法重复地将区间二等分，选择保留其端点预测变化的新子区间。该过程持续进行，直到区间宽度 $(b-a)$ 小于指定的容差 $10^{-6}$。最终的 $b$ 值作为 $\\varepsilon^\\star(x,y)$ 的数值近似。这个值是真实下确界的上界，并且随着容差的减小而收敛于它。\n\n这个结构化的程序保证了在给定的数值精度下，能够找到沿固定 FGSM 方向的最小扰动预算。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves for the minimal FGSM adversarial budget for a specified set of test cases.\n    \"\"\"\n    # Model Parameters\n    W1 = np.array([\n        [0.6, -0.4, 0.2, 0.1, -0.3],\n        [-0.2, 0.5, -0.1, 0.4, 0.2],\n        [0.3, 0.1, 0.4, -0.5, 0.2],\n        [-0.1, 0.3, 0.2, 0.2, -0.4]\n    ])\n    b1 = np.array([[0.0], [0.1], [-0.05], [0.05]])\n    w2 = np.array([[0.7], [-0.6], [0.5], [-0.4]])\n    b2 = 0.1\n\n    # Search Hyperparameters\n    eps_max = 0.5\n    N = 512\n    tol = 1e-6\n\n    # Test Cases\n    test_cases = [\n        (np.array([[0.7], [0.2], [0.4], [0.6], [0.3]]), 1),\n        (np.array([[0.1], [0.9], [0.2], [0.1], [0.8]]), 0),\n        (np.array([[0.5], [0.5], [0.5], [0.5], [0.5]]), 1),\n        (np.array([[0.0], [0.0], [0.0], [0.0], [0.0]]), 0),\n        (np.array([[1.0], [1.0], [1.0], [1.0], [1.0]]), 0),\n        (np.array([[0.9], [0.1], [0.8], [0.2], [0.1]]), 1),\n    ]\n\n    def forward(x):\n        \"\"\"Computes the forward pass of the neural network.\"\"\"\n        z1 = W1 @ x + b1\n        a1 = np.tanh(z1)\n        z2 = w2.T @ a1 + b2\n        p = 1.0 / (1.0 + np.exp(-z2))\n        return p.item(), z1, a1, z2.item()\n\n    def predict_from_logit(z2):\n        \"\"\"Predicts class from logit.\"\"\"\n        return 1 if z2 >= 0 else 0\n\n    def gradient(y, p_theta, a1):\n        \"\"\"Computes the gradient of the loss with respect to the input.\"\"\"\n        scalar_term = p_theta - y\n        # Elementwise multiplication for grad_z1\n        grad_z1 = scalar_term * w2 * (1.0 - a1**2)\n        # Matrix multiplication for backpropagation to input\n        grad_x = W1.T @ grad_z1\n        return grad_x\n\n    def find_epsilon_star(x, y):\n        \"\"\"\n        Finds the minimal adversarial budget for a single (x, y) pair.\n        \"\"\"\n        p0, _, a1_0, z2_0 = forward(x)\n        y_hat0 = predict_from_logit(z2_0)\n\n        # Per problem definition, if model is already incorrect, eps_star is 0.\n        if y_hat0 != y:\n            return 0.0\n\n        grad_vec = gradient(y, p0, a1_0)\n        v = np.sign(grad_vec)\n\n        # If gradient is zero, no perturbation is applied, so no flip can occur.\n        if np.all(v == 0):\n            return eps_max\n\n        def get_prediction_at_eps(eps):\n            \"\"\"Helper function to get prediction for a perturbed input.\"\"\"\n            x_adv = x + eps * v\n            x_adv_clipped = np.clip(x_adv, 0.0, 1.0)\n            _, _, _, z2_adv = forward(x_adv_clipped)\n            return predict_from_logit(z2_adv)\n\n        # Coarse grid search to find a bracketing interval [a, b]\n        eps_grid = np.linspace(0.0, eps_max, N)\n        a_bracket, b_bracket = -1.0, -1.0\n        \n        for i in range(1, N):\n            if get_prediction_at_eps(eps_grid[i]) != y_hat0:\n                a_bracket, b_bracket = eps_grid[i-1], eps_grid[i]\n                break\n        \n        # If no flip is found across the entire grid, return eps_max\n        if b_bracket == -1.0:\n            return eps_max\n\n        # Bisection search to refine the interval\n        while (b_bracket - a_bracket) > tol:\n            mid = (a_bracket + b_bracket) / 2.0\n            # Handle precision limits where mid is identical to a bound\n            if mid == a_bracket or mid == b_bracket:\n                break\n            if get_prediction_at_eps(mid) != y_hat0:\n                b_bracket = mid\n            else:\n                a_bracket = mid\n        \n        return b_bracket\n\n    results = []\n    for x_vec, y_val in test_cases:\n        eps_star = find_epsilon_star(x_vec, y_val)\n        results.append(eps_star)\n\n    # Print the final results in the specified format\n    print(f\"[{','.join(f'{r:.4f}' for r in results)}]\")\n\nsolve()\n```", "id": "2387277"}]}