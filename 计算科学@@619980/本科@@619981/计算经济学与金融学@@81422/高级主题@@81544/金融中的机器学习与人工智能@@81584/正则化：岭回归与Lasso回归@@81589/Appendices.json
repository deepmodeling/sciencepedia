{"hands_on_practices": [{"introduction": "本练习提供了一个具体的计算案例，以揭示 LASSO 和岭回归（Ridge）在处理相关预测变量时的根本区别。通过这个计算 [@problem_id:2426291]，您将亲眼看到 LASSO 的 $L_1$ 惩罚项如何通过将其中一个系数设为零来实现稀疏性，而岭回归的 $L_2$ 惩罚项则倾向于将两个相关变量的系数收缩在一起，产生一种“分组”效应。这对于理解它们在特征选择和参数稳定化方面的各自优势至关重要。", "problem": "一位分析师正在使用两个高度相关的预测变量，对一个投资组合的单期超额收益率建立一个线性预测模型，该模型不含截距项，涵盖两个时间周期。设响应向量为 $y \\in \\mathbb{R}^{2}$，预测变量矩阵为 $X \\in \\mathbb{R}^{2 \\times 2}$，其列向量 $x_{1}$ 和 $x_{2}$ 由下式给出\n$$\ny=\\begin{pmatrix}1 \\\\ 0\\end{pmatrix}, \\quad x_{1}=\\begin{pmatrix}1 \\\\ 0\\end{pmatrix}, \\quad x_{2}=\\begin{pmatrix}2 \\\\ 1\\end{pmatrix}, \\quad X=\\begin{pmatrix}1 & 2 \\\\ 0 & 1\\end{pmatrix}.\n$$\n数据生成过程为 $y=x_{1}$ (无噪声)。考虑两种正则化估计量：\n- 最小绝对收缩和选择算子 (LASSO) 估计量 $\\hat{\\beta}^{\\text{LASSO}}(\\lambda)$，定义为下式的任意一个极小化子\n$$\n\\min_{\\beta \\in \\mathbb{R}^{2}} \\; \\frac{1}{2}\\|y - X\\beta\\|_{2}^{2} + \\lambda \\|\\beta\\|_{1},\n$$\n惩罚参数为 $\\lambda=\\frac{6}{5}$。\n- 岭回归估计量 $\\hat{\\beta}^{\\text{Ridge}}(\\alpha)$，定义为下式的唯一极小化子\n$$\n\\min_{\\beta \\in \\mathbb{R}^{2}} \\; \\frac{1}{2}\\|y - X\\beta\\|_{2}^{2} + \\frac{\\alpha}{2} \\|\\beta\\|_{2}^{2},\n$$\n惩罚参数为 $\\alpha=\\frac{1}{2}$。\n\n精确计算这两个系数向量，并以单一行向量 $\\big[\\hat{\\beta}_{1}^{\\text{LASSO}}, \\hat{\\beta}_{2}^{\\text{LASSO}}, \\hat{\\beta}_{1}^{\\text{Ridge}}, \\hat{\\beta}_{2}^{\\text{Ridge}}\\big]$ 的形式报告。请给出精确值，不要四舍五入。你的最终答案必须是此单一行向量。", "solution": "该问题要求计算两种正则化线性回归的系数向量：Ridge 估计量和 LASSO 估计量。问题定义明确，科学上合理，且所有必要数据均已提供。我们将分别求解每种估计量。\n\n给定数据如下：\n响应向量 $y = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$。\n预测变量矩阵 $X = \\begin{pmatrix} 1 & 2 \\\\ 0 & 1 \\end{pmatrix}$。\n系数向量为 $\\beta = \\begin{pmatrix} \\beta_1 \\\\ \\beta_2 \\end{pmatrix}$。\n\n首先，我们计算一些必要的矩阵乘积。\n$X$ 的转置是 $X^T = \\begin{pmatrix} 1 & 0 \\\\ 2 & 1 \\end{pmatrix}$。\n格拉姆矩阵是 $X^T X = \\begin{pmatrix} 1 & 0 \\\\ 2 & 1 \\end{pmatrix} \\begin{pmatrix} 1 & 2 \\\\ 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 1 & 2 \\\\ 2 & 5 \\end{pmatrix}$。\n乘积 $X^T y$ 是 $X^T y = \\begin{pmatrix} 1 & 0 \\\\ 2 & 1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}$。\n\n**1. 岭回归估计量**\n\n岭回归的目标函数由下式给出：\n$$L_{\\text{Ridge}}(\\beta) = \\frac{1}{2}\\|y - X\\beta\\|_{2}^{2} + \\frac{\\alpha}{2} \\|\\beta\\|_{2}^{2}$$\n惩罚参数为 $\\alpha = \\frac{1}{2}$。\n这个目标函数是严格凸且可微的。通过将其关于 $\\beta$ 的梯度设为零，可以找到极小化子 $\\hat{\\beta}^{\\text{Ridge}}$：\n$$\\nabla_{\\beta} L_{\\text{Ridge}} = -X^T(y - X\\beta) + \\alpha\\beta = 0$$\n整理各项可得岭回归的正规方程组：\n$$(X^T X + \\alpha I) \\beta = X^T y$$\n其中 $I$ 是 $2 \\times 2$ 的单位矩阵。因此解为：\n$$\\hat{\\beta}^{\\text{Ridge}} = (X^T X + \\alpha I)^{-1} X^T y$$\n我们代入给定的值：\n$$X^T X + \\alpha I = \\begin{pmatrix} 1 & 2 \\\\ 2 & 5 \\end{pmatrix} + \\frac{1}{2} \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 1 + \\frac{1}{2} & 2 \\\\ 2 & 5 + \\frac{1}{2} \\end{pmatrix} = \\begin{pmatrix} \\frac{3}{2} & 2 \\\\ 2 & \\frac{11}{2} \\end{pmatrix}$$\n为求该矩阵的逆，我们首先计算其行列式：\n$$\\det(X^T X + \\alpha I) = \\left(\\frac{3}{2}\\right)\\left(\\frac{11}{2}\\right) - (2)(2) = \\frac{33}{4} - 4 = \\frac{33 - 16}{4} = \\frac{17}{4}$$\n逆矩阵为：\n$$(X^T X + \\alpha I)^{-1} = \\frac{1}{\\frac{17}{4}} \\begin{pmatrix} \\frac{11}{2} & -2 \\\\ -2 & \\frac{3}{2} \\end{pmatrix} = \\frac{4}{17} \\begin{pmatrix} \\frac{11}{2} & -2 \\\\ -2 & \\frac{3}{2} \\end{pmatrix} = \\begin{pmatrix} \\frac{22}{17} & -\\frac{8}{17} \\\\ -\\frac{8}{17} & \\frac{6}{17} \\end{pmatrix}$$\n现在我们可以计算岭回归估计量：\n$$\\hat{\\beta}^{\\text{Ridge}} = \\begin{pmatrix} \\frac{22}{17} & -\\frac{8}{17} \\\\ -\\frac{8}{17} & \\frac{6}{17} \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix} = \\begin{pmatrix} \\frac{22}{17} - \\frac{16}{17} \\\\ -\\frac{8}{17} + \\frac{12}{17} \\end{pmatrix} = \\begin{pmatrix} \\frac{6}{17} \\\\ \\frac{4}{17} \\end{pmatrix}$$\n因此，$\\hat{\\beta}_{1}^{\\text{Ridge}} = \\frac{6}{17}$ 且 $\\hat{\\beta}_{2}^{\\text{Ridge}} = \\frac{4}{17}$。\n\n**2. LASSO 估计量**\n\nLASSO 的目标函数为：\n$$L_{\\text{LASSO}}(\\beta) = \\frac{1}{2}\\|y - X\\beta\\|_{2}^{2} + \\lambda \\|\\beta\\|_{1}$$\n惩罚参数为 $\\lambda = \\frac{6}{5}$。由于 $L_1$-范数的不可微性，我们使用次梯度最优性条件。极小化子 $\\hat{\\beta}^{\\text{LASSO}}$ 必须满足 $0 \\in \\partial L_{\\text{LASSO}}(\\hat{\\beta})$。\n次梯度条件为对每个分量 $j \\in \\{1, 2\\}$，有 $X^T(y - X\\hat{\\beta})_j \\in \\lambda \\cdot \\partial |\\beta_j| |_{\\hat{\\beta}_j}$。这可以写成：\n$$\n\\begin{cases}\n(X^T(y - X\\hat{\\beta}))_j = \\lambda \\cdot \\text{sign}(\\hat{\\beta}_j) & \\text{if } \\hat{\\beta}_j \\neq 0 \\\\\n|(X^T(y - X\\hat{\\beta}))_j| \\le \\lambda & \\text{if } \\hat{\\beta}_j = 0\n\\end{cases}\n$$\n让我们计算与残差相关的向量 $c(\\beta) = X^T(y - X\\beta)$：\n$$c(\\beta) = X^T y - X^T X \\beta = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix} - \\begin{pmatrix} 1 & 2 \\\\ 2 & 5 \\end{pmatrix}\\begin{pmatrix} \\beta_1 \\\\ \\beta_2 \\end{pmatrix} = \\begin{pmatrix} 1 - \\beta_1 - 2\\beta_2 \\\\ 2 - 2\\beta_1 - 5\\beta_2 \\end{pmatrix}$$\n我们分析系数活跃集的可能情况。\n\n情况 1：$\\hat{\\beta}_1 = 0$，$\\hat{\\beta}_2 \\ne 0$。\n条件为 $|c_1(\\hat{\\beta})| \\le \\lambda$ 和 $c_2(\\hat{\\beta}) = \\lambda \\cdot \\text{sign}(\\hat{\\beta}_2)$。\n当 $\\hat\\beta_1 = 0$ 时，我们有 $c_2 = 2 - 5\\hat{\\beta}_2$。\n如果 $\\hat{\\beta}_2 > 0$：$2 - 5\\hat{\\beta}_2 = \\lambda = \\frac{6}{5} \\implies 5\\hat{\\beta}_2 = 2 - \\frac{6}{5} = \\frac{4}{5} \\implies \\hat{\\beta}_2 = \\frac{4}{25}$。这与 $\\hat{\\beta}_2 > 0$ 一致。\n我们必须检查 $\\hat{\\beta}_1=0$ 的条件：$|c_1| \\le \\lambda$。\n$|c_1| = |1 - 0 - 2\\hat{\\beta}_2| = |1 - 2(\\frac{4}{25})| = |1 - \\frac{8}{25}| = |\\frac{17}{25}| = \\frac{17}{25}$。\n条件是 $\\frac{17}{25} \\le \\frac{6}{5}$。由于 $\\frac{6}{5} = \\frac{30}{25}$，我们有 $\\frac{17}{25} \\le \\frac{30}{25}$，此式成立。\n所以，$\\hat{\\beta}^{\\text{LASSO}} = \\begin{pmatrix} 0 \\\\ \\frac{4}{25} \\end{pmatrix}$ 是一个有效的解。\n\n如果 $\\hat{\\beta}_2 < 0$：$2 - 5\\hat{\\beta}_2 = -\\lambda = -\\frac{6}{5} \\implies 5\\hat{\\beta}_2 = 2 + \\frac{6}{5} = \\frac{16}{5} \\implies \\hat{\\beta}_2 = \\frac{16}{25}$。这与假设 $\\hat{\\beta}_2 < 0$ 矛盾。\n\n情况 2：$\\hat{\\beta}_1 \\ne 0$，$\\hat{\\beta}_2 = 0$。\n条件为 $c_1(\\hat{\\beta}) = \\lambda \\cdot \\text{sign}(\\hat{\\beta}_1)$ 和 $|c_2(\\hat{\\beta})| \\le \\lambda$。\n当 $\\hat\\beta_2 = 0$ 时，我们有 $c_1 = 1 - \\hat\\beta_1$。\n如果 $\\hat{\\beta}_1 > 0$：$1 - \\hat{\\beta}_1 = \\lambda = \\frac{6}{5} \\implies \\hat{\\beta}_1 = 1 - \\frac{6}{5} = -\\frac{1}{5}$。矛盾。\n如果 $\\hat{\\beta}_1 < 0$：$1 - \\hat{\\beta}_1 = -\\lambda = -\\frac{6}{5} \\implies \\hat{\\beta}_1 = 1 + \\frac{6}{5} = \\frac{11}{5}$。矛盾。\n这种情况无解。\n\n情况 3：$\\hat{\\beta}_1 \\ne 0$，$\\hat{\\beta}_2 \\ne 0$。\n这要求解方程组 $c_j(\\hat{\\beta}) = \\lambda \\cdot \\text{sign}(\\hat{\\beta}_j)$，其中 $j=1,2$。\n1) $\\hat{\\beta}_1 > 0, \\hat{\\beta}_2 > 0$：$\\hat{\\beta}_1 + 2\\hat{\\beta}_2 = 1 - \\frac{6}{5} = -\\frac{1}{5}$。两个正数之和不能为负。矛盾。\n2) $\\hat{\\beta}_1 > 0, \\hat{\\beta}_2 < 0$：$\\hat{\\beta}_1 + 2\\hat{\\beta}_2 = 1 - \\frac{6}{5} = -\\frac{1}{5}$ 和 $2\\hat{\\beta}_1 + 5\\hat{\\beta}_2 = 2 - (-\\frac{6}{5}) = \\frac{16}{5}$。解此方程组得 $\\hat{\\beta}_2 = \\frac{18}{5}$，这与 $\\hat{\\beta}_2 < 0$ 矛盾。\n3) $\\hat{\\beta}_1 < 0, \\hat{\\beta}_2 > 0$：$\\hat{\\beta}_1 + 2\\hat{\\beta}_2 = 1 - (-\\frac{6}{5}) = \\frac{11}{5}$ 和 $2\\hat{\\beta}_1 + 5\\hat{\\beta}_2 = 2 - \\frac{6}{5} = \\frac{4}{5}$。解此方程组得 $\\hat{\\beta}_2 = -\\frac{18}{5}$，这与 $\\hat{\\beta}_2 > 0$ 矛盾。\n4) $\\hat{\\beta}_1 < 0, \\hat{\\beta}_2 < 0$：$\\hat{\\beta}_1 + 2\\hat{\\beta}_2 = 1 - (-\\frac{6}{5}) = \\frac{11}{5}$。两个负数之和不能为正。矛盾。\n不存在两个系数都非零的解。\n\n由于 LASSO 目标函数是严格凸的（因为 $X$ 是满秩的），所以必须存在唯一的极小化子。我们对所有可能的活跃集的分析只得出一个有效的解。\n因此，LASSO 估计量为 $\\hat{\\beta}^{\\text{LASSO}} = \\begin{pmatrix} 0 \\\\ \\frac{4}{25} \\end{pmatrix}$。\n所以，$\\hat{\\beta}_{1}^{\\text{LASSO}} = 0$ 且 $\\hat{\\beta}_{2}^{\\text{LASSO}} = \\frac{4}{25}$。\n\n**最终答案组合**\n问题要求最终答案为单一行向量 $\\big[\\hat{\\beta}_{1}^{\\text{LASSO}}, \\hat{\\beta}_{2}^{\\text{LASSO}}, \\hat{\\beta}_{1}^{\\text{Ridge}}, \\hat{\\beta}_{2}^{\\text{Ridge}}\\big]$。\n代入计算出的值：\n$$\\begin{pmatrix} 0 & \\frac{4}{25} & \\frac{6}{17} & \\frac{4}{17} \\end{pmatrix}$$\n这就是最终结果。", "answer": "$$\n\\boxed{\\begin{pmatrix} 0 & \\frac{4}{25} & \\frac{6}{17} & \\frac{4}{17} \\end{pmatrix}}\n$$", "id": "2426291"}, {"introduction": "在相关性概念的基础上，这个练习 [@problem_id:2426293] 提出了一个关于极端情况——完全多重共线性——的思想实验。您将分析当一个预测变量被完全复制时，岭回归和 LASSO 的解会发生什么变化。此练习对于理解正则化方法如何在普通最小二乘法（OLS）会失效的情况下，依然能确保解的唯一性和稳定性至关重要，深刻揭示了惩罚项在处理病态问题时的数学特性。", "problem": "考虑一个计算金融中的线性预测模型，其结果变量为 $y \\in \\mathbb{R}^{n}$（例如，超额市场回报），使用预测变量矩阵 $X \\in \\mathbb{R}^{n \\times p}$ 和系数向量 $\\beta \\in \\mathbb{R}^{p}$。假设将一列 $x_{j} \\in \\mathbb{R}^{n}$ 复制并附加到 $X$ 上，形成一个增广矩阵 $\\tilde{X} \\in \\mathbb{R}^{n \\times (p+1)}$，其中第 $j$ 个预测变量以两个相同的列 $x_{j}^{(1)} = x_{j}$ 和 $x_{j}^{(2)} = x_{j}$ 出现两次。设 $\\lambda > 0$ 是一个固定值。考虑最小化 $L_{2}$ 正则化最小二乘的岭惩罚（ridge-penalized）估计量，以及最小化 $L_{1}$ 正则化最小二乘的最小绝对收缩和选择算子（LASSO）估计量，每种估计量都分别使用原始矩阵 $X$ 和增广矩阵 $\\tilde{X}$ 进行一次拟合。\n\n哪个陈述最准确地描述了当添加 $x_{j}$ 的相同副本时，解会发生什么变化？\n\nA. 对于岭回归（Ridge），最优解仍然是唯一的，并为重复列分配相等的系数；在重复方向上的有效收缩减小，因此拟合值通常相对于没有重复的模型发生变化。对于 LASSO，最优系数向量集在重复坐标上变得不唯一，但拟合值和 $x_{j}$ 的总系数效应（重复项之和）与没有重复的模型中的相同。\n\nB. 岭回归（Ridge）和 LASSO 都产生唯一的系数向量，并且由于重复，拟合值保持不变；系数可能会在重复列之间重新缩放，但预测值与没有重复的模型中的相同。\n\nC. 岭回归（Ridge）因为设计矩阵变为奇异而失去系数的唯一性，但拟合值不变；LASSO 由于 $L_{1}$ 惩罚而保持唯一。\n\nD. LASSO 对重复的预测变量施加更重的惩罚，因此比没有重复的模型更多地收缩重复系数之和，从而改变了拟合值；岭回归（Ridge）不受重复的影响，因为 $L_{2}$-惩罚使得最小化器对相同的列具有不变性。", "solution": "首先必须验证问题陈述的科学和逻辑完整性。\n\n**步骤1：提取已知条件**\n- 一个线性预测模型将结果变量 $y \\in \\mathbb{R}^{n}$ 与预测变量矩阵 $X \\in \\mathbb{R}^{n \\times p}$ 通过系数向量 $\\beta \\in \\mathbb{R}^{p}$ 相关联。\n- 一个增广矩阵 $\\tilde{X} \\in \\mathbb{R}^{n \\times (p+1)}$ 是通过复制 $X$ 的一列 $x_j \\in \\mathbb{R}^{n}$ 形成的。\n- 复制的列是相同的：$x_{j}^{(1)} = x_{j}$ 和 $x_{j}^{(2)} = x_{j}$。\n- 使用一个固定的正则化参数 $\\lambda > 0$。\n- 考虑两种估计量：\n    1. 岭回归（Ridge regression），它最小化 $L_2$ 正则化最小二乘目标函数。\n    2. LASSO，它最小化 $L_1$ 正则化最小二乘目标函数。\n- 每种估计量都应用于原始问题（使用矩阵 $X$）和增广问题（使用矩阵 $\\tilde{X}$）。\n- 问题要求描述这种复制对解（系数和拟合值）的影响。\n\n**步骤2：使用提取的已知条件进行验证**\n该问题在统计学习和计算金融领域内是良定义的。它提出了一个关于两种标准正则化方法——岭回归和 LASSO 回归——在面对完全多重共线性（高维数据中的常见问题）时的基本性质的问题。\n\n- **科学依据：** 该问题基于岭回归和 LASSO 回归已建立的数学理论。所有概念都是统计学和计量经济学中的标准概念。前提是合理的。\n- **良定义性：** 该问题提供了推导估计量行为所需的所有信息。对于固定的 $\\lambda > 0$，岭回归和 LASSO 的解都存在。关于这些解的唯一性和特征的问题是分析的核心部分，而不是问题陈述中的缺陷。\n- **客观性：** 该问题以精确的数学语言陈述，没有主观性或歧义。\n\n问题陈述内部一致，科学有效，并具有清晰的结构，可以从中推导出明确、可验证的答案。\n\n**步骤3：结论和行动**\n问题有效。可以进行解的严格推导。\n\n**解的推导**\n\n设原始设计矩阵为 $X = [x_1, \\dots, x_j, \\dots, x_p]$。不失一般性，增广矩阵 $\\tilde{X}$ 可以通过在原始列旁边插入 $x_j$ 的副本来写成 $\\tilde{X} = [x_1, \\dots, x_j, x_j, \\dots, x_p]$。相应的系数向量是 $\\tilde{\\beta} \\in \\mathbb{R}^{p+1}$，我们可以将其划分为 $(\\beta_1, \\dots, \\beta_{j-1}, \\beta_{j,1}, \\beta_{j,2}, \\beta_{j+1}, \\dots, \\beta_p)$。\n\n**岭回归（Ridge Regression）分析**\n\n原始问题的岭回归估计量 $\\hat{\\beta}_{\\text{ridge}}$ 最小化目标函数：\n$$ J(\\beta) = \\|y - X\\beta\\|_2^2 + \\lambda \\|\\beta\\|_2^2 $$\n对于 $\\lambda > 0$，解是唯一的，由 $\\hat{\\beta}_{\\text{ridge}} = (X^T X + \\lambda I_p)^{-1} X^T y$ 给出。\n\n对于增广问题，岭回归估计量 $\\tilde{\\hat{\\beta}}_{\\text{ridge}}$ 最小化：\n$$ \\tilde{J}(\\tilde{\\beta}) = \\|y - \\tilde{X}\\tilde{\\beta}\\|_2^2 + \\lambda \\|\\tilde{\\beta}\\|_2^2 $$\n该目标函数的Hessian矩阵是 $2(\\tilde{X}^T\\tilde{X} + \\lambda I_{p+1})$。由于 $\\tilde{X}^T\\tilde{X}$ 是半正定的，且对于 $\\lambda > 0$，$ \\lambda I_{p+1}$ 是正定的，因此它们的和是正定的。这保证了 $\\tilde{J}(\\tilde{\\beta})$ 是一个严格凸函数，并有一个唯一的最小化子 $\\tilde{\\hat{\\beta}}_{\\text{ridge}}$。\n\n让我们检查解的结构。涉及重复列的预测项是 $x_j \\beta_{j,1} + x_j \\beta_{j,2} = x_j(\\beta_{j,1} + \\beta_{j,2})$。涉及这些系数的惩罚项是 $\\lambda(\\beta_{j,1}^2 + \\beta_{j,2}^2)$。\n考虑在固定总和 $S = \\beta_{j,1} + \\beta_{j,2}$ 的情况下最小化惩罚部分 $\\beta_{j,1}^2 + \\beta_{j,2}^2$。通过标准的优化论证（例如，拉格朗日乘子法），当 $\\beta_{j,1} = \\beta_{j,2} = S/2$ 时达到最小值。因此，唯一解必须对相同的列具有相等的系数。\n\n现在，让我们将增广问题与原始问题进行比较。设增广模型中第 $j$ 个预测变量的总系数为 $\\beta_j^* = \\beta_{j,1} + \\beta_{j,2}$。由于 $\\beta_{j,1} = \\beta_{j,2}$，我们得到 $\\beta_j^* = 2\\beta_{j,1}$，惩罚项变为 $\\lambda(\\beta_{j,1}^2 + \\beta_{j,2}^2) = \\lambda( (\\beta_j^*/2)^2 + (\\beta_j^*/2)^2 ) = \\lambda \\frac{(\\beta_j^*)^2}{2}$。\n因此，增广问题等价于最小化：\n$$ \\|y - X\\beta^*\\|_2^2 + \\lambda \\left( \\sum_{k \\neq j} (\\beta_k^*)^2 + \\frac{1}{2}(\\beta_j^*)^2 \\right) $$\n这是一个加权岭回归问题，其中对第 $j$ 个系数的惩罚实际上减半了。由于目标函数已改变，解向量 $\\beta^*$ 通常将不同于原始的 $\\hat{\\beta}_{\\text{ridge}}$。因此，拟合值 $\\hat{y} = X\\beta^*$ 通常也会改变。\n\n**LASSO分析**\n\n原始问题的 LASSO 估计量 $\\hat{\\beta}_{\\text{lasso}}$ 最小化：\n$$ L(\\beta) = \\|y - X\\beta\\|_2^2 + \\lambda \\|\\beta\\|_1 $$\n对于增广问题，LASSO 估计量 $\\tilde{\\hat{\\beta}}_{\\text{lasso}}$ 最小化：\n$$ \\tilde{L}(\\tilde{\\beta}) = \\|y - \\tilde{X}\\tilde{\\beta}\\|_2^2 + \\lambda \\|\\tilde{\\beta}\\|_1 $$\n预测项是 $\\tilde{X}\\tilde{\\beta} = X\\beta^*$，其中 $\\beta_k^* = \\tilde{\\beta}_k$ 对于 $k \\neq j$，且 $\\beta_j^* = \\tilde{\\beta}_{j,1} + \\tilde{\\beta}_{j,2}$。惩罚项是 $\\lambda(\\sum_{k \\neq j} |\\tilde{\\beta}_k| + |\\tilde{\\beta}_{j,1}| + |\\tilde{\\beta}_{j,2}|)$。\n设 $\\hat{\\beta}$ 是原始 LASSO 问题的最优解。增广问题的任何最优解 $\\tilde{\\hat{\\beta}}$ 必须产生相同或更低的目标函数值。\n让我们从 $\\hat{\\beta}$ 构建一个候选解 $\\tilde{\\beta}$。我们为不属于 $\\{j_1, j_2\\}$（重复列的索引）的 $k$ 设定 $\\tilde{\\beta}_k = \\hat{\\beta}_k$。我们需要选择 $\\tilde{\\beta}_{j,1}$ 和 $\\tilde{\\beta}_{j,2}$ 以使得 $\\tilde{\\beta}_{j,1} + \\tilde{\\beta}_{j,2} = \\hat{\\beta}_j$。通过这种选择，残差平方和项是相同的：$\\|y - \\tilde{X}\\tilde{\\beta}\\|_2^2=\\|y - X\\hat{\\beta}\\|_2^2$。\n惩罚项变为 $\\lambda(\\sum_{k \\neq j} |\\hat{\\beta}_k| + |\\tilde{\\beta}_{j,1}| + |\\tilde{\\beta}_{j,2}|)$。为了与原始问题的目标函数值匹配，我们需要 $|\\tilde{\\beta}_{j,1}| + |\\tilde{\\beta}_{j,2}| = |\\hat{\\beta}_j|$。\n根据三角不等式， $|\\tilde{\\beta}_{j,1}| + |\\tilde{\\beta}_{j,2}| \\ge |\\tilde{\\beta}_{j,1} + \\tilde{\\beta}_{j,2}| = |\\hat{\\beta}_j|$。等式成立的充要条件是 $\\tilde{\\beta}_{j,1}$ 和 $\\tilde{\\beta}_{j,2}$ 具有相同的符号（或至少有一个为零）。\n\n因此，任何满足以下条件的对 $(\\tilde{\\beta}_{j,1}, \\tilde{\\beta}_{j,2})$：\n1. $\\tilde{\\beta}_{j,1} + \\tilde{\\beta}_{j,2} = \\hat{\\beta}_j$\n2. $\\text{sign}(\\tilde{\\beta}_{j,1}) = \\text{sign}(\\tilde{\\beta}_{j,2}) = \\text{sign}(\\hat{\\beta}_j)$ (如果 $\\hat{\\beta}_j \\neq 0$)\n都将是增广问题的最优解。例如，如果 $\\hat{\\beta}_j > 0$，将 $\\hat{\\beta}_j$ 分配到两个非负部分 $\\tilde{\\beta}_{j,1} = \\alpha \\hat{\\beta}_j$ 和 $\\tilde{\\beta}_{j,2} = (1-\\alpha)\\hat{\\beta}_j$（对于 $\\alpha \\in [0, 1]$）的任何分布都是有效的。\n这表明最优系数向量集 $\\tilde{\\hat{\\beta}}$ 不再是唯一的。然而，第 $j$ 个预测变量的总效应 $\\tilde{\\beta}_{j,1} + \\tilde{\\beta}_{j,2}$ 是唯一确定的，并且等于 $\\hat{\\beta}_j$。因此，拟合值也保持不变：\n$\\tilde{\\hat{y}}_{\\text{lasso}} = \\tilde{X}\\tilde{\\hat{\\beta}} = X\\hat{\\beta} = \\hat{y}_{\\text{lasso}}$。\n\n**逐项分析**\n\nA. **对于岭回归，最优解仍然是唯一的，并为重复列分配相等的系数；在重复方向上的有效收缩减小，因此拟合值通常相对于没有重复的模型发生变化。对于 LASSO，最优系数向量集在重复坐标上变得不唯一，但拟合值和 $x_{j}$ 的总系数效应（重复项之和）与没有重复的模型中的相同。**\n该陈述与上述推导完全一致。对于岭回归，解是唯一的，重复项的系数相等，有效惩罚的变化改变了拟合值。对于 LASSO，重复项的单个系数不是唯一的，但它们的和以及拟合值是不变的。\n结论：**正确**。\n\nB. **岭回归（Ridge）和 LASSO 都产生唯一的系数向量，并且由于重复，拟合值保持不变；系数可能会在重复列之间重新缩放，但预测值与没有重复的模型中的相同。**\n这是不正确的。岭回归的拟合值会改变，而 LASSO 的系数向量不是唯一的。\n结论：**不正确**。\n\nC. **岭回归（Ridge）因为设计矩阵变为奇异而失去系数的唯一性，但拟合值不变；LASSO 由于 $L_{1}$-惩罚而保持唯一。**\n这是不正确的。由于 $L_2$ 惩罚项确保了严格凸性，岭回归的解仍然是唯一的。其拟合值确实会改变。LASSO 的系数解变得不唯一。\n结论：**不正确**。\n\nD. **LASSO 对重复的预测变量施加更重的惩罚，因此比没有重复的模型更多地收缩重复系数之和，从而改变了拟合值；岭回归（Ridge）不受重复的影响，因为 $L_{2}$-惩罚使得最小化器对相同的列具有不变性。**\n这是不正确的。LASSO 对总效应的惩罚保持不变，因此拟合值也保持不变。岭回归受到重复的影响，因为对重复预测变量的有效惩罚减少了。\n结论：**不正确**。", "answer": "$$\\boxed{A}$$", "id": "2426293"}, {"introduction": "为了完成我们对正则化的深入探讨，我们将研究当惩罚项变得极其大时的极限行为。这个练习 [@problem_id:2426322] 要求您从第一性原理出发，证明当惩罚参数 $\\lambda$ 趋于无穷大时，岭回归和 LASSO 的斜率系数都将被驱动至零。理解这一极限情况有助于将正则化视为一个连续的光谱，其一端是复杂的无惩罚模型，另一端则是最简单的基准模型，从而从根本上理解惩罚项的作用。", "problem": "一位计量经济学家将一项资产的超额回报建模为在 $n$ 个时间周期内观察到的 $p$ 个预测变量的线性函数。令响应向量为 $y \\in \\mathbb{R}^{n}$，预测变量矩阵为 $X \\in \\mathbb{R}^{n \\times p}$，其中第 $i$ 行为 $x_{i}^{\\top} \\in \\mathbb{R}^{p}$。考虑带有截距项 $ \\beta_{0} \\in \\mathbb{R} $ 和斜率向量 $\\beta \\in \\mathbb{R}^{p}$ 的线性模型，其中截距项不被惩罚。对于给定的惩罚水平 $\\lambda \\geq 0$，将岭回归 (Ridge) 目标函数定义为\n$$\nQ_{\\lambda}^{\\mathrm{R}}(\\beta_{0},\\beta) \\equiv \\frac{1}{n}\\sum_{i=1}^{n}\\left(y_{i}-\\beta_{0}-x_{i}^{\\top}\\beta\\right)^{2}+\\lambda \\|\\beta\\|_{2}^{2},\n$$\n并将最小绝对值收缩和选择算子 (LASSO) 目标函数定义为\n$$\nQ_{\\lambda}^{\\mathrm{L}}(\\beta_{0},\\beta) \\equiv \\frac{1}{n}\\sum_{i=1}^{n}\\left(y_{i}-\\beta_{0}-x_{i}^{\\top}\\beta\\right)^{2}+\\lambda \\|\\beta\\|_{1}.\n$$\n对于每个 $\\lambda \\geq 0$，令 $(\\beta_{0}^{\\mathrm{R}}(\\lambda),\\beta^{\\mathrm{R}}(\\lambda))$ 和 $(\\beta_{0}^{\\mathrm{L}}(\\lambda),\\beta^{\\mathrm{L}}(\\lambda))$ 分别为 $Q_{\\lambda}^{\\mathrm{R}}$ 和 $Q_{\\lambda}^{\\mathrm{L}}$ 的最小值点。样本均值表示为\n$$\n\\bar{y} \\equiv \\frac{1}{n}\\sum_{i=1}^{n} y_{i}\n\\quad\\text{和}\\quad\n\\bar{x} \\equiv \\frac{1}{n}\\sum_{i=1}^{n} x_{i}.\n$$\n仅假设 $n \\geq 1$ 和 $p \\geq 1$，且在两个目标函数中截距项都不受惩罚。\n\n从基本定义出发证明，当 $\\lambda \\to \\infty$ 时，$\\beta^{\\mathrm{R}}(\\lambda)$ 和 $\\beta^{\\mathrm{L}}(\\lambda)$ 均收敛于 $\\mathbb{R}^{p}$ 中的零向量。然后，确定当 $\\lambda \\to \\infty$ 时截距参数 $\\beta_{0}^{\\mathrm{R}}(\\lambda)$ (等价于 $\\beta_{0}^{\\mathrm{L}}(\\lambda)$) 的精确极限的闭式解，该解仅用 $\\bar{y}$ 和 $\\bar{x}$ 表示。\n\n您的最终答案应为给出当 $\\lambda \\to \\infty$ 时截距极限的单一解析表达式。不需要进行数值近似。", "solution": "该问题是良定的，有科学依据，并包含了进行严谨数学推导所需的所有信息。因此，该问题被视为有效。我们将按问题陈述的要求，分两部分进行解答。首先，我们将证明当惩罚参数 $\\lambda$ 趋向于无穷大时，岭回归和 LASSO 回归的斜率向量均收敛于零向量。其次，我们将确定在相同条件下截距参数的极限。\n\n令一般目标函数表示为 $Q_{\\lambda}(\\beta_{0}, \\beta)$，它代表岭回归目标函数 $Q_{\\lambda}^{\\mathrm{R}}$ 或 LASSO 目标函数 $Q_{\\lambda}^{\\mathrm{L}}$。它可以写成：\n$$\nQ_{\\lambda}(\\beta_{0}, \\beta) = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\beta_0 - x_i^\\top \\beta)^2 + \\lambda P(\\beta)\n$$\n其中，对于岭回归，$P(\\beta) = \\|\\beta\\|_{2}^{2}$；对于 LASSO 回归，$P(\\beta) = \\|\\beta\\|_{1}$。令 $(\\beta_0(\\lambda), \\beta(\\lambda))$ 表示在给定 $\\lambda \\geq 0$ 时使该函数最小化的参数对 $(\\beta_0, \\beta)$。\n\n首先，我们证明 $\\lim_{\\lambda \\to \\infty} \\beta(\\lambda) = \\mathbf{0}$。\n根据最小值点的定义，对于任何其他参数对 $(\\beta_0', \\beta')$，以下不等式成立：\n$$\nQ_{\\lambda}(\\beta_0(\\lambda), \\beta(\\lambda)) \\leq Q_{\\lambda}(\\beta_0', \\beta')\n$$\n我们选择一个便于比较的特定点：$(\\beta_0', \\beta') = (\\bar{y}, \\mathbf{0})$，其中 $\\mathbf{0}$ 是 $\\mathbb{R}^p$ 中的零向量。目标函数在该点的值为：\n$$\nQ_{\\lambda}(\\bar{y}, \\mathbf{0}) = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\bar{y} - x_i^\\top \\mathbf{0})^2 + \\lambda P(\\mathbf{0})\n$$\n对于岭回归和 LASSO 的惩罚项，都有 $P(\\mathbf{0}) = 0$。因此，表达式简化为：\n$$\nQ_{\\lambda}(\\bar{y}, \\mathbf{0}) = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\bar{y})^2\n$$\n这个量是一个有限常数，仅取决于数据 $y$ 而不依赖于 $\\lambda$。我们用 $C$ 表示这个常数。从最小值点的定义得出的不等式变为：\n$$\nQ_{\\lambda}(\\beta_0(\\lambda), \\beta(\\lambda)) = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\beta_0(\\lambda) - x_i^\\top \\beta(\\lambda))^2 + \\lambda P(\\beta(\\lambda)) \\leq C\n$$\n第一项，即均方误差，必然是非负的。因此，我们可以为惩罚项推导出一个不等式：\n$$\n\\lambda P(\\beta(\\lambda)) \\leq C\n$$\n对于 $\\lambda > 0$，我们可以写成：\n$$\nP(\\beta(\\lambda)) \\leq \\frac{C}{\\lambda}\n$$\n当 $\\lambda \\to \\infty$ 时，右侧的 $\\frac{C}{\\lambda}$ 趋近于 $0$。由于惩罚函数 $P(\\beta)$ 总是非负的，根据夹逼定理 (Squeeze Theorem)，我们必有 $\\lim_{\\lambda \\to \\infty} P(\\beta(\\lambda)) = 0$。\n\n对于岭回归，$P(\\beta) = \\|\\beta\\|_2^2 = \\sum_{j=1}^{p} (\\beta_j)^2$。条件 $\\lim_{\\lambda \\to \\infty} \\|\\beta^{\\mathrm{R}}(\\lambda)\\|_2^2 = 0$ 意味着向量的每个分量都必须趋于零。因此，$\\lim_{\\lambda \\to \\infty} \\beta^{\\mathrm{R}}(\\lambda) = \\mathbf{0}$。\n对于 LASSO 回归，$P(\\beta) = \\|\\beta\\|_1 = \\sum_{j=1}^{p} |\\beta_j|$。条件 $\\lim_{\\lambda \\to \\infty} \\|\\beta^{\\mathrm{L}}(\\lambda)\\|_1 = 0$ 也意味着每个分量都必须趋于零。因此，$\\lim_{\\lambda \\to \\infty} \\beta^{\\mathrm{L}}(\\lambda) = \\mathbf{0}$。\n这就完成了问题第一部分的证明。\n\n其次，我们确定当 $\\lambda \\to \\infty$ 时截距参数 $\\beta_0(\\lambda)$ 的极限。\n对于任何固定的 $\\lambda \\geq 0$，对于给定的斜率向量 $\\beta$，最优截距 $\\beta_0(\\lambda)$ 是通过对 $\\beta_0$ 最小化目标函数来找到的。由于惩罚项不依赖于 $\\beta_0$，我们只需要对均方误差项求导。\n$$\n\\frac{\\partial Q_{\\lambda}}{\\partial \\beta_0} = \\frac{\\partial}{\\partial \\beta_0} \\left[ \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\beta_0 - x_i^\\top \\beta)^2 \\right] = \\frac{1}{n} \\sum_{i=1}^{n} 2(y_i - \\beta_0 - x_i^\\top \\beta)(-1)\n$$\n将此偏导数设为零，以找到与最优 $\\beta(\\lambda)$ 对应的最优 $\\beta_0(\\lambda)$：\n$$\n-\\frac{2}{n} \\sum_{i=1}^{n} (y_i - \\beta_0(\\lambda) - x_i^\\top \\beta(\\lambda)) = 0\n$$\n$$\n\\sum_{i=1}^{n} y_i - \\sum_{i=1}^{n} \\beta_0(\\lambda) - \\sum_{i=1}^{n} x_i^\\top \\beta(\\lambda) = 0\n$$\n使用样本均值的定义，$\\bar{y} = \\frac{1}{n}\\sum_{i=1}^n y_i$ 和 $\\bar{x} = \\frac{1}{n}\\sum_{i=1}^n x_i$：\n$$\nn\\bar{y} - n\\beta_0(\\lambda) - \\left( \\sum_{i=1}^{n} x_i \\right)^\\top \\beta(\\lambda) = 0\n$$\n$$\nn\\bar{y} - n\\beta_0(\\lambda) - (n\\bar{x})^\\top \\beta(\\lambda) = 0\n$$\n由于 $n \\geq 1$，我们可以两边除以 $n$：\n$$\n\\bar{y} - \\beta_0(\\lambda) - \\bar{x}^\\top \\beta(\\lambda) = 0\n$$\n这为我们提供了对于任何 $\\lambda \\geq 0$ 的最优截距的精确表达式：\n$$\n\\beta_0(\\lambda) = \\bar{y} - \\bar{x}^\\top \\beta(\\lambda)\n$$\n这个关系对岭回归和 LASSO 估计量都有效，因为推导过程不依赖于惩罚项 $P(\\beta)$ 的形式。为了求 $\\lambda \\to \\infty$ 时的极限，我们将极限算子应用于此表达式：\n$$\n\\lim_{\\lambda \\to \\infty} \\beta_0(\\lambda) = \\lim_{\\lambda \\to \\infty} (\\bar{y} - \\bar{x}^\\top \\beta(\\lambda))\n$$\n由于 $\\bar{y}$ 和 $\\bar{x}$ 是关于 $\\lambda$ 的常数，且内积是连续函数，我们可以将极限移入表达式内部：\n$$\n\\lim_{\\lambda \\to \\infty} \\beta_0(\\lambda) = \\bar{y} - \\bar{x}^\\top \\left( \\lim_{\\lambda \\to \\infty} \\beta(\\lambda) \\right)\n$$\n在我们证明的第一部分，我们已经确定 $\\lim_{\\lambda \\to \\infty} \\beta(\\lambda) = \\mathbf{0}$。代入这个结果即可得到最终答案：\n$$\n\\lim_{\\lambda \\to \\infty} \\beta_0(\\lambda) = \\bar{y} - \\bar{x}^\\top \\mathbf{0} = \\bar{y}\n$$\n因此，当惩罚参数 $\\lambda$ 趋于无穷大时，岭回归和 LASSO 的斜率系数都被收缩到零，而截距项收敛于响应变量的样本均值 $\\bar{y}$。", "answer": "$$\\boxed{\\bar{y}}$$", "id": "2426322"}]}