{"hands_on_practices": [{"introduction": "我们的实践之旅始于构建一个完整的交易策略。一个成功的量化策略需要将市场洞察转化为明确的交易规则。本练习将指导您构建一个经典的反向交易策略，它利用期权市场中的看跌-看涨期权成交量比率（Put-Call Ratio）作为市场情绪的衡量指标 [@problem_id:2371402]。通过从头开始实施，您将掌握一个量化策略的完整生命周期：从计算指标、生成交易信号到考虑交易成本等现实因素并进行回测。", "problem": "给定一个离散时间金融市场，其中包含一个标的资产，以及每日期权市场的活动，这些活动由看涨和看跌期权的交易量汇总。对于由整数时间 $t \\in \\{0,1,\\dots,T-1\\}$ 索引的每个交易日，令 $C_t \\ge 0$ 表示看涨期权交易量， $P_t \\ge 0$ 表示看跌期权交易量，并令 $r_t$ 表示该资产在第 $t$ 天的简单回报率，以小数形式表示（例如，$r_t = 0.01$ 表示 $1$ 个百分点的增长，必须将其视为小数 $0.01$ 而不是百分比）。逆向情绪交易规则定义如下。首先，定义看涨-看跌期权成交量比率 $q_t$ 为\n$$\nq_t =\n\\begin{cases}\n\\dfrac{C_t}{P_t},  \\text{如果 } P_t  0, \\\\\n+\\infty,  \\text{如果 } P_t = 0 \\text{ 且 } C_t  0, \\\\\n0,  \\text{如果 } P_t = 0 \\text{ 且 } C_t = 0.\n\\end{cases}\n$$\n给定两个阈值 $t_\\ell$ 和 $t_h$，其中 $0  t_\\ell  t_h$，定义交易信号 $s_t$ 为\n$$\ns_t =\n\\begin{cases}\n+1,  \\text{如果 } q_t  t_\\ell, \\\\\n-1,  \\text{如果 } q_t  t_h, \\\\\n0,  \\text{否则。}\n\\end{cases}\n$$\n将 $s_t \\in \\{-1,0,+1\\}$ 解释为在第 $t$ 天收盘时决定的、适用于第二天的头寸：$+1$ 表示在标的资产中的完全投资多头头寸，$-1$ 表示在标的资产中的完全投资空头头寸，而 $0$ 表示无头寸。令 $c \\ge 0$ 为一个比例交易成本（以小数形式表示），每当在第 $t$ 天收盘时头寸发生变化时支付一次。假设初始头寸为 $s_{-1} = 0$。第 $t+1$ 天的单日利润为\n$$\np_{t+1} = s_t \\, r_{t+1} - c \\cdot \\mathbf{1}\\{s_t \\ne s_{t-1}\\},\n$$\n对于 $t = 0,1,\\dots,T-2$，其中 $\\mathbf{1}\\{\\cdot\\}$ 是指示函数。从第 $1$ 天到第 $T-1$ 天这个时间范围内的累计简单回报率为\n$$\nR = \\prod_{u=1}^{T-1} (1 + p_u) - 1.\n$$\n您的程序必须为下面提供的每个测试用例计算 $R$，并将每个 $R$ 四舍五入到 $6$ 位小数后输出。\n\n测试套件（每个用例提供 $C_t$、$P_t$、$r_t$、$t_\\ell$、$t_h$ 和 $c$）：\n- 用例 1（理想路径，信号和成本可变）：$T = 6$，其中 $C_t$ 等于 $100, 150, 80, 50, 200, 90$；$P_t$ 等于 $200, 100, 120, 100, 50, 110$；$r_t$ 等于 $0.01, -0.005, 0.008, -0.02, 0.015, 0.005$；$t_\\ell = 0.8$；$t_h = 1.2$；$c = 0.001$。\n- 用例 2（边界条件，$q_t$ 等于阈值时不产生交易）：$T = 5$，其中 $C_t$ 等于 $50, 100, 200, 60, 40$；$P_t$ 等于 $100, 50, 100, 120, 80$；$r_t$ 等于 $0.01, 0.02, -0.01, 0.005, -0.005$；$t_\\ell = 0.5$；$t_h = 2.0$；$c = 0.002$。\n- 用例 3（看跌期权交易量为零和两者交易量均为零，严格的逆向行为）：$T = 6$，其中 $C_t$ 等于 $0, 10, 20, 0, 5, 0$；$P_t$ 等于 $0, 0, 5, 0, 10, 0$；$r_t$ 等于 $0.01, -0.02, 0.03, -0.01, 0.005, 0.002$；$t_\\ell = 0.8$；$t_h = 3.0$；$c = 0.0015$。\n- 用例 4（持续的极端情绪，主要是单边头寸且换手率最低）：$T = 5$，其中 $C_t$ 等于 $300, 320, 310, 305, 315$；$P_t$ 等于 $100, 90, 95, 100, 85$；$r_t$ 等于 $0.01, -0.005, 0.002, 0.004, -0.003$；$t_\\ell = 0.7$；$t_h = 2.5$；$c = 0.0005$。\n\n要求的最终输出格式：您的程序应生成单行输出，其中包含四个用例的结果，形式为方括号括起来的逗号分隔列表，每个值四舍五入到 6 位小数，例如，`[0.123456,-0.010000,0.000000,0.250000]`。", "solution": "该问题要求计算一种基于对期权市场情绪的逆向解读的量化交易策略的累计回报率。该策略在一个离散时间范围 $t \\in \\{0, 1, \\dots, T-1\\}$ 内实施。对问题陈述的验证证实了其科学和数学上的合理性、完整性和客观性。它构成了一个计算金融学中的适定问题。我们继续进行求解。\n\n该解决方案通过算法逐日模拟该策略得出。该过程包括三个主要阶段：信号生成、每日利润计算和累计回报率聚合。\n\n**1. 信号生成**\n\n对于从 $t=0$ 到 $t=T-1$ 的每个交易日 $t$，确定一个交易信号 $s_t \\in \\{-1, 0, +1\\}$。该信号指示了下一天 $t+1$ 要持有的头寸。信号是根据看涨-看跌期权成交量比率 $q_t$ 得出的。\n\n首先，根据每日看涨期权交易量 $C_t$ 和看跌期权交易量 $P_t$ 计算比率 $q_t$：\n$$\nq_t =\n\\begin{cases}\n\\dfrac{C_t}{P_t},  \\text{如果 } P_t  0, \\\\\n+\\infty,  \\text{如果 } P_t = 0 \\text{ 且 } C_t  0, \\\\\n0,  \\text{如果 } P_t = 0 \\text{ 且 } C_t = 0.\n\\end{cases}\n$$\n$q_t=+\\infty$ 的值在计算上使用浮点无穷大来处理。\n\n接下来，通过将 $q_t$ 与两个预定义的阈值——一个下阈值 $t_\\ell$ 和一个上阈值 $t_h$（其中 $0  t_\\ell  t_h$）进行比较，来确定信号 $s_t$：\n$$\ns_t =\n\\begin{cases}\n+1,  \\text{如果 } q_t  t_\\ell, \\\\\n-1,  \\text{如果 } q_t  t_h, \\\\\n0,  \\text{否则。}\n\\end{cases}\n$$\n信号 $s_t=+1$ 代表多头头寸，$s_t=-1$ 代表空头头寸，$s_t=0$ 代表中性（无）头寸。对每一天 $t$ 重复此过程，以生成信号的时间序列 $s_0, s_1, \\dots, s_{T-1}$。\n\n**2. 每日盈亏（PL）计算**\n\n对于 $u \\in \\{1, \\dots, T-1\\}$，计算第 $u$ 天的利润 $p_u$。利润 $p_{t+1}$ 取决于信号 $s_t$（在第 $t$ 天收盘时确定）、第 $t+1$ 天资产的简单回报率 $r_{t+1}$ 以及比例交易成本 $c$。第 $t+1$ 天的利润公式如下：\n$$\np_{t+1} = s_t \\cdot r_{t+1} - c \\cdot \\mathbf{1}\\{s_t \\ne s_{t-1}\\}\n$$\n对于 $t = 0, 1, \\dots, T-2$。初始头寸给定为 $s_{-1} = 0$。\n\n$s_t \\cdot r_{t+1}$ 项代表投资回报。多头头寸 ($s_t=+1$) 从正回报 ($r_{t+1}0$) 中获利，从负回报中亏损。空头头寸 ($s_t=-1$) 从负回报 ($r_{t+1}0$) 中获利，从正回报中亏损。\n\n$c \\cdot \\mathbf{1}\\{s_t \\ne s_{t-1}\\}$ 项代表交易成本。指示函数 $\\mathbf{1}\\{\\cdot\\}$ 在其参数为真时为 $1$，否则为 $0$。仅当第 $t$ 天结束时的头寸 ($s_t$) 与第 $t$ 天期间持有的头寸 (由 $s_{t-1}$ 确定) 不同时，才会产生 $c$ 的成本。通过从 $t=0$ 到 $t=T-2$ 进行迭代，计算出每日利润序列 $p_1, p_2, \\dots, p_{T-1}$。\n\n**3. 累计回报率计算**\n\n该策略在从第 $1$ 天到第 $T-1$ 天的时间范围内的总体表现由累计简单回报率 $R$ 来衡量。这是通过几何方式复利计算每日回报 $(1+p_u)$ 得出的：\n$$\nR = \\prod_{u=1}^{T-1} (1 + p_u) - 1.\n$$\n这个最终值 $R$ 代表了初始资本的总增长，已扣除所有交易成本。\n\n该实现将通过依次应用这些步骤来处理每个测试用例。$C_t$、$P_t$ 和 $r_t$ 的时间序列存储在数组中。首先计算并存储所有 $t$ 的信号 $s_t$。然后，第二个循环使用信号和回报率计算每日利润 $p_{t+1}$。最后，$(1+p_u)$ 的累积乘积得出总回报。每个用例的结果按指定要求四舍五入到 6 位小数。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef calculate_cumulative_return(T, C, P, r, t_ell, t_h, c):\n    \"\"\"\n    Calculates the cumulative return of the contrarian trading strategy.\n\n    Args:\n        T (int): The total number of days in the data series.\n        C (np.ndarray): Array of call option volumes.\n        P (np.ndarray): Array of put option volumes.\n        r (np.ndarray): Array of asset's simple returns.\n        t_ell (float): The lower threshold for the call-put ratio.\n        t_h (float): The upper threshold for the call-put ratio.\n        c (float): The proportional transaction cost.\n\n    Returns:\n        float: The cumulative simple return R.\n    \"\"\"\n    \n    # Step 1: Generate trading signals s_t for t = 0, ..., T-1.\n    s = np.zeros(T)\n    for t in range(T):\n        C_t, P_t = C[t], P[t]\n        \n        q_t = 0.0\n        if P_t > 0:\n            q_t = C_t / P_t\n        elif P_t == 0 and C_t > 0:\n            q_t = np.inf\n        # If P_t == 0 and C_t == 0, q_t remains 0.0 as initialized.\n        \n        if q_t  t_ell:\n            s[t] = 1.0\n        elif q_t > t_h:\n            s[t] = -1.0\n        # Otherwise, s[t] remains 0.0 as initialized.\n\n    # Step 2: Calculate daily profits p_{t+1} for t = 0, ..., T-2.\n    # This yields profits p_1, ..., p_{T-1}.\n    profits = []\n    s_prev = 0.0 # Initial position s_{-1} = 0.\n    \n    # The loop runs for t from 0 to T-2.\n    for t in range(T - 1):\n        s_t = s[t]\n        r_t_plus_1 = r[t + 1]\n        \n        # Calculate transaction cost\n        transaction_cost = c if s_t != s_prev else 0.0\n        \n        # Calculate profit for day t+1\n        profit_t_plus_1 = s_t * r_t_plus_1 - transaction_cost\n        profits.append(profit_t_plus_1)\n        \n        # Update s_prev for the next iteration\n        s_prev = s_t\n\n    # Step 3: Calculate the cumulative simple return R.\n    # R = product(1 + p_u) - 1, for u = 1, ..., T-1.\n    if not profits:\n        return 0.0\n\n    compounded_factors = [1 + p for p in profits]\n    cumulative_return = np.prod(compounded_factors) - 1.0\n    \n    return cumulative_return\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    test_cases = [\n        {\n            \"T\": 6,\n            \"C\": np.array([100, 150, 80, 50, 200, 90]),\n            \"P\": np.array([200, 100, 120, 100, 50, 110]),\n            \"r\": np.array([0.01, -0.005, 0.008, -0.02, 0.015, 0.005]),\n            \"t_ell\": 0.8, \"t_h\": 1.2, \"c\": 0.001\n        },\n        {\n            \"T\": 5,\n            \"C\": np.array([50, 100, 200, 60, 40]),\n            \"P\": np.array([100, 50, 100, 120, 80]),\n            \"r\": np.array([0.01, 0.02, -0.01, 0.005, -0.005]),\n            \"t_ell\": 0.5, \"t_h\": 2.0, \"c\": 0.002\n        },\n        {\n            \"T\": 6,\n            \"C\": np.array([0, 10, 20, 0, 5, 0]),\n            \"P\": np.array([0, 0, 5, 0, 10, 0]),\n            \"r\": np.array([0.01, -0.02, 0.03, -0.01, 0.005, 0.002]),\n            \"t_ell\": 0.8, \"t_h\": 3.0, \"c\": 0.0015\n        },\n        {\n            \"T\": 5,\n            \"C\": np.array([300, 320, 310, 305, 315]),\n            \"P\": np.array([100, 90, 95, 100, 85]),\n            \"r\": np.array([0.01, -0.005, 0.002, 0.004, -0.003]),\n            \"t_ell\": 0.7, \"t_h\": 2.5, \"c\": 0.0005\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        R = calculate_cumulative_return(\n            case[\"T\"], case[\"C\"], case[\"P\"], case[\"r\"],\n            case[\"t_ell\"], case[\"t_h\"], case[\"c\"]\n        )\n        # Format to 6 decimal places.\n        results.append(f\"{R:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "2371402"}, {"introduction": "拥有一个好的交易信号只是第一步；高效地执行交易同样至关重要，因为大额订单本身就可能对市场价格产生不利影响，即所谓的“市场冲击”（market impact）。本练习模拟了一种高级的订单执行算法，它通过将一个大的“母订单”拆分成一系列较小的“子订单”来最小化市场冲击 [@problem_id:2371356]。这些子订单的规模遵循帕累托分布（Pareto distribution），以模仿“噪音交易者”的行为，从而掩盖真实交易意图。这个实践问题揭示了交易执行这一关键挑战，并介绍了成交量加权平均价（VWAP）和执行差额（Implementation Shortfall）等核心绩效指标。", "problem": "一个买方执行算法旨在执行一个大小为 $Q$ 的大额母订单，通过将其拆分为一系列子订单来模仿噪音交易者，这些子订单的大小是帕累托分布的独立抽样。市场影响是暂时的，并且与子订单大小相对于基准交易量的比例呈线性关系。未受订单影响的中间价根据离散时间几何布朗运动 (GBM) 演变。目标是针对几组不同的参数集，根据第一性原理计算出子订单的数量、成交量加权平均执行价格以及相对于到达价格的执行差额。\n\n定义与建模假设：\n- 母订单方向：买入。\n- 母订单大小：$Q \\in \\mathbb{R}_{0}$ 股。\n- 子订单大小：$(S_i)_{i \\geq 1}$ 独立同分布，遵循帕累托 I 型分布，其最小（尺度）参数为 $x_{\\min} \\in \\mathbb{R}_{0}$，形状参数为 $\\alpha \\in \\mathbb{R}_{1}$。对于所有 $s \\geq x_{\\min}$，其累积分布函数为 $F(s) = 1 - \\left(\\frac{x_{\\min}}{s}\\right)^{\\alpha}$。一个子订单大小的抽样值 $S_i^{\\text{raw}}$ 通过逆变换采样从一个单位均匀变量 $U_i \\sim \\text{Uniform}(0,1)$ 映射而来：\n$$\nS_i^{\\text{raw}} \\;=\\; x_{\\min} \\,(1 - U_i)^{-1/\\alpha}.\n$$\n子订单被顺序生成，直到累计成交量达到或超过 $Q$。已执行的子订单大小会被截断，以确保刚好完成母订单：\n$$\nS_i \\;=\\; \\min\\{S_i^{\\text{raw}},\\, Q - \\sum_{j=1}^{i-1} S_j\\}.\n$$\n- 未受影响的中间价：设 $P_0 \\in \\mathbb{R}_{0}$ 为到达中间价。对于 $i \\geq 1$，未受影响的中间价按每个子订单一个时间步长演变，遵循离散时间几何布朗运动，时间步长为 $\\Delta t \\in \\mathbb{R}_{0}$，漂移为 $\\mu \\in \\mathbb{R}$，波动率为 $\\sigma \\in \\mathbb{R}_{\\geq 0}$：\n$$\nP_i \\;=\\; P_{i-1}\\,\\exp\\!\\Big(\\big(\\mu - \\tfrac{1}{2}\\sigma^2\\big)\\Delta t \\;+\\; \\sigma \\sqrt{\\Delta t}\\,Z_i\\Big),\n$$\n其中 $(Z_i)_{i \\geq 1}$ 是独立的标准正态变量。\n- 临时线性影响：第 $i$ 个子订单的执行价格为\n$$\np_i \\;=\\; P_i \\;+\\; \\lambda \\left(\\frac{S_i}{V_0}\\right),\n$$\n其中影响系数为 $\\lambda \\in \\mathbb{R}_{\\geq 0}$，基准交易量为 $V_0 \\in \\mathbb{R}_{0}$。\n- 已实现成本、成交量加权平均价格 (VWAP) 和执行差额：设 $N$ 为完成母订单所需的（随机）子订单数量。已实现成本为\n$$\nC \\;=\\; \\sum_{i=1}^{N} S_i\\,p_i.\n$$\n成交量加权平均价格为\n$$\n\\text{VWAP} \\;=\\; \\frac{C}{Q}.\n$$\n相对于到达中间价的执行差额为\n$$\n\\text{IS} \\;=\\; C \\;-\\; P_0\\,Q.\n$$\n\n随机数输入（本任务中为确定性有限序列）：对于所有测试用例，使用相同的两个有限序列，每个测试用例都从头开始。单位均匀序列为\n$$\nU \\;=\\; (0.05,\\,0.22,\\,0.73,\\,0.91,\\,0.41,\\,0.63,\\,0.81,\\,0.19,\\,0.34,\\,0.68,\\,0.95,\\,0.12,\\,0.57,\\,0.86,\\,0.15,\\,0.47,\\,0.66,\\,0.37,\\,0.08,\\,0.99,\\,0.28,\\,0.76,\\,0.83,\\,0.44,\\,0.52,\\,0.88,\\,0.03,\\,0.61,\\,0.72,\\,0.25).\n$$\n标准正态序列为\n$$\nZ \\;=\\; (0.10,\\,-0.35,\\,0.50,\\,-1.20,\\,0.00,\\,1.10,\\,-0.70,\\,0.30,\\,-0.20,\\,0.90,\\,-0.45,\\,0.75,\\,-0.05,\\,0.40,\\,-0.95,\\,0.55,\\,-0.15,\\,0.20,\\,-0.60,\\,1.25,\\,-0.80,\\,0.05,\\,0.35,\\,-0.25,\\,0.65,\\,-1.10,\\,0.15,\\,-0.05,\\,0.85,\\,-0.50).\n$$\n每个测试用例必须从第一个元素开始使用这些序列，并根据需要逐个元素地进行。所提供的参数集保证了所需元素的数量不会超过可用元素的数量。\n\n测试套件（四个不同的参数集）：\n- 测试 $1$ （一般情况）：$Q=10000$, $x_{\\min}=200$, $\\alpha=1.5$, $P_0=100.0$, $\\mu=0.0$, $\\sigma=0.02$, $\\Delta t=1.0$, $\\lambda=0.01$, $V_0=5000$。\n- 测试 $2$ （无影响，无波动性）：$Q=5000$, $x_{\\min}=100$, $\\alpha=2.0$, $P_0=50.0$, $\\mu=0.0$, $\\sigma=0.0$, $\\Delta t=1.0$, $\\lambda=0.0$, $V_0=4000$。\n- 测试 $3$ （接近边界的重尾，仅有影响）：$Q=1200$, $x_{\\min}=50$, $\\alpha=1.05$, $P_0=25.0$, $\\mu=0.0$, $\\sigma=0.0$, $\\Delta t=1.0$, $\\lambda=0.02$, $V_0=3000$。\n- 测试 $4$ （小额母订单等于最小规模，有波动性）：$Q=200$, $x_{\\min}=200$, $\\alpha=3.0$, $P_0=10.0$, $\\mu=0.0$, $\\sigma=0.05$, $\\Delta t=1.0$, $\\lambda=0.005$, $V_0=2000$。\n\n每个测试用例要求输出：\n- 子订单数量 $N$（一个整数），\n- 成交量加权平均价格 $\\text{VWAP}$（一个浮点数），\n- 执行差额 $\\text{IS}$（一个浮点数）。\n\n最终输出格式：\n- 您的程序应生成一行输出，其中包含按顺序 $1$ 到 $4$ 的测试用例结果，格式为单一的列表之列表，不含空格。对于每个测试，输出三元组 $[N,\\text{VWAP},\\text{IS}]$，其中 $\\text{VWAP}$ 和 $\\text{IS}$ 四舍五入到小数点后恰好 $6$ 位。例如，整体输出必须如下所示：\n$$\n[[N_1,\\text{VWAP}_1,\\text{IS}_1],[N_2,\\text{VWAP}_2,\\text{IS}_2],[N_3,\\text{VWAP}_3,\\text{IS}_3],[N_4,\\text{VWAP}_4,\\text{IS}_4]].\n$$", "solution": "问题陈述已经过严格审查，并被确定为有效。它在科学上基于计算金融学的原理，问题设定良好，具有清晰、确定性的模拟路径，且表述客观。所有必需的公式、参数和数据均已提供，从而可以得到唯一且可验证的解。该问题要求实现一种算法交易策略的路径依赖模拟。我们将根据定义的第一性原理构建算法来着手解决。\n\n问题的核心是模拟一个大额母订单（大小为 $Q$）的执行过程，该母订单被分解为一系列较小的子订单。该模拟是路径依赖的，意味着每一步的结果都取决于所有先前步骤的结果。必须对四个不同的参数集执行此模拟，使用两个预定义的伪随机数序列以确保结果的确定性和可复现性。\n\n单个参数集的算法流程如下：\n\n1.  **初始化**：我们从时间 $0$（即发送第一个子订单之前）的市场和订单状态开始。\n    - 待执行的总量是母订单大小 $Q$。\n    - 累计成交量 $Q_{\\text{filled}}$ 初始化为 $0$。\n    - 累计执行成本 $C$ 初始化为 $0$。\n    - 子订单数量 $N$ 初始化为 $0$。\n    - 初始未受影响的中间价是到达价格，$P_{\\text{unaffected},0} = P_0$。\n    - 我们维护所提供的均匀序列 $U$ 和标准正态序列 $Z$ 的索引，两者都从 $0$ 开始。\n\n2.  **迭代执行循环**：模拟通过逐个生成子订单进行，直到总成交量等于母订单大小 $Q$。对于每个子订单 $i=1, 2, \\ldots$：\n    - 子订单计数 $N$ 增加。\n    - 计算剩余待成交量：$Q_{\\text{rem}} = Q - Q_{\\text{filled}}$。\n    - 使用逆变换采样法根据帕累托分布生成一个原始子订单大小 $S_i^{\\text{raw}}$。这需要来自均匀序列的第 $i$ 个值 $U_i$。公式为：\n    $$\n    S_i^{\\text{raw}} = x_{\\min} (1 - U_i)^{-1/\\alpha}\n    $$\n    - 子订单的实际执行大小 $S_i$ 通过取原始大小和剩余数量的最小值来确定。这确保了总执行量不会超过 $Q$。\n    $$\n    S_i = \\min\\{S_i^{\\text{raw}}, Q_{\\text{rem}}\\}\n    $$\n    - 未受影响的中间价根据离散时间几何布朗运动模型演变一步。这使用来自标准正态序列的第 $i$ 个值 $Z_i$。当前步骤的价格 $P_{\\text{unaffected},i}$ 从上一步的价格 $P_{\\text{unaffected},i-1}$ 计算得出：\n    $$\n    P_{\\text{unaffected},i} = P_{\\text{unaffected},i-1} \\exp\\left( \\left(\\mu - \\frac{1}{2}\\sigma^2\\right)\\Delta t + \\sigma \\sqrt{\\Delta t} Z_i \\right)\n    $$\n    - 第 $i$ 个子订单的执行价格 $p_i$ 是通过将临时线性市场影响加到新的未受影响的中间价 $P_{\\text{unaffected},i}$ 上计算得出的。影响与子订单大小 $S_i$ 相对于基准交易量 $V_0$ 的比例成正比。\n    $$\n    p_i = P_{\\text{unaffected},i} + \\lambda \\left(\\frac{S_i}{V_0}\\right)\n    $$\n    - 执行第 $i$ 个子订单的成本是 $S_i p_i$。该金额被加到累计成本 $C$ 中。\n    $$\n    C \\leftarrow C + S_i p_i\n    $$\n    - 更新累计成交量：\n    $$\n    Q_{\\text{filled}} \\leftarrow Q_{\\text{filled}} + S_i\n    $$\n    - 此循环继续进行，直到 $Q_{\\text{filled}} = Q$。问题陈述保证了所提供的随机序列对于所有测试用例都足够长，以满足此条件。\n\n3.  **最终指标计算**：循环终止后，从汇总的模拟结果中计算最终的性能指标。\n    - 子订单的总数是 $N$。\n    - 成交量加权平均价格 (VWAP) 是总成本除以总数量：\n    $$\n    \\text{VWAP} = \\frac{C}{Q}\n    $$\n    - 执行差额 (IS) 是总执行成本与以到达价格 $P_0$ 执行整个订单的理论成本之间的差额：\n    $$\n    \\text{IS} = C - P_0 Q\n    $$\n\n我们将实现这一完整过程，并将其应用于四个指定的测试用例中的每一个。然后，将每个案例得到的三元组 $[N, \\text{VWAP}, \\text{IS}]$ 格式化为所需的精度和结构。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the algorithmic trading simulation problem for the given test cases.\n    \"\"\"\n    \n    # Provided deterministic sequences for random variables.\n    U_seq = [\n        0.05, 0.22, 0.73, 0.91, 0.41, 0.63, 0.81, 0.19, 0.34, 0.68,\n        0.95, 0.12, 0.57, 0.86, 0.15, 0.47, 0.66, 0.37, 0.08, 0.99,\n        0.28, 0.76, 0.83, 0.44, 0.52, 0.88, 0.03, 0.61, 0.72, 0.25\n    ]\n    Z_seq = [\n        0.10, -0.35, 0.50, -1.20, 0.00, 1.10, -0.70, 0.30, -0.20, 0.90,\n        -0.45, 0.75, -0.05, 0.40, -0.95, 0.55, -0.15, 0.20, -0.60, 1.25,\n        -0.80, 0.05, 0.35, -0.25, 0.65, -1.10, 0.15, -0.05, 0.85, -0.50\n    ]\n\n    # Test suite with four parameter sets.\n    test_cases = [\n        # Test 1 (general case)\n        {'Q': 10000, 'x_min': 200, 'alpha': 1.5, 'P0': 100.0, 'mu': 0.0, 'sigma': 0.02, 'delta_t': 1.0, 'lambda_': 0.01, 'V0': 5000},\n        # Test 2 (no impact, no volatility)\n        {'Q': 5000, 'x_min': 100, 'alpha': 2.0, 'P0': 50.0, 'mu': 0.0, 'sigma': 0.0, 'delta_t': 1.0, 'lambda_': 0.0, 'V0': 4000},\n        # Test 3 (heavy tail, impact only)\n        {'Q': 1200, 'x_min': 50, 'alpha': 1.05, 'P0': 25.0, 'mu': 0.0, 'sigma': 0.0, 'delta_t': 1.0, 'lambda_': 0.02, 'V0': 3000},\n        # Test 4 (small parent equal to minimum size)\n        {'Q': 200, 'x_min': 200, 'alpha': 3.0, 'P0': 10.0, 'mu': 0.0, 'sigma': 0.05, 'delta_t': 1.0, 'lambda_': 0.005, 'V0': 2000},\n    ]\n\n    results = []\n    \n    for params in test_cases:\n        # Extract parameters for clarity\n        Q = params['Q']\n        x_min = params['x_min']\n        alpha = params['alpha']\n        P0 = params['P0']\n        mu = params['mu']\n        sigma = params['sigma']\n        delta_t = params['delta_t']\n        lambda_ = params['lambda_']\n        V0 = params['V0']\n        \n        # Initialize state variables\n        cumulative_volume_filled = 0.0\n        total_cost = 0.0\n        n_child_orders = 0\n        p_unaffected = P0\n        u_idx, z_idx = 0, 0\n        \n        # Simulation loop\n        while np.isclose(cumulative_volume_filled, Q) == False:\n            n_child_orders += 1\n            \n            # Get random variates from sequences\n            U_i = U_seq[u_idx]\n            Z_i = Z_seq[z_idx]\n            u_idx += 1\n            z_idx += 1\n\n            # 1. Generate child order size\n            s_raw = x_min * (1 - U_i)**(-1 / alpha)\n            \n            remaining_q = Q - cumulative_volume_filled\n            s_i = min(s_raw, remaining_q)\n            \n            # 2. Evolve unaffected mid-price\n            price_drift = (mu - 0.5 * sigma**2) * delta_t\n            price_shock = sigma * np.sqrt(delta_t) * Z_i\n            p_unaffected = p_unaffected * np.exp(price_drift + price_shock)\n            \n            # 3. Calculate execution price with impact\n            p_i = p_unaffected + lambda_ * (s_i / V0)\n            \n            # 4. Update state\n            total_cost += s_i * p_i\n            cumulative_volume_filled += s_i\n\n        # Calculate final metrics\n        N = n_child_orders\n        VWAP = total_cost / Q if Q > 0 else 0\n        IS = total_cost - P0 * Q\n        \n        results.append([N, VWAP, IS])\n\n    # Format the final output string exactly as specified\n    formatted_results = []\n    for res in results:\n        N, vwap, is_val = res\n        # Format floats to exactly 6 decimal places.\n        vwap_str = f\"{vwap:.6f}\"\n        is_val_str = f\"{is_val:.6f}\"\n        formatted_results.append(f\"[{N},{vwap_str},{is_val_str}]\")\n\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "2371356"}, {"introduction": "前面的练习主要关注基于固定规则的策略，而现代算法交易正越来越多地转向自适应方法。本次高级实践将引领您进入人工智能的一个强大分支：强化学习（Reinforcement Learning）[@problem_id:2371418]。您将构建一个Q-learning智能体，它能够学会在不同的市场环境（如牛市或熊市）中，选择最优的交易策略（如动量策略或均值回归策略）。通过完成这个练习，您将体验一种全新的策略设计范式，即让算法根据历史经验动态调整其行为，而不是遵循一成不变的静态规则。", "problem": "要求您实现一个完整、可运行的程序，在一个有限状态马尔可夫决策过程 (MDP) 表示的市场状态中，使用 Q-learning 训练一个离策略 (off-policy) 控制智能体。智能体的状态是市场状态，其动作是算法交易策略。环境是完全指定且平稳的。您的任务是在训练结束后，针对一个小型测试集，计算由学习到的状态-动作价值函数所导出的贪心策略。您的程序必须按照末尾指定的格式，精确地产生一行输出。\n\n此问题的基本基础是具有平稳转移概率和平稳奖励函数的有限状态马尔可夫决策过程 (MDP) 的定义、带折扣因子的折扣回报的定义，以及 MDP 中的最优控制由贝尔曼最优性条件 (Bellman optimality condition) 刻画的原则。您必须在这些基础上，使用 Q-learning 实现离策略时间差分 (TD) 控制，并且除了允许的库之外，不得依赖任何外部库。\n\n状态空间和动作空间：\n- 有 $3$ 个状态（市场状态），按固定顺序 $[0,1,2]$ 索引，分别对应 $[\\,牛市,熊市,震荡市\\,]$。\n- 有 $3$ 个动作（交易策略），按固定顺序 $[0,1,2]$ 索引，分别对应 $[\\,动量,均值回归,现金/空仓\\,]$。\n\n回合式训练与参数安排：\n- 每次训练运行包含 $E$ 个回合，每个回合有固定的步数上限 $H$ 步。\n- 每个回合的初始状态从 $\\{0,1,2\\}$ 中独立均匀抽取。\n- 探索使用 $\\varepsilon$-贪心策略，其探索率在总训练步数 $T = E \\times H$ 内线性衰减。具体来说，如果 $t \\in \\{0,1,\\dots,T-1\\}$ 是整个训练运行期间的全局步数计数，那么\n$$\n\\varepsilon(t) \\;=\\; \\max\\!\\left\\{\\varepsilon_{\\min},\\; \\varepsilon_{\\text{start}} \\;+\\; \\frac{t}{T-1}\\,\\big(\\varepsilon_{\\min} - \\varepsilon_{\\text{start}}\\big)\\right\\}.\n$$\n- 每个状态-动作对的学习率是基于访问次数 $N(s,a)$ 的递减序列，由下式给出\n$$\n\\alpha(s,a) \\;=\\; \\frac{\\alpha_0}{1 + N(s,a)}.\n$$\n- 折扣因子为 $\\gamma \\in [0,1)$。\n- 所有随机抽样（初始状态、$\\varepsilon$-贪心策略下的动作取样以及下一状态转移）都必须使用固定的伪随机数生成器种子 $7$，以保证可复现性。\n- 奖励是确定性的，由奖励矩阵给出；不存在外生的奖励噪声。\n- 平局打破规则：当需要对动作取最大值时（无论是在利用（exploitation）阶段选择动作，还是在训练后计算贪心策略时），如果多个动作达到相同的最大值，则选择索引最小的那个动作。\n\n环境规范：\n- 奖励函数由一个矩阵 $R \\in \\mathbb{R}^{3 \\times 3}$ 给出，其中 $R[s,a]$ 是在状态 $s$ 下采取动作 $a$ 时的确定性即时奖励。\n- 转移动态由一个与动作无关的转移矩阵 $P \\in [0,1]^{3 \\times 3}$（其行和为 1，对所有动作都相同）给出，或者由一个与动作相关的核 $\\{P^{(a)}\\}_{a=0}^2$（其中 $P^{(a)} \\in [0,1]^{3 \\times 3}$ 且每行之和为 1）给出。\n\n您必须实现与上述描述一致的 Q-learning（离策略 TD 控制），在训练期间使用 $\\varepsilon$-贪心行为，并仅在报告最终策略时使用贪心动作选择。\n\n测试集：\n实现您的解决方案以处理以下四个测试用例。对于每个用例，从头开始训练一个新的智能体，然后在规定的状态顺序下，将最终的贪心策略报告为一个包含三个整数的列表 $[\\,\\pi(0),\\,\\pi(1),\\,\\pi(2)\\,]$。\n\n- 用例 A (理想路径；动作无关动态；非零折扣):\n    - 折扣: $\\gamma = 0.95$。\n    - 回合与步数上限: $E = 10000$, $H = 40$。\n    - 探索: $\\varepsilon_{\\text{start}} = 0.8$, $\\varepsilon_{\\min} = 0.05$。\n    - 学习率基础值: $\\alpha_0 = 0.5$。\n    - 奖励:\n    $$\n    R \\;=\\; \\begin{bmatrix}\n    0.8  -0.3  0.1 \\\\\n    -0.6  0.5  0.05 \\\\\n    0.2  0.3  0.02\n    \\end{bmatrix}.\n    $$\n    - 转移 (动作无关):\n    $$\n    P \\;=\\; \\begin{bmatrix}\n    0.85  0.05  0.10 \\\\\n    0.05  0.80  0.15 \\\\\n    0.20  0.20  0.60\n    \\end{bmatrix}.\n    $$\n\n- 用例 B (边界条件；零折扣的短视控制):\n    - 折扣: $\\gamma = 0$。\n    - 回合与步数上限: $E = 2000$, $H = 30$。\n    - 探索: $\\varepsilon_{\\text{start}} = 0.8$, $\\varepsilon_{\\min} = 0.1$。\n    - 学习率基础值: $\\alpha_0 = 0.5$。\n    - 奖励: 与用例 A 中的 $R$ 相同。\n    - 转移: 与用例 A 中的 $P$ 相同 (动作无关)。\n\n- 用例 C (边缘情况；一个状态的即时奖励存在平局；动作无关动态):\n    - 折扣: $\\gamma = 0.95$。\n    - 回合与步数上限: $E = 10000$, $H = 40$。\n    - 探索: $\\varepsilon_{\\text{start}} = 0.8$, $\\varepsilon_{\\min} = 0.05$。\n    - 学习率基础值: $\\alpha_0 = 0.5$。\n    - 奖励:\n    $$\n    R \\;=\\; \\begin{bmatrix}\n    0.8  -0.3  0.1 \\\\\n    -0.6  0.5  0.05 \\\\\n    0.3  0.3  0.02\n    \\end{bmatrix}.\n    $$\n    - 转移 (动作无关): 与用例 A 中的 $P$ 相同。\n    - 注意：在状态 $2$ (震荡市) 中，动作 $0$ 和 $1$ 的即时奖励相等。您的平局打破规则必须在最大值中选择最小的索引。\n\n- 用例 D (动作相关动态；长期效应很重要):\n    - 折扣: $\\gamma = 0.99$。\n    - 回合与步数上限: $E = 20000$, $H = 50$。\n    - 探索: $\\varepsilon_{\\text{start}} = 0.9$, $\\varepsilon_{\\min} = 0.05$。\n    - 学习率基础值: $\\alpha_0 = 0.5$。\n    - 奖励:\n    $$\n    R \\;=\\; \\begin{bmatrix}\n    0.7  0.2  0.1 \\\\\n    -0.1  0.05  0.02 \\\\\n    0.25  0.35  0.02\n    \\end{bmatrix}.\n    $$\n    - 动作相关的转移:\n    $$\n    P^{(0)} \\;=\\; \\begin{bmatrix}\n    0.92  0.03  0.05 \\\\\n    0.99  0.005  0.005 \\\\\n    0.50  0.10  0.40\n    \\end{bmatrix},\\quad\n    P^{(1)} \\;=\\; \\begin{bmatrix}\n    0.80  0.05  0.15 \\\\\n    0.05  0.94  0.01 \\\\\n    0.40  0.20  0.40\n    \\end{bmatrix},\\quad\n    P^{(2)} \\;=\\; \\begin{bmatrix}\n    0.70  0.10  0.20 \\\\\n    0.01  0.98  0.01 \\\\\n    0.10  0.30  0.60\n    \\end{bmatrix}.\n    $$\n\n输出规范：\n- 每个用例的训练完成后，使用上述平局打破规则计算贪心策略 $\\pi(s) = \\arg\\max_{a \\in \\{0,1,2\\}} Q(s,a)$。\n- 您的程序应生成单行输出，其中包含所有四个用例的结果，格式为逗号分隔的列表的列表，不含空格，并用方括号括起来。例如，打印的行必须如下所示\n\"[[x0,x1,x2],[y0,y1,y2],[z0,z1,z2],[w0,w1,w2]]\"\n其中每个符号都是一个整数。请用 A、B、C 和 D 四个用例实际学习到的策略（按此顺序）替换这些占位符。", "solution": "所提出的问题是强化学习领域一个定义明确且可通过计算验证的练习，具体而言是应用 Q-learning 算法于一个有限状态马尔可夫决策过程 (MDP)。任务是为一个简化的市场状态模型确定最优控制策略。问题陈述在科学上是严谨的，并为得出唯一、可复现的解提供了所有必要的参数和规范。我们将着手进行推导和实现。\n\n问题的核心在于马尔可夫决策过程的框架，它由一个元组 $(S, A, P, R, \\gamma)$ 定义。\n状态空间 $S$ 包含 $3$ 种市场状态：$s \\in \\{0, 1, 2\\}$，对应 $[\\text{牛市}, \\text{熊市}, \\text{震荡市}]$。\n动作空间 $A$ 包含 $3$ 种交易策略：$a \\in \\{0, 1, 2\\}$，对应 $[\\text{动量}, \\text{均值回归}, \\text{现金/空仓}]$。\n奖励函数 $R(s, a)$ 为在状态 $s$ 采取动作 $a$ 提供的即时确定性奖励。\n转移概率函数 $P(s'|s, a)$ 给出在状态 $s$ 采取动作 $a$ 后，转移到状态 $s'$ 的概率。\n折扣因子 $\\gamma \\in [0, 1)$ 决定了未来奖励的现值。\n\n强化学习智能体的目标是找到一个最优策略 $\\pi^*(s)$，该策略将状态映射到动作，以最大化期望折扣回报。在策略 $\\pi$ 下，一个状态-动作对 $(s, a)$ 的价值由状态-动作价值函数 $Q^{\\pi}(s, a)$ 给出，它表示从状态 $s$ 开始，采取动作 $a$，然后遵循策略 $\\pi$ 的期望回报。\n\n最优状态-动作价值函数 $Q^*(s, a) = \\max_{\\pi} Q^{\\pi}(s, a)$ 必须满足贝尔曼最优性方程 (Bellman optimality equation)：\n$$\nQ^*(s, a) = \\mathbb{E}\\left[ R_{t+1} + \\gamma \\max_{a'} Q^*(S_{t+1}, a') \\mid S_t=s, A_t=a \\right]\n$$\n对于确定性奖励函数 $R(s,a)$，这可以简化为：\n$$\nQ^*(s, a) = R(s, a) + \\gamma \\sum_{s' \\in S} P(s'|s, a) \\max_{a'} Q^*(s', a')\n$$\n\n由于转移模型 $P$ 和奖励函数 $R$ 是已知的，这个系统可以使用诸如价值迭代之类的动态规划方法来解决。然而，问题指定使用 Q-learning，这是一种无模型的时间差分 (TD) 学习算法。Q-learning 直接估计最优 $Q^*$-值，而无需 $P$ 和 $R$ 的显式知识（尽管在我们的案例中它们是已知的，并用于模拟环境）。\n\nQ-learning 通过基于经验元组 $(S_t, A_t, R_{t+1}, S_{t+1})$ 迭代更新 $Q$-值的估计值 $Q(s, a)$ 来工作。在训练过程的每个全局时间步 $t$，智能体处于状态 $S_t$ 并采取动作 $A_t$。它观察到一个奖励 $R_{t+1}$ 和下一个状态 $S_{t+1}$。更新规则是：\n$$\nQ(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha_t(S_t, A_t) \\left[ \\underbrace{R_{t+1} + \\gamma \\max_{a} Q(S_{t+1}, a)}_{\\text{TD 目标}} - Q(S_t, A_t) \\right]\n$$\n这里，$\\alpha_t(S_t, A_t)$ 是在步骤 $t$ 时对 $(S_t, A_t)$ 的学习率。为了使算法收敛，学习率必须满足 Robbins-Monro 条件，本问题中通过基于访问次数 $N(s, a)$ 的递减调度来近似：\n$$\n\\alpha(s, a) = \\frac{\\alpha_0}{1 + N(s, a)}\n$$\n其中给定了基础学习率 $\\alpha_0$。\n\nQ-learning 是一种离策略 (off-policy) 算法。这意味着它可以在遵循一个不同的、探索性策略的同时学习最优策略。问题指定了一个 $\\varepsilon$-贪心行为策略。在每一步 $t$，智能体以概率 $\\varepsilon(t)$ 选择一个随机动作，并以概率 $1 - \\varepsilon(t)$ 选择贪心动作 $a = \\arg\\max_{a'} Q(S_t, a')$。探索率 $\\varepsilon(t)$ 在总训练步数 $T = E \\times H$ 内从一个起始值 $\\varepsilon_{\\text{start}}$ 线性衰减到一个最小值 $\\varepsilon_{\\min}$：\n$$\n\\varepsilon(t) = \\max\\left\\{\\varepsilon_{\\min}, \\varepsilon_{\\text{start}} + \\frac{t}{T-1}\\left(\\varepsilon_{\\min} - \\varepsilon_{\\text{start}}\\right)\\right\\}\n$$\n这确保了在训练开始时有足够的探索，并随着 $Q$-值估计变得更加可靠而逐渐转向利用。\n\n实现将首先初始化一个 $Q$-表 $Q(s, a) \\in \\mathbb{R}^{3 \\times 3}$ 为全零。一个访问计数矩阵 $N(s, a) \\in \\mathbb{Z}^{3 \\times 3}$ 也被初始化为零。训练将进行 $E$ 个回合，每个回合持续 $H$ 步。一个固定种子为 $7$ 的伪随机数生成器将用于所有随机元素：初始状态抽样、$\\varepsilon$-贪心动作选择以及从转移概率分布 $P(\\cdot|s, a)$ 中抽样下一状态。\n\n在每个测试用例的训练完成后，最终学习到的 $Q$-表被用来推导纯贪心策略 $\\pi(s)$：\n$$\n\\pi(s) = \\arg\\max_{a \\in A} Q(s, a)\n$$\n问题指定了一个平局打破规则：如果对于给定状态，多个动作产生相同的最大 $Q$-值，则选择索引最小的动作。在 $\\varepsilon$-贪心策略的利用阶段和最终策略的推导过程中，都将严格遵守此规则。该算法将针对指定的四个测试用例分别实现，并报告最终得到的策略。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the Q-learning problem for four test cases and prints the results.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case A\n        {\n            \"name\": \"A\",\n            \"gamma\": 0.95,\n            \"E\": 10000,\n            \"H\": 40,\n            \"eps_start\": 0.8,\n            \"eps_min\": 0.05,\n            \"alpha_0\": 0.5,\n            \"R\": np.array([\n                [0.8, -0.3, 0.1],\n                [-0.6, 0.5, 0.05],\n                [0.2, 0.3, 0.02]\n            ]),\n            \"P\": np.array([\n                [0.85, 0.05, 0.10],\n                [0.05, 0.80, 0.15],\n                [0.20, 0.20, 0.60]\n            ]),\n            \"action_dependent_P\": False,\n        },\n        # Case B\n        {\n            \"name\": \"B\",\n            \"gamma\": 0.0,\n            \"E\": 2000,\n            \"H\": 30,\n            \"eps_start\": 0.8,\n            \"eps_min\": 0.1,\n            \"alpha_0\": 0.5,\n            \"R\": np.array([\n                [0.8, -0.3, 0.1],\n                [-0.6, 0.5, 0.05],\n                [0.2, 0.3, 0.02]\n            ]),\n            \"P\": np.array([\n                [0.85, 0.05, 0.10],\n                [0.05, 0.80, 0.15],\n                [0.20, 0.20, 0.60]\n            ]),\n            \"action_dependent_P\": False,\n        },\n        # Case C\n        {\n            \"name\": \"C\",\n            \"gamma\": 0.95,\n            \"E\": 10000,\n            \"H\": 40,\n            \"eps_start\": 0.8,\n            \"eps_min\": 0.05,\n            \"alpha_0\": 0.5,\n            \"R\": np.array([\n                [0.8, -0.3, 0.1],\n                [-0.6, 0.5, 0.05],\n                [0.3, 0.3, 0.02]\n            ]),\n            \"P\": np.array([\n                [0.85, 0.05, 0.10],\n                [0.05, 0.80, 0.15],\n                [0.20, 0.20, 0.60]\n            ]),\n            \"action_dependent_P\": False,\n        },\n        # Case D\n        {\n            \"name\": \"D\",\n            \"gamma\": 0.99,\n            \"E\": 20000,\n            \"H\": 50,\n            \"eps_start\": 0.9,\n            \"eps_min\": 0.05,\n            \"alpha_0\": 0.5,\n            \"R\": np.array([\n                [0.7, 0.2, 0.1],\n                [-0.1, 0.05, 0.02],\n                [0.25, 0.35, 0.02]\n            ]),\n            \"P\": np.array([\n                # P for action 0\n                [[0.92, 0.03, 0.05], [0.99, 0.005, 0.005], [0.50, 0.10, 0.40]],\n                # P for action 1\n                [[0.80, 0.05, 0.15], [0.05, 0.94, 0.01], [0.40, 0.20, 0.40]],\n                # P for action 2\n                [[0.70, 0.10, 0.20], [0.01, 0.98, 0.01], [0.10, 0.30, 0.60]],\n            ]),\n            \"action_dependent_P\": True,\n        }\n    ]\n\n    all_results = []\n    \n    for case in test_cases:\n        # Initialize RNG with fixed seed for reproducibility for each case\n        rng = np.random.default_rng(seed=7)\n        \n        # Extract parameters\n        gamma = case[\"gamma\"]\n        E = case[\"E\"]\n        H = case[\"H\"]\n        eps_start = case[\"eps_start\"]\n        eps_min = case[\"eps_min\"]\n        alpha_0 = case[\"alpha_0\"]\n        R = case[\"R\"]\n        P = case[\"P\"]\n        action_dependent_P = case[\"action_dependent_P\"]\n\n        num_states = 3\n        num_actions = 3\n        \n        # Initialize Q-table and visit counts\n        q_table = np.zeros((num_states, num_actions))\n        visit_counts = np.zeros((num_states, num_actions))\n        \n        total_steps = E * H\n        global_step_count = 0\n\n        # Training loop\n        for episode in range(E):\n            current_state = rng.choice(num_states)\n            \n            for step in range(H):\n                # Epsilon calculation\n                if total_steps > 1:\n                    epsilon = eps_start + (global_step_count / (total_steps - 1)) * (eps_min - eps_start)\n                    epsilon = max(eps_min, epsilon)\n                else:\n                    epsilon = eps_start\n\n                # Action selection (epsilon-greedy)\n                if rng.random()  epsilon:\n                    action = rng.choice(num_actions)\n                else:\n                    # Exploit: choose best action, np.argmax breaks ties by taking the first one\n                    action = np.argmax(q_table[current_state, :])\n                \n                # Get reward\n                reward = R[current_state, action]\n                \n                # Get next state from transition dynamics\n                if action_dependent_P:\n                    transition_probs = P[action, current_state, :]\n                else:\n                    transition_probs = P[current_state, :]\n                next_state = rng.choice(num_states, p=transition_probs)\n                \n                # Q-learning update\n                visit_counts[current_state, action] += 1\n                alpha = alpha_0 / (1 + visit_counts[current_state, action])\n                \n                max_next_q = np.max(q_table[next_state, :])\n                td_target = reward + gamma * max_next_q\n                td_error = td_target - q_table[current_state, action]\n                \n                q_table[current_state, action] += alpha * td_error\n                \n                # Move to next state\n                current_state = next_state\n                global_step_count += 1\n\n        # Derive final greedy policy\n        final_policy = []\n        for s in range(num_states):\n            # np.argmax handles the tie-breaking rule (select smallest index)\n            best_action = np.argmax(q_table[s, :])\n            final_policy.append(int(best_action))\n            \n        all_results.append(final_policy)\n\n    # Final print statement in the exact required format.\n    # The str representation of a list of lists is already in the right format.\n    # We just need to remove spaces.\n    print(str(all_results).replace(\" \", \"\"))\n\nsolve()\n```", "id": "2371418"}]}