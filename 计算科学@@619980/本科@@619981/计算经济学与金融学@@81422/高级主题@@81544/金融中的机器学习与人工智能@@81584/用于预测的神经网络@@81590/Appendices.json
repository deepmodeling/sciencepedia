{"hands_on_practices": [{"introduction": "循环神经网络（RNN）通过维持一个作为过去信息记忆的“隐藏状态”来处理序列。这个状态在每个时间步更新，使得网络能够捕捉时间上的依赖关系。本练习 ([@problem_id:2387285]) 提供了一个具体的 RNN 前向传播计算实践，你将通过手动追踪输入特征如何更新隐藏状态并最终导出一个预测结果，从而对这些强大的模型如何处理时间序列数据获得基础性的理解。", "problem": "考虑一个金融背景下的二元预测任务。对于每只股票，您在一个固定的时间范围内观察到一个离散时间序列的互动特征。在每个时间步 $t \\in \\{1,\\dots,T\\}$，特征向量为 $x_t \\in \\mathbb{R}^2$，其分量为 $x_t = (v_t, s_t)$，其中 $v_t$ 是一个标准化的评论速度度量，$s_t$ 是一个标准化的情绪得分。目标是为每个序列生成一个在 $(0,1)$ 区间内的单一概率，该概率表示该股票在下一个时间步将转变为高关注度状态，此处解读为一次潜在的“网红股”事件。\n\n您获得一个定义如下的预测规则。设隐藏状态维度为 $d = 3$，参数如下\n$$\nW_{xh} = \\begin{bmatrix}\n0.8  -0.4 \\\\\n0.3  0.5 \\\\\n-0.2  0.7\n\\end{bmatrix},\\quad\nW_{hh} = \\begin{bmatrix}\n0.1  0.0  -0.3 \\\\\n0.2  0.4  0.0 \\\\\n-0.5  0.1  0.2\n\\end{bmatrix},\\quad\nb_h = \\begin{bmatrix}\n0.05 \\\\ -0.1 \\\\ 0.0\n\\end{bmatrix},\n$$\n$$\nW_{hy} = \\begin{bmatrix}\n0.6  -0.2  0.4\n\\end{bmatrix},\\quad\nb_y = -0.05.\n$$\n定义隐藏状态递归，初始条件为 $h_0 = \\mathbf{0} \\in \\mathbb{R}^3$，且对于 $t=1,\\dots,T$，\n$$\nh_t = \\tanh\\!\\left(W_{xh}\\,x_t + W_{hh}\\,h_{t-1} + b_h\\right),\n$$\n其中 $\\tanh(\\cdot)$ 逐元素作用。处理完整个序列后，构成对数几率 (logit)\n$$\nz = W_{hy}\\,h_T + b_y,\n$$\n以及预测概率\n$$\np = \\sigma(z) = \\frac{1}{1 + e^{-z}}.\n$$\n\n测试套件。对于以下每个序列，您必须根据上述规则计算出相应的概率 $p$。\n\n- 情况 A（典型的互动上升和积极情绪；$T=4$）：\n  $$\n  \\left[(0.1, 0.4),\\ (0.2, 0.5),\\ (0.3, 0.6),\\ (0.4, 0.7)\\right].\n  $$\n\n- 情况 B（围绕零的近中性动态；$T=4$）：\n  $$\n  \\left[(0.0, 0.0),\\ (0.0, 0.1),\\ (0.0, -0.1),\\ (0.0, 0.0)\\right].\n  $$\n\n- 情况 C（高速度伴随负面情绪；$T=4$）：\n  $$\n  \\left[(0.5, -0.6),\\ (0.6, -0.5),\\ (0.7, -0.4),\\ (0.8, -0.3)\\right].\n  $$\n\n- 情况 D（振荡特征，包括负速度；$T=4$）：\n  $$\n  \\left[(-0.2, 0.9),\\ (0.2, -0.9),\\ (-0.1, 0.8),\\ (0.1, -0.8)\\right].\n  $$\n\n您的程序必须输出一行，其中包含按顺序排列的对应于情况 A、B、C 和 D 的四个概率的列表。每个概率必须四舍五入到恰好 $6$ 位小数。输出格式必须是方括号括起来的逗号分隔列表，例如：\n$$\n[\\text{p\\_A},\\text{p\\_B},\\text{p\\_C},\\text{p\\_D}]\n$$\n其中 $\\text{p\\_A}$、$\\text{p\\_B}$、$\\text{p\\_C}$ 和 $\\text{p\\_D}$ 分别是情况 A 到 D 的概率 $p$ 的四舍五入后的小数表示。", "solution": "问题陈述已经过验证，并被认定为有效。它具有科学依据，定义明确，客观且自洽。它描述了一个标准的离散时间循环神经网络（RNN），这是计算科学中的一个基本模型，并为确定性计算提供了所有必需的参数、初始条件和输入数据。其中没有矛盾、歧义或违反科学原则之处。因此，我们可以着手求解。\n\n该问题要求基于一个特征向量序列来计算预测概率 $p$。该模型是一个隐藏状态维度为 $d=3$ 的简单 RNN。对于从 $1$ 到固定范围 $T$ 的每个离散时间步 $t$，都会处理一个输入特征向量 $x_t \\in \\mathbb{R}^2$。该特征向量由两个分量组成，$x_t = (v_t, s_t)$，分别代表评论速度和情绪得分。\n\n模型的核心是隐藏状态递归。隐藏状态向量 $h_t \\in \\mathbb{R}^3$ 随时间根据以下方程演化：\n$$\nh_t = \\tanh\\!\\left(W_{xh}\\,x_t + W_{hh}\\,h_{t-1} + b_h\\right)\n$$\n这个计算对 $t = 1, \\dots, T$ 执行。初始隐藏状态给定为零向量，$h_0 = \\mathbf{0} \\in \\mathbb{R}^3$。函数 $\\tanh(\\cdot)$ 是双曲正切函数，逐元素地应用于向量参数。参数是权重矩阵 $W_{xh} \\in \\mathbb{R}^{3 \\times 2}$ 和 $W_{hh} \\in \\mathbb{R}^{3 \\times 3}$，以及一个偏置向量 $b_h \\in \\mathbb{R}^{3}$。它们的值提供如下：\n$$\nW_{xh} = \\begin{bmatrix}\n0.8  -0.4 \\\\\n0.3  0.5 \\\\\n-0.2  0.7\n\\end{bmatrix},\\quad\nW_{hh} = \\begin{bmatrix}\n0.1  0.0  -0.3 \\\\\n0.2  0.4  0.0 \\\\\n-0.5  0.1  0.2\n\\end{bmatrix},\\quad\nb_h = \\begin{bmatrix}\n0.05 \\\\ -0.1 \\\\ 0.0\n\\end{bmatrix}\n$$\n\n在处理完长度为 $T$ 的整个输入序列后，最终的隐藏状态 $h_T$ 被用来计算一个标量对数几率值 $z$。这代表了最终激活函数的输入。对数几率的计算方式是：\n$$\nz = W_{hy}\\,h_T + b_y\n$$\n其中 $W_{hy} \\in \\mathbb{R}^{1 \\times 3}$ 是一个权重矩阵（或向量），$b_y \\in \\mathbb{R}$ 是一个标量偏置。提供的值如下：\n$$\nW_{hy} = \\begin{bmatrix}\n0.6  -0.2  0.4\n\\end{bmatrix},\\quad\nb_y = -0.05\n$$\n\n最后，通过将 sigmoid 函数 $\\sigma(\\cdot)$ 应用于对数几率 $z$，得到预测概率 $p$。sigmoid 函数将对数几率压缩到区间 $(0, 1)$ 内，这对于概率是必需的。\n$$\np = \\sigma(z) = \\frac{1}{1 + e^{-z}}\n$$\n\n每个测试用例的计算过程如下：\n$1$. 初始化隐藏状态为零向量：$h \\leftarrow \\begin{bmatrix} 0  0  0 \\end{bmatrix}^T$。\n$2$. 对于从 $1$ 到 $T=4$ 的每个时间步 $t$：\n    a. 从给定序列中取输入向量 $x_t$。\n    b. 计算激活函数的参数：$u_t = W_{xh}\\,x_t + W_{hh}\\,h + b_h$。\n    c. 更新隐藏状态：$h \\leftarrow \\tanh(u_t)$。\n$3$. 循环完成后，$h$ 现在持有 $h_T$ 的值。\n$4$. 计算对数几率：$z = W_{hy}\\,h + b_y$。\n$5$. 计算最终概率：$p = 1 / (1 + \\exp(-z))$。\n\n此确定性过程应用于四个指定的输入序列（情况 A、B、C 和 D），每个序列的长度为 $T=4$。然后将得到的四个概率收集起来并按要求格式化。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes forecast probabilities for four equity engagement sequences using a\n    specified Recurrent Neural Network (RNN) model.\n    \"\"\"\n    \n    # Define the parameters of the RNN model.\n    # All vectors are defined as column vectors for correct matrix algebra.\n    W_xh = np.array([\n        [0.8, -0.4],\n        [0.3, 0.5],\n        [-0.2, 0.7]\n    ])\n    W_hh = np.array([\n        [0.1, 0.0, -0.3],\n        [0.2, 0.4, 0.0],\n        [-0.5, 0.1, 0.2]\n    ])\n    b_h = np.array([[0.05], [-0.1], [0.0]])\n    W_hy = np.array([[0.6, -0.2, 0.4]])\n    b_y = -0.05\n\n    # Define the test cases from the problem statement.\n    # The order of cases is fixed to A, B, C, D.\n    test_cases = [\n        # Case A: Typical rising engagement and positive sentiment\n        [(0.1, 0.4), (0.2, 0.5), (0.3, 0.6), (0.4, 0.7)],\n        # Case B: Near-neutral dynamics around zero\n        [(0.0, 0.0), (0.0, 0.1), (0.0, -0.1), (0.0, 0.0)],\n        # Case C: High velocity with negative sentiment\n        [(0.5, -0.6), (0.6, -0.5), (0.7, -0.4), (0.8, -0.3)],\n        # Case D: Oscillatory features\n        [(-0.2, 0.9), (0.2, -0.9), (-0.1, 0.8), (0.1, -0.8)]\n    ]\n\n    results = []\n    \n    # Sigmoid function for final probability calculation.\n    def sigmoid(z):\n        return 1 / (1 + np.exp(-z))\n\n    for sequence in test_cases:\n        # Initialize hidden state h_0 = [0, 0, 0]^T.\n        # Dimension is (3, 1) to represent a column vector.\n        h = np.zeros((3, 1))\n\n        # Iterate through the time steps of the sequence.\n        for x_tuple in sequence:\n            # Reshape input tuple into a (2, 1) column vector.\n            x_t = np.array(x_tuple).reshape(2, 1)\n            \n            # Apply the RNN recurrence relation.\n            # h_t = tanh(W_xh * x_t + W_hh * h_{t-1} + b_h)\n            h = np.tanh(W_xh @ x_t + W_hh @ h + b_h)\n        \n        # After the loop, h is the final hidden state h_T.\n        # Compute the logit z = W_hy * h_T + b_y.\n        # The result of matrix multiplication is a 1x1 array; .item() extracts the scalar.\n        z = (W_hy @ h + b_y).item()\n        \n        # Compute the final probability p = sigma(z).\n        p = sigmoid(z)\n        \n        results.append(p)\n\n    # Format the results to exactly 6 decimal places and create the output string.\n    formatted_results = [f\"{res:.6f}\" for res in results]\n    \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "2387285"}, {"introduction": "正则化是一种通过惩罚过大的模型权重来防止过拟合的技术。$L_1$ 和 $L_2$ 正则化是两种常见类型，但效果不同：$L_2$（“岭回归”）倾向于收缩权重，而 $L_1$（“Lasso”）则能将某些权重精确地缩减至零，从而实现特征选择。在本练习 ([@problem_id:2414325]) 中，你将实现并比较 $L_1$ 和 $L_2$ 正则化在一个线性预测模型中的效果，这将让你直观地了解正则化如何影响模型权重，并帮助创建更简洁、更具可解释性的模型——这在金融建模中至关重要，因为理解关键驱动因素与预测准确性同等重要。", "problem": "要求您在一个受神经网络启发的用于公司收益的线性预测模型中，形式化并实现一个关于 $L_1$ 与 $L_2$ 正则化对特征稀疏性影响的受控比较。该模型是一个单层线性网络（一个线性神经元），它是神经网络的一个特例，在此用于预测一个代表归一化公司收益的标准化目标。该设计的基础是在凸正则化下的带有均方误差的经验风险最小化。请从以下基础开始。\n\n1. 带均方误差的经验风险最小化：给定输入矩阵 $X \\in \\mathbb{R}^{n \\times d}$ 和目标向量 $y \\in \\mathbb{R}^n$，选择权重 $w \\in \\mathbb{R}^d$ 以最小化\n$$\n\\frac{1}{2n} \\lVert y - X w \\rVert_2^2 + \\lambda \\, \\mathcal{R}(w),\n$$\n其中 $n$ 是样本数，$d$ 是特征数，$\\lambda \\ge 0$ 是正则化强度，$\\mathcal{R}$ 是一个惩罚泛函。\n\n2. $L_1$ 和 $L_2$ 正则化的定义：对于 $L_1$，定义 $\\mathcal{R}(w) = \\lVert w \\rVert_1 = \\sum_{j=1}^d \\lvert w_j \\rvert$；对于 $L_2$，定义 $\\mathcal{R}(w) = \\frac{1}{2} \\lVert w \\rVert_2^2 = \\frac{1}{2}\\sum_{j=1}^d w_j^2$。\n\n3. 特征稀疏性通过支撑集大小来量化，定义为满足 $\\lvert w_j \\rvert$ 超过一个小的数值阈值 $\\tau  0$ 的索引 $j \\in \\{1,\\dots,d\\}$ 的数量。\n\n构建一个确定性设计，该设计通过使用平滑变化且相关的基函数来模拟常用于收益预测的金融预测变量。\n\nA. 数据构建（确定性，无随机性）：\n- 设 $n = 64$ 且 $t = 0,1,2,\\dots,n-1$。\n- 使用三角函数定义四个基础预测变量：\n  - $b_1(t) = \\cos\\!\\left(2\\pi \\cdot 1 \\cdot t / n\\right)$，\n  - $b_2(t) = \\sin\\!\\left(2\\pi \\cdot 1 \\cdot t / n\\right)$，\n  - $b_3(t) = \\cos\\!\\left(2\\pi \\cdot 2 \\cdot t / n\\right)$，\n  - $b_4(t) = \\sin\\!\\left(2\\pi \\cdot 2 \\cdot t / n\\right)$。\n- 将每个 $b_j$ 标准化为零均值和单位方差，以获得 $s_j$，$j \\in \\{1,2,3,4\\}$。\n- 将目标（归一化公司收益）定义为一个线性组合：\n  $$\n  y = 0.8\\, s_1 + 1.2\\, s_3 - 1.5\\, s_4.\n  $$\n- 为引入真实的多元共线性，定义六个额外的预测变量作为标准化基的混合，并加入一个小的正交扰动。设 $q(t) = \\cos\\!\\left(2\\pi \\cdot 5 \\cdot t / n\\right)$ 并将其标准化为 $s_q$。然后定义：\n  - $f_1 = s_1$，\n  - $f_2 = s_2$，\n  - $f_3 = s_3$，\n  - $f_4 = s_4$，\n  - $f_5 = 0.8\\, s_1 + 0.2\\, s_2 + 0.01\\, s_q$，\n  - $f_6 = 0.5\\, s_2 - 0.3\\, s_3 + 0.01\\, s_q$，\n  - $f_7 = 0.1\\, s_1 + 0.9\\, s_4 + 0.01\\, s_q$，\n  - $f_8 = 0.3\\, s_3 + 0.4\\, s_4 + 0.3\\, s_2 + 0.01\\, s_q$，\n  - $f_9 = 0.6\\, s_1 - 0.1\\, s_3 + 0.01\\, s_q$，\n  - $f_{10} = 0.2\\, s_2 + 0.2\\, s_3 + 0.6\\, s_4 + 0.01\\, s_q$。\n- 将每个 $f_j$ 标准化为零均值和单位方差，以形成最终的设计列 $X_{\\cdot j}$，$j \\in \\{1,\\dots,10\\}$。将 $y$ 标准化为零均值（它已经是了，但需在数值上强制执行）。这将得到 $X \\in \\mathbb{R}^{64 \\times 10}$ 和 $y \\in \\mathbb{R}^{64}$。\n\nB. 需要实现的求解器：\n- 对于 $L_2$ 正则化（岭回归），通过求解线性系统以闭式解形式求解一阶最优性条件：\n$$\n\\left(\\frac{1}{n} X^\\top X + \\lambda I_d \\right) w = \\frac{1}{n} X^\\top y,\n$$\n其中 $I_d$ 是 $d \\times d$ 的单位矩阵。\n- 对于 $L_1$ 正则化（lasso），实现带软阈值处理的循环坐标下降。对每个坐标 $j \\in \\{1,\\dots,d\\}$，定义\n$$\nH_j = \\frac{1}{n}\\sum_{i=1}^n X_{ij}^2 \\quad \\text{和} \\quad \\rho_j = \\frac{1}{n} \\sum_{i=1}^n X_{ij} \\left( y_i - \\sum_{k \\ne j} X_{ik} w_k \\right),\n$$\n并更新\n$$\nw_j \\leftarrow \\frac{\\operatorname{soft}(\\rho_j,\\lambda)}{H_j}, \\quad \\text{其中} \\quad \\operatorname{soft}(a,\\lambda) = \\operatorname{sign}(a)\\,\\max\\{ \\lvert a \\rvert - \\lambda, \\, 0 \\}.\n$$\n循环迭代坐标，直到 $w$ 在一次完整遍历中的 $\\ell_\\infty$ 范数变化低于某个容差 $\\varepsilon  0$。\n\nC. 特征稀疏性度量：\n- 将支撑集阈值定义为 $\\tau = 10^{-6}$。\n- 支撑集大小 $s(w)$ 是满足 $\\lvert w_j \\rvert  \\tau$ 的索引 $j$ 的数量。\n\n您必须：\n1. 按规定构建 $X$ 和 $y$。\n2. 按上述方法实现 $L_2$ (岭回归) 求解器和 $L_1$ (lasso) 求解器。\n3. 对于下面的每个测试用例，计算学习到的 $w$ 并报告 $s(w)$。\n\n测试套件（五个用例）：\n- 用例 1：$L_2$，$\\lambda = 0.0$。\n- 用例 2：$L_2$，$\\lambda = 10.0$。\n- 用例 3：$L_1$，$\\lambda = 0.0$。\n- 用例 4：$L_1$，$\\lambda = 0.5$。\n- 用例 5：$L_1$，$\\lambda = 2.2$。\n\n最终输出格式要求：\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，结果顺序与测试用例相同。具体输出格式为：\n$$\n[ s(w^{(1)}), \\, s(w^{(2)}), \\, s(w^{(3)}), \\, s(w^{(4)}), \\, s(w^{(5)}) ].\n$$\n只打印此列表，不含任何附加文本。角度以弧度为单位；无物理单位适用；所有数值答案必须为整数。程序必须是完全确定性的，并且不需要用户输入。", "solution": "所呈现的问题陈述是一项在计算统计学及其在金融建模中应用的表述清晰且科学合理练习。它自成体系，定义了所有必要的参数和流程，并且没有任何矛盾或含糊之处。因此，该问题被认为是 **有效的**，我将继续提供完整解答。\n\n该任务是比较 $L_1$ 与 $L_2$ 正则化在线性预测模型中引发稀疏性的效果。这将通过构建一个具有受控多元共线性的合成数据集，求解正则化回归问题，并量化所得权重向量的稀疏性来完成。\n\n**A 部分：数据构建**\n\n第一步是确定性地生成设计矩阵 $X \\in \\mathbb{R}^{64 \\times 10}$ 和目标向量 $y \\in \\mathbb{R}^{64}$。样本数量为 $n=64$，特征数量为 $d=10$。\n\n1.  定义一个时间索引向量 $t = [0, 1, \\dots, n-1]$。\n2.  使用三角函数在离散时间索引 $t$ 上生成四个正交的基础预测变量：\n    $$ b_1(t) = \\cos(2\\pi t / n), \\quad b_2(t) = \\sin(2\\pi t / n), \\quad b_3(t) = \\cos(4\\pi t / n), \\quad b_4(t) = \\sin(4\\pi t / n) $$\n3.  每个基础预测变量向量 $b_j$ 都被标准化为零均值和单位方差。一个向量 $v$ 的标准化由 $(v - \\mu_v) / \\sigma_v$ 给出，其中 $\\mu_v$ 是均值，$\\sigma_v$ 是总体标准差。将标准化后的向量记为 $s_1, s_2, s_3, s_4$。\n4.  目标向量 $y$ 代表归一化的公司收益，被构建为这三个标准化基的已知线性组合：\n    $$ y = 0.8 s_1 + 1.2 s_3 - 1.5 s_4 $$\n    根据构造，$y$ 的均值为零，因此不需要进一步的均值中心化。\n5.  为了模拟经济数据中常见的多元共线性，构建了十个特征 $f_1, \\dots, f_{10}$。前四个是基础预测变量本身。随后的六个是基的线性组合，并添加了一个小的正交扰动以防止完全的线性依赖。扰动来自于 $q(t) = \\cos(10\\pi t / n)$，它被标准化为一个向量 $s_q$。特征定义如下：\n$$\n\\begin{align*}\n    f_1 = s_1 \\\\\n    f_2 = s_2 \\\\\n    f_3 = s_3 \\\\\n    f_4 = s_4 \\\\\n    f_5 = 0.8 s_1 + 0.2 s_2 + 0.01 s_q \\\\\n    f_6 = 0.5 s_2 - 0.3 s_3 + 0.01 s_q \\\\\n    f_7 = 0.1 s_1 + 0.9 s_4 + 0.01 s_q \\\\\n    f_8 = 0.3 s_3 + 0.4 s_4 + 0.3 s_2 + 0.01 s_q \\\\\n    f_9 = 0.6 s_1 - 0.1 s_3 + 0.01 s_q \\\\\n    f_{10} = 0.2 s_2 + 0.2 s_3 + 0.6 s_4 + 0.01 s_q\n\\end{align*}\n$$\n6.  最后，每个特征向量 $f_j$ 都被标准化为零均值和单位方差。这十个标准化向量构成了最终设计矩阵 $X$ 的列。\n\n**B 部分：求解器**\n\n该问题要求为一般的经验风险最小化问题实现两个求解器：\n$$ \\min_{w \\in \\mathbb{R}^d} \\left\\{ \\frac{1}{2n} \\lVert y - X w \\rVert_2^2 + \\lambda \\mathcal{R}(w) \\right\\} $$\n\n**$L_2$ 正则化（岭回归）**\n\n对于 $L_2$ 正则化，惩罚项为 $\\mathcal{R}(w) = \\frac{1}{2} \\lVert w \\rVert_2^2$。对于任何 $\\lambda > 0$，目标函数都是严格凸的，其梯度为：\n$$ \\nabla_w J(w) = \\frac{1}{n} X^\\top(Xw - y) + \\lambda w $$\n将梯度设为零，$\\nabla_w J(w) = 0$，得到一阶最优性条件：\n$$ \\frac{1}{n} X^\\top X w - \\frac{1}{n} X^\\top y + \\lambda w = 0 $$\n$$ \\left( \\frac{1}{n} X^\\top X + \\lambda I_d \\right) w = \\frac{1}{n} X^\\top y $$\n这是问题陈述中指定的线性系统，其中 $I_d$ 是 $d \\times d$ 的单位矩阵。该系统可以使用标准的线性代数程序直接求解。对于 $\\lambda = 0$ 的情况，这简化为普通最小二乘法 (OLS) 的正规方程。\n\n**$L_1$ 正则化 (Lasso)**\n\n对于 $L_1$ 正则化，惩罚项为 $\\mathcal{R}(w) = \\lVert w \\rVert_1 = \\sum_{j=1}^d |w_j|$。目标函数是凸的，但在任何 $w_j = 0$ 的点上不可微。使用迭代方法，特别是循环坐标下降法，来找到解。\n\n该算法一次更新一个权重坐标 $w_j$，同时保持所有其他权重 $w_k$ ($k \\neq j$) 固定。坐标 $j$ 的次梯度最优性条件导出更新规则：\n$$ w_j \\leftarrow \\frac{\\operatorname{soft}(\\rho_j, \\lambda)}{H_j} $$\n其中软阈值算子为 $\\operatorname{soft}(a, \\lambda) = \\operatorname{sign}(a) \\max(|a| - \\lambda, 0)$，且\n$$ \\rho_j = \\frac{1}{n} \\sum_{i=1}^n X_{ij} \\left( y_i - \\sum_{k \\ne j} X_{ik} w_k \\right), \\quad H_j = \\frac{1}{n}\\sum_{i=1}^n X_{ij}^2 $$\n由于设计矩阵 $X$ 的每一列都标准化为单位方差，因此对于所有 $j \\in \\{1, \\dots, d\\}$，$H_j = 1$。更新规则简化为：\n$$ w_j \\leftarrow \\operatorname{soft}(\\rho_j, \\lambda) $$\n算法重复地循环遍历所有坐标 $j=1, \\dots, d$，直到连续两次完整遍历之间权重向量 $w$ 的变化量低于一个小的容差 $\\varepsilon$。在此实现中，对 $w$ 变化的$\\ell_\\infty$范数使用 $\\varepsilon = 10^{-9}$ 的容差。对于 $\\lambda = 0$ 的情况，软阈值算子变为恒等函数，即 $\\operatorname{soft}(\\rho_j, 0) = \\rho_j$，算法变为求解 OLS 正规方程的 Gauss-Seidel 方法。\n\n**C 部分：特征稀疏性**\n\n稀疏性通过权重向量 $w$ 的支撑集大小来衡量。支撑集大小 $s(w)$ 是其绝对值大于数值阈值 $\\tau = 10^{-6}$ 的权重数量：\n$$ s(w) = |\\{j \\in \\{1, \\dots, d\\} : |w_j| > \\tau \\}| $$\n这个度量将为从五个指定测试用例中获得的解向量 $w$ 分别计算。\n\n现在的实现将首先生成数据，然后对每个测试用例应用指定的求解器，最后计算每个所得权重向量的支撑集大小。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Formalizes and implements a controlled comparison of L1 and L2 regularization\n    on feature sparsity in a linear forecasting model.\n    \"\"\"\n\n    # ------------------\n    # --- Parameters ---\n    # ------------------\n    N_SAMPLES = 64\n    N_FEATURES = 10\n    SPARSITY_THRESHOLD = 1e-6\n    LASSO_TOLERANCE = 1e-9\n    LASSO_MAX_ITER = 5000\n\n    # ----------------------------\n    # --- Part A: Data Construction ---\n    # ----------------------------\n\n    def standardize(v):\n        \"\"\"Standardizes a vector to have zero mean and unit variance.\"\"\"\n        # Using ddof=0 for population standard deviation, as is standard in ML.\n        return (v - v.mean()) / v.std(ddof=0)\n\n    t = np.arange(N_SAMPLES)\n\n    # Base predictors\n    b1 = np.cos(2 * np.pi * 1 * t / N_SAMPLES)\n    b2 = np.sin(2 * np.pi * 1 * t / N_SAMPLES)\n    b3 = np.cos(2 * np.pi * 2 * t / N_SAMPLES)\n    b4 = np.sin(2 * np.pi * 2 * t / N_SAMPLES)\n\n    # Standardized bases\n    s1 = standardize(b1)\n    s2 = standardize(b2)\n    s3 = standardize(b3)\n    s4 = standardize(b4)\n\n    # Target vector\n    y = 0.8 * s1 + 1.2 * s3 - 1.5 * s4\n    # Enforce zero mean numerically, as specified.\n    y = y - y.mean()\n\n    # Perturbation vector\n    q = np.cos(2 * np.pi * 5 * t / N_SAMPLES)\n    s_q = standardize(q)\n\n    # Feature construction\n    f_vectors = [\n        s1,\n        s2,\n        s3,\n        s4,\n        0.8 * s1 + 0.2 * s2 + 0.01 * s_q,\n        0.5 * s2 - 0.3 * s3 + 0.01 * s_q,\n        0.1 * s1 + 0.9 * s4 + 0.01 * s_q,\n        0.3 * s3 + 0.4 * s4 + 0.3 * s2 + 0.01 * s_q,\n        0.6 * s1 - 0.1 * s3 + 0.01 * s_q,\n        0.2 * s2 + 0.2 * s3 + 0.6 * s4 + 0.01 * s_q,\n    ]\n\n    # Final design matrix X\n    X = np.zeros((N_SAMPLES, N_FEATURES))\n    for j, f_vec in enumerate(f_vectors):\n        X[:, j] = standardize(f_vec)\n\n    # -----------------------\n    # --- Part B: Solvers ---\n    # -----------------------\n\n    def ridge_solver(X_mat, y_vec, lam):\n        \"\"\"Solves Ridge regression using the closed-form solution.\"\"\"\n        n, d = X_mat.shape\n        A = (1 / n) * (X_mat.T @ X_mat) + lam * np.identity(d)\n        b = (1 / n) * (X_mat.T @ y_vec)\n        w = np.linalg.solve(A, b)\n        return w\n\n    def soft_threshold(a, lam):\n        \"\"\"Soft-thresholding operator.\"\"\"\n        return np.sign(a) * np.maximum(np.abs(a) - lam, 0)\n    \n    def lasso_solver(X_mat, y_vec, lam):\n        \"\"\"Solves Lasso regression using cyclic coordinate descent.\"\"\"\n        n, d = X_mat.shape\n        w = np.zeros(d)\n        \n        # H_j = (1/n) * sum(X_ij^2). Since columns are standardized to unit variance, H_j = 1.\n        # We can verify this for safety.\n        H_j = np.sum(X_mat**2, axis=0) / n\n\n        for _ in range(LASSO_MAX_ITER):\n            w_old = np.copy(w)\n            for j in range(d):\n                # Calculate rho_j according to the definition\n                # rho_j = (1/n) * X_j^T * (y - sum_{k!=j} X_k w_k)\n                y_pred = X_mat @ w\n                residual_without_j = y_vec - y_pred + X_mat[:, j] * w[j]\n                rho_j = (1 / n) * (X_mat[:, j].T @ residual_without_j)\n                \n                # Update w_j\n                w[j] = soft_threshold(rho_j, lam) / H_j[j]\n\n            if np.linalg.norm(w - w_old, ord=np.inf)  LASSO_TOLERANCE:\n                break\n        return w\n\n    # --------------------------------\n    # --- Part C: Sparsity Metric  Execution ---\n    # --------------------------------\n\n    def calculate_support_size(w_vec):\n        \"\"\"Calculates the support size of a weight vector.\"\"\"\n        return np.sum(np.abs(w_vec)  SPARSITY_THRESHOLD)\n\n    test_cases = [\n        {'type': 'l2', 'lambda': 0.0},\n        {'type': 'l2', 'lambda': 10.0},\n        {'type': 'l1', 'lambda': 0.0},\n        {'type': 'l1', 'lambda': 0.5},\n        {'type': 'l1', 'lambda': 2.2},\n    ]\n\n    results = []\n    for case in test_cases:\n        lam = case['lambda']\n        if case['type'] == 'l2':\n            w_solution = ridge_solver(X, y, lam)\n        else: # type == 'l1'\n            w_solution = lasso_solver(X, y, lam)\n        \n        sparsity = calculate_support_size(w_solution)\n        results.append(sparsity)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2414325"}, {"introduction": "损失函数定义了神经网络在训练过程中试图最小化的目标。虽然像均方误差（Mean Squared Error）这样的标准损失函数很常用，但它们可能无法捕捉金融应用的特定目标，例如正确预测资产价格变动的方向。本练习 ([@problem_id:2414391]) 要求你设计并实现一个自定义的损失函数，该函数会严厉惩罚方向预测错误的模型。这个实践展示了如何将特定领域的知识——在这里是金融领域中方向准确性的重要性——直接嵌入到训练过程中，从而使模型更具实用价值。", "problem": "您正在为训练一个用于计算经济学和金融学中预测资产回报的神经网络设计一个损失函数。其目标是严厉惩罚回报方向预测错误的预测，而不仅仅是误差的大小。我们在由 $t \\in \\{1,\\dots,T\\}$ 索引的离散时间中工作。设实现的简单回报为 $r_t \\in \\mathbb{R}$，模型的预测为 $\\hat{r}_t \\in \\mathbb{R}$。标准基线是均方误差，即经验风险 $(1/T)\\sum_{t=1}^T (r_t - \\hat{r}_t)^2$。\n\n从统计学习中使用的基本原理出发：\n- 一个样本上的经验风险最小化是基于对每个样本损失的平均。\n- 平方损失鼓励精确的量值匹配。\n- 为编码方向准确性，一个基于间隔的惩罚使用乘积 $r_t \\hat{r}_t$ 作为带符号的一致性度量，其中 $r_t \\hat{r}_t  0$ 表示符号一致，而 $r_t \\hat{r}_t  0$ 表示不一致。\n- 当满足间隔阈值时为 $0$，而在违反间隔时线性增长的最小凸非负惩罚，是仿射函数的铰链形式。\n\n您的任务：\n1. 根据上述原理，推导出一个凸的单样本损失 $\\ell_t$。该损失通过一个方向违规惩罚来增强平方误差 $(r_t - \\hat{r}_t)^2$。此惩罚项仅取决于间隔 $r_t \\hat{r}_t$ 和两个非负超参数：一个惩罚权重 $\\lambda \\ge 0$ 和一个非负间隔阈值 $m \\ge 0$。该惩罚必须：\n   - 只要满足间隔要求，即 $r_t \\hat{r}_t \\ge m$，就恒为零。\n   - 只要 $r_t \\hat{r}_t  m$，就是相对于阈值的不足量的线性函数，其斜率为 $\\lambda$。\n   - 对所有 $(r_t,\\hat{r}_t)$ 均为非负。\n   - 使得总的单样本损失 $\\ell_t$ 关于 $\\hat{r}_t$ 是凸的。\n2. 将总体损失定义为样本平均值 $L = \\frac{1}{T}\\sum_{t=1}^T \\ell_t$。\n3. 实现一个程序，为下面这组 $(r,\\hat{r},\\lambda,m)$ 元组测试套件计算 $L$。在每种情况下，$r$ 和 $\\hat{r}$ 是长度为 $T$ 的数组，而 $\\lambda$ 和 $m$ 是标量：\n   - A 例（理想情况，方向和量值均完美）：$r = [0.01,-0.02]$，$\\hat{r} = [0.01,-0.02]$，$\\lambda = 100$，m = 0。\n   - B 例（方向总是错误，零间隔）：$r = [0.01,-0.02]$，$\\hat{r} = [-0.01,0.02]$，$\\lambda = 100$，m = 0。\n   - C 例（边界/间隔测试，小回报和正间隔）：$r = [0.005,-0.003,0.004]$，$\\hat{r} = [0.0,-0.001,0.002]$，$\\lambda = 50$，m = 0.001。\n   - D 例（正确性混合，零间隔）：$r = [0.02,-0.01,-0.015]$，$\\hat{r} = [0.03,0.005,0.010]$，$\\lambda = 200$，m = 0。\n   - E 例（回退到纯平方误差）：$r = [0.01,-0.02,0.03]$，$\\hat{r} = [0.009,-0.018,0.029]$，$\\lambda = 0$，m = 0.005。\n4. 程序必须为每种情况计算标量损失 $L$，并生成单行输出，其中包含五个结果，格式为方括号内用逗号分隔的列表，每个数字四舍五入到 6 位小数，例如 $[x_1,x_2,x_3,x_4,x_5]$。\n\n您的程序必须是一个完整的、可运行的实现，它无需用户输入即可执行这些计算。不涉及物理单位。不使用角度。任何地方都不得使用百分比；当出现可能被非正式地描述为百分比的量时，必须用小数表示。通过将回报视为小的实数来确保科学真实性，认识到方向准确性是通过 $r_t \\hat{r}_t$ 的符号来编码的，并且附加的惩罚会强烈抑制符号错误。", "solution": "所提出的问题是在金融资产回报预测背景下，为训练神经网络设计一个专用损失函数的任务。该问题具有科学依据、论证严谨，且其所有组成部分都清晰界定。这是一个有效的数学工程问题。我将继续进行推导和求解。\n\n目标是为单个时间步长 $t$ 推导一个单样本损失函数，记为 $\\ell_t$。该损失必须用一个针对方向不准确性的惩罚项来增强标准的平方误差。一个大小为 $T$ 的样本的总损失是经验风险，即这些单样本损失的样本平均值。\n\n设 $r_t$ 为实现回报，$\\hat{r}_t$ 为预测回报。在时间 $t$ 的单样本损失 $\\ell_t$ 由两部分组成：用于量值准确性的平方误差项和用于方向准确性的惩罚项，我们记为 $P_t$。\n$$\n\\ell_t(\\hat{r}_t) = (r_t - \\hat{r}_t)^2 + P_t(\\hat{r}_t)\n$$\n\n惩罚项 $P_t$ 必须遵守问题陈述中概述的几个原则。\n1.  它是一个关于由乘积 $r_t \\hat{r}_t$ 定义的间隔、一个惩罚权重 $\\lambda \\ge 0$ 和一个间隔阈值 $m \\ge 0$ 的函数。\n2.  如果满足间隔条件，即 $r_t \\hat{r}_t \\ge m$，则惩罚必须为零。\n3.  如果违反间隔条件，即 $r_t \\hat{r}_t  m$，则惩罚必须是不足量 $m - r_t \\hat{r}_t$ 的线性函数，斜率为 $\\lambda$。\n4.  惩罚必须为非负，即 $P_t \\ge 0$。\n\n这些条件唯一地指定了 $P_t$ 的形式。仅当 $m - r_t \\hat{r}_t  0$ 时，惩罚项非零，且在这种情况下，它等于 $\\lambda (m - r_t \\hat{r}_t)$。对于所有其他情况，它为零。这正是铰链损失的定义，可以使用正部函数 $\\max(0, x)$ 紧凑地写出。\n因此，惩罚项为：\n$$\nP_t(\\hat{r}_t) = \\lambda \\cdot \\max(0, m - r_t \\hat{r}_t)\n$$\n此形式满足所有要求。由于 $\\lambda \\ge 0$ 并且 $\\max$ 函数总是非负的，所以 $P_t \\ge 0$。其他条件由构造满足。\n\n因此，完整的单样本损失函数为：\n$$\n\\ell_t(\\hat{r}_t) = (r_t - \\hat{r}_t)^2 + \\lambda \\cdot \\max(0, m - r_t \\hat{r}_t)\n$$\n一个关键要求是 $\\ell_t$ 必须是关于预测 $\\hat{r}_t$ 的凸函数。凸函数的和也是凸函数。我们分别分析每一项。\n\n第一项，$L_{SE}(\\hat{r}_t) = (r_t - \\hat{r}_t)^2$，是 $\\hat{r}_t$ 的二次函数。其关于 $\\hat{r}_t$ 的二阶导数为 $\\frac{\\partial^2}{\\partial \\hat{r}_t^2} (r_t - \\hat{r}_t)^2 = 2$，该值严格为正。因此，平方误差项是严格凸的。\n\n第二项，$P_t(\\hat{r}_t) = \\lambda \\cdot \\max(0, m - r_t \\hat{r}_t)$，也是凸的。我们可以通过认识到它是一个凸函数 $f(x) = \\max(0,x)$ 与一个关于 $\\hat{r}_t$ 的仿射函数 $g(\\hat{r}_t) = m - r_t \\hat{r}_t$ 的复合来确定这一点。凸函数与仿射映射的复合是凸的。由于非负标量 $\\lambda$ 保持凸性，因此 $P_t(\\hat{r}_t)$ 相对于 $\\hat{r}_t$ 是凸的。\n\n由于 $\\ell_t$ 的两项关于 $\\hat{r}_t$ 都是凸的，它们的和 $\\ell_t(\\hat{r}_t)$ 也是一个凸函数。此性质对于确保最小化损失函数的优化问题是良定的，并且基于梯度的方法将收敛到全局最小值至关重要。\n\n整个样本的总体损失函数 $L$ 是经验风险，定义为 $T$ 个观测值的单样本损失的平均值：\n$$\nL = \\frac{1}{T} \\sum_{t=1}^T \\ell_t = \\frac{1}{T} \\sum_{t=1}^T \\left[ (r_t - \\hat{r}_t)^2 + \\lambda \\cdot \\max(0, m - r_t \\hat{r}_t) \\right]\n$$\n这是待实现的损失函数的最终形式。\n\n对于问题的计算部分，我们将此公式应用于所提供的五个测试用例。对于每个 $(r, \\hat{r}, \\lambda, m)$ 元组，其中 $r$ 和 $\\hat{r}$ 是长度为 $T$ 的向量，计算过程如下：\n1.  对每个 $t \\in \\{1,\\dots,T\\}$，计算逐元素的平方误差 $(r_t - \\hat{r}_t)^2$。\n2.  计算逐元素的间隔乘积 $r_t \\hat{r}_t$。\n3.  计算逐元素的惩罚项 $\\lambda \\cdot \\max(0, m - r_t \\hat{r}_t)$。\n4.  将每个样本点的平方误差和惩罚相加得到 $\\ell_t$。\n5.  计算这些单样本损失的平均值以求得 $L$。\n\n五种情况的计算值如下：\n- A 例：$L_A = 0.000000$\n- B 例：$L_B = 0.026000$\n- C 例：$L_C = 0.049828$\n- D 例：$L_D = 0.013650$\n- E 例：$L_E = 0.000002$\n\n实现将遵循此逻辑以生成所需的输出。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the custom loss function for several test cases in financial forecasting.\n\n    The loss function L is defined as the average of per-sample losses l_t:\n    L = (1/T) * sum_{t=1 to T} l_t\n    where l_t = (r_t - r_hat_t)^2 + lambda * max(0, m - r_t * r_hat_t)\n    - r_t: realized return\n    - r_hat_t: predicted return\n    - lambda: penalty weight\n    - m: margin threshold\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case A: happy path, perfect direction and magnitude\n        {'r': np.array([0.01, -0.02]), 'r_hat': np.array([0.01, -0.02]), 'lam': 100.0, 'm': 0.0},\n        # Case B: direction always wrong, zero margin\n        {'r': np.array([0.01, -0.02]), 'r_hat': np.array([-0.01, 0.02]), 'lam': 100.0, 'm': 0.0},\n        # Case C: boundary/margin test with small returns and positive margin\n        {'r': np.array([0.005, -0.003, 0.004]), 'r_hat': np.array([0.0, -0.001, 0.002]), 'lam': 50.0, 'm': 0.001},\n        # Case D: mixed correctness, zero margin\n        {'r': np.array([0.02, -0.01, -0.015]), 'r_hat': np.array([0.03, 0.005, 0.010]), 'lam': 200.0, 'm': 0.0},\n        # Case E: fallback to pure squared error\n        {'r': np.array([0.01, -0.02, 0.03]), 'r_hat': np.array([0.009, -0.018, 0.029]), 'lam': 0.0, 'm': 0.005}\n    ]\n\n    results = []\n    for case in test_cases:\n        r = case['r']\n        r_hat = case['r_hat']\n        lam = case['lam']\n        m = case['m']\n\n        # Ensure inputs are NumPy arrays for vectorized operations\n        if not isinstance(r, np.ndarray):\n            r = np.array(r)\n        if not isinstance(r_hat, np.ndarray):\n            r_hat = np.array(r_hat)\n\n        # 1. Squared error term\n        squared_error = (r - r_hat)**2\n\n        # 2. Directional penalty term\n        # The margin is the product of the realized and predicted returns\n        margin = r * r_hat\n        # The penalty is applied when the margin is below the threshold m\n        penalty = lam * np.maximum(0, m - margin)\n\n        # 3. Per-sample loss is the sum of the two terms\n        per_sample_loss = squared_error + penalty\n\n        # 4. The overall loss is the mean of the per-sample losses\n        total_loss = np.mean(per_sample_loss)\n\n        # Append the formatted result\n        results.append(f\"{total_loss:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n\n```", "id": "2414391"}]}