{"hands_on_practices": [{"introduction": "我们从构建交易策略的一个基本模块开始：如何执行单笔最优交易。这个练习旨在探讨一个经典权衡问题，即如何在市场预测带来的预期利润和随交易规模增大的执行成本（即滑点）之间找到平衡。通过解决这个单步优化问题，你将为后续构建更复杂的序贯决策策略打下坚实的基础。[@problem_id:2426687]", "problem": "给定一个单步交易决策问题，其中交易代理选择一个带符号的交易规模 $v$（正 $v$ 表示买入，负 $v$ 表示卖出）来最大化一个带惩罚的期望回报。单位即时期望价格变动为 $\\mu$，市场波动率参数为 $\\sigma$。瞬时滑点惩罚被建模为一种单位效应，该效应随波动率和交易规模绝对值的平方根而增加，遵循经验性的平方根冲击模型：单位滑点为 $s_{\\text{per}}(v,\\sigma) = k\\,\\sigma\\,|v|^{1/2}$，其中 $k$ 是一个正常数。以货币单位计算的总滑点成本为 $C(v,\\sigma) = s_{\\text{per}}(v,\\sigma)\\,|v| = k\\,\\sigma\\,|v|^{3/2}$。因此，采取行动 $v$ 的带惩罚期望回报为\n$$\nR(v;\\mu,\\sigma,k) = \\mu\\,v \\;-\\; k\\,\\sigma\\,|v|^{3/2}.\n$$\n选择变量 $v$ 受到硬性交易限制 $|v| \\le V_{\\max}$ 的约束。\n\n任务：对于下面测试套件中的每组参数，计算唯一的最优交易规模 $v^\\star$，该规模能在满足 $|v|\\le V_{\\max}$ 的所有实数 $v$ 中最大化 $R(v;\\mu,\\sigma,k)$。当因简并性而存在多个最大化解时，选择 $v^\\star = 0$ 作为打破僵局的约定。您的程序必须根据 $R(v;\\mu,\\sigma,k)$ 的定义来实现这一点，并为每个测试用例输出最优的 $v^\\star$。\n\n测试套件（每个项目是一个元组 $(\\mu,\\sigma,k,V_{\\max})$）：\n- 案例 $1$：$(0.01, 0.02, 0.5, 10)$\n- 案例 $2$：$(-0.015, 0.03, 0.7, 20)$\n- 案例 $3$：$(0.02, 0, 1, 5)$\n- 案例 $4$：$(0, 0.05, 0.5, 100)$\n- 案例 $5$：$(0.05, 0.02, 0.2, 1)$\n- 案例 $6$：$(0.01, 0.5, 2, 100)$\n\n最终输出格式：您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果，顺序与上述案例相同。将每个结果四舍五入到 $6$ 位小数。例如，一个包含三个假设结果的有效输出将如下所示：$[0.123456,-0.500000,1.000000]$。", "solution": "问题陈述已提交以供验证。\n\n### 步骤 1：提取已知条件\n- **目标函数**：带惩罚的期望回报为 $R(v;\\mu,\\sigma,k) = \\mu\\,v \\;-\\; k\\,\\sigma\\,|v|^{3/2}$。\n- **选择变量**：$v$，一个代表带符号交易规模的实数。\n- **参数**：\n    - $\\mu$：单位期望价格变动。\n    - $\\sigma$：市场波动率参数。\n    - $k$：一个正常数，$k>0$。\n- **约束**：交易规模受限于 $|v| \\le V_{\\max}$。\n- **打破僵局规则**：如果因简并性存在多个最大化解，则选择 $v^\\star = 0$。\n- **任务**：计算在约束条件下最大化 $R(v;\\mu,\\sigma,k)$ 的最优交易规模 $v^\\star$。\n- **测试套件**：\n    - 案例 $1$：$(\\mu,\\sigma,k,V_{\\max}) = (0.01, 0.02, 0.5, 10)$\n    - 案例 $2$：$(\\mu,\\sigma,k,V_{\\max}) = (-0.015, 0.03, 0.7, 20)$\n    - 案例 $3$：$(\\mu,\\sigma,k,V_{\\max}) = (0.02, 0, 1, 5)$\n    - 案例 $4$：$(\\mu,\\sigma,k,V_{\\max}) = (0, 0.05, 0.5, 100)$\n    - 案例 $5$：$(\\mu,\\sigma,k,V_{\\max}) = (0.05, 0.02, 0.2, 1)$\n    - 案例 $6$：$(\\mu,\\sigma,k,V_{\\max}) = (0.01, 0.5, 2, 100)$\n\n### 步骤 2：使用提取的已知条件进行验证\n1.  **科学依据**：该问题描述了金融执行中的一个经典权衡：在最大化可预测价格变动（$\\mu v$）带来的期望利润的同时，最小化交易成本。成本函数 $C(v,\\sigma) = k\\,\\sigma\\,|v|^{3/2}$ 是“平方根市场冲击”定律的一种表示，这是一个在金融市场中通过经验观察到的现象。该模型是计算金融学中使用的标准简化表示。它具有科学依据。\n2.  **适定性**：任务是在紧集 $[-V_{\\max}, V_{\\max}]$ 上最大化一个连续函数 $R(v)$。根据极值定理，最大值保证存在。解的唯一性必须被验证。该问题是适定的。\n3.  **客观性**：问题以精确的数学术语陈述，使用了无歧义的定义和客观标准。\n4.  **完整性**：为每个案例提供了所有必需的参数（$\\mu, \\sigma, k, V_{\\max}$）。目标函数和约束被完全指定。设置是完整的。\n5.  **缺陷**：该问题没有表现出任何不合格的缺陷。在金融背景下，这些参数在物理上是合理的。其中一个案例包含 $\\sigma=0$，这是一个有效且重要的极限情况，代表一个没有波动率的市场，必须得到正确处理。\n\n### 步骤 3：结论与行动\n该问题是有效的。将提供一个严谨的解决方案。\n\n目标是找到使函数 $R(v) = \\mu v - k \\sigma |v|^{3/2}$ 在区间 $v \\in [-V_{\\max}, V_{\\max}]$ 上最大化的 $v^\\star$。函数 $R(v)$ 是连续的，但其在 $v=0$ 处的导数没有定义。因此，我们通过分别考虑 $v>0$、$v<0$ 和 $v=0$ 的情况来分析该问题。在 $v=0$ 处的值是 $R(0)=0$。\n\n情况 1：$v > 0$。\n函数为 $R(v) = \\mu v - k \\sigma v^{3/2}$。我们寻求其在 $(0, V_{\\max}]$ 上的最大值。\n关于 $v$ 的一阶和二阶导数是：\n$$\nR'(v) = \\mu - \\frac{3}{2} k \\sigma v^{1/2}\n$$\n$$\nR''(v) = -\\frac{3}{4} k \\sigma v^{-1/2}\n$$\n给定 $k>0$ 且我们暂时假设 $\\sigma>0$，对于所有 $v>0$ 都有 $R''(v) < 0$。这表明 $R(v)$ 在 $v>0$ 时是严格凹函数，因此它在该定义域内至多有一个局部最大值。\n\n为了找到无约束的最大化解，我们令 $R'(v) = 0$：\n$$\n\\mu - \\frac{3}{2} k \\sigma v^{1/2} = 0 \\implies v^{1/2} = \\frac{2\\mu}{3k\\sigma}\n$$\n只有当 $\\mu > 0$ 时，$v^{1/2}$ 才存在正实数解。在这种情况下，无约束的最优交易为 $v_{\\text{unc}} = \\left(\\frac{2\\mu}{3k\\sigma}\\right)^2$。\n如果 $\\mu \\le 0$，则对所有 $v>0$ 都有 $R'(v) \\le 0$，这意味着 $R(v)$ 在 $(0, \\infty)$ 上是非增的。因此，在 $[0, V_{\\max}]$ 上的最大值在 $v=0$ 处取得。\n如果 $\\mu > 0$，$R(v)$ 的凹性意味着在 $[0, V_{\\max}]$ 上的最大值出现在 $v^\\star_+ = \\min(v_{\\text{unc}}, V_{\\max})$。所得的回报 $R(v^\\star_+)$ 将是非负的。\n\n情况 2：$v < 0$。\n令 $u = -v > 0$。以 $u$ 表示的函数是 $R(u) = \\mu(-u) - k\\sigma u^{3/2} = -\\mu u - k\\sigma u^{3/2}$。我们在 $u \\in (0, V_{\\max}]$ 上最大化此函数。\n关于 $u$ 的一阶导数是：\n$$\nR'(u) = -\\mu - \\frac{3}{2} k \\sigma u^{1/2}\n$$\n令 $R'(u) = 0$ 得出 $u^{1/2} = \\frac{-2\\mu}{3k\\sigma}$。只有当 $\\mu < 0$ 时，$u^{1/2}$ 才存在正实数解。无约束的最优规模是 $u_{\\text{unc}} = \\left(\\frac{-2\\mu}{3k\\sigma}\\right)^2 = \\left(\\frac{2|\\mu|}{3k\\sigma}\\right)^2$。\n如果 $\\mu \\ge 0$，则对所有 $u>0$ 都有 $R'(u) < 0$，意味着该函数是递减的。对于负 $v$（正 $u$）值，最大值在 $v \\to 0^-$ 处取得，得到 $R(0)=0$。\n如果 $\\mu < 0$，最优的正规模 $u$ 是 $u^\\star = \\min(u_{\\text{unc}}, V_{\\max})$，这对应于一个负向交易 $v^\\star_- = -u^\\star$。回报将是非负的。\n\n总体解决方案综合：\n- 如果 $\\mu > 0$，任何交易 $v < 0$ 都会产生 $R(v) < 0$，而最优交易 $v^\\star_+ > 0$ 会产生 $R(v^\\star_+) \\ge 0$。因此，全局最优解是 $v^\\star = v^\\star_+$。\n- 如果 $\\mu < 0$，任何交易 $v > 0$ 都会产生 $R(v) < 0$，而最优交易 $v^\\star_- < 0$ 会产生 $R(v^\\star_-) \\ge 0$。因此，全局最优解是 $v^\\star = v^\\star_-$。\n- 如果 $\\mu = 0$（且 $\\sigma > 0$），函数为 $R(v) = -k\\sigma|v|^{3/2}$，其在 $v^\\star = 0$ 处最大化。\n\n综合这些对于 $\\sigma > 0$ 的结果：\n最优交易规模的绝对值为 $v_{\\text{mag}} = \\min\\left( \\left(\\frac{2|\\mu|}{3k\\sigma}\\right)^2, V_{\\max} \\right)$。交易的符号必须与 $\\mu$ 的符号相匹配。如果 $\\mu=0$，则绝对值为 $0$。\n所以，$v^\\star = \\text{sign}(\\mu) \\cdot \\min\\left( \\left(\\frac{2|\\mu|}{3k\\sigma}\\right)^2, V_{\\max} \\right)$。\n\n特殊情况：$\\sigma = 0$。\n目标函数简化为 $R(v) = \\mu v$。我们必须在 $v \\in [-V_{\\max}, V_{\\max}]$ 上最大化这个线性函数。\n- 如果 $\\mu > 0$，$R(v)$ 在上界处取得最大值，$v^\\star = V_{\\max}$。\n- 如果 $\\mu < 0$，$R(v)$ 在下界处取得最大值，$v^\\star = -V_{\\max}$。\n- 如果 $\\mu = 0$，$R(v)=0$ 对所有 $v$ 成立。这是一个简并情况。所有 $v \\in [-V_{\\max}, V_{\\max}]$ 都是最大化解。根据规定的打破僵局规则，我们必须选择 $v^\\star = 0$。\n这可以紧凑地写作 $v^\\star = V_{\\max} \\cdot \\text{sign}(\\mu)$。\n\n最终算法：\n1. 对于一组给定的参数 $(\\mu, \\sigma, k, V_{\\max})$：\n2. 如果 $\\sigma = 0$，最优交易为 $v^\\star = V_{\\max} \\cdot \\text{sign}(\\mu)$。\n3. 如果 $\\sigma > 0$：\n   - 如果 $\\mu = 0$，最优交易为 $v^\\star = 0$。\n   - 如果 $\\mu \\ne 0$，计算无约束的最优绝对值 $v_{\\text{unc\\_mag}} = \\left(\\frac{2|\\mu|}{3k\\sigma}\\right)^2$。有约束的绝对值为 $v_{\\text{mag}} = \\min(v_{\\text{unc\\_mag}}, V_{\\max})$。最优交易为 $v^\\star = \\text{sign}(\\mu) \\cdot v_{\\text{mag}}$。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the optimal trade size v* for a series of one-step trading problems.\n    The problem is to maximize R(v) = mu*v - k*sigma*|v|^(3/2) subject to |v| <= V_max.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # Each case is a tuple (mu, sigma, k, V_max).\n    test_cases = [\n        (0.01, 0.02, 0.5, 10),\n        (-0.015, 0.03, 0.7, 20),\n        (0.02, 0, 1, 5),\n        (0, 0.05, 0.5, 100),\n        (0.05, 0.02, 0.2, 1),\n        (0.01, 0.5, 2, 100),\n    ]\n\n    results = []\n    for case in test_cases:\n        mu, sigma, k, V_max = case\n        \n        v_star = 0.0\n\n        if sigma == 0:\n            # Special case: no volatility implies no slippage penalty.\n            # Maximize R(v) = mu*v.\n            # Optimal trade is at the boundary, with sign matching mu.\n            # np.sign(0) is 0, correctly handling the mu=0 degenerate case.\n            v_star = V_max * np.sign(mu)\n        else:\n            # Standard case with volatility and slippage.\n            if mu == 0:\n                # If mu is 0, R(v) = -k*sigma*|v|^(3/2), which is maximized at v=0.\n                v_star = 0.0\n            else:\n                # Unconstrained optimal trade magnitude from setting R'(v)=0.\n                v_unc_mag = (2 * abs(mu) / (3 * k * sigma))**2\n                \n                # The optimal magnitude is constrained by V_max.\n                v_mag = min(v_unc_mag, V_max)\n                \n                # The sign of the trade should match the sign of the expected return.\n                v_star = np.sign(mu) * v_mag\n        \n        results.append(v_star)\n    \n    # Format the results to 6 decimal places and join into a single string.\n    formatted_results = [f\"{r:.6f}\" for r in results]\n    \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "2426687"}, {"introduction": "真实的交易并非单次博弈，而是一系列环环相扣的决策。这个练习将我们的视野从单步决策扩展到多周期模型，目标是在整个交易日内优化库存路径，同时考虑持有隔夜头寸所带来的显著惩罚。你将运用动态规划这一强大技术来求解最优交易策略，它是许多强化学习算法的理论基石。[@problem_id:2426659]", "problem": "您的任务是将隔夜风险形式化为期末库存惩罚，并为一个离散时间的、单一资产的交易日计算最优期望回报。在该交易日中，代理学习在收市前轧平其头寸。该交易日包含有限数量的日内决策时间点。在每个决策时间点，代理每个时期最多可调整其库存一个单位，并受到严格的库存边界约束。\n\n环境定义如下。存在一个有限时间范围，其中日内决策时间点由 $t \\in \\{0,1,\\dots,T-1\\}$ 索引，市场在 $t = T$ 收市。代理在时间 $t$ 的库存为 $p_t \\in \\{-P_{\\max},-P_{\\max}+1,\\dots,P_{\\max}-1,P_{\\max}\\}$，且 $p_0 = 0$。在每个 $t \\in \\{0,1,\\dots,T-1\\}$，代理选择一个行动 $a_t \\in \\{-1,0,1\\}$。库存转移遵循\n$$\np_{t+1} = \\mathrm{clip}(p_t + a_t, -P_{\\max}, P_{\\max}),\n$$\n其中 $\\mathrm{clip}(x,\\ell,u) = \\min\\{u,\\max\\{\\ell,x\\}\\}$。\n\n在时期 $[t,t+1]$ 内的期望价格变动是给定的，记为 $\\mu_t$。在时间 $t$ 采取任何行动的每笔交易成本为 $c \\cdot |a_t|$。每个时期的持续风险惩罚在时间 $t$ 为 $k \\cdot p_{t+1}^2$。在收市时，会施加隔夜风险惩罚：$-\\beta \\cdot p_T^2$。\n\n在时间 $t$ 作出决策的每时期期望回报定义为：在 $[t,t+1]$ 期间持有 $p_{t+1}$ 的期望交易利润，减去交易成本和持续库存惩罚：\n$$\nr_t(p_t,a_t) = p_{t+1} \\cdot \\mu_t - c\\cdot|a_t| - k\\cdot p_{t+1}^2.\n$$\n期末贡献为\n$$\nr_T(p_T) = -\\beta \\cdot p_T^2.\n$$\n目标是计算从 $p_0=0$ 开始的最优期望总回报，\n$$\nR^\\star = \\max_{\\{a_t\\}_{t=0}^{T-1}} \\sum_{t=0}^{T-1} r_t(p_t,a_t) + r_T(p_T),\n$$\n在 $p_{t+1}$ 的转移动态和行动约束条件下。\n\n必须使用确定性的决胜规则，以确保在任何决策时间 $t$ 当多个行动产生相同的最大期望值时，最优行动是唯一的。决胜规则如下：\n- 在所有能最大化期望值的行动中，优先选择那些能使 $|p_{t+1}|$ 最小的行动。\n- 如果仍有平局，在这些行动中，优先选择 $|a_t|$ 最小的行动。\n- 如果仍有平局，则按照 $-1 < 0 < 1$ 的常规顺序选择最小的 $a_t$。\n\n对于每个测试用例，从 $p_0=0$ 开始，计算：\n- 最优期望总回报 $R^\\star$。\n- 在指定的决胜规则下，遵循最优行动策略所引出的最终期末库存 $p_T$。\n\n您的程序应为每个测试用例返回一个数对 $\\left(R^\\star, p_T\\right)$，其中 $R^\\star$ 使用标准四舍五入保留到小数点后 $6$ 位，$p_T$ 是一个整数。将所有测试用例的结果按下列顺序汇总到一个列表中。\n\n测试套件：\n1.  案例 A（混合漂移率的通用多周期）：$T=4$, $\\mu = [0.02, 0.01, 0.00, -0.01]$, $c=0.003$, $k=0.0005$, $\\beta=0.05$, $P_{\\max}=3$。\n2.  案例 B（零漂移率基线）：$T=3$, $\\mu = [0.00, 0.00, 0.00]$, $c=0.001$, $k=0.00$, $\\beta=0.10$, $P_{\\max}=2$。\n3.  案例 C（立即收市边界）：$T=1$, $\\mu = [0.05]$, $c=0.002$, $k=0.00$, $\\beta=0.03$, $P_{\\max}=1$。\n4.  案例 D（临近收市时的平仓激励）：$T=2$, $\\mu = [0.03, 0.03]$, $c=0.002$, $k=0.00$, $\\beta=0.20$, $P_{\\max}=2$。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，顺序为 $[\\text{案例A } R^\\star,\\text{案例A } p_T,\\text{案例B } R^\\star,\\text{案例B } p_T,\\text{案例C } R^\\star,\\text{案例C } p_T,\\text{案例D } R^\\star,\\text{案例D } p_T]$。\n- $R^\\star$ 条目必须打印为保留 6 位小数的十进制数，$p_T$ 条目必须打印为整数。", "solution": "所呈现的问题是一个离散时间、有限范围的最优控制问题，这是计算金融学中为最优执行策略建模的一种标准范式。该问题是适定的且在科学上是合理的，允许通过动态规划求解。其目标是找到一个行动序列 $\\{a_t\\}_{t=0}^{T-1}$，以最大化从初始库存 $p_0=0$ 开始的总期望回报。\n\n解决方案通过定义一个价值函数 $V_t(p)$ 来进行，该函数表示在时间 $t$ 的库存为 $p_t = p$ 的条件下，从时间 $t$ 到市场在时间 $T$ 收市可能获得的最大回报总和。该问题通过反向归纳法解决，从期末时间 $T$ 开始，逆向推导回初始时间 $t=0$。\n\n在任何时间 $t \\in \\{0, 1, \\dots, T\\}$，系统的状态是代理的库存 $p_t$。状态空间是离散且有限的，由 $p_t \\in \\{-P_{\\max}, \\dots, P_{\\max}\\}$ 给出。在任何决策时间 $t \\in \\{0, 1, \\dots, T-1\\}$，行动空间也是离散且有限的：$a_t \\in \\{-1, 0, 1\\}$。\n\nBellman 最优性原理为价值函数提供了递归关系。在期末时间 $t=T$，价值函数仅由隔夜风险惩罚确定：\n$$\nV_T(p_T) = r_T(p_T) = -\\beta \\cdot p_T^2\n$$\n对于任何之前的时刻 $t \\in \\{T-1, T-2, \\dots, 0\\}$，价值函数 $V_t(p_t)$ 是在所有可能的行动 $a_t$ 上可实现的最大值。这个值是即时回报 $r_t(p_t, a_t)$ 与后续状态的价值 $V_{t+1}(p_{t+1})$ 的总和：\n$$\nV_t(p_t) = \\max_{a_t \\in \\{-1, 0, 1\\}} \\left\\{ r_t(p_t, a_t) + V_{t+1}(p_{t+1}) \\right\\}\n$$\n其中状态转移由 $p_{t+1} = \\mathrm{clip}(p_t + a_t, -P_{\\max}, P_{\\max})$ 给出，即时回报为 $r_t(p_t, a_t) = p_{t+1} \\cdot \\mu_t - c \\cdot |a_t| - k \\cdot p_{t+1}^2$。\n\n让我们定义品质函数，或称 Q 值，$Q_t(p_t, a_t)$，作为在时间 $t$、状态 $p_t$ 下采取行动 $a_t$ 的价值：\n$$\nQ_t(p_t, a_t) = p_{t+1} \\cdot \\mu_t - c \\cdot |a_t| - k \\cdot p_{t+1}^2 + V_{t+1}(p_{t+1})\n$$\nBellman 方程可以写作 $V_t(p_t) = \\max_{a_t} Q_t(p_t, a_t)$。\n\n计算流程如下：\n1.  初始化一个表格，用于存储所有 $t \\in \\{0, \\dots, T\\}$ 和 $p \\in \\{-P_{\\max}, \\dots, P_{\\max}\\}$ 的价值 $V_t(p)$。\n2.  在 $t=T$ 时，用所有可能的期末库存 $p_T$ 的期末价值 $V_T(p_T) = -\\beta \\cdot p_T^2$ 填充表格。\n3.  算法从 $t=T-1$ 开始向后迭代至 $t=0$。在每一步 $t$ 中，对于每个可能的库存状态 $p_t$，使用已知的 $V_{t+1}$ 值计算所有三个行动 $a_t \\in \\{-1, 0, 1\\}$ 的 $Q_t(p_t, a_t)$ 值。\n4.  为确保最优行动 $\\pi_t(p_t)$ 的唯一性，必须严格执行指定的决胜规则。当多个行动产生相同的最大 $Q$ 值时，通过字典序比较来精化选择。所选行动 $a^\\star$ 必须是依次满足以下标准的行动：\n    a. 最大化 $Q$ 值 $Q_t(p_t, a_t)$。\n    b. 最小化下一库存的绝对值 $|p_{t+1}|$。\n    c. 最小化行动的绝对值 $|a_t|$。\n    d. 最小化行动值本身，遵循自然排序 $-1 < 0 < 1$。\n5.  存储最优价值 $V_t(p_t) = Q_t(p_t, \\pi_t(p_t))$。\n6.  从 $p_0=0$ 开始的整个交易日的最优期望总回报为 $R^\\star = V_0(0)$。\n7.  要找到相应的期末库存 $p_T$，可以从初始状态 $(t,p) = (0,0)$ 开始，使用计算出的最优策略 $\\pi_t(p_t)$ 向前模拟轨迹。算法中采用了一种更直接的方法，即在反向回溯过程中维护另一个表格，该表格将每个状态-时间对 $(t, p_t)$ 映射到如果从该点开始遵循最优策略所产生的期末库存 $p_T$。设此为 $P_T(t, p_t)$。那么 $P_T(T, p_T) = p_T$，对于 $t < T$, $P_T(t, p_t) = P_T(t+1, \\mathrm{clip}(p_t + \\pi_t(p_t), -P_{\\max}, P_{\\max}))$。期末库存的最终答案即为 $p_T = P_T(0, 0)$。\n\n这种动态规划方法保证了在模型假设下能够找到全局最优解。该实现为每个测试用例计算这些值，以确定指定的输出。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the given test cases for the optimal trading problem.\n    \"\"\"\n\n    def solve_case(T, mu, c, k, beta, Pmax):\n        \"\"\"\n        Solves a single instance of the optimal trading problem using dynamic programming.\n\n        Args:\n            T (int): Number of time steps.\n            mu (list): List of expected price changes.\n            c (float): Transaction cost per share.\n            k (float): Running inventory risk penalty coefficient.\n            beta (float): Terminal inventory risk penalty coefficient.\n            Pmax (int): Maximum absolute inventory.\n\n        Returns:\n            tuple: A tuple containing the optimal total reward (R_star) and the\n                   resulting terminal inventory (p_T).\n        \"\"\"\n        # State space for inventory p\n        num_p_states = 2 * Pmax + 1\n        p_states = np.arange(-Pmax, Pmax + 1)\n        \n        # DP tables\n        # V_table stores the value function V_t(p)\n        V_table = np.zeros((T + 1, num_p_states))\n        # pT_table stores the resulting terminal inventory p_T if starting from state p at time t\n        pT_table = np.zeros((T + 1, num_p_states), dtype=np.int32)\n\n        # Helper function to map inventory p to an array index\n        def p_to_idx(p):\n            return int(p + Pmax)\n\n        # --- Backward Pass ---\n\n        # Time t = T: Terminal condition\n        for p_idx, p_T in enumerate(p_states):\n            V_table[T, p_idx] = -beta * p_T**2\n            pT_table[T, p_idx] = p_T\n\n        # Recursion from t = T-1 down to 0\n        for t in range(T - 1, -1, -1):\n            for p_idx, p_t in enumerate(p_states):\n                \n                candidates = []\n                \n                # Evaluate all possible actions a_t in {-1, 0, 1}\n                for a_t in [-1, 0, 1]:\n                    # State transition\n                    p_t_plus_1 = int(np.clip(p_t + a_t, -Pmax, Pmax))\n                    p_t_plus_1_idx = p_to_idx(p_t_plus_1)\n                    \n                    # Immediate reward\n                    reward = p_t_plus_1 * mu[t] - c * abs(a_t) - k * p_t_plus_1**2\n                    \n                    # Q-value (sum of immediate reward and future value)\n                    q_value = reward + V_table[t + 1, p_t_plus_1_idx]\n                    \n                    # Store candidate with its tie-breaking criteria as a sortable tuple:\n                    # 1. Maximize Q-value (equivalent to minimizing -q_value)\n                    # 2. Minimize |p_{t+1}|\n                    # 3. Minimize |a_t|\n                    # 4. Minimize a_t\n                    sort_key = (-q_value, abs(p_t_plus_1), abs(a_t), a_t)\n                    candidates.append((sort_key, p_t_plus_1, q_value))\n                \n                # Sort to find the best action according to the tie-breaking rules\n                candidates.sort(key=lambda x: x[0])\n                \n                # The best choice is the first element after sorting\n                best_choice = candidates[0]\n                best_p_next = best_choice[1]\n                best_q = best_choice[2]\n                \n                # Store results in the DP tables\n                V_table[t, p_idx] = best_q\n                \n                # Propagate the terminal position for this state-time pair\n                best_p_next_idx = p_to_idx(best_p_next)\n                pT_table[t, p_idx] = pT_table[t + 1, best_p_next_idx]\n\n        # --- Extract Final Answer ---\n        \n        # The solution corresponds to the initial state p_0 = 0 at t = 0\n        p0_idx = p_to_idx(0)\n        \n        # Optimal total expected reward\n        R_star = V_table[0, p0_idx]\n        \n        # Resulting terminal inventory\n        p_T_final = pT_table[0, p0_idx]\n        \n        return R_star, p_T_final\n\n    test_cases = [\n        # Case A: general multi-period with mixed drifts\n        {'T': 4, 'mu': [0.02, 0.01, 0.00, -0.01], 'c': 0.003, 'k': 0.0005, 'beta': 0.05, 'Pmax': 3},\n        # Case B: zero drift baseline\n        {'T': 3, 'mu': [0.00, 0.00, 0.00], 'c': 0.001, 'k': 0.00, 'beta': 0.10, 'Pmax': 2},\n        # Case C: immediate close boundary\n        {'T': 1, 'mu': [0.05], 'c': 0.002, 'k': 0.00, 'beta': 0.03, 'Pmax': 1},\n        # Case D: flattening incentive near close\n        {'T': 2, 'mu': [0.03, 0.03], 'c': 0.002, 'k': 0.00, 'beta': 0.20, 'Pmax': 2},\n    ]\n\n    all_results = []\n    for case in test_cases:\n        R_star, p_T = solve_case(\n            case['T'], case['mu'], case['c'], case['k'], case['beta'], case['Pmax']\n        )\n        all_results.append(format(R_star, '.6f'))\n        all_results.append(p_T)\n\n    # Print the final output in the exact required format\n    print(f\"[{','.join(map(str, all_results))}]\")\n\nsolve()\n```", "id": "2426659"}, {"introduction": "我们如何信任一个自动化交易智能体的决策？最后一个练习将聚焦于可解释性这一关键议题，通过构建一个决策逻辑清晰透明的智能体来回答这个问题。你将通过对历史交易数据拟合一个线性模型来学习一个价值函数，从而能够直观地解释驱动智能体进行买卖决策的关键因素。[@problem_id:2426627]", "problem": "您的任务是，在一个程式化的交易环境中，通过使用一个线性模型来表示行动价值函数，从而构建一个可解释的强化学习 (RL) 交易代理。该交易环境被建模为一个离散时间的马尔可夫决策过程 (MDP)，其时间索引为 $t \\in \\{0,1,\\dots,T-1\\}$，行动集为 $\\mathcal{A}=\\{-1,0,1\\}$，分别代表空头、平仓和多头头寸，并包含一个单一风险资产的随机收益过程。\n\n环境动态与特征：\n- 资产的简单收益率 $r_t$ 服从一个一阶自回归过程：\n$$r_t = \\mu + \\varphi \\, r_{t-1} + \\sigma \\, \\varepsilon_t,$$\n其中 $\\varepsilon_t \\sim \\mathcal{N}(0,1)$ 是独立同分布的， $r_{-1}=0$，参数为 $\\mu \\in \\mathbb{R}$、$\\varphi \\in \\mathbb{R}$ 和 $\\sigma > 0$。\n- 将收益率的指数移动平均 (EMA) $m_t$ 递归定义为\n$$m_t = (1-\\alpha) m_{t-1} + \\alpha r_t,$$\n其中 $m_{-1}=0$ 且平滑参数 $\\alpha \\in (0,1]$。\n- 时间 $t$ 的状态特征向量是\n$$\\phi(s_t) = \\begin{bmatrix} 1 \\\\ r_{t-1} \\\\ m_{t-1} \\end{bmatrix},$$\n约定 $r_{-1} = 0$ 和 $m_{-1} = 0$。\n\n行为策略与奖励：\n- 生成数据的行为策略与状态无关，并且从 $\\{-1,0,1\\}$ 中均匀随机地抽取 $a_t$。\n- 设 $a_{-1}=0$。在时间 $t$ 的单步奖励为\n$$u_t = a_t \\, r_t \\;-\\; c \\, |a_t - a_{t-1}|,$$\n其中 $c \\ge 0$ 是单位交易成本率（以小数而非百分比表示）。\n\n折扣回报与线性行动价值模型：\n- 对于给定的折扣因子 $\\gamma \\in [0,1)$ 和视界 $H \\in \\mathbb{N}$，将从 $t$ 开始的截断折扣回报定义为\n$$G_t \\;=\\; \\sum_{k=0}^{K_t-1} \\gamma^k \\, u_{t+k}, \\quad \\text{其中 } K_t = \\min\\{H,\\, T-t\\}。$$\n- 考虑由 $\\theta \\in \\mathbb{R}^3$ 参数化的线性行动价值函数（线性 $Q$ 函数）：\n$$Q_\\theta(s_t,a_t) \\;=\\; \\theta^\\top \\big(a_t \\, \\phi(s_t)\\big)。$$\n\n估计目标：\n- 给定由上述行为策略和环境生成的轨迹 $\\{(s_t,a_t,u_t)\\}_{t=0}^{T-1}$，定义正则化经验风险\n$$J(\\theta) \\;=\\; \\sum_{t=0}^{T-1} \\Big(Q_\\theta(s_t,a_t) - G_t\\Big)^2 \\;+\\; \\lambda \\, \\|\\theta\\|_2^2,$$\n其中 $\\lambda \\ge 0$ 是一个正则化参数。假设所有参数都满足确保目标函数是严格凸的条件，因此存在唯一的最小化子。\n\n任务：\n- 对于每个测试用例（下面列出的参数集），使用指定的随机种子模拟环境和行为策略，以生成高斯冲击 $\\varepsilon_t$ 和随机行动 $a_t$。\n- 计算特征 $\\phi(s_t)$、奖励 $u_t$ 和折扣回报 $G_t$。\n- 为该测试用例计算 $J(\\theta)$ 的唯一最小化子 $\\hat{\\theta}$。\n- 您的程序必须为每个测试用例输出学习到的系数向量 $\\hat{\\theta}$，其形式为一个包含实数的列表 $[\\hat{\\theta}_0,\\hat{\\theta}_1,\\hat{\\theta}_2]$。将所有测试用例的结果按相同顺序汇总到一个列表中。\n\n测试套件（四个用例以确保覆盖）：\n- 用例 $1$（基线）： \n  - $T=200$, $\\mu=0$, $\\varphi=0.1$, $\\sigma=0.02$, $\\alpha=0.2$, $\\lambda=0.1$, $\\gamma=0.95$, $c=0.0005$, $H=50$, seed $=42$。\n- 用例 $2$（短视，无正则化）：\n  - $T=200$, $\\mu=0$, $\\varphi=0$, $\\sigma=0.02$, $\\alpha=0.2$, $\\lambda=0$, $\\gamma=0$, $c=0.0005$, $H=1$, seed $=7$。\n- 用例 $3$（高正则化）：\n  - $T=200$, $\\mu=0$, $\\varphi=0.3$, $\\sigma=0.02$, $\\alpha=0.2$, $\\lambda=10$, $\\gamma=0.9$, $c=0.001$, $H=50$, seed $=123$。\n- 用例 $4$（高交易成本）：\n  - $T=200$, $\\mu=0$, $\\varphi=0.1$, $\\sigma=0.02$, $\\alpha=0.2$, $\\lambda=0.1$, $\\gamma=0.95$, $c=0.01$, $H=50$, seed $=99$。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果，其中每个元素是对应一个测试用例的三维列表，顺序与上文相同。例如，一种可接受的格式是\n$$\\big[\\,[\\hat{\\theta}_{0}^{(1)},\\hat{\\theta}_{1}^{(1)},\\hat{\\theta}_{2}^{(1)}],\\;[\\hat{\\theta}_{0}^{(2)},\\hat{\\theta}_{1}^{(2)},\\hat{\\theta}_{2}^{(2)}],\\;[\\hat{\\theta}_{0}^{(3)},\\hat{\\theta}_{1}^{(3)},\\hat{\\theta}_{2}^{(3)}],\\;[\\hat{\\theta}_{0}^{(4)},\\hat{\\theta}_{1}^{(4)},\\hat{\\theta}_{2}^{(4)}]\\big].$$\n- 所有数值都应以实数形式打印。本问题不涉及任何单位或角度。当参数中出现百分比时，应按照上文规定，以小数形式提供和处理。", "solution": "问题要求我们找到参数向量 $\\hat{\\theta}$，以最小化正则化经验风险函数 $J(\\theta)$。这是一个统计学习中的标准问题，具体来说，它采用岭回归 (Ridge Regression) 的形式。\n\n待最小化的目标函数由以下公式给出：\n$$J(\\theta) = \\sum_{t=0}^{T-1} \\Big(Q_\\theta(s_t,a_t) - G_t\\Big)^2 + \\lambda \\, \\|\\theta\\|_2^2$$\n其中线性行动价值函数为 $Q_\\theta(s_t,a_t) = \\theta^\\top \\big(a_t \\, \\phi(s_t)\\big)$。\n\n这个目标函数可以用矩阵-向量表示法更方便地表达。设参数向量 $\\theta$ 的维度为 $d=3$。我们定义一个大小为 $T \\times d$ 的设计矩阵 $X$ 和一个大小为 $T \\times 1$ 的目标向量 $y$。$X$ 的第 $t$ 行，记为 $X_t$，对应于时间 $t$ 预测的特征向量：\n$$X_t = \\big(a_t \\, \\phi(s_t)\\big)^\\top = a_t \\begin{bmatrix} 1 & r_{t-1} & m_{t-1} \\end{bmatrix}$$\n目标向量 $y$ 由采样的截断折扣回报组成：\n$$y = \\begin{bmatrix} G_0 \\\\ G_1 \\\\ \\vdots \\\\ G_{T-1} \\end{bmatrix}$$\n通过这些定义，目标函数 $J(\\theta)$ 可以重写为岭回归的标准形式：\n$$J(\\theta) = \\|X\\theta - y\\|_2^2 + \\lambda \\|\\theta\\|_2^2 = (X\\theta - y)^\\top(X\\theta - y) + \\lambda \\theta^\\top\\theta$$\n这个函数 $J(\\theta)$ 是凸函数。问题陈述保证了满足存在唯一最小化子的条件。为了找到这个最小化子 $\\hat{\\theta}$，我们计算 $J(\\theta)$ 关于 $\\theta$ 的梯度，并将其设为零：\n$$\\nabla_\\theta J(\\theta) = \\frac{\\partial}{\\partial \\theta} \\left( \\theta^\\top X^\\top X \\theta - 2y^\\top X \\theta + y^\\top y + \\lambda \\theta^\\top \\theta \\right)$$\n$$\\nabla_\\theta J(\\theta) = 2 X^\\top X \\theta - 2 X^\\top y + 2 \\lambda I \\theta = 0$$\n其中 $I$ 是 $d \\times d$ 的单位矩阵。\n重新整理这些项，我们得到正规方程：\n$$(X^\\top X + \\lambda I) \\theta = X^\\top y$$\n唯一的最小化子 $\\hat{\\theta}$ 是这个线性方程组的解。\n\n为每个测试用例找到 $\\hat{\\theta}$ 的计算过程如下：\n1.  **数据生成**：我们首先根据指定动态模拟 $t \\in \\{0, 1, \\dots, T-1\\}$ 的时间序列数据。\n    - 使用给定的种子初始化随机数生成器以保证可复现性。\n    - 生成两个随机序列：用于收益过程的来自 $\\mathcal{N}(0,1)$ 的高斯冲击 $\\{\\varepsilon_t\\}_{t=0}^{T-1}$，以及来自 $\\{-1, 0, 1\\}$ 上均匀分布的行动 $\\{a_t\\}_{t=0}^{T-1}$。\n    - 使用 AR($1$) 过程迭代生成资产收益率 $r_t$：$r_t = \\mu + \\varphi r_{t-1} + \\sigma \\varepsilon_t$，初始条件为 $r_{-1}=0$。\n    - 迭代生成收益率的指数移动平均 (EMA) $m_t$：$m_t = (1-\\alpha) m_{t-1} + \\alpha r_t$，初始条件为 $m_{-1}=0$。\n    - 单步奖励 $u_t$ 计算为 $u_t = a_t r_t - c|a_t - a_{t-1}|$，初始条件为 $a_{-1}=0$。\n\n2.  **目标与特征构建**：利用模拟的轨迹，我们构建回归问题的各个组成部分。\n    - 对于每个时间步 $t \\in \\{0, \\dots, T-1\\}$，使用公式 $G_t = \\sum_{k=0}^{K_t-1} \\gamma^k u_{t+k}$ 计算目标值 $G_t$ （截断折扣回报），其中 $K_t = \\min(H, T-t)$。\n    - 组装 $T \\times 3$ 的设计矩阵 $X$，其第 $t$ 行由 $a_t [1, r_{t-1}, m_{t-1}]$ 给出。\n    - 形成 $T \\times 1$ 的目标向量 $y$，其第 $t$ 个元素为 $G_t$。\n\n3.  **求解 $\\hat{\\theta}$**：通过求解线性系统 $(X^\\top X + \\lambda I) \\hat{\\theta} = X^\\top y$ 来找到最优参数向量 $\\hat{\\theta}$。这可以使用标准的线性代数求解器以数值方式完成，该方法通常比直接计算矩阵的逆更稳定。得到的向量 $\\hat{\\theta} = [\\hat{\\theta}_0, \\hat{\\theta}_1, \\hat{\\theta}_2]^\\top$ 构成了线性行动价值函数近似的学习系数。\n\n对四个指定的测试用例中的每一个都实施了这整个过程，从而产生四个不同的系数向量。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the results.\n    \"\"\"\n\n    def compute_theta(T, mu, phi_r, sigma, alpha, lam, gam, c, H, seed):\n        \"\"\"\n        Computes the optimal theta_hat for a given set of parameters.\n\n        This function implements the three-step procedure:\n        1. Data Generation: Simulates returns, EMAs, actions, and rewards.\n        2. Target and Feature Construction: Computes discounted returns (G_t) and\n           builds the design matrix (X) and target vector (y).\n        3. Solving for theta_hat: Solves the Ridge Regression normal equations.\n        \"\"\"\n        # 1. Data Generation\n        rng = np.random.default_rng(seed)\n        \n        # Generate random shocks and actions for the entire trajectory\n        epsilons = rng.standard_normal(T)\n        actions_random = rng.choice([-1, 0, 1], size=T)\n        \n        # History arrays: index t corresponds to time t-1.\n        # Size T+1 to hold values from t=-1 to t=T-1.\n        r_hist = np.zeros(T + 1)  # r_hist[t] = r_{t-1}\n        m_hist = np.zeros(T + 1)  # m_hist[t] = m_{t-1}\n        a_hist = np.zeros(T + 1)  # a_hist[t] = a_{t-1}\n\n        # Reward vector: index t corresponds to time t.\n        u_vec = np.zeros(T)\n        \n        # Time-stepping simulation from t=0 to T-1\n        for t in range(T):\n            r_prev = r_hist[t]\n            m_prev = m_hist[t]\n            a_prev = a_hist[t]\n\n            # Calculate r_t, m_t, a_t at time t\n            r_t = mu + phi_r * r_prev + sigma * epsilons[t]\n            m_t = (1 - alpha) * m_prev + alpha * r_t\n            a_t = actions_random[t]\n            \n            # Calculate reward u_t\n            u_t = a_t * r_t - c * np.abs(a_t - a_prev)\n            \n            # Store new values in history arrays\n            r_hist[t + 1] = r_t\n            m_hist[t + 1] = m_t\n            a_hist[t + 1] = a_t\n            u_vec[t] = u_t\n            \n        # 2. Target and Feature Construction\n        \n        # Compute truncated discounted returns G_t\n        G_vec = np.zeros(T)\n        gam_powers = np.power(gam, np.arange(H))\n        \n        for t in range(T):\n            K_t = min(H, T - t)\n            # Sum of discounted future rewards\n            G_vec[t] = np.sum(gam_powers[:K_t] * u_vec[t : t + K_t])\n            \n        # Construct design matrix X and target vector y\n        d = 3  # Dimension of theta\n        X = np.zeros((T, d))\n        y = G_vec\n        \n        for t in range(T):\n            # State features are from time t-1\n            r_prev = r_hist[t]\n            m_prev = m_hist[t]\n            phi_st = np.array([1.0, r_prev, m_prev])\n            \n            # Action is from time t\n            a_t = a_hist[t + 1]\n            \n            # The t-th row of the design matrix\n            X[t, :] = a_t * phi_st\n            \n        # 3. Solving for theta_hat\n        \n        # Formulate the normal equations: A * theta = b\n        A = X.T @ X + lam * np.identity(d)\n        b = X.T @ y\n        \n        # Solve the linear system for theta\n        theta_hat = np.linalg.solve(A, b)\n        \n        return theta_hat.tolist()\n\n    # Test suite (four cases to ensure coverage):\n    # T, mu, phi, sigma, alpha, lambda, gamma, c, H, seed\n    test_cases = [\n        # Case 1 (baseline):\n        (200, 0.0, 0.1, 0.02, 0.2, 0.1, 0.95, 0.0005, 50, 42),\n        # Case 2 (myopic, no regularization):\n        (200, 0.0, 0.0, 0.02, 0.2, 0.0, 0.0, 0.0005, 1, 7),\n        # Case 3 (high regularization):\n        (200, 0.0, 0.3, 0.02, 0.2, 10.0, 0.9, 0.001, 50, 123),\n        # Case 4 (high transaction costs):\n        (200, 0.0, 0.1, 0.02, 0.2, 0.1, 0.95, 0.01, 50, 99),\n    ]\n\n    all_results = []\n    for params in test_cases:\n        # Unpack parameters and call the computation function\n        T, mu, phi_r, sigma, alpha, lam, gam, c, H, seed = params\n        theta_hat = compute_theta(T, mu, phi_r, sigma, alpha, lam, gam, c, H, seed)\n        all_results.append(theta_hat)\n        \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, all_results))}]\")\n\nsolve()\n```", "id": "2426627"}]}