{"hands_on_practices": [{"introduction": "我们从一个最简单的非平凡案例开始：一个双变量系统。这个练习将通过解析推导，帮助你直观地理解主成分分析如何将相关变量的方差进行分解和重组。通过这个理想化的场景 [@problem_id:1946278]，你将清晰地看到变量间的相关性是如何直接决定第一个主成分所能解释的方差比例的，从而将抽象的特征值概念与具体的数据特性联系起来。", "problem": "一架自主环境监测无人机使用一对相同的传感器来测量大气压力。设这两个传感器的读数，在减去其长期平均值进行中心化后，由随机变量 $X_1$ 和 $X_2$ 表示。\n\n这些读数的联合行为由一个二元随机向量 $(X_1, X_2)$ 描述，其协方差矩阵为 $\\Sigma$。由于这两个传感器是相同类型的，并受到相似的环境波动影响，因此它们具有相同的方差，$\\text{Var}(X_1) = \\text{Var}(X_2) = \\sigma^2$，其中 $\\sigma > 0$ 是一个常数。它们的读数也是相关的，相关系数为 $\\rho$，满足 $0 < \\rho < 1$。因此，协方差矩阵由下式给出：\n$$\n\\Sigma = \\begin{pmatrix} \\sigma^2 & \\rho\\sigma^2 \\\\ \\rho\\sigma^2 & \\sigma^2 \\end{pmatrix}\n$$\n为了减少数据冗余并识别主要的变异轴，工程团队应用了主成分分析 (PCA)。PCA 将原始的相关变量 $(X_1, X_2)$ 变换为一组新的不相关变量，称为主成分。第一主成分被定义为能够捕获最大可能方差的 $X_1$ 和 $X_2$ 的线性组合。\n\n确定数据中总方差由第一主成分解释的比例。将您的答案表示为含 $\\rho$ 的符号表达式。", "solution": "我们给定一个中心化的二元随机向量，其协方差矩阵为\n$$\n\\Sigma=\\begin{pmatrix}\\sigma^{2} & \\rho\\sigma^{2} \\\\ \\rho\\sigma^{2} & \\sigma^{2}\\end{pmatrix},\n$$\n其中 $0<\\rho<1$ 且 $\\sigma>0$。在 PCA 中，主成分的方差是协方差矩阵的特征值。第一主成分所解释的总方差比例等于其特征值除以总方差，而总方差等于 $\\Sigma$ 的迹。\n\n首先，我们通过求解特征方程来计算 $\\Sigma$ 的特征值\n$$\n\\det(\\Sigma-\\lambda I)=0.\n$$\n我们有\n$$\n\\det\\begin{pmatrix}\\sigma^{2}-\\lambda & \\rho\\sigma^{2} \\\\ \\rho\\sigma^{2} & \\sigma^{2}-\\lambda\\end{pmatrix}\n=(\\sigma^{2}-\\lambda)^{2}-(\\rho\\sigma^{2})^{2}=0.\n$$\n因此，\n$$\n(\\sigma^{2}-\\lambda)^{2}=\\rho^{2}\\sigma^{4}\n\\quad\\Longrightarrow\\quad\n\\sigma^{2}-\\lambda=\\pm\\rho\\sigma^{2}\n\\quad\\Longrightarrow\\quad\n\\lambda=\\sigma^{2}(1\\pm\\rho).\n$$\n由于 $0<\\rho<1$，最大的特征值是\n$$\n\\lambda_{1}=\\sigma^{2}(1+\\rho).\n$$\n总方差等于 $\\Sigma$ 的迹，\n$$\n\\operatorname{tr}(\\Sigma)=\\sigma^{2}+\\sigma^{2}=2\\sigma^{2},\n$$\n这也等于特征值之和 $\\sigma^{2}(1+\\rho)+\\sigma^{2}(1-\\rho)=2\\sigma^{2}$。因此，第一主成分所解释的总方差比例为\n$$\n\\frac{\\lambda_{1}}{\\operatorname{tr}(\\Sigma)}=\\frac{\\sigma^{2}(1+\\rho)}{2\\sigma^{2}}=\\frac{1+\\rho}{2}.\n$$", "answer": "$$\\boxed{\\frac{1+\\rho}{2}}$$", "id": "1946278"}, {"introduction": "从理论走向实践，我们面临的第一个挑战就是数据的量纲问题。金融数据通常包含不同单位和尺度的变量，例如以美元计价的股价和以百万为单位的交易量。这个练习 [@problem_id:2421735] 要求你通过编程生成一个可控的模拟环境，亲手验证为何对未经标准化的原始数据进行 PCA 会产生严重误导性的结果，并量化这种扭曲的程度。", "problem": "要求您使用主成分分析（PCA）的基本原理，论证未对以不同单位度量的变量进行标准化会如何扭曲估计出的主方向和解释方差。您将在纯数学框架下，通过一个模拟典型金融变量（如价格和交易量）的合成数据生成过程来完成此项工作。您将实现完整的流程，并报告量化诊断指标，用以比较对原始数据与对标准化数据执行PCA的结果。\n\n基本原理：\n- PCA旨在寻找最大化样本方差的标准正交方向。给定一个中心化的数据矩阵 $X \\in \\mathbb{R}^{T \\times n}$，样本协方差矩阵为 $\\Sigma = \\frac{1}{T-1} X^\\top X$。主成分是 $\\Sigma$ 的特征向量，与其从大到小排列的特征值相对应。\n- 标准化将每个变量 $x_j$ 转换为 $\\tilde{x}_j = \\frac{x_j - \\bar{x}_j}{\\hat{\\sigma}_j}$，其中 $\\bar{x}_j$ 是样本均值，$\\hat{\\sigma}_j$ 是样本标准差，从而使得每个标准化变量的样本方差为1。对标准化数据进行PCA等同于对样本相关系数矩阵进行PCA。\n- 对变量应用对角缩放 $D = \\operatorname{diag}(s_1,\\dots,s_n)$，即 $X \\mapsto X D$，会将协方差矩阵的元素乘以 $s_i s_j$，因此会改变特征向量，除非所有的 $s_j$ 都相等。\n\n数据生成过程：\n- 对于每个测试用例 $k$，固定 $T_k \\in \\mathbb{N}$、变量数量 $n_k \\in \\mathbb{N}$、因子载荷 $b^{(k)} \\in \\mathbb{R}^{n_k}$、特异标准差 $u^{(k)} \\in \\mathbb{R}^{n_k}$ 以及单位尺度 $s^{(k)} \\in \\mathbb{R}^{n_k}$。\n- 对于 $t = 1,\\dots,T_k$，生成一个单一共同因子 $f_t \\sim \\mathcal{N}(0,1)$；并生成特异噪声 $e_{t,j} \\sim \\mathcal{N}(0,(u^{(k)}_j)^2)$。所有这些因子和噪声在 $t$ 和 $j$ 上相互独立。\n- 对于 $t=1,\\dots,T_k$ 和 $j=1,\\dots,n_k$，构造原始观测值 $x_{t,j} = s^{(k)}_j \\left( b^{(k)}_j f_t + e_{t,j} \\right)$。\n- 在计算任何协方差之前，通过减去其样本均值来将 $X$ 的每一列中心化。\n\n每个测试用例的计算任务：\n- 从中心化的原始数据 $X$ 计算样本协方差矩阵 $\\Sigma_{\\text{raw}}$，并获取第一主成分的单位范数特征向量 $v_{\\text{raw}}$ 及其特征值 $\\lambda_{\\text{raw}}$。\n- 将 $X$ 的每一列标准化，使其样本方差为1，从而得到 $Z$；计算 $\\Sigma_{\\text{std}} = \\frac{1}{T_k-1} Z^\\top Z$（即样本相关系数矩阵），并获取第一主成分的单位范数特征向量 $v_{\\text{std}}$ 及其特征值 $\\lambda_{\\text{std}}$。\n- 计算角度 $\\theta = \\arccos\\!\\left( \\left| v_{\\text{raw}}^\\top v_{\\text{std}} \\right| \\right)$；以弧度为单位报告 $\\theta$。\n- 计算解释方差份额的差异 $\\Delta = \\left| \\frac{\\lambda_{\\text{raw}}}{\\operatorname{tr}(\\Sigma_{\\text{raw}})} - \\frac{\\lambda_{\\text{std}}}{\\operatorname{tr}(\\Sigma_{\\text{std}})} \\right|$，该值必须以小数形式报告（而非百分比）。\n\n随机性与可复现性：\n- 在整个实验中使用固定的伪随机数生成器种子 $314159$，以确保结果的可复现性。\n\n测试套件：\n- 共有3个测试用例。对于每个测试用例 $k$，使用以下参数 $(T_k, n_k, b^{(k)}, u^{(k)}, s^{(k)})$：\n  - 用例1（单位相似，两个变量）：\n    - $T_1 = 500$, $n_1 = 2$,\n    - $b^{(1)} = [1.0, 0.9]$,\n    - $u^{(1)} = [0.1, 0.1]$,\n    - $s^{(1)} = [1.0, 1.2]$.\n  - 用例2（单位不匹配，两个变量：一个因尺度而占主导）：\n    - $T_2 = 500$, $n_2 = 2$,\n    - $b^{(2)} = [1.0, 0.9]$,\n    - $u^{(2)} = [0.1, 0.1]$,\n    - $s^{(2)} = [1000.0, 1.0]$.\n  - 用例3（单位不匹配，三个变量：一个巨大尺度，一个中等尺度，一个微小尺度）：\n    - $T_3 = 800$, $n_3 = 3$,\n    - $b^{(3)} = [0.2, 1.0, 1.0]$,\n    - $u^{(3)} = [0.3, 0.2, 0.2]$,\n    - $s^{(3)} = [1000.0, 1.0, 0.01]$.\n\n每个测试用例的必需输出：\n- 一个包含两个浮点数的列表 `[\\theta, \\Delta]`，其中 $\\theta$ 是以弧度为单位的角度，$\\Delta$ 是解释方差份额的绝对差值。两个值都必须四舍五入到恰好6位小数。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含一个由各用例列表组成的、以逗号分隔的列表，并用方括号括起来，例如 `[[\\theta_1,\\Delta_1],[\\theta_2,\\Delta_2],[\\theta_3,\\Delta_3]]`。每个浮点数需四舍五入到恰好6位小数，角度以弧度为单位。", "solution": "所提出的问题是计算统计学中一个有效且定义明确的练习，专门用于论证主成分分析（PCA）对变量尺度的敏感性。它在科学上是合理的，基于线性代数和统计学的基础原理，并且所有参数和过程都已足够清晰地说明，从而能够得出一个唯一的、可验证的解。我们将着手进行分析。\n\n其核心论点是，PCA作为一种方差最大化技术，不具备尺度不变性。当变量以迥异的单位度量时（例如，以美元计价的股价与以百万股计的交易量），方差最大的变量将在机制上主导第一个主成分。这通常是所选单位造成的一种人为现象，而非其真实内在重要性的指标。标准化是标准的补救措施，它将所有变量转换到同一尺度（单位方差），从而使分析的重点在于数据的相关性结构，而不是任意的度量尺度。\n\n我们首先将数据生成和分析流程形式化。\n\n**1. 数据生成过程**\n\n对于每个测试用例 $k$，给定样本量 $T_k$、变量数量 $n_k$、因子载荷 $b^{(k)} \\in \\mathbb{R}^{n_k}$、特异标准差 $u^{(k)} \\in \\mathbb{R}^{n_k}$ 以及单位尺度 $s^{(k)} \\in \\mathbb{R}^{n_k}$。\n\n数据由单因子模型生成。一个共同的潜在因子 $f_t$ 从标准正态分布中抽取，即 $f_t \\sim \\mathcal{N}(0, 1)$，此过程对每个时间点 $t=1, \\dots, T_k$ 进行。对于每个变量 $j=1, \\dots, n_k$，一个特异噪声项 $e_{t,j}$ 从 $\\mathcal{N}(0, (u^{(k)}_j)^2)$ 中抽取。所有的 $f_t$ 和 $e_{t,j}$ 都是相互独立的。\n\n在时间 $t$ 变量 $j$ 的观测值构造如下：\n$$\nx_{t,j} = s^{(k)}_j \\left( b^{(k)}_j f_t + e_{t,j} \\right)\n$$\n这就构成了一个数据矩阵 $X \\in \\mathbb{R}^{T_k \\times n_k}$，其列代表不同的变量。尺度因子 $s^{(k)}_j$ 代表变量 $j$ 的任意度量单位。\n\n**2. 对原始数据进行PCA（基于协方差的PCA）**\n\nPCA的第一步是通过减去列向样本均值来中心化数据。设 $\\bar{x}_j = \\frac{1}{T_k} \\sum_{t=1}^{T_k} x_{t,j}$ 为第 $j$ 个变量的样本均值。中心化后的数据矩阵，记为 $X_c$，其元素为 $(X_c)_{t,j} = x_{t,j} - \\bar{x}_j$。\n\n然后计算样本协方差矩阵 $\\Sigma_{\\text{raw}}$：\n$$\n\\Sigma_{\\text{raw}} = \\frac{1}{T_k-1} X_c^\\top X_c\n$$\n主成分是 $\\Sigma_{\\text{raw}}$ 的特征向量。我们对该矩阵进行特征分解：\n$$\n\\Sigma_{\\text{raw}} V = V \\Lambda\n$$\n其中 $V$ 是标准正交特征向量矩阵，$\\Lambda$ 是对应的特征值对角矩阵。特征值按降序排列，$\\lambda_1 \\ge \\lambda_2 \\ge \\dots \\ge \\lambda_{n_k}$。第一主成分是与最大特征值 $\\lambda_1$ 相关联的特征向量 $v_1$。在本问题中，我们将此特征向量记为 $v_{\\text{raw}}$，特征值记为 $\\lambda_{\\text{raw}}$。\n\n**3. 对标准化数据进行PCA（基于相关系数的PCA）**\n\n为了消除任意尺度的影响，我们对数据进行标准化。对于原始数据矩阵 $X$ 的每一列 $j$，我们计算其样本标准差，$\\hat{\\sigma}_j = \\sqrt{\\frac{1}{T_k-1} \\sum_{t=1}^{T_k} (x_{t,j} - \\bar{x}_j)^2}$。\n\n标准化数据矩阵 $Z$ 的元素构造如下：\n$$\nz_{t,j} = \\frac{x_{t,j} - \\bar{x}_j}{\\hat{\\sigma}_j}\n$$\n根据构造，$Z$ 的每一列的样本均值为0，样本方差为1。\n\n然后对这个标准化数据 $Z$ 执行PCA。相关矩阵是 $Z$ 的样本协方差矩阵，我们记为 $\\Sigma_{\\text{std}}$：\n$$\n\\Sigma_{\\text{std}} = \\frac{1}{T_k-1} Z^\\top Z\n$$\n由于 $Z$ 的每一列都具有单位方差，$\\Sigma_{\\text{std}}$ 的对角元素都为1，而非对角元素 $(i, j)$ 是原始变量 $x_i$ 和 $x_j$ 之间的样本相关系数。因此，$\\Sigma_{\\text{std}}$ 是 $X$ 的样本相关系数矩阵。\n\n我们对 $\\Sigma_{\\text{std}}$ 进行特征分解，以找到其最大特征值 $\\lambda_{\\text{std}}$ 和对应的特征向量 $v_{\\text{std}}$。\n\n**4. 诊断指标**\n\n为了量化因未进行标准化而引起的扭曲，我们计算两个指标：\n\n- **主成分之间的夹角**：主成分方向 $v_{\\text{raw}}$ 和 $v_{\\text{std}}$ 是 $\\mathbb{R}^{n_k}$ 中的单位向量。它们之间的夹角衡量了最大方差方向移动了多少。由于特征向量的定义仅在符号上是唯一的（即，如果 $v$ 是一个特征向量，那么 $-v$ 也是），我们计算它们所张成的直线之间的锐角：\n  $$\n  \\theta = \\arccos\\left( \\left| v_{\\text{raw}}^\\top v_{\\text{std}} \\right| \\right)\n  $$\n  $\\theta=0$ 的值表示完全对齐，而一个大的角度（接近 $\\pi/2$）表示严重错位。\n\n- **解释方差份额的差异**：第一主成分所解释的总方差比例由其特征值除以所有特征值之和给出。特征值之和等于矩阵的迹，$\\operatorname{tr}(\\Sigma) = \\sum_{j=1}^{n_k} \\Sigma_{jj}$，这代表了数据中的总方差。我们计算解释方差份额的绝对差异：\n  $$\n  \\Delta = \\left| \\frac{\\lambda_{\\text{raw}}}{\\operatorname{tr}(\\Sigma_{\\text{raw}})} - \\frac{\\lambda_{\\text{std}}}{\\operatorname{tr}(\\Sigma_{\\text{std}})} \\right|\n  $$\n  请注意，对于标准化数据，$\\operatorname{tr}(\\Sigma_{\\text{std}}) = n_k$，即变量的数量。一个大的 $\\Delta$ 值表明两种方法对第一主成分重要性的评估截然不同。\n\n将使用指定的参数和固定的随机种子，对每个测试用例执行此过程，以保证可复现性。预计结果将显示，用例1（尺度相似）的扭曲最小，而用例2和用例3（尺度迥异）的扭曲显著，从而验证了在实践中标准化的必要性。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the PCA problem by comparing results from raw and standardized data.\n\n    This function iterates through a set of predefined test cases. For each case, it:\n    1. Generates synthetic financial data based on a single-factor model with specified scales.\n    2. Performs PCA on the raw, centered data.\n    3. Performs PCA on the standardized data (equivalent to using the correlation matrix).\n    4. Computes two diagnostic metrics:\n        - The angle between the first principal components from the raw and standardized analyses.\n        - The absolute difference in the fraction of variance explained by the first component.\n    5. Collects and formats the results according to the problem specification.\n    \"\"\"\n    # Use a fixed pseudorandom number generator seed for reproducibility.\n    seed = 314159\n    rng = np.random.default_rng(seed)\n\n    # Test cases defined as (T_k, n_k, b^(k), u^(k), s^(k)))\n    test_cases = [\n        (500, 2, np.array([1.0, 0.9]), np.array([0.1, 0.1]), np.array([1.0, 1.2])),\n        (500, 2, np.array([1.0, 0.9]), np.array([0.1, 0.1]), np.array([1000.0, 1.0])),\n        (800, 3, np.array([0.2, 1.0, 1.0]), np.array([0.3, 0.2, 0.2]), np.array([1000.0, 1.0, 0.01]))\n    ]\n\n    all_results = []\n\n    for T, n, b, u, s in test_cases:\n        # 1. Data Generation\n        # Generate common factor f_t ~ N(0,1)\n        f = rng.normal(loc=0.0, scale=1.0, size=T)\n        \n        # Generate idiosyncratic noises e_{t,j} ~ N(0, u_j^2)\n        # E is a T x n matrix\n        E = rng.normal(loc=0.0, scale=u, size=(T, n))\n        \n        # Construct raw observations x_{t,j} = s_j * (b_j * f_t + e_{t,j})\n        X = s * (np.outer(f, b) + E)\n\n        # 2. PCA on Raw Data\n        # Center the data matrix X\n        X_centered = X - np.mean(X, axis=0)\n        \n        # Compute the sample covariance matrix (ddof=1 for unbiased estimator)\n        Sigma_raw = np.cov(X_centered, rowvar=False, ddof=1)\n        \n        # Eigendecomposition of the symmetric covariance matrix\n        # eigh returns eigenvalues in ascending order and corresponding eigenvectors in columns\n        eigvals_raw, eigvecs_raw = np.linalg.eigh(Sigma_raw)\n        \n        # The first principal component corresponds to the largest eigenvalue\n        lambda_raw = eigvals_raw[-1]\n        v_raw = eigvecs_raw[:, -1]\n\n        # 3. PCA on Standardized Data\n        # Standardize the data matrix X\n        stds = np.std(X, axis=0, ddof=1)\n        Z = X_centered / stds\n        \n        # Compute the sample correlation matrix (covariance of standardized data)\n        Sigma_std = np.cov(Z, rowvar=False, ddof=1)\n        \n        # Eigendecomposition of the correlation matrix\n        eigvals_std, eigvecs_std = np.linalg.eigh(Sigma_std)\n        \n        # The first principal component corresponds to the largest eigenvalue\n        lambda_std = eigvals_std[-1]\n        v_std = eigvecs_std[:, -1]\n\n        # 4. Diagnostic Computations\n        # Angle theta between the first principal components\n        # Take absolute value of dot product to handle sign ambiguity of eigenvectors\n        cos_theta = np.abs(np.dot(v_raw, v_std))\n        # Clip to prevent domain errors with arccos due to potential floating point inaccuracies\n        theta = np.arccos(np.clip(cos_theta, -1.0, 1.0))\n\n        # Difference in explained variance shares\n        total_var_raw = np.trace(Sigma_raw)\n        total_var_std = np.trace(Sigma_std) # This is always n for a correlation matrix\n        \n        share_raw = lambda_raw / total_var_raw\n        share_std = lambda_std / total_var_std\n        \n        delta = np.abs(share_raw - share_std)\n\n        # Append results, rounded to 6 decimal places\n        all_results.append([round(theta, 6), round(delta, 6)])\n\n    # Format the final output string as specified\n    formatted_results = [f\"[{theta:.6f},{delta:.6f}]\" for theta, delta in all_results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "2421735"}, {"introduction": "除了变量尺度不一致外，数据质量的另一个常见威胁是极端异常值的存在。由于 PCA 本质上是一个最大化方差的过程，它对数据中的极端值非常敏感，一个离群点就可能“绑架”整个分析。在这个练习 [@problem_id:2421778] 中，你将构建一个合成数据集，其中第一个主成分完全由一个精心安插的异常值所主导。这个实践将教会你在从 PCA 结果中得出结论之前，保持审慎并检查极端数据点的影响。", "problem": "您的任务是，从第一性原理出发，构建合成的横截面资产回报面板，其中主成分分析（PCA）的第一个主成分（PC）由单个极端观测值驱动。考虑一个包含 $T$ 个时间点和 $N$ 个资产的面板。令 $X \\in \\mathbb{R}^{T \\times N}$ 表示数据矩阵，其行是时间点，列是资产。通过从 $X$ 的每一列中减去其样本均值，来定义中心化矩阵 $X_c$。将第一个主成分（PC1）定义为任意单位向量 $v_1 \\in \\mathbb{R}^N$，该向量能最大化投影数据的样本方差，即 $v_1 \\in \\arg\\max_{\\|v\\|=1} \\mathrm{Var}(X_c v)$，其中 $\\mathrm{Var}(X_c v)$ 表示标量时间序列 $X_c v$ 的样本方差。将 PC1 的方差解释率定义为 $\\rho_1 = \\dfrac{\\mathrm{Var}(X_c v_1)}{\\sum_{j=1}^N \\mathrm{Var}(X_c e_j)}$，其中 $\\{e_j\\}$ 是 $\\mathbb{R}^N$ 中的标准基向量。\n\n构造 $X$ 的过程如下。对于给定的整数 $N \\ge 2$、$T \\ge 3$、非负振幅 $A \\ge 0$、时间索引 $t^\\star \\in \\{0,1,\\dots,T-1\\}$ 以及等于第一个标准基向量 $e_1$ 的单位方向 $u \\in \\mathbb{R}^N$，令 $Z \\in \\mathbb{R}^{T \\times N}$ 的条目是独立的、服从均值为零、方差为一的高斯分布的变量。通过设置 $X = Z$ 然后仅修改由 $t^\\star$ 索引的单行来定义 $X$，即 $X_{t^\\star,\\cdot} \\leftarrow X_{t^\\star,\\cdot} + A u^\\top$。行索引 $t^\\star$ 从零开始计数。\n\n对于每个指定的参数元组，您的程序必须：\n- 使用给定的 $(N,T,A,t^\\star)$ 按上述方式构造 $X$。\n- 为含有离群值的数据集计算 PC1 及其方差解释率 $\\rho_1$。\n- 移除离群观测值（从 $X$ 中删除由 $t^\\star$ 索引的行），通过列均值重新中心化剩余数据，重新计算 PC1，并在删减后的数据集上计算其方差解释率 $\\tilde{\\rho}_1$。\n- 计算包含离群值的数据集的 PC1 载荷向量与方向 $u$ 之间的绝对对齐度，即 $\\alpha = |\\langle v_1, u \\rangle|$。\n- 返回一个布尔值，指示以下所有主导条件是否同时成立：$\\rho_1 > \\tau_{\\mathrm{high}}$、$\\tilde{\\rho}_1 < \\tau_{\\mathrm{low}}$ 和 $\\alpha > \\tau_{\\mathrm{align}}$，其中 $(\\tau_{\\mathrm{high}}, \\tau_{\\mathrm{low}}, \\tau_{\\mathrm{align}})$ 是下面测试套件中提供的阈值。\n\n使用以下参数值测试套件，其中每个案例都是一个元组 $(N,T,A,t^\\star,\\tau_{\\mathrm{high}},\\tau_{\\mathrm{low}},\\tau_{\\mathrm{align}},\\text{seed})$：\n- 案例 A（理想路径，强离群值）：$(5, 200, 50, 100, 0.6, 0.35, 0.95, 20231102)$。\n- 案例 B（边界幅度离群值）：$(5, 200, 12, 150, 0.28, 0.27, 0.90, 20231103)$。\n- 案例 C（边界情况，无离群值）：$(8, 400, 0, 50, 0.25, 0.20, 0.90, 20231104)$。\n\n您的程序应生成单行输出，其中包含用方括号括起来的、以逗号分隔的结果列表。对于上述三个案例，输出格式必须严格为“[bA,bB,bC]”的形式，其中 $bA$、$bB$ 和 $bC$ 分别是对应于案例 A、B 和 C 的布尔值。", "solution": "该问题要求分析在一个合成的横截面数据集中，第一个主成分对单个显著离群值的敏感性。我们必须首先验证问题的完整性。该问题定义明确，科学上基于线性代数和统计学的原理，并且所有参数都已指定。它提出了稳健统计学中的一个标准计算任务。因此，该问题是有效的，我们可以继续进行求解。\n\n问题的核心在于应用主成分分析（PCA），这是一种降维技术，用于识别数据集中方差最大的方向。令 $X \\in \\mathbb{R}^{T \\times N}$ 为数据矩阵，其中 $T$ 代表时间点，$N$ 代表资产。PCA 的第一步是通过减去每列（资产）的均值来对数据进行中心化。这将得到中心化矩阵 $X_c$。\n\n第一个主成分（PC1）被定义为载荷向量 $v_1 \\in \\mathbb{R}^N$，它是一个单位向量，能够最大化数据投影到其上的方差。在数学上，这表示为：\n$$\nv_1 = \\arg\\max_{\\|v\\|=1} \\mathrm{Var}(X_c v)\n$$\n长度为 $T$ 的投影时间序列 $y = X_c v$ 的样本方差由 $\\mathrm{Var}(y) = \\frac{1}{T-1} \\sum_{i=1}^T (y_i - \\bar{y})^2$ 给出。由于 $X_c$ 是中心化的，任何投影 $X_c v$ 的均值都为零。因此，方差表达式得以简化，从而得到以下优化问题：\n$$\nv_1 = \\arg\\max_{\\|v\\|=1} \\frac{1}{T-1} (X_c v)^\\top (X_c v) = \\arg\\max_{\\|v\\|=1} v^\\top \\left(\\frac{1}{T-1} X_c^\\top X_c\\right) v = \\arg\\max_{\\|v\\|=1} v^\\top S v\n$$\n其中 $S = \\frac{1}{T-1} X_c^\\top X_c$ 是资产的样本协方差矩阵。根据 Rayleigh 商定理，最大化 $v^\\top S v$ 的向量 $v_1$ 是 $S$ 对应于其最大特征值 $\\lambda_1$ 的特征向量。该特征向量构成了第一个主成分的载荷向量。\n\nPC1 的方差解释率，记为 $\\rho_1$，是第一个主成分所捕获的总方差的比例。总方差是所有资产方差的总和，等于协方差矩阵的迹 $\\mathrm{Tr}(S)$。PC1 捕获的方差是 $\\lambda_1$。因此，\n$$\n\\rho_1 = \\frac{\\lambda_1}{\\mathrm{Tr}(S)}\n$$\n在计算上，对协方差矩阵 $S$ 进行特征分解可能数值不稳定，特别是对于病态数据。一种更稳健的方法是使用中心化数据矩阵 $X_c$ 的奇异值分解（SVD）。令 $X_c$ 的 SVD 为：\n$$\nX_c = U \\Sigma V^\\top\n$$\n其中 $U \\in \\mathbb{R}^{T \\times T}$ 和 $V \\in \\mathbb{R}^{N \\times N}$ 是正交矩阵，而 $\\Sigma \\in \\mathbb{R}^{T \\times N}$ 的对角线上包含奇异值 $s_k$。$V$ 的列是主成分载荷向量，因此 $v_1$是 $V$ 的第一列。$S$ 的特征值与 $X_c$ 的奇异值通过 $\\lambda_k = \\frac{s_k^2}{T-1}$ 相关联。方差解释率 $\\rho_1$ 可以用奇异值表示，这样可以巧妙地避免对样本大小项的依赖：\n$$\n\\rho_1 = \\frac{\\lambda_1}{\\sum_{k=1}^N \\lambda_k} = \\frac{s_1^2 / (T-1)}{\\sum_{k=1}^N s_k^2 / (T-1)} = \\frac{s_1^2}{\\sum_{k=1}^N s_k^2}\n$$\n所有计算都将使用这种基于 SVD 的方法。\n\n对于每个给定的案例 $(N, T, A, t^\\star, \\tau_{\\mathrm{high}}, \\tau_{\\mathrm{low}}, \\tau_{\\mathrm{align}}, \\text{seed})$，其步骤如下：\n\n1.  **数据生成：** 使用指定的随机种子以保证可复现性，生成一个基准矩阵 $Z \\in \\mathbb{R}^{T \\times N}$，其条目独立同分布于标准正态分布 $\\mathcal{N}(0, 1)$。通过在元素 $(t^\\star, 0)$ 处引入一个幅度为 $A$ 的点离群值来形成数据矩阵 $X$，这对应于方向 $u = e_1$：\n    $$\n    X = Z, \\quad X_{t^\\star, 0} \\leftarrow Z_{t^\\star, 0} + A\n    $$\n\n2.  **完整数据集分析：**\n    - 通过减去列样本均值将矩阵 $X$ 中心化，以获得 $X_c$。\n    - 计算 $X_c$ 的 SVD：$X_c = U \\Sigma V^\\top$。\n    - PC1 载荷向量是 $V$ 的第一列，对应于 $V^\\top$ 的第一行。令此为 $v_1$。\n    - 方差解释率计算为 $\\rho_1 = s_1^2 / \\sum_k s_k^2$。\n    - 与离群值方向 $u=e_1$ 的对齐度计算为 $\\alpha = |\\langle v_1, e_1 \\rangle| = |(v_1)_1|$，即 $v_1$ 第一个元素的绝对值。\n\n3.  **删减数据集分析：**\n    - 从 $X$ 中删除离群观测值（即索引为 $t^\\star$ 的整行），形成一个新矩阵 $X' \\in \\mathbb{R}^{(T-1) \\times N}$。\n    - 使用其自身的列样本均值将这个删减后的矩阵 $X'$ 重新中心化，生成 $X'_c$。此时观测数量为 $T-1$。\n    - 计算 $X'_c$ 的 SVD。令新的奇异值为 $s'_k$。\n    - 删减数据集 PC1 的方差解释率计算为 $\\tilde{\\rho}_1 = (s'_1)^2 / \\sum_k (s'_k)^2$。\n\n4.  **主导条件验证：** 最后一步是评估所有三个指定条件是否同时满足：\n    $$\n    (\\rho_1 > \\tau_{\\mathrm{high}}) \\land (\\tilde{\\rho}_1 < \\tau_{\\mathrm{low}}) \\land (\\alpha > \\tau_{\\mathrm{align}})\n    $$\n    该案例的结果是一个布尔值，代表此逻辑表达式的结果。对每个提供的测试案例重复此完整过程。实现将严格遵循这些步骤。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem for the given test cases.\n    For each case, it constructs a synthetic dataset with an outlier,\n    analyzes its principal components, re-analyzes the dataset after\n    removing the outlier, and checks if a set of dominance conditions are met.\n    \"\"\"\n    # Test suite format: (N, T, A, t_star, tau_high, tau_low, tau_align, seed)\n    test_cases = [\n        (5, 200, 50, 100, 0.6, 0.35, 0.95, 20231102),  # Case A\n        (5, 200, 12, 150, 0.28, 0.27, 0.90, 20231103),  # Case B\n        (8, 400, 0, 50, 0.25, 0.20, 0.90, 20231104),   # Case C\n    ]\n\n    results = []\n    for case in test_cases:\n        N, T, A, t_star, tau_high, tau_low, tau_align, seed = case\n        \n        # --- Analysis with Outlier ---\n\n        # 1. Generate Data\n        rng = np.random.default_rng(seed)\n        Z = rng.standard_normal(size=(T, N))\n        X = Z.copy()\n        # Add the outlier. u is e_1, so we modify the first column (index 0).\n        if A > 0:\n            X[t_star, 0] += A\n\n        # 2. Center Data\n        X_c = X - X.mean(axis=0)\n\n        # 3. Perform PCA (via SVD) and Calculate Metrics\n        # We use full_matrices=False for efficiency\n        _, s, Vh = np.linalg.svd(X_c, full_matrices=False)\n        \n        # Explained variance ratio rho_1\n        rho_1 = (s[0]**2) / np.sum(s**2)\n        \n        # PC1 loading vector v_1 is the first row of Vh (V transpose)\n        v_1 = Vh[0, :]\n        \n        # Alignment alpha with u = e_1\n        # <v_1, e_1> is just the first element of v_1\n        alpha = np.abs(v_1[0])\n\n        # --- Analysis without Outlier ---\n        \n        # 1. Remove Outlier Row\n        X_prime = np.delete(X, t_star, axis=0)\n        \n        # 2. Re-center Reduced Data\n        X_prime_c = X_prime - X_prime.mean(axis=0)\n        \n        # 3. Perform PCA on Reduced Data\n        _, s_prime, _ = np.linalg.svd(X_prime_c, full_matrices=False)\n        \n        # Explained variance ratio tilde_rho_1\n        tilde_rho_1 = (s_prime[0]**2) / np.sum(s_prime**2)\n\n        # --- Final Check ---\n        \n        # 4. Evaluate Dominance Conditions\n        holds = (rho_1 > tau_high) and (tilde_rho_1  tau_low) and (alpha > tau_align)\n        results.append(holds)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2421778"}]}