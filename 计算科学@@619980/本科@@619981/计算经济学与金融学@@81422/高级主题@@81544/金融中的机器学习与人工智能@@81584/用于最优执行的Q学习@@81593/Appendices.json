{"hands_on_practices": [{"introduction": "在我们训练一个智能体学习最优策略之前，我们必须首先精确地构建它将要互动的“世界”。本练习将引导你实现一个交易执行环境的核心动力学。你将通过编写代码来模拟一个预设的交易策略，并计算其在包含价格冲击和库存惩罚的复杂环境下的总回报。通过这个实践 [@problem_id:2423600]，你将对强化学习问题中的状态、动作、转移和奖励等基本构件建立一个坚实而直观的理解。", "problem": "给定一个在有限期界内出售固定库存的离散时间执行环境。在每个时间步，交易员选择一个非负整数动作，指定要卖出的数量。该交易以包含临时价格影响的执行价格产生即时现金流，并永久性地移动后续步骤中使用的中间价。如果在截止日期前全部库存未能清算，则会施加期末惩罚。目标是为每个指定的参数集计算给定动作序列所获得的总回合奖励。\n\n形式上，令时间索引为 $t \\in \\{0,1,\\dots,T-1\\}$，截止日期为 $T$。状态包含当前中间价 $S_t \\in \\mathbb{R}$ 和剩余库存 $x_t \\in \\mathbb{N}_0$。交易员选择一个意图动作 $a_t \\in \\mathbb{N}_0$，代表要卖出的单位数量。执行数量为\n$$\na'_t \\equiv \\min\\{a_t, x_t\\}。\n$$\n在时间 $t$ 的执行价格是\n$$\n\\tilde{S}_t \\equiv S_t - \\eta\\, a'_t,\n$$\n其中 $\\eta \\ge 0$ 是临时影响系数。在时间 $t$ 的即时奖励（现金流）是\n$$\nr_t \\equiv a'_t \\, \\tilde{S}_t = a'_t \\left(S_t - \\eta\\, a'_t\\right)。\n$$\n库存和价格演化如下\n$$\nx_{t+1} = x_t - a'_t,\\qquad\nS_{t+1} = S_t - \\kappa\\, a'_t + \\epsilon_t,\n$$\n其中 $\\kappa \\ge 0$ 是永久影响系数，而 $\\{\\epsilon_t\\}_{t=0}^{T-1}$ 是一个指定的外生价格冲击的确定性序列。在期末时间 $T$，对任何剩余库存施加惩罚：\n$$\nR_T \\equiv -\\lambda\\, x_T^2,\n$$\n其中 $\\lambda \\ge 0$。总回合奖励为\n$$\nG \\equiv \\sum_{t=0}^{T-1} r_t + R_T。\n$$\n所有价格、成本和奖励均以任意货币单位计；不涉及物理单位。\n\n实现一个程序，为下面测试套件中的每个参数集计算给定动作序列的 $G$。对于每个案例，使用上述动态和给定参数。您必须将 $a_t$ 视为意图动作，并按规定执行 $a'_t = \\min\\{a_t, x_t\\}$。\n\n测试套件（每个案例指定 $(S_0, x_0, T, \\kappa, \\eta, \\lambda, \\{a_t\\}_{t=0}^{T-1}, \\{\\epsilon_t\\}_{t=0}^{T-1})$）：\n- 案例 1：$S_0 = 100.0$，$x_0 = 5$，$T = 3$，$\\kappa = 0.2$，$\\eta = 0.5$，$\\lambda = 10.0$，动作为 [2,2,1]，冲击为 [0.0,0.0,0.0]。\n- 案例 2：$S_0 = 100.0$，$x_0 = 5$，$T = 3$，$\\kappa = 0.2$，$\\eta = 0.5$，$\\lambda = 10.0$，动作为 [1,1,1]，冲击为 [0.0,0.0,0.0]。\n- 案例 3：$S_0 = 100.0$，$x_0 = 0$，$T = 3$，$\\kappa = 0.2$，$\\eta = 0.5$，$\\lambda = 10.0$，动作为 [2,2,2]，冲击为 [0.0,0.0,0.0]。\n- 案例 4：$S_0 = 50.0$，$x_0 = 3$，$T = 2$，$\\kappa = 0.3$，$\\eta = 1.0$，$\\lambda = 5.0$，动作为 [2,2]，冲击为 [1.5,0.0]。\n- 案例 5：$S_0 = 20.0$，$x_0 = 10$，$T = 2$，$\\kappa = 0.0$，$\\eta = 0.0$，$\\lambda = 100.0$，动作为 [0,0]，冲击为 [0.0,0.0]。\n- 案例 6：$S_0 = 100.0$，$x_0 = 4$，$T = 2$，$\\kappa = 0.5$，$\\eta = 0.25$，$\\lambda = 1.0$，动作为 [3,1]，冲击为 [-10.0,0.0]。\n\n您的程序应生成一行输出，其中包含六个案例的结果，格式为方括号内以逗号分隔的列表。每个结果必须保留六位小数。输出必须遵循以下确切格式：\n- 仅一行。\n- 列表用方括号括起来。\n- 值之间用逗号分隔。\n- 每个值显示小数点后恰好六位数字。", "solution": "问题陈述是有效的。它在计算金融领域，特别是关于最优执行模型，提出了一个定义明确、具有科学依据的任务。该问题要求在离散的有限时间期界内模拟一个交易过程，其所有控制方程、参数和初始条件都已明确无误地给出。目标是为一个给定的动作序列计算总回合奖励，这是一个基于指定模型动态的确定性计算。\n\n该模型通过中间价 $S_t$ 和剩余库存 $x_t$ 来描述系统在时间 $t$ 的状态。该过程在时间步 $t \\in \\{0, 1, \\dots, T-1\\}$ 上进行模拟。\n\n为给定的参数集 $(S_0, x_0, T, \\kappa, \\eta, \\lambda, \\{a_t\\}_{t=0}^{T-1}, \\{\\epsilon_t\\}_{t=0}^{T-1})$ 计算总回合奖励 $G$ 的步骤如下。\n\n首先，将总奖励累加器初始化为零，我们称之为 $G_{acc} = 0$。初始状态由 $(S_0, x_0)$ 给出。\n\n接下来，我们从 $0$ 到 $T-1$ 遍历每个时间步 $t$。在每一步中：\n$1$. 确定执行数量 $a'_t$。由于交易员卖出的库存不能超过可用库存，执行数量是意图动作 $a_t$ 和当前库存 $x_t$ 的最小值。\n$$\na'_t = \\min\\{a_t, x_t\\}\n$$\n$2$. 计算在时间 $t$ 获得的即时奖励 $r_t$。这是销售产生的现金流，等于执行数量 $a'_t$ 与执行价格 $\\tilde{S}_t$ 的乘积。执行价格是中间价 $S_t$ 根据与交易规模成比例（系数为 $\\eta$）的临时价格影响进行调整后的价格。\n$$\nr_t = a'_t \\cdot \\tilde{S}_t = a'_t \\cdot (S_t - \\eta \\cdot a'_t)\n$$\n$3$. 将此即时奖励加到累积总奖励中：\n$$\nG_{acc} \\leftarrow G_{acc} + r_t\n$$\n$4$. 为下一个时间步 $t+1$ 更新状态变量。库存因执行数量 $a'_t$ 而减少。中间价通过考虑与 $a'_t$ 成比例（系数为 $\\kappa$）的永久价格影响以及外生价格冲击 $\\epsilon_t$ 来更新。\n$$\nx_{t+1} = x_t - a'_t\n$$\n$$\nS_{t+1} = S_t - \\kappa \\cdot a'_t + \\epsilon_t\n$$\n此循环持续到 $t=T-1$。循环的最后一步之后，状态为 $(S_T, x_T)$。\n\n最后，我们对在截止日期 $T$ 仍未售出的任何库存 $x_T$ 施加期末惩罚 $R_T$。该惩罚是剩余库存的二次函数，惩罚系数为 $\\lambda$。\n$$\nR_T = -\\lambda \\cdot x_T^2\n$$\n总回合奖励 $G$ 是在整个交易期界内累积的所有即时奖励与期末惩罚之和。\n$$\nG = G_{acc} + R_T = \\left(\\sum_{t=0}^{T-1} r_t\\right) + R_T\n$$\n对于每个提供的测试案例，此过程都会为 $G$ 产生一个唯一的、确定性的值。该计算是所给公式的直接应用。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the total episodic reward for a series of optimal execution problems.\n    \"\"\"\n    # Test suite (S0, x0, T, kappa, eta, lambda, actions, shocks)\n    test_cases = [\n        # Case 1\n        (100.0, 5, 3, 0.2, 0.5, 10.0, [2, 2, 1], [0.0, 0.0, 0.0]),\n        # Case 2\n        (100.0, 5, 3, 0.2, 0.5, 10.0, [1, 1, 1], [0.0, 0.0, 0.0]),\n        # Case 3\n        (100.0, 0, 3, 0.2, 0.5, 10.0, [2, 2, 2], [0.0, 0.0, 0.0]),\n        # Case 4\n        (50.0, 3, 2, 0.3, 1.0, 5.0, [2, 2], [1.5, 0.0]),\n        # Case 5\n        (20.0, 10, 2, 0.0, 0.0, 100.0, [0, 0], [0.0, 0.0]),\n        # Case 6\n        (100.0, 4, 2, 0.5, 0.25, 1.0, [3, 1], [-10.0, 0.0]),\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        s0, x0, T, kappa, eta, lambd, actions, shocks = case\n        \n        # Initialize state variables\n        s_t = s0\n        x_t = x0\n        total_reward = 0.0\n\n        # Simulate the execution process over the time horizon\n        for t in range(T):\n            action_t = actions[t]\n            shock_t = shocks[t]\n\n            # 1. Determine executed quantity\n            executed_quantity = min(action_t, x_t)\n\n            # 2. Calculate immediate reward\n            if executed_quantity > 0:\n                execution_price = s_t - eta * executed_quantity\n                immediate_reward = executed_quantity * execution_price\n                total_reward += immediate_reward\n            \n            # 3. Update state for the next time step\n            x_t_plus_1 = x_t - executed_quantity\n            s_t_plus_1 = s_t - kappa * executed_quantity + shock_t\n            \n            # Move to the next state\n            x_t = x_t_plus_1\n            s_t = s_t_plus_1\n\n        # 4. Calculate terminal penalty\n        terminal_penalty = -lambd * (x_t ** 2)\n        total_reward += terminal_penalty\n        \n        results.append(total_reward)\n\n    # Format the results as specified\n    formatted_results = [f\"{r:.6f}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "2423600"}, {"introduction": "奖励函数是引导智能体学习的唯一信号，它定义了何为“好”的行为。本练习聚焦于为最优执行问题设计奖励函数的艺术与科学。你将构建一个单步奖励函数 [@problem_id:2423592]，它需要精妙地平衡多个相互冲突的目标：获得优于市场中间价的执行价格、最小化因主动交易（“跨越价差”）造成的成本，以及惩罚未完成的交易库存。", "problem": "考虑为强化学习 (RL) 中的动作价值函数 (Q-函数) 设计一个单步奖励，该设计应用于限价订单市场中单一资产的最优执行。设决策时间点的状态包括当前中间价 $m$、瞬时买卖价差 $s>0$，以及代理的目标方向 $\\sigma \\in \\{+1,-1\\}$，其中 $\\sigma=+1$ 表示买入目标 (获取股份)，$\\sigma=-1$ 表示卖出目标 (清算股份)。在该步骤中，代理以平均执行价格 $p$ 执行 $x \\ge 0$ 股，产生一个指示符 $I_{\\text{cross}} \\in \\{0,1\\}$，如果该行为穿过价差 (市价行为)，则指示符等于 $1$，如果该行为是被动的 (非市价行为)，则等于 $0$。该步骤之后，剩余库存为 $R \\ge 0$。参数 $\\lambda_{\\text{cross}} \\ge 0$ 和 $\\lambda_{\\text{rem}} \\ge 0$ 是给定的非负权重。\n\n将每步奖励 $r$ 定义为每股价格改善和穿越指示符的唯一仿射函数，该函数同时满足以下原则：\n- 价格改善原则：每股贡献为 $\\sigma (m - p)$，奖励那些根据目标方向，执行价格优于中间价的执行。\n- 穿越价差抑制：穿越行为会产生额外的每股惩罚，该惩罚与价差 $s$ 成正比，由 $\\lambda_{\\text{cross}}$ 加权，并由 $I_{\\text{cross}}$ 激活。\n- 未执行库存惩罚：与剩余库存 $R$ 成正比的惩罚，由 $\\lambda_{\\text{rem}}$ 加权。\n\n这些原则意味着奖励为\n$$\nr \\;=\\; x \\,\\big[\\, \\sigma (m - p) \\;-\\; \\lambda_{\\text{cross}} \\, I_{\\text{cross}} \\, s \\,\\big] \\;-\\; \\lambda_{\\text{rem}} \\, R.\n$$\n\n实现一个程序，对下面的每个测试用例，使用输入的 $(m, s, \\sigma, p, x, I_{\\text{cross}}, R, \\lambda_{\\text{cross}}, \\lambda_{\\text{rem}})$ 值计算 $r$，所有算术运算均以实数进行。输出必须四舍五入到六位小数。\n\n使用以下测试套件：\n- 测试用例 1 (买入，在买价被动成交): $m=100.00$, $s=0.02$, $\\sigma=+1$, $p=99.99$, $x=100$, $I_{\\text{cross}}=0$, $R=900$, $\\lambda_{\\text{cross}}=0.5$, $\\lambda_{\\text{rem}}=0.001$。\n- 测试用例 2 (买入，在卖价市价成交): $m=100.00$, $s=0.02$, $\\sigma=+1$, $p=100.01$, $x=100$, $I_{\\text{cross}}=1$, $R=900$, $\\lambda_{\\text{cross}}=0.5$, $\\lambda_{\\text{rem}}=0.001$。\n- 测试用例 3 (卖出，在卖价被动成交): $m=250.50$, $s=0.10$, $\\sigma=-1$, $p=250.55$, $x=50$, $I_{\\text{cross}}=0$, $R=0$, $\\lambda_{\\text{cross}}=0.25$, $\\lambda_{\\text{rem}}=0.0$。\n- 测试用例 4 (零价差边界情况): $m=10.00$, $s=0.00$, $\\sigma=+1$, $p=10.00$, $x=10$, $I_{\\text{cross}}=1$, $R=0$, $\\lambda_{\\text{cross}}=999.0$, $\\lambda_{\\text{rem}}=0.0$。\n- 测试用例 5 (无执行，仅有剩余库存惩罚): $m=100.00$, $s=0.20$, $\\sigma=+1$, $p=100.00$, $x=0$, $I_{\\text{cross}}=0$, $R=1000$, $\\lambda_{\\text{cross}}=0.5$, $\\lambda_{\\text{rem}}=0.01$。\n- 测试用例 6 (卖出，在买价市价成交但部分完成): $m=50.00$, $s=0.04$, $\\sigma=-1$, $p=49.98$, $x=200$, $I_{\\text{cross}}=1$, $R=300$, $\\lambda_{\\text{cross}}=0.1$, $\\lambda_{\\text{rem}}=0.05$。\n\n您的程序应生成单行输出，包含一个用方括号括起来的逗号分隔列表形式的结果 (例如, $[r_1,r_2,\\dots]$)，其中每个 $r_i$ 都四舍五入到六位小数，并且没有附加文本。", "solution": "所提出的问题要求为参与最优执行的 Q-学习代理计算单步奖励，记为 $r$。在尝试解决方案之前，需要对问题陈述进行验证。\n\n首先，我们提取给定信息。\n状态和动作空间变量定义如下：\n- 中间价: $m$\n- 买卖价差: $s > 0$\n- 代理的目标方向: $\\sigma \\in \\{+1, -1\\}$，其中 $+1$ 为买入，$-1$ 为卖出。\n- 执行股数: $x \\ge 0$\n- 平均执行价格: $p$\n- 穿越价差指示符: $I_{\\text{cross}} \\in \\{0, 1\\}$\n- 剩余库存: $R \\ge 0$\n- 非负权重: $\\lambda_{\\text{cross}} \\ge 0$ 和 $\\lambda_{\\text{rem}} \\ge 0$。\n\n单步奖励函数 $r$ 被明确定义为：\n$$\nr \\;=\\; x \\,\\big[\\, \\sigma (m - p) \\;-\\; \\lambda_{\\text{cross}} \\, I_{\\text{cross}} \\, s \\,\\big] \\;-\\; \\lambda_{\\text{rem}} \\, R.\n$$\n问题是为给定的参数测试套件计算 $r$。\n\n接下来，我们验证问题。\n1.  **科学依据**：该公式在计算金融和算法交易中是标准的。奖励函数是三个关键绩效指标的线性组合：执行差额（价格改善）、穿越价差成本和库存惩罚。这些是市场微观结构中基础且完善的概念。该模型是对最优执行中权衡关系的有效（尽管简化了的）表示。\n2.  **适定性**：该问题要求直接评估给定的代数表达式。对于测试用例中提供的每组输入，该公式都会产生一个唯一的、定义明确的实数。该设定既不是欠约束也不是过约束的。\n3.  **客观性**：问题以精确的数学语言陈述，没有歧义、主观性或意见。一个测试用例包含 $s=0$，这在技术上违反了前提 $s>0$。然而，这是一个边界情况，并不使公式本身失效，因为公式对于 $s=0$ 是有明确定义的。因此，该问题被认为是有效的。\n\n问题是有效的。我们继续进行求解。计算过程是为每个测试用例直接应用所提供的公式。\n\n根据指导原则，奖励函数可以分解为三个组成部分：\n1.  **价格改善项**：$x \\cdot \\sigma (m - p)$。该项衡量相对于决策时中间价 $m$ 的表现。如果 $\\sigma = +1$ (买入)，当执行价格 $p$ 低于 $m$ 时获得正奖励。如果 $\\sigma = -1$ (卖出)，当 $p$ 高于 $m$ 时获得正奖励。这与低买高卖的目标一致。总贡献按执行股数 $x$ 进行缩放。\n2.  **穿越价差惩罚项**：$-x \\cdot \\lambda_{\\text{cross}} \\cdot I_{\\text{cross}} \\cdot s$。该项惩罚为获得即时执行而“穿越价差”的激进行为。指示符 $I_{\\text{cross}}=1$ 激活此惩罚。其大小与股数 $x$、当前价差 $s$ 和参数 $\\lambda_{\\text{cross}}$ 成正比。如果行为是被动的 ($I_{\\text{cross}}=0$)，则该项为零。\n3.  **剩余库存惩罚项**：$-\\lambda_{\\text{rem}} \\cdot R$。该项惩罚代理未能完成其执行目标，以剩余库存 $R$ 表示。其大小由参数 $\\lambda_{\\text{rem}}$ 控制。\n\n我们现在为每个测试用例计算奖励 $r$。\n\n**测试用例 1**：\n- 输入: $m=100.00$, $s=0.02$, $\\sigma=+1$, $p=99.99$, $x=100$, $I_{\\text{cross}}=0$, $R=900$, $\\lambda_{\\text{cross}}=0.5$, $\\lambda_{\\text{rem}}=0.001$。\n- 计算:\n$$\nr_1 = 100 \\cdot [+1 \\cdot (100.00 - 99.99) - 0.5 \\cdot 0 \\cdot 0.02] - 0.001 \\cdot 900\n$$\n$$\nr_1 = 100 \\cdot (0.01 - 0) - 0.9 = 1.0 - 0.9 = 0.1\n$$\n\n**测试用例 2**：\n- 输入: $m=100.00$, $s=0.02$, $\\sigma=+1$, $p=100.01$, $x=100$, $I_{\\text{cross}}=1$, $R=900$, $\\lambda_{\\text{cross}}=0.5$, $\\lambda_{\\text{rem}}=0.001$。\n- 计算:\n$$\nr_2 = 100 \\cdot [+1 \\cdot (100.00 - 100.01) - 0.5 \\cdot 1 \\cdot 0.02] - 0.001 \\cdot 900\n$$\n$$\nr_2 = 100 \\cdot (-0.01 - 0.01) - 0.9 = 100 \\cdot (-0.02) - 0.9 = -2.0 - 0.9 = -2.9\n$$\n\n**测试用例 3**：\n- 输入: $m=250.50$, $s=0.10$, $\\sigma=-1$, $p=250.55$, $x=50$, $I_{\\text{cross}}=0$, $R=0$, $\\lambda_{\\text{cross}}=0.25$, $\\lambda_{\\text{rem}}=0.0$。\n- 计算:\n$$\nr_3 = 50 \\cdot [-1 \\cdot (250.50 - 250.55) - 0.25 \\cdot 0 \\cdot 0.10] - 0.0 \\cdot 0\n$$\n$$\nr_3 = 50 \\cdot [-1 \\cdot (-0.05) - 0] - 0 = 50 \\cdot (0.05) = 2.5\n$$\n\n**测试用例 4**：\n- 输入: $m=10.00$, $s=0.00$, $\\sigma=+1$, $p=10.00$, $x=10$, $I_{\\text{cross}}=1$, $R=0$, $\\lambda_{\\text{cross}}=999.0$, $\\lambda_{\\text{rem}}=0.0$。\n- 计算:\n$$\nr_4 = 10 \\cdot [+1 \\cdot (10.00 - 10.00) - 999.0 \\cdot 1 \\cdot 0.00] - 0.0 \\cdot 0\n$$\n$$\nr_4 = 10 \\cdot (0 - 0) - 0 = 0.0\n$$\n\n**测试用例 5**：\n- 输入: $m=100.00$, $s=0.20$, $\\sigma=+1$, $p=100.00$, $x=0$, $I_{\\text{cross}}=0$, $R=1000$, $\\lambda_{\\text{cross}}=0.5$, $\\lambda_{\\text{rem}}=0.01$。\n- 计算: 由于 $x=0$，整个第一项为 $0$。\n$$\nr_5 = 0 \\cdot [\\dots] - 0.01 \\cdot 1000 = 0 - 10.0 = -10.0\n$$\n\n**测试用例 6**：\n- 输入: $m=50.00$, $s=0.04$, $\\sigma=-1$, $p=49.98$, $x=200$, $I_{\\text{cross}}=1$, $R=300$, $\\lambda_{\\text{cross}}=0.1$, $\\lambda_{\\text{rem}}=0.05$。\n- 计算:\n$$\nr_6 = 200 \\cdot [-1 \\cdot (50.00 - 49.98) - 0.1 \\cdot 1 \\cdot 0.04] - 0.05 \\cdot 300\n$$\n$$\nr_6 = 200 \\cdot [-1 \\cdot (0.02) - 0.004] - 15.0 = 200 \\cdot (-0.02 - 0.004) - 15.0\n$$\n$$\nr_6 = 200 \\cdot (-0.024) - 15.0 = -4.8 - 15.0 = -19.8\n$$\n\n最终结果四舍五入到六位小数后为：$r_1=0.100000$，$r_2=-2.900000$，$r_3=2.500000$，$r_4=0.000000$，$r_5=-10.000000$，以及 $r_6=-19.800000$。这些结果将在程序中实现。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the single-step reward 'r' for a series of test cases in an\n    optimal execution problem, based on a given formula.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    # Each tuple represents a test case with parameters:\n    # (m, s, sigma, p, x, I_cross, R, lambda_cross, lambda_rem)\n    test_cases = [\n        (100.00, 0.02, +1, 99.99, 100, 0, 900, 0.5, 0.001),  # Test case 1\n        (100.00, 0.02, +1, 100.01, 100, 1, 900, 0.5, 0.001),  # Test case 2\n        (250.50, 0.10, -1, 250.55, 50, 0, 0, 0.25, 0.0),      # Test case 3\n        (10.00, 0.00, +1, 10.00, 10, 1, 0, 999.0, 0.0),       # Test case 4\n        (100.00, 0.20, +1, 100.00, 0, 0, 1000, 0.5, 0.01),     # Test case 5\n        (50.00, 0.04, -1, 49.98, 200, 1, 300, 0.1, 0.05),     # Test case 6\n    ]\n\n    results = []\n    for case in test_cases:\n        # Unpack the parameters for clarity\n        m, s, sigma, p, x, I_cross, R, lambda_cross, lambda_rem = case\n        \n        # Calculate the reward 'r' using the provided formula:\n        # r = x * [sigma * (m - p) - lambda_cross * I_cross * s] - lambda_rem * R\n        \n        price_improvement_term = sigma * (m - p)\n        spread_crossing_penalty = lambda_cross * I_cross * s\n        execution_component = x * (price_improvement_term - spread_crossing_penalty)\n        \n        inventory_penalty = lambda_rem * R\n        \n        reward = execution_component - inventory_penalty\n        \n        # Round the result to six decimal places as required.\n        rounded_reward = round(reward, 6)\n        results.append(f\"{rounded_reward:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "2423592"}, {"introduction": "在定义了环境和奖励结构之后，是时候让我们的智能体开始学习了。这个综合性练习将指导你完整地实现 Q-learning 算法，来解决一个风格化的最优执行问题。你将亲自训练一个智能体，并探索折扣因子 $\\gamma$ 这个关键超参数如何影响其最终学到的交易策略 [@problem_id:2423640]。通过观察智能体行为从“短视”到“远见”的变化，你将深刻理解智能体在即时收益与长期目标之间进行权衡的内在机制。", "problem": "给定一个程式化的最优执行问题，该问题被表述为一个离散时间的马尔可夫决策过程（MDP）。一个强化学习（RL）智能体通过标准的Q学习递归来学习一个动作价值函数。目标是检验折扣因子 $\\gamma$ 如何影响智能体的有效交易期限。有效交易期限定义为，在由学习到的动作价值函数引出的贪心策略下，完全清算一个固定库存所需的时间步数。\n\nMDP 的具体规定如下。\n\n- 时间：$t \\in \\{0,1,\\dots,T\\}$，具有一个固定的期限 $T$。当 $t=T$ 或库存降为零时，以先发生者为准，该回合终止。\n- 状态：$s_t=(t,x_t)$，其中库存 $x_t \\in \\{0,1,\\dots,X_0\\}$ 是在时间 $t$ 剩余的离散单位数量。\n- 动作：在状态 $s_t=(t,x_t)$，智能体选择一个整数交易规模 $a_t \\in \\{0,1,\\dots,\\min(s_{\\max},x_t)\\}$，表示在时间段 $t$ 内卖出的单位数量。\n- 转移：库存确定性地演变为 $x_{t+1}=x_t - a_t$，时间递增 $t \\mapsto t+1$。\n- 即时奖励：在状态 $s_t=(t,x_t)$ 采取动作 $a_t$ 的奖励为\n$$\nr_t \\;=\\; -\\big(k a_t^2 \\;+\\; \\lambda x_t^2\\big),\n$$\n其中 $k>0$ 是交易成本曲率，$\\lambda>0$ 是库存持有风险惩罚。终止状态的持续价值为零。\n- 目标：对于一个固定的折扣因子 $\\gamma \\in [0,1]$，智能体寻求最大化期望折扣回报 $\\sum_{t=0}^{T-1} \\gamma^t r_t$。\n\nQ学习智能体根据以下递归式更新动作价值估计\n$$\nQ(s_t,a_t) \\;\\leftarrow\\; (1-\\alpha)\\,Q(s_t,a_t) \\;+\\; \\alpha \\left[r_t \\;+\\; \\gamma \\,\\max_{a' \\in \\mathcal{A}(s_{t+1})} Q(s_{t+1},a')\\right],\n$$\n并约定，如果 $s_{t+1}$ 是终止状态（即 $t+1=T$ 或 $x_{t+1}=0$），则目标值简化为 $r_t$，因为没有持续价值。这里 $\\alpha \\in (0,1]$ 是学习率，$\\mathcal{A}(s)$ 表示在状态 $s$ 的可行动作集合。在学习过程中，智能体遵循一个 $\\varepsilon$-贪心行为策略：以概率 $\\varepsilon$ 均匀随机地选择一个可行动作，以概率 $1-\\varepsilon$ 选择一个使当前 $Q$ 估计值最大化的贪心动作。$\\operatorname{argmax}$ 中的平局总是通过选择最大的动作来打破（即，在所有最大化项中选择最大的 $a$）。\n\n训练期间用于动作选择的所有随机性都必须由一个以种子 $0$ 初始化的伪随机数生成器产生，以确保可复现性。\n\n使用的参数值如下：\n- 期限 $T=10$。\n- 初始库存 $X_0=10$。\n- 每期最大卖出规模 $s_{\\max}=3$。\n- 交易成本曲率 $k=0.05$。\n- 库存持有风险惩罚 $\\lambda=0.10$。\n- 学习率 $\\alpha=0.10$。\n- 探索概率 $\\varepsilon=0.20$。\n- 训练回合数 $N_{\\text{episodes}}=20000$。\n- 每个训练回合从固定的初始状态 $s_0=(0,X_0)$ 开始。\n\n对于下面测试集中的每个折扣因子 $\\gamma$，你必须：\n- 根据上述规格训练Q学习智能体。\n- 训练后，导出由学习到的动作价值函数引出的贪心策略。\n- 从固定的初始状态 $s_0=(0,X_0)$ 开始，模拟无探索的贪心策略，以获得清算时间 $L(\\gamma)$，其定义为使得 $x_{\\ell}=0$ 成立的最小整数 $\\ell \\in \\{0,1,\\dots,T\\}$。如果在 $T$ 时刻前库存未被完全清算，则定义 $L(\\gamma)=T$。\n\n折扣因子测试集：\n- $\\gamma \\in \\{0.0, 0.3, 0.6, 0.9, 0.99, 1.0\\}$。\n\n你的程序必须输出六个清算时间，形式为单行、方括号内用逗号分隔的列表，顺序与测试集中的顺序一致。要求的最终输出格式是：\n- 单行：例如，$[1,2,3,4,5,6]$，其中每个条目是一个整数清算时间，对应于测试集中相同位置的折扣因子。输出行中不允许有空格。", "solution": "所呈现的问题是计算金融领域内，特别是最优交易执行方面，一个有效且适定的最优控制问题。它要求实现Q学习算法来求解一个离散时间的马尔可夫决策过程（MDP）。我将首先阐明其理论基础，然后描述获得解决方案的算法流程。\n\n该问题被表述为一个由元组 $(\\mathcal{S}, \\mathcal{A}, P, R, \\gamma)$ 定义的MDP。\n- 状态空间 $\\mathcal{S}$ 由状态对 $s_t=(t,x_t)$ 组成，其中 $t \\in \\{0, 1, \\dots, T\\}$ 是时间步， $x_t \\in \\{0, 1, \\dots, X_0\\}$ 是剩余库存。当 $T=10$ 和 $X_0=10$ 时，状态数量为 $(T+1) \\times (X_0+1) = 11 \\times 11 = 121$。\n- 状态 $s_t=(t,x_t)$ 处的动作空间 $\\mathcal{A}(s_t)$ 由允许的交易规模集合 $a_t \\in \\{0, 1, \\dots, \\min(s_{\\max}, x_t)\\}$ 组成，其中 $s_{\\max}=3$。\n- 转移函数是确定性的。给定状态 $s_t=(t,x_t)$ 和动作 $a_t$，下一个状态是 $s_{t+1}=(t+1, x_t - a_t)$。\n- 即时奖励函数 $r(s_t, a_t)$ 由 $r_t = -\\big(k\\,a_t^2 + \\lambda\\,x_t^2\\big)$ 给出。该函数捕捉了最优执行中的基本权衡：卖出大量 $a_t$ 会产生高额的二次交易成本 ($k\\,a_t^2$)，而持有大量库存 $x_t$ 会产生高额的二次持有风险成本 ($\\lambda\\,x_t^2$)。智能体的目标是最小化总累积成本，这等同于最大化总累积负奖励。\n- 折扣因子 $\\gamma \\in [0,1]$ 决定了未来奖励的现值。\n\n目标是找到一个最优策略 $\\pi^*: \\mathcal{S} \\to \\mathcal{A}$，使得从任何起始状态出发的期望折扣奖励总和（即价值函数）最大化。我们使用Q学习，一种无模型的强化学习算法，来寻找最优动作价值函数 $Q^*(s,a)$，它表示从状态 $s$ 开始，采取动作 $a$，然后遵循最优策略所能获得的最大期望回报。最优动作价值函数满足 Bellman 最优性方程：\n$$\nQ^*(s,a) = \\mathbb{E}\\left[r + \\gamma \\max_{a' \\in \\mathcal{A}(s')} Q^*(s', a') \\mid s, a\\right]\n$$\n其中 $s'$ 是在状态 $s$ 采取动作 $a$ 后的下一个状态。鉴于转移是确定性的，该方程简化为：\n$$\nQ^*(s_t, a_t) = r_t + \\gamma \\max_{a' \\in \\mathcal{A}(s_{t+1})} Q^*(s_{t+1}, a')\n$$\nQ学习通过使用与环境交互产生的样本来更新估计值 $Q(s,a)$，从而迭代地逼近 $Q^*$。其更新规则是：\n$$\nQ(s_t, a_t) \\leftarrow (1-\\alpha)Q(s_t, a_t) + \\alpha \\left[r_t + \\gamma \\max_{a' \\in \\mathcal{A}(s_{t+1})} Q(s_{t+1}, a')\\right]\n$$\n此处，$\\alpha=0.10$ 是学习率。训练期间使用 $\\varepsilon=0.20$ 的 $\\varepsilon$-贪心策略来确保对状态-动作空间的探索。为了保证可复现性，随机种子固定为 $0$。\n\n折扣因子 $\\gamma$ 的作用是这个问题的核心。它决定了智能体的时间偏好。\n- 当 $\\gamma \\approx 0$ 时，智能体是短视的，会大幅折扣未来的奖励（成本）。它主要寻求最大化即时奖励 $r_t = -k\\,a_t^2 - \\lambda\\,x_t^2$。由于在状态 $s_t$ 时 $x_t$ 是固定的，这简化为最大化 $-k\\,a_t^2$，即通过最小化 $a_t$ 来实现。智能体学会了尽可能少地交易，导致清算时间非常长或清算不完全。清算时间 $L(\\gamma)$ 应该很大，很可能是 $T=10$。\n- 当 $\\gamma \\approx 1$ 时，智能体是有远见的。项 $\\gamma \\max_{a'} Q(s_{t+1}, a')$ 变得非常重要。该项携带了关于所有未来成本的信息。智能体认识到持有库存（$x > 0$）将产生一系列的未来持有成本。为了最小化总成本，它被激励去更快地清算库存，即使这意味着要承担更高的即时交易成本。这将导致更短的清算时间。\n\n因此，我们预期清算时间 $L(\\gamma)$ 是折扣因子 $\\gamma$ 的一个非增函数。\n\n解决方案的实现步骤如下：\n1. 对于每个给定的 $\\gamma$ 值，将一个维度为 $(T+1, X_0+1, s_{\\max}+1)$ 的Q表初始化为零。\n2. 对智能体进行 $N_{\\text{episodes}} = 20000$ 个回合的训练。在每个回合中，从 $s_0=(0, 10)$ 开始，智能体与环境交互，直到达到终止状态（$t=T$ 或 $x=0$）。\n3. 在每一步中，通过 $\\varepsilon$-贪心策略选择一个动作。平局打破规则，即在具有相同Q值的动作中倾向于选择最大的交易规模，被严格执行。\n4. Q表根据指定的递归式进行更新。\n5. 训练结束后，提取贪心策略 $\\pi_G(s) = \\arg\\max_a Q(s,a)$。同样的平局打破规则适用。\n6. 从 $s_0=(0, 10)$开始，遵循 $\\pi_G$ 运行一次确定性的模拟。将库存 $x_t$ 降至 $0$ 所需的时间步数记录为清算时间 $L(\\gamma)$。如果在 $t=T$ 时库存不为零，则将 $L(\\gamma)$ 设为 $T$。\n此过程对所有指定的 $\\gamma$ 值重复进行，并报告最终的清算时间。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the optimal execution problem using Q-learning for a suite of discount factors.\n    \"\"\"\n    \n    # Problem parameters\n    T = 10\n    X0 = 10\n    S_MAX = 3\n    K = 0.05\n    LAMBDA = 0.10\n\n    # Q-learning parameters\n    ALPHA = 0.10\n    EPSILON = 0.20\n    N_EPISODES = 20000\n    SEED = 0\n    \n    # Test suite for the discount factor\n    gammas = [0.0, 0.3, 0.6, 0.9, 0.99, 1.0]\n    \n    liquidation_times = []\n\n    for gamma in gammas:\n        # Initialize Q-table: Q(t, x, a)\n        # Dimensions: (time_steps, inventory_levels, action_choices)\n        q_table = np.zeros((T + 1, X0 + 1, S_MAX + 1))\n        \n        # Initialize pseudo-random number generator for reproducibility\n        rng = np.random.default_rng(SEED)\n\n        # Main training loop\n        for _ in range(N_EPISODES):\n            t = 0\n            x = X0\n            \n            # An episode runs until a terminal state is reached\n            while t < T and x > 0:\n                current_state_t = t\n                current_state_x = x\n\n                # Determine the set of feasible actions\n                feasible_actions = list(range(min(S_MAX, current_state_x) + 1))\n\n                # Epsilon-greedy action selection\n                if rng.random() < EPSILON:\n                    # Exploration: choose a random feasible action\n                    action = rng.choice(feasible_actions)\n                else:\n                    # Exploitation: choose the best action based on Q-values\n                    q_values_for_state = q_table[current_state_t, current_state_x, feasible_actions]\n                    max_q = np.max(q_values_for_state)\n                    \n                    # Tie-breaking: choose the largest action among those with max Q-value\n                    best_actions = [a for i, a in enumerate(feasible_actions) if q_values_for_state[i] == max_q]\n                    action = max(best_actions)\n\n                # Execute action: calculate reward and find next state\n                reward = -(K * action**2 + LAMBDA * current_state_x**2)\n                next_t = t + 1\n                next_x = x - action\n\n                # Q-table update\n                is_next_state_terminal = (next_t == T) or (next_x == 0)\n                \n                if is_next_state_terminal:\n                    q_max_next = 0.0\n                else:\n                    next_feasible_actions = list(range(min(S_MAX, next_x) + 1))\n                    q_max_next = np.max(q_table[next_t, next_x, next_feasible_actions])\n                \n                target = reward + gamma * q_max_next\n                \n                old_q_value = q_table[current_state_t, current_state_x, action]\n                q_table[current_state_t, current_state_x, action] = \\\n                    (1 - ALPHA) * old_q_value + ALPHA * target\n\n                # Transition to the next state\n                t = next_t\n                x = next_x\n        \n        # After training, simulate the greedy policy to find liquidation time\n        t_sim = 0\n        x_sim = X0\n        liquidation_time = T # Default if not liquidated by T\n\n        while t_sim < T and x_sim > 0:\n            current_state_t_sim = t_sim\n            current_state_x_sim = x_sim\n\n            # Select greedy action with the apecified tie-breaking rule\n            sim_feasible_actions = list(range(min(S_MAX, current_state_x_sim) + 1))\n            q_values_sim = q_table[current_state_t_sim, current_state_x_sim, sim_feasible_actions]\n            max_q_sim = np.max(q_values_sim)\n            best_actions_sim = [a for i, a in enumerate(sim_feasible_actions) if q_values_sim[i] == max_q_sim]\n            action_sim = max(best_actions_sim)\n\n            x_sim -= action_sim\n            t_sim += 1\n            \n            if x_sim == 0:\n                liquidation_time = t_sim\n                break\n        \n        liquidation_times.append(liquidation_time)\n        \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, liquidation_times))}]\")\n\nsolve()\n```", "id": "2423640"}]}