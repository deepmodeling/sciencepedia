{"hands_on_practices": [{"introduction": "标准的决策树算法通常使用基尼不纯度或信息熵等通用指标来决定如何分裂节点。然而，在经济和金融应用中，不同类型的误分类成本往往是不对称的。此练习将让您亲手实践如何定制决策树的核心分裂逻辑，使其反映真实的经济目标，例如对错误地将高收益客户分类为低价值客户的行为施加更重的惩罚。掌握这项技能对于将机器学习模型应用于解决实际商业问题至关重要([@problem_id:2386905])。", "problem": "给定一个源于计算经济学和金融学的二元分类任务，其中每个观测值代表一个客户，每个标签指示客户是否为高价值客户，并且每个观测值都带有一个收入权重。二叉决策树中一个节点的分裂是通过在单个实值特征上选择一个阈值，并在生成的每个叶节点中分配一个恒定的类别预测，以最小化一个自定义的经济损失。对于每个观测索引 $i$，$i \\in \\{1,\\dots,n\\}$，设标量特征为 $x_i \\in \\mathbb{R}$，二元标签为 $y_i \\in \\{0,1\\}$，非负收入权重为 $r_i \\in \\mathbb{R}_{\\ge 0}$。给定错分成本参数 $\\alpha \\in \\mathbb{R}_{>0}$ 和 $\\beta \\in \\mathbb{R}_{>0}$。对于任何阈值 $\\tau \\in \\mathbb{R}$，定义左区域 $L(\\tau) = \\{i : x_i < \\tau\\}$ 和右区域 $R(\\tau) = \\{i : x_i \\ge \\tau\\}$。在一个区域 $S \\in \\{L(\\tau), R(\\tau)\\}$ 中，如果恒定预测为 $c \\in \\{0,1\\}$，则由观测值 $i \\in S$ 贡献的错分损失在 $c = y_i$ 时为 $0$，在 $y_i = 1$ 且 $c = 0$ 时等于 $\\alpha \\cdot r_i$，在 $y_i = 0$ 且 $c = 1$ 时等于 $\\beta$。叶节点的预测 $c_S(\\tau)$ 必须最小化 $S$ 内的总损失。如果在叶节点中预测 $0$ 和 $1$ 的损失相等，则选择 $c_S(\\tau) = 0$。总分裂损失是两个叶节点损失之和，一个来自 $L(\\tau)$，一个来自 $R(\\tau)$。从一个有限的候选阈值集合 $\\mathcal{T} \\subset \\mathbb{R}$ 中，选择 $\\tau^\\star \\in \\mathcal{T}$ 以最小化总分裂损失。如果多个阈值达到相同的最小总损失，则选择数值上最小的 $\\tau^\\star$。对于下面的每个测试用例，您必须确定 $\\tau^\\star$、对应的最小化总损失，以及两个叶节点的预测 $c_{L}(\\tau^\\star)$ 和 $c_{R}(\\tau^\\star)$。\n\n测试套件。对于每个用例，给定数组 $x$、$y$、$r$，参数 $\\alpha$、$\\beta$ 和候选阈值 $\\mathcal{T}$。请严格应用上述定义。\n\n- 用例 A（一般情况）：\n  - $x = [\\,1.0,\\,2.0,\\,2.5,\\,3.5,\\,4.0,\\,5.0\\,]$\n  - $y = [\\,0,\\,1,\\,0,\\,1,\\,1,\\,0\\,]$\n  - $r = [\\,10.0,\\,200.0,\\,5.0,\\,150.0,\\,300.0,\\,2.0\\,]$\n  - $\\alpha = 5.0$, $\\beta = 1.0$\n  - $\\mathcal{T} = \\{\\,1.5,\\,2.25,\\,3.0,\\,3.75,\\,4.5\\,\\}$\n- 用例 B（边界与平局情况；包括一个空叶节点和一个叶节点平局）：\n  - $x = [\\,0.0,\\,0.1,\\,0.2,\\,0.3\\,]$\n  - $y = [\\,1,\\,1,\\,0,\\,0\\,]$\n  - $r = [\\,1.0,\\,1.0,\\,1.0,\\,1.0\\,]$\n  - $\\alpha = 1.0$, $\\beta = 1.0$\n  - $\\mathcal{T} = \\{\\,-1.0,\\,0.15,\\,1.0\\,\\}$\n- 用例 C（包含一个极高收入正例的边缘情况）：\n  - $x = [\\,10.0,\\,20.0,\\,30.0,\\,40.0,\\,50.0\\,]$\n  - $y = [\\,0,\\,0,\\,1,\\,0,\\,0\\,]$\n  - $r = [\\,1.0,\\,1.0,\\,1000.0,\\,1.0,\\,1.0\\,]$\n  - $\\alpha = 10.0$, $\\beta = 1.0$\n  - $\\mathcal{T} = \\{\\,15.0,\\,25.0,\\,35.0,\\,45.0\\,\\}$\n\n最终输出格式。您的程序必须生成一行包含结果列表的内容，每个测试用例一个结果，顺序和结构如下：对于每个测试用例，输出列表 $[\\,\\tau^\\star,\\,\\text{total\\_loss}(\\tau^\\star),\\,c_{L}(\\tau^\\star),\\,c_{R}(\\tau^\\star)\\,]$。将三个用例的列表聚合成一个单一列表，并在一行上打印该列表。每个用例条目中的 $\\tau^\\star$ 和 $\\text{total\\_loss}(\\tau^\\star)$ 必须是实数；$c_{L}(\\tau^\\star)$ 和 $c_{R}(\\tau^\\star)$ 的条目必须是 $\\{0,1\\}$ 中的整数。", "solution": "所呈现的问题是在决策树构建背景下的一个良构优化问题，特别为计算经济学应用量身定制。问题陈述具有科学依据，内容自洽，且逻辑一致。不存在任何模糊或矛盾之处。任务是从一个给定的有限集合 $\\mathcal{T}$ 中找到一个最优的分裂阈值 $\\tau^\\star$，该阈值可以最小化一个自定义的、非对称的、加权的经济损失函数。\n\n流程如下：\n对于候选集合 $\\mathcal{T}$ 中的每个候选阈值 $\\tau$，我们必须计算总分裂损失。这包括划分数据，确定每个生成分区（叶节点）的最优预测，计算每个叶节点的损失，并将这些损失相加。\n\n首先，让我们形式化单个区域 $S$ 的计算，该区域可以是左区域 $L(\\tau) = \\{i : x_i < \\tau\\}$ 或右区域 $R(\\tau) = \\{i : x_i \\ge \\tau\\}$。一个恒定的预测 $c_S \\in \\{0, 1\\}$ 将被分配给该区域中的所有观测值。区域 $S$ 的总损失取决于此预测。\n\n设 $S_0 = \\{i \\in S : y_i = 0\\}$ 为 $S$ 中负类观测值的索引集合，$S_1 = \\{i \\in S : y_i = 1\\}$ 为正类观测值的索引集合。\n\n如果我们预测 $c_S = 0$，我们将错分所有正类观测值。每个此类观测值 $i \\in S_1$ 的损失为 $\\alpha \\cdot r_i$。预测为 $0$ 的总损失是：\n$$L_0(S) = \\sum_{i \\in S_1} \\alpha \\cdot r_i$$\n\n如果我们预测 $c_S = 1$，我们将错分所有负类观测值。每个此类观测值 $i \\in S_0$ 的损失为 $\\beta$。预测为 $1$ 的总损失是：\n$$L_1(S) = \\sum_{i \\in S_0} \\beta = \\beta \\cdot |S_0|$$\n其中 $|S_0|$ 是 $S$ 中负类观测值的数量。\n\n该区域的最优预测 $c_S(\\tau)$ 是使此损失最小化的预测。根据问题的平局打破规则，如果 $L_0(S) = L_1(S)$，我们必须选择预测 $c_S(\\tau) = 0$。因此，区域 $S$ 中预测的决策规则是：\n$$\nc_S(\\tau) =\n\\begin{cases}\n1 & \\text{如果 } L_1(S) < L_0(S) \\\\\n0 & \\text{如果 } L_0(S) \\le L_1(S)\n\\end{cases}\n$$\n如果决策规则被正确应用，区域 $S$ 对应的最小损失为 $\\text{Loss}(S) = \\min(L_0(S), L_1(S))$。具体来说，如果 $c_S(\\tau) = 1$，$\\text{Loss}(S)$ 为 $L_1(S)$；如果 $c_S(\\tau) = 0$，则为 $L_0(S)$。\n对于空区域 $S = \\emptyset$，$S_0$ 和 $S_1$ 都是空的。因此，$L_0(\\emptyset) = 0$ 且 $L_1(\\emptyset) = 0$。平局打破规则规定 $c_\\emptyset(\\tau)=0$，损失为 $0$。\n\n给定阈值 $\\tau$ 的总分裂损失是左区域和右区域的最小损失之和：\n$$\\text{TotalLoss}(\\tau) = \\text{Loss}(L(\\tau)) + \\text{Loss}(R(\\tau))$$\n\n总体任务是从候选集 $\\mathcal{T}$ 中找到最优阈值 $\\tau^\\star$，以最小化此总损失：\n$$\\tau^\\star = \\arg\\min_{\\tau \\in \\mathcal{T}} \\text{TotalLoss}(\\tau)$$\n如果多个阈值产生相同的最小损失，问题规定选择数值上最小的一个。\n\n我们现在将此流程应用于每个测试用例。\n\n**用例 A**\n- 数据：$x = [1.0, 2.0, 2.5, 3.5, 4.0, 5.0]$, $y = [0, 1, 0, 1, 1, 0]$, $r = [10.0, 200.0, 5.0, 150.0, 300.0, 2.0]$\n- 参数：$\\alpha = 5.0$, $\\beta = 1.0$\n- 候选阈值：$\\mathcal{T} = \\{1.5, 2.25, 3.0, 3.75, 4.5\\}$\n\n1.  **$\\tau = 1.5$**:\n    - $L(1.5)$: 索引 {$1$} ($x_1=1.0$)。$S_0=\\{1\\}$, $S_1=\\emptyset$。\n      $L_0(L) = 0$, $L_1(L) = \\beta \\cdot 1 = 1.0$。$L_0 \\le L_1 \\implies c_L=0$, $\\text{Loss}_L = 0.0$。\n    - $R(1.5)$: 索引 {$2,3,4,5,6$}。$S_0=\\{3,6\\}$, $S_1=\\{2,4,5\\}$。\n      $L_0(R) = 5.0 \\cdot (200.0 + 150.0 + 300.0) = 3250.0$。$L_1(R) = 1.0 \\cdot 2 = 2.0$。\n      $L_1 < L_0 \\implies c_R=1$, $\\text{Loss}_R = 2.0$。\n    - 总损失 = $0.0 + 2.0 = 2.0$。\n\n2.  **$\\tau = 2.25$**:\n    - $L(2.25)$: 索引 {$1,2$}。$S_0=\\{1\\}$, $S_1=\\{2\\}$。\n      $L_0(L) = 5.0 \\cdot 200.0 = 1000.0$。$L_1(L) = 1.0 \\cdot 1 = 1.0$。$c_L=1$, $\\text{Loss}_L = 1.0$。\n    - $R(2.25)$: 索引 {$3,4,5,6$}。$S_0=\\{3,6\\}$, $S_1=\\{4,5\\}$。\n      $L_0(R) = 5.0 \\cdot (150.0 + 300.0) = 2250.0$。$L_1(R) = 1.0 \\cdot 2 = 2.0$。$c_R=1$, $\\text{Loss}_R = 2.0$。\n    - 总损失 = $1.0 + 2.0 = 3.0$。\n\n3.  **$\\tau = 3.0$**:\n    - $L(3.0)$: 索引 {$1,2,3$}。$S_0=\\{1,3\\}$, $S_1=\\{2\\}$。\n      $L_0(L) = 5.0 \\cdot 200.0 = 1000.0$。$L_1(L) = 1.0 \\cdot 2 = 2.0$。$c_L=1$, $\\text{Loss}_L = 2.0$。\n    - $R(3.0)$: 索引 {$4,5,6$}。$S_0=\\{6\\}$, $S_1=\\{4,5\\}$。\n      $L_0(R) = 5.0 \\cdot (150.0 + 300.0) = 2250.0$。$L_1(R) = 1.0 \\cdot 1 = 1.0$。$c_R=1$, $\\text{Loss}_R=1.0$。\n    - 总损失 = $2.0 + 1.0 = 3.0$。\n\n4.  **$\\tau = 3.75$**:\n    - $L(3.75)$: 索引 {$1,2,3,4$}。$S_0=\\{1,3\\}$, $S_1=\\{2,4\\}$。\n      $L_0(L) = 5.0 \\cdot (200.0 + 150.0) = 1750.0$。$L_1(L) = 1.0 \\cdot 2 = 2.0$。$c_L=1$, $\\text{Loss}_L=2.0$。\n    - $R(3.75)$: 索引 {$5,6$}。$S_0=\\{6\\}$, $S_1=\\{5\\}$。\n      $L_0(R) = 5.0 \\cdot 300.0 = 1500.0$。$L_1(R) = 1.0 \\cdot 1 = 1.0$。$c_R=1$, $\\text{Loss}_R=1.0$。\n    - 总损失 = $2.0 + 1.0 = 3.0$。\n\n5.  **$\\tau = 4.5$**:\n    - $L(4.5)$: 索引 {$1,2,3,4,5$}。$S_0=\\{1,3\\}$, $S_1=\\{2,4,5\\}$。\n      $L_0(L) = 5.0 \\cdot (200.0+150.0+300.0) = 3250.0$。$L_1(L) = 1.0 \\cdot 2 = 2.0$。$c_L=1$, $\\text{Loss}_L=2.0$。\n    - $R(4.5)$: 索引 {$6$}。$S_0=\\{6\\}$, $S_1=\\emptyset$。\n      $L_0(R) = 0$。$L_1(R) = 1.0 \\cdot 1 = 1.0$。$c_R=0$, $\\text{Loss}_R=0.0$。\n    - 总损失 = $2.0 + 0.0 = 2.0$。\n\n最小损失为 $2.0$，在 $\\tau=1.5$ 和 $\\tau=4.5$ 时出现。根据平局打破规则，我们选择最小的阈值。\n结果：$\\tau^\\star = 1.5$，总损失 = $2.0$，$c_L(\\tau^\\star)=0$，$c_R(\\tau^\\star)=1$。\n\n**用例 B**\n- 数据：$x = [0.0, 0.1, 0.2, 0.3]$, $y = [1, 1, 0, 0]$, $r = [1.0, 1.0, 1.0, 1.0]$\n- 参数：$\\alpha = 1.0$, $\\beta = 1.0$\n- 候选阈值：$\\mathcal{T} = \\{-1.0, 0.15, 1.0\\}$\n\n1.  **$\\tau = -1.0$**:\n    - $L(-1.0)$: $\\emptyset$。$\\text{Loss}_L = 0.0$，$c_L=0$（根据平局打破规则）。\n    - $R(-1.0)$: 索引 {$1,2,3,4$}。$S_0=\\{3,4\\}$, $S_1=\\{1,2\\}$。\n      $L_0(R) = 1.0 \\cdot (1.0+1.0) = 2.0$。$L_1(R) = 1.0 \\cdot 2 = 2.0$。\n      $L_0 \\le L_1 \\implies c_R=0$, $\\text{Loss}_R = 2.0$。\n    - 总损失 = $0.0 + 2.0 = 2.0$。\n\n2.  **$\\tau = 0.15$**:\n    - $L(0.15)$: 索引 {$1,2$}。$S_0=\\emptyset$, $S_1=\\{1,2\\}$。\n      $L_0(L) = 1.0 \\cdot (1.0+1.0) = 2.0$。$L_1(L) = 1.0 \\cdot 0 = 0$。$c_L=1$, $\\text{Loss}_L=0.0$。\n    - $R(0.15)$: 索引 {$3,4$}。$S_0=\\{3,4\\}$, $S_1=\\emptyset$。\n      $L_0(R) = 0$。$L_1(R) = 1.0 \\cdot 2 = 2.0$。$c_R=0$, $\\text{Loss}_R=0.0$。\n    - 总损失 = $0.0 + 0.0 = 0.0$。\n\n3.  **$\\tau = 1.0$**:\n    - $L(1.0)$: 索引 {$1,2,3,4$}。与 $R(-1.0)$ 相同。$c_L=0$, $\\text{Loss}_L=2.0$。\n    - $R(1.0)$: $\\emptyset$。$\\text{Loss}_R = 0.0$，$c_R=0$。\n    - 总损失 = $2.0 + 0.0 = 2.0$。\n\n最小损失为 $0.0$，仅在 $\\tau=0.15$ 时出现。\n结果：$\\tau^\\star = 0.15$，总损失 = $0.0$，$c_L(\\tau^\\star)=1$，$c_R(\\tau^\\star)=0$。\n\n**用例 C**\n- 数据：$x = [10.0, 20.0, 30.0, 40.0, 50.0]$, $y = [0, 0, 1, 0, 0]$, $r = [1.0, 1.0, 1000.0, 1.0, 1.0]$\n- 参数：$\\alpha = 10.0$, $\\beta = 1.0$\n- 候选阈值：$\\mathcal{T} = \\{15.0, 25.0, 35.0, 45.0\\}$\n\n1.  **$\\tau = 15.0$**:\n    - $L(15.0)$: 索引 {$1$}。$S_0=\\{1\\}$, $S_1=\\emptyset$。$c_L=0$, $\\text{Loss}_L=0.0$。\n    - $R(15.0)$: 索引 {$2,3,4,5$}。$S_0=\\{2,4,5\\}$, $S_1=\\{3\\}$。\n      $L_0(R) = 10.0 \\cdot 1000.0 = 10000.0$。$L_1(R) = 1.0 \\cdot 3 = 3.0$。$c_R=1$, $\\text{Loss}_R=3.0$。\n    - 总损失 = $0.0 + 3.0 = 3.0$。\n\n2.  **$\\tau = 25.0$**:\n    - $L(25.0)$: 索引 {$1,2$}。$S_0=\\{1,2\\}$, $S_1=\\emptyset$。$c_L=0$, $\\text{Loss}_L=0.0$。\n    - $R(25.0)$: 索引 {$3,4,5$}。$S_0=\\{4,5\\}$, $S_1=\\{3\\}$。\n      $L_0(R) = 10.0 \\cdot 1000.0 = 10000.0$。$L_1(R) = 1.0 \\cdot 2 = 2.0$。$c_R=1$, $\\text{Loss}_R=2.0$。\n    - 总损失 = $0.0 + 2.0 = 2.0$。\n\n3.  **$\\tau = 35.0$**:\n    - $L(35.0)$: 索引 {$1,2,3$}。$S_0=\\{1,2\\}$, $S_1=\\{3\\}$。\n      $L_0(L) = 10.0 \\cdot 1000.0 = 10000.0$。$L_1(L) = 1.0 \\cdot 2 = 2.0$。$c_L=1$, $\\text{Loss}_L=2.0$。\n    - $R(35.0)$: 索引 {$4,5$}。$S_0=\\{4,5\\}$, $S_1=\\emptyset$。$c_R=0$, $\\text{Loss}_R=0.0$。\n    - 总损失 = $2.0 + 0.0 = 2.0$。\n\n4.  **$\\tau = 45.0$**:\n    - $L(45.0)$: 索引 {$1,2,3,4$}。$S_0=\\{1,2,4\\}$, $S_1=\\{3\\}$。\n      $L_0(L) = 10.0 \\cdot 1000.0 = 10000.0$。$L_1(L) = 1.0 \\cdot 3 = 3.0$。$c_L=1$, $\\text{Loss}_L=3.0$。\n    - $R(45.0)$: 索引 {$5$}。$S_0=\\{5\\}$, $S_1=\\emptyset$。$c_R=0$, $\\text{Loss}_R=0.0$。\n    - 总损失 = $3.0 + 0.0 = 3.0$。\n\n最小损失为 $2.0$，在 $\\tau=25.0$ 和 $\\tau=35.0$ 时出现。选择最小的阈值给出 $\\tau^\\star=25.0$。\n结果：$\\tau^\\star = 25.0$，总损失 = $2.0$，$c_L(\\tau^\\star)=0$，$c_R(\\tau^\\star)=1$。", "answer": "```python\nimport numpy as np\n\ndef calculate_leaf_stats(y_leaf, r_leaf, alpha, beta):\n    \"\"\"\n    Calculates the minimum loss and optimal prediction for a single leaf (region).\n\n    Args:\n        y_leaf (np.ndarray): Binary labels {0, 1} for the observations in the leaf.\n        r_leaf (np.ndarray): Revenue weights for the observations in the leaf.\n        alpha (float): Cost parameter for misclassifying a y=1 as 0.\n        beta (float): Cost parameter for misclassifying a y=0 as 1.\n\n    Returns:\n        tuple[float, int]: A tuple containing (minimum_loss, optimal_prediction).\n    \"\"\"\n    # For an empty leaf, loss is 0. Prediction is 0 by tie-breaking rule.\n    if len(y_leaf) == 0:\n        return 0.0, 0\n\n    # Calculate loss if we predict 0 for the entire leaf.\n    # This misclassifies all observations where y=1.\n    mask_y1 = (y_leaf == 1)\n    cost_if_0 = alpha * np.sum(r_leaf[mask_y1])\n\n    # Calculate loss if we predict 1 for the entire leaf.\n    # This misclassifies all observations where y=0.\n    mask_y0 = (y_leaf == 0)\n    cost_if_1 = beta * np.sum(mask_y0)\n\n    # Determine optimal prediction and corresponding loss.\n    # Tie-breaker: if costs are equal, predict 0.\n    if cost_if_1 < cost_if_0:\n        return float(cost_if_1), 1\n    else:\n        return float(cost_if_0), 0\n\ndef solve():\n    \"\"\"\n    Solves the decision tree split optimization problem for a suite of test cases.\n    \"\"\"\n    test_cases = [\n        {\n            \"x\": np.array([1.0, 2.0, 2.5, 3.5, 4.0, 5.0]),\n            \"y\": np.array([0, 1, 0, 1, 1, 0]),\n            \"r\": np.array([10.0, 200.0, 5.0, 150.0, 300.0, 2.0]),\n            \"alpha\": 5.0, \"beta\": 1.0,\n            \"T_set\": [1.5, 2.25, 3.0, 3.75, 4.5]\n        },\n        {\n            \"x\": np.array([0.0, 0.1, 0.2, 0.3]),\n            \"y\": np.array([1, 1, 0, 0]),\n            \"r\": np.array([1.0, 1.0, 1.0, 1.0]),\n            \"alpha\": 1.0, \"beta\": 1.0,\n            \"T_set\": [-1.0, 0.15, 1.0]\n        },\n        {\n            \"x\": np.array([10.0, 20.0, 30.0, 40.0, 50.0]),\n            \"y\": np.array([0, 0, 1, 0, 0]),\n            \"r\": np.array([1.0, 1.0, 1000.0, 1.0, 1.0]),\n            \"alpha\": 10.0, \"beta\": 1.0,\n            \"T_set\": [15.0, 25.0, 35.0, 45.0]\n        }\n    ]\n\n    all_results = []\n    for case in test_cases:\n        x, y, r, alpha, beta, T_set = case[\"x\"], case[\"y\"], case[\"r\"], case[\"alpha\"], case[\"beta\"], case[\"T_set\"]\n        \n        best_tau = None\n        best_loss = float('inf')\n        best_c_L = -1\n        best_c_R = -1\n\n        for tau in T_set:\n            # Partition the data into left and right regions based on the threshold.\n            left_mask = x < tau\n            right_mask = x >= tau\n\n            # Calculate loss and prediction for the left leaf.\n            loss_L, c_L = calculate_leaf_stats(y[left_mask], r[left_mask], alpha, beta)\n            \n            # Calculate loss and prediction for the right leaf.\n            loss_R, c_R = calculate_leaf_stats(y[right_mask], r[right_mask], alpha, beta)\n\n            total_loss = loss_L + loss_R\n\n            # Check if this threshold provides a better or equal-but-smaller result.\n            if total_loss < best_loss:\n                best_loss = total_loss\n                best_tau = tau\n                best_c_L = c_L\n                best_c_R = c_R\n            elif total_loss == best_loss:\n                # Tie-breaker: choose the numerically smallest threshold.\n                if best_tau is None or tau < best_tau:\n                    best_tau = tau\n                    best_c_L = c_L\n                    best_c_R = c_R\n        \n        # Store the result for the current case.\n        case_result = [float(best_tau), float(best_loss), int(best_c_L), int(best_c_R)]\n        all_results.append(case_result)\n\n    # Print the aggregated results in the specified list-of-lists format.\n    print(all_results)\n\nsolve()\n```", "id": "2386905"}, {"introduction": "此练习将决策树分裂这一抽象概念与识别金融时间序列中的结构性突变这一重要的具体问题联系起来。您会发现，一个简单的单层决策树本身就是一个强大的优化工具，它通过最小化平方误差和来找到数据中最显著的变化点。这项实践旨在揭示机器学习与经典统计学之间的深刻联系，展示了如何用树模型来解决变化点检测问题([@problem_id:2386904])。", "problem": "给定一个单变量金融时间序列的十进制对数回报率 $\\{y_t\\}_{t=0}^{T-1}$，其中 $T \\in \\mathbb{N}$ 且每个 $y_t \\in \\mathbb{R}$。考虑一个分段常数双机制模型，该模型在整数分割位置 $\\tau$ 处存在单一结构性断点，其中左侧分段的索引为 $\\{0,1,\\dots,\\tau-1\\}$，右侧分段的索引为 $\\{\\tau,\\dots,T-1\\}$。对于任何可接受的分割 $\\tau$，将左侧分段均值 $\\mu_L(\\tau)$ 和右侧分段均值 $\\mu_R(\\tau)$ 定义为\n$$\n\\mu_L(\\tau) \\equiv \\frac{1}{\\tau}\\sum_{t=0}^{\\tau-1} y_t,\\qquad\n\\mu_R(\\tau) \\equiv \\frac{1}{T-\\tau}\\sum_{t=\\tau}^{T-1} y_t,\n$$\n以及总平方误差\n$$\n\\mathrm{SSE}(\\tau) \\equiv \\sum_{t=0}^{\\tau-1}\\bigl(y_t - \\mu_L(\\tau)\\bigr)^2 \\;+\\; \\sum_{t=\\tau}^{T-1}\\bigl(y_t - \\mu_R(\\tau)\\bigr)^2.\n$$\n给定一个最小叶子大小 $m \\in \\mathbb{N}$，如果分割 $\\tau$ 满足 $m \\le \\tau \\le T - m$，则该分割是可接受的。您的任务是，对于每个测试用例，计算一个整数 $\\tau^\\star$，该值能在所有可接受的分割上最小化 $\\mathrm{SSE}(\\tau)$。如果存在多个最小值点（包括数值上的并列），请选择其中最小的 $\\tau^\\star$。如果不存在可接受的分割（即，如果 $T < 2m$），则该测试用例的输出定义为 $-1$。\n\n所有 $y_t$ 值均为十进制对数回报率，必须作为无量纲实数处理（无单位）。不要将任何结果表示为百分比；所有数值量必须是小数或整数。\n\n测试套件。对于每个测试用例，给定配对 $(\\{y_t\\}_{t=0}^{T-1}, m)$：\n\n- 测试 1 (均值明显漂移)：$T = 10$, $y = [\\,0.000, 0.000, 0.000, 0.000, 0.000, 0.020, 0.020, 0.020, 0.020, 0.020\\,]$, $m = 1$.\n- 测试 2 (常数序列，按最小分割打破并列)：$T = 6$, $y = [\\,0.010, 0.010, 0.010, 0.010, 0.010, 0.010\\,]$, $m = 2$.\n- 测试 3 (接近末尾的单一极端观测值，与最小叶子大小相关)：$T = 8$, $y = [\\,0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.100\\,]$, $m = 2$.\n- 测试 4 (边界情况，恰好只有一个可接受的分割)：$T = 4$, $y = [\\,0.010, 0.020, 0.030, 0.040\\,]$, $m = 2$.\n- 测试 5 (漂移后出现更高的常数均值)：$T = 10$, $y = [\\,0.000, 0.010, 0.020, 0.030, 0.040, 0.060, 0.060, 0.060, 0.060, 0.060\\,]$, $m = 1$.\n\n要求的最终输出格式。您的程序应生成单行输出，其中包含按上述测试顺序排列的结果，形式为方括号括起来的逗号分隔列表。例如，如果五个答案是 $\\tau_1^\\star,\\dots,\\tau_5^\\star$，则打印\n[${\\tau_1^\\star}$,${\\tau_2^\\star}$,${\\tau_3^\\star}$,${\\tau_4^\\star}$,${\\tau_5^\\star}$]。", "solution": "在尝试解决问题之前，需对提出的问题进行验证。\n\n### 步骤 1：提取已知条件\n- **时间序列**：一个单变量序列 $\\{y_t\\}_{t=0}^{T-1}$，其中 $T \\in \\mathbb{N}$ 且 $y_t \\in \\mathbb{R}$。\n- **模型**：一个分段常数双机制模型，在整数分割位置 $\\tau$ 处存在单一结构性断点。\n- **分段**：\n    - 左侧分段索引：$\\{0, 1, \\dots, \\tau-1\\}$。\n    - 右侧分段索引：$\\{\\tau, \\dots, T-1\\}$。\n- **分段均值**：\n    - 左侧分段均值：$\\mu_L(\\tau) \\equiv \\frac{1}{\\tau}\\sum_{t=0}^{\\tau-1} y_t$。\n    - 右侧分段均值：$\\mu_R(\\tau) \\equiv \\frac{1}{T-\\tau}\\sum_{t=\\tau}^{T-1} y_t$。\n- **目标函数**：总平方误差，$\\mathrm{SSE}(\\tau) \\equiv \\sum_{t=0}^{\\tau-1}\\bigl(y_t - \\mu_L(\\tau)\\bigr)^2 \\;+\\; \\sum_{t=\\tau}^{T-1}\\bigl(y_t - \\mu_R(\\tau)\\bigr)^2$。\n- **约束条件**：\n    - 最小叶子大小：$m \\in \\mathbb{N}$。\n    - 可接受的分割：如果一个分割 $\\tau$ 满足 $m \\le \\tau \\le T - m$，则它是可接受的。\n- **任务**：\n    - 计算一个整数 $\\tau^\\star$，该值能在所有可接受的分割上最小化 $\\mathrm{SSE}(\\tau)$。\n    - 并列处理规则：如果存在多个最小值点，选择其中最小的 $\\tau^\\star$。\n    - 无可接受分割：如果 $T < 2m$，输出为 $-1$。\n- **测试套件**：提供了五个测试用例，每个由一对 $(\\{y_t\\}_{t=0}^{T-1}, m)$ 组成。\n\n### 步骤 2：使用提取的已知条件进行验证\n评估问题的有效性。\n- **科学依据**：该问题是变点检测的一个经典例子，这是统计学、信号处理和计量经济学中的一个基本课题。在假设随机变量序列在其他方面是具有恒定方差的独立同分布高斯变量的情况下，最小化平方误差和 (SSE) 等同于对序列均值断点的最大似然估计。这是一个成熟且科学合理的模型。\n- **适定性**：该问题是适定的。$\\tau$ 的优化域是整数的有限集 $\\{m, m+1, \\dots, T-m\\}$。对于此域中的每个 $\\tau$，目标函数 $\\mathrm{SSE}(\\tau)$ 都有唯一定义。由于我们是在一个有限集合上最小化一个实值函数，因此可以保证最小值的存在。并列处理规则确保了选择唯一的解。不存在可接受分割的条件 ($T < 2m$) 也被明确定义。\n- **目标**：问题陈述使用了精确的数学定义，没有歧义。目标明确，解的标准也已明确给出。\n\n### 步骤 3：结论与行动\n该问题是有效的。它是一个明确定义的数学优化问题，基于标准的统计理论。将提供一个解决方案。\n\n### 解法推导\n目标是找到 $\\tau^\\star = \\arg\\min_{m \\le \\tau \\le T-m} \\mathrm{SSE}(\\tau)$。\n\n一种朴素的方法是遍历每个可接受的分割 $\\tau$，并对每个 $\\tau$ 通过对数据点求和来计算分段均值和 SSE。对于给定的 $\\tau$，计算 $\\mu_L(\\tau)$ 和 $\\mu_R(\\tau)$ 分别需要 $O(\\tau)$ 和 $O(T-\\tau)$ 次操作。类似地，计算两个平方和也需要 $O(\\tau)$ 和 $O(T-\\tau)$ 次操作。单个 $\\tau$ 的总复杂度为 $O(T)$。由于 $\\tau$ 有 $O(T)$ 个可能的值，这种朴素方法的总复杂度为 $O(T^2)$。当 $T$ 很大时，这种方法效率低下。\n\n通过认识到 SSE 可以使用一个著名的方差恒等式来计算，可以构建一个更高效的算法：对于一组数 $\\{x_i\\}_{i=1}^n$，与均值的平方偏差和为 $\\sum_{i=1}^n (x_i - \\bar{x})^2 = \\sum_{i=1}^n x_i^2 - n\\bar{x}^2 = \\sum_{i=1}^n x_i^2 - \\frac{1}{n}(\\sum_{i=1}^n x_i)^2$。\n\n将此应用于我们的两个分段：\n左侧分段的 SSE 为：\n$$ \\mathrm{SSE}_L(\\tau) = \\sum_{t=0}^{\\tau-1} (y_t - \\mu_L(\\tau))^2 = \\left(\\sum_{t=0}^{\\tau-1} y_t^2\\right) - \\frac{1}{\\tau}\\left(\\sum_{t=0}^{\\tau-1} y_t\\right)^2 $$\n右侧分段的 SSE 为：\n$$ \\mathrm{SSE}_R(\\tau) = \\sum_{t=\\tau}^{T-1} (y_t - \\mu_R(\\tau))^2 = \\left(\\sum_{t=\\tau}^{T-1} y_t^2\\right) - \\frac{1}{T-\\tau}\\left(\\sum_{t=\\tau}^{T-1} y_t\\right)^2 $$\n总 SSE 为 $\\mathrm{SSE}(\\tau) = \\mathrm{SSE}_L(\\tau) + \\mathrm{SSE}_R(\\tau)$。\n\n为了对所有 $\\tau$ 高效地计算这些和，我们可以预先计算序列 $y_t$ 及其平方 $y_t^2$ 的前缀和（累积和）。我们定义两个大小均为 $T+1$ 的辅助数组 $C_1$ 和 $C_2$：\n$$ C_1[k] = \\sum_{t=0}^{k-1} y_t \\quad \\text{for } k > 0, \\text{ with } C_1[0] = 0 $$\n$$ C_2[k] = \\sum_{t=0}^{k-1} y_t^2 \\quad \\text{for } k > 0, \\text{ with } C_2[0] = 0 $$\n这些前缀和数组可以在 $O(T)$ 时间内计算出来。\n\n使用这些数组，任何分段上的和都可以在 $O(1)$ 时间内计算。对于在 $\\tau$ 处的分割：\n- 左侧分段 $[0, \\dots, \\tau-1]$ 的和：\n  - $\\sum_{t=0}^{\\tau-1} y_t = C_1[\\tau]$\n  - $\\sum_{t=0}^{\\tau-1} y_t^2 = C_2[\\tau]$\n- 右侧分段 $[\\tau, \\dots, T-1]$ 的和：\n  - $\\sum_{t=\\tau}^{T-1} y_t = \\sum_{t=0}^{T-1} y_t - \\sum_{t=0}^{\\tau-1} y_t = C_1[T] - C_1[\\tau]$\n  - $\\sum_{t=\\tau}^{T-1} y_t^2 = \\sum_{t=0}^{T-1} y_t^2 - \\sum_{t=0}^{\\tau-1} y_t^2 = C_2[T] - C_2[\\tau]$\n\n将这些代入 SSE 公式：\n$$ \\mathrm{SSE}_L(\\tau) = C_2[\\tau] - \\frac{(C_1[\\tau])^2}{\\tau} $$\n$$ \\mathrm{SSE}_R(\\tau) = (C_2[T] - C_2[\\tau]) - \\frac{(C_1[T] - C_1[\\tau])^2}{T-\\tau} $$\n因此，在初始的 $O(T)$ 预计算之后，给定 $\\tau$ 的总 SSE 可以在 $O(1)$ 时间内计算出来。\n\n总体算法如下：\n1.  首先，检查是否存在任何可接受的分割。如果 $T < 2m$，则没有分割 $\\tau$ 能同时满足 $\\tau \\ge m$ 和 $T-\\tau \\ge m$。在这种情况下，程序终止并返回 $-1$。\n2.  为给定的时间序列 $\\{y_t\\}$ 和 $\\{y_t^2\\}$ 计算前缀和数组 $C_1$ 和 $C_2$。这需要 $O(T)$ 时间。\n3.  为找到的最小 SSE 初始化一个变量 $\\mathrm{SSE}_{\\min}$ 为一个非常大的值（或无穷大），并将最佳分割 $\\tau^\\star$ 初始化为一个默认值（例如 $-1$ 或第一个可接受的分割）。\n4.  遍历所有从 $m$ 到 $T-m$ 的可接受分割 $\\tau$。\n5.  对于每个 $\\tau$，使用预先计算的前缀和和推导出的公式，在 $O(1)$ 时间内计算 $\\mathrm{SSE}(\\tau)$。\n6.  如果计算出的 $\\mathrm{SSE}(\\tau)$ 严格小于当前的 $\\mathrm{SSE}_{\\min}$，则更新 $\\mathrm{SSE}_{\\min} = \\mathrm{SSE}(\\tau)$ 和 $\\tau^\\star = \\tau$。使用严格不等式 (`<`) 确保在 SSE 相等的情况下，保留遇到的第一个（因此也是最小的）$\\tau$ 值，这满足了问题的并列处理规则。\n7.  循环完成后，$\\tau^\\star$ 将保存最优分割点。返回 $\\tau^\\star$。\n\n该算法的总时间复杂度为 $O(T)$，主要由前缀和计算决定，这比朴素的 $O(T^2)$ 方法有了显著的改进。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves for the optimal structural break point in a time series.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test 1: clear mean shift\n        (np.array([0.000, 0.000, 0.000, 0.000, 0.000, 0.020, 0.020, 0.020, 0.020, 0.020]), 1),\n        # Test 2: constant series, tie-breaking by smallest split\n        (np.array([0.010, 0.010, 0.010, 0.010, 0.010, 0.010]), 2),\n        # Test 3: single extreme observation near the end with minimum leaf size\n        (np.array([0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.100]), 2),\n        # Test 4: boundary case with exactly one admissible split\n        (np.array([0.010, 0.020, 0.030, 0.040]), 2),\n        # Test 5: drift followed by a higher constant mean\n        (np.array([0.000, 0.010, 0.020, 0.030, 0.040, 0.060, 0.060, 0.060, 0.060, 0.060]), 1),\n    ]\n\n    results = []\n    \n    for y, m in test_cases:\n        T = len(y)\n        \n        # Step 1: Check for existence of an admissible split.\n        if T < 2 * m:\n            results.append(-1)\n            continue\n            \n        # Step 2: Compute prefix sums for y and y^2.\n        # We prepend a 0 to make indexing C[k] = sum_{i=0}^{k-1} y_i natural.\n        y_sq = y**2\n        \n        # C1[k] = sum_{t=0}^{k-1} y_t\n        C1 = np.zeros(T + 1)\n        np.cumsum(y, out=C1[1:])\n\n        # C2[k] = sum_{t=0}^{k-1} y_t^2\n        C2 = np.zeros(T + 1)\n        np.cumsum(y_sq, out=C2[1:])\n        \n        min_sse = np.inf\n        best_tau = -1\n        \n        # Step 3: Iterate through all admissible splits.\n        # The range is m <= tau <= T - m.\n        for tau in range(m, T - m + 1):\n            # Calculate sums for the left segment [0, ..., tau-1]\n            # The length of the left segment is tau.\n            sum_L = C1[tau]\n            sum_sq_L = C2[tau]\n            \n            # Calculate sums for the right segment [tau, ..., T-1]\n            # The length of the right segment is T - tau.\n            sum_R = C1[T] - C1[tau]\n            sum_sq_R = C2[T] - C2[tau]\n            \n            # Calculate SSE for left and right segments\n            # SSE = sum(y_i^2) - (sum(y_i))^2 / n\n            sse_L = sum_sq_L - (sum_L**2) / tau\n            sse_R = sum_sq_R - (sum_R**2) / (T - tau)\n            \n            current_sse = sse_L + sse_R\n            \n            # Step 4: Update minimum SSE and best tau.\n            # Strict inequality handles the tie-breaking rule (smallest tau).\n            if current_sse < min_sse:\n                min_sse = current_sse\n                best_tau = tau\n        \n        results.append(best_tau)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2386904"}, {"introduction": "特征重要性是基于树的模型为模型解释性提供的关键洞见之一，但如果不加批判地使用，这个强大的工具也可能产生误导。本练习模拟了一个在实践中常见且危险的数据问题——目标泄漏（target leakage）。您将构建一个随机森林模型，并亲眼见证模型如何被“欺骗”，将极高的重要性分配给一个实际上没有预测能力的变量，这是以数据为中心的建模中的一堂重要课程([@problem_id:2386893])。", "problem": "你正在为一个信贷投资组合的二元贷款违约结果建模。对于每个观测值 $i \\in \\{1,\\dots,n\\}$，设二元目标为 $y_i \\in \\{0,1\\}$，其中 $y_i = 1$ 表示违约。你将生成具有经济可解释性的合成协变量，然后添加一个微妙的、会泄露关于 $y_i$ 信息的事后协变量。随后，你必须量化决策树桩（一种单次分裂的决策森林）集成模型如何根据重要性对协变量进行排序，并为每个测试用例报告最重要协变量的零基索引。索引必须以整数形式报告。\n\n数据生成过程：\n- 设基础协变量的数量为 $p_b = 5$。对于每个观测值 $i$，从一个均值为零的多元正态分布中抽取一个基础特征向量 $\\mathbf{x}_i \\in \\mathbb{R}^{p_b}$，其协方差矩阵为 $\\Sigma(\\rho) \\in \\mathbb{R}^{p_b \\times p_b}$，定义如下\n$$\n\\Sigma(\\rho) = (1-\\rho) I_{p_b} + \\rho \\mathbf{1}\\mathbf{1}^\\top,\n$$\n其中 $I_{p_b}$ 是大小为 $p_b$ 的单位矩阵，$\\mathbf{1}$ 是 $p_b$ 维的全一向量。标量 $\\rho \\in (-\\frac{1}{p_b-1},1)$ 控制基础特征间的共同相关性。\n- 独立于 $\\mathbf{x}_i$ 抽取一个特异性宏观因子 $m_i \\sim \\mathcal{N}(0,1)$。\n- 通过一个逻辑指数定义一个潜在得分 $s_i$：\n$$\ns_i = \\beta_0 + \\sum_{j=1}^{p_b} \\beta_j x_{i,j} + \\gamma m_i,\n$$\n系数固定为 $\\beta_0 = -0.5$，$\\beta_1 = 0.8$，$\\beta_2 = -1.0$，$\\beta_3 = 0.6$，$\\beta_4 = 0.0$，$\\beta_5 = 0.5$ 以及 $\\gamma = 0.7$。\n- 通过逻辑函数定义违约概率 $p_i$：\n$$\np_i = \\frac{1}{1 + e^{-s_i}}.\n$$\n- 对每个 $i$ 独立地抽取二元结果 $y_i \\sim \\text{Bernoulli}(p_i)$。\n- 定义一个会泄露目标信息的事后协变量 $z_i$ 如下\n$$\nz_i = \\lambda \\, y_i + \\delta_i,\n$$\n其中 $\\delta_i \\sim \\mathcal{N}(0,\\sigma^2)$ 独立于所有其他变量。参数 $\\lambda \\in \\mathbb{R}$ 控制信息泄露的强度，$\\sigma \\ge 0$ 控制掩盖泄露的噪声量。\n- 在建模时，你将按如下方式构成特征向量 $\\tilde{\\mathbf{x}}_i$。如果测试用例标志 $\\text{include\\_leak} = 1$，则设置\n$$\n\\tilde{\\mathbf{x}}_i = \\big[x_{i,1}, x_{i,2}, x_{i,3}, x_{i,4}, x_{i,5}, z_i\\big] \\in \\mathbb{R}^{p},\n$$\n其中 $p = p_b + 1$，并且信息泄露协变量位于零基索引 $p_b$ 处。如果 $\\text{include\\_leak} = 0$，则设置\n$$\n\\tilde{\\mathbf{x}}_i = \\big[x_{i,1}, x_{i,2}, x_{i,3}, x_{i,4}, x_{i,5}\\big] \\in \\mathbb{R}^{p},\n$$\n其中 $p = p_b$。\n\n模型与重要性：\n- 考虑一个由 $T = 200$ 个决策树桩（单次分裂决策树）组成的集成模型。对于每棵树 $t \\in \\{1,\\dots,T\\}$：\n  - 通过从 $\\{1,\\dots,n\\}$ 中有放回地抽样，获得一个大小为 $n$ 的自助样本。\n  - 令 $m_{\\text{try}} = \\lceil \\sqrt{p} \\rceil$，并从 $\\{0,\\dots,p-1\\}$ 中均匀随机地选择 $m_{\\text{try}}$ 个不同的特征作为分裂候选。\n  - 对于每个选定的特征 $j$，考虑形式为 $x_{j} \\le \\tau$ 的分裂，其中阈值 $\\tau$ 取自自助样本上该特征排序后观测值的连续值之间的中点，并排除会导致子节点为空的阈值。令 $G(S)$ 表示二元标签集合 $S$ 的基尼不纯度：\n  $$\n  G(S) = 1 - \\sum_{c \\in \\{0,1\\}} \\left(\\frac{n_c}{|S|}\\right)^2,\n  $$\n  其中 $n_c$ 是集合 $S$ 中类别 $c$ 的计数。对于一个标签多重集为 $S$ 的父节点，分裂为左子节点 $S_L$ 和右子节点 $S_R$ 后，不纯度减少量定义为\n  $$\n  \\Delta G = G(S) - \\left(\\frac{|S_L|}{|S|}G(S_L) + \\frac{|S_R|}{|S|}G(S_R)\\right).\n  $$\n  - 在所考虑的特征和阈值中，选择能使 $\\Delta G$ 最大化的特征 $j^\\star$ 和阈值 $\\tau^\\star$。通过在 $(j^\\star,\\tau^\\star)$ 处进行分裂来生长一个决策树桩。\n  - 将所选分裂实现的不纯度减少量 $\\Delta G^\\star$ 归因于特征 $j^\\star$。\n- 定义特征 $j$ 的重要性为在 $T$ 棵树上归因于它的不纯度减少量之和：\n$$\nI_j = \\sum_{t=1}^{T} \\Delta G_t \\cdot \\mathbf{1}\\{j_t^\\star = j\\}.\n$$\n- 对于每个测试用例，计算使 $I_j$ 最大化的索引 $j_{\\max} \\in \\{0,\\dots,p-1\\}$。如果出现平局，则取达到最大值的最小索引。\n\n测试套件：\n为了可复现性，每个测试用例在数据生成和集成模型构建两方面都使用一个独立的随机种子 $s$。使用以下四个测试用例，每个用例由元组 $(n,\\sigma,\\lambda,\\rho,\\text{include\\_leak}, s)$ 指定：\n- 用例 A: $(3000, 0.1, 0.9, 0.2, 1, 11)$。\n- 用例 B: $(3000, 0.3, 0.6, 0.2, 1, 12)$。\n- 用例 C: $(1200, 0.1, 0.9, 0.2, 0, 13)$。\n- 用例 D: $(3000, 0.6, 0.4, 0.2, 1, 14)$。\n\n要求的程序行为和输出：\n- 对于每个测试用例，根据上述过程生成数据，训练前述的集成模型，计算特征重要性 $\\{I_j\\}_{j=0}^{p-1}$，并返回最重要特征的零基索引 $j_{\\max}$。\n- 你的程序必须产生单行输出，其中包含四个索引，以逗号分隔的列表形式，并用方括号括起来，顺序与测试用例相同，例如 [$i_1$,$i_2$,$i_3$,$i_4$]。不应打印任何额外文本。", "solution": "所提出的问题是计算统计学和机器学习领域中一个有效且定义明确的练习，具体涉及在基于树的集成模型中评估特征重要性。数据生成过程被严格定义，并在金融计量经济学的标准模型中有其科学依据。任务是使用决策树桩集成模型中的基尼不纯度减少量来量化特征重要性，并识别出最具影响力的特征，尤其是在存在“泄露”协变量的情况下。该问题是客观、自洽且算法上明确的，因此允许一个唯一、可复现的解。\n\n对于每个测试用例，求解方法论分为两个主要阶段：数据生成和模型训练与重要性计算。所有的数学实体，包括变量、参数和数值，都按要求使用 LaTeX 表示。\n\n**1. 数据生成过程**\n\n对于每个指定的测试用例，根据以下随机过程生成一个大小为 $n$ 的合成数据集。使用随机种子 $s$ 来确保可复现性。\n\n- **基础协变量**：对于每个观测值 $i \\in \\{1, \\dots, n\\}$，从一个多元正态分布 $\\mathcal{N}(\\mathbf{0}, \\Sigma(\\rho))$ 中抽取一组 $p_b = 5$ 个基础协变量，记为向量 $\\mathbf{x}_i \\in \\mathbb{R}^{p_b}$。协方差矩阵 $\\Sigma(\\rho)$ 定义为\n$$\n\\Sigma(\\rho) = (1-\\rho) I_{p_b} + \\rho \\mathbf{1}\\mathbf{1}^\\top\n$$\n其中 $I_{p_b}$ 是 $p_b \\times p_b$ 的单位矩阵，$\\mathbf{1}$ 是一个 $p_b$ 维的全一向量。参数 $\\rho$ 控制这些基础特征之间的等相关性。\n\n- **潜在得分与违约概率**：独立地抽取一个特异性因子 $m_i \\sim \\mathcal{N}(0,1)$。潜在得分 $s_i$ 被构造为基础协变量和宏观因子的线性组合：\n$$\ns_i = \\beta_0 + \\sum_{j=1}^{p_b} \\beta_j x_{i,j} + \\gamma m_i\n$$\n系数固定为 $\\beta_0 = -0.5$，$\\boldsymbol{\\beta}_{1..p_b} = [0.8, -1.0, 0.6, 0.0, 0.5]$，以及 $\\gamma = 0.7$。请注意，$x_{i,4}$ 的系数为 $\\beta_4 = 0.0$，这使得该特征在构造上相对于潜在得分是无信息的。然后，使用标准逻辑函数将潜在得分转换为违约概率 $p_i$：\n$$\np_i = \\frac{1}{1 + e^{-s_i}}\n$$\n\n- **二元结果**：二元目标变量 $y_i \\in \\{0, 1\\}$ 表示未违约 ($0$) 或违约 ($1$)，它是根据生成的概率从伯努利分布中抽取的，$y_i \\sim \\text{Bernoulli}(p_i)$。\n\n- **泄露协变量**：生成一个事后协变量 $z_i$ 来模拟来自目标变量的信息泄露。其定义是：\n$$\nz_i = \\lambda \\, y_i + \\delta_i\n$$\n其中 $\\delta_i$ 是从正态分布 $\\mathcal{N}(0, \\sigma^2)$ 中抽取的噪声项。参数 $\\lambda$ 控制泄露的强度，$\\sigma$ 控制噪声水平。$|\\lambda|$ 与 $\\sigma$ 的高比率意味着 $z_i$ 和 $y_i$ 之间存在强大且易于检测的联系。\n\n- **最终特征矩阵**：组装完整的特征矩阵 $\\tilde{\\mathbf{X}}$。如果 `include_leak` 标志为 $1$，则特征集为 $\\tilde{\\mathbf{x}}_i = [x_{i,1}, \\dots, x_{i,5}, z_i] \\in \\mathbb{R}^{6}$。泄露特征 $z_i$ 位于最后一个位置（零基索引为 5）。如果标志为 $0$，则仅使用基础协变量，$\\tilde{\\mathbf{x}}_i = [x_{i,1}, \\dots, x_{i,5}] \\in \\mathbb{R}^{5}$。特征的数量用 $p$ 表示。\n\n**2. 特征重要性量化**\n\n每个特征的重要性通过训练一个由 $T = 200$ 个决策树桩组成的集成模型来确定。决策树桩是只有一个分裂的决策树。\n\n- **集成构建**：对于集成模型中的 $T$ 个树桩中的每一个：\n    1. 通过从完整数据集 $(\\tilde{\\mathbf{X}}, \\mathbf{y})$ 中进行有放回抽样，创建一个大小为 $n$ 的自助样本。\n    2. 随机选择一个由 $m_{\\text{try}} = \\lceil \\sqrt{p} \\rceil$ 个不同特征组成的子集。\n    3. 对于每个选定的特征，找到最优分裂。一个分裂由一个特征 $j$ 和一个阈值 $\\tau$ 定义。分裂的质量由基尼不纯度减少量 $\\Delta G$ 来衡量。一组标签 $S$ 的基尼不纯度由下式给出：\n    $$\n    G(S) = 1 - \\sum_{c \\in \\{0,1\\}} \\left(\\frac{n_c}{|S|}\\right)^2\n    $$\n    其中 $n_c$ 是集合 $S$ 中类别 $c$ 的计数。不纯度减少量是父节点的不纯度与两个子节点不纯度的加权平均之差。\n    4. 选择能产生最大不纯度减少量 $\\Delta G^\\star$ 的特征 $j^\\star$ 和阈值 $\\tau^\\star$ 作为树桩的分裂。潜在的阈值是自助样本中特征的连续唯一排序值的中点。\n\n- **重要性聚合**：一个特征 $j$ 的重要性，记作 $I_j$，计算为它在集成模型中所有树上所贡献的不纯度减少量之和：\n$$\nI_j = \\sum_{t=1}^{T} \\Delta G_t \\cdot \\mathbf{1}\\{j_t^\\star = j\\}\n$$\n其中 $\\mathbf{1}\\{\\cdot\\}$ 是指示函数。\n\n- **结果**：最后，对于每个测试用例，识别出具有最高重要性得分的特征的零基索引，$j_{\\max} = \\arg\\max_j I_j$。平局通过选择最小的索引来解决。此索引是该用例的输出。该过程使用 Python 实现，遵循指定的库和随机种子，以确保结果可验证。", "answer": "```python\nimport numpy as np\nfrom math import ceil, sqrt\n\ndef _gini_impurity(y):\n    \"\"\"Calculates the Gini impurity of a set of binary labels.\"\"\"\n    n_samples = len(y)\n    if n_samples == 0:\n        return 0.0\n    n1_count = np.sum(y)\n    p1 = n1_count / n_samples\n    p0 = 1.0 - p1\n    return 1.0 - (p0**2 + p1**2)\n\ndef _find_best_split(X_boot, y_boot, feature_indices):\n    \"\"\"Finds the best split for a single decision stump.\"\"\"\n    n_samples = len(y_boot)\n    if n_samples <= 1:\n        return -1, -1.0\n\n    best_gain = -1.0\n    best_feature = -1\n\n    gini_parent = _gini_impurity(y_boot)\n    n1_total = np.sum(y_boot)\n    \n    for j in feature_indices:\n        feature_values = X_boot[:, j]\n        \n        unique_vals = np.unique(feature_values)\n        if len(unique_vals) < 2:\n            continue\n            \n        # Efficiently find the best split for feature j\n        sorted_indices = np.argsort(feature_values)\n        y_sorted = y_boot[sorted_indices]\n        x_sorted = feature_values[sorted_indices]\n        \n        y_cumsum = np.cumsum(y_sorted)\n\n        split_points = np.where(x_sorted[:-1] != x_sorted[1:])[0]\n\n        for i in split_points:\n            n_left = i + 1\n            n_right = n_samples - n_left\n\n            n1_left = y_cumsum[i]\n            n1_right = n1_total - n1_left\n\n            gini_left = 1.0 - ((n1_left / n_left)**2 + ((n_left - n1_left) / n_left)**2)\n            gini_right = 1.0 - ((n1_right / n_right)**2 + ((n_right - n1_right) / n_right)**2)\n            \n            gain = gini_parent - (n_left / n_samples * gini_left + n_right / n_samples * gini_right)\n\n            if gain > best_gain:\n                best_gain = gain\n                best_feature = j\n                \n    return best_feature, best_gain\n\ndef solve():\n    \"\"\"\n    Main function to run the simulation for all test cases and print the results.\n    \"\"\"\n    test_cases = [\n        # (n, sigma, lambda, rho, include_leak, seed)\n        (3000, 0.1, 0.9, 0.2, 1, 11),\n        (3000, 0.3, 0.6, 0.2, 1, 12),\n        (1200, 0.1, 0.9, 0.2, 0, 13),\n        (3000, 0.6, 0.4, 0.2, 1, 14),\n    ]\n\n    results = []\n\n    for n, sigma, lam, rho, include_leak, seed in test_cases:\n        # Set seed for reproducibility for the entire test case\n        np.random.seed(seed)\n\n        # 1. Data Generation\n        p_b = 5\n        betas = np.array([0.8, -1.0, 0.6, 0.0, 0.5])\n        beta_0 = -0.5\n        gamma = 0.7\n        \n        # Covariance matrix\n        cov_matrix = (1 - rho) * np.identity(p_b) + rho * np.ones((p_b, p_b))\n        \n        # Base features\n        X_base = np.random.multivariate_normal(np.zeros(p_b), cov_matrix, n)\n        \n        # Macro factor\n        m = np.random.randn(n)\n        \n        # Latent score\n        s = beta_0 + X_base @ betas + gamma * m\n        \n        # Default probability\n        p_default = 1.0 / (1.0 + np.exp(-s))\n        \n        # Binary outcome\n        y = np.random.binomial(1, p_default, n)\n        \n        # Assemble final feature matrix\n        if include_leak == 1:\n            delta = np.random.normal(0, sigma, n)\n            z = lam * y + delta\n            X = np.c_[X_base, z]\n        else:\n            X = X_base\n        \n        n_samples, p = X.shape\n\n        # 2. Ensemble Training and Importance Calculation\n        T = 200\n        m_try = ceil(sqrt(p))\n        importances = np.zeros(p)\n        \n        all_indices = np.arange(n_samples)\n\n        for _ in range(T):\n            # Bootstrap sample\n            boot_indices = np.random.choice(all_indices, size=n_samples, replace=True)\n            X_boot, y_boot = X[boot_indices], y[boot_indices]\n            \n            # Feature subsampling\n            feature_indices = np.random.choice(p, size=m_try, replace=False)\n            \n            # Find best split for this stump\n            best_feature, best_gain = _find_best_split(X_boot, y_boot, feature_indices)\n            \n            if best_feature != -1:\n                importances[best_feature] += best_gain\n\n        # 3. Find the most important feature index\n        j_max = np.argmax(importances)\n        results.append(j_max)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2386893"}]}