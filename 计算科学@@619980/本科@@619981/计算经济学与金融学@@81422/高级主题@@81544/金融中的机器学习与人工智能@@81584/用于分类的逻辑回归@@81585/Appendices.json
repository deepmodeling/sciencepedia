{"hands_on_practices": [{"introduction": "理论知识需要通过实践来巩固。本次练习将指导你从头开始，仅使用基本的数值库来实现带正则化的逻辑斯谛回归。你将通过预测银行倒闭这一经典的金融问题[@problem_id:2407577]，亲手实现基于牛顿-拉弗森法（Newton-Raphson method）的求解器，从而深刻理解模型优化的核心机制，例如梯度和海森矩阵在参数估计中的作用。", "problem": "要求您基于第一性原理，实现一个适用于计算金融领域中违约预测的二元分类模型。将银行倒闭指标建模为一个伯努利随机变量，其成功概率通过典则逻辑斯谛链接与一个解释变量的线性指数相关联。您必须推导、实现并求解最大似然估计问题，其中仅对斜率系数（不包括截距）施加一个平方欧几里得惩罚。您的实现必须仅使用标准数值库中可用的基本数值线性代数和优化构建模块。\n\n其经济背景是基于两项资本充足率度量来预测银行是否会倒闭：资本充足率和不良贷款率。您将获得一个固定的、小规模的银行训练数据集，每家银行有两个特征：资本充足率和不良贷款率，均为小数形式（非百分比）。设这两个特征分别用 $x_1$ 和 $x_2$ 表示，并在线性指数中包含一个等于 $1$ 的截距项 $x_0$。训练数据集包含以下 $16$ 个观测值：\n- 样本 1：$(x_1, x_2) = (0.12, 0.03)$，标签 $y = 0$。\n- 样本 2：$(x_1, x_2) = (0.10, 0.05)$，标签 $y = 0$。\n- 样本 3：$(x_1, x_2) = (0.08, 0.07)$，标签 $y = 0$。\n- 样本 4：$(x_1, x_2) = (0.06, 0.12)$，标签 $y = 1$。\n- 样本 5：$(x_1, x_2) = (0.09, 0.10)$，标签 $y = 0$。\n- 样本 6：$(x_1, x_2) = (0.07, 0.11)$，标签 $y = 1$。\n- 样本 7：$(x_1, x_2) = (0.05, 0.14)$，标签 $y = 1$。\n- 样本 8：$(x_1, x_2) = (0.11, 0.04)$，标签 $y = 0$。\n- 样本 9：$(x_1, x_2) = (0.13, 0.06)$，标签 $y = 0$。\n- 样本 10：$(x_1, x_2) = (0.04, 0.15)$，标签 $y = 1$。\n- 样本 11：$(x_1, x_2) = (0.09, 0.02)$，标签 $y = 0$。\n- 样本 12：$(x_1, x_2) = (0.06, 0.08)$，标签 $y = 0$。\n- 样本 13：$(x_1, x_2) = (0.07, 0.05)$，标签 $y = 0$。\n- 样本 14：$(x_1, x_2) = (0.05, 0.09)$，标签 $y = 1$。\n- 样本 15：$(x_1, x_2) = (0.12, 0.10)$，标签 $y = 0$。\n- 样本 16：$(x_1, x_2) = (0.03, 0.12)$，标签 $y = 1$。\n\n您的任务是：\n- 从独立观测的伯努利似然和广义线性模型中的逻辑斯谛链接定义出发。\n- 推导待最小化的目标函数，即负对数似然函数加上对斜率系数的 $L_2$ 惩罚。不对截距项进行惩罚。\n- 推导梯度和一个适用于牛顿类方法的合适的二阶近似，该方法能保证凸目标的收敛性。\n- 实现一个仅使用基本数值运算、向量化和线性代数的求解器。确保对于大的正或负线性指数具有数值稳定性。\n- 使用不同的正则化强度多次训练模型，然后为指定的特征向量计算样本外预测的倒闭概率。\n\n测试套件和要求的输出：\n- 您必须拟合模型三次，每次使用相同的训练数据，但使用不同的正则化强度 $\\lambda$ 和一个不同的单一样本外银行 $(x_1, x_2)$，您必须为该银行生成 $[0,1]$ 区间内的预测倒闭概率（小数形式）：\n    1. 情况 A（一般情况）：$\\lambda = 1.0$，在 $(x_1, x_2) = (0.055, 0.13)$ 处进行评估。\n    2. 情况 B（接近决策边界）：$\\lambda = 0.01$，在 $(x_1, x_2) = (0.07, 0.10)$ 处进行评估。\n    3. 情况 C（强正则化边界情况）：$\\lambda = 100.0$，在 $(x_1, x_2) = (0.03, 0.15)$ 处进行评估。\n- 对于每种情况，输出一个浮点数，该浮点数等于为指定银行预测的倒闭概率，四舍五入到恰好 $6$ 位小数。不涉及物理单位。所有比率都是小数，而不是百分比。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，结果按上述情况的顺序排列，例如 $[p_A,p_B,p_C]$，其中每个条目都四舍五入到恰好 $6$ 位小数。\n- 程序必须是完全自包含的，不得读取任何输入或写入任何文件，并且必须仅使用指定的数值库。", "solution": "问题陈述已经过验证，并被认为是有效的。它具有科学依据、是良定的、客观的，并包含了推导唯一且有意义的解所需的所有必要信息。该任务是广义线性模型的惩罚最大似然估计的一个标准应用，这是统计机器学习和计算经济学的基石。我们现在将进行形式化推导和求解。\n\n该问题要求构建一个用于银行倒闭预测的二元分类模型。银行 $i$ 的结果（用 $y_i$ 表示）是一个伯努利随机变量，其中 $y_i=1$ 表示倒闭，$y_i=0$ 表示未倒闭。该模型通过一个逻辑斯谛函数将倒闭概率 $p_i = P(y_i=1)$ 与一组解释变量联系起来。\n\n设第 $i$ 家银行的特征向量为 $\\mathbf{x}_i = [x_{i0}, x_{i1}, x_{i2}]^T$，其中 $x_{i0} = 1$ 是截距项，$x_{i1}$ 是资本充足率，$x_{i2}$ 是不良贷款率。设相应的系数向量为 $\\boldsymbol{\\beta} = [\\beta_0, \\beta_1, \\beta_2]^T$。线性指数 $\\eta_i$ 由内积给出：\n$$\n\\eta_i = \\mathbf{x}_i^T \\boldsymbol{\\beta} = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2}\n$$\n倒闭概率 $p_i$ 使用典则逻辑斯谛链接函数（也称为 Sigmoid 函数 $\\sigma(\\cdot)$）进行建模：\n$$\np_i(\\boldsymbol{\\beta}) = \\sigma(\\eta_i) = \\frac{1}{1 + e^{-\\eta_i}}\n$$\n对于单个银行，在给定其特征 $\\mathbf{x}_i$ 和参数 $\\boldsymbol{\\beta}$ 的条件下，观测到结果 $y_i$ 的似然由伯努利概率质量函数描述：\n$$\nL_i(\\boldsymbol{\\beta} | y_i, \\mathbf{x}_i) = p_i^{y_i} (1 - p_i)^{1 - y_i}\n$$\n假设观测是独立的，一个包含 $N$ 家银行的数据集的总似然是各单个似然的乘积：\n$$\n\\mathcal{L}(\\boldsymbol{\\beta}) = \\prod_{i=1}^{N} L_i(\\boldsymbol{\\beta}) = \\prod_{i=1}^{N} p_i^{y_i} (1 - p_i)^{1 - y_i}\n$$\n为便于分析和数值计算，我们使用对数似然函数 $\\ell(\\boldsymbol{\\beta})$：\n$$\n\\ell(\\boldsymbol{\\beta}) = \\log \\mathcal{L}(\\boldsymbol{\\beta}) = \\sum_{i=1}^{N} \\left[ y_i \\log(p_i) + (1 - y_i) \\log(1 - p_i) \\right]\n$$\n代入 $p_i$ 和 $1-p_i = \\frac{1}{1+e^{\\eta_i}}$ 的表达式，我们可以简化单个观测的对数似然：\n$$\n\\ell_i(\\boldsymbol{\\beta}) = y_i \\log\\left(\\frac{p_i}{1-p_i}\\right) + \\log(1-p_i) = y_i \\eta_i - \\log(1+e^{\\eta_i})\n$$\n总对数似然则为：\n$$\n\\ell(\\boldsymbol{\\beta}) = \\sum_{i=1}^{N} \\left( y_i \\eta_i - \\log(1+e^{\\eta_i}) \\right)\n$$\n该问题要求最小化负对数似然，并对斜率系数 $\\beta_1$ 和 $\\beta_2$（但不包括截距 $\\beta_0$）施加平方欧几里得惩罚（$L_2$ 正则化）。正则化参数为 $\\lambda$。待最小化的目标函数 $J(\\boldsymbol{\\beta})$ 为：\n$$\nJ(\\boldsymbol{\\beta}) = - \\ell(\\boldsymbol{\\beta}) + \\frac{\\lambda}{2} \\left( \\beta_1^2 + \\beta_2^2 \\right) = \\sum_{i=1}^{N} \\left[ \\log(1+e^{\\eta_i}) - y_i \\eta_i \\right] + \\frac{\\lambda}{2} \\sum_{j=1}^{2} \\beta_j^2\n$$\n因子 $1/2$ 是一个标准惯例，用以简化梯度。\n\n为求解此最小化问题，我们采用 Newton-Raphson 优化算法，这是一种需要目标函数的梯度和海森矩阵的二阶方法。\n\n$J(\\boldsymbol{\\beta})$ 的梯度，记作 $\\nabla J(\\boldsymbol{\\beta})$，是关于 $\\boldsymbol{\\beta}$ 各分量的偏导数向量。\n首先，我们求 $\\eta_i = \\mathbf{x}_i^T \\boldsymbol{\\beta}$ 关于 $\\boldsymbol{\\beta}$ 的偏导数，即 $\\mathbf{x}_i$。\n$$\n\\nabla J(\\boldsymbol{\\beta}) = \\sum_{i=1}^{N} \\left[ \\frac{\\partial}{\\partial \\boldsymbol{\\beta}} \\log(1+e^{\\eta_i}) - \\frac{\\partial}{\\partial \\boldsymbol{\\beta}} (y_i \\eta_i) \\right] + \\frac{\\partial}{\\partial \\boldsymbol{\\beta}} \\left( \\frac{\\lambda}{2} \\sum_{j=1}^{2} \\beta_j^2 \\right)\n$$\n$$\n\\nabla J(\\boldsymbol{\\beta}) = \\sum_{i=1}^{N} \\left[ \\frac{e^{\\eta_i}}{1+e^{\\eta_i}} \\mathbf{x}_i - y_i \\mathbf{x}_i \\right] + \\lambda [0, \\beta_1, \\beta_2]^T\n$$\n注意到 $e^{\\eta_i}/(1+e^{\\eta_i}) = p_i$，我们得到：\n$$\n\\nabla J(\\boldsymbol{\\beta}) = \\sum_{i=1}^{N} (p_i - y_i) \\mathbf{x}_i + \\lambda \\mathbf{P} \\boldsymbol{\\beta}\n$$\n其中 $\\mathbf{P}$ 是一个将截距项置为零的投影矩阵：$\\mathbf{P} = \\mathrm{diag}(0, 1, 1)$。在矩阵表示法中，设 $\\mathbf{X}$ 为 $N \\times 3$ 的设计矩阵，$\\mathbf{y}$ 为 $N \\times 1$ 的标签向量，$\\mathbf{p}$ 为 $N \\times 1$ 的预测概率向量。梯度为：\n$$\n\\nabla J(\\boldsymbol{\\beta}) = \\mathbf{X}^T (\\mathbf{p} - \\mathbf{y}) + \\lambda \\mathbf{P} \\boldsymbol{\\beta}\n$$\n海森矩阵 $\\mathbf{H}(\\boldsymbol{\\beta})$ 是二阶偏导数矩阵 $\\nabla^2 J(\\boldsymbol{\\beta})$。将梯度对 $\\boldsymbol{\\beta}^T$ 求导：\n$$\n\\mathbf{H}(\\boldsymbol{\\beta}) = \\frac{\\partial}{\\partial \\boldsymbol{\\beta}^T} \\left( \\sum_{i=1}^N (p_i - y_i) \\mathbf{x}_i \\right) + \\lambda \\mathbf{P} = \\sum_{i=1}^N \\frac{\\partial p_i}{\\partial \\boldsymbol{\\beta}^T} \\mathbf{x}_i + \\lambda \\mathbf{P}\n$$\nSigmoid 函数的导数是 $\\sigma'(\\eta_i) = \\sigma(\\eta_i)(1-\\sigma(\\eta_i)) = p_i(1-p_i)$。\n$$\n\\frac{\\partial p_i}{\\partial \\boldsymbol{\\beta}^T} = \\frac{\\partial \\sigma(\\eta_i)}{\\partial \\boldsymbol{\\beta}^T} = \\sigma'(\\eta_i) \\frac{\\partial \\eta_i}{\\partial \\boldsymbol{\\beta}^T} = p_i(1-p_i) \\mathbf{x}_i^T\n$$\n将此代入海森矩阵的表达式中：\n$$\n\\mathbf{H}(\\boldsymbol{\\beta}) = \\sum_{i=1}^{N} p_i(1-p_i) \\mathbf{x}_i \\mathbf{x}_i^T + \\lambda \\mathbf{P}\n$$\n在矩阵表示法中，设 $\\mathbf{W}$ 为一个 $N \\times N$ 的对角矩阵，其对角元素为 $W_{ii} = p_i(1-p_i)$。海森矩阵为：\n$$\n\\mathbf{H}(\\boldsymbol{\\beta}) = \\mathbf{X}^T \\mathbf{W} \\mathbf{X} + \\lambda \\mathbf{P}\n$$\n对于 $\\lambda > 0$ 和非退化数据，该海森矩阵是正定的，这保证了目标函数 $J(\\boldsymbol{\\beta})$ 是严格凸的，并有唯一的最小值。用于找到此最小值的 Newton-Raphson 更新步骤是：\n$$\n\\boldsymbol{\\beta}_{k+1} = \\boldsymbol{\\beta}_k - \\mathbf{H}(\\boldsymbol{\\beta}_k)^{-1} \\nabla J(\\boldsymbol{\\beta}_k)\n$$\n在实践中，我们不计算海森矩阵的逆。相反，我们求解线性系统以获得更新步长 $\\Delta \\boldsymbol{\\beta}_k = - \\mathbf{H}(\\boldsymbol{\\beta}_k)^{-1} \\nabla J(\\boldsymbol{\\beta}_k)$：\n$$\n\\mathbf{H}(\\boldsymbol{\\beta}_k) \\Delta \\boldsymbol{\\beta}_k = - \\nabla J(\\boldsymbol{\\beta}_k)\n$$\n然后，我们更新参数：$\\boldsymbol{\\beta}_{k+1} = \\boldsymbol{\\beta}_k + \\Delta \\boldsymbol{\\beta}_k$。算法从一个初始猜测（例如 $\\boldsymbol{\\beta}_0 = \\mathbf{0}$）开始，并进行迭代，直到梯度的范数低于指定的容差或达到最大迭代次数。\n\n为确保数值稳定性，Sigmoid 函数 $\\sigma(z) = 1/(1+e^{-z})$ 的实现必须小心，以避免当 $z$ 是一个很大的负数时发生溢出。一个鲁棒的实现是：\n$$\n\\sigma(z) = \\begin{cases} 1 / (1 + e^{-z}) & \\text{if } z \\ge 0 \\\\ e^z / (1 + e^z) & \\text{if } z < 0 \\end{cases}\n$$\n这确保了指数始终为非正数，从而防止溢出。\n\n实现将包含一个函数，该函数接收训练数据 $(\\mathbf{X}, \\mathbf{y})$ 和正则化参数 $\\lambda$，执行 Newton-Raphson 优化以找到最优的 $\\boldsymbol{\\beta}$，然后使用这个估计的 $\\boldsymbol{\\beta}$ 来预测新的样本外数据的倒闭概率。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n# from scipy is not used as per constraints to implement from first principles.\n\ndef solve():\n    \"\"\"\n    Implements and solves a regularized logistic regression problem\n    for bank failure prediction from first principles.\n    \"\"\"\n\n    # Training data provided in the problem statement.\n    # N = 16 observations\n    # Features: x_1 = capital adequacy ratio, x_2 = nonperforming loans ratio\n    # Label: y = 1 for failure, 0 for non-failure\n    training_data = np.array([\n        [0.12, 0.03, 0.], [0.10, 0.05, 0.], [0.08, 0.07, 0.], [0.06, 0.12, 1.],\n        [0.09, 0.10, 0.], [0.07, 0.11, 1.], [0.05, 0.14, 1.], [0.11, 0.04, 0.],\n        [0.13, 0.06, 0.], [0.04, 0.15, 1.], [0.09, 0.02, 0.], [0.06, 0.08, 0.],\n        [0.07, 0.05, 0.], [0.05, 0.09, 1.], [0.12, 0.10, 0.], [0.03, 0.12, 1.]\n    ])\n\n    # Feature matrix X (with intercept) and target vector y\n    N = training_data.shape[0]\n    X_features = training_data[:, :2]\n    X = np.c_[np.ones(N), X_features]  # Design matrix, shape (16, 3)\n    y = training_data[:, 2]            # Target vector, shape (16,)\n\n    # Test cases from the problem statement\n    test_cases = [\n        # (lambda, (x1, x2))\n        (1.0, (0.055, 0.13)),\n        (0.01, (0.07, 0.10)),\n        (100.0, (0.03, 0.15)),\n    ]\n\n    def _sigmoid(z):\n        \"\"\"Numerically stable sigmoid function.\"\"\"\n        # Use np.where to handle arrays efficiently\n        return np.where(z >= 0, \n                        1 / (1 + np.exp(-z)), \n                        np.exp(z) / (1 + np.exp(z)))\n\n    def _fit_logistic_ridge(X, y, lambda_val, max_iter=100, tol=1e-9):\n        \"\"\"\n        Fits a logistic regression model with L2 penalty on slope coefficients\n        using the Newton-Raphson method.\n\n        Args:\n            X (np.ndarray): Design matrix with shape (N, D), including intercept.\n            y (np.ndarray): Target vector with shape (N,).\n            lambda_val (float): Regularization strength.\n            max_iter (int): Maximum number of iterations.\n            tol (float): Convergence tolerance for the gradient norm.\n\n        Returns:\n            np.ndarray: Estimated coefficient vector beta with shape (D,).\n        \"\"\"\n        n_samples, n_features = X.shape\n        beta = np.zeros(n_features)\n\n        # Regularization matrix P, does not penalize the intercept (beta_0)\n        P = np.diag([0.] + [1.] * (n_features - 1))\n\n        for i in range(max_iter):\n            # Calculate linear predictor, eta = X @ beta\n            eta = X.dot(beta)\n\n            # Calculate probabilities, p = sigma(eta)\n            p = _sigmoid(eta)\n\n            # Calculate gradient of the objective function\n            # g = X.T @ (p - y) + lambda * P @ beta\n            g = X.T.dot(p - y) + lambda_val * P.dot(beta)\n            \n            # Check for convergence\n            if np.linalg.norm(g)  tol:\n                break\n            \n            # Calculate Hessian of the objective function\n            # H = X.T @ W @ X + lambda * P\n            # W is a diagonal matrix with W_ii = p_i * (1 - p_i)\n            # For efficiency, we can compute X.T @ W @ X without explicitly forming W\n            # H = (X.T * (p * (1 - p))) @ X + lambda_val * P\n            w_diag = p * (1 - p)\n            H = X.T.dot(np.diag(w_diag)).dot(X) + lambda_val * P\n            \n            # Solve the linear system H * delta_beta = -g for the update step\n            # This is numerically more stable than inverting H\n            delta_beta = np.linalg.solve(H, -g)\n\n            # Update beta\n            beta += delta_beta\n        \n        return beta\n\n    def _predict_proba(x_new_features, beta):\n        \"\"\"\n        Predicts the probability for a new observation.\n\n        Args:\n            x_new_features (tuple): A tuple (x1, x2) for the new observation.\n            beta (np.ndarray): The trained coefficient vector.\n\n        Returns:\n            float: The predicted probability P(y=1).\n        \"\"\"\n        x_new = np.array([1.] + list(x_new_features))\n        eta_new = x_new.dot(beta)\n        return _sigmoid(eta_new)\n\n    results = []\n    for lambda_val, x_new_features in test_cases:\n        # Fit the model for the given lambda\n        beta_hat = _fit_logistic_ridge(X, y, lambda_val)\n        \n        # Predict the probability for the new observation\n        p_hat = _predict_proba(x_new_features, beta_hat)\n        \n        # Format the result to 6 decimal places and append\n        results.append(f\"{p_hat:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "2407577"}, {"introduction": "在亲手构建了模型之后，我们转向更高效的实践方式——利用专业的优化库。本次练习[@problem_id:2407571]将使用 Python 中强大的 `scipy` 库来解决一个企业金融领域的分类问题：预测企业并购能否创造股东价值。通过这个实践，你将学会如何将理论模型转化为实际代码，并探索正则化参数 $\\lambda$ 对模型预测能力的具体影响。", "problem": "考虑一个二元分类问题，其中每个观测值代表一宗已完成的企业收购。对于观测值 $i \\in \\{1,\\dots,n\\}$，令 $\\mathbf{z}_i \\in \\mathbb{R}^d$ 表示在收购公告时测量的 $d$ 个可观测特征组成的向量，令 $y_i \\in \\{0,1\\}$ 为收购方在收购完成后一年内获得正异常回报的指示符。数据中所有的分数均已表示为 $[0,1]$ 区间内的小数。给定 $n = 14$ 个观测值，每个观测值有 $d = 4$ 个特征，这些数据被收集到矩阵 $Z \\in \\mathbb{R}^{n \\times d}$ 和标签向量 $\\mathbf{y} \\in \\{0,1\\}^n$ 中。这些特征按列分别为：相对于目标公司公告前价格的交易溢价、相对规模（目标公司市值除以收购方市值）、收购方净杠杆率以及行业重叠指数。训练数据如下\n$$\nZ=\\begin{bmatrix}\n0.20  0.30  0.25  0.80\\\\\n0.35  0.60  0.50  0.20\\\\\n0.10  0.20  0.15  0.90\\\\\n0.45  0.70  0.60  0.10\\\\\n0.25  0.40  0.30  0.70\\\\\n0.30  0.50  0.55  0.30\\\\\n0.15  0.25  0.20  0.85\\\\\n0.50  0.80  0.65  0.05\\\\\n0.18  0.35  0.22  0.75\\\\\n0.40  0.55  0.45  0.40\\\\\n0.22  0.32  0.28  0.60\\\\\n0.28  0.45  0.35  0.50\\\\\n0.38  0.65  0.58  0.25\\\\\n0.12  0.18  0.12  0.95\n\\end{bmatrix},\\quad\n\\mathbf{y}=\\begin{bmatrix}\n1\\\\\n0\\\\\n1\\\\\n0\\\\\n1\\\\\n0\\\\\n1\\\\\n0\\\\\n1\\\\\n0\\\\\n1\\\\\n1\\\\\n0\\\\\n1\n\\end{bmatrix}.\n$$\n假设使用以下概率模型。对于每个观测值 $i$，定义增广特征向量 $\\mathbf{x}_i = \\begin{bmatrix}1  \\mathbf{z}_i^\\top\\end{bmatrix}^\\top \\in \\mathbb{R}^{d+1}$，并令 $\\boldsymbol{\\theta}=\\begin{bmatrix}\\beta_0  \\boldsymbol{\\beta}^\\top\\end{bmatrix}^\\top \\in \\mathbb{R}^{d+1}$。在给定 $\\mathbf{z}_i$ 的条件下 $y_i=1$ 的条件概率为\n$$\n\\mathbb{P}\\!\\left(y_i=1 \\mid \\mathbf{z}_i\\right)=\\sigma\\!\\left(\\mathbf{x}_i^\\top \\boldsymbol{\\theta}\\right),\\quad \\sigma(t)=\\frac{1}{1+e^{-t}}。\n$$\n参数 $\\boldsymbol{\\theta}$ 由惩罚对数似然函数的唯一最大化器（若存在）确定\n$$\n\\ell_\\lambda(\\boldsymbol{\\theta})=\\sum_{i=1}^{n}\\left[y_i\\log\\!\\left(\\sigma\\!\\left(\\mathbf{x}_i^\\top \\boldsymbol{\\theta}\\right)\\right)+(1-y_i)\\log\\!\\left(1-\\sigma\\!\\left(\\mathbf{x}_i^\\top \\boldsymbol{\\theta}\\right)\\right)\\right]-\\frac{\\lambda}{2}\\left\\|\\boldsymbol{\\beta}\\right\\|_2^2,\n$$\n其中 $\\lambda \\ge 0$ 是给定的惩罚参数，截距 $\\beta_0$不参与惩罚。\n\n您的任务是，对下方的每个测试用例，计算一宗具有特征 $\\mathbf{z}^\\star \\in \\mathbb{R}^d$ 的新收购能创造价值的预测概率，该概率定义为\n$$\np^\\star=\\sigma\\!\\left(\\begin{bmatrix}1  (\\mathbf{z}^\\star)^\\top\\end{bmatrix}\\boldsymbol{\\theta}_\\lambda^\\star\\right),\n$$\n其中 $\\boldsymbol{\\theta}_\\lambda^\\star$ 是使用上述训练数据 $(Z,\\mathbf{y})$ 针对指定的 $\\lambda$ 对 $\\ell_\\lambda(\\boldsymbol{\\theta})$ 进行最大化所得到的任意最大化器。\n\n测试套件（每个用例指定 $(\\lambda,\\mathbf{z}^\\star)$）：\n- 用例 $1$：$\\lambda=0.1$，$\\mathbf{z}^\\star=\\begin{bmatrix}0.27\\\\0.40\\\\0.33\\\\0.55\\end{bmatrix}$。\n- 用例 $2$：$\\lambda=10.0$，$\\mathbf{z}^\\star=\\begin{bmatrix}0.27\\\\0.40\\\\0.33\\\\0.55\\end{bmatrix}$。\n- 用例 $3$：$\\lambda=0.0$，$\\mathbf{z}^\\star=\\begin{bmatrix}0.55\\\\0.90\\\\0.70\\\\0.05\\end{bmatrix}$。\n- 用例 $4$：$\\lambda=0.01$，$\\mathbf{z}^\\star=\\begin{bmatrix}0.08\\\\0.15\\\\0.10\\\\0.98\\end{bmatrix}$。\n\n要求：\n- 使用上述指定的模型和训练数据，为每个用例计算 $p^\\star$。\n- 将所有输出表示为 $[0,1]$ 区间内的实数，并精确到 $6$ 位小数。\n- 您的程序应生成单行输出，其中包含用逗号分隔并用方括号括起来的结果列表，结果顺序与上述用例顺序相同，例如 $\\texttt{[0.123456,0.234567,0.345678,0.456789]}$。", "solution": "所呈现的问题是 L2 正则化逻辑回归在二元分类中的一个明确定义的应用，这是计算金融学和计量经济学中的一种标准技术。任务是在给定一组训练数据和特定的正则化参数的情况下，为几个测试用例计算正向结果的预测概率。该问题具有科学依据，在数学上是一致的，并包含所有必要信息，以便在 $\\lambda  0$ 的情况下确定唯一解，在 $\\lambda=0$ 的情况下确定数值可计算的解。\n\n该模型由具有特征 $\\mathbf{z}_i \\in \\mathbb{R}^d$ 的观测值出现正向结果（$y_i=1$）的条件概率定义：\n$$ \\mathbb{P}(y_i=1 \\mid \\mathbf{z}_i) = \\sigma(\\mathbf{x}_i^\\top \\boldsymbol{\\theta}) $$\n其中 $\\mathbf{x}_i = \\begin{bmatrix}1  \\mathbf{z}_i^\\top\\end{bmatrix}^\\top$ 是维度为 $d+1$ 的增广特征向量，$\\boldsymbol{\\theta} \\in \\mathbb{R}^{d+1}$ 是参数向量，而 $\\sigma(t) = (1+e^{-t})^{-1}$ 是逻辑 sigmoid 函数。\n\n参数向量 $\\boldsymbol{\\theta}_\\lambda^\\star = \\begin{bmatrix}\\beta_{0,\\lambda}^\\star  (\\boldsymbol{\\beta}_\\lambda^\\star)^\\top\\end{bmatrix}^\\top$ 通过最大化惩罚对数似然函数得到：\n$$ \\ell_\\lambda(\\boldsymbol{\\theta})=\\sum_{i=1}^{n}\\left[y_i\\log(p_i)+(1-y_i)\\log(1-p_i)\\right]-\\frac{\\lambda}{2}\\left\\|\\boldsymbol{\\beta}\\right\\|_2^2 $$\n其中 $p_i = \\sigma(\\mathbf{x}_i^\\top \\boldsymbol{\\theta})$，$n=14$ 是观测值的数量，而 $\\boldsymbol{\\beta} = \\begin{bmatrix}\\beta_1  \\dots  \\beta_d\\end{bmatrix}^\\top$ 是 $d=4$ 个特征的系数，不包括截距 $\\beta_0$。\n\n为解决这个最大化问题，我们等价地最小化惩罚对数似然的负值。令该目标函数为 $f(\\boldsymbol{\\theta}) = -\\ell_\\lambda(\\boldsymbol{\\theta})$。这是一个凸优化问题，其解可以通过数值方法找到。目标函数为：\n$$ f(\\boldsymbol{\\theta}) = -\\left( \\sum_{i=1}^{n}\\left[y_i\\log(p_i)+(1-y_i)\\log(1-p_i)\\right] \\right) + \\frac{\\lambda}{2}\\left\\|\\boldsymbol{\\beta}\\right\\|_2^2 $$\n为了稳健的数值计算，交叉熵损失项可以表示为更稳定的形式。要最小化的总损失的正确表达式为：\n$$ f(\\boldsymbol{\\theta}) = \\sum_{i=1}^n \\left[ (1-y_i)\\log(1+e^{\\mathbf{x}_i^\\top\\boldsymbol{\\theta}}) + y_i\\log(1+e^{-\\mathbf{x}_i^\\top\\boldsymbol{\\theta}}) \\right] + \\frac{\\lambda}{2} \\sum_{j=1}^d \\beta_j^2 $$\n我们将使用一种拟牛顿优化算法，特别是 L-BFGS-B，该算法需要目标函数的梯度。梯度 $\\nabla f(\\boldsymbol{\\theta})$ 推导如下：\n$$ \\nabla f(\\boldsymbol{\\theta}) = X^\\top(\\mathbf{p} - \\mathbf{y}) + \\lambda \\boldsymbol{\\theta}_{\\text{pen}} $$\n在这里，$X \\in \\mathbb{R}^{n \\times (d+1)}$ 是设计矩阵，通过在特征矩阵 $Z$ 前添加一列全为 1 的列构成。向量 $\\mathbf{y} \\in \\{0,1\\}^n$ 包含标签，$\\mathbf{p} = \\sigma(X\\boldsymbol{\\theta})$ 是预测概率的向量，而 $\\boldsymbol{\\theta}_{\\text{pen}} = \\begin{bmatrix}0  \\beta_1  \\dots  \\beta_d\\end{bmatrix}^\\top$ 是参数向量，其截距分量被置零以反映 $\\beta_0$ 不受惩罚的事实。\n\n对于由一对 $(\\lambda, \\mathbf{z}^\\star)$ 定义的每个测试用例，执行以下步骤：\n$1$. 为给定的 $\\lambda$ 定义目标函数 $f(\\boldsymbol{\\theta})$ 及其梯度 $\\nabla f(\\boldsymbol{\\theta})$。\n$2$. 一个数值优化器从初始猜测（例如 $\\boldsymbol{\\theta}^{(0)} = \\mathbf{0}$）开始最小化 $f(\\boldsymbol{\\theta})$，以找到最优参数向量 $\\boldsymbol{\\theta}_\\lambda^\\star$。\n$3$. 新的特征向量 $\\mathbf{z}^\\star$ 被增广为 $\\mathbf{x}^\\star = \\begin{bmatrix}1  (\\mathbf{z}^\\star)^\\top\\end{bmatrix}^\\top$。\n$4$. 预测概率 $p^\\star$ 计算为 $p^\\star = \\sigma((\\mathbf{x}^\\star)^\\top \\boldsymbol{\\theta}_\\lambda^\\star)$。\n\n对所有四个测试用例重复此过程，并将所得概率四舍五入到指定的精度。", "answer": "```python\nimport numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.special import expit\n\ndef solve():\n    \"\"\"\n    Solves the L2-regularized logistic regression problem for the given test cases.\n    \"\"\"\n    # Training data provided in the problem statement.\n    Z = np.array([\n        [0.20, 0.30, 0.25, 0.80],\n        [0.35, 0.60, 0.50, 0.20],\n        [0.10, 0.20, 0.15, 0.90],\n        [0.45, 0.70, 0.60, 0.10],\n        [0.25, 0.40, 0.30, 0.70],\n        [0.30, 0.50, 0.55, 0.30],\n        [0.15, 0.25, 0.20, 0.85],\n        [0.50, 0.80, 0.65, 0.05],\n        [0.18, 0.35, 0.22, 0.75],\n        [0.40, 0.55, 0.45, 0.40],\n        [0.22, 0.32, 0.28, 0.60],\n        [0.28, 0.45, 0.35, 0.50],\n        [0.38, 0.65, 0.58, 0.25],\n        [0.12, 0.18, 0.12, 0.95]\n    ])\n    \n    y = np.array([1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1])\n\n    # Test suite from the problem statement.\n    test_cases = [\n        (0.1, np.array([0.27, 0.40, 0.33, 0.55])),\n        (10.0, np.array([0.27, 0.40, 0.33, 0.55])),\n        (0.0, np.array([0.55, 0.90, 0.70, 0.05])),\n        (0.01, np.array([0.08, 0.15, 0.10, 0.98]))\n    ]\n    \n    # Augment the feature matrix Z with an intercept column.\n    n, d = Z.shape\n    X = np.hstack([np.ones((n, 1)), Z])\n\n    def objective_function(theta, X_mat, y_vec, lambda_val):\n        \"\"\"\n        Computes the negative penalized log-likelihood (objective to minimize).\n        A small epsilon is used for numerical stability of the logarithm,\n        although L-BFGS-B with a good gradient is generally robust.\n        \"\"\"\n        m = X_mat.shape[0]\n        u = X_mat @ theta\n        p = expit(u)\n        \n        # To prevent log(0), clip probabilities to be within a safe range.\n        eps = 1e-15\n        p = np.clip(p, eps, 1 - eps)\n        \n        log_likelihood = np.sum(y_vec * np.log(p) + (1 - y_vec) * np.log(1 - p))\n        \n        # L2 penalty term (Ridge), excluding the intercept.\n        beta = theta[1:]\n        penalty = (lambda_val / 2) * np.sum(beta**2)\n        \n        return -(log_likelihood - penalty)\n\n    def gradient_function(theta, X_mat, y_vec, lambda_val):\n        \"\"\"\n        Computes the gradient of the negative penalized log-likelihood.\n        \"\"\"\n        m = X_mat.shape[0]\n        u = X_mat @ theta\n        p = expit(u)\n        \n        error = p - y_vec\n        grad_log_likelihood = X_mat.T @ error\n        \n        # Gradient of the L2 penalty term.\n        grad_penalty = lambda_val * theta\n        grad_penalty[0] = 0  # No penalty on the intercept.\n        \n        return grad_log_likelihood + grad_penalty\n\n    results = []\n    for lambda_val, z_star in test_cases:\n        # Initial guess for the parameters.\n        initial_theta = np.zeros(d + 1)\n\n        # Perform the optimization to find the best parameters.\n        opt_result = minimize(\n            fun=objective_function,\n            x0=initial_theta,\n            args=(X, y, lambda_val),\n            method='L-BFGS-B',\n            jac=gradient_function\n        )\n        theta_star = opt_result.x\n\n        # Augment the test vector and compute the prediction.\n        x_star = np.hstack([1, z_star])\n        p_star = expit(x_star @ theta_star)\n        \n        results.append(p_star)\n\n    # Format output according to the problem requirements.\n    formatted_results = [\"{:.6f}\".format(res) for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "2407571"}, {"introduction": "现实世界中的许多金融和经济问题，如欺诈检测或市场崩盘预测，都涉及“稀有事件”，这导致了数据类别极度不平衡。本次练习[@problem_id:2407556]将带你应对这一挑战，通过预测体育联赛中的冠军归属（一个典型的稀有事件）来学习处理不平衡数据。你不仅会实现处理类别权重的逻辑斯谛回归，还将掌握一系列在类别不平衡时至关重要的评估指标，如精确率（Precision）、召回率（Recall）和 F1 分数。", "problem": "要求您编写一个完整、可运行的程序，用于训练和评估一个逻辑回归分类器，以解决一个稀有事件预测问题。该问题的框架如下。一个球队赛季由一个标准化的赛季统计数据向量表示，包括：胜率、场均净胜分、薪资与联盟中位数的比率以及一个季后赛经验指数。目标变量是该球队在该赛季是否赢得总冠军。赢得总冠军是一个稀有事件。模型是带有截距项的逻辑回归，并为正类提供一个可选的类别权重，以解决类别不平衡问题。评估的重点是类别不平衡指标和阈值效应。\n\n从以下基本原理开始：\n- 每个观测值的结果是一个伯努利随机变量，其成功概率由一个逻辑连接函数参数化。设 $y_i \\in \\{0,1\\}$ 为二元标签，$x_i \\in \\mathbb{R}^d$ 为特征向量。\n- 逻辑连接函数设定 $p_i = \\Pr(y_i = 1 \\mid x_i) = \\sigma(z_i)$，其中 $z_i = \\beta_0 + x_i^\\top \\beta$，而 $\\sigma(u) = \\dfrac{1}{1 + e^{-u}}$。\n- （可选）加权的、带 $\\ell_2$ 正则化（岭回归）的负对数似然函数需要被最小化。如果 $w_i  0$ 是观测权重，$\\lambda \\ge 0$ 是正则化强度，那么目标函数是\n$$\n\\mathcal{L}(\\beta_0, \\beta) = -\\sum_{i=1}^{n} w_i \\left[ y_i \\log p_i + (1 - y_i) \\log(1 - p_i) \\right] + \\frac{\\lambda}{2} \\|\\beta\\|_2^2,\n$$\n其中截距项 $\\beta_0$ 不受惩罚。\n\n您的程序必须：\n- 通过在全量数据上（无小批量）使用 Newton–Raphson 方法最小化上述目标函数来实现逻辑回归训练。使用精确的梯度和 Hessian 矩阵，为保证稳定性采用回溯线搜索，并且不对截距项进行惩罚。通过为 $y_i = 1$ 设置 $w_i = w_+$ 和为 $y_i = 0$ 设置 $w_i = 1$ 来允许一个标量正类权重 $w_+  1$；非加权训练使用 $w_+ = 1$。\n- 训练后，在测试集上计算预测概率 $\\hat{p}_i = \\sigma(\\beta_0 + x_i^\\top \\beta)$，并使用一个固定阈值 $\\tau \\in (0,1)$ 将它们转换为类别预测：如果 $\\hat{p}_i \\ge \\tau$ 则预测 $\\hat{y}_i = 1$，否则预测 $\\hat{y}_i = 0$。\n- 在测试集上计算以下评估指标，其中 $\\mathrm{TP}$、$\\mathrm{FP}$、$\\mathrm{TN}$、$\\mathrm{FN}$ 分别是真阳性、假阳性、真阴性和假阴性：\n    - 精确率 (Precision)：$\\mathrm{Prec} = \\dfrac{\\mathrm{TP}}{\\mathrm{TP} + \\mathrm{FP}}$，如果分母非零，否则定义为 $0$。\n    - 召回率 (Recall)（真阳性率）：$\\mathrm{Rec} = \\dfrac{\\mathrm{TP}}{\\mathrm{TP} + \\mathrm{FN}}$，如果分母非零，否则定义为 $0$。\n    - $F_1$ 分数：$F_1 = \\dfrac{2 \\cdot \\mathrm{Prec} \\cdot \\mathrm{Rec}}{\\mathrm{Prec} + \\mathrm{Rec}}$，如果分母非零，否则定义为 $0$。\n    - 平衡准确率 (Balanced accuracy)：$\\mathrm{BAcc} = \\dfrac{1}{2}\\left( \\dfrac{\\mathrm{TP}}{\\mathrm{TP} + \\mathrm{FN}} + \\dfrac{\\mathrm{TN}}{\\mathrm{TN} + \\mathrm{FP}} \\right)$，如果任何一个分数的分母为零，则该分数定义为 $0$。\n    - 马修斯相关系数 (Matthews Correlation Coefficient, MCC)：$\\mathrm{MCC} = \\dfrac{\\mathrm{TP}\\cdot \\mathrm{TN} - \\mathrm{FP}\\cdot \\mathrm{FN}}{\\sqrt{(\\mathrm{TP} + \\mathrm{FP})(\\mathrm{TP} + \\mathrm{FN})(\\mathrm{TN} + \\mathrm{FP})(\\mathrm{TN} + \\mathrm{FN})}}$，如果分母为零，则定义为 $0$。\n    - 精确率-召回率曲线下面积 (AUC-PR)：通过将测试实例按 $\\hat{p}_i$ 降序排序，并在累积阈值处评估精确率和召回率来构建精确率-召回率曲线；使用梯形法则在召回率上计算面积。\n\n训练和测试数据在下方以显式数值数组的形式提供。每个样本是一个包含四个特征的向量 $\\left[x_1, x_2, x_3, x_4\\right]$，分别对应胜率、场均净胜分、薪资与联盟中位数的比率和季后赛经验指数。标签 $y$ 表示是否赢得总冠军。\n\n请完全按照给定的数组使用这些特征和标签。\n\n- 训练集 $\\left(X_{\\text{train}}, y_{\\text{train}}\\right)$，其中 $n = 16$：\n    - $[0.55, -1.0, 0.90, 0.20] \\rightarrow 0$\n    - $[0.60, 1.50, 1.10, 0.30] \\rightarrow 0$\n    - $[0.48, -3.50, 0.85, 0.10] \\rightarrow 0$\n    - $[0.70, 5.00, 1.20, 0.50] \\rightarrow 0$\n    - $[0.75, 6.00, 1.40, 0.60] \\rightarrow 1$\n    - $[0.65, 3.00, 1.00, 0.40] \\rightarrow 0$\n    - $[0.80, 8.50, 1.50, 0.70] \\rightarrow 1$\n    - $[0.42, -6.00, 0.75, 0.10] \\rightarrow 0$\n    - $[0.58, 0.50, 0.95, 0.20] \\rightarrow 0$\n    - $[0.62, 2.00, 1.05, 0.35] \\rightarrow 0$\n    - $[0.68, 4.00, 1.10, 0.45] \\rightarrow 0$\n    - $[0.85, 10.00, 1.60, 0.80] \\rightarrow 1$\n    - $[0.50, -2.00, 0.90, 0.15] \\rightarrow 0$\n    - $[0.73, 5.50, 1.30, 0.55] \\rightarrow 0$\n    - $[0.66, 3.50, 1.15, 0.45] \\rightarrow 0$\n    - $[0.77, 7.00, 1.35, 0.60] \\rightarrow 0$\n\n- 测试集 A $\\left(X_{\\text{A}}, y_{\\text{A}}\\right)$，其中 $m = 6$：\n    - $[0.74, 6.00, 1.25, 0.50] \\rightarrow 1$\n    - $[0.57, 0.00, 1.00, 0.20] \\rightarrow 0$\n    - $[0.82, 9.00, 1.55, 0.75] \\rightarrow 1$\n    - $[0.45, -4.50, 0.80, 0.05] \\rightarrow 0$\n    - $[0.63, 2.50, 1.10, 0.40] \\rightarrow 0$\n    - $[0.79, 7.50, 1.45, 0.65] \\rightarrow 1$\n\n- 测试集 B $\\left(X_{\\text{B}}, y_{\\text{B}}\\right)$，其中 $m = 8$：\n    - $[0.61, 1.50, 1.05, 0.30] \\rightarrow 0$\n    - $[0.52, -1.50, 0.90, 0.12] \\rightarrow 0$\n    - $[0.76, 6.50, 1.40, 0.60] \\rightarrow 1$\n    - $[0.59, 0.50, 0.95, 0.25] \\rightarrow 0$\n    - $[0.47, -4.00, 0.82, 0.08] \\rightarrow 0$\n    - $[0.66, 3.00, 1.12, 0.40] \\rightarrow 0$\n    - $[0.64, 2.00, 1.08, 0.38] \\rightarrow 0$\n    - $[0.54, -0.50, 0.97, 0.20] \\rightarrow 0$\n\n测试套件。所有情况均在 $\\left(X_{\\text{train}}, y_{\\text{train}}\\right)$ 上进行训练，但改变阈值和正类权重以测试不同方面：\n- 情况 1（理想路径）：非加权训练，$\\lambda = 1.0$，$\\tau = 0.5$，最大迭代次数 $= 100$，容差 $= 10^{-9}$；在测试集 A 上评估。\n- 情况 2（阈值边界）：非加权训练，$\\lambda = 1.0$，$\\tau = 0.9$，最大迭代次数 $= 100$，容差 $= 10^{-9}$；在测试集 A 上评估。\n- 情况 3（类别不平衡缓解）：加权训练，正类权重 $w_+ = 3.0$，$\\lambda = 1.0$，$\\tau = 0.5$，最大迭代次数 $= 100$，容差 $= 10^{-9}$；在测试集 B 上评估。\n\n最终输出要求：\n- 对于每种情况，按此确切顺序将指标报告为列表：$[\\mathrm{Prec}, \\mathrm{Rec}, F_1, \\mathrm{BAcc}, \\mathrm{MCC}, \\mathrm{AUC\\mbox{-}PR}]$，每个值四舍五入到 6 位小数。\n- 您的程序应生成单行输出，其中包含三个情况级别列表的逗号分隔列表形式的结果，并用方括号括起来，例如：$[[a_1,\\dots,a_6],[b_1,\\dots,b_6],[c_1,\\dots,c_6]]$，其中每个 $a_j$、$b_j$、$c_j$ 都是四舍五入到 6 位小数的浮点数。\n\n不允许用户输入或外部文件；程序必须完全自包含且是确定性的。本问题不涉及角度。不涉及物理单位。所有答案均为浮点数，必须按规定进行四舍五入。", "solution": "所提出的问题是计算统计学中一个明确定义的任务：为一个二元分类问题，特别是稀有事件场景，实现、训练和评估一个逻辑回归分类器。该问题有科学依据、数学上完备且算法上明确。因此，它是有效的，我们从第一性原理出发构建一个严谨的解决方案。\n\n问题的核心是找到一个逻辑回归模型的参数，以最小化一个指定的目标函数。该模型通过逻辑函数 $p_i = \\sigma(\\beta_0 + x_i^\\top \\beta)$ 预测特征向量为 $x_i \\in \\mathbb{R}^d$ 的观测值的正向结果（$y_i=1$）的概率，其中 $\\sigma(u) = (1 + e^{-u})^{-1}$。为方便表示，我们将特征向量增广为 $\\tilde{x}_i = [1, x_i^\\top]^\\top$，将参数向量增广为 $\\tilde{\\beta} = [\\beta_0, \\beta^\\top]^\\top$，因此线性部分为 $z_i = \\tilde{x}_i^\\top \\tilde{\\beta}$。\n\n需要最小化的目标函数是加权的负对数似然函数，并对特征系数 $\\beta$（但不包括截距项 $\\beta_0$）施加 $\\ell_2$ 惩罚：\n$$\n\\mathcal{L}(\\tilde{\\beta}) = -\\sum_{i=1}^{n} w_i \\left[ y_i \\log p_i + (1 - y_i) \\log(1 - p_i) \\right] + \\frac{\\lambda}{2} \\beta^\\top \\beta\n$$\n在这里，$n$ 是训练样本的数量，$w_i$ 是样本权重，$\\lambda$ 是正则化强度。如果 $y_i=1$，权重 $w_i$ 设置为指定的正类权重 $w_+$；如果 $y_i=0$，则设置为 $1$。\n\n优化过程使用 Newton-Raphson 方法，这是一种二阶迭代算法。每次迭代根据以下规则更新参数向量 $\\tilde{\\beta}$：\n$$\n\\tilde{\\beta}^{(k+1)} = \\tilde{\\beta}^{(k)} + \\alpha \\Delta \\tilde{\\beta}^{(k)}\n$$\n其中 $\\alpha$ 是步长，$\\Delta \\tilde{\\beta}^{(k)}$ 是牛顿步，通过求解以下线性系统得到：\n$$\n\\mathbf{H}(\\tilde{\\beta}^{(k)}) \\Delta \\tilde{\\beta}^{(k)} = -\\nabla \\mathcal{L}(\\tilde{\\beta}^{(k)})\n$$\n在此方程中，$\\nabla \\mathcal{L}$ 是目标函数 $\\mathcal{L}$ 的梯度（一阶偏导数向量），$\\mathbf{H}$ 是其 Hessian 矩阵（二阶偏导数矩阵）。\n\n梯度向量 $\\nabla \\mathcal{L}(\\tilde{\\beta})$ 推导如下：\n$$\n\\nabla \\mathcal{L}(\\tilde{\\beta}) = \\tilde{X}^\\top W (p - y) + \\lambda \\Lambda \\tilde{\\beta}\n$$\n此处，$\\tilde{X}$ 是一个 $n \\times (d+1)$ 的增广设计矩阵，$W$ 是一个包含样本权重 $w_i$ 的 $n \\times n$ 对角矩阵，$p$ 是预测概率向量，$y$ 是真实标签向量，$\\Lambda$ 是一个 $(d+1) \\times (d+1)$ 的对角矩阵，其中 $\\Lambda_{00}=0$ 且对于 $j0$ 有 $\\Lambda_{jj}=1$，这确保了截距项不受惩罚。\n\nHessian 矩阵 $\\mathbf{H}(\\tilde{\\beta})$ 推导如下：\n$$\n\\mathbf{H}(\\tilde{\\beta}) = \\tilde{X}^\\top S \\tilde{X} + \\lambda \\Lambda\n$$\n其中 $S$ 是一个 $n \\times n$ 的对角矩阵，其元素为 $S_{ii} = w_i p_i (1 - p_i)$。由于此 Hessian 矩阵是对称的，并且当 $\\lambda  0$ 时是正定的，因此求解牛顿步的线性系统有唯一解。\n\n为确保稳健的收敛，步长 $\\alpha$ 使用回溯线搜索来确定。从 $\\alpha=1$ 开始，步长以因子 $\\rho$（例如 $0.5$）迭代减小，直到满足 Armijo-Goldstein 条件：\n$$\n\\mathcal{L}(\\tilde{\\beta}^{(k)} + \\alpha \\Delta \\tilde{\\beta}^{(k)}) \\le \\mathcal{L}(\\tilde{\\beta}^{(k)}) + c_1 \\alpha (\\nabla \\mathcal{L}(\\tilde{\\beta}^{(k)}))^\\top \\Delta \\tilde{\\beta}^{(k)}\n$$\n其中 $c_1$ 是一个很小的常数（例如 $10^{-4}$）。\n\n训练后，在测试集上评估模型的性能。计算预测概率 $\\hat{p}_i$ 并使用指定的阈值 $\\tau$ 将其转换为类别标签 $\\hat{y}_i \\in \\{0,1\\}$。评估依赖于几个指标，这些指标根据真阳性（$\\mathrm{TP}$）、假阳性（$\\mathrm{FP}$）、真阴性（$\\mathrm{TN}$）和假阴性（$\\mathrm{FN}$）定义：\n- **精确率 (Precision)**：$\\mathrm{Prec} = \\mathrm{TP} / (\\mathrm{TP} + \\mathrm{FP})$\n- **召回率 (Recall)**：$\\mathrm{Rec} = \\mathrm{TP} / (\\mathrm{TP} + \\mathrm{FN})$\n- **$F_1$ 分数**：$F_1 = 2 \\cdot (\\mathrm{Prec} \\cdot \\mathrm{Rec}) / (\\mathrm{Prec} + \\mathrm{Rec})$\n- **平衡准确率 (Balanced Accuracy)**：$\\mathrm{BAcc} = 0.5 \\cdot (\\mathrm{Rec} + \\mathrm{TN} / (\\mathrm{TN} + \\mathrm{FP}))$\n- **马修斯相关系数 (MCC)**：$\\mathrm{MCC} = (\\mathrm{TP}\\cdot \\mathrm{TN} - \\mathrm{FP}\\cdot \\mathrm{FN}) / \\sqrt{(\\mathrm{TP} + \\mathrm{FP})(\\mathrm{TP} + \\mathrm{FN})(\\mathrm{TN} + \\mathrm{FP})(\\mathrm{TN} + \\mathrm{FN})}$\n如果分母为零，每个指标都定义为 $0$。\n\n**精确率-召回率曲线下面积 (AUC-PR)** 通过数值积分计算。测试实例按预测概率降序排序。通过依次考虑已排序实例的不断增大的子集，生成一系列精确率和召回率值。然后使用梯形法则计算所得曲线下的面积。非递减的召回率值作为积分点。曲线锚定在起始点 $(\\text{recall}=0, \\text{precision}=1)$，以正确计算召回率范围开始部分的面积。\n\n所提供的实现封装了这整个过程。它定义了必要的数据结构，实现了带有回溯线搜索的 Newton-Raphson 训练器，并为问题陈述中指定的三个测试用例计算了全套评估指标。", "answer": "```python\nimport numpy as np\nfrom scipy.linalg import solve as solve_linear_system\n\ndef solve():\n    \"\"\"\n    Main function to run the logistic regression experiments as specified.\n    \"\"\"\n    \n    # --- Data Definition ---\n    X_train = np.array([\n        [0.55, -1.0, 0.90, 0.20], [0.60, 1.50, 1.10, 0.30], [0.48, -3.50, 0.85, 0.10],\n        [0.70, 5.00, 1.20, 0.50], [0.75, 6.00, 1.40, 0.60], [0.65, 3.00, 1.00, 0.40],\n        [0.80, 8.50, 1.50, 0.70], [0.42, -6.00, 0.75, 0.10], [0.58, 0.50, 0.95, 0.20],\n        [0.62, 2.00, 1.05, 0.35], [0.68, 4.00, 1.10, 0.45], [0.85, 10.00, 1.60, 0.80],\n        [0.50, -2.00, 0.90, 0.15], [0.73, 5.50, 1.30, 0.55], [0.66, 3.50, 1.15, 0.45],\n        [0.77, 7.00, 1.35, 0.60]\n    ])\n    y_train = np.array([0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0])\n\n    X_A = np.array([\n        [0.74, 6.00, 1.25, 0.50], [0.57, 0.00, 1.00, 0.20], [0.82, 9.00, 1.55, 0.75],\n        [0.45, -4.50, 0.80, 0.05], [0.63, 2.50, 1.10, 0.40], [0.79, 7.50, 1.45, 0.65]\n    ])\n    y_A = np.array([1, 0, 1, 0, 0, 1])\n\n    X_B = np.array([\n        [0.61, 1.50, 1.05, 0.30], [0.52, -1.50, 0.90, 0.12], [0.76, 6.50, 1.40, 0.60],\n        [0.59, 0.50, 0.95, 0.25], [0.47, -4.00, 0.82, 0.08], [0.66, 3.00, 1.12, 0.40],\n        [0.64, 2.00, 1.08, 0.38], [0.54, -0.50, 0.97, 0.20]\n    ])\n    y_B = np.array([0, 0, 1, 0, 0, 0, 0, 0])\n    \n    test_cases = [\n        # (lambda_reg, w_plus, tau, X_test, y_test, max_iter, tol)\n        (1.0, 1.0, 0.5, X_A, y_A, 100, 1e-9),\n        (1.0, 1.0, 0.9, X_A, y_A, 100, 1e-9),\n        (1.0, 3.0, 0.5, X_B, y_B, 100, 1e-9)\n    ]\n    \n    results = []\n    \n    for case in test_cases:\n        lambda_reg, w_plus, tau, X_test, y_test, max_iter, tol = case\n        \n        # Train model\n        beta = _logistic_regression_train(X_train, y_train, lambda_reg, w_plus, max_iter, tol)\n        \n        # Get predictions\n        y_proba = _predict_proba(X_test, beta)\n        y_pred = (y_proba >= tau).astype(int)\n        \n        # Evaluate metrics\n        metrics = _evaluate_metrics(y_test, y_pred, y_proba)\n        results.append(metrics)\n        \n    # Format and print the final output\n    formatted_results = [f\"[{','.join(map(str, res))}]\" for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\n\ndef _sigmoid(z):\n    # Clip to avoid overflow/underflow in exp\n    z_clipped = np.clip(z, -500, 500)\n    p = 1 / (1 + np.exp(-z_clipped))\n    # Clip probabilities to avoid log(0)\n    return np.clip(p, 1e-15, 1 - 1e-15)\n\ndef _calculate_cost(X_aug, y, beta, weights, lambda_reg):\n    p = _sigmoid(X_aug @ beta)\n    log_likelihood = -np.sum(weights * (y * np.log(p) + (1 - y) * np.log(1 - p)))\n    reg_term = (lambda_reg / 2) * np.dot(beta[1:], beta[1:])\n    return log_likelihood + reg_term\n\ndef _logistic_regression_train(X, y, lambda_reg, w_plus, max_iter, tol):\n    n_samples, n_features = X.shape\n    X_aug = np.c_[np.ones(n_samples), X]\n    beta = np.zeros(n_features + 1)\n    \n    weights = np.ones(n_samples)\n    weights[y == 1] = w_plus\n\n    lambda_vec = np.full(n_features + 1, lambda_reg)\n    lambda_vec[0] = 0.0\n\n    # Newton-Raphson iterations\n    for _ in range(max_iter):\n        z = X_aug @ beta\n        p = _sigmoid(z)\n        \n        error = p - y\n        grad = X_aug.T @ (weights * error) + lambda_vec * beta\n        \n        S_diag = weights * p * (1 - p)\n        H = (X_aug.T * S_diag) @ X_aug + np.diag(lambda_vec)\n        \n        # Solve H * delta_beta = -grad\n        # Use scipy.linalg.solve for stability and performance\n        search_dir = solve_linear_system(H, -grad, assume_a='sym')\n        \n        # Backtracking line search\n        alpha = 1.0\n        c1 = 1e-4\n        rho = 0.5\n        cost_current = _calculate_cost(X_aug, y, beta, weights, lambda_reg)\n        grad_dot_dir = np.dot(grad, search_dir)\n        \n        while True:\n            beta_new = beta + alpha * search_dir\n            cost_new = _calculate_cost(X_aug, y, beta_new, weights, lambda_reg)\n            if cost_new = cost_current + c1 * alpha * grad_dot_dir or alpha  1e-9:\n                break\n            alpha *= rho\n        \n        step = alpha * search_dir\n        beta += step\n        \n        if np.linalg.norm(step)  tol:\n            break\n            \n    return beta\n\ndef _predict_proba(X, beta):\n    X_aug = np.c_[np.ones(X.shape[0]), X]\n    return _sigmoid(X_aug @ beta)\n\ndef _evaluate_metrics(y_true, y_pred, y_proba):\n    tp = np.sum((y_true == 1)  (y_pred == 1))\n    tn = np.sum((y_true == 0)  (y_pred == 0))\n    fp = np.sum((y_true == 0)  (y_pred == 1))\n    fn = np.sum((y_true == 1)  (y_pred == 0))\n    \n    precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n    recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n    \n    f1_score = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n    \n    tpr = recall\n    tnr = tn / (tn + fp) if (tn + fp) > 0 else 0.0\n    balanced_accuracy = 0.5 * (tpr + tnr)\n\n    mcc_denom_sq = (tp + fp) * (tp + fn) * (tn + fp) * (tn + fn)\n    mcc = (tp * tn - fp * fn) / np.sqrt(mcc_denom_sq) if mcc_denom_sq > 0 else 0.0\n        \n    # AUC-PR calculation\n    sort_indices = np.argsort(y_proba)[::-1]\n    y_true_sorted = y_true[sort_indices]\n    \n    tps = np.cumsum(y_true_sorted)\n    fps = np.cumsum(1 - y_true_sorted)\n    \n    total_positives = np.sum(y_true)\n    if total_positives == 0:\n        auc_pr = 0.0\n    else:\n        recalls = tps / total_positives\n        precisions = tps / (tps + fps)\n        \n        # Keep only points where recall changes (at positive instances)\n        is_positive = (y_true_sorted == 1)\n        recall_points = recalls[is_positive]\n        precision_points = precisions[is_positive]\n\n        # Prepend starting point for trapezoidal rule\n        recall_aug = np.concatenate([[0.], recall_points])\n        precision_aug = np.concatenate([[1.], precision_points])\n        \n        auc_pr = np.trapz(precision_aug, recall_aug)\n\n    metrics = [precision, recall, f1_score, balanced_accuracy, mcc, auc_pr]\n    return [round(m, 6) for m in metrics]\n\nif __name__ == '__main__':\n    solve()\n```", "id": "2407556"}]}