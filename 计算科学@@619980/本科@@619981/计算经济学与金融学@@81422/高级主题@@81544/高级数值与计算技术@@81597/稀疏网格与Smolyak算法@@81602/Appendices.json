{"hands_on_practices": [{"introduction": "第一个实践练习探讨了稀疏网格背后的基本动机：维度灾难。通过为五种资产的一篮子期权定价，你将直接比较计算成本高昂的完全张量积方法与更高效的 Smolyak 稀疏网格方法，从而具体理解稀疏网格如何为高维问题提供切实可行的解决方案。[@problem_id:2396782]", "problem": "考虑在风险中性 Black–Scholes 框架下，一份关于 $d$ 种风险资产构成的资产篮子的欧式看涨期权。令 $S_i(0)$ 表示资产 $i$ 的初始价格，$r$ 为恒定无风险利率，$\\sigma_i$ 为资产 $i$ 的波动率，$T$ 为到期时间。在风险中性测度下，资产 $i$ 的到期价格由下式给出：\n$$\nS_i(T) = S_i(0)\\,\\exp\\!\\Big( \\big(r - \\tfrac{1}{2}\\sigma_i^2\\big)T + \\sigma_i \\sqrt{T}\\, Z_i \\Big),\n$$\n其中 $Z_i$ 是独立的标准正态随机变量。该篮子看涨期权的收益为\n$$\n\\Pi = \\max\\!\\Big(\\frac{1}{d}\\sum_{i=1}^{d} S_i(T) - K,\\, 0\\Big),\n$$\n其中行权价为 $K$。在时间 $0$ 的无套利价格为\n$$\nV_0 = e^{-rT}\\,\\mathbb{E}\\big[\\Pi\\big].\n$$\n\n您的任务是使用高斯求积法对 $d=5$ 的情况下的 $V_0$ 进行近似，并通过比较全张量积高斯求积与从相同一维规则构建的 Smolyak 稀疏网格，来数值化地展示维度灾难。\n\n您必须从第一性原理出发，将标准正态分布下的 $d$ 维期望转化为高斯-埃尔米特 (Gaussian–Hermite) 求积。回顾一维情况下 $n$ 阶的高斯-埃尔米特求积法则：\n$$\n\\int_{-\\infty}^{\\infty} e^{-x^2} f(x)\\,dx \\approx \\sum_{j=1}^{n} w_j f(x_j),\n$$\n其中 $x_j$ 是节点，$w_j$ 是权重。证明对于一个标准正态随机变量 $Z$，\n$$\n\\mathbb{E}[g(Z)] = \\frac{1}{\\sqrt{\\pi}}\\int_{-\\infty}^{\\infty} e^{-x^2}\\, g\\!\\big(\\sqrt{2}\\,x\\big)\\,dx,\n$$\n并将此推广到 $d$ 个独立标准正态变量，以证明使用一维高斯-埃尔米特法则的 $d$ 维张量积是合理的。利用上述原理，构建：\n\n- 一个每维有 $n$ 个节点（总节点数为 $n^d$）的 $d$ 维张量积高斯-埃尔米特求积。\n- 一个各向同性水平为 $L$ 的 Smolyak 稀疏网格，它由相同的一维高斯-埃尔米特族 $\\{Q_\\ell\\}_{\\ell \\ge 1}$ 构建，其中一维阶数为 $m(\\ell) = 2\\ell - 1$ 。使用带有如下指标集的 Smolyak 组合公式：\n$$\n\\mathcal{I}(L,d) = \\big\\{\\boldsymbol{\\ell}\\in \\mathbb{N}^d : 1 \\le \\ell_k, \\ \\ |\\boldsymbol{\\ell}|_1 \\le L + d - 1 \\big\\},\n$$\n以及如下系数：\n$$\nc(\\boldsymbol{\\ell}) = (-1)^{L + d - 1 - |\\boldsymbol{\\ell}|_1}\\binom{d-1}{L + d - 1 - |\\boldsymbol{\\ell}|_1}.\n$$\n此处，$Q_{\\ell}$ 表示阶数为 $m(\\ell)$ 的一维高斯-埃尔米特法则，而 $d$ 维算子是张量积 $\\bigotimes_{k=1}^d Q_{\\ell_k}$。\n\n基于上述基础，设计一个算法，该算法应能：\n- 通过在每个维度上进行变量替换 $z = \\sqrt{2}\\,x$，将期望映射到带高斯-埃尔米特权重的积分。\n- 对 $d$ 维情况正确应用归一化因子 $\\pi^{-d/2}$。\n- 构建张量网格和 Smolyak 稀疏网格，并在所有需要的节点上计算折现后的收益值。\n- 统计每种方法的函数求值次数，作为计算成本的代理指标。\n\n测试套件。实现以下两个计算测试用例和一个比较任务：\n\n- 情况 A (理想路径, $d=5$):\n  - 参数：$S_0 = (100,\\, 90,\\, 110,\\, 95,\\, 105)$，$\\sigma = (0.2,\\, 0.25,\\, 0.15,\\, 0.3,\\, 0.18)$，$r = 0.02$，$T = 1.0$，$K = 100$。\n  - 每维 $n=3$ 个节点的张量积高斯-埃尔米特求积。\n  - 各向同性水平 $L=3$ 且一维阶数 $m(\\ell) = 2\\ell - 1$ 的 Smolyak 稀疏网格。\n  - 一个通过每维 $n_{\\text{ref}}=9$ 个节点的张量积高斯-埃尔米特求积计算的高精度参考值。\n  - 需要计算的输出：\n    - 张量积近似值相对于参考值的绝对误差（浮点数）。\n    - 稀疏网格近似值相对于参考值的绝对误差（浮点数）。\n    - 张量积的节点数（整数）。\n    - 稀疏网格的节点数（整数），取 Smolyak 组合中所有组成项的张量积大小之和。\n\n- 情况 B (零波动率边界条件, $d=5$):\n  - 参数：与情况 A 中的 $S_0$, $r$, $T$, $K$ 相同，但 $\\sigma = (0,\\,0,\\,0,\\,0,\\,0)$。\n  - 确切价格由以下确定性值给出：\n    $$\n    V_0^{\\text{det}} = e^{-rT}\\,\\max\\!\\Big(\\frac{1}{d}\\sum_{i=1}^{d} S_i(0)\\,e^{rT} - K,\\, 0\\Big) = \\max\\!\\Big(\\frac{1}{d}\\sum_{i=1}^{d} S_i(0) - K e^{-rT},\\, 0\\Big).\n    $$\n  - 验证 $n=3$ 的张量积求积和 $L=3$ 的稀疏网格是否都在 $10^{-10}$ 的绝对容差内与 $V_0^{\\text{det}}$ 匹配。输出一个布尔值，表示两者是否都通过测试。\n\n- 效率比较：\n  - 对于情况 A，输出一个布尔值，表示稀疏网格是否在节点数严格少于张量积的情况下，达到了小于或等于张量积的绝对误差。\n\n最终输出格式。您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表。列表条目必须按顺序为：\n- 情况 A 的张量积绝对误差（浮点数）。\n- 情况 A 的稀疏网格绝对误差（浮点数）。\n- 情况 A 的张量积节点数（整数）。\n- 情况 A 的稀疏网格节点数（整数）。\n- 情况 B 的边界条件检查（布尔值）。\n- 情况 A 的效率比较检查（布尔值）。\n\n例如，输出格式必须为：\n$$\n[\\text{err\\_tensor},\\text{err\\_sparse},\\text{nodes\\_tensor},\\text{nodes\\_sparse},\\text{boundary\\_ok},\\text{efficiency\\_ok}],\n$$\n其中两个误差为十进制数，两个节点数为整数，最后两个条目为布尔值。不应打印任何额外文本。", "solution": "我们从欧式衍生物的风险中性定价原理开始：在时间 $T$ 的收益 $\\Pi$ 在时间 $0$ 的价格 $V_0$ 由 $V_0 = e^{-rT}\\,\\mathbb{E}[\\Pi]$ 给出，其中期望是在风险中性测度下计算的。在 Black–Scholes 框架中，对于独立的标准正态变量 $Z_i \\sim \\mathcal{N}(0,1)$，到期资产价格为\n$$\nS_i(T) = S_i(0)\\,\\exp\\!\\Big( \\big(r - \\tfrac{1}{2}\\sigma_i^2\\big)T + \\sigma_i \\sqrt{T}\\, Z_i \\Big),\n$$\n篮子看涨期权的收益为\n$$\n\\Pi = \\max\\!\\Big(\\frac{1}{d}\\sum_{i=1}^{d} S_i(T) - K,\\, 0\\Big).\n$$\n因此，\n$$\nV_0 = e^{-rT}\\,\\mathbb{E}\\left[\\max\\!\\Big(\\frac{1}{d}\\sum_{i=1}^{d} S_i(0)\\,\\exp\\!\\big( \\big(r - \\tfrac{1}{2}\\sigma_i^2\\big)T + \\sigma_i \\sqrt{T}\\, Z_i \\big) - K,\\, 0\\Big)\\right].\n$$\n\n为了与高斯-埃尔米特求积关联起来，我们将标准正态定律下的期望转换为带有高斯-埃尔米特权重的积分。对于单个标准正态随机变量 $Z \\sim \\mathcal{N}(0,1)$ 和一个合适的测试函数 $g$，我们有\n$$\n\\mathbb{E}[g(Z)] = \\int_{-\\infty}^{\\infty} \\frac{1}{\\sqrt{2\\pi}} e^{-z^2/2} g(z)\\,dz.\n$$\n通过变量替换 $z = \\sqrt{2}\\,x$（因此 $dz = \\sqrt{2}\\,dx$），可以得到\n$$\n\\mathbb{E}[g(Z)] = \\frac{1}{\\sqrt{2\\pi}} \\int_{-\\infty}^{\\infty} e^{-(\\sqrt{2}x)^2/2} g(\\sqrt{2}x)\\,\\sqrt{2}\\,dx = \\frac{1}{\\sqrt{\\pi}}\\int_{-\\infty}^{\\infty} e^{-x^2} g(\\sqrt{2}\\,x)\\,dx.\n$$\n在 $d$ 维情况下，对于独立标准正态变量 $\\boldsymbol{Z} = (Z_1,\\dots,Z_d)$，其联合密度可以分解，通过分量式地应用变量替换，我们得到\n$$\n\\mathbb{E}[g(\\boldsymbol{Z})] = \\frac{1}{\\pi^{d/2}} \\int_{\\mathbb{R}^d} e^{-\\|\\boldsymbol{x}\\|_2^2}\\, g(\\sqrt{2}\\,\\boldsymbol{x})\\, d\\boldsymbol{x}.\n$$\n因此，$d$ 维高斯-埃尔米特张量积求积得出\n$$\n\\mathbb{E}[g(\\boldsymbol{Z})] \\approx \\frac{1}{\\pi^{d/2}} \\sum_{j_1=1}^{n}\\cdots \\sum_{j_d=1}^{n} \\Big(\\prod_{k=1}^{d} w_{j_k}\\Big)\\, g\\!\\Big(\\sqrt{2}\\,x_{j_1},\\dots,\\sqrt{2}\\,x_{j_d}\\Big),\n$$\n其中 $(x_{j},w_{j})$ 是与权重 $e^{-x^2}$ 相关联的 $n$ 阶一维高斯-埃尔米特节点和权重。\n\n将此应用于篮子收益，我们定义\n$$\ng(\\boldsymbol{z}) = \\max\\!\\Big(\\frac{1}{d}\\sum_{i=1}^{d} S_i(0)\\,\\exp\\!\\big( \\big(r - \\tfrac{1}{2}\\sigma_i^2\\big)T + \\sigma_i \\sqrt{T}\\, z_i \\big) - K,\\, 0\\Big),\n$$\n价格的近似值为\n$$\nV_0 \\approx e^{-rT}\\,\\frac{1}{\\pi^{d/2}} \\sum_{j_1=1}^{n}\\cdots \\sum_{j_d=1}^{n} \\Big(\\prod_{k=1}^{d} w_{j_k}\\Big)\\, g\\!\\Big(\\sqrt{2}\\,x_{j_1},\\dots,\\sqrt{2}\\,x_{j_d}\\Big).\n$$\n\n此张量积法则使用 $n^d$ 次函数求值。对于 $d=5$，节点数量以 $n^5$ 的速度增长，这是维度灾难的一种体现：即使 $n$ 的小幅增加也会导致成本的指数级增长。\n\n为了缓解这个问题，我们考虑从相同的一维高斯-埃尔米特族构建的 Smolyak 稀疏网格。令 $Q_{\\ell}$ 表示水平为 $\\ell \\in \\mathbb{N}$、阶数为 $m(\\ell)=2\\ell-1$ 的一维高斯-埃尔米特求积。对于 $d$ 维的各向同性水平 $L \\in \\mathbb{N}$，Smolyak 算子为\n$$\nA(L,d) = \\sum_{\\boldsymbol{\\ell}\\in \\mathcal{I}(L,d)} c(\\boldsymbol{\\ell}) \\bigotimes_{k=1}^{d} Q_{\\ell_k},\n$$\n其指标集为\n$$\n\\mathcal{I}(L,d) = \\big\\{\\boldsymbol{\\ell}\\in \\mathbb{N}^d : 1 \\le \\ell_k,\\ \\ |\\boldsymbol{\\ell}|_1 \\le L + d - 1\\big\\},\n$$\n以及组合系数\n$$\nc(\\boldsymbol{\\ell}) = (-1)^{L + d - 1 - |\\boldsymbol{\\ell}|_1}\\binom{d-1}{L + d - 1 - |\\boldsymbol{\\ell}|_1}.\n$$\n由此得到的期望的稀疏网格近似为\n$$\n\\mathbb{E}[g(\\boldsymbol{Z})] \\approx \\frac{1}{\\pi^{d/2}} \\sum_{\\boldsymbol{\\ell}\\in \\mathcal{I}(L,d)} c(\\boldsymbol{\\ell}) \\sum_{j_1=1}^{m(\\ell_1)} \\cdots \\sum_{j_d=1}^{m(\\ell_d)} \\left(\\prod_{k=1}^{d} w_{j_k}^{(\\ell_k)}\\right) g\\!\\Big(\\sqrt{2}\\,x_{j_1}^{(\\ell_1)},\\dots,\\sqrt{2}\\,x_{j_d}^{(\\ell_d)}\\Big),\n$$\n其中 $(x_j^{(\\ell)}, w_j^{(\\ell)})$ 是阶数为 $m(\\ell)$ 的一维高斯-埃尔米特节点和权重。请注意，这种组合方法利用了与张量积相同的一维法则，但通过 Smolyak 系数混合了不同阶数的张量积。计算成本是所有 $\\boldsymbol{\\ell} \\in \\mathcal{I}(L,d)$ 的张量积大小 $\\prod_{k=1}^d m(\\ell_k)$ 的总和；对于相当的精度，该成本随 $d$ 的增长比 $n^d$ 更为平缓。\n\n算法设计：\n\n- 通过一个用于正交多项式的稳定生成器，预先计算所有所需阶数的一维高斯-埃尔米特节点和权重。这些节点和权重可以精确地对最高 $2n-1$ 次的多项式与 $e^{-x^2}$ 的乘积进行积分，并且权重之和等于 $\\sqrt{\\pi}$，确保了对常数被积函数的精确性。\n- 通过对一维节点和权重进行笛卡尔积来构建张量积求积，应用 $\\sqrt{2}$ 缩放以映射到标准正态分布，并乘以权重乘积。将累加和乘以 $\\pi^{-d/2}$ 和 $e^{-rT}$。\n- 通过遍历 $\\mathcal{I}(L,d)$ 中的所有多重指标 $\\boldsymbol{\\ell}$ 来实现 Smolyak 稀疏网格，计算组合系数 $c(\\boldsymbol{\\ell})$，对每个 $\\boldsymbol{\\ell}$ 使用维度 $k$ 中的 $m(\\ell_k)$ 个点构建张量网格，使用相同的归一化因子计算并累加加权贡献。将所有 $\\boldsymbol{\\ell}$ 的张量大小之和作为计算工作量。\n- 对于 $\\sigma_i = 0$ 的边界情况，到期价格是确定性的，$S_i(T) = S_i(0)e^{rT}$，因此确切价格为 $V_0^{\\text{det}} = \\max\\!\\big(\\frac{1}{d}\\sum_i S_i(0) - K e^{-rT}, 0\\big)$。由于高斯-埃尔米特法则能精确积分常数，张量积和稀疏网格近似都应与该值匹配，误差仅限于舍入误差。\n\n测试套件实现：\n\n- 情况 A：$d=5$, $S_0=(100,90,110,95,105)$, $\\sigma=(0.2,0.25,0.15,0.3,0.18)$, $r=0.02$, $T=1.0$, $K=100$。计算：\n  - 使用张量积 $n_{\\text{ref}}=9$ 的参考值。\n  - 使用张量积 $n=3$；记录绝对误差和节点数 $3^5$。\n  - 使用 $L=3$ 和 $m(\\ell)=2\\ell-1$ 的 Smolyak 稀疏网格；记录相对于参考值的绝对误差以及稀疏网格节点数（作为 $\\mathcal{I}(L,d)$ 上张量大小的总和）。\n- 情况 B：与情况 A 相同，但 $\\sigma=\\boldsymbol{0}$。验证张量积 $n=3$ 和稀疏网格 $L=3$ 的结果是否都在 $V_0^{\\text{det}}$ 的 $10^{-10}$ 容差内。\n- 效率比较：对于情况 A，检查稀疏网格是否在节点数严格少于张量积的情况下，达到了小于或等于张量积的误差。\n\n程序将生成单行输出：\n$[\\text{err\\_tensor},\\text{err\\_sparse},\\text{nodes\\_tensor},\\text{nodes\\_sparse},\\text{boundary\\_ok},\\text{efficiency\\_ok}]$,\n其中的条目如上定义。此设计通过张量网格的 $n^5$ 缩放展示了维度灾难，并将其与 Smolyak 稀疏网格进行对比，后者对于相同的一维高斯-埃尔米特族，可以用少得多的函数求值次数达到具有竞争力的精度。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom numpy.polynomial.hermite import hermgauss\nimport math\nfrom itertools import product\n\ndef gh_rule(n):\n    # One-dimensional Gauss-Hermite nodes and weights for weight e^{-x^2}\n    x, w = hermgauss(n)\n    return x, w\n\ndef basket_payoff(Z, S0, sigmas, r, T, K):\n    # Z: shape (N, d)\n    d = Z.shape[1]\n    S0 = np.asarray(S0, dtype=float)\n    sigmas = np.asarray(sigmas, dtype=float)\n    mu = (r - 0.5 * sigmas**2) * T\n    sgsqrtT = sigmas * math.sqrt(T)\n    # exponent per point and dimension\n    exponents = mu + sgsqrtT * Z  # shape (N, d)\n    ST = S0 * np.exp(exponents)   # broadcasting over (N, d)\n    avg = np.mean(ST, axis=1)\n    payoff = np.maximum(avg - K, 0.0)\n    return payoff\n\ndef tensor_gauss_hermite_price(S0, sigmas, r, T, K, n):\n    d = len(S0)\n    x, w = gh_rule(n)\n    # Build tensor grid via meshgrid\n    grids_x = np.meshgrid(*([x]*d), indexing='ij')\n    grids_w = np.meshgrid(*([w]*d), indexing='ij')\n    # Flatten\n    X_cols = [g.ravel() for g in grids_x]  # list length d, each shape (n**d,)\n    W_arrays = [gw.ravel() for gw in grids_w]\n    # Product weights\n    W_prod = np.ones_like(W_arrays[0])\n    for Wa in W_arrays:\n        W_prod *= Wa\n    # Map to standard normals: Z = sqrt(2) * x\n    Z = np.sqrt(2.0) * np.column_stack(X_cols)  # shape (N, d)\n    payoff = basket_payoff(Z, S0, sigmas, r, T, K)\n    # Normalization factor for d-dim expectation\n    factor = math.pi ** (-d / 2.0)\n    expectation = factor * np.sum(W_prod * payoff)\n    price = math.exp(-r * T) * expectation\n    nodes = n ** d\n    return price, nodes\n\ndef generate_multi_indices_sum_d(s, d, min_level=1):\n    # Generate all tuples of length d of positive integers >= min_level with sum s\n    # Use recursive backtracking\n    result = []\n    def backtrack(prefix, remaining, k):\n        if k == d - 1:\n            last = remaining\n            if last >= min_level:\n                result.append(tuple(prefix + [last]))\n            return\n        # Each component at least min_level\n        min_val = min_level\n        # Remaining must allow at least min_level for remaining dims\n        max_val = remaining - (d - k - 1) * min_level\n        for val in range(min_val, max_val + 1):\n            backtrack(prefix + [val], remaining - val, k + 1)\n    backtrack([], s, 0)\n    return result\n\ndef smolyak_sparse_price(S0, sigmas, r, T, K, L):\n    d = len(S0)\n    # Precompute 1D rules for levels 1..(L + d - 1) because indices can reach that in sum, but each component level is bounded by sum\n    # However, for efficiency we build on demand and cache by level\n    rule_cache = {}\n    def one_d_rule_for_level(level):\n        if level not in rule_cache:\n            n = 2 * level - 1\n            rule_cache[level] = gh_rule(n)\n        return rule_cache[level]\n\n    total_nodes_cost = 0\n    factor = math.pi ** (-d / 2.0)\n    acc = 0.0\n    # Iterate sums s from d to L + d - 1\n    for s in range(d, L + d):\n        # Binomial coefficient for all multi-indices with |ell|_1 = s\n        comb_coeff = math.comb(d - 1, L + d - 1 - s)\n        sign = -1 if ((L + d - 1 - s) % 2 == 1) else 1\n        c_s = sign * comb_coeff\n        if c_s == 0:\n            continue\n        # All multi-indices with sum s\n        for ell in generate_multi_indices_sum_d(s, d, min_level=1):\n            # Build per-dimension nodes and weights at levels in ell\n            x_list = []\n            w_list = []\n            n_points_list = []\n            for lev in ell:\n                xk, wk = one_d_rule_for_level(lev)\n                x_list.append(xk)\n                w_list.append(wk)\n                n_points_list.append(len(xk))\n            # Tensor product for this multi-index\n            grids_x = np.meshgrid(*x_list, indexing='ij')\n            grids_w = np.meshgrid(*w_list, indexing='ij')\n            # Flatten\n            X_cols = [g.ravel() for g in grids_x]\n            W_arrays = [gw.ravel() for gw in grids_w]\n            # Product weights\n            W_prod = np.ones_like(W_arrays[0])\n            for Wa in W_arrays:\n                W_prod *= Wa\n            # Map to Z\n            Z = np.sqrt(2.0) * np.column_stack(X_cols)\n            payoff = basket_payoff(Z, S0, sigmas, r, T, K)\n            contrib = factor * np.sum(W_prod * payoff)\n            acc += c_s * contrib\n            # Cost accumulation: number of nodes in this tensor product\n            nodes_this = 1\n            for npt in n_points_list:\n                nodes_this *= npt\n            total_nodes_cost += nodes_this\n    price = math.exp(-r * T) * acc\n    return price, total_nodes_cost\n\ndef solve():\n    # Define the test cases from the problem statement.\n\n    # Case A parameters\n    S0 = [100.0, 90.0, 110.0, 95.0, 105.0]\n    sigmas_A = [0.2, 0.25, 0.15, 0.3, 0.18]\n    r = 0.02\n    T = 1.0\n    K = 100.0\n    d = len(S0)\n\n    # Reference with tensor n_ref=9\n    price_ref, nodes_ref = tensor_gauss_hermite_price(S0, sigmas_A, r, T, K, n=9)\n\n    # Tensor with n=3\n    price_tensor, nodes_tensor = tensor_gauss_hermite_price(S0, sigmas_A, r, T, K, n=3)\n    err_tensor = abs(price_tensor - price_ref)\n\n    # Sparse grid with L=3\n    price_sparse, nodes_sparse = smolyak_sparse_price(S0, sigmas_A, r, T, K, L=3)\n    err_sparse = abs(price_sparse - price_ref)\n\n    # Case B: zero volatility boundary\n    sigmas_B = [0.0, 0.0, 0.0, 0.0, 0.0]\n    # Deterministic price\n    mean_S0 = sum(S0) / d\n    price_det = max(mean_S0 - K * math.exp(-r * T), 0.0)\n\n    price_tensor_B, _ = tensor_gauss_hermite_price(S0, sigmas_B, r, T, K, n=3)\n    price_sparse_B, _ = smolyak_sparse_price(S0, sigmas_B, r, T, K, L=3)\n\n    tol = 1e-10\n    boundary_ok = (abs(price_tensor_B - price_det) <= tol) and (abs(price_sparse_B - price_det) <= tol)\n\n    # Efficiency comparison for Case A\n    efficiency_ok = (err_sparse <= err_tensor) and (nodes_sparse < nodes_tensor)\n\n    results = [\n        float(err_tensor),\n        float(err_sparse),\n        int(nodes_tensor),\n        int(nodes_sparse),\n        bool(boundary_ok),\n        bool(efficiency_ok),\n    ]\n\n    # Final print statement in the exact required format.\n    # Ensure default Python representation for floats and booleans.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2396782"}, {"introduction": "现实世界的模型通常表现出各向异性，即某些变量的影响远大于其他变量。本练习将超越“一刀切”的各向同性网格，转向先验加密技术。你将学习如何构建一个各向异性稀疏网格，将更多的计算资源分配给更重要的维度，从而实现更高效的近似。[@problem_id:2432646]", "problem": "您的任务是使用基于Smolyak构造的各向异性稀疏网格来逼近超立方体上的高维函数。定义域为单位超立方体 $[0,1]^{10}$。令 $d = 10$。对每个维度 $i \\in \\{1,\\dots,d\\}$ 和每个层级 $\\ell_i \\in \\mathbb{N}$（$\\ell_i \\ge 1$），定义一维嵌套二进网格\n$$\nX_{\\ell_i} = \\left\\{ \\frac{j}{2^{\\ell_i - 1}} \\,:\\, j = 0,1,\\dots,2^{\\ell_i - 1} \\right\\} \\subset [0,1].\n$$\n对于多重索引 $\\boldsymbol{\\ell} = (\\ell_1,\\dots,\\ell_d)$，定义全张量网格\n$$\nG_{\\boldsymbol{\\ell}} = X_{\\ell_1} \\times \\cdots \\times X_{\\ell_d}.\n$$\n令 $U_{\\boldsymbol{\\ell}}$ 为 $G_{\\boldsymbol{\\ell}}$ 上的 $d$ 元多线性节点插值算子，它通过二进网格上的一维线性拉格朗日基函数的张量积来定义。\n\n令 $\\boldsymbol{\\alpha} = (\\alpha_1,\\dots,\\alpha_d)$ 为一个正整数各向异性权重向量，并令 $Q \\in \\mathbb{N}$ 且 $Q \\ge 0$。定义向下闭合索引集\n$$\n\\mathcal{I}(\\boldsymbol{\\alpha}, Q) = \\left\\{ \\boldsymbol{\\ell} \\in \\mathbb{N}^d \\,:\\, \\ell_i \\ge 1 \\text{ 对所有 } i, \\ \\sum_{i=1}^d \\alpha_i (\\ell_i - 1) \\le Q \\right\\}.\n$$\n考虑全网格插值算子的线性组合\n$$\n\\mathcal{S}_{\\boldsymbol{\\alpha},Q} = \\sum_{\\boldsymbol{\\ell} \\in \\mathcal{I}(\\boldsymbol{\\alpha}, Q)} c_{\\boldsymbol{\\ell}} \\, U_{\\boldsymbol{\\ell}},\n$$\n其中系数 $\\{c_{\\boldsymbol{\\ell}}\\}$ 由插值一致性条件所刻画\n$$\n\\sum_{\\boldsymbol{k} \\in \\mathcal{I}(\\boldsymbol{\\alpha}, Q), \\, \\boldsymbol{k} \\ge \\boldsymbol{\\ell}} c_{\\boldsymbol{k}} = 1 \\quad \\text{对于每个 } \\boldsymbol{\\ell} \\in \\mathcal{I}(\\boldsymbol{\\alpha}, Q),\n$$\n其中 $\\boldsymbol{k} \\ge \\boldsymbol{\\ell}$ 表示分量偏序，即对所有 $i$ 都有 $k_i \\ge \\ell_i$。此条件确保 $\\mathcal{S}_{\\boldsymbol{\\alpha},Q}$ 在 $\\bigcup_{\\boldsymbol{\\ell} \\in \\mathcal{I}(\\boldsymbol{\\alpha}, Q)} G_{\\boldsymbol{\\ell}}$ 中的每个节点上都对目标函数进行插值。\n\n您的程序必须在 $[0,1]^{10}$ 上实现 $\\mathcal{S}_{\\boldsymbol{\\alpha},Q}$ 并在指定点上对其求值。使用以下两个目标函数，每个函数都针对 $\\boldsymbol{x} = (x_1,\\dots,x_{10}) \\in [0,1]^{10}$ 定义：\n- $f_1(\\boldsymbol{x}) = \\exp(x_1) + \\sin(2\\pi x_2) + 0.1 \\sum_{i=3}^{10} 0.5^{\\,i} \\, x_i^2$.\n- $f_2(\\boldsymbol{x}) = \\log(1 + 5 x_1) + \\sqrt{1 + x_2} + 0.01 \\sum_{i=3}^{10} x_i$.\n所有三角函数参数都应解释为弧度。自然对数函数是 $\\log(\\cdot)$，主平方根是 $\\sqrt{\\cdot}$。\n\n使用以下求值点集 $\\mathcal{E} = \\{\\boldsymbol{y}^{(k)}\\}_{k=1}^{5} \\subset [0,1]^{10}$，其中每个 $\\boldsymbol{y}^{(k)}$ 均明确给出：\n- $\\boldsymbol{y}^{(1)} = (0.13,\\,0.77,\\,0.50,\\,0.20,\\,0.80,\\,0.33,\\,0.66,\\,0.10,\\,0.90,\\,0.42)$,\n- $\\boldsymbol{y}^{(2)} = (0.31,\\,0.62,\\,0.25,\\,0.75,\\,0.40,\\,0.60,\\,0.20,\\,0.80,\\,0.35,\\,0.65)$,\n- $\\boldsymbol{y}^{(3)} = (0.73,\\,0.27,\\,0.15,\\,0.85,\\,0.55,\\,0.45,\\,0.05,\\,0.95,\\,0.22,\\,0.78)$,\n- $\\boldsymbol{y}^{(4)} = (0.50,\\,0.50,\\,0.10,\\,0.90,\\,0.30,\\,0.70,\\,0.25,\\,0.75,\\,0.40,\\,0.60)$,\n- $\\boldsymbol{y}^{(5)} = (0.21,\\,0.84,\\,0.63,\\,0.37,\\,0.12,\\,0.88,\\,0.47,\\,0.53,\\,0.19,\\,0.81)$.\n\n定义偏重前两个维度的各向异性权重为\n$$\n\\boldsymbol{\\alpha}^{\\mathrm{aniso}} = (1,\\,1,\\,4,\\,4,\\,4,\\,4,\\,4,\\,4,\\,4,\\,4),\n$$\n以及各向同性权重为\n$$\n\\boldsymbol{\\alpha}^{\\mathrm{iso}} = (1,\\,1,\\,1,\\,1,\\,1,\\,1,\\,1,\\,1,\\,1,\\,1).\n$$\n\n测试套件。您的程序必须执行以下四个测试用例，并为每个用例报告一个标量结果：\n- 测试 1：使用 $\\boldsymbol{\\alpha} = \\boldsymbol{\\alpha}^{\\mathrm{aniso}}$ 和 $Q = 8$ 构建针对 $f_1$ 的 $\\mathcal{S}_{\\boldsymbol{\\alpha},Q}$。计算在 $\\mathcal{E}$ 上的最大绝对误差，\n$$\n\\max_{\\boldsymbol{y} \\in \\mathcal{E}} \\left| \\mathcal{S}_{\\boldsymbol{\\alpha},Q}[f_1](\\boldsymbol{y}) - f_1(\\boldsymbol{y}) \\right|.\n$$\n- 测试 2：使用 $\\boldsymbol{\\alpha} = \\boldsymbol{\\alpha}^{\\mathrm{iso}}$ 和 $Q = 3$ 构建针对 $f_1$ 的 $\\mathcal{S}_{\\boldsymbol{\\alpha},Q}$。计算在 $\\mathcal{E}$ 上的最大绝对误差。\n- 测试 3：使用 $\\boldsymbol{\\alpha} = \\boldsymbol{\\alpha}^{\\mathrm{aniso}}$ 和 $Q = 6$ 构建针对 $f_2$ 的 $\\mathcal{S}_{\\boldsymbol{\\alpha},Q}$。计算在 $\\mathcal{E}$ 上的最大绝对误差。\n- 测试 4：使用 $\\boldsymbol{\\alpha} = \\boldsymbol{\\alpha}^{\\mathrm{aniso}}$ 和 $Q = 8$ 构建针对 $f_1$ 的 $\\mathcal{S}_{\\boldsymbol{\\alpha},Q}$。在单点 $\\boldsymbol{0} = (0,0,0,0,0,0,0,0,0,0)$ 上计算绝对误差，\n$$\n\\left| \\mathcal{S}_{\\boldsymbol{\\alpha},Q}[f_1](\\boldsymbol{0}) - f_1(\\boldsymbol{0}) \\right|.\n$$\n\n最终输出格式。您的程序应生成单行输出，其中包含按顺序排列的四个结果，格式为方括号括起来的逗号分隔列表，例如\n$$\n[{\\tt r1},{\\tt r2},{\\tt r3},{\\tt r4}],\n$$\n其中每个 ${\\tt rj}$ 是一个实数（一个浮点值）。", "solution": "用户提供了一个定义明确的数值分析问题，具体涉及使用各向异性稀疏网格逼近高维函数。该问题具有科学依据，内部一致，并包含解决所需的所有信息。因此，我将着手提供一个完整的解决方案。\n\n核心任务是在指定点 $\\boldsymbol{y} \\in [0,1]^{10}$ 上计算稀疏网格插值 $\\mathcal{S}_{\\boldsymbol{\\alpha},Q}[f]$。该插值使用组合技术公式定义：\n$$\n\\mathcal{S}_{\\boldsymbol{\\alpha},Q}[f](\\boldsymbol{y}) = \\sum_{\\boldsymbol{\\ell} \\in \\mathcal{I}(\\boldsymbol{\\alpha}, Q)} c_{\\boldsymbol{\\ell}} \\, U_{\\boldsymbol{\\ell}}[f](\\boldsymbol{y})\n$$\n其中 $f$ 是目标函数，$\\boldsymbol{y}$ 是一个求值点，$\\mathcal{I}(\\boldsymbol{\\alpha}, Q)$ 是一个向下闭合的多重索引集，$c_{\\boldsymbol{\\ell}}$ 是组合系数，$U_{\\boldsymbol{\\ell}}[f](\\boldsymbol{y})$ 是全张量积多线性插值算子的值。\n\n该解决方案通过针对每个测试用例的一系列模块化步骤来实现：\n1.  **生成索引集**：构造集合 $\\mathcal{I}(\\boldsymbol{\\alpha}, Q) = \\left\\{ \\boldsymbol{\\ell} \\in \\mathbb{N}^d \\,:\\, \\ell_i \\ge 1, \\ \\sum_{i=1}^d \\alpha_i (\\ell_i - 1) \\le Q \\right\\}$。为高效实现此目的，我们定义层级向量 $\\boldsymbol{k} = (\\ell_1-1, \\dots, \\ell_d-1)$，其中每个 $k_i \\ge 0$。条件变为 $\\sum_{i=1}^d \\alpha_i k_i \\le Q$。采用递归回溯算法来寻找所有满足此不等式的有效向量 $\\boldsymbol{k} \\in \\mathbb{N}_0^d$。相应的多重索引 $\\boldsymbol{\\ell} = \\boldsymbol{k} + \\mathbf{1}$ 构成了集合 $\\mathcal{I}(\\boldsymbol{\\alpha}, Q)$。该集合存储在哈希集中以实现高效查找。\n\n2.  **计算组合系数**：系数 $c_{\\boldsymbol{\\ell}}$ 由确保算子具有插值性的一致性条件确定。对于一个向下闭合的索引集 $\\mathcal{I}$，该条件意味着 $c_{\\boldsymbol{\\ell}}$ 的唯一解由容斥原理给出：\n    $$\n    c_{\\boldsymbol{\\ell}} = \\sum_{J \\subseteq \\{1, \\dots, d\\}} (-1)^{|J|} \\mathbb{I}(\\boldsymbol{\\ell} + \\mathbf{e}_J \\in \\mathcal{I}(\\boldsymbol{\\alpha}, Q))\n    $$\n    其中 $\\mathbf{e}_J = \\sum_{j \\in J} \\mathbf{e}_j$ 且 $\\mathbf{e}_j$ 是第 $j$ 个标准基向量。$\\mathbb{I}(\\cdot)$ 是指示函数，如果条件成立则为 1，否则为 0。对于每个 $\\boldsymbol{\\ell} \\in \\mathcal{I}$，我们遍历维度的所有 $2^d$ 个子集 $J$，检查邻居索引 $\\boldsymbol{\\ell}+\\mathbf{e}_J$ 是否在 $\\mathcal{I}$ 中，并对带符号的贡献求和。对于 $d=10$，每个系数需要进行 $2^{10} = 1024$ 次检查，这在计算上是可行的。\n\n3.  **全张量积插值算子的求值**：项 $U_{\\boldsymbol{\\ell}}[f](\\boldsymbol{y})$ 表示在点 $\\boldsymbol{y}$ 上对全张量网格 $G_{\\boldsymbol{\\ell}} = X_{\\ell_1} \\times \\cdots \\times X_{\\ell_d}$ 上的多线性插值算子进行求值。这是通过递归实现的。$d$ 维插值算子的值是通过在第一维进行一维线性插值获得的，其中所需的两个网格点上的值本身是通过在其余变量上进行 $(d-1)$ 维插值计算的。这个过程在 $d$ 步后终止，需要在 $G_{\\boldsymbol{\\ell}}$ 中包围 $\\boldsymbol{y}$ 的超矩形单元的 $2^d$ 个角点上进行函数求值。\n\n4.  **通过记忆化进行优化**：由于存在冗余计算，一个朴素的实现方式在计算上是不可行的。对于不同的 $\\boldsymbol{\\ell}$，对 $U_{\\boldsymbol{\\ell}}[f](\\boldsymbol{y})$ 的求值会重复需要 $f$ 在相同网格点上的值。为了消除这种冗余，所有的函数求值 $f(\\boldsymbol{x})$ 都会被记忆化（缓存）在一个字典中。当需要特定网格点 $\\boldsymbol{x}$ 上的 $f$ 值时，会首先检查缓存，只有在值尚未计算过的情况下才会对函数进行求值。\n\n5.  **组装与误差计算**：对于每个测试用例和每个求值点 $\\boldsymbol{y}$，最终的逼近是通过对所有 $\\boldsymbol{\\ell} \\in \\mathcal{I}(\\boldsymbol{\\alpha}, Q)$ 上的贡献 $c_{\\boldsymbol{\\ell}} U_{\\boldsymbol{\\ell}}[f](\\boldsymbol{y})$ 求和来组装的。然后，绝对误差计算为 $|\\mathcal{S}_{\\boldsymbol{\\alpha},Q}[f](\\boldsymbol{y}) - f(\\boldsymbol{y})|$。按照要求，报告在求值集 $\\mathcal{E}$ 上的这些误差的最大值。\n\n对于测试用例 4，求值点是 $\\boldsymbol{y} = \\boldsymbol{0} = (0,\\dots,0)$。点 $\\boldsymbol{0}$ 是每个网格 $G_{\\boldsymbol{\\ell}}$ 的成员，因为一维网格 $X_{\\ell_i}$ 总是包含 0。稀疏网格构造保证了在稀疏网格并集 $\\mathcal{H}_{\\boldsymbol{\\alpha},Q} = \\bigcup_{\\boldsymbol{\\ell} \\in \\mathcal{I}} G_{\\boldsymbol{\\ell}}$ 中的所有点上都进行插值。由于 $\\boldsymbol{0} \\in \\mathcal{H}_{\\boldsymbol{\\alpha},Q}$，我们必须有 $\\mathcal{S}_{\\boldsymbol{\\alpha},Q}[f_1](\\boldsymbol{0}) = f_1(\\boldsymbol{0})$，这意味着绝对误差恰好为 0。这可作为对实现正确性的一个解析性检验。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom itertools import combinations\nimport math\n\ndef solve():\n    \"\"\"\n    Implements and evaluates an anisotropic sparse grid interpolant.\n    \"\"\"\n    d = 10\n\n    # Define target functions\n    def f1(x_tuple):\n        x = np.array(x_tuple)\n        term1 = np.exp(x[0])\n        term2 = np.sin(2 * np.pi * x[1])\n        term3 = 0.1 * np.sum([0.5**(i + 3) * x[i + 2]**2 for i in range(8)])\n        return term1 + term2 + term3\n\n    def f2(x_tuple):\n        x = np.array(x_tuple)\n        term1 = np.log(1 + 5 * x[0])\n        term2 = np.sqrt(1 + x[1])\n        term3 = 0.01 * np.sum(x[2:])\n        return term1 + term2 + term3\n\n    # Define evaluation points\n    evaluation_set = [\n        (0.13, 0.77, 0.50, 0.20, 0.80, 0.33, 0.66, 0.10, 0.90, 0.42),\n        (0.31, 0.62, 0.25, 0.75, 0.40, 0.60, 0.20, 0.80, 0.35, 0.65),\n        (0.73, 0.27, 0.15, 0.85, 0.55, 0.45, 0.05, 0.95, 0.22, 0.78),\n        (0.50, 0.50, 0.10, 0.90, 0.30, 0.70, 0.25, 0.75, 0.40, 0.60),\n        (0.21, 0.84, 0.63, 0.37, 0.12, 0.88, 0.47, 0.53, 0.19, 0.81),\n    ]\n\n    # Define anisotropy weights\n    alpha_aniso = (1, 1, 4, 4, 4, 4, 4, 4, 4, 4)\n    alpha_iso = (1, 1, 1, 1, 1, 1, 1, 1, 1, 1)\n\n    # Define test cases\n    test_cases = [\n        {'f': f1, 'alpha': alpha_aniso, 'Q': 8, 'eval_points': evaluation_set},\n        {'f': f1, 'alpha': alpha_iso,   'Q': 3, 'eval_points': evaluation_set},\n        {'f': f2, 'alpha': alpha_aniso, 'Q': 6, 'eval_points': evaluation_set},\n        {'f': f1, 'alpha': alpha_aniso, 'Q': 8, 'eval_points': [tuple([0.0]*d)]},\n    ]\n\n    memo_indices = {}\n    memo_coeffs = {}\n\n    def generate_indices(alpha, Q):\n        indices = set()\n        k_levels = []\n\n        def find_k(dim_idx, current_sum):\n            if dim_idx == d:\n                indices.add(tuple(k + 1 for k in k_levels))\n                return\n\n            max_k = (Q - current_sum) // alpha[dim_idx]\n            for ki in range(max_k + 1):\n                k_levels.append(ki)\n                find_k(dim_idx + 1, current_sum + alpha[dim_idx] * ki)\n                k_levels.pop()\n        \n        find_k(0, 0)\n        return indices\n\n    def calculate_coeffs(index_set):\n        coeffs = {}\n        e_vectors = np.identity(d, dtype=int)\n        for l_tuple in index_set:\n            l_vec = np.array(l_tuple)\n            c_l = 0\n            for size in range(d + 1):\n                for J in combinations(range(d), size):\n                    l_prime_vec = l_vec.copy()\n                    for j_idx in J:\n                        l_prime_vec[j_idx] += 1\n                    \n                    if tuple(l_prime_vec) in index_set:\n                        c_l += (-1)**size\n            coeffs[l_tuple] = c_l\n        return coeffs\n    \n    def get_interpolant_evaluator(f_func, y_tuple, l_tuple, memo_f):\n        def recursive_eval(dim, partial_point):\n            if dim == d:\n                point = tuple(partial_point)\n                if point not in memo_f:\n                    memo_f[point] = f_func(point)\n                return memo_f[point]\n\n            k = l_tuple[dim]\n            y_i = y_tuple[dim]\n\n            if k == 1:\n                # Grid is {0, 1}\n                weight = y_i\n                if abs(weight) < 1e-15: return recursive_eval(dim + 1, partial_point + [0.0])\n                if abs(weight - 1.0) < 1e-15: return recursive_eval(dim + 1, partial_point + [1.0])\n                \n                val_left = recursive_eval(dim + 1, partial_point + [0.0])\n                val_right = recursive_eval(dim + 1, partial_point + [1.0])\n                return (1.0 - weight) * val_left + weight * val_right\n            \n            m = 1 << (k - 1)  # 2**(k-1)\n            \n            if abs(y_i - 1.0) < 1e-15:\n                return recursive_eval(dim + 1, partial_point + [1.0])\n\n            pos = y_i * m\n            j = int(pos)\n            weight = pos - j\n\n            left_coord = j / m\n            \n            if weight < 1e-15:\n                return recursive_eval(dim + 1, partial_point + [left_coord])\n\n            right_coord = (j + 1) / m\n            \n            val_left = recursive_eval(dim + 1, partial_point + [left_coord])\n            val_right = recursive_eval(dim + 1, partial_point + [right_coord])\n            return (1.0 - weight) * val_left + weight * val_right\n\n        return recursive_eval(0, [])\n\n    results = []\n    for case in test_cases:\n        f_func = case['f']\n        alpha = case['alpha']\n        Q = case['Q']\n        eval_points = case['eval_points']\n        \n        case_key = (alpha, Q)\n        \n        if case_key in memo_indices:\n            index_set = memo_indices[case_key]\n        else:\n            index_set = generate_indices(alpha, Q)\n            memo_indices[case_key] = index_set\n\n        if case_key in memo_coeffs:\n            coeffs = memo_coeffs[case_key]\n        else:\n            coeffs = calculate_coeffs(index_set)\n            memo_coeffs[case_key] = coeffs\n        \n        memo_f = {}\n        max_abs_error = 0.0\n\n        for y_tuple in eval_points:\n            approx_val = 0.0\n            for l_tuple in index_set:\n                c_l = coeffs[l_tuple]\n                if abs(c_l) < 1e-15:\n                    continue\n                \n                u_l_f_y = get_interpolant_evaluator(f_func, y_tuple, l_tuple, memo_f)\n                approx_val += c_l * u_l_f_y\n            \n            true_val = f_func(y_tuple)\n            abs_error = abs(approx_val - true_val)\n            \n            if abs_error > max_abs_error:\n                max_abs_error = abs_error\n        \n        results.append(max_abs_error)\n\n    print(f\"[{','.join(f'{r:.12f}' for r in results)}]\")\n\nsolve()\n```", "id": "2432646"}, {"introduction": "稀疏网格的终极力量在于其能够根据函数的局部行为进行动态自适应。在最后一个实践中，你将实现一个真正的自适应算法。该算法利用层级盈余作为误差指标，自动在函数最复杂的区域添加网格点，即使对于非光滑函数，也能提供高效而精确的近似。[@problem_id:2432623]", "problem": "您需要设计并实现一种基于 Smolyak 构造的自适应稀疏网格插值。此插值使用超立方体域 $[0,1]^d$ 上的分层逐片线性（帽状）基函数。该算法必须以分层盈余系数的量值为加密指标，自适应地加密网格。您的实现必须是一个完整、可运行的程序，能够为一组已定义的测试套件计算指定的定量输出。\n\n所需的算法元素必须源自以下基础：\n- 一维分层基的核心定义：对于层级 $l \\in \\mathbb{N}$ 和奇数索引 $i \\in \\{1,3,\\dots,2^l - 1\\}$，定义一维节点 $x_{l,i} = i / 2^l$ 和帽状基函数\n$$\n\\varphi_{l,i}(x) = \\max\\left(1 - 2^l \\left| x - \\frac{i}{2^l} \\right|, 0\\right).\n$$\n- 多维张量基：对于 $d \\in \\mathbb{N}$，多层级 $\\boldsymbol{l} = (l_1,\\dots,l_d)$ 和多重索引 $\\boldsymbol{i} = (i_1,\\dots,i_d)$（其中每个 $i_j \\in \\{1,3,\\dots,2^{l_j}-1\\}$），节点为 $\\boldsymbol{x}_{\\boldsymbol{l},\\boldsymbol{i}} = (i_1 2^{-l_1},\\dots,i_d 2^{-l_d})$，基函数可分解为\n$$\n\\Phi_{\\boldsymbol{l},\\boldsymbol{i}}(\\boldsymbol{x}) = \\prod_{j=1}^d \\varphi_{l_j,i_j}(x_j).\n$$\n- 分层插值：给定一个多重索引 $(\\boldsymbol{l},\\boldsymbol{i})$ 的集合 $\\mathcal{A}$，插值函数为\n$$\n\\mathcal{I} f(\\boldsymbol{x}) = \\sum_{(\\boldsymbol{l},\\boldsymbol{i}) \\in \\mathcal{A}} \\alpha_{\\boldsymbol{l},\\boldsymbol{i}} \\, \\Phi_{\\boldsymbol{l},\\boldsymbol{i}}(\\boldsymbol{x}),\n$$\n其中分层盈余系数由以下公式递归定义\n$$\n\\alpha_{\\boldsymbol{l},\\boldsymbol{i}} = f\\!\\left(\\boldsymbol{x}_{\\boldsymbol{l},\\boldsymbol{i}}\\right) - \\mathcal{I}_{\\text{prev}} f\\!\\left(\\boldsymbol{x}_{\\boldsymbol{l},\\boldsymbol{i}}\\right),\n$$\n其中 $\\mathcal{I}_{\\text{prev}}$ 表示由所有先前添加的、在向下闭合意义上对应于严格更粗节点的基函数所构成的插值函数。\n\n您的自适应算法必须：\n1. 使用层级为 $\\boldsymbol{l} = (1,\\dots,1)$、索引为 $\\boldsymbol{i} = (1,\\dots,1)$ 的单个内部节点进行初始化，计算其分层盈余，并将其插入一个以盈余绝对值为键的自适应队列中。\n2. 迭代地选择队列中具有最大绝对盈余的节点，并通过添加其容许子节点来对其进行加密。沿坐标 $j$ 的子节点将 $l_j$ 增加 1，并将 $i_j$ 替换为 $\\{2 i_j - 1, 2 i_j + 1\\}$ 中的一个，同时保持所有其他坐标不变。对于每个新添加的子节点，使用当前插值函数计算其分层盈余，并将该子节点插入队列中。\n3. 当所有当前可用节点的最大绝对盈余低于给定容差 $ \\tau > 0$，或达到指定的网格点最大数量 $N_{\\max}$ 时，终止算法。\n\n使用此算法为以下测试套件构建插值函数，并为每种情况在指定的验证网格上计算最大绝对插值误差。所有角度（如适用）必须以弧度为单位进行解释。\n\n对于每个测试，定义：\n- 维度 $d$。\n- 目标函数 $f : [0,1]^d \\to \\mathbb{R}$。\n- 容差 $\\tau$ 和上限 $N_{\\max}$。\n- 一个验证网格，由每个维度上在开区间 $(0,1)$ 内的 $m$ 个等距点（具体为 $x_k = \\frac{k}{m+1}$，其中 $k = 1,2,\\dots,m$）的笛卡尔积形成。\n- 需要计算的最终输出：实际使用的网格点数（一个整数）和在验证网格上的最大绝对误差（一个浮点数），即：\n$$\nE_{\\max} = \\max_{\\boldsymbol{x} \\in \\mathcal{G}_m} \\left| f(\\boldsymbol{x}) - \\mathcal{I} f(\\boldsymbol{x}) \\right|.\n$$\n\n测试套件：\n- 情况 A（理想情况，三维光滑可分函数）：\n  - $d = 3$。\n  - $f(\\boldsymbol{x}) = \\exp\\!\\left(0.5\\,x_1 - 0.3\\,x_2 + 0.2\\,x_3\\right)$。\n  - $\\tau = 10^{-3}$, $N_{\\max} = 500$。\n  - 验证网格参数 $m = 9$。\n- 情况 B（二维各向异性局部高斯凸起）：\n  - $d = 2$。\n  - $f(\\boldsymbol{x}) = \\exp\\!\\left(-40 \\sum_{j=1}^2 (x_j - c_j)^2\\right)$ 其中 $\\boldsymbol{c} = (0.2, 0.8)$。\n  - $\\tau = 5 \\times 10^{-4}$, $N_{\\max} = 600$。\n  - 验证网格参数 $m = 25$。\n- 情况 C（一维非光滑绝对值扭折）：\n  - $d = 1$。\n  - $f(x) = |x - 0.3|$。\n  - $\\tau = 2 \\times 10^{-4}$, $N_{\\max} = 300$。\n  - 验证网格参数 $m = 200$。\n- 情况 D（带轻微振荡的四维中等光滑函数）：\n  - $d = 4$。\n  - $f(\\boldsymbol{x}) = \\prod_{j=1}^4 \\left(1 + 0.1\\,x_j\\right) + 0.01 \\sum_{j=1}^4 \\sin(2\\pi x_j)$，角度以弧度为单位。\n  - $\\tau = 2 \\times 10^{-3}$, $N_{\\max} = 400$。\n  - 验证网格参数 $m = 7$。\n\n实现约束和最终输出格式：\n- 您的程序必须是自包含的，且不得读取任何输入。它必须实现上述自适应稀疏网格插值算法，并为每个测试用例计算数对 $(N, E_{\\max})$，其中 $N$ 是终止时实际使用的网格点数。\n- 您的程序应生成单行输出，其中包含一个扁平、逗号分隔的 Python 列表形式的所有结果，格式为 [N_A,E_A,N_B,E_B,N_C,E_C,N_D,E_D]，其中 $N_\\cdot$ 是整数，$E_\\cdot$ 是浮点数。误差值必须以十进制数（而非分数）报告，并且角度（如果存在）必须以弧度解释。", "solution": "该问题陈述已经过验证，被认为是合理的。它具有科学依据、是适定且客观的。它提出了一个明确的任务：基于 Smolyak 构造和逐片线性分层基，设计并实现一个自适应稀疏网格插值算法。其定义、算法步骤和测试用例都经过严谨的规定，足以得出一个唯一且可验证的解。\n\n其目标是为函数 $f: [0,1]^d \\to \\mathbb{R}$ 构建一个自适应稀疏网格插值。定义域是 $d$ 维超立方体 $[0,1]^d$。该插值方案建立在逐片线性“帽状”函数的分层基之上。\n\n在一维情况下，对于给定的层级 $l \\in \\mathbb{N} = \\{1, 2, 3, \\dots\\}$，一组节点被定义在 $x_{l,i} = i / 2^l$ 处，其中索引 $i$ 为奇数，满足 $i \\in \\{1, 3, \\dots, 2^l - 1\\}$。与每个这样的节点相关联的是一个帽状基函数：\n$$\n\\varphi_{l,i}(x) = \\max\\left(1 - 2^l \\left| x - x_{l,i} \\right|, 0\\right)\n$$\n该基函数仅在区间 $(0,1)$ 的内部定义，这意味着最终的插值函数在边界上将为零。\n\n对于 $d$ 维问题，基函数通过张量积构造形成。分层网格中的一个点由多层级 $\\boldsymbol{l} = (l_1, \\dots, l_d) \\in \\mathbb{N}^d$ 和多重索引 $\\boldsymbol{i} = (i_1, \\dots, i_d)$ 标识，其中每个分量 $i_j$ 是满足 $1 \\le i_j \\le 2^{l_j}-1$ 的奇数。相应的网格节点是 $\\boldsymbol{x}_{\\boldsymbol{l},\\boldsymbol{i}} = (x_{l_1,i_1}, \\dots, x_{l_d,i_d})$，多维基函数为：\n$$\n\\Phi_{\\boldsymbol{l},\\boldsymbol{i}}(\\boldsymbol{x}) = \\prod_{j=1}^d \\varphi_{l_j,i_j}(x_j)\n$$\n\n对于一个选定的多重索引 $(\\boldsymbol{l}, \\boldsymbol{i})$ 集合 $\\mathcal{A}$，稀疏网格插值函数 $\\mathcal{I}f(\\boldsymbol{x})$ 是这些基函数的线性组合：\n$$\n\\mathcal{I} f(\\boldsymbol{x}) = \\sum_{(\\boldsymbol{l},\\boldsymbol{i}) \\in \\mathcal{A}} \\alpha_{\\boldsymbol{l},\\boldsymbol{i}} \\, \\Phi_{\\boldsymbol{l},\\boldsymbol{i}}(\\boldsymbol{x})\n$$\n系数 $\\alpha_{\\boldsymbol{l},\\boldsymbol{i}}$ 是分层盈余，以递归方式定义。对于一个新添加到网格中的点 $\\boldsymbol{x}_{\\boldsymbol{l},\\boldsymbol{i}}$，其盈余是真实函数值与由所有先前包含的、更粗的点所构造的插值函数在该点的值之差：\n$$\n\\alpha_{\\boldsymbol{l},\\boldsymbol{i}} = f(\\boldsymbol{x}_{\\boldsymbol{l},\\boldsymbol{i}}) - \\sum_{(\\boldsymbol{l}',\\boldsymbol{i}') \\in \\mathcal{A}_{\\text{prev}}} \\alpha_{\\boldsymbol{l}',\\boldsymbol{i}'} \\, \\Phi_{\\boldsymbol{l}',\\boldsymbol{i}'}(\\boldsymbol{x}_{\\boldsymbol{l},\\boldsymbol{i}})\n$$\n\n指定任务的核心是算法的自适应特性。自适应性由作为误差指标的分层盈余的量值驱动。该算法按以下步骤进行：\n\n1. 初始化：过程始于最粗层级上的单个内部节点，由多层级 $\\boldsymbol{l} = (1, \\dots, 1)$ 和多重索引 $\\boldsymbol{i} = (1, \\dots, 1)$ 指定。这对应于点 $\\boldsymbol{x} = (0.5, \\dots, 0.5)$。由于初始插值函数为零，其盈余就是 $\\alpha_{\\boldsymbol{l},\\boldsymbol{i}} = f(\\boldsymbol{x}_{\\boldsymbol{l},\\boldsymbol{i}})$。该点连同其盈余被添加到网格和优先队列中，该队列按盈余的绝对值大小排序。\n\n2. 迭代加密：算法进入一个循环，直到满足终止条件。在每次迭代中：\n   a. 选择：从优先队列中选择并移除具有最大绝对盈余 $|\\alpha|$ 的网格点 $(\\boldsymbol{l}_{\\text{p}}, \\boldsymbol{i}_{\\text{p}})$。此为加密的父点。\n   b. 加密：通过生成父点的容许子节点来加密网格。对于每个维度 $j \\in \\{1, \\dots, d\\}$，生成两个子节点。子节点的多层级由父节点在维度 $j$ 的层级加一得到，即 $l'_j = l_j+1$，而对于 $k \\neq j$ 则 $l'_k = l_k$。新层级 $l'_j$ 对应的索引 $i'_j$ 由 $\\{2i_j-1, 2i_j+1\\}$ 给出，而对于 $k \\neq j$ 则 $i'_k=i_k$。\n   c. 更新：对于每个新生成的子节点，使用上述公式计算其分层盈余，其中求和遍及当前网格中的所有点。新点及其盈余被添加到网格数据结构中，并插入到优先队列中。\n\n3. 终止：如果满足以下两个条件之一，迭代过程将停止：\n   a. 优先队列中的最大绝对盈余降至指定的容差 $\\tau$ 以下。\n   b. 网格中的总点数达到定义的最大值 $N_{\\max}$。\n\n终止时，算法已构建了一个稀疏网格和相应的插值函数 $\\mathcal{I}f(\\boldsymbol{x})$。最后一步是评估其精度。这是通过在预定义的验证网格 $\\mathcal{G}_m$ 上计算最大绝对误差 $E_{\\max}$ 来完成的：\n$$\nE_{\\max} = \\max_{\\boldsymbol{x} \\in \\mathcal{G}_m} \\left| f(\\boldsymbol{x}) - \\mathcal{I} f(\\boldsymbol{x}) \\right|\n$$\n验证网格 $\\mathcal{G}_m$ 是每个维度在 $(0,1)$ 内 $m$ 个等距点的笛卡尔积。\n\n该实现将包含一个主驱动函数，用于遍历指定的测试用例。对于每种情况，一个专用的求解函数将执行自适应算法。关键数据结构将包括一个用于存储网格点及其相关数据（盈余、坐标）的字典，以及一个来自 Python `heapq` 模块的最小堆，用作优先队列。将实现辅助函数来计算一维和多维基函数，以及整个插值函数。最终输出将是每个测试用例中网格使用的点数 $N$ 和计算出的最大误差 $E_{\\max}$。", "answer": "```python\nimport numpy as np\nimport heapq\nimport itertools\nimport math\n\ndef solve():\n    \"\"\"\n    Main function to define test cases and run the adaptive sparse grid solver for each.\n    \"\"\"\n\n    # --- Test Case Definitions ---\n    # Case A: Smooth separable function\n    def f_A(x):\n        return np.exp(0.5 * x[0] - 0.3 * x[1] + 0.2 * x[2])\n\n    # Case B: Anisotropic localized Gaussian bump\n    def f_B(x):\n        c = np.array([0.2, 0.8])\n        return np.exp(-40.0 * np.sum((x - c)**2))\n\n    # Case C: Non-smooth absolute value kink\n    def f_C(x):\n        return np.abs(x[0] - 0.3)\n\n    # Case D: Smooth function with mild oscillation\n    def f_D(x):\n        prod_term = np.prod(1.0 + 0.1 * x)\n        sin_term = 0.01 * np.sum(np.sin(2.0 * np.pi * x))\n        return prod_term + sin_term\n        \n    test_cases = [\n        {'d': 3, 'f': f_A, 'tau': 1e-3, 'n_max': 500, 'm': 9},\n        {'d': 2, 'f': f_B, 'tau': 5e-4, 'n_max': 600, 'm': 25},\n        {'d': 1, 'f': f_C, 'tau': 2e-4, 'n_max': 300, 'm': 200},\n        {'d': 4, 'f': f_D, 'tau': 2e-3, 'n_max': 400, 'm': 7}\n    ]\n\n    results = []\n    for case in test_cases:\n        N, E_max = solve_case(case['d'], case['f'], case['tau'], case['n_max'], case['m'])\n        results.extend([N, E_max])\n\n    print(f\"[{','.join(map(str, results))}]\")\n\n\ndef solve_case(d, f, tau, n_max, m):\n    \"\"\"\n    Solves a single adaptive sparse grid interpolation problem.\n    \"\"\"\n    \n    # Memoization caches for basis function calculations\n    phi_1d_cache = {}\n    phi_multi_d_cache = {}\n\n    def phi_1d(x, l, i):\n        \"\"\"Evaluates the 1D hierarchical hat basis function.\"\"\"\n        cache_key = (x, l, i)\n        if cache_key in phi_1d_cache:\n            return phi_1d_cache[cache_key]\n\n        # Using 1 << l which is equivalent to 2**l for integers\n        val = max(0.0, 1.0 - abs((1 << l) * x - i))\n        phi_1d_cache[cache_key] = val\n        return val\n\n    def phi_multi_d(x_vec, l_vec, i_vec):\n        \"\"\"Evaluates the multidimensional tensor-product basis function.\"\"\"\n        cache_key = (tuple(x_vec), l_vec, i_vec)\n        if cache_key in phi_multi_d_cache:\n            return phi_multi_d_cache[cache_key]\n        \n        prod = 1.0\n        for j in range(d):\n            prod *= phi_1d(x_vec[j], l_vec[j], i_vec[j])\n        phi_multi_d_cache[cache_key] = prod\n        return prod\n\n    def evaluate_interpolant(x_vec, grid_points):\n        \"\"\"Evaluates the sparse grid interpolant at a point x_vec.\"\"\"\n        total = 0.0\n        for (l_vec, i_vec), data in grid_points.items():\n            alpha = data['surplus']\n            basis_val = phi_multi_d(x_vec, l_vec, i_vec)\n            total += alpha * basis_val\n        return total\n\n    grid_points = {}\n    priority_queue = []\n\n    # 1. Initialize with the first point\n    l0 = tuple([1] * d)\n    i0 = tuple([1] * d)\n    \n    x0 = tuple(i / (1 << l) for l, i in zip(l0, i0))\n    f_val = f(np.array(x0))\n    alpha0 = f_val  # I_prev is 0\n    \n    grid_points[(l0, i0)] = {'surplus': alpha0, 'coords': x0}\n    heapq.heappush(priority_queue, (-abs(alpha0), l0, i0))\n\n    # 2. Main adaptive loop\n    while priority_queue:\n        if len(grid_points) >= n_max:\n            break\n        \n        neg_abs_alpha_max, _, _ = priority_queue[0]\n        if -neg_abs_alpha_max < tau:\n            break\n\n        # Pop parent point with largest surplus\n        _, l_parent, i_parent = heapq.heappop(priority_queue)\n\n        # Generate and add children\n        for j in range(d):  # Dimension to refine\n            l_child_list = list(l_parent)\n            l_child_list[j] += 1\n            l_child = tuple(l_child_list)\n            \n            i_child_val_1 = 2 * i_parent[j] - 1\n            i_child_val_2 = 2 * i_parent[j] + 1\n            \n            for i_child_val in [i_child_val_1, i_child_val_2]:\n                if len(grid_points) >= n_max:\n                    break\n\n                i_child_list = list(i_parent)\n                i_child_list[j] = i_child_val\n                i_child = tuple(i_child_list)\n\n                if (l_child, i_child) in grid_points:\n                    continue\n                \n                # Calculate surplus for the new child point\n                x_child = tuple(i / (1 << l) for l, i in zip(l_child, i_child))\n                f_val_child = f(np.array(x_child))\n                \n                # Clear evaluation caches for new point evaluation\n                phi_1d_cache.clear()\n                phi_multi_d_cache.clear()\n                \n                I_prev_at_child = evaluate_interpolant(x_child, grid_points)\n                alpha_child = f_val_child - I_prev_at_child\n\n                # Add child to grid and priority queue\n                grid_points[(l_child, i_child)] = {'surplus': alpha_child, 'coords': x_child}\n                heapq.heappush(priority_queue, (-abs(alpha_child), l_child, i_child))\n            \n            if len(grid_points) >= n_max:\n                break\n    \n    # Final number of grid points used\n    N = len(grid_points)\n\n    # 3. Error calculation on validation grid\n    axis_pts = (np.arange(1, m + 1, dtype=float)) / (m + 1.0)\n    validation_grid = itertools.product(*([axis_pts] * d))\n    \n    max_error = 0.0\n    for x_val_tuple in validation_grid:\n        x_val = np.array(x_val_tuple)\n        f_true = f(x_val)\n        \n        phi_1d_cache.clear()\n        phi_multi_d_cache.clear()\n        \n        f_interp = evaluate_interpolant(x_val, grid_points)\n        max_error = max(max_error, abs(f_true - f_interp))\n            \n    return N, max_error\n\nif __name__ == \"__main__\":\n    solve()\n\n```", "id": "2432623"}]}