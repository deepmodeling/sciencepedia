{"hands_on_practices": [{"introduction": "理论提供了地图，但实践才是真正的旅程。第一个练习将指导您从零开始实现一个完整的自助粒子滤波器。通过将其应用于一个经典的随机波动率模型，您不仅将掌握该算法的基本步骤，还将研究贝叶斯推断中的一个关键方面：结果对先验分布的敏感性。这项练习将为您理解粒子滤波器在实际问题中的运作方式打下坚实的基础。[@problem_id:2418266]", "problem": "实现并分析一个适用于计算金融领域的非线性状态空间模型的序列蒙特卡洛（SMC）粒子滤波器，并量化估计性能对初始粒子分布 $p(x_0)$ 的敏感性。您必须比较弥散先验与一个尖锐但错误的先验。您的程序必须是一个完整的、可运行的脚本，能够模拟数据、运行滤波器，并为一个小型测试套件输出量化指标。\n\n潜状态模型是一个常用于金融回报的随机波动率模型：\n- 状态转移（潜在对数波动率）：$x_t = \\mu + \\phi \\left(x_{t-1} - \\mu\\right) + \\sigma_v \\,\\eta_t$，其中 $\\eta_t \\sim \\mathcal{N}(0,1)$ 在时间 $t$ 上独立同分布。\n- 观测（回报）：$y_t \\mid x_t \\sim \\mathcal{N}\\left(0, \\exp(x_t)\\right)$，在给定 $x_t$ 的条件下，在时间 $t$ 上独立同分布。\n\n假设以下基础设定：\n- 如上所述的潜在马尔可夫过程以及给定状态下观测值的条件独立性。\n- 目标是使用粒子滤波器来近似滤波分布序列 $p(x_t \\mid y_{1:t})$。\n- 待实现的粒子滤波器是自举（bootstrap）粒子滤波器，它使用转移密度作为提议分布。\n\n待执行的任务：\n- 从模型中为每个测试用例模拟一个数据集 $\\{(x_t, y_t)\\}_{t=1}^T$。从转移动态所蕴含的平稳分布中初始化 $x_0$，即 $x_0 \\sim \\mathcal{N}\\!\\left(\\mu, \\frac{\\sigma_v^2}{1-\\phi^2}\\right)$，然后在 $t \\in \\{1,\\dots,T\\}$ 的范围内向前生成 $(x_t, y_t)$。\n- 实现一个具有 $N$ 个粒子、系统性重采样以及在有效样本量为 $N/2$ 时进行重采样的自举粒子滤波器。\n- 使用对数权重以避免数值下溢，并计算每个时间点 $t$ 的滤波均值 $\\hat{m}_t = \\sum_{i=1}^N w_t^{(i)} x_t^{(i)}$，其中 $w_t^{(i)}$ 是时间 $t$ 的归一化权重。\n- 对每个测试用例，在相同的模拟数据上运行粒子滤波器两次：\n  1. 弥散先验：$x_0^{(i)} \\sim \\mathcal{N}(m_0, s_0^2)$，其中 $m_0 = \\mu$ 且 $s_0 = 3.0$。\n  2. 尖锐但错误的先验：$x_0^{(i)} \\sim \\mathcal{N}(m_b, s_b^2)$，其中 $m_b = \\mu + 2\\,\\sigma_x$ 且 $s_b = 0.05$，而 $\\sigma_x = \\sigma_v / \\sqrt{1-\\phi^2}$ 是 $x_t$ 的平稳标准差。\n- 对每次运行，计算前 $H$ 步的均方根误差：\n$$\\mathrm{RMSE}_{1:H} = \\sqrt{\\frac{1}{H} \\sum_{t=1}^H \\left(\\hat{m}_t - x_t\\right)^2}.$$\n- 对每个测试用例，计算比率\n$$R = \\frac{\\mathrm{RMSE}^{\\text{peaked}}_{1:H}}{\\mathrm{RMSE}^{\\text{diffuse}}_{1:H}},$$\n这样，$R > 1$ 表示弥散先验在早期阶段产生的误差低于尖锐但错误的先验。\n\n算法要求：\n- 使用系统性重采样。\n- 使用有效样本量 $ESS = \\left(\\sum_{i=1}^N (w_t^{(i)})^2 \\right)^{-1}$ 并在 $ESS < N/2$ 时触发重采样。\n- 内部使用对数权重以保证数值稳定性。\n- 所有模拟和滤波过程必须由具有下述指定固定种子的伪随机数生成器驱动，以确保确定性。\n\n测试套件：\n- 对下面的每组参数，使用给定的种子模拟一次数据，然后使用不同的固定种子运行两种滤波器（弥散先验和尖峰先验）。对每种情况，报告比率 $R$。\n- 情况1（基线持续性与大量粒子）：\n  - $\\mu = -0.7$, $\\phi = 0.95$, $\\sigma_v = 0.15$, $T = 200$, $H = 25$, $N = 1000$。\n  - 数据模拟种子：$2023001$。\n  - 粒子滤波器种子：弥散 $= 9020001$，尖峰 $= 9020002$。\n- 情况2（少量粒子，权重退化风险更高）：\n  - $\\mu = -0.7$, $\\phi = 0.95$, $\\sigma_v = 0.15$, $T = 200$, $H = 25$, $N = 150$。\n  - 数据模拟种子：$2023002$。\n  - 粒子滤波器种子：弥散 $= 9020011$，尖峰 $= 9020012$。\n- 情况3（高持续性，对 $p(x_0)$ 的敏感性更强）：\n  - $\\mu = -0.7$, $\\phi = 0.985$, $\\sigma_v = 0.15$, $T = 200$, $H = 25$, $N = 1000$。\n  - 数据模拟种子：$2023003$。\n  - 粒子滤波器种子：弥散 $= 9020021$，尖峰 $= 9020022$。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，结果严格按照测试用例的顺序排列： $[R_1,R_2,R_3]$。\n- 每个 $R_j$ 必须是精确到六位小数的浮点数。\n- 不应打印任何其他文本。\n\n本问题不涉及物理单位。在随机数生成中可能隐含出现的任何角度都是无关的；不需要角度单位。\n\n您的程序必须是自包含的，无需任何输入，并且在指定种子下是可复现的。它必须实现所有必需的步骤，不依赖任何外部数据。输出必须完全由上述规范和您的确定性伪随机数使用方式决定。", "solution": "问题陈述已被解析和验证。我们发现此问题具有科学依据、提法适定、客观且完整。它描述了一项标准的计算统计任务：将序列蒙特卡洛（SMC）方法应用于金融计量经济学中常见的状态空间模型。所有参数、算法和评估指标都得到了明确的规定。因此，该问题被视为有效，并将构建相应的解决方案。\n\n该问题要求实现一个自举（bootstrap）粒子滤波器，用于估计随机波动率模型中的潜在对数波动率 $x_t$。该模型由两个方程定义：\n\n1. 潜在对数波动率过程 $x_t$ 的状态转移方程。这是一个一阶自回归过程，AR($1$)：\n$$x_t = \\mu + \\phi \\left(x_{t-1} - \\mu\\right) + \\sigma_v \\,\\eta_t, \\quad \\eta_t \\sim \\mathcal{N}(0,1)$$\n此处，$\\mu$ 是该过程的长期均值，$\\phi$ 是持续性参数，$\\sigma_v$ 是对数波动率的波动率。为使过程平稳，要求 $|\\phi| < 1$。该过程的平稳分布是高斯分布：\n$$x_t \\sim \\mathcal{N}\\!\\left(\\mu, \\frac{\\sigma_v^2}{1-\\phi^2}\\right)$$\n\n2. 金融回报 $y_t$ 的观测方程。在给定当前对数波动率的条件下，回报是条件独立的，并服从均值为 $0$、方差为 $\\exp(x_t)$ 的正态分布：\n$$y_t \\mid x_t \\sim \\mathcal{N}\\left(0, \\exp(x_t)\\right)$$\n该公式捕捉了众所周知的波动率聚集效应，即大的（小的）回报之后很可能跟随着大的（小的）回报。\n\n目标是为 $t = 1, \\dots, T$ 近似滤波分布 $p(x_t \\mid y_{1:t})$。自举粒子滤波器通过顺序更新一组 $N$ 个加权随机样本（即“粒子”）$\\{x_t^{(i)}, w_t^{(i)}\\}_{i=1}^N$ 来实现这一目标。这些粒子代表了滤波分布的离散近似。算法针对每个时间步 $t=1, \\dots, T$ 迭代进行。\n\n本文实现的算法遵循以下标准步骤，从近似 $p(x_{t-1} \\mid y_{1:t-1})$ 的一组粒子 $\\{x_{t-1}^{(i)}\\}_{i=1}^N$ 及相关联的归一化权重 $\\{w_{t-1}^{(i)}\\}_{i=1}^N$ 开始：\n\n**步骤1：重采样。** 权重退化是粒子滤波器中的一个常见问题，即经过几次迭代后，一个粒子的权重接近1，而所有其他粒子的权重接近0。为减轻此问题，我们计算有效样本量 $ESS = \\left(\\sum_{i=1}^N (w_{t-1}^{(i)})^2 \\right)^{-1}$。如果 $ESS$ 低于指定的阈值 $N/2$，则执行重采样步骤。使用系统性重采样，这是一种高效的方差缩减技术。它涉及从现有集合 $\\{x_{t-1}^{(i)}\\}_{i=1}^N$ 中有放回地选择 $N$ 个新粒子，其中选择粒子 $i$ 的概率是其权重 $w_{t-1}^{(i)}$。重采样后，新的粒子集是未加权的，因此所有权重被重置为 $w_{t-1}^{(i)} = 1/N$。\n\n**步骤2：传播。** 每个粒子根据状态转移动态随时间向前演化。由于使用的是自举滤波器，提议分布就是状态转移密度本身。对于每个粒子 $i$，抽取一个新的状态：\n$$x_t^{(i)} \\sim p(x_t \\mid x_{t-1}^{(i)}) = \\mathcal{N}\\left(\\mu + \\phi(x_{t-1}^{(i)} - \\mu), \\sigma_v^2\\right)$$\n这产生了一组新的粒子 $\\{x_t^{(i)}\\}_{i=1}^N$，它代表了时间 $t$ 的先验分布 $p(x_t \\mid y_{1:t-1})$。\n\n**步骤3：加权。** 更新重要性权重以融入新的观测值 $y_t$。每个粒子的更新后（未归一化）权重是其先前权重与给定新粒子状态 $x_t^{(i)}$ 下观测值 $y_t$ 的似然的乘积：\n$$\\tilde{w}_t^{(i)} = w_{t-1}^{(i)} \\cdot p(y_t \\mid x_t^{(i)})$$\n似然 $p(y_t \\mid x_t^{(i)})$ 由 $\\mathcal{N}(0, \\exp(x_t^{(i)}))$ 的概率密度函数在 $y_t$ 处的值给出。为保证数值稳定性，计算在对数域中进行：\n$$\\log \\tilde{w}_t^{(i)} = \\log w_{t-1}^{(i)} + \\log p(y_t \\mid x_t^{(i)})$$\n其中 $\\log p(y_t \\mid x_t^{(i)}) = -\\frac{1}{2}\\log(2\\pi) - \\frac{1}{2}x_t^{(i)} - \\frac{y_t^2}{2\\exp(x_t^{(i)})}$。\n\n**步骤4：归一化与估计。** 未归一化的对数权重被归一化以确保它们的和为一。首先，为防止数值溢出，在进行指数运算前减去最大的对数权重。归一化后的权重为：\n$$w_t^{(i)} = \\frac{\\exp(\\log \\tilde{w}_t^{(i)})}{\\sum_{j=1}^N \\exp(\\log \\tilde{w}_t^{(j)})}$$\n随后，时间 $t$ 的状态均值的滤波估计值计算为粒子的加权平均值：\n$$\\hat{m}_t = \\sum_{i=1}^N w_t^{(i)} x_t^{(i)}$$\n\n该分析要求比较滤波器在两种不同初始粒子分布 $p(x_0)$ 下的性能。一个“弥散”先验 $\\mathcal{N}(\\mu, 3.0^2)$，以真实的平稳均值为中心但具有高方差。这反映了较弱的先验知识。一个“尖锐但错误”的先验 $\\mathcal{N}(\\mu + 2\\sigma_x, 0.05^2)$（其中 $\\sigma_x$ 是平稳标准差），反映了强烈但错误的置信。比较通过初始时间范围 $H$ 内的均方根误差（$\\mathrm{RMSE}$）之比 $R$ 来量化：\n$$R = \\frac{\\mathrm{RMSE}^{\\text{peaked}}_{1:H}}{\\mathrm{RMSE}^{\\text{diffuse}}_{1:H}}, \\quad \\mathrm{其中} \\quad \\mathrm{RMSE}_{1:H} = \\sqrt{\\frac{1}{H} \\sum_{t=1}^H \\left(\\hat{m}_t - x_t\\right)^2}$$\n比率 $R > 1$ 表明弥散先验在滤波的初始阶段表现更优。这是符合预期的，因为一个尖锐且错误的先验可能会误导滤波器，尤其是在一个状态演化缓慢的高度持续性过程中，此时需要更多时间让数据来纠正初始误差。实现将遵循这些原则，使用指定的参数和随机种子以确保可复现性。", "answer": "```python\nimport numpy as np\nfrom scipy.stats import norm\n\ndef systematic_resample(weights, rng):\n    \"\"\"\n    Performs systematic resampling.\n\n    Args:\n        weights (np.ndarray): Array of normalized particle weights.\n        rng (np.random.Generator): A numpy random number generator.\n\n    Returns:\n        np.ndarray: Indices of resampled particles.\n    \"\"\"\n    N = len(weights)\n    # Generate N ordered pointers from a single random draw\n    u = rng.uniform(0.0, 1.0 / N)\n    positions = u + np.arange(N) / N\n    \n    # Calculate cumulative sum of weights\n    cumulative_weights = np.cumsum(weights)\n    \n    # Find indices of particles to keep\n    indices = np.searchsorted(cumulative_weights, positions)\n    return indices\n\ndef simulate_data(mu, phi, sigma_v, T, seed):\n    \"\"\"\n    Simulates data from the stochastic volatility model.\n\n    Args:\n        mu (float): Long-run mean of log-volatility.\n        phi (float): Persistence parameter.\n        sigma_v (float): Volatility of log-volatility.\n        T (int): Number of time steps.\n        seed (int): Seed for the random number generator.\n\n    Returns:\n        tuple[np.ndarray, np.ndarray]: True states x (T+1) and observations y (T).\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    \n    x = np.zeros(T + 1)\n    y = np.zeros(T)\n    \n    # Stationary distribution for x_0\n    if abs(phi) < 1:\n        sigma_x_sq = sigma_v**2 / (1 - phi**2)\n        sigma_x = np.sqrt(sigma_x_sq)\n        x[0] = mu + sigma_x * rng.standard_normal()\n    else: # Handle non-stationary case if necessary, here we assume it's stationary.\n        x[0] = mu\n        \n    for t in range(1, T + 1):\n        # State transition\n        x[t] = mu + phi * (x[t-1] - mu) + sigma_v * rng.standard_normal()\n        \n        # Observation\n        obs_std_dev = np.exp(x[t] / 2.0)\n        y[t-1] = obs_std_dev * rng.standard_normal()\n        \n    return x, y\n\ndef run_particle_filter(y_data, mu, phi, sigma_v, N, m_prior, s_prior, seed, H):\n    \"\"\"\n    Runs the bootstrap particle filter for the stochastic volatility model.\n\n    Args:\n        y_data (np.ndarray): Array of observations.\n        mu, phi, sigma_v (float): Model parameters.\n        N (int): Number of particles.\n        m_prior, s_prior (float): Mean and std dev for the initial particle distribution.\n        seed (int): Seed for the filter's random number generator.\n        H (int): Horizon for RMSE calculation (unused in function, for context).\n\n    Returns:\n        np.ndarray: Array of filtered state mean estimates.\n    \"\"\"\n    T = len(y_data)\n    rng = np.random.default_rng(seed)\n    \n    # Initialization (t=0)\n    # This represents the particle approximation of p(x_0)\n    particles_tm1 = rng.normal(loc=m_prior, scale=s_prior, size=N)\n    weights_tm1 = np.full(N, 1.0 / N)\n    \n    estimates = np.zeros(T)\n    \n    # Use precomputed constant for log-likelihood\n    LOG_2PI_HALF = 0.5 * np.log(2 * np.pi)\n\n    # Main loop for t=1,...,T (Python index 0 to T-1)\n    for t in range(T):\n        y_obs = y_data[t]\n        \n        # --- Step 1: Resampling (based on weights from t-1) ---\n        ess = 1.0 / np.sum(weights_tm1**2)\n        if ess < N / 2.0:\n            indices = systematic_resample(weights_tm1, rng)\n            particles_tm1 = particles_tm1[indices]\n            # After resampling, weights are implicitly reset\n            log_weights_tm1 = np.full(N, -np.log(N))\n        else:\n            log_weights_tm1 = np.log(weights_tm1)\n\n        # --- Step 2: Propagation (from t-1 to t) ---\n        noise = rng.standard_normal(N)\n        particles_t = mu + phi * (particles_tm1 - mu) + sigma_v * noise\n\n        # --- Step 3: Weighting (using observation y_t) ---\n        log_var = particles_t\n        log_likelihoods = -LOG_2PI_HALF - 0.5 * log_var - (y_obs**2) / (2.0 * np.exp(log_var))\n        \n        # Update log-weights\n        unnorm_log_weights_t = log_weights_tm1 + log_likelihoods\n        \n        # Normalize weights for stability\n        max_log_weight = np.max(unnorm_log_weights_t)\n        temp_weights = np.exp(unnorm_log_weights_t - max_log_weight)\n        weights_t = temp_weights / np.sum(temp_weights)\n\n        # --- Step 4: Estimation ---\n        estimates[t] = np.sum(weights_t * particles_t)\n\n        # --- Step 5: Prepare for next iteration ---\n        particles_tm1 = particles_t\n        weights_tm1 = weights_t\n        \n    return estimates\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and produce the final output.\n    \"\"\"\n    test_cases = [\n        # Case 1\n        {'mu': -0.7, 'phi': 0.95, 'sigma_v': 0.15, 'T': 200, 'H': 25, 'N': 1000,\n         'data_seed': 2023001, 'pf_seeds': {'diffuse': 9020001, 'peaked': 9020002}},\n        # Case 2\n        {'mu': -0.7, 'phi': 0.95, 'sigma_v': 0.15, 'T': 200, 'H': 25, 'N': 150,\n         'data_seed': 2023002, 'pf_seeds': {'diffuse': 9020011, 'peaked': 9020012}},\n        # Case 3\n        {'mu': -0.7, 'phi': 0.985, 'sigma_v': 0.15, 'T': 200, 'H': 25, 'N': 1000,\n         'data_seed': 2023003, 'pf_seeds': {'diffuse': 9020021, 'peaked': 9020022}},\n    ]\n\n    results = []\n\n    for case in test_cases:\n        mu, phi, sigma_v, T, H, N = case['mu'], case['phi'], case['sigma_v'], case['T'], case['H'], case['N']\n        \n        # Simulate data once for the case\n        x_true, y_data = simulate_data(mu, phi, sigma_v, T, case['data_seed'])\n\n        # Define prior parameters\n        # Diffuse prior\n        m0_diffuse = mu\n        s0_diffuse = 3.0\n        \n        # Peaked, incorrect prior\n        sigma_x = sigma_v / np.sqrt(1 - phi**2)\n        m0_peaked = mu + 2.0 * sigma_x\n        s0_peaked = 0.05\n        \n        # Run filters\n        estimates_diffuse = run_particle_filter(\n            y_data, mu, phi, sigma_v, N, m0_diffuse, s0_diffuse, case['pf_seeds']['diffuse'], H\n        )\n        estimates_peaked = run_particle_filter(\n            y_data, mu, phi, sigma_v, N, m0_peaked, s0_peaked, case['pf_seeds']['peaked'], H\n        )\n        \n        # Compute RMSE over the first H steps\n        # x_true[0] is x_0, x_true[1:H+1] corresponds to y_data[0:H]\n        errors_diffuse = estimates_diffuse[:H] - x_true[1:H+1]\n        rmse_diffuse = np.sqrt(np.mean(errors_diffuse**2))\n        \n        errors_peaked = estimates_peaked[:H] - x_true[1:H+1]\n        rmse_peaked = np.sqrt(np.mean(errors_peaked**2))\n        \n        # Compute the ratio R\n        ratio_R = rmse_peaked / rmse_diffuse\n        results.append(f\"{ratio_R:.6f}\")\n\n    # Print final output in the required format\n    print(f\"[{','.join(results)}]\")\n\n# Execute the solution\nsolve()\n```", "id": "2418266"}, {"introduction": "一旦您能为一个*给定*的模型估计状态，下一个合乎逻辑的步骤就是估计模型本身的参数。本练习将您带入非线性状态空间模型的参数估计领域，这是计量经济学中的一项常见任务。您将使用粒子滤波器，但不再是将其作为最终目的，而是作为一个强大的引擎来近似一个非线性菲利普斯曲线模型的似然函数，从而最终从数据中找出最可能的参数。[@problem_id:2418262]", "problem": "考虑一个带有随时间变化的非加速通货膨胀失业率（NAIRU）的非线性菲利普斯曲线。令不可观测的状态为 NAIRU，记作 $n_t$，观测到的失业率为 $u_t$，观测到的通货膨胀为 $\\pi_t$。该模型是一个由以下公式定义的非线性状态空间系统：\n$$\n\\pi_t = \\alpha + \\beta \\left(u_t - n_t\\right) + \\gamma \\left(u_t - n_t\\right)^3 + \\varepsilon_t,\n$$\n$$\nn_t - \\mu = \\rho \\left(n_{t-1} - \\mu\\right) + \\eta_t,\n$$\n其中 $\\varepsilon_t \\sim \\mathcal{N}(0,\\sigma_\\varepsilon^2)$，$\\eta_t \\sim \\mathcal{N}(0,\\sigma_\\eta^2)$，且 $\\alpha = 0$。初始状态 $n_0$ 根据状态方程所蕴含的平稳分布进行分布，即：\n$$\nn_0 \\sim \\mathcal{N}\\!\\left(\\mu,\\;\\frac{\\sigma_\\eta^2}{1-\\rho^2}\\right).\n$$\n您的任务是为三种不同的参数配置（测试用例）模拟数据，然后对每个测试用例，在指定的有限网格上计算参数向量 $\\theta = (\\beta,\\gamma,\\sigma_\\varepsilon,\\sigma_\\eta)$ 的最大似然估计，同时保持 $\\alpha=0$、$\\mu$ 和 $\\rho$ 在每个测试用例中指定的值固定不变。\n\n数据生成必须按以下方式进行。\n\n- 公共的观测失业率过程 $u_t$：\n  - 时间长度 $T = 120$。\n  - 自回归过程 $u_t = \\mu_u + \\phi\\,(u_{t-1}-\\mu_u) + \\nu_t$，其中 $\\nu_t \\sim \\mathcal{N}(0,\\sigma_u^2)$。\n  - 参数：$\\mu_u = 5.5$，$\\phi = 0.8$，$\\sigma_u = 0.3$，以及初始值 $u_0 = \\mu_u$。\n  - 使用伪随机种子 $100$ 来生成 $\\{u_t\\}_{t=1}^T$。\n- 对每个测试用例 $i \\in \\{1,2,3\\}$，使用上述模型和特定于该测试用例的真实参数来模拟潜在状态 $\\{n_t\\}_{t=1}^T$ 和通货膨胀 $\\{\\pi_t\\}_{t=1}^T$。对每个测试用例 $i$，使用伪随机种子 $200+i$ 来生成状态方程和测量方程中的新息。\n\n该测试套件由以下三个用例定义，每个用例都提供了固定的 $(\\mu,\\rho)$ 和用于数据模拟的真实参数 $(\\beta^\\star,\\gamma^\\star,\\sigma_\\varepsilon^\\star,\\sigma_\\eta^\\star)$，以及您必须在其上计算和最大化似然函数的有限网格。\n\n- 用例 $1$ (一般情况)：\n  - 固定参数：$\\mu = 5.5$，$\\rho = 0.95$。\n  - 真实模拟参数：$\\beta^\\star = -0.5$，$\\gamma^\\star = 0.06$，$\\sigma_\\varepsilon^\\star = 0.2$，$\\sigma_\\eta^\\star = 0.1$。\n  - 估计网格：\n    - $\\beta \\in \\{-0.6,\\,-0.5\\}$，\n    - $\\gamma \\in \\{0.04,\\,0.06\\}$，\n    - $\\sigma_\\varepsilon \\in \\{0.18,\\,0.22\\}$，\n    - $\\sigma_\\eta \\in \\{0.09,\\,0.11\\}$。\n- 用例 $2$ (近似线性的菲利普斯曲线，小状态噪声)：\n  - 固定参数：$\\mu = 5.5$，$\\rho = 0.9$。\n  - 真实模拟参数：$\\beta^\\star = -0.4$，$\\gamma^\\star = 0.0$，$\\sigma_\\varepsilon^\\star = 0.15$，$\\sigma_\\eta^\\star = 0.02$。\n  - 估计网格：\n    - $\\beta \\in \\{-0.5,\\,-0.4\\}$，\n    - $\\gamma \\in \\{0.0,\\,0.02\\}$，\n    - $\\sigma_\\varepsilon \\in \\{0.12,\\,0.18\\}$，\n    - $\\sigma_\\eta \\in \\{0.01,\\,0.03\\}$。\n- 用例 $3$ (高度持续性的 NAIRU 和更强的非线性)：\n  - 固定参数：$\\mu = 5.5$，$\\rho = 0.98$。\n  - 真实模拟参数：$\\beta^\\star = -0.6$，$\\gamma^\\star = 0.2$，$\\sigma_\\varepsilon^\\star = 0.25$，$\\sigma_\\eta^\\star = 0.15$。\n  - 估计网格：\n    - $\\beta \\in \\{-0.7,\\,-0.6\\}$，\n    - $\\gamma \\in \\{0.18,\\,0.2\\}$，\n    - $\\sigma_\\varepsilon \\in \\{0.22,\\,0.28\\}$，\n    - $\\sigma_\\eta \\in \\{0.13,\\,0.17\\}$。\n\n对每个测试用例，给定观测对 $\\{(u_t,\\pi_t)\\}_{t=1}^T$ 和固定的 $(\\mu,\\rho)$，计算指定网格中每个参数元组的对数似然值，并选择使该对数似然最大化的参数元组。您的程序必须对所有测试用例使用相同的观测失业率序列 $\\{u_t\\}_{t=1}^T$。为保证可复现性，在似然计算中使用的任何额外伪随机数都必须使用伪随机种子 $300+i$（对于测试用例 $i$）生成，并且在同一测试用例的所有参数元组中必须相同地重用。\n\n您的程序必须按测试用例 1、2、3 的顺序输出最大化参数元组的串联列表，其中每个元组按 $(\\beta,\\gamma,\\sigma_\\varepsilon,\\sigma_\\eta)$ 的顺序排列。报告的每个数字必须精确到小数点后六位。最终输出必须是单行文本，包含一个用方括号括起来的逗号分隔列表，例如 $[\\beta_1,\\gamma_1,\\sigma_{\\varepsilon,1},\\sigma_{\\eta,1},\\beta_2,\\gamma_2,\\sigma_{\\varepsilon,2},\\sigma_{\\eta,2},\\beta_3,\\gamma_3,\\sigma_{\\varepsilon,3},\\sigma_{\\eta,3}]$。\n\n所有测试用例的答案都必须是浮点数。不涉及物理单位；不要包含任何单位符号。不涉及角度。所有类似分数的值都必须以小数形式提供。输出必须严格遵守上述单行格式。", "solution": "该问题陈述构成了一个计算计量经济学领域中定义明确的练习，特别是在非线性状态空间模型的参数估计范畴内。它具有科学依据，内部一致，并为唯一、可验证的解提供了所有必要信息。因此，该问题被认定是有效的。\n\n核心任务是为一个非线性菲利普斯曲线模型找到参数向量 $\\theta = (\\beta, \\gamma, \\sigma_\\varepsilon, \\sigma_\\eta)$ 的最大似然估计，其中搜索空间被限制在一个有限网格。该模型由通货膨胀 $\\pi_t$ 的测量方程和不可观测的 NAIRU $n_t$ 的状态方程定义：\n$$\n\\pi_t = \\beta \\left(u_t - n_t\\right) + \\gamma \\left(u_t - n_t\\right)^3 + \\varepsilon_t, \\quad \\varepsilon_t \\sim \\mathcal{N}(0, \\sigma_\\varepsilon^2)\n$$\n$$\nn_t = \\mu + \\rho \\left(n_{t-1} - \\mu\\right) + \\eta_t, \\quad \\eta_t \\sim \\mathcal{N}(0, \\sigma_\\eta^2)\n$$\n\n测量方程中的三次项 $(u_t - n_t)^3$ 引入的非线性使得似然函数在分析上难以处理。对于线性高斯状态空间模型，卡尔曼滤波器可以精确计算似然函数。然而，对于非线性或非高斯系统，则需要使用数值近似方法。解决此类问题的标准且有原则的方法是使用序贯蒙特卡罗（SMC）方法，通常称为粒子滤波器。\n\n其方法是为指定网格上的每个参数向量 $\\theta$ 近似计算对数似然函数 $\\log \\mathcal{L}(\\theta | Y_{1:T})$。对数似然可以分解为条件对数似然的总和：\n$$\n\\log \\mathcal{L}(\\theta | Y_{1:T}) = \\sum_{t=1}^T \\log p(y_t | Y_{1:t-1}; \\theta)\n$$\n其中 $Y_{1:t} = \\{(\\pi_1, u_1), \\dots, (\\pi_t, u_t)\\}$ 表示到时间 $t$ 为止的观测历史。粒子滤波器为每个预测密度项 $p(\\pi_t | Y_{1:t-1}; \\theta)$ 提供了一个数值估计。\n\n我们将实现一种称为自举滤波器（Bootstrap Filter）的特定粒子滤波器变体。对于给定的参数向量 $\\theta$ 和一组 $N$ 个粒子，算法流程如下：\n\n1.  **初始化 (t=0)**：从初始状态分布中抽取一组 $N$ 个粒子 $\\{n_0^{(j)}\\}_{j=1}^N$，该分布是状态过程的平稳分布：$n_0 \\sim \\mathcal{N}(\\mu, \\sigma_\\eta^2/(1-\\rho^2))$。总对数似然初始化为 $0$。\n\n2.  **序贯更新 (for t = 1 to T)**：\n    a.  **预测 (传播)**：根据状态转移方程将每个粒子向前传播。为每个粒子抽取一个随机新息：\n        $$\n        n_t^{(j)} = \\mu + \\rho(n_{t-1}^{(j)} - \\mu) + \\eta_t^{(j)}, \\quad \\text{where } \\eta_t^{(j)} \\sim \\mathcal{N}(0, \\sigma_\\eta^2)\n        $$\n    b.  **加权**：根据每个传播后的粒子 $n_t^{(j)}$ 对当前观测值 $\\pi_t$ 的解释程度，为其分配一个权重 $w_t^{(j)}$。该权重是测量误差在观测值处的概率密度函数（PDF）值：\n        $$\n        \\hat{\\pi}_t^{(j)} = \\beta(u_t - n_t^{(j)}) + \\gamma(u_t - n_t^{(j)})^3\n        $$\n        $$\n        w_t^{(j)} = p(\\pi_t | n_t^{(j)}, u_t; \\theta) = \\frac{1}{\\sqrt{2\\pi\\sigma_\\varepsilon^2}} \\exp\\left(-\\frac{(\\pi_t - \\hat{\\pi}_t^{(j)})^2}{2\\sigma_\\varepsilon^2}\\right)\n        $$\n    c.  **似然近似**：条件似然 $p(\\pi_t | Y_{1:t-1}; \\theta)$ 通过未归一化权重的平均值来近似：\n        $$\n        \\hat{p}(\\pi_t | Y_{1:t-1}; \\theta) \\approx \\frac{1}{N} \\sum_{j=1}^N w_t^{(j)}\n        $$\n        通过加上该平均权重的对数来更新总对数似然。如果平均权重为零，则对数似然为负无穷大，表明该参数向量与观测值不兼容。\n    d.  **重采样**：为解决粒子退化问题（即一个粒子获得了所有权重），从当前粒子集 $\\{n_t^{(j)}\\}$ 中有放回地抽取一组新粒子，抽样概率由归一化权重 $W_t^{(j)} = w_t^{(j)} / \\sum_k w_t^{(k)}$ 给出。我们采用系统重采样，这是一种高效且低方差的技术。重采样后的粒子将被用于时间 $t+1$ 的预测步骤。\n\n总体的计算流程如下：\n首先，模拟公共的观测失业率序列 $\\{u_t\\}_{t=1}^{T=120}$。然后，对三个测试用例中的每一个：\n1.  使用该测试用例的真实参数和指定的随机种子模拟通货膨胀序列 $\\{\\pi_t\\}_{t=1}^{T=120}$。\n2.  使用为估计步骤指定的种子，预先生成粒子滤波器所需的随机变量集（用于初始抽取、传播和重采样）。这确保了滤波器本身的随机性对于参数网格上的所有点都是相同的，从而保证了对它们似然值的公平比较。\n3.  使用粒子滤波器计算该测试用例网格中每个参数元组的近似对数似然。\n4.  将产生最高对数似然值的参数元组确定为该用例的最大似然估计。\n\n最后，将所有三个用例的估计参数串联起来，并按照问题规范进行格式化。", "answer": "```python\nimport numpy as np\nfrom scipy.stats import norm\nimport itertools\n\ndef solve():\n    \"\"\"\n    Main solver function to run simulations and estimations for all test cases.\n    \"\"\"\n    # Global parameters\n    T = 120\n    U_MU = 5.5\n    U_PHI = 0.8\n    U_SIGMA = 0.3\n    U_INITIAL = U_MU\n    U_SEED = 100\n    N_PARTICLES = 2000\n\n    # Test case definitions\n    test_cases = [\n        {\n            \"case_id\": 1,\n            \"fixed_params\": {\"mu\": 5.5, \"rho\": 0.95},\n            \"true_params\": {\"beta\": -0.5, \"gamma\": 0.06, \"sigma_e\": 0.2, \"sigma_n\": 0.1},\n            \"grid\": {\n                \"beta\": [-0.6, -0.5],\n                \"gamma\": [0.04, 0.06],\n                \"sigma_e\": [0.18, 0.22],\n                \"sigma_n\": [0.09, 0.11],\n            },\n            \"sim_seed\": 201,\n            \"est_seed\": 301,\n        },\n        {\n            \"case_id\": 2,\n            \"fixed_params\": {\"mu\": 5.5, \"rho\": 0.9},\n            \"true_params\": {\"beta\": -0.4, \"gamma\": 0.0, \"sigma_e\": 0.15, \"sigma_n\": 0.02},\n            \"grid\": {\n                \"beta\": [-0.5, -0.4],\n                \"gamma\": [0.0, 0.02],\n                \"sigma_e\": [0.12, 0.18],\n                \"sigma_n\": [0.01, 0.03],\n            },\n            \"sim_seed\": 202,\n            \"est_seed\": 302,\n        },\n        {\n            \"case_id\": 3,\n            \"fixed_params\": {\"mu\": 5.5, \"rho\": 0.98},\n            \"true_params\": {\"beta\": -0.6, \"gamma\": 0.2, \"sigma_e\": 0.25, \"sigma_n\": 0.15},\n            \"grid\": {\n                \"beta\": [-0.7, -0.6],\n                \"gamma\": [0.18, 0.2],\n                \"sigma_e\": [0.22, 0.28],\n                \"sigma_n\": [0.13, 0.17],\n            },\n            \"sim_seed\": 203,\n            \"est_seed\": 303,\n        }\n    ]\n\n    def generate_unemployment_data(T, mu_u, phi, sigma_u, u0, seed):\n        rng = np.random.default_rng(seed)\n        u = np.zeros(T)\n        u_prev = u0\n        nu = rng.normal(0, sigma_u, T)\n        for t in range(T):\n            u_t = mu_u + phi * (u_prev - mu_u) + nu[t]\n            u[t] = u_t\n            u_prev = u_t\n        return u\n\n    def simulate_model_data(T, u_series, true_params, fixed_params, seed):\n        rng = np.random.default_rng(seed)\n        mu, rho = fixed_params[\"mu\"], fixed_params[\"rho\"]\n        beta, gamma, sigma_e, sigma_n = true_params.values()\n        \n        n_series = np.zeros(T)\n        pi_series = np.zeros(T)\n        \n        # Initial state n_0 from stationary distribution\n        sigma_n0_sq = sigma_n**2 / (1 - rho**2)\n        n_prev = rng.normal(mu, np.sqrt(sigma_n0_sq))\n\n        innov_eta = rng.normal(0, sigma_n, T)\n        innov_eps = rng.normal(0, sigma_e, T)\n\n        for t in range(T):\n            # State equation\n            n_t = mu + rho * (n_prev - mu) + innov_eta[t]\n            n_series[t] = n_t\n            \n            # Measurement equation\n            u_gap = u_series[t] - n_t\n            pi_t = beta * u_gap + gamma * (u_gap**3) + innov_eps[t]\n            pi_series[t] = pi_t\n            \n            n_prev = n_t\n            \n        return pi_series\n\n    def particle_filter_log_likelihood(pi_series, u_series, params, fixed_params, N, rand_draws):\n        beta, gamma, sigma_e, sigma_n = params\n        mu, rho = fixed_params[\"mu\"], fixed_params[\"rho\"]\n        T = len(pi_series)\n        z_init, z_prop, u_resample = rand_draws\n\n        # Initial state distribution\n        if rho**2 >= 1 or sigma_n <= 0 or sigma_e <= 0:\n            return -np.inf\n        sigma_n0_sq = sigma_n**2 / (1 - rho**2)\n        sigma_n0 = np.sqrt(sigma_n0_sq)\n\n        # Initialization (t=0)\n        particles = mu + sigma_n0 * z_init\n        log_likelihood = 0.0\n\n        for t in range(T):\n            # Prediction/Propagation\n            particles = mu + rho * (particles - mu) + sigma_n * z_prop[t]\n\n            # Weighting\n            u_gap = u_series[t] - particles\n            pi_expected = beta * u_gap + gamma * (u_gap**3)\n            \n            # Using scipy.stats.norm.logpdf for numerical stability with small weights\n            log_weights = norm.logpdf(pi_series[t], loc=pi_expected, scale=sigma_e)\n            \n            # Log-Likelihood update using LogSumExp trick\n            if np.all(np.isneginf(log_weights)):\n                return -np.inf\n                \n            max_log_weight = np.max(log_weights)\n            weights_stable = np.exp(log_weights - max_log_weight)\n            mean_weight = np.mean(weights_stable)\n            \n            log_likelihood += max_log_weight + np.log(mean_weight)\n            \n            # Normalization\n            normalized_weights = weights_stable / np.sum(weights_stable)\n            \n            # Resampling (Systematic)\n            if np.sum(normalized_weights) > 0:\n                positions = (np.arange(N) + u_resample[t]) / N\n                cum_weights = np.cumsum(normalized_weights)\n                indices = np.searchsorted(cum_weights, positions)\n                particles = particles[indices]\n            else: # All weights were zero.\n                return -np.inf\n\n        return log_likelihood\n\n\n    # --- Main Execution ---\n    # Step 1: Generate common unemployment data\n    u_data = generate_unemployment_data(T, U_MU, U_PHI, U_SIGMA, U_INITIAL, U_SEED)\n\n    final_results = []\n    \n    for case in test_cases:\n        # Step 2: Simulate case-specific inflation data\n        pi_data = simulate_model_data(T, u_data, case[\"true_params\"], case[\"fixed_params\"], case[\"sim_seed\"])\n\n        # Step 3: Pre-generate random numbers for the particle filter\n        rng_est = np.random.default_rng(case[\"est_seed\"])\n        rand_draws_pf = (\n            rng_est.standard_normal(N_PARTICLES),  # For initialization\n            rng_est.standard_normal((T, N_PARTICLES)),  # For propagation\n            rng_est.random(T),  # For resampling\n        )\n\n        # Step 4: Grid search for MLE\n        max_log_lik = -np.inf\n        best_params = None\n\n        param_grid = list(itertools.product(\n            case[\"grid\"][\"beta\"],\n            case[\"grid\"][\"gamma\"],\n            case[\"grid\"][\"sigma_e\"],\n            case[\"grid\"][\"sigma_n\"]\n        ))\n        \n        for params_tuple in param_grid:\n            log_lik = particle_filter_log_likelihood(\n                pi_data, u_data, params_tuple, case[\"fixed_params\"], N_PARTICLES, rand_draws_pf\n            )\n            \n            if log_lik > max_log_lik:\n                max_log_lik = log_lik\n                best_params = params_tuple\n        \n        final_results.extend(best_params)\n\n    # Format and print the final output\n    print(f\"[{','.join([f'{val:.6f}' for val in final_results])}]\")\n\nsolve()\n```", "id": "2418262"}, {"introduction": "状态空间框架具有令人难以置信的通用性，甚至可以让我们对结构性突变等离散事件进行建模。本练习旨在挑战您检测系统动态中的一个突然变化。然而，其中暗藏玄机：问题设定中的一个关键假设——状态演化过程中没有随机冲击——使得在任何给定的假设下，潜在路径都变得确定。这提供了一个关于模型分析的宝贵教训，表明有时仔细检查模型可以揭示一条更简单的解决路径，从而绕过复杂的滤波工具，并加深我们对这些工具在何种情况下才真正不可或缺的理解。[@problem_id:2418273]", "problem": "考虑一个由非线性状态空间模型生成的单变量时间序列，该模型在控制潜状态动态的时不变参数中存在一个单一的结构性断点。设潜状态表示为 $x_t$，观测序列表示为 $y_t$，其中 $t \\in \\{1,\\dots,T\\}$。模型规定如下：\n- 状态转移方程：$x_t = \\theta_t + \\rho \\, x_{t-1} + \\sigma_{\\eta} \\, \\eta_t$，其中 $\\eta_t \\sim \\mathcal{N}(0,1)$。\n- 观测方程：$y_t = \\frac{1}{2} x_t^2 + \\sigma_{\\varepsilon} \\, \\varepsilon_t$，其中 $\\varepsilon_t \\sim \\mathcal{N}(0,1)$。\n- 参数 $\\theta_t$ 的断点结构：存在一个未知断点时间 $\\tau \\in \\{1,\\dots,T\\}$，使得当 $t \\le \\tau$ 时 $\\theta_t = \\theta_1$，当 $t > \\tau$ 时 $\\theta_t = \\theta_2$。$\\tau$ 的先验分布是在 $\\{1,\\dots,T\\}$ 上的均匀分布。\n- 初始条件 $x_0$ 是已知的。\n\n你的任务是构建一个程序，对于下方的每个测试用例，该程序接收模型和由所提供的数据生成过程所蕴含的观测时间序列，并返回结构性断点时间的最大后验估计 $\\hat{\\tau} \\in \\{1,\\dots,T\\}$，结果以整数形式报告。\n\n对于所有测试用例中的数据生成，适用以下条件：\n- 状态新息的标准差为零，即 $\\sigma_{\\eta} = 0$，因此在给定参数和 $\\tau$ 的情况下，$x_t$ 的演化是确定性的。\n- 观测新息的标准差为 $\\sigma_{\\varepsilon} = 0.1$。\n- 自回归系数为 $\\rho = 0.7$。\n- 初始状态为 $x_0 = 0$。\n- 时间范围为 $T = 30$。\n- 观测噪声样本 $\\{\\varepsilon_t\\}_{t=1}^{T}$ 是固定的，由以下列表给出：\n$\\big[\\, 0.03,\\,-0.02,\\,0.01,\\,0.00,\\,-0.01,\\,0.02,\\,-0.03,\\,0.04,\\,-0.02,\\,0.01,\\,0.00,\\,-0.04,\\,0.05,\\,-0.01,\\,0.02,\\,-0.02,\\,0.03,\\,-0.03,\\,0.01,\\,0.00,\\,0.02,\\,-0.01,\\,0.04,\\,-0.02,\\,0.01,\\,-0.03,\\,0.02,\\,0.00,\\,-0.01,\\,0.03\\,\\big]$。\n- 对于每个具有指定 $(\\theta_1,\\theta_2,\\tau_{\\text{true}})$ 的测试用例，通过递推式 $x_t = \\theta_t + \\rho \\, x_{t-1}$（其中 $\\theta_t$ 由 $\\tau_{\\text{true}}$ 定义）确定性地构建潜路径 $\\{x_t\\}$，然后使用上述固定的 $\\{\\varepsilon_t\\}$，通过 $y_t = \\frac{1}{2} x_t^2 + \\sigma_{\\varepsilon} \\, \\varepsilon_t$ 定义观测序列。所得到的 $\\{y_t\\}$ 构成了你的估计过程应当使用的数据；在估计中，你不得使用 $\\{\\varepsilon_t\\}$ 或 $\\tau_{\\text{true}}$。\n\n测试套件：\n- 用例 A (样本内无断点)：$\\theta_1 = 0.5$, $\\theta_2 = 1.5$, $\\tau_{\\text{true}} = 30$。\n- 用例 B (早期断点)：$\\theta_1 = 0.5$, $\\theta_2 = 1.5$, $\\tau_{\\text{true}} = 8$。\n- 用例 C (晚期断点)：$\\theta_1 = 0.5$, $\\theta_2 = 1.5$, $\\tau_{\\text{true}} = 22$。\n\n对于每个用例，仅基于模型所蕴含的似然以及 $\\tau$ 的均匀先验，计算断点时间的最大后验估计 $\\hat{\\tau}$。你的程序应产生单行输出，其中包含按用例 A、B、C 顺序排列的三个估计断点时间，形式为用方括号括起来的逗号分隔列表，例如 $[\\hat{\\tau}_A,\\hat{\\tau}_B,\\hat{\\tau}_C]$。输出必须是整数。此问题不涉及物理单位，角度也不适用。解决方案不得读取任何外部输入；所有需要的值均在此处提供，或必须完全按照规定进行硬编码。", "solution": "该问题是有效的。这是一个关于带有结构性断点的非线性状态空间模型参数估计的适定练习。我们接下来进行求解。\n\n目标是找到未知结构性断点时间 $\\tau$ 的最大后验 (MAP) 估计。参数 $\\tau$ 可以在集合 $\\{1, \\dots, T\\}$ 中取任意整数值，其中 $T=30$ 是时间范围。MAP 估计，记作 $\\hat{\\tau}$，是在给定观测数据 $y_{1:T} = \\{y_1, \\dots, y_T\\}$ 的条件下，使后验概率最大化的 $\\tau$ 值。根据 Bayes 定理，后验概率为：\n$$\nP(\\tau | y_{1:T}) = \\frac{P(y_{1:T} | \\tau) P(\\tau)}{P(y_{1:T})}\n$$\n我们希望找到 $\\hat{\\tau} = \\arg\\max_{\\tau \\in \\{1, \\dots, T\\}} P(\\tau | y_{1:T})$。分母 $P(y_{1:T})$ 是一个与 $\\tau$ 无关的归一化常数，在优化中可以忽略。问题指定了 $\\tau$ 的一个均匀先验分布，使得对于所有 $\\tau \\in \\{1, \\dots, T\\}$ 都有 $P(\\tau) = 1/T$。由于这个先验对于 $\\tau$ 也是常数，它不影响最大值的位置。因此，MAP 估计问题简化为最大似然 (ML) 估计问题：\n$$\n\\hat{\\tau} = \\arg\\max_{\\tau \\in \\{1, \\dots, T\\}} P(y_{1:T} | \\tau)\n$$\n其中 $P(y_{1:T} | \\tau)$ 是在假设的断点时间 $\\tau$ 条件下，观测数据的似然。处理对数似然 $\\mathcal{L}(\\tau) = \\log P(y_{1:T} | \\tau)$ 通常更方便。使似然最大化的 $\\tau$ 值同样也会使对数似然最大化。\n$$\n\\hat{\\tau} = \\arg\\max_{\\tau \\in \\{1, \\dots, T\\}} \\mathcal{L}(\\tau)\n$$\n现在我们来定义似然函数。一个关键的简化条件是：状态新息的标准差为零，$\\sigma_{\\eta} = 0$。这使得对于给定的参数集，状态转移方程是确定性的。具体来说，对于任何假设的断点时间 $k \\in \\{1, \\dots, T\\}$，整个潜状态路径 $\\{x_t(k)\\}_{t=1}^T$ 都由以下递推式唯一确定：\n$$\nx_t(k) = \\theta_t(k) + \\rho x_{t-1}(k)\n$$\n初始条件为 $x_0(k) = x_0 = 0$。参数 $\\theta_t(k)$ 定义为当 $t \\le k$ 时 $\\theta_t(k) = \\theta_1$，当 $t > k$ 时 $\\theta_t(k) = \\theta_2$。因为对于给定的 $k$，潜路径是确定性的，所以无需通过滤波技术（例如粒子滤波器）进行状态估计，而如果 $\\sigma_{\\eta} > 0$，这将是必需的。\n\n观测方程为 $y_t = \\frac{1}{2} x_t^2 + \\sigma_{\\varepsilon} \\varepsilon_t$，其中 $\\varepsilon_t \\sim \\mathcal{N}(0,1)$。这意味着在给定状态 $x_t$ 的条件下，每个观测值 $y_t$ 都服从条件正态分布：\n$$\ny_t | x_t \\sim \\mathcal{N}\\left(\\mu_t, \\sigma_{\\varepsilon}^2\\right) \\quad \\text{其中} \\quad \\mu_t = \\frac{1}{2} x_t^2\n$$\n对于单个观测值 $y_t$，在由假设 $\\tau=k$ 所确定的状态 $x_t(k)$ 给定的条件下，其概率密度函数为：\n$$\np(y_t | x_t(k)) = \\frac{1}{\\sqrt{2\\pi\\sigma_{\\varepsilon}^2}} \\exp\\left( -\\frac{\\left(y_t - \\frac{1}{2} x_t(k)^2\\right)^2}{2\\sigma_{\\varepsilon}^2} \\right)\n$$\n观测误差 $\\varepsilon_t$ 在时间上是独立的。因此，序列 $y_{1:T}$ 的总似然是各个概率密度的乘积：\n$$\nP(y_{1:T} | \\tau=k) = \\prod_{t=1}^T p(y_t | x_t(k))\n$$\n相应的对数似然为：\n$$\n\\mathcal{L}(k) = \\log P(y_{1:T} | \\tau=k) = \\sum_{t=1}^T \\log p(y_t | x_t(k)) = \\sum_{t=1}^T \\left[ -\\frac{1}{2}\\log(2\\pi\\sigma_{\\varepsilon}^2) - \\frac{\\left(y_t - \\frac{1}{2} x_t(k)^2\\right)^2}{2\\sigma_{\\varepsilon}^2} \\right]\n$$\n为了最大化关于 $k$ 的 $\\mathcal{L}(k)$，我们可以忽略常数项 $-\\frac{1}{2}\\log(2\\pi\\sigma_{\\varepsilon}^2)$ 和正常数缩放因子 $1/(2\\sigma_{\\varepsilon}^2)$。因此，最大化对数似然等价于最小化观测数据与模型对 $y_t$ 均值的预测值之间的误差平方和 (SSE)：\n$$\n\\hat{\\tau} = \\arg\\min_{k \\in \\{1, \\dots, T\\}} \\sum_{t=1}^T \\left(y_t - \\frac{1}{2} x_t(k)^2\\right)^2\n$$\n估计过程如下。对于每个测试用例：\n1. 首先，根据问题说明，使用提供的真实参数 $(\\theta_1, \\theta_2, \\tau_{\\text{true}})$ 和固定的噪声序列 $\\{\\varepsilon_t\\}_{t=1}^T$ 生成观测时间序列 $\\{y_t\\}_{t=1}^T$。\n2. 然后，为了估计 $\\hat{\\tau}$，遍历从 1 到 $T=30$ 的每一个可能的候选断点时间 $k$。\n3. 对于每个候选 $k$，使用已知的 $\\theta_1, \\theta_2, \\rho$ 和 $x_0$ 的值，计算假设的确定性状态路径 $\\{x_t(k)\\}_{t=1}^T$。\n4. 计算误差平方和，$SSE(k) = \\sum_{t=1}^T (y_t - \\frac{1}{2}x_t(k)^2)^2$。\n5. 估计值 $\\hat{\\tau}$ 是产生最小 SSE 的 $k$ 值。如果多个 $k$ 值产生相同的最小 SSE，则按照惯例选择其中最小的 $k$（例如，通过 `argmin`）。\n\n对每个测试用例实施此过程，以找到相应的估计值 $\\hat{\\tau}$。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves for the maximum a posteriori estimate of a structural break time\n    in a nonlinear state-space model for three test cases.\n    \"\"\"\n\n    # Global parameters and data generation settings from the problem statement\n    sigma_eps = 0.1\n    rho = 0.7\n    x0 = 0.0\n    T = 30\n    epsilon_t_draws = np.array([\n        0.03, -0.02, 0.01, 0.00, -0.01, 0.02, -0.03, 0.04, -0.02, 0.01,\n        0.00, -0.04, 0.05, -0.01, 0.02, -0.02, 0.03, -0.03, 0.01, 0.00,\n        0.02, -0.01, 0.04, -0.02, 0.01, -0.03, 0.02, 0.00, -0.01, 0.03\n    ])\n\n    test_cases = [\n        {'theta1': 0.5, 'theta2': 1.5, 'tau_true': 30},  # Case A\n        {'theta1': 0.5, 'theta2': 1.5, 'tau_true': 8},   # Case B\n        {'theta1': 0.5, 'theta2': 1.5, 'tau_true': 22}   # Case C\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        theta1 = case['theta1']\n        theta2 = case['theta2']\n        tau_true = case['tau_true']\n\n        #\n        # Step 1: Generate the observed time series y_t for the current case.\n        # This part simulates the data that the estimation procedure will use.\n        #\n        x_true = np.zeros(T + 1)\n        x_true[0] = x0\n        y_obs = np.zeros(T)\n        \n        for t in range(1, T + 1):\n            theta_t = theta1 if t <= tau_true else theta2\n            x_true[t] = theta_t + rho * x_true[t-1]\n            # y_obs is 0-indexed, so y_t corresponds to y_obs[t-1]\n            y_obs[t-1] = 0.5 * x_true[t]**2 + sigma_eps * epsilon_t_draws[t-1]\n            \n        #\n        # Step 2: Estimate the break time tau by maximizing the likelihood.\n        # This is equivalent to minimizing the sum of squared errors (SSE).\n        # The estimator only has access to y_obs and the model structure/parameters\n        # (theta1, theta2, rho, sigma_eps), not tau_true or epsilon_t_draws.\n        #\n        \n        sse_values = []\n        possible_taus = range(1, T + 1)\n        \n        for k in possible_taus:\n            # For each candidate break time k, compute the hypothetical state path\n            # and the corresponding SSE.\n            x_hypothetical = np.zeros(T + 1)\n            x_hypothetical[0] = x0\n            current_sse = 0.0\n            \n            for t in range(1, T + 1):\n                theta_t_hyp = theta1 if t <= k else theta2\n                x_hypothetical[t] = theta_t_hyp + rho * x_hypothetical[t-1]\n                \n                y_pred = 0.5 * x_hypothetical[t]**2\n                current_sse += (y_obs[t-1] - y_pred)**2\n                \n            sse_values.append(current_sse)\n            \n        # The MAP/ML estimate for tau is the one that minimizes the SSE.\n        # np.argmin returns the 0-based index of the minimum SSE.\n        # Since possible_taus starts from 1, the estimated tau is index + 1.\n        min_sse_index = np.argmin(sse_values)\n        tau_hat = possible_taus[min_sse_index]\n        \n        results.append(tau_hat)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2418273"}]}