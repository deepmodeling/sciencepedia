{"hands_on_practices": [{"introduction": "在深入研究复杂的算法之前，掌握动态环境下最优决策的理论基础至关重要。这个经典的求职问题 [@problem_id:2388575] 探讨了个体对风险的态度如何塑造其最优策略。通过分析保留工资策略，你将建立起刻画最优行为的直觉，并理解效用理论对动态选择的深远影响。", "problem": "考虑一个离散时间、无限期界、离散状态和行动空间的求职问题。时间由 $t \\in \\{0,1,2,\\dots\\}$ 索引，并以因子 $\\beta \\in (0,1)$ 进行贴现。失业工人在每个时期获得失业救济金 $b \\ge 0$。在每个失业时期，工人观察到一个工资报价 $w \\in \\mathcal{W} \\equiv \\{w_1,\\dots,w_N\\}$，其中 $\\mathcal{W}$ 是一个包含 $N \\in \\mathbb{N}$ 个元素的有限集，并且 $0 < w_1 < \\dots < w_N$。工资报价在时间上是独立同分布的，其已知概率为 $\\mathbb{P}(w = w_i) = p_i$，其中对所有 $i \\in \\{1,\\dots,N\\}$ 都有 $p_i > 0$ 且 $\\sum_{i=1}^N p_i = 1$。\n\n状态空间是离散的：它包括与当前报价配对的失业状态 $(U,w_i)$（其中 $i \\in \\{1,\\dots,N\\}$）和就业状态 $(E,w_i)$（其中 $i \\in \\{1,\\dots,N\\}$）。行动空间是离散的：在状态 $(U,w)$ 下观察到 $w \\in \\mathcal{W}$ 时，工人选择一个行动 $a \\in \\{\\text{接受}, \\text{拒绝}\\}$。如果工人接受了 $w$，他将以工资 $w$ 永久就业，没有离职，并在之后的每个时期获得 $w$。如果工人拒绝了，他将保持失业状态，在当前时期获得 $b$，并在下一时期抽取新的工资报价。\n\n时期效用为 $u(c)$，其中 $u:\\mathbb{R}_+ \\to \\mathbb{R}$ 是一个严格递增且严格凹的函数。消费等于每个时期的收入，因此在失业时 $c = b$，在就业时 $c = w$。工人选择一个平稳马尔可夫策略，以在无限期界内最大化期望贴现效用。\n\n与基准的风险中性情况 $u(c) = c$ 相比，以下关于风险规避（严格凹的 $u$）下的最优策略的陈述哪一项是正确的？\n\nA. 最优策略仍然是关于 $w$ 的保留工资规则（接受所有高于 $\\mathcal{W}$ 中某个阈值的 $w$），并且保留工资弱低于风险中性情况下的保留工资。\n\nB. 最优策略仍然是关于 $w$ 的保留工资规则，但保留工资弱高于风险中性情况下的保留工资。\n\nC. 风险规避破坏了策略对 $w$ 的单调性，因此在 $\\mathcal{W}$ 上不存在保留工资阈值。\n\nD. 在风险规避下，无论工资报价的分布或 $b$ 的值如何，工人都总是会接受 $\\mathcal{W}$ 中的任何报价。\n\nE. 当在决策前观察到工资报价时，风险偏好不影响最优策略，因此风险规避和风险中性的策略是一致的。", "solution": "首先应对问题陈述进行严格的验证。\n\n### 步骤1：提取已知条件\n- **时间**：离散的，$t \\in \\{0, 1, 2, \\dots\\}$，无限期界。\n- **贴现因子**：$\\beta \\in (0,1)$。\n- **失业状态**：工人处于失业状态，获得救济金 $b \\ge 0$。\n- **工资报价**：在每个失业时期，从有限集 $\\mathcal{W} \\equiv \\{w_1, \\dots, w_N\\}$ 中抽取一个工资报价 $w$，其中 $N \\in \\mathbb{N}$ 且 $0 < w_1 < \\dots < w_N$。\n- **报价分布**：工资报价是独立同分布的（i.i.d.），已知概率为 $\\mathbb{P}(w = w_i) = p_i > 0$ 对于所有 $i \\in \\{1, \\dots, N\\}$，且 $\\sum_{i=1}^N p_i = 1$。\n- **状态空间**：离散的，由失业状态 $(U, w_i)$ 和就业状态 $(E, w_i)$ 组成，其中 $i \\in \\{1, \\dots, N\\}$。\n- **行动空间**：离散的。在状态 $(U, w)$ 下，工人选择一个行动 $a \\in \\{\\text{接受}, \\text{拒绝}\\}$。\n- **状态转移**：\n    - 如果“接受” $w$：状态永久变为 $(E, w)$。没有离职。\n    - 如果“拒绝” $w$：下一时期的状态为 $(U, \\cdot)$。在当前时期获得 $b$。\n- **效用与消费**：时期效用为 $u(c)$，其中 $u: \\mathbb{R}_+ \\to \\mathbb{R}$ 是严格递增且严格凹的（风险规避）。消费 $c$ 等于收入，即失业时为 $b$，就业时为 $w$。\n- **目标**：通过一个平稳马尔可夫策略在无限期界内最大化期望贴现效用。\n- **比较**：问题要求比较风险规避（严格凹的 $u$）下的最优策略与基准的风险中性情况（$u(c)=c$）。\n\n### 步骤2：使用提取的已知条件进行验证\n1.  **科学依据**：该问题描述了一个经典的求职模型，通常称为 McCall 模型。这是劳动经济学和宏观经济学中的一个基础模型，其坚实基础是动态规划和期望效用理论的原理。该模型是科学上合理的。\n2.  **适定性**：该问题是适定的。对于有界回报（此处效用有界，因为工资来自有限集）和贴现因子 $\\beta \\in (0,1)$ 的贴现动态规划问题，其平稳最优策略的存在性和唯一性由标准结果保证。贝尔曼算子是一个压缩映射。\n3.  **客观性**：问题以精确、客观的数学术语陈述。所有参数和函数都得到了明确定义。\n4.  **无其他缺陷**：问题设置是完整的、连贯的且非平凡的。它不存在任何其他列出的缺陷。\n\n### 步骤3：结论与行动\n问题陈述是有效的。我将继续推导解决方案。\n\n### 推导\n我们通过为风险中性和风险规避两种情况定义相关的价值函数，并使用贝尔曼方程来分析此问题。\n\n**1. 风险中性情况: $u(c) = c$**\n\n令 $V_{RN}(U)$ 为失业的期望价值（事前，即在知道本期报价之前）。令 $V_{RN}(E,w)$ 为以工资 $w$ 受雇的价值。\n以工资 $w$ 受雇的价值是永久获得 $w$ 的现值：\n$$ V_{RN}(E,w) = \\sum_{t=0}^{\\infty} \\beta^t w = \\frac{w}{1-\\beta} $$\n失业时，工人观察到一个工资报价 $w$ 并必须选择接受或拒绝。\n- 如果接受报价 $w$，其价值为 $V_{RN}(E,w) = \\frac{w}{1-\\beta}$。\n- 如果拒绝报价 $w$，工人在当前时期获得失业救济金 $b$，并在下一时期继续寻找。其续存价值为 $\\beta V_{RN}(U)$。因此，拒绝的价值为 $b + \\beta V_{RN}(U)$。\n\n在观察到报价 $w$ 后，失业的价值是这两个选项中的最大值：\n$$ V_{RN}(U,w) = \\max \\left\\{ \\frac{w}{1-\\beta}, \\ b + \\beta V_{RN}(U) \\right\\} $$\n失业的事前价值 $V_{RN}(U)$ 是对所有可能工资报价的期望：\n$$ V_{RN}(U) = \\mathbb{E}_w \\left[ \\max \\left\\{ \\frac{w}{1-\\beta}, \\ b + \\beta V_{RN}(U) \\right\\} \\right] = \\sum_{i=1}^N p_i \\max \\left\\{ \\frac{w_i}{1-\\beta}, \\ b + \\beta V_{RN}(U) \\right\\} $$\n最优策略是当且仅当 $\\frac{w}{1-\\beta} \\ge b + \\beta V_{RN}(U)$ 时接受报价 $w$。这个不等式定义了一个保留工资 $w^*_{RN}$，使得工人在接受和拒绝之间无差异。\n$$ \\frac{w^*_{RN}}{1-\\beta} = b + \\beta V_{RN}(U) \\implies w^*_{RN} = (1-\\beta) \\left( b + \\beta V_{RN}(U) \\right) $$\n$b + \\beta V_{RN}(U)$ 这一项是再多寻找一个时期的价值。由于该值相对于当前报价 $w$ 是一个常数，因此最优策略是一个保留工资规则：接受任何 $w \\ge w^*_{RN}$ 的报价，拒绝任何 $w < w^*_{RN}$ 的报价。\n将 $V_{RN}(U)$ 的表达式代入保留工资的方程，得到一个关于 $w^*_{RN}$ 的隐式方程：\n$$ w^*_{RN} = (1-\\beta) \\left( b + \\beta \\mathbb{E}_w \\left[ \\max \\left\\{ \\frac{w}{1-\\beta}, \\frac{w^*_{RN}}{1-\\beta} \\right\\} \\right] \\right) $$\n$$ w^*_{RN} = (1-\\beta)b + \\beta \\mathbb{E}_w \\left[ \\max\\{w, w^*_{RN}\\} \\right] $$\n\n**2. 风险规避情况: 严格凹的 $u(c)$**\n\n令 $V_{RA}(U)$ 和 $V_{RA}(E,w)$ 为相应的价值函数。\n以工资 $w$ 受雇的价值是：\n$$ V_{RA}(E,w) = \\sum_{t=0}^{\\infty} \\beta^t u(w) = \\frac{u(w)}{1-\\beta} $$\n当失业并观察到报价 $w$ 时：\n- 如果接受，效用为 $\\frac{u(w)}{1-\\beta}$。\n- 如果拒绝，效用为 $u(b) + \\beta V_{RA}(U)$。\n\n失业的事前价值是：\n$$ V_{RA}(U) = \\mathbb{E}_w \\left[ \\max \\left\\{ \\frac{u(w)}{1-\\beta}, \\ u(b) + \\beta V_{RA}(U) \\right\\} \\right] $$\n最优策略是如果 $\\frac{u(w)}{1-\\beta} \\ge u(b) + \\beta V_{RA}(U)$ 就接受报价 $w$。\n令 $u^*$ 为保留效用水平，由继续寻找的价值定义：$u^* = (1-\\beta)(u(b) + \\beta V_{RA}(U))$。规则是如果 $u(w) \\ge u^*$ 就接受。\n由于 $u$ 是严格递增的，存在一个反函数 $u^{-1}$。该规则等价于如果 $w \\ge u^{-1}(u^*)$ 就接受。这证实了保留工资的存在，我们称之为 $w^*_{RA} = u^{-1}(u^*)$。\n保留工资 $w^*_{RA}$ 由无差异条件定义：\n$$ \\frac{u(w^*_{RA})}{1-\\beta} = u(b) + \\beta V_{RA}(U) $$\n将 $V_{RA}(U)$ 的表达式代入，得到一个关于 $w^*_{RA}$ 的隐式方程：\n$$ \\frac{u(w^*_{RA})}{1-\\beta} = u(b) + \\beta \\mathbb{E}_w \\left[ \\max \\left\\{ \\frac{u(w)}{1-\\beta}, \\frac{u(w^*_{RA})}{1-\\beta} \\right\\} \\right] $$\n$$ u(w^*_{RA}) = (1-\\beta)u(b) + \\beta \\mathbb{E}_w \\left[ \\max\\{u(w), u(w^*_{RA})\\} \\right] $$\n\n**3. 保留工资的比较**\n\n我们必须比较 $w^*_{RN}$ 和 $w^*_{RA}$。我们将证明 $w^*_{RA} \\le w^*_{RN}$。\n从风险中性情况，我们有不动点方程：\n$$ w^*_{RN} = (1-\\beta)b + \\beta \\mathbb{E}[\\max\\{w, w^*_{RN}\\}] $$\n从风险规避情况，令 $v^* = u(w^*_{RA})$。其不动点方程是：\n$$ v^* = (1-\\beta)u(b) + \\beta \\mathbb{E}[\\max\\{u(w), v^*\\}] $$\n因为 $u$ 是严格递增的，所以 $\\max\\{u(w), u(x)\\} = u(\\max\\{w, x\\})$。因此，$v^*$ 的方程可以写成：\n$$ u(w^*_{RA}) = (1-\\beta)u(b) + \\beta \\mathbb{E}[u(\\max\\{w, w^*_{RA}\\})] $$\n\n让我们使用一个与凹函数相关的重要性质。对于任何凹函数 $u$，任何常数 $x$ 和 $\\alpha \\in [0,1]$，以及任何随机变量 $Y$，以下不等式成立：\n$$ (1-\\alpha)u(x) + \\alpha \\mathbb{E}[u(Y)] \\le u((1-\\alpha)x + \\alpha \\mathbb{E}[Y]) $$\n此不等式可以通过定义 $f(p) = u((1-p)x + p \\mathbb{E}[Y]) - ((1-p)u(x) + p \\mathbb{E}[u(Y)])$（其中 $p \\in [0,1]$）来证明。我们有 $f(0)=0$，并且根据詹森不等式，$f(1) = u(\\mathbb{E}[Y]) - \\mathbb{E}[u(Y)] \\ge 0$。其二阶导数为 $f''(p) = u''((1-p)x + p \\mathbb{E}[Y]) (\\mathbb{E}[Y]-x)^2 \\le 0$，所以 $f$ 是凹函数。一个在 $[0,1]$ 区间上的凹函数，如果其在端点处非负，则其在整个区间内都必须是非负的。因此，该不等式得到证明。\n\n现在，让我们应用这个不等式。令 $\\alpha = \\beta$，$x = b$，随机变量为 $Y = \\max\\{w, w^*_{RN}\\}$。我们得到：\n$$ (1-\\beta)u(b) + \\beta \\mathbb{E}[u(\\max\\{w, w^*_{RN}\\})] \\le u\\left((1-\\beta)b + \\beta \\mathbb{E}[\\max\\{w, w^*_{RN}\\}]\\right) $$\n右边等于来自风险中性情况不动点方程的 $u(w^*_{RN})$。所以我们有：\n$$ (1-\\beta)u(b) + \\beta \\mathbb{E}[u(\\max\\{w, w^*_{RN}\\})] \\le u(w^*_{RN}) $$\n让我们定义一个算子 $T(v) = (1-\\beta)u(b) + \\beta \\mathbb{E}[\\max\\{u(w), v\\}]$。风险规避情况下的保留效用 $u(w^*_{RA})$ 是该算子的唯一不动点，即 $T(u(w^*_{RA})) = u(w^*_{RA})$。算子 $T$ 是一个压缩映射。\n上述不等式可以写成 $T(u(w^*_{RN})) \\le u(w^*_{RN})$。\n由于 $T$ 是一个增函数且是一个压缩算子，并且 $T(u(w^*_{RN})) \\le u(w^*_{RN})$，那么其不动点必然小于或等于 $u(w^*_{RN})$。也就是说，$u(w^*_{RA}) \\le u(w^*_{RN})$。\n由于 $u$ 是一个严格递增函数，这意味着 $w^*_{RA} \\le w^*_{RN}$。\n这个不等式是弱不等式，因为如果所有报价要么非常高（总是被接受）要么非常低（总是被拒绝），那么两种策略以及由此产生的有效保留工资可能完全相同。一般而言，对于一个非退化问题，该不等式是严格的（$w^*_{RA} < w^*_{RN}$），因为 $u$ 的凹性是严格的，这意味着风险规避的代理人对确定性有更强的偏好，并愿意接受较低的工资来结束寻找工作的不确定性。\n\n### 逐项分析\n\n**A. 最优策略仍然是关于 $w$ 的保留工资规则（接受所有高于 $\\mathcal{W}$ 中某个阈值的 $w$），并且保留工资弱低于风险中性情况下的保留工资。**\n我的推导表明，风险规避代理人的最优策略是如果 $w \\ge w^*_{RA}$ 就接受，这是一个保留工资规则。我的推导也证明了 $w^*_{RA} \\le w^*_{RN}$。因此，该陈述是**正确的**。\n\n**B. 最优策略仍然是关于 $w$ 的保留工资规则，但保留工资弱高于风险中性情况下的保留工资。**\n这与推导出的 $w^*_{RA} \\le w^*_{RN}$ 结果相矛盾。直观上，风险规避的代理人比风险中性的代理人更厌恶求职的不确定性，因此更急于接受一份工作，这意味着其保留工资更低，而不是更高。该陈述是**不正确的**。\n\n**C. 风险规避破坏了策略对 $w$ 的单调性，因此在 $\\mathcal{W}$ 上不存在保留工资阈值。**\n风险规避代理人的决策规则是如果 $u(w) \\ge u^*$ 就接受，其中 $u^*$ 是一个常数。由于 $u(w)$ 对 $w$ 是严格递增的，如果报价 $w_i$ 是可接受的，那么任何报价 $w_j > w_i$ 也是可接受的，因为 $u(w_j) > u(w_i) \\ge u^*$。这保留了决策对 $w$ 的单调性，所以存在一个保留工资策略。该陈述是**不正确的**。\n\n**D. 在风险规避下，无论工资报价的分布或 $b$ 的值如何，工人都总是会接受 $\\mathcal{W}$ 中的任何报价。**\n这是一个错误的泛化。如果失业救济金 $b$ 相对于 $\\mathcal{W}$ 中的工资报价非常高，那么可能对于所有 $w_i \\in \\mathcal{W}$ 都有 $u(b) + \\beta V_{RA}(U) > \\frac{u(w_i)}{1-\\beta}$。在这种情况下，工人会拒绝所有报价。决策取决于参数 $b$、$\\beta$ 和 $w$ 的分布。该陈述是**不正确的**。\n\n**E. 当在决策前观察到工资报价时，风险偏好不影响最优策略，因此风险规避和风险中性的策略是一致的。**\n风险偏好（体现在 $u(\\cdot)$ 的曲率上）是评估这种权衡的基础。接受或拒绝的决定取决于比较一个确定的收入流的价值（如果接受）和一个关于未来收入的彩票的价值（如果拒绝）。风险规避的代理人和风险中性的代理人对这个彩票的评估是不同的。当前报价已知这一事实，并不能消除未来报价的不确定性，而这正是求职问题的关键所在。如上所示，保留工资 $w^*_{RA}$ 和 $w^*_{RN}$ 通常是不同的。该陈述是**不正确的**。", "answer": "$$\\boxed{A}$$", "id": "2388575"}, {"introduction": "在建立了对最优策略的理论理解之后，我们现在转向一个涉及经典“探索-利用”（exploration-exploitation）权衡的计算问题。在这个采矿勘探场景中 [@problem_id:2388581]，你将使用动态规划来决定何时利用已知资源，何时勘探未知领域。这项练习将为你提供解决有限期界决策问题的强大技术——反向归纳法——的实践经验，并使“信息价值”这一概念变得具体可感。", "problem": "一家矿业公司正在考虑一个网格世界地图上的有限期决策问题，该地图具有离散的单元格类型。每个单元格处于三种状态之一：未知、贫矿或富矿。时间是离散的，共包含 $T$ 期的有限期界。在每个时期，公司可以钻探恰好一个单元格。钻探一个未知的单元格会立即揭示其真实类型，并在该时期产生即时采矿收益；钻探一个已知是贫矿或富矿的单元格会在此期间产生相应的收益。钻探之后，单元格的状态会持续存在，并可在未来时期再次进行钻探。公司的目标是最大化各时期收益的预期折现总和。\n\n建模。设网格包含 $N$ 个单元格。对于一个未知单元格，其为富矿的先验概率为 $ \\theta \\in [0,1]$，其为贫矿的概率则为 $1-\\theta$。设钻探一个已知贫矿的每期收益为 $r_P \\ge 0$，钻探一个已知富矿的收益为 $r_R \\ge r_P$。钻探一个未知单元格，除了实现的采矿收益外，还会产生每次钻探 $c_U \\ge 0$ 的勘探成本。折现因子为 $\\beta \\in [0,1]$。在类型给定的条件下，每个单元格的状态与其他单元格相互独立，且类型不随时间改变。公司在 $T$ 个时期内，每期钻探一个单元格。初始状态下，所有单元格均为未知。\n\n状态、行动和转移。定义计数 $(k_R, k_P, k_U)$ 分别表示当前已知为富矿、贫矿和未知的单元格数量，因此 $k_R + k_P + k_U = N$。在状态 $(k_R, k_P, k_U)$ 下的一个行动是选择一个可用的类别进行钻探：如果 $k_R > 0$，则钻探富矿；如果 $k_P > 0$，则钻探贫矿；或者如果 $k_U > 0$，则钻探未知单元格。即时回报如下：\n- 如果钻探富矿：$r_R$。\n- 如果钻探贫矿：$r_P$。\n- 如果钻探未知单元格：有 $\\theta$ 的概率发现该单元格为富矿，立即产生 $r_R - c_U$ 的收益；有 $1-\\theta$ 的概率发现为贫矿，立即产生 $r_P - c_U$ 的收益。\n\n钻探一个未知单元格后，下一状态以概率 $\\theta$ 更新为 $(k_R+1, k_P, k_U-1)$，或以概率 $1-\\theta$ 更新为 $(k_R, k_P+1, k_U-1)$。钻探已知类型的单元格后，计数保持不变。\n\n目标。设 $V_T(k_R,k_P,k_U)$ 表示从状态 $(k_R,k_P,k_U)$ 开始，在剩余 $T$ 个时期内的最优期望值。目标是计算 $V_T(0,0,N)$，即从所有单元格都处于未知状态开始的最优预期总折现收益。预期的总收益是在 $t = 1, \\dots, T$ 各个时期的即时收益乘以 $\\beta^{t-1}$ 后的总和。\n\n任务。编写一个完整、可运行的程序，该程序针对下面给出的每组参数，通过在基于计数的状空间上使用精确动态规划来计算最优期望值 $V_T(0,0,N)$，并返回结果。\n\n测试套件。使用以下参数集，每组参数指定为 $(N, T, \\theta, r_P, r_R, c_U, \\beta)$，其中所有概率都应解释为小数（而非百分比）：\n1. $(3, 3, 0.4, 1.0, 5.0, 0.5, 0.95)$\n2. $(4, 5, 0.5, 0.5, 6.0, 0.0, 0.9)$\n3. $(2, 2, 0.3, 1.0, 4.0, 2.5, 1.0)$\n4. $(1, 3, 0.5, 0.0, 10.0, 1.0, 1.0)$\n5. $(3, 3, 0.7, 1.0, 3.0, 0.2, 0.0)$\n\n最终输出格式。您的程序应生成一行输出，其中包含以逗号分隔的浮点数列表形式的结果，四舍五入到 $6$ 位小数，并用方括号括起来，顺序与测试套件相同（例如，$[x_1,x_2,x_3,x_4,x_5]$）。不应打印任何其他文本。输出中不涉及物理单位，并且所有概率都必须作为小数而不是百分比来处理。", "solution": "对问题陈述进行验证。\n\n### 步骤 1：提取已知信息\n- **领域**：一个网格世界地图上的有限期决策问题。\n- **单元格状态**：未知、贫矿、富矿。\n- **时间期界**：$T$ 个离散时期。\n- **行动**：每期钻探一个单元格。\n- **目标**：最大化各时期收益的预期折现总和，即 $\\sum_{t=1}^{T} \\beta^{t-1} \\times (\\text{第 } t \\text{ 期的收益})$。\n- **网格大小**：$N$ 个单元格。\n- **初始状态**：所有 $N$ 个单元格均为未知。\n- **参数**：\n    - 未知单元格为富矿的先验概率：$\\theta \\in [0,1]$。\n    - 钻探贫矿的收益：$r_P \\ge 0$。\n    - 钻探富矿的收益：$r_R \\ge r_P$。\n    - 钻探未知单元格的勘探成本：$c_U \\ge 0$。\n    - 折现因子：$\\beta \\in [0,1]$。\n- **状态表示**：$(k_R, k_P, k_U)$，表示富矿、贫矿和未知单元格的数量，其中 $k_R + k_P + k_U = N$。\n- **从状态 $(k_R, k_P, k_U)$ 出发的行动**：\n    - 钻探富矿（如果 $k_R > 0$）。\n    - 钻探贫矿（如果 $k_P > 0$）。\n    - 钻探未知单元格（如果 $k_U > 0$）。\n- **即时回报**：\n    - 钻探富矿：$r_R$。\n    - 钻探贫矿：$r_P$。\n    - 钻探未知单元格：以概率 $\\theta$ 获得 $r_R - c_U$，以概率 $1-\\theta$ 获得 $r_P - c_U$。\n- **状态转移**：\n    - 钻探富矿/贫矿：状态 $(k_R, k_P, k_U)$ 保持不变。\n    - 钻探未知单元格：状态以概率 $\\theta$ 变为 $(k_R+1, k_P, k_U-1)$，或以概率 $1-\\theta$ 变为 $(k_R, k_P+1, k_U-1)$。\n- **任务**：使用精确动态规划计算从初始状态开始的最优期望值 $V_T(0,0,N)$。\n\n### 步骤 2：使用提取的已知信息进行验证\n根据指定标准对问题进行评估。\n- **科学基础**：该问题是一个经典的有限期动态规划问题，是运筹学、强化学习和计算经济学中的一个基本模型。其公式建立在公认的数学原理之上。它是有效的。\n- **适定性**：该问题是适定的。状态空间虽然大，但是有限且离散的。行动空间是有限的。时间期界是有限的。目标函数有明确定义。这些条件保证了存在一个唯一的、稳定的、有意义的最优值，并且可以通过反向归纳法进行计算。\n- **目标**：该问题使用精确、无歧义的数学语言陈述。所有参数都已定义，目标有明确的公式指定。它不含主观或非科学的主张。\n\n该问题没有表现出科学上不健全、不完整、矛盾或不可行性等任何缺陷。它是其领域内一个标准的、可解的问题。\n\n### 步骤 3：结论和行动\n问题有效。将提供一个解决方案。\n\n---\n\n该问题是一个有限期离散时间动态规划问题。解决方案是通过使用最优性原理，在每个时间点为每个状态计算价值函数来找到的。这是通过反向归纳法实现的。\n\n设状态由元组 $(t, k_R, k_P)$ 定义，其中 $t$ 是剩余的时间周期数，$k_R$ 是已知为富矿的单元格数量，$k_P$ 是已知为贫矿的单元格数量。未知单元格的数量则隐含为 $k_U = N - k_R - k_P$。价值函数 $V_t(k_R, k_P)$ 表示在剩余 $t$ 个时期的情况下，给定当前状态所能获得的最大预期未来折现收益总和。我们的目标是计算 $V_T(0, 0)$。\n\n动态规划的递归关系由 Bellman 方程定义。\n基准情况是当剩余 $t=0$ 个时期时，此时无法再采取任何行动，因此价值为 $0$。\n$$V_0(k_R, k_P) = 0 \\quad \\forall k_R, k_P \\text{ such that } k_R+k_P \\le N$$\n\n对于 $t > 0$，价值函数 $V_t(k_R, k_P)$ 是在可用行动集合 $A(k_R, k_P)$ 上可获得的最大值。\n$$V_t(k_R, k_P) = \\max_{a \\in A(k_R, k_P)} \\left\\{ R(k_R, k_P, a) + \\beta \\, \\mathbb{E}[V_{t-1}(k'_R, k'_P) \\mid k_R, k_P, a] \\right\\}$$\n其中 $R(k_R, k_P, a)$ 是在当前状态下采取行动 $a$ 的即时期望收益，期望值是针对下一状态 $(k'_R, k'_P)$ 计算的。\n\n可能的行动及其对应的值如下：\n1. **钻探一个富矿**（如果 $k_R > 0$ 则可用）：\n即时收益为 $r_R$。状态不发生改变。此行动的价值是：\n$$v_{\\text{drill_R}} = r_R + \\beta V_{t-1}(k_R, k_P)$$\n\n2. **钻探一个贫矿**（如果 $k_P > 0$ 则可用）：\n即时收益为 $r_P$。状态不发生改变。此行动的价值是：\n$$v_{\\text{drill_P}} = r_P + \\beta V_{t-1}(k_R, k_P)$$\n\n3. **钻探一个未知单元格**（如果 $k_U = N - k_R - k_P > 0$ 则可用）：\n即时期望收益为 $\\theta (r_R - c_U) + (1-\\theta) (r_P - c_U) = \\theta r_R + (1-\\theta)r_P - c_U$。下一状态以概率 $\\theta$ 变为 $(k_R+1, k_P)$，或以概率 $1-\\theta$ 变为 $(k_R, k_P+1)$。此行动的价值是：\n$$v_{\\text{drill_U}} = (\\theta r_R + (1-\\theta)r_P - c_U) + \\beta \\left( \\theta V_{t-1}(k_R+1, k_P) + (1-\\theta) V_{t-1}(k_R, k_P+1) \\right)$$\n\n状态 $(t, k_R, k_P)$ 的价值函数是所有可用行动价值的最大值。例如，如果 $k_R > 0$、$k_P > 0$ 且 $k_U > 0$，则：\n$$V_t(k_R, k_P) = \\max\\{v_{\\text{drill_R}}, v_{\\text{drill_P}}, v_{\\text{drill_U}}\\}$$\n\n算法通过反向归纳法进行：\n1. 初始化一个维度为 $(T+1) \\times (N+1) \\times (N+1)$ 的表 $V$，用于存储 $V_t(k_R, k_P)$ 的值。将所有 $V_0(k_R, k_P)$ 设置为 $0$。\n2. 将 $t$ 从 $1$ 迭代到 $T$。\n3. 对于每个 $t$，遍历所有有效状态 $(k_R, k_P)$，其中 $k_R \\in [0, N]$ 且 $k_P \\in [0, N-k_R]$。\n4. 对于每个状态，使用从 $V_{t-1}$ 表中预先计算的值来计算所有可用行动的价值。\n5. 将这些价值的最大值存储在 $V_t(k_R, k_P)$ 中。\n6. 最终结果是初始状态下的价值：$V_T(0, 0)$。\n\n这个过程保证了能够计算出最优的预期总折现收益。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve_case(N, T, theta, r_P, r_R, c_U, beta):\n    \"\"\"\n    Computes the optimal expected value for a single set of parameters\n    using dynamic programming.\n    \"\"\"\n    # V[t, k_R, k_P] stores the max expected value with t periods remaining,\n    # given k_R known rich cells and k_P known poor cells.\n    V = np.zeros((T + 1, N + 1, N + 1))\n\n    # Iterate forwards in time remaining, from t=1 to T.\n    # This corresponds to backward induction in calendar time.\n    for t in range(1, T + 1):\n        for k_R in range(N + 1):\n            for k_P in range(N - k_R + 1):\n                k_U = N - k_R - k_P\n                \n                # A list to store the value of each possible action\n                action_values = []\n                \n                # Action: Drill a known Rich Deposit (if available)\n                if k_R > 0:\n                    # Immediate payoff is r_R. State does not change.\n                    # Continuation value depends on V[t-1] for the same state.\n                    val_drill_R = r_R + beta * V[t - 1, k_R, k_P]\n                    action_values.append(val_drill_R)\n                \n                # Action: Drill a known Poor Deposit (if available)\n                if k_P > 0:\n                    # Immediate payoff is r_P. State does not change.\n                    val_drill_P = r_P + beta * V[t - 1, k_R, k_P]\n                    action_values.append(val_drill_P)\n                \n                # Action: Drill an Unknown cell (if available)\n                if k_U > 0:\n                    # Expected immediate payoff from exploration\n                    expected_immediate_payoff = theta * r_R + (1 - theta) * r_P - c_U\n                    \n                    # Expected continuation value, averaged over possible outcomes\n                    # Outcome 1: cell is Rich (prob theta), state becomes (k_R+1, k_P)\n                    # Outcome 2: cell is Poor (prob 1-theta), state becomes (k_R, k_P+1)\n                    expected_continuation_value = beta * (\n                        theta * V[t - 1, k_R + 1, k_P] +\n                        (1 - theta) * V[t - 1, k_R, k_P + 1]\n                    )\n                    val_drill_U = expected_immediate_payoff + expected_continuation_value\n                    action_values.append(val_drill_U)\n                \n                # The value of the current state is the maximum over possible actions.\n                # If N>0 and T>0, there is always at least one action.\n                if action_values:\n                    V[t, k_R, k_P] = max(action_values)\n\n    # The result is the value at the initial state: T periods remaining,\n    # 0 known Rich, 0 known Poor.\n    return V[T, 0, 0]\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the final output.\n    \"\"\"\n    # Each test case is a tuple: (N, T, theta, r_P, r_R, c_U, beta)\n    test_cases = [\n        (3, 3, 0.4, 1.0, 5.0, 0.5, 0.95),\n        (4, 5, 0.5, 0.5, 6.0, 0.0, 0.9),\n        (2, 2, 0.3, 1.0, 4.0, 2.5, 1.0),\n        (1, 3, 0.5, 0.0, 10.0, 1.0, 1.0),\n        (3, 3, 0.7, 1.0, 3.0, 0.2, 0.0),\n    ]\n\n    results = []\n    for case in test_cases:\n        result = solve_case(*case)\n        # Round the result to 6 decimal places as required.\n        results.append(f\"{result:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "2388581"}, {"introduction": "我们之前的例子都假设环境的模型是已知的。但如果模型未知，我们该怎么办？这项练习 [@problem_id:2388619] 将向你介绍一种强大的无模型（model-free）强化学习算法——$Q$-学习。通过从零开始构建一个自主交易代理，你将学会如何直接从经验中推导最优策略，从而解决一个需要通过与环境直接互动来发现其动态规律的问题。", "problem": "您的任务是实现一个完整的、可运行的程序，该程序训练一个使用离散状态和动作空间的表格型强化学习 (RL) 代理，仅基于相对强弱指数 (RSI) 技术指标进行单只股票的交易。该程序必须遵循马尔可夫决策过程 (MDP) 的形式化，并使用源自贝尔曼最优性方程的基本原理来优化其行为，本说明中不提供捷径公式。\n\n环境定义如下。代理观察到的状态是由 RSI 状态区间和其当前头寸组成的一对。RSI 状态区间是根据价格序列计算得出的，它是一个关于回看窗口 $w$ 的函数，使用最近 $w$ 个单周期价格变化的平均收益和平均亏损的标准定义：\n- 设价格序列为 $\\{P_t\\}_{t=0}^{T-1}$，其中 $T \\geq w + 2$，并定义对于 $t \\in \\{1, \\dots, T-1\\}$ 的单步价格变化为 $\\Delta_t = P_t - P_{t-1}$。\n- 对于每个 $t \\geq w$ 的时间点，将平均收益定义为在 $k \\in \\{t-w+1, \\dots, t\\}$ 范围内 $\\max(\\Delta_k, 0)$ 的算术平均值，将平均亏损定义为在同一窗口内 $\\max(-\\Delta_k, 0)$ 的算术平均值。\n- 定义相对强度为 $RS_t = \\dfrac{\\text{平均收益}}{\\text{平均亏损}}$，并遵循以下约定以确保数学上的良定性：如果平均亏损为 $0$ 且平均收益为严格正数，则设 $RS_t = +\\infty$，这意味着相对强弱指数 (RSI) $RSI_t = 100$；如果平均收益为 $0$ 且平均亏损为严格正数，则设 $RSI_t = 0$；如果两个平均值都为 $0$，则设 $RSI_t = 50$。否则，当两个平均值均为正数时，设 $RSI_t = 100 - \\dfrac{100}{1 + RS_t}$。\n- 使用阈值 $30$ 和 $70$ 将 RSI 离散化为不同的状态区间，如下所示：如果 $RSI_t \\leq 30$，则为超卖 (Oversold)；如果 $30 < RSI_t < 70$，则为中性 (Neutral)；如果 $RSI_t \\geq 70$，则为超买 (Overbought)。\n\n在时间 $t$ 的完整离散状态是 $s_t = (r_t, p_t)$，其中 $r_t \\in \\{\\text{Oversold}, \\text{Neutral}, \\text{Overbought}\\}$ 是 RSI 状态区间，$p_t \\in \\{0,1\\}$ 是当前头寸（$0$ 表示平仓，$1$ 表示持有多头一单位）。离散动作空间是 $A = \\{\\text{Hold}, \\text{Buy}, \\text{Sell}\\}$（持有、买入、卖出）。\n\n周期内动态如下：\n- 在每个时间点 $t$（$t \\in \\{w, \\dots, T-2\\}$），代理观察状态 $s_t$，选择一个动作 $a_t \\in A$，其头寸确定性地更新：\n  - 如果 $a_t = \\text{Buy}$，则 $p_{t+} = 1$。\n  - 如果 $a_t = \\text{Sell}$，则 $p_{t+} = 0$。\n  - 如果 $a_t = \\text{Hold}$，则 $p_{t+} = p_t$。\n- 仅当 $p_{t+} \\neq p_t$ 时（即仅当头寸实际发生变化时），才会产生交易成本 $c \\geq 0$。\n- 单步奖励为 $r_t = p_{t+}\\cdot (P_{t+1} - P_t) - \\mathbf{1}\\{p_{t+} \\neq p_t\\}\\, c$。\n- 下一个状态 $s_{t+1}$ 使用下一个 RSI 状态区间和更新后的头寸 $p_{t+}$。\n\n回合 (Episode) 定义为从时间索引 $t \\in \\{w, \\dots, T-2\\}$ 进行一次从左到右的遍历，起始头寸为 $p_w = 0$。学习代理必须实现源自贝尔曼最优性原理和样本备份的表格型 $Q$ 学习。您必须使用 $\\epsilon$-贪心行为策略，在探索时对三个动作进行均匀随机探索，在利用时通过选择任何能最大化当前动作价值估计的动作来进行贪心动作选择。通过选择索引最小的动作来确定性地打破平局。对所有随机化过程使用固定的伪随机种子 $12345$，以确保结果可复现。\n\n在训练指定数量的回合后，在用于训练的相同价格路径上评估贪心策略（评估期间设置 $\\epsilon = 0$），从 $p_w = 0$ 开始，并对 $t \\in \\{w, \\dots, T-2\\}$ 使用相同的奖励构建方式。在评估结束时，如果最终头寸为 $1$，则通过卖出强制平仓，并减去一次交易成本 $c$；平仓时没有额外的价格变化。测试用例的评估指标是最终累计财富，等于评估期间的奖励总和加上结束时可能产生的强制平仓成本。不涉及物理单位。\n\n您的程序必须实现上述要求，并运行以下测试套件。对于每个测试用例，您将获得价格路径 $\\{P_t\\}$、窗口长度 $w$、学习率 $\\alpha \\in (0,1]$、折扣因子 $\\gamma \\in [0,1]$、探索率 $\\epsilon \\in [0,1]$、训练回合数 $E \\in \\mathbb{N}$ 和交易成本 $c \\geq 0$。\n\n测试套件：\n- 案例 1（带小幅回调的趋势，理想路径）：价格 $[100, 101, 102, 101, 103, 105, 104, 106, 108, 110]$, $w = 3$, $\\alpha = 0.3$, $\\gamma = 0.9$, $\\epsilon = 0.1$, $E = 200$, $c = 0.05$。\n- 案例 2（横盘且有噪声，边界 $\\gamma = 0$）：价格 $[100.0, 100.5, 99.8, 100.2, 99.7, 100.1, 99.9, 100.0, 99.8, 100.2]$, $w = 3$, $\\alpha = 0.5$, $\\gamma = 0.0$, $\\epsilon = 0.2$, $E = 300$, $c = 0.05$。\n- 案例 3（从一开始就贪心，无探索）：价格 $[10, 11, 12, 13, 14, 15]$, $w = 2$, $\\alpha = 0.3$, $\\gamma = 0.9$, $\\epsilon = 0.0$, $E = 50$, $c = 0.1$。\n- 案例 4（边缘 RSI 状态区间，包括 $0$ 和 $100$）：价格 $[50, 49, 48, 47, 46, 47, 48, 49, 50, 49]$, $w = 3$, $\\alpha = 0.4$, $\\gamma = 0.8$, $\\epsilon = 0.15$, $E = 250$, $c = 0.05$。\n\n实现要求：\n- 状态空间是离散的，由 $3$ 个 RSI 状态区间和 $2$ 个头寸状态的笛卡尔积构成，恰好有 $6$ 个状态。\n- 动作空间是离散的，恰好有 $3$ 个动作，顺序为 $\\{\\text{Hold}, \\text{Buy}, \\text{Sell}\\}$，索引为 $\\{0, 1, 2\\}$。\n- 将所有动作价值 $Q(s,a)$ 初始化为 $0$。\n- 对所有随机性使用固定的伪随机种子 $12345$。\n- 评估为所述的 $4$ 个案例中的每一个生成最终财富。\n\n最终输出格式：\n- 您的程序应产生单行输出，包含一个用方括号括起来的逗号分隔列表的结果，例如 $[x_1,x_2,x_3,x_4]$。\n- 每个 $x_i$ 都必须是精确到 $6$ 位小数的浮点数。\n\n您的任务：完全按照规定实现程序，使其按顺序运行这 $4$ 个案例，并打印上述描述的单行内容。不需要用户输入，也不允许使用外部文件。实现应是自包含的、纯算法的。确保所有随机化都使用固定的种子 $12345$，以便结果可以复现。", "solution": "所提出的问题是强化学习在金融交易场景中的一个良态应用。它具有科学依据，内部一致，并且详细说明足以支持唯一的算法解决方案。因此，我们将着手对其进行形式化分析和实现。\n\n该问题是训练一个代理，使用从 $Q$ 学习算法派生的策略进行单只股票交易。环境被建模为一个有限状态马尔可夫决策过程 (MDP)，这对于离散性质的状态和动作是合适的。一个 MDP 由一个元组 $(S, A, P, R, \\gamma)$ 正式定义，其中：\n- $S$ 是状态集合。\n- $A$ 是动作集合。\n- $P$ 是状态转移概率函数，$P(s'|s, a) = \\text{Pr}(S_{t+1}=s' | S_t=s, A_t=a)$。\n- $R$ 是奖励函数，$R(s, a, s') = \\mathbb{E}[R_{t+1} | S_t=s, A_t=a, S_{t+1}=s']$。\n- $\\gamma \\in [0, 1]$ 是未来奖励的折扣因子。\n\n在这个具体问题中，这些组成部分定义如下：\n\n状态空间 $S$ 是相对强弱指数 (RSI) 状态区间集合与可能头寸集合的笛卡尔积。设 RSI 状态区间为 $R_{reg} = \\{\\text{Oversold}, \\text{Neutral}, \\text{Overbought}\\}$，头寸为 $P_{pos} = \\{0, 1\\}$。在时间 $t$ 的状态是 $s_t = (r_t, p_t) \\in R_{reg} \\times P_{pos}$。这导致 $|S| = 3 \\times 2 = 6$ 个不同的状态。\n\n动作空间是 $A = \\{\\text{Hold}, \\text{Buy}, \\text{Sell}\\}$，一个包含 $3$ 个离散动作的集合。\n\n状态转移由代理的动作和一个外生的价格序列 $\\{P_t\\}_{t=0}^{T-1}$ 决定。给定一个状态 $s_t = (r_t, p_t)$ 和一个动作 $a_t$，代理的头寸确定性地转移到一个新的头寸 $p_{t+}$。下一个 RSI 状态区间 $r_{t+1}$ 由截至时间 $t+1$ 的价格序列决定。因此，下一个状态是 $s_{t+1} = (r_{t+1}, p_{t+})$。由于价格序列是固定的，对于任何给定的动作，转移动态是确定性的。\n\n在时间 $t$，从状态 $s_t=(r_t, p_t)$ 采取动作 $a_t$ 的奖励函数由价格变化的盈利或亏损给出，并根据交易成本进行调整。动作后的头寸 $p_{t+}$ 决定了对价格变化 $P_{t+1} - P_t$ 的敞口。奖励 $r_t$ 是：\n$$r_t = p_{t+} \\cdot (P_{t+1} - P_t) - \\mathbf{1}\\{p_{t+} \\neq p_t\\} \\cdot c$$\n其中 $c$ 是交易成本，$\\mathbf{1}\\{\\cdot\\}$ 是指示函数。\n\n代理的目标是学习一个最优策略 $\\pi^*: S \\to A$，以最大化从任何给定状态开始的期望累计折扣奖励。这是通过学习最优动作价值函数 $Q^*(s, a)$ 来实现的，$Q^*(s, a)$ 表示在状态 $s$ 采取动作 $a$ 之后，一直以最优方式行动所能获得的最大期望折扣未来奖励。$Q^*$ 函数满足贝尔曼最优性方程：\n$$Q^*(s, a) = \\mathbb{E} \\left[ R_{t+1} + \\gamma \\max_{a' \\in A} Q^*(S_{t+1}, a') \\mid S_t=s, A_t=a \\right]$$\n对于这个问题中的确定性转移（给定价格路径），期望算子是多余的。对于由动作 $a_t$ 引起的从 $s_t$ 到 $s_{t+1}$ 的转移，伴随即时奖励 $r_t$，贝尔曼方程简化为：\n$$Q^*(s_t, a_t) = r_t + \\gamma \\max_{a' \\in A} Q^*(s_{t+1}, a')$$\n$Q$ 学习是一种无模型的时序差分 (TD) 控制算法，它迭代地逼近 $Q^*(s, a)$。代理不需要先验地知道转移或奖励函数。它从样本转移 $(s_t, a_t, r_t, s_{t+1})$ 中学习。在每次这样的转移之后，动作价值表中的条目 $Q(s_t, a_t)$ 按照以下规则更新：\n$$Q(s_t, a_t) \\leftarrow Q(s_t, a_t) + \\alpha \\left( r_t + \\gamma \\max_{a' \\in A} Q(s_{t+1}, a') - Q(s_t, a_t) \\right)$$\n这里，$\\alpha \\in (0, 1]$ 是学习率，它控制更新的步长。项 $r_t + \\gamma \\max_{a'} Q(s_{t+1}, a')$ 是 TD 目标，它作为贝尔曼方程右侧的基于样本的估计。\n\n为确保对状态-动作空间的充分探索，必须选择一个允许非贪心动作的行为策略。指定的 $\\epsilon$-贪心策略通过以概率 $\\epsilon$ 选择一个随机动作，并以概率 $1-\\epsilon$ 选择贪心动作（即最大化当前 $Q(s, \\cdot)$ 的动作）来完成此任务。\n\n实现将按以下步骤进行：\n$1$。将根据指定公式预先计算给定价格序列的 RSI 值，包括对零值平均收益或亏损的约定。然后将这些连续的 RSI 值映射到三个离散的状态区间。\n$2$。状态空间和动作空间将被映射为整数索引，以便高效地使用 NumPy 数组，该数组将表示 $Q$ 表，并初始化为全零。具体来说，一个状态 $(r, p)$，其中 $r \\in \\{0, 1, 2\\}$ 和 $p \\in \\{0, 1\\}$，将被映射到一个索引 $i_s = r \\cdot 2 + p$。动作将被索引为 $0, 1, 2$。$Q$ 表的大小将为 $6 \\times 3$。\n$3$。训练过程将迭代指定的次数 $E$ 个回合。每个回合包括对价格序列的有效时间步长从 $t=w$ 到 $t=T-2$ 的单次遍历。\n$4$。在每个回合内，代理以零头寸开始。在每个时间步 $t$，它观察状态 $s_t$，通过 $\\epsilon$-贪心策略选择一个动作 $a_t$，观察奖励 $r_t$ 和下一个状态 $s_{t+1}$，并使用 $Q$ 学习规则更新 $Q$ 表。\n$5$。训练完成后，对学习到的策略进行评估。评估遵循相同的时间轨迹，但 $\\epsilon=0$，这意味着代理总是相对于学习到的 $Q$ 值贪婪地行动。在此遍历过程中累计总财富。如果在评估期结束时代理持有头寸，则应用最终的平仓成本。最终累计财富是性能指标。\n\n所有随机化都由一个固定的种子控制，以确保可复现性，这对于科学验证是强制性的。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and runs the Q-learning trading agent problem for the given test suite.\n    \"\"\"\n    test_cases = [\n        {\n            \"prices\": [100.0, 101.0, 102.0, 101.0, 103.0, 105.0, 104.0, 106.0, 108.0, 110.0],\n            \"w\": 3, \"alpha\": 0.3, \"gamma\": 0.9, \"epsilon\": 0.1, \"E\": 200, \"c\": 0.05\n        },\n        {\n            \"prices\": [100.0, 100.5, 99.8, 100.2, 99.7, 100.1, 99.9, 100.0, 99.8, 100.2],\n            \"w\": 3, \"alpha\": 0.5, \"gamma\": 0.0, \"epsilon\": 0.2, \"E\": 300, \"c\": 0.05\n        },\n        {\n            \"prices\": [10.0, 11.0, 12.0, 13.0, 14.0, 15.0],\n            \"w\": 2, \"alpha\": 0.3, \"gamma\": 0.9, \"epsilon\": 0.0, \"E\": 50, \"c\": 0.1\n        },\n        {\n            \"prices\": [50.0, 49.0, 48.0, 47.0, 46.0, 47.0, 48.0, 49.0, 50.0, 49.0],\n            \"w\": 3, \"alpha\": 0.4, \"gamma\": 0.8, \"epsilon\": 0.15, \"E\": 250, \"c\": 0.05\n        }\n    ]\n\n    results = []\n    \n    # --- Mappings ---\n    # RSI Regimes: {0: Oversold, 1: Neutral, 2: Overbought}\n    # Positions: {0: Flat, 1: Long}\n    # States: (rsi_regime, position_state) -> rsi_regime * 2 + position_state\n    # Actions: {0: Hold, 1: Buy, 2: Sell}\n    NUM_STATES = 6\n    NUM_ACTIONS = 3\n    \n    for case in test_cases:\n        prices_np = np.array(case[\"prices\"], dtype=np.float64)\n        w = case[\"w\"]\n        alpha = case[\"alpha\"]\n        gamma = case[\"gamma\"]\n        epsilon = case[\"epsilon\"]\n        E = case[\"E\"]\n        c = case[\"c\"]\n        T = len(prices_np)\n\n        # --- 1. Pre-calculate RSI and Regimes ---\n        rsi_values = np.full(T, np.nan)\n        deltas = prices_np[1:] - prices_np[:-1]\n        \n        for t in range(w, T):\n            window_deltas = deltas[t-w:t]\n            gains = np.maximum(window_deltas, 0)\n            losses = np.maximum(-window_deltas, 0)\n            \n            avg_gain = np.mean(gains)\n            avg_loss = np.mean(losses)\n\n            if avg_loss == 0:\n                if avg_gain == 0:\n                    rsi_values[t] = 50.0\n                else:\n                    rsi_values[t] = 100.0\n            else:\n                rs = avg_gain / avg_loss\n                rsi_values[t] = 100.0 - (100.0 / (1.0 + rs))\n\n        rsi_regimes = np.full(T, -1, dtype=int)\n        rsi_regimes[rsi_values <= 30] = 0  # Oversold\n        rsi_regimes[(rsi_values > 30) & (rsi_values < 70)] = 1 # Neutral\n        rsi_regimes[rsi_values >= 70] = 2  # Overbought\n\n        # --- 2. Training Phase ---\n        q_table = np.zeros((NUM_STATES, NUM_ACTIONS))\n        rng = np.random.RandomState(12345)\n\n        for _ in range(E):\n            current_pos = 0\n            for t in range(w, T - 1):\n                # Current state\n                current_rsi_regime = rsi_regimes[t]\n                current_state_idx = current_rsi_regime * 2 + current_pos\n                \n                # Action selection (epsilon-greedy)\n                if rng.rand() < epsilon:\n                    action_idx = rng.randint(0, NUM_ACTIONS)\n                else:\n                    # Break ties by choosing the smallest index\n                    q_values = q_table[current_state_idx, :]\n                    action_idx = np.argmax(q_values)\n                \n                # State transition and reward\n                prev_pos = current_pos\n                if action_idx == 0: # Hold\n                    next_pos = prev_pos\n                elif action_idx == 1: # Buy\n                    next_pos = 1\n                else: # Sell\n                    next_pos = 0\n                \n                transaction_cost = c if next_pos != prev_pos else 0.0\n                reward = next_pos * (prices_np[t+1] - prices_np[t]) - transaction_cost\n\n                # Next state\n                next_rsi_regime = rsi_regimes[t+1]\n                next_state_idx = next_rsi_regime * 2 + next_pos\n                \n                # Q-table update\n                old_q_value = q_table[current_state_idx, action_idx]\n                next_max_q = np.max(q_table[next_state_idx, :])\n                td_target = reward + gamma * next_max_q\n                new_q_value = old_q_value + alpha * (td_target - old_q_value)\n                q_table[current_state_idx, action_idx] = new_q_value\n\n                # Update position for next step in episode\n                current_pos = next_pos\n        \n        # --- 3. Evaluation Phase ---\n        total_wealth = 0.0\n        current_pos = 0\n\n        for t in range(w, T - 1):\n            # Current state\n            current_rsi_regime = rsi_regimes[t]\n            current_state_idx = current_rsi_regime * 2 + current_pos\n\n            # Action selection (greedy)\n            q_values = q_table[current_state_idx, :]\n            action_idx = np.argmax(q_values)\n\n            # State transition and reward\n            prev_pos = current_pos\n            if action_idx == 0: # Hold\n                next_pos = prev_pos\n            elif action_idx == 1: # Buy\n                next_pos = 1\n            else: # Sell\n                next_pos = 0\n            \n            transaction_cost = c if next_pos != prev_pos else 0.0\n            reward = next_pos * (prices_np[t+1] - prices_np[t]) - transaction_cost\n            total_wealth += reward\n\n            # Update position for next step in evaluation\n            current_pos = next_pos\n        \n        # Final liquidation\n        if current_pos == 1:\n            total_wealth -= c\n\n        results.append(round(total_wealth, 6))\n\n    formatted_results = [f\"{r:.6f}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "2388619"}]}