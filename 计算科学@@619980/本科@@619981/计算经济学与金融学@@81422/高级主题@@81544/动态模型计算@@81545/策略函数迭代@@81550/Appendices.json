{"hands_on_practices": [{"introduction": "理论知识是基础，真正的掌握源于实践。我们首先通过一个经典的机器更换问题 [@problem_id:2419699]，来学习价值函数迭代 (Value Function Iteration)。这个练习将引导你解决一个无穷视界的随机动态规划问题，学会通过迭代贝尔曼方程来处理不确定性，并找到最优决策。", "problem": "考虑一个单一生产性机器的无穷期、离散时间决策问题。时间由 $t \\in \\{0,1,2,\\dots\\}$ 索引。在每个时期 $t$ 开始时，决策者观察机器的状况状态 $s_t \\in \\{1,2,\\dots,S\\}$ 和一个重置成本状态 $i_t \\in \\{1,2,\\dots,M\\}$。状态 $s_t$ 索引了从最好 ($s_t=1$) 到最差 ($s_t=S$) 的状况。重置成本状态 $i_t$ 决定了该时期的即时重置成本。\n\n行动集：在每个时期，决策者选择一个行动 $a_t \\in \\{0,1\\}$，其中 $a_t=1$ 表示“立即重置”，$a_t=0$ 表示“本期继续运行”。\n\n成本与动态：\n- 如果 $a_t=1$，则支付等于 $C_{i_t}$ 的即时成本，机器在下一时期被重置到最佳状况，因此 $s_{t+1}=1$，并且重置成本状态根据一个马尔可夫转移矩阵 $\\Pi \\in \\mathbb{R}^{M \\times M}$ 演化，其元素为 $\\pi_{i j} = \\mathbb{P}(i_{t+1}=j \\mid i_t=i)$。\n- 如果 $a_t=0$，则支付即时成本 $m_{s_t}$。机器在本期内发生故障的概率为 $q_{s_t}$；在这种情况下，会产生额外的即时故障惩罚 $f$，并且需要按当前时期的重置成本 $C_{i_t}$ 进行即时重置，以在下一时期恢复功能。在概率为 $1 - q_{s_t}$ 的情况下，不发生故障。下一时期的状况遵循以下规则：\n  - 如果发生故障，$s_{t+1}=1$。\n  - 如果未发生故障，$s_{t+1}=\\min\\{s_t+1, S\\}$。\n  重置成本状态 $i_t$ 始终根据 $\\Pi$ 演化，与行动 $a_t$ 和故障的发生与否无关。\n\n目标：以折扣因子 $\\beta \\in (0,1)$ 最小化期望折扣成本总和：\n$$\n\\min_{\\{a_t\\}_{t \\ge 0}} \\mathbb{E}\\left[\\sum_{t=0}^{\\infty} \\beta^t \\cdot \\text{cost}(s_t,i_t,a_t)\\right],\n$$\n其中 $\\text{cost}(s_t,i_t,1) = C_{i_t}$ 且 $\\text{cost}(s_t,i_t,0) = m_{s_t} + q_{s_t}\\,(C_{i_t} + f)$。\n\n设价值函数为 $V(s,i)$，表示从一个时期开始时状态为 $(s,i)$ 的最小期望折扣成本。最优平稳策略是一个映射 $\\pi^{\\star}(s,i) \\in \\{0,1\\}$，它在每个状态 $(s,i)$ 下都最小化贝尔曼目标。\n\n报告结果的状态排序：对于一个给定的包含 $S$ 和 $M$ 的测试用例，将状态的有序列表定义为\n$$\n[(1,1), (1,2), \\dots, (1,M), (2,1), (2,2), \\dots, (2,M), \\dots, (S,1), \\dots, (S,M)].\n$$\n您的程序必须按此顺序计算所有状态的最优平稳策略 $\\pi^{\\star}(s,i)$，并将其报告为一个整数列表（其中 $1$ 表示“立即重置”，$0$ 表示“继续运行”）。\n\n参数集测试套件：\n- 测试用例 1：\n  - $S = 4$，$M = 2$。\n  - 折扣因子 $\\beta = 0.95$。\n  - 维护成本 $m = [1.0, 2.0, 3.0, 4.0]$，对应 $s \\in \\{1,2,3,4\\}$。\n  - 故障概率 $q = [0.05, 0.10, 0.20, 0.40]$，对应 $s \\in \\{1,2,3,4\\}$。\n  - 故障惩罚 $f = 5.0$。\n  - 重置成本 $C = [8.0, 14.0]$，对应 $i \\in \\{1,2\\}$。\n  - 重置成本转移矩阵\n    $$\n    \\Pi = \\begin{bmatrix}\n    0.9 & 0.1 \\\\\n    0.2 & 0.8\n    \\end{bmatrix}.\n    $$\n- 测试用例 2：\n  - $S = 4$，$M = 2$。\n  - 折扣因子 $\\beta = 0.95$。\n  - 维护成本 $m = [0.0, 1.0, 2.0, 3.0]$，对应 $s \\in \\{1,2,3,4\\}$。\n  - 故障概率 $q = [0.20, 0.40, 0.60, 0.80]$，对应 $s \\in \\{1,2,3,4\\}$。\n  - 故障惩罚 $f = 20.0$。\n  - 重置成本 $C = [8.0, 12.0]$，对应 $i \\in \\{1,2\\}$。\n  - 重置成本转移矩阵\n    $$\n    \\Pi = \\begin{bmatrix}\n    0.85 & 0.15 \\\\\n    0.25 & 0.75\n    \\end{bmatrix}.\n    $$\n- 测试用例 3：\n  - $S = 4$，$M = 2$。\n  - 折扣因子 $\\beta = 0.90$。\n  - 维护成本 $m = [0.5, 0.6, 0.7, 0.8]$，对应 $s \\in \\{1,2,3,4\\}$。\n  - 故障概率 $q = [0.0, 0.0, 0.0, 0.0]$，对应 $s \\in \\{1,2,3,4\\}$。\n  - 故障惩罚 $f = 0.0$。\n  - 重置成本 $C = [10.0, 20.0]$，对应 $i \\in \\{1,2\\}$。\n  - 重置成本转移矩阵\n    $$\n    \\Pi = \\begin{bmatrix}\n    0.95 & 0.05 \\\\\n    0.05 & 0.95\n    \\end{bmatrix}.\n    $$\n\n所需输出：对于每个测试用例，输出一个长度为 $S \\cdot M$ 的列表，其中包含按指定状态顺序排列的最优二元决策 $\\pi^{\\star}(s,i)$。将所有测试用例的结果汇总到单行中，形如列表的列表，不含空格，例如 $[[\\dots],[\\dots],[\\dots]]$。\n\n您的程序应生成单行输出，其中包含用方括号括起来的逗号分隔列表形式的结果，格式与上述描述完全一致。", "solution": "所述问题是有效的。它构成了一个标准的、适定的、无穷期、离散时间的动态规划问题，通常称为马尔可夫决策过程 (Markov Decision Process, MDP)。所有参数、状态转移动态和成本结构都已明确定义，从而可以计算出唯一的最优平稳策略。该问题基于最优控制和计算经济学的成熟理论。\n\n目标是找到最优平稳策略 $\\pi^{\\star}(s,i)$，以最小化无穷期的总期望折扣成本。系统在任何时间 $t$ 的状态由序对 $(s_t, i_t)$ 给出，其中 $s_t \\in \\{1, 2, \\dots, S\\}$ 是机器状况，$i_t \\in \\{1, 2, \\dots, M\\}$ 是重置成本状态。决策者选择一个行动 $a_t \\in \\{0, 1\\}$，其中 $a_t=0$ 是“维持”，$a_t=1$ 是“重置”。\n\n此问题的解由价值函数 $V(s,i)$ 的贝尔曼方程刻画，该函数表示从状态 $(s,i)$ 开始的最小期望折扣成本。贝尔曼方程为：\n$$\nV(s,i) = \\min \\{ V_0(s,i), V_1(s,i) \\}\n$$\n其中 $V_a(s,i)$ 是在状态 $(s,i)$ 下采取行动 $a$ 的相关价值。这些通常被称为行动价值函数。\n\n让我们定义单周期前瞻期望价值，它取决于当前成本状态 $i$ 和下一时期机器状态 $s'$，如下：\n$$\nE(s', i) = \\sum_{j=1}^{M} \\pi_{ij} V(s', j)\n$$\n其中 $\\pi_{ij}$ 是从成本状态 $i$ 转移到成本状态 $j$ 的概率，由转移矩阵 $\\Pi$ 给出。\n\n“重置”行动 ($a=1$) 的行动价值函数由下式给出：\n$$\nV_1(s,i) = C_i + \\beta E(1, i)\n$$\n这是因为重置机器会产生即时成本 $C_i$，并在下一时期将机器状态转移到最佳状况 $s'=1$。\n\n“维持”行动 ($a=0$) 的行动价值函数更为复杂。期望即时成本为 $m_s + q_s(f+C_i)$。下一时期的机器状态取决于是否发生故障。故障以概率 $q_s$ 发生，将机器重置为状态 $s'=1$。如果不发生故障（概率为 $1-q_s$），机器状况恶化至状态 $s'=\\min\\{s+1, S\\}$。因此，“维持”的价值为：\n$$\nV_0(s,i) = \\left( m_s + q_s(f+C_i) \\right) + \\beta \\left[ q_s E(1, i) + (1-q_s) E(\\min\\{s+1, S\\}, i) \\right]\n$$\n该问题通过寻找贝尔曼算子的不动点来解决，这可以通过价值迭代法实现。该迭代算法过程如下：\n1. 初始化所有状态的价值函数，例如，在迭代 $k=0$ 时，对所有 $(s,i)$ 设 $V_k(s,i) = 0$。\n2. 对于每个后续迭代 $k=1, 2, \\dots$，使用上一轮迭代的值 $V_{k-1}$ 来更新所有状态 $(s,i)$ 的价值函数：\n$$\nV_k(s,i) = \\min \\left\\{ \\left( m_s + q_s(f+C_i) \\right) + \\beta \\left[ q_s E_{k-1}(1, i) + (1-q_s) E_{k-1}(\\min\\{s+1, S\\}, i) \\right], \\quad C_i + \\beta E_{k-1}(1, i) \\right\\}\n$$\n其中 $E_{k-1}(s',i) = \\sum_{j=1}^{M} \\pi_{ij} V_{k-1}(s', j)$。\n3. 迭代持续进行，直到价值函数收敛，即所有状态的最大变化小于预定义的容差 $\\epsilon$：$\\max_{s,i} |V_k(s,i) - V_{k-1}(s,i)| < \\epsilon$。由于折扣因子 $\\beta \\in (0,1)$，贝尔曼算子是一个收缩映射，这保证了其收敛到一个唯一的不动点 $V^\\star$。\n\n一旦获得收敛的价值函数 $V^\\star$，最优平稳策略 $\\pi^\\star(s,i)$ 就通过为每个状态选择最小化单周期贝尔曼前瞻的行动来确定：\n$$\n\\pi^\\star(s,i) = \\arg\\min_{a \\in \\{0,1\\}} V_a(s,i)\n$$\n具体而言，如果重置的价值小于维持的价值，则策略为重置。遵循平局时维持的约定：\n$$\n\\pi^\\star(s,i) = \\begin{cases} 0 & \\text{if } V_0(s,i) \\le V_1(s,i) \\\\ 1 & \\text{if } V_0(s,i) > V_1(s,i) \\end{cases}\n$$\n为了计算效率，实现将使用 NumPy 的向量化操作。价值函数 $V(s,i)$ 将存储在一个 $S \\times M$ 矩阵中。期望未来价值将通过与转移矩阵 $\\Pi$ 的转置进行矩阵乘法来计算。最优策略将从最终的价值函数中导出，并按照指定的状态排序格式化为一个扁平化列表。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef compute_optimal_policy(S, M, beta, m, q, f, C, Pi):\n    \"\"\"\n    Solves for the optimal replacement policy using value iteration.\n\n    Args:\n        S (int): Number of machine condition states.\n        M (int): Number of replacement cost states.\n        beta (float): Discount factor.\n        m (np.ndarray): Vector of maintenance costs for each condition state.\n        q (np.ndarray): Vector of breakdown probabilities for each condition state.\n        f (float): Failure penalty.\n        C (np.ndarray): Vector of replacement costs for each cost state.\n        Pi (np.ndarray): Transition matrix for replacement cost states.\n\n    Returns:\n        list: A flattened list of optimal actions (0 for keep, 1 for replace)\n              ordered by state (s, i) in row-major order.\n    \"\"\"\n    # Initialize value function\n    V = np.zeros((S, M))\n\n    # Value iteration parameters\n    tolerance = 1e-9\n    max_iterations = 10000\n\n    for _ in range(max_iterations):\n        V_old = V.copy()\n\n        # Calculate expected future values for all states (s,i)\n        # ExpectedV[s, i] = sum_{j} Pi[i, j] * V[s, j]\n        ExpectedV = V @ Pi.T\n\n        # --- Value of replacing (a=1) ---\n        # Immediate cost C_i + discounted future value from state s=1\n        # The expected future value for s=1 is the first row of ExpectedV.\n        EV_s1_vec = ExpectedV[0, :]\n        # val_replace is a 1xM row vector, will be broadcast to SxM for comparison\n        val_replace = C + beta * EV_s1_vec\n\n        # --- Value of keeping (a=0) ---\n        # Reshape parameter vectors for broadcasting\n        m_col = m.reshape(-1, 1)  # S x 1\n        q_col = q.reshape(-1, 1)  # S x 1\n        C_row = C.reshape(1, -1)  # 1 x M\n\n        # Expected immediate cost: m_s + q_s * (f + C_i)\n        immediate_cost_keep = m_col + q_col * (f + C_row)\n\n        # Expected future cost: beta * [q_s * E[V(1,j)] + (1-q_s) * E[V(min{s+1,S},j)]]\n        s_indices = np.arange(S)\n        s_next_keep_indices = np.minimum(s_indices + 1, S - 1)\n        EV_s_next_keep_matrix = ExpectedV[s_next_keep_indices, :]\n        \n        future_cost_keep = beta * (\n            q_col * EV_s1_vec.reshape(1, -1) + (1 - q_col) * EV_s_next_keep_matrix\n        )\n\n        val_keep = immediate_cost_keep + future_cost_keep\n\n        # Bellman update\n        V = np.minimum(val_keep, val_replace)\n\n        # Check for convergence\n        if np.max(np.abs(V - V_old)) < tolerance:\n            break\n    \n    # After convergence, determine the optimal policy using the final val_keep and val_replace\n    # Policy is 1 (replace) if val_keep > val_replace, 0 (keep) otherwise.\n    policy = (val_keep > val_replace).astype(int)\n\n    # Flatten the policy matrix to a list in row-major order\n    return policy.flatten().tolist()\n\ndef solve():\n    \"\"\"\n    Defines the test cases, solves for the optimal policy for each,\n    and prints the results in the required format.\n    \"\"\"\n    test_cases = [\n        {\n            \"S\": 4, \"M\": 2, \"beta\": 0.95,\n            \"m\": np.array([1.0, 2.0, 3.0, 4.0]),\n            \"q\": np.array([0.05, 0.10, 0.20, 0.40]),\n            \"f\": 5.0,\n            \"C\": np.array([8.0, 14.0]),\n            \"Pi\": np.array([[0.9, 0.1], [0.2, 0.8]]),\n        },\n        {\n            \"S\": 4, \"M\": 2, \"beta\": 0.95,\n            \"m\": np.array([0.0, 1.0, 2.0, 3.0]),\n            \"q\": np.array([0.20, 0.40, 0.60, 0.80]),\n            \"f\": 20.0,\n            \"C\": np.array([8.0, 12.0]),\n            \"Pi\": np.array([[0.85, 0.15], [0.25, 0.75]]),\n        },\n        {\n            \"S\": 4, \"M\": 2, \"beta\": 0.90,\n            \"m\": np.array([0.5, 0.6, 0.7, 0.8]),\n            \"q\": np.array([0.0, 0.0, 0.0, 0.0]),\n            \"f\": 0.0,\n            \"C\": np.array([10.0, 20.0]),\n            \"Pi\": np.array([[0.95, 0.05], [0.05, 0.95]]),\n        },\n    ]\n\n    results = []\n    for params in test_cases:\n        policy = compute_optimal_policy(\n            params[\"S\"], params[\"M\"], params[\"beta\"], params[\"m\"],\n            params[\"q\"], params[\"f\"], params[\"C\"], params[\"Pi\"]\n        )\n        results.append(policy)\n\n    # Format the final output string without spaces\n    result_strings = [f\"[{','.join(map(str, res))}]\" for res in results]\n    final_output = f\"[{','.join(result_strings)}]\"\n    \n    print(final_output)\n\nsolve()\n```", "id": "2419699"}, {"introduction": "在掌握了价值函数迭代的基础上，我们现在转向核心主题——策略函数迭代 (Policy Function Iteration, PFI)。我们将通过求解一个带有约束的新古典增长模型来实践这一方法 [@problem_id:2419733]。与价值函数迭代不同，PFI 直接对决策规则（即策略）进行迭代，通常能更快收敛；这个练习将帮助你掌握 PFI 的核心循环——策略评估和策略改进，并学会使用欧拉方程残差来检验解的准确性。", "problem": "考虑一个代表性代理人的无限期界、确定性动态规划问题，该代理人选择下一期的资本以最大化其生命周期效用。代理人的消费效用由恒定相对风险厌恶（CRRA）效用函数 $u(c)$ 给出，定义为当 $\\sigma \\neq 1$ 时，$u(c) = \\frac{c^{1-\\sigma}}{1-\\sigma}$；当 $\\sigma = 1$ 时，$u(c) = \\log(c)$。资源约束为 $c + k' = A k^{\\alpha} + (1-\\delta) k$，其中 $k$ 是当前资本，$k'$ 是下一期资本，$A$ 是全要素生产率，$\\alpha$ 是资本份额，$\\delta$ 是折旧率。代理人以因子 $\\beta \\in (0,1)$ 对未来进行贴现。下一期资本受一个偶尔约束的不等式约束 $k' \\ge \\bar{k}$ 的限制。可行性还要求非负消费，这意味着 $k' \\le A k^{\\alpha} + (1-\\delta) k$。资本的状态空间被限制在一个网格 $\\mathcal{K} = \\{k_1, k_2, \\dots, k_N\\}$ 上，其中 $k_1 = \\bar{k}$，$k_N = k_{\\max}$，该网格由 $N$ 个均匀分布的点组成。\n\n对于下面的每一组参数，计算稳态最优策略函数 $g: \\mathcal{K} \\to \\mathcal{K}$，该函数将每个网格点 $k \\in \\mathcal{K}$ 映射到同一网格上的一个最优选择 $k' = g(k)$，并满足 $k' \\in [\\bar{k}, A k^{\\alpha} + (1-\\delta) k]$ 的约束。然后定义以下诊断指标：\n\n- 诊断指标 $1$：下限约束生效的网格点比例，计算为满足 $g(k) \\le \\bar{k} + \\tau$ 的 $k \\in \\mathcal{K}$ 的比例，其中约束容差为 $\\tau = 10^{-10}$。\n\n- 诊断指标 $2$：中位网格索引处的策略值，即 $g(k_m)$，其中 $m = \\lfloor (N-1)/2 \\rfloor + 1$ 是 $\\{1,2,\\dots,N\\}$ 中的中位索引。\n\n- 诊断指标 $3$：在计算出的策略下，整个网格上欧拉方程残差的最大绝对值，对每个 $k \\in \\mathcal{K}$ 定义为\n$$\nr(k) \\equiv \\left| u'(c(k)) - \\beta \\, u'(c(g(k))) \\left(\\alpha A \\, g(k)^{\\alpha-1} + 1 - \\delta \\right) \\right|,\n$$\n其中 $c(k) = A k^{\\alpha} + (1-\\delta)k - g(k)$，$u'(c)$ 是消费的边际效用，当 $\\sigma \\neq 1$ 时为 $u'(c) = c^{-\\sigma}$，当 $\\sigma = 1$ 时为 $u'(c) = \\frac{1}{c}$。需要报告的诊断指标 $3$ 是 $\\max_{k \\in \\mathcal{K}} r(k)$。\n\n资本网格 $\\mathcal{K}$ 定义为在 $\\bar{k}$ 和 $k_{\\max}$ 之间（含两端）均匀分布的 $N$ 个点组成的集合。对于所有计算，在每个 $k \\in \\mathcal{K}$ 处使用可行性条件 $k' \\in [\\bar{k}, A k^{\\alpha} + (1-\\delta) k]$，并且仅在 $c > 0$ 时评估 $u(c)$。\n\n测试套件：\n- 测试 $1$：$(\\alpha, \\beta, \\delta, A, \\sigma, \\bar{k}, N, k_{\\max}) = (0.33, 0.96, 0.08, 1.0, 2.0, 0.01, 200, 5.0)$。\n- 测试 $2$：$(\\alpha, \\beta, \\delta, A, \\sigma, \\bar{k}, N, k_{\\max}) = (0.33, 0.96, 0.08, 1.0, 2.0, 0.50, 200, 5.0)$。\n- 测试 $3$：$(\\alpha, \\beta, \\delta, A, \\sigma, \\bar{k}, N, k_{\\max}) = (0.35, 0.94, 0.10, 0.90, 1.0, 1.50, 200, 5.0)$。\n\n您的程序必须为每个测试在指定的网格上计算最优稳态策略函数 $g$，并按顺序为每个测试返回上述三个诊断指标（作为实数）。最终输出格式必须将所有测试结果聚合到单行中，形式为包含在方括号内的逗号分隔列表，依次包含测试 $1$ 的 $3$ 个诊断指标、测试 $2$ 的 $3$ 个诊断指标以及测试 $3$ 的 $3$ 个诊断指标。\n\n最终输出格式：\n您的程序应生成单行输出，其中包含一个方括号括起来的逗号分隔列表形式的结果（例如，“[result1,result2,result3,...]”）。", "solution": "用户提供了一个来自计算宏观经济学的明确定义的问题。我将首先确认其有效性，然后提供一个完整的解决方案。\n\n### 步骤1：提取已知条件\n\n- **效用函数**：当 $\\sigma \\neq 1$ 时为 $u(c) = \\frac{c^{1-\\sigma}}{1-\\sigma}$，当 $\\sigma = 1$ 时为 $u(c) = \\log(c)$。\n- **资源约束**：$c + k' = A k^{\\alpha} + (1-\\delta) k$。\n- **参数**：全要素生产率 $A$，资本份额 $\\alpha$，折旧率 $\\delta$，CRRA 系数 $\\sigma$，以及贴现因子 $\\beta \\in (0,1)$。\n- **状态和选择变量**：当前资本 $k$（状态），下一期资本 $k'$（选择）。\n- **约束条件**：$k' \\ge \\bar{k}$（下限）和 $k' \\le A k^{\\alpha} + (1-\\delta) k$（可行性）。要求 $c>0$。\n- **状态空间**：一个离散网格 $\\mathcal{K} = \\{k_1, k_2, \\dots, k_N\\}$，它是由从 $k_1 = \\bar{k}$ 到 $k_N = k_{\\max}$ 的 $N$ 个均匀间隔的点组成的集合。\n- **策略函数**：一个稳态映射 $g: \\mathcal{K} \\to \\mathcal{K}$，使得 $k' = g(k)$。\n- **诊断指标 1**：在网格点 $k \\in \\mathcal{K}$ 中，$g(k) \\le \\bar{k} + \\tau$（容差 $\\tau = 10^{-10}$）的比例。\n- **诊断指标 2**：中位网格索引 $m = \\lfloor (N-1)/2 \\rfloor + 1$ 处的策略值 $g(k_m)$。\n- **诊断指标 3**：欧拉方程残差的最大绝对值，$\\max_{k \\in \\mathcal{K}} r(k)$，其中\n  $r(k) \\equiv \\left| u'(c(k)) - \\beta \\, u'(c(g(k))) \\left(\\alpha A \\, g(k)^{\\alpha-1} + 1 - \\delta \\right) \\right|$,\n  其中 $c(k) = A k^{\\alpha} + (1-\\delta)k - g(k)$，$u'(c)$ 是边际效用。\n- **测试用例**：\n  - 测试 1: $(\\alpha, \\beta, \\delta, A, \\sigma, \\bar{k}, N, k_{\\max}) = (0.33, 0.96, 0.08, 1.0, 2.0, 0.01, 200, 5.0)$\n  - 测试 2: $(\\alpha, \\beta, \\delta, A, \\sigma, \\bar{k}, N, k_{\\max}) = (0.33, 0.96, 0.08, 1.0, 2.0, 0.50, 200, 5.0)$\n  - 测试 3: $(\\alpha, \\beta, \\delta, A, \\sigma, \\bar{k}, N, k_{\\max}) = (0.35, 0.94, 0.10, 0.90, 1.0, 1.50, 200, 5.0)$\n\n### 步骤2：使用提取的已知条件进行验证\n\n- **科学依据**：该问题描述了在离散时间、确定性设置下的标准新古典增长模型（也称为 Ramsey-Cass-Koopmans 模型）。这是宏观经济学中的一个基础模型。所有组成部分——CRRA效用、Cobb-Douglas 生产技术、资本积累动态——都是标准的，并经过严格建立。该问题在科学上是合理的。\n- **适定性**：该问题是求解一个贴现动态规划问题的稳态最优策略函数。状态空间 $\\mathcal{K}$ 是一个紧集，对于任何可行策略，效用函数都是有界的。根据压缩映射定理，存在唯一的稳态最优策略函数。该问题是适定的。\n- **客观性**：该问题使用精确的数学定义和客观语言进行陈述。没有主观或模棱两可的术语。\n- **结论**：该问题不违反任何指定的无效标准。它是一个完整、一致且可解的科学问题。\n\n### 步骤3：裁决与行动\n\n问题陈述是**有效**的。将提供一个解决方案。\n\n### 基于原理的解决方案设计\n\n该问题要求解一个无限期界动态规划问题的稳态最优策略函数 $g(k)$。此问题的贝尔曼方程为：\n$$\nV(k) = \\max_{k' \\in \\mathcal{K} \\text{ and feasible}} \\left\\{ u(A k^{\\alpha} + (1-\\delta)k - k') + \\beta V(k') \\right\\}\n$$\n其中 $V(k)$ 是价值函数。最优策略函数 $g(k)$ 是对每个 $k$ 给出能达到最大值的 $k'$ 的函数。\n\n我们将使用**策略函数迭代（PFI）**算法来求解策略函数 $g(k)$。这种迭代方法寻找贝尔曼算子的不动点，该不动点对应于稳态最优策略。\n\nPFI 算法按以下步骤进行：\n1.  **初始化**：\n    -   将资本网格 $\\mathcal{K}$ 定义为一个包含从 $\\bar{k}$ 到 $k_{\\max}$ 的 $N$ 个点的数组。\n    -   从策略函数的一个初始猜测 $g_0(k)$ 开始。一个简单且可行的初始策略是为下一期选择最小可能资本，即对所有 $k \\in \\mathcal{K}$，令 $g_0(k) = \\bar{k}$。\n\n2.  **迭代**：重复以下两个步骤，直到策略函数收敛。设当前迭代为 $j$。\n    a. **策略评估**：给定当前策略 $g_j(k)$，计算相应的价值函数 $V_j(k)$。此价值函数是以下线性泛函方程的解：\n       $$\n       V_j(k) = u(A k^{\\alpha} + (1-\\delta)k - g_j(k)) + \\beta V_j(g_j(k))\n       $$\n       这代表了一个关于 $N$ 个值 $\\{V_j(k_i)\\}_{i=1}^N$ 的 $N$ 个线性方程组。与直接通过矩阵求逆（其计算密集度为 $O(N^3)$）求解这个系统不同，我们可以通过对固定策略下的价值函数进行迭代来找到 $V_j$：\n       $$\n       V_{j, s+1}(k) = R_j(k) + \\beta V_{j,s}(g_j(k))\n       $$\n       其中 $R_j(k) = u(A k^{\\alpha} + (1-\\delta)k - g_j(k))$ 是策略 $g_j$ 下的报酬。此内部循环会一直迭代，直到 $V_{j,s}$ 收敛到 $V_j$。\n\n    b. **策略改进**：使用评估步骤得到的价值函数 $V_j$，通过为每个状态 $k \\in \\mathcal{K}$ 求解最大化问题来找到一个改进的策略 $g_{j+1}(k)$：\n       $$\n       g_{j+1}(k) = \\underset{k' \\in \\mathcal{K} \\text{ s.t. } \\bar{k} \\le k' < Y(k)}{\\arg\\max} \\left\\{ u(Y(k) - k') + \\beta V_j(k') \\right\\}\n       $$\n       其中 $Y(k) = A k^{\\alpha} + (1-\\delta)k$ 是可用的总资源。对最优 $k'$ 的搜索是在所有满足可行性约束的网格点上进行的。严格不等式 $k' < Y(k)$ 确保消费始终为正，这是对数和 CRRA 效用函数所要求的。\n\n3.  **收敛**：当策略函数在迭代之间不再变化时，即对所有 $k \\in \\mathcal{K}$，有 $g_{j+1}(k) = g_j(k)$，算法终止。得到的策略就是稳态最优策略 $g(k)$。\n\n一旦计算出最优策略函数 $g(k)$，三个诊断指标按如下方式计算：\n- **诊断指标 1**：资本下限约束生效的网格点比例。由于策略选择 $g(k)$ 必须位于网格 $\\mathcal{K}$ 上，且第一个网格点是 $\\bar{k}$，因此对于一个很小的 $\\tau$，条件 $g(k) \\le \\bar{k} + \\tau$ 等价于 $g(k) = \\bar{k}$。我们计算使 $g(k) = \\bar{k}$ 的状态 $k \\in \\mathcal{K}$ 的比例。\n- **诊断指标 2**：中位资本网格点 $k_m$ 处的策略函数值。在问题中，索引 $m$ 是基于1的，因此对于大小为 $N$ 的基于0的数组，这对应于索引 $\\lfloor(N-1)/2\\rfloor$。我们报告 $g(k_{\\lfloor(N-1)/2\\rfloor})$。\n- **诊断指标 3**：整个网格上的最大绝对欧拉方程误差。对于每个状态 $k$，我们计算消费 $c(k) = Y(k) - g(k)$ 和下一期的消费 $c(g(k)) = Y(g(k)) - g(g(k))$。然后，我们使用提供的公式计算残差 $r(k)$，并找出在所有 $k \\in \\mathcal{K}$ 上的最大值。这可作为我们计算出的策略准确性的度量。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the dynamic programming problem for all test cases\n    and print the results in the specified format.\n    \"\"\"\n    test_cases = [\n        (0.33, 0.96, 0.08, 1.0, 2.0, 0.01, 200, 5.0),\n        (0.33, 0.96, 0.08, 1.0, 2.0, 0.50, 200, 5.0),\n        (0.35, 0.94, 0.10, 0.90, 1.0, 1.50, 200, 5.0),\n    ]\n\n    results = []\n    for params in test_cases:\n        alpha, beta, delta, A, sigma, k_bar, N, k_max = params\n        diagnostics = compute_policy_and_diagnostics(\n            alpha, beta, delta, A, sigma, k_bar, int(N), k_max\n        )\n        results.extend(diagnostics)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef compute_policy_and_diagnostics(alpha, beta, delta, A, sigma, k_bar, N, k_max):\n    \"\"\"\n    Computes the stationary optimal policy function using Policy Function Iteration\n    and calculates the required diagnostics.\n    \"\"\"\n    # 1. Setup the economic environment\n    k_grid = np.linspace(k_bar, k_max, N)\n\n    # Define utility and marginal utility functions based on sigma\n    if sigma == 1.0:\n        u = lambda c: np.log(c)\n        u_prime = lambda c: 1.0 / c\n    else:\n        u = lambda c: (c**(1.0 - sigma)) / (1.0 - sigma)\n        u_prime = lambda c: c**(-sigma)\n\n    # 2. Policy Function Iteration (PFI)\n    policy_idx = np.zeros(N, dtype=int)  # Initial policy: g(k) = k_bar for all k\n    \n    max_pfi_iter = 200\n    pfi_tol = 1e-9\n\n    V = np.zeros(N) # Initialize value function\n\n    for pfi_iter in range(max_pfi_iter):\n        # --- Policy Evaluation ---\n        # Iteratively find the value function for the current policy\n        max_v_iter = 5000\n        v_tol = 1e-9\n        \n        k_prime_eval = k_grid[policy_idx]\n        c_eval = A * k_grid**alpha + (1.0 - delta) * k_grid - k_prime_eval\n        \n        # Consumption must be positive\n        R = np.full(N, -np.inf)\n        positive_c_mask = c_eval > 0\n        R[positive_c_mask] = u(c_eval[positive_c_mask])\n        \n        for v_iter in range(max_v_iter):\n            V_next = R + beta * V[policy_idx]\n            if np.max(np.abs(V_next - V)) < v_tol:\n                break\n            V = V_next\n        else: # This 'else' belongs to the for-loop, runs if no 'break'\n            pass # Convergence not reached, but proceed with current V\n\n        # --- Policy Improvement ---\n        new_policy_idx = np.zeros(N, dtype=int)\n        y = A * k_grid**alpha + (1.0 - delta) * k_grid\n\n        for i in range(N):\n            # Find feasible choices for k' (must allow for positive consumption)\n            # k_prime < y[i]\n            valid_k_prime_end_idx = np.searchsorted(k_grid, y[i], side='left')\n\n            if valid_k_prime_end_idx == 0:\n                # No feasible choice leads to positive consumption\n                new_policy_idx[i] = 0 # Default to k_bar\n                continue\n\n            choice_indices = np.arange(valid_k_prime_end_idx)\n            k_prime_choices = k_grid[choice_indices]\n            c_choices = y[i] - k_prime_choices\n            \n            value_choices = u(c_choices) + beta * V[choice_indices]\n            \n            best_choice_local_idx = np.argmax(value_choices)\n            new_policy_idx[i] = choice_indices[best_choice_local_idx]\n\n        # --- Convergence Check ---\n        if np.max(np.abs(new_policy_idx - policy_idx)) < 1:\n            policy_idx = new_policy_idx\n            break\n        \n        policy_idx = new_policy_idx\n\n    # 3. Calculate Diagnostics\n    # Final policy g(k) = k'\n    g_k = k_grid[policy_idx]\n    \n    # Diagnostic 1: Fraction of grid points where the lower bound is binding\n    # Since g(k) is on the grid and k_bar is the first grid point,\n    # g(k) <= k_bar + tau is equivalent to g(k) == k_bar.\n    diag1 = np.mean(policy_idx == 0)\n    \n    # Diagnostic 2: Policy value at the median grid index\n    # Problem: m = floor((N-1)/2) + 1 (1-based), Python: (N-1)//2 (0-based)\n    median_idx = (N - 1) // 2\n    diag2 = g_k[median_idx]\n    \n    # Diagnostic 3: Maximum absolute Euler equation residual\n    c = A * k_grid**alpha + (1.0 - delta) * k_grid - g_k\n    \n    # Find g(g(k))\n    g_of_g_k_policy_idx = policy_idx[policy_idx]\n    g_of_g_k = k_grid[g_of_g_k_policy_idx]\n    \n    # Consumption at the next state, c' = c(g(k))\n    c_prime = A * g_k**alpha + (1.0 - delta) * g_k - g_of_g_k\n    \n    # Ensure all consumptions are positive before taking logs or negative powers\n    if np.any(c <= 0) or np.any(c_prime <= 0):\n        # This indicates a problem, as feasible policies should yield c > 0.\n        # However, to prevent crashes, we'll signal an error with a large residual.\n        # In a well-behaved model, this branch is not taken.\n        return diag1, diag2, np.inf\n\n    u_prime_c = u_prime(c)\n    u_prime_c_prime = u_prime(c_prime)\n    \n    # Marginal return on capital (k')\n    return_on_k_prime = alpha * A * g_k**(alpha - 1.0) + (1.0 - delta)\n    \n    residuals = np.abs(u_prime_c - beta * u_prime_c_prime * return_on_k_prime)\n    diag3 = np.max(residuals)\n    \n    return diag1, diag2, diag3\n\nif __name__ == '__main__':\n    solve()\n```", "id": "2419733"}, {"introduction": "为了应对更真实的经济场景，我们的最后一个实践将 PFI 应用于随机新古典增长模型 [@problem_id:2419647]。通过引入随机的生产率冲击，我们构建了一个更贴近现实的宏观经济模型，这凸显了 PFI 在处理不确定环境中的强大能力。完成这个练习后，你不仅能求解出最优策略，还将学会如何通过计算模型的稳态分布，来分析经济在长期内的宏观表现。", "problem": "给定一个关于最优资本积累的平稳无限期动态规划问题，其状态和行动空间均为离散的。状态是一个数对 $(k,z)$，其中 $k$ 是当前资本，$z$ 是当前全要素生产率。选择是下一期的资本 $k'$。每期效用函数为 $u(c)$，具有恒定相对风险厌恶（CRRA）特性，资源约束为 $c = z k^{\\alpha} + (1-\\delta)k - k'$。跨期目标是贝尔曼方程的唯一不动点\n$$\nV(k,z) = \\max_{k' \\in \\mathcal{K}} \\left\\{ u\\left(z k^{\\alpha} + (1-\\delta)k - k'\\right) + \\beta \\, \\mathbb{E}\\left[ V(k',z') \\mid z \\right] \\right\\},\n$$\n其中 $\\mathcal{K}$ 是一个有限的资本选择网格，$z$ 服从一个具有转移矩阵 $P$ 的有限状态时齐马尔可夫链。效用函数为 $u(c) = \\frac{c^{1-\\sigma}}{1-\\sigma}$（当 $c > 0$ 时）和 $u(c) = -10^{12}$（当 $c \\le 0$ 时，这是一个很大的负数，用于表示不可行性）。期望 $\\mathbb{E}[V(k',z') \\mid z]$ 是关于下一期生产率 $z'$ 的期望，以当前 $z$ 为条件，并使用给定的马尔可夫转移矩阵 $P$ 计算。行动空间被限制在网格 $\\mathcal{K}$ 上。\n\n状态空间是离散的：$z$ 在有限集 $\\mathcal{Z} = \\{z_1,\\dots,z_S\\}$ 中取值，$k$ 在网格 $\\mathcal{K} = \\{k_1,\\dots,k_N\\}$ 中取值，其中 $k_1 = \\underline{k}$，$k_N = \\overline{k}$，并且网格在这些界限之间是均匀间隔的。马尔可夫转移矩阵 $P$ 是一个 $S \\times S$ 的行随机矩阵，其元素为 $P_{ij} = \\Pr(z' = z_j \\mid z = z_i)$。\n\n对于下方的每组参数，计算：\n1. 最优平稳策略 $\\pi(k,z) \\in \\mathcal{K}$，它在给定的网格上求解贝尔曼方程。平局打破规则为：如果多个 $k'$ 达到最大值，则选择 $\\mathcal{K}$ 中最小的那个 $k'$。\n2. 与 $\\pi$ 相关联的对应价值函数 $V(k,z)$。\n3. 由 $(\\pi,P)$ 导出的在联合状态空间 $\\mathcal{K} \\times \\mathcal{Z}$ 上的马尔可夫转移矩阵 $Q$，其元素为 $Q\\left((k,z),(k',z')\\right) = \\mathbf{1}\\{k' = \\pi(k,z)\\} \\cdot P(z,z')$。\n4. 在 $\\mathcal{K} \\times \\mathcal{Z}$ 上的唯一平稳分布 $\\mu$，满足 $\\mu = \\mu Q$，该分布是通过将 $Q$ 反复应用于 $\\mathcal{K} \\times \\mathcal{Z}$ 上的初始均匀分布而得到的极限。\n5. 在平稳分布下的期望资本 $\\mathbb{E}_{\\mu}[k]$ 和期望消费 $\\mathbb{E}_{\\mu}[c]$，其中 $c(k,z) = z k^{\\alpha} + (1-\\delta)k - \\pi(k,z)$。\n6. 计算出的解的贝尔曼残差，定义为\n$$\nR = \\max_{(k,z) \\in \\mathcal{K} \\times \\mathcal{Z}} \\left| \\left( \\max_{k' \\in \\mathcal{K}} \\left\\{ u\\left(z k^{\\alpha} + (1-\\delta)k - k'\\right) + \\beta \\sum_{z' \\in \\mathcal{Z}} P(z,z') V(k',z') \\right\\} \\right) - V(k,z) \\right|.\n$$\n\n您的程序必须为测试套件中的每组参数计算以上各项。对于每组参数，生成一个包含三个实数的列表：$[\\mathbb{E}_{\\mu}[k], \\mathbb{E}_{\\mu}[c], R]$，每个条目四舍五入到 $6$ 位小数。最终的程序输出必须是单行文本，包含所有测试用例的列表串联结果，形式为单个逗号分隔的列表并用方括号括起，例如 $[x_1,x_2,x_3,x_4,x_5,x_6,\\dots]$。\n\n使用以下参数集测试套件：\n- 测试用例 1 (基准):\n  - $\\alpha = 0.35$, $\\beta = 0.96$, $\\delta = 0.08$, $\\sigma = 2.0$.\n  - $\\mathcal{Z} = \\{0.9, 1.1\\}$，转移矩阵为 $P = \\begin{bmatrix}0.9 & 0.1 \\\\ 0.1 & 0.9\\end{bmatrix}$。\n  - 资本网格 $\\mathcal{K}$：从 $\\underline{k} = 0.001$ 到 $\\overline{k} = 5.0$ 的 $N = 200$ 个均匀间隔点。\n- 测试用例 2 (高耐心):\n  - $\\alpha = 0.35$, $\\beta = 0.995$, $\\delta = 0.08$, $\\sigma = 2.0$.\n  - $\\mathcal{Z} = \\{0.9, 1.1\\}$，转移矩阵为 $P = \\begin{bmatrix}0.9 & 0.1 \\\\ 0.1 & 0.9\\end{bmatrix}$。\n  - 资本网格 $\\mathcal{K}$：从 $\\underline{k} = 0.001$ 到 $\\overline{k} = 5.0$ 的 $N = 200$ 个均匀间隔点。\n- 测试用例 3 (低波动性，不同曲率和折旧):\n  - $\\alpha = 0.35$, $\\beta = 0.96$, $\\delta = 0.10$, $\\sigma = 3.0$.\n  - $\\mathcal{Z} = \\{1.0, 1.0\\}$，转移矩阵为 $P = \\begin{bmatrix}0.5 & 0.5 \\\\ 0.5 & 0.5\\end{bmatrix}$。\n  - 资本网格 $\\mathcal{K}$：从 $\\underline{k} = 0.001$ 到 $\\overline{k} = 4.0$ 的 $N = 180$ 个均匀间隔点。\n\n角度单位不适用。不涉及物理单位。您的程序应该生成单行输出，包含一个用方括号括起来的逗号分隔列表形式的结果，条目排序为 $[\\mathbb{E}_{\\mu}[k]^{(1)},\\mathbb{E}_{\\mu}[c]^{(1)},R^{(1)},\\mathbb{E}_{\\mu}[k]^{(2)},\\mathbb{E}_{\\mu}[c]^{(2)},R^{(2)},\\mathbb{E}_{\\mu}[k]^{(3)},\\mathbb{E}_{\\mu}[c]^{(3)},R^{(3)}]$，其中上标表示测试用例的索引。每个数值条目必须四舍五入到 $6$ 位小数。", "solution": "所呈现的问题是一个规范的平稳无限期动态规划问题，具体来说，是随机最优增长模型的一个离散化版本。任务是找到最优储蓄策略并描述系统的长期行为。该问题是适定的；在给定参数下，贝尔曼算子是一个压缩映射，这保证了价值函数及相关的最优平稳策略的存在性和唯一性。我们将使用策略函数迭代（PFI）算法来解决这个问题。\n\n系统的状态是一个数对 $(k,z)$，其中 $k \\in \\mathcal{K}$ 是资本存量，$z \\in \\mathcal{Z}$ 是生产率水平。代理人选择下一期的资本存量 $k' \\in \\mathcal{K}$ 以最大化未来效用的折现和。目标是找到价值函数 $V(k,z)$，使其成为贝尔曼方程的唯一不动点：\n$$\nV(k,z) = \\max_{k' \\in \\mathcal{K}} \\left\\{ u\\left(z k^{\\alpha} + (1-\\delta)k - k'\\right) + \\beta \\, \\mathbb{E}\\left[ V(k',z') \\mid z \\right] \\right\\}\n$$\n效用函数是恒定相对风险厌恶（CRRA）形式，$u(c) = \\frac{c^{1-\\sigma}}{1-\\sigma}$，并对非正消费施以巨大的惩罚。期望是关于生产率 $z$ 的马尔可夫过程计算的，即 $\\mathbb{E}[V(k',z') \\mid z_i] = \\sum_{j=1}^S P_{ij} V(k', z_j)$。\n\n策略函数迭代（PFI）算法是一个迭代过程，通过在两个步骤之间交替进行来找到最优策略 $\\pi(k,z)$：\n\n1.  **策略评估**：对于一个给定的策略函数 $\\pi_j$，我们计算无限期遵循该策略所关联的价值 $V^{\\pi_j}$。这个价值函数是由固定策略下的贝尔曼算子定义的线性方程组的解：\n    $$\n    V^{\\pi_j}(k,z) = u\\left(z k^{\\alpha} + (1-\\delta)k - \\pi_j(k,z)\\right) + \\beta \\sum_{s'=1}^S P(z, z_{s'}) V^{\\pi_j}(\\pi_j(k,z), z_{s'})\n    $$\n    这个方程组可以通过矩阵求逆 $(I - \\beta M_{\\pi_j})^{-1} u_{\\pi_j}$ 来求解（其中 $M_{\\pi_j}$ 是由该策略导出的转移矩阵），或者，对于大型状态空间更实用的方法是，通过迭代上述算子直到价值函数收敛。我们将在每个评估步骤中采用后一种迭代方法。\n\n2.  **策略改进**：给定评估步骤得到的价值函数 $V^{\\pi_j}$，我们通过为每个状态 $(k,z)$ 解决单步最大化问题来找到一个新的、改进的策略 $\\pi_{j+1}$：\n    $$\n    \\pi_{j+1}(k,z) = \\arg\\max_{k' \\in \\mathcal{K}} \\left\\{ u\\left(z k^{\\alpha} + (1-\\delta)k - k'\\right) + \\beta \\sum_{s'=1}^S P(z, z_{s'}) V^{\\pi_j}(k', z_{s'}) \\right\\}\n    $$\n    指定的平局打破规则——在所有最优解中选择最小的 $k'$——将被应用。\n\n该算法从一个初始的策略猜测 $\\pi_0$ 开始，并重复这两个步骤，直到策略函数不再改变，即 $\\pi_{j+1} = \\pi_j$。此时，最优策略 $\\pi^*$ 及其对应的价值函数 $V^*$ 就被找到了。\n\n在获得收敛的策略 $\\pi^*$ 和价值函数 $V^*$ 之后，我们计算所需的统计数据：\n\n-   **状态转移矩阵 $Q$**：构建联合离散状态空间 $\\mathcal{K} \\times \\mathcal{Z}$ 上的马尔可夫转移矩阵。其元素为 $Q((k,z),(k',z')) = \\mathbf{1}\\{k' = \\pi^*(k,z)\\} \\cdot P(z,z')$. 这是一个 $(NS \\times NS)$ 的矩阵，其中 $N=|\\mathcal{K}|$ 且 $S=|\\mathcal{Z}|$。\n\n-   **平稳分布 $\\mu$**：唯一的平稳分布 $\\mu$ 是一个长度为 $NS$ 的行向量，满足属性 $\\mu = \\mu Q$。它使用幂法计算：从一个初始的均匀分布 $\\mu_0$ 开始，我们迭代 $\\mu_{m+1} = \\mu_m Q$ 直至收敛。\n\n-   **期望值**：将平稳分布 $\\mu$ 表示为每个状态 $(k_i, z_s)$ 的概率矩阵 $\\mu_{is}$，我们计算期望资本和期望消费：\n    $$\n    \\mathbb{E}_{\\mu}[k] = \\sum_{i=1}^N \\sum_{s=1}^S \\mu_{is} k_i\n    $$\n    $$\n    \\mathbb{E}_{\\mu}[c] = \\sum_{i=1}^N \\sum_{s=1}^S \\mu_{is} \\left( z_s k_i^{\\alpha} + (1-\\delta)k_i - k_{\\pi^*(i,s)} \\right)\n    $$\n    其中 $k_{\\pi^*(i,s)}$ 是策略函数返回的索引所对应的资本水平。\n\n-   **贝尔曼残差 $R$**：这个量度量了数值解的精度。它被定义为计算出的价值函数 $V^*$ 与再次应用完整贝尔曼算子得到的结果之间的最大绝对差：\n    $$\n    R = \\max_{(k,z) \\in \\mathcal{K} \\times \\mathcal{Z}} \\left| \\left( \\max_{k' \\in \\mathcal{K}} \\left\\{ u\\left(c(k,z,k')\\right) + \\beta \\sum_{z'} P(z,z') V^*(k',z') \\right\\} \\right) - V^*(k,z) \\right|\n    $$\n\n实现将使用向量化的 `NumPy` 操作来确保计算效率。平局打破规则由 `numpy.argmax` 自然处理，该函数返回第一个最大值的索引。", "answer": "```python\nimport numpy as np\n\ndef solve_model(alpha, beta, delta, sigma, z_grid, P, k_grid):\n    \"\"\"\n    Solves the optimal growth model using Policy Function Iteration.\n    \"\"\"\n    N = len(k_grid)\n    S = len(z_grid)\n    \n    # Utility function\n    # Note: The case sigma = 1 is not in test cases, so we ignore it.\n    def u(c):\n        utility = np.full(c.shape, -1e12)\n        positive_c = c > 0\n        utility[positive_c] = (c[positive_c]**(1 - sigma)) / (1 - sigma)\n        return utility\n\n    # --- Policy Function Iteration Setup ---\n    # V: value function (N, S)\n    # pi: policy function, stores index of k' (N, S)\n    V = np.zeros((N, S))\n    pi = np.zeros((N, S), dtype=int) \n\n    # PFI convergence parameters\n    max_pfi_iter = 200\n    pfi_iter = 0\n    policy_converged = False\n\n    # Policy Evaluation convergence parameters\n    max_eval_iter = 2000\n    eval_tol = 1e-9\n\n    # Pre-calculate consumption for all state-action pairs\n    # Shape: (N_k, N_z, N_k') -> (i, s, j)\n    k_matrix_i = k_grid[:, np.newaxis, np.newaxis]\n    z_matrix_s = z_grid[np.newaxis, :, np.newaxis]\n    k_matrix_j = k_grid[np.newaxis, np.newaxis, :]\n    \n    consumption = z_matrix_s * (k_matrix_i**alpha) + (1 - delta) * k_matrix_i - k_matrix_j\n    utility_all_choices = u(consumption)\n\n    # --- PFI Loop ---\n    while not policy_converged and pfi_iter < max_pfi_iter:\n        pfi_iter += 1\n        pi_old = pi.copy()\n\n        # 1. Policy Evaluation Step\n        V_eval = V.copy() # Use V from previous PFI step as starting point\n        for _ in range(max_eval_iter):\n            k_prime_indices = pi # Shape (N, S)\n            \n            # Expected value term: E[V(k', z')|z]\n            # V @ P.T gives E_z'[V(k, z')] for each k, z\n            EV = V_eval @ P.T # Shape (N, S)\n            \n            # Select the expected value for the policy-dictated k'\n            # EV[pi[i, s], s] for each (i,s)\n            expected_val_at_pi = EV[pi, np.arange(S)] # Shape (N, S)\n\n            # Consumption under current policy\n            k_prime_values = k_grid[k_prime_indices] # Shape (N, S)\n            c_at_pi = z_grid[None,:] * (k_grid[:,None]**alpha) + (1-delta)*k_grid[:,None] - k_prime_values\n            \n            # Value function update\n            V_new = u(c_at_pi) + beta * expected_val_at_pi\n            \n            if np.max(np.abs(V_new - V_eval)) < eval_tol:\n                break\n            V_eval = V_new\n        V = V_eval\n\n        # 2. Policy Improvement Step\n        EV = V @ P.T # Expected value, shape (N,S)\n        \n        # RHS of Bellman equation for all choices\n        # utility_all_choices (N, S, N)\n        # EV.T (S, N) -> broadcast to (N, S, N)\n        rhs_bellman = utility_all_choices + beta * EV.T[np.newaxis, :, :]\n\n        # Find new policy and new value function\n        # argmax provides the tie-breaking rule (smallest index/k')\n        pi = np.argmax(rhs_bellman, axis=2)\n        \n        # Check for convergence\n        if np.array_equal(pi, pi_old):\n            policy_converged = True\n\n    if not policy_converged:\n        print(\"Warning: Policy function iteration did not converge.\")\n\n    # --- Post-convergence calculations ---\n    # Final value function from one VFI step on optimal policy\n    EV = V @ P.T\n    rhs_bellman = utility_all_choices + beta * EV.T[np.newaxis, :, :]\n    V_final = np.max(rhs_bellman, axis=2)\n\n    # Bellman Residual\n    residual = np.max(np.abs(V_final - V))\n\n    # Construct the transition matrix Q on the joint state space\n    NS = N * S\n    Q = np.zeros((NS, NS))\n    for i in range(N):\n        for s in range(S):\n            old_idx = s * N + i\n            policy_k_idx = pi[i, s]\n            for t in range(S):\n                new_idx = t * N + policy_k_idx\n                Q[old_idx, new_idx] = P[s, t]\n\n    # Stationary Distribution mu\n    mu = np.ones(NS) / NS\n    max_mu_iter = 5000\n    mu_tol = 1e-10\n    for _ in range(max_mu_iter):\n        mu_new = mu @ Q\n        if np.max(np.abs(mu_new - mu)) < mu_tol:\n            mu = mu_new\n            break\n        mu = mu_new\n    \n    mu_matrix = mu.reshape((S, N)).T # Shape (N, S)\n\n    # Expected Capital\n    expected_k = np.sum(mu_matrix * k_grid[:, np.newaxis])\n\n    # Expected Consumption\n    k_prime_indices_opt = pi\n    k_prime_values_opt = k_grid[k_prime_indices_opt]\n    consumption_opt = z_grid[None,:] * (k_grid[:,None]**alpha) + \\\n                      (1 - delta) * k_grid[:,None] - k_prime_values_opt\n    expected_c = np.sum(mu_matrix * consumption_opt)\n    \n    return [round(expected_k, 6), round(expected_c, 6), round(residual, 6)]\n\ndef solve():\n    test_cases = [\n        {\n            \"alpha\": 0.35, \"beta\": 0.96, \"delta\": 0.08, \"sigma\": 2.0,\n            \"z_grid\": np.array([0.9, 1.1]),\n            \"P\": np.array([[0.9, 0.1], [0.1, 0.9]]),\n            \"N\": 200, \"k_min\": 0.001, \"k_max\": 5.0\n        },\n        {\n            \"alpha\": 0.35, \"beta\": 0.995, \"delta\": 0.08, \"sigma\": 2.0,\n            \"z_grid\": np.array([0.9, 1.1]),\n            \"P\": np.array([[0.9, 0.1], [0.1, 0.9]]),\n            \"N\": 200, \"k_min\": 0.001, \"k_max\": 5.0\n        },\n        {\n            \"alpha\": 0.35, \"beta\": 0.96, \"delta\": 0.10, \"sigma\": 3.0,\n            \"z_grid\": np.array([1.0, 1.0]),\n            \"P\": np.array([[0.5, 0.5], [0.5, 0.5]]),\n            \"N\": 180, \"k_min\": 0.001, \"k_max\": 4.0\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        k_grid = np.linspace(case[\"k_min\"], case[\"k_max\"], case[\"N\"])\n        \n        result_triplet = solve_model(\n            case[\"alpha\"], case[\"beta\"], case[\"delta\"], case[\"sigma\"],\n            case[\"z_grid\"], case[\"P\"], k_grid\n        )\n        results.extend(result_triplet)\n    \n    # Format the output as per problem specification\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2419647"}]}