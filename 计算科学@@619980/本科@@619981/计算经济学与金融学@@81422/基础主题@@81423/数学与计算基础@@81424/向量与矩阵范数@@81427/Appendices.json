{"hands_on_practices": [{"introduction": "虽然矩阵的某些范数（如弗罗贝尼乌斯范数）易于直接计算，但矩阵2-范数 $\\lVert A \\rVert_2$ 的计算则不那么直接。本练习将指导你实现幂迭代法，这是一种经典的数值算法，用于估算矩阵的2-范数。通过这个实践，你不仅将掌握一项核心计算技能，还将深入理解2-范数、矩阵 $A^\\top A$ 的最大特征值以及 $A$ 的最大奇异值之间的内在联系。[@problem_id:2449590]", "problem": "您的任务是设计并实现一个确定性程序，该程序使用基于幂迭代的算法来估计实矩阵 $A \\in \\mathbb{R}^{m \\times n}$ 的诱导矩阵 $2$-范数 $\\lVert A \\rVert_2$。您只能从向量和矩阵范数的基本定义以及对称矩阵特征值的基本性质出发。您的目标是推导、证明并编写一个算法，该算法不依赖于显式构造除标准矩阵-向量乘法之外的任何矩阵乘积，并且适用于方阵和矩形矩阵。\n\n需完成的任务：\n1. 从诱导矩阵 $2$-范数的核心定义 $\\lVert A \\rVert_2 = \\sup_{\\mathbf{x} \\neq \\mathbf{0}} \\frac{\\lVert A \\mathbf{x} \\rVert_2}{\\lVert \\mathbf{x} \\rVert_2}$ 以及对于任意实矩阵 $A$，$A^\\top A$ 均为对称正半定矩阵这一事实出发，推导出一个迭代方案，通过重复应用 $A \\mathbf{x}$ 和 $A^\\top \\mathbf{y}$ 形式的矩阵-向量乘法来估计 $\\lVert A \\rVert_2$，而无需显式构造 $A^\\top A$。您的推导必须基于这些定义和性质，并应包含一个基于估计值变化的明确停止准则。\n2. 将所推导的算法实现为一个完整、可运行的程序。该算法必须：\n   - 使用 $\\mathbb{R}^n$ 中的一个确定性非零向量进行初始化，在 $\\ell_2$ 意义上对其进行归一化，并在每次迭代中仅使用 $A \\mathbf{x}$ 和 $A^\\top \\mathbf{y}$ 操作。\n   - 当连续迭代中估计范数的相对变化低于容差 $\\varepsilon = 10^{-10}$ 时，或当达到最大迭代次数 $10^4$ 次时终止，以先到者为准。\n   - 稳健地处理 $A = 0$ 的边界情况，得出估计值 $\\lVert A \\rVert_2 = 0$。\n   - 返回 $\\lVert A \\rVert_2$ 的一个非负估计值。\n3. 您的程序必须评估以下矩阵测试套件，并按指定格式报告估计的范数：\n   - 情况 1 (方形，对称正定)：\n     $$A_1 = \\begin{bmatrix} 3 & 1 \\\\ 1 & 3 \\end{bmatrix}.$$\n   - 情况 2 (方形，高度非正规)：\n     $$A_2 = \\begin{bmatrix} 1 & 10 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 0.1 \\end{bmatrix}.$$\n   - 情况 3 (高矩形)：\n     $$A_3 = \\begin{bmatrix} 1 & 2 \\\\ 0 & 1 \\\\ 2 & 0 \\\\ 0 & 0 \\end{bmatrix}.$$\n   - 情况 4 (宽矩形)：\n     $$A_4 = \\begin{bmatrix} 1 & 0 & 2 & 0 \\\\ 0 & 1 & 0 & 1 \\end{bmatrix}.$$\n   - 情况 5 (零矩阵)：\n     $$A_5 = \\begin{bmatrix} 0 & 0 & 0 \\\\ 0 & 0 & 0 \\\\ 0 & 0 & 0 \\end{bmatrix}.$$\n   - 情况 6 (方形，具有近似相等的多个主导奇异值)：\n     $$A_6 = \\begin{bmatrix} 1 & 0 \\\\ 0 & 0.999 \\end{bmatrix}.$$\n4. 最终输出格式：您的程序应生成单行输出，其中包含从 $A_1$ 到 $A_6$ 的六个估计范数的结果，以逗号分隔列表的形式呈现，并用方括号括起来。每个值必须四舍五入到 8 位小数。例如，一个有效的输出格式是\n   $$[\\text{v}_1,\\text{v}_2,\\text{v}_3,\\text{v}_4,\\text{v}_5,\\text{v}_6],$$\n   其中每个 $\\text{v}_i$ 是一个四舍五入到 8 位的十进制数。不应打印任何额外的文本或空行。\n\n实现约束：\n- 程序必须完全自包含，无需用户输入，且仅使用 Python 标准库和允许的库。\n- 本问题不使用角度。\n- 不涉及物理单位。", "solution": "该问题要求推导并实现一个迭代算法，用以估计实矩阵 $A \\in \\mathbb{R}^{m \\times n}$ 的诱导矩阵 $2$-范数 $\\lVert A \\rVert_2$。推导过程必须基于第一性原理，并避免显式构造如 $A^\\top A$ 之类的矩阵乘积。\n\n首先对问题陈述进行验证。\n\n**步骤 1：提取已知条件**\n- **定义**：诱导矩阵 2-范数定义为 $\\lVert A \\rVert_2 = \\sup_{\\mathbf{x} \\neq \\mathbf{0}} \\frac{\\lVert A \\mathbf{x} \\rVert_2}{\\lVert \\mathbf{x} \\rVert_2}$。\n- **性质**：对于任何实矩阵 $A$，矩阵 $A^\\top A$ 是对称正半定矩阵。\n- **算法目标**：推导一个仅使用 $A \\mathbf{x}$ 和 $A^\\top \\mathbf{y}$ 形式的矩阵-向量乘法来估计 $\\lVert A \\rVert_2$ 的迭代方案。\n- **实现约束**：\n  - **初始化**：使用 $\\mathbb{R}^n$ 中的一个确定性的、归一化的、非零的向量。\n  - **停止准则**：当范数估计值的相对变化小于容差 $\\varepsilon = 10^{-10}$ 时，或在最多 $10^4$ 次迭代后终止。\n  - **边界情况**：正确处理零矩阵 $A = 0$，得到估计值 $0$。\n  - **返回值**：函数必须返回 $\\lVert A \\rVert_2$ 的一个非负估计值。\n- **测试套件**：提供了六个矩阵（$A_1$ 到 $A_6$）用于评估。\n- **输出格式**：单行输出，包含一个用方括号括起来的、以逗号分隔的六个估计范数列表，四舍五入到 8 位小数。\n\n**步骤 2：使用提取的已知条件进行验证**\n1.  **科学严谨性**：该问题基于诱导 2-范数的标准定义及其与 $A^\\top A$ 最大特征值的基本关系。所提出的幂迭代法是数值线性代数中用于寻找主导特征值的经典且科学可靠的算法。该问题牢固地植根于已建立的数学原理。\n2.  **适定性**：该问题是适定的。它要求估计一个唯一定义的数学量（$\\lVert A \\rVert_2$）。算法的终止由最大迭代次数限制和收敛准则保证。\n3.  **客观性**：该问题以精确、客观的数学语言陈述，没有歧义或主观论断。\n4.  **缺陷分析**：\n    - **科学或事实上的不健全**：无。前提是正确的。\n    - **非形式化或不相关**：无。该问题是计算工程和数值分析中的一个标准任务，直接涉及向量和矩阵范数。\n    - **不完整或矛盾的设置**：无。所有必要的组成部分都已指定：目标、方法的约束、终止准则和测试用例。\n    - **不切实际或不可行**：无。该算法是实用的，测试矩阵是标准示例。\n    - **病态或结构不良**：无。结构清晰，从理论推导到实现都有指导。\n    - **超出科学可验证性**：无。算法的正确性及其结果的准确性可以通过已知的解析解或标准库函数（例如，奇异值分解）进行验证。\n\n**步骤 3：结论与行动**\n该问题被判定为**有效**。将提供一个完整、合理的解决方案。\n\n**推导与算法设计**\n\n起点是矩阵 $A \\in \\mathbb{R}^{m \\times n}$ 的诱导矩阵 2-范数的定义：\n$$\n\\lVert A \\rVert_2 = \\sup_{\\mathbf{x} \\in \\mathbb{R}^n, \\mathbf{x} \\neq \\mathbf{0}} \\frac{\\lVert A \\mathbf{x} \\rVert_2}{\\lVert \\mathbf{x} \\rVert_2}\n$$\n由于范数总是非负的，我们可以考虑它的平方：\n$$\n\\lVert A \\rVert_2^2 = \\left( \\sup_{\\mathbf{x} \\neq \\mathbf{0}} \\frac{\\lVert A \\mathbf{x} \\rVert_2}{\\lVert \\mathbf{x} \\rVert_2} \\right)^2 = \\sup_{\\mathbf{x} \\neq \\mathbf{0}} \\frac{\\lVert A \\mathbf{x} \\rVert_2^2}{\\lVert \\mathbf{x} \\rVert_2^2}\n$$\n使用欧几里得范数的定义 $\\lVert \\mathbf{v} \\rVert_2^2 = \\mathbf{v}^\\top \\mathbf{v}$，我们可以将表达式重写为：\n$$\n\\lVert A \\rVert_2^2 = \\sup_{\\mathbf{x} \\neq \\mathbf{0}} \\frac{(A \\mathbf{x})^\\top (A \\mathbf{x})}{\\mathbf{x}^\\top \\mathbf{x}} = \\sup_{\\mathbf{x} \\neq \\mathbf{0}} \\frac{\\mathbf{x}^\\top A^\\top A \\mathbf{x}}{\\mathbf{x}^\\top \\mathbf{x}}\n$$\n该表达式是矩阵 $B = A^\\top A$ 的瑞利商。线性代数中的一个基本定理指出，对称矩阵的瑞利商的上确界是其最大特征值 $\\lambda_{\\text{max}}$。矩阵 $B = A^\\top A$ 确实是对称的（因为 $(A^\\top A)^\\top = A^\\top (A^\\top)^\\top = A^\\top A$）并且是正半定的。因此，我们得到了关键关系：\n$$\n\\lVert A \\rVert_2^2 = \\lambda_{\\text{max}}(A^\\top A)\n$$\n这意味着诱导矩阵 2-范数是 $A^\\top A$ 最大特征值的平方根：\n$$\n\\lVert A \\rVert_2 = \\sqrt{\\lambda_{\\text{max}}(A^\\top A)}\n$$\n根据定义，值 $\\sqrt{\\lambda_{\\text{max}}(A^\\top A)}$ 也是 $A$ 的最大奇异值，记为 $\\sigma_1(A)$。\n\n现在问题简化为在不显式计算矩阵 $A^\\top A$ 的情况下找到 $\\lambda_{\\text{max}}(A^\\top A)$。这可以使用**幂迭代**法来实现。幂迭代法是一种迭代算法，用于找到绝对值最大的特征值（主导特征值）及其对应的特征向量。对于像 $A^\\top A$ 这样的对称正半定矩阵，所有特征值都是实数且非负，因此绝对值最大的特征值就是 $\\lambda_{\\text{max}}$。\n\n矩阵 $B$ 的标准幂迭代法如下：\n1.  从一个非零向量 $\\mathbf{v}_0$ 开始。\n2.  对 $k = 1, 2, \\dots$ 进行迭代：$\\mathbf{v}_k = \\frac{B \\mathbf{v}_{k-1}}{\\lVert B \\mathbf{v}_{k-1} \\rVert_2}$。\n只要初始向量 $\\mathbf{v}_0$ 在该特征向量方向上具有非零分量，向量序列 $\\{\\mathbf{v}_k\\}$ 就会收敛到与 $\\lambda_{\\text{max}}(B)$ 对应的特征向量。\n\n在我们的例子中，$B = A^\\top A$。迭代步骤是 $\\mathbf{v}_k \\propto (A^\\top A) \\mathbf{v}_{k-1}$。按照要求，我们通过执行两个连续的矩阵-向量乘积来避免构造 $A^\\top A$：\n1.  首先，计算 $\\mathbf{y}_{k-1} = A \\mathbf{v}_{k-1}$。\n2.  然后，计算 $\\mathbf{x}_k = A^\\top \\mathbf{y}_{k-1}$。\n因此，核心更新是 $\\mathbf{x}_k = A^\\top (A \\mathbf{v}_{k-1})$。下一个归一化向量是 $\\mathbf{v}_k = \\mathbf{x}_k / \\lVert \\mathbf{x}_k \\rVert_2$。\n\n我们还需要在每次迭代中估计 $\\lambda_{\\text{max}}(A^\\top A)$。这可以从使用当前特征向量估计 $\\mathbf{v}_{k-1}$ 的瑞利商中获得：\n$$\n\\lambda_k \\approx \\frac{\\mathbf{v}_{k-1}^\\top (A^\\top A) \\mathbf{v}_{k-1}}{\\mathbf{v}_{k-1}^\\top \\mathbf{v}_{k-1}}\n$$\n由于 $\\mathbf{v}_{k-1}$ 是一个单位向量（$\\lVert \\mathbf{v}_{k-1} \\rVert_2 = 1$），其分母为 $1$。分子变为：\n$$\n\\mathbf{v}_{k-1}^\\top A^\\top A \\mathbf{v}_{k-1} = (A \\mathbf{v}_{k-1})^\\top (A \\mathbf{v}_{k-1}) = \\mathbf{y}_{k-1}^\\top \\mathbf{y}_{k-1} = \\lVert \\mathbf{y}_{k-1} \\rVert_2^2\n$$\n所以，在第 $k$ 次迭代时，最大特征值的估计值为 $\\lambda_k = \\lVert A \\mathbf{v}_{k-1} \\rVert_2^2$。\n因此，矩阵 2-范数的估计值 $\\sigma_k = \\sqrt{\\lambda_k}$ 简单地是：\n$$\n\\sigma_k = \\lVert A \\mathbf{v}_{k-1} \\rVert_2 = \\lVert \\mathbf{y}_{k-1} \\rVert_2\n$$\n这提供了一种在每次迭代中简单高效地更新范数估计值的方法。\n\n**最终算法：**\n设 $A$ 为一个 $m \\times n$ 矩阵。设 $\\varepsilon = 10^{-10}$ 为容差， $K_{\\text{max}} = 10^4$ 为最大迭代次数。\n\n1.  **处理平凡情况**：如果 $n=0$，则定义域为空，因此 $\\lVert A \\rVert_2 = 0$。\n2.  **初始化**：\n    - 选择一个确定性的非零起始向量 $\\mathbf{v}_0 \\in \\mathbb{R}^n$。一个标准的选择是全一向量。\n    - 将其归一化：$\\mathbf{v} \\leftarrow \\frac{\\mathbf{v}_0}{\\lVert \\mathbf{v}_0 \\rVert_2}$。\n    - 初始化范数估计值，例如 $\\sigma_{\\text{new}} \\leftarrow 0$。\n3.  **迭代**：对于 $k=1, \\dots, K_{\\text{max}}$：\n    a.  存储上一次的估计值：$\\sigma_{\\text{old}} \\leftarrow \\sigma_{\\text{new}}$。\n    b.  应用第一个矩阵-向量乘积：$\\mathbf{y} \\leftarrow A \\mathbf{v}$。\n    c.  更新范数估计值：$\\sigma_{\\text{new}} \\leftarrow \\lVert \\mathbf{y} \\rVert_2$。\n    d.  **检查收敛性**：如果 $k>1$ 且 $|\\sigma_{\\text{new}} - \\sigma_{\\text{old}}| < \\varepsilon \\cdot \\sigma_{\\text{new}}$，则跳出循环。\n    e.  **处理零矩阵情况**：如果 $\\sigma_{\\text{new}} = 0$，则意味着 $\\mathbf{y}=\\mathbf{0}$。这意味着 $A\\mathbf{v}=\\mathbf{0}$。$A$ 是奇异矩阵，或者可能是零矩阵。该算法正确地得出 $\\sigma_{\\text{new}} = 0$ 并应终止。我们可以跳出循环。\n    f.  应用第二个矩阵-向量乘积：$\\mathbf{x} \\leftarrow A^\\top \\mathbf{y}$。\n    g.  为下一次迭代归一化结果向量：$\\mathbf{v} \\leftarrow \\frac{\\mathbf{x}}{\\lVert \\mathbf{x} \\rVert_2}$。如果 $\\lVert \\mathbf{x} \\rVert_2 = 0$，则跳出循环。这种情况发生在 $\\mathbf{y}$ 处于 $A^\\top$ 的零空间中时，对于 $\\mathbf{y}=A\\mathbf{v}$ 这意味着 $A\\mathbf{v}=\\mathbf{0}$，从而正确地得到范数为 $0$。\n4.  **返回**：最终的估计值 $\\sigma_{\\text{new}}$。\n\n该算法仅使用矩阵-向量乘积，遵守所有约束，并能正确估计 $\\lVert A \\rVert_2$。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef estimate_induced_2_norm(A, tol=1e-10, max_iter=10000):\n    \"\"\"\n    Estimates the induced matrix 2-norm (largest singular value) of a real matrix A\n    using the power iteration method.\n\n    The algorithm iteratively computes v_k = A^T * A * v_{k-1} without explicitly\n    forming the matrix A^T*A. The norm is estimated as ||A*v_k||_2.\n\n    Args:\n        A (np.ndarray): The input matrix, m x n.\n        tol (float): The relative tolerance for convergence.\n        max_iter (int): The maximum number of iterations.\n\n    Returns:\n        float: The estimated induced 2-norm of A.\n    \"\"\"\n    # Get matrix dimensions\n    m, n = A.shape\n\n    # Handle the edge case of a matrix with zero columns.\n    if n == 0:\n        return 0.0\n\n    # Initialize with a deterministic non-zero vector in R^n.\n    # A vector of ones is a standard deterministic choice.\n    # The power method might fail if this initial vector is orthogonal\n    # to the dominant eigenvector of A^T*A. In practice, for general\n    # matrices and with finite-precision arithmetic, this is rare.\n    v = np.ones(n)\n    v /= np.linalg.norm(v)\n\n    norm_est = 0.0\n\n    for _ in range(max_iter):\n        norm_est_prev = norm_est\n\n        # First matrix-vector product: y = A*v\n        y = A @ v\n\n        # Update the norm estimate: ||A||_2 approx ||y||_2\n        norm_est = np.linalg.norm(y)\n\n        # Check for convergence using relative change.\n        # This check is safe because for a non-zero matrix, norm_est converges\n        # to a positive value.\n        if norm_est > 0 and abs(norm_est - norm_est_prev)  tol * norm_est:\n            break\n        \n        # Handle the case where A is the zero matrix or v is in the null space of A.\n        if norm_est == 0:\n            return 0.0\n\n        # Second matrix-vector product: x = A^T*y\n        x = A.T @ y\n        \n        # Normalize the vector for the next iteration.\n        norm_x = np.linalg.norm(x)\n\n        # If norm_x is zero, the iteration has converged to the null space.\n        if norm_x == 0:\n            break\n            \n        v = x / norm_x\n\n    return norm_est\n\n\ndef solve():\n    \"\"\"\n    Defines the test cases, runs the norm estimation, and prints the results.\n    \"\"\"\n    \n    A1 = np.array([[3.0, 1.0], \n                   [1.0, 3.0]])\n\n    A2 = np.array([[1.0, 10.0, 0.0],\n                   [0.0, 1.0, 0.0],\n                   [0.0, 0.0, 0.1]])\n\n    A3 = np.array([[1.0, 2.0],\n                   [0.0, 1.0],\n                   [2.0, 0.0],\n                   [0.0, 0.0]])\n\n    A4 = np.array([[1.0, 0.0, 2.0, 0.0],\n                   [0.0, 1.0, 0.0, 1.0]])\n\n    A5 = np.array([[0.0, 0.0, 0.0],\n                   [0.0, 0.0, 0.0],\n                   [0.0, 0.0, 0.0]])\n\n    A6 = np.array([[1.0, 0.0],\n                   [0.0, 0.999]])\n                   \n    test_cases = [A1, A2, A3, A4, A5, A6]\n\n    results = []\n    for A in test_cases:\n        norm_estimate = estimate_induced_2_norm(A, tol=1e-10, max_iter=10000)\n        results.append(f\"{norm_estimate:.8f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "2449590"}, {"introduction": "在计算经济学中，求解线性方程组 $Ax=b$ 是一项基本任务，但解的可靠性并非总是理所当然。本练习引导你设计一个数值实验，以直观地揭示“病态矩阵”问题：即便是输入数据 $b$ 中一个微小的扰动，也可能导致解 $x$ 出现巨大的误差。通过这个实践，你将切身体会到矩阵的条件数（一个基于范数的概念）为何是评估数值计算结果可靠性的关键指标。[@problem_id:2449583]", "problem": "设计并实现一个完整、可运行的程序，执行一个数值实验，以证明对于一个病态矩阵 $A$，线性系统 $A x = b$ 中右端项 $b$ 的一个小的相对误差可以导致解 $x$ 的一个大的相对误差。该实验必须严格基于基本原理：范数、相对误差和线性方程组求解的定义。所有量必须使用向量和矩阵的 $2$-范数。程序必须为每个指定的测试用例计算放大因子 $r$，其定义如下：\n$$\nr \\;=\\; \\frac{\\|x_{\\epsilon} - x^\\star\\|_{2} / \\|x^\\star\\|_{2}}{\\|\\delta b\\|_{2} / \\|b\\|_{2}},\n$$\n其中，$x^\\star$ 是对应于未扰动右端项 $b$ 的精确解，$\\delta b$ 是对 $b$ 的一个扰动，而 $x_{\\epsilon}$ 是 $A x = b + \\delta b$ 的解。程序必须对每个测试用例使用以下实验设置：\n- 令 $x^\\star$ 为 $\\mathbb{R}^n$ 中所有分量均为 $1$ 的向量。\n- 令 $b = A x^\\star$。\n- 令扰动方向 $v \\in \\mathbb{R}^n$ 的分量定义为 $v_i = \\frac{(-1)^{i-1}}{\\sqrt{n}}$，其中 $i = 1, \\dots, n$，因此 $\\|v\\|_2 = 1$。\n- 令 $\\delta b = \\epsilon \\, \\|b\\|_2 \\, v$，其中 $\\epsilon$ 是为该测试设定的相对扰动大小。\n- 令 $x_{\\epsilon}$ 为 $A x = b + \\delta b$ 的解。\n\n您的程序必须为每个测试用例计算放大因子 $r$ (一个浮点数)。测试用例集如下，每个用例由其用例编号标识，并按所列顺序执行：\n- 用例 $1$：$A$ 是希尔伯特矩阵 $H \\in \\mathbb{R}^{5 \\times 5}$，其元素为 $H_{ij} = \\frac{1}{i + j - 1}$，其中 $i, j \\in \\{1,\\dots,5\\}$，且 $\\epsilon = 10^{-8}$。\n- 用例 $2$：$A$ 是希尔伯特矩阵 $H \\in \\mathbb{R}^{10 \\times 10}$，其元素为 $H_{ij} = \\frac{1}{i + j - 1}$，其中 $i, j \\in \\{1,\\dots,10\\}$，且 $\\epsilon = 10^{-8}$。\n- 用例 $3$：$A$ 是单位矩阵 $I \\in \\mathbb{R}^{8 \\times 8}$，且 $\\epsilon = 10^{-8}$。\n- 用例 $4$：$A \\in \\mathbb{R}^{2 \\times 2}$ 由下式给出：\n$$\nA = \\begin{bmatrix}\n1  1 \\\\\n1  1 + 10^{-10}\n\\end{bmatrix},\n$$\n且 $\\epsilon = 10^{-12}$。\n\n所有范数必须是 $2$-范数。不涉及物理单位。不使用角度。不得使用百分比；所有比率和大小必须表示为十进制浮点数。\n\n您的程序应产生单行输出，其中包含用方括号括起来的逗号分隔列表形式的结果（例如，“[r1,r2,r3,r4]”）。每个条目必须是对应测试用例的放大因子 $r$，四舍五入到 $6$ 位有效数字，并按用例 $1$ 到 $4$ 的顺序排列。", "solution": "该实验是根据线性代数和基于范数的误差分析的基本原理设计的。我们考虑一个具有精确数据的线性系统 $A x = b$ 及其受扰动的对应系统 $A x = b + \\delta b$。对于给定的矩阵 $A \\in \\mathbb{R}^{n \\times n}$ 和精确解向量 $x^\\star \\in \\mathbb{R}^n$，我们定义 $b = A x^\\star$。我们接着按如下方式构造一个具有预设相对大小 $\\epsilon$ 的扰动 $\\delta b$。令 $v \\in \\mathbb{R}^n$ 的分量为 $v_i = \\frac{(-1)^{i-1}}{\\sqrt{n}}$，其中 $i = 1, \\dots, n$。根据构造，该向量 $v$ 满足 $\\|v\\|_2 = 1$：\n$$\n\\|v\\|_2^2 = \\sum_{i=1}^n \\left(\\frac{1}{\\sqrt{n}}\\right)^2 = \\frac{n}{n} = 1.\n$$\n我们设 $\\delta b = \\epsilon \\, \\|b\\|_2 \\, v$。那么 $b$ 中的相对扰动恰好是 $\\frac{\\|\\delta b\\|_2}{\\|b\\|_2} = \\epsilon$，因为\n$$\n\\|\\delta b\\|_2 = \\epsilon \\, \\|b\\|_2 \\, \\|v\\|_2 = \\epsilon \\, \\|b\\|_2.\n$$\n令 $x_\\epsilon$ 表示扰动系统 $A x = b + \\delta b$ 的解。解的误差等于\n$$\nx_\\epsilon - x^\\star = A^{-1}\\,(b+\\delta b) - A^{-1} b = A^{-1}\\,\\delta b.\n$$\n因此 $x$ 的相对误差为\n$$\n\\frac{\\|x_\\epsilon - x^\\star\\|_2}{\\|x^\\star\\|_2} = \\frac{\\|A^{-1}\\,\\delta b\\|_2}{\\|x^\\star\\|_2}.\n$$\n程序为每个用例报告的放大因子 $r$ 是\n$$\nr = \\frac{\\|x_{\\epsilon} - x^\\star\\|_{2} / \\|x^\\star\\|_{2}}{\\|\\delta b\\|_{2} / \\|b\\|_{2}}.\n$$\n根据范数的性质和矩阵 $2$-范数的定义，我们可以将此放大因子与矩阵 $2$-范数下的条件数 $\\kappa_2(A) = \\|A\\|_2 \\, \\|A^{-1}\\|_2$ 联系起来。具体来说，使用 $\\|A^{-1} \\delta b\\|_2 \\le \\|A^{-1}\\|_2 \\, \\|\\delta b\\|_2$ 和 $\\|b\\|_2 = \\|A x^\\star\\|_2 \\le \\|A\\|_2 \\, \\|x^\\star\\|_2$，我们得到\n$$\n\\frac{\\|x_\\epsilon - x^\\star\\|_2}{\\|x^\\star\\|_2} \\le \\|A^{-1}\\|_2 \\, \\|\\delta b\\|_2 \\,\\frac{1}{\\|x^\\star\\|_2}\n\\le \\|A^{-1}\\|_2 \\, \\|A\\|_2 \\, \\frac{\\|\\delta b\\|_2}{\\|b\\|_2} = \\kappa_2(A) \\, \\frac{\\|\\delta b\\|_2}{\\|b\\|_2}.\n$$\n因此\n$$\nr \\le \\kappa_2(A).\n$$\n这个不等式表明，解的相对误差最大可被放大约等于条件数的倍数。对于像单位矩阵 $I$ 这样的良态矩阵，我们有 $\\kappa_2(I) = 1$，因此 $r$ 应该接近于 $1$。对于像希尔伯特矩阵这样的病态矩阵，$\\kappa_2(A)$ 非常大，即使一个非常小的 $\\epsilon$ 也会导致 $x$ 中产生一个大的相对误差，从而得到一个大的 $r$。\n\n该测试套件涵盖了若干种情况：\n- 用例 $1$ 使用一个 $5$ 阶的希尔伯特矩阵，该矩阵是病态的，但规模适中。\n- 用例 $2$ 使用一个 $10$ 阶的希尔伯特矩阵，它病态更严重，通常会产生大得多的放大因子 $r$。\n- 用例 $3$ 使用一个 $8$ 阶的单位矩阵，它是完全良态的，因此 $r$ 应该约等于 $1$。\n- 用例 $4$ 使用一个近奇异的 $2 \\times 2$ 矩阵，其一个元素的值存在 $10^{-10}$ 的微小差异，这会产生一个非常大的放大因子。\n\n对于每个用例，程序构造 $x^\\star$、$b$、单位范数扰动方向 $v$、具有指定 $\\epsilon$ 的扰动 $\\delta b$，求解 $x_\\epsilon$，计算相对误差，并报告四舍五入到 $6$ 位有效数字的 $r$。最终输出是包含列表 $[r_1, r_2, r_3, r_4]$ 的单行文本，并按此顺序排列。这个过程直接且清晰地展示了 $A$ 的病态性质如何将 $b$ 中的微小相对扰动放大为解 $x$ 中的巨大相对误差。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef hilbert(n: int) -> np.ndarray:\n    # H[i,j] = 1 / (i + j + 1) with zero-based i,j; but use one-based formula directly\n    i = np.arange(1, n + 1).reshape(-1, 1)\n    j = np.arange(1, n + 1).reshape(1, -1)\n    return 1.0 / (i + j - 1.0)\n\ndef alternating_unit_vector(n: int) -> np.ndarray:\n    # v_i = (-1)^(i-1) / sqrt(n), i = 1..n\n    signs = (-1.0) ** np.arange(n)\n    v = signs / np.sqrt(n)\n    # Ensure unit norm numerically\n    return v / np.linalg.norm(v, 2)\n\ndef amplification_factor(A: np.ndarray, eps: float) -> float:\n    n = A.shape[0]\n    x_star = np.ones(n, dtype=float)\n    b = A @ x_star\n    nb = np.linalg.norm(b, 2)\n    if nb == 0.0:\n        # Degenerate, but not expected with provided tests; return NaN-like large value\n        return float('nan')\n    v = alternating_unit_vector(n)\n    delta_b = eps * nb * v\n    b_tilde = b + delta_b\n    # Solve for perturbed solution\n    x_tilde = np.linalg.solve(A, b_tilde)\n    # Relative errors\n    rel_b = np.linalg.norm(delta_b, 2) / nb\n    rel_x = np.linalg.norm(x_tilde - x_star, 2) / np.linalg.norm(x_star, 2)\n    # Amplification factor\n    return rel_x / rel_b\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each test case is a tuple: (A_matrix, epsilon)\n    A1 = hilbert(5)\n    eps1 = 1e-8\n\n    A2 = hilbert(10)\n    eps2 = 1e-8\n\n    A3 = np.eye(8, dtype=float)\n    eps3 = 1e-8\n\n    A4 = np.array([[1.0, 1.0],\n                   [1.0, 1.0 + 1e-10]], dtype=float)\n    eps4 = 1e-12\n\n    test_cases = [\n        (A1, eps1),\n        (A2, eps2),\n        (A3, eps3),\n        (A4, eps4),\n    ]\n\n    results = []\n    for A, eps in test_cases:\n        r = amplification_factor(A, eps)\n        # Round to 6 significant digits\n        if np.isnan(r) or np.isinf(r):\n            results.append(\"nan\")\n        else:\n            results.append(f\"{r:.6g}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "2449583"}, {"introduction": "范数不仅用于分析矩阵特性，它们还为我们提供了衡量数据点之间“距离”的工具，这在金融数据分析中至关重要。本练习将让你亲手操作k-均值聚类算法，并探索使用不同向量范数（如 $L_1$、$L_2$ 和 $L_\\infty$ 范数）作为距离度量，会如何影响对公司财务数据的分类结果。你将看到，一个看似抽象的数学选择，实际上对我们如何理解和组织数据有着直接而深刻的影响。[@problem_id:2447279]", "problem": "一组公司由代表各公司的三个财务比率向量表示，每个向量均位于 $\\mathbb{R}^3$ 中。考虑$\\mathbb{R}^3$上的三种向量范数：$L_1$ 范数定义为 $\\lVert x \\rVert_1 = \\sum_{j=1}^3 \\lvert x_j \\rvert$，$L_2$ 范数定义为 $\\lVert x \\rVert_2 = \\left(\\sum_{j=1}^3 x_j^2\\right)^{1/2}$，以及$L_\\infty$ 范数定义为 $\\lVert x \\rVert_\\infty = \\max\\{\\lvert x_1 \\rvert, \\lvert x_2 \\rvert, \\lvert x_3 \\rvert\\}$。对于任意两个向量 $x,y \\in \\mathbb{R}^3$，在范数 $p \\in \\{1,2,\\infty\\}$ 下的距离定义为 $d_p(x,y) = \\lVert x-y \\rVert_p$。\n\n您必须实现以下聚类过程，该过程由范数 $p \\in \\{1,2,\\infty\\}$ 的选择参数化：\n- 簇的数量为 $K=2$。\n- 按照下面每个数据集的指定，初始化簇质心 $\\mu_1^{(0)}, \\mu_2^{(0)} \\in \\mathbb{R}^3$。\n- 在迭代 $t \\in \\{0,1,2,\\dots\\}$ 时，执行分配步骤：对于每个数据向量 $x_i$，将其分配给最小化 $d_p(x_i,\\mu_{k}^{(t)})$ 的簇索引 $k^\\star \\in \\{1,2\\}$。如果出现距离相等的情况，则选择达到最小值的最小簇索引 $k^\\star$。\n- 执行更新步骤：对于每个簇 $k \\in \\{1,2\\}$，如果在迭代 $t$ 时分配给簇 $k$ 的向量集合非空，则将 $\\mu_k^{(t+1)}$ 设置为这些已分配向量的算术平均值；如果该集合为空，则设置 $\\mu_k^{(t+1)} = \\mu_k^{(t)}$。\n- 当某次迭代与前一次迭代相比，所有分配都没有发生任何变化时，或者在 $T=10$ 次迭代后停止，以先发生者为准。\n\n所有量均为无单位的比率；不涉及物理单位。不涉及角度。不出现百分比。\n\n您的任务是，对下文测试套件中列出的数据集和范数运行上述过程，并为每个测试用例报告最终的簇分配向量。该向量是 $\\{1,2\\}$ 中的整数列表，其顺序与所列公司的顺序相同。簇标签必须按规定从1开始索引。算术平均值是 $\\mathbb{R}^3$ 中的逐分量平均值。\n\n测试套件：\n- 数据集 A（理想情况；分离良好的簇）：\n  - 公司（按顺序）：$x_1=(0.9,1.1,1.0)$, $x_2=(1.2,0.9,1.1)$, $x_3=(1.0,1.2,0.8)$, $x_4=(8.1,7.9,8.0)$, $x_5=(7.8,8.2,8.1)$, $x_6=(8.0,8.1,7.9)$。\n  - 初始质心：$\\mu_1^{(0)}=(1.0,1.0,1.0)$, $\\mu_2^{(0)}=(8.0,8.0,8.0)$。\n  - 要测试的范数：$p \\in \\{2,1,\\infty\\}$，产生三个不同的测试用例。\n- 数据集 B（边界条件；需要打破平局的等距点）：\n  - 公司（按顺序）：$x_1=(0.0,0.0,0.0)$, $x_2=(2.0,0.0,0.0)$, $x_3=(1.0,0.0,0.0)$。\n  - 初始质心：$\\mu_1^{(0)}=(0.0,0.0,0.0)$, $\\mu_2^{(0)}=(2.0,0.0,0.0)$。\n  - 要测试的范数：$p \\in \\{2\\}$，产生一个测试用例。\n- 数据集 C（边缘情况；各向异性和影响不同范数的离群点）：\n  - 公司（按顺序）：$x_1=(0.1,0.1,0.1)$, $x_2=(0.0,0.0,0.0)$, $x_3=(0.2,0.1,0.0)$, $x_4=(4.2,0.1,0.0)$, $x_5=(3.8,-0.1,0.0)$, $x_6=(3.0,0.0,5.0)$。\n  - 初始质心： $\\mu_1^{(0)}=(0.0,0.0,0.0)$, $\\mu_2^{(0)}=(4.0,0.0,0.0)$。\n  - 要测试的范数：$p \\in \\{2,\\infty,1\\}$，产生三个不同的测试用例。\n\n因此，总共有 $7$ 个测试用例，顺序如下：\n$($A, $p=2)$, $($A, $p=1)$, $($A, $p=\\infty)$, $($B, $p=2)$, $($C, $p=2)$, $($C, $p=\\infty)$, $($C, $p=1)$。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含用方括号括起来的逗号分隔列表形式的结果，每个元素本身是一个整数列表，表示相应测试用例的最终簇标签，顺序如上所述。例如，一个包含两个测试用例的输出可能看起来像 $[[1,1,2],[2,1]]$。", "solution": "所提出的问题是一个定义明确且计算上可行的任务。它要求实现 K-means 聚类算法的一种变体，这是无监督机器学习和数据分析中的一个基本过程。该问题具有科学依据，依赖于向量范数（$L_1$、$L_2$ 和 $L_\\infty$）和算术平均值的标准定义。所有参数、初始条件、数据和程序规则——包括终止和打破平局的规则——都已足够精确地指定，以保证一个唯一的、确定性的结果。将这些方法应用于财务比率向量是计算经济学和金融学领域的一个标准应用，其中不同的范数可以代表经济实体之间‘距离’或‘相异性’的不同概念。例如，$L_2$ 范数对应于我们熟悉的欧几里得距离，$L_1$ 范数（或‘曼哈顿距离’）是各比率绝对差之和，而 $L_\\infty$ 范数（或‘切比雪夫距离’）则由任何比率中的最大单一差值决定，这可能与最坏情况分析相关。范数的选择从根本上改变了空间的几何形状，从而影响簇边界的形状，并因此影响最终的分配结果。因此，该问题是理解理论范数实际应用的一个有效且富有启发性的练习。\n\n需要实现的算法是一个迭代过程，定义如下。给定 $\\mathbb{R}^3$ 中的一组数据向量 $\\{x_i\\}_{i=1}^N$、一组包含 $K=2$ 个的初始质心 $\\{\\mu_1^{(0)}, \\mu_2^{(0)}\\}$ 以及一个选定的范数 $p \\in \\{1, 2, \\infty\\}$，我们通过由 $t=0, 1, 2, \\dots$ 索引的迭代进行处理。每次迭代包括两个步骤。\n\n首先是**分配步骤**：每个数据向量 $x_i$ 被分配到一个簇 $k^\\star \\in \\{1, 2\\}$。该分配是通过找到距离 $x_i$ 最近的簇质心 $\\mu_k^{(t)}$ 来完成的，距离由 $d_p(x_i, \\mu_k^{(t)}) = \\lVert x_i - \\mu_k^{(t)} \\rVert_p$ 衡量。因此，簇索引 $k^\\star$ 由下式给出：\n$$\nk^\\star = \\underset{k \\in \\{1,2\\}}{\\arg\\min} \\lVert x_i - \\mu_k^{(t)} \\rVert_p\n$$\n问题指定了一个打破平局的规则：如果到两个质心的距离相等，则该点被分配给簇 1。设 $C_k^{(t)}$ 是在迭代 $t$ 时分配给簇 $k$ 的数据向量的索引集合。\n\n其次是**更新步骤**：为下一次迭代重新计算质心 $\\mu_k^{(t+1)}$。对于每个簇 $k$，如果分配给它的向量集合非空（即 $C_k^{(t)} \\neq \\emptyset$），新的质心是所有分配给该簇的向量的逐分量算术平均值：\n$$\n\\mu_k^{(t+1)} = \\frac{1}{|C_k^{(t)}|} \\sum_{i \\in C_k^{(t)}} x_i\n$$\n如果一个簇变为空（$C_k^{(t)} = \\emptyset$），其质心不被更新：$\\mu_k^{(t+1)} = \\mu_k^{(t)}$。\n\n该过程在以下两种情况之一**终止**：在迭代 $t$ 时的簇分配与迭代 $t-1$ 时的分配完全相同，表示已收敛；或者已完成最多 $T=10$ 次迭代。\n\n解决方案将通过一个程序生成，该程序会为七个指定的测试用例（结合了三个数据集 A、B、C 与不同的范数 $p=2, 1, \\infty$）系统地执行此算法。该程序将计算每个用例的最终簇分配，并根据指定要求格式化输出。使用 NumPy 适合进行高效准确的向量和矩阵运算，包括计算范数和均值。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the clustering problem for all test cases as specified.\n    \"\"\"\n\n    def get_norm_func(p):\n        \"\"\"Returns a function to compute the distance based on the specified norm.\"\"\"\n        if p == 1:\n            return lambda a, b: np.linalg.norm(a - b, ord=1)\n        elif p == 2:\n            return lambda a, b: np.linalg.norm(a - b, ord=2)\n        elif p == 'inf':\n            return lambda a, b: np.linalg.norm(a - b, ord=np.inf)\n        else:\n            raise ValueError(\"Unsupported norm type.\")\n\n    def run_clustering(data, initial_centroids, p, max_iter=10):\n        \"\"\"\n        Runs the specified k-means variant clustering algorithm.\n\n        Args:\n            data (np.ndarray): The dataset of points (N, D).\n            initial_centroids (np.ndarray): The initial cluster centroids (K, D).\n            p (int or 'inf'): The norm to use for distance calculation (1, 2, or 'inf').\n            max_iter (int): Maximum number of iterations.\n\n        Returns:\n            list: A list of final cluster assignments (1-indexed).\n        \"\"\"\n        num_points = data.shape[0]\n        num_clusters = initial_centroids.shape[0]\n        \n        centroids = np.copy(initial_centroids)\n        assignments = np.zeros(num_points, dtype=int)\n        distance_func = get_norm_func(p)\n\n        for _ in range(max_iter):\n            old_assignments = np.copy(assignments)\n\n            # Assignment step\n            for i in range(num_points):\n                point = data[i]\n                \n                # Use a small epsilon for floating point comparisons to be robust,\n                # but the problem rule is a direct tie-break.\n                # d1 = d2 is used to handle ties, assigning to cluster 1.\n                dist_to_c1 = distance_func(point, centroids[0])\n                dist_to_c2 = distance_func(point, centroids[1])\n                \n                if dist_to_c1 = dist_to_c2:\n                    assignments[i] = 1\n                else:\n                    assignments[i] = 2\n\n            # Termination check\n            if np.array_equal(assignments, old_assignments):\n                break\n\n            # Update step\n            new_centroids = np.copy(centroids)\n            for k in range(1, num_clusters + 1):\n                # Get all points assigned to cluster k\n                cluster_points = data[assignments == k]\n                \n                if cluster_points.shape[0] > 0:\n                    # Update centroid to the mean of the assigned points\n                    new_centroids[k-1] = np.mean(cluster_points, axis=0)\n                # Else: centroid remains unchanged, as handled by copying `centroids` beforehand.\n            \n            centroids = new_centroids\n\n        return assignments.tolist()\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"data\": np.array([\n                [0.9, 1.1, 1.0], [1.2, 0.9, 1.1], [1.0, 1.2, 0.8],\n                [8.1, 7.9, 8.0], [7.8, 8.2, 8.1], [8.0, 8.1, 7.9]\n            ]),\n            \"centroids\": np.array([[1.0, 1.0, 1.0], [8.0, 8.0, 8.0]]),\n            \"norm\": 2,\n        },\n        {\n            \"data\": np.array([\n                [0.9, 1.1, 1.0], [1.2, 0.9, 1.1], [1.0, 1.2, 0.8],\n                [8.1, 7.9, 8.0], [7.8, 8.2, 8.1], [8.0, 8.1, 7.9]\n            ]),\n            \"centroids\": np.array([[1.0, 1.0, 1.0], [8.0, 8.0, 8.0]]),\n            \"norm\": 1,\n        },\n        {\n            \"data\": np.array([\n                [0.9, 1.1, 1.0], [1.2, 0.9, 1.1], [1.0, 1.2, 0.8],\n                [8.1, 7.9, 8.0], [7.8, 8.2, 8.1], [8.0, 8.1, 7.9]\n            ]),\n            \"centroids\": np.array([[1.0, 1.0, 1.0], [8.0, 8.0, 8.0]]),\n            \"norm\": 'inf',\n        },\n        {\n            \"data\": np.array([\n                [0.0, 0.0, 0.0], [2.0, 0.0, 0.0], [1.0, 0.0, 0.0]\n            ]),\n            \"centroids\": np.array([[0.0, 0.0, 0.0], [2.0, 0.0, 0.0]]),\n            \"norm\": 2,\n        },\n        {\n            \"data\": np.array([\n                [0.1, 0.1, 0.1], [0.0, 0.0, 0.0], [0.2, 0.1, 0.0],\n                [4.2, 0.1, 0.0], [3.8, -0.1, 0.0], [3.0, 0.0, 5.0]\n            ]),\n            \"centroids\": np.array([[0.0, 0.0, 0.0], [4.0, 0.0, 0.0]]),\n            \"norm\": 2,\n        },\n        {\n            \"data\": np.array([\n                [0.1, 0.1, 0.1], [0.0, 0.0, 0.0], [0.2, 0.1, 0.0],\n                [4.2, 0.1, 0.0], [3.8, -0.1, 0.0], [3.0, 0.0, 5.0]\n            ]),\n            \"centroids\": np.array([[0.0, 0.0, 0.0], [4.0, 0.0, 0.0]]),\n            \"norm\": 'inf',\n        },\n        {\n            \"data\": np.array([\n                [0.1, 0.1, 0.1], [0.0, 0.0, 0.0], [0.2, 0.1, 0.0],\n                [4.2, 0.1, 0.0], [3.8, -0.1, 0.0], [3.0, 0.0, 5.0]\n            ]),\n            \"centroids\": np.array([[0.0, 0.0, 0.0], [4.0, 0.0, 0.0]]),\n            \"norm\": 1,\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        result = run_clustering(case[\"data\"], case[\"centroids\"], case[\"norm\"])\n        results.append(result)\n\n    # Format the final output string exactly as specified, with no spaces.\n    results_str_list = [f\"[{','.join(map(str, r))}]\" for r in results]\n    final_output_str = f\"[{','.join(results_str_list)}]\"\n    \n    print(final_output_str)\n\nsolve()\n```", "id": "2447279"}]}