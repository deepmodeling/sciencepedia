{"hands_on_practices": [{"introduction": "简单的双变量相关性分析有时会产生误导，因为它无法区分两个变量之间的直接关系与由共同的第三方因素（即混杂变量）驱动的间接关系。在金融领域，个别资产的回报率通常会一起波动，仅仅因为它们都受到整体市场趋势的影响。本练习将指导你从基本原理出发，通过线性回归的思想来计算偏相关性，从而在剥离了市场指数（例如代表纳斯达克100指数的QQQ）的共同影响后，揭示两种资产（例如苹果和微软股票）之间真实的、直接的关联 [@problem_id:2385103]。掌握这项技能对于在复杂的市场环境中进行精确的风险评估和资产配置至关重要。", "problem": "您将获得三个金融回报序列，分别代表苹果（Apple）、微软（Microsoft）的每日对数回报，以及一个代表纳斯达克100指数（NASDAQ-100）的宽基科技交易所交易基金（ETF）的每日对数回报。您的任务是，在控制 ETF 影响的情况下，计算苹果和微软每日回报之间的偏相关。您必须仅从样本协方差、样本相关系数和普通最小二乘法（OLS）中的线性投影概念这些基本定义出发，推导出您的方法，然后将其实现为一个完整的、可运行的程序。\n\n从以下基本依据开始：\n- 两个等长（长度为 $T$）实值序列 $x$ 和 $y$ 之间的样本协方差是它们去均值后值的乘积的平均值。\n- 样本相关系数是样本协方差除以两个序列样本标准差的乘积。\n- 普通最小二乘法（OLS）中的线性投影是一种操作，它将一个随机变量映射到另一个随机变量的线性生成空间上，以最小化均方误差；等价地，它通过最小化残差平方和来计算。\n\n您不得假定或使用任何专门的偏相关闭式表达式；相反，您必须基于上述基本依据，推导出如何移除控制变量的线性影响，然后计算剩余信息之间的相关性。对于所提供的测试用例，您的实现必须是数值稳定的。\n\n数据生成方式规定如下。对于每个具有参数 $(\\text{seed}, T, \\beta_A, \\beta_M, \\sigma_Q, \\sigma_A, \\sigma_M, \\rho_u)$ 的测试用例：\n1. 生成一个市场因子 $q_t \\sim \\mathcal{N}(0, \\sigma_Q^2)$，在 $t \\in \\{1,\\dots,T\\}$ 上独立同分布。\n2. 对于每个时间点 $t$，从一个协方差矩阵如下的零均值二元正态分布中抽取一个二元噪声向量 $(u_{A,t}, u_{M,t})^\\top$\n$$\n\\Sigma_u \\;=\\; \\begin{bmatrix}\n\\sigma_A^2 & \\rho_u \\, \\sigma_A \\sigma_M \\\\\n\\rho_u \\, \\sigma_A \\sigma_M & \\sigma_M^2\n\\end{bmatrix}.\n$$\n3. 对所有 $t \\in \\{1,\\dots,T\\}$，构造序列\n$$\n\\text{AAPL}_t \\;=\\; \\beta_A \\, q_t + u_{A,t}, \\quad\n\\text{MSFT}_t \\;=\\; \\beta_M \\, q_t + u_{M,t}, \\quad\n\\text{QQQ}_t \\;=\\; q_t,\n$$\n\n您的程序必须：\n- 对每个测试用例，计算在控制 $\\text{QQQ}_t$ 的情况下，$\\text{AAPL}_t$ 和 $\\text{MSFT}_t$ 之间的样本偏相关。\n- 仅使用上面列出的定义来推导正确的算法。在应用任何线性投影之前，所有序列都必须进行中心化（去均值）。\n- 将所有测试用例的结果以指定格式输出到单行，每个数字四舍五入到小数点后六位。\n\n测试套件：\n每个元组为 $(\\text{seed}, T, \\beta_A, \\beta_M, \\sigma_Q, \\sigma_A, \\sigma_M, \\rho_u)$。\n- 案例 1：$(12345, 252, 1.2, 1.0, 0.015, 0.020, 0.018, 0.5)$。\n- 案例 2：$(20201, 252, 1.2, 1.0, 0.015, 0.020, 0.018, 0.0)$。\n- 案例 3：$(54321, 252, 1.2, 1.0, 0.015, 0.020, 0.018, -0.6)$。\n- 案例 4：$(777, 12, 1.2, 1.0, 0.015, 0.020, 0.018, 0.4)$。\n- 案例 5：$(9999, 252, 1.0, 1.0, 0.020, 0.005, 0.005, 0.95)$。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含五个测试用例的五个偏相关系数，形式为用方括号括起来的逗号分隔列表，每个数字四舍五入到小数点后六位，且无空格。例如：\"[0.123456,-0.010203,0.000000,0.876543,-0.333333]\"。\n- 不允许用户输入；程序必须完全自包含，并可根据上述参数复现。", "solution": "我们寻求在以交易所交易基金（ETF）为条件时，苹果和微软回报序列之间的偏相关。此构建过程仅使用样本协方差、样本相关系数和线性投影的定义。\n\n假设有三个实值序列 $\\{x_t\\}_{t=1}^T$、$\\{y_t\\}_{t=1}^T$ 和 $\\{z_t\\}_{t=1}^T$，其中 $x_t$ 代表苹果的回报，$y_t$ 代表微软的回报，$z_t$ 代表 ETF 的回报。定义中心化序列\n$$\n\\tilde{x}_t \\;=\\; x_t - \\bar{x}, \\quad \\tilde{y}_t \\;=\\; y_t - \\bar{y}, \\quad \\tilde{z}_t \\;=\\; z_t - \\bar{z},\n$$\n其中 $\\bar{x} = \\frac{1}{T}\\sum_{t=1}^T x_t$，而 $\\bar{y}$ 和 $\\bar{z}$ 的定义与此类似。\n\n$x$ 和 $y$ 之间的样本协方差是\n$$\n\\widehat{\\operatorname{Cov}}(x,y) \\;=\\; \\frac{1}{T-1} \\sum_{t=1}^T \\tilde{x}_t \\tilde{y}_t \\;=\\; \\frac{1}{T-1}\\, \\tilde{x}^\\top \\tilde{y}.\n$$\n样本方差是 $\\widehat{\\operatorname{Var}}(x) = \\widehat{\\operatorname{Cov}}(x,x)$，而样本相关系数是\n$$\n\\widehat{\\rho}(x,y) \\;=\\; \\frac{\\widehat{\\operatorname{Cov}}(x,y)}{\\sqrt{\\widehat{\\operatorname{Var}}(x)\\,\\widehat{\\operatorname{Var}}(y)}}.\n$$\n\n为了控制 ETF $z$ 的影响，我们使用线性投影来移除其对 $x$ 和 $y$ 的线性影响。考虑将 $\\tilde{x}$ 线性投影到 $\\tilde{z}$ 上的问题。我们寻求标量系数 $\\beta_{x\\mid z}$ 以最小化残差平方和：\n$$\n\\beta_{x\\mid z} \\;=\\; \\arg\\min_{\\beta \\in \\mathbb{R}} \\sum_{t=1}^T \\left(\\tilde{x}_t - \\beta \\tilde{z}_t\\right)^2.\n$$\n根据普通最小二乘法（OLS）的正规方程，最优的 $\\beta_{x\\mid z}$ 满足\n$$\n\\tilde{z}^\\top \\left(\\tilde{x} - \\beta_{x\\mid z} \\tilde{z}\\right) \\;=\\; 0,\n$$\n可得\n$$\n\\beta_{x\\mid z} \\;=\\; \\frac{\\tilde{z}^\\top \\tilde{x}}{\\tilde{z}^\\top \\tilde{z}} \\;=\\; \\frac{(T-1)\\,\\widehat{\\operatorname{Cov}}(z,x)}{(T-1)\\,\\widehat{\\operatorname{Var}}(z)} \\;=\\; \\frac{\\widehat{\\operatorname{Cov}}(z,x)}{\\widehat{\\operatorname{Var}}(z)}.\n$$\n定义残差\n$$\nr^x_t \\;=\\; \\tilde{x}_t - \\beta_{x\\mid z}\\,\\tilde{z}_t.\n$$\n根据对称性，将 $y$ 投影到 $z$ 后的残差为\n$$\nr^y_t \\;=\\; \\tilde{y}_t - \\beta_{y\\mid z}\\,\\tilde{z}_t, \\quad \\text{其中} \\quad \\beta_{y\\mid z} \\;=\\; \\frac{\\widehat{\\operatorname{Cov}}(z,y)}{\\widehat{\\operatorname{Var}}(z)}.\n$$\n线性投影的关键属性是正交性：$\\sum_{t=1}^T r^x_t \\tilde{z}_t = 0$ 且 $\\sum_{t=1}^T r^y_t \\tilde{z}_t = 0$。根据定义，控制了 $z$ 之后 $x$ 和 $y$ 之间的偏相关即为残差 $r^x$ 和 $r^y$ 之间的样本相关系数：\n$$\n\\widehat{\\rho}(x,y \\mid z) \\;=\\; \\frac{\\sum_{t=1}^T r^x_t r^y_t}{\\sqrt{\\left(\\sum_{t=1}^T (r^x_t)^2\\right)\\left(\\sum_{t=1}^T (r^y_t)^2\\right)}}.\n$$\n分母使用了每次投影的残差平方和。因为所有序列在投影前都已中心化，所以在对 $z$ 的标量投影中不需要截距项。\n\n这种残差化方法直接从定义推导而来，避免了任何专门的简便公式。在当前设定下，该方法是数值稳定的，因为在下文指定随机化构建中，分母 $\\sum_{t=1}^T \\tilde{z}_t^2$ 恒为正。\n\n每个测试用例的数据生成使用一个潜因子 $q_t$ 和特异性噪声 $(u_{A,t},u_{M,t})^\\top$：\n- 独立抽取 $q_t \\sim \\mathcal{N}(0, \\sigma_Q^2)$。\n- 从 $\\mathcal{N}\\left(0, \\Sigma_u\\right)$ 中抽取 $(u_{A,t},u_{M,t})^\\top$，其中\n$$\n\\Sigma_u \\;=\\; \\begin{bmatrix}\n\\sigma_A^2 & \\rho_u \\, \\sigma_A \\sigma_M \\\\\n\\rho_u \\, \\sigma_A \\sigma_M & \\sigma_M^2\n\\end{bmatrix}.\n$$\n- 设置 $x_t = \\beta_A q_t + u_{A,t}$，$y_t = \\beta_M q_t + u_{M,t}$，以及 $z_t = q_t$。\n\n案例解读：\n- 在案例1中，在剔除 ETF 的影响后，苹果和微软之间存在直接的正相关关系，因此预期偏相关为正，且小于无条件相关系数，因为共同的 ETF 效应已被移除。\n- 在案例2中，除了 ETF 之外没有直接联系，因此预期偏相关接近于零。\n- 在案例3中，直接联系是负向的，因此预期偏相关为负。\n- 在案例4中，小样本凸显了抽样变异性；该方法仍然适用，但估计值噪声更大。\n- 在案例5中，特异性成分高度相关，且其方差相对于 ETF 较小，导致了高的无条件相关系数；在控制 ETF 后，偏相关反映了强烈的直接特异性联系。\n\n实现算法摘要：\n1. 对每个测试用例，按规定使用给定种子生成 $\\{x_t,y_t,z_t\\}_{t=1}^T$。\n2. 对每个序列进行中心化。\n3. 计算 $\\beta_{x\\mid z} = (\\tilde{z}^\\top \\tilde{x})/(\\tilde{z}^\\top \\tilde{z})$ 和 $\\beta_{y\\mid z} = (\\tilde{z}^\\top \\tilde{y})/(\\tilde{z}^\\top \\tilde{z})$。\n4. 形成残差 $r^x = \\tilde{x} - \\beta_{x\\mid z}\\tilde{z}$ 和 $r^y = \\tilde{y} - \\beta_{y\\mid z}\\tilde{z}$。\n5. 计算 $r^x$ 和 $r^y$ 之间的样本相关系数，即为偏相关。\n6. 将每个结果四舍五入到小数点后六位，并按要求格式打印列表。\n\n指定的输出必须是单行：一个由方括号括起来的、包含五个四舍五入后偏相关系数的逗号分隔列表，且无空格。", "answer": "```python\nimport numpy as np\n\ndef generate_data(seed, T, beta_A, beta_M, sigma_Q, sigma_A, sigma_M, rho_u):\n    rng = np.random.default_rng(seed)\n    # Generate market factor q_t ~ N(0, sigma_Q^2)\n    q = rng.normal(loc=0.0, scale=sigma_Q, size=T)\n    # Construct covariance matrix for idiosyncratic noises\n    cov_u = np.array([\n        [sigma_A**2, rho_u * sigma_A * sigma_M],\n        [rho_u * sigma_A * sigma_M, sigma_M**2]\n    ])\n    # Generate idiosyncratic noise (u_A, u_M) ~ N(0, cov_u)\n    U = rng.multivariate_normal(mean=[0.0, 0.0], cov=cov_u, size=T)\n    u_A = U[:, 0]\n    u_M = U[:, 1]\n    # Construct returns\n    AAPL = beta_A * q + u_A\n    MSFT = beta_M * q + u_M\n    QQQ = q.copy()\n    return AAPL, MSFT, QQQ\n\ndef demean(x):\n    return x - np.mean(x)\n\ndef partial_correlation_xy_given_z(x, y, z):\n    # Center all series\n    x_c = demean(x)\n    y_c = demean(y)\n    z_c = demean(z)\n    # Guard against degenerate z variance (should not occur in provided tests)\n    denom_z = np.dot(z_c, z_c)\n    if denom_z <= 0:\n        # If z has zero variance, partial correlation reduces to ordinary correlation\n        # But this path should not be taken in test cases.\n        rx = x_c\n        ry = y_c\n    else:\n        beta_xz = np.dot(z_c, x_c) / denom_z\n        beta_yz = np.dot(z_c, y_c) / denom_z\n        rx = x_c - beta_xz * z_c\n        ry = y_c - beta_yz * z_c\n    # Compute correlation between residuals\n    num = np.dot(rx, ry)\n    denom = np.sqrt(np.dot(rx, rx) * np.dot(ry, ry))\n    # Handle potential numerical issues\n    if denom == 0:\n        return 0.0\n    return float(num / denom)\n\ndef solve():\n    # Define the test cases as specified:\n    # Each tuple: (seed, T, beta_A, beta_M, sigma_Q, sigma_A, sigma_M, rho_u)\n    test_cases = [\n        (12345, 252, 1.2, 1.0, 0.015, 0.020, 0.018, 0.5),\n        (20201, 252, 1.2, 1.0, 0.015, 0.020, 0.018, 0.0),\n        (54321, 252, 1.2, 1.0, 0.015, 0.020, 0.018, -0.6),\n        (777,   12,  1.2, 1.0, 0.015, 0.020, 0.018, 0.4),\n        (9999,  252, 1.0, 1.0, 0.020, 0.005, 0.005, 0.95),\n    ]\n\n    results = []\n    for (seed, T, beta_A, beta_M, sigma_Q, sigma_A, sigma_M, rho_u) in test_cases:\n        AAPL, MSFT, QQQ = generate_data(seed, T, beta_A, beta_M, sigma_Q, sigma_A, sigma_M, rho_u)\n        pcorr = partial_correlation_xy_given_z(AAPL, MSFT, QQQ)\n        results.append(f\"{pcorr:.6f}\")\n\n    print(f\"[{','.join(results)}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "2385103"}, {"introduction": "金融时间序列数据以其“尖峰厚尾”的特性而闻名，这意味着极端事件（如市场崩盘或数据录入错误造成的异常值）发生的频率远高于正态分布的预期。这些异常值会对经典的样本协方差和相关性估计产生极大的扭曲，导致错误的分析结论。本练习将带你亲手构建一个对异常值不敏感的稳健估计器 [@problem_id:2385068]。你将从零开始实现“缩尾处理”（winsorization）方法，该方法通过对数据的极端值进行截断或替换，从而在不完全丢弃数据的情况下有效降低异常点的影响，获得更可靠的关联性度量。", "problem": "您的任务是实现并测试一个用于资产回报的稳健协方差与相关性估计器，该估计器通过缩尾处理来减轻单个巨大异常值事件的影响。您必须编写一个完整、可运行的程序，将该估计器应用于一个固定的测试套件，并按下文指定的单行格式打印结果。\n\n该估计器必须基于以下基本定义从头构建。\n\n1. 定义。\n   - 设两种资产的回报序列为有限样本 $\\{x_i\\}_{i=1}^n$ 和 $\\{y_i\\}_{i=1}^n$。\n   - $x$ 和 $y$ 之间的无偏样本协方差为\n     $$\\widehat{\\mathrm{Cov}}(x,y) \\equiv \\frac{1}{n-1} \\sum_{i=1}^n \\left(x_i - \\bar{x}\\right)\\left(y_i - \\bar{y}\\right),$$\n     其中 $\\bar{x} \\equiv \\frac{1}{n} \\sum_{i=1}^n x_i$ 且 $\\bar{y} \\equiv \\frac{1}{n} \\sum_{i=1}^n y_i$。\n   - 无偏样本方差为 $\\widehat{\\mathrm{Var}}(x) \\equiv \\widehat{\\mathrm{Cov}}(x,x)$，Pearson 相关系数为\n     $$\\widehat{\\rho}(x,y) \\equiv \\frac{\\widehat{\\mathrm{Cov}}(x,y)}{\\sqrt{\\widehat{\\mathrm{Var}}(x)\\,\\widehat{\\mathrm{Var}}(y)}}.$$\n     若 $\\widehat{\\mathrm{Var}}(x)=0$ 或 $\\widehat{\\mathrm{Var}}(y)=0$，则定义 $\\widehat{\\rho}(x,y) \\equiv 0$。\n\n2. 缩尾处理算子。\n   - 对于给定的缩尾水平 $\\alpha \\in [0,0.5]$，样本 $v \\in \\mathbb{R}^n$ 在概率 $q \\in [0,1]$ 处的线性插值经验分位数定义如下。设 $v_{(1)} \\le \\dots \\le v_{(n)}$ 表示顺序统计量。设 $r \\equiv q\\,(n-1)$，$\\ell \\equiv \\lfloor r \\rfloor$， $u \\equiv \\lceil r \\rceil$ 且 $t \\equiv r - \\ell$。那么\n     $$Q_q(v) \\equiv (1-t)\\,v_{(\\ell+1)} + t\\,v_{(u+1)}.$$\n   - 样本 $v$ 经过 $\\alpha$-缩尾处理后的序列 $w$ 通过将值限制在对称分位数带内进行逐元素定义：\n     $$w_i \\equiv \\min\\Big(\\max\\big(v_i,\\,Q_\\alpha(v)\\big),\\,Q_{1-\\alpha}(v)\\Big).$$\n\n3. 稳健协方差与相关性。\n   - 给定两个序列 $x$ 和 $y$，使用上述定义独立计算它们的 $\\alpha$-缩尾版本 $x^{(w)}$ 和 $y^{(w)}$。然后使用无偏公式计算 $\\widehat{\\mathrm{Cov}}(x^{(w)},y^{(w)})$ 和 $\\widehat{\\rho}(x^{(w)},y^{(w)})$。这就构成了所要求的稳健估计器。\n\n程序要求。\n\n- 严格按照定义实现上述估计器。不要使用任何与指定线性插值分位数定义不同的内置协方差或分位数例程。\n- 数值输出必须表示为无量纲小数。将每个报告的浮点数四舍五入到 $6$ 位小数。\n\n测试套件。\n\n将您的实现应用于以下五个测试用例，每个用例包含两个回报序列和一个缩尾水平 $\\alpha$：\n\n- 案例 A (共同异常值，正常路径): \n  - $x = [0.01,\\,0.02,\\,-0.01,\\,0.015,\\,-0.005,\\,0.03,\\,-0.02,\\,0.025,\\,-0.015,\\,0.5]$,\n  - $y = [0.008,\\,0.018,\\,-0.012,\\,0.017,\\,-0.004,\\,0.028,\\,-0.018,\\,0.03,\\,-0.013,\\,0.45]$,\n  - $\\alpha = 0.1$.\n- 案例 B (无主要异常值):\n  - $x = [0.01,\\,0.012,\\,0.009,\\,0.011,\\,0.013,\\,0.008,\\,0.010,\\,0.012]$,\n  - $y = [0.02,\\,0.019,\\,0.021,\\,0.018,\\,0.022,\\,0.020,\\,0.0195,\\,0.0215]$,\n  - $\\alpha = 0.1$.\n- 案例 C (无缩尾处理基线):\n  - $x = [0.01,\\,0.02,\\,-0.01,\\,0.015,\\,-0.005,\\,0.03,\\,-0.02,\\,0.025,\\,-0.015,\\,0.5]$,\n  - $y = [0.008,\\,0.018,\\,-0.012,\\,0.017,\\,-0.004,\\,0.028,\\,0.03,\\,-0.013,\\,0.45,\\,-0.018]$,\n  - $\\alpha = 0.0$.\n- 案例 D (在中位数处进行极端缩尾处理):\n  - $x = [-0.01,\\,0.0,\\,0.02,\\,0.0]$,\n  - $y = [0.03,\\,0.0,\\,-0.01,\\,0.0]$,\n  - $\\alpha = 0.5$.\n- 案例 E (单个序列中的单侧异常值):\n  - $x = [0.01,\\,0.012,\\,0.009,\\,0.011,\\,0.013,\\,0.008,\\,0.010,\\,1.0]$,\n  - $y = [0.02,\\,0.019,\\,0.021,\\,0.018,\\,0.022,\\,0.020,\\,0.0195,\\,0.0205]$,\n  - $\\alpha = 0.125$.\n\n对于每个案例，计算并返回一对浮点数：\n- 稳健协方差 $\\widehat{\\mathrm{Cov}}(x^{(w)},y^{(w)})$，\n- 稳健相关系数 $\\widehat{\\rho}(x^{(w)},y^{(w)})$。\n\n最终输出格式。\n\n- 您的程序应生成单行输出，其中包含一个列表的列表形式的结果，每个内部列表对应一个测试用例，顺序为 A、B、C、D、E。每个内部列表必须是 $[\\mathrm{cov},\\mathrm{corr}]$ 的格式，两个浮点数都四舍五入到 $6$ 位小数。输出中不得包含任何空格。例如：\"[[0.000123,0.456789],[...],...]\".", "solution": "根据既定标准对问题陈述进行验证。\n\n### 步骤 1：提取给定信息\n问题提供了以下定义、公式和数据：\n\n- **数据序列**：两个有限样本 $\\{x_i\\}_{i=1}^n$ 和 $\\{y_i\\}_{i=1}^n$。\n- **无偏样本协方差**：\n$$\n\\widehat{\\mathrm{Cov}}(x,y) \\equiv \\frac{1}{n-1} \\sum_{i=1}^n \\left(x_i - \\bar{x}\\right)\\left(y_i - \\bar{y}\\right)\n$$\n其中 $\\bar{x}$ 和 $\\bar{y}$ 是样本均值。\n- **无偏样本方差**：$\\widehat{\\mathrm{Var}}(x) \\equiv \\widehat{\\mathrm{Cov}}(x,x)$。\n- **Pearson 相关系数**：\n$$\n\\widehat{\\rho}(x,y) \\equiv \\frac{\\widehat{\\mathrm{Cov}}(x,y)}{\\sqrt{\\widehat{\\mathrm{Var}}(x)\\,\\widehat{\\mathrm{Var}}(y)}}\n$$\n附加条件是，如果任一方差为零，则 $\\widehat{\\rho}(x,y) \\equiv 0$。\n- **线性插值经验分位数**：对于一个样本 $v$，排序为 $v_{(1)} \\le \\dots \\le v_{(n)}$，在概率 $q \\in [0,1]$ 处的分位数定义为：\n$$\nQ_q(v) \\equiv (1-t)\\,v_{(\\ell+1)} + t\\,v_{(u+1)}\n$$\n其中 $r \\equiv q\\,(n-1)$，$\\ell \\equiv \\lfloor r \\rfloor$， $u \\equiv \\lceil r \\rceil$ 且 $t \\equiv r - \\ell$。\n- **缩尾处理算子**：对于一个缩尾水平 $\\alpha \\in [0,0.5]$，缩尾后的序列 $w$ 由以下公式给出：\n$$\nw_i \\equiv \\min\\Big(\\max\\big(v_i,\\,Q_\\alpha(v)\\big),\\,Q_{1-\\alpha}(v)\\Big)\n$$\n- **稳健估计器**：该估计器定义为 $\\widehat{\\mathrm{Cov}}(x^{(w)},y^{(w)})$ 和 $\\widehat{\\rho}(x^{(w)},y^{(w)})$，其中 $x^{(w)}$ 和 $y^{(w)}$ 分别是 $x$ 和 $y$ 的 $\\alpha$-缩尾版本。\n- **测试套件**：提供了五个特定的测试用例 (A-E)，每个用例都包含两个数据序列 $x$, $y$ 和一个缩尾水平 $\\alpha$。\n- **程序要求**：从基本原理出发实现估计器，不使用与给定定义不符的库函数计算协方差或分位数。数值输出四舍五入到 $6$ 位小数。\n\n### 步骤 2：使用提取的信息进行验证\n对问题进行有效性评估：\n\n1.  **科学依据**：该问题基于稳健统计学中标准、公认的原理。缩尾处理是减轻异常值影响的经典技术。样本协方差、相关性和线性插值分位数的公式是统计学和数据分析中的标准定义。该问题在事实和科学上是合理的。\n2.  **定义明确**：该问题被构建为一个清晰的计算任务。对于任何给定的有效输入（两个数值序列和一个参数 $\\alpha$），操作序列都有明确无歧义的定义，从而导向唯一的解。对零方差的特殊处理确保了相关系数总是有定义的。\n3.  **客观性**：问题陈述使用了精确的数学语言和定义。它完全不含主观、模糊或基于观点的主张。\n4.  **自洽且一致**：所有必要的公式、定义和测试数据都在问题陈述中提供。没有内部矛盾。\n5.  **相关性**：该任务与指定领域直接相关：*计算经济学和金融学*中的*协方差和相关性估计*。\n\n### 步骤 3：结论与行动\n该问题是**有效的**。这是一个定义明确、科学合理的计算练习。开始求解。\n\n### 基于原理的设计\n解决方案要求从基本定义出发实现一个稳健的统计估计器。逻辑结构将由三个主要部分组成，这种方法能确保清晰性、正确性，并遵循指定的基本原理构建要求。\n\n1.  **分位数计算**：缩尾处理算子的基石是经验分位数 $Q_q(v)$。将实现一个函数，根据指定的线性插值公式计算此值。对于给定的样本 $v$ 和概率 $q$，首先对样本进行排序以获得顺序统计量 $v_{(i)}$。计算秩 $r = q(n-1)$。整数部分 $\\ell = \\lfloor r \\rfloor$ 和小数部分 $t = r - \\ell$ 决定了在索引为 $\\ell+1$ 和 $\\ell+2$ 的顺序统计量之间的插值（使用基于 1 的顺序统计量索引，对应于基于 0 的数组索引 `l` 和 `l+1`）。然后应用公式 $Q_q(v) = (1-t)v_{(\\ell+1)} + t v_{(u+1)}$（根据问题陈述中给出的定义进行调整，该定义可简化为此形式）。为满足问题约束，这必须从头开始实现。\n\n2.  **缩尾处理**：缩尾处理算子 $w_i = \\min(\\max(v_i, Q_\\alpha(v)), Q_{1-\\alpha}(v))$ 将数据截断在下分位数和上分位数处。将创建一个函数来执行此操作。它将首先通过调用之前设计的分位数函数来计算 $\\alpha$-分位数 $q_{low} = Q_\\alpha(v)$ 和 $(1-\\alpha)$-分位数 $q_{high} = Q_{1-\\alpha}(v)$。然后，它会将输入序列 $v$ 的每个元素限制在区间 $[q_{low}, q_{high}]$ 内。\n\n3.  **稳健协方差与相关性**：主要的计算过程包括将缩尾处理算子独立应用于两个输入序列 $x$ 和 $y$，以获得它们的稳健版本 $x^{(w)}$ 和 $y^{(w)}$。随后，为这些经过缩尾处理的序列计算标准的无偏样本协方差和 Pearson 相关系数。直接应用无偏协方差公式 $\\widehat{\\mathrm{Cov}}(a, b) = \\frac{1}{n-1} \\sum (a_i - \\bar{a})(b_i - \\bar{b})$。相关系数由协方差和方差导出，特殊情况是如果任一方差为零，则相关系数定义为零。这可以防止除以零，并确保结果确定。\n\n最终的程序将组织这些组件。它将遍历所提供的测试套件，为每个用例应用完整的估计过程，并将所得的协方差-相关性对格式化为指定的单行字符串输出。所有数值将按要求四舍五入到六位小数。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n\n    test_cases = [\n        # Case A (co-outlier, happy path)\n        (\n            [0.01, 0.02, -0.01, 0.015, -0.005, 0.03, -0.02, 0.025, -0.015, 0.5],\n            [0.008, 0.018, -0.012, 0.017, -0.004, 0.028, -0.018, 0.03, -0.013, 0.45],\n            0.1\n        ),\n        # Case B (no major outliers)\n        (\n            [0.01, 0.012, 0.009, 0.011, 0.013, 0.008, 0.010, 0.012],\n            [0.02, 0.019, 0.021, 0.018, 0.022, 0.020, 0.0195, 0.0215],\n            0.1\n        ),\n        # Case C (no winsorization baseline)\n        (\n            [0.01, 0.02, -0.01, 0.015, -0.005, 0.03, -0.02, 0.025, -0.015, 0.5],\n            [0.008, 0.018, -0.012, 0.017, -0.004, 0.028, 0.03, -0.013, 0.45, -0.018],\n            0.0\n        ),\n        # Case D (extreme winsorization at median)\n        (\n            [-0.01, 0.0, 0.02, 0.0],\n            [0.03, 0.0, -0.01, 0.0],\n            0.5\n        ),\n        # Case E (single-sided outlier in one series)\n        (\n            [0.01, 0.012, 0.009, 0.011, 0.013, 0.008, 0.010, 1.0],\n            [0.02, 0.019, 0.021, 0.018, 0.022, 0.020, 0.0195, 0.0205],\n            0.125\n        ),\n    ]\n\n    results = []\n    for x, y, alpha in test_cases:\n        cov, corr = compute_robust_estimator(np.asarray(x), np.asarray(y), alpha)\n        results.append((cov, corr))\n\n    # Format the output string precisely as specified.\n    formatted_results = [f\"[{cov:.6f},{corr:.6f}]\" for cov, corr in results]\n    output_string = f\"[{','.join(formatted_results)}]\"\n    print(output_string)\n\ndef _quantile(v: np.ndarray, q: float) -> float:\n    \"\"\"\n    Computes the empirical quantile using linear interpolation as specified.\n    This implementation is from first principles as required.\n    \"\"\"\n    v_sorted = np.sort(v)\n    n = len(v_sorted)\n    \n    if n == 1:\n        return v_sorted[0]\n    \n    r = q * (n - 1)\n    l = int(r)\n    \n    # Handle case where q=1.0, making l=n-1\n    if l >= n - 1:\n        return v_sorted[n-1]\n        \n    t = r - l\n    \n    # The formula Q_q(v) = (1-t)v_(l+1) + t*v_(u+1) where u=ceil(r)\n    # simplifies to linear interpolation between v_sorted[l] and v_sorted[l+1]\n    # because if r is not integer, u = l+1.\n    val1 = v_sorted[l]\n    val2 = v_sorted[l+1]\n    \n    return (1.0 - t) * val1 + t * val2\n\ndef _winsorize(v: np.ndarray, alpha: float) -> np.ndarray:\n    \"\"\"\n    Applies symmetric alpha-winsorization to a data series.\n    \"\"\"\n    if alpha < 0.0 or alpha > 0.5:\n        raise ValueError(\"alpha must be in [0, 0.5]\")\n    \n    lower_bound = _quantile(v, alpha)\n    upper_bound = _quantile(v, 1.0 - alpha)\n    \n    return np.clip(v, lower_bound, upper_bound)\n\ndef _compute_unbiased_cov_corr(x: np.ndarray, y: np.ndarray) -> tuple[float, float]:\n    \"\"\"\n    Calculates unbiased sample covariance and Pearson correlation.\n    \"\"\"\n    n = len(x)\n    if n < 2:\n        return 0.0, 0.0\n\n    mean_x = np.mean(x)\n    mean_y = np.mean(y)\n\n    # Unbiased covariance (ddof=1)\n    cov = np.sum((x - mean_x) * (y - mean_y)) / (n - 1)\n\n    # Unbiased variances\n    var_x = np.sum((x - mean_x)**2) / (n - 1)\n    var_y = np.sum((y - mean_y)**2) / (n - 1)\n\n    if var_x == 0.0 or var_y == 0.0:\n        corr = 0.0\n    else:\n        corr = cov / np.sqrt(var_x * var_y)\n        \n    return cov, corr\n\ndef compute_robust_estimator(x: np.ndarray, y: np.ndarray, alpha: float) -> tuple[float, float]:\n    \"\"\"\n    Computes the robust covariance and correlation by winsorizing the series first.\n    \"\"\"\n    x_w = _winsorize(x, alpha)\n    y_w = _winsorize(y, alpha)\n    \n    return _compute_unbiased_cov_corr(x_w, y_w)\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "2385068"}, {"introduction": "在现代资产组合管理中，我们常常需要处理数百甚至数千种资产，但可用的历史数据周期（样本量 $n$）却相对有限，有时甚至少于资产数量（维度 $p$）。在这种“高维”场景下，传统的样本协方差矩阵会变得极其不稳定且充满估计误差，直接用于资产配置将导致极端且不切实际的权重。本高级练习将向你介绍解决这一“维度灾难”的强大技术——Ledoit-Wolf线性收缩估计 [@problem_id:2385059]。你将从理论推导开始，亲手实现这一被广泛应用的估计器，通过将不稳定的样本协方差矩阵向一个更稳健的结构化目标（如对角矩阵）进行“收缩”，从而获得在有限数据下更为精确和稳定的协方差估计。", "problem": "您的任务是，当资产数量接近样本规模时，为高维资产收益数据推导并实现一个适用的高维协方差矩阵的线性收缩估计量。该估计量将样本协方差矩阵向一个结构化目标进行收缩。目标是计算最优收缩强度的可实现估计量，并通过实证检验当资产数量接近观测数量时该估计量如何变化。\n\n请在以下纯数学设置中进行操作。设 $X \\in \\mathbb{R}^{n \\times p}$ 为一个数据矩阵，包含 $p$ 个资产的 $n$ 次观测，其行向量为 $x_{i}^{\\top} \\in \\mathbb{R}^{p}$。通过减去列均值来定义中心化数据矩阵，样本协方差矩阵定义为\n$$\nS \\equiv \\frac{1}{n} \\sum_{i=1}^{n} \\left(x_{i} - \\bar{x}\\right)\\left(x_{i} - \\bar{x}\\right)^{\\top} = \\frac{1}{n} X_{c}^{\\top} X_{c},\n$$\n其中 $\\bar{x} \\in \\mathbb{R}^{p}$ 是样本均值，$X_{c}$ 表示均值中心化矩阵。考虑线性收缩估计量\n$$\n\\widehat{\\Sigma}(\\delta) \\equiv (1 - \\delta) S + \\delta F,\n$$\n其中收缩强度为 $\\delta \\in [0,1]$，收缩目标为 $F \\equiv \\mu I_{p}$，$\\mu \\equiv \\frac{\\operatorname{tr}(S)}{p}$，$I_{p}$ 是 $p \\times p$ 的单位矩阵。目标是选择 $\\delta$ 以最小化期望平方Frobenius损失\n$$\n\\mathcal{R}(\\delta) \\equiv \\mathbb{E}\\left[ \\left\\| \\widehat{\\Sigma}(\\delta) - \\Sigma \\right\\|_{F}^{2} \\right],\n$$\n其中 $\\Sigma$ 是真实但未知的协方差矩阵，$\\|\\cdot\\|_{F}$ 是Frobenius范数，期望是相对于数据生成过程计算的。\n\n仅从在此背景下有效的核心定义和性质出发——即样本协方差矩阵的定义、内积的双线性性、$\\mathbb{E}[S] = \\Sigma$、期望的线性性以及Frobenius范数恒等式 $\\|A\\|_{F}^{2} = \\langle A, A \\rangle$——推导一个最优收缩强度 $\\delta^{*}$ 的样本可计算估计量，该估计量不涉及未知的总体量。您的推导必须将任何涉及未知 $\\Sigma$ 的期望明确转换为仅依赖于 $X$ 的统计量。然后从头开始实现该估计量。\n\n用于测试的数据生成过程如下。对于给定的 $(n,p,\\rho)$ 且 $\\rho \\in (-1,1)$，从一个均值为零、相关矩阵为 $C_{\\rho} \\in \\mathbb{R}^{p \\times p}$ 的 $p$ 元正态分布中抽取 $n$ 个独立向量，其中相关矩阵定义为\n$$\n\\left(C_{\\rho}\\right)_{ij} \\equiv \\rho^{|i-j|}, \\quad i,j \\in \\{1,\\dots,p\\}.\n$$\n因此，每个模拟数据集都具有单位方差和Toeplitz相关结构。\n\n实现要求：\n- 使用归一化 $S = \\frac{1}{n} X_{c}^{\\top} X_{c}$。\n- 实现一个函数，该函数同时返回估计的 $\\delta^{*}$ 和相应的收缩协方差 $\\widehat{\\Sigma}(\\delta^{*})$。\n- 为保证数值稳定性，如果您最终表达式中 $\\delta^{*}$ 的分母为零，则设 $\\delta^{*} = 0$，并始终将 $\\delta^{*}$ 裁剪到区间 $[0,1]$ 内。\n- 您不能调用任何预构建的收缩或协方差估计器；所有计算都必须使用基础线性代数从第一性原理构建。\n\n测试套件规范：\n- 固定一个随机种子以确保可复现性。\n- 对于 $n = 200$ 和 $\\rho = 0.3$，按顺序为 $p \\in \\{10, 50, 100, 150, 190, 220\\}$ 计算 $\\delta^{*}$，在相同的种子序列下为每个 $p$ 使用独立模拟的数据集。\n- 边缘情况A（$p \\approx n$ 边界附近，弱互相关）：$(n,p,\\rho) = (50, 45, 0.0)$。\n- 边缘情况B（强互相关且 $p > n$）：$(n,p,\\rho) = (200, 250, 0.8)$。\n\n您的程序应生成单行输出，其中包含用方括号括起来的逗号分隔的结果列表。该列表必须严格按照以下顺序包含八个估计的收缩强度 $\\delta^{*}$：对于 $n = 200$ 和 $\\rho = 0.3$ 的六个 $p \\in \\{10, 50, 100, 150, 190, 220\\}$ 的值，其后是边缘情况A的单个值，最后是边缘情况B的单个值。每个值都必须以浮点数形式打印。不应生成任何图表；仅数值本身就可作为即可视化输出，用于后续的绘图或比较。", "solution": "问题要求推导和实现一个用于高维协方差矩阵的线性收缩估计量。该估计量的形式为 $\\widehat{\\Sigma}(\\delta) = (1 - \\delta) S + \\delta F$，其中 $S$ 是样本协方差矩阵，$F = \\mu I_p$ 是一个缩放后的单位矩阵目标，$\\delta \\in [0,1]$ 是收缩强度。目标是找到最优的 $\\delta$，以最小化期望平方Frobenius损失 $\\mathcal{R}(\\delta) = \\mathbb{E}[ \\| \\widehat{\\Sigma}(\\delta) - \\Sigma \\|_{F}^{2} ]$，其中 $\\Sigma$ 是真实总体协方差矩阵。\n\n推导必须从第一性原理开始，并得出一个最优收缩强度 $\\delta^*$ 的样本可计算估计量，将所有涉及未知总体量（如 $\\Sigma$）的项转换为可从数据矩阵 $X$ 计算的统计量。\n\n**步骤1：最优收缩强度 $\\delta^*$ 的推导**\n\n损失函数由下式给出：\n$$ \\mathcal{R}(\\delta) = \\mathbb{E}\\left[ \\| (1-\\delta)S + \\delta F - \\Sigma \\|_{F}^{2} \\right] $$\n我们可以将范数内的项重写为与真实协方差 $\\Sigma$ 的偏差：\n$$ (1-\\delta)S + \\delta F - \\Sigma = (S - \\Sigma) - \\delta(S - F) $$\nFrobenius范数由内积 $\\langle A, B \\rangle_F = \\operatorname{tr}(A^\\top B)$ 导出。利用性质 $\\|A-B\\|_F^2 = \\|A\\|_F^2 - 2\\langle A, B \\rangle_F + \\|B\\|_F^2$ 和期望的线性性，损失函数变为：\n$$ \\mathcal{R}(\\delta) = \\mathbb{E}[\\|S - \\Sigma\\|_F^2] - 2\\delta \\mathbb{E}[\\langle S - \\Sigma, S - F \\rangle_F] + \\delta^2 \\mathbb{E}[\\|S - F\\|_F^2] $$\n这是一个关于 $\\delta$ 的二次函数。为找到最小化 $\\mathcal{R}(\\delta)$ 的 $\\delta$ 值，我们对 $\\delta$ 求导并将导数设为零：\n$$ \\frac{d\\mathcal{R}(\\delta)}{d\\delta} = -2\\mathbb{E}[\\langle S - \\Sigma, S - F \\rangle_F] + 2\\delta \\mathbb{E}[\\|S - F\\|_F^2] = 0 $$\n解出 $\\delta$ 即可得到最优收缩强度，我们将其表示为 $\\delta^*$：\n$$ \\delta^* = \\frac{\\mathbb{E}[\\langle S - \\Sigma, S - F \\rangle_F]}{\\mathbb{E}[\\|S - F\\|_F^2]} $$\n这个 $\\delta^*$ 的表达式依赖于未知的总体矩阵 $\\Sigma$ 和无法从单个数据样本中计算的期望。我们的下一步是为分子和分母推导基于样本的估计量。\n\n令 $N_{pop} = \\mathbb{E}[\\langle S - \\Sigma, S - F \\rangle_F]$ 和 $D_{pop} = \\mathbb{E}[\\|S - F\\|_F^2]$。我们寻求估计量 $\\hat{N}$ 和 $\\hat{D}$，使得 $\\hat{\\delta}^* = \\hat{N}/\\hat{D}$ 是 $\\delta^*$ 的一个相合估计量。\n\n**步骤2：分母样本估计量 $\\hat{D}$ 的推导**\n\n分母 $D_{pop} = \\mathbb{E}[\\|S - F\\|_F^2]$ 是样本协方差矩阵 $S$ 和收缩目标 $F$ 之间的期望平方距离。该量的一个自然且相合的估计量是其样本类比，通过去掉期望算子得到：\n$$ \\hat{D} = \\|S - F\\|_F^2 $$\n鉴于收缩目标为 $F = \\mu I_p$（其中 $\\mu = \\frac{\\operatorname{tr}(S)}{p}$），并且 $S$ 和 $F$ 都是对称矩阵，我们可以展开此表达式：\n$$ \\hat{D} = \\operatorname{tr}((S-F)^2) = \\operatorname{tr}(S^2 - 2SF + F^2) $$\n利用迹算子的线性性：\n$$ \\hat{D} = \\operatorname{tr}(S^2) - 2\\operatorname{tr}(SF) + \\operatorname{tr}(F^2) $$\n涉及 $F$ 的项可以简化为：\n$$ \\operatorname{tr}(SF) = \\operatorname{tr}(S(\\mu I_p)) = \\mu \\operatorname{tr}(S) $$\n$$ \\operatorname{tr}(F^2) = \\operatorname{tr}((\\mu I_p)^2) = \\operatorname{tr}(\\mu^2 I_p) = p\\mu^2 $$\n将这些代回 $\\hat{D}$ 的表达式中：\n$$ \\hat{D} = \\operatorname{tr}(S^2) - 2\\mu\\operatorname{tr}(S) + p\\mu^2 $$\n最后，代入 $\\mu = \\frac{\\operatorname{tr}(S)}{p}$ 的定义：\n$$ \\hat{D} = \\operatorname{tr}(S^2) - 2\\frac{\\operatorname{tr}(S)}{p}\\operatorname{tr}(S) + p\\left(\\frac{\\operatorname{tr}(S)}{p}\\right)^2 = \\operatorname{tr}(S^2) - \\frac{2}{p}(\\operatorname{tr}(S))^2 + \\frac{1}{p}(\\operatorname{tr}(S))^2 $$\n$$ \\hat{D} = \\operatorname{tr}(S^2) - \\frac{1}{p}(\\operatorname{tr}(S))^2 $$\n这个 $\\hat{D}$ 的最终形式仅依赖于样本协方差矩阵 $S$，因此可以从数据中计算得出。\n\n**步骤3：分子样本估计量 $\\hat{N}$ 的推导**\n\n分子 $N_{pop} = \\mathbb{E}[\\langle S - \\Sigma, S - F \\rangle_F]$ 涉及未知的 $\\Sigma$，无法通过简单的样本类比进行估计。为该项推导一个相合估计量是Ledoit-Wolf方法论的基石，并且需要高级的渐近论证。根据已有文献（Ledoit & Wolf, 2004），$N_{pop}$ 的一个相合估计量由下式给出：\n$$ \\hat{N} = \\frac{1}{n}\\sum_{i=1}^{n} \\| (x_i - \\bar{x})(x_i - \\bar{x})^\\top - S \\|_F^2 $$\n其中 $y_i = x_i - \\bar{x}$ 表示第 $i$ 个中心化观测向量。让我们展开此表达式，以获得一个适合计算的公式。\n$$ \\hat{N} = \\frac{1}{n}\\sum_{i=1}^{n} \\operatorname{tr}\\left( (y_i y_i^\\top - S)^2 \\right) = \\frac{1}{n}\\sum_{i=1}^{n} \\left[ \\operatorname{tr}((y_i y_i^\\top)^2) - 2\\operatorname{tr}(y_i y_i^\\top S) + \\operatorname{tr}(S^2) \\right] $$\n我们简化迹项：\n- 第一项使用性质 $\\operatorname{tr}(AB) = \\operatorname{tr}(BA)$：$\\operatorname{tr}((y_i y_i^\\top)^2) = \\operatorname{tr}(y_i (y_i^\\top y_i) y_i^\\top) = (y_i^\\top y_i) \\operatorname{tr}(y_i y_i^\\top) = (y_i^\\top y_i)(y_i^\\top y_i) = \\|y_i\\|_2^4$。\n- 第二项是一个二次型：$\\operatorname{tr}(y_i y_i^\\top S) = \\operatorname{tr}(y_i^\\top S y_i) = y_i^\\top S y_i$。\n将这些代回，我们得到：\n$$ \\hat{N} = \\frac{1}{n}\\sum_{i=1}^{n} \\left( \\|y_i\\|_2^4 - 2 y_i^\\top S y_i + \\operatorname{tr}(S^2) \\right) $$\n将此式进一步分解：\n$$ \\hat{N} = \\left(\\frac{1}{n}\\sum_{i=1}^{n} \\|y_i\\|_2^4 \\right) - \\frac{2}{n}\\sum_{i=1}^{n} (y_i^\\top S y_i) + \\operatorname{tr}(S^2) $$\n$\\hat{N}$ 表达式中的每个分量都可以从中心化数据 $y_i$ 和样本协方差矩阵 $S$ 计算出来。\n\n**步骤4：最终的可计算估计量和算法**\n\n通过组合样本估计量 $\\hat{N}$ 和 $\\hat{D}$，我们得到最优收缩强度的Ledoit-Wolf估计量：\n$$ \\hat{\\delta}^* = \\frac{\\hat{N}}{\\hat{D}} $$\n为确保数值稳定性和有效性，我们进行两项调整：\n1. 如果分母 $\\hat{D}$ 为零，这意味着 $S$ 已经与单位矩阵成比例。在这种情况下，不需要收缩，因此我们设 $\\hat{\\delta}^* = 0$。\n2. 收缩强度 $\\delta$ 必须位于区间 $[0,1]$ 内。因此，计算出的比率被裁剪到这个范围内。\n\n最终的估计量是：\n$$ \\hat{\\delta}^*_{\\text{final}} = \\max\\left(0, \\min\\left(1, \\frac{\\hat{N}}{\\hat{D}}\\right)\\right) $$\n一旦计算出 $\\hat{\\delta}^*_{\\text{final}}$，收缩协方差矩阵就按如下方式构建：\n$$ \\widehat{\\Sigma}(\\hat{\\delta}^*_{\\text{final}}) = (1 - \\hat{\\delta}^*_{\\text{final}})S + \\hat{\\delta}^*_{\\text{final}}F $$\n这就完成了线性收缩估计的完全可实现算法的推导。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import linalg\n\ndef generate_data(n, p, rho, rng):\n    \"\"\"\n    Generates n samples from a p-variate normal distribution with zero mean\n    and a Toeplitz correlation matrix C_rho.\n    \"\"\"\n    # Construct the first column of the Toeplitz correlation matrix\n    first_col = np.array([rho**i for i in range(p)])\n    \n    # Create the Toeplitz correlation matrix\n    C_rho = linalg.toeplitz(first_col)\n    \n    # Generate data from multivariate normal distribution\n    # Mean is zero, and covariance is the correlation matrix C_rho\n    mean = np.zeros(p)\n    X = rng.multivariate_normal(mean, C_rho, size=n)\n    \n    return X\n\ndef estimate_lw_shrinkage(X):\n    \"\"\"\n    Computes the Ledoit-Wolf linear shrinkage estimator for the covariance matrix.\n\n    Args:\n        X (np.ndarray): Data matrix of shape (n, p), where n is the number of\n                        observations and p is the number of assets.\n\n    Returns:\n        tuple: A tuple containing:\n            - delta_star (float): The estimated optimal shrinkage intensity.\n            - Sigma_hat (np.ndarray): The shrunken covariance matrix.\n    \"\"\"\n    n, p = X.shape\n    \n    # 1. Center the data\n    mean_x = np.mean(X, axis=0)\n    Y = X - mean_x\n    \n    # 2. Compute sample covariance matrix S (using 1/n normalization)\n    S = (Y.T @ Y) / n\n    \n    # 3. Compute shrinkage target parameter mu\n    tr_S = np.trace(S)\n    mu = tr_S / p\n    \n    # 4. Compute denominator estimator d_hat_sq\n    S_sq = S @ S\n    tr_S_sq = np.trace(S_sq)\n    d_hat_sq = tr_S_sq - (tr_S**2) / p\n    \n    # Handle case where d_hat_sq is zero\n    if d_hat_sq == 0:\n        return 0.0, S\n\n    # 5. Compute numerator estimator b_hat_sq\n    # This is a vectorized implementation of the formula for N_hat (or b^2 in LW paper)\n    # N_hat = (1/n)*sum_i(||y_i y_i^T - S||_F^2)\n    #       = (1/n)*sum_i(||y_i||^4) - (2/n)*sum_i(y_i^T S y_i) + tr(S^2)\n    sum_y_norm4_div_n = np.sum(np.sum(Y**2, axis=1)**2) / n\n    sum_ySy_div_n = np.sum(np.diag(Y @ S @ Y.T)) / n # More stable than sum(Y*(Y@S))\n    \n    b_hat_sq = sum_y_norm4_div_n - 2 * sum_ySy_div_n + tr_S_sq\n\n    # 6. Compute shrinkage intensity delta_star\n    delta_star = b_hat_sq / d_hat_sq\n    \n    # 7. Clip delta_star to the interval [0, 1]\n    delta_star = np.clip(delta_star, 0.0, 1.0)\n    \n    # 8. Compute the shrunken covariance matrix\n    F = mu * np.eye(p)\n    Sigma_hat = (1 - delta_star) * S + delta_star * F\n    \n    return delta_star, Sigma_hat\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print the results.\n    \"\"\"\n    # Fix a random seed for reproducibility\n    seed = 42\n    rng = np.random.default_rng(seed)\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Main test suite: n=200, rho=0.3, vary p\n        (200, 10, 0.3),\n        (200, 50, 0.3),\n        (200, 100, 0.3),\n        (200, 150, 0.3),\n        (200, 190, 0.3),\n        (200, 220, 0.3),\n        # Edge case A: p approx n, weak correlation\n        (50, 45, 0.0),\n        # Edge case B: p > n, strong correlation\n        (200, 250, 0.8),\n    ]\n\n    results = []\n    for n, p, rho in test_cases:\n        # Generate data for the current case\n        X = generate_data(n, p, rho, rng)\n        \n        # Estimate shrinkage intensity\n        delta_star, _ = estimate_lw_shrinkage(X)\n        results.append(delta_star)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "2385059"}]}