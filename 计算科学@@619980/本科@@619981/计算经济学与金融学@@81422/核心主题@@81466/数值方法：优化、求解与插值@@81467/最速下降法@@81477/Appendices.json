{"hands_on_practices": [{"introduction": "任何迭代优化算法的第一步都是确定从起点出发的方向。对于最速下降法，这个选择是明确且直观的。本练习 [@problem_id:2221567] 将让您为著名的 Rosenbrock 函数（优化领域的一个标准基准函数）计算初始搜索方向，从而巩固最速下降方向就是负梯度方向 $-\\nabla f(x)$ 这一基本原理。在构建完整的迭代算法之前，掌握这第一步至关重要。", "problem": "在数值优化领域，新算法的性能通常使用一组标准测试函数进行基准测试。Rosenbrock函数就是其中之一，由于其狭窄的抛物线形山谷，该函数难以最小化。\n\n考虑一个由下式给出的二维形式的Rosenbrock函数\n$$f(x,y) = (a-x)^2 + b(y-x^2)^2$$\n其中 $a$ 和 $b$ 是正常数。\n\n一个迭代最小化算法从点 $(x_0, y_0) = (0, 0)$ 开始。该算法的第一步是确定初始搜索方向。该方向被定义为函数值从起始点下降最快的向量。\n\n确定这个初始搜索方向向量 $\\mathbf{d}_0$。用常数 $a$ 和 $b$ 将您的答案表示为一个列向量。", "solution": "函数 $f$ 在某点下降最快的方向是负梯度方向，因此从 $(x_{0},y_{0})=(0,0)$ 出发的初始搜索方向是 $\\mathbf{d}_{0}=-\\nabla f(0,0)$。\n\n计算 $f(x,y)=(a-x)^{2}+b(y-x^{2})^{2}$ 的梯度：\n- 关于 $x$ 的偏导数为\n$$\n\\frac{\\partial f}{\\partial x}=2(x-a)-4bx(y-x^{2}).\n$$\n- 关于 $y$ 的偏导数为\n$$\n\\frac{\\partial f}{\\partial y}=2b(y-x^{2}).\n$$\n\n在 $(0,0)$ 处计算这些偏导数的值：\n$$\n\\frac{\\partial f}{\\partial x}(0,0)=-2a,\\qquad \\frac{\\partial f}{\\partial y}(0,0)=0.\n$$\n因此，\n$$\n\\nabla f(0,0)=\\begin{pmatrix}-2a\\\\ 0\\end{pmatrix},\n\\quad\n\\mathbf{d}_{0}=-\\nabla f(0,0)=\\begin{pmatrix}2a\\\\ 0\\end{pmatrix}.\n$$", "answer": "$$\\boxed{\\begin{pmatrix}2a\\\\0\\end{pmatrix}}$$", "id": "2221567"}, {"introduction": "在确定了下降方向后，下一个问题是应该沿该方向走多远。通过手动执行几步算法，我们可以揭示其动态行为，特别是在处理某些特定类型的函数时。本练习 [@problem_id:2221576] 要求您为一个简单的二次函数手动计算前两次迭代，这将让您对算法的路径有一个具体、可计算的理解。通过这个过程，您会亲眼见证并理解为什么即使每一步都采用最优步长，最速下降法有时仍会呈现收敛缓慢的“之”字形（zigzag）模式。", "problem": "考虑无约束优化问题，最小化函数 $f(x, y) = 10x^2 + y^2$。优化过程从点 $\\mathbf{x}_0 = (x_0, y_0) = (1, 1)$ 开始。\n\n将使用最速下降算法。对于每次迭代 $k$，步长（记为 $\\alpha_k > 0$）通过精确线搜索确定。这意味着对于给定的点 $\\mathbf{x}_k$ 和下降方向 $\\mathbf{p}_k$，选择步长 $\\alpha_k$ 以求得单变量函数 $g(\\alpha) = f(\\mathbf{x}_k + \\alpha \\mathbf{p}_k)$ 的全局最小值。\n\n计算该方法经过两次完整迭代后的点 $\\mathbf{x}_2 = (x_2, y_2)$ 的坐标。最终答案中的坐标应表示为最简精确分数形式。", "solution": "我们使用带精确线搜索的最速下降法，从 $\\mathbf{x}_{0}=(1,1)$ 开始最小化 $f(x,y)=10x^{2}+y^{2}$。梯度和Hessian矩阵为\n$$\n\\nabla f(x,y)=\\begin{pmatrix}20x\\\\2y\\end{pmatrix},\\qquad H=\\begin{pmatrix}20&0\\\\0&2\\end{pmatrix}.\n$$\n在第 $k$ 次迭代中，设 $\\mathbf{g}_{k}=\\nabla f(\\mathbf{x}_{k})$，方向为 $\\mathbf{p}_{k}=-\\mathbf{g}_{k}$，精确线搜索旨在最小化 $g(\\alpha)=f(\\mathbf{x}_{k}+\\alpha\\mathbf{p}_{k})$。由于 $H$ 是常数（函数是二次的），导数为\n$$\ng'(\\alpha)=\\nabla f(\\mathbf{x}_{k}-\\alpha \\mathbf{g}_{k})^{\\mathsf{T}}(-\\mathbf{g}_{k})=\\left(\\mathbf{g}_{k}-\\alpha H\\mathbf{g}_{k}\\right)^{\\mathsf{T}}(-\\mathbf{g}_{k})=-(\\mathbf{g}_{k}^{\\mathsf{T}}\\mathbf{g}_{k})+\\alpha\\,\\mathbf{g}_{k}^{\\mathsf{T}}H\\mathbf{g}_{k}.\n$$\n令 $g'(\\alpha)=0$ 可得到精确步长\n$$\n\\alpha_{k}=\\frac{\\mathbf{g}_{k}^{\\mathsf{T}}\\mathbf{g}_{k}}{\\mathbf{g}_{k}^{\\mathsf{T}}H\\mathbf{g}_{k}}.\n$$\n\n第 0 次迭代：$\\mathbf{x}_{0}=(1,1)$ 给出\n$$\n\\mathbf{g}_{0}=\\begin{pmatrix}20\\\\2\\end{pmatrix},\\quad \\mathbf{g}_{0}^{\\mathsf{T}}\\mathbf{g}_{0}=404,\\quad H\\mathbf{g}_{0}=\\begin{pmatrix}400\\\\4\\end{pmatrix},\\quad \\mathbf{g}_{0}^{\\mathsf{T}}H\\mathbf{g}_{0}=8008,\n$$\n因此\n$$\n\\alpha_{0}=\\frac{404}{8008}=\\frac{101}{2002}.\n$$\n则\n$$\n\\mathbf{x}_{1}=\\mathbf{x}_{0}-\\alpha_{0}\\mathbf{g}_{0}=\\begin{pmatrix}1-20\\alpha_{0}\\\\1-2\\alpha_{0}\\end{pmatrix}=\\begin{pmatrix}1-\\frac{1010}{1001}\\\\1-\\frac{101}{1001}\\end{pmatrix}=\\begin{pmatrix}-\\frac{9}{1001}\\\\\\frac{900}{1001}\\end{pmatrix}.\n$$\n\n第 1 次迭代：$\\mathbf{x}_{1}=\\left(-\\frac{9}{1001},\\,\\frac{900}{1001}\\right)$ 给出\n$$\n\\mathbf{g}_{1}=\\begin{pmatrix}20x_{1}\\\\2y_{1}\\end{pmatrix}=\\begin{pmatrix}-\\frac{180}{1001}\\\\\\frac{1800}{1001}\\end{pmatrix}.\n$$\n计算\n$$\n\\mathbf{g}_{1}^{\\mathsf{T}}\\mathbf{g}_{1}=\\frac{180^{2}+1800^{2}}{1001^{2}}=\\frac{180^{2}\\cdot 101}{1001^{2}},\\qquad\nH\\mathbf{g}_{1}=\\begin{pmatrix}-\\frac{3600}{1001}\\\\\\frac{3600}{1001}\\end{pmatrix},\\qquad\n\\mathbf{g}_{1}^{\\mathsf{T}}H\\mathbf{g}_{1}=\\frac{180\\cdot 3600}{1001^{2}}(1+10)=\\frac{7128000}{1001^{2}}.\n$$\n因此\n$$\n\\alpha_{1}=\\frac{\\mathbf{g}_{1}^{\\mathsf{T}}\\mathbf{g}_{1}}{\\mathbf{g}_{1}^{\\mathsf{T}}H\\mathbf{g}_{1}}=\\frac{3272400}{7128000}=\\frac{101}{220}.\n$$\n更新\n$$\n\\mathbf{x}_{2}=\\mathbf{x}_{1}-\\alpha_{1}\\mathbf{g}_{1}=\\begin{pmatrix}-\\frac{9}{1001}-\\frac{101}{220}\\left(-\\frac{180}{1001}\\right)\\\\ \\frac{900}{1001}-\\frac{101}{220}\\left(\\frac{1800}{1001}\\right)\\end{pmatrix}=\\begin{pmatrix}-\\frac{9}{1001}+\\frac{909}{11011}\\\\ \\frac{900}{1001}-\\frac{9090}{11011}\\end{pmatrix}=\\begin{pmatrix}\\frac{810}{11011}\\\\ \\frac{810}{11011}\\end{pmatrix}.\n$$\n该分数为最简形式，因为 $\\gcd(810,11011)=1$。", "answer": "$$\\boxed{\\begin{pmatrix} \\frac{810}{11011} \\\\ \\frac{810}{11011} \\end{pmatrix}}$$", "id": "2221576"}, {"introduction": "虽然手动计算有助于深入理解，但解决现实世界的问题需要通过编程实现。本练习旨在帮助您跨越理论理解与实际应用之间的鸿沟。一个稳健的最速下降算法实现不仅需要计算梯度，还需要采用一种实用的步长选择方法，例如回溯线搜索，以确保目标函数在每次迭代中都有充分的下降。在此练习 [@problem_id:2434090] 中，您将实现完整的带回溯线搜索的最速下降算法来解决一个效用最大化问题，从而将抽象的公式转化为一个实用的优化工具，并掌握将其应用于经济和金融领域具体问题的能力。", "problem": "考虑以下由二次效用函数给出的两个变量的无约束效用最大化问题\n$$u(x) = b^{\\top} x - \\tfrac{1}{2} x^{\\top} Q x,$$\n其中 $x \\in \\mathbb{R}^{2}$，$b \\in \\mathbb{R}^{2}$，且 $Q \\in \\mathbb{R}^{2 \\times 2}$ 是对称正定矩阵。该问题可以等价地表示为目标函数的无约束最小化问题\n$$f(x) = -u(x) = \\tfrac{1}{2} x^{\\top} Q x - b^{\\top} x.$$\n设迭代由 $x^{(k+1)} = x^{(k)} - \\alpha_{k} \\nabla f\\left(x^{(k)}\\right)$ 定义，其中 $\\nabla f(x) = Qx - b$。在每次迭代中，步长 $\\alpha_{k}$ 必须满足充分下降条件\n$$f\\left(x^{(k)} - \\alpha_{k} \\nabla f\\left(x^{(k)}\\right)\\right) \\le f\\left(x^{(k)}\\right) + c \\, \\alpha_{k} \\, \\nabla f\\left(x^{(k)}\\right)^{\\top}\\left(-\\nabla f\\left(x^{(k)}\\right)\\right),$$\n其中 $\\alpha_{k}$ 从几何序列 $\\{\\alpha_{0} \\rho^{m} : m \\in \\mathbb{N} \\cup \\{0\\}\\}$ 中选择，$\\alpha_{0} \\in \\mathbb{R}_{++}$、$\\rho \\in (0,1)$ 和 $c \\in (0,1)$ 为固定常数。所有范数计算均使用欧几里得范数 $\\|\\cdot\\|_{2}$。当 $\\|\\nabla f(x^{(k)})\\|_{2} \\le \\varepsilon$ 或迭代次数达到最大值 $N_{\\max}$ 时，算法必须终止。\n\n对于每个测试用例，计算所述过程产生的最终迭代点 $x^{(T)}$，并计算 $u(x)$ 的唯一最大化点 $x^{\\star}$，它等价于 $f(x)$ 的唯一最小化点且满足 $Q x^{\\star} = b$。对于每个测试用例，报告标量欧几里得误差 $\\|x^{(T)} - x^{\\star}\\|_{2}$。\n\n所有测试用例均使用以下固定参数值：初始步长参数 $\\alpha_{0} = 1$，回溯收缩因子 $\\rho = 0.5$，充分下降常数 $c = 10^{-4}$，容差 $\\varepsilon = 10^{-8}$，以及最大迭代次数 $N_{\\max} = 10000$。所有矩阵和向量的具体值如下。\n\n测试套件：\n- 测试用例 1：$Q = \\begin{bmatrix} 2 & 0 \\\\ 0 & 1 \\end{bmatrix}$，$b = \\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix}$，$x^{(0)} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$。\n- 测试用例 2：$Q = \\begin{bmatrix} 100 & 0 \\\\ 0 & 1 \\end{bmatrix}$，$b = \\begin{bmatrix} 100 \\\\ 1 \\end{bmatrix}$，$x^{(0)} = \\begin{bmatrix} 10 \\\\ -10 \\end{bmatrix}$。\n- 测试用例 3：$Q = \\begin{bmatrix} 3 & 1 \\\\ 1 & 2 \\end{bmatrix}$，$b = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$，$x^{(0)} = \\begin{bmatrix} \\tfrac{2}{5} \\\\ -\\tfrac{1}{5} \\end{bmatrix}$。\n- 测试用例 4：$Q = \\begin{bmatrix} 4 & 1.5 \\\\ 1.5 & 1 \\end{bmatrix}$，$b = \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix}$，$x^{(0)} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$。\n\n最终输出格式：\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，按测试用例的顺序排列结果，即 $[\\|x^{(T)}_{1} - x^{\\star}_{1}\\|_{2}, \\|x^{(T)}_{2} - x^{\\star}_{2}\\|_{2}, \\|x^{(T)}_{3} - x^{\\star}_{3}\\|_{2}, \\|x^{(T)}_{4} - x^{\\star}_{4}\\|_{2}]$。结果必须是实数（浮点数）。此问题不涉及物理单位。", "solution": "所述问题是有效的。这是一个在数值优化领域中定义明确且具有科学依据的问题，专门关注应用于二次目标函数的最速下降法。所有必要的参数和数据都已提供，术语精确，并且没有内部矛盾或逻辑缺陷。该问题涉及最小化一个严格凸的二次函数，对于此类函数，带有回溯线搜索的最速下降法是一种标准的收敛算法。我们现在将着手求解。\n\n问题是最小化二次目标函数 $f(x) = \\frac{1}{2} x^{\\top} Q x - b^{\\top} x$，其中 $x \\in \\mathbb{R}^{2}$，$b \\in \\mathbb{R}^{2}$，且 $Q \\in \\mathbb{R}^{2 \\times 2}$ 是一个对称正定矩阵。\n\n首先，我们确定解的存在性和唯一性。目标函数的Hessian矩阵是 $\\nabla^2 f(x) = Q$。由于对于所有测试用例，$Q$ 都被给定为正定矩阵，因此函数 $f(x)$ 是严格凸的。在 $\\mathbb{R}^{n}$ 上的严格凸函数至多有一个最小化点。由于 $f(x)$ 是强制的（即，当 $\\|x\\|_2 \\to \\infty$ 时 $f(x) \\to \\infty$），因此保证存在一个唯一的全局最小化点，我们将其表示为 $x^{\\star}$。\n\n一阶必要最优性条件指出，在最小化点处，目标函数的梯度必须为零。$f(x)$ 的梯度是 $\\nabla f(x) = Qx - b$。将梯度设为零，得到最优性条件：\n$$\n\\nabla f(x^{\\star}) = Qx^{\\star} - b = 0\n$$\n这是一个线性方程组 $Qx^{\\star} = b$。由于 $Q$ 是正定的，它是可逆的。因此，唯一的解析解由下式给出：\n$$\nx^{\\star} = Q^{-1} b\n$$\n\n寻找解的数值过程是最速下降法。这是一个迭代算法，生成一个收敛到 $x^{\\star}$ 的点序列 $\\{x^{(k)}\\}_{k=0}^{\\infty}$。迭代定义为：\n$$\nx^{(k+1)} = x^{(k)} + \\alpha_k p^{(k)}\n$$\n其中 $p^{(k)}$ 是搜索方向，$\\alpha_k > 0$ 是步长。对于最速下降法，搜索方向选择为当前迭代点梯度的负方向，因为这是函数局部下降最快的方向。\n$$\np^{(k)} = -\\nabla f(x^{(k)}) = -(Qx^{(k)} - b) = b - Qx^{(k)}\n$$\n因此，更新规则为：\n$$\nx^{(k+1)} = x^{(k)} - \\alpha_k \\nabla f(x^{(k)})\n$$\n\n步长 $\\alpha_k$ 使用回溯线搜索过程确定，以满足充分下降条件，也称为 Armijo 条件。这确保了每一步都朝着最小值取得有意义的进展。该条件是：\n$$\nf(x^{(k+1)}) \\le f(x^{(k)}) + c \\, \\alpha_k \\, \\nabla f(x^{(k)})^{\\top} p^{(k)}\n$$\n代入 $p^{(k)} = -\\nabla f(x^{(k)})$，条件变为：\n$$\nf\\left(x^{(k)} - \\alpha_k \\nabla f\\left(x^{(k)}\\right)\\right) \\le f\\left(x^{(k)}\\right) - c \\, \\alpha_k \\, \\|\\nabla f\\left(x^{(k)}\\right)\\|^{2}_{2}\n$$\n在每次迭代 $k$ 中选择 $\\alpha_k$ 的过程如下：\n1. 从初始步长 $\\alpha = \\alpha_0 = 1$ 开始。\n2. 当 Armijo 条件不满足时，缩小步长：$\\alpha \\leftarrow \\rho \\alpha$。这里，$\\rho = 0.5$。\n3. 一旦条件满足，设置 $\\alpha_k = \\alpha$。\n给定的参数为 $c = 10^{-4}$，$\\alpha_0 = 1$ 和 $\\rho = 0.5$。\n\n当满足以下两个条件之一时，算法终止：\n1. 梯度的范数小于指定的容差 $\\varepsilon = 10^{-8}$：$\\|\\nabla f(x^{(k)})\\|_{2} \\le \\varepsilon$。这表明迭代点非常接近梯度为零的最优解 $x^{\\star}$。\n2. 迭代次数 $k$ 达到允许的最大次数 $N_{\\max} = 10000$。\n\n令 $x^{(T)}$ 为算法产生的最终迭代点。每个测试用例最终要求的输出是欧几里得误差 $\\|x^{(T)} - x^{\\star}\\|_{2}$。对于每个给定的测试用例（$Q$，$b$，$x^{(0)}$），我们将首先计算解析解 $x^{\\star} = Q^{-1}b$，然后执行所述的迭代算法来找到 $x^{(T)}$。\n\n对于测试用例3，给定 $Q = \\begin{bmatrix} 3 & 1 \\\\ 1 & 2 \\end{bmatrix}$，$b = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$，以及 $x^{(0)} = \\begin{bmatrix} 2/5 \\\\ -1/5 \\end{bmatrix}$。解析解是 $x^{\\star} = Q^{-1}b = \\frac{1}{5}\\begin{bmatrix} 2 & -1 \\\\ -1 & 3 \\end{bmatrix}\\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} 2/5 \\\\ -1/5 \\end{bmatrix}$。因此，初始点 $x^{(0)}$ 就是精确解 $x^{\\star}$。在这种情况下，起始点的梯度为 $\\nabla f(x^{(0)}) = Qx^{(0)} - b = 0$。终止条件 $\\|\\nabla f(x^{(0)})\\|_{2} = 0 \\le \\varepsilon$ 在迭代 $k=0$ 时立即满足。算法终止，得到 $x^{(T)} = x^{(0)}$，且误差 $\\|x^{(T)} - x^{\\star}\\|_{2}$ 为 $0$。\n\n实现将对所有测试用例遵循此逻辑。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the unconstrained utility maximization problem for four test cases\n    using the steepest descent method with backtracking line search.\n    \"\"\"\n    # Fixed parameters for the algorithm\n    alpha0 = 1.0\n    rho = 0.5\n    c = 1e-4\n    epsilon = 1e-8\n    N_max = 10000\n\n    # Test cases defined in the problem statement\n    test_cases = [\n        {\n            \"Q\": np.array([[2.0, 0.0], [0.0, 1.0]]),\n            \"b\": np.array([2.0, 1.0]),\n            \"x0\": np.array([0.0, 0.0]),\n        },\n        {\n            \"Q\": np.array([[100.0, 0.0], [0.0, 1.0]]),\n            \"b\": np.array([100.0, 1.0]),\n            \"x0\": np.array([10.0, -10.0]),\n        },\n        {\n            \"Q\": np.array([[3.0, 1.0], [1.0, 2.0]]),\n            \"b\": np.array([1.0, 0.0]),\n            \"x0\": np.array([2.0 / 5.0, -1.0 / 5.0]),\n        },\n        {\n            \"Q\": np.array([[4.0, 1.5], [1.5, 1.0]]),\n            \"b\": np.array([1.0, 2.0]),\n            \"x0\": np.array([0.0, 0.0]),\n        },\n    ]\n\n    results = []\n\n    def objective_function(x, Q, b):\n        \"\"\"Computes the value of the objective function f(x).\"\"\"\n        return 0.5 * x.T @ Q @ x - b.T @ x\n\n    for case in test_cases:\n        Q = case[\"Q\"]\n        b = case[\"b\"]\n        x_k = case[\"x0\"].copy()\n\n        # Compute the analytical solution x_star\n        x_star = np.linalg.solve(Q, b)\n\n        # Main loop for the steepest descent algorithm\n        for _ in range(N_max):\n            # Compute the gradient at the current iterate x_k\n            grad_f = Q @ x_k - b\n            grad_norm = np.linalg.norm(grad_f)\n\n            # Check for termination based on gradient norm\n            if grad_norm <= epsilon:\n                break\n            \n            # Backtracking line search to find the step size alpha_k\n            alpha = alpha0\n            f_k = objective_function(x_k, Q, b)\n            grad_norm_sq = grad_norm**2 # More efficient than dot product\n\n            while True:\n                # Armijo condition check\n                f_new = objective_function(x_k - alpha * grad_f, Q, b)\n                if f_new <= f_k - c * alpha * grad_norm_sq:\n                    break\n                \n                # Shrink alpha if condition is not met\n                alpha *= rho\n\n            # Update the iterate\n            x_k = x_k - alpha * grad_f\n        \n        # The loop terminates, x_k is the terminal iterate x_T\n        x_terminal = x_k\n\n        # Compute the final Euclidean error\n        error = np.linalg.norm(x_terminal - x_star)\n        results.append(error)\n\n    # Print the results in the specified format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2434090"}]}