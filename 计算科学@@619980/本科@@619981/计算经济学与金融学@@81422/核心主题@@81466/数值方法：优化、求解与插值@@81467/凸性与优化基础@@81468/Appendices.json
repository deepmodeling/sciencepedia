{"hands_on_practices": [{"introduction": "我们将从金融领域一个经典而核心的应用开始：投资组合优化。这个练习的目标是构建一个“全局最小方差”投资组合，即在所有可能的资产配置中找到风险（以方差衡量）最低的一种。通过求解这个问题，您不仅能练习二次型优化，还能通过一个可能反直觉的案例，深刻理解多样化投资的真正含义——即资产之间的相关性，而非仅仅其个体风险，才是决定整体投资组合风险的关键 [@problem_id:2384386]。", "problem": "考虑两种风险资产，分别标记为资产 $A$ 和资产 $B$。它们的年化标准差分别为 $\\sigma_A = 0.20$ 和 $\\sigma_B = 0.50$。它们收益率之间的相关性为 $\\rho = 0.95$。投资者可以采用任何满足 $w_A + w_B = 1$ 的实值投资组合权重 $(w_A, w_B)$；允许卖空。投资组合的方差为 $w^{\\top} \\Sigma w$，其中 $\\Sigma$ 是协方差矩阵，其元素为 $\\Sigma_{11} = \\sigma_A^2$，$\\Sigma_{22} = \\sigma_B^2$ 以及 $\\Sigma_{12} = \\Sigma_{21} = \\rho \\sigma_A \\sigma_B$。\n\n将全局最小方差投资组合定义为在约束条件 $w_A + w_B = 1$ 下，使得 $w^{\\top} \\Sigma w$ 最小化的全投资组合 $(w_A, w_B)$。\n\n在全局最小方差投资组合中，资产 $B$（方差较高的资产）的权重是多少？将您的答案四舍五入到四位有效数字。请以纯数字形式表示您的答案（不带单位）。", "solution": "问题陈述已经过验证，并被认定为有效。它提出了一个来自现代投资组合理论领域的标准、适定的约束优化问题，该理论是计算金融的核心组成部分。所有提供的数据和条件都是科学合理且一致的。\n\n目标是找到投资组合权重 $(w_A, w_B)$，以在投资组合被完全投资的约束下最小化投资组合方差 $V$。对于资产 $A$ 和 $B$ 两种资产，投资组合方差由以下二次型给出：\n$$V(w_A, w_B) = w_A^2 \\sigma_A^2 + w_B^2 \\sigma_B^2 + 2 w_A w_B \\rho \\sigma_A \\sigma_B$$\n其中 $w_A$ 和 $w_B$ 分别是资产 $A$ 和资产 $B$ 的权重。权重受以下线性约束：\n$$w_A + w_B = 1$$\n这个约束使我们能够将问题简化为单个变量的无约束优化问题。通过将 $w_A = 1 - w_B$ 代入方差方程，我们将方差 $V$ 表示为仅关于 $w_B$ 的函数：\n$$V(w_B) = (1 - w_B)^2 \\sigma_A^2 + w_B^2 \\sigma_B^2 + 2 (1 - w_B) w_B \\rho \\sigma_A \\sigma_B$$\n为了找到使该方差最小化的 $w_B$ 的值，我们必须通过求 $V(w_B)$ 关于 $w_B$ 的一阶导数并将其设为零来找到临界点。这是微分学的一个直接应用。\n$$\\frac{d V}{d w_B} = \\frac{d}{d w_B} \\left[ (1 - 2w_B + w_B^2)\\sigma_A^2 + w_B^2 \\sigma_B^2 + (2w_B - 2w_B^2)\\rho \\sigma_A \\sigma_B \\right]$$\n逐项应用微分的幂法则得到：\n$$\\frac{d V}{d w_B} = (-2 + 2w_B)\\sigma_A^2 + 2w_B \\sigma_B^2 + (2 - 4w_B)\\rho \\sigma_A \\sigma_B$$\n将导数设为零以定位极值点：\n$$(-2 + 2w_B)\\sigma_A^2 + 2w_B \\sigma_B^2 + (2 - 4w_B)\\rho \\sigma_A \\sigma_B = 0$$\n我们现在通过合并包含 $w_B$ 的项来求解 $w_B$：\n$$2w_B\\sigma_A^2 - 2\\sigma_A^2 + 2w_B \\sigma_B^2 + 2\\rho \\sigma_A \\sigma_B - 4w_B\\rho \\sigma_A \\sigma_B = 0$$\n$$w_B(2\\sigma_A^2 + 2\\sigma_B^2 - 4\\rho \\sigma_A \\sigma_B) = 2\\sigma_A^2 - 2\\rho \\sigma_A \\sigma_B$$\n两边除以 $2$ 可以简化表达式：\n$$w_B(\\sigma_A^2 + \\sigma_B^2 - 2\\rho \\sigma_A \\sigma_B) = \\sigma_A^2 - \\rho \\sigma_A \\sigma_B$$\n分离出 $w_B$ 得到全局最小方差投资组合中资产 $B$ 权重的一般公式：\n$$w_B = \\frac{\\sigma_A^2 - \\rho \\sigma_A \\sigma_B}{\\sigma_A^2 + \\sigma_B^2 - 2\\rho \\sigma_A \\sigma_B}$$\n二阶导数 $\\frac{d^2 V}{d w_B^2} = 2(\\sigma_A^2 + \\sigma_B^2 - 2\\rho \\sigma_A \\sigma_B)$，由于 $|\\rho|  1$ 恒为正，这证实了所找到的临界点是一个全局最小值点。\n\n现在，我们代入所给的数值：$\\sigma_A = 0.20$，$\\sigma_B = 0.50$ 和 $\\rho = 0.95$。\n首先，我们计算方差和协方差：\n$$\\sigma_A^2 = (0.20)^2 = 0.04$$\n$$\\sigma_B^2 = (0.50)^2 = 0.25$$\n$$\\text{Cov}(A,B) = \\rho \\sigma_A \\sigma_B = (0.95)(0.20)(0.50) = 0.095$$\n将这些值代入推导出的 $w_B$ 公式中：\n$$w_B = \\frac{0.04 - 0.095}{0.04 + 0.25 - 2(0.095)}$$\n$$w_B = \\frac{-0.055}{0.29 - 0.19}$$\n$$w_B = \\frac{-0.055}{0.10} = -0.55$$\n问题要求答案四舍五入到四位有效数字。因此，资产 $B$ 的权重是 $-0.5500$。负值表示对资产 $B$ 持有空头头寸，这是问题陈述所允许的。因此，资产 A 的权重是 $w_A = 1 - w_B = 1 - (-0.55) = 1.55$。", "answer": "$$\n\\boxed{-0.5500}\n$$", "id": "2384386"}, {"introduction": "在掌握了无约束优化的基础后，我们将转向在经济学中更为普遍的场景：带约束的优化问题。本练习将模拟一个企业在必须满足特定生产配额的条件下，如何选择投入组合以实现成本最小化 [@problem_id:2384397]。这个过程将引导您应用Karush-Kuhn-Tucker (KKT) 条件，并且更为重要的是，让您亲手揭示拉格朗日乘子（Lagrange multiplier）背后深刻的经济含义——“影子价格”，这是理解资源分配和决策分析的基石。", "problem": "一个竞争性企业使用两种投入 $x_1$ 和 $x_2$ 来生产产出 $y$，其投入价格分别为 $w_1$ 和 $w_2$，生产函数为 $f(x_1,x_2) = x_1^{1/2} x_2^{1/2}$。该企业必须满足至少 $q_0$ 单位的生产配额。企业选择投入以在满足配额的前提下最小化总成本。假设单位投入价格 $w_1 = 4$，$w_2 = 1$，生产配额 $q_0 = 10$ 单位产出。\n\n建立成本最小化问题及其带有单个不等式约束的拉格朗日函数，使用 Karush-Kuhn-Tucker (KKT) 条件推导出一阶必要条件，并求解最优投入和相关的拉格朗日乘数。使用最优解下的对偶变量（拉格朗日乘数）来估计当生产配额从 $q_0$ 增加到 $q_0 + 1$（即增加一单位）时，企业最小总成本的变化。请以美元表示最终答案。提供一个单一的数值，无需四舍五入。", "solution": "对问题陈述进行验证。\n\n第一步：提取已知条件。\n- 生产函数: $f(x_1,x_2) = x_1^{1/2} x_2^{1/2}$\n- 投入: $x_1$, $x_2$\n- 投入价格: $w_1 = 4$ 美元/单位, $w_2 = 1$ 美元/单位\n- 生产配额: 至少 $q_0 = 10$ 单位\n- 目标: 最小化总成本, $C = w_1 x_1 + w_2 x_2$。\n- 任务: 建立成本最小化问题及其拉格朗日函数，使用 Karush-Kuhn-Tucker (KKT) 条件推导出一阶必要条件，求解最优投入和拉格朗日乘数，并使用该乘数估计配额增加一单位时最小成本的变化。\n\n第二步：使用提取的已知条件进行验证。\n- **科学依据：** 该问题是微观经济学理论中的一个标准练习，特别是企业理论。它使用了 Cobb-Douglas 生产函数，这是经济学中的一个经典模型。所有概念都已成熟。这在科学上是合理的。\n- **适定性：** 该问题是一个约束优化问题。目标函数是线性的（因此是凸函数），约束函数 $g(x_1, x_2) = x_1^{1/2} x_2^{1/2}$ 是拟凹函数，它定义了一个凸的上水平集。这种结构确保了这是一个具有唯一解的适定最小化问题。\n- **客观性：** 该问题使用精确、无歧义的数学和经济术语进行陈述。\n\n第三步：结论与行动。\n该问题是有效的。它在科学上是合理的、适定的和客观的。我将继续进行解答。\n\n企业的问题是在其产出 $y = f(x_1, x_2)$ 至少为配额量 $q_0$ 的约束下，最小化其总成本 $C(x_1, x_2) = w_1 x_1 + w_2 x_2$。投入的非负性 $x_1 \\ge 0$ 和 $x_2 \\ge 0$ 是由生产函数的形式所隐含要求的。\n\n代入给定值，该优化问题为：\n$$\n\\text{最小化 } C(x_1, x_2) = 4x_1 + x_2\n$$\n$$\n\\text{约束条件为 } x_1^{1/2} x_2^{1/2} \\ge 10\n$$\n\n为了应用 Karush-Kuhn-Tucker (KKT) 框架，我们将约束写成标准形式 $g(x) \\le 0$。\n约束条件 $x_1^{1/2} x_2^{1/2} \\ge 10$ 等价于 $10 - x_1^{1/2} x_2^{1/2} \\le 0$。\n\n拉格朗日函数 $\\mathcal{L}$ 的构造方法是目标函数加上拉格朗日乘数 $\\lambda$ 乘以约束函数：\n$$\n\\mathcal{L}(x_1, x_2, \\lambda) = 4x_1 + x_2 + \\lambda(10 - x_1^{1/2} x_2^{1/2})\n$$\n\n最优解 $(x_1^*, x_2^*, \\lambda^*)$ 的 KKT 条件如下：\n1.  一阶条件（平稳性）：拉格朗日函数对选择变量的偏导数必须为零。\n    $$\n    \\frac{\\partial \\mathcal{L}}{\\partial x_1} = 4 - \\lambda \\left( \\frac{1}{2} x_1^{-1/2} x_2^{1/2} \\right) = 0\n    $$\n    $$\n    \\frac{\\partial \\mathcal{L}}{\\partial x_2} = 1 - \\lambda \\left( \\frac{1}{2} x_1^{1/2} x_2^{-1/2} \\right) = 0\n    $$\n2.  原始可行性：解必须满足原始约束。\n    $$\n    10 - x_1^{1/2} x_2^{1/2} \\le 0 \\quad \\text{或} \\quad x_1^{1/2} x_2^{1/2} \\ge 10\n    $$\n3.  对偶可行性：在最小化问题中，与`小于或等于`不等式约束相关联的拉格朗日乘数必须为非负。\n    $$\n    \\lambda \\ge 0\n    $$\n4.  互补松弛性：在最优点，要么约束是紧的，要么乘数为零（或两者都成立）。\n    $$\n    \\lambda (10 - x_1^{1/2} x_2^{1/2}) = 0\n    $$\n\n我们现在求解这个条件系统。\n从一阶条件出发，假设 $x_1  0$ 且 $x_2  0$：\n$$\n4 = \\frac{\\lambda}{2} \\sqrt{\\frac{x_2}{x_1}} \\quad (1)\n$$\n$$\n1 = \\frac{\\lambda}{2} \\sqrt{\\frac{x_1}{x_2}} \\quad (2)\n$$\n等式右边为正，这意味着 $\\lambda$ 必须严格为正（$\\lambda  0$）。如果 $\\lambda$ 为零，一阶条件将要求 $4=0$ 和 $1=0$，这是荒谬的。\n因为 $\\lambda  0$，互补松弛性条件 $\\lambda (10 - x_1^{1/2} x_2^{1/2}) = 0$ 意味着约束必须是紧的：\n$$\n10 - x_1^{1/2} x_2^{1/2} = 0 \\implies x_1^{1/2} x_2^{1/2} = 10 \\quad (3)\n$$\n现在，我们可以通过将方程 $(1)$ 除以方程 $(2)$ 来求解 $x_1$ 和 $x_2$ 之间的关系：\n$$\n\\frac{4}{1} = \\frac{\\frac{\\lambda}{2} \\sqrt{\\frac{x_2}{x_1}}}{\\frac{\\lambda}{2} \\sqrt{\\frac{x_1}{x_2}}} = \\frac{\\sqrt{x_2}}{\\sqrt{x_1}} \\cdot \\frac{\\sqrt{x_2}}{\\sqrt{x_1}} = \\frac{x_2}{x_1}\n$$\n这给出了企业的最优投入扩展路径：$x_2 = 4x_1$。\n\n将此关系代入紧约束，即方程 $(3)$ 中：\n$$\n\\sqrt{x_1 (4x_1)} = 10\n$$\n$$\n\\sqrt{4x_1^2} = 10\n$$\n$$\n2x_1 = 10\n$$\n求解 $x_1$ 得到第一种投入的最优数量：\n$$\nx_1^* = 5\n$$\n现在，我们求第二种投入的最优数量：\n$$\nx_2^* = 4x_1^* = 4(5) = 20\n$$\n最优投入组合是 $(x_1^*, x_2^*) = (5, 20)$。\n\n最后，我们使用方程 $(2)$ 和最优投入来求解拉格朗日乘数 $\\lambda^*$：\n$$\n1 = \\frac{\\lambda^*}{2} \\sqrt{\\frac{x_1^*}{x_2^*}} = \\frac{\\lambda^*}{2} \\sqrt{\\frac{5}{20}} = \\frac{\\lambda^*}{2} \\sqrt{\\frac{1}{4}} = \\frac{\\lambda^*}{2} \\cdot \\frac{1}{2} = \\frac{\\lambda^*}{4}\n$$\n这得出：\n$$\n\\lambda^* = 4\n$$\n最优拉格朗日乘数为 $\\lambda^*=4$。我们确认 $\\lambda^* \\ge 0$，满足对偶可行性条件。\n\n在成本最小化背景下，拉格朗日乘数 $\\lambda^*$ 的经济学解释是约束的边际成本。它表示企业的最小成本相对于生产配额 $q_0$ 放宽的变化率。\n即 $\\lambda^* = \\frac{dC^*(q_0)}{dq_0}$，其中 $C^*$ 是最小化后的成本。\n\n问题要求估计当生产配额增加一单位，从 $q_0 = 10$ 增加到 $q_0' = 11$ 时，最小总成本的变化。这个变化 $\\Delta q_0$ 是 $11 - 10 = 1$。\n使用一阶线性近似，最小成本的变化 $\\Delta C^*$ 为：\n$$\n\\Delta C^* \\approx \\lambda^* \\cdot \\Delta q_0\n$$\n代入计算出的值：\n$$\n\\Delta C^* \\approx 4 \\cdot 1 = 4\n$$\n企业最小总成本的估计增加值为 $4$ 美元。\n初始最小成本为 $C^* = 4x_1^* + x_2^* = 4(5) + 20 = 20 + 20 = 40$。新的估计最小成本将是 $40+4=44$。", "answer": "$$\\boxed{4}$$", "id": "2384397"}, {"introduction": "理论分析为我们提供了优化的基本框架，但在处理复杂的高维问题时，我们必须依赖计算方法。这个实践将带您进入数值优化的核心，通过亲手实现并比较两种最经典的算法：梯度下降法（一阶方法）与牛顿法（二阶方法）[@problem_id:2384404]。您将通过实验直观地看到，问题的“病态”程度（通过海森矩阵的条件数来衡量）如何剧烈影响算法的收敛速度，从而深刻体会到不同优化方法在计算简易性与收敛效率之间的权衡。", "problem": "考虑一个有限维欧几里得空间中凹二次效用函数的最大化问题。设效用函数为 $u(\\mathbf{x}) = \\mathbf{b}^{\\top}\\mathbf{x} - \\tfrac{1}{2}\\mathbf{x}^{\\top}\\mathbf{Q}\\mathbf{x}$，其中 $\\mathbf{x} \\in \\mathbb{R}^{n}$，$\\mathbf{b} \\in \\mathbb{R}^{n}$，且 $\\mathbf{Q} \\in \\mathbb{R}^{n \\times n}$ 是对称正定矩阵。该效用函数是严格凹的，其负数定义了一个凸最小化问题。定义凸目标函数 $f(\\mathbf{x}) = -u(\\mathbf{x}) = \\tfrac{1}{2}\\mathbf{x}^{\\top}\\mathbf{Q}\\mathbf{x} - \\mathbf{b}^{\\top}\\mathbf{x}$。$f$ 的海森矩阵等于 $\\mathbf{Q}$，海森矩阵关于欧几里得范数的条件数为 $\\kappa(\\mathbf{Q}) = \\lambda_{\\max}(\\mathbf{Q})/\\lambda_{\\min}(\\mathbf{Q})$，其中 $\\lambda_{\\max}$ 和 $\\lambda_{\\min}$ 分别表示最大和最小特征值。你将比较求解 $\\min_{\\mathbf{x}} f(\\mathbf{x})$ 的两种算法的收敛行为：带回溯线搜索的梯度下降法和带回溯线搜索的牛顿法。\n\n仅从多元微积分和凸分析的核心定义出发（梯度作为偏导数向量，海森矩阵作为梯度的雅可比矩阵，无约束凸最小化问题的一阶最优性条件，以及Armijo充分下降条件），推导出为上述二次函数 $f(\\mathbf{x})$ 实现这两种算法所必需的解析表达式。按如下方式实现这两种算法：\n\n- 带回溯的梯度下降法：在迭代点 $\\mathbf{x}_{k}$，使用负梯度计算下降方向，并执行满足Armijo充分下降条件的回溯线搜索来选择步长。更新迭代点并重复，直至收敛。\n- 带回溯的牛顿法：在迭代点 $\\mathbf{x}_{k}$，通过求解由海森矩阵和梯度定义的线性系统来计算牛顿方向，并执行满足Armijo充分下降条件的回溯线搜索来选择步长。更新迭代点并重复，直至收敛。\n\n对两种方法使用以下共享参数：\n- Armijo参数 $c_{1} = 10^{-4}$，\n- 回溯收缩因子 $\\tau = \\tfrac{1}{2}$，\n- 初始试探步长 $\\alpha_{0} = 1$，\n- 基于梯度欧几里得范数的停止容差：当 $\\lVert \\nabla f(\\mathbf{x}_{k}) \\rVert_{2} \\le \\varepsilon$ 且 $\\varepsilon = 10^{-6}$ 时停止，\n- 梯度下降法的最大迭代次数为 $200{,}000$，牛顿法的最大迭代次数为 $1{,}000$。\n\n测试套件。在以下四个测试用例上运行这两种方法，每个用例由 $(\\mathbf{Q}, \\mathbf{b}, \\mathbf{x}_{0})$ 指定，其中 $\\mathbf{x}_{0}$ 是起始点。\n\n- 用例 $1$（边界最优，病态但平凡）：\n  $$\\mathbf{Q} = \\begin{bmatrix} 1  0 \\\\ 0  100 \\end{bmatrix},\\quad \\mathbf{b} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix},\\quad \\mathbf{x}_{0} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}.$$\n- 用例 $2$（病态，中等）：\n  $$\\mathbf{Q} = \\begin{bmatrix} 1  0 \\\\ 0  100 \\end{bmatrix},\\quad \\mathbf{b} = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix},\\quad \\mathbf{x}_{0} = \\begin{bmatrix} 10 \\\\ -10 \\end{bmatrix}.$$\n- 用例 $3$（病态，更强）：\n  $$\\mathbf{Q} = \\begin{bmatrix} 1  0 \\\\ 0  500 \\end{bmatrix},\\quad \\mathbf{b} = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix},\\quad \\mathbf{x}_{0} = \\begin{bmatrix} 10 \\\\ -10 \\end{bmatrix}.$$\n- 用例 $4$（病态，更强）：\n  $$\\mathbf{Q} = \\begin{bmatrix} 1  0 \\\\ 0  2000 \\end{bmatrix},\\quad \\mathbf{b} = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix},\\quad \\mathbf{x}_{0} = \\begin{bmatrix} 10 \\\\ -10 \\end{bmatrix}.$$\n\n对于每个用例，运行两种算法，直到满足停止准则或达到迭代次数上限。对于每个用例，返回迭代次数的有序对 $[\\text{iters\\_GD}, \\text{iters\\_Newton}]$，其中 $\\text{iters\\_GD}$ 是梯度下降法使用的迭代次数，$\\text{iters\\_Newton}$ 是牛顿法使用的迭代次数。\n\n你的程序应生成单行输出，其中包含一个由方括号括起来的逗号分隔列表形式的结果，每个测试用例对应一个子列表，顺序与上文相同。例如，输出格式必须为\n$$[\\,[\\text{iters\\_GD}^{(1)},\\text{iters\\_Newton}^{(1)}],\\,[\\text{iters\\_GD}^{(2)},\\text{iters\\_Newton}^{(2)}],\\,[\\text{iters\\_GD}^{(3)},\\text{iters\\_Newton}^{(3)}],\\,[\\text{iters\\_GD}^{(4)},\\text{iters\\_Newton}^{(4)}]\\,].$$\n输出中的所有数字必须是整数，且不应打印任何额外文本。", "solution": "对所给定的问题陈述进行验证。\n\n**第1步：提取给定信息**\n\n- **目标函数：** 最小化 $f(\\mathbf{x}) = \\tfrac{1}{2}\\mathbf{x}^{\\top}\\mathbf{Q}\\mathbf{x} - \\mathbf{b}^{\\top}\\mathbf{x}$，其中 $\\mathbf{x}, \\mathbf{b} \\in \\mathbb{R}^{n}$ 且 $\\mathbf{Q} \\in \\mathbb{R}^{n \\times n}$ 是对称正定矩阵。\n- **算法：** 梯度下降法和牛顿法，两者均带有回溯线搜索。\n- **线搜索参数：**\n    - Armijo充分下降参数： $c_{1} = 10^{-4}$。\n    - 收缩因子： $\\tau = \\tfrac{1}{2}$。\n    - 初始步长： $\\alpha_{0} = 1$。\n- **终止条件：**\n    - 梯度范数的停止容差： $\\lVert \\nabla f(\\mathbf{x}_{k}) \\rVert_{2} \\le \\varepsilon = 10^{-6}$。\n    - 梯度下降法的最大迭代次数： $200,000$。\n    - 牛顿法的最大迭代次数： $1,000$。\n- **测试用例：** 提供了四个用例，每个用例由一个元组 $(\\mathbf{Q}, \\mathbf{b}, \\mathbf{x}_{0})$ 定义。\n    1.  $\\mathbf{Q} = \\begin{bmatrix} 1  0 \\\\ 0  100 \\end{bmatrix}, \\mathbf{b} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}, \\mathbf{x}_{0} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$。\n    2.  $\\mathbf{Q} = \\begin{bmatrix} 1  0 \\\\ 0  100 \\end{bmatrix}, \\mathbf{b} = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}, \\mathbf{x}_{0} = \\begin{bmatrix} 10 \\\\ -10 \\end{bmatrix}$。\n    3.  $\\mathbf{Q} = \\begin{bmatrix} 1  0 \\\\ 0  500 \\end{bmatrix}, \\mathbf{b} = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}, \\mathbf{x}_{0} = \\begin{bmatrix} 10 \\\\ -10 \\end{bmatrix}$。\n    4.  $\\mathbf{Q} = \\begin{bmatrix} 1  0 \\\\ 0  2000 \\end{bmatrix}, \\mathbf{b} = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}, \\mathbf{x}_{0} = \\begin{bmatrix} 10 \\\\ -10 \\end{bmatrix}$。\n\n**第2步：使用提取的给定信息进行验证**\n\n该问题具有科学依据，是适定且客观的。这是数值凸优化中的一个标准问题，比较了一阶和二阶方法在凸二次函数上的性能。所有参数、初始条件和矩阵都已明确定义。矩阵 $\\mathbf{Q}$ 是对称的，并且由于对角线元素为正，因此是正定的，从而保证了 $f(\\mathbf{x})$ 的严格凸性以及唯一极小值点的存在。用例1是一个平凡的边界情况，其中起始点即为解，这对于任何正确实现都是一个有效的测试。该问题是完整的、一致的且科学上合理的。\n\n**第3步：结论与行动**\n\n此问题**有效**。将提供一个解法。\n\n**推导与算法设计**\n\n目标是找到 $\\mathbf{x}^* = \\arg\\min_{\\mathbf{x} \\in \\mathbb{R}^n} f(\\mathbf{x})$，其中 $f(\\mathbf{x}) = \\tfrac{1}{2}\\mathbf{x}^{\\top}\\mathbf{Q}\\mathbf{x} - \\mathbf{b}^{\\top}\\mathbf{x}$。\n\n**一阶和二阶导数**\n为了应用基于梯度的优化方法，我们首先推导 $f(\\mathbf{x})$ 的梯度和海森矩阵。使用矩阵微积分的基本法则：\n$f(\\mathbf{x})$ 的梯度是其偏导数的向量，$\\nabla f(\\mathbf{x}) = \\left[ \\frac{\\partial f}{\\partial x_i} \\right]_{i=1}^n$。\n$$\n\\nabla f(\\mathbf{x}) = \\nabla \\left( \\tfrac{1}{2}\\mathbf{x}^{\\top}\\mathbf{Q}\\mathbf{x} - \\mathbf{b}^{\\top}\\mathbf{x} \\right) = \\tfrac{1}{2}(\\mathbf{Q} + \\mathbf{Q}^{\\top})\\mathbf{x} - \\mathbf{b}\n$$\n因为 $\\mathbf{Q}$ 是对称的，所以 $\\mathbf{Q} = \\mathbf{Q}^{\\top}$，梯度可简化为：\n$$\n\\nabla f(\\mathbf{x}) = \\mathbf{Q}\\mathbf{x} - \\mathbf{b}\n$$\n$f(\\mathbf{x})$ 的海森矩阵是二阶偏导数的矩阵，$\\nabla^2 f(\\mathbf{x}) = \\left[ \\frac{\\partial^2 f}{\\partial x_i \\partial x_j} \\right]_{i,j=1}^n$。它是梯度的雅可比矩阵：\n$$\n\\nabla^2 f(\\mathbf{x}) = \\nabla (\\mathbf{Q}\\mathbf{x} - \\mathbf{b}) = \\mathbf{Q}\n$$\n海森矩阵是常数，等于矩阵 $\\mathbf{Q}$。因为 $\\mathbf{Q}$ 是正定的，所以 $f(\\mathbf{x})$ 是一个严格凸函数。\n\n**最优性条件**\n对于无约束凸优化问题，点 $\\mathbf{x}^*$ 成为全局极小值点的一阶充要条件是梯度为零：\n$$\n\\nabla f(\\mathbf{x}^*) = \\mathbf{Q}\\mathbf{x}^* - \\mathbf{b} = \\mathbf{0}\n$$\n因为 $\\mathbf{Q}$ 是可逆的，所以唯一解由 $\\mathbf{x}^* = \\mathbf{Q}^{-1}\\mathbf{b}$ 给出。我们的任务是迭代地找到这个解。\n\n**迭代算法**\n两种算法都遵循更新规则 $\\mathbf{x}_{k+1} = \\mathbf{x}_{k} + \\alpha_k \\mathbf{p}_k$，其中 $\\mathbf{p}_k$ 是一个搜索方向，$\\alpha_k  0$ 是通过线搜索确定的步长。\n\n**1. 带回溯线搜索的梯度下降法**\n- **搜索方向：** 最速下降方向是负梯度：\n  $$\n  \\mathbf{p}_k = -\\nabla f(\\mathbf{x}_k) = -(\\mathbf{Q}\\mathbf{x}_k - \\mathbf{b}) = \\mathbf{b} - \\mathbf{Q}\\mathbf{x}_k\n  $$\n- **线搜索：** 步长 $\\alpha_k$ 通过回溯法找到。从一个初始试探步长 $\\alpha = \\alpha_0$ 开始，我们迭代地将其乘以一个因子 $\\tau$ 来减小它，直到满足Armijo充分下降条件：\n  $$\n  f(\\mathbf{x}_k + \\alpha \\mathbf{p}_k) \\le f(\\mathbf{x}_k) + c_1 \\alpha \\nabla f(\\mathbf{x}_k)^{\\top} \\mathbf{p}_k\n  $$\n  代入 $\\mathbf{p}_k = -\\nabla f(\\mathbf{x}_k)$，该条件变为：\n  $$\n  f(\\mathbf{x}_k - \\alpha \\nabla f(\\mathbf{x}_k)) \\le f(\\mathbf{x}_k) - c_1 \\alpha \\lVert \\nabla f(\\mathbf{x}_k) \\rVert_2^2\n  $$\n该算法通过生成一个迭代序列 $\\{\\mathbf{x}_k\\}$ 来进行，直到 $\\lVert \\nabla f(\\mathbf{x}_k) \\rVert_2 \\le \\varepsilon$。已知梯度下降法的收敛速率会随着条件数 $\\kappa(\\mathbf{Q})$ 的增大而降低。\n\n**2. 带回溯线搜索的牛顿法**\n- **搜索方向：** 牛顿法使用函数的二阶近似。牛顿方向 $\\mathbf{p}_k^{\\text{N}}$ 是通过求解以下线性系统找到的：\n  $$\n  \\nabla^2 f(\\mathbf{x}_k) \\mathbf{p}_k^{\\text{N}} = -\\nabla f(\\mathbf{x}_k)\n  $$\n  对于我们的二次目标函数，这变为：\n  $$\n  \\mathbf{Q} \\mathbf{p}_k^{\\text{N}} = -(\\mathbf{Q}\\mathbf{x}_k - \\mathbf{b})\n  $$\n  求解 $\\mathbf{p}_k^{\\text{N}}$ 得到：\n  $$\n  \\mathbf{p}_k^{\\text{N}} = -\\mathbf{Q}^{-1}(\\mathbf{Q}\\mathbf{x}_k - \\mathbf{b}) = \\mathbf{Q}^{-1}\\mathbf{b} - \\mathbf{x}_k = \\mathbf{x}^* - \\mathbf{x}_k\n  $$\n  牛顿方向直接从当前迭代点指向精确的极小值点。\n- **线搜索：** 线搜索的执行方式与梯度下降法中相同。让我们分析初始试探步长 $\\alpha=1$ 的Armijo条件：\n  $$\n  f(\\mathbf{x}_k + \\mathbf{p}_k^{\\text{N}}) \\le f(\\mathbf{x}_k) + c_1 \\nabla f(\\mathbf{x}_k)^{\\top} \\mathbf{p}_k^{\\text{N}}\n  $$\n  对于二次函数，$f(\\mathbf{x}_k + \\alpha\\mathbf{p}_k^{\\text{N}})$ 在 $\\mathbf{x}_k$ 附近的泰勒展开到二阶是精确的：\n  $$\n  f(\\mathbf{x}_k + \\alpha\\mathbf{p}_k^{\\text{N}}) = f(\\mathbf{x}_k) + \\alpha \\nabla f(\\mathbf{x}_k)^{\\top}\\mathbf{p}_k^{\\text{N}} + \\frac{1}{2}\\alpha^2 (\\mathbf{p}_k^{\\text{N}})^{\\top}\\mathbf{Q}\\mathbf{p}_k^{\\text{N}}\n  $$\n  代入 $\\mathbf{Q}\\mathbf{p}_k^{\\text{N}} = -\\nabla f(\\mathbf{x}_k)$:\n  $$\n  f(\\mathbf{x}_k + \\alpha\\mathbf{p}_k^{\\text{N}}) = f(\\mathbf{x}_k) + \\alpha \\nabla f(\\mathbf{x}_k)^{\\top}\\mathbf{p}_k^{\\text{N}} - \\frac{1}{2}\\alpha^2 \\nabla f(\\mathbf{x}_k)^{\\top}\\mathbf{p}_k^{\\text{N}}\n  $$\n  Armijo条件变为：\n  $$\n  f(\\mathbf{x}_k) + \\alpha \\nabla f(\\mathbf{x}_k)^{\\top}\\mathbf{p}_k^{\\text{N}} - \\frac{1}{2}\\alpha^2 \\nabla f(\\mathbf{x}_k)^{\\top}\\mathbf{p}_k^{\\text{N}} \\le f(\\mathbf{x}_k) + c_1 \\alpha \\nabla f(\\mathbf{x}_k)^{\\top} \\mathbf{p}_k^{\\text{N}}\n  $$\n  由于 $\\mathbf{p}_k^{\\text{N}}$ 是一个下降方向，$\\nabla f(\\mathbf{x}_k)^{\\top}\\mathbf{p}_k^{\\text{N}} = -(\\mathbf{p}_k^{\\text{N}})^{\\top}\\mathbf{Q}\\mathbf{p}_k^{\\text{N}}  0$。我们可以用这个负项相除，不等号反向：\n  $$\n  1 - \\frac{1}{2}\\alpha \\ge c_1 \\implies \\alpha \\le 2(1-c_1)\n  $$\n  当 $c_1=10^{-4}$ 时，条件是 $\\alpha \\le 2(1 - 10^{-4}) = 1.9998$。由于初始试探步长为 $\\alpha_0 = 1$，小于 $1.9998$，因此步长 $\\alpha_k=1$ 将总是被接受。\n  更新是 $\\mathbf{x}_{k+1} = \\mathbf{x}_k + 1 \\cdot (\\mathbf{x}^* - \\mathbf{x}_k) = \\mathbf{x}^*$。\n  因此，对于任何非最优的起始点，带有指定回溯参数的牛顿法将在单次迭代中收敛到精确解。\n\n**实现摘要**\n实现将包含两个函数，每个算法一个。每个函数将根据其特定规则迭代更新解向量 $\\mathbf{x}$，并执行回溯线搜索来确定步长。当梯度的欧几里得范数低于容差 $\\varepsilon$ 或达到最大迭代次数时，循环终止。\n对于用例1，由于 $\\mathbf{x}_0 = \\mathbf{0}$ 和 $\\mathbf{b} = \\mathbf{0}$，初始梯度 $\\nabla f(\\mathbf{x}_0) = \\mathbf{Q}\\mathbf{0} - \\mathbf{0} = \\mathbf{0}$。停止条件在开始时即满足，因此两种算法都将报告 $0$ 次迭代。\n对于用例2、3和4，牛顿法预计需要 $1$ 次迭代，而梯度下降法预计需要大量的迭代次数，该次数随 $\\mathbf{Q}$ 的条件数的增加而增加。", "answer": "[[0,0],[38510,1],[84358,1],[138758,1]]", "id": "2384404"}]}