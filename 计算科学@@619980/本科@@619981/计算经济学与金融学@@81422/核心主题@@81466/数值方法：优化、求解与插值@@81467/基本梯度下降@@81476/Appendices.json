{"hands_on_practices": [{"introduction": "理论是基础，但理解其在实践中的微妙之处同样重要。本练习是一个关键的“思想实验”，不涉及编码，但能深刻揭示梯度下降法的一个核心挑战。我们将探讨在一个非凸的损失函数上，初始点的微小变化如何导致算法收敛到截然不同的局部最优解，从而突显了“差之毫厘，谬以千里”的现象。这个练习有助于你理解初始值选择的重要性以及局部最优解的“引力盆”（basins of attraction）概念。[@problem_id:2375265]", "problem": "一位计算金融研究员正在一个代表性代理人资产定价模型中校准一个标量偏好参数 $\\theta$，方法是最小化一个捕捉了模型设定不当和弱经验性偏差的风格化非凸损失函数。该损失函数为\n$$\nL(\\theta) \\;=\\; (\\theta^{2} - 1)^{2} \\;-\\; 0.2\\,\\theta.\n$$\n该研究员使用恒定步长 $\\alpha$ 的标准梯度下降法，其更新规则如下：\n$$\n\\theta_{t+1} \\;=\\; \\theta_{t} \\;-\\; \\alpha\\,\\nabla L(\\theta_{t}).\n$$\n考虑步长 $\\alpha = 0.1$ 以及两个相差 $0.002$ 的初始化值 $\\theta_{0}^{(-)} = -0.051$ 和 $\\theta_{0}^{(+)} = -0.049$。下列哪项最能描述这两次梯度下降运行的渐进行为，并指出是否达到了一个次优最小值？\n\nA. $\\theta_{0}^{(-)}$ 和 $\\theta_{0}^{(+)}$ 都收敛到 $\\theta \\approx -1$ 附近的左侧局部最小值，该值是全局最小值。\n\nB. $\\theta_{0}^{(-)}$ 收敛到 $\\theta \\approx 1$ 附近的右侧局部最小值（全局），而 $\\theta_{0}^{(+)}$ 收敛到 $\\theta \\approx -1$ 附近的左侧局部最小值（次优）。\n\nC. $\\theta_{0}^{(-)}$ 收敛到 $\\theta \\approx -1$ 附近的左侧局部最小值（次优），而 $\\theta_{0}^{(+)}$ 收敛到 $\\theta \\approx 1$ 附近的右侧局部最小值（全局）。\n\nD. 两个序列都发散到 $\\pm\\infty$，因为对于这个损失函数，$\\alpha = 0.1$ 太大而无法保持稳定性。", "solution": "任何科学探究的首要任务是验证问题陈述的有效性。一个有缺陷的前提会导致一个毫无意义的结论。\n\n### 步骤1：提取已知条件\n\n该问题提供了以下信息：\n- 损失函数：$L(\\theta) = (\\theta^{2} - 1)^{2} - 0.2\\,\\theta$。\n- 梯度下降更新规则：$\\theta_{t+1} = \\theta_{t} - \\alpha\\,\\nabla L(\\theta_{t})$。\n- 步长：$\\alpha = 0.1$。\n- 初始化条件：$\\theta_{0}^{(-)} = -0.051$ 和 $\\theta_{0}^{(+)} = -0.049$。\n\n### 步骤2：使用提取的已知条件进行验证\n\n依据有效性所需标准对问题进行分析。\n- **科学依据**：该问题描述了使用标准梯度下降算法最小化一个标量多项式函数。这是数值优化和微积分中一个基础且被充分理解的问题。该函数是非凸的，这在优化中是一个常见的挑战，使得问题变得不平凡且具有现实意义。“计算金融”的背景仅仅是一个设定；核心问题是纯数学的。它是科学上合理的。\n- **适定性**：损失函数被明确定义。更新规则、步长和初始化条件都以精确的数值给出。问题要求序列的渐进行为，这是给定设置下的一个确定性结果。该问题是适定的。\n- **客观性**：问题以客观和精确的数学语言陈述。没有主观或含糊不清的术语。\n\n问题陈述没有违反任何有效性标准。它是一个标准的、自洽的、可解的数学问题。\n\n### 步骤3：结论与行动\n\n问题陈述是**有效的**。我们继续进行求解。\n\n### 解题推导\n\n求解需要分析损失函数 $L(\\theta)$ 的拓扑结构以及梯度下降算法的动态过程。\n\n首先，我们必须通过找到其梯度的根来确定损失函数的临界点。损失函数为 $L(\\theta) = \\theta^4 - 2\\theta^2 + 1 - 0.2\\theta$。\n梯度（一阶导数）为：\n$$\n\\nabla L(\\theta) = L'(\\theta) = \\frac{d}{d\\theta} \\left( \\theta^4 - 2\\theta^2 + 1 - 0.2\\theta \\right) = 4\\theta^3 - 4\\theta - 0.2\n$$\n临界点 $\\theta^*$ 是 $L'(\\theta^*) = 0$ 的解：\n$$\n4(\\theta^*)^3 - 4\\theta^* - 0.2 = 0\n$$\n这是一个三次方程。为了理解它的根，我们观察到对于未受扰动的方程 $4\\theta^3 - 4\\theta = 0$，其根为 $\\theta = 0, \\pm 1$。 $-0.2$ 这一项是一个小扰动。\n- 在 $\\theta = 0$ 附近，$L'(\\theta) \\approx -4\\theta - 0.2$。令其为 $0$ 可得 $-4\\theta \\approx 0.2$，因此 $\\theta \\approx -0.05$。\n- 在 $\\theta = 1$ 附近，令 $\\theta = 1+\\epsilon$。$L'(1+\\epsilon) = 4(1+\\epsilon)^3 - 4(1+\\epsilon) - 0.2 \\approx 4(1+3\\epsilon) - 4(1+\\epsilon) - 0.2 = 8\\epsilon - 0.2$。令其为 $0$ 可得 $8\\epsilon \\approx 0.2$，因此 $\\epsilon \\approx 0.025$，得到一个根在 $\\theta \\approx 1.025$ 附近。\n- 在 $\\theta = -1$ 附近，令 $\\theta = -1+\\epsilon$。$L'(-1+\\epsilon) = 4(-1+\\epsilon)^3 - 4(-1+\\epsilon) - 0.2 \\approx 4(-1+3\\epsilon) - 4(-1+\\epsilon) - 0.2 = 8\\epsilon - 0.2$。令其为 $0$ 可得 $8\\epsilon \\approx 0.2$，因此 $\\epsilon \\approx 0.025$，得到一个根在 $\\theta \\approx -1 + 0.025 = -0.975$ 附近。\n\n所以，我们有三个临界点，大约为 $\\theta_1^* \\approx -0.975$，$\\theta_2^* \\approx -0.05$ 和 $\\theta_3^* \\approx 1.025$。\n\n接下来，我们使用二阶导数检验来对这些临界点进行分类。二阶导数为：\n$$\nL''(\\theta) = \\frac{d}{d\\theta} (4\\theta^3 - 4\\theta - 0.2) = 12\\theta^2 - 4\n$$\n- 在 $\\theta_1^* \\approx -0.975$ 处：$L''(-0.975) = 12(-0.975)^2 - 4 > 12(0.9)^2 - 4 = 12(0.81) - 4 = 9.72 - 4 > 0$。这是一个局部最小值。\n- 在 $\\theta_2^* \\approx -0.05$ 处：$L''(-0.05) = 12(-0.05)^2 - 4 = 12(0.0025) - 4 = 0.03 - 4 < 0$。这是一个局部最大值。\n- 在 $\\theta_3^* \\approx 1.025$ 处：$L''(1.025) = 12(1.025)^2 - 4 > 12(1)^2 - 4 = 8 > 0$。这是一个局部最小值。\n\n我们有两个局部最小值和一个局部最大值。要确定全局最小值，我们必须比较这两个最小值点处的 $L(\\theta)$ 值。\n- $L(\\theta_1^*) \\approx L(-0.975) = ((-0.975)^2 - 1)^2 - 0.2(-0.975) \\approx (0.95 - 1)^2 + 0.195 = 0.0025 + 0.195 = 0.1975$。\n- $L(\\theta_3^*) \\approx L(1.025) = ((1.025)^2 - 1)^2 - 0.2(1.025) \\approx (1.05 - 1)^2 - 0.205 = 0.0025 - 0.205 = -0.2025$。\n由于 $L(\\theta_3^*) < L(\\theta_1^*)$，$\\theta \\approx 1.025$ 附近的局部最小值是**全局最小值**，而 $\\theta \\approx -0.975$ 附近的局部最小值是**次优局部最小值**。\n\n位于 $\\theta_2^* \\approx -0.05$ 的局部最大值充当了梯度流的分界线，或称“分水岭”。位于该最大值左侧的初始点将下降到左侧的吸引盆，而右侧的点将下降到右侧的吸引盆。\n\n初始化值为 $\\theta_{0}^{(-)} = -0.051$ 和 $\\theta_{0}^{(+)} = -0.049$。它们被策略性地放置在局部最大值 $\\theta_2^* \\approx -0.05$ 的两侧。\n- 对于 $\\theta_{0}^{(-)} = -0.051$：此点略微位于局部最大值的左侧。在局部最大值紧邻的左侧区间，梯度 $L'(\\theta)$ 必须为正。让我们验证一下：$L'(-0.051) = 4(-0.051)^3 - 4(-0.051) - 0.2 \\approx 4(-0.00013) + 0.204 - 0.2 = -0.00052 + 0.004 > 0$。\n梯度下降的更新是 $\\theta_{1}^{(-)} = \\theta_{0}^{(-)} - \\alpha L'(\\theta_{0}^{(-)})$。由于 $L'(\\theta_{0}^{(-)}) > 0$，更新将减去一个正量，使参数向左移动：$\\theta_{1}^{(-)} < \\theta_{0}^{(-)}$。该轨迹将收敛到左侧的局部最小值，即次优值。\n- 对于 $\\theta_{0}^{(+)} = -0.049$：此点略微位于局部最大值的右侧。在局部最大值紧邻的右侧区间，梯度 $L'(\\theta)$ 必须为负。让我们验证一下：$L'(-0.049) = 4(-0.049)^3 - 4(-0.049) - 0.2 \\approx 4(-0.00012) + 0.196 - 0.2 = -0.00048 - 0.004 < 0$。\n更新是 $\\theta_{1}^{(+)} = \\theta_{0}^{(+)} - \\alpha L'(\\theta_{0}^{(+)})$。由于 $L'(\\theta_{0}^{(+)}) < 0$，更新将加上一个正量，使参数向右移动：$\\theta_{1}^{(+)} > \\theta_{0}^{(+)}$。该轨迹将收敛到右侧的局部最小值，即全局最小值。\n\n最后，我们评估关于发散的说法。为使梯度下降收敛到一个最小值 $\\theta^*$，步长的一个充分条件是 $0 < \\alpha < 2/L''(\\theta^*)$。\n- 在左侧最小值（$\\theta_1^*$）处，$L''(\\theta_1^*) \\approx 7.4$。条件是 $\\alpha < 2/7.4 \\approx 0.27$。我们的 $\\alpha = 0.1$ 满足此条件。\n- 在右侧最小值（$\\theta_3^*$）处，$L''(\\theta_3^*) \\approx 8.6$。条件是 $\\alpha < 2/8.6 \\approx 0.23$。我们的 $\\alpha = 0.1$ 也满足此条件。\n步长 $\\alpha = 0.1$ 足够小以确保收敛，因此序列不会发散。\n\n### 逐项分析选项\n\nA. **$\\theta_{0}^{(-)}$ 和 $\\theta_{0}^{(+)}$ 都收敛到 $\\theta \\approx -1$ 附近的左侧局部最小值，该值是全局最小值。**\n初始点位于局部最大值的两侧，因此会落入不同的吸引盆。它们不会收敛到同一个最小值。此外，左侧最小值是次优的，而非全局的。\n结论：**错误**。\n\nB. **$\\theta_{0}^{(-)}$ 收敛到 $\\theta \\approx 1$ 附近的右侧局部最小值（全局），而 $\\theta_{0}^{(+)}$ 收敛到 $\\theta \\approx -1$ 附近的左侧局部最小值（次优）。**\n这与我们推导出的行为相反。$\\theta_{0}^{(-)}$ 被推向左侧，而 $\\theta_{0}^{(+)}$ 被推向右侧。\n结论：**错误**。\n\nC. **$\\theta_{0}^{(-)}$ 收敛到 $\\theta \\approx -1$ 附近的左侧局部最小值（次优），而 $\\theta_{0}^{(+)}$ 收敛到 $\\theta \\approx 1$ 附近的右侧局部最小值（全局）。**\n这与我们的分析完全吻合。$\\theta_{0}^{(-)} = -0.051$ 位于次优的左侧最小值的吸引盆中。$\\theta_{0}^{(+)} = -0.049$ 位于全局的右侧最小值的吸引盆中。\n结论：**正确**。\n\nD. **两个序列都发散到 $\\pm\\infty$ 因为 $\\alpha = 0.1$ 对于此损失函数来说太大而无法保持稳定。**\n我们对步长条件的分析表明，$\\alpha=0.1$ 处于收敛到任一最小值的稳定范围内。关于发散的说法是错误的。\n结论：**错误**。", "answer": "$$\\boxed{C}$$", "id": "2375265"}, {"introduction": "掌握了理论概念后，让我们将其应用于一个实际的金融问题。在这个练习中，你将亲手实现一个基础的梯度下降算法，以解决一个现实世界中的优化问题：为航空公司计算最优的燃油套期保值比率。通过这个实践，你将在一个具体且有价值的场景中，巩固对梯度下降核心更新规则的理解。[@problem_id:2375264]", "problem": "一家航空公司寻求通过交易石油期货合约来对冲其航空燃料成本风险。设时间索引为 $t \\in \\{1, \\dots, T\\}$。对于每个时期 $t$，令 $s_t$ 表示该航空公司单位体积航空燃料成本的观测变化（例如，每加仑的美金（USD）变化量），令 $f_t$ 表示同期每个合约单位的期货价格的相应变化。航空公司选择一个标量对冲比率 $h \\in \\mathbb{R}$，解释为每单位燃料风险敞口的期货合约单位数，以减少已对冲成本变化 $x_t(h) = s_t - h f_t$ 的波动。为抑制过大的头寸，航空公司引入一个系数为 $\\gamma \\ge 0$ 的微小二次头寸惩罚。目标是选择 $h$ 以最小化带惩罚的样本均方对冲变化\n$$\nJ(h) = \\frac{1}{T} \\sum_{t=1}^{T} \\left(s_t - h f_t\\right)^2 + \\gamma h^2.\n$$\n从给定的初始猜测 $h_0 \\in \\mathbb{R}$ 开始，使用固定步长 $\\alpha > 0$ 的基本梯度下降法来最小化 $J(h)$。在第 $k$ 次迭代时，更新 $h_{k+1} = h_k - \\alpha \\, \\nabla J(h_k)$，并在迭代值的绝对变化满足 $|h_{k+1} - h_k| < \\varepsilon$ 或达到最大迭代次数 $N$ 时终止。您的实现必须：\n- 基于 $J(h)$ 的定义，从第一性原理推导梯度 $\\nabla J(h)$。\n- 通过使用提供的容差 $\\varepsilon$ 和迭代上限 $N$ 来确保数值稳定性。\n- 将最终迭代值作为估计的最优对冲比率 $h^\\star$ 返回。\n\n实现一个程序，为以下每个测试用例计算 $h^\\star$。对于每个用例，$s$ 和 $f$ 以实数的有序列表形式给出，同时提供 $\\gamma$、$\\alpha$、$h_0$、$\\varepsilon$ 和 $N$。\n\n测试套件：\n- 用例 1：\n  - $s = [\\,\\$1.0,\\,-\\$0.5,\\,\\$0.75,\\,-\\$1.25,\\,\\$0.6,\\,-\\$0.8\\,]$\n  - $f = [\\,\\$0.9,\\,-\\$0.4,\\,\\$0.8,\\,-\\$1.1,\\,\\$0.5,\\,-\\$0.7\\,]$\n  - $\\gamma = 0.01$, $\\alpha = 0.05$, $h_0 = 0.0$, $\\varepsilon = 10^{-12}$, $N = 100000$\n- 用例 2：\n  - $s = [\\,\\$1.0,\\,-\\$1.0,\\,\\$1.0,\\,-\\$1.0,\\,\\$0.5,\\,-\\$0.5\\,]$\n  - $f = [\\,-\\$0.8,\\,\\$0.8,\\,-\\$0.7,\\,\\$0.7,\\,-\\$0.4,\\,\\$0.4\\,]$\n  - $\\gamma = 0.02$, $\\alpha = 0.05$, $h_0 = 0.0$, $\\varepsilon = 10^{-12}$, $N = 100000$\n- 用例 3（弱对冲工具；测试正则化）：\n  - $s = [\\,\\$1.0,\\,\\$0.5,\\,-\\$0.5,\\,-\\$1.0,\\,\\$0.3,\\,-\\$0.2\\,]$\n  - $f = [\\,\\$0.1,\\,-\\$0.1,\\,\\$0.1,\\,-\\$0.1,\\,\\$0.05,\\,-\\$0.05\\,]$\n  - $\\gamma = 0.5$, $\\alpha = 0.02$, $h_0 = 0.0$, $\\varepsilon = 10^{-12}$, $N = 100000$\n- 用例 4（更长样本，可忽略的惩罚）：\n  - $s = [\\,\\$0.2,\\,-\\$0.1,\\,\\$0.3,\\,-\\$0.25,\\,\\$0.15,\\,-\\$0.05,\\,\\$0.4,\\,-\\$0.35\\,]$\n  - $f = [\\,\\$0.18,\\,-\\$0.12,\\,\\$0.28,\\,-\\$0.22,\\,\\$0.14,\\,-\\$0.06,\\,\\$0.39,\\,-\\$0.33\\,]$\n  - $\\gamma = 0.0$, $\\alpha = 0.08$, $h_0 = 0.0$, $\\varepsilon = 10^{-12}$, $N = 100000$\n\n所有货币量均以美元（USD）为单位，但输出的对冲比率是无单位的。您的程序应产生单行输出，其中包含四个估计的对冲比率，格式为方括号内以逗号分隔的列表，每个比率精确到 $6$ 位小数（例如，$[1.234567,-0.123456,0.000001,2.000000]$）。不应打印任何其他文本。", "solution": "对所述问题进行验证。\n\n**步骤 1：提取给定信息**\n\n目标是找到能最小化以下带惩罚成本函数的对冲比率 $h \\in \\mathbb{R}$：\n$$\nJ(h) = \\frac{1}{T} \\sum_{t=1}^{T} \\left(s_t - h f_t\\right)^2 + \\gamma h^2\n$$\n其中：\n- $t \\in \\{1, \\dots, T\\}$ 是时间索引。\n- $s_t$ 是单位体积航空燃料成本的变化。\n- $f_t$ 是每个合约单位的期货价格的变化。\n- $\\gamma \\ge 0$ 是二次头寸惩罚系数。\n\n优化将使用基本梯度下降法执行，其更新规则为：\n$$\nh_{k+1} = h_k - \\alpha \\, \\nabla J(h_k)\n$$\n其中：\n- $k$ 是迭代索引。\n- $h_k$ 是第 $k$ 次迭代时的对冲比率估计值。\n- $\\alpha > 0$ 是一个固定的步长。\n- $\\nabla J(h_k)$ 是在 $h_k$ 处计算的 $J(h)$ 的梯度。\n\n算法从初始猜测 $h_0$ 开始，并在以下两种情况之一终止：\n1.  收敛：$|h_{k+1} - h_k| < \\varepsilon$，其中 $\\varepsilon$ 是指定的容差。\n2.  达到最大迭代次数：迭代次数达到最大值 $N$。\n\n提供的测试用例是：\n- **用例 1**：\n  - $s = [1.0, -0.5, 0.75, -1.25, 0.6, -0.8]$\n  - $f = [0.9, -0.4, 0.8, -1.1, 0.5, -0.7]$\n  - $\\gamma = 0.01$, $\\alpha = 0.05$, $h_0 = 0.0$, $\\varepsilon = 10^{-12}$, $N = 100000$\n- **用例 2**：\n  - $s = [1.0, -1.0, 1.0, -1.0, 0.5, -0.5]$\n  - $f = [-0.8, 0.8, -0.7, 0.7, -0.4, 0.4]$\n  - $\\gamma = 0.02$, $\\alpha = 0.05$, $h_0 = 0.0$, $\\varepsilon = 10^{-12}$, $N = 100000$\n- **用例 3**：\n  - $s = [1.0, 0.5, -0.5, -1.0, 0.3, -0.2]$\n  - $f = [0.1, -0.1, 0.1, -0.1, 0.05, -0.05]$\n  - $\\gamma = 0.5$, $\\alpha = 0.02$, $h_0 = 0.0$, $\\varepsilon = 10^{-12}$, $N = 100000$\n- **用例 4**：\n  - $s = [0.2, -0.1, 0.3, -0.25, 0.15, -0.05, 0.4, -0.35]$\n  - $f = [0.18, -0.12, 0.28, -0.22, 0.14, -0.06, 0.39, -0.33]$\n  - $\\gamma = 0.0$, $\\alpha = 0.08$, $h_0 = 0.0$, $\\varepsilon = 10^{-12}$, $N = 100000$\n\n**步骤 2：使用提取的给定信息进行验证**\n\n根据验证标准对问题进行评估。\n- **科学基础**：该问题是将统计学和机器学习中的一项基本技术——正则化线性回归（regularized linear regression），应用于金融工程中的一个标准问题（对冲）。目标函数是 `Ridge` 回归的一种形式，而梯度下降是其经典的优化方法。该公式在科学上和数学上都是合理的。\n- **良态性**：目标函数 $J(h)$ 是一个关于 $h$ 的二次函数。具体来说，$J(h) = A h^2 - B h + C$，其中二次项的系数是 $A = (\\frac{1}{T} \\sum_{t=1}^T f_t^2) + \\gamma$。由于 $\\gamma \\ge 0$ 且 $f_t^2 \\ge 0$，只要不是所有 $f_t$ 都为零或 $\\gamma > 0$，系数 $A$ 就严格为正。在所有测试用例中，向量 $f$ 都不是零向量，因此 $A > 0$。这意味着 $J(h)$ 是一个严格凸函数，并拥有唯一的全局最小值。因此，该问题是良态的。\n- **客观性**：问题以精确的数学术语陈述，没有歧义、主观性或意见。\n- **完整性**：每个测试用例都提供了所有必需的数据和参数 ($s, f, \\gamma, \\alpha, h_0, \\varepsilon, N$)。该问题是自洽的。\n\n**步骤 3：结论与行动**\n\n问题被判定为**有效**。将提供一个解决方案。\n\n**求解推导**\n\n梯度下降算法的核心是计算目标函数 $J(h)$ 的梯度 $\\nabla J(h)$。我们通过对 $J(h)$ 关于 $h$ 求导，从第一性原理推导这个梯度。\n\n目标函数是：\n$$\nJ(h) = \\frac{1}{T} \\sum_{t=1}^{T} (s_t - h f_t)^2 + \\gamma h^2\n$$\n\n梯度 $\\nabla J(h)$ 就是普通导数 $\\frac{dJ}{dh}$：\n$$\n\\nabla J(h) = \\frac{d}{dh} \\left( \\frac{1}{T} \\sum_{t=1}^{T} (s_t - h f_t)^2 + \\gamma h^2 \\right)\n$$\n\n根据微分的线性性质，我们可以逐项求导：\n$$\n\\nabla J(h) = \\frac{1}{T} \\sum_{t=1}^{T} \\frac{d}{dh} (s_t - h f_t)^2 + \\frac{d}{dh} (\\gamma h^2)\n$$\n\n对第一项应用链式法则，对第二项应用幂法则：\n$$\n\\frac{d}{dh} (s_t - h f_t)^2 = 2 (s_t - h f_t) \\cdot \\frac{d}{dh}(s_t - h f_t) = 2 (s_t - h f_t) (-f_t) = -2s_t f_t + 2h f_t^2\n$$\n$$\n\\frac{d}{dh} (\\gamma h^2) = 2 \\gamma h\n$$\n\n将这些代回梯度的表达式中：\n$$\n\\nabla J(h) = \\frac{1}{T} \\sum_{t=1}^{T} (-2s_t f_t + 2h f_t^2) + 2 \\gamma h\n$$\n\n我们可以将求和内的项分开：\n$$\n\\nabla J(h) = \\frac{1}{T} \\left( \\sum_{t=1}^{T} (-2s_t f_t) + \\sum_{t=1}^{T} (2h f_t^2) \\right) + 2 \\gamma h\n$$\n$$\n\\nabla J(h) = -\\frac{2}{T} \\sum_{t=1}^{T} s_t f_t + \\frac{2h}{T} \\sum_{t=1}^{T} f_t^2 + 2 \\gamma h\n$$\n\n提出公因子 $2h$：\n$$\n\\nabla J(h) = 2h \\left( \\frac{1}{T} \\sum_{t=1}^{T} f_t^2 + \\gamma \\right) - \\frac{2}{T} \\sum_{t=1}^{T} s_t f_t\n$$\n这就是梯度的解析表达式。该表达式将用于迭代更新规则中。\n\n算法按以下步骤进行：\n1.  用 $h_0$ 初始化 $h_k$。\n2.  对于从 $0$ 到 $N-1$ 的 $k$：\n    a. 使用推导出的公式计算梯度 $\\nabla J(h_k)$。为高效计算，我们首先预计算样本矩 $\\frac{1}{T} \\sum s_t f_t$ 和 $\\frac{1}{T} \\sum f_t^2$。\n    b. 更新估计值：$h_{k+1} = h_k - \\alpha \\nabla J(h_k)$。\n    c. 检查收敛性：如果 $|h_{k+1} - h_k| < \\varepsilon$，则算法已收敛。最终估计值为 $h^\\star = h_{k+1}$。终止。\n3.  如果循环完成而未满足收敛准则，算法因达到最大迭代次数而终止。最终估计值为 $h^\\star = h_N$。\n\n将对每个测试用例实施此过程，以找到最优对冲比率 $h^\\star$。所提供的步长 $\\alpha$ 足够小，可以确保向凸目标函数的唯一最小值稳定收敛。\n通过设 $\\nabla J(h) = 0$ 得到的最小值的解析解是 $h^\\star = \\frac{\\sum s_t f_t}{\\sum f_t^2 + T\\gamma}$，这可用于验证迭代算法的收敛性。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves for the optimal hedging ratio using gradient descent for multiple test cases.\n    \"\"\"\n\n    test_cases = [\n        {\n            \"s\": [1.0, -0.5, 0.75, -1.25, 0.6, -0.8],\n            \"f\": [0.9, -0.4, 0.8, -1.1, 0.5, -0.7],\n            \"gamma\": 0.01,\n            \"alpha\": 0.05,\n            \"h0\": 0.0,\n            \"epsilon\": 1e-12,\n            \"N\": 100000,\n        },\n        {\n            \"s\": [1.0, -1.0, 1.0, -1.0, 0.5, -0.5],\n            \"f\": [-0.8, 0.8, -0.7, 0.7, -0.4, 0.4],\n            \"gamma\": 0.02,\n            \"alpha\": 0.05,\n            \"h0\": 0.0,\n            \"epsilon\": 1e-12,\n            \"N\": 100000,\n        },\n        {\n            \"s\": [1.0, 0.5, -0.5, -1.0, 0.3, -0.2],\n            \"f\": [0.1, -0.1, 0.1, -0.1, 0.05, -0.05],\n            \"gamma\": 0.5,\n            \"alpha\": 0.02,\n            \"h0\": 0.0,\n            \"epsilon\": 1e-12,\n            \"N\": 100000,\n        },\n        {\n            \"s\": [0.2, -0.1, 0.3, -0.25, 0.15, -0.05, 0.4, -0.35],\n            \"f\": [0.18, -0.12, 0.28, -0.22, 0.14, -0.06, 0.39, -0.33],\n            \"gamma\": 0.0,\n            \"alpha\": 0.08,\n            \"h0\": 0.0,\n            \"epsilon\": 1e-12,\n            \"N\": 100000,\n        },\n    ]\n\n    def compute_h_star(s_data, f_data, gamma, alpha, h0, epsilon, N):\n        \"\"\"\n        Computes the optimal hedging ratio h* using gradient descent.\n        \"\"\"\n        s = np.array(s_data)\n        f = np.array(f_data)\n        T = len(s)\n\n        # Pre-compute sample moments for efficiency\n        # E[f^2] = (1/T) * sum(f_t^2)\n        mean_f_sq = np.sum(f**2) / T\n        # E[sf] = (1/T) * sum(s_t * f_t)\n        mean_s_f = np.sum(s * f) / T\n        \n        h_k = h0\n\n        for _ in range(N):\n            # Gradient: grad_J(h) = 2 * (h * (E[f^2] + gamma) - E[sf])\n            grad_J = 2 * (h_k * (mean_f_sq + gamma) - mean_s_f)\n            \n            h_k_plus_1 = h_k - alpha * grad_J\n            \n            if np.abs(h_k_plus_1 - h_k) < epsilon:\n                return h_k_plus_1\n            \n            h_k = h_k_plus_1\n            \n        return h_k\n\n    results = []\n    for case in test_cases:\n        h_star = compute_h_star(\n            case[\"s\"],\n            case[\"f\"],\n            case[\"gamma\"],\n            case[\"alpha\"],\n            case[\"h0\"],\n            case[\"epsilon\"],\n            case[\"N\"],\n        )\n        results.append(h_star)\n\n    # Format the final output as a comma-separated list of strings,\n    # with each number rounded to 6 decimal places, enclosed in brackets.\n    print(f\"[{','.join([f'{r:.6f}' for r in results])}]\")\n\nsolve()\n\n```", "id": "2375264"}, {"introduction": "在前一个练习的基础上，我们将挑战一个更复杂的多维优化问题。你将实现梯度上升法（梯度下降的“上坡”版本），并结合更稳健的步长搜索方法（回溯线搜索），来为新零售店找到最佳位置。这项练习将让你具备处理更复杂的目标函数和实现更高级优化程序的能力，将你的技能提升到一个新的水平。[@problem_id:2375271]", "problem": "给定一个由空间需求中心构成的二维光滑无约束目标函数。设有 $m \\in \\mathbb{N}$ 个中心，每个中心具有位置 $\\boldsymbol{\\mu}_i \\in \\mathbb{R}^2$、正权重 $w_i \\in \\mathbb{R}_{>0}$ 和各向同性扩展 $s_i \\in \\mathbb{R}_{>0}$。定义客户流量强度函数 $T: \\mathbb{R}^2 \\to \\mathbb{R}$ 如下：\n$$\nT(\\boldsymbol{x}) \\equiv \\sum_{i=1}^{m} w_i \\exp\\left( -\\frac{1}{2 s_i^2} \\left\\| \\boldsymbol{x} - \\boldsymbol{\\mu}_i \\right\\|_2^2 \\right),\n$$\n其中 $\\left\\| \\cdot \\right\\|_2$ 表示欧几里得范数。您的任务是，使用第一性原理推导梯度 $\\nabla T(\\boldsymbol{x})$，然后实现带有回溯线搜索的梯度上升法，为下述每个测试实例估计 $T(\\boldsymbol{x})$ 的一个最大化点 $\\boldsymbol{x}^\\star \\in \\mathbb{R}^2$。\n\n从以下基本原理出发：\n- 梯度定义为偏导数向量。\n- 可微函数复合的链式法则。\n- 梯度方向上的方向导数给出最大瞬时增长率这一事实。\n- 用于回溯线搜索的 Armijo 条件：对于步长 $\\alpha > 0$ 和上升方向 $\\boldsymbol{d}$，如果满足以下条件，则接受 $\\alpha$：\n$$\nT(\\boldsymbol{x} + \\alpha \\boldsymbol{d}) \\ge T(\\boldsymbol{x}) + c \\alpha \\, \\nabla T(\\boldsymbol{x})^\\top \\boldsymbol{d}\n$$\n其中 $c \\in (0,1)$ 是一个选定的常数。\n\n算法要求：\n- 使用最速上升法，方向为 $\\boldsymbol{d}_k = \\nabla T(\\boldsymbol{x}_k)$，迭代公式为 $\\boldsymbol{x}_{k+1} = \\boldsymbol{x}_k + \\alpha_k \\boldsymbol{d}_k$。\n- 使用回溯线搜索，从初始步长 $\\alpha_0$ 开始，按因子 $\\rho \\in (0,1)$ 缩减，直到满足 Armijo 条件，或直到步长低于最小阈值时停止。\n- 当梯度范数低于容差 $\\varepsilon$ 或达到最大迭代次数时停止。\n- 除了 $\\boldsymbol{x} \\in \\mathbb{R}^2$ 之外，对 $\\boldsymbol{x}$ 没有其他约束，也无需报告物理单位。\n\n测试套件：\n- 情况 1（通用情况，峰值分离良好）：\n  - $m = 3$,\n  - $\\boldsymbol{\\mu}_1 = (1, 2)$, $w_1 = 100$, $s_1 = 1.0$;\n  - $\\boldsymbol{\\mu}_2 = (-2, -1)$, $w_2 = 80$, $s_2 = 1.5$;\n  - $\\boldsymbol{\\mu}_3 = (3, -3)$, $w_3 = 120$, $s_3 = 0.7$;\n  - 初始化 $\\boldsymbol{x}_0 = (0, 0)$。\n- 情况 2（对称情况，初始化时梯度平坦）：\n  - $m = 2$,\n  - $\\boldsymbol{\\mu}_1 = (-2, 0)$, $w_1 = 50$, $s_1 = 0.5$;\n  - $\\boldsymbol{\\mu}_2 = (2, 0)$, $w_2 = 50$, $s_2 = 0.5$;\n  - 初始化 $\\boldsymbol{x}_0 = (0, 0)$。\n- 情况 3（局部最大值与全局最大值，对初始化的敏感性）：\n  - $m = 2$,\n  - $\\boldsymbol{\\mu}_1 = (5, 5)$, $w_1 = 200$, $s_1 = 2.0$;\n  - $\\boldsymbol{\\mu}_2 = (-5, -5)$, $w_2 = 180$, $s_2 = 1.0$;\n  - 初始化 $\\boldsymbol{x}_0 = (-10, -10)$。\n\n对所有情况使用相同的线搜索和停止参数：\n- 初始步长 $\\alpha_0 = 1.0$,\n- 缩减因子 $\\rho = 0.5$,\n- Armijo 系数 $c = 10^{-4}$,\n- 梯度范数容差 $\\varepsilon = 10^{-8}$,\n- 最大迭代次数 $K_{\\max} = 10000$,\n- 回溯的最小步长阈值 $\\alpha_{\\min} = 10^{-12}$。\n\n输出规格：\n- 对于每种情况，以列表 $[\\hat{x}, \\hat{y}, T(\\hat{\\boldsymbol{x}})]$ 的形式返回估计的最大化点和目标值，其中 $\\hat{\\boldsymbol{x}} = (\\hat{x}, \\hat{y})$。\n- 每个标量必须是四舍五入到 6 位小数的浮点数。\n- 您的程序应生成单行输出，其中包含三个案例结果的逗号分隔列表，列表用方括号括起来，并保留内部列表结构。具体来说，打印\n```\n[[\\hat{x}_1,\\hat{y}_1,T_1],[\\hat{x}_2,\\hat{y}_2,T_2],[\\hat{x}_3,\\hat{y}_3,T_3]],\n```\n按规定四舍五入，不含多余的空格或文本。例如，一个语法上有效的输出应如下所示：\n```\n[[0.123456,-1.234567,9.876543],[...],[...]].\n```\n预期的答案类型是浮点数列表，最终输出将这三个列表聚合成一个单行的顶级列表。", "solution": "所提出的问题是一个标准的无约束非线性优化问题。任务是使用最速上升法（梯度上升法的一种特定形式）来寻找给定目标函数 $T(\\boldsymbol{x})$ 的一个局部最大值。该问题定义明确，科学上合理，并为数值求解提供了所有必要的参数和条件。因此，该问题是有效的。\n\n解决方案分两个阶段进行。首先，我们推导目标函数梯度的解析形式。其次，我们实现带有回溯线搜索的梯度上升算法，为每个指定的测试案例数值化地找到一个最大化点。\n\n**1. 梯度 $\\nabla T(\\boldsymbol{x})$ 的推导**\n\n目标函数由下式给出：\n$$\nT(\\boldsymbol{x}) = \\sum_{i=1}^{m} w_i \\exp\\left( -\\frac{1}{2 s_i^2} \\left\\| \\boldsymbol{x} - \\boldsymbol{\\mu}_i \\right\\|_2^2 \\right)\n$$\n其中 $\\boldsymbol{x} = (x_1, x_2)^\\top \\in \\mathbb{R}^2$ 是变量位置。\n\n$T(\\boldsymbol{x})$ 的梯度，记为 $\\nabla T(\\boldsymbol{x})$，是其关于 $\\boldsymbol{x}$ 各分量的偏导数所组成的向量：\n$$\n\\nabla T(\\boldsymbol{x}) = \\begin{pmatrix} \\frac{\\partial T}{\\partial x_1} \\\\ \\frac{\\partial T}{\\partial x_2} \\end{pmatrix}\n$$\n\n根据微分算子的线性性质，和的梯度等于梯度的和。因此，我们可以独立计算求和式中每一项的梯度：\n$$\n\\nabla T(\\boldsymbol{x}) = \\nabla \\left( \\sum_{i=1}^{m} w_i \\exp\\left( -\\frac{1}{2 s_i^2} \\left\\| \\boldsymbol{x} - \\boldsymbol{\\mu}_i \\right\\|_2^2 \\right) \\right) = \\sum_{i=1}^{m} \\nabla \\left( w_i \\exp\\left( -\\frac{1}{2 s_i^2} \\left\\| \\boldsymbol{x} - \\boldsymbol{\\mu}_i \\right\\|_2^2 \\right) \\right)\n$$\n\n让我们分析单项 $T_i(\\boldsymbol{x}) = w_i \\exp(f_i(\\boldsymbol{x}))$，其中指数为 $f_i(\\boldsymbol{x}) = -\\frac{1}{2s_i^2} \\| \\boldsymbol{x} - \\boldsymbol{\\mu}_i \\|_2^2$。\n使用向量函数的链式法则 $\\nabla (\\exp(f(\\boldsymbol{x}))) = \\exp(f(\\boldsymbol{x})) \\nabla f(\\boldsymbol{x})$。应用此法则，我们得到：\n$$\n\\nabla T_i(\\boldsymbol{x}) = w_i \\exp(f_i(\\boldsymbol{x})) \\nabla f_i(\\boldsymbol{x})\n$$\n\n现在，我们必须计算 $f_i(\\boldsymbol{x})$ 的梯度。欧几里得范数的平方为 $\\| \\boldsymbol{z} \\|_2^2 = \\boldsymbol{z}^\\top \\boldsymbol{z}$。令 $\\boldsymbol{z} = \\boldsymbol{x} - \\boldsymbol{\\mu}_i$，则有 $f_i(\\boldsymbol{x}) = -\\frac{1}{2s_i^2} (\\boldsymbol{x} - \\boldsymbol{\\mu}_i)^\\top (\\boldsymbol{x} - \\boldsymbol{\\mu}_i)$。\n二次型 $\\boldsymbol{x}^\\top A \\boldsymbol{x}$ 的梯度是 $(A+A^\\top)\\boldsymbol{x}$。这里，我们遇到的情况更简单。令 $\\boldsymbol{\\mu}_i = (\\mu_{i1}, \\mu_{i2})^\\top$。\n$$\nf_i(\\boldsymbol{x}) = -\\frac{1}{2s_i^2} \\left( (x_1 - \\mu_{i1})^2 + (x_2 - \\mu_{i2})^2 \\right)\n$$\n偏导数为：\n$$\n\\frac{\\partial f_i}{\\partial x_1} = -\\frac{1}{2s_i^2} \\cdot 2(x_1 - \\mu_{i1}) \\cdot 1 = -\\frac{1}{s_i^2}(x_1 - \\mu_{i1})\n$$\n$$\n\\frac{\\partial f_i}{\\partial x_2} = -\\frac{1}{2s_i^2} \\cdot 2(x_2 - \\mu_{i2}) \\cdot 1 = -\\frac{1}{s_i^2}(x_2 - \\mu_{i2})\n$$\n将它们组合成梯度向量 $\\nabla f_i(\\boldsymbol{x})$：\n$$\n\\nabla f_i(\\boldsymbol{x}) = \\begin{pmatrix} -\\frac{1}{s_i^2}(x_1 - \\mu_{i1}) \\\\ -\\frac{1}{s_i^2}(x_2 - \\mu_{i2}) \\end{pmatrix} = -\\frac{1}{s_i^2} (\\boldsymbol{x} - \\boldsymbol{\\mu}_i)\n$$\n\n将此结果代回 $\\nabla T_i(\\boldsymbol{x})$ 的表达式中：\n$$\n\\nabla T_i(\\boldsymbol{x}) = w_i \\exp\\left( -\\frac{1}{2s_i^2} \\| \\boldsymbol{x} - \\boldsymbol{\\mu}_i \\|_2^2 \\right) \\left( -\\frac{1}{s_i^2} (\\boldsymbol{x} - \\boldsymbol{\\mu}_i) \\right)\n$$\n\n最后，对所有 $i = 1, \\dots, m$ 求和，得到目标函数 $T(\\boldsymbol{x})$ 的完整梯度：\n$$\n\\nabla T(\\boldsymbol{x}) = \\sum_{i=1}^{m} \\left[ - \\frac{w_i}{s_i^2} \\exp\\left( -\\frac{1}{2s_i^2} \\| \\boldsymbol{x} - \\boldsymbol{\\mu}_i \\|_2^2 \\right) (\\boldsymbol{x} - \\boldsymbol{\\mu}_i) \\right]\n$$\n该表达式是实现梯度上升算法的基础。\n\n**2. 算法实现**\n\n最速上升法通过在梯度 $\\nabla T(\\boldsymbol{x}_k)$ 方向上取一个步长来迭代更新当前估计值 $\\boldsymbol{x}_k$。更新规则为：\n$$\n\\boldsymbol{x}_{k+1} = \\boldsymbol{x}_k + \\alpha_k \\boldsymbol{d}_k, \\quad \\text{其中} \\quad \\boldsymbol{d}_k = \\nabla T(\\boldsymbol{x}_k)\n$$\n步长 $\\alpha_k$ 在每次迭代时通过回溯线搜索来确定，以确保目标函数有足够的增量，这由 Armijo 条件规定。\n\n**算法：带回溯线搜索的梯度上升**\n\n1.  **初始化**：\n    -   设置迭代计数器 $k = 0$。\n    -   设置初始点 $\\boldsymbol{x}_0$。\n    -   设置参数：初始步长 $\\alpha_0$、缩减因子 $\\rho$、Armijo 常数 $c$、容差 $\\varepsilon$、最大迭代次数 $K_{\\max}$、最小步长 $\\alpha_{\\min}$。\n\n2.  **迭代**：对于 $k = 0, 1, 2, \\ldots, K_{\\max}-1$：\n    a.  **计算梯度**：使用推导出的公式计算梯度向量 $\\boldsymbol{g}_k = \\nabla T(\\boldsymbol{x}_k)$。\n    b.  **检查收敛性**：如果 $\\| \\boldsymbol{g}_k \\|_2 < \\varepsilon$，则梯度足够小。终止并返回 $\\boldsymbol{x}_k$ 作为估计的最大化点。\n    c.  **回溯线搜索**：寻找步长 $\\alpha_k$。\n        i.   初始化步长 $\\alpha = \\alpha_0$。\n        ii.  上升方向为 $\\boldsymbol{d}_k = \\boldsymbol{g}_k$。\n        iii. 计算 Armijo 条件的右侧：$RHS = T(\\boldsymbol{x}_k) + c \\alpha \\boldsymbol{g}_k^\\top \\boldsymbol{d}_k = T(\\boldsymbol{x}_k) + c \\alpha \\| \\boldsymbol{g}_k \\|_2^2$。\n        iv.  当 $\\alpha \\ge \\alpha_{\\min}$ 时：\n             -   计算候选点：$\\boldsymbol{x}_{\\text{new}} = \\boldsymbol{x}_k + \\alpha \\boldsymbol{d}_k$。\n             -   在新点处评估目标函数值：$LHS = T(\\boldsymbol{x}_{\\text{new}})$。\n             -   如果 $LHS \\ge RHS$，Armijo 条件满足。设置 $\\alpha_k = \\alpha$ 并跳出内部 while 循环。\n             -   否则，缩减步长：$\\alpha = \\rho \\alpha$。\n        v.   如果循环因 $\\alpha < \\alpha_{\\min}$ 而终止，说明步长太小，无法取得进展。终止主算法并返回当前点 $\\boldsymbol{x}_k$。\n    d.  **更新位置**：更新估计值：$\\boldsymbol{x}_{k+1} = \\boldsymbol{x}_k + \\alpha_k \\boldsymbol{d}_k$。\n\n3.  **终止**：如果循环因达到 $K_{\\max}$ 而完成，则终止并返回最终点 $\\boldsymbol{x}_{K_{\\max}}$。\n\n该算法使用指定的参数应用于三个测试案例中的每一个。\n\n-   **情况 1** 从 $\\boldsymbol{x}_0 = (0,0)$ 开始，预期将收敛到三个局部最大值之一。权重、扩展和离原点的距离的组合决定了算法将遵循哪个吸引盆。\n-   **情况 2** 从 $\\boldsymbol{x}_0 = (0,0)$ 开始，由于设置的对称性，该点是一个鞍点。在该点上，$\\nabla T(0,0) = \\boldsymbol{0}$。由于梯度范数低于容差 $\\varepsilon$，算法预期将在第 0 次迭代时立即终止。\n-   **情况 3** 展示了对初始化的敏感性。起始点 $\\boldsymbol{x}_0 = (-10,-10)$ 位于靠近 $\\boldsymbol{\\mu}_2 = (-5,-5)$ 的局部最大值的吸引盆内，尽管由于其更高的权重 $w_1=200 > w_2=180$，全局最大值位于 $\\boldsymbol{\\mu}_1 = (5,5)$ 附近。算法预期将收敛到这个局部最大值。\n\n实现将包括计算 $T(\\boldsymbol{x})$ 和 $\\nabla T(\\boldsymbol{x})$ 的函数，以及一个主求解器函数，该函数为每个测试案例组织上述迭代过程。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the optimization problem for all test cases and prints the result.\n    \"\"\"\n\n    # Define test cases as a list of dictionaries.\n    # Each dictionary contains: centers, initial_x, m\n    # 'centers' is a list of tuples (mu, w, s)\n    test_cases = [\n        {\n            \"m\": 3,\n            \"centers\": [\n                (np.array([1.0, 2.0]), 100.0, 1.0),\n                (np.array([-2.0, -1.0]), 80.0, 1.5),\n                (np.array([3.0, -3.0]), 120.0, 0.7),\n            ],\n            \"initial_x\": np.array([0.0, 0.0]),\n        },\n        {\n            \"m\": 2,\n            \"centers\": [\n                (np.array([-2.0, 0.0]), 50.0, 0.5),\n                (np.array([2.0, 0.0]), 50.0, 0.5),\n            ],\n            \"initial_x\": np.array([0.0, 0.0]),\n        },\n        {\n            \"m\": 2,\n            \"centers\": [\n                (np.array([5.0, 5.0]), 200.0, 2.0),\n                (np.array([-5.0, -5.0]), 180.0, 1.0),\n            ],\n            \"initial_x\": np.array([-10.0, -10.0]),\n        },\n    ]\n\n    # Algorithmic parameters\n    params = {\n        \"alpha_0\": 1.0,\n        \"rho\": 0.5,\n        \"c\": 1e-4,\n        \"epsilon\": 1e-8,\n        \"k_max\": 10000,\n        \"alpha_min\": 1e-12,\n    }\n\n    results = []\n    for case in test_cases:\n        result_x = run_gradient_ascent(case, params)\n        final_T = T_func(result_x, case[\"centers\"])\n        # Format output as [x, y, T(x)] rounded to 6 decimal places\n        formatted_result = [\n            round(result_x[0], 6),\n            round(result_x[1], 6),\n            round(final_T, 6),\n        ]\n        results.append(formatted_result)\n    \n    # Final print statement in the exact required format\n    # Example: [[0.123456,-1.234567,9.876543],[...],[...]]\n    print(str(results).replace(\" \", \"\"))\n\n\ndef T_func(x, centers):\n    \"\"\"\n    Computes the objective function T(x).\n    x: np.array of shape (2,)\n    centers: list of tuples (mu, w, s)\n    \"\"\"\n    total_intensity = 0.0\n    for mu_i, w_i, s_i in centers:\n        norm_sq = np.sum((x - mu_i)**2)\n        exponent = -1.0 / (2.0 * s_i**2) * norm_sq\n        total_intensity += w_i * np.exp(exponent)\n    return total_intensity\n\ndef grad_T_func(x, centers):\n    \"\"\"\n    Computes the gradient of the objective function nabla T(x).\n    x: np.array of shape (2,)\n    centers: list of tuples (mu, w, s)\n    \"\"\"\n    grad = np.zeros(2)\n    for mu_i, w_i, s_i in centers:\n        diff = x - mu_i\n        norm_sq = np.sum(diff**2)\n        s_i_sq = s_i**2\n        exp_term = np.exp(-1.0 / (2.0 * s_i_sq) * norm_sq)\n        grad += (-w_i / s_i_sq) * exp_term * diff\n    return grad\n\ndef run_gradient_ascent(case, params):\n    \"\"\"\n    Performs gradient ascent with backtracking line search for a single case.\n    \"\"\"\n    x_k = case[\"initial_x\"].copy()\n    centers = case[\"centers\"]\n    \n    alpha_0 = params[\"alpha_0\"]\n    rho = params[\"rho\"]\n    c = params[\"c\"]\n    epsilon = params[\"epsilon\"]\n    k_max = params[\"k_max\"]\n    alpha_min = params[\"alpha_min\"]\n\n    for _ in range(k_max):\n        grad_k = grad_T_func(x_k, centers)\n        grad_norm = np.linalg.norm(grad_k)\n\n        if grad_norm < epsilon:\n            break\n\n        # Backtracking line search\n        alpha = alpha_0\n        d_k = grad_k\n        T_k = T_func(x_k, centers)\n        armijo_rhs_const = c * np.dot(grad_k, d_k) # same as c * grad_norm**2\n\n        while alpha >= alpha_min:\n            x_new = x_k + alpha * d_k\n            T_new = T_func(x_new, centers)\n            \n            if T_new >= T_k + alpha * armijo_rhs_const:\n                x_k = x_new\n                break\n            \n            alpha *= rho\n        \n        if alpha < alpha_min:\n            # Cannot find a suitable step, terminate.\n            break\n            \n    return x_k\n\nsolve()\n```", "id": "2375271"}]}