{"hands_on_practices": [{"introduction": "要有效地应用Q学习，我们必须首先理解问题建模的哪些方面会影响最终的最优策略。这个练习 [@problem_id:3163629] 提供了一个动手计算的机会，让你探究Q值和最优动作对奖励函数变换的敏感性。通过一个具体的例子，你将发现简单的仿射变换（affine transformations）与更复杂的、依赖于状态的奖励塑造（reward shaping）之间的关键区别，这是设计高效强化学习智能体的一个核心概念。", "problem": "考虑一个无限时域折扣马尔可夫决策过程 (MDP)，其状态集为 $\\{s_0, s_1, s_2\\}$，折扣因子为 $\\gamma = 0.9$。状态 $s_0$ 有两个可用动作，$a_L$ 和 $a_R$，其确定性转移和立即奖励如下：\n- 在 $s_0$ 执行动作 $a_L$ 得到立即奖励 $r(s_0,a_L) = 0$ 并转移到 $s_1$。\n- 在 $s_0$ 执行动作 $a_R$ 得到立即奖励 $r(s_0,a_R) = 1$ 并转移到 $s_2$。\n\n状态 $s_1$ 和 $s_2$ 是终止状态，因为它们各自只有一个虚拟动作，该动作会导致一个吸收终止条件：\n- 在 $s_1$ 中，唯一的动作产生立即奖励 $r(s_1,\\text{dummy}) = 5$，然后转移到一个没有后续奖励的吸收终止条件。\n- 在 $s_2$ 中，唯一的动作产生立即奖励 $r(s_2,\\text{dummy}) = 0$，然后转移到一个没有后续奖励的吸收终止条件。\n\n按通常意义定义最优动作价值函数 $Q^\\star(s,a)$，即从状态-动作对 $(s,a)$ 开始，之后遵循最优策略所能获得的期望折扣回报的上确界。现在定义一种逐状态奖励归一化，它通过以下方式将原始奖励函数 $r$ 映射到一个新函数 $r_{\\text{norm}}$\n$$\nr_{\\text{norm}}(s,a) \\;=\\; r(s,a) \\;-\\; \\mu(s), \\quad \\text{其中 } \\mu(s) \\equiv \\frac{1}{|\\mathcal{A}(s)|} \\sum_{a' \\in \\mathcal{A}(s)} r(s,a')\n$$\n其中 $\\mathcal{A}(s)$ 是状态 $s$ 中的可用动作集。令 $Q^\\star_{\\text{norm}}$ 为在使用相同折扣因子 $\\gamma$ 的情况下，基于 $r_{\\text{norm}}$ 的最优动作价值函数。\n\n仅使用回报、最优性和贝尔曼最优性原理的定义，推断这种逐状态归一化对 $s_0$ 处动作排序的影响。然后，考虑表格式Q学习更新\n$$\nQ_{t+1}(s_t,a_t) \\;=\\; Q_t(s_t,a_t) \\;+\\; \\alpha_t \\Big( r_t \\;+\\; \\gamma \\max_{a'} Q_t(s_{t+1},a') \\;-\\; Q_t(s_t,a_t) \\Big),\n$$\n其中 $\\alpha_t \\in (0,1]$ 是学习率，并讨论此更新对奖励 $r$ 的仿射变换的不变性。\n\n选择所有正确的陈述：\n\nA. 在上述MDP中，将 $r$ 替换为逐状态中心化奖励 $r_{\\text{norm}}(s,a) = r(s,a) - \\mu(s)$，会反转 $Q^\\star(s_0,a_L)$ 和 $Q^\\star(s_0,a_R)$ 相对于原始奖励的排序。\n\nB. 对于任何MDP和任何 $\\gamma \\in (0,1)$，将 $r$ 替换为 $r'(s,a,s') = r(s,a,s') + c$（其中 $c \\in \\mathbb{R}$ 是一个常数），在每个状态下的最优贪心动作集合保持不变。\n\nC. 对于任何MDP和任何 $\\gamma \\in (0,1)$，将 $r$ 替换为 $r'(s,a,s') = a\\, r(s,a,s')$（其中标量 $a > 0$），在每个状态下的最优贪心动作集合保持不变。\n\nD. 对于任何MDP和任何 $\\gamma \\in (0,1)$，将 $r$ 替换为 $r'(s,a,s') = r(s,a,s') + b(s)$（其中 $b(s)$ 是一个仅依赖于当前状态 $s$ 的任意函数），在每个状态下的最优贪心动作集合保持不变。\n\nE. 在具有任意初始化 $Q_0$ 的表格式Q学习中，将奖励替换为 $r'(s,a,s') = r(s,a,s') + c$（其中 $c$ 是常数），会使得随时间 $t$ 变化的整个贪心策略轨迹与在 $r$ 下的轨迹严格相同（在每个时间点对每个访问过的状态都做出相同的动作选择），而与 $Q_0$ 无关。\n\nF. 令 $r'(s,a,s') = a\\, r(s,a,s') + b$，其中 $a > 0$ 且 $b \\in \\mathbb{R}$。如果初始化 $Q'_0(s,a) = a\\, Q_0(s,a) + \\tfrac{b}{1-\\gamma}$ 并对 $r'$ 运行表格式Q学习，同时从 $Q_0$ 开始对 $r$ 运行表格式Q学习，那么对于每个时间 $t$ 和每个状态-动作对 $(s,a)$，都有 $Q'_t(s,a) = a\\, Q_t(s,a) + \\tfrac{b}{1-\\gamma}$，因此两条学习轨迹上的贪心策略在每个时间点都重合。", "solution": "首先验证问题陈述，以确保其科学合理、定义明确且客观。\n\n### 问题验证\n\n**步骤1：提取的已知条件**\n- 考虑一个无限时域折扣马尔可夫决策过程 (MDP)。\n- 状态集：$\\{s_0, s_1, s_2\\}$。\n- 折扣因子：$\\gamma = 0.9$。\n- 从状态 $s_0$ 出发的动作和确定性转移/奖励：\n  - 动作 $a_L$：转移到 $s_1$，立即奖励 $r(s_0, a_L) = 0$。\n  - 动作 $a_R$：转移到 $s_2$，立即奖励 $r(s_0, a_R) = 1$。\n- 状态 $s_1$：单个虚拟动作产生立即奖励 $r(s_1, \\text{dummy}) = 5$ 并转移到一个终止吸收状态。\n- 状态 $s_2$：单个虚拟动作产生立即奖励 $r(s_2, \\text{dummy}) = 0$ 并转移到一个终止吸收状态。\n- 最优动作价值函数：$Q^\\star(s,a)$ 定义为期望折扣回报的上确界。\n- 逐状态奖励归一化：$r_{\\text{norm}}(s,a) = r(s,a) - \\mu(s)$，其中 $\\mu(s) = \\frac{1}{|\\mathcal{A}(s)|} \\sum_{a' \\in \\mathcal{A}(s)} r(s,a')$。\n- 表格式Q学习更新规则：$Q_{t+1}(s_t,a_t) = Q_t(s_t,a_t) + \\alpha_t ( r_t + \\gamma \\max_{a'} Q_t(s_{t+1},a') - Q_t(s_t,a_t) )$，学习率为 $\\alpha_t \\in (0,1]$。\n\n**步骤2：使用提取的已知条件进行验证**\n- **科学基础：** 该问题在MDP和强化学习的标准数学框架内构建。所有术语和概念都是标准的且定义明确。\n- **适定性：** 该MDP具有确定性的转移和奖励，被完全指定。所提出的问题是关于价值函数和学习算法性质的明确定义的数学探究。待计算的量存在唯一解。\n- **客观性：** 问题以精确、正式的语言陈述，没有歧义或主观内容。\n- **一致性和完整性：** 问题提供了所有必要信息，并且不包含内部矛盾。\n\n**步骤3：结论与行动**\n问题有效。解决方案将继续分析每个选项。\n\n### 解答与选项分析\n\n最优动作价值函数 $Q^\\star(s, a)$ 满足贝尔曼最优方程。对于确定性转移 $s \\xrightarrow{a} s'$，该方程为：\n$$Q^\\star(s,a) = r(s,a) + \\gamma \\max_{a'} Q^\\star(s', a') = r(s,a) + \\gamma V^\\star(s')$$\n其中 $V^\\star(s) = \\max_a Q^\\star(s,a)$ 是最优状态价值函数。\n\n**A. 对具有奖励归一化的特定MDP的分析**\n\n首先，我们计算原始奖励函数 $r$ 的最优Q值。\n状态 $s_1$ 和 $s_2$ 实际上是终止状态。在这些状态下的一个动作会给出一个最终奖励，然后过程结束（转移到一个 $V=0$ 的吸收状态）。\n对于状态 $s_1$：\n$$Q^\\star(s_1, \\text{dummy}) = r(s_1, \\text{dummy}) + \\gamma \\cdot 0 = 5$$\n$$V^\\star(s_1) = 5$$\n对于状态 $s_2$：\n$$Q^\\star(s_2, \\text{dummy}) = r(s_2, \\text{dummy}) + \\gamma \\cdot 0 = 0$$\n$$V^\\star(s_2) = 0$$\n现在，对于状态 $s_0$：\n$$Q^\\star(s_0, a_L) = r(s_0, a_L) + \\gamma V^\\star(s_1) = 0 + 0.9 \\cdot 5 = 4.5$$\n$$Q^\\star(s_0, a_R) = r(s_0, a_R) + \\gamma V^\\star(s_2) = 1 + 0.9 \\cdot 0 = 1$$\n排序为 $Q^\\star(s_0, a_L) > Q^\\star(s_0, a_R)$，所以最优动作是 $a_L$。\n\n接下来，我们计算归一化奖励 $r_{\\text{norm}}$。\n对于 $s_0$，$\\mathcal{A}(s_0)=\\{a_L, a_R\\}$，所以 $|\\mathcal{A}(s_0)|=2$。\n$$\\mu(s_0) = \\frac{1}{2}(r(s_0, a_L) + r(s_0, a_R)) = \\frac{1}{2}(0+1) = 0.5$$\n$$r_{\\text{norm}}(s_0, a_L) = r(s_0, a_L) - \\mu(s_0) = 0 - 0.5 = -0.5$$\n$$r_{\\text{norm}}(s_0, a_R) = r(s_0, a_R) - \\mu(s_0) = 1 - 0.5 = 0.5$$\n对于 $s_1$，$\\mathcal{A}(s_1)=\\{\\text{dummy}\\}$, 所以 $|\\mathcal{A}(s_1)|=1$。\n$$\\mu(s_1) = r(s_1, \\text{dummy}) = 5$$\n$$r_{\\text{norm}}(s_1, \\text{dummy}) = 5 - 5 = 0$$\n对于 $s_2$，$\\mathcal{A}(s_2)=\\{\\text{dummy}\\}$, 所以 $|\\mathcal{A}(s_2)|=1$。\n$$\\mu(s_2) = r(s_2, \\text{dummy}) = 0$$\n$$r_{\\text{norm}}(s_2, \\text{dummy}) = 0 - 0 = 0$$\n\n现在我们计算 $r_{\\text{norm}}$ 的最优Q值，$Q^\\star_{\\text{norm}}$。\n对于 $s_1$：$Q^\\star_{\\text{norm}}(s_1, \\text{dummy}) = r_{\\text{norm}}(s_1, \\text{dummy}) = 0$，所以 $V^\\star_{\\text{norm}}(s_1) = 0$。\n对于 $s_2$：$Q^\\star_{\\text{norm}}(s_2, \\text{dummy}) = r_{\\text{norm}}(s_2, \\text{dummy}) = 0$，所以 $V^\\star_{\\text{norm}}(s_2) = 0$。\n对于 $s_0$：\n$$Q^\\star_{\\text{norm}}(s_0, a_L) = r_{\\text{norm}}(s_0, a_L) + \\gamma V^\\star_{\\text{norm}}(s_1) = -0.5 + 0.9 \\cdot 0 = -0.5$$\n$$Q^\\star_{\\text{norm}}(s_0, a_R) = r_{\\text{norm}}(s_0, a_R) + \\gamma V^\\star_{\\text{norm}}(s_2) = 0.5 + 0.9 \\cdot 0 = 0.5$$\n新的排序为 $Q^\\star_{\\text{norm}}(s_0, a_R) > Q^\\star_{\\text{norm}}(s_0, a_L)$，所以最优动作是 $a_R$。\n在 $s_0$ 处的动作排序被反转了。\n\n对A的结论：**正确**。\n\n**B. 对常数奖励平移的不变性**\n\n令 $r'(s,a,s') = r(s,a,s') + c$。令 $Q^\\star$ 和 $Q'^\\star$ 分别为对应的最优动作价值函数。\n我们可以通过对价值迭代的步骤进行归纳或直接验证来证明 $Q'^\\star(s,a) = Q^\\star(s,a) + \\frac{c}{1-\\gamma}$。\n假设这个关系成立。$Q'^\\star$ 的贝尔曼方程为：\n$$Q'^\\star(s,a) = \\mathbb{E}_{s'}[r(s,a,s') + c + \\gamma \\max_{a'} Q'^\\star(s', a')]$$\n代入我们假设的关系：\n$$Q^\\star(s,a) + \\frac{c}{1-\\gamma} = \\mathbb{E}_{s'}[r(s,a,s') + c + \\gamma \\max_{a'} (Q^\\star(s', a') + \\frac{c}{1-\\gamma})]$$\n$$Q^\\star(s,a) + \\frac{c}{1-\\gamma} = \\mathbb{E}_{s'}[r(s,a,s')] + c + \\gamma \\mathbb{E}_{s'}[\\max_{a'} Q^\\star(s', a')] + \\gamma \\frac{c}{1-\\gamma}$$\n$$Q^\\star(s,a) + \\frac{c}{1-\\gamma} = (\\mathbb{E}_{s'}[r(s,a,s')] + \\gamma \\mathbb{E}_{s'}[\\max_{a'} Q^\\star(s', a')]) + (c + \\frac{\\gamma c}{1-\\gamma})$$\n第一个括号是 $Q^\\star(s,a)$。第二个是 $\\frac{c(1-\\gamma)+\\gamma c}{1-\\gamma} = \\frac{c}{1-\\gamma}$。该方程成立。\n在状态 $s$ 的最优策略由 $\\arg\\max_a Q^\\star(s,a)$ 给出。对于新的奖励，它是 $\\arg\\max_a Q'^\\star(s,a)$。\n$$\\arg\\max_a Q'^\\star(s,a) = \\arg\\max_a \\left( Q^\\star(s,a) + \\frac{c}{1-\\gamma} \\right) = \\arg\\max_a Q^\\star(s,a)$$\n因为 $\\frac{c}{1-\\gamma}$ 对于动作 $a$ 是一个常数，所以它不影响 argmax。最优动作的集合保持不变。\n\n对B的结论：**正确**。\n\n**C. 对奖励的正向缩放的不变性**\n\n令 $r'(s,a,s') = a\\,r(s,a,s')$，其中 $a > 0$。令 $Q^\\star$ 和 $Q'^\\star$ 分别为对应的最优动作价值函数。\n我们断言 $Q'^\\star(s,a) = a\\,Q^\\star(s,a)$。我们使用贝尔曼方程来验证这一点。\n$$Q'^\\star(s,a) = \\mathbb{E}_{s'}[a\\,r(s,a,s') + \\gamma \\max_{a'} Q'^\\star(s', a')]$$\n$$a\\,Q^\\star(s,a) = \\mathbb{E}_{s'}[a\\,r(s,a,s') + \\gamma \\max_{a'} (a\\,Q^\\star(s', a'))]$$\n因为 $a>0$，所以 $\\max_{a'} (a\\,X(a')) = a\\,\\max_{a'} X(a')$。\n$$a\\,Q^\\star(s,a) = \\mathbb{E}_{s'}[a\\,r(s,a,s') + \\gamma a \\max_{a'} Q^\\star(s', a')]$$\n$$a\\,Q^\\star(s,a) = a \\left( \\mathbb{E}_{s'}[r(s,a,s') + \\gamma \\max_{a'} Q^\\star(s', a')] \\right)$$\n两边除以 $a$ 就恢复了 $Q^\\star$ 的贝尔曼方程。该关系成立。\n新的策略由以下确定：\n$$\\arg\\max_a Q'^\\star(s,a) = \\arg\\max_a (a\\,Q^\\star(s,a))$$\n因为 $a > 0$，最大化 $a\\,Q^\\star(s,a)$ 等价于最大化 $Q^\\star(s,a)$。最优动作的集合保持不变。\n\n对C的结论：**正确**。\n\n**D. 对状态依赖的奖励平移的不变性**\n\n令 $r'(s,a,s') = r(s,a,s') + b(s)$。这是一种奖励塑造的一般形式。为了使最优策略保持不变，奖励塑造必须是“基于势能的”形式 $F(s,s') = \\gamma\\Phi(s') - \\Phi(s)$，其中 $\\Phi$ 是状态上的某个实值函数。一个仅依赖于源状态的塑造项 $b(s)$ 通常不具有这种形式。\n我们可以使用选项A的结果作为一个直接的反例。在该案例中，奖励转换为 $r_{\\text{norm}}(s,a) = r(s,a) - \\mu(s)$。这是本选项中转换的一个实例，其中 $b(s) = -\\mu(s)$。正如我们所计算的，状态 $s_0$ 的最优策略从偏好 $a_L$ 变为偏好 $a_R$。因此，状态依赖的奖励平移通常不会保持最优贪心动作的集合。\n\n对D的结论：**不正确**。\n\n**E. 在常数奖励平移下Q学习轨迹的不变性**\n\n该陈述声称，对于 $r' = r+c$ 和任意初始化 $Q_0$（意味着 $Q'_0 = Q_0$），贪心策略序列是相同的。\n让我们追踪第一次更新。假设在 $t=0$ 时，我们处于状态 $s_0$，贪心动作是 $a_0 = \\arg\\max_a Q_0(s_0,a)$。两个系统都选择相同的动作。在观察到转移到 $s_1$ 和奖励 $r_0$（或 $r'_0 = r_0+c$）后，更新如下：\n$$Q_1(s_0, a_0) = (1-\\alpha_0)Q_0(s_0,a_0) + \\alpha_0(r_0 + \\gamma \\max_{a'} Q_0(s_1,a'))$$\n$$Q'_1(s_0, a_0) = (1-\\alpha_0)Q'_0(s_0,a_0) + \\alpha_0(r'_0 + \\gamma \\max_{a'} Q'_0(s_1,a'))$$\n在 $Q'_0 = Q_0$ 和 $r'_0=r_0+c$ 的情况下：\n$$Q'_1(s_0, a_0) = (1-\\alpha_0)Q_0(s_0,a_0) + \\alpha_0(r_0+c + \\gamma \\max_{a'} Q_0(s_1,a'))$$\n$$Q'_1(s_0, a_0) = Q_1(s_0, a_0) + \\alpha_0 c$$\n对于任何其他状态-动作对 $(s,a) \\ne (s_0, a_0)$，没有更新发生，所以 $Q_1(s,a) = Q_0(s,a)$ 且 $Q'_1(s,a) = Q_0(s,a)$。\n现在考虑时间 $t=1$ 时在 $s_0$ 的贪心策略。它是 $\\arg\\max_a Q_1(s_0,a)$ 对比 $\\arg\\max_a Q'_1(s_0,a)$。\n除了 $a_0$ 之外的动作的Q值与 $Q_0$ 相比没有变化，而 $Q(s_0,a_0)$ 已经更新。$Q'$ 的更新包含一个额外的项 $\\alpha_0 c$。\n这可能会改变动作排序。设 $Q_0(s_0, a_0) = 10, Q_0(s_0, a_1)=9.9$。选择动作 $a_0$。设TD目标较低，例如 $r_0 + \\gamma \\max Q_0(s_1, \\cdot) = 5$，且 $\\alpha_0=0.1$。\n$Q_1(s_0, a_0) = (1-0.1) \\cdot 10 + 0.1 \\cdot 5 = 9.5$。在 $s_0$ 的新策略偏好 $a_1$，因为 $Q_1(s_0, a_1)=9.9 > 9.5$。\n对于 $Q'$，设 $c=20$。TD目标是 $5+20=25$。\n$Q'_1(s_0, a_0) = (1-0.1) \\cdot 10 + 0.1 \\cdot 25 = 9 + 2.5 = 11.5$。在 $s_0$ 的新策略仍然偏好 $a_0$，因为 $Q'_1(s_0,a_0)=11.5 > Q'_1(s_0,a_1)=9.9$。\n贪心策略出现分歧。\n\n对E的结论：**不正确**。\n\n**F. 在特殊初始化下，Q学习轨迹对仿射变换的不变性**\n\n奖励转换为 $r' = a\\,r + b$ ($a>0$)，初始化为 $Q'_0(s,a) = a\\,Q_0(s,a) + \\frac{b}{1-\\gamma}$。该陈述声称对于所有 $t$，都有 $Q'_t(s,a) = a\\,Q_t(s,a) + \\frac{b}{1-\\gamma}$，并且策略重合。\n如果这个Q值关系对所有 $t$ 都成立，那么贪心策略是相同的：\n$\\arg\\max_{a'} Q'_t(s, a') = \\arg\\max_{a'} (a\\,Q_t(s,a') + \\frac{b}{1-\\gamma}) = \\arg\\max_{a'} Q_t(s,a')$。\n这意味着两个过程遵循相同的 $(s_t, a_t)$ 轨迹。\n我们通过归纳法证明Q值关系。\n基本情况 ($t=0$)：根据初始化的定义，关系成立。\n归纳步骤：假设对于所有 $(s,a)$，都有 $Q'_t(s,a) = a\\,Q_t(s,a) + \\frac{b}{1-\\gamma}$。考虑在步骤 $t$ 对 $(s_t, a_t)$ 的更新。\n$$Q'_{t+1}(s_t,a_t) = (1-\\alpha_t)Q'_t(s_t,a_t) + \\alpha_t \\left( r'_t + \\gamma \\max_{a'} Q'_t(s_{t+1},a') \\right)$$\n代入归纳假设和 $r'_t=ar_t+b$：\n$$Q'_{t+1}(s_t,a_t) = (1-\\alpha_t)\\left(a\\,Q_t(s_t,a_t) + \\frac{b}{1-\\gamma}\\right) + \\alpha_t \\left( ar_t+b + \\gamma \\max_{a'} \\left(a\\,Q_t(s_{t+1},a') + \\frac{b}{1-\\gamma}\\right) \\right)$$\n因为 $a>0$，$\\max$ 项简化为：\n$$ = (1-\\alpha_t)\\left(a\\,Q_t + \\frac{b}{1-\\gamma}\\right) + \\alpha_t \\left( ar_t+b + \\gamma \\left(a\\max_{a'}Q_t + \\frac{b}{1-\\gamma}\\right) \\right)$$\n将乘以 $a$ 的项分组：\n$$a \\left[ (1-\\alpha_t)Q_t(s_t,a_t) + \\alpha_t \\left( r_t + \\gamma \\max_{a'} Q_t(s_{t+1},a') \\right) \\right] = a\\,Q_{t+1}(s_t,a_t)$$\n将涉及 $b$ 的项分组：\n$$ (1-\\alpha_t)\\frac{b}{1-\\gamma} + \\alpha_t b + \\alpha_t \\gamma \\frac{b}{1-\\gamma} = b \\left[ \\frac{1-\\alpha_t}{1-\\gamma} + \\alpha_t + \\frac{\\alpha_t \\gamma}{1-\\gamma} \\right]$$\n$$ = b \\left[ \\frac{1-\\alpha_t + \\alpha_t(1-\\gamma) + \\alpha_t\\gamma}{1-\\gamma} \\right] = b \\left[ \\frac{1-\\alpha_t + \\alpha_t - \\alpha_t\\gamma + \\alpha_t\\gamma}{1-\\gamma} \\right] = b \\frac{1}{1-\\gamma}$$\n结合这些，我们得到 $Q'_{t+1}(s_t,a_t) = a\\,Q_{t+1}(s_t,a_t) + \\frac{b}{1-\\gamma}$。\n对于任何 $(s,a) \\neq (s_t, a_t)$，Q值不变，所以关系也成立。归纳完成。\n\n对F的结论：**正确**。", "answer": "$$\\boxed{ABCF}$$", "id": "3163629"}, {"introduction": "Q学习的更新过程常常受到随机奖励和基于噪声价值估计的自举（bootstrapping）的影响，这可能导致贪婪策略不稳定地振荡。这个实践性的编程练习 [@problem_id:3163593] 让你在一个简单的环境中亲身观察这一现象。你将实现并比较标准Q学习与使用Polyak平均（Polyak averaging）平滑Q值估计的版本，从而展示一种提高学习稳定性的强大技术。", "problem": "考虑一个有限马尔可夫决策过程 (MDP)，其具有单个状态 $s$ 和两个动作 $a \\in \\{0,1\\}$。动态是确定性的：执行任一动作后，系统都将以概率 $1$ 停留在状态 $s$。在时间 $t$ 的即时奖励 $R_t$ 是随机的，并且取决于所选的动作：在给定动作 $a$ 的条件下，奖励 $R_t$ 是从一个均值为 $\\mu_a$、标准差为 $\\sigma$ 的正态分布中独立抽取的样本。设折扣因子为 $\\gamma \\in (0,1)$。\n\n定义在时间 $t$ 的动作价值函数 $Q_t(s,a)$，并对两个动作均初始化为 $Q_0(s,a)=0$。学习过程遵循贝尔曼最优性递归的随机近似，其形式为学习率为 $\\alpha \\in (0,1]$ 的单步时序差分更新：\n$$\nQ_{t+1}(s,a_t) = (1-\\alpha) Q_t(s,a_t) + \\alpha \\left( R_t + \\gamma \\max_{b \\in \\{0,1\\}} Q_t(s,b) \\right),\n$$\n且对于 $a \\neq a_t$，有 $Q_{t+1}(s,a) = Q_t(s,a)$。在时间 $t$ 的动作选择是相对于当前估计的贪心选择，即 $a_t \\in \\arg\\max_{a \\in \\{0,1\\}} Q_t(s,a)$，平局则确定性地选择较小的动作索引来打破。\n\n定义在时间 $t \\ge 2$ 发生的一次“策略翻转”为：在时间 $t$ 选择的贪心动作与在时间 $t-1$ 选择的贪心动作不同。\n\n现在引入一个平滑的动作价值估计器 $\\bar{Q}_t(s,a)$，初始化为 $\\bar{Q}_0(s,a)=0$，它通过对原始 $Q_t$ 进行 Polyak 式指数移动平均来定义：\n$$\n\\bar{Q}_t(s,a) = \\rho \\,\\bar{Q}_{t-1}(s,a) + (1-\\rho) \\, Q_t(s,a),\n$$\n其中平滑系数为 $\\rho \\in [0,1]$。在平滑变体中，动作选择是相对于 $\\bar{Q}_t$ 的贪心选择，即 $a_t \\in \\arg\\max_{a \\in \\{0,1\\}} \\bar{Q}_t(s,a)$，并使用相同的平局打破规则。底层的 $Q_t$ 仍然使用与上述相同的时序差分规则进行更新；在每个时间步中，平滑更新在 $Q_t$ 更新之后应用。\n\n你的任务是为每个测试用例实现两个在 $T$ 个时间步范围内的模拟：\n- 原始贪心模拟：相对于 $Q_t$ 的贪心动作选择。\n- 平滑贪心模拟：相对于 $\\bar{Q}_t$ 的贪心动作选择。\n\n对于每个模拟，统计在 $T$ 个时间步内的策略翻转总次数。为了保证可复现性，在每个测试用例中，请使用指定的随机种子来初始化一个伪随机数生成器，并从正确的正态分布中抽取奖励。\n\n为每个测试用例报告一个布尔值：当且仅当平滑贪心选择下的翻转次数严格小于原始贪心选择下的翻转次数时，该值为真，否则为假。\n\n使用以下测试套件，其中每个用例指定为 $(\\mu_0, \\mu_1, \\sigma, \\alpha, \\gamma, T, \\rho, \\text{seed})$：\n- 用例 1：$(0.0, 0.0, 1.0, 0.9, 0.9, 400, 0.95, 123)$。\n- 用例 2：$(0.05, 0.0, 1.0, 0.9, 0.9, 400, 0.95, 456)$。\n- 用例 3：$(0.0, 0.0, 0.0, 0.9, 0.9, 200, 0.95, 789)$。\n- 用例 4：$(0.0, 0.0, 1.0, 0.9, 0.9, 400, 1.0, 321)$。\n\n所有量都是无单位的。你的程序应该产生单行输出，其中包含用方括号括起来的结果，结果是一个逗号分隔的列表，每个条目按顺序对应相应测试用例的布尔结果，例如 $[{\\text{True}},{\\text{False}},{\\text{True}},{\\text{True}}]$。", "solution": "已对用户提供的问题陈述进行分析和验证。\n\n### 第 1 步：提取已知信息\n\n-   **MDP**：单个状态 $s$ 和两个动作 $a \\in \\{0,1\\}$。\n-   **转移**：确定性，$P(s' = s | s, a) = 1$。\n-   **奖励**：随机性，$R_t \\sim \\mathcal{N}(\\mu_a, \\sigma)$，对于所选动作 $a_t$。\n-   **折扣因子**：$\\gamma \\in (0,1)$。\n-   **动作价值函数**：$Q_t(s,a)$，初始化为 $Q_0(s,a)=0$，对于 $a \\in \\{0,1\\}$。\n-   **学习率**：$\\alpha \\in (0,1]$。\n-   **TD 更新规则**：$Q_{t+1}(s,a_t) = (1-\\alpha) Q_t(s,a_t) + \\alpha \\left( R_t + \\gamma \\max_{b \\in \\{0,1\\}} Q_t(s,b) \\right)$，且对于 $a \\neq a_t$，有 $Q_{t+1}(s,a) = Q_t(s,a)$。\n-   **原始动作选择**：贪心选择，$a_t \\in \\arg\\max_{a \\in \\{0,1\\}} Q_t(s,a)$。平局通过选择较小的动作索引（即动作 0）来打破。\n-   **平滑动作价值函数**：$\\bar{Q}_t(s,a) = \\rho \\,\\bar{Q}_{t-1}(s,a) + (1-\\rho) \\, Q_t(s,a)$，初始化为 $\\bar{Q}_0(s,a)=0$。\n-   **平滑系数**：$\\rho \\in [0,1]$。\n-   **平滑动作选择**：贪心选择，$a_t \\in \\arg\\max_{a \\in \\{0,1\\}} \\bar{Q}_t(s,a)$，使用相同的平局打破规则。在每个时间步中，平滑更新在 $Q_t$ 更新之后应用。\n-   **策略翻转**：在时间 $t \\ge 2$ 发生，当贪心动作 $a_t$ 不同于贪心动作 $a_{t-1}$ 时。\n-   **任务**：对于每个测试用例，在 $T$ 个时间步的范围内模拟两种情景（原始贪心选择和平滑贪心选择）。计算每种情景下的策略翻转总次数。\n-   **输出**：对于每个测试用例，输出一个布尔值：如果平滑模拟的翻转次数严格少于原始模拟的翻转次数，则为真，否则为假。\n-   **测试套件**：$(\\mu_0, \\mu_1, \\sigma, \\alpha, \\gamma, T, \\rho, \\text{seed})$\n    -   用例 1：$(0.0, 0.0, 1.0, 0.9, 0.9, 400, 0.95, 123)$\n    -   用例 2：$(0.05, 0.0, 1.0, 0.9, 0.9, 400, 0.95, 456)$\n    -   用例 3：$(0.0, 0.0, 0.0, 0.9, 0.9, 200, 0.95, 789)$\n    -   用例 4：$(0.0, 0.0, 1.0, 0.9, 0.9, 400, 1.0, 321)$\n\n### 第 2 步：使用提取的已知信息进行验证\n\n该问题在科学上基于强化学习的既定理论，特别是 Q-learning。它描述了一个适定（well-posed）的计算任务。所有参数、初始条件、更新规则和动作选择机制都已明确定义。为每个测试用例使用随机种子确保了随机模拟是可复现的，并能产生唯一的解。在时间 $t \\ge 2$ 定义“策略翻转”略显不合常规（因为它忽略了 $a_0$ 和 $a_1$ 之间第一次可能的翻转），但这个定义是明确的，可以按所述实现。该问题是客观的、可形式化的，并提出了标准 Q-learning 与一个带有价值函数平滑的变体之间的非平凡比较。\n\n### 第 3 步：结论与行动\n\n问题陈述被判定为**有效**。将提供一个解决方案。\n\n### 基于原则的算法设计\n\n该问题要求在一个简单的单状态马尔可夫决策过程（MDP）中，实现并比较 Q-learning 代理的两个变体。解决方案的核心是构建一个模拟，该模拟迭代地更新动作价值估计并记录代理的行为。\n\n由于 MDP 只有一个状态 $s$，动作价值函数 $Q_t(s,a)$ 可以简化为一个双元素向量，其中元素对应于动作 $a=0$ 和 $a=1$ 的价值。我们将此向量表示为 $\\mathbf{Q}_t = [Q_t(s,0), Q_t(s,1)]$。类似地，平滑动作价值函数由向量 $\\bar{\\mathbf{Q}}_t$ 表示。\n\n对于每个测试用例，都在 $t=0$ 到 $t=T-1$ 的 $T$ 个时间步范围内运行两个独立的模拟。每个模拟都必须使用指定的种子重新初始化随机数生成器，以确保底层随机奖励过程的可比性。\n\n**模拟循环 ($t = 0, \\dots, T-1$)**\n\n模拟以离散时间步进行。在每个时间步 $t$ 开始时，代理拥有从上一步结转而来的动作价值估计 $\\mathbf{Q}_t$，以及在平滑情况下的 $\\bar{\\mathbf{Q}}_t$。\n\n1.  **动作选择**：基于当前的价值估计选择一个动作 $a_t$。\n    -   在“原始”模拟中，代理相对于 $\\mathbf{Q}_t$ 是贪心的：$a_t = \\arg\\max_{a \\in \\{0,1\\}} Q_t(s,a)$。\n    -   在“平滑”模拟中，代理相对于 $\\bar{\\mathbf{Q}}_t$ 是贪心的：$a_t = \\arg\\max_{a \\in \\{0,1\\}} \\bar{Q}_t(s,a)$。\n    -   平局打破规则规定，如果 $Q_t(s,0) = Q_t(s,1)$（或 $\\bar{Q}_t(s,0) = \\bar{Q}_t(s,1)$），则选择动作 $a=0$。\n\n2.  **奖励观察**：从正态分布 $\\mathcal{N}(\\mu_{a_t}, \\sigma)$ 中抽取一个奖励 $R_t$。\n\n3.  **动作价值更新（$Q$ 更新）**：核心学习步骤使用单步时序差分（TD）更新。所选动作 $a_t$ 的价值被更新，而另一个保持不变。新的价值向量 $\\mathbf{Q}_{t+1}$ 计算如下：\n    $$\n    Q_{t+1}(s, a_t) = (1-\\alpha) Q_t(s, a_t) + \\alpha \\left( R_t + \\gamma \\max_{b \\in \\{0,1\\}} Q_t(s,b) \\right)\n    $$\n    $$\n    Q_{t+1}(s, a) = Q_t(s, a) \\quad \\text{for } a \\neq a_t\n    $$\n    请注意，TD 目标 $R_t + \\gamma \\max_{b} Q_t(s,b)$ 是使用当前更新*之前*的价值估计 $\\mathbf{Q}_t$ 计算的。\n\n4.  **平滑动作价值更新（$\\bar{Q}$ 更新）**：此步骤仅在平滑模拟中执行。根据问题描述，此更新在 $Q$ 更新*之后*发生。新的平滑价值向量 $\\bar{\\mathbf{Q}}_{t+1}$ 是通过其先前值 $\\bar{\\mathbf{Q}}_t$ 和新计算的原始值 $\\mathbf{Q}_{t+1}$ 的指数移动平均（EMA）计算得出的：\n    $$\n    \\bar{\\mathbf{Q}}_{t+1} = \\rho \\bar{\\mathbf{Q}}_t + (1-\\rho) \\mathbf{Q}_{t+1}\n    $$\n    这种平滑旨在抑制由奖励的随机性引起的原始 Q 值的波动，从而可能产生更稳定的策略。\n\n**策略翻转计算**\n\n在完成 $T$ 个时间步的模拟后，将得到一个已执行动作的历史记录 $(a_0, a_1, \\dots, a_{T-1})$。每当时间 $t$ 的动作与时间 $t-1$ 的动作不同时，就计为一次策略翻转。根据问题的字面定义，此计数是针对 $t \\ge 2$ 进行的。因此，总翻转次数是 $a_t \\ne a_{t-1}$ 在 $t \\in \\{2, 3, \\dots, T-1\\}$ 中发生的次数。\n\n**最终比较**\n\n对于每个测试用例，将平滑模拟的翻转次数（`smoothed_flips`）与原始模拟的翻转次数（`raw_flips`）进行比较。该用例的最终输出是严格不等式 `smoothed_flips  raw_flips` 的布尔结果。", "answer": "```python\nimport numpy as np\n\ndef run_simulation(params, use_smoothing):\n    \"\"\"\n    Runs a single Q-learning simulation for a given set of parameters.\n\n    Args:\n        params (tuple): A tuple containing the simulation parameters:\n                        (mu0, mu1, sigma, alpha, gamma, T, rho, seed).\n        use_smoothing (bool): If True, uses smoothed Q-values for action selection.\n\n    Returns:\n        int: The total number of policy flips.\n    \"\"\"\n    mu0, mu1, sigma, alpha, gamma, T, rho, seed = params\n    mu = np.array([mu0, mu1])\n\n    rng = np.random.default_rng(seed)\n\n    Q = np.zeros(2)\n    Q_bar = np.zeros(2) if use_smoothing else None\n    \n    actions_history = []\n\n    for t in range(T):\n        # 1. Select action\n        if use_smoothing:\n            # Greedy action selection with respect to smoothed Q-values\n            # Tie-breaking: choose smaller index (action 0)\n            action = 0 if Q_bar[0] = Q_bar[1] else 1\n        else:\n            # Greedy action selection with respect to raw Q-values\n            action = 0 if Q[0] = Q[1] else 1\n        actions_history.append(action)\n\n        # 2. Get reward\n        reward = rng.normal(loc=mu[action], scale=sigma)\n\n        # 3. Update raw Q-value\n        # The TD target uses the Q-values from the beginning of the step\n        td_target = reward + gamma * np.max(Q)\n        Q[action] = (1 - alpha) * Q[action] + alpha * td_target\n        \n        # 4. Update smoothed Q-value (if applicable)\n        # This update uses the newly computed Q-value, as instructed\n        if use_smoothing:\n            Q_bar = rho * Q_bar + (1 - rho) * Q\n            \n    # Count flips for t = 2\n    flips = 0\n    if T = 3:\n        for t in range(2, T):\n            if actions_history[t] != actions_history[t-1]:\n                flips += 1\n            \n    return flips\n\ndef solve():\n    \"\"\"\n    Solves the problem by running simulations for each test case and comparing\n    the number of policy flips between raw and smoothed Q-learning.\n    \"\"\"\n    test_cases = [\n        # (mu0,   mu1, sigma, alpha, gamma,   T,  rho, seed)\n        (0.0,   0.0,   1.0,   0.9,   0.9, 400, 0.95,  123),\n        (0.05,  0.0,   1.0,   0.9,   0.9, 400, 0.95,  456),\n        (0.0,   0.0,   0.0,   0.9,   0.9, 200, 0.95,  789),\n        (0.0,   0.0,   1.0,   0.9,   0.9, 400,  1.0,  321),\n    ]\n\n    results = []\n    for case in test_cases:\n        # Run raw Q-learning simulation\n        raw_flips = run_simulation(case, use_smoothing=False)\n        \n        # Run smoothed Q-learning simulation\n        smoothed_flips = run_simulation(case, use_smoothing=True)\n        \n        # Compare flip counts and append boolean result\n        results.append(smoothed_flips  raw_flips)\n\n    # Format and print the final output\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3163593"}, {"introduction": "现代强化学习智能体通常使用经验回放（experience replay）来高效地从历史数据中学习，而优先经验回放（prioritized replay）通过关注“意外”的事件进一步增强了这一点。然而，这种非均匀采样会引入偏差。这个练习 [@problem_id:3163626] 将引导你对优先回放进行解析式分析，你将精确计算时序差分（TD）误差估计器的偏差和方差。通过实现这些解析公式，你将对重要性采样（importance sampling）如何纠正偏差及其所涉及的权衡获得深刻的、定量的理解。", "problem": "给定一个小的、有限的马尔可夫决策过程 (MDP)，要求您分析优先经验回放对Q学习中单步时间差分统计量估计的影响。其基础是以下一系列定义。\n\n首先从具有有限状态空间和动作空间的马尔可夫决策过程、Q学习更新规则以及时间差分误差的标准定义开始。在Q学习中，对于一个采样得到的转移 $(s,a,r,s')$，时间差分误差定义为 $\\delta = r + \\gamma \\max_{a'} Q(s',a') - Q(s,a)$，其中 $\\gamma \\in [0,1)$，$Q$ 表示当前的动作价值函数。优先回放根据分配给每个转移的优先级，按照非均匀概率从回放缓冲区 $\\mathcal{D}$ 中采样转移。优先级通常是绝对时间差分误差的函数。一种常见的方案将转移 $i$ 的采样概率设置为 $p(i) = \\dfrac{(\\lvert \\delta_i \\rvert + \\epsilon)^{\\alpha}}{\\sum_{j \\in \\mathcal{D}} (\\lvert \\delta_j \\rvert + \\epsilon)^{\\alpha}}$，其中 $\\epsilon > 0$ 确保所有转移的概率都非零，而 $\\alpha \\ge 0$ 控制优先级的程度。在估计基于缓冲区上均匀分布的期望时，使用重要性采样权重来校正由非均匀采样引入的偏差。\n\n在本问题中，您将在一个简化但具有科学意义的环境中工作，以量化优先采样下估计量的偏差和方差，并使用重要性采样来校正偏差。考虑一个大小为 $N$ 的回放缓冲区 $\\mathcal{D}$，其中包含从一个固定的数据收集策略中抽取的转移。在 MDP 中，这些转移分为两种性质截然不同的类型：“稀有”转移（奖励为 $R > 0$）和“普通”转移（奖励为 $0$）。假设Q值对于所有 $(s,a)$ 都固定为 $Q(s,a)=0$，并且折扣因子 $\\gamma$ 是 $[0,1)$ 中的任意值；当 $Q(s,a)=0$ 时，每个转移的时间差分误差等于其即时奖励，即 $\\delta_i = r_i$。缓冲区包含 $N_{\\text{rare}}$ 个稀有转移和 $N_{\\text{common}}=N-N_{\\text{rare}}$ 个普通转移。对于带有参数 $\\alpha \\ge 0$ 和 $\\epsilon > 0$ 的优先回放，每个转移 $i$ 以概率 \n$$\np(i) = \\frac{(\\lvert \\delta_i \\rvert + \\epsilon)^{\\alpha}}{\\sum_{j=1}^{N} (\\lvert \\delta_j \\rvert + \\epsilon)^{\\alpha}}\n$$\n被采样。\n\n您需要估计时间差分误差在均匀缓冲区上的均值，\n$$\n\\mu \\equiv \\frac{1}{N} \\sum_{i=1}^{N} \\delta_i,\n$$\n使用基于从优先分布 $p(i)$ 中进行 $m$ 次有放回独立采样的三个估计量：\n- 无加权优先采样估计量（无校正），定义为所抽取 $\\delta$ 值的样本均值（等效于重要性指数 $\\beta = 0$）。\n- 使用重要性采样权重进行部分校正的估计量，指数为 $\\beta \\in (0,1)$，定义为 $w(i)^{\\beta} \\delta_i$ 的样本均值，其中 $w(i) = \\frac{1/N}{p(i)}$。\n- 使用 $\\beta = 1$ 进行完全校正的无偏估计量，定义为 $w(i) \\delta_i$ 的样本均值。\n\n对于这三个估计量中的每一个，请解析地（非蒙特卡洛模拟）计算在从 $p(i)$ 进行 $m$ 次独立同分布抽样下的样本均值估计量的偏差和方差。您可以使用以下基本事实：\n- $m$ 个独立同分布随机变量的样本均值的期望值等于总体期望。\n- $m$ 个独立同分布随机变量的样本均值的方差等于总体方差除以 $m$。\n- 对于任意在有限个值上定义的离散分布，其期望和方差可以通过对支持点进行有限求和来计算。\n\n您必须实现一个完整、可运行的程序，该程序对下面的测试套件中的每个参数集计算：\n- 在优先采样下无校正（等效于 $\\beta=0$）的样本均值估计量的解析偏差和方差。\n- 使用指定的部分校正指数 $\\beta \\in (0,1)$ 时的解析偏差和方差。\n- 使用完全校正 $\\beta=1$ 时的解析偏差和方差。\n\n使用以下参数集测试套件。每个集合是一个元组 $(N, N_{\\text{rare}}, R, \\alpha, \\epsilon, m, \\beta_{\\text{half}})$，所有量均采用标准数学单位：\n- 情况A（理想路径）：$(N=\\;1000, N_{\\text{rare}}=\\;10, R=\\;1.0, \\alpha=\\;0.6, \\epsilon=\\;10^{-3}, m=\\;256, \\beta_{\\text{half}}=\\;0.5)$。\n- 情况B（强优先级和极端稀有性）：$(N=\\;1000, N_{\\text{rare}}=\\;1, R=\\;2.0, \\alpha=\\;1.0, \\epsilon=\\;10^{-6}, m=\\;64, \\beta_{\\text{half}}=\\;0.5)$。\n- 情况C（无优先级基线）：$(N=\\;1000, N_{\\text{rare}}=\\;10, R=\\;1.0, \\alpha=\\;0.0, \\epsilon=\\;10^{-3}, m=\\;256, \\beta_{\\text{half}}=\\;0.5)$。\n- 情况D（无稀有事件边界）：$(N=\\;100, N_{\\text{rare}}=\\;0, R=\\;1.0, \\alpha=\\;0.7, \\epsilon=\\;10^{-4}, m=\\;32, \\beta_{\\text{half}}=\\;0.5)$。\n- 情况E（零奖励边界）：$(N=\\;200, N_{\\text{rare}}=\\;20, R=\\;0.0, \\alpha=\\;0.9, \\epsilon=\\;10^{-3}, m=\\;128, \\beta_{\\text{half}}=\\;0.5)$。\n\n对于每种情况，您的程序应输出一个包含六个浮点数的列表：\n$[\\text{bias}_{\\beta=0}, \\text{var}_{\\beta=0}, \\text{bias}_{\\beta=\\beta_{\\text{half}}}, \\text{var}_{\\beta=\\beta_{\\text{half}}}, \\text{bias}_{\\beta=1}, \\text{var}_{\\beta=1}]$，\n其中 $\\text{var}$ 表示样本均值估计量的方差，$\\text{bias}$ 表示估计量的期望值与 $\\mu$ 之间的差异。所有结果必须四舍五入到六位小数。\n\n最终输出格式要求：您的程序应生成单行输出，其中包含一个逗号分隔的各情况结果列表，每个情况的结果是如上所述的列表，所有结果都包含在一对单独的方括号中，例如：\n\"[[x_{1},x_{2},x_{3},x_{4},x_{5},x_{6}],[y_{1},y_{2},y_{3},y_{4},y_{5},y_{6}],...]\"。", "solution": "此问题的目标是解析计算在优先经验回放下三种不同时间差分 (TD) 误差估计量的偏差和方差。分析在一个简化但具有代表性的马尔可夫决策过程 (MDP) 环境中进行。这三种估计量分别对应于不使用重要性采样 (IS) 校正 ($\\beta=0$)、部分校正 ($\\beta \\in (0,1)$) 和完全校正 ($\\beta=1$)。\n\n分析按以下步骤进行：\n1.  定义简化的MDP和回放缓冲区的组成部分。\n2.  指明TD误差的真实均值，这是待估计的目标量。\n3.  建立优先回放方案下的采样概率公式。\n4.  为从回放缓冲区中抽取的单个经校正的样本定义一个随机变量。\n5.  对于任意校正指数 $\\beta$，推导该随机变量的期望和方差的通用表达式。\n6.  根据这些表达式，计算对于三个指定的 $\\beta$ 值，在 $m$ 次抽样下样本均值估计量的偏差和方差。\n\n**1. 模型与回放缓冲区构成**\n回放缓冲区 $\\mathcal{D}$ 包含 $N$ 个转移。这些转移被分为两类：\n- $N_{\\text{rare}}$ 个“稀有”转移，每个奖励为 $R$，因此TD误差为 $\\delta_i = R$。\n- $N_{\\text{common}} = N - N_{\\text{rare}}$ 个“普通”转移，每个奖励为 $0$，因此TD误差为 $\\delta_j = 0$。\n\n此简化源于问题陈述，其中动作价值函数对所有状态-动作对 $(s,a)$ 固定为 $Q(s,a)=0$，使得TD误差 $\\delta = r + \\gamma \\max_{a'} Q(s',a') - Q(s,a)$ 等于即时奖励 $r$。\n\n**2. 目标量：真实均值**\n我们希望估计的量是假设在缓冲区 $\\mathcal{D}$ 上均匀分布的TD误差的均值。这个真实均值，记为 $\\mu$，是：\n$$ \\mu = \\frac{1}{N} \\sum_{i=1}^{N} \\delta_i = \\frac{N_{\\text{rare}} \\cdot R + N_{\\text{common}} \\cdot 0}{N} = \\frac{N_{\\text{rare}} R}{N} $$\n\n**3. 优先采样分布**\n采样转移 $i$ 的概率 $p(i)$ 与 $(\\lvert \\delta_i \\rvert + \\epsilon)^{\\alpha}$ 成正比。\n对于一个稀有转移，$\\lvert \\delta_i \\rvert = R$ (假设 $R \\ge 0$)，所以优先级项是 $(R + \\epsilon)^{\\alpha}$。\n对于一个普通转移，$\\lvert \\delta_j \\rvert = 0$，所以优先级项是 $(\\epsilon)^{\\alpha}$。\n\n归一化常数 $Z$ 是缓冲区中所有转移的这些优先级项的总和：\n$$ Z = \\sum_{j=1}^{N} (\\lvert \\delta_j \\rvert + \\epsilon)^{\\alpha} = N_{\\text{rare}}(R + \\epsilon)^{\\alpha} + N_{\\text{common}}(\\epsilon)^{\\alpha} $$\n采样单个特定稀有转移的概率是：\n$$ p_{\\text{rare}} = \\frac{(R + \\epsilon)^{\\alpha}}{Z} $$\n采样单个特定普通转移的概率是：\n$$ p_{\\text{common}} = \\frac{(\\epsilon)^{\\alpha}}{Z} $$\n\n**4. 经校正的样本估计量**\n$\\mu$ 的一个估计量是通过对从优先分布 $p(i)$ 中抽取的 $m$ 个独立同分布 (i.i.d.) 样本取样本均值来构成的。为了校正采样偏差，每个采样的TD误差 $\\delta_i$ 乘以一个重要性采样权重 $w(i) = \\frac{1/N}{p(i)}$，并取其 $\\beta$ 次幂，其中 $\\beta \\in [0,1]$。\n\n令 $X_{\\beta}$ 为表示单个经校正样本的随机变量，$X_{\\beta} = w(i)^{\\beta} \\delta_i$。估计量是 $\\hat{\\mu}_{\\beta} = \\frac{1}{m} \\sum_{k=1}^{m} X_{\\beta}^{(k)}$，其中每个 $X_{\\beta}^{(k)}$ 都是一个独立的抽样。\n\n$X_{\\beta}$ 的值取决于抽到的是稀有转移还是普通转移：\n- 如果抽到稀有转移，则 $\\delta_i=R$，其值为 $x_{\\text{rare}} = \\left(\\frac{1/N}{p_{\\text{rare}}}\\right)^{\\beta} R$。这种情况发生的总概率为 $P(\\text{draw rare}) = N_{\\text{rare}} p_{\\text{rare}}$。\n- 如果抽到普通转移，则 $\\delta_j=0$，其值为 $x_{\\text{common}} = 0$。这种情况发生的总概率为 $P(\\text{draw common}) = N_{\\text{common}} p_{\\text{common}}$。\n\n**5. 偏差和方差的通用推导**\n估计量 $\\hat{\\mu}_{\\beta}$ 的偏差和方差可以从单次抽样的期望 $E[X_\\beta]$ 和方差 $V[X_\\beta]$ 中得出。\n- 偏差：$B[\\hat{\\mu}_{\\beta}] = E[\\hat{\\mu}_{\\beta}] - \\mu = E[X_{\\beta}] - \\mu$。\n- 估计量方差：$V[\\hat{\\mu}_{\\beta}] = \\frac{V[X_{\\beta}]}{m} = \\frac{E[X_{\\beta}^2] - (E[X_{\\beta}])^2}{m}$。\n\n$X_{\\beta}$ 的期望是：\n$$ E[X_{\\beta}] = x_{\\text{rare}} \\cdot P(\\text{draw rare}) + x_{\\text{common}} \\cdot P(\\text{draw common}) $$\n$$ E[X_{\\beta}] = \\left(\\frac{1}{N p_{\\text{rare}}}\\right)^{\\beta} R \\cdot (N_{\\text{rare}} p_{\\text{rare}}) + 0 = R \\cdot N_{\\text{rare}} \\cdot (p_{\\text{rare}})^{1-\\beta} N^{-\\beta} $$\n$X_{\\beta}^2$ 的期望是：\n$$ E[X_{\\beta}^2] = (x_{\\text{rare}})^2 \\cdot P(\\text{draw rare}) + (x_{\\text{common}})^2 \\cdot P(\\text{draw common}) $$\n$$ E[X_{\\beta}^2] = \\left(\\left(\\frac{1}{N p_{\\text{rare}}}\\right)^{\\beta} R\\right)^2 \\cdot (N_{\\text{rare}} p_{\\text{rare}}) + 0 = R^2 \\cdot N_{\\text{rare}} \\cdot (p_{\\text{rare}})^{1-2\\beta} N^{-2\\beta} $$\n\n**6. 对特定 $\\beta$ 值的分析**\n这些通用公式现在可以应用于我们感兴趣的三种情况。\n\n**情况 1：无校正 ($\\beta=0$)**\n该估计量是未经校正的TD误差的简单样本均值。\n- 期望：$E[X_0] = R \\cdot N_{\\text{rare}} \\cdot p_{\\text{rare}}$。\n- 偏差：$B[\\hat{\\mu}_0] = R \\cdot N_{\\text{rare}} \\cdot p_{\\text{rare}} - \\mu$。\n- 二阶矩：$E[X_0^2] = R^2 \\cdot N_{\\text{rare}} \\cdot p_{\\text{rare}}$。\n- 估计量方差：$V[\\hat{\\mu}_0] = \\frac{1}{m}(R^2 N_{\\text{rare}} p_{\\text{rare}} - (R N_{\\text{rare}} p_{\\text{rare}})^2) = \\frac{1}{m} R^2 N_{\\text{rare}} p_{\\text{rare}}(1 - N_{\\text{rare}} p_{\\text{rare}})$。\n该估计量通常是有偏的，与均匀采样基线相比方差较低，因为优先级采样专注于特定类型的转移。\n\n**情况 2：完全校正 ($\\beta=1$)**\n该估计量使用标准的重要性采样校正。\n- 期望：$E[X_1] = R \\cdot N_{\\text{rare}} \\cdot (p_{\\text{rare}})^{1-1} N^{-1} = \\frac{R N_{\\text{rare}}}{N} = \\mu$。\n- 偏差：$B[\\hat{\\mu}_1] = \\mu - \\mu = 0$。根据重要性采样理论，该估计量是无偏的。\n- 二阶矩：$E[X_1^2] = R^2 \\cdot N_{\\text{rare}} \\cdot (p_{\\text{rare}})^{1-2} N^{-2} = \\frac{R^2 N_{\\text{rare}}}{N^2 p_{\\text{rare}}}$。\n- 估计量方差：$V[\\hat{\\mu}_1] = \\frac{1}{m}(E[X_1^2] - \\mu^2) = \\frac{1}{m}\\left(\\frac{R^2 N_{\\text{rare}}}{N^2 p_{\\text{rare}}} - \\left(\\frac{R N_{\\text{rare}}}{N}\\right)^2\\right) = \\frac{R^2}{m N^2}\\left(\\frac{N_{\\text{rare}}}{p_{\\text{rare}}} - N_{\\text{rare}}^2\\right)$。\n该估计量的方差可能高于或低于均匀采样的方差，这取决于均匀分布和优先分布之间的不匹配程度。\n\n**情况 3：部分校正 ($\\beta = \\beta_{\\text{half}} \\in (0,1)$)**\n这代表了一种权衡，旨在减少无校正估计量的偏差，同时控制完全校正估计量可能过高的方差。偏差和方差通过将 $\\beta = \\beta_{\\text{half}}$ 代入第5节推导的通用公式直接计算。\n- 偏差：$B[\\hat{\\mu}_{\\beta_{\\text{half}}}] = R \\cdot N_{\\text{rare}} \\cdot (p_{\\text{rare}})^{1-\\beta_{\\text{half}}} N^{-\\beta_{\\text{half}}} - \\mu$。\n- 估计量方差：$V[\\hat{\\mu}_{\\beta_{\\text{half}}}] = \\frac{1}{m}\\left( R^2 N_{\\text{rare}} (p_{\\text{rare}})^{1-2\\beta_{\\text{half}}} N^{-2\\beta_{\\text{half}}} - (E[X_{\\beta_{\\text{half}}}])^2 \\right)$。\n\n这些推导出的表达式允许对每个测试用例所需量进行直接的解析计算。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the analytic bias and variance of TD error estimators\n    under prioritized experience replay for a suite of test cases.\n    \"\"\"\n    # Parameter sets as (N, N_rare, R, alpha, epsilon, m, beta_half)\n    test_cases = [\n        (1000, 10, 1.0, 0.6, 1e-3, 256, 0.5), # Case A\n        (1000, 1, 2.0, 1.0, 1e-6, 64, 0.5),  # Case B\n        (1000, 10, 1.0, 0.0, 1e-3, 256, 0.5), # Case C\n        (100, 0, 1.0, 0.7, 1e-4, 32, 0.5),   # Case D\n        (200, 20, 0.0, 0.9, 1e-3, 128, 0.5), # Case E\n    ]\n\n    all_results = []\n    \n    for case in test_cases:\n        N, N_rare, R, alpha, epsilon, m, beta_half = case\n        \n        # Use floating point numbers for calculations to ensure precision and\n        # compatibility with functions like np.power.\n        N, N_rare, R, m = float(N), float(N_rare), float(R), float(m)\n\n        case_results = []\n\n        # The true mean of the TD errors over the uniform buffer distribution.\n        # This is the target value for our estimators.\n        mu = (N_rare * R) / N if N  0 else 0.0\n        \n        # The derived analytical formulas work for boundary cases (N_rare=0 or R=0)\n        # where mu becomes 0. In these cases, all TD errors are 0, so any sample is 0,\n        # leading to an estimator that is always 0, with zero bias and variance.\n        # The formulas correctly reflect this.\n        \n        if N_rare == 0 or R == 0.0:\n            # If no rare events or reward is zero, all deltas are 0.\n            # All estimators will be 0, so bias and variance are 0.\n            p_rare = 1.0/N if N  0 else 1.0 # Not used but avoids division by zero if N=0\n        else:\n            N_common = N - N_rare\n            # Calculate priorities for rare and common transitions.\n            # priority is proportional to (|delta| + epsilon)^alpha\n            priority_term_rare = np.power(R + epsilon, alpha)\n            priority_term_common = np.power(epsilon, alpha)\n            \n            # Normalization constant Z is the sum of all priorities.\n            Z = N_rare * priority_term_rare + N_common * priority_term_common\n            \n            # Probability of sampling a single specific rare transition.\n            # Z cannot be zero since N > 0 and epsilon > 0.\n            p_rare = priority_term_rare / Z\n        \n        betas_to_test = [0.0, beta_half, 1.0]\n        \n        for beta in betas_to_test:\n            # Let X_beta be the random variable for a single corrected sample: w(i)^beta * delta_i\n            # We calculate its population expectation E[X_beta] and variance Var(X_beta).\n            \n            if N_rare == 0 or R == 0.0:\n                 E_X_beta = 0.0\n                 E_X_beta_sq = 0.0\n            else:\n                # E[X_beta] = R * N_rare * (p_rare)^(1-beta) * N^(-beta)\n                E_X_beta = R * N_rare * np.power(p_rare, 1 - beta) * np.power(N, -beta)\n                \n                # E[X_beta^2] = R^2 * N_rare * (p_rare)^(1-2*beta) * N^(-2*beta)\n                E_X_beta_sq = np.power(R, 2) * N_rare * np.power(p_rare, 1 - 2 * beta) * np.power(N, -2 * beta)\n\n            # Bias of the estimator is E[estimator] - true_mean\n            bias = E_X_beta - mu\n            \n            # Population variance Var(X_beta) = E[X_beta^2] - (E[X_beta])^2\n            V_X_beta = E_X_beta_sq - np.power(E_X_beta, 2)\n            \n            # The estimator's variance is Var(X_beta) / m for a sample size of m.\n            var_estimator = V_X_beta / m\n            \n            # Due to floating point inaccuracies, a variance that should be zero\n            # might calculate as a tiny negative number. Clamp at 0.\n            var_estimator = max(0.0, var_estimator)\n            \n            case_results.extend([bias, var_estimator])\n            \n        all_results.append([round(x, 6) for x in case_results])\n\n    # Format the final output string as a list of lists, per requirements.\n    # e.g., \"[[r1_1, r1_2, ...], [r2_1, r2_2, ...]]\"\n    formatted_results = [f\"[{','.join(map(str, res))}]\" for res in all_results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3163626"}]}