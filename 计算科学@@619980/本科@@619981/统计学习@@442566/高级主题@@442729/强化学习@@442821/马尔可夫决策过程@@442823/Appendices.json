{"hands_on_practices": [{"introduction": "本练习旨在通过动手编程，巩固求解马尔可夫决策过程（MDP）核心算法的理解。你将实现价值迭代的两种变体——同步（类雅可比）和异步（类高斯-赛德尔）更新——来计算最优价值函数，并体会它们与求解贝尔曼方程这一线性系统时所用数值方法的深刻联系。通过这个实践，你不仅能掌握动态规划的基本操作，还能加深对算法收敛性和效率的认识。[@problem_id:3245192]", "problem": "给定一个有折扣的、有限状态、有限动作的马尔可夫决策过程 (MDP)，其目标是通过迭代法计算 Bellman 最优性方程的唯一不动点。设状态集为 $\\mathcal{S} = \\{0, 1, \\dots, n-1\\}$，动作集为 $\\mathcal{A} = \\{0, 1, \\dots, m-1\\}$。对于每个状态-动作对 $(s,a)$，存在一个期望即时奖励 $r(s,a)$ 和一个关于下一状态的转移概率核 $P(s' \\mid s,a)$。折扣因子为 $\\gamma \\in [0,1)$。用 $v \\in \\mathbb{R}^n$ 表示状态上的价值向量。\n\n基本原理：\n- Bellman 最优性算子 $\\mathcal{T}$ 对 $v \\in \\mathbb{R}^n$ 按分量定义如下\n$$\n(\\mathcal{T} v)(s) \\triangleq \\max_{a \\in \\mathcal{A}} \\left( r(s,a) + \\gamma \\sum_{s' \\in \\mathcal{S}} P(s' \\mid s,a) v(s') \\right).\n$$\n- 最优价值 $v^\\star$ 是满足 $v^\\star = \\mathcal{T} v^\\star$ 的 $\\mathcal{T}$ 的唯一不动点。\n- 对于任意固定策略 $\\pi : \\mathcal{S} \\to \\mathcal{A}$，策略评估方程是线性的：\n$$\nv_\\pi = r_\\pi + \\gamma P_\\pi v_\\pi, \\quad \\text{等价于} \\quad (I - \\gamma P_\\pi) v_\\pi = r_\\pi,\n$$\n其中 $r_\\pi(s) \\triangleq r(s,\\pi(s))$ 且 $(P_\\pi)_{s,s'} \\triangleq P(s' \\mid s,\\pi(s))$。\n\n任务：\n- 实现两种用于最优性方程的迭代方案：\n  1. 同步价值迭代 (Jacobi 式)，它使用上一次的迭代结果同时更新 $v$ 的所有分量，\n  2. Gauss-Seidel 式价值迭代，它在每次扫描中按固定顺序原地更新分量，并立即重用新更新的分量。\n- 实现两种用于评估给定固定策略 $\\pi$ 的迭代方案：\n  1. 用于线性系统 $(I - \\gamma P_\\pi) v = r_\\pi$ 的同步策略评估 (Jacobi 方法)，\n  2. 用于相同线性系统的 Gauss-Seidel 策略评估。\n\n停止规则：\n- 对于每种方法，从 $v^{(0)} = 0$ (零向量) 开始。在每次完整迭代（一次完整的同步更新或一次对所有状态的完整原地扫描）后，计算更新幅度\n$$\n\\Delta^{(k)} \\triangleq \\lVert v^{(k)} - v^{(k-1)} \\rVert_\\infty,\n$$\n并在第一个满足 $\\Delta^{(k)} \\le \\varepsilon$ 的 $k$ 处停止，其中 $\\varepsilon$ 是给定的容差。对所有方法和测试使用相同的 $\\varepsilon$。计算并报告收敛所需的迭代次数。\n\n与线性系统迭代法的关系：\n- 对于固定策略情况，设 $A \\triangleq I - \\gamma P_\\pi$ 和 $b \\triangleq r_\\pi$，同步策略评估与应用于 $A v = b$ 的 Jacobi 方法一致，而原地策略评估与应用于 $A v = b$ 的 Gauss-Seidel 方法一致。\n\n测试套件：\n实现您的程序以运行以下三个测试。每个测试指定 $(P, R, \\gamma, \\pi)$，其中 $P$ 是一个分量为 $P[s,a,s']$ 的张量，$R$ 是一个分量为 $R[s,a] = r(s,a)$ 的矩阵，$\\gamma$ 是折扣因子，$\\pi$ 是用于策略评估子测试的固定策略。\n\n- 测试 1 (正常路径，较小的 $\\gamma$)：\n  - 状态：$\\{0,1,2\\}$，$n = 3$；动作：$\\{0,1\\}$，$m = 2$。\n  - 转移 $P$：\n    - 动作 0：$P(0 \\mid 0,0) = 0.5$, $P(1 \\mid 0,0) = 0.5$；$P(1 \\mid 1,0) = 0.5$, $P(2 \\mid 1,0) = 0.5$；$P(2 \\mid 2,0) = 1.0$。\n    - 动作 1：$P(0 \\mid 0,1) = 0.7$, $P(1 \\mid 0,1) = 0.3$；$P(0 \\mid 1,1) = 0.4$, $P(2 \\mid 1,1) = 0.6$；$P(1 \\mid 2,1) = 1.0$。\n    - 所有未指明的 $P(s' \\mid s,a)$ 均为 $0$。\n  - 奖励 $R$：$R(0,0) = 5$, $R(1,0) = 0$, $R(2,0) = 0$；$R(0,1) = 4$, $R(1,1) = 1$, $R(2,1) = 2$。\n  - 折扣：$\\gamma = 0.9$。\n  - 用于评估的固定策略：$\\pi(0) = 0$, $\\pi(1) = 1$, $\\pi(2) = 0$。\n- 测试 2 (边界情况，$\\gamma$ 接近 1)：\n  - $P$ 和 $R$ 与测试 1 相同。\n  - 折扣：$\\gamma = 0.99$。\n  - 用于评估的固定策略：$\\pi$ 与测试 1 相同。\n- 测试 3 (边缘情况，吸收结构)：\n  - 状态：$\\{0,1\\}$，$n = 2$；动作：$\\{0,1\\}$，$m = 2$。\n  - 转移 $P$：\n    - 动作 0：$P(1 \\mid 0,0) = 1.0$；$P(1 \\mid 1,0) = 1.0$。\n    - 动作 1：$P(0 \\mid 0,1) = 1.0$；$P(1 \\mid 1,1) = 1.0$。\n    - 所有未指明的 $P(s' \\mid s,a)$ 均为 $0$。\n  - 奖励 $R$：$R(0,0) = 1$, $R(1,0) = 0$；$R(0,1) = 0$, $R(1,1) = 0$。\n  - 折扣：$\\gamma = 0.95$。\n  - 用于评估的固定策略：$\\pi(0) = 1$, $\\pi(1) = 0$。\n  \n容差：\n- 对所有方法和所有测试使用 $\\varepsilon = 10^{-8}$。\n\n所需输出：\n- 对于每个测试，计算并返回以下四个整数：\n  1. $N_{\\text{opt, sync}}$：同步价值迭代收敛所需的迭代次数，\n  2. $N_{\\text{opt, GS}}$：Gauss-Seidel 式价值迭代收敛所需的迭代次数，\n  3. $N_{\\text{eval, Jacobi}}$：同步策略评估 (Jacobi) 收敛所需的迭代次数，\n  4. $N_{\\text{eval, GS}}$：Gauss-Seidel 策略评估收敛所需的迭代次数。\n- 将三个测试的结果按以下顺序汇总到一个扁平列表中\n$$\n[\\;N_{\\text{opt, sync}}^{(1)},\\; N_{\\text{opt, GS}}^{(1)},\\; N_{\\text{eval, Jacobi}}^{(1)},\\; N_{\\text{eval, GS}}^{(1)},\\; N_{\\text{opt, sync}}^{(2)},\\; N_{\\text{opt, GS}}^{(2)},\\; N_{\\text{eval, Jacobi}}^{(2)},\\; N_{\\text{eval, GS}}^{(2)},\\; N_{\\text{opt, sync}}^{(3)},\\; N_{\\text{opt, GS}}^{(3)},\\; N_{\\text{eval, Jacobi}}^{(3)},\\; N_{\\text{eval, GS}}^{(3)}\\;].\n$$\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的、以逗号分隔的列表，列表内无空格。例如，如果有两个测试，每个测试产生两个整数，则格式为 \"[1,2,3,4]\"。在本问题中，该行必须按上述顺序包含 $12$ 个整数。", "solution": "问题陈述已经过验证。\n\n### 步骤 1：提取给定信息\n\n- **集合**：状态空间 $\\mathcal{S} = \\{0, 1, \\dots, n-1\\}$，动作空间 $\\mathcal{A} = \\{0, 1, \\dots, m-1\\}$。\n- **MDP 组件**：\n  - 状态-动作对 $(s,a)$ 的期望即时奖励：$r(s,a)$。\n  - 转移概率核：$P(s' \\mid s,a)$。\n  - 折扣因子：$\\gamma \\in [0,1)$。\n- **价值向量**：$v \\in \\mathbb{R}^n$。\n- **Bellman 最优性算子**：$(\\mathcal{T} v)(s) \\triangleq \\max_{a \\in \\mathcal{A}} \\left( r(s,a) + \\gamma \\sum_{s' \\in \\mathcal{S}} P(s' \\mid s,a) v(s') \\right)$。\n- **最优价值函数**：$v^\\star$ 是满足 $v^\\star = \\mathcal{T} v^\\star$ 的唯一不动点。\n- **固定策略评估**：对于策略 $\\pi : \\mathcal{S} \\to \\mathcal{A}$，其价值函数 $v_\\pi$ 满足线性系统 $(I - \\gamma P_\\pi) v_\\pi = r_\\pi$，其中 $r_\\pi(s) \\triangleq r(s,\\pi(s))$ 且 $(P_\\pi)_{s,s'} \\triangleq P(s' \\mid s,\\pi(s))$。\n- **待实现的迭代方案**：\n  1. 同步价值迭代 (Jacobi 式)。\n  2. Gauss-Seidel 式价值迭代。\n  3. 同步策略评估 (用于线性系统的 Jacobi 方法)。\n  4. 用于线性系统的 Gauss-Seidel 策略评估。\n- **停止规则**：\n  - 初始条件：$v^{(0)} = 0$。\n  - 迭代更新：$\\Delta^{(k)} \\triangleq \\lVert v^{(k)} - v^{(k-1)} \\rVert_\\infty$。\n  - 终止条件：在第一个满足 $\\Delta^{(k)} \\le \\varepsilon$ 的迭代 $k$ 处停止。\n  - 容差：$\\varepsilon = 10^{-8}$。\n- **测试套件**：提供了三个具体的测试用例，每个用例定义了转移张量 $P$、奖励矩阵 $R$、折扣因子 $\\gamma$ 和一个固定策略 $\\pi$。\n  - 测试 1：$n=3, m=2, \\gamma=0.9$。\n  - 测试 2：$n=3, m=2, \\gamma=0.99$。\n  - 测试 3：$n=2, m=2, \\gamma=0.95$。\n- **所需输出**：三个测试中每个测试的迭代次数 ($N_{\\text{opt, sync}}, N_{\\text{opt, GS}}, N_{\\text{eval, Jacobi}}, N_{\\text{eval, GS}}$)，汇总成一个包含 12 个整数的列表。\n\n### 步骤 2：使用提取的信息进行验证\n\n- **科学依据充分**：该问题基于马尔可夫决策过程和动态规划的标准理论。Bellman 最优性方程、策略评估方程以及迭代方法 (价值迭代、Jacobi、Gauss-Seidel) 是该领域和数值线性代数中的基本概念。条件 $\\gamma \\in [0,1)$ 至关重要，因为它确保 Bellman 算子 $\\mathcal{T}$ 是关于无穷范数的压缩映射，从而保证了唯一不动点 $v^\\star$ 的存在性以及价值迭代从任何起点开始的收敛性。这是动态规划的一个基石性成果。该问题在科学和数学上是合理的。\n- **适定性**：该问题是适定的。对于每种算法和测试用例，任务是找出达到指定精度所需的迭代次数。由于 $\\gamma  1$，所描述的所有迭代方法都保证收敛到唯一解。提供了初始条件 $v^{(0)}=0$，且停止准则 $\\lVert v^{(k)} - v^{(k-1)} \\rVert_\\infty \\le \\varepsilon$ 是明确的。因此，对于每个所需的计算，都存在一个唯一的、稳定的、有意义的整数解（迭代次数）。\n- **客观性**：该问题使用精确的数学语言和定义进行陈述。所有数据和要求都是客观指定的，没有主观解释的余地。\n- **完整性和一致性**：问题陈述是自洽的。它为每个测试用例提供了所有必要的数据 ($P$, $R$, $\\gamma$, $\\pi$)、所需的算法、初始条件和精确的停止规则。所提供的信息中没有矛盾之处。\n- **其他缺陷**：该问题没有表现出任何其他缺陷，例如不切实际、结构不良、琐碎或无法验证。测试用例探讨了算法的不同方面，例如对 $\\gamma$ 的敏感性以及在有吸收态时的行为。\n\n### 步骤 3：结论与行动\n\n该问题有效。将提供一个合理的解决方案。\n\n目标是计算给定马尔可夫决策过程 (MDP) 的最优价值函数 $v^\\star$，并评估给定策略 $\\pi$。我们将实现四种经典的迭代算法来实现这一目标，使用指定的收敛准则。对于每种算法，我们都从所有条目均为 0 的初始价值向量 $v^{(0)}$ 开始。\n\n前两种方法处理非线性的 Bellman 最优性方程 $v^\\star = \\mathcal{T}v^\\star$。\n\n第一种算法是**同步价值迭代**。该方法类似于用于线性系统的 Jacobi 方法。在每次迭代 $k$ 中，每个状态 $s$ 的价值都使用前一次迭代 $v^{(k-1)}$ 的价值同时更新。整个向量 $v$ 的更新规则通过应用 Bellman 算子给出：$v^{(k)} = \\mathcal{T}v^{(k-1)}$。按分量表示为：\n$$\nv^{(k)}(s) = \\max_{a \\in \\mathcal{A}} \\left( r(s,a) + \\gamma \\sum_{s' \\in \\mathcal{S}} P(s' \\mid s,a) v^{(k-1)}(s') \\right) \\quad \\text{对所有 } s \\in \\mathcal{S}.\n$$\n计算 $v^{(k)}$ 的所有分量需要 $v^{(k-1)}$ 的一个完整副本。\n\n第二种算法是 **Gauss-Seidel 式价值迭代**。该方法执行原地更新，类似于 Gauss-Seidel 方法。状态按固定顺序进行迭代，例如 $s = 0, 1, \\dots, n-1$。在计算状态 $s$ 的新价值时，算法会使用所有其他状态的最新计算值。具体来说，对于状态 $s'  s$，使用当前迭代 $k$ 的新值；而对于状态 $s' \\ge s$，使用迭代 $k-1$ 的旧值。迭代 $k$ 的扫描中的更新规则是：\n$$\nv(s) \\leftarrow \\max_{a \\in \\mathcal{A}} \\left( r(s,a) + \\gamma \\sum_{s'=0}^{s-1} P(s' \\mid s,a) v^{(k)}(s') + \\gamma \\sum_{s'=s}^{n-1} P(s' \\mid s,a) v^{(k-1)}(s') \\right).\n$$\n这很自然地通过原地更新价值向量来实现。Gauss-Seidel 式的更新通常比其同步对应方法收敛得更快。\n\n接下来两种方法求解用于策略评估的线性方程组 $(I - \\gamma P_\\pi) v_\\pi = r_\\pi$。这可以重写为不动点方程 $v_\\pi = r_\\pi + \\gamma P_\\pi v_\\pi$。\n\n第三种算法是**同步策略评估**，这正是应用于该系统的 **Jacobi 方法**。从 $v^{(0)}=0$ 开始，迭代方案是：\n$$\nv^{(k)} = r_\\pi + \\gamma P_\\pi v^{(k-1)}.\n$$\n按分量表示为：\n$$\nv^{(k)}(s) = r(s, \\pi(s)) + \\gamma \\sum_{s' \\in \\mathcal{S}} P(s' \\mid s, \\pi(s)) v^{(k-1)}(s').\n$$\n与同步价值迭代一样，需要保留前一次迭代 $v^{(k-1)}$ 的一个完整副本。\n\n第四种算法是 **Gauss-Seidel 策略评估**。这将 **Gauss-Seidel 方法**应用于策略评估系统。更新是原地执行的。对于固定顺序 $s=0, 1, \\dots, n-1$ 中的每个状态 $s$，更新公式为：\n$$\nv(s) \\leftarrow r(s, \\pi(s)) + \\gamma \\sum_{s' \\in \\mathcal{S}} P(s' \\mid s, \\pi(s)) v(s').\n$$\n右侧的求和隐式地使用了在当前扫描中已更新状态的新值和尚未更新状态的旧值。\n\n对于所有四种方法，迭代将持续进行，直到价值向量的变化足够小，即由无穷范数度量的变化满足：$\\lVert v^{(k)} - v^{(k-1)} \\rVert_\\infty \\le \\varepsilon$，其中给定的容差 $\\varepsilon = 10^{-8}$。满足此条件所需的迭代次数 $k$ 将被记录在每种方法和每个测试用例中。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef synchronous_value_iteration(P, R, gamma, tol):\n    \"\"\"\n    Computes the optimal value function using synchronous value iteration (Jacobi-style).\n\n    Args:\n        P (np.ndarray): Transition probability tensor P[s, a, s'].\n        R (np.ndarray): Reward matrix R[s, a].\n        gamma (float): Discount factor.\n        tol (float): Convergence tolerance.\n\n    Returns:\n        int: Number of iterations to converge.\n    \"\"\"\n    n_states, _ = R.shape\n    v = np.zeros(n_states)\n    k = 0\n    while True:\n        k += 1\n        v_prev = v.copy()\n        \n        # Vectorized computation of Q-values for all state-action pairs\n        q_values = R + gamma * (P @ v_prev)  # Shape (n_states, n_actions)\n        v = np.max(q_values, axis=1)        # Shape (n_states,)\n        \n        delta = np.max(np.abs(v - v_prev))\n        if delta = tol:\n            return k\n\ndef gauss_seidel_value_iteration(P, R, gamma, tol):\n    \"\"\"\n    Computes the optimal value function using Gauss-Seidel-style value iteration.\n\n    Args:\n        P (np.ndarray): Transition probability tensor P[s, a, s'].\n        R (np.ndarray): Reward matrix R[s, a].\n        gamma (float): Discount factor.\n        tol (float): Convergence tolerance.\n\n    Returns:\n        int: Number of iterations to converge.\n    \"\"\"\n    n_states, _ = R.shape\n    v = np.zeros(n_states)\n    k = 0\n    while True:\n        k += 1\n        v_prev = v.copy()\n        \n        for s in range(n_states):\n            # The mat-vec product uses the current state of v, which includes\n            # in-place updates from the current sweep for s_prime  s.\n            q_values_s = R[s, :] + gamma * (P[s, :, :] @ v)\n            v[s] = np.max(q_values_s)\n        \n        delta = np.max(np.abs(v - v_prev))\n        if delta = tol:\n            return k\n\ndef synchronous_policy_evaluation(P, R, pi, gamma, tol):\n    \"\"\"\n    Evaluates a fixed policy using the synchronous Jacobi method.\n\n    Args:\n        P (np.ndarray): Transition probability tensor P[s, a, s'].\n        R (np.ndarray): Reward matrix R[s, a].\n        pi (np.ndarray): Fixed policy vector.\n        gamma (float): Discount factor.\n        tol (float): Convergence tolerance.\n\n    Returns:\n        int: Number of iterations to converge.\n    \"\"\"\n    n_states = len(pi)\n    \n    # Construct r_pi and P_pi from the policy\n    s_indices = np.arange(n_states)\n    r_pi = R[s_indices, pi]\n    P_pi = P[s_indices, pi, :]\n    \n    v = np.zeros(n_states)\n    k = 0\n    while True:\n        k += 1\n        v_prev = v.copy()\n        v = r_pi + gamma * (P_pi @ v_prev)\n        \n        delta = np.max(np.abs(v - v_prev))\n        if delta = tol:\n            return k\n\ndef gauss_seidel_policy_evaluation(P, R, pi, gamma, tol):\n    \"\"\"\n    Evaluates a fixed policy using the Gauss-Seidel method.\n\n    Args:\n        P (np.ndarray): Transition probability tensor P[s, a, s'].\n        R (np.ndarray): Reward matrix R[s, a].\n        pi (np.ndarray): Fixed policy vector.\n        gamma (float): Discount factor.\n        tol (float): Convergence tolerance.\n\n    Returns:\n        int: Number of iterations to converge.\n    \"\"\"\n    n_states = len(pi)\n    \n    # Construct r_pi and P_pi from the policy\n    s_indices = np.arange(n_states)\n    r_pi = R[s_indices, pi]\n    P_pi = P[s_indices, pi, :]\n\n    v = np.zeros(n_states)\n    k = 0\n    while True:\n        k += 1\n        v_prev = v.copy()\n        \n        for s in range(n_states):\n            # In-place update using the current state of v\n            v[s] = r_pi[s] + gamma * (P_pi[s, :] @ v)\n\n        delta = np.max(np.abs(v - v_prev))\n        if delta = tol:\n            return k\n\ndef solve():\n    \"\"\"\n    Sets up and runs the test cases, then prints the results.\n    \"\"\"\n    tol = 1e-8\n\n    # --- Test Case 1 ---\n    # n=3, m=2, gamma=0.9\n    P1 = np.zeros((3, 2, 3))\n    P1[0, 0, 0] = 0.5; P1[0, 0, 1] = 0.5\n    P1[1, 0, 1] = 0.5; P1[1, 0, 2] = 0.5\n    P1[2, 0, 2] = 1.0\n    P1[0, 1, 0] = 0.7; P1[0, 1, 1] = 0.3\n    P1[1, 1, 0] = 0.4; P1[1, 1, 2] = 0.6\n    P1[2, 1, 1] = 1.0\n    R1 = np.array([[5., 4.], [0., 1.], [0., 2.]])\n    gamma1 = 0.9\n    pi1 = np.array([0, 1, 0])\n\n    # --- Test Case 2 ---\n    # n=3, m=2, gamma=0.99 (same P and R as Test 1)\n    P2 = P1\n    R2 = R1\n    gamma2 = 0.99\n    pi2 = pi1\n    \n    # --- Test Case 3 ---\n    # n=2, m=2, gamma=0.95\n    P3 = np.zeros((2, 2, 2))\n    P3[0, 0, 1] = 1.0\n    P3[1, 0, 1] = 1.0\n    P3[0, 1, 0] = 1.0\n    P3[1, 1, 1] = 1.0\n    R3 = np.array([[1., 0.], [0., 0.]])\n    gamma3 = 0.95\n    pi3 = np.array([1, 0])\n\n    test_cases = [\n        (P1, R1, gamma1, pi1),\n        (P2, R2, gamma2, pi2),\n        (P3, R3, gamma3, pi3),\n    ]\n\n    results = []\n    for P, R, gamma, pi in test_cases:\n        N_opt_sync = synchronous_value_iteration(P, R, gamma, tol)\n        N_opt_GS = gauss_seidel_value_iteration(P, R, gamma, tol)\n        N_eval_Jacobi = synchronous_policy_evaluation(P, R, pi, gamma, tol)\n        N_eval_GS = gauss_seidel_policy_evaluation(P, R, pi, gamma, tol)\n        results.extend([N_opt_sync, N_opt_GS, N_eval_Jacobi, N_eval_GS])\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```", "id": "3245192"}, {"introduction": "在从经验中学习时，Q学习算法可能会因对带噪声的价值估计取最大值而产生系统性的高估，这一现象被称为“最大化偏差”（maximization bias）。本练习将引导你通过第一性原理，在一个简化的场景中精确推导出这种偏差的数学表达式。你还将进一步分析双重Q学习（Double Q-learning）如何通过解耦动作选择和价值评估来有效消除这一偏差，从而深刻理解现代强化学习算法设计的精妙之处。[@problem_id:3145285]", "problem": "考虑一个马尔可夫决策过程（MDP），它有一个非终止状态 $s$ 和两个可用动作 $a_{1}$ 和 $a_{2}$。真实的动作价值为 $q^{\\ast}(s,a_{1})=\\mu_{1}$ 和 $q^{\\ast}(s,a_{2})=\\mu_{2}$，其中 $\\mu_{1}\\geq \\mu_{2}$。假设一个表格型智能体执行单步时序差分自举，其中目标值中的唯一误差来源是在带噪声的、无偏的价值估计上使用贪心算子。具体来说，智能体当前的估计值为 $\\hat{q}(s,a_{i})=\\mu_{i}+\\varepsilon_{i}$，其中 $\\varepsilon_{1}$ 和 $\\varepsilon_{2}$ 是独立同分布的，服从 $\\mathcal{N}(0,\\sigma^{2})$。在这一步的目标值中不涉及额外的奖励或折扣因子；重点完全在于由带噪声估计的贪心最大化所引起的偏差。\n\n1. 使用Q学习目标 $T_{Q}(s)=\\max\\{\\hat{q}(s,a_{1}),\\hat{q}(s,a_{2})\\}$ 的定义，从第一性原理推导期望偏差 $b_{Q}=\\mathbb{E}[T_{Q}(s)]-\\mu_{1}$ 的解析表达式，用 $\\Delta=\\mu_{1}-\\mu_{2}$ 和 $\\sigma$ 表示。\n\n2. 现在考虑双重Q学习（DQL），它维护两个独立的无偏估计器 $Q^{A}$ 和 $Q^{B}$，两者具有相同的噪声模型：$\\hat{q}^{A}(s,a_{i})=\\mu_{i}+\\varepsilon^{A}_{i}$ 和 $\\hat{q}^{B}(s,a_{i})=\\mu_{i}+\\varepsilon^{B}_{i}$，其中所有噪声都是独立同分布的，服从 $\\mathcal{N}(0,\\sigma^{2})$。双重Q学习的目标是 $T_{DQ}(s)=\\hat{q}^{B}(s,\\arg\\max_{a}\\hat{q}^{A}(s,a))$。推导期望偏差 $b_{DQ}=\\mathbb{E}[T_{DQ}(s)]-\\mu_{1}$ 的解析表达式，用 $\\Delta$ 和 $\\sigma$ 表示。\n\n3. 通过计算 $\\Delta b=b_{Q}-b_{DQ}$ 来量化由双重Q学习带来的偏差减少量，并将其表示为关于 $\\Delta$、$\\sigma$ 和标准正态累积分布函数 $\\Phi$ 的单个闭式解析表达式。你的最终答案必须是这个单一表达式。将 $\\Phi(x)$ 定义为标准正态分布的累积分布函数。不要提供数值近似。", "solution": "该问题是适定的且有科学依据，代表了强化学习中最大化偏差的标准理论分析。我们将按要求分三部分进行推导。\n\n设定涉及一个单一状态 $s$ 和两个动作 $a_1, a_2$，其真实价值为 $q^{\\ast}(s,a_{1})=\\mu_{1}$ 和 $q^{\\ast}(s,a_{2})=\\mu_{2}$，其中 $\\mu_{1}\\geq \\mu_{2}$。智能体的价值估计是带噪声且无偏的。对于一个通用动作 $a_i$，其估计值为 $\\hat{q}(s,a_{i})=\\mu_{i}+\\varepsilon_{i}$，其中 $\\varepsilon_{i} \\sim \\mathcal{N}(0,\\sigma^{2})$ 是独立同分布（i.i.d.）的随机变量。我们定义 $\\Delta = \\mu_{1}-\\mu_{2} \\geq 0$。我们分别用 $\\phi(x)$ 和 $\\Phi(x)$ 表示标准正态分布 $\\mathcal{N}(0,1)$ 的概率密度函数（PDF）和累积分布函数（CDF）。\n\n第1部分：Q学习的期望偏差, $b_{Q}$\n\nQ学习的目标由 $T_{Q}(s)=\\max\\{\\hat{q}(s,a_{1}),\\hat{q}(s,a_{2})\\}$ 给出。\n期望偏差定义为 $b_{Q}=\\mathbb{E}[T_{Q}(s)]-\\mu_{1}$，其中 $\\mu_1$ 是真实最优动作的价值。\n$b_{Q} = \\mathbb{E}[\\max\\{\\mu_{1}+\\varepsilon_{1}, \\mu_{2}+\\varepsilon_{2}\\}] - \\mu_{1}$。\n我们使用恒等式 $\\max(x,y) = x + \\max(0, y-x)$。\n$\\mathbb{E}[\\max\\{\\mu_{1}+\\varepsilon_{1}, \\mu_{2}+\\varepsilon_{2}\\}] = \\mathbb{E}[\\mu_{1}+\\varepsilon_{1} + \\max\\{0, (\\mu_{2}+\\varepsilon_{2}) - (\\mu_{1}+\\varepsilon_{1})\\}]$。\n根据期望的线性性质，且由于 $\\mathbb{E}[\\varepsilon_{1}]=0$：\n$\\mathbb{E}[T_{Q}(s)] = \\mu_{1} + \\mathbb{E}[\\max\\{0, (\\mu_{2}-\\mu_{1}) + (\\varepsilon_{2}-\\varepsilon_{1})\\}]$。\n因此，偏差为：\n$b_{Q} = \\mathbb{E}[\\max\\{0, -\\Delta + (\\varepsilon_{2}-\\varepsilon_{1})\\}]$。\n设一个新的随机变量为 $W = \\varepsilon_{2}-\\varepsilon_{1}$。由于 $\\varepsilon_1$ 和 $\\varepsilon_2$ 是独立同分布的 $\\mathcal{N}(0, \\sigma^2)$，$W$ 也服从正态分布。\n$\\mathbb{E}[W] = \\mathbb{E}[\\varepsilon_{2}] - \\mathbb{E}[\\varepsilon_{1}] = 0 - 0 = 0$。\n$\\text{Var}(W) = \\text{Var}(\\varepsilon_{2}) + \\text{Var}(\\varepsilon_{1}) = \\sigma^2 + \\sigma^2 = 2\\sigma^2$，因为独立性。\n所以，$W \\sim \\mathcal{N}(0, 2\\sigma^2)$。\n设 $Z = W - \\Delta$。那么 $Z \\sim \\mathcal{N}(-\\Delta, 2\\sigma^2)$。偏差为 $b_Q = \\mathbb{E}[\\max\\{0, Z\\}]$。\n这是一个右删失正态变量的期望。对于一个随机变量 $Y \\sim \\mathcal{N}(\\mu_Y, \\sigma_Y^2)$，其公式为 $\\mathbb{E}[\\max\\{0,Y\\}] = \\mu_Y \\Phi(\\frac{\\mu_Y}{\\sigma_Y}) + \\sigma_Y \\phi(\\frac{\\mu_Y}{\\sigma_Y})$。\n在我们的例子中，$\\mu_Y = -\\Delta$ 且 $\\sigma_Y = \\sqrt{2\\sigma^2} = \\sigma\\sqrt{2}$。\n将这些代入公式：\n$b_{Q} = (-\\Delta) \\Phi\\left(\\frac{-\\Delta}{\\sigma\\sqrt{2}}\\right) + \\sigma\\sqrt{2} \\phi\\left(\\frac{-\\Delta}{\\sigma\\sqrt{2}}\\right)$。\n使用恒等式 $\\Phi(-x) = 1-\\Phi(x)$ 和 $\\phi(-x) = \\phi(x)$：\n$b_{Q} = -\\Delta(1-\\Phi(\\frac{\\Delta}{\\sigma\\sqrt{2}})) + \\sigma\\sqrt{2}\\phi(\\frac{\\Delta}{\\sigma\\sqrt{2}})$。\n\n第2部分：双重Q学习的期望偏差, $b_{DQ}$\n\n双重Q学习使用两个独立的估计器 $Q^A$ 和 $Q^B$，其估计值为 $\\hat{q}^{A}(s,a_{i})=\\mu_{i}+\\varepsilon^{A}_{i}$ 和 $\\hat{q}^{B}(s,a_{i})=\\mu_{i}+\\varepsilon^{B}_{i}$，其中所有噪声项 $\\varepsilon^{A}_i, \\varepsilon^{B}_i$ 都是独立同分布的 $\\mathcal{N}(0,\\sigma^{2})$。\n目标是 $T_{DQ}(s)=\\hat{q}^{B}(s,a^{\\ast})$，其中 $a^{\\ast} = \\arg\\max_{a}\\hat{q}^{A}(s,a)$。\n期望偏差是 $b_{DQ} = \\mathbb{E}[T_{DQ}(s)] - \\mu_1$。\n我们通过以动作选择为条件来计算期望，该选择仅依赖于A估计值。\n$\\mathbb{E}[T_{DQ}(s)] = \\mathbb{E}_{A,B}[\\hat{q}^{B}(s, a^{\\ast})] = \\mathbb{E}_{A}[\\mathbb{E}_{B}[\\hat{q}^{B}(s, a^{\\ast}) | a^{\\ast}]]$。\n由于 $a^{\\ast}$ 是由 $\\varepsilon^A$ 噪声决定的，它与 $\\varepsilon^B$ 噪声无关。对B的内部期望得出所选动作的B估计值的真实均值：\n$\\mathbb{E}_{B}[\\hat{q}^{B}(s, a^{\\ast}) | a^{\\ast}] = \\mu_{a^{\\ast}}$。\n所以，$\\mathbb{E}[T_{DQ}(s)] = \\mathbb{E}_{A}[\\mu_{a^{\\ast}}]$。\n期望是关于 $a^{\\ast}$ 的选择：\n$\\mathbb{E}_{A}[\\mu_{a^{\\ast}}] = P(a^{\\ast}=a_1)\\mu_1 + P(a^{\\ast}=a_2)\\mu_2$。\n让我们求概率 $P(a^{\\ast}=a_1)$。\n$P(a^{\\ast}=a_1) = P(\\hat{q}^{A}(s,a_1) \\geq \\hat{q}^{A}(s,a_2)) = P(\\mu_1+\\varepsilon^{A}_1 \\geq \\mu_2+\\varepsilon^{A}_2)$。\n$P(a^{\\ast}=a_1) = P(\\varepsilon^{A}_1 - \\varepsilon^{A}_2 \\geq \\mu_2 - \\mu_1) = P(\\varepsilon^{A}_1 - \\varepsilon^{A}_2 \\geq -\\Delta)$。\n随机变量 $W' = \\varepsilon^{A}_1 - \\varepsilon^{A}_2$ 服从分布 $\\mathcal{N}(0, 2\\sigma^2)$。\n$P(W' \\geq -\\Delta) = P\\left(\\frac{W'}{\\sigma\\sqrt{2}} \\geq \\frac{-\\Delta}{\\sigma\\sqrt{2}}\\right)$。 设 $U = W'/(\\sigma\\sqrt{2}) \\sim \\mathcal{N}(0,1)$。\n$P(a^{\\ast}=a_1) = P(U \\geq \\frac{-\\Delta}{\\sigma\\sqrt{2}}) = 1 - \\Phi(\\frac{-\\Delta}{\\sigma\\sqrt{2}}) = \\Phi(\\frac{\\Delta}{\\sigma\\sqrt{2}})$。\n并且 $P(a^{\\ast}=a_2) = 1 - P(a^{\\ast}=a_1) = 1 - \\Phi(\\frac{\\Delta}{\\sigma\\sqrt{2}})$。\n现在，将这些概率代回到 $\\mathbb{E}[T_{DQ}(s)]$ 的表达式中：\n$\\mathbb{E}[T_{DQ}(s)] = \\mu_1 \\Phi(\\frac{\\Delta}{\\sigma\\sqrt{2}}) + \\mu_2 (1-\\Phi(\\frac{\\Delta}{\\sigma\\sqrt{2}}))$。\n偏差是：\n$b_{DQ} = \\mathbb{E}[T_{DQ}(s)] - \\mu_1 = \\mu_1 \\Phi(\\dots) + \\mu_2 (1-\\Phi(\\dots)) - \\mu_1$。\n$b_{DQ} = (\\Phi(\\dots) - 1)\\mu_1 + (1-\\Phi(\\dots))\\mu_2 = (1-\\Phi(\\dots))(\\mu_2-\\mu_1) = -(1-\\Phi(\\frac{\\Delta}{\\sigma\\sqrt{2}}))\\Delta$。\n\n第3部分：偏差减少量, $\\Delta b$\n\n偏差减少量是差值 $\\Delta b = b_{Q} - b_{DQ}$。\n$b_{Q} = -\\Delta\\left(1-\\Phi\\left(\\frac{\\Delta}{\\sigma\\sqrt{2}}\\right)\\right) + \\sigma\\sqrt{2}\\phi\\left(\\frac{\\Delta}{\\sigma\\sqrt{2}}\\right)$。\n$b_{DQ} = -\\Delta\\left(1-\\Phi\\left(\\frac{\\Delta}{\\sigma\\sqrt{2}}\\right)\\right)$。\n从 $b_{Q}$ 中减去 $b_{DQ}$：\n$\\Delta b = \\left[-\\Delta\\left(1-\\Phi\\left(\\frac{\\Delta}{\\sigma\\sqrt{2}}\\right)\\right) + \\sigma\\sqrt{2}\\phi\\left(\\frac{\\Delta}{\\sigma\\sqrt{2}}\\right)\\right] - \\left[-\\Delta\\left(1-\\Phi\\left(\\frac{\\Delta}{\\sigma\\sqrt{2}}\\right)\\right)\\right]$。\n包含 $\\Delta$ 和 $\\Phi$ 的项完全抵消。\n$\\Delta b = \\sigma\\sqrt{2}\\phi\\left(\\frac{\\Delta}{\\sigma\\sqrt{2}}\\right)$。\n问题要求最终答案用 $\\Delta$、$\\sigma$ 和 $\\Phi$ 表示。然而，严谨的推导表明CDF项 $\\Phi$ 被抵消，只留下一个依赖于PDF $\\phi$ 的表达式。为了给出一个自包含的闭式表达式，我们代入标准正态PDF的定义，$\\phi(x) = \\frac{1}{\\sqrt{2\\pi}}\\exp(-\\frac{x^2}{2})$。\n$\\Delta b = \\sigma\\sqrt{2} \\cdot \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{1}{2}\\left(\\frac{\\Delta}{\\sigma\\sqrt{2}}\\right)^2\\right)$。\n$\\Delta b = \\frac{\\sigma\\sqrt{2}}{\\sqrt{2}\\sqrt{\\pi}} \\exp\\left(-\\frac{\\Delta^2}{2 \\cdot 2\\sigma^2}\\right)$。\n$\\Delta b = \\frac{\\sigma}{\\sqrt{\\pi}} \\exp\\left(-\\frac{\\Delta^2}{4\\sigma^2}\\right)$。\n这是偏差减少量的最终解析表达式。由于精确抵消，它是 $\\Delta$ 和 $\\sigma$ 的函数，但不是 $\\Phi$ 的函数。这是唯一正确的最终表达式。", "answer": "$$\\boxed{\\frac{\\sigma}{\\sqrt{\\pi}} \\exp\\left(-\\frac{\\Delta^2}{4\\sigma^2}\\right)}$$", "id": "3145285"}, {"introduction": "在许多现实应用中，我们无法直接在线测试新策略，而是需要利用已有策略（行为策略）收集的数据来评估新策略（目标策略）的性能，这就是所谓的离策略评估（Off-Policy Evaluation）。本练习将指导你实现一个强大且实用的估计算法——双重鲁棒（Doubly Robust）估计。通过结合基于模型的预测和重要性采样修正，该方法能够在模型或重要性权重任一正确时提供无偏估计，是现代策略评估工具箱中的关键一环。[@problem_id:3145244]", "problem": "您将处理三个在马尔可夫决策过程（MDP）设置下的离策略评估任务。任务目标是实现一个双重鲁棒估计器，该估计器结合了基于模型的价值估计和重要性采样修正。这项工作必须从概率论和马尔可夫性质的基本原理推导得出。您的程序必须为每个测试用例计算三个量：逐决策重要性采样估计、目标策略下基于模型的价值估计，以及双重鲁棒估计。您的程序必须输出一行，其中包含一个列表的列表，每个内部列表是对应一个测试用例的三个浮点数，并按指定顺序排列。\n\n需要使用的基本原理：\n- 一个马尔可夫决策过程（MDP）由一个有限状态空间 $S$、一个有限动作空间 $A$、一个转移核 $P(s' \\mid s,a)$、一个即时奖励函数 $r(s,a)$、一个折扣因子 $\\gamma \\in (0,1]$、一个有限时域 $H \\in \\mathbb{N}$ 和一个描述决策规则的策略 $\\pi(a \\mid s)$ 组成。\n- 马尔可夫性质指出，对于每个时间步 $t$，有 $P(s_{t+1} \\mid s_{0:t}, a_{0:t}) = P(s_{t+1} \\mid s_t, a_t)$ 和 $r_t = r(s_t,a_t)$。\n- 在策略 $\\pi$ 下，从一个初始状态分布 $d_0(s)$ 开始的期望折扣回报为 $V^\\pi = \\mathbb{E}_{s_0 \\sim d_0}\\left[\\sum_{t=0}^{H-1} \\gamma^t r(s_t,a_t)\\right]$，其中 $(s_t,a_t)$ 遵循由 $\\pi$ 控制的 MDP 动态。\n- 重要性采样（IS）基于恒等式 $\\mathbb{E}_p[f(X)] = \\mathbb{E}_q\\left[f(X)\\frac{p(X)}{q(X)}\\right]$，当 $p$ 相对于 $q$ 绝对连续时成立。此方法逐步应用于轨迹，以校正行为策略和目标策略之间的不匹配。\n- 对观测到的转移进行最大似然估计（MLE），可用于构建一个基于模型的估计 $\\hat{P}(s' \\mid s,a)$ 和 $\\hat{r}(s,a)$，这些估计可以导出用于计算基于模型的价值函数的动态规划递归。\n\n您的实现要求：\n- 对每个测试用例，使用提供的行为轨迹，通过最大似然估计来估计模型 $\\hat{P}(s' \\mid s,a)$。对每个状态-动作对的下一状态计数使用加一拉普拉斯平滑，以确保概率有良好定义。对于即时奖励估计器 $\\hat{r}(s,a)$，使用每个对 $(s,a)$ 观测到的奖励的经验平均值；如果某个对 $(s,a)$ 从未被观测到，则设置 $\\hat{r}(s,a) = 0$。\n- 通过在有限时域 $H$ 上进行动态规划，计算目标策略 $\\pi$ 下的基于模型的价值，记为 $\\hat{V}^\\pi$。使用由每个测试用例的初始状态数据集导出的经验初始状态分布 $d_0$ 来评估期望回报：$\\hat{V}^\\pi = \\sum_{s \\in S} d_0(s)\\hat{V}_0(s)$，其中 $\\hat{V}_t(s)$ 通过递归 $\\hat{Q}_t(s,a) = \\hat{r}(s,a) + \\gamma \\sum_{s' \\in S} \\hat{P}(s' \\mid s,a)\\hat{V}_{t+1}(s')$ 和 $\\hat{V}_t(s) = \\sum_{a \\in A} \\pi(a \\mid s)\\hat{Q}_t(s,a)$ 计算，终端边界条件为对所有 $s \\in S$，$\\hat{V}_H(s)=0$。\n- 使用行为轨迹和逐步似然比 $\\rho_{0:t} = \\prod_{k=0}^{t} \\frac{\\pi(a_k \\mid s_k)}{b(a_k \\mid s_k)}$ 来计算逐决策重要性采样（IS）估计器，其中 $b(a \\mid s)$ 是给定测试用例的行为策略。逐决策 IS 估计是所有片段的经逐步校正的折扣奖励总和的平均值。\n- 实现一个双重鲁棒（DR）估计器，该估计器使用上面计算的基于模型的 $\\hat{Q}_t$ 和 $\\hat{V}_t$ 作为控制变量，并将其与逐决策重要性权重相结合。其构造必须遵循以下原则：如果重要性权重正确或模型正确，则估计器是无偏的。使用基于相同的 $\\rho_{0:t}$ 权重和上面定义的动态规划 $\\hat{Q}_t$ 和 $\\hat{V}_t$ 的标准逐步组合；不引入裁剪。\n- 所有计算必须仅基于提供的轨迹和策略；不允许使用外部数据或用户输入。\n\n测试套件：\n对于每个测试用例，状态空间为 $S = \\{0,1,2\\}$，动作空间为 $A = \\{0,1\\}$。片段是固定长度的，并以元组序列 $(s_t,a_t,r_t,s_{t+1})$ 的形式给出，其中 $t=0,\\dots,H-1$。策略 $b(a \\mid s)$ 和 $\\pi(a \\mid s)$ 按状态指定为动作上的概率分布。\n\n- 测试用例 1（正常路径）：\n    - 折扣因子：$\\gamma = 0.95$。\n    - 时域：$H=3$。\n    - 行为策略 $b$：\n        - $b(0 \\mid 0) = 0.7$, $b(1 \\mid 0) = 0.3$。\n        - $b(0 \\mid 1) = 0.4$, $b(1 \\mid 1) = 0.6$。\n        - $b(0 \\mid 2) = 0.5$, $b(1 \\mid 2) = 0.5$。\n    - 目标策略 $\\pi$：\n        - $\\pi(0 \\mid 0) = 0.2$, $\\pi(1 \\mid 0) = 0.8$。\n        - $\\pi(0 \\mid 1) = 0.6$, $\\pi(1 \\mid 1) = 0.4$。\n        - $\\pi(0 \\mid 2) = 0.3$, $\\pi(1 \\mid 2) = 0.7$。\n    - 片段（每个长度为 $H=3$）：\n        1. $(0,0,1.0,1),(1,1,0.5,2),(2,1,1.2,2)$\n        2. $(0,1,1.5,2),(2,1,1.0,2),(2,0,0.7,1)$\n        3. $(1,0,0.8,0),(0,1,1.1,2),(2,1,1.0,2)$\n        4. $(2,0,0.9,1),(1,0,0.6,0),(0,1,1.4,2)$\n        5. $(1,1,0.4,2),(2,1,1.3,2),(2,0,0.5,1)$\n        6. $(0,0,1.1,1),(1,1,0.6,2),(2,1,1.0,2)$\n- 测试用例 2（边界条件：行为策略等于目标策略）：\n    - 折扣因子：$\\gamma = 0.95$。\n    - 时域：$H=3$。\n    - 行为策略 $b$ 和目标策略 $\\pi$ 相同：\n        - $(0 \\mid 0) = 0.5$, $(1 \\mid 0) = 0.5$。\n        - $(0 \\mid 1) = 0.2$, $(1 \\mid 1) = 0.8$。\n        - $(0 \\mid 2) = 0.6$, $(1 \\mid 2) = 0.4$。\n    - 片段（每个长度为 $H=3$）：\n        1. $(0,0,1.0,1),(1,1,1.2,2),(2,0,0.4,1)$\n        2. $(1,1,0.7,2),(2,0,0.6,0),(0,1,1.1,2)$\n        3. $(2,0,0.9,1),(1,1,0.8,2),(2,1,1.0,2)$\n        4. $(0,1,1.4,2),(2,0,0.5,0),(0,1,1.2,2)$\n        5. $(2,0,0.6,0),(0,1,1.0,2),(2,0,0.7,1)$\n- 测试用例 3（边缘情况：不带裁剪的大重要性权重）：\n    - 折扣因子：$\\gamma = 0.95$。\n    - 时域：$H=4$。\n    - 行为策略 $b$：\n        - $b(0 \\mid 0) = 0.8$, $b(1 \\mid 0) = 0.2$。\n        - $b(0 \\mid 1) = 0.3$, $b(1 \\mid 1) = 0.7$。\n        - $b(0 \\mid 2) = 0.7$, $b(1 \\mid 2) = 0.3$。\n    - 目标策略 $\\pi$：\n        - $\\pi(0 \\mid 0) = 0.1$, $\\pi(1 \\mid 0) = 0.9$。\n        - $\\pi(0 \\mid 1) = 0.85$, $\\pi(1 \\mid 1) = 0.15$。\n        - $\\pi(0 \\mid 2) = 0.1$, $\\pi(1 \\mid 2) = 0.9$。\n    - 片段（每个长度为 $H=4$）：\n        1. $(0,0,0.9,1),(1,1,0.3,2),(2,0,0.5,0),(0,1,1.0,2)$\n        2. $(1,1,0.6,2),(2,1,0.9,2),(2,0,0.4,1),(1,0,0.7,0)$\n        3. $(2,0,0.8,0),(0,0,0.9,1),(1,1,0.6,2),(2,1,1.2,2)$\n        4. $(0,1,1.3,2),(2,0,0.5,1),(1,0,0.6,0),(0,1,0.9,2)$\n\n输出规范：\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表。每个测试用例贡献一个内部列表，包含三个浮点数，顺序为 $[\\text{IS}, \\hat{V}^\\pi, \\text{DR}]$。因此，整体输出必须类似于 $[[x_{11},x_{12},x_{13}],[x_{21},x_{22},x_{23}],[x_{31},x_{32},x_{33}]]$，其中每个 $x_{ij}$ 都是一个浮点数。\n\n额外限制：\n- 离策略评估（OPE）、重要性采样（IS）、双重鲁棒（DR）和最大似然估计（MLE）的缩写在您的解决方案中首次使用时必须明确定义。\n- 角度单位或物理单位在此处不相关；请勿包含它们。\n- 不允许用户输入。所有数据均已在上方提供，并且必须嵌入到解决方案中。", "solution": "该问题要求在有限时域马尔可夫决策过程（MDP）中，实现并比较三种用于离策略评估（OPE）的估计器。离策略评估（OPE）是指利用在一种策略（行为策略 $b$）下收集的数据，来估计一个新策略（目标策略 $\\pi$）的期望回报的任务。我们需要计算 $\\pi$ 的价值，使用以下方法：\n1.  一个逐决策重要性采样（IS）估计器。\n2.  一个基于模型的估计器，该模型通过最大似然估计（MLE）学习得到。\n3.  一个双重鲁棒（DR）估计器，它结合了前两种方法。\n\n我们将首先根据所提供的规范，正式定义每个估计器的组成部分和推导过程。状态空间为 $S = \\{0, 1, 2\\}$，动作空间为 $A = \\{0, 1\\}$，时域 $H$ 和折扣因子 $\\gamma$ 因测试用例而异。数据以 $N$ 条轨迹集合的形式提供，其中每条轨迹 $i$ 是一个转移序列 $\\tau^{(i)} = \\{(s_t^{(i)}, a_t^{(i)}, r_t^{(i)}, s_{t+1}^{(i)})\\}_{t=0}^{H-1}$。\n\n**1. 基于模型的估计**\n\n基于模型的方法包括两个阶段：首先，从数据中学习 MDP 的模型；其次，使用此模型计算目标策略 $\\pi$ 的价值。\n\n**1.1. 模型学习**\n我们从所有轨迹的聚合数据集中估计转移概率 $\\hat{P}(s' \\mid s, a)$ 和奖励函数 $\\hat{r}(s, a)$。\n-   **转移模型 $\\hat{P}$**：我们使用带加一（拉普拉斯）平滑的最大似然估计（MLE）。令 $N(s,a,s')$ 为在状态 $s$ 采取动作 $a$ 后转移到状态 $s'$ 的观测次数。令 $N(s,a) = \\sum_{s' \\in S} N(s,a,s')$ 为状态-动作对 $(s,a)$ 的总计数。平滑后的概率为：\n    $$\n    \\hat{P}(s' \\mid s, a) = \\frac{N(s, a, s') + 1}{\\sum_{s'' \\in S} (N(s, a, s'') + 1)} = \\frac{N(s, a, s') + 1}{N(s, a) + |S|}\n    $$\n    这种平滑确保了没有转移会被赋予零概率，这对于动态规划步骤至关重要。\n-   **奖励模型 $\\hat{r}$**：状态-动作对 $(s,a)$ 的奖励被估计为该对观测到的所有奖励的经验平均值。令 $R(s,a)$ 为在状态 $s$ 采取动作 $a$ 后观测到的奖励之和。\n    $$\n    \\hat{r}(s, a) = \\begin{cases} \\frac{R(s, a)}{N(s, a)}  \\text{if } N(s, a) > 0 \\\\ 0  \\text{if } N(s, a) = 0 \\end{cases}\n    $$\n\n**1.2. 通过动态规划计算价值**\n利用估计的模型 $(\\hat{P}, \\hat{r})$，我们可以使用动态规划计算目标策略 $\\pi$ 的状态价值函数 $\\hat{V}_t(s)$ 和状态-动作价值函数 $\\hat{Q}_t(s,a)$。递归从时域 $H$ 开始向后进行：\n-   **边界条件**：在终端时间步 $t=H$，价值为零：对所有 $s \\in S$，$\\hat{V}_H(s) = 0$。\n-   **反向递归（对 $t = H-1, \\dots, 0$）**：\n    1.  计算状态-动作价值函数 $\\hat{Q}_t(s,a)$：\n        $$\n        \\hat{Q}_t(s, a) = \\hat{r}(s, a) + \\gamma \\sum_{s' \\in S} \\hat{P}(s' \\mid s, a) \\hat{V}_{t+1}(s')\n        $$\n    2.  计算目标策略 $\\pi$ 下的状态价值函数 $\\hat{V}_t(s)$：\n        $$\n        \\hat{V}_t(s) = \\sum_{a \\in A} \\pi(a \\mid s) \\hat{Q}_t(s, a)\n        $$\n-   **最终的基于模型的估计**：策略 $\\pi$ 的总价值是在初始状态分布 $d_0$ 下，时间 $t=0$ 时的期望价值。我们使用数据集中的初始状态 $s_0^{(i)}$ 的经验分布：\n    $$\n    d_0(s) = \\frac{1}{N} \\sum_{i=1}^N \\mathbb{I}(s_0^{(i)} = s)\n    $$\n    其中 $\\mathbb{I}(\\cdot)$ 是指示函数。那么，基于模型的估计为：\n    $$\n    \\hat{V}^\\pi_{\\text{Model}} = \\sum_{s \\in S} d_0(s) \\hat{V}_0(s) = \\frac{1}{N} \\sum_{i=1}^N \\hat{V}_0(s_0^{(i)})\n    $$\n\n**2. 逐决策重要性采样（IS）估计器**\n\n重要性采样（IS）是一种在拥有来自不同分布的样本时，估计某分布性质的技术。在 OPE 中，它用于校正行为策略 $b$ 和目标策略 $\\pi$ 之间的不匹配。逐决策变体在轨迹的每一步都应用此校正。\n\n单个时间步 $t$ 的重要性权重是比率 $\\rho_t = \\frac{\\pi(a_t \\mid s_t)}{b(a_t \\mid s_t)}$。一条轨迹到时间 $t$ 的累积重要性权重是逐步权重的乘积：\n$$\n\\rho_{0:t} = \\prod_{k=0}^{t} \\rho_k = \\prod_{k=0}^{t} \\frac{\\pi(a_k \\mid s_k)}{b(a_k \\mid s_k)}\n$$\n逐决策 IS 估计器 $\\hat{V}^\\pi_{\\text{PDIS}}$ 是所有轨迹的折扣奖励总和的平均值，其中每一项都通过相应的累积重要性权重进行重新加权：\n$$\n\\hat{V}^\\pi_{\\text{PDIS}} = \\frac{1}{N} \\sum_{i=1}^N \\sum_{t=0}^{H-1} \\gamma^t \\rho_{0:t}^{(i)} r_t^{(i)}\n$$\n该估计器是无偏的，但可能会有高方差问题，特别是当策略 $\\pi$ 和 $b$ 非常不同，导致重要性权重很大时。\n\n**3. 双重鲁棒（DR）估计器**\n\n双重鲁棒（DR）估计器结合了基于模型的估计器和 IS 估计器，以利用两者的优势。它之所以是“双重鲁棒”的，是因为如果*或者*学习到的模型 $(\\hat{P}, \\hat{r})$ 是正确的，*或者*重要性权重是正确的（即行为策略 $b$ 是已知的），它就能提供对真实价值 $V^\\pi$ 的无偏估计。\n\nDR 估计器使用基于模型的估计作为控制变量来减少 IS 估计器的方差。单条轨迹 $i$ 的逐步 DR 估计器构造如下：\n$$\n\\hat{V}_{\\text{DR}}^{(i)} = \\hat{V}_0(s_0^{(i)}) + \\sum_{t=0}^{H-1} \\gamma^t \\rho_{0:t}^{(i)} \\delta_t^{(i)}\n$$\n其中 $\\hat{V}_0(s_0^{(i)})$ 是对初始状态的基于模型的价值预测，而 $\\delta_t^{(i)}$ 是模型在观测到的转移上的单步时序差分误差，定义为：\n$$\n\\delta_t^{(i)} = r_t^{(i)} + \\gamma \\hat{V}_{t+1}(s_{t+1}^{(i)}) - \\hat{Q}_t(s_t^{(i)}, a_t^{(i)})\n$$\n这里，$r_t^{(i)}$ 和 $s_{t+1}^{(i)}$ 是从轨迹中实际观测到的奖励和下一状态，而 $\\hat{V}_{t+1}$ 和 $\\hat{Q}_t$ 是通过在学习模型上进行动态规划计算出的价值函数。如果模型是完美的，$\\mathbb{E}[\\delta_t^{(i)}] = 0$，估计器就简化为基于模型的估计器。如果模型是错误的，第二项则通过重要性采样比率加权来校正模型的误差。\n\n最终的 DR 估计是所有轨迹的平均值：\n$$\n\\hat{V}^\\pi_{\\text{DR}} = \\frac{1}{N} \\sum_{i=1}^N \\hat{V}_{\\text{DR}}^{(i)}\n$$\n该估计器通常比 IS 估计器具有低得多的方差，同时保留了在比基于模型的估计器更弱的条件下是无偏的特性。\n\n**算法流程总结**\n对于每个测试用例，计算过程如下：\n1.  为策略、轨迹和 MDP 参数（$|S|$, $|A|$, $\\gamma$, $H$）初始化数据结构。\n2.  **模型估计**：\n    -   解析轨迹数据以计算计数 $N(s,a,s')$、$N(s,a)$ 和奖励总和 $R(s,a)$。\n    -   使用拉普拉斯平滑计算转移模型 $\\hat{P}$，使用经验平均值计算奖励模型 $\\hat{r}$。\n3.  **基于模型的评估**：\n    -   初始化 $\\hat{V}_H(s) = 0$。\n    -   从 $H-1$ 向下迭代到 $0$ 来计算所有的 $\\hat{Q}_t$ 和 $\\hat{V}_t$ 数组。\n    -   通过对数据集中的初始状态 $s_0$ 的 $\\hat{V}_0(s_0)$ 取平均值，来计算 $\\hat{V}^\\pi_{\\text{Model}}$。\n4.  **IS 和 DR 估计**：\n    -   将 IS 和 DR 估计的总和初始化为零。\n    -   对于数据集中的每条轨迹 $i$：\n        -   初始化累积重要性权重 $\\rho_{prod} = 1.0$，以及用于 IS 和 DR 的每条轨迹的总和。\n        -   轨迹 $i$ 的 DR 总和以基于模型的价值 $\\hat{V}_0(s_0^{(i)})$ 开始。\n        -   从 $t=0$ 迭代到 $H-1$：\n            -   更新 $\\rho_{prod} \\leftarrow \\rho_{prod} \\times \\frac{\\pi(a_t^{(i)} \\mid s_t^{(i)})}{b(a_t^{(i)} \\mid s_t^{(i)})}$。\n            -   将 $\\gamma^t \\rho_{prod} r_t^{(i)}$ 添加到轨迹 $i$ 的 IS 总和中。\n            -   计算 $\\delta_t^{(i)} = r_t^{(i)} + \\gamma \\hat{V}_{t+1}(s_{t+1}^{(i)}) - \\hat{Q}_t(s_t^{(i)}, a_t^{(i)})$。\n            -   将 $\\gamma^t \\rho_{prod} \\delta_t^{(i)}$ 添加到轨迹 $i$ 的 DR 总和中。\n        -   将完成的每条轨迹的总和添加到 IS 和 DR 的总和中。\n5.  **最终估计**：\n    -   将总和除以轨迹数量 $N$ 以获得最终的 $\\hat{V}^\\pi_{\\text{PDIS}}$ 和 $\\hat{V}^\\pi_{\\text{DR}}$ 估计值。\n6.  收集当前测试用例的三个估计值 $[\\hat{V}^\\pi_{\\text{PDIS}}, \\hat{V}^\\pi_{\\text{Model}}, \\hat{V}^\\pi_{\\text{DR}}]$。\n7.  对所有测试用例重复上述步骤，并格式化最终输出。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the off-policy evaluation problem for the three given test cases.\n    \"\"\"\n    \n    test_cases = [\n        # Test Case 1\n        {\n            \"gamma\": 0.95,\n            \"H\": 3,\n            \"b\": np.array([[0.7, 0.3], [0.4, 0.6], [0.5, 0.5]]),\n            \"pi\": np.array([[0.2, 0.8], [0.6, 0.4], [0.3, 0.7]]),\n            \"episodes\": [\n                [(0, 0, 1.0, 1), (1, 1, 0.5, 2), (2, 1, 1.2, 2)],\n                [(0, 1, 1.5, 2), (2, 1, 1.0, 2), (2, 0, 0.7, 1)],\n                [(1, 0, 0.8, 0), (0, 1, 1.1, 2), (2, 1, 1.0, 2)],\n                [(2, 0, 0.9, 1), (1, 0, 0.6, 0), (0, 1, 1.4, 2)],\n                [(1, 1, 0.4, 2), (2, 1, 1.3, 2), (2, 0, 0.5, 1)],\n                [(0, 0, 1.1, 1), (1, 1, 0.6, 2), (2, 1, 1.0, 2)],\n            ],\n        },\n        # Test Case 2\n        {\n            \"gamma\": 0.95,\n            \"H\": 3,\n            \"b\": np.array([[0.5, 0.5], [0.2, 0.8], [0.6, 0.4]]),\n            \"pi\": np.array([[0.5, 0.5], [0.2, 0.8], [0.6, 0.4]]),\n            \"episodes\": [\n                [(0, 0, 1.0, 1), (1, 1, 1.2, 2), (2, 0, 0.4, 1)],\n                [(1, 1, 0.7, 2), (2, 0, 0.6, 0), (0, 1, 1.1, 2)],\n                [(2, 0, 0.9, 1), (1, 1, 0.8, 2), (2, 1, 1.0, 2)],\n                [(0, 1, 1.4, 2), (2, 0, 0.5, 0), (0, 1, 1.2, 2)],\n                [(2, 0, 0.6, 0), (0, 1, 1.0, 2), (2, 0, 0.7, 1)],\n            ],\n        },\n        # Test Case 3\n        {\n            \"gamma\": 0.95,\n            \"H\": 4,\n            \"b\": np.array([[0.8, 0.2], [0.3, 0.7], [0.7, 0.3]]),\n            \"pi\": np.array([[0.1, 0.9], [0.85, 0.15], [0.1, 0.9]]),\n            \"episodes\": [\n                [(0, 0, 0.9, 1), (1, 1, 0.3, 2), (2, 0, 0.5, 0), (0, 1, 1.0, 2)],\n                [(1, 1, 0.6, 2), (2, 1, 0.9, 2), (2, 0, 0.4, 1), (1, 0, 0.7, 0)],\n                [(2, 0, 0.8, 0), (0, 0, 0.9, 1), (1, 1, 0.6, 2), (2, 1, 1.2, 2)],\n                [(0, 1, 1.3, 2), (2, 0, 0.5, 1), (1, 0, 0.6, 0), (0, 1, 0.9, 2)],\n            ]\n        }\n    ]\n\n    all_results = []\n    \n    for case in test_cases:\n        gamma = case[\"gamma\"]\n        H = case[\"H\"]\n        b = case[\"b\"]\n        pi = case[\"pi\"]\n        episodes = case[\"episodes\"]\n        num_episodes = len(episodes)\n        \n        S_size = b.shape[0]\n        A_size = b.shape[1]\n\n        # 1. Model Estimation (MLE with Laplace smoothing)\n        transition_counts = np.zeros((S_size, A_size, S_size))\n        reward_sums = np.zeros((S_size, A_size))\n        sa_counts = np.zeros((S_size, A_size))\n\n        for ep in episodes:\n            for t, (s, a, r, s_next) in enumerate(ep):\n                transition_counts[s, a, s_next] += 1\n                reward_sums[s, a] += r\n                sa_counts[s, a] += 1\n        \n        P_hat = np.zeros((S_size, A_size, S_size))\n        r_hat = np.zeros((S_size, A_size))\n\n        for s in range(S_size):\n            for a in range(A_size):\n                if sa_counts[s, a]  0:\n                    r_hat[s, a] = reward_sums[s, a] / sa_counts[s, a]\n                    # Laplace smoothing for P_hat\n                    P_hat[s, a, :] = (transition_counts[s, a, :] + 1) / (sa_counts[s, a] + S_size)\n                else:\n                    # If (s,a) is not observed, r_hat is 0 and P_hat is uniform (from smoothing)\n                    r_hat[s, a] = 0.0\n                    P_hat[s, a, :] = 1.0 / S_size\n\n        # 2. Model-based value estimation (Dynamic Programming)\n        V_hat = np.zeros((H + 1, S_size))\n        Q_hat = np.zeros((H, S_size, A_size))\n\n        for t in range(H - 1, -1, -1):\n            V_next = V_hat[t + 1, :]\n            expected_V_next = np.sum(P_hat * V_next, axis=2) # Shape: (S_size, A_size)\n            Q_hat[t, :, :] = r_hat + gamma * expected_V_next\n            V_hat[t, :] = np.sum(pi * Q_hat[t, :, :], axis=1)\n\n        initial_states = [ep[0][0] for ep in episodes]\n        V_model_based = np.mean([V_hat[0, s0] for s0 in initial_states])\n        \n        # 3. IS and DR estimation\n        total_is_return = 0.0\n        total_dr_return = 0.0\n        \n        for i in range(num_episodes):\n            ep = episodes[i]\n            s0 = ep[0][0]\n            \n            # Per-decision IS\n            episode_is_return = 0.0\n            rho_product = 1.0\n            for t in range(H):\n                s, a, r, s_next = ep[t]\n                rho_t = pi[s, a] / b[s, a]\n                rho_product *= rho_t\n                episode_is_return += (gamma**t) * rho_product * r\n            total_is_return += episode_is_return\n\n            # Doubly Robust\n            episode_dr_return = V_hat[0, s0]\n            rho_product = 1.0\n            for t in range(H):\n                s, a, r, s_next = ep[t]\n                rho_t = pi[s, a] / b[s, a]\n                rho_product *= rho_t\n                \n                delta_t = r + gamma * V_hat[t + 1, s_next] - Q_hat[t, s, a]\n                episode_dr_return += (gamma**t) * rho_product * delta_t\n            total_dr_return += episode_dr_return\n            \n        V_is = total_is_return / num_episodes\n        V_dr = total_dr_return / num_episodes\n        \n        all_results.append([V_is, V_model_based, V_dr])\n\n    # Format output as a list of lists of floats\n    print(f\"{all_results}\")\n\nsolve()\n```", "id": "3145244"}]}