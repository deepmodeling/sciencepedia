{"hands_on_practices": [{"introduction": "策略梯度方法的核心思想是调整策略参数，以增加能够带来更高回报的动作的概率。本练习将引导你从第一性原理出发，为标准的 softmax 策略推导这一关键的梯度表达式。掌握这一推导过程，对于理解策略梯度算法的工作原理和为学习更高级的技术打下坚实基础至关重要。[@problem_id:3157980]", "problem": "一个有限动作随机决策问题拥有一个大小为 $K$ 的动作集，标记为 $\\{1,2,\\dots,K\\}$。一个平稳随机策略以概率 $\\pi_{\\theta}(i)$ 选择动作 $i$，其中参数 $\\theta \\in \\mathbb{R}^{K}$ 是无约束的，并通过 softmax 变换映射到概率单纯形\n$$\n\\pi_{\\theta}(i) \\;=\\; \\frac{\\exp(\\theta_{i})}{\\sum_{j=1}^{K} \\exp(\\theta_{j})} \\quad \\text{for } i \\in \\{1,2,\\dots,K\\}.\n$$\n对于每个动作 $i$，奖励 $R(i)$ 是一个具有有限均值 $\\mu_{i} = \\mathbb{E}[R(i)]$ 的随机变量。考虑期望回报\n$$\nJ(\\theta) \\;=\\; \\mathbb{E}_{a \\sim \\pi_{\\theta}}[R(a)] \\;=\\; \\sum_{i=1}^{K} \\pi_{\\theta}(i)\\,\\mu_{i}.\n$$\n假设梯度和期望的交换由标准正则性条件保证，并且所有对象都是良定义的。\n\n任务：\n1. 仅从上述定义出发，推导梯度 $\\nabla_{\\theta} J(\\theta)$ 的一个显式闭式表达式，该表达式用 $\\{\\pi_{\\theta}(i)\\}_{i=1}^{K}$ 和 $\\{\\mu_{i}\\}_{i=1}^{K}$ 表示，不引入任何额外的参数化或约束。\n2. 使用你的表达式，分析当 $\\theta$ 的一个分量 $\\theta_{k} \\to +\\infty$ 而所有其他分量保持固定时，$\\nabla_{\\theta} J(\\theta)$ 会发生什么。说明梯度在此极限下是否消失，并从第一性原理证明你的结论。\n3. 将问题具体化为 $K=3$，奖励均值 $\\mu = (\\mu_{1},\\mu_{2},\\mu_{3}) = (1,0,-1)$，参数 $\\theta = (0,\\ln 2,-\\ln 2)$。精确计算梯度向量 $\\nabla_{\\theta} J(\\theta)$。\n\n答案格式要求：使用 LaTeX pmatrix 环境，将特定情况下的最终梯度向量以单行形式提供。最终的方框答案中不要包含任何单位或文本。不需要四舍五入。", "solution": "该问题是适定的，在统计学习理论中有其科学依据，并包含了得到唯一解所需的所有信息。我们按要求进行推导和分析。\n\n### 第1部分：梯度 $\\nabla_{\\theta} J(\\theta)$ 的推导\n\n要最大化的目标函数是期望回报，由下式给出：\n$$\nJ(\\theta) = \\mathbb{E}_{a \\sim \\pi_{\\theta}}[R(a)] = \\sum_{i=1}^{K} \\pi_{\\theta}(i)\\,\\mu_{i}\n$$\n其中 $\\mu_i = \\mathbb{E}[R(i)]$ 是动作 $i$ 的平均奖励，而 $\\pi_{\\theta}(i)$ 是 softmax 策略：\n$$\n\\pi_{\\theta}(i) = \\frac{\\exp(\\theta_{i})}{\\sum_{j=1}^{K} \\exp(\\theta_{j})}\n$$\n我们想要计算 $J(\\theta)$ 关于参数向量 $\\theta$ 的梯度，记作 $\\nabla_{\\theta} J(\\theta)$。该梯度的第 $k$ 个分量是偏导数 $\\frac{\\partial J(\\theta)}{\\partial \\theta_k}$。\n\n从 $J(\\theta)$ 的定义开始，我们对 $\\theta_k$ 求导：\n$$\n\\frac{\\partial J(\\theta)}{\\partial \\theta_k} = \\frac{\\partial}{\\partial \\theta_k} \\sum_{i=1}^{K} \\pi_{\\theta}(i)\\,\\mu_{i} = \\sum_{i=1}^{K} \\mu_{i} \\frac{\\partial \\pi_{\\theta}(i)}{\\partial \\theta_k}\n$$\n由于求和是有限的，微分和求和的交换是合理的。\n\n为了计算 softmax 函数 $\\pi_{\\theta}(i)$ 的导数，我们可以使用“对数导数技巧”，即 $\\nabla \\pi = \\pi \\nabla \\ln \\pi$。我们来计算 $\\ln \\pi_{\\theta}(i)$ 的导数：\n$$\n\\ln \\pi_{\\theta}(i) = \\ln\\left(\\frac{\\exp(\\theta_i)}{\\sum_{j=1}^{K} \\exp(\\theta_j)}\\right) = \\theta_i - \\ln\\left(\\sum_{j=1}^{K} \\exp(\\theta_j)\\right)\n$$\n现在，我们对该表达式关于 $\\theta_k$ 求导：\n$$\n\\frac{\\partial}{\\partial \\theta_k} \\ln \\pi_{\\theta}(i) = \\frac{\\partial \\theta_i}{\\partial \\theta_k} - \\frac{\\frac{\\partial}{\\partial \\theta_k} \\sum_{j=1}^{K} \\exp(\\theta_j)}{\\sum_{j=1}^{K} \\exp(\\theta_j)}\n$$\n$\\theta_i$ 关于 $\\theta_k$ 的导数是克罗内克δ函数 $\\delta_{ik}$。求和项的导数是：\n$$\n\\frac{\\partial}{\\partial \\theta_k} \\sum_{j=1}^{K} \\exp(\\theta_j) = \\exp(\\theta_k)\n$$\n将此代回，我们得到：\n$$\n\\frac{\\partial}{\\partial \\theta_k} \\ln \\pi_{\\theta}(i) = \\delta_{ik} - \\frac{\\exp(\\theta_k)}{\\sum_{j=1}^{K} \\exp(\\theta_j)} = \\delta_{ik} - \\pi_{\\theta}(k)\n$$\n使用对数导数技巧 $\\frac{\\partial \\pi_{\\theta}(i)}{\\partial \\theta_k} = \\pi_{\\theta}(i) \\frac{\\partial \\ln \\pi_{\\theta}(i)}{\\partial \\theta_k}$，我们发现：\n$$\n\\frac{\\partial \\pi_{\\theta}(i)}{\\partial \\theta_k} = \\pi_{\\theta}(i) (\\delta_{ik} - \\pi_{\\theta}(k))\n$$\n这给出了两种情况：\n1. 如果 $i=k$：$\\frac{\\partial \\pi_{\\theta}(k)}{\\partial \\theta_k} = \\pi_{\\theta}(k) (1 - \\pi_{\\theta}(k))$\n2. 如果 $i \\neq k$：$\\frac{\\partial \\pi_{\\theta}(i)}{\\partial \\theta_k} = \\pi_{\\theta}(i) (0 - \\pi_{\\theta}(k)) = -\\pi_{\\theta}(i) \\pi_{\\theta}(k)$\n\n现在我们将这个结果代回到 $J(\\theta)$ 梯度的表达式中：\n$$\n\\frac{\\partial J(\\theta)}{\\partial \\theta_k} = \\sum_{i=1}^{K} \\mu_{i} \\left[ \\pi_{\\theta}(i) (\\delta_{ik} - \\pi_{\\theta}(k)) \\right]\n$$\n我们可以展开这个和式：\n$$\n\\frac{\\partial J(\\theta)}{\\partial \\theta_k} = \\mu_k \\pi_{\\theta}(k) (1 - \\pi_{\\theta}(k)) + \\sum_{i \\neq k} \\mu_i (-\\pi_{\\theta}(i) \\pi_{\\theta}(k))\n$$\n提出公因子 $\\pi_{\\theta}(k)$：\n$$\n\\frac{\\partial J(\\theta)}{\\partial \\theta_k} = \\pi_{\\theta}(k) \\left[ \\mu_k (1 - \\pi_{\\theta}(k)) - \\sum_{i \\neq k} \\mu_i \\pi_{\\theta}(i) \\right]\n$$\n$$\n\\frac{\\partial J(\\theta)}{\\partial \\theta_k} = \\pi_{\\theta}(k) \\left[ \\mu_k - \\mu_k \\pi_{\\theta}(k) - \\sum_{i \\neq k} \\mu_i \\pi_{\\theta}(i) \\right]\n$$\n认识到括号内的项几乎构成了 $J(\\theta)$ 的和式：\n$$\n\\frac{\\partial J(\\theta)}{\\partial \\theta_k} = \\pi_{\\theta}(k) \\left[ \\mu_k - \\left( \\mu_k \\pi_{\\theta}(k) + \\sum_{i \\neq k} \\mu_i \\pi_{\\theta}(i) \\right) \\right]\n$$\n括号中的项恰好是 $\\sum_{i=1}^{K} \\mu_i \\pi_{\\theta}(i) = J(\\theta)$。因此，我们得到了这个优雅的闭式表达式：\n$$\n\\frac{\\partial J(\\theta)}{\\partial \\theta_k} = \\pi_{\\theta}(k) (\\mu_k - J(\\theta))\n$$\n完整的梯度向量 $\\nabla_{\\theta} J(\\theta)$ 是一个向量，其第 $k$ 个分量由该表达式给出。\n\n### 第2部分：当 $\\theta_k \\to +\\infty$ 时的极限分析\n\n我们需要分析当一个分量（比如 $\\theta_k$）趋向于 $+\\infty$ 而所有其他分量 $\\theta_j$ ($j \\neq k$) 保持固定时，梯度 $\\nabla_{\\theta} J(\\theta)$ 的行为。\n\n首先，我们来分析策略概率 $\\pi_{\\theta}(i)$ 和期望回报 $J(\\theta)$ 的极限。\n选择动作 $k$ 的概率是：\n$$\n\\pi_{\\theta}(k) = \\frac{\\exp(\\theta_k)}{\\sum_{j=1}^{K} \\exp(\\theta_j)} = \\frac{\\exp(\\theta_k)}{\\exp(\\theta_k) + \\sum_{j \\neq k} \\exp(\\theta_j)}\n$$\n将分子和分母同除以 $\\exp(\\theta_k)$：\n$$\n\\pi_{\\theta}(k) = \\frac{1}{1 + \\sum_{j \\neq k} \\exp(\\theta_j - \\theta_k)}\n$$\n当 $\\theta_k \\to +\\infty$ 时，对于任何固定的 $\\theta_j$ ($j \\neq k$)，项 $\\theta_j - \\theta_k \\to -\\infty$。因此，$\\exp(\\theta_j - \\theta_k) \\to 0$。\n因此，$\\pi_{\\theta}(k)$ 的极限是：\n$$\n\\lim_{\\theta_k \\to +\\infty} \\pi_{\\theta}(k) = \\frac{1}{1 + 0} = 1\n$$\n对于任何其他动作 $i \\neq k$，其概率是：\n$$\n\\pi_{\\theta}(i) = \\frac{\\exp(\\theta_i)}{\\sum_{j=1}^{K} \\exp(\\theta_j)} = \\frac{\\exp(\\theta_i - \\theta_k)}{1 + \\sum_{j \\neq k} \\exp(\\theta_j - \\theta_k)}\n$$\n当 $\\theta_k \\to +\\infty$ 时，分子 $\\exp(\\theta_i - \\theta_k) \\to 0$，分母趋向于 $1$。因此：\n$$\n\\lim_{\\theta_k \\to +\\infty} \\pi_{\\theta}(i) = 0 \\quad \\text{for } i \\neq k\n$$\n这表明策略变为确定性的，以概率 $1$ 选择动作 $k$。\n\n接下来，我们求期望回报 $J(\\theta)$ 的极限：\n$$\n\\lim_{\\theta_k \\to +\\infty} J(\\theta) = \\lim_{\\theta_k \\to +\\infty} \\sum_{i=1}^{K} \\pi_{\\theta}(i)\\mu_i = \\left(\\lim_{\\theta_k \\to +\\infty} \\pi_{\\theta}(k)\\right)\\mu_k + \\sum_{i \\neq k} \\left(\\lim_{\\theta_k \\to +\\infty} \\pi_{\\theta}(i)\\right)\\mu_i\n$$\n$$\n\\lim_{\\theta_k \\to +\\infty} J(\\theta) = (1)\\mu_k + \\sum_{i \\neq k} (0)\\mu_i = \\mu_k\n$$\n期望回报收敛于确定性选择的动作 $k$ 的平均奖励。\n\n现在我们可以分析梯度向量 $\\nabla_{\\theta} J(\\theta)$ 的每个分量的极限。令 $g_i = (\\nabla_{\\theta} J(\\theta))_i = \\pi_{\\theta}(i)(\\mu_i - J(\\theta))$。\n\n对于梯度的第 $k$ 个分量：\n$$\n\\lim_{\\theta_k \\to +\\infty} g_k = \\lim_{\\theta_k \\to +\\infty} \\pi_{\\theta}(k)(\\mu_k - J(\\theta)) = \\left(\\lim_{\\theta_k \\to +\\infty} \\pi_{\\theta}(k)\\right) \\left(\\mu_k - \\lim_{\\theta_k \\to +\\infty} J(\\theta)\\right)\n$$\n$$\n\\lim_{\\theta_k \\to +\\infty} g_k = (1) (\\mu_k - \\mu_k) = 0\n$$\n对于任何其他分量 $i \\neq k$：\n$$\n\\lim_{\\theta_k \\to +\\infty} g_i = \\lim_{\\theta_k \\to +\\infty} \\pi_{\\theta}(i)(\\mu_i - J(\\theta)) = \\left(\\lim_{\\theta_k \\to +\\infty} \\pi_{\\theta}(i)\\right) \\left(\\mu_i - \\lim_{\\theta_k \\to +\\infty} J(\\theta)\\right)\n$$\n$$\n\\lim_{\\theta_k \\to +\\infty} g_i = (0) (\\mu_i - \\mu_k) = 0\n$$\n由于梯度向量的所有分量都趋向于 $0$，梯度 $\\nabla_{\\theta} J(\\theta)$ 在此极限下消失。这是因为 softmax 函数达到饱和，使得策略概率对 logit 的进一步增加不敏感，从而“拉平”了目标函数的地形。\n\n### 第3部分：特定情况的计算\n\n给定 $K=3$，奖励均值 $\\mu = (1, 0, -1)$，参数 $\\theta = (0, \\ln 2, -\\ln 2)$。\n\n首先，我们计算 logit 的指数：\n- $\\exp(\\theta_1) = \\exp(0) = 1$\n- $\\exp(\\theta_2) = \\exp(\\ln 2) = 2$\n- $\\exp(\\theta_3) = \\exp(-\\ln 2) = \\exp(\\ln(2^{-1})) = \\frac{1}{2}$\n\n这些指数的和是：\n$$\nS = \\sum_{j=1}^{3} \\exp(\\theta_j) = 1 + 2 + \\frac{1}{2} = 3.5 = \\frac{7}{2}\n$$\n接下来，我们计算动作概率 $\\pi_{\\theta}(i) = \\frac{\\exp(\\theta_i)}{S}$：\n- $\\pi_{\\theta}(1) = \\frac{1}{7/2} = \\frac{2}{7}$\n- $\\pi_{\\theta}(2) = \\frac{2}{7/2} = \\frac{4}{7}$\n- $\\pi_{\\theta}(3) = \\frac{1/2}{7/2} = \\frac{1}{7}$\n概率之和为 $\\frac{2}{7}+\\frac{4}{7}+\\frac{1}{7} = \\frac{7}{7} = 1$，符合预期。\n\n现在，我们计算期望回报 $J(\\theta)$：\n$$\nJ(\\theta) = \\sum_{i=1}^{3} \\pi_{\\theta}(i)\\mu_i = \\left(\\frac{2}{7}\\right)(1) + \\left(\\frac{4}{7}\\right)(0) + \\left(\\frac{1}{7}\\right)(-1) = \\frac{2}{7} - \\frac{1}{7} = \\frac{1}{7}\n$$\n最后，我们使用公式 $(\\nabla_{\\theta} J(\\theta))_k = \\pi_{\\theta}(k)(\\mu_k - J(\\theta))$ 计算梯度向量的每个分量：\n\n- 分量 1 ($k=1$)：\n$$\n(\\nabla_{\\theta} J(\\theta))_1 = \\pi_{\\theta}(1)(\\mu_1 - J(\\theta)) = \\frac{2}{7}\\left(1 - \\frac{1}{7}\\right) = \\frac{2}{7}\\left(\\frac{6}{7}\\right) = \\frac{12}{49}\n$$\n- 分量 2 ($k=2$)：\n$$\n(\\nabla_{\\theta} J(\\theta))_2 = \\pi_{\\theta}(2)(\\mu_2 - J(\\theta)) = \\frac{4}{7}\\left(0 - \\frac{1}{7}\\right) = \\frac{4}{7}\\left(-\\frac{1}{7}\\right) = -\\frac{4}{49}\n$$\n- 分量 3 ($k=3$)：\n$$\n(\\nabla_{\\theta} J(\\theta))_3 = \\pi_{\\theta}(3)(\\mu_3 - J(\\theta)) = \\frac{1}{7}\\left(-1 - \\frac{1}{7}\\right) = \\frac{1}{7}\\left(-\\frac{8}{7}\\right) = -\\frac{8}{49}\n$$\n梯度向量为 $\\nabla_{\\theta} J(\\theta) = \\left(\\frac{12}{49}, -\\frac{4}{49}, -\\frac{8}{49}\\right)$。", "answer": "$$\\boxed{\\begin{pmatrix} \\frac{12}{49}  -\\frac{4}{49}  -\\frac{8}{49} \\end{pmatrix}}$$", "id": "3157980"}, {"introduction": "尽管基础的 REINFORCE 算法在理论上是正确的，但其梯度估计值往往存在高方差问题，导致学习过程缓慢或不稳定。本练习通过引入基线（baseline）这一常见的方差降低技术，来解决这个关键问题。你将推导并实现最优基线系数，从而亲身体验如何让策略梯度方法变得更加实用和高效。[@problem_id:3163430]", "problem": "考虑一个分幕式强化学习（RL）设定，其中有一个由 $\\theta$ 参数化的随机策略，记为 $\\pi_{\\theta}(a \\mid s)$。令 $G_t$ 为在时间 $t$ 针对状态 $s_t$ 和动作 $a_t$ 观测到的回报，令 $z_t = \\nabla_{\\theta} \\log \\pi_{\\theta}(a_t \\mid s_t)$ 为得分函数（也称为策略的对数导数）。定义一个仅依赖于状态 $s$ 的基线函数 $b(s)$。一种常见的方差缩减技术是使用形式为 $b(s) = \\alpha \\hat{V}(s)$ 的控制变量基线，其中 $\\hat{V}(s)$ 是状态值函数的估计值，$\\alpha \\in \\mathbb{R}$ 是一个待确定的标量。那么，在时间 $t$ 的梯度估计量变为 $g_t(\\alpha) = \\left(G_t - \\alpha \\hat{V}(s_t)\\right) z_t$。\n\n从核心定义和经过充分检验的事实（包括策略梯度的对数导数技巧和方差缩减中的控制变量原理）出发，推导一个选择最优标量 $\\alpha$ 的过程，该 $\\alpha$ 能够最小化从有限样本集 $\\{(G_i, \\hat{V}(s_i), z_i)\\}_{i=1}^{n}$ 构建的梯度估计量的经验方差。您必须使用一个方差准则，该准则对于向量值随机变量而言是一个数学上良定义的标量度量。对于本问题，使用经验协方差的迹作为方差度量，即，将梯度估计量的样本方差计算为 $\\frac{1}{n-1} \\sum_{i=1}^{n} \\left\\| g_i(\\alpha) - \\bar{g}(\\alpha) \\right\\|_2^2$，其中 $\\bar{g}(\\alpha) = \\frac{1}{n} \\sum_{i=1}^{n} g_i(\\alpha)$ 是该估计量的样本均值，$\\|\\cdot\\|_2$ 是欧几里得范数。您的推导除了所提供的定义之外，不得假设任何特殊的分布形式，并且必须从控制变量和得分函数的基本性质出发。\n\n实现一个完整的程序，该程序：\n- 通过最小化经验二阶矩 $\\sum_{i=1}^{n} \\left\\| g_i(\\alpha) \\right\\|_2^2$ 来从提供的样本中计算经验最优标量 $\\alpha$，并理解当基线与动作无关时，梯度估计量的总体均值不依赖于基线，因此最小化此二阶矩与最小化上述指定的方差度量将得到相同的最小化器。\n- 计算不带基线（即 $\\alpha = 0$）和带最优基线（即使用经验最优 $\\alpha$）时梯度估计量的样本方差（经验协方差的迹）。\n- 处理经验最小化问题是病态的边界情况，例如当最优性条件中的分母为零时；在这种情况下，设置 $\\alpha = 0$。\n\n您的程序必须是确定性的，不需要用户输入，并使用以下包含指定数组的测试套件。每个测试用例提供 $(G, \\hat{V}, Z)$：\n1. 包含非零和零得分函数值的正常路径：\n   - $G = [1.5, 0.5, 2.0, 1.0, 0.0]$\n   - $\\hat{V} = [1.0, 0.2, 1.8, 0.9, 0.1]$\n   - $Z = \\left[\\,[0.8, -0.3],\\ [0.1, 0.2],\\ [1.2, 0.5],\\ [0.7, 0.1],\\ [0.0, 0.0]\\,\\right]$\n2. 基线估计恒为零的边界情况：\n   - $G = [2.0, 1.0, -0.5, 0.3]$\n   - $\\hat{V} = [0.0, 0.0, 0.0, 0.0]$\n   - $Z = \\left[\\,[0.5, -0.1],\\ [0.4, 0.3],\\ [0.2, 0.2],\\ [1.0, -0.5]\\,\\right]$\n3. $G$ 和 $\\hat{V}$ 之间存在负相关且得分函数为三维的情况：\n   - $G = [1.0, 2.0, -1.0, 0.0, 0.5, -0.2]$\n   - $\\hat{V} = [-1.5, -1.0, 1.0, 0.2, -0.1, 0.3]$\n   - $Z = \\left[\\,[0.3, -0.7, 0.1],\\ [0.5, 0.2, -0.4],\\ [0.8, 0.1, 0.3],\\ [0.0, 0.6, 0.6],\\ [1.0, -0.2, 0.0],\\ [0.2, 0.2, -0.1]\\,\\right]$\n\n对于每个测试用例，您的程序必须计算三元组 $[\\alpha, \\mathrm{var}_0, \\mathrm{var}_{\\alpha}]$，其中 $\\alpha$ 是经验最优标量，$\\mathrm{var}_0$ 是 $\\alpha = 0$ 时梯度估计量的样本方差，$\\mathrm{var}_{\\alpha}$ 是使用经验最优 $\\alpha$ 时的样本方差，所有值都为实数。最终输出必须是单行，包含一个用方括号括起来的逗号分隔列表，其中每个测试用例表示为一个三元素列表，例如 $\\left[\\left[\\alpha_1,\\mathrm{var}_{0,1},\\mathrm{var}_{\\alpha,1}\\right],\\left[\\alpha_2,\\mathrm{var}_{0,2},\\mathrm{var}_{\\alpha,2}\\right],\\left[\\alpha_3,\\mathrm{var}_{0,3},\\mathrm{var}_{\\alpha,3}\\right]\\right]$。不涉及物理单位，所有量都是无量纲的实数。", "solution": "该问题要求为策略梯度估计量中的基线推导一个最优标量系数，目标是最小化经验方差。我们给定一个包含 $n$ 个样本的集合 $\\{(G_i, \\hat{V}_i, z_i)\\}_{i=1}^{n}$，其中 $G_i$ 是回报，$\\hat{V}_i = \\hat{V}(s_i)$ 是状态值估计，而 $z_i = \\nabla_{\\theta} \\log \\pi_{\\theta}(a_i \\mid s_i)$ 是得分函数向量。梯度估计量为 $g_i(\\alpha) = (G_i - \\alpha \\hat{V}_i) z_i$。\n\n### 使用基线进行方差缩减的原理\n\n在策略梯度估计量 $g_t = (G_t - b(s_t)) z_t$ 中使用基线 $b(s)$ 的核心思想是在不引入偏差的情况下减少梯度估计的方差。策略梯度定理为 $\\nabla_{\\theta} J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_{\\theta}}[\\sum_t G_t z_t]$。为确保估计量保持无偏，基线项的期望必须为零。对于一个仅依赖于状态 $s_t$ 而不依赖于动作 $a_t$ 的基线 $b(s_t)$，其对期望梯度的贡献为：\n$$\n\\mathbb{E}[b(s_t) z_t] = \\mathbb{E}_{s_t} \\left[ b(s_t) \\mathbb{E}_{a_t \\sim \\pi_{\\theta}(\\cdot|s_t)}[\\nabla_{\\theta} \\log \\pi_{\\theta}(a_t|s_t)] \\right]\n$$\n内部期望是得分函数的期望，它始终为零：\n$$\n\\mathbb{E}_{a_t \\sim \\pi_{\\theta}(\\cdot|s_t)}[\\nabla_{\\theta} \\log \\pi_{\\theta}(a_t|s_t)] = \\sum_{a_t} \\pi_{\\theta}(a_t|s_t) \\frac{\\nabla_{\\theta}\\pi_{\\theta}(a_t|s_t)}{\\pi_{\\theta}(a_t|s_t)} = \\nabla_{\\theta} \\sum_{a_t} \\pi_{\\theta}(a_t|s_t) = \\nabla_{\\theta}(1) = \\mathbf{0}\n$$\n因此，$\\mathbb{E}[b(s_t) z_t] = \\mathbf{0}$，并且梯度估计量的总体均值 $\\mathbb{E}[g_t(\\alpha)]$ 与 $\\alpha$ 无关。这种无关性意味着最小化估计量的总体方差等价于最小化其总体二阶矩。\n\n### 目标函数的构建与推导\n\n问题要求找到最优的 $\\alpha$ 来最小化经验方差，该方差定义为经验协方差矩阵的迹：\n$$\n\\mathrm{Var}_{\\mathrm{emp}}(\\alpha) = \\frac{1}{n-1} \\sum_{i=1}^{n} \\| g_i(\\alpha) - \\bar{g}(\\alpha) \\|_2^2\n$$\n其中 $\\bar{g}(\\alpha) = \\frac{1}{n}\\sum_{i=1}^{n} g_i(\\alpha)$ 是样本均值。\n\n问题陈述指引我们通过最小化经验二阶矩 $\\sum_{i=1}^{n} \\| g_i(\\alpha) \\|_2^2$ 来简化此问题。这是一个常见且实用的简化，因为这个更简单目标的最小化器通常是真实经验方差最小化器的一个良好近似，尤其是在样本数量 $n$ 很大时。我们将按照指示，使用这个简化的目标进行推导。\n\n令待最小化的目标函数为 $J(\\alpha)$：\n$$\nJ(\\alpha) = \\sum_{i=1}^{n} \\| g_i(\\alpha) \\|_2^2 = \\sum_{i=1}^{n} \\| (G_i - \\alpha \\hat{V}_i) z_i \\|_2^2\n$$\n由于 $G_i$、$\\hat{V}_i$ 和 $\\alpha$ 都是标量，我们可以将它们从范数中提出来：\n$$\nJ(\\alpha) = \\sum_{i=1}^{n} (G_i - \\alpha \\hat{V}_i)^2 \\| z_i \\|_2^2\n$$\n展开平方项：\n$$\nJ(\\alpha) = \\sum_{i=1}^{n} \\left( G_i^2 - 2\\alpha G_i \\hat{V}_i + \\alpha^2 \\hat{V}_i^2 \\right) \\| z_i \\|_2^2\n$$\n这是一个关于 $\\alpha$ 的二次函数。为了找到最小值，我们计算其关于 $\\alpha$ 的导数并令其为零：\n$$\n\\frac{dJ}{d\\alpha} = \\frac{d}{d\\alpha} \\sum_{i=1}^{n} \\left( G_i^2 \\| z_i \\|_2^2 - 2\\alpha G_i \\hat{V}_i \\| z_i \\|_2^2 + \\alpha^2 \\hat{V}_i^2 \\| z_i \\|_2^2 \\right) = 0\n$$\n$$\n\\sum_{i=1}^{n} \\left( -2 G_i \\hat{V}_i \\| z_i \\|_2^2 + 2\\alpha \\hat{V}_i^2 \\| z_i \\|_2^2 \\right) = 0\n$$\n将包含 $\\alpha$ 的项和不包含 $\\alpha$ 的项分组：\n$$\n2\\alpha \\sum_{i=1}^{n} \\hat{V}_i^2 \\| z_i \\|_2^2 = 2 \\sum_{i=1}^{n} G_i \\hat{V}_i \\| z_i \\|_2^2\n$$\n求解最优的 $\\alpha$，记为 $\\alpha^*$：\n$$\n\\alpha^* = \\frac{\\sum_{i=1}^{n} G_i \\hat{V}_i \\| z_i \\|_2^2}{\\sum_{i=1}^{n} \\hat{V}_i^2 \\| z_i \\|_2^2}\n$$\n二阶导数 $\\frac{d^2J}{d\\alpha^2} = 2 \\sum_{i=1}^{n} \\hat{V}_i^2 \\| z_i \\|_2^2$ 是非负的，这证实了只要分母不为零，$\\alpha^*$ 的这个值就对应一个最小值。\n\n### 边界情况处理\n\n$\\alpha^*$ 的表达式仅在分母不为零时才是良定义的。分母 $\\sum_{i=1}^{n} \\hat{V}_i^2 \\| z_i \\|_2^2$ 是非负项之和。它为零当且仅当对于每个样本 $i$，$\\hat{V}_i = 0$ 或 $z_i = \\mathbf{0}$。在这种情况下，基线项 $\\alpha \\hat{V}_i z_i$ 对所有 $i$ 都为零，这意味着梯度估计量 $g_i(\\alpha)$ 与 $\\alpha$ 无关。目标函数 $J(\\alpha)$ 变为常数，任何 $\\alpha$ 都是一个最小化器。问题指明，在这种病态情况下，我们应设置 $\\alpha = 0$，这是一个合理的选择，因为它对应于不使用基线。\n\n### 计算步骤\n\n对于每个测试用例，实现将遵循以下步骤：\n1. 给定 $G$、$\\hat{V}$ 和 $Z$ 的数据数组，为每个得分向量 $z_i$ 计算其欧几里得范数的平方 $\\|z_i\\|_2^2$。\n2. 计算 $\\alpha^*$ 的分子，即 $\\sum_{i=1}^{n} G_i \\hat{V}_i \\| z_i \\|_2^2$。\n3. 计算 $\\alpha^*$ 的分母，即 $\\sum_{i=1}^{n} \\hat{V}_i^2 \\| z_i \\|_2^2$。\n4. 如果分母为零（或数值上接近于零），则设置 $\\alpha^* = 0$。否则，使用推导出的公式计算 $\\alpha^*$。\n5. 定义一个函数来计算样本方差 $\\mathrm{var}(\\alpha) = \\frac{1}{n-1} \\sum_{i=1}^{n} \\| g_i(\\alpha) - \\bar{g}(\\alpha) \\|_2^2$。这包括：\n    a. 为所有 $i=1, \\dots, n$ 计算梯度样本 $g_i(\\alpha) = (G_i - \\alpha \\hat{V}_i) z_i$。\n    b. 计算样本均值向量 $\\bar{g}(\\alpha) = \\frac{1}{n}\\sum_i g_i(\\alpha)$。\n    c. 对偏差的欧几里得范数平方 $\\| g_i(\\alpha) - \\bar{g}(\\alpha) \\|_2^2$ 求和，然后除以 $n-1$。\n6. 计算不使用基线时估计量的方差，$\\mathrm{var}_0 = \\mathrm{var}(\\alpha=0)$。\n7. 计算使用最优基线时估计量的方差，$\\mathrm{var}_{\\alpha} = \\mathrm{var}(\\alpha=\\alpha^*)$。\n8. 为每个测试用例收集三元组 $[\\alpha^*, \\mathrm{var}_0, \\mathrm{var}_{\\alpha}]$。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the policy gradient baseline problem for a suite of test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (\n            # 1. Happy path\n            [1.5, 0.5, 2.0, 1.0, 0.0],\n            [1.0, 0.2, 1.8, 0.9, 0.1],\n            [[0.8, -0.3], [0.1, 0.2], [1.2, 0.5], [0.7, 0.1], [0.0, 0.0]],\n        ),\n        (\n            # 2. Boundary case where V_hat is zero\n            [2.0, 1.0, -0.5, 0.3],\n            [0.0, 0.0, 0.0, 0.0],\n            [[0.5, -0.1], [0.4, 0.3], [0.2, 0.2], [1.0, -0.5]],\n        ),\n        (\n            # 3. Case with negative correlation and 3D score function\n            [1.0, 2.0, -1.0, 0.0, 0.5, -0.2],\n            [-1.5, -1.0, 1.0, 0.2, -0.1, 0.3],\n            [[0.3, -0.7, 0.1], [0.5, 0.2, -0.4], [0.8, 0.1, 0.3],\n             [0.0, 0.6, 0.6], [1.0, -0.2, 0.0], [0.2, 0.2, -0.1]],\n        ),\n    ]\n\n    def compute_optimal_baseline(G_list, V_hat_list, Z_list):\n        \"\"\"\n        Computes the optimal alpha and variances for a single test case.\n        \"\"\"\n        G = np.array(G_list, dtype=float)\n        V_hat = np.array(V_hat_list, dtype=float)\n        Z = np.array(Z_list, dtype=float)\n        n = G.shape[0]\n\n        if n == 1:\n            # Variance is not well-defined for n = 1 with ddof=1\n            return [0.0, 0.0, 0.0]\n\n        # Calculate squared L2 norm of each score vector z_i\n        z_norm_sq = np.sum(Z**2, axis=1)\n\n        # Calculate numerator and denominator for alpha_star\n        alpha_num = np.sum(G * V_hat * z_norm_sq)\n        alpha_den = np.sum(V_hat**2 * z_norm_sq)\n\n        # Handle edge case where the denominator is zero\n        if np.isclose(alpha_den, 0):\n            alpha_opt = 0.0\n        else:\n            alpha_opt = alpha_num / alpha_den\n\n        def calculate_variance(alpha):\n            \"\"\"Helper function to compute the sample variance for a given alpha.\"\"\"\n            # Calculate gradient estimator samples g_i(alpha) = (G_i - alpha*V_hat_i) * z_i\n            coeffs = G - alpha * V_hat\n            # Use broadcasting to multiply each scalar coefficient with its corresponding z vector\n            g_samples = coeffs[:, np.newaxis] * Z\n\n            # Compute sample mean of the gradient estimators\n            g_mean = np.mean(g_samples, axis=0)\n\n            # Compute deviations from the mean\n            deviations = g_samples - g_mean\n            \n            # Sum of squared L2 norms of deviations\n            sum_sq_norms = np.sum(deviations**2)\n            \n            # The sample variance is the trace of the empirical covariance matrix\n            # with denominator n-1 (ddof=1)\n            variance = sum_sq_norms / (n - 1)\n            return variance\n\n        # Calculate variance with no baseline (alpha = 0)\n        var_0 = calculate_variance(0.0)\n\n        # Calculate variance with optimal baseline\n        var_alpha = calculate_variance(alpha_opt)\n\n        return [alpha_opt, var_0, var_alpha]\n\n    results = []\n    for case in test_cases:\n        result = compute_optimal_baseline(*case)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    # The str() representation of a list of lists is almost correct.\n    # We just need to remove spaces to match the specified format.\n    print(str(results).replace(\" \", \"\"))\n\nsolve()\n```", "id": "3163430"}, {"introduction": "在理论和实践之间架起桥梁至关重要。在推导出复杂的解析梯度之后，必须验证我们的代码是否正确地实现了它们。本练习介绍梯度检查（gradient checking），这是一种强大的调试技术，它将解析梯度与使用有限差分的数值近似进行比较。通过完成此练习，你将获得一个可靠的工具来调试自己的强化学习实现。[@problem_id:3227766]", "problem": "考虑一个源于强化学习（RL）中参数化策略的可微损失，此问题纯粹以数学术语形式提出。假设有 $A = 3$ 个动作和一个参数矩阵 $\\Theta \\in \\mathbb{R}^{3 \\times 2}$，其行由动作索引，列由特征索引。对于每个状态 $s$，定义 $z_a(s) = \\Theta_{a,\\cdot}^\\top \\phi(s)$ 和 softmax 概率 $p_a(s) = \\exp(z_a(s)) \\big/ \\sum_{b=1}^{3} \\exp(z_b(s))$。在状态 $s$ 的期望奖励由 $\\sum_{a=1}^{3} p_a(s) \\, r_a(s)$ 给出，其中依赖于动作的奖励为 $r_a(s) = c_a + d_a^\\top \\phi(s)$。考虑在两个状态 $s_1$ 和 $s_2$ 上的分布，其特征向量和权重定义如下，以及一个正则化损失\n$$\nL(\\Theta) = - \\sum_{s \\in \\{s_1, s_2\\}} w(s) \\sum_{a=1}^{3} p_a(s) \\, r_a(s) + \\frac{\\lambda}{2} \\|\\Theta\\|_F^2,\n$$\n其中 $\\|\\Theta\\|_F$ 是弗罗贝尼乌斯范数，$\\lambda \\ge 0$ 是一个正则化参数。\n\n你的任务是使用有限差分估计梯度 $\\nabla L(\\Theta)$，并将其与从第一性原理推导出的解析梯度进行比较。你必须：\n- 从导数的定义和光滑函数的泰勒展开出发，为 $\\nabla L(\\Theta)$ 的每个分量构建前向差分近似和中心差分近似，将 $\\Theta$ 视为 $\\mathbb{R}^{6}$ 中的向量（通过堆叠其元素得到）。\n- 使用链式法则、指数函数的导数以及 softmax 函数的定义来推导解析梯度 $\\nabla L(\\Theta)$。不要假设任何预先给定的梯度公式；相反，应直接从上述定义推导它。\n\n使用以下具体的、科学上一致的设置：\n- 动作数量：$A = 3$。\n- 特征数量：$F = 2$。\n- 状态特征和权重：\n  - $s_1$: $\\phi(s_1) = [0.7, -1.1]$, $w(s_1) = 0.6$。\n  - $s_2$: $\\phi(s_2) = [-0.2, 0.9]$, $w(s_2) = 0.4$。\n- 奖励参数：\n  - $c = [1.1, -0.4, 0.7]$。\n  - $d_1 = [0.3, -0.4]$, $d_2 = [-0.1, 0.25]$, $d_3 = [0.05, 0.02]$, 因此 $r_a(s) = c_a + d_a^\\top \\phi(s)$。\n\n使用标量步长 $h$ 为 $\\Theta$ 的每个条目实现前向和中心有限差分近似：\n- 分量 $i$ 的前向差分：$g^{\\text{fd}}_i \\approx \\frac{L(\\Theta + h e_i) - L(\\Theta)}{h}$。\n- 分量 $i$ 的中心差分：$g^{\\text{cd}}_i \\approx \\frac{L(\\Theta + h e_i) - L(\\Theta - h e_i)}{2h}$，\n其中 $e_i$ 是 $\\mathbb{R}^{6}$ 中的第 $i$ 个标准基向量，它被映射回 $\\Theta$ 的形状。\n\n计算每个有限差分近似与解析梯度之间差值的欧几里得范数，即 $\\|g^{\\text{fd}} - \\nabla L(\\Theta)\\|_2$ 和 $\\|g^{\\text{cd}} - \\nabla L(\\Theta)\\|_2$。\n\n测试套件：\n为以下参数集提供结果，这些参数集涵盖了典型情况、对称性和正则化边界、大步长行为以及极小步长对舍入误差的敏感性：\n1. 情况 1 (正常路径)：$\\Theta = \\begin{bmatrix} 0.2  -0.1 \\\\ 0.0  0.3 \\\\ -0.25  0.15 \\end{bmatrix}$, $h = 10^{-4}$, $\\lambda = 0.05$。\n2. 情况 2 (对称策略，无正则化)：$\\Theta = \\begin{bmatrix} 0.0  0.0 \\\\ 0.0  0.0 \\\\ 0.0  0.0 \\end{bmatrix}$, $h = 10^{-4}$, $\\lambda = 0.0$。\n3. 情况 3 (大步长，强正则化)：$\\Theta = \\begin{bmatrix} 1.0  -0.5 \\\\ 0.8  0.2 \\\\ -0.6  0.3 \\end{bmatrix}$, $h = 10^{-1}$, $\\lambda = 1.2$。\n4. 情况 4 (极小步长，可能存在舍入误差)：$\\Theta = \\begin{bmatrix} -0.3  0.4 \\\\ 0.5  -0.7 \\\\ 0.1  0.2 \\end{bmatrix}$, $h = 10^{-8}$, $\\lambda = 0.05$。\n\n最终输出格式：\n你的程序应该生成单行输出，其中包含上述四个测试用例的结果，格式为由方括号括起来的逗号分隔列表，其中每个元素是该用例的配对 $[\\|g^{\\text{fd}} - \\nabla L(\\Theta)\\|_2,\\|g^{\\text{cd}} - \\nabla L(\\Theta)\\|_2]$。例如，输出必须类似于 $[[a_1,b_1],[a_2,b_2],[a_3,b_3],[a_4,b_4]]$，其中 $a_i$ 和 $b_i$ 为浮点数。不涉及物理单位或角度；所有数值输出都必须是实数。", "solution": "用户提供了一个在数值方法和科学计算领域内定义明确的数学问题，该问题应用于一个机器学习概念。任务是使用解析推导和数值有限差分近似两种方法计算给定损失函数的梯度，然后比较它们的结果。\n\n### 问题验证\n\n**第 1 步：提取已知条件**\n\n- **模型参数**：动作数量 $A = 3$，特征数量 $F = 2$。参数矩阵 $\\Theta \\in \\mathbb{R}^{3 \\times 2}$。\n- **策略定义**：动作 $a$ 的分数为 $z_a(s) = \\Theta_{a,\\cdot}^\\top \\phi(s)$。策略是动作上的 softmax 分布：$p_a(s) = \\exp(z_a(s)) \\big/ \\sum_{b=1}^{3} \\exp(z_b(s))$。\n- **奖励函数**：依赖于动作的奖励为 $r_a(s) = c_a + d_a^\\top \\phi(s)$。\n- **损失函数**：正则化损失为 $L(\\Theta) = - \\sum_{s \\in \\{s_1, s_2\\}} w(s) \\sum_{a=1}^{3} p_a(s) \\, r_a(s) + \\frac{\\lambda}{2} \\|\\Theta\\|_F^2$，其中 $\\|\\cdot\\|_F$ 是弗罗贝尼乌斯范数，$\\lambda \\ge 0$。\n- **状态数据**：\n  - $s_1$：$\\phi(s_1) = [0.7, -1.1]$, $w(s_1) = 0.6$。\n  - $s_2$：$\\phi(s_2) = [-0.2, 0.9]$, $w(s_2) = 0.4$。\n- **奖励参数**：\n  - $c = [1.1, -0.4, 0.7]$。\n  - $d_1 = [0.3, -0.4]$, $d_2 = [-0.1, 0.25]$, $d_3 = [0.05, 0.02]$。\n- **有限差分近似**：\n  - 前向差分：$g^{\\text{fd}}_i \\approx \\frac{L(\\Theta + h e_i) - L(\\Theta)}{h}$。\n  - 中心差分：$g^{\\text{cd}}_i \\approx \\frac{L(\\Theta + h e_i) - L(\\Theta - h e_i)}{2h}$。\n- **任务**：计算 $\\|g^{\\text{fd}} - \\nabla L(\\Theta)\\|_2$ 和 $\\|g^{\\text{cd}} - \\nabla L(\\Theta)\\|_2$。\n- **测试用例**：提供了四组特定的 $(\\Theta, h, \\lambda)$。\n\n**第 2 步：使用提取的已知条件进行验证**\n\n- **科学上合理**：该问题牢固地植根于标准的数学和计算原理。它结合了数值分析（有限差分）、线性代数（矩阵运算、范数）和机器学习（策略梯度、softmax 函数、正则化）中的概念。它在科学上是合理的。\n- **良构的**：损失函数 $L(\\Theta)$ 是关于 $\\Theta$ 中参数的一个光滑（无限可微）函数。因此，其梯度 $\\nabla L(\\Theta)$ 是定义明确且唯一的。计算此梯度及其数值近似的问题有一个唯一且有意义的解。\n- **客观的**：该问题使用精确的数学符号和客观的语言陈述，没有任何主观性或歧义。\n- **完整性和一致性**：所有必需的数据、函数和参数都已提供。设置中没有矛盾之处。\n- **相关性**：该问题与数值方法中导数的有限差分近似主题直接相关。\n\n**第 3 步：结论与行动**\n\n该问题是有效的。这是一个结构良好的微积分和数值计算练习。将继续进行求解。\n\n### 解析梯度推导\n\n总损失 $L(\\Theta)$ 是一个基于奖励的项和一个正则化项的和：\n$$\nL(\\Theta) = L_{rew}(\\Theta) + L_{reg}(\\Theta)\n$$\n其中\n$$\nL_{rew}(\\Theta) = - \\sum_{s \\in \\{s_1, s_2\\}} w(s) \\sum_{a=1}^{3} p_a(s) \\, r_a(s)\n\\quad \\text{和} \\quad\nL_{reg}(\\Theta) = \\frac{\\lambda}{2} \\|\\Theta\\|_F^2\n$$\n梯度是线性的，因此我们可以分别计算每个部分的梯度：$\\nabla L(\\Theta) = \\nabla L_{rew}(\\Theta) + \\nabla L_{reg}(\\Theta)$。我们求总损失关于矩阵 $\\Theta$ 的任意元素 $\\Theta_{kj}$ 的偏导数，其中 $k$ 是动作索引（$1 \\le k \\le 3$），$j$ 是特征索引（$1 \\le j \\le 2$）。\n\n**1. 正则化项的梯度**\n\n弗罗贝尼乌斯范数的平方是所有元素的平方和：$\\|\\Theta\\|_F^2 = \\sum_{a=1}^3 \\sum_{i=1}^2 \\Theta_{ai}^2$。\n偏导数是：\n$$\n\\frac{\\partial L_{reg}}{\\partial \\Theta_{kj}} = \\frac{\\partial}{\\partial \\Theta_{kj}} \\left( \\frac{\\lambda}{2} \\sum_{a=1}^3 \\sum_{i=1}^2 \\Theta_{ai}^2 \\right) = \\frac{\\lambda}{2} (2 \\Theta_{kj}) = \\lambda \\Theta_{kj}\n$$\n以矩阵形式表示，这意味着 $\\nabla L_{reg}(\\Theta) = \\lambda \\Theta$。\n\n**2. 奖励项的梯度**\n\n我们应用链式法则来求 $\\frac{\\partial L_{rew}}{\\partial \\Theta_{kj}}$。奖励 $r_a(s)$ 和权重 $w(s)$ 不依赖于 $\\Theta$。\n$$\n\\frac{\\partial L_{rew}}{\\partial \\Theta_{kj}} = - \\sum_{s} w(s) \\sum_{a=1}^{3} r_a(s) \\frac{\\partial p_a(s)}{\\partial \\Theta_{kj}}\n$$\n概率 $p_a(s)$ 通过分数 $z_b(s) = \\sum_{i=1}^2 \\Theta_{bi} \\phi_i(s)$ 依赖于 $\\Theta_{kj}$。首先，我们求分数 $z_b(s)$ 关于 $\\Theta_{kj}$ 的导数：\n$$\n\\frac{\\partial z_b(s)}{\\partial \\Theta_{kj}} = \\frac{\\partial}{\\partial \\Theta_{kj}} \\sum_{i=1}^2 \\Theta_{bi} \\phi_i(s) = \\delta_{bk} \\phi_j(s)\n$$\n其中 $\\delta_{bk}$ 是克罗内克 δ。如果动作索引 $b$ 与 $k$ 匹配，则此导数为 $\\phi_j(s)$，否则为 0。\n\n接下来，我们求 softmax 激活 $p_a(s)$ 关于分数 $z_b(s)$ 的导数。这是一个标准结果：\n$$\n\\frac{\\partial p_a(s)}{\\partial z_b(s)} = p_a(s) (\\delta_{ab} - p_b(s))\n$$\n现在，我们使用链式法则得到 $\\frac{\\partial p_a(s)}{\\partial \\Theta_{kj}}$：\n$$\n\\frac{\\partial p_a(s)}{\\partial \\Theta_{kj}} = \\sum_{b=1}^{3} \\frac{\\partial p_a(s)}{\\partial z_b(s)} \\frac{\\partial z_b(s)}{\\partial \\Theta_{kj}} = \\frac{\\partial p_a(s)}{\\partial z_k(s)} \\phi_j(s) = p_a(s) (\\delta_{ak} - p_k(s)) \\phi_j(s)\n$$\n将此代入 $\\frac{\\partial L_{rew}}{\\partial \\Theta_{kj}}$ 的表达式中：\n$$\n\\frac{\\partial L_{rew}}{\\partial \\Theta_{kj}} = - \\sum_s w(s) \\sum_{a=1}^{3} r_a(s) \\left[ p_a(s) (\\delta_{ak} - p_k(s)) \\phi_j(s) \\right]\n$$\n将不依赖于求和索引 $a$ 的项提取出来：\n$$\n= - \\sum_s w(s) \\phi_j(s) \\left[ \\sum_{a=1}^{3} r_a(s) p_a(s) (\\delta_{ak} - p_k(s)) \\right]\n$$\n展开内部对 $a$ 的求和：\n$$\n= - \\sum_s w(s) \\phi_j(s) \\left[ (r_k(s) p_k(s)) - p_k(s) \\sum_{a=1}^{3} r_a(s) p_a(s) \\right]\n$$\n令 $R(s) = \\sum_{a=1}^{3} p_a(s) r_a(s)$ 为当前策略下状态 $s$ 的期望奖励。我们可以简化表达式：\n$$\n\\frac{\\partial L_{rew}}{\\partial \\Theta_{kj}} = - \\sum_s w(s) \\phi_j(s) p_k(s) [r_k(s) - R(s)]\n$$\n这是强化学习中策略梯度方法的一种常见形式。\n\n**3. 完整的解析梯度**\n\n结合这两个部分，总损失的偏导数为：\n$$\n\\frac{\\partial L}{\\partial \\Theta_{kj}} = \\lambda \\Theta_{kj} - \\sum_{s \\in \\{s_1,s_2\\}} w(s) p_k(s) (r_k(s) - R(s)) \\phi_j(s)\n$$\n这个表达式给出了我们解析梯度矩阵 $\\nabla L(\\Theta)$ 的每个分量。在矩阵表示法中，对于每个状态 $s$，奖励项的梯度贡献是一个外积。设 $\\mathbf{p}(s)$ 为概率列向量，$\\mathbf{r}(s)$ 为奖励列向量，$\\mathbf{\\phi}(s)$ 为特征列向量。梯度为：\n$$\n\\nabla L(\\Theta) = \\lambda \\Theta - \\sum_{s \\in \\{s_1, s_2\\}} w(s) \\left( \\mathbf{p}(s) \\circ (\\mathbf{r}(s) - (\\mathbf{p}(s)^\\top \\mathbf{r}(s))\\mathbf{1}) \\right) \\mathbf{\\phi}(s)^\\top\n$$\n其中 `o` 表示逐元素乘积，$\\mathbf{1}$ 是一个全为 1 的向量。\n\n### 数值实现\n\n为解决此问题，我们使用 `numpy` 库在 Python 中实现以下组件。\n\n**1. 损失函数 `L(Theta, lambda_val)`**\n该函数实现了 $L(\\Theta)$ 的公式。对于每个状态，它计算分数 $z_a(s)$、softmax 概率 $p_a(s)$（使用一个标准的数值稳定技巧，即在进行指数运算前减去最大分数值）、奖励 $r_a(s)$ 以及特定于状态的期望奖励。将这些值与权重 $w(s)$ 相加求和，取反后，再加上弗罗贝尼乌斯范数正则化项。\n\n**2. 解析梯度 `analytical_grad(Theta, lambda_val)`**\n该函数实现了 $\\nabla L(\\Theta)$ 的推导公式。它首先计算正则化贡献 $\\lambda \\Theta$。然后，对于每个状态，它计算概率 $\\mathbf{p}(s)$、奖励 $\\mathbf{r}(s)$ 和期望奖励 $R(s)$。接着，它使用如上所述的外积构造该状态的奖励梯度贡献，并从正则化梯度中减去所有状态的加权和。\n\n**3. 有限差分梯度**\n有限差分近似是通过将 $\\Theta$ 视为一个包含 6 个元素的向量来计算的。我们遍历每个元素 $\\Theta_{kj}$。\n- **前向差分 `forward_diff_grad`**：对于每个元素，我们计算 $g^{\\text{fd}}_{kj} = (L(\\Theta + h e_{kj}) - L(\\Theta)) / h$，其中 $e_{kj}$ 是一个除了在位置 $(k,j)$ 处为 1 外其余都为零的矩阵。\n- **中心差分 `central_diff_grad`**：类似地，我们计算 $g^{\\text{cd}}_{kj} = (L(\\Theta + h e_{kj}) - L(\\Theta - h e_{kj})) / (2h)$。\n\n**4. 误差计算**\n在计算解析梯度 $\\nabla L(\\Theta)$ 和两个数值近似 $g^{\\text{fd}}$ 和 $g^{\\text{cd}}$ 之后，我们计算每种近似的误差。误差定义为展平后的梯度矩阵之间差值的欧几里得范数。这使用 `numpy.linalg.norm(approx_grad - analytical_grad)` 计算。\n\n**5. 测试用例分析**\n四个测试用例旨在探究近似方法的行为：\n- **情况 1**：一个具有适中参数的标准场景，两种近似方法都应表现良好，其中中心差分更精确。\n- **情况 2**：一个对称情况，其中 $\\Theta = \\mathbf{0}$ 且无正则化 ($\\lambda=0$)。这将 softmax 简化为均匀分布，提供了一个很好的合理性检查。\n- **情况 3**：一个大步长 $h=10^{-1}$ 和强正则化。大步长预计会显著增加近似误差，特别是对于误差阶为 $O(h)$ 的前向差分，相比之下中心差分的误差阶为 $O(h^2)$。\n- **情况 4**：一个非常小的步长 $h=10^{-8}$。这种情况测试了由于浮点精度限制导致的数值不稳定性。当 $h$ 过小时，有限差分公式分子中的减法可能会遭受灾难性抵消，与适度小的 $h$ 相比，这可能会增加误差。\n\n最终的可运行代码为所有测试用例实现了此逻辑，并按指定格式化输出。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem of comparing analytical and finite difference gradients\n    for a given reinforcement learning loss function.\n    \"\"\"\n    #\n    # --- Problem Constants ---\n    #\n    # State features and weights\n    phi_s1 = np.array([0.7, -1.1])\n    w_s1 = 0.6\n    phi_s2 = np.array([-0.2, 0.9])\n    w_s2 = 0.4\n    states = [(phi_s1, w_s1), (phi_s2, w_s2)]\n\n    # Reward parameters\n    c = np.array([1.1, -0.4, 0.7])\n    d_mat = np.array([\n        [0.3, -0.4],\n        [-0.1, 0.25],\n        [0.05, 0.02]\n    ])\n\n    #\n    # --- Helper Functions ---\n    #\n\n    def calculate_loss(Theta, lambda_val):\n        \"\"\"\n        Computes the loss L(Theta) given parameters Theta and lambda.\n        \"\"\"\n        total_reward_term = 0.0\n        for phi, w in states:\n            # Scores z_a(s) = Theta @ phi\n            z = Theta @ phi\n            \n            # Stable softmax probabilities p_a(s)\n            # Subtract max(z) for numerical stability\n            exp_z = np.exp(z - np.max(z))\n            p = exp_z / np.sum(exp_z)\n            \n            # Rewards r_a(s) = c_a + d_a^T @ phi\n            r = c + d_mat @ phi\n            \n            # Expected reward for the state\n            expected_reward = np.dot(p, r)\n            total_reward_term += w * expected_reward\n            \n        # Regularization term\n        reg_term = (lambda_val / 2.0) * np.sum(Theta**2)\n        \n        # Total loss\n        loss = -total_reward_term + reg_term\n        return loss\n\n    def analytical_grad(Theta, lambda_val):\n        \"\"\"\n        Computes the analytical gradient of L(Theta).\n        \"\"\"\n        # Gradient of the regularization term\n        grad = lambda_val * Theta\n        \n        # Sum contributions from the reward term for each state\n        for phi, w in states:\n            # Scores z_a(s)\n            z = Theta @ phi\n            \n            # Stable softmax probabilities p_a(s)\n            exp_z = np.exp(z - np.max(z))\n            p = exp_z / np.sum(exp_z)\n            \n            # Rewards r_a(s)\n            r = c + d_mat @ phi\n            \n            # Expected reward R(s)\n            R_s = np.dot(p, r)\n            \n            # Pre-factor for the state-specific gradient component\n            # This is the vector g_s with elements g_k(s) = p_k(s) * (r_k(s) - R(s))\n            g_s = p * (r - R_s)\n            \n            # Gradient contribution from reward term (outer product)\n            # Shape is (3x1) outer (1x2) - 3x2\n            state_grad_contrib = -w * np.outer(g_s, phi)\n            \n            grad += state_grad_contrib\n            \n        return grad\n\n    def forward_diff_grad(Theta, lambda_val, h):\n        \"\"\"\n        Computes the gradient using forward difference approximation.\n        \"\"\"\n        num_actions, num_features = Theta.shape\n        grad_fd = np.zeros_like(Theta)\n        base_loss = calculate_loss(Theta, lambda_val)\n        \n        for i in range(num_actions):\n            for j in range(num_features):\n                Theta_perturbed = Theta.copy()\n                Theta_perturbed[i, j] += h\n                perturbed_loss = calculate_loss(Theta_perturbed, lambda_val)\n                grad_fd[i, j] = (perturbed_loss - base_loss) / h\n                \n        return grad_fd\n\n    def central_diff_grad(Theta, lambda_val, h):\n        \"\"\"\n        Computes the gradient using central difference approximation.\n        \"\"\"\n        num_actions, num_features = Theta.shape\n        grad_cd = np.zeros_like(Theta)\n        \n        for i in range(num_actions):\n            for j in range(num_features):\n                Theta_plus = Theta.copy()\n                Theta_plus[i, j] += h\n                loss_plus = calculate_loss(Theta_plus, lambda_val)\n                \n                Theta_minus = Theta.copy()\n                Theta_minus[i, j] -= h\n                loss_minus = calculate_loss(Theta_minus, lambda_val)\n                \n                grad_cd[i, j] = (loss_plus - loss_minus) / (2.0 * h)\n                \n        return grad_cd\n\n    #\n    # --- Test Suite Execution ---\n    #\n    test_cases = [\n        (np.array([[0.2, -0.1], [0.0, 0.3], [-0.25, 0.15]]), 1e-4, 0.05),\n        (np.array([[0.0, 0.0], [0.0, 0.0], [0.0, 0.0]]), 1e-4, 0.0),\n        (np.array([[1.0, -0.5], [0.8, 0.2], [-0.6, 0.3]]), 1e-1, 1.2),\n        (np.array([[-0.3, 0.4], [0.5, -0.7], [0.1, 0.2]]), 1e-8, 0.05)\n    ]\n\n    results = []\n    for Theta_test, h_test, lambda_test in test_cases:\n        # Calculate all three gradients\n        grad_analytic = analytical_grad(Theta_test, lambda_test)\n        grad_fd = forward_diff_grad(Theta_test, lambda_test, h_test)\n        grad_cd = central_diff_grad(Theta_test, lambda_test, h_test)\n        \n        # Compute the Euclidean norm of the differences\n        # np.linalg.norm computes the Frobenius norm for matrices, which is\n        # equivalent to the Euclidean norm of the flattened vector.\n        error_fd = np.linalg.norm(grad_fd - grad_analytic)\n        error_cd = np.linalg.norm(grad_cd - grad_analytic)\n        \n        results.append([error_fd, error_cd])\n\n    # Final print statement in the exact required format.\n    # The format is [[a_1,b_1],[a_2,b_2],...]\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3227766"}]}