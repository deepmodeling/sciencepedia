{"hands_on_practices": [{"introduction": "要真正掌握价值函数和策略函数的概念，没有什么比通过一个具体的例子从头到尾计算一遍更有用了。这个练习将引导你使用动态规划中的“倒序归纳”法，在一个具有有限时间范围的马尔可夫决策过程中，一步步地计算出最优价值函数和最优策略。通过这个过程，你将亲身体会到为什么在有限期界问题中，最优决策往往与所剩时间有关，即最优策略是“非平稳的”[@problem_id:3190850]。", "problem": "考虑以下有限期马尔可夫决策过程 (MDP)，其时间范围为 $H = 3$。状态空间为 $\\mathcal{S} = \\{s, j, z\\}$。动作空间取决于状态：在状态 $s$，智能体可以选择动作 $a$ 或 $b$；在状态 $j$ 和 $z$，只有一个可用动作。单步奖励函数 $r(\\cdot,\\cdot)$ 和转移动态如下：\n- 如果当前状态为 $s$ 且选择动作 $a$，则立即奖励为 $r(s,a) = 2$，下一状态以概率 $1$ 变为 $z$。\n- 如果当前状态为 $s$ 且选择动作 $b$，则立即奖励为 $r(s,b) = 0$，下一状态以概率 $\\tfrac{1}{2}$ 变为 $j$，以概率 $\\tfrac{1}{2}$ 变为 $s$。\n- 如果当前状态为 $j$，则立即奖励为 $r(j,\\cdot) = 5$，下一状态以概率 $1$ 变为 $z$。\n- 如果当前状态为 $z$，则立即奖励为 $r(z,\\cdot) = 0$，下一状态以概率 $1$ 变为 $z$。\n\n假设每个回合恰好有 $H = 3$ 个决策阶段，并且在每个阶段 $t \\in \\{1,2,3\\}$，奖励从该阶段的当前状态-动作对中累积。设一个时间相关的策略表示为 $\\pi = \\{\\pi_{1}, \\pi_{2}, \\pi_{3}\\}$，其中 $\\pi_{t}$ 在时间 $t$ 将当前状态映射到一个动作。设策略的有限期价值函数定义为\n$$\nV_{t}^{\\pi}(x) \\equiv \\mathbb{E}^{\\pi}\\!\\left[\\sum_{k=t}^{H} r\\!\\left(S_{k}, A_{k}\\right)\\,\\middle|\\, S_{t} = x \\right],\n$$\n对于任何状态 $x \\in \\mathcal{S}$ 和时间 $t \\in \\{1,2,3\\}$。这里 $\\mathbb{E}^{\\pi}[\\cdot]$ 表示在由策略 $\\pi$、马尔可夫性质和给定的转移概率所导出的轨迹分布下的期望。\n\n仅使用这些定义、马尔可夫性质和全期望定律，完成以下任务：\n1) 对于每个时间 $t \\in \\{1,2,3\\}$，确定在最优策略下，哪个动作在状态 $s$ 处能最大化期望回报，并由此证明最优策略在整个时间范围内是稳态的还是非稳态的。\n2) 计算 $t \\in \\{1,2,3\\}$ 的最优有限期价值 $V_{t}^{\\ast}(s)$，并比较它们在不同 $t$ 时的值，解释其差异的原因。\n\n请以最简分数的形式给出 $V_{1}^{\\ast}(s)$ 的精确值作为最终答案。不要四舍五入；请提供精确值。", "solution": "该问题对一个有限期马尔可夫决策过程 (MDP) 提供了完整且一致的描述。它在科学上基于随机最优控制理论，是适定且客观的。所有必要的参数——状态空间 $\\mathcal{S}$、动作空间 $\\mathcal{A}$、奖励函数 $r$、转移概率 $P$ 和时间范围 $H$——都已明确定义。该问题是有效的，可以使用动态规划（反向归纳法）的标准方法来解决。\n\n对于一个策略 $\\pi = \\{\\pi_1, \\pi_2, \\dots, \\pi_H\\}$，在时间 $t$ 从状态 $x$ 开始的价值函数定义为：\n$$V_{t}^{\\pi}(x) = \\mathbb{E}^{\\pi}\\left[\\sum_{k=t}^{H} r(S_k, A_k) \\, \\middle| \\, S_t = x \\right]$$\n我们可以为此价值函数推导出递归关系。通过拆分求和项并应用全期望定律：\n$$V_{t}^{\\pi}(x) = \\mathbb{E}^{\\pi}\\left[r(S_t, A_t) + \\sum_{k=t+1}^{H} r(S_k, A_k) \\, \\middle| \\, S_t = x \\right]$$\n$$V_{t}^{\\pi}(x) = \\mathbb{E}^{\\pi}[r(S_t, A_t) | S_t = x] + \\mathbb{E}^{\\pi}\\left[\\sum_{k=t+1}^{H} r(S_k, A_k) \\, \\middle| \\, S_t = x \\right]$$\n给定策略 $A_t = \\pi_t(S_t)$，第一项是 $r(x, \\pi_t(x))$。对于第二项，我们以下一状态 $S_{t+1}$ 为条件：\n$$\\mathbb{E}^{\\pi}\\left[\\sum_{k=t+1}^{H} r(S_k, A_k) \\, \\middle| \\, S_t = x \\right] = \\sum_{x' \\in \\mathcal{S}} P(x'|x, \\pi_t(x)) \\, \\mathbb{E}^{\\pi}\\left[\\sum_{k=t+1}^{H} r(S_k, A_k) \\, \\middle| \\, S_{t+1} = x' \\right]$$\n根据定义，内部的期望是 $V_{t+1}^{\\pi}(x')$。因此，我们得到了给定策略 $\\pi$ 的贝尔曼方程：\n$$V_{t}^{\\pi}(x) = r(x, \\pi_t(x)) + \\sum_{x' \\in \\mathcal{S}} P(x'|x, \\pi_t(x)) V_{t+1}^{\\pi}(x')$$\n最优价值函数 $V_t^*(x) = \\max_{\\pi} V_t^\\pi(x)$，是通过在每一步选择能使右侧最大化的动作来找到的。这给出了有限期的贝尔曼最优性方程：\n$$V_t^*(x) = \\max_{u \\in \\mathcal{A}(x)} \\left\\{ r(x, u) + \\sum_{x' \\in \\mathcal{S}} P(x'|x, u) V_{t+1}^*(x') \\right\\}$$\n这个过程从最后的时间步 $H$ 开始，然后向后推导。对于 $t=H=3$，价值函数定义中的求和只有一项：\n$$V_3^{\\pi}(x) = \\mathbb{E}^{\\pi}[r(S_3, A_3) | S_3=x] = r(x, \\pi_3(x))$$\n因此，在时间 $t=3$ 时的最优价值函数就是最大的立即奖励：\n$$V_3^*(x) = \\max_{u \\in \\mathcal{A}(x)} r(x, u)$$\n这可以看作是应用贝尔曼方程，并使用终端条件 $V_{H+1}^*(x) = V_4^*(x) = 0$（对于所有状态 $x \\in \\mathcal{S}$）。\n\n我们现在用反向归纳法来解决这个问题。时间范围为 $H=3$。\n\n**时间步 $t=3$：**\n我们为每个状态 $x \\in \\{s, j, z\\}$ 计算 $V_3^*(x)$。\n-   对于状态 $s$：$V_3^*(s) = \\max_{u \\in \\{a,b\\}} r(s,u) = \\max\\{r(s,a), r(s,b)\\} = \\max\\{2, 0\\} = 2$。最优动作为 $\\pi_3^*(s) = a$。\n-   对于状态 $j$：只有一个动作。$V_3^*(j) = r(j, \\cdot) = 5$。\n-   对于状态 $z$：只有一个动作。$V_3^*(z) = r(z, \\cdot) = 0$。\n\n**时间步 $t=2$：**\n我们使用 $V_3^*(x')$ 的值来计算 $V_2^*(x)$。\n$$V_2^*(x) = \\max_{u \\in \\mathcal{A}(x)} \\left\\{ r(x, u) + \\sum_{x' \\in \\mathcal{S}} P(x'|x, u) V_3^*(x') \\right\\}$$\n-   对于状态 $s$：我们比较采取动作 $a$ 与动作 $b$ 的价值。\n    -   动作 $a$：期望回报为 $r(s, a) + P(z|s,a)V_3^*(z) = 2 + 1 \\cdot 0 = 2$。\n    -   动作 $b$：期望回报为 $r(s, b) + P(j|s,b)V_3^*(j) + P(s|s,b)V_3^*(s) = 0 + \\frac{1}{2} \\cdot V_3^*(j) + \\frac{1}{2} \\cdot V_3^*(s) = \\frac{1}{2}(5) + \\frac{1}{2}(2) = \\frac{5}{2} + 1 = \\frac{7}{2}$。\n    -   $V_2^*(s) = \\max\\{2, \\frac{7}{2}\\} = \\frac{7}{2}$。最优动作为 $\\pi_2^*(s) = b$。\n-   对于状态 $j$：$V_2^*(j) = r(j, \\cdot) + P(z|j, \\cdot)V_3^*(z) = 5 + 1 \\cdot 0 = 5$。\n-   对于状态 $z$：$V_2^*(z) = r(z, \\cdot) + P(z|z, \\cdot)V_3^*(z) = 0 + 1 \\cdot 0 = 0$。\n\n**时间步 $t=1$：**\n我们使用 $V_2^*(x')$ 的值来计算 $V_1^*(x)$。\n$$V_1^*(x) = \\max_{u \\in \\mathcal{A}(x)} \\left\\{ r(x, u) + \\sum_{x' \\in \\mathcal{S}} P(x'|x, u) V_2^*(x') \\right\\}$$\n-   对于状态 $s$：我们比较采取动作 $a$ 与动作 $b$ 的价值。\n    -   动作 $a$：期望回报为 $r(s, a) + P(z|s,a)V_2^*(z) = 2 + 1 \\cdot 0 = 2$。\n    -   动作 $b$：期望回报为 $r(s, b) + P(j|s,b)V_2^*(j) + P(s|s,b)V_2^*(s) = 0 + \\frac{1}{2} \\cdot V_2^*(j) + \\frac{1}{2} \\cdot V_2^*(s) = \\frac{1}{2}(5) + \\frac{1}{2}(\\frac{7}{2}) = \\frac{5}{2} + \\frac{7}{4} = \\frac{10}{4} + \\frac{7}{4} = \\frac{17}{4}$。\n    -   $V_1^*(s) = \\max\\{2, \\frac{17}{4}\\} = \\frac{17}{4}$。最优动作为 $\\pi_1^*(s) = b$。\n-   对于状态 $j$：$V_1^*(j) = r(j, \\cdot) + P(z|j, \\cdot)V_2^*(z) = 5 + 1 \\cdot 0 = 5$。\n-   对于状态 $z$：$V_1^*(z) = r(z, \\cdot) + P(z|z, \\cdot)V_2^*(z) = 0 + 1 \\cdot 0 = 0$。\n\n现在我们可以回答具体问题了。\n\n1) 对于每个时间 $t \\in \\{1,2,3\\}$，在状态 $s$ 的最优动作为：\n-   在 $t=3$ 时，$\\pi_3^*(s) = a$，因为 $r(s,a)=2 > r(s,b)=0$。\n-   在 $t=2$ 时，$\\pi_2^*(s) = b$，因为动作 $b$ 的期望未来回报（为 $\\frac{7}{2}$）大于动作 $a$ 的回报（为 $2$）。\n-   在 $t=1$ 时，$\\pi_1^*(s) = b$，因为动作 $b$ 的期望未来回报（为 $\\frac{17}{4}$）大于动作 $a$ 的回报（为 $2$）。\n如果对于任何给定状态，最优动作不随时间变化，则该策略是稳态的。在这里，状态 $s$ 的最优动作不是恒定的：$\\pi_1^*(s) = b$, $\\pi_2^*(s) = b$，但 $\\pi_3^*(s) = a$。因此，最优策略 $\\pi^*$ 是**非稳态的**。这是有限期 MDP 的一个典型特征；动作的选择取决于剩余的时间。在 $t=3$ 时，没有未来，智能体会贪婪地选择立即奖励。在 $t=1$ 和 $t=2$ 时，智能体通过选择动作 $b$ 放弃了 $2$ 的立即奖励，以换取转移到具有更高未来价值的状态（$j$ 和 $s$ 本身）的机会，从而获得更大的总体期望回报。\n\n2) $t \\in \\{1,2,3\\}$ 的最优有限期价值 $V_t^*(s)$ 为：\n-   $V_1^*(s) = \\frac{17}{4} = 4.25$\n-   $V_2^*(s) = \\frac{7}{2} = 3.5$\n-   $V_3^*(s) = 2$\n比较它们，我们看到 $V_1^*(s) > V_2^*(s) > V_3^*(s)$。\n价值 $V_t^*(s)$ 表示从时间 $t$ 到时间范围结束（$t=3$）所能累积的最大可能期望总奖励。\n$V_3^*(s)$ 是一个阶段（$k=3$）的期望奖励。\n$V_2^*(s)$ 是两个阶段（$k=2,3$）的期望奖励。\n$V_1^*(s)$ 是三个阶段（$k=1,2,3$）的期望奖励。\n由于此 MDP 中的所有单步奖励都是非负的，因此随着阶段数的增加，累积奖励是非递减的。在最后一步之前的每一步，都有机会累积更多奖励。例如，$V_2^*(s)$ 大于 $V_3^*(s)$，因为从 $t=2$ 开始比从 $t=3$ 开始多了一个时间步来收集奖励。严格不等式成立，因为在 $t=2$ 时， $s$ 处的最优动作会导向可以在 $t=3$ 获得正奖励的状态。类似地，$V_1^*(s)$ 大于 $V_2^*(s)$，因为还有一个阶段（$t=1$）可以收集奖励。\n\n最终答案要求的是 $V_1^*(s)$ 的精确值。\n$$V_1^*(s) = \\frac{17}{4}$$", "answer": "$$\\boxed{\\frac{17}{4}}$$", "id": "3190850"}, {"introduction": "在理想世界中，我们知道所有行为的后果，但现实中我们常常需要“边走边看”。这个练习探讨了强化学习中的一个核心挑战：探索与利用的权衡。你将通过一个思想实验和具体计算，理解一种被称为“乐观初始化”的巧妙策略如何通过鼓励智能体尝试未知的行动，来系统性地进行探索，从而避免过早地锁定在次优选择上[@problem_id:3190816]。", "problem": "考虑一个具有有限状态集和有界奖励的确定性马尔可夫决策过程（MDP）。假设智能体使用表格型动作价值学习，其单步更新规则为\n$$\nQ_{t+1}(s_{t},a_{t}) \\leftarrow (1-\\alpha)Q_{t}(s_{t},a_{t}) + \\alpha\\left(r_{t} + \\gamma \\max_{a'} Q_{t}(s_{t+1},a')\\right),\n$$\n其中，对于所有状态-动作对，有 $Q_{0}(s,a)=Q_{0}>0$，学习率 $\\alpha \\in (0,1]$，折扣因子 $\\gamma \\in (0,1)$。在每个决策时刻 $t$，智能体根据 $Q_{t}$ 贪婪地选择动作，平局由一个固定的、依赖于状态但任意的顺序来打破。奖励是有界的，$0 \\le r_{t} \\le R_{\\max}$。\n\nA部分（概念性）。仅使用上述定义，并且除了确定性和有界奖励外，不假设任何特定的MDP结构，请论证为什么在纯贪婪策略下，乐观初始化 $Q_{0} > \\frac{R_{\\max}}{1-\\gamma}$ 会鼓励探索。具体来说，请论证在任何状态下，一旦一个动作被尝试过至少一次，其$Q$值就会下降到低于该状态下任何未尝试过的动作的$Q$值，从而确保智能体最终会尝试该状态下所有可用的动作。\n\nB部分（计算）。现在将问题具体化到一个确定性的网格世界，其中有一个非终止的起始单元格 $s$ 和一个终止的目标单元格 $g$。在 $s$ 中正好有两个可用动作：\n- 动作 $a_{R}$ 确定性地移动到 $g$ 并获得奖励 $r=1$，之后回合结束，智能体被重置到 $s$，不再有后续奖励。\n- 动作 $a_{L}$ 确定性地使智能体停留在 $s$ 并获得奖励 $r=0$。\n\n设智能体使用相同的单步更新规则，学习率 $\\alpha=1$，折扣因子 $\\gamma \\in (0,1)$，贪婪动作选择（平局时选择 $a_{L}$），以及初始化 $Q_{0}(s,a_{L})=Q_{0}(s,a_{R})=Q_{0}>0$，而 $Q_{0}(g,\\cdot)=0$。定义收敛时间 $T$ 为最小的整数，使得在恰好 $T$ 次动作选择及其相关更新之后，下一次访问 $s$ 时的贪婪动作是 $a_{R}$，并且在之后所有对 $s$ 的访问中都保持为 $a_{R}$。\n\n推导 $T$ 作为 $Q_{0}$ 和 $\\gamma$ 的函数的闭式表达式。你的最终答案必须是单一的解析表达式，而不是不等式或方程。不需要进行数值取整。", "solution": "用户提供了一个关于在确定性马尔可夫决策过程中进行Q学习的两部分问题。该问题提法明确，科学上基于强化学习的原理，并包含完整解答所需的所有信息。\n\n### A部分：通过乐观初始化论证探索的合理性\n\n目标是论证为什么在纯贪婪策略下，乐观初始化 $Q_{0} > \\frac{R_{\\max}}{1-\\gamma}$ 会鼓励探索。在此背景下，探索意味着智能体最终会尝试任何给定状态下的所有可用动作。\n\n首先，我们必须理解数量 $\\frac{R_{\\max}}{1-\\gamma}$ 的意义。对于任何状态 $s$ 和任何策略 $\\pi$，状态价值函数 $V^{\\pi}(s)$ 是从状态 $s$ 开始并遵循策略 $\\pi$ 的未来奖励的期望折扣总和。由于奖励有界，$r_t \\le R_{\\max}$，因此任何状态的最大可能价值受折扣最大奖励之和的限制：\n$$V^*(s) = \\max_{\\pi} V^{\\pi}(s) \\le \\sum_{t=0}^{\\infty} \\gamma^t R_{\\max} = \\frac{R_{\\max}}{1-\\gamma}$$\n最优动作价值函数 $Q^*(s,a)$ 也同样有界。因此，条件 $Q_{0}(s,a) = Q_{0} > \\frac{R_{\\max}}{1-\\gamma}$ 对所有 $(s,a)$ 对成立，意味着所有初始Q值都严格大于任何可能的最优Q值。这就是“乐观初始化”的定义。\n\n现在，我们论证任何动作一旦被执行，其Q值将更新为一个严格小于 $Q_0$ 的值。这将使得该状态下任何未尝试的动作对贪婪智能体显得更具吸引力，从而鼓励智能体选择它们。\n\n我们用归纳法证明，对于所有 $t \\ge 1$，任何至少更新过一次的Q值 $Q_t(s,a)$ 都严格小于 $Q_0$，而任何未被更新的Q值都保持为 $Q_0$。\n设 $(s_t, a_t)$ 是在决策时刻 $t$ 选择的状态-动作对。更新规则是：\n$$Q_{t+1}(s_{t},a_{t}) = (1-\\alpha)Q_{t}(s_{t},a_{t}) + \\alpha\\left(r_{t} + \\gamma \\max_{a'} Q_{t}(s_{t+1},a')\\right)$$\n所有其他Q值保持不变：对于 $(s,a) \\neq (s_t,a_t)$，有 $Q_{t+1}(s,a) = Q_t(s,a)$。\n\n**基本情况：** 在 $t=0$ 时，没有值被更新。所有值都是 $Q_0$。假设在 $t=0$ 时，智能体在状态 $s_0$ 执行动作 $a_0$。Q值为 $Q_0(s_0, a_0) = Q_0$。\n更新规则给出：\n$$Q_{1}(s_{0},a_{0}) = (1-\\alpha)Q_{0} + \\alpha\\left(r_{0} + \\gamma \\max_{a'} Q_{0}(s_{1},a')\\right)$$\n因为对于所有 $a'$，有 $Q_0(s_1, a') = Q_0$，表达式简化为：\n$$Q_{1}(s_{0},a_{0}) = (1-\\alpha)Q_{0} + \\alpha(r_{0} + \\gamma Q_{0})$$\n为了证明 $Q_1(s_0, a_0)  Q_0$，我们需要证明目标值 $r_0 + \\gamma Q_0$ 小于 $Q_0$。根据给定条件 $Q_0  \\frac{R_{\\max}}{1-\\gamma}$，可以得出 $Q_0(1-\\gamma)  R_{\\max}$。由于 $r_0 \\le R_{\\max}$，我们有 $r_0  Q_0(1-\\gamma)$，整理后得到 $r_0 + \\gamma Q_0  Q_0$。\n新值 $Q_1(s_0, a_0)$ 是 $Q_0$ 和一个严格小于 $Q_0$ 的值的凸组合。由于 $\\alpha \\in (0,1]$，新值必须严格小于 $Q_0$。所有其他Q值保持为 $Q_0$。\n\n**归纳步骤：** 假设在时刻 $t$，所有Q值都满足 $Q_t(s,a) \\le Q_0$。让我们计算 $(s_t, a_t)$ 的更新。目标值为 $T_t = r_t + \\gamma \\max_{a'} Q_t(s_{t+1}, a')$。\n根据归纳假设，$\\max_{a'} Q_t(s_{t+1}, a') \\le Q_0$。因此，目标值有界：\n$$T_t \\le r_t + \\gamma Q_0$$\n如前所述，$r_t + \\gamma Q_0  Q_0$。所以，$T_t  Q_0$。\n新值为 $Q_{t+1}(s_t, a_t) = (1-\\alpha)Q_t(s_t, a_t) + \\alpha T_t$。这是 $Q_t(s_t, a_t) \\le Q_0$ 和 $T_t  Q_0$ 的凸组合。因此，$Q_{t+1}(s_t, a_t)  Q_0$。\n对于任何其他对 $(s,a) \\neq (s_t, a_t)$，有 $Q_{t+1}(s,a) = Q_t(s,a) \\le Q_0$。\n因此，通过归纳法，任何Q值一旦更新，就会变得严格小于 $Q_0$。\n\n**结论：** 考虑任何状态 $s$。设 $A_{tried}$ 为在状态 $s$ 中已尝试过的动作集合，而 $A_{untried}$ 为未尝试过的动作集合。对于任何 $a_{tried} \\in A_{tried}$，其值 $Q_t(s, a_{tried})$ 至少被更新过一次，因此 $Q_t(s, a_{tried})  Q_0$。对于任何 $a_{untried} \\in A_{untried}$，其值从未被更新，因此 $Q_t(s, a_{untried}) = Q_0$。\n一个在状态 $s$ 的贪婪智能体选择动作 $a = \\arg\\max_{a'} Q_t(s, a')$。最大值是 $Q_0$，它对应于 $A_{untried}$ 中的任何动作。因此，智能体保证会从 $A_{untried}$ 中选择一个动作（具体选择由平局打破规则决定）。这个过程会一直持续到 $A_{untried}$ 为空，从而确保状态 $s$ 中的每个动作最终都会被尝试。这表明，使用贪婪策略的乐观初始化会鼓励系统性的探索。\n\n### B部分：收敛时间的计算\n\n问题指定了一个确定性的MDP，有两个状态 $s$ 和 $g$，以及从 $s$ 出发的两个动作 $a_L$ 和 $a_R$。给定 $\\alpha=1$。单步更新规则简化为：\n$$Q_{t+1}(s_t, a_t) \\leftarrow r_t + \\gamma \\max_{a'} Q_t(s_{t+1}, a')$$\n\n我们来追踪Q值 $Q(s, a_L)$ 和 $Q(s, a_R)$ 的演变。\n初始值：$Q_0(s, a_L) = Q_0$ 和 $Q_0(s, a_R) = Q_0$。终止状态的值对于所有 $t$ 均为 $Q_t(g, \\cdot) = 0$。\n\n**步骤0 (t=0):**\n- 状态：$s_0=s$。Q值相等：$Q_0(s, a_L) = Q_0(s, a_R)$。\n- 动作：平局规则倾向于 $a_L$，所以 $a_0 = a_L$。\n- 结果：奖励 $r_0=0$，下一状态 $s_1=s$。\n- 更新：$Q_1(s, a_L) \\leftarrow r_0 + \\gamma \\max_{a'} Q_0(s, a') = 0 + \\gamma \\max(Q_0, Q_0) = \\gamma Q_0$。\n- 另一个Q值不变：$Q_1(s, a_R) = Q_0(s, a_R) = Q_0$。\n- 经过1次动作后，值为 $Q_1(s, a_L) = \\gamma Q_0$ 和 $Q_1(s, a_R) = Q_0$。\n\n**步骤1 (t=1):**\n- 状态：$s_1=s$。因为 $\\gamma \\in (0,1)$ 且 $Q_00$，我们有 $Q_1(s, a_R)  Q_1(s, a_L)$。\n- 动作：贪婪选择是 $a_1 = a_R$。\n- 结果：奖励 $r_1=1$，下一状态 $s_2=g$（终止）。回合结束。\n- 更新：$Q_2(s, a_R) \\leftarrow r_1 + \\gamma \\max_{a'} Q_1(g, a') = 1 + \\gamma \\cdot 0 = 1$。\n- 另一个Q值不变：$Q_2(s, a_L) = Q_1(s, a_L) = \\gamma Q_0$。\n- 经过2次动作后，智能体被重置到状态 $s$，值为 $Q_2(s, a_L) = \\gamma Q_0$ 和 $Q_2(s, a_R) = 1$。\n\n**后续步骤 (t $\\ge$ 2):**\n智能体现在处于状态 $s$，并比较 $Q(s, a_L) = \\gamma Q_0$ 和 $Q(s, a_R) = 1$。贪婪选择取决于哪个更大。\n设 $m$ 是连续选择 $a_L$ 的额外次数。只要 $Q(s,a_L) \\ge Q(s,a_R)$，这种情况就会发生。\n\n- 如果 $\\gamma Q_0  1$，在 $t=2$ 时智能体选择 $a_R$。更新为 $Q_3(s, a_R) \\leftarrow 1$，所以值不变。对 $a_R$ 的选择变得稳定。在 $T=2$ 步后达到收敛。在这种情况下，$m=0$。\n\n- 如果 $\\gamma Q_0 \\ge 1$，在 $t=2$ 时智能体选择 $a_L$。\n  - 更新：$Q_3(s, a_L) \\leftarrow 0 + \\gamma \\max\\{Q_2(s, a_L), Q_2(s, a_R)\\} = \\gamma \\max\\{\\gamma Q_0, 1\\} = \\gamma (\\gamma Q_0) = \\gamma^2 Q_0$。\n  - $Q_3(s, a_R)=1$。\n- 在 $t=3$ 时，智能体比较 $\\gamma^2 Q_0$ 和 $1$。如果 $\\gamma^2 Q_0 \\ge 1$，它会再次选择 $a_L$。\n\n这个模式会持续下去。设 $m$ 是在初始的 $(a_L, a_R)$ 序列之后选择 $a_L$ 的次数。如果条件 $Q(s,a_L) \\ge 1$ 成立，则 $a_L$ 会被第 $(k+1)$ 次选择（其中 $k=0, 1, ..., m-1$）。在初始回合（2步）和 $k$ 次额外的 $a_L$ 选择之后，值为 $Q(s, a_L) = \\gamma^{k+1} Q_0$。因此，再次选择 $a_L$ 的条件是 $\\gamma^{k+1} Q_0 \\ge 1$。\n\n$m$ 是满足 $\\gamma^{k+1} Q_0 \\ge 1$ 的非负整数 $k$ 的数量。\n$$\\gamma^{k+1} \\ge \\frac{1}{Q_0}$$\n以 $1/\\gamma  1$ 为底取对数：\n$$k+1 \\le \\log_{1/\\gamma}\\left(\\frac{1}{Q_0^{-1}}\\right) = \\log_{1/\\gamma}(Q_0)$$\n$$k \\le \\log_{1/\\gamma}(Q_0) - 1$$\n我们需要计算满足此条件的非负整数 $k$ 的数量。\n如果 $\\log_{1/\\gamma}(Q_0)  1$（即 $Q_0  1/\\gamma$ 或 $\\gamma Q_0  1$），则 $\\log_{1/\\gamma}(Q_0) - 1  0$，因此没有非负整数 $k$ 满足该不等式。在这种情况下，$m=0$。\n如果 $\\log_{1/\\gamma}(Q_0) \\ge 1$，非负整数为 $k=0, 1, \\dots, \\lfloor \\log_{1/\\gamma}(Q_0) - 1 \\rfloor$。数量为 $m = \\lfloor \\log_{1/\\gamma}(Q_0) - 1 \\rfloor + 1 = \\lfloor \\log_{1/\\gamma}(Q_0) \\rfloor$。\n综合这两种情况，$m = \\max(0, \\lfloor \\log_{1/\\gamma}(Q_0) \\rfloor)$。\n\n直到收敛的总动作数 $T$ 是所执行动作的总和。\n- 初始 $a_L$ 的1次动作。\n- 第一次 $a_R$ 的1次动作。\n- 后续 $a_L$ 选择的 $m$ 次动作。\n总动作数：$T = 1 + 1 + m = 2 + m$。\n在这些 $T$ 次动作之后，在决策时刻 $t=T$，值 $Q_T(s, a_L)$ 变为 $\\gamma^{m+1} Q_0$。根据 $m$ 的定义，我们有 $\\gamma^{m+1}Q_0  1$。因此，在 $t=T$ 时，智能体比较 $Q_T(s, a_L)  1$ 和 $Q_T(s, a_R) = 1$，并选择 $a_R$。这个选择在未来的所有访问中都保持稳定。这符合问题对 $T$ 的定义。\n\n因此，$T$ 的最终表达式为：\n$$T = 2 + \\max\\left(0, \\left\\lfloor \\log_{1/\\gamma}(Q_0) \\right\\rfloor\\right)$$\n这也可以使用自然对数写成：\n$$T = 2 + \\max\\left(0, \\left\\lfloor \\frac{\\ln(Q_0)}{\\ln(1/\\gamma)} \\right\\rfloor\\right)$$", "answer": "$$ \\boxed{2 + \\max\\left(0, \\left\\lfloor \\frac{\\ln(Q_0)}{\\ln(1/\\gamma)} \\right\\rfloor\\right)} $$", "id": "3190816"}, {"introduction": "从数据中学习是强大的，但也充满了陷阱。这个练习模拟了一个在离线强化学习中常见的警示性场景：基于从有限且可能具有误导性的数据中学到的价值函数进行“贪心”改进，有时反而会导致策略变得更差。这个例子揭示了统计过拟合的风险，并启发我们思考如何借鉴机器学习中的验证思想来防止这种“策略退化”现象[@problem_id:3190845]。", "problem": "考虑一个有限时域、单步的马尔可夫决策过程 (MDP)，其中有一个非终止状态 $s$ 和两个动作 $a$ 和 $b$。当在状态 $s$ 采取任一动作时，该回合会立即终止并产生一个随机奖励。奖励分布如下：\n- 如果采取动作 $a$，奖励 $R_a$ 以 $0.5$ 的概率为 $+1$，以 $0.5$ 的概率为 $-1$。\n- 如果采取动作 $b$，奖励 $R_b$ 确定性地为 $+0.4$。\n\n假设用于收集静态数据集的行为策略 $\\mu$ 在状态 $s$ 下均匀随机地选择动作，因此数据集中有 $n_a=5$ 个来自动作 $a$ 的样本和 $n_b=5$ 个来自动作 $b$ 的样本。在该数据集的一次特定实现中，所有五个观测到的动作 $a$ 的奖励恰好都是 $+1$，而所有五个观测到的动作 $b$ 的奖励都是 $+0.4$。一个灵活的动作价值函数模型 $\\hat q(s,\\cdot)$ 通过对这些数据进行经验风险最小化来拟合，并精确地插值了经验均值，在状态 $s$ 上得到 $\\hat q(s,a)=1$ 和 $\\hat q(s,b)=0.4$。\n\n定义一个策略 $\\pi$ 为确定性策略 $\\pi(s)=b$（即在每个回合中，在状态 $s$ 都选择 $b$），并根据 $\\hat q$ 定义一个贪心策略 $\\pi'$ 为 $\\pi'(s)=\\arg\\max_{u\\in\\{a,b\\}} \\hat q(s,u)$。设折扣因子 $\\gamma=1$，时域为单个时间步。价值函数 $v^\\pi(s)$ 由标准的固定策略评估表达式 $v^\\pi(s)=\\mathbb{E}_\\pi\\left[\\sum_{t=0}^{T-1} \\gamma^t R_t \\mid S_0=s\\right]$ 定义，在此简化为该策略所引出的动作分布下的期望立即奖励。\n\n从基本原理出发——即价值函数 $v^\\pi(s)$、动作价值函数 $q^\\pi(s,a)$（即在状态 $s$ 采取动作 $a$ 然后遵循策略 $\\pi$ 的期望回报）以及贪心改进算子的定义——确定在当前设定下，以下哪些陈述是正确的。你的推理应考虑到统计学习效应（对有限数据的过拟合），并应从给定的定义出发，而不是从任何专门的策略改进保证出发。\n\nA. 在这个 MDP 中，当前策略 $\\pi$ 下的 $v^\\pi(s)$ 超过了由 $\\hat q$ 引出的贪心策略 $\\pi'$ 下的 $v^{\\pi'}(s)$，因此使用过拟合的 $\\hat q$ 进行贪心改进会严格降低状态 $s$ 的真实价值。\n\nB. 一种实际的缓解措施是将数据划分为训练集和验证集，在 $\\hat q$ 模型按轮次 $t$ 训练时计算一系列贪心策略 $\\{\\pi_t\\}$，在验证集上离策略地估计 $v^{\\pi_t}(s)$，并在验证集上对 $v^{\\pi_t}(s)$ 的估计值不再提升的最小 $t$ 处停止（交叉验证早停），从而减少因过拟合导致策略回归的可能性。\n\nC. 因为 $\\hat q$ 的经验训练误差随着训练迭代单调递减，所以引出的贪心策略序列的真实价值 $v^{\\pi_t}(s)$ 必然单调递增，因此不会发生策略回归。\n\nD. 如果行为策略 $\\mu$ 探索了两个动作，那么在验证集上进行重要性采样总是能产生对 $v^{\\pi_t}(s)$ 的低方差、无偏估计，从而使早停变得不必要。\n\n选择所有适用的选项。", "solution": "首先验证问题陈述，以确保其科学上合理、定义明确且客观。\n\n### 步骤 1：提取已知条件\n-   **MDP 结构**：一个有限时域、单步的马尔可夫决策过程 (MDP)。\n-   **状态空间**：一个非终止状态，表示为 $s$。\n-   **动作空间**：两个动作，$a$ 和 $b$。\n-   **转移/终止**：在状态 $s$ 采取的任何动作都会导致回合立即终止。\n-   **奖励分布（真实）**：\n    -   动作 $a$：奖励 $R_a$ 以 $0.5$ 的概率为 $+1$，以 $0.5$ 的概率为 $-1$。\n    -   动作 $b$：奖励 $R_b$ 确定性地为 $+0.4$。\n-   **行为策略**：$\\mu$ 均匀随机地选择动作：$\\mu(a|s) = \\mu(b|s) = 0.5$。\n-   **数据集**：在 $\\mu$ 下收集的静态数据集，其中动作 $a$ 有 $n_a=5$ 个样本，动作 $b$ 有 $n_b=5$ 个样本。\n-   **观测数据实现**：\n    -   对于动作 $a$ 的所有 $5$ 个样本，观测到的奖励均为 $+1$。\n    -   对于动作 $b$ 的所有 $5$ 个样本，观测到的奖励均为 $+0.4$。\n-   **动作价值模型**：一个灵活的模型 $\\hat q(s, \\cdot)$ 通过经验风险最小化进行拟合。\n-   **拟合模型值**：该模型精确地插值了经验均值，得到 $\\hat q(s,a)=1$ 和 $\\hat q(s,b)=0.4$。\n-   **策略定义**：\n    -   $\\pi$ 是一个确定性策略：$\\pi(s)=b$。\n    -   $\\pi'$ 是一个关于 $\\hat q$ 的贪心策略：$\\pi'(s)=\\arg\\max_{u\\in\\{a,b\\}} \\hat q(s,u)$。\n-   **参数**：折扣因子 $\\gamma=1$，时域 $T=1$。\n-   **价值函数定义**：$v^\\pi(s)=\\mathbb{E}_\\pi\\left[\\sum_{t=0}^{T-1} \\gamma^t R_t \\mid S_0=s\\right]$。由于 $T=1$ 和 $\\gamma=1$，这可以简化为 $v^\\pi(s) = \\mathbb{E}[R_1 | S_0=s, A_0 \\sim \\pi(\\cdot|s)]$。\n\n### 步骤 2：使用提取的已知条件进行验证\n-   **科学依据**：该问题是强化学习和统计学习领域内一个构造良好的例子。它使用了 MDP、价值函数、策略、经验风险最小化和过拟合等标准概念。有限样本偏离其真实期望的情景是统计学中的一个基本概念；一枚公平硬币连续掷出 5 次正面朝上的实现具有非零概率（$(0.5)^5 = 0.03125$），而这里给出的情景是类似的。该问题在科学上是合理的。\n-   **定义明确**：所有必要信息都已提供。MDP、策略和学习结果都得到了精确定义，从而可以进行明确的计算和评估。\n-   **客观性**：问题以该领域的标准精确、客观和正式的语言陈述。它不包含任何主观性断言。\n-   **结论**：该问题是有效的。它提出了一个清晰且具有启发性的情景，说明了在离线设置中基于过拟合的价值估计进行策略改进的危险。\n\n### 步骤 3：推导与选项分析\n我们根据已验证的陈述来解决问题。\n\n**从基本原理推导**\n\n1.  **真实动作价值函数**：真实动作价值函数 $q^*(s,u)$ 是在状态 $s$ 采取动作 $u$ 的真实期望奖励。\n    -   对于动作 $a$：$q^*(s,a) = \\mathbb{E}[R_a] = (0.5 \\times 1) + (0.5 \\times -1) = 0.5 - 0.5 = 0$。\n    -   对于动作 $b$：$q^*(s,b) = \\mathbb{E}[R_b] = 1 \\times 0.4 = 0.4$。\n\n2.  **真实最优策略**：最优策略 $\\pi^*$ 选择具有最高真实期望回报的动作。由于 $q^*(s,b) = 0.4  q^*(s,a) = 0$，最优策略是总是选择动作 $b$。\n\n3.  **策略 $\\pi$ 的价值**：策略 $\\pi$ 定义为 $\\pi(s)=b$。其价值 $v^\\pi(s)$ 是遵循该策略时的期望回报。\n    -   $v^\\pi(s) = \\mathbb{E}_{\\pi}[R_1 | S_0=s] = q^*(s, \\pi(s)) = q^*(s,b) = 0.4$。\n\n4.  **拟合的动作价值函数 $\\hat q$**：问题陈述 $\\hat q$ 是通过经验风险最小化拟合的，并且插值了经验均值。\n    -   动作 $a$ 的经验均值：基于 $5$ 个观测到的奖励 $+1$，均值为 $\\frac{1}{5}\\sum_{i=1}^{5} 1 = 1$。因此，$\\hat q(s,a) = 1$。\n    -   动作 $b$ 的经验均值：基于 $5$ 个观测到的奖励 $+0.4$，均值为 $\\frac{1}{5}\\sum_{i=1}^{5} 0.4 = 0.4$。因此，$\\hat q(s,b) = 0.4$。\n    -   注意，$\\hat q(s,a)=1$ 是对真实值 $q^*(s,a)=0$ 的严重高估，这是由一个不具代表性但可能出现的随机样本造成的。这就是过拟合。\n\n5.  **贪心策略 $\\pi'$**：策略 $\\pi'$ 是关于 $\\hat q$ 的贪心策略。\n    -   $\\pi'(s) = \\arg\\max_{u \\in \\{a,b\\}} \\hat q(s,u) = \\arg\\max(1, 0.4) = a$。\n\n6.  **策略 $\\pi'$ 的价值**：价值 $v^{\\pi'}(s)$ 是遵循 $\\pi'$ 时的真实期望回报。\n    -   $v^{\\pi'}(s) = \\mathbb{E}_{\\pi'}[R_1 | S_0=s] = q^*(s, \\pi'(s)) = q^*(s,a) = 0$。\n\n**逐项分析**\n\n**A. 在这个 MDP 中，当前策略 $\\pi$ 下的 $v^\\pi(s)$ 超过了由 $\\hat q$ 引出的贪心策略 $\\pi'$ 下的 $v^{\\pi'}(s)$，因此使用过拟合的 $\\hat q$ 进行贪心改进会严格降低状态 $s$ 的真实价值。**\n\n-   根据我们的推导，我们有 $v^\\pi(s) = 0.4$ 和 $v^{\\pi'}(s) = 0$。\n-   陈述的第一部分“$v^\\pi(s)$ 超过 $v^{\\pi'}(s)$”是正确的，因为 $0.4  0$。\n-   策略“改进”步骤将策略从 $\\pi$（选择 $b$）更改为 $\\pi'$（选择 $a$）。这个改变是基于过拟合的估计值 $\\hat q(s,a) = 1  \\hat q(s,b) = 0.4$。\n-   这一改变的结果是，策略在状态 $s$ 的真实价值从 $v^\\pi(s)=0.4$ 下降到 $v^{\\pi'}(s)=0$。\n-   这种由近似价值函数建议的策略改变导致真实策略价值下降的现象，被称为策略回归。这里发生这种情况是因为价值函数 $\\hat q$ 对一个有限的、具有误导性的数据集发生了过拟合。\n-   该陈述准确地描述了这种情况。\n-   **结论：正确。**\n\n**B. 一种实际的缓解措施是将数据划分为训练集和验证集，在 $\\hat q$ 模型按轮次 $t$ 训练时计算一系列贪心策略 $\\{\\pi_t\\}$，在验证集上离策略地估计 $v^{\\pi_t}(s)$，并在验证集上对 $v^{\\pi_t}(s)$ 的估计值不再提升的最小 $t$ 处停止（交叉验证早停），从而减少因过拟合导致策略回归的可能性。**\n\n-   这个选项描述了一种应用于离线强化学习的机器学习标准技术。\n-   当模型过分学习训练数据的细节，从而失去对新的、未见过数据的泛化能力时，就会发生过拟合。在强化学习中，这可能表现为一个不准确的价值函数，导致一个糟糕的策略。\n-   所提出的缓解措施是使用一个验证集，该验证集不用于训练模型 $\\hat q_t$。在每个训练轮次 $t$，引出的贪心策略 $\\pi_t$ 在这个留出数据上进行评估。这种评估必须是离策略的（例如，使用重要性采样），因为验证数据是用行为策略 $\\mu$ 而不是 $\\pi_t$ 收集的。\n-   通过在验证集上跟踪 $\\pi_t$ 的估计价值，可以观察到模型何时开始过拟合。过拟合会导致验证性能停滞或下降，即使训练性能仍在持续提高。\n-   在验证性能最佳的点停止（早停）是一种行之有效的启发式方法，用于找到一个泛化能力好的模型，在此背景下，即找到一个在真实环境中表现良好的策略。这直接缓解了 A 选项中所示的策略回归。\n-   **结论：正确。**\n\n**C. 因为 $\\hat q$ 的经验训练误差随着训练迭代单调递减，所以引出的贪心策略序列的真实价值 $v^{\\pi_t}(s)$ 必然单调递增，因此不会发生策略回归。**\n\n-   该陈述假定模型的训练误差与引出策略的真实价值之间存在必然的正相关关系。\n-   问题本身的设置就是一个决定性的反例。模型 $\\hat q$ 被拟合为“精确地插值经验均值”，这意味着它在训练数据上达到了可能的最小经验误差（例如，零均方误差）。设这是训练的最终状态 $\\hat q_T$。\n-   考虑一个假设的训练起点 $\\hat q_0$，其中 $\\hat q_0(s,a) = \\hat q_0(s,b) = 0$。贪心策略 $\\pi_0$ 将会选择 $b$（假设平局时倾向于选择 $b$），其真实价值为 $v^{\\pi_0}(s) = 0.4$。\n-   随着训练的进行，$\\hat q_t(s,a)$ 将从 $0$ 增加到 $1$，而 $\\hat q_t(s,b)$ 将从 $0$ 增加到 $0.4$。$\\hat q_t$ 的经验训练误差将单调递减。\n-   一旦 $\\hat q_t(s,a)$ 变得大于 $\\hat q_t(s,b)$（这将在 $\\hat q_t(s,a)  0.4$ 时发生），贪心策略 $\\pi_t$ 将从选择 $b$ 翻转为选择 $a$。就在那一刻，策略的真实价值 $v^{\\pi_t}(s)$ 从 $0.4$ 降至 $0$。\n-   因此，$\\hat q$ 的单调递减的训练误差可能对应策略真实价值的急剧下降。该陈述是错误的。\n-   **结论：不正确。**\n\n**D. 如果行为策略 $\\mu$ 探索了两个动作，那么在验证集上进行重要性采样总是能产生对 $v^{\\pi_t}(s)$ 的低方差、无偏估计，从而使早停变得不必要。**\n\n-   这个选项提出了两个强有力但错误的论断。\n-   首先，它声称只要行为策略是探索性的（即，对于目标策略采取的所有动作 $u$，都有 $\\mu(u|s)0$），重要性采样（IS）“总是能产生低方差”的估计。虽然在这种条件下 IS 提供了无偏估计，但其方差是一个重大的实践问题。IS 估计器的方差取决于重要性权重 $\\rho = \\frac{\\pi(A|S)}{\\mu(A|S)}$ 的方差。如果目标策略 $\\pi$ 与行为策略 $\\mu$ 大相径庭，这些比率可能会很大，导致估计的方差极高。“总是低方差”的说法是错误的。\n-   其次，它声称一个好的估计器使得“早停变得不必要”。这误解了验证和早停的作用。早停是一种模型选择算法。它*需要*一个泛化性能的估计器才能工作。拥有一个好的估计器（无偏、低方差）是使早停有效的原因；它并不能消除对早停的需求。如果没有像早停这样的选择标准，人们只会将模型在训练数据上训练到完成，这将直接导致问题中描述的过拟合模型 $\\hat q$ 和次优策略 $\\pi'$。估计器是模型选择的工具，而不是替代品。\n-   **结论：不正确。**", "answer": "$$\\boxed{AB}$$", "id": "3190845"}]}