{"hands_on_practices": [{"introduction": "策略梯度法是强化学习中一种寻找最优策略的强大方法。然而，本练习将揭示一个关键陷阱：如果探索不足，这些方法很容易陷入次优解。通过亲手计算 [@problem_id:3169897]，你将了解目标函数的“地形”如何欺骗一个简单的梯度上升智能体，并理解为什么更好的探索机制至关重要。", "problem": "考虑以下基于强化学习基本原理的马尔可夫决策过程（MDP）构建与分析任务。一个马尔可夫决策过程（MDP）由一组状态、一组动作、一个转移核、一个奖励函数和一个折扣因子指定。给定以下具有两个状态和分幕式界限的 MDP。\n\n- 状态：过程从状态 $s_0$ 开始。如果动作转移到状态 $s_1$，则该幕继续；否则，立即终止。\n- 在 $s_0$ 处的动作：智能体选择两个动作之一，$a \\in \\{\\text{left}, \\text{right}\\}$。如果 $a=\\text{left}$，该幕终止，立即奖励为 $2$。如果 $a=\\text{right}$，该幕确定性地转移到 $s_1$，立即奖励为 $0$。\n- 在 $s_1$ 处的动作：智能体选择两个动作之一，$b \\in \\{\\text{safe}, \\text{risky}\\}$。如果 $b=\\text{safe}$，该幕终止，奖励为 $0$。如果 $b=\\text{risky}$，该幕终止，以 $1/2$ 的概率获得奖励 $10$，以 $1/2$ 的概率获得奖励 $-1$。\n- 折扣因子：折扣因子为 $\\gamma=1$。\n\n智能体使用一个稳态、随机、可微的策略，该策略由两个实标量 $\\theta_1$ 和 $\\theta_2$ 参数化，并在每个状态下具有独立的 logistic 参数化：\n- 在 $s_0$ 处，选择 $\\text{right}$ 的概率为 $\\pi(\\text{right}\\mid s_0)=\\sigma(\\theta_1)$，选择 $\\text{left}$ 的概率为 $\\pi(\\text{left}\\mid s_0)=1-\\sigma(\\theta_1)$。\n- 在 $s_1$ 处，选择 $\\text{risky}$ 的概率为 $\\pi(\\text{risky}\\mid s_1)=\\sigma(\\theta_2)$，选择 $\\text{safe}$ 的概率为 $\\pi(\\text{safe}\\mid s_1)=1-\\sigma(\\theta_2)$。\n\n这里 $\\sigma(x)$ 表示 logistic 函数，定义为 $\\sigma(x)=\\frac{1}{1+\\exp(-x)}$。\n\n仅使用期望回报的基本定义和微分的链式法则，并且不援引任何专门的策略梯度定理，完成以下任务：\n1. 从起始状态 $s_0$ 推导期望分幕回报 $J(\\theta_1,\\theta_2)$，用 $\\sigma(\\theta_1)$ 和 $\\sigma(\\theta_2)$ 表示。\n2. 通过链式法则和 $\\sigma(x)$ 的性质，显式计算偏导数 $\\frac{\\partial J}{\\partial \\theta_1}$ 和 $\\frac{\\partial J}{\\partial \\theta_2}$。\n3. 分析 $J(\\theta_1,\\theta_2)$ 的驻点，并描述在何种条件下，从一个较小的 $\\sigma(\\theta_1)$ 初始化开始的朴素策略梯度上升会收敛到一个避开 $s_1$ 的次优局部最大值。为产生这种行为的 $\\theta_1$ 和 $\\theta_2$ 之间的耦合提供直观解释。\n4. 作为一个具体的计算成果，确定在 $\\theta_1$ 为有限值时（即不依赖于饱和状态 $\\sigma(\\theta_1)\\in\\{0,1\\}$），使 $\\frac{\\partial J}{\\partial \\theta_1}=0$ 的 $\\sigma(\\theta_2)$ 的精确值。将你的最终答案表示为单个简化的分数或封闭形式的表达式。无需四舍五入。", "solution": "该问题陈述是强化学习中一个明确定义的练习，它具有科学依据、适定性且客观。其中不包含任何使其无效的缺陷。我们可以开始求解。\n\n该问题要求使用基本原理来分析一个简单的马尔可夫决策过程（MDP）。我们已知 MDP 的结构、奖励以及一个参数化的策略。我们将推导期望回报，计算其梯度，并分析其驻点，以理解策略梯度方法的一个潜在陷阱。\n\n为简化表示，令 $\\sigma_1 = \\sigma(\\theta_1)$ 和 $\\sigma_2 = \\sigma(\\theta_2)$。策略定义如下：\n- 在状态 $s_0$：$\\pi(\\text{right}\\mid s_0) = \\sigma_1$ 且 $\\pi(\\text{left}\\mid s_0) = 1-\\sigma_1$。\n- 在状态 $s_1$：$\\pi(\\text{risky}\\mid s_1) = \\sigma_2$ 且 $\\pi(\\text{safe}\\mid s_1) = 1-\\sigma_2$。\n\n折扣因子为 $\\gamma=1$。所有幕最多在两步内终止。\n\n**1. 期望分幕回报 $J(\\theta_1, \\theta_2)$ 的推导**\n\n期望回报 $J(\\theta_1, \\theta_2)$ 是起始状态 $s_0$ 的价值，我们可以将其表示为 $V^{\\pi}(s_0)$。我们可以通过考虑从 $s_0$ 开始的可能轨迹来计算它。\n\n从 $s_0$ 出发有两个初始选择：\n- 动作 'left'：以 $1-\\sigma_1$ 的概率采取此动作。该幕立即终止，奖励为 $2$。此路径对总期望回报的贡献为 $(1-\\sigma_1) \\times 2$。\n- 动作 'right'：以 $\\sigma_1$ 的概率采取此动作。智能体获得立即奖励 $0$ 并转移到状态 $s_1$。此路径的总回报是立即奖励加上后续状态的折扣价值。由于 $\\gamma=1$，这等于 $0 + 1 \\times V^{\\pi}(s_1)$。\n\n首先，我们必须计算从状态 $s_1$ 出发的期望回报 $V^{\\pi}(s_1)$。从 $s_1$ 开始，智能体可以采取两种动作：\n- 动作 'safe'：以 $1-\\sigma_2$ 的概率采取。该幕终止，奖励为 $0$。\n- 动作 'risky'：以 $\\sigma_2$ 的概率采取。该幕终止，有 $1/2$ 的概率获得奖励 $10$，有 $1/2$ 的概率获得奖励 $-1$。此动作的期望奖励为 $10 \\times \\frac{1}{2} + (-1) \\times \\frac{1}{2} = 5 - \\frac{1}{2} = 4.5$。\n\n因此，在策略 $\\pi$ 下状态 $s_1$ 的价值为：\n$$V^{\\pi}(s_1) = (1-\\sigma_2) \\times 0 + \\sigma_2 \\times (4.5) = 4.5\\sigma_2$$\n\n现在我们可以写出 $J(\\theta_1, \\theta_2) = V^{\\pi}(s_0)$ 的完整表达式：\n$$J(\\theta_1, \\theta_2) = (1-\\sigma_1) \\times 2 + \\sigma_1 \\times (0 + \\gamma V^{\\pi}(s_1))$$\n代入 $\\gamma=1$ 和 $V^{\\pi}(s_1)$ 的表达式：\n$$J(\\theta_1, \\theta_2) = 2(1-\\sigma_1) + \\sigma_1 (4.5\\sigma_2)$$\n$$J(\\theta_1, \\theta_2) = 2 - 2\\sigma_1 + 4.5\\sigma_1\\sigma_2$$\n将 $\\sigma_1$ 因子提出：\n$$J(\\theta_1, \\theta_2) = 2 + \\sigma_1(4.5\\sigma_2 - 2)$$\n\n**2. 偏导数的计算**\n\n为了计算关于 $\\theta_1$ 和 $\\theta_2$ 的偏导数，我们使用链式法则。我们首先回顾 logistic 函数 $\\sigma(x)$ 的导数：$\\frac{d\\sigma(x)}{dx} = \\sigma(x)(1-\\sigma(x))$。\n\n关于 $\\theta_1$ 的偏导数：\n$$\\frac{\\partial J}{\\partial \\theta_1} = \\frac{\\partial J}{\\partial \\sigma_1} \\frac{\\partial \\sigma_1}{\\partial \\theta_1}$$\n从 $J$ 的表达式，我们求得 $\\frac{\\partial J}{\\partial \\sigma_1}$：\n$$\\frac{\\partial J}{\\partial \\sigma_1} = \\frac{\\partial}{\\partial \\sigma_1} (2 - 2\\sigma_1 + 4.5\\sigma_1\\sigma_2) = -2 + 4.5\\sigma_2$$\n$\\sigma_1 = \\sigma(\\theta_1)$ 的导数是：\n$$\\frac{\\partial \\sigma_1}{\\partial \\theta_1} = \\sigma(\\theta_1)(1-\\sigma(\\theta_1)) = \\sigma_1(1-\\sigma_1)$$\n将它们结合起来得到：\n$$\\frac{\\partial J}{\\partial \\theta_1} = (4.5\\sigma_2 - 2) \\sigma_1(1-\\sigma_1)$$\n\n关于 $\\theta_2$ 的偏导数：\n$$\\frac{\\partial J}{\\partial \\theta_2} = \\frac{\\partial J}{\\partial \\sigma_2} \\frac{\\partial \\sigma_2}{\\partial \\theta_2}$$\n从 $J$ 的表达式，我们求得 $\\frac{\\partial J}{\\partial \\sigma_2}$：\n$$\\frac{\\partial J}{\\partial \\sigma_2} = \\frac{\\partial}{\\partial \\sigma_2} (2 - 2\\sigma_1 + 4.5\\sigma_1\\sigma_2) = 4.5\\sigma_1$$\n$\\sigma_2 = \\sigma(\\theta_2)$ 的导数是：\n$$\\frac{\\partial \\sigma_2}{\\partial \\theta_2} = \\sigma(\\theta_2)(1-\\sigma(\\theta_2)) = \\sigma_2(1-\\sigma_2)$$\n将它们结合起来得到：\n$$\\frac{\\partial J}{\\partial \\theta_2} = 4.5 \\sigma_1 \\sigma_2(1-\\sigma_2)$$\n\n**3. 驻点分析与次优收敛**\n\n驻点出现在梯度为零的地方，即 $\\frac{\\partial J}{\\partial \\theta_1} = 0$ 和 $\\frac{\\partial J}{\\partial \\theta_2} = 0$。\n\n从 $\\frac{\\partial J}{\\partial \\theta_1} = (4.5\\sigma_2 - 2) \\sigma_1(1-\\sigma_1) = 0$，我们有三种可能性：\n(a) $\\sigma_1 = 0$ (对应于 $\\theta_1 \\to -\\infty$)\n(b) $\\sigma_1 = 1$ (对应于 $\\theta_1 \\to +\\infty$)\n(c) $4.5\\sigma_2 - 2 = 0 \\implies \\sigma_2 = \\frac{2}{4.5} = \\frac{4}{9}$\n\n从 $\\frac{\\partial J}{\\partial \\theta_2} = 4.5 \\sigma_1 \\sigma_2(1-\\sigma_2) = 0$，我们有三种可能性：\n(d) $\\sigma_1 = 0$\n(e) $\\sigma_2 = 0$ (对应于 $\\theta_2 \\to -\\infty$)\n(f) $\\sigma_2 = 1$ (对应于 $\\theta_2 \\to +\\infty$)\n\n驻点是满足这两个方程的这些条件的组合。\n- 如果 $\\sigma_1=0$，则对于任意 $\\sigma_2$ 的值，两个方程都满足。这形成了一条驻点线。期望回报为 $J = 2 + 0 \\times (4.5\\sigma_2-2) = 2$。这对应于在 $s_0$ 始终选择 'left' 的确定性策略。\n- 如果 $\\sigma_1=1$，第一个方程得到满足。为了使第二个方程成立，我们需要 $\\sigma_2=0$ 或 $\\sigma_2=1$。\n  - $(\\sigma_1, \\sigma_2) = (1, 0)$：$J = 2 + 1(4.5(0)-2) = 0$。这是一个局部最小值。\n  - $(\\sigma_1, \\sigma_2) = (1, 1)$：$J = 2 + 1(4.5(1)-2) = 4.5$。这是全局最大值。\n- 如果 $\\sigma_1 \\in (0,1)$ 且 $\\sigma_2 \\in (0,1)$，那么要使梯度为零，我们需要 $4.5\\sigma_2 - 2 = 0$（来自方程1）和 $4.5\\sigma_1 = 0$（来自方程2）。第二个条件意味着 $\\sigma_1=0$，这与 $\\sigma_1 \\in (0,1)$ 矛盾。因此，没有内部驻点。\n\n全局最大回报为 $4.5$，当智能体总是走 'right' 然后选择 'risky' 时达到。然而，存在一个回报为 $2$ 的次优局部最大值（一条驻点线）。\n\n策略梯度上升的行为严重依赖于初始参数化，特别是 $\\theta_2$。$\\theta_1$ 的梯度为 $\\frac{\\partial J}{\\partial \\theta_1} \\propto (4.5\\sigma_2 - 2)$。\n- 如果初始时 $\\sigma_2 > 4/9$，那么 $4.5\\sigma_2 - 2 > 0$。梯度 $\\frac{\\partial J}{\\partial \\theta_1}$ 为正。策略梯度上升将增加 $\\theta_1$，导致 $\\sigma_1$ 增加。这鼓励了对状态 $s_1$ 的探索，使智能体能够学习到高潜在奖励，并最终收敛到最优策略 $(\\sigma_1, \\sigma_2) = (1,1)$。\n- 如果初始时 $\\sigma_2  4/9$，那么 $4.5\\sigma_2 - 2  0$。梯度 $\\frac{\\partial J}{\\partial \\theta_1}$ 为负。策略梯度上升将减小 $\\theta_1$，导致 $\\sigma_1$ 趋向于 $0$。这意味着智能体访问 $s_1$ 的可能性变小。随着 $\\sigma_1 \\to 0$，与 $\\sigma_1$ 成正比的 $\\theta_2$ 的梯度也消失了。对 $\\theta_2$ 的学习实际上停止了。智能体收敛到策略 $\\sigma_1=0$，获得一个稳定但次优的回报 $2$。它被困住了，因为它在 $s_1$ 的初始策略不够有前景，无法证明探索该状态比获得 $2$ 的保证奖励更合理。\n\n$\\theta_1$ 和 $\\theta_2$ 之间的耦合之所以产生，是因为 $\\theta_2$ 的梯度取决于到达 $s_1$ 的概率（即取决于 $\\sigma_1$），而 $\\theta_1$ 的梯度则取决于处于 $s_1$ 的期望价值（即取决于 $\\sigma_2$）。这造成了一个“鸡生蛋还是蛋生鸡”的问题，这是强化学习中探索挑战的典型特征。\n\n**4. 最终计算成果**\n\n我们需要找到在 $\\theta_1$ 为有限值时使 $\\frac{\\partial J}{\\partial \\theta_1}=0$ 的 $\\sigma(\\theta_2)$ 的值。有限的 $\\theta_1$ 意味着 $\\sigma(\\theta_1) = \\sigma_1$ 不在其饱和点 $0$ 或 $1$ 上。因此，$\\sigma_1(1-\\sigma_1) \\neq 0$。\n\n偏导数的表达式为：\n$$\\frac{\\partial J}{\\partial \\theta_1} = (4.5\\sigma_2-2) \\sigma_1(1-\\sigma_1)$$\n要使该式在 $\\sigma_1 \\in (0,1)$ 时为零，项 $(4.5\\sigma_2-2)$ 必须为零。\n$$4.5\\sigma_2 - 2 = 0$$\n求解 $\\sigma_2 = \\sigma(\\theta_2)$：\n$$4.5\\sigma_2 = 2$$\n$$\\frac{9}{2}\\sigma_2 = 2$$\n$$\\sigma_2 = 2 \\times \\frac{2}{9} = \\frac{4}{9}$$\n这是阈值。当状态 $s_1$ 的策略取此特定值时，状态 $s_0$ 的智能体对于选择 'left' 获得的保证奖励 $2$ 和选择 'right' 获得的期望奖励完全无差异，因为 $V^\\pi(s_1) = 4.5 \\times (4/9) = 2$。", "answer": "$$\\boxed{\\frac{4}{9}}$$", "id": "3169897"}, {"introduction": "对于复杂的现实世界问题，我们无法精确计算每个状态的价值，必须使用函数近似。本练习将探讨这一近似带来的后果。通过一个具体的例子 [@problem_id:3169919]，我们将看到一个看似合理的“贪心”策略改进步骤，当基于一个近似的价值函数时，反而可能导致策略性能显著下降，这打破了在精确求解情况下的收敛保证。", "problem": "考虑以下折扣因子为 $\\gamma=\\frac{9}{10}$ 的有限马尔可夫决策过程 (MDP)。状态空间为 $\\{s_0,s_1,s_2,s_{\\mathrm{T}}\\}$，其中 $s_{\\mathrm{T}}$ 是终止状态。起始状态为 $s_0$。动态和奖励是确定性的，定义如下：\n- 在 $s_0$ 中，有两个动作。动作 $A$ 产生即时奖励 $0$ 并转移到 $s_1$。动作 $B$ 产生即时奖励 $1$ 并转移到 $s_2$。\n- 在 $s_1$ 中，只有一个可用动作，其产生即时奖励 $10$ 并转移到 $s_{\\mathrm{T}}$。\n- 在 $s_2$ 中，只有一个可用动作，其产生即时奖励 $0$ 并转移到 $s_{\\mathrm{T}}$。\n- 在 $s_{\\mathrm{T}}$ 中，回合终止，不再收集奖励。\n\n设初始策略 $\\pi_0$ 在 $s_0$ 中选择动作 $A$（在 $s_1$ 和 $s_2$ 中的唯一选择是隐含的）。使用线性函数近似 (LFA) 对 $\\pi_0$ 进行策略评估，其特征映射 $\\phi:\\{s_0,s_1,s_2,s_{\\mathrm{T}}\\}\\to\\mathbb{R}$ 由 $\\phi(s)=1$ 对 $s\\in\\{s_0,s_1,s_2\\}$ 和 $\\phi(s_{\\mathrm{T}})=0$ 给出，近似器为 $\\hat{v}(s)=w\\,\\phi(s)$，参数为 $w\\in\\mathbb{R}$。定义拟合参数 $w^{\\star}$ 以最小化在非终止状态集 $\\{s_0,s_1,s_2\\}$ 上与 $\\pi_0$ 的真实状态值的均方误差，权重均匀，即 $w^{\\star}\\in\\arg\\min_{w}\\frac{1}{3}\\sum_{s\\in\\{s_0,s_1,s_2\\}}\\left(V^{\\pi_0}(s)-w\\right)^{2}$。\n\n使用这个拟合的 $\\hat{v}$，通过单步前瞻计算近似动作值，来定义在 $s_0$ 处的贪婪策略改进，\n$$\\hat{q}(s_0,a)\\;=\\;r(s_0,a)\\;+\\;\\gamma\\,\\hat{v}\\!\\left(s^{\\prime}\\right),$$\n其中 $s^{\\prime}$ 是在动作 $a\\in\\{A,B\\}$ 下的确定性下一状态。令 $\\pi_{\\mathrm{greedy}}$ 选择在 $\\{A,B\\}$ 中使 $\\hat{q}(s_0,a)$ 最大化的动作。计算由这次贪婪改进引起的在起始状态下的真实性能变化，\n$$\\Delta\\;=\\;V^{\\pi_{\\mathrm{greedy}}}(s_0)\\;-\\;V^{\\pi_0}(s_0),$$\n并将其确切值报告为单个实数。不要四舍五入；提供确切值。", "solution": "用户希望计算在使用线性函数近似进行一步策略改进后，起始状态的性能变化。该过程涉及几个步骤：首先，评估初始策略 $\\pi_0$；其次，将函数近似器拟合到这些真实值上；第三，使用近似器执行贪婪策略更新；第四，评估新策略；最后，计算起始状态下的价值差异。\n\n状态空间为 $\\mathcal{S}=\\{s_0,s_1,s_2,s_{\\mathrm{T}}\\}$，其中 $s_{\\mathrm{T}}$ 是一个终止状态。起始状态为 $s_0$。折扣因子为 $\\gamma=\\frac{9}{10}$。动态和奖励是确定性的。\n\n首先，我们为初始策略 $\\pi_0$ 计算真实的状态价值函数 $V^{\\pi_0}(s)$。策略 $\\pi_0$ 在状态 $s_0$ 中选择动作 $A$。在状态 $s_1$ 和 $s_2$ 中，动作是确定性的。终止状态的价值始终为 $V(s_{\\mathrm{T}})=0$。\n\n在任何策略下，状态 $s_1$ 的价值由唯一的可用动作决定，该动作产生奖励 $10$ 并转移到 $s_{\\mathrm{T}}$：\n$$V^{\\pi_0}(s_1) = R(s_1) + \\gamma V^{\\pi_0}(s_{\\mathrm{T}}) = 10 + \\gamma \\cdot 0 = 10$$\n\n在任何策略下，状态 $s_2$ 的价值由唯一的可用动作决定，该动作产生奖励 $0$ 并转移到 $s_{\\mathrm{T}}$：\n$$V^{\\pi_0}(s_2) = R(s_2) + \\gamma V^{\\pi_0}(s_{\\mathrm{T}}) = 0 + \\gamma \\cdot 0 = 0$$\n\n在策略 $\\pi_0$ 下，在状态 $s_0$ 中采取动作 $A$。这会产生奖励 $0$ 并转移到状态 $s_1$。因此，起始状态 $s_0$ 的价值为：\n$$V^{\\pi_0}(s_0) = R(s_0, A) + \\gamma V^{\\pi_0}(s_1) = 0 + \\frac{9}{10} \\cdot 10 = 9$$\n\n所以，在策略 $\\pi_0$ 下，非终止状态的真实价值为 $V^{\\pi_0}(s_0)=9$，$V^{\\pi_0}(s_1)=10$ 和 $V^{\\pi_0}(s_2)=0$。\n\n接下来，我们为线性函数近似器 $\\hat{v}(s) = w\\,\\phi(s)$ 找到参数 $w^{\\star}$，其中特征映射为 $\\phi(s)=1$ 对 $s\\in\\{s_0,s_1,s_2\\}$ 和 $\\phi(s_{\\mathrm{T}})=0$。这意味着对于所有非终止状态，$\\hat{v}(s)=w$。参数 $w^{\\star}$ 是通过最小化非终止状态上的均方误差 (MSE) 来找到的：\n$$J(w) = \\frac{1}{3}\\sum_{s\\in\\{s_0,s_1,s_2\\}} \\left(V^{\\pi_0}(s)-\\hat{v}(s)\\right)^{2} = \\frac{1}{3}\\left(\\left(V^{\\pi_0}(s_0)-w\\right)^{2} + \\left(V^{\\pi_0}(s_1)-w\\right)^{2} + \\left(V^{\\pi_0}(s_2)-w\\right)^{2}\\right)$$\n代入真实值：\n$$J(w) = \\frac{1}{3}\\left(\\left(9-w\\right)^{2} + \\left(10-w\\right)^{2} + \\left(0-w\\right)^{2}\\right)$$\n为了找到最小值，我们将关于 $w$ 的导数设为零：\n$$\\frac{dJ}{dw} = \\frac{1}{3}\\left(2(9-w)(-1) + 2(10-w)(-1) + 2w\\right) = 0$$\n$$-2(9-w) - 2(10-w) + 2w = 0$$\n$$-(9-w) - (10-w) + w = 0$$\n$$-9 + w - 10 + w + w = 0$$\n$$3w - 19 = 0$$\n$$w^{\\star} = \\frac{19}{3}$$\n这个结果是预料之中的，因为最小化平方误差和的 $w$ 值是目标值的平均值：$w^{\\star} = \\frac{9+10+0}{3} = \\frac{19}{3}$。\n\n因此，近似价值函数为 $\\hat{v}(s) = \\frac{19}{3}$ 对 $s \\in \\{s_0,s_1,s_2\\}$，且 $\\hat{v}(s_{\\mathrm{T}})=0$。\n\n现在，我们通过定义一个基于起始状态 $s_0$ 处的近似动作价值 $\\hat{q}(s_0,a)$ 的新贪婪策略 $\\pi_{\\mathrm{greedy}}$ 来执行策略改进。动作价值的公式是 $\\hat{q}(s_0,a) = r(s_0,a) + \\gamma\\,\\hat{v}(s^{\\prime})$。\n\n对于动作 $A$，奖励为 $r(s_0,A)=0$，下一状态为 $s^{\\prime}=s_1$：\n$$\\hat{q}(s_0,A) = 0 + \\gamma \\hat{v}(s_1) = \\frac{9}{10} \\cdot \\frac{19}{3} = \\frac{3 \\cdot 19}{10} = \\frac{57}{10} = 5.7$$\n\n对于动作 $B$，奖励为 $r(s_0,B)=1$，下一状态为 $s^{\\prime}=s_2$：\n$$\\hat{q}(s_0,B) = 1 + \\gamma \\hat{v}(s_2) = 1 + \\frac{9}{10} \\cdot \\frac{19}{3} = 1 + \\frac{57}{10} = \\frac{10}{10} + \\frac{57}{10} = \\frac{67}{10} = 6.7$$\n\n由于 $\\hat{q}(s_0,B) > \\hat{q}(s_0,A)$，贪婪策略 $\\pi_{\\mathrm{greedy}}$ 在状态 $s_0$ 中选择动作 $B$：\n$$\\pi_{\\mathrm{greedy}}(s_0) = \\arg\\max_{a\\in\\{A,B\\}} \\hat{q}(s_0,a) = B$$\n\n接下来，我们计算在这个新策略下起始状态的真实价值 $V^{\\pi_{\\mathrm{greedy}}}(s_0)$。使用策略 $\\pi_{\\mathrm{greedy}}$，在 $s_0$ 处采取动作 $B$，这会得到即时奖励 $1$ 并转移到 $s_2$。状态 $s_2$ 的价值不变，仍为 $V^{\\pi_{\\mathrm{greedy}}}(s_2)=0$。\n$$V^{\\pi_{\\mathrm{greedy}}}(s_0) = R(s_0, B) + \\gamma V^{\\pi_{\\mathrm{greedy}}}(s_2) = 1 + \\frac{9}{10} \\cdot 0 = 1$$\n\n最后，我们计算性能变化 $\\Delta$：\n$$\\Delta = V^{\\pi_{\\mathrm{greedy}}}(s_0) - V^{\\pi_0}(s_0)$$\n代入计算出的值：\n$$\\Delta = 1 - 9 = -8$$\n基于函数近似器的“贪婪”更新导致了一个比初始策略差得多的策略。这是近似强化学习中的一个已知问题，即价值函数近似中的误差可能会误导策略改进步骤。", "answer": "$$\\boxed{-8}$$", "id": "3169919"}, {"introduction": "除了迭代方法，我们还可以从一个完全不同的角度来精确求解马尔可夫决策过程（MDP）。本练习介绍 MDP 的线性规划（LP）表述，它将寻找最优策略的问题构建为一个约束优化问题 [@problem_id:3169918]。通过构建原始和对偶 LP，我们不仅能找到最优价值函数，还能通过“状态-动作占用度量”这一概念，对 MDP 的底层结构获得更深刻的理解。", "problem": "考虑一个折扣、无限时域的马尔可夫决策过程 (MDP)，其状态集有限且动作集是确定性的。该马尔可夫决策过程 (MDP) 由元组 $\\left(\\mathcal{S}, \\{\\mathcal{A}(s)\\}_{s \\in \\mathcal{S}}, P, r, \\gamma, d_0\\right)$ 指定，其中 $\\mathcal{S}$ 是有限的状态集，$\\mathcal{A}(s)$ 是在状态 $s$ 下可用的有限动作集，$P(s' \\mid s,a)$ 是转移核，$r(s,a)$ 是有界奖励函数，$\\gamma \\in (0,1)$ 是折扣因子，$d_0$ 是初始状态分布。对于一个策略 $\\pi$，其折扣占用度量 $\\rho^{\\pi}(s,a)$ 定义为\n$$\n\\rho^{\\pi}(s,a) \\;=\\; (1-\\gamma)\\sum_{t=0}^{\\infty}\\gamma^{t}\\,\\Pr^{\\pi}(s_t = s, a_t = a).\n$$\n初始分布为 $d_0(s_0) = 1$ 且 $d_0(s_1) = 0$，其中 $\\mathcal{S} = \\{s_0, s_1\\}$。动作集为 $\\mathcal{A}(s_0)=\\{a_0,a_1\\}$ 和 $\\mathcal{A}(s_1)=\\{b_0,b_1\\}$。动态和奖励如下：\n- 在 $s_0$：采取动作 $a_0$ 得到奖励 $r(s_0,a_0)=1$ 并以概率 $P(s_1 \\mid s_0,a_0)=1$ 转移到 $s_1$；采取动作 $a_1$ 得到奖励 $r(s_0,a_1)=0$ 并以概率 $P(s_0 \\mid s_0,a_1)=1$ 转移到 $s_0$。\n- 在 $s_1$：采取动作 $b_0$ 得到奖励 $r(s_1,b_0)=2$ 并以概率 $P(s_1 \\mid s_1,b_0)=1$ 转移到 $s_1$；采取动作 $b_1$ 得到奖励 $r(s_1,b_1)=-1$ 并以概率 $P(s_0 \\mid s_1,b_1)=1$ 转移到 $s_0$。\n\n任务：\n- 使用折扣占用度量的定义和马尔可夫性质下的概率流守恒，构建一个以变量 $\\rho(s,a)$ 进行优化的原始线性规划 (LP) 问题，以最大化缩放后的期望折扣回报。明确写出针对上述 MDP 的目标函数和所有约束条件，包括非负性约束。\n- 通过为每个状态引入一个对偶变量来推导对偶 LP，并说明对偶约束如何恢复状态价值函数最优性条件的一种形式。\n- 计算初始状态下的最优值 $V^{*}(s_0)$，结果表示为关于 $\\gamma$ 的封闭形式解析表达式。只需提供 $V^{*}(s_0)$ 的最终表达式作为答案。无需四舍五入。", "solution": "该问题要求为给定的马尔可夫决策过程 (MDP) 构建其原始和对偶线性规划 (LP) 表示，然后计算初始状态 $s_0$ 下的最优状态价值函数 $V^{*}(s_0)$。我们将按顺序解决每个部分。\n\n首先，我们构建原始 LP。该 LP 的变量是每个状态-动作对 $(s,a)$ 的折扣占用度量 $\\rho(s,a)$。目标是最大化缩放后的期望总折扣奖励，该奖励由各个奖励乘以其对应占用度量的加权和给出。约束条件强制执行状态占用概率流的守恒。\n\n要最大化的目标函数是：\n$$\n\\sum_{s \\in \\mathcal{S}} \\sum_{a \\in \\mathcal{A}(s)} r(s,a) \\rho(s,a)\n$$\n在我们的具体案例中，这表现为：\n$$\nZ_P = r(s_0, a_0)\\rho(s_0, a_0) + r(s_0, a_1)\\rho(s_0, a_1) + r(s_1, b_0)\\rho(s_1, b_0) + r(s_1, b_1)\\rho(s_1, b_1)\n$$\n代入给定的奖励值 $r(s_0,a_0)=1$、$r(s_0,a_1)=0$、$r(s_1,b_0)=2$ 和 $r(s_1,b_1)=-1$：\n$$\nZ_P = 1 \\cdot \\rho(s_0, a_0) + 0 \\cdot \\rho(s_0, a_1) + 2 \\cdot \\rho(s_1, b_0) - 1 \\cdot \\rho(s_1, b_1) = \\rho(s_0, a_0) + 2\\rho(s_1, b_0) - \\rho(s_1, b_1)\n$$\n约束条件源自占用度量的贝尔曼流方程，该方程指出，对于任何状态 $s'$，离开 $s'$ 的总占用度量等于初始占用度量加上从所有其他状态流入的总折扣占用度量。其一般形式为：\n$$\n\\sum_{a \\in \\mathcal{A}(s')} \\rho(s', a) = (1-\\gamma) d_0(s') + \\gamma \\sum_{s \\in \\mathcal{S}} \\sum_{a \\in \\mathcal{A}(s)} P(s' \\mid s, a) \\rho(s, a)\n$$\n对于我们的 MDP，我们有两个状态 $s_0$ 和 $s_1$。\n对于状态 $s_0$：\n给定 $d_0(s_0)=1$，$P(s_0 \\mid s_0,a_1)=1$ 和 $P(s_0 \\mid s_1,b_1)=1$。所有其他到 $s_0$ 的转移概率为 $0$。\n$$\n\\rho(s_0, a_0) + \\rho(s_0, a_1) = (1-\\gamma)(1) + \\gamma (P(s_0 \\mid s_0, a_1) \\rho(s_0, a_1) + P(s_0 \\mid s_1, b_1) \\rho(s_1, b_1))\n$$\n$$\n\\rho(s_0, a_0) + \\rho(s_0, a_1) = 1-\\gamma + \\gamma (\\rho(s_0, a_1) + \\rho(s_1, b_1))\n$$\n重新整理得到第一个约束条件：\n$$\n\\rho(s_0, a_0) + (1-\\gamma)\\rho(s_0, a_1) - \\gamma \\rho(s_1, b_1) = 1-\\gamma\n$$\n对于状态 $s_1$：\n给定 $d_0(s_1)=0$，$P(s_1 \\mid s_0,a_0)=1$ 和 $P(s_1 \\mid s_1,b_0)=1$。所有其他到 $s_1$ 的转移概率为 $0$。\n$$\n\\rho(s_1, b_0) + \\rho(s_1, b_1) = (1-\\gamma)(0) + \\gamma (P(s_1 \\mid s_0, a_0) \\rho(s_0, a_0) + P(s_1 \\mid s_1, b_0) \\rho(s_1, b_0))\n$$\n$$\n\\rho(s_1, b_0) + \\rho(s_1, b_1) = \\gamma (\\rho(s_0, a_0) + \\rho(s_1, b_0))\n$$\n重新整理得到第二个约束条件：\n$$\n-\\gamma \\rho(s_0, a_0) + (1-\\gamma)\\rho(s_1, b_0) + \\rho(s_1, b_1) = 0\n$$\n最后，占用度量必须是非负的：对于所有 $(s,a)$，$\\rho(s,a) \\geq 0$。\n\n完整的原始 LP 为：\n最大化 $\\rho(s_0, a_0) + 2\\rho(s_1, b_0) - \\rho(s_1, b_1)$\n在以下约束条件下：\n1. $\\rho(s_0, a_0) + (1-\\gamma)\\rho(s_0, a_1) - \\gamma \\rho(s_1, b_1) = 1-\\gamma$\n2. $-\\gamma \\rho(s_0, a_0) + (1-\\gamma)\\rho(s_1, b_0) + \\rho(s_1, b_1) = 0$\n3. $\\rho(s_0, a_0) \\geq 0$, $\\rho(s_0, a_1) \\geq 0$, $\\rho(s_1, b_0) \\geq 0$, $\\rho(s_1, b_1) \\geq 0$\n\n接下来，我们推导对偶 LP。我们为原始 LP 中的两个等式约束各引入一个对偶变量，记为 $V(s_0)$ 和 $V(s_1)$。对偶目标是最小化这些变量的线性组合，其系数由原始约束的右侧项给出：\n$$\nZ_D = (1-\\gamma)V(s_0) + 0 \\cdot V(s_1) = (1-\\gamma)V(s_0)\n$$\n对偶约束的形成方式是，确保对于每个原始变量 $\\rho(s,a)$，其在原始约束中的系数向量与对偶变量向量的内积，大于或等于其在原始目标函数中的系数。\n\n对于 $\\rho(s_0, a_0)$: $1 \\cdot V(s_0) - \\gamma \\cdot V(s_1) \\geq 1$\n对于 $\\rho(s_0, a_1)$: $(1-\\gamma)V(s_0) + 0 \\cdot V(s_1) \\geq 0 \\implies V(s_0) \\geq 0$ (因为 $1-\\gamma > 0$)\n对于 $\\rho(s_1, b_0)$: $0 \\cdot V(s_0) + (1-\\gamma)V(s_1) \\geq 2 \\implies V(s_1) \\geq \\frac{2}{1-\\gamma}$\n对于 $\\rho(s_1, b_1)$: $-\\gamma V(s_0) + 1 \\cdot V(s_1) \\geq -1$\n\n对偶 LP 为：\n最小化 $(1-\\gamma)V(s_0)$\n在以下约束条件下：\n1. $V(s_0) - \\gamma V(s_1) \\geq 1$\n2. $V(s_0) \\geq 0$\n3. $V(s_1) \\geq \\frac{2}{1-\\gamma}$\n4. $V(s_1) - \\gamma V(s_0) \\geq -1$\n\n这些对偶约束恢复了最优性条件的一种形式。让我们使用奖励和转移概率重写它们：\n1. $V(s_0) \\geq 1 + \\gamma V(s_1) = r(s_0, a_0) + \\gamma \\sum_{s'} P(s'|s_0, a_0)V(s')$\n2. $V(s_0) \\geq 0 = r(s_0, a_1) + \\gamma V(s_0) - \\gamma V(s_0) \\iff (1-\\gamma)V(s_0) \\geq 0$，即 $V(s_0) \\geq r(s_0,a_1)+\\gamma \\sum_{s'} P(s'|s_0, a_1)V(s')$\n3. $(1-\\gamma)V(s_1) \\geq 2 \\iff V(s_1) \\geq 2 + \\gamma V(s_1)$，即 $V(s_1) \\geq r(s_1, b_0) + \\gamma \\sum_{s'} P(s'|s_1, b_0)V(s')$\n4. $V(s_1) \\geq -1 + \\gamma V(s_0)$，即 $V(s_1) \\geq r(s_1, b_1) + \\gamma \\sum_{s'} P(s'|s_1, b_1)V(s')$\n每个约束都形如 $V(s) \\geq r(s,a) + \\gamma \\sum_{s'} P(s'|s,a)V(s')$。对于每个状态 $s$，针对所有动作 $a \\in \\mathcal{A}(s)$ 的约束集合等价于 $V(s) \\geq \\max_{a \\in \\mathcal{A}(s)}\\{r(s,a) + \\gamma \\sum_{s'} P(s'|s,a)V(s')\\}$。这正是条件 $V \\geq T^*V$，其中 $T^*$ 是贝尔曼最优算子。对偶 LP 在此条件下最小化 $V(s)$ 值的加权和，其解产生最优状态价值函数 $V^*$。\n\n最后，我们计算 $V^*(s_0)$。最优价值函数 $V^*$ 是贝尔曼最优算子的唯一不动点，满足 $V^* = T^*V^*$。我们写出该 MDP 的贝尔曼最优方程：\n$$\nV^*(s_0) = \\max \\left\\{ r(s_0,a_0) + \\gamma V^*(s_1), \\ r(s_0,a_1) + \\gamma V^*(s_0) \\right\\} = \\max \\left\\{ 1 + \\gamma V^*(s_1), \\ \\gamma V^*(s_0) \\right\\}\n$$\n$$\nV^*(s_1) = \\max \\left\\{ r(s_1,b_0) + \\gamma V^*(s_1), \\ r(s_1,b_1) + \\gamma V^*(s_0) \\right\\} = \\max \\left\\{ 2 + \\gamma V^*(s_1), \\ -1 + \\gamma V^*(s_0) \\right\\}\n$$\n考虑关于 $V^*(s_1)$ 的方程。项 $2 + \\gamma V^*(s_1)$ 代表采取动作 $b_0$ 并停留在状态 $s_1$ 的价值。如果这是最优选择，则 $V^*(s_1) = 2 + \\gamma V^*(s_1)$，这意味着 $(1-\\gamma)V^*(s_1) = 2$，即 $V^*(s_1) = \\frac{2}{1-\\gamma}$。这个值对应于每一步都获得奖励 2，并进行适当折扣。\n我们来分析每个动作在何种条件下是最优的。\n在状态 $s_1$，如果 $2 + \\gamma V^*(s_1) \\geq -1 + \\gamma V^*(s_0)$，则动作 $b_0$ 是最优的，这等价于 $V^*(s_1) = \\frac{2}{1-\\gamma}$。这在 $\\frac{2}{1-\\gamma} \\geq -1 + \\gamma V^*(s_0)$ 时成立。\n在状态 $s_0$，如果 $1 + \\gamma V^*(s_1) \\geq \\gamma V^*(s_0)$，则动作 $a_0$ 是最优的。由于 $V^*(s_0)$ 必须为正（因为可以获得奖励 1），我们知道 $V^*(s_0) > \\gamma V^*(s_0)$。因此，如果 $1 + \\gamma V^*(s_1) \\geq \\gamma V^*(s_0)$，则在 $s_0$ 的最优动作是 $a_0$。如果 $a_0$ 是最优的，那么 $V^*(s_0) = 1 + \\gamma V^*(s_1)$。\n\n让我们假设最优策略是 $\\pi^*(s_0) = a_0$ 和 $\\pi^*(s_1) = b_0$。那么我们有方程组：\n$$\nV^*(s_0) = 1 + \\gamma V^*(s_1)\n$$\n$$\nV^*(s_1) = 2 + \\gamma V^*(s_1) \\implies V^*(s_1) = \\frac{2}{1-\\gamma}\n$$\n将 $V^*(s_1)$ 的值代入 $V^*(s_0)$ 的方程中：\n$$\nV^*(s_0) = 1 + \\gamma \\left(\\frac{2}{1-\\gamma}\\right) = \\frac{1-\\gamma+2\\gamma}{1-\\gamma} = \\frac{1+\\gamma}{1-\\gamma}\n$$\n我们必须验证这个解与我们的假设是一致的。\n假设 $\\pi^*(s_1)=b_0$ 要求 $\\frac{2}{1-\\gamma} \\geq -1 + \\gamma V^*(s_0)$。代入 $V^*(s_0) = \\frac{1+\\gamma}{1-\\gamma}$：\n$$\n\\frac{2}{1-\\gamma} \\geq -1 + \\gamma \\frac{1+\\gamma}{1-\\gamma} = \\frac{-(1-\\gamma) + \\gamma(1+\\gamma)}{1-\\gamma} = \\frac{-1+2\\gamma+\\gamma^2}{1-\\gamma}\n$$\n由于 $1-\\gamma > 0$，这简化为 $2 \\geq -1+2\\gamma+\\gamma^2$，即 $\\gamma^2+2\\gamma-3 \\leq 0$。因式分解得到 $(\\gamma+3)(\\gamma-1) \\leq 0$。对于 $\\gamma \\in (0,1)$，$\\gamma+3 > 0$ 且 $\\gamma-1  0$，所以不等式成立。该假设是一致的。\n假设 $\\pi^*(s_0)=a_0$ 要求 $1 + \\gamma V^*(s_1) \\geq \\gamma V^*(s_0)$。我们有 $V^*(s_0) = 1 + \\gamma V^*(s_1)$，所以等式成立，该假设是一致的。\n因此，计算出的值是正确的。初始状态下的最优值是 $V^*(s_0)$。", "answer": "$$\\boxed{\\frac{1+\\gamma}{1-\\gamma}}$$", "id": "3169918"}]}