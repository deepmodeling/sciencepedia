{"hands_on_practices": [{"introduction": "为了理解RNN的计算能力，一个有效的方法是将其与我们熟悉的计算模型（如有限状态机，FSM）联系起来。这个练习 [@problem_id:3168429] 提供了一个具体的分析性任务：手动构建一个极简RNN的权重，使其隐藏状态的行为能够模拟一个特定的FSM。通过这个过程，我们将揭开隐藏状态的神秘面纱，展示其向量空间中的不同区域如何与FSM的离散状态一一对应。", "problem": "考虑字母表 $\\{a, b\\}$ 上的一个正则语言，该语言包含所有以符号 $a$ 结尾的字符串。一个识别该语言的确定性有限状态机 (FSM) 有两个状态：$q_a$（最后观测到的符号是 $a$）和 $q_b$（最后观测到的符号是 $b$）。其转移规则是：在时间 $t$ 看到输入符号 $x_t \\in \\{a, b\\}$ 时，如果 $x_t = a$，则下一个状态是 $q_a$；如果 $x_t = b$，则下一个状态是 $q_b$，这与前一个状态无关。\n\n你将构建一个简单的循环神经网络 (RNN) 来模拟这个 FSM，方法是将每个 FSM 状态映射到隐藏状态空间的一个不同区域，使得在每个时间 $t$，隐藏向量 $h_t \\in \\mathbb{R}^2$ 都位于与当前 FSM 状态相对应的区域内。使用以下基本定义作为你推导的基础：\n- Elman RNN 的循环公式为 $h_t = \\tanh(W_{hh} h_{t-1} + W_{xh} x_t + b_h)$，其中 $\\tanh(\\cdot)$ 是逐元素应用的，$W_{hh} \\in \\mathbb{R}^{2 \\times 2}$，$W_{xh} \\in \\mathbb{R}^{2 \\times 2}$，以及 $b_h \\in \\mathbb{R}^2$。\n- 独热输入编码：对于 $x_t = a$，$x_t = (1, 0)^{\\top}$；对于 $x_t = b$，$x_t = (0, 1)^{\\top}$。\n\n施加架构约束 $W_{hh} = 0$ 和 $b_h = 0$ 以最小化循环复杂度，并按如下方式定义状态到区域的映射：\n- 状态 $q_a$ 的区域：$\\mathcal{R}_a = \\{h \\in \\mathbb{R}^2 : h_1  0 \\text{ 且 } h_2  0\\}$。\n- 状态 $q_b$ 的区域：$\\mathcal{R}_b = \\{h \\in \\mathbb{R}^2 : h_1  0 \\text{ 且 } h_2  0\\}$。\n\n选择一个正增益参数 $\\alpha  0$，并要求预激活 $z_t = W_{xh} x_t$ 满足：当 $x_t = a$ 时 $z_t = \\alpha v_a$，当 $x_t = b$ 时 $z_t = \\alpha v_b$，其中 $v_a = (1, -1)^{\\top}$ 且 $v_b = (-1, 1)^{\\top}$。在这些约束下，非线性函数 $\\tanh(\\cdot)$ 会将 $h_t$ 驱入相应的区域 $\\mathcal{R}_a$ 或 $\\mathcal{R}_b$。\n\n根据第一性原理和上述定义，推导出能够实现此行为的显式矩阵 $W_{xh} \\in \\mathbb{R}^{2 \\times 2}$（用参数 $\\alpha$ 表示）。你的最终答案必须是单个矩阵 $W_{xh}$，表达式必须精确，不得四舍五入。", "solution": "该问题要求推导一个简单循环神经网络 (RNN) 的输入到隐藏层权重矩阵 $W_{xh}$，该网络用于模拟一个特定的确定性有限状态机 (FSM)。\n\n首先，我们对问题陈述进行验证。\n\n**步骤 1：提取已知条件**\n- **RNN 循环关系：** $h_t = \\tanh(W_{hh} h_{t-1} + W_{xh} x_t + b_h)$\n- **维度：** $h_t \\in \\mathbb{R}^2$，$W_{hh} \\in \\mathbb{R}^{2 \\times 2}$，$W_{xh} \\in \\mathbb{R}^{2 \\times 2}$，$b_h \\in \\mathbb{R}^2$。\n- **输入编码：** 对于输入符号 '$a$'，$x_t = (1, 0)^{\\top}$。对于输入符号 '$b$'，$x_t = (0, 1)^{\\top}$。\n- **架构约束：** $W_{hh} = 0$（$\\mathbb{R}^{2 \\times 2}$ 中的零矩阵）和 $b_h = 0$（$\\mathbb{R}^2$ 中的零向量）。\n- **状态到区域的映射：**\n  - 状态 $q_a$ 对应区域 $\\mathcal{R}_a = \\{h \\in \\mathbb{R}^2 : h_1  0 \\text{ 且 } h_2  0\\}$。\n  - 状态 $q_b$ 对应区域 $\\mathcal{R}_b = \\{h \\in \\mathbb{R}^2 : h_1  0 \\text{ 且 } h_2  0\\}$。\n- **预激活约束：** 预激活为 $z_t = W_{xh} x_t$。\n- **增益参数：** 一个正常数 $\\alpha  0$。\n- **预激活目标：**\n  - 如果 $x_t = a$，则 $z_t = \\alpha v_a$，其中 $v_a = (1, -1)^{\\top}$。\n  - 如果 $x_t = b$，则 $z_t = \\alpha v_b$，其中 $v_b = (-1, 1)^{\\top}$。\n- **目标：** 推导矩阵 $W_{xh}$。\n\n**步骤 2：使用提取的已知条件进行验证**\n该问题具有科学依据，提法恰当且客观。它提供了一套一致且完备的数学约束来确定一个唯一的矩阵 $W_{xh}$。该任务是理解 RNN 机制的一个标准练习。所描述的 FSM 很简单，其行为对于状态是无记忆的（仅取决于当前输入），这一点通过约束 $W_{hh} = 0$ 直接反映出来。所有术语都有正式定义。该问题是有效的。\n\n**步骤 3：结论与行动**\n问题有效。我们继续进行求解。\n\n推导从通用的 Elman RNN 循环关系开始：\n$$h_t = \\tanh(W_{hh} h_{t-1} + W_{xh} x_t + b_h)$$\n问题施加了约束 $W_{hh} = 0$ 和 $b_h = 0$。将这些代入循环关系可显著简化该式：\n$$h_t = \\tanh(0 \\cdot h_{t-1} + W_{xh} x_t + 0)$$\n$$h_t = \\tanh(W_{xh} x_t)$$\n逐元素 $\\tanh$ 函数内部的表达式是预激活，记作 $z_t = W_{xh} x_t$。因此，隐藏状态为 $h_t = \\tanh(z_t)$。\n\n目标是求矩阵 $W_{xh} \\in \\mathbb{R}^{2 \\times 2}$。让我们用其列向量 $c_1$ 和 $c_2$ 来表示 $W_{xh}$：\n$$W_{xh} = \\begin{pmatrix} c_1  c_2 \\end{pmatrix} = \\begin{pmatrix} w_{11}  w_{12} \\\\ w_{21}  w_{22} \\end{pmatrix}$$\n其中 $c_1 = \\begin{pmatrix} w_{11} \\\\ w_{21} \\end{pmatrix}$ 且 $c_2 = \\begin{pmatrix} w_{12} \\\\ w_{22} \\end{pmatrix}$。\n\n我们有两个基于输入符号的给定条件，可以用来确定这些列向量。\n\n**情况 1：输入符号为 '$a$'**\n输入向量是独热编码 $x_t = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$。我们计算预激活 $z_t$：\n$$z_t = W_{xh} x_t = \\begin{pmatrix} w_{11}  w_{12} \\\\ w_{21}  w_{22} \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} w_{11} \\\\ w_{21} \\end{pmatrix} = c_1$$\n问题规定，对于此输入，预激活必须是 $z_t = \\alpha v_a$，其中 $v_a = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$。\n因此，我们得到第一个列向量：\n$$c_1 = \\alpha v_a = \\alpha \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} \\alpha \\\\ -\\alpha \\end{pmatrix}$$\n\n**情况 2：输入符号为 '$b$'**\n输入向量是独热编码 $x_t = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$。我们计算预激活 $z_t$：\n$$z_t = W_{xh} x_t = \\begin{pmatrix} w_{11}  w_{12} \\\\ w_{21}  w_{22} \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} w_{12} \\\\ w_{22} \\end{pmatrix} = c_2$$\n问题规定，对于此输入，预激活必须是 $z_t = \\alpha v_b$，其中 $v_b = \\begin{pmatrix} -1 \\\\ 1 \\end{pmatrix}$。\n因此，我们得到第二个列向量：\n$$c_2 = \\alpha v_b = \\alpha \\begin{pmatrix} -1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} -\\alpha \\\\ \\alpha \\end{pmatrix}$$\n\n现在我们由列向量 $c_1$ 和 $c_2$ 组装矩阵 $W_{xh}$：\n$$W_{xh} = \\begin{pmatrix} c_1  c_2 \\end{pmatrix} = \\begin{pmatrix} \\alpha  -\\alpha \\\\ -\\alpha  \\alpha \\end{pmatrix}$$\n\n为了确认，我们来检查该矩阵是否能将隐藏状态 $h_t$ 正确地置于指定区域内。\n如果输入是 '$a$'，$x_t = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$，预激活为 $z_t = \\begin{pmatrix} \\alpha \\\\ -\\alpha \\end{pmatrix}$。隐藏状态为：\n$$h_t = \\tanh(z_t) = \\begin{pmatrix} \\tanh(\\alpha) \\\\ \\tanh(-\\alpha) \\end{pmatrix} = \\begin{pmatrix} \\tanh(\\alpha) \\\\ -\\tanh(\\alpha) \\end{pmatrix}$$\n因为 $\\alpha  0$，我们知道 $\\tanh(\\alpha)  0$。因此，$h_t$ 的第一个分量是正的（$h_1 = \\tanh(\\alpha)  0$），第二个分量是负的（$h_2 = -\\tanh(\\alpha)  0$）。这会将 $h_t$ 置于区域 $\\mathcal{R}_a$ 中，符合 FSM 状态 $q_a$ 的要求。\n\n如果输入是 '$b$'，$x_t = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$，预激活为 $z_t = \\begin{pmatrix} -\\alpha \\\\ \\alpha \\end{pmatrix}$。隐藏状态为：\n$$h_t = \\tanh(z_t) = \\begin{pmatrix} \\tanh(-\\alpha) \\\\ \\tanh(\\alpha) \\end{pmatrix} = \\begin{pmatrix} -\\tanh(\\alpha) \\\\ \\tanh(\\alpha) \\end{pmatrix}$$\n因为 $\\alpha  0$，$h_t$ 的第一个分量是负的（$h_1 = -\\tanh(\\alpha)  0$），第二个分量是正的（$h_2 = \\tanh(\\alpha)  0$）。这会将 $h_t$ 置于区域 $\\mathcal{R}_b$ 中，符合 FSM 状态 $q_b$ 的要求。\n\n推导出的矩阵 $W_{xh}$ 满足问题中指定的所有条件。RNN 的状态 $h_t$ 完全由当前输入 $x_t$ 决定，这正确地模拟了给定 FSM 的无记忆转移规则。", "answer": "$$\\boxed{\\begin{pmatrix} \\alpha  -\\alpha \\\\ -\\alpha  \\alpha \\end{pmatrix}}$$", "id": "3168429"}, {"introduction": "基于RNN可以模拟有限状态机的思想，这个实践 [@problem_id:3168367] 将探究当任务需要的能力可能超越FSM时，RNN的学习和泛化表现。我们将训练一个RNN来区分两种不同形式语言中的合法字符串，并测试其对未在训练中见过的更长序列的推断能力。这个经验性实验将具体揭示RNN在处理需要无限记忆（例如计数）的任务时所面临的实际挑战。", "problem": "您的任务是，从基本原理出发，设计并实现一个基于循环神经网络（RNN）的二元分类器，用于研究在字母表 $\\Sigma = \\{a,b\\}$ 上的两种形式语言中，对超出训练序列长度的外推能力。您将使用基于梯度的学习来实现经验风险最小化，并评估对于一个正则语言和一个上下文无关语言在更长序列上的外推能力。您的程序必须实现一个简单的、完全指定的 RNN，并为一个给定的测试套件报告定量的泛化指标。\n\n定义与建模假设：\n- 正则语言是 $L_{\\mathrm{reg}} = \\{ a^i b^j \\mid i \\ge 1, j \\ge 1 \\}$，它包含所有由正数个符号 $a$ 后跟正数个符号 $b$ 组成的字符串。\n- 上下文无关语言是 $L_{\\mathrm{cf}} = \\{ a^n b^n \\mid n \\ge 1 \\}$，它包含所有由恰好 $n$ 个符号 $a$ 后跟恰好 $n$ 个符号 $b$ 组成的字符串。\n- 一个带有隐藏状态 $\\mathbf{h}_t \\in \\mathbb{R}^H$ 的循环神经网络（RNN）处理一个由独热向量组成的序列 $(\\mathbf{x}_1,\\dots,\\mathbf{x}_T)$，其中 $\\mathbf{x}_t \\in \\{0,1\\}^2$，$\\mathbf{x}_t = [1,0]^\\top$ 代表 $a$，$\\mathbf{x}_t = [0,1]^\\top$ 代表 $b$。RNN 的状态更新公式为\n$$\n\\mathbf{h}_t = \\tanh\\!\\left(W_{xh}\\mathbf{x}_t + W_{hh}\\mathbf{h}_{t-1} + \\mathbf{b}_h\\right),\n$$\n其中 $\\mathbf{h}_0 = \\mathbf{0}$，$W_{xh} \\in \\mathbb{R}^{H \\times 2}$，$W_{hh} \\in \\mathbb{R}^{H \\times H}$，以及 $\\mathbf{b}_h \\in \\mathbb{R}^H$。序列级别的输出仅在最后一个时间步 $T$ 计算，其公式为\n$$\n\\hat{y} = \\sigma\\!\\left(\\mathbf{w}_{hy}^\\top \\mathbf{h}_T + b_y\\right),\n$$\n其中 $\\mathbf{w}_{hy} \\in \\mathbb{R}^H$，$b_y \\in \\mathbb{R}$，且 $\\sigma(z) = \\frac{1}{1+e^{-z}}$ 是 logistic sigmoid 函数。\n- 训练目标是最小化经验二元交叉熵\n$$\n\\mathcal{L} = -\\frac{1}{N}\\sum_{k=1}^N \\left[y^{(k)} \\log \\hat{y}^{(k)} + \\left(1 - y^{(k)}\\right) \\log \\left(1 - \\hat{y}^{(k)}\\right)\\right],\n$$\n该最小化在包含 $N$ 个标注序列的有限训练集 $\\{(\\mathbf{x}^{(k)}_{1:T_k}, y^{(k)})\\}_{k=1}^N$ 上进行，其中 $y^{(k)} \\in \\{0,1\\}$。优化必须通过随时间反向传播获得的基于梯度的更新来执行，且不使用任何自动微分库。\n\n数据生成协议：\n- 对于正则语言 $L_{\\mathrm{reg}}$，正例是字符串 $a^i b^j$，其中 $i \\in \\{1,\\dots,N_{\\mathrm{train}}\\}$ 且 $j \\in \\{1,\\dots,N_{\\mathrm{train}}\\}$。反例是字符串 $a^i b^j a^k$，其中 $i,j,k \\in \\{1,\\dots,N_{\\mathrm{train}}\\}$，这违反了单调排序约束。正例标签为 1，反例标签为 0。\n- 对于上下文无关语言 $L_{\\mathrm{cf}}$，正例是字符串 $a^n b^n$，其中 $n \\in \\{1,\\dots,N_{\\mathrm{train}}\\}$。对于每个 $n$，反例是 $a^n b^{n+1}$ 和 $a^{n+1} b^n$。正例标签为 1，反例标签为 0。\n- 为评估外推能力，测试集仅包含长度严格大于训练集最大长度的序列。对于 $L_{\\mathrm{reg}}$，测试正例是 $a^n b^n$，测试反例是 $a^n b^n a$，其中 $n \\in \\{N_{\\mathrm{train}}{+}1,\\dots,N_{\\mathrm{test}}\\}$。对于 $L_{\\mathrm{cf}}$，测试正例是 $a^n b^n$，测试反例是 $a^n b^{n+1}$ 和 $a^{n+1} b^n$，其中 $n \\in \\{N_{\\mathrm{train}}{+}1,\\dots,N_{\\mathrm{test}}\\}$。\n\n实现约束：\n- 仅使用 Python 标准库和指定的数值计算库。不允许使用外部数据集或用户输入。\n- 使用指定的单层 RNN，激活函数为 $\\tanh$，输出为 logistic sigmoid。为保证可复现性，应使用固定的随机种子，并采用随机梯度下降或小批量梯度下降，学习率调度策略需明确定义或自行实现一种自适应方法。\n\n需要计算的量：\n- 对于每个测试用例，在相应的训练集上训练 RNN，然后计算外推测试准确率，即在仅限于长度 $n \\in \\{N_{\\mathrm{train}}{+}1,\\dots,N_{\\mathrm{test}}\\}$ 的测试集上正确预测的比例。使用 $0.5$ 作为对 $\\hat{y}$ 的决策阈值来映射到类别标签。\n- 将最终的外推准确率报告为四舍五入到三位小数的浮点数。\n\n测试套件：\n- 测试用例 #1 (常规情况，正则语言)：语言类型 = $L_{\\mathrm{reg}}$，随机种子 = $1$，隐藏层大小 $H = 16$，训练最大长度 $N_{\\mathrm{train}} = 5$，测试最大长度 $N_{\\mathrm{test}} = 12$，训练轮次 = $300$，学习率 = $0.03$。\n- 测试用例 #2 (对比，上下文无关语言)：语言类型 = $L_{\\mathrm{cf}}$，随机种子 = $2$，隐藏层大小 $H = 16$，训练最大长度 $N_{\\mathrm{train}} = 5$，测试最大长度 $N_{\\mathrm{test}} = 12$，训练轮次 = $300$，学习率 = $0.03$。\n- 测试用例 #3 (容量边界，小隐藏层大小的正则语言)：语言类型 = $L_{\\mathrm{reg}}$，随机种子 = $3$，隐藏层大小 $H = 2$，训练最大长度 $N_{\\mathrm{train}} = 3$，测试最大长度 $N_{\\mathrm{test}} = 8$，训练轮次 = $300$，学习率 = $0.03$。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含按上述测试用例顺序排列的三个外推准确率，格式为用方括号括起来的逗号分隔列表，例如 `[0.975,0.533,0.912]`。数值必须精确到三位小数。", "solution": "我们从形式语言的基本定义和循环神经网络（RNN）的计算模型开始。正则语言 $L_{\\mathrm{reg}} = \\{ a^i b^j \\mid i \\ge 1, j \\ge 1 \\}$ 可被一个具有有限状态数的确定性有限自动机识别，该自动机强制执行单调排序约束：消耗一个或多个 $a$ 符号，然后消耗一个或多个 $b$ 符号，并拒绝任何在 $b$ 之后出现的 $a$。上下文无关语言 $L_{\\mathrm{cf}} = \\{ a^n b^n \\mid n \\ge 1 \\}$ 要求对 $a$ 和 $b$ 符号数量之差进行无界计数，这是有限自动机无法表示的。虽然循环神经网络是一个可微动态系统，原则上可以模拟各种行为，但实际训练和有限精度常常限制其在训练范围之外进行计数外推的能力。\n\n模型详述：RNN 维护一个隐藏状态 $\\mathbf{h}_t \\in \\mathbb{R}^H$，并处理表示当前符号的输入向量 $\\mathbf{x}_t \\in \\{0,1\\}^2$。状态更新公式为\n$$\n\\mathbf{h}_t = \\tanh\\!\\left(W_{xh}\\mathbf{x}_t + W_{hh}\\mathbf{h}_{t-1} + \\mathbf{b}_h\\right).\n$$\n序列级别的预测 $\\hat{y} \\in (0,1)$ 在最后一步产生，公式为\n$$\n\\hat{y} = \\sigma\\!\\left(\\mathbf{w}_{hy}^\\top \\mathbf{h}_T + b_y\\right),\n$$\n其中 $\\sigma(z) = \\frac{1}{1+e^{-z}}$。训练目标是经验二元交叉熵\n$$\n\\mathcal{L} = -\\frac{1}{N}\\sum_{k=1}^N \\left[y^{(k)} \\log \\hat{y}^{(k)} + \\left(1 - y^{(k)}\\right) \\log \\left(1 - \\hat{y}^{(k)}\\right)\\right].\n$$\n\n随时间反向传播：对于带有标签 $y \\in \\{0,1\\}$ 的单个序列，将时间步 $t$ 的预激活定义为\n$$\n\\mathbf{a}_t = W_{xh}\\mathbf{x}_t + W_{hh}\\mathbf{h}_{t-1} + \\mathbf{b}_h,\\quad \\mathbf{h}_t = \\tanh(\\mathbf{a}_t).\n$$\n令 $z = \\mathbf{w}_{hy}^\\top \\mathbf{h}_T + b_y$ 且 $\\hat{y} = \\sigma(z)$。损失函数对输出预激活的导数为\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial z} = \\hat{y} - y,\n$$\n由此得出\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{w}_{hy}} = (\\hat{y} - y)\\,\\mathbf{h}_T,\\quad \\frac{\\partial \\mathcal{L}}{\\partial b_y} = \\hat{y} - y.\n$$\n流入最终隐藏状态的梯度为\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{h}_T} = (\\hat{y} - y)\\,\\mathbf{w}_{hy}.\n$$\n使用 $\\frac{d}{d\\mathbf{a}} \\tanh(\\mathbf{a}) = \\mathbf{1} - \\tanh^2(\\mathbf{a})$，将时间步 $t$ 的局部误差信号定义为\n$$\n\\boldsymbol{\\delta}_t = \\left(\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{h}_t}\\right) \\odot \\left(\\mathbf{1} - \\mathbf{h}_t \\odot \\mathbf{h}_t\\right),\n$$\n其中 $\\odot$ 表示逐元素乘法。对于 $t = T$，\n$$\n\\boldsymbol{\\delta}_T = \\left((\\hat{y} - y)\\,\\mathbf{w}_{hy}\\right) \\odot \\left(\\mathbf{1} - \\mathbf{h}_T \\odot \\mathbf{h}_T\\right).\n$$\n循环权重、输入权重和偏置的梯度累积如下\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial W_{xh}} \\mathrel{+}= \\boldsymbol{\\delta}_t\\,\\mathbf{x}_t^\\top,\\quad\n\\frac{\\partial \\mathcal{L}}{\\partial W_{hh}} \\mathrel{+}= \\boldsymbol{\\delta}_t\\,\\mathbf{h}_{t-1}^\\top,\\quad\n\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{b}_h} \\mathrel{+}= \\boldsymbol{\\delta}_t.\n$$\n误差通过以下方式随时间反向传播\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{h}_{t-1}} = W_{hh}^\\top \\boldsymbol{\\delta}_t,\\quad\n\\boldsymbol{\\delta}_{t-1} = \\left(W_{hh}^\\top \\boldsymbol{\\delta}_t\\right) \\odot \\left(\\mathbf{1} - \\mathbf{h}_{t-1} \\odot \\mathbf{h}_{t-1}\\right).\n$$\n此递归从 $t = T, T-1, \\dots, 1$ 进行。可以对 $\\ell_2$ 范数应用梯度裁剪以稳定训练。\n\n优化：参数 $\\Theta = \\{W_{xh}, W_{hh}, \\mathbf{b}_h, \\mathbf{w}_{hy}, b_y\\}$ 通过一阶方法更新。像 Adam 这样的自适应矩估计方法为每个参数 $\\theta \\in \\Theta$ 维护一阶和二阶矩估计\n$$\n\\mathbf{m}_t = \\beta_1 \\mathbf{m}_{t-1} + (1 - \\beta_1)\\,\\nabla_\\theta \\mathcal{L},\\quad\n\\mathbf{v}_t = \\beta_2 \\mathbf{v}_{t-1} + (1 - \\beta_2)\\,\\left(\\nabla_\\theta \\mathcal{L}\\right)^2,\n$$\n带有偏差校正\n$$\n\\hat{\\mathbf{m}}_t = \\frac{\\mathbf{m}_t}{1 - \\beta_1^t},\\quad\n\\hat{\\mathbf{v}}_t = \\frac{\\mathbf{v}_t}{1 - \\beta_2^t},\n$$\n以及更新规则\n$$\n\\theta \\leftarrow \\theta - \\alpha \\frac{\\hat{\\mathbf{m}}_t}{\\sqrt{\\hat{\\mathbf{v}}_t} + \\epsilon},\n$$\n其中 $\\alpha$ 是学习率，$\\beta_1 \\in (0,1)$，$\\beta_2 \\in (0,1)$，且 $\\epsilon  0$。\n\n数据生成原理：对于 $L_{\\mathrm{reg}}$，正例强制执行单调块结构 $a^i b^j$（其中 $i \\ge 1$ 且 $j \\ge 1$），而反例 $a^i b^j a^k$（其中 $k \\ge 1$）明确违反了该结构，从而防止了简单的启发式方法。对于 $L_{\\mathrm{cf}}$，正例 $a^n b^n$ 要求计数相等；反例 $a^n b^{n+1}$ 和 $a^{n+1} b^n$ 是“差一点”的例子，迫使模型学习相等性，而不仅仅是排序或总长度。训练使用的序列长度最高为 $N_{\\mathrm{train}}$，而外推能力则在 $n \\in \\{N_{\\mathrm{train}}{+}1,\\dots,N_{\\mathrm{test}}\\}$ 上进行评估。\n\n预期结果与解释：由于定义 $L_{\\mathrm{reg}}$ 的属性是有限状态的，一个使用 $\\tanh$ 单元训练的 RNN 可以学习一个内部状态机，该状态机能区分是否在任何 $b$ 之后看到了 $a$，并凭借其状态动力学外推到更长的序列，从而产生高的外推准确率。相比之下，$L_{\\mathrm{cf}}$ 需要精确的无界计数来检查计数的相等性，对于一个仅在小的 $n$ 上训练的简单 RNN 来说，这种行为很难学习和外推。因此，我们预期在 $n  N_{\\mathrm{train}}$ 时，对 $L_{\\mathrm{cf}}$ 的外推准确率会较低。$H = 2$ 的容量边界情况进一步探究了最小隐藏维度是否足以应对正则模式，以及学习到的类状态机动力学的鲁棒性如何。\n\n算法流程：\n- 用小的随机值初始化参数，用零初始化矩估计。\n- 根据规定为 $L_{\\mathrm{reg}}$ 或 $L_{\\mathrm{cf}}$ 生成训练集，使用固定的随机种子以保证可复现性。\n- 在固定的训练轮次中，遍历（随机打乱的）训练集，执行前向传播计算 $\\hat{y}$，通过随时间反向传播计算损失和梯度，应用梯度裁剪，并通过 Adam 更新参数。\n- 训练后，为所选语言生成指定长度范围 $n \\in \\{N_{\\mathrm{train}}{+}1,\\dots,N_{\\mathrm{test}}\\}$ 的测试序列。使用 $0.5$ 的阈值计算预测结果，并计算正确预测的比例（准确率）。\n- 将每个测试用例的准确率四舍五入到三位小数，并以要求的聚合格式打印结果。\n\n此解决方案将 RNN 序列处理和基于梯度的学习的数学基础与形式语言上可凭经验检验的外推行为联系起来，通过测量 $n  N_{\\mathrm{train}}$ 时的准确率，可以直接比较正则泛化和上下文无关泛化的能力。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef sigmoid(x):\n    # Numerically stable sigmoid\n    # Clip to avoid overflow in exp\n    x = np.clip(x, -40.0, 40.0)\n    return 1.0 / (1.0 + np.exp(-x))\n\ndef one_hot_seq_a_b(count_a, count_b):\n    # 'a' - [1,0], 'b' - [0,1]\n    seq = []\n    for _ in range(count_a):\n        seq.append(np.array([1.0, 0.0], dtype=np.float64))\n    for _ in range(count_b):\n        seq.append(np.array([0.0, 1.0], dtype=np.float64))\n    return seq\n\ndef one_hot_seq_a_b_a(count_a, count_b, count_a2):\n    seq = []\n    for _ in range(count_a):\n        seq.append(np.array([1.0, 0.0], dtype=np.float64))\n    for _ in range(count_b):\n        seq.append(np.array([0.0, 1.0], dtype=np.float64))\n    for _ in range(count_a2):\n        seq.append(np.array([1.0, 0.0], dtype=np.float64))\n    return seq\n\ndef generate_dataset_regular(N_train):\n    # Positives: a^i b^j, i,j in 1..N_train\n    # Negatives: a^i b^j a^k, i,j,k in 1..N_train\n    data = []\n    # positives\n    for i in range(1, N_train + 1):\n        for j in range(1, N_train + 1):\n            seq = one_hot_seq_a_b(i, j)\n            data.append((seq, 1))\n    # negatives\n    for i in range(1, N_train + 1):\n        for j in range(1, N_train + 1):\n            for k in [1]:  # one violation per (i,j) to keep dataset small\n                seq = one_hot_seq_a_b_a(i, j, k)\n                data.append((seq, 0))\n    return data\n\ndef generate_dataset_cf(N_train):\n    # Positives: a^n b^n, n in 1..N_train\n    # Negatives: a^n b^{n+1}, a^{n+1} b^n\n    data = []\n    for n in range(1, N_train + 1):\n        data.append((one_hot_seq_a_b(n, n), 1))\n        data.append((one_hot_seq_a_b(n, n + 1), 0))\n        data.append((one_hot_seq_a_b(n + 1, n), 0))\n    return data\n\ndef generate_testset_regular(N_train, N_test):\n    # Test positives: a^n b^n, n in N_train+1..N_test\n    # Test negatives: a^n b^n a, n in N_train+1..N_test\n    data = []\n    for n in range(N_train + 1, N_test + 1):\n        data.append((one_hot_seq_a_b(n, n), 1))\n        data.append((one_hot_seq_a_b_a(n, n, 1), 0))\n    return data\n\ndef generate_testset_cf(N_train, N_test):\n    data = []\n    for n in range(N_train + 1, N_test + 1):\n        data.append((one_hot_seq_a_b(n, n), 1))\n        data.append((one_hot_seq_a_b(n, n + 1), 0))\n        data.append((one_hot_seq_a_b(n + 1, n), 0))\n    return data\n\nclass SimpleRNNBinary:\n    def __init__(self, input_dim, hidden_dim, seed=0):\n        rng = np.random.default_rng(seed)\n        self.H = hidden_dim\n        self.D = input_dim\n        # Parameter initialization (small random)\n        scale = 0.1\n        self.W_xh = rng.normal(0.0, scale, size=(self.H, self.D))\n        self.W_hh = rng.normal(0.0, scale, size=(self.H, self.H))\n        # Spectral stabilization for W_hh (optional small scaling)\n        # self.W_hh *= 0.9\n        self.b_h = np.zeros((self.H,), dtype=np.float64)\n        self.w_hy = rng.normal(0.0, scale, size=(self.H,))\n        self.b_y = 0.0\n\n        # Adam optimizer states\n        self.m = { 'W_xh': np.zeros_like(self.W_xh),\n                   'W_hh': np.zeros_like(self.W_hh),\n                   'b_h':  np.zeros_like(self.b_h),\n                   'w_hy': np.zeros_like(self.w_hy),\n                   'b_y':  0.0 }\n        self.v = { 'W_xh': np.zeros_like(self.W_xh),\n                   'W_hh': np.zeros_like(self.W_hh),\n                   'b_h':  np.zeros_like(self.b_h),\n                   'w_hy': np.zeros_like(self.w_hy),\n                   'b_y':  0.0 }\n        self.t = 0  # Adam time step\n\n    def forward(self, seq):\n        # seq: list of one-hot vectors (D,)\n        h_prev = np.zeros((self.H,), dtype=np.float64)\n        hs = []\n        for x_t in seq:\n            a_t = self.W_xh @ x_t + self.W_hh @ h_prev + self.b_h\n            h_t = np.tanh(a_t)\n            hs.append(h_t)\n            h_prev = h_t\n        h_T = hs[-1] if hs else np.zeros((self.H,), dtype=np.float64)\n        z = float(self.w_hy @ h_T + self.b_y)\n        y_hat = sigmoid(z)\n        cache = {'hs': hs, 'seq': seq, 'h0': np.zeros((self.H,), dtype=np.float64)}\n        return y_hat, cache\n\n    def bptt(self, y_hat, y_true, cache, grad_clip=5.0):\n        hs = cache['hs']\n        seq = cache['seq']\n        H = self.H\n\n        # Initialize grads\n        g_W_xh = np.zeros_like(self.W_xh)\n        g_W_hh = np.zeros_like(self.W_hh)\n        g_b_h  = np.zeros_like(self.b_h)\n        g_w_hy = np.zeros_like(self.w_hy)\n        g_b_y  = 0.0\n\n        # Output layer grads\n        dz = (y_hat - y_true)  # scalar\n        g_w_hy += dz * hs[-1]\n        g_b_y  += dz\n\n        # Backprop through time\n        dh_next = dz * self.w_hy  # gradient wrt h_T\n        for t in reversed(range(len(seq))):\n            h_t = hs[t]\n            # derivative of tanh\n            dtanh = (1.0 - h_t * h_t)\n            delta_t = dh_next * dtanh  # (H,)\n            x_t = seq[t]\n            h_prev = hs[t-1] if t > 0 else np.zeros_like(h_t)\n\n            # Accumulate grads\n            g_W_xh += np.outer(delta_t, x_t)\n            g_W_hh += np.outer(delta_t, h_prev)\n            g_b_h  += delta_t\n\n            # Propagate to previous hidden\n            dh_prev = self.W_hh.T @ delta_t\n            dh_next = dh_prev\n\n        # Gradient clipping\n        def clip_inplace(arr, clip):\n            norm = np.linalg.norm(arr)\n            if norm > clip and norm > 0.0:\n                arr *= (clip / norm)\n        clip_inplace(g_W_xh, grad_clip)\n        clip_inplace(g_W_hh, grad_clip)\n        clip_inplace(g_b_h,  grad_clip)\n        # Scalars/vectors clipping\n        if abs(g_b_y) > grad_clip:\n            g_b_y = np.sign(g_b_y) * grad_clip\n        # w_hy clip\n        w_norm = np.linalg.norm(g_w_hy)\n        if w_norm > grad_clip and w_norm > 0.0:\n            g_w_hy *= (grad_clip / w_norm)\n\n        grads = {\n            'W_xh': g_W_xh,\n            'W_hh': g_W_hh,\n            'b_h':  g_b_h,\n            'w_hy': g_w_hy,\n            'b_y':  g_b_y\n        }\n        return grads\n\n    def adam_update(self, grads, lr, beta1=0.9, beta2=0.999, eps=1e-8):\n        self.t += 1\n        for name in grads:\n            g = grads[name]\n            self.m[name] = beta1 * self.m[name] + (1 - beta1) * g\n            self.v[name] = beta2 * self.v[name] + (1 - beta2) * (g * g)\n            m_hat = self.m[name] / (1 - beta1 ** self.t)\n            v_hat = self.v[name] / (1 - beta2 ** self.t)\n            update = lr * (m_hat / (np.sqrt(v_hat) + eps))\n            if name == 'W_xh':\n                self.W_xh -= update\n            elif name == 'W_hh':\n                self.W_hh -= update\n            elif name == 'b_h':\n                self.b_h  -= update\n            elif name == 'w_hy':\n                self.w_hy -= update\n            elif name == 'b_y':\n                self.b_y  -= float(update)\n\ndef train_model(model, dataset, epochs, lr, seed):\n    rng = np.random.default_rng(seed)\n    # Shuffle and train with Adam\n    for epoch in range(epochs):\n        # Shuffle dataset order\n        indices = np.arange(len(dataset))\n        rng.shuffle(indices)\n        for idx in indices:\n            seq, y = dataset[idx]\n            y_hat, cache = model.forward(seq)\n            grads = model.bptt(y_hat, y, cache)\n            model.adam_update(grads, lr=lr)\n\ndef evaluate_accuracy(model, dataset):\n    correct = 0\n    total = 0\n    for seq, y in dataset:\n        y_hat, _ = model.forward(seq)\n        pred = 1 if y_hat >= 0.5 else 0\n        correct += int(pred == y)\n        total += 1\n    return correct / total if total > 0 else 0.0\n\ndef run_test_case(language_type, seed, hidden_size, N_train, N_test, epochs, lr):\n    # language_type: 'regular' or 'context_free'\n    rng = np.random.default_rng(seed)\n    model = SimpleRNNBinary(input_dim=2, hidden_dim=hidden_size, seed=seed)\n\n    if language_type == 'regular':\n        train_data = generate_dataset_regular(N_train)\n        test_data  = generate_testset_regular(N_train, N_test)\n    elif language_type == 'context_free':\n        train_data = generate_dataset_cf(N_train)\n        test_data  = generate_testset_cf(N_train, N_test)\n    else:\n        raise ValueError(\"Unknown language type.\")\n\n    train_model(model, train_data, epochs=epochs, lr=lr, seed=seed)\n    acc = evaluate_accuracy(model, test_data)\n    return acc\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each test case: (language_type, seed, hidden_size, N_train, N_test, epochs, lr)\n    test_cases = [\n        ('regular',       1, 16, 5, 12, 300, 0.03),\n        ('context_free',  2, 16, 5, 12, 300, 0.03),\n        ('regular',       3,  2, 3,  8, 300, 0.03),\n    ]\n\n    results = []\n    for case in test_cases:\n        language_type, seed, hidden_size, N_train, N_test, epochs, lr = case\n        acc = run_test_case(language_type, seed, hidden_size, N_train, N_test, epochs, lr)\n        # Round to three decimals\n        results.append(f\"{acc:.3f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3168367"}, {"introduction": "上一个练习展示了RNN在处理某些任务时的局限性。这最后一个实践 [@problem_id:3167589] 将深入探讨这些局限性的根本原因——在长时间跨度上维持信息的困难。我们将通过训练一个RNN来执行二进制加法，一个“进位”操作与信息传播高度相似的任务，不仅观察其失败，还将推导并实现一种估计网络有效记忆长度的方法。这将在RNN的参数和其处理长距离依赖的能力之间建立起一个定量的联系。", "problem": "要求您设计、分析并实现一个完整的程序，该程序构建并训练一个循环神经网络 (RNN) 来模拟一个简单的算法任务（二进制加法），并诊断因内存有限而产生的原则性失败案例。该任务必须纯粹用数学术语来描述，并使用统计学习和循环神经网络的基本原理解決。您的程序的最终输出必须将来自一个固定测试套件的结果汇总为指定格式的单行文本。\n\n此问题的根本基础是标准 Elman 循环神经网络的定义以及二进制加法的因果结构。考虑一个带有双曲正切激活函数的循环神经网络 (RNN)，其中隐藏状态的更新由以下递归关系定义：\n$$\n\\mathbf{h}_t = \\tanh\\!\\Big(\\mathbf{W}_{hh} \\mathbf{h}_{t-1} + \\mathbf{W}_{xh} \\mathbf{x}_t + \\mathbf{b}_h\\Big),\n$$\n其输出 logits 为：\n$$\n\\mathbf{o}_t = \\mathbf{W}_{hy} \\mathbf{h}_t + \\mathbf{b}_y,\n$$\n概率输出为：\n$$\n\\mathbf{y}_t = \\sigma(\\mathbf{o}_t),\n$$\n其中 $\\sigma(\\cdot)$ 表示逐元素的逻辑S型函数。在时间 $t$ 的输入是一对位 $\\mathbf{x}_t = (a_t, b_t)$，其中 $a_t \\in \\{0,1\\}$ 和 $b_t \\in \\{0,1\\}$，从最低有效位（$t = 0$）扫描到最高有效位（$t = T-1$）。网络在每个时间步必须输出两个位：和位 $s_t$ 和进位输出位 $c_t$。真实目标 $(s_t, c_t)$ 是根据因果加法规则从基本原理生成的：初始化 $c_{-1} = 0$，并在每个时间步 $t$ 计算：\n$$\nu_t = a_t + b_t + c_{t-1}, \\quad s_t = u_t \\bmod 2, \\quad c_t = \\left\\lfloor \\frac{u_t}{2} \\right\\rfloor.\n$$\n经过 $T$ 步之后，完整的和可以恢复为：\n$$\nS = \\sum_{t=0}^{T-1} s_t 2^t + c_{T-1} \\cdot 2^T.\n$$\n\n您的程序必须执行以下操作：\n\n- 实现并训练上述 RNN，使其在随机生成的 $T_{\\text{train}}$-位整数对上模拟按位加法算法，其中 $T_{\\text{train}} = 8$。使用在时间和输出上求和的二元交叉熵损失。使用带有梯度裁剪的随机梯度下降。初始化 $\\mathbf{h}_{-1} = \\mathbf{0}$。在每次参数更新后，强制执行谱半径约束：\n$$\n\\rho\\big(\\mathbf{W}_{hh}\\big) \\le \\rho_{\\max},\n$$\n其中 $\\rho_{\\max} = 0.7$，必要时通过重新缩放 $\\mathbf{W}_{hh}$ 来实现。此约束是强制性的，旨在引入有限的记忆长度。\n\n- 从基本定义出发，推导并实现一个对已训练网络有效记忆长度 $L_{\\varepsilon}$ 的原则性估计器。该长度表示为最小整数 $L$，使得 $L$ 步之前的信息对当前隐藏状态的影响被一个容差 $\\varepsilon$ 所限制。您的推导必须从 RNN 的递归关系开始，通过雅可比矩阵乘积来表达隐藏状态之间的影响，并使用基于范数或谱半径的界来获得一个可以用学习到的参数和平均激活导数计算的估计器 $L_{\\varepsilon}$。在代码中，设置 $\\varepsilon = 0.05$。\n\n- 将特定输入对 $(\\{a_t\\}, \\{b_t\\})$ 的进位链长度定义为，从最低有效位到最高有效位按因果关系计算时，运行中的进位位等于 $1$ 的最大连续时间步数。也就是说，如果 $\\{c_t\\}$ 是由上述真实加法规则产生的进位序列，则进位链长度为：\n$$\n\\max_{i \\le j} \\Big\\{ (j-i+1) \\; \\text{such that} \\; c_i = c_{i+1} = \\cdots = c_j = 1 \\Big\\}.\n$$\n\n- 通过比较估计的记忆长度 $L_{\\varepsilon}$ 与输入的进位链长度，来预测给定输入对的充分性与失败。如果 $L_{\\varepsilon}$ 大于或等于进位链长度，则预测“充分”；否则，预测“不充分”。然后，通过计算每个时间步输出的完整预测和，并将其与真实的整数和进行比较，来对该输入对上的已训练 RNN 进行经验性评估。\n\n- 测试套件。对所有测试输入使用固定的序列长度 $T = 12$ 位，并在以下五个非负整数输入对上评估已训练的网络，每个整数都用 $12$ 位表示（在计算过程中最低有效位在前）：\n    - 情况 $1$（理想路径）：$(A,B) = (25, 6)$。\n    - 情况 $2$（边界邻近）：$(A,B) = (255, 1)$。\n    - 情况 $3$（超越边界）：$(A,B) = (1023, 1)$。\n    - 情况 $4$（零的边缘情况）：$(A,B) = (0, 0)$。\n    - 情况 $5$（极端进位传播）：$(A,B) = (4095, 1)$。\n  选择这些输入是为了检验关于进位传播的典型行为、边界条件和重要的边缘情况。\n\n- 最终输出格式。您的程序应生成一个单行输出，其中包含一个用方括号括起来的逗号分隔列表的结果。对于每个测试用例，按顺序输出两个布尔值：\n    1. 第一个布尔值表示估计的记忆长度 $L_{\\varepsilon}$ 是否足以应对该情况下的最大进位链长度。\n    2. 第二个布尔值表示已训练的 RNN 是否为该情况生成了完全正确的整数和。\n  因此，最后一行必须总共包含 $10$ 个布尔值，格式如下：\n$$\n[p_1,a_1,p_2,a_2,p_3,a_3,p_4,a_4,p_5,a_5],\n$$\n其中 $p_i$ 是预测的充分性，而 $a_i$ 是情况 $i$ 的实际正确性。\n\n本规范中的所有数学量和数字都必须按其标准数学意义进行解释，并且在适用的情况下，答案必须表示为布尔值并完全按照定义进行评估。此问题不涉及任何物理单位或角度单位。", "solution": "该问题要求设计、实现并分析一个循环神经网络 (RNN) 来执行二进制加法。问题的关键在于从理论上和经验上研究网络的记忆限制，这些限制是通过对循环权重矩阵施加谱半径约束来有意引入的。解决方案主要分三个阶段进行：首先，定义模型和训练过程；其次，推导出一个对网络有效记忆长度的原则性估计器；第三，实现完整的系统，以检验以下假设：通过将此记忆长度与任务所需的进位传播长度进行比较，可以预测网络在长进位加法上的失败。\n\n### 1. RNN 模型与二进制加法任务\n\nRNN 架构是一个标准的 Elman 网络。隐藏状态 $\\mathbf{h}_t \\in \\mathbb{R}^{d_h}$ 根据以下递归关系演化：\n$$\n\\mathbf{h}_t = \\tanh(\\mathbf{W}_{hh} \\mathbf{h}_{t-1} + \\mathbf{W}_{xh} \\mathbf{x}_t + \\mathbf{b}_h)\n$$\n其中 $\\mathbf{x}_t \\in \\{0,1\\}^2$ 是代表两个位 $(a_t, b_t)$ 的输入向量，$\\mathbf{W}_{hh} \\in \\mathbb{R}^{d_h \\times d_h}$，$\\mathbf{W}_{xh} \\in \\mathbb{R}^{d_h \\times 2}$，以及 $\\mathbf{b}_h \\in \\mathbb{R}^{d_h}$ 是参数。初始隐藏状态为 $\\mathbf{h}_{-1} = \\mathbf{0}$。\n\n在每个时间步 $t$，网络产生输出 logits $\\mathbf{o}_t \\in \\mathbb{R}^2$ 和概率输出 $\\mathbf{y}_t \\in (0,1)^2$：\n$$\n\\mathbf{o}_t = \\mathbf{W}_{hy} \\mathbf{h}_t + \\mathbf{b}_y\n$$\n$$\n\\mathbf{y}_t = \\sigma(\\mathbf{o}_t)\n$$\n其中 $\\mathbf{W}_{hy} \\in \\mathbb{R}^{2 \\times d_h}$，$\\mathbf{b}_y \\in \\mathbb{R}^2$，而 $\\sigma(\\cdot)$ 是逐元素的逻辑S型函数。$\\mathbf{y}_t$ 的两个分量是网络对和位 $s_t$ 和进位输出位 $c_t$ 的概率性预测。\n\n训练的真实标签（ground truth）由二进制加法规则生成。给定输入位 $(a_t, b_t)$ 和输入进位 $c_{t-1}$（其中 $c_{-1}=0$），真实和位 $s_t$ 和进位输出位 $c_t$ 为：\n$$\nu_t = a_t + b_t + c_{t-1}\n$$\n$$\ns_t = u_t \\pmod 2\n$$\n$$\nc_t = \\left\\lfloor \\frac{u_t}{2} \\right\\rfloor\n$$\n\n### 2. 训练过程\n\n训练网络以最小化其预测 $\\mathbf{y}_t = (y_{t,s}, y_{t,c})$ 与真实目标 $(s_t, c_t)$ 之间在长度为 $T$ 的序列上的总二元交叉熵 (BCE) 损失：\n$$\nL = -\\sum_{t=0}^{T-1} \\left[ s_t \\log(y_{t,s}) + (1-s_t) \\log(1-y_{t,s}) + c_t \\log(y_{t,c}) + (1-c_t) \\log(1-y_{t,c}) \\right]\n$$\n参数使用随机梯度下降 (SGD) 进行更新，梯度通过随时间反向传播 (BPTT) 计算。为防止梯度爆炸，所有梯度都被裁剪到最大范数。训练的一个关键特征是对循环权重矩阵强制执行谱半径约束，$\\rho(\\mathbf{W}_{hh}) \\le \\rho_{\\max}$，其中 $\\rho(\\cdot)$ 表示谱半径（最大绝对特征值）。如果在每次梯度更新后违反了该约束，则通过重新缩放 $\\mathbf{W}_{hh}$ 来实现：\n$$\n\\text{if } \\rho(\\mathbf{W}_{hh})  \\rho_{\\max}, \\quad \\mathbf{W}_{hh} \\leftarrow \\mathbf{W}_{hh} \\frac{\\rho_{\\max}}{\\rho(\\mathbf{W}_{hh})}\n$$\n这个约束限制了 RNN 的长期记忆能力，因为它迫使循环动态的线性部分是收缩的。对于此问题，我们使用 $\\rho_{\\max} = 0.7$ 并在 $T_{\\text{train}}=8$ 位的数字上进行训练。诸如隐藏维度、学习率和训练周期等超参数必须被适当地选择；我们选择 $d_h=4$，学习率为 $\\eta=0.05$，训练周期为 $3000$ 次。\n\n### 3. 有效记忆长度 $L_{\\varepsilon}$ 的推导\n\n有效记忆长度 $L_{\\varepsilon}$ 是信息可以传播的时间步数，直到其影响衰减到阈值 $\\varepsilon$ 以下。我们可以通过分析时间 $t$ 的隐藏状态相对于过去某个时间 $t-L$ 的隐藏状态的雅可比矩阵 $\\frac{\\partial \\mathbf{h}_t}{\\partial \\mathbf{h}_{t-L}}$ 来将其形式化。\n\n在递归关系上使用链式法则：\n$$\n\\frac{\\partial \\mathbf{h}_k}{\\partial \\mathbf{h}_{k-1}} = \\frac{\\partial \\tanh(\\mathbf{z}_k)}{\\partial \\mathbf{z}_k} \\frac{\\partial \\mathbf{z}_k}{\\partial \\mathbf{h}_{k-1}} = \\text{diag}(1-\\tanh^2(\\mathbf{z}_k)) \\cdot \\mathbf{W}_{hh} = \\mathbf{D}_k \\mathbf{W}_{hh}\n$$\n其中 $\\mathbf{z}_k = \\mathbf{W}_{hh} \\mathbf{h}_{k-1} + \\dots$ 是时间 $k$ 的激活前的值，而 $\\mathbf{D}_k$ 是激活导数的对角矩阵。完整的雅可比矩阵是这些项的乘积：\n$$\n\\frac{\\partial \\mathbf{h}_t}{\\partial \\mathbf{h}_{t-L}} = \\frac{\\partial \\mathbf{h}_t}{\\partial \\mathbf{h}_{t-1}} \\frac{\\partial \\mathbf{h}_{t-1}}{\\partial \\mathbf{h}_{t-2}} \\cdots \\frac{\\partial \\mathbf{h}_{t-L+1}}{\\partial \\mathbf{h}_{t-L}} = \\prod_{k=t-L+1}^{t} (\\mathbf{D}_k \\mathbf{W}_{hh})\n$$\n该雅可比矩阵的范数限定了影响的大小。为了获得一个易于处理的估计器，我们做两个近似。首先，我们用一个标量矩阵近似 $\\bar{d} \\mathbf{I}$ 来代替时变矩阵 $\\mathbf{D}_k$，其中 $\\bar{d}$ 是从训练数据样本中估计出的、在所有隐藏单元和时间步上导数 $1-\\tanh^2(z)$ 的平均值。其次，我们利用这样一个性质：对于大的 $L$，矩阵幂的范数 $\\|(\\bar{d}\\mathbf{W}_{hh})^L\\|$ 主要由其谱半径 $\\rho(\\bar{d}\\mathbf{W}_{hh})^L$ 决定。\n因此，经过 $L$ 步的影响衰减因子可近似为 $(\\bar{d} \\cdot \\rho(\\mathbf{W}_{hh}))^L$。我们寻求使该值小于或等于 $\\varepsilon$ 的最小整数 $L$：\n$$\n(\\bar{d} \\cdot \\rho(\\mathbf{W}_{hh}))^L \\le \\varepsilon\n$$\n对两边取对数来解出 $L$：\n$$\nL \\log(\\bar{d} \\cdot \\rho(\\mathbf{W}_{hh})) \\le \\log(\\varepsilon)\n$$\n由于 $\\rho(\\mathbf{W}_{hh}) \\le \\rho_{\\max} = 0.7  1$ 且 $\\bar{d} \\le 1$，项 $\\bar{d} \\cdot \\rho(\\mathbf{W}_{hh})$ 小于 $1$，使其对数为负。除以这个负数会反转不等号：\n$$\nL \\ge \\frac{\\log(\\varepsilon)}{\\log(\\bar{d} \\cdot \\rho(\\mathbf{W}_{hh}))}\n$$\n有效记忆长度 $L_\\varepsilon$ 的估计器是满足此条件的最小整数：\n$$\nL_{\\varepsilon} = \\left\\lceil \\frac{\\log(\\varepsilon)}{\\log(\\bar{d} \\cdot \\rho(\\mathbf{W}_{hh}))} \\right\\rceil\n$$\n在实现中，我们使用 $\\varepsilon = 0.05$。一个较长的 $L_\\varepsilon$ 意味着网络可以维持信息更长的时间。\n\n### 4. 失败预测与评估\n\n二进制加法任务的记忆需求由其最长进位链决定。一个输入对的“进位链长度”是真实进位输出位 $c_t$ 为 $1$ 的最大连续时间步数。为了让网络正确执行加法，其有效记忆长度 $L_{\\varepsilon}$ 必须至少与所需的进位链长度 $L_{\\text{carry}}$ 一样长。这给了我们一个预测规则：\n- 如果 $L_{\\varepsilon} \\ge L_{\\text{carry}}$，预测“充分”。\n- 如果 $L_{\\varepsilon}  L_{\\text{carry}}$，预测“不充分”。\n\n然后将此预测与已训练 RNN 在测试用例上的实际性能进行比较。经验正确性是通过将测试整数的 $T=12$ 位表示输入网络，将其输出 $(y_{t,s}, y_{t,c})$ 二值化以得到预测位 $(s'_t, c'_t)$，计算预测的整数和 $S_{\\text{pred}}$，并将其与真实和 $S_{\\text{true}}$ 进行比较来确定的。\n$$\nS_{\\text{pred}} = \\sum_{t=0}^{T-1} s'_t 2^t + c'_{T-1} 2^T\n$$\n当且仅当 $S_{\\text{pred}} = S_{\\text{true}}$ 时，网络性能被视为正确。最终输出汇总了给定测试套件的这些比较结果。", "answer": "```python\nimport numpy as np\nimport scipy.linalg\nimport math\n\n# Use a fixed random seed for reproducibility of training.\nnp.random.seed(42)\n\ndef solve():\n    \"\"\"\n    Main function to construct, train, and evaluate the RNN for binary addition.\n    \"\"\"\n\n    # --- Configuration ---\n    # RNN architecture\n    INPUT_DIM = 2\n    HIDDEN_DIM = 4  # A small hidden size is sufficient for this task.\n    OUTPUT_DIM = 2\n\n    # Training parameters\n    LEARNING_RATE = 0.05\n    N_EPOCHS = 3000\n    GRAD_CLIP_THRESHOLD = 1.0\n    T_TRAIN = 8  # Train on 8-bit numbers.\n\n    # Memory analysis parameters\n    RHO_MAX = 0.7\n    EPSILON = 0.05\n\n    # Testing parameters\n    T_TEST = 12 # Test on 12-bit numbers.\n\n    # --- Helper Functions ---\n    def int_to_binary_array(n, num_bits):\n        binary_str = format(n, f'0{num_bits}b')\n        return np.array([int(bit) for bit in reversed(binary_str)], dtype=np.float64)\n\n    def get_true_targets(a_bits, b_bits):\n        seq_len = len(a_bits)\n        s_bits = np.zeros(seq_len, dtype=np.float64)\n        c_bits = np.zeros(seq_len, dtype=np.float64)\n        carry_in = 0\n        for t in range(seq_len):\n            u_t = a_bits[t] + b_bits[t] + carry_in\n            s_bits[t] = u_t % 2\n            c_bits[t] = u_t // 2\n            carry_in = c_bits[t]\n        return s_bits, c_bits\n\n    class RNN:\n        \"\"\"A simple Elman Recurrent Neural Network.\"\"\"\n        def __init__(self, input_dim, hidden_dim, output_dim):\n            # Xavier/Glorot initialization for weights\n            limit_xh = np.sqrt(6.0 / (input_dim + hidden_dim))\n            limit_hh = np.sqrt(6.0 / (hidden_dim + hidden_dim))\n            limit_hy = np.sqrt(6.0 / (hidden_dim + output_dim))\n            \n            self.W_xh = np.random.uniform(-limit_xh, limit_xh, (hidden_dim, input_dim))\n            self.W_hh = np.random.uniform(-limit_hh, limit_hh, (hidden_dim, hidden_dim))\n            self.W_hy = np.random.uniform(-limit_hy, limit_hy, (output_dim, hidden_dim))\n            \n            self.b_h = np.zeros((hidden_dim, 1))\n            self.b_y = np.zeros((output_dim, 1))\n\n        def _sigmoid(self, x):\n            return 1.0 / (1.0 + np.exp(-x))\n\n        def forward(self, x_seq):\n            seq_len = x_seq.shape[0]\n            h = np.zeros((self.W_hh.shape[0], 1))\n            \n            history = {\n                'h': { -1: h }, # Store h_t values, key is time index\n                'y': [],\n                'deriv_activations': []\n            }\n            \n            for t in range(seq_len):\n                x_t = x_seq[t].reshape(-1, 1)\n                h_pre_act = self.W_xh @ x_t + self.W_hh @ h + self.b_h\n                h = np.tanh(h_pre_act)\n                o_t = self.W_hy @ h + self.b_y\n                y_t = self._sigmoid(o_t)\n                \n                history['h'][t] = h\n                history['y'].append(y_t)\n                history['deriv_activations'].append(1 - h**2)\n                \n            return history\n\n        def rescale_W_hh(self, rho_max):\n            eigenvalues = scipy.linalg.eigvals(self.W_hh)\n            rho = np.max(np.abs(eigenvalues))\n            if rho > rho_max:\n                self.W_hh *= rho_max / rho\n\n    def train_rnn(model, num_epochs, seq_len, learning_rate, clip_threshold, rho_max):\n        \"\"\"Train the RNN model using SGD and BPTT.\"\"\"\n        for epoch in range(num_epochs):\n            # Generate a random training sample\n            a_int = np.random.randint(0, 2**seq_len)\n            b_int = np.random.randint(0, 2**seq_len)\n            a_bits = int_to_binary_array(a_int, seq_len)\n            b_bits = int_to_binary_array(b_int, seq_len)\n            x_seq = np.vstack((a_bits, b_bits)).T\n            s_true, c_true = get_true_targets(a_bits, b_bits)\n            y_true_seq = np.vstack((s_true, c_true)).T\n\n            # Forward pass\n            history = model.forward(x_seq)\n            \n            # --- Backward Pass (BPTT) ---\n            dW_xh, dW_hh, dW_hy = np.zeros_like(model.W_xh), np.zeros_like(model.W_hh), np.zeros_like(model.W_hy)\n            db_h, db_y = np.zeros_like(model.b_h), np.zeros_like(model.b_y)\n            \n            delta_h_future = np.zeros_like(model.b_h)\n            \n            for t in reversed(range(seq_len)):\n                y_pred_t = history['y'][t]\n                y_true_t = y_true_seq[t].reshape(-1, 1)\n                h_t = history['h'][t]\n                h_prev = history['h'][t - 1]\n                x_t = x_seq[t].reshape(-1, 1)\n                deriv_act_t = history['deriv_activations'][t]\n\n                delta_o_t = y_pred_t - y_true_t\n                dW_hy += delta_o_t @ h_t.T\n                db_y += delta_o_t\n                \n                delta_h_from_output = model.W_hy.T @ delta_o_t\n                delta_h_total = delta_h_from_output + delta_h_future\n                \n                delta_pre_act_h = delta_h_total * deriv_act_t\n                \n                dW_hh += delta_pre_act_h @ h_prev.T\n                dW_xh += delta_pre_act_h @ x_t.T\n                db_h += delta_pre_act_h\n                \n                delta_h_future = model.W_hh.T @ delta_pre_act_h\n            \n            # Gradient clipping\n            grads = [dW_xh, dW_hh, dW_hy, db_h, db_y]\n            total_norm = np.sqrt(sum(np.sum(g**2) for g in grads))\n            if total_norm > clip_threshold:\n                for g in grads:\n                    g *= clip_threshold / total_norm\n\n            # SGD update\n            model.W_xh -= learning_rate * dW_xh\n            model.W_hh -= learning_rate * dW_hh\n            model.W_hy -= learning_rate * dW_hy\n            model.b_h -= learning_rate * db_h\n            model.b_y -= learning_rate * db_y\n            \n            # Enforce spectral radius constraint\n            model.rescale_W_hh(rho_max)\n        \n        return model\n\n    def calculate_memory_length(model, rho_max, epsilon):\n        # Estimate average activation derivative\n        num_samples = 100\n        all_deriv_activations = []\n        for _ in range(num_samples):\n            a_int = np.random.randint(0, 2**T_TRAIN)\n            b_int = np.random.randint(0, 2**T_TRAIN)\n            a_bits = int_to_binary_array(a_int, T_TRAIN)\n            b_bits = int_to_binary_array(b_int, T_TRAIN)\n            x_seq = np.vstack((a_bits, b_bits)).T\n            history = model.forward(x_seq)\n            all_deriv_activations.extend([d.flatten() for d in history['deriv_activations']])\n        \n        d_avg_scalar = np.mean(np.concatenate(all_deriv_activations))\n        \n        rho_W_hh = np.max(np.abs(scipy.linalg.eigvals(model.W_hh)))\n        \n        # Calculate L_epsilon\n        log_numerator = np.log(epsilon)\n        decay_factor = d_avg_scalar * rho_W_hh\n        \n        if decay_factor >= 1.0:\n            return float('inf')\n        if decay_factor == 0:\n            return 1\n            \n        log_denominator = np.log(decay_factor)\n        L_eps = math.ceil(log_numerator / log_denominator)\n        return L_eps\n\n    def calculate_carry_chain_length(A, B, num_bits):\n        a_bits = int_to_binary_array(A, num_bits)\n        b_bits = int_to_binary_array(B, num_bits)\n        _, c_bits = get_true_targets(a_bits, b_bits)\n        \n        max_len = 0\n        current_len = 0\n        for bit in c_bits:\n            if bit == 1:\n                current_len += 1\n            else:\n                max_len = max(max_len, current_len)\n                current_len = 0\n        max_len = max(max_len, current_len)\n        return max_len\n\n    # --- Main Execution ---\n    rnn = RNN(INPUT_DIM, HIDDEN_DIM, OUTPUT_DIM)\n    \n    # Train the RNN\n    trained_rnn = train_rnn(rnn, N_EPOCHS, T_TRAIN, LEARNING_RATE, GRAD_CLIP_THRESHOLD, RHO_MAX)\n    \n    # Analyze the trained network's memory\n    L_epsilon = calculate_memory_length(trained_rnn, RHO_MAX, EPSILON)\n    \n    # Test Suite\n    test_cases = [\n        (25, 6),\n        (255, 1),\n        (1023, 1),\n        (0, 0),\n        (4095, 1),\n    ]\n\n    results = []\n    for A, B in test_cases:\n        # 1. Predict sufficiency based on memory length\n        L_carry = calculate_carry_chain_length(A, B, T_TEST)\n        p_sufficient = (L_epsilon >= L_carry)\n        \n        # 2. Evaluate actual performance\n        a_bits = int_to_binary_array(A, T_TEST)\n        b_bits = int_to_binary_array(B, T_TEST)\n        x_seq = np.vstack((a_bits, b_bits)).T\n        \n        history = trained_rnn.forward(x_seq)\n        \n        s_pred_bits = np.round([y[0,0] for y in history['y']])\n        c_pred_bits = np.round([y[1,0] for y in history['y']])\n        \n        S_pred = 0\n        for t in range(T_TEST):\n            S_pred += s_pred_bits[t] * (2**t)\n        # Add final carry-out\n        S_pred += c_pred_bits[T_TEST - 1] * (2**T_TEST)\n        \n        S_true = A + B\n        a_correct = (S_pred == S_true)\n        \n        results.extend([p_sufficient, a_correct])\n        \n    # Format and print the final output\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3167589"}]}