{"hands_on_practices": [{"introduction": "梯度下降是深度学习优化的基石，但其成功在很大程度上取决于学习率的选择。一个过大的学习率会导致训练过程不稳定，甚至发散。本练习将通过一个二次目标函数，直观地展示学习率与Hessian矩阵特征值之间的关键关系，这是理解优化稳定性的第一步。通过动手编程，您将亲眼见证当学习率超过理论边界时，优化轨迹如何从平稳收敛变为振荡甚至发散，从而为调试复杂的神经网络训练过程建立坚实的直觉 [@problem_id:3154406]。", "problem": "考虑二次目标函数 $f(x) = \\tfrac{1}{2} x^\\top H x$，其中 $H \\in \\mathbb{R}^{d \\times d}$ 是一个实对称矩阵（海森矩阵），$x \\in \\mathbb{R}^d$。使用恒定学习率 $\\eta  0$ 的梯度下降法应用更新规则 $x_{t+1} = x_t - \\eta \\nabla f(x_t)$，其中迭代由 $t \\in \\{0,1,2,\\dots\\}$ 索引。在二次型的情况下，这会产生一个线性迭代 $x_{t+1} = (I - \\eta H) x_t$。迭代 $x_t$ 的行为（收敛、振荡或发散）取决于学习率 $\\eta$ 和 $H$ 的特征值之间的相互作用。\n\n您的任务是编写一个程序，对于给定的一个包含矩阵 $H$ 和学习率 $\\eta$ 的小型测试套件，该程序将：\n- 从指定的初始点 $x_0$ 开始，使用更新规则 $x_{t+1} = x_t - \\eta H x_t$ 模拟梯度下降 $T$ 次迭代。\n- 计算迭代算子 $I - \\eta H$ 的谱半径，其定义为 $I - \\eta H$ 特征值的最大绝对值。\n- 通过计算比率 $\\|x_T\\|_2 / \\|x_0\\|_2$ 来量化范数的经验变化。\n- 根据谱半径使用整数代码对行为进行分类：\n  - 如果谱半径严格小于 $1$，则为 $0$。\n  - 如果谱半径在一个小的数值容差内等于 $1$，则为 $1$。\n  - 如果谱半径大于 $1$，则为 $2$。\n\n使用以下测试套件。对于所有情况，使用 $x_0 = \\begin{bmatrix}1 \\\\ -1\\end{bmatrix}$ 和 $T = 60$。\n- 情况 A（稳定，理想路径）：$H = \\begin{bmatrix}3  0 \\\\ 0  1\\end{bmatrix}$，$\\eta = 0.3$。\n- 情况 B（边界条件）：$H = \\begin{bmatrix}3  0 \\\\ 0  1\\end{bmatrix}$，$\\eta = \\tfrac{2}{3}$。\n- 情况 C（超出边界）：$H = \\begin{bmatrix}3  0 \\\\ 0  1\\end{bmatrix}$，$\\eta = 0.9$。\n- 情况 D（具有负特征值的非凸边缘情况）：$H = \\begin{bmatrix}-1  0 \\\\ 0  4\\end{bmatrix}$，$\\eta = 0.3$。\n- 情况 E（病态但仍在界限内）：$H = \\begin{bmatrix}10  0 \\\\ 0  0.1\\end{bmatrix}$，$\\eta = 0.19$。\n- 情况 F（病态且超出界限）：$H = \\begin{bmatrix}10  0 \\\\ 0  0.1\\end{bmatrix}$，$\\eta = 0.21$。\n\n实现细节和输出：\n- 对于每种情况，计算 $H$ 的特征值 $\\{\\lambda_i\\}$，然后计算 $I - \\eta H$ 的谱半径为 $\\max_i |1 - \\eta \\lambda_i|$。\n- 对 $t = 0,1,\\dots,T-1$ 模拟迭代 $x_{t+1} = x_t - \\eta H x_t$，并计算比率 $r_{\\text{emp}} = \\|x_T\\|_2 / \\|x_0\\|_2$。\n- 使用上述由 $I - \\eta H$ 的谱半径确定的整数代码对行为进行分类。\n- 每种情况的最终输出是一个列表，按顺序包含谱半径（浮点数）、经验比率 $r_{\\text{emp}}$（浮点数）和分类代码（整数）。\n\n最终输出格式：\n- 您的程序应生成一行输出，其中包含所有六种情况的结果，形式为用方括号括起来的逗号分隔列表，其中每种情况贡献一个形式为 $[\\text{spectral\\_radius}, \\text{empirical\\_ratio}, \\text{classification}]$ 的列表。例如，格式为 $[[a_1,b_1,c_1],[a_2,b_2,c_2],\\dots,[a_6,b_6,c_6]]$。", "solution": "该问题要求分析应用于二次目标函数 $f(x) = \\frac{1}{2} x^\\top H x$ 的梯度下降算法的行为。该分析既涉及基于海森矩阵 $H$ 和学习率 $\\eta$ 的性质的理论计算，也涉及迭代过程的经验模拟。\n\n问题的核心在于理解将梯度下降应用于此特定目标函数所产生的线性动力系统。该函数的梯度为 $\\nabla f(x) = Hx$。梯度下降更新规则由下式给出：\n$$\nx_{t+1} = x_t - \\eta \\nabla f(x_t)\n$$\n代入梯度，我们得到一个线性递推关系：\n$$\nx_{t+1} = x_t - \\eta H x_t = (I - \\eta H) x_t\n$$\n其中 $I$ 是单位矩阵，$G = I - \\eta H$ 是迭代算子。此递推关系的解为 $x_t = G^t x_0$。迭代 $x_t$ 的長期行为由矩阵 $G$ 的性质决定。\n\n为了使迭代序列 $x_t$ 收敛到零向量（如果 $H$ 是正定的，这是唯一的最小值），迭代矩阵 $G$ 的谱半径严格小于 $1$ 是一个充要条件。谱半径，记为 $\\rho(G)$，定义为 $G$ 的特征值的最大绝对值。\n$$\n\\rho(G) = \\max_{i} |\\lambda_i(G)|\n$$\n其中 $\\lambda_i(G)$ 是 $G$ 的特征值。\n\n如果 $\\lambda_i(H)$ 是海森矩阵 $H$ 的特征值，那么 $G = I - \\eta H$ 的特征值由 $1 - \\eta \\lambda_i(H)$ 给出。因此，谱半径可以直接根据 $H$ 的特征值和学习率 $\\eta$ 计算得出：\n$$\n\\rho(I - \\eta H) = \\max_{i} |1 - \\eta \\lambda_i(H)|\n$$\n梯度下降算法的行为可以根据 $\\rho(I - \\eta H)$ 的值进行分类：\n- 如果 $\\rho(I - \\eta H)  1$，迭代的范数 $\\|x_t\\|_2$ 将衰减到 $0$。这是收敛的条件。\n- 如果 $\\rho(I - \\eta H) = 1$，迭代的范数可能会保持有界（振荡），或者如果绝对值为 $1$ 的特征值对应的若尔当块尺寸大于 $1$，则范数可能多项式增长。在本问题中，由于 $H$ 是对称的，$G$ 也是对称的，所有若尔当块的尺寸都为 $1$，因此范数不会发散。\n- 如果 $\\rho(I - \\eta H)  1$，迭代的范数 $\\|x_t\\|_2$ 将呈指数级增长，该方法发散。\n\n对于一个凸问题，其中 $H$ 是正定的（所有 $\\lambda_i(H)  0$），收敛条件 $\\rho(I - \\eta H)  1$ 简化为找到一个 $\\eta$ 使得对于所有的 $i$ 都有 $|1 - \\eta \\lambda_i(H)|  1$。这个不等式等价于 $-1  1 - \\eta \\lambda_i(H)  1$，这意味着 $0  \\eta \\lambda_i(H)  2$。这必须对所有特征值 $\\lambda_i(H)$ 成立，从而得出众所周知的学习率条件：$0  \\eta  \\frac{2}{\\lambda_{\\max}(H)}$，其中 $\\lambda_{\\max}(H)$ 是 $H$ 的最大特征值。\n\n范数的经验比率 $\\|x_T\\|_2 / \\|x_0\\|_2$ 预期会与谱半径相关。对于对称矩阵 $G$，其诱导 $2$-范数等于谱半径，即 $\\|G\\|_2 = \\rho(G)$。因此，我们有界 $\\|x_t\\|_2 \\le \\|G\\|_2^t \\|x_0\\|_2 = \\rho(G)^t \\|x_0\\|_2$。因此，比率 $\\|x_t\\|_2 / \\|x_0\\|_2$ 的上界是 $\\rho(G)^t$。实际值取决于初始向量 $x_0$ 在 $G$ 的特征空间上的投影。\n\n我们现在分析每个测试用例，初始向量为 $x_0 = \\begin{bmatrix}1 \\\\ -1\\end{bmatrix}$，迭代次数为 $T=60$。\n\n**情况 A：** $H = \\begin{bmatrix}3  0 \\\\ 0  1\\end{bmatrix}$，$\\eta = 0.3$。\n- $H$ 的特征值：$\\lambda_1 = 3$，$\\lambda_2 = 1$。\n- $I - \\eta H$ 的特征值：$1 - 0.3 \\cdot 3 = 0.1$ 和 $1 - 0.3 \\cdot 1 = 0.7$。\n- 谱半径：$\\rho = \\max(|0.1|, |0.7|) = 0.7$。\n- 分类：由于 $\\rho=0.7  1$，分类为 $0$（收敛）。\n- 经验比率预计会很小。\n\n**情况 B：** $H = \\begin{bmatrix}3  0 \\\\ 0  1\\end{bmatrix}$，$\\eta = \\frac{2}{3}$。\n- $H$ 的特征值：$\\lambda_1 = 3$，$\\lambda_2 = 1$。\n- $I - \\eta H$ 的特征值：$1 - \\frac{2}{3} \\cdot 3 = -1$ 和 $1 - \\frac{2}{3} \\cdot 1 = \\frac{1}{3}$。\n- 谱半径：$\\rho = \\max(|-1|, |\\frac{1}{3}|) = 1.0$。\n- 分类：由于 $\\rho=1.0$，分类为 $1$（边界）。误差向量的一个分量会振荡，而另一个分量会衰减。\n\n**情况 C：** $H = \\begin{bmatrix}3  0 \\\\ 0  1\\end{bmatrix}$，$\\eta = 0.9$。\n- $H$ 的特征值：$\\lambda_1 = 3$，$\\lambda_2 = 1$。\n- $I - \\eta H$ 的特征值：$1 - 0.9 \\cdot 3 = -1.7$ 和 $1 - 0.9 \\cdot 1 = 0.1$。\n- 谱半径：$\\rho = \\max(|-1.7|, |0.1|) = 1.7$。\n- 分类：由于 $\\rho=1.7  1$，分类为 $2$（发散）。迭代值将呈指数级增长。\n\n**情况 D：** $H = \\begin{bmatrix}-1  0 \\\\ 0  4\\end{bmatrix}$，$\\eta = 0.3$。\n- 由于 $H$ 有一个负特征值，这个目标函数是非凸的。\n- $H$ 的特征值：$\\lambda_1 = -1$，$\\lambda_2 = 4$。\n- $I - \\eta H$ 的特征值：$1 - 0.3 \\cdot (-1) = 1.3$ 和 $1 - 0.3 \\cdot 4 = -0.2$。\n- 谱半径：$\\rho = \\max(|1.3|, |-0.2|) = 1.3$。\n- 分类：由于 $\\rho=1.3  1$，分类为 $2$（发散）。梯度下降将会发散，远离原点的鞍点。\n\n**情况 E：** $H = \\begin{bmatrix}10  0 \\\\ 0  0.1\\end{bmatrix}$，$\\eta = 0.19$。\n- 这个矩阵是病态的，条件数为 $\\kappa = \\frac{10}{0.1} = 100$。理论学习率上限为 $\\frac{2}{\\lambda_{\\max}} = \\frac{2}{10} = 0.2$。\n- $H$ 的特征值：$\\lambda_1 = 10$，$\\lambda_2 = 0.1$。\n- $I - \\eta H$ 的特征值：$1 - 0.19 \\cdot 10 = -0.9$ 和 $1 - 0.19 \\cdot 0.1 = 0.981$。\n- 谱半径：$\\rho = \\max(|-0.9|, |0.981|) = 0.981$。\n- 分类：由于 $\\rho=0.981  1$，分类为 $0$（收敛）。然而，由于谱半径非常接近 $1$，收敛会很慢。\n\n**情况 F：** $H = \\begin{bmatrix}10  0 \\\\ 0  0.1\\end{bmatrix}$，$\\eta = 0.21$。\n- 与情况 E 相同的病态矩阵，但学习率 $\\eta=0.21$ 略高于稳定性阈值 $0.2$。\n- $H$ 的特征值：$\\lambda_1 = 10$，$\\lambda_2 = 0.1$。\n- $I - \\eta H$ 的特征值：$1 - 0.21 \\cdot 10 = -1.1$ 和 $1 - 0.21 \\cdot 0.1 = 0.979$。\n- 谱半径：$\\rho = \\max(|-1.1|, |0.979|) = 1.1$。\n- 分类：由于 $\\rho=1.1  1$，分类为 $2$（发散）。这表明了梯度下降对学习率的敏感性，特别是对于病态问题。\n\n以下程序将实现这些计算和模拟。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Analyzes the behavior of gradient descent on a quadratic objective function\n    for a given suite of test cases.\n\n    For each case, it computes:\n    1. The spectral radius of the iteration operator.\n    2. The empirical ratio of norms after T iterations.\n    3. A classification code based on the spectral radius.\n    \"\"\"\n    \n    # Define the six test cases from the problem statement.\n    test_cases = [\n        # Case A: stable, happy path\n        {'H': np.array([[3.0, 0.0], [0.0, 1.0]]), 'eta': 0.3},\n        # Case B: boundary condition\n        {'H': np.array([[3.0, 0.0], [0.0, 1.0]]), 'eta': 2.0 / 3.0},\n        # Case C: over the bound\n        {'H': np.array([[3.0, 0.0], [0.0, 1.0]]), 'eta': 0.9},\n        # Case D: nonconvex edge case\n        {'H': np.array([[-1.0, 0.0], [0.0, 4.0]]), 'eta': 0.3},\n        # Case E: ill-conditioned but under the bound\n        {'H': np.array([[10.0, 0.0], [0.0, 0.1]]), 'eta': 0.19},\n        # Case F: ill-conditioned and over the bound\n        {'H': np.array([[10.0, 0.0], [0.0, 0.1]]), 'eta': 0.21}\n    ]\n\n    # Shared parameters for all simulations\n    x0 = np.array([1.0, -1.0])\n    T = 60\n    norm_x0 = np.linalg.norm(x0)\n    \n    all_results = []\n    \n    for case in test_cases:\n        H = case['H']\n        eta = case['eta']\n        \n        # 1. Compute the spectral radius of the iteration operator G = I - eta*H\n        # Since H is symmetric, its eigenvalues are real. We use eigvalsh for stability.\n        # The eigenvalues of G are 1 - eta * lambda_i(H).\n        eigenvalues_H = np.linalg.eigvalsh(H)\n        eigenvalues_G = 1.0 - eta * eigenvalues_H\n        spectral_radius = np.max(np.abs(eigenvalues_G))\n        \n        # 2. Simulate gradient descent for T iterations\n        x_t = x0.copy()\n        for _ in range(T):\n            # Update rule: x_{t+1} = x_t - eta * H * x_t\n            x_t = x_t - eta * (H @ x_t)\n            \n        # 3. Compute the empirical change in norm\n        norm_xT = np.linalg.norm(x_t)\n        empirical_ratio = norm_xT / norm_x0\n        \n        # 4. Classify the behavior based on the spectral radius\n        classification = 0  # Default to convergence\n        if np.isclose(spectral_radius, 1.0):\n            classification = 1  # Boundary condition\n        elif spectral_radius > 1.0:\n            classification = 2  # Divergence\n            \n        all_results.append([spectral_radius, empirical_ratio, classification])\n\n    # Format the final output string as a list of lists.\n    # e.g., [[valA1,valA2,valA3],[valB1,valB2,valB3],...]\n    result_strings = []\n    for res in all_results:\n        # Format each sublist to '[val1,val2,val3]' without extra spaces\n        formatted_res = f\"[{res[0]},{res[1]},{res[2]}]\"\n        result_strings.append(formatted_res)\n    \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(result_strings)}]\")\n\nsolve()\n```", "id": "3154406"}, {"introduction": "为了克服普通梯度下降的局限性，自适应优化方法（如Adam）应运而生，但它们也并非万能。在面对梯度“尖锐”或充满噪声的复杂损失地貌时，即便是Adam也可能表现出不稳定的行为，导致训练效果不佳。本练习将引导您实现并比较Adam及其变体AdaBelief，后者通过修改二阶矩的估计方式来增强对梯度变化的信念，从而在复杂环境中获得更稳定的更新步长。通过在一个精心设计的非凸函数上进行实验，您将学会如何量化评估不同优化器的稳定性，并理解优化器设计的前沿思想 [@problem_id:3154379]。", "problem": "您需要在一个特意构建的、具有尖峰梯度的一维目标函数上，实现并比较两种自适应优化方法，并定量评估在随机噪声下的训练稳定性。这两种方法是自适应矩估计 (Adam) 和 AdaBelief。AdaBelief 是 Adam 的一个变体，其二阶累积量基于当前梯度与其一阶矩估计（即优化器的“信念”）之间的偏差。\n\n构建目标函数\n$$\nf(x) \\;=\\; \\tfrac{1}{2}\\,x^2 \\;+\\; s\\bigl(1 - \\cos(\\omega x)\\bigr) \\;+\\; \\tfrac{1}{2}\\,s\\bigl(1 - \\cos(3\\,\\omega x)\\bigr),\n$$\n其中 $x \\in \\mathbb{R}$，$s  0$ 且 $\\omega  0$。当 $s$ 取值适中时，该选择确保在 $x = 0$ 附近存在唯一的极小值点，而梯度\n$$\n\\nabla f(x) \\;=\\; x \\;+\\; s\\,\\omega\\,\\sin(\\omega x) \\;+\\; \\tfrac{3}{2}\\,s\\,\\omega\\,\\sin(3\\,\\omega x)\n$$\n则表现出由 $\\omega$ 控制的高频尖峰。训练动态是随机的：使用带噪声的梯度 $g_t = \\nabla f(x_t) + \\xi_t$，其中 $\\xi_t$ 是从均值为零、方差为 $\\sigma^2$ 的高斯分布中抽取的独立样本，即 $\\xi_t \\sim \\mathcal{N}(0,\\sigma^2)$。\n\n从指定的初始点 $x_0$ 开始，在同一个实现的噪声序列 $\\{\\xi_t\\}_{t=1}^T$ 上对每个优化器运行 $T$ 次更新，以确保公平比较。使用标准的一阶和二阶项的偏差校正指数移动平均来实现这两种优化器；除了方法的标准定义外，不要硬编码任何封闭形式或快捷表达式。使用固定的超参数 $\\beta_1 = 0.9$，$\\beta_2 = 0.999$ 和 $\\varepsilon = 10^{-8}$。\n\n为每个优化器在每个测试用例上定义并计算以下指标：\n- 最终目标函数值 $f(x_T)$。\n- 平均更新幅度的平方\n$$\n\\overline{\\Delta^2} \\;=\\; \\frac{1}{T} \\sum_{t=1}^{T} (x_{t} - x_{t-1})^2.\n$$\n- 振荡次数，定义为索引对 $(t-1,t)$（其中 $t \\in \\{1,\\dots,T\\}$）的数量，满足 $x_{t-1}$ 和 $x_t$ 符号相反（即乘积 $x_{t-1}\\,x_t  0$）。该指标衡量穿越 $x=0$ 直线的次数。\n\n对于每个测试用例，输出一个布尔值结果，指示 AdaBelief 是否在有噪声的梯度下比 Adam 更好地稳定了训练。如果以下所有条件同时成立，则声明 AdaBelief “更稳定”：\n- 其平均更新幅度的平方严格小于 Adam 的，即 $\\overline{\\Delta^2}_{\\text{AdaBelief}}  \\overline{\\Delta^2}_{\\text{Adam}}$（允许 $10^{-12}$ 的数值容差）。\n- 其振荡次数小于或等于 Adam 的。\n- 其最终目标函数值不比 Adam 的差超过 $10^{-3}$ 的容差，即 $f(x_T)_{\\text{AdaBelief}} \\le f(x_T)_{\\text{Adam}} + 10^{-3}$。\n\n使用以下测试套件；每个用例指定了 $(\\omega, s, \\sigma, \\alpha, T, \\text{seed}, x_0)$：\n- 用例 1: $\\omega = 25.0$, $s = 0.4$, $\\sigma = 0.1$, $\\alpha = 0.01$, $T = 300$, $\\text{seed} = 42$, $x_0 = 2.0$。\n- 用例 2: $\\omega = 60.0$, $s = 0.6$, $\\sigma = 0.3$, $\\alpha = 0.005$, $T = 500$, $\\text{seed} = 123$, $x_0 = -3.0$。\n- 用例 3 (边缘用例，无噪声): $\\omega = 25.0$, $s = 0.4$, $\\sigma = 0.0$, $\\alpha = 0.005$, $T = 300$, $\\text{seed} = 7$, $x_0 = 1.0$。\n- 用例 4 (高噪声): $\\omega = 25.0$, $s = 0.4$, $\\sigma = 0.5$, $\\alpha = 0.01$, $T = 500$, $\\text{seed} = 99$, $x_0 = 2.5$。\n\n您的程序应产生单行输出，其中包含一个用方括号括起来的逗号分隔列表（例如，“[true_case1,true_case2,true_case3,true_case4]”），其中每个条目都是一个布尔值，指示 AdaBelief 是否在相应测试用例中比 Adam 更稳定。不应打印任何其他文本。", "solution": "该问题要求在随机噪声下，对一个指定的一维非凸目标函数上的两种自适应优化算法 Adam 和 AdaBelief 进行对比分析。目标是根据一组定量指标，确定哪种优化器表现出更高的稳定性。解决方案的步骤是：首先定义问题的数学组成部分，然后详细说明优化器和评估指标的实现，最后对每个测试用例执行此过程。\n\n### 1. 问题阐述\n\n需要最小化的目标函数由下式给出：\n$$\nf(x) \\;=\\; \\tfrac{1}{2}\\,x^2 \\;+\\; s\\bigl(1 - \\cos(\\omega x)\\bigr) \\;+\\; \\tfrac{1}{2}\\,s\\bigl(1 - \\cos(3\\,\\omega x)\\bigr)\n$$\n其中 $x \\in \\mathbb{R}$ 是待优化的参数，$s  0$ 和 $\\omega  0$ 是控制函数形状的常数。第一项 $\\tfrac{1}{2}x^2$ 提供了一个凸基础，而余弦项引入了高频的非凸振荡。\n\n该函数关于 $x$ 的梯度是：\n$$\n\\nabla f(x) \\;=\\; x \\;+\\; s\\,\\omega\\,\\sin(\\omega x) \\;+\\; \\tfrac{3}{2}\\,s\\,\\omega\\,\\sin(3\\,\\omega x)\n$$\n优化在随机环境下进行。在每个时间步 $t$，优化器无法获取真实的梯度 $\\nabla f(x_t)$，而是获取一个带噪声的版本：\n$$\ng_t = \\nabla f(x_t) + \\xi_t\n$$\n其中 $x_t$ 是在步骤 $t$ 的参数值，$\\xi_t$ 是一个从零均值高斯分布中独立采样的噪声项，$\\xi_t \\sim \\mathcal{N}(0, \\sigma^2)$。\n\n### 2. 优化器算法\n\nAdam 和 AdaBelief 都是自适应学习率方法，它们使用梯度（一阶矩）和梯度平方（二阶矩）的指数移动平均来为每个参数调整学习率。优化从 $x_0$ 开始初始化，并运行 $T$ 步。固定的超参数为 $\\beta_1 = 0.9$，$\\beta_2 = 0.999$ 和 $\\varepsilon = 10^{-8}$。学习率用 $\\alpha$ 表示。\n\n在从 $1$ 到 $T$ 的每一步 $t$ 中：\n1.  计算带噪声的梯度 $g_t = \\nabla f(x_{t-1}) + \\xi_t$。\n2.  更新一阶矩估计 $m_t$：\n    $$\n    m_t = \\beta_1 m_{t-1} + (1 - \\beta_1) g_t\n    $$\n3.  更新二阶矩估计 $v_t$。这一步是两种算法的不同之处。\n    -   **Adam**：二阶矩是梯度平方的指数移动平均。\n        $$\n        v_t^{\\text{Adam}} = \\beta_2 v_{t-1}^{\\text{Adam}} + (1 - \\beta_2) g_t^2\n        $$\n    -   **AdaBelief**：二阶矩是当前梯度 $g_t$ 与其一阶矩估计 $m_t$ 之间平方差的指数移动平均。这就是对当前梯度的“信念”。\n        $$\n        v_t^{\\text{AdaBelief}} = \\beta_2 v_{t-1}^{\\text{AdaBelief}} + (1 - \\beta_2) (g_t - m_t)^2\n        $$\n4.  计算偏差校正的矩估计。由于 $m_0$ 和 $v_0$ 初始化为 0，这些估计值会偏向零，尤其是在初始步骤中。偏差按如下方式进行校正：\n    $$\n    \\hat{m}_t = \\frac{m_t}{1 - \\beta_1^t}\n    $$\n    $$\n    \\hat{v}_t = \\frac{v_t}{1 - \\beta_2^t}\n    $$\n5.  更新参数 $x_t$：\n    $$\n    x_t = x_{t-1} - \\alpha \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\varepsilon}\n    $$\n    项 $\\varepsilon$ 是一个很小的常数，用于防止除以零。\n\n### 3. 评估指标与稳定性准则\n\n为了定量比较 Adam 和 AdaBelief 的稳定性，为每个优化器的轨迹 $\\{x_t\\}_{t=0}^T$ 计算三个指标：\n\n-   **最终目标函数值 $f(x_T)$**：在最终参数 $x_T$ 处的目标函数值。\n-   **平均更新幅度的平方 $\\overline{\\Delta^2}$**：该指标衡量优化过程中的平均步长，计算方式如下：\n    $$\n    \\overline{\\Delta^2} \\;=\\; \\frac{1}{T} \\sum_{t=1}^{T} (x_{t} - x_{t-1})^2\n    $$\n    较小的值表明收敛过程更平滑、更稳定。\n-   **振荡次数**：参数轨迹穿越 $x=0$ 直线的次数。这被计算为满足 $x_{t-1} \\cdot x_t  0$ 的索引 $t \\in \\{1, \\dots, T\\}$ 的数量。较少的振荡次数表示一条更直接通往极小值点的路径。\n\n当且仅当以下三个条件全部满足时，AdaBelief 被声明为比 Adam “更稳定”：\n1.  $\\overline{\\Delta^2}_{\\text{AdaBelief}}  \\overline{\\Delta^2}_{\\text{Adam}}$（对于严格不等式，允许 $10^{-12}$ 的数值容差）。\n2.  $\\text{oscillation\\_count}_{\\text{AdaBelief}} \\le \\text{oscillation\\_count}_{\\text{Adam}}$。\n3.  $f(x_T)_{\\text{AdaBelief}} \\le f(x_T)_{\\text{Adam}} + 10^{-3}$，意味着其最终目标函数值没有显著差于 Adam。\n\n### 4. 实现策略\n\n对于每个由参数 $(\\omega, s, \\sigma, \\alpha, T, \\text{seed}, x_0)$ 定义的测试用例，执行以下过程：\n\n1.  使用指定的 `seed` 和标准差 $\\sigma$ 生成一个高斯噪声样本序列 $\\{\\xi_t\\}_{t=1}^T$。对两种优化器使用相同的噪声序列，对于公平比较它们的动态特性至关重要。\n2.  进行两次独立的优化运行，一次用于 Adam，一次用于 AdaBelief，两者都从 $x_0$ 开始，并使用相同的学习率 $\\alpha$ 和噪声序列 $\\{\\xi_t\\}_{t=1}^T$。\n3.  在每次运行期间，存储参数的完整轨迹 $\\{x_0, x_1, \\dots, x_T\\}$。\n4.  两次运行都完成后，根据各自的轨迹为每个优化器计算三个评估指标（$f(x_T)$、$\\overline{\\Delta^2}$ 和振荡次数）。\n5.  将这些指标与三个稳定性条件进行比较，为该测试用例生成一个单一的布尔值结果。\n6.  对所有四个测试用例重复此过程，并将最终结果汇总成一个列表。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef run_optimization_and_compare(w, s, sig, alpha, T, seed, x0):\n    \"\"\"\n    Runs Adam and AdaBelief optimizers for a given test case and compares them\n    based on the specified stability criteria.\n    \"\"\"\n    BETA1 = 0.9\n    BETA2 = 0.999\n    EPSILON = 1e-8\n\n    # Define objective function and its gradient\n    f = lambda x: 0.5 * x**2 + s * (1 - np.cos(w * x)) + 0.5 * s * (1 - np.cos(3 * w * x))\n    grad_f = lambda x: x + s * w * np.sin(w * x) + 1.5 * s * w * np.sin(3 * w * x)\n\n    # Generate a single noise sequence for both optimizers for a fair comparison\n    rng = np.random.default_rng(seed)\n    noise = rng.normal(loc=0.0, scale=sig, size=T)\n\n    # --- Run Adam Optimizer ---\n    x_adam_traj = np.zeros(T + 1)\n    x_adam_traj[0] = x0\n    m_adam, v_adam = 0.0, 0.0\n    for t in range(1, T + 1):\n        # Current parameter value is from the previous step\n        current_x = x_adam_traj[t-1]\n        \n        # Calculate stochastic gradient\n        g = grad_f(current_x) + noise[t-1]\n        \n        # Update moments\n        m_adam = BETA1 * m_adam + (1 - BETA1) * g\n        v_adam = BETA2 * v_adam + (1 - BETA2) * (g**2)\n        \n        # Bias correction\n        m_hat = m_adam / (1 - BETA1**t)\n        v_hat = v_adam / (1 - BETA2**t)\n        \n        # Update parameter\n        x_adam_traj[t] = current_x - alpha * m_hat / (np.sqrt(v_hat) + EPSILON)\n\n    # --- Run AdaBelief Optimizer ---\n    x_adabelief_traj = np.zeros(T + 1)\n    x_adabelief_traj[0] = x0\n    m_adabelief, v_adabelief = 0.0, 0.0\n    for t in range(1, T + 1):\n        current_x = x_adabelief_traj[t-1]\n        \n        g = grad_f(current_x) + noise[t-1]\n        \n        m_adabelief = BETA1 * m_adabelief + (1 - BETA1) * g\n        # The key difference: second moment uses (g - m)^2\n        v_adabelief = BETA2 * v_adabelief + (1 - BETA2) * ((g - m_adabelief)**2)\n        \n        m_hat = m_adabelief / (1 - BETA1**t)\n        v_hat = v_adabelief / (1 - BETA2**t)\n        \n        x_adabelief_traj[t] = current_x - alpha * m_hat / (np.sqrt(v_hat) + EPSILON)\n\n    # --- Calculate Metrics for Adam ---\n    f_T_adam = f(x_adam_traj[T])\n    delta_sq_avg_adam = np.mean((x_adam_traj[1:] - x_adam_traj[:-1])**2)\n    osc_count_adam = np.sum((x_adam_traj[:-1] * x_adam_traj[1:])  0)\n\n    # --- Calculate Metrics for AdaBelief ---\n    f_T_adabelief = f(x_adabelief_traj[T])\n    delta_sq_avg_adabelief = np.mean((x_adabelief_traj[1:] - x_adabelief_traj[:-1])**2)\n    osc_count_adabelief = np.sum((x_adabelief_traj[:-1] * x_adabelief_traj[1:])  0)\n    \n    # --- Compare metrics based on stability criteria ---\n    # 1. Avg squared update magnitude is strictly smaller (with tolerance)\n    cond1 = delta_sq_avg_adabelief  delta_sq_avg_adam - 1e-12\n    # 2. Oscillation count is less than or equal\n    cond2 = osc_count_adabelief = osc_count_adam\n    # 3. Final objective is not worse by more than a tolerance\n    cond3 = f_T_adabelief = f_T_adam + 1e-3\n\n    return cond1 and cond2 and cond3\n\ndef solve():\n    # Define the test cases as tuples of:\n    # (omega, s, sigma, alpha, T, seed, x0)\n    test_cases = [\n        # Case 1\n        (25.0, 0.4, 0.1, 0.01, 300, 42, 2.0),\n        # Case 2\n        (60.0, 0.6, 0.3, 0.005, 500, 123, -3.0),\n        # Case 3\n        (25.0, 0.4, 0.0, 0.005, 300, 7, 1.0),\n        # Case 4\n        (25.0, 0.4, 0.5, 0.01, 500, 99, 2.5),\n    ]\n\n    results = []\n    for case in test_cases:\n        omega, s, sigma, alpha, T, seed, x0 = case\n        is_more_stable = run_optimization_and_compare(omega, s, sigma, alpha, T, seed, x0)\n        results.append(str(is_more_stable).lower())\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3154379"}, {"introduction": "除了调整学习率，我们还可以通过对参数空间施加约束来直接提升训练的稳定性。这种方法不仅可以作为一种有效的正则化手段，防止参数值爆炸，还能在非凸优化问题中引导优化器走向更平滑的区域。在本练习中，您将为一个小型的ReLU神经网络实现投影梯度下降（Projected Gradient Descent, PGD），在每次梯度更新后，将参数投影回一个预定义的 $\\ell_{\\infty}$ 范数球内。通过对比有约束和无约束的优化过程，您将深入理解约束如何有效控制梯度范数、避免参数过大，并最终稳定非凸损失函数的训练过程 [@problem_id:3154367]。", "problem": "考虑在一个参数向量满足$\\ell_{\\infty}$范数约束的条件下，对一个单隐藏单元修正线性单元 (ReLU) 神经网络进行经验风险最小化。模型定义如下。输入是一个数据集 $\\{(x_i,y_i)\\}_{i=1}^n$，其中 $x_i \\in \\mathbb{R}^2$ 且 $y_i \\in \\mathbb{R}$。网络输出为 $f(x;\\theta) = w_2 \\cdot \\max\\{0, w_1^\\top x + b_1\\} + b_2$，其中 $\\theta = (w_1, b_1, w_2, b_2) \\in \\mathbb{R}^5$，$w_1 \\in \\mathbb{R}^2$，$b_1 \\in \\mathbb{R}$，$w_2 \\in \\mathbb{R}$ 且 $b_2 \\in \\mathbb{R}$。经验损失是均方误差 $L(\\theta) = \\frac{1}{n}\\sum_{i=1}^n \\left(f(x_i;\\theta) - y_i\\right)^2$。对于给定的常数 $c  0$，可行集是权重空间 $\\{\\theta \\in \\mathbb{R}^5 : \\|\\theta\\|_\\infty \\leq c\\}$。\n\n您的任务是从相同的初始化开始，实现两种优化过程：无约束梯度下降和到集合 $\\{\\theta : \\|\\theta\\|_\\infty \\leq c\\}$ 上的约束投影梯度下降。投影定义为到可行集上的欧几里得投影。您必须：\n\n1. 使用链式法则和标准微积分方法，推导并实现损失函数 $L(\\theta)$ 相对于 $\\theta$ 的梯度，以处理ReLU非线性。在 $w_1^\\top x_i + b_1 = 0$ 的点，使用与ReLU一致的任何有效次梯度；在代码中，当 $w_1^\\top x_i + b_1  0$ 时，导数指示函数取1，否则取0。\n2. 实现无约束梯度下降：$\\theta_{t+1} = \\theta_t - \\eta \\nabla L(\\theta_t)$，其中 $t = 0,1,\\dots,T-1$，$\\eta  0$ 是步长，$T$ 是迭代次数。\n3. 实现投影梯度下降 (PGD)：$\\theta_{t+1} = \\Pi(\\theta_t - \\eta \\nabla L(\\theta_t))$，其中 $t = 0,1,\\dots,T-1$，$\\Pi$ 是到集合 $\\{\\theta : \\|\\theta\\|_\\infty \\leq c\\}$ 上的欧几里得投影。确保轨迹在所有迭代中都保持可行。如果初始化是不可行的，则在第一步之前通过对初始 $\\theta$ 应用投影来强制其可行。\n4. 通过为每个优化过程收集以下指标，来量化投影对于非凸损失 $L(\\theta)$ 的稳定效应：\n   - 最终损失 $L(\\theta_T)$。\n   - 整个轨迹上的最大梯度范数，$\\max_{0 \\leq t  T} \\|\\nabla L(\\theta_t)\\|_2$。\n   - 对于无约束下降，遇到的最大绝对参数值，$\\max_{0 \\leq t \\leq T} \\|\\theta_t\\|_\\infty$。\n   - 对于PGD，投影主动裁剪至少一个坐标的次数，即预投影更新 $(\\theta_t - \\eta \\nabla L(\\theta_t))$ 中存在绝对值严格大于 $c$ 的坐标的迭代次数 $t$ 的计数。\n5. 定义一个稳定性的布尔指标，当投影使最大梯度范数减少了至少 $\\delta = 0.1$ 的相对裕度时，即当 $\\frac{\\max_t \\|\\nabla L(\\theta_t)\\|_2^{\\text{unconstrained}} - \\max_t \\|\\nabla L(\\theta_t)\\|_2^{\\text{projected}}}{\\max_t \\|\\nabla L(\\theta_t)\\|_2^{\\text{unconstrained}}} \\geq 0.1$ 时，或者当无约束轨迹违反约束条件时，即 $\\max_t \\|\\theta_t\\|_\\infty^{\\text{unconstrained}}  c$ 时，该指标返回真。如果分母为零，则将相对裕度视为零。\n\n使用以下大小为 $n = 50$ 的确定性数据集：\n- 对于 $i = 1,2,\\dots,50$，定义 $x_i \\in \\mathbb{R}^2$ 为 $x_{i,1} = i/50$ 和 $x_{i,2} = 0.5 \\cdot (-1)^i$。\n- 定义 $y_i \\in \\mathbb{R}$ 为：如果 $x_{i,1} + x_{i,2}  0.6$，则 $y_i = 1$，否则 $y_i = 0$。\n\n对以下四个测试用例中的每一个运行两种优化过程。每个测试用例指定了约束水平 $c$、步长 $\\eta$、迭代次数 $T$ 和初始参数向量 $\\theta_0 = (w_{1,1}, w_{1,2}, b_1, w_2, b_2)$:\n\n- 测试用例 1: $c = 0.5$, $\\eta = 0.5$, $T = 100$, $\\theta_0 = (1.0, -1.0, 0.0, 2.0, 0.0)$。\n- 测试用例 2: $c = 10.0$, $\\eta = 0.05$, $T = 300$, $\\theta_0 = (0.1, 0.1, 0.1, 0.1, 0.1)$。\n- 测试用例 3: $c = 1.0$, $\\eta = 0.3$, $T = 200$, $\\theta_0 = (2.0, -2.0, 1.0, 8.0, -0.5)$。\n- 测试用例 4: $c = 1.0$, $\\eta = 0.4$, $T = 150$, $\\theta_0 = (1.0, -0.2, 0.3, -1.0, 0.0)$。\n\n对于每个测试用例，生成一个包含七个值的列表：\n- $L(\\theta_T)^{\\text{projected}}$ 四舍五入到6位小数。\n- $L(\\theta_T)^{\\text{unconstrained}}$ 四舍五入到6位小数。\n- $\\max_t \\|\\nabla L(\\theta_t)\\|_2^{\\text{projected}}$ 四舍五入到6位小数。\n- $\\max_t \\|\\nabla L(\\theta_t)\\|_2^{\\text{unconstrained}}$ 四舍五入到6位小数。\n- $\\max_t \\|\\theta_t\\|_\\infty^{\\text{unconstrained}}$ 四舍五入到6位小数。\n- PGD中投影激活的整数计数。\n- 如上定义的稳定性布尔值。\n\n您的程序应生成单行输出，其中包含四个测试用例的结果，格式为用方括号括起来的逗号分隔的列表的列表，不含空格。例如，输出格式必须为 $[[\\cdots],[\\cdots],[\\cdots],[\\cdots]]$，其中每个内部列表按上述顺序包含一个测试用例的七个值。不涉及物理单位。不出现角度。相对裕度用小数表示，而不是百分比。", "solution": "该问题要求实现并比较用于训练单隐藏单元ReLU神经网络的无约束梯度下降和投影梯度下降。分析的重点是将网络参数投影到$\\ell_{\\infty}$范数球上的稳定效应。\n\n首先，我们形式化模型和损失函数的梯度。数据集为 $\\{(x_i, y_i)\\}_{i=1}^n$，其中 $x_i \\in \\mathbb{R}^2$ 且 $y_i \\in \\mathbb{R}$。参数向量为 $\\theta = (w_{1,1}, w_{1,2}, b_1, w_2, b_2) \\in \\mathbb{R}^5$，可以分组为 $\\theta = (w_1, b_1, w_2, b_2)$，其中 $w_1 \\in \\mathbb{R}^2$。\n\n对于输入 $x$，网络的预测由下式给出：\n$$ f(x;\\theta) = w_2 \\cdot \\max\\{0, w_1^\\top x + b_1\\} + b_2 $$\n经验损失是均方误差 (MSE)：\n$$ L(\\theta) = \\frac{1}{n} \\sum_{i=1}^n \\left(f(x_i;\\theta) - y_i\\right)^2 $$\n\n为了实现基于梯度的优化，我们必须计算损失的梯度 $\\nabla L(\\theta)$。这通过应用链式法则来实现。令 $z_i = w_1^\\top x_i + b_1$ 为隐藏单元的预激活值，$a_i = \\max\\{0, z_i\\}$ 为其激活值（ReLU函数）。样本 $i$ 的预测为 $f_i = w_2 a_i + b_2$。\n\n损失函数相对于参数 $p \\in \\theta$ 的导数为：\n$$ \\frac{\\partial L}{\\partial p} = \\frac{1}{n} \\sum_{i=1}^n \\frac{\\partial}{\\partial p} (f_i - y_i)^2 = \\frac{1}{n} \\sum_{i=1}^n 2(f_i - y_i) \\frac{\\partial f_i}{\\partial p} $$\n我们定义共同的误差项 $\\delta_i = 2(f_i - y_i)$。每个参数的偏导数如下：\n\n1.  **相对于 $b_2$**：$\\frac{\\partial f_i}{\\partial b_2} = 1$。\n    $$ \\frac{\\partial L}{\\partial b_2} = \\frac{1}{n} \\sum_{i=1}^n \\delta_i $$\n2.  **相对于 $w_2$**：$\\frac{\\partial f_i}{\\partial w_2} = a_i$。\n    $$ \\frac{\\partial L}{\\partial w_2} = \\frac{1}{n} \\sum_{i=1}^n \\delta_i a_i $$\n3.  **相对于 $b_1$**：$\\frac{\\partial f_i}{\\partial b_1} = \\frac{\\partial f_i}{\\partial a_i} \\frac{\\partial a_i}{\\partial z_i} \\frac{\\partial z_i}{\\partial b_1} = w_2 \\cdot I(z_i  0) \\cdot 1$，其中 $I(\\cdot)$ 是指示函数。在 $z_i=0$ 处ReLU导数的次梯度被选为0，这与问题规范 $I(z_i  0)$ 一致。\n    $$ \\frac{\\partial L}{\\partial b_1} = \\frac{1}{n} \\sum_{i=1}^n \\delta_i w_2 I(w_1^\\top x_i + b_1  0) $$\n4.  **相对于 $w_1$**：$\\frac{\\partial f_i}{\\partial w_1} = \\frac{\\partial f_i}{\\partial a_i} \\frac{\\partial a_i}{\\partial z_i} \\frac{\\partial z_i}{\\partial w_1} = w_2 \\cdot I(z_i  0) \\cdot x_i$。\n    $$ \\nabla_{w_1} L = \\frac{1}{n} \\sum_{i=1}^n \\delta_i w_2 I(w_1^\\top x_i + b_1  0) x_i $$\n\n完整的梯度 $\\nabla L(\\theta) \\in \\mathbb{R}^5$ 由这些偏导数组合而成。\n\n两种优化过程是：\n1.  **无约束梯度下降**：一种迭代方法，它沿着与梯度相反的方向更新参数。\n    $$ \\theta_{t+1} = \\theta_t - \\eta \\nabla L(\\theta_t) $$\n    其中 $\\theta_t$ 是第 $t$ 次迭代的参数向量，$\\eta  0$ 是步长，$T$ 是总迭代次数。\n2.  **投影梯度下降 (PGD)**：此方法首先执行一个标准的梯度下降步骤，然后将得到的参数投影回可行集。可行集是半径为 $c$ 的$\\ell_{\\infty}$范数球，定义为 $S = \\{\\theta \\in \\mathbb{R}^5 : \\|\\theta\\|_\\infty \\leq c\\}$。\n    $$ \\theta_{t+1} = \\Pi_S(\\theta_t - \\eta \\nabla L(\\theta_t)) $$\n    $\\ell_{\\infty}$范数球的投影算子 $\\Pi_S$ 是一个分量裁剪函数：\n    $$ (\\Pi_S(\\theta))_j = \\text{clip}(\\theta_j, -c, c) = \\max(-c, \\min(\\theta_j, c)) $$\n    如果初始参数向量 $\\theta_0$ 在可行集 $S$ 之外，它将在第一次迭代前被投影到 $S$ 上。\n\n投影的稳定效应通过几个指标进行评估。我们跟踪最终损失 $L(\\theta_T)$、轨迹上的最大梯度范数 $\\|\\nabla L(\\theta_t)\\|_2$ 以及最大参数值 $\\|\\theta_t\\|_\\infty$。对于PGD，我们计算投影被激活的迭代次数（即至少有一个参数被裁剪）。根据无约束轨迹是否违反权重约束，或者在投影情况下最大梯度范数是否减少了至少 $\\delta=0.1$ 的相对裕度，来计算一个稳定性布尔指标。\n\n实现过程首先是生成确定性数据集。然后，对每个测试用例，从 $\\theta_0$ 开始运行两种优化算法，共进行 $T$ 次迭代。在这些运行过程中收集所有需要的指标。最后，汇总结果并按规定格式化。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and compares unconstrained and projected gradient descent\n    for a single-hidden-unit ReLU network.\n    \"\"\"\n\n    # Helper function to generate the deterministic dataset\n    def generate_data():\n        n = 50\n        i_vals = np.arange(1, n + 1)\n        x1 = i_vals / n\n        x2 = 0.5 * (-1)**i_vals\n        X = np.stack([x1, x2], axis=1).astype(np.float64)\n        Y = (x1 + x2 > 0.6).astype(np.float64)\n        return X, Y, n\n\n    # Model-related functions\n    def forward(theta, X):\n        w1, b1, w2, b2 = theta[0:2], theta[2], theta[3], theta[4]\n        z = X @ w1 + b1\n        a = np.maximum(0, z)\n        return w2 * a + b2\n\n    def loss(theta, X, Y):\n        f_pred = forward(theta, X)\n        return np.mean((f_pred - Y)**2)\n\n    def gradient(theta, X, Y, n):\n        w1, b1, w2, b2 = theta[0:2], theta[2], theta[3], theta[4]\n        \n        # Forward pass to get intermediate values\n        z = X @ w1 + b1\n        a = np.maximum(0, z)\n        f_pred = w2 * a + b2\n        \n        # Backward pass (chain rule)\n        d_loss_d_f = 2 * (f_pred - Y) / n\n        \n        grad_b2 = np.sum(d_loss_d_f)\n        grad_w2 = np.sum(d_loss_d_f * a)\n        \n        d_loss_d_pre_act = d_loss_d_f * w2 * (z > 0).astype(np.float64)\n        \n        grad_b1 = np.sum(d_loss_d_pre_act)\n        grad_w1 = d_loss_d_pre_act[:, np.newaxis] * X\n        grad_w1 = np.sum(grad_w1, axis=0)\n        \n        return np.array([grad_w1[0], grad_w1[1], grad_b1, grad_w2, grad_b2])\n\n    def project(theta, c):\n        return np.clip(theta, -c, c)\n\n    # Main function to run optimization for a single test case\n    def run_optimization_for_case(case_params, X, Y, n):\n        c, eta, T, theta0_tuple = case_params\n        theta0 = np.array(theta0_tuple, dtype=np.float64)\n        delta = 0.1\n\n        # --- Unconstrained Gradient Descent ---\n        theta_u = theta0.copy()\n        u_grad_norms = []\n        u_param_norms = [np.linalg.norm(theta_u, ord=np.inf)]\n        for _ in range(T):\n            grad_u = gradient(theta_u, X, Y, n)\n            u_grad_norms.append(np.linalg.norm(grad_u, ord=2))\n            theta_u -= eta * grad_u\n            u_param_norms.append(np.linalg.norm(theta_u, ord=np.inf))\n        \n        final_loss_u = loss(theta_u, X, Y)\n        max_grad_norm_u = np.max(u_grad_norms) if u_grad_norms else 0.0\n        max_param_norm_u = np.max(u_param_norms) if u_param_norms else 0.0\n\n        # --- Projected Gradient Descent ---\n        theta_p = theta0.copy()\n        # Enforce feasibility at initialization\n        if np.linalg.norm(theta_p, ord=np.inf) > c:\n            theta_p = project(theta_p, c)\n        \n        p_grad_norms = []\n        projection_activations = 0\n        for _ in range(T):\n            grad_p = gradient(theta_p, X, Y, n)\n            p_grad_norms.append(np.linalg.norm(grad_p, ord=2))\n            theta_temp = theta_p - eta * grad_p\n            if np.linalg.norm(theta_temp, ord=np.inf) > c:\n                projection_activations += 1\n            theta_p = project(theta_temp, c)\n            \n        final_loss_p = loss(theta_p, X, Y)\n        max_grad_norm_p = np.max(p_grad_norms) if p_grad_norms else 0.0\n\n        # --- Stabilization Check ---\n        stabilized = False\n        if max_param_norm_u > c:\n            stabilized = True\n        else:\n            # Use a small epsilon to avoid division by zero for stable gradients\n            if max_grad_norm_u > 1e-12:\n                rel_margin = (max_grad_norm_u - max_grad_norm_p) / max_grad_norm_u\n                if rel_margin >= delta:\n                    stabilized = True\n        \n        # Round numerical results to 6 decimal places\n        # We assume parameters are chosen such that results are finite.\n        return [\n            round(final_loss_p, 6),\n            round(final_loss_u, 6),\n            round(max_grad_norm_p, 6),\n            round(max_grad_norm_u, 6),\n            round(max_param_norm_u, 6),\n            projection_activations,\n            stabilized\n        ]\n\n    # --- Main Execution Logic ---\n    X, Y, n = generate_data()\n    test_cases = [\n        (0.5, 0.5, 100, (1.0, -1.0, 0.0, 2.0, 0.0)),\n        (10.0, 0.05, 300, (0.1, 0.1, 0.1, 0.1, 0.1)),\n        (1.0, 0.3, 200, (2.0, -2.0, 1.0, 8.0, -0.5)),\n        (1.0, 0.4, 150, (1.0, -0.2, 0.3, -1.0, 0.0)),\n    ]\n    \n    all_results = []\n    for case in test_cases:\n        all_results.append(run_optimization_for_case(case, X, Y, n))\n    \n    # Format the final output string as a list of lists with no spaces\n    print(str(all_results).replace(\" \", \"\"))\n\nsolve()\n```", "id": "3154367"}]}