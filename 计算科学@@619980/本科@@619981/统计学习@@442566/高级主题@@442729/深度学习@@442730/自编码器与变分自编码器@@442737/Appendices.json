{"hands_on_practices": [{"introduction": "训练变分自编码器（VAE）依赖于反向传播来最小化一个损失函数。这个过程的一个关键步骤是计算重构损失（reconstruction loss）相对于网络输出的梯度。本练习将通过推导二元交叉熵损失的梯度，揭示一个驱动学习的、形式上惊人简洁且直观的误差信号，从而帮助您建立对VAE学习机制的基础理解。这个练习是理解解码器如何根据误差调整自身参数的第一步 [@problem_id:66106]。", "problem": "在逆向材料设计领域，变分自编码器 (VAE) 是一种强大的生成模型，用于学习材料的连续潜在表示，并生成具有所需性质的新型材料结构。一个常见的应用是生成结构指纹，这些指纹通常表示为高维二元向量。\n\n考虑一个 VAE，其解码器网络负责重建一个 $D$ 维的二元材料指纹，记为向量 $x \\in \\{0, 1\\}^D$。解码器的最后一层产生一个激活前值的向量 $z \\in \\mathbb{R}^D$。通过逐元素应用 Sigmoid 激活函数，这些激活前的值被转换为重建的指纹概率 $\\hat{x} \\in (0, 1)^D$：\n$$ \\hat{x}_i = \\sigma(z_i) = \\frac{1}{1 + \\exp(-z_i)} $$\n对于每个分量 $i=1, \\dots, D$。\n\n重建的质量由二元交叉熵 (BCE) 损失来衡量，它是 VAE 总损失函数的一个组成部分。BCE 重建损失由下式给出：\n$$ L_{rec}(x, \\hat{x}) = - \\sum_{i=1}^{D} [x_i \\log(\\hat{x}_i) + (1 - x_i) \\log(1 - \\hat{x}_i)] $$\n其中 $\\log$ 表示自然对数。\n\n为了使用像反向传播这样的基于梯度的优化方法来训练 VAE，有必要计算损失函数相对于网络参数的梯度。这个过程中的一个关键步骤是求取损失相对于最后一层激活前输出 $z$ 的梯度。\n\n推导重建损失相对于激活前向量 $z$ 的梯度向量的解析表达式，记为 $\\nabla_z L_{rec}$。", "solution": "重建损失为\n$$\nL_{rec}(x,\\hat x)=-\\sum_{i=1}^D\\bigl[x_i\\ln\\hat x_i+(1-x_i)\\ln(1-\\hat x_i)\\bigr],\n\\quad \\hat x_i=\\sigma(z_i),\\quad \\sigma'(z_i)=\\hat x_i(1-\\hat x_i).\n$$\n逐项微分，\n$$\n\\frac{\\partial L_{rec}}{\\partial z_i}\n=-\\Bigl[x_i\\frac{1}{\\hat x_i}-(1-x_i)\\frac{1}{1-\\hat x_i}\\Bigr]\\sigma'(z_i)\n=-\\Bigl[x_i(1-\\hat x_i)-(1-x_i)\\hat x_i\\Bigr]\n=\\hat x_i-x_i.\n$$\n因此，向量形式为，\n$$\n\\nabla_z L_{rec}=\\hat x-x.\n$$", "answer": "$$\\boxed{\\hat{x}-x}$$", "id": "66106"}, {"introduction": "变分自编码器（VAE）中“变分”二字是其与标准自编码器的核心区别，也赋予了它强大的生成能力。这通过将输入编码为潜空间中的一个概率分布而非单个点来实现。本练习通过一个思想实验，探讨潜空间中随机性的关键作用 [@problem_id:2439791]。通过思考当潜空间采样方差强制为零时会发生什么，我们可以清晰地理解为何KL散度（Kullback–Leibler divergence）正则化项对于构建结构化的潜空间和生成新颖、多样化的数据至关重要。", "problem": "一个变分自编码器 (Variational Autoencoder, VAE) 在单细胞 RNA 测序 (scRNA-seq) 基因表达数据上进行训练，其中每个细胞由一个向量 $x \\in \\mathbb{N}^G$ 表示。编码器定义了一个近似后验 $q_{\\phi}(z \\mid x) = \\mathcal{N}(\\mu_{\\phi}(x), \\operatorname{diag}(\\sigma_{\\phi}^2(x)))$，该后验是关于潜变量 $z \\in \\mathbb{R}^d$ 的，其先验为 $p(z) = \\mathcal{N}(0, I)$。解码器指定了一个适用于计数数据的似然 $p_{\\theta}(x \\mid z)$。训练过程最大化证据下界 (Evidence Lower Bound, ELBO)：\n$$\n\\mathcal{L}(\\theta,\\phi;x) = \\mathbb{E}_{q_{\\phi}(z \\mid x)}\\left[\\log p_{\\theta}(x \\mid z)\\right] - D_{\\mathrm{KL}}\\!\\left(q_{\\phi}(z \\mid x)\\,\\|\\,p(z)\\right),\n$$\n使用重参数化技巧 $z = \\mu_{\\phi}(x) + \\sigma_{\\phi}(x) \\odot \\epsilon$（其中 $\\epsilon \\sim \\mathcal{N}(0, I)$）。考虑修改训练过程，以便在潜变量采样步骤中，将方差设置为零，即对于每个 $x$，$z$ 都被确定性地设置为 $z = \\mu_{\\phi}(x)$。\n\n哪种说法最能描述这种修改对生物数据学习和底层目标的影响？\n\nA. 模型有效地变成了一个确定性自编码器，其中 $z=\\mu_{\\phi}(x)$，破坏了随机性和不确定性校准；当 $\\sigma_{\\phi}(x) \\to 0$ 时，Kullback–Leibler 散度 $D_{\\mathrm{KL}}(q_{\\phi}(z \\mid x)\\,\\|\\,p(z))$ 发散，并且合成细胞的生成多样性崩溃。\n\nB. 噪声的去除提高了样本多样性，因为解码器接收到更“干净”的潜码，并且训练更稳定，不影响变分目标。\n\nC. 它通过强制 $q_{\\phi}(z \\mid x)$ 与先验匹配（即 $\\mu_{\\phi}(x)\\approx 0$ 和 $\\sigma_{\\phi}(x)\\approx 1$）来引发后验坍塌，从而增强了解耦和生成能力。\n\nD. 消除 $z$ 中的随机性使得证据下界等于真实的对数证据，因为蒙特卡洛估计误差消失了。\n\nE. 它只会加速训练；不确定性估计和生成属性不受影响，因为解码器可以重新引入可变性。", "solution": "在尝试给出解决方案之前，将对问题陈述进行验证。\n\n### 步骤 1：提取已知条件\n-   **模型**：变分自编码器 (VAE)。\n-   **数据**：单细胞 RNA 测序 (scRNA-seq) 基因表达数据，由向量 $x \\in \\mathbb{N}^G$ 表示。变量 $G$ 代表基因数量。\n-   **编码器（近似后验）**：$q_{\\phi}(z \\mid x) = \\mathcal{N}(\\mu_{\\phi}(x), \\operatorname{diag}(\\sigma_{\\phi}^2(x)))$。\n-   **潜变量**：$z \\in \\mathbb{R}^d$，其中 $d$ 是潜空间的维度。\n-   **先验分布**：$p(z) = \\mathcal{N}(0, I)$，一个标准多变量正态分布。\n-   **解码器（似然）**：$p_{\\theta}(x \\mid z)$，指定为适用于计数数据的分布。\n-   **目标函数**：最大化证据下界 (ELBO)：\n    $$\n    \\mathcal{L}(\\theta,\\phi;x) = \\mathbb{E}_{q_{\\phi}(z \\mid x)}\\left[\\log p_{\\theta}(x \\mid z)\\right] - D_{\\mathrm{KL}}\\!\\left(q_{\\phi}(z \\mid x)\\,\\|\\,p(z)\\right)\n    $$\n-   **训练方法**：使用重参数化技巧进行采样：$z = \\mu_{\\phi}(x) + \\sigma_{\\phi}(x) \\odot \\epsilon$，其中 $\\epsilon \\sim \\mathcal{N}(0, I)$。\n-   **提议的修改**：在潜变量采样步骤中，将方差设置为零，这意味着潜变量被确定性地设置为 $z = \\mu_{\\phi}(x)$，适用于每个输入 $x$。\n\n### 步骤 2：使用提取的已知条件进行验证\n评估问题陈述的有效性。\n\n-   **科学基础**：该问题描述了一个标准的 VAE 架构和训练目标。它在 scRNA-seq 数据上的应用是计算生物学中一个成熟且重要的领域。编码器、先验、ELBO 和重参数化技巧的数学公式都是正确且标准的。提议的修改是一个假设性的“如果……会怎样”的情景，旨在测试对 VAE 基本原理的理解，这是一种有效的科学和教学方法。该问题牢固地建立在机器学习和统计学的既定原则之上。\n-   **问题定义良好**：问题要求分析一个特定修改的后果。这是一个清晰明确的问题，可以从 VAE 的数学定义中推导出确定的答案。\n-   **客观性**：语言正式、精确，没有主观性陈述。\n\n该问题在科学上是合理的、自洽的且定义良好。不存在不一致或逻辑缺陷。\n\n### 步骤 3：结论与行动\n问题陈述是**有效的**。将推导解决方案。\n\n### 解题推导\n该问题要求分析将潜变量 $z$ 确定性地设置为 $z = \\mu_{\\phi}(x)$ 来修改 VAE 训练过程所带来的后果。这等价于将近似后验的方差 $\\sigma_{\\phi}^2(x)$ 的极限取为零。我们必须分析此修改对 ELBO 目标函数两项的影响。\n\nELBO 由下式给出：\n$$\n\\mathcal{L}(\\theta,\\phi;x) = \\underbrace{\\mathbb{E}_{q_{\\phi}(z \\mid x)}\\left[\\log p_{\\theta}(x \\mid z)\\right]}_{\\text{重构项}} - \\underbrace{D_{\\mathrm{KL}}\\!\\left(q_{\\phi}(z \\mid x)\\,\\|\\,p(z)\\right)}_{\\text{正则化项}}\n$$\n\n$1$. **重构项分析**：\n重构项是在给定潜变量下数据的期望对数似然。期望是针对近似后验 $q_{\\phi}(z \\mid x)$ 计算的。当我们强制 $z = \\mu_{\\phi}(x)$ 时，我们实际上是用一个以 $\\mu_{\\phi}(x)$ 为中心的狄拉克δ函数替换了分布 $q_{\\phi}(z \\mid x)$。对狄拉克δ函数的期望简化为其中心点的求值。因此，重构项变为：\n$$\n\\mathbb{E}_{q_{\\phi}(z \\mid x)}\\left[\\log p_{\\theta}(x \\mid z)\\right] \\longrightarrow \\log p_{\\theta}(x \\mid z=\\mu_{\\phi}(x))\n$$\n这是一个标准的、确定性自编码器的目标。编码器将输入 $x$ 映射到潜空间中的一个单点 $\\mu_{\\phi}(x)$，解码器则尝试从该点重构 $x$。这一改变完全消除了潜编码过程中的随机性，随之而来的是模型表示潜空间不确定性（即，对于观察到的表达谱 $x$，其“真实”细胞状态的不确定性）的能力也丧失了。\n\n$2$. **正则化项分析**：\n正则化项是近似后验 $q_{\\phi}(z \\mid x) = \\mathcal{N}(\\mu_{\\phi}(x), \\operatorname{diag}(\\sigma_{\\phi}^2(x)))$ 与先验 $p(z) = \\mathcal{N}(0, I)$ 之间的 Kullback–Leibler (KL) 散度。该 KL 散度的解析表达式为：\n$$\nD_{\\mathrm{KL}}\\!\\left(q_{\\phi}(z \\mid x)\\,\\|\\,p(z)\\right) = \\frac{1}{2} \\sum_{j=1}^{d} \\left( \\mu_{\\phi,j}^2(x) + \\sigma_{\\phi,j}^2(x) - 1 - \\log(\\sigma_{\\phi,j}^2(x)) \\right)\n$$\n其中求和遍及潜空间的 $d$ 个维度。\n该修改意味着对于所有维度 $j=1, \\dots, d$，都有 $\\sigma_{\\phi,j}^2(x) \\to 0$。让我们在极限情况下检查各项：\n-   $\\mu_{\\phi,j}^2(x)$ 保持有限。\n-   $\\sigma_{\\phi,j}^2(x) \\to 0$。\n-   $\\log(\\sigma_{\\phi,j}^2(x)) \\to -\\infty$。\n\n因此，$-\\log(\\sigma_{\\phi,j}^2(x))$ 项趋向于 $+\\infty$。结果，整个 KL 散度发散到正无穷：\n$$\n\\lim_{\\sigma_{\\phi}^2(x) \\to 0} D_{\\mathrm{KL}}\\!\\left(q_{\\phi}(z \\mid x)\\,\\|\\,p(z)\\right) = \\infty\n$$\n由于 KL 散度项在 ELBO 中是被减去的，目标函数 $\\mathcal{L}(\\theta,\\phi;x)$ 会趋向于 $-\\infty$。对于一个最大化问题，这是一个灾难性的失败。优化过程将被一个针对零方差的无限惩罚所主导。\n\n$3$. **对模型属性的影响**：\n-   **生成能力**：KL 正则化的一个关键目的是迫使所有数据点的聚合后验分布近似于先验 $p(z)$。这构造了潜空间，确保其是密集和连续的，从而允许通过从先验 $p(z)$ 中采样一个潜向量 $z'$ 并用 $p_{\\theta}(x \\mid z')$ 对其解码来有意义地生成新数据。通过有效地移除此正则化（或使其成为一个无限的惩罚），编码器可以自由地将潜均值 $\\mu_{\\phi}(x)$ 放置在潜空间中的任何位置以优化重构。这将导致一个“破碎”或“有间隙”的潜空间，其中从先验 $p(z)$ 抽取的大多数点都不对应于任何学习到的数据流形，导致解码器产生无意义的输出。生成多样性因此崩溃。\n-   **不确定性校准**：VAE 通过为每个数据点学习一个分布 $q_{\\phi}(z \\mid x)$ 来建模不确定性。方差 $\\sigma_{\\phi}^2(x)$ 量化了潜表示的不确定性。根据定义，将此方差设置为零会破坏此能力。\n\n### 逐项分析\n\n**A. 模型有效地变成了一个确定性自编码器，其中 $z=\\mu_{\\phi}(x)$，破坏了随机性和不确定性校准；当 $\\sigma_{\\phi}(x) \\to 0$ 时，Kullback–Leibler 散度 $D_{\\mathrm{KL}}(q_{\\phi}(z \\mid x)\\,\\|\\,p(z))$ 发散，并且合成细胞的生成多样性崩溃。**\n-   `变成了一个确定性自编码器，其中 z=mu_phi(x)`：正确，如重构项分析所示。\n-   `破坏了随机性和不确定性校准`：正确。后验变成了一个点质量。\n-   `当 sigma_phi(x) -> 0 时 ... KL 散度 ... 发散`：正确，从 KL 散度的解析公式推导得出。\n-   `生成多样性 ... 崩溃`：正确，由于失去了 KL 项的正则化效果。\n这个陈述准确地总结了所有关键后果。**正确**。\n\n**B. 噪声的去除提高了样本多样性，因为解码器接收到更“干净”的潜码，并且训练更稳定，不影响变分目标。**\n-   `提高了样本多样性`：不正确。它破坏了生成能力并导致多样性崩溃。\n-   `解码器接收到更“干净”的潜码`：虽然从技术上讲，潜码不再有噪声是正确的，但这对于 VAE 框架是有害的。\n-   `训练更稳定`：不正确。发散的 KL 项会使训练在数值上不稳定。\n-   `不影响变分目标`：不正确。它从根本上改变了目标，使其变得不适定。\n**不正确**。\n\n**C. 它通过强制 $q_{\\phi}(z \\mid x)$ 与先验匹配（即 $\\mu_{\\phi}(x)\\approx 0$ 和 $\\sigma_{\\phi}(x)\\approx 1$）来引发后验坍塌，从而增强了解耦和生成能力。**\n-   `引发后验坍塌`：不正确。后验坍塌发生在对于所有 $x$，后验 $q_{\\phi}(z \\mid x)$ 都变得等于先验 $p(z)$ 时，意味着 $\\mu_{\\phi}(x) \\approx 0$ 和 $\\sigma_{\\phi}(x) \\approx 1$。提议的修改强制 $\\sigma_{\\phi}(x) \\to 0$，这与后验坍塌相反。\n-   `增强了解耦和生成能力`：不正确。这两个属性都会严重退化或被破坏。\n**不正确**。\n\n**D. 消除 $z$ 中的随机性使得证据下界等于真实的对数证据，因为蒙特卡洛估计误差消失了。**\n-   这个陈述混淆了两个不同的概念。ELBO 和真实对数证据 $\\log p(x)$ 之间的差距是近似后验与真实后验之间的 KL 散度，即 $D_{\\mathrm{KL}}(q_{\\phi}(z \\mid x) \\| p_{\\theta}(z \\mid x))$。这个差距不会消失；$q$ 中的一个δ函数通常比一个学习到的高斯分布更能差地近似真实后验，所以这个差距很可能会增加。蒙特卡洛估计误差指的是估计重构项梯度的误差，而不是 ELBO 本身相对于对数证据的值。\n**不正确**。\n\n**E. 它只会加速训练；不确定性估计和生成属性不受影响，因为解码器可以重新引入可变性。**\n-   `只会加速训练`：不正确。主要影响在于模型的基本属性。此外，训练很可能变得不稳定，而不是更快。\n-   `不确定性估计 ... 不受影响`：不正确。它们被完全破坏了。\n-   `生成属性不受影响`：不正确。它们崩溃了。\n-   `解码器可以重新引入可变性`：解码器建模的是观测噪声，这与潜变量不确定性以及生成新数据类型的机制有根本的不同。\n**不正确**。", "answer": "$$\\boxed{A}$$", "id": "2439791"}, {"introduction": "变分自编码器（VAE）的目标函数，即证据下界（ELBO），需要在两个相互竞争的目标之间取得精妙的平衡：一是精确的数据重构，二是潜空间的正则化。本练习将深入探讨VAE训练动态中的一个复杂问题：学习观测噪声水平如何可能破坏这种平衡，并导致一种被称为“后验坍塌”（posterior collapse）的训练失败模式 [@problem_id:3100707]。理解这一挑战对于在实践中训练稳健的VAE至关重要，它鼓励您像从业者一样思考，分析问题并评估稳定训练的有效策略。", "problem": "考虑一个具有各向同性高斯观测模型的变分自编码器 (VAE)。设观测数据为 $\\{x^{(i)}\\}_{i=1}^{n}$，其中每个 $x^{(i)} \\in \\mathbb{R}^{d}$，潜变量为 $z \\in \\mathbb{R}^{k}$，解码器均值为 $f_{\\theta}(z)$，以及一个可学习的标量观测方差 $\\sigma^{2} > 0$，使得 $p(x \\mid z) = \\mathcal{N}\\!\\big(f_{\\theta}(z), \\sigma^{2} I_{d}\\big)$。训练目标是证据下界 (ELBO)，\n$$\n\\mathcal{L}(\\theta, \\phi, \\sigma^{2})\n=\n\\mathbb{E}_{q_{\\phi}(z \\mid x)}\\!\\left[\\log p_{\\theta}(x \\mid z)\\right]\n-\n\\mathrm{KL}\\!\\left(q_{\\phi}(z \\mid x)\\,\\|\\,p(z)\\right),\n$$\n其中 $\\mathrm{KL}$ 表示 Kullback-Leibler (KL) 散度。假设 $p(z)$ 是一个固定的标准正态先验，而 $q_{\\phi}(z \\mid x)$ 是一个高斯编码器。\n\n从第一性原理分析，学习观测方差 $\\sigma^{2}$ 如何改变重建项 $\\mathbb{E}_{q_{\\phi}(z \\mid x)}\\!\\left[\\log p_{\\theta}(x \\mid z)\\right]$ 和潜变量正则化项 $\\mathrm{KL}\\!\\left(q_{\\phi}(z \\mid x)\\,\\|\\,p(z)\\right)$ 之间的有效平衡，并提出一个有原则的校准程序以避免平凡解，例如后验坍塌（即 $q_{\\phi}(z \\mid x) \\approx p(z)$ 且解码器忽略 $z$）。\n\n选择所有正确的陈述：\n\nA. 学习 $\\sigma^{2}$ 会在参数梯度中以一个与 $1/\\sigma^{2}$ 成比例的因子有效地重新缩放重建项的贡献，这会使得当 $\\sigma^{2}$ 增加时，KL 项相对更占主导地位；一种有原则的校准方法是对 $\\sigma^{2}$ 施加一个共轭逆伽马先验并执行最大后验 (MAP) 估计，以防止 $\\sigma^{2}$ 膨胀并稳定这种权衡。\n\nB. 当学习 $\\sigma^{2}$ 时，ELBO 对输入数据 $x$ 的任何重新缩放都变得不变，因此不需要校准。\n\nC. 联合优化 $\\sigma^{2}$ 存在一个闭式更新，该更新将 $\\sigma^{2}$ 匹配到平均每维度重建残差，如果无约束地使用，这可能让模型在早期增加 $\\sigma^{2}$ 并忽略潜变量编码；一个实用的校准方法是在初始预热阶段将 $\\sigma^{2}$ 固定在一个目标噪声水平（或等效地，在预热期间使用一个 $\\beta < 1$ 的 $\\beta$-VAE），之后再释放 $\\sigma^{2}$ 并为其设置一个下界。\n\nD. 学习 $\\sigma^{2}$ 降低了后验坍塌的风险，因为减小重建项的权重会迫使潜变量编码携带更多信息。\n\nE. 一个合理的校准方法是将 $x$ 归一化为单位方差并将 $\\sigma^{2}$ 固定为 1，或者在可用时将 $\\sigma^{2}$ 设置为已知的传感器噪声水平，这样重建项就能反映物理噪声的尺度，而 KL 项不会因为任意选择 $\\sigma^{2}$ 而被隐式地增加权重。", "solution": "用户希望我分析可学习观测方差 $\\sigma^2$ 在变分自编码器 (VAE) 中的作用，并评估几个关于其影响和潜在校准方法的陈述。\n\n首先，我将形式化训练目标并分析 $\\sigma^2$ 的影响。问题陈述 VAE 具有各向同性高斯观测模型，$p(x \\mid z) = \\mathcal{N}\\!\\big(f_{\\theta}(z), \\sigma^{2} I_{d}\\big)$。给定潜变量编码 $z \\in \\mathbb{R}^k$，单个数据点 $x \\in \\mathbb{R}^d$ 的对数似然是：\n$$\n\\log p_{\\theta}(x \\mid z) = \\log \\left( \\frac{1}{(2\\pi \\sigma^2)^{d/2}} \\exp\\left(-\\frac{\\|x - f_{\\theta}(z)\\|^2}{2\\sigma^2}\\right) \\right)\n$$\n$$\n\\log p_{\\theta}(x \\mid z) = -\\frac{d}{2} \\log(2\\pi) - \\frac{d}{2} \\log(\\sigma^2) - \\frac{1}{2\\sigma^2} \\|x - f_{\\theta}(z)\\|^2\n$$\n训练目标是最大化每个数据点的证据下界 (ELBO)：\n$$\n\\mathcal{L}(\\theta, \\phi, \\sigma^{2}) = \\mathbb{E}_{q_{\\phi}(z \\mid x)}\\!\\left[\\log p_{\\theta}(x \\mid z)\\right] - \\mathrm{KL}\\!\\left(q_{\\phi}(z \\mid x)\\,\\|\\,p(z)\\right)\n$$\n代入 $\\log p_{\\theta}(x \\mid z)$ 的表达式：\n$$\n\\mathcal{L}(\\theta, \\phi, \\sigma^{2}) = \\mathbb{E}_{q_{\\phi}(z \\mid x)}\\!\\left[-\\frac{d}{2} \\log(2\\pi) - \\frac{d}{2} \\log(\\sigma^2) - \\frac{1}{2\\sigma^2} \\|x - f_{\\theta}(z)\\|^2\\right] - \\mathrm{KL}\\!\\left(q_{\\phi}(z \\mid x)\\,\\|\\,p(z)\\right)\n$$\n由于涉及 $\\sigma^2$ 和常数的项不依赖于 $z$，我们可以将它们从期望中提出来：\n$$\n\\mathcal{L}(\\theta, \\phi, \\sigma^{2}) = -\\frac{d}{2} \\log(2\\pi) - \\frac{d}{2} \\log(\\sigma^2) - \\frac{1}{2\\sigma^2} \\mathbb{E}_{q_{\\phi}(z \\mid x)}\\!\\left[\\|x - f_{\\theta}(z)\\|^2\\right] - \\mathrm{KL}\\!\\left(q_{\\phi}(z \\mid x)\\,\\|\\,p(z)\\right)\n$$\n训练 VAE 涉及最大化这个 $\\mathcal{L}$，这等价于最小化负 ELBO，通常表示为损失函数 $L_{VAE} = -\\mathcal{L}$。我们忽略常数项 $-\\frac{d}{2} \\log(2\\pi)$:\n$$\nL_{VAE} = \\frac{1}{2\\sigma^2} \\mathbb{E}_{q_{\\phi}(z \\mid x)}\\!\\left[\\|x - f_{\\theta}(z)\\|^2\\right] + \\frac{d}{2} \\log(\\sigma^2) + \\mathrm{KL}\\!\\left(q_{\\phi}(z \\mid x)\\,\\|\\,p(z)\\right)\n$$\n这种形式揭示了核心的权衡关系。损失由三项组成：一个由 $\\frac{1}{2\\sigma^2}$ 加权的重建误差项，一个惩罚大 $\\sigma^2$ 的项（来自高斯分布的归一化常数），以及 KL 散度正则化项。\n\n参数 $\\sigma^2$ 直接调节重建和正则化之间的平衡。如果我们固定 $\\theta$ 和 $\\phi$ 并对 $\\sigma^2$ 进行优化，我们可以对 $\\mathcal{L}$ 关于 $\\sigma^2$ 求导并令其为 0。设 $R = \\mathbb{E}_{q_{\\phi}(z \\mid x)}\\!\\left[\\|x - f_{\\theta}(z)\\|^2\\right]$ 为期望平方重建误差。\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial (\\sigma^2)} = \\frac{1}{2(\\sigma^2)^2} R - \\frac{d}{2\\sigma^2} = 0\n$$\n$$\n\\implies \\sigma^2 = \\frac{R}{d} = \\frac{1}{d} \\mathbb{E}_{q_{\\phi}(z \\mid x)}\\!\\left[\\|x - f_{\\theta}(z)\\|^2\\right]\n$$\n这表明最优的 $\\sigma^2$ 是平均每维度重建误差。\n\n在联合优化期间，如果解码器尚未训练好，重建误差 $R$ 会很大。优化过程会推动 $\\sigma^2$ 变大以进行补偿。随着 $\\sigma^2$ 增加，重建项上的权重 $\\frac{1}{2\\sigma^2}$ 会减小。这使得 KL 散度项在总损失中相对更占主导地位。为了最小化现在占主导地位的 KL 项，模型会推动近似后验 $q_{\\phi}(z \\mid x)$ 去匹配先验 $p(z)$。这就是所谓的“后验坍塌”。潜变量编码 $z$ 不再携带关于输入 $x$ 的重要信息，解码器学会忽略它们，输出一个平均的重建结果。这是一个平凡的、不希望出现的局部最小值。\n\n现在我将评估每个选项。\n\n**A. 学习 $\\sigma^{2}$ 会在参数梯度中以一个与 $1/\\sigma^{2}$ 成比例的因子有效地重新缩放重建项的贡献，这会使得当 $\\sigma^{2}$ 增加时，KL 项相对更占主导地位；一种有原则的校准方法是对 $\\sigma^{2}$ 施加一个共轭逆伽马先验并执行最大后验 (MAP) 估计，以防止 $\\sigma^{2}$ 膨胀并稳定这种权衡。**\n这个陈述是正确的。要最小化的损失是 $L_{VAE} = \\frac{1}{2\\sigma^2}R + \\frac{d}{2}\\log(\\sigma^2) + \\mathrm{KL}$。重建误差 R 对损失的贡献确实被 $\\frac{1}{2\\sigma^2}$ 缩放。随着 $\\sigma^2$ 增加，这个缩放因子减小，从而降低了重建项相对于 KL 项的重要性。这可能导致如上所述的后验坍塌。对 $\\sigma^2$ 施加先验是正则化参数的一种标准贝叶斯方法。高斯分布方差的共轭先验是逆伽马分布。使用此先验进行最大后验估计会向损失中添加一个正则化项，该项惩罚远离先验众数的方差值，从而有效防止 $\\sigma^2$ 不受控制地增长。这稳定了各项之间的权衡。\n**结论：正确。**\n\n**B. 当学习 $\\sigma^{2}$ 时，ELBO 对输入数据 $x$ 的任何重新缩放都变得不变，因此不需要校准。**\n这个陈述是错误的。我们来检验不变性的说法。假设我们对数据进行重新缩放 $x' = c x$，其中标量 $c > 0$。一个用于重缩放数据的最优解码器会学会相应地缩放其输出，$f'_{\\theta}(z) = c f_{\\theta}(z)$。新的重建误差是 $R' = \\mathbb{E}[\\|x' - f'_{\\theta}(z)\\|^2] = c^2 \\mathbb{E}[\\|x-f_{\\theta}(z)\\|^2] = c^2 R$。对于重缩放问题，最优方差将是 $(\\sigma')^2 = R'/d = c^2 R/d = c^2 \\sigma^2$。让我们将这些代入 ELBO 中依赖于数据尺度的部分：$\\mathcal{L}_{recon} = -\\frac{1}{2\\sigma^2} R - \\frac{d}{2}\\log(\\sigma^2)$。对于重缩放问题，这变为：\n$$\n\\mathcal{L}'_{recon} = -\\frac{1}{2(\\sigma')^2} R' - \\frac{d}{2}\\log((\\sigma')^2) = -\\frac{1}{2(c^2\\sigma^2)} (c^2 R) - \\frac{d}{2}\\log(c^2\\sigma^2)\n$$\n$$\n= -\\frac{1}{2\\sigma^2} R - \\frac{d}{2}(\\log(c^2) + \\log(\\sigma^2)) = \\left(-\\frac{1}{2\\sigma^2} R - \\frac{d}{2}\\log(\\sigma^2)\\right) - \\frac{d}{2}\\log(c^2)\n$$\n$$\n= \\mathcal{L}_{recon} - d \\log(c)\n$$\nELBO 不是不变的；它改变了一个常数量 $-d \\log(c)$。因此，前提是错误的，校准仍然是一个相关的问题。\n**结论：错误。**\n\n**C. 联合优化 $\\sigma^{2}$ 存在一个闭式更新，该更新将 $\\sigma^{2}$ 匹配到平均每维度重建残差，如果无约束地使用，这可能让模型在早期增加 $\\sigma^{2}$ 并忽略潜变量编码；一个实用的校准方法是在初始预热阶段将 $\\sigma^{2}$ 固定在一个目标噪声水平（或等效地，在预热期间使用一个 $\\beta  < 1$ 的 $\\beta$-VAE），之后再释放 $\\sigma^{2}$ 并为其设置一个下界。**\n这个陈述准确地描述了其底层机制和一个标准的实用解决方案。如上所推导，对于固定的 $\\theta, \\phi$，最优的 $\\sigma^2$ 是 $\\sigma^2 = R/d$，即平均每维度重建残差。关于这可能导致 $\\sigma^2$ 早期增加和随后的后验坍塌的分析也是正确的。一个常见且有效的缓解策略是“预热”VAE。在这个初始阶段，可以要么将 $\\sigma^2$ 固定到一个合理的值（例如 $\\sigma^2=1$），要么等效地通过将其乘以一个逐渐退火到 1 的因子 $\\beta < 1$ 来减小 KL 项的权重。两种方法最初都给予重建项更高的相对权重，迫使解码器学习使用潜变量编码 $z$ 进行重建。预热后，可以联合学习 $\\sigma^2$，通常带有一个小的下界以防止数值上坍塌到零。\n**结论：正确。**\n\n**D. 学习 $\\sigma^{2}$ 降低了后验坍塌的风险，因为减小重建项的权重会迫使潜变量编码携带更多信息。**\n这个陈述的说法与实际情况相反。学习 $\\sigma^2$ 允许模型增加它，这会*减小*重建项的权重。对重建误差的较小惩罚为模型将输入 $x$ 的有用信息编码到潜变量编码 $z$ 中提供了*更少*的激励。模型不那么被“强迫”使用 $z$。因此，模型更容易通过使 $q_{\\phi}(z \\mid x)$ 信息贫乏且接近先验 $p(z)$ 来最小化 KL 项。因此，在没有校准的情况下学习 $\\sigma^2$ 会*增加*后验坍塌的风险，而不是减少它。\n**结论：错误。**\n\n**E. 一个合理的校准方法是将 $x$ 归一化为单位方差并将 $\\sigma^{2}$ 固定为 1，或者在可用时将 $\\sigma^{2}$ 设置为已知的传感器噪声水平，这样重建项就能反映物理噪声的尺度，而 KL 项不会因为任意选择 $\\sigma^{2}$ 而被隐式地增加权重。**\n这个陈述提出了另一种有效的策略：根本不学习 $\\sigma^2$，而是将其固定到一个有原则的值。如果输入数据 $x$ 经过预处理，在每个维度上具有近似单位方差，那么将观测方差固定为 $\\sigma^2=1$ 就建立了一个固定的、合理的权衡。它隐含地表明，模型应该解释数据结构，任何剩余的残差误差都被视为噪声，其方差与数据本身的方差相当。如果物理噪声过程是已知的（例如，来自传感器规格），将 $\\sigma^2$ 固定为该已知值是一种更有原则的方法。在这两种情况下，通过固定 $\\sigma^2$，我们阻止模型动态地改变重建项的权重，从而避免导致后验坍塌的不稳定性。这是一种合理且常见的做法。\n**结论：正确。**\n\n总而言之，陈述 A、C 和 E 是关于学习 $\\sigma^2$ 的影响和有效的校准策略的正确描述。", "answer": "$$\\boxed{ACE}$$", "id": "3100707"}]}