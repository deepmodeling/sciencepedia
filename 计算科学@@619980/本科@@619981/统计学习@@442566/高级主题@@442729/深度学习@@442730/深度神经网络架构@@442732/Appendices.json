{"hands_on_practices": [{"introduction": "在设计神经网络时，一个核心的权衡是在固定的参数预算下，是构建一个更宽的网络还是一个更深的网络。这个实践练习 ([@problem_id:3113786]) 将引导你通过一个简化的理论模型来量化探索这一权衡。通过将抽象的近似误差和估计误差概念转化为一个具体的优化问题，你将学会如何在实际中导航这一关键的架构设计空间。", "problem": "考虑具有整流线性单元（ReLU）激活函数、输入维度为 $d$、一个标量输出、隐藏层宽度为 $m$（所有隐藏层宽度相同）以及深度为 $L$（隐藏层的数量）的全连接深度前馈神经网络。设总参数预算为 $P$。在使用经验风险最小化（ERM）的监督学习中，将期望风险标准地分解为近似误差和估计误差，这启发我们在固定预算下寻找一个能够平衡表达能力和可学习性的架构。\n\n使用以下与上下文相适应的基本原理：\n- 偏差-方差权衡与近似-估计分解：一个训练好的网络的期望风险可以分解为一个近似误差项和一个估计误差项。\n- 容量度量与泛化：估计误差与诸如 Rademacher 复杂度之类的容量度量成比例，后者随着参数数量和深度的增加而增加。\n- 全连接网络的参数计数：参数数量是各层权重和偏置的总和。\n\n通过计算权重和偏置，为具有 $L$ 个宽度为 $m$ 的隐藏层和输入维度为 $d$ 的网络定义总参数数量 $P_{\\text{total}}(m,L;d)$：\n- 第一层权重：$d \\cdot m$；第一层偏置：$m$。\n- 第 2 到第 $L$ 个隐藏层：共 $(L-1)$ 层，每层有 $m \\cdot m$ 个权重和 $m$ 个偏置。\n- 输出层权重：$m$；输出层偏置：$1$。\n\n因此，\n$$\nP_{\\text{total}}(m,L;d) \\;=\\; (L-1)\\,m^2 \\;+\\; (d+L+1)\\,m \\;+\\; 1.\n$$\n\n将数据集大小为 $n$ 时架构 $(m,L)$ 的泛化误差上界建模为近似项和估计项之和：\n- 近似误差项：\n$$\n\\mathcal{A}(m,L) \\;=\\; \\frac{C_a}{\\,m^{\\gamma}\\,(1+L)^{\\lambda}\\,},\n$$\n其中常数 $C_a > 0$，$\\gamma \\in (0,1]$ 和 $\\lambda \\in (0,1]$ 体现了宽度和深度的收益递减效应。\n- 估计误差项：\n$$\n\\mathcal{E}(m,L) \\;=\\; C_e \\,\\sqrt{\\frac{P_{\\text{total}}(m,L;d)\\,\\log(1+m)\\,L}{n}},\n$$\n其中常数 $C_e > 0$，反映了容量随参数数量和深度的增长。\n\n组合的理论上界为\n$$\n\\mathcal{G}(m,L) \\;=\\; \\mathcal{A}(m,L) \\;+\\; \\mathcal{E}(m,L).\n$$\n\n在约束条件 $P_{\\text{total}}(m,L;d) \\le P$ 以及 $m,L \\in \\mathbb{N}$、$m \\ge 1$、$L \\ge 1$ 下，找出使 $\\mathcal{G}(m,L)$ 最小化的架构 $(m,L)$。如果有多个架构在 $10^{-9}$ 的容差范围内达到相同的最小值，则通过优先选择最小的 $P_{\\text{total}}(m,L;d)$，然后是最小的 $L$，最后是最小的 $m$ 来打破平局。\n\n为确保科学真实性和计算可行性，在 $1 \\le L \\le L_{\\text{cap}}$ 的范围内搜索 $L$，其中\n$$\nL_{\\text{cap}} \\;=\\; \\min\\!\\Big(L_{\\max}, \\,\\big\\lfloor \\sqrt{P} \\big\\rfloor\\Big), \\quad L_{\\max} \\;=\\; \\left\\lfloor \\frac{P - d - 1}{2} \\right\\rfloor,\n$$\n这保证了在 $m=1$ 时的可行性，并通过一个由预算驱动的启发式方法来限制深度。对于搜索范围内的每个 $L$，从 $1$ 到满足 $P_{\\text{total}}(m,L;d) \\le P$ 的最大整数 $m_{\\max}(L)$ 中选择 $m$。对于 $L=1$，这简化为一个线性界限 $m \\le \\left\\lfloor \\dfrac{P - 1}{d + 2} \\right\\rfloor$。对于 $L>1$，求解二次不等式 $(L-1)m^2 + (d + L + 1)m + 1 - P \\le 0$ 以获得较大的根。\n\n您的程序必须实现此搜索，并为下面的每个测试用例生成最优的 $(m,L)$。\n\n测试套件（每个测试用例是一个元组 $(d,n,P,C_a,C_e,\\gamma,\\lambda)$）：\n- 案例 1：$(10, 5000, 5000, 1.0, 0.5, 0.5, 0.3)$。\n- 案例 2：$(20, 1500, 600, 1.0, 0.6, 0.5, 0.3)$。\n- 案例 3：$(5, 20000, 25000, 0.8, 0.4, 0.5, 0.3)$。\n- 案例 4：$(5, 800, 25000, 0.8, 0.4, 0.5, 0.3)$。\n- 案例 5：$(30, 5000, 1200, 1.2, 0.5, 0.5, 0.3)$。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果，每个结果是所选的架构，表示为一个双元素列表 $[m,L]$，并按给定顺序排列。例如，输出应类似于 $[[m_1,L_1],[m_2,L_2],[m_3,L_3],[m_4,L_4],[m_5,L_5]]$，不含任何额外文本。", "solution": "所呈现的问题是深度学习架构理论分析中一个有效且良构的练习。它要求在总参数数量的约束下，找到由其宽度 $m$ 和深度 $L$ 定义的最优神经网络架构，以最小化一个理论上的泛化误差上界。该问题在科学上植根于统计学习理论的原理，特别是近似误差和估计误差之间的权衡，并在数学上被形式化为一个离散优化问题。\n\n目标是为全连接前馈神经网络找到能最小化泛化误差上界 $\\mathcal{G}(m,L)$ 的架构 $(m, L)$。该网络具有输入维度 $d$、 $L$ 个宽度均为 $m$ 的隐藏层，以及一个标量输出。搜索受到总参数预算 $P$ 的约束。变量 $m$ 和 $L$ 均为正整数。\n\n此类网络的总参数数量 $P_{\\text{total}}(m,L;d)$ 由每层权重和偏置的总和给出：\n$$\nP_{\\text{total}}(m,L;d) = (L-1)m^2 + (d+L+1)m + 1\n$$\n此公式正确地计算了第一层（输入到隐藏层）、$L-1$ 个隐藏层之间以及最终输出层的参数。\n\n泛化误差上界 $\\mathcal{G}(m,L)$ 被建模为两个相互竞争的项之和：近似误差 $\\mathcal{A}(m,L)$ 和估计误差 $\\mathcal{E}(m,L)$。\n$$\n\\mathcal{G}(m,L) = \\mathcal{A}(m,L) + \\mathcal{E}(m,L)\n$$\n\n近似误差，\n$$\n\\mathcal{A}(m,L) = \\frac{C_a}{m^{\\gamma}(1+L)^{\\lambda}}\n$$\n表示网络函数类别逼近真实 underlying 函数的内在局限性。该项随着宽度 $m$ 和深度 $L$ 的增加而减小，反映了更大网络增强的表达能力。参数 $\\gamma \\in (0,1]$ 和 $\\lambda \\in (0,1]$ 分别模拟了增加宽度和深度的收益递减效应。\n\n估计误差，\n$$\n\\mathcal{E}(m,L) = C_e \\sqrt{\\frac{P_{\\text{total}}(m,L;d)\\log(1+m)L}{n}}\n$$\n表示从大小为 $n$ 的有限数据集中学习网络参数所产生的误差。该项随着模型复杂度的增加而增加，这里的模型复杂度由参数数量 $P_{\\text{total}}$、深度 $L$ 和与宽度相关的因子 $\\log(1+m)$ 来体现。这种形式的动机来自于像 Rademacher 复杂度这样的容量度量，它们通常与参数数量和其他架构属性成比例。\n\n因此，优化问题是：\n$$\n\\min_{m,L \\in \\mathbb{N}, m \\ge 1, L \\ge 1} \\mathcal{G}(m,L) \\quad \\text{subject to} \\quad P_{\\text{total}}(m,L;d) \\le P\n$$\n\n为了解决这个问题，我们必须在有效架构的离散有限空间上进行搜索。问题提供了一种结构化的方法来定义这个搜索空间。\n\n首先，深度 $L$ 的搜索范围被限制在 $1 \\le L \\le L_{\\text{cap}}$。上界 $L_{\\text{cap}}$ 定义为：\n$$\nL_{\\text{cap}} = \\min\\left( L_{\\max}, \\lfloor \\sqrt{P} \\rfloor \\right), \\quad \\text{where} \\quad L_{\\max} = \\left\\lfloor \\frac{P - d - 1}{2} \\right\\rfloor\n$$\n$L_{\\max}$ 项是从最简单宽度 $m=1$ 的参数约束推导出来的。对于 $m=1$，$P_{\\text{total}}(1,L;d) = 2L+d+1$。要求 $2L+d+1 \\le P$ 得出 $L \\le (P-d-1)/2$。这确保了对于搜索范围中任何高达 $L_{\\max}$ 的 $L$，至少存在一个有效的架构（即 $m=1$ 的架构）。$\\lfloor \\sqrt{P} \\rfloor$ 的额外上限是一种保持计算可行的启发式方法。\n\n其次，对于其有效范围内的每个选定深度 $L$，宽度 $m$ 的搜索范围被限制在 $1 \\le m \\le m_{\\max}(L)$。值 $m_{\\max}(L)$ 是满足参数预算不等式 $P_{\\text{total}}(m,L;d) \\le P$ 的最大整数 $m$。\n- 对于 $L=1$，该不等式对 $m$ 是线性的：$(d+2)m+1 \\le P$，这给出了 $m_{\\max}(1) = \\lfloor (P-1)/(d+2) \\rfloor$。\n- 对于 $L>1$，该不等式对 $m$ 是二次的：$(L-1)m^2 + (d+L+1)m + (1-P) \\le 0$。由于 $m^2$ 的系数是正的，$m$ 的有效范围位于相应二次方程的两个根之间。由于 $m$ 必须为正，我们只关心提供上界限的正根。设 $a=L-1$，$b=d+L+1$ 和 $c=1-P$。较大的根是 $\\frac{-b + \\sqrt{b^2-4ac}}{2a}$。因此，$m_{\\max}(L) = \\left\\lfloor \\frac{-(d+L+1) + \\sqrt{(d+L+1)^2 - 4(L-1)(1-P)}}{2(L-1)} \\right\\rfloor$。\n\n求解算法是在此定义的空间上进行系统性搜索：\n1. 初始化一个结构来存储迄今为止找到的最佳架构 $(m^*, L^*)$，以及其泛化误差 $\\mathcal{G}^*$ 和参数数量 $P_{\\text{total}}^*$。将 $\\mathcal{G}^*$ 初始化为无穷大。\n2. 从 1 到 $L_{\\text{cap}}$ 遍历每个深度 $L$。\n3. 对于每个 $L$，计算最大有效宽度 $m_{\\max}(L)$。如果 $m_{\\max}(L)  1$，则此深度不存在有效架构，继续下一个 $L$。\n4. 从 1 到 $m_{\\max}(L)$ 遍历每个宽度 $m$。\n5. 对于每对 $(m, L)$，计算 $P_{\\text{total}}(m,L;d)$ 和总误差 $\\mathcal{G}(m,L)$。\n6. 使用指定的平局打破规则，将计算出的 $\\mathcal{G}(m,L)$ 与迄今为止的最佳值 $\\mathcal{G}^*$ 进行比较。如果当前架构 $(m,L)$ 根据元组 $(\\mathcal{G}, P_{\\text{total}}, L, m)$ 的字典序比较更优，则更新最佳架构。具体来说，如果满足以下条件之一，则找到了新的最佳架构：\n   - $\\mathcal{G}(m,L)  \\mathcal{G}^* - 10^{-9}$，或\n   - $|\\mathcal{G}(m,L) - \\mathcal{G}^*| \\le 10^{-9}$ 且 $P_{\\text{total}}(m,L;d)  P_{\\text{total}}^*$，或\n   - $|\\mathcal{G}(m,L) - \\mathcal{G}^*| \\le 10^{-9}$，$P_{\\text{total}}(m,L;d) = P_{\\text{total}}^*$，且 $L  L^*$，或\n   - $|\\mathcal{G}(m,L) - \\mathcal{G}^*| \\le 10^{-9}$，$P_{\\text{total}}(m,L;d) = P_{\\text{total}}^*$，$L = L^*$，且 $m  m^*$。\n\n这种穷举搜索保证在受限的搜索空间内找到 $\\mathcal{G}(m,L)$ 的全局最小值，而详细的平局打破程序确保能确定唯一的解。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Finds the optimal neural network architecture (m, L) by minimizing a theoretical \n    generalization error bound under a parameter budget constraint.\n    \"\"\"\n    test_cases = [\n        # (d, n, P, C_a, C_e, gamma, lambda)\n        (10, 5000, 5000, 1.0, 0.5, 0.5, 0.3),\n        (20, 1500, 600, 1.0, 0.6, 0.5, 0.3),\n        (5, 20000, 25000, 0.8, 0.4, 0.5, 0.3),\n        (5, 800, 25000, 0.8, 0.4, 0.5, 0.3),\n        (30, 5000, 1200, 1.2, 0.5, 0.5, 0.3)\n    ]\n\n    all_results = []\n\n    for d, n, P, C_a, C_e, gamma, lmbda in test_cases:\n        \n        best_g = float('inf')\n        best_p_total = float('inf')\n        best_m = -1\n        best_l = -1\n\n        # Determine the search range for depth L\n        # L_max ensures that at least m=1 is a feasible architecture\n        if P - d - 1  0: # Ensure argument to floor is non-negative\n             l_max = 0\n        else:\n             l_max = int(np.floor((P - d - 1) / 2))\n        \n        l_cap = min(l_max, int(np.floor(np.sqrt(P))))\n\n        # Iterate through all valid depths L\n        for L in range(1, l_cap + 1):\n            m_max = 0\n            if L == 1:\n                # Linear inequality for m\n                if d + 2 > 0:\n                    m_max = int(np.floor((P - 1) / (d + 2)))\n            else: # L > 1\n                # Quadratic inequality for m: a*m^2 + b*m + c = 0\n                a = float(L - 1)\n                b = float(d + L + 1)\n                c = float(1 - P)\n                \n                discriminant = b**2 - 4*a*c\n                if discriminant >= 0:\n                    # Positive root gives the upper bound for m\n                    m_root = (-b + np.sqrt(discriminant)) / (2*a)\n                    if m_root >= 1:\n                        m_max = int(np.floor(m_root))\n\n            if m_max  1:\n                continue\n\n            # Iterate through all valid widths m for the given L\n            for m in range(1, m_max + 1):\n                # Calculate total parameters\n                p_total = (L - 1) * m**2 + (d + L + 1) * m + 1\n                \n                # The search for m is bounded by m_max, which is calculated to satisfy\n                # p_total = P. This explicit check is a safeguard.\n                if p_total > P:\n                    continue\n\n                # Calculate Approximation Error\n                approx_error = C_a / (np.power(m, gamma) * np.power(1 + L, lmbda))\n                \n                # Calculate Estimation Error\n                est_error_radical = (p_total * np.log(1 + m) * L) / n\n                est_error = C_e * np.sqrt(est_error_radical)\n                \n                # Total generalization error bound\n                g_current = approx_error + est_error\n\n                # Apply update logic with tie-breaking\n                # Tolerance for floating point comparison of G\n                g_diff = g_current - best_g\n                is_better = False\n                \n                if g_diff  -1e-9:\n                    is_better = True\n                elif abs(g_diff) = 1e-9:\n                    # Tie in G, check P_total\n                    if p_total  best_p_total:\n                        is_better = True\n                    elif p_total == best_p_total:\n                        # Tie in P_total, check L\n                        if L  best_l:\n                            is_better = True\n                        elif L == best_l:\n                            # Tie in L, check m\n                            if m  best_m:\n                                is_better = True\n                \n                if is_better:\n                    best_g = g_current\n                    best_p_total = p_total\n                    best_m = m\n                    best_l = L\n\n        all_results.append([best_m, best_l])\n    \n    # Format the final output string to match the required format \"[[m1,L1],[m2,L2],...]\"\n    # by removing spaces from the default string representation of a list of lists.\n    output_str = str(all_results).replace(\" \", \"\")\n    print(output_str)\n\nsolve()\n```", "id": "3113786"}, {"introduction": "从宏观的架构设计转向层内的微观交互，我们来探究两个常用工具——批归一化（Batch Normalization）和权重衰减（Weight Decay）——之间微妙的相互作用。这个练习 ([@problem_id:3113809]) 通过一个简化的模型，让你能够分离并分析对批归一化缩放参数 $\\gamma$ 施加权重衰减如何影响模型的有效学习和分类边界（margin），从而揭示现代网络训练中更深层次的动态。", "problem": "考虑一个简化的深度神经网络架构，它由一个仿射通道、一个带有可学习缩放参数的批量归一化 (BN) 层以及一个二元分类器组成。数据集由标量输入和二元标签组成。设原始输入为 $x_i \\in \\mathbb{R}$，标签为 $y_i \\in \\{-1,+1\\}$，其中 $i \\in \\{1,\\dots,n\\}$。批量归一化 (BN) 变换使用批量均值、方差和一个数值稳定器 $\\varepsilon  0$ 来计算归一化特征，如下所示\n$$\n\\hat{x}_i = \\frac{x_i - \\mu}{\\sqrt{\\sigma^2 + \\varepsilon}},\n$$\n其中\n$$\n\\mu = \\frac{1}{n}\\sum_{i=1}^n x_i,\\quad \\sigma^2 = \\frac{1}{n}\\sum_{i=1}^n (x_i - \\mu)^2.\n$$\n我们将 BN 的平移参数设置为零，仅保留 BN 的缩放参数 $\\gamma \\in \\mathbb{R}$。样本 $i$ 的分类器得分为\n$$\ns_i(\\gamma) = \\gamma \\hat{x}_i.\n$$\n我们采用经验风险最小化方法，其中逻辑损失和权重衰减（也称为 $\\ell_2$ 正则化）直接应用于 BN 缩放参数。训练目标为\n$$\nL(\\gamma) = \\frac{1}{n}\\sum_{i=1}^n \\log\\!\\left(1 + \\exp\\!\\left(-y_i\\, s_i(\\gamma)\\right)\\right) + \\frac{\\lambda}{2}\\,\\gamma^2,\n$$\n其中 $\\lambda \\ge 0$ 是权重衰减系数。我们通过函数间隔来量化分类置信度\n$$\nm(\\gamma) = \\min_{i \\in \\{1,\\dots,n\\}} \\left( y_i\\, s_i(\\gamma) \\right) = \\min_{i \\in \\{1,\\dots,n\\}} \\left( y_i\\, \\gamma\\, \\hat{x}_i \\right).\n$$\n\n基本原理。使用经验风险最小化与逻辑损失，完全按照上述定义来定义批量归一化 (BN)，并按所述将权重衰减应用于 $\\gamma$。不要引入任何替代的损失函数、架构或正则化器。任务是基于这些原理推导出计算 $L(\\gamma)$ 的唯一最小值点 $\\gamma^\\star$ 的算法，然后计算 $m(\\gamma^\\star)$。\n\n您的程序必须：\n- 完全按照规定实现 BN 变换，对每个测试用例使用给定的 $\\varepsilon$。\n- 基于目标函数的凸性，使用有原则的数值方法在 $\\gamma \\in \\mathbb{R}$ 上优化 $L(\\gamma)$ 以获得 $\\gamma^\\star$。\n- 计算间隔 $m(\\gamma^\\star)$ 并将其报告为实数。\n- 对于数值报告，将每个间隔四舍五入到 6 位小数。\n- 生成一行输出，其中包含用逗号分隔并用方括号括起来的结果列表，例如 $[\\,0.123456,0.234567,0.345678,0.456789\\,]$。\n\n测试套件。使用以下四个测试用例来研究权重衰减与 BN 缩放参数之间的相互作用及其对间隔的影响。每个用例提供 $(x,y,\\varepsilon,\\lambda)$：\n- 案例 1 (通用类可分配置，中等权重衰减)：\n  - $x = [\\, -2.0,\\,-0.5,\\,0.0,\\,0.5,\\,2.0 \\,]$\n  - $y = [\\, -1,\\,-1,\\,+1,\\,+1,\\,+1 \\,]$\n  - $\\varepsilon = 10^{-5}$\n  - $\\lambda = 0.2$\n- 案例 2 (不可分配置，对 $\\gamma$ 无显式权重衰减的边界情况)：\n  - $x = [\\, -1.0,\\,-0.8,\\,-0.2,\\,0.1,\\,0.2,\\,0.9 \\,]$\n  - $y = [\\, -1,\\,+1,\\,-1,\\,+1,\\,-1,\\,+1 \\,]$\n  - $\\varepsilon = 10^{-5}$\n  - $\\lambda = 0.0$\n- 案例 3 (近退化方差，测试通过 $\\varepsilon$ 实现的 BN 稳定性)：\n  - $x = [\\, 1.0,\\,1.01,\\,0.99,\\,1.02 \\,]$\n  - $y = [\\, -1,\\,+1,\\,-1,\\,+1 \\,]$\n  - $\\varepsilon = 10^{-2}$\n  - $\\lambda = 0.1$\n- 案例 4 (强权重衰减抑制缩放)：\n  - $x = [\\, -3.0,\\,-1.0,\\,1.0,\\,3.0 \\,]$\n  - $y = [\\, -1,\\,-1,\\,+1,\\,+1 \\,]$\n  - $\\varepsilon = 10^{-5}$\n  - $\\lambda = 5.0$\n\n答案规范。您的程序应生成单行输出，其中包含四个案例的间隔，按所列顺序排列，四舍五入到 6 位小数，并格式化为用逗号分隔并用方括号括起来的列表，例如 $[\\,m_1,m_2,m_3,m_4\\,]$，其中每个 $m_k$ 都是一个实数。不允许有其他输出。不涉及角度，也没有出现物理单位，因此不需要进行单位转换。", "solution": "该问题要求在一个简单分类器优化其唯一的学习参数——批量归一化 (BN) 缩放参数 $\\gamma$ 后，确定其函数间隔。该解决方案基于凸优化的原理。\n\n首先，我们对问题进行形式化。输入数据由数据对 $(x_i, y_i)$ 组成，其中 $i=1, \\dots, n$，$x_i \\in \\mathbb{R}$ 是标量特征，$y_i \\in \\{-1, +1\\}$ 是二元标签。这些特征经过批量归一化处理。BN 的统计数据，即批量均值 $\\mu$ 和批量方差 $\\sigma^2$，计算如下：\n$$\n\\mu = \\frac{1}{n}\\sum_{i=1}^n x_i\n$$\n$$\n\\sigma^2 = \\frac{1}{n}\\sum_{i=1}^n (x_i - \\mu)^2\n$$\n然后使用数值稳定器 $\\varepsilon  0$ 计算归一化特征 $\\hat{x}_i$：\n$$\n\\hat{x}_i = \\frac{x_i - \\mu}{\\sqrt{\\sigma^2 + \\varepsilon}}\n$$\n样本 $i$ 的分类器得分为 $s_i(\\gamma) = \\gamma \\hat{x}_i$。目标是找到最优缩放参数 $\\gamma^\\star$，以最小化正则化的逻辑损失：\n$$\nL(\\gamma) = \\frac{1}{n}\\sum_{i=1}^n \\log\\!\\left(1 + \\exp\\!\\left(-y_i\\, \\gamma\\, \\hat{x}_i\\right)\\right) + \\frac{\\lambda}{2}\\,\\gamma^2\n$$\n其中 $\\lambda \\ge 0$ 是权重衰减系数。\n\n任务的核心是解决一维优化问题 $\\gamma^\\star = \\arg\\min_{\\gamma \\in \\mathbb{R}} L(\\gamma)$。为了建立一个有原则的求解方法，我们分析目标函数 $L(\\gamma)$ 的凸性。\n函数 $f(u) = \\log(1+e^u)$ 是凸函数。指数的参数 $u_i(\\gamma) = -y_i \\gamma \\hat{x}_i$ 是 $\\gamma$ 的线性函数。由于凸函数与仿射映射的复合是凸的，因此每一项 $\\log\\!\\left(1 + \\exp\\!\\left(-y_i\\, \\gamma\\, \\hat{x}_i\\right)\\right)$ 都是关于 $\\gamma$ 的凸函数。凸函数的和也是凸的，所以 $L(\\gamma)$ 的第一项，即平均逻辑损失，是凸的。\n第二项 $\\frac{\\lambda}{2}\\gamma^2$ 是 $\\ell_2$ 正则化项。对于 $\\lambda \\ge 0$，这是一个凸函数。对于 $\\lambda  0$，它是严格凸的。\n\n一个凸函数与一个严格凸函数之和是严格凸的。因此，对于任何 $\\lambda  0$，$L(\\gamma)$ 是严格凸的。一个严格凸函数有唯一的全局最小值点。对于 $\\lambda = 0$ 的情况，$L(\\gamma)$ 是凸的。只要数据在特征空间中不能被一个过原点的超平面完美分离（即，并非所有的 $y_i \\hat{x}_i$ 都具有相同的符号），唯一的最小值点仍然存在，这对于给定的测试用例是成立的。\n\n唯一的最小值点 $\\gamma^\\star$ 可以通过求解目标函数一阶导数的根 $\\frac{dL}{d\\gamma} = 0$ 来找到。该导数为：\n$$\n\\frac{dL}{d\\gamma} = -\\frac{1}{n}\\sum_{i=1}^n y_i \\hat{x}_i S(-y_i \\gamma \\hat{x}_i) + \\lambda\\gamma\n$$\n其中 $S(z) = \\frac{1}{1+e^{-z}}$ 是 sigmoid 函数。这是一个关于 $\\gamma$ 的非线性方程，需要使用数值求根或优化算法。由于 $L(\\gamma)$ 是一个一维凸函数，标准的数值方法如牛顿法或 Brent 方法保证能收敛到唯一的全局最小值。我们将采用一个数值优化程序来找到 $\\gamma^\\star$。\n\n一旦确定了 $\\gamma^\\star$，我们就计算函数间隔，其定义为所有样本中置信度缩放得分的最小值：\n$$\nm(\\gamma^\\star) = \\min_{i \\in \\{1,\\dots,n\\}} \\left( y_i\\, s_i(\\gamma^\\star) \\right) = \\min_{i \\in \\{1,\\dots,n\\}} \\left( y_i\\, \\gamma^\\star \\hat{x}_i \\right)\n$$\n这个值量化了分类器在其“最坏情况”样本上的性能。\n\n每个测试用例的总体算法如下：\n$1$. 给定输入数据 $(x, y)$ 和参数 $(\\varepsilon, \\lambda)$，计算批量统计数据 $\\mu$ 和 $\\sigma^2$。\n$2$. 计算所有样本的归一化特征 $\\hat{x}$。\n$3$. 按规定定义目标函数 $L(\\gamma)$。为确保逻辑损失项 $\\log(1+\\exp(z))$ 的数值计算稳健，使用恒等式 $\\log(1+\\exp(z)) = \\log(1+\\exp(-\\vert z\\vert)) + \\max(z,0)$ 来防止浮点溢出。\n$4$. 使用一个数值标量优化算法，例如 Brent 方法，来找到 $L(\\gamma)$ 的唯一最小值点 $\\gamma^\\star$。\n$5$. 计算函数间隔 $m(\\gamma^\\star) = \\min(y \\odot (\\gamma^\\star \\hat{x}))$，其中 $\\odot$ 表示逐元素乘积。\n$6$. 报告所得的间隔，四舍五入到 6 位小数。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import minimize_scalar\n\ndef solve():\n    \"\"\"\n    Solves for the optimal BN scale parameter `gamma` and computes the\n    functional margin for a series of test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1 (general separable-like configuration, moderate weight decay)\n        {\n            \"x\": np.array([-2.0, -0.5, 0.0, 0.5, 2.0]),\n            \"y\": np.array([-1, -1, 1, 1, 1]),\n            \"eps\": 1e-5,\n            \"lam\": 0.2\n        },\n        # Case 2 (non-separable configuration, no explicit weight decay)\n        {\n            \"x\": np.array([-1.0, -0.8, -0.2, 0.1, 0.2, 0.9]),\n            \"y\": np.array([-1, 1, -1, 1, -1, 1]),\n            \"eps\": 1e-5,\n            \"lam\": 0.0\n        },\n        # Case 3 (near-degenerate variance, tests BN stabilization via eps)\n        {\n            \"x\": np.array([1.0, 1.01, 0.99, 1.02]),\n            \"y\": np.array([-1, 1, -1, 1]),\n            \"eps\": 1e-2,\n            \"lam\": 0.1\n        },\n        # Case 4 (strong weight decay suppresses scale)\n        {\n            \"x\": np.array([-3.0, -1.0, 1.0, 3.0]),\n            \"y\": np.array([-1, -1, 1, 1]),\n            \"eps\": 1e-5,\n            \"lam\": 5.0\n        }\n    ]\n\n    results = []\n\n    for case in test_cases:\n        x, y, eps, lam = case[\"x\"], case[\"y\"], case[\"eps\"], case[\"lam\"]\n        n = float(len(x))\n\n        # Step 1  2: Batch Normalization\n        mu = np.mean(x)\n        # Use ddof=0 for population variance, consistent with the problem statement\n        sigma_sq = np.var(x, ddof=0)\n        x_hat = (x - mu) / np.sqrt(sigma_sq + eps)\n\n        # Step 3: Define objective function L(gamma)\n        def objective_function(gamma, x_hat, y, n, lam):\n            \"\"\"\n            Computes the regularized logistic loss.\n            Uses a numerically stable implementation for log(1 + exp(z)).\n            \"\"\"\n            # Argument of the exponential in the logistic loss\n            z = -y * gamma * x_hat\n            \n            # Numerically stable computation of log(1 + exp(z)) for each sample\n            log_loss_terms = np.log(1 + np.exp(-np.abs(z))) + np.maximum(z, 0)\n            \n            # Average logistic loss\n            avg_log_loss = np.sum(log_loss_terms) / n\n            \n            # L2 regularization term\n            regularization = (lam / 2.0) * (gamma**2)\n            \n            return avg_log_loss + regularization\n\n        # Step 4: Find the optimal gamma\n        # minimize_scalar finds the minimum of a single-variable function.\n        # For a convex function, this finds the unique global minimum.\n        opt_result = minimize_scalar(\n            objective_function,\n            args=(x_hat, y, n, lam)\n        )\n        gamma_star = opt_result.x\n\n        # Step 5: Compute the functional margin\n        margin = np.min(y * gamma_star * x_hat)\n        \n        results.append(margin)\n\n    # Final print statement in the exact required format.\n    formatted_results = [f\"{r:.6f}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3113809"}, {"introduction": "一个模型的可靠性不仅在于其准确性，还在于其校准性——即预测的置信度是否反映了真实的正确率。标签平滑（Label smoothing）是一种简单而强大的技术，用于防止模型对其预测变得过于自信。这个实践 ([@problem_id:3113811]) 要求你推导并计算该技术对模型内部的logit边界和外部的校准误差的直接影响，从而清晰地展示其工作原理及权衡。", "problem": "考虑一个 $K$ 类分类器，其具有单个 logit 向量 $\\mathbf{z} \\in \\mathbb{R}^K$ 和 Softmax 函数，对每个类别索引 $i \\in \\{1,\\dots,K\\}$，定义为 $p_i(\\mathbf{z}) = \\exp(z_i) \\big/ \\sum_{j=1}^K \\exp(z_j)$。训练目标是交叉熵损失 $L(\\mathbf{z};\\mathbf{y}) = -\\sum_{i=1}^K y_i \\log p_i(\\mathbf{z})$，其中 $\\mathbf{y}$ 是一个目标概率向量。使用参数 $\\epsilon \\in [0,1)$ 的标签平滑将真实类别索引 $t$ 的 one-hot 目标替换为 $\\tilde{y}_t = 1-\\epsilon$ 和对所有 $j \\neq t$ 的 $\\tilde{y}_j = \\epsilon/(K-1)$，这会产生一个总和为 $1$ 的有效概率分布。\n\n根据 Softmax 和交叉熵的基本定义，并仅使用关于这些函数的普遍接受的事实（包括交叉熵分解为 $H(\\mathbf{y}) + \\mathrm{KL}(\\mathbf{y}\\,\\|\\,\\mathbf{p}(\\mathbf{z}))$ 的恒等式，其中 $H$ 表示熵，$\\mathrm{KL}$ 是 Kullback–Leibler 散度），推导以下内容：\n\n1. 在标签平滑下，最小化的 Softmax 输出等于平滑后的目标，即 $\\mathbf{p}(\\mathbf{z}^\\star) = \\tilde{\\mathbf{y}}$。利用这一点，通过取 $z^\\star_i = \\log \\tilde{y}_i + c$（对于某个常数 $c \\in \\mathbb{R}$）来获得达到最小值的 logits $\\mathbf{z}^\\star$ 的一个规范表示，然后将真实类别的 logit 差额定义为 $m^\\star(\\epsilon,K) = z^\\star_t - \\max_{j \\neq t} z^\\star_j$。使用自然对数 $\\log(\\cdot)$ 将 $m^\\star(\\epsilon,K)$ 表示为 $\\epsilon$ 和 $K$ 的闭式函数，并确定其在 $\\epsilon = 0$ 时的极限值。\n\n2. 为了在数学上受控的设定中分析校准，考虑一个无噪声评估场景，在该场景中，模型的 top-1 预测始终是真实类别，且以任何置信度分数为条件的准确率均为 $1$。将单箱期望校准误差 (ECE) 定义为真实类别的平均预测置信度与经验准确率之间的绝对差。在相同的最小化解 $\\mathbf{p}(\\mathbf{z}^\\star) = \\tilde{\\mathbf{y}}$ 下，推导出 ECE 作为 $\\epsilon$ 和 $K$ 的函数的闭式表达式，并尽可能地简化它。\n\n您的任务是实现一个完整的程序，根据您的推导，为每个测试用例 $(K,\\epsilon)$ 计算值对 $[m^\\star(\\epsilon,K), \\mathrm{ECE}(\\epsilon,K)]$。使用自然对数。当 $m^\\star(\\epsilon,K)$ 发散时，将其表示为 $+\\infty$。\n\n用于覆盖典型、边缘和边界情况的参数值测试套件：\n- 情况 1：$K=10$，$\\epsilon=0.1$。\n- 情况 2：$K=5$，$\\epsilon=0.2$。\n- 情况 3：$K=2$，$\\epsilon=0$。\n- 情况 4：$K=3$，$\\epsilon=0.001$。\n- 情况 5：$K=100$，$\\epsilon=0.9$。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含结果，形式为对的列表，每个测试用例一对，顺序与上述相同。每个数值必须四舍五入到六位小数。如果一个值为 $+\\infty$，则将其打印为文字字符串 inf，不带任何附加字符。确切要求的格式是：\n  - $[[m_1,e_1],[m_2,e_2],[m_3,e_3],[m_4,e_4],[m_5,e_5]]$\n  其中每个 $m_i$ 和 $e_i$ 分别是第 $i$ 个测试用例的差额和校准值，例如 $[4.394449,0.100000]$。", "solution": "用户提供的问题被评估为有效，因为它在科学上基于统计学习的原理，是适定的、客观的且内部一致的。我们继续进行推导和求解。\n\n该问题要求推导与使用标签平滑的 $K$ 类分类器相关的两个量：最优 logit 差额 $m^\\star(\\epsilon,K)$ 和期望校准误差 $\\mathrm{ECE}(\\epsilon,K)$。\n\n目标概率向量 $\\mathbf{y}$ 和预测概率向量 $\\mathbf{p}(\\mathbf{z})$ 之间的交叉熵损失由 $L(\\mathbf{z};\\mathbf{y}) = -\\sum_{i=1}^K y_i \\log p_i(\\mathbf{z})$ 给出。信息论中的一个标准结果是，该损失可以分解为目标分布的熵和目标分布与预测分布之间的 Kullback-Leibler (KL) 散度：\n$$\nL(\\mathbf{z};\\mathbf{y}) = H(\\mathbf{y}) + \\mathrm{KL}(\\mathbf{y}\\,\\|\\,\\mathbf{p}(\\mathbf{z}))\n$$\n其中 $H(\\mathbf{y}) = -\\sum_i y_i \\log y_i$ 且 $\\mathrm{KL}(\\mathbf{y}\\,\\|\\,\\mathbf{p}(\\mathbf{z})) = \\sum_i y_i \\log(y_i/p_i(\\mathbf{z}))$。\n\n在我们的例子中，目标向量是经过标签平滑的向量 $\\tilde{\\mathbf{y}}$，其中对于一个真实类别索引 $t$，其分量为：\n$$\n\\tilde{y}_t = 1-\\epsilon\n$$\n$$\n\\tilde{y}_j = \\frac{\\epsilon}{K-1} \\quad \\text{for } j \\neq t\n$$\n要最小化的损失是 $L(\\mathbf{z};\\tilde{\\mathbf{y}})$。熵项 $H(\\tilde{\\mathbf{y}})$ 不依赖于模型的 logits $\\mathbf{z}$，因此最小化损失 $L$ 等价于最小化 KL 散度项 $\\mathrm{KL}(\\tilde{\\mathbf{y}}\\,\\|\\,\\mathbf{p}(\\mathbf{z}))$。KL 散度 $\\mathrm{KL}(Q\\|P)$ 是非负的，且当且仅当分布相同时（即 $P=Q$）才最小化为 $0$。因此，当模型的 Softmax 输出 $\\mathbf{p}(\\mathbf{z}^\\star)$ 等于平滑后的目标分布 $\\tilde{\\mathbf{y}}$ 时，损失被最小化。\n$$\n\\mathbf{p}(\\mathbf{z}^\\star) = \\tilde{\\mathbf{y}}\n$$\n\n**1. Logit 差额 $m^\\star(\\epsilon,K)$ 的推导**\n\n我们从最优条件 $\\mathbf{p}(\\mathbf{z}^\\star) = \\tilde{\\mathbf{y}}$ 开始。Softmax 函数定义为 $p_i(\\mathbf{z}) = \\exp(z_i) / \\sum_{j=1}^K \\exp(z_j)$。在最优状态下，对于每个类别 $i$：\n$$\np_i(\\mathbf{z}^\\star) = \\frac{\\exp(z^\\star_i)}{\\sum_{j=1}^K \\exp(z^\\star_j)} = \\tilde{y}_i\n$$\n对两边取自然对数，得到：\n$$\n\\log(p_i(\\mathbf{z}^\\star)) = z^\\star_i - \\log\\left(\\sum_{j=1}^K \\exp(z^\\star_j)\\right) = \\log(\\tilde{y}_i)\n$$\n这意味着 $z^\\star_i = \\log(\\tilde{y}_i) + c$，其中 $c = \\log(\\sum_{j=1}^K \\exp(z^\\star_j))$ 是一个与类别索引 $i$ 无关的常数。这证实了最优 logits 的规范表示。\n\n真实类别 $t$ 的 logit 差额定义为 $m^\\star(\\epsilon,K) = z^\\star_t - \\max_{j \\neq t} z^\\star_j$。使用推导出的最优 logits 形式：\n$$\nz^\\star_t = \\log(\\tilde{y}_t) + c = \\log(1-\\epsilon) + c\n$$\n对于任何不正确的类别 $j \\neq t$，logits 均相等：\n$$\nz^\\star_j = \\log(\\tilde{y}_j) + c = \\log\\left(\\frac{\\epsilon}{K-1}\\right) + c\n$$\n因此，$\\max_{j \\neq t} z^\\star_j$ 就是 $\\log\\left(\\frac{\\epsilon}{K-1}\\right) + c$。\n\n差额 $m^\\star(\\epsilon,K)$ 是这些 logits 之间的差。常数 $c$ 被消掉了：\n$$\nm^\\star(\\epsilon,K) = \\left(\\log(1-\\epsilon) + c\\right) - \\left(\\log\\left(\\frac{\\epsilon}{K-1}\\right) + c\\right) = \\log(1-\\epsilon) - \\log\\left(\\frac{\\epsilon}{K-1}\\right)\n$$\n使用对数性质 $\\log a - \\log b = \\log(a/b)$，我们得到闭式表达式：\n$$\nm^\\star(\\epsilon,K) = \\log\\left(\\frac{1-\\epsilon}{\\epsilon/(K-1)}\\right) = \\log\\left(\\frac{(1-\\epsilon)(K-1)}{\\epsilon}\\right)\n$$\n对于 $\\epsilon = 0$ 的情况，我们考虑其极限 $\\epsilon \\to 0^+$。给定 $K \\ge 2$，项 $(1-\\epsilon)(K-1)$ 趋近于 $K-1  0$。分母 $\\epsilon$ 从正方向趋近于 $0$。因此，对数的参数趋近于 $+\\infty$。\n$$\n\\lim_{\\epsilon \\to 0^+} m^\\star(\\epsilon,K) = \\lim_{\\epsilon \\to 0^+} \\log\\left(\\frac{(1-\\epsilon)(K-1)}{\\epsilon}\\right) = +\\infty\n$$\n\n**2. 期望校准误差 $\\mathrm{ECE}(\\epsilon,K)$ 的推导**\n\n单箱期望校准误差定义为真实类别的平均预测置信度与经验准确率之间的绝对差。\n$$\n\\mathrm{ECE} = \\left| \\text{平均置信度} - \\text{准确率} \\right|\n$$\n问题指定了一个无噪声评估场景，其中：\n- 经验准确率为 $1$。\n- 模型的预测已在标签平滑下收敛到最优解，因此输出概率由 $\\mathbf{p}(\\mathbf{z}^\\star) = \\tilde{\\mathbf{y}}$ 给出。\n- 模型的 top-1 预测始终是真实类别。\n\n“真实类别的平均预测置信度”要求我们找到分配给每个样本真实类别的概率，并对这些值求平均。在这种受控设定下，对于任何真实类别为 $t$ 的样本，模型始终输出概率向量 $\\tilde{\\mathbf{y}}$。因此，分配给真实类别的概率始终是 $\\tilde{y}_t = 1-\\epsilon$。这个常数值的平均值就是 $1-\\epsilon$。\n$$\n\\text{平均置信度} = 1-\\epsilon\n$$\n经验准确率给定为 $1$。\n$$\n\\text{准确率} = 1\n$$\n将这些代入 ECE 公式：\n$$\n\\mathrm{ECE}(\\epsilon,K) = \\left| (1-\\epsilon) - 1 \\right| = \\left| -\\epsilon \\right|\n$$\n由于标签平滑参数 $\\epsilon$ 在范围 $[0,1)$ 内，它是非负的。因此，$|\\!-\\epsilon| = \\epsilon$。\n$$\n\\mathrm{ECE}(\\epsilon,K) = \\epsilon\n$$\n此结果与类别数 $K$ 无关。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the optimal logit margin and Expected Calibration Error (ECE)\n    for a K-class classifier with label smoothing parameter epsilon.\n    \"\"\"\n\n    # Test suite of parameter values (K, epsilon)\n    test_cases = [\n        (10, 0.1),\n        (5, 0.2),\n        (2, 0),\n        (3, 0.001),\n        (100, 0.9),\n    ]\n\n    results = []\n    for K, epsilon in test_cases:\n        # 1. Calculate the logit margin m_star\n        # m_star is given by log(((1 - epsilon) * (K - 1)) / epsilon)\n        if epsilon == 0:\n            # The limit as epsilon -> 0+ of the margin is +infinity.\n            m_star = float('inf')\n        else:\n            # This branch handles epsilon > 0. The problem states epsilon in [0, 1).\n            # The context implies K >= 2, so K-1 >= 1.\n            m_star = np.log((1 - epsilon) * (K - 1) / epsilon)\n\n        # 2. Calculate the Expected Calibration Error (ECE)\n        # ECE is given by epsilon in this idealized scenario.\n        ece = epsilon\n\n        # Format the results for the final output string.\n        # If m_star is infinity, print the literal string 'inf'.\n        # Otherwise, round the numerical value to 6 decimal places.\n        if m_star == float('inf'):\n            m_star_str = 'inf'\n        else:\n            m_star_str = f\"{m_star:.6f}\"\n        \n        ece_str = f\"{ece:.6f}\"\n        \n        results.append(f\"[{m_star_str},{ece_str}]\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3113811"}]}