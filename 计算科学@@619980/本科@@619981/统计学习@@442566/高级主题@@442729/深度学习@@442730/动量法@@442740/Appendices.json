{"hands_on_practices": [{"introduction": "要真正理解一个优化算法的工作原理，没有什么比亲手进行一次具体计算更能加深理解了。第一个实践练习将引导您在一个简单的二次代价函数上，逐步执行经典动量法的前几个步骤。通过手动计算更新过程，您将对速度矢量 $v_t$ 如何累积梯度信息并影响参数 $w_t$ 的优化路径建立起扎实的直观认识。[@problem_id:2187765]", "problem": "一个优化算法用于最小化成本函数 $f(x, y)$，该函数取决于两个参数 $x$ 和 $y$。成本函数由下式给出：\n$$f(x, y) = x^2 + 2y^2$$\n\n所选用的算法是经典动量法。在步骤 $t$，参数向量 $w_t = (x_t, y_t)$ 和速度向量 $v_t$ 的更新规则如下：\n1. $v_t = \\gamma v_{t-1} + \\eta \\nabla f(w_{t-1})$\n2. $w_t = w_{t-1} - v_t$\n\n其中 $\\nabla f(w_{t-1})$ 是成本函数在前一个参数向量 $w_{t-1}$ 处计算的梯度。\n\n优化从初始参数向量 $w_0 = (x_0, y_0) = (4, 2)$ 开始。初始速度向量为 $v_0 = (0, 0)$。算法的超参数设置为学习率 $\\eta = 0.1$ 和动量参数 $\\gamma = 0.9$。\n\n你的任务是计算经过两次完整更新步骤后的参数向量 $w_2 = (x_2, y_2)$。将你的答案表示为一个包含两个分量的行向量。", "solution": "成本函数为 $f(x,y)=x^{2}+2y^{2}$，所以其梯度为\n$$\n\\nabla f(x,y)=\\begin{pmatrix} \\frac{\\partial f}{\\partial x} \\\\[4pt] \\frac{\\partial f}{\\partial y} \\end{pmatrix}\n=\\begin{pmatrix} 2x \\\\[4pt] 4y \\end{pmatrix}.\n$$\n动量更新为 $v_{t}=\\gamma v_{t-1}+\\eta\\,\\nabla f(w_{t-1})$ 和 $w_{t}=w_{t-1}-v_{t}$，其中 $w_{0}=\\begin{pmatrix}4\\\\[2pt]2\\end{pmatrix}$，$v_{0}=\\begin{pmatrix}0\\\\[2pt]0\\end{pmatrix}$，$\\eta=\\frac{1}{10}$，以及 $\\gamma=\\frac{9}{10}$。\n\n第一步 $t=1$：\n$$\n\\nabla f(w_{0})=\\begin{pmatrix}2\\cdot 4\\\\[2pt]4\\cdot 2\\end{pmatrix}\n=\\begin{pmatrix}8\\\\[2pt]8\\end{pmatrix},\\qquad\nv_{1}=\\frac{9}{10}\\begin{pmatrix}0\\\\[2pt]0\\end{pmatrix}+\\frac{1}{10}\\begin{pmatrix}8\\\\[2pt]8\\end{pmatrix}\n=\\begin{pmatrix}\\frac{4}{5}\\\\[2pt]\\frac{4}{5}\\end{pmatrix},\n$$\n$$\nw_{1}=w_{0}-v_{1}=\\begin{pmatrix}4\\\\[2pt]2\\end{pmatrix}-\\begin{pmatrix}\\frac{4}{5}\\\\[2pt]\\frac{4}{5}\\end{pmatrix}\n=\\begin{pmatrix}\\frac{16}{5}\\\\[2pt]\\frac{6}{5}\\end{pmatrix}.\n$$\n\n第二步 $t=2$：\n$$\n\\nabla f(w_{1})=\\begin{pmatrix}2\\cdot \\frac{16}{5}\\\\[2pt]4\\cdot \\frac{6}{5}\\end{pmatrix}\n=\\begin{pmatrix}\\frac{32}{5}\\\\[2pt]\\frac{24}{5}\\end{pmatrix},\n$$\n$$\nv_{2}=\\frac{9}{10}\\begin{pmatrix}\\frac{4}{5}\\\\[2pt]\\frac{4}{5}\\end{pmatrix}+\\frac{1}{10}\\begin{pmatrix}\\frac{32}{5}\\\\[2pt]\\frac{24}{5}\\end{pmatrix}\n=\\begin{pmatrix}\\frac{36}{50}\\\\[2pt]\\frac{36}{50}\\end{pmatrix}+\\begin{pmatrix}\\frac{32}{50}\\\\[2pt]\\frac{24}{50}\\end{pmatrix}\n=\\begin{pmatrix}\\frac{68}{50}\\\\[2pt]\\frac{60}{50}\\end{pmatrix}\n=\\begin{pmatrix}\\frac{34}{25}\\\\[2pt]\\frac{6}{5}\\end{pmatrix},\n$$\n$$\nw_{2}=w_{1}-v_{2}=\\begin{pmatrix}\\frac{16}{5}\\\\[2pt]\\frac{6}{5}\\end{pmatrix}-\\begin{pmatrix}\\frac{34}{25}\\\\[2pt]\\frac{6}{5}\\end{pmatrix}\n=\\begin{pmatrix}\\frac{80}{25}-\\frac{34}{25}\\\\[2pt]0\\end{pmatrix}\n=\\begin{pmatrix}\\frac{46}{25}\\\\[2pt]0\\end{pmatrix}.\n$$\n因此，经过两次完整更新后，参数向量是行向量 $\\begin{pmatrix}\\frac{46}{25}  0\\end{pmatrix}$。", "answer": "$$\\boxed{\\begin{pmatrix} \\frac{46}{25}  0 \\end{pmatrix}}$$", "id": "2187765"}, {"introduction": "在经典动量法的基础上，Nesterov 加速梯度（NAG）方法引入了一个虽然微妙但功能强大的改变：它在一个“前瞻”位置计算梯度。本实践从计算转向概念分析，要求您检查一段伪代码并判断其代表哪种算法。这项技能对于在实际的机器学习应用中识别和正确实现优化器至关重要。[@problem_id:2187801]", "problem": "在机器学习领域，迭代优化算法通过更新参数向量 $w$ 来最小化损失函数 $L(w)$。两种流行的基于动量的方法是经典动量法（Classical Momentum）和 Nesterov 加速梯度（Nesterov Accelerated Gradient, NAG）。\n\n动量的核心思想是在持续下降的方向上加速运动，并抑制振荡。这是通过在更新规则中加入一个“速度”项 $v$ 来实现的。更新依赖于学习率 $\\eta$ 和动量系数 $\\gamma$。\n\n这两种方法之间的关键区别在于计算损失函数梯度的*位置*。\n- **经典动量法**：在当前参数位置 $w_t$ 计算梯度。\n- **Nesterov 加速梯度 (NAG)**：首先，它沿着先前速度的方向进行一次“前瞻”步骤。然后，它在这个前瞻位置计算梯度，以进行更具信息量的修正。\n\n考虑以下用于优化算法单个更新步骤的伪代码。函数 `compute_gradient_at(point)` 计算损失函数 $L$ 在给定 `point` 处的梯度。\n\n```\nfunction update_step(w, v, eta, gamma):\n    // w: current parameter vector\n    // v: current velocity vector\n    // eta: learning rate\n    // gamma: momentum coefficient\n\n    // Step 1: Calculate a temporary, projected position\n    w_projected = w - gamma * v\n\n    // Step 2: Compute the gradient at the projected position\n    grad = compute_gradient_at(w_projected)\n\n    // Step 3: Update the velocity vector\n    v_new = gamma * v + eta * grad\n\n    // Step 4: Update the parameter vector\n    w_new = w - v_new\n\n    // Return the updated parameter and velocity vectors\n    return w_new, v_new\n```\n\n该伪代码实现了哪种优化算法？\n\nA. 带动量的简单梯度下降法（经典动量法）\n\nB. Nesterov 加速梯度 (NAG)\n\nC. 简单梯度下降法\n\nD. Adagrad\n\nE. RMSprop", "solution": "给定一个更新方案，其参数为 $w$ 和速度 $v$，学习率为 $\\eta$，动量系数为 $\\gamma$。该伪代码执行以下步骤：\n1) 预测位置：\n$$w_{\\text{proj}}=w-\\gamma v.$$\n2) 在预测位置的梯度：\n$$g=\\nabla L(w_{\\text{proj}})=\\nabla L(w-\\gamma v).$$\n3) 速度更新：\n$$v_{\\text{new}}=\\gamma v+\\eta g.$$\n4) 参数更新：\n$$w_{\\text{new}}=w-v_{\\text{new}}=w-(\\gamma v+\\eta g).$$\n\n经典动量法使用\n$$v_{t+1}=\\gamma v_{t}+\\eta \\nabla L(w_{t}),\\qquad w_{t+1}=w_{t}-v_{t+1},$$\n也就是说，梯度是在当前点 $w_{t}$ 处计算的。\n\nNesterov 加速梯度首先计算一个前瞻点\n$$\\tilde{w}_{t}=w_{t}-\\gamma v_{t},$$\n然后在此处计算梯度，\n$$g_{t}=\\nabla L(\\tilde{w}_{t})=\\nabla L(w_{t}-\\gamma v_{t}),$$\n并更新\n$$v_{t+1}=\\gamma v_{t}+\\eta g_{t},\\qquad w_{t+1}=w_{t}-v_{t+1}.$$\n\n将这些公式与伪代码进行比较，我们发现它与 Nesterov 的过程完全一致：梯度是在前瞻位置 $w-\\gamma v$ 计算的，然后应用标准的动量式速度和参数更新。因此，该伪代码实现的算法是 Nesterov 加速梯度。\n\n它不是简单梯度下降法，因为存在速度项；它不是经典动量法，因为梯度不是在 $w$ 处计算的；它也不是 Adagrad 或 RMSprop，因为没有使用累积平方梯度进行逐参数的自适应缩放。", "answer": "$$\\boxed{B}$$", "id": "2187801"}, {"introduction": "为什么 Nesterov 的“前瞻”步骤如此有效？本练习旨在深入探讨 NAG 方法优势的核心机制，尤其是在动量可能导致“过冲”的情况下。我们将分析一个优化器已经越过最小值的假设情景，并要求您推导 NAG 的参数更新步骤 $\\Delta \\theta$。这将从数学上证明，与标准动量法相比，NAG 如何利用前瞻梯度信息进行更智能的修正，从而有效地“刹车”并减少振荡。[@problem_id:2187789]", "problem": "考虑一个机器学习模型，其参数正通过一个迭代优化算法进行更新。在步骤 $t$，优化器的状态由参数向量 $\\theta_t$ 和一个累积动量向量 $v_t$ 描述。优化器的目标是最小化损失函数 $J(\\theta)$。更新由学习率 $\\eta > 0$ 和动量系数 $\\gamma \\in (0, 1)$ 控制。\n\n两种常见算法，即标准动量 (SM) 和 Nesterov 加速梯度 (NAG)，的更新规则如下：\n\n- **标准动量 (SM):**\n  1. 计算梯度：$g_t = \\nabla J(\\theta_t)$\n  2. 更新动量：$v_{t+1} = \\gamma v_t + \\eta g_t$\n  3. 更新参数：$\\theta_{t+1} = \\theta_t - v_{t+1}$\n\n- **Nesterov 加速梯度 (NAG):**\n  1. 计算前瞻位置：$\\theta_{lookahead} = \\theta_t - \\gamma v_t$\n  2. 计算前瞻位置的梯度：$g_{lookahead} = \\nabla J(\\theta_{lookahead})$\n  3. 更新动量：$v_{t+1} = \\gamma v_t + \\eta g_{lookahead}$\n  4. 更新参数：$\\theta_{t+1} = \\theta_t - v_{t+1}$\n\n现在，考虑一种特殊情况，即优化器可能已经越过了一个局部最小值。在当前位置 $\\theta_t$ 的梯度指向与动量向量完全相反的方向，使得 $\\nabla J(\\theta_t) = -c v_t$，其中 $c$ 为某个正常数。\n\n为了分析 NAG 的行为，假设在 $\\theta_t$ 的局部邻域内，损失函数的梯度近似线性变化。也就是说，对于一个小的位移向量 $d$，梯度可以近似为 $\\nabla J(\\theta_t + d) \\approx \\nabla J(\\theta_t) + H d$，其中 Hessian 矩阵 $H$ 是一个常数正定矩阵。为进行此分析，假设此关系是精确的，并将 Hessian 矩阵简化为 $H = \\lambda I$，其中 $\\lambda$ 是表示局部曲率的正常数标量，$I$ 是单位矩阵。因此，该近似变为一个等式：$\\nabla J(\\theta_t + d) = \\nabla J(\\theta_t) + \\lambda d$。\n\n你的任务是在这些条件下，确定 Nesterov 加速梯度 (NAG) 算法单步的参数更新向量 $\\Delta \\theta = \\theta_{t+1} - \\theta_t$。请用 $\\eta$、$\\gamma$、$c$、$\\lambda$ 和动量向量 $v_t$ 的符号表达式来表示你的答案。", "solution": "我们已知 Nesterov 加速梯度 (NAG) 的更新步骤：\n1) 使用当前动量计算前瞻位置：$\\theta_{\\text{lookahead}} = \\theta_{t} - \\gamma v_{t}$。\n2) 计算前瞻位置的梯度：$g_{\\text{lookahead}} = \\nabla J(\\theta_{\\text{lookahead}})$。\n3) 更新动量：$v_{t+1} = \\gamma v_{t} + \\eta g_{\\text{lookahead}}$。\n4) 更新参数：$\\theta_{t+1} = \\theta_{t} - v_{t+1}$。\n\n我们假设梯度的局部线性模型为 Hessian 矩阵 $H = \\lambda I$，即 $\\nabla J(\\theta_{t} + d) = \\nabla J(\\theta_{t}) + \\lambda d$，并且已知当前梯度与动量方向相反，$\\nabla J(\\theta_{t}) = - c v_{t}$，其中 $c > 0$。\n\n首先，计算前瞻点的梯度。从 $\\theta_{t}$ 到前瞻点的位移是 $d = \\theta_{\\text{lookahead}} - \\theta_{t} = - \\gamma v_{t}$。使用线性梯度模型，\n$$\ng_{\\text{lookahead}} = \\nabla J(\\theta_{\\text{lookahead}}) = \\nabla J(\\theta_{t}) + \\lambda d = - c v_{t} + \\lambda(- \\gamma v_{t}) = -\\left(c + \\gamma \\lambda\\right) v_{t}.\n$$\n使用 NAG 规则更新动量：\n$$\nv_{t+1} = \\gamma v_{t} + \\eta g_{\\text{lookahead}} = \\gamma v_{t} + \\eta \\left[-\\left(c + \\gamma \\lambda\\right) v_{t}\\right] = \\left[\\gamma - \\eta \\left(c + \\gamma \\lambda\\right)\\right] v_{t}.\n$$\n更新参数并构成参数更新向量 $\\Delta \\theta = \\theta_{t+1} - \\theta_{t}$：\n$$\n\\theta_{t+1} = \\theta_{t} - v_{t+1} \\quad \\Longrightarrow \\quad \\Delta \\theta = - v_{t+1} = - \\left[\\gamma - \\eta \\left(c + \\gamma \\lambda\\right)\\right] v_{t} = \\left[\\eta \\left(c + \\gamma \\lambda\\right) - \\gamma\\right] v_{t}.\n$$\n因此，在所述假设下，NAG 的单步参数更新为\n$$\n\\Delta \\theta = \\left[\\eta \\left(c + \\gamma \\lambda\\right) - \\gamma\\right] v_{t}.\n$$", "answer": "$$\\boxed{\\left[\\eta\\left(c+\\gamma\\lambda\\right)-\\gamma\\right]v_{t}}$$", "id": "2187789"}]}