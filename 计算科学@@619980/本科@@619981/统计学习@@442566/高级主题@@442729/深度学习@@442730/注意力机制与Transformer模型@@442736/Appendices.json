{"hands_on_practices": [{"introduction": "理论知识是基础，但真正的理解来自于实践。全局注意力虽然功能强大，能够捕捉序列中任意两个位置之间的依赖关系，但其$O(n^2)$的计算复杂度在处理长序列时会成为瓶颈。本练习将指导你动手实现全局注意力和作为其高效替代方案的局部窗口注意力。通过量化这两种机制之间的近似误差[@problem_id:3100324]，你将深刻理解在计算效率和模型表达能力（特别是捕捉长程依赖的能力）之间进行权衡的现实意义。", "problem": "您将通过量化将注意力限制在有限窗口内时引入的近似误差，来比较单头注意力设置下的全局注意力和局部窗口注意力。您将从基本原理出发实现这两种机制，并计算作为窗口半宽度 $w$ 的函数的误差度量。\n\n定义与设置：\n- 设序列长度为 $n \\in \\mathbb{N}$。位置由 $i,j \\in \\{1,\\dots,n\\}$ 索引。\n- 定义注意力 logit 函数 $s_{i,j}$ 为\n$$\ns_{i,j} = -\\frac{(i-j)^2}{2\\sigma^2} + \\alpha \\cdot \\mathbf{1}\\{j = n - i + 1\\},\n$$\n其中 $\\sigma > 0$ 控制局部性，$\\alpha \\in \\mathbb{R}$ 提升单个目标索引 $j = n-i+1$ 以编码长程依赖关系，而 $\\mathbf{1}\\{\\cdot\\}$ 是指示函数。\n- 给定温度 $\\tau > 0$，定义全局注意力权重\n$$\na_{i,j} = \\frac{\\exp\\!\\left(s_{i,j}/\\tau\\right)}{\\sum_{k=1}^{n} \\exp\\!\\left(s_{i,k}/\\tau\\right)}.\n$$\n- 定义半宽度为 $w \\in \\mathbb{N}\\cup\\{0\\}$ 的局部窗口注意力权重，仅对窗口 $\\{j:\\ |i-j|\\le w\\}$ 内的索引进行归一化：\n$$\n\\tilde{a}_{i,j}^{(w)} =\n\\begin{cases}\n\\frac{\\exp\\!\\left(s_{i,j}/\\tau\\right)}{\\sum\\limits_{k:\\ |i-k|\\le w} \\exp\\!\\left(s_{i,k}/\\tau\\right)}  \\text{如果 } |i-j| \\le w,\\\\\n0  \\text{否则}.\n\\end{cases}\n$$\n- 令值为标量信号\n$$\nv_j = \\sin\\!\\left(\\omega j\\right)\\quad\\text{其中}\\quad \\omega = \\frac{2\\pi}{n},\n$$\n角度以弧度为单位。\n- 全局注意力输出为\n$$\no_i = \\sum_{j=1}^{n} a_{i,j} \\, v_j,\n$$\n而局部窗口输出为\n$$\n\\tilde{o}_i^{(w)} = \\sum_{j=1}^{n} \\tilde{a}_{i,j}^{(w)} \\, v_j.\n$$\n- 定义均方近似误差为 $w$ 的函数：\n$$\nE(w) = \\frac{1}{n}\\sum_{i=1}^{n} \\left(o_i - \\tilde{o}_i^{(w)}\\right)^2.\n$$\n\n任务：\n- 根据这些定义，实现一个程序，为下面提供的每个测试用例计算 $E(w)$。不允许有随机元素；所有计算必须是确定性的。\n- 正弦函数的角度必须以弧度为单位。\n\n测试套件 (每个用例为 $(n,\\sigma,\\tau,\\alpha,w)$)：\n- 用例 1：$(32, 4.0, 1.0, 2.0, 3)$。\n- 用例 2：$(32, 4.0, 1.0, 2.0, 0)$。\n- 用例 3：$(32, 4.0, 1.0, 2.0, 31)$。\n- 用例 4：$(32, 2.0, 1.0, 6.0, 3)$。\n- 用例 5：$(64, 8.0, 0.5, 3.0, 5)$。\n\n答案规格：\n- 对于每个测试用例，输出单个浮点数 $E(w)$，精确到 $6$ 位小数。\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，结果的顺序与上述测试套件的顺序一致（例如，$[x_1,x_2,x_3,x_4,x_5]$，其中每个 $x_k$ 是用例 $k$ 的 $E(w)$ 的四舍五入值）。", "solution": "该问题要求通过为几组参数计算均方近似误差 $E(w)$，来对全局注意力和局部窗口注意力机制进行定量比较。解法将遵循所提供的数学定义，从基本原理推导得出。\n\n其核心是，我们必须为每个查询位置 $i$ 计算两个量：全局注意力输出 $o_i$ 和局部窗口注意力输出 $\\tilde{o}_i^{(w)}$。误差 $E(w)$ 则是这两个输出序列之间的均方差。\n\n**1. 准备工作：索引和值信号**\n\n问题对长度为 $n$ 的序列使用基于 $1$ 的索引，其中位置由 $i, j \\in \\{1, \\dots, n\\}$ 索引。值信号是将由注意力机制聚合的信号，是位置的正弦函数：\n$$\nv_j = \\sin(\\omega j) \\quad \\text{其中} \\quad \\omega = \\frac{2\\pi}{n}\n$$\n我们可以预先计算值向量 $v = (v_1, v_2, \\dots, v_n)$，以供后续步骤使用。\n\n**2. 注意力 Logits**\n\n两种注意力机制都使用相同的注意力 logit 函数 $s_{i,j}$，该函数为键位置 $j$ 相对于查询位置 $i$ 的相关性打分：\n$$\ns_{i,j} = -\\frac{(i-j)^2}{2\\sigma^2} + \\alpha \\cdot \\mathbf{1}\\{j = n - i + 1\\}\n$$\n该函数有两个组成部分：\n- 一个类高斯项 $-\\frac{(i-j)^2}{2\\sigma^2}$，它为更接近 $i$ 的位置 $j$ 分配更高的分数。参数 $\\sigma$ 控制此局部性偏置的宽度。\n- 一个长程项 $\\alpha \\cdot \\mathbf{1}\\{j = n - i + 1\\}$，如果位置 $j$ 相对于序列两端与位置 $i$ 对称，则增加一个特定的奖励 $\\alpha$。这显式地建模了一种特定的非局部依赖关系。\n\n在实现上，我们可以构建一个 $n \\times n$ 的矩阵 $S$，其中 $S_{i,j} = s_{i,j}$。\n\n**3. 全局注意力**\n\n全局注意力权重 $a_{i,j}$ 是通过对所有可能的键位置 $j \\in \\{1, \\dots, n\\}$ 应用 softmax 函数计算得出的，并由温度参数 $\\tau > 0$ 进行缩放：\n$$\na_{i,j} = \\frac{\\exp(s_{i,j}/\\tau)}{\\sum_{k=1}^{n} \\exp(s_{i,k}/\\tau)}\n$$\n为了数值稳定性，特别是当一些 logits $s_{i,j}/\\tau$ 很大时，我们使用 log-sum-exp 技巧。对于每个查询 $i$，令 $m_i = \\max_{k} (s_{i,k}/\\tau)$。该公式可以改写为：\n$$\na_{i,j} = \\frac{\\exp(s_{i,j}/\\tau - m_i)}{\\sum_{k=1}^{n} \\exp(s_{i,k}/\\tau - m_i)}\n$$\n这样可以防止浮点数溢出。在计算出 $n \\times n$ 的注意力权重矩阵 $A = [a_{i,j}]$ 后，全局注意力输出向量 $o = (o_1, \\dots, o_n)$ 通过矩阵 $A$ 和值向量 $v$ 的矩阵向量乘积计算得出：\n$$\no_i = \\sum_{j=1}^{n} a_{i,j} v_j \\quad \\implies \\quad o = A v\n$$\n\n**4. 局部窗口注意力**\n\n局部窗口注意力机制将 softmax 归一化限制在查询位置 $i$ 周围一个半宽度为 $w$ 的窗口内。为查询 $i$ 所考虑的键集是 $W_i = \\{k \\in \\{1, \\dots, n\\} : |i-k| \\le w\\}$。局部注意力权重 $\\tilde{a}_{i,j}^{(w)}$ 定义如下：\n$$\n\\tilde{a}_{i,j}^{(w)} =\n\\begin{cases}\n\\frac{\\exp(s_{i,j}/\\tau)}{\\sum_{k \\in W_i} \\exp(s_{i,k}/\\tau)}  \\text{如果 } j \\in W_i,\\\\\n0  \\text{否则}.\n\\end{cases}\n$$\n在计算上，这可以被高效地实现。对于每个查询 $i$，我们首先确定索引 $k \\in W_i$。然后，我们仅对 logits $\\{s_{i,k}/\\tau : k \\in W_i\\}$ 应用稳定的 softmax 函数。得到的权重被放置在局部注意力矩阵 $\\tilde{A}^{(w)}$ 的第 $i$ 行的相应位置，该行所有其他条目均为 $0$。\n\n一种等效且可向量化的方法是创建一个 $n \\times n$ 的布尔掩码，对于满足 $|i-j| \\le w$ 的配对 $(i, j)$，其值为 `true`。然后我们可以创建一个修改后的 logit 矩阵，其中窗口外的条目被设置为 $-\\infty$。对此掩码矩阵直接应用标准的稳定 softmax 过程即可得到 $\\tilde{A}^{(w)}$，因为 $\\exp(-\\infty)=0$。\n\n局部窗口输出向量 $\\tilde{o}^{(w)} = (\\tilde{o}_1^{(w)}, \\dots, \\tilde{o}_n^{(w)})$ 的计算方式与全局情况类似：\n$$\n\\tilde{o}_i^{(w)} = \\sum_{j=1}^{n} \\tilde{a}_{i,j}^{(w)} v_j \\quad \\implies \\quad \\tilde{o}^{(w)} = \\tilde{A}^{(w)} v\n$$\n\n**5. 均方近似误差**\n\n最后，近似误差 $E(w)$ 是全局和局部输出向量相应元素之间差的平方的均值：\n$$\nE(w) = \\frac{1}{n}\\sum_{i=1}^{n} \\left(o_i - \\tilde{o}_i^{(w)}\\right)^2\n$$\n该度量量化了局部注意力机制对全局注意力机制的近似程度。如果 $w$ 足够大以覆盖整个序列（即 $w \\ge n-1$），则局部注意力变得与全局注意力完全相同，误差 $E(w)$ 将为 $0$。\n\n实现将针对每个测试用例遵循这些步骤，并使用 `numpy` 中的向量化操作来提高效率。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the mean squared approximation error between global and local windowed\n    attention for a series of test cases.\n    \"\"\"\n\n    def compute_E(n: int, sigma: float, tau: float, alpha: float, w: int) -> float:\n        \"\"\"\n        Calculates the mean squared approximation error E(w) for a given set of parameters.\n\n        Args:\n            n: Sequence length.\n            sigma: Standard deviation for the locality-based logit term.\n            tau: Temperature for softmax.\n            alpha: Coefficient for the long-range dependency term.\n            w: Half-width of the local attention window.\n\n        Returns:\n            The computed error E(w) as a float.\n        \"\"\"\n        # Step 1: Define 1-based index arrays for vectorized computation.\n        # i_vals will be a column vector (n, 1) and j_vals a row vector (1, n).\n        i_vals = np.arange(1, n + 1, dtype=float).reshape(n, 1)\n        j_vals = np.arange(1, n + 1, dtype=float).reshape(1, n)\n\n        # Step 2: Calculate the n x n attention logit matrix S.\n        # s_{i,j} = -(i-j)^2 / (2*sigma^2) + alpha * 1{j = n-i+1}\n        indicator = (j_vals == (n - i_vals + 1)).astype(float)\n        S = -((i_vals - j_vals)**2) / (2 * sigma**2) + alpha * indicator\n\n        # Step 3: Calculate the value vector v.\n        # v_j = sin(omega * j) with omega = 2*pi/n\n        omega = 2 * np.pi / n\n        # Use np.arange(1, n+1) for 1-based j indices.\n        v = np.sin(omega * np.arange(1, n + 1, dtype=float))\n\n        # Shared step: Scale logits by temperature.\n        S_scaled = S / tau\n\n        # --- Part 1: Global Attention ---\n        \n        # Step 4: Compute global attention weights A using a numerically stable softmax.\n        max_logits_global = np.max(S_scaled, axis=1, keepdims=True)\n        exp_logits_global = np.exp(S_scaled - max_logits_global)\n        sum_exp_logits_global = np.sum(exp_logits_global, axis=1, keepdims=True)\n        A = exp_logits_global / sum_exp_logits_global\n        \n        # Step 5: Compute the global attention output vector o.\n        o = A @ v\n\n        # --- Part 2: Local Windowed Attention ---\n\n        # Step 6: Create a boolean mask for the local window where |i-j| = w.\n        window_mask = np.abs(i_vals - j_vals) = w\n        \n        # Step 7: Compute local attention weights Atilde. First, mask the scaled logits.\n        # Values outside the window become -inf, ensuring they are zero after softmax.\n        S_scaled_local = np.where(window_mask, S_scaled, -np.inf)\n        \n        # Apply stable softmax to the windowed logits.\n        max_logits_local = np.max(S_scaled_local, axis=1, keepdims=True)\n        exp_logits_local = np.exp(S_scaled_local - max_logits_local)\n        sum_exp_logits_local = np.sum(exp_logits_local, axis=1, keepdims=True)\n        \n        # The sum won't be zero because each window contains at least the diagonal element.\n        Atilde = exp_logits_local / sum_exp_logits_local\n        \n        # Step 8: Compute the local windowed attention output vector otilde.\n        otilde = Atilde @ v\n\n        # --- Part 3: Error Calculation ---\n\n        # Step 9: Calculate the mean squared approximation error E(w).\n        error = np.mean((o - otilde)**2)\n        \n        return error\n\n    # Test suite from the problem statement\n    test_cases = [\n        (32, 4.0, 1.0, 2.0, 3),   # Case 1\n        (32, 4.0, 1.0, 2.0, 0),   # Case 2\n        (32, 4.0, 1.0, 2.0, 31),  # Case 3\n        (32, 2.0, 1.0, 6.0, 3),   # Case 4\n        (64, 8.0, 0.5, 3.0, 5),   # Case 5\n    ]\n\n    results = []\n    for params in test_cases:\n        # Unpack parameters and compute the result for each case\n        result = compute_E(*params)\n        results.append(result)\n\n    # Format the final output string as per the specification\n    # e.g., [0.123456,0.789012,...]\n    formatted_results = [f\"{r:.6f}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\n# Execute the main function.\nsolve()\n```", "id": "3100324"}, {"introduction": "正则化是控制模型复杂度、防止过拟合的核心技术，但它对模型内部工作机制的影响通常比较抽象。本练习旨在揭示L2正则化（权重衰减）对注意力机制的具体影响。你将通过编程实践发现，增加正则化强度如何通过限制参数$w$的大小，进而使注意力分布变得更加“平滑”或“发散”[@problem_id:3100379]。这种平滑效应鼓励模型关注更广泛的上下文而非仅仅聚焦于少数几个“尖锐”的信号，从而提升了模型的泛化能力，而我们将使用香农熵这一工具来精确度量这种平滑度。", "problem": "考虑 Transformer 模型中使用的单头缩放点积注意力机制，其中一个学习到的查询向量与固定的键向量相互作用，以产生一个注意力分布。令 $d$ 表示特征维度。键由一个固定矩阵 $V \\in \\mathbb{R}^{n \\times d}$ 的行给出，查询是一个学习到的参数向量 $w \\in \\mathbb{R}^{d}$。注意力 logit 定义为 $z_i = \\frac{w^\\top v_i}{\\sqrt{d}}$，其中 $i \\in \\{1,\\dots,n\\}$，$v_i$ 是 $V$ 的第 $i$ 行。注意力分布 $A \\in \\mathbb{R}^{n}$ 是通过 Softmax 函数获得的，对于任何 $z \\in \\mathbb{R}^{n}$，其定义为 $A_i = \\frac{\\exp(z_i)}{\\sum_{j=1}^{n}\\exp(z_j)}$。注意力的平滑度由 Shannon 熵 $H(A) = -\\sum_{i=1}^{n} A_i \\log A_i$ 来衡量，其中对数是自然对数（单位是奈特）。\n\n查询向量 $w$ 是从一组输入-目标对中学习得到的，使用一个带有欧几里得二范数（L2）权重衰减的线性模型，该方法也称为岭回归正则化。具体来说，给定 $m$ 个输入向量（作为矩阵 $X \\in \\mathbb{R}^{m \\times d}$ 的行）和一个目标向量 $t \\in \\mathbb{R}^{m}$，带有权重衰减 $\\lambda \\ge 0$ 的学习参数是正则化最小二乘问题的唯一最小化器：\n$$\n\\min_{w \\in \\mathbb{R}^{d}} \\ \\|Xw - t\\|_2^2 + \\lambda \\|w\\|_2^2,\n$$\n其解为\n$$\nw_\\lambda = (X^\\top X + \\lambda I_d)^{-1} X^\\top t,\n$$\n其中 $I_d$ 是 $d \\times d$ 的单位矩阵。$\\lambda = 0$ 的情况恢复为普通最小二乘法 (OLS)。\n\n你的任务：\n\n1. 仅使用基本定义和经过充分检验的公式作为基础，从第一性原理出发，推导为什么在 $V$ 的行范数有界且 $X^\\top t$ 固定的假设下，增加权重衰减参数 $\\lambda$ 倾向于增加注意力熵 $H(A)$（即，使注意力更平滑）。你的推导应从岭回归正则化、Softmax 和 Shannon 熵的定义开始，并逻辑地解释增加 $\\lambda$、缩小参数幅度、减小注意力 logit 尺度和增加熵之间的关系。不要引入任何未经验证的捷径。\n\n2. 实现一个完整、可运行的程序，使用上述定义为下面测试套件中指定的每个 $\\lambda$ 计算 $H(A)$。该程序必须：\n   - 使用给定的输入 $X$、$t$ 和正则化参数 $\\lambda$ 计算 $w_\\lambda$。\n   - 通过 $z = \\frac{V w_\\lambda}{\\sqrt{d}}$ 计算 logit $z$。\n   - 通过 $z$ 的 Softmax 计算注意力分布 $A$。\n   - 计算 $H(A) = -\\sum_{i=1}^{n} A_i \\log A_i$。\n\n使用以下固定的测试套件参数：\n- 维度 $d = 4$（一个整数）。\n- 数据点数量 $m = 6$（一个整数）。\n- 键的数量 $n = 5$（一个整数）。\n- 数据矩阵 $X \\in \\mathbb{R}^{6 \\times 4}$：\n$$\nX = \\begin{bmatrix}\n1.0  -0.5  0.3  0.0 \\\\\n0.9  0.2  -0.1  0.5 \\\\\n0.3  1.2  0.8  -0.3 \\\\\n-0.7  0.5  -1.1  0.4 \\\\\n1.1  -0.8  0.0  0.6 \\\\\n0.0  0.3  0.9  -0.9\n\\end{bmatrix}\n$$\n- 目标向量 $t \\in \\mathbb{R}^{6}$：\n$$\nt = \\begin{bmatrix}\n1.2 \\\\\n0.8 \\\\\n0.5 \\\\\n-0.3 \\\\\n1.0 \\\\\n0.2\n\\end{bmatrix}\n$$\n- 键矩阵 $V \\in \\mathbb{R}^{5 \\times 4}$：\n$$\nV = \\begin{bmatrix}\n0.6  -0.1  0.2  0.0 \\\\\n0.1  0.5  -0.4  0.3 \\\\\n-0.3  0.7  0.8  -0.5 \\\\\n1.0  -0.9  0.2  0.1 \\\\\n0.0  0.0  0.5  0.5\n\\end{bmatrix}\n$$\n- 正则化参数 $\\lambda$（一个非负实数列表）：\n$$\n\\Lambda = [0.0, \\ 0.1, \\ 1.0, \\ 10.0, \\ 100.0]\n$$\n\n答案规范：\n- 对于 $\\Lambda$ 中的每个 $\\lambda$，你的程序必须输出相应的注意力熵 $H(A)$，作为一个实数（浮点数）。\n- 你的程序应生成单行输出，其中包含熵值，顺序与 $\\Lambda$ 中的顺序相同，格式化为用方括号括起来的逗号分隔列表。例如，输出必须看起来像 $[h_1,h_2,\\dots,h_k]$，其中每个 $h_i$ 是一个浮点数。以奈特为单位表示值；不涉及物理单位或角度单位。\n\n覆盖性设计：\n- 测试套件包括“理想路径”情况（$\\lambda = 0.1$，$\\lambda = 1.0$），无正则化的边界情况（$\\lambda = 0.0$），以及强正则化的边缘情况（$\\lambda = 10.0$，$\\lambda = 100.0$）。这个范围应该揭示 $H(A)$ 如何随 $\\lambda$ 变化，并通过注意力平滑度说明其与过拟合控制的联系。", "solution": "我们从基础定义和经过充分检验的公式开始。带有欧几里得二范数（L2）权重衰减的参数向量定义为\n$$\nw_\\lambda = \\arg\\min_{w \\in \\mathbb{R}^d} \\left\\{ \\|Xw - t\\|_2^2 + \\lambda \\|w\\|_2^2 \\right\\}\n= (X^\\top X + \\lambda I_d)^{-1} X^\\top t,\n$$\n其中 $X \\in \\mathbb{R}^{m \\times d}$ 的行是数据点，$t \\in \\mathbb{R}^{m}$ 是目标向量，$\\lambda \\ge 0$ 是权重衰减参数。\n\n对于一个查询 $w \\in \\mathbb{R}^{d}$ 和键 $v_i \\in \\mathbb{R}^{d}$，缩放点积注意力 logit 定义为\n$$\nz_i = \\frac{w^\\top v_i}{\\sqrt{d}}, \\quad i \\in \\{1,\\dots,n\\}.\n$$\n注意力分布是 $z$ 的 Softmax，其条目为\n$$\nA_i = \\frac{\\exp(z_i)}{\\sum_{j=1}^{n} \\exp(z_j)}.\n$$\n$A$ 的 Shannon 熵是\n$$\nH(A) = -\\sum_{i=1}^{n} A_i \\log A_i,\n$$\n使用自然对数并以奈特为单位度量。\n\n$\\lambda$ 如何影响 $H(A)$ 的基于原理的推导：\n\n1. 从 $w_\\lambda$ 的定义出发，考虑 $X^\\top X$ 的谱分解，$X^\\top X$ 是对称半正定矩阵。设 $X^\\top X = U \\operatorname{diag}(s_1,\\dots,s_d) U^\\top$，其中 $U$ 是正交矩阵，$s_k \\ge 0$ 是特征值。那么\n$$\nw_\\lambda = U \\operatorname{diag}\\left(\\frac{1}{s_1+\\lambda},\\dots,\\frac{1}{s_d+\\lambda}\\right) U^\\top X^\\top t.\n$$\n对于固定的 $X^\\top t$，增加 $\\lambda$ 会严格增加每个分母 $s_k + \\lambda$，这反过来会缩小 $w_\\lambda$ 的每个谱分量。\n\n2. $w_\\lambda$ 的欧几里得范数是 $\\lambda$ 的单调递减函数。确实，\n$$\n\\|w_\\lambda\\|_2^2 = \\sum_{k=1}^{d} \\left(\\frac{c_k}{s_k + \\lambda}\\right)^2,\n$$\n其中 $c_k$ 是 $U^\\top X^\\top t$ 在特征基中的坐标。随着 $\\lambda$ 的增加，每一项都会减小，因为对于 $x  0$，映射 $x \\mapsto \\left(\\frac{c}{x}\\right)^2$ 是递减的。\n\n3. 每个注意力 logit 的幅度可以通过柯西-施瓦茨不等式来界定：\n$$\n|z_i| = \\left|\\frac{w_\\lambda^\\top v_i}{\\sqrt{d}}\\right| \\le \\frac{\\|w_\\lambda\\|_2 \\, \\|v_i\\|_2}{\\sqrt{d}}.\n$$\n如果 $V$ 的行范数有界，那么随着 $\\lambda$ 的增加，$\\|w_\\lambda\\|_2$ 减小，从而 logit $z_i$ 的绝对尺度也减小。\n\n4. 当 logit 被统一缩小时，Softmax 分布的熵会增加。要理解这一点，考虑一个固定的非恒定向量 $z \\in \\mathbb{R}^{n}$，并定义 $A^{(s)} = \\operatorname{Softmax}(s z)$，其中 $s \\in (0, \\infty)$。这等效于应用一个温度 $T = 1/s$。当 $s$ 减小（即 $T$ 增加）时，分布 $A^{(s)}$ 变得更加均匀。对于一个严格不均匀的 $z$，函数 $s \\mapsto H(A^{(s)})$ 对 $s$ 是严格递减的，因此对 $T$ 是严格递增的。直观地说，缩小 logit 会减少条目之间的置信度差异，将 $A$ 推向均匀分布，后者在 $H_{\\max} = \\log n$ 处使熵最大化。\n\n5. 结合第 1-4 点，增加 $\\lambda$ 会缩小 $w_\\lambda$，从而减小 logit 的尺度，因此增加熵 $H(A)$，使注意力更平滑。这一机制揭示了与过拟合控制的联系：较小的 $\\lambda$ 允许更大的参数幅度，这可能导致对特异模式的过拟合，产生尖锐的、低熵的注意力；较大的 $\\lambda$ 会惩罚这种大幅度，鼓励更平滑、更高熵的注意力，从而不易过拟合。\n\n程序算法设计：\n\n- 输入是固定的矩阵 $X$、$V$ 和向量 $t$，以及正则化参数列表 $\\Lambda$。\n- 对于 $\\Lambda$ 中的每个 $\\lambda$，使用线性系统 $(X^\\top X + \\lambda I_d) w_\\lambda = X^\\top t$ 计算 $w_\\lambda$。为确保数值稳定性并避免显式矩阵求逆，应使用可靠的线性求解器来解此系统。\n- 计算 logit $z = \\frac{V w_\\lambda}{\\sqrt{d}}$。\n- 通过数值稳定的 Softmax 计算 $A$：在进行指数运算前减去 $\\max(z)$ 以防止溢出，即 $A_i = \\frac{\\exp(z_i - \\max(z))}{\\sum_j \\exp(z_j - \\max(z))}$。\n- 使用自然对数计算熵 $H(A) = -\\sum_i A_i \\log A_i$。\n- 收集所有 $\\lambda \\in \\Lambda$ 的熵，并以单行、逗号分隔、方括号括起来的形式打印。\n\n测试套件覆盖范围和预期的定性行为：\n\n- 对于 $\\lambda = 0.0$（无正则化），$w_\\lambda$ 通常具有较大数量级，产生更尖锐的注意力和较低的熵。\n- 对于 $\\lambda = 0.1$ 和 $\\lambda = 1.0$（中等正则化），与 $\\lambda = 0.0$ 相比，熵应该会增加。\n- 对于 $\\lambda = 10.0$ 和 $\\lambda = 100.0$（强正则化），$w_\\lambda$ 变得非常小，logit 趋近于零，注意力在 $n=5$ 个条目上趋于均匀分布，并且 $H(A)$ 趋近于 $\\log 5$。\n\n程序遵循这些步骤，并以指定的单行格式 $[h_1,h_2,\\dots,h_k]$ 输出熵值，其中每个 $h_i$ 是一个以奈特为单位的浮点数。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef softmax(z: np.ndarray) - np.ndarray:\n    \"\"\"Compute numerically stable softmax.\"\"\"\n    z_max = np.max(z)\n    exp_z = np.exp(z - z_max)\n    return exp_z / np.sum(exp_z)\n\ndef ridge_solution(X: np.ndarray, t: np.ndarray, lam: float) - np.ndarray:\n    \"\"\"\n    Compute w_lambda = (X^T X + lam I)^{-1} X^T t\n    using a linear solver for numerical stability.\n    \"\"\"\n    d = X.shape[1]\n    XT_X = X.T @ X\n    A = XT_X + lam * np.eye(d)\n    b = X.T @ t\n    # Use solve; in case of numerical issues, fall back to least-squares.\n    try:\n        w = np.linalg.solve(A, b)\n    except np.linalg.LinAlgError:\n        # Fall back to pseudo-inverse based computation\n        w = np.linalg.pinv(A) @ b\n    return w\n\ndef attention_entropy(V: np.ndarray, w: np.ndarray, d: int) - float:\n    \"\"\"Compute attention logits, softmax, and Shannon entropy in nats.\"\"\"\n    logits = (V @ w) / np.sqrt(d)\n    A = softmax(logits)\n    # Shannon entropy: -sum A_i log A_i (natural log - nats)\n    # Avoid log(0) by ensuring A has no exact zeros (softmax ensures this).\n    H = -np.sum(A * np.log(A))\n    return float(H)\n\ndef solve():\n    # Define the test cases from the problem statement.\n    d = 4  # feature dimension\n    m = 6  # number of data points\n    n = 5  # number of keys\n\n    X = np.array([\n        [ 1.0, -0.5,  0.3,  0.0],\n        [ 0.9,  0.2, -0.1,  0.5],\n        [ 0.3,  1.2,  0.8, -0.3],\n        [-0.7,  0.5, -1.1,  0.4],\n        [ 1.1, -0.8,  0.0,  0.6],\n        [ 0.0,  0.3,  0.9, -0.9]\n    ], dtype=float)\n\n    t = np.array([1.2, 0.8, 0.5, -0.3, 1.0, 0.2], dtype=float)\n\n    V = np.array([\n        [ 0.6, -0.1,  0.2,  0.0],\n        [ 0.1,  0.5, -0.4,  0.3],\n        [-0.3,  0.7,  0.8, -0.5],\n        [ 1.0, -0.9,  0.2,  0.1],\n        [ 0.0,  0.0,  0.5,  0.5]\n    ], dtype=float)\n\n    lambdas = [0.0, 0.1, 1.0, 10.0, 100.0]\n\n    results = []\n    for lam in lambdas:\n        w_lam = ridge_solution(X, t, lam)\n        H = attention_entropy(V, w_lam, d)\n        results.append(H)\n\n    # Format results with consistent precision\n    formatted = [f\"{x:.6f}\" for x in results]\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(formatted)}]\")\n\nsolve()\n```", "id": "3100379"}, {"introduction": "Transformer模型通过堆叠多个注意力层来构建深层网络结构，那么信息在这些层之间是如何被逐步提炼和处理的呢？一个普遍的观察是，随着层数的加深，不同的注意力头会趋向于“专业化”，各自负责识别特定的模式。本练习引入了香农熵作为一个强大的分析工具，用于量化注意力分布的“集中”程度[@problem_id:3100381]。通过计算并比较不同层级的平均注意力熵，你可以亲手验证并理解注意力从宽泛到集中的演变过程，从而洞察深度模型内部的层级化信息处理机制。", "problem": "考虑一个在统计学习中使用的 Transformer 模型中的简化多头注意力机制。对于每个层索引 $\\ell \\in \\{1,\\dots,L\\}$ 和头索引 $h \\in \\{1,\\dots,H\\}$，注意力由一个矩阵 $A^{(\\ell,h)} \\in \\mathbb{R}^{Q \\times K}$ 表示，其中 $A^{(\\ell,h)}$ 的每一行是对于 $Q$ 个查询之一的 $K$ 个键的概率分布，该分布由兼容性得分的 softmax 变换产生。softmax 变换将一个实值得分向量 $s \\in \\mathbb{R}^K$ 转换为一个概率向量 $p \\in \\mathbb{R}^K$，其分量为 $p_i = \\exp(s_i) \\big/ \\sum_{j=1}^K \\exp(s_j)$，确保了非负性并且各分量之和为 $1$。对于一个概率向量 $p = (p_1,\\dots,p_K)$，以自然单位（nats）表示的 Shannon 熵为 $H(p) = -\\sum_{i=1}^K p_i \\log p_i$，并约定根据连续性将 $0 \\log 0$ 取为 $0$。\n\n将每层平均注意力熵 $H_\\ell$ 定义为所有头和查询的行熵的平均值：\n$$\nH_\\ell = \\frac{1}{H Q} \\sum_{h=1}^H \\sum_{q=1}^Q H\\Big(A^{(\\ell,h)}_{q,:}\\Big),\n$$\n其中 $A^{(\\ell,h)}_{q,:}$ 表示 $A^{(\\ell,h)}$ 的第 $q$ 行。我们说，如果严格不等式 $H_1  H_2  \\dots  H_L$ 成立，则模型在更深层表现出更低的熵。为了使决策对浮点舍入具有鲁棒性，使用一个容差 $\\varepsilon = 10^{-9}$，并将 $H_i  H_{i+1}$ 解释为 $H_i - H_{i+1}  \\varepsilon$。\n\n你的任务是编写一个完整的、可运行的程序，该程序为一组测试用例计算每个层 $\\ell$ 的 $H_\\ell$，在规定的容差下判断熵是否随深度严格递减，并输出一行包含在方括号内的、逗号分隔的布尔值列表。每个布尔值对应一个测试用例，并指出更深的层是否平均表现出更低的熵。\n\n使用以下测试套件。每个测试用例指定整数 $L$、$H$、$Q$、$K$ 以及注意力矩阵 $A^{(\\ell,h)}$。所有熵必须以 nats 为单位计算，并且输出必须仅为布尔值。\n\n测试用例 1 (一般情况；预期递减):\n- $L = 3$, $H = 2$, $Q = 3$, $K = 4$。\n- 层 $\\ell = 1$，头 $h = 1$：\n$$\nA^{(1,1)} =\n\\begin{bmatrix}\n0.25  0.25  0.25  0.25 \\\\\n0.40  0.30  0.20  0.10 \\\\\n0.30  0.30  0.20  0.20\n\\end{bmatrix}.\n$$\n- 层 $\\ell = 1$，头 $h = 2$：\n$$\nA^{(1,2)} =\n\\begin{bmatrix}\n0.35  0.25  0.20  0.20 \\\\\n0.30  0.30  0.20  0.20 \\\\\n0.25  0.25  0.25  0.25\n\\end{bmatrix}.\n$$\n- 层 $\\ell = 2$，头 $h = 1$：\n$$\nA^{(2,1)} =\n\\begin{bmatrix}\n0.60  0.20  0.10  0.10 \\\\\n0.70  0.10  0.10  0.10 \\\\\n0.50  0.30  0.10  0.10\n\\end{bmatrix}.\n$$\n- 层 $\\ell = 2$，头 $h = 2$：\n$$\nA^{(2,2)} =\n\\begin{bmatrix}\n0.55  0.25  0.10  0.10 \\\\\n0.60  0.20  0.10  0.10 \\\\\n0.50  0.30  0.10  0.10\n\\end{bmatrix}.\n$$\n- 层 $\\ell = 3$，头 $h = 1$：\n$$\nA^{(3,1)} =\n\\begin{bmatrix}\n0.90  0.10  0.00  0.00 \\\\\n0.85  0.05  0.05  0.05 \\\\\n0.95  0.05  0.00  0.00\n\\end{bmatrix}.\n$$\n- 层 $\\ell = 3$，头 $h = 2$：\n$$\nA^{(3,2)} =\n\\begin{bmatrix}\n0.88  0.12  0.00  0.00 \\\\\n0.92  0.08  0.00  0.00 \\\\\n0.80  0.15  0.05  0.00\n\\end{bmatrix}.\n$$\n\n测试用例 2 (边界情况；各层熵相等):\n- $L = 2$, $H = 1$, $Q = 2$, $K = 3$。\n- 层 $\\ell = 1$，头 $h = 1$：\n$$\nA^{(1,1)} =\n\\begin{bmatrix}\n0.50  0.25  0.25 \\\\\n0.20  0.40  0.40\n\\end{bmatrix}.\n$$\n- 层 $\\ell = 2$，头 $h = 1$ (与层 $\\ell = 1$ 相同):\n$$\nA^{(2,1)} =\n\\begin{bmatrix}\n0.50  0.25  0.25 \\\\\n0.20  0.40  0.40\n\\end{bmatrix}.\n$$\n\n测试用例 3 (边缘情况；具有零概率和非单调熵的混合模式):\n- $L = 3$, $H = 1$, $Q = 3$, $K = 5$。\n- 层 $\\ell = 1$，头 $h = 1$：\n$$\nA^{(1,1)} =\n\\begin{bmatrix}\n0.30  0.25  0.20  0.15  0.10 \\\\\n0.25  0.25  0.25  0.15  0.10 \\\\\n0.40  0.20  0.20  0.10  0.10\n\\end{bmatrix}.\n$$\n- 层 $\\ell = 2$，头 $h = 1$：\n$$\nA^{(2,1)} =\n\\begin{bmatrix}\n0.70  0.30  0.00  0.00  0.00 \\\\\n0.60  0.20  0.10  0.10  0.00 \\\\\n0.80  0.10  0.05  0.05  0.00\n\\end{bmatrix}.\n$$\n- 层 $\\ell = 3$，头 $h = 1$：\n$$\nA^{(3,1)} =\n\\begin{bmatrix}\n0.50  0.20  0.15  0.10  0.05 \\\\\n0.45  0.25  0.15  0.10  0.05 \\\\\n0.40  0.30  0.15  0.10  0.05\n\\end{bmatrix}.\n$$\n\n你的程序必须：\n- 在每个测试用例中，为每个 $\\ell$ 计算以 nats 为单位的 $H_\\ell$。\n- 为每个测试用例确定在容差 $\\varepsilon = 10^{-9}$ 下，$H_1  H_2  \\dots  H_L$ 是否成立。\n- 生成一行输出，其中包含用方括号括起来的、逗号分隔的结果列表（例如，`[{\\tt True},{\\tt False},{\\tt True}]`）。\n\n除了用于熵的 nats 外，不涉及任何物理单位；不使用角度和百分比。每个测试用例的输出类型是布尔值。程序必须是自包含的，并且不得从文件或用户读取输入。", "solution": "该问题要求在一个简化的多头注意力机制中，分析每层平均注意力熵。我们的任务是对于一组给定的测试用例，确定该熵度量是否在模型的连续层之间表现出严格递减的趋势。\n\n解决方案是通过一系列有原则的步骤来制定的，从 Shannon 熵的基本定义开始，逐步构建到指定的决策标准。\n\n首先，我们讨论 Shannon 熵的核心概念。对于一个由向量 $p = (p_1, p_2, \\dots, p_K)$ 表示的离散概率分布，其中 $\\sum_{i=1}^K p_i = 1$ 且 $p_i \\ge 0$，以自然单位（nats）定义的 Shannon 熵 $H(p)$ 为：\n$$\nH(p) = -\\sum_{i=1}^K p_i \\log(p_i)\n$$\n这里，$\\log$ 表示自然对数。熵量化了分布的不确定性或随机性。一个集中在单个结果上的分布（例如，$p = (1, 0, \\dots, 0)$）的熵为 0，表示完全确定。相反，一个均匀分布（例如，对所有 $i$ 都有 $p_i = 1/K$）具有最大可能的熵 $\\log(K)$，表示最大不确定性。问题规定了 $0 \\log 0$ 定义为 $0$ 的约定，这来自于极限 $\\lim_{x \\to 0^+} x \\log x = 0$。这个约定对于处理某些分量为零的稀疏概率向量至关重要。\n\n接下来，我们形式化每层平均注意力熵 $H_\\ell$。输入数据由每个层 $\\ell \\in \\{1, \\dots, L\\}$ 和每个头 $h \\in \\{1, \\dots, H\\}$ 的注意力矩阵 $A^{(\\ell,h)} \\in \\mathbb{R}^{Q \\times K}$ 组成。给定注意力矩阵 $A^{(\\ell,h)}$ 的 $Q$ 行中的每一行都是关于 $K$ 个键的概率分布。数量 $H_\\ell$ 是单个层 $\\ell$ 内所有这些逐行概率分布的 Shannon 熵的平均值。其定义是：\n$$\nH_\\ell = \\frac{1}{H Q} \\sum_{h=1}^H \\sum_{q=1}^Q H\\Big(A^{(\\ell,h)}_{q,:}\\Big)\n$$\n其中 $A^{(\\ell,h)}_{q,:}$ 是矩阵 $A^{(\\ell,h)}$ 的第 $q$ 行。分母 $H \\times Q$ 表示在层 $\\ell$ 中进行平均的分布总数（每个查询-头对一个）。\n\n主要任务是验证模型是否在更深层表现出更低的熵。这被形式化为每层平均熵严格递减的条件：\n$$\nH_1  H_2  \\dots  H_L\n$$\n这是一个不等式链。要满足该条件，每对相邻层 $(\\ell, \\ell+1)$ 都必须对所有 $\\ell \\in \\{1, \\dots, L-1\\}$ 满足 $H_\\ell  H_{\\ell+1}$。由于数值计算中可能存在浮点不精确性，简单的 `` 比较是不够的。问题提供了一个使用容差 $\\varepsilon = 10^{-9}$ 的鲁棒标准。严格不等式 $H_\\ell  H_{\\ell+1}$ 应解释为：\n$$\nH_\\ell - H_{\\ell+1}  \\varepsilon\n$$\n如果这个条件对于任何一对相邻层不成立，则整个测试用例被认为不具有严格递减熵的性质。\n\n解决每个测试用例问题的算法如下：\n1.  初始化一个列表，用于存储计算出的每层平均熵，我们称之为 `layer_entropies`。\n2.  对于从 $1$ 到 $L$ 的每个层 $\\ell$：\n    a. 初始化一个变量 `current_layer_total_entropy` 为 $0$。\n    b. 对于从 $1$ 到 $H$ 的每个头 $h$：\n        i. 访问注意力矩阵 $A^{(\\ell,h)}$。\n        ii. 对于从 $1$ 到 $Q$ 的每个查询行 $q$：\n            - 提取行向量 $p = A^{(\\ell,h)}_{q,:}$。\n            - 计算 Shannon 熵 $H(p)$。为处理 $0 \\log 0 = 0$ 的情况，我们首先筛选向量 $p$，只保留其非零元素 $p_{nz}$，然后计算 $-\\sum p_{nz,i} \\log(p_{nz,i})$。\n            - 将计算出的熵 $H(p)$ 加到 `current_layer_total_entropy`。\n    c. 计算该层的平均熵，$H_\\ell = \\text{current\\_layer\\_total\\_entropy} / (H \\times Q)$。\n    d. 将 $H_\\ell$ 追加到 `layer_entropies` 列表。\n3.  计算完所有 $H_\\ell$ 值后，检查严格递减条件。\n    a. 假设条件成立，例如，通过将布尔标志 `is_strictly_decreasing` 设置为 `True`。\n    b. 从 $\\ell = 0$ 到 $L-2$ 迭代（对 `layer_entropies` 列表使用基于 0 的索引）。\n    c. 对于每个 $\\ell$，检查 `layer_entropies`[$\\ell$] $-$ `layer_entropies`[$\\ell+1$] $ \\varepsilon$ 是否成立。\n    d. 如果检查对任何 $\\ell$ 失败，则将 `is_strictly_decreasing` 设置为 `False` 并中断循环，因为整个条件已被违反。\n4.  `is_strictly_decreasing` 的最终值是该测试用例的结果。对所有提供的测试用例重复此过程。\n\n这个结构化的过程确保了问题陈述中的所有定义和约束都得到遵守，从而得出一个正确且可验证的结果。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem of determining if average attention entropy strictly\n    decreases with layer depth for a set of test cases.\n    \"\"\"\n    \n    # Define the tolerance for strict inequality checks.\n    epsilon = 1e-9\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test Case 1\n        {\n            \"params\": {'L': 3, 'H': 2, 'Q': 3, 'K': 4},\n            \"data\": [\n                # Layer 1\n                [\n                    np.array([\n                        [0.25, 0.25, 0.25, 0.25],\n                        [0.40, 0.30, 0.20, 0.10],\n                        [0.30, 0.30, 0.20, 0.20]\n                    ]),\n                    np.array([\n                        [0.35, 0.25, 0.20, 0.20],\n                        [0.30, 0.30, 0.20, 0.20],\n                        [0.25, 0.25, 0.25, 0.25]\n                    ])\n                ],\n                # Layer 2\n                [\n                    np.array([\n                        [0.60, 0.20, 0.10, 0.10],\n                        [0.70, 0.10, 0.10, 0.10],\n                        [0.50, 0.30, 0.10, 0.10]\n                    ]),\n                    np.array([\n                        [0.55, 0.25, 0.10, 0.10],\n                        [0.60, 0.20, 0.10, 0.10],\n                        [0.50, 0.30, 0.10, 0.10]\n                    ])\n                ],\n                # Layer 3\n                [\n                    np.array([\n                        [0.90, 0.10, 0.00, 0.00],\n                        [0.85, 0.05, 0.05, 0.05],\n                        [0.95, 0.05, 0.00, 0.00]\n                    ]),\n                    np.array([\n                        [0.88, 0.12, 0.00, 0.00],\n                        [0.92, 0.08, 0.00, 0.00],\n                        [0.80, 0.15, 0.05, 0.00]\n                    ])\n                ]\n            ]\n        },\n        # Test Case 2\n        {\n            \"params\": {'L': 2, 'H': 1, 'Q': 2, 'K': 3},\n            \"data\": [\n                # Layer 1\n                [\n                    np.array([\n                        [0.50, 0.25, 0.25],\n                        [0.20, 0.40, 0.40]\n                    ])\n                ],\n                # Layer 2\n                [\n                    np.array([\n                        [0.50, 0.25, 0.25],\n                        [0.20, 0.40, 0.40]\n                    ])\n                ]\n            ]\n        },\n        # Test Case 3\n        {\n            \"params\": {'L': 3, 'H': 1, 'Q': 3, 'K': 5},\n            \"data\": [\n                # Layer 1\n                [\n                    np.array([\n                        [0.30, 0.25, 0.20, 0.15, 0.10],\n                        [0.25, 0.25, 0.25, 0.15, 0.10],\n                        [0.40, 0.20, 0.20, 0.10, 0.10]\n                    ])\n                ],\n                # Layer 2\n                [\n                    np.array([\n                        [0.70, 0.30, 0.00, 0.00, 0.00],\n                        [0.60, 0.20, 0.10, 0.10, 0.00],\n                        [0.80, 0.10, 0.05, 0.05, 0.00]\n                    ])\n                ],\n                # Layer 3\n                [\n                    np.array([\n                        [0.50, 0.20, 0.15, 0.10, 0.05],\n                        [0.45, 0.25, 0.15, 0.10, 0.05],\n                        [0.40, 0.30, 0.15, 0.10, 0.05]\n                    ])\n                ]\n            ]\n        }\n    ]\n\n    def calculate_shannon_entropy(p: np.ndarray) - float:\n        \"\"\"\n        Computes the Shannon entropy of a probability distribution vector.\n        Handles the 0*log(0) = 0 case by filtering out zero probabilities.\n        \"\"\"\n        p_nonzero = p[p  0]\n        if p_nonzero.size == 0:\n            return 0.0\n        return -np.sum(p_nonzero * np.log(p_nonzero))\n\n    results = []\n    for case in test_cases:\n        params = case[\"params\"]\n        L, H, Q = params['L'], params['H'], params['Q']\n        layers_data = case[\"data\"]\n        \n        layer_entropies = []\n        for l in range(L):\n            layer_data = layers_data[l]\n            total_layer_entropy = 0.0\n            \n            for h in range(H):\n                attention_matrix = layer_data[h]\n                for q in range(Q):\n                    prob_vector = attention_matrix[q, :]\n                    total_layer_entropy += calculate_shannon_entropy(prob_vector)\n            \n            avg_layer_entropy = total_layer_entropy / (H * Q)\n            layer_entropies.append(avg_layer_entropy)\n            \n        is_strictly_decreasing = True\n        if L  1:\n            for i in range(L - 1):\n                # Check H_i  H_{i+1} using the specified tolerance\n                if not (layer_entropies[i] - layer_entropies[i+1]  epsilon):\n                    is_strictly_decreasing = False\n                    break\n        else:\n            # A single layer trivially satisfies the condition, but \n            # problem implies L  1. A strict interpretation might depend on\n            # how to handle a chain of zero inequalities. False is safer.\n            # However, all test cases have L = 2.\n            pass\n\n        results.append(is_strictly_decreasing)\n\n    # Final print statement in the exact required format.\n    # Note: str(True) - 'True', str(False) - 'False'\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3100381"}]}