{"hands_on_practices": [{"introduction": "长短期记忆网络（LSTM）的核心优势在于其处理长距离依赖关系的能力。通过一个经典的“加法问题”，我们将亲手探索并量化标准循环神经网络（RNN）中存在的梯度消失问题。你将比较RNN、门控循环单元（GRU）和LSTM在不同序列长度下维持学习信号的能力，从而直观地理解门控机制为何能有效捕获长期记忆 [@problem_id:3191191]。", "problem": "要求您对序列学习中著名的“加法问题”进行形式化和分析，重点关注深度学习中的长期依赖问题。加法问题的定义如下：给定一个长度为 $L$、由区间 $[0,1]$ 内的实数组成的序列，其中恰好有两个位置被标记，目标是这两个标记位置上数值的总和。模型会逐步处理该序列，并在最后一个时间步输出一个标量值。为了分离长期依赖的影响，考虑最坏情况，即第一个标记位置在 $t=1$，最终输出在 $t=L$ 产生，因此最早相关时间步的学习信号必须穿过 $L-1$ 个循环转换。\n\n使用三种架构进行分析：\n- 循环神经网络（RNN），其循环更新使用平滑非线性函数，\n- 门控循环单元（GRU），\n- 长短期记忆网络（LSTM）。\n\n假设以下科学上标准且被广泛接受的基础：\n- 复合函数的微分链式法则适用于时间反向传播，因此较早时间步的梯度是跨时间步的雅可比矩阵的乘积。\n- 为了稳定性，反向传播的梯度大小由沿时间步传递记忆的路径上的循环雅可比因子的算子范数决定。\n- 在原点附近，循环神经网络（RNN）的循环雅可比矩阵主要由循环权重矩阵决定，其长期行为由该矩阵的谱半径控制。\n- 门控循环单元（GRU）和长短期记忆网络（LSTM）架构中的门控机制将前一个状态与门值相乘，从而直接缩放流经主记忆路径的梯度。\n\n为了使比较明确且计算上易于处理，对分析采用以下一致的简化：\n- 将循环神经网络（RNN）的循环雅可比矩阵视为近似时间不变的，其谱半径为 $\\rho$。\n- 将长短期记忆网络（LSTM）的遗忘门和门控循环单元（GRU）的更新门视为跨时间步的恒定标量：LSTM 的遗忘门为 $f$，GRU 的更新门为 $z$。\n- 使用归一化的输出和损失，以便循环路径之外的恒定乘法因子可以被吸收到一个单一的正标量 $\\alpha$ 中。\n\n在所有计算中使用的参数值：\n- 循环神经网络（RNN）的 $\\rho = 0.90$\n- 长短期记忆网络（LSTM）的 $f = 0.99$\n- 门控循环单元（GRU）的 $z = 0.05$\n- $\\alpha = 1.0$。\n\n将最早标记输入的“训练信号幅度”定义为损失函数相对于最早标记输入的梯度大小，该梯度沿主记忆路径从时间 $t=L$ 传播回 $t=1$。利用上述基础（链式法则和雅可比矩阵的乘积），推导出每种架构的训练信号幅度表达式，该表达式是关于 $L$ 和给定参数的函数。然后在程序中实现这些表达式。\n\n测试套件：\n- 序列长度 $L \\in \\{50, 75, 100, 500, 1000\\}$。\n- 对于每个 $L$，按以下顺序计算每种架构的训练信号幅度：循环神经网络（RNN）、门控循环单元（GRU）、长短期记忆网络（LSTM）。\n\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，列表内没有空格。该列表按 $L$ 的升序将测试套件的结果展平。也就是说，输出必须是：\n$[$RNN$(50)$,GRU$(50)$,LSTM$(50)$,RNN$(75)$,GRU$(75)$,LSTM$(75)$,RNN$(100)$,GRU$(100)$,LSTM$(100)$,RNN$(500)$,GRU$(500)$,LSTM$(500)$,RNN$(1000)$,GRU$(1000)$,LSTM$(1000)]$，\n其中每个条目都是一个表示训练信号幅度（无单位）的浮点数。最终输出为浮点数；不涉及物理单位或角度，也不得打印百分比。", "solution": "用户要求对“加法问题”进行形式化分析，以比较三种循环架构——循环神经网络（RNN）、门控循环单元（GRU）和长短期记忆网络（LSTM）——处理长期依赖的能力。该分析侧重于从最后一个时间步传播回第一个相关输入的梯度信号的幅度。\n\n问题验证如下：\n- **第 1 步：提取给定条件**\n  - 问题：针对长度为 $L$ 的序列的“加法问题”。\n  - 目标：计算当输出在 $t=L$ 时，时间 $t=1$ 处的输入的“训练信号幅度”。\n  - 架构：RNN、GRU、LSTM。\n  - 基础：\n    - 时间反向传播基于链式法则。\n    - 梯度幅度由循环雅可比矩阵的算子范数决定。\n    - RNN 的雅可比矩阵行为由循环权重矩阵的谱半径控制。\n    - GRU 和 LSTM 中的门控机制缩放梯度流。\n  - 分析简化：\n    - RNN 的循环雅可比矩阵被视为时间不变的，谱半径为 $\\rho$。\n    - LSTM 的遗忘门是恒定标量 $f$。\n    - GRU 的更新门是恒定标量 $z$。\n    - 单一正标量 $\\alpha$ 吸收循环路径外的所有恒定乘法因子。\n  - 参数值：\n    - $\\rho = 0.90$\n    - $f = 0.99$\n    - $z = 0.05$\n    - $\\alpha = 1.0$\n  - 测试套件：\n    - 序列长度 $L \\in \\{50, 75, 100, 500, 1000\\}$。\n    - 计算顺序：对每个 $L$ 依次计算 RNN、GRU、LSTM。\n\n- **第 2 步：使用提取的给定条件进行验证**\n  - **科学依据：** 该问题牢固地植根于深度学习的既定原则，特别是循环网络中的梯度流分析（梯度消失/爆炸问题）。这些简化是为复杂系统创建易于处理的分析模型的标准做法。\n  - **适定性：** 问题定义清晰，包含了所有必要的参数和假设，从而为每种情况得出了唯一的、可计算的解。\n  - **客观性：** 语言精确且技术性强，没有主观性。\n\n- **第 3 步：结论与行动**\n  - 该问题被判定为**有效**。简化被明确陈述，旨在分离梯度传播的核心机制，这是一种标准且信息丰富的分析技术。继续进行求解。\n\n分析训练信号幅度的核心原理是时间反向传播（BPTT）。损失函数 $\\mathcal{L}$ 相对于某个时间步 $t$ 的隐藏状态 $h_t$ 的梯度是通过链式法则，从后一个时间步 $t+1$ 传播梯度来计算的：\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial h_t} = \\frac{\\partial \\mathcal{L}}{\\partial h_{t+1}} \\frac{\\partial h_{t+1}}{\\partial h_t}\n$$\n为了从在时间 $t=L$ 计算的损失中找到相对于时间 $t=1$ 的状态的梯度，我们必须递归地应用此规则 $L-1$ 步：\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial h_1} = \\frac{\\partial \\mathcal{L}}{\\partial h_L} \\left( \\prod_{t=2}^{L} \\frac{\\partial h_t}{\\partial h_{t-1}} \\right)\n$$\n项 $\\frac{\\partial h_t}{\\partial h_{t-1}}$ 是在时间 $t$ 的循环雅可比矩阵。$t=1$ 处输入的“训练信号幅度”主要由该雅可比矩阵乘积的幅度决定，它确定了从 $t=L$ 的输出产生的误差信号在传播回 $t=1$ 的过程中被放大或减小的程度。问题定义了一个常数 $\\alpha$ 来吸收所有非循环因子，例如 $\\frac{\\partial \\mathcal{L}}{\\partial h_L}$ 和最后一步 $\\frac{\\partial h_1}{\\partial x_1}$。因此，信号幅度 $S(L)$ 与雅可比矩阵乘积的范数成正比。让我们为每种架构分析这一点。\n\n**循环神经网络（RNN）**\nRNN 的隐藏状态更新形式为 $h_t = \\phi(W_{hh} h_{t-1} + W_{xh} x_t + b_h)$，其中 $\\phi$ 是一个非线性激活函数，如 $\\tanh$。循环雅可比矩阵为 $\\frac{\\partial h_t}{\\partial h_{t-1}} = \\text{diag}(\\phi'(...))W_{hh}$。这些矩阵乘积的长期行为由循环权重矩阵 $W_{hh}$ 的谱半径 $\\rho$ 决定。问题通过假设每个雅可比步骤的有效幅度贡献是一个常数因子 $\\rho$ 来简化分析。将信号在 $L-1$ 个时间步上传播，导致该因子被乘以 $L-1$ 次。\n因此，对于长度为 $L$ 的序列，训练信号幅度 $S_{RNN}$ 为：\n$$\nS_{RNN}(L) = \\alpha \\rho^{L-1}\n$$\n给定 $\\alpha = 1.0$ 和 $\\rho = 0.90$，我们有：\n$$\nS_{RNN}(L) = (0.90)^{L-1}\n$$\n\n**长短期记忆网络（LSTM）**\nLSTM 处理长依赖能力的关键在于其单元状态 $c_t$，它通过门控机制进行更新：$c_t = f_t \\odot c_{t-1} + i_t \\odot \\tilde{c}_t$。这里，$f_t$ 是遗忘门，$\\odot$ 表示逐元素乘法。通过单元状态从 $c_t$ 到 $c_{t-1}$ 的梯度路径主要由遗忘门缩放，即 $\\frac{\\partial c_t}{\\partial c_{t-1}} \\approx \\text{diag}(f_t)$。问题通过假设遗忘门在所有时间步上都是一个恒定标量 $f$ 来简化这一点。沿主记忆路径反向流动的梯度信号在 $L-1$ 个步骤中的每一步都被 $f$ 缩放。\n训练信号幅度 $S_{LSTM}$ 为：\n$$\nS_{LSTM}(L) = \\alpha f^{L-1}\n$$\n给定 $\\alpha = 1.0$ 和 $f = 0.99$，我们有：\n$$\nS_{LSTM}(L) = (0.99)^{L-1}\n$$\n\n**门控循环单元（GRU）**\nGRU 的状态更新为 $h_t = (1-z_t) \\odot h_{t-1} + z_t \\odot \\tilde{h}_t$，其中 $z_t$ 是更新门。项 $(1-z_t)$ 充当动态遗忘门，控制前一个状态 $h_{t-1}$ 有多少被传递到当前状态 $h_t$。因此，$h_t$ 相对于 $h_{t-1}$ 的梯度直接被这个因子缩放。问题通过假设更新门 $z$ 是一个恒定标量来简化分析。因此，在 $L-1$ 个反向传播步骤中，每一步的缩放因子是 $(1-z)$。\n训练信号幅度 $S_{GRU}$ 为：\n$$\nS_{GRU}(L) = \\alpha (1-z)^{L-1}\n$$\n给定 $\\alpha = 1.0$ 和 $z = 0.05$，缩放因子是 $(1-0.05) = 0.95$。我们有：\n$$\nS_{GRU}(L) = (0.95)^{L-1}\n$$\n\n现在将实现这些推导出的表达式，以计算指定测试套件所需的值。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the training-signal magnitude for RNN, GRU, and LSTM architectures\n    for the \"adding problem\" over various sequence lengths.\n    \"\"\"\n    # Define the parameters from the problem statement.\n    rho_rnn = 0.90  # Spectral radius for RNN\n    f_lstm = 0.99   # Forget gate value for LSTM\n    z_gru = 0.05    # Update gate value for GRU\n    alpha = 1.0     # Constant multiplicative factor (can be omitted as it's 1.0)\n    \n    # Define the test cases for sequence length L from the problem statement.\n    test_cases_L = [50, 75, 100, 500, 1000]\n\n    results = []\n    for L in test_cases_L:\n        # The number of recurrent transitions is L-1.\n        exponent = L - 1\n\n        # Calculate the training-signal magnitude for each architecture.\n        # S(L) = alpha * (base)^(L-1)\n        \n        # RNN\n        s_rnn = alpha * (rho_rnn ** exponent)\n        \n        # GRU\n        # The scaling factor is (1 - z)\n        s_gru = alpha * ((1 - z_gru) ** exponent)\n        \n        # LSTM\n        s_lstm = alpha * (f_lstm ** exponent)\n        \n        # Append results in the specified order: RNN, GRU, LSTM\n        results.append(s_rnn)\n        results.append(s_gru)\n        results.append(s_lstm)\n\n    # Format the final output as a comma-separated list of floats in brackets.\n    # e.g., [val1,val2,val3,...]\n    output_str = f\"[{','.join(map(str, results))}]\"\n    print(output_str)\n\nsolve()\n```", "id": "3191191"}, {"introduction": "我们已经了解到LSTM能够维持长期信息，本练习将深入探究其背后的核心机制——细胞状态（cell state）。你将首先通过数学证明来揭示细胞状态实现完美记忆保存的条件。接着，你将设计一个诊断工具，用于识别序列中信息近似“恒定”的片段，从而亲身体验遗忘门和输入门是如何协同工作，精确控制信息流的 [@problem_id:3142761]。", "problem": "考虑一个长短期记忆（LSTM）单元，其单元状态更新由以下基本的、逐元素的递归关系定义\n$$\n\\mathbf{c}_t=\\mathbf{f}_t\\odot \\mathbf{c}_{t-1}+\\mathbf{i}_t\\odot \\mathbf{g}_t,\n$$\n其中 $t\\in\\{1,2,\\ldots,T\\}$，$\\mathbf{c}_t\\in\\mathbb{R}^d$ 是在时间 $t$ 的单元状态，$\\mathbf{f}_t\\in[0,1]^d$ 是遗忘门，$\\mathbf{i}_t\\in[0,1]^d$ 是输入门，$\\mathbf{g}_t\\in[-1,1]^d$ 是候选单元输入，$\\odot$ 表示哈达玛积（Hadamard product）。假设给定一个初始状态 $\\mathbf{c}_0\\in\\mathbb{R}^d$。请完全基于此定义以及向量范数和不等式的标准性质进行推导。\n\n任务：\n1) 证明如果对于所有 $t$，$\\mathbf{f}_t=\\mathbf{1}$ 且 $\\mathbf{i}_t=\\mathbf{0}$，那么对于所有 $t$，$\\mathbf{c}_t=\\mathbf{c}_0$。你的推理应仅使用单元更新定义和一种有效的证明方法。\n2) 当 $\\mathbf{c}_t$ 可观测时，提出一种有原则的诊断方法，用于检测已训练网络中接近保守的内存段。定义一个标量的单步度量 $s_t$，使用欧几里得范数来量化相对变化，并基于给定的递归关系和范数不等式解释其合理性。将接近保守的段定义为 $s_t$ 保持在给定容差 $\\varepsilon>0$ 以下的连续时间索引序列。\n3) 将该诊断方法实现为一个算法。给定 $\\{\\mathbf{c}_t\\}_{t=0}^T$、一个容差 $\\varepsilon>0$ 和一个最小段长度 $L\\in\\mathbb{N}$，该算法返回：\n   - 满足 $s_t\\le \\varepsilon$ 的连续时间步 $t\\in\\{1,\\ldots,T\\}$ 的最长序列的整数长度，以及\n   - 一个布尔值，指示是否存在至少一个长度不小于 $L$ 的序列。\n使用欧几里得范数并定义\n$$\ns_t=\\frac{\\lVert \\mathbf{c}_t-\\mathbf{c}_{t-1}\\rVert_2}{\\lVert \\mathbf{c}_{t-1}\\rVert_2+\\delta},\n$$\n其中固定数值稳定器 $\\delta=10^{-12}$。\n4) 对于下面的每个测试用例，首先通过使用指定的 $\\mathbf{f}_t$、$\\mathbf{i}_t$、$\\mathbf{g}_t$ 和 $\\mathbf{c}_0$ 模拟 LSTM 递归来生成 $\\{\\mathbf{c}_t\\}_{t=0}^T$。然后应用你的诊断方法来生成所需的两个输出。\n\n测试套件：\n- 用例 A（完全保守）：$T=20$，$d=3$，$\\mathbf{c}_0=[2,-1,0.5]$，对于所有 $t$，$\\mathbf{f}_t=\\mathbf{1}$，$\\mathbf{i}_t=\\mathbf{0}$，$\\mathbf{g}_t=[0.3,-0.4,0.5]$，$\\varepsilon=10^{-12}$，$L=10$。\n- 用例 B（通过遗忘门引起的缓慢指数漂移）：$T=30$，$d=1$，$\\mathbf{c}_0=[1.0]$，对于所有 $t$，$\\mathbf{f}_t=[0.99]$，$\\mathbf{i}_t=[0.0]$，$\\mathbf{g}_t=[0.0]$，$\\varepsilon=0.005$，$L=5$。\n- 用例 C（小的恒定输入注入）：$T=25$，$d=2$，$\\mathbf{c}_0=[1.0,1.0]$，对于所有 $t$，$\\mathbf{f}_t=[1.0,1.0]$，$\\mathbf{i}_t=[0.02,0.02]$，$\\mathbf{g}_t=[0.5,0.5]$，$\\varepsilon=0.02$，$L=10$。\n- 用例 D（零状态，精确保守）：$T=10$，$d=3$，$\\mathbf{c}_0=[0.0,0.0,0.0]$，对于所有 $t$，$\\mathbf{f}_t=\\mathbf{1}$，$\\mathbf{i}_t=\\mathbf{0}$，$\\mathbf{g}_t=[0.7,-0.3,0.1]$，$\\varepsilon=10^{-12}$，$L=5$。\n\n最终输出格式：\n你的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表。每个元素对应一个测试用例（按 A, B, C, D 的顺序），其本身是一个包含两个元素的列表：\n- 最长的接近保守的步数序列的整数长度，以及\n- 一个布尔值，指示是否存在长度至少为 $L$ 的序列。\n该行不得包含空格。例如，一个包含两个用例的有效输出应如 `[[3,True],[0,False]]`。", "solution": "该问题经评估有效。它在科学上基于循环神经网络的原理，特别是长短期记忆（LSTM）单元架构。任务设定明确、客观且自成体系，为获得唯一解提供了所有必要的定义、方程和数据。该问题要求结合数学证明、有原则的先验推理和算法实现，这是高级计算科学问题的标准格式。\n\n### 任务 1：状态保守性证明\n\n题目要求我们证明，如果在所有时间步 $t \\in \\{1, 2, \\ldots, T\\}$，遗忘门 $\\mathbf{f}_t = \\mathbf{1}$（全一向量）且输入门 $\\mathbf{i}_t = \\mathbf{0}$（全零向量），那么单元状态保持不变，即对于所有 $t$，$\\mathbf{c}_t = \\mathbf{c}_0$。\n\n我们将使用数学归纳法原理。设 $P(t)$ 为命题 $\\mathbf{c}_t = \\mathbf{c}_0$。\n\n**基础情形：** 我们必须证明 $P(1)$ 为真。单元状态更新方程为：\n$$\n\\mathbf{c}_t=\\mathbf{f}_t\\odot \\mathbf{c}_{t-1}+\\mathbf{i}_t\\odot \\mathbf{g}_t\n$$\n当 $t=1$ 时，方程变为：\n$$\n\\mathbf{c}_1=\\mathbf{f}_1\\odot \\mathbf{c}_0+\\mathbf{i}_1\\odot \\mathbf{g}_1\n$$\n代入给定条件 $\\mathbf{f}_1 = \\mathbf{1}$ 和 $\\mathbf{i}_1 = \\mathbf{0}$：\n$$\n\\mathbf{c}_1 = \\mathbf{1} \\odot \\mathbf{c}_0 + \\mathbf{0} \\odot \\mathbf{g}_1\n$$\n与向量 $\\mathbf{1}$ 的哈达玛积是恒等操作（$\\mathbf{1} \\odot \\mathbf{v} = \\mathbf{v}$），而与向量 $\\mathbf{0}$ 的哈达玛积结果为零向量（$\\mathbf{0} \\odot \\mathbf{v} = \\mathbf{0}$）。因此：\n$$\n\\mathbf{c}_1 = \\mathbf{c}_0 + \\mathbf{0} = \\mathbf{c}_0\n$$\n基础情形 $P(1)$ 成立。\n\n**归纳假设：** 假设对于某个整数 $k \\ge 1$，命题 $P(k)$ 为真，即 $\\mathbf{c}_k = \\mathbf{c}_0$。\n\n**归纳步骤：** 我们必须证明 $P(k+1)$ 为真，即 $\\mathbf{c}_{k+1} = \\mathbf{c}_0$。当 $t=k+1$ 时的更新规则为：\n$$\n\\mathbf{c}_{k+1}=\\mathbf{f}_{k+1}\\odot \\mathbf{c}_k+\\mathbf{i}_{k+1}\\odot \\mathbf{g}_{k+1}\n$$\n代入给定条件 $\\mathbf{f}_{k+1} = \\mathbf{1}$ 和 $\\mathbf{i}_{k+1} = \\mathbf{0}$：\n$$\n\\mathbf{c}_{k+1}=\\mathbf{1}\\odot \\mathbf{c}_k+\\mathbf{0}\\odot \\mathbf{g}_{k+1}\n$$\n现在，我们应用归纳假设 $\\mathbf{c}_k = \\mathbf{c}_0$：\n$$\n\\mathbf{c}_{k+1} = \\mathbf{1} \\odot \\mathbf{c}_0 + \\mathbf{0}\n$$\n$$\n\\mathbf{c}_{k+1} = \\mathbf{c}_0\n$$\n因此，$P(k+1)$ 为真。\n\n根据数学归纳法原理，对于所有整数 $t \\ge 1$，$\\mathbf{c}_t = \\mathbf{c}_0$。证明完毕。\n\n### 任务 2：用于近乎保守内存的有原则的诊断方法\n\n目标是设计一个标量度量 $s_t$，用以量化单元状态 $\\mathbf{c}_t$ 在相邻时间步之间的变化。如果一个状态不发生改变，即 $\\mathbf{c}_t \\approx \\mathbf{c}_{t-1}$，则称其为“保守的”。\n\n我们来分析变化向量 $\\Delta\\mathbf{c}_t = \\mathbf{c}_t - \\mathbf{c}_{t-1}$。使用递归关系：\n$$\n\\Delta\\mathbf{c}_t = (\\mathbf{f}_t\\odot \\mathbf{c}_{t-1}+\\mathbf{i}_t\\odot \\mathbf{g}_t) - \\mathbf{c}_{t-1}\n$$\n通过加上和减去 $\\mathbf{1}\\odot\\mathbf{c}_{t-1} = \\mathbf{c}_{t-1}$，我们可以分离出门控效应：\n$$\n\\Delta\\mathbf{c}_t = (\\mathbf{f}_t\\odot \\mathbf{c}_{t-1} - \\mathbf{c}_{t-1}) + (\\mathbf{i}_t\\odot \\mathbf{g}_t)\n$$\n从第一项中提出因子 $\\mathbf{c}_{t-1}$，得到：\n$$\n\\Delta\\mathbf{c}_t = (\\mathbf{f}_t - \\mathbf{1})\\odot \\mathbf{c}_{t-1} + \\mathbf{i}_t\\odot \\mathbf{g}_t\n$$\n这个方程揭示了单元状态变化的两个来源：\n1.  **遗忘：** 项 $(\\mathbf{f}_t - \\mathbf{1})\\odot \\mathbf{c}_{t-1}$ 表示前一状态 $\\mathbf{c}_{t-1}$ 中被“遗忘”的部分。要实现保守，该项需接近于零，这在 $\\mathbf{f}_t \\approx \\mathbf{1}$ 时发生。\n2.  **输入：** 项 $\\mathbf{i}_t\\odot \\mathbf{g}_t$ 表示写入单元状态的新信息。要实现保守，该项需接近于零，这在 $\\mathbf{i}_t \\approx \\mathbf{0}$ 时发生（因为 $\\mathbf{g}_t$ 有界）。\n\n为了量化此变化的幅度，我们使用欧几里得范数 $\\lVert \\Delta\\mathbf{c}_t \\rVert_2 = \\lVert \\mathbf{c}_t - \\mathbf{c}_{t-1} \\rVert_2$。这给出了变化的绝对度量。然而，对于一个小的状态向量，某个幅度的变化可能是显著的，但对于一个大的状态向量则可以忽略不计。因此，一个更稳健的度量是*相对*变化，它通过状态向量自身的幅度来归一化绝对变化。\n\n所提出的诊断方法，\n$$\ns_t=\\frac{\\lVert \\mathbf{c}_t-\\mathbf{c}_{t-1}\\rVert_2}{\\lVert \\mathbf{c}_{t-1}\\rVert_2+\\delta},\n$$\n正是这种相对变化。它将变化向量的幅度度量为前一个状态向量幅度的分数。\n\n*   通过使用欧几里得范数，它提供了状态向量所有 $d$ 个维度上总变化的标量摘要。\n*   通过用 $\\lVert \\mathbf{c}_{t-1} \\rVert_2$ 进行归一化，它成为一个尺度不变的度量。\n*   小常数 $\\delta > 0$ 是一个标准的数值稳定器，用于防止在 $\\mathbf{c}_{t-1} = \\mathbf{0}$ 的情况下出现除以零的错误。在这种特殊情况下，如测试用例 D 所示，如果 $\\mathbf{f}_t=\\mathbf{1}$ 且 $\\mathbf{i}_t=\\mathbf{0}$，那么 $\\mathbf{c}_t = \\mathbf{c}_{t-1} = \\mathbf{0}$，分子为零，$s_t=0$，正确地指示了完全保守。\n\n一个非常小的 $s_t$ 值（即对于某个小的 $\\varepsilon > 0$，$s_t \\le \\varepsilon$）表示 $\\lVert \\mathbf{c}_t - \\mathbf{c}_{t-1} \\rVert_2$ 相对于 $\\lVert \\mathbf{c}_{t-1} \\rVert_2$ 很小。当变化的两个来源——遗忘和输入——都最小时，即当 $\\mathbf{f}_t \\approx \\mathbf{1}$ 和 $\\mathbf{i}_t \\approx \\mathbf{0}$ 时，此条件得到满足。因此，$s_t$ 是一个用于检测近乎保守内存段的有原则且适当的诊断方法。\n\n### 任务 3：诊断实现算法\n\n该算法的输入为单元状态序列 $\\{\\mathbf{c}_t\\}_{t=0}^T$、一个容差 $\\varepsilon$ 和一个最小长度 $L$。它分两个主要阶段进行。\n\n**阶段 1：计算每步的相对变化**\n1.  初始化一个空列表 `s_values`。\n2.  对 $t$ 从 $1$ 到 $T$ 进行迭代。\n    a. 计算分子：`norm_diff` = $\\lVert \\mathbf{c}_t - \\mathbf{c}_{t-1} \\rVert_2$。\n    b. 计算分母：`norm_prev` = $\\lVert \\mathbf{c}_{t-1} \\rVert_2 + \\delta$。\n    c. 计算 $s_t = \\frac{\\text{norm\\_diff}}{\\text{norm\\_prev}}$ 并将其附加到 `s_values`。\n\n**阶段 2：寻找最长的近乎保守序列**\n1.  初始化 `max_run_length = 0` 和 `current_run_length = 0`。\n2.  遍历 `s_values`（对应于 $t=1, \\ldots, T$）。\n    a. 如果当前值 $s_t \\le \\varepsilon$：\n       i. 将 `current_run_length` 增加 1。\n    b. 否则（序列中断）：\n       i. 更新 `max_run_length = \\max(\\text{max\\_run\\_length, current\\_run\\_length})`。\n       ii. 重置 `current_run_length = 0`。\n3.  循环结束后，执行最后一次更新以计入延伸到序列末尾的序列：`max_run_length = \\max(\\text{max\\_run\\_length, current\\_run\\_length})`。\n4.  确定布尔输出：`found_long_run = (\\text{max\\_run\\_length} \\ge L)`。\n5.  返回元组 (`max_run_length`, `found_long_run`)。\n\n该算法首先计算每步的度量，然后应用标准的线性扫描来找到满足给定条件的最长连续子序列，从而正确地实现了所需的诊断方法。\n\n### 任务 4：应用于测试用例\n\n将上述算法与每个测试用例的 LSTM 递归的初步模拟相结合，可以得出结果。模拟步骤只是对 $t=1, \\ldots, T$ 迭代应用方程 $\\mathbf{c}_t=\\mathbf{f}_t\\odot \\mathbf{c}_{t-1}+\\mathbf{i}_t\\odot \\mathbf{g}_t$。实现代码在最终答案中提供。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the LSTM memory conservation problem by simulating the cell state\n    updates for given test cases and then applying a diagnostic to find\n    near-conserved segments.\n    \"\"\"\n    \n    delta = 1e-12\n\n    test_cases = [\n        {\n            # Case A: perfect conservation\n            \"T\": 20, \"d\": 3, \"c0\": np.array([2.0, -1.0, 0.5]),\n            \"ft_gen\": lambda t: np.ones(3),\n            \"it_gen\": lambda t: np.zeros(3),\n            \"gt_gen\": lambda t: np.array([0.3, -0.4, 0.5]),\n            \"epsilon\": 1e-12, \"L\": 10\n        },\n        {\n            # Case B: slow exponential drift via forget gate\n            \"T\": 30, \"d\": 1, \"c0\": np.array([1.0]),\n            \"ft_gen\": lambda t: np.array([0.99]),\n            \"it_gen\": lambda t: np.array([0.0]),\n            \"gt_gen\": lambda t: np.array([0.0]),\n            \"epsilon\": 0.005, \"L\": 5\n        },\n        {\n            # Case C: small constant input injection\n            \"T\": 25, \"d\": 2, \"c0\": np.array([1.0, 1.0]),\n            \"ft_gen\": lambda t: np.ones(2),\n            \"it_gen\": lambda t: np.array([0.02, 0.02]),\n            \"gt_gen\": lambda t: np.array([0.5, 0.5]),\n            \"epsilon\": 0.02, \"L\": 10\n        },\n        {\n            # Case D: zero state, exact conservation\n            \"T\": 10, \"d\": 3, \"c0\": np.array([0.0, 0.0, 0.0]),\n            \"ft_gen\": lambda t: np.ones(3),\n            \"it_gen\": lambda t: np.zeros(3),\n            \"gt_gen\": lambda t: np.array([0.7, -0.3, 0.1]),\n            \"epsilon\": 1e-12, \"L\": 5\n        }\n    ]\n\n    def simulate_lstm(T, c0, ft_gen, it_gen, gt_gen):\n        \"\"\"Generates the cell state sequence {c_t}.\"\"\"\n        c_series = [c0]\n        c_prev = c0\n        for t in range(1, T + 1):\n            ft = ft_gen(t)\n            it = it_gen(t)\n            gt = gt_gen(t)\n            # LSTM cell state update recurrence\n            c_t = ft * c_prev + it * gt\n            c_series.append(c_t)\n            c_prev = c_t\n        return c_series\n\n    def run_diagnostic(c_series, epsilon, L):\n        \"\"\"\n        Calculates the longest run of near-conservation and checks if it\n        meets the minimum length L.\n        \"\"\"\n        T = len(c_series) - 1\n        \n        # Stage 1: Compute per-step relative change s_t\n        s_values = []\n        for t in range(1, T + 1):\n            c_t = c_series[t]\n            c_prev = c_series[t-1]\n            \n            norm_diff = np.linalg.norm(c_t - c_prev, ord=2)\n            norm_prev = np.linalg.norm(c_prev, ord=2)\n            \n            s_t = norm_diff / (norm_prev + delta)\n            s_values.append(s_t)\n            \n        # Stage 2: Find the longest run where s_t = epsilon\n        max_run_length = 0\n        current_run_length = 0\n        for s_t in s_values:\n            if s_t = epsilon:\n                current_run_length += 1\n            else:\n                if current_run_length  max_run_length:\n                    max_run_length = current_run_length\n                current_run_length = 0\n        \n        # Final check for a run extending to the end\n        if current_run_length  max_run_length:\n            max_run_length = current_run_length\n            \n        found_long_run = max_run_length = L\n        \n        return [max_run_length, found_long_run]\n\n    results = []\n    for case in test_cases:\n        # Generate the time series data for the cell states\n        c_series = simulate_lstm(\n            case[\"T\"], case[\"c0\"], case[\"ft_gen\"],\n            case[\"it_gen\"], case[\"gt_gen\"]\n        )\n        # Apply the diagnostic to the generated series\n        result = run_diagnostic(c_series, case[\"epsilon\"], case[\"L\"])\n        results.append(result)\n\n    # Format the final output string exactly as specified, with no spaces.\n    case_strings = [f\"[{r[0]},{str(r[1])}]\" for r in results]\n    final_output_string = f\"[{','.join(case_strings)}]\"\n    print(final_output_string.replace(\"True\", \"True\").replace(\"False\", \"False\"))\n\nsolve()\n```", "id": "3142761"}, {"introduction": "LSTM的强大之处在于它能通过学习来决定何时记忆、何时遗忘。最后的这个练习将带你从分析固定的门控行为，转向通过一个简化的训练过程来主动塑造它们的行为。你将通过实现一个“条件性遗忘”任务，使用一个辅助的“塑造损失”函数并执行一步梯度下降，来体验LSTM的门控单元是如何被训练以根据未来事件做出依赖于上下文的决策的 [@problem_id:3188495]。", "problem": "给出长短期记忆 (LSTM) 单元的标准定义，其包含遗忘门、输入门和输出门。logistic Sigmoid 函数定义为 $\\sigma(z) = \\dfrac{1}{1 + e^{-z}}$，双曲正切函数为 $\\tanh(z)$。该单元使用以下核心更新结构，其中所有操作均为逐元素操作，并且在本任务中所有量均为标量：\n- 遗忘门预激活值 $a^f_t$、输入门预激活值 $a^i_t$、输出门预激活值 $a^o_t$ 和候选预激活值 $a^g_t$ 是输入 $x_t$ 和前一时刻隐藏状态 $h_{t-1}$ 的仿射函数。\n- 门和候选值的计算如下：$f_t = \\sigma(a^f_t)$，$i_t = \\sigma(a^i_t)$，$o_t = \\sigma(a^o_t)$ 以及 $g_t = \\tanh(a^g_t)$。\n- 单元状态和隐藏状态的更新如下：$c_t = f_t \\, c_{t-1} + i_t \\, g_t$ 和 $h_t = o_t \\, \\tanh(c_t)$。\n\n我们将考虑通过对单步未来目标使用教师强制（teacher forcing）来塑造遗忘门，以实现“条件性遗忘”。具体来说，假设有一个二进制的教师强制标签 $y_{t+1} \\in \\{0,1\\}$，它指示下一个事件是否为边界事件（$y_{t+1} = 1$）或不是（$y_{t+1} = 0$）。定义一个期望的遗忘水平\n$$\nf_t^\\star = \n\\begin{cases}\n0.1,  \\text{if } y_{t+1} = 1, \\\\\n0.9,  \\text{if } y_{t+1} = 0.\n\\end{cases}\n$$\n在遗忘门上引入一个塑造损失（shaping loss），\n$$\n\\mathcal{L}_{\\text{forget}}(f_t) = \\dfrac{1}{2}\\left(f_t - f_t^\\star\\right)^2,\n$$\n并仅使用损失 $\\mathcal{L}_{\\text{forget}}$ 和学习率 $\\eta$ 对遗忘门预激活值 $a^f_t$ 执行单步梯度下降（在此步骤中将所有其他预激活值视为常数）。在这一步之后，重新计算更新后的遗忘门 $\\tilde{f}_t = \\sigma(\\tilde{a}^f_t)$，并使用 $\\tilde{f}_t$ 进行单元状态和隐藏状态的更新。\n\n你的任务是实现一个程序，为每个测试用例计算在对 $a^f_t$ 就 $\\mathcal{L}_{\\text{forget}}$ 进行恰好一次梯度下降步骤后，更新的遗忘门值 $\\tilde{f}_t$。对所有用例使用以下固定的标量参数：\n- 遗忘门参数：$w^{(f)}_x = 0.9$, $w^{(f)}_h = 0.1$, $b^{(f)} = -0.2$。\n- 输入门参数：$w^{(i)}_x = 0.5$, $w^{(i)}_h = 0.3$, $b^{(i)} = 0.1$。\n- 输出门参数：$w^{(o)}_x = -0.4$, $w^{(o)}_h = 0.2$, $b^{(o)} = 0.0$。\n- 候选值参数：$w^{(g)}_x = 0.8$, $w^{(g)}_h = -0.5$, $b^{(g)} = 0.05$。\n- 输出头（最终答案中未使用，但为完整性而定义）：$w^{(y)} = 1.2$, $b^{(y)} = -0.1$。\n- 遗忘门塑造步骤的学习率：$\\eta = 2.0$。\n\n对于每个测试用例，按顺序计算以下内容：\n1. 使用给定参数计算预激活值 $a^f_t = w^{(f)}_x x_t + w^{(f)}_h h_{t-1} + b^{(f)}$、$a^i_t$、$a^o_t$ 和 $a^g_t$。\n2. 计算初始门值 $f_t = \\sigma(a^f_t)$、$i_t = \\sigma(a^i_t)$、$o_t = \\sigma(a^o_t)$ 和 $g_t = \\tanh(a^g_t)$。\n3. 根据教师强制标签 $y_{t+1}$ 按上述定义计算目标值 $f_t^\\star$。\n4. 对 $a^f_t$ 执行一步梯度下降以获得 $\\tilde{a}^f_t$，使用链式法则最小化 $\\mathcal{L}_{\\text{forget}}(f_t)$，同时保持 $a^i_t$、$a^o_t$ 和 $a^g_t$ 固定。\n5. 计算更新后的遗忘门 $\\tilde{f}_t = \\sigma(\\tilde{a}^f_t)$。\n6. （可选）使用 $\\tilde{f}_t$ 更新 $c_t$ 和 $h_t$（这不影响所需输出）。\n7. 记录该用例的 $\\tilde{f}_t$。\n\n测试套件（每个用例是一个元组 $(x_t, h_{t-1}, c_{t-1}, y_{t+1})$）：\n- 用例 $1$：$(0.7, 0.2, 0.5, 1)$。\n- 用例 $2$：$(-0.3, -0.1, -0.4, 0)$。\n- 用例 $3$：$(-2.22, 0.0, 0.9, 1)$。\n- 用例 $4$：$(5.0, 5.0, 1.2, 1)$。\n\n角度单位不适用。不涉及物理单位。所有数值输出必须以实数形式报告。\n\n最终输出格式：您的程序应生成单行输出，其中包含按测试套件顺序排列的结果，形式为方括号括起来的逗号分隔列表。每个 $\\tilde{f}_t$ 必须四舍五入到恰好六位小数。例如，格式必须类似于 $[\\tilde{f}_1,\\tilde{f}_2,\\tilde{f}_3,\\tilde{f}_4]$。", "solution": "该问题被评估为有效，因为它科学地基于循环神经网络（特别是长短期记忆（LSTM）单元）的原理，并且问题定义清晰、客观、完整，格式良好。因此，我们可以进行形式化的求解。\n\n任务是计算 LSTM 遗忘门的值（表示为 $\\tilde{f}_t$），该值是在为塑造其行为而设计的单步梯度下降之后得到的。这种塑造由一个辅助损失函数 $\\mathcal{L}_{\\text{forget}}$ 指导，该函数鼓励门的输出匹配目标值 $f_t^\\star$。\n\n该过程对每个测试用例涉及以下计算序列。\n\n**1. 梯度下降更新规则的推导**\n\n遗忘门预激活值 $a^f_t$ 的梯度下降更新由下式给出：\n$$\n\\tilde{a}^f_t = a^f_t - \\eta \\frac{\\partial \\mathcal{L}_{\\text{forget}}}{\\partial a^f_t}\n$$\n其中 $\\eta$ 是学习率，$\\tilde{a}^f_t$ 是更新后的预激活值。\n\n为了计算梯度 $\\frac{\\partial \\mathcal{L}_{\\text{forget}}}{\\partial a^f_t}$，我们必须应用链式法则，因为损失 $\\mathcal{L}_{\\text{forget}}$ 是遗忘门 $f_t$ 的函数，而 $f_t$ 又是预激活值 $a^f_t$ 的函数。\n\n损失函数定义为：\n$$\n\\mathcal{L}_{\\text{forget}}(f_t) = \\frac{1}{2}(f_t - f_t^\\star)^2\n$$\n遗忘门是通过对其预激活值应用 Sigmoid 函数 $\\sigma(z) = \\frac{1}{1 + e^{-z}}$ 得到的：\n$$\nf_t = \\sigma(a^f_t)\n$$\n\n链式法则表明：\n$$\n\\frac{\\partial \\mathcal{L}_{\\text{forget}}}{\\partial a^f_t} = \\frac{\\partial \\mathcal{L}_{\\text{forget}}}{\\partial f_t} \\cdot \\frac{\\partial f_t}{\\partial a^f_t}\n$$\n\n首先，我们计算损失函数关于门输出 $f_t$ 的导数：\n$$\n\\frac{\\partial \\mathcal{L}_{\\text{forget}}}{\\partial f_t} = \\frac{\\partial}{\\partial f_t} \\left[ \\frac{1}{2}(f_t - f_t^\\star)^2 \\right] = (f_t - f_t^\\star)\n$$\n\n接下来，我们计算遗忘门激活函数 $\\sigma(a^f_t)$ 关于其输入 $a^f_t$ 的导数。Sigmoid 函数 $\\sigma(z)$ 的导数是 $\\sigma(z)(1 - \\sigma(z))$。因此：\n$$\n\\frac{\\partial f_t}{\\partial a^f_t} = \\frac{\\partial \\sigma(a^f_t)}{\\partial a^f_t} = \\sigma(a^f_t)(1 - \\sigma(a^f_t)) = f_t(1 - f_t)\n$$\n\n结合这两部分，完整的梯度是：\n$$\n\\frac{\\partial \\mathcal{L}_{\\text{forget}}}{\\partial a^f_t} = (f_t - f_t^\\star) \\cdot f_t(1 - f_t)\n$$\n\n将此梯度代入更新规则，我们得到更新后预激活值的表达式：\n$$\n\\tilde{a}^f_t = a^f_t - \\eta (f_t - f_t^\\star) f_t(1 - f_t)\n$$\n\n最后一步是通过对这个新的预激活值应用 Sigmoid 函数来计算更新后的遗忘门值 $\\tilde{f}_t$：\n$$\n\\tilde{f}_t = \\sigma(\\tilde{a}^f_t)\n$$\n\n**2. 一般情况的逐步计算**\n\n对于每个由元组 $(x_t, h_{t-1}, c_{t-1}, y_{t+1})$ 指定的测试用例，我们使用提供的固定参数：\n- 遗忘门参数：$w^{(f)}_x = 0.9$, $w^{(f)}_h = 0.1$, $b^{(f)} = -0.2$。\n- 学习率：$\\eta = 2.0$。\n\n过程如下：\n\n- **步骤 A：计算初始遗忘门预激活值 $a^f_t$**。\n  预激活值 $a^f_t$ 是输入 $x_t$ 和前一时刻隐藏状态 $h_{t-1}$ 的仿射函数：\n  $$\n  a^f_t = w^{(f)}_x x_t + w^{(f)}_h h_{t-1} + b^{(f)}\n  $$\n\n- **步骤 B：计算初始遗忘门值 $f_t$**。\n  对预激活值应用 Sigmoid 函数：\n  $$\n  f_t = \\sigma(a^f_t) = \\frac{1}{1 + e^{-a^f_t}}\n  $$\n\n- **步骤 C：确定目标遗忘水平 $f_t^\\star$**。\n  目标由教师强制标签 $y_{t+1}$ 决定：\n  $$\n  f_t^\\star = \n  \\begin{cases}\n  0.1,  \\text{if } y_{t+1} = 1 \\\\\n  0.9,  \\text{if } y_{t+1} = 0\n  \\end{cases}\n  $$\n\n- **步骤 D：执行梯度下降更新**。\n  使用推导出的规则计算更新后的预激活值 $\\tilde{a}^f_t$：\n  $$\n  \\tilde{a}^f_t = a^f_t - \\eta (f_t - f_t^\\star) f_t(1 - f_t)\n  $$\n\n- **步骤 E：计算最终更新后的遗忘门值 $\\tilde{f}_t$**。\n  对更新后的预激活值应用 Sigmoid 函数：\n  $$\n  \\tilde{f}_t = \\sigma(\\tilde{a}^f_t) = \\frac{1}{1 + e^{-\\tilde{a}^f_t}}\n  $$\n\n此过程是确定性的，并将应用于问题陈述中提供的四个测试用例中的每一个。输入门、输出门和候选单元状态的参数是为了 LSTM 定义的完整性而提供的，但对于此特定计算并非必需。同样，前一时刻的单元状态 $c_{t-1}$ 也未使用。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the updated forget gate value for an LSTM cell after one\n    gradient descent step based on a shaping loss.\n    \"\"\"\n    # Fixed parameters as defined in the problem statement.\n    w_fx = 0.9\n    w_fh = 0.1\n    b_f = -0.2\n    eta = 2.0\n\n    # Test suite: each case is a tuple (x_t, h_{t-1}, c_{t-1}, y_{t+1})\n    test_cases = [\n        (0.7, 0.2, 0.5, 1),   # Case 1\n        (-0.3, -0.1, -0.4, 0), # Case 2\n        (-2.22, 0.0, 0.9, 1), # Case 3\n        (5.0, 5.0, 1.2, 1),    # Case 4\n    ]\n\n    def sigma(z):\n        \"\"\"The logistic sigmoid function.\"\"\"\n        return 1.0 / (1.0 + np.exp(-z))\n\n    results = []\n    for case in test_cases:\n        x_t, h_tm1, _, y_tp1 = case\n\n        # Step 1: Compute pre-activation a^f_t\n        # a^f_t = w^(f)_x * x_t + w^(f)_h * h_{t-1} + b^(f)\n        a_f_t = w_fx * x_t + w_fh * h_tm1 + b_f\n\n        # Step 2: Compute initial forget gate f_t\n        # f_t = sigma(a^f_t)\n        f_t = sigma(a_f_t)\n\n        # Step 3: Compute target f_t_star based on the teacher-forced label y_{t+1}\n        # f_t_star is 0.1 if y_{t+1} = 1, and 0.9 if y_{t+1} = 0.\n        f_t_star = 0.1 if y_tp1 == 1 else 0.9\n\n        # Step 4: Take one gradient descent step on a^f_t\n        # The gradient of the shaping loss L_forget w.r.t a^f_t is:\n        # dL/da = (f_t - f_t_star) * f_t * (1 - f_t)\n        grad_L_wrt_a_f_t = (f_t - f_t_star) * f_t * (1.0 - f_t)\n        \n        # Update a^f_t\n        # a_tilde^f_t = a^f_t - eta * dL/da\n        a_f_t_tilde = a_f_t - eta * grad_L_wrt_a_f_t\n\n        # Step 5: Compute the updated forget gate f_t_tilde\n        # f_tilde_t = sigma(a_tilde^f_t)\n        f_t_tilde = sigma(a_f_t_tilde)\n\n        # Step 7: Record the result for this case.\n        results.append(f_t_tilde)\n\n    # Format the final results as a comma-separated list of strings,\n    # with each value rounded to six decimal places, enclosed in brackets.\n    formatted_results = [f\"{r:.6f}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3188495"}]}