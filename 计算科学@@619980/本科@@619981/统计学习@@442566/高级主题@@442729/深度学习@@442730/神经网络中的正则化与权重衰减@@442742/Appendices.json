{"hands_on_practices": [{"introduction": "为了真正理解权重衰减的机制，我们首先从一个理想化的场景入手。这个练习通过假设一个恒定的损失梯度，帮助我们推导出权重在梯度下降过程中的最终稳定状态或“不动点”。通过求解这个问题，你将亲眼看到权重衰减是如何在最小化损失和收缩权重大小之间取得平衡的，从而揭示其最核心的作用原理。[@problem_id:3169493]", "problem": "您正在使用梯度下降（GD）和权重衰减来训练一个神经网络的单层，其参数向量为 $w \\in \\mathbb{R}^{d}$，惩罚目标函数为 $J(w) = L(w) + \\frac{\\lambda}{2}\\|w\\|^{2}$，其中 $\\lambda > 0$ 是权重衰减系数。假设在迭代过程中，未正则化损失的梯度是恒定的，等于一个固定的向量 $g \\in \\mathbb{R}^{d}$，并且学习率是一个常数 $\\eta > 0$。仅从应用于 $J(w)$ 的梯度下降更新的定义出发，并基于平方欧几里得范数的梯度是其系数缩放的单位矩阵这一事实，推导在所述假设下的不动点 $w^{\\star}$（即迭代收敛时，迭代值所收敛到的值）。您可以假设 $\\eta$ 和 $\\lambda$ 的取值能保证迭代对于任何初始化都会收敛。请仅用 $g$ 和 $\\lambda$ 将您的最终答案表示为闭式解析表达式。请勿引入任何数值。", "solution": "用户要求在特定假设下推导带权重衰减的梯度下降更新的不动点。首先验证问题的正确性和可解性。\n\n### 步骤 1：提取已知条件\n- 参数向量为 $w \\in \\mathbb{R}^{d}$。\n- 优化算法为梯度下降（GD）。\n- 惩罚目标函数为 $J(w) = L(w) + \\frac{\\lambda}{2}\\|w\\|^{2}$。\n- 权重衰减系数为 $\\lambda > 0$。\n- 未正则化损失的梯度是一个常数向量：$\\nabla L(w) = g \\in \\mathbb{R}^{d}$。\n- 学习率是一个常数 $\\eta > 0$。\n- 正则化项的梯度由“平方欧几里得范数的梯度是其系数缩放的单位矩阵”这一陈述给出，这意味着 $\\nabla \\left(\\frac{\\lambda}{2}\\|w\\|^{2}\\right) = \\lambda w$。\n- 假设对于任何初始化，GD迭代都会收敛到一个不动点 $w^{\\star}$。\n\n### 步骤 2：使用提取的已知条件进行验证\n该问题描述了机器学习中L2正则化（权重衰减）的标准理论分析。假设损失函数 $\\nabla L(w)$ 的梯度是一个常数向量 $g$，这是一个为了使分析易于处理的简化。这是优化理论中一个常见的建模选择，代表了算法在简单的二次目标上或在梯度近似恒定的局部区域中的行为。所有给定的项（$\\lambda$, $\\eta$）都有明确的定义。目标是找到迭代过程的不动点，这是一个数学上适定的问题。该问题是自洽的，在其理论背景下是科学合理的，并且没有矛盾或含糊之处。\n\n### 步骤 3：结论与行动\n该问题是有效的。将提供完整的解答。\n\n### 不动点的推导\n梯度下降过程根据以下规则在每次迭代 $t$ 中更新参数向量 $w$：\n$$\nw^{(t+1)} = w^{(t)} - \\eta \\nabla J(w^{(t)})\n$$\n其中 $w^{(t)}$ 是第 $t$ 次迭代时的参数向量，$\\eta$ 是学习率。\n\n要最小化的目标函数为：\n$$\nJ(w) = L(w) + \\frac{\\lambda}{2}\\|w\\|^{2}\n$$\n为了应用更新规则，我们首先需要计算 $J(w)$ 相对于 $w$ 的梯度。利用梯度算子的线性性质，我们有：\n$$\n\\nabla J(w) = \\nabla L(w) + \\nabla \\left(\\frac{\\lambda}{2}\\|w\\|^{2}\\right)\n$$\n问题陈述为此梯度计算提供了两个关键信息：\n1. 未正则化损失的梯度是一个常数向量 $g$：\n$$\n\\nabla L(w) = g\n$$\n2. 正则化项 $\\frac{\\lambda}{2}\\|w\\|^{2}$ 的梯度是 $\\lambda w$。这与标准向量微积分一致，因为平方欧几里得范数 $\\|w\\|^{2} = w^T w$ 的梯度是 $2w$，所以 $\\nabla \\left(\\frac{\\lambda}{2}\\|w\\|^{2}\\right) = \\frac{\\lambda}{2}(2w) = \\lambda w$。\n\n将这些代入 $\\nabla J(w)$ 的表达式中，我们得到：\n$$\n\\nabla J(w) = g + \\lambda w\n$$\n请注意，总目标函数的梯度不是恒定的，因为它取决于当前的参数向量 $w$。\n\n现在，通过将此梯度代入通用更新方程，我们可以写出该问题的特定梯度下降更新规则：\n$$\nw^{(t+1)} = w^{(t)} - \\eta (g + \\lambda w^{(t)})\n$$\n此迭代的不动点，记为 $w^{\\star}$，是迭代不再改变的值。也就是说，如果 $w^{(t)} = w^{\\star}$，那么 $w^{(t+1)}$ 也将等于 $w^{\\star}$。我们通过在更新方程中设置 $w^{(t+1)} = w^{(t)} = w^{\\star}$ 来找到这个点：\n$$\nw^{\\star} = w^{\\star} - \\eta (g + \\lambda w^{\\star})\n$$\n现在我们可以求解这个方程以得到 $w^{\\star}$。从两边减去 $w^{\\star}$ 得：\n$$\n0 = - \\eta (g + \\lambda w^{\\star})\n$$\n由于学习率 $\\eta$ 是一个给定的正常数（$\\eta > 0$），我们可以在等式两边同时除以 $-\\eta$ 而不影响等式成立：\n$$\n0 = g + \\lambda w^{\\star}\n$$\n这个方程揭示了不动点是目标函数梯度为零向量的点，这也是 $J(w)$ 的临界点的定义。\n\n为了分离出 $w^{\\star}$，我们重新整理方程：\n$$\n\\lambda w^{\\star} = -g\n$$\n最后，由于权重衰减系数 $\\lambda$ 也是一个给定的正常数（$\\lambda > 0$），我们可以除以 $\\lambda$：\n$$\nw^{\\star} = -\\frac{1}{\\lambda} g\n$$\n这就是在所述假设下梯度下降迭代不动点的闭式表达式。结果仅取决于损失的恒定梯度 $g$ 和权重衰减系数 $\\lambda$，符合题目要求。学习率 $\\eta$ 影响收敛到这个不动点的速率，但不影响其值。迭代收敛的假设确保了 $w^{\\star}$ 是序列 $\\{w^{(t)}\\}$ 的极限。", "answer": "$$\n\\boxed{-\\frac{g}{\\lambda}}\n$$", "id": "3169493"}, {"introduction": "在掌握了权重衰减的基本原理后，我们来探讨一个至关重要的实际问题：它如何与数据缩放相互作用？这个练习将引导你分析当输入特征被重新缩放时，为了保持模型优化的目标等价，权重衰减系数 $\\lambda$ 需要如何相应地调整。完成这个推导将让你深刻理解为什么在实践中，数据预处理和正则化策略必须协同考虑。[@problem_id:3169486]", "problem": "考虑一个单层线性神经元（一个单层神经网络），其参数向量为 $w \\in \\mathbb{R}^{d}$，它将输入 $x \\in \\mathbb{R}^{d}$ 映射到预测值 $f_{w}(x) = w^{\\top} x$。该网络在一个数据集 $\\{(x_{i}, y_{i})\\}_{i=1}^{n}$ 上进行训练，通过最小化使用均方误差 (MSE) 和 $\\ell_{2}$ 权重衰减的经验风险。形式上，目标函数为\n$$\nJ(w; X, y, \\lambda) = \\frac{1}{n} \\sum_{i=1}^{n} \\left(w^{\\top} x_{i} - y_{i}\\right)^{2} + \\frac{\\lambda}{2} \\|w\\|^{2},\n$$\n其中 $X \\in \\mathbb{R}^{n \\times d}$ 是设计矩阵，其行向量为 $x_{i}^{\\top}$，$y \\in \\mathbb{R}^{n}$ 是目标向量，$\\lambda > 0$ 是权重衰减系数，$\\|\\cdot\\|$ 表示欧几里得范数。假设每个输入都被一个正标量 $\\alpha > 0$ 重缩放，使得对所有 $i$ 都有 $x_{i} \\mapsto x_{i}' = \\alpha x_{i}$，重缩放后的设计矩阵变为 $X' = \\alpha X$。为了保持对原始输入 $x$ 的预测值相同，我们同时将权重重参数化为 $w' = \\frac{1}{\\alpha} w$，从而使得对所有 $x$ 都有 $f_{w'}(x') = f_{w}(x)$。\n\n定义重缩放后的目标函数\n$$\nJ(w'; X', y, \\lambda') = \\frac{1}{n} \\sum_{i=1}^{n} \\left({w'}^{\\top} x_{i}' - y_{i}\\right)^{2} + \\frac{\\lambda'}{2} \\|w'\\|^{2},\n$$\n其中 $\\lambda' > 0$ 是在输入重缩放后应用的新权重衰减系数。从使用 MSE 和 $\\ell_{2}$ 正则化的经验风险最小化的基本定义出发，确定 $\\lambda'$ 关于 $\\alpha$ 和 $\\lambda$ 的表达式，该表达式使得总目标函数值在联合变换 $(x_{i}, w, \\lambda) \\mapsto (x_{i}', w', \\lambda')$ 下保持不变，即\n$$\nJ(w; X, y, \\lambda) = J\\!\\left(\\frac{1}{\\alpha} w; \\alpha X, y, \\lambda'\\right)\n$$\n对所有的 $w$，$X$ 和 $y$ 均成立。\n\n你的最终答案必须是一个关于 $\\alpha$ 和 $\\lambda$ 的单一闭式解析表达式。无需四舍五入。", "solution": "首先将根据既定标准对问题进行验证。\n\n### 问题验证\n\n**步骤 1：提取已知条件**\n-   模型：单层线性神经元，参数为 $w \\in \\mathbb{R}^{d}$，输入为 $x \\in \\mathbb{R}^{d}$。\n-   预测函数：$f_{w}(x) = w^{\\top} x$。\n-   数据集：$\\{(x_{i}, y_{i})\\}_{i=1}^{n}$，设计矩阵为 $X \\in \\mathbb{R}^{n \\times d}$，目标向量为 $y \\in \\mathbb{R}^{n}$。\n-   原始目标函数：$J(w; X, y, \\lambda) = \\frac{1}{n} \\sum_{i=1}^{n} \\left(w^{\\top} x_{i} - y_{i}\\right)^{2} + \\frac{\\lambda}{2} \\|w\\|^{2}$，其中 $\\lambda > 0$ 且 $\\|\\cdot\\|$ 是欧几里得范数。\n-   输入重缩放：对于正标量 $\\alpha > 0$，有 $x_{i} \\mapsto x_{i}' = \\alpha x_{i}$。重缩放后的设计矩阵为 $X' = \\alpha X$。\n-   权重重参数化：$w \\mapsto w' = \\frac{1}{\\alpha} w$。问题陈述这确保了 $f_{w'}(x') = f_{w}(x)$。\n-   重缩放后的目标函数：$J(w'; X', y, \\lambda') = \\frac{1}{n} \\sum_{i=1}^{n} \\left({w'}^{\\top} x_{i}' - y_{i}\\right)^{2} + \\frac{\\lambda'}{2} \\|w'\\|^{2}$，其中 $\\lambda' > 0$。\n-   不变性条件：找到 $\\lambda'$ 的表达式，使得 $J(w; X, y, \\lambda) = J\\!\\left(\\frac{1}{\\alpha} w; \\alpha X, y, \\lambda'\\right)$ 对所有 $w$，$X$ 和 $y$ 均成立。\n\n**步骤 2：使用提取的已知条件进行验证**\n-   **科学性：** 该问题是统计学习中的一个标准练习，具体是分析岭回归（带有 $\\ell_{2}$ 正则化的线性回归）的性质。所有概念都是机器学习理论的基础。\n-   **适定性：** 问题提供了明确的目标和特定的不变性条件。它要求参数之间存在一个唯一确定的关系，这可以通过直接的代数运算找到。\n-   **客观性：** 问题使用形式化的数学语言和定义进行陈述，没有主观或模糊的术语。\n-   **完整性和一致性：** 所有必需的变量、函数和变换都已明确定义。设置中没有矛盾。条件 $f_{w'}(x') = f_{w}(x)$ 与 $x'$ 和 $w'$ 的定义是一致的，因为 ${w'}^{\\top} x' = (\\frac{1}{\\alpha}w)^{\\top}(\\alpha x) = \\frac{1}{\\alpha} w^{\\top} \\alpha x = w^{\\top} x$。\n\n**步骤 3：结论与行动**\n该问题被认为是**有效的**，因为它具有科学性、适定性、客观性和内部一致性。可以进行求解过程。\n\n### 求解\n\n目标是确定 $\\lambda'$ 关于 $\\alpha$ 和 $\\lambda$ 的表达式，以确保目标函数值在指定的变换下保持不变。不变性条件是：\n$$\nJ(w; X, y, \\lambda) = J\\!\\left(w'; X', y, \\lambda'\\right)\n$$\n其中 $w' = \\frac{1}{\\alpha} w$ 且 $X'$ 是行向量为 $x_{i}'^{\\top} = (\\alpha x_{i})^{\\top}$ 的矩阵。\n\n让我们通过代入 $x_{i}'$ 和 $w'$ 的表达式来分析重缩放后的目标函数 $J(w'; X', y, \\lambda')$。\n重缩放后的目标函数由两部分组成：均方误差 (MSE) 项和正则化项。\n\n首先，考虑 MSE 项：\n$$\n\\text{MSE}' = \\frac{1}{n} \\sum_{i=1}^{n} \\left({w'}^{\\top} x_{i}' - y_{i}\\right)^{2}\n$$\n代入 $w' = \\frac{1}{\\alpha} w$ 和 $x_{i}' = \\alpha x_{i}$：\n$$\n{w'}^{\\top} x_{i}' = \\left(\\frac{1}{\\alpha} w\\right)^{\\top} (\\alpha x_{i})\n$$\n对于一个标量 $c$ 和一个向量 $a$，使用转置的性质 $(c a)^{\\top} = c a^{\\top}$：\n$$\n{w'}^{\\top} x_{i}' = \\left(\\frac{1}{\\alpha} w^{\\top}\\right) (\\alpha x_{i})\n$$\n由于标量乘法满足交换律和结合律：\n$$\n{w'}^{\\top} x_{i}' = \\left(\\frac{1}{\\alpha} \\cdot \\alpha\\right) (w^{\\top} x_{i}) = 1 \\cdot (w^{\\top} x_{i}) = w^{\\top} x_{i}\n$$\n这证实了对于给定的数据点，其预测值在变换后保持不变，正如问题中所述。\n因此，重缩放后目标函数中的 MSE 项与原始的 MSE 项相同：\n$$\n\\text{MSE}' = \\frac{1}{n} \\sum_{i=1}^{n} \\left(w^{\\top} x_{i} - y_{i}\\right)^{2} = \\text{MSE}\n$$\n\n接下来，考虑重缩放后目标函数中的正则化项：\n$$\n\\text{Reg}' = \\frac{\\lambda'}{2} \\|w'\\|^{2}\n$$\n代入 $w' = \\frac{1}{\\alpha} w$：\n$$\n\\text{Reg}' = \\frac{\\lambda'}{2} \\left\\|\\frac{1}{\\alpha} w\\right\\|^{2}\n$$\n对于一个标量 $c$ 和向量 $v$，使用欧几里得范数的性质 $\\|c v\\| = |c| \\|v\\|$：\n$$\n\\left\\|\\frac{1}{\\alpha} w\\right\\|^{2} = \\left(\\left|\\frac{1}{\\alpha}\\right| \\|w\\|\\right)^{2} = \\left(\\frac{1}{\\alpha}\\right)^{2} \\|w\\|^{2} = \\frac{1}{\\alpha^2} \\|w\\|^{2}\n$$\n问题陈述 $\\alpha > 0$，所以 $|\\frac{1}{\\alpha}| = \\frac{1}{\\alpha}$。\n将此结果代回 $\\text{Reg}'$ 的表达式中：\n$$\n\\text{Reg}' = \\frac{\\lambda'}{2} \\left(\\frac{1}{\\alpha^2} \\|w\\|^{2}\\right) = \\frac{\\lambda'}{2\\alpha^2} \\|w\\|^{2}\n$$\n现在，我们可以用原始变量来写出完整的重缩放后的目标函数：\n$$\nJ(w'; X', y, \\lambda') = \\frac{1}{n} \\sum_{i=1}^{n} \\left(w^{\\top} x_{i} - y_{i}\\right)^{2} + \\frac{\\lambda'}{2\\alpha^2} \\|w\\|^{2}\n$$\n不变性条件要求该表达式对所有的 $w$，$X$ 和 $y$ 都与原始目标函数相等：\n$$\nJ(w; X, y, \\lambda) = \\frac{1}{n} \\sum_{i=1}^{n} \\left(w^{\\top} x_{i} - y_{i}\\right)^{2} + \\frac{\\lambda}{2} \\|w\\|^{2}\n$$\n令两个总目标函数的表达式相等：\n$$\n\\frac{1}{n} \\sum_{i=1}^{n} \\left(w^{\\top} x_{i} - y_{i}\\right)^{2} + \\frac{\\lambda}{2} \\|w\\|^{2} = \\frac{1}{n} \\sum_{i=1}^{n} \\left(w^{\\top} x_{i} - y_{i}\\right)^{2} + \\frac{\\lambda'}{2\\alpha^2} \\|w\\|^{2}\n$$\nMSE 项是相同的，因此可以消掉，只剩下正则化项的等式：\n$$\n\\frac{\\lambda}{2} \\|w\\|^{2} = \\frac{\\lambda'}{2\\alpha^2} \\|w\\|^{2}\n$$\n这个等式必须对任何参数向量 $w \\in \\mathbb{R}^d$ 都成立。对于任何 $w$ 不是零向量的非平凡情况（即 $\\|w\\|^2 \\neq 0$），我们可以将等式两边同时除以 $\\frac{1}{2}\\|w\\|^2$：\n$$\n\\lambda = \\frac{\\lambda'}{\\alpha^2}\n$$\n解出 $\\lambda'$ 即可得到所需的关系：\n$$\n\\lambda' = \\alpha^2 \\lambda\n$$\n这个结果表明，为了在输入特征缩放因子为 $\\alpha$、相应权重缩放因子为 $1/\\alpha$ 的情况下保持目标函数等价，权重衰减系数 $\\lambda$ 必须缩放 $\\alpha^2$ 倍。", "answer": "$$\\boxed{\\alpha^{2} \\lambda}$$", "id": "3169486"}, {"introduction": "权重衰减（$\\ell_2$ 正则化）并非唯一的正则化方法，它与旨在促进稀疏性的 $\\ell_1$ 正则化有何不同？这个思辨性练习将你置于一个包含相关特征的经典场景中，让你比较这两种正则化方法的表现。通过分析此案例，你将理解权重衰减特有的“分组效应”，并明晰在模型稳定性和稀疏性之间进行权衡的智慧，从而更深刻地把握选择权重衰减的理由。[@problem_id:3169449]", "problem": "考虑一个单输出神经网络，它有一个全连接层和恒等激活函数，因此其预测由 $\\hat{y} = \\mathbf{w}^{\\top}\\mathbf{x}$ 给出，其中权重向量为 $\\mathbf{w} \\in \\mathbb{R}^{2}$，输入为 $\\mathbf{x} \\in \\mathbb{R}^{2}$。数据根据带有高斯协变量的线性模型生成：$\\mathbf{x} \\sim \\mathcal{N}(\\mathbf{0}, \\Sigma)$，其中 $\\Sigma = \\begin{pmatrix} 1  \\rho \\\\ \\rho  1 \\end{pmatrix}$，相关性参数为 $\\rho \\in [-1, 1]$，标签满足 $y = \\boldsymbol{\\theta}^{\\top}\\mathbf{x} + \\varepsilon$，其中 $\\boldsymbol{\\theta} = (t, t)^{\\top}$（对于某个 $t \\neq 0$），$\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2})$ 是独立噪声。在训练过程中应用了两种正则化方案：\n- $\\ell_{1}$ 正则化（也称为 Lasso），其惩罚参数产生一个稀疏解 $\\mathbf{w}^{(1)} = (2a, 0)^{\\top}$（对于某个 $a > 0$）。\n- $\\ell_{2}$ 正则化（权重衰减），其惩罚参数产生一个非稀疏解 $\\mathbf{w}^{(2)} = (a, a)^{\\top}$（对于相同的 $a > 0$）。\n请注意，$\\|\\mathbf{w}^{(1)}\\|_{1} = \\|\\mathbf{w}^{(2)}\\|_{1} = 2a$，但 $\\mathbf{w}^{(1)}$ 比 $\\mathbf{w}^{(2)}$ 更稀疏。假设测试输入与训练输入遵循相同的分布，性能通过期望均方误差（MSE）来评估，定义为 $\\mathbb{E}\\big[(y - \\mathbf{w}^{\\top}\\mathbf{x})^{2}\\big]$。\n\n哪个选项正确地指出了一个在稀疏性方面 $\\ell_{1}$ 正则化优于 $\\ell_{2}$，但 $\\ell_{2}$（权重衰减）产生了严格更优的预测性能的情况，并根据特征相关性和估计器稳定性解释了这一结果？\n\nA. 当特征呈正相关（$0 \\le \\rho  1$）且真实信号分布在两个输入上（$\\boldsymbol{\\theta} = (t, t)^{\\top}$，其中 $t \\neq 0$）时，对于任何 $a > 0$，权重衰减解 $\\mathbf{w}^{(2)} = (a, a)^{\\top}$ 获得的期望 MSE 严格低于稀疏解 $\\mathbf{w}^{(1)} = (2a, 0)^{\\top}$，而 $\\mathbf{w}^{(1)}$ 仍然更稀疏。发生这种情况是因为 $\\ell_{2}$ 将权重分配到相关的方向上，从而减少方差并稳定估计器，而 $\\ell_{1}$ 则任意地丢弃一个相关输入，增加了偏差和不稳定性。\n\nB. 对于任何 $0 \\le \\rho  1$ 的相关水平，$\\ell_{1}$ 正则化总是产生比 $\\ell_{2}$ 更低的期望 MSE，因为它移除了冗余特征；相关性不影响估计器的稳定性，并且稀疏性直接转化为更好的预测准确性。\n\nC. 仅在特征完全相关（$\\rho = 1$）的情况下，权重衰减在预测性能上才优于 $\\ell_{1}$；否则，两种方法的期望 MSE 相等，因为它们正则化了相同的总大小 $\\|\\mathbf{w}\\|_{1} = 2a$。\n\nD. 当特征呈强负相关（例如 $\\rho \\approx -0.9$）且真实信号分布在两个输入上（$\\boldsymbol{\\theta} = (t, t)^{\\top}$）时，稀疏解 $\\mathbf{w}^{(1)} = (2a, 0)^{\\top}$ 比权重衰减解 $\\mathbf{w}^{(2)} = (a, a)^{\\top}$ 更稳定，产生更低的期望 MSE，因为丢弃一个负相关的特征会减少有害方差；$\\ell_{2}$ 保留了两个输入，因此会过拟合。", "solution": "用户希望我解决一个关于简单线性模型中正则化的问题。\n\n我的计划是：\n1.  **解构请求：** 我将分析角色、任务、格式规则和验证过程。\n2.  **执行计划：**\n    *   **验证问题：** 我将逐字提取所有给定信息，并根据科学原则进行核对，确保问题是适定的且客观的。\n    *   **推导解答：** 如果问题有效，我将为两种正则化方案计算期望均方误差（MSE）。\n    *   **比较方案：** 我将比较 MSE，以确定在哪种条件下一种方案优于另一种。\n    *   **评估选项：** 我将基于我的推导和已建立的统计理论分析每个选项（A、B、C、D）。\n3.  **构建最终输出：** 我将以指定的 XML 样式和 LaTeX 格式编写解决方案和最终答案。\n\n---\n\n### 第 1 步：问题验证\n\n我将首先按照指示验证问题陈述。\n\n#### 第 1.1 步：提取已知信息\n\n-   **模型：** 单输出神经网络，一个全连接层，恒等激活函数。\n-   **预测：** $\\hat{y} = \\mathbf{w}^{\\top}\\mathbf{x}$。\n-   **权重向量：** $\\mathbf{w} \\in \\mathbb{R}^{2}$。\n-   **输入向量：** $\\mathbf{x} \\in \\mathbb{R}^{2}$。\n-   **输入分布：** $\\mathbf{x} \\sim \\mathcal{N}(\\mathbf{0}, \\Sigma)$。\n-   **协方差矩阵：** $\\Sigma = \\begin{pmatrix} 1  \\rho \\\\ \\rho  1 \\end{pmatrix}$。\n-   **相关性参数：** $\\rho \\in [-1, 1]$。\n-   **标签生成过程：** $y = \\boldsymbol{\\theta}^{\\top}\\mathbf{x} + \\varepsilon$。\n-   **真实参数向量：** $\\boldsymbol{\\theta} = (t, t)^{\\top}$，对于某个 $t \\neq 0$。\n-   **噪声：** $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2})$，独立于 $\\mathbf{x}$。\n-   **$\\ell_1$ 正则化解（Lasso）：** $\\mathbf{w}^{(1)} = (2a, 0)^{\\top}$，对于某个 $a > 0$。\n-   **$\\ell_2$ 正则化解（权重衰减）：** $\\mathbf{w}^{(2)} = (a, a)^{\\top}$，对于相同的 $a > 0$。\n-   **稀疏性：** $\\mathbf{w}^{(1)}$ 比 $\\mathbf{w}^{(2)}$ 更稀疏。\n-   **测试数据分布：** 与训练分布相同。\n-   **性能指标：** 期望均方误差（MSE），$\\mathbb{E}\\big[(y - \\mathbf{w}^{\\top}\\mathbf{x})^{2}\\big]$。\n\n#### 第 1.2 步：使用提取的已知信息进行验证\n\n-   **科学依据：** 这个问题是统计学习理论中一个标准的、公认的问题，涉及在线性模型中使用相关高斯特征时，对 Ridge ($\\ell_2$) 和 Lasso ($\\ell_1$) 回归的比较。所有组成部分都基于统计学和机器学习的基本原理。\n-   **适定性：** 问题是适定的。所有变量、分布和参数都已明确定义，且目标——比较两种特定估计器的期望 MSE——是明确的。可以进行唯一的比较。\n-   **客观性：** 问题陈述以精确的数学语言表达，没有任何主观或基于意见的声明。\n\n该问题未表现出任何无效性缺陷：\n1.  **科学/事实不健全：** 无。该设置是用于说明 $\\ell_1$ 和 $\\ell_2$ 正则化性质的经典示例。\n2.  **非形式化/不相关：** 无。该问题是形式化的，并与指定主题直接相关。\n3.  **不完整/矛盾：** 无。该问题提供了计算和比较两种估计器期望 MSE 所需的所有信息。假设这些特定的解 $\\mathbf{w}^{(1)}$ 和 $\\mathbf{w}^{(2)}$ 是由它们各自的正则化方案（对于某些惩罚参数）产生的，这是一个用于比较分析的有效设置。\n4.  **不现实/不可行：** 该设置是一个理论模型，但它完全可行且在数学上是一致的。\n5.  **不适定：** 问题定义明确，并能得出确定的结果。\n6.  **伪深刻/琐碎：** 问题涉及正则化中的一个核心、非琐碎的概念：在存在特征相关性的情况下，稀疏性与稳定性之间的权衡。\n7.  **超出科学可验证性：** 这些声明可以通过数学推导直接验证。\n\n#### 第 1.3 步：结论与行动\n\n问题陈述是**有效的**。我将继续进行解答。\n\n### 第 2 步：推导与分析\n\n性能指标是期望均方误差（MSE），$\\mathbb{E}\\big[(y - \\mathbf{w}^{\\top}\\mathbf{x})^{2}\\big]$。期望是关于 $\\mathbf{x}$ 和 $\\varepsilon$ 的联合分布计算的。\n\n让我们分解 MSE。我们有 $y = \\boldsymbol{\\theta}^{\\top}\\mathbf{x} + \\varepsilon$。\n$$\ny - \\mathbf{w}^{\\top}\\mathbf{x} = (\\boldsymbol{\\theta}^{\\top}\\mathbf{x} + \\varepsilon) - \\mathbf{w}^{\\top}\\mathbf{x} = (\\boldsymbol{\\theta} - \\mathbf{w})^{\\top}\\mathbf{x} + \\varepsilon\n$$\n对该表达式求平方得到：\n$$\n(y - \\mathbf{w}^{\\top}\\mathbf{x})^{2} = \\left((\\boldsymbol{\\theta} - \\mathbf{w})^{\\top}\\mathbf{x}\\right)^2 + 2\\varepsilon (\\boldsymbol{\\theta} - \\mathbf{w})^{\\top}\\mathbf{x} + \\varepsilon^2\n$$\n取期望，我们注意到 $\\varepsilon$ 独立于 $\\mathbf{x}$ 且 $\\mathbb{E}[\\varepsilon] = 0$。因此，交叉项消失：$\\mathbb{E}\\left[2\\varepsilon (\\boldsymbol{\\theta} - \\mathbf{w})^{\\top}\\mathbf{x}\\right] = 2\\mathbb{E}[\\varepsilon] \\mathbb{E}\\left[(\\boldsymbol{\\theta} - \\mathbf{w})^{\\top}\\mathbf{x}\\right] = 0$。我们还有 $\\mathbb{E}[\\varepsilon^2] = \\text{Var}(\\varepsilon) = \\sigma^2$。\n\n因此，期望 MSE 为：\n$$\n\\text{MSE}(\\mathbf{w}) = \\mathbb{E}\\left[\\left((\\boldsymbol{\\theta} - \\mathbf{w})^{\\top}\\mathbf{x}\\right)^2\\right] + \\sigma^2\n$$\n第一项是二次型的期望。对于一个随机向量 $\\mathbf{x} \\sim \\mathcal{N}(\\mathbf{0}, \\Sigma)$ 和一个固定向量 $\\mathbf{v} = \\boldsymbol{\\theta} - \\mathbf{w}$，随机变量 $Z = \\mathbf{v}^{\\top}\\mathbf{x}$ 服从均值为 $\\mathbb{E}[Z] = \\mathbf{v}^{\\top}\\mathbb{E}[\\mathbf{x}] = 0$、方差为 $\\text{Var}(Z) = \\mathbf{v}^{\\top}\\Sigma\\mathbf{v}$ 的正态分布。因此，$\\mathbb{E}[Z^2] = \\text{Var}(Z) + (\\mathbb{E}[Z])^2 = \\mathbf{v}^{\\top}\\Sigma\\mathbf{v}$。\n\n所以，期望 MSE 简化为：\n$$\n\\text{MSE}(\\mathbf{w}) = (\\boldsymbol{\\theta} - \\mathbf{w})^{\\top}\\Sigma(\\boldsymbol{\\theta} - \\mathbf{w}) + \\sigma^2\n$$\n项 $\\sigma^2$ 是两种估计器共有的不可约误差。要比较它们，我们只需要比较风险项 $R(\\mathbf{w}) = (\\boldsymbol{\\theta} - \\mathbf{w})^{\\top}\\Sigma(\\boldsymbol{\\theta} - \\mathbf{w})$。\n\n我们已知 $\\boldsymbol{\\theta} = (t, t)^{\\top}$，$\\mathbf{w}^{(1)} = (2a, 0)^{\\top}$，$\\mathbf{w}^{(2)} = (a, a)^{\\top}$，以及 $\\Sigma = \\begin{pmatrix} 1  \\rho \\\\ \\rho  1 \\end{pmatrix}$。\n\n**对 $\\ell_1$ 解 $\\mathbf{w}^{(1)}$ 的分析：**\n差向量为 $\\boldsymbol{\\theta} - \\mathbf{w}^{(1)} = (t, t)^{\\top} - (2a, 0)^{\\top} = (t - 2a, t)^{\\top}$。\n风险为：\n$$\nR(\\mathbf{w}^{(1)}) = (t - 2a, t) \\begin{pmatrix} 1  \\rho \\\\ \\rho  1 \\end{pmatrix} \\begin{pmatrix} t - 2a \\\\ t \\end{pmatrix}\n$$\n$$\nR(\\mathbf{w}^{(1)}) = (t - 2a, t) \\begin{pmatrix} (t - 2a) + \\rho t \\\\ \\rho(t - 2a) + t \\end{pmatrix}\n$$\n$$\nR(\\mathbf{w}^{(1)}) = (t - 2a)[(t - 2a) + \\rho t] + t[\\rho(t - 2a) + t]\n$$\n$$\nR(\\mathbf{w}^{(1)}) = (t - 2a)^2 + \\rho t(t - 2a) + \\rho t(t - 2a) + t^2\n$$\n$$\nR(\\mathbf{w}^{(1)}) = (t - 2a)^2 + 2\\rho t(t-2a) + t^2\n$$\n$$\nR(\\mathbf{w}^{(1)}) = (t^2 - 4at + 4a^2) + (2\\rho t^2 - 4a\\rho t) + t^2\n$$\n$$\nR(\\mathbf{w}^{(1)}) = 4a^2 - 4at(1+\\rho) + 2t^2(1+\\rho)\n$$\n\n**对 $\\ell_2$ 解 $\\mathbf{w}^{(2)}$ 的分析：**\n差向量为 $\\boldsymbol{\\theta} - \\mathbf{w}^{(2)} = (t, t)^{\\top} - (a, a)^{\\top} = (t - a, t - a)^{\\top}$。\n风险为：\n$$\nR(\\mathbf{w}^{(2)}) = (t - a, t - a) \\begin{pmatrix} 1  \\rho \\\\ \\rho  1 \\end{pmatrix} \\begin{pmatrix} t - a \\\\ t - a \\end{pmatrix}\n$$\n设 $c = t - a$。表达式为 $(c, c) \\Sigma (c, c)^{\\top}$。\n$$\nR(\\mathbf{w}^{(2)}) = (c, c) \\begin{pmatrix} c + \\rho c \\\\ \\rho c + c \\end{pmatrix} = c(c + \\rho c) + c(\\rho c + c) = 2c^2 + 2\\rho c^2 = 2c^2(1+\\rho)\n$$\n代回 $c = t - a$：\n$$\nR(\\mathbf{w}^{(2)}) = 2(t - a)^2(1 + \\rho)\n$$\n$$\nR(\\mathbf{w}^{(2)}) = 2(t^2 - 2at + a^2)(1 + \\rho)\n$$\n$$\nR(\\mathbf{w}^{(2)}) = (2t^2 - 4at + 2a^2)(1+\\rho) = 2t^2(1+\\rho) - 4at(1+\\rho) + 2a^2(1+\\rho)\n$$\n\n**比较：**\n我们想找到 $\\ell_2$ 解性能严格更优的条件，即 $\\text{MSE}(\\mathbf{w}^{(2)})  \\text{MSE}(\\mathbf{w}^{(1)})$，这等价于 $R(\\mathbf{w}^{(2)})  R(\\mathbf{w}^{(1)})$。\n\n$$\nR(\\mathbf{w}^{(1)}) = 4a^2 - 4at(1+\\rho) + 2t^2(1+\\rho)\n$$\n$$\nR(\\mathbf{w}^{(2)}) = 2a^2(1+\\rho) - 4at(1+\\rho) + 2t^2(1+\\rho)\n$$\n\n通过消去共同项 $-4at(1+\\rho) + 2t^2(1+\\rho)$，比较 $R(\\mathbf{w}^{(2)})  R(\\mathbf{w}^{(1)})$ 简化为：\n$$\n2a^2(1+\\rho)  4a^2\n$$\n因为 $a > 0$，我们可以除以 $2a^2$：\n$$\n1 + \\rho  2\n$$\n$$\n\\rho  1\n$$\n问题指定 $\\rho \\in [-1, 1]$。因此，对于任何严格小于 1 的 $\\rho$ 值，$\\ell_2$ 解 $\\mathbf{w}^{(2)}$ 产生的期望 MSE 严格低于 $\\ell_1$ 解 $\\mathbf{w}^{(1)}$。如果 $\\rho = 1$，它们的 MSE 相等。\n\n这个结果很直观。真实模型是 $\\boldsymbol{\\theta} = (t,t)^{\\top}$，意味着两个特征同等重要。$\\ell_2$ 解 $\\mathbf{w}^{(2)} = (a,a)^{\\top}$ 遵循了这种结构，而稀疏的 $\\ell_1$ 解 $\\mathbf{w}^{(1)}=(2a,0)^{\\top}$ 错误地丢弃了一个相关特征。当特征相关时（$\\rho > 0$），它们携带冗余信息。Lasso ($\\ell_1$) 会任意选择其中一个，这可能是一个不稳定的选择，并通过将一个真实的非零系数设为零而引入偏差。Ridge ($\\ell_2$) 通过其“分组效应”，将相关特征的系数一起收缩，从而得到一个更稳定的估计器，它将权重分布在相关组中，这在本例中更好地代表了真实的底层模型。\n\n### 第 3 步：逐项分析\n\n**A. 当特征呈正相关（$0 \\le \\rho  1$）且真实信号分布在两个输入上（$\\boldsymbol{\\theta} = (t, t)^{\\top}$，其中 $t \\neq 0$）时，对于任何 $a > 0$，权重衰减解 $\\mathbf{w}^{(2)} = (a, a)^{\\top}$ 获得的期望 MSE 严格低于稀疏解 $\\mathbf{w}^{(1)} = (2a, 0)^{\\top}$，而 $\\mathbf{w}^{(1)}$ 仍然更稀疏。发生这种情况是因为 $\\ell_{2}$ 将权重分配到相关的方向上，从而减少方差并稳定估计器，而 $\\ell_{1}$ 则任意地丢弃一个相关输入，增加了偏差和不稳定性。**\n\n-   **性能声明：** 正确。条件 $0 \\le \\rho  1$ 是我们推导出的条件 $\\rho  1$ 的一个子集，在此条件下 $\\text{MSE}(\\mathbf{w}^{(2)})  \\text{MSE}(\\mathbf{w}^{(1)})$。\n-   **稀疏性声明：** 正确。$\\mathbf{w}^{(1)}$ 有一个零分量，而 $\\mathbf{w}^{(2)}$ 没有。\n-   **解释：** 正确。该推理准确地描述了 $\\ell_2$ 正则化的“分组效应”，它通过在相关特征间分配权重来获得更好的稳定性和更低的误差，这与 $\\ell_1$ 倾向于任意选择一个特征的做法形成对比。这是对该现象的标准、公认的解释。\n\n结论：**正确**。\n\n**B. 对于任何 $0 \\le \\rho  1$ 的相关水平，$\\ell_{1}$ 正则化总是产生比 $\\ell_{2}$ 更低的期望 MSE，因为它移除了冗余特征；相关性不影响估计器的稳定性，并且稀疏性直接转化为更好的预测准确性。**\n\n-   **性能声明：** 不正确。我们的推导显示了相反的结果：对于 $0 \\le \\rho  1$，$\\ell_2$ 产生严格更低的期望 MSE。\n-   **解释：** 不正确。这个解释包含多个谬误。移除特征并不总是有益的，特别是当真实系数非零时。相关性深刻影响估计器的稳定性。稀疏性并不自动转化为更好的准确性，尤其是在真实模型不稀疏的情况下。\n\n结论：**不正确**。\n\n**C. 仅在特征完全相关（$\\rho = 1$）的情况下，权重衰减在预测性能上才优于 $\\ell_{1}$；否则，两种方法的期望 MSE 相等，因为它们正则化了相同的总大小 $\\|\\mathbf{w}\\|_{1} = 2a$。**\n\n-   **性能声明：** 不正确。我们的推导显示，当 $\\rho=1$ 时，MSE 相等。当 $\\rho  1$ 时，$\\ell_2$ 优于 $\\ell_1$。该选项颠倒了这一关系。\n-   **解释：** 不正确。$\\|\\mathbf{w}^{(1)}\\|_{1} = \\|\\mathbf{w}^{(2)}\\|_{1}$ 这一事实对于问题设置而言是偶然的，并不是性能差异的原因。性能由 MSE 计算决定，而不是由某个特定范数的相等性决定。\n\n结论：**不正确**。\n\n**D. 当特征呈强负相关（例如 $\\rho \\approx -0.9$）且真实信号分布在两个输入上（$\\boldsymbol{\\theta} = (t, t)^{\\top}$）时，稀疏解 $\\mathbf{w}^{(1)} = (2a, 0)^{\\top}$ 比权重衰减解 $\\mathbf{w}^{(2)} = (a, a)^{\\top}$ 更稳定，产生更低的期望 MSE，因为丢弃一个负相关的特征会减少有害方差；$\\ell_{2}$ 保留了两个输入，因此会过拟合。**\n\n-   **性能声明：** 不正确。条件 $\\rho \\approx -0.9$ 属于 $\\rho  1$ 的范围，我们的计算证明了在这种情况下 $\\ell_2$ 的 MSE 更低，而不是 $\\ell_1$。\n-   **解释：** 不正确。推理是有缺陷的。当特征呈负相关时，使用它们的和，如在 $\\ell_2$ 预测 $\\hat{y} = a(x_1+x_2)$ 中，实际上更稳定。和的方差是 $\\text{Var}(x_1+x_2) = \\text{Var}(x_1) + \\text{Var}(x_2) + 2\\text{Cov}(x_1,x_2) = 1+1+2\\rho = 2(1+\\rho)$。对于 $\\rho = -0.9$，此方差为 $2(0.1)=0.2$。基于 $x_1$ 的 $\\ell_1$ 预测的方差与 $\\text{Var}(x_1)=1$ 成正比。因此，保留两个负相关的特征会减少方差；在这种情况下它不会导致过拟合。\n\n结论：**不正确**。", "answer": "$$\\boxed{A}$$", "id": "3169449"}]}