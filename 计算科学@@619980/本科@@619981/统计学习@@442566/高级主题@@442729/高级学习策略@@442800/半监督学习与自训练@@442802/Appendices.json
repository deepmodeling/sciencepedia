{"hands_on_practices": [{"introduction": "自训练的成功关键取决于其伪标签的质量。虽然使用高置信度阈值可以过滤掉不确定的预测，但这引出了一个核心问题：阈值、基础分类器的性能和新增数据中的噪声水平之间精确的数学关系是怎样的？本练习将引导您完成一项基础推导，以量化伪标签中的预期噪声率 [@problem_id:3172749]。通过理解这种关系，您将能分析自训练中的核心权衡，并为设定置信度阈值做出更明智的决策。", "problem": "考虑一个二元分类问题，其标签为 $y \\in \\{0,1\\}$，类别先验概率为 $\\pi = \\Pr(y=1) \\in (0,1)$。一个固定的分类器产生预测 $\\hat{y} \\in \\{0,1\\}$，其混淆率为 $\\alpha = \\Pr(\\hat{y}=1 \\mid y=0)$ 和 $\\beta = \\Pr(\\hat{y}=0 \\mid y=1)$。该分类器配备了校准的置信度分数，因此对于任何输入 $x$，预测的最高类别概率被用作置信度 $c(x) \\in [0,1]$。在一个自训练过程中，未标记的输入由该分类器赋予伪标签，并且只有当其置信度超过一个固定的阈值 $\\tau \\in (0.5,1)$ 时，才被添加到训练集中。\n\n定义选择率 $s_{0}(\\tau) = \\Pr(c(X) \\ge \\tau \\mid y=0)$ 和 $s_{1}(\\tau) = \\Pr(c(X) \\ge \\tau \\mid y=1)$，并假设在每个真实类别内部，超过阈值 $\\tau$ 与分类器是否正确预测是独立的，因此混淆率 $\\alpha$ 和 $\\beta$ 统一适用于被选择的子集。令 $n_{L}$ 表示标记数据集的大小，$n_{U}$ 表示未标记池的大小。假设所有被选择的未标记输入都被添加，从而产生期望数量为 $m = n_{U}\\big((1-\\pi)s_{0}(\\tau) + \\pi s_{1}(\\tau)\\big)$ 的伪标签样本。\n\n使用全概率定律和经验0-1损失的定义，推导在阈值 $\\tau$ 下添加的伪标签中的期望噪声率的表达式，其定义为\n$$\n\\eta(\\tau) = \\Pr\\big(\\hat{y} \\neq y \\,\\big|\\, c(X) \\ge \\tau\\big),\n$$\n用 $\\alpha$、$\\beta$、$\\pi$、$s_{0}(\\tau)$ 和 $s_{1}(\\tau)$ 来表示。然后，在一个简化的假设下，即重新训练的分类器完美拟合添加的伪标签，分析当在增强数据集上通过经验风险最小化（ERM）进行重新训练时，这个期望噪声率如何影响经验0-1风险，并给出增强数据集上的期望经验0-1损失，作为 $n_{L}$、标记数据集上的经验损失 $R_{L}$、$m$ 和 $\\eta(\\tau)$ 的函数。\n\n请以 $\\eta(\\tau)$ 的闭式解析表达式的形式给出您的最终答案。无需四舍五入。", "solution": "该问题被评估为有效，因为它是自洽的，在统计学习理论中有科学依据，并且是良定的。我们可以开始推导。\n\n该问题要求进行两项推导。首先，是添加的伪标签中的期望噪声率 $\\eta(\\tau)$ 的表达式。其次，是对增强数据集上的期望经验0-1损失的分析。\n\n**第1部分：期望噪声率 $\\eta(\\tau)$ 的推导**\n\n添加的伪标签中的期望噪声率被定义为，在实例被选中进行伪标签化的条件下，伪标签不正确的概率。选择标准是置信度 $c(X)$ 超过阈值 $\\tau$。形式上，这表示为：\n$$\n\\eta(\\tau) = \\Pr\\big(\\hat{y} \\neq y \\,\\big|\\, c(X) \\ge \\tau\\big)\n$$\n我们使用条件概率的定义：\n$$\n\\eta(\\tau) = \\frac{\\Pr(\\hat{y} \\neq y, c(X) \\ge \\tau)}{\\Pr(c(X) \\ge \\tau)}\n$$\n我们将选择事件表示为 $S = \\{c(X) \\ge \\tau\\}$。表达式变为：\n$$\n\\eta(\\tau) = \\frac{\\Pr(\\hat{y} \\neq y, S)}{\\Pr(S)}\n$$\n我们将使用全概率定律，以真实类别标签 $y \\in \\{0, 1\\}$ 为条件，分别计算分子和分母。\n\n首先，我们计算分母 $\\Pr(S) = \\Pr(c(X) \\ge \\tau)$：\n$$\n\\Pr(S) = \\Pr(S \\mid y=0)\\Pr(y=0) + \\Pr(S \\mid y=1)\\Pr(y=1)\n$$\n根据问题陈述，我们已知：\n- 类别先验 $\\pi = \\Pr(y=1)$，这意味着 $\\Pr(y=0) = 1-\\pi$。\n- 类条件选择率 $s_{0}(\\tau) = \\Pr(c(X) \\ge \\tau \\mid y=0)$ 和 $s_{1}(\\tau) = \\Pr(c(X) \\ge \\tau \\mid y=1)$。\n\n将这些已知条件代入 $\\Pr(S)$ 的表达式中：\n$$\n\\Pr(S) = s_{0}(\\tau)(1-\\pi) + s_{1}(\\tau)\\pi\n$$\n\n接下来，我们计算分子 $\\Pr(\\hat{y} \\neq y, S)$。同样，我们使用全概率定律，以真实类别 $y$ 为条件：\n$$\n\\Pr(\\hat{y} \\neq y, S) = \\Pr(\\hat{y} \\neq y, S \\mid y=0)\\Pr(y=0) + \\Pr(\\hat{y} \\neq y, S \\mid y=1)\\Pr(y=1)\n$$\n让我们分析和中的每一项：\n1.  对于 $y=0$ 的情况，不正确的预测意味着 $\\hat{y}=1$。因此，第一项是 $\\Pr(\\hat{y} = 1, S \\mid y=0)\\Pr(y=0)$。\n2.  对于 $y=1$ 的情况，不正确的预测意味着 $\\hat{y}=0$。因此，第二项是 $\\Pr(\\hat{y} = 0, S \\mid y=1)\\Pr(y=1)$。\n\n问题陈述了一个关键的独立性假设：“在每个真实类别内部，超过阈值 $\\tau$ 与分类器是否正确预测是独立的”。\n这可以形式化为：\n- 给定 $y=0$，不正确预测的事件 $\\{\\hat{y}=1\\}$ 与选择事件 $S$ 是独立的。\n  $$ \\Pr(\\hat{y}=1, S \\mid y=0) = \\Pr(\\hat{y}=1 \\mid y=0) \\Pr(S \\mid y=0) $$\n- 给定 $y=1$，不正确预测的事件 $\\{\\hat{y}=0\\}$ 与选择事件 $S$ 是独立的。\n  $$ \\Pr(\\hat{y}=0, S \\mid y=1) = \\Pr(\\hat{y}=0 \\mid y=1) \\Pr(S \\mid y=1) $$\n\n我们已知混淆率 $\\alpha = \\Pr(\\hat{y}=1 \\mid y=0)$ 和 $\\beta = \\Pr(\\hat{y}=0 \\mid y=1)$。使用这些以及选择率，条件概率变为：\n- $\\Pr(\\hat{y}=1, S \\mid y=0) = \\alpha \\cdot s_{0}(\\tau)$\n- $\\Pr(\\hat{y}=0, S \\mid y=1) = \\beta \\cdot s_{1}(\\tau)$\n\n现在，我们可以写出分子的完整表达式：\n$$\n\\Pr(\\hat{y} \\neq y, S) = \\big( \\alpha \\cdot s_{0}(\\tau) \\big) (1-\\pi) + \\big( \\beta \\cdot s_{1}(\\tau) \\big) \\pi\n$$\n\n最后，我们将分子和分母结合起来，得到 $\\eta(\\tau)$ 的表达式：\n$$\n\\eta(\\tau) = \\frac{\\alpha s_{0}(\\tau)(1-\\pi) + \\beta s_{1}(\\tau)\\pi}{s_{0}(\\tau)(1-\\pi) + s_{1}(\\tau)\\pi}\n$$\n\n**第2部分：期望经验风险的分析**\n\n问题的第二部分要求计算增强数据集上的期望经验0-1损失。增强数据集 $D_{aug}$ 由大小为 $n_L$ 的原始标记集 $D_L$ 和大小为 $m$ 的伪标签样本集 $D_{pseudo}$ 组成。总大小为 $n_L + m$。\n\n为了量化噪声的影响，对于重新训练的分类器 $h_{new}$，这个增强集上的经验0-1损失必须相对于*真实*标签进行评估。令 $y_i$ 为实例 $x_i$ 的真实标签。真实的经验损失是：\n$$\nR_{D_{aug}}^{true}(h_{new}) = \\frac{1}{n_L + m} \\left( \\sum_{(x_i, y_i) \\in D_L} \\mathbb{I}(h_{new}(x_i) \\neq y_i) + \\sum_{(x_j, y_j) \\in D_{pseudo}} \\mathbb{I}(h_{new}(x_j) \\neq y_j) \\right)\n$$\n问题指明，重新训练的分类器“完美拟合添加的伪标签”。这意味着对于任何伪标签样本 $(x_j, \\hat{y}_j) \\in D_{pseudo}$，我们有 $h_{new}(x_j) = \\hat{y}_j$。将此代入第二个和式中：\n$$\nR_{D_{aug}}^{true}(h_{new}) = \\frac{1}{n_L + m} \\left( \\sum_{(x_i, y_i) \\in D_L} \\mathbb{I}(h_{new}(x_i) \\neq y_i) + \\sum_{(x_j, y_j) \\in D_{pseudo}} \\mathbb{I}(\\hat{y}_j \\neq y_j) \\right)\n$$\n第一个和式是原始标记集上的总损失，即 $n_L R_L$，其中 $R_L$ 是 $h_{new}$ 在标记数据集上的经验损失。表达式变为：\n$$\nR_{D_{aug}}^{true}(h_{new}) = \\frac{n_L R_L + \\sum_{j \\in D_{pseudo}} \\mathbb{I}(\\hat{y}_j \\neq y_j)}{n_L + m}\n$$\n我们需要找到*期望*经验损失。期望是针对未标记数据的随机选择来计算的。\n$$\nE[R_{D_{aug}}^{true}(h_{new})] = \\frac{1}{n_L+m} E\\left[ n_L R_L + \\sum_{j \\in D_{pseudo}} \\mathbb{I}(\\hat{y}_j \\neq y_j) \\right]\n$$\n假设 $R_L$ 是该过程的一个固定结果，并利用期望的线性性：\n$$\nE[R_{D_{aug}}^{true}(h_{new})] = \\frac{n_L R_L + E\\left[\\sum_{j \\in D_{pseudo}} \\mathbb{I}(\\hat{y}_j \\neq y_j)\\right]}{n_L+m}\n$$\n这个和式是针对 $m$ 个被选择的伪标签样本。对于每个这样的样本（索引为 $j$），项 $\\mathbb{I}(\\hat{y}_j \\neq y_j)$ 是一个伯努利随机变量。其期望是其伪标签不正确的概率。根据定义，这就是在样本被选中的条件下，预测不正确的概率，这恰好是 $\\eta(\\tau)$。\n$$\nE[\\mathbb{I}(\\hat{y}_j \\neq y_j)] = \\Pr(\\hat{y} \\neq y \\mid c(X) \\ge \\tau) = \\eta(\\tau)\n$$\n由于这对 $m$ 个独立同分布的被选样本中的每一个都成立，所以和的期望是：\n$$\nE\\left[\\sum_{j \\in D_{pseudo}} \\mathbb{I}(\\hat{y}_j \\neq y_j)\\right] = \\sum_{j=1}^{m} E[\\mathbb{I}(\\hat{y}_j \\neq y_j)] = \\sum_{j=1}^{m} \\eta(\\tau) = m \\eta(\\tau)\n$$\n将此代回，增强数据集上的期望经验0-1损失是：\n$$\nE[R_{D_{aug}}^{true}(h_{new})] = \\frac{n_L R_L + m \\eta(\\tau)}{n_L + m}\n$$\n这个表达式表明，最终的有效损失是原始标记数据上的损失 $R_L$ 和伪标签的噪声率 $\\eta(\\tau)$ 的加权平均值。\n\n然而，问题只要求在最终答案中给出 $\\eta(\\tau)$ 的表达式。\n$$\n\\eta(\\tau) = \\frac{\\alpha s_0(\\tau)(1-\\pi) + \\beta s_1(\\tau)\\pi}{(1-\\pi)s_0(\\tau) + \\pi s_1(\\tau)}\n$$\n这也可以用更紧凑的形式写出，但当前的形式清晰且是直接推导出来的。\n对最终答案的表达式进行最后检查：\n$\\eta(\\tau) = \\frac{\\Pr(\\text{错误}, \\text{选中})}{\\Pr(\\text{选中})} = \\frac{\\Pr(\\text{错误}|y=0)\\Pr(y=0)\\Pr(\\text{选中}|y=0) + \\Pr(\\text{错误}|y=1)\\Pr(y=1)\\Pr(\\text{选中}|y=1)}{\\Pr(\\text{选中}|y=0)\\Pr(y=0) + \\Pr(\\text{选中}|y=1)\\Pr(y=1)}$。\n不，这是错的。独立性是基于 $y$ 的条件。所以 $\\Pr(\\text{错误}, \\text{选中} | y=0) = \\Pr(\\text{错误}|y=0)\\Pr(\\text{选中}|y=0)$。\n$\\Pr(\\text{错误}, \\text{选中}) = \\Pr(\\text{错误}, \\text{选中} | y=0) \\Pr(y=0) + \\Pr(\\text{错误}, \\text{选中} | y=1) \\Pr(y=1) = \\Pr(\\text{错误} | y=0) \\Pr(\\text{选中} | y=0) \\Pr(y=0) + \\Pr(\\text{错误} | y=1) \\Pr(\\text{选中} | y=1) \\Pr(y=1)$。\n$\\Pr(\\text{错误}|y=0) = \\Pr(\\hat{y}=1|y=0) = \\alpha$。\n$\\Pr(\\text{错误}|y=1) = \\Pr(\\hat{y}=0|y=1) = \\beta$。\n所以分子是 $\\alpha s_0(\\tau) (1-\\pi) + \\beta s_1(\\tau) \\pi$。\n分母是 $s_0(\\tau)(1-\\pi) + s_1(\\tau)\\pi$。\n推导是正确的。\n最终答案是 $\\eta(\\tau)$ 的表达式。", "answer": "$$\n\\boxed{\\frac{\\alpha s_{0}(\\tau)(1-\\pi) + \\beta s_{1}(\\tau)\\pi}{s_{0}(\\tau)(1-\\pi) + s_{1}(\\tau)\\pi}}\n$$", "id": "3172749"}, {"introduction": "在现实世界的应用中，我们初始的标注数据集本身往往是不完美的，可能包含一定比例的噪声标签。这就带来了一个关键问题：自训练是能够帮助纠正这些初始错误，还是会陷入一种被称为“错误放大”的恶性循环中，从而使情况变得更糟？本练习旨在探讨这一关键动态 [@problem_id:3172790]。您将推导出在何种条件下，自训练能够扮演“去噪”的角色，提升整体标签质量；又在何种条件下，它会因为不断传播自身的错误而导致模型性能下降。", "problem": "考虑一个二分类问题，其特征为 $X \\in \\mathcal{X}$，干净标签为 $Y \\in \\{0,1\\}$。您观察到一个带标签的样本，其中每个干净标签都以对称标签噪声率 $\\rho \\in (0, \\tfrac{1}{2})$ 独立翻转：对每个样本，观测到的标签等于 $1-Y$ 的概率为 $\\rho$，等于 $Y$ 的概率为 $1-\\rho$。您还有一个从相同的 $X$ 边缘分布中抽取的无标签样本。\n\n一个概率分类器在带噪声的带标签样本上被训练至最优，即对于任何 $x$，它都返回受污染的后验概率 $g(x) = \\mathbb{P}(\\tilde{Y}=1 \\mid X=x)$，其中 $\\tilde{Y}$ 表示观测到的噪声标签。定义干净后验概率为 $\\eta(x) = \\mathbb{P}(Y=1 \\mid X=x)$。\n\n进行一轮自训练（ST），过程如下。在无标签样本上，根据 $g(x)$ 的贝叶斯决策分配伪标签 $\\hat{Y}(x) \\in \\{0,1\\}$，即 $\\hat{Y}(x) = \\mathbf{1}\\{g(x) \\geq \\tfrac{1}{2}\\}$。只接受那些置信度边际超过固定阈值 $\\gamma \\in [0, \\tfrac{1}{2})$ 的伪标签，这意味着接受集是 $\\{x: |g(x) - \\tfrac{1}{2}| \\geq \\gamma\\}$。将原始的带噪声的带标签集合与被接受的带伪标签的样本结合起来，并将结合后的集合的有效标签噪声率视为相对于干净标签 $Y$ 的错误标签比例。\n\n从对称标签噪声和条件概率的定义出发，推导 $g(x)$ 和 $\\eta(x)$ 之间的精确仿射关系。然后，仅使用此关系和接受规则 $|g(x) - \\tfrac{1}{2}| \\geq \\gamma$，计算被接受样本中伪标签错误率的一个与分布无关的最坏情况上界。利用此上界，确定最小置信度边际 $\\gamma_{\\star}(\\rho)$，使得对于任何 $\\gamma \\geq \\gamma_{\\star}(\\rho)$ 且 $\\gamma  \\tfrac{1}{2} - \\rho$，一轮 ST 后的有效标签噪声率严格低于原始噪声率 $\\rho$，而与 $X$ 的分布无关。\n\n您的最终答案应为 $\\gamma_{\\star}(\\rho)$ 关于 $\\rho$ 的一个单一闭式表达式。最终答案中不要包含任何不等式。", "solution": "首先对问题进行验证，以确保其具有科学依据、是良构的且客观的。\n\n### 步骤 1：提取已知条件\n-   **问题领域**：二分类。\n-   **特征与标签**：$X \\in \\mathcal{X}$，干净标签 $Y \\in \\{0, 1\\}$。\n-   **噪声模型**：对称标签噪声，噪声率为 $\\rho \\in (0, \\frac{1}{2})$。观测到的噪声标签为 $\\tilde{Y}$。对每个样本，$\\mathbb{P}(\\tilde{Y} = 1-Y) = \\rho$ 且 $\\mathbb{P}(\\tilde{Y} = Y) = 1-\\rho$，其中噪声与 $X$ 无关。\n-   **数据**：一个带噪声标签 $\\tilde{Y}$ 的有标签样本，以及一个从相同 $X$ 边缘分布中抽取的无标签样本。\n-   **分类器**：一个在噪声数据上训练至最优的概率分类器，得到受污染的后验概率 $g(x) = \\mathbb{P}(\\tilde{Y}=1 \\mid X=x)$。\n-   **干净后验概率**：$\\eta(x) = \\mathbb{P}(Y=1 \\mid X=x)$。\n-   **自训练 (ST) 规则**：\n    -   伪标签分配：$\\hat{Y}(x) = \\mathbf{1}\\{g(x) \\geq \\frac{1}{2}\\}$。\n    -   接受标准：如果 $|g(x) - \\frac{1}{2}| \\geq \\gamma$，则接受样本 $x$，其中 $\\gamma \\in [0, \\frac{1}{2})$ 是一个固定阈值。\n-   **目标**：\n    1.  推导 $g(x)$ 和 $\\eta(x)$ 之间的关系。\n    2.  计算被接受样本的伪标签错误率的一个与分布无关的最坏情况上界。\n    3.  找到最小置信度边际 $\\gamma_{\\star}(\\rho)$，使得对于任何 $\\gamma \\geq \\gamma_{\\star}(\\rho)$ 且 $\\gamma  \\frac{1}{2} - \\rho$，组合数据集（原始噪声集 + 被接受的伪标签集）的有效标签噪声率严格低于 $\\rho$。\n\n### 步骤 2：使用提取的已知条件进行验证\n问题陈述在科学上是合理的，它基于统计学习的既定原则，特别是带标签噪声学习和半监督学习。后验概率、对称噪声和自训练等概念是该领域的标准概念。问题是良构的，所有术语和条件都得到了精确定义，最终要求一个具体的、可推导的量。语言是客观和正式的。整个设定是自洽的，没有矛盾。约束条件 $\\rho \\in (0, \\frac{1}{2})$ 是标准的，确保了标签不是完全随机的，而 $\\gamma  \\frac{1}{2} - \\rho$ 是一个一致的约束，其意义在推导过程中会变得清晰。该问题是非平凡的，需要进行严格的数学推导。\n\n### 步骤 3：结论与行动\n问题是 **有效的**。将提供完整解答。\n\n### 推导过程\n解答过程按问题陈述的要求分三个阶段进行。\n\n**1. $g(x)$ 和 $\\eta(x)$ 之间的关系**\n\n受污染的后验概率 $g(x)$ 定义为 $g(x) = \\mathbb{P}(\\tilde{Y}=1 \\mid X=x)$。使用全概率公式，我们可以通过以干净标签 $Y$ 为条件来展开此式：\n$$g(x) = \\mathbb{P}(\\tilde{Y}=1 \\mid Y=1, X=x)\\mathbb{P}(Y=1 \\mid X=x) + \\mathbb{P}(\\tilde{Y}=1 \\mid Y=0, X=x)\\mathbb{P}(Y=0 \\mid X=x)$$\n对称噪声模型表明，标签翻转概率与特征 $X$ 无关。因此，$\\mathbb{P}(\\tilde{Y}=1 \\mid Y=1, X=x) = \\mathbb{P}(\\tilde{Y}=1 \\mid Y=1) = 1-\\rho$ 且 $\\mathbb{P}(\\tilde{Y}=1 \\mid Y=0, X=x) = \\mathbb{P}(\\tilde{Y}=1 \\mid Y=0) = \\rho$。干净后验概率由 $\\eta(x) = \\mathbb{P}(Y=1 \\mid X=x)$ 和 $1-\\eta(x) = \\mathbb{P}(Y=0 \\mid X=x)$ 给出。\n将这些代入 $g(x)$ 的方程中：\n$$g(x) = (1-\\rho)\\eta(x) + \\rho(1-\\eta(x))$$\n$$g(x) = (1-\\rho)\\eta(x) + \\rho - \\rho\\eta(x)$$\n$$g(x) = (1-2\\rho)\\eta(x) + \\rho$$\n这就是受污染的后验概率 $g(x)$ 和干净的后验概率 $\\eta(x)$ 之间的仿射关系。由于 $\\rho \\in (0, \\frac{1}{2})$，项 $1-2\\rho$ 非零，我们可以反转这个关系，用 $g(x)$ 来表示 $\\eta(x)$：\n$$\\eta(x) = \\frac{g(x) - \\rho}{1-2\\rho}$$\n\n**2. 最坏情况下的伪标签错误率**\n\n伪标签 $\\hat{Y}(x)$ 是基于受污染后验概率的贝叶斯决策规则分配的：$\\hat{Y}(x) = \\mathbf{1}\\{g(x) \\geq \\frac{1}{2}\\}$。如果一个样本的置信度边际 $|g(x) - \\frac{1}{2}|$ 至少为 $\\gamma$，它就会被接受用于自训练。这个接受条件将接受集划分为两个不相交的子集：\n-   集合 1：$\\{x \\mid g(x) \\geq \\frac{1}{2} + \\gamma\\}$。对于这些样本，伪标签为 $\\hat{Y}(x) = 1$。\n-   集合 2：$\\{x \\mid g(x) \\leq \\frac{1}{2} - \\gamma\\}$。对于这些样本，伪标签为 $\\hat{Y}(x) = 0$。\n\n对于给定的 $x$，伪标签错误率是伪标签 $\\hat{Y}(x)$ 与真实干净标签 $Y$ 不匹配的概率，即 $\\mathbb{P}(\\hat{Y}(x) \\neq Y \\mid X=x)$。让我们为任何被接受的样本找到此错误率的一个上界。\n\n对于集合 1 中的一个样本（$g(x) \\geq \\frac{1}{2} + \\gamma$ 且 $\\hat{Y}(x)=1$），如果真实标签是 $Y=0$，则会发生错误。其概率为：\n$$\\mathbb{P}(Y=0 \\mid X=x) = 1 - \\eta(x) = 1 - \\frac{g(x) - \\rho}{1-2\\rho} = \\frac{(1-2\\rho) - (g(x) - \\rho)}{1-2\\rho} = \\frac{1 - \\rho - g(x)}{1-2\\rho}$$\n由于 $1-2\\rho  0$，这个错误概率是 $g(x)$ 的一个递减函数。为了找到一个最坏情况（最大）上界，我们必须使用该集合中 $g(x)$ 的最小值，即 $g(x) = \\frac{1}{2} + \\gamma$。\n$$\\text{错误率 (集合 1)} \\leq \\frac{1 - \\rho - (\\frac{1}{2} + \\gamma)}{1-2\\rho} = \\frac{\\frac{1}{2} - \\rho - \\gamma}{1-2\\rho}$$\n\n对于集合 2 中的一个样本（$g(x) \\leq \\frac{1}{2} - \\gamma$ 且 $\\hat{Y}(x)=0$），如果真实标签是 $Y=1$，则会发生错误。其概率为：\n$$\\mathbb{P}(Y=1 \\mid X=x) = \\eta(x) = \\frac{g(x) - \\rho}{1-2\\rho}$$\n由于 $1-2\\rho  0$，这个错误概率是 $g(x)$ 的一个递增函数。为了找到一个最坏情况上界，我们必须使用该集合中 $g(x)$ 的最大值，即 $g(x) = \\frac{1}{2} - \\gamma$。\n$$\\text{错误率 (集合 2)} \\leq \\frac{(\\frac{1}{2} - \\gamma) - \\rho}{1-2\\rho} = \\frac{\\frac{1}{2} - \\rho - \\gamma}{1-2\\rho}$$\n两种情况都得出了相同的最坏情况错误上界。只要样本满足接受标准，这个上界就与 $x$ 和底层数据分布无关。令 $\\rho_{\\text{ST}}$ 表示接受集上的伪标签错误率。我们已经建立了一个与分布无关的上界：\n$$\\rho_{\\text{ST}} \\leq \\frac{\\frac{1}{2} - \\rho - \\gamma}{1-2\\rho}$$\n\n**3. 最小置信度边际 $\\gamma_{\\star}(\\rho)$**\n\n问题要求找到一个条件，以确保组合数据集的有效标签噪声率严格低于原始噪声率 $\\rho$。设原始的带噪声的带标签集合为 $S_L$，大小为 $n_L$；被接受的带伪标签样本的集合为 $S_A$，大小为 $n_A$。$S_L$ 中的错误数量约为 $n_L \\rho$。$S_A$ 中的错误数量为 $n_A \\rho_{\\text{ST}}$。\n有效噪声率 $\\rho_{\\text{eff}}$ 是总错误数除以组合数据集的总大小：\n$$\\rho_{\\text{eff}} = \\frac{n_L \\rho + n_A \\rho_{\\text{ST}}}{n_L + n_A}$$\n我们要求 $\\rho_{\\text{eff}}  \\rho$：\n$$\\frac{n_L \\rho + n_A \\rho_{\\text{ST}}}{n_L + n_A}  \\rho$$\n$$n_L \\rho + n_A \\rho_{\\text{ST}}  n_L \\rho + n_A \\rho$$\n假设 $n_A > 0$（即自训练添加了一些样本），这个不等式简化为：\n$$\\rho_{\\text{ST}}  \\rho$$\n为了保证无论数据分布如何，这个条件都成立，我们必须强制要求 $\\rho_{\\text{ST}}$ 的最坏情况上界严格小于 $\\rho$：\n$$\\frac{\\frac{1}{2} - \\rho - \\gamma}{1-2\\rho}  \\rho$$\n由于 $\\rho \\in (0, \\frac{1}{2})$，我们有 $1-2\\rho > 0$。我们可以在不等式两边同乘以 $1-2\\rho$ 而不改变不等号的方向：\n$$\\frac{1}{2} - \\rho - \\gamma  \\rho(1-2\\rho)$$\n$$\\frac{1}{2} - \\rho - \\gamma  \\rho - 2\\rho^2$$\n现在，我们求解 $\\gamma$：\n$$\\frac{1}{2} - 2\\rho + 2\\rho^2  \\gamma$$\n这个不等式指明了 $\\gamma$ 为保证噪声率降低所必须满足的条件。问题要求的是最小置信度边际 $\\gamma_{\\star}(\\rho)$，使得对于任何 $\\gamma \\geq \\gamma_{\\star}(\\rho)$，该条件都成立。这个最小值就是为 $\\gamma$ 推导出的区间的下界。\n因此，最小置信度边际是：\n$$\\gamma_{\\star}(\\rho) = \\frac{1}{2} - 2\\rho + 2\\rho^2$$\n这也可以写成 $\\gamma_{\\star}(\\rho) = 2(\\rho - \\frac{1}{2})^2$。问题约束 $\\gamma  \\frac{1}{2} - \\rho$ 确保了 $\\gamma_{\\star}(\\rho)$ 是一个有效的选择，因为对于 $\\rho \\in (0, \\frac{1}{2})$，有 $2(\\rho - \\frac{1}{2})^2  \\frac{1}{2} - \\rho$。", "answer": "$$\\boxed{\\frac{1}{2} - 2\\rho + 2\\rho^2}$$", "id": "3172790"}, {"introduction": "自训练面临的一个重大风险是，在未标注数据池中可能混杂着来自“分布外”（Out-of-Distribution, OOD）的样本——即那些类别从未在初始标注数据中出现过的例子。模型对这类数据做出高置信度的错误预测，会严重污染训练过程。本练习将通过要求您设计一个基于模型不确定性的安全机制来应对这一挑战 [@problem_id:3172796]。您将运用贝叶斯决策理论，为一个基于熵的异常检测器确定一个最佳阈值，以有效过滤掉这些OOD输入，从而最大程度地减少因错误标注未知类别而造成的损害。", "problem": "一个带有参数 $\\theta$ 的分类器，通过对其 logits 应用 softmax 变换，为每个输入 $x$ 生成一个关于 $K$ 个已见类的分类分布 $p_{\\theta}(y \\mid x)$。在采用自训练的半监督学习中，当模型足够自信时，未标记的输入 $\\{x_{i}\\}$ 会被赋予伪标签，否则将被搁置。然而，未标记样本池中也包含来自未见类（分布外，OOD）的输入，必须防止这些输入被赋予伪标签，以避免误差累积。\n\n考虑一个异常检测器，它对香农熵 $H(p_{\\theta}(x)) = -\\sum_{j=1}^{K} p_{\\theta,j}(x) \\ln p_{\\theta,j}(x)$ 设置阈值：如果 $H(p_{\\theta}(x)) \\leq \\tau$，则声明输入 $x$ 为分布内（有资格获得伪标签），否则声明其为异常（拒绝）。当 softmax 的 logits $z_{j}(x)$ 校准良好时，该检测器在本质上等同于对能量分数 $E(x) = -\\ln\\left(\\sum_{j=1}^{K} \\exp(z_{j}(x))\\right)$ 设置阈值，因为更高的不确定性（更平坦的 softmax 分布）会同时增加熵和能量。\n\n假设在两种情况下，熵的科学合理模型如下：\n- 对于来自已见类的输入，$H \\mid \\text{seen} \\sim \\mathcal{N}(\\mu_{s}, \\sigma^{2})$，其中 $\\mu_{s} = 0.4$ 且 $\\sigma = 0.25$（奈特）。\n- 对于来自未见类的输入，$H \\mid \\text{unseen} \\sim \\mathcal{N}(\\mu_{u}, \\sigma^{2})$，其中 $\\mu_{u} = 1.3$ 且 $\\sigma = 0.25$（奈特）。\n\n设未标记样本池中已见类输入的先验概率为 $\\pi_{s} = 0.85$，未见类输入的先验概率为 $\\pi_{u} = 0.15$。假设错误接受（将未见类声明为已见类，导致错误标记）的成本为 $C_{\\text{FA}} = 4$，而错误拒绝（将已见类声明为未见类，从而放弃一个有用的伪标签）的成本为 $C_{\\text{FR}} = 1$。\n\n从贝叶斯决策理论的原理和上述定义出发，推导出贝叶斯最优阈值 $\\tau^{\\star}$，该阈值在此熵阈值规则下最小化检测权衡的期望成本。然后使用给定参数计算 $\\tau^{\\star}$ 的数值。将最终数值答案四舍五入到四位有效数字，并以奈特为单位表示。", "solution": "问题要求解出贝叶斯最优决策阈值 $\\tau^{\\star}$，用于根据模型预测的香non熵 $H$ 将输入分类为属于“已见”类或“未见”类。最优阈值是使与分类错误相关的总期望成本（贝叶斯风险）最小化的阈值。\n\n决策规则定义如下：\n- 如果观测到的熵 $h \\leq \\tau$，则声明为“已见”。\n- 如果观测到的熵 $h  \\tau$，则声明为“未见”。\n\n两类错误及其相关成本是：\n1.  **错误接受 (FA)**：将“未见”输入声明为“已见”。当真实类别为“未见”但 $h \\leq \\tau$ 时发生。成本为 $C_{\\text{FA}} = 4$。\n2.  **错误拒绝 (FR)**：将“已见”输入声明为“未见”。当真实类别为“已见”但 $h  \\tau$ 时发生。成本为 $C_{\\text{FR}} = 1$。\n\n各类别的先验概率为 $\\pi_{s} = P(\\text{seen}) = 0.85$ 和 $\\pi_{u} = P(\\text{unseen}) = 0.15$。\n\n每个类别下熵 $H$ 的条件分布为正态分布：\n- 对于“已见”输入：$p(h|\\text{seen}) = p_s(h) = \\mathcal{N}(h; \\mu_s, \\sigma^2)$，其中 $\\mu_s = 0.4$ 且 $\\sigma = 0.25$。\n- 对于“未见”输入：$p(h|\\text{unseen}) = p_u(h) = \\mathcal{N}(h; \\mu_u, \\sigma^2)$，其中 $\\mu_u = 1.3$ 且 $\\sigma = 0.25$。\n\n根据贝叶斯决策理论，我们必须最小化期望成本，即风险 $R(\\tau)$。风险是每种类型错误的成本与其发生概率的乘积之和。\n\n错误接受的概率是输入为“未见”且被声明为“已见”的联合概率：\n$$ P(\\text{FA}) = P(h \\leq \\tau, \\text{unseen}) = P(h \\leq \\tau | \\text{unseen}) P(\\text{unseen}) = \\pi_u \\int_{-\\infty}^{\\tau} p_u(h) \\, dh $$\n错误拒绝的概率是输入为“已见”且被声明为“未见”的联合概率：\n$$ P(\\text{FR}) = P(h  \\tau, \\text{seen}) = P(h  \\tau | \\text{seen}) P(\\text{seen}) = \\pi_s \\int_{\\tau}^{\\infty} p_s(h) \\, dh $$\n\n总期望成本 $R(\\tau)$ 是成本与其概率乘积之和：\n$$ R(\\tau) = C_{\\text{FA}} P(\\text{FA}) + C_{\\text{FR}} P(\\text{FR}) $$\n$$ R(\\tau) = C_{\\text{FA}} \\pi_u \\int_{-\\infty}^{\\tau} p_u(h) \\, dh + C_{\\text{FR}} \\pi_s \\int_{\\tau}^{\\infty} p_s(h) \\, dh $$\n\n为了找到最小化 $R(\\tau)$ 的最优阈值 $\\tau^{\\star}$，我们将 $R(\\tau)$ 对 $\\tau$ 求导，并令导数等于零。使用微积分基本定理（特别是莱布尼茨积分法则）：\n$$ \\frac{d}{d\\tau} \\int_{-\\infty}^{\\tau} f(x) \\, dx = f(\\tau) $$\n$$ \\frac{d}{d\\tau} \\int_{\\tau}^{\\infty} f(x) \\, dx = -f(\\tau) $$\n\n将此应用于风险函数 $R(\\tau)$：\n$$ \\frac{dR(\\tau)}{d\\tau} = C_{\\text{FA}} \\pi_u p_u(\\tau) + C_{\\text{FR}} \\pi_s (-p_s(\\tau)) $$\n令导数等于零以找到最小值：\n$$ C_{\\text{FA}} \\pi_u p_u(\\tau^{\\star}) - C_{\\text{FR}} \\pi_s p_s(\\tau^{\\star}) = 0 $$\n$$ C_{\\text{FA}} \\pi_u p_u(\\tau^{\\star}) = C_{\\text{FR}} \\pi_s p_s(\\tau^{\\star}) $$\n这个方程定义了贝叶斯最优阈值 $\\tau^{\\star}$。它表示在决策边界上，两个类别的加权似然相等，其中权重是成本和先验概率的乘积。\n\n现在，我们代入正态分布的概率密度函数 (PDF)：\n$$ p_s(h) = \\frac{1}{\\sqrt{2\\pi}\\sigma} \\exp\\left(-\\frac{(h - \\mu_s)^2}{2\\sigma^2}\\right) $$\n$$ p_u(h) = \\frac{1}{\\sqrt{2\\pi}\\sigma} \\exp\\left(-\\frac{(h - \\mu_u)^2}{2\\sigma^2}\\right) $$\n将这些代入 $\\tau = \\tau^{\\star}$ 的最优性条件：\n$$ C_{\\text{FA}} \\pi_u \\frac{1}{\\sqrt{2\\pi}\\sigma} \\exp\\left(-\\frac{(\\tau - \\mu_u)^2}{2\\sigma^2}\\right) = C_{\\text{FR}} \\pi_s \\frac{1}{\\sqrt{2\\pi}\\sigma} \\exp\\left(-\\frac{(\\tau - \\mu_s)^2}{2\\sigma^2}\\right) $$\n归一化常数 $\\frac{1}{\\sqrt{2\\pi}\\sigma}$ 从两侧消去：\n$$ C_{\\text{FA}} \\pi_u \\exp\\left(-\\frac{(\\tau - \\mu_u)^2}{2\\sigma^2}\\right) = C_{\\text{FR}} \\pi_s \\exp\\left(-\\frac{(\\tau - \\mu_s)^2}{2\\sigma^2}\\right) $$\n为了求解 $\\tau$，我们对两边取自然对数：\n$$ \\ln(C_{\\text{FA}} \\pi_u) - \\frac{(\\tau - \\mu_u)^2}{2\\sigma^2} = \\ln(C_{\\text{FR}} \\pi_s) - \\frac{(\\tau - \\mu_s)^2}{2\\sigma^2} $$\n重新整理各项以分离出 $\\tau$：\n$$ \\frac{(\\tau - \\mu_s)^2}{2\\sigma^2} - \\frac{(\\tau - \\mu_u)^2}{2\\sigma^2} = \\ln(C_{\\text{FR}} \\pi_s) - \\ln(C_{\\text{FA}} \\pi_u) $$\n乘以 $2\\sigma^2$ 并展开平方项：\n$$ (\\tau^2 - 2\\tau\\mu_s + \\mu_s^2) - (\\tau^2 - 2\\tau\\mu_u + \\mu_u^2) = 2\\sigma^2 \\ln\\left(\\frac{C_{\\text{FR}} \\pi_s}{C_{\\text{FA}} \\pi_u}\\right) $$\n$\\tau^2$ 项被消掉：\n$$ 2\\tau\\mu_u - 2\\tau\\mu_s + \\mu_s^2 - \\mu_u^2 = 2\\sigma^2 \\ln\\left(\\frac{C_{\\text{FR}} \\pi_s}{C_{\\text{FA}} \\pi_u}\\right) $$\n$$ 2\\tau(\\mu_u - \\mu_s) = \\mu_u^2 - \\mu_s^2 + 2\\sigma^2 \\ln\\left(\\frac{C_{\\text{FR}} \\pi_s}{C_{\\text{FA}} \\pi_u}\\right) $$\n因式分解 $\\mu_u^2 - \\mu_s^2 = (\\mu_u - \\mu_s)(\\mu_u + \\mu_s)$:\n$$ 2\\tau(\\mu_u - \\mu_s) = (\\mu_u - \\mu_s)(\\mu_u + \\mu_s) + 2\\sigma^2 \\ln\\left(\\frac{C_{\\text{FR}} \\pi_s}{C_{\\text{FA}} \\pi_u}\\right) $$\n由于 $\\mu_u \\neq \\mu_s$，我们可以除以 $2(\\mu_u - \\mu_s)$:\n$$ \\tau = \\frac{\\mu_u + \\mu_s}{2} + \\frac{\\sigma^2}{\\mu_u - \\mu_s} \\ln\\left(\\frac{C_{\\text{FR}} \\pi_s}{C_{\\text{FA}} \\pi_u}\\right) $$\n这就是贝叶斯最优阈值 $\\tau^{\\star}$ 的解析表达式。\n\n现在，我们代入给定的数值：\n$\\mu_{s} = 0.4$、$\\mu_{u} = 1.3$、$\\sigma = 0.25$、$\\pi_{s} = 0.85$、$\\pi_{u} = 0.15$、 $C_{\\text{FR}} = 1$、 $C_{\\text{FA}} = 4$。\n\n第一项是均值的中点：\n$$ \\frac{\\mu_u + \\mu_s}{2} = \\frac{1.3 + 0.4}{2} = \\frac{1.7}{2} = 0.85 $$\n对数的参数是：\n$$ \\frac{C_{\\text{FR}} \\pi_s}{C_{\\text{FA}} \\pi_u} = \\frac{1 \\times 0.85}{4 \\times 0.15} = \\frac{0.85}{0.60} = \\frac{17}{12} $$\n剩下的项是：\n$$ \\mu_u - \\mu_s = 1.3 - 0.4 = 0.9 $$\n$$ \\sigma^2 = (0.25)^2 = 0.0625 $$\n将这些代入 $\\tau^{\\star}$ 的表达式中：\n$$ \\tau^{\\star} = 0.85 + \\frac{0.0625}{0.9} \\ln\\left(\\frac{17}{12}\\right) $$\n现在我们计算数值：\n$$ \\ln\\left(\\frac{17}{12}\\right) \\approx \\ln(1.41666...) \\approx 0.3483203 $$\n$$ \\frac{0.0625}{0.9} = \\frac{625 \\times 10^{-4}}{9 \\times 10^{-1}} = \\frac{625}{9000} = \\frac{5}{72} \\approx 0.0694444 $$\n结合这些：\n$$ \\tau^{\\star} \\approx 0.85 + (0.0694444) \\times (0.3483203) $$\n$$ \\tau^{\\star} \\approx 0.85 + 0.0241889 $$\n$$ \\tau^{\\star} \\approx 0.8741889 $$\n将结果四舍五入到四位有效数字，得到 $0.8742$。\n单位是奈特，因为熵是使用自然对数计算的。", "answer": "$$\n\\boxed{0.8742}\n$$", "id": "3172796"}]}