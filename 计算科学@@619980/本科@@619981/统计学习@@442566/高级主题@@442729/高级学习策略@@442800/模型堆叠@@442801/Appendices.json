{"hands_on_practices": [{"introduction": "元学习器（meta-learner）的目标函数选择对堆叠模型的最终性能至关重要。本实践将指导您为一个二元分类任务实现堆叠模型，并直接比较两种严格正常评分规则（proper scoring rules）——布里尔分数（Brier score）和对数损失（Logarithmic loss）——所产生的最优权重。通过这个练习，您将亲身体验目标函数如何塑造最终的集成模型。[@problem_id:3175480]", "problem": "您将处理一个二元概率分类的堆叠（stacking）任务。现有 $M$ 个基础概率分类器，它们为 $N$ 个带标签的样本生成预测概率。一个堆叠预测器构成了基础预测的凸组合。令 $y_i \\in \\{0,1\\}$ 表示样本 $i \\in \\{1,\\dots,N\\}$ 的真实标签，令 $p_{ij} \\in [0,1]$ 表示基础模型 $j \\in \\{1,\\dots,M\\}$ 对样本 $i$ 的预测概率。令 $w = (w_1,\\dots,w_M)$ 为堆叠权重。样本 $i$ 的混合预测概率定义为\n$$\n\\hat{p}_i(w) = \\sum_{j=1}^M w_j\\, p_{ij},\n$$\n并满足以下凸性约束条件\n$$\nw_j \\ge 0 \\text{ for all } j \\in \\{1,\\dots,M\\}, \\quad \\sum_{j=1}^M w_j = 1.\n$$\n\n我们将使用两种严格正常评分规则（strictly proper scoring rules）来评估预测结果：\n1. 对数损失（对于伯努利分布结果，也称为负对数似然），\n$$\n\\ell_{\\mathrm{log}}(y_i,\\hat{p}_i) = -\\left[y_i \\log(\\hat{p}_i) + (1-y_i)\\log(1-\\hat{p}_i)\\right],\n$$\n按照惯例，为避免出现 $\\log(0)$，在计算对数之前，我们会对 $\\hat{p}_i$ 进行裁剪，即 $\\hat{p}_i \\leftarrow \\min\\{\\max\\{\\hat{p}_i,\\epsilon\\},1-\\epsilon\\}$，其中 $\\epsilon$ 是一个固定的值 $10^{-15}$。\n2. Brier 分数，\n$$\n\\ell_{\\mathrm{brier}}(y_i,\\hat{p}_i) = \\left(y_i - \\hat{p}_i\\right)^2.\n$$\n\n您的任务是，为每个给定的测试用例，计算在每种评分规则下使经验风险最小化的堆叠权重向量 $w$，即，\n$$\n\\min_{w \\in \\mathbb{R}^M} \\frac{1}{N}\\sum_{i=1}^N \\ell_{\\mathrm{log}}(y_i,\\hat{p}_i(w)) \\quad \\text{subject to } w_j \\ge 0, \\ \\sum_{j=1}^M w_j = 1,\n$$\n和\n$$\n\\min_{w \\in \\mathbb{R}^M} \\frac{1}{N}\\sum_{i=1}^N \\ell_{\\mathrm{brier}}(y_i,\\hat{p}_i(w)) \\quad \\text{subject to } w_j \\ge 0, \\ \\sum_{j=1}^M w_j = 1.\n$$\n然后，通过报告 $L_1$ 差值 $\\|w^{\\mathrm{brier}} - w^{\\mathrm{log}}\\|_1 = \\sum_{j=1}^M |w^{\\mathrm{brier}}_j - w^{\\mathrm{log}}_j|$ 来比较得到的权重向量。\n\n请从经验风险最小化、概率单纯形上的凸组合以及上述两种严格正常评分规则的基本定义出发。在这些基础上推导出必要的最优性梯度或条件，并实现一个能满足约束条件的数值稳定求解器。\n\n请使用以下固定的测试套件。每个用例都指定了 $N$、$M$、标签 $y$ 以及基础模型预测矩阵 $P$（其第 $i$ 行为 $(p_{i1},\\dots,p_{iM})$）。在每个用例中，模型的顺序为 $m_1, m_2, m_3$。\n\n测试用例 1（一般情况，$N=8, M=3$）：\n- $y = [0,1,0,1,0,1,0,1]$.\n- $m_1$ 预测值：$[0.3,0.7,0.4,0.6,0.3,0.65,0.35,0.7]$。\n- $m_2$ 预测值：$[0.2,0.8,0.3,0.7,0.4,0.5,0.2,0.85]$。\n- $m_3$ 预测值：$[0.1,0.9,0.2,0.8,0.2,0.8,0.3,0.7]$。\n\n测试用例 2（两个相同模型，$N=4, M=3$）：\n- $y = [0,0,1,1]$.\n- $m_1$ 预测值：$[0.1,0.2,0.8,0.9]$。\n- $m_2$ 预测值：$[0.1,0.2,0.8,0.9]$。\n- $m_3$ 预测值：$[0.2,0.2,0.7,0.85]$。\n\n测试用例 3（存在近乎完美的模型，$N=6, M=3$）：\n- $y = [1,0,1,0,1,0]$.\n- $m_1$ 预测值：$[0.6,0.4,0.55,0.45,0.65,0.35]$。\n- $m_2$ 预测值：$[0.3,0.7,0.4,0.6,0.35,0.65]$。\n- $m_3$ 预测值：$[0.99,0.01,0.98,0.02,0.97,0.03]$。\n\n测试用例 4（用于测试稳定性的概率极值，$N=4, M=3$）：\n- $y = [1,1,0,0]$.\n- $m_1$ 预测值：$[0.0,0.1,1.0,0.9]$。\n- $m_2$ 预测值：$[0.2,0.0,0.8,1.0]$。\n- $m_3$ 预测值：$[0.05,0.05,0.95,0.95]$。\n\n实现细节和要求：\n- 对于对数损失，在目标函数及其导数中都对 $\\hat{p}_i(w)$ 应用 $\\epsilon = 10^{-15}$ 的裁剪，以确保数值稳定性。\n- 在您的优化程序中，请严格执行约束条件 $w_j \\ge 0$ 和 $\\sum_{j=1}^M w_j = 1$。\n- 对于每个测试用例，返回三项内容：Brier 最优权重向量 $w^{\\mathrm{brier}}$、对数损失最优权重向量 $w^{\\mathrm{log}}$ 以及它们之间的 $L_1$ 差值。\n- 将所有报告的实数四舍五入到 $6$ 位小数。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含四个测试用例的结果，格式为无空格、逗号分隔的列表的列表。对于每个测试用例，输出一个包含三项的列表：$M$ 个 Brier 权重的列表、$M$ 个对数损失权重的列表以及标量 $L_1$ 差值。例如，一个有效的整体格式是\n$[[[w_{1,1},w_{1,2},w_{1,3}],[v_{1,1},v_{1,2},v_{1,3}],d_1],[[w_{2,1},w_{2,2},w_{2,3}],[v_{2,1},v_{2,2},v_{2,3}],d_2],[[w_{3,1},w_{3,2},w_{3,3}],[v_{3,1},v_{3,2},v_{3,3}],d_3],[[w_{4,1},w_{4,2},w_{4,3}],[v_{4,1},v_{4,2},v_{4,3}],d_4]]$,\n其中每个 $w_{k,j}$、$v_{k,j}$ 和 $d_k$ 都精确打印到 $6$ 位小数。", "solution": "该问题要求为 $M$ 个概率分类器的堆叠（stacking）集成找到最优权重向量 $w \\in \\mathbb{R}^M$。最优权重是指在两种不同的评分规则（Brier 分数和对数损失）下，使 $N$ 个样本上评估的经验风险最小化的权重。权重向量必须满足凸性约束条件 $w_j \\ge 0$（对于所有 $j \\in \\{1,\\dots,M\\}$）和 $\\sum_{j=1}^M w_j = 1$。这组约束定义了标准的 $(M-1)$-单纯形，这是一个紧凸集，记为 $\\Delta^{M-1}$。\n\n令 $y \\in \\{0,1\\}^N$ 为真实标签向量，令 $P$ 为 $N \\times M$ 矩阵，其中 $P_{ij} = p_{ij}$ 是基础模型 $j$ 对样本 $i$ 的预测概率。样本 $i$ 的堆叠预测是基础预测的凸组合：\n$$\n\\hat{p}_i(w) = \\sum_{j=1}^M w_j P_{ij}\n$$\n用向量表示法，所有 $N$ 个样本的堆叠预测向量为 $\\hat{p}(w) = Pw$。\n\n任务是求解两个独立的约束优化问题。\n\n### 1. Brier 分数的最小化\n\n单个预测的 Brier 分数为 $\\ell_{\\mathrm{brier}}(y_i, \\hat{p}_i) = (y_i - \\hat{p}_i)^2$。经验风险是所有样本的平均 Brier 分数：\n$$\nR_{\\mathrm{brier}}(w) = \\frac{1}{N} \\sum_{i=1}^N (y_i - \\hat{p}_i(w))^2 = \\frac{1}{N} \\|y - Pw\\|_2^2\n$$\n优化问题为：\n$$\n\\min_{w \\in \\Delta^{M-1}} R_{\\mathrm{brier}}(w)\n$$\n这是一个凸优化问题。为了使用基于梯度的方法求解，我们首先展开目标函数：\n$$\nR_{\\mathrm{brier}}(w) = \\frac{1}{N} (y - Pw)^T(y - Pw) = \\frac{1}{N} (y^Ty - 2y^TPw + w^TP^TPw)\n$$\n这是一个关于 $w$ 的二次函数。这类特定问题被称为二次规划（Quadratic Program, QP）。优化算法需要目标函数相对于 $w$ 的梯度。使用向量微积分的法则，我们求得梯度为：\n$$\n\\nabla_w R_{\\mathrm{brier}}(w) = \\frac{1}{N} \\nabla_w (y^Ty - 2y^TPw + w^TP^TPw) = \\frac{1}{N} (-2P^Ty + 2P^TPw) = \\frac{2}{N} (P^TPw - P^Ty)\n$$\n这也可以用预测向量 $\\hat{p} = Pw$ 来表示，即 $\\nabla_w R_{\\mathrm{brier}}(w) = \\frac{2}{N} P^T(Pw - y) = -\\frac{2}{N} P^T(y - \\hat{p})$。\n\n### 2. 对数损失的最小化\n\n单个预测的对数损失为 $\\ell_{\\mathrm{log}}(y_i, \\hat{p}_i) = -[y_i \\log(\\hat{p}_i) + (1-y_i)\\log(1-\\hat{p}_i)]$。经验风险是平均对数损失：\n$$\nR_{\\mathrm{log}}(w) = \\frac{1}{N} \\sum_{i=1}^N -\\left[y_i \\log(\\hat{p}_i(w)) + (1-y_i)\\log(1-\\hat{p}_i(w))\\right]\n$$\n为保证数值稳定性，在传入对数函数之前，预测概率 $\\hat{p}_i$ 被裁剪到 $[\\epsilon, 1-\\epsilon]$ 范围内，其中 $\\epsilon=10^{-15}$。令 $\\hat{p}_c(w) = \\text{clip}(\\hat{p}(w), \\epsilon, 1-\\epsilon)$。目标函数为 $R_{\\mathrm{log}}(w)$，其中 $\\hat{p}_i(w)$ 被替换为 $\\hat{p}_{ic}(w)$。该函数是凸函数。优化问题为：\n$$\n\\min_{w \\in \\Delta^{M-1}} R_{\\mathrm{log}}(w)\n$$\n为求梯度，我们对每个分量 $w_k$ 求导：\n$$\n\\frac{\\partial R_{\\mathrm{log}}}{\\partial w_k} = \\frac{1}{N} \\sum_{i=1}^N -\\left[ \\frac{y_i}{\\hat{p}_{ic}} \\frac{\\partial \\hat{p}_{ic}}{\\partial w_k} - \\frac{1-y_i}{1-\\hat{p}_{ic}} \\frac{\\partial \\hat{p}_{ic}}{\\partial w_k} \\right]\n$$\n假设最优解对所有样本都位于裁剪范围的内部，则 $\\frac{\\partial \\hat{p}_{ic}}{\\partial w_k} = \\frac{\\partial \\hat{p}_i}{\\partial w_k} = P_{ik}$。导数变为：\n$$\n\\frac{\\partial R_{\\mathrm{log}}}{\\partial w_k} = -\\frac{1}{N} \\sum_{i=1}^N \\left( \\frac{y_i}{\\hat{p}_{ic}} - \\frac{1-y_i}{1-\\hat{p}_{ic}} \\right) P_{ik}\n$$\n以向量形式表示，梯度为：\n$$\n\\nabla_w R_{\\mathrm{log}}(w) = -\\frac{1}{N} P^T \\left( \\frac{y}{\\hat{p}_c} - \\frac{1-y}{1-\\hat{p}_c} \\right)\n$$\n其中除法是逐元素执行的。将 $\\hat{p}$ 裁剪为 $\\hat{p}_c$ 对于防止除以零和确保梯度有明确定义至关重要。\n\n### 3. 数值求解策略\n\n这两个问题都是带线性约束的凸优化实例。一个适用于此的算法是序列最小二乘规划（Sequential Least Squares Programming, SLSQP），它可以处理等式和不等式约束。我们使用 `scipy.optimize.minimize` 中提供的实现。\n\n优化问题配置如下：\n- **目标函数**：$R_{\\mathrm{brier}}(w)$ 或 $R_{\\mathrm{log}}(w)$。\n- **雅可比矩阵（梯度）**：$\\nabla_w R_{\\mathrm{brier}}(w)$ 或 $\\nabla_w R_{\\mathrm{log}}(w)$。\n- **约束条件**：\n    1. 等式约束：$\\sum_{j=1}^M w_j - 1 = 0$。\n    2. 不等式约束（边界）：$w_j \\ge 0$（对于所有 $j=1, \\dots, M$）。\n- **初始猜测**：一个均匀的权重向量，$w_j = 1/M$（对于所有 $j$），是一个合理的起点。\n\n此设置可以实现对最优权重向量 $w^{\\mathrm{brier}}$ 和 $w^{\\mathrm{log}}$ 的数值稳定且高效的计算。最后一步是计算 $L_1$ 差值 $\\|w^{\\mathrm{brier}} - w^{\\mathrm{log}}\\|_1 = \\sum_{j=1}^M |w^{\\mathrm{brier}}_j - w^{\\mathrm{log}}_j|$。", "answer": "```python\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef solve():\n    \"\"\"\n    Main function to solve the stacking optimization problems for all test cases.\n    \"\"\"\n    epsilon = 1e-15\n\n    test_cases = [\n        # Test case 1 (general case, N=8, M=3)\n        {\n            \"y\": np.array([0, 1, 0, 1, 0, 1, 0, 1]),\n            \"P\": np.array([\n                [0.3, 0.2, 0.1],\n                [0.7, 0.8, 0.9],\n                [0.4, 0.3, 0.2],\n                [0.6, 0.7, 0.8],\n                [0.3, 0.4, 0.2],\n                [0.65, 0.5, 0.8],\n                [0.35, 0.2, 0.3],\n                [0.7, 0.85, 0.7]\n            ])\n        },\n        # Test case 2 (two identical models, N=4, M=3)\n        {\n            \"y\": np.array([0, 0, 1, 1]),\n            \"P\": np.array([\n                [0.1, 0.1, 0.2],\n                [0.2, 0.2, 0.2],\n                [0.8, 0.8, 0.7],\n                [0.9, 0.9, 0.85]\n            ])\n        },\n        # Test case 3 (near-perfect model present, N=6, M=3)\n        {\n            \"y\": np.array([1, 0, 1, 0, 1, 0]),\n            \"P\": np.array([\n                [0.6, 0.3, 0.99],\n                [0.4, 0.7, 0.01],\n                [0.55, 0.4, 0.98],\n                [0.45, 0.6, 0.02],\n                [0.65, 0.35, 0.97],\n                [0.35, 0.65, 0.03]\n            ])\n        },\n        # Test case 4 (probability extremes, N=4, M=3)\n        {\n            \"y\": np.array([1, 1, 0, 0]),\n            \"P\": np.array([\n                [0.0, 0.2, 0.05],\n                [0.1, 0.0, 0.05],\n                [1.0, 0.8, 0.95],\n                [0.9, 1.0, 0.95]\n            ])\n        }\n    ]\n\n    def solve_weights(y, P, loss_type, epsilon):\n        \"\"\"\n        Solves for the optimal stacking weights for a given loss function.\n        \"\"\"\n        N, M = P.shape\n        w0 = np.full(M, 1 / M)\n\n        if loss_type == 'brier':\n            def objective(w):\n                phat = P @ w\n                return np.mean((y - phat)**2)\n\n            def jacobian(w):\n                phat = P @ w\n                return (2 / N) * P.T @ (phat - y)\n        elif loss_type == 'log':\n            def objective(w):\n                phat = P @ w\n                phat_clipped = np.clip(phat, epsilon, 1 - epsilon)\n                return -np.mean(y * np.log(phat_clipped) + (1 - y) * np.log(1 - phat_clipped))\n\n            def jacobian(w):\n                phat = P @ w\n                phat_clipped = np.clip(phat, epsilon, 1 - epsilon)\n                grad_term = y / phat_clipped - (1 - y) / (1 - phat_clipped)\n                return -(1 / N) * P.T @ grad_term\n        else:\n            raise ValueError(\"Invalid loss_type specified.\")\n\n        constraints = ({'type': 'eq', 'fun': lambda w: np.sum(w) - 1})\n        bounds = tuple((0, None) for _ in range(M))\n        \n        result = minimize(\n            fun=objective,\n            x0=w0,\n            method='SLSQP',\n            jac=jacobian,\n            bounds=bounds,\n            constraints=constraints,\n            tol=1e-12\n        )\n        # It's possible for weights to be very small negative numbers due to precision.\n        # Clip at 0 and re-normalize to strictly satisfy constraints.\n        w_opt = np.maximum(0, result.x)\n        w_opt /= np.sum(w_opt)\n        return w_opt\n\n    results = []\n    for case in test_cases:\n        y, P = case[\"y\"], case[\"P\"]\n        \n        w_brier = solve_weights(y, P, 'brier', epsilon)\n        w_log = solve_weights(y, P, 'log', epsilon)\n        \n        l1_diff = np.sum(np.abs(w_brier - w_log))\n        \n        # Format results to 6 decimal places as strings\n        w_brier_str = '[' + ','.join([f\"{x:.6f}\" for x in w_brier]) + ']'\n        w_log_str = '[' + ','.join([f\"{x:.6f}\" for x in w_log]) + ']'\n        l1_diff_str = f\"{l1_diff:.6f}\"\n        \n        case_result_str = f\"[{w_brier_str},{w_log_str},{l1_diff_str}]\"\n        results.append(case_result_str)\n\n    # Final print statement in the exact required format.\n    final_output = '[' + ','.join(results) + ']'\n    print(final_output)\n\nsolve()\n```", "id": "3175480"}, {"introduction": "真实世界的数据往往不满足普通最小二乘法（OLS）关于误差方差恒定（同方差性）的假设。本实践探讨了如何通过使用加权最小二乘法（WLS）来调整堆叠模型的元学习器，以应对异方差性问题。在这个练习中，误差方差较小的观测值将被赋予更大的权重，这展示了堆叠框架在处理特定数据特征时的灵活性。[@problem_id:3175509]", "problem": "实现一个堆叠集成模型，其中的元学习器在合成异方差数据上通过加权最小二乘法（WLS）进行训练，使用与方差倒数 $1/\\hat{\\sigma}^2(x)$ 成正比的权重。并将其与通过普通最小二乘法（OLS）训练的未加权元学习器进行评估。您的程序必须是完全确定性的，并生成一行包含布尔值列表的输出，具体格式如下所述。\n\n使用的基本设置和基础定义：\n- 堆叠集成模型结合了 $M$ 个基预测器。给定一个训练集 $\\{(x_i, y_i)\\}_{i=1}^n$，定义第一层设计矩阵 $Z \\in \\mathbb{R}^{n \\times M}$，其列是 $M$ 个基学习器的折外预测（out-of-fold predictions）。元学习器是一个线性模型，用于预测 $\\hat{y} = \\beta_0 + \\sum_{j=1}^M \\beta_j z_{j}$。\n- 普通最小二乘法（OLS）通过最小化 $\\sum_{i=1}^n (y_i - \\beta_0 - z_i^\\top \\beta)^2$ 来估计元参数 $\\beta$。\n- 在具有特定于观测值的方差 $\\sigma_i^2$ 的独立高斯噪声下，最大似然估计（Maximum Likelihood Estimation）会导出加权最小二乘法（WLS），该方法最小化 $\\sum_{i=1}^n w_i (y_i - \\beta_0 - z_i^\\top \\beta)^2$，其中 $w_i \\propto 1/\\sigma_i^2$。\n- 为避免堆叠过程中的目标泄漏（target leakage），第一层矩阵 $Z$ 必须由通过 $K$-折交叉验证（CV）获得的折外预测构建，其中 $K \\in \\mathbb{N}$ 且 $K \\ge 2$。\n\n使用的合成数据生成方法：\n- 输入特征 $x$ 从区间 $[-3, 3]$ 上的均匀分布中独立采样。\n- 均值函数为 $f(x) = 1.5 \\sin(1.2 x) + 0.3 x$。\n- 异方差噪声的标准差为 $\\sigma(x) = 0.3 + 0.7 \\lvert x \\rvert / 3$，同方差情况则使用 $\\sigma(x) \\equiv 0.6$。响应值为 $y = f(x) + \\varepsilon$，其中 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2(x))$。\n- 用于堆叠的基学习器：\n  - 基学习器 1：基于特征 $[1, x]$ 的线性回归。\n  - 基学习器 2：基于特征 $[1, x, x^2, x^3]$ 的三次回归。\n- 使用 $K = 5$ 折为元学习器构建折外预测。\n\n用于 WLS 权重的方差模型：\n- 通过对数线性模型估计条件方差。首先，在完整训练数据上拟合一个三次多项式作为均值模型 $\\tilde{f}(x)$，并计算残差 $r_i = y_i - \\tilde{f}(x_i)$。然后，使用 OLS 将 $\\log(r_i^2 + \\epsilon)$ 对特征 $[1, \\lvert x_i \\rvert]$ 进行回归，以获得系数 $\\gamma$。估计的方差为 $\\hat{\\sigma}^2(x) = \\exp(\\gamma_0 + \\gamma_1 \\lvert x \\rvert)$，并应用一个下限 $v_{\\min} > 0$，即 $\\hat{\\sigma}^2(x) \\leftarrow \\max(\\hat{\\sigma}^2(x), v_{\\min})$。除非在测试中另有说明，否则使用 $\\epsilon = 10^{-8}$ 和 $v_{\\min} = 10^{-6}$。\n- WLS 的权重与 $1/\\hat{\\sigma}^2(x)$ 成正比；不要在样本间进行归一化，因为将所有权重乘以一个共同的正标量不会改变 WLS 解。\n\n评估协议：\n- 对于下述每个测试用例，使用指定的随机种子生成独立的训练集和测试集。训练集大小为 $n_{\\text{train}}$，测试集大小为 $n_{\\text{test}}$。使用所述的堆叠过程，通过以下方式产生测试预测：\n  - 在折外第一层训练预测上训练的 OLS 元学习器。\n  - 使用指定的神谕权重（oracle weights）$w_i \\propto 1/\\sigma^2(x_i)$ 或估计权重 $w_i \\propto 1/\\hat{\\sigma}^2(x_i)$ 训练的 WLS 元学习器。\n- 报告在测试集上平均的测试均方误差（MSE），$ \\text{MSE} = \\frac{1}{n_{\\text{test}}} \\sum_{i=1}^{n_{\\text{test}}} (y_i - \\hat{y}_i)^2$。\n\n要实现的测试套件及需要输出的布尔值：\n- 测试 $1$（异方差，神谕权重和估计权重）：\n  - 参数：$n_{\\text{train}} = 800$，$n_{\\text{test}} = 5000$，种子 $= 123$。\n  - 数据：异方差。\n  - 在测试集上计算三个 MSE：$\\text{MSE}_{\\text{OLS}}$、$\\text{MSE}_{\\text{WLS-oracle}}$（使用 $w_i \\propto 1/\\sigma^2(x_i)$ 和已知的 $\\sigma(x)$）以及 $\\text{MSE}_{\\text{WLS-est}}$（使用 $w_i \\propto 1/\\hat{\\sigma}^2(x_i)$）。\n  - 输出两个布尔值：\n    - $b_1$：$\\text{MSE}_{\\text{WLS-oracle}}  \\text{MSE}_{\\text{OLS}}$ 是否成立。\n    - $b_2$：$\\text{MSE}_{\\text{WLS-est}}  \\text{MSE}_{\\text{OLS}}$ 是否成立。\n- 测试 $2$（同方差，估计权重近似为常数）：\n  - 参数：$n_{\\text{train}} = 3000$，$n_{\\text{test}} = 5000$，种子 $= 456$。\n  - 数据：同方差，$\\sigma(x) \\equiv 0.6$。\n  - 计算 $\\text{MSE}_{\\text{OLS}}$ 和 $\\text{MSE}_{\\text{WLS-est}}$。\n  - 输出一个布尔值：\n    - $b_3$：$\\lvert \\text{MSE}_{\\text{WLS-est}} - \\text{MSE}_{\\text{OLS}} \\rvert \\le \\tau$ 是否成立，容差 $\\tau = 10^{-2}$。\n- 测试 $3$（在极端但为正的权重下的稳定性）：\n  - 参数：$n_{\\text{train}} = 800$，$n_{\\text{test}} = 5000$，种子 $= 321$。\n  - 数据：异方差。\n  - 在求倒数之前，将估计的方差乘以 $c = 10^{-6}$ 进行修改，并在计算 $1/\\hat{\\sigma}^2(x)$ 时使用极小的下限 $v_{\\min} = 10^{-12}$。使用这些权重训练 WLS 元学习器并计算测试 MSE。\n  - 输出一个布尔值：\n    - $b_4$：所有拟合的元参数和最终的测试 MSE 是否为有限实数。\n- 测试 $4$（错误设定的权重损害性能）：\n  - 参数：$n_{\\text{train}} = 800$，$n_{\\text{test}} = 5000$，种子 $= 789$。\n  - 数据：异方差。\n  - 构建与 $\\hat{\\sigma}^2(x)$（而不是其倒数）成正比的错误设定的权重，并使用这些错误设定的权重训练 WLS 元学习器。计算 $\\text{MSE}_{\\text{WLS-bad}}$ 并与 $\\text{MSE}_{\\text{OLS}}$ 进行比较。\n  - 输出一个布尔值：\n    - $b_5$：$\\text{MSE}_{\\text{OLS}} \\le \\text{MSE}_{\\text{WLS-bad}}$ 是否成立。\n- 测试 $5$（WLS 对权重的全局缩放的不变性）：\n  - 使用与测试 1 相同的数据生成方式（异方差，种子 $= 123$，$n_{\\text{train}} = 800$，$n_{\\text{test}} = 5000$）。\n  - 使用神谕权重 $w_i \\propto 1/\\sigma^2(x_i)$ 计算测试预测，然后再次使用经过重新缩放的权重 $w_i' = c \\cdot w_i$（其中 $c = 7$）进行计算。测量两组测试预测之间的最大绝对差。\n  - 输出一个布尔值：\n    - $b_6$：最大绝对差是否小于 $\\delta = 10^{-10}$。\n\n实现要求：\n- 使用 $K = 5$ 折构建折外第一层预测。\n- 仅使用线性代数和基本数值运算；不要使用外部机器学习库。\n- 所有随机数生成必须由指定的种子控制，以确保可复现性。\n- 最终输出格式：您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表的结果，顺序为 $[b_1, b_2, b_3, b_4, b_5, b_6]$。\n\n您的程序必须是一个单一、完整、可运行的脚本，该脚本只产生一行指定格式的输出，不得包含任何额外文本。", "solution": "用户希望实现一个堆叠集成模型，并评估使用加权最小二乘法（WLS）训练的元学习器与使用普通最小二乘法（OLS）训练的元学习器的性能。此分析将在表现出异方差性的合成数据上进行。\n\n### 1. 问题验证\n\n已根据指定的验证协议对问题陈述进行了仔细审查。\n\n- **步骤1：提取既定条件**：所有参数、模型和程序均已逐字提取。这包括合成数据生成过程、基学习器的定义、折外预测方法（$K=5$ 交叉验证）、OLS 和 WLS 元学习器的公式、用于估计观测方差的特定模型，以及包含相应评估标准的五个详细测试套件。\n- **步骤2：对照标准进行验证**：问题被确定为有效。\n    - 它**在科学上是有根据的**，基于统计学习的既定原则，特别是集成方法和非恒定方差下的回归。\n    - 它是**适定 (well-posed) 的**，所有变量、函数和程序都有明确的定义，确保在给定指定随机种子的情况下，每个测试用例都有唯一且可复现的结果。\n    - 它是**客观的**，使用精确的数学语言和量化评估指标（均方误差），没有任何主观或基于意见的陈述。\n- **步骤3：结论**：问题是**有效的**。未发现任何缺陷。我将继续提供完整的解决方案。\n\n### 2. 方法论框架\n\n问题的核心是在一个堆叠集成框架内比较两种用于线性元学习器的估计策略。该集成模型结合了两个基学习器的预测。\n\n**2.1. 数据生成**\n\n生成合成数据以控制底层的真实模型和噪声结构。对于一个包含 $n$ 个样本的集合：\n- 特征 $x_i$ 从均匀分布中抽取：$x_i \\sim U[-3, 3]$。\n- 真实的底层函数是 $f(x) = 1.5 \\sin(1.2 x) + 0.3 x$。\n- 响应 $y_i$ 是通过向真实函数值添加高斯噪声生成的：$y_i = f(x_i) + \\varepsilon_i$。\n- 噪声 $\\varepsilon_i$ 从 $\\mathcal{N}(0, \\sigma^2(x_i))$ 中抽取，其中标准差 $\\sigma(x)$ 可以是：\n    - **异方差**：$\\sigma(x) = 0.3 + 0.7 \\lvert x \\rvert / 3$。噪声方差依赖于输入 $x$。\n    - **同方差**：$\\sigma(x) \\equiv 0.6$。噪声方差是恒定的。\n\n**2.2. 堆叠集成结构**\n\n该集成模型包含两个层次：基学习器（第0层）和元学习器（第1层）。\n\n- **基学习器**：\n    - 学习器1：一个简单的线性回归模型，使用特征 $[1, x]$。\n    - 学习器2：一个三次多项式回归模型，使用特征 $[1, x, x^2, x^3]$。\n\n- **第1层数据生成**：为防止目标泄漏，元学习器的训练数据是使用折外预测构建的。大小为 $n_{\\text{train}}$ 的训练集被分成 $K=5$ 折。对于每一折，基学习器在其余 $K-1$ 折上进行训练，然后用于对留出的那一折进行预测。这些对所有 $n_{\\text{train}}$ 个样本的预测集合构成了第1层设计矩阵 $\\mathbf{Z} \\in \\mathbb{R}^{n_{\\text{train}} \\times 2}$。$\\mathbf{Z}$ 的第 $i$ 行，表示为 $\\mathbf{z}_i$，包含了两个基学习器对训练样本 $(x_i, y_i)$ 的折外预测。\n\n**2.3. 元学习器**\n\n元学习器是一个线性模型，它结合基学习器的预测来产生最终预测 $\\hat{y}$。它包含一个截距项 $\\beta_0$：\n$$\n\\hat{y}_i = \\beta_0 + \\beta_1 z_{i1} + \\beta_2 z_{i2} = \\mathbf{z}_{\\text{aug}, i}^T \\boldsymbol{\\beta}\n$$\n其中 $\\mathbf{z}_{\\text{aug}, i} = [1, z_{i1}, z_{i2}]^T$ 是增广的第1层特征向量，$\\boldsymbol{\\beta} = [\\beta_0, \\beta_1, \\beta_2]^T$ 是元参数向量。\n\n- **普通最小二乘法 (OLS)**：此方法假设误差是同方差的，并通过最小化残差平方和来找到 $\\boldsymbol{\\beta}$：\n    $$\n    \\text{RSS} = \\sum_{i=1}^{n_{\\text{train}}} (y_i - \\mathbf{z}_{\\text{aug}, i}^T \\boldsymbol{\\beta})^2\n    $$\n    解由正规方程给出：$\\hat{\\boldsymbol{\\beta}}_{\\text{OLS}} = (\\mathbf{Z}_{\\text{aug}}^T \\mathbf{Z}_{\\text{aug}})^{-1} \\mathbf{Z}_{\\text{aug}}^T \\mathbf{y}$。\n\n- **加权最小二乘法 (WLS)**：当误差是独立的但具有非恒定方差（异方差性）时，此方法是最优的。它通过最小化加权残差平方和来找到 $\\boldsymbol{\\beta}$：\n    $$\n    \\text{WRSS} = \\sum_{i=1}^{n_{\\text{train}}} w_i (y_i - \\mathbf{z}_{\\text{aug}, i}^T \\boldsymbol{\\beta})^2\n    $$\n    其中权重 $w_i$ 理想情况下与误差方差成反比，$w_i \\propto 1/\\sigma_i^2$。解由加权正规方程给出：$\\hat{\\boldsymbol{\\beta}}_{\\text{WLS}} = (\\mathbf{Z}_{\\text{aug}}^T \\mathbf{W} \\mathbf{Z}_{\\text{aug}})^{-1} \\mathbf{Z}_{\\text{aug}}^T \\mathbf{W} \\mathbf{y}$，其中 $\\mathbf{W}$ 是一个对角矩阵，其对角元素为 $W_{ii} = w_i$。\n\n**2.4. WLS 权重估计**\n\n由于真实方差 $\\sigma^2(x)$ 在实践中通常是未知的，因此必须对其进行估计。该协议指定了一个用于方差的对数线性模型：\n1.  对完整的训练数据 $(x_i, y_i)$ 拟合一个三次多项式模型 $\\tilde{f}(x)$，以获得条件均值的估计。\n2.  计算平方残差 $r_i^2 = (y_i - \\tilde{f}(x_i))^2$。\n3.  对平方残差的对数进行建模。拟合一个线性模型，以从特征 $[1, \\lvert x_i \\rvert]$ 预测 $\\log(r_i^2 + \\epsilon)$，其中 $\\epsilon=10^{-8}$ 确保对数的参数为正。这将产生系数 $\\boldsymbol{\\gamma} = [\\gamma_0, \\gamma_1]^T$。\n4.  然后，估计的方差为 $\\hat{\\sigma}^2(x) = \\exp(\\gamma_0 + \\gamma_1 \\lvert x \\rvert)$。\n5.  应用一个下限 $v_{\\min} = 10^{-6}$ 以防止极小的方差估计：$\\hat{\\sigma}^2(x) \\leftarrow \\max(\\hat{\\sigma}^2(x), v_{\\min})$。\n6.  用于 WLS 的估计权重为 $w_i = 1/\\hat{\\sigma}^2(x_i)$。\n\n**2.5. 评估**\n\n对于每个测试用例，都会生成一个大小为 $n_{\\text{test}}$ 的独立测试集。在*完整*训练集上训练的基学习器用于生成测试集第1层矩阵 $\\mathbf{Z}_{\\text{test}}$。然后在训练集第1层数据上训练的 OLS 和 WLS 元学习器，被用于对 $\\mathbf{Z}_{\\text{test}}$ 进行最终预测。性能通过测试集上的均方误差（MSE）来衡量：\n$$\n\\text{MSE} = \\frac{1}{n_{\\text{test}}} \\sum_{i=1}^{n_{\\text{test}}} (y_{\\text{test}, i} - \\hat{y}_{\\text{test}, i})^2\n$$\n然后执行五个测试中概述的具体数值比较，以生成所需的布尔输出。该过程稳健地测试了 WLS 在各种条件下的效能，包括理想（神谕权重）、实际（估计权重）和病态（错误设定的权重）场景。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n# from scipy import ...\n\ndef solve():\n    \"\"\"\n    Main function to run the suite of tests for stacking with WLS meta-learner.\n    \"\"\"\n\n    def fit_linear_model(X, y, weights=None):\n        \"\"\"\n        Fits a linear model using OLS or WLS.\n        \n        Args:\n            X (np.ndarray): Design matrix.\n            y (np.ndarray): Target vector.\n            weights (np.ndarray, optional): Weights for WLS. If None, performs OLS.\n        \n        Returns:\n            np.ndarray: Fitted coefficients.\n        \"\"\"\n        if weights is None:\n            # OLS\n            coeffs = np.linalg.lstsq(X, y, rcond=None)[0]\n        else:\n            # WLS: transform to an equivalent OLS problem\n            sqrt_w = np.sqrt(weights)\n            X_w = sqrt_w[:, np.newaxis] * X\n            y_w = sqrt_w * y\n            coeffs = np.linalg.lstsq(X_w, y_w, rcond=None)[0]\n        return coeffs\n\n    def predict_linear_model(X, coeffs):\n        \"\"\"Predicts using a fitted linear model.\"\"\"\n        return X @ coeffs\n\n    def create_design_matrix(x, degree):\n        \"\"\"Creates a polynomial design matrix [1, x, x^2, ..., x^degree].\"\"\"\n        return np.vander(x, N=degree + 1, increasing=True)\n\n    def generate_data(n_samples, seed, heteroskedastic):\n        \"\"\"Generates synthetic data based on the problem specification.\"\"\"\n        rng = np.random.default_rng(seed)\n        x = rng.uniform(-3, 3, n_samples)\n        f_x = 1.5 * np.sin(1.2 * x) + 0.3 * x\n        \n        if heteroskedastic:\n            sigma_x = 0.3 + 0.7 * np.abs(x) / 3\n        else:\n            sigma_x = np.full_like(x, 0.6)\n            \n        epsilon = rng.normal(0, sigma_x, n_samples)\n        y = f_x + epsilon\n        return x, y, sigma_x\n\n    def get_out_of_fold_predictions(x_train, y_train, K):\n        \"\"\"\n        Generates level-1 data (out-of-fold predictions) using K-fold CV.\n        \n        Returns:\n            np.ndarray: Level-1 design matrix Z.\n        \"\"\"\n        n_train = len(y_train)\n        Z_train = np.zeros((n_train, 2))\n        \n        # Use deterministic fold splits\n        fold_indices = np.array_split(np.arange(n_train), K)\n\n        for k in range(K):\n            val_idx = fold_indices[k]\n            train_idx = np.concatenate([fold_indices[j] for j in range(K) if j != k])\n\n            x_fold_train, y_fold_train = x_train[train_idx], y_train[train_idx]\n            x_fold_val = x_train[val_idx]\n\n            # Base Learner 1: Linear\n            X1_fold_train = create_design_matrix(x_fold_train, 1)\n            coeffs1 = fit_linear_model(X1_fold_train, y_fold_train)\n            X1_fold_val = create_design_matrix(x_fold_val, 1)\n            Z_train[val_idx, 0] = predict_linear_model(X1_fold_val, coeffs1)\n\n            # Base Learner 2: Cubic\n            X2_fold_train = create_design_matrix(x_fold_train, 3)\n            coeffs2 = fit_linear_model(X2_fold_train, y_fold_train)\n            X2_fold_val = create_design_matrix(x_fold_val, 3)\n            Z_train[val_idx, 1] = predict_linear_model(X2_fold_val, coeffs2)\n            \n        return Z_train\n\n    def estimate_variance_params(x_train, y_train, epsilon):\n        \"\"\"\n        Estimates the parameters of the log-linear variance model.\n        \n        Returns:\n            np.ndarray: Coefficients gamma.\n        \"\"\"\n        # 1. Fit mean model (cubic poly)\n        X_mean = create_design_matrix(x_train, 3)\n        mean_coeffs = fit_linear_model(X_mean, y_train)\n        y_pred_mean = predict_linear_model(X_mean, mean_coeffs)\n        \n        # 2. Compute residuals\n        residuals = y_train - y_pred_mean\n        \n        # 3. Regress log(r^2 + eps) on [1, |x|]\n        log_sq_res = np.log(residuals**2 + epsilon)\n        X_var = create_design_matrix(np.abs(x_train), 1)\n        gamma = fit_linear_model(X_var, log_sq_res)\n        \n        return gamma\n\n    def get_estimated_variance(x, gamma, v_min):\n        \"\"\"Calculates estimated variance for given x and gamma.\"\"\"\n        X_var = create_design_matrix(np.abs(x), 1)\n        log_var = predict_linear_model(X_var, gamma)\n        var_est = np.exp(log_var)\n        return np.maximum(var_est, v_min)\n\n\n    results = []\n\n    # --- Test 1  5: Heteroskedastic, Oracle vs. Estimated Weights  Scaling Invariance ---\n    n_train, n_test, seed = 800, 5000, 123\n    x_train, y_train, sigma_train = generate_data(n_train, seed, heteroskedastic=True)\n    x_test, y_test, sigma_test = generate_data(n_test, seed + 1, heteroskedastic=True)\n\n    # Level-1 data\n    Z_train = get_out_of_fold_predictions(x_train, y_train, K=5)\n    Z_train_aug = np.c_[np.ones(n_train), Z_train]\n\n    # Base learners trained on full training data for test predictions\n    coeffs1_full = fit_linear_model(create_design_matrix(x_train, 1), y_train)\n    coeffs2_full = fit_linear_model(create_design_matrix(x_train, 3), y_train)\n    \n    Z_test = np.zeros((n_test, 2))\n    Z_test[:, 0] = predict_linear_model(create_design_matrix(x_test, 1), coeffs1_full)\n    Z_test[:, 1] = predict_linear_model(create_design_matrix(x_test, 3), coeffs2_full)\n    Z_test_aug = np.c_[np.ones(n_test), Z_test]\n\n    # OLS meta-learner\n    beta_ols = fit_linear_model(Z_train_aug, y_train)\n    y_pred_ols = predict_linear_model(Z_test_aug, beta_ols)\n    mse_ols = np.mean((y_test - y_pred_ols)**2)\n\n    # WLS with Oracle weights\n    weights_oracle = 1 / sigma_train**2\n    beta_wls_oracle = fit_linear_model(Z_train_aug, y_train, weights=weights_oracle)\n    y_pred_wls_oracle = predict_linear_model(Z_test_aug, beta_wls_oracle)\n    mse_wls_oracle = np.mean((y_test - y_pred_wls_oracle)**2)\n    \n    # WLS with Estimated weights\n    gamma_est = estimate_variance_params(x_train, y_train, epsilon=1e-8)\n    var_est_train = get_estimated_variance(x_train, gamma_est, v_min=1e-6)\n    weights_est = 1 / var_est_train\n    beta_wls_est = fit_linear_model(Z_train_aug, y_train, weights=weights_est)\n    y_pred_wls_est = predict_linear_model(Z_test_aug, beta_wls_est)\n    mse_wls_est = np.mean((y_test - y_pred_wls_est)**2)\n\n    b1 = mse_wls_oracle  mse_ols\n    b2 = mse_wls_est  mse_ols\n    results.extend([b1, b2])\n\n    # --- Test 5: Invariance to weight scaling ---\n    weights_oracle_scaled = 7.0 * weights_oracle\n    beta_wls_oracle_scaled = fit_linear_model(Z_train_aug, y_train, weights=weights_oracle_scaled)\n    y_pred_wls_oracle_scaled = predict_linear_model(Z_test_aug, beta_wls_oracle_scaled)\n    max_diff_test5 = np.max(np.abs(y_pred_wls_oracle - y_pred_wls_oracle_scaled))\n    b6 = max_diff_test5  1e-10\n\n    # --- Test 2: Homoskedastic case ---\n    n_train, n_test, seed = 3000, 5000, 456\n    x_train, y_train, _ = generate_data(n_train, seed, heteroskedastic=False)\n    x_test, y_test, _ = generate_data(n_test, seed + 1, heteroskedastic=False)\n    \n    Z_train = get_out_of_fold_predictions(x_train, y_train, K=5)\n    Z_train_aug = np.c_[np.ones(n_train), Z_train]\n    \n    coeffs1_full = fit_linear_model(create_design_matrix(x_train, 1), y_train)\n    coeffs2_full = fit_linear_model(create_design_matrix(x_train, 3), y_train)\n    Z_test = np.c_[\n        predict_linear_model(create_design_matrix(x_test, 1), coeffs1_full),\n        predict_linear_model(create_design_matrix(x_test, 3), coeffs2_full)\n    ]\n    Z_test_aug = np.c_[np.ones(n_test), Z_test]\n\n    beta_ols = fit_linear_model(Z_train_aug, y_train)\n    y_pred_ols = predict_linear_model(Z_test_aug, beta_ols)\n    mse_ols_t2 = np.mean((y_test - y_pred_ols)**2)\n\n    gamma_est = estimate_variance_params(x_train, y_train, epsilon=1e-8)\n    var_est_train = get_estimated_variance(x_train, gamma_est, v_min=1e-6)\n    weights_est = 1 / var_est_train\n    beta_wls_est = fit_linear_model(Z_train_aug, y_train, weights=weights_est)\n    y_pred_wls_est = predict_linear_model(Z_test_aug, beta_wls_est)\n    mse_wls_est_t2 = np.mean((y_test - y_pred_wls_est)**2)\n\n    b3 = np.abs(mse_wls_est_t2 - mse_ols_t2) = 1e-2\n    results.append(b3)\n\n    # --- Test 3: Stability under extreme weights ---\n    n_train, n_test, seed = 800, 5000, 321\n    x_train, y_train, _ = generate_data(n_train, seed, heteroskedastic=True)\n    x_test, y_test, _ = generate_data(n_test, seed + 1, heteroskedastic=True)\n\n    Z_train = get_out_of_fold_predictions(x_train, y_train, K=5)\n    Z_train_aug = np.c_[np.ones(n_train), Z_train]\n\n    gamma_est = estimate_variance_params(x_train, y_train, epsilon=1e-8)\n    var_est_train = get_estimated_variance(x_train, gamma_est, v_min=1e-12)\n    \n    # Modify variance and weights\n    var_est_mod = var_est_train * 1e-6\n    weights_extreme = 1 / var_est_mod\n    \n    beta_wls_extreme = fit_linear_model(Z_train_aug, y_train, weights=weights_extreme)\n    \n    coeffs1_full = fit_linear_model(create_design_matrix(x_train, 1), y_train)\n    coeffs2_full = fit_linear_model(create_design_matrix(x_train, 3), y_train)\n    Z_test = np.c_[\n        predict_linear_model(create_design_matrix(x_test, 1), coeffs1_full),\n        predict_linear_model(create_design_matrix(x_test, 3), coeffs2_full)\n    ]\n    Z_test_aug = np.c_[np.ones(n_test), Z_test]\n\n    y_pred_wls_extreme = predict_linear_model(Z_test_aug, beta_wls_extreme)\n    mse_wls_extreme = np.mean((y_test - y_pred_wls_extreme)**2)\n\n    b4 = np.all(np.isfinite(beta_wls_extreme)) and np.isfinite(mse_wls_extreme)\n    results.append(b4)\n\n    # --- Test 4: Misspecified weights ---\n    n_train, n_test, seed = 800, 5000, 789\n    x_train, y_train, _ = generate_data(n_train, seed, heteroskedastic=True)\n    x_test, y_test, _ = generate_data(n_test, seed + 1, heteroskedastic=True)\n    \n    Z_train = get_out_of_fold_predictions(x_train, y_train, K=5)\n    Z_train_aug = np.c_[np.ones(n_train), Z_train]\n\n    coeffs1_full = fit_linear_model(create_design_matrix(x_train, 1), y_train)\n    coeffs2_full = fit_linear_model(create_design_matrix(x_train, 3), y_train)\n    Z_test = np.c_[\n        predict_linear_model(create_design_matrix(x_test, 1), coeffs1_full),\n        predict_linear_model(create_design_matrix(x_test, 3), coeffs2_full)\n    ]\n    Z_test_aug = np.c_[np.ones(n_test), Z_test]\n    \n    beta_ols = fit_linear_model(Z_train_aug, y_train)\n    y_pred_ols = predict_linear_model(Z_test_aug, beta_ols)\n    mse_ols_t4 = np.mean((y_test - y_pred_ols)**2)\n\n    gamma_est = estimate_variance_params(x_train, y_train, epsilon=1e-8)\n    var_est_train = get_estimated_variance(x_train, gamma_est, v_min=1e-6)\n    weights_bad = var_est_train  # Proportional to variance, not inverse\n    \n    beta_wls_bad = fit_linear_model(Z_train_aug, y_train, weights=weights_bad)\n    y_pred_wls_bad = predict_linear_model(Z_test_aug, beta_wls_bad)\n    mse_wls_bad = np.mean((y_test - y_pred_wls_bad)**2)\n    \n    b5 = mse_ols_t4 = mse_wls_bad\n    results.append(b5)\n    \n    # Add Test 5 result at the end\n    results.append(b6)\n    \n    # Final output formatting: [b1, b2, b3, b4, b5, b6]\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3175509"}, {"introduction": "除了实现，理解堆叠模型的理论特性同样关键。本实践是一个思想实验，旨在探讨当基础模型高度相关（即其预测存在共线性）时会发生什么，这种情况会导致堆叠权重存在非唯一解。您将分析不同的正则化技术（如岭回归和LASSO）如何解决这种模糊性，从而确保获得一个稳定且可识别的解。[@problem_id:3175491]", "problem": "一个预测堆叠系统使用 $M$ 个基学习器。对于一个有 $n$ 个观测值的数据集，令 $Z \\in \\mathbb{R}^{n \\times M}$ 的各列收集了基学习器的样本外预测，并令 $w \\in \\mathbb{R}^{M}$ 为堆叠权重。堆叠预测器是线性映射 $\\hat{y}(w) = Z w \\in \\mathbb{R}^{n}$。假设 $\\operatorname{rank}(Z) = r$，其中 $r  M$。\n\n仅从线性预测、矩阵的列空间和零空间、秩-零度定理、Moore–Penrose 伪逆以及凸优化的性质出发，回答以下问题：\n\n1. 推导使 $\\hat{y}(w_{1}) = \\hat{y}(w_{2})$ 成立的关于 $w_{1}$ 和 $w_{2}$ 的一个充要条件。利用此条件，刻画出能产生与给定参考权重 $w_{0}$ 相同预测的所有权重集合。你的刻画必须是显式的，并使用与 $Z$ 相关的基本线性代数对象进行参数化。\n\n2. 考虑带有平方误差和可分离正则化项的经验风险最小化问题，\n$$\n\\min_{w \\in \\mathbb{R}^{M}} \\; \\frac{1}{2n} \\| y - Z w \\|_{2}^{2} + \\lambda \\, \\Omega(w),\n$$\n其中 $\\lambda \\ge 0$。对于以下每种选择，论证 $w$ 的可识别性以及预测 $\\hat{y} = Z w$ 的可识别性：\n   - (i) 无正则化：$\\lambda = 0$ 且 $\\Omega(w) \\equiv 0$。\n   - (ii) 岭正则化：$\\Omega(w) = \\|w\\|_{2}^{2}$ 且 $\\lambda  0$。\n   - (iii) 最小绝对收缩和选择算子 (LASSO)：$\\Omega(w) = \\|w\\|_{1}$ 且 $\\lambda  0$。\n在每种情况下，论证优化器 $w^{\\star}$ 是否唯一，预测 $\\hat{y}^{\\star} = Z w^{\\star}$ 是否唯一，以及解如何与由 $Z$ 导出的 $\\mathbb{R}^{M}$ 的线性代数分解相关联。\n\n3. 令 $\\mathcal{W}(w_{0})$ 表示能产生与任意参考权重 $w_{0} \\in \\mathbb{R}^{M}$ 完全相同预测的所有权重向量的完整集合。$\\mathcal{W}(w_{0})$ 的维度用 $M$ 和 $r$ 表示是什么？将此维度作为你的最终答案，以 $M$ 和 $r$ 的封闭形式表达式报告。\n\n本题无需进行数值四舍五入。按要求提供最终答案，不要包含任何单位。", "solution": "本题研究线性堆叠模型中参数和预测的可识别性，特别是在基学习器的预测是共线的情况下。问题陈述的验证如下。\n\n从问题陈述中提取的已知条件：\n- 基学习器的数量为 $M$。\n- 观测值的数量为 $n$。\n- 样本外预测矩阵为 $Z \\in \\mathbb{R}^{n \\times M}$。\n- 堆叠权重向量为 $w \\in \\mathbb{R}^{M}$。\n- 堆叠预测器定义为线性映射 $\\hat{y}(w) = Z w \\in \\mathbb{R}^{n}$。\n- 预测矩阵的秩为 $\\operatorname{rank}(Z) = r$。\n- 已知 $r  M$。\n- 解必须从线性预测、列空间 ($\\mathcal{C}(Z)$)、零空间 ($\\mathcal{N}(Z)$)、秩-零度定理、Moore–Penrose 伪逆以及凸优化的性质出发进行推导。\n- 经验风险最小化问题是 $\\min_{w \\in \\mathbb{R}^{M}} \\; \\frac{1}{2n} \\| y - Z w \\|_{2}^{2} + \\lambda \\, \\Omega(w)$，其中 $\\lambda \\ge 0$。\n- 为正则化项 $\\Omega(w)$ 指定了三种情况：(i) $\\Omega(w) = 0$，(ii) $\\Omega(w) = \\|w\\|_{2}^{2}$ 且 $\\lambda  0$，以及 (iii) $\\Omega(w) = \\|w\\|_{1}$ 且 $\\lambda  0$。\n\n该问题具有科学依据，是适定且客观的。它是统计学习和优化理论中一个标准的、形式化的问题。其前提在数学上是一致的，问题定义明确，能够基于既定的数学原理得出一个唯一且有意义的解。条件 $r  M$ 至关重要，它代表了基学习器相互关联的常见实际情况。因此，该问题被认为是有效的。\n\n第 1 部分：等价权重的刻画。\n\n两个权重向量 $w_{1} \\in \\mathbb{R}^{M}$ 和 $w_{2} \\in \\mathbb{R}^{M}$ 产生相同的堆叠预测，当且仅当 $\\hat{y}(w_{1}) = \\hat{y}(w_{2})$。根据预测器的定义，该等式为：\n$$\nZ w_{1} = Z w_{2}\n$$\n重新整理各项，我们得到：\n$$\nZ w_{1} - Z w_{2} = 0\n$$\n利用矩阵乘法的分配律，这变为：\n$$\nZ (w_{1} - w_{2}) = 0\n$$\n该方程表明，向量差 $(w_{1} - w_{2})$ 通过线性变换 $Z$ 映射到零向量。根据定义，所有这类向量的集合构成了矩阵 $Z$ 的零空间（或核），记为 $\\mathcal{N}(Z)$。因此，使 $w_{1}$ 和 $w_{2}$ 产生相同预测的充要条件是它们的差必须位于 $Z$ 的零空间中：\n$$\n(w_{1} - w_{2}) \\in \\mathcal{N}(Z)\n$$\n现在，我们来刻画能产生与给定参考权重向量 $w_{0}$ 相同预测的所有权重集合，记为 $\\mathcal{W}(w_{0})$。一个向量 $w$ 属于 $\\mathcal{W}(w_{0})$ 当且仅当 $\\hat{y}(w) = \\hat{y}(w_{0})$。根据我们推导出的条件，这等价于：\n$$\n(w - w_{0}) \\in \\mathcal{N}(Z)\n$$\n这意味着存在某个向量 $v \\in \\mathcal{N}(Z)$，使得 $w - w_{0} = v$。解出 $w$，我们发现 $\\mathcal{W}(w_{0})$ 的任何元素都可以表示为：\n$$\nw = w_{0} + v \\quad \\text{for some } v \\in \\mathcal{N}(Z)\n$$\n这明确地将集合 $\\mathcal{W}(w_{0})$ 刻画为所有通过将 $Z$ 的零空间中的任意向量加到参考向量 $w_{0}$ 上而形成的向量的集合。这个集合是 $\\mathbb{R}^{M}$ 的一个仿射子空间，可以紧凑地写为：\n$$\n\\mathcal{W}(w_{0}) = w_{0} + \\mathcal{N}(Z) = \\{ w_{0} + v \\mid v \\in \\mathcal{N}(Z) \\}\n$$\n\n第 2 部分：正则化经验风险最小化的可识别性分析。\n\n一般的目标函数是 $L(w) = \\frac{1}{2n} \\| y - Z w \\|_{2}^{2} + \\lambda \\, \\Omega(w)$。第一项，即平方误差损失，是 $w$ 的一个凸函数。其海森矩阵是 $\\nabla_{w}^{2} \\left(\\frac{1}{2n} \\| y - Z w \\|_{2}^{2}\\right) = \\frac{1}{n} Z^{T} Z$。由于 $\\operatorname{rank}(Z) = r  M$，矩阵 $Z^{T} Z \\in \\mathbb{R}^{M \\times M}$ 的秩也为 $r$。这意味着 $Z^{T} Z$ 是半正定的，但不是正定的。它的零空间与 $Z$ 的零空间相同，即 $\\mathcal{N}(Z^{T}Z) = \\mathcal{N}(Z)$，由于 $r  M$，这是一个非平凡的零空间。因此，平方误差损失项是凸的，但不是严格凸的。\n\n(i) 无正则化 ($\\lambda = 0$)：\n该问题是标准的普通最小二乘 (OLS) 问题：$\\min_{w} \\frac{1}{2n} \\| y - Z w \\|_{2}^{2}$。\n- $w^{\\star}$ 的唯一性：由于目标函数不是严格凸的，优化器 $w^{\\star}$ 不是唯一的。如果 $w^{\\star}$ 是一个最小化解，那么对于任何非零向量 $v \\in \\mathcal{N}(Z)$，向量 $w^{\\star} + v$ 也是一个最小化解。这是因为 $Z(w^{\\star} + v) = Z w^{\\star} + Zv = Z w^{\\star}$，所以目标函数的值保持不变。所有解的集合是仿射子空间 $w^{\\star}_{p} + \\mathcal{N}(Z)$，其中 $w^{\\star}_{p} = Z^{+}y$ 是由 Moore-Penrose 伪逆给出的特解。因此，$w^{\\star}$ **不是唯一的**。\n- $\\hat{y}^{\\star}$ 的唯一性：预测为 $\\hat{y}^{\\star} = Z w^{\\star}$。设 $w^{\\star}_{1}$ 和 $w^{\\star}_{2}$ 是两个不同的最优权重向量。从第 1 部分可知，$w^{\\star}_{2} - w^{\\star}_{1} \\in \\mathcal{N}(Z)$。因此，$Z w^{\\star}_{1} = Z w^{\\star}_{2}$。这表明所有最优权重向量都会导致相同的预测向量。唯一的预测 $\\hat{y}^{\\star}$ 是目标向量 $y$ 在 $Z$ 的列空间 $\\mathcal{C}(Z)$ 上的正交投影。这可以写为 $\\hat{y}^{\\star} = Z Z^{+} y$。因此，预测 $\\hat{y}^{\\star}$ 是**唯一的**。\n\n(ii) 岭正则化 ($\\Omega(w) = \\|w\\|_{2}^{2}$, $\\lambda  0$)：\n目标函数是 $L(w) = \\frac{1}{2n} \\| y - Z w \\|_{2}^{2} + \\lambda \\|w\\|_{2}^{2}$。\n- $w^{\\star}$ 的唯一性：目标函数的海森矩阵是 $\\nabla^{2} L(w) = \\frac{1}{n} Z^{T} Z + 2 \\lambda I$。矩阵 $\\frac{1}{n} Z^{T} Z$ 是半正定的。由于 $\\lambda  0$，矩阵 $2 \\lambda I$ 是正定的。一个半正定矩阵和一个正定矩阵的和是正定的。具有正定海森矩阵的函数是严格凸的。在 $\\mathbb{R}^{M}$ 上定义的严格凸函数最多只有一个全局最小值。由于该函数也是强制的（即当 $\\|w\\|_{2} \\to \\infty$ 时，$L(w) \\to \\infty$），因此存在唯一的最小化解。因此，优化器 $w^{\\star}$ 是**唯一的**。\n- $\\hat{y}^{\\star}$ 的唯一性：由于优化器 $w^{\\star}$ 是唯一的，因此得到的预测 $\\hat{y}^{\\star} = Z w^{\\star}$ 也必然是**唯一的**。岭惩罚项通过选择L2范数最小的解（在那些可能具有相似损失值的解中），有效地解决了由非平凡零空间 $\\mathcal{N}(Z)$ 引起的模糊性。\n\n(iii) LASSO 正则化 ($\\Omega(w) = \\|w\\|_{1}$, $\\lambda  0$)：\n目标函数是 $L(w) = \\frac{1}{2n} \\| y - Z w \\|_{2}^{2} + \\lambda \\|w\\|_{1}$。\n- $w^{\\star}$ 的唯一性：目标函数是凸的平方误差项和凸的L1范数项之和。因此，总和是凸的。然而，两个项都不能保证是严格凸的，并且它们的和通常也不是严格凸的。条件 $r  M$ 意味着 $Z$ 的列之间存在线性相关性。这可能导致 $w^{\\star}$ 的解不唯一。例如，如果两列相同，$Z_{i} = Z_{j}$，则损失项仅取决于和 $w_{i} + w_{j}$，而惩罚项是 $\\lambda (|w_{i}|+|w_{j}|)$。对于固定的和 $w_{i} + w_{j} = c$，当将 $w_i$ 或 $w_j$ 中的一个设为 $c$ 而另一个设为 $0$ 时，惩罚项最小化，这导致了多个解。由于 $\\operatorname{rank}(Z)  M$，总存在一个非零的 $v \\in \\mathcal{N}(Z)$，并且可以满足不唯一性的条件。因此，优化器 $w^{\\star}$ 通常**不是唯一的**。\n- $\\hat{y}^{\\star}$ 的唯一性：为了分析预测 $\\hat{y} = Z w$ 的唯一性，我们可以根据 $\\hat{y}$ 重构优化问题。设 $\\hat{y}$ 是 $Z$ 的列空间 $\\mathcal{C}(Z)$ 中的一个向量。对于任何这样的 $\\hat{y}$，产生它的权重集合是一个仿射子空间 $\\{w \\mid Zw = \\hat{y}\\}$。问题可以表示为：\n$$\n\\min_{\\hat{y} \\in \\mathcal{C}(Z)} \\left( \\frac{1}{2n} \\|y - \\hat{y}\\|_{2}^{2} + \\lambda \\min_{w: Zw=\\hat{y}} \\|w\\|_{1} \\right)\n$$\n让我们分析关于 $\\hat{y}$ 的新目标函数。项 $f(\\hat{y}) = \\frac{1}{2n} \\|y - \\hat{y}\\|_{2}^{2}$ 是 $\\hat{y}$ 的一个严格凸函数。项 $g(\\hat{y}) = \\min_{w: Zw=\\hat{y}} \\|w\\|_{1}$ 是 $\\hat{y}$ 的一个凸函数（可以证明它是 $\\mathcal{C}(Z)$ 上的一个范数）。一个严格凸函数 ($f(\\hat{y})$) 和一个凸函数 ($\\lambda g(\\hat{y})$) 的和是严格凸的。因此，关于 $\\hat{y}$ 的优化问题有唯一解。预测 $\\hat{y}^{\\star}$ 是**唯一的**。\n\n第 3 部分：等价权重集合的维度。\n\n从第 1 部分可知，能产生与参考向量 $w_{0}$ 相同预测的所有权重向量的集合是仿射子空间 $\\mathcal{W}(w_{0}) = w_{0} + \\mathcal{N}(Z)$。仿射子空间的维度定义为相应线性子空间的维度，在此情况下即为零空间 $\\mathcal{N}(Z)$ 的维度。\n我们需要求 $\\operatorname{dim}(\\mathcal{N}(Z))$。题目引导我们使用秩-零度定理。对于任何矩阵 $Z \\in \\mathbb{R}^{n \\times M}$，该定理陈述如下：\n$$\n\\operatorname{rank}(Z) + \\operatorname{nullity}(Z) = M\n$$\n其中 $\\operatorname{rank}(Z)$ 是列空间（和行空间）的维度，而 $\\operatorname{nullity}(Z) = \\operatorname{dim}(\\mathcal{N}(Z))$ 是零空间的维度。\n我们已知 $\\operatorname{rank}(Z) = r$，并且 $Z$ 的列数（对应于线性映射的定义域维度）是 $M$。将这些值代入定理中：\n$$\nr + \\operatorname{dim}(\\mathcal{N}(Z)) = M\n$$\n求解零空间的维度，我们得到：\n$$\n\\operatorname{dim}(\\mathcal{N}(Z)) = M - r\n$$\n因此，集合 $\\mathcal{W}(w_{0})$ 的维度是 $M - r$。", "answer": "$$\n\\boxed{M - r}\n$$", "id": "3175491"}]}