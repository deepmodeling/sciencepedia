{"hands_on_practices": [{"introduction": "我们首先从一个基本问题开始：SHAP 到底在解释什么？本练习 [@problem_id:3173341] 构建了一个合成数据集，其真实的数据生成过程是已知的。通过比较真实过程的 SHAP 值与一个错误设定的线性模型的 SHAP 值，我们可以清楚地看到 SHAP 解释的是*模型*的预测，而这未必与真实的潜在特征贡献一致，这是避免过度解读的关键一课。", "problem": "给定一个综合设定的场景，供您从第一性原理出发研究 Shapley 可加性解释 (Shapley Additive Explanations, SHAP)。一个模型根据三个独立的输入特征生成一个标量预测值。您必须实现一个完整的、可运行的程序，该程序能够 (i) 从已知的数据生成过程中生成数据，(ii) 通过普通最小二乘法拟合一个设定错误的线性模型，(iii) 对真实的数据生成函数，使用干预性定义计算其精确的基准 Shapley 归因，(iv) 对已拟合的线性模型，在相同的干预性语义下计算其精确的 Shapley 归因，以及 (v) 针对一组固定的测试输入，报告两者之间的量化差异。\n\n基本原理：\n- 合作博弈论中针对特征函数的 Shapley 值：对于一个模型输出函数 $f(\\mathbf{x})$，其特征索引集为 $\\mathcal{M}=\\{1,\\dots,M\\}$，以及一个实例 $\\mathbf{x}$，定义集合函数 $v(S)=\\mathbb{E}[f(X)\\mid X_S=\\mathbf{x}_S]$，其中期望是关于指定的背景分布计算的，且 $S\\subseteq\\mathcal{M}$。特征 $i$ 的 Shapley 值为\n$$\n\\phi_i(\\mathbf{x})=\\sum_{S\\subseteq \\mathcal{M}\\setminus\\{i\\}} \\frac{|S|!\\,(M-|S|-1)!}{M!}\\,\\Big(v(S\\cup\\{i\\})-v(S)\\Big).\n$$\n干预性 SHAP (Interventional SHAP) 使用边际特征分布的乘积作为背景分布，因此如果特征在数据生成过程中是独立的，条件期望会相应地分解。\n- 普通最小二乘法 (OLS) 求解 $\\min_{\\boldsymbol{\\beta}}\\sum_{n=1}^N \\big(y_n - \\beta_0 - \\sum_{j=1}^M \\beta_j x_{n,j}\\big)^2$，得到线性预测器 $\\hat f(\\mathbf{x})=\\hat\\beta_0+\\sum_{j=1}^M \\hat\\beta_j x_j$。\n\n数据生成过程：\n- 特征数量 $M=3$。所有角度均以弧度为单位。\n- 特征分布相互独立：\n  - $X_1\\sim \\mathcal{N}(\\mu_1,\\sigma_1^2)$，其中 $\\mu_1=1.0$，$\\sigma_1=2.0$。\n  - $X_2\\sim \\mathrm{Uniform}(a_2,b_2)$，其中 $a_2=-1.0$， $b_2=2.0$。\n  - $X_3\\sim \\mathrm{Exponential}(\\lambda_3)$，其率参数 $\\lambda_3=1.5$ (因此 $\\mathbb{E}[X_3]=1/\\lambda_3$)。\n- 真实回归函数与带噪声的输出：\n  - 设常数为 $c=0.7$, $w_1=3.0$, $w_2=2.5$, $w_3=1.2$ 和 $w_{12}=-1.7$。\n  - 定义真实的条件均值函数 $f^\\star(\\mathbf{x}) = c + w_1 x_1 + w_2 x_2^2 + w_3 \\sin(x_3) + w_{12} x_1 x_2$。\n  - 观测值为 $Y = f^\\star(\\mathbf{X}) + \\varepsilon$，其中 $\\varepsilon\\sim \\mathcal{N}(0,\\sigma_\\varepsilon^2)$，与 $\\mathbf{X}$ 独立，且 $\\sigma_\\varepsilon=0.3$。\n- 在 $N=20000$ 个独立训练样本上，通过 OLS 拟合设定错误的线性模型 $\\hat f(\\mathbf{x})=\\hat\\beta_0+\\hat\\beta_1 x_1+\\hat\\beta_2 x_2+\\hat\\beta_3 x_3$。\n\n干预性背景下的基准 SHAP：\n- 使用干预性定义 $v(S)=\\mathbb{E}[f^\\star(X)\\mid X_S=\\mathbf{x}_S]$ 和真实的独立特征分布。这将导出一个基线 $f^\\star_0=\\mathbb{E}[f^\\star(X)]$ 和归因 $\\boldsymbol{\\phi}^{\\mathrm{true}}(\\mathbf{x})$，它们的和为 $f^\\star(\\mathbf{x})-f^\\star_0$。\n- 您必须推导并使用在指定分布下 $\\mathbb{E}[X_2^2]$ 和 $\\mathbb{E}[\\sin(X_3)]$ 的闭式表达式。所有角度均以弧度为单位。\n\n针对已拟合线性模型的基于模型的 SHAP：\n- 在与上述相同的干预性背景（边际分布的乘积）下，为已拟合的线性预测器 $\\hat f(\\mathbf{x})$ 计算精确的 SHAP 向量 $\\boldsymbol{\\phi}^{\\mathrm{lin}}(\\mathbf{x})$。\n\n差异度量：\n- 对于任何实例 $\\mathbf{x}$，将总绝对归因差异定义为\n$$\nD(\\mathbf{x})=\\sum_{i=1}^3 \\left|\\phi^{\\mathrm{lin}}_i(\\mathbf{x}) - \\phi^{\\mathrm{true}}_i(\\mathbf{x})\\right|.\n$$\n\n测试组：\n- 使用以下四个测试输入，其中 $x_3$ 以弧度为单位指定：\n  1. $\\mathbf{x}^{(1)}=\\big(\\mu_1,\\ \\tfrac{a_2+b_2}{2},\\ \\tfrac{1}{\\lambda_3}\\big)$。\n  2. $\\mathbf{x}^{(2)}=\\big(\\mu_1+\\sigma_1,\\ b_2,\\ \\tfrac{1}{\\lambda_3}+1.0\\big)$。\n  3. $\\mathbf{x}^{(3)}=\\big(\\mu_1-2\\sigma_1,\\ a_2,\\ \\tfrac{1}{\\lambda_3}+3.0\\big)$。\n  4. $\\mathbf{x}^{(4)}=\\big(\\mu_1+0.5,\\ 0.0,\\ \\arcsin(\\mathbb{E}[\\sin(X_3)])\\big)$，其中 $\\arcsin$ 表示在 $[0,\\tfrac{\\pi}{2}]$ 内的主值。\n\n要求的计算与输出：\n- 使用固定的随机种子生成训练数据，以确保确定性。\n- 通过 OLS 拟合线性模型。\n- 对于每个测试输入 $\\mathbf{x}^{(k)}$，在相同的干预性背景下，计算基准干预性 SHAP 向量 $\\boldsymbol{\\phi}^{\\mathrm{true}}(\\mathbf{x}^{(k)})$ 和线性模型的 SHAP 向量 $\\boldsymbol{\\phi}^{\\mathrm{lin}}(\\mathbf{x}^{(k)})$。\n- 对于每个测试输入，计算如上定义的 $D(\\mathbf{x}^{(k)})$。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含四个差异值，以逗号分隔的列表形式，并用方括号括起来，顺序与四个测试输入相对应，即形如 $[d_1,d_2,d_3,d_4]$ 的字符串，其中每个 $d_k$ 是一个浮点数。", "solution": "该问题要求对一个真实的、非线性的数据生成过程和一个根据该过程数据拟合的设定错误的线性模型，对其 Shapley 可加性解释 (SHAP) 进行量化比较。任务的核心是为真实函数和线性近似推导并实现干预性 SHAP 值的精确公式。\n\n首先，我们通过推导必要的期望值来建立理论基础，这些期望值构成了 SHAP 计算的基线。其次，我们基于真实模型函数 $f^\\star(\\mathbf{x})$ 推导出基准 SHAP 值 $\\boldsymbol{\\phi}^{\\mathrm{true}}(\\mathbf{x})$ 的闭式表达式。再次，我们为拟合的普通最小二乘法 (OLS) 线性模型 $\\hat{f}(\\mathbf{x})$ 推导出相应的 SHAP 值 $\\boldsymbol{\\phi}^{\\mathrm{lin}}(\\mathbf{x})$。最后，我们概述了为给定测试组获取指定差异度量的完整计算过程。\n\n干预性 SHAP 框架要求在背景分布上计算期望，该背景分布被定义为各独立特征边际分布的乘积。特征分布给定如下：\n- $X_1 \\sim \\mathcal{N}(\\mu_1, \\sigma_1^2)$，其中 $\\mu_1=1.0$，$\\sigma_1=2.0$。\n- $X_2 \\sim \\mathrm{Uniform}(a_2, b_2)$，其中 $a_2=-1.0$， $b_2=2.0$。\n- $X_3 \\sim \\mathrm{Exponential}(\\lambda_3)$，其中 $\\lambda_3=1.5$。\n真实的数据生成函数是 $f^\\star(\\mathbf{x}) = c + w_1 x_1 + w_2 x_2^2 + w_3 \\sin(x_3) + w_{12} x_1 x_2$。SHAP 的计算需要函数中各项的期望值。\n\n一阶矩为：\n- $\\mathbb{E}[X_1] = \\mu_1 = 1.0$。\n- $\\mathbb{E}[X_2] = \\frac{a_2+b_2}{2} = \\frac{-1.0+2.0}{2} = 0.5$。\n- $\\mathbb{E}[X_3] = \\frac{1}{\\lambda_3} = \\frac{1}{1.5} = \\frac{2}{3}$。\n我们还需要 $f^\\star(\\mathbf{x})$ 中非线性项的期望：\n- $\\mathbb{E}[X_2^2]$：对于均匀分布 $\\mathrm{U}(a, b)$，其二阶矩为 $\\mathbb{E}[X^2] = \\frac{1}{b-a} \\int_a^b x^2 dx = \\frac{b^2+ab+a^2}{3}$。\n$$ \\mathbb{E}[X_2^2] = \\frac{(2.0)^2 + (2.0)(-1.0) + (-1.0)^2}{3} = \\frac{4.0 - 2.0 + 1.0}{3} = \\frac{3.0}{3.0} = 1.0. $$\n- $\\mathbb{E}[\\sin(X_3)]$：对于率参数为 $\\lambda_3$ 的指数分布，其概率密度函数 (PDF) 为 $p(x) = \\lambda_3 e^{-\\lambda_3 x}$（当 $x \\ge 0$ 时）。该期望通过 $\\sin(x)$ 的拉普拉斯变换计算：\n$$ \\mathbb{E}[\\sin(X_3)] = \\int_0^\\infty \\sin(x) \\lambda_3 e^{-\\lambda_3 x} dx = \\lambda_3 \\int_0^\\infty \\sin(x) e^{-\\lambda_3 x} dx. $$\n该积分是 $\\sin(t)$（其中 $a=1$）的拉普拉斯变换，在 $s=\\lambda_3$ 处求值，结果为 $\\frac{1}{s^2+1}$。\n$$ \\mathbb{E}[\\sin(X_3)] = \\lambda_3 \\left( \\frac{1}{\\lambda_3^2+1} \\right) = \\frac{1.5}{1.5^2+1} = \\frac{1.5}{3.25} = \\frac{6}{13}. $$\n这些值对于计算两个模型的基线（期望）预测至关重要。\n\nSHAP 值由 $\\phi_i(\\mathbf{x}) = \\sum_{S\\subseteq \\mathcal{M}\\setminus\\{i\\}} \\frac{|S|!\\,(M-|S|-1)!}{M!}\\,\\big(v(S\\cup\\{i\\})-v(S)\\big)$ 定义，其中 $v(S)=\\mathbb{E}[f^\\star(X)\\mid X_S=\\mathbf{x}_S]$。由于特征独立，这可以简化为对不在 $S$ 中的特征求期望。SHAP 的一个关键特性是可加性：如果 $f = g_1 + g_2$，则 $\\phi_i(f) = \\phi_i(g_1) + \\phi_i(g_2)$。我们将 $f^\\star(\\mathbf{x})$ 分解如下：\n$$ f^\\star(\\mathbf{x}) = c + \\underbrace{w_1 x_1}_{f_1(x_1)} + \\underbrace{w_2 x_2^2}_{f_2(x_2)} + \\underbrace{w_3 \\sin(x_3)}_{f_3(x_3)} + \\underbrace{w_{12} x_1 x_2}_{f_{12}(x_1, x_2)}. $$\n我们为每个分量计算 SHAP 值。对于单变量函数 $f_i(x_i)$，唯一非零的 SHAP 值是针对特征 $i$ 的：$\\phi_i(f_i) = f_i(x_i) - \\mathbb{E}[f_i(X_i)]$。对于交互项 $g(x_1, x_2) = x_1 x_2$，在 3 特征空间中，从 Shapley 公式推导出的特征 1 和 2 的归因是：\n$$ \\phi_1(g) = \\frac{1}{2}(x_1-\\mathbb{E}[X_1])(x_2+\\mathbb{E}[X_2]), \\quad \\phi_2(g) = \\frac{1}{2}(x_2-\\mathbb{E}[X_2])(x_1+\\mathbb{E}[X_1]). $$\n利用可加性结合这些结果，我们得到 $f^\\star$ 的精确 SHAP 值：\n- $\\phi_1^{\\mathrm{true}}(\\mathbf{x}) = \\phi_1(f_1) + \\phi_1(f_{12}) = w_1(x_1 - \\mathbb{E}[X_1]) + \\frac{w_{12}}{2}(x_1 - \\mathbb{E}[X_1])(x_2 + \\mathbb{E}[X_2])$.\n- $\\phi_2^{\\mathrm{true}}(\\mathbf{x}) = \\phi_2(f_2) + \\phi_2(f_{12}) = w_2(x_2^2 - \\mathbb{E}[X_2^2]) + \\frac{w_{12}}{2}(x_2 - \\mathbb{E}[X_2])(x_1 + \\mathbb{E}[X_1])$.\n- $\\phi_3^{\\mathrm{true}}(\\mathbf{x}) = \\phi_3(f_3) = w_3(\\sin(x_3) - \\mathbb{E}[\\sin(X_3)])$.\n这些方程提供了基准归因，我们将用它与线性模型的归因进行比较。\n\n设定错误的模型是一个通过 OLS 拟合的线性函数：$\\hat{f}(\\mathbf{x}) = \\hat\\beta_0 + \\hat\\beta_1 x_1 + \\hat\\beta_2 x_2 + \\hat\\beta_3 x_3$。系数 $(\\hat\\beta_0, \\hat\\beta_1, \\hat\\beta_2, \\hat\\beta_3)$ 是从 $N=20000$ 个训练样本中估计出来的。对于具有独立特征的线性模型，干预性 SHAP 值具有简单、精确的形式。总贡献 $ \\hat{f}(\\mathbf{x}) - \\mathbb{E}[\\hat{f}(X)] $ 为：\n$$ (\\hat\\beta_0 + \\sum_{j=1}^3 \\hat\\beta_j x_j) - (\\hat\\beta_0 + \\sum_{j=1}^3 \\hat\\beta_j \\mathbb{E}[X_j]) = \\sum_{j=1}^3 \\hat\\beta_j(x_j - \\mathbb{E}[X_j]). $$\n该表达式已经是一个各项之和，其中每一项仅依赖于单个特征。这种唯一的分解直接给出了 SHAP 值：\n$$ \\phi_i^{\\mathrm{lin}}(\\mathbf{x}) = \\hat\\beta_i(x_i - \\mathbb{E}[X_i]). $$\n具体到我们的 3 特征模型：\n- $\\phi_1^{\\mathrm{lin}}(\\mathbf{x}) = \\hat\\beta_1(x_1 - \\mathbb{E}[X_1])$.\n- $\\phi_2^{\\mathrm{lin}}(\\mathbf{x}) = \\hat\\beta_2(x_2 - \\mathbb{E}[X_2])$.\n- $\\phi_3^{\\mathrm{lin}}(\\mathbf{x}) = \\hat\\beta_3(x_3 - \\mathbb{E}[X_3])$.\n这些公式表明，线性模型错误地假设一个特征的归因仅仅是其中心化后的值乘以一个单一系数，而忽略了真实函数 $f^\\star$ 中存在的任何非线性或交互作用。\n\n实现过程如下：\n$1$. **数据生成**：从为 $X_1$、$X_2$ 和 $X_3$ 指定的独立分布中生成 $N=20000$ 个样本。\n$2$. **真实输出**：计算真实函数值 $f^\\star(\\mathbf{X})$ 并添加高斯噪声 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma_\\varepsilon^2)$（其中 $\\sigma_\\varepsilon=0.3$）以获得观测输出 $Y$。\n$3$. **OLS 拟合**：构建设计矩阵 $\\mathbf{X}_{\\mathrm{b}}$，其中包含一列 1 作为截距项。求解正规方程 $\\mathbf{X}_{\\mathrm{b}}^T \\mathbf{X}_{\\mathrm{b}} \\hat{\\boldsymbol{\\beta}} = \\mathbf{X}_{\\mathrm{b}}^T Y$ 以找到 OLS 系数向量 $\\hat{\\boldsymbol{\\beta}} = (\\hat\\beta_0, \\hat\\beta_1, \\hat\\beta_2, \\hat\\beta_3)^T$。\n$4$. **SHAP 计算**：对于所提供测试组中的每个测试输入 $\\mathbf{x}^{(k)}$：\n    - 使用推导出的公式和已知参数（$w_i$ 等）计算基准 SHAP 向量 $\\boldsymbol{\\phi}^{\\mathrm{true}}(\\mathbf{x}^{(k)})$。\n    - 使用拟合的系数 $\\hat\\beta_i$ 计算线性模型的 SHAP 向量 $\\boldsymbol{\\phi}^{\\mathrm{lin}}(\\mathbf{x}^{(k)})$。\n$5$. **差异度量**：为每个测试输入计算总绝对归因差异：\n$$ D(\\mathbf{x}^{(k)}) = \\sum_{i=1}^3 \\left|\\phi^{\\mathrm{lin}}_i(\\mathbf{x}^{(k)}) - \\phi^{\\mathrm{true}}_i(\\mathbf{x}^{(k)})\\right|. $$\n程序将执行这些步骤，并报告四个测试用例计算出的 $D(\\mathbf{x}^{(k)})$ 值。固定的随机种子可确保结果的确定性和可复现性。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements the full procedure to calculate SHAP value discrepancies.\n    \"\"\"\n    # Use a fixed random seed for reproducibility\n    SEED = 0\n    rng = np.random.default_rng(SEED)\n\n    # ----------------------------------------------------------------------\n    # 1. Define constants and parameters from the problem statement\n    # ----------------------------------------------------------------------\n    # Data-generating process parameters\n    N = 20000\n    mu1, sigma1 = 1.0, 2.0\n    a2, b2 = -1.0, 2.0\n    lambda3 = 1.5\n    \n    # True function parameters\n    c = 0.7\n    w1 = 3.0\n    w2 = 2.5\n    w3 = 1.2\n    w12 = -1.7\n    \n    # Noise parameter\n    sigma_eps = 0.3\n\n    # ----------------------------------------------------------------------\n    # 2. Generate training data\n    # ----------------------------------------------------------------------\n    X1_train = rng.normal(mu1, sigma1, N)\n    X2_train = rng.uniform(a2, b2, N)\n    X3_train = rng.exponential(1.0 / lambda3, N)\n    X_train = np.stack([X1_train, X2_train, X3_train], axis=1)\n\n    f_star = c + w1*X1_train + w2*X2_train**2 + w3*np.sin(X3_train) + w12*X1_train*X2_train\n    Y_train = f_star + rng.normal(0, sigma_eps, N)\n\n    # ----------------------------------------------------------------------\n    # 3. Fit the misspecified linear model by OLS\n    # ----------------------------------------------------------------------\n    X_b = np.c_[np.ones(N), X_train]\n    # Solve X_b.T @ X_b @ beta_hat = X_b.T @ Y_train\n    beta_hat = np.linalg.solve(X_b.T @ X_b, X_b.T @ Y_train)\n    beta0_hat, beta1_hat, beta2_hat, beta3_hat = beta_hat\n\n    # ----------------------------------------------------------------------\n    # 4. Define background expectations and test suite\n    # ----------------------------------------------------------------------\n    E_X1 = mu1\n    E_X2 = (a2 + b2) / 2.0\n    E_X3 = 1.0 / lambda3\n    E_X2_sq = 1.0  # Derived as (b2^3 - a2^3) / (3*(b2-a2)) = 1.0\n    E_sinX3 = lambda3 / (lambda3**2 + 1.0) # Derived from Laplace transform\n\n    test_cases = [\n        (mu1, (a2 + b2) / 2.0, 1.0 / lambda3),\n        (mu1 + sigma1, b2, 1.0 / lambda3 + 1.0),\n        (mu1 - 2 * sigma1, a2, 1.0 / lambda3 + 3.0),\n        (mu1 + 0.5, 0.0, np.arcsin(E_sinX3))\n    ]\n\n    discrepancies = []\n    \n    # ----------------------------------------------------------------------\n    # 5. Calculate SHAP values and discrepancies for each test case\n    # ----------------------------------------------------------------------\n    for x_test in test_cases:\n        x1, x2, x3 = x_test\n\n        # --- Ground-truth SHAP values ---\n        phi1_true = w1 * (x1 - E_X1) + (w12 / 2.0) * (x1 - E_X1) * (x2 + E_X2)\n        phi2_true = w2 * (x2**2 - E_X2_sq) + (w12 / 2.0) * (x2 - E_X2) * (x1 + E_X1)\n        phi3_true = w3 * (np.sin(x3) - E_sinX3)\n        \n        # --- Linear model SHAP values ---\n        phi1_lin = beta1_hat * (x1 - E_X1)\n        phi2_lin = beta2_hat * (x2 - E_X2)\n        phi3_lin = beta3_hat * (x3 - E_X3)\n        \n        # --- Total absolute attribution discrepancy ---\n        D_x = (\n            np.abs(phi1_lin - phi1_true) + \n            np.abs(phi2_lin - phi2_true) + \n            np.abs(phi3_lin - phi3_true)\n        )\n        discrepancies.append(D_x)\n\n    # ----------------------------------------------------------------------\n    # 6. Format and print the final output\n    # ----------------------------------------------------------------------\n    print(f\"[{','.join(f'{d:.7f}' for d in discrepancies)}]\")\n\nsolve()\n\n```", "id": "3173341"}, {"introduction": "在理解了 SHAP 解释的是模型之后，我们现在将其作为一个强大的诊断工具来使用。这个实践 [@problem_id:3173394] 探讨了表示周期性特征（如一年中的月份）时遇到的常见挑战。我们将看到，简单的序数编码会导致误导性的 SHAP 值，而正确的正弦-余弦编码则能解决这些问题，产生更合乎逻辑和平衡的特征归因。", "problem": "要求您实现一个可复现的实验，以演示当 Shapley 加性解释 (SHAP) 用于周期性特征的序数编码时可能产生的误导，以及周期性正弦-余弦编码如何恢复平衡的归因。核心对象是一个确定性模型，它将日历月份映射到一个标量输出。请使用以下规范。\n\n- 基本原理：使用合作博弈论中 Shapley 值的定义以及 Shapley 加性解释 (SHAP) 中针对缺失特征的条件期望语义。具体来说，值函数使用给定已观测特征子集的模型输出的条件期望，而一个特征的 Shapley 值是其在所有可能的子集和排列上的平均边际贡献。不要假设任何特设的简化公式；从定义和基本对称性出发构建推理。\n\n- 数据集和目标：构建月份集合 $\\{1,2,\\dots,12\\}$。将每个月份 $m$ 与一个以弧度为单位的角度 $\\theta_m = 2\\pi m / 12$ 关联。定义一个确定性基准真相模型\n$$\nf(m) = A \\sin(\\theta_m) + B \\cos(\\theta_m),\n$$\n其中 $A = 2$ 且 $B = 1$。\n\n- 两种编码和模型：\n  1. 序数编码：将月份视为单个整型特征 $m$，并对 $m \\in \\{1,\\dots,12\\}$ 上的基准真相输出 $f(m)$ 拟合一个最小二乘线性模型 $g(m) = \\alpha m + \\beta$。在具有单个特征的 SHAP 框架中，实例 $m$ 的 Shapley 值是模型在 $m$ 处的输出与在月份经验分布下的基线期望之间的差值，即\n  $$\n  \\phi_{\\text{ord}}(m) = g(m) - \\mathbb{E}[g(M)],\n  $$\n  其中 $M$ 在 $\\{1,\\dots,12\\}$ 上均匀分布。\n  2. 周期性正弦-余弦编码：用两个特征 $s_m = \\sin(\\theta_m)$ 和 $c_m = \\cos(\\theta_m)$ 表示每个月份 $m$，并使用线性模型 $h(s,c) = A s + B c$。对于 SHAP，使用基于月份经验均匀分布的条件期望语义。Shapley 值 $\\phi_s(m)$ 和 $\\phi_c(m)$ 应通过联盟 $\\{\\}$、$\\{s\\}$、$\\{c\\}$ 和 $\\{s,c\\}$ 的条件期望和边际贡献，利用 $\\sin$ 和 $\\cos$ 在 12 个等距角度上的对称性，从第一性原理进行计算。\n\n- 角度单位：所有角度必须以弧度为单位。\n\n- 测试套件：计算以下量以评估误解和归因平衡。对所有期望值，使用在 $\\{1,\\dots,12\\}$ 上的均匀经验月份分布。\n  1. 跨循环边界的邻接误解检查：计算序数模型下边界处相邻月份的绝对 SHAP 差距，\n  $$\n  \\Delta_{\\text{ord}} = \\left|\\phi_{\\text{ord}}(12) - \\phi_{\\text{ord}}(1)\\right|,\n  $$\n  并与周期性模型的 SHAP 和差距进行比较，\n  $$\n  \\Delta_{\\text{cyc}} = \\left|(\\phi_s(12)+\\phi_c(12)) - (\\phi_s(1)+\\phi_c(1))\\right|.\n  $$\n  输出一个布尔值，指示 $\\Delta_{\\text{ord}} > \\Delta_{\\text{cyc}}$ 是否成立。\n  2. 对于正弦值相同但余弦值不同的月份（$m=1$ 和 $m=5$）的等正弦值归因平衡：输出两个布尔值，\n     - $\\phi_s(1)$ 是否在数值容差 $10^{-12}$ 内等于 $\\phi_s(5)$，以及\n     - $\\phi_c(1)$ 和 $\\phi_c(5)$ 是否在数值容差 $10^{-12}$ 内大小相等、符号相反；即 $\\phi_c(1)+\\phi_c(5)$ 是否在数值上为零。\n  3. 在一个正弦主导的月份（$m=3$，角度 $\\theta_3 = \\pi/2$）的明确 SHAP 分量：以浮点数形式输出 $\\phi_s(3)$ 和 $\\phi_c(3)$。\n  4. 在一个余弦主导的月份（$m=6$，角度 $\\theta_6 = \\pi$）的明确 SHAP 分量：以浮点数形式输出 $\\phi_s(6)$ 和 $\\phi_c(6)$。\n  5. SHAP 和一致性：对于月份 $m \\in \\{1,3,5,6,12\\}$，验证 $\\phi_s(m)+\\phi_c(m)$ 在容差 $10^{-12}$ 内等于 $f(m)$（回顾一下，根据对称性，基线期望 $\\mathbb{E}[f(M)]$ 为零）。输出一个布尔值，该值当且仅当此一致性对所有五个月份都成立时为真。\n  6. 邻接边界的误解比率：输出浮点数\n  $$\n  R = \\frac{\\Delta_{\\text{ord}}}{\\max(\\Delta_{\\text{cyc}}, 10^{-12})}.\n  $$\n\n- 最终输出格式：您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果，顺序完全如下：\n  $$\n  [\\text{adjacency\\_boolean},\\ \\text{equal\\_sine\\_boolean},\\ \\text{cos\\_opposite\\_boolean},\\ \\phi_s(3),\\ \\phi_c(3),\\ \\phi_s(6),\\ \\phi_c(6),\\ \\text{sum\\_consistency\\_boolean},\\ R].\n  $$\n所有数值输出必须是标准浮点格式，不带单位；所有布尔值必须是标准逻辑值。", "solution": "问题陈述经验证是科学上合理的、良定的、客观且完整的。它提出了一个严谨的练习，旨在应用 Shapley 加性解释 (SHAP) 的基本原理，以演示周期性变量特征工程中的一个已知问题。我们将给出完整的解法。\n\n问题的核心是计算和比较周期性特征（日历月份）的两种不同表示方法的 SHAP 值。基准真相模型是月份 $m \\in \\{1, 2, \\dots, 12\\}$ 的确定性函数：\n$$\nf(m) = A \\sin(\\theta_m) + B \\cos(\\theta_m)\n$$\n其中 $A = 2$，$B = 1$，且 $\\theta_m = 2\\pi m / 12$。月份的经验分布在 $\\{1, \\dots, 12\\}$ 上是均匀的。\n\n### 序数编码模型分析\n\n首先，我们考虑序数编码，其中月份被视为单个整型特征 $m$。使用普通最小二乘法将一个简单的线性模型 $g(m) = \\alpha m + \\beta$ 拟合到基准真相数据 $(m, f(m))$（$m=1, \\dots, 12$）。\n\n系数 $\\alpha$ 和 $\\beta$ 由下式给出：\n$$\n\\alpha = \\frac{\\sum_{m=1}^{12} (m - \\bar{m})(f(m) - \\bar{f})}{\\sum_{m=1}^{12} (m - \\bar{m})^2}, \\quad \\beta = \\bar{f} - \\alpha \\bar{m}\n$$\n此处，$\\bar{m} = \\mathbb{E}[M] = \\frac{1}{12}\\sum_{m=1}^{12} m = 6.5$。基准真相函数的平均值 $\\bar{f} = \\mathbb{E}[f(M)]$ 为：\n$$\n\\bar{f} = \\frac{1}{12}\\sum_{m=1}^{12} \\left(A \\sin\\left(\\frac{2\\pi m}{12}\\right) + B \\cos\\left(\\frac{2\\pi m}{12}\\right)\\right) = 0\n$$\n这是因为正弦和余弦在一组等距分布点上的完整周期内的和为零。\n当 $\\bar{f}=0$ 时，系数简化为：\n$$\n\\alpha = \\frac{\\sum_{m=1}^{12} (m - 6.5)f(m)}{\\sum_{m=1}^{12} (m - 6.5)^2}, \\quad \\beta = -\\alpha \\bar{m} = -6.5\\alpha\n$$\n分母是一个标准和，$\\sum_{m=1}^{12} (m - 6.5)^2 = 143$。分子可以进行符号计算：\n$$\n\\sum_{m=1}^{12} m f(m) = A \\sum_{m=1}^{12} m \\sin\\left(\\frac{2\\pi m}{12}\\right) + B \\sum_{m=1}^{12} m \\cos\\left(\\frac{2\\pi m}{12}\\right)\n$$\n使用已知的有限三角和恒等式，$\\sum_{k=1}^{N} k \\sin(2\\pi k/N) = -N/2 \\cot(\\pi/N)$ 和 $\\sum_{k=1}^{N} k \\cos(2\\pi k/N) = N/2$。对于 $N=12$，$A=2$ 和 $B=1$：\n$$\n\\sum_{m=1}^{12} m f(m) = 2 \\left(-6 \\cot\\left(\\frac{\\pi}{12}\\right)\\right) + 1 \\left(\\frac{12}{2}\\right) = -12(2+\\sqrt{3}) + 6 = -24 - 12\\sqrt{3} + 6 = -18 - 12\\sqrt{3}\n$$\n由于 $\\sum f(m)=0$，分子为 $\\sum (m-6.5)f(m) = \\sum m f(m) = -18 - 12\\sqrt{3}$。\n$$\n\\alpha = \\frac{-18 - 12\\sqrt{3}}{143} \\approx -0.27122\n$$\n单特征序数模型的 SHAP 值为 $\\phi_{\\text{ord}}(m) = g(m) - \\mathbb{E}[g(M)]$。\n$$\n\\mathbb{E}[g(M)] = \\mathbb{E}[\\alpha M + \\beta] = \\alpha\\mathbb{E}[M] + \\beta = \\alpha\\bar{m} + \\beta = \\alpha\\bar{m} + (\\bar{f} - \\alpha\\bar{m}) = \\bar{f} = 0\n$$\n因此，$\\phi_{\\text{ord}}(m) = g(m) = \\alpha m + \\beta$。循环边界处的邻接差距为：\n$$\n\\Delta_{\\text{ord}} = |\\phi_{\\text{ord}}(12) - \\phi_{\\text{ord}}(1)| = |(12\\alpha+\\beta) - (\\alpha+\\beta)| = |11\\alpha| = 11 \\frac{18 + 12\\sqrt{3}}{143} = \\frac{18 + 12\\sqrt{3}}{13} \\approx 2.9834\n$$\n这个巨大的差距反映了模型未能理解月份 12 与月份 1 相邻。\n\n### 周期性正弦-余弦编码模型分析\n\n接下来，我们使用双特征编码：$s_m = \\sin(\\theta_m)$ 和 $c_m = \\cos(\\theta_m)$。模型为 $h(s, c) = As + Bc$，这本身就是基准真相模型。我们从第一性原理计算实例 $(s_m, c_m)$ 的 SHAP 值 $\\phi_s(m)$ 和 $\\phi_c(m)$。\n\n特征（参与者）为 $F=\\{s, c\\}$。Shapley 值为：\n$$\n\\phi_s(m) = \\frac{1}{2}\\left(v(\\{s\\}) - v(\\emptyset)\\right) + \\frac{1}{2}\\left(v(\\{s,c\\}) - v(\\{c\\})\\right)\n$$\n$$\n\\phi_c(m) = \\frac{1}{2}\\left(v(\\{c\\}) - v(\\emptyset)\\right) + \\frac{1}{2}\\left(v(\\{s,c\\}) - v(\\{s\\})\\right)\n$$\n值函数 $v(S)$ 是模型输出的条件期望，其中联盟 $S$ 中的特征被固定为该实例的值，而其他特征则在其条件分布上取平均。该分布在 12 个月上是均匀的。\n\n1.  $v(\\emptyset) = \\mathbb{E}[h(S,C)] = \\mathbb{E}[f(M)] = 0$，如前所示。\n2.  $v(\\{s,c\\}) = \\mathbb{E}[h(S,C) | s=s_m, c=c_m] = h(s_m, c_m) = A s_m + B c_m = f(m)$。\n3.  $v(\\{s\\}) = \\mathbb{E}[h(S,C) | s=s_m] = A s_m + B \\mathbb{E}[C | s=s_m]$。由于对于具有相同正弦值的角度（即 $\\theta$ 和 $\\pi-\\theta$），余弦函数具有对称性，因此对于所有 $m$，我们有 $\\mathbb{E}[C | s=s_m] = 0$。因此，$v(\\{s\\}) = A s_m$。\n4.  $v(\\{c\\}) = \\mathbb{E}[h(S,C) | c=c_m] = A \\mathbb{E}[S | c=c_m] + B c_m$。由于对于具有相同余弦值的角度（即 $\\theta$ 和 $2\\pi-\\theta$），正弦函数具有对称性，因此对于所有 $m$，我们有 $\\mathbb{E}[S | c=c_m] = 0$。因此，$v(\\{c\\}) = B c_m$。\n\n将这些值代入 Shapley 公式：\n$$\n\\phi_s(m) = \\frac{1}{2}(A s_m - 0) + \\frac{1}{2}((A s_m + B c_m) - B c_m) = A s_m\n$$\n$$\n\\phi_c(m) = \\frac{1}{2}(B c_m - 0) + \\frac{1}{2}((A s_m + B c_m) - A s_m) = B c_m\n$$\nSHAP 值就是线性模型的各个项：$\\phi_s(m) = 2 \\sin(\\theta_m)$ 和 $\\phi_c(m) = \\cos(\\theta_m)$。\nSHAP 归因之和为 $\\phi_s(m) + \\phi_c(m) = A s_m + B c_m = f(m)$。由于 $\\mathbb{E}[f]=0$，这符合 SHAP 的性质 $\\sum \\phi_i = f(x) - \\mathbb{E}[f]$。\n\n周期性模型的邻接差距为：\n$$\n\\Delta_{\\text{cyc}} = |(\\phi_s(12)+\\phi_c(12)) - (\\phi_s(1)+\\phi_c(1))| = |f(12) - f(1)|\n$$\n$$\nf(12) = 2\\sin(2\\pi) + \\cos(2\\pi) = 1\n$$\n$$\nf(1) = 2\\sin(\\pi/6) + \\cos(\\pi/6) = 2(1/2) + \\sqrt{3}/2 = 1+\\sqrt{3}/2\n$$\n$$\n\\Delta_{\\text{cyc}} = |1 - (1+\\sqrt{3}/2)| = \\sqrt{3}/2 \\approx 0.8660\n$$\n\n### 测试套件评估\n\n我们现在计算所需的量。\n\n1.  **邻接误解**：$\\Delta_{\\text{ord}} \\approx 2.9834$ 且 $\\Delta_{\\text{cyc}} \\approx 0.8660$。由于 $2.9834 > 0.8660$，输出为 `True`。序数模型在年份边界处产生了一个巨大的人为间断点。\n2.  **等正弦值归因**：对于 $m=1$（$\\theta=\\pi/6$）和 $m=5$（$\\theta=5\\pi/6$），$\\sin(\\theta_1)=\\sin(\\theta_5)=1/2$。\n    $\\phi_s(1) = A s_1 = 2(1/2) = 1$。$\\phi_s(5) = A s_5 = 2(1/2) = 1$。它们相等，因此输出为 `True`。\n3.  **反符号余弦值归因**：对于 $m=1, 5$，$\\cos(\\theta_1)=\\sqrt{3}/2$ 且 $\\cos(\\theta_5)=-\\sqrt{3}/2$。\n    $\\phi_c(1) = B c_1 = \\sqrt{3}/2$。$\\phi_c(5) = B c_5 = -\\sqrt{3}/2$。它们的和为 0，因此输出为 `True`。归因正确地反映了特征之间的关系。\n4.  **$m=3$ 处的 SHAP 分量**：$\\theta_3=\\pi/2$，因此 $s_3=1, c_3=0$。\n    $\\phi_s(3) = A s_3 = 2(1) = 2.0$。\n    $\\phi_c(3) = B c_3 = 1(0) = 0.0$。\n5.  **$m=6$ 处的 SHAP 分量**：$\\theta_6=\\pi$，因此 $s_6=0, c_6=-1$。\n    $\\phi_s(6) = A s_6 = 2(0) = 0.0$。\n    $\\phi_c(6) = B c_6 = 1(-1) = -1.0$。\n6.  **SHAP 和一致性**：如前所示，对于所有 $m$，$\\phi_s(m) + \\phi_c(m) = f(m)$。此检查将对所有指定月份通过。输出为 `True`。\n7.  **误解比率**：\n    $$\n    R = \\frac{\\Delta_{\\text{ord}}}{\\Delta_{\\text{cyc}}} = \\frac{(18 + 12\\sqrt{3})/13}{\\sqrt{3}/2} = \\frac{2(18 + 12\\sqrt{3})}{13\\sqrt{3}} = \\frac{36\\sqrt{3} + 72}{39} = \\frac{12\\sqrt{3} + 24}{13} \\approx 3.4450\n    $$\n    序数模型的边界差距比周期性模型的大 3.4 倍以上，这量化了这种误解。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements a reproducible experiment on SHAP values for cyclic features.\n    \"\"\"\n    # Define constants from the problem statement\n    A = 2.0\n    B = 1.0\n    TOL = 1e-12\n\n    # Construct the base dataset\n    months = np.arange(1, 13)\n    thetas = 2 * np.pi * months / 12\n    f_vals = A * np.sin(thetas) + B * np.cos(thetas)\n\n    # --- Ordinal Model Analysis ---\n    # Fit a least-squares linear model g(m) = alpha*m + beta\n    alpha, beta = np.polyfit(months, f_vals, 1)\n\n    # The SHAP value is phi_ord(m) = g(m) - E[g(M)].\n    # E[g(M)] = E[alpha*M + beta] = alpha*E[M] + beta.\n    # Since beta = mean(f) - alpha*mean(m) and mean(f) is numerically ~0,\n    # E[g(M)] is also numerically ~0.\n    # Therefore, phi_ord(m) is approximately g(m).\n    phi_ord_12 = alpha * 12 + beta\n    phi_ord_1 = alpha * 1 + beta\n    delta_ord = np.abs(phi_ord_12 - phi_ord_1)\n\n    # --- Cyclic Sine-Cosine Model Analysis ---\n    # Features are s_m = sin(theta_m) and c_m = cos(theta_m)\n    # The model is h(s,c) = A*s + B*c.\n    # As derived in the solution, due to feature distribution symmetries,\n    # the SHAP values are phi_s(m) = A*s_m and phi_c(m) = B*c_m.\n    s_vals = np.sin(thetas)\n    c_vals = np.cos(thetas)\n\n    phi_s = A * s_vals\n    phi_c = B * c_vals\n    \n    # Month indices for numpy arrays (0-indexed)\n    m1_idx, m3_idx, m5_idx, m6_idx, m12_idx = 0, 2, 4, 5, 11\n\n    # SHAP sum for the cyclic model\n    phi_sum_cyclic_12 = phi_s[m12_idx] + phi_c[m12_idx]\n    phi_sum_cyclic_1 = phi_s[m1_idx] + phi_c[m1_idx]\n    delta_cyc = np.abs(phi_sum_cyclic_12 - phi_sum_cyclic_1)\n\n    # --- Test Suite Evaluation ---\n\n    # 1. Adjacency misinterpretation check\n    adjacency_boolean = delta_ord > delta_cyc\n\n    # 2. Equal-sine attribution balance for m=1 and m=5\n    equal_sine_boolean = np.isclose(phi_s[m1_idx], phi_s[m5_idx], atol=TOL)\n\n    # 3. Opposite-cosine attribution balance for m=1 and m=5\n    cos_opposite_boolean = np.isclose(phi_c[m1_idx] + phi_c[m5_idx], 0, atol=TOL)\n\n    # 4. Explicit SHAP components at m=3\n    phi_s_3 = phi_s[m3_idx]\n    phi_c_3 = phi_c[m3_idx]\n\n    # 5. Explicit SHAP components at m=6\n    phi_s_6 = phi_s[m6_idx]\n    phi_c_6 = phi_c[m6_idx]\n\n    # 6. SHAP-sum consistency\n    test_months_indices = [m1_idx, m3_idx, m5_idx, m6_idx, m12_idx]\n    shap_sums = phi_s[test_months_indices] + phi_c[test_months_indices]\n    f_vals_test = f_vals[test_months_indices]\n    sum_consistency_boolean = np.all(np.isclose(shap_sums, f_vals_test, atol=TOL))\n    \n    # 7. Misinterpretation ratio\n    R = delta_ord / max(delta_cyc, 1e-12)\n\n    # Assemble final results list\n    results = [\n        adjacency_boolean,\n        equal_sine_boolean,\n        cos_opposite_boolean,\n        phi_s_3,\n        phi_c_3,\n        phi_s_6,\n        phi_c_6,\n        sum_consistency_boolean,\n        R\n    ]\n\n    # Format and print the final output\n    # Booleans are lowercased for standard Python str() conversion\n    formatted_results = [str(r).lower() if isinstance(r, bool) else f\"{r:.10f}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3173394"}, {"introduction": "最后，我们来解决建模中最棘手的问题之一：多重共线性。本练习 [@problem_id:3173371] 探究了 TreeSHAP 如何在决策树模型中为两个高度相关的特征分配贡献。通过分析 SHAP 值，我们可以更深入地理解该算法的路径加权机制如何处理共享信息，这对于解释基于真实世界复杂数据的模型至关重要。", "problem": "给定一个双特征决策树模型和一个表现出强共线性的合成数据生成过程。您的任务是实现一个程序，使用 TreeSHAP (Tree Shapley Additive explanations) 的路径概率语义，为此树模型计算 Shapley 可加性解释 (SHAP)，并定量检验两个相关特征是否获得了相等的贡献度。程序必须按指定格式生成单行输出。\n\n考虑以下设置。\n\n- 数据生成过程：\n  - 有两个特征，$X_1$ 和 $X_2$。根据 $X_1 \\sim \\mathcal{N}(0,1)$、$\\epsilon \\sim \\mathcal{N}(0,\\sigma^2)$ 和 $X_2 = X_1 + \\epsilon$ 生成数据。这里 $\\mathcal{N}$ 表示正态分布。假设 $X_1$ 和 $\\epsilon$ 之间是独立的。\n- 模型：一个固定的深度为 $2$ 的决策树，其结构如下：\n  - 根节点在阈值为 $0$ 处对 $X_1$ 进行分裂。如果 $x_1 \\le 0$，则进入左子节点；否则，进入右叶节点。\n  - 左子节点在阈值为 $0$ 处对 $X_2$ 进行分裂。如果 $x_2 \\le 0$，则进入值为 $v_{LL}$ 的左叶节点；否则，进入值为 $v_{LR}$ 的右叶节点。\n  - 根节点的右子节点是一个值为 $v_R$ 的叶节点。\n  - 使用 $v_{LL}=-1.0$、$v_{LR}=+1.0$ 和 $v_R=+2.0$。\n- 待解释的实例：使用固定点 $x=(x_1,x_2)=(0.2,0.25)$。\n\n使用的基本原理和定义：\n- 让来自合作博弈论的 Shapley 值定义每个特征的归因值。对于 $d=2$ 个特征和联盟 $S \\subseteq \\{1,2\\}$ 的价值函数 $v(S)$，特征 $i$ 的 Shapley 值是其在所有特征排列上的平均边际贡献。不提供快捷公式；请使用基本定义。\n- 在 TreeSHAP 语义下，当分裂中使用的特征在联盟 $S$ 中缺失时，模型预测通过沿着两个子分支进行评估，并根据到达该节点的随机训练样本会走该分支的经验或分布概率（路径概率）进行加权。对于给定的树，这简化为使用在每次分裂时，以到达该分裂点为条件下，向左或向右的概率。\n\n作为基础起点使用的数学事实：\n- 对于均值为零、方差为单位值、相关系数为 $\\rho$ 的二元正态分布，其象限概率为 $P(X \\le 0, Y \\le 0) = \\tfrac{1}{4} + \\tfrac{1}{2\\pi}\\arcsin(\\rho)$。这是一个经过充分检验的公式。\n- 对于给定的数据生成过程，$(X_1,X_2)$ 是联合正态分布，其中 $\\operatorname{Var}(X_1)=1$，$\\operatorname{Var}(X_2)=1+\\sigma^2$，且 $\\operatorname{Cov}(X_1,X_2)=1$。因此，相关系数为 $\\rho = \\dfrac{1}{\\sqrt{1+\\sigma^2}}$。\n\n您的任务：\n1) 根据第一性原理和上述定义，在 TreeSHAP 语义下，为固定点 $x=(0.2,0.25)$ 推导出联盟 $S\\in\\{\\emptyset,\\{1\\},\\{2\\},\\{1,2\\}\\}$ 的联盟值 $v(S)$。特别是，当 $X_1$ 或 $X_2$ 缺失时，您必须计算在根节点对 $X_1$ 进行分裂和在左子节点对 $X_2$ 进行分裂时的路径概率。使用从数据生成分布中获得的精确概率（而非经验采样）。\n2) 使用 $d=2$ 特征的 Shapley 定义，计算固定点 $x$ 的每个特征的归因值 $\\phi_1$ 和 $\\phi_2$。\n3) 通过改变 $\\sigma$ 来研究共线性对 TreeSHAP 路径概率的影响，并确定 SHAP 是否为 $X_1$ 和 $X_2$ 分配了相等的贡献度。对每个 $\\sigma$ 量化差异 $\\phi_1 - \\phi_2$。\n\n测试套件：\n- 使用以下 $\\sigma$ 值集合：$\\{0.0,\\;0.1,\\;0.5,\\;1.0,\\;3.0\\}$。\n\n要求输出：\n- 对于测试套件中的每个 $\\sigma$，计算 Shapley 值并返回一个列表 $[\\phi_1,\\phi_2,\\phi_1-\\phi_2]$，其中每个值都四舍五入到 $6$ 位小数。\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，列表中的每个元素是对应一个 $\\sigma$ 的列表，顺序与给定顺序相同。例如，一个有效的输出格式是 $[[a_1,b_1,c_1],[a_2,b_2,c_2],\\dots]$，其中的数值条目四舍五入到 $6$ 位小数。\n\n不涉及物理单位。角度、百分比或其他单位约定在此不适用。所有数值量必须是纯实数。最终结果必须完全由给定的定义和指定的测试套件确定。解决方案不得使用任何外部数据或随机性，并且不得要求用户输入。", "solution": "## 问题验证\n\n### 第 1 步：提取已知条件\n- **数据生成过程**：\n    - 两个特征，$X_1$ 和 $X_2$。\n    - $X_1 \\sim \\mathcal{N}(0,1)$。\n    - $\\epsilon \\sim \\mathcal{N}(0,\\sigma^2)$。\n    - $X_2 = X_1 + \\epsilon$。\n    - $X_1$ 和 $\\epsilon$ 是独立的。\n- **模型**：一个固定的深度为 $2$ 的决策树。\n    - 根节点：在阈值 0 处对 $X_1$ 进行分裂。如果 $x_1 \\le 0$ 则向左，如果 $x_1 > 0$ 则向右。\n    - 根节点的右子节点：值为 $v_R = +2.0$ 的叶节点。\n    - 根节点的左子节点：在阈值 0 处对 $X_2$ 进行分裂。如果 $x_2 \\le 0$ 则向左，如果 $x_2 > 0$ 则向右。\n    - 左子节点的左叶节点：值为 $v_{LL} = -1.0$。\n    - 左子节点的右叶节点：值为 $v_{LR} = +1.0$。\n- **待解释的实例**：$x=(x_1,x_2)=(0.2,0.25)$。\n- **基本定义**：\n    - 对于 $d=2$ 个特征，特征 $i$ 的 Shapley 值：由所有排列上的平均边际贡献定义。\n    - TreeSHAP 语义：当分裂中使用的特征在联盟 $S$ 中缺失时，沿着两个子分支进行评估，并根据到达该节点的随机样本会走该分支的路径概率进行加权。使用从数据生成分布中获得的精确概率。\n- **数学事实**：\n    - 对于均值为零、方差为单位值、相关系数为 $\\rho$ 的二元正态分布 $(X,Y)$，其象限概率为 $P(X \\le 0, Y \\le 0) = \\tfrac{1}{4} + \\tfrac{1}{2\\pi}\\arcsin(\\rho)$。\n    - 对于给定的过程，$(X_1, X_2)$ 是联合正态分布，其 $\\operatorname{Var}(X_1)=1$，$\\operatorname{Var}(X_2)=1+\\sigma^2$，$\\operatorname{Cov}(X_1,X_2)=1$，相关系数为 $\\rho = \\dfrac{1}{\\sqrt{1+\\sigma^2}}$。\n- **测试套件**：$\\sigma \\in \\{0.0, 0.1, 0.5, 1.0, 3.0\\}$。\n- **要求输出**：对于每个 $\\sigma$，一个列表 $[\\phi_1, \\phi_2, \\phi_1-\\phi_2]$，四舍五入到 6 位小数。最终输出是这些列表的列表。\n\n### 第 2 步：使用提取的已知条件进行验证\n根据验证标准评估此问题：\n- **科学性**：该问题植根于合作博弈论（Shapley 值）及其在机器学习可解释性（SHAP）中应用的既定理论。数据生成过程和模型是合成的，但在数学上是明确定义的。所有组件都与标准统计和概率论一致。\n- **良态性**：该问题是良态的。它提供了所有必要的信息：一个特定的模型、一个特定的数据分布、一个特定的待解释实例，以及用于计算所需量的明确定义。这些任务导向一个唯一的、可计算的解决方案。\n- **客观性**：该问题以精确、客观的数学语言陈述。它没有歧义、主观性或基于意见的主张。\n\n该问题未表现出任何无效性缺陷。这是一个在指定统计学习主题内的严谨、自洽的理论练习。\n\n### 第 3 步：结论与行动\n问题是有效的。将提供完整的解决方案。\n\n## 解题推导\n\n对于两个特征，Shapley 值 $\\phi_1$ 和 $\\phi_2$ 定义如下：\n$$ \\phi_1 = \\frac{1}{2}\\left[ (v(\\{1\\}) - v(\\emptyset)) + (v(\\{1,2\\}) - v(\\{2\\})) \\right] $$\n$$ \\phi_2 = \\frac{1}{2}\\left[ (v(\\{2\\}) - v(\\emptyset)) + (v(\\{1,2\\}) - v(\\{1\\})) \\right] $$\n此处，$v(S)$ 是联盟 $S \\subseteq \\{1, 2\\}$ 的价值函数，表示在给定联盟 $S$ 中特征值的情况下，模型的期望输出。我们必须计算所有四个联盟的 $v(S)$ 值：$\\emptyset$、$\\{1\\}$、$\\{2\\}$ 和 $\\{1,2\\}$。待解释的实例为 $x = (x_1, x_2) = (0.2, 0.25)$。设模型表示为 $f(x_1, x_2)$。\n\n### 1. 联盟值 $v(S)$ 的计算\n\n- **$v(\\{1,2\\})$**：两个特征都已知。我们将实例 $x=(0.2, 0.25)$ 输入决策树。\n  - 根节点在 $X_1$ 上分裂。由于 $x_1 = 0.2 > 0$，我们进入右子节点。\n  - 右子节点是一个值为 $v_R = 2.0$ 的叶节点。\n  - 因此，$v(\\{1,2\\}) = f(0.2, 0.25) = 2.0$。\n\n- **$v(\\{1\\})$**：特征 $X_1$ 已知 ($x_1=0.2$)，而 $X_2$ 未知。根据 TreeSHAP 语义，我们通过对 $X_2$ 的分布进行平均来评估模型。\n  - 根节点在 $X_1$ 上分裂。由于 $x_1 = 0.2 > 0$，路径固定为右子节点。\n  - 模型输出为 $v_R = 2.0$，与 $X_2$ 的值无关。对 $X_2$ 的分裂不在此路径上。\n  - 因此，期望是平凡的：$v(\\{1\\}) = E[f(0.2, X_2)] = 2.0$。\n\n- **$v(\\{2\\})$**：特征 $X_2$ 已知 ($x_2=0.25$)，而 $X_1$ 未知。我们对 $X_1$ 的分布进行平均。\n  - 根节点在 $X_1$ 上分裂，而 $X_1$ 是缺失的。根据指定的“路径概率语义”，我们沿着两个分支前进，并按边际概率加权。\n  - 路径 1 (左)：以概率 $P(X_1 \\le 0) = 0.5$，我们进入左子树。\n  - 路径 2 (右)：以概率 $P(X_1 > 0) = 0.5$，我们进入右叶节点。输出为 $v_R = 2.0$。\n  - 在左子树中，模型在 $X_2$ 上分裂。我们知道 $x_2=0.25$。由于 $0.25 > 0$，我们进入该子树的右叶节点，其值为 $v_{LR}=1.0$。\n  - 结合这些路径，期望输出为：\n    $$ v(\\{2\\}) = P(X_1 \\le 0) \\cdot v_{LR} + P(X_1 > 0) \\cdot v_R = 0.5 \\cdot (1.0) + 0.5 \\cdot (2.0) = 0.5 + 1.0 = 1.5 $$\n  - 注意，该值与相关性参数 $\\sigma$ 无关。\n\n- **$v(\\emptyset)$**：两个特征都未知。这是模型在整个数据分布上的基线期望输出。\n  - $v(\\emptyset) = E[f(X_1, X_2)] = P(X_1 > 0) \\cdot v_R + P(X_1 \\le 0, X_2 \\le 0) \\cdot v_{LL} + P(X_1 \\le 0, X_2 > 0) \\cdot v_{LR}$。\n  - 我们有 $P(X_1 > 0) = 0.5$。叶节点的值为 $v_R=2.0$，$v_{LL}=-1.0$，$v_{LR}=1.0$。\n  - 我们使用提供的二元正态分布象限概率公式：$P(X_1 \\le 0, X_2 \\le 0) = \\frac{1}{4} + \\frac{1}{2\\pi}\\arcsin(\\rho)$，其中 $\\rho = \\frac{1}{\\sqrt{1+\\sigma^2}}$。\n  - 另一个叶节点的概率为 $P(X_1 \\le 0, X_2 > 0) = P(X_1 \\le 0) - P(X_1 \\le 0, X_2 \\le 0) = 0.5 - (\\frac{1}{4} + \\frac{\\arcsin(\\rho)}{2\\pi}) = \\frac{1}{4} - \\frac{\\arcsin(\\rho)}{2\\pi}$。\n  - 将这些代入期望公式：\n    $$ v(\\emptyset) = 0.5 \\cdot (2.0) + \\left(\\frac{1}{4} + \\frac{\\arcsin(\\rho)}{2\\pi}\\right)(-1.0) + \\left(\\frac{1}{4} - \\frac{\\arcsin(\\rho)}{2\\pi}\\right)(1.0) $$\n    $$ v(\\emptyset) = 1.0 - \\frac{1}{4} - \\frac{\\arcsin(\\rho)}{2\\pi} + \\frac{1}{4} - \\frac{\\arcsin(\\rho)}{2\\pi} $$\n    $$ v(\\emptyset) = 1.0 - \\frac{2\\arcsin(\\rho)}{2\\pi} = 1.0 - \\frac{\\arcsin(\\rho)}{\\pi} $$\n\n### 2. Shapley 值 $\\phi_1$ 和 $\\phi_2$ 的计算\n\n现在我们将联盟值代入 Shapley 公式。\n- 对于 $\\phi_1$：\n  - 给定 $\\emptyset$ 时 $\\{1\\}$ 的边际贡献：$v(\\{1\\}) - v(\\emptyset) = 2.0 - \\left(1.0 - \\frac{\\arcsin(\\rho)}{\\pi}\\right) = 1.0 + \\frac{\\arcsin(\\rho)}{\\pi}$。\n  - 给定 $\\{2\\}$ 时 $\\{1\\}$ 的边际贡献：$v(\\{1,2\\}) - v(\\{2\\}) = 2.0 - 1.5 = 0.5$。\n  - 平均边际贡献：\n    $$ \\phi_1 = \\frac{1}{2} \\left[ \\left(1.0 + \\frac{\\arcsin(\\rho)}{\\pi}\\right) + 0.5 \\right] = \\frac{1}{2} \\left[ 1.5 + \\frac{\\arcsin(\\rho)}{\\pi} \\right] = 0.75 + \\frac{\\arcsin(\\rho)}{2\\pi} $$\n\n- 对于 $\\phi_2$：\n  - 给定 $\\emptyset$ 时 $\\{2\\}$ 的边际贡献：$v(\\{2\\}) - v(\\emptyset) = 1.5 - \\left(1.0 - \\frac{\\arcsin(\\rho)}{\\pi}\\right) = 0.5 + \\frac{\\arcsin(\\rho)}{\\pi}$。\n  - 给定 $\\{1\\}$ 时 $\\{2\\}$ 的边际贡献：$v(\\{1,2\\}) - v(\\{1\\}) = 2.0 - 2.0 = 0.0$。\n  - 平均边际贡献：\n    $$ \\phi_2 = \\frac{1}{2} \\left[ \\left(0.5 + \\frac{\\arcsin(\\rho)}{\\pi}\\right) + 0.0 \\right] = \\frac{1}{2} \\left[ 0.5 + \\frac{\\arcsin(\\rho)}{\\pi} \\right] = 0.25 + \\frac{\\arcsin(\\rho)}{2\\pi} $$\n\n### 3. 共线性研究\n问题要求量化差异 $\\phi_1 - \\phi_2$。\n$$ \\phi_1 - \\phi_2 = \\left(0.75 + \\frac{\\arcsin(\\rho)}{2\\pi}\\right) - \\left(0.25 + \\frac{\\arcsin(\\rho)}{2\\pi}\\right) = 0.5 $$\n此结果表明，对于给定的模型结构和待解释的实例，特征归因值的差异是恒定的，不依赖于共线性的程度 $\\sigma$。由 $\\rho=1/\\sqrt{1+\\sigma^2}$ 表示的共线性确实会影响基线值 $v(\\emptyset)$，从而影响单个的 SHAP 值 $\\phi_1$ 和 $\\phi_2$，但它对两者的影响是相同的，导致在它们的差值中被抵消。归因值的不相等 ($\\phi_1 > \\phi_2$)源于树的结构和实例的特定特征值；对于 $x_1=0.2$，模型的输出仅由 $X_1$ 决定，这使得它对这个特定预测的影响更大。\n\n现在将实施程序，为指定的 $\\sigma$ 值测试套件计算这些值。\n- 对于每个 $\\sigma$，我们计算 $\\rho = 1/\\sqrt{1+\\sigma^2}$。对于 $\\sigma=0$，$\\rho=1$。\n- 然后我们计算 $\\phi_1 = 0.75 + \\arcsin(\\rho)/(2\\pi)$ 和 $\\phi_2 = 0.25 + \\arcsin(\\rho)/(2\\pi)$。\n- 差值固定为 $\\phi_1 - \\phi_2 = 0.5$。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes SHAP values for a two-feature decision tree model with collinear data.\n    \"\"\"\n\n    # Define the test cases for sigma from the problem statement.\n    test_cases_sigma = [0.0, 0.1, 0.5, 1.0, 3.0]\n\n    all_results = []\n    \n    for sigma in test_cases_sigma:\n        # Step 1: Calculate the correlation rho based on sigma.\n        # The relationship is rho = 1 / sqrt(1 + sigma^2).\n        if sigma == 0.0:\n            rho = 1.0\n        else:\n            rho = 1.0 / np.sqrt(1.0 + sigma**2)\n\n        # Step 2: Calculate arcsin(rho).\n        arcsin_rho = np.arcsin(rho)\n        \n        # Step 3: Compute phi_1 and phi_2 using the derived formulas.\n        # phi_1 = 0.75 + arcsin(rho) / (2 * pi)\n        # phi_2 = 0.25 + arcsin(rho) / (2 * pi)\n        phi_1 = 0.75 + arcsin_rho / (2.0 * np.pi)\n        phi_2 = 0.25 + arcsin_rho / (2.0 * np.pi)\n        \n        # Step 4: Compute the difference phi_1 - phi_2.\n        # As derived, this difference is a constant 0.5.\n        diff = phi_1 - phi_2\n        \n        # Store the results for this sigma value.\n        all_results.append([phi_1, phi_2, diff])\n\n    # Step 5: Format the output string as specified.\n    # The output should be a single line: [[a1,b1,c1],[a2,b2,c2],...]\n    # Each number must be rounded to 6 decimal places.\n    result_strings = []\n    for res_list in all_results:\n        # Format the numbers in the inner list as strings with 6 decimal places.\n        s_list = [f\"{val:.6f}\" for val in res_list]\n        # Join them into a string representation of a list: \"[v1,v2,v3]\"\n        result_strings.append(f\"[{','.join(s_list)}]\")\n    \n    # Join the string representations of inner lists with commas.\n    final_output_str = f\"[{','.join(result_strings)}]\"\n    \n    print(final_output_str)\n\nsolve()\n```", "id": "3173371"}]}