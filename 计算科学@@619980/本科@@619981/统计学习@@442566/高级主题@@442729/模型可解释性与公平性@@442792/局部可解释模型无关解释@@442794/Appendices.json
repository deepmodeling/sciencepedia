{"hands_on_practices": [{"introduction": "LIME的核心思想是使用一个简单的、可解释的代理模型来局部逼近复杂的黑盒函数。标准的LIME通常使用加权最小二乘法来拟合这个代理模型，但这种方法对异常值（outliers）非常敏感，可能导致解释结果产生偏差。本实践将引导你通过一个具体的编码任务[@problem_id:3140869]，比较基于$L_2$损失（最小二乘）和Huber损失（一种对异常值更鲁棒的损失函数）的局部线性代理模型，从而深刻理解构建可靠LIME解释时鲁棒性的重要性。", "problem": "您的任务是实现并比较两种局部线性代理（surrogate）解释，这两种解释均属于局部可解释模型无关解释（LIME）的范畴。其中一种使用平方误差损失（记为 $L_2$ 损失）进行训练，另一种使用 Huber 损失进行训练，目的是检验模型对于在远离目标点 $x_0$ 处采样的离群点扰动的鲁棒性。\n\n这项任务的核心是拟合一个局部加权的线性代理模型，以在固定点 $x_0 \\in \\mathbb{R}^p$ 附近近似一个平滑的黑盒回归函数 $f:\\mathbb{R}^p \\to \\mathbb{R}$。该局部代理模型使用原始特征作为可解释特征，其局部性通过一个核函数来强制实现，该核函数会降低远离 $x_0$ 的点的权重。您必须同时实现基于 $L_2$ 的代理模型（加权最小二乘法）和基于 Huber 损失的代理模型（加权鲁棒回归）。然后，您将定量地比较估计出的局部系数与 $f$ 在 $x_0$ 处的真实梯度。\n\n基本原理：\n- 在局部可解释模型无关解释（LIME）中，局部代理模型是通过最小化一个线性模型上的局部性加权经验风险来定义的。对于一个带有局部性权重 $\\{w_i\\}_{i=1}^n$ 的数据集 $\\{(x_i,y_i)\\}_{i=1}^n$， $L_2$ 代理模型求解\n$$\n\\min_{\\beta_0,\\beta \\in \\mathbb{R}^p} \\sum_{i=1}^n w_i \\left(y_i - \\beta_0 - \\beta^\\top x_i\\right)^2,\n$$\n而 Huber 损失代理模型求解\n$$\n\\min_{\\beta_0,\\beta \\in \\mathbb{R}^p} \\sum_{i=1}^n w_i \\,\\rho_\\delta\\!\\left(y_i - \\beta_0 - \\beta^\\top x_i\\right),\n$$\n其中，带有阈值 $\\delta>0$ 的 Huber 损失 $\\rho_\\delta$ 定义为\n$$\n\\rho_\\delta(r)=\\begin{cases}\n\\frac{1}{2} r^2,  \\text{若 } |r|\\le \\delta,\\\\\n\\delta |r| - \\frac{1}{2}\\delta^2,  \\text{若 } |r|>\\delta.\n\\end{cases}\n$$\n- 高斯核是编码局部性的标准选择。对于带宽 $\\sigma>0$，定义\n$$\nw_i = \\exp\\!\\left(-\\frac{\\|x_i-x_0\\|_2^2}{2\\sigma^2}\\right).\n$$\n\n您将使用一个固定的、维度 $p=5$ 的平滑黑盒模型 $f$，其定义如下\n$$\nf(x) \\;=\\; \\tanh\\!\\left(a^\\top x\\right) \\;+\\; \\frac{1}{2}\\left(b^\\top x\\right)^2 \\;+\\; c^\\top x \\;+\\; 0.2\\,\\sin\\!\\left(d^\\top x\\right),\n$$\n其中 $a,b,c,d \\in \\mathbb{R}^5$ 是固定向量，$x\\in\\mathbb{R}^5$。必须计算出精确的解析梯度 $\\nabla f(x)$，并用它来评估代理模型在 $x_0$ 处的斜率系数的质量。\n\n邻域数据必须通过如下方式对 $x_0$ 进行扰动来生成。给定一个污染比例 $\\gamma \\in [0,1]$、一个远距离离群点尺度 $R>0$、局部噪声标准差 $s_{\\text{local}}>0$ 以及远距离离群点噪声标准差 $s_{\\text{far}}>0$，构建 $n$ 个扰动点，其中包括 $n_{\\text{out}}=\\lfloor \\gamma n \\rfloor$ 个“远”点和 $n_{\\text{in}}=n-n_{\\text{out}}$ 个“近”点：\n- 近点：$x_i = x_0 + \\epsilon_i$，其中 $\\epsilon_i \\sim \\mathcal{N}(0, s_{\\text{local}}^2 I_p)$，对于 $i=1,\\dots,n_{\\text{in}}$。\n- 远点：通过对标准高斯向量进行归一化，均匀采样单位方向 $u_j \\in \\mathbb{S}^{p-1}$，然后设置 $x_j = x_0 + R\\,u_j + \\eta_j$，其中 $\\eta_j \\sim \\mathcal{N}(0, s_{\\text{far}}^2 I_p)$，对于 $j=1,\\dots,n_{\\text{out}}$。\n\n对于每个合成数据集，使用带宽为 $\\sigma>0$ 的高斯核计算 $y_i=f(x_i)$ 和局部性权重 $w_i$。在 $x_0$ 处拟合两个局部代理模型：\n- $L_2$ 代理模型：通过最小化加权平方误差。\n- Huber 损失代理模型：通过最小化加权 Huber 目标函数（使用固定的阈值 $\\delta>0$）。\n\n评估：令 $\\widehat{\\beta}^{(2)}$ 表示从 $L_2$ 代理模型得到的斜率向量，$\\widehat{\\beta}^{(H)}$ 表示从 Huber 代理模型得到的斜率向量。令 $g_0=\\nabla f(x_0)$ 为在 $x_0$ 处的真实梯度。计算每个代理模型的斜率误差，即欧几里得范数\n$$\nE_2 = \\|\\widehat{\\beta}^{(2)}-g_0\\|_2, \\qquad E_H=\\|\\widehat{\\beta}^{(H)}-g_0\\|_2.\n$$\n将 Huber 代理模型相对于 $L_2$ 代理模型的改进程度报告为单个浮点数\n$$\n\\Delta \\;=\\; E_2 - E_H.\n$$\n正的 $\\Delta$ 值表示 Huber 代理模型在 $x_0$ 处对真实局部斜率的估计更为准确。\n\n实现要求：\n- 使用 $p=5$, $n=400$, $s_{\\text{local}}=0.4$, $s_{\\text{far}}=0.1$, $\\delta=1.0$。使用固定的随机种子以确保结果是确定性的。\n- 从独立的标准正态分布中抽取 $x_0$, $a$, $b$, $c$, 和 $d$ 一次，并在所有测试用例中保持不变。\n- 使用高斯核和给定的 $\\sigma$ 来计算权重。\n- 通过加权最小二乘法求解 $L_2$ 问题。通过一种有原则的方法（如迭代重加权最小二乘法，IRLS）求解 Huber 问题，并正确地将局部性权重 $\\{w_i\\}$ 整合到目标函数中。收敛容差和迭代上限可以合理选择，但解必须是数值稳定的。\n\n测试套件：\n为以下五个参数集 $(\\gamma, R, \\sigma)$ 运行您的程序：\n1. $(0.0, 5.0, 1.0)$\n2. $(0.2, 5.0, 1.0)$\n3. $(0.6, 10.0, 1.0)$\n4. $(0.2, 5.0, 0.2)$\n5. $(0.2, 5.0, 5.0)$\n\n对于每种情况，按规定构建邻域数据集，拟合两种代理模型，计算 $E_2$、$E_H$，并返回 $\\Delta=E_2-E_H$ 作为浮点数。为确保可比性，在一次运行中，除了测试用例参数外，底层的随机性必须由单个全局种子固定。\n\n最终输出格式：\n您的程序应生成单行输出，其中包含五个 $\\Delta$ 值的结果，以逗号分隔的列表形式呈现，四舍五入到六位小数，并用方括号括起来，例如，\"[0.123456,0.000001,-0.010203,0.500000,0.250000]\"。", "solution": "目标是实现并比较两种用于可解释机器学习解释的局部线性代理模型，具体来说，是在局部可解释模型无关解释（LIME）的框架内。其中一个代理模型基于标准的平方误差（$L_2$）损失，而另一个则采用鲁棒的 Huber 损失。比较的重点在于估计的局部线性系数（斜率）相对于黑盒函数真实梯度的准确性，尤其是在局部邻域数据中存在离群点的情况下。\n\n### 1. 问题阐述\n\n我们给定一个平滑、非线性的黑盒函数 $f:\\mathbb{R}^p \\to \\mathbb{R}$，其维度固定为 $p=5$。该函数定义为：\n$$\nf(x) = \\tanh(a^\\top x) + \\frac{1}{2}\\left(b^\\top x\\right)^2 + c^\\top x + 0.2\\,\\sin(d^\\top x)\n$$\n其中 $a, b, c, d \\in \\mathbb{R}^p$ 是固定的参数向量。我们需要用一个线性模型来近似此函数在目标点 $x_0 \\in \\mathbb{R}^p$ 处的局部行为。局部线性近似斜率的真实值为函数 $f$ 在 $x_0$ 处评估的梯度，我们记为 $g_0 = \\nabla f(x_0)$。使用链式法则，逐分量求导可得解析梯度：\n$$\n\\nabla f(x) = \\left(1 - \\tanh^2(a^\\top x)\\right)a + \\left(b^\\top x\\right)b + c + 0.2\\cos(d^\\top x)d\n$$\n\n为了训练局部代理模型，我们在 $x_0$ 周围生成一个合成数据集 $\\{(x_i, y_i)\\}_{i=1}^n$，其中 $y_i=f(x_i)$。这个大小为 $n=400$ 的数据集被一部分比例为 $\\gamma$ 的离群点污染。数据集中有 $n_{\\text{in}} = n - \\lfloor \\gamma n \\rfloor$ 个“近”点和 $n_{\\text{out}} = \\lfloor \\gamma n \\rfloor$ 个“远”点。\n- 近点（内点）：$x_i = x_0 + \\epsilon_i$，噪声 $\\epsilon_i \\sim \\mathcal{N}(0, s_{\\text{local}}^2 I_p)$。\n- 远点（离群点）：$x_j = x_0 + R\\,u_j + \\eta_j$，其中 $u_j$ 是从单位球面 $\\mathbb{S}^{p-1}$ 上均匀采样的， $R$ 是一个大的距离尺度，$\\eta_j \\sim \\mathcal{N}(0, s_{\\text{far}}^2 I_p)$ 是一个小扰动。\n\n为了强制实现局部性，每个数据点 $(x_i, y_i)$ 都被赋予一个权重 $w_i$，该权重基于其与 $x_0$ 的接近程度，由一个带宽为 $\\sigma$ 的高斯核确定：\n$$\nw_i = \\exp\\!\\left(-\\frac{\\|x_i-x_0\\|_2^2}{2\\sigma^2}\\right)\n$$\n\n### 2. 局部代理模型\n\n代理模型是一个线性函数 $g(x) = \\beta_0 + \\beta^\\top x$，其中 $\\beta_0 \\in \\mathbb{R}$ 是截距，$\\beta \\in \\mathbb{R}^p$ 是斜率向量。这些参数通过最小化一个加权损失函数来找到。令 $\\tilde{x}_i = [1, x_i^\\top]^\\top$ 为增广特征向量，$\\tilde{\\beta} = [\\beta_0, \\beta^\\top]^\\top$ 为完整的系数向量。\n\n#### 2.1. $L_2$ 损失代理模型（加权最小二乘法）\n标准方法是最小化加权平方误差之和：\n$$\n\\min_{\\tilde{\\beta} \\in \\mathbb{R}^{p+1}} \\sum_{i=1}^n w_i \\left(y_i - \\tilde{x}_i^\\top \\tilde{\\beta}\\right)^2\n$$\n这是一个标准的加权最小二乘法（WLS）问题。在矩阵形式中，令 $\\tilde{X}$ 为 $n \\times (p+1)$ 的设计矩阵，其行是 $\\tilde{x}_i^\\top$，$\\mathbf{y}$ 是响应向量，$W$ 是一个对角线元素为局部性权重 $w_i$ 的 $n \\times n$ 对角矩阵。目标是最小化 $(\\mathbf{y} - \\tilde{X}\\tilde{\\beta})^\\top W (\\mathbf{y} - \\tilde{X}\\tilde{\\beta})$。估计系数的闭式解（记为 $\\widehat{\\tilde{\\beta}}^{(2)}$）是：\n$$\n\\widehat{\\tilde{\\beta}}^{(2)} = (\\tilde{X}^\\top W \\tilde{X})^{-1} \\tilde{X}^\\top W \\mathbf{y}\n$$\n用于解释的斜率向量是 $\\widehat{\\beta}^{(2)}$，它由 $\\widehat{\\tilde{\\beta}}^{(2)}$ 的最后 $p$ 个元素组成。\n\n#### 2.2. Huber 损失代理模型（鲁棒回归）\n为了提高对离群点的鲁棒性，我们用带有阈值 $\\delta > 0$ 的 Huber 损失 $\\rho_\\delta(\\cdot)$ 替换平方误差：\n$$\n\\rho_\\delta(r) = \\begin{cases}\n\\frac{1}{2} r^2,  \\text{若 } |r|\\le \\delta \\\\\n\\delta |r| - \\frac{1}{2}\\delta^2,  \\text{若 } |r| > \\delta\n\\end{cases}\n$$\nHuber 代理模型的系数 $\\widehat{\\tilde{\\beta}}^{(H)}$ 通过求解以下凸优化问题得到：\n$$\n\\min_{\\tilde{\\beta} \\in \\mathbb{R}^{p+1}} \\sum_{i=1}^n w_i \\rho_\\delta\\left(y_i - \\tilde{x}_i^\\top \\tilde{\\beta}\\right)\n$$\n该问题没有闭式解，需要使用迭代重加权最小二乘法（IRLS）进行数值求解。一阶最优性条件（估计方程）为 $\\sum_{i=1}^n w_i \\psi_\\delta(r_i) \\tilde{x}_i = \\mathbf{0}$，其中 $r_i = y_i - \\tilde{x}_i^\\top \\tilde{\\beta}$，$\\psi_\\delta(r) = \\rho'_\\delta(r)$ 是 Huber 损失的导数。这些方程可以改写为 $\\sum_{i=1}^n w_i \\omega_i(r_i) r_i \\tilde{x}_i = \\mathbf{0}$，其中 $\\omega_i(r_i) = \\psi_\\delta(r_i)/r_i$ 是依赖于残差的权重。\nIRLS 算法流程如下：\n1.  初始化系数 $\\tilde{\\beta}^{(0)}$，例如，使用 $L_2$ 解 $\\widehat{\\tilde{\\beta}}^{(2)}$。\n2.  对于迭代 $k=0, 1, 2, \\dots$ 直到收敛：\n    a.  计算残差：$r_i^{(k)} = y_i - \\tilde{x}_i^\\top \\tilde{\\beta}^{(k)}$。\n    b.  计算 IRLS 权重：如果 $|r_i^{(k)}| \\le \\delta$，则 $\\omega_i^{(k)} = 1$；如果 $|r_i^{(k)}| > \\delta$，则 $\\omega_i^{(k)} = \\delta / |r_i^{(k)}|$。这些权重会降低大残差数据点的影响。\n    c.  构建一个总权重对角矩阵 $W_{\\text{total}}^{(k)}$，其对角线元素为 $w_i \\cdot \\omega_i^{(k)}$，结合了局部性权重和残差权重。\n    d.  通过使用这些总权重求解 WLS 问题来更新系数：\n        $$\n        \\tilde{\\beta}^{(k+1)} = (\\tilde{X}^\\top W_{\\text{total}}^{(k)} \\tilde{X})^{-1} \\tilde{X}^\\top W_{\\text{total}}^{(k)} \\mathbf{y}\n        $$\n3.  当系数的变化 $\\|\\tilde{\\beta}^{(k+1)} - \\tilde{\\beta}^{(k)}\\|_2$ 小于一个很小的容差时，算法终止。最终的斜率向量是 $\\widehat{\\beta}^{(H)}$，由收敛后的 $\\tilde{\\beta}$ 的最后 $p$ 个元素组成。\n\n### 3. 评估\n每个代理模型的质量通过其估计的斜率向量与真实梯度 $g_0 = \\nabla f(x_0)$ 之间的欧几里得距离来衡量：\n$$\nE_2 = \\|\\widehat{\\beta}^{(2)}-g_0\\|_2, \\qquad E_H=\\|\\widehat{\\beta}^{(H)}-g_0\\|_2\n$$\n最终报告的指标是 Huber 代理模型相对于 $L_2$ 代理模型的改进程度，定义为其误差之差：\n$$\n\\Delta = E_2 - E_H\n$$\n$\\Delta$ 的正值表示 Huber 损失代理模型对 $x_0$ 处 $f$ 的局部行为提供了更准确的估计，证明了其对离群点污染的鲁棒性。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and compares L2-loss and Huber-loss local surrogate models.\n    \"\"\"\n    # --- Problem Constants and Fixed Parameters ---\n    P = 5\n    N = 400\n    S_LOCAL = 0.4\n    S_FAR = 0.1\n    DELTA = 1.0\n    RANDOM_SEED = 42\n\n    # Initialize a random number generator for reproducibility\n    rng = np.random.default_rng(RANDOM_SEED)\n\n    # Generate and fix the model parameters and the point of interest\n    x0 = rng.standard_normal(size=P)\n    a = rng.standard_normal(size=P)\n    b = rng.standard_normal(size=P)\n    c = rng.standard_normal(size=P)\n    d = rng.standard_normal(size=P)\n\n    test_cases = [\n        (0.0, 5.0, 1.0),\n        (0.2, 5.0, 1.0),\n        (0.6, 10.0, 1.0),\n        (0.2, 5.0, 0.2),\n        (0.2, 5.0, 5.0),\n    ]\n\n    # --- Helper Functions ---\n\n    def f_model(x_vec, a_p, b_p, c_p, d_p):\n        \"\"\"The black-box function f(x).\"\"\"\n        return (\n            np.tanh(a_p @ x_vec)\n            + 0.5 * (b_p @ x_vec) ** 2\n            + c_p @ x_vec\n            + 0.2 * np.sin(d_p @ x_vec)\n        )\n\n    def grad_f_model(x_vec, a_p, b_p, c_p, d_p):\n        \"\"\"The analytical gradient of f(x).\"\"\"\n        grad = (\n            (1 - np.tanh(a_p @ x_vec) ** 2) * a_p\n            + (b_p @ x_vec) * b_p\n            + c_p\n            + 0.2 * np.cos(d_p @ x_vec) * d_p\n        )\n        return grad\n\n    def solve_wls(X_tilde, y, weights):\n        \"\"\"Solves a weighted least squares problem.\"\"\"\n        W = np.diag(weights)\n        # Using np.linalg.solve for stability: (X.T @ W @ X) @ beta = X.T @ W @ y\n        lhs = X_tilde.T @ W @ X_tilde\n        rhs = X_tilde.T @ W @ y\n        try:\n            beta_tilde = np.linalg.solve(lhs, rhs)\n        except np.linalg.LinAlgError:\n            # Fallback to pseudoinverse if singular\n            beta_tilde = np.linalg.pinv(lhs) @ rhs\n        return beta_tilde\n\n    def solve_huber_irls(X_tilde, y, locality_weights, delta, tol=1e-7, max_iter=100):\n        \"\"\"Solves a Huber regression problem using IRLS.\"\"\"\n        # Initialize with the standard WLS solution\n        beta_tilde = solve_wls(X_tilde, y, locality_weights)\n\n        for _ in range(max_iter):\n            residuals = y - X_tilde @ beta_tilde\n            abs_residuals = np.abs(residuals)\n            \n            # IRLS weights: handles r=0 case correctly via np.where\n            irls_weights = np.where(abs_residuals <= delta, 1.0, delta / abs_residuals)\n            \n            total_weights = locality_weights * irls_weights\n            \n            beta_tilde_new = solve_wls(X_tilde, y, total_weights)\n            \n            # Check for convergence\n            if np.linalg.norm(beta_tilde_new - beta_tilde) < tol:\n                beta_tilde = beta_tilde_new\n                break\n            \n            beta_tilde = beta_tilde_new\n            \n        return beta_tilde\n\n    results = []\n\n    # --- Main Loop over Test Cases ---\n    for gamma, R, sigma in test_cases:\n        # 1. Generate neighborhood data\n        n_out = int(np.floor(gamma * N))\n        n_in = N - n_out\n\n        # Near points\n        epsilons = rng.normal(scale=S_LOCAL, size=(n_in, P))\n        X_in = x0 + epsilons\n\n        # Far points\n        if n_out > 0:\n            Z = rng.normal(size=(n_out, P))\n            directions = Z / np.linalg.norm(Z, axis=1, keepdims=True)\n            etas = rng.normal(scale=S_FAR, size=(n_out, P))\n            X_out = x0 + R * directions + etas\n            X = np.vstack((X_in, X_out))\n        else:\n            X = X_in\n\n        # Compute responses y = f(x)\n        y = np.array([f_model(x_i, a, b, c, d) for x_i in X])\n\n        # 2. Compute locality weights\n        distances_sq = np.sum((X - x0) ** 2, axis=1)\n        locality_weights = np.exp(-distances_sq / (2 * sigma**2))\n\n        # 3. Prepare matrices for regression\n        X_tilde = np.c_[np.ones(N), X]\n\n        # 4. Solve for L2 surrogate\n        beta_tilde_2 = solve_wls(X_tilde, y, locality_weights)\n        beta_hat_2 = beta_tilde_2[1:]\n\n        # 5. Solve for Huber surrogate\n        beta_tilde_H = solve_huber_irls(X_tilde, y, locality_weights, DELTA)\n        beta_hat_H = beta_tilde_H[1:]\n\n        # 6. Evaluate against the true gradient\n        g0 = grad_f_model(x0, a, b, c, d)\n        \n        e2 = np.linalg.norm(beta_hat_2 - g0)\n        eH = np.linalg.norm(beta_hat_H - g0)\n        \n        delta_error = e2 - eH\n        results.append(delta_error)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{v:.6f}' for v in results)}]\")\n\nsolve()\n```", "id": "3140869"}, {"introduction": "尽管局部线性模型是LIME的基石，但其解释能力受限于模型本身的线性假设。当黑盒模型表现出强烈非线性（如阶跃、突变）时，线性代理模型可能无法准确捕捉其局部行为，甚至会给出误导性的解释。本实践旨在通过一个精心设计的思想实验[@problem_id:3140899]，探讨LIME在面对具有尖锐阈值效应的函数时的表现，帮助你学会批判性地评估线性解释的适用范围和潜在缺陷。", "problem": "考虑一个黑盒实值函数 $f:\\mathbb{R}^d \\to \\mathbb{R}$，它在第一个坐标上表现出明显的阈值效应。该函数定义为\n$$\nf(\\mathbf{x}) \\;=\\; J \\cdot \\mathbf{1}\\{x_1 \\ge \\tau\\},\n$$\n其中 $\\mathbf{x} = (x_1,\\dots,x_d)$，$J \\in \\mathbb{R}$ 是一个固定的跳跃幅度，$\\tau \\in \\mathbb{R}$ 是阈值，$\\mathbf{1}\\{\\cdot\\}$ 是指示函数。\n\n使用局部可解释模型无关解释 (Local Interpretable Model-Agnostic Explanations, LIME) 的原理构建一个局部代理模型。该代理模型是在目标点 $\\mathbf{x}_0 \\in \\mathbb{R}^d$ 处拟合的局部加权线性回归。样本 $\\mathbf{x}$ 的权重由欧几里得范数中的指数核函数给出\n$$\nw(\\mathbf{x}) \\;=\\; \\exp\\!\\left(-\\frac{\\|\\mathbf{x} - \\mathbf{x}_0\\|_2^2}{2\\sigma^2}\\right),\n$$\n核宽度为 $\\sigma > 0$。局部线性代理模型具有以下形式\n$$\n\\hat{f}(\\mathbf{x}) \\;=\\; \\beta_0 + \\sum_{j=1}^d \\beta_j x_j,\n$$\n其中系数通过最小化加权最小二乘目标函数获得\n$$\n\\min_{\\beta_0,\\beta_1,\\dots,\\beta_d} \\sum_{i=1}^N w(\\mathbf{x}^{(i)}) \\left( f(\\mathbf{x}^{(i)}) - \\beta_0 - \\sum_{j=1}^d \\beta_j x^{(i)}_j \\right)^2,\n$$\n给定一组 $N$ 个扰动样本 $\\{\\mathbf{x}^{(i)}\\}_{i=1}^N$，这些样本独立地从以 $\\mathbf{x}_0$ 为中心、各坐标上各向同性标准差为 $s$ 的高斯分布中抽取：\n$$\n\\mathbf{x}^{(i)} \\sim \\mathcal{N}\\!\\left(\\mathbf{x}_0, s^2 \\mathbf{I}_d\\right).\n$$\n\n定义以下可度量的概念，以判断局部代理模型是否捕捉到了 $\\mathbf{x}_0$ 附近的阈值效应：\n- 在第一个坐标上跨越核尺度的真实局部对比度为\n$$\nC_{\\text{true}} \\;=\\; \\left|\\, f\\big(\\mathbf{x}_0 + \\sigma \\mathbf{e}_1\\big) - f\\big(\\mathbf{x}_0 - \\sigma \\mathbf{e}_1\\big) \\,\\right|,\n$$\n其中 $\\mathbf{e}_1$ 是 $\\mathbb{R}^d$ 中的第一个规范基向量。对于给定的 $f$，如果 $(x_{0,1} - \\sigma)  \\tau \\le (x_{0,1} + \\sigma)$，则简化为 $C_{\\text{true}} = J$，否则 $C_{\\text{true}} = 0$。\n- 在相同尺度上由代理模型预测的局部对比度为\n$$\nC_{\\text{pred}} \\;=\\; \\left|\\, \\hat{f}\\big(\\mathbf{x}_0 + \\sigma \\mathbf{e}_1\\big) - \\hat{f}\\big(\\mathbf{x}_0 - \\sigma \\mathbf{e}_1\\big) \\,\\right| \\;=\\; \\left|\\, 2\\sigma \\,\\beta_1 \\,\\right|.\n$$\n\n如果绝对误差满足以下条件，则声明局部代理模型捕捉到了阈值\n$$\n\\left| C_{\\text{pred}} - C_{\\text{true}} \\right| \\;\\le\\; \\varepsilon \\,\\max\\{1, |J|\\},\n$$\n对于一个固定的容差 $\\varepsilon > 0$。\n\n任务。编写一个完整的程序，该程序：\n1. 实现指定的函数 $f$。\n2. 对于下面的每个测试用例，从 $\\mathcal{N}(\\mathbf{x}_0, s^2 \\mathbf{I}_d)$ 生成 $N$ 个样本 $\\mathbf{x}^{(i)}$，并使用固定的伪随机种子 $12345$ 以确保可复现性。\n3. 使用测试用例的核宽度 $\\sigma$ 计算权重 $w(\\mathbf{x}^{(i)})$。\n4. 使用带截距项的加权最小二乘法拟合局部加权线性回归，以获得 $(\\beta_0,\\dots,\\beta_d)$。任何数值稳定的方法都是可以接受的，但它必须精确地实现所述的加权目标函数。\n5. 按定义计算 $C_{\\text{true}}$ 和 $C_{\\text{pred}}$，并返回一个布尔值，指示在 $\\varepsilon = 0.35$ 的情况下捕捉准则是否成立。\n6. 将所有测试用例的布尔结果汇总到单行输出中，该输出包含一个用方括号括起来的逗号分隔列表，格式完全如下：例如，“[True,False,True]”。\n\n测试套件。使用以下四个测试用例，每个用例由元组 $(d,\\tau,J,\\mathbf{x}_0,s,\\sigma,N)$ 定义：\n- 用例 A (接近阈值，小核)：$(d=2,\\tau=0,J=4,\\mathbf{x}_0=(0.02,0.0),s=0.2,\\sigma=0.08,N=12000)$。\n- 用例 B (远离阈值)：$(d=2,\\tau=0,J=4,\\mathbf{x}_0=(0.5,0.0),s=0.2,\\sigma=0.08,N=8000)$。\n- 用例 C (恰好在阈值上，非常宽的核)：$(d=2,\\tau=0,J=4,\\mathbf{x}_0=(0.0,0.0),s=0.05,\\sigma=0.5,N=12000)$。\n- 用例 D (恰好在阈值上，非常窄的核，更高维度)：$(d=3,\\tau=0,J=4,\\mathbf{x}_0=(0.0,0.1,-0.1),s=0.2,\\sigma=0.02,N=20000)$。\n\n角度单位不适用。此问题中不出现物理单位。您的程序应生成一行输出，其中包含布尔结果，格式为方括号括起来的逗号分隔列表，并严格按照上述测试套件用例的顺序，例如：“[True,False,True,True]”。不应打印任何其他文本。", "solution": "### 第 1 步：提取已知信息\n\n问题提供了以下定义和数据：\n\n- **黑盒函数**：$f:\\mathbb{R}^d \\to \\mathbb{R}$ 定义为 $f(\\mathbf{x}) = J \\cdot \\mathbf{1}\\{x_1 \\ge \\tau\\}$，其中 $\\mathbf{x} = (x_1,\\dots,x_d)$，$J \\in \\mathbb{R}$ 是跳跃幅度，$\\tau \\in \\mathbb{R}$ 是阈值，$\\mathbf{1}\\{\\cdot\\}$ 是指示函数。\n- **LIME 权重核**：$w(\\mathbf{x}) = \\exp\\!\\left(-\\frac{\\|\\mathbf{x} - \\mathbf{x}_0\\|_2^2}{2\\sigma^2}\\right)$，对于目标点 $\\mathbf{x}_0$ 和核宽度 $\\sigma > 0$。\n- **局部代理模型**：一个线性模型 $\\hat{f}(\\mathbf{x}) = \\beta_0 + \\sum_{j=1}^d \\beta_j x_j$。\n- **目标函数**：系数 $(\\beta_0, \\beta_1, \\dots, \\beta_d)$ 通过最小化加权最小二乘误差来确定：\n$$ \\min_{\\beta_0,\\beta_1,\\dots,\\beta_d} \\sum_{i=1}^N w(\\mathbf{x}^{(i)}) \\left( f(\\mathbf{x}^{(i)}) - \\beta_0 - \\sum_{j=1}^d \\beta_j x^{(i)}_j \\right)^2 $$\n- **扰动样本**：一组 $N$ 个样本 $\\{\\mathbf{x}^{(i)}\\}_{i=1}^N$，从 $\\mathbf{x}^{(i)} \\sim \\mathcal{N}(\\mathbf{x}_0, s^2 \\mathbf{I}_d)$ 中抽取。\n- **真实局部对比度**：$C_{\\text{true}} = \\left|\\, f\\big(\\mathbf{x}_0 + \\sigma \\mathbf{e}_1\\big) - f\\big(\\mathbf{x}_0 - \\sigma \\mathbf{e}_1\\big) \\,\\right|$。\n- **预测局部对比度**：$C_{\\text{pred}} = \\left|\\, \\hat{f}\\big(\\mathbf{x}_0 + \\sigma \\mathbf{e}_1\\big) - \\hat{f}\\big(\\mathbf{x}_0 - \\sigma \\mathbf{e}_1\\big) \\,\\right| = \\left|\\, 2\\sigma \\,\\beta_1 \\,\\right|$。\n- **捕捉准则**：$\\left| C_{\\text{pred}} - C_{\\text{true}} \\right| \\le \\varepsilon \\,\\max\\{1, |J|\\}$，固定容差 $\\varepsilon = 0.35$。\n- **可复现性**：必须使用固定的伪随机种子 $12345$ 进行样本生成。\n- **测试用例**：\n    - 用例 A：$(d=2, \\tau=0, J=4, \\mathbf{x}_0=(0.02, 0.0), s=0.2, \\sigma=0.08, N=12000)$\n    - 用例 B：$(d=2, \\tau=0, J=4, \\mathbf{x}_0=(0.5, 0.0), s=0.2, \\sigma=0.08, N=8000)$\n    - 用例 C：$(d=2, \\tau=0, J=4, \\mathbf{x}_0=(0.0, 0.0), s=0.05, \\sigma=0.5, N=12000)$\n    - 用例 D：$(d=3, \\tau=0, J=4, \\mathbf{x}_0=(0.0, 0.1, -0.1), s=0.2, \\sigma=0.02, N=20000)$\n\n### 第 2 步：使用提取的已知信息进行验证\n\n- **科学依据**：该问题在统计学习领域有充分的依据，特别是关于局部可解释模型无关解释 (LIME)。所描述的方法——局部加权线性回归、高斯核加权和高斯扰动——都是标准技术。函数 $f(\\mathbf{x})$ 是一个简单、定义明确的数学函数（阶跃函数）。\n- **适定性**：该问题是适定的。目标是根据一系列确定性计算，在给定一组固定参数和固定随机种子的情况下，计算一个布尔值。在指定条件下（样本从连续分布中抽取），加权最小二乘问题有唯一、稳定的解。\n- **客观性**：该问题以精确、客观的数学语言陈述。所有定义、参数和标准都是定量的、明确的。\n- **完整性和一致性**：该问题是自洽的。为每个测试用例提供了所有必要的数据和定义。没有内部矛盾。\n\n### 第 3 步：结论与行动\n\n该问题是有效的。它是一个基于机器学习既定原则的、定义明确的计算任务。将提供一个完整的解决方案。\n\n### 解决方案\n\n任务是确定对于几个测试用例，一个 LIME 风格的局部线性代理模型是否能捕捉到给定函数 $f(\\mathbf{x})$ 的急剧阈值行为。这通过比较 $f(\\mathbf{x})$ 在一个局部区间上的真实变化与代理模型预测的变化来评估。为每个测试用例实现以下算法。\n\n首先，对于一个给定的参数为 $(d, \\tau, J, \\mathbf{x}_0, s, \\sigma, N)$ 的测试用例，我们生成 $N$ 个扰动样本。为了可复现性，伪随机数生成器以值 $12345$ 作为种子。每个样本 $\\mathbf{x}^{(i)}$ 从多元正态分布 $\\mathcal{N}(\\mathbf{x}_0, s^2 \\mathbf{I}_d)$ 中抽取，其中 $\\mathbf{I}_d$ 是 $d \\times d$ 的单位矩阵。\n\n接着，我们为每个样本 $\\mathbf{x}^{(i)}$ 评估黑盒函数 $f(\\mathbf{x}^{(i)}) = J \\cdot \\mathbf{1}\\{x_1^{(i)} \\ge \\tau\\}$。这会产生一个响应向量 $\\mathbf{y} \\in \\mathbb{R}^N$，其中每个元素 $y_i = f(\\mathbf{x}^{(i)})$。\n\nLIME方法的核心是通过解决一个加权最小二乘问题来拟合一个局部代理模型。每个样本 $\\mathbf{x}^{(i)}$ 的权重使用指数核函数 $w(\\mathbf{x}^{(i)}) = \\exp\\!\\left(-\\frac{\\|\\mathbf{x}^{(i)} - \\mathbf{x}_0\\|_2^2}{2\\sigma^2}\\right)$ 计算。这些权重 $w_i = w(\\mathbf{x}^{(i)})$ 构成了权重矩阵 $\\mathbf{W}$ 的对角线元素。\n\n线性代理模型是 $\\hat{f}(\\mathbf{x}) = \\beta_0 + \\sum_{j=1}^d \\beta_j x_j$。为了找到系数向量 $\\boldsymbol{\\beta} = (\\beta_0, \\beta_1, \\dots, \\beta_d)^T$，我们求解加权最小二乘法的正规方程：\n$$ \\boldsymbol{\\beta} = (\\mathbf{X}_{\\text{aug}}^T \\mathbf{W} \\mathbf{X}_{\\text{aug}})^{-1} \\mathbf{X}_{\\text{aug}}^T \\mathbf{W} \\mathbf{y} $$\n在这里，$\\mathbf{y}$ 是函数值 $f(\\mathbf{x}^{(i)})$ 的 $N \\times 1$ 向量。$\\mathbf{X}_{\\text{aug}}$ 是 $N \\times (d+1)$ 的增广设计矩阵，通过在 $N \\times d$ 的样本矩阵前添加一列全为1的列来构建：$\\mathbf{X}_{\\text{aug}} = [\\mathbf{1}_N, \\mathbf{X}]$。该方程使用稳定的线性代数求解器对 $\\boldsymbol{\\beta}$ 进行数值求解，这比直接计算矩阵的逆更稳健。\n\n在确定系数向量 $\\boldsymbol{\\beta}$ 后，我们可以计算预测的局部对比度。我们感兴趣的系数是 $\\beta_1$，它对应于特征 $x_1$。预测的对比度由 $C_{\\text{pred}} = |2\\sigma \\beta_1|$ 给出。\n\n然后我们计算真实的局部对比度 $C_{\\text{true}}$，它被定义为函数 $f(\\mathbf{x})$ 在第一个坐标上跨越区间 $[x_{0,1} - \\sigma, x_{0,1} + \\sigma]$ 的实际变化：\n$$ C_{\\text{true}} = \\left| f(\\mathbf{x}_0 + \\sigma\\mathbf{e}_1) - f(\\mathbf{x}_0 - \\sigma\\mathbf{e}_1) \\right| $$\n其中 $\\mathbf{e}_1$ 是第一个标准基向量。这通过在两个点 $\\mathbf{x}_{\\text{plus}} = \\mathbf{x}_0 + \\sigma\\mathbf{e}_1$ 和 $\\mathbf{x}_{\\text{minus}} = \\mathbf{x}_0 - \\sigma\\mathbf{e}_1$ 上评估 $f$ 来计算。\n\n最后，我们应用捕捉准则。如果预测对比度和真实对比度之间的绝对误差在指定的容差范围内，则认为局部代理模型已经捕捉到了阈值效应：\n$$ |C_{\\text{pred}} - C_{\\text{true}}| \\le \\varepsilon \\max\\{1, |J|\\} $$\n使用给定的值 $\\varepsilon = 0.35$ 对此不等式进行评估。将得到的布尔值（True 或 False）记录下来用于该测试用例。对所有四个测试用例重复整个过程，并将布尔结果序列格式化为最终输出。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the LIME surrogate model validation problem for a series of test cases.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    # Each case is a tuple: (d, tau, J, x_0, s, sigma, N)\n    test_cases = [\n        # Case A: near-threshold, small kernel\n        (2, 0.0, 4.0, (0.02, 0.0), 0.2, 0.08, 12000),\n        # Case B: far from threshold\n        (2, 0.0, 4.0, (0.5, 0.0), 0.2, 0.08, 8000),\n        # Case C: exactly at threshold, very wide kernel\n        (2, 0.0, 4.0, (0.0, 0.0), 0.05, 0.5, 12000),\n        # Case D: exactly at threshold, very narrow kernel, higher dimension\n        (3, 0.0, 4.0, (0.0, 0.1, -0.1), 0.2, 0.02, 20000),\n    ]\n\n    # Fixed tolerance for the capture criterion\n    epsilon = 0.35\n    \n    # Store boolean results for each case\n    results = []\n\n    for case in test_cases:\n        d, tau, J, x_0_tuple, s, sigma, N = case\n        x_0 = np.array(x_0_tuple)\n\n        # Set the pseudo-random seed for reproducibility for each case\n        np.random.seed(12345)\n        \n        # 1. Generate N samples from N(x_0, s^2 * I_d)\n        samples = x_0 + s * np.random.standard_normal(size=(N, d))\n        \n        # 2. Evaluate the black-box function f(x) for all samples\n        # f(x) = J * 1{x_1 >= tau}\n        f_values = J * (samples[:, 0] >= tau).astype(float)\n        \n        # 3. Compute weights w(x) for all samples\n        # w(x) = exp(-||x - x_0||^2 / (2 * sigma^2))\n        sq_dists = np.sum((samples - x_0)**2, axis=1)\n        weights = np.exp(-sq_dists / (2 * sigma**2))\n        \n        # 4. Fit the locally weighted linear regression\n        # Create the augmented design matrix X_aug with an intercept column\n        X_aug = np.hstack([np.ones((N, 1)), samples])\n        Y = f_values\n        \n        # Construct the matrices for the normal equation: (X^T W X) beta = X^T W Y\n        # To do this efficiently, we use broadcasting with the weights vector\n        # instead of creating a large diagonal matrix W.\n        # Let A = X^T W X and b = X^T W Y\n        \n        # A = X_aug.T @ (weights[:, np.newaxis] * X_aug)\n        A = X_aug.T @ (weights.reshape(-1, 1) * X_aug) \n        # b = X_aug.T @ (weights * Y)\n        b = X_aug.T @ (weights * Y)\n        \n        # Solve the linear system A * beta = b for beta\n        try:\n            beta = np.linalg.solve(A, b)\n        except np.linalg.LinAlgError:\n            # Use pseudo-inverse as a fallback for singular or ill-conditioned matrices\n            beta = np.linalg.pinv(A) @ b\n        \n        # The coefficient for the first coordinate, x_1, is beta[1]\n        beta_1 = beta[1]\n        \n        # 5. Compute C_true and C_pred\n        \n        # C_true = | f(x_0 + sigma*e_1) - f(x_0 - sigma*e_1) |\n        f_plus = J if (x_0[0] + sigma) >= tau else 0.0\n        f_minus = J if (x_0[0] - sigma) >= tau else 0.0\n        C_true = np.abs(f_plus - f_minus)\n        \n        # C_pred = | 2 * sigma * beta_1 |\n        C_pred = np.abs(2 * sigma * beta_1)\n        \n        # 6. Apply the capture criterion\n        abs_error = np.abs(C_pred - C_true)\n        tolerance_threshold = epsilon * np.max([1.0, np.abs(J)])\n        \n        is_captured = abs_error <= tolerance_threshold\n        results.append(is_captured)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3140899"}, {"introduction": "真实世界的数据和模型中常常存在特征交互效应，即一个特征的影响力依赖于另一个特征的取值，这是简单的线性模型无法捕捉的。为了获得更丰富、更准确的局部解释，我们可以扩展LIME框架以检测这些交互作用。本实践将指导你实现一个分层的LIME程序[@problem_id:3140901]，它首先拟合一个只包含主效应的线性模型，然后通过分析其残差来智能地识别并加入重要的特征交互项，从而生成更具表现力的局部代理模型。", "problem": "您的任务是设计并实现一个分层局部可解释模型无关解释 (LIME) 流程，用于在特定查询点检测黑盒预测器中的成对特征交互。局部可解释模型无关解释 (LIME) 通过在查询点附近采样扰动输入，并使用基于相似度的权重拟合一个简单的代理模型，来局部逼近一个黑盒预测器。分层扩展的流程是，首先仅使用主效应拟合一个线性代理模型，然后根据第一阶段拟合残差中的证据，用一组选定的成对交互项来增强该模型。您的实现必须以基础定义为指导，而非使用简化公式。\n\n基本原理：\n- 黑盒预测器是一个函数 $f: \\mathbb{R}^d \\to \\mathbb{R}$，它将一个特征向量 $\\mathbf{x} \\in \\mathbb{R}^d$ 映射到一个实值预测。\n- 局部性通过在查询点 $\\mathbf{x}_0$ 附近采样扰动 $\\mathbf{z}_i$ 并通过核函数 $w_i = K(\\lVert \\mathbf{z}_i - \\mathbf{x}_0 \\rVert)$ 对每个样本加权来建模，其中 $K$ 是一个关于距离的非负递减函数。\n- 加权最小二乘代理模型最小化加权残差平方和。在第一阶段，仅考虑截距和主效应，得到一个线性代理模型，用单个特征来解释 $f(\\mathbf{z}_i)$。在第二阶段，考虑使用选定的成对交互项进行增强，以捕捉残差中剩余的非可加结构。\n- 加权拟合优度通过加权平方和及其比率来量化，形成一个加权决定系数。\n\n任务：\n1. 通过抽取 $N$ 个扰动 $\\mathbf{z}_i \\sim \\mathcal{N}(\\mathbf{x}_0, \\operatorname{diag}(\\mathbf{s}^2))$ 来实现在 $\\mathbf{x}_0$ 周围的局部采样，其中 $\\mathbf{s}$ 是每个特征的标准差向量。请在程序内部使用固定的随机种子以确保可复现性。特征是无量纲的，并被限制在一个合理的范围内；不需要任何物理单位。\n2. 将局部性的径向核函数定义为 $w_i = \\exp\\!\\left(-\\frac{\\lVert \\mathbf{z}_i - \\mathbf{x}_0 \\rVert_2^2}{2\\sigma^2}\\right)$，对于给定的带宽 $\\sigma > 0$。\n3. 第一阶段（主效应）：\n   - 使用一个截距和中心化的主效应 $u_{i,j} = z_{i,j} - x_{0,j}$（其中 $j = 1,\\dots,d$）来构建设计矩阵。拟合一个加权岭最小二乘代理模型，该模型最小化 $f(\\mathbf{z}_i)$ 与各列线性组合之间的加权平方差，且仅对非截距系数进行正则化。设正则化强度为 $\\lambda > 0$。\n   - 计算加权残差 $r_i = f(\\mathbf{z}_i) - \\hat{y}_i$ 和加权决定系数 $R^2_{\\text{lin}}$，该系数源自相对于加权均值的已解释方差的基本定义。\n4. 第二阶段（交互增强）：\n   - 对于每对不同的特征 $(p,q)$（其中 $1 \\le p  q \\le d$），构建交互回归量 $v_{i,p,q} = u_{i,p} \\cdot u_{i,q}$（中心化特征的乘积）。\n   - 通过计算其与残差 $r_i$ 的加权相关性来量化每个交互回归量的证据，该相关性基于加权均值、加权协方差和加权方差。选择最多 $k$ 个绝对相关性最大且超过阈值 $\\tau$ 的交互对。如果没有任何一对超过阈值，则不选择。\n   - 用选定的成对交互项增强设计矩阵，并重新拟合加权岭代理模型，以获得新的加权决定系数 $R^2_{\\text{aug}}$。\n5. 交互检测：\n   - 令 $S_{\\text{true}}$ 为测试用例中真实存在的交互特征对集合。令 $S_{\\text{sel}}$ 为算法选择的交互对集合。定义两个布尔度量：\n     - 精确匹配正确性：当且仅当 $S_{\\text{sel}} = S_{\\text{true}}$ 时，$C_{\\text{exact}}$ 为真。\n     - 一致性检测：当且仅当 $S_{\\text{true}}$ 非空且 $S_{\\text{sel}}$ 与 $S_{\\text{true}}$ 有交集，或者 $S_{\\text{true}}$ 为空且 $S_{\\text{sel}}$ 也为空时，$C_{\\text{cons}}$ 为真。\n   - 同时计算加权拟合的改进量 $\\Delta R^2 = R^2_{\\text{aug}} - R^2_{\\text{lin}}$，结果为浮点数。不允许使用百分比；将 $\\Delta R^2$ 报告为小数值。\n6. 输出规范：\n   - 对于每个测试用例，您的程序必须输出一个列表 $[C_{\\text{exact}}, C_{\\text{cons}}, \\Delta R^2]$。\n   - 最终的程序输出必须是单行，包含一个按顺序排列的各测试用例结果的列表，以逗号分隔并用方括号括起来。例如，包含两个测试用例的输出应如下所示：`[[\\text{True},\\text{True},0.123456],[\\text{False},\\text{True},0.000000]]`。您必须将每个 $\\Delta R^2$ 格式化为小数点后恰好六位。\n\n测试套件：\n为以下四个测试用例实现该流程；每个测试用例使用 $d=3$ 个特征，真实的交互被定义为使用从零开始的索引的索引对集合。对于每个用例，定义 $f$、$\\mathbf{x}_0$、$N$、$\\sigma$、$\\mathbf{s}$、$k$、$\\tau$ 和 $\\lambda$，并报告指定的输出。\n\n- 用例 1（有一个交互的顺利路径）：\n  - $f(\\mathbf{x}) = x_1 x_2 + 0.2 x_3 + 0.1 \\sin(\\pi x_1)$。\n  - $\\mathbf{x}_0 = [0.3, -0.5, 0.1]$，$N = 1200$，$\\sigma = 0.7$，$\\mathbf{s} = [0.5, 0.5, 0.5]$，$k = 3$，$\\tau = 0.1$，$\\lambda = 10^{-3}$。\n  - $S_{\\text{true}} = \\{(0,1)\\}$。\n- 用例 2（无交互，纯可加模型）：\n  - $f(\\mathbf{x}) = 0.7 x_1 - 0.4 x_2 + 0.5 x_3$。\n  - $\\mathbf{x}_0 = [-0.2, 0.4, -0.1]$，$N = 800$，$\\sigma = 0.6$，$\\mathbf{s} = [0.5, 0.5, 0.5]$，$k = 3$，$\\tau = 0.3$，$\\lambda = 10^{-3}$。\n  - $S_{\\text{true}} = \\varnothing$。\n- 用例 3（强主效应加交互）：\n  - $f(\\mathbf{x}) = 4.0 x_1 + x_1 x_2 + 0.3 \\sin(2 \\pi x_3)$。\n  - $\\mathbf{x}_0 = [0.1, 0.8, 0.0]$，$N = 1000$，$\\sigma = 0.7$，$\\mathbf{s} = [0.5, 0.5, 0.5]$，$k = 3$，$\\tau = 0.1$，$\\lambda = 10^{-3}$。\n  - $S_{\\text{true}} = \\{(0,1)\\}$。\n- 用例 4（带二次项的相关结构）：\n  - $f(\\mathbf{x}) = (x_1 + x_2)^2 - 0.3 x_3$。\n  - $\\mathbf{x}_0 = [0.6, 0.6, -0.3]$，$N = 1200$，$\\sigma = 0.7$，$\\mathbf{s} = [0.5, 0.5, 0.5]$，$k = 3$，$\\tau = 0.15$，$\\lambda = 10^{-3}$。\n  - $S_{\\text{true}} = \\{(0,1)\\}$。\n\n您的程序应生成单行输出，其中包含一个以逗号分隔并用方括号括起来的结果列表，每个元素本身是一个形式为 $[C_{\\text{exact}}, C_{\\text{cons}}, \\Delta R^2]$ 的列表，其中 $\\Delta R^2$ 打印到小数点后六位，例如：`[[\\text{True},\\text{True},0.123456],[\\text{False},\\text{True},0.000000],[\\text{True},\\text{True},0.234567],[\\text{True},\\text{True},0.345678]]`。", "solution": "该问题要求设计并实现一个分层的两阶段流程，用于在特定查询点 $\\mathbf{x}_0$ 检测黑盒模型 $f(\\mathbf{x})$ 中的成对特征交互。该方法是局部可解释模型无关解释 (LIME) 的扩展，其中复杂的模型被一个更简单、可解释的代理模型在局部进行逼近。该流程基于加权最小二乘和统计相关的原理。\n\n首先，通过采样 $N$ 个扰动点 $\\mathbf{z}_i$ 来定义查询点 $\\mathbf{x}_0 \\in \\mathbb{R}^d$ 的局部邻域。这些点从一个多元正态分布中抽取，即 $\\mathbf{z}_i \\sim \\mathcal{N}(\\mathbf{x}_0, \\operatorname{diag}(\\mathbf{s}^2))$，其中 $\\mathbf{s}$ 是每个特征的标准差向量。每个样本 $\\mathbf{z}_i$ 的局部性由一个权重 $w_i$ 量化，该权重使用径向核函数计算：\n$$w_i = \\exp\\!\\left(-\\frac{\\lVert \\mathbf{z}_i - \\mathbf{x}_0 \\rVert_2^2}{2\\sigma^2}\\right)$$\n在这里，$\\sigma$ 是核带宽，控制局部邻域的大小。较大的 $\\sigma$ 会导致更全局的逼近。\n\n**第一阶段：主效应模型**\n\n第一阶段旨在捕捉特征的线性、可加效应。一个在中心化特征 $u_{i,j} = z_{i,j} - x_{0,j}$ 上线性的代理模型被拟合到黑盒预测 $y_i = f(\\mathbf{z}_i)$。该模型形式如下：\n$$\\hat{y}_i = \\beta_0 + \\sum_{j=1}^{d} \\beta_j u_{i,j}$$\n系数 $\\boldsymbol{\\beta} = [\\beta_0, \\beta_1, \\dots, \\beta_d]^T$ 使用加权岭回归进行估计，该方法最小化目标函数：\n$$\\mathcal{L}(\\boldsymbol{\\beta}) = \\sum_{i=1}^{N} w_i (y_i - \\hat{y}_i)^2 + \\lambda \\sum_{j=1}^{d} \\beta_j^2$$\n正则化项的强度为 $\\lambda > 0$，仅应用于主效应系数（$\\beta_1, \\dots, \\beta_d$），而不应用于截距 $\\beta_0$。这可以防止过拟合并稳定解。以矩阵形式表示，令 $\\mathbf{X}_{\\text{lin}}$ 为设计矩阵，其列包括截距和 $d$ 个中心化特征，$\\mathbf{y}$ 为黑盒预测向量，$\\mathbf{W}$ 为权重 $w_i$ 的对角矩阵。系数向量 $\\boldsymbol{\\beta}_{\\text{lin}}$ 是正规方程的解：\n$$(\\mathbf{X}_{\\text{lin}}^T \\mathbf{W} \\mathbf{X}_{\\text{lin}} + \\mathbf{\\Lambda}) \\boldsymbol{\\beta}_{\\text{lin}} = \\mathbf{X}_{\\text{lin}}^T \\mathbf{W} \\mathbf{y}$$\n其中 $\\mathbf{\\Lambda}$ 是一个对角矩阵，对应主效应的条目为 $\\lambda$，对应截距的条目为 $0$。\n\n拟合后，我们计算加权决定系数 $R^2_{\\text{lin}}$ 来量化拟合优度。其定义为：\n$$R^2_{\\text{lin}} = 1 - \\frac{\\text{SS}_{\\text{res}}}{\\text{SS}_{\\text{tot}}} = 1 - \\frac{\\sum_{i=1}^N w_i(y_i - \\hat{y}_{i, \\text{lin}})^2}{\\sum_{i=1}^N w_i(y_i - \\bar{y}_w)^2}$$\n其中 $\\hat{y}_{i, \\text{lin}}$ 是第一阶段模型的预测值，$\\bar{y}_w = \\frac{\\sum w_i y_i}{\\sum w_i}$ 是真实预测的加权均值。此阶段的残差 $r_i = y_i - \\hat{y}_{i, \\text{lin}}$ 代表了黑盒函数行为中未被线性、可加模型捕捉到的部分。\n\n**第二阶段：交互增强**\n\n第二阶段通过检查残差 $r_i$ 来探查成对交互效应。其假设是，如果特征 $p$ 和 $q$ 之间存在交互，残差将与交互项 $v_{i,p,q} = u_{i,p} \\cdot u_{i,q}$ 相关。每个潜在交互 $(p,q)$（其中 $1 \\le p  q \\le d$）的证据由残差向量 $\\mathbf{r}$ 和交互回归量向量 $\\mathbf{v}_{p,q}$ 之间的加权皮尔逊相关系数来量化：\n$$\\rho_w(\\mathbf{r}, \\mathbf{v}_{p,q}) = \\frac{\\text{Cov}_w(\\mathbf{r}, \\mathbf{v}_{p,q})}{\\sqrt{\\sigma^2_w(\\mathbf{r})\\sigma^2_w(\\mathbf{v}_{p,q})}}$$\n在这里，$\\text{Cov}_w$ 和 $\\sigma^2_w$ 分别表示使用权重 $w_i$ 计算的加权协方差和加权方差。\n\n通过选择最多 $k$ 个绝对加权相关性最大且该值超过阈值 $\\tau$ 的对，形成一组候选交互。如果选择了任何交互，它们将作为新列添加到设计矩阵中，从而创建一个增强矩阵 $\\mathbf{X}_{\\text{aug}}$。然后使用这个增强矩阵重新拟合加权岭回归，得到一组新的系数和一个新的加权决定系数 $R^2_{\\text{aug}}$。如果没有选择交互，则 $R^2_{\\text{aug}}$ 简单地等于 $R^2_{\\text{lin}}$。由于包含了交互项而带来的拟合改进由 $\\Delta R^2 = R^2_{\\text{aug}} - R^2_{\\text{lin}}$ 度量。\n\n最后，将选择的交互对集合 $S_{\\text{sel}}$ 与真实集合 $S_{\\text{true}}$进行比较，以使用两个布尔度量来评估流程的准确性：精确匹配正确性 ($C_{\\text{exact}}$) 和一致性检测 ($C_{\\text{cons}}$)。\n\n通过在初始采样步骤中使用固定的随机种子，整个流程得以确定性地实现，确保了所有测试用例结果的可复现性。", "answer": "```python\nimport numpy as np\nfrom itertools import combinations\nimport_scipy = False\ntry:\n    from scipy.special import expit\nexcept ImportError:\n    # This is just to satisfy the version requirement in problem description,\n    # but the library is not used in the solution.\n    import_scipy = False\n\n\ndef solve():\n    \"\"\"\n    Main function to run the hierarchical LIME procedure on all test cases\n    and print the formatted results.\n    \"\"\"\n\n    def solve_case(f, x0, d, N, sigma, s, k, tau, lambda_reg, S_true):\n        \"\"\"\n        Solves a single test case of the hierarchical LIME procedure.\n        \"\"\"\n        # Set a fixed seed for reproducibility.\n        rng = np.random.default_rng(seed=0)\n\n        # 1. Local Sampling: Generate N perturbations around x0.\n        x0_np = np.array(x0, dtype=float)\n        s_np = np.array(s, dtype=float)\n        cov_matrix = np.diag(s_np**2)\n        z_samples = rng.multivariate_normal(x0_np, cov_matrix, size=N)\n        y_true = np.array([f(z) for z in z_samples])\n\n        # 2. Locality Kernel: Compute weights for each sample.\n        distances_sq = np.sum((z_samples - x0_np)**2, axis=1)\n        weights = np.exp(-distances_sq / (2 * sigma**2))\n        sum_weights = np.sum(weights)\n        W = np.diag(weights)\n\n        # 3. Stage 1 (Main Effects Model)\n        # Construct design matrix with centered features.\n        u_centered = z_samples - x0_np\n        X_lin = np.hstack([np.ones((N, 1)), u_centered])\n        \n        # Fit weighted ridge regression.\n        reg_matrix_lin = lambda_reg * np.diag([0] + [1] * d)\n        \n        A_lin = X_lin.T @ W @ X_lin + reg_matrix_lin\n        b_lin = X_lin.T @ W @ y_true\n        beta_lin = np.linalg.solve(A_lin, b_lin)\n        \n        y_pred_lin = X_lin @ beta_lin\n        residuals = y_true - y_pred_lin\n        \n        # Compute weighted R-squared.\n        y_mean_w = np.sum(weights * y_true) / sum_weights\n        ss_tot = np.sum(weights * (y_true - y_mean_w)**2)\n        ss_res_lin = np.sum(weights * residuals**2)\n        r_squared_lin = 1 - ss_res_lin / ss_tot if ss_tot > 1e-12 else 0.0\n\n        # 4. Stage 2 (Interaction Augmentation)\n        feature_pairs = list(combinations(range(d), 2))\n        interaction_correlations = []\n        \n        res_mean_w = np.sum(weights * residuals) / sum_weights\n        res_var_w = np.sum(weights * (residuals - res_mean_w)**2) / sum_weights\n        \n        if res_var_w > 1e-12: # Only check for correlations if residuals have variance.\n            for p, q in feature_pairs:\n                v_interaction = u_centered[:, p] * u_centered[:, q]\n                \n                v_mean_w = np.sum(weights * v_interaction) / sum_weights\n                v_var_w = np.sum(weights * (v_interaction - v_mean_w)**2) / sum_weights\n                \n                if v_var_w > 1e-12:\n                    cov_w = np.sum(weights * (residuals - res_mean_w) * (v_interaction - v_mean_w)) / sum_weights\n                    corr = cov_w / np.sqrt(res_var_w * v_var_w)\n                    interaction_correlations.append({'pair': (p, q), 'corr': abs(corr)})\n\n        # Select interactions based on correlation threshold and k.\n        interaction_correlations.sort(key=lambda x: x['corr'], reverse=True)\n        selected_interactions = [ic['pair'] for ic in interaction_correlations if ic['corr'] > tau]\n        S_sel = set(selected_interactions[:k])\n\n        # If interactions are selected, augment the model and refit.\n        if not S_sel:\n            r_squared_aug = r_squared_lin\n        else:\n            sorted_sel = sorted(list(S_sel))\n            interaction_cols = [(u_centered[:, p] * u_centered[:, q])[:, np.newaxis] for p, q in sorted_sel]\n            X_aug = np.hstack([X_lin] + interaction_cols)\n            \n            num_interactions = len(S_sel)\n            reg_matrix_aug = lambda_reg * np.diag([0] + [1] * d + [1] * num_interactions)\n            \n            A_aug = X_aug.T @ W @ X_aug + reg_matrix_aug\n            b_aug = X_aug.T @ W @ y_true\n            beta_aug = np.linalg.solve(A_aug, b_aug)\n            \n            y_pred_aug = X_aug @ beta_aug\n            ss_res_aug = np.sum(weights * (y_true - y_pred_aug)**2)\n            r_squared_aug = 1 - ss_res_aug / ss_tot if ss_tot > 1e-12 else 0.0\n\n        # 5. Calculate final metrics.\n        delta_r2 = r_squared_aug - r_squared_lin\n        \n        C_exact = (S_sel == S_true)\n        \n        if not S_true:  # S_true is empty\n            C_cons = (not S_sel)\n        else:  # S_true is not empty\n            C_cons = bool(S_sel.intersection(S_true))\n\n        return C_exact, C_cons, delta_r2\n\n    test_cases = [\n        {\n            \"f\": lambda x: x[0] * x[1] + 0.2 * x[2] + 0.1 * np.sin(np.pi * x[0]),\n            \"x0\": [0.3, -0.5, 0.1], \"d\": 3, \"N\": 1200, \"sigma\": 0.7, \"s\": [0.5, 0.5, 0.5],\n            \"k\": 3, \"tau\": 0.1, \"lambda_reg\": 1e-3, \"S_true\": {(0, 1)}\n        },\n        {\n            \"f\": lambda x: 0.7 * x[0] - 0.4 * x[1] + 0.5 * x[2],\n            \"x0\": [-0.2, 0.4, -0.1], \"d\": 3, \"N\": 800, \"sigma\": 0.6, \"s\": [0.5, 0.5, 0.5],\n            \"k\": 3, \"tau\": 0.3, \"lambda_reg\": 1e-3, \"S_true\": set()\n        },\n        {\n            \"f\": lambda x: 4.0 * x[0] + x[0] * x[1] + 0.3 * np.sin(2 * np.pi * x[2]),\n            \"x0\": [0.1, 0.8, 0.0], \"d\": 3, \"N\": 1000, \"sigma\": 0.7, \"s\": [0.5, 0.5, 0.5],\n            \"k\": 3, \"tau\": 0.1, \"lambda_reg\": 1e-3, \"S_true\": {(0, 1)}\n        },\n        {\n            \"f\": lambda x: (x[0] + x[1])**2 - 0.3 * x[2],\n            \"x0\": [0.6, 0.6, -0.3], \"d\": 3, \"N\": 1200, \"sigma\": 0.7, \"s\": [0.5, 0.5, 0.5],\n            \"k\": 3, \"tau\": 0.15, \"lambda_reg\": 1e-3, \"S_true\": {(0, 1)}\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        result = solve_case(**case)\n        results.append(result)\n\n    # 6. Format the output as specified.\n    inner_results_str = []\n    for res in results:\n        c_exact, c_cons, delta_r2 = res\n        inner_str = f\"[{str(c_exact)},{str(c_cons)},{delta_r2:.6f}]\"\n        inner_results_str.append(inner_str)\n    \n    final_output = f\"[{','.join(inner_results_str)}]\"\n    print(final_output)\n\nsolve()\n```", "id": "3140901"}]}