{"hands_on_practices": [{"introduction": "模型解释性的一项关键任务是理解特征如何协同工作。一个经典的“纯交互”案例是异或（XOR）函数，其中单个特征本身没有任何作用，只有组合起来才有效果。这项练习将让您亲手比较不同的解释方法——部分依赖图（PDP）、个体条件期望（ICE）和 Shapley 交互值——在捕捉这种交互作用时的成功与失败，从而揭示全局解释与局部解释之间的关键差异。通过这个思想实验，您将体会到为何平均化的全局方法有时会掩盖模型行为的真相 [@problem_id:3132648]。", "problem": "给定一个定义在两个特征上的二元交互模型。设 $x = (x_{1}, x_{2})$，其中每个 $x_{i} \\in \\{0,1\\}$。将目标函数定义为异或交互 $f(x) = \\mathbf{1}[x_{1} \\oplus x_{2}]$，其中 $\\mathbf{1}[\\cdot]$ 是指示函数，$\\oplus$ 表示逻辑异或。考虑一个背景数据生成过程，其中 $X_{1} \\sim \\text{Bernoulli}(p_{1})$ 且 $X_{2} \\sim \\text{Bernoulli}(p_{2})$，彼此独立。必须根据此背景计算可解释性量，并使用以下基本定义：\n- 偏依赖图 (PDP)：对于特征 $i \\in \\{1,2\\}$，偏依赖函数为 $PD_{i}(a) = \\mathbb{E}_{X_{-i}}[f(x) \\mid x_{i}=a]$，其中期望是针对 $X_{-i}$ 的背景分布计算的，将其他特征视为随机变量。\n- 个体条件期望 (ICE)：对于一个具有 $x_{-i}^{\\star}$ 的固定实例，特征 $i$ 的个体条件期望曲线是 $ICE_{i}(a \\mid x_{-i}^{\\star}) = f(x)$，其中 $x_{i}=a$ 且 $x_{-i}=x_{-i}^{\\star}$。\n- Shapley交互指数：对于点 $x$ 处的特征 $i$ 和 $j$，将联盟 $S \\subseteq \\{1,2\\}$ 的特征函数定义为 $v(S) = \\mathbb{E}[f(X) \\mid X_{S} = x_{S}]$，这是在一个干预背景下计算的，其中 $S$ 之外的特征从其边缘分布中独立抽样。在点 $x$ 处，特征1和2之间的Shapley交互值是平均二阶离散差分\n$$\\phi_{1,2}^{\\mathrm{int}}(x) = \\sum_{S \\subseteq \\{1,2\\}\\setminus\\{1,2\\}} \\frac{|S|!\\,(2 - |S| - 2)!}{2\\,(2-1)!} \\big(v(S \\cup \\{1,2\\}) - v(S \\cup \\{1\\}) - v(S \\cup \\{2\\}) + v(S)\\big),$$\n对于2个特征，该公式简化为\n$$\\phi_{1,2}^{\\mathrm{int}}(x) = \\frac{1}{2}\\big(v(\\{1,2\\}) - v(\\{1\\}) - v(\\{2\\}) + v(\\varnothing)\\big).$$\n上述所有期望都是相对于指定的独立伯努利背景计算的。\n\n你的任务是：编写一个完整的程序，该程序构建 $f(x)$，并为每个指定的测试用例 $(p_{1}, p_{2})$ 计算以下四个量。这四个量共同比较了偏依赖图 (PDP)、个体条件期望 (ICE) 和 Shapley 交互指数如何捕捉模型的纯粹交互作用。\n\n对于每个测试用例 $(p_{1}, p_{2})$，计算：\n1) 特征1的PDP主效应大小，定义为 $M_{1} = \\left|PD_{1}(1) - PD_{1}(0)\\right|$。\n2) 特征2的PDP主效应大小，定义为 $M_{2} = \\left|PD_{2}(1) - PD_{2}(0)\\right|$。\n3) 特征1的ICE异质性度量，使用两个参考实例 $x_{2}^{\\star} \\in \\{0,1\\}$ 定义，即在 $x_{1} \\in \\{0,1\\}$ 范围内两条ICE曲线之间的平均逐点绝对差：\n$$H_{\\mathrm{ICE}} = \\frac{1}{2} \\sum_{a \\in \\{0,1\\}} \\left| ICE_{1}(a \\mid x_{2}^{\\star}=0) - ICE_{1}(a \\mid x_{2}^{\\star}=1) \\right|.$$\n4) 在4个输入点 $x \\in \\{0,1\\}^{2}$ 上的平均绝对Shapley交互大小：\n$$A_{\\mathrm{SHAP}} = \\frac{1}{4} \\sum_{x \\in \\{0,1\\}^{2}} \\left|\\phi_{1,2}^{\\mathrm{int}}(x)\\right|,$$\n其中每个 $\\phi_{1,2}^{\\mathrm{int}}(x)$ 使用如上定义的干预背景。\n\n使用以下背景参数值测试套件：\n- 情况1： $(p_{1}, p_{2}) = (0.5, 0.5)$。\n- 情况2： $(p_{1}, p_{2}) = (0.5, 0.8)$。\n- 情况3： $(p_{1}, p_{2}) = (0.5, 1.0)$。\n\n你的程序应为每个测试用例输出一个结果，每个结果都是一个列表 $[M_{1}, M_{2}, H_{\\mathrm{ICE}}, A_{\\mathrm{SHAP}}]$，并按此顺序排列。最终输出必须是包含这三个列表的单行列表，以逗号分隔并用方括号括起来，例如： \"[[m_{1,1},m_{1,2},m_{1,3},m_{1,4}],[m_{2,1},m_{2,2},m_{2,3},m_{2,4}],[m_{3,1},m_{3,2},m_{3,3},m_{3,4}]]\"。\n\n覆盖范围设计说明：\n- 情况1测试了对称背景，其中 $p_{1}=p_{2}$ 且二者都等于 $0.5$。在此情况下，纯粹的交互作用应导致 $M_{1} = 0$ 和 $M_{2} = 0$，但ICE和Shapley交互会检测到非零的交互作用。\n- 情况2在 $X_{2}$ 中引入了边缘不平衡，表明即使在 $f$ 中没有任何加性主效应，$M_{1}$ 也可能变为非零。\n- 情况3是 $X_{2}$ 的一个极端倾斜情况，其中 $p_{2} = 1.0$，用于在退化的背景分布下测试这些度量的边界行为，同时保持干预期望的良好定义。\n\n不涉及物理单位。所有返回的量必须是实数。输出格式必须严格遵循上述描述的单行格式。不要打印任何解释性文本，只输出所需的最终列表。", "solution": "问题陈述经过正式评估，被认定为有效。它是自洽的，在统计学习和模型可解释性理论方面有科学依据，并且是适定问题。所有定义和变量的陈述都足够精确，可以得出一个唯一且有意义的解。因此，我们可以着手对所需量进行正式推导。\n\n目标函数是作用于两个二元特征 $x_1, x_2 \\in \\{0, 1\\}$ 的异或 (XOR) 函数。它由 $f(x_1, x_2) = \\mathbf{1}[x_1 \\oplus x_2]$ 给出。该函数的具体值为：\n$$f(0, 0) = 0$$\n$$f(0, 1) = 1$$\n$$f(1, 0) = 1$$\n$$f(1, 1) = 0$$\n背景数据由两个独立的伯努利随机变量生成，$X_1 \\sim \\text{Bernoulli}(p_1)$ 和 $X_2 \\sim \\text{Bernoulli}(p_2)$。这意味着 $P(X_1=1) = p_1$，$P(X_1=0) = 1-p_1$，对 $X_2$ 也类似。我们现在将推导所要求的四个量的解析表达式。\n\n**1. 特征1的PDP主效应大小 ($M_1$)**\n\n特征1在值 $a \\in \\{0, 1\\}$ 处的偏依赖图 (PDP) 函数定义为 $PD_1(a) = \\mathbb{E}_{X_2}[f(a, X_2)]$。期望是针对 $X_2$ 的背景分布计算的。\n\n当 $a=1$ 时：\n$$PD_1(1) = \\mathbb{E}_{X_2}[f(1, X_2)] = f(1, 0) \\cdot P(X_2=0) + f(1, 1) \\cdot P(X_2=1)$$\n$$PD_1(1) = 1 \\cdot (1-p_2) + 0 \\cdot p_2 = 1-p_2$$\n当 $a=0$ 时：\n$$PD_1(0) = \\mathbb{E}_{X_2}[f(0, X_2)] = f(0, 0) \\cdot P(X_2=0) + f(0, 1) \\cdot P(X_2=1)$$\n$$PD_1(0) = 0 \\cdot (1-p_2) + 1 \\cdot p_2 = p_2$$\n特征1的PDP主效应大小为 $M_1 = |PD_1(1) - PD_1(0)|$。\n$$M_1 = |(1-p_2) - p_2| = |1 - 2p_2|$$\n\n**2. 特征2的PDP主效应大小 ($M_2$)**\n\n类似地，特征2在值 $a \\in \\{0, 1\\}$ 处的PDP函数是 $PD_2(a) = \\mathbb{E}_{X_1}[f(X_1, a)]$。\n\n当 $a=1$ 时：\n$$PD_2(1) = \\mathbb{E}_{X_1}[f(X_1, 1)] = f(0, 1) \\cdot P(X_1=0) + f(1, 1) \\cdot P(X_1=1)$$\n$$PD_2(1) = 1 \\cdot (1-p_1) + 0 \\cdot p_1 = 1-p_1$$\n当 $a=0$ 时：\n$$PD_2(0) = \\mathbb{E}_{X_1}[f(X_1, 0)] = f(0, 0) \\cdot P(X_1=0) + f(1, 0) \\cdot P(X_1=1)$$\n$$PD_2(0) = 0 \\cdot (1-p_1) + 1 \\cdot p_1 = p_1$$\n特征2的PDP主效应大小为 $M_2 = |PD_2(1) - PD_2(0)|$。\n$$M_2 = |(1-p_1) - p_1| = |1 - 2p_1|$$\n\n**3. ICE异质性度量 ($H_{\\mathrm{ICE}}$)**\n\n对于一个固定值 $x_2^\\star$，特征1的个体条件期望 (ICE) 曲线是 $ICE_1(a | x_2^\\star) = f(a, x_2^\\star)$。异质性度量 $H_{\\mathrm{ICE}}$ 比较了通过设置 $x_2^\\star=0$ 和 $x_2^\\star=1$ 生成的两条可能的ICE曲线。\n\n当 $x_2^\\star=0$ 时，ICE曲线为 $ICE_1(a|0) = f(a, 0)$。\n$$ICE_1(0|0) = f(0, 0) = 0$$\n$$ICE_1(1|0) = f(1, 0) = 1$$\n当 $x_2^\\star=1$ 时，ICE曲线为 $ICE_1(a|1) = f(a, 1)$。\n$$ICE_1(0|1) = f(0, 1) = 1$$\n$$ICE_1(1|1) = f(1, 1) = 0$$\n异质性度量定义为 $H_{\\mathrm{ICE}} = \\frac{1}{2} \\sum_{a \\in \\{0,1\\}} |ICE_1(a | x_2^\\star=0) - ICE_1(a | x_2^\\star=1)|$。\n$$H_{\\mathrm{ICE}} = \\frac{1}{2} \\left[ |ICE_1(0|0) - ICE_1(0|1)| + |ICE_1(1|0) - ICE_1(1|1)| \\right]$$\n$$H_{\\mathrm{ICE}} = \\frac{1}{2} \\left[ |0 - 1| + |1 - 0| \\right] = \\frac{1}{2} (1 + 1) = 1$$\n该度量是一个常数值1，表明特征1的影响总是根据特征2的值而完全反转，这正是XOR交互的定义。该度量与背景分布概率 $p_1$ 和 $p_2$ 无关。\n\n**4. 平均绝对Shapley交互大小 ($A_{\\mathrm{SHAP}}$)**\n\nShapley交互值为 $\\phi_{1,2}^{\\mathrm{int}}(x) = \\frac{1}{2} \\left(v(\\{1,2\\}) - v(\\{1\\}) - v(\\{2\\}) + v(\\varnothing)\\right)$。我们首先计算特征函数 $v(S) = \\mathbb{E}[f(X) | X_S = x_S]$ 的各项。\n$v(\\{1,2\\}) = f(x_1, x_2)$，因为两个特征都是固定的。\n$v(\\{1\\}) = \\mathbb{E}_{X_2}[f(x_1, X_2)] = PD_1(x_1)$。当 $x_1=0$ 时，其值为 $p_2$；当 $x_1=1$ 时，其值为 $1-p_2$。\n$v(\\{2\\}) = \\mathbb{E}_{X_1}[f(X_1, x_2)] = PD_2(x_2)$。当 $x_2=0$ 时，其值为 $p_1$；当 $x_2=1$ 时，其值为 $1-p_1$。\n$v(\\varnothing) = \\mathbb{E}[f(X_1, X_2)] = P(X_1=0, X_2=1) + P(X_1=1, X_2=0) = (1-p_1)p_2 + p_1(1-p_2)$。\n\n我们现在为四个点 $x \\in \\{0,1\\}^2$ 中的每一个计算 $\\phi_{1,2}^{\\mathrm{int}}(x)$。\n- 对于 $x=(0,0)$：\n$\\phi_{1,2}^{\\mathrm{int}}(0,0) = \\frac{1}{2} [f(0,0) - v(\\{1\\}) - v(\\{2\\}) + v(\\varnothing)] = \\frac{1}{2} [0 - p_2 - p_1 + (1-p_1)p_2 + p_1(1-p_2)] = \\frac{1}{2}[ -p_1 - p_2 + p_2 - p_1 p_2 + p_1 - p_1 p_2 ] = \\frac{1}{2}[-2p_1 p_2] = -p_1 p_2$。\n- 对于 $x=(0,1)$：\n$\\phi_{1,2}^{\\mathrm{int}}(0,1) = \\frac{1}{2} [f(0,1) - v(\\{1\\}) - v(\\{2\\}) + v(\\varnothing)] = \\frac{1}{2} [1 - p_2 - (1-p_1) + (1-p_1)p_2 + p_1(1-p_2)] = \\frac{1}{2}[p_1 - p_2 + p_2 - p_1 p_2 + p_1 - p_1 p_2] = \\frac{1}{2}[2p_1 - 2p_1 p_2] = p_1(1-p_2)$。\n- 对于 $x=(1,0)$：\n$\\phi_{1,2}^{\\mathrm{int}}(1,0) = \\frac{1}{2} [f(1,0) - v(\\{1\\}) - v(\\{2\\}) + v(\\varnothing)] = \\frac{1}{2} [1 - (1-p_2) - p_1 + (1-p_1)p_2 + p_1(1-p_2)] = \\frac{1}{2}[p_2 - p_1 + p_2 - p_1 p_2 + p_1 - p_1 p_2] = \\frac{1}{2}[2p_2 - 2p_1 p_2] = p_2(1-p_1)$。\n- 对于 $x=(1,1)$：\n$\\phi_{1,2}^{\\mathrm{int}}(1,1) = \\frac{1}{2} [f(1,1) - v(\\{1\\}) - v(\\{2\\}) + v(\\varnothing)] = \\frac{1}{2} [0 - (1-p_2) - (1-p_1) + (1-p_1)p_2 + p_1(1-p_2)] = \\frac{1}{2}[-2+p_1+p_2 + p_2 - p_1 p_2 + p_1 - p_1 p_2] = \\frac{1}{2}[2p_1+2p_2-2 - 2p_1 p_2] = - (1 - p_1 - p_2 + p_1 p_2) = -(1-p_1)(1-p_2)$。\n\n平均绝对Shapley交互大小是 $A_{\\mathrm{SHAP}} = \\frac{1}{4} \\sum_{x \\in \\{0,1\\}^2} |\\phi_{1,2}^{\\mathrm{int}}(x)|$。由于 $p_1, p_2 \\in [0, 1]$，所有项都是非负的。\n$$A_{\\mathrm{SHAP}} = \\frac{1}{4} \\left[ |-p_1 p_2| + |p_1(1-p_2)| + |p_2(1-p_1)| + |-(1-p_1)(1-p_2)| \\right]$$\n$$A_{\\mathrm{SHAP}} = \\frac{1}{4} \\left[ p_1 p_2 + p_1(1-p_2) + p_2(1-p_1) + (1-p_1)(1-p_2) \\right]$$\n展开并简化括号中的和：\n$$p_1 p_2 + p_1 - p_1 p_2 + p_2 - p_1 p_2 + 1 - p_1 - p_2 + p_1 p_2 = 1$$\n因此，平均绝对Shapley交互大小是恒定的：\n$$A_{\\mathrm{SHAP}} = \\frac{1}{4}$$\n\n**公式摘要和测试用例计算**\n\n推导出的表达式为：\n1. $M_1 = |1 - 2p_2|$\n2. $M_2 = |1 - 2p_1|$\n3. $H_{\\mathrm{ICE}} = 1$\n4. $A_{\\mathrm{SHAP}} = 0.25$\n\n我们将这些应用于给定的测试用例：\n- **情况1**：$(p_1, p_2) = (0.5, 0.5)$\n  - $M_1 = |1 - 2(0.5)| = |1 - 1| = 0.0$\n  - $M_2 = |1 - 2(0.5)| = |1 - 1| = 0.0$\n  - $H_{\\mathrm{ICE}} = 1.0$\n  - $A_{\\mathrm{SHAP}} = 0.25$\n  - 结果： $[0.0, 0.0, 1.0, 0.25]$\n\n- **情况2**：$(p_1, p_2) = (0.5, 0.8)$\n  - $M_1 = |1 - 2(0.8)| = |1 - 1.6| = 0.6$\n  - $M_2 = |1 - 2(0.5)| = |1 - 1| = 0.0$\n  - $H_{\\mathrm{ICE}} = 1.0$\n  - $A_{\\mathrm{SHAP}} = 0.25$\n  - 结果： $[0.6, 0.0, 1.0, 0.25]$\n\n- **情况3**：$(p_1, p_2) = (0.5, 1.0)$\n  - $M_1 = |1 - 2(1.0)| = |1 - 2| = 1.0$\n  - $M_2 = |1 - 2(0.5)| = |1 - 1| = 0.0$\n  - $H_{\\mathrm{ICE}} = 1.0$\n  - $A_{\\mathrm{SHAP}} = 0.25$\n  - 结果： $[1.0, 0.0, 1.0, 0.25]$", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes interpretability quantities for the XOR function f(x1, x2) = x1 ^ x2.\n    \n    The quantities are:\n    1. M1: PDP main-effect magnitude for feature 1.\n    2. M2: PDP main-effect magnitude for feature 2.\n    3. H_ICE: ICE heterogeneity metric for feature 1.\n    4. A_SHAP: Average absolute Shapley interaction magnitude.\n\n    These are computed for three test cases of background distribution parameters (p1, p2).\n    \"\"\"\n    \n    # Test cases for (p1, p2)\n    test_cases = [\n        (0.5, 0.5), # Case 1\n        (0.5, 0.8), # Case 2\n        (0.5, 1.0), # Case 3\n    ]\n\n    all_results = []\n    \n    for p1, p2 in test_cases:\n        # Based on the analytical derivations:\n        # M_1 = |1 - 2*p2|\n        # M_2 = |1 - 2*p1|\n        # H_ICE = 1\n        # A_SHAP = 0.25\n        \n        # 1. PDP main-effect magnitude for feature 1\n        m1 = np.abs(1 - 2 * p2)\n        \n        # 2. PDP main-effect magnitude for feature 2\n        m2 = np.abs(1 - 2 * p1)\n        \n        # 3. ICE heterogeneity metric\n        # The metric is constant for the XOR function.\n        # H_ICE = 0.5 * (|f(0,0)-f(0,1)| + |f(1,0)-f(1,1)|)\n        # H_ICE = 0.5 * (|0-1| + |1-0|) = 0.5 * (1 + 1) = 1.0\n        h_ice = 1.0\n        \n        # 4. Average absolute Shapley interaction magnitude\n        # This metric is also constant for the XOR function under the given definition.\n        # The sum of absolute interaction values is:\n        # |p1*p2| + |p1*(1-p2)| + |p2*(1-p1)| + |(1-p1)*(1-p2)| = 1\n        # A_SHAP = (1/4) * 1 = 0.25\n        a_shap = 0.25\n        \n        # Store the results as a list of floating-point numbers\n        result = [float(m1), float(m2), float(h_ice), float(a_shap)]\n        all_results.append(result)\n\n    # Format the output string as specified in the problem statement.\n    # e.g., \"[[0.0,0.0,1.0,0.25],[0.6,0.0,1.0,0.25],[1.0,0.0,1.0,0.25]]\"\n    string_formatted_results = []\n    for res_list in all_results:\n        # Convert each inner list to its string representation and remove spaces\n        list_str = str(res_list).replace(\" \", \"\")\n        string_formatted_results.append(list_str)\n    \n    final_output = f\"[{','.join(string_formatted_results)}]\"\n    \n    print(final_output)\n\nsolve()\n```", "id": "3132648"}, {"introduction": "在现实世界的数据中，特征之间往往存在相关性，这使得将重要性归因于单个特征变得非常复杂。这种现象被称为“共线性”，是解释机器学习模型时必须面对的核心挑战之一。本练习将引导您直接探索这一难题，通过分析一个具有相关特征的线性模型，您将比较标准 SHAP 值与分组 SHAP 值的表现，从而深入理解当特征并非相互独立时，如何生成更忠实、更公平的解释 [@problem_id:3132668]。", "problem": "给定一个线性模型、一个高斯背景分布以及一个待解释的特定实例。您的任务是实现基于 SHapley Additive exPlanations (SHAP) 的特征归因方法，比较标准特征级归因与将一组共线性特征视为单个参与者的分组归因，并在共线性条件下量化公平性准则。\n\n考虑以下设置。设模型为线性函数\n$$\nf(\\mathbf{x}) = b + \\mathbf{w}^\\top \\mathbf{x},\n$$\n其参数向量为 $$\\mathbf{w} = (1, 1, 1/2)$$，偏置为 $$b = 0$$。待解释的实例为\n$$\n\\mathbf{x} = (1, 1, 2).\n$$\n输入 $$\\mathbf{X} = (X_1, X_2, X_3)$$ 的背景分布为多元正态分布 $$\\mathcal{N}(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})$$，其\n$$\n\\boldsymbol{\\mu} = (0,0,0)\n$$\n协方差矩阵为\n$$\n\\boldsymbol{\\Sigma}(\\rho) = \\begin{pmatrix}\n1  \\rho  0\\\\\n\\rho  1  0\\\\\n0  0  1\n\\end{pmatrix},\n$$\n对于给定的相关系数参数 $$\\rho \\in [-1,1]$$。特征 $$X_1$$ 和 $$X_2$$ 相关，相关系数为 $$\\rho$$，而 $$X_3$$ 与这两者都独立。\n\n将特征索引集定义为 $$N = \\{1,2,3\\}$$，并为任意子集 $$S \\subseteq N$$ 定义值函数为\n$$\nv(S) = \\mathbb{E}\\big[f(\\mathbf{X}) \\,\\big|\\, \\mathbf{X}_S = \\mathbf{x}_S\\big],\n$$\n其中 $$\\mathbf{X}_S$$ 和 $$\\mathbf{x}_S$$ 表示由 $$S$$ 索引的 $$\\mathbf{X}$$ 和 $$\\mathbf{x}$$ 的子向量。\n\n对于 $$i \\in N$$ 的标准特征级 SHAP 值 $$\\phi_i$$ 是参与者为 $$N$$ 的合作博弈 $$v(S)$$ 的 Shapley 值。针对联盟 $$G_1 = \\{1,2\\}$$ 的分组 SHAP 将该联盟视为单个参与者，并在商博弈中计算 Shapley 值 $$\\phi_{G_1}$$，该商博弈的参与者为 $$\\{G_1, \\{3\\}\\}$$，其值函数继承自 $$v(S)$$，通过将每个参与者联盟映射到其成员特征索引的并集得到。\n\n您的程序必须：\n- 在给定的高斯背景下，使用多元正态分布的条件期望精确地实现值函数 $$v(S)$$。\n- 计算该模型和实例的标准 SHAP 值 $$\\phi_1, \\phi_2, \\phi_3$$。\n- 计算联盟 $$G_1 = \\{1,2\\}$$ 的分组 Shapley 值 $$\\phi_{G_1}$$。\n- 为每个测试用例量化三个诊断指标：\n  1. 可加性误差：\n     $$\n     \\left|\\sum_{i \\in N} \\phi_i - \\big(f(\\mathbf{x}) - \\mathbb{E}[f(\\mathbf{X})]\\big)\\right|.\n     $$\n  2. 组一致性误差：\n     $$\n     \\left| \\phi_{G_1} - (\\phi_1 + \\phi_2) \\right|.\n     $$\n  3. 共线性组内的对称性偏差：\n     $$\n     \\left| \\phi_1 - \\phi_2 \\right|.\n     $$\n\n使用以下相关性值的测试套件：\n- $$\\rho = 0.0$$（独立特征），\n- $$\\rho = 0.9$$（高度共线性），\n- $$\\rho = 1.0$$（完全共线性）。\n\n对于所有测试用例，取 $$\\mathbf{w} = (1,1,1/2)$$、$$b = 0$$、$$\\boldsymbol{\\mu} = (0,0,0)$$ 和 $$\\mathbf{x} = (1,1,2)$$。\n\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果，其中每个元素对应一个测试用例（按测试套件的顺序），并且本身是一个包含三个浮点数的列表\n$$\n[\\text{additivity\\_error}, \\text{group\\_consistency\\_error}, \\text{symmetry\\_deviation}].\n$$\n例如，输出格式必须为\n$$\n[[a_1,b_1,c_1],[a_2,b_2,c_2],[a_3,b_3,c_3]],\n$$\n其中每个 $$a_k, b_k, c_k$$ 都是一个浮点数。此问题不涉及任何物理单位或角度，输出的任何地方都不应使用百分比。", "solution": "用户希望解决一个关于线性模型在高斯背景分布下的 SHAP (SHapley Additive exPlanations) 问题。该问题要求计算标准和分组的 SHAP 值，并分析在不同共线性水平下的三个诊断指标。\n\n### **问题验证**\n问题陈述具有科学依据、适定且客观。它为机器学习可解释性领域中一个明确定义的任务提供了一套完整且一致的给定条件。其基础概念——线性模型、多元正态分布、条件期望和 Shapley 值——在数学和统计学中都是标准的。完全相关（$\\rho=1$）的情况会导致奇异协方差矩阵，但所需的条件期望仍然是良定义的，因为条件变量与线性依赖关系一致。该问题是有效的。\n\n### **分步解决方案**\n\n解决方案分为四个步骤：\n1.  为所有特征联盟 $S$ 推导值函数 $v(S)$。\n2.  计算标准特征级 SHAP 值 $\\phi_1, \\phi_2, \\phi_3$。\n3.  计算联盟 $G_1 = \\{1,2\\}$ 的分组 SHAP 值 $\\phi_{G_1}$。\n4.  为每个 $\\rho$ 值计算三个指定的诊断指标。\n\n**1. 值函数 $v(S)$ 的推导**\n\n模型为 $f(\\mathbf{x}) = \\mathbf{w}^\\top \\mathbf{x} = 1 \\cdot x_1 + 1 \\cdot x_2 + \\frac{1}{2} \\cdot x_3$，因为 $b=0$。背景分布为 $\\mathbf{X} \\sim \\mathcal{N}(\\mathbf{0}, \\boldsymbol{\\Sigma}(\\rho))$。值函数定义为条件期望 $v(S) = \\mathbb{E}[f(\\mathbf{X}) | \\mathbf{X}_S = \\mathbf{x}_S]$。根据期望的线性性质：\n$$v(S) = \\mathbf{w}^\\top \\mathbb{E}[\\mathbf{X} | \\mathbf{X}_S = \\mathbf{x}_S]$$\n对于均值为 $\\boldsymbol{\\mu} = \\mathbf{0}$ 的多元正态分布，其条件期望为 $\\mathbb{E}[\\mathbf{X}_{\\bar{S}} | \\mathbf{X}_S = \\mathbf{x}_S] = \\boldsymbol{\\Sigma}_{\\bar{S}S} \\boldsymbol{\\Sigma}_{SS}^{-1} \\mathbf{x}_S$，其中 $\\bar{S}=N\\setminus S$。我们作为条件的 $\\mathbf{X}$ 的分量保持在 $\\mathbf{x}=(1,1,2)$ 中的值不变。\n\n我们为所有 $S \\subseteq N = \\{1,2,3\\}$ 计算 $v(S)$：\n-   $S = \\emptyset$: $v(\\emptyset) = \\mathbb{E}[f(\\mathbf{X})] = \\mathbf{w}^\\top \\mathbb{E}[\\mathbf{X}] = \\mathbf{w}^\\top \\boldsymbol{\\mu} = 0$。\n-   $S = \\{1\\}$: 我们需要 $\\mathbb{E}[X_2|X_1=x_1]$ 和 $\\mathbb{E}[X_3|X_1=x_1]$。因为 $X_3$ 与 $X_1$ 独立，所以 $\\mathbb{E}[X_3|X_1=x_1]=\\mathbb{E}[X_3]=0$。在给定 $X_1=x_1$ 的条件下 $X_2$ 的条件均值为 $\\mathbb{E}[X_2|X_1=x_1]=\\mu_2 + \\frac{\\sigma_{12}}{\\sigma_{11}}(x_1-\\mu_1) = 0 + \\frac{\\rho}{1}(x_1-0) = \\rho x_1$。\n    $v(\\{1\\}) = w_1x_1 + w_2(\\rho x_1) + w_3(0) = 1(1) + 1(\\rho \\cdot 1) = 1+\\rho$。\n-   $S = \\{2\\}$: 根据与特征 1 的对称性， $v(\\{2\\}) = w_2x_2 + w_1(\\rho x_2) = 1(1) + 1(\\rho \\cdot 1) = 1+\\rho$。\n-   $S = \\{3\\}$: $X_1, X_2$ 与 $X_3$ 独立，所以 $\\mathbb{E}[X_1|X_3=x_3]=0$ 且 $\\mathbb{E}[X_2|X_3=x_3]=0$。\n    $v(\\{3\\}) = w_3x_3 = \\frac{1}{2}(2) = 1$。\n-   $S = \\{1,2\\}$: 我们需要 $\\mathbb{E}[X_3|X_1=x_1, X_2=x_2]$。由于 $X_3$ 与 $(X_1, X_2)$ 独立，此值为 $\\mathbb{E}[X_3]=0$。\n    $v(\\{1,2\\}) = w_1x_1 + w_2x_2 = 1(1) + 1(1) = 2$。即使对于 $\\rho=1$，此式也成立，因为我们不需要对 $\\boldsymbol{\\Sigma}_{\\{1,2\\},\\{1,2\\}}$ 求逆。\n-   $S = \\{1,3\\}$: 我们需要 $\\mathbb{E}[X_2|X_1=x_1, X_3=x_3]$。$X_3$ 与 $(X_1, X_2)$ 的独立性意味着这只是 $\\mathbb{E}[X_2|X_1=x_1]=\\rho x_1$。\n    $v(\\{1,3\\}) = w_1x_1 + w_3x_3 + w_2(\\rho x_1) = 1(1) + \\frac{1}{2}(2) + 1(\\rho \\cdot 1) = 2+\\rho$。\n-   $S = \\{2,3\\}$: 根据与 $\\{1,3\\}$ 的对称性， $v(\\{2,3\\}) = 2+\\rho$。\n-   $S = \\{1,2,3\\}$: $v(\\{1,2,3\\}) = f(\\mathbf{x}) = 1(1)+1(1)+\\frac{1}{2}(2) = 3$。\n\n**2. 标准 SHAP 值的计算**\n\n对于参与者集合为 $N$ 的博弈，参与者 $i$ 的 Shapley 值为：\n$$\\phi_i(v) = \\sum_{S \\subseteq N \\setminus \\{i\\}} \\frac{|S|! (|N| - |S| - 1)!}{|N|!} [v(S \\cup \\{i\\}) - v(S)]$$\n对于 $|N|=3$，组合权重在 $|S|=0$ 时为 $1/3$，在 $|S|=1$ 时为 $1/6$，在 $|S|=2$ 时为 $1/3$。\n\n-   **$\\phi_1$**:\n    $\\phi_1 = \\frac{1}{3}[v(\\{1\\}) - v(\\emptyset)] + \\frac{1}{6}[v(\\{1,2\\}) - v(\\{2\\})] + \\frac{1}{6}[v(\\{1,3\\}) - v(\\{3\\})] + \\frac{1}{3}[v(\\{1,2,3\\}) - v(\\{2,3\\})]$\n    $\\phi_1 = \\frac{1}{3}(1+\\rho) + \\frac{1}{6}(2 - (1+\\rho)) + \\frac{1}{6}((2+\\rho) - 1) + \\frac{1}{3}(3 - (2+\\rho))$\n    $\\phi_1 = \\frac{1+\\rho}{3} + \\frac{1-\\rho}{6} + \\frac{1+\\rho}{6} + \\frac{1-\\rho}{3} = \\frac{2(1+\\rho)+(1-\\rho)+(1+\\rho)+2(1-\\rho)}{6} = \\frac{6}{6} = 1$。\n\n-   **$\\phi_2$**: 问题设置相对于特征 1 和 2 是对称的（$w_1=w_2$，$x_1=x_2$，并且它们在协方差矩阵中的角色是可交换的）。得到的值函数 $v(S)$ 也是对称的。根据 Shapley 值的对称性公理，$\\phi_1 = \\phi_2$。因此，$\\phi_2 = 1$。\n\n-   **$\\phi_3$**:\n    $\\phi_3 = \\frac{1}{3}[v(\\{3\\}) - v(\\emptyset)] + \\frac{1}{6}[v(\\{1,3\\}) - v(\\{1\\})] + \\frac{1}{6}[v(\\{2,3\\}) - v(\\{2\\})] + \\frac{1}{3}[v(\\{1,2,3\\}) - v(\\{1,2\\})]$\n    $\\phi_3 = \\frac{1}{3}(1-0) + \\frac{1}{6}((2+\\rho) - (1+\\rho)) + \\frac{1}{6}((2+\\rho) - (1+\\rho)) + \\frac{1}{3}(3-2)$\n    $\\phi_3 = \\frac{1}{3}(1) + \\frac{1}{6}(1) + \\frac{1}{6}(1) + \\frac{1}{3}(1) = \\frac{1}{3} + \\frac{1}{3} + \\frac{1}{3} = 1$。\n\n标准 SHAP 值为 $(\\phi_1, \\phi_2, \\phi_3) = (1, 1, 1)$，与 $\\rho$ 无关。\n\n**3. 分组 SHAP 值 $\\phi_{G_1}$ 的计算**\n\n我们考虑一个有两个参与者的新博弈：$P_1 = G_1 = \\{1,2\\}$ 和 $P_2 = \\{3\\}$。此博弈的值函数 $v'$ 继承自 $v$：\n- $v'(\\emptyset) = v(\\emptyset) = 0$\n- $v'(\\{P_1\\}) = v(\\{1,2\\}) = 2$\n- $v'(\\{P_2\\}) = v(\\{3\\}) = 1$\n- $v'(\\{P_1, P_2\\}) = v(\\{1,2,3\\}) = 3$\n\n在这个双人博弈中，参与者 $P_1$ 的 Shapley 值为：\n$$\\phi_{G_1} = \\frac{1}{2}[v'(\\{P_1\\}) - v'(\\emptyset)] + \\frac{1}{2}[v'(\\{P_1,P_2\\}) - v'(\\{P_2\\})]$$\n$$\\phi_{G_1} = \\frac{1}{2}(2 - 0) + \\frac{1}{2}(3 - 1) = 1 + 1 = 2$$\n分组 Shapley 值 $\\phi_{G_1}$ 为 $2$，同样与 $\\rho$ 无关。\n\n**4. 诊断指标的计算**\n对于每个测试用例 $\\rho \\in \\{0.0, 0.9, 1.0\\}$，我们计算三个指标。\n\n1.  **可加性误差**：$|\\sum_{i \\in N} \\phi_i - (f(\\mathbf{x}) - \\mathbb{E}[f(\\mathbf{X})])|$。\n    SHAP 值的总和是 $\\phi_1+\\phi_2+\\phi_3 = 1+1+1 = 3$。需要解释的总效应是 $f(\\mathbf{x}) - v(\\emptyset) = 3 - 0 = 3$。Shapley 值的有效性公理保证了 $\\sum \\phi_i = v(N) - v(\\emptyset)$。\n    误差为 $|3 - 3| = 0$。\n\n2.  **组一致性误差**：$| \\phi_{G_1} - (\\phi_1 + \\phi_2) |$。\n    我们有 $\\phi_{G_1} = 2$ 且 $\\phi_1 + \\phi_2 = 1 + 1 = 2$。\n    误差为 $|2 - 2| = 0$。此值为零，因为该博弈是可加分离的。特征集 $\\{1,2\\}$ 与 $\\{3\\}$ 统计独立，且线性模型在这些集合之间没有交互项。这意味着对于 $S \\subseteq \\{1,2\\}$ 和 $T \\subseteq \\{3\\}$，有 $v(S \\cup T) = v(S) + v(T)$，从而导致了这种形式的组一致性。\n\n3.  **对称性偏差**：$| \\phi_1 - \\phi_2 |$。\n    如前所述，该问题相对于特征 1 和 2 是对称的。这使得它们的 Shapley 值必然相等。\n    误差为 $|1 - 1| = 0$。\n\n对于所有的 $\\rho$ 值，所有三个诊断指标均为 0。由于问题的结构特性（与特征 3 的独立性以及特征 1 和 2 之间的对称性），特征 1 和 2 之间的具体共线性水平不影响这些特定的诊断指标。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes SHAP-based diagnostics for a linear model with collinear features.\n    \n    The problem setup allows for an exact analytical solution, which is implemented\n    here. The diagnostic values are found to be zero in all cases due to the\n    structural properties of the problem:\n    1. Additivity Error is zero by the efficiency property of Shapley values.\n    2. Symmetry Deviation is zero due to the symmetric treatment of features 1 and 2\n       in the model, instance, and data distribution.\n    3. Group Consistency Error is zero because the game is additively separable,\n       a consequence of the feature set {1,2} being statistically independent of {3}.\n    \"\"\"\n    w = np.array([1.0, 1.0, 0.5])\n    x_inst = np.array([1.0, 1.0, 2.0])\n    # Bias b = 0 and background mean mu = (0,0,0) are constants in the problem.\n\n    test_rhos = [0.0, 0.9, 1.0]\n    \n    all_results = []\n    \n    for rho in test_rhos:\n        # 1. Define the value function v(S) based on analytical derivations.\n        #    v(S) = E[f(X) | X_S = x_S]\n        #    The keys are tuples of feature indices (1-based for clarity).\n        v = {}\n        v[()] = 0.0\n        v[(1,)] = (1.0 + rho) * x_inst[0]  \n        v[(2,)] = (1.0 + rho) * x_inst[1]  \n        v[(3,)] = w[2] * x_inst[2]          \n        v[(1, 2)] = w[0] * x_inst[0] + w[1] * x_inst[1]\n        v[(1, 3)] = w[0] * x_inst[0] + w[2] * x_inst[2] + w[1] * rho * x_inst[0]\n        v[(2, 3)] = w[1] * x_inst[1] + w[2] * x_inst[2] + w[0] * rho * x_inst[1]\n        v[(1, 2, 3)] = np.dot(w, x_inst)\n\n        # 2. Compute standard SHAP values for N=3 players.\n        #    The combinatorial weights are 1/3 for |S|=0, 1/6 for |S|=1, 1/3 for |S|=2.\n        \n        # phi_1 for feature 1\n        mc1_s0 = v[(1,)] - v[()]\n        mc1_s2 = v[(1, 2)] - v[(2,)]\n        mc1_s3 = v[(1, 3)] - v[(3,)]\n        mc1_s23 = v[(1, 2, 3)] - v[(2, 3)]\n        phi_1 = (1.0/3.0)*mc1_s0 + (1.0/6.0)*mc1_s2 + (1.0/6.0)*mc1_s3 + (1.0/3.0)*mc1_s23\n\n        # phi_2 for feature 2\n        mc2_s0 = v[(2,)] - v[()]\n        mc2_s1 = v[(1, 2)] - v[(1,)]\n        mc2_s3 = v[(2, 3)] - v[(3,)]\n        mc2_s13 = v[(1, 2, 3)] - v[(1, 3)]\n        phi_2 = (1.0/3.0)*mc2_s0 + (1.0/6.0)*mc2_s1 + (1.0/6.0)*mc2_s3 + (1.0/3.0)*mc2_s13\n        \n        # phi_3 for feature 3\n        mc3_s0 = v[(3,)] - v[()]\n        mc3_s1 = v[(1, 3)] - v[(1,)]\n        mc3_s2 = v[(2, 3)] - v[(2,)]\n        mc3_s12 = v[(1, 2, 3)] - v[(1, 2)]\n        phi_3 = (1.0/3.0)*mc3_s0 + (1.0/6.0)*mc3_s1 + (1.0/6.0)*mc3_s2 + (1.0/3.0)*mc3_s12\n\n        # 3. Compute grouped Shapley value phi_G1 for G1={1,2}.\n        #    This is a 2-player game with players P1=G1 and P2={3}.\n        v_prime = {\n            'empty': v[()],\n            'P1': v[(1, 2)],\n            'P2': v[(3,)],\n            'P12': v[(1, 2, 3)]\n        }\n        \n        phi_G1 = 0.5 * (v_prime['P1'] - v_prime['empty']) + 0.5 * (v_prime['P12'] - v_prime['P2'])\n        \n        # 4. Quantify diagnostics.\n        fx_minus_Ex = v[(1, 2, 3)] - v[()]\n        \n        additivity_error = abs((phi_1 + phi_2 + phi_3) - fx_minus_Ex)\n        group_consistency_error = abs(phi_G1 - (phi_1 + phi_2))\n        symmetry_deviation = abs(phi_1 - phi_2)\n        \n        case_results = [additivity_error, group_consistency_error, symmetry_deviation]\n        all_results.append(case_results)\n\n    # Format the final output string to match the problem specification exactly.\n    # The format is [[a_1,b_1,c_1],[a_2,b_2,c_2],[a_3,b_3,c_3]] with no spaces.\n    results_str = str(all_results).replace(\" \", \"\")\n    print(results_str)\n\nsolve()\n```", "id": "3132668"}, {"introduction": "一个解释只有在可靠时才有用。我们如何才能信任模型给出的特征重要性分数呢？这最后一个练习将向您介绍一种强大而通用的“健全性检查”（sanity check）方法：随机化测试。通过在一个标签被完全随机化的数据集（其中不存在任何真实模式）上训练模型，您将学习如何验证一个解释方法是否能正确地报告几乎为零的重要性。这项实践将帮助您建立一项验证和调试可解释性工具的关键技能，确保您的发现是基于真实信号，而非模型或方法的巧合 [@problem_id:3132651]。", "problem": "您的任务是在统计学习可解释性的框架内，为随机化标签下的解释重要性设计一个程序化的健全性检查。设定为具有标准正交设计的线性回归和模型无关的重要性定义。\n\n考虑每个测试用例的以下设置：\n- 数据：对于整数 $n \\geq d \\geq 2$，构造一个设计矩阵 $X \\in \\mathbb{R}^{n \\times d}$，其列是标准正交的，并经过缩放以满足 $X^\\top X = n I_d$。同时，生成一个标签向量 $y \\in \\mathbb{R}^n$，其元素是独立的标准正态分布。\n- 模型：使用普通最小二乘法 (OLS) 拟合模型，利用 $X$ 的标准正交性推导出的闭式解公式，以获得系数 $\\hat{w} \\in \\mathbb{R}^d$。\n- 解释重要性：将特征 $j$ 的原始重要性定义为其系数的平方 $\\hat{w}_j^2$，并将归一化的解释重要性分布 $p \\in \\mathbb{R}^d$ 定义为 $p_j = \\hat{w}_j^2 / \\sum_{k=1}^d \\hat{w}_k^2$，因此 $\\sum_{j=1}^d p_j = 1$ 且 $p_j \\geq 0$。\n- 检验统计量：令 $u \\in \\mathbb{R}^d$ 为 $d$ 个坐标上的均匀分布，因此 $u_j = 1/d$。将与均匀分布的平方欧几里得偏差定义为\n$$\nT = \\|p - u\\|_2^2 = \\sum_{j=1}^d \\left(p_j - \\frac{1}{d}\\right)^2.\n$$\n\n在随机化标签 $y$ 和标准正交设计 $X$ 的条件下，根据第一性原理可以推导出 $p$ 的一个有原则的期望基线。您的任务是：\n1. 通过对高斯随机矩阵进行标准正交化并缩放以满足 $X^\\top X = n I_d$ 的方式，确定性地（给定一个种子）生成 $X$。\n2. 确定性地（给定一个种子）生成 $y$，其元素是独立的标准正态分布。\n3. 计算 OLS 估计值 $\\hat{w}$，然后计算归一化的重要性分布 $p$ 和统计量 $T$。\n4. 在随机化标签的原假设下，使用关于 $p$ 的正确基线分布论证，推导出期望值 $\\mathbb{E}[T]$ 和方差 $\\mathrm{Var}(T)$（仅用 $d$ 表示），并计算标准化分数\n$$\nz = \\frac{T - \\mathbb{E}[T]}{\\sqrt{\\mathrm{Var}(T)}}.\n$$\n5. 通过测试 $\\lvert z \\rvert \\leq \\tau$（阈值 $\\tau = 4.0$）来判断观测到的统计量 $T$ 是否与原假设基线一致。为每个测试用例输出布尔决策。\n\n测试套件：\n- 案例 1：$n = 200$, $d = 5$, 随机种子 $s = 0$。\n- 案例 2：$n = 50$, $d = 2$, 随机种子 $s = 1$。\n- 案例 3：$n = 80$, $d = 10$, 随机种子 $s = 2$。\n- 案例 4：$n = 15$, $d = 15$, 随机种子 $s = 3$。\n\n所有随机性必须是可复现的，并完全由给定的种子决定。此问题中不涉及物理单位。\n\n最终输出格式：\n您的程序应生成一行输出，其中包含四个测试用例的布尔结果，格式为逗号分隔的列表，并用方括号括起来（例如，\"[True,False,True,True]\"）。", "solution": "该问题要求我们对从普通最小二乘法 (OLS) 模型中得出的特征重要性分数进行健全性检查。该检查基于一个原假设，即标签是随机噪声，意味着特征与目标之间没有真实关系。在此原假设下，我们预期特征重要性会遵循一个特定的统计分布。任务是推导一个检验统计量的期望值和方差，该统计量用于衡量观测到的重要性与均匀分布的偏差，然后利用这些矩来标准化观测到的统计量，并检验其是否落在一个合理的范围内。\n\n解决方案分为两个主要部分：首先，对检验统计量的矩进行理论推导；其次，为给定的测试用例实现数值计算过程。\n\n### 理论推导\n\n1.  **模型与估计量分布**\n\n    我们给定一个设计矩阵 $X \\in \\mathbb{R}^{n \\times d}$，其列经过缩放以满足 $X^\\top X = n I_d$，其中 $I_d$ 是 $d \\times d$ 的单位矩阵。标签向量 $y \\in \\mathbb{R}^n$ 由独立同分布 (i.i.d.) 的标准正态随机变量组成，因此 $y \\sim \\mathcal{N}(0, I_n)$。\n\n    系数 $w$ 的 OLS 估计量由 $\\hat{w} = (X^\\top X)^{-1} X^\\top y$ 给出。代入 $X$ 的给定属性，我们得到：\n    $$\n    \\hat{w} = (n I_d)^{-1} X^\\top y = \\frac{1}{n} X^\\top y\n    $$\n    由于 $\\hat{w}$ 是高斯向量 $y$ 的线性变换，它也是一个多元高斯分布。其均值为：\n    $$\n    \\mathbb{E}[\\hat{w}] = \\frac{1}{n} X^\\top \\mathbb{E}[y] = \\frac{1}{n} X^\\top 0 = 0\n    $$\n    其协方差矩阵为：\n    $$\n    \\mathrm{Cov}(\\hat{w}) = \\mathbb{E}[\\hat{w}\\hat{w}^\\top] = \\mathbb{E}\\left[\\left(\\frac{1}{n} X^\\top y\\right)\\left(\\frac{1}{n} X^\\top y\\right)^\\top\\right] = \\frac{1}{n^2} X^\\top \\mathbb{E}[yy^\\top] X = \\frac{1}{n^2} X^\\top \\mathrm{Cov}(y) X\n    $$\n    由于 $\\mathrm{Cov}(y) = I_n$，这可以简化为：\n    $$\n    \\mathrm{Cov}(\\hat{w}) = \\frac{1}{n^2} X^\\top I_n X = \\frac{1}{n^2} (X^\\top X) = \\frac{1}{n^2} (n I_d) = \\frac{1}{n} I_d\n    $$\n    因此，OLS 系数服从分布 $\\hat{w} \\sim \\mathcal{N}(0, \\frac{1}{n} I_d)$。这意味着对于 $j=1, \\dots, d$，各个系数 $\\hat{w}_j$ 是独立同分布的，且 $\\hat{w}_j \\sim \\mathcal{N}(0, 1/n)$。\n\n2.  **归一化重要性的分布**\n\n    特征 $j$ 的原始重要性定义为 $\\hat{w}_j^2$。让我们考虑缩放后的变量 $v_j = n \\hat{w}_j^2$。由于 $\\sqrt{n}\\hat{w}_j \\sim \\mathcal{N}(0, 1)$，每个 $v_j = (\\sqrt{n}\\hat{w}_j)^2$ 都服从自由度为1的卡方分布，即 $v_j \\sim \\chi^2(1)$。因为 $\\hat{w}_j$ 是独立的，所以 $v_j$ 也是独立同分布的。\n\n    归一化的重要性分布 $p$ 由其分量 $p_j$ 定义：\n    $$\n    p_j = \\frac{\\hat{w}_j^2}{\\sum_{k=1}^d \\hat{w}_k^2} = \\frac{n \\hat{w}_j^2}{\\sum_{k=1}^d n \\hat{w}_k^2} = \\frac{v_j}{\\sum_{k=1}^d v_k}\n    $$\n    由一组独立同分布的伽马分布变量归一化后形成的向量服从狄利克雷分布。一个 $\\chi^2(k)$ 分布是形状参数为 $k/2$、尺度参数为 $2$ 的伽马分布。在这里，$v_j \\sim \\chi^2(1)$，这是一个形状参数 $\\alpha = 1/2$ 的伽马分布。因此，向量 $p = (p_1, \\dots, p_d)$ 服从参数为 $(\\alpha_1, \\dots, \\alpha_d)$ 的狄利克雷分布，其中所有 $\\alpha_j = 1/2$。\n    $$\n    p \\sim \\mathrm{Dirichlet}(1/2, 1/2, \\dots, 1/2)\n    $$\n\n3.  **检验统计量的矩**\n\n    检验统计量是 $T = \\|p - u\\|_2^2 = \\sum_{j=1}^d (p_j - 1/d)^2$。为了计算其均值和方差，我们首先需要找到狄利克雷分布的矩。令 $p \\sim \\mathrm{Dirichlet}(\\alpha_1, \\dots, \\alpha_d)$ 并且令 $\\alpha_0 = \\sum_k \\alpha_k$。原始矩由以下公式给出：\n    $$\n    \\mathbb{E}\\left[\\prod_{i=1}^d p_i^{k_i}\\right] = \\frac{\\Gamma(\\alpha_0)}{\\Gamma(\\alpha_0 + \\sum k_i)} \\prod_{i=1}^d \\frac{\\Gamma(\\alpha_i + k_i)}{\\Gamma(\\alpha_i)}\n    $$\n    在我们的情况下，对所有 $j$ 都有 $\\alpha_j = 1/2$，所以 $\\alpha_0 = d/2$。\n\n    我们可以简化 $T$ 的表达式：\n    $$\n    T = \\sum_{j=1}^d \\left(p_j^2 - \\frac{2p_j}{d} + \\frac{1}{d^2}\\right) = \\sum_{j=1}^d p_j^2 - \\frac{2}{d}\\sum_{j=1}^d p_j + \\sum_{j=1}^d \\frac{1}{d^2}\n    $$\n    由于 $\\sum_j p_j = 1$，这变为：\n    $$\n    T = \\sum_{j=1}^d p_j^2 - \\frac{2}{d} + \\frac{d}{d^2} = \\sum_{j=1}^d p_j^2 - \\frac{1}{d}\n    $$\n\n4.  **T 的期望值**\n\n    期望值为 $\\mathbb{E}[T] = \\mathbb{E}\\left[\\sum_j p_j^2 - 1/d\\right] = \\sum_j \\mathbb{E}[p_j^2] - 1/d$。\n    根据对称性，$\\mathbb{E}[p_j^2]$ 对所有 $j$ 都是相同的。让我们计算 $\\mathbb{E}[p_1^2]$：\n    $$\n    \\mathbb{E}[p_1^2] = \\frac{\\Gamma(d/2)}{\\Gamma(d/2+2)} \\frac{\\Gamma(1/2+2)}{\\Gamma(1/2)} = \\frac{\\Gamma(d/2)}{(d/2+1)(d/2)\\Gamma(d/2)} \\frac{(3/2)(1/2)\\Gamma(1/2)}{\\Gamma(1/2)} = \\frac{1}{(d+2)/2 \\cdot d/2} \\cdot \\frac{3}{4} = \\frac{3}{d(d+2)}\n    $$\n    因此，$\\mathbb{E}[T] = d \\cdot \\mathbb{E}[p_1^2] - 1/d = d \\cdot \\frac{3}{d(d+2)} - \\frac{1}{d} = \\frac{3}{d+2} - \\frac{1}{d} = \\frac{3d - (d+2)}{d(d+2)}$，得出：\n    $$\n    \\mathbb{E}[T] = \\frac{2(d-1)}{d(d+2)}\n    $$\n\n5.  **T 的方差**\n\n    方差为 $\\mathrm{Var}(T) = \\mathrm{Var}\\left(\\sum_j p_j^2 - 1/d\\right) = \\mathrm{Var}\\left(\\sum_j p_j^2\\right)$。\n    $$\n    \\mathrm{Var}\\left(\\sum_j p_j^2\\right) = \\mathbb{E}\\left[\\left(\\sum_j p_j^2\\right)^2\\right] - \\left(\\mathbb{E}\\left[\\sum_j p_j^2\\right]\\right)^2\n    $$\n    我们已有 $\\mathbb{E}\\left[\\sum_j p_j^2\\right] = d \\cdot \\mathbb{E}[p_1^2] = \\frac{3}{d+2}$。\n    平方和可以展开为：\n    $$\n    \\mathbb{E}\\left[\\left(\\sum_j p_j^2\\right)^2\\right] = \\mathbb{E}\\left[\\sum_j p_j^4 + \\sum_{i \\neq j} p_i^2 p_j^2\\right] = d \\cdot \\mathbb{E}[p_1^4] + d(d-1) \\cdot \\mathbb{E}[p_1^2 p_2^2]\n    $$\n    我们需要四阶矩：\n    $$\n    \\mathbb{E}[p_1^4] = \\frac{\\Gamma(d/2)}{\\Gamma(d/2+4)} \\frac{\\Gamma(1/2+4)}{\\Gamma(1/2)} = \\frac{105}{d(d+2)(d+4)(d+6)}\n    $$\n    $$\n    \\mathbb{E}[p_1^2 p_2^2] = \\frac{\\Gamma(d/2)}{\\Gamma(d/2+4)} \\frac{\\Gamma(1/2+2)}{\\Gamma(1/2)} \\frac{\\Gamma(1/2+2)}{\\Gamma(1/2)} = \\frac{9}{d(d+2)(d+4)(d+6)}\n    $$\n    将这些代入 $\\mathbb{E}[(\\sum_j p_j^2)^2]$ 的表达式中：\n    $$\n    \\mathbb{E}\\left[\\left(\\sum_j p_j^2\\right)^2\\right] = d \\cdot \\frac{105}{d(d+2)(d+4)(d+6)} + d(d-1) \\frac{9}{d(d+2)(d+4)(d+6)} = \\frac{105 + 9(d-1)}{(d+2)(d+4)(d+6)} = \\frac{9d+96}{(d+2)(d+4)(d+6)}\n    $$\n    现在，我们计算方差：\n    $$\n    \\mathrm{Var}(T) = \\frac{9d+96}{(d+2)(d+4)(d+6)} - \\left(\\frac{3}{d+2}\\right)^2 = \\frac{(9d+96)(d+2) - 9(d+4)(d+6)}{(d+2)^2(d+4)(d+6)}\n    $$\n    分子是 $ (9d^2+18d+96d+192) - 9(d^2+10d+24) = (9d^2+114d+192) - (9d^2+90d+216) = 24d - 24 = 24(d-1) $。\n    因此，方差为：\n    $$\n    \\mathrm{Var}(T) = \\frac{24(d-1)}{(d+2)^2(d+4)(d+6)}\n    $$\n\n### 算法步骤\n\n对于每个具有参数 $(n, d, s)$ 的测试用例：\n1.  用种子 $s$ 设置一个随机数生成器。\n2.  生成一个 $n \\times d$ 的矩阵 $A$，其元素是独立同分布的 $\\mathcal{N}(0,1)$。\n3.  对 $A$ 进行 QR 分解，得到一个具有标准正交列的 $n \\times d$ 矩阵 $Q$。构造 $X = \\sqrt{n} Q$。这满足 $X^\\top X = n I_d$。\n4.  生成一个 $n$ 维向量 $y$，其元素是独立同分布的 $\\mathcal{N}(0,1)$。\n5.  计算 OLS 系数：$\\hat{w} = \\frac{1}{n} X^\\top y$。\n6.  计算归一化的重要性分布：$p_j = \\hat{w}_j^2 / \\sum_k \\hat{w}_k^2$。\n7.  计算检验统计量：$T = \\sum_j (p_j - 1/d)^2$。\n8.  使用推导出的公式和给定的 $d$ 计算理论均值 $\\mathbb{E}[T]$ 和方差 $\\mathrm{Var}(T)$。\n9.  计算标准化分数：$z = (T - \\mathbb{E}[T]) / \\sqrt{\\mathrm{Var}(T)}$。\n10. 通过检查是否 $|z| \\leq 4.0$ 来确定结果是否与原假设基线一致。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem of conducting a sanity check for explanation importances\n    under randomized labels.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (200, 5, 0),  # Case 1\n        (50, 2, 1),   # Case 2\n        (80, 10, 2),  # Case 3\n        (15, 15, 3),  # Case 4\n    ]\n\n    results = []\n    for n, d, seed in test_cases:\n        # Step 1  2: Generate data deterministically based on the seed\n        rng = np.random.default_rng(seed)\n\n        # Generate a Gaussian random matrix\n        A = rng.standard_normal(size=(n, d))\n\n        # Orthonormalize A and scale to get X such that X^T X = n * I_d\n        # np.linalg.qr gives Q with Q^T Q = I_d\n        Q, _ = np.linalg.qr(A)\n        X = np.sqrt(n) * Q\n\n        # Generate random labels y\n        y = rng.standard_normal(size=n)\n\n        # Step 3: Compute OLS estimate, importances, and test statistic\n        \n        # OLS estimate w_hat = (X^T X)^-1 X^T y = (n I_d)^-1 X^T y = (1/n) X^T y\n        w_hat = (1.0 / n) * X.T @ y\n\n        # Raw importances are the squared coefficients\n        raw_importances = w_hat**2\n\n        # Normalized importance distribution p\n        sum_raw_importances = np.sum(raw_importances)\n        if sum_raw_importances == 0:\n            # This case is theoretically possible but has measure zero.\n            # If it happens, importances are undefined; we can treat T as 0 or\n            # use a uniform distribution if all w_hat are 0.\n            # A uniform distribution for p gives T=0.\n            p = np.full(d, 1.0 / d)\n        else:\n            p = raw_importances / sum_raw_importances\n            \n        # Uniform distribution u\n        u = np.full(d, 1.0 / d)\n\n        # Test statistic T = ||p - u||_2^2\n        T = np.sum((p - u)**2)\n\n        # Step 4: Compute theoretical moments and standardized score\n        \n        # Cast d to float to ensure floating point arithmetic\n        d_f = float(d)\n        \n        # Expected value of T: E[T] = 2(d-1) / (d(d+2))\n        E_T = 2.0 * (d_f - 1.0) / (d_f * (d_f + 2.0))\n        \n        # Variance of T: Var(T) = 24(d-1) / ((d+2)^2 * (d+4) * (d+6))\n        # This is valid for d >= 2 as per problem constraints.\n        var_numerator = 24.0 * (d_f - 1.0)\n        var_denominator = (d_f + 2.0)**2 * (d_f + 4.0) * (d_f + 6.0)\n        Var_T = var_numerator / var_denominator\n        \n        std_dev_T = np.sqrt(Var_T)\n\n        # Standardized score z\n        if std_dev_T > 0:\n            z = (T - E_T) / std_dev_T\n        else:\n            # This case shouldn't happen for d>=2, but as a safeguard.\n            z = 0.0 if np.isclose(T, E_T) else np.inf\n\n        # Step 5: Decision based on the z-score\n        tau = 4.0\n        is_consistent = np.abs(z) = tau\n        results.append(is_consistent)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, [r.capitalize() for r in results]))}]\")\n\nsolve()\n```", "id": "3132651"}]}