{"hands_on_practices": [{"introduction": "得到一个点估计值 $\\hat{\\tau}$ 只是故事的一半；我们还需要了解其不确定性才能进行统计推断（如构建置信区间和进行假设检验）。这项练习 [@problem_id:3115377] 深入探讨了双重差分（DiD）估计量方差的“底层”机制。它将向你展示，关于误差结构（特别是在地理单元或时间周期内的相关性，即“聚类”）的假设，如何直接影响我们估计的精确度。对于任何严谨的实证研究者来说，这都是一项基本功。", "problem": "考虑一个平衡面板，其中地点由 $i \\in \\{1,\\dots,N\\}$ 索引，时间由 $t \\in \\{1,\\dots,T\\}$ 索引。一项政策在时间 $t^{\\ast}$ 引入，并应用于一部分地点，称为处理组 $\\mathcal{G}$，其基数为 $N_{G}$；而其余地点构成控制组 $\\mathcal{C}$，其基数为 $N_{C}$，因此 $N_{G} + N_{C} = N$。定义政策前时期为 $\\mathcal{P} = \\{t : t  t^{\\ast}\\}$，其长度为 $T_{0}$；政策后时期为 $\\mathcal{S} = \\{t : t \\ge t^{\\ast}\\}$，其长度为 $T_{1}$，且 $T_{0} + T_{1} = T$。结果变量 $y_{it}$ 服从以下加法模型\n$$\ny_{it} = \\alpha_{i} + \\gamma_{t} + \\tau D_{it} + u_{it},\n$$\n其中，如果 $i \\in \\mathcal{G}$ 且 $t \\in \\mathcal{S}$，则 $D_{it} = 1$，否则 $D_{it} = 0$。待估参数 $\\tau$ 是平均处理效应。定义双重差分（Difference-in-Differences, DiD）估计量\n$$\n\\hat{\\tau} = \\big(\\bar{y}_{\\mathcal{G},\\mathcal{S}} - \\bar{y}_{\\mathcal{G},\\mathcal{P}}\\big) - \\big(\\bar{y}_{\\mathcal{C},\\mathcal{S}} - \\bar{y}_{\\mathcal{C},\\mathcal{P}}\\big),\n$$\n其中 $\\bar{y}_{\\mathcal{G},\\mathcal{S}}$ 表示 $y_{it}$ 在 $i \\in \\mathcal{G}$ 和 $t \\in \\mathcal{S}$ 上的平均值，$\\bar{y}_{\\mathcal{G},\\mathcal{P}}$、$\\bar{y}_{\\mathcal{C},\\mathcal{S}}$ 和 $\\bar{y}_{\\mathcal{C},\\mathcal{P}}$ 的定义与此类似。\n\n假设 $u_{it}$ 的均值为零，并具有以下由常数 $\\sigma^{2} > 0$、$\\rho_{L} \\in [-1,1]$ 和 $\\rho_{T} \\in [-1,1]$ 定义的协方差结构：\n- 对所有 $i,t$，$\\mathrm{Var}(u_{it}) = \\sigma^{2}$，\n- 对所有 $i$ 和所有 $t \\neq s$，$\\mathrm{Cov}(u_{it}, u_{is}) = \\rho_{L}\\sigma^{2}$（地点层面的序列相关），\n- 对所有 $t$ 和所有 $i \\neq j$，$\\mathrm{Cov}(u_{it}, u_{jt}) = \\rho_{T}\\sigma^{2}$（时间层面的截面相关），\n- 当 $i \\neq j$ 且 $t \\neq s$ 时，$\\mathrm{Cov}(u_{it}, u_{js}) = 0$。\n\n从涉及随机变量线性组合的第一性原理和上述定义出发（不要调用任何现成的聚类稳健公式），推导出在以下条件下 $\\mathrm{Var}(\\hat{\\tau})$ 的闭式表达式：\n1. 仅按地点聚类（设 $\\rho_{T} = 0$），\n2. 仅按时间聚类（设 $\\rho_{L} = 0$），\n3. 按地点和时间双向聚类（允许 $\\rho_{L}$ 和 $\\rho_{T}$ 均存在）。\n\n用 $N_{G}$、$N_{C}$、$T_{0}$、$T_{1}$、$\\sigma^{2}$、$\\rho_{L}$ 和 $\\rho_{T}$ 表示你的最终结果。将三个方差按（仅按地点、仅按时间、双向）的顺序以单行矩阵的形式给出。无需四舍五入。", "solution": "该问题要求在给定的误差项 $u_{it}$ 协方差结构下，求出双重差分（DiD）估计量 $\\hat{\\tau}$ 的方差。该问题内部逻辑一致，具有坚实的统计学理论基础，且提法明确。我们可以着手求解。\n\n首先，我们将 DiD 估计量 $\\hat{\\tau}$ 用模型的各组成部分表示。该估计量定义为：\n$$\n\\hat{\\tau} = \\big(\\bar{y}_{\\mathcal{G},\\mathcal{S}} - \\bar{y}_{\\mathcal{G},\\mathcal{P}}\\big) - \\big(\\bar{y}_{\\mathcal{C},\\mathcal{S}} - \\bar{y}_{\\mathcal{C},\\mathcal{P}}\\big)\n$$\n将模型 $y_{it} = \\alpha_{i} + \\gamma_{t} + \\tau D_{it} + u_{it}$ 代入四个平均值 $\\bar{y}_{\\cdot,\\cdot}$ 的定义中，我们观察到固定效应 $\\alpha_i$ 和 $\\gamma_t$ 以及真实的处理效应参数 $\\tau$ 都被差分掉了。令 $\\bar{u}_{\\mathcal{G},\\mathcal{S}}$ 为 $u_{it}$ 在 $i \\in \\mathcal{G}$ 和 $t \\in \\mathcal{S}$ 上的平均值，其他项也类似定义。该估计量可以写成：\n$$\n\\hat{\\tau} = \\tau + \\big(\\bar{u}_{\\mathcal{G},\\mathcal{S}} - \\bar{u}_{\\mathcal{G},\\mathcal{P}}\\big) - \\big(\\bar{u}_{\\mathcal{C},\\mathcal{S}} - \\bar{u}_{\\mathcal{C},\\mathcal{P}}\\big)\n$$\n由于对所有 $i,t$ 都有 $\\mathrm{E}[u_{it}]=0$，因此可得 $\\mathrm{E}[\\hat{\\tau}] = \\tau$。该估计量的方差由误差项平均值的组合的方差决定：\n$$\n\\mathrm{Var}(\\hat{\\tau}) = \\mathrm{Var}\\left( \\big(\\bar{u}_{\\mathcal{G},\\mathcal{S}} - \\bar{u}_{\\mathcal{G},\\mathcal{P}}\\big) - \\big(\\bar{u}_{\\mathcal{C},\\mathcal{S}} - \\bar{u}_{\\mathcal{C},\\mathcal{P}}\\big) \\right)\n$$\n我们可以将其表示为误差项 $u_{it}$ 的单个线性组合的方差。令 $W = (\\bar{u}_{\\mathcal{G},\\mathcal{S}} - \\bar{u}_{\\mathcal{G},\\mathcal{P}}) - (\\bar{u}_{\\mathcal{C},\\mathcal{S}} - \\bar{u}_{\\mathcal{C},\\mathcal{P}})$。我们可以将 $W$ 写成 $W = \\sum_{i=1}^{N} \\sum_{t=1}^{T} w_{it} u_{it}$，其中权重 $w_{it}$ 定义如下：\n\\begin{itemize}\n    \\item 对于 $i \\in \\mathcal{G}$ 和 $t \\in \\mathcal{S}$（处理组，政策后时期）：$w_{it} = \\frac{1}{N_G T_1}$\n    \\item 对于 $i \\in \\mathcal{G}$ 和 $t \\in \\mathcal{P}$（处理组，政策前时期）：$w_{it} = -\\frac{1}{N_G T_0}$\n    \\item 对于 $i \\in \\mathcal{C}$ 和 $t \\in \\mathcal{S}$（控制组，政策后时期）：$w_{it} = -\\frac{1}{N_C T_1}$\n    \\item 对于 $i \\in \\mathcal{C}$ 和 $t \\in \\mathcal{P}$（控制组，政策前时期）：$w_{it} = \\frac{1}{N_C T_0}$\n\\end{itemize}\n该线性组合的方差由下式给出：\n$$\n\\mathrm{Var}(\\hat{\\tau}) = \\mathrm{Var}(W) = \\mathrm{Var}\\left(\\sum_{i,t} w_{it} u_{it}\\right) = \\sum_{i,t} \\sum_{j,s} w_{it} w_{js} \\mathrm{Cov}(u_{it}, u_{js})\n$$\n我们使用问题中提供的协方差结构：\n\\begin{itemize}\n    \\item 如果 $i=j$ 且 $t=s$，则 $\\mathrm{Cov}(u_{it}, u_{js}) = \\sigma^{2}$。\n    \\item 如果 $i=j$ 且 $t \\neq s$，则 $\\mathrm{Cov}(u_{it}, u_{js}) = \\rho_{L}\\sigma^{2}$。\n    \\item 如果 $i \\neq j$ 且 $t=s$，则 $\\mathrm{Cov}(u_{it}, u_{js}) = \\rho_{T}\\sigma^{2}$。\n    \\item 如果 $i \\neq j$ 且 $t \\neq s$，则 $\\mathrm{Cov}(u_{it}, u_{js}) = 0$。\n\\end{itemize}\n我们可以根据这些情况将双重求和分成三个部分：\n$$\n\\mathrm{Var}(\\hat{\\tau}) = \\sigma^2 \\sum_{i,t} w_{it}^2 + \\rho_L \\sigma^2 \\sum_{i} \\sum_{t \\neq s} w_{it} w_{is} + \\rho_T \\sigma^2 \\sum_{t} \\sum_{i \\neq j} w_{it} w_{jt}\n$$\n让我们逐项计算。\n\n第 1 项：权重平方和。\n$$\n\\sum_{i,t} w_{it}^2 = \\sum_{i \\in \\mathcal{G}, t \\in \\mathcal{S}} \\left(\\frac{1}{N_G T_1}\\right)^2 + \\sum_{i \\in \\mathcal{G}, t \\in \\mathcal{P}} \\left(\\frac{-1}{N_G T_0}\\right)^2 + \\sum_{i \\in \\mathcal{C}, t \\in \\mathcal{S}} \\left(\\frac{-1}{N_C T_1}\\right)^2 + \\sum_{i \\in \\mathcal{C}, t \\in \\mathcal{P}} \\left(\\frac{1}{N_C T_0}\\right)^2\n$$\n这些求和中的项数分别为 $N_G T_1$、$N_G T_0$、$N_C T_1$ 和 $N_C T_0$。\n$$\n\\sum_{i,t} w_{it}^2 = \\frac{N_G T_1}{N_G^2 T_1^2} + \\frac{N_G T_0}{N_G^2 T_0^2} + \\frac{N_C T_1}{N_C^2 T_1^2} + \\frac{N_C T_0}{N_C^2 T_0^2} = \\frac{1}{N_G T_1} + \\frac{1}{N_G T_0} + \\frac{1}{N_C T_1} + \\frac{1}{N_C T_0}\n$$\n这可以因式分解为：\n$$\n\\sum_{i,t} w_{it}^2 = \\frac{1}{N_G}\\left(\\frac{1}{T_1} + \\frac{1}{T_0}\\right) + \\frac{1}{N_C}\\left(\\frac{1}{T_1} + \\frac{1}{T_0}\\right) = \\left(\\frac{1}{N_G} + \\frac{1}{N_C}\\right)\\left(\\frac{1}{T_0} + \\frac{1}{T_1}\\right)\n$$\n方差的这一部分贡献了 $\\sigma^2 \\left(\\frac{1}{N_G} + \\frac{1}{N_C}\\right)\\left(\\frac{1}{T_0} + \\frac{1}{T_1}\\right)$。\n\n第 2 项：地点层面的相关性。求和为 $\\sum_{i} \\sum_{t \\neq s} w_{it} w_{is}$。我们使用恒等式 $\\sum_{t \\neq s} a_t a_s = (\\sum_t a_t)^2 - \\sum_t a_t^2$。\n对于任何地点 $i \\in \\mathcal{G}$：\n$$ \\sum_{t} w_{it} = \\sum_{t \\in \\mathcal{S}} \\frac{1}{N_G T_1} + \\sum_{t \\in \\mathcal{P}} \\frac{-1}{N_G T_0} = T_1 \\cdot \\frac{1}{N_G T_1} - T_0 \\cdot \\frac{1}{N_G T_0} = \\frac{1}{N_G} - \\frac{1}{N_G} = 0 $$\n类似地，对于任何地点 $i \\in \\mathcal{C}$：\n$$ \\sum_{t} w_{it} = \\sum_{t \\in \\mathcal{S}} \\frac{-1}{N_C T_1} + \\sum_{t \\in \\mathcal{P}} \\frac{1}{N_C T_0} = -T_1 \\cdot \\frac{1}{N_C T_1} + T_0 \\cdot \\frac{1}{N_C T_0} = -\\frac{1}{N_C} + \\frac{1}{N_C} = 0 $$\n因此，对于所有 $i$，$\\sum_t w_{it} = 0$。所以，$\\sum_{t \\neq s} w_{it} w_{is} = 0 - \\sum_t w_{it}^2$。\n$$\n\\sum_{i} \\sum_{t \\neq s} w_{it} w_{is} = \\sum_{i} \\left( -\\sum_{t} w_{it}^2 \\right) = - \\sum_{i,t} w_{it}^2 = -\\left(\\frac{1}{N_G} + \\frac{1}{N_C}\\right)\\left(\\frac{1}{T_0} + \\frac{1}{T_1}\\right)\n$$\n这一部分贡献了 $-\\rho_L \\sigma^2 \\left(\\frac{1}{N_G} + \\frac{1}{N_C}\\right)\\left(\\frac{1}{T_0} + \\frac{1}{T_1}\\right)$。\n\n第 3 项：时间层面的相关性。求和为 $\\sum_{t} \\sum_{i \\neq j} w_{it} w_{jt}$。我们使用恒等式 $\\sum_{i \\neq j} a_i a_j = (\\sum_i a_i)^2 - \\sum_i a_i^2$。\n对于任何时间 $t \\in \\mathcal{S}$：\n$$ \\sum_{i} w_{it} = \\sum_{i \\in \\mathcal{G}} \\frac{1}{N_G T_1} + \\sum_{i \\in \\mathcal{C}} \\frac{-1}{N_C T_1} = N_G \\cdot \\frac{1}{N_G T_1} - N_C \\cdot \\frac{1}{N_C T_1} = \\frac{1}{T_1} - \\frac{1}{T_1} = 0 $$\n类似地，对于任何时间 $t \\in \\mathcal{P}$：\n$$ \\sum_{i} w_{it} = \\sum_{i \\in \\mathcal{G}} \\frac{-1}{N_G T_0} + \\sum_{i \\in \\mathcal{C}} \\frac{1}{N_C T_0} = -N_G \\cdot \\frac{1}{N_G T_0} + N_C \\cdot \\frac{1}{N_C T_0} = -\\frac{1}{T_0} + \\frac{1}{T_0} = 0 $$\n因此，对于所有 $t$，$\\sum_i w_{it} = 0$。所以，$\\sum_{i \\neq j} w_{it} w_{jt} = 0 - \\sum_i w_{it}^2$。\n$$\n\\sum_{t} \\sum_{i \\neq j} w_{it} w_{jt} = \\sum_{t} \\left( -\\sum_{i} w_{it}^2 \\right) = - \\sum_{i,t} w_{it}^2 = -\\left(\\frac{1}{N_G} + \\frac{1}{N_C}\\right)\\left(\\frac{1}{T_0} + \\frac{1}{T_1}\\right)\n$$\n这一部分贡献了 $-\\rho_T \\sigma^2 \\left(\\frac{1}{N_G} + \\frac{1}{N_C}\\right)\\left(\\frac{1}{T_0} + \\frac{1}{T_1}\\right)$。\n\n将所有三项的贡献加起来，我们得到 $\\hat{\\tau}$ 方差的一般表达式：\n$$\n\\mathrm{Var}(\\hat{\\tau}) = \\sigma^2(1 - \\rho_L - \\rho_T) \\left(\\frac{1}{N_G} + \\frac{1}{N_C}\\right) \\left(\\frac{1}{T_0} + \\frac{1}{T_1}\\right)\n$$\n这是双向聚类的一般表达式。\n\n现在我们可以求出所要求的三种特定情况的解。\n\n1.  **仅按地点聚类**：我们设 $\\rho_T = 0$。\n    $$\n    \\mathrm{Var}(\\hat{\\tau})_{\\text{loc}} = \\sigma^2(1 - \\rho_L) \\left(\\frac{1}{N_G} + \\frac{1}{N_C}\\right) \\left(\\frac{1}{T_0} + \\frac{1}{T_1}\\right)\n    $$\n\n2.  **仅按时间聚类**：我们设 $\\rho_L = 0$。\n    $$\n    \\mathrm{Var}(\\hat{\\tau})_{\\text{time}} = \\sigma^2(1 - \\rho_T) \\left(\\frac{1}{N_G} + \\frac{1}{N_C}\\right) \\left(\\frac{1}{T_0} + \\frac{1}{T_1}\\right)\n    $$\n\n3.  **双向聚类**：这是上面推导的一般公式。\n    $$\n    \\mathrm{Var}(\\hat{\\tau})_{\\text{two-way}} = \\sigma^2(1 - \\rho_L - \\rho_T) \\left(\\frac{1}{N_G} + \\frac{1}{N_C}\\right) \\left(\\frac{1}{T_0} + \\frac{1}{T_1}\\right)\n    $$\n\n我们按照要求将这三个结果呈现在一个单行矩阵中。", "answer": "$$\n\\boxed{\\begin{pmatrix} \\sigma^2(1 - \\rho_L) \\left(\\frac{1}{N_G} + \\frac{1}{N_C}\\right) \\left(\\frac{1}{T_0} + \\frac{1}{T_1}\\right)  \\sigma^2(1 - \\rho_T) \\left(\\frac{1}{N_G} + \\frac{1}{N_C}\\right) \\left(\\frac{1}{T_0} + \\frac{1}{T_1}\\right)  \\sigma^2(1 - \\rho_L - \\rho_T) \\left(\\frac{1}{N_G} + \\frac{1}{N_C}\\right) \\left(\\frac{1}{T_0} + \\frac{1}{T_1}\\right) \\end{pmatrix}}\n$$", "id": "3115377"}, {"introduction": "双重差分法的基石是平行趋势假设。但如果未被观测到的宏观冲击对处理组和控制组的影响不同，从而违反了这一假设，我们对处理效应 $\\tau$ 的估计就会产生偏误。这项练习 [@problem_id:3115451] 探讨了一种巧妙的解决方案：使用一个“安慰剂”结果变量。这是一个概念性的挑战，它将教你如何像计量经济学家一样思考，寻找创新的方法来代理未观测到的混淆因素，从而净化你的估计结果。", "problem": "一项政策在 $t=1$ 时刻在处理地区实施，但在此之前没有实施，而控制地区从未接受过该政策。令单位由 $i \\in \\{1,\\dots,N\\}$ 索引，时间由 $t \\in \\{0,1\\}$ 索引。定义 $D_i \\in \\{0,1\\}$ 为单位 $i$ 的处理指示变量（处理单位为 $1$，控制单位为 $0$），并令 $\\Delta X_i \\equiv X_{i,1}-X_{i,0}$ 表示随时间的一阶差分。考虑以下情景，其动因是双重差分法（DiD）中平行趋势假设的违背：存在一个宏观冲击 $M_t$，其变化 $\\Delta M \\equiv M_1 - M_0 \\neq 0$ 在不同单位上具有异质性的影响。数据生成过程为\n$$\n\\Delta Y_i \\;=\\; \\tau D_i \\;+\\; \\gamma_i\\,\\Delta M \\;+\\; \\Delta \\varepsilon_i,\n$$\n其中 $Y_{i,t}$ 是主要结果变量，$\\tau$ 是 $t=1$ 时处理组的平均处理效应，$\\gamma_i$ 捕获单位 $i$ 对宏观冲击的暴露程度，而 $\\Delta \\varepsilon_i$ 是一个异质性扰动项，满足 $\\mathbb{E}[\\Delta \\varepsilon_i\\mid D_i]=0$。由于 $\\gamma_i$ 可能与 $D_i$ 相关，标准的 DiD 对比会因 $\\gamma_i \\Delta M$ 而遭受遗漏变量偏误。我们被告知，除了 $Y_{i,t}$ 之外，还可以收集一个安慰剂结果变量 $Z_{i,t}$，它会对宏观冲击做出反应，但本身不受处理的影响。\n\n从第一性原理出发，使用线性回归的遗漏变量偏误逻辑和一阶差分的定义，推断一个辅助性的“控制结果变量”如何能帮助恢复或改善对 $\\tau$ 的识别。哪个选项最正确地提出了一个科学上可信的安慰剂结果变量，陈述了一个使用该安慰剂结果变量作为控制变量的有效的调整后 DiD 设定，并阐明了包含它能减少或消除遗漏变量偏误的最低识别条件？\n\nA. 对于一项针对特定城市工资的政策，使用城市层面的人均汽油销售额 $Z_{i,t}$ 作为安慰剂结果变量，因为它与宏观需求周期共同变动，但不受工资政策的直接影响。假设\n$$\n\\Delta Z_i \\;=\\; \\delta_i\\,\\Delta M \\;+\\; \\Delta \\eta_i,\\quad \\text{with } \\delta_i \\;=\\; k\\,\\gamma_i \\text{ for some constant } k \\neq 0,\\ \\text{and } \\mathbb{E}[\\Delta \\eta_i\\mid D_i]=0,\n$$\n并且处理对 $Z_{i,t}$ 没有直接影响。然后估计\n$$\n\\Delta Y_i \\;=\\; \\alpha \\;+\\; \\tau D_i \\;+\\; \\beta\\,\\Delta Z_i \\;+\\; u_i.\n$$\n如果 $\\Delta \\eta_i=0$（完美代理变量），宏观项满足 $\\gamma_i\\Delta M=(1/k)\\Delta Z_i$，因此包含 $\\Delta Z_i$ 可以完全消除偏误并恢复 $\\tau$。如果 $\\Delta \\eta_i\\neq 0$ 但 $\\Delta Z_i$ 与 $\\gamma_i\\Delta M$ 正相关，那么包含 $\\Delta Z_i$ 会减少（尽管不一定能消除）$\\hat{\\tau}$ 中的偏误。\n\nB. 通过设置 $Z_{i,t}=Y_{i,t}$，使用处理单位自身的主要结果变量 $Y_{i,t}$ 作为安慰剂，并估计\n$$\n\\Delta Y_i \\;=\\; \\alpha \\;+\\; \\tau D_i \\;+\\; \\beta\\,\\Delta Z_i \\;+\\; u_i.\n$$\n由于 $Z_{i,t}$ 等于 $Y_{i,t}$，它能追踪所有冲击，因此控制它总能消除 $\\hat{\\tau}$ 中的偏误，无需任何额外假设。\n\nC. 使用城市层面的降雨量 $Z_{i,t}$ 作为安慰剂结果变量，并估计水平双向固定效应模型\n$$\nY_{i,t} \\;=\\; \\alpha_i \\;+\\; \\lambda_t \\;+\\; \\tau (D_i \\cdot \\mathbb{1}\\{t=1\\}) \\;+\\; \\beta Z_{i,t} \\;+\\; \\varepsilon_{i,t}.\n$$\n降雨量是一个外生冲击，因此包含它可以吸收宏观经济混淆因素，并消除 $\\hat{\\tau}$ 中的偏误，即使对宏观需求的暴露程度在各单位之间存在差异。\n\nD. 使用主要结果变量的全国平均值 $\\bar{Y}_t$ 作为安慰剂变量，并估计\n$$\n\\Delta Y_i \\;=\\; \\alpha \\;+\\; \\tau D_i \\;+\\; \\beta\\,\\Delta \\bar{Y} \\;+\\; u_i.\n$$\n由于 $\\Delta \\bar{Y}$ 捕获了宏观冲击，因此包含它足以消除 $\\hat{\\tau}$ 中的偏误，无论宏观冲击在各单位上的影响方式如何。", "solution": "用户希望我验证并解决一个与在存在异质性趋势时使用双重差分法（DiD）估计方法相关的问题。\n\n### 问题验证\n\n**步骤 1：提取已知信息**\n\n-   一项政策在处理地区 $t=1$ 时刻实施，但在此之前没有实施。控制地区从未接受过该政策。\n-   单位由 $i \\in \\{1,\\dots,N\\}$ 索引，时间由 $t \\in \\{0,1\\}$ 索引。\n-   $D_i \\in \\{0,1\\}$ 是单位 $i$ 的处理指示变量。\n-   $\\Delta X_i \\equiv X_{i,1}-X_{i,0}$ 表示随时间的一阶差分。\n-   存在一个宏观冲击 $M_t$，其变化 $\\Delta M \\equiv M_1 - M_0 \\neq 0$。\n-   结果变量 $Y_i$ 变化的数据生成过程（DGP）是：\n    $$ \\Delta Y_i \\;=\\; \\tau D_i \\;+\\; \\gamma_i\\,\\Delta M \\;+\\; \\Delta \\varepsilon_i $$\n-   $\\tau$ 是 $t=1$ 时处理组的平均处理效应（ATT）。\n-   $\\gamma_i$ 是一个单位特定的参数，捕获对宏观冲击的暴露程度。\n-   $\\Delta \\varepsilon_i$ 是一个异质性扰动项，满足 $\\mathbb{E}[\\Delta \\varepsilon_i\\mid D_i]=0$。\n-   暴露程度 $\\gamma_i$ 可能与处理状态 $D_i$ 相关。\n-   有一个安慰剂结果变量 $Z_{i,t}$ 可用，它对宏观冲击有反应，但不受处理的影响。\n-   任务是找到一个选项，该选项正确地提出了一个可信的安慰剂、一个有效的调整后 DiD 设定，以及恢复 $\\tau$ 的最低识别条件。\n\n**步骤 2：使用提取的已知信息进行验证**\n\n-   **科学依据：** 该问题牢固地植根于因果推断的计量经济学理论，具体解决的是双重差分估计量面临的一个已知挑战：由于对总体冲击的异质性反应而导致的平行趋势假设违背。所提出的使用安慰剂或“控制”结果变量的方法是文献中公认的一种策略，通常与控制函数或代理变量方法有关。\n-   **适定性：** 该问题提供了一个清晰且数学上精确的数据生成过程。明确了感兴趣的参数（$\\tau$），指出了识别问题的来源（$\\text{Cov}(D_i, \\gamma_i) \\neq 0$），并且问题要求对提出的解决方案进行分析。这是一个适定问题。\n-   **客观性：** 问题以正式、客观的语言陈述，没有歧义或主观看法。\n\n该问题没有表现出说明中列出的任何缺陷（例如，科学上不健全、不完整、矛盾等）。这是计量经济学中一个标准的、有深度的问题。\n\n**步骤 3：结论与行动**\n\n问题陈述是**有效的**。我将继续推导解决方案并评估各个选项。\n\n### 从第一性原理推导\n\n目标是从以下数据生成过程（DGP）中获得 $\\tau$ 的一致估计量：\n$$ \\Delta Y_i \\;=\\; \\tau D_i \\;+\\; \\gamma_i\\,\\Delta M \\;+\\; \\Delta \\varepsilon_i $$\n标准的 DiD 估计量比较处理组结果变量的平均变化与控制组结果变量的平均变化。其期望值为：\n$$ \\mathbb{E}[\\hat{\\tau}_{DiD}] = \\mathbb{E}[\\Delta Y_i | D_i=1] - \\mathbb{E}[\\Delta Y_i | D_i=0] $$\n代入 DGP 并使用迭代期望定律：\n$$ \\mathbb{E}[\\Delta Y_i | D_i] = \\mathbb{E}[\\tau D_i + \\gamma_i \\Delta M + \\Delta \\varepsilon_i | D_i] = \\tau D_i + \\mathbb{E}[\\gamma_i | D_i]\\Delta M + \\mathbb{E}[\\Delta \\varepsilon_i | D_i] $$\n给定 $\\mathbb{E}[\\Delta \\varepsilon_i | D_i]=0$，这简化为 $\\tau D_i + \\mathbb{E}[\\gamma_i | D_i]\\Delta M$。\n因此，DiD 估计量的期望值为：\n$$ \\mathbb{E}[\\hat{\\tau}_{DiD}] = (\\tau \\cdot 1 + \\mathbb{E}[\\gamma_i | D_i=1]\\Delta M) - (\\tau \\cdot 0 + \\mathbb{E}[\\gamma_i | D_i=0]\\Delta M) $$\n$$ \\mathbb{E}[\\hat{\\tau}_{DiD}] = \\tau + \\left( \\mathbb{E}[\\gamma_i | D_i=1] - \\mathbb{E}[\\gamma_i | D_i=0] \\right) \\Delta M $$\n偏误项 $\\left( \\mathbb{E}[\\gamma_i | D_i=1] - \\mathbb{E}[\\gamma_i | D_i=0] \\right) \\Delta M$ 不为零，因为如前所述，$\\gamma_i$ 与 $D_i$ 相关（意味着 $\\mathbb{E}[\\gamma_i | D_i=1] \\neq \\mathbb{E}[\\gamma_i | D_i=0]$）且 $\\Delta M \\neq 0$。这是一个经典的遗漏变量偏误，其中未观测到的混淆项是 $C_i = \\gamma_i \\Delta M$。\n\n为了解决这个问题，我们可以为未观测到的混淆项使用一个代理变量。问题建议使用一个安慰剂结果变量 $Z_{i,t}$，它不受处理的影响，但受到相同宏观冲击的影响。让我们考虑对 $\\Delta Y_i$ 进行关于 $D_i$ 和代理变量 $\\Delta Z_i$ 的多元回归：\n$$ \\Delta Y_i \\;=\\; \\alpha \\;+\\; \\tau D_i \\;+\\; \\beta\\,\\Delta Z_i \\;+\\; u_i $$\n为了使此回归中 $\\tau$ 的 OLS 估计量保持一致性，回归量 $D_i$ 必须与误差项 $u_i$ 不相关，以 $\\Delta Z_i$ 为条件。让我们分析这种情况成立的条件。\n\n一个有效的代理变量 $\\Delta Z_i$ 必须与混淆项 $C_i = \\gamma_i \\Delta M$ 相关。假设安慰剂结果变量 $Z_{i,t}$ 的 DGP 类似于 $Y_{i,t}$，但没有处理效应：\n$$ \\Delta Z_i \\;=\\; \\delta_i \\Delta M \\;+\\; \\Delta \\eta_i $$\n其中 $\\delta_i$ 是安慰剂对冲击的暴露程度，$\\Delta \\eta_i$ 是其异质性误差。为了使 $\\Delta Z_i$ 成为一个有用的代理变量，我们需要在异质性暴露 $\\gamma_i$ 和 $\\delta_i$ 之间存在一个稳定的关系。\n\n一个强但有效的假设是这些暴露程度是成比例的：$\\delta_i = k \\gamma_i$，其中 $k$ 为某个非零常数。在此假设下：\n$$ \\Delta Z_i = k \\gamma_i \\Delta M + \\Delta \\eta_i $$\n由此，我们可以用可观测的代理变量 $\\Delta Z_i$ 和未观测的误差 $\\Delta \\eta_i$ 来表示未观测的混淆项：\n$$ \\gamma_i \\Delta M = \\frac{1}{k} (\\Delta Z_i - \\Delta \\eta_i) $$\n现在，将此代入 $\\Delta Y_i$ 的原始 DGP：\n$$ \\Delta Y_i = \\tau D_i + \\frac{1}{k} (\\Delta Z_i - \\Delta \\eta_i) + \\Delta \\varepsilon_i $$\n$$ \\Delta Y_i = \\tau D_i + \\frac{1}{k} \\Delta Z_i + \\left(\\Delta \\varepsilon_i - \\frac{1}{k} \\Delta \\eta_i \\right) $$\n该方程具有所提议的回归模型 $\\Delta Y_i = \\alpha + \\tau D_i + \\beta \\Delta Z_i + u_i$ 的形式，其中 $\\beta = 1/k$，误差项为 $u_i = \\Delta \\varepsilon_i - \\frac{1}{k} \\Delta \\eta_i$。\n\n为了使 $\\tau$ 的 OLS 估计量保持一致性，我们需要 $\\text{Cov}(D_i, u_i) = 0$。\n$$ \\text{Cov}(D_i, u_i) = \\text{Cov}\\left(D_i, \\Delta \\varepsilon_i - \\frac{1}{k} \\Delta \\eta_i\\right) = \\text{Cov}(D_i, \\Delta \\varepsilon_i) - \\frac{1}{k}\\text{Cov}(D_i, \\Delta \\eta_i) $$\n问题陈述了 $\\mathbb{E}[\\Delta \\varepsilon_i | D_i] = 0$，这意味着 $\\text{Cov}(D_i, \\Delta \\varepsilon_i) = 0$。因此，要消除偏误，我们需要 $\\text{Cov}(D_i, \\Delta \\eta_i) = 0$。这等同于要求安慰剂结果变量的异质性误差也与处理状态均值独立，即 $\\mathbb{E}[\\Delta \\eta_i | D_i] = 0$。\n\n总而言之，使用安慰剂结果变量 $Z_i$ 来识别 $\\tau$ 的一组充分条件是：\n1.  安慰剂不受处理的直接影响。\n2.  安慰剂对宏观冲击的异质性暴露程度 $\\delta_i$ 与主要结果变量的暴露程度 $\\gamma_i$ 成比例。\n3.  安慰剂结果变量过程中的异质性误差 $\\Delta \\eta_i$ 与处理状态 $D_i$ 均值独立。\n\n### 逐项分析\n\n**A. 对于一项针对特定城市工资的政策，使用城市层面的人均汽油销售额 $Z_{i,t}$ 作为安慰剂结果变量...**\n-   **可信的安慰剂：** 是的。城市层面的汽油销售额很可能受到宏观经济状况（$M_t$）的影响，但不太可能受到地方工资政策的直接影响（除非该政策范围非常大）。这满足了安慰剂对冲击有反应但对处理无反应的要求。\n-   **有效的设定：** 该选项提出了 $\\Delta Y_i = \\alpha + \\tau D_i + \\beta \\Delta Z_i + u_i$。这是在一阶差分中的正确代理变量回归，与我们的推导一致。\n-   **识别条件：** 该选项为安慰剂提出了一个 DGP $\\Delta Z_i = \\delta_i \\Delta M + \\Delta \\eta_i$，并明确指出了比例因子载荷这一关键识别假设，即 $\\delta_i = k \\gamma_i$，以及 $\\mathbb{E}[\\Delta \\eta_i | D_i] = 0$。如上文推导所示，这些条件足以一致地估计 $\\tau$。对于完美代理变量（$\\Delta \\eta_i = 0$）和不完美代理变量情况的解释也是标准且正确的。\n-   **结论：正确。** 该选项正确地识别了一个可信的安慰剂，指定了适当的估计模型，并阐明了一套解决问题的有效识别假设。\n\n**B. 通过设置 $Z_{i,t}=Y_{i,t}$，使用处理单位自身的主要结果变量 $Y_{i,t}$ 作为安慰剂...**\n-   **可信的安慰剂：** 否。安慰剂结果变量的一个核心要求是它*不受*处理的影响。根据定义，主要结果变量 $Y_{i,t}$ 受处理的影响。这违反了该方法的前提。\n-   **有效的设定：** 所提议的回归是 $\\Delta Y_i = \\alpha + \\tau D_i + \\beta \\Delta Z_i + u_i$。代入 $\\Delta Z_i = \\Delta Y_i$ 得到 $\\Delta Y_i = \\alpha + \\tau D_i + \\beta \\Delta Y_i + u_i$。这是一个将结果变量对自身进行回归的无意义操作，（如果天真地估计）会导致完全共线性，或产生无意义的系数。它不能用于识别 $\\tau$。\n-   **结论：不正确。** 所提议的安慰剂无效，其推理从根本上就是错误的。\n\n**C. 使用城市层面的降雨量 $Z_{i,t}$ 作为安慰剂结果变量，并估计水平双向固定效应模型...**\n-   **可信的安慰剂：** 否。混淆冲击是*宏观*冲击 $M_t$。安慰剂/代理变量必须与这个混淆因素相关。降雨量通常是一个异质性的、与天气相关的冲击，而不是宏观经济冲击。没有先验理由相信一个城市对国家经济周期的敏感度（$\\gamma_i$）与其降雨模式相关。所提议的安慰剂不满足代理变量的相关性条件。\n-   **有效的设定：** 该选项提出了一个带有双向固定效应的水平回归，这与问题中设定的一阶差分框架不同。虽然两者相关，但核心问题是，将降雨量 $Z_{i,t}$ 作为控制变量，无法解释异质性项 $\\gamma_i \\Delta M$，除非 $\\gamma_i$ 恰好是降雨量的函数，而这是不切实际的。声称“外生冲击”（如降雨量）“吸收了宏观经济混淆因素”是不合逻辑的；相关性也是一个必要条件。\n-   **结论：不正确。** 所提议的安慰剂与指定的混淆因素无关。\n\n**D. 使用主要结果变量的全国平均值 $\\bar{Y}_t$ 作为安慰剂变量...**\n-   **可信的安慰剂：** 否。所提议的控制变量是 $\\Delta \\bar{Y} = \\bar{Y}_1 - \\bar{Y}_0$。这是全国平均结果变量的变化。在 $\\Delta Y_i$ 对其他变量的横截面回归中，$\\Delta \\bar{Y}$ 对所有单位 $i$ 都是一个常数。\n-   **有效的设定：** 回归是 $\\Delta Y_i = \\alpha + \\tau D_i + \\beta \\Delta \\bar{Y} + u_i$。由于 $\\Delta \\bar{Y}$ 不随 $i$ 变化，它与截距项完全共线。$\\beta \\Delta \\bar{Y}$ 项被完全吸收到常数项中，使得该回归等同于简单的（有偏误的）DiD 回归：$\\Delta Y_i = \\alpha' + \\tau D_i + u_i$。这个设定无法解决遗漏变量问题。问题源于*异质性*项 $\\gamma_i \\Delta M$，其中异质性是跨 $i$ 的。用一个在 $i$ 之间恒定的总和项进行控制，无法解决这种异质性。\n-   **结论：不正确。** 一个在观测单位之间保持不变的控制变量，无法控制在这些相同单位之间变化的混淆因素。", "answer": "$$\\boxed{A}$$", "id": "3115451"}, {"introduction": "真实世界的数据往往是“杂乱”的，可能包含极端值或异常值，这些值会严重扭曲对处理效应 $\\tau$ 的标准估计结果（如普通最小二乘法）。这项练习 [@problem_id:3115350] 介绍了一种强大的技术——使用Huber损失的M估计法——来构建一个对异常值不那么敏感的稳健双重差分估计量。通过编程和模拟，你将实现该方法并探索其“崩溃点”——即在估计量给出无意义结果之前，它所能承受的数据污染程度。", "problem": "考虑一个两期、两组的双重差分（difference-in-differences）设定，其中观测单元 $i \\in \\{1,\\dots,n\\}$ 在时间点 $t \\in \\{0,1\\}$ 被观测。令 $D_i \\in \\{0,1\\}$ 表示单元 $i$ 是否属于处理组（处理仅在处理后时期发生）。假设观测到的结果满足加性模型 $Y_{it} = \\mu + \\alpha_i + \\lambda_t + \\tau \\cdot (D_i \\cdot \\mathbb{1}\\{t=1\\}) + \\epsilon_{it}$，其中 $\\mu$ 是全局均值，$\\alpha_i$ 是单元固定效应，$\\lambda_t$ 是时间效应，$\\tau$ 是我们感兴趣的处理效应，$\\epsilon_{it}$ 是误差项。在标准的平行趋势假设下，对时间作一阶差分可以消除单元固定效应，得到每个单元的变化量\n$$\n\\Delta Y_i \\equiv Y_{i1} - Y_{i0} = \\lambda + \\tau D_i + u_i,\n$$\n其中 $\\lambda \\equiv \\lambda_1 - \\lambda_0$ 且 $u_i \\equiv \\epsilon_{i1} - \\epsilon_{i0}$。在存在重尾误差 $u_i$ 的情况下，普通最小二乘法对异常值很敏感。为了获得稳健性，考虑一种 $M$-估计方法，该方法通过最小化应用于差分后双重差分模型的回归残差的 Huber 损失函数来进行估计：\n$$\n\\min_{\\beta_0, \\beta_1} \\sum_{i=1}^n \\rho_\\kappa\\!\\left(\\Delta Y_i - \\beta_0 - \\beta_1 D_i\\right),\n$$\n其中 $\\beta_0$ 作为 $\\lambda$ 的代理（估计），$\\beta_1$ 作为 $\\tau$ 的代理（估计），Huber 损失函数定义为\n$$\n\\rho_\\kappa(z) = \n\\begin{cases}\n\\frac{1}{2} z^2,   \\text{if } |z| \\le \\kappa, \\\\\n\\kappa |z| - \\frac{1}{2}\\kappa^2,   \\text{if } |z|  \\kappa.\n\\end{cases}\n$$\n其相关的得分函数为\n$$\n\\psi_\\kappa(z) = \\rho_\\kappa'(z) = \n\\begin{cases}\nz,   \\text{if } |z| \\le \\kappa, \\\\\n\\kappa \\cdot \\mathrm{sign}(z),   \\text{if } |z|  \\kappa.\n\\end{cases}\n$$\n该 $M$-估计量的一种稳健实现是使用迭代重加权最小二乘法（iteratively reweighted least squares），其权重由 $\\psi_\\kappa$ 决定。为稳定迭代过程，使用一个从残差中通过中位数绝对偏差（MAD）估计的稳健尺度参数 $s$，即 $s = c_{\\mathrm{MAD}} \\cdot \\mathrm{MAD}$，其中 $c_{\\mathrm{MAD}} = 1.4826$。\n\n你的任务是：\n- 从第一性原理推导出一个算法，用于计算在差分后的双重差分模型残差上使用 Huber 损失函数的稳健 $M$-估计量 $(\\hat{\\beta}_0, \\hat{\\beta}_1)$。在一个迭代重加权最小二乘法方案中实现该算法，该方案在每次迭代中通过一个稳健的尺度参数 $s$ 重新调整残差的尺度，并根据 Huber 得分函数更新权重。\n- 使用一个与差分后模型 $\\Delta Y_i = \\lambda + \\tau D_i + u_i$ 一致的合成数据生成过程，检验在重尾误差和对抗性污染下，$\\tau$ 的稳健估计量的实际崩溃行为。具体而言：\n  - 对每个 $i$ 独立地生成 $D_i \\sim \\mathrm{Bernoulli}(p_T)$。\n  - 从自由度为 $\\nu$ 的学生t分布中生成 $u_i$。当 $\\nu  2$ 时，通过乘以 $\\sigma \\sqrt{\\frac{\\nu - 2}{\\nu}}$ 来调整抽样值，使其方差为 $\\sigma^2$；当 $\\nu \\le 2$ 时，仅乘以 $\\sigma$ 来设定一个名义尺度。\n  - 构建 $\\Delta Y_i = \\lambda + \\tau D_i + u_i$。\n  - 通过均匀随机选择比例为 $p$ 的索引，并添加符号随机、大小为 $A$ 的大异常值来模拟对抗性污染，即对于选中的索引，将 $\\Delta Y_i$ 替换为 $\\Delta Y_i + s_i A$，$s_i \\in \\{-1, +1\\}$。\n- 定义实际崩溃准则如下：对于给定的污染比例 $p$，计算稳健估计值 $\\hat{\\tau}(p)$，如果对于一个固定的容差 $\\delta_{\\mathrm{tol}}$，有 $|\\hat{\\tau}(p) - \\tau|  \\delta_{\\mathrm{tol}}$，则宣告崩溃。实际崩溃点是在指定网格上宣告崩溃的最小 $p$ 值。\n- 实现该过程，以计算在一系列污染比例 $p \\in \\{0, 0.05, 0.10, \\dots, 0.50\\}$ 的网格上的实际崩溃点。\n\n使用以下参数化测试套件（所有标量与结果的单位相同，自由度无单位）。对于每个测试用例，你必须：\n- 使用固定的随机种子模拟 $\\Delta Y_i$ 和 $D_i$ 以保证可复现性。\n- 在指定的网格上计算实际崩溃点。\n- 以 $[0,1]$ 范围内的小数形式返回实际崩溃点，四舍五入到三位小数。如果网格内没有发生崩溃，则返回网格中的最大值。\n\n测试套件：\n- 案例 1：$n = 800$, $p_T = 0.5$, $\\tau = 2$, $\\lambda = 0$, $\\nu = 5$, $\\sigma = 1$, $A = 1000$, $\\delta_{\\mathrm{tol}} = 1.0$, seed $= 12345$。\n- 案例 2：$n = 800$, $p_T = 0.5$, $\\tau = 2$, $\\lambda = 0$, $\\nu = 2$, $\\sigma = 1$, $A = 1000$, $\\delta_{\\mathrm{tol}} = 1.0$, seed $= 23456$。\n- 案例 3：$n = 800$, $p_T = 0.5$, $\\tau = 2$, $\\lambda = 0$, $\\nu = 1$, $\\sigma = 1$, $A = 1000$, $\\delta_{\\mathrm{tol}} = 1.0$, seed $= 34567$。\n- 案例 4：$n = 100$, $p_T = 0.5$, $\\tau = 2$, $\\lambda = 0$, $\\nu = 2$, $\\sigma = 1$, $A = 1000$, $\\delta_{\\mathrm{tol}} = 1.0$, seed $= 45678$。\n\n你的程序应生成单行输出，其中包含四个测试用例的实际崩溃点，形式为用方括号括起来的逗号分隔列表，例如 $[\\text{result1},\\text{result2},\\text{result3},\\text{result4}]$。所有四个值都必须是四舍五入到三位小数的小数。", "solution": "所提供的问题被认为是有效的。它在稳健统计学应用于标准计量经济模型——双重差分（DiD）框架的领域内，提出了一个良构的任务。模拟和估计所需的所有必要参数和条件都已提供，只有一个小小的例外，即 Huber 损失参数 $\\kappa$。对此，我们对标准化残差采用标准值 $\\kappa = 1.345$，这是稳健 M 估计中一个被广泛接受的惯例，其目标是在高斯数据上达到 95% 的效率。\n\n问题的核心是使用迭代重加权最小二乘法（IRLS）算法为 DiD 模型实现一个 $M$-估计量。我们首先从第一性原理推导该算法，然后描述其在模拟研究中的实现，以确定该估计量的实际崩溃点。\n\n### 算法推导：用于 Huber DiD 的迭代重加权最小二乘法\n\n问题在于找到参数 $(\\hat{\\beta}_0, \\hat{\\beta}_1)$，以最小化残差的 Huber 损失之和：\n$$\n\\min_{\\beta_0, \\beta_1} \\sum_{i=1}^n \\rho_\\kappa\\left(\\Delta Y_i - \\beta_0 - \\beta_1 D_i\\right)\n$$\n此处，$\\beta_0$ 是时间趋势 $\\lambda$ 的估计量，$\\beta_1$ 是处理效应 $\\tau$ 的估计量。为使估计量具有尺度不变性，残差 $r_i = \\Delta Y_i - (\\beta_0 + \\beta_1 D_i)$ 通过一个稳健的尺度度量 $s$ 进行标准化。然后，最小化问题应用于经过尺度调整的残差。通过对目标函数关于 $\\beta_0$ 和 $\\beta_1$ 求导并将结果设为零，可获得一阶条件。使用得分函数 $\\psi_\\kappa(z) = \\rho_\\kappa'(z)$，估计方程为：\n$$\n\\begin{align*}\n\\frac{\\partial}{\\partial \\beta_0}: \\quad  \\sum_{i=1}^n -\\psi_\\kappa\\left(\\frac{\\Delta Y_i - \\beta_0 - \\beta_1 D_i}{s}\\right) = 0 \\\\\n\\frac{\\partial}{\\partial \\beta_1}: \\quad  \\sum_{i=1}^n -\\psi_\\kappa\\left(\\frac{\\Delta Y_i - \\beta_0 - \\beta_1 D_i}{s}\\right) D_i = 0\n\\end{align*}\n$$\n这些方程构成了关于 $\\beta_0$ 和 $\\beta_1$ 的一个非线性方程组。IRLS 算法提供了一个寻找解的迭代过程。我们为每个观测值 $i$ 定义一个权重 $w_i = \\psi_\\kappa(z_i) / z_i$，其中 $z_i = r_i/s$ 是经过尺度调整的残差。根据此定义，得分函数可以写为 $\\psi_\\kappa(z_i) = w_i z_i$。将此代入估计方程，得到：\n$$\n\\begin{align*}\n\\sum_{i=1}^n w_i \\frac{r_i}{s} = 0 \\quad \\implies \\quad \\sum_{i=1}^n w_i (\\Delta Y_i - \\beta_0 - \\beta_1 D_i) = 0 \\\\\n\\sum_{i=1}^n w_i \\frac{r_i}{s} D_i = 0 \\quad \\implies \\quad \\sum_{i=1}^n w_i (\\Delta Y_i - \\beta_0 - \\beta_1 D_i) D_i = 0\n\\end{align*}\n$$\n这些是 $\\Delta Y_i$ 对截距项和 $D_i$ 进行加权最小二乘（WLS）回归的正规方程，权重为 $w_i$。对于两组模型（控制组 $D_i=0$，处理组 $D_i=1$）的特定情况，$\\beta_0$ 和 $\\beta_1$ 的 WLS 解大大简化。参数 $\\beta_0$ 代表控制组的（加权）平均结果，而 $\\beta_0+\\beta_1$ 代表处理组的（加权）平均结果。\n$$\n\\hat{\\beta}_0 = \\frac{\\sum_{i:D_i=0} w_i \\Delta Y_i}{\\sum_{i:D_i=0} w_i} \\qquad \\text{and} \\qquad \\hat{\\beta}_0 + \\hat{\\beta}_1 = \\frac{\\sum_{i:D_i=1} w_i \\Delta Y_i}{\\sum_{i:D_i=1} w_i}\n$$\n由此，处理效应 $\\tau$ 的估计值为：\n$$\n\\hat{\\beta}_1 = \\hat{\\tau} = \\frac{\\sum_{i:D_i=1} w_i \\Delta Y_i}{\\sum_{i:D_i=1} w_i} - \\frac{\\sum_{i:D_i=0} w_i \\Delta Y_i}{\\sum_{i:D_i=0} w_i}\n$$\n由于权重 $w_i$ 依赖于残差 $r_i$，而残差又依赖于 $\\beta_0$ 和 $\\beta_1$，这表明需要一个迭代解法。\n\n### IRLS 算法流程\n\n1.  **初始化**：从一个初始估计 $\\boldsymbol{\\hat{\\beta}}^{(0)} = (\\hat{\\beta}_0^{(0)}, \\hat{\\beta}_1^{(0)})$ 开始。一个简单的选择是普通最小二乘（OLS）估计，这对应于将所有权重 $w_i$ 设为 1。\n2.  **迭代**：对每一步 $k=0, 1, 2, \\dots$：\n    a.  计算残差：$r_i^{(k)} = \\Delta Y_i - (\\hat{\\beta}_0^{(k)} + \\hat{\\beta}_1^{(k)} D_i)$。\n    b.  使用残差的中位数绝对偏差（MAD）估计稳健尺度参数 $s^{(k)}$：$s^{(k)} = c_{\\mathrm{MAD}} \\cdot \\mathrm{median}_i(|r_i^{(k)} - \\mathrm{median}_j(r_j^{(k)})|)$，常数 $c_{\\mathrm{MAD}}=1.4826$ 用于保证在正态误差分布下的一致性。如果 $s^{(k)}$ 接近于零，可以停止迭代。\n    c.  计算经过尺度调整的残差 $z_i^{(k)} = r_i^{(k)} / s^{(k)}$。\n    d.  基于参数为 $\\kappa = 1.345$ 的 Huber 函数计算新权重 $w_i^{(k+1)}$：\n        $$\n        w_i^{(k+1)} = \\begin{cases} 1   \\text{if } |z_i^{(k)}| \\le \\kappa \\\\ \\kappa / |z_i^{(k)}|   \\text{if } |z_i^{(k)}|  \\kappa \\end{cases}\n        $$\n    e.  使用权重 $w_i^{(k+1)}$ 计算控制组和处理组的加权平均值，以更新参数估计：\n        $$\n        \\hat{\\beta}_0^{(k+1)} = \\frac{\\sum_{i:D_i=0} w_i^{(k+1)} \\Delta Y_i}{\\sum_{i:D_i=0} w_i^{(k+1)}} ; \\quad \\hat{\\beta}_1^{(k+1)} = \\frac{\\sum_{i:D_i=1} w_i^{(k+1)} \\Delta Y_i}{\\sum_{i:D_i=1} w_i^{(k+1)}} - \\hat{\\beta}_0^{(k+1)}\n        $$\n3.  **终止**：当系数向量 $\\boldsymbol{\\hat{\\beta}} = (\\hat{\\beta}_0, \\hat{\\beta}_1)^T$ 的相对变化小于一个很小的容差时，或者达到最大迭代次数后，迭代停止。\n\n### 实际崩溃点的模拟\n\n实际崩溃点是通过向数据中注入比例递增 $p$ 的对抗性异常值，并观察估计量 $\\hat{\\tau}(p)$ 何时偏离真实值 $\\tau$ 超过指定的容差 $\\delta_{\\mathrm{tol}}$ 来确定的。\n\n1.  **数据生成**：对于一个固定的随机种子，生成一个“干净”的数据集。\n    -   对 $i=1, \\dots, n$，生成处理指标 $D_i \\sim \\mathrm{Bernoulli}(p_T)$。\n    -   重尾误差 $u_i$ 从自由度为 $\\nu$ 的学生t分布中抽取，并由 $\\sigma$ 进行尺度调整（如果 $\\nu2$，则使用方差调整因子）。\n    -   构建结果变量：$\\Delta Y_i = \\lambda + \\tau D_i + u_i$。\n2.  **污染循环**：在一系列污染比例 $p \\in \\{0, 0.05, \\dots, 0.50\\}$ 的网格上进行迭代。\n    -   对每个 $p$，通过选择一个大小为 $\\lfloor p \\cdot n \\rfloor$ 的观测值随机子集并添加一个大值 $\\pm A$ 来创建一个污染数据集。\n3.  **估计与崩溃检查**：\n    -   将上述 IRLS 算法应用于污染数据集，以获得估计值 $\\hat{\\tau}(p)$。\n    -   检查崩溃准则：$|\\hat{\\tau}(p) - \\tau|  \\delta_{\\mathrm{tol}}$。\n    -   实际崩溃点是网格中第一个满足此准则的 $p$ 值。如果网格中没有任何 $p$ 值满足此准则，则认为崩溃点至少为网格中的最大值，即 0.50。```python\nimport numpy as np\n\ndef huber_did(Y, D, kappa=1.345, max_iter=100, tol=1e-6):\n    \"\"\"\n    Computes the Huber M-estimator for a collapsed difference-in-differences model\n    using Iteratively Reweighted Least Squares (IRLS).\n\n    Args:\n        Y (np.ndarray): vector of outcome changes (Delta Y_i).\n        D (np.ndarray): vector of treatment indicators (D_i).\n        kappa (float): Huber loss parameter for standardized residuals.\n        max_iter (int): Maximum number of iterations for IRLS.\n        tol (float): Convergence tolerance for the coefficient vector.\n\n    Returns:\n        float: The estimated treatment effect (beta_1).\n    \"\"\"\n    ctrl_mask = (D == 0)\n    treat_mask = (D == 1)\n    \n    if not np.any(ctrl_mask) or not np.any(treat_mask):\n        return np.nan # Should not happen with problem data generation\n        \n    # Initial estimate using Ordinary Least Squares (OLS)\n    beta0 = np.mean(Y[ctrl_mask])\n    mean_treat = np.mean(Y[treat_mask])\n    beta1 = mean_treat - beta0\n    beta = np.array([beta0, beta1])\n\n    for _ in range(max_iter):\n        beta_old = beta.copy()\n        \n        # 1. Compute residuals\n        residuals = Y - (beta[0] + beta[1] * D)\n        \n        # 2. Compute robust scale using Median Absolute Deviation (MAD)\n        median_residuals = np.median(residuals)\n        mad = np.median(np.abs(residuals - median_residuals))\n        \n        # Scale factor c_MAD for consistency at the Normal model\n        scale = 1.4826 * mad\n        \n        # If scale is near-zero, residuals are likely constant.\n        # The unweighted estimate is stable and optimal.\n        if scale  1e-8:\n            break\n            \n        # 3. Compute Huber weights\n        z = residuals / scale\n        weights = np.ones_like(z)\n        huber_idx = np.abs(z)  kappa\n        weights[huber_idx] = kappa / np.abs(z[huber_idx])\n        \n        # 4. Update estimates via Weighted Least Squares (WLS)\n        # For the two-group case, this simplifies to weighted means.\n        w_ctrl = weights[ctrl_mask]\n        Y_ctrl = Y[ctrl_mask]\n        sum_w_ctrl = np.sum(w_ctrl)\n        \n        w_treat = weights[treat_mask]\n        Y_treat = Y[treat_mask]\n        sum_w_treat = np.sum(w_treat)\n        \n        # Pathological case: if all weights in a group are zero, revert to OLS.\n        if sum_w_ctrl  1e-8 or sum_w_treat  1e-8:\n            beta0 = np.mean(Y[ctrl_mask])\n            mean_treat = np.mean(Y[treat_mask])\n            beta1 = mean_treat - beta0\n            beta = np.array([beta0, beta1])\n            break\n\n        beta0_new = np.sum(w_ctrl * Y_ctrl) / sum_w_ctrl\n        mean_treat_new = np.sum(w_treat * Y_treat) / sum_w_treat\n        beta1_new = mean_treat_new - beta0_new\n        beta = np.array([beta0_new, beta1_new])\n\n        # 5. Check for convergence\n        rel_diff = np.linalg.norm(beta - beta_old) / (np.linalg.norm(beta_old) + 1e-8)\n        if rel_diff  tol:\n            break\n            \n    return beta[1]\n\ndef find_breakdown_point(n, p_T, tau, lam, nu, sigma, A, delta_tol, seed):\n    \"\"\"\n    Simulates data and finds the practical breakdown point of the Huber DiD estimator.\n    \n    Returns:\n        float: The smallest contamination fraction 'p' that causes breakdown, \n               rounded to three decimal places.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    \n    # 1. Generate clean data\n    D = rng.binomial(1, p_T, size=n)\n    u_raw = rng.standard_t(nu, size=n)\n    \n    if nu  2:\n        scale_factor = sigma * np.sqrt((nu - 2) / nu)\n    else:\n        scale_factor = sigma\n    u = u_raw * scale_factor\n    \n    Delta_Y_clean = lam + tau * D + u\n    \n    # 2. Iterate over contamination grid\n    p_grid = np.arange(0.0, 0.51, 0.05)\n    all_indices = np.arange(n)\n\n    for p in p_grid:\n        Delta_Y_contaminated = Delta_Y_clean.copy()\n        n_outliers = int(round(p * n))\n        \n        if n_outliers  0:\n            outlier_indices = rng.choice(all_indices, size=n_outliers, replace=False)\n            signs = rng.choice([-1, 1], size=n_outliers)\n            Delta_Y_contaminated[outlier_indices] += signs * A\n            \n        # 3. Estimate effect and check for breakdown\n        tau_hat = huber_did(Delta_Y_contaminated, D)\n        \n        if np.abs(tau_hat - tau)  delta_tol:\n            return round(p, 3)\n            \n    # If no breakdown occurred, return the largest p value\n    return round(p_grid[-1], 3)\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print the results.\n    \"\"\"\n    test_cases = [\n        # n, p_T, tau, lambda, nu, sigma, A, delta_tol, seed\n        (800, 0.5, 2, 0, 5, 1, 1000, 1.0, 12345),\n        (800, 0.5, 2, 0, 2, 1, 1000, 1.0, 23456),\n        (800, 0.5, 2, 0, 1, 1, 1000, 1.0, 34567),\n        (100, 0.5, 2, 0, 2, 1, 1000, 1.0, 45678),\n    ]\n\n    results = []\n    for case_params in test_cases:\n        result = find_breakdown_point(*case_params)\n        results.append(f\"{result:.3f}\")\n\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "answer": "$$\\boxed{[0.250, 0.250, 0.250, 0.500]}$$", "id": "3115350"}]}