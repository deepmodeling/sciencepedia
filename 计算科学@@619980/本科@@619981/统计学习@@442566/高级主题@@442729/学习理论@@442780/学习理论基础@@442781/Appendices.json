{"hands_on_practices": [{"introduction": "理解模型为何能够泛化的第一步是衡量其假设类的复杂性。在二元分类问题中，Vapnik-Chervonenkis (VC) 维是实现这一目标的基础工具。本练习将引导你从第一性原理出发，计算单调合取式的VC维，并将其与奇偶函数进行比较，从而揭示这一核心概念的深刻见解及其局限性。[@problem_id:3138549]", "problem": "考虑布尔超立方体 $\\{0,1\\}^{d}$，其中 $d \\in \\mathbb{N}$。定义一个单调合取假设类如下：对于每个子集 $S \\subseteq [d] := \\{1,2,\\dots,d\\}$，假设 $h_{S} : \\{0,1\\}^{d} \\to \\{0,1\\}$ 由 $h_{S}(x) = \\bigwedge_{i \\in S} x_{i}$ 给出，其中 $\\bigwedge$ 表示逻辑与。空合取 $S=\\varnothing$ 被解释为对所有 $x \\in \\{0,1\\}^{d}$ 都有 $h_{\\varnothing}(x)=1$。\n\n从 Vapnik–Chervonenkis (VC) 维的基础定义和打散的概念出发，并仅使用统计学习理论中已得到充分验证的事实，如增长函数和一致收敛论证，完成以下任务：\n\n1. 正式构建单调合取类 $\\mathcal{H}_{\\mathrm{mc}} := \\{ h_{S} : S \\subseteq [d] \\}$，并从第一性原理确定其 Vapnik–Chervonenkis (VC) 维 $d_{\\mathrm{VC}}(\\mathcal{H}_{\\mathrm{mc}})$。您的推导必须通过一个显式的被打散的集合来证明一个匹配的下界，并通过基数或增长函数论证来证明一个上界。\n\n2. 定义奇偶性假设类 $\\mathcal{P} := \\{ g_{S} : S \\subseteq [d] \\}$，其中 $g_{S}(x) = \\bigoplus_{i \\in S} x_{i}$，$\\bigoplus$ 表示模 2 加法。使用相同的基础方法，在常见的准确度参数 $\\epsilon \\in (0,1)$ 和置信度参数 $\\delta \\in (0,1)$ 下，比较在可实现情况下，$\\mathcal{H}_{\\mathrm{mc}}$ 与 $\\mathcal{P}$ 的可能近似正确 (PAC) 学习的样本复杂度。将此比较表示为一个封闭形式的比率 $R(d,\\epsilon,\\delta)$，该比率是相同地应用于这两个类的标准一致收敛 (VC) 界所指示的最小充分样本大小之比；假设界中的任何通用常数在两个类之间是共享的。\n\n3. 基于第一性原理，定性地讨论这些结构差异如何与输入端的独立随机位翻转以及率为 $\\eta \\in (0,1)$ 的随机分类噪声下的噪声敏感性相关。\n\n您的最终答案必须是一个单行矩阵，其中包含 $d_{\\mathrm{VC}}(\\mathcal{H}_{\\mathrm{mc}})$ 和比率 $R(d,\\epsilon,\\delta)$，并简化为封闭形式的表达式。不需要四舍五入，最终答案中不包含任何单位。", "solution": "该问题陈述是计算学习理论中一个定义明确的练习。所有术语都有正式定义，任务明确，其科学背景植根于 Vapnik–Chervonenkis (VC) 理论和可能近似正确 (PAC) 学习的基础原理。该问题是有效的，并且可以按陈述求解。\n\n### 1. 单调合取 ($\\mathcal{H}_{\\mathrm{mc}}$) 的 VC 维\n\n一个假设类 $\\mathcal{H}$ 的 Vapnik–Chervonenkis (VC) 维，记作 $d_{\\mathrm{VC}}(\\mathcal{H})$，是指能被 $\\mathcal{H}$ 打散的最大点集的大小。如果对于集合 $C$ 中点的每一种可能的二分（标记），在 $\\mathcal{H}$ 中都存在一个假设能够实现该标记，则称集合 $C$ 被 $\\mathcal{H}$ 打散。我们将通过证明匹配的下界和上界来确定 $d_{\\mathrm{VC}}(\\mathcal{H}_{\\mathrm{mc}})$。\n\n#### 下界：$d_{\\mathrm{VC}}(\\mathcal{H}_{\\mathrm{mc}}) \\ge d$\n\n为了建立一个为 $d$ 的下界，我们必须在 $\\{0,1\\}^{d}$ 中展示一个包含 $d$ 个点的集合，该集合能被 $\\mathcal{H}_{\\mathrm{mc}}$ 打散。令 $\\mathbf{1}$ 表示 $\\{0,1\\}^d$ 中全为 1 的向量，令 $e_i$ 为标准基向量，其在第 $i$ 个位置为 1，其他位置为 0。我们假设集合 $C = \\{x^{(1)}, x^{(2)}, \\dots, x^{(d)}\\}$，其中 $x^{(i)} = \\mathbf{1} - e_i$。因此，每个向量 $x^{(i)}$ 在坐标 $i$ 处有一个唯一的 0，在其他所有位置都是 1。\n\n为了证明 $C$ 被打散，我们必须证明对于任意标记向量 $y = (y_1, y_2, \\dots, y_d) \\in \\{0,1\\}^d$，都存在一个假设 $h_S \\in \\mathcal{H}_{\\mathrm{mc}}$，使得对所有 $i \\in \\{1, \\dots, d\\}$ 都有 $h_S(x^{(i)}) = y_i$。\n\n假设 $h_S$ 定义为 $h_S(x) = \\bigwedge_{j \\in S} x_j$。$h_S(x^{(i)})$ 的值由 $x^{(i)}$ 在属于集合 $S$ 的索引上的分量决定。根据构造，$x^{(i)}$ 向量的分量满足：对所有 $j \\neq i$ 有 $(x^{(i)})_j=1$，且 $(x^{(i)})_i=0$。\n\n因此，当且仅当存在至少一个索引 $j \\in S$ 使得 $(x^{(i)})_j = 0$ 时，合取 $\\bigwedge_{j \\in S} (x^{(i)})_j$ 等于 $0$。这当且仅当索引 $i$（$x^{(i)}$ 中唯一零的位置）是 $S$ 的一个元素时发生。\n形式化地：\n$$\nh_S(x^{(i)}) = 0 \\iff \\exists j \\in S \\text{ 使得 } (x^{(i)})_j=0 \\iff i \\in S\n$$\n等价地，$h_S(x^{(i)}) = 1 \\iff i \\notin S$。\n\n我们的目标是找到一个集合 $S$ 来生成所需的标记 $y$。我们需要对每个 $i \\in \\{1, \\dots, d\\}$ 都有 $h_S(x^{(i)}) = y_i$。\n根据上述逻辑，我们必须满足条件：\n$$\ny_i = 0 \\iff i \\in S\n$$\n这个条件唯一地定义了所需的集合 $S$。我们选择 $S = \\{i \\in \\{1, \\dots, d\\} \\mid y_i = 0\\}$。\n\n选择了这个 $S$ 后，我们来验证标记：\n- 如果我们需要用 $y_k=0$ 来标记 $x^{(k)}$，我们的构造将 $k$ 放入 $S$ 中。那么 $h_S(x^{(k)})=0$，这是正确的。\n- 如果我们需要用 $y_j=1$ 来标记 $x^{(j)}$，我们的构造确保 $j \\notin S$。那么 $h_S(x^{(j)})=1$，这也是正确的。\n\n由于对于 $2^d$ 种可能的标记 $y \\in \\{0,1\\}^d$ 中的任何一种，我们都可以构造出相应的假设 $h_S \\in \\mathcal{H}_{\\mathrm{mc}}$，因此大小为 $d$ 的集合 $C$ 被打散了。这证明了 $d_{\\mathrm{VC}}(\\mathcal{H}_{\\mathrm{mc}}) \\ge d$。\n\n#### 上界：$d_{\\mathrm{VC}}(\\mathcal{H}_{\\mathrm{mc}}) \\le d$\n\n学习理论中的一个标准结果表明，对于任何有限假设类 $\\mathcal{H}$，其 VC 维受其基数的对数限制：$d_{\\mathrm{VC}}(\\mathcal{H}) \\le \\log_2(|\\mathcal{H}|)$。\n\n单调合取类 $\\mathcal{H}_{\\mathrm{mc}} = \\{h_S : S \\subseteq [d]\\}$ 是通过选择坐标 $[d] = \\{1, 2, \\dots, d\\}$ 的一个子集来构造的。这样的子集的数量是 $2^d$。每个不同的子集 $S$ 都定义了一个唯一的假设函数。因此，该假设类的基数是 $|\\mathcal{H}_{\\mathrm{mc}}| = 2^d$。\n\n应用基数界，我们得到：\n$$\nd_{\\mathrm{VC}}(\\mathcal{H}_{\\mathrm{mc}}) \\le \\log_2(|\\mathcal{H}_{\\mathrm{mc}}|) = \\log_2(2^d) = d\n$$\n这证明了任何大小为 $d+1$ 的集合都不能被打散，因此 $d_{\\mathrm{VC}}(\\mathcal{H}_{\\mathrm{mc}}) \\le d$。\n\n结合下界和上界，我们得出结论，单调合取类的 VC 维恰好是 $d$。\n$$\nd_{\\mathrm{VC}}(\\mathcal{H}_{\\mathrm{mc}}) = d\n$$\n\n### 2. 样本复杂度比较\n\n为了比较 $\\mathcal{H}_{\\mathrm{mc}}$ 和奇偶性类 $\\mathcal{P}$ 的样本复杂度，我们首先需要确定 $\\mathcal{P}$ 的 VC 维。奇偶性假设类是 $\\mathcal{P} := \\{ g_{S} : S \\subseteq [d] \\}$，其中 $g_{S}(x) = \\bigoplus_{i \\in S} x_{i}$。\n\n#### 奇偶性类 ($\\mathcal{P}$) 的 VC 维\n\n与对 $\\mathcal{H}_{\\mathrm{mc}}$ 的分析类似，我们建立下界和上界。\n\n**下界：$d_{\\mathrm{VC}}(\\mathcal{P}) \\ge d$**\n我们展示一个能被 $\\mathcal{P}$ 打散的包含 $d$ 个点的集合。考虑标准基向量的集合，$C = \\{e_1, e_2, \\dots, e_d\\}$。\n对于一个任意的标记 $y = (y_1, y_2, \\dots, y_d) \\in \\{0,1\\}^d$，我们必须找到一个集合 $S \\subseteq [d]$，使得对所有 $i \\in \\{1, \\dots, d\\}$ 都有 $g_S(e_i) = y_i$。\n将假设 $g_S$ 应用于 $e_i$ 得到 $g_S(e_i) = \\bigoplus_{j \\in S} (e_i)_j$。因为只有当 $j=i$ 时 $(e_i)_j=1$，否则为 $0$，所以如果 $i \\in S$，则模 2 和为 $1$，如果 $i \\notin S$，则为 $0$。\n$$\ng_S(e_i) = 1 \\iff i \\in S\n$$\n为了实现标记 $y$，我们需要 $g_S(e_i) = y_i$，这意味着我们必须满足条件：\n$$\ny_i = 1 \\iff i \\in S\n$$\n这唯一地定义了所需的集合 $S = \\{i \\in \\{1, \\dots, d\\} \\mid y_i = 1\\}$。因为对于每个可能的标记向量 $y$，都存在这样一个唯一的集合，所以集合 $C$ 被 $\\mathcal{P}$ 打散了。因此，$d_{\\mathrm{VC}}(\\mathcal{P}) \\ge d$。\n\n**上界：$d_{\\mathrm{VC}}(\\mathcal{P}) \\le d$**\n奇偶性类 $\\mathcal{P}$ 也是由 $[d]$ 的所有子集定义的，所以其基数为 $|\\mathcal{P}| = 2^d$。应用与之前相同的基数界：\n$$\nd_{\\mathrm{VC}}(\\mathcal{P}) \\le \\log_2(|\\mathcal{P}|) = \\log_2(2^d) = d\n$$\n我们得出结论，$d_{\\mathrm{VC}}(\\mathcal{P}) = d$。\n\n#### 样本大小之比\n\n在可实现情况下的 PAC 学习的样本复杂度由依赖于假设类的 VC 维、准确度参数 $\\epsilon$ 和置信度参数 $\\delta$ 的界给出。一个常见的标准界的形式为：\n$$\nN(\\mathcal{H}, \\epsilon, \\delta) \\ge \\frac{K}{\\epsilon} \\left(d_{\\mathrm{VC}}(\\mathcal{H}) \\ln\\left(\\frac{1}{\\epsilon}\\right) + \\ln\\left(\\frac{1}{\\delta}\\right)\\right)\n$$\n其中 $K$ 是一个通用常数。问题要求我们将一个相同的标准界应用于这两个类。\n\n对于单调合取类 $\\mathcal{H}_{\\mathrm{mc}}$，最小充分样本大小 $N_{\\mathrm{mc}}$ 由该界确定，其中 $d_{\\mathrm{VC}}(\\mathcal{H}_{\\mathrm{mc}}) = d$：\n$$\nN_{\\mathrm{mc}}(d,\\epsilon,\\delta) = \\frac{K}{\\epsilon} \\left(d \\ln\\left(\\frac{1}{\\epsilon}\\right) + \\ln\\left(\\frac{1}{\\delta}\\right)\\right)\n$$\n对于奇偶性类 $\\mathcal{P}$，最小充分样本大小 $N_{\\mathcal{P}}$ 由相同的界确定，其中 $d_{\\mathrm{VC}}(\\mathcal{P}) = d$：\n$$\nN_{\\mathcal{P}}(d,\\epsilon,\\delta) = \\frac{K}{\\epsilon} \\left(d \\ln\\left(\\frac{1}{\\epsilon}\\right) + \\ln\\left(\\frac{1}{\\delta}\\right)\\right)\n$$\n由于两个类具有相同的 VC 维，任何仅依赖于 $d_{\\mathrm{VC}}$、$\\epsilon$ 和 $\\delta$ 的标准样本复杂度界都将为两者产生相同的值。\n\n因此，比率 $R(d,\\epsilon,\\delta)$ 为：\n$$\nR(d,\\epsilon,\\delta) = \\frac{N_{\\mathrm{mc}}(d,\\epsilon,\\delta)}{N_{\\mathcal{P}}(d,\\epsilon,\\delta)} = \\frac{\\frac{K}{\\epsilon} \\left(d \\ln\\left(\\frac{1}{\\epsilon}\\right) + \\ln\\left(\\frac{1}{\\delta}\\right)\\right)}{\\frac{K}{\\epsilon} \\left(d \\ln\\left(\\frac{1}{\\epsilon}\\right) + \\ln\\left(\\frac{1}{\\delta}\\right)\\right)} = 1\n$$\n这表明，从由 VC 维衡量的信息论样本复杂度的角度来看，这两个类是无法区分的。\n\n### 3. 关于噪声敏感性的定性讨论\n\n相同的 VC 维和由此产生的样本复杂度界掩盖了这两个类之间深刻的结构差异，这些差异在考虑噪声时变得显而易见。\n\n**输入中的独立随机位翻转：**\n考虑一个输入 $x$ 和一个由 $S$ 参数化的假设。令 $x'$ 是通过翻转单个位 $x_k$ 形成的。\n\n- 对于**单调合取 ($h_S$)**：输出 $h_S(x) = \\bigwedge_{i \\in S} x_i$ 仅在 $k \\in S$ 时受翻转 $x_k$ 的影响。如果 $k \\notin S$，则该函数对翻转完全不敏感。如果 $k \\in S$，函数的输出可能会翻转。一个关键特征是其非均匀敏感性。对于一个输入 $x$ 使得 $h_S(x)=1$（即对所有 $i \\in S$ 都有 $x_i=1$），任何这样的 $x_k$ 单个翻转为 $0$ 都会保证输出变为 $0$。然而，对于一个输入使得 $h_S(x)=0$ 的情况，单个位翻转可能会也可能不会改变输出，这取决于是否有其他位 $x_j$（对于 $j \\in S$）也为 $0$。该函数对 $S$ 之外的属性翻转具有鲁棒性。\n\n- 对于**奇偶性 ($g_S$)**：输出 $g_S(x) = \\bigoplus_{i \\in S} x_i$ 也仅在 $k \\in S$ 时受翻转 $x_k$ 的影响。然而，其效果截然不同。如果 $k \\in S$，将 $x_k$ 翻转为 $1-x_k$ *总是*会翻转函数的输出，因为 $g_S(x') = g_S(x) \\oplus 1$。这意味着对于*每个*输入点 $x$，奇偶性函数对任何相关属性的位翻转都具有最大敏感性。函数的输出随着输入的变化而迅速振荡。\n\n**随机分类噪声 (RCN)：**\n这是指真实标签 $y$ 以某个概率 $\\eta$ 翻转为 $1-y$ 的情况。\n\n- **单调合取**的结构意味着一种平滑性。如果一个输入 $x$ 被 $h_S$ 标记为 $1$，那么任何按位大于或等于 $x$ 的输入 $x'$（即 $x'_i \\ge x_i, \\forall i$）也将被标记为 $1$。这意味着在汉明距离上相近的点很可能共享相同的标签。学习算法可以利用这种局部相关性；例如，它可以将在标签一致的邻居“海洋”中带有意外标签的孤立点视为可能被噪声破坏的点。\n\n- **奇偶性**函数的结构没有这种平滑性。任何汉明距离为 1 的两个点（如果差异位在 $S$ 中）都具有相反的标签。标签没有局部相关性。一个点及其最近邻提供了棋盘状的标签模式。在存在 RCN 的情况下，学习算法没有局部结构可以利用来区分真实标签和翻转的标签。任何标记在局部看起来都是合理的。这使得在不可知（容错）设置下学习奇偶性在计算上非常困难，这一事实并未被仅衡量信息论复杂度的 VC 维所捕捉。存在用于在噪声下学习合取的有效算法，而已知在噪声下学习奇偶性对于高效算法而言是计算上困难的。", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\nd & 1\n\\end{pmatrix}\n}\n$$", "id": "3138549"}, {"introduction": "VC维是分类问题的关键，但它不适用于输出实数值的回归任务。我们需要更丰富的工具，例如胖打散维度（fat-shattering dimension），来衡量不同尺度下的复杂性。本练习将在强大的再生核希尔伯特空间 (RKHS) 框架内探索此概念，将模型容量与具体的回归泛化保证联系起来。[@problem_id:3138564]", "problem": "考虑一个定义在输入空间上的实值假设类，该输入空间具有一个从再生核希尔伯特空间 (Reproducing Kernel Hilbert Space, RKHS) 引出的特征映射。令输入域为 $\\mathcal{X} \\subset \\mathbb{R}^{d}$，其上装备了高斯核 $k(x,x') = \\exp\\!\\big(-\\|x-x'\\|^{2}/(2\\sigma^{2})\\big)$，并令 $\\phi:\\mathcal{X}\\to\\mathcal{H}$ 为进入 RKHS $\\mathcal{H}$ 的相关特征映射。已知高斯核是通用的，特别地，对于任何不同的点，其格拉姆矩阵 (Gram matrix) 是严格正定的。假设对于所有 $x \\in \\mathcal{X}$，都有 $\\|\\phi(x)\\|_{\\mathcal{H}} = \\sqrt{k(x,x)} = 1$。定义假设类\n$$\nH_{B} \\;=\\; \\{\\, f_{w}(x) = \\langle w, \\phi(x) \\rangle_{\\mathcal{H}} \\;:\\; \\|w\\|_{\\mathcal{H}} \\leq B \\,\\},\n$$\n其中 $B > 0$ 是一个固定的半径。\n\n你将研究基于核心定义和经过充分检验的结果的三个方面：Vapnik–Chervonenkis (VC) 维、尺度为 $\\gamma$ 的胖打散 (fat-shattering)，以及在绝对损失下的回归泛化率。对于二元分类，考虑符号类 $\\mathrm{sign}(H_{B}) = \\{\\, \\mathrm{sign}(f_{w}(x)) : f_{w}\\in H_{B} \\,\\}$。对于回归，假设输出 $y \\in [-1,1]$ 并使用绝对损失 $\\ell(y, f(x)) = |\\,y - f(x)\\,|$。\n\n任务：\n- 利用高斯 RKHS 的通用性和不同点的格拉姆矩阵的正定性，构建 $H_{B}$，使得二元类 $\\mathrm{sign}(H_{B})$ 具有无穷大的 Vapnik–Chervonenkis (VC) 维，而实值类 $H_{B}$ 在任何固定尺度 $\\gamma > 0$ 下具有有限的胖打散维。\n- 从尺度为 $\\gamma$ 的胖打散定义以及希尔伯特空间中线性泛函的标准范数-间隔关系出发，推导出 $H_{B}$ 在尺度 $\\gamma$ 下的胖打散维的一个显式上界，该上界用 $B$ 和 $\\gamma$ 表示。\n- 从经验风险和期望风险的定义、对称化原理以及线性函数类的 Rademacher 复杂度出发，推导出在绝对损失下对 $H_{B}$ 的期望一致泛化间隙，用样本量 $n$ 和半径 $B$ 表示。然后，确定保证该期望一致间隙至多为 $\\gamma$ 的最小样本量 $n$。\n\n你的最终答案必须是关于最小样本量 $n$ 的一个单一闭式表达式，用 $B$ 和 $\\gamma$ 表示。无需单位。如果你需要四舍五入，请说明你的舍入依据的有效数字；然而，预期的答案是精确的，不需要四舍五入。", "solution": "在尝试给出解答之前，首先对问题陈述进行验证。\n\n### 步骤 1：提取已知条件\n- 输入空间：$\\mathcal{X} \\subset \\mathbb{R}^{d}$。\n- 核函数：高斯核，$k(x,x') = \\exp(-\\|x-x'\\|^{2}/(2\\sigma^{2}))$。\n- 特征映射：$\\phi:\\mathcal{X}\\to\\mathcal{H}$，映射到一个再生核希尔伯特空间 (RKHS) $\\mathcal{H}$。\n- 核的性质：通用；对于不同的点，格拉姆矩阵是严格正定的。\n- 归一化：对于所有 $x \\in \\mathcal{X}$，$\\|\\phi(x)\\|_{\\mathcal{H}} = \\sqrt{k(x,x)} = 1$。\n- 假设类：$H_{B} = \\{\\, f_{w}(x) = \\langle w, \\phi(x) \\rangle_{\\mathcal{H}} \\;:\\; \\|w\\|_{\\mathcal{H}} \\leq B \\,\\}$。\n- 参数：$B > 0$。\n- 二元假设类：$\\mathrm{sign}(H_{B}) = \\{\\, \\mathrm{sign}(f_{w}(x)) : f_{w}\\in H_{B} \\,\\}$。\n- 回归目标：$y \\in [-1,1]$。\n- 损失函数：绝对损失，$\\ell(y, f(x)) = |y - f(x)|$。\n\n### 步骤 2：使用提取的已知条件进行验证\n该问题在统计学习理论的既定框架内是良定义的。它涉及 RKHS、核方法、Vapnik–Chervonenkis (VC) 维、胖打散维和 Rademacher 复杂度等标准概念。所有已知条件都是一致的，并且足以进行所需的推导。\n- **科学基础：** 问题基于机器学习理论的基本原理。高斯核和 RKHS 的性质陈述正确。\n- **良态的：** 任务明确，并能导出一个唯一的、可推导的解。\n- **客观性：** 问题以精确的数学语言陈述，没有歧义或主观性。\n- **完整性与一致性：** 提供的信息是自洽的，没有矛盾。条件 $\\|\\phi(x)\\|_{\\mathcal{H}} = 1$ 与高斯核的定义一致，因为 $k(x,x)=\\exp(-\\|x-x\\|^2/(2\\sigma^2)) = \\exp(0) = 1$。\n\n### 步骤 3：结论与行动\n问题被判定为**有效**。将提供一个完整的、带推理过程的解答。\n\n***\n\n### 解答推导\n\n解答过程将按问题陈述中提出的三个任务进行。\n\n**任务 1：VC 维与胖打散维分析**\n\n第一个任务是解释为什么二元类 $\\mathrm{sign}(H_{B})$ 具有无限的 VC 维，而实值类 $H_{B}$ 对任何固定尺度 $\\gamma > 0$ 具有有限的胖打散维。\n\n*   **$\\mathrm{sign}(H_{B})$ 的 Vapnik–Chervonenkis 维**：\n    一个假设类的 VC 维是该类能打散的点的最大数量 $n$。要打散一个点集 $\\{x_1, \\dots, x_n\\}$，对于任何标签 $(y_1, \\dots, y_n) \\in \\{-1, 1\\}^n$，都必须存在一个函数 $f_w \\in H_B$ 使得对所有 $i \\in \\{1, \\dots, n\\}$ 都有 $\\mathrm{sign}(f_w(x_i)) = y_i$。\n    问题陈述指出，对于任何不同点的集合，其格拉姆矩阵都是严格正定的。这意味着对于任何 $n$ 个不同点的集合 $\\{x_1, \\dots, x_n\\}$，特征向量 $\\{\\phi(x_1), \\dots, \\phi(x_n)\\}$ 在 $\\mathcal{H}$ 中是线性无关的。\n    由于这种线性无关性，对于任何标签 $(y_1, \\dots, y_n)$，我们可以找到一个向量 $w_0 \\in \\mathcal{H}$，使得对所有 $i$ 都满足插值条件 $\\langle w_0, \\phi(x_i) \\rangle_{\\mathcal{H}} = y_i$。这个 $w_0$ 可以构造为特征向量的线性组合，$w_0 = \\sum_{j=1}^n c_j \\phi(x_j)$，其中系数 $c_j$ 通过求解一个线性系统得到。\n    由于 $y_i \\in \\{-1, 1\\}$，$w_0$ 不可能是零向量，所以 $\\|w_0\\|_{\\mathcal{H}} > 0$。我们定义一个新向量 $w = \\alpha w_0$，其中标量 $\\alpha > 0$。函数为 $f_w(x) = \\langle \\alpha w_0, \\phi(x) \\rangle_{\\mathcal{H}} = \\alpha \\langle w_0, \\phi(x) \\rangle_{\\mathcal{H}}$。\n    函数在 $x_i$ 处的输出符号为 $\\mathrm{sign}(f_w(x_i)) = \\mathrm{sign}(\\alpha y_i) = y_i$。为确保 $f_w \\in H_B$，我们必须满足范数约束 $\\|w\\|_{\\mathcal{H}} \\le B$。这意味着 $\\|\\alpha w_0\\|_{\\mathcal{H}} = \\alpha \\|w_0\\|_{\\mathcal{H}} \\le B$。由于 $B>0$ 且 $\\|w_0\\|_{\\mathcal{H}}>0$，我们总可以取一个足够小的 $\\alpha > 0$，例如 $\\alpha = B/\\|w_0\\|_{\\mathcal{H}}$，来满足这个约束。\n    这个过程对任何有限数量的点 $n$ 都有效。因此，任何有限点集都可以被 $\\mathrm{sign}(H_{B})$ 打散，这意味着其 VC 维是无限的。\n\n*   **$H_{B}$ 的胖打散维**：\n    由于范数约束 $\\|w\\|_{\\mathcal{H}} \\le B$，胖打散维是有限的。如果一个点集能被 $\\gamma$-打散，意味着函数值可以被推高或推低超过某个阈值至少 $\\gamma$。对 $w$ 的范数约束限制了函数 $\\langle w, \\phi(x) \\rangle_{\\mathcal{H}}$ 的变化范围，这反过来又限制了它 $\\gamma$-打散任意大集合的能力。下一个任务通过推导一个显式上界来提供这一有限性的定量证明。\n\n**任务 2：胖打散维的上界**\n\n令 $\\mathrm{fat}_{H_B}(\\gamma)$ 为 $H_B$ 在尺度 $\\gamma$ 下的胖打散维。设 $\\{x_1, \\dots, x_n\\}$ 是一个被 $H_B$ $\\gamma$-打散的 $n$ 个点的集合。根据定义，存在实数（见证）$r_1, \\dots, r_n$，使得对于任何二元向量 $b = (b_1, \\dots, b_n) \\in \\{-1, 1\\}^n$，都存在一个函数 $f_{w_b} \\in H_B$（即 $\\|w_b\\|_{\\mathcal{H}} \\le B$）满足对所有 $i \\in \\{1, \\dots, n\\}$：\n$b_i (f_{w_b}(x_i) - r_i) \\ge \\gamma$。\n这个不等式对向量 $-b \\in \\{-1, 1\\}^n$ 也成立，所以存在一个 $w_{-b}$ 且 $\\|w_{-b}\\|_{\\mathcal{H}} \\le B$ 使得：\n$(-b_i) (f_{w_{-b}}(x_i) - r_i) \\ge \\gamma$。\n重写第二个不等式得到 $b_i (r_i - f_{w_{-b}}(x_i)) \\ge \\gamma$。\n将两个不等式相加得到：\n$b_i (f_{w_b}(x_i) - r_i) + b_i (r_i - f_{w_{-b}}(x_i)) \\ge 2\\gamma$。\n$b_i (f_{w_b}(x_i) - f_{w_{-b}}(x_i)) \\ge 2\\gamma$。\n代入 $f_w(x) = \\langle w, \\phi(x) \\rangle_{\\mathcal{H}}$：\n$b_i \\langle w_b - w_{-b}, \\phi(x_i) \\rangle_{\\mathcal{H}} \\ge 2\\gamma$。\n令 $v_b = w_b - w_{-b}$。根据三角不等式，$\\|v_b\\|_{\\mathcal{H}} \\le \\|w_b\\|_{\\mathcal{H}} + \\|w_{-b}\\|_{\\mathcal{H}} \\le B + B = 2B$。\n将不等式对 $i=1, \\dots, n$ 求和：\n$\\sum_{i=1}^n b_i \\langle v_b, \\phi(x_i) \\rangle_{\\mathcal{H}} \\ge \\sum_{i=1}^n 2\\gamma = 2n\\gamma$。\n根据内积的线性性质：\n$\\langle v_b, \\sum_{i=1}^n b_i \\phi(x_i) \\rangle_{\\mathcal{H}} \\ge 2n\\gamma$。\n应用 Cauchy-Schwarz 不等式：\n$\\|v_b\\|_{\\mathcal{H}} \\left\\| \\sum_{i=1}^n b_i \\phi(x_i) \\right\\|_{\\mathcal{H}} \\ge \\langle v_b, \\sum_{i=1}^n b_i \\phi(x_i) \\rangle_{\\mathcal{H}} \\ge 2n\\gamma$。\n使用 $\\|v_b\\|_{\\mathcal{H}}$ 的界：\n$2B \\left\\| \\sum_{i=1}^n b_i \\phi(x_i) \\right\\|_{\\mathcal{H}} \\ge 2n\\gamma$。\n$B \\left\\| \\sum_{i=1}^n b_i \\phi(x_i) \\right\\|_{\\mathcal{H}} \\ge n\\gamma$。\n这个不等式对任何 $b \\in \\{-1, 1\\}^n$ 的选择都成立。我们可以对两边平方，并对均匀随机选择的 $b$（其中每个 $b_i$ 是独立的 Rademacher 随机变量）取期望：\n$\\mathbb{E}_b \\left[ B^2 \\left\\| \\sum_{i=1}^n b_i \\phi(x_i) \\right\\|_{\\mathcal{H}}^2 \\right] \\ge n^2\\gamma^2$。\n$B^2 \\mathbb{E}_b \\left[ \\sum_{i=1}^n \\sum_{j=1}^n b_i b_j \\langle \\phi(x_i), \\phi(x_j) \\rangle_{\\mathcal{H}} \\right] \\ge n^2\\gamma^2$。\n根据期望的线性性质，并使用 $\\mathbb{E}[b_i b_j] = \\delta_{ij}$ (Kronecker delta)：\n$B^2 \\sum_{i=1}^n \\sum_{j=1}^n \\delta_{ij} \\langle \\phi(x_i), \\phi(x_j) \\rangle_{\\mathcal{H}} \\ge n^2\\gamma^2$。\n$B^2 \\sum_{i=1}^n \\langle \\phi(x_i), \\phi(x_i) \\rangle_{\\mathcal{H}} \\ge n^2\\gamma^2$。\n$B^2 \\sum_{i=1}^n \\|\\phi(x_i)\\|_{\\mathcal{H}}^2 \\ge n^2\\gamma^2$。\n使用给定的条件 $\\|\\phi(x)\\|_{\\mathcal{H}} = 1$：\n$B^2 n \\ge n^2\\gamma^2$。\n因为 $n>0$，我们可以除以 $n$：$B^2 \\ge n\\gamma^2$。\n解出 $n$，我们得到任何 $\\gamma$-打散集的大小的上界：\n$n \\le \\frac{B^2}{\\gamma^2}$。\n因此，胖打散维是有限的，且其上界为 $\\mathrm{fat}_{H_B}(\\gamma) \\le \\frac{B^2}{\\gamma^2}$。\n\n**任务 3：泛化间隙与所需样本量**\n\n目标是推导绝对损失下的期望一致泛化间隙，并找到保证该间隙至多为 $\\gamma$ 的最小样本量 $n$。\n\n对于假设类 $H_B$ 和损失函数 $\\ell$，期望一致泛化间隙为 $\\mathbb{E}_S[\\sup_{f \\in H_B} |R(f) - \\hat{R}_S(f)|]$，其中 $S$ 是大小为 $n$ 的样本，$R(f) = \\mathbb{E}_{X,Y}[\\ell(Y, f(X))]$ 是真实风险，$\\hat{R}_S(f) = \\frac{1}{n}\\sum_{i=1}^n \\ell(y_i, f(x_i))$ 是经验风险。\n\n1.  **对称化：** 一个标准结果将此间隙限制为损失类 $\\mathcal{L}_{H_B} = \\{(x,y) \\mapsto |y-f(x)| : f \\in H_B\\}$ 的 Rademacher 复杂度的两倍。\n    $\\mathbb{E}_S[\\sup_{f \\in H_B} |R(f) - \\hat{R}_S(f)|] \\le 2 \\mathfrak{R}_n(\\mathcal{L}_{H_B})$。\n    损失类的 Rademacher 复杂度为 $\\mathfrak{R}_n(\\mathcal{L}_{H_B}) = \\mathbb{E}_{S, \\epsilon} \\left[ \\sup_{f \\in H_B} \\frac{1}{n} \\sum_{i=1}^n \\epsilon_i |y_i - f(x_i)| \\right]$。\n\n2.  **压缩原理：** 绝对损失函数 $\\ell(y, a) = |y-a|$ 关于其第二个参数 $a$ 是 1-Lipschitz 的。Ledoux-Talagrand 压缩原理指出，对于一个 $L$-Lipschitz 函数 $\\psi$，复合类 $\\psi \\circ H_B$ 的 Rademacher 复杂度最多是 $H_B$ 的 Rademacher 复杂度的 $L$ 倍。此处 $L=1$。\n    $\\mathfrak{R}_n(\\mathcal{L}_{H_B}) \\le 1 \\cdot \\mathfrak{R}_n(H_B)$。\n\n3.  **$H_B$ 的 Rademacher 复杂度：** 现在我们计算原始假设类 $H_B$ 的 Rademacher 复杂度。\n    $\\mathfrak{R}_n(H_B) = \\mathbb{E}_{S, \\epsilon} \\left[ \\sup_{f \\in H_B} \\frac{1}{n} \\sum_{i=1}^n \\epsilon_i f(x_i) \\right]$。\n    代入 $f(x_i) = \\langle w, \\phi(x_i) \\rangle_{\\mathcal{H}}$ 且 $\\|w\\|_{\\mathcal{H}} \\le B$：\n    $\\mathfrak{R}_n(H_B) = \\mathbb{E}_{S, \\epsilon} \\left[ \\sup_{\\|w\\|_{\\mathcal{H}} \\le B} \\frac{1}{n} \\sum_{i=1}^n \\epsilon_i \\langle w, \\phi(x_i) \\rangle_{\\mathcal{H}} \\right]$。\n    $\\mathfrak{R}_n(H_B) = \\mathbb{E}_{S, \\epsilon} \\left[ \\frac{1}{n} \\sup_{\\|w\\|_{\\mathcal{H}} \\le B} \\left\\langle w, \\sum_{i=1}^n \\epsilon_i \\phi(x_i) \\right\\rangle_{\\mathcal{H}} \\right]$。\n    在球 $\\|w\\|_{\\mathcal{H}} \\le B$ 上，$\\langle w, v \\rangle$ 的上确界是 $B\\|v\\|_{\\mathcal{H}}$。因此：\n    $\\mathfrak{R}_n(H_B) = \\mathbb{E}_{S, \\epsilon} \\left[ \\frac{B}{n} \\left\\| \\sum_{i=1}^n \\epsilon_i \\phi(x_i) \\right\\|_{\\mathcal{H}} \\right]$。\n    使用 Jensen 不等式 ($\\mathbb{E}[X] \\le \\sqrt{\\mathbb{E}[X^2]}$)：\n    $\\mathfrak{R}_n(H_B) \\le \\frac{B}{n} \\sqrt{\\mathbb{E}_{S, \\epsilon} \\left[ \\left\\| \\sum_{i=1}^n \\epsilon_i \\phi(x_i) \\right\\|_{\\mathcal{H}}^2 \\right]}$。\n    平方根内的项是 $\\mathbb{E}_{S, \\epsilon} \\left[ \\sum_{i,j} \\epsilon_i \\epsilon_j \\langle \\phi(x_i), \\phi(x_j) \\rangle_{\\mathcal{H}} \\right]$。首先对 $\\epsilon$ 取期望（对于固定的样本 $S$）并使用 $\\mathbb{E}_\\epsilon[\\epsilon_i\\epsilon_j]=\\delta_{ij}$：\n    $\\mathbb{E}_{\\epsilon} \\left[ \\dots \\right] = \\sum_{i=1}^n \\langle \\phi(x_i), \\phi(x_i) \\rangle_{\\mathcal{H}} = \\sum_{i=1}^n \\|\\phi(x_i)\\|_{\\mathcal{H}}^2$。\n    使用给定的条件 $\\|\\phi(x_i)\\|_{\\mathcal{H}}=1$，这个和等于 $n$。该结果与样本 $S$ 无关，因此对 $S$ 取期望不会改变它。\n    $\\mathfrak{R}_n(H_B) \\le \\frac{B}{n} \\sqrt{n} = \\frac{B}{\\sqrt{n}}$。\n\n4.  **最终界与样本量：** 综合以上步骤，期望一致泛化间隙的界为：\n    $\\mathbb{E}_S[\\sup_{f \\in H_B} |R(f) - \\hat{R}_S(f)|] \\le 2 \\mathfrak{R}_n(H_B) \\le \\frac{2B}{\\sqrt{n}}$。\n    为保证该间隙至多为 $\\gamma$，我们设置上界小于或等于 $\\gamma$：\n    $\\frac{2B}{\\sqrt{n}} \\le \\gamma$。\n    解出 $n$：\n    $\\sqrt{n} \\ge \\frac{2B}{\\gamma} \\implies n \\ge \\left(\\frac{2B}{\\gamma}\\right)^2 = \\frac{4B^2}{\\gamma^2}$。\n    保证这个界的最小样本量 $n$ 是 $\\frac{4B^2}{\\gamma^2}$。", "answer": "$$\\boxed{\\frac{4B^{2}}{\\gamma^{2}}}$$", "id": "3138564"}, {"introduction": "泛化能力不仅取决于假设类，学习算法本身也扮演着至关重要的角色。本练习将我们的焦点转移到算法稳定性上，这是一个强有力的视角，用以理解梯度下降中的早停等策略如何充当隐式正则化。你将推导出训练步数如何直接控制模型的稳定性，进而约束其泛化误差。[@problem_id:3138528]", "problem": "令 $S = (z_{1}, \\dots, z_{n})$ 是一个大小为 $n$ 的数据集，其样本从样本空间 $\\mathcal{Z}$ 上的一个未知分布中独立同分布地抽取。考虑一个参数为 $w \\in \\mathbb{R}^{d}$ 的参数化预测器，通过最小化经验风险 $F_{S}(w) = \\frac{1}{n} \\sum_{i=1}^{n} f(w; z_{i})$ 进行训练，其中损失 $f(w; z)$ 满足以下性质：\n- 对于每一个 $z \\in \\mathcal{Z}$，函数 $w \\mapsto f(w; z)$ 是凸函数且可微。\n- 存在一个常数 $G > 0$，使得对于所有 $w, w' \\in \\mathbb{R}^{d}$ 和所有 $z \\in \\mathcal{Z}$，都有 $|f(w; z) - f(w'; z)| \\leq G \\|w - w'\\|$，即 $f(\\cdot; z)$ 对于 $w$ 在 $z$ 上是一致 $G$-利普希茨的。\n- 对于某个 $L > 0$，经验风险 $F_{S}(w)$ 对于 $w$ 是凸函数且 $L$-平滑的，这意味着其梯度是 $L$-利普希茨的：对于所有 $w, w' \\in \\mathbb{R}^{d}$，都有 $\\|\\nabla F_{S}(w) - \\nabla F_{S}(w')\\| \\leq L \\|w - w'\\|$。\n\n使用确定性梯度下降进行训练，步长固定为 $\\eta \\in (0, 2/L]$，从一个与数据无关的初始值 $w_{0} = 0$ 开始，生成迭代序列 $w_{t+1}(S) = w_{t}(S) - \\eta \\nabla F_{S}(w_{t}(S))$，其中 $t = 0, 1, 2, \\dots$。将经过 $T$ 次迭代后的早停解定义为 $w_{T}(S)$。\n\n令 $S'$ 为任何与 $S$ 仅有一个样本不同的数据集。如果 $\\sup_{z \\in \\mathcal{Z}} |f(w_{T}(S); z) - f(w_{T}(S'); z)| \\leq \\beta_{T}$，则称学习算法在第 $T$ 次迭代时具有参数为 $\\beta_{T}$ 的一致稳定性。\n仅使用上述假设，推导在第 $T$ 次迭代时早停解的期望泛化误差 $\\mathbb{E}_{S}\\big[ F(w_{T}(S)) - F_{S}(w_{T}(S)) \\big]$ 的一个闭式上界，该上界用 $n$、$G$、$\\eta$ 和 $T$ 表示，其中 $F(w) = \\mathbb{E}_{z}[f(w; z)]$ 表示总体风险。你的最终答案必须是单一的解析表达式。不需要四舍五入。", "solution": "用户想要推导期望泛化误差的一个上界。\n\n### 步骤 1：问题验证\n\n**1.1. 提取已知条件**\n- 数据集：$S = (z_{1}, \\dots, z_{n})$，从 $\\mathcal{Z}$ 上的一个未知分布中独立同分布 (i.i.d.) 抽取。\n- 预测器参数：$w \\in \\mathbb{R}^{d}$。\n- 经验风险：$F_{S}(w) = \\frac{1}{n} \\sum_{i=1}^{n} f(w; z_{i})$。\n- 总体风险：$F(w) = \\mathbb{E}_{z}[f(w; z)]$。\n- 损失函数性质：\n    - 对于每一个 $z \\in \\mathcal{Z}$，$w \\mapsto f(w; z)$ 是凸函数且可微。\n    - 存在一个常数 $G > 0$，使得 $f(\\cdot; z)$ 对于 $w$ 在 $z$ 上是一致 $G$-利普希茨的：对于所有 $w, w' \\in \\mathbb{R}^{d}$ 和所有 $z \\in \\mathcal{Z}$，都有 $|f(w; z) - f(w'; z)| \\leq G \\|w - w'\\|$。\n- 经验风险性质：\n    - $F_{S}(w)$ 是凸函数。\n    - 对于某个 $L > 0$，$F_{S}(w)$ 对于 $w$ 是 $L$-平滑的，即 $\\|\\nabla F_{S}(w) - \\nabla F_{S}(w')\\| \\leq L \\|w - w'\\|$。\n- 训练算法：确定性梯度下降。\n    - 初始化：$w_{0} = 0$。\n    - 迭代序列：$w_{t+1}(S) = w_{t}(S) - \\eta \\nabla F_{S}(w_{t}(S))$，其中 $t = 0, 1, 2, \\dots$。\n    - 步长：$\\eta \\in (0, 2/L]$。\n    - 早停解：$w_{T}(S)$。\n- 稳定性：如果对于任何与 $S$ 仅相差一个样本的数据集 $S'$，都有 $\\sup_{z \\in \\mathcal{Z}} |f(w_{T}(S); z) - f(w_{T}(S'); z)| \\leq \\beta_{T}$，则该算法具有参数为 $\\beta_{T}$ 的一致稳定性。\n- 目标：推导期望泛化误差 $\\mathbb{E}_{S}[ F(w_{T}(S)) - F_{S}(w_{T}(S)) ]$ 的一个闭式上界，该上界用 $n$、$G$、$\\eta$ 和 $T$ 表示。\n\n**1.2. 已知条件的验证**\n该问题在统计学习理论领域内是适定的，并具有科学依据。这些假设（凸性、损失函数的利普希茨连续性、经验风险的平滑性）和算法（梯度下降）都是标准的。一致稳定性的概念及其与泛化性的联系是学习理论中的一个基本课题。问题要求一个依赖于 $n, G, \\eta, T$ 但不依赖于 $L$ 的上界。参数 $L$ 用于定义步长 $\\eta$ 的有效范围，并且如下所示，最终的上界不需要其显式值，因为它的作用是确保梯度下降（GD）更新步骤的一个关键性质（非扩张性）。所有术语都定义清晰，整个设置在内部是一致的。\n\n**1.3. 结论**\n问题是有效的。我们开始求解。\n\n### 步骤 2：求解推导\n\n推导过程主要分为四个步骤。首先，我们将期望泛化误差与一致稳定性参数 $\\beta_T$ 联系起来。其次，我们将 $\\beta_T$ 与在相邻数据集上训练的参数之间的几何距离 $\\|w_T(S) - w_T(S')\\|$ 联系起来。第三，我们推导这个距离的递推关系。最后，我们求解该递推关系并组合结果以获得最终的上界。\n\n**第一部分：通过稳定性约束泛化误差**\n\n令 $S = (z_1, \\dots, z_n)$ 为训练集。假设 $w_T(S)$ 的期望泛化误差由 $\\mathbb{E}_{S}[F(w_T(S)) - F_S(w_T(S))]$ 给出。\n根据 $F(w)$ 和 $F_S(w)$ 的定义：\n$$\n\\mathbb{E}_{S}[F(w_T(S)) - F_S(w_T(S))] = \\mathbb{E}_{S}\\left[ \\mathbb{E}_{z'}[f(w_T(S); z')] - \\frac{1}{n}\\sum_{i=1}^{n}f(w_T(S); z_i) \\right]\n$$\n其中 $z'$ 是一个与 $z_i$ 同分布且独立于 $S$ 的随机变量。根据期望的线性性质以及 $z_i$ 是独立同分布的，我们可以将其写为：\n$$\n\\frac{1}{n}\\sum_{i=1}^{n} \\mathbb{E}_{S}\\left[ \\mathbb{E}_{z'_i}[f(w_T(S); z'_i)] - f(w_T(S); z_i) \\right]\n$$\n其中每个 $z'_i$ 是 $z_i$ 的一个独立副本。令 $S^{(i)}$ 是数据集 $S$ 中第 $i$ 个样本 $z_i$ 被替换为 $z'_i$ 后的数据集。$(S, z'_i)$ 的联合分布与 $(S^{(i)}, z_i)$ 的联合分布相同。因此，$\\mathbb{E}_{S, z'_i}[f(w_T(S); z_i)] = \\mathbb{E}_{S^{(i)}, z_i}[f(w_T(S^{(i)}); z'_i)]$。更简单地说，对于任何函数 $g$，都有 $\\mathbb{E}_{S, z'_i}[g(S, z_i)] = \\mathbb{E}_{S^{(i)}, z_i}[g(S^{(i)}, z'_i)]$。\n期望可以重写为：\n$$\n\\frac{1}{n}\\sum_{i=1}^{n} \\mathbb{E}_{S, z'_i}\\left[ f(w_T(S); z'_i) - f(w_T(S); z_i) \\right]\n$$\n我们在期望内部加上和减去 $f(w_T(S^{(i)}); z'_i)$：\n$$\n= \\frac{1}{n}\\sum_{i=1}^{n} \\mathbb{E}_{S, z'_i}\\left[ f(w_T(S); z'_i) - f(w_T(S^{(i)}); z'_i) + f(w_T(S^{(i)}); z'_i) - f(w_T(S); z_i) \\right]\n$$\n最后两项的期望相互抵消：$\\mathbb{E}_{S, z'_i}[f(w_T(S^{(i)}); z'_i)] = \\mathbb{E}_{S, z'_i}[f(w_T(S); z_i)]$。\n我们剩下：\n$$\n\\mathbb{E}_{S}[F(w_T(S)) - F_S(w_T(S))] = \\frac{1}{n}\\sum_{i=1}^{n} \\mathbb{E}_{S, z'_i}\\left[ f(w_T(S); z'_i) - f(w_T(S^{(i)}); z'_i) \\right]\n$$\n根据一致稳定性的定义，对于任何 $z$，都有 $|f(w_T(S); z) - f(w_T(S^{(i)}); z)| \\leq \\beta_T$。因此：\n$$\n\\mathbb{E}_{S}[F(w_T(S)) - F_S(w_T(S))] \\leq \\frac{1}{n}\\sum_{i=1}^{n} \\mathbb{E}_{S, z'_i}\\left[ \\beta_T \\right] = \\beta_T\n$$\n所以，问题简化为寻找 $\\beta_T$ 的一个上界。\n\n**第二部分：用参数距离约束稳定性**\n\n令 $S$ 和 $S'$ 是两个仅相差一个样本的数据集。根据一致稳定性的定义，$\\beta_T = \\sup_{S, S', z} |f(w_T(S); z) - f(w_T(S'); z)|$。\n损失函数 $f(\\cdot; z)$ 对于其第一个参数 $w$ 是 $G$-利普希茨的。这意味着：\n$$\n|f(w_T(S); z) - f(w_T(S'); z)| \\leq G \\|w_T(S) - w_T(S')\\|\n$$\n这个不等式对所有 $z$ 都成立，所以对 $z$ 取上确界，我们得到：\n$$\n\\sup_{z \\in \\mathcal{Z}} |f(w_T(S); z) - f(w_T(S'); z)| \\leq G \\|w_T(S) - w_T(S')\\|\n$$\n因此，$\\beta_T \\leq G \\sup_{S, S'} \\|w_T(S) - w_T(S')\\|$，其中上确界是对所有相邻数据集取的。我们现在的目标是约束 $\\|w_T(S) - w_T(S')\\|$。\n\n**第三部分：参数距离的递推关系**\n\n令 $\\delta_t = \\|w_t(S) - w_t(S')\\|$。初始条件是 $\\delta_0 = \\|w_0 - w_0\\| = \\|0 - 0\\| = 0$。\n梯度下降的迭代公式为：\n$w_{t+1}(S) = w_t(S) - \\eta \\nabla F_S(w_t(S))$\n$w_{t+1}(S') = w_t(S') - \\eta \\nabla F_{S'}(w_t(S'))$\n在第 $t+1$ 步的距离是：\n$$\n\\delta_{t+1} = \\|w_t(S) - w_t(S') - \\eta ( \\nabla F_S(w_t(S)) - \\nabla F_{S'}(w_t(S')) ) \\|\n$$\n我们在范数内加上和减去 $\\eta \\nabla F_S(w_t(S'))$ 并使用三角不等式：\n$$\n\\delta_{t+1} \\leq \\| w_t(S) - \\eta \\nabla F_S(w_t(S)) - (w_t(S') - \\eta \\nabla F_S(w_t(S'))) \\| + \\| \\eta (\\nabla F_S(w_t(S')) - \\nabla F_{S'}(w_t(S'))) \\|\n$$\n我们来分析第一项。对于一个凸的、$L$-平滑的函数 $F_S$ 和步长 $\\eta \\in (0, 2/L]$，算子 $T(w) = w - \\eta \\nabla F_S(w)$ 是非扩张的。这意味着 $\\|T(w) - T(w')\\| \\leq \\|w - w'\\|$。\n为了证明这一点，我们使用凸 $L$-平滑函数的梯度的共强制性 (co-coercivity)：$\\langle \\nabla F_S(w) - \\nabla F_S(w'), w - w' \\rangle \\geq \\frac{1}{L}\\|\\nabla F_S(w) - \\nabla F_S(w')\\|^2$。\n$$\n\\|T(w) - T(w')\\|^2 = \\|(w-w') - \\eta(\\nabla F_S(w) - \\nabla F_S(w'))\\|^2 \\\\\n= \\|w-w'\\|^2 - 2\\eta\\langle w-w', \\nabla F_S(w) - \\nabla F_S(w') \\rangle + \\eta^2\\|\\nabla F_S(w) - \\nabla F_S(w')\\|^2 \\\\\n\\leq \\|w-w'\\|^2 - \\frac{2\\eta}{L}\\|\\nabla F_S(w) - \\nabla F_S(w')\\|^2 + \\eta^2\\|\\nabla F_S(w) - \\nabla F_S(w')\\|^2 \\\\\n= \\|w-w'\\|^2 + \\eta(\\eta - \\frac{2}{L})\\|\\nabla F_S(w) - \\nabla F_S(w')\\|^2\n$$\n由于 $\\eta \\in (0, 2/L]$，项 $\\eta(\\eta - 2/L) \\leq 0$。因此，$\\|T(w) - T(w')\\|^2 \\leq \\|w-w'\\|^2$，这证明了非扩张性。\n将此性质应用于我们关于 $\\delta_{t+1}$ 的不等式的第一项：\n$$\n\\| w_t(S) - \\eta \\nabla F_S(w_t(S)) - (w_t(S') - \\eta \\nabla F_S(w_t(S'))) \\| \\leq \\|w_t(S) - w_t(S')\\| = \\delta_t\n$$\n这简化了 $\\delta_{t+1}$ 的递推关系：\n$$\n\\delta_{t+1} \\leq \\delta_t + \\eta \\|\\nabla F_S(w_t(S')) - \\nabla F_{S'}(w_t(S')) \\|\n$$\n现在我们约束梯度差项。假设 $S'$ 与 $S$ 的不同之处在于用 $z'_k$ 替换了 $z_k$。\n$$\n\\nabla F_S(w) - \\nabla F_{S'}(w) = \\frac{1}{n} \\sum_{i=1}^n \\nabla f(w; z_i) - \\frac{1}{n} \\left( \\sum_{i \\neq k} \\nabla f(w; z_i) + \\nabla f(w; z'_k) \\right) = \\frac{1}{n}(\\nabla f(w; z_k) - \\nabla f(w; z'_k))\n$$\n由于 $f(\\cdot; z)$ 是 $G$-利普希茨的，其梯度范数必须被 $G$ 约束：$\\|\\nabla f(w; z)\\| \\leq G$。\n$$\n\\|\\nabla F_S(w) - \\nabla F_{S'}(w)\\| \\leq \\frac{1}{n} (\\|\\nabla f(w; z_k)\\| + \\|\\nabla f(w; z'_k)\\|) \\leq \\frac{1}{n}(G + G) = \\frac{2G}{n}\n$$\n这个界与 $w$ 无关。将其代入 $\\delta_{t+1}$ 的递推关系中：\n$$\n\\delta_{t+1} \\leq \\delta_t + \\eta \\frac{2G}{n}\n$$\n\n**第四部分：求解递推关系与最终上界**\n\n我们有递推关系 $\\delta_{t+1} \\leq \\delta_t + \\frac{2G\\eta}{n}$ 和初始条件 $\\delta_0 = 0$。将其展开 $T$ 步：\n$$\n\\delta_T \\leq \\delta_{T-1} + \\frac{2G\\eta}{n} \\leq \\delta_{T-2} + 2 \\cdot \\frac{2G\\eta}{n} \\leq \\dots \\leq \\delta_0 + T \\cdot \\frac{2G\\eta}{n}\n$$\n因为 $\\delta_0 = 0$，我们有 $\\delta_T = \\|w_T(S) - w_T(S')\\| \\leq \\frac{2GT\\eta}{n}$。\n现在，我们将其代回到 $\\beta_T$ 的上界中：\n$$\n\\beta_T \\leq G \\sup_{S, S'} \\|w_T(S) - w_T(S')\\| \\leq G \\left( \\frac{2GT\\eta}{n} \\right) = \\frac{2G^2T\\eta}{n}\n$$\n最后，我们用它来约束期望泛化误差：\n$$\n\\mathbb{E}_{S}[F(w_T(S)) - F_S(w_T(S))] \\leq \\beta_T \\leq \\frac{2G^2T\\eta}{n}\n$$\n这就提供了一个用指定参数 $n, G, \\eta, T$ 表示的闭式上界。", "answer": "$$\n\\boxed{\\frac{2G^2T\\eta}{n}}\n$$", "id": "3138528"}]}