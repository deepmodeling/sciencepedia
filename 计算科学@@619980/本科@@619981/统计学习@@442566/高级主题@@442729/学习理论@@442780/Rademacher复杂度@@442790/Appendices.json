{"hands_on_practices": [{"introduction": "要真正掌握一个理论概念，最好的方法莫过于从第一性原理出发亲手计算。本练习将引导你对一个简单、有限的假设类别直接应用经验 Rademacher 复杂度的定义。通过这个过程，你将建立关于 Rademacher 随机变量如何度量一个函数类“拟合”随机噪声能力的坚实直觉。[@problem_id:694900]", "problem": "在统计学习理论中，经验Rademacher复杂度是衡量一个函数类相对于给定数据集丰富程度的指标。它在推导依赖于数据的泛化界中起着至关重要的作用。\n\n设 $S = \\{z_1, z_2, \\ldots, z_n\\}$ 是 $\\mathbb{R}$ 中一个由 $n$ 个不同点组成的固定集合。设 $\\boldsymbol{\\sigma} = (\\sigma_1, \\sigma_2, \\ldots, \\sigma_n)$ 是一个由独立Rademacher随机变量组成的向量，其中每个 $\\sigma_i$ 以相等的概率取值于 $\\{-1, 1\\}$：$P(\\sigma_i = 1) = P(\\sigma_i = -1) = 1/2$。\n\n对于一个函数类 $\\mathcal{F}$（其中每个 $f \\in \\mathcal{F}$ 都从 $\\mathbb{R}$ 映射到 $\\mathbb{R}$），其相对于集合 $S$ 的经验Rademacher复杂度定义为：\n$$\n\\hat{\\mathfrak{R}}_S(\\mathcal{F}) = \\mathbb{E}_{\\boldsymbol{\\sigma}} \\left[ \\sup_{f \\in \\mathcal{F}} \\frac{1}{n} \\sum_{i=1}^n \\sigma_i f(z_i) \\right]\n$$\n\n考虑一个假设类 $\\mathcal{H}$，它是由从 $\\mathbb{R}$ 映射到 $\\{-1, 1\\}$ 的函数组成的。当这些函数在集合 $S$ 的数据点上求值时，它们会生成一组 $\\mathbb{R}^n$ 中的向量。假设这个向量集合，记为 $\\mathcal{H}|_S = \\{ (h(z_1), \\ldots, h(z_n)) \\mid h \\in \\mathcal{H} \\}$，对于某个向量 $\\mathbf{v} \\in \\{-1, 1\\}^n$，恰好由两个向量 $\\mathbf{v}$ 和 $-\\mathbf{v}$ 组成。\n\n你的任务是计算该类的经验Rademacher复杂度 $\\hat{\\mathfrak{R}}_S(\\mathcal{H})$。最终表达式应为一个关于 $n$ 的封闭形式公式。", "solution": "1. 定义：\n\n$$\n\\hat{\\mathfrak{R}}_S(\\mathcal{H})\n=\\mathbb{E}_{\\boldsymbol{\\sigma}}\\left[\\sup_{h\\in\\mathcal{H}}\\frac1n\\sum_{i=1}^n\\sigma_i\\,h(z_i)\\right].\n$$\n\n2. 由于 $\\mathcal{H}|_S=\\{\\mathbf{v},-\\mathbf{v}\\}$，\n\n$$\n\\sup_{h\\in\\mathcal{H}}\\sum_{i=1}^n\\sigma_i\\,h(z_i)\n=\\max\\left\\{\\sum_i\\sigma_i v_i,\\,-\\sum_i\\sigma_i v_i\\right\\}\n=\\left|\\sum_{i=1}^n\\sigma_i v_i\\right|.\n$$\n\n3. 令 $\\tau_i=\\sigma_i v_i$。则 $\\tau_i$ 是独立同分布 (i.i.d.) 的Rademacher变量，且\n\n$$\n\\hat{\\mathfrak{R}}_S(\\mathcal{H})\n=\\frac1n\\,\\mathbb{E}\\left[\\left|\\sum_{i=1}^n\\tau_i\\right|\\right]\n=\\frac1n\\,\\mathbb{E}\\left[|S_n|\\right],\\quad\nS_n=\\sum_{i=1}^n\\tau_i.\n$$\n\n4. 一个已知的组合公式得出\n\n$$\n\\mathbb{E}|S_n|\n=\\sum_{k=0}^n\\left|2k-n\\right|\\binom nk2^{-n}\n=n\\,\\frac{\\binom{n-1}{\\lfloor\\frac{n-1}2\\rfloor}}{2^{\\,n-1}}.\n$$\n\n5. 因此\n\n$$\n\\hat{\\mathfrak{R}}_S(\\mathcal{H})\n=\\frac1n\\mathbb{E}|S_n|\n=\\frac{\\binom{n-1}{\\lfloor\\frac{n-1}2\\rfloor}}{2^{\\,n-1}}.\n$$", "answer": "$$\\boxed{\\frac{\\binom{n-1}{\\lfloor\\frac{n-1}2\\rfloor}}{2^{\\,n-1}}}$$", "id": "694900"}, {"introduction": "掌握了有限函数类之后，下一步是挑战无限函数类，这在现代机器学习中更为常见。本练习引入了再生核希尔伯特空间（RKHS）中的单位球，这是支持向量机等方法背后的强大概念。你将学习如何利用“表示定理技巧”（representer trick）计算这个无限函数类的复杂度，并通过其格拉姆矩阵（Gram matrix）将其与核函数的性质联系起来。[@problem_id:759130]", "problem": "设 $\\mathcal{F}$ 是一个实值函数类。$\\mathcal{F}$ 关于有限点集 $S = \\{x_1, \\dots, x_n\\} \\subset \\mathbb{R}$ 的经验 Rademacher 复杂度定义为\n$$ \\hat{\\mathcal{R}}_S(\\mathcal{F}) = \\frac{1}{n} \\mathbb{E}_{\\boldsymbol{\\sigma}} \\left[ \\sup_{f \\in \\mathcal{F}} \\sum_{i=1}^n \\sigma_i f(x_i) \\right] $$\n其中 $\\boldsymbol{\\sigma} = (\\sigma_1, \\dots, \\sigma_n)$ 是一个由独立 Rademacher 随机变量组成的向量，对于每个 $i$，都有 $\\mathbb{P}(\\sigma_i = 1) = \\mathbb{P}(\\sigma_i = -1) = 1/2$。\n\n考虑一个函数类，该类是再生核希尔伯特空间 (RKHS) 的单位球。设 RKHS $\\mathcal{H}_k$ 由周期核 $k(x, x') = \\cos(\\omega(x-x'))$ 生成，其中对于给定的周期 $P > 0$，$\\omega = 2\\pi/P$。空间 $\\mathcal{H}_k$ 由形如 $f(x) = a \\cos(\\omega x) + b \\sin(\\omega x)$ (其中 $a, b \\in \\mathbb{R}$) 的函数构成，其 RKHS 范数的平方由 $\\|f\\|_{\\mathcal{H}_k}^2 = a^2+b^2$ 给出。\n\n我们感兴趣的函数类是该 RKHS 中的单位球：\n$$ \\mathcal{F} = \\left\\{ f \\in \\mathcal{H}_k : \\|f\\|_{\\mathcal{H}_k} \\le 1 \\right\\} $$\n\n计算该函数类 $\\mathcal{F}$ 在特定的 $n=3$ 个点集 $S = \\{x_1, x_2, x_3\\} = \\{0, P/4, P/2\\}$ 上的经验 Rademacher 复杂度 $\\hat{\\mathcal{R}}_S(\\mathcal{F})$。", "solution": "1. 我们使用表示定理技巧 (representer trick)：对于任意核为 $k$ 的 RKHS $\\mathcal{H}_k$ 和任意 Rademacher 向量 $\\boldsymbol{\\sigma}$，\n   $$\\sup_{\\|f\\|_{\\mathcal{H}_k}\\le1}\\sum_{i=1}^n\\sigma_i f(x_i)\n     = \\left\\|\\sum_{i=1}^n\\sigma_i k(x_i,\\cdot)\\right\\|_{\\mathcal{H}_k}.$$\n2. 因此\n   $$\\hat{\\mathcal{R}}_S(\\mathcal{F})\n     = \\frac1n \\mathbb{E}_{\\boldsymbol{\\sigma}}\\left[\\left\\|\\sum_{i=1}^3\\sigma_i k(x_i,\\cdot)\\right\\|_{\\mathcal{H}_k}\\right]\n     = \\frac1n \\mathbb{E}_{\\boldsymbol{\\sigma}}\\!\\left[\\sqrt{\\sum_{i,j=1}^3\\sigma_i\\sigma_j\\,k(x_i,x_j)}\\right].$$\n3. 对于 $x_1=0,\\;x_2=P/4,\\;x_3=P/2$，以及 $\\omega=2\\pi/P$，\n   $k(x_i,x_j)=\\cos\\bigl(\\omega(x_i-x_j)\\bigr)$ 给出 Gram 矩阵\n   $$K=\\begin{pmatrix}\n     1  0  -1\\\\\n     0  1  0\\\\\n    -1  0  1\n   \\end{pmatrix}.$$\n   因此\n   $$\\sum_{i,j}\\sigma_i\\sigma_j\\,k(x_i,x_j)\n     =\\sigma_1^2+\\sigma_2^2+\\sigma_3^2-2\\sigma_1\\sigma_3\n     =3-2\\sigma_1\\sigma_3.$$\n4. 由于 $\\sigma_1\\sigma_3=\\pm1$ 且取每个值的概率均为 $1/2$，\n   $$\\mathbb{E}\\left[\\sqrt{3-2\\sigma_1\\sigma_3}\\right]\n     =\\tfrac12\\sqrt{3-2\\cdot1}+\\tfrac12\\sqrt{3-2\\cdot(-1)}\n     =\\tfrac12(1+\\sqrt5).$$\n5. 因此\n   $$\\hat{\\mathcal{R}}_S(\\mathcal{F})\n     =\\frac1{3}\\cdot\\frac{1+\\sqrt5}2\n     =\\frac{1+\\sqrt5}6.$$", "answer": "$$\\boxed{\\frac{1+\\sqrt{5}}{6}}$$", "id": "759130"}, {"introduction": "理论计算是必不可少的，但 Rademacher 复杂度的真正价值在于它能够预测模型的泛化行为。最后的这个练习将从理论转向计算实验。你将实现岭回归（ridge regression），使用蒙特卡洛模拟来估计所学模型的经验 Rademacher 复杂度，并通过实验验证在不同正则化水平下，复杂度与测试误差之间的关系。[@problem_id:3165153]", "problem": "您将编写一个完整的程序，该程序为带岭正则化的线性预测器估计经验 Rademacher 复杂度，并通过在一系列正则化强度上测量单调关联来比较其经验趋势与测试性能。该任务必须纯粹从第一性原理出发，使用核心定义来解决。为了可复现性和普适性，数据集将综合生成，但其设计旨在模拟真实的线性建模条件。\n\n从以下基本定义和事实开始。\n\n1. 经验 Rademacher 复杂度。对于样本 $S = \\{x_1,\\dots,x_n\\} \\subset \\mathbb{R}^d$ 和一个实值函数 $f:\\mathbb{R}^d \\to \\mathbb{R}$ 的假设类 $\\mathcal{F}$，经验 Rademacher 复杂度为\n$$\n\\hat{\\mathfrak{R}}_S(\\mathcal{F}) \\;=\\; \\mathbb{E}_{\\sigma}\\left[\\frac{1}{n}\\sup_{f \\in \\mathcal{F}} \\sum_{i=1}^{n} \\sigma_i f(x_i)\\right],\n$$\n其中期望是关于独立的 Rademacher 变量 $\\sigma_i$ 计算的，这些变量以相等的概率取值于 $\\{-1,+1\\}$。\n\n2. 具有 $\\ell_2$ 范数界的线性类。对于线性函数类 $\\mathcal{F}_B = \\{f_w(x) = w^\\top x: \\lVert w \\rVert_2 \\le B\\}$ 和固定的输入 $x_1,\\dots,x_n$，内部的上确界具有闭式解\n$$\n\\sup_{\\lVert w \\rVert_2 \\le B} \\sum_{i=1}^n \\sigma_i w^\\top x_i \\;=\\; B \\left\\lVert \\sum_{i=1}^n \\sigma_i x_i \\right\\rVert_2,\n$$\n因此\n$$\n\\hat{\\mathfrak{R}}_S(\\mathcal{F}_B) \\;=\\; \\frac{B}{n}\\,\\mathbb{E}_\\sigma\\left[\\left\\lVert \\sum_{i=1}^n \\sigma_i x_i \\right\\rVert_2\\right].\n$$\n当期望不是解析计算时，可以通过对 $\\sigma \\in \\{-1,+1\\}^n$ 的 $R$ 次独立抽样进行蒙特卡洛平均来估计。\n\n3. 岭正则化线性预测。给定数据 $(x_i,y_i) \\in \\mathbb{R}^d \\times \\mathbb{R}$（其中 $i \\in \\{1,\\dots,n\\}$）和一个正则化参数 $\\lambda > 0$，岭估计量 $w_\\lambda \\in \\mathbb{R}^d$ 是下式的任意最小化子\n$$\n\\frac{1}{n}\\sum_{i=1}^n \\left(y_i - w^\\top x_i\\right)^2 + \\lambda \\lVert w \\rVert_2^2.\n$$\n一个经过充分检验的事实是，解满足正规方程\n$$\n\\left(\\frac{1}{n}X^\\top X + \\lambda I_d\\right)w_\\lambda \\;=\\; \\frac{1}{n}X^\\top y,\n$$\n其中 $X \\in \\mathbb{R}^{n \\times d}$ 是设计矩阵，$y \\in \\mathbb{R}^n$ 是响应向量。\n\n问题目标。对于每个指定的数据集和一系列正则化强度 $\\lambda$，您将：\n- 拟合岭回归以获得 $w_\\lambda$。\n- 设置 $B_\\lambda = \\lVert w_\\lambda \\rVert_2$，并通过蒙特卡洛方法在训练输入上计算经验 Rademacher 复杂度 $\\hat{\\mathfrak{R}}_S(\\mathcal{F}_{B_\\lambda})$。\n- 评估测试均方误差\n$$\n\\text{MSE}_\\lambda \\;=\\; \\frac{1}{n_{\\text{test}}}\\sum_{j=1}^{n_{\\text{test}}}\\left(w_\\lambda^\\top x_j^{(\\text{test})} - y_j^{(\\text{test})}\\right)^2.\n$$\n- 计算经验 Rademacher 复杂度向量 $\\{\\hat{\\mathfrak{R}}_S(\\mathcal{F}_{B_\\lambda})\\}$ 与测试均方误差向量 $\\{\\text{MSE}_\\lambda\\}$ 在 $\\lambda$ 值网格上的 Spearman 秩相关系数。Spearman 相关性是一个在 $[-1,1]$ 之间的实数，用于量化单调关联。\n\n数据生成协议（自包含）。对于每个测试用例，按如下方式从具有加性高斯噪声的线性模型生成数据：\n- 抽取一个真实向量 $w_\\star \\in \\mathbb{R}^d$，其条目为独立的标准正态分布，并将其重新缩放以使 $\\lVert w_\\star \\rVert_2 = 1$。\n- 抽取设计矩阵 $X_{\\text{train}} \\in \\mathbb{R}^{n_{\\text{train}} \\times d}$ 和 $X_{\\text{test}} \\in \\mathbb{R}^{n_{\\text{test}} \\times d}$，其条目独立地从均值为 $0$、方差为 $1/d$ 的正态分布中采样。此缩放确保 $\\mathbb{E}\\lVert x \\rVert_2^2 \\approx 1$。\n- 抽取独立的噪声向量 $\\varepsilon_{\\text{train}} \\in \\mathbb{R}^{n_{\\text{train}}}$ 和 $\\varepsilon_{\\text{test}} \\in \\mathbb{R}^{n_{\\text{test}}}$，其条目独立地从均值为 $0$、方差为 $\\sigma^2$ 的正态分布中采样。\n- 设置 $y_{\\text{train}} = X_{\\text{train}} w_\\star + \\varepsilon_{\\text{train}}$ 和 $y_{\\text{test}} = X_{\\text{test}} w_\\star + \\varepsilon_{\\text{test}}$。\n\n经验 Rademacher 复杂度的估计。对于给定的 $B_\\lambda$ 和训练输入 $S=\\{x_i\\}_{i=1}^{n_{\\text{train}}}$，估计\n$$\n\\hat{\\mathfrak{R}}_S(\\mathcal{F}_{B_\\lambda}) \\;\\approx\\; \\frac{B_\\lambda}{n_{\\text{train}}} \\cdot \\frac{1}{R}\\sum_{r=1}^{R}\\left\\lVert \\sum_{i=1}^{n_{\\text{train}}} \\sigma_i^{(r)} x_i \\right\\rVert_2,\n$$\n其中每个 $\\sigma^{(r)} \\in \\{-1,+1\\}^{n_{\\text{train}}}$ 具有独立的 Rademacher 条目，而 $R$ 是您必须在程序中固定的一个正整数。\n\n测试套件。使用以下三个测试用例，每个用例由一个元组 $(\\text{seed}, n_{\\text{train}}, n_{\\text{test}}, d, \\sigma)$ 指定：\n- 用例 1：$(0,\\, 40,\\, 200,\\, 100,\\, 0.1)$。\n- 用例 2：$(1,\\, 120,\\, 200,\\, 20,\\, 0.5)$。\n- 用例 3：$(2,\\, 50,\\, 200,\\, 200,\\, 1.0)$。\n\n正则化网格。对所有用例使用相同的 $\\lambda$ 值网格：\n$$\n\\Lambda \\;=\\; \\{10^{-6},\\, 10^{-4},\\, 10^{-3},\\, 10^{-2},\\, 10^{-1},\\, 1,\\, 10\\}.\n$$\n\n对每种情况，您的程序必须执行以下操作：\n- 使用指定的元组和给定的种子初始化随机数生成器来生成数据。\n- 对于每个 $\\lambda \\in \\Lambda$，在训练集上拟合岭回归，计算 $B_\\lambda$，使用您选择的蒙特卡洛样本大小 $R$（为所有情况固定一个单一的 $R$ 值）来估计 $\\hat{\\mathfrak{R}}_S(\\mathcal{F}_{B_\\lambda})$，并在测试集上计算 $\\text{MSE}_\\lambda$。\n- 计算向量 $\\{\\hat{\\mathfrak{R}}_S(\\mathcal{F}_{B_\\lambda})\\}_{\\lambda \\in \\Lambda}$ 和 $\\{\\text{MSE}_\\lambda\\}_{\\lambda \\in \\Lambda}$ 之间的 Spearman 秩相关系数。\n- 将相关系数记录为该用例的浮点数。\n\n答案规范和最终输出格式：\n- 每个测试用例的答案是一个浮点数：如上定义的 Spearman 秩相关系数，范围在 $[-1,1]$。\n- 您的程序应生成单行输出，其中包含三个相关系数，按用例顺序排列，作为逗号分隔的列表并用方括号括起，例如 $[c_1,c_2,c_3]$。\n- 此任务不涉及物理单位。", "solution": "该问题已经过验证，被认为是科学上合理、定义明确且自包含的。它展示了一个来自统计学习理论的标准计算实验，要求从第一性原理出发实现数据生成、模型拟合和复杂度测量。目标是凭经验研究正则化线性模型类的 Rademacher 复杂度与其泛化性能之间的关系。现在将提供一个完整且论证充分的解决方案。\n\n问题的核心是在一系列正则化强度 $\\lambda$ 上分析两个量之间的单调关联。这两个量是：\n1. 一类线性预测器的经验 Rademacher 复杂度，$\\hat{\\mathfrak{R}}_S(\\mathcal{F}_{B_\\lambda})$。该类 $\\mathcal{F}_{B_\\lambda} = \\{f_w(x) = w^\\top x: \\lVert w \\rVert_2 \\le B_\\lambda\\}$ 由一个依赖于数据的半径 $B_\\lambda = \\lVert w_\\lambda \\rVert_2$ 定义，其中 $w_\\lambda$ 是给定 $\\lambda$ 的岭回归问题的解。\n2. 在留出的测试集上的均方误差 (MSE)，$\\text{MSE}_\\lambda$，它作为模型泛化误差的估计。\n\n对于每个指定的测试用例，分析将按以下步骤进行。\n\n**1. 综合数据生成**\n数据是从一个真实线性模型 $y = Xw_\\star + \\varepsilon$ 生成的。\n- 通过从标准正态分布中采样并将其 $\\ell_2$-范数归一化为 $1$（即 $\\lVert w_\\star \\rVert_2 = 1$），创建一个真实参数向量 $w_\\star \\in \\mathbb{R}^d$。\n- 训练和测试设计矩阵 $X_{\\text{train}} \\in \\mathbb{R}^{n_{\\text{train}} \\times d}$ 和 $X_{\\text{test}} \\in \\mathbb{R}^{n_{\\text{test}} \\times d}$，其条目为从正态分布 $N(0, 1/d)$ 中独立同分布地抽取。方差缩放确保了特征向量 $x_i$ 的期望平方 $\\ell_2$-范数为 $\\mathbb{E}[\\lVert x_i \\rVert_2^2] = d \\cdot (1/d) = 1$。\n- 加性噪声向量 $\\varepsilon_{\\text{train}}$ 和 $\\varepsilon_{\\text{test}}$，其条目从 $N(0, \\sigma^2)$ 中抽取，其中 $\\sigma$ 是指定的噪声标准差。\n- 最终的响应向量计算为 $y_{\\text{train}} = X_{\\text{train}} w_\\star + \\varepsilon_{\\text{train}}$ 和 $y_{\\text{test}} = X_{\\text{test}} w_\\star + \\varepsilon_{\\text{test}}$。\n\n**2. 岭回归**\n对于指定网格 $\\Lambda$ 中的每个正则化参数 $\\lambda$ 值，通过求解正规方程来找到岭估计量 $w_\\lambda$：\n$$\n\\left(\\frac{1}{n_{\\text{train}}}X_{\\text{train}}^\\top X_{\\text{train}} + \\lambda I_d\\right)w_\\lambda = \\frac{1}{n_{\\text{train}}}X_{\\text{train}}^\\top y_{\\text{train}}\n$$\n其中 $I_d$ 是 $d \\times d$ 的单位矩阵。这构成了一个形式为 $A\\mathbf{w}=\\mathbf{b}$ 的线性系统，其中 $A = (\\frac{1}{n_{\\text{train}}}X_{\\text{train}}^\\top X_{\\text{train}} + \\lambda I_d)$ 且 $\\mathbf{b} = (\\frac{1}{n_{\\text{train}}}X_{\\text{train}}^\\top y_{\\text{train}})$。由于 $\\lambda > 0$ 且 $X_{\\text{train}}^\\top X_{\\text{train}}$ 是半正定的，矩阵 $A$ 是正定的，因此是可逆的，保证了 $w_\\lambda$ 的唯一解。该系统使用标准的数值线性代数程序求解。\n\n**3. 经验 Rademacher 复杂度估计**\n在训练样本 $S = \\{x_1, \\dots, x_{n_{\\text{train}}}\\}$ 上，类 $\\mathcal{F}_{B_\\lambda}$ 的经验 Rademacher 复杂度的解析表达式为：\n$$\n\\hat{\\mathfrak{R}}_S(\\mathcal{F}_{B_\\lambda}) = \\frac{B_\\lambda}{n_{\\text{train}}}\\,\\mathbb{E}_\\sigma\\left[\\left\\lVert \\sum_{i=1}^{n_{\\text{train}}} \\sigma_i x_i \\right\\rVert_2\\right]\n$$\n其中 $B_\\lambda = \\lVert w_\\lambda \\rVert_2$，期望是关于 Rademacher 变量 $\\sigma_i \\in \\{-1, +1\\}$ 计算的。\n该期望通过蒙特卡洛模拟来估计。我们抽取 $R$ 个独立的随机向量 $\\sigma^{(r)} = (\\sigma_1^{(r)}, \\dots, \\sigma_{n_{\\text{train}}}^{(r)})$，其中 $r=1, \\dots, R$。估计值则为：\n$$\n\\hat{\\mathfrak{R}}_S(\\mathcal{F}_{B_\\lambda}) \\approx \\frac{B_\\lambda}{n_{\\text{train}}} \\cdot \\frac{1}{R}\\sum_{r=1}^{R}\\left\\lVert \\sum_{i=1}^{n_{\\text{train}}} \\sigma_i^{(r)} x_i \\right\\rVert_2\n$$\n在计算上，如果我们形成一个矩阵 $\\Sigma \\in \\mathbb{R}^{R \\times n_{\\text{train}}}$，其中每一行是一个向量 $\\sigma^{(r)}$，那么对于所有的 $r$，和 $\\sum_i \\sigma_i^{(r)} x_i$ 可以通过一次矩阵乘法 $\\Sigma X_{\\text{train}}$ 高效计算。然后计算得到的 $R$ 个在 $\\mathbb{R}^d$ 中的向量的 $\\ell_2$-范数并取平均。\n\n**4. 测试误差评估**\n每个学习到的模型 $w_\\lambda$ 的性能在未见的测试数据上进行评估。测试均方误差计算如下：\n$$\n\\text{MSE}_\\lambda = \\frac{1}{n_{\\text{test}}}\\sum_{j=1}^{n_{\\text{test}}}\\left(w_\\lambda^\\top x_j^{(\\text{test})} - y_j^{(\\text{test})}\\right)^2\n$$\n\n**5. 单调关联分析**\n在计算了复杂度向量 $\\{\\hat{\\mathfrak{R}}_S(\\mathcal{F}_{B_\\lambda})\\}_{\\lambda \\in \\Lambda}$ 和测试误差向量 $\\{\\text{MSE}_\\lambda\\}_{\\lambda \\in \\Lambda}$ 之后，使用 Spearman 秩相关系数 $\\rho$ 来量化它们之间的单调关联。该系数等同于在秩变换后的数据上计算的 Pearson 相关系数。它对变量之间的特定函数关系不敏感，仅捕捉单调趋势的强度和方向。$\\rho = 1$ 的值表示完美的递增单调关系，$\\rho = -1$ 表示完美的递减单调关系，$\\rho = 0$ 表示没有单调关系。高的正相关性将表明，经验 Rademacher 复杂度是模型在不同正则化水平下泛化行为的一个良好指标。", "answer": "```python\nimport numpy as np\nfrom scipy.stats import spearmanr\n\ndef solve():\n    \"\"\"\n    Solves the problem of estimating empirical Rademacher complexity for ridge\n    regression and comparing it to test performance via Spearman correlation.\n    \"\"\"\n    test_cases = [\n        # (seed, n_train, n_test, d, sigma_noise)\n        (0, 40, 200, 100, 0.1),\n        (1, 120, 200, 20, 0.5),\n        (2, 50, 200, 200, 1.0),\n    ]\n\n    lambda_grid = np.array([1e-6, 1e-4, 1e-3, 1e-2, 1e-1, 1, 10])\n\n    # Fix the number of Monte Carlo samples for Rademacher complexity estimation.\n    R = 500\n\n    correlation_results = []\n\n    for seed, n_train, n_test, d, sigma_noise in test_cases:\n        rng = np.random.default_rng(seed)\n\n        # 1. Synthetic Data Generation\n        # Generate ground-truth vector w_star (d x 1) and normalize\n        w_star = rng.standard_normal(size=(d, 1))\n        w_star /= np.linalg.norm(w_star)\n\n        # Generate design matrices (n x d)\n        X_train = rng.normal(loc=0.0, scale=1.0/np.sqrt(d), size=(n_train, d))\n        X_test = rng.normal(loc=0.0, scale=1.0/np.sqrt(d), size=(n_test, d))\n\n        # Generate noise vectors (n x 1)\n        eps_train = rng.normal(loc=0.0, scale=sigma_noise, size=(n_train, 1))\n        eps_test = rng.normal(loc=0.0, scale=sigma_noise, size=(n_test, 1))\n\n        # Generate response vectors y (n x 1)\n        y_train = X_train @ w_star + eps_train\n        y_test = X_test @ w_star + eps_test\n\n        rademacher_complexities = []\n        test_mses = []\n\n        # Pre-compute parts for Rademacher complexity estimation that do not depend on lambda\n        # Generate R Rademacher random vectors\n        sigma_vectors = rng.choice([-1.0, 1.0], size=(R, n_train))\n        # Compute the sum of sigma_i * x_i for each of the R vectors\n        sum_sigma_x = sigma_vectors @ X_train  # Resulting shape (R, d)\n        # Compute the L2 norm for each of the R resulting d-dimensional vectors\n        norms_of_sum_sigma_x = np.linalg.norm(sum_sigma_x, axis=1)\n        # Compute the Monte Carlo estimate of the expectation term\n        mean_norm_expectation_part = np.mean(norms_of_sum_sigma_x)\n\n        # Pre-compute parts for ridge regression\n        XtX = X_train.T @ X_train\n        Xty = X_train.T @ y_train\n        I_d = np.identity(d)\n        \n        for lambda_val in lambda_grid:\n            # 2. Ridge Regression\n            # Solve (1/n * XtX + lambda * I) w = 1/n * Xty\n            A = (1.0 / n_train) * XtX + lambda_val * I_d\n            b = (1.0 / n_train) * Xty\n            w_lambda = np.linalg.solve(A, b)\n\n            # 3. Empirical Rademacher Complexity Estimation\n            B_lambda = np.linalg.norm(w_lambda)\n            rad_comp = (B_lambda / n_train) * mean_norm_expectation_part\n            rademacher_complexities.append(rad_comp)\n\n            # 4. Test Error Evaluation\n            y_pred_test = X_test @ w_lambda\n            mse = np.mean((y_pred_test - y_test)**2)\n            test_mses.append(mse)\n\n        # 5. Monotonic Association Analysis\n        # Use scipy.stats.spearmanr to compute the correlation coefficient.\n        # The result object has 'correlation' and 'pvalue' attributes.\n        correlation_result = spearmanr(rademacher_complexities, test_mses)\n        correlation_results.append(correlation_result.correlation)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, correlation_results))}]\")\n\nsolve()\n```", "id": "3165153"}]}