{"hands_on_practices": [{"introduction": "掌握学习曲线的第一步是能够识别其揭示的常见模式。这项练习模拟了一个典型的场景，即一个深度学习模型在训练过程中表现出过拟合的迹象。通过分析给定的训练和验证损失曲线 [@problem_id:3115493]，你需要准确诊断问题所在并选择最恰当的干预措施，这是每位机器学习实践者都必须具备的核心诊断技能。", "problem": "一个二元图像分类器使用随机梯度下降进行训练，在一个大小为 $50{,}000$ 的数据集上最小化二元交叉熵，数据集按 $80\\%/20\\%$ 的比例划分为训练集和验证集。该模型是一个中等深度的卷积神经网络，使用修正线性单元（ReLU）激活函数，除了批归一化（Batch Normalization）外没有显式的正则化。在训练过程中，您记录到学习曲线呈现以下典型模式：\n- 训练损失 $\\ell_{\\text{train}}(t)$ 从第 $t=1$ 个周期的大约 $0.69$ 稳步下降到第 $t=30$ 个周期的 $0.05$。\n- 验证损失 $\\ell_{\\text{val}}(t)$ 从第 $t=1$ 个周期的大约 $0.69$ 轻微下降到第 $t=5$ 个周期的 $0.62$，然后上升到第 $t=30$ 个周期的 $0.80$。\n- 到第 $t=30$ 个周期，训练准确率上升到 $98\\%$ 以上，而验证准确率在 $t\\approx 6$ 时达到约 $76\\%$ 的峰值，然后逐渐下降到 $70\\%$。\n\n假设数据是一次性随机划分的，并且是按类别分层的。除了在众包标记数据集中自然预期会有的少量标签噪声（估计低于 $2\\%$）外，没有已知的标签噪声。没有应用数据增强。优化器、学习率调度和批量大小在其他方面是稳定的，并产生单调递减的训练损失。\n\n使用以下基本定义：(i) 训练集上的经验风险 $R_{S}(f)=\\frac{1}{n}\\sum_{i=1}^{n}\\ell(f(x_i),y_i)$ 近似于优化目标，(ii) 验证风险近似于总体风险 $R(f)=\\mathbb{E}_{(x,y)\\sim \\mathcal{D}}[\\ell(f(x),y)]$，以及 (iii) 过拟合对应于在某个周期之后，由于高方差/低正则化，经验风险 $R_{S}(f)$ 下降而总体风险 $R(f)$ 上升。请根据这些学习曲线，选择最佳诊断和一系列有针对性的干预措施。\n\n哪个选项与这些曲线最一致，并以上述原则为基础？\n\nA. 模型由于正则化不足（正则化过弱）而过拟合。推荐的干预措施：增加系数为 $\\lambda>0$ 的 $\\ell_{2}$ 权重衰减，添加在验证集上调整保留概率的 Dropout，应用与标签不变性一致的更强的数据增强，在最小化 $\\ell_{\\text{val}}(t)$ 的周期处使用早停，并可选择性地减少有效容量（例如，减少滤波器数量）。\n\nB. 模型欠拟合。推荐的干预措施：增加模型深度和宽度，移除所有正则化，并进行更长时间的训练，直到两种损失都接近于零。\n\nC. 模型存在训练集和验证集之间的数据泄露问题。推荐的干预措施：重新构建数据集划分以避免泄露，并在其他方面保持当前的训练配置不变，因为性能差异主要是由泄露引起的。\n\nD. 优化器是主要问题；学习率太低，导致在训练损失下降的同时验证损失却在增加。推荐的干预措施：大幅增加学习率，并移除所有正则化以加速收敛，同时保持相同的训练周期数。", "solution": "用户希望我验证问题陈述，如果有效，则提供详细的解决方案。\n\n### 第一步：提取已知信息\n-   **模型：** 一个中等深度的卷积神经网络（CNN），使用修正线性单元（ReLU）激活函数。\n-   **任务：** 二元图像分类。\n-   **训练：** 使用随机梯度下降（SGD）最小化二元交叉熵。\n-   **数据集大小：** $50{,}000$ 张图像。\n-   **数据划分：** $80\\%$ 训练集（$n=40{,}000$），$20\\%$ 验证集（$m=10{,}000$）。该划分是随机且按类别分层的。\n-   **正则化：** 除了批归一化（Batch Normalization）外没有显式的正则化。\n-   **数据增强：** 未应用。\n-   **标签噪声：** 估计低于 $2\\%$。\n-   **学习曲线数据：**\n    -   训练损失 $\\ell_{\\text{train}}(t)$：从第 $t=1$ 个周期的大约 $0.69$ 稳步下降到第 $t=30$ 个周期的 $0.05$。\n    -   验证损失 $\\ell_{\\text{val}}(t)$：从第 $t=1$ 个周期的 $0.69$ 下降到第 $t=5$ 个周期的 $0.62$，然后上升到第 $t=30$ 个周期的 $0.80$。\n    -   训练准确率：到第 $t=30$ 个周期上升到 $> 98\\%$。\n    -   验证准确率：在 $t\\approx 6$ 时达到约 $76\\%$ 的峰值，然后下降到约 $70\\%$。\n-   **给定的定义：**\n    -   经验风险（训练损失）：$R_{S}(f)=\\frac{1}{n}\\sum_{i=1}^{n}\\ell(f(x_i),y_i)$。\n    -   总体风险（由验证风险近似）：$R(f)=\\mathbb{E}_{(x,y)\\sim \\mathcal{D}}[\\ell(f(x),y)]$。\n    -   过拟合：在某个周期之后，由于高方差/低正则化，经验风险 $R_{S}(f)$ 下降而总体风险 $R(f)$ 上升。\n\n### 第二步：使用提取的已知信息进行验证\n问题陈述描述了一个训练深度学习模型时的经典且现实的场景。\n\n-   **科学依据：** 该问题牢固地植根于统计学习理论的原理和神经网络训练的实践方面。训练/验证损失、准确率、过拟合、正则化等概念以及具体的数值都是合理的。对于二元分类任务，初始损失约为 $0.69$ 对应于 $-\\ln(0.5)$，这是随机猜测模型的期望损失，证实了模型的正确初始化。所描述的训练性能和验证性能之间的差异是教科书式的案例。\n-   **问题明确性：** 问题清晰地描述了模型的训练动态并提出了一个精确的问题。所提供的信息足以根据既定定义做出诊断。\n-   **客观性：** 问题陈述是客观的，呈现了数值数据并要求根据所提供的理论原则进行解释。没有主观或含糊不清的语言。\n\n该问题没有表现出验证清单中列出的任何缺陷。它在科学上是合理的、自洽的且问题明确。\n\n### 第三步：结论与行动\n问题陈述是**有效的**。我将继续进行解决方案的推导。\n\n### 推导与选项分析\n\n问题的核心是解读所提供的学习曲线。问题给出了过拟合的定义：经验风险 $R_S(f)$（由训练损失 $\\ell_{\\text{train}}$ 近似）下降，而总体风险 $R(f)$（由验证损失 $\\ell_{\\text{val}}$ 近似）上升。\n\n1.  **训练性能分析：** 训练损失 $\\ell_{\\text{train}}(t)$ 下降到非常低的值 $0.05$，训练准确率上升到 $98\\%$ 以上。这表明模型具有足够的能力来学习训练数据。优化过程是有效的，模型没有欠拟合。一个欠拟合的模型在训练集上就会表现出高损失和低准确率。\n\n2.  **验证性能与泛化差距分析：** 验证损失 $\\ell_{\\text{val}}(t)$ 最初下降，表明模型正在学习可泛化的特征。然而，在第 $t=5$ 个周期之后，它开始上升。验证准确率同样在 $t\\approx 6$ 左右达到峰值然后下降。训练曲线和验证曲线之间的这种差异是一个明确的信号。泛化误差（由 $\\ell_{\\text{val}}$ 近似）和训练误差（$\\ell_{\\text{train}}$）之间的差距很大并且在不断增大。\n\n3.  **诊断：** 观察到的行为——在第 $t=5$ 个周期之后，$\\ell_{\\text{train}}(t)$ 下降而 $\\ell_{\\text{val}}(t)$ 上升——与所提供的**过拟合**定义完全匹配。模型正在记忆训练数据，包括其特定的噪声和抽样伪影，而牺牲了其泛化到来自同一分布的未见新数据的能力。问题指出这是由于“高方差/低正则化”。模型被描述为“中等深度的CNN”，“除了批归一化外没有显式正则化”且“没有数据增强”，这是一种极易发生过拟合的设置，从而证实了这一诊断。\n\n4.  **推荐的干预措施：** 为了解决过拟合（一个高方差问题），目标是降低模型的方差或其完全记忆训练数据的能力。标准技术包括：\n    -   **增加正则化：** 添加惩罚模型复杂度的机制，如 $\\ell_2$ 权重衰减或 Dropout。\n    -   **数据增强：** 通过对图像应用保留标签的变换（例如，旋转、翻转、颜色抖动）来人为地扩充训练集。这是一种非常强大的正则化形式，因为它迫使模型学习更鲁棒和不变的特征。\n    -   **早停：** 在验证损失最小（或验证准确率最大）的点停止训练过程，在本例中大约是第 $t=5$ 或 $t=6$ 个周期。这可以防止模型进一步降低其泛化性能。\n    -   **减少模型容量：** 使用更小的模型（更少的层或每层更少的滤波器）来限制其记忆训练数据的能力。\n\n现在，我将根据此分析评估每个选项。\n\n**A. 模型由于正则化不足（正则化过弱）而过拟合。推荐的干预措施：增加系数为 $\\lambda>0$ 的 $\\ell_{2}$ 权重衰减，添加在验证集上调整保留概率的 Dropout，应用与标签不变性一致的更强的数据增强，在最小化 $\\ell_{\\text{val}}(t)$ 的周期处使用早停，并可选择性地减少有效容量（例如，减少滤波器数量）。**\n\n-   **诊断：** “由于正则化不足而过拟合”的诊断与我们对学习曲线和模型描述的分析完全一致。\n-   **干预措施：** 所有列出的干预措施——$\\ell_2$ 衰减、Dropout、数据增强、早停和减少模型容量——都是用于解决过拟合的标准、有针对性且适当的方法。\n-   **结论：** **正确**。该选项提供了准确的诊断和一套全面的正确补救措施。\n\n**B. 模型欠拟合。推荐的干预措施：增加模型深度和宽度，移除所有正则化，并进行更长时间的训练，直到两种损失都接近于零。**\n\n-   **诊断：** “欠拟合”的诊断是错误的。模型实现了 $0.05$ 的训练损失和 $>98\\%$ 的训练准确率，这表明它很好地拟合了训练数据，与欠拟合相反。\n-   **干预措施：** 推荐的措施（增加容量、移除正则化）是针对欠拟合的治疗方法。将它们应用于这个过拟合的模型会显著加剧问题。\n-   **结论：** **错误**。\n\n**C. 模型存在训练集和验证集之间的数据泄露问题。推荐的干预措施：重新构建数据集划分以避免泄露，并在其他方面保持当前的训练配置不变，因为性能差异主要是由泄露引起的。**\n\n-   **诊断：** 这个诊断与观察到的症状不符。如果发生了显著的数据泄露（即验证数据污染了训练集），那么验证损失应该会更紧密地跟随训练损失一起下降。训练损失和验证损失之间的巨大差异是干净划分上过拟合的经典标志，而不是数据泄露。\n-   **干预措施：** 虽然确保划分干净是很好的做法，但在这里它不是主要问题。保持“当前的训练配置不变”将忽略根本原因——模型的高方差和缺乏正则化——这很可能导致即使在新的、完全干净的划分上也会发生过拟合。\n-   **结论：** **错误**。\n\n**D. 优化器是主要问题；学习率太低，导致在训练损失下降的同时验证损失却在增加。推荐的干预措施：大幅增加学习率，并移除所有正则化以加速收敛，同时保持相同的训练周期数。**\n\n-   **诊断：** 这个诊断在理论上是不成立的。学习率太低会导致收敛缓慢；它不会导致验证损失与训练损失发散。这种发散是模型泛化的问题，而不是优化动态的问题。声称低学习率“导致验证损失增加”是错误的。此外，问题陈述优化器设置是“稳定的，并产生单调递减的训练损失”，这与优化器是“主要问题”的说法相矛盾。\n-   **干预措施：** 增加学习率可能导致不稳定，而移除正则化会恶化现有的过拟合。这些建议是错误的。\n-   **结论：** **错误**。", "answer": "$$\\boxed{A}$$", "id": "3115493"}, {"introduction": "为了超越简单的模式识别，我们需要理解学习曲线背后的量化原理。这个问题将使用偏差-方差分解来构建学习曲线的数学模型，帮助我们理解为什么不同复杂度的模型的学习曲线会发生交叉。通过计算交叉点所需的数据量 [@problem_id:3138225]，你将深入体会模型容量、偏差、方差和样本量之间的相互作用，从而做出更明智的模型选择。", "problem": "考虑一个监督回归任务，其输入为 $x \\in \\mathcal{X}$，输出为 $y \\in \\mathbb{R}$，由 $y = f(x) + \\varepsilon$ 生成，其中 $\\varepsilon$ 是均值为零、方差为 $\\sigma^2$ 的噪声。模型性能通过预期的验证均方误差（mean squared error (MSE)）来衡量，该误差是训练样本量 $n$ 的函数，记为 $L_{val}(n)$。对于在 $n$ 个样本上训练的固定模型类别，一个被广泛接受的分解是\n$$\n\\mathbb{E}[(y - \\hat{f}(x))^2] \\;=\\; \\sigma^2 \\;+\\; \\text{Bias}[\\hat{f}(x)]^2 \\;+\\; \\text{Var}[\\hat{f}(x)],\n$$\n其中期望是针对训练集和验证样本的抽取计算的。对于许多估计器，根据经验和理论，方差项大致按 $v/n$ 的比例缩放，其中 $v$ 是一个依赖于模型的常数，而对于固定的模型类别，平方偏差在 $n$ 上近似为常数。\n\n您正在通过它们的学习曲线 $L_{val}(n)$ 比较两个模型：\n- 一个低容量模型，其平方偏差为 $b_L^2 = 0.49$，方差系数为 $v_L = 3$。\n- 一个高容量模型，其平方偏差为 $b_H^2 = 0.01$，方差系数为 $v_H = 33$。\n假设不可约噪声方差 $\\sigma^2 = 1$，并且上述缩放关系对两个模型都成立。\n\n定义样本量 $n^\\star$ 为最小的整数 $n$，使得对于所有 $n \\ge n^\\star$，高容量模型的预期验证损失严格低于低容量模型。哪个选项确定了 $n^\\star$ 并根据偏差-方差权衡正确解释了学习曲线的交叉点？\n\nA. $n^\\star = 45$，因为交叉点发生在方差项之差等于原始偏差之和时，超过该点后，高容量模型的方差变得可以忽略不计。\n\nB. $n^\\star = 63$，因为交叉点发生在平方偏差的减少量抵消了较大的方差项时，即当 $\\frac{v_H - v_L}{n} = b_L^2 - b_H^2$ 时，并且对于 $n \\ge 63$，尽管高容量模型在小样本量 $n$ 时方差较高，但其较低的平方偏差占主导地位。\n\nC. $n^\\star = 30$，因为仅方差系数的差异就决定了阈值，而与偏差无关。\n\nD. 没有交叉点，因为不可约噪声 $\\sigma^2$ 使得高容量模型永远无法超越低容量模型。\n\nE. $n^\\star = 60$，因为设置 $\\frac{v_H - v_L}{n} = b_L^2 + b_H^2$ 得到了高容量模型开始胜出的阈值。", "solution": "用户希望我通过首先验证问题陈述，然后提供详细的解决方案来解决问题。\n\n### 问题验证\n\n**步骤 1：提取已知条件**\n\n问题陈述提供了以下信息：\n- 一个监督回归任务，其输入为 $x \\in \\mathcal{X}$，输出为 $y \\in \\mathbb{R}$。\n- 数据生成过程为 $y = f(x) + \\varepsilon$。\n- $\\varepsilon$ 是均值为零、方差为 $\\sigma^2$ 的噪声。\n- 性能度量是预期的验证均方误差，$L_{val}(n)$。\n- 偏差-方差分解由 $\\mathbb{E}[(y - \\hat{f}(x))^2] = \\sigma^2 + \\text{Bias}[\\hat{f}(x)]^2 + \\text{Var}[\\hat{f}(x)]$ 给出。\n- 方差项按 $\\text{Var}[\\hat{f}(x)] \\approx v/n$ 的比例缩放，其中 $v$ 是一个常数。\n- 对于固定的模型类别，平方偏差项 $\\text{Bias}[\\hat{f}(x)]^2$ 在 $n$ 上近似为常数。\n- 不可约噪声方差为 $\\sigma^2 = 1$。\n- 低容量模型参数：平方偏差 $b_L^2 = 0.49$，方差系数 $v_L = 3$。\n- 高容量模型参数：平方偏差 $b_H^2 = 0.01$，方差系数 $v_H = 33$。\n- $n^\\star$ 被定义为最小的整数 $n$，使得对于所有 $n \\ge n^\\star$，高容量模型的预期验证损失严格低于低容量模型。\n\n**步骤 2：使用提取的已知条件进行验证**\n\n问题陈述根据以下标准进行评估：\n- **科学依据：** 该问题基于统计学习理论中基本的偏差-方差权衡，这是机器学习诊断的基石。将均方误差分解为偏差、方差和不可约误差是标准做法。对偏差（对于固定的有偏模型为常数）和方差（$\\propto 1/n$）的缩放假设被广泛使用，并且对于许多常见的估计器在理论上是合理的。所有方面都与既定原则一致。\n- **适定性：** 问题提供了构建两个模型损失函数所需的所有数值和函数形式。目标是基于一个明确的不等式找到一个特定的整数 $n^\\star$，这是一个定义明确的数学任务，会导向唯一的解。\n- **客观性：** 问题以精确的、定量的术语陈述。参数以固定值的形式给出，定义是形式化的。没有主观或模糊的语言。\n- **完整性和一致性：** 问题是自洽的。计算学习曲线所需的所有变量（$b_L^2, v_L, b_H^2, v_H, \\sigma^2$）都已提供。设置中没有矛盾之处。\n- **现实性：** 所选的数值是为了清晰地说明这种权衡。虽然这些值是针对此问题的，但它们并非科学上不合理。高容量模型具有较低偏差但较高方差的情景是一个经典案例。\n\n**步骤 3：结论和行动**\n\n问题陈述是**有效的**。它在科学上是合理的、适定的、客观的和完整的。我现在将继续进行解答。\n\n### 解题推导\n\n对于在大小为 $n$ 的样本上训练的模型，其预期的验证损失 $L_{val}(n)$ 由不可约误差、平方偏差和方差之和给出。\n\n对于低容量模型 (L)，预期损失为：\n$$\nL_{val, L}(n) = \\sigma^2 + b_L^2 + \\frac{v_L}{n}\n$$\n代入给定值：\n$$\nL_{val, L}(n) = 1 + 0.49 + \\frac{3}{n} = 1.49 + \\frac{3}{n}\n$$\n\n对于高容量模型 (H)，预期损失为：\n$$\nL_{val, H}(n) = \\sigma^2 + b_H^2 + \\frac{v_H}{n}\n$$\n代入给定值：\n$$\nL_{val, H}(n) = 1 + 0.01 + \\frac{33}{n} = 1.01 + \\frac{33}{n}\n$$\n\n问题将 $n^\\star$ 定义为最小的整数 $n$，使得对于所有大于或等于 $n^\\star$ 的训练集大小，高容量模型的预期验证损失都严格低于低容量模型。这对应于找到满足 $L_{val, H}(n)  L_{val, L}(n)$ 的最小整数 $n$，并且该不等式对自身及所有后续整数都成立。\n\n我们来建立不等式：\n$$\nL_{val, H}(n)  L_{val, L}(n)\n$$\n$$\n1.01 + \\frac{33}{n}  1.49 + \\frac{3}{n}\n$$\n不可约误差项 $\\sigma^2=1$ 同时出现在两侧，因此不影响比较。我们可以减去它，这等同于比较可约误差项（平方偏差 + 方差）：\n$$\n0.01 + \\frac{33}{n}  0.49 + \\frac{3}{n}\n$$\n为了解出 $n$，我们分离包含 $n$ 的项。从两侧减去 $\\frac{3}{n}$ 和 $0.01$：\n$$\n\\frac{33}{n} - \\frac{3}{n}  0.49 - 0.01\n$$\n$$\n\\frac{30}{n}  0.48\n$$\n由于 $n$ 是样本量，$n > 0$。我们可以在不等式两侧同乘以 $n$ 而不改变不等号的方向：\n$$\n30  0.48 \\cdot n\n$$\n现在，解出 $n$：\n$$\nn > \\frac{30}{0.48}\n$$\n为了计算这个分数，我们可以将 $0.48$ 写成 $48/100$：\n$$\nn > \\frac{30}{48/100} = \\frac{30 \\times 100}{48} = \\frac{3000}{48}\n$$\n简化分数：\n$$\n\\frac{3000}{48} = \\frac{1500}{24} = \\frac{750}{12} = \\frac{375}{6} = \\frac{125}{2} = 62.5\n$$\n所以，不等式对于 $n > 62.5$ 成立。\n问题要求的是 $n^\\star$，即高容量模型对于所有 $n \\ge n^\\star$ 都严格更优的最小*整数* $n$。由于必须满足条件 $n > 62.5$，因此 $n$ 的最小整数值为 $63$。对于任何 $n \\ge 63$，不等式将继续成立，因为 $\\frac{30}{n}$ 项会继续减小，使得 $\\frac{30}{n}  0.48$ 的左侧更小。因此，$n^\\star = 63$。\n\n学习曲线的交叉点发生在 $n = 62.5$。在这一点上，总的可约误差相等：$b_H^2 + \\frac{v_H}{n} = b_L^2 + \\frac{v_L}{n}$。重新整理得到 $b_L^2 - b_H^2 = \\frac{v_H - v_L}{n}$。这个方程表明，交叉点发生在：高容量模型在平方偏差上的优势（$b_L^2 - b_H^2$）与其在方差上的劣势（$\\frac{v_H - v_L}{n}$）正好相互抵消。对于 $n > 62.5$，方差劣势缩小，偏差优势占主导地位，使得高容量模型更优。\n\n### 逐项分析\n\n**A. $n^\\star = 45$，因为交叉点发生在方差项之差等于原始偏差之和时，超过该点后，高容量模型的方差变得可以忽略不计。**\n- 值 $n^\\star = 45$ 是不正确的。我们的计算得出 $n^\\star = 63$。\n- 推理是有缺陷的。交叉点由平方偏差决定，而不是原始偏差，并且是由平方偏差的*差*而不是和决定。高容量模型的方差并没有变得可以忽略不计；而是其相对于低容量模型的*超额*方差变得小于偏差的改进量。\n- **结论：** 错误。\n\n**B. $n^\\star = 63$，因为交叉点发生在平方偏差的减少量抵消了较大的方差项时，即当 $\\frac{v_H - v_L}{n} = b_L^2 - b_H^2$ 时，并且对于 $n \\ge 63$，尽管高容量模型在小样本量 $n$ 时方差较高，但其较低的平方偏差占主导地位。**\n- 值 $n^\\star = 63$ 是正确的。\n- 推理是合理的。交叉点被正确定位为“平方偏差的减少量”（$b_L^2 - b_H^2$）等于“较大的方差项”（更准确地说，是方差项的差，$\\frac{v_H - v_L}{n}$）的点。所提供的方程 $\\frac{v_H - v_L}{n} = b_L^2 - b_H^2$ 正确地描述了我们计算出的平衡点 $n = 62.5$。结论是，对于 $n \\ge 63$，高容量模型较低的平方偏差占主导地位，这是对结果的正确解释。\n- **结论：** 正确。\n\n**C. $n^\\star = 30$，因为仅方差系数的差异就决定了阈值，而与偏差无关。**\n- 值 $n^\\star = 30$ 是不正确的。\n- 推理是根本错误的。这个问题是偏差-*方差*权衡的经典例证。哪个模型更好的阈值明确地取决于偏差和方差项。声称它“与偏差无关”是错误的。数字 $30$ 仅仅是 $v_H - v_L$ 的值。\n- **结论：** 错误。\n\n**D. 没有交叉点，因为不可约噪声 $\\sigma^2$ 使得高容量模型永远无法超越低容量模型。**\n- “没有交叉点”的说法是不正确的。我们计算出交叉点在 $n = 62.5$。\n- 推理是不正确的。不可约噪声 $\\sigma^2$ 是两个模型损失函数共有的一个加法项。在比较两个模型时，这个项会抵消掉，对哪个模型更好或它们的学习曲线在何处交叉没有影响。\n- **结论：** 错误。\n\n**E. $n^\\star = 60$，因为设置 $\\frac{v_H - v_L}{n} = b_L^2 + b_H^2$ 得到了高容量模型开始胜出的阈值。**\n- 值 $n^\\star = 60$ 是不正确的。\n- 推理基于一个不正确的公式。它使用了平方偏差的*和*（$b_L^2 + b_H^2 = 0.49 + 0.01 = 0.50$）而不是*差*（$b_L^2 - b_H^2 = 0.48$）。正确的比较涉及到偏差的增益与方差的损失。使用不正确的公式会得到 $n = \\frac{30}{0.50} = 60$，这就解释了错误值的来源。\n- **结论：** 错误。", "answer": "$$\\boxed{B}$$", "id": "3138225"}, {"introduction": "学习曲线分析不仅能诊断问题，还能指导模型开发中的关键决策。这项练习将你置于一个现实场景：在一个类别不平衡的数据集上，你需要决定下一步是收集更多的少数类样本还是多数类样本。通过估算学习曲线的局部斜率来判断哪种数据能带来最大的性能提升 [@problem_id:3138165]，你将学会如何将理论分析转化为具体、有价值的行动。", "problem": "给定一个存在严重类别不平衡的二元分类场景，其中两个类别是“正常”和“异常”。学习器在包含 $n_a$ 个异常样本和 $n_n$ 个正常样本的数据集上进行训练。验证性能由验证损失 $L_{\\mathrm{val}}(n_a,n_n)$ 量化，其定义为在留出的验证分布上的期望损失。在独立同分布 (i.i.d.) 采样和经验风险最小化 (ERM) 的假设下，随着更多信息丰富的训练数据被添加，$L_{\\mathrm{val}}$ 的期望是非递增的，但有限样本的波动和类别不平衡使得局部行为依赖于所添加样本的类型。\n\n您的任务是实现一个程序，针对一组案例，通过分析局部学习曲线来决定在当前阶段下，收集更多的异常样本还是正常样本能最有效地降低 $L_{\\mathrm{val}}$。对于每个案例，您将获得两个局部化的学习曲线片段：\n\n- 一个仅添加异常样本的序列：位置 $\\{x^{(a)}_i\\}$ 代表（相对于基线）添加的异常样本数量，以及在这些位置上于验证集上测得的相应验证损失 $\\{L^{(a)}_i\\}$。\n- 一个仅添加正常样本的序列：位置 $\\{x^{(n)}_j\\}$ 代表（相对于基线）添加的正常样本数量，以及在这些位置上于验证集上测得的相应验证损失 $\\{L^{(n)}_j\\}$。\n\n您必须使用局部线性模型来估计添加异常样本与正常样本对每个样本的局部边际效应。具体来说，对于一个序列 $\\{(x_k, L_k)\\}$，使用普通最小二乘法 (OLS) 对最后的 $K$ 个点拟合一条直线 $L \\approx \\alpha + \\beta x$，其中 $K$ 是一个小的整数。斜率 $\\beta$ 估计了在当前阶段下，每增加一个相应类别的样本时，$\\frac{\\partial L_{\\mathrm{val}}}{\\partial n}$ 的局部离散导数。如果序列较短，则使用 $K=3$ 或所有可用点。\n\n使用一个小的容忍度 $\\epsilon$ 来定义决策规则，以避免对可忽略的差异做出反应：\n\n- 计算 $\\hat{g}_a$ 作为仅异常样本序列的 OLS 斜率 $\\beta$，以及 $\\hat{g}_n$ 作为仅正常样本序列的 OLS 斜率 $\\beta$。这里 $\\hat{g}_a$ 估计 $\\frac{\\partial L_{\\mathrm{val}}}{\\partial n_a}$，$\\hat{g}_n$ 估计 $\\frac{\\partial L_{\\mathrm{val}}}{\\partial n_n}$。\n- 如果 $\\hat{g}_a \\ge 0$ 和 $\\hat{g}_n \\ge 0$ 同时成立，输出 $-1$（局部添加任一类别样本都无预期改善）。\n- 否则，使用容忍度 $\\epsilon$ 比较斜率：\n  - 如果 $\\hat{g}_a  \\hat{g}_n - \\epsilon$，输出 $1$（收集异常样本）。\n  - 否则，如果 $\\hat{g}_n  \\hat{g}_a - \\epsilon$，输出 $0$（收集正常样本）。\n  - 否则，输出 $-1$（在容忍度范围内无差异）。\n\n对所有案例均使用 $\\epsilon = 10^{-5}$ 和 $K=3$。\n\n您的程序必须解决以下测试套件，其中每个案例由异常样本和正常样本的位置和损失元组指定。以下所有数字均为十进制数（非百分比），代表没有物理单位的计数或损失值。\n\n- 案例 1：\n  - 仅异常样本位置：$[0,10,20,30]$\n  - 仅异常样本损失：$[0.210,0.200,0.193,0.188]$\n  - 仅正常样本位置：$[0,100,200,300]$\n  - 仅正常样本损失：$[0.210,0.207,0.205,0.204]$\n- 案例 2：\n  - 仅异常样本位置：$[0,10,20,30]$\n  - 仅异常样本损失：$[0.160,0.1595,0.1590,0.1587]$\n  - 仅正常样本位置：$[0,100,200,300]$\n  - 仅正常样本损失：$[0.160,0.152,0.147,0.145]$\n- 案例 3：\n  - 仅异常样本位置：$[0,20,40,60]$\n  - 仅异常样本损失：$[0.300,0.298,0.296,0.294]$\n  - 仅正常样本位置：$[0,100,200,300]$\n  - 仅正常样本损失：$[0.300,0.290,0.280,0.270]$\n- 案例 4：\n  - 仅异常样本位置：$[0,10,20,30,40]$\n  - 仅异常样本损失：$[0.250,0.245,0.242,0.243,0.241]$\n  - 仅正常样本位置：$[0,10,20,30,40]$\n  - 仅正常样本损失：$[0.250,0.248,0.2478,0.2477,0.2476]$\n- 案例 5：\n  - 仅异常样本位置：$[0,5,10,15]$\n  - 仅异常样本损失：$[0.180,0.181,0.182,0.1825]$\n  - 仅正常样本位置：$[0,5,10,15]$\n  - 仅正常样本损失：$[0.180,0.1805,0.181,0.1815]$\n\n最终输出格式：您的程序应生成单行文本，其中包含按顺序排列的各案例决策，编码为整数并用逗号分隔，置于方括号内。使用编码 $1$ 表示“收集异常样本”，$0$ 表示“收集正常样本”，$-1$ 表示“无差异或无改善”（例如 $[1,0,-1,1,-1]$）。", "solution": "该问题要求一种决策算法，用于在存在类别不平衡的二元分类任务中指导数据收集。目标是确定增加更多“异常”样本还是“正常”样本更可能降低验证损失 $L_{\\mathrm{val}}$。该决策基于对学习曲线的局部分析。对于每个案例，我们都得到了两个验证损失测量序列：一个只添加异常样本，$\\{ (x^{(a)}_i, L^{(a)}_i) \\}$；另一个只添加正常样本，$\\{ (x^{(n)}_j, L^{(n)}_j) \\}$。变量 $x_i$ 表示所添加的某一类别样本的数量。\n\n该方法的核心是估计验证损失相对于各类所添加样本数量的局部变化率。这个变化率，或称局部离散导数，对于异常样本表示为 $\\frac{\\partial L_{\\mathrm{val}}}{\\partial n_a}$，对于正常样本表示为 $\\frac{\\partial L_{\\mathrm{val}}}{\\partial n_n}$。负值表示增加该类别的更多样本正在降低验证损失，这是期望的结果。\n\n我们通过对学习曲线数据拟合一个局部线性模型 $L \\approx \\alpha + \\beta x$ 来估计这些导数。这条线的斜率 $\\beta$ 作为我们对导数的估计值。问题指定使用普通最小二乘法 (OLS) 对每个序列的最后 $K=3$ 个数据点来计算这个斜率。如果一个序列的点数少于 $K$ 个，则使用所有可用点。对于一组 $M$ 个点 $\\{(x_k, L_k)\\}_{k=1}^M$，斜率 $\\beta$ 的 OLS 估计值由以下公式给出：\n$$\n\\beta = \\frac{\\sum_{k=1}^{M} (x_k - \\bar{x})(L_k - \\bar{L})}{\\sum_{k=1}^{M} (x_k - \\bar{x})^2}\n$$\n其中 $\\bar{x} = \\frac{1}{M}\\sum_{k=1}^{M} x_k$ 和 $\\bar{L} = \\frac{1}{M}\\sum_{k=1}^{M} L_k$ 是样本均值。\n\n设 $\\hat{g}_a$ 为仅异常样本序列的估计斜率，$\\hat{g}_n$ 为仅正常样本序列的估计斜率。然后应用决策规则，使用容忍度 $\\epsilon = 10^{-5}$ 来避免基于统计上不显著的差异做出决策。规则如下：\n\n1. 如果 $\\hat{g}_a \\ge 0$ 和 $\\hat{g}_n \\ge 0$ 同时成立，这表明局部添加任一类别的样本都预计不会改善（甚至可能恶化）验证损失。在这种情况下，输出为 $-1$。\n\n2. 否则，我们比较斜率：\n- 如果 $\\hat{g}_a  \\hat{g}_n - \\epsilon$，异常样本的下降速率显著大于正常样本。决策是收集异常样本，输出为 $1$。\n- 否则，如果 $\\hat{g}_n  \\hat{g}_a - \\epsilon$，正常样本的下降速率显著更大。决策是收集正常样本，输出为 $0$。\n- 否则，变化率的差异在容忍度 $\\epsilon$ 之内，因此我们持中立态度。输出为 $-1$。\n\n现在，我们将使用 $K=3$ 和 $\\epsilon=10^{-5}$ 将此程序应用于所提供的 5 个测试案例中的每一个。\n\n**案例 1：**\n- 异常样本：最后 3 个点是 $\\{(10, 0.200), (20, 0.193), (30, 0.188)\\}$。计算出的 OLS 斜率为 $\\hat{g}_a = -0.0006$。\n- 正常样本：最后 3 个点是 $\\{(100, 0.207), (200, 0.205), (300, 0.204)\\}$。计算出的 OLS 斜率为 $\\hat{g}_n = -0.000015$。\n- 决策：两个斜率均为负。我们检查是否 $\\hat{g}_a  \\hat{g}_n - \\epsilon$：$-0.0006  -0.000015 - 10^{-5} \\implies -0.0006  -0.000025$。此不等式成立。决策是收集异常样本。结果：$1$。\n\n**案例 2：**\n- 异常样本：最后 3 个点是 $\\{(10, 0.1595), (20, 0.1590), (30, 0.1587)\\}$。计算出的 OLS 斜率为 $\\hat{g}_a \\approx -0.00004$。\n- 正常样本：最后 3 个点是 $\\{(100, 0.152), (200, 0.147), (300, 0.145)\\}$。计算出的 OLS 斜率为 $\\hat{g}_n = -0.000035$。\n- 决策：两个斜率均为负。\n  - 检查 $\\hat{g}_a  \\hat{g}_n - \\epsilon$：$-0.00004  -0.000035 - 10^{-5} \\implies -0.00004  -0.000045$。此为假。\n  - 检查 $\\hat{g}_n  \\hat{g}_a - \\epsilon$：$-0.000035  -0.00004 - 10^{-5} \\implies -0.000035  -0.00005$。此为假。\n  差异不足以支持偏好其中一个。决策为无差异。结果：$-1$。\n\n**案例 3：**\n- 异常样本：最后 3 个点是 $\\{(20, 0.298), (40, 0.296), (60, 0.294)\\}$。由于这些点完全共线，OLS 斜率是精确的。\n  $\\hat{g}_a = \\frac{0.294-0.298}{60-20} = \\frac{-0.004}{40} = -0.0001$。\n- 正常样本：最后 3 个点是 $\\{(100, 0.290), (200, 0.280), (300, 0.270)\\}$。这些点也完全共线。\n  $\\hat{g}_n = \\frac{0.270-0.290}{300-100} = \\frac{-0.02}{200} = -0.0001$。\n- 决策：斜率相同，$\\hat{g}_a = \\hat{g}_n = -0.0001$。涉及 $\\epsilon$ 的两个比较条件都无法满足。决策为无差异。结果：$-1$。\n\n**案例 4：**\n- 异常样本：有 5 个点；我们使用最后 3 个：$\\{(20, 0.242), (30, 0.243), (40, 0.241)\\}$。计算出的 OLS 斜率为 $\\hat{g}_a = -0.00005$。\n- 正常样本：有 5 个点；我们使用最后 3 个：$\\{(20, 0.2478), (30, 0.2477), (40, 0.2476)\\}$。计算出的 OLS 斜率为 $\\hat{g}_n = -0.00001$。\n- 决策：两个斜率均为负。我们检查是否 $\\hat{g}_a  \\hat{g}_n - \\epsilon$：$-0.00005  -0.00001 - 10^{-5} \\implies -0.00005  -0.00002$。此不等式成立。决策是收集异常样本。结果：$1$。\n\n**案例 5：**\n- 异常样本：最后 3 个点是 $\\{(5, 0.181), (10, 0.182), (15, 0.1825)\\}$。计算出的 OLS 斜率为 $\\hat{g}_a \\approx 0.00015$。\n- 正常样本：最后 3 个点是 $\\{(5, 0.1805), (10, 0.181), (15, 0.1815)\\}$。计算出的 OLS 斜率为 $\\hat{g}_n = 0.0001$。\n- 决策：两个斜率均为正，$\\hat{g}_a > 0$ 和 $\\hat{g}_n > 0$。根据第一条规则，这表明局部添加任一类别的样本都无预期改善。决策为无改善。结果：$-1$。\n\n五个案例的最终决策序列是 $[1, -1, -1, 1, -1]$。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef calculate_slope(positions, losses, K):\n    \"\"\"\n    Calculates the OLS slope for the last K points of a sequence.\n\n    Args:\n        positions (list of float): The x-values (number of samples).\n        losses (list of float): The y-values (validation loss).\n        K (int): The number of recent points to use for the linear fit.\n\n    Returns:\n        float: The slope of the linear regression line.\n    \"\"\"\n    if len(positions)  2:\n        # Not enough points to fit a line.\n        return 0.0\n\n    # Determine the number of points to use for fitting.\n    num_points_to_fit = min(len(positions), K)\n\n    # Slice the last `num_points_to_fit` points.\n    x_fit = np.array(positions[-num_points_to_fit:])\n    y_fit = np.array(losses[-num_points_to_fit:])\n\n    # Calculate the means.\n    x_mean = np.mean(x_fit)\n    y_mean = np.mean(y_fit)\n\n    # Calculate the numerator and denominator for the slope formula.\n    # beta = sum((x_i - x_mean) * (y_i - y_mean)) / sum((x_i - x_mean)^2)\n    numerator = np.sum((x_fit - x_mean) * (y_fit - y_mean))\n    denominator = np.sum((x_fit - x_mean)**2)\n\n    if np.isclose(denominator, 0):\n        # This case occurs if all x_fit values are identical, which means\n        # no new samples were added in the window. The slope is taken as 0.\n        return 0.0\n\n    slope = numerator / denominator\n    return slope\n\ndef solve():\n    \"\"\"\n    Solves the problem for the given test suite of learning curve data.\n    \"\"\"\n    # Define constants from the problem statement.\n    K = 3\n    epsilon = 1e-5\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        { # Case 1\n            \"anomalies_pos\": [0, 10, 20, 30],\n            \"anomalies_loss\": [0.210, 0.200, 0.193, 0.188],\n            \"normals_pos\": [0, 100, 200, 300],\n            \"normals_loss\": [0.210, 0.207, 0.205, 0.204],\n        },\n        { # Case 2\n            \"anomalies_pos\": [0, 10, 20, 30],\n            \"anomalies_loss\": [0.160, 0.1595, 0.1590, 0.1587],\n            \"normals_pos\": [0, 100, 200, 300],\n            \"normals_loss\": [0.160, 0.152, 0.147, 0.145],\n        },\n        { # Case 3\n            \"anomalies_pos\": [0, 20, 40, 60],\n            \"anomalies_loss\": [0.300, 0.298, 0.296, 0.294],\n            \"normals_pos\": [0, 100, 200, 300],\n            \"normals_loss\": [0.300, 0.290, 0.280, 0.270],\n        },\n        { # Case 4\n            \"anomalies_pos\": [0, 10, 20, 30, 40],\n            \"anomalies_loss\": [0.250, 0.245, 0.242, 0.243, 0.241],\n            \"normals_pos\": [0, 10, 20, 30, 40],\n            \"normals_loss\": [0.250, 0.248, 0.2478, 0.2477, 0.2476],\n        },\n        { # Case 5\n            \"anomalies_pos\": [0, 5, 10, 15],\n            \"anomalies_loss\": [0.180, 0.181, 0.182, 0.1825],\n            \"normals_pos\": [0, 5, 10, 15],\n            \"normals_loss\": [0.180, 0.1805, 0.181, 0.1815],\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        # Calculate local slopes for anomalies and normals\n        g_a = calculate_slope(case[\"anomalies_pos\"], case[\"anomalies_loss\"], K)\n        g_n = calculate_slope(case[\"normals_pos\"], case[\"normals_loss\"], K)\n\n        # Apply the decision rule\n        decision = -1  # Default to indifferent\n        if g_a >= 0 and g_n >= 0:\n            decision = -1  # No expected improvement from either\n        elif g_a  g_n - epsilon:\n            decision = 1  # Collect anomalies\n        elif g_n  g_a - epsilon:\n            decision = 0  # Collect normals\n        else:\n            decision = -1  # Indifferent within tolerance\n        \n        results.append(decision)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3138165"}]}