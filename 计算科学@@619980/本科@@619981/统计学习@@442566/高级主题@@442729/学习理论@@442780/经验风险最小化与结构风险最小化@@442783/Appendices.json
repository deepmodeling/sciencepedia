{"hands_on_practices": [{"introduction": "本练习提供了一个具体的、基于计算的入门，以帮助理解经验拟合与模型复杂度之间的权衡。我们将看到一个高次多项式如何通过完美拟合数据点来达到零训练误差，但代价是剧烈振荡——这是过拟合的典型标志[@problem_id:3118270]。通过对模型的“粗糙度”引入一个简单的惩罚项（一种结构风险），我们可以量化地表达对更平滑、更合理模型的偏好，即使它不能完美拟合训练数据。这个实践对于理解为什么“仅仅最小化训练误差”通常不是正确的目标至关重要。", "problem": "考虑一个由$4$个有序输入-输出对组成的数据集$D = \\{(x_i,y_i)\\}_{i=1}^{4}$，其中$x_1=0$, $x_2=1$, $x_3=2$, $x_4=3$以及$y_1=0$, $y_2=3$, $y_3=0$, $y_4=3$。令假设类别$\\mathcal{H}_d$为所有次数至多为$d$的多项式$f:\\mathbb{R}\\to\\mathbb{R}$。一个样本$(x,y)$上的损失是平方损失$\\ell(f;(x,y)) = (y - f(x))^2$。经验风险是样本均方损失\n$$\nR_{\\text{emp}}(f;D) \\;=\\; \\frac{1}{4}\\sum_{i=1}^{4} \\bigl(y_i - f(x_i)\\bigr)^2.\n$$\n定义两个学习原则：\n- 经验风险最小化 (ERM)：选择任意 $\\hat{f}_{\\text{ERM},d} \\in \\arg\\min_{f\\in \\mathcal{H}_d} R_{\\text{emp}}(f;D)$。\n- 结构风险最小化 (SRM)，带有对（离散化的）一阶导数的全变分惩罚：对于一个函数$f$，将其在内部索引$i$处的离散二阶差分定义为$\\Delta^2 f(x_i) := f(x_{i+1}) - 2 f(x_i) + f(x_{i-1})$（使用自然排序$x_1  x_2  x_3  x_4$）。全变分惩罚是\n$$\n\\mathrm{TV}(f;D) \\;=\\; \\sum_{i=2}^{3} \\left|\\, \\Delta^2 f(x_i) \\,\\right|.\n$$\n给定正则化权重$\\lambda  0$，SRM的目标函数是\n$$\nJ_{\\lambda}(f;D) \\;=\\; R_{\\text{emp}}(f;D) \\;+\\; \\lambda\\, \\mathrm{TV}(f;D).\n$$\n\n任务：\n1. 仅使用以上定义，论证为什么在$\\mathcal{H}_3$上的ERM可以在数据集$D$上实现经验风险为$0$，但这对应于一个在样本点间振荡的拟合。\n2. 计算唯一的最小二乘线性预测器$\\hat{f}_{\\text{ERM},1}(x) = a x + b$，它在$\\mathcal{H}_1$上最小化$R_{\\text{emp}}(f;D)$，并评估其经验风险$R_{\\text{emp}}(\\hat{f}_{\\text{ERM},1};D)$和其惩罚项$\\mathrm{TV}(\\hat{f}_{\\text{ERM},1};D)$。\n3. 考虑三次插值$\\hat{f}_{\\text{ERM},3}\\in\\mathcal{H}_3$，它通过满足对所有$i\\in\\{1,2,3,4\\}$都有$\\hat{f}_{\\text{ERM},3}(x_i)=y_i$来达到零经验风险。评估其经验风险$R_{\\text{emp}}(\\hat{f}_{\\text{ERM},3};D)$和其惩罚项$\\mathrm{TV}(\\hat{f}_{\\text{ERM},3};D)$。\n4. 令$\\lambda^{\\star}$为$\\lambda$的临界值，在该值下SRM目标函数对于线性最小二乘预测器$\\hat{f}_{\\text{ERM},1}$和三次插值$\\hat{f}_{\\text{ERM},3}$是无差异的，即$J_{\\lambda^{\\star}}(\\hat{f}_{\\text{ERM},1};D) = J_{\\lambda^{\\star}}(\\hat{f}_{\\text{ERM},3};D)$。精确计算$\\lambda^{\\star}$并以一个既约分数形式表示你的最终答案。\n\n你的最终答案必须是$\\lambda^{\\star}$的精确分数值。不要四舍五入。", "solution": "该问题是有效的。我们按顺序处理每个任务来给出解答。\n\n数据集是$D = \\{(x_i, y_i)\\}_{i=1}^{4}$，其中的点是$(0,0)$, $(1,3)$, $(2,0)$, 和$(3,3)$。假设类别$\\mathcal{H}_d$是次数至多为$d$的多项式。经验风险是$R_{\\text{emp}}(f;D) = \\frac{1}{4}\\sum_{i=1}^{4} (y_i - f(x_i))^2$。惩罚项是$\\mathrm{TV}(f;D) = \\sum_{i=2}^{3} |\\Delta^2 f(x_i)|$，其中$\\Delta^2 f(x_i) = f(x_{i+1}) - 2 f(x_i) + f(x_{i-1})$。\n\n任务1：关于在$\\mathcal{H}_3$上进行ERM的论证\n$\\mathcal{H}_3$中的一个多项式形式为$f(x) = c_3 x^3 + c_2 x^2 + c_1 x + c_0$，它有$4$个自由参数（系数$c_0, c_1, c_2, c_3$）。要找到一个最小化经验风险的函数$f \\in \\mathcal{H}_3$，我们试图让平方误差$(y_i - f(x_i))^2$尽可能小。如果我们能找到一个插值数据的函数，即对所有$i \\in \\{1, 2, 3, 4\\}$都有$f(x_i) = y_i$，那么每个平方误差项都将为零。这将导致经验风险为$R_{\\text{emp}}(f;D) = 0$。由于风险是非负项的和，所以$0$是可能的最小值。\n\n对于我们的$4$个数据点，插值条件$f(x_i) = y_i$构成了一个关于$4$个未知系数的$4$个线性方程组。该方程组的矩阵是一个基于不同输入点$x_1=0, x_2=1, x_3=2, x_4=3$的范德蒙矩阵。由于这些点是不同的，该矩阵是可逆的，这保证了存在一个唯一的、次数至多为$3$的多项式穿过所有四个数据点。令这个多项式为$\\hat{f}_{\\text{ERM},3}$。对于这个多项式，$R_{\\text{emp}}(\\hat{f}_{\\text{ERM},3};D) = \\frac{1}{4}\\sum_{i=1}^{4} (y_i - y_i)^2 = 0$。因此，在$\\mathcal{H}_3$上的ERM可以实现零经验风险。\n\n$y$值的序列是$0, 3, 0, 3$。一个插值这些点的函数必须在$x=0$时从$y=0$上升到$x=1$时的$y=3$，然后在$x=2$时下降回$y=0$，最后在$x=3$时再次上升到$y=3$。这种上升-下降-上升的行为是振荡函数的特征。这样的函数在区间$(0,3)$内必须至少有两个转折点（极值点）。这与简单的单调函数形成对比，是过拟合的典型标志，即模型过于复杂，不仅捕捉了潜在趋势，也捕捉了样本中的噪声或随机波动。\n\n任务2：线性预测器$\\hat{f}_{\\text{ERM},1}$\n我们想要找到线性函数$\\hat{f}_{\\text{ERM},1}(x) = ax+b$来最小化$R_{\\text{emp}}(f;D)$。这等价于最小化平方误差之和，$S(a,b) = \\sum_{i=1}^{4} (y_i - (ax_i+b))^2$。我们通过将关于$a$和$b$的偏导数设为零来找到最小值，从而得到正规方程。\n\n$\\frac{\\partial S}{\\partial b} = \\sum_{i=1}^4 2(y_i - ax_i - b)(-1) = 0$，这可以简化为$\\sum(y_i - ax_i - b) = 0$，或$(\\sum y_i) - a(\\sum x_i) - 4b = 0$。\n$\\sum x_i = 0+1+2+3 = 6$以及$\\sum y_i = 0+3+0+3=6$，我们得到$6 - 6a - 4b = 0$，简化为$3a+2b=3$。\n\n$\\frac{\\partial S}{\\partial a} = \\sum_{i=1}^4 2(y_i - ax_i - b)(-x_i) = 0$，这可以简化为$\\sum(y_i x_i - ax_i^2 - bx_i) = 0$，或$(\\sum x_i y_i) - a(\\sum x_i^2) - b(\\sum x_i) = 0$。\n$\\sum x_i = 6$。\n$\\sum x_i^2 = 0^2+1^2+2^2+3^2 = 0+1+4+9=14$。\n$\\sum x_i y_i = (0)(0) + (1)(3) + (2)(0) + (3)(3) = 0+3+0+9=12$。\n所以，$12 - 14a - 6b = 0$，简化为$7a+3b=6$。\n\n我们求解这个线性方程组：\n1) $3a+2b=3$\n2) $7a+3b=6$\n将(1)乘以$3$，(2)乘以$2$：\n$9a+6b=9$\n$14a+6b=12$\n用第二个方程减去第一个方程得到$5a=3 \\implies a = \\frac{3}{5}$。\n代入(1)中：$3(\\frac{3}{5}) + 2b = 3 \\implies \\frac{9}{5} + 2b = 3 \\implies 2b = 3 - \\frac{9}{5} = \\frac{6}{5} \\implies b = \\frac{3}{5}$。\n所以，线性预测器是$\\hat{f}_{\\text{ERM},1}(x) = \\frac{3}{5}x + \\frac{3}{5}$。\n\n现在我们评估其经验风险，$R_{\\text{emp}}(\\hat{f}_{\\text{ERM},1};D)$:\n$f(x_1=0) = \\frac{3}{5}$，误差$e_1 = 0 - \\frac{3}{5} = -\\frac{3}{5}$。\n$f(x_2=1) = \\frac{6}{5}$，误差$e_2 = 3 - \\frac{6}{5} = \\frac{9}{5}$。\n$f(x_3=2) = \\frac{9}{5}$，误差$e_3 = 0 - \\frac{9}{5} = -\\frac{9}{5}$。\n$f(x_4=3) = \\frac{12}{5}$，误差$e_4 = 3 - \\frac{12}{5} = \\frac{3}{5}$。\n平方误差之和是$\\sum e_i^2 = (-\\frac{3}{5})^2 + (\\frac{9}{5})^2 + (-\\frac{9}{5})^2 + (\\frac{3}{5})^2 = \\frac{9}{25} + \\frac{81}{25} + \\frac{81}{25} + \\frac{9}{25} = \\frac{180}{25} = \\frac{36}{5}$。\n$R_{\\text{emp}}(\\hat{f}_{\\text{ERM},1};D) = \\frac{1}{4} \\sum e_i^2 = \\frac{1}{4} \\cdot \\frac{36}{5} = \\frac{9}{5}$。\n\n接下来，我们评估其惩罚项，$\\mathrm{TV}(\\hat{f}_{\\text{ERM},1};D)$:\n输入$x_i$是等距的。对于任何线性函数$f(x)=ax+b$和等距点$x_{i-1}, x_i, x_{i+1}$，其离散二阶差分为零：\n$\\Delta^2 f(x_i) = f(x_{i+1}) - 2f(x_i) + f(x_{i-1}) = (a x_{i+1}+b) - 2(a x_i+b) + (a x_{i-1}+b) = a(x_{i+1}-2x_i+x_{i-1}) = a((x_i+h)-2x_i+(x_i-h)) = 0$，其中$h$是间距。\n因此，$\\mathrm{TV}(\\hat{f}_{\\text{ERM},1};D) = |\\Delta^2 f(x_2)| + |\\Delta^2 f(x_3)| = |0| + |0| = 0$。\n\n任务3：三次插值$\\hat{f}_{\\text{ERM},3}$\n根据定义，这个函数对数据进行插值，所以对所有$i$都有$\\hat{f}_{\\text{ERM},3}(x_i) = y_i$。\n其经验风险为$R_{\\text{emp}}(\\hat{f}_{\\text{ERM},3};D) = \\frac{1}{4}\\sum_{i=1}^{4} (y_i - \\hat{f}_{\\text{ERM},3}(x_i))^2 = \\frac{1}{4}\\sum_{i=1}^{4} (y_i - y_i)^2 = 0$。\n\n为了评估其惩罚项$\\mathrm{TV}(\\hat{f}_{\\text{ERM},3};D)$，我们用$y_i$值代替$f(x_i)$：\n$f(x_1)=y_1=0$, $f(x_2)=y_2=3$, $f(x_3)=y_3=0$, $f(x_4)=y_4=3$。\n$\\Delta^2 f(x_2) = f(x_3) - 2f(x_2) + f(x_1) = y_3 - 2y_2 + y_1 = 0 - 2(3) + 0 = -6$。\n$\\Delta^2 f(x_3) = f(x_4) - 2f(x_3) + f(x_2) = y_4 - 2y_3 + y_2 = 3 - 2(0) + 3 = 6$。\n$\\mathrm{TV}(\\hat{f}_{\\text{ERM},3};D) = |\\Delta^2 f(x_2)| + |\\Delta^2 f(x_3)| = |-6| + |6| = 6+6=12$。\n\n任务4：临界值$\\lambda^{\\star}$\nSRM目标函数是$J_{\\lambda}(f;D) = R_{\\text{emp}}(f;D) + \\lambda \\mathrm{TV}(f;D)$。\n我们寻找值$\\lambda^{\\star}$，使得两个模型的目标函数值相等：\n$J_{\\lambda^{\\star}}(\\hat{f}_{\\text{ERM},1};D) = J_{\\lambda^{\\star}}(\\hat{f}_{\\text{ERM},3};D)$。\n\n对于线性模型$\\hat{f}_{\\text{ERM},1}$：\n$J_{\\lambda}(\\hat{f}_{\\text{ERM},1};D) = R_{\\text{emp}}(\\hat{f}_{\\text{ERM},1};D) + \\lambda\\mathrm{TV}(\\hat{f}_{\\text{ERM},1};D) = \\frac{9}{5} + \\lambda \\cdot 0 = \\frac{9}{5}$。\n\n对于三次模型$\\hat{f}_{\\text{ERM},3}$：\n$J_{\\lambda}(\\hat{f}_{\\text{ERM},3};D) = R_{\\text{emp}}(\\hat{f}_{\\text{ERM},3};D) + \\lambda\\mathrm{TV}(\\hat{f}_{\\text{ERM},3};D) = 0 + \\lambda \\cdot 12 = 12\\lambda$。\n\n令它们在$\\lambda=\\lambda^{\\star}$时相等：\n$\\frac{9}{5} = 12\\lambda^{\\star}$\n求解$\\lambda^{\\star}$：\n$\\lambda^{\\star} = \\frac{1}{12} \\cdot \\frac{9}{5} = \\frac{9}{60}$。\n化简分数：\n$\\lambda^{\\star} = \\frac{3 \\cdot 3}{3 \\cdot 20} = \\frac{3}{20}$。\n\n这个$\\lambda^{\\star}$值代表了无差异点。对于$\\lambda  \\lambda^{\\star}$，SRM目标函数更偏好于更复杂的三次模型（因为$12\\lambda  9/5$）。对于$\\lambda > \\lambda^{\\star}$，它更偏好于更简单的线性模型（因为$12\\lambda > 9/5$），对三次拟合的振荡性质的惩罚，重于对其完美数据拟合的奖励。", "answer": "$$\\boxed{\\frac{3}{20}}$$", "id": "3118270"}, {"introduction": "在惩罚复杂度的直观思想之上，本练习引入了一个更形式化、更强大的统计学习理论工具：经验Rademacher复杂度。通过一个涉及多项式回归的编程练习，你将学习如何近似计算这种依赖于数据的复杂度度量，并用它来选择最优的模型阶数[@problem_id:3118242]。这个练习超越了简单的惩罚项，带你了解一种量化模型类别拟合随机噪声能力的原则性方法，而这正是SRM框架的核心。你还将研究一个重要的实际问题：特征缩放如何影响模型选择。", "problem": "考虑一维多项式回归，其假设类由多项式次数索引。对于每个整数次数 $k \\in \\{0,1,2,3,4\\}$，定义假设类 $\\mathcal{H}_k$，它由函数 $f_{\\boldsymbol{w}}(x) = \\sum_{j=0}^{k} w_j x^j$ 组成，其中系数向量 $\\boldsymbol{w} \\in \\mathbb{R}^{k+1}$ 满足欧几里得范数约束 $\\|\\boldsymbol{w}\\|_2 \\leq B$。使用以下基本定义：经验风险最小化 (ERM) 的定义、结构风险最小化 (SRM) 的定义，以及经验 Rademacher 复杂度的定义。对于每个 $k$，ERM 选择一个系数向量 $\\hat{\\boldsymbol{w}}_k$，该向量最小化平方损失下的经验风险。SRM 通过最小化经验风险与一个数据依赖的复杂度项之间的权衡来选择次数 $k$。\n\n数据集按如下方式确定性地构建。设样本大小为 $n = 20$。输入 $x_i$ 在区间 $[-1,1]$ 内均匀分布，即 $x_i = -1 + \\frac{2(i-1)}{n-1}$，其中 $i = 1,2,\\dots,n$。输出由一个带噪声的二次模型生成：$y_i = 1 - 2 x_i + 0.5 x_i^2 + \\epsilon_i$，其中 $\\epsilon_i$ 是独立的的高斯噪声变量，满足 $\\epsilon_i \\sim \\mathcal{N}(0,\\sigma^2)$ 且 $\\sigma = 0.1$。使用固定的随机种子 $123$ 来生成 $(\\epsilon_i)_{i=1}^n$ 以确保可复现性。\n\n对于任意缩放因子 $s  0$，定义缩放后的输入 $x_i^{(s)} = s \\cdot x_i$。对于每个次数 $k$，定义特征映射 $\\boldsymbol{\\phi}_k(x) = (x^0, x^1, \\dots, x^k)^\\top \\in \\mathbb{R}^{k+1}$ 以及相应的设计矩阵 $\\Phi_k^{(s)} \\in \\mathbb{R}^{n \\times (k+1)}$，其第 $i$ 行为 $\\boldsymbol{\\phi}_k(x_i^{(s)})^\\top$。\n\n对于每个 $k$ 和缩放因子 $s$，计算：\n- 在 ERM 解下，平方损失的经验风险，\n  $$\\hat{R}_k^{(s)} = \\frac{1}{n} \\sum_{i=1}^{n} \\left(y_i - \\hat{\\boldsymbol{w}}_k^{(s)\\top} \\boldsymbol{\\phi}_k(x_i^{(s)})\\right)^2,$$\n  其中 $\\hat{\\boldsymbol{w}}_k^{(s)}$ 是经验风险在 $\\boldsymbol{w} \\in \\mathbb{R}^{k+1}$ 上的任意最小化子，并且可以在多个最小化子中任意选择以打破平局。\n- 在缩放数据集 $(x_i^{(s)})_{i=1}^n$ 上评估的假设类 $\\mathcal{H}_k$ 的经验 Rademacher 复杂度，\n  $$\\hat{\\mathfrak{R}}_n(\\mathcal{H}_k; s) = \\mathbb{E}_{\\boldsymbol{\\sigma}} \\left[ \\sup_{\\|\\boldsymbol{w}\\|_2 \\leq B} \\frac{1}{n} \\sum_{i=1}^{n} \\sigma_i \\, \\boldsymbol{w}^\\top \\boldsymbol{\\phi}_k(x_i^{(s)}) \\right],$$\n  其中 $\\boldsymbol{\\sigma} = (\\sigma_1,\\dots,\\sigma_n)$ 具有独立的 Rademacher 条目，满足 $\\mathbb{P}(\\sigma_i = 1) = \\mathbb{P}(\\sigma_i = -1) = \\frac{1}{2}$，且 $B = 1$。通过对 $M = 2000$ 次独立的 $\\boldsymbol{\\sigma}$ 抽样进行蒙特卡洛平均来近似这个期望，使用固定的随机种子 $999$ 以确保可复现性。\n\n将次数 $k$ 在缩放因子为 $s$ 和权衡参数 $c  0$ 时的 SRM 选择准则定义为\n$$\\mathrm{SRM}(k; s,c) = \\hat{R}_k^{(s)} + c \\cdot \\hat{\\mathfrak{R}}_n(\\mathcal{H}_k; s).$$\n选择次数\n$$k^*(s,c) = \\arg\\min_{k \\in \\{0,1,2,3,4\\}} \\mathrm{SRM}(k; s,c),$$\n若出现平局，则选择较小的 $k$。\n\n使用计算出的经验 Rademacher 复杂度值，讨论特征缩放 $x \\mapsto s x$ 如何通过特征向量 $\\boldsymbol{\\phi}_k(x_i^{(s)})$ 的幅值以及出现在上确界中的范数来影响 $\\hat{\\mathfrak{R}}_n(\\mathcal{H}_k; s)$，并进而讨论这如何影响 SRM 选择的次数 $k^*(s,c)$。\n\n实现一个完整的、可运行的程序，该程序：\n- 根据指定的 $n=20$、$x_i \\in [-1,1]$、$\\sigma = 0.1$ 以及用于噪声的随机种子 $123$ 来构建数据集 $(x_i,y_i)_{i=1}^n$。\n- 对于每个次数 $k \\in \\{0,1,2,3,4\\}$、测试套件中的每个缩放因子 $s$ 和每个 $c$，计算 $\\hat{R}_k^{(s)}$，使用随机种子 $999$ 和 $M = 2000$ 个蒙特卡洛样本来近似 $\\hat{\\mathfrak{R}}_n(\\mathcal{H}_k; s)$，然后计算 $\\mathrm{SRM}(k; s,c)$ 和 $k^*(s,c)$。\n- 生成单行输出，其中包含所有测试用例的选择次数 $k^*(s,c)$，格式为方括号内以逗号分隔的列表。\n\n使用以下测试套件，它探讨了不同的方面：\n- 案例 1：$s = 1.0$, $c = 0.05$ (基线缩放和中等复杂度惩罚；理想路径)。\n- 案例 2：$s = 0.5$, $c = 0.05$ (减小的特征幅值；复杂度降低)。\n- 案例 3：$s = 2.0$, $c = 0.05$ (增大的特征幅值；复杂度增加)。\n- 案例 4：$s = 1.0$, $c = 0.5$ (强复杂度惩罚；趋向于选择更低次数的边界情况)。\n- 案例 5：$s = 0.1$, $c = 0.05$ (非常小的缩放；复杂度显著降低)。\n- 案例 6：$s = 1.0$, $c = 0.0$ (纯粹的 ERM，无复杂度惩罚；边界情况)。\n\n你的程序应生成单行输出，其中包含结果，格式为方括号内以逗号分隔的列表（例如 $[r_1,r_2,r_3,r_4,r_5,r_6]$，其中每个 $r_i$ 是一个整数次数）。", "solution": "### 方法论与讨论\n\n该问题的解决策略遵循结构风险最小化（SRM）框架，通过平衡经验风险（模型拟合度）与模型复杂度来选择最优的多项式次数 $k$。\n\n**1. 经验风险计算：**\n对于每个多项式次数 $k \\in \\{0, 1, 2, 3, 4\\}$ 和缩放因子 $s$，我们首先构建相应的设计矩阵 $\\Phi_k^{(s)}$。然后，通过求解标准的无约束线性最小二乘问题来找到最优权重向量 $\\hat{\\boldsymbol{w}}_k^{(s)}$，其解由正规方程给出，在数值上通过 `numpy.linalg.lstsq` 实现。最小化的经验风险 $\\hat{R}_k^{(s)}$ 即为在该最优权重下的均方误差。\n\n**2. Rademacher 复杂度计算：**\n模型复杂度由经验 Rademacher 复杂度 $\\hat{\\mathfrak{R}}_n(\\mathcal{H}_k; s)$ 度量，它衡量了假设类 $\\mathcal{H}_k$ 拟合随机噪声的能力。其计算利用了闭式解：\n$$ \\hat{\\mathfrak{R}}_n(\\mathcal{H}_k; s) = \\mathbb{E}_{\\boldsymbol{\\sigma}} \\left[ \\frac{B}{n} \\|\\Phi_k^{(s)\\top} \\boldsymbol{\\sigma}\\|_2 \\right] $$\n我们通过生成 $M=2000$ 个独立的 Rademacher 随机向量 $\\boldsymbol{\\sigma}$，并计算 $\\frac{B}{n} \\|\\Phi_k^{(s)\\top} \\boldsymbol{\\sigma}\\|_2$ 的蒙特卡洛平均值来近似这个期望。\n\n**3. SRM 模型选择：**\n对于给定的权衡参数 $c$，SRM 准则定义为 $\\mathrm{SRM}(k; s, c) = \\hat{R}_k^{(s)} + c \\cdot \\hat{\\mathfrak{R}}_n(\\mathcal{H}_k; s)$。我们为每个 $k$ 计算该值，并选择使 SRM 准则最小的次数 $k^*$ 作为最优模型。\n\n**4. 特征缩放的影响分析：**\n特征缩放因子 $s$ 对 Rademacher 复杂度有显著影响，进而影响 SRM 的模型选择：\n- **复杂度与缩放的关系：** Rademacher 复杂度与范数 $\\|\\Phi_k^{(s)\\top} \\boldsymbol{\\sigma}\\|_2$ 成正比，而设计矩阵 $\\Phi_k^{(s)}$ 的元素是 $(sx_i)^j$。当 $s > 1$ 时，高阶特征 $(j > 1)$ 的值被放大，导致 $\\Phi_k^{(s)}$ 的范数增加，从而增加了高阶模型（大 $k$）的复杂度惩罚。反之，当 $s  1$ 时，特征值被压缩，降低了复杂度惩罚。\n- **对模型选择的影响：**\n    - **增大 $s$ (例如 $s=2.0$)** 会使 SRM 更倾向于惩罚复杂模型，因此会选择次数更低的模型以避免过拟合。\n    - **减小 $s$ (例如 $s=0.5$ 或 $s=0.1$)** 会使高阶模型的复杂度“看起来”更低，SRM 准则中的惩罚项变小。这使得模型选择更依赖于经验风险 $\\hat{R}_k^{(s)}$，可能会导致选择次数更高的模型，因为它们能更好地拟合数据。\n    - **增大 $c$ (例如 $c=0.5$)** 增加了复杂度惩罚的权重，使 SRM 强烈偏好更简单的模型，通常会选择 $k=0$ 或 $k=1$。\n    - **$c=0$** 完全移除了复杂度惩罚，SRM 退化为 ERM，它会选择经验风险最低的模型。对于嵌套的多项式类，这通常是可用的最高次数（此处为 $k=4$），因为它有最多的自由度来拟合数据，包括噪声。\n\n由于真实数据由二次函数生成，我们期望在基准情况下（如 $s=1.0, c=0.05$），SRM 能够正确识别出 $k=2$ 是最优选择。其他情况将展示上述权衡如何改变选择结果。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the polynomial regression model selection problem using SRM.\n    \"\"\"\n    # Define problem parameters\n    n = 20\n    degrees = [0, 1, 2, 3, 4]\n    B = 1.0\n    sigma = 0.1\n    noise_seed = 123\n    monte_carlo_samples = 2000\n    monte_carlo_seed = 999\n\n    # Define the test suite\n    test_cases = [\n        # (s, c)\n        (1.0, 0.05),  # Case 1: Baseline\n        (0.5, 0.05),  # Case 2: Reduced feature magnitudes\n        (2.0, 0.05),  # Case 3: Increased feature magnitudes\n        (1.0, 0.5),   # Case 4: Strong complexity penalty\n        (0.1, 0.05),  # Case 5: Very small scaling\n        (1.0, 0.0),   # Case 6: Pure ERM (no penalty)\n    ]\n\n    # Generate the dataset\n    x = np.linspace(-1.0, 1.0, n)\n    noise_rng = np.random.RandomState(noise_seed)\n    epsilon = noise_rng.normal(0, sigma, n)\n    y_true = 1 - 2 * x + 0.5 * x**2\n    y = y_true + epsilon\n\n    # Prepare for Monte Carlo approximation of Rademacher complexity\n    rademacher_rng = np.random.RandomState(monte_carlo_seed)\n    sigma_matrix = rademacher_rng.choice([-1, 1], size=(monte_carlo_samples, n))\n\n    results = []\n    # Loop over all test cases\n    for s, c in test_cases:\n        srm_values = []\n        # Loop over all degrees k\n        for k in degrees:\n            # 1. Construct scaled design matrix\n            x_scaled = s * x\n            # np.vander creates columns in decreasing power order by default.\n            # a Vandermonde matrix of order N-1 for a k-th degree polynomial. so N=k+1.\n            # increasing=True puts powers as (x^0, x^1, ..., x^k)\n            phi = np.vander(x_scaled, N=k + 1, increasing=True)\n            \n            # 2. Compute Empirical Risk (ERM)\n            # Find w_hat using unconstrained least squares\n            w_hat, residuals, _, _ = np.linalg.lstsq(phi, y, rcond=None)\n            \n            # Calculate predictions and empirical risk\n            y_hat = phi @ w_hat\n            emp_risk = np.mean((y - y_hat)**2)\n            \n            # 3. Approximate Empirical Rademacher Complexity\n            # phi.T has shape (k+1, n)\n            # sigma_matrix.T has shape (n, M)\n            # The product has shape (k+1, M)\n            # Each column is phi.T @ sigma_j\n            term_inside_norm = phi.T @ sigma_matrix.T\n            # Take L2 norm over axis 0 (columns) - shape (M,)\n            norms = np.linalg.norm(term_inside_norm, axis=0)\n            \n            rad_complexity = np.mean((B / n) * norms)\n            \n            # 4. Compute SRM criterion\n            srm = emp_risk + c * rad_complexity\n            srm_values.append(srm)\n        \n        # 5. Select best k\n        # np.argmin breaks ties by taking the first occurrence (smallest k)\n        k_star = degrees[np.argmin(srm_values)]\n        results.append(k_star)\n\n    # Print results in the required format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```", "id": "3118242"}, {"introduction": "结构风险最小化是一个通用原则，其应用范围超出了在多项式等静态模型之间进行选择。这个动手编程实践将展示如何应用SRM来规范算法本身的复杂度，并以流行的AdaBoost算法为例。你将把提升（boosting）的轮数视为“结构”，并使用基于分类间隔（margin）分布的泛化界来实现一种形式的提前停止[@problem_id:3118279]。通过将其结果与纯粹基于ERM的方法（最小化训练误差）进行比较，你将更深刻地体会到SRM如何帮助防止迭代算法中的过拟合，并促进产生具有更好泛化性能的解。", "problem": "您将实现并比较经验风险最小化（ERM）和结构风险最小化（SRM），应用于早停的自适应提升（AdaBoost）算法。其中，提升轮数 $T$ 作为结构参数。问题背景为二元分类，标签在 $\\{-1,+1\\}$ 中，使用实数轴上的决策桩作为基学习器类别。您必须从经验风险、提升方法、间隔以及一个基于间隔分布且经过充分检验的泛化界的核心定义出发。您的程序将训练 AdaBoost，直至一个固定的最大轮数 $T_{\\max}$。对于每个 $T \\in \\{1,2,\\dots,T_{\\max}\\}$，程序需要计算训练样本上的归一化间隔分布，并使用基于间隔的泛化界通过 SRM 方法选择 $T$。您还将通过最小化训练误差的 ERM 方法选择 $T$，并比较这两种方法选择的 $T$ 值。所有计算均为纯数学运算，不涉及物理单位。\n\n需要使用的基本概念和定义：\n- 二元分类样本 $S=\\{(x_i,y_i)\\}_{i=1}^m$，其中 $x_i \\in \\mathbb{R}$ 且 $y_i \\in \\{-1,+1\\}$。在第 $T$ 轮的经验风险（训练误差）是 0-1 损失的平均值：$\\frac{1}{m}\\sum_{i=1}^m \\mathbf{1}\\{y_i \\neq \\mathrm{sign}(F_T(x_i))\\}$，其中如果 $z \\ge 0$ 则 $\\mathrm{sign}(z)=+1$，否则为 $-1$。\n- 基础假设类 $\\mathcal{H}$ 是单变量决策桩的集合，$h_{s,p}(x) = \\mathrm{sign}(p(x-s))$，其中 $s \\in \\mathbb{R}$ 是一个阈值，$p \\in \\{-1,+1\\}$ 是一个极性。在样本权重 $D$ 下，$h$ 的加权经验误差为 $\\sum_{i=1}^m D_i \\mathbf{1}\\{h(x_i)\\neq y_i\\}$。\n- AdaBoost 通过迭代地选择 $h_t \\in \\mathcal{H}$ 来最小化当前加权误差，从而构建一个加权投票分类器 $F_T(x)=\\sum_{t=1}^T \\alpha_t h_t(x)$。其中，设置 $\\alpha_t=\\tfrac{1}{2}\\ln\\!\\big(\\frac{1-\\varepsilon_t}{\\varepsilon_t}\\big)$（$\\varepsilon_t$ 是 $h_t$ 的加权误差），并更新权重 $D_i \\propto D_i \\exp(-\\alpha_t y_i h_t(x_i))$。最终的分类器是 $\\mathrm{sign}(F_T(x))$。\n- 在第 $T$ 轮，样本 $i$ 的归一化间隔为 $\\rho_i(T)=\\frac{y_i F_T(x_i)}{\\sum_{t=1}^T |\\alpha_t|}$，对于 $T \\ge 1$，其值位于 $[-1,+1]$ 区间内。\n- 您将使用以下经过充分检验的基于间隔分布的泛化界：对于任意 $\\theta \\in (0,1]$，在样本的抽取上，以至少 $1-\\delta$ 的概率，$\\mathrm{sign}(F_T)$ 的真实分类误差至多为\n$$\nB(T,\\theta) \\;=\\; \\frac{1}{m}\\sum_{i=1}^m \\mathbf{1}\\{\\rho_i(T) \\le \\theta\\} \\;+\\; \\sqrt{\\frac{ 2 d \\ln\\!\\big(\\frac{2 e m}{d}\\big) + \\ln\\!\\big(\\frac{2}{\\delta}\\big)}{m\\,\\theta^2}},\n$$\n其中 $d$ 是基础假设类 $\\mathcal{H}$ 的 Vapnik–Chervonenkis (VC) 维，$m$ 是样本大小，$e$ 是自然对数的底数，$\\delta \\in (0,1)$ 是一个置信度参数。对于实数轴上的决策桩，您必须使用 $d=1$。当结构是提升轮数 $T$ 时，该界作为 SRM 准则。\n\n您的任务：\n1. 严格按照上述定义，使用决策桩作为基学习器实现 AdaBoost，运行固定的最大轮数 $T_{\\max}$。为构建决策桩，您必须在给定的阈值 $s$ 上进行搜索，这些阈值包括排序后不同训练输入之间的中点，以及两个极端阈值（一个严格小于最小输入，一个严格大于最大输入）。您必须考虑两种极性 $p \\in \\{-1,+1\\}$。对于任何出现的 $\\varepsilon_t=0$ 或 $\\varepsilon_t=1$ 的情况，需将该值裁剪到严格在 $(0,1)$ 范围内，以确保 $\\alpha_t$ 是有限的。\n2. 对于每个 $T \\in \\{1,2,\\dots,T_{\\max}\\}$，计算经验训练误差和归一化间隔 $\\rho_i(T)$，$i \\in \\{1,\\dots,m\\}$。\n3. 对于提供的间隔阈值网格 $\\Theta$，为每个 $\\theta \\in \\Theta$ 计算 $B(T,\\theta)$，并将在 $T \\in \\{1,\\dots,T_{\\max}\\}$ 上最小化 $\\min_{\\theta \\in \\Theta} B(T,\\theta)$ 的最小 $T$ 定义为 SRM 选择的 $T_{\\mathrm{SRM}}$。将在 $T \\in \\{1,\\dots,T_{\\max}\\}$ 上最小化经验训练误差的最小 $T$ 定义为 ERM 选择的 $T_{\\mathrm{ERM}}$。\n\n数据和测试套件：\n- 在实数轴上使用两个确定性数据集。定义 $m=20$，输入为 $x_i = -2.85 + 0.3\\, i$，其中 $i \\in \\{0,1,2,\\dots,19\\}$。定义标签为 $y_i = \\mathrm{sign}(\\sin(2.5\\, x_i))$，并遵循 $\\mathrm{sign}(0)=+1$ 的约定。\n- 数据集 $\\mathrm{A}$ 是如上定义的 $S_{\\mathrm{A}}=\\{(x_i,y_i)\\}_{i=1}^{20}$。\n- 数据集 $\\mathrm{B}$ 是通过向 $S_{\\mathrm{A}}$ 中引入标签噪声得到的：翻转基于 1 的索引为 3 和 17 的标签，即用 $-y_3$ 替换 $y_3$，用 $-y_{17}$ 替换 $y_{17}$，其他标签保持不变。\n- 对两个数据集均使用 VC 维 $d=1$。\n- 使用间隔阈值网格 $\\Theta=\\{0.05,\\,0.1,\\,0.2,\\,0.3,\\,0.4\\}$。\n- 使用以下三个测试用例，每个用例指定为一个元组 $(\\text{数据集}, T_{\\max}, \\delta)$：\n    - 用例 1：$(\\mathrm{A},\\,30,\\,0.1)$\n    - 用例 2：$(\\mathrm{A},\\,30,\\,0.01)$\n    - 用例 3：$(\\mathrm{B},\\,30,\\,0.1)$\n\n要求的最终输出格式：\n- 您的程序必须生成单行输出，其中包含一个扁平的、逗号分隔的六个整数列表，并用方括号括起来。该列表必须是 $[T_{\\mathrm{SRM}}^{(1)}, T_{\\mathrm{ERM}}^{(1)}, T_{\\mathrm{SRM}}^{(2)}, T_{\\mathrm{ERM}}^{(2)}, T_{\\mathrm{SRM}}^{(3)}, T_{\\mathrm{ERM}}^{(3)}]$，其中上标表示按上述顺序列出的用例索引。\n\n注意和约束：\n- 所有数学计算必须遵循上述定义。由于使用了 $\\sin(\\cdot)$，角度（如果涉及）默认为弧度。\n- 当出现多个最小值时，必须选择最小的 $T$ 来打破平局。\n- 您的实现必须是完全确定性的。", "solution": "### 算法解析\n\n本问题的核心是实现 AdaBoost 算法，并在此过程中为每一轮提升（$T=1, \\dots, T_{\\max}$）评估两种不同的模型选择准则：经验风险最小化（ERM）和结构风险最小化（SRM）。\n\n**1. AdaBoost 核心逻辑**\n我们从一个均匀的样本权重分布 $D$ 开始。在每一轮 $t$：\n- **寻找最佳弱学习器：** 我们在所有可能的决策桩（由阈值 $s$ 和极性 $p$ 定义）中进行搜索，找到一个能最小化当前加权分类误差 $\\varepsilon_t$ 的弱学习器 $h_t$。\n- **计算学习器权重：** 基于其误差 $\\varepsilon_t$，为 $h_t$ 分配一个权重 $\\alpha_t = \\frac{1}{2}\\ln(\\frac{1-\\varepsilon_t}{\\varepsilon_t})$。误差越小，权重越大。\n- **更新样本权重：** 增加被 $h_t$ 错误分类的样本的权重，使下一轮的弱学习器更关注这些“困难”样本。\n- **累积模型：** 每一轮的模型 $F_T(x)$ 是到目前为止所有弱学习器 $h_t$ 的加权和：$F_T(x) = \\sum_{t=1}^T \\alpha_t h_t(x)$。\n\n**2. ERM 准则**\n在每一轮 $T$ 结束时，我们计算集成分类器 $F_T(x)$ 在整个训练集上的 0-1 训练误差（即被错误分类的样本比例）。$T_{\\mathrm{ERM}}$ 就是使这个训练误差达到最小值的最小轮数 $T$。\n\n**3. SRM 准则**\nSRM 准则基于一个更复杂的泛化误差界，该界不仅考虑了拟合，还考虑了分类的“置信度”，由间隔（margin）来衡量。\n- **间隔计算：** 对于每个样本 $(x_i, y_i)$，其归一化间隔 $\\rho_i(T)$ 是衡量其被正确分类且距离决策边界有多远的指标。\n- **泛化界：** 我们使用问题中提供的基于间隔的泛化界 $B(T, \\theta)$。这个界包含两部分：\n    - **经验项：** 训练集中间隔小于阈值 $\\theta$ 的样本比例。这惩罚了那些虽然分类正确但置信度不高的预测。\n    - **复杂度项：** 一个惩罚模型复杂性的项，它随着 $\\theta$ 的减小而增大。\n- **SRM 选择：** 对于每一轮 $T$，我们通过在给定的 $\\theta$ 网格上搜索来找到最紧的界，即 $R_{\\mathrm{SRM}}(T) = \\min_{\\theta \\in \\Theta} B(T, \\theta)$。$T_{\\mathrm{SRM}}$ 就是使这个最小界 $R_{\\mathrm{SRM}}(T)$ 达到最小值的最小轮数 $T$。\n\n通过比较 $T_{\\mathrm{ERM}}$ 和 $T_{\\mathrm{SRM}}$，我们可以观察到 SRM 如何通过惩罚低置信度的分类来选择一个可能更早停止、泛化能力更好的模型，尤其是在有噪声的数据集上。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and compares Empirical Risk Minimization (ERM) and Structural Risk Minimization (SRM)\n    for early-stopped AdaBoost with decision stumps.\n    \"\"\"\n\n    # Define custom sign function as per problem statement: sign(z)=+1 for z = 0, -1 otherwise.\n    def my_sign(arr: np.ndarray) - np.ndarray:\n        return np.where(arr = 0, 1.0, -1.0)\n\n    # Function to generate datasets as specified.\n    def generate_data(dataset_id: str, m: int) - tuple[np.ndarray, np.ndarray]:\n        X = -2.85 + 0.3 * np.arange(m)\n        Y = my_sign(np.sin(2.5 * X))\n        if dataset_id == 'B':\n            # Flip labels at 1-based indices 3 and 17 (0-based 2 and 16).\n            Y[2] *= -1\n            Y[16] *= -1\n        return X, Y\n\n    # Function to predict using a single decision stump.\n    def predict_stump(stump: dict, X: np.ndarray) - np.ndarray:\n        s = stump['s']\n        p = stump['p']\n        return my_sign(p * (X - s))\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        ('A', 30, 0.1),\n        ('A', 30, 0.01),\n        ('B', 30, 0.1),\n    ]\n\n    # Global parameters defined in the problem.\n    m = 20\n    # The problem mandates using d=1.\n    d = 1.0\n    theta_grid = np.array([0.05, 0.1, 0.2, 0.3, 0.4])\n    \n    final_results = []\n\n    for case in test_cases:\n        dataset_id, T_max, delta = case\n        \n        # 1. Generate data for the current case.\n        X, Y = generate_data(dataset_id, m)\n\n        # 2. Prepare for AdaBoost loop.\n        # Initialize sample weights.\n        D = np.ones(m) / m\n        \n        # Generate all possible thresholds once to optimize the inner loop.\n        sorted_unique_X = np.unique(X)\n        thresholds = (sorted_unique_X[:-1] + sorted_unique_X[1:]) / 2.0\n        # Add extreme thresholds.\n        thresholds = np.append(thresholds, [sorted_unique_X[0] - 1.0, sorted_unique_X[-1] + 1.0])\n\n        # Storage for results at each round T.\n        emp_errors_per_T = []\n        srm_risks_per_T = []\n        \n        # Cumulative classifier scores F_T(x_i) for each sample point.\n        F_T_X = np.zeros(m)\n        # Cumulative sum of |alpha_t|, used for margin normalization.\n        sum_abs_alpha = 0.0\n\n        # 3. Main AdaBoost loop. In each iteration t, we find the best weak learner,\n        #    and then compute the ERM and SRM criteria for the combined classifier of size T=t.\n        for T in range(1, T_max + 1):\n            # a. Find the best decision stump (weak learner).\n            min_error = float('inf')\n            best_stump = None\n\n            for p in [-1, 1]:\n                for s in thresholds:\n                    stump = {'s': s, 'p': p}\n                    h_preds = predict_stump(stump, X)\n                    # Calculate weighted error.\n                    error = np.sum(D[h_preds != Y])\n                    \n                    if error  min_error:\n                        min_error = error\n                        best_stump = stump\n            \n            # b. Calculate the weight alpha_t for the best stump.\n            epsilon_t = min_error\n            # Clip epsilon to avoid log(0) or division by zero, as per instructions.\n            epsilon_t = np.clip(epsilon_t, 1e-15, 1 - 1e-15)\n            alpha_t = 0.5 * np.log((1.0 - epsilon_t) / epsilon_t)\n\n            # c. Update sample weights D.\n            h_t_preds = predict_stump(best_stump, X)\n            D *= np.exp(-alpha_t * Y * h_t_preds)\n            D /= np.sum(D)\n\n            # d. Update cumulative scores and alpha sum for this round T.\n            F_T_X += alpha_t * h_t_preds\n            # alpha_t is non-negative since epsilon_t = 0.5.\n            sum_abs_alpha += alpha_t \n\n            # e. Calculate ERM criterion for round T.\n            y_pred_T = my_sign(F_T_X)\n            emp_error_T = np.sum(y_pred_T != Y) / m\n            emp_errors_per_T.append(emp_error_T)\n\n            # f. Calculate SRM criterion for round T.\n            if sum_abs_alpha == 0:\n                srm_risks_per_T.append(float('inf'))\n                continue\n            \n            # Calculate normalized margins.\n            margins = (Y * F_T_X) / sum_abs_alpha\n\n            # Calculate the generalization bound for each theta.\n            srm_bound_numerator = 2.0 * d * np.log(2.0 * np.e * m / d) + np.log(2.0 / delta)\n            \n            bounds_for_T = []\n            for theta in theta_grid:\n                # Empirical margin error.\n                margin_violations = np.sum(margins = theta) / m\n                # Complexity term from the bound.\n                complexity_term = np.sqrt(srm_bound_numerator / (m * theta**2))\n                bound = margin_violations + complexity_term\n                bounds_for_T.append(bound)\n            \n            # SRM risk for round T is the minimum of the bound over the theta grid.\n            srm_risk_T = np.min(bounds_for_T)\n            srm_risks_per_T.append(srm_risk_T)\n\n        # 4. Select T_ERM and T_SRM by finding the minimum of the respective criteria.\n        # np.argmin finds the first index of the minimum, satisfying the \"smallest T\" rule.\n        T_ERM = np.argmin(emp_errors_per_T) + 1\n        T_SRM = np.argmin(srm_risks_per_T) + 1\n        \n        final_results.extend([T_SRM, T_ERM])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, final_results))}]\")\n\nsolve()\n```", "id": "3118279"}]}