{"hands_on_practices": [{"introduction": "这项练习是一个基础性训练，旨在直接比较两个最重要的集中不等式：Hoeffding 不等式和 Bernstein 不等式。通过计算在知道和不知道数据方差两种情况下所需的样本量，你将量化拥有更多统计信息所带来的实际好处 [@problem_id:3189962]。这个练习将提升你为特定问题选择合适理论工具的能力，并让你体会其中的权衡。", "problem": "一位数据科学家正在估计一个二元分类任务中的有界损失的均值。设 $\\{X_{i}\\}_{i=1}^{n}$ 为独立同分布的随机变量，满足 $X_{i} \\in [0,1]$ 且 $\\mathbb{E}[X_{i}] = \\mu$。目标是选择一个样本大小 $n$，使得经验均值 $\\hat{\\mu}_{n} = \\frac{1}{n}\\sum_{i=1}^{n} X_{i}$ 满足 $|\\hat{\\mu}_{n} - \\mu| \\leq \\varepsilon$ 的概率至少为 $1 - \\delta$，其中 $\\varepsilon \\in (0,1)$ 和 $\\delta \\in (0,1)$ 是给定的设计参数。\n\n考虑两种方法：\n\n1) 一种仅使用范围的方法，该方法只利用了 $X_{i} \\in [0,1]$ 这一信息。\n\n2) 一种方差感知方法，该方法额外利用了 $\\operatorname{Var}(X_{i}) = \\sigma^{2}$ 的知识，其中 $\\sigma^{2} \\in [0, \\tfrac{1}{4}]$ 是已知的。\n\n定义 $n_{\\text{H}}(\\varepsilon,\\delta)$ 为在仅使用范围的方法下获得的充足样本大小，而 $n_{\\text{B}}(\\varepsilon,\\delta,\\sigma^{2})$ 为在方差感知方法下获得的充足样本大小。令样本大小节省因子为\n$$\nS(\\varepsilon,\\delta,\\sigma^{2}) \\equiv \\frac{n_{\\text{H}}(\\varepsilon,\\delta)}{n_{\\text{B}}(\\varepsilon,\\delta,\\sigma^{2})}.\n$$\n\n使用关于有界独立随机变量的基础集中不等式结果，以及在方差信息可用时的方差知识，推导 $S(\\varepsilon,\\delta,\\sigma^{2})$ 的一个精确简化表达式，该表达式仅为 $\\varepsilon$ 和 $\\sigma^{2}$ 的函数。你的最终答案必须是单个闭式表达式；不允许使用不等式或隐式定义。不要提供数值近似。", "solution": "问题要求推导样本大小节省因子 $S(\\varepsilon,\\delta,\\sigma^{2}) = \\frac{n_{\\text{H}}(\\varepsilon,\\delta)}{n_{\\text{B}}(\\varepsilon,\\delta,\\sigma^{2})}$，其中 $n_{\\text{H}}$ 和 $n_{\\text{B}}$ 是从集中不等式推导出的充足样本大小。目标是找到一个仅依赖于 $\\varepsilon$ 和 $\\sigma^{2}$ 的 $S$ 的表达式。这将通过对每种情况应用合适的基础集中不等式，然后计算推导出的样本大小的比率来实现。\n\n首先，我们确定仅使用范围方法下的充足样本大小 $n_{\\text{H}}(\\varepsilon,\\delta)$。给定 $\\{X_{i}\\}_{i=1}^{n}$ 是独立同分布（i.i.d.）的随机变量，且 $X_{i} \\in [0,1]$。令 $\\mathbb{E}[X_i] = \\mu$ 且 $\\hat{\\mu}_{n} = \\frac{1}{n}\\sum_{i=1}^{n} X_{i}$。适用于此场景的工具是霍夫丁不等式（Hoeffding's inequality），它界定了一组有界独立随机变量之和与其期望值的偏差。对于样本均值，霍夫丁不等式表述为：\n$$P(|\\hat{\\mu}_{n} - \\mu| \\ge \\varepsilon) \\le 2 \\exp(-2n\\varepsilon^2)$$\n此不等式成立，因为每个 $X_i$ 的范围是 $1-0=1$。我们要求偏差大于 $\\varepsilon$ 的概率至多为 $\\delta$：\n$$2 \\exp(-2n\\varepsilon^2) \\le \\delta$$\n为了找到一个充足的样本大小 $n$，我们对 $n$ 求解此不等式：\n$$\\exp(-2n\\varepsilon^2) \\le \\frac{\\delta}{2}$$\n$$-2n\\varepsilon^2 \\le \\ln\\left(\\frac{\\delta}{2}\\right)$$\n$$2n\\varepsilon^2 \\ge -\\ln\\left(\\frac{\\delta}{2}\\right) = \\ln\\left(\\frac{2}{\\delta}\\right)$$\n$$n \\ge \\frac{1}{2\\varepsilon^2}\\ln\\left(\\frac{2}{\\delta}\\right)$$\n因此，仅使用范围方法的充足样本大小为：\n$$n_{\\text{H}}(\\varepsilon,\\delta) = \\frac{1}{2\\varepsilon^2}\\ln\\left(\\frac{2}{\\delta}\\right)$$\n\n接下来，我们确定方差感知方法下的充足样本大小 $n_{\\text{B}}(\\varepsilon,\\delta,\\sigma^{2})$。除了之前的条件，我们还知道 $\\operatorname{Var}(X_{i}) = \\sigma^{2}$。对于有界变量，包含方差信息的基础结果是伯恩斯坦不等式（Bernstein's inequality）。我们将其应用于零均值变量 $Y_i = X_i - \\mu$。我们有 $\\mathbb{E}[Y_i] = 0$ 且 $\\operatorname{Var}(Y_i) = \\operatorname{Var}(X_i) = \\sigma^2$。由于 $X_i \\in [0,1]$ 且 $\\mu = \\mathbb{E}[X_i] \\in [0,1]$，变量 $Y_i$ 是有界的，即 $|Y_i| = |X_i - \\mu| \\le \\max(\\mu, 1-\\mu) \\le 1$。我们取上界 $M=1$。双边伯恩斯坦不等式的一种常见形式是：\n$$P\\left(\\left|\\frac{1}{n}\\sum_{i=1}^{n} Y_i\\right| \\ge \\varepsilon\\right) \\le 2 \\exp\\left(-\\frac{n\\varepsilon^2}{2(\\sigma^2 + M\\varepsilon/3)}\\right)$$\n将 $|\\hat{\\mu}_{n} - \\mu|$ 代替 $\\left|\\frac{1}{n}\\sum_{i=1}^{n} Y_i\\right|$ 并设 $M=1$：\n$$P(|\\hat{\\mu}_{n} - \\mu| \\ge \\varepsilon) \\le 2 \\exp\\left(-\\frac{n\\varepsilon^2}{2(\\sigma^2 + \\varepsilon/3)}\\right)$$\n我们要求此概率至多为 $\\delta$：\n$$2 \\exp\\left(-\\frac{n\\varepsilon^2}{2(\\sigma^2 + \\varepsilon/3)}\\right) \\le \\delta$$\n求解充足的样本大小 $n$：\n$$\\exp\\left(-\\frac{n\\varepsilon^2}{2(\\sigma^2 + \\varepsilon/3)}\\right) \\le \\frac{\\delta}{2}$$\n$$-\\frac{n\\varepsilon^2}{2(\\sigma^2 + \\varepsilon/3)} \\le \\ln\\left(\\frac{\\delta}{2}\\right)$$\n$$\\frac{n\\varepsilon^2}{2(\\sigma^2 + \\varepsilon/3)} \\ge -\\ln\\left(\\frac{\\delta}{2}\\right) = \\ln\\left(\\frac{2}{\\delta}\\right)$$\n$$n \\ge \\frac{2(\\sigma^2 + \\varepsilon/3)}{\\varepsilon^2}\\ln\\left(\\frac{2}{\\delta}\\right)$$\n因此，方差感知方法的充足样本大小为：\n$$n_{\\text{B}}(\\varepsilon,\\delta,\\sigma^{2}) = \\frac{2(\\sigma^2 + \\varepsilon/3)}{\\varepsilon^2}\\ln\\left(\\frac{2}{\\delta}\\right)$$\n\n最后，我们通过计算 $n_{\\text{H}}$ 和 $n_{\\text{B}}$ 的比率来计算样本大小节省因子 $S(\\varepsilon,\\delta,\\sigma^{2})$：\n$$S(\\varepsilon,\\delta,\\sigma^{2}) = \\frac{n_{\\text{H}}(\\varepsilon,\\delta)}{n_{\\text{B}}(\\varepsilon,\\delta,\\sigma^{2})} = \\frac{\\frac{1}{2\\varepsilon^2}\\ln\\left(\\frac{2}{\\delta}\\right)}{\\frac{2(\\sigma^2 + \\varepsilon/3)}{\\varepsilon^2}\\ln\\left(\\frac{2}{\\delta}\\right)}$$\n项 $\\ln\\left(\\frac{2}{\\delta}\\right)$ 和 $\\varepsilon^2$ 被消去，得到一个仅依赖于 $\\sigma^2$ 和 $\\varepsilon$ 的表达式，正如所要求：\n$$S(\\sigma^{2},\\varepsilon) = \\frac{1/2}{2(\\sigma^2 + \\varepsilon/3)} = \\frac{1}{4(\\sigma^2 + \\varepsilon/3)}$$\n为了提供一个分母中不含分数的简化闭式表达式，我们可以将其重写为：\n$$S(\\sigma^{2},\\varepsilon) = \\frac{1}{4\\sigma^2 + \\frac{4\\varepsilon}{3}} = \\frac{1}{\\frac{12\\sigma^2 + 4\\varepsilon}{3}} = \\frac{3}{12\\sigma^2 + 4\\varepsilon} = \\frac{3}{4(3\\sigma^2 + \\varepsilon)}$$\n这就是样本大小节省因子的最终简化表达式。", "answer": "$$\n\\boxed{\\frac{3}{4(3\\sigma^2 + \\varepsilon)}}\n$$", "id": "3189962"}, {"introduction": "在掌握了单个均值的集中性之后，这个问题将挑战升级到整个函数类上的一致收敛。你将分析简单但至关重要的阈值函数类，计算其 VC 维，并运用强大的 Dvoretzky–Kiefer–Wolfowitz (DKW) 不等式来界定一致偏差 [@problem_id:3189954]。这项练习提供了一个具体的例子，展示了我们如何确保一个模型家族在训练集上的表现与在未见数据上的表现相近。", "problem": "考虑实线上的单调阈值函数假设类，其定义为 $\\mathcal{H} = \\{ h_{t} : \\mathbb{R} \\to \\{0,1\\} \\mid h_{t}(x) = \\mathbf{1}\\{x \\le t\\},\\ t \\in \\mathbb{R} \\}$。设 $X_{1},\\dots,X_{n}$ 是从 $\\mathbb{R}$ 上的任意分布 $P$ 中抽取的独立同分布的实值样本，其累积分布函数为 $F$。对于 $h \\in \\mathcal{H}$，定义真实风险 $P(h) = \\mathbb{E}[h(X)]$ 和经验风险 $P_{n}(h) = \\frac{1}{n} \\sum_{i=1}^{n} h(X_{i})$。\n\n任务：\n1. 从第一性原理出发，通过直接应用打散（shattering）的定义，计算 $\\mathcal{H}$ 的 Vapnik–Chervonenkis (VC) 维数。\n2. 设 $\\epsilon \\in (0,1)$ 且 $\\delta \\in (0,1)$。仅使用 Dvoretzky–Kiefer–Wolfowitz 不等式的精确常数形式作为基础事实，推导最小整数样本量 $n^{\\star}(\\epsilon,\\delta)$，使得对于 $\\mathbb{R}$ 上的每一个分布 $P$，一致偏差界\n$$\n\\sup_{h \\in \\mathcal{H}} \\big| P(h) - P_{n}(h) \\big| \\le \\epsilon\n$$\n以至少 $1-\\delta$ 的概率（相对于样本的抽取）同时成立。\n3. 通过将 $\\mathcal{H}$ 上的一致偏差问题，对于一个在其（累积分布函数）中位数处连续且严格递增的分布，简化为在固定阈值处的二项比例偏差问题，论证在任何形式为\n$$\n\\sup_{P} \\Pr\\!\\left( \\sup_{h \\in \\mathcal{H}} \\big| P(h) - P_{n}(h) \\big|  \\epsilon \\right) \\le K \\exp\\!\\big( - c\\, n \\epsilon^{2} \\big),\n$$\n的无分布亚高斯尾界中（该界对所有足够大的 $n$ 和固定区间 $(0,\\epsilon_{0}]$ 中的所有 $\\epsilon$ 成立），最大可能的指数常数 $c$ 是一个固定的数值。确定这个最优的 $c$。\n\n按以下顺序将您的最终答案报告为单行矩阵：$\\mathcal{H}$ 的 VC 维数、$n^{\\star}(\\epsilon,\\delta)$ 的精确闭式表达式以及最优指数常数 $c$。不需要四舍五入，答案必须是精确的闭式表达式。", "solution": "### 解答\n\n解答分为三部分，对应于问题陈述中的三个任务。\n\n**任务1：$\\mathcal{H}$ 的 VC 维数**\n一个假设类 $\\mathcal{H}$ 的 Vapnik–Chervonenkis (VC) 维数，记作 $\\mathrm{VCdim}(\\mathcal{H})$，是 $\\mathcal{H}$ 能打散（shatter）的最大点集的大小。如果对于每一种可能的标签组合 $(y_1, \\dots, y_m) \\in \\{0, 1\\}^m$，都存在一个假设 $h \\in \\mathcal{H}$ 使得对所有的 $i \\in \\{1, \\dots, m\\}$ 都有 $h(x_i) = y_i$，那么点集 $\\{x_1, \\dots, x_m\\}$ 就被称为可被 $\\mathcal{H}$ 打散。我们假设类中的假设是 $h_t(x) = \\mathbf{1}\\{x \\le t\\}$。\n\n首先，我们证明 $\\mathrm{VCdim}(\\mathcal{H}) \\ge 1$。考虑任意单点 $\\{x_1\\}$。存在 $2^1 = 2$ 种可能的标签组合：$\\{0\\}$ 和 $\\{1\\}$。\n*   为了得到标签 $\\{1\\}$，我们需要 $h_t(x_1) = 1$，即 $x_1 \\le t$。我们可以选择 $t = x_1$。\n*   为了得到标签 $\\{0\\}$，我们需要 $h_t(x_1) = 0$，即 $x_1 > t$。我们可以选择 $t = x_1 - 1$。\n由于单个点可以被完全打散，因此 $\\mathrm{VCdim}(\\mathcal{H}) \\ge 1$。\n\n接下来，我们证明 $\\mathrm{VCdim}(\\mathcal{H})  2$。考虑任意两个不同点的集合 $\\{x_1, x_2\\}$。不失一般性，设 $x_1  x_2$。存在 $2^2 = 4$ 种可能的标签组合：$(0,0), (0,1), (1,0), (1,1)$。我们来看看是否能实现所有这些组合。\n*   对于 $(0,0)$：我们需要 $h_t(x_1)=0$ 和 $h_t(x_2)=0$。这意味着 $x_1 > t$ 和 $x_2 > t$。我们可以选择任意 $t  x_1$，例如 $t=x_1 - 1$。\n*   对于 $(1,1)$：我们需要 $h_t(x_1)=1$ 和 $h_t(x_2)=1$。这意味着 $x_1 \\le t$ 和 $x_2 \\le t$。我们可以选择任意 $t \\ge x_2$，例如 $t=x_2$。\n*   对于 $(1,0)$：我们需要 $h_t(x_1)=1$ 和 $h_t(x_2)=0$。这意味着 $x_1 \\le t$ 和 $x_2 > t$。我们可以选择任意满足 $x_1 \\le t  x_2$ 的 $t$，例如 $t=x_1$。\n*   对于 $(0,1)$：我们需要 $h_t(x_1)=0$ 和 $h_t(x_2)=1$。这意味着 $x_1 > t$ 和 $x_2 \\le t$。这将意味着 $t  x_1$ 且 $t \\ge x_2$。由于我们假设了 $x_1  x_2$，因此不可能找到这样的 $t$。\n\n因为对于任意点对 $\\{x_1, x_2\\}$ 且 $x_1  x_2$，标签组合 $(0,1)$ 都无法生成，所以 $\\mathcal{H}$ 不能打散任何大小为 2 的集合。因此，VC 维数为 1。\n\n**任务2：样本量 $n^{\\star}(\\epsilon,\\delta)$**\n我们想要界定的一致偏差是 $\\sup_{h \\in \\mathcal{H}} |P(h) - P_n(h)|$。对于给定的假设类 $\\mathcal{H}$，真实风险和经验风险是：\n$P(h_t) = \\mathbb{E}[\\mathbf{1}\\{X \\le t\\}] = \\Pr(X \\le t) = F(t)$，其中 $F$ 是数据生成分布 $P$ 的累积分布函数。\n$P_n(h_t) = \\frac{1}{n} \\sum_{i=1}^n \\mathbf{1}\\{X_i \\le t\\} = \\hat{F}_n(t)$，其中 $\\hat{F}_n$ 是经验累积分布函数。\n因此，一致偏差等价于真实累积分布函数与经验累积分布函数之间的 Kolmogorov-Smirnov 距离：\n$$ \\sup_{h \\in \\mathcal{H}} \\big| P(h) - P_{n}(h) \\big| = \\sup_{t \\in \\mathbb{R}} \\big| F(t) - \\hat{F}_n(t) \\big| $$\n该问题要求使用由 Massart (1990) 建立的 Dvoretzky–Kiefer–Wolfowitz (DKW) 不等式的精确常数形式。该不等式表明，对于任何 $n \\ge 1$ 和任何 $\\epsilon > 0$，\n$$ \\Pr\\left( \\sup_{t \\in \\mathbb{R}} |F(t) - \\hat{F}_n(t)| > \\epsilon \\right) \\le 2 \\exp(-2n\\epsilon^2). $$\n我们想要找到最小的整数 $n^{\\star}(\\epsilon, \\delta)$，使得其补事件的概率至少为 $1-\\delta$：\n$$ \\Pr\\left( \\sup_{t \\in \\mathbb{R}} |F(t) - \\hat{F}_n(t)| \\le \\epsilon \\right) \\ge 1 - \\delta. $$\n这等价于确保尾事件的概率至多为 $\\delta$：\n$$ \\Pr\\left( \\sup_{t \\in \\mathbb{R}} |F(t) - \\hat{F}_n(t)| > \\epsilon \\right) \\le \\delta. $$\n通过应用 DKW 不等式，我们可以通过将其上界设置为小于或等于 $\\delta$ 来满足此条件：\n$ 2 \\exp(-2n\\epsilon^2) \\le \\delta $。\n我们现在对 $n$ 求解这个不等式：\n$$ \\exp(-2n\\epsilon^2) \\le \\frac{\\delta}{2} $$\n对两边取自然对数：\n$$ -2n\\epsilon^2 \\le \\ln\\left(\\frac{\\delta}{2}\\right) $$\n两边乘以 $-1$ 并反转不等号：\n$$ 2n\\epsilon^2 \\ge -\\ln\\left(\\frac{\\delta}{2}\\right) = \\ln\\left(\\left(\\frac{\\delta}{2}\\right)^{-1}\\right) = \\ln\\left(\\frac{2}{\\delta}\\right) $$\n最后，分离出 $n$：\n$$ n \\ge \\frac{1}{2\\epsilon^2} \\ln\\left(\\frac{2}{\\delta}\\right) $$\n由于 $n$ 必须是整数，满足此条件的最小整数 $n$ 是右侧表达式的向上取整。\n$$ n^{\\star}(\\epsilon,\\delta) = \\left\\lceil \\frac{1}{2\\epsilon^2} \\ln\\left(\\frac{2}{\\delta}\\right) \\right\\rceil. $$\n\n**任务3：最优指数常数 $c$**\n题目要求我们找到最大可能的常数 $c$，使得对于某个常数 $K$，以下界对所有分布 $P$、所有足够大的 $n$ 以及所有 $\\epsilon \\in (0, \\epsilon_0]$ 成立：\n$$ \\sup_{P} \\Pr\\!\\left( \\sup_{h \\in \\mathcal{H}} |P(h) - P_n(h)| > \\epsilon \\right) \\le K \\exp(-cn\\epsilon^2). $$\n左侧是 $\\sup_{P} \\Pr\\!\\left( \\sup_{t \\in \\mathbb{R}} |F(t) - \\hat{F}_n(t)| > \\epsilon \\right)$。常数为 $c=2$ 的 DKW 不等式表明这样的界是存在的。为了证明 $c=2$ 是最优（最大可能）值，我们必须建立一个与之匹配的衰减速率下界。\n\n我们按照提示，将问题简化为二项比例的偏差问题。对所有分布的上确界必须大于或等于任何单个分布的概率。此外，对所有阈值 $t$ 的上确界必须大于或等于在任何单个阈值 $t_0$ 处的偏差。\n$$ \\sup_{P} \\Pr\\!\\left( \\sup_{t \\in \\mathbb{R}} |F(t) - \\hat{F}_n(t)| > \\epsilon \\right) \\ge \\Pr_{P_0}\\!\\left( |F_{0}(t_0) - \\hat{F}_{n,0}(t_0)| > \\epsilon \\right) $$\n对于任何特定分布 $P_0$ 和阈值 $t_0$。\n\n我们选择 $P_0$ 为 $[0,1]$ 上的均匀分布，即 $X \\sim U(0,1)$，其累积分布函数为 $F_0(t) = t$（对于 $t \\in [0,1]$）。我们选择阈值 $t_0 = 1/2$，这是该分布的中位数。在这一点上，$F_0(1/2) = 1/2$。\n经验值为 $\\hat{F}_{n,0}(1/2) = \\frac{1}{n} \\sum_{i=1}^n \\mathbf{1}\\{X_i \\le 1/2\\}$。设 $Y_i = \\mathbf{1}\\{X_i \\le 1/2\\}$。由于 $X_i \\sim U(0,1)$，$Y_i$ 是独立同分布的伯努利随机变量，其成功概率为 $p=\\Pr(X_i \\le 1/2) = F_0(1/2) = 1/2$。\n和 $S_n = \\sum_{i=1}^n Y_i = n \\hat{F}_{n,0}(1/2)$ 服从二项分布，$S_n \\sim \\mathrm{Binomial}(n, 1/2)$。\n\n在 $t_0=1/2$ 处的偏差为 $|\\hat{F}_{n,0}(1/2) - F_0(1/2)| = |\\frac{S_n}{n} - \\frac{1}{2}|$。\n这个量的大偏差行为是众所周知的。根据 Sanov 定理，概率 $\\Pr(|\\frac{S_n}{n} - \\frac{1}{2}| > \\epsilon)$ 以由 Kullback-Leibler (KL) 散度给出的速率呈指数衰减。对于大的 $n$ 和小的 $\\epsilon$，\n$$ \\Pr\\left(\\left|\\frac{S_n}{n} - \\frac{1}{2}\\right| > \\epsilon\\right) \\approx \\exp\\left(-n \\inf_{|q-1/2| \\ge \\epsilon} D_{KL}(q || 1/2)\\right), $$\n其中 $D_{KL}(q || p) = q \\ln(q/p) + (1-q)\\ln((1-q)/(1-p))$。下确界在边界处达到，例如，在 $q = 1/2 + \\epsilon$ 处。我们来分析速率函数 $D_{KL}(1/2+\\epsilon || 1/2)$：\n$$ D_{KL}(1/2+\\epsilon || 1/2) = \\left(\\frac{1}{2}+\\epsilon\\right) \\ln\\left(\\frac{1/2+\\epsilon}{1/2}\\right) + \\left(\\frac{1}{2}-\\epsilon\\right) \\ln\\left(\\frac{1/2-\\epsilon}{1/2}\\right) $$\n$$ = \\left(\\frac{1}{2}+\\epsilon\\right) \\ln(1+2\\epsilon) + \\left(\\frac{1}{2}-\\epsilon\\right) \\ln(1-2\\epsilon) $$\n对于小的 $x$，使用泰勒级数展开 $\\ln(1+x) = x - x^2/2 + x^3/3 - \\dots$：\n$$ \\ln(1+2\\epsilon) = 2\\epsilon - \\frac{(2\\epsilon)^2}{2} + O(\\epsilon^3) = 2\\epsilon - 2\\epsilon^2 + O(\\epsilon^3) $$\n$$ \\ln(1-2\\epsilon) = -2\\epsilon - \\frac{(-2\\epsilon)^2}{2} + O(\\epsilon^3) = -2\\epsilon - 2\\epsilon^2 + O(\\epsilon^3) $$\n将这些代入 KL 散度的表达式中：\n$$ D_{KL}(1/2+\\epsilon || 1/2) \\approx \\left(\\frac{1}{2}+\\epsilon\\right)(2\\epsilon - 2\\epsilon^2) + \\left(\\frac{1}{2}-\\epsilon\\right)(-2\\epsilon - 2\\epsilon^2) $$\n$$ = (\\epsilon - \\epsilon^2 + 2\\epsilon^2 - 2\\epsilon^3) + (-\\epsilon - \\epsilon^2 + 2\\epsilon^2 + 2\\epsilon^3) + O(\\epsilon^4) $$\n$$ = 2\\epsilon^2 + O(\\epsilon^4) $$\n对于小的 $\\epsilon$，大偏差率中的主导项是 $2\\epsilon^2$。这意味着对于大的 $n$，存在一个常数 $C'>0$ 使得\n$$ \\sup_{P} \\Pr\\!\\left( \\sup_{t} |F(t) - \\hat{F}_n(t)| > \\epsilon \\right) \\ge \\Pr_{U(0,1)}\\!\\left( \\left|\\hat{F}_n(1/2) - 1/2\\right| > \\epsilon \\right) \\ge C' \\exp(-2n\\epsilon^2). $$\n如果我们有一个形式为 $K \\exp(-cn\\epsilon^2)$ 的上界，它必须容纳这个下界。为了使不等式 $C' \\exp(-2n\\epsilon^2) \\le K \\exp(-cn\\epsilon^2)$ 对所有大的 $n$ 成立，上界的指数衰减率不能快于下界的指数衰减率。这要求 $-c \\ge -2$，即 $c \\le 2$。\n\n由于 DKW 不等式提供了一个 $c=2$ 的上界，而我们对一个特定案例的分析表明不可能有 $c>2$，因此常数 $c$ 的最大可能值为 2。", "answer": "$$\n\\boxed{\\begin{pmatrix} 1  \\left\\lceil \\frac{1}{2\\epsilon^2} \\ln\\left(\\frac{2}{\\delta}\\right) \\right\\rceil  2 \\end{pmatrix}}\n$$", "id": "3189954"}, {"introduction": "这项高级练习将深入探讨分析泛化能力最强大的工具之一：Rademacher 复杂度。你将应用这一概念，比较分别由 $\\ell_1$ 和 $\\ell_2$ 范数约束的线性模型的复杂度，这两种约束分别对应于 LASSO 和 Ridge 回归 [@problem_id:3189970]。这项练习揭示了假设空间的几何形态与模型泛化能力之间的深刻联系，为 $\\ell_1$ 正则化在高维环境中可能更具优势提供了理论依据。", "problem": "考虑在 $\\mathbb{R}^d$ 上具有 $f_{\\mathbf{w}}(\\mathbf{x}) = \\langle \\mathbf{w}, \\mathbf{x} \\rangle$ 形式的实值分数的线性分类。给定一个固定样本 $S = \\{(\\mathbf{x}_i, y_i)\\}_{i=1}^n$，其中 $\\mathbf{x}_i \\in \\mathbb{R}^d$ 且 $y_i \\in \\{-1, +1\\}$。假设对于所有 $i \\in \\{1,\\dots,n\\}$，特征向量满足 $\\|\\mathbf{x}_i\\|_2 \\le R_2$ 和 $\\|\\mathbf{x}_i\\|_\\infty \\le R_\\infty$，其中 $R_2, R_\\infty > 0$ 是已知的半径。考虑两个假设类\n$\\mathcal{H}_2 = \\{\\mathbf{w} \\in \\mathbb{R}^d : \\|\\mathbf{w}\\|_2 \\le B_2\\}$ 和 $\\mathcal{H}_1 = \\{\\mathbf{w} \\in \\mathbb{R}^d : \\|\\mathbf{w}\\|_1 \\le B_1\\}$，\n其半径为 $B_2, B_1 > 0$。设 $\\ell:\\mathbb{R}\\times\\{-1,+1\\}\\to[0,1]$ 是一个在其第一个参数上是 1-Lipschitz 的损失函数。$\\ell \\circ \\mathcal{H}_p$ 的一致收敛可以通过经验 Rademacher 复杂度和集中不等式来控制。\n\n哪个选项正确地刻画了 $\\mathcal{H}_2$ 和 $\\mathcal{H}_1$ 的经验 Rademacher 复杂度如何随 $n$、$d$、$R_2$ 和 $R_\\infty$ 变化，并正确地解释了任何维度依赖性的几何原因？\n\nA. 对于任何此类样本，$\\mathcal{H}_2$ 的经验 Rademacher 复杂度满足 $\\widehat{\\mathfrak{R}}_S(\\mathcal{H}_2) \\le \\dfrac{B_2 R_2}{\\sqrt{n}}$，而 $\\mathcal{H}_1$ 的经验 Rademacher 复杂度满足 $\\widehat{\\mathfrak{R}}_S(\\mathcal{H}_1) \\le \\dfrac{B_1 R_\\infty \\sqrt{2 \\log(2d)}}{\\sqrt{n}}$。因此，$\\ell_1$ 类产生了一个 $\\sqrt{\\log d}$ 因子，因为其对偶范数是 $\\ell_\\infty$（坐标上的最大值），而 $\\ell_2$ 类的界中没有显式的 $d$。使用 $\\|\\mathbf{x}\\|_2 \\le \\sqrt{d}\\,\\|\\mathbf{x}\\|_\\infty$，如果 $B_1 \\approx B_2$ 且 $d$ 很大，$\\ell_1$ 类的复杂度可能比 $\\ell_2$ 类小约 $\\sqrt{d/\\log d}$ 倍。\n\nB. 如果 $\\|\\mathbf{x}_i\\|_2 \\le R_2$，则 $\\widehat{\\mathfrak{R}}_S(\\mathcal{H}_1) \\le \\dfrac{B_1 R_2}{\\sqrt{n}}$（与维度无关），因为 $\\ell_1$ 的对偶范数是 $\\ell_2$；因此，$\\ell_1$ 正则化消除了对 $d$ 的任何依赖。\n\nC. 当 $\\|\\mathbf{x}_i\\|_\\infty \\le R_\\infty$ 时，由于取坐标上的最大值，$\\ell_2$ 类在其 Rademacher 复杂度中必然表现出一个 $\\sqrt{\\log d}$ 因子，而在相同条件下，$\\ell_1$ 类与维度无关。\n\nD. 在对 $\\|\\mathbf{x}_i\\|_\\infty$ 没有任何界定的情况下，只要 $\\|\\mathbf{x}_i\\|_2 \\le R_2$，$\\ell_1$ 和 $\\ell_2$ 类就具有相同的经验 Rademacher 复杂度，直到通用常数为止。", "solution": "问题陈述是统计学习理论中一个有效的练习。我们将首先推导经验 Rademacher 复杂度的相关界，然后评估每个选项。\n\n函数类 $\\mathcal{F}$ 在样本 $S = \\{z_1, \\dots, z_n\\}$ 上的经验 Rademacher 复杂度定义为：\n$$ \\widehat{\\mathfrak{R}}_S(\\mathcal{F}) = \\mathbb{E}_{\\boldsymbol{\\sigma}} \\left[ \\sup_{f \\in \\mathcal{F}} \\frac{1}{n} \\sum_{i=1}^n \\sigma_i f(z_i) \\right] $$\n其中 $\\sigma_1, \\dots, \\sigma_n$ 是独立的 Rademacher 随机变量，即 $\\mathbb{P}(\\sigma_i = 1) = \\mathbb{P}(\\sigma_i = -1) = 1/2$。\n\n在这个问题中，函数是损失函数 $\\ell$ 和线性函数类 $\\mathcal{H}_p$ 的复合。设完整的函数类为 $\\mathcal{L}_p = \\{(\\mathbf{x}, y) \\mapsto \\ell(\\langle \\mathbf{w}, \\mathbf{x} \\rangle, y) \\mid \\mathbf{w} \\in \\mathcal{H}_p \\}$。样本为 $S = \\{(\\mathbf{x}_i, y_i)\\}_{i=1}^n$。\n损失函数 $\\ell$ 在其第一个参数上是 1-Lipschitz 的。根据 Ledoux-Talagrand 收缩不等式，复合类 $\\mathcal{L}_p$ 的 Rademacher 复杂度受底层线性函数类的 Rademacher 复杂度约束。设 $\\mathcal{F}_p = \\{\\mathbf{x} \\mapsto \\langle \\mathbf{w}, \\mathbf{x} \\rangle \\mid \\mathbf{w} \\in \\mathcal{H}_p \\}$。收缩原理给出：\n$$ \\widehat{\\mathfrak{R}}_S(\\mathcal{L}_p) \\le 1 \\cdot \\widehat{\\mathfrak{R}}_S(\\mathcal{F}_p) $$\n我们基于数据点 $\\{\\mathbf{x}_1, \\dots, \\mathbf{x}_n\\}$ 计算 $\\widehat{\\mathfrak{R}}_S(\\mathcal{F}_p)$：\n$$ \\widehat{\\mathfrak{R}}_S(\\mathcal{F}_p) = \\mathbb{E}_{\\boldsymbol{\\sigma}} \\left[ \\sup_{\\mathbf{w} \\in \\mathcal{H}_p} \\frac{1}{n} \\sum_{i=1}^n \\sigma_i \\langle \\mathbf{w}, \\mathbf{x}_i \\rangle \\right] $$\n根据内积的线性性质，这可以写成：\n$$ \\widehat{\\mathfrak{R}}_S(\\mathcal{F}_p) = \\frac{1}{n} \\mathbb{E}_{\\boldsymbol{\\sigma}} \\left[ \\sup_{\\mathbf{w} \\in \\mathcal{H}_p} \\left\\langle \\mathbf{w}, \\sum_{i=1}^n \\sigma_i \\mathbf{x}_i \\right\\rangle \\right] $$\n上确界 $\\sup_{\\|\\mathbf{w}\\|_p \\le B_p} \\langle \\mathbf{w}, \\mathbf{v} \\rangle$ 等于 $B_p \\|\\mathbf{v}\\|_{p^*}$，其中 $\\|\\cdot\\|_{p^*}$ 是 $\\|\\cdot\\|_p$ 的对偶范数。这给出了通用公式：\n$$ \\widehat{\\mathfrak{R}}_S(\\mathcal{F}_p) = \\frac{B_p}{n} \\mathbb{E}_{\\boldsymbol{\\sigma}} \\left[ \\left\\| \\sum_{i=1}^n \\sigma_i \\mathbf{x}_i \\right\\|_{p^*} \\right] $$\n\n现在我们分析两个具体情况。\n\n**情况1：$\\ell_2$ 类，$\\mathcal{H}_2 = \\{\\mathbf{w} \\in \\mathbb{R}^d : \\|\\mathbf{w}\\|_2 \\le B_2\\}$**\n这里，$p=2$，对偶范数也是 $\\ell_2$ 范数，因为 $p^* = 2$。\n$$ \\widehat{\\mathfrak{R}}_S(\\mathcal{F}_2) = \\frac{B_2}{n} \\mathbb{E}_{\\boldsymbol{\\sigma}} \\left[ \\left\\| \\sum_{i=1}^n \\sigma_i \\mathbf{x}_i \\right\\|_2 \\right] $$\n使用 Jensen 不等式 ($\\mathbb{E}[X] \\le \\sqrt{\\mathbb{E}[X^2]}$)：\n$$ \\mathbb{E}_{\\boldsymbol{\\sigma}} \\left[ \\left\\| \\sum_{i=1}^n \\sigma_i \\mathbf{x}_i \\right\\|_2 \\right] \\le \\sqrt{ \\mathbb{E}_{\\boldsymbol{\\sigma}} \\left[ \\left\\| \\sum_{i=1}^n \\sigma_i \\mathbf{x}_i \\right\\|_2^2 \\right] } $$\n平方根内的项是：\n$$ \\mathbb{E}_{\\boldsymbol{\\sigma}} \\left[ \\left\\langle \\sum_{i=1}^n \\sigma_i \\mathbf{x}_i, \\sum_{j=1}^n \\sigma_j \\mathbf{x}_j \\right\\rangle \\right] = \\sum_{i=1}^n \\sum_{j=1}^n \\mathbb{E}[\\sigma_i \\sigma_j] \\langle \\mathbf{x}_i, \\mathbf{x}_j \\rangle = \\sum_{i=1}^n \\|\\mathbf{x}_i\\|_2^2 $$\n这里使用了这样一个事实：$\\mathbb{E}[\\sigma_i \\sigma_j] = \\delta_{ij}$，因为 $\\sigma_i$ 是独立的且均值为零。\n给定 $\\|\\mathbf{x}_i\\|_2 \\le R_2$，我们有 $\\sum_{i=1}^n \\|\\mathbf{x}_i\\|_2^2 \\le n R_2^2$。\n因此，\n$$ \\widehat{\\mathfrak{R}}_S(\\mathcal{F}_2) \\le \\frac{B_2}{n} \\sqrt{n R_2^2} = \\frac{B_2 R_2}{\\sqrt{n}} $$\n这为 $\\mathcal{H}_2$ 的复杂度提供了一个与维度无关的界。\n\n**情况2：$\\ell_1$ 类，$\\mathcal{H}_1 = \\{\\mathbf{w} \\in \\mathbb{R}^d : \\|\\mathbf{w}\\|_1 \\le B_1\\}$**\n这里，$p=1$，对偶范数是 $\\ell_\\infty$ 范数，$p^* = \\infty$。\n$$ \\widehat{\\mathfrak{R}}_S(\\mathcal{F}_1) = \\frac{B_1}{n} \\mathbb{E}_{\\boldsymbol{\\sigma}} \\left[ \\left\\| \\sum_{i=1}^n \\sigma_i \\mathbf{x}_i \\right\\|_\\infty \\right] $$\n$\\ell_\\infty$ 范数是各分量绝对值的最大值。设 $\\mathbf{z} = \\sum_{i=1}^n \\sigma_i \\mathbf{x}_i$。那么 $\\|\\mathbf{z}\\|_\\infty = \\max_{j \\in \\{1,\\dots,d\\}} |z_j| = \\max_{j \\in \\{1,\\dots,d\\}} \\left| \\sum_{i=1}^n \\sigma_i x_{i,j} \\right|$。\n这个量 $\\mathbb{E}[\\max_j |\\sum_i \\sigma_i x_{i,j}|]$ 可以使用次高斯变量最大值的标准结果来界定。一个众所周知的不等式（与 Massart 引理相关）是：\n$$ \\mathbb{E}_{\\boldsymbol{\\sigma}} \\left[ \\max_{j=1,\\dots,d} \\left| \\sum_{i=1}^n \\sigma_i x_{i,j} \\right| \\right] \\le \\sqrt{2 \\log(2d)} \\cdot \\max_{j=1,\\dots,d} \\sqrt{\\sum_{i=1}^n x_{i,j}^2} $$\n我们已知 $\\|\\mathbf{x}_i\\|_\\infty \\le R_\\infty$，这意味着对所有 $i, j$ 都有 $|x_{i,j}| \\le R_\\infty$。因此，对于任意 $j$，$\\sum_{i=1}^n x_{i,j}^2 \\le \\sum_{i=1}^n R_\\infty^2 = n R_\\infty^2$。\n将此代入不等式中：\n$$ \\mathbb{E}_{\\boldsymbol{\\sigma}} \\left[ \\left\\| \\sum_{i=1}^n \\sigma_i \\mathbf{x}_i \\right\\|_\\infty \\right] \\le \\sqrt{2 \\log(2d)} \\cdot \\sqrt{n R_\\infty^2} = R_\\infty \\sqrt{n} \\sqrt{2 \\log(2d)} $$\n这得到了 $\\mathcal{H}_1$ 的复杂度界：\n$$ \\widehat{\\mathfrak{R}}_S(\\mathcal{F}_1) \\le \\frac{B_1}{n} \\left( R_\\infty \\sqrt{n} \\sqrt{2 \\log(2d)} \\right) = \\frac{B_1 R_\\infty \\sqrt{2 \\log(2d)}}{\\sqrt{n}} $$\n这个界表现出对维度 $d$ 的温和对数依赖性。\n\n现在，我们评估这些选项。\n\n**A.** “对于任何此类样本，$\\mathcal{H}_2$ 的经验 Rademacher 复杂度满足 $\\widehat{\\mathfrak{R}}_S(\\mathcal{H}_2) \\le \\dfrac{B_2 R_2}{\\sqrt{n}}$，而 $\\mathcal{H}_1$ 的经验 Rademacher 复杂度满足 $\\widehat{\\mathfrak{R}}_S(\\mathcal{H}_1) \\le \\dfrac{B_1 R_\\infty \\sqrt{2 \\log(2d)}}{\\sqrt{n}}$。因此，$\\ell_1$ 类产生了一个 $\\sqrt{\\log d}$ 因子，因为其对偶范数是 $\\ell_\\infty$（坐标上的最大值），而 $\\ell_2$ 类的界中没有显式的 $d$。使用 $\\|\\mathbf{x}\\|_2 \\le \\sqrt{d}\\,\\|\\mathbf{x}\\|_\\infty$，如果 $B_1 \\approx B_2$ 且 $d$ 很大，$\\ell_1$ 类的复杂度可能比 $\\ell_2$ 类小约 $\\sqrt{d/\\log d}$ 倍。”\n- 提出的两个界正是上面使用范数约束的自然配对（$B_2, R_2$ 和 $B_1, R_\\infty$）推导出的界。这部分是**正确的**。\n- 对 $\\sqrt{\\log d}$ 因子来源的解释也是**正确的**：它源于对 $\\ell_\\infty$ 对偶范数所固有的 $d$ 个坐标上的最大值进行界定，而 $\\ell_2$ 对偶范数的计算避免了这一点，从而得到了一个与维度无关的界（当数据在 $\\ell_2$ 中有界时）。\n- 最后的比较是高维统计中的一个标准论证。如果我们假设数据受 $\\|\\mathbf{x}_i\\|_\\infty \\le R_\\infty$ 约束，那么对 $\\|\\mathbf{x}_i\\|_2$ 最紧的可用界是 $R_2 = \\sqrt{d} R_\\infty$。对应的 $\\mathcal{H}_2$ 的 Rademacher 界将按 $\\frac{B_2 (\\sqrt{d} R_\\infty)}{\\sqrt{n}}$ 比例缩放，即与 $\\sqrt{d}$ 成正比。$\\mathcal{H}_1$ 的界按 $\\frac{B_1 R_\\infty \\sqrt{2 \\log(2d)}}{\\sqrt{n}}$ 比例缩放，即与 $\\sqrt{\\log d}$ 成正比。对于大的 $d$，$\\mathcal{H}_2$ 的界与 $\\mathcal{H}_1$ 的界之比约为 $\\sqrt{d}/\\sqrt{\\log d}$。这意味着 $\\ell_1$ 类的复杂度界要小得多。该陈述被谨慎地表述为“可能具有更小的复杂度”，这得到了此界限分析的支持。这部分是**正确的**。\n因此，整个选项是正确的。\n\n**B.** “如果 $\\|\\mathbf{x}_i\\|_2 \\le R_2$，则 $\\widehat{\\mathfrak{R}}_S(\\mathcal{H}_1) \\le \\dfrac{B_1 R_2}{\\sqrt{n}}$（与维度无关），因为 $\\ell_1$ 的对偶范数是 $\\ell_2$；因此，$\\ell_1$ 正则化消除了对 $d$ 的任何依赖。”\n- 不等式 $\\widehat{\\mathfrak{R}}_S(\\mathcal{H}_1) \\le \\frac{B_1 R_2}{\\sqrt{n}}$ 可以被推导出来。$\\widehat{\\mathfrak{R}}_S(\\mathcal{F}_1) = \\frac{B_1}{n} \\mathbb{E}_{\\boldsymbol{\\sigma}} [ \\| \\sum_i \\sigma_i \\mathbf{x}_i \\|_\\infty ]$。因为对于任何向量 $\\mathbf{v}$ 都有 $\\|\\mathbf{v}\\|_\\infty \\le \\|\\mathbf{v}\\|_2$，所以我们有 $\\mathbb{E}[\\|\\cdot\\|_\\infty] \\le \\mathbb{E}[\\|\\cdot\\|_2]$。使用从 $\\mathcal{H}_2$ 分析中得到的结果，$\\mathbb{E}[\\| \\sum_i \\sigma_i \\mathbf{x}_i \\|_2] \\le R_2 \\sqrt{n}$。因此，该不等式成立。\n- 然而，其理由“因为 $\\ell_1$ 的对偶范数是 $\\ell_2$”在事实上是**不正确的**。$\\ell_1$ 的对偶范数是 $\\ell_\\infty$。$\\ell_2$ 的对偶范数是 $\\ell_2$。这个根本性错误使其推理无效。\n\n**C.** “当 $\\|\\mathbf{x}_i\\|_\\infty \\le R_\\infty$ 时，由于取坐标上的最大值，$\\ell_2$ 类在其 Rademacher 复杂度中必然表现出一个 $\\sqrt{\\log d}$ 因子，而在相同条件下，$\\ell_1$ 类与维度无关。”\n- 这个陈述颠倒了这两个类的角色。$\\ell_2$ 类的复杂度界与 $\\ell_2$ 对偶范数有关，它不涉及坐标上的最大值，也不会引入 $\\sqrt{\\log d}$ 因子。使用 $\\|\\mathbf{x}_i\\|_\\infty \\le R_\\infty$ 会给出一个与 $\\sqrt{d}$ 成比例的 $\\mathcal{H}_2$ 的界（因为 $\\|\\mathbf{x}\\|_2 \\le \\sqrt{d}\\|\\mathbf{x}\\|_\\infty$），而不是 $\\sqrt{\\log d}$。\n- 相反，正是 $\\ell_1$ 类由于其 $\\ell_\\infty$ 对偶范数，其复杂度界依赖于 $\\sqrt{\\log d}$，而这恰恰是在 $\\|\\mathbf{x}_i\\|_\\infty \\le R_\\infty$ 条件下。\n- 整个陈述是**不正确的**。\n\n**D.** “在对 $\\|\\mathbf{x}_i\\|_\\infty$ 没有任何界定的情况下，只要 $\\|\\mathbf{x}_i\\|_2 \\le R_2$，$\\ell_1$ 和 $\\ell_2$ 类就具有相同的经验 Rademacher 复杂度，直到通用常数为止。”\n- 在 $\\|\\mathbf{x}_i\\|_2 \\le R_2$ 的条件下，我们有界 $\\widehat{\\mathfrak{R}}_S(\\mathcal{H}_2) \\le \\frac{B_2 R_2}{\\sqrt{n}}$ 和，如 B 的分析所示，$\\widehat{\\mathfrak{R}}_S(\\mathcal{H}_1) \\le \\frac{B_1 R_2}{\\sqrt{n}}$。虽然上界看起来相似，但实际的复杂度是 $\\frac{B_2}{n}\\mathbb{E}[\\|\\sum\\sigma_i\\mathbf{x}_i\\|_2]$ 和 $\\frac{B_1}{n}\\mathbb{E}[\\|\\sum\\sigma_i\\mathbf{x}_i\\|_\\infty]$。范数 $\\|\\cdot\\|_2$ 和 $\\|\\cdot\\|_\\infty$ 在通用常数范围内并不等价；它们的关系依赖于维度 $d$ ($1 \\le \\|\\mathbf{v}\\|_2/\\|\\mathbf{v}\\|_\\infty \\le \\sqrt{d}$)。正如反例（例如，数据向量是正交的 vs. 共线的）所展示的，这两个复杂度的比率可以依赖于 $n$和数据结构，而不仅仅是一个通用常数。这个断言太强了，通常是错误的。例如，如果我们使用包含维度的 $\\mathcal{H}_1$ 的界，该界由 $\\|\\mathbf{x}_i\\|_\\infty \\le \\|\\mathbf{x}_i\\|_2 \\le R_2$ 推导而来，我们得到 $\\widehat{\\mathfrak{R}}_S(\\mathcal{H}_1) \\le \\frac{B_1 R_2 \\sqrt{2 \\log(2d)}}{\\sqrt{n}}$，这与 $\\mathcal{H}_2$ 的界明确地差了一个 $\\sqrt{\\log d}$ 因子。因此，复杂度不相同。此选项是**不正确的**。\n\n基于分析，只有选项 A 是完全正确的。", "answer": "$$\\boxed{A}$$", "id": "3189970"}]}