{"hands_on_practices": [{"introduction": "我们从一个最基础的例子开始。在估计一个数据集的“中心”时，两种最常见的统计量是均值（对应于平方损失）和中位数（对应于绝对值损失）。本练习 [@problem_id:3098721] 将通过一个具体的编程任务，向你展示为什么中位数被认为是一个“稳健”的统计量，它在面对极端异常值时表现出更强的算法稳定性。", "problem": "您将分析两种常数函数回归算法在重尾误差下的算法稳定性。考虑一个数据集 $S = \\{(x_j, y_j)\\}_{j=1}^n$，其中输入为 $x_j \\in \\mathbb{R}$，输出为 $y_j \\in \\mathbb{R}$。定义两种学习算法，它们通过最小化经验风险来拟合常数预测器 $f_S(x) \\equiv c$：(a) 平方损失回归，其返回样本均值；(b) 绝对损失回归（中位数回归或分位数为 $q = \\tfrac{1}{2}$ 的分位数回归），其返回样本中位数。对于单点替换，将 $S^{(i,y^\\star)}$ 定义为仅将 $S$ 中的第 $i$ 个响应 $y_i$ 替换为 $y^\\star$，而所有其他数据对保持不变所得到的数据集。对于任意固定的输入 $x_0 \\in \\mathbb{R}$，将算法 $A$ 在 $(S, i, y^\\star)$ 上的经验点替换稳定性定义为\n$$\n\\Delta_A(S,i,y^\\star) \\equiv \\bigl| f_S(x_0) - f_{S^{(i,y^\\star)}}(x_0) \\bigr|.\n$$\n由于预测器是常数函数，$\\Delta_A(S,i,y^\\star)$ 不依赖于 $x_0$。\n\n您的任务是编写一个程序，对于一个确定性的重尾数据集族 $S$，计算 $\\Delta_{\\text{mean}}$ 和 $\\Delta_{\\text{median}}$，并为一组指定的测试用例验证 $\\Delta_{\\text{median}} \\le \\Delta_{\\text{mean}}$ 是否成立。您必须按照如下方式，使用重尾分布的分位数网格来确定性地构建 $S$，无需任何随机性。\n\n1. 位置为 $0$、尺度为 $\\gamma > 0$ 的柯西分布：对于 $j \\in \\{0,1,\\dots,n-1\\}$，令\n$$\np_j = \\frac{j + \\tfrac{1}{2}}{n}, \\quad y_j = \\gamma \\cdot \\tan\\!\\bigl(\\pi(p_j - \\tfrac{1}{2})\\bigr).\n$$\n2. 尺度（最小值）为 $x_m > 0$、形状为 $\\alpha > 0$ 的帕累托分布：对于 $j \\in \\{0,1,\\dots,n-1\\}$，令\n$$\np_j = \\frac{j + \\tfrac{1}{2}}{n}, \\quad y_j = \\frac{x_m}{(1 - p_j)^{1/\\alpha}}.\n$$\n\n在这两种情况下，由于输入对于常数预测器是无关紧要的，因此对所有 $j$ 均设置 $x_j \\equiv 0$。对于偶数 $n$，样本中位数必须定义为两个中央顺序统计量的平均值。对每个测试用例，计算\n$$\n\\Delta_{\\text{mean}}(S,i,y^\\star) = \\bigl|\\bar{y}(S) - \\bar{y}(S^{(i,y^\\star)})\\bigr|, \\quad \n\\Delta_{\\text{median}}(S,i,y^\\star) = \\bigl|\\operatorname{med}(S) - \\operatorname{med}(S^{(i,y^\\star)})\\bigr|.\n$$\n返回一个布尔值，指示 $\\Delta_{\\text{median}} \\le \\Delta_{\\text{mean}}$ 是否成立。\n\n使用以下参数值测试套件，这些参数共同覆盖了典型、边界和边缘场景。每个测试用例都指定了分布族、数据集大小 $n$、重尾参数、替换索引 $i$（从零开始）和替换值 $y^\\star$。\n\n- 测试用例 1（对称重尾，奇数 $n$）：\n  - 分布族：柯西分布\n  - 参数：$n = 101$, $\\gamma = 1$\n  - 替换：$i = 0$, $y^\\star = 10^6$。\n- 测试用例 2（对称重尾，偶数 $n$）：\n  - 分布族：柯西分布\n  - 参数：$n = 100$, $\\gamma = 1$\n  - 替换：$i = 99$, $y^\\star = -10^6$。\n- 测试用例 3（非对称重尾，奇数 $n$）：\n  - 分布族：帕累托分布\n  - 参数：$n = 99$, $x_m = 1$, $\\alpha = 1.5$\n  - 替换：$i = 0$, $y^\\star = 10^9$。\n- 测试用例 4（非对称重尾，偶数 $n$）：\n  - 分布族：帕累托分布\n  - 参数：$n = 50$, $x_m = 1$, $\\alpha = 2$\n  - 替换：$i = 0$, $y^\\star = 10^6$。\n\n您的程序必须为每个测试用例计算一个布尔值，该值指示在指定的点替换下，中位数回归是否至少与均值回归一样稳定，即是否\n$$\n\\Delta_{\\text{median}}(S,i,y^\\star) \\le \\Delta_{\\text{mean}}(S,i,y^\\star).\n$$\n\n最终输出格式：您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表（例如，$[r_1,r_2,r_3,r_4]$），其中每个 $r_k$ 是测试用例 $k$ 的布尔值。\n\n不涉及物理单位。所有数值结果都是无单位的实数和布尔值。不使用角度。不使用百分比。禁止使用任何随机性；您必须完全按照指定的方式使用分位数网格来确定性地构建 $S$。", "solution": "该问题被评估为 **有效**。它在科学上植根于统计学习理论，特别是算法稳定性，并且问题是良定的，提供了所有必要的数据和定义。任务是比较基于均值（平方损失）和中位数（绝对损失）的回归估计量在确定性生成的重尾数据集上的稳定性。该问题是客观的、可形式化的，并且在计算上是可行的。我们将继续提供完整的解决方案。\n\n我们的目标是比较两个常数回归预测器（样本均值和样本中位数）的点替换稳定性。算法 $A$ 的稳定性是通过当训练集 $S$ 中的一个点被改变时其输出预测器的变化来衡量的。稳定性度量定义为：\n$$\n\\Delta_A(S,i,y^\\star) \\equiv \\bigl| f_S(x_0) - f_{S^{(i,y^\\star)}}(x_0) \\bigr|\n$$\n其中 $S^{(i,y^\\star)}$ 是将数据集 $S$ 的第 $i$ 个响应 $y_i$ 替换为新值 $y^\\star$ 后的数据集。由于预测器是常数函数 $f_S(x) \\equiv c$，这简化为在原始数据集和修改后数据集上拟合的常数值之间的绝对差。\n\n设数据集为 $S = \\{y_j\\}_{j=0}^{n-1}$，包含 $n$ 个标量响应。输入 $x_j$ 是无关紧要的。\n\n**1. 样本均值的稳定性**\n\n第一个算法对应于最小化平方误差损失 $\\sum_{j=0}^{n-1} (y_j - c)^2$，它产生样本均值作为其估计值：\n$$\nf_S(x) \\equiv \\bar{y}(S) = \\frac{1}{n} \\sum_{j=0}^{n-1} y_j\n$$\n修改后的数据集 $S^{(i,y^\\star)}$ 的元素为 $\\{y_0, \\dots, y_{i-1}, y^\\star, y_{i+1}, \\dots, y_{n-1}\\}$。这个新数据集的样本均值是：\n$$\n\\bar{y}(S^{(i,y^\\star)}) = \\frac{1}{n} \\left( \\left(\\sum_{j=0}^{n-1} y_j\\right) - y_i + y^\\star \\right) = \\frac{1}{n} \\sum_{j=0}^{n-1} y_j - \\frac{y_i}{n} + \\frac{y^\\star}{n} = \\bar{y}(S) + \\frac{y^\\star - y_i}{n}\n$$\n因此，均值的稳定性度量 $\\Delta_{\\text{mean}}$ 是：\n$$\n\\Delta_{\\text{mean}}(S,i,y^\\star) = \\bigl| \\bar{y}(S) - \\bar{y}(S^{(i,y^\\star)}) \\bigr| = \\left| \\bar{y}(S) - \\left( \\bar{y}(S) + \\frac{y^\\star - y_i}{n} \\right) \\right| = \\left| - \\frac{y^\\star - y_i}{n} \\right| = \\frac{|y_i - y^\\star|}{n}\n$$\n这个公式表明，均值的变化与扰动的大小 $|y_i - y^\\star|$ 成正比，与数据集大小 $n$ 成反比。对于一个大的扰动（异常值），这个变化可以是任意大的。\n\n**2. 样本中位数的稳定性**\n\n第二个算法对应于最小化绝对误差损失 $\\sum_{j=0}^{n-1} |y_j - c|$，它产生样本中位数作为其估计值：\n$$\nf_S(x) \\equiv \\operatorname{med}(S)\n$$\n样本中位数是根据排序后的数据计算的。设 $y_{(0)} \\le y_{(1)} \\le \\dots \\le y_{(n-1)}$ 为数据集 $S$ 的顺序统计量。\n- 如果 $n$ 是奇数，中位数是中央元素，$\\operatorname{med}(S) = y_{((n-1)/2)}$。\n- 如果 $n$ 是偶数，中位数是两个中央元素的平均值，$\\operatorname{med}(S) = \\frac{1}{2} (y_{(n/2 - 1)} + y_{(n/2)})$。\n\n与均值不同，中位数的稳定性 $\\Delta_{\\text{median}}$ 没有简单的闭式表达式。它必须通过算法计算：\n1.  生成初始数据集 $S = \\{y_j\\}_{j=0}^{n-1}$。\n2.  计算其中位数 $\\operatorname{med}(S)$。\n3.  通过用 $y^\\star$ 替换 $y_i$ 来构建修改后的数据集 $S^{(i,y^\\star)}$。\n4.  计算修改后数据集的中位数 $\\operatorname{med}(S^{(i,y^\\star)})$。这需要重新排序或找到新的中央元素。\n5.  计算稳定性：$\\Delta_{\\text{median}}(S,i,y^\\star) = |\\operatorname{med}(S) - \\operatorname{med}(S^{(i,y^\\star)})|$。\n\n中位数是众所周知的稳健统计量。它的值取决于数据点的排序位置，而不是它们的量值。因此，与均值相比，用一个极端异常值 $y^\\star$ 替换一个点 $y_i$ 通常只会引起中位数发生小得多的变化，因为它可能只会将中位数的位置移动到相邻的数据点。\n\n**3. 数据集生成**\n\n数据集是使用重尾分布的分位数函数（逆累积分布函数）确定性地构建的。这确保了数据具有所需的分布特性，而无需引入随机性。\n分位数网格点为 $p_j = \\frac{j + 0.5}{n}$，其中 $j \\in \\{0, 1, \\dots, n-1\\}$。\n\n- **柯西分布：** 位置为 $0$，尺度为 $\\gamma > 0$。其分位数函数为 $F^{-1}(p) = \\gamma \\tan(\\pi(p - 0.5))$。\n  $$\n  y_j = \\gamma \\cdot \\tan\\!\\bigl(\\pi(p_j - \\tfrac{1}{2})\\bigr)\n  $$\n- **帕累托分布：** 尺度（最小值）为 $x_m > 0$，形状为 $\\alpha > 0$。其分位数函数为 $F^{-1}(p) = x_m / (1-p)^{1/\\alpha}$。\n  $$\n  y_j = \\frac{x_m}{(1 - p_j)^{1/\\alpha}}\n  $$\n由于分位数函数是单调的，并且 $p_j$ 值是有序的，因此生成的据集 $\\{y_j\\}$ 已经是有序的。\n\n**4. 针对每个测试用例的计算**\n\n对于每个指定的测试用例，我们执行以下步骤：\n1.  定义参数：分布族、$n$、重尾参数（$\\gamma$ 或 $(x_m, \\alpha)$）、替换索引 $i$ 和替换值 $y^\\star$。\n2.  使用相应的公式生成初始的已排序数据集 $S = \\{y_j\\}_{j=0}^{n-1}$。\n3.  使用解析公式计算均值的稳定性：$\\Delta_{\\text{mean}} = \\frac{|y_i - y^\\star|}{n}$。\n4.  通过算法计算中位数的稳定性：\n    a.  计算原始中位数 $\\operatorname{med}(S)$。\n    b.  通过替换 $y_i$ 创建修改后的数据集 $S^{(i,y^\\star)}$。\n    c.  计算新的中位数 $\\operatorname{med}(S^{(i,y^\\star)})$。\n    d.  计算 $\\Delta_{\\text{median}} = |\\operatorname{med}(S) - \\operatorname{med}(S^{(i,y^\\star)})|$。\n5.  比较稳定性值，并确定 $\\Delta_{\\text{median}} \\le \\Delta_{\\text{mean}}$ 是否成立。结果是一个布尔值。\n\n此过程被系统地应用于问题描述中提供的所有测试用例，以生成最终的布尔结果列表。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Analyzes and compares the algorithmic stability of mean and median regression\n    under point replacement for deterministically generated heavy-tailed datasets.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        {'family': 'Cauchy', 'n': 101, 'gamma': 1, 'xm': None, 'alpha': None, 'i': 0, 'y_star': 1e6},\n        {'family': 'Cauchy', 'n': 100, 'gamma': 1, 'xm': None, 'alpha': None, 'i': 99, 'y_star': -1e6},\n        {'family': 'Pareto', 'n': 99, 'gamma': None, 'xm': 1, 'alpha': 1.5, 'i': 0, 'y_star': 1e9},\n        {'family': 'Pareto', 'n': 50, 'gamma': None, 'xm': 1, 'alpha': 2, 'i': 0, 'y_star': 1e6},\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        # Unpack parameters for the current test case\n        n = case['n']\n        i = case['i']\n        y_star = case['y_star']\n        \n        # Step 1: Generate the initial dataset S = {y_j}\n        # The quantile points p_j\n        p = (np.arange(n) + 0.5) / n\n        \n        if case['family'] == 'Cauchy':\n            gamma = case['gamma']\n            # Generate sorted data using the Cauchy quantile function\n            y = gamma * np.tan(np.pi * (p - 0.5))\n        elif case['family'] == 'Pareto':\n            xm = case['xm']\n            alpha = case['alpha']\n            # Generate sorted data using the Pareto quantile function\n            y = xm / (1 - p)**(1 / alpha)\n        else:\n            # This path should not be reached with the given test cases\n            raise ValueError(f\"Unknown distribution family: {case['family']}\")\n\n        # The data y is generated sorted, as it's based on an ordered sequence of quantiles p_j\n        # and monotonic quantile functions.\n        \n        # Step 2: Compute stability of the mean\n        # We use the analytical formula: Delta_mean = |y_i - y_star| / n\n        y_i = y[i]\n        delta_mean = np.abs(y_i - y_star) / n\n        \n        # Step 3: Compute stability of the median\n        # a. Calculate the median of the original dataset\n        med_s = np.median(y)\n        \n        # b. Create the modified dataset S^(i, y_star)\n        y_mod = y.copy()\n        y_mod[i] = y_star\n        \n        # c. Calculate the median of the modified dataset\n        # np.median internally sorts the array, which is necessary here.\n        med_s_mod = np.median(y_mod)\n        \n        # d. Compute the stability measure for the median\n        delta_median = np.abs(med_s - med_s_mod)\n        \n        # Step 4: Compare stabilities and record the boolean result\n        is_median_more_stable = delta_median = delta_mean\n        results.append(is_median_more_stable)\n\n    # Final print statement in the exact required format.\n    # The str() of a boolean in Python is 'True' or 'False'.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3098721"}, {"introduction": "基于前一个练习的发现，我们现在将简单统计量的概念推广到线性回归模型中。本练习 [@problem_id:3098786] 将探究平方损失的非稳健性如何导致学习到的模型被单个异常值严重影响。你将实现一个更稳定的替代方案，即使用 Huber 损失函数，这是一种结合了平方损失和绝对值损失优点的混合方法。", "problem": "考虑一个具有标量输入和输出的监督学习场景，其假设类别由形式为 $f_{\\mathbf{w}}(x) = w x$ 的一维线性预测器组成，参数 $w \\in \\mathbb{R}$。设 $S = \\{(x_i, y_i)\\}_{i=1}^n$ 是一个训练数据集。将经验风险最小化（ERM）原则定义为在 $w$ 上最小化经验风险 $\\frac{1}{n} \\sum_{i=1}^n \\ell(y_i, f_{\\mathbf{w}}(x_i))$，其中 $\\ell$ 是一个指定的损失函数。我们将比较两种损失函数：(i) 平方损失 $\\ell_{\\text{sq}}(y, \\hat{y}) = (y - \\hat{y})^2$，以及 (ii) Huber 损失 $\\ell_{\\text{Huber}, \\delta}(y, \\hat{y})$，该损失由一个阈值 $\\delta  0$ 参数化，定义如下：\n$$\n\\ell_{\\text{Huber}, \\delta}(y, \\hat{y}) =\n\\begin{cases}\n\\frac{1}{2}(y - \\hat{y})^2  \\text{如果 } |y - \\hat{y}| \\le \\delta, \\\\\n\\delta \\left(|y - \\hat{y}| - \\frac{1}{2}\\delta\\right)  \\text{如果 } |y - \\hat{y}|  \\delta.\n\\end{cases}\n$$\n在单样本替换稳定性意义下的算法稳定性，关注的是当单个训练样本被修改时，已学习预测器的敏感度。对于给定的数据集 $S$，设 $f_S$ 表示在指定损失下的 ERM 解，并设 $S^{(k \\to y_k')}$ 表示通过将索引为 $k$ 的单个响应 $y_k$ 替换为 $y_k'$ 而形成的数据集，所有其他的 $(x_i, y_i)$ 保持不变。设 $f_{S^{(k \\to y_k')}}$ 为修改后数据集上的 ERM 解。对于固定的查询点 $x_\\star \\in \\mathbb{R}$，将因替换引起的预测偏移定义为\n$$\n\\Delta_{\\text{loss}}(S, k, y_k', x_\\star) = \\left| f_S(x_\\star) - f_{S^{(k \\to y_k')}}(x_\\star) \\right|.\n$$\n您的任务是实现一个完整、可运行的程序，该程序能够：\n- 对于每个指定的数据集，为平方损失和 Huber 损失（使用给定的阈值 $\\delta$）构建 ERM 解 $f_S$ 和 $f_{S^{(k \\to y_k')}}$。\n- 在给定的 $x_\\star$ 处，计算平方损失的绝对预测偏移 $\\Delta_{\\text{sq}}$ 和 Huber 损失的绝对预测偏移 $\\Delta_{\\text{Huber}}$。\n- 将所有测试用例的结果汇总到满足要求格式的单个输出行中。\n\n从上述基本定义出发，生成数值上精确的解。除非可以从所述设定的第一性原理推导出闭式公式，否则不要假设任何闭式公式。\n\n使用以下参数值测试套件，涵盖一个包含极端离群值的典型情况、一个包含负离群值的混合符号设计，以及一个替换不改变数据集的边界条件：\n\n- 测试用例 1（典型离群值）：\n  - $n = 5$，\n  - $\\mathbf{x} = (1,2,3,4,5)$，\n  - $\\mathbf{y} = (1,2,3,4,1000)$，\n  - 替换索引 $k = 5$ 和 $y_k' = 5$，\n  - Huber 阈值 $\\delta = 1.0$，\n  - 查询点 $x_\\star = 2.0$。\n\n- 测试用例 2（混合符号和负离群值）：\n  - $n = 4$，\n  - $\\mathbf{x} = (-2,-1,0.5,1.5)$，\n  - $\\mathbf{y} = (1,-200,0.5,-1)$，\n  - 替换索引 $k = 2$ 和 $y_k' = -2$，\n  - Huber 阈值 $\\delta = 0.5$，\n  - 查询点 $x_\\star = -1.0$。\n\n- 测试用例 3（边界：无变化）：\n  - $n = 3$，\n  - $\\mathbf{x} = (1,2,3)$，\n  - $\\mathbf{y} = (1,2,3)$，\n  - 替换索引 $k = 2$ 和 $y_k' = 2$，\n  - Huber 阈值 $\\delta = 1.0$，\n  - 查询点 $x_\\star = 1.0$。\n\n要求的最终输出格式：\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表的结果，顺序完全如下：\n$$\n[\\Delta_{\\text{sq}}^{(1)}, \\Delta_{\\text{Huber}}^{(1)}, \\Delta_{\\text{sq}}^{(2)}, \\Delta_{\\text{Huber}}^{(2)}, \\Delta_{\\text{sq}}^{(3)}, \\Delta_{\\text{Huber}}^{(3)}],\n$$\n其中上标表示测试用例编号。每个条目必须是一个浮点数。不允许外部输入；所有值都在上面指定，并且必须嵌入到程序中。", "solution": "该问题要求我们比较一个简单线性模型下，两种经验风险最小化（ERM）预测器的单样本替换稳定性。该模型由假设类别 $f_w(x) = wx$ 给出，其中 $w \\in \\mathbb{R}$。我们获得一个训练集 $S = \\{(x_i, y_i)\\}_{i=1}^n$。目标是找到参数 $w$，以最小化经验风险 $R_S(w) = \\frac{1}{n} \\sum_{i=1}^n \\ell(y_i, w x_i)$，这里会针对两种不同的损失函数：平方损失和 Huber 损失。然后，我们通过测量当一个训练标签 $y_k$ 被替换为 $y_k'$ 时，在点 $x_\\star$ 上预测值的变化来量化稳定性。\n\n首先，我们推导每种损失函数的 ERM 解。\n\n**平方损失的 ERM 解**\n平方损失函数为 $\\ell_{\\text{sq}}(y, \\hat{y}) = (y - \\hat{y})^2$。对于我们的模型，经验风险是：\n$$\nR_S(w) = \\frac{1}{n} \\sum_{i=1}^n (y_i - w x_i)^2\n$$\n这是关于 $w$ 的二次函数，并且是凸函数。为了找到最小值，我们对 $w$ 求导并令其为零：\n$$\n\\frac{dR_S(w)}{dw} = \\frac{1}{n} \\sum_{i=1}^n \\frac{d}{dw} (y_i - w x_i)^2 = \\frac{1}{n} \\sum_{i=1}^n 2(y_i - w x_i)(-x_i) = -\\frac{2}{n} \\sum_{i=1}^n (y_i x_i - w x_i^2)\n$$\n将导数设为零可得：\n$$\n\\sum_{i=1}^n (y_i x_i - w x_i^2) = 0\n$$\n$$\n\\sum_{i=1}^n y_i x_i = w \\sum_{i=1}^n x_i^2\n$$\n假设并非所有 $x_i$ 都为零，即 $\\sum_{i=1}^n x_i^2  0$，我们得到最优参数的唯一闭式解，我们将其表示为 $w_{\\text{sq}}$：\n$$\nw_{\\text{sq}} = \\frac{\\sum_{i=1}^n x_i y_i}{\\sum_{i=1}^n x_i^2}\n$$\n这就是我们熟悉的、对于通过原点的回归模型的普通最小二乘（OLS）估计量。\n\n**Huber 损失的 ERM 解**\nHuber 损失定义为：\n$$\n\\ell_{\\text{Huber}, \\delta}(y, \\hat{y}) =\n\\begin{cases}\n\\frac{1}{2}(y - \\hat{y})^2  \\text{如果 } |y - \\hat{y}| \\le \\delta, \\\\\n\\delta \\left(|y - \\hat{y}| - \\frac{1}{2}\\delta\\right)  \\text{如果 } |y - \\hat{y}|  \\delta.\n\\end{cases}\n$$\n经验风险是 $R_S(w) = \\frac{1}{n} \\sum_{i=1}^n \\ell_{\\text{Huber}, \\delta}(y_i, w x_i)$。Huber 损失是一个凸函数，由于凸函数的和仍然是凸函数，所以 $R_S(w)$ 也是关于 $w$ 的凸函数。因此，存在一个全局最小值，可以使用标准的凸优化技术找到。\n\n与平方损失不同，对于所有数据集，不存在一个简单的 $w$ 的闭式解。最优的 $w$ 是平稳性条件的解，该条件要求风险的次梯度包含零。$\\ell_{\\text{Huber}, \\delta}$ 关于其第二个参数的次梯度（在残差 $r=y-\\hat{y}$ 处计算）是一个通常表示为 $\\psi(r)$ 的函数：\n$$\n\\psi(r) = \\begin{cases}\nr  \\text{如果 } |r| \\le \\delta \\\\\n\\delta \\cdot \\text{sign}(r)  \\text{如果 } |r|  \\delta \\\\\n\\left[-\\delta, \\delta\\right]  \\text{如果 } |r| = \\delta\n\\end{cases}\n$$\n$R_S(w)$ 的平稳性条件是：\n$$\n0 \\in \\frac{\\partial R_S(w)}{\\partial w} \\implies 0 \\in \\sum_{i=1}^n x_i \\cdot \\psi(y_i - w x_i)\n$$\n这个方程是关于 $w$ 的非线性方程，因为 $\\psi$ 的形式取决于 $w$ 本身。虽然可以通过分析分段线性函数 $\\sum_i x_i \\psi(y_i - w x_i)$ 来求解，但一个更直接和鲁棒的方法是执行凸函数 $R_S(w)$ 的一维数值最小化。我们可以使用一个数值求解器，例如 `scipy.optimize.minimize_scalar`，来找到值 $w_{\\text{Huber}} = \\arg\\min_w R_S(w)$。\n\n**算法和预测偏移的计算**\n\n对每个测试用例和每种损失函数（平方损失和 Huber 损失），我们执行以下步骤：\n$1$. 设原始数据集为 $S = \\{(\\mathbf{x}_i, \\mathbf{y}_i)\\}_{i=1}^n$。\n$2$. 设修改后的数据集为 $S^{(k \\to y_k')} = \\{(\\mathbf{x}_i, \\mathbf{y}'_i)\\}_{i=1}^n$，其中 $\\mathbf{y}'$ 与 $\\mathbf{y}$ 相同，只是 $\\mathbf{y}'_k = y_k'$。\n$3$. 对于平方损失：\n    a. 计算 $w_{\\text{sq}, S} = (\\sum_{i=1}^n x_i y_i) / (\\sum_{i=1}^n x_i^2)$。\n    b. 计算 $w_{\\text{sq}, S'} = (\\sum_{i=1}^n x_i y'_i) / (\\sum_{i=1}^n x_i^2)$。\n    c. 计算偏移 $\\Delta_{\\text{sq}} = |(w_{\\text{sq}, S} - w_{\\text{sq}, S'}) x_\\star|$。\n$4$. 对于参数为 $\\delta$ 的 Huber 损失：\n    a. 定义目标函数 $J(w) = \\sum_{i=1}^n \\ell_{\\text{Huber}, \\delta}(y_i, w x_i)$。\n    b. 使用数值优化器为原始数据 $\\mathbf{y}$ 找到 $w_{\\text{Huber}, S} = \\arg\\min_w J(w)$。\n    c. 使用相同的优化器为修改后的数据 $\\mathbf{y}'$ 找到 $w_{\\text{Huber}, S'} = \\arg\\min_w J(w)$。\n    d. 计算偏移 $\\Delta_{\\text{Huber}} = |(w_{\\text{Huber}, S} - w_{\\text{Huber}, S'}) x_\\star|$。\n\n该过程将应用于问题陈述中指定的三个测试用例中的每一个。然后，将每个案例得到的 $\\Delta_{\\text{sq}}$ 和 $\\Delta_{\\text{Huber}}$ 值收集起来，并按要求格式化。最终的 Python 程序实现了这些计算。一个辅助函数计算给定 $w$ 的总 Huber 风险，然后将其传递给 `scipy.optimize.minimize_scalar` 以找到最优参数。平方损失使用了闭式解。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import optimize\n\ndef solve():\n    \"\"\"\n    Computes the prediction shift for linear ERM with squared and Huber losses\n    across a suite of test cases, demonstrating algorithmic stability.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"n\": 5, \"x\": np.array([1.0, 2.0, 3.0, 4.0, 5.0]), \"y\": np.array([1.0, 2.0, 3.0, 4.0, 1000.0]),\n            \"k\": 5, \"y_k_prime\": 5.0, \"delta\": 1.0, \"x_star\": 2.0\n        },\n        {\n            \"n\": 4, \"x\": np.array([-2.0, -1.0, 0.5, 1.5]), \"y\": np.array([1.0, -200.0, 0.5, -1.0]),\n            \"k\": 2, \"y_k_prime\": -2.0, \"delta\": 0.5, \"x_star\": -1.0\n        },\n        {\n            \"n\": 3, \"x\": np.array([1.0, 2.0, 3.0]), \"y\": np.array([1.0, 2.0, 3.0]),\n            \"k\": 2, \"y_k_prime\": 2.0, \"delta\": 1.0, \"x_star\": 1.0\n        }\n    ]\n\n    results = []\n\n    def huber_risk(w, x, y, d):\n        \"\"\"Computes the total empirical risk for the Huber loss.\"\"\"\n        residuals = y - w * x\n        abs_residuals = np.abs(residuals)\n        \n        # Use boolean indexing for vectorized computation\n        small_err_mask = abs_residuals = d\n        large_err_mask = ~small_err_mask\n\n        loss = np.zeros_like(residuals, dtype=float)\n        loss[small_err_mask] = 0.5 * residuals[small_err_mask]**2\n        loss[large_err_mask] = d * (abs_residuals[large_err_mask] - 0.5 * d)\n        \n        return np.sum(loss)\n\n    for case in test_cases:\n        x_vec = case[\"x\"]\n        y_vec = case[\"y\"]\n        k = case[\"k\"]\n        y_k_prime = case[\"y_k_prime\"]\n        delta = case[\"delta\"]\n        x_star = case[\"x_star\"]\n\n        y_vec_prime = np.copy(y_vec)\n        # Note: problem uses 1-based indexing for k\n        y_vec_prime[k - 1] = y_k_prime\n\n        # Squared Loss Calculation\n        sum_x_sq = np.dot(x_vec, x_vec)\n        \n        w_sq_S = np.dot(x_vec, y_vec) / sum_x_sq\n        w_sq_S_prime = np.dot(x_vec, y_vec_prime) / sum_x_sq\n        delta_sq = np.abs((w_sq_S - w_sq_S_prime) * x_star)\n        results.append(delta_sq)\n\n        # Huber Loss Calculation\n        # Minimize for the original dataset S\n        res_S = optimize.minimize_scalar(huber_risk, args=(x_vec, y_vec, delta))\n        w_huber_S = res_S.x\n\n        # Minimize for the modified dataset S'\n        res_S_prime = optimize.minimize_scalar(huber_risk, args=(x_vec, y_vec_prime, delta))\n        w_huber_S_prime = res_S_prime.x\n        \n        delta_huber = np.abs((w_huber_S - w_huber_S_prime) * x_star)\n        results.append(delta_huber)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3098786"}, {"introduction": "到目前为止，我们关注的是目标变量 $y$ 中的异常值。但是，当特征 $x$ 中存在异常值时会发生什么呢？本练习 [@problem_id:3098822] 引入了“杠杆点” (leverage points) 这一关键概念，并展示了它们如何对模型产生不成比例的巨大影响。你将推导并实现一种衡量这种影响的方法，并亲眼见证正则化（特别是岭回归）如何成为增强模型稳定性、抵抗高杠杆点影响的强大工具。", "problem": "您将研究岭回归中的算法稳定性，通过量化在移除单个高杠杆率训练点时，学习到的预测器如何变化。请从第一性原理出发，使用线性代数方法。从带有平方损失的岭回归的经验风险最小化（ERM）公式、正规方程以及用于矩阵逆的秩一更新的 Sherman–Morrison 恒等式开始。您也可以使用 Cauchy–Schwarz 不等式以及帽子矩阵和杠杆分数的定义。不要使用或假设任何直接给出最终表达式的结果；相反，应从基本定义推导它们。\n\n给定一个由设计矩阵 $X \\in \\mathbb{R}^{n \\times d}$ 和响应向量 $y \\in \\mathbb{R}^{n}$ 组成的训练集，考虑使用正则化参数 $\\lambda \\gt 0$ 的岭回归，其中学习到的权重向量 $w_S \\in \\mathbb{R}^{d}$ 最小化目标函数\n$$\nJ(w) = \\tfrac{1}{2}\\|X w - y\\|_2^2 + \\tfrac{\\lambda}{2}\\|w\\|_2^2.\n$$\n对于任意 $x \\in \\mathbb{R}^{d}$，将学习到的预测器定义为 $f_S(x) = x^\\top w_S$。令 $S \\setminus \\{i\\}$ 表示移除了第 $i$ 个样本 $(x_i, y_i)$ 的数据集，并令 $f_{S \\setminus \\{i\\}}(x)$ 为相应的预测器。令 $A = X^\\top X + \\lambda I_d$，其中 $I_d$ 是 $d \\times d$ 的单位矩阵。\n\n任务：\n1. 从岭回归的正规方程和 Sherman–Morrison 恒等式出发，推导在任意查询点 $x \\in \\mathbb{R}^{d}$ 上的预测变化 $f_S(x) - f_{S \\setminus \\{i\\}}(x)$ 的精确表达式。该表达式应使用 $A^{-1}$、$x$、$x_i$、残差 $y_i - f_S(x_i)$ 以及岭回归下第 $i$ 个训练点的杠杆分数 $h_{ii}$ 来表示。杠杆分数定义为岭帽子矩阵 $H = X A^{-1} X^\\top$ 的第 $i$ 个对角元素，即 $h_{ii} = x_i^\\top A^{-1} x_i$。\n2. 在由 $A^{-1}$ 导出的二次型上使用 Cauchy–Schwarz 不等式，推导 $\\lvert f_S(x) - f_{S \\setminus \\{i\\}}(x) \\rvert$ 的一个可计算上界，该上界仅依赖于 $\\lvert y_i - f_S(x_i) \\rvert$、杠杆分数 $h_{ii}$ 和查询杠杆率 $h_x = x^\\top A^{-1} x$。\n3. 对于下方的每个测试用例，在一个有限查询集 $Q$ 上计算两个量：\n   - 实际最大预测变化 $\\max_{x \\in Q} \\lvert f_S(x) - f_{S \\setminus \\{i\\}}(x) \\rvert$。\n   - 仅含杠杆率的 Cauchy–Schwarz 上界 $\\max_{x \\in Q} \\Big(\\sqrt{h_x \\, h_{ii}} \\cdot \\lvert y_i - f_S(x_i) \\rvert \\big/ (1 - h_{ii})\\Big)$。\n   同时报告被移除点的杠杆分数 $h_{ii}$。解释高杠杆分数如何指示稳定性风险。\n\n实现要求：\n- 您的程序必须实现该推导过程，并完全按照规定计算所要求的量。\n- 所有向量和矩阵均为实值，且所有计算在维度上是一致的。\n- 本问题中不涉及物理单位或角度。\n\n测试套件：\n对于每个用例，给定 $(X, y, \\lambda, i, Q)$：\n- 用例 1（高杠杆率、大残差、弱正则化）：\n  - $X = \\begin{bmatrix} 0.0 \\\\ 0.2 \\\\ 0.4 \\\\ 0.6 \\\\ 0.8 \\\\ 10.0 \\end{bmatrix}$ 为一个 $n \\times d$ 矩阵，其中 $n = 6$ 且 $d = 1$。\n  - $y = \\begin{bmatrix} 0.0,\\, 0.4,\\, 0.8,\\, 1.2,\\, 1.6,\\, 35.0 \\end{bmatrix}^\\top$。\n  - $\\lambda = 10^{-6}$。\n  - $i = 5$（从零开始的索引；这是第六行，即杠杆点）。\n  - $Q = \\begin{bmatrix} 0.0 \\\\ 1.0 \\\\ 2.0 \\\\ 5.0 \\\\ 10.0 \\end{bmatrix}$ 为一个 $m \\times d$ 矩阵，其中 $d = 1$。\n- 用例 2（相同数据，更强正则化）：\n  - $X, y$ 与用例 1 相同。\n  - $\\lambda = 5.0$。\n  - $i = 5$。\n  - $Q$ 与用例 1 相同。\n- 用例 3（无杠杆率异常值，弱正则化）：\n  - $X = \\begin{bmatrix} 0.0 \\\\ 0.25 \\\\ 0.5 \\\\ 0.75 \\\\ 1.0 \\\\ 1.25 \\end{bmatrix}$ 为一个 $n \\times d$ 矩阵，其中 $n = 6$ 且 $d = 1$。\n  - $y$ 由 $y_j = 1.5 \\cdot x_j + 0.5 + \\varepsilon_j$ 给出，其中 $\\varepsilon = \\begin{bmatrix} 0.0,\\, 0.02,\\, -0.01,\\, 0.0,\\, 0.03,\\, -0.02 \\end{bmatrix}^\\top$，即 $y = \\begin{bmatrix} 0.5,\\, 0.875,\\, 1.24,\\, 1.625,\\, 2.03,\\, 2.355 \\end{bmatrix}^\\top$。\n  - $\\lambda = 10^{-6}$。\n  - $i = 2$。\n  - $Q = \\begin{bmatrix} 0.0 \\\\ 0.5 \\\\ 1.0 \\\\ 1.5 \\end{bmatrix}$。\n- 用例 4（高杠杆率点与趋势一致，弱正则化）：\n  - $X$ 与用例 1 相同。\n  - $y$ 由精确的线性趋势 $y_j = 2.0 \\cdot x_j$ 给出，即 $y = \\begin{bmatrix} 0.0,\\, 0.4,\\, 0.8,\\, 1.2,\\, 1.6,\\, 20.0 \\end{bmatrix}^\\top$。\n  - $\\lambda = 10^{-6}$。\n  - $i = 5$。\n  - $Q$ 与用例 1 相同。\n\n要求的最终输出格式：\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表。\n- 对于每个用例，按此顺序输出三个浮点数：$[\\text{actual\\_max\\_change}, \\, h_{ii}, \\, \\text{cs\\_bound}]$。\n- 将四个用例的结果连接成一个单一的扁平列表。\n- 将每个浮点数四舍五入到 $6$ 位小数。\n- 带有占位符的所需结构示例：$[a\\_1, h\\_1, b\\_1, a\\_2, h\\_2, b\\_2, a\\_3, h\\_3, b\\_3, a\\_4, h\\_4, b\\_4]$。", "solution": "该问题要求通过推导和计算在移除单个数据点时预测器的变化，来分析岭回归的算法稳定性。推导将按要求从第一性原理出发。\n\n### 步骤 1：预测变化的推导（任务 1）\n\n岭回归的目标函数由以下公式给出：\n$$\nJ(w) = \\frac{1}{2}\\|Xw - y\\|_2^2 + \\frac{\\lambda}{2}\\|w\\|_2^2\n$$\n其中 $X \\in \\mathbb{R}^{n \\times d}$ 是设计矩阵，$y \\in \\mathbb{R}^{n}$ 是响应向量，$w \\in \\mathbb{R}^{d}$ 是权重向量，$\\lambda  0$ 是正则化参数。\n\n为了找到最小化 $J(w)$ 的最优权重向量 $w_S$，我们计算关于 $w$ 的梯度并将其设为零：\n$$\n\\nabla_w J(w) = X^\\top(Xw - y) + \\lambda w = 0\n$$\n这导出了岭回归的正规方程：\n$$\n(X^\\top X + \\lambda I_d)w = X^\\top y\n$$\n定义 $A = X^\\top X + \\lambda I_d$，其中 $I_d$ 是 $d \\times d$ 的单位矩阵，则对于完整数据集 $S$ 的解是：\n$$\nw_S = (X^\\top X + \\lambda I_d)^{-1} X^\\top y = A^{-1} X^\\top y\n$$\n由于 $\\lambda  0$ 且 $X^\\top X$ 是半正定的，因此 $A$ 保证是正定的，从而可逆。\n\n现在，考虑数据集 $S \\setminus \\{i\\}$，其中第 $i$ 个样本 $(x_i, y_i)$ 已被移除。令 $X_{\\setminus i} \\in \\mathbb{R}^{(n-1) \\times d}$ 和 $y_{\\setminus i} \\in \\mathbb{R}^{n-1}$ 分别是移除了第 $i$ 行和第 $i$ 个元素的数据矩阵。对于这个缩减数据集的权重向量 $w_{S \\setminus \\{i\\}}$ 是相应岭回归问题的解：\n$$\nw_{S \\setminus \\{i\\}} = (X_{\\setminus i}^\\top X_{\\setminus i} + \\lambda I_d)^{-1} X_{\\setminus i}^\\top y_{\\setminus i}\n$$\n我们可以用完整数据集的量来表示 $X_{\\setminus i}^\\top X_{\\setminus i}$ 和 $X_{\\setminus i}^\\top y_{\\setminus i}$。矩阵 $X^\\top X$ 是外积之和 $\\sum_{j=1}^n x_j x_j^\\top$。移除第 $i$ 个点后得到：\n$$\nX_{\\setminus i}^\\top X_{\\setminus i} = \\sum_{j \\neq i} x_j x_j^\\top = \\left(\\sum_{j=1}^n x_j x_j^\\top\\right) - x_i x_i^\\top = X^\\top X - x_i x_i^\\top\n$$\n类似地，对于叉积项：\n$$\nX_{\\setminus i}^\\top y_{\\setminus i} = \\sum_{j \\neq i} x_j y_j = \\left(\\sum_{j=1}^n x_j y_j\\right) - x_i y_i = X^\\top y - x_i y_i\n$$\n令 $A_{\\setminus i} = X_{\\setminus i}^\\top X_{\\setminus i} + \\lambda I_d$。代入 $X_{\\setminus i}^\\top X_{\\setminus i}$ 的表达式：\n$$\nA_{\\setminus i} = (X^\\top X - x_i x_i^\\top) + \\lambda I_d = (X^\\top X + \\lambda I_d) - x_i x_i^\\top = A - x_i x_i^\\top\n$$\n这表明 $A_{\\setminus i}$ 是 $A$ 的一个秩一更新。我们可以使用 Sherman–Morrison 公式来求其逆：对于一个可逆矩阵 $M$ 和向量 $u, v$，有 $(M - uv^\\top)^{-1} = M^{-1} + \\frac{M^{-1}uv^\\top M^{-1}}{1 - v^\\top M^{-1}u}$。\n令 $M=A$ 和 $u=v=x_i$，我们得到：\n$$\nA_{\\setminus i}^{-1} = (A - x_i x_i^\\top)^{-1} = A^{-1} + \\frac{A^{-1}x_i x_i^\\top A^{-1}}{1 - x_i^\\top A^{-1}x_i}\n$$\n项 $x_i^\\top A^{-1} x_i$ 是第 $i$ 个点的杠杆分数，记为 $h_{ii}$。因此，\n$$\nA_{\\setminus i}^{-1} = A^{-1} + \\frac{A^{-1}x_i x_i^\\top A^{-1}}{1 - h_{ii}}\n$$\n现在我们可以写出 $w_{S \\setminus \\{i\\}}$ 的表达式：\n$$\nw_{S \\setminus \\{i\\}} = A_{\\setminus i}^{-1} (X^\\top y - x_i y_i) = \\left(A^{-1} + \\frac{A^{-1}x_i x_i^\\top A^{-1}}{1 - h_{ii}}\\right) (X^\\top y - x_i y_i)\n$$\n展开此式，并代入 $w_S = A^{-1} X^\\top y$：\n$$\nw_{S \\setminus \\{i\\}} = A^{-1}X^\\top y - A^{-1}x_i y_i + \\frac{A^{-1}x_i x_i^\\top A^{-1}X^\\top y - A^{-1}x_i (x_i^\\top A^{-1}x_i) y_i}{1-h_{ii}} \\\\\n= w_S - A^{-1}x_i y_i + \\frac{A^{-1}x_i x_i^\\top w_S - A^{-1}x_i h_{ii} y_i}{1-h_{ii}}\n$$\n我们来求权重向量的变化量 $w_S - w_{S \\setminus \\{i\\}}$：\n$$\nw_S - w_{S \\setminus \\{i\\}} = A^{-1}x_i y_i - \\frac{A^{-1}x_i (x_i^\\top w_S - h_{ii} y_i)}{1 - h_{ii}} \\\\\n= A^{-1}x_i \\left( y_i - \\frac{x_i^\\top w_S - h_{ii} y_i}{1 - h_{ii}} \\right) \\\\\n= A^{-1}x_i \\left( \\frac{y_i(1 - h_{ii}) - (x_i^\\top w_S - h_{ii} y_i)}{1 - h_{ii}} \\right) \\\\\n= A^{-1}x_i \\left( \\frac{y_i - y_i h_{ii} - x_i^\\top w_S + h_{ii} y_i}{1 - h_{ii}} \\right) \\\\\n= A^{-1}x_i \\frac{y_i - x_i^\\top w_S}{1 - h_{ii}}\n$$\n项 $y_i - x_i^\\top w_S$ 是第 $i$ 个点的残差，即 $y_i - f_S(x_i)$。在查询点 $x$ 处的预测器变化是 $f_S(x) - f_{S \\setminus \\{i\\}}(x) = x^\\top(w_S - w_{S \\setminus \\{i\\}})$。代入权重变化的表达式：\n$$\nf_S(x) - f_{S \\setminus \\{i\\}}(x) = x^\\top \\left( A^{-1}x_i \\frac{y_i - x_i^\\top w_S}{1 - h_{ii}} \\right)\n$$\n重新整理标量，我们得到预测变化的精确表达式：\n$$\nf_S(x) - f_{S \\setminus \\{i\\}}(x) = \\frac{y_i - f_S(x_i)}{1 - h_{ii}} x^\\top A^{-1} x_i\n$$\n\n### 步骤 2：Cauchy-Schwarz 上界的推导（任务 2）\n\n为了界定预测变化的大小，我们取上述推导表达式的绝对值：\n$$\n\\lvert f_S(x) - f_{S \\setminus \\{i\\}}(x) \\rvert = \\left\\lvert \\frac{y_i - f_S(x_i)}{1 - h_{ii}} x^\\top A^{-1} x_i \\right\\rvert = \\frac{\\lvert y_i - f_S(x_i) \\rvert}{\\lvert 1 - h_{ii} \\rvert} \\lvert x^\\top A^{-1} x_i \\rvert\n$$\n由于 $A = X^\\top X + \\lambda I_d$ 且 $\\lambda  0$，因此 $A$ 是对称正定（SPD）矩阵。其逆 $A^{-1}$ 也是对称正定的。对于任何对称正定矩阵 $M$，我们可以定义一个内积 $\\langle u, v \\rangle_M = u^\\top M v$。此内积的 Cauchy-Schwarz 不等式为 $\\lvert \\langle u, v \\rangle_M \\rvert^2 \\le \\langle u, u \\rangle_M \\langle v, v \\rangle_M$。\n将此应用于 $M = A^{-1}$、$u = x$ 和 $v = x_i$：\n$$\n\\lvert x^\\top A^{-1} x_i \\rvert \\le \\sqrt{(x^\\top A^{-1} x)(x_i^\\top A^{-1} x_i)}\n$$\n使用给定的查询杠杆率 $h_x = x^\\top A^{-1} x$ 和点的杠杆分数 $h_{ii} = x_i^\\top A^{-1} x_i$ 的定义，不等式变为：\n$$\n\\lvert x^\\top A^{-1} x_i \\rvert \\le \\sqrt{h_x h_{ii}}\n$$\n已知岭回归的杠杆分数 $h_{ii}$ 在范围 $(0, 1)$ 内。因此，$1 - h_{ii}  0$，且 $\\lvert 1 - h_{ii} \\rvert = 1 - h_{ii}$。将 $\\lvert x^\\top A^{-1} x_i \\rvert$ 的界限代入预测变化大小的表达式中，我们得到上界：\n$$\n\\lvert f_S(x) - f_{S \\setminus \\{i\\}}(x) \\rvert \\le \\frac{\\lvert y_i - f_S(x_i) \\rvert}{1 - h_{ii}} \\sqrt{h_x h_{ii}}\n$$\n这就是所求的可计算上界。\n\n### 步骤 3：数值计算与解释（任务 3）\n\n现在将实现推导出的公式，以计算给定测试用例所需的量。这些量是：\n1.  $\\max_{x \\in Q} \\lvert f_S(x) - f_{S \\setminus \\{i\\}}(x) \\rvert$：在查询集 $Q$ 上的最大实际预测变化。\n2.  $h_{ii}$：被移除点的杠杆分数。\n3.  $\\max_{x \\in Q} \\Big(\\frac{\\lvert y_i - f_S(x_i) \\rvert}{1 - h_{ii}} \\sqrt{h_x h_{ii}}\\Big)$：在查询集 $Q$ 上 Cauchy–Schwarz 上界的最大值。\n\n对这些量在不同测试用例中的分析揭示了关于算法稳定性的关键见解。推导出的界限表明，一个点 $(x_i, y_i)$ 具有影响力（即，移除它会导致预测器发生较大变化）需要同时满足两个条件：\n-   该点具有高杠杆分数 $h_{ii}$，其值接近 $1$。这使得分母 $1 - h_{ii}$ 变小，从而放大了效应。\n-   该点具有大残差 $\\lvert y_i - f_S(x_i) \\rvert$，意味着在完整数据集上训练的模型对它的解释效果不佳。\n\n用例 1（高杠杆率、大残差）和用例 4（高杠杆率、小残差）将突显这种相互作用。用例 2 展示了增加正则化参数 $\\lambda$ 如何影响稳定性，而用例 3 则作为一个没有高杠杆率点的基准。请注意，在 $d=1$ 的情况下，Cauchy-Schwarz 不等式变为等式，因此实际变化和界限将是相同的。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the algorithmic stability problem for ridge regression.\n    The function iterates through predefined test cases, calculates the required\n    quantities based on the derived formulas, and prints the results in the\n    specified format.\n    \"\"\"\n\n    test_cases = [\n        # Case 1: High-leverage with large residual, weak regularization\n        {\n            \"X\": np.array([[0.0], [0.2], [0.4], [0.6], [0.8], [10.0]]),\n            \"y\": np.array([[0.0], [0.4], [0.8], [1.2], [1.6], [35.0]]),\n            \"lambda\": 1e-6,\n            \"i\": 5,\n            \"Q\": np.array([[0.0], [1.0], [2.0], [5.0], [10.0]])\n        },\n        # Case 2: Same data, stronger regularization\n        {\n            \"X\": np.array([[0.0], [0.2], [0.4], [0.6], [0.8], [10.0]]),\n            \"y\": np.array([[0.0], [0.4], [0.8], [1.2], [1.6], [35.0]]),\n            \"lambda\": 5.0,\n            \"i\": 5,\n            \"Q\": np.array([[0.0], [1.0], [2.0], [5.0], [10.0]])\n        },\n        # Case 3: No leverage outlier, weak regularization\n        {\n            \"X\": np.array([[0.0], [0.25], [0.5], [0.75], [1.0], [1.25]]),\n            \"y\": np.array([[0.5], [0.875], [1.24], [1.625], [2.03], [2.355]]),\n            \"lambda\": 1e-6,\n            \"i\": 2,\n            \"Q\": np.array([[0.0], [0.5], [1.0], [1.5]])\n        },\n        # Case 4: High-leverage aligned with the trend, weak regularization\n        {\n            \"X\": np.array([[0.0], [0.2], [0.4], [0.6], [0.8], [10.0]]),\n            \"y\": np.array([[0.0], [0.4], [0.8], [1.2], [1.6], [20.0]]),\n            \"lambda\": 1e-6,\n            \"i\": 5,\n            \"Q\": np.array([[0.0], [1.0], [2.0], [5.0], [10.0]])\n        }\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        X, y, lambda_reg, i, Q = case[\"X\"], case[\"y\"], case[\"lambda\"], case[\"i\"], case[\"Q\"]\n        n, d = X.shape\n\n        # Step 1: Compute full dataset solution w_S\n        A = X.T @ X + lambda_reg * np.eye(d)\n        A_inv = np.linalg.inv(A)\n        w_S = A_inv @ X.T @ y\n\n        # Step 2: Extract point i and compute its leverage and residual\n        x_i_row = X[i:i+1, :]  # Shape (1, d)\n        x_i_col = x_i_row.T    # Shape (d, 1)\n        y_i_val = y[i, 0]\n\n        # Leverage score h_ii = x_i^T A^{-1} x_i\n        h_ii = (x_i_row @ A_inv @ x_i_col)[0, 0]\n\n        # Residual r_i = y_i - f_S(x_i)\n        f_S_xi = (x_i_row @ w_S)[0, 0]\n        r_i = y_i_val - f_S_xi\n\n        # Step 3: Compute actual maximum prediction change over Q\n        # Change formula: (r_i / (1 - h_ii)) * x^T A^{-1} x_i\n        change_factor = r_i / (1 - h_ii)\n        \n        # Q @ A_inv @ x_i_col broadcasts the calculation over all x in Q\n        cross_terms = Q @ A_inv @ x_i_col # Shape (m, 1)\n        \n        actual_changes = np.abs(change_factor * cross_terms)\n        actual_max_change = np.max(actual_changes)\n\n        # Step 4: Compute the Cauchy-Schwarz upper bound over Q\n        # Bound formula: |r_i|/(1-h_ii) * sqrt(h_x * h_ii)\n        \n        # Compute query leverages h_x = x^T A^{-1} x for all x in Q\n        # np.sum((Q @ A_inv) * Q, axis=1) is an efficient way to get diagonals of Q A_inv Q.T\n        h_Q = np.sum((Q @ A_inv) * Q, axis=1) # Shape (m,)\n        \n        bounds_vec = (np.abs(r_i) / (1 - h_ii)) * np.sqrt(h_Q * h_ii)\n        cs_bound = np.max(bounds_vec)\n\n        # Append results for this case\n        results.extend([actual_max_change, h_ii, cs_bound])\n        \n    # Format the final output string as per the problem description\n    formatted_results = [f\"{val:.6f}\" for val in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3098822"}]}