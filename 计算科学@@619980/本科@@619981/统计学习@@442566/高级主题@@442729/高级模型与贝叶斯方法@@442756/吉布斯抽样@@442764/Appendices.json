{"hands_on_practices": [{"introduction": "要理解任何复杂的算法，最好从一个具体的步骤开始。这个练习 [@problem_id:1920320] 通过引导你完成一次完整的迭代，揭开了吉布斯采样器的神秘面纱。你将看到如何利用每个变量的条件分布，在给定其他变量当前值的情况下，对其进行序贯更新，从而使系统从一个状态转移到下一个状态。", "problem": "考虑一个二维随机向量 $(X, Y)$，其联合概率分布由以下完全条件分布定义：\n- 给定 $Y=y$ 时，$X$ 的条件分布是率参数为 $y$ 的指数分布。其概率密度函数为 $p(x|y) = y \\exp(-yx)$，其中 $x > 0$。\n- 给定 $X=x$ 时，$Y$ 的条件分布是均值参数为 $x$ 的泊松分布。其概率质量函数为 $p(y=k|x) = \\frac{x^k \\exp(-x)}{k!}$，其中 $k \\in \\{0, 1, 2, \\dots\\}$。\n\n您的任务是执行吉布斯采样器的一次完整迭代。从初始状态 $(x^{(0)}, y^{(0)}) = (2, 3)$ 开始，您将生成一个新状态 $(x^{(1)}, y^{(1)})$。迭代的步骤如下：首先，从分布 $p(x|y^{(0)})$ 中抽取一个样本 $x^{(1)}$，然后，使用这个新值 $x^{(1)}$，从分布 $p(y|x^{(1)})$ 中抽取一个样本 $y^{(1)}$。\n\n为了生成所需的随机变量，您必须使用逆变换采样法。使用以下从均匀(0,1)分布中抽取的随机数：\n- 为了生成 $x^{(1)}$，使用均匀随机数 $u_x = 0.600$。\n- 为了生成 $y^{(1)}$，使用均匀随机数 $u_y = 0.750$。\n\n新状态 $(x^{(1)}, y^{(1)})$ 的数值是多少？$x^{(1)}$ 的值必须四舍五入到四位有效数字。", "solution": "我们使用逆变换采样法执行一次吉布斯更新。\n\n1) 从 $p(x \\mid y^{(0)}=3)$ 中采样 $x^{(1)}$。\n对于率参数为 $y$ 的指数分布，其条件累积分布函数(CDF)为\n$$\nF(x \\mid y)=1-\\exp(-yx), \\quad x>0.\n$$\n逆变换采样使用 $u_{x}=F(x \\mid y)$，因此\n$$\nx^{(1)}=F^{-1}(u_{x})=-\\frac{1}{y^{(0)}}\\ln\\!\\bigl(1-u_{x}\\bigr).\n$$\n当 $y^{(0)}=3$ 且 $u_{x}=0.600$ 时，\n$$\nx^{(1)}=-\\frac{1}{3}\\ln(1-0.600)=-\\frac{1}{3}\\ln(0.4)=\\frac{1}{3}\\ln(2.5)\\approx 0.3054302439.\n$$\n四舍五入到四位有效数字：$x^{(1)}=0.3054$。\n\n2) 使用 $u_{y}=0.750$ 从 $p(y \\mid x^{(1)})$ 中采样 $y^{(1)}$。\n对于均值为 $x$ 的泊松分布，其概率质量函数(pmf)为\n$$\np(y=k \\mid x)=\\frac{x^{k}\\exp(-x)}{k!}, \\quad k\\in\\{0,1,2,\\dots\\}.\n$$\n对于离散分布，逆变换采样选择满足 $F(k \\mid x)=\\sum_{j=0}^{k}p(j \\mid x)\\ge u_{y}$ 的最小 $k$。\n\n当 $x=x^{(1)}=\\frac{1}{3}\\ln(2.5)$ 时，计算\n$$\np(0 \\mid x)=\\exp(-x)=\\exp\\!\\Bigl(-\\tfrac{1}{3}\\ln(2.5)\\Bigr)=2.5^{-1/3}\\approx 0.7368.\n$$\n由于 $p(0 \\mid x)=0.7368  0.750$，我们继续计算 $k=1$ 的情况：\n$$\np(1 \\mid x)=x\\exp(-x)=x\\,2.5^{-1/3}\\approx 0.30543\\times 0.7368\\approx 0.2250.\n$$\n然后\n$$\nF(1 \\mid x)=p(0 \\mid x)+p(1 \\mid x)\\approx 0.7368+0.2250=0.96180.750,\n$$\n因此，满足 $F(k \\mid x)\\ge 0.750$ 的最小 $k$ 是 $k=1$。所以 $y^{(1)}=1$。\n\n因此，新状态为 $(x^{(1)},y^{(1)})=(0.3054,1)$，其中 $x^{(1)}$ 已四舍五入到四位有效数字。", "answer": "$$\\boxed{\\begin{pmatrix}0.3054  1\\end{pmatrix}}$$", "id": "1920320"}, {"introduction": "虽然功能强大，但吉布斯采样并非万能丹，在某些情况下可能会失效。这个实践 [@problem_id:3125089] 探讨了一种经典的失效模式：采样器被困在概率空间的某个区域，无法探索整个分布。通过实现一个巧妙的重参数化解决方案，你将对马尔可夫链蒙特卡洛（MCMC）方法中至关重要的遍历性（ergodicity）概念有更深刻的理解。", "problem": "考虑以下在二维向量 $X = (X_1, X_2) \\in \\mathbb{R}^2$ 上的目标分布。设其支撑集为两个等权重的、不相交的轴对齐矩形的并集。定义\n$$\n\\mathcal{A} = \\{(x_1, x_2): x_1 \\in [-3,-1],\\ x_2 \\in [-3,-1]\\},\n\\quad\n\\mathcal{B} = \\{(x_1, x_2): x_1 \\in [1,3],\\ x_2 \\in [1,3]\\},\n$$\n目标密度为\n$$\n\\pi(x_1,x_2) \\propto \\mathbf{1}_{\\mathcal{A}}(x_1,x_2) + \\mathbf{1}_{\\mathcal{B}}(x_1,x_2),\n$$\n其中 $\\mathbf{1}_{S}$ 是集合 $S$ 的指示函数。两个分量 $\\mathcal{A}$ 和 $\\mathcal{B}$ 互不重叠，并由一个零概率区域隔开。\n\n从核心定义开始：吉布斯采样器是一种马尔可夫链，它通过从全条件分布中采样来迭代更新坐标。如果一个马尔可夫链能够以正概率访问支撑集中的任何状态，并且拥有一个唯一的平稳分布，使得从任何起始点出发都能收敛到该分布，那么它就是不可约和遍历的。对于此目标分布，在原始 $(x_1, x_2)$ 坐标系下的全条件分布为：\n- 如果 $x_2 \\in [-3,-1]$，则 $X_1 \\mid X_2 = x_2 \\sim \\text{Uniform}([-3,-1])$。\n- 如果 $x_2 \\in [1,3]$，则 $X_1 \\mid X_2 = x_2 \\sim \\text{Uniform}([1,3])$。\n- 对于 $X_2 \\mid X_1$ 的情况类似。\n\n通过观察可以发现，如果链从 $\\mathcal{A}$ 开始，条件分布会强制它永远停留在 $\\mathcal{A}$ 中；对于 $\\mathcal{B}$ 也是如此。这违反了不可约性，并展示了吉布斯采样器在非遍历、支撑集不连通的设置中的一种失效模式。\n\n一种补救方法是使用一个旋转来重参数化状态，该旋转将不连通的部分与单个坐标对齐。定义一个到新坐标 $(U,V)$ 的双射线性映射：\n$$\nU = X_1 - X_2,\\quad V = X_1 + X_2,\n$$\n其逆映射为\n$$\nX_1 = \\frac{U+V}{2},\\quad X_2 = \\frac{V-U}{2}.\n$$\n在此映射下，支撑集在 $(u,v)$ 空间中变成了两个不相交的水平带：\n$$\n\\mathcal{A}_{uv} = \\{(u,v): u \\in [-2,2],\\ v \\in [-6,-2]\\},\\quad\n\\mathcal{B}_{uv} = \\{(u,v): u \\in [-2,2],\\ v \\in [2,6]\\}.\n$$\n(u,v) 坐标系下的全条件分布具有以下形式：\n- 对于任何 $v \\in [-6,-2] \\cup [2,6]$，我们有 $U \\mid V=v \\sim \\text{Uniform}([-2,2])$。\n- 对于任何 $u \\in [-2,2]$，我们有 $V \\mid U=u$ 的支撑集在并集 $[-6,-2] \\cup [2,6]$ 上，并且在两个区间上的权重相等（因为目标分布为等长度的带分配了等质量）。对 $V$ 的采样会在这两个不相交的区间之间交替进行，从而在映射回原始空间后，能够在不连通的分量之间移动。\n\n您的任务是实现并比较两种吉布斯采样器：\n\n1. 原始坐标 $(x_1,x_2)$ 中的吉布斯采样器。\n   - 在每次迭代中，根据当前的 $X_2$ 从其全条件分布更新 $X_1$。\n   - 然后根据更新后的 $X_1$ 从其全条件分布更新 $X_2$。\n   - 由于支撑集不连通，该采样器是非遍历的，并将始终停留在初始化时所在的分量中。\n\n2. 使用上述旋转的重参数化 $(u,v)$ 吉布斯采样器。\n   - 在每次迭代中，通过在 $[-2,2]$ 上均匀采样来更新 $U \\mid V$。\n   - 然后更新 $V \\mid U$，首先以等概率选择两个区间 $[-6,-2]$ 或 $[2,6]$ 中的一个，然后在本轮选择的区间内均匀采样。\n   - 仅为了评估，通过 $x_1 = (u+v)/2$，$x_2 = (v-u)/2$ 将样本映射回 $(x_1,x_2)$。\n\n诊断和可测量的量。对于给定的运行，计算经验分数\n$$\n\\widehat{p}_+ = \\frac{1}{T - B}\\sum_{t=B+1}^{T} \\mathbf{1}\\{X_1^{(t)}  0\\},\n$$\n其中 $T$ 是总迭代次数， $B$ 是预烧期，$X_1^{(t)}$ 是第 $t$ 次迭代时的第一个坐标。在卡住的原始坐标吉布斯采样器中，如果从 $\\mathcal{A}$ 初始化，$\\widehat{p}_+$ 恰好为 $0$；如果从 $\\mathcal{B}$ 初始化，则恰好为 $1$。在重参数化的采样器中，链会遍历两个分量，$\\widehat{p}_+$ 应该接近 $0.5$。\n\n程序要求：\n- 严格按照规定实现两种采样器。\n- 为每个测试用例使用固定的随机数生成器种子以保证可复现性。\n- 对于每个测试用例，运行采样器 $T$ 次迭代，预烧期为 $B$，并报告四舍五入到三位小数的 $\\widehat{p}_+$。\n\n测试套件：\n- 案例1（原始 $(x_1,x_2)$，卡在 $\\mathcal{A}$ 中）：$T=10000$，$B=1000$，种子 $= 123$，在 $(x_1,x_2)=(-2.5,-2.0)$ 处初始化。\n- 案例2（原始 $(x_1,x_2)$，卡在 $\\mathcal{B}$ 中）：$T=10000$，$B=1000$，种子 $= 456$，在 $(x_1,x_2)=(2.5,2.1)$ 处初始化。\n- 案例3（重参数化 $(u,v)$，在分量间遍历）：$T=10000$，$B=1000$，种子 $= 789$，在 $\\mathcal{A}$ 中的 $(x_1,x_2)=(-2.0,-2.0)$ 处初始化，然后映射到 $(u,v)$。\n\n最终输出格式：\n您的程序应生成单行输出，其中包含按测试用例顺序排列的结果，格式为方括号括起来的逗号分隔列表，并四舍五入到三位小数，例如：\"[0.000,1.000,0.503]\"。", "solution": "该问题要求实现并比较两种吉布斯采样策略，用于处理一个具有不连通支撑集的双峰目标分布。其目标是展示吉布斯采样器的一种常见失效模式，以及如何通过重参数化来解决该问题。\n\n目标概率密度 $\\pi(x_1, x_2)$ 定义在 $\\mathbb{R}^2$ 上，与两个不相交集合上的指示函数之和成正比。其支撑集是两个轴对齐正方形 $\\mathcal{A}$ 和 $\\mathcal{B}$ 的并集，定义如下：\n$$\n\\mathcal{A} = \\{(x_1, x_2): x_1 \\in [-3,-1],\\ x_2 \\in [-3,-1]\\}\n$$\n$$\n\\mathcal{B} = \\{(x_1, x_2): x_1 \\in [1,3],\\ x_2 \\in [1,3]\\}\n$$\n密度由以下公式给出：\n$$\n\\pi(x_1,x_2) \\propto \\mathbf{1}_{\\mathcal{A}}(x_1,x_2) + \\mathbf{1}_{\\mathcal{B}}(x_1,x_2)\n$$\n其中 $\\mathbf{1}_{S}$ 是集合 $S$ 的指示函数。由于 $\\mathcal{A}$ 和 $\\mathcal{B}$ 的面积相等（均为 $4$），该分布为每个分量分配了相等的概率质量 $0.5$。\n\n**采样器1：原始坐标吉布斯采样器**\n吉布斯采样器通过在给定所有其他变量当前值的情况下，从其全条件分布中迭代地对每个变量进行采样，从而从一个多元分布中生成一系列样本。对于目标密度 $\\pi(x_1, x_2)$，全条件分布为：\n\n1.  给定 $X_2=x_2$ 时 $X_1$ 的分布：\n    - 如果 $x_2 \\in [-3,-1]$，状态 $(x_1, x_2)$ 必须位于 $\\mathcal{A}$。因此，$X_1$ 的条件分布是在相应区间上的均匀分布：$X_1 \\mid X_2=x_2 \\sim \\text{Uniform}([-3,-1])$。\n    - 如果 $x_2 \\in [1,3]$，状态必须位于 $\\mathcal{B}$，所以 $X_1 \\mid X_2=x_2 \\sim \\text{Uniform}([1,3])$。\n    - 对于任何其他的 $x_2$ 值，由于概率为零，条件密度未定义。\n\n2.  给定 $X_1=x_1$ 时 $X_2$ 的分布：\n    - 根据对称性，如果 $x_1 \\in [-3,-1]$，则 $X_2 \\mid X_1=x_1 \\sim \\text{Uniform}([-3,-1])$。\n    - 如果 $x_1 \\in [1,3]$，则 $X_2 \\mid X_1=x_1 \\sim \\text{Uniform}([1,3])$。\n\n从迭代 $(t)$ 到 $(t+1)$ 的算法如下：\n1.  采样 $x_1^{(t+1)} \\sim p(x_1 \\mid x_2^{(t)})$。\n2.  采样 $x_2^{(t+1)} \\sim p(x_2 \\mid x_1^{(t+1)})$。\n\n这个采样器是非遍历的。一个遍历的马尔可夫链必须是不可约的，意味着它可以从其状态空间的任何部分到达任何其他部分。在这里，如果链以一个状态 $(x_1^{(0)}, x_2^{(0)}) \\in \\mathcal{A}$ 初始化，那么 $x_1$ 的条件分布将只在 $[-3,-1]$ 上有支撑，随后的 $x_2$ 的条件分布也将在 $[-3,-1]$ 上有支撑。链将被无限期地困在 $\\mathcal{A}$ 内部。类似地，一个在 $\\mathcal{B}$ 中初始化的链永远不会转移到 $\\mathcal{A}$。这违反了不可约性，采样器无法收敛到真实的目标分布 $\\pi$。\n\n**采样器2：重参数化吉布斯采样器**\n为克服此问题，我们使用一个双射线性映射（一个45度旋转和缩放）来重参数化状态空间：\n$$\nU = X_1 - X_2, \\quad V = X_1 + X_2\n$$\n其逆变换为：\n$$\nX_1 = \\frac{U+V}{2}, \\quad X_2 = \\frac{V-U}{2}\n$$\n在此映射下，支撑区域在 $(u,v)$ 平面中变换为不相交的水平带：\n- 对于 $(x_1,x_2) \\in \\mathcal{A}$：$u=x_1-x_2 \\in [-2,2]$ 且 $v=x_1+x_2 \\in [-6,-2]$。所以，$\\mathcal{A}_{uv} = \\{(u,v): u \\in [-2,2], v \\in [-6,-2]\\}$。\n- 对于 $(x_1,x_2) \\in \\mathcal{B}$：$u=x_1-x_2 \\in [-2,2]$ 且 $v=x_1+x_2 \\in [2,6]$。所以，$\\mathcal{B}_{uv} = \\{(u,v): u \\in [-2,2], v \\in [2,6]\\}$。\n\n该变换的雅可比行列式是一个常数 $|J|=2$，因此变换后的密度 $\\pi_{uv}(u,v)$ 在新的支撑集 $\\mathcal{A}_{uv} \\cup \\mathcal{B}_{uv}$ 上保持均匀。在 $(u,v)$ 坐标系下的全条件分布为：\n\n1.  给定 $V=v$ 时 $U$ 的分布：\n    - 对于任何 $v \\in [-6,-2] \\cup [2,6]$，$U$ 的支撑集是区间 $[-2,2]$。因此，条件分布是 $U \\mid V=v \\sim \\text{Uniform}([-2,2])$。\n\n2.  给定 $U=u$ 时 $V$ 的分布：\n    - 对于任何 $u \\in [-2,2]$，$V$ 的支撑集是两个不相交区间的并集：$[-6,-2] \\cup [2,6]$。原始分布为 $\\mathcal{A}$ 和 $\\mathcal{B}$ 分配了相等的质量。线性变换保留了均匀性和相对质量。$V$ 的两个区间的长度相等（为 $4$）。因此，$V$ 的条件分布包括以等概率（$0.5$）选择两个区间中的一个，然后在其内部进行均匀采样。\n\n从迭代 $(t)$ 到 $(t+1)$ 的算法如下：\n1.  采样 $u^{(t+1)} \\sim p(u \\mid v^{(t)}) = \\text{Uniform}([-2,2])$。\n2.  采样 $v^{(t+1)} \\sim p(v \\mid u^{(t+1)})$，这是通过以各 $0.5$ 的概率从 $\\{[-6,-2], [2,6]\\}$ 中选择一个区间，然后在所选区间内进行均匀采样来完成的。\n\n这个重参数化的采样器是遍历的。$V$ 的条件分布明确允许链在对应于原始模式 $\\mathcal{A}$ 和 $\\mathcal{B}$ 的两个区域之间跳跃。因此，该链可以探索分布的整个支撑集。\n\n**诊断度量**\n采样器的性能通过计算 $X_1  0$ 的样本的经验分数来评估：\n$$\n\\widehat{p}_+ = \\frac{1}{T - B}\\sum_{t=B+1}^{T} \\mathbf{1}\\{X_1^{(t)}  0\\}\n$$\n其中 $T$ 是总迭代次数， $B$ 是预烧期。\n- 对于在 $\\mathcal{A}$ 中初始化的原始采样器，其中 $x_1  0$ 总是成立，我们预期 $\\widehat{p}_+ = 0$。\n- 对于在 $\\mathcal{B}$ 中初始化的原始采样器，其中 $x_1  0$ 总是成立，我们预期 $\\widehat{p}_+ = 1$。\n- 对于重参数化的采样器，它会探索两种模式，我们预期它会花费大约一半的时间在对应于 $\\mathcal{B}$ 的区域（其中 $x_10$），另一半时间在对应于 $\\mathcal{A}$ 的区域（其中 $x_10$）。因此，我们预期 $\\widehat{p}_+ \\approx 0.5$。\n\n这些测试用例将验证这些理论预期。\n- 案例1：原始采样器，$T=10000$，$B=1000$，种子 $= 123$，初始点 $(-2.5,-2.0) \\in \\mathcal{A}$。\n- 案例2：原始采样器，$T=10000$，$B=1000$，种子 $= 456$，初始点 $(2.5,2.1) \\in \\mathcal{B}$。\n- 案例3：重参数化采样器，$T=10000$，$B=1000$，种子 $= 789$，初始点 $(-2.0,-2.0)$（在 $\\mathcal{A}$ 中）。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef gibbs_original(T, B, seed, initial_x):\n    \"\"\"\n    Implements the Gibbs sampler in the original (x1, x2) coordinates.\n    This sampler is expected to get stuck in one of the two modes.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    x1, x2 = initial_x\n    \n    samples_x1 = []\n    \n    for t in range(T):\n        # Update x1 based on the current x2\n        if -3.0 = x2 = -1.0:\n            x1 = rng.uniform(-3.0, -1.0)\n        elif 1.0 = x2 = 3.0:\n            x1 = rng.uniform(1.0, 3.0)\n        else:\n            # This case should not be reached with valid initial points\n            raise ValueError(\"x2 is outside the support.\")\n            \n        # Update x2 based on the new x1\n        if -3.0 = x1 = -1.0:\n            x2 = rng.uniform(-3.0, -1.0)\n        elif 1.0 = x1 = 3.0:\n            x2 = rng.uniform(1.0, 3.0)\n        else:\n            raise ValueError(\"x1 is outside the support.\")\n\n        if t = B:\n            samples_x1.append(x1)\n            \n    count_positive = sum(1 for s in samples_x1 if s  0)\n    p_hat_plus = count_positive / (T - B)\n    return p_hat_plus\n\ndef gibbs_reparameterized(T, B, seed, initial_x):\n    \"\"\"\n    Implements the Gibbs sampler in the reparameterized (u, v) coordinates.\n    This sampler should be ergodic and explore both modes.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    \n    # Map initial point from (x1, x2) to (u, v)\n    x1_init, x2_init = initial_x\n    u = x1_init - x2_init\n    v = x1_init + x2_init\n    \n    samples_x1 = []\n    \n    for t in range(T):\n        # Update u by sampling from its full conditional U | V=v\n        u = rng.uniform(-2.0, 2.0)\n        \n        # Update v by sampling from its full conditional V | U=u\n        # This involves choosing one of two intervals with equal probability\n        if rng.random()  0.5:\n            # Sample from the interval [-6, -2]\n            v = rng.uniform(-6.0, -2.0)\n        else:\n            # Sample from the interval [2, 6]\n            v = rng.uniform(2.0, 6.0)\n\n        if t = B:\n            # Map back to (x1, x2) for evaluation and store x1\n            x1 = (u + v) / 2.0\n            samples_x1.append(x1)\n            \n    count_positive = sum(1 for s in samples_x1 if s  0)\n    p_hat_plus = count_positive / (T - B)\n    return p_hat_plus\n    \ndef solve():\n    \"\"\"\n    Defines and runs the test cases specified in the problem statement.\n    \"\"\"\n    test_cases = [\n        {'sampler': 'original', 'T': 10000, 'B': 1000, 'seed': 123, 'initial_point': (-2.5, -2.0)},\n        {'sampler': 'original', 'T': 10000, 'B': 1000, 'seed': 456, 'initial_point': (2.5, 2.1)},\n        {'sampler': 'reparameterized', 'T': 10000, 'B': 1000, 'seed': 789, 'initial_point': (-2.0, -2.0)},\n    ]\n\n    results = []\n    for case in test_cases:\n        if case['sampler'] == 'original':\n            result = gibbs_original(case['T'], case['B'], case['seed'], case['initial_point'])\n        elif case['sampler'] == 'reparameterized':\n            result = gibbs_reparameterized(case['T'], case['B'], case['seed'], case['initial_point'])\n        \n        results.append(f\"{result:.3f}\")\n\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3125089"}, {"introduction": "现在，让我们将所学技能应用于一个重要的机器学习任务：在数据中发现隐藏的簇。这个练习 [@problem_id:3235855] 将指导你为一个贝叶斯高斯混合模型实现吉布斯采样器，以执行聚类分析。你将看到采样器如何迭代地优化对簇归属、簇特性及其相对比例的估计，从而揭示数据中潜在的结构。", "problem": "实现一个完整的贝叶斯高斯混合模型吉布斯采样器，对一维或二维数据进行聚类。该实现必须是一个单一、独立的程序，仅使用 Numerical Python (NumPy) 库和 Python 标准库。采样器必须使用一个已知的球形协方差的共轭贝叶斯模型，并且必须通过贝叶斯定理推导出的全条件分布来更新成分分配、成分均值和混合权重。最终输出必须是定量的，并可通过提供的测试套件进行测试。\n\n推导的基本依据必须从混合模型的定义、贝叶斯定理和条件独立性，以及高斯族中关于共轭先验的成熟理论出发。具体来说，该模型是一个有限混合模型，混合权重具有对称狄利克雷先验，成分均值具有正态先验，其似然由具有已知球形协方差的高斯分布给出。采样器必须通过推导潜成分指标、混合权重和成分均值的全条件分布来构建。\n\n模型假设：\n- 数据集由点 $\\{x_i\\}_{i=1}^N$ 组成，其中 $x_i \\in \\mathbb{R}^d$，且 $d \\in \\{1,2\\}$。\n- 共有 $K$ 个混合成分。每个数据点 $x_i$ 都有一个潜成分标签 $z_i \\in \\{0,1,\\dots,K-1\\}$。\n- 对于所有成分 $k$，似然为 $x_i \\mid z_i=k, \\mu_k \\sim \\mathcal{N}(\\mu_k, \\sigma^2 I_d)$，其中 $I_d$ 是 $d \\times d$ 的单位矩阵，$\\sigma^2$ 是已知的。\n- 每个成分均值的先验为 $\\mu_k \\sim \\mathcal{N}(\\mu_0, \\tau^2 I_d)$，其中 $\\mu_0$ 和 $\\tau^2$ 是已知的。\n- 混合权重为 $\\pi = (\\pi_0,\\dots,\\pi_{K-1}) \\sim \\operatorname{Dirichlet}(\\alpha,\\dots,\\alpha)$，其中对称集中度参数为 $\\alpha$。\n\n您的程序必须：\n- 使用贝叶斯定理以及正态似然与正态先验、分类似然与狄利克雷先验的共轭性，推导并实现 $z_i$、$\\pi$ 和 $\\mu_k$ 的全条件分布。\n- 通过将已知协方差视为 $\\sigma^2 I_d$ 来支持 $d=1$ 和 $d=2$ 两种情况。\n- 每个测试用例使用固定的随机种子以确保可复现性。\n- 运行吉布斯采样器指定的迭代次数，并通过对每个数据点取马尔可夫链最后 $L$ 次迭代中最频繁的标签来生成最终的预测聚类标签。\n- 使用调整兰德指数（ARI）计算聚类质量，表示为闭区间 $[0,1]$ 内的小数。\n\n调整兰德指数（ARI）必须通过配对计数从真实标签和预测标签的列联表中精确计算，并且必须报告为小数点后保留四位的十进制数。不涉及物理单位。不涉及角度。不得使用百分比；ARI 必须表示为小数。\n\n测试套件：\n提供三个测试用例，用于检验采样器的不同方面。在所有情况下，数据都是从已知的高斯混合模型中综合生成，然后由实现的吉布斯采样器进行聚类。程序必须嵌入并运行以下测试用例，无需任何用户输入：\n\n- 用例 $1$（理想路径，一维，两个清晰分离的簇）：\n    - 维度：$d=1$。\n    - 真实混合均值：$\\{-3.0, 3.0\\}$。\n    - 已知标准差：$\\sigma=0.6$（因此方差 $\\sigma^2=0.36$）。\n    - 每个成分的点数：$\\{30, 30\\}$，因此 $N=60$。\n    - 模型中的成分数：$K=2$。\n    - 先验均值：$\\mu_0=0.0$。\n    - 先验方差缩放：$\\tau^2=4.0$。\n    - 狄利克雷集中度：$\\alpha=1.0$（对称）。\n    - 吉布斯迭代次数：$T=1000$。\n    - 共识窗口长度：$L=200$（使用最后 $L$ 次迭代为每个点形成共识分配）。\n    - 随机种子：$123$。\n    - 此用例的必需输出：ARI，表示为四舍五入到小数点后四位的小数。\n\n- 用例 $2$（二维，三个中等分离的簇）：\n    - 维度：$d=2$。\n    - 真实混合均值：$\\{(-3.0,-3.0), (0.0,4.0), (4.0,-1.0)\\}$。\n    - 已知标准差：$\\sigma=0.7$（因此方差 $\\sigma^2=0.49$）。\n    - 每个成分的点数：$\\{40, 40, 40\\}$，因此 $N=120$。\n    - 模型中的成分数：$K=3$。\n    - 先验均值：$\\mu_0=(0.0, 0.0)$。\n    - 先验方差缩放：$\\tau^2=4.0$。\n    - 狄利克雷集中度：$\\alpha=1.0$（对称）。\n    - 吉布斯迭代次数：$T=1200$。\n    - 共识窗口长度：$L=200$。\n    - 随机种子：$321$。\n    - 此用例的必需输出：ARI，表示为四舍五入到小数点后四位的小数。\n\n- 用例 $3$（边界情况，来自两个簇的一维数据，但模型指定三个成分，测试空成分或冗余成分的处理）：\n    - 维度：$d=1$。\n    - 真实混合均值：$\\{-2.0, 2.0\\}$。\n    - 已知标准差：$\\sigma=0.8$（因此方差 $\\sigma^2=0.64$）。\n    - 每个成分的点数：$\\{50, 50\\}$，因此 $N=100$。\n    - 模型中的成分数：$K=3$。\n    - 先验均值：$\\mu_0=0.0$。\n    - 先验方差缩放：$\\tau^2=4.0$。\n    - 狄利克雷集中度：$\\alpha=1.0$（对称）。\n    - 吉布斯迭代次数：$T=1200$。\n    - 共识窗口长度：$L=200$。\n    - 随机种子：$555$。\n    - 此用例的必需输出：ARI，表示为四舍五入到小数点后四位的小数。\n\n最终输出格式：\n您的程序应生成单行输出，其中包含用方括号括起来的逗号分隔的结果列表。该列表必须精确包含用例 1、用例 2、用例 3 的 ARI，每个值都四舍五入到小数点后四位。例如，输出必须类似于 `[0.9876,0.9543,0.8732]`（这只是格式示例，不是预期的数值）。", "solution": "问题陈述概述了一个完整且定义明确的计算统计学任务：实现一个用于贝叶斯高斯混合模型（GMM）的吉布斯采样器，并使用调整兰德指数（ARI）在合成数据集上评估其性能。\n\n### 第一步：提取已知条件\n\n-   **模型**：用于聚类的贝叶斯高斯混合模型。\n-   **数据**：$N$ 个点 $\\{x_i\\}_{i=1}^N$，其中 $x_i \\in \\mathbb{R}^d$，维度 $d$ 为 1 或 2。\n-   **潜变量**：每个数据点 $x_i$ 都有一个成分标签 $z_i \\in \\{0, 1, \\dots, K-1\\}$。\n-   **似然**：给定成分分配 $z_i=k$ 和成分均值 $\\mu_k$ 时，数据点 $x_i$ 的分布是多元正态分布，$p(x_i | z_i=k, \\mu_k) = \\mathcal{N}(x_i | \\mu_k, \\sigma^2 I_d)$。协方差是球形的，具有已知的方差 $\\sigma^2$。$I_d$ 是 $d \\times d$ 的单位矩阵。\n-   **先验**：\n    -   成分均值：对于 $k \\in \\{0, \\dots, K-1\\}$，$\\mu_k \\sim \\mathcal{N}(\\mu_0, \\tau^2 I_d)$。超参数 $\\mu_0$ 和 $\\tau^2$ 是已知的。\n    -   混合权重：$\\pi = (\\pi_0, \\dots, \\pi_{K-1}) \\sim \\operatorname{Dirichlet}(\\alpha, \\dots, \\alpha)$。对称集中度超参数 $\\alpha$ 是已知的。\n-   **任务**：通过推导并使用 $z_i$、$\\pi$ 和 $\\mu_k$ 的全条件分布来实现吉布斯采样器。\n-   **执行**：采样器运行 $T$ 次迭代。最终的聚类分配通过为每个点找到在最后 $L$ 次迭代中最频繁（众数）的分配来确定。\n-   **评估**：聚类的质量通过与综合生成的真实标签比较的调整兰德指数（ARI）来衡量。\n-   **测试套件**：提供了三个具体的测试用例，包含所有必要的参数：\n    -   用例 1：$d=1, K=2$，清晰分离。\n    -   用例 2：$d=2, K=3$，中等分离。\n    -   用例 3：$d=1$，数据来自一个双成分混合，但模型假设 $K=3$。\n-   **输出**：三个测试用例的 ARI 值列表，格式为 `[ari1,ari2,ari3]`，每个 ARI 四舍五入到小数点后四位。\n\n### 第二步：使用提取的已知条件进行验证\n\n-   **科学上合理**：该问题是贝叶斯推断和马尔可夫链蒙特卡洛（MCMC）方法，特别是吉布斯采样的标准应用。该模型是经典的、带共轭先验的 GMM，是无监督机器学习的基石。所有原理都源于概率论和统计学。该问题有效。\n-   **定义良好**：使用共轭先验确保了全条件分布是标准的、定义良好的分布，从中进行采样是直接的。问题规范提供了足够的细节（超参数、迭代次数等），以确保可以实现一个唯一的程序。固定的随机种子确保了结果的可复现性。该问题有效。\n-   **客观的**：问题以精确的数学语言陈述。评估指标（ARI）是衡量聚类相似性的标准、客观度量。问题没有主观断言。该问题有效。\n-   **自洽且一致**：问题提供了所有必要的模型定义、超参数和测试用例参数。没有内部矛盾。该问题有效。\n\n### 第三步：结论与行动\n\n该问题有效。我将继续进行解答，首先推导吉布斯采样器所需的全条件分布，然后提供实现。\n\n### 全条件分布的推导\n\n吉布斯采样算法迭代地从每个变量（或变量块）的条件分布中进行采样，该条件分布以所有其他变量的当前值和数据为条件。这些就是全条件分布。\n\n**1. 成分分配 $z_i$ 的全条件分布**\n\n我们需要找到 $p(z_i | \\mathbf{x}, \\mathbf{z}_{-i}, \\mathbf{\\mu}, \\mathbf{\\pi})$，其中 $\\mathbf{z}_{-i}$ 表示除 $z_i$ 之外的所有成分分配。由于模型中的条件独立性假设，这简化为 $p(z_i=k | x_i, \\mathbf{\\mu}, \\mathbf{\\pi})$。\n\n使用贝叶斯定理：\n$$p(z_i = k | x_i, \\mathbf{\\mu}, \\mathbf{\\pi}) \\propto p(x_i | z_i = k, \\mathbf{\\mu}, \\mathbf{\\pi}) \\cdot p(z_i = k | \\mathbf{\\mu}, \\mathbf{\\pi})$$\n$x_i$ 的条件分布仅依赖于其自身成分的均值 $\\mu_k$，$z_i$ 的先验仅依赖于混合权重 $\\pi$。\n$$p(z_i = k | x_i, \\mathbf{\\mu}, \\mathbf{\\pi}) \\propto p(x_i | z_i = k, \\mu_k) \\cdot p(z_i = k | \\mathbf{\\pi})$$\n代入模型定义：\n$$p(z_i = k | x_i, \\mathbf{\\mu}, \\mathbf{\\pi}) \\propto \\mathcal{N}(x_i | \\mu_k, \\sigma^2 I_d) \\cdot \\pi_k$$\n这意味着对于每个数据点 $x_i$，它的新分配 $z_i$ 是从一个在 $\\{0, \\dots, K-1\\}$ 上的分类分布中采样的，其概率与 $\\pi_k \\mathcal{N}(x_i | \\mu_k, \\sigma^2 I_d)$ 成正比，我们可以写成：\n$$p(z_i=k|\\dots) \\propto \\pi_k \\exp\\left(-\\frac{1}{2\\sigma^2} \\|x_i - \\mu_k\\|^2\\right)$$\n对每个 $k \\in \\{0, \\dots, K-1\\}$ 计算这些值，然后进行归一化，形成分类分布的概率向量。\n\n**2. 混合权重 $\\pi$ 的全条件分布**\n\n我们寻求分布 $p(\\pi | \\mathbf{x}, \\mathbf{z}, \\mathbf{\\mu})$。根据条件独立性，这等于 $p(\\pi | \\mathbf{z})$。\n$$p(\\mathbf{\\pi} | \\mathbf{z}) \\propto p(\\mathbf{z} | \\mathbf{\\pi}) \\cdot p(\\mathbf{\\pi})$$\n似然项 $p(\\mathbf{z} | \\mathbf{\\pi})$ 是一组独立同分布的分类变量的似然，而 $p(\\mathbf{\\pi})$ 是狄利克雷先验。\n$$p(\\mathbf{z} | \\mathbf{\\pi}) = \\prod_{i=1}^N p(z_i | \\mathbf{\\pi}) = \\prod_{i=1}^N \\pi_{z_i} = \\prod_{k=0}^{K-1} \\pi_k^{N_k}$$\n其中 $N_k = \\sum_{i=1}^N \\mathbb{I}(z_i = k)$ 是分配给成分 $k$ 的数据点数量。狄利克雷先验为：\n$$p(\\mathbf{\\pi}) \\propto \\prod_{k=0}^{K-1} \\pi_k^{\\alpha - 1}$$\n将它们结合起来得到后验分布：\n$$p(\\mathbf{\\pi} | \\mathbf{z}) \\propto \\left( \\prod_{k=0}^{K-1} \\pi_k^{N_k} \\right) \\left( \\prod_{k=0}^{K-1} \\pi_k^{\\alpha - 1} \\right) = \\prod_{k=0}^{K-1} \\pi_k^{N_k + \\alpha - 1}$$\n这是一个狄利克雷分布的核。因此，$\\pi$ 的全条件分布是：\n$$\\mathbf{\\pi} | \\mathbf{z} \\sim \\operatorname{Dirichlet}(\\alpha + N_0, \\alpha + N_1, \\dots, \\alpha + N_{K-1})$$\n这展示了狄利克雷先验与分类/多项式似然的共轭性。\n\n**3. 成分均值 $\\mu_k$ 的全条件分布**\n\n我们需要 $p(\\mu_k | \\mathbf{x}, \\mathbf{z}, \\mathbf{\\pi}, \\mathbf{\\mu}_{-k})$。这简化为 $p(\\mu_k | \\{x_i: z_i=k\\})$。\n$$p(\\mu_k | \\mathbf{x}, \\mathbf{z}) \\propto \\left( \\prod_{i: z_i=k} p(x_i | \\mu_k) \\right) p(\\mu_k)$$\n似然部分涉及分配给成分 $k$ 的数据点，而先验是 $\\mu_k$ 的正态分布。\n$$p(\\mu_k | \\mathbf{x}, \\mathbf{z}) \\propto \\left( \\prod_{i: z_i=k} \\mathcal{N}(x_i | \\mu_k, \\sigma^2 I_d) \\right) \\mathcal{N}(\\mu_k | \\mu_0, \\tau^2 I_d)$$\n高斯均值与高斯先验的后验分布也是高斯分布。后验精度是先验精度和似然精度之和。先验的精度是 $(1/\\tau^2)I_d$。来自一个数据点的精度是 $(1/\\sigma^2)I_d$。对于 $N_k$ 个数据点，似然精度是 $(N_k/\\sigma^2)I_d$。\n后验精度矩阵是 $\\mathbf{P}_{\\text{post}} = \\left(\\frac{N_k}{\\sigma^2} + \\frac{1}{\\tau^2}\\right)I_d$。后验协方差是 $\\mathbf{\\Sigma}_{\\text{post}} = \\mathbf{P}_{\\text{post}}^{-1} = \\hat{\\tau}_k^2 I_d$，其中：\n$$\\hat{\\tau}_k^2 = \\left(\\frac{N_k}{\\sigma^2} + \\frac{1}{\\tau^2}\\right)^{-1}$$\n后验均值是先验均值和数据均值的精度加权平均。令 $\\bar{x}_k = \\frac{1}{N_k}\\sum_{i:z_i=k} x_i$。\n$$\\hat{\\mu}_k = \\mathbf{\\Sigma}_{\\text{post}} \\left(\\frac{1}{\\sigma^2} \\sum_{i:z_i=k} x_i + \\frac{1}{\\tau^2} \\mu_0 \\right) = \\hat{\\tau}_k^2 \\left( \\frac{N_k}{\\sigma^2} \\bar{x}_k + \\frac{1}{\\tau^2} \\mu_0 \\right)$$\n如果 $N_k=0$，$\\mu_k$ 的后验分布恢复为其先验分布 $\\mathcal{N}(\\mu_0, \\tau^2 I_d)$。因此，全条件分布为：\n$$\\mu_k | \\mathbf{x}, \\mathbf{z} \\sim \\mathcal{N}(\\hat{\\mu}_k, \\hat{\\tau}_k^2 I_d)$$\n这说明了正态先验与正态似然（对于均值参数）的共轭性。\n\n### 吉布斯采样算法\n\n1.  **初始化**：将 $\\mathbf{z}, \\mathbf{\\mu}, \\mathbf{\\pi}$ 初始化为随机或启发式的值。\n2.  **迭代**：对于 $t = 1, \\dots, T$：\n    a. 采样 $\\mathbf{\\pi}^{(t+1)} \\sim p(\\mathbf{\\pi} | \\mathbf{z}^{(t)})$。\n    b. 对每个成分 $k=0, \\dots, K-1$，采样 $\\mu_k^{(t+1)} \\sim p(\\mu_k | \\mathbf{x}, \\mathbf{z}^{(t)})$。\n    c. 对每个数据点 $i=1, \\dots, N$，采样 $z_i^{(t+1)} \\sim p(z_i | x_i, \\mathbf{\\mu}^{(t+1)}, \\mathbf{\\pi}^{(t+1)})$。\n3.  **共识**：收集样本 $\\{\\mathbf{z}^{(t)}\\}_{t=T-L+1}^T$。对于每个点 $i$，最终分配 $\\hat{z}_i$ 是 $\\{z_i^{(t)}\\}_{t=T-L+1}^T$ 的众数。\n\n此过程从联合后验分布 $p(\\mathbf{z}, \\mathbf{\\mu}, \\mathbf{\\pi} | \\mathbf{x})$ 中生成样本。实现将遵循此逻辑。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nimport collections\n\ndef calculate_ari(labels_true, labels_pred):\n    \"\"\"\n    Computes the Adjusted Rand Index (ARI) from a contingency table.\n    This implementation is based on the pair-counting formula and does not use\n    any external libraries beyond NumPy.\n    \"\"\"\n    # Create the contingency table\n    n_true_labels = np.max(labels_true) + 1 if labels_true.size  0 else 0\n    n_pred_labels = np.max(labels_pred) + 1 if labels_pred.size  0 else 0\n    contingency_table = np.zeros((n_true_labels, n_pred_labels), dtype=int)\n    for i in range(len(labels_true)):\n        contingency_table[labels_true[i], labels_pred[i]] += 1\n    \n    def n_choose_2(n):\n        # Using np.asarray to handle both scalars and arrays\n        n_arr = np.asarray(n)\n        return n_arr * (n_arr-1) / 2.0\n\n    sum_comb_nij = np.sum(n_choose_2(contingency_table))\n    sum_comb_a = np.sum(n_choose_2(np.sum(contingency_table, axis=1)))\n    sum_comb_b = np.sum(n_choose_2(np.sum(contingency_table, axis=0)))\n\n    N = len(labels_true)\n    if N  2:\n        return 1.0 if n_true_labels == n_pred_labels else 0.0\n\n    comb_N = n_choose_2(N)\n    \n    expected_index = sum_comb_a * sum_comb_b / comb_N\n    max_index = (sum_comb_a + sum_comb_b) / 2.0\n    \n    denominator = max_index - expected_index\n    if denominator == 0:\n        return 0.0 # Or 1.0 if index also equals max_index. Standard def is 0.\n        \n    ari = (sum_comb_nij - expected_index) / denominator\n    return ari\n\ndef run_gmm_gibbs_sampler(case):\n    \"\"\"\n    Runs the full Gibbs sampler for one test case.\n    \"\"\"\n    d = case['d']\n    true_means = case['true_means']\n    sigma = case['sigma']\n    counts_per_component = case['counts_per_component']\n    N = sum(counts_per_component)\n    K = case['K']\n    mu_0_raw = case['mu_0']\n    tau_sq = case['tau_sq']\n    alpha = case['alpha']\n    T = case['T']\n    L = case['L']\n    seed = case['seed']\n    \n    rng = np.random.default_rng(seed)\n\n    # 1. Generate synthetic data\n    X = []\n    true_labels = []\n    mu_0 = np.array(mu_0_raw) if d  1 else np.array([mu_0_raw])\n    sigma_sq = sigma**2\n\n    for i, mean in enumerate(true_means):\n        n_points = counts_per_component[i]\n        cov = sigma_sq * np.eye(d)\n        points = rng.multivariate_normal(np.array(mean), cov, size=n_points)\n        X.append(points)\n        true_labels.append(np.full(n_points, i, dtype=int))\n    \n    X = np.vstack(X)\n    true_labels = np.concatenate(true_labels)\n    if d == 1:\n        X = X.reshape(-1, 1)\n\n    # 2. Gibbs Sampler\n    # Initialization\n    z = rng.integers(0, K, size=N)\n    pi = rng.dirichlet(np.full(K, alpha))\n    mu = rng.multivariate_normal(mu_0, tau_sq * np.eye(d), size=K)\n    \n    z_history = np.zeros((L, N), dtype=int)\n\n    # Main loop\n    for t in range(T):\n        # a. Update pi\n        counts = np.bincount(z, minlength=K)\n        pi = rng.dirichlet(alpha + counts)\n        \n        # b. Update mu_k for each component k\n        for k in range(K):\n            X_k = X[z == k]\n            N_k = X_k.shape[0]\n            \n            if N_k == 0:\n                # If cluster is empty, draw from prior\n                mu_hat_k = mu_0\n                tau_hat_k_sq = tau_sq\n            else:\n                prec_post = (N_k / sigma_sq) + (1 / tau_sq)\n                tau_hat_k_sq = 1.0 / prec_post\n                \n                sum_x_k = np.sum(X_k, axis=0)\n                mu_hat_k = tau_hat_k_sq * (sum_x_k / sigma_sq + mu_0 / tau_sq)\n\n            mu[k] = rng.multivariate_normal(mu_hat_k, tau_hat_k_sq * np.eye(d))\n        \n        # c. Update z_i for each data point i\n        # Vectorized calculation of log probabilities\n        dist_sq = np.sum((X[:, np.newaxis, :] - mu[np.newaxis, :, :])**2, axis=2) # Shape (N, K)\n        log_likelihoods = -0.5 * dist_sq / sigma_sq\n        log_probs_unnorm = np.log(pi) + log_likelihoods\n\n        # Normalize probabilities using log-sum-exp trick and sample\n        for i in range(N):\n            log_p_i = log_probs_unnorm[i, :]\n            log_p_i_stable = log_p_i - np.max(log_p_i)\n            p_i = np.exp(log_p_i_stable)\n            p_i /= np.sum(p_i)\n            z[i] = rng.choice(K, p=p_i)\n            \n        # Store z if in consensus window\n        if t = T - L:\n            z_history[t - (T - L)] = z\n\n    # 3. Form consensus assignments\n    pred_labels = np.zeros(N, dtype=int)\n    for i in range(N):\n        # Find the most frequent label (mode)\n        point_history = z_history[:, i]\n        counts = np.bincount(point_history, minlength=K)\n        pred_labels[i] = np.argmax(counts)\n        \n    # 4. Compute ARI\n    ari = calculate_ari(true_labels, pred_labels)\n    \n    return ari\n\ndef solve():\n    \"\"\"\n    Main function to define, run, and report results for all test cases.\n    \"\"\"\n    test_cases = [\n        {   # Case 1\n            \"d\": 1,\n            \"true_means\": [[-3.0], [3.0]],\n            \"sigma\": 0.6,\n            \"counts_per_component\": [30, 30],\n            \"K\": 2,\n            \"mu_0\": 0.0,\n            \"tau_sq\": 4.0,\n            \"alpha\": 1.0,\n            \"T\": 1000,\n            \"L\": 200,\n            \"seed\": 123\n        },\n        {   # Case 2\n            \"d\": 2,\n            \"true_means\": [[-3.0, -3.0], [0.0, 4.0], [4.0, -1.0]],\n            \"sigma\": 0.7,\n            \"counts_per_component\": [40, 40, 40],\n            \"K\": 3,\n            \"mu_0\": [0.0, 0.0],\n            \"tau_sq\": 4.0,\n            \"alpha\": 1.0,\n            \"T\": 1200,\n            \"L\": 200,\n            \"seed\": 321\n        },\n        {   # Case 3\n            \"d\": 1,\n            \"true_means\": [[-2.0], [2.0]],\n            \"sigma\": 0.8,\n            \"counts_per_component\": [50, 50],\n            \"K\": 3,\n            \"mu_0\": 0.0,\n            \"tau_sq\": 4.0,\n            \"alpha\": 1.0,\n            \"T\": 1200,\n            \"L\": 200,\n            \"seed\": 555\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        ari = run_gmm_gibbs_sampler(case)\n        results.append(f\"{ari:.4f}\")\n\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3235855"}]}