{"hands_on_practices": [{"introduction": "“核技巧”并非支持向量机（SVM）的专利，它可以被应用于许多线性模型，以创造出强大的非线性版本。在此实践中，我们将通过实现核逻辑斯谛回归（Kernel Logistic Regression）来探索这一点，这是一种概率分类器。这项练习将要求我们在对偶空间中工作，并实现一个牛顿法优化器，从而让我们深入了解这些模型的内部工作原理。[@problem_id:3136166]", "problem": "完全在对偶空间中，利用核技巧，使用牛顿法实现核逻辑斯谛回归。然后，比较其在零初始化与通过核岭回归解进行热启动时的收敛行为。您的程序必须是一个单一、完整、可运行的脚本，无需用户输入即可生成指定格式的最终结果。\n\n基本原理和设置：\n- 给定一个二元分类数据集 $\\{(x_i,t_i)\\}_{i=1}^n$，其中 $x_i \\in \\mathbb{R}^d$，目标 $t_i \\in \\{-1, +1\\}$。考虑一个正定核 $k(\\cdot,\\cdot)$，其格拉姆矩阵为 $K \\in \\mathbb{R}^{n \\times n}$，其中 $K_{ij} = k(x_i,x_j)$。令 $f(\\cdot)$ 位于由 $k$ 导出的再生核希尔伯特空间中，并根据表示定理，将其限制为 $f(\\cdot)=\\sum_{i=1}^n \\alpha_i k(x_i,\\cdot)$，因此在训练集上的函数值向量为 $f = K \\alpha$，其中 $\\alpha \\in \\mathbb{R}^n$。\n- 使用带有正则化参数 $\\lambda > 0$ 的逻辑斯谛负对数似然：\n  $$J(\\alpha) = \\sum_{i=1}^n \\log\\!\\big(1 + \\exp(-t_i f_i)\\big) + \\frac{\\lambda}{2}\\,\\alpha^\\top K \\alpha,$$\n  其中 $f = K \\alpha$。您必须仅使用链式法则、逻辑斯谛函数的导数以及核矩阵的性质，来推导 $J(\\alpha)$ 相对于 $\\alpha$ 的梯度和海森矩阵。除了经过充分检验的逻辑斯谛损失公式和线性代数恒等式外，不要假设任何结果。\n- 使用带有长度尺度 $\\ell > 0$ 的径向基函数（高斯）核：\n  $$k(x,x') = \\exp\\!\\left(-\\frac{\\lVert x-x' \\rVert_2^2}{2 \\ell^2}\\right)。$$\n\n算法要求：\n- 在对偶空间中实现牛顿法以最小化 $J(\\alpha)$：\n  - 在每次迭代中，构建关于 $\\alpha$ 的梯度和海森矩阵，并通过求解一个线性系统来计算牛顿步。您必须实现回溯线搜索，以确保当完整步长不减少目标函数值时，$J(\\alpha)$ 能够下降。使用回溯因子 $0.5$。\n  - 当梯度的欧几里得范数最多为 $10^{-6}$ 或迭代次数达到 $50$ 时终止，以先发生者为准。当您计算一个搜索方向时，计为一次牛顿迭代，无论您执行了多少次回溯缩减。\n  - 为了数值稳定性，您可以在求解的海森系统中添加一个微小的岭项 $\\epsilon I$，其中 $\\epsilon = 10^{-10}$。\n- 实现核岭回归（KRR），使用相同的核和相同的正则化强度 $\\lambda$ 处理标签 $t \\in \\{-1,+1\\}$。使用标准的 KRR 系统\n  $$(K + \\lambda I)\\,\\alpha_{\\mathrm{krr}} = t,$$\n  通过线性代数精确求解。使用 $\\alpha_{\\mathrm{krr}}$作为牛顿法的热启动。\n\n测试套件：\n对于以下所有情况，在 $\\mathbb{R}^2$ 中生成数据，作为两个大小相等的高斯团。使用独立同分布的高斯噪声，其协方差为各向同性的 $\\sigma^2 I_2$。为保证可复现性，请使用指定的伪随机种子。使用给定的长度尺度 $\\ell$ 构建核。\n\n- 情况 A (理想情况)：\n  - 种子：$0$。\n  - 总样本数：$n = 60$ (两类之间平衡)。\n  - 均值：正类均值 $\\mu_+ = (1, 1)$，负类均值 $\\mu_- = (-1, -1)$。\n  - 标准差：$\\sigma = 0.3$。\n  - 核长度尺度：$\\ell = 1.0$。\n  - 正则化：$\\lambda = 10^{-2}$。\n- 情况 B (近似常数核边缘情况)： \n  - 种子：$1$。\n  - 总样本数：$n = 60$ (平衡)。\n  - 均值：正类均值 $\\mu_+ = (0.7, -0.7)$，负类均值 $\\mu_- = (-0.7, 0.7)$。\n  - 标准差：$\\sigma = 0.4$。\n  - 核长度尺度：$\\ell = 5.0$。\n  - 正则化：$\\lambda = 10^{-2}$。\n- 情况 C (重叠且更强非线性)：\n  - 种子：$2$。\n  - 总样本数：$n = 80$ (平衡)。\n  - 均值：正类均值 $\\mu_+ = (0.5, 0.5)$，负类均值 $\\mu_- = (-0.5, -0.5)$。\n  - 标准差：$\\sigma = 0.8$。\n  - 核长度尺度：$\\ell = 0.5$。\n  - 正则化：$\\lambda = 10^{-2}$。\n\n需要计算和比较的内容：\n- 对于每种情况，运行牛顿法两次：\n  1. 使用 $\\alpha^{(0)} = 0$ 初始化，并获得达到终止标准所需的迭代次数 $I_0$。\n  2. 使用 $\\alpha^{(0)} = \\alpha_{\\mathrm{krr}}$ 初始化，并获得所需的迭代次数 $I_{\\mathrm{krr}}$。\n- 对于每种情况，定义一个指示符 $B$，如果 $I_{\\mathrm{krr}} < I_0$，则 $B$ 等于 $1$，否则等于 $0$。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含三种情况的结果，格式为逗号分隔的列表的列表，严格如下：\n  $$[\\,[I_0^{(A)}, I_{\\mathrm{krr}}^{(A)}, B^{(A)}],\\,[I_0^{(B)}, I_{\\mathrm{krr}}^{(B)}, B^{(B)}],\\,[I_0^{(C)}, I_{\\mathrm{krr}}^{(C)}, B^{(C)}]\\,].$$\n- 除此单行外，不应打印任何额外的文本或空格。不涉及物理单位，所有计算均为实值浮点运算。", "solution": "用户提供了一个有效的问题。解决方案如下所示。\n\n### I. 问题形式化\n\n目标是最小化核逻辑斯谛回归（KLR）的正则化负对数似然。数据集由数据对 $\\{(x_i, t_i)\\}_{i=1}^n$ 组成，其中 $x_i \\in \\mathbb{R}^d$ 是输入向量， $t_i \\in \\{-1, +1\\}$ 是二元类别标签。使用一个正定核 $k(\\cdot, \\cdot)$，决策函数以对偶形式表示为 $f(\\cdot) = \\sum_{j=1}^n \\alpha_j k(x_j, \\cdot)$。在训练数据上的函数评估向量为 $f = K\\alpha$，其中 $K$ 是 $n \\times n$ 的格拉姆矩阵，其元素为 $K_{ij} = k(x_i, x_j)$，而 $\\alpha \\in \\mathbb{R}^n$ 是对偶系数向量。\n\n需要关于 $\\alpha$ 最小化的目标函数是：\n$$\nJ(\\alpha) = \\sum_{i=1}^n \\log\\left(1 + \\exp(-t_i f_i)\\right) + \\frac{\\lambda}{2} \\alpha^\\top K \\alpha\n$$\n其中 $f_i = (K\\alpha)_i = \\sum_{j=1}^n K_{ij} \\alpha_j$ 且 $\\lambda > 0$ 是正则化参数。该函数是凸函数，确保存在唯一最小值。我们将使用牛顿法进行优化。\n\n### II. 梯度和海森矩阵的推导\n\n要应用牛顿法，我们必须计算目标函数的梯度 $\\nabla J(\\alpha)$ 和海森矩阵 $\\nabla^2 J(\\alpha)$。\n\n#### 梯度推导\n\n梯度是一个在 $\\mathbb{R}^n$ 中的向量，其分量为 $\\frac{\\partial J}{\\partial \\alpha_k}$。我们应用链式法则。\n$$\n\\frac{\\partial J}{\\partial \\alpha_k} = \\frac{\\partial}{\\partial \\alpha_k} \\left( \\sum_{i=1}^n \\log(1 + e^{-t_i f_i}) \\right) + \\frac{\\partial}{\\partial \\alpha_k} \\left( \\frac{\\lambda}{2} \\alpha^\\top K \\alpha \\right)\n$$\n损失项相对于 $f_i$ 的导数是：\n$$\n\\frac{\\partial}{\\partial f_i} \\log(1 + e^{-t_i f_i}) = \\frac{1}{1 + e^{-t_i f_i}} \\cdot (-t_i e^{-t_i f_i}) = -t_i \\frac{e^{-t_i f_i}}{1 + e^{-t_i f_i}} = -t_i \\frac{1}{1 + e^{t_i f_i}} = -t_i \\sigma(-t_i f_i)\n$$\n其中 $\\sigma(z) = (1+e^{-z})^{-1}$ 是 sigmoid 函数。\n$f_i$ 相对于 $\\alpha_k$ 的导数是：\n$$\n\\frac{\\partial f_i}{\\partial \\alpha_k} = \\frac{\\partial}{\\partial \\alpha_k} \\sum_{j=1}^n K_{ij} \\alpha_j = K_{ik}\n$$\n对损失部分使用链式法则将它们结合起来：\n$$\n\\frac{\\partial}{\\partial \\alpha_k} \\left( \\sum_{i=1}^n \\log(1 + e^{-t_i f_i}) \\right) = \\sum_{i=1}^n \\frac{\\partial \\log(1+e^{-t_i f_i})}{\\partial f_i} \\frac{\\partial f_i}{\\partial \\alpha_k} = \\sum_{i=1}^n \\left( -t_i \\sigma(-t_i f_i) \\right) K_{ik}\n$$\n正则化项的梯度是二次型的标准形式（注意 $K$ 是对称的）：\n$$\n\\frac{\\partial}{\\partial \\alpha_k} \\left( \\frac{\\lambda}{2} \\alpha^\\top K \\alpha \\right) = \\lambda (K\\alpha)_k = \\lambda \\sum_{i=1}^n K_{ki} \\alpha_i\n$$\n结合两项，梯度的第 $k$ 个分量是：\n$$\n\\nabla_k J(\\alpha) = \\sum_{i=1}^n K_{ki} (-t_i \\sigma(-t_i f_i)) + \\lambda \\sum_{i=1}^n K_{ki} \\alpha_i = \\sum_{i=1}^n K_{ki} \\left( -t_i \\sigma(-t_i f_i) + \\lambda \\alpha_i \\right)\n$$\n在矩阵向量表示法中，令 $g_f$ 是一个向量，其元素为 $(g_f)_i = -t_i \\sigma(-t_i f_i)$。梯度为：\n$$\n\\nabla J(\\alpha) = K g_f + \\lambda K \\alpha = K(g_f + \\lambda \\alpha)\n$$\n\n#### 海森矩阵推导\n\n海森矩阵是一个 $n \\times n$ 的矩阵，其元素为 $H_{k\\ell} = \\frac{\\partial^2 J}{\\partial \\alpha_k \\partial \\alpha_\\ell}$。我们将梯度分量对 $\\alpha_\\ell$ 求导：\n$$\n\\frac{\\partial^2 J}{\\partial \\alpha_k \\partial \\alpha_\\ell} = \\frac{\\partial}{\\partial \\alpha_\\ell} \\left( \\sum_{i=1}^n K_{ki} (-t_i \\sigma(-t_i f_i)) \\right) + \\frac{\\partial}{\\partial \\alpha_\\ell} \\left( \\lambda (K\\alpha)_k \\right)\n$$\n对于损失项，我们再次使用链式法则，对 $f_j$ 进行微分：\n$$\n\\frac{\\partial}{\\partial \\alpha_\\ell} (-t_i \\sigma(-t_i f_i)) = \\sum_{j=1}^n \\frac{\\partial (-t_i \\sigma(-t_i f_i))}{\\partial f_j} \\frac{\\partial f_j}{\\partial \\alpha_\\ell}\n$$\n该项仅依赖于 $f_i$，因此对 $j$ 的求和简化为 $j=i$。导数为：\n$$\n\\frac{d}{df_i} (-t_i \\sigma(-t_i f_i)) = -t_i \\cdot \\frac{d}{df_i} \\sigma(-t_i f_i) = -t_i \\cdot \\left[ \\sigma'(-t_i f_i) \\cdot (-t_i) \\right] = t_i^2 \\sigma'(-t_i f_i) = \\sigma'(-t_i f_i)\n$$\n利用性质 $\\sigma'(z) = \\sigma(z)(1-\\sigma(z)) = \\sigma(z)\\sigma(-z)$：\n$$\n\\frac{d}{df_i} (-t_i \\sigma(-t_i f_i)) = \\sigma(-t_i f_i)(1 - \\sigma(-t_i f_i)) = \\sigma(-t_i f_i)\\sigma(t_i f_i)\n$$\n令 $W$ 为一个对角矩阵，其对角线元素为 $W_{ii} = \\sigma(t_i f_i)\\sigma(-t_i f_i)$。\n梯度分量 $\\nabla_k J(\\alpha)$ 的损失部分的导数为：\n$$\n\\frac{\\partial}{\\partial \\alpha_\\ell} \\sum_{i=1}^n K_{ki}(-t_i\\sigma(-t_i f_i)) = \\sum_{i=1}^n K_{ki} W_{ii} K_{i\\ell} = (K W K)_{k\\ell}\n$$\n梯度的正则化部分的导数为：\n$$\n\\frac{\\partial}{\\partial \\alpha_\\ell} (\\lambda (K\\alpha)_k) = \\frac{\\partial}{\\partial \\alpha_\\ell} \\left( \\lambda \\sum_{j=1}^n K_{kj} \\alpha_j \\right) = \\lambda K_{k\\ell}\n$$\n合并各项，海森矩阵为：\n$$\n\\nabla^2 J(\\alpha) = K W K + \\lambda K\n$$\n\n### III. 算法设计\n\n#### 牛顿法\n\n在每次迭代 $m$ 中，我们通过求解线性系统来找到一个搜索方向 $\\Delta\\alpha^{(m)}$：\n$$\n\\left(\\nabla^2 J(\\alpha^{(m)}) + \\epsilon I\\right) \\Delta\\alpha^{(m)} = -\\nabla J(\\alpha^{(m)})\n$$\n其中 $\\epsilon = 10^{-10}$ 是一个小的正则化项，以确保数值稳定性。然后更新系数向量：\n$$\n\\alpha^{(m+1)} = \\alpha^{(m)} + \\eta \\Delta\\alpha^{(m)}\n$$\n步长 $\\eta$ 由回溯线搜索确定。\n\n#### 回溯线搜索\n\n为保证目标函数 $J(\\alpha)$ 的下降，我们从一个完整的步长 $\\eta=1$ 开始。如果 $J(\\alpha + \\eta\\Delta\\alpha) \\ge J(\\alpha)$，我们反复将步长减半，$\\eta \\leftarrow 0.5\\eta$，直到满足条件 $J(\\alpha + \\eta\\Delta\\alpha) < J(\\alpha)$。\n\n#### 核岭回归热启动\n\n核岭回归（KRR）解决了一个更简单的“最小二乘”版本的问题，找到系数 $\\alpha_{\\mathrm{krr}}$ 以最小化 $\\|\\sum_i \\alpha_i k(x_i, \\cdot) - t\\|_{\\text{L2}}^2 + \\lambda \\|\\alpha\\|^2_I$。在使用核技巧的对偶表示中，这导出了一个闭式解：\n$$\n(K + \\lambda I) \\alpha_{\\mathrm{krr}} = t\n$$\n其中 $t$ 是标签 $\\{-1, +1\\}$ 的向量。解 $\\alpha_{\\mathrm{krr}}$ 为 KLR 优化提供了一个良好的初始猜测，因为 KRR 可以看作是 KLR 的一个近似。我们比较从 $\\alpha^{(0)} = 0$ 开始与从 $\\alpha^{(0)} = \\alpha_{\\mathrm{krr}}$ 开始的牛顿法收敛情况。在每种情况下记录收敛所需的迭代次数。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.spatial.distance import cdist\nfrom scipy.special import expit\n\ndef solve():\n    \"\"\"\n    Implements and compares Kernel Logistic Regression solvers.\n    \"\"\"\n\n    # --- Helper Function for Data Generation ---\n    def generate_data(seed, n, mu_pos, mu_neg, sigma):\n        \"\"\"Generates a two-class Gaussian blob dataset.\"\"\"\n        rng = np.random.default_rng(seed)\n        n_pos = n // 2\n        n_neg = n - n_pos\n        \n        # Draw samples from two multivariate normal distributions\n        X_pos = rng.multivariate_normal(mu_pos, sigma**2 * np.eye(2), n_pos)\n        X_neg = rng.multivariate_normal(mu_neg, sigma**2 * np.eye(2), n_neg)\n        \n        # Combine data and create labels\n        X = np.vstack((X_pos, X_neg))\n        t = np.hstack((np.ones(n_pos), -np.ones(n_neg)))\n        \n        return X, t\n\n    # --- Helper Function for RBF Kernel ---\n    def rbf_kernel(X1, X2, length_scale):\n        \"\"\"Computes the RBF (Gaussian) kernel matrix.\"\"\"\n        # Calculate squared Euclidean distances between all pairs of points\n        sq_dists = cdist(X1, X2, 'sqeuclidean')\n        # Apply the kernel function\n        return np.exp(-sq_dists / (2 * length_scale**2))\n\n    # --- Helper Function for Kernel Ridge Regression ---\n    def solve_krr(K, t, lambda_reg):\n        \"\"\"Solves for the dual coefficients of Kernel Ridge Regression.\"\"\"\n        n = K.shape[0]\n        # Solve the linear system (K + lambda*I) * alpha = t\n        A = K + lambda_reg * np.eye(n)\n        alpha_krr = np.linalg.solve(A, t)\n        return alpha_krr\n\n    # --- Kernel Logistic Regression Solver Class ---\n    class KernelLogisticRegression:\n        \"\"\"\n        Implements Kernel Logistic Regression using Newton's method with backtracking.\n        \"\"\"\n        def __init__(self, K, t, lambda_reg, tol=1e-6, max_iter=50, hessian_eps=1e-10):\n            self.K = K\n            self.t = t\n            self.lambda_reg = lambda_reg\n            self.tol = tol\n            self.max_iter = max_iter\n            self.hessian_eps = hessian_eps\n            self.n = K.shape[0]\n\n        def _objective(self, alpha):\n            \"\"\"Computes the KLR objective function J(alpha).\"\"\"\n            f = self.K @ alpha\n            tf = self.t * f\n            # Use np.logaddexp for numerical stability: log(1+exp(-x))\n            log_likelihood = np.sum(np.logaddexp(0, -tf))\n            regularization = (self.lambda_reg / 2.0) * (alpha @ self.K @ alpha)\n            return log_likelihood + regularization\n\n        def _gradient(self, alpha):\n            \"\"\"Computes the gradient of J(alpha).\"\"\"\n            f = self.K @ alpha\n            tf = self.t * f\n            # Gradient of loss w.r.t. f is -t * sigma(-t*f)\n            g_f = -self.t * expit(-tf)\n            grad = self.K @ (g_f + self.lambda_reg * alpha)\n            return grad\n\n        def _hessian(self, alpha):\n            \"\"\"Computes the Hessian of J(alpha).\"\"\"\n            f = self.K @ alpha\n            tf = self.t * f\n            # W_ii = sigma(t_i*f_i) * (1 - sigma(t_i*f_i)) = sigma(t_i*f_i) * sigma(-t_i*f_i)\n            # expit(x) is the sigmoid function sigma(x)\n            w_diag = expit(tf) * expit(-tf)\n            W = np.diag(w_diag)\n            hess = self.K @ W @ self.K + self.lambda_reg * self.K\n            return hess\n\n        def solve(self, alpha0):\n            \"\"\"Performs optimization starting from alpha0.\"\"\"\n            alpha = np.copy(alpha0)\n            \n            for i in range(self.max_iter):\n                grad = self._gradient(alpha)\n                \n                # Termination condition: gradient norm\n                if np.linalg.norm(grad) <= self.tol:\n                    return i\n                \n                # Compute search direction\n                hess = self._hessian(alpha)\n                H_reg = hess + self.hessian_eps * np.eye(self.n)\n                \n                try:\n                    delta_alpha = np.linalg.solve(H_reg, -grad)\n                except np.linalg.LinAlgError:\n                    # Fallback if regularized Hessian is still numerically singular\n                    return i + 1\n\n                # Backtracking line search with factor 0.5\n                eta = 1.0\n                J_current = self._objective(alpha)\n                while True:\n                    alpha_new = alpha + eta * delta_alpha\n                    J_new = self._objective(alpha_new)\n                    \n                    if J_new < J_current:\n                        break # Found a step size that improves objective\n                    \n                    eta *= 0.5\n                    \n                    # Failsafe to prevent infinitely small steps\n                    if eta < 1e-12:\n                        break\n                \n                if eta < 1e-12:\n                    # Could not find a descent direction, terminate\n                    return i + 1\n\n                alpha = alpha + eta * delta_alpha\n\n            return self.max_iter\n\n    # --- Test Suite Definition ---\n    test_cases = [\n        # Case A: Happy path, well-separated data\n        {'seed': 0, 'n': 60, 'mu_pos': (1, 1), 'mu_neg': (-1, -1), 'sigma': 0.3, 'l': 1.0, 'lambda_reg': 1e-2},\n        # Case B: Near-constant kernel edge case\n        {'seed': 1, 'n': 60, 'mu_pos': (0.7, -0.7), 'mu_neg': (-0.7, 0.7), 'sigma': 0.4, 'l': 5.0, 'lambda_reg': 1e-2},\n        # Case C: High overlap, non-linear case\n        {'seed': 2, 'n': 80, 'mu_pos': (0.5, 0.5), 'mu_neg': (-0.5, -0.5), 'sigma': 0.8, 'l': 0.5, 'lambda_reg': 1e-2},\n    ]\n\n    all_results = []\n    \n    # --- Main Loop to Process Test Cases ---\n    for case in test_cases:\n        # Generate data and kernel matrix\n        X, t = generate_data(case['seed'], case['n'], case['mu_pos'], case['mu_neg'], case['sigma'])\n        K = rbf_kernel(X, X, case['l'])\n    \n        # Initialize the KLR solver\n        klr_solver = KernelLogisticRegression(K, t, case['lambda_reg'])\n        \n        # Run 1: Initialize with alpha = 0\n        alpha0_zero = np.zeros(case['n'])\n        I0 = klr_solver.solve(alpha0_zero)\n        \n        # Run 2: Initialize with KRR warm start\n        alpha_krr = solve_krr(K, t, case['lambda_reg'])\n        Ikrr = klr_solver.solve(alpha_krr)\n        \n        # Determine if warm-start was better\n        B = 1 if Ikrr < I0 else 0\n        \n        all_results.append(f\"[{I0},{Ikrr},{B}]\")\n\n    # Final print statement in the exact required format\n    print(f\"[{','.join(all_results)}]\")\n\nsolve()\n```", "id": "3136166"}, {"introduction": "核方法的真正威力不仅在于使用诸如径向基函数（RBF）核之类的标准核函数，更在于设计能够编码我们对问题的先验知识的自定义核函数。本实践将演示如何构建一个对特定变换（符号翻转）具有不变性的核。我们将看到这个优雅的理论思想如何导出一个鲁棒的模型，用于分类具有物理模糊性的信号。[@problem_id:3136231]", "problem": "考虑设计一种超越支持向量机（SVM）的核方法，该方法能够对全局符号翻转下的输入表示编码不变性。设输入域为 $\\mathbb{R}^d$ 中的向量，并考虑二元群 $\\{+1,-1\\}$ 通过 $x \\mapsto s x$（其中 $s \\in \\{+1,-1\\}$）在 $\\mathbb{R}^d$ 上的作用。目标是在 $\\mathbb{R}^d$ 上构建一个正定核，当任何输入被其符号翻转版本替换时，该核函数产生相同的值，然后使用该核函数在支持向量机以外的方法中解决一个涉及具有相位不确定性的波形的监督学习问题。\n\n从以下基本依据出发：\n1. 一个函数 $k: \\mathcal{X} \\times \\mathcal{X} \\to \\mathbb{R}$ 是一个正定核，当且仅当对于任意有限集合 $\\{x_i\\}_{i=1}^n \\subset \\mathcal{X}$ 和任意实系数 $\\{c_i\\}_{i=1}^n$，都满足 $\\sum_{i=1}^n \\sum_{j=1}^n c_i c_j k(x_i,x_j) \\geq 0$。\n2. 一个正定核 $k$ 定义了一个再生核希尔伯特空间（RKHS）$\\mathcal{H}$，使得对于任意 $f \\in \\mathcal{H}$ 和 $x \\in \\mathcal{X}$，再生性质成立：$f(x) = \\langle f, k(\\cdot, x) \\rangle_{\\mathcal{H}}$。\n3. 核岭回归（KRR）通过最小化 $\\sum_{i=1}^n (f(x_i) - y_i)^2 + \\lambda \\lVert f \\rVert_{\\mathcal{H}}^2$ 在RKHS中解决正则化经验风险最小化问题，其中正则化参数 $\\lambda > 0$。\n\n您的任务如下：\nA. 使用上述基本原理，推导出一个通过将某个空间 $\\mathcal{Z}$ 上的已知正定核 $\\kappa$ 与一个变换 $\\phi: \\mathcal{X} \\to \\mathcal{Z}$ 复合而获得的核 $k(x,y) = \\kappa(\\phi(x), \\phi(y))$ 在 $\\mathcal{X}$ 上成为正定核的条件。利用此条件来论证一种编码全局符号翻转不变性的设计。\nB. 在 $\\mathbb{R}^d$ 上实现两个核：一个基准高斯径向基函数（RBF）核和一个在 $x \\mapsto -x$ 下不变的变换核。基准核必须是 $k_{\\text{rbf}}(x,y) = \\exp\\left(-\\frac{\\lVert x-y \\rVert_2^2}{2\\sigma^2}\\right)$，其中带宽 $\\sigma > 0$。不变核必须通过一种有原则的变换来构建，以确保在上述群作用下的不变性。\nC. 使用每个核函数实现用于二元分类（标签在 $\\{-1,+1\\}$ 中）的核岭回归（KRR）。给定训练数据 $\\{(x_i, y_i)\\}_{i=1}^n$，KRR 预测器必须具有 $f(\\cdot) = \\sum_{i=1}^n \\alpha_i k(x_i, \\cdot)$ 的形式，其中系数满足 $(K + \\lambda I)\\alpha = y$，其中 $K_{ij} = k(x_i, x_j)$ 且 $\\lambda > 0$。\nD. 使用 $x(t) = \\sin(2\\pi f t + \\varphi)$ 的样本为两个类别生成合成波形数据，其中相位 $\\varphi$ 未知，从 $[0, 2\\pi)$ 均匀采样，并在 $[0,1)$ 内的 $L$ 个等间隔时间点上进行测量。对于每个波形，以概率 $p \\in [0,1]$ 应用随机全局符号翻转以模拟传感器方向模糊性，并添加标准差为 $\\sigma_{\\text{noise}} > 0$ 的独立高斯噪声。将每个波形归一化为单位 $\\ell_2$-范数。使用两个不同的整数频率来定义类别，并分别分配标签 $+1$ 和 $-1$。\nE. 在不同的相位不确定性、符号翻转和噪声条件下，使用基准核和不变核评估分类准确率。\n\n实现一个单一程序，使用以下参数值的测试套件。对于每种情况，请精确使用指定的值，并固定随机数生成器种子以确保可复现性。此问题不涉及物理单位，所有角度均以弧度为单位。\n\n测试用例 1（具有中等不确定性的一般情况）：\n- 每个类别的训练样本数：60。\n- 每个类别的测试样本数：200。\n- 每个波形的时间样本数：128。\n- 频率：标签为 $+1$ 时 $f_{+} = 3$，标签为 $-1$ 时 $f_{-} = 7$。\n- 相位分布：在 $[0, 2\\pi)$ 上均匀分布。\n- 符号翻转概率：$p = 0.5$。\n- 噪声标准差：$\\sigma_{\\text{noise}} = 0.05$。\n- 核带宽：$\\sigma = 1.0$。\n- 正则化：$\\lambda = 10^{-3}$。\n- 随机种子：$12345$。\n\n测试用例 2（具有最大符号不确定性且无噪声的边界情况）：\n- 每个类别的训练样本数：20。\n- 每个类别的测试样本数：200。\n- 每个波形的时间样本数：64。\n- 频率：标签为 $+1$ 时 $f_{+} = 2$，标签为 $-1$ 时 $f_{-} = 5$。\n- 相位分布：在 $[0, 2\\pi)$ 上均匀分布。\n- 符号翻转概率：$p = 1.0$。\n- 噪声标准差：$\\sigma_{\\text{noise}} = 0.0$。\n- 核带宽：$\\sigma = 1.0$。\n- 正则化：$\\lambda = 10^{-3}$。\n- 随机种子：$54321$。\n\n测试用例 3（不变性属性的边缘情况检查）：\n- 构建一个单一波形，其 $L = 256$，频率 $f = 4$，相位 $\\varphi = 0$，无噪声，无符号翻转。\n- 计算该波形与其符号翻转版本之间的不变核值，并将其与该波形自身的不变核值进行比较。\n- 核带宽：$\\sigma = 1.0$。\n\n您的程序必须生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，结果按以下顺序排列：\n- 对于测试用例 1：基准分类准确率（浮点数），不变核分类准确率（浮点数）。\n- 对于测试用例 2：基准分类准确率（浮点数），不变核分类准确率（浮点数）。\n- 对于测试用例 3：一个布尔值，指示不变核对于一个波形及其符号翻转版本是否产生与该波形自身相同的值（使用 $10^{-9}$ 的数值容差）。\n\n例如，最终输出格式必须是 $[result_1,result_2,result_3,result_4,result_5]$，其中 result_1 到 result_4 是浮点数，result_5 是布尔值。", "solution": "问题陈述是有效的。它在科学上基于统计学习理论，定义清晰，包含了所有必要的参数和定义，其目标也以数学精度进行了陈述。这些任务结构逻辑清晰，从理论推导开始，接着进行实现，最后以实证评估结束，构成了一个完整且可验证的核方法练习。\n\n### A. 不变核的推导与设计\n\n第一个任务是建立一种构造新的正定（PD）核的方法，然后应用它来设计一个对全局符号翻转不变的核。\n\n一个函数 $k: \\mathcal{X} \\times \\mathcal{X} \\to \\mathbb{R}$ 是一个正定核，如果对于任意有限的点集 $\\{x_i\\}_{i=1}^n \\subset \\mathcal{X}$ 和任意实系数 $\\{c_i\\}_{i=1}^n$，以下条件成立：\n$$\n\\sum_{i=1}^n \\sum_{j=1}^n c_i c_j k(x_i, x_j) \\geq 0\n$$\n设 $\\kappa: \\mathcal{Z} \\times \\mathcal{Z} \\to \\mathbb{R}$ 是空间 $\\mathcal{Z}$ 上的一个已知正定核，$\\phi: \\mathcal{X} \\to \\mathcal{Z}$ 是一个任意函数或特征映射。我们可以通过复合定义 $\\mathcal{X}$ 上的一个新核 $k$：$k(x, y) = \\kappa(\\phi(x), \\phi(y))$。为了证明 $k$ 也是 $\\mathcal{X}$ 上的一个正定核，我们取任意有限集 $\\{x_i\\}_{i=1}^n \\subset \\mathcal{X}$ 和系数 $\\{c_i\\}_{i=1}^n \\subset \\mathbb{R}$，并考察其二次型：\n$$\n\\sum_{i=1}^n \\sum_{j=1}^n c_i c_j k(x_i, x_j) = \\sum_{i=1}^n \\sum_{j=1}^n c_i c_j \\kappa(\\phi(x_i), \\phi(x_j))\n$$\n让我们定义一个点集 $\\{z_i\\}_{i=1}^n \\subset \\mathcal{Z}$，其中 $z_i = \\phi(x_i)$。该表达式变为：\n$$\n\\sum_{i=1}^n \\sum_{j=1}^n c_i c_j \\kappa(z_i, z_j)\n$$\n由于 $\\kappa$ 是 $\\mathcal{Z}$ 上的一个正定核，根据定义，此和保证为非负。因此，对于任何特征映射 $\\phi$，$k(x, y) = \\kappa(\\phi(x), \\phi(y))$ 是 $\\mathcal{X}$ 上的一个有效的正定核。\n\n为了在群作用 $x \\mapsto s x$（其中 $s \\in G = \\{+1, -1\\}$）下编码不变性，我们寻求一个核 $k(x, y)$，使得对于所有 $s \\in G$，都有 $k(sx, y) = k(x, y)$ 和 $k(x, sy) = k(x, y)$。使用复合形式 $k(x, y) = \\kappa(\\phi(x), \\phi(y))$，一个直接的实现方法是设计一个本身就是不变的特征映射 $\\phi$，即 $\\phi(x) = \\phi(-x)$。例如，如果我们选择 $\\phi(x) = xx^T$，它将 $\\mathbb{R}^d$ 中的向量映射到 $\\mathbb{R}^{d \\times d}$ 中的矩阵，我们有 $\\phi(-x) = (-x)(-x)^T = xx^T = \\phi(x)$。如果我们接着在矩阵空间上使用线性核 $\\kappa(A, B) = \\langle A, B \\rangle_F = \\text{tr}(A^TB)$，我们得到不变核 $k(x, y) = \\text{tr}((xx^T)^T(yy^T)) = (x^Ty)^2$，这是 2 次齐次多项式核。\n\n一种更通用且强大的构建不变核的技术是通过在群作用上对给定的基核进行平均。设 $k_{\\text{base}}$ 是任意正定核。可以通过对称化构造一个不变核 $k_{\\text{inv}}$：\n$$\nk_{\\text{inv}}(x, y) = \\frac{1}{|G|^2} \\sum_{s_1 \\in G} \\sum_{s_2 \\in G} k_{\\text{base}}(s_1 x, s_2 y)\n$$\n对于我们的群 $G = \\{+1, -1\\}$，这展开为：\n$$\nk_{\\text{inv}}(x, y) = \\frac{1}{4} [k_{\\text{base}}(x, y) + k_{\\text{base}}(x, -y) + k_{\\text{base}}(-x, y) + k_{\\text{base}}(-x, -y)]\n$$\n这个新核是由正定核乘以一个正常数（$1/4$）后的和，所以它也是一个正定核。问题指定使用高斯 RBF 核作为基核：$k_{\\text{rbf}}(x, y) = \\exp\\left(-\\frac{\\lVert x-y \\rVert_2^2}{2\\sigma^2}\\right)$。这个核具有性质 $k_{\\text{rbf}}(-u, -v) = \\exp\\left(-\\frac{\\lVert -u-(-v) \\rVert_2^2}{2\\sigma^2}\\right) = \\exp\\left(-\\frac{\\lVert -(u-v) \\rVert_2^2}{2\\sigma^2}\\right) = k_{\\text{rbf}}(u, v)$。同样，$k_{\\text{rbf}}(-x, y) = \\exp\\left(-\\frac{\\lVert -x-y \\rVert_2^2}{2\\sigma^2}\\right) = \\exp\\left(-\\frac{\\lVert x+y \\rVert_2^2}{2\\sigma^2}\\right) = k_{\\text{rbf}}(x, -y)$。将这些对称性代入和中得到：\n$$\nk_{\\text{inv}}(x, y) = \\frac{1}{4} [k_{\\text{rbf}}(x, y) + k_{\\text{rbf}}(x, -y) + k_{\\text{rbf}}(x, -y) + k_{\\text{rbf}}(x, y)] = \\frac{1}{2} [k_{\\text{rbf}}(x, y) + k_{\\text{rbf}}(x, -y)]\n$$\n这将是我们实现的不变核。它正确地对第二个参数的可能符号进行了平均，并且其对称性也确保了对第一个参数的不变性。\n\n### B. 核函数实现\n\n我们在 $\\mathbb{R}^d$ 上实现两个核：\n1.  **基准 RBF 核**：$k_{\\text{rbf}}(x, y) = \\exp\\left(-\\frac{\\lVert x-y \\rVert_2^2}{2\\sigma^2}\\right)$。这是一个标准选择，对 $x$ 和 $y$ 的相对位置敏感。\n2.  **不变 RBF 核**：$k_{\\text{inv}}(x, y) = \\frac{1}{2} [k_{\\text{rbf}}(x, y) + k_{\\text{rbf}}(x, -y)]$。该核有效地计算了点对 $\\{x, -x\\}$ 与点对 $\\{y, -y\\}$ 之间的相似性，使其对输入的全局符号翻转不敏感。\n\n### C. 核岭回归（KRR）实现\n\nKRR 在与核 $k$ 关联的 RKHS $\\mathcal{H}$ 中寻找一个函数 $f$，以最小化正则化最小二乘误差：$\\min_{f \\in \\mathcal{H}} \\sum_{i=1}^n (y_i - f(x_i))^2 + \\lambda \\lVert f \\rVert_{\\mathcal{H}}^2$。表示定理保证解的形式为 $f(\\cdot) = \\sum_{i=1}^n \\alpha_i k(x_i, \\cdot)$。将此代入目标函数并求解系数 $\\alpha = (\\alpha_1, \\dots, \\alpha_n)^T$，得到线性系统：\n$$\n(K + \\lambda I)\\alpha = y\n$$\n其中 $K$ 是 $n \\times n$ 的格拉姆矩阵，其元素为 $K_{ij} = k(x_i, x_j)$，$y = (y_1, \\dots, y_n)^T$ 是标签向量，$I$ 是单位矩阵。训练过程包括计算 $K$ 并求解此系统以得到 $\\alpha$。对于新数据点 $x_{\\text{test}}$ 的预测，我们计算函数值 $f(x_{\\text{test}}) = \\sum_{i=1}^n \\alpha_i k(x_i, x_{\\text{test}})$。对于标签为 $\\{-1, +1\\}$ 的二元分类，预测类别为 $\\text{sign}(f(x_{\\text{test}}))$。\n\n### D. 合成数据生成\n\n我们生成合成波形来模拟具有相位和符号模糊性的场景。波形是 $\\mathbb{R}^L$ 中的一个向量，通过在 $[0, 1)$ 内的 $L$ 个等间隔时间点 $t$ 对 $x(t) = \\sin(2\\pi f t + \\varphi)$ 进行采样获得。两个类别由不同的频率 $f_+$ 和 $f_-$ 定义。每个样本的生成过程如下：\n1.  根据类别标签选择一个频率（$f_+$ 或 $f_-$）。\n2.  从均匀分布 $U[0, 2\\pi)$ 中采样一个相位 $\\varphi$。\n3.  生成无噪声波形 $x_{\\text{clean}}(t) = \\sin(2\\pi f t + \\varphi)$。\n4.  添加独立同分布的高斯噪声：$x_{\\text{noisy}} = x_{\\text{clean}} + \\epsilon$，其中 $\\epsilon \\sim \\mathcal{N}(0, \\sigma_{\\text{noise}}^2 I)$。\n5.  应用随机符号翻转：以概率 $p$ 将 $x_{\\text{noisy}}$ 替换为 $-x_{\\text{noisy}}$。\n6.  将最终向量归一化，使其具有单位 $\\ell_2$-范数：$x = x_{\\text{final}}/\\lVert x_{\\text{final}} \\rVert_2$。此步骤至关重要，因为它确保所有输入都位于单位超球面上，使得像 RBF 这样的基于距离的核仅依赖于向量之间的夹角。\n\n### E. 评估流程\n\n在两种具有不同不确定性水平的测试用例上，评估使用基准核和不变核的 KRR 性能。对于每种情况，我们生成训练和测试数据集。我们在训练集上训练两个 KRR 模型，每种核一个，通过计算它们各自的 $\\alpha$ 向量。然后我们预测测试集的标签并计算分类准确率，定义为正确预测标签的比例。对于第三个测试用例，我们通过直接比较特定波形 $x$ 的 $k_{\\text{inv}}(x, x)$ 和 $k_{\\text{inv}}(x, -x)$ 来验证 $k_{\\text{inv}}$ 的不变性，确认它们在一个小的数值容差内相等。这验证了理论构造。预期不变核的表现将优于基准核，尤其是在符号翻转概率 $p$ 较高时，因为它被设计为对这种特定的数据扰动免疫。", "answer": "```python\nimport numpy as np\n\ndef k_rbf(x, y, sigma):\n    \"\"\"Computes the Gaussian RBF kernel between two vectors.\"\"\"\n    norm_sq = np.sum((x - y)**2)\n    return np.exp(-norm_sq / (2 * sigma**2))\n\ndef k_inv(x, y, sigma):\n    \"\"\"Computes the sign-invariant kernel based on the RBF kernel.\"\"\"\n    val1 = k_rbf(x, y, sigma)\n    val2 = k_rbf(x, -y, sigma)\n    return 0.5 * (val1 + val2)\n\ndef compute_gram_matrix(X, kernel_func, sigma):\n    \"\"\"Computes the Gram matrix for a dataset X and a given kernel.\"\"\"\n    n_samples = X.shape[0]\n    K = np.zeros((n_samples, n_samples))\n    for i in range(n_samples):\n        for j in range(i, n_samples):\n            val = kernel_func(X[i], X[j], sigma)\n            K[i, j] = val\n            K[j, i] = val\n    return K\n\ndef train_krr(K, y, lambda_reg):\n    \"\"\"Trains Kernel Ridge Regression by solving for alpha.\"\"\"\n    n_samples = K.shape[0]\n    I = np.eye(n_samples)\n    alpha = np.linalg.solve(K + lambda_reg * I, y)\n    return alpha\n\ndef predict_krr(X_test, X_train, alpha, kernel_func, sigma):\n    \"\"\"Makes predictions using a trained KRR model.\"\"\"\n    n_test = X_test.shape[0]\n    n_train = X_train.shape[0]\n    \n    K_cross = np.zeros((n_test, n_train))\n    for i in range(n_test):\n        for j in range(n_train):\n            K_cross[i, j] = kernel_func(X_test[i], X_train[j], sigma)\n            \n    f_values = K_cross @ alpha\n    y_pred = np.sign(f_values)\n    # Handle the case where f_value is 0; classify as +1 by convention.\n    y_pred[y_pred == 0] = 1\n    return y_pred\n\ndef generate_data(n_samples_class, L, f_pos, f_neg, p_flip, sigma_noise, rng):\n    \"\"\"Generates synthetic waveform data for two classes.\"\"\"\n    t = np.linspace(0, 1, L, endpoint=False)\n    X = []\n    y = []\n\n    for label, freq in [(1, f_pos), (-1, f_neg)]:\n        for _ in range(n_samples_class):\n            phase = rng.uniform(0, 2 * np.pi)\n            waveform = np.sin(2 * np.pi * freq * t + phase)\n            \n            # Add noise\n            noise = rng.normal(0, sigma_noise, L)\n            waveform += noise\n            \n            # Apply sign flip\n            if rng.random() < p_flip:\n                waveform *= -1\n            \n            # Normalize to unit l2-norm\n            norm = np.linalg.norm(waveform)\n            if norm > 1e-9:\n                waveform /= norm\n            \n            X.append(waveform)\n            y.append(label)\n            \n    return np.array(X), np.array(y)\n\ndef run_classification_test(params):\n    \"\"\"Runs a full classification test for a given set of parameters.\"\"\"\n    rng = np.random.default_rng(params['seed'])\n    \n    # Generate training data\n    X_train, y_train = generate_data(\n        params['n_train_class'], params['L'], params['f_pos'], params['f_neg'],\n        params['p_flip'], params['sigma_noise'], rng)\n    \n    # Generate testing data with a separate RNG sequence\n    X_test, y_test = generate_data(\n        params['n_test_class'], params['L'], params['f_pos'], params['f_neg'],\n        params['p_flip'], params['sigma_noise'], rng)\n\n    accuracies = []\n    for kernel_func in [k_rbf, k_inv]:\n        # Train\n        K_train = compute_gram_matrix(X_train, kernel_func, params['sigma'])\n        alpha = train_krr(K_train, y_train, params['lambda_reg'])\n        \n        # Predict and evaluate\n        y_pred = predict_krr(X_test, X_train, alpha, kernel_func, params['sigma'])\n        accuracy = np.mean(y_pred == y_test)\n        accuracies.append(accuracy)\n        \n    return accuracies\n\n\ndef solve():\n    \"\"\"Main function to run all test cases and print results.\"\"\"\n    \n    results = []\n\n    # Test Case 1\n    params1 = {\n        'n_train_class': 60, 'n_test_class': 200, 'L': 128,\n        'f_pos': 3, 'f_neg': 7, 'p_flip': 0.5, 'sigma_noise': 0.05,\n        'sigma': 1.0, 'lambda_reg': 1e-3, 'seed': 12345\n    }\n    acc_base_1, acc_inv_1 = run_classification_test(params1)\n    results.extend([acc_base_1, acc_inv_1])\n\n    # Test Case 2\n    params2 = {\n        'n_train_class': 20, 'n_test_class': 200, 'L': 64,\n        'f_pos': 2, 'f_neg': 5, 'p_flip': 1.0, 'sigma_noise': 0.0,\n        'sigma': 1.0, 'lambda_reg': 1e-3, 'seed': 54321\n    }\n    acc_base_2, acc_inv_2 = run_classification_test(params2)\n    results.extend([acc_base_2, acc_inv_2])\n\n    # Test Case 3\n    L3, f3, sigma3 = 256, 4, 1.0\n    t3 = np.linspace(0, 1, L3, endpoint=False)\n    waveform = np.sin(2 * np.pi * f3 * t3)\n    norm = np.linalg.norm(waveform)\n    if norm > 1e-9:\n        waveform /= norm\n\n    v1 = k_inv(waveform, -waveform, sigma3)\n    v2 = k_inv(waveform, waveform, sigma3)\n    \n    is_invariant = np.isclose(v1, v2, atol=1e-9)\n    results.append(is_invariant)\n\n    # Format and print the final output\n    # Convert boolean to lowercase 'true'/'false' as often expected, although\n    # default str() would be 'True'/'False'. Using default for compliance.\n    output_str = [f\"{r:.10f}\" if isinstance(r, float) else str(r) for r in results]\n    print(f\"[{','.join(output_str)}]\")\n\nsolve()\n\n```", "id": "3136231"}, {"introduction": "核方法的应用远远超出了监督分类和回归的范畴。在本实践中，我们将探索一个强大的应用：使用最大均值差异（Maximum Mean Discrepancy, MMD）来衡量两个概率分布之间的“距离”。我们将实现一个基于梯度的方法来调整数据生成器，使其输出分布与目标分布相匹配，这是现代生成模型中的一项核心任务。[@problem_id:3136216]", "problem": "你的任务是，在一个再生核希尔伯特空间（RKHS）中，使用最大均值差异（MMD）目标，为参数化生成器推导并实现一个基于梯度的、符合原理的调优流程，重点关注超越支持向量机的核方法。你的生成器是一个一维的简单位置-尺度模拟器，目标是来自单变量正态分布的样本。你必须仅使用核心定义和基本微积分法则，计算经验平方MMD关于生成器参数的梯度，然后使用此梯度通过梯度下降来调优参数。\n\n从以下基本出发点开始：\n- 对于核函数 $k$，分布 $\\mathbb{P}$ 和 $\\mathbb{Q}$ 之间的平方最大均值差异（MMD）是它们核均值嵌入之差的平方RKHS范数：\n$$\\operatorname{MMD}^2(\\mathbb{P},\\mathbb{Q}) \\equiv \\lVert \\mu_{\\mathbb{P}} - \\mu_{\\mathbb{Q}} \\rVert_{\\mathcal{H}}^2,$$\n其中 $\\mu_{\\mathbb{P}} \\equiv \\mathbb{E}_{X \\sim \\mathbb{P}}[k(\\cdot, X)]$ 和 $\\mu_{\\mathbb{Q}} \\equiv \\mathbb{E}_{Y \\sim \\mathbb{Q}}[k(\\cdot, Y)]$ 位于一个再生核希尔伯特空间（RKHS）中。\n- 利用极化恒等式和RKHS内积的双线性，一个被广泛使用且经过充分检验的表达式是\n$$\\operatorname{MMD}^2(\\mathbb{P},\\mathbb{Q}) = \\mathbb{E}_{X,X' \\sim \\mathbb{P}}[k(X,X')] + \\mathbb{E}_{Y,Y' \\sim \\mathbb{Q}}[k(Y,Y')] - 2 \\mathbb{E}_{X \\sim \\mathbb{P}, Y \\sim \\mathbb{Q}}[k(X,Y)]。$$\n- 给定样本 $\\{x_i\\}_{i=1}^n$ 和 $\\{y_j\\}_{j=1}^m$，$\\operatorname{MMD}^2$ 的经验（有偏）估计量是\n$$\\widehat{\\operatorname{MMD}}^2 = \\frac{1}{n^2} \\sum_{i=1}^{n}\\sum_{i'=1}^{n} k(x_i,x_{i'}) + \\frac{1}{m^2} \\sum_{j=1}^{m}\\sum_{j'=1}^{m} k(y_j,y_{j'}) - \\frac{2}{nm} \\sum_{i=1}^{n}\\sum_{j=1}^{m} k(x_i,y_j)。$$\n- 带宽为 $\\sigma$ 的高斯（径向基函数）核是 $k(u,v) = \\exp\\!\\left(-\\frac{\\lVert u - v\\rVert^2}{2\\sigma^2}\\right)$。\n- 使用标准的多变量微积分（链式法则和乘积法则）来对复合函数求导。\n\n生成器模型与任务：\n- 考虑一个一维生成器 $g_{\\theta}(z) = \\theta_1 + \\theta_2 z$，其参数向量为 $\\theta = (\\theta_1, \\theta_2)$，基础噪声 $z \\sim \\mathcal{N}(0,1)$ 在优化步骤中保持固定以保证可复现性。\n- 给定一个固定的目标数据集 $\\{x_i\\}_{i=1}^n$ 和由固定噪声 $\\{z_j\\}_{j=1}^m$ 生成的样本 $\\{y_j\\}_{j=1}^m$（其中 $y_j = g_{\\theta}(z_j)$），使用高斯核推导 $\\nabla_{\\theta} \\widehat{\\operatorname{MMD}}^2$，从上述定义出发，不使用简化公式。\n- 实现一个程序，该程序能够：\n  - 计算高斯核的经验平方MMD $\\widehat{\\operatorname{MMD}}^2$。\n  - 使用你的推导计算梯度 $\\nabla_{\\theta} \\widehat{\\operatorname{MMD}}^2$。\n  - 使用固定的学习率和指定的步数运行梯度下降以更新 $\\theta$。\n  - 为每个测试用例返回最终调优的参数和最终的经验平方MMD。\n\n测试套件：\n- 使用以下测试用例。对于每个用例，使用提供的随机种子独立生成目标数据 $\\{x_i\\}$ 和固定的生成器噪声 $\\{z_j\\}$。目标数据从单变量正态分布 $\\mathcal{N}(\\mu_{\\mathrm{tgt}}, s_{\\mathrm{tgt}}^2)$ 中抽取，其中 $\\mu_{\\mathrm{tgt}}$ 是目标均值， $s_{\\mathrm{tgt}}$ 是目标标准差。\n  1. 情况 A（顺利情况）：$\\mu_{\\mathrm{tgt}} = 1.0$, $s_{\\mathrm{tgt}} = 0.5$, $n = 200$, $m = 200$, 初始 $\\theta^{(0)} = (0.3, 0.8)$, 核带宽 $\\sigma = 1.0$, 学习率 $\\alpha = 0.1$, 步数 $T = 200$, 种子 $= 123$。\n  2. 情况 B（恒等目标）：$\\mu_{\\mathrm{tgt}} = 0.0$, $s_{\\mathrm{tgt}} = 1.0$, $n = 200$, $m = 200$, 初始 $\\theta^{(0)} = (0.2, 1.5)$, 核带宽 $\\sigma = 1.0$, 学习率 $\\alpha = 0.1$, 步数 $T = 200$, 种子 $= 456$。\n  3. 情况 C（平移和缩放）：$\\mu_{\\mathrm{tgt}} = -1.0$, $s_{\\mathrm{tgt}} = 0.7$, $n = 200$, $m = 200$, 初始 $\\theta^{(0)} = (0.0, 0.5)$, 核带宽 $\\sigma = 1.0$, 学习率 $\\alpha = 0.1$, 步数 $T = 250$, 种子 $= 789$。\n  4. 情况 D（小样本鲁棒性）：$\\mu_{\\mathrm{tgt}} = 0.5$, $s_{\\mathrm{tgt}} = 0.9$, $n = 20$, $m = 20$, 初始 $\\theta^{(0)} = (1.0, 0.2)$, 核带宽 $\\sigma = 1.0$, 学习率 $\\alpha = 0.1$, 步数 $T = 150$, 种子 $= 31415$。\n\n最终输出格式：\n- 你的程序应生成单行输出，其中包含一个用方括号括起来的、不含空格的逗号分隔列表。对于每个测试用例，按 A、B、C、D 的顺序，输出最终参数 $\\theta_1$，然后是 $\\theta_2$，然后是最终的经验平方MMD值，所有值都四舍五入到恰好 $6$ 位小数。因此，最终输出应为一个长度为 $12$ 的扁平列表，形式如下：\n$[\\theta_{1}^{A},\\theta_{2}^{A},\\widehat{\\operatorname{MMD}}^{2}_{A},\\theta_{1}^{B},\\theta_{2}^{B},\\widehat{\\operatorname{MMD}}^{2}_{B},\\theta_{1}^{C},\\theta_{2}^{C},\\widehat{\\operatorname{MMD}}^{2}_{C},\\theta_{1}^{D},\\theta_{2}^{D},\\widehat{\\operatorname{MMD}}^{2}_{D}]$，\n其中数值条目四舍五入到 $6$ 位小数，且不含额外文本。", "solution": "我们从再生核希尔伯特空间（RKHS）中最大均值差异（MMD）的定义开始。对于一个具有RKHS $\\mathcal{H}$的正定核 $k$ 和核均值嵌入 $\\mu_{\\mathbb{P}} \\equiv \\mathbb{E}[k(\\cdot, X)]$ 与 $\\mu_{\\mathbb{Q}} \\equiv \\mathbb{E}[k(\\cdot, Y)]$，平方MMD为\n$$\\operatorname{MMD}^2(\\mathbb{P},\\mathbb{Q}) \\equiv \\lVert \\mu_{\\mathbb{P}} - \\mu_{\\mathbb{Q}} \\rVert_{\\mathcal{H}}^2.$$\n利用RKHS内积的双线性和再生性质，可以得到一个被广泛使用且经过充分检验的公式\n$$\\operatorname{MMD}^2(\\mathbb{P},\\mathbb{Q}) = \\mathbb{E}_{X,X'}[k(X,X')] + \\mathbb{E}_{Y,Y'}[k(Y,Y')] - 2 \\mathbb{E}_{X,Y}[k(X,Y)],$$\n其中期望是针对独立副本 $X, X' \\sim \\mathbb{P}$ 和 $Y, Y' \\sim \\mathbb{Q}$ 计算的。对于经验样本 $\\{x_i\\}_{i=1}^{n}$ 和 $\\{y_j\\}_{j=1}^{m}$，有偏估计量为\n$$\\widehat{\\operatorname{MMD}}^2 = \\frac{1}{n^2} \\sum_{i=1}^{n}\\sum_{i'=1}^{n} k(x_i,x_{i'}) + \\frac{1}{m^2} \\sum_{j=1}^{m}\\sum_{j'=1}^{m} k(y_j,y_{j'}) - \\frac{2}{nm} \\sum_{i=1}^{n}\\sum_{j=1}^{m} k(x_i,y_j)。$$\n\n我们考虑一个一维生成器 $g_{\\theta}(z) = \\theta_1 + \\theta_2 z$，其基础噪声为 $z \\sim \\mathcal{N}(0,1)$。对于固定的噪声样本 $\\{z_j\\}_{j=1}^{m}$，令 $y_j(\\theta) = \\theta_1 + \\theta_2 z_j$。我们使用高斯核\n$$k(u,v) = \\exp\\!\\left(-\\frac{(u-v)^2}{2\\sigma^2}\\right),$$\n其带宽为 $\\sigma > 0$。经验平方MMD通过对 $\\{y_j(\\theta)\\}_{j=1}^{m}$ 的依赖性，成为一个关于 $\\theta$ 的可微函数。\n\n现在我们从第一性原理推导 $\\nabla_{\\theta} \\widehat{\\operatorname{MMD}}^2$。令 $L(\\theta) \\equiv \\widehat{\\operatorname{MMD}}^2(\\{x_i\\}, \\{y_j(\\theta)\\})$。根据链式法则，\n$$\\nabla_{\\theta} L(\\theta) = \\sum_{j=1}^{m} \\frac{\\partial L}{\\partial y_j} \\frac{\\partial y_j}{\\partial \\theta},$$\n其中 $\\frac{\\partial y_j}{\\partial \\theta} = \\begin{bmatrix} \\frac{\\partial y_j}{\\partial \\theta_1} \\\\ \\frac{\\partial y_j}{\\partial \\theta_2} \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ z_j \\end{bmatrix}$。\n\n因此，我们只需计算 $\\frac{\\partial L}{\\partial y_j}$。使用经验表达式，\n$$L(\\theta) = \\underbrace{\\frac{1}{n^2} \\sum_{i=1}^{n}\\sum_{i'=1}^{n} k(x_i,x_{i'})}_{\\text{常数，与 } \\theta \\text{ 无关}} + \\frac{1}{m^2} \\sum_{a=1}^{m}\\sum_{b=1}^{m} k(y_a,y_b) - \\frac{2}{nm} \\sum_{i=1}^{n}\\sum_{j=1}^{m} k(x_i,y_j)。$$\n第一项不依赖于 $\\theta$。对于第二项，关于 $y_j$ 的导数汇集了 $y_j$ 作为第一或第二参数时的贡献：\n$$\\frac{\\partial}{\\partial y_j} \\left( \\frac{1}{m^2} \\sum_{a=1}^{m}\\sum_{b=1}^{m} k(y_a,y_b) \\right) = \\frac{1}{m^2} \\left( \\sum_{b=1}^{m} \\frac{\\partial}{\\partial y_j} k(y_j, y_b) + \\sum_{a=1}^{m} \\frac{\\partial}{\\partial y_j} k(y_a, y_j) \\right)。$$\n对于高斯核，关于第二个参数的梯度等于\n$$\\frac{\\partial}{\\partial v} k(u,v) = k(u,v)\\cdot \\frac{u - v}{\\sigma^2}。$$\n因此，\n$$\\frac{\\partial}{\\partial y_j} k(y_j, y_b) = k(y_j, y_b) \\cdot \\frac{y_b - y_j}{\\sigma^2}, \\quad \\frac{\\partial}{\\partial y_j} k(y_a, y_j) = k(y_a, y_j) \\cdot \\frac{y_a - y_j}{\\sigma^2}。$$\n根据对称性 $k(y_j,y_b) = k(y_b,y_j)$，这两个和具有相同的形式，合在一起为：\n$$\\frac{\\partial}{\\partial y_j} \\left( \\frac{1}{m^2} \\sum_{a,b} k(y_a,y_b) \\right) = \\frac{2}{m^2 \\sigma^2} \\sum_{b=1}^{m} k(y_j, y_b)\\, (y_b - y_j)。$$\n对于交叉项，\n$$\\frac{\\partial}{\\partial y_j} \\left( - \\frac{2}{nm} \\sum_{i=1}^{n}\\sum_{j=1}^{m} k(x_i,y_j) \\right) = - \\frac{2}{nm} \\sum_{i=1}^{n} \\frac{\\partial}{\\partial y_j} k(x_i, y_j) = - \\frac{2}{nm \\sigma^2} \\sum_{i=1}^{n} k(x_i, y_j)\\, (x_i - y_j)。$$\n将这些结合起来，得到关于每个生成器样本的标量导数：\n$$\\frac{\\partial L}{\\partial y_j} = \\frac{2}{m^2 \\sigma^2} \\sum_{b=1}^{m} k(y_j, y_b)\\, (y_b - y_j) \\;-\\; \\frac{2}{nm \\sigma^2} \\sum_{i=1}^{n} k(x_i, y_j)\\, (x_i - y_j)。$$\n\n使用链式法则和 $y_j(\\theta) = \\theta_1 + \\theta_2 z_j$ 可得\n$$\\frac{\\partial L}{\\partial \\theta_1} = \\sum_{j=1}^{m} \\frac{\\partial L}{\\partial y_j} \\cdot 1, \\quad \\frac{\\partial L}{\\partial \\theta_2} = \\sum_{j=1}^{m} \\frac{\\partial L}{\\partial y_j} \\cdot z_j。$$\n\n算法实现：\n- 使用成对差异和高斯核计算核矩阵 $K_{YY}$（其元素为 $k(y_j, y_b)$）和 $K_{XY}$（其元素为 $k(x_i, y_j)$）。\n- 将求和向量化。令 $y \\in \\mathbb{R}^{m}$ 和 $x \\in \\mathbb{R}^{n}$。定义成对差异矩阵 $\\Delta_{YY}$（其元素为 $(y_b - y_j)$）和 $\\Delta_{XY}$（其元素为 $(x_i - y_j)$）。然后\n$$\\left[ \\frac{\\partial L}{\\partial y_j} \\right]_{j=1}^{m} = \\frac{2}{m^2 \\sigma^2} \\left( K_{YY} \\odot \\Delta_{YY} \\right) \\mathbf{1}_m \\;-\\; \\frac{2}{nm \\sigma^2} \\left( K_{XY} \\odot \\Delta_{XY} \\right)^{\\top} \\mathbf{1}_n,$$\n其中 $\\odot$ 表示逐元素乘法，$\\mathbf{1}_k$ 是长度为 $k$ 的全1向量；求和通过矩阵-向量运算在相应的轴上进行。\n- 分别使用权重 $1$ 和 $z_j$ 进行求和，以累积 $\\frac{\\partial L}{\\partial \\theta_1}$ 和 $\\frac{\\partial L}{\\partial \\theta_2}$。\n- 使用学习率 $\\alpha$ 和指定的步数执行梯度下降更新 $\\theta \\leftarrow \\theta - \\alpha \\nabla_{\\theta} L$。\n\n为何这是正确的：\n- 出发点是MMD的RKHS定义及其经过充分检验的期望展开式；经验估计量是一个一致的置入式估计。\n- 对经验目标的微分仅使用了链式法则和高斯核的导数，而高斯核处处光滑。\n- 生成器是从参数到样本的可微映射，因此链式法则直接适用；雅可比矩阵 $\\frac{\\partial y_j}{\\partial \\theta}$ 在固定噪声 $z_j$ 上是线性的。\n- 在迭代过程中使用固定的 $\\{x_i\\}$ 和 $\\{z_j\\}$ 确保了目标函数是确定且可微的，适合进行梯度下降。\n\n数值规格和输出：\n- 针对四种情况，严格按照规定实现梯度下降：\n  1. 情况 A：$\\mu_{\\mathrm{tgt}} = 1.0$, $s_{\\mathrm{tgt}} = 0.5$, $n = 200$, $m = 200$, $\\theta^{(0)} = (0.3, 0.8)$, $\\sigma = 1.0$, $\\alpha = 0.1$, $T = 200$, 种子 $= 123$。\n  2. 情况 B：$\\mu_{\\mathrm{tgt}} = 0.0$, $s_{\\mathrm{tgt}} = 1.0$, $n = 200$, $m = 200$, $\\theta^{(0)} = (0.2, 1.5)$, $\\sigma = 1.0$, $\\alpha = 0.1$, $T = 200$, 种子 $= 456$。\n  3. 情况 C：$\\mu_{\\mathrm{tgt}} = -1.0$, $s_{\\mathrm{tgt}} = 0.7$, $n = 200$, $m = 200$, $\\theta^{(0)} = (0.0, 0.5)$, $\\sigma = 1.0$, $\\alpha = 0.1$, $T = 250$, 种子 $= 789$。\n  4. 情况 D：$\\mu_{\\mathrm{tgt}} = 0.5$, $s_{\\mathrm{tgt}} = 0.9$, $n = 20$, $m = 20$, $\\theta^{(0)} = (1.0, 0.2)$, $\\sigma = 1.0$, $\\alpha = 0.1$, $T = 150$, 种子 $= 31415$。\n- 优化后，按 A, B, C, D 的顺序，为每个情况报告最终的 $\\theta_1$, $\\theta_2$ 和最终的经验平方MMD值，四舍五入到6位小数，并连接成一个单一的、不含空格、用方括号括起来的扁平列表。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef rbf_kernel_matrix(a: np.ndarray, b: np.ndarray, sigma: float) -> np.ndarray:\n    # Compute pairwise squared distances and Gaussian kernel.\n    # a shape: (n,), b shape: (m,)\n    diff = a[:, None] - b[None, :]\n    K = np.exp(-0.5 * (diff ** 2) / (sigma ** 2))\n    return K\n\ndef mmd2_biased(x: np.ndarray, y: np.ndarray, sigma: float) -> float:\n    K_xx = rbf_kernel_matrix(x, x, sigma)\n    K_yy = rbf_kernel_matrix(y, y, sigma)\n    K_xy = rbf_kernel_matrix(x, y, sigma)\n    term_xx = K_xx.mean()\n    term_yy = K_yy.mean()\n    term_xy = K_xy.mean()\n    return float(term_xx + term_yy - 2.0 * term_xy)\n\ndef grad_y_mmd2_biased(x: np.ndarray, y: np.ndarray, sigma: float) -> np.ndarray:\n    # Computes gradient of biased empirical MMD^2 w.r.t. each y_j (1D).\n    n = x.shape[0]\n    m = y.shape[0]\n    K_yy = rbf_kernel_matrix(y, y, sigma)           # shape (m, m)\n    K_xy = rbf_kernel_matrix(x, y, sigma)           # shape (n, m)\n    # Differences\n    diff_yy = y[None, :] - y[:, None]               # shape (m, m): (y_b - y_j)\n    diff_xy = x[:, None] - y[None, :]               # shape (n, m): (x_i - y_j)\n    # Weighted sums\n    part1 = (K_yy * diff_yy).sum(axis=1)            # shape (m,)\n    part2 = (K_xy * diff_xy).sum(axis=0)            # shape (m,)\n    factor1 = 2.0 / (m * m * sigma * sigma)\n    factor2 = 2.0 / (n * m * sigma * sigma)\n    grad_y = factor1 * part1 - factor2 * part2      # shape (m,)\n    return grad_y\n\ndef optimize_theta(x: np.ndarray, z: np.ndarray, theta0: tuple, sigma_k: float, lr: float, steps: int) -> tuple:\n    a, b = float(theta0[0]), float(theta0[1])\n    for _ in range(steps):\n        y = a + b * z\n        gy = grad_y_mmd2_biased(x, y, sigma_k)\n        ga = gy.sum()\n        gb = (gy * z).sum()\n        a = a - lr * ga\n        b = b - lr * gb\n    # Final stats\n    y = a + b * z\n    mmd2 = mmd2_biased(x, y, sigma_k)\n    return a, b, mmd2\n\ndef run_case(mu_tgt: float, s_tgt: float, n: int, m: int, theta0: tuple, sigma_k: float, lr: float, steps: int, seed: int):\n    rng = np.random.default_rng(seed)\n    x = rng.normal(loc=mu_tgt, scale=s_tgt, size=n).astype(float)\n    z = rng.normal(loc=0.0, scale=1.0, size=m).astype(float)\n    a, b, mmd2 = optimize_theta(x, z, theta0, sigma_k, lr, steps)\n    return a, b, mmd2\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case A\n        {\"mu\": 1.0, \"s\": 0.5, \"n\": 200, \"m\": 200, \"theta0\": (0.3, 0.8), \"sigma_k\": 1.0, \"lr\": 0.1, \"steps\": 200, \"seed\": 123},\n        # Case B\n        {\"mu\": 0.0, \"s\": 1.0, \"n\": 200, \"m\": 200, \"theta0\": (0.2, 1.5), \"sigma_k\": 1.0, \"lr\": 0.1, \"steps\": 200, \"seed\": 456},\n        # Case C\n        {\"mu\": -1.0, \"s\": 0.7, \"n\": 200, \"m\": 200, \"theta0\": (0.0, 0.5), \"sigma_k\": 1.0, \"lr\": 0.1, \"steps\": 250, \"seed\": 789},\n        # Case D\n        {\"mu\": 0.5, \"s\": 0.9, \"n\": 20, \"m\": 20, \"theta0\": (1.0, 0.2), \"sigma_k\": 1.0, \"lr\": 0.1, \"steps\": 150, \"seed\": 31415},\n    ]\n\n    results = []\n    for case in test_cases:\n        a, b, mmd2 = run_case(\n            mu_tgt=case[\"mu\"],\n            s_tgt=case[\"s\"],\n            n=case[\"n\"],\n            m=case[\"m\"],\n            theta0=case[\"theta0\"],\n            sigma_k=case[\"sigma_k\"],\n            lr=case[\"lr\"],\n            steps=case[\"steps\"],\n            seed=case[\"seed\"]\n        )\n        results.extend([a, b, mmd2])\n\n    # Format results to 6 decimal places, no spaces inside the list.\n    formatted = \"[\" + \",\".join(f\"{v:.6f}\" for v in results) + \"]\"\n    print(formatted)\n\nsolve()\n```", "id": "3136216"}]}