{"hands_on_practices": [{"introduction": "Probit 回归的系数通常通过最大化对数似然函数来估计。这个过程的核心是强大的数值优化算法，例如 Newton-Raphson 方法。通过亲手实现 Newton-Raphson 算法的单步迭代，你将不仅能理解模型拟合的数学原理，包括梯度（得分向量）和 Hessian 矩阵，还能直面并解决在极端值附近可能出现的数值稳定性挑战。这个练习将带你深入了解 Probit 模型估计的“引擎盖”之下。[@problem_id:3162253]", "problem": "考虑一个二元响应模型，其观测值为独立同分布（IID）的 $\\{(x_i,y_i)\\}_{i=1}^n$，其中 $y_i \\in \\{0,1\\}$ 且 $x_i \\in \\mathbb{R}^p$ 是固定预测变量。在概率单位回归（probit regression）模型中，条件成功概率被建模为 $p_i = \\Pr(y_i=1 \\mid x_i) = \\Phi(\\eta_i)$，其中线性预测变量为 $\\eta_i = x_i^\\top \\beta$，$\\Phi(\\cdot)$ 是标准正态累积分布函数。参数向量 $\\beta \\in \\mathbb{R}^p$ 的对数似然函数为 $\\ell(\\beta) = \\sum_{i=1}^n \\{ y_i \\log \\Phi(\\eta_i) + (1-y_i)\\log(1-\\Phi(\\eta_i)) \\}$。从伯努利似然和微分链式法则的基本定义出发，执行以下任务：\n\n- 推导得分向量 $s(\\beta)$（$\\ell(\\beta)$ 的梯度）和观测到的 Hessian 矩阵 $H(\\beta)$（$s(\\beta)$ 的雅可比矩阵），并用 $x_i$、$\\eta_i$、$\\Phi(\\eta_i)$ 以及标准正态概率密度函数 $\\varphi(\\eta_i)$ 表示。\n- 基于您的表达式，实现一次精确的牛顿-拉夫逊（Newton–Raphson）迭代，从当前迭代值 $\\beta^{\\text{old}}$ 生成更新后的参数向量 $\\beta^{\\text{new}}$。您的实现必须对 $\\eta_i$ 的极端值具有数值鲁棒性，特别是对于大的正或负 $\\eta_i$，此时直接计算诸如 $\\varphi(\\eta_i)/\\Phi(\\eta_i)$ 或 $\\varphi(\\eta_i)/(1-\\Phi(\\eta_i))$ 的比率可能不稳定。使用对数域计算和稳定的变换来避免灾难性抵消。如果观测到的 Hessian 矩阵是奇异或病态的，应用一个小的对角岭正则化，该正则化在普通尺度上可以忽略不计，但能确保线性求解的良定义性。\n- 在您的实现注释和求解推导中，分析为什么在 $\\eta_i$ 极端值附近会出现数值不稳定性，以及您选择的稳定化方法如何缓解它。\n\n您的程序必须实现一个函数，该函数给定设计矩阵 $X \\in \\mathbb{R}^{n \\times p}$、二元响应向量 $y \\in \\{0,1\\}^n$ 和当前迭代值 $\\beta^{\\text{old}} \\in \\mathbb{R}^p$，返回一次牛顿-拉夫逊更新 $\\beta^{\\text{new}}$。使用此函数处理以下测试套件。对于每个测试用例，计算并返回更新后的系数向量 $\\beta^{\\text{new}}$，四舍五入到六位小数。\n\n测试套件（每个用例指定 $(X, y, \\beta^{\\text{old}})$）：\n\n- 用例 A（基准，良态）：$X = \\begin{bmatrix} 1  -1 \\\\ 1  0 \\\\ 1  1 \\\\ 1  2 \\end{bmatrix}$，$y = \\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\\\ 1 \\end{bmatrix}$，$\\beta^{\\text{old}} = \\begin{bmatrix} 0 \\\\ 0.5 \\end{bmatrix}$。\n- 用例 B（混合极端尾部）：$X = \\begin{bmatrix} 1  -10 \\\\ 1  0 \\\\ 1  10 \\end{bmatrix}$，$y = \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix}$，$\\beta^{\\text{old}} = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}$。\n- 用例 C（几乎分离）：$X = \\begin{bmatrix} 1  -6 \\\\ 1  -3 \\\\ 1  0 \\\\ 1  3 \\\\ 1  6 \\end{bmatrix}$，$y = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 1 \\\\ 1 \\end{bmatrix}$，$\\beta^{\\text{old}} = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}$。\n- 用例 D（边界极端）：$X = \\begin{bmatrix} 1  50 \\\\ 1  -50 \\end{bmatrix}$，$y = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}$，$\\beta^{\\text{old}} = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}$。\n\n您的程序应生成单行输出，其中包含结果，格式为方括号括起来的逗号分隔列表的列表。每个内部列表对应一个用例，并且必须按顺序包含更新后的系数，四舍五入到六位小数。例如，一个包含两个用例、每个用例两个系数的输出必须看起来像 $[[a_{11},a_{12}],[a_{21},a_{22}]]$，其中每个 $a_{ij}$ 都是一个十进制数。此问题不涉及物理单位、角度单位或百分比。输出必须与指定格式完全匹配，并按 A、B、C、D 的顺序包含所有四个用例的结果。", "solution": "用户希望解决一个关于概率单位回归的统计学习问题。\n\n**1. 问题验证**\n\n用户指定了一个计算统计学中的标准问题：实现用于拟合概率单位回归模型的牛顿-拉夫逊算法。所有必要的组成部分都已提供：模型规范、对数似然函数、任务（推导和实现），以及一组具有明确定义的输入（$X$、$y$、$\\beta^{\\text{old}}$）和所需输出格式的测试用例。该问题要求仔细处理数值稳定性，这是科学计算中的一个标准且重要的话题。该问题具有科学依据，是良定义的、客观的。它不包含任何矛盾、歧义或伪科学主张。\n\n**结论：问题有效。**\n\n**2. 求解推导与实现策略**\n\n问题的核心是找到概率单位模型参数 $\\beta$ 的最大似然估计（MLE）。牛顿-拉夫逊算法是一种迭代方法，用于寻找函数一阶导数（在本例中为得分向量）的根，这对应于函数本身（对数似然）的极值点。单次迭代将当前参数估计 $\\beta^{\\text{old}}$ 更新为新的估计 $\\beta^{\\text{new}}$。\n\n对于给定的预测变量 $x_i \\in \\mathbb{R}^p$ 的二元结果 $y_i \\in \\{0, 1\\}$，概率单位模型由成功概率定义：\n$$p_i = \\Pr(y_i=1 \\mid x_i) = \\Phi(x_i^\\top \\beta) = \\Phi(\\eta_i)$$\n其中 $\\eta_i = x_i^\\top \\beta$ 是线性预测变量，$\\Phi(\\cdot)$ 是标准正态分布的累积分布函数（CDF）。$n$ 个独立观测值的对数似然函数是：\n$$\\ell(\\beta) = \\sum_{i=1}^n \\{ y_i \\log \\Phi(\\eta_i) + (1-y_i)\\log(1-\\Phi(\\eta_i)) \\}$$\n\n**2.1. 得分向量推导**\n\n得分向量 $s(\\beta)$ 是对数似然函数关于 $\\beta$ 的梯度。我们通过应用链式法则来找到它。设 $\\varphi(\\cdot)$ 为标准正态分布的概率密度函数（PDF），其中 $\\frac{d}{dz}\\Phi(z) = \\varphi(z)$。\n\n单个观测值 $\\ell_i$ 的对数似然关于参数分量 $\\beta_j$ 的导数是：\n$$\\frac{\\partial \\ell_i}{\\partial \\beta_j} = \\frac{\\partial \\ell_i}{\\partial \\eta_i} \\frac{\\partial \\eta_i}{\\partial \\beta_j}$$\n第一项是：\n$$\\frac{\\partial \\ell_i}{\\partial \\eta_i} = y_i \\frac{1}{\\Phi(\\eta_i)} \\frac{\\partial \\Phi(\\eta_i)}{\\partial \\eta_i} + (1-y_i) \\frac{1}{1-\\Phi(\\eta_i)} \\frac{\\partial (1-\\Phi(\\eta_i))}{\\partial \\eta_i} = y_i \\frac{\\varphi(\\eta_i)}{\\Phi(\\eta_i)} - (1-y_i) \\frac{\\varphi(\\eta_i)}{1-\\Phi(\\eta_i)}$$\n第二项很简单，$\\frac{\\partial \\eta_i}{\\partial \\beta_j} = \\frac{\\partial}{\\partial \\beta_j} (x_i^\\top \\beta) = x_{ij}$。\n结合这些，得分向量的第 $j$ 个分量是：\n$$s_j(\\beta) = \\frac{\\partial \\ell}{\\partial \\beta_j} = \\sum_{i=1}^n x_{ij} \\left( y_i \\frac{\\varphi(\\eta_i)}{\\Phi(\\eta_i)} - (1-y_i) \\frac{\\varphi(\\eta_i)}{1-\\Phi(\\eta_i)} \\right)$$\n在向量形式中，为了符号清晰和数值稳定性管理，我们定义两个项：$\\lambda(\\eta) = \\frac{\\varphi(\\eta)}{\\Phi(\\eta)}$ 和 $\\lambda'(\\eta) = -\\frac{\\varphi(\\eta)}{1-\\Phi(\\eta)}$。得分向量可以表示为：\n$$s(\\beta) = \\sum_{i=1}^n x_i \\cdot \\left( y_i \\lambda(\\eta_i) + (1-y_i) \\lambda'(\\eta_i) \\right) = X^\\top c$$\n其中 $c$ 是一个向量，其分量为 $c_i = y_i \\lambda(\\eta_i) + (1-y_i) \\lambda'(\\eta_i)$。\n\n**2.2. 观测 Hessian 矩阵推导**\n\n观测 Hessian 矩阵 $H(\\beta)$ 是对数似然的二阶偏导数矩阵。其元素 $H_{jk}$ 为 $\\frac{\\partial^2 \\ell}{\\partial \\beta_j \\partial \\beta_k}$。我们通过对得分向量分量 $s_j(\\beta)$ 关于 $\\beta_k$ 求导来找到它：\n$$H_{jk}(\\beta) = \\frac{\\partial s_j(\\beta)}{\\partial \\beta_k} = \\sum_{i=1}^n x_{ij} \\frac{\\partial}{\\partial \\beta_k} \\left( \\frac{\\partial \\ell_i}{\\partial \\eta_i} \\right) = \\sum_{i=1}^n x_{ij} \\frac{\\partial^2 \\ell_i}{\\partial \\eta_i^2} \\frac{\\partial \\eta_i}{\\partial \\beta_k} = \\sum_{i=1}^n x_{ij} x_{ik} \\frac{\\partial^2 \\ell_i}{\\partial \\eta_i^2}$$\n我们需要 $\\ell_i$ 关于 $\\eta_i$ 的二阶导数。我们对 $\\frac{\\partial \\ell_i}{\\partial \\eta_i} = y_i \\lambda(\\eta_i) + (1-y_i) \\lambda'(\\eta_i)$ 进行微分。使用商法则以及 $\\frac{d}{d\\eta}\\varphi(\\eta) = -\\eta\\varphi(\\eta)$ 这个事实，我们得到：\n$$\\frac{d}{d\\eta}\\lambda(\\eta) = \\frac{d}{d\\eta}\\left(\\frac{\\varphi(\\eta)}{\\Phi(\\eta)}\\right) = \\frac{-\\eta\\varphi(\\eta)\\Phi(\\eta) - \\varphi(\\eta)^2}{\\Phi(\\eta)^2} = -\\eta\\lambda(\\eta) - \\lambda(\\eta)^2$$\n$$\\frac{d}{d\\eta}\\lambda'(\\eta) = \\frac{d}{d\\eta}\\left(-\\frac{\\varphi(\\eta)}{1-\\Phi(\\eta)}\\right) = -\\eta\\lambda'(\\eta) - \\lambda'(\\eta)^2$$\n因此，二阶导数是：\n$$\\frac{\\partial^2 \\ell_i}{\\partial \\eta_i^2} = y_i \\left(-\\eta_i \\lambda(\\eta_i) - \\lambda(\\eta_i)^2\\right) + (1-y_i) \\left(-\\eta_i \\lambda'(\\eta_i) - \\lambda'(\\eta_i)^2\\right)$$\nHessian 矩阵可以紧凑地写为 $H(\\beta) = X^\\top D X$，其中 $D$ 是一个对角矩阵，其对角线元素为 $d_{ii} = \\frac{\\partial^2 \\ell_i}{\\partial \\eta_i^2}$。对于一个严格凹的对数似然函数，$H(\\beta)$ 是负定的。\n\n**2.3. 牛顿-拉夫逊更新步骤**\n\n牛顿-拉夫逊方法用一个二次函数来近似对数似然曲面，并向其最大值迈进。更新规则是：\n$$\\beta^{\\text{new}} = \\beta^{\\text{old}} - [H(\\beta^{\\text{old}})]^{-1} s(\\beta^{\\text{old}})$$\n在计算上，这是通过求解线性系统 $H(\\beta^{\\text{old}}) \\Delta\\beta = s(\\beta^{\\text{old}})$ 来获得步长 $\\Delta\\beta$，然后更新 $\\beta^{\\text{new}} = \\beta^{\\text{old}} - \\Delta\\beta$ 来实现的。\n\n**2.4. 数值稳定性与正则化**\n\n直接计算 $\\lambda(\\eta_i)$ 和 $\\lambda'(\\eta_i)$ 对于 $\\eta_i$ 的极端值在数值上是不稳定的。\n- 对于大的正 $\\eta_i$，$1-\\Phi(\\eta_i)$ 和 $\\varphi(\\eta_i)$ 都会下溢到 0，使得 $\\lambda'(\\eta_i)$ 成为一个不稳定的 $0/0$ 形式。\n- 对于大的负 $\\eta_i$，$\\Phi(\\eta_i)$ 和 $\\varphi(\\eta_i)$ 都会下溢到 0，使得 $\\lambda(\\eta_i)$ 成为一个不稳定的 $0/0$ 形式。\n\n为了解决这个问题，我们在对数域中计算这些量，这样可以保持数值精度：\n$$\\lambda(\\eta) = \\exp(\\log(\\varphi(\\eta)) - \\log(\\Phi(\\eta)))$$\n$$\\lambda'(\\eta) = -\\exp(\\log(\\varphi(\\eta)) - \\log(1-\\Phi(\\eta)))$$\n专门的函数，如 `scipy.stats.norm.logpdf`、`logcdf` 和 `logsf`（对数生存函数），被设计用来精确计算这些对数量，即使参数位于分布的极端尾部。\n\n此外，如果数据是完全或几乎可分的，$\\beta$ 的最大似然估计会位于无穷远处，Hessian 矩阵会变得奇异或病态。这使得求解 $\\Delta\\beta$ 的线性系统无法进行。为防止这种情况，我们对 Hessian 矩阵应用少量的岭正则化。我们求解 $(H - \\epsilon I) \\Delta\\beta = s$，其中 $I$ 是单位矩阵，$\\epsilon$ 是一个小的正常数（例如 $10^{-8}$）。这确保了矩阵是可逆的，同时在良态情况下对解的影响可以忽略不计。这个过程是 Levenberg-Marquardt 类型调整的一种形式，可以稳定数值求逆过程。", "answer": "```python\nimport numpy as np\nfrom scipy.stats import norm\n\ndef solve():\n    \"\"\"\n    Computes a single Newton-Raphson update for probit regression coefficients\n    for a suite of test cases.\n    \"\"\"\n\n    test_cases = [\n        # Case A (baseline, well-behaved)\n        (\n            [[1, -1], [1, 0], [1, 1], [1, 2]],\n            [0, 0, 1, 1],\n            [0, 0.5]\n        ),\n        # Case B (extreme tails mixed)\n        (\n            [[1, -10], [1, 0], [1, 10]],\n            [1, 0, 0],\n            [0, 1]\n        ),\n        # Case C (nearly separated)\n        (\n            [[1, -6], [1, -3], [1, 0], [1, 3], [1, 6]],\n            [0, 0, 0, 1, 1],\n            [0, 1]\n        ),\n        # Case D (boundary extreme)\n        (\n            [[1, 50], [1, -50]],\n            [0, 1],\n            [0, 1]\n        )\n    ]\n\n    def probit_newton_step(X, y, beta_old):\n        \"\"\"\n        Performs one numerically robust Newton-Raphson iteration for probit regression.\n\n        - Analyzes instability and mitigation:\n          Direct computation of ratios like phi(eta)/Phi(eta) is unstable for extreme\n          eta where both numerator and denominator underflow to zero. This function\n          mitigates this by operating in the log domain using scipy's specialized\n          functions (logpdf, logcdf, logsf), which retain precision in the tails.\n          The expression exp(log(phi) - log(Phi)) is numerically stable.\n          \n          For (quasi-)separated data, the Hessian can be singular. This is handled\n          by adding a small diagonal ridge term (epsilon * I), which ensures the\n          matrix is invertible without significantly affecting the step in\n          well-conditioned cases.\n        \"\"\"\n        p = X.shape[1]\n        epsilon = 1e-8  # Regularization parameter\n\n        # 1. Compute linear predictor\n        eta = X @ beta_old\n\n        # 2. Compute components for score and Hessian stably in the log domain\n        log_phi = norm.logpdf(eta)\n        log_Phi = norm.logcdf(eta)\n        log_sf = norm.logsf(eta)  # log(1 - Phi(eta))\n\n        # lambda_i = phi(eta_i) / Phi(eta_i)\n        lambda_i = np.exp(log_phi - log_Phi)\n        # lambda_prime_i = -phi(eta_i) / (1 - Phi(eta_i))\n        lambda_prime_i = -np.exp(log_phi - log_sf)\n\n        # 3. Compute score vector s(beta)\n        s_components = np.where(y == 1, lambda_i, lambda_prime_i)\n        score = X.T @ s_components\n\n        # 4. Compute observed Hessian matrix H(beta)\n        d_i_if_y1 = -eta * lambda_i - lambda_i**2\n        d_i_if_y0 = -eta * lambda_prime_i - lambda_prime_i**2\n        d = np.where(y == 1, d_i_if_y1, d_i_if_y0)\n        \n        # H = X^T * diag(d) * X\n        hessian = (X.T * d) @ X\n\n        # 5. Regularize Hessian and solve for the update step delta_beta\n        hessian_reg = hessian - epsilon * np.eye(p)\n        \n        # The Newton-Raphson update is beta_new = beta_old - inv(H) * s.\n        # We solve H * delta_beta = s, then update beta_new = beta_old - delta_beta.\n        delta_beta = np.linalg.solve(hessian_reg, score)\n\n        # 6. Update beta\n        beta_new = beta_old - delta_beta\n        return beta_new\n\n    results = []\n    for X_list, y_list, beta_old_list in test_cases:\n        X = np.array(X_list, dtype=float)\n        y = np.array(y_list, dtype=float)\n        beta_old = np.array(beta_old_list, dtype=float)\n        beta_new = probit_newton_step(X, y, beta_old)\n        results.append(beta_new)\n    \n    # Format the output exactly as specified.\n    rounded_results = [np.round(res, 6) for res in results]\n    str_results = []\n    for res in rounded_results:\n        # Format each sublist as \"[v1,v2,...]\" without spaces after commas\n        str_results.append(f\"[{','.join(map(str, res))}]\")\n    \n    final_output = f\"[{','.join(str_results)}]\"\n    print(final_output)\n\nsolve()\n```", "id": "3162253"}, {"introduction": "Probit 模型的一个挑战是其系数 $\\beta$ 并不直接等于特征变化对成功概率的边际效应。这个练习将引导你探索一个常见的预处理步骤——特征标准化——如何影响模型系数及其解释。通过对原始和标准化后的特征分别拟合 Probit 模型，你将定量地验证系数之间的确定性转换关系，并揭示标准化如何使不同尺度的变量效应更具可比性。这项实践对于正确解释和比较模型中变量的重要性至关重要。[@problem_id:3162301]", "problem": "考虑一个使用 probit 连接函数的二元响应模型。设 $Y_i \\in \\{0,1\\}$ 为观测结果，协变量 $X_i \\in \\mathbb{R}^{p}$，潜变量为 $Y_i^* = X_i^\\top \\beta + \\varepsilon_i$，其中 $\\varepsilon_i \\sim \\mathcal{N}(0,1)$ 在观测 $i$ 之间相互独立。$Y_i = 1$ 的概率为 $P(Y_i=1 \\mid X_i) = \\Phi(X_i^\\top \\beta)$，其中 $\\Phi(\\cdot)$ 表示标准正态累积分布函数 (CDF)。参数 $\\beta$ 通过最大化对数似然 $\\sum_{i=1}^{n} \\left[ Y_i \\log \\Phi(X_i^\\top \\beta) + (1-Y_i) \\log \\left(1 - \\Phi(X_i^\\top \\beta)\\right) \\right]$ 进行最大似然估计 (MLE)。给定一个带有截距项的数据集，对每个非截距项特征 $j$ 定义特征标准化为 $Z_{ij} = (X_{ij} - \\mu_j)/\\sigma_j$，其中 $\\mu_j$ 是特征 $j$ 的样本均值，$\\sigma_j$ 是其样本标准差。截距项不进行标准化。\n\n您的任务是实现一个程序，该程序：\n- 从潜变量模型 $Y_i^* = X_i^\\top \\beta + \\varepsilon_i$（其中 $\\varepsilon_i \\sim \\mathcal{N}(0,1)$）生成合成数据集，并设置 $Y_i = \\mathbf{1}\\{Y_i^*  0\\}$。程序必须使用固定的随机种子 $42$ 以保证可复现性。\n- 对每个数据集，在原始特征（包括截距）上通过 MLE 拟合一个 probit 回归模型以获得 $\\hat{\\beta}$，然后将非截距项特征标准化为零均值和单位方差，并重新拟合 probit 回归模型以获得标准化特征上的 $\\hat{\\beta}^{\\text{std}}$。\n- 推导并量化验证由标准化引起的线性重参数化。具体来说，对于非截距项斜率，从 $\\hat{\\beta}$ 到标准化斜率的确定性映射为 $\\hat{\\beta}^{\\text{map}}_j = \\hat{\\beta}_j \\sigma_j$（对每个特征 $j$），标准化的截距为 $\\hat{\\beta}^{\\text{map}}_0 = \\hat{\\beta}_0 + \\sum_{j=1}^{p} \\hat{\\beta}_j \\mu_j$。计算 $\\hat{\\beta}^{\\text{std}}$ 和 $\\hat{\\beta}^{\\text{map}}$ 之间所有系数的最大绝对差，结果为一个浮点数。\n- 计算均值处的边际效应 (MEM)。特征 $k$ 在点 $x$ 处的边际效应为 $\\frac{\\partial}{\\partial x_k} \\Phi(X^\\top \\beta) = \\phi(X^\\top \\beta)\\,\\beta_k$，其中 $\\phi(\\cdot)$ 表示标准正态概率密度函数 (PDF)。在原始特征的样本均值处，令 $\\eta_{\\text{mean}} = \\hat{\\beta}_0 + \\sum_{j=1}^{p} \\hat{\\beta}_j \\mu_j$ 并计算 $\\phi(\\eta_{\\text{mean}})$。对于原始（未标准化）特征，计算每单位变化的 MEM：$\\text{MEM}^{\\text{orig}}_j = \\phi(\\eta_{\\text{mean}})\\,\\hat{\\beta}_j$，以及每标准差变化的 MEM：$\\text{MEM}^{\\text{orig,1sd}}_j = \\phi(\\eta_{\\text{mean}})\\,\\hat{\\beta}_j \\sigma_j$。对于标准化特征，每单位变化的 MEM 等于 $\\text{MEM}^{\\text{std}}_j = \\phi(\\eta_{\\text{mean}})\\,\\hat{\\beta}^{\\text{std}}_j$。计算向量 $\\text{MEM}^{\\text{std}}$ 和 $\\text{MEM}^{\\text{orig,1sd}}$ 之间的最大绝对差，结果为一个浮点数。\n\n使用平滑的目标函数稳健地实现数值优化。仅使用标准正态 CDF $\\Phi(\\cdot)$ 和 PDF $\\phi(\\cdot)$ 以及一个基于梯度的优化器。\n\n测试套件：\n- 情况 A（单特征）：$n=1000$，一个预测变量 $x_1 \\sim \\mathcal{N}(2,3^2)$，系数 $(\\beta_0,\\beta_1)=(-0.5,1.5)$。\n- 情况 B（双特征，异构尺度）：$n=2000$，$x_1 \\sim \\text{Uniform}[0,10]$，$x_2 \\sim \\mathcal{N}(-1,0.5^2)$，系数 $(\\beta_0,\\beta_1,\\beta_2)=(0.5,0.2,-2.0)$。\n- 情况 C（双特征，近饱和区域）：$n=1500$，$x_1 \\sim \\mathcal{N}(0,1^2)$，$x_2 \\sim \\mathcal{N}(0,1^2)$，系数 $(\\beta_0,\\beta_1,\\beta_2)=(3.0,-0.5,0.5)$。\n\n对于每种情况，输出两个浮点数：\n- 最大绝对系数映射误差：$\\max_{j \\in \\{0,1,\\dots,p\\}} \\left| \\hat{\\beta}^{\\text{std}}_j - \\hat{\\beta}^{\\text{map}}_j \\right|$。\n- 最大绝对MEM等价性误差：$\\max_{j \\in \\{1,\\dots,p\\}} \\left| \\text{MEM}^{\\text{std}}_j - \\text{MEM}^{\\text{orig,1sd}}_j \\right|$。\n\n最终输出格式：\n您的程序应生成单行输出，包含一个列表的列表，对应三种情况，每个内部列表 $[\\text{mapping\\_error},\\text{mem\\_error}]$ 的值四舍五入到小数点后六位，格式严格如下：\n$[[v_{A1},v_{A2}],[v_{B1},v_{B2}],[v_{C1},v_{C2}]]$。", "solution": "该问题要求实现并验证 probit 回归模型的关键性质，特别是关于特征标准化对估计系数和边际效应的影响。解决方案涉及数据生成、通过最大似然进行模型估计，以及对理论上等价的量进行量化比较。\n\n首先，我们定义 probit 模型。对于具有协变量向量 $X_i \\in \\mathbb{R}^{p+1}$（包括截距项 $X_{i0}=1$）的观测 $i$，其正向结果 $Y_i=1$ 的概率由标准正态累积分布函数 (CDF) $\\Phi(\\cdot)$ 应用于协变量的线性组合给出：\n$$\nP(Y_i=1 \\mid X_i) = \\Phi(X_i^\\top \\beta)\n$$\n这里，$\\beta \\in \\mathbb{R}^{p+1}$ 是待估计的系数向量。该模型源于一个潜变量公式 $Y_i^* = X_i^\\top \\beta + \\varepsilon_i$，其中 $\\varepsilon_i \\sim \\mathcal{N}(0,1)$ 是独立同分布的标准正态误差，观测到的结果是 $Y_i = \\mathbf{1}\\{Y_i^*  0\\}$。\n\n系数 $\\beta$ 通过最大化对数似然函数来估计。对于一个包含 $n$ 个观测的数据集 $\\{(X_i, Y_i)\\}_{i=1}^n$，对数似然为：\n$$\n\\mathcal{L}(\\beta) = \\sum_{i=1}^{n} \\left[ Y_i \\log \\Phi(X_i^\\top \\beta) + (1-Y_i) \\log \\left(1 - \\Phi(X_i^\\top \\beta)\\right) \\right]\n$$\n为了使用基于梯度的数值优化器执行此最大化过程，我们需要 $\\mathcal{L}(\\beta)$ 相对于 $\\beta$ 的梯度。令 $\\eta_i = X_i^\\top \\beta$。单个观测 $i$ 的梯度为：\n$$\n\\nabla_{\\beta} \\ell_i(\\beta) = \\left( \\frac{Y_i}{\\Phi(\\eta_i)} - \\frac{1-Y_i}{1 - \\Phi(\\eta_i)} \\right) \\phi(\\eta_i) X_i\n$$\n其中 $\\phi(\\cdot)$ 是标准正态概率密度函数 (PDF)，即 $\\Phi(\\cdot)$ 的导数。总梯度是所有观测上的总和：$\\nabla_{\\beta} \\mathcal{L} = \\sum_{i=1}^{n} \\nabla_{\\beta} \\ell_i(\\beta)$。我们将使用该梯度的负数来最小化负对数似然函数 $-\\mathcal{L}(\\beta)$。\n\n接下来，我们分析特征标准化的影响。对于每个非截距项特征 $j \\in \\{1, \\dots, p\\}$，我们定义其标准化版本为 $Z_{ij} = (X_{ij} - \\mu_j) / \\sigma_j$，其中 $\\mu_j$ 和 $\\sigma_j$ 分别是特征 $j$ 的样本均值和标准差。截距项保持为 $Z_{i0} = X_{i0} = 1$。原始的线性预测变量 $X_i^\\top \\beta$ 可以用标准化特征重新表示。通过代入 $X_{ij} = Z_{ij}\\sigma_j + \\mu_j$，我们得到：\n$$\nX_i^\\top \\beta = \\beta_0 + \\sum_{j=1}^{p} X_{ij}\\beta_j = \\beta_0 + \\sum_{j=1}^{p} (Z_{ij}\\sigma_j + \\mu_j)\\beta_j\n$$\n$$\nX_i^\\top \\beta = \\left(\\beta_0 + \\sum_{j=1}^{p} \\beta_j\\mu_j\\right) + \\sum_{j=1}^{p} Z_{ij}(\\beta_j\\sigma_j)\n$$\n此表达式是标准化特征的线性预测变量形式 $Z_i^\\top \\beta^{\\text{std}}$，其中新系数 $\\beta^{\\text{std}}$ 与原始系数 $\\beta$ 存在确定性关系：\n$$\n\\beta^{\\text{std}}_0 = \\beta_0 + \\sum_{j=1}^{p} \\beta_j\\mu_j\n$$\n$$\n\\beta^{\\text{std}}_j = \\beta_j\\sigma_j \\quad \\text{for } j \\in \\{1, \\dots, p\\}\n$$\n问题要求我们验证这种重参数化。我们首先通过在原始特征 $X$ 上拟合模型来估计 $\\hat{\\beta}$。然后，我们使用上述公式以及 $\\hat{\\beta}$、$\\mu_j$ 和 $\\sigma_j$ 来计算理论上映射的标准化系数 $\\hat{\\beta}^{\\text{map}}$。我们还通过直接在标准化特征 $Z$ 上拟合模型来估计 $\\hat{\\beta}^{\\text{std}}$。$\\hat{\\beta}^{\\text{map}}$ 和 $\\hat{\\beta}^{\\text{std}}$ 之间的微小差异将从数值上验证重参数化理论，任何差异都可归因于优化器的容差。映射误差计算为 $\\max_j |\\hat{\\beta}^{\\text{std}}_j - \\hat{\\beta}^{\\text{map}}_j|$。\n\n最后，我们考察边际效应，它衡量的是特征变化引起的成功概率的变化。特征 $k$ 的边际效应为：\n$$\n\\text{ME}_k(X) = \\frac{\\partial P(Y=1 \\mid X)}{\\partial X_k} = \\frac{\\partial}{\\partial X_k} \\Phi(X^\\top \\beta) = \\phi(X^\\top \\beta) \\beta_k\n$$\n该效应取决于所有协变量 $X$ 的值。一种常见的做法是在特征的样本均值处对其进行评估，这被称为均值处的边际效应 (MEM)。令 $\\eta_{\\text{mean}} = \\hat{\\beta}_0 + \\sum_{j=1}^{p} \\hat{\\beta}_j \\mu_j$。请注意，这正是映射后截距 $\\hat{\\beta}^{\\text{map}}_0$ 的公式。\n对于原始模型，$X_j$ 每单位变化的 MEM 是 $\\text{MEM}^{\\text{orig}}_j = \\phi(\\eta_{\\text{mean}})\\hat{\\beta}_j$。$X_j$ 每标准差变化的 MEM 是：\n$$\n\\text{MEM}^{\\text{orig,1sd}}_j = \\phi(\\eta_{\\text{mean}})\\hat{\\beta}_j\\sigma_j\n$$\n对于标准化模型，标准化特征 $Z_j$ 的单位变化等同于原始特征 $X_j$ 的一个标准差变化。标准化模型的 MEM 在均值处（其中当 $j \\ge 1$ 时 $\\bar{Z}_j=0$）评估为：\n$$\n\\text{MEM}^{\\text{std}}_j = \\phi(\\hat{\\beta}^{\\text{std}}_0)\\hat{\\beta}^{\\text{std}}_j\n$$\n由于 $\\eta_{\\text{mean}} = \\hat{\\beta}^{\\text{map}}_0 \\approx \\hat{\\beta}^{\\text{std}}_0$，我们可以按照指示在两个计算中一致地使用 $\\phi(\\eta_{\\text{mean}})$。任务是比较 $\\text{MEM}^{\\text{orig,1sd}}_j$ 与 $\\text{MEM}^{\\text{std}}_j = \\phi(\\eta_{\\text{mean}})\\hat{\\beta}^{\\text{std}}_j$。其等价性直接源于系数映射：\n$$\n\\text{MEM}^{\\text{orig,1sd}}_j = \\phi(\\eta_{\\text{mean}})\\hat{\\beta}_j\\sigma_j = \\phi(\\eta_{\\text{mean}})\\hat{\\beta}^{\\text{map}}_j \\approx \\phi(\\eta_{\\text{mean}})\\hat{\\beta}^{\\text{std}}_j = \\text{MEM}^{\\text{std}}_j\n$$\n因此，MEM 等价性误差 $\\max_{j \\ge 1} | \\text{MEM}^{\\text{std}}_j - \\text{MEM}^{\\text{orig,1sd}}_j |$ 应该非常小，这反映了系数映射的准确性。\n\n计算流程包括：\n1. 对每个测试用例，使用固定的随机种子从潜变量模型生成合成数据，以确保可复现性。\n2. 在原始数据上拟合 probit 模型以获得 $\\hat{\\beta}$。\n3. 计算特征均值 $\\mu_j$ 和标准差 $\\sigma_j$，并创建标准化特征矩阵 $Z$。\n4. 在标准化数据上拟合 probit 模型以获得 $\\hat{\\beta}^{\\text{std}}$。\n5. 根据 $\\hat{\\beta}$ 计算 $\\hat{\\beta}^{\\text{map}}$ 并计算系数映射误差。\n6. 计算边际效应 $\\text{MEM}^{\\text{orig,1sd}}$ 和 $\\text{MEM}^{\\text{std}}$ 并计算 MEM 等价性误差。\n这为理论关系提供了完整的验证。", "answer": "```python\nimport numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.stats import norm\n\ndef solve():\n    \"\"\"\n    Main function to run the analysis for all test cases.\n    \"\"\"\n\n    def fit_probit(X, y):\n        \"\"\"Fits a probit model using MLE.\"\"\"\n        \n        def neg_log_likelihood(beta, X, y):\n            \"\"\"Computes the negative log-likelihood for probit regression.\"\"\"\n            eta = X @ beta\n            # Use log-CDF and log-SF for numerical stability\n            log_phi = norm.logcdf(eta)\n            log_one_minus_phi = norm.logsf(eta)\n            \n            # log_phi can be -inf if eta is very small; log_one_minus_phi can be -inf if eta is very large.\n            # Replace -inf with a large negative number to prevent nan in sum.\n            log_phi[np.isneginf(log_phi)] = -1e9\n            log_one_minus_phi[np.isneginf(log_one_minus_phi)] = -1e9\n\n            log_L = np.sum(y * log_phi + (1 - y) * log_one_minus_phi)\n            return -log_L\n\n        def grad_neg_log_likelihood(beta, X, y):\n            \"\"\"Computes the gradient of the negative log-likelihood.\"\"\"\n            eta = X @ beta\n            pdf_val = norm.pdf(eta)\n            cdf_val = norm.cdf(eta)\n            \n            # Handle potential division by zero\n            # cdf_val can be 0 or 1 at the extremes of eta\n            # Hazard rate: lambda(eta) = pdf(eta) / cdf(eta)\n            # Reverse hazard rate: pdf(eta) / (1-cdf(eta))\n            # The limits are well-behaved, but numerically can be tricky\n            # Use a small epsilon to avoid 0 in denominator\n            epsilon = 1e-12\n            \n            term1 = y * (pdf_val / (cdf_val + epsilon))\n            term2 = (1 - y) * (pdf_val / (1 - cdf_val + epsilon))\n            \n            # The term inside the sum is (term1 - term2)\n            # The gradient for coeff beta_j is sum_i(X_ij * (term1_i - term2_i))\n            # which can be computed with a matrix-vector product\n            grad = X.T @ (term1 - term2)\n            return -grad\n\n        initial_guess = np.zeros(X.shape[1])\n        result = minimize(\n            fun=neg_log_likelihood,\n            x0=initial_guess,\n            args=(X, y),\n            method='BFGS',\n            jac=grad_neg_log_likelihood,\n            options={'gtol': 1e-7}\n        )\n        return result.x\n\n    def analyze_case(n, feature_specs, true_beta, rng):\n        \"\"\"Generates data, fits models, and computes errors for one case.\"\"\"\n        # 1. Generate data\n        p_features = len(feature_specs)\n        X_no_intercept = np.zeros((n, p_features))\n\n        for j, spec in enumerate(feature_specs):\n            dist_type, p1, p2 = spec\n            if dist_type == 'norm':\n                X_no_intercept[:, j] = rng.normal(loc=p1, scale=p2, size=n)\n            elif dist_type == 'uniform':\n                X_no_intercept[:, j] = rng.uniform(low=p1, high=p2, size=n)\n        \n        X_orig = np.c_[np.ones(n), X_no_intercept]\n        \n        epsilon = rng.normal(0, 1, n)\n        y_star = X_orig @ np.array(true_beta) + epsilon\n        y = (y_star  0).astype(int)\n\n        # 2. Fit model on original features\n        beta_hat_orig = fit_probit(X_orig, y)\n\n        # 3. Standardize features\n        mu = np.mean(X_no_intercept, axis=0)\n        sigma = np.std(X_no_intercept, axis=0)\n        X_std_no_intercept = (X_no_intercept - mu) / sigma\n        X_std = np.c_[np.ones(n), X_std_no_intercept]\n\n        # 4. Fit model on standardized features\n        beta_hat_std = fit_probit(X_std, y)\n\n        # 5. Compute reparameterization error\n        beta_hat_map = np.zeros_like(beta_hat_orig)\n        beta_hat_map[0] = beta_hat_orig[0] + np.dot(beta_hat_orig[1:], mu)\n        beta_hat_map[1:] = beta_hat_orig[1:] * sigma\n        mapping_error = np.max(np.abs(beta_hat_std - beta_hat_map))\n\n        # 6. Compute marginal effects error\n        eta_mean = beta_hat_orig[0] + np.dot(beta_hat_orig[1:], mu)\n        phi_eta_mean = norm.pdf(eta_mean)\n        \n        mem_orig_1sd = phi_eta_mean * beta_hat_orig[1:] * sigma\n        mem_std = phi_eta_mean * beta_hat_std[1:]\n        mem_error = np.max(np.abs(mem_std - mem_orig_1sd))\n        \n        return [round(mapping_error, 6), round(mem_error, 6)]\n\n    # Fixed random seed for reproducibility of the entire process\n    rng = np.random.default_rng(42)\n\n    test_cases = [\n        # Case A: n=1000, 1 feature N(2, 3^2), beta=(-0.5, 1.5)\n        (1000, [('norm', 2, 3)], [-0.5, 1.5]),\n        # Case B: n=2000, U[0,10], N(-1, 0.5^2), beta=(0.5, 0.2, -2.0)\n        (2000, [('uniform', 0, 10), ('norm', -1, 0.5)], [0.5, 0.2, -2.0]),\n        # Case C: n=1500, N(0,1), N(0,1), beta=(3.0, -0.5, 0.5)\n        (1500, [('norm', 0, 1), ('norm', 0, 1)], [3.0, -0.5, 0.5]),\n    ]\n\n    all_results = []\n    for n, specs, beta in test_cases:\n        case_results = analyze_case(n, specs, beta, rng)\n        all_results.append(case_results)\n\n    formatted_results = [f\"[{res[0]},{res[1]}]\" for res in all_results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3162301"}, {"introduction": "在实践中，我们常常希望模型的平均预测概率能够与数据中观测到的真实事件发生率（即样本流行率）保持一致，这被称为模型校准。在这个高级练习中，你将探索截距项 $\\beta_0$ 在模型校准中的关键作用。通过固定斜率系数并调整截距，使模型的平均预测与样本流行率相匹配，你将深入理解截距如何控制模型的“基准水平”，以及这种调整如何反过来影响其他系数的估计。[@problem_id:3162317]", "problem": "给定一个二元响应模型，其中每个观测由一个特征向量 $x_i \\in \\mathbb{R}^d$ 和一个二元标签 $y_i \\in \\{0,1\\}$ 描述。该模型是一个通过潜变量构造定义的 probit 回归：存在一个未观测到的潜变量 $z_i = \\beta_0 + x_i^\\top \\beta + \\epsilon_i$，其中 $\\epsilon_i \\sim \\mathcal{N}(0,1)$ 在各观测之间独立，观测到的标签为 $y_i = \\mathbf{1}\\{z_i \\ge 0\\}$。在此模型下，条件概率为 $\\mathbb{P}(y_i = 1 \\mid x_i) = \\Phi(\\beta_0 + x_i^\\top \\beta)$，其中 $\\Phi(\\cdot)$ 是标准正态累积分布函数 (CDF)。令 $\\phi(\\cdot)$ 表示标准正态概率密度函数 (PDF)。在独立同分布的观测下，参数向量 $(\\beta_0, \\beta)$ 的对数似然函数为\n$$\n\\ell(\\beta_0, \\beta) = \\sum_{i=1}^n \\left[ y_i \\log \\Phi\\left(\\beta_0 + x_i^\\top \\beta\\right) + (1 - y_i) \\log \\left(1 - \\Phi\\left(\\beta_0 + x_i^\\top \\beta\\right)\\right) \\right].\n$$\n以此为基础，您的程序必须为每个测试用例执行以下步骤：\n\n1) 通过在所有 $(\\beta_0, \\beta) \\in \\mathbb{R}^{1+d}$ 上最大化 $\\ell(\\beta_0, \\beta)$ 来估计无约束最大似然 (ML) 参数 $(\\hat{\\beta}_0, \\hat{\\beta})$。\n\n2) 定义观测流行率 $\\bar{y} = \\frac{1}{n}\\sum_{i=1}^n y_i$。保持斜率向量 $\\hat{\\beta}$ 固定，通过找到解方程\n$$\ng(\\beta_0) = \\frac{1}{n}\\sum_{i=1}^n \\Phi\\left(\\beta_0 + x_i^\\top \\hat{\\beta}\\right) - \\bar{y} = 0\n$$\n的 $\\tilde{\\beta}_0 \\in \\mathbb{R}$ 来调整截距。\n从第一性原理出发，论证为何 $g(\\beta_0)$ 是 $\\beta_0$ 的严格递增函数，并实现一个稳健的一维寻根算法以获得 $\\tilde{\\beta}_0$。\n\n3) 将截距固定为 $\\tilde{\\beta}_0$，通过在 $\\beta \\in \\mathbb{R}^d$ 上最大化对数似然 $\\ell(\\tilde{\\beta}_0, \\beta)$ 来重新估计斜率系数（即，将截距视为一个偏移量，仅优化非截距系数）。将重新拟合的斜率向量表示为 $\\hat{\\beta}^{\\text{fixed}}$。计算变化的欧几里得范数 $\\left\\|\\hat{\\beta} - \\hat{\\beta}^{\\text{fixed}}\\right\\|_2$。\n\n4) 对每个测试用例，计算并记录以下七个量（浮点数）：\n- $\\hat{\\beta}_0$ (无约束 ML 截距估计),\n- $\\tilde{\\beta}_0$ (在固定斜率下匹配观测流行率的调整后截距),\n- $\\bar{y}$ (观测流行率),\n- $\\bar{p}_{\\text{orig}} = \\frac{1}{n}\\sum_{i=1}^n \\Phi\\left(\\hat{\\beta}_0 + x_i^\\top \\hat{\\beta}\\right)$ (无约束 ML 拟合下的平均预测概率),\n- $\\bar{p}_{\\text{tuned-frozen}} = \\frac{1}{n}\\sum_{i=1}^n \\Phi\\left(\\tilde{\\beta}_0 + x_i^\\top \\hat{\\beta}\\right)$ (冻结斜率、仅调整截距后的平均预测概率),\n- $\\bar{p}_{\\text{refit}} = \\frac{1}{n}\\sum_{i=1}^n \\Phi\\left(\\tilde{\\beta}_0 + x_i^\\top \\hat{\\beta}^{\\text{fixed}}\\right)$ (固定截距、重新拟合斜率后的平均预测概率),\n- $\\Delta = \\left\\|\\hat{\\beta} - \\hat{\\beta}^{\\text{fixed}}\\right\\|_2$ (斜率变化的欧几里得范数)。\n\n测试套件。为覆盖典型和边缘场景，请使用以下三个从具有独立 $\\epsilon_i \\sim \\mathcal{N}(0,1)$ 的 probit 潜变量模型生成的合成测试用例：\n- Case A (均衡流行率): $n = 300$, $d = 2$, 真实参数 $(\\beta_0^\\star, \\beta^\\star) = (0.0, [0.8, -0.5])$, 特征 $x_i$ 从 $\\mathcal{N}(0,1)$ 独立地按坐标采样，随机种子为 $0$ 以保证可复现性。\n- Case B (罕见事件): $n = 500$, $d = 3$, 真实参数 $(\\beta_0^\\star, \\beta^\\star) = (-1.5, [1.2, 0.0, -0.7])$, 特征 $x_i$ 从 $\\mathcal{N}(0,1)$ 独立地按坐标采样，随机种子为 $1$。\n- Case C (高流行率): $n = 400$, $d = 2$, 真实参数 $(\\beta_0^\\star, \\beta^\\star) = (2.0, [-0.4, 0.6])$, 特征 $x_i$ 从 $\\mathcal{N}(0,1)$ 独立地按坐标采样，随机种子为 $2$。\n\n最终输出格式。您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，每个元素对应一个测试用例，并且本身是按上述顺序排列的七个浮点数的列表。例如，输出必须如下所示：\n$$\n\\left[ [a_1,a_2,a_3,a_4,a_5,a_6,a_7], [b_1,b_2,b_3,b_4,b_5,b_6,b_7], [c_1,c_2,c_3,c_4,c_5,c_6,c_7] \\right],\n$$\n所有条目都表示为十进制浮点数。本问题不涉及物理单位或角度单位。", "solution": "该问题要求对 probit 回归模型进行多步估计和分析。该过程涉及无约束最大似然估计、校准模型截距以匹配观测流行率，以及在校准后的截距保持固定时对斜率系数进行最终的重新拟合。解决方案将首先定义必要的数学对象，然后详细说明问题各部分的计算步骤。\n\nprobit 模型将具有特征 $x_i \\in \\mathbb{R}^d$ 的观测出现正向结果 ($y_i=1$) 的概率指定为：\n$$\n\\mathbb{P}(y_i = 1 \\mid x_i; \\beta_0, \\beta) = \\Phi(\\beta_0 + x_i^\\top \\beta)\n$$\n其中 $\\Phi(\\cdot)$ 是标准正态分布的累积分布函数 (CDF)，$\\theta = (\\beta_0, \\beta^\\top)^\\top$ 是 $(d+1)$ 维参数向量。$n$ 个独立观测 $(x_i, y_i)$ 样本的对数似然函数为：\n$$\n\\ell(\\beta_0, \\beta) = \\sum_{i=1}^n \\left[ y_i \\log \\Phi(\\eta_i) + (1 - y_i) \\log(1 - \\Phi(\\eta_i)) \\right]\n$$\n其中 $\\eta_i = \\beta_0 + x_i^\\top \\beta$。在数值上，将 $\\log(1 - \\Phi(\\eta_i))$ 写作 $\\log(\\Phi(-\\eta_i))$ 更为稳定。\n\n### 步骤 1：无约束最大似然估计\n\n第一个任务是通过最大化 $\\ell(\\beta_0, \\beta)$ 来找到最大似然 (ML) 估计 $(\\hat{\\beta}_0, \\hat{\\beta})$。这是一个数值优化问题。标准做法是最小化负对数似然，对于 probit 模型而言，它是一个凸函数，保证了唯一的全局最小值（前提是数据不是完全可分的）。令 $X$ 为 $n \\times (d+1)$ 的设计矩阵，其中第 $i$ 行为 $(1, x_i^\\top)$，令 $\\theta = (\\beta_0, \\beta^\\top)^\\top$ 为完整的参数向量。所有观测的线性预测器为 $\\eta = X\\theta$。负对数似然是：\n$$\n-\\ell(\\theta) = -\\sum_{i=1}^n \\left[ y_i \\log \\Phi(\\eta_i) + (1 - y_i) \\log \\Phi(-\\eta_i) \\right]\n$$\n为了使用基于梯度的优化器，我们需要 $-\\ell(\\theta)$ 的梯度。令 $\\phi(\\cdot)$ 为标准正态概率密度函数 (PDF)。关于参数向量 $\\theta$ 的梯度是：\n$$\n\\nabla_\\theta (-\\ell(\\theta)) = -\\sum_{i=1}^n \\left[ y_i \\frac{\\phi(\\eta_i)}{\\Phi(\\eta_i)} \\nabla_\\theta \\eta_i - (1-y_i) \\frac{\\phi(-\\eta_i)}{\\Phi(-\\eta_i)} \\nabla_\\theta \\eta_i \\right]\n$$\n由于 $\\nabla_\\theta \\eta_i = (1, x_i^\\top) = X_{i, \\cdot}^\\top$ 且 $\\phi(\\eta_i) = \\phi(-\\eta_i)$，这可以简化为：\n$$\n\\nabla_\\theta (-\\ell(\\theta)) = -\\sum_{i=1}^n X_{i, \\cdot}^\\top \\left( y_i \\frac{\\phi(\\eta_i)}{\\Phi(\\eta_i)} - (1-y_i) \\frac{\\phi(\\eta_i)}{1-\\Phi(\\eta_i)} \\right) = \\sum_{i=1}^n X_{i, \\cdot}^\\top \\left( (1-y_i) \\frac{\\phi(\\eta_i)}{1-\\Phi(\\eta_i)} - y_i \\frac{\\phi(\\eta_i)}{\\Phi(\\eta_i)} \\right)\n$$\n我们将使用一个拟牛顿优化算法，例如 BFGS，来最小化 $-\\ell(\\theta)$ 并找到 $\\hat{\\theta} = (\\hat{\\beta}_0, \\hat{\\beta}^\\top)^\\top$。\n\n### 步骤 2：截距调整与寻根\n\n第二步涉及校准模型的平均预测以匹配样本流行率 $\\bar{y} = \\frac{1}{n} \\sum_{i=1}^n y_i$。我们把斜率向量固定在 ML 估计值 $\\hat{\\beta}$ 上，并通过求解方程 $g(\\beta_0) = 0$ 来找到一个新的截距 $\\tilde{\\beta}_0$，其中：\n$$\ng(\\beta_0) = \\frac{1}{n}\\sum_{i=1}^n \\Phi\\left(\\beta_0 + x_i^\\top \\hat{\\beta}\\right) - \\bar{y}\n$$\n问题要求论证为何 $g(\\beta_0)$ 是严格递增的。我们可以通过检查它关于 $\\beta_0$ 的导数来证明这一点。使用链式法则，以及 $\\frac{d}{dz}\\Phi(z) = \\phi(z)$ 这一事实：\n$$\ng'(\\beta_0) = \\frac{d}{d\\beta_0} g(\\beta_0) = \\frac{1}{n}\\sum_{i=1}^n \\frac{d}{d\\beta_0} \\Phi\\left(\\beta_0 + x_i^\\top \\hat{\\beta}\\right) = \\frac{1}{n}\\sum_{i=1}^n \\phi\\left(\\beta_0 + x_i^\\top \\hat{\\beta}\\right)\n$$\n标准正态 PDF $\\phi(z) = \\frac{1}{\\sqrt{2\\pi}} e^{-z^2/2}$ 对所有实数 $z$ 都严格为正。由于求和中的每一项都严格为正，它们的和也严格为正。因此，对于所有 $\\beta_0 \\in \\mathbb{R}$，$g'(\\beta_0)  0$。这证明了 $g(\\beta_0)$ 是 $\\beta_0$ 的一个严格递增函数。\n\n这种单调性保证了如果根存在，它就是唯一的。对于 $\\bar{y} \\in (0,1)$，根保证存在，因为 $\\lim_{\\beta_0 \\to -\\infty} g(\\beta_0) = -\\bar{y}  0$ 且 $\\lim_{\\beta_0 \\to \\infty} g(\\beta_0) = 1-\\bar{y}  0$。可以稳健地应用一维寻根算法，如 Brent 方法，来找到 $\\tilde{\\beta}_0$。\n\n### 步骤 3：固定截距下的斜率重拟合\n\n在第三步中，我们将截距固定在校准值 $\\tilde{\\beta}_0$ 上，并通过最大化对数似然来重新估计斜率系数 $\\beta$。优化问题现在是：\n$$\n\\hat{\\beta}^{\\text{fixed}} = \\arg\\max_{\\beta \\in \\mathbb{R}^d} \\ell(\\tilde{\\beta}_0, \\beta)\n$$\n这等价于在 $d$ 个斜率系数上最小化负对数似然。目标函数是 $-\\ell(\\tilde{\\beta}_0, \\beta)$，优化过程与步骤 1 类似，但搜索空间被限制在 $\\beta \\in \\mathbb{R}^d$。线性预测器是 $\\eta_i = \\tilde{\\beta}_0 + x_i^\\top \\beta$。梯度仅相对于 $\\beta$ 计算，因此它是一个 $d$ 维向量。\n$$\n\\nabla_\\beta (-\\ell(\\tilde{\\beta}_0, \\beta)) = \\sum_{i=1}^n x_i \\left( (1-y_i) \\frac{\\phi(\\eta_i)}{1-\\Phi(\\eta_i)} - y_i \\frac{\\phi(\\eta_i)}{\\Phi(\\eta_i)} \\right)\n$$\n在找到 $\\hat{\\beta}^{\\text{fixed}}$ 后，我们计算斜率向量变化的欧几里得范数：$\\Delta = \\left\\|\\hat{\\beta} - \\hat{\\beta}^{\\text{fixed}}\\right\\|_2$。\n\n### 步骤 4：计算和报告量\n\n最后，对每个测试用例，我们收集从上述过程中得到的七个特定量：\n1.  $\\hat{\\beta}_0$：来自无约束 ML 拟合的截距。\n2.  $\\tilde{\\beta}_0$：来自寻根的调整后截距。\n3.  $\\bar{y}$：数据中的观测流行率。\n4.  $\\bar{p}_{\\text{orig}} = \\frac{1}{n}\\sum_{i=1}^n \\Phi\\left(\\hat{\\beta}_0 + x_i^\\top \\hat{\\beta}\\right)$：来自无约束模型的平均预测概率。从 MLE 的一阶条件可知，该值通常非常接近 $\\bar{y}$。\n5.  $\\bar{p}_{\\text{tuned-frozen}} = \\frac{1}{n}\\sum_{i=1}^n \\Phi\\left(\\tilde{\\beta}_0 + x_i^\\top \\hat{\\beta}\\right)$：调整截距后的平均预测概率。根据 $\\tilde{\\beta}_0$ 的构造，该值必须在数值公差范围内等于 $\\bar{y}$。\n6.  $\\bar{p}_{\\text{refit}} = \\frac{1}{n}\\sum_{i=1}^n \\Phi\\left(\\tilde{\\beta}_0 + x_i^\\top \\hat{\\beta}^{\\text{fixed}}\\right)$：使用固定的、调整后的截距重新拟合斜率后的平均预测概率。\n7.  $\\Delta = \\left\\|\\hat{\\beta} - \\hat{\\beta}^{\\text{fixed}}\\right\\|_2$：斜率向量变化的欧几里得范数。\n\n该实现将根据指定的参数和随机种子为每个测试用例生成数据，然后执行这四个步骤以产生所需的结果。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import norm\nfrom scipy.optimize import minimize, root_scalar\n\ndef generate_data(n, d, beta_star_full, seed):\n    \"\"\"Generates synthetic data for the probit model.\"\"\"\n    rng = np.random.default_rng(seed)\n    X = rng.normal(0, 1, size=(n, d))\n    eps = rng.normal(0, 1, size=n)\n    \n    beta0_star, beta_star = beta_star_full[0], beta_star_full[1:]\n    z = beta0_star + X @ beta_star + eps\n    y = (z = 0).astype(int)\n    return X, y\n\ndef process_single_case(n, d, true_params, seed):\n    \"\"\"\n    Executes the full analysis pipeline for a single test case.\n    \"\"\"\n    X, y = generate_data(n, d, np.array(true_params), seed)\n    d = X.shape[1]\n    X_aug = np.hstack([np.ones((n, 1)), X])\n\n    # --- Step 1: Unconstrained MLE ---\n    def neg_log_likelihood(theta, X, y):\n        eta = X @ theta\n        # Using norm.logcdf is numerically stable for log(Phi(z))\n        log_p = norm.logcdf(eta)\n        log_1_minus_p = norm.logcdf(-eta)\n        \n        # Handle cases where log P or log(1-P) is -inf\n        log_p[np.isneginf(log_p)] = -1e12 # large negative number\n        log_1_minus_p[np.isneginf(log_1_minus_p)] = -1e12\n\n        return -np.sum(y * log_p + (1 - y) * log_1_minus_p)\n\n    def grad_neg_log_likelihood(theta, X, y):\n        eta = X @ theta\n        # Use log-space calculations for numerical stability of phi/Phi\n        log_pdf_val = norm.logpdf(eta)\n        log_cdf_val = norm.logcdf(eta)\n        log_sf_val = norm.logcdf(-eta)\n        \n        ratio1 = np.exp(log_pdf_val - log_sf_val)\n        ratio2 = np.exp(log_pdf_val - log_cdf_val)\n\n        # Handle potential NaNs or Infs ifcdf or sf is 0\n        ratio1[np.isnan(ratio1) | np.isinf(ratio1)] = 0\n        ratio2[np.isnan(ratio2) | np.isinf(ratio2)] = 0\n\n        grad_term = (1 - y) * ratio1 - y * ratio2\n        return grad_term @ X\n\n    theta0 = np.zeros(d + 1)\n    res_unconstrained = minimize(\n        neg_log_likelihood,\n        theta0,\n        args=(X_aug, y),\n        jac=grad_neg_log_likelihood,\n        method='BFGS'\n    )\n    beta_hat_unconstrained = res_unconstrained.x\n    beta0_hat = beta_hat_unconstrained[0]\n    beta_hat = beta_hat_unconstrained[1:]\n\n    # --- Step 2: Intercept Tuning ---\n    y_bar = np.mean(y)\n\n    def g(beta0_scalar, X_slopes, beta_slopes, prevalence):\n        eta = beta0_scalar + X_slopes @ beta_slopes\n        mean_pred_prob = np.mean(norm.cdf(eta))\n        return mean_pred_prob - prevalence\n    \n    # root_scalar needs a bracket where signs of g differ\n    # g(-inf) = -y_bar  0, g(+inf) = 1-y_bar  0, so a bracket exists\n    # If y_bar is 0 or 1, the root is at +/- infinity. The problem context makes this very unlikely.\n    bracket_low, bracket_high = -20, 20\n    while g(bracket_low, X, beta_hat, y_bar) * g(bracket_high, X, beta_hat, y_bar)  0:\n        bracket_low *= 2\n        bracket_high *= 2\n\n    res_root = root_scalar(\n        g,\n        args=(X, beta_hat, y_bar),\n        method='brentq',\n        bracket=[bracket_low, bracket_high]\n    )\n    beta0_tilde = res_root.root\n\n    # --- Step 3: Refitting Slopes ---\n    def neg_log_likelihood_fixed_b0(beta_slopes, X_slopes, y, b0_fixed):\n        eta = b0_fixed + X_slopes @ beta_slopes\n        log_p = norm.logcdf(eta)\n        log_1_minus_p = norm.logcdf(-eta)\n        log_p[np.isneginf(log_p)] = -1e12\n        log_1_minus_p[np.isneginf(log_1_minus_p)] = -1e12\n        return -np.sum(y * log_p + (1 - y) * log_1_minus_p)\n\n    def grad_neg_log_likelihood_fixed_b0(beta_slopes, X_slopes, y, b0_fixed):\n        eta = b0_fixed + X_slopes @ beta_slopes\n        log_pdf_val = norm.logpdf(eta)\n        log_cdf_val = norm.logcdf(eta)\n        log_sf_val = norm.logcdf(-eta)\n        ratio1 = np.exp(log_pdf_val - log_sf_val)\n        ratio2 = np.exp(log_pdf_val - log_cdf_val)\n        ratio1[np.isnan(ratio1) | np.isinf(ratio1)] = 0\n        ratio2[np.isnan(ratio2) | np.isinf(ratio2)] = 0\n        grad_term = (1 - y) * ratio1 - y * ratio2\n        return grad_term @ X_slopes\n\n    beta_slopes_0 = beta_hat # Use previous slope estimate as initial guess\n    res_refit = minimize(\n        neg_log_likelihood_fixed_b0,\n        beta_slopes_0,\n        args=(X, y, beta0_tilde),\n        jac=grad_neg_log_likelihood_fixed_b0,\n        method='BFGS'\n    )\n    beta_hat_fixed = res_refit.x\n\n    # --- Step 4: Compute and Record Quantities ---\n    # a) beta0_hat is already computed\n    # b) beta0_tilde is already computed\n    # c) y_bar is already computed\n    \n    # d) Mean predicted probability under unconstrained ML fit\n    eta_orig = beta0_hat + X @ beta_hat\n    p_bar_orig = np.mean(norm.cdf(eta_orig))\n\n    # e) Mean predicted probability after tuning intercept\n    eta_tuned_frozen = beta0_tilde + X @ beta_hat\n    p_bar_tuned_frozen = np.mean(norm.cdf(eta_tuned_frozen))\n\n    # f) Mean predicted probability after refitting slopes\n    eta_refit = beta0_tilde + X @ beta_hat_fixed\n    p_bar_refit = np.mean(norm.cdf(eta_refit))\n\n    # g) Euclidean norm of slope changes\n    delta_beta = np.linalg.norm(beta_hat - beta_hat_fixed)\n\n    return [\n        beta0_hat, beta0_tilde, y_bar,\n        p_bar_orig, p_bar_tuned_frozen, p_bar_refit,\n        delta_beta\n    ]\n\ndef solve():\n    test_cases = [\n        {'n': 300, 'd': 2, 'true_params': [0.0, 0.8, -0.5], 'seed': 0},\n        {'n': 500, 'd': 3, 'true_params': [-1.5, 1.2, 0.0, -0.7], 'seed': 1},\n        {'n': 400, 'd': 2, 'true_params': [2.0, -0.4, 0.6], 'seed': 2}\n    ]\n\n    results = []\n    for case in test_cases:\n        case_results = process_single_case(case['n'], case['d'], case['true_params'], case['seed'])\n        results.append(case_results)\n\n    # Final print statement in the exact required format.\n    # str(list) automatically adds spaces after commas. We need to format it without spaces.\n    formatted_results = []\n    for res_list in results:\n        formatted_results.append(f\"[{','.join(f'{x:.8f}' for x in res_list)}]\")\n    \n    # Final format is a list of lists.\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3162317"}]}