{"hands_on_practices": [{"introduction": "拉普拉斯近似的核心思想是后验分布在众数附近可以用一个高斯分布来很好地近似。这个练习将带你通过一个理论推导，来亲手验证这一思想。我们将从一个接近高斯分布的贝叶斯模型出发，运用微扰理论计算非高斯项对后验众数的一阶修正，并分析被忽略的高阶项，从而深刻理解拉普拉斯近似在何种条件下能够达到极高的精度。 [@problem_id:3137251]", "problem": "考虑一个统计学习中的单参数贝叶斯模型，其中数据点 $y_{1}, y_{2}, \\dots, y_{n}$ 在给定参数 $\\theta$ 的情况下是条件独立同分布的，其高斯似然为 $p(y_{i} \\mid \\theta) = \\mathcal{N}(y_{i}; \\theta, \\sigma^{2})$，其中 $\\sigma^{2} > 0$ 是已知的。设 $\\theta$ 的先验是轻度非高斯的，其对数形式定义为\n$$\n\\ln p(\\theta) = -\\frac{\\theta^{2}}{2 \\tau^{2}} - \\epsilon \\, \\frac{\\theta^{4}}{4 \\tau^{4}} + C,\n$$\n其中 $\\tau^{2} > 0$ 是已知的，$0  \\epsilon \\ll 1$ 是一个小的无量纲参数，它将高斯先验扰动为更轻的尾部，而 $C$ 是一个归一化常数。后验分布为 $p(\\theta \\mid y_{1:n}) \\propto p(y_{1:n} \\mid \\theta) \\, p(\\theta)$。\n\n从贝叶斯法则和最大后验（MAP）估计量的定义出发，使用拉普拉斯近似，通过在基准高斯情况 $\\epsilon = 0$ 周围对对数后验进行泰勒展开，来刻画其众数附近的后验分布。通过求解对数后验的平稳性条件并按 $\\epsilon$ 的幂次展开，推导相对于高斯先验基准的最大后验估计量 $\\theta^{*}$ 的一阶（关于 $\\epsilon$）修正项。\n\n此外，利用泰勒展开来确定在基准众数处评估的拉普拉斯近似中第一个被忽略的项（涉及对数后验的三阶和四阶导数）在 $\\epsilon$ 中的尺度关系，并解释为什么当 $\\epsilon$ 很小且组合精度不趋于零时，这些项是可忽略的。\n\n你的最终答案应该是一个关于最大后验估计量 $\\theta^{*}$ 的单一闭式解析表达式，精确到 $\\epsilon$ 的一阶，用 $n$、$\\sigma^{2}$、$\\tau^{2}$、$\\bar{y} = \\frac{1}{n} \\sum_{i=1}^{n} y_{i}$ 和 $\\epsilon$ 表示。不需要进行数值四舍五入。", "solution": "我们从基本定义开始。在高斯采样模型下，似然函数为\n$$\np(y_{1:n} \\mid \\theta) = \\prod_{i=1}^{n} \\mathcal{N}(y_{i}; \\theta, \\sigma^{2}),\n$$\n其对数似然为\n$$\n\\ln p(y_{1:n} \\mid \\theta) = -\\frac{1}{2 \\sigma^{2}} \\sum_{i=1}^{n} (y_{i} - \\theta)^{2} + \\text{const} = -\\frac{n}{2 \\sigma^{2}} (\\theta - \\bar{y})^{2} + \\text{const},\n$$\n其中 $\\bar{y} = \\frac{1}{n} \\sum_{i=1}^{n} y_{i}$，我们省略不依赖于 $\\theta$ 的加性常数。先验通过以下方式指定\n$$\n\\ln p(\\theta) = -\\frac{\\theta^{2}}{2 \\tau^{2}} - \\epsilon \\, \\frac{\\theta^{4}}{4 \\tau^{4}} + C,\n$$\n在对 $\\theta$ 求导时，我们再次省略加性常数 $C$。\n\n令 $A = \\frac{n}{\\sigma^{2}}$ 表示似然精度，$B = \\frac{1}{\\tau^{2}}$ 表示先验精度。对数后验（不计加性常数）为\n$$\nf(\\theta) \\equiv \\ln p(y_{1:n} \\mid \\theta) + \\ln p(\\theta)\n= -\\frac{A}{2} (\\theta - \\bar{y})^{2} - \\frac{B}{2} \\theta^{2} - \\epsilon \\, \\frac{B^{2}}{4} \\theta^{4}.\n$$\n最大后验（MAP）估计量 $\\theta^{*}$ 定义为 $f(\\theta)$ 的最大化者，等价地，是满足 $f'(\\theta^{*}) = 0$ 且二阶导数为负（确保为最大值）的平稳点。\n\n计算一阶导数：\n$$\nf'(\\theta) = -A(\\theta - \\bar{y}) - B \\theta - \\epsilon \\, B^{2} \\theta^{3}.\n$$\n平稳性条件 $f'(\\theta^{*}) = 0$ 为\n$$\nA \\bar{y} - (A + B) \\theta^{*} - \\epsilon \\, B^{2} (\\theta^{*})^{3} = 0.\n$$\n我们通过展开，对小的 $\\epsilon$ 进行微扰求解\n$$\n\\theta^{*} = \\mu_{0} + \\epsilon \\, \\mu_{1} + \\mathcal{O}(\\epsilon^{2}),\n$$\n并代入平稳性方程。使用 $(\\theta^{*})^{3} = \\mu_{0}^{3} + 3 \\epsilon \\, \\mu_{0}^{2} \\mu_{1} + \\mathcal{O}(\\epsilon^{2})$，方程变为\n$$\nA \\bar{y} - (A + B) (\\mu_{0} + \\epsilon \\mu_{1}) - \\epsilon \\, B^{2} \\left( \\mu_{0}^{3} + \\mathcal{O}(\\epsilon) \\right) = 0.\n$$\n按 $\\epsilon$ 的幂次合并项，零阶方程为\n$$\nA \\bar{y} - (A + B) \\mu_{0} = 0 \\quad \\Rightarrow \\quad \\mu_{0} = \\frac{A}{A + B} \\, \\bar{y}.\n$$\n一阶方程为\n$$\n-(A + B) \\mu_{1} - B^{2} \\mu_{0}^{3} = 0 \\quad \\Rightarrow \\quad \\mu_{1} = -\\frac{B^{2}}{A + B} \\, \\mu_{0}^{3}.\n$$\n代入 $\\mu_{0} = \\frac{A}{A + B} \\bar{y}$ 以得到\n$$\n\\mu_{1} = -\\frac{B^{2}}{A + B} \\left( \\frac{A}{A + B} \\right)^{3} \\bar{y}^{3}\n= -\\frac{A^{3} B^{2}}{(A + B)^{4}} \\, \\bar{y}^{3}.\n$$\n因此，精确到 $\\epsilon$ 一阶的最大后验估计量为\n$$\n\\theta^{*} = \\frac{A}{A + B} \\, \\bar{y} \\; - \\; \\epsilon \\, \\frac{A^{3} B^{2}}{(A + B)^{4}} \\, \\bar{y}^{3} \\; + \\; \\mathcal{O}(\\epsilon^{2}).\n$$\n回到原始参数 $A = \\frac{n}{\\sigma^{2}}$ 和 $B = \\frac{1}{\\tau^{2}}$，我们有\n$$\n\\theta^{*} = \\frac{\\frac{n}{\\sigma^{2}}}{\\frac{n}{\\sigma^{2}} + \\frac{1}{\\tau^{2}}} \\, \\bar{y}\n\\; - \\; \\epsilon \\, \\frac{\\left( \\frac{n}{\\sigma^{2}} \\right)^{3} \\left( \\frac{1}{\\tau^{2}} \\right)^{2}}{\\left( \\frac{n}{\\sigma^{2}} + \\frac{1}{\\tau^{2}} \\right)^{4}} \\, \\bar{y}^{3}\n\\; + \\; \\mathcal{O}(\\epsilon^{2}).\n$$\n\n为了评估拉普拉斯近似中可忽略的修正项，我们考察在基准众数 $\\mu_{0}$ 附近 $f(\\theta)$ 的高阶导数。二阶、三阶和四阶导数分别为\n$$\nf''(\\theta) = - (A + B) - 3 \\epsilon \\, B^{2} \\theta^{2}, \\quad\nf^{(3)}(\\theta) = - 6 \\epsilon \\, B^{2} \\theta, \\quad\nf^{(4)}(\\theta) = - 6 \\epsilon \\, B^{2}.\n$$\n在 $\\theta = \\mu_{0}$ 处，Hessian 的大小为\n$$\nH_{0} \\equiv - f''(\\mu_{0}) = (A + B) + 3 \\epsilon \\, B^{2} \\mu_{0}^{2},\n$$\n其首项为 $A + B$，相对修正为 $\\frac{3 \\epsilon \\, B^{2} \\mu_{0}^{2}}{A + B} = \\mathcal{O}(\\epsilon)$。控制拉普拉斯误差项的无量纲非高斯性度量包括\n$$\nR_{3} \\equiv \\frac{| f^{(3)}(\\mu_{0}) |}{| f''(\\mu_{0}) |^{3/2}} \\approx \\frac{6 \\epsilon \\, B^{2} | \\mu_{0} |}{(A + B)^{3/2}},\n\\quad\nR_{4} \\equiv \\frac{| f^{(4)}(\\mu_{0}) |}{| f''(\\mu_{0}) |^{2}} \\approx \\frac{6 \\epsilon \\, B^{2}}{(A + B)^{2}}.\n$$\n当 $A + B$ 不趋于零时，$R_{3}$ 和 $R_{4}$ 均为 $\\epsilon$ 阶，这表明对于小的 $\\epsilon$，拉普拉斯近似中的高阶修正项是可忽略的。因此，上面推导的一阶 MAP 修正可靠地捕捉了轻度非高斯先验的影响，并且在这种情况下，围绕众数的拉普拉斯高斯近似仍然非常精确。\n\n所要求的最终表达式是精确到 $\\epsilon$ 一阶的最大后验估计量 $\\theta^{*}$。", "answer": "$$\\boxed{\\frac{\\frac{n}{\\sigma^{2}}}{\\frac{n}{\\sigma^{2}} + \\frac{1}{\\tau^{2}}} \\,\\bar{y} \\; - \\; \\epsilon \\, \\frac{\\left( \\frac{n}{\\sigma^{2}} \\right)^{3} \\left( \\frac{1}{\\tau^{2}} \\right)^{2}}{\\left( \\frac{n}{\\sigma^{2}} + \\frac{1}{\\tau^{2}} \\right)^{4}} \\, \\bar{y}^{3}}$$", "id": "3137251"}, {"introduction": "在处理多参数模型时，参数之间的相关性是不可忽视的重要特征。这个编程练习将向你展示拉普拉斯近似如何有效地捕捉这种相关性。你将构建一个后验分布存在强相关的贝叶斯线性模型，并看到拉普拉斯近似（在此例中为精确后验）如何通过其协方差矩阵来描述参数的椭圆形置信域，并将其与无法捕捉相关性的独立高斯变分近似进行对比。 [@problem_id:3137211]", "problem": "您必须编写一个完整、可运行的程序，该程序构建一个具有强相关的双参数贝叶斯后验，计算拉普拉斯近似的协方差和相关性，并通过评估 Kullback–Leibler 散度 (KL) 将其与独立高斯变分近似进行比较。请在纯数学环境下，使用以下生成模型作为基本基础进行全部工作。\n\n考虑一个具有两个参数的贝叶斯线性模型。对于给定的数据矩阵 $X \\in \\mathbb{R}^{n \\times 2}$ 和响应向量 $y \\in \\mathbb{R}^{n}$，假设似然是方差已知的高斯分布，先验是各向同性高斯分布：\n- 似然：$y \\mid \\theta \\sim \\mathcal{N}(X \\theta, \\sigma^{2} I)$，其中 $\\theta \\in \\mathbb{R}^{2}$，$I$ 是单位矩阵。\n- 先验：$\\theta \\sim \\mathcal{N}(0, \\tau^{2} I)$。\n\n您必须根据贝叶斯法则和多元正态分布的标准恒等式，推导出后验 $p(\\theta \\mid y)$ 的形式、其众数处的拉普拉斯近似的形式，以及最小化从 $q(\\theta)$ 到拉普拉斯近似的 Kullback–Leibler 散度 (KL) 的最佳独立高斯变分近似 $q(\\theta)$。除了这些基本事实和恒等式之外，您不得假定任何快捷公式。\n\n每个测试用例的数据生成应按以下方式进行：\n- 从标准正态分布中抽取独立同分布的 $x_{1} \\in \\mathbb{R}^{n}$。\n- 从标准正态分布中抽取独立同分布的 $z \\in \\mathbb{R}^{n}$，且与 $x_{1}$ 独立。\n- 构建 $x_{2} = \\rho x_{1} + \\sqrt{1 - \\rho^{2}} \\, z$ 以通过 $\\rho \\in (-1, 1)$ 控制共线性。\n- 构成 $X = [x_{1}, x_{2}] \\in \\mathbb{R}^{n \\times 2}$。\n- 固定真实参数向量 $\\theta_{\\text{true}} \\in \\mathbb{R}^{2}$，生成噪声 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2} I)$，并设置 $y = X \\theta_{\\text{true}} + \\varepsilon$。\n\n您的程序必须：\n- 在最大后验 (MAP) 点计算拉普拉斯近似。在此模型中，众数处的拉普拉斯协方差等于众数处后验精度（负 Hessian 矩阵）的逆。\n- 在拉普拉斯协方差下计算后验相关性，即拉普拉斯协方差所隐含的 $\\theta$ 的两个坐标之间的相关性。\n- 计算最小化从 $q(\\theta)$ 到拉普拉斯近似的 Kullback–Leibler 散度 (KL) 的最佳独立高斯变分近似 $q(\\theta) = \\mathcal{N}(\\mu, \\operatorname{diag}(s^{2}))$，然后评估此 KL。\n- 对每个测试用例，报告两个浮点数：拉普拉斯后验相关性和最佳独立高斯变分近似相对于拉普拉斯近似的 KL 值。\n\n此问题不涉及角度。不涉及物理单位；报告纯数字。所有最终数值输出应四舍五入到 $6$ 位小数。\n\n测试套件：\n使用以下测试用例，其中 $n$ 是样本数，$\\rho$ 控制特征共线性，$\\sigma$ 是噪声标准差，$\\tau$ 是先验标准差，最后一个元素是确保可复现性的随机种子。在所有情况下均使用固定的真实参数 $\\theta_{\\text{true}} = [1.5, -1.0]^{\\top}$。\n\n- 案例 $1$：$(n, \\rho, \\sigma, \\tau, \\text{seed}) = (200, 0.95, 0.5, 3.0, 0)$\n- 案例 $2$：$(n, \\rho, \\sigma, \\tau, \\text{seed}) = (200, 0.50, 0.5, 3.0, 1)$\n- 案例 $3$：$(n, \\rho, \\sigma, \\tau, \\text{seed}) = (200, 0.995, 0.5, 3.0, 2)$\n\n要求的最终输出格式：\n- 您的程序应生成单行输出，其中包含一个包含 $2 \\times 3 = 6$ 个浮点数的列表：对于上述顺序的每个测试用例，附加拉普拉斯后验相关性，然后是 KL 值。最终输出必须是单行，格式完全如下\n- 示例形状（非实际值）：$[c_{1},k_{1},c_{2},k_{2},c_{3},k_{3}]$\n- 每个值必须四舍五入到 $6$ 位小数。", "solution": "该问题是有效的，因为它具有科学依据、问题定义明确、客观且自成一体。我们将进行完整的推导和求解。\n\n### 1. 后验分布推导\n\n分析从使用贝叶斯法则寻找参数 $\\theta$ 的后验分布开始。后验与似然和先验的乘积成正比：\n$$\np(\\theta | y, X) \\propto p(y | \\theta, X) p(\\theta)\n$$\n似然和先验被给定为高斯分布：\n-   似然：$p(y | \\theta, X) = \\mathcal{N}(y | X\\theta, \\sigma^2 I) \\propto \\exp\\left(-\\frac{1}{2\\sigma^2}(y - X\\theta)^T(y - X\\theta)\\right)$\n-   先验：$p(\\theta) = \\mathcal{N}(\\theta | 0, \\tau^2 I) \\propto \\exp\\left(-\\frac{1}{2\\tau^2}\\theta^T\\theta\\right)$\n\n因此，后验分布的对数为：\n$$\n\\log p(\\theta | y, X) = -\\frac{1}{2\\sigma^2}(y - X\\theta)^T(y - X\\theta) - \\frac{1}{2\\tau^2}\\theta^T\\theta + C\n$$\n其中 $C$ 是一个与 $\\theta$ 无关的归一化常数。为了确定后验的形式，我们展开各项并按 $\\theta$ 的幂次分组：\n$$\n\\log p(\\theta | y, X) = -\\frac{1}{2\\sigma^2}(y^Ty - 2y^TX\\theta + \\theta^T X^T X \\theta) - \\frac{1}{2\\tau^2}\\theta^T\\theta + C\n$$\n$$\n\\log p(\\theta | y, X) = -\\frac{1}{2}\\left( \\theta^T \\left(\\frac{1}{\\sigma^2}X^TX + \\frac{1}{\\tau^2}I\\right) \\theta - 2\\theta^T \\left(\\frac{1}{\\sigma^2}X^Ty\\right) \\right) + C'\n$$\n这个表达式是 $\\theta$ 的二次型，这表明后验分布也是一个多元高斯分布，$p(\\theta | y, X) = \\mathcal{N}(\\theta | \\mu_p, \\Sigma_p)$。一个一般的多元高斯分布 $\\mathcal{N}(\\mu, \\Sigma)$ 的对数密度为 $-\\frac{1}{2}(\\theta-\\mu)^T\\Sigma^{-1}(\\theta-\\mu) + \\text{const} = -\\frac{1}{2}(\\theta^T\\Sigma^{-1}\\theta - 2\\theta^T\\Sigma^{-1}\\mu + \\text{const})$。\n\n通过将对数后验中的项与一般形式进行比较，我们可以确定后验精度矩阵 $\\Lambda_p = \\Sigma_p^{-1}$ 和后验均值 $\\mu_p$：\n$$\n\\Lambda_p = \\Sigma_p^{-1} = \\frac{1}{\\sigma^2}X^TX + \\frac{1}{\\tau^2}I\n$$\n$$\n\\Lambda_p \\mu_p = \\frac{1}{\\sigma^2}X^Ty \\implies \\mu_p = \\Lambda_p^{-1}\\left(\\frac{1}{\\sigma^2}X^Ty\\right) = \\left(\\frac{1}{\\sigma^2}X^TX + \\frac{1}{\\tau^2}I\\right)^{-1}\\left(\\frac{1}{\\sigma^2}X^Ty\\right)\n$$\n因此，后验是 $p(\\theta | y, X) = \\mathcal{N}(\\theta | \\mu_p, \\Lambda_p^{-1})$。\n\n### 2. 拉普拉斯近似和后验相关性\n\n拉普拉斯近似为后验分布提供了一个高斯近似，其中心位于后验的众数（最大后验估计或 MAP 估计，$\\theta_{\\text{MAP}}$）。此高斯分布的协方差由在众数处评估的对数后验的 Hessian 矩阵的负逆给出。\n\n首先，我们通过将对数后验关于 $\\theta$ 的梯度设置为零来找到众数：\n$$\n\\nabla_\\theta \\log p(\\theta | y, X) = \\frac{1}{\\sigma^2}(X^Ty - X^TX\\theta) - \\frac{1}{\\tau^2}\\theta = 0\n$$\n求解 $\\theta$ 得到 $\\theta_{\\text{MAP}} = \\mu_p$。由于后验是精确的高斯分布，其众数等于其均值。\n\n接下来，我们计算 Hessian 矩阵（二阶导数矩阵）：\n$$\n\\nabla_\\theta^2 \\log p(\\theta | y, X) = -\\frac{1}{\\sigma^2}X^TX - \\frac{1}{\\tau^2}I = -\\Lambda_p\n$$\nHessian 矩阵是常数，不依赖于 $\\theta$。拉普拉斯近似的协方差 $\\Sigma_L$ 是：\n$$\n\\Sigma_L = \\left(-\\nabla_\\theta^2 \\log p(\\theta | y, X)\\Big|_{\\theta_{\\text{MAP}}}\\right)^{-1} = (\\Lambda_p)^{-1} = \\Sigma_p\n$$\n对于这个特定模型，拉普拉斯近似不是一个近似；它是精确的后验分布，$q_L(\\theta) = p(\\theta | y, X)$。所要求的后验相关性是根据这个精确的协方差矩阵 $\\Sigma_L = \\Sigma_p$ 计算的。对于一个 $2 \\times 2$ 的协方差矩阵 $\\Sigma_L = \\begin{pmatrix} \\Sigma_{11}  \\Sigma_{12} \\\\ \\Sigma_{21}  \\Sigma_{22} \\end{pmatrix}$，相关性是：\n$$\n\\rho_{12} = \\frac{\\Sigma_{12}}{\\sqrt{\\Sigma_{11}\\Sigma_{22}}}\n$$\n\n### 3. 变分近似和 KL 散度\n\n我们寻求最佳的独立高斯变分近似 $q(\\theta) = q_1(\\theta_1)q_2(\\theta_2) = \\mathcal{N}(\\theta | \\mu_q, \\Sigma_q)$，其中 $\\Sigma_q$ 是一个对角矩阵。目标是最小化从 $q(\\theta)$ 到真实后验 $p(\\theta|y,X)$ 的 Kullback-Leibler (KL) 散度，在本例中这等同于拉普拉斯近似 $q_L(\\theta)$。目标是最小化 $\\text{KL}(q || q_L)$。\n\n两个多元高斯分布 $q=\\mathcal{N}(\\mu_q, \\Sigma_q)$ 和 $p=\\mathcal{N}(\\mu_p, \\Sigma_p)$ 之间的 KL 散度的一般公式是：\n$$\n\\text{KL}(q || p) = \\frac{1}{2} \\left[ \\log\\frac{|\\Sigma_p|}{|\\Sigma_q|} - d + \\text{tr}(\\Sigma_p^{-1}\\Sigma_q) + (\\mu_p - \\mu_q)^T\\Sigma_p^{-1}(\\mu_p - \\mu_q) \\right]\n$$\n其中 $d$ 是维度（这里 $d=2$）。当 $\\mu_q = \\mu_p$ 时，涉及均值的项在零处最小化。\n\n对于协方差，对于高斯目标 $p(\\theta) = \\mathcal{N}(\\mu_p, \\Sigma_p)$，平均场变分推断的标准结果是，最优的因子化分布 $q(\\theta) = \\mathcal{N}(\\mu_q, \\Sigma_q)$（其中 $\\Sigma_q$ 是对角阵）具有 $\\mu_q = \\mu_p$ 且其精度矩阵 $\\Lambda_q = \\Sigma_q^{-1}$ 等于目标精度矩阵的对角线，即 $\\Lambda_q = \\text{diag}(\\Lambda_p)$。也就是说，$\\Sigma_q = (\\text{diag}(\\Lambda_p))^{-1}$。\n\n当 $\\mu_q = \\mu_p$ 时，KL 散度简化为：\n$$\n\\text{KL}(q || p) = \\frac{1}{2} \\left[ \\log\\frac{|\\Sigma_p|}{|\\Sigma_q|} - 2 + \\text{tr}(\\Sigma_p^{-1}\\Sigma_q) \\right]\n$$\n我们来评估这些项。设 $\\Lambda_p = \\Sigma_p^{-1}$。变分精度为 $\\Lambda_q = \\text{diag}(\\Lambda_{p,11}, \\Lambda_{p,22})$。迹项变为：\n$$\n\\text{tr}(\\Sigma_p^{-1}\\Sigma_q) = \\text{tr}(\\Lambda_p \\Lambda_q^{-1}) = \\text{tr}\\left( \\begin{pmatrix} \\Lambda_{p,11}  \\Lambda_{p,12} \\\\ \\Lambda_{p,21}  \\Lambda_{p,22} \\end{pmatrix} \\begin{pmatrix} 1/\\Lambda_{p,11}  0 \\\\ 0  1/\\Lambda_{p,22} \\end{pmatrix} \\right) = \\text{tr}\\begin{pmatrix} 1  \\dots \\\\ \\dots  1 \\end{pmatrix} = 2\n$$\n对数行列式比率为：\n$$\n\\frac{|\\Sigma_p|}{|\\Sigma_q|} = \\frac{|\\Lambda_q|}{|\\Lambda_p|} = \\frac{\\Lambda_{p,11}\\Lambda_{p,22}}{\\Lambda_{p,11}\\Lambda_{p,22} - \\Lambda_{p,12}^2} = \\frac{1}{1 - \\frac{\\Lambda_{p,12}^2}{\\Lambda_{p,11}\\Lambda_{p,22}}}\n$$\n从协方差矩阵 $\\Sigma_p$ 计算的相关性为 $\\rho_{12}$。从精度矩阵 $\\Lambda_p$ 计算的相关性为 $\\rho_{\\Lambda,12} = \\frac{\\Lambda_{p,12}}{\\sqrt{\\Lambda_{p,11}\\Lambda_{p,22}}}$。对于一个 $2 \\times 2$ 矩阵，$\\rho_{12} = -\\rho_{\\Lambda,12}$ 成立。因此，$\\rho_{12}^2 = \\rho_{\\Lambda,12}^2$。\n$$\n\\frac{|\\Sigma_p|}{|\\Sigma_q|} = \\frac{1}{1 - \\rho_{12}^2}\n$$\n将这些代入 KL 公式，得到一个非常简单的结果：\n$$\n\\text{KL}(q || p) = \\frac{1}{2} \\left[ \\log\\left(\\frac{1}{1 - \\rho_{12}^2}\\right) - 2 + 2 \\right] = -\\frac{1}{2}\\log(1 - \\rho_{12}^2)\n$$\nKL 散度，它衡量了因子化近似的不足之处，仅取决于参数的后验相关性的平方。\n\n### 4. 算法\n\n对于每个测试用例：\n1.  设置随机种子。根据指定的过程，使用参数 $n$ 和 $\\rho$ 生成具有相关列的数据矩阵 $X \\in \\mathbb{R}^{n \\times 2}$。\n2.  计算后验精度矩阵 $\\Lambda_p = \\frac{1}{\\sigma^2}X^T X + \\frac{1}{\\tau^2}I$。\n3.  对 $\\Lambda_p$ 求逆以找到后验协方差矩阵 $\\Sigma_p = \\Sigma_L = \\Lambda_p^{-1}$。\n4.  从 $\\Sigma_L$ 中提取元素 $\\Sigma_{11}$、$\\Sigma_{22}$ 和 $\\Sigma_{12}$。\n5.  计算后验相关性 $\\rho_{12} = \\Sigma_{12} / \\sqrt{\\Sigma_{11}\\Sigma_{22}}$。\n6.  计算最佳独立高斯变分近似的 KL 散度为 $\\text{KL} = -0.5 \\log(1 - \\rho_{12}^2)$。\n7.  将两个结果四舍五入到 6 位小数并存储它们。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem for the given test cases.\n    For a Bayesian linear model with Gaussian likelihood and prior, the posterior\n    is also Gaussian. This function computes its properties.\n    \"\"\"\n    \n    # Test cases: (n, rho, sigma, tau, seed)\n    test_cases = [\n        (200, 0.95, 0.5, 3.0, 0),\n        (200, 0.50, 0.5, 3.0, 1),\n        (200, 0.995, 0.5, 3.0, 2),\n    ]\n\n    # Fixed true parameter vector for data generation\n    theta_true = np.array([1.5, -1.0])\n\n    results = []\n    for case in test_cases:\n        n, rho, sigma, tau, seed = case\n\n        # 1. Data Generation\n        # Set a random number generator with a seed for reproducibility.\n        rng = np.random.default_rng(seed)\n\n        # Generate x1 and z from standard normal distributions.\n        x1 = rng.standard_normal(n)\n        z = rng.standard_normal(n)\n        \n        # Construct x2 to have a specified correlation rho with x1.\n        x2 = rho * x1 + np.sqrt(1 - rho**2) * z\n        \n        # Form the data matrix X.\n        X = np.stack([x1, x2], axis=1) # Shape: (n, 2)\n        \n        # Generate response vector y. Note: y is not needed for the posterior\n        # covariance, correlation, or the KL divergence, as these depend only\n        # on X, sigma, and tau in this model.\n        # eps = rng.normal(0, sigma, n)\n        # y = X @ theta_true + eps\n\n        # 2. Compute Posterior Precision and Covariance (Laplace Approximation)\n        # The posterior precision matrix is Lambda_p = (1/sigma^2) * X'X + (1/tau^2) * I\n        XtX = X.T @ X\n        lambda_p = (1 / sigma**2) * XtX + (1 / tau**2) * np.eye(2)\n        \n        # The posterior covariance matrix is the inverse of the precision matrix.\n        # This is also the covariance of the Laplace approximation.\n        sigma_l = np.linalg.inv(lambda_p)\n\n        # 3. Compute Posterior Correlation\n        # Extract elements of the covariance matrix.\n        sigma_11 = sigma_l[0, 0]\n        sigma_22 = sigma_l[1, 1]\n        sigma_12 = sigma_l[0, 1]\n        \n        # Calculate the correlation coefficient.\n        correlation = sigma_12 / np.sqrt(sigma_11 * sigma_22)\n\n        # 4. Compute KL Divergence\n        # The KL divergence for the best independent Gaussian variational approximation\n        # has a simple closed form related to the posterior correlation.\n        kl_divergence = -0.5 * np.log(1 - correlation**2)\n\n        # Append rounded results to the list.\n        results.append(round(correlation, 6))\n        results.append(round(kl_divergence, 6))\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```", "id": "3137211"}, {"introduction": "尽管拉普拉斯近似非常强大，但它并非万能。当后验分布呈现明显偏态时，其对称的高斯假设可能会导致错误的推断。这个练习将通过一个经典的泊松-伽马共轭模型，让你亲手构建一个偏态后验分布，并展示以众数为中心的拉普拉斯近似如何在其均值估计上产生偏差，进而影响对期望损失等后验量的评估。 [@problem_id:3137250]", "problem": "考虑以下贝叶斯单参数模型，其中独立观测值来自泊松分布，先验为伽马分布。设 $\\{y_i\\}_{i=1}^n$ 是来自 $\\text{Poisson}(\\lambda)$ 的独立抽样，其中率 $\\lambda  0$ 未知，并使用形状参数为 $a  0$、率参数为 $b  0$ 的伽马先验 $\\lambda \\sim \\text{Gamma}(a,b)$。使用贝叶斯定理以及泊松似然和伽马先验的定义作为基本依据。最大后验 (MAP) 估计量是后验众数。拉普拉斯近似通过一个以众数为中心的高斯分布来近似最大后验（MAP）附近的后验密度，其方差等于在众数处对数后验的负二阶导数（对于标量即负海森值）的倒数。\n\n你的任务：\n- 推导后验密度（无需计算归一化常数），并将其表示为一种公认的参数形式。\n- 通过对对数后验求导，推导后验均值和后验众数。\n- 使用以以后验众数为中心的拉普拉斯近似，从众数处的对数后验的二阶导数推导出标量高斯方差。\n- 对于由平方损失 $L(\\lambda; t) = (\\lambda - t)^2$ 给出的凸风险，计算在精确后验和拉普拉斯近似下的期望损失。仅使用这些分布的期望定义以及你推导出的量。\n- 实现一个程序，对于下面测试套件中的每个测试用例，计算精确期望损失与拉普拉斯近似期望损失之间的绝对差。\n\n测试套件：\n- 用例 1：先验 $(a,b) = (2.5, 1.0)$，数据 $y = (0,1,2)$，损失目标 $t$ 等于此用例的后验众数。\n- 用例 2：先验 $(a,b) = (0.3, 1.0)$，数据 $y = (1)$，损失目标 $t$ 等于此用例数据的样本均值。\n- 用例 3：先验 $(a,b) = (2.0, 1.0)$，数据 $y = (10,12,9,11,8)$，损失目标 $t = 10.0$。\n- 用例 4：先验 $(a,b) = (3.0, 0.5)$，数据 $y = (0,0,0,1)$，损失目标 $t = 0.8$。\n\n程序要求：\n- 对每个用例，计算后验形状参数 $\\alpha$ 和率参数 $\\beta$、后验均值和众数、众数处的拉普拉斯方差，以及在精确后验和拉普拉斯近似下的期望平方损失。\n- 对每个用例，输出精确期望损失与拉普拉斯近似期望损失之间的绝对差，结果为浮点数。\n- 你的程序应生成单行输出，包含一个用方括号括起来的逗号分隔列表，其中每个数字四舍五入到六位小数（例如，$[0.123456,0.000001,2.718282,1.000000]$）。\n\n本问题陈述中的所有数学符号、变量、函数、运算符和数字都使用 LaTeX 书写。本问题不涉及物理单位、角度或百分比；所有数值输出必须是不带单位的实数。", "solution": "该问题要求对一个涉及泊松似然和伽马先验的贝叶斯单参数模型进行深入分析。此框架是贝叶斯统计中共轭族的一个经典例子。我们将首先验证问题陈述的合理性（经确认为合理），然后系统地推导所需的量。\n\n模型规定如下：\n- 数据 $\\{y_i\\}_{i=1}^n$ 是来自参数为 $\\lambda$ 的泊松分布的独立同分布抽样：$y_i \\mid \\lambda \\sim \\text{Poisson}(\\lambda)$。\n- 未知参数 $\\lambda$ 的先验分布是形状参数为 $a$、率参数为 $b$ 的伽马分布：$\\lambda \\sim \\text{Gamma}(a, b)$。\n\n泊松分布的概率质量函数 (PMF) 为 $P(y \\mid \\lambda) = \\frac{e^{-\\lambda}\\lambda^y}{y!}$，其中 $y \\in \\{0, 1, 2, \\dots\\}$。伽马分布的概率密度函数 (PDF) 为 $p(\\lambda \\mid a, b) = \\frac{b^a}{\\Gamma(a)}\\lambda^{a-1}e^{-b\\lambda}$，其中 $\\lambda  0$。\n\n首先，我们推导 $\\lambda$ 的后验密度。根据贝叶斯定理，后验密度与似然和先验密度的乘积成正比：\n$$ p(\\lambda \\mid y_1, \\dots, y_n) \\propto P(y_1, \\dots, y_n \\mid \\lambda) \\, p(\\lambda) $$\n鉴于观测值是独立的，似然是各个 PMF 的乘积：\n$$ P(y_1, \\dots, y_n \\mid \\lambda) = \\prod_{i=1}^n \\frac{e^{-\\lambda}\\lambda^{y_i}}{y_i!} = \\frac{e^{-n\\lambda} \\lambda^{\\sum_{i=1}^n y_i}}{\\prod_{i=1}^n y_i!} $$\n作为 $\\lambda$ 的函数，似然与 $e^{-n\\lambda} \\lambda^{\\sum y_i}$ 成正比。先验密度与 $\\lambda^{a-1}e^{-b\\lambda}$ 成正比。\n结合这两者，后验密度为：\n$$ p(\\lambda \\mid \\mathbf{y}) \\propto \\left(e^{-n\\lambda} \\lambda^{\\sum_{i=1}^n y_i}\\right) \\left(\\lambda^{a-1}e^{-b\\lambda}\\right) $$\n$$ p(\\lambda \\mid \\mathbf{y}) \\propto \\lambda^{(\\sum y_i + a) - 1} e^{-(n+b)\\lambda} $$\n这个表达式是伽马分布的核。因此，后验分布也是伽马分布，这证明了伽马先验对于泊松似然的共轭性。后验分布为 $\\lambda \\mid \\mathbf{y} \\sim \\text{Gamma}(\\alpha, \\beta)$，更新后的参数为：\n- 后验形状参数：$\\alpha = \\sum_{i=1}^n y_i + a$\n- 后验率参数：$\\beta = n + b$\n\n第二，我们推导后验均值和后验众数。对于伽马分布 $\\text{Gamma}(\\alpha, \\beta)$，其均值和众数是众所周知的量。\n- 后验均值为 $E[\\lambda \\mid \\mathbf{y}] = \\frac{\\alpha}{\\beta} = \\frac{\\sum y_i + a}{n + b}$。\n- 后验众数（最大后验估计，$\\lambda_{\\text{MAP}}$）通过最大化后验密度得到。对于形状参数 $\\alpha  1$ 的伽马分布，众数为 $\\lambda_{\\text{MAP}} = \\frac{\\alpha - 1}{\\beta} = \\frac{\\sum y_i + a - 1}{n + b}$。问题中提供的所有测试用例都满足条件 $\\alpha  1$。\n\n第三，我们推导拉普拉斯近似的方差。拉普拉斯近似使用一个以以后验众数 $\\lambda_{\\text{MAP}}$ 为中心的高斯（正态）分布来近似后验密度。其方差我们记为 $\\sigma_L^2$，是在众数处求值的对数后验密度的负二阶导数的倒数。\n对数后验（忽略一个加法常数）为：\n$$ \\log p(\\lambda \\mid \\mathbf{y}) = (\\alpha-1)\\log\\lambda - \\beta\\lambda + C $$\n关于 $\\lambda$ 的一阶导数为：\n$$ \\frac{d}{d\\lambda} \\log p(\\lambda \\mid \\mathbf{y}) = \\frac{\\alpha-1}{\\lambda} - \\beta $$\n二阶导数为：\n$$ \\frac{d^2}{d\\lambda^2} \\log p(\\lambda \\mid \\mathbf{y}) = -\\frac{\\alpha-1}{\\lambda^2} $$\n在众数 $\\lambda_{\\text{MAP}} = \\frac{\\alpha - 1}{\\beta}$ 处计算负二阶导数（即观测费雪信息）：\n$$ J(\\lambda_{\\text{MAP}}) = -\\left(-\\frac{\\alpha-1}{(\\frac{\\alpha-1}{\\beta})^2}\\right) = \\frac{\\alpha-1}{(\\alpha-1)^2 / \\beta^2} = \\frac{\\beta^2}{\\alpha-1} $$\n拉普拉斯近似的方差是这个量的倒数：\n$$ \\sigma_L^2 = [J(\\lambda_{\\text{MAP}})]^{-1} = \\frac{\\alpha-1}{\\beta^2} $$\n因此，后验的拉普拉斯近似为 $\\lambda_{\\text{approx}} \\sim N(\\mu_L, \\sigma_L^2)$，其中均值为 $\\mu_L = \\lambda_{\\text{MAP}} = \\frac{\\alpha-1}{\\beta}$，方差为 $\\sigma_L^2 = \\frac{\\alpha-1}{\\beta^2}$。\n\n第四，我们计算期望平方损失 $L(\\lambda; t) = (\\lambda - t)^2$。在 $\\lambda$ 的某一概率分布下，期望损失由 $E[(\\lambda-t)^2]$ 给出。这可以使用方差的定义 $\\text{Var}(\\lambda) = E[\\lambda^2] - (E[\\lambda])^2$ 展开：\n$$ E[(\\lambda-t)^2] = E[\\lambda^2 - 2t\\lambda + t^2] = E[\\lambda^2] - 2tE[\\lambda] + t^2 $$\n$$ E[(\\lambda-t)^2] = (\\text{Var}(\\lambda) + (E[\\lambda])^2) - 2tE[\\lambda] + t^2 = \\text{Var}(\\lambda) + (E[\\lambda] - t)^2 $$\n这个公式将期望损失与方差以及分布均值相对于目标 $t$ 的偏差平方联系起来。\n\n我们将此公式应用于精确后验和拉普拉斯近似。\n- 对于精确后验 $\\lambda \\mid \\mathbf{y} \\sim \\text{Gamma}(\\alpha, \\beta)$：\n  - 均值：$E_{\\text{post}}[\\lambda] = \\frac{\\alpha}{\\beta}$\n  - 方差：$\\text{Var}_{\\text{post}}(\\lambda) = \\frac{\\alpha}{\\beta^2}$\n  - 期望损失：$E_{\\text{exact}} = \\text{Var}_{\\text{post}}(\\lambda) + (E_{\\text{post}}[\\lambda] - t)^2 = \\frac{\\alpha}{\\beta^2} + \\left(\\frac{\\alpha}{\\beta} - t\\right)^2$。\n- 对于拉普拉斯近似 $\\lambda_{\\text{approx}} \\sim N(\\mu_L, \\sigma_L^2)$：\n  - 均值：$E_{\\text{Laplace}}[\\lambda] = \\mu_L = \\frac{\\alpha-1}{\\beta}$\n  - 方差：$\\text{Var}_{\\text{Laplace}}(\\lambda) = \\sigma_L^2 = \\frac{\\alpha-1}{\\beta^2}$\n  - 期望损失：$E_{\\text{Laplace}} = \\text{Var}_{\\text{Laplace}}(\\lambda) + (E_{\\text{Laplace}}[\\lambda] - t)^2 = \\frac{\\alpha-1}{\\beta^2} + \\left(\\frac{\\alpha-1}{\\beta} - t\\right)^2$。\n\n程序将实现这些最终公式，为每个测试用例计算绝对差 $|E_{\\text{exact}} - E_{\\text{Laplace}}|$。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem for the given test suite by calculating the absolute difference\n    between the expected squared loss under the exact posterior and under the Laplace approximation.\n    \"\"\"\n    \n    # Test suite definition: (prior_a, prior_b, data_y, t_config)\n    # t_config is a tuple (type, value) where type is 'mode', 'mean', or 'value'.\n    test_cases = [\n        (2.5, 1.0, [0, 1, 2], ('mode', None)),\n        (0.3, 1.0, [1], ('mean', None)),\n        (2.0, 1.0, [10, 12, 9, 11, 8], ('value', 10.0)),\n        (3.0, 0.5, [0, 0, 0, 1], ('value', 0.8)),\n    ]\n\n    results = []\n\n    for case in test_cases:\n        a_prior, b_prior, y, t_config = case\n        \n        # Convert y to a numpy array for easier calculations\n        y = np.array(y)\n        \n        # Calculate sufficient statistics from data\n        n = len(y)\n        sum_y = np.sum(y)\n        \n        # Calculate posterior parameters\n        # Posterior is Gamma(alpha, beta)\n        alpha_post = sum_y + a_prior\n        beta_post = float(n + b_prior)\n\n        # Ensure posterior mode is well-defined (alpha > 1)\n        if alpha_post = 1:\n            # This case is not expected based on problem validation\n            # but good practice to handle.\n            results.append(np.nan)\n            continue\n\n        # Determine the loss target t based on the configuration\n        t_type, t_val = t_config\n        t = 0.0\n        if t_type == 'value':\n            t = t_val\n        elif t_type == 'mode':\n            # Posterior mode (MAP)\n            t = (alpha_post - 1) / beta_post\n        elif t_type == 'mean':\n            # Sample mean of the data\n            t = np.mean(y)\n\n        # === Calculations for the exact posterior: Gamma(alpha_post, beta_post) ===\n        \n        # Mean of the exact posterior\n        mean_exact = alpha_post / beta_post\n        # Variance of the exact posterior\n        var_exact = alpha_post / (beta_post**2)\n        # Expected squared loss for the exact posterior\n        expected_loss_exact = var_exact + (mean_exact - t)**2\n\n        # === Calculations for the Laplace approximation: Normal(mu_L, sigma_L^2) ===\n        \n        # Mean of the Laplace approximation is the posterior mode\n        mean_laplace = (alpha_post - 1) / beta_post\n        # Variance of the Laplace approximation\n        var_laplace = (alpha_post - 1) / (beta_post**2)\n        # Expected squared loss for the Laplace approximation\n        expected_loss_laplace = var_laplace + (mean_laplace - t)**2\n\n        # Calculate the absolute difference between the two expected losses\n        abs_diff = abs(expected_loss_exact - expected_loss_laplace)\n        results.append(abs_diff)\n    \n    # Format the output as a comma-separated list of strings with 6 decimal places\n    output_str = \",\".join([f\"{res:.6f}\" for res in results])\n    print(f\"[{output_str}]\")\n\nsolve()\n```", "id": "3137250"}]}