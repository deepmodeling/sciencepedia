{"hands_on_practices": [{"introduction": "为了在再生核希尔伯特空间 (RKHS) 中自如地操作，我们必须首先掌握其最基本的运算法则。这个练习将引导你运用再生核的关键性质——即通过核函数计算内积——来计算空间中一个特定函数的范数 [@problem_id:1033834]。这不仅是对定义的直接应用，更是理解函数如何作为 RKHS 中“向量”的第一步。", "problem": "设 $\\mathcal{H}$ 是一个定义在 $X = \\mathbb{R}$ 上的实值函数的再生核希尔伯特空间 (RKHS)。该空间 $\\mathcal{H}$ 由一个正定核 $K: \\mathbb{R} \\times \\mathbb{R} \\to \\mathbb{R}$ 定义。$\\mathcal{H}$ 中的内积记为 $\\langle \\cdot, \\cdot \\rangle_{\\mathcal{H}}$，其特点是具有再生性质：对于任意函数 $f \\in \\mathcal{H}$ 和任意点 $x \\in \\mathbb{R}$，我们有 $f(x) = \\langle f, K_x \\rangle_{\\mathcal{H}}$，其中函数 $K_x \\in \\mathcal{H}$ 定义为 $K_x(y) = K(y, x)$。该性质的一个直接推论是，对于任意两点 $x, y \\in \\mathbb{R}$，相应核函数的内积为 $\\langle K_x, K_y \\rangle_{\\mathcal{H}} = K(x, y)$。函数 $f \\in \\mathcal{H}$ 的范数则由 $\\|f\\|_{\\mathcal{H}} = \\sqrt{\\langle f, f \\rangle_{\\mathcal{H}}}$ 给出。\n\n考虑 $\\mathcal{H}$ 被赋予高斯核的特定情况：\n$$\nK(x, y) = \\exp(-\\gamma(x-y)^2)\n$$\n其中 $\\gamma$ 是一个正实数参数。\n\n我们定义函数 $f \\in \\mathcal{H}$ 为以点 $a$ 和 $b$ 为中心的两个核函数的线性组合：\n$$\nf = c_a K_a - c_b K_b\n$$\n其中 $c_a, c_b$ 为实数常量且 $a, b \\in \\mathbb{R}$。\n\n你的任务是在特定参数值 $\\gamma=1$、$c_a=3$、$c_b=1$、$a=0$ 和 $b=1$ 的情况下，计算该函数的范数平方 $\\|f\\|_{\\mathcal{H}}^2$。", "solution": "1. 在一个 RKHS 中，对于 $f=\\sum_i c_iK_{x_i}$ 我们有\n$$\n\\|f\\|_{\\mathcal H}^2\n=\\Big\\langle\\sum_i c_iK_{x_i},\\sum_j c_jK_{x_j}\\Big\\rangle\n=\\sum_{i,j}c_ic_j\\langle K_{x_i},K_{x_j}\\rangle\n=\\sum_{i,j}c_ic_jK(x_i,x_j).\n$$\n\n2. 这里 $f=3K_0-1K_1$，所以\n$$\n\\|f\\|_{\\mathcal H}^2\n=3^2K(0,0)+(-1)^2K(1,1)-2\\cdot3\\cdot1\\,K(0,1).\n$$\n\n3. 对于 $K(x,y)=\\exp(-(x-y)^2)$，我们有 $K(0,0)=1$，$K(1,1)=1$，$K(0,1)=e^{-1}$。因此\n$$\n\\|f\\|_{\\mathcal H}^2\n=9\\cdot1+1\\cdot1-6\\,e^{-1}\n=10-6e^{-1}.\n$$", "answer": "$$\\boxed{10-6e^{-1}}$$", "id": "1033834"}, {"introduction": "在掌握了基本计算之后，下一个挑战是建立代数操作与特征空间中几何直觉之间的联系。本练习探讨了“中心化”这一常见操作，揭示了对格拉姆矩阵 (Gram matrix) 进行中心化等价于将数据在特征空间中的“质心”平移到原点 [@problem_id:3170311]。理解这一点对于后续学习如核主成分分析 (Kernel PCA) 等关注数据结构而非绝对位置的算法至关重要。", "problem": "考虑一个定义在输入集 $\\mathcal{X}$ 上的正半定核 $k$，其关联的特征映射为 $\\phi:\\mathcal{X}\\to\\mathcal{H}$，该映射将输入集映入一个再生核希尔伯特空间 (RKHS) $\\mathcal{H}$，并满足 $k(x,x')=\\langle \\phi(x),\\phi(x')\\rangle_{\\mathcal{H}}$ 对所有 $x,x'\\in\\mathcal{X}$ 成立。给定一个数据集 $\\{x_{i}\\}_{i=1}^{n}\\subset\\mathcal{X}$，并将其 Gram 矩阵记为 $K\\in\\mathbb{R}^{n\\times n}$，其元素为 $K_{ij}=k(x_{i},x_{j})$。定义样本均值嵌入 $\\mu:=\\frac{1}{n}\\sum_{i=1}^{n}\\phi(x_{i})\\in\\mathcal{H}$ 和中心化特征向量 $\\tilde{\\phi}(x_{i}):=\\phi(x_{i})-\\mu$。中心化 Gram 矩阵 $K^{\\mathrm{c}}\\in\\mathbb{R}^{n\\times n}$ 由内积 $K^{\\mathrm{c}}_{ij}:=\\langle \\tilde{\\phi}(x_{i}),\\tilde{\\phi}(x_{j})\\rangle_{\\mathcal{H}}$ 构成。\n\n1. 仅使用上述定义和基础线性代数，推导出一个关于 $K^{\\mathrm{c}}$ 的闭式矩阵表达式，该表达式用 $K$ 以及一个由单位矩阵和全一向量构造的 $n\\times n$ 矩阵来表示。您的表达式必须完全简化，并且只依赖于 $K$ 和 $n$。\n\n2. 现在，特化到 $\\mathbb{R}^{2}$ 上的线性核 $k(u,v)=u^{\\top}v$。构造两个各包含三个点的数据集：\n   - 数据集 $\\mathcal{A}$：$x_{1}=(0,0)$，$x_{2}=(1,0)$，$x_{3}=(0,1)$。\n   - 数据集 $\\mathcal{B}$：$y_{i}=x_{i}+c$ 对所有 $i$ 成立，其中 $c=(2,2)$。\n   计算未中心化的 Gram 矩阵 $K^{\\mathcal{A}}$ 和 $K^{\\mathcal{B}}$，然后计算它们的中心化版本 $K^{\\mathcal{A},\\mathrm{c}}$ 和 $K^{\\mathcal{B},\\mathrm{c}}$。\n\n3. 将中心化 Gram 矩阵之差的弗罗贝尼乌斯范数 $\\|K^{\\mathcal{A},\\mathrm{c}}-K^{\\mathcal{B},\\mathrm{c}}\\|_{F}$ 作为一个实数报告。无需四舍五入。\n\n最后，在特征空间中均值嵌入的背景下解释该结果，说明为什么这两个数据集可以有相同的中心化 Gram 矩阵，但却有不同的未中心化 Gram 矩阵。", "solution": "**第1部分：中心化 Gram 矩阵表达式的推导**\n\n我们的任务是推导中心化 Gram 矩阵 $K^{\\mathrm{c}}$ 的闭式矩阵表达式，该表达式用未中心化的 Gram 矩阵 $K$ 和样本大小 $n$ 表示。\n\n中心化 Gram 矩阵 $K^{\\mathrm{c}}$ 的元素定义为 $K^{\\mathrm{c}}_{ij} = \\langle \\tilde{\\phi}(x_{i}), \\tilde{\\phi}(x_{j}) \\rangle_{\\mathcal{H}}$，其中 $\\tilde{\\phi}(x_{i}) = \\phi(x_{i}) - \\mu$ 是中心化特征向量，$\\mu = \\frac{1}{n}\\sum_{k=1}^{n}\\phi(x_{k})$ 是样本均值嵌入。\n\n利用内积的双线性性质，我们展开 $K^{\\mathrm{c}}_{ij}$ 的表达式：\n$$\nK^{\\mathrm{c}}_{ij} = \\langle \\phi(x_{i}) - \\mu, \\phi(x_{j}) - \\mu \\rangle_{\\mathcal{H}} = \\langle \\phi(x_{i}), \\phi(x_{j}) \\rangle_{\\mathcal{H}} - \\langle \\phi(x_{i}), \\mu \\rangle_{\\mathcal{H}} - \\langle \\mu, \\phi(x_{j}) \\rangle_{\\mathcal{H}} + \\langle \\mu, \\mu \\rangle_{\\mathcal{H}}\n$$\n\n现在我们使用核函数 $k(x,x') = \\langle \\phi(x), \\phi(x') \\rangle_{\\mathcal{H}}$ 和 Gram 矩阵的元素 $K_{ij} = k(x_i, x_j)$ 来表示每一项。\n\n第一项就是未中心化 Gram 矩阵元素的定义：\n$$\n\\langle \\phi(x_{i}), \\phi(x_{j}) \\rangle_{\\mathcal{H}} = k(x_{i}, x_{j}) = K_{ij}\n$$\n\n对于第二项，我们代入 $\\mu$ 的定义：\n$$\n\\langle \\phi(x_{i}), \\mu \\rangle_{\\mathcal{H}} = \\left\\langle \\phi(x_{i}), \\frac{1}{n}\\sum_{k=1}^{n}\\phi(x_{k}) \\right\\rangle_{\\mathcal{H}} = \\frac{1}{n}\\sum_{k=1}^{n}\\langle \\phi(x_{i}), \\phi(x_{k}) \\rangle_{\\mathcal{H}} = \\frac{1}{n}\\sum_{k=1}^{n}k(x_{i}, x_{k}) = \\frac{1}{n}\\sum_{k=1}^{n}K_{ik}\n$$\n\n由于内积的对称性，第三项是类似的：\n$$\n\\langle \\mu, \\phi(x_{j}) \\rangle_{\\mathcal{H}} = \\left\\langle \\frac{1}{n}\\sum_{k=1}^{n}\\phi(x_{k}), \\phi(x_{j}) \\right\\rangle_{\\mathcal{H}} = \\frac{1}{n}\\sum_{k=1}^{n}\\langle \\phi(x_{k}), \\phi(x_{j}) \\rangle_{\\mathcal{H}} = \\frac{1}{n}\\sum_{k=1}^{n}k(x_{k}, x_{j}) = \\frac{1}{n}\\sum_{k=1}^{n}K_{kj}\n$$\n\n对于第四项，我们有：\n$$\n\\langle \\mu, \\mu \\rangle_{\\mathcal{H}} = \\left\\langle \\frac{1}{n}\\sum_{k=1}^{n}\\phi(x_{k}), \\frac{1}{n}\\sum_{l=1}^{n}\\phi(x_{l}) \\right\\rangle_{\\mathcal{H}} = \\frac{1}{n^2}\\sum_{k=1}^{n}\\sum_{l=1}^{n}\\langle \\phi(x_{k}), \\phi(x_{l}) \\rangle_{\\mathcal{H}} = \\frac{1}{n^2}\\sum_{k=1}^{n}\\sum_{l=1}^{n}K_{kl}\n$$\n\n结合这些项，单个元素 $K^{\\mathrm{c}}_{ij}$ 的表达式为：\n$$\nK^{\\mathrm{c}}_{ij} = K_{ij} - \\frac{1}{n}\\sum_{k=1}^{n}K_{ik} - \\frac{1}{n}\\sum_{k=1}^{n}K_{kj} + \\frac{1}{n^2}\\sum_{k=1}^{n}\\sum_{l=1}^{n}K_{kl}\n$$\n\n为了以矩阵形式表示，令 $I_n$ 为 $n \\times n$ 单位矩阵，$J_n$ 为 $n \\times n$ 全一矩阵。上述表达式可以看作是某个矩阵乘积的第 $(i,j)$ 个元素。\n项 $\\frac{1}{n}\\sum_{k=1}^{n}K_{ik}$ 是矩阵 $\\frac{1}{n}KJ_n$ 的第 $(i,j)$ 个元素。\n项 $\\frac{1}{n}\\sum_{k=1}^{n}K_{kj}$ 是矩阵 $\\frac{1}{n}J_nK$ 的第 $(i,j)$ 个元素。\n项 $\\frac{1}{n^2}\\sum_{k,l}K_{kl}$ 是矩阵 $\\frac{1}{n^2}J_nKJ_n$ 的第 $(i,j)$ 个元素。\n\n因此，我们可以写出矩阵方程：\n$$\nK^{\\mathrm{c}} = K - \\frac{1}{n}KJ_n - \\frac{1}{n}J_nK + \\frac{1}{n^2}J_nKJ_n\n$$\n\n这个表达式可以被因式分解。我们定义中心化矩阵 $H_n = I_n - \\frac{1}{n}J_n$。这个矩阵是由单位矩阵和全一矩阵构造的，符合要求。现在我们计算乘积 $H_n K H_n$：\n$$\nH_n K H_n = \\left(I_n - \\frac{1}{n}J_n\\right) K \\left(I_n - \\frac{1}{n}J_n\\right) = \\left(K - \\frac{1}{n}J_nK\\right) \\left(I_n - \\frac{1}{n}J_n\\right)\n$$\n$$\n= K - \\frac{1}{n}KJ_n - \\frac{1}{n}J_nK + \\frac{1}{n^2}J_nKJ_n\n$$\n这正是我们为 $K^{\\mathrm{c}}$ 推导出的表达式。因此，闭式表达式为：\n$$\nK^{\\mathrm{c}} = H_n K H_n = \\left(I_n - \\frac{1}{n}J_n\\right) K \\left(I_n - \\frac{1}{n}J_n\\right)\n$$\n\n**第2部分：针对特定数据集的计算**\n\n我们特化到 $\\mathbb{R}^2$ 上的线性核 $k(u,v)=u^{\\top}v$ 和 $n=3$ 的情况。\n数据集为：\n- 数据集 $\\mathcal{A}$：$x_{1}=(0,0)$，$x_{2}=(1,0)$，$x_{3}=(0,1)$。\n- 数据集 $\\mathcal{B}$：$y_{i}=x_{i}+c$，其中 $c=(2,2)$，得到 $y_{1}=(2,2)$，$y_{2}=(3,2)$，$y_{3}=(2,3)$。\n\n首先，我们计算未中心化的 Gram 矩阵 $K^{\\mathcal{A}}$，其元素为 $K^{\\mathcal{A}}_{ij} = x_i^\\top x_j$：\n$$\nK^{\\mathcal{A}} = \\begin{pmatrix} x_1^\\top x_1  x_1^\\top x_2  x_1^\\top x_3 \\\\ x_2^\\top x_1  x_2^\\top x_2  x_2^\\top x_3 \\\\ x_3^\\top x_1  x_3^\\top x_2  x_3^\\top x_3 \\end{pmatrix} = \\begin{pmatrix} 0  0  0 \\\\ 0  1  0 \\\\ 0  0  1 \\end{pmatrix}\n$$\n\n接下来，我们计算未中心化的 Gram 矩阵 $K^{\\mathcal{B}}$，其元素为 $K^{\\mathcal{B}}_{ij} = y_i^\\top y_j$：\n$$\nK^{\\mathcal{B}} = \\begin{pmatrix} y_1^\\top y_1  y_1^\\top y_2  y_1^\\top y_3 \\\\ y_2^\\top y_1  y_2^\\top y_2  y_2^\\top y_3 \\\\ y_3^\\top y_1  y_3^\\top y_2  y_3^\\top y_3 \\end{pmatrix} = \\begin{pmatrix} 8  10  10 \\\\ 10  13  12 \\\\ 10  12  13 \\end{pmatrix}\n$$\n\n现在我们计算中心化的 Gram 矩阵。对于 $n=3$，中心化矩阵为：\n$$\nH_3 = I_3 - \\frac{1}{3}J_3 = \\begin{pmatrix} 1  0  0 \\\\ 0  1  0 \\\\ 0  0  1 \\end{pmatrix} - \\frac{1}{3}\\begin{pmatrix} 1  1  1 \\\\ 1  1  1 \\\\ 1  1  1 \\end{pmatrix} = \\frac{1}{3}\\begin{pmatrix} 2  -1  -1 \\\\ -1  2  -1 \\\\ -1  -1  2 \\end{pmatrix}\n$$\n\n我们计算 $K^{\\mathcal{A},\\mathrm{c}} = H_3 K^{\\mathcal{A}} H_3$：\n$$\nK^{\\mathcal{A},\\mathrm{c}} = \\frac{1}{3}\\begin{pmatrix} 2  -1  -1 \\\\ -1  2  -1 \\\\ -1  -1  2 \\end{pmatrix} \\begin{pmatrix} 0  0  0 \\\\ 0  1  0 \\\\ 0  0  1 \\end{pmatrix} \\frac{1}{3}\\begin{pmatrix} 2  -1  -1 \\\\ -1  2  -1 \\\\ -1  -1  2 \\end{pmatrix}\n$$\n$$\n= \\frac{1}{9}\\begin{pmatrix} 0  -1  -1 \\\\ 0  2  -1 \\\\ 0  -1  2 \\end{pmatrix} \\begin{pmatrix} 2  -1  -1 \\\\ -1  2  -1 \\\\ -1  -1  2 \\end{pmatrix} = \\frac{1}{9}\\begin{pmatrix} 2  -1  -1 \\\\ -1  5  -4 \\\\ -1  -4  5 \\end{pmatrix}\n$$\n\n另外，对于线性核，特征映射为 $\\phi(x)=x$。中心化特征为 $\\tilde{x}_i = x_i - \\mu_x$。\n$\\mu_x = \\frac{1}{3}(x_1+x_2+x_3) = \\frac{1}{3}((0,0)+(1,0)+(0,1)) = (\\frac{1}{3},\\frac{1}{3})$。\n数据集 $\\mathcal{B}$ 的中心化特征向量是 $\\tilde{y}_i = y_i - \\mu_y$。其均值为 $\\mu_y = \\frac{1}{3}\\sum(x_i+c) = (\\frac{1}{3}\\sum x_i) + c = \\mu_x+c$。\n所以，$\\tilde{y}_i = (x_i+c) - (\\mu_x+c) = x_i - \\mu_x = \\tilde{x}_i$。\n由于两个数据集的中心化特征向量相同 ($\\tilde{y}_i = \\tilde{x}_i$)，它们的成对内积也必定相同。\n$$\nK^{\\mathcal{B},\\mathrm{c}}_{ij} = \\langle \\tilde{y}_i, \\tilde{y}_j \\rangle = \\tilde{y}_i^\\top \\tilde{y}_j = \\tilde{x}_i^\\top \\tilde{x}_j = K^{\\mathcal{A},\\mathrm{c}}_{ij}\n$$\n因此，$K^{\\mathcal{B},\\mathrm{c}} = K^{\\mathcal{A},\\mathrm{c}}$。我们得到：\n$$\nK^{\\mathcal{A},\\mathrm{c}} = K^{\\mathcal{B},\\mathrm{c}} = \\frac{1}{9}\\begin{pmatrix} 2  -1  -1 \\\\ -1  5  -4 \\\\ -1  -4  5 \\end{pmatrix}\n$$\n\n**第3部分：弗罗贝尼乌斯范数与解释**\n\n我们被要求报告中心化 Gram 矩阵之差的弗罗贝尼乌斯范数。由于 $K^{\\mathcal{A},\\mathrm{c}} = K^{\\mathcal{B},\\mathrm{c}}$，它们的差是 $3 \\times 3$ 的零矩阵 $\\mathbf{0}_{3\\times 3}$。弗罗贝尼乌斯范数定义为 $\\|A\\|_F = \\sqrt{\\sum_{i,j} |A_{ij}|^2}$。\n对于零矩阵，范数为：\n$$\n\\|K^{\\mathcal{A},\\mathrm{c}} - K^{\\mathcal{B},\\mathrm{c}}\\|_{F} = \\|\\mathbf{0}_{3\\times 3}\\|_{F} = \\sqrt{0} = 0\n$$\n\n**解释：**\n结果 $\\|K^{\\mathcal{A},\\mathrm{c}}-K^{\\mathcal{B},\\mathrm{c}}\\|_{F} = 0$ 表明，尽管数据集 $\\mathcal{A}$ 和 $\\mathcal{B}$ 的未中心化 Gram 矩阵 $K^{\\mathcal{A}}$ 和 $K^{\\mathcal{B}}$ 不同，但它们的中心化 Gram 矩阵是相同的。这种不变性在均值嵌入和特征映射的背景下有清晰的解释。\n\n在特征空间中进行中心化，即通过操作 $\\tilde{\\phi}(x) = \\phi(x) - \\mu$，其目的是移除数据云在 RKHS $\\mathcal{H}$ 中的“位置”信息（即均值 $\\mu$），而只保留其“形状”信息（即数据点相对于其均值的几何结构）。\n\n对于线性核 $k(u,v)=u^\\top v$ 的特定情况，特征映射可以看作是恒等映射，$\\phi(x)=x$。数据集 $\\mathcal{B}$ 是数据集 $\\mathcal{A}$ 经过一个常数向量 $c$ 的刚性平移得到的，即 $y_i = x_i + c$。因为特征映射是线性的，输入空间中的这种平移对应于特征空间中完全相同的平移：$\\phi(y_i) = \\phi(x_i+c) = x_i+c = \\phi(x_i) + \\phi(c)$（这里我们可以将 $c$ 视为 $\\phi(c)$）。\n\n因此，数据集 $\\mathcal{B}$ 的均值嵌入也相对于数据集 $\\mathcal{A}$ 的均值嵌入平移了 $c$：\n$\\mu_{\\mathcal{B}} = \\frac{1}{n}\\sum_i \\phi(y_i) = \\frac{1}{n}\\sum_i (\\phi(x_i)+c) = (\\frac{1}{n}\\sum_i \\phi(x_i)) + c = \\mu_{\\mathcal{A}} + c$。\n\n当我们计算数据集 $\\mathcal{B}$ 的中心化特征向量时，平移项 $c$ 完全抵消了：\n$\\tilde{\\phi}(y_i) = \\phi(y_i) - \\mu_{\\mathcal{B}} = (\\phi(x_i) + c) - (\\mu_{\\mathcal{A}} + c) = \\phi(x_i) - \\mu_{\\mathcal{A}} = \\tilde{\\phi}(x_i)$。\n\n结果是，两个数据集的中心化特征向量完全相同。由于中心化 Gram 矩阵 $K^\\mathrm{c}$ 是由这些中心化向量的内积组成的，因此它对于两个数据集必须是相同的。这表明对于线性核，数据相对于其均值的几何结构对于数据在输入空间中的全局平移是不变的。因此，依赖于 $K^\\mathrm{c}$ 的算法，例如核主成分分析（Kernel PCA），会从两个数据集中提取出完全相同的方差结构。", "answer": "$$\n\\boxed{0}\n$$", "id": "3170311"}]}