{"hands_on_practices": [{"introduction": "理解共轭先验是掌握贝叶斯推断核心机制的基石。本练习将探讨经典的泊松-伽马（Poisson-Gamma）共轭模型，这是对组件故障或事件发生次数等计数数据进行建模的基石。通过从第一性原理出发推导后验分布和后验预测分布，你将掌握在贝叶斯框架下更新信念并进行预测所需的基础技能。[@problem_id:3104618]", "problem": "一项可靠性研究记录了在不同监测时长内元件故障的次数。对于观测 $i \\in \\{1,\\dots,n\\}$，设 $y_{i}$ 是在暴露时长 $E_{i}$（以时间单位计）内的观测计数。假设一个带暴露偏移量的泊松回归：以一个未知的基准故障率 $\\theta$（每单位暴露时长）为条件，各计数值独立地满足 $y_{i} \\mid \\theta \\sim \\text{Poisson}(E_{i}\\,\\theta)$。$\\theta$ 的先验分布是形状-率参数化的 $\\text{Gamma}(a,b)$ 分布，其密度函数为 $p(\\theta) = \\dfrac{b^{a}}{\\Gamma(a)}\\,\\theta^{a-1}\\exp(-b\\,\\theta)$，其中 $\\theta>0$。从泊松似然和伽马先验的定义出发，推导在暴露时长 $E_{\\text{new}}$ 下观测到的未来计数 $y_{\\text{new}}$ 的后验预测分布，将预测概率质量函数 $p(y_{\\text{new}}=k \\mid \\text{data})$ 以 $a$、$b$、$\\sum_{i=1}^{n} y_{i}$、$\\sum_{i=1}^{n} E_{i}$ 和 $E_{\\text{new}}$ 表示的封闭形式表达出来，其中 $k$ 为任意非负整数。\n\n现在考虑一个有 $n=5$ 个观测值的数据集，其暴露时长为 $(E_{1},E_{2},E_{3},E_{4},E_{5}) = (0.5,\\,1.0,\\,0.75,\\,0.25,\\,1.5)$，计数值为 $(y_{1},y_{2},y_{3},y_{4},y_{5}) = (1,\\,2,\\,2,\\,0,\\,1)$。取先验超参数 $a=2$ 和 $b=2$。使用你推导出的后验预测分布，计算当未来暴露时长为 $E_{\\text{new}}=2$ 时，下一个观测计数等于 $4$ 的后验预测概率。将你的最终答案表示为小数并四舍五入到四位有效数字。", "solution": "该问题要求推导泊松-伽马共轭模型的后验预测分布，并进行后续的数值计算。这个过程包括三个主要步骤：首先，确定率参数 $\\theta$ 的后验分布；其次，使用这个后验分布来推导新观测值的后验预测分布；第三，将此结果应用于给定的具体数据。\n\n设数据记为 $\\mathcal{D} = \\{(y_i, E_i)\\}_{i=1}^n$。模型规定，在给定率参数 $\\theta$ 的条件下，计数值 $y_i$ 是条件独立的，服从均值为 $E_i\\theta$ 的泊松分布。数据的似然函数是各个泊松概率质量函数的乘积：\n$$p(\\mathcal{D} \\mid \\theta) = \\prod_{i=1}^{n} p(y_i \\mid \\theta) = \\prod_{i=1}^{n} \\frac{(E_i \\theta)^{y_i} \\exp(-E_i \\theta)}{y_i!}$$\n这个表达式可以通过分离依赖于 $\\theta$ 的项来重写：\n$$p(\\mathcal{D} \\mid \\theta) = \\left( \\prod_{i=1}^{n} \\frac{E_i^{y_i}}{y_i!} \\right) \\theta^{\\sum_{i=1}^{n} y_i} \\exp\\left(-\\theta \\sum_{i=1}^{n} E_i\\right)$$\n作为 $\\theta$ 的函数，似然函数正比于 $\\theta^{\\sum y_i} \\exp(-\\theta \\sum E_i)$。\n\n$\\theta$ 的先验分布是伽马分布，$\\theta \\sim \\text{Gamma}(a,b)$，其概率密度函数为：\n$$p(\\theta) = \\frac{b^a}{\\Gamma(a)} \\theta^{a-1} \\exp(-b\\theta)$$\n先验分布的核为 $p(\\theta) \\propto \\theta^{a-1} \\exp(-b\\theta)$。\n\n根据贝叶斯定理，给定数据 $\\mathcal{D}$ 时 $\\theta$ 的后验分布正比于似然函数与先验分布的乘积：\n$$p(\\theta \\mid \\mathcal{D}) \\propto p(\\mathcal{D} \\mid \\theta) p(\\theta)$$\n$$p(\\theta \\mid \\mathcal{D}) \\propto \\left( \\theta^{\\sum_{i=1}^{n} y_i} \\exp\\left(-\\theta \\sum_{i=1}^{n} E_i\\right) \\right) \\left( \\theta^{a-1} \\exp(-b\\theta) \\right)$$\n合并包含 $\\theta$ 的项：\n$$p(\\theta \\mid \\mathcal{D}) \\propto \\theta^{a + \\sum_{i=1}^{n} y_i - 1} \\exp\\left(-\\left(b + \\sum_{i=1}^{n} E_i\\right)\\theta\\right)$$\n这是伽马分布的核。因此，由于共轭性，后验分布也是伽马分布。具体来说，$\\theta \\mid \\mathcal{D} \\sim \\text{Gamma}(a', b')$，其中后验超参数为：\n$$a' = a + \\sum_{i=1}^{n} y_i$$\n$$b' = b + \\sum_{i=1}^{n} E_i$$\n\n接下来，我们推导在给定暴露时长 $E_{\\text{new}}$ 下新观测值 $y_{\\text{new}}$ 的后验预测分布。这个新观测值的似然函数为 $y_{\\text{new}} \\mid \\theta \\sim \\text{Poisson}(E_{\\text{new}}\\theta)$。后验预测概率质量函数（PMF）是通过将新观测值的似然函数与 $\\theta$ 的后验分布的乘积对 $\\theta$ 的所有可能值进行积分（边缘化）得到的：\n$$p(y_{\\text{new}}=k \\mid \\mathcal{D}) = \\int_0^\\infty p(y_{\\text{new}}=k \\mid \\theta) \\, p(\\theta \\mid \\mathcal{D}) \\, d\\theta$$\n代入 $y_{\\text{new}}$ 的泊松PMF和 $\\theta$ 的伽马后验密度：\n$$p(y_{\\text{new}}=k \\mid \\mathcal{D}) = \\int_0^\\infty \\left( \\frac{(E_{\\text{new}}\\theta)^k \\exp(-E_{\\text{new}}\\theta)}{k!} \\right) \\left( \\frac{(b')^{a'}}{\\Gamma(a')} \\theta^{a'-1} \\exp(-b'\\theta) \\right) d\\theta$$\n我们可以从积分中提出不依赖于 $\\theta$ 的项：\n$$p(y_{\\text{new}}=k \\mid \\mathcal{D}) = \\frac{E_{\\text{new}}^k (b')^{a'}}{k! \\Gamma(a')} \\int_0^\\infty \\theta^{k+a'-1} \\exp(-(E_{\\text{new}}+b')\\theta) d\\theta$$\n该积分是伽马密度 $\\text{Gamma}(k+a', E_{\\text{new}}+b')$ 的核，其值为 $\\frac{\\Gamma(k+a')}{(E_{\\text{new}}+b')^{k+a'}}$。\n将此结果代回表达式中：\n$$p(y_{\\text{new}}=k \\mid \\mathcal{D}) = \\frac{E_{\\text{new}}^k (b')^{a'}}{k! \\Gamma(a')} \\frac{\\Gamma(k+a')}{(E_{\\text{new}}+b')^{k+a'}}$$\n这可以重排成负二项分布的PMF形式：\n$$p(y_{\\text{new}}=k \\mid \\mathcal{D}) = \\frac{\\Gamma(k+a')}{k!\\Gamma(a')} \\frac{(b')^{a'} E_{\\text{new}}^k}{(E_{\\text{new}}+b')^{k+a'}} = \\binom{k+a'-1}{k} \\left(\\frac{b'}{E_{\\text{new}}+b'}\\right)^{a'} \\left(\\frac{E_{\\text{new}}}{E_{\\text{new}}+b'}\\right)^k$$\n代入 $a'$ 和 $b'$ 的表达式，得到最终的封闭形式：\n$$p(y_{\\text{new}}=k \\mid \\mathcal{D}) = \\binom{k + a + \\sum y_i - 1}{k} \\left(\\frac{b + \\sum E_i}{b + \\sum E_i + E_{\\text{new}}}\\right)^{a + \\sum y_i} \\left(\\frac{E_{\\text{new}}}{b + \\sum E_i + E_{\\text{new}}}\\right)^k$$\n\n对于数值计算部分，我们有给定的数据：\n先验超参数：$a=2$，$b=2$。\n暴露时长：$(E_1, \\dots, E_5) = (0.5, 1.0, 0.75, 0.25, 1.5)$。\n计数值：$(y_1, \\dots, y_5) = (1, 2, 2, 0, 1)$。\n未来观测参数：$k=4$，$E_{\\text{new}}=2$。\n\n首先，我们计算数据中的总和：\n$$\\sum_{i=1}^5 y_i = 1+2+2+0+1 = 6$$\n$$\\sum_{i=1}^5 E_i = 0.5+1.0+0.75+0.25+1.5 = 4.0$$\n接下来，我们计算后验超参数 $a'$ 和 $b'$：\n$$a' = a + \\sum y_i = 2 + 6 = 8$$\n$$b' = b + \\sum E_i = 2 + 4.0 = 6.0$$\n现在我们使用推导出的PMF来计算 $p(y_{\\text{new}}=4 \\mid \\mathcal{D})$：\n$$p(y_{\\text{new}}=4 \\mid \\mathcal{D}) = \\binom{4+8-1}{4} \\left(\\frac{6}{2+6}\\right)^8 \\left(\\frac{2}{2+6}\\right)^4$$\n$$p(y_{\\text{new}}=4 \\mid \\mathcal{D}) = \\binom{11}{4} \\left(\\frac{6}{8}\\right)^8 \\left(\\frac{2}{8}\\right)^4 = \\binom{11}{4} \\left(\\frac{3}{4}\\right)^8 \\left(\\frac{1}{4}\\right)^4$$\n我们计算二项式系数：\n$$\\binom{11}{4} = \\frac{11 \\times 10 \\times 9 \\times 8}{4 \\times 3 \\times 2 \\times 1} = 11 \\times 10 \\times 3 = 330$$\n概率为：\n$$p(y_{\\text{new}}=4 \\mid \\mathcal{D}) = 330 \\times \\left(\\frac{3}{4}\\right)^8 \\left(\\frac{1}{4}\\right)^4 = 330 \\times \\frac{3^8}{4^8} \\times \\frac{1^4}{4^4} = 330 \\times \\frac{3^8}{4^{12}}$$\n$$3^8 = 6561$$\n$$4^{12} = 16777216$$\n$$p(y_{\\text{new}}=4 \\mid \\mathcal{D}) = 330 \\times \\frac{6561}{16777216} = \\frac{2165130}{16777216} \\approx 0.1290518$$\n四舍五入到四位有效数字，结果为 $0.1291$。", "answer": "$$\\boxed{0.1291}$$", "id": "3104618"}, {"introduction": "从理论基础转向实际应用，我们现在来探讨在现实场景中先验选择如何影响模型行为。本练习聚焦于贝叶斯逻辑回归，并研究模型在存在离群点时的关键概念——稳健性。通过对比标准的正态先验和重尾的柯西（Cauchy）先验，你将学习如何构建对极端或潜在错误数据点不敏感的模型，这是应用机器学习中的一项至关重要的技能。[@problem_id:3104628]", "problem": "您的任务是实现并分析带有重尾系数先验的贝叶斯逻辑回归，以评估在高杠杆点存在时后验行为。目标是从第一性原理推导出必要的表达式，然后计算最大后验（MAP）估计和后验的拉普拉斯近似。您将构建一个算法来处理斜率系数的标准高斯先验和鲁棒的柯西先验。\n\n从以下基本基础开始：\n- 贝叶斯定理：给定数据 $\\mathcal{D} = \\{(x_i, y_i)\\}_{i=1}^n$、参数 $\\theta$、先验 $p(\\theta)$ 和似然 $p(\\mathcal{D} \\mid \\theta)$，后验为 $p(\\theta \\mid \\mathcal{D}) \\propto p(\\mathcal{D} \\mid \\theta) p(\\theta)$。\n- 具有伯努利结果的逻辑回归模型：$y_i \\in \\{0,1\\}$，其中 $y_i \\sim \\mathrm{Bernoulli}(\\pi_i)$ 且 $\\pi_i = \\sigma(\\eta_i)$，这里 $\\eta_i = \\beta_0 + \\beta_1 x_i$ 且 $\\sigma(z) = \\frac{1}{1+e^{-z}}$。\n- 独立观测的对数似然可加性：$\\log p(\\mathcal{D} \\mid \\theta) = \\sum_{i=1}^n \\log p(y_i \\mid \\theta)$。\n\n您必须：\n1) 在两种先验下推导 $(\\beta_0,\\beta_1)$ 的对数后验：\n- 适用于所有情况的截距项先验：$\\beta_0 \\sim \\mathcal{N}(0,\\sigma_0^2)$，其中 $\\sigma_0 = 10$。\n- 斜率先验情况 A (高斯)：$\\beta_1 \\sim \\mathcal{N}(0, s^2)$，其中 $s = 2.5$。\n- 斜率先验情况 B (柯西)：$\\beta_1 \\sim \\mathrm{Cauchy}(0, s)$，其中 $s = 2.5$。\n2) 根据定义，推导对数后验关于 $(\\beta_0,\\beta_1)$ 的梯度和海森矩阵。\n3) 实现一个数值优化器，通过最小化负对数后验来找到最大后验（MAP）估计量 $\\hat{\\beta} = (\\hat{\\beta}_0, \\hat{\\beta}_1)$。然后，通过在 MAP 处对负对数后验的海森矩阵求逆来计算拉普拉斯近似，以获得近似的后验协方差矩阵。\n4) 通过在目标协变量值 $x^{\\star} = 3$ 处计算基于 MAP 的后验预测点估计，来评估有无高杠杆点时的后验行为。具体来说，计算 $\\hat{p}(y=1 \\mid x^{\\star}) = \\sigma(\\hat{\\beta}_0 + \\hat{\\beta}_1 x^{\\star})$。\n\n测试套件：\n使用以下确定性数据集来运行算法。每个数据集都是一个由 $(x_i,y_i)$ 对组成的列表：\n\n- 数据集 $\\mathcal{D}_0$ (基准)：$[(-2,0),(-1,0),(0,0),(1,1),(2,1)]$。\n- 数据集 $\\mathcal{D}_+$ (添加一个一致的高杠杆点)：$\\mathcal{D}_0 \\cup \\{(50,1)\\}$。\n- 数据集 $\\mathcal{D}_-$ (添加一个矛盾的高杠杆点)：$\\mathcal{D}_0 \\cup \\{(50,0)\\}$。\n- 数据集 $\\mathcal{D}_{\\mathrm{sep}}$ (完全分离边界情况)：$[(-2,0),(-1,0),(1,1),(2,1)]$。\n\n对于每个数据集，运行模型两次，一次使用高斯斜率先验，另一次使用柯西斜率先验，同时保持截距项先验如指定的那样固定。对于每次运行，使用上述 MAP 方法计算单个标量 $\\hat{p}(y=1 \\mid x^{\\star}=3)$。\n\n要求的最终输出格式：\n- 您的程序必须生成单行输出，其中包含一个由方括号括起来的、包含 $8$ 个浮点数的逗号分隔列表，顺序如下：\n$[\\hat{p}_{\\mathrm{Cauchy}}(\\mathcal{D}_0), \\hat{p}_{\\mathrm{Cauchy}}(\\mathcal{D}_+), \\hat{p}_{\\mathrm{Cauchy}}(\\mathcal{D}_-), \\hat{p}_{\\mathrm{Cauchy}}(\\mathcal{D}_{\\mathrm{sep}}), \\hat{p}_{\\mathrm{Normal}}(\\mathcal{D}_0), \\hat{p}_{\\mathrm{Normal}}(\\mathcal{D}_+), \\hat{p}_{\\mathrm{Normal}}(\\mathcal{D}_-), \\hat{p}_{\\mathrm{Normal}}(\\mathcal{D}_{\\mathrm{sep}})]$。\n- 每个数字应四舍五入到恰好 $6$ 位小数。\n- 不涉及物理单位。\n- 不涉及角度。\n- 不得使用百分比；概率必须以 $[0,1]$ 区间内的小数形式报告。\n\n科学真实性和约束：\n- 仅使用定义和基础微积分来推导必要的表达式。\n- 对逻辑函数使用数值稳定的计算，以避免在大的 $\\lvert x \\rvert$（特别是 $x = 50$）时发生溢出。\n- 输出必须是可复现的，并且不应需要任何随机性。\n\n您的最终程序必须是自包含的，无需输入，并严格按照指定格式打印单行输出。", "solution": "该问题要求在斜率系数的两种不同先验分布（高斯（正态）先验和重尾柯西先验）下实现和分析贝叶斯逻辑回归。目标是找到模型参数的最大后验（MAP）估计，并用它进行预测，从而评估高杠杆数据点的影响。推导将按要求从第一性原理出发。\n\n设模型参数为 $\\beta = (\\beta_0, \\beta_1)^T$。数据为 $\\mathcal{D} = \\{(x_i, y_i)\\}_{i=1}^n$，其中 $y_i \\in \\{0, 1\\}$。\n\n**1. 模型设定**\n\n逻辑回归模型假设结果 $y_i$ 是从伯努利分布中抽取的，其概率 $\\pi_i$ 依赖于协变量 $x_i$：\n$$y_i \\sim \\mathrm{Bernoulli}(\\pi_i)$$\n概率 $\\pi_i$ 通过逻辑（sigmoid）函数 $\\sigma(z) = \\frac{1}{1+e^{-z}}$ 与线性预测器 $\\eta_i = \\beta_0 + \\beta_1 x_i$ 相关联：\n$$\\pi_i = \\sigma(\\eta_i) = \\sigma(\\beta_0 + \\beta_1 x_i)$$\n\n给定参数 $\\beta$ 的单个观测 $(x_i, y_i)$ 的似然为 $p(y_i \\mid \\beta) = \\pi_i^{y_i} (1-\\pi_i)^{1-y_i}$。对于一个独立同分布的数据集，总对数似然是各个对数似然之和：\n$$\\mathcal{LL}(\\beta) = \\log p(\\mathcal{D} \\mid \\beta) = \\sum_{i=1}^n \\left[ y_i \\log(\\pi_i) + (1-y_i)\\log(1-\\pi_i) \\right]$$\n通过代入 $\\pi_i = \\sigma(\\eta_i)$，可以得到一个更方便且数值上更稳定的形式：\n$$\\mathcal{LL}(\\beta) = \\sum_{i=1}^n \\left[ y_i \\eta_i - \\log(1+e^{\\eta_i}) \\right]$$\n其中 $\\log(1+e^{\\eta_i})$ 是 softplus 函数。\n\n根据贝叶斯定理，后验分布正比于似然与先验的乘积：$p(\\beta \\mid \\mathcal{D}) \\propto p(\\mathcal{D} \\mid \\beta)p(\\beta)$。我们考虑斜率系数 $\\beta_1$ 的两种先验情况，而截距项 $\\beta_0$ 的先验是固定的。\n- **截距项先验：** $\\beta_0 \\sim \\mathcal{N}(0, \\sigma_0^2)$，其中 $\\sigma_0 = 10$。\n  对数先验为 $\\log p(\\beta_0) = -\\frac{\\beta_0^2}{2\\sigma_0^2} + C_0$。\n- **斜率先验情况 A (高斯)：** $\\beta_1 \\sim \\mathcal{N}(0, s^2)$，其中 $s = 2.5$。\n  对数先验为 $\\log p(\\beta_1) = -\\frac{\\beta_1^2}{2s^2} + C_{1,G}$。\n- **斜率先验情况 B (柯西)：** $\\beta_1 \\sim \\mathrm{Cauchy}(0, s)$，其中 $s = 2.5$。\n  概率密度函数为 $p(\\beta_1) = \\frac{1}{\\pi s (1+(\\beta_1/s)^2)} \\propto \\frac{1}{s^2+\\beta_1^2}$。\n  对数先验为 $\\log p(\\beta_1) = -\\log(s^2+\\beta_1^2) + C_{1,C}$。\n\n**2. MAP估计的目标函数**\n\nMAP 估计 $\\hat{\\beta}$ 是后验分布的众数。它可以通过最大化对数后验找到，这等价于最小化负对数后验。令 $f(\\beta)$ 为该目标函数，忽略常数。\n\n**情况 A (高斯斜率先验)：** 总的负对数后验是负对数似然和负对数先验（对应于正则化项）之和：\n$$f_G(\\beta) = \\underbrace{\\sum_{i=1}^n \\left[ \\log(1+e^{\\eta_i}) - y_i \\eta_i \\right]}_{\\text{负对数似然}} + \\underbrace{\\frac{\\beta_0^2}{2\\sigma_0^2}}_{\\text{$\\beta_0$ 的先验}} + \\underbrace{\\frac{\\beta_1^2}{2s^2}}_{\\text{$\\beta_1$ 的先验}}$$\n\n**情况 B (柯西斜率先验)：**\n$$f_C(\\beta) = \\sum_{i=1}^n \\left[ \\log(1+e^{\\eta_i}) - y_i \\eta_i \\right] + \\frac{\\beta_0^2}{2\\sigma_0^2} + \\log(s^2 + \\beta_1^2)$$\n高斯先验对 $\\beta_1$ 施加二次惩罚，对离群值很敏感。柯西先验的惩罚项 $\\log(s^2+\\beta_1^2)$ 增长得慢得多，使其更具鲁棒性。\n\n**3. 用于优化的梯度和海森矩阵**\n\n为找到 $f(\\beta)$ 的最小值，我们采用牛顿法，该方法需要梯度 $\\nabla f(\\beta)$ 和海森矩阵 $\\nabla^2 f(\\beta)$。我们首先计算负对数似然部分 $f_{LL}(\\beta)$ 的这些值。\n利用性质 $\\frac{d}{dz}\\log(1+e^z) = \\sigma(z)$，梯度为：\n$$\\nabla f_{LL}(\\beta) = \\sum_{i=1}^n \\begin{pmatrix} \\frac{\\partial \\eta_i}{\\partial \\beta_0}(\\sigma(\\eta_i) - y_i) \\\\ \\frac{\\partial \\eta_i}{\\partial \\beta_1}(\\sigma(\\eta_i) - y_i) \\end{pmatrix} = \\sum_{i=1}^n \\begin{pmatrix} \\pi_i - y_i \\\\ (\\pi_i-y_i)x_i \\end{pmatrix}$$\n利用性质 $\\frac{d}{dz}\\sigma(z) = \\sigma(z)(1-\\sigma(z))$，海森矩阵为：\n$$\\nabla^2 f_{LL}(\\beta) = \\sum_{i=1}^n \\begin{pmatrix} \\frac{\\partial^2 f_{LL}}{\\partial \\beta_0^2}  \\frac{\\partial^2 f_{LL}}{\\partial \\beta_0 \\partial \\beta_1} \\\\ \\frac{\\partial^2 f_{LL}}{\\partial \\beta_1 \\partial \\beta_0}  \\frac{\\partial^2 f_{LL}}{\\partial \\beta_1^2} \\end{pmatrix} = \\sum_{i=1}^n \\pi_i(1-\\pi_i) \\begin{pmatrix} 1  x_i \\\\ x_i  x_i^2 \\end{pmatrix}$$\n\n总梯度和总海森矩阵是似然部分和先验部分之和。\n**情况 A (高斯)：**\n$$\\nabla f_G(\\beta) = \\nabla f_{LL}(\\beta) + \\begin{pmatrix} \\beta_0/\\sigma_0^2 \\\\ \\beta_1/s^2 \\end{pmatrix}$$\n$$\\nabla^2 f_G(\\beta) = \\nabla^2 f_{LL}(\\beta) + \\begin{pmatrix} 1/\\sigma_0^2  0 \\\\ 0  1/s^2 \\end{pmatrix}$$\n目标函数 $f_G(\\beta)$ 是凸的，保证了唯一最小值的存在。\n\n**情况 B (柯西)：**\n$$\\nabla f_C(\\beta) = \\nabla f_{LL}(\\beta) + \\begin{pmatrix} \\beta_0/\\sigma_0^2 \\\\ 2\\beta_1/(s^2+\\beta_1^2) \\end{pmatrix}$$\n$$\\nabla^2 f_C(\\beta) = \\nabla^2 f_{LL}(\\beta) + \\begin{pmatrix} 1/\\sigma_0^2  0 \\\\ 0  (2s^2 - 2\\beta_1^2)/(s^2+\\beta_1^2)^2 \\end{pmatrix}$$\n目标函数 $f_C(\\beta)$ 不保证是凸的，因为如果 $|\\beta_1| > s$，柯西先验的海森项可能为负。然而，对于典型的数据集，仍然可以找到唯一的 MAP 估计。\n\n**4. 数值解**\n\nMAP 估计 $\\hat{\\beta}$ 通过初始化 $\\beta^{(0)} = (0,0)^T$ 并迭代应用牛顿-拉夫逊更新规则直到收敛来找到：\n$$\\beta^{(k+1)} = \\beta^{(k)} - [\\nabla^2 f(\\beta^{(k)})]^{-1} \\nabla f(\\beta^{(k)})$$\n对后验 $p(\\beta \\mid \\mathcal{D})$ 的拉普拉斯近似是一个高斯分布 $\\mathcal{N}(\\hat{\\beta}, \\Sigma)$，其中协方差是在 MAP 处负对数后验的海森矩阵的逆，即 $\\Sigma = [\\nabla^2 f(\\hat{\\beta})]^{-1}$。虽然为优化计算了海森矩阵，但最终输出不需要其逆矩阵。\n\n一旦找到 $\\hat{\\beta} = (\\hat{\\beta}_0, \\hat{\\beta}_1)$，就使用 MAP 估计计算新数据点 $x^{\\star}$ 的后验预测概率：\n$$\\hat{p}(y=1 \\mid x^{\\star}, \\mathcal{D}) \\approx \\sigma(\\hat{\\beta}_0 + \\hat{\\beta}_1 x^{\\star})$$\n此过程将应用于每个数据集，分别使用高斯和柯西先验设定。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements Bayesian logistic regression with Gaussian and Cauchy priors to find\n    MAP estimates and compute posterior predictive probabilities.\n    \"\"\"\n    # Define problem parameters from the statement.\n    sigma0 = 10.0\n    s = 2.5\n    x_star = 3.0\n\n    # Define the test cases from the problem statement.\n    d0_data = np.array([[-2.0, 0.0], [-1.0, 0.0], [0.0, 0.0], [1.0, 1.0], [2.0, 1.0]])\n    datasets = {\n        'D0': d0_data,\n        'D+': np.vstack([d0_data, [50.0, 1.0]]),\n        'D-': np.vstack([d0_data, [50.0, 0.0]]),\n        'D_sep': np.array([[-2.0, 0.0], [-1.0, 0.0], [1.0, 1.0], [2.0, 1.0]])\n    }\n    dataset_order = ['D0', 'D+', 'D-', 'D_sep']\n\n    def stable_sigmoid(z):\n        \"\"\"Numerically stable sigmoid function.\"\"\"\n        z = np.asarray(z)\n        # Use np.exp's behavior with vectors/scalars to handle both cases efficiently.\n        return np.where(z >= 0, \n                        1 / (1 + np.exp(-z)), \n                        np.exp(z) / (1 + np.exp(z)))\n\n    def find_map_beta(X, y, prior_type):\n        \"\"\"\n        Finds the Maximum A Posteriori (MAP) estimate for beta using Newton's method.\n        The parameters sigma0 and s are taken from the outer scope.\n        \"\"\"\n        beta = np.zeros(2, dtype=np.float64)\n        num_iterations = 30  # Ample for convergence in these cases\n        tolerance = 1e-9\n\n        for _ in range(num_iterations):\n            b0, b1 = beta[0], beta[1]\n            eta = X @ beta\n            pi = stable_sigmoid(eta)\n\n            # Gradient and Hessian of the negative log-likelihood part.\n            grad_ll = X.T @ (pi - y)\n            weights = pi * (1 - pi)\n            hess_ll = (X.T * weights) @ X\n\n            # Add contributions from the negative log-prior.\n            if prior_type == 'cauchy':\n                # Gaussian prior for beta0\n                grad_b0_prior = b0 / sigma0**2\n                hess_b0_prior = 1 / sigma0**2\n                # Cauchy prior for beta1\n                denom_b1 = (s**2 + b1**2)\n                grad_b1_prior = (2 * b1) / denom_b1\n                hess_b1_prior = (2 * s**2 - 2 * b1**2) / (denom_b1**2)\n            elif prior_type == 'normal':\n                # Gaussian prior for beta0\n                grad_b0_prior = b0 / sigma0**2\n                hess_b0_prior = 1 / sigma0**2\n                # Gaussian prior for beta1\n                grad_b1_prior = b1 / s**2\n                hess_b1_prior = 1 / s**2\n            \n            # Combine likelihood and prior parts to get posterior derivatives.\n            gradient = grad_ll + np.array([grad_b0_prior, grad_b1_prior])\n            hessian = hess_ll + np.diag([hess_b0_prior, hess_b1_prior])\n            \n            # Newton-Raphson update step. Use solve for numerical stability.\n            try:\n                step = np.linalg.solve(hessian, -gradient)\n            except np.linalg.LinAlgError:\n                # Use pseudo-inverse if Hessian is singular (unlikely with priors).\n                step = np.linalg.pinv(hessian) @ -gradient\n\n            beta += step\n            \n            if np.linalg.norm(step)  tolerance:\n                break\n        \n        return beta\n\n    cauchy_results = []\n    normal_results = []\n\n    for ds_name in dataset_order:\n        data = datasets[ds_name]\n        y = data[:, 1]\n        X = np.c_[np.ones(len(y)), data[:, 0]]\n        \n        # --- Case B: Cauchy slope prior ---\n        beta_map_cauchy = find_map_beta(X, y, 'cauchy')\n        pred_prob_cauchy = stable_sigmoid(beta_map_cauchy[0] + beta_map_cauchy[1] * x_star)\n        cauchy_results.append(pred_prob_cauchy)\n        \n        # --- Case A: Normal slope prior ---\n        beta_map_normal = find_map_beta(X, y, 'normal')\n        pred_prob_normal = stable_sigmoid(beta_map_normal[0] + beta_map_normal[1] * x_star)\n        normal_results.append(pred_prob_normal)\n\n    # Combine results in the specified order for the final output.\n    final_results = cauchy_results + normal_results\n    \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{val:.6f}' for val in final_results)}]\")\n\nsolve()\n```", "id": "3104628"}, {"introduction": "贝叶斯学习的力量远不止于所有数据都带标签的传统监督问题。这最后一个练习将通过引导你构建一个半监督分类器来展示其灵活性。你将学习一个生成模型如何利用无标签数据来优化其对类别结构的理解，这体现了贝叶斯原理如何为数据稀疏环境下的学习提供一个自然的框架。[@problem_id:3104582]", "problem": "要求您为一个一维特征构建和比较两个二元贝叶斯分类器，使用一种半监督方法，通过混合模型利用未标记数据。该学习任务必须从适用于统计学习和贝叶斯学习的第一性原理推导得出。您必须使用的基本基础包括贝叶斯定理、贝塔-伯努利共轭先验和高斯似然。不应假定目标量的任何快捷公式；所有量都必须通过基于这些基础的原则性推导获得。\n\n考虑以下模型和数据描述。特征是一个实值随机变量 $x \\in \\mathbb{R}$，类别标签为 $y \\in \\{0,1\\}$。类条件似然是具有已知均值和共同已知标准差的高斯分布：\n$$\np(x \\mid y=0) = \\mathcal{N}(x \\mid \\mu_0, \\sigma^2), \\quad\np(x \\mid y=1) = \\mathcal{N}(x \\mid \\mu_1, \\sigma^2),\n$$\n其中 $\\mu_0 \\in \\mathbb{R}$，$\\mu_1 \\in \\mathbb{R}$ 和 $\\sigma \\in \\mathbb{R}_{0}$ 是给定的。类别概率 $\\pi = p(y=1)$ 的先验是 $\\operatorname{Beta}(\\alpha_0, \\beta_0)$，其超参数为 $\\alpha_0 \\in \\mathbb{R}_{0}$ 和 $\\beta_0 \\in \\mathbb{R}_{0}$。给定类别 $y=1$ 和 $y=0$ 的已标记计数 $n_1$ 和 $n_0$，以及一个由未知标签的特征值组成的未标记集 $U = \\{u_1, \\dots, u_m\\}$。还给定一个测试特征值集 $T = \\{t_1, \\dots, t_k\\}$，您必须报告在这两个分类器下这些点的后验类别概率。\n\n任务：\n1. 构建仅监督的贝叶斯分类器：将贝塔先验与已标记计数结合，形成 $\\pi$ 的后验分布，然后使用贝叶斯定理计算每个 $x \\in T$ 的 $p(y=1 \\mid x)$。在此分类器中，使用 $\\pi$ 的后验均值作为点估计。\n2. 构建半监督贝叶斯分类器：在混合模型解释下，使用未标记集 $U$ 对先验执行单次软分配更新。具体步骤为：从仅监督分类器对 $\\pi$ 的当前估计开始，计算每个 $u \\in U$ 的贝叶斯责任（Bayes-responsibility），将这些责任解释为期望类别计数，将这些期望计数作为额外的伪观测值并入贝塔后验中，然后重新计算 $\\pi$ 的后验均值。使用这个更新后的 $\\pi$ 来计算每个 $x \\in T$ 的 $p(y=1 \\mid x)$。\n3. 对每个 $t \\in T$，计算后验偏移，定义为 $\\Delta(t) = p_{\\text{semi}}(y=1 \\mid t) - p_{\\text{sup}}(y=1 \\mid t)$，其中 $p_{\\text{semi}}(y=1 \\mid t)$ 是半监督后验概率，$p_{\\text{sup}}(y=1 \\mid t)$ 是仅监督后验概率。\n\n用于覆盖不同情况的测试套件参数：\n- 案例1（理想情况，中等信息量的未标记数据）：$(\\mu_0, \\mu_1, \\sigma, \\alpha_0, \\beta_0, n_1, n_0) = (-2.0, 2.0, 1.0, 1.0, 1.0, 5, 3)$，$U = \\{-2.5, -1.5, 1.8, 2.2, 0.0\\}$，$T = \\{-1.0, 0.0, 2.5\\}$。\n- 案例2（边界情况，无未标记数据）：$(\\mu_0, \\mu_1, \\sigma, \\alpha_0, \\beta_0, n_1, n_0) = (-1.0, 1.0, 1.0, 2.0, 2.0, 4, 4)$，$U = \\{\\}$，$T = \\{-2.0, 2.0\\}$。\n- 案例3（边缘情况，未标记数据集中在一个类别附近）：$(\\mu_0, \\mu_1, \\sigma, \\alpha_0, \\beta_0, n_1, n_0) = (0.0, 3.0, 0.5, 2.0, 2.0, 2, 2)$，$U = \\{2.8, 3.1, 3.3, 2.9\\}$，$T = \\{2.0, 3.0\\}$。\n- 案例4（边界情况，类条件密度无法区分）：$(\\mu_0, \\mu_1, \\sigma, \\alpha_0, \\beta_0, n_1, n_0) = (0.0, 0.0, 1.0, 1.0, 9.0, 1, 9)$，$U = \\{0.1, -0.1, 0.0\\}$，$T = \\{-1.0, 1.0\\}$。\n\n您的程序必须为每个案例实现上述过程，并计算每个案例的后验偏移列表 $\\{\\Delta(t) : t \\in T\\}$。所有答案都应表示为十进制数（浮点数）。您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表。列表中的每个元素对应一个案例，其本身是该案例的后验偏移列表，顺序与 $T$ 中的元素相同。例如，包含四个案例的输出格式应如下所示：`[[\\Delta_{1,1}, \\Delta_{1,2}, \\dots],[\\Delta_{2,1}, \\Delta_{2,2}, \\dots],[\\Delta_{3,1}, \\Delta_{3,2}, \\dots],[\\Delta_{4,1}, \\Delta_{4,2}, \\dots]]`。", "solution": "该问题要求为一个一维特征 $x \\in \\mathbb{R}$ 构建和比较一个仅监督贝叶斯分类器和一个半监督贝叶斯分类器。我们将按照规定从第一性原理推导所有需要的量。\n\n模型定义如下：\n- 类别标签 $y \\in \\{0, 1\\}$。\n- 类别先验概率 $\\pi = p(y=1)$。该参数的先验是贝塔分布：$p(\\pi) = \\operatorname{Beta}(\\pi \\mid \\alpha_0, \\beta_0)$。\n- 类条件似然，是具有共享方差的高斯分布：\n$$ p(x \\mid y=0) = \\mathcal{N}(x \\mid \\mu_0, \\sigma^2) $$\n$$ p(x \\mid y=1) = \\mathcal{N}(x \\mid \\mu_1, \\sigma^2) $$\n高斯概率密度函数（PDF）由 $\\mathcal{N}(x \\mid \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x - \\mu)^2}{2\\sigma^2}\\right)$ 给出。为简洁起见，我们将类别 1 的似然表示为 $L_1(x) = p(x \\mid y=1)$，类别 0 的似然表示为 $L_0(x) = p(x \\mid y=0)$。\n\n贝叶斯分类的核心是计算给定特征值下的类别后验概率 $p(y=1 \\mid x)$。使用贝叶斯定理：\n$$ p(y=1 \\mid x) = \\frac{p(x \\mid y=1)p(y=1)}{p(x)} = \\frac{p(x \\mid y=1)p(y=1)}{p(x \\mid y=1)p(y=1) + p(x \\mid y=0)p(y=0)} $$\n代入 $\\pi = p(y=1)$ 和 $1-\\pi = p(y=0)$，我们得到：\n$$ p(y=1 \\mid x, \\pi) = \\frac{L_1(x) \\pi}{L_1(x) \\pi + L_0(x)(1 - \\pi)} $$\n两个分类器在如何估计 $\\pi$ 方面会有所不同。\n\n### 1. 仅监督分类器\n\n对于仅监督分类器，我们使用已标记数据计数（类别 $y=1$ 的计数为 $n_1$，类别 $y=0$ 的计数为 $n_0$）来更新 $\\pi$ 的先验。\n\n$\\pi$ 的先验是 $p(\\pi) = \\operatorname{Beta}(\\pi \\mid \\alpha_0, \\beta_0)$，其形式为：\n$$ p(\\pi) \\propto \\pi^{\\alpha_0 - 1} (1-\\pi)^{\\beta_0 - 1} $$\n在 $n_1+n_0$ 次试验中观测到 $n_1$ 次成功（类别 1）和 $n_0$ 次失败（类别 0）的似然由伯努利似然函数给出：\n$$ p(D_L \\mid \\pi) \\propto \\pi^{n_1} (1-\\pi)^{n_0} $$\n其中 $D_L$ 代表已标记数据。\n\n$\\pi$ 的后验分布通过应用贝叶斯法则找到：\n$$ p(\\pi \\mid D_L) \\propto p(D_L \\mid \\pi) p(\\pi) \\propto \\left( \\pi^{n_1} (1-\\pi)^{n_0} \\right) \\left( \\pi^{\\alpha_0 - 1} (1-\\pi)^{\\beta_0 - 1} \\right) = \\pi^{\\alpha_0 + n_1 - 1} (1-\\pi)^{\\beta_0 + n_0 - 1} $$\n这是贝塔分布的核，是贝塔先验和伯努利似然之间共轭关系的结果。后验分布是：\n$$ p(\\pi \\mid D_L) = \\operatorname{Beta}(\\pi \\mid \\alpha_0 + n_1, \\beta_0 + n_0) $$\n令后验超参数为 $\\alpha_{\\text{sup}} = \\alpha_0 + n_1$ 和 $\\beta_{\\text{sup}} = \\beta_0 + n_0$。\n\n问题指定使用 $\\pi$ 的后验均值作为点估计。$\\operatorname{Beta}(\\alpha, \\beta)$ 分布的均值是 $\\frac{\\alpha}{\\alpha + \\beta}$。因此，$\\pi$ 的监督估计是：\n$$ \\hat{\\pi}_{\\text{sup}} = E[\\pi \\mid D_L] = \\frac{\\alpha_{\\text{sup}}}{\\alpha_{\\text{sup}} + \\beta_{\\text{sup}}} = \\frac{\\alpha_0 + n_1}{\\alpha_0 + n_1 + \\beta_0 + n_0} $$\n那么，对于测试点 $t \\in T$ 的监督后验类别概率是：\n$$ p_{\\text{sup}}(y=1 \\mid t) = \\frac{L_1(t) \\hat{\\pi}_{\\text{sup}}}{L_1(t) \\hat{\\pi}_{\\text{sup}} + L_0(t)(1 - \\hat{\\pi}_{\\text{sup}})} $$\n\n### 2. 半监督分类器\n\n半监督方法通过一次软分配更新来整合未标记数据集 $U = \\{u_1, \\dots, u_m\\}$，这等同于贝叶斯框架下期望最大化（EM）式算法的一次迭代。\n\n**E-步骤（软分配）：** 我们首先为每个未标记数据点 $u_i \\in U$ 计算类别 1 的“责任”（responsibility）。责任 $r_i$ 是在给定类别先验的当前估计 $\\hat{\\pi}_{\\text{sup}}$ 的条件下，点 $u_i$ 属于类别 1 的后验概率。\n$$ r_i = p(y=1 \\mid u_i, \\hat{\\pi}_{\\text{sup}}) = \\frac{L_1(u_i) \\hat{\\pi}_{\\text{sup}}}{L_1(u_i) \\hat{\\pi}_{\\text{sup}} + L_0(u_i)(1 - \\hat{\\pi}_{\\text{sup}})} $$\n这些责任被解释为期望计数。来自未标记数据的类别 1 的总期望计数（或伪计数）是 $N_{1,U} = \\sum_{i=1}^m r_i$。类似地，类别 0 的总期望计数是 $N_{0,U} = \\sum_{i=1}^m (1 - r_i) = m - N_{1,U}$。\n\n**M-步骤（参数更新）：** 我们通过将这些伪计数与原始的已标记计数一起并入，来更新 $\\pi$ 的后验分布。这意味着将伪计数加到监督后验的超参数上：\n$$ \\alpha_{\\text{semi}} = \\alpha_{\\text{sup}} + N_{1,U} = \\alpha_0 + n_1 + N_{1,U} $$\n$$ \\beta_{\\text{semi}} = \\beta_{\\text{sup}} + N_{0,U} = \\beta_0 + n_0 + m - N_{1,U} $$\n$\\pi$ 的新后验分布是 $p(\\pi \\mid D_L, D_U) = \\operatorname{Beta}(\\pi \\mid \\alpha_{\\text{semi}}, \\beta_{\\text{semi}})$。\n\n$\\pi$ 的更新后的点估计是这个新后验的均值：\n$$ \\hat{\\pi}_{\\text{semi}} = E[\\pi \\mid D_L, D_U] = \\frac{\\alpha_{\\text{semi}}}{\\alpha_{\\text{semi}} + \\beta_{\\text{semi}}} = \\frac{\\alpha_0 + n_1 + N_{1,U}}{\\alpha_0 + n_1 + \\beta_0 + n_0 + m} $$\n对于测试点 $t \\in T$ 的半监督后验类别概率是：\n$$ p_{\\text{semi}}(y=1 \\mid t) = \\frac{L_1(t) \\hat{\\pi}_{\\text{semi}}}{L_1(t) \\hat{\\pi}_{\\text{semi}} + L_0(t)(1 - \\hat{\\pi}_{\\text{semi}})} $$\n\n### 3. 后验偏移\n\n最后，对于每个测试点 $t \\in T$，后验偏移 $\\Delta(t)$ 定义为半监督和仅监督的类别 1 后验概率之差：\n$$ \\Delta(t) = p_{\\text{semi}}(y=1 \\mid t) - p_{\\text{sup}}(y=1 \\mid t) $$\n这个量衡量了整合未标记数据 $U$ 对点 $t$ 处的分类决策的影响。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import norm\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for all test cases.\n    It constructs and compares supervised and semi-supervised Bayesian classifiers.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            'params': {'mu0': -2.0, 'mu1': 2.0, 'sigma': 1.0, 'alpha0': 1.0, 'beta0': 1.0, 'n1': 5, 'n0': 3},\n            'U': np.array([-2.5, -1.5, 1.8, 2.2, 0.0]),\n            'T': np.array([-1.0, 0.0, 2.5])\n        },\n        {\n            'params': {'mu0': -1.0, 'mu1': 1.0, 'sigma': 1.0, 'alpha0': 2.0, 'beta0': 2.0, 'n1': 4, 'n0': 4},\n            'U': np.array([]),\n            'T': np.array([-2.0, 2.0])\n        },\n        {\n            'params': {'mu0': 0.0, 'mu1': 3.0, 'sigma': 0.5, 'alpha0': 2.0, 'beta0': 2.0, 'n1': 2, 'n0': 2},\n            'U': np.array([2.8, 3.1, 3.3, 2.9]),\n            'T': np.array([2.0, 3.0])\n        },\n        {\n            'params': {'mu0': 0.0, 'mu1': 0.0, 'sigma': 1.0, 'alpha0': 1.0, 'beta0': 9.0, 'n1': 1, 'n0': 9},\n            'U': np.array([0.1, -0.1, 0.0]),\n            'T': np.array([-1.0, 1.0])\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        shifts = solve_case(**case)\n        results.append(shifts)\n    \n    # Print the final result in the exact required format.\n    # The default str(list) in Python provides the correct spacing.\n    print(f\"[{','.join(map(str, results))}]\")\n\n\ndef solve_case(params, U, T):\n    \"\"\"\n    Solves a single case, calculating the posterior shifts.\n    \n    Args:\n        params (dict): Dictionary of model parameters.\n        U (np.ndarray): Array of unlabeled feature values.\n        T (np.ndarray): Array of test feature values.\n        \n    Returns:\n        list: A list of posterior shifts delta(t) for each t in T.\n    \"\"\"\n    mu0, mu1, sigma = params['mu0'], params['mu1'], params['sigma']\n    alpha0, beta0 = params['alpha0'], params['beta0']\n    n1, n0 = params['n1'], params['n0']\n\n    def posterior_y_given_x(x, pi_hat):\n        \"\"\"Calculates p(y=1 | x) for a given pi_hat.\"\"\"\n        # Check for the edge case where likelihoods are identical\n        if mu0 == mu1:\n            return np.full_like(x, pi_hat, dtype=float)\n\n        l1 = norm.pdf(x, loc=mu1, scale=sigma)\n        l0 = norm.pdf(x, loc=mu0, scale=sigma)\n        \n        numerator = l1 * pi_hat\n        denominator = numerator + l0 * (1.0 - pi_hat)\n        \n        # Handle potential division by zero if both likelihoods and pi are zero/one,\n        # although unlikely with the given constraints.\n        # If denominator is 0, it implies l1 and l0 are both 0.\n        # The posterior is undefined, but we can return 0.5 as a neutral guess.\n        # With Gaussian likelihoods this is practically impossible.\n        # Let's set the posterior to pi_hat if likelihoods are equal, for numerical stability.\n        posterior = np.full_like(denominator, pi_hat, dtype=float)\n        # Avoid division by zero when both likelihoods are zero\n        # which can happen far from means, though probabilities would be ~0.\n        valid_den = denominator > 0\n        posterior[valid_den] = numerator[valid_den] / denominator[valid_den]\n        \n        return posterior\n\n    # 1. Supervised-only classifier\n    alpha_sup = alpha0 + n1\n    beta_sup = beta0 + n0\n    pi_sup_hat = alpha_sup / (alpha_sup + beta_sup)\n    p_sup_at_T = posterior_y_given_x(T, pi_sup_hat)\n\n    # 2. Semi-supervised classifier\n    # E-step: Compute responsibilities for unlabeled data\n    m = len(U)\n    N_1_U = 0.0\n    if m > 0:\n        responsibilities = posterior_y_given_x(U, pi_sup_hat)\n        N_1_U = np.sum(responsibilities)\n    \n    # M-step: Update pi estimate\n    alpha_semi = alpha_sup + N_1_U\n    #\n    # The total number of pseudo-observations is m.\n    # N_0_U = m - N_1_U\n    # beta_semi = beta_sup + N_0_U\n    # Combining these steps into one to avoid floating point issues with N_0_U:\n    beta_semi = beta_sup + (m - N_1_U)\n\n    pi_semi_hat = alpha_semi / (alpha_semi + beta_semi)\n    p_semi_at_T = posterior_y_given_x(T, pi_semi_hat)\n    \n    # 3. Compute posterior shifts\n    shifts = p_semi_at_T - p_sup_at_T\n    \n    return shifts.tolist()\n\nsolve()\n\n```", "id": "3104582"}]}