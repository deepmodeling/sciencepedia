{"hands_on_practices": [{"introduction": "本练习是推导非共轭模型变分更新的基础实践。我们将研究贝叶斯逻辑回归模型，其后验分布难以解析求解，并学习如何利用局部二次下界来推导参数变分分布的更新规则。掌握处理非共轭性的这项技能是将变分推断应用于众多现实世界问题的基石。[@problem_id:691486]", "problem": "考虑一个用于二元分类任务的贝叶斯逻辑回归模型。我们给定一个数据点 $(x, t)$，其中 $x=(x_1, x_2)^T \\in \\mathbb{R}^2$ 是特征向量， $t \\in \\{-1, 1\\}$ 是对应的类别标签。标签的似然由 $p(t|w,x) = \\sigma(t w^T x)$ 给出，其中 $w=(w_1, w_2)^T$ 是权重向量，$\\sigma(z) = (1+e^{-z})^{-1}$ 是逻辑 sigmoid 函数。\n\n权重被赋予一个精度为 $\\alpha > 0$ 的零均值各向同性高斯先验：\n$$p(w) = \\mathcal{N}(w | 0, \\alpha^{-1}I)$$\n其中 $I$ 是 $2 \\times 2$ 的单位矩阵。\n\n真实的后验分布 $p(w|x,t)$ 是难以处理的。我们使用变分推断和一个分解的高斯后验近似（平均场）来近似它：\n$$q(w) = q_1(w_1)q_2(w_2) = \\mathcal{N}(w_1|\\mu_1, v_1)\\mathcal{N}(w_2|\\mu_2, v_2)$$\n\n为了处理似然和先验的非共轭性，证据下界（ELBO）中的对数似然项被近似。项 $\\log \\sigma(z)$ 被一个包含变分参数 $\\xi \\in \\mathbb{R}$ 的二次函数所下界：\n$$ \\log \\sigma(z) \\ge \\log \\sigma(\\xi) + \\frac{1}{2}(z - \\xi) - \\lambda(\\xi)(z^2 - \\xi^2) $$\n其中函数 $\\lambda(\\xi)$ 定义为：\n$$ \\lambda(\\xi) = \\frac{\\tanh(\\xi/2)}{4\\xi} $$\n\n变分参数被迭代更新。考虑分布 $q_1(w_1)$ 的单次更新步骤。假设其他因子（$\\mu_2$）的参数和局部界（$\\xi$）是固定的，找出最大化 ELBO 的最优均值 $\\mu_1$ 的表达式。你的答案应使用 $x_1, x_2, t, \\alpha, \\xi$ 和 $\\mu_2$ 来表示。", "solution": "1. 对对数似然项进行下界处理：\n$$\\log\\sigma(tw^T x)\\ge\\log\\sigma(\\xi)+\\frac12\\,t(w_1x_1+w_2x_2)-\\lambda(\\xi)\\bigl[t^2(w_1x_1+w_2x_2)^2-\\xi^2\\bigr].$$\n\n2. 对数先验：\n$$\\log p(w)=-\\frac12\\alpha(w_1^2+w_2^2)+\\text{const}.$$\n\n3. 构造 $w_1$ 的变分指数部分：\n$$\nE_{q_2}[\\log p(w)+\\log\\sigma(tw^T x)]\n=\\;-\\tfrac12\\,\\alpha\\,w_1^2\n+\\underbrace{\\tfrac12\\,t\\,x_1\\,w_1}_{\\text{来自线性项}}\n-\\lambda(\\xi)\\,E_{q_2}[(w_1x_1+w_2x_2)^2]\n+\\text{const}.\n$$\n\n4. 计算 $E_{q_2}[(w_1x_1+w_2x_2)^2]$：\n$$\n(w_1x_1+\\mu_2x_2)^2+v_2x_2^2\n=w_1^2x_1^2+2w_1x_1\\mu_2x_2+\\text{const}.\n$$\n\n5. 收集关于 $w_1$ 的二次项和线性项：\n- 二次项：$-\\tfrac12(\\alpha+2\\lambda(\\xi)x_1^2)w_1^2$。\n- 线性项：$(\\tfrac12t x_1-2\\lambda(\\xi)x_1x_2\\mu_2)w_1$。\n\n6. 读出高斯分布的参数：\n$$\n\\text{精度}=\\alpha+2\\lambda(\\xi)x_1^2,\\quad\n\\text{自然均值}=\\tfrac12t x_1-2\\lambda(\\xi)x_1x_2\\mu_2\n$$\n所以\n$$\n\\mu_1=\\frac{\\tfrac12t x_1-2\\lambda(\\xi)x_1x_2\\mu_2}{\\alpha+2\\lambda(\\xi)x_1^2}.\n$$", "answer": "$$\\boxed{\\frac{\\tfrac12\\,t\\,x_1 \\;-\\;2\\,\\lambda(\\xi)\\,x_1\\,x_2\\,\\mu_2}{\\alpha+2\\,\\lambda(\\xi)\\,x_1^2}}$$", "id": "691486"}, {"introduction": "在单步推导的基础上，本练习要求您为一个高斯混合模型实现一个完整的坐标上升变分推断（CAVI）算法。您将推导潜在分配和成分均值的迭代更新方程，从而获得关于如何协同优化变分因子的实践经验。此练习还将提供对混合模型中常见的“标签切换”现象的实用见解。[@problem_id:3191998]", "problem": "考虑一个一维、双组分高斯混合模型，该模型具有潜在的组分指示变量和未知的组分均值。设模型有 $K=2$ 个组分和 $N$ 个标量观测值 $\\{x_n\\}_{n=1}^N$。其生成模型由以下经过充分检验的结构定义：\n- 潜在指示变量 $z_n \\in \\{1,2\\}$ 从一个分类分布中独立抽取，该分布的混合权重为 $\\pi = (\\pi_1,\\pi_2)$，其中 $\\pi_k \\in (0,1)$ 且 $\\pi_1 + \\pi_2 = 1$。\n- 在给定 $z_n = k$ 和组分均值 $\\mu_k$ 的条件下，每个观测值 $x_n$ 从一个已知方差为 $\\sigma^2$ 的高斯分布中抽取，即 $x_n \\mid (z_n=k,\\mu_k) \\sim \\mathcal{N}(\\mu_k,\\sigma^2)$。\n- 未知的组分均值具有独立的高斯先验分布 $\\mu_k \\sim \\mathcal{N}(m_{0k},\\tau_{0k}^{-1})$，其中 $\\tau_{0k} > 0$ 是先验精度，而 $m_{0k}$ 是先验均值。\n\n使用变分推断中的证据下界 (ELBO) 定义以及平均场分解 $q(z,\\mu) = \\left(\\prod_{n=1}^N q(z_n)\\right)\\left(\\prod_{k=1}^2 q(\\mu_k)\\right)$，从第一性原理推导坐标上升变分更新的公式，用于更新：\n1. 责任度 $r_{nk} = q(z_n = k)$，其中 $n \\in \\{1,\\dots,N\\}$ 且 $k \\in \\{1,2\\}$。\n2. 变分因子 $q(\\mu_k)$，包括其均值和精度的闭式表达式。\n\n将您的推导建立在基础的 ELBO 恒等式 $\\mathcal{L}(q) = \\mathbb{E}_q[\\log p(x,z,\\mu)] - \\mathbb{E}_q[\\log q(z,\\mu)]$ 以及每个因子的最优平均场因子更新规则 $q^*(\\theta_i) \\propto \\exp\\left(\\mathbb{E}_{q(\\theta_{\\setminus i})}[\\log p(x,\\theta)]\\right)$之上。除了这些定义，不要使用任何捷径公式。\n\n推导出更新公式后，实现一个确定性的坐标上升算法，该算法：\n- 将责任度均匀初始化，即对所有 $n,k$，$r_{nk} = 1/2$。\n- 交替更新 $q(\\mu_k)$ 的参数和责任度 $r_{nk}$。\n- 当连续两次迭代中所有 $n,k$ 的责任度绝对差之和小于或等于容忍度 $T$ 时，或达到最大迭代次数时停止。\n\n使用以下测试套件来评估标签切换（label switching）的影响和一个边界情况。对于每个测试，根据需要运行算法两次，并根据描述比较结果。在比较责任度时，使用容忍度 $T = 10^{-8}$。\n\n- 测试 1 (理想路径，完全置换下的标签不变性)：\n  - 观测值：$x = [\\, -2.2,\\, -1.9,\\, -1.7,\\, 1.5,\\, 1.8,\\, 2.2 \\,]$。\n  - 已知方差：$\\sigma^2 = 0.25$。\n  - 混合权重：$\\pi = [\\, 0.5,\\, 0.5 \\,]$。\n  - 先验均值：$m_0 = [\\, -2.0,\\, 2.0 \\,]$。\n  - 先验精度：$\\tau_0 = [\\, 1.0,\\, 1.0 \\,]$。\n  运行算法以获得责任度 $R^{(A)}$。然后再次运行算法，但这次将标签完全置换，即交换混合权重和先验均值：$\\pi' = [\\, 0.5,\\, 0.5 \\,]$, $m_0' = [\\, 2.0,\\, -2.0 \\,]$, $\\tau_0' = [\\, 1.0,\\, 1.0 \\,]$，生成 $R^{(B)}$。检查 $R^{(A)}$ 是否在列置换后与 $R^{(B)}$ 相等，容忍度为 $T$。结果应为一个布尔值。\n\n- 测试 2 (边缘情况，破坏不变性的部分置换)：\n  - 观测值：$x = [\\, -2.2,\\, -1.9,\\, -1.7,\\, 1.5,\\, 1.8,\\, 2.2 \\,]$。\n  - 已知方差：$\\sigma^2 = 0.25$。\n  - 混合权重：$\\pi = [\\, 0.7,\\, 0.3 \\,]$。\n  - 先验均值：$m_0 = [\\, -2.0,\\, 2.0 \\,]$。\n  - 先验精度：$\\tau_0 = [\\, 1.0,\\, 1.0 \\,]$。\n  运行算法以获得责任度 $R^{(C)}$。然后再次运行算法，但这次交换先验均值，而混合权重保持原始顺序（部分置换）：$m_0' = [\\, 2.0,\\, -2.0 \\,]$, $\\pi' = [\\, 0.7,\\, 0.3 \\,]$，生成 $R^{(D)}$。确定即使在列置换后，$R^{(C)}$ 与 $R^{(D)}$ 的差异是否也超出了容忍度 $T$。结果应为一个布尔值，表示非不变性。\n\n- 测试 3 (边界条件，不可区分的组分)：\n  - 观测值：$x = [\\, -1.0,\\, 1.0 \\,]$。\n  - 已知方差：$\\sigma^2 = 1.0$。\n  - 混合权重：$\\pi = [\\, 0.5,\\, 0.5 \\,]$。\n  - 先验均值：$m_0 = [\\, 0.0,\\, 0.0 \\,]$。\n  - 先验精度：$\\tau_0 = [\\, 100.0,\\, 100.0 \\,]$。\n  运行算法一次，并计算所有数据点和组分上责任度与 $0.5$ 的最大绝对偏差。结果应为一个浮点数。\n\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表的结果（例如， $[\\,\\text{result1},\\text{result2},\\text{result3}\\,]$），其中 $\\text{result1}$ 和 $\\text{result2}$ 是测试 1 和 2 的布尔值，$\\text{result3}$ 是测试 3 的浮点数。", "solution": "用户提供的问题是有效的。这是一个统计学习领域中定义明确的问题，具体涉及变分推断。它具有清晰一致的模型定义、具体的算法任务和可验证的测试用例。所有必要的数据和参数都已提供。\n\n问题是为一个双组分高斯混合模型 (GMM) 推导并实现一个坐标上升变分推断 (CAVI) 算法。推导将基于平均场近似和最优变分因子的一般形式。\n\n### 1. 模型规范\n\n设观测数据为一组 $N$ 个标量 $\\{x_n\\}_{n=1}^N$。潜在变量是组分均值 $\\mu = \\{\\mu_1, \\mu_2\\}$ 和组分分配 $z = \\{z_n\\}_{n=1}^N$，其中每个 $z_n \\in \\{1, 2\\}$。为方便起见，我们使用独热编码表示 $z_n$，即如果第 $n$ 个观测值来自组分 $k$，则 $z_{nk}=1$，否则 $z_{nk}=0$。\n\n生成过程如下：\n- 每个组分均值的先验是独立的高斯分布：\n$$p(\\mu_k) = \\mathcal{N}(\\mu_k | m_{0k}, \\tau_{0k}^{-1})$$\n其中 $m_{0k}$ 是先验均值，$\\tau_{0k} > 0$ 是先验精度。\n- 潜在组分指示变量 $z_n$ 从一个具有已知混合权重 $\\pi = (\\pi_1, \\pi_2)$ 的分类分布中抽取：\n$$p(z_n | \\pi) = \\prod_{k=1}^2 \\pi_k^{z_{nk}}$$\n- 每个观测值 $x_n$ 在给定其分配的组分 $k$ 和相应均值 $\\mu_k$ 的条件下，从一个已知方差为 $\\sigma^2$ 的高斯分布中抽取：\n$$p(x_n | z_n, \\mu) = \\prod_{k=1}^2 \\mathcal{N}(x_n | \\mu_k, \\sigma^2)^{z_{nk}}$$\n\n所有变量（观测变量和潜在变量）的完整联合概率分布由下式给出：\n$$p(x, z, \\mu) = p(\\mu) p(z | \\pi) p(x | z, \\mu) = \\left(\\prod_{k=1}^2 p(\\mu_k)\\right) \\left(\\prod_{n=1}^N p(z_n | \\pi)\\right) \\left(\\prod_{n=1}^N p(x_n | z_n, \\mu)\\right)$$\n联合分布的对数是：\n$$\\log p(x, z, \\mu) = \\sum_{k=1}^2 \\log p(\\mu_k) + \\sum_{n=1}^N \\sum_{k=1}^2 z_{nk} \\log \\pi_k + \\sum_{n=1}^N \\sum_{k=1}^2 z_{nk} \\log \\mathcal{N}(x_n | \\mu_k, \\sigma^2)$$\n\n### 2. 变分推断设置\n\n我们使用一个平均场变分族来近似真实的后验分布 $p(z, \\mu | x)$。该变分分布 $q(z, \\mu)$ 可分解为：\n$$q(z, \\mu) = q(z)q(\\mu) = \\left(\\prod_{n=1}^N q(z_n)\\right) \\left(\\prod_{k=1}^2 q(\\mu_k)\\right)$$\n其中每个 $q(z_n)$ 是一个在 $K=2$ 个组分上的分类分布，由概率 $r_{nk} = q(z_n=k)$ 表征，而每个 $q(\\mu_k)$ 是关于组分 $k$ 的均值的分布。\n\n坐标上升算法通过固定其他因子来迭代优化每个因子 $q(\\theta_i)$。因子 $q^*(\\theta_i)$ 的最优形式由下式给出：\n$$\\log q^*(\\theta_i) = \\mathbb{E}_{q(\\theta_{\\setminus i})}[\\log p(x, \\theta)] + \\mathrm{constant}$$\n其中 $\\theta_i$ 是潜在变量之一（$z_n$ 或 $\\mu_k$），而 $\\theta_{\\setminus i}$ 表示所有其他潜在变量。\n\n### 3. $q(\\mu_k)$ 更新公式的推导\n\n为了找到 $q(\\mu_k)$ 的最优形式，我们应用通用更新规则。最优分布 $q^*(\\mu_k)$ 的对数与对数联合概率关于所有其他因子 $q(z)$ 和 $q(\\mu_{j \\neq k})$ 的期望成正比：\n$$\\log q^*(\\mu_k) = \\mathbb{E}_{q(z)}[\\log p(x, z, \\mu)] + \\mathrm{constant}$$\n我们只需要考虑 $\\log p(x, z, \\mu)$ 中依赖于 $\\mu_k$ 的项：\n$$\\log q^*(\\mu_k) = \\mathbb{E}_{q(z)}\\left[\\sum_{n=1}^N z_{nk} \\log \\mathcal{N}(x_n | \\mu_k, \\sigma^2)\\right] + \\log \\mathcal{N}(\\mu_k | m_{0k}, \\tau_{0k}^{-1}) + \\mathrm{const}$$\n期望 $\\mathbb{E}_{q(z)}[z_{nk}]$ 是责任度 $r_{nk}$。令 $\\tau = 1/\\sigma^2$ 为已知的数据精度。\n$$\\log q^*(\\mu_k) = \\sum_{n=1}^N r_{nk} \\left(-\\frac{1}{2} \\log(2\\pi\\sigma^2) - \\frac{\\tau}{2}(x_n - \\mu_k)^2\\right) + \\left(-\\frac{1}{2} \\log(2\\pi\\tau_{0k}^{-1}) - \\frac{\\tau_{0k}}{2}(\\mu_k - m_{0k})^2\\right) + \\mathrm{const}$$\n为了确定 $q^*(\\mu_k)$ 的形式，我们收集包含 $\\mu_k$ 的项：\n$$\\log q^*(\\mu_k) = -\\frac{\\tau}{2} \\sum_{n=1}^N r_{nk}(x_n^2 - 2x_n\\mu_k + \\mu_k^2) - \\frac{\\tau_{0k}}{2}(\\mu_k^2 - 2\\mu_k m_{0k} + m_{0k}^2) + \\mathrm{const}$$\n按 $\\mu_k$ 的幂次对各项进行分组：\n- 含 $\\mu_k^2$ 的项：$-\\frac{1}{2}\\mu_k^2 \\left(\\tau_{0k} + \\tau \\sum_{n=1}^N r_{nk}\\right)$\n- 含 $\\mu_k$ 的项：$\\mu_k \\left(\\tau_{0k}m_{0k} + \\tau \\sum_{n=1}^N r_{nk}x_n\\right)$\n这是关于 $\\mu_k$ 的二次型，是高斯密度对数的特征。一个高斯分布 $\\mathcal{N}(\\mu | m, \\lambda^{-1})$ 的对数密度形式为 $-\\frac{\\lambda}{2}\\mu^2 + \\lambda m \\mu + \\mathrm{const}$。\n通过匹配系数，我们发现 $q^*(\\mu_k)$ 是一个高斯分布 $\\mathcal{N}(\\mu_k | m_k, \\tau_k^{-1})$，其精度为 $\\tau_k$，均值为 $m_k$：\n$$\\tau_k = \\tau_{0k} + \\tau \\sum_{n=1}^N r_{nk}$$\n$$m_k = \\frac{\\tau_{0k}m_{0k} + \\tau \\sum_{n=1}^N r_{nk}x_n}{\\tau_k}$$\n这些就是变分分布 $q(\\mu_k)$ 参数的更新方程。\n\n### 4. $q(z_n)$ 更新公式的推导\n\n类似地，我们通过取对数联合概率关于所有其他因子 $\\{q(z_j)\\}_{j \\neq n}$ 和 $\\{q(\\mu_k)\\}_{k=1}^2$ 的期望来找到 $q(z_n)$ 的最优形式：\n$$\\log q^*(z_n) = \\mathbb{E}_{q(\\mu)}[\\log p(x, z, \\mu)] + \\mathrm{constant}$$\n我们收集依赖于 $z_n$ 的项：\n$$\\log q^*(z_n) = \\sum_{k=1}^2 z_{nk} \\log \\pi_k + \\sum_{k=1}^2 z_{nk} \\mathbb{E}_{q(\\mu_k)}[\\log \\mathcal{N}(x_n | \\mu_k, \\sigma^2)] + \\mathrm{const}$$\n这意味着 $q^*(z_n)$ 是一个分类分布。组分 $k$ 的对数概率是：\n$$\\log q^*(z_n=k) \\equiv \\log r_{nk} \\propto \\log \\pi_k + \\mathbb{E}_{q(\\mu_k)}[\\log \\mathcal{N}(x_n | \\mu_k, \\sigma^2)]$$\n我们来展开这个期望项：\n$$\\mathbb{E}_{q(\\mu_k)}[\\log \\mathcal{N}(x_n | \\mu_k, \\sigma^2)] = \\mathbb{E}_{q(\\mu_k)}\\left[-\\frac{1}{2}\\log(2\\pi\\sigma^2) - \\frac{\\tau}{2}(x_n - \\mu_k)^2\\right]$$\n$$= -\\frac{1}{2}\\log(2\\pi\\sigma^2) - \\frac{\\tau}{2} \\mathbb{E}_{q(\\mu_k)}[x_n^2 - 2x_n\\mu_k + \\mu_k^2]$$\n$$= -\\frac{1}{2}\\log(2\\pi\\sigma^2) - \\frac{\\tau}{2} (x_n^2 - 2x_n\\mathbb{E}[\\mu_k] + \\mathbb{E}[\\mu_k^2])$$\n根据我们推导出的 $q(\\mu_k) = \\mathcal{N}(\\mu_k | m_k, \\tau_k^{-1})$ 的形式，我们有：\n$$\\mathbb{E}[\\mu_k] = m_k$$\n$$\\mathbb{E}[\\mu_k^2] = \\mathrm{Var}[\\mu_k] + (\\mathbb{E}[\\mu_k])^2 = \\tau_k^{-1} + m_k^2$$\n将这些代入 $\\log q^*(z_n=k)$ 的表达式中，并舍去与 $k$ 无关的常数项（如 $-\\frac{1}{2}\\log(2\\pi\\sigma^2)$ 和 $-\\frac{\\tau}{2}x_n^2$）：\n$$\\log \\tilde{\\rho}_{nk} \\propto \\log \\pi_k + \\tau x_n \\mathbb{E}[\\mu_k] - \\frac{\\tau}{2} \\mathbb{E}[\\mu_k^2]$$\n$$\\log \\tilde{\\rho}_{nk} = \\log \\pi_k + \\tau x_n m_k - \\frac{\\tau}{2}(m_k^2 + \\tau_k^{-1})$$\n责任度 $r_{nk} = q(z_n=k)$ 通过对指数化后的值进行归一化得到：\n$$r_{nk} = \\frac{\\exp(\\log \\tilde{\\rho}_{nk})}{\\sum_{j=1}^2 \\exp(\\log \\tilde{\\rho}_{nj})}$$\n\n### 5. 算法总结\n\n坐标上升变分推断 (CAVI) 算法的步骤如下：\n1.  **初始化**：初始化责任度 $r_{nk}$ (例如，均匀初始化为 $r_{nk} = 1/2$）。\n2.  **迭代**直至收敛：\n    a. **更新 $q(\\mu)$ (类 M 步)**：对于每个组分 $k=1,2$，使用当前的责任度 $r_{nk}$ 更新变分分布 $q(\\mu_k)$ 的参数 $m_k$ 和 $\\tau_k$：\n       $$\\tau_k \\leftarrow \\tau_{0k} + \\tau \\sum_{n=1}^N r_{nk}$$\n       $$m_k \\leftarrow \\frac{\\tau_{0k}m_{0k} + \\tau \\sum_{n=1}^N r_{nk}x_n}{\\tau_k}$$\n    b. **更新 $q(z)$ (类 E 步)**：对于每个数据点 $n=1,\\dots,N$，使用更新后的 $q(\\mu)$ 参数来更新责任度 $r_{nk}$：\n       $$\\log \\tilde{\\rho}_{nk} \\leftarrow \\log \\pi_k + \\tau x_n m_k - \\frac{\\tau}{2}(m_k^2 + \\tau_k^{-1})$$\n       $$r_{nk} \\leftarrow \\frac{\\exp(\\log \\tilde{\\rho}_{nk})}{\\sum_{j=1}^2 \\exp(\\log \\tilde{\\rho}_{nj})}$$\n    c. **检查收敛性**：如果责任度的变化小于容忍度 $T$ 或达到最大迭代次数，则停止。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the variational inference problem for the provided test cases.\n    \"\"\"\n\n    def run_cavi(x, sigma_sq, pi, m0, tau0, tol, max_iter=100):\n        \"\"\"\n        Runs the Coordinate Ascent Variational Inference algorithm for a GMM.\n\n        Args:\n            x (np.ndarray): 1D array of observations.\n            sigma_sq (float): Known variance of the Gaussian components.\n            pi (np.ndarray): 1D array of mixing weights.\n            m0 (np.ndarray): 1D array of prior means for component means.\n            tau0 (np.ndarray): 1D array of prior precisions for component means.\n            tol (float): Convergence tolerance for responsibilities.\n            max_iter (int): Maximum number of iterations.\n\n        Returns:\n            np.ndarray: A (N, K) array of final responsibilities.\n        \"\"\"\n        N = x.shape[0]\n        K = pi.shape[0]\n        tau = 1.0 / sigma_sq\n\n        # Initialize responsibilities uniformly\n        r_nk = np.full((N, K), 1.0 / K)\n\n        for i in range(max_iter):\n            r_nk_old = r_nk.copy()\n\n            # M-step: Update q(mu_k)\n            # Sum of responsibilities for each component\n            N_k = np.sum(r_nk, axis=0) # Shape (K,)\n            # Update variational precision for mu_k\n            tau_k = tau0 + tau * N_k # Shape (K,)\n            # Update variational mean for mu_k\n            # r_nk.T is (K,N), x is (N,). (r_nk.T @ x) is sum(r_nk * x) for each k\n            sum_r_x = r_nk.T @ x # Shape (K,)\n            m_k = (tau0 * m0 + tau * sum_r_x) / tau_k # Shape (K,)\n\n            # E-step: Update q(z_n), i.e., the responsibilities r_nk\n            E_mu_k_sq = 1.0 / tau_k + m_k**2 # Shape (K,)\n            \n            # Use broadcasting for efficiency. x[:, np.newaxis] is (N,1)\n            # m_k and E_mu_k_sq are (K,) which broadcasts to (N,K)\n            log_rho_nk = np.log(pi) + tau * x[:, np.newaxis] * m_k - (tau / 2) * E_mu_k_sq\n            \n            # Log-sum-exp trick for numerical stability\n            log_rho_nk_max = np.max(log_rho_nk, axis=1, keepdims=True)\n            log_rho_nk_stable = log_rho_nk - log_rho_nk_max\n            rho_nk = np.exp(log_rho_nk_stable)\n            \n            # Normalize to get responsibilities\n            r_nk = rho_nk / np.sum(rho_nk, axis=1, keepdims=True)\n\n            # Check for convergence\n            diff = np.sum(np.abs(r_nk - r_nk_old))\n            if diff = tol:\n                break\n        \n        return r_nk\n\n    T = 10**-8\n    results = []\n\n    # Test 1: Happy path, label invariance under full permutation\n    x1 = np.array([-2.2, -1.9, -1.7, 1.5, 1.8, 2.2])\n    sigma_sq1 = 0.25\n    pi_A = np.array([0.5, 0.5])\n    m0_A = np.array([-2.0, 2.0])\n    tau0_1 = np.array([1.0, 1.0])\n    R_A = run_cavi(x1, sigma_sq1, pi_A, m0_A, tau0_1, T)\n\n    pi_B = np.array([0.5, 0.5]) # Same pi\n    m0_B = np.array([2.0, -2.0]) # Swapped means\n    tau0_B = np.array([1.0, 1.0])\n    R_B = run_cavi(x1, sigma_sq1, pi_B, m0_B, tau0_B, T)\n\n    # Check for equality up to column permutation (label switching)\n    is_invariant = np.allclose(R_A, R_B, atol=T) or np.allclose(R_A, R_B[:, ::-1], atol=T)\n    results.append(is_invariant)\n\n    # Test 2: Edge case, partial permutation that breaks invariance\n    x2 = np.array([-2.2, -1.9, -1.7, 1.5, 1.8, 2.2])\n    sigma_sq2 = 0.25\n    pi_C = np.array([0.7, 0.3])\n    m0_C = np.array([-2.0, 2.0])\n    tau0_2 = np.array([1.0, 1.0])\n    R_C = run_cavi(x2, sigma_sq2, pi_C, m0_C, tau0_2, T)\n    \n    pi_D = np.array([0.7, 0.3]) # Unswapped pi\n    m0_D = np.array([2.0, -2.0]) # Swapped means\n    tau0_D = np.array([1.0, 1.0])\n    R_D = run_cavi(x2, sigma_sq2, pi_D, m0_D, tau0_D, T)\n\n    # Check if results are different even after accounting for label switching\n    is_non_invariant = not (np.allclose(R_C, R_D, atol=T) or np.allclose(R_C, R_D[:, ::-1], atol=T))\n    results.append(is_non_invariant)\n\n    # Test 3: Boundary condition, indistinguishable components\n    x3 = np.array([-1.0, 1.0])\n    sigma_sq3 = 1.0\n    pi_3 = np.array([0.5, 0.5])\n    m0_3 = np.array([0.0, 0.0])\n    tau0_3 = np.array([100.0, 100.0])\n    R_3 = run_cavi(x3, sigma_sq3, pi_3, m0_3, tau0_3, T)\n    max_dev = np.max(np.abs(R_3 - 0.5))\n    results.append(max_dev)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3191998"}, {"introduction": "在处理海量数据集时，全批量变分推断在计算上是不可行的。本练习介绍随机变分推断（SVI），并通过推导和实现一种自适应批量大小策略，来探索计算成本与梯度精度之间的关键权衡。通过分析随机梯度的方差，您将学习如何智能地选择小批量的大小，以确保高效稳定的收敛。[@problem_id:3192033]", "problem": "在统计学习中，考虑一个单变量正态-正态模型，其中观测值是独立同分布的。观测数据为 $\\{y_i\\}_{i=1}^N$，其中 $y_i \\mid \\mu \\sim \\mathcal{N}(\\mu,\\sigma^2)$，方差 $\\sigma^2$ 已知，先验分布为 $\\mu \\sim \\mathcal{N}(\\mu_0,\\tau_0^2)$。设变分族为 $q(\\mu)=\\mathcal{N}(m,v)$，其变分参数为 $m$ 和 $v$。优化目标是证据下界 (Evidence Lower Bound, ELBO)，记为 $\\mathcal{L}(m,v)$，我们使用随机变分推断 (Stochastic Variational Inference, SVI) 对 $\\mathcal{L}(m,v)$ 执行梯度上升。\n\n从 ELBO 的基本定义出发，\n$$\n\\mathcal{L}(m,v) = \\mathbb{E}_{q(\\mu)}[\\log p(y,\\mu)] - \\mathbb{E}_{q(\\mu)}[\\log q(\\mu)],\n$$\n其中 $p(y,\\mu)=p(y\\mid\\mu)p(\\mu)$ 且 $\\log p(y\\mid\\mu) = -\\frac{N}{2}\\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2}\\sum_{i=1}^N (y_i-\\mu)^2$，以及 $\\log p(\\mu) = -\\frac{1}{2}\\log(2\\pi\\tau_0^2) - \\frac{1}{2\\tau_0^2}(\\mu-\\mu_0)^2$，推导关于变分均值 $m$ 的梯度，记为 $\\nabla_m \\mathcal{L}(m,v)$。推导过程仅依赖于正态随机变量期望的基本性质以及期望符号下的微分线性性质。在推导中，除了这些基础知识外，不要提供或使用任何快捷公式。\n\n通过从 $\\{1,\\dots,N\\}$ 中进行有放回的均匀随机抽样，定义一个大小为 $b$ 的小批量 (minibatch)。使用这样的小批量构建 $\\nabla_m \\mathcal{L}(m,v)$ 的一个无偏估计量。然后，从无偏性要求和采样索引的独立性出发，推导这个基于小批量的梯度估计量的方差，将其表示为 $b$ 的函数。假设方差仅由数据似然项产生，而先验项的贡献相对于小批量抽样是确定性的。\n\n为了研究收敛与计算成本之间的权衡，设每次迭代的壁钟时间 (wall-clock time) 由线性函数 $t(b)=t_0+t_1 b$ 建模，其中常数 $t_00$ 和 $t_10$。将小批量梯度估计量的信噪比 (Signal-to-Noise Ratio, SNR) 定义为\n$$\n\\mathrm{SNR}(b)=\\frac{\\left(\\mathbb{E}[\\hat{g}_b]\\right)^2}{\\mathrm{Var}(\\hat{g}_b)},\n$$\n其中 $\\hat{g}_b$ 是在当前参数 $m$ 处 $\\nabla_m \\mathcal{L}(m,v)$ 的无偏估计量。提出一种自适应批量大小策略 (adaptive batching strategy)，在给定的迭代步 $m$ 处，选择能够达到用户指定的信噪比阈值 $\\rho0$ 的最小批量大小 $b$，并满足约束条件 $1 \\le b \\le N$。\n\n您的任务是实现一个完整、可运行的程序，该程序能够：\n- 为每个测试用例生成合成数据，通过对 $i=1,\\dots,N$ 独立抽取 $y_i \\sim \\mathcal{N}(\\mu_{\\mathrm{true}},\\sigma^2)$，并使用提供的随机种子以确保可复现性。\n- 在给定的 $m$ 初始值处计算精确梯度 $\\nabla_m \\mathcal{L}(m,v)$（注意，在此模型中，关于 $m$ 的梯度不依赖于 $v$）。\n- 在给定的 $m$ 处，计算整个数据集中数据似然项对梯度的单样本贡献的方差。\n- 应用您提出的自适应批量大小规则选择满足信噪比阈值 $\\rho$ 的 $b$，并将结果裁剪到区间 $[1,N]$ 内。\n- 报告所有测试用例所选的批量大小（整数）。\n\n测试套件：\n- 案例 $1$ (常规路径)：$N=512$，$\\sigma^2=1.0$，$\\mu_0=0.0$，$\\tau_0^2=10.0$，$\\mu_{\\mathrm{true}}=1.0$， $m_{\\mathrm{init}}=-2.0$，$\\rho=5.0$， $t_0=10^{-4}$， $t_1=10^{-6}$，$\\text{seed}=123$。\n- 案例 $2$ (边界条件：接近最优解时的高信噪比)：$N=64$，$\\sigma^2=2.0$，$\\mu_0=0.0$，$\\tau_0^2=5.0$，$\\mu_{\\mathrm{true}}=0.5$， $m_{\\mathrm{init}}=0.45$，$\\rho=50.0$， $t_0=10^{-4}$， $t_1=10^{-6}$，$\\text{seed}=321$。\n- 案例 $3$ (边缘情况：大数据集，远离最优解)：$N=1024$，$\\sigma^2=0.5$，$\\mu_0=0.0$，$\\tau_0^2=20.0$，$\\mu_{\\mathrm{true}}=-1.5$， $m_{\\mathrm{init}}=3.0$，$\\rho=1.0$， $t_0=10^{-4}$， $t_1=10^{-6}$，$\\text{seed}=999$。\n\n所有量都没有物理单位，应被视为无量纲。您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表（例如，“[result1,result2,result3]”），每个结果是对应测试用例所选的自适应批量大小 $b$（一个整数），按案例 1、2 和 3 的顺序排列。", "solution": "经评估，用户提供的问题是有效的，因为它具有科学依据、内容自洽、提法明确且客观。它提出了一个统计学习中的标准问题，可以通过严谨的数学推导和实现来解决。\n\n解决方案分为四个阶段进行：\n1.  推导证据下界 (ELBO) 关于变分均值参数 $m$ 的梯度。\n2.  使用小批量构建无偏随机梯度估计量。\n3.  推导该随机估计量的方差。\n4.  基于信噪比 (SNR) 制定自适应批量大小规则。\n\n### 1. 推导 ELBO 梯度 $\\nabla_m \\mathcal{L}(m,v)$\n\n变分分布 $q(\\mu) = \\mathcal{N}(m,v)$ 的证据下界 (ELBO) 由下式给出：\n$$\n\\mathcal{L}(m,v) = \\mathbb{E}_{q(\\mu)}[\\log p(y,\\mu)] - \\mathbb{E}_{q(\\mu)}[\\log q(\\mu)]\n$$\n其中 $y = \\{y_i\\}_{i=1}^N$。我们旨在求取关于变分均值的梯度 $\\nabla_m \\mathcal{L}(m,v)$。\n\n对于像正态分布 $q(\\mu; m, v)$ 这样的位置族分布，一个关键性质是关于位置参数 $m$ 的梯度可以与期望算子互换。这可以通过重参数化技巧 (reparameterization trick) 来理解，其中 $\\mu = m + \\sqrt{v}\\epsilon$ 且 $\\epsilon \\sim \\mathcal{N}(0,1)$。梯度变为 $\\nabla_m \\mathbb{E}_{\\epsilon}[f(m+\\sqrt{v}\\epsilon)] = \\mathbb{E}_{\\epsilon}[\\nabla_m f(m+\\sqrt{v}\\epsilon)] = \\mathbb{E}_{\\epsilon}[f'(\\mu)|_{\\mu=m+\\sqrt{v}\\epsilon} \\cdot 1] = \\mathbb{E}_{q(\\mu)}[\\nabla_{\\mu}f(\\mu)]$。应用此性质，我们得到：\n$$\n\\nabla_m \\mathcal{L}(m,v) = \\mathbb{E}_{q(\\mu)}[\\nabla_{\\mu}\\log p(y,\\mu)] - \\mathbb{E}_{q(\\mu)}[\\nabla_{\\mu}\\log q(\\mu)]\n$$\n我们来分析第二项。变分分布的得分函数 (score function) 是 $\\nabla_{\\mu}\\log q(\\mu)$：\n$$\n\\log q(\\mu) = \\log\\left(\\frac{1}{\\sqrt{2\\pi v}}\\exp\\left(-\\frac{(\\mu-m)^2}{2v}\\right)\\right) = -\\frac{1}{2}\\log(2\\pi v) - \\frac{(\\mu-m)^2}{2v}\n$$\n$$\n\\nabla_{\\mu}\\log q(\\mu) = \\frac{d}{d\\mu}\\left(-\\frac{(\\mu-m)^2}{2v}\\right) = -\\frac{2(\\mu-m)}{2v} = -\\frac{\\mu-m}{v}\n$$\n该得分函数的期望是：\n$$\n\\mathbb{E}_{q(\\mu)}[\\nabla_{\\mu}\\log q(\\mu)] = \\mathbb{E}_{q(\\mu)}\\left[-\\frac{\\mu-m}{v}\\right] = -\\frac{1}{v}(\\mathbb{E}_{q(\\mu)}[\\mu] - m) = -\\frac{1}{v}(m - m) = 0\n$$\n这是一个普遍结论：一个分布对其自身参数的得分期望为零。因此，ELBO 梯度简化为：\n$$\n\\nabla_m \\mathcal{L}(m,v) = \\mathbb{E}_{q(\\mu)}[\\nabla_{\\mu}\\log p(y,\\mu)]\n$$\n联合对数概率为 $\\log p(y,\\mu) = \\log p(y|\\mu) + \\log p(\\mu)$。我们对每一部分关于 $\\mu$ 求导：\n$$\n\\log p(y|\\mu) = \\sum_{i=1}^N \\log p(y_i|\\mu) = \\sum_{i=1}^N \\left(-\\frac{1}{2}\\log(2\\pi\\sigma^2) - \\frac{(y_i-\\mu)^2}{2\\sigma^2}\\right)\n$$\n$$\n\\nabla_{\\mu} \\log p(y|\\mu) = \\sum_{i=1}^N \\frac{d}{d\\mu} \\left(-\\frac{(y_i-\\mu)^2}{2\\sigma^2}\\right) = \\sum_{i=1}^N \\frac{2(y_i-\\mu)}{2\\sigma^2} = \\frac{1}{\\sigma^2}\\sum_{i=1}^N(y_i-\\mu)\n$$\n对于先验部分：\n$$\n\\log p(\\mu) = -\\frac{1}{2}\\log(2\\pi\\tau_0^2) - \\frac{(\\mu-\\mu_0)^2}{2\\tau_0^2}\n$$\n$$\n\\nabla_{\\mu} \\log p(\\mu) = \\frac{d}{d\\mu} \\left(-\\frac{(\\mu-\\mu_0)^2}{2\\tau_0^2}\\right) = -\\frac{2(\\mu-\\mu_0)}{2\\tau_0^2} = -\\frac{\\mu-\\mu_0}{\\tau_0^2}\n$$\n将这些结合起来，并对 $q(\\mu)$ 求期望，其中 $\\mathbb{E}_{q(\\mu)}[\\mu] = m$：\n$$\n\\nabla_m \\mathcal{L}(m,v) = \\mathbb{E}_{q(\\mu)}\\left[\\frac{1}{\\sigma^2}\\sum_{i=1}^N(y_i-\\mu) - \\frac{\\mu-\\mu_0}{\\tau_0^2}\\right]\n$$\n根据期望的线性性质：\n$$\n\\nabla_m \\mathcal{L}(m,v) = \\frac{1}{\\sigma^2}\\sum_{i=1}^N(y_i-\\mathbb{E}_{q(\\mu)}[\\mu]) - \\frac{\\mathbb{E}_{q(\\mu)}[\\mu]-\\mu_0}{\\tau_0^2}\n$$\n$$\n\\nabla_m \\mathcal{L}(m,v) = \\frac{1}{\\sigma^2}\\left(\\sum_{i=1}^N y_i - Nm\\right) + \\frac{\\mu_0-m}{\\tau_0^2}\n$$\n正如问题中所述，此梯度（我们记为 $g_m$）不依赖于变分方差 $v$。\n\n### 2. 随机梯度估计量\n\n完整梯度 $g_m$ 可以写成一个依赖数据的项和一个先验项的和：\n$$\ng_m = \\left(\\sum_{i=1}^N \\frac{y_i-m}{\\sigma^2}\\right) + \\left(\\frac{\\mu_0-m}{\\tau_0^2}\\right)\n$$\n在随机变分推断 (SVI) 中，对整个数据集的求和被近似。设小批量 $S$ 是从 $\\{1, \\dots, N\\}$ 中有放回均匀随机抽取的 $b$ 个索引的集合。对于和 $\\sum_{i=1}^N f(y_i)$ 的一个无偏估计量是 $\\frac{N}{b}\\sum_{j \\in S} f(y_j)$。将此应用于梯度的依赖数据部分，我们构建随机梯度估计量 $\\hat{g}_b$：\n$$\n\\hat{g}_b = \\frac{N}{b}\\sum_{j \\in S} \\frac{y_j-m}{\\sigma^2} + \\frac{\\mu_0-m}{\\tau_0^2}\n$$\n该估计量是无偏的，因为对小批量 $S$ 的随机抽样求期望可得：\n$$\n\\mathbb{E}_{S}[\\hat{g}_b] = \\mathbb{E}_{S}\\left[\\frac{N}{b}\\sum_{j \\in S} \\frac{y_j-m}{\\sigma^2}\\right] + \\frac{\\mu_0-m}{\\tau_0^2} = \\frac{N}{b} \\sum_{k=1}^b \\mathbb{E}_{j_k}\\left[\\frac{y_{j_k}-m}{\\sigma^2}\\right] + \\frac{\\mu_0-m}{\\tau_0^2}\n$$\n其中 $j_k$ 是小批量中的独立同分布索引。单次抽样的期望是 $\\mathbb{E}_{j_k}[\\dots] = \\frac{1}{N}\\sum_{i=1}^N \\frac{y_i-m}{\\sigma^2}$。\n$$\n\\mathbb{E}_{S}[\\hat{g}_b] = \\frac{N}{b} \\cdot b \\cdot \\left(\\frac{1}{N}\\sum_{i=1}^N \\frac{y_i-m}{\\sigma^2}\\right) + \\frac{\\mu_0-m}{\\tau_0^2} = g_m\n$$\n\n### 3. 随机估计量的方差\n\n$\\hat{g}_b$ 的方差源于小批量的随机抽样。先验项相对于此抽样是确定性的。\n$$\n\\mathrm{Var}(\\hat{g}_b) = \\mathrm{Var}\\left(\\frac{N}{b}\\sum_{j \\in S} \\frac{y_j-m}{\\sigma^2}\\right) = \\left(\\frac{N}{b}\\right)^2 \\mathrm{Var}\\left(\\sum_{j \\in S} \\frac{y_j-m}{\\sigma^2}\\right)\n$$\n由于 $S$ 中的索引是独立同分布抽取的，和的方差等于方差的和：\n$$\n\\mathrm{Var}\\left(\\sum_{j \\in S} \\frac{y_j-m}{\\sigma^2}\\right) = b \\cdot \\mathrm{Var}_{\\text{single sample}}\\left(\\frac{y_j-m}{\\sigma^2}\\right)\n$$\n我们将单个数据点对梯度的贡献定义为 $h_i = \\frac{y_i-m}{\\sigma^2}$。从集合 $\\{h_1, \\dots, h_N\\}$ 中单次抽样的方差是这些值的总体方差，我们记为 $V_h$：\n$$\nV_h = \\mathrm{Var}_{j \\sim \\mathrm{Unif}(\\{1..N\\})}(h_j) = \\frac{1}{N}\\sum_{i=1}^N h_i^2 - \\left(\\frac{1}{N}\\sum_{i=1}^N h_i\\right)^2\n$$\n结合这些结果，随机梯度估计量的方差为：\n$$\n\\mathrm{Var}(\\hat{g}_b) = \\left(\\frac{N}{b}\\right)^2 (b \\cdot V_h) = \\frac{N^2}{b}V_h\n$$\n\n### 4. 自适应批量大小策略\n\n估计量 $\\hat{g}_b$ 的信噪比 (SNR) 定义为：\n$$\n\\mathrm{SNR}(b) = \\frac{(\\mathbb{E}[\\hat{g}_b])^2}{\\mathrm{Var}(\\hat{g}_b)} = \\frac{g_m^2}{\\frac{N^2}{b}V_h} = \\frac{b g_m^2}{N^2 V_h}\n$$\n自适应批量大小规则要求找到满足 $\\mathrm{SNR}(b) \\ge \\rho$ 的最小整数批量大小 $b$，其中 $\\rho  0$ 是给定的阈值，且需满足约束 $1 \\le b \\le N$。\n我们建立不等式：\n$$\n\\frac{b g_m^2}{N^2 V_h} \\ge \\rho\n$$\n假设 $g_m^2  0$ 和 $V_h  0$，我们求解 $b$：\n$$\nb \\ge \\rho \\frac{N^2 V_h}{g_m^2}\n$$\n满足此条件的最小整数 $b$ 可通过对右侧表达式取上整 (ceiling) 得到。设 $b_{req} = \\rho \\frac{N^2 V_h}{g_m^2}$。则最小整数为 $\\lceil b_{req} \\rceil$。\n最后，我们通过裁剪结果来应用约束 $1 \\le b \\le N$：\n$$\nb_{\\text{adaptive}} = \\min(N, \\max(1, \\lceil b_{req} \\rceil))\n$$\n此公式给出了需要实现的批量大小。如果 $g_m=0$，信噪比为 $0$（当 $V_h0$ 时），因此任何正的 $\\rho$ 都无法满足。在这种情况下，$b_{req}$ 是无限大，裁剪到 $N$ 提供了可能的最大批量大小，以试图达到尽可能高的信噪比。如果 $V_h=0$，所有单样本梯度贡献都相同，噪声为零，此时 $b=1$ 就足够了。\n\n实现部分将根据数据和参数计算 $g_m$ 和 $V_h$，然后使用 $b_{\\text{adaptive}}$ 的公式来确定每个测试用例的批量大小。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nimport math\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # The parameters t_0 and t_1 are specified in the problem but are not needed\n    # for the adaptive batch size calculation, which is based solely on the SNR threshold rho.\n    test_cases = [\n        # (N, sigma^2, mu_0, tau_0^2, mu_true, m_init, rho, seed)\n        (512, 1.0, 0.0, 10.0, 1.0, -2.0, 5.0, 123),  # Case 1\n        (64, 2.0, 0.0, 5.0, 0.5, 0.45, 50.0, 321),    # Case 2\n        (1024, 0.5, 0.0, 20.0, -1.5, 3.0, 1.0, 999),  # Case 3\n    ]\n\n    results = []\n    for case in test_cases:\n        N, sigma2, mu0, tau02, mu_true, m_init, rho, seed = case\n        result = _calculate_adaptive_batch_size(N, sigma2, mu0, tau02, mu_true, m_init, rho, seed)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef _calculate_adaptive_batch_size(N, sigma2, mu0, tau02, mu_true, m_init, rho, seed):\n    \"\"\"\n    Calculates the adaptive batch size for a single test case.\n\n    Args:\n        N (int): Number of data points.\n        sigma2 (float): Known variance of the likelihood.\n        mu0 (float): Mean of the prior.\n        tau02 (float): Variance of the prior.\n        mu_true (float): True mean for data generation.\n        m_init (float): Initial value for the variational mean parameter m.\n        rho (float): Target Signal-to-Noise Ratio (SNR).\n        seed (int): Random seed for data generation.\n\n    Returns:\n        int: The computed adaptive batch size, clipped to [1, N].\n    \"\"\"\n    # 1. Generate synthetic data using the provided random seed for reproducibility.\n    rng = np.random.default_rng(seed)\n    sigma = np.sqrt(sigma2)\n    y = rng.normal(loc=mu_true, scale=sigma, size=N)\n\n    # 2. Compute the per-sample gradient contributions from the data-likelihood term.\n    # h_i = (y_i - m) / sigma^2\n    h = (y - m_init) / sigma2\n\n    # 3. Compute the exact full gradient of the ELBO w.r.t. m.\n    # g_m = sum(h_i) + (mu_0 - m) / tau_0^2\n    grad_likelihood_term = np.sum(h)\n    grad_prior_term = (mu0 - m_init) / tau02\n    g_m = grad_likelihood_term + grad_prior_term\n\n    # 4. Compute the population variance of the per-sample gradient contributions.\n    # V_h = Var({h_i})\n    # np.var calculates the population variance by default (ddof=0).\n    V_h = np.var(h)\n\n    # 5. Calculate the required batch size b_req to meet the SNR threshold rho.\n    # b_req = rho * (N^2 * V_h) / g_m^2\n    g_m_squared = g_m**2\n    if g_m_squared  1e-12:  # Handle numerical instability for near-zero gradients.\n        # If the gradient is zero, SNR is zero (for V_h > 0). Any rho > 0 is\n        # unachievable. We select the max batch size to get the best possible SNR.\n        b_req = np.inf\n    elif V_h == 0:\n        # If V_h is zero, the gradient estimator has zero variance.\n        # The SNR is infinite. The smallest batch size is sufficient.\n        b_req = 0.0\n    else:\n        b_req = rho * (N**2 * V_h) / g_m_squared\n\n    # 6. The smallest integer batch size is the ceiling of b_req.\n    b_adaptive = math.ceil(b_req)\n\n    # 7. Clip the result to the valid range [1, N] as per the problem constraints.\n    b_final = int(np.clip(b_adaptive, 1, N))\n\n    return b_final\n\nsolve()\n```", "id": "3192033"}]}