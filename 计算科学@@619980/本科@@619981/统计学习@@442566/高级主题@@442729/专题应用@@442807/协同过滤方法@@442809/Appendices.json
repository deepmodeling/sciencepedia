{"hands_on_practices": [{"introduction": "我们的第一个实践是基础性的，你将亲手实现一个完整的“学习到排序”（learning-to-rank）协同过滤模型。这个练习将教你如何将用户的评分数据（$R_{uj}$）转化为成对偏好约束（例如，如果用户 $u$ 对物品 $j$ 的评分高于物品 $k$，则模型应预测 $\\hat{R}_{uj} \\gt \\hat{R}_{uk}$），并使用随机梯度下降（SGD）来学习能够满足这些顺序的用户和物品嵌入向量。通过这个编码实践 [@problem_id:3110042]，你将深入理解这类模型内部的工作原理。", "problem": "要求您实现一个成对排序协同过滤算法，该算法强制执行从观测到的用户评分中派生的偏序约束。其背景是用于推荐系统的统计学习。您将使用一个潜因子模型和成对 hinge 损失从观测评分中学习用户和物品的嵌入，然后评估学习到的模型在多大程度上满足观测到的排序约束。\n\n请实现一个程序，对每个测试用例，从基本原理出发完成以下操作：\n- 将用户-物品评分矩阵中的条目视为序数偏好的观测值。缺失条目编码为 $0$，不携带任何信息。\n- 对每个用户 $u$，构建所有有序物品对 $(j,k)$，使得 $R_{u j} > R_{u k}$，其中 $R_{u j}$ 表示用户 $u$ 对物品 $j$ 的观测评分。每个这样的三元组 $(u,j,k)$ 构成一个严格偏序约束，模型必须旨在通过 $\\hat{R}_{u j} > \\hat{R}_{u k}$ 来满足该约束。\n- 使用一个 $k$ 维潜因子模型，其预测分数为 $\\hat{R}_{u i} = p_u^\\top q_i$，其中 $p_u \\in \\mathbb{R}^k$ 是用户 $u$ 的潜向量，而 $q_i \\in \\mathbb{R}^k$ 是物品 $i$ 的潜向量。使用从零均值、小标准差的正态分布中抽取的独立样本来初始化所有潜向量。\n- 将经验风险定义为带有固定正间隔 $\\gamma$ 和 $\\ell_2$ 正则化的成对 hinge 损失。也就是说，对于每个约束 $(u,j,k)$，其损失贡献为 $\\max\\left(0,\\ \\gamma - \\left(p_u^\\top q_j - p_u^\\top q_k\\right)\\right)$，总损失为正则化添加了 $\\frac{\\lambda}{2}\\left(\\lVert P\\rVert_F^2 + \\lVert Q\\rVert_F^2\\right)$，其中 $P$ 和 $Q$ 分别是用户和物品因子的堆叠矩阵。\n- 使用随机梯度下降（SGD）优化参数，其中仅当相关的间隔违规（margin violation）为正时，才应用 hinge 项的次梯度，而 $\\ell_2$ 正则化在整个训练过程中都应用。确保在每个轮次使用固定的随机种子来随机化约束的顺序，以使结果可复现。必须显式实现随机梯度下降（SGD）；不要依赖外部优化例程。\n- 训练后，计算排序准确率，即在为该测试用例构建的所有约束中被满足的约束的比例。一个约束 $(u,j,k)$ 当且仅当 $\\hat{R}_{u j} - \\hat{R}_{u k} > 0$ 时被视为满足。如果一个用户没有贡献任何约束（例如，因为只有一个评分或评分相同），则其对总分母的贡献为零约束。如果一个测试用例产生零个总约束，为避免除以零，按惯例将准确率定义为 $1.0$。预测中的平局情况，即 $\\hat{R}_{u j} - \\hat{R}_{u k} = 0$，应被视为不满足约束。\n\n您的程序必须处理以下三个测试用例。在每个用例中，评分是集合 $\\{1,2,3,4,5\\}$ 中的整数，缺失条目编码为 $0$。所有测试用例使用相同的超参数：潜维度 $k = 3$，学习率 $\\eta = 0.05$，正则化系数 $\\lambda = 0.01$，间隔 $\\gamma = 1.0$，轮次数量 $T = 200$，以及用于初始化和每轮约束排序的随机种子 $s = 42$。\n\n测试用例 1：\n- 形状为 $3 \\times 4$ 的评分矩阵 $R^{(1)}$：\n  - 第 0 行：$[5,3,0,1]$\n  - 第 1 行：$[0,4,2,0]$\n  - 第 2 行：$[2,0,5,4]$\n\n测试用例 2：\n- 形状为 $3 \\times 4$ 的评分矩阵 $R^{(2)}$：\n  - 第 0 行：$[3,3,1,0]$ (评分相等的物品之间不构成约束)\n  - 第 1 行：$[0,0,5,0]$ (单个评分不产生约束)\n  - 第 2 行：$[4,4,4,4]$ (所有评分相等不产生约束)\n\n测试用例 3：\n- 形状为 $4 \\times 5$ 的评分矩阵 $R^{(3)}$：\n  - 第 0 行：$[1,0,4,0,2]$\n  - 第 1 行：$[0,5,0,3,0]$\n  - 第 2 行：$[2,0,0,0,1]$\n  - 第 3 行：$[0,4,1,0,0]$\n\n需要遵守的实现细节：\n- 仅使用对每个用户 $u$ 满足 $R_{u j} > R_{u k}$ 的配对 $(j,k)$ 从 $R$ 构建约束集；不要从相等的评分中创建约束。\n- 使用指定的潜因子模型和带有间隔 $\\gamma$ 及正则化系数 $\\lambda$ 的 $\\ell_2$ 正则化的 hinge 损失。\n- 使用 SGD 实现对约束集进行 $T$ 次完整遍历的训练。使用固定的种子 $s$ 初始化参数并在每个轮次中打乱约束顺序，以确保可复现性。\n- 对每个测试用例，计算最终的排序准确率，结果为 $[0,1]$ 范围内的浮点数。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含一个由方括号括起来的、逗号分隔的三个浮点数列表，按测试用例的顺序排列，每个数字精确到小数点后四位（例如，$[0.9750,0.5000,1.0000]$）。", "solution": "用户-物品评分矩阵被视为序数偏好数据的来源。该问题要求使用潜因子模型实现一个成对排序协同过滤算法。模型参数通过随机梯度下降（SGD）优化一个特定的目标函数来学习。\n\n### 1. 模型与目标函数\n\n模型的核心是为每个用户和物品学习潜因子表示。对于一个用户 $u$ 和一个物品 $i$，它们各自的潜向量为 $p_u \\in \\mathbb{R}^k$ 和 $q_i \\in \\mathbb{R}^k$，其中 $k$ 是潜空间的维度。用户 $u$ 对物品 $i$ 的预测偏好分数由点积给出：\n$$\n\\hat{R}_{ui} = p_u^\\top q_i\n$$\n学习过程不直接预测评分值，而是旨在保留数据中观测到的偏好相对顺序。对于每个用户 $u$，如果物品 $j$ 的观测评分严格大于物品 $k$ 的评分，即 $R_{uj} > R_{uk}$，则为每一对物品 $(j,k)$ 生成一个严格偏序约束 $(u,j,k)$。所有此类约束的集合表示为 $\\mathcal{C}$。\n\n模型通过最小化一个由成对 hinge 损失和 $\\ell_2$ 正则化定义的经验风险函数来进行训练。对于每个约束 $(u,j,k) \\in \\mathcal{C}$，模型应预测 $\\hat{R}_{uj} > \\hat{R}_{uk}$。损失函数惩罚违反此顺序的情况，特别是在 $\\hat{R}_{uj} - \\hat{R}_{uk}$ 不大于指定间隔 $\\gamma$ 时。要最小化的总目标函数 $L$ 为：\n$$\nL(P, Q) = \\sum_{(u,j,k) \\in \\mathcal{C}} \\max\\left(0, \\gamma - (\\hat{R}_{uj} - \\hat{R}_{uk})\\right) + \\frac{\\lambda}{2}\\left(\\lVert P \\rVert_F^2 + \\lVert Q \\rVert_F^2\\right)\n$$\n在这里，$P$ 和 $Q$ 分别是通过堆叠所有用户向量 $p_u$ 和物品向量 $q_i$ 形成的矩阵。$\\lVert \\cdot \\rVert_F^2$ 项表示弗罗贝尼乌斯范数的平方，即 $\\sum_u \\lVert p_u \\rVert_2^2 + \\sum_i \\lVert q_i \\rVert_2^2$。超参数 $\\lambda > 0$ 控制正则化的强度，通过惩罚较大的参数值来防止过拟合。间隔给定为 $\\gamma=1.0$。\n\n### 2. 算法实现\n\n#### 2.1. 约束生成\n首先，从输入评分矩阵 $R$ 中提取偏好约束集 $\\mathcal{C}$。对于每个用户 $u$，我们识别出他们已评分的所有物品（评分非零）。然后，对于这些已评分物品的每个有序对，比如物品 $j$ 和物品 $k'$，如果 $R_{uj} > R_{uk'}$，则将三元组 $(u,j,k')$ 添加到 $\\mathcal{C}$ 中。平局（$R_{uj} = R_{uk'}$）和缺失的评分不贡献约束。\n\n#### 2.2. 参数初始化\n初始化大小为 $N_u \\times k$ 的用户因子矩阵 $P$ 和大小为 $N_i \\times k$ 的物品因子矩阵 $Q$。$N_u$ 是用户数量，$N_i$ 是物品数量。每个条目都从一个零均值、小标准差（选择 $0.1$ 是一个合适的选择）的正态分布中抽取。使用固定的随机种子（$s=42$）以确保可复现性。\n\n#### 2.3. 随机梯度下降 (SGD) 优化\n模型参数 $P$ 和 $Q$ 通过在训练数据上迭代固定轮次（$T=200$）进行优化。在每个轮次中，约束集 $\\mathcal{C}$ 被随机打乱（使用相同的种子随机数生成器）以确保无偏更新。然后算法遍历 $\\mathcal{C}$ 中的每个约束 $(u,j,k')$ 并执行一次 SGD 更新。\n\n对于单个约束 $(u,j,k')$，计算目标函数关于相关参数 $p_u$、$q_j$ 和 $q_{k'}$ 的梯度。令 $x_{ujk'} = p_u^\\top q_j - p_u^\\top q_{k'}$。指示函数 $\\mathbb{I}[\\gamma - x_{ujk'} > 0]$ 在间隔被违反时为 $1$，否则为 $0$。梯度为：\n$$\n\\nabla_{p_u} L = \\lambda p_u + \\mathbb{I}[\\gamma - x_{ujk'} > 0] \\cdot (q_{k'} - q_j)\n$$\n$$\n\\nabla_{q_j} L = \\lambda q_j + \\mathbb{I}[\\gamma - x_{ujk'} > 0] \\cdot (-p_u)\n$$\n$$\n\\nabla_{q_{k'}} L = \\lambda q_{k'} + \\mathbb{I}[\\gamma - x_{ujk'} > 0] \\cdot (p_u)\n$$\n然后，通过朝负梯度方向迈出一小步来更新参数，步长由学习率 $\\eta=0.05$ 缩放：\n$$\np_u \\leftarrow p_u - \\eta \\nabla_{p_u} L\n$$\n$$\nq_j \\leftarrow q_j - \\eta \\nabla_{q_j} L\n$$\n$$\nq_{k'} \\leftarrow q_{k'} - \\eta \\nabla_{q_{k'}} L\n$$\n为确保正确更新，在该步骤开始时的 $p_u$、$q_j$ 和 $q_{k'}$ 的值必须用于该步骤内的所有梯度计算。\n\n### 3. 评估\n经过 $T$ 轮训练后，使用排序准确率来评估模型的性能。该指标是学习到的模型正确满足的约束在总约束集 $\\mathcal{C}$ 中所占的比例。一个约束 $(u,j,k')$ 被认为满足，如果物品 $j$ 的预测分数严格大于物品 $k'$ 的预测分数，即 $\\hat{R}_{uj} - \\hat{R}_{uk'} > 0$。平局（$\\hat{R}_{uj} - \\hat{R}_{uk'} = 0$）被计为不满足。准确率计算如下：\n$$\n\\text{准确率} = \\frac{|\\{(u,j,k') \\in \\mathcal{C} \\mid p_u^\\top q_j - p_u^\\top q_{k'} > 0\\}|}{|\\mathcal{C}|}\n$$\n如果一个测试用例导致生成的约束为零（$|\\mathcal{C}| = 0$），则按照惯例，准确率定义为 $1.0$。", "answer": "```python\nimport numpy as np\n\ndef run_pairwise_ranking(R, k, eta, lamb, gamma, T, seed):\n    \"\"\"\n    Implements and evaluates the pairwise-ranking collaborative filtering algorithm.\n\n    Args:\n        R (np.ndarray): The user-item rating matrix.\n        k (int): The number of latent dimensions.\n        eta (float): The learning rate for SGD.\n        lamb (float): The regularization coefficient.\n        gamma (float): The margin for the hinge loss.\n        T (int): The number of training epochs.\n        seed (int): The random seed for reproducibility.\n\n    Returns:\n        float: The final ranking accuracy.\n    \"\"\"\n    num_users, num_items = R.shape\n\n    # Step 1: Construct partial order constraints from the rating matrix.\n    # A constraint is a tuple (user_id, preferred_item_id, less_preferred_item_id).\n    constraints = []\n    for u in range(num_users):\n        rated_items_indices = np.where(R[u, :] > 0)[0]\n        # Generate pairs of items rated by the same user\n        for i in range(len(rated_items_indices)):\n            for j in range(len(rated_items_indices)):\n                if i == j:\n                    continue\n                item_j = rated_items_indices[i]\n                item_k = rated_items_indices[j]\n                # Add constraint if a strict preference is observed\n                if R[u, item_j] > R[u, item_k]:\n                    constraints.append((u, item_j, item_k))\n\n    # If there are no constraints, accuracy is 1.0 by definition.\n    if not constraints:\n        return 1.0\n\n    # Step 2: Initialize parameters (user and item latent factor matrices).\n    # The same RNG is used for initialization and shuffling for reproducibility.\n    rng = np.random.default_rng(seed)\n    std_dev = 0.1  # A small standard deviation for initialization\n    P = rng.normal(loc=0.0, scale=std_dev, size=(num_users, k))\n    Q = rng.normal(loc=0.0, scale=std_dev, size=(num_items, k))\n\n    # Step 3: Train using Stochastic Gradient Descent (SGD).\n    for epoch in range(T):\n        rng.shuffle(constraints)  # Shuffle constraints for each epoch\n        for u, j, k_item in constraints:\n            # Get current factors. Make copies to ensure simultaneous updates.\n            p_u = P[u, :].copy()\n            q_j = Q[j, :].copy()\n            q_k = Q[k_item, :].copy()\n\n            # Calculate the difference in predicted scores\n            pred_diff = np.dot(p_u, q_j) - np.dot(p_u, q_k)\n\n            # Check for margin violation to determine the hinge loss gradient part\n            if gamma - pred_diff > 0:\n                grad_pu_hinge = q_k - q_j\n                grad_qj_hinge = -p_u\n                grad_qk_hinge = p_u\n            else:\n                grad_pu_hinge = 0.0\n                grad_qj_hinge = 0.0\n                grad_qk_hinge = 0.0\n            \n            # Update parameters by taking a step against the combined gradient\n            # (regularization + hinge loss)\n            P[u, :] -= eta * (lamb * p_u + grad_pu_hinge)\n            Q[j, :] -= eta * (lamb * q_j + grad_qj_hinge)\n            Q[k_item, :] -= eta * (lamb * q_k + grad_qk_hinge)\n\n    # Step 4: Evaluate ranking accuracy on the set of constraints.\n    satisfied_count = 0\n    for u, j, k_item in constraints:\n        # A constraint is satisfied if the predicted preference order matches the observed one.\n        pred_diff = np.dot(P[u, :], Q[j, :]) - np.dot(P[u, :], Q[k_item, :])\n        if pred_diff > 0:\n            satisfied_count += 1\n            \n    accuracy = satisfied_count / len(constraints)\n    return accuracy\n\ndef solve():\n    \"\"\"\n    Main function to run the algorithm on all test cases and print the results.\n    \"\"\"\n    # Define hyperparameters as specified in the problem\n    k = 3\n    eta = 0.05\n    lamb = 0.01  # `lambda` is a keyword, so `lamb` is used\n    gamma = 1.0\n    T = 200\n    seed = 42\n\n    # Define the test cases from the problem statement\n    test_cases = [\n        np.array([\n            [5, 3, 0, 1],\n            [0, 4, 2, 0],\n            [2, 0, 5, 4]\n        ], dtype=np.float64),\n        np.array([\n            [3, 3, 1, 0],\n            [0, 0, 5, 0],\n            [4, 4, 4, 4]\n        ], dtype=np.float64),\n        np.array([\n            [1, 0, 4, 0, 2],\n            [0, 5, 0, 3, 0],\n            [2, 0, 0, 0, 1],\n            [0, 4, 1, 0, 0]\n        ], dtype=np.float64)\n    ]\n\n    results = []\n    for R_case in test_cases:\n        accuracy = run_pairwise_ranking(R_case, k, eta, lamb, gamma, T, seed)\n        # Format the result to exactly four decimal places\n        results.append(f\"{accuracy:.4f}\")\n\n    # Final print statement in the exact required format\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3110042"}, {"introduction": "在我们实现了一个成对模型之后，现在我们将焦点转向最具影响力的成对排序方法之一——贝叶斯个性化排序（BPR）的数学核心。这项练习 [@problem_id:3110073] 要求你推导 BPR 目标函数的梯度。掌握这一计算是理解模型如何学习的关键，并为你从零开始实现自己的 BPR 优化器提供了必要的工具。", "problem": "一个推荐系统使用贝叶斯个性化排序（BPR），该方法通过一个逻辑斯蒂链接对成对偏好进行建模：对于一个用户 $i$ 和物品 $j$、$k$，用户 $i$ 偏好物品 $j$ 胜过物品 $k$ 的概率是 $p\\big((i,j,k)\\big) = \\sigma\\!\\big(\\Delta_{ijk}\\big)$，其中 $\\sigma(x) = \\frac{1}{1 + \\exp(-x)}$ 且 $\\Delta_{ijk} = u_i^{\\top}(v_j - v_k)$。训练目标是正则化的负对数似然\n$$\n\\mathcal{L}(U,V) = - \\sum_{(i,j,k) \\in \\mathcal{S}} \\ln\\!\\big(\\sigma(\\Delta_{ijk})\\big) + \\frac{\\lambda}{2} \\sum_{i} \\|u_i\\|_2^2 + \\frac{\\lambda}{2} \\sum_{\\ell} \\|v_{\\ell}\\|_2^2,\n$$\n其中 $U = \\{u_i\\}$ 是用户嵌入，$V = \\{v_{\\ell}\\}$ 是物品嵌入，$\\lambda > 0$ 是一个正则化系数，$\\mathcal{S}$ 是观测到的用户-物品正负对集合。\n\n从这个定义和逻辑斯蒂函数的性质出发，对于一个固定的用户 $i$，推导出梯度 $\\nabla_{u_i} \\mathcal{L}$，它应是 $\\{(j,k) \\in \\mathcal{S}_i\\}$、用户嵌入 $u_i$ 以及物品嵌入 $v_j$ 和 $v_k$ 的函数。然后，对于嵌入维度 $d = 3$，用户 $i$ 有两个观测对 $\\mathcal{S}_i = \\{(j_1,k_1),(j_2,k_2)\\}$，正则化系数 $\\lambda = 0.1$ 的具体实例，以及\n$$\nu_i = \\begin{pmatrix} 0.5 \\\\ -1.0 \\\\ 0.75 \\end{pmatrix}, \\quad\nv_{j_1} = \\begin{pmatrix} 0.2 \\\\ 0.0 \\\\ -0.1 \\end{pmatrix}, \\quad\nv_{k_1} = \\begin{pmatrix} -0.3 \\\\ 0.4 \\\\ 0.0 \\end{pmatrix},\n$$\n$$\nv_{j_2} = \\begin{pmatrix} 1.0 \\\\ -0.5 \\\\ 0.25 \\end{pmatrix}, \\quad\nv_{k_2} = \\begin{pmatrix} -0.5 \\\\ 0.2 \\\\ 0.1 \\end{pmatrix},\n$$\n计算欧几里得范数 $\\|\\nabla_{u_i} \\mathcal{L}\\|_2$。将您的最终数值答案四舍五入到四位有效数字。将您的最终答案表示为不带单位的纯数字。", "solution": "面向用户的问题是，首先推导贝叶斯个性化排序（BPR）损失函数关于用户嵌入的梯度，然后为一个特定的数值实例计算该梯度的欧几里得范数。\n\nBPR 目标函数如下：\n$$\n\\mathcal{L}(U,V) = - \\sum_{(i,j,k) \\in \\mathcal{S}} \\ln\\!\\big(\\sigma(\\Delta_{ijk})\\big) + \\frac{\\lambda}{2} \\sum_{i} \\|u_i\\|_2^2 + \\frac{\\lambda}{2} \\sum_{\\ell} \\|v_{\\ell}\\|_2^2\n$$\n其中 $\\Delta_{ijk} = u_i^{\\top}(v_j - v_k)$ 且 $\\sigma(x) = (1 + \\exp(-x))^{-1}$。\n\n我们需要求解对于一个固定用户 $i$ 的梯度 $\\nabla_{u_i} \\mathcal{L}$。当对特定的用户嵌入 $u_i$ 求导时，我们只需考虑 $\\mathcal{L}$ 中依赖于 $u_i$ 的项。这些是涉及用户 $i$ 的偏好项和针对 $u_i$ 的正则化项。令 $\\mathcal{S}_i$ 为用户 $i$ 的观测对集合，即 $\\mathcal{S}_i = \\{(j,k) | (i,j,k) \\in \\mathcal{S}\\}$。\n\n损失函数中依赖于 $u_i$ 的部分，记作 $\\mathcal{L}_i$，为：\n$$\n\\mathcal{L}_i(u_i) = - \\sum_{(j,k) \\in \\mathcal{S}_i} \\ln\\!\\big(\\sigma(u_i^{\\top}(v_j - v_k))\\big) + \\frac{\\lambda}{2} \\|u_i\\|_2^2\n$$\n梯度为 $\\nabla_{u_i} \\mathcal{L} = \\nabla_{u_i} \\mathcal{L}_i(u_i)$。我们可以分别计算每一项的梯度。\n\n正则化项的梯度是一个标准结果：\n$$\n\\nabla_{u_i} \\left(\\frac{\\lambda}{2} \\|u_i\\|_2^2\\right) = \\nabla_{u_i} \\left(\\frac{\\lambda}{2} u_i^{\\top}u_i\\right) = \\lambda u_i\n$$\n\n对于负对数似然项，我们关注单个三元组 $(i,j,k)$。我们使用链式法则来求解 $\\ln(\\sigma(\\Delta_{ijk}))$ 的梯度。\n$$\n\\nabla_{u_i} \\ln(\\sigma(\\Delta_{ijk})) = \\frac{d}{d\\sigma} \\ln(\\sigma) \\cdot \\frac{d\\sigma}{d\\Delta_{ijk}} \\cdot \\nabla_{u_i} \\Delta_{ijk}\n$$\n这些导数是：\n$1$. $\\frac{d}{d\\sigma} \\ln(\\sigma) = \\frac{1}{\\sigma(\\Delta_{ijk})}$。\n$2$. 逻辑斯蒂函数的导数是 $\\sigma'(x) = \\sigma(x)(1 - \\sigma(x))$。因此，$\\frac{d\\sigma}{d\\Delta_{ijk}} = \\sigma(\\Delta_{ijk})(1 - \\sigma(\\Delta_{ijk}))$。\n$3$. $\\Delta_{ijk}$ 是 $u_i$ 的一个线性函数。对于 $\\Delta_{ijk} = u_i^{\\top}(v_j - v_k)$，其梯度为 $\\nabla_{u_i} \\Delta_{ijk} = v_j - v_k$。\n\n结合这些结果：\n$$\n\\nabla_{u_i} \\ln(\\sigma(\\Delta_{ijk})) = \\frac{1}{\\sigma(\\Delta_{ijk})} \\cdot \\sigma(\\Delta_{ijk})(1 - \\sigma(\\Delta_{ijk})) \\cdot (v_j - v_k) = (1 - \\sigma(\\Delta_{ijk}))(v_j - v_k)\n$$\n利用性质 $1 - \\sigma(x) = \\frac{1+\\exp(-x)-1}{1+\\exp(-x)} = \\frac{\\exp(-x)}{1+\\exp(-x)} = \\frac{1}{\\exp(x)+1} = \\sigma(-x)$，我们可以写出：\n$$\n\\nabla_{u_i} \\ln(\\sigma(\\Delta_{ijk})) = \\sigma(-\\Delta_{ijk})(v_j - v_k)\n$$\n现在，我们可以写出完整损失 $\\mathcal{L}_i$ 的梯度：\n$$\n\\nabla_{u_i} \\mathcal{L}_i(u_i) = \\nabla_{u_i} \\left( - \\sum_{(j,k) \\in \\mathcal{S}_i} \\ln(\\sigma(\\Delta_{ijk})) + \\frac{\\lambda}{2} \\|u_i\\|_2^2 \\right)\n$$\n$$\n\\nabla_{u_i} \\mathcal{L} = - \\sum_{(j,k) \\in \\mathcal{S}_i} \\sigma(-\\Delta_{ijk})(v_j - v_k) + \\lambda u_i\n$$\n这就是关于用户嵌入 $u_i$ 的梯度的通用表达式。\n\n接下来，我们为所提供的具体实例计算这个梯度。\n参数如下：\n$\\lambda = 0.1$。\n$u_i = \\begin{pmatrix} 0.5 \\\\ -1.0 \\\\ 0.75 \\end{pmatrix}$。\n用户 $i$ 的配对集合是 $\\mathcal{S}_i = \\{(j_1,k_1),(j_2,k_2)\\}$，所以求和中有两项。\n物品嵌入如下：\n$v_{j_1} = \\begin{pmatrix} 0.2 \\\\ 0.0 \\\\ -0.1 \\end{pmatrix}, v_{k_1} = \\begin{pmatrix} -0.3 \\\\ 0.4 \\\\ 0.0 \\end{pmatrix}$。\n$v_{j_2} = \\begin{pmatrix} 1.0 \\\\ -0.5 \\\\ 0.25 \\end{pmatrix}, v_{k_2} = \\begin{pmatrix} -0.5 \\\\ 0.2 \\\\ 0.1 \\end{pmatrix}$。\n\n梯度是：\n$$\n\\nabla_{u_i} \\mathcal{L} = -\\sigma(-\\Delta_{ij_1k_1})(v_{j_1} - v_{k_1}) - \\sigma(-\\Delta_{ij_2k_2})(v_{j_2} - v_{k_2}) + \\lambda u_i\n$$\n\n对于第一对 $(j_1,k_1)$：\n$v_{j_1} - v_{k_1} = \\begin{pmatrix} 0.2 - (-0.3) \\\\ 0.0 - 0.4 \\\\ -0.1 - 0.0 \\end{pmatrix} = \\begin{pmatrix} 0.5 \\\\ -0.4 \\\\ -0.1 \\end{pmatrix}$。\n$\\Delta_{ij_1k_1} = u_i^{\\top}(v_{j_1} - v_{k_1}) = (0.5)(0.5) + (-1.0)(-0.4) + (0.75)(-0.1) = 0.25 + 0.4 - 0.075 = 0.575$。\n系数是 $\\sigma(-\\Delta_{ij_1k_1}) = \\sigma(-0.575) = \\frac{1}{1 + \\exp(0.575)} \\approx 0.360093$。\n\n对于第二对 $(j_2,k_2)$：\n$v_{j_2} - v_{k_2} = \\begin{pmatrix} 1.0 - (-0.5) \\\\ -0.5 - 0.2 \\\\ 0.25 - 0.1 \\end{pmatrix} = \\begin{pmatrix} 1.5 \\\\ -0.7 \\\\ 0.15 \\end{pmatrix}$。\n$\\Delta_{ij_2k_2} = u_i^{\\top}(v_{j_2} - v_{k_2}) = (0.5)(1.5) + (-1.0)(-0.7) + (0.75)(0.15) = 0.75 + 0.7 + 0.1125 = 1.5625$。\n系数是 $\\sigma(-\\Delta_{ij_2k_2}) = \\sigma(-1.5625) = \\frac{1}{1 + \\exp(1.5625)} \\approx 0.173291$。\n\n正则化项是：\n$\\lambda u_i = 0.1 \\begin{pmatrix} 0.5 \\\\ -1.0 \\\\ 0.75 \\end{pmatrix} = \\begin{pmatrix} 0.05 \\\\ -0.1 \\\\ 0.075 \\end{pmatrix}$。\n\n现在我们组合梯度向量，记作 $g = \\nabla_{u_i} \\mathcal{L}$：\n$$\ng = -0.360093 \\begin{pmatrix} 0.5 \\\\ -0.4 \\\\ -0.1 \\end{pmatrix} - 0.173291 \\begin{pmatrix} 1.5 \\\\ -0.7 \\\\ 0.15 \\end{pmatrix} + \\begin{pmatrix} 0.05 \\\\ -0.1 \\\\ 0.075 \\end{pmatrix}\n$$\n梯度向量 $g = (g_1, g_2, g_3)^{\\top}$ 的分量是：\n$g_1 = - (0.360093)(0.5) - (0.173291)(1.5) + 0.05 = -0.1800465 - 0.2599365 + 0.05 = -0.389983$。\n$g_2 = - (0.360093)(-0.4) - (0.173291)(-0.7) - 0.1 = 0.1440372 + 0.1213037 - 0.1 = 0.1653409$。\n$g_3 = - (0.360093)(-0.1) - (0.173291)(0.15) + 0.075 = 0.0360093 - 0.02599365 + 0.075 = 0.08501565$。\n\n所以，梯度向量近似为：\n$$\n\\nabla_{u_i} \\mathcal{L} \\approx \\begin{pmatrix} -0.389983 \\\\ 0.165341 \\\\ 0.085016 \\end{pmatrix}\n$$\n最后，我们计算其欧几里得范数 $\\|\\nabla_{u_i} \\mathcal{L}\\|_2$：\n$$\n\\|\\nabla_{u_i} \\mathcal{L}\\|_2 = \\sqrt{g_1^2 + g_2^2 + g_3^2}\n$$\n$$\n\\|\\nabla_{u_i} \\mathcal{L}\\|_2 \\approx \\sqrt{(-0.389983)^2 + (0.165341)^2 + (0.085016)^2}\n$$\n$$\n\\|\\nabla_{u_i} \\mathcal{L}\\|_2 \\approx \\sqrt{0.152087 + 0.027338 + 0.007228} = \\sqrt{0.186653} \\approx 0.4320335\n$$\n在中间计算中使用更高的精度会得到 $\\sqrt{0.186652072} \\approx 0.432032489$。四舍五入到四位有效数字得到 $0.4320$。", "answer": "$$\\boxed{0.4320}$$", "id": "3110073"}, {"introduction": "构建模型只是成功的一半，正确地评估它同样至关重要。在现实世界中，用户反馈（如点击）受到物品呈现位置的严重影响，这导致了“曝光偏差”。这项练习 [@problem_id:3110057] 直面这一挑战，你将实现一个考虑了曝光偏差的评估指标——反倾向得分归一化折损累计增益（IPS-NDCG）。通过构建这个工具，你将能够更准确地评估模型的真实性能。", "problem": "您的任务是设计一个程序，以使用协同过滤评估原则来评估在位置依赖的曝光偏差下的排名推荐。该场景包含多个用户，每个用户都会看到一个物品的排名列表。每个位置都有被看到的倾向性，只有被曝光的物品才可能被点击。目标是推导并实现一个能够校正非均匀曝光的、感知曝光的排名质量估计，并将其与忽略曝光偏差的朴素估计进行比较。\n\n从以下基本定义开始：\n\n- 折损累计增益 (Discounted Cumulative Gain, DCG) 通过对位置折损后的增益求和来评估一个排名列表。对于一个列表长度为 $L$ 的用户、一个单调增益函数 $g(\\cdot)$ 和一个位置折损函数 $d(r)$，$\\mathrm{DCG}$ 是对所有位置 $r \\in \\{1,\\dots,L\\}$ 的求和。使用标准的二元相关性增益 $g(y_r) = y_r$ 和对数折损 $d(r) = 1/\\log_2(1 + r)$，其中 $\\log_2(\\cdot)$ 表示以 2 为底的对数。\n- 列表长度为 $L$ 时的归一化折损累计增益 (Normalized Discounted Cumulative Gain, NDCG) 是 $\\mathrm{DCG}$ 除以理想 DCG，其中理想 DCG (记为 $\\mathrm{IDCG}$) 是通过将物品按其真实相关性标签 $y_r \\in \\{0,1\\}$ 降序排列所能达到的 DCG。\n\n在离线协同过滤评估中，观测到的点击会受到曝光的影响。设 $e_r \\in \\{0,1\\}$ 表示位置 $r$ 是否被曝光，$c_r \\in \\{0,1\\}$ 为观测到的点击。假设点击遵循确定性关系 $c_r = e_r \\cdot y_r$（无额外噪声）。设 $\\hat{\\pi}_r \\in (0,1]$ 为位置 $r$ 曝光的估计倾向性。在非均匀曝光下，使用通用的逆倾向得分原则：对于任何对曝光实例的求和，当曝光是独立的且模型正确时，通过 $1/\\hat{\\pi}_r$ 进行加权，可以得到目标策略下相应期望的无偏估计量。\n\n您的任务：\n\n1.  使用逆倾向权重 $1/\\hat{\\pi}_r$ 推导位置折损增益和的曝光感知估计量，然后通过理想 DCG 进行归一化，以获得逆倾向得分归一化折损累计增益 (IPS-NDCG)。同时定义忽略曝光偏差的朴素 NDCG，它直接使用观测到的点击而不使用倾向权重。\n2.  在用户级别实现这两种指标，并通过对每个用户的 NDCG 值求平均来跨用户聚合，为每个测试用例的每种方法生成一个单一的分数。\n3.  对于每个测试用例，计算 IPS-NDCG 和朴素 NDCG 之间的差异（IPS 减去朴素值）。您的程序应将这些差异输出为浮点数。\n\n测试套件规范：\n\n- 使用以下四个测试用例。在每个用例中，列表已经按排名顺序给出，因此位置 $r = 1,2,\\dots$ 遵循给定的数组。对于每个用户，您将获得真实相关性 $y$、曝光倾向性 $\\hat{\\pi}$ 和已实现的曝光 $e$ 的数组。观测到的点击由 $c_r = e_r \\cdot y_r$ 定义。所有数字必须严格按照给定的值处理。\n\n- 测试用例 1（单个用户，列表长度 5）：\n  - 真实相关性 $y$: $[0,1,1,1,1]$\n  - 倾向性 $\\hat{\\pi}$: $[0.95,0.8,0.5,0.2,0.05]$\n  - 曝光 $e$: $[1,1,1,0,0]$\n\n- 测试用例 2（两个用户，每个列表长度 4）：\n  - 用户 A：\n    - 真实相关性 $y^{(A)}$: $[1,0,1,0]$\n    - 倾向性 $\\hat{\\pi}^{(A)}$: $[0.9,0.7,0.4,0.2]$\n    - 曝光 $e^{(A)}$: $[1,1,0,0]$\n  - 用户 B：\n    - 真实相关性 $y^{(B)}$: $[0,1,1,1]$\n    - 倾向性 $\\hat{\\pi}^{(B)}$: $[0.95,0.6,0.3,0.1]$\n    - 曝光 $e^{(B)}$: $[1,1,1,0]$\n\n- 测试用例 3（单个用户，列表长度 4；没有观测到点击的边缘情况）：\n  - 真实相关性 $y$: $[0,0,0,1]$\n  - 倾向性 $\\hat{\\pi}$: $[0.9,0.7,0.5,0.01]$\n  - 曝光 $e$: $[1,1,1,0]$\n\n- 测试用例 4（单个用户，列表长度 5；在深层位置有强位置偏差）：\n  - 真实相关性 $y$: $[0,0,1,0,1]$\n  - 倾向性 $\\hat{\\pi}$: $[0.98,0.9,0.4,0.2,0.02]$\n  - 曝光 $e$: $[1,1,1,0,1]$\n\n最终输出格式规范：\n\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表的结果。每个元素对应一个测试用例，是该测试用例中（如果适用）所有用户平均的浮点数差异 $\\Delta = \\mathrm{IPS}\\text{-}\\mathrm{NDCG} - \\mathrm{naïve}\\text{-}\\mathrm{NDCG}$。例如，一个有四个测试用例的输出必须看起来像 $[\\Delta_1,\\Delta_2,\\Delta_3,\\Delta_4]$。", "solution": "该问题要求推导和实现两种排名评估指标，即朴素归一化折损累计增益 (naïve Normalized Discounted Cumulative Gain, NDCG) 和使用逆倾向得分 (Inverse Propensity Scoring) 的曝光感知版本 (IPS-NDCG)，以评估在位置依赖的曝光偏差下排名推荐的质量。\n\n### 第 1 步：排名指标的公式推导\n\n我们已知折损累计增益 (DCG) 及其归一化版本 NDCG 的基本定义。假设一个用户的排名列表长度为 $L$。列表中的位置由 $r \\in \\{1, 2, \\dots, L\\}$ 表示。\n\n增益函数是二元相关性 $g(y_r) = y_r$，其中 $y_r \\in \\{0, 1\\}$ 是位置 $r$ 处物品的真实相关性。\n位置折损函数是 $d(r) = 1/\\log_2(1+r)$，其中 $\\log_2$ 是以 2 为底的对数。\n\n在离线评估设置中，我们不直接观察 $y_r$。相反，我们观察点击 $c_r \\in \\{0, 1\\}$，它同时取决于相关性 $y_r$ 和该位置是否被曝光（由 $e_r \\in \\{0, 1\\}$ 表示）。点击被确定性地建模为 $c_r = e_r \\cdot y_r$。位置 $r$ 的估计曝光概率由倾向性得分 $\\hat{\\pi}_r \\in (0, 1]$ 给出。\n\n**理想折损累计增益 (IDCG)**\n\n朴素 NDCG 和 IPS-NDCG 都由相同的量——理想折损累计增益 (IDCG) 进行归一化。IDCG 代表呈现给用户的物品集合可能达到的最大 DCG。它通过将物品按其真实相关性 $y_r$ 降序排列，然后计算此理想排名的 DCG 来得到。设 $y^{\\text{ideal}}$ 是按降序排序的真实相关性标签 $y$ 的向量。\n\n$$\n\\mathrm{IDCG} = \\sum_{r=1}^{L} d(r) \\cdot y^{\\text{ideal}}_r = \\sum_{r=1}^{L} \\frac{y^{\\text{ideal}}_r}{\\log_2(1+r)}\n$$\n\n如果所有物品的相关性都为零（即 $\\sum y_r = 0$），则 $\\mathrm{IDCG} = 0$。在这种情况下，任何 NDCG 通常被定义为 $0$。\n\n**朴素 NDCG**\n\n朴素方法忽略了曝光偏差，将观测到的点击 $c_r$ 视为真实的关性标签。\n\n朴素折损累计增益 $\\mathrm{DCG}_{\\text{naïve}}$ 是折损后的观测点击之和：\n$$\n\\mathrm{DCG}_{\\text{naïve}} = \\sum_{r=1}^{L} d(r) \\cdot c_r = \\sum_{r=1}^{L} \\frac{c_r}{\\log_2(1+r)}\n$$\n朴素归一化折损累计增益 $\\mathrm{NDCG}_{\\text{naïve}}$ 是该值由 IDCG 归一化得到：\n$$\n\\mathrm{NDCG}_{\\text{naïve}} = \\frac{\\mathrm{DCG}_{\\text{naïve}}}{\\mathrm{IDCG}}\n$$\n\n**逆倾向得分 NDCG (IPS-NDCG)**\n\n为了校正曝光偏差，我们使用逆倾向得分 (IPS) 原理。目标是获得真实 DCG（即 $\\sum_{r=1}^{L} d(r) y_r$）的无偏估计。这个和可以通过将观测到的交互所带来的增益用其观测（曝光）倾向性的倒数进行加权来估计。\n\n位置 $r$ 处增益的 IPS 估计量为 $\\frac{e_r y_r}{\\hat{\\pi}_r} = \\frac{c_r}{\\hat{\\pi}_r}$。如果倾向性模型是正确的（即 $\\hat{\\pi}_r$ 是真实的曝光概率 $P(e_r=1)$），那么这个估计量对于真实增益 $y_r$ 是无偏的。\n\n基于 IPS 的折损累计增益 $\\mathrm{DCG}_{\\text{IPS}}$ 是这些经倾向性加权的折损增益之和：\n$$\n\\mathrm{DCG}_{\\text{IPS}} = \\sum_{r=1}^{L} d(r) \\cdot \\frac{c_r}{\\hat{\\pi}_r} = \\sum_{r=1}^{L} \\frac{c_r}{\\hat{\\pi}_r \\log_2(1+r)}\n$$\nIPS 归一化折损累计增益 $\\mathrm{NDCG}_{\\text{IPS}}$ 则是：\n$$\n\\mathrm{NDCG}_{\\text{IPS}} = \\frac{\\mathrm{DCG}_{\\text{IPS}}}{\\mathrm{IDCG}}\n$$\n\n### 第 2 步：聚合与最终计算\n\n对于涉及多个用户的测试用例，指标是为每个用户单独计算，然后求平均值。设 $N$ 是一个测试用例中的用户数。最终报告的分数是：\n$$\n\\overline{\\mathrm{NDCG}}_{\\text{naïve}} = \\frac{1}{N} \\sum_{u=1}^{N} \\mathrm{NDCG}_{\\text{naïve}}^{(u)}\n$$\n$$\n\\overline{\\mathrm{NDCG}}_{\\text{IPS}} = \\frac{1}{N} \\sum_{u=1}^{N} \\mathrm{NDCG}_{\\text{IPS}}^{(u)}\n$$\n每个测试用例所需的输出是差异 $\\Delta$：\n$$\n\\Delta = \\overline{\\mathrm{NDCG}}_{\\text{IPS}} - \\overline{\\mathrm{NDCG}}_{\\text{naïve}}\n$$\n此过程将应用于四个指定的测试用例中的每一个。实现将使用数值数组来表示向量 $y$、 $c$、 $e$ 和 $\\hat{\\pi}$，并为提高效率执行向量化计算。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem of evaluating ranked recommendations under position-dependent exposure bias.\n    Computes the difference between IPS-NDCG and naïve-NDCG for a suite of test cases.\n    \"\"\"\n\n    def calculate_ndcgs(y: np.ndarray, pi: np.ndarray, e: np.ndarray):\n        \"\"\"\n        Calculates naïve NDCG and IPS-NDCG for a single user's ranked list.\n\n        Args:\n            y (np.ndarray): Array of true relevance labels (0 or 1).\n            pi (np.ndarray): Array of estimated exposure propensities.\n            e (np.ndarray): Array of realized exposures (0 or 1).\n\n        Returns:\n            tuple[float, float]: A tuple containing (naïve_ndcg, ips_ndcg).\n        \"\"\"\n        L = len(y)\n        if L == 0:\n            return 0.0, 0.0\n\n        # Calculate observed clicks\n        c = e * y\n\n        # Calculate position discounts: d(r) = 1 / log2(r + 1)\n        # For positions r=1,...,L, we need log2(2),...,log2(L+1)\n        ranks = np.arange(1, L + 1)\n        discounts = 1.0 / np.log2(ranks + 1)\n\n        # Calculate naive DCG\n        dcg_naive = np.sum(c * discounts)\n\n        # Calculate IPS-based DCG\n        # Add a small epsilon to pi to avoid division by zero, although problem states pi > 0\n        dcg_ips = np.sum((c / pi) * discounts)\n\n        # Calculate Ideal DCG (IDCG)\n        y_ideal = np.sort(y)[::-1]\n        idcg = np.sum(y_ideal * discounts)\n\n        if idcg == 0:\n            # If there are no relevant items, NDCG is 0\n            return 0.0, 0.0\n\n        ndcg_naive = dcg_naive / idcg\n        ndcg_ips = dcg_ips / idcg\n\n        return ndcg_naive, ips_ndcg\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test Case 1 (single user)\n        [\n            {\n                \"y\": np.array([0, 1, 1, 1, 1]),\n                \"pi\": np.array([0.95, 0.8, 0.5, 0.2, 0.05]),\n                \"e\": np.array([1, 1, 1, 0, 0]),\n            }\n        ],\n        # Test Case 2 (two users)\n        [\n            { # User A\n                \"y\": np.array([1, 0, 1, 0]),\n                \"pi\": np.array([0.9, 0.7, 0.4, 0.2]),\n                \"e\": np.array([1, 1, 0, 0]),\n            },\n            { # User B\n                \"y\": np.array([0, 1, 1, 1]),\n                \"pi\": np.array([0.95, 0.6, 0.3, 0.1]),\n                \"e\": np.array([1, 1, 1, 0]),\n            }\n        ],\n        # Test Case 3 (single user, no clicks)\n        [\n            {\n                \"y\": np.array([0, 0, 0, 1]),\n                \"pi\": np.array([0.9, 0.7, 0.5, 0.01]),\n                \"e\": np.array([1, 1, 1, 0]),\n            }\n        ],\n        # Test Case 4 (single user, strong bias)\n        [\n            {\n                \"y\": np.array([0, 0, 1, 0, 1]),\n                \"pi\": np.array([0.98, 0.9, 0.4, 0.2, 0.02]),\n                \"e\": np.array([1, 1, 1, 0, 1]),\n            }\n        ],\n    ]\n\n    results = []\n    for case_data in test_cases:\n        num_users = len(case_data)\n        total_naive_ndcg = 0.0\n        total_ips_ndcg = 0.0\n\n        for user_data in case_data:\n            naive_ndcg, ips_ndcg = calculate_ndcgs(\n                user_data[\"y\"], user_data[\"pi\"], user_data[\"e\"]\n            )\n            total_naive_ndcg += naive_ndcg\n            total_ips_ndcg += ips_ndcg\n        \n        avg_naive_ndcg = total_naive_ndcg / num_users\n        avg_ips_ndcg = total_ips_ndcg / num_users\n        \n        difference = avg_ips_ndcg - avg_naive_ndcg\n        results.append(difference)\n\n    # Format the final output string as [res1,res2,res3,res4]\n    print(f\"[{','.join(f'{r:.7f}' for r in results)}]\")\n\nsolve()\n```", "id": "3110057"}]}