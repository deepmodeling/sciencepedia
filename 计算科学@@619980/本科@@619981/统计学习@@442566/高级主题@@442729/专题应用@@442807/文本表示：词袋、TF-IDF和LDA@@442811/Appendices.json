{"hands_on_practices": [{"introduction": "本练习旨在直接比较词袋（BoW）模型与$TF-IDF$权重方案。通过分析一个$k$近邻分类器的决策边界如何变化，你将对$TF-IDF$为何通常比简单的词频计数更有效获得一个具体的几何直觉。这个练习 [@problem_id:3179893] 旨在建立关于词权重如何改变特征空间并影响分类结果的基础直觉。", "problem": "考虑使用 $k$-近邻 (k-NN) 进行二元文本分类，在文档的向量表示上使用余弦相似度。两种标准的表示方法是词袋 (BoW) 和词频-逆文档频率 (TF-IDF)。在 BoW 中，每个文档 $d$ 由一个固定词汇表上的原始词项计数 $tf_{d,j} \\in \\mathbb{N}$ 表示。在 TF-IDF 中，每个词项 $j$ 的权重为 $tf_{d,j} \\cdot \\mathrm{idf}_{j}$，其中逆文档频率 (IDF) 定义为 $\\mathrm{idf}_{j} = \\ln\\!\\left(\\frac{N}{df_{j}}\\right)$，$N$ 是语料库大小，$df_{j}$ 是词项 $j$ 的文档频率。两个向量之间的余弦相似度是它们经过 $\\ell^{2}$-归一化后的表示的点积。在本问题中，所有向量都在加权后进行 $\\ell^{2}$-归一化。我们使用 $k=1$ (1-近邻)进行分类。\n\n一个合成语料库的大小为 $N = 1000$，词汇表词项为 $\\{t_{1}, t_{2}, t_{3}\\}$。给定两个带标签的训练文档及其 BoW 计数：\n- 类别 $\\mathcal{A}$：$D_{\\mathcal{A}}$ 的计数为 $(tf_{D_{\\mathcal{A}},1}, tf_{D_{\\mathcal{A}},2}, tf_{D_{\\mathcal{A}},3}) = (10, 0, 0)$。\n- 类别 $\\mathcal{B}$：$D_{\\mathcal{B}}$ 的计数为 $(tf_{D_{\\mathcal{B}},1}, tf_{D_{\\mathcal{B}},2}, tf_{D_{\\mathcal{B}},3}) = (0, 2, 3)$。\n\n一个查询文档 $q$ 的计数为 $(tf_{q,1}, tf_{q,2}, tf_{q,3}) = (3, 0, 1)$。文档频率为 $df_{1} = 900$，$df_{2} = 50$，而 $df_{3}$ 未知。令 $w_{j} = \\mathrm{idf}_{j} = \\ln\\!\\left(\\frac{N}{df_{j}}\\right)$ 表示词项 $t_{j}$ 的 IDF。\n\n从上述核心定义（BoW 计数、TF-IDF 加权、$\\ell^{2}$-归一化向量上的余弦相似度，以及基于最大余弦相似度的 1-NN 决策）出发，完成以下任务：\n\n1. 将类别 $\\mathcal{A}$ 和 $\\mathcal{B}$ 之间的决策边界定义为与 $D_{\\mathcal{A}}$ 和 $D_{\\mathcal{B}}$ 的余弦相似度相等的查询点的轨迹。使用给定的合成数据，推导一个用 $w_{1}$、$w_{2}$ 和 $w_{3}$ 表示的解析条件，在该条件下，基于 BoW 的 1-NN 和基于 TF-IDF 的 1-NN 对固定查询 $q$ 产生不同的分类决策。你的推导必须从所提供的基本定义开始，并遵循第一性原理进行。\n\n2. 计算唯一的临界文档频率 $df_{3}^{\\star}$，在该频率下，TF-IDF 决策边界穿过查询 $q$（即 $q$ 与 $D_{\\mathcal{A}}$ 和 $D_{\\mathcal{B}}$ 的 TF-IDF 余弦相似度相等），而 BoW 1-NN 将 $q$ 分类到类别 $\\mathcal{A}$。使用给定的 $N$、$df_{1}$ 和 $df_{2}$ 来确定 $w_{1}$ 和 $w_{2}$，并求解 $df_{3}^{\\star}$。将最终数值答案四舍五入到四位有效数字。将你的答案表示为实值文档计数。为了概念上的可视化，请解释在 $(t_{1}, t_{3})$ 子空间中，相对于 BoW，TF-IDF 加权如何改变决策边界，但只需报告所要求的数值 $df_{3}^{\\star}$。", "solution": "该问题要求我们使用 1-近邻 (1-NN) 分类器和余弦相似度，在两种不同的文本表示方案（词袋 BoW 和词频-逆文档频率 TF-IDF）下，分析查询文档 $q$ 的分类。我们将首先推导一个一般条件，在该条件下，两种方案对给定查询产生不同的结果，然后求解一个特定的参数值，使得 TF-IDF 决策边界穿过该查询点。\n\n首先，我们根据给定的词频计数 $(tf_1, tf_2, tf_3)$，为训练文档 $D_{\\mathcal{A}}$、$D_{\\mathcal{B}}$ 和查询文档 $q$ 定义未归一化的向量。\n\n对于 BoW 表示，向量是原始词项计数：\n$v_{\\mathcal{A}} = (10, 0, 0)$\n$v_{\\mathcal{B}} = (0, 2, 3)$\n$v_{q} = (3, 0, 1)$\n\n对于 TF-IDF 表示，各分量由 $w_j = \\mathrm{idf}_j = \\ln(N/df_j)$ 加权。得到的向量是：\n$v'_{\\mathcal{A}} = (10 \\cdot w_1, 0 \\cdot w_2, 0 \\cdot w_3) = (10w_1, 0, 0)$\n$v'_{\\mathcal{B}} = (0 \\cdot w_1, 2 \\cdot w_2, 3 \\cdot w_3) = (0, 2w_2, 3w_3)$\n$v'_{q} = (3 \\cdot w_1, 0 \\cdot w_2, 1 \\cdot w_3) = (3w_1, 0, w_3)$\n\n两个向量 $u$ 和 $v$ 之间的余弦相似度由 $S(u, v) = \\frac{u \\cdot v}{\\|u\\|_2 \\|v\\|_2}$ 给出。1-NN 决策规则将查询分配给具有最高余弦相似度的训练文档的类别。\n\n我们首先分析 BoW 的情况。我们计算 BoW 向量的 $\\ell^2$-范数：\n$\\|v_{\\mathcal{A}}\\|_2 = \\sqrt{10^2 + 0^2 + 0^2} = 10$\n$\\|v_{\\mathcal{B}}\\|_2 = \\sqrt{0^2 + 2^2 + 3^2} = \\sqrt{4+9} = \\sqrt{13}$\n$\\|v_{q}\\|_2 = \\sqrt{3^2 + 0^2 + 1^2} = \\sqrt{9+1} = \\sqrt{10}$\n\nBoW 情况下的余弦相似度为：\n$S_{\\text{BoW}}(q, D_{\\mathcal{A}}) = \\frac{v_q \\cdot v_{\\mathcal{A}}}{\\|v_q\\|_2 \\|v_{\\mathcal{A}}\\|_2} = \\frac{(3)(10) + (0)(0) + (1)(0)}{\\sqrt{10} \\cdot 10} = \\frac{30}{10\\sqrt{10}} = \\frac{3}{\\sqrt{10}}$\n$S_{\\text{BoW}}(q, D_{\\mathcal{B}}) = \\frac{v_q \\cdot v_{\\mathcal{B}}}{\\|v_q\\|_2 \\|v_{\\mathcal{B}}\\|_2} = \\frac{(3)(0) + (0)(2) + (1)(3)}{\\sqrt{10} \\sqrt{13}} = \\frac{3}{\\sqrt{130}}$\n\n为了比较这些值，我们注意到 $10  130$，所以 $\\sqrt{10}  \\sqrt{130}$，这意味着 $\\frac{1}{\\sqrt{10}}  \\frac{1}{\\sqrt{130}}$。因此，$S_{\\text{BoW}}(q, D_{\\mathcal{A}})  S_{\\text{BoW}}(q, D_{\\mathcal{B}})$。基于 BoW 的 1-NN 分类器将查询 $q$ 分类到类别 $\\mathcal{A}$。\n\n现在，我们来分析 TF-IDF 的情况。TF-IDF 向量的范数是：\n$\\|v'_{\\mathcal{A}}\\|_2 = \\sqrt{(10w_1)^2} = 10w_1$ (因为 $w_1 = \\ln(1000/900)0$)\n$\\|v'_{\\mathcal{B}}\\|_2 = \\sqrt{(2w_2)^2 + (3w_3)^2} = \\sqrt{4w_2^2 + 9w_3^2}$\n$\\|v'_{q}\\|_2 = \\sqrt{(3w_1)^2 + (w_3)^2} = \\sqrt{9w_1^2 + w_3^2}$\n\nTF-IDF 情况下的余弦相似度为：\n$S_{\\text{TF-IDF}}(q, D_{\\mathcal{A}}) = \\frac{v'_q \\cdot v'_{\\mathcal{A}}}{\\|v'_q\\|_2 \\|v'_{\\mathcal{A}}\\|_2} = \\frac{(3w_1)(10w_1)}{\\sqrt{9w_1^2 + w_3^2} \\cdot 10w_1} = \\frac{3w_1}{\\sqrt{9w_1^2 + w_3^2}}$\n$S_{\\text{TF-IDF}}(q, D_{\\mathcal{B}}) = \\frac{v'_q \\cdot v'_{\\mathcal{B}}}{\\|v'_q\\|_2 \\|v'_{\\mathcal{B}}\\|_2} = \\frac{(3w_1)(0) + (0)(2w_2) + (w_3)(3w_3)}{\\sqrt{9w_1^2 + w_3^2} \\sqrt{4w_2^2 + 9w_3^2}} = \\frac{3w_3^2}{\\sqrt{9w_1^2 + w_3^2} \\sqrt{4w_2^2 + 9w_3^2}}$\n\n**第 1 部分：产生不同决策的条件**\n如果 TF-IDF 分类器将 $q$ 分类到类别 $\\mathcal{B}$，那么基于 BoW 和 TF-IDF 的分类器会产生不同的决策。这种情况发生在 $S_{\\text{TF-IDF}}(q, D_{\\mathcal{B}})  S_{\\text{TF-IDF}}(q, D_{\\mathcal{A}})$ 时。\n$$ \\frac{3w_3^2}{\\sqrt{9w_1^2 + w_3^2} \\sqrt{4w_2^2 + 9w_3^2}}  \\frac{3w_1}{\\sqrt{9w_1^2 + w_3^2}} $$\n由于项 $\\sqrt{9w_1^2 + w_3^2}$ 是正数，我们可以将其乘到不等式两边，并同时除以 $3$：\n$$ \\frac{w_3^2}{\\sqrt{4w_2^2 + 9w_3^2}}  w_1 $$\n假设 $w_1, w_2, w_3$ 是正数（对于 $1 \\le df_j  N$ 的情况，它们确实是正数），不等式两边都为正，所以我们可以对它们进行平方而不改变不等号的方向：\n$$ \\frac{w_3^4}{4w_2^2 + 9w_3^2}  w_1^2 $$\n$$ w_3^4  w_1^2 (4w_2^2 + 9w_3^2) $$\n$$ w_3^4  4w_1^2 w_2^2 + 9w_1^2 w_3^2 $$\n重新整理各项，得到最终的解析条件：\n$$ w_3^4 - 9w_1^2 w_3^2 - 4w_1^2 w_2^2  0 $$\n\n**第 2 部分：临界文档频率 $df_3^{\\star}$ 的计算**\n当相似度相等时，即 $S_{\\text{TF-IDF}}(q, D_{\\mathcal{A}}) = S_{\\text{TF-IDF}}(q, D_{\\mathcal{B}})$，TF-IDF 决策边界穿过查询点 $q$。这对应于上面推导出的条件的等式情况：\n$$ w_3^4 - 9w_1^2 w_3^2 - 4w_1^2 w_2^2 = 0 $$\n这是一个关于变量 $w_3^2$ 的一元二次方程。令 $x = w_3^2$。方程为 $x^2 - (9w_1^2)x - (4w_1^2 w_2^2) = 0$。\n\n我们首先使用提供的数据计算 $w_1$ 和 $w_2$ 的值：$N=1000$，$df_1=900$，$df_2=50$。\n$w_1 = \\ln\\left(\\frac{1000}{900}\\right) = \\ln\\left(\\frac{10}{9}\\right)$\n$w_2 = \\ln\\left(\\frac{1000}{50}\\right) = \\ln(20)$\n\n我们使用一元二次方程求根公式 $x = \\frac{-b \\pm \\sqrt{b^2-4ac}}{2a}$ 求解 $x=w_3^2$，其中 $a=1$，$b=-9w_1^2$，$c=-4w_1^2 w_2^2$。\n$$ x = w_3^2 = \\frac{9w_1^2 \\pm \\sqrt{(-9w_1^2)^2 - 4(1)(-4w_1^2 w_2^2)}}{2} $$\n$$ w_3^2 = \\frac{9w_1^2 \\pm \\sqrt{81w_1^4 + 16w_1^2 w_2^2}}{2} $$\n由于 $w_3^2$ 必须是非负的，我们必须取正根。平方根下的项 $\\sqrt{81w_1^4 + 16w_1^2 w_2^2}  \\sqrt{81w_1^4} = 9w_1^2$，所以如果我们选择负号，分子将为负。因此，我们选择正号：\n$$ w_3^2 = \\frac{9w_1^2 + \\sqrt{81w_1^4 + 16w_1^2 w_2^2}}{2} $$\n现在我们代入权重的数值：\n$w_1 = \\ln(10/9) \\approx 0.1053605$\n$w_2 = \\ln(20) \\approx 2.9957323$\n$w_1^2 \\approx 0.0111008$\n$w_2^2 \\approx 8.974415$\n\n关于 $x=w_3^2$ 的一元二次方程近似为：\n$x^2 - 9(0.0111008)x - 4(0.0111008)(8.974415) = 0$\n$x^2 - 0.0999072x - 0.398556 = 0$\n\n求解 $x$：\n$$ x = \\frac{0.0999072 + \\sqrt{(-0.0999072)^2 - 4(1)(-0.398556)}}{2} $$\n$$ x = \\frac{0.0999072 + \\sqrt{0.0099814 + 1.594224}}{2} $$\n$$ x = \\frac{0.0999072 + \\sqrt{1.6042054}}{2} $$\n$$ x = \\frac{0.0999072 + 1.2665723}{2} = \\frac{1.3664795}{2} = 0.68323975 $$\n所以，$w_3^2 \\approx 0.68323975$。取平方根求 $w_3$：\n$w_3 = \\sqrt{0.68323975} \\approx 0.8265831$\n\n临界文档频率 $df_3^{\\star}$ 可通过反转 IDF 公式 $w_3 = \\ln(N/df_3^{\\star})$ 求得：\n$$ df_3^{\\star} = \\frac{N}{\\exp(w_3)} = \\frac{1000}{\\exp(0.8265831)} $$\n$$ df_3^{\\star} = \\frac{1000}{2.285460} \\approx 437.5492 $$\n四舍五入到四位有效数字，我们得到 $df_3^{\\star} = 437.5$。正如开头所确定的，BoW 分类器将 $q$ 分类到类别 $\\mathcal{A}$，因此这个结果满足问题的所有条件。", "answer": "$$ \\boxed{437.5} $$", "id": "3179893"}, {"introduction": "为了超越像TF-IDF这样的简单启发式方法，我们现在将探索概率主题模型。这个练习 [@problem_id:3179899] 要求你从第一性原理出发，为基础的单混合一元文法模型（Mixture of Unigrams）推导期望最大化（EM）算法。这个过程不仅能巩固你对潜变量建模的理解，还将揭示其概念上的局限性，从而阐明为何需要像潜在狄利克雷分配（Latent Dirichlet Allocation, LDA）这样更高级的模型。", "problem": "考虑一个语料库的词袋表示，该语料库包含 $D$ 个文档，词汇量大小为 $V$，以及 $K$ 个潜在主题。每个文档 $d \\in \\{1,\\dots,D\\}$ 由一个计数向量 $\\mathbf{n}_{d} = (n_{d,1},\\dots,n_{d,V})$ 表示，其总长度为 $N_{d} = \\sum_{v=1}^{V} n_{d,v}$。在 unigram 混合模型（多项分布的混合）中，每个文档 $d$ 的生成过程如下：根据在概率单纯形（$\\sum_{k=1}^{K} \\pi_{k} = 1$, $\\pi_{k} \\geq 0$）上的混合比例 $\\boldsymbol{\\pi} = (\\pi_{1},\\dots,\\pi_{K})$ 抽取一个单一的潜在主题 $z_{d} \\in \\{1,\\dots,K\\}$，然后从一个特定于主题的多项分布 $\\boldsymbol{\\phi}_{k} = (\\phi_{k,1},\\dots,\\phi_{k,V})$（在词汇单纯形上，$\\sum_{v=1}^{V} \\phi_{k,v} = 1$, $\\phi_{k,v} \\geq 0$）中独立地抽取 $N_{d}$ 个词元。在主题 $k$ 下，文档 $d$ 的观测数据似然为 $p(\\mathbf{n}_{d} \\mid z_{d}=k) = \\frac{N_{d}!}{\\prod_{v=1}^{V} n_{d,v}!} \\prod_{v=1}^{V} \\phi_{k,v}^{n_{d,v}}$。\n\n期望最大化（EM）算法通过迭代一个期望步骤和一个最大化步骤，来寻求最大化观测对数似然 $\\ln p(\\{\\mathbf{n}_{d}\\}_{d=1}^{D} \\mid \\Theta)$，其中 $\\Theta = \\{\\boldsymbol{\\pi}, \\{\\boldsymbol{\\phi}_{k}\\}_{k=1}^{K}\\}$。期望步骤计算给定当前参数下潜在变量的后验分布，而最大化步骤则在概率单纯形约束下最大化期望完整数据对数似然。请从这些核心定义和通用的 EM 框架开始；不要引用任何预先推导出的更新公式。\n\n任务：\n- 从第一性原理出发，推导 unigram 混合模型的 EM 更新公式：期望步骤中响应度 $\\gamma_{d,k} = p(z_{d}=k \\mid \\mathbf{n}_{d}, \\Theta)$ 的表达式，以及在最大化步骤中使用拉格朗日乘子强制执行单纯形约束的参数 $\\pi_{k}$ 和 $\\phi_{k,v}$ 的更新公式。\n- 比较 unigram 混合模型与潜在狄利克雷分配（LDA; Latent Dirichlet Allocation）在概念和建模上的局限性。LDA 在词元级别上分配主题，并对文档-主题比例和主题-词分布施加狄利克雷先验。解释为什么“每个文档单一主题”的假设限制了模型的表达能力，并将其与词袋设置联系起来。讨论词频-逆文档频率（TF–IDF; Term Frequency–Inverse Document Frequency）加权与概率生成建模有何不同，以及为什么天真地用 TF–IDF 权重替代计数会破坏多项分布似然的解释。\n- 分析收敛性：论证为什么该模型的 EM 算法能保证观测对数似然在迭代过程中非递减，并解释由于非凸性而可能收敛到局部最优解。基于 EM 下界和 Jensen 不等式，提供一个简洁且有原则的解释。\n- 最后，考虑以下具体的语料库和初始化。设 $V = 3$，$K = 2$，$D = 2$。文档如下：\n  - 文档 $d=1$: $\\mathbf{n}_{1} = (n_{1,1}, n_{1,2}, n_{1,3}) = (2, 1, 0)$，其中 $N_{1} = 3$，\n  - 文档 $d=2$: $\\mathbf{n}_{2} = (n_{2,1}, n_{2,2}, n_{2,3}) = (0, 1, 2)$，其中 $N_{2} = 3$。\n  初始化参数为 $\\boldsymbol{\\pi} = (\\pi_{1}, \\pi_{2}) = (0.5, 0.5)$，$\\boldsymbol{\\phi}_{1} = (\\phi_{1,1}, \\phi_{1,2}, \\phi_{1,3}) = (0.6, 0.3, 0.1)$，以及 $\\boldsymbol{\\phi}_{2} = (\\phi_{2,1}, \\phi_{2,2}, \\phi_{2,3}) = (0.1, 0.3, 0.6)$。执行一次完整的 EM 迭代（计算响应度然后更新参数），然后计算 $\\phi_{1,1}$ 的更新值。将您的最终数值答案四舍五入到四位有效数字。", "solution": "该问题是有效的，因为它在科学上基于统计学习理论，定义明确，提供了所有必要的数据和定义，并且其表述是客观的。它要求基于标准模型和算法进行推导、概念分析和数值计算。\n\n### 第1部分：期望最大化（EM）更新的推导\n\nunigram 混合模型假设每个文档 $d$ 都由一个单一的潜在主题 $z_d$ 生成，该主题从一个带参数 $\\boldsymbol{\\pi}$ 的分布中抽取，然后其词语从相应的主题-词分布 $\\boldsymbol{\\phi}_{z_d}$ 中抽取。我们旨在找到参数 $\\Theta = \\{\\boldsymbol{\\pi}, \\{\\boldsymbol{\\phi}_{k}\\}_{k=1}^{K}\\}$，以最大化一个包含 $D$ 个文档的语料库的观测数据对数似然，即 $\\mathcal{L}(\\Theta) = \\ln p(\\{\\mathbf{n}_{d}\\}_{d=1}^{D} \\mid \\Theta) = \\sum_{d=1}^{D} \\ln p(\\mathbf{n}_{d} \\mid \\Theta)$。期望最大化（EM）算法是为此目的设计的迭代方法。\n\n单个文档 $\\mathbf{n}_d$ 的边际似然通过对潜在主题分配求和得到：\n$$p(\\mathbf{n}_{d} \\mid \\Theta) = \\sum_{k=1}^{K} p(\\mathbf{n}_{d}, z_{d}=k \\mid \\Theta) = \\sum_{k=1}^{K} p(\\mathbf{n}_{d} \\mid z_d=k, \\Theta) p(z_d=k \\mid \\Theta)$$\n$$p(\\mathbf{n}_{d} \\mid \\Theta) = \\sum_{k=1}^{K} \\pi_k \\left( \\frac{N_{d}!}{\\prod_{v=1}^{V} n_{d,v}!} \\prod_{v=1}^{V} \\phi_{k,v}^{n_{d,v}} \\right)$$\n\n**E-步骤：计算响应度**\n\n期望步骤计算每个文档的潜在主题分配的后验概率，给定观测到的词计数 $\\mathbf{n}_d$ 和当前的参数估计 $\\Theta^{(t)}$。这些后验概率被称为响应度，记为 $\\gamma_{d,k}$。\n$$\\gamma_{d,k} = p(z_{d}=k \\mid \\mathbf{n}_{d}, \\Theta^{(t)})$$\n使用贝叶斯定理：\n$$\\gamma_{d,k} = \\frac{p(\\mathbf{n}_{d} \\mid z_{d}=k, \\Theta^{(t)}) p(z_{d}=k \\mid \\Theta^{(t)})}{p(\\mathbf{n}_{d} \\mid \\Theta^{(t)})} = \\frac{p(\\mathbf{n}_{d} \\mid z_{d}=k, \\Theta^{(t)}) p(z_{d}=k \\mid \\Theta^{(t)})}{\\sum_{j=1}^{K} p(\\mathbf{n}_{d} \\mid z_{d}=j, \\Theta^{(t)}) p(z_{d}=j \\mid \\Theta^{(t)})}$$\n代入模型定义，$p(z_{d}=k \\mid \\Theta^{(t)}) = \\pi_k^{(t)}$ 和 $p(\\mathbf{n}_{d} \\mid z_{d}=k, \\Theta^{(t)}) = C_d \\prod_{v=1}^{V} (\\phi_{k,v}^{(t)})^{n_{d,v}}$，其中 $C_d = \\frac{N_{d}!}{\\prod_{v=1}^{V} n_{d,v}!}$ 是多项式系数。由于 $C_d$ 不依赖于主题索引 $k$ 或参数，它可以从分子和分母中约去。\n$$\\gamma_{d,k} = \\frac{\\pi_k^{(t)} \\prod_{v=1}^{V} (\\phi_{k,v}^{(t)})^{n_{d,v}}}{\\sum_{j=1}^{K} \\pi_j^{(t)} \\prod_{v=1}^{V} (\\phi_{j,v}^{(t)})^{n_{d,v}}}$$\n这是 E-步骤中响应度的更新公式。\n\n**M-步骤：最大化期望完整数据对数似然**\n\n最大化步骤找到新的参数估计 $\\Theta^{(t+1)}$，以最大化期望完整数据对数似然 $Q(\\Theta \\mid \\Theta^{(t)})$。完整数据对数似然为：\n$$\\ln p(\\{\\mathbf{n}_d, z_d\\}_{d=1}^D \\mid \\Theta) = \\sum_{d=1}^{D} \\ln p(\\mathbf{n}_d, z_d \\mid \\Theta) = \\sum_{d=1}^{D} \\sum_{k=1}^{K} \\mathbb{I}(z_d=k) \\ln \\left( \\pi_k p(\\mathbf{n}_d \\mid z_d=k, \\Theta) \\right)$$\n$$= \\sum_{d=1}^{D} \\sum_{k=1}^{K} \\mathbb{I}(z_d=k) \\left( \\ln \\pi_k + \\ln C_d + \\sum_{v=1}^{V} n_{d,v} \\ln \\phi_{k,v} \\right)$$\n其中 $\\mathbb{I}(\\cdot)$ 是指示函数。期望 $Q(\\Theta \\mid \\Theta^{(t)})$ 是关于后验分布 $p(\\{z_d\\} \\mid \\{\\mathbf{n}_d\\}, \\Theta^{(t)})$ 计算的。指示函数的期望是响应度：$E[\\mathbb{I}(z_d=k)] = p(z_d=k \\mid \\mathbf{n}_d, \\Theta^{(t)}) = \\gamma_{d,k}$。\n$$Q(\\Theta \\mid \\Theta^{(t)}) = \\sum_{d=1}^{D} \\sum_{k=1}^{K} \\gamma_{d,k} \\left( \\ln \\pi_k + \\sum_{v=1}^{V} n_{d,v} \\ln \\phi_{k,v} \\right) + \\text{const.}$$\n项 $\\sum_{d=1}^D \\sum_{k=1}^K \\gamma_{d,k} \\ln C_d$ 相对于 $\\Theta$ 是常数，在最大化过程中可以忽略。我们必须在约束条件 $\\sum_{k=1}^{K} \\pi_k = 1$ 和对所有 $k \\in \\{1,\\dots,K\\}$ 都有 $\\sum_{v=1}^{V} \\phi_{k,v} = 1$ 的情况下最大化 $Q$。\n\n$\\pi_k$ 的更新：我们构造涉及 $\\boldsymbol{\\pi}$ 的项的拉格朗日函数：\n$$\\mathcal{L}_{\\pi} = \\sum_{d=1}^{D} \\sum_{k=1}^{K} \\gamma_{d,k} \\ln \\pi_k + \\lambda_{\\pi} \\left( \\sum_{k=1}^{K} \\pi_k - 1 \\right)$$\n对 $\\pi_k$ 求导并令其为零：\n$$\\frac{\\partial \\mathcal{L}_{\\pi}}{\\partial \\pi_k} = \\frac{1}{\\pi_k} \\sum_{d=1}^{D} \\gamma_{d,k} + \\lambda_{\\pi} = 0 \\implies \\pi_k = -\\frac{\\sum_{d=1}^{D} \\gamma_{d,k}}{\\lambda_{\\pi}}$$\n对 $k$ 求和：$\\sum_{k=1}^{K} \\pi_k = 1 = -\\frac{1}{\\lambda_{\\pi}} \\sum_{k=1}^{K} \\sum_{d=1}^{D} \\gamma_{d,k}$。由于 $\\sum_{k=1}^{K} \\gamma_{d,k} = 1$，双重求和等于 $D$。因此，$1 = -D/\\lambda_{\\pi}$，所以 $\\lambda_{\\pi} = -D$。代回得到更新规则：\n$$\\pi_k^{(t+1)} = \\frac{\\sum_{d=1}^{D} \\gamma_{d,k}}{D}$$\n\n$\\phi_{k,v}$ 的更新：涉及 $\\boldsymbol{\\phi}_k$ 的项对于每个主题 $k$ 是独立的。对于固定的 $k$，我们构造拉格朗日函数：\n$$\\mathcal{L}_{\\phi_k} = \\sum_{d=1}^{D} \\gamma_{d,k} \\sum_{v=1}^{V} n_{d,v} \\ln \\phi_{k,v} + \\lambda_{\\phi_k} \\left( \\sum_{v=1}^{V} \\phi_{k,v} - 1 \\right)$$\n对 $\\phi_{k,v}$ 求导并令其为零：\n$$\\frac{\\partial \\mathcal{L}_{\\phi_k}}{\\partial \\phi_{k,v}} = \\frac{1}{\\phi_{k,v}} \\sum_{d=1}^{D} \\gamma_{d,k} n_{d,v} + \\lambda_{\\phi_k} = 0 \\implies \\phi_{k,v} = -\\frac{\\sum_{d=1}^{D} \\gamma_{d,k} n_{d,v}}{\\lambda_{\\phi_k}}$$\n对 $v$ 求和：$\\sum_{v=1}^{V} \\phi_{k,v} = 1 = -\\frac{1}{\\lambda_{\\phi_k}} \\sum_{v=1}^{V} \\sum_{d=1}^{D} \\gamma_{d,k} n_{d,v}$。令双重求和为 $S_k = \\sum_{d=1}^{D} \\gamma_{d,k} \\sum_{v=1}^{V} n_{d,v} = \\sum_{d=1}^{D} \\gamma_{d,k} N_d$。这是分配给主题 $k$ 的有效总词数。那么 $1 = -S_k/\\lambda_{\\phi_k}$，所以 $\\lambda_{\\phi_k} = -S_k$。代回得到更新规则：\n$$\\phi_{k,v}^{(t+1)} = \\frac{\\sum_{d=1}^{D} \\gamma_{d,k} n_{d,v}}{\\sum_{d=1}^{D} \\gamma_{d,k} N_d} = \\frac{\\sum_{d=1}^{D} \\gamma_{d,k} n_{d,v}}{\\sum_{j=1}^{V} \\sum_{d=1}^{D} \\gamma_{d,k} n_{d,j}}$$\n\n### 第2部分：概念和建模上的局限性\n\nunigram 混合模型（MoU）虽然是一个基础的主题模型，但与更高级的模型如潜在狄利克雷分配（LDA）相比，具有显著的局限性，其概率性质也使其与启发式方法如词频-逆文档频率（TF–IDF）区别开来。\n\n- **unigram 混合模型 vs. 潜在狄利克雷分配（LDA）**：主要区别在于对文档级主题构成的建模。MoU 假设每个文档都恰好由一个主题生成。这种“每个文档单一主题”的假设严重限制了其表达能力。一个文档（例如，一篇科学论文、一篇新闻文章）通常讨论多个主题。LDA 通过假设每个文档是主题的混合来解决这个问题。对于文档中的每个词元，都从该文档特定的主题混合中选择一个主题，然后从所选主题的词分布中抽取该词。这允许一个文档由一个关于主题的比例向量来表示（例如，40% 的物理学，30% 的计算机科学，30% 的数学），这对于大多数真实世界的语料库来说更为现实。这种词元级别的主题分配比 MoU 中的文档级别分配是一种更细粒度、更强大的表示。词袋设置（忽略词序）是两种模型共有的，但 LDA 通过对主题混合建模更有效地利用了它。\n\n- **TF-IDF vs. 概率模型**：TF-IDF 是一种数值加权方案，而不是一个概率生成模型。它对于文档 $d$ 中词项 $t$ 的值 $\\text{TF-IDF}(t,d)$ 是一个启发式分数，旨在反映该词项在该语料库中对该文档的重要性。相比之下，像 MoU 和 LDA 这样的概率模型为文档是如何创建的提供了一个生成过程的故事。这些模型的参数（例如，$\\boldsymbol{\\phi}_k$）是具有清晰解释的概率。在多项分布似然函数 $p(\\mathbf{n}_{d} \\mid ...)=C_d\\prod_{v} \\phi_{k,v}^{n_{d,v}}$ 中，天真地用非整数实数值的 TF-IDF 权重替代整数词计数 $n_{d,v}$ 从根本上破坏了模型。多项分布是为离散计数定义的。使用实数值指数 $\\phi_{k,v}^{\\text{TF-IDF}_{d,v}}$ 会改变该函数，但它将不再表示在多项模型下观测到计数的似然。包括生成过程和基于原理的 EM 参数估计在内的统计基础将因此失效。\n\n### 第3部分：收敛性分析\n\nEM 算法保证产生一系列参数估计 $\\Theta^{(t)}$，使得观测数据对数似然 $\\mathcal{L}(\\Theta^{(t)}) = \\ln p(X \\mid \\Theta^{(t)})$ 是非递减的。这可以通过在对数似然上引入证据下界（ELBO）来证明。对于潜在变量 $Z$ 上的任何辅助分布 $q(Z)$，凹的对数函数的 Jensen 不等式允许我们写出：\n$$\\mathcal{L}(\\Theta) = \\ln \\sum_Z p(X, Z \\mid \\Theta) = \\ln \\sum_Z q(Z)\\frac{p(X, Z \\mid \\Theta)}{q(Z)} \\geq \\sum_Z q(Z) \\ln \\frac{p(X, Z \\mid \\Theta)}{q(Z)} = \\mathcal{F}(q, \\Theta)$$\nEM 算法可以看作是在这个下界 $\\mathcal{F}(q, \\Theta)$ 上的坐标上升算法。\n1.  **E-步骤**：固定参数 $\\Theta^{(t)}$，我们关于 $q(Z)$ 最大化 $\\mathcal{F}(q, \\Theta^{(t)})$。当 $q(Z)$ 被设置为潜在变量的后验分布 $p(Z \\mid X, \\Theta^{(t)})$ 时，达到最大值。此时，下界是紧的，即 $\\mathcal{L}(\\Theta^{(t)}) = \\mathcal{F}(p(Z \\mid X, \\Theta^{(t)}), \\Theta^{(t)})$。\n2.  **M-步骤**：固定 $q(Z) = p(Z \\mid X, \\Theta^{(t)})$，我们通过关于 $\\Theta$ 最大化 $\\mathcal{F}(q, \\Theta)$ 来找到新的参数 $\\Theta^{(t+1)}$。根据最大化的定义，$\\mathcal{F}(q, \\Theta^{(t+1)}) \\geq \\mathcal{F}(q, \\Theta^{(t)})$。\n\n结合这些步骤，我们得到序列：\n$$\\mathcal{L}(\\Theta^{(t+1)}) \\geq \\mathcal{F}(p(Z \\mid X, \\Theta^{(t)}), \\Theta^{(t+1)}) \\geq \\mathcal{F}(p(Z \\mid X, \\Theta^{(t)}), \\Theta^{(t)}) = \\mathcal{L}(\\Theta^{(t)})$$\n因此，$\\mathcal{L}(\\Theta^{(t+1)}) \\geq \\mathcal{L}(\\Theta^{(t)})$。\n\n然而，混合模型的对数似然曲面通常是非凸的。EM 是在这个曲面上的一个确定性的爬山过程。虽然它保证能找到一个驻点（局部最大值或鞍点），但它不保证能找到全局最大值。它最终收敛到的参数对初始参数设置 $\\Theta^{(0)}$ 是敏感的。\n\n### 第4部分：数值计算\n\n给定：$V=3$，$K=2$，$D=2$。\n- 文档 $d=1$: $\\mathbf{n}_{1} = (2, 1, 0)$，$N_1 = 3$。\n- 文档 $d=2$: $\\mathbf{n}_{2} = (0, 1, 2)$，$N_2 = 3$。\n初始参数 $\\Theta^{(0)}$:\n- $\\boldsymbol{\\pi}^{(0)} = (0.5, 0.5)$。\n- $\\boldsymbol{\\phi}_{1}^{(0)} = (0.6, 0.3, 0.1)$。\n- $\\boldsymbol{\\phi}_{2}^{(0)} = (0.1, 0.3, 0.6)$。\n\n**E-步骤：计算响应度 $\\gamma_{d,k}$**\n\n首先，我们计算未归一化的概率 $\\alpha_{d,k} = \\pi_k^{(0)} \\prod_{v=1}^{V} (\\phi_{k,v}^{(0)})^{n_{d,v}}$。\n对于文档 $d=1$：\n- $\\alpha_{1,1} = 0.5 \\times (0.6^2 \\times 0.3^1 \\times 0.1^0) = 0.5 \\times (0.36 \\times 0.3) = 0.5 \\times 0.108 = 0.054$。\n- $\\alpha_{1,2} = 0.5 \\times (0.1^2 \\times 0.3^1 \\times 0.6^0) = 0.5 \\times (0.01 \\times 0.3) = 0.5 \\times 0.003 = 0.0015$。\n归一化常数是 $\\sum_j \\alpha_{1,j} = 0.054 + 0.0015 = 0.0555$。\n- $\\gamma_{1,1} = \\frac{0.054}{0.0555} = \\frac{540}{555} = \\frac{36}{37}$。\n- $\\gamma_{1,2} = \\frac{0.0015}{0.0555} = \\frac{15}{555} = \\frac{1}{37}$。\n\n对于文档 $d=2$：\n- $\\alpha_{2,1} = 0.5 \\times (0.6^0 \\times 0.3^1 \\times 0.1^2) = 0.5 \\times (0.3 \\times 0.01) = 0.5 \\times 0.003 = 0.0015$。\n- $\\alpha_{2,2} = 0.5 \\times (0.1^0 \\times 0.3^1 \\times 0.6^2) = 0.5 \\times (0.3 \\times 0.36) = 0.5 \\times 0.108 = 0.054$。\n归一化常数是 $\\sum_j \\alpha_{2,j} = 0.0015 + 0.054 = 0.0555$。\n- $\\gamma_{2,1} = \\frac{0.0015}{0.0555} = \\frac{15}{555} = \\frac{1}{37}$。\n- $\\gamma_{2,2} = \\frac{0.054}{0.0555} = \\frac{540}{555} = \\frac{36}{37}$。\n\n**M-步骤：更新参数**\n\n我们需要计算 $\\phi_{1,1}$ 的更新值，记为 $\\phi_{1,1}^{(1)}$。\n使用推导出的公式：\n$$\\phi_{1,1}^{(1)} = \\frac{\\sum_{d=1}^{2} \\gamma_{d,1} n_{d,1}}{\\sum_{d=1}^{2} \\gamma_{d,1} N_d}$$\n分子计算：\n$$\\sum_{d=1}^{2} \\gamma_{d,1} n_{d,1} = \\gamma_{1,1} n_{1,1} + \\gamma_{2,1} n_{2,1} = \\left(\\frac{36}{37}\\right) \\times 2 + \\left(\\frac{1}{37}\\right) \\times 0 = \\frac{72}{37}$$\n分母计算：\n$$\\sum_{d=1}^{2} \\gamma_{d,1} N_d = \\gamma_{1,1} N_1 + \\gamma_{2,1} N_2 = \\left(\\frac{36}{37}\\right) \\times 3 + \\left(\\frac{1}{37}\\right) \\times 3 = \\frac{108}{37} + \\frac{3}{37} = \\frac{111}{37} = 3$$\n因此，更新后的参数是：\n$$\\phi_{1,1}^{(1)} = \\frac{72/37}{3} = \\frac{72}{37 \\times 3} = \\frac{24}{37}$$\n作为小数，四舍五入到四位有效数字：\n$$\\phi_{1,1}^{(1)} \\approx 0.6486486... \\approx 0.6486$$", "answer": "$$\n\\boxed{0.6486}\n$$", "id": "3179899"}, {"introduction": "基于我们对概率模型和$IDF$的理解，这最后一个练习将这些概念应用于一个实际的异常检测问题。你将设计一个系统，利用两种不同的信号 [@problem_id:3179937] 来检测“稀有主题泄漏”——即来自未知主题的词语的出现。这个练习展示了如何将概率推理与统计启发式方法相结合，以构建能够识别文本数据中新颖或意外内容的稳健系统。", "problem": "设计并实现一个程序，该程序利用从概率文本模型的第一性原理和逆文档频率的定义中派生出的两个互补信号，来检测文档中的罕见主题泄露。罕见主题泄露指的是，一个文档从训练中未出现过的未知主题中抽取了少量词语，这应导致在已知主题上的后验分布出现异常，并使得高逆文档频率的词语被过度代表。\n\n从以下基础开始：\n- 词袋表示法将每个文档视为在一个固定词汇表上的计数，而单一潜在主题下的文档似然由一个具有特定主题分类概率的多项式模型给出。主题先验是分类分布。根据这些定义，推导出给定词袋计数的条件下主题的后验概率，并将异常得分定义为一减去最大后验质量，该得分表示模型对任何单个已知主题的置信度有多低。\n- 逆文档频率 (IDF) 是一种经过充分检验的统计量，它能提升在整个训练集中稀有词语的权重。使用平滑逆文档频率的定义 $idf(w) = \\ln\\!\\left(\\dfrac{1+N}{1+df(w)}\\right) + 1$，其中 $N$ 是训练文档的数量，$df(w)$ 是包含词语 $w$ 至少一次的训练文档数量。\n\n你的任务：\n- 为每个测试文档计算两个布尔标志：\n  1. 一个后验异常标志，指示已知主题上的后验分布是否异常弥散：在具有分类先验和多项式似然的单主题词袋模型下，从第一性原理计算后验概率 $p(z \\mid d)$；定义异常得分 $A_{\\text{post}}(d) = 1 - \\max_{z} p(z \\mid d)$，并在 $A_{\\text{post}}(d) \\ge \\gamma$ 时标记。\n  2. 一个逆文档频率标志，指示文档是否包含大部分高$idf$值的词项：计算文档中其$idf$值至少达到一个固定阈值 $t_{\\text{high}}$ 的词元所占的比例 $f_{\\text{high}}(d)$；并在 $f_{\\text{high}}(d) \\ge \\tau$ 时标记。\n\n使用以下完全指定的设置。\n\n词汇表、主题-词分布和主题先验：\n- 词汇表大小为 $V = 12$，其索引和标签为：$0\\to$ \"common0\", $1\\to$ \"common1\", $2\\to$ \"t0a\", $3\\to$ \"t0b\", $4\\to$ \"t0c\", $5\\to$ \"t0d\", $6\\to$ \"t1a\", $7\\to$ \"t1b\", $8\\to$ \"t1c\", $9\\to$ \"t1d\", $10\\to$ \"rareA\", $11\\to$ \"rareB\"。\n- 已知主题数量 $K = 2$。已知主题上的主题先验（分类分布）是均匀的：$\\pi = [0.5, 0.5]$。\n- 已知主题的主题-词概率（每行总和为 $1$）：\n  - 主题 $0$：$\\phi_{0} = [0.1, 0.1, 0.185, 0.185, 0.185, 0.185, 0.014, 0.014, 0.014, 0.014, 0.002, 0.002]$。\n  - 主题 $1$：$\\phi_{1} = [0.1, 0.1, 0.014, 0.014, 0.014, 0.014, 0.185, 0.185, 0.185, 0.185, 0.002, 0.002]$。\n- 后验异常阈值：$\\gamma = 0.3$。\n\n用于计算逆文档频率的训练语料库：\n- 训练文档数量 $N = 6$。\n- 训练文档由词汇表索引列表给出：\n  - $d_{\\text{train},0} = [2, 3, 4, 5, 2, 3, 0, 1]$,\n  - $d_{\\text{train},1} = [3, 5, 2, 4, 0]$,\n  - $d_{\\text{train},2} = [6, 7, 8, 9, 6, 7, 0, 1]$,\n  - $d_{\\text{train},3} = [6, 8, 9, 7, 1]$,\n  - $d_{\\text{train},4} = [0, 1, 0, 1]$,\n  - $d_{\\text{train},5} = [2, 6, 3, 7, 0, 1]$。\n- 使用平滑逆文档频率定义 $idf(w) = \\ln\\!\\left(\\dfrac{1+N}{1+df(w)}\\right) + 1$ 并使用自然对数。\n- 高$idf$阈值和比例阈值：$t_{\\text{high}} = 2.5$, $\\tau = 0.25$。\n\n测试套件（五个文档），每个都以词汇表索引列表的形式给出：\n- 测试 $1$（正常路径，主要为主题 $0$ 的词）：$[2, 2, 3, 4, 5, 0, 1, 2]$。\n- 测试 $2$（轻度泄露：少量未知词嵌入主题 $0$ 的内容中）：$[2, 3, 10, 0, 2, 1, 11, 10, 2]$。\n- 测试 $3$（强度泄露：多个未知词加上一个常用词）：$[10, 10, 11, 11, 0]$。\n- 测试 $4$（边界情况：仅有未知词）：$[10, 11, 10, 11, 10]$。\n- 测试 $5$（模糊情况：仅有常用词）：$[0, 1]$。\n\n实现要求：\n- 使用基于固定词汇表的词袋计数来表示每个文档。\n- 对于后验异常标志，严格从主题上的分类先验和单一潜在主题假设下的多项式文档似然出发，推导出 $p(z \\mid d)$，然后计算 $A_{\\text{post}}(d) = 1 - \\max_{z} p(z \\mid d)$。如果 $A_{\\text{post}}(d) \\ge \\gamma$，则进行标记。\n- 对于逆文档频率标志，根据定义从训练语料库中计算 $idf(w)$，计算测试文档中 $idf(w) \\ge t_{\\text{high}}$ 的词元所占的比例 $f_{\\text{high}}(d)$，如果 $f_{\\text{high}}(d) \\ge \\tau$，则进行标记。\n\n最终输出格式：\n- 你的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表。每个测试用例的结果必须是一个包含两个布尔值的列表，顺序为 $[posterior\\_anomaly\\_flag, idf\\_flag]$。例如，整体输出应类似于 $[[b_{1}, b_{2}], [b_{3}, b_{4}], \\ldots]$，其中 $b_{i} \\in \\{\\text{True}, \\text{False}\\}$。\n- 运行时不提供外部输入；所有数据必须嵌入到程序中。", "solution": "我们首先从第一性原理和核心定义出发，将两种检测信号形式化。\n\n来自单主题词袋模型的后验异常：\n- 从主题上的分类先验 $p(z) = \\pi_{z}$（其中 $z \\in \\{0, 1, \\ldots, K-1\\}$）和单一潜在主题 $z$ 下的多项式文档似然开始，其中词概率由主题-词分布 $\\phi_{z} \\in \\Delta^{V-1}$ 给出。给定一个表示为词袋计数 $\\{c_{w}\\}_{w=0}^{V-1}$ 的文档 $d$，其总词元数为 $n = \\sum_{w=0}^{V-1} c_{w}$，多项式似然与 $\\prod_{w=0}^{V-1} \\phi_{z}(w)^{c_{w}}$ 成正比，多项式系数在计算关于 $z$ 的后验时会在不同主题间抵消。\n- 根据贝叶斯法则和这些定义，主题上的未归一化后验为 $p(z \\mid d) \\propto p(z)\\,p(d \\mid z) \\propto \\pi_{z}\\,\\prod_{w=0}^{V-1} \\phi_{z}(w)^{c_{w}}$。为在实践中避免数值下溢，标准做法是在对数域中计算：$\\log \\tilde{p}(z \\mid d) = \\log \\pi_{z} + \\sum_{w=0}^{V-1} c_{w} \\log \\phi_{z}(w)$，然后减去所有 $z$ 中的最大值并取指数，通过 softmax 进行归一化，从而得到 $p(z \\mid d)$。\n- 定义异常得分 $A_{\\text{post}}(d) = 1 - \\max_{z} p(z \\mid d)$。当某个主题能清晰解释文档时，该得分较小；当后验分布弥散时（例如，在 $1/K$ 处均匀分布），该得分较大。给定阈值 $\\gamma$，如果 $A_{\\text{post}}(d) \\ge \\gamma$，我们标记为后验异常。\n\n逆文档频率标志：\n- 对于包含 $N$ 个文档的训练语料库，计算文档频率 $df(w)$，即包含词语 $w$ 至少一次的训练文档数量。\n- 使用平滑逆文档频率定义 $idf(w) = \\ln\\!\\left(\\dfrac{1+N}{1+df(w)}\\right) + 1$，并使用自然对数，这是一个核心且广泛使用的定义。这种平滑处理避免了当 $df(w) = 0$ 时出现除以零的情况。\n- 对于一个具有词元序列的测试文档 $d$，计算其 $idf$ 值达到或超过固定高阈值 $t_{\\text{high}}$ 的词元所占的比例 $f_{\\text{high}}(d)$。如果对于选定的 $\\tau$，有 $f_{\\text{high}}(d) \\ge \\tau$，我们标记为逆文档频率异常。这将“未知主题泄露会引入训练中不存在的稀有词语，而这些词语恰好具有较大的 $idf$ 值”这一直觉操作化了。\n\n具体实例化：\n- 我们使用 $K = 2$ 个主题，均匀先验 $\\pi = [0.5, 0.5]$，词汇表大小为 $V = 12$。主题-词分布以数值形式指定，将大部分概率质量置于不相交的特定主题词子集上，将中等质量分配给两个常用词（索引 $0$ 和 $1$），并将极小的质量分配给两个稀有词（索引 $10$ 和 $11$），以模拟类似未知主题的词元。这确保了来自未知主题的词语不会偏向任何一个已知主题，从而在这些词语普遍存在时，使后验分布变得弥散并增大 $A_{\\text{post}}(d)$。\n- 后验概率在对数域中计算为 $\\log \\tilde{p}(z \\mid d) = \\log \\pi_{z} + \\sum_{w} c_{w} \\log \\phi_{z}(w)$，然后通过对 $z$ 进行 softmax 归一化。异常得分 $A_{\\text{post}}(d) = 1 - \\max_{z} p(z \\mid d)$ 与 $\\gamma = 0.3$ 进行比较。\n- 对于 $idf$，我们在 $N = 6$ 个训练文档上计算 $df(w)$，然后计算 $idf(w) = \\ln\\!\\left(\\dfrac{1+6}{1+df(w)}\\right) + 1$。训练中未出现的词（索引 $10$ 和 $11$）将具有最大的 $idf$ 值（约 $2.9459$），而常用词（索引 $0$ 和 $1$）的 $idf$ 值将等于 $1$。我们设置 $t_{\\text{high}} = 2.5$ 以仅捕获真正未见过的词，并设置 $\\tau = 0.25$，以便当一个文档中至少有四分之一的词元是此类高 $idf$ 词时将其标记。\n\n测试套件覆盖范围和预期定性行为：\n- 测试 $1$（主要为主题 $0$ 的词）应产生一个集中的后验（小的 $A_{\\text{post}}$）并且很少或没有高 $idf$ 的词元，因此预期两个标志都为假。\n- 测试 $2$（轻度泄露）包含少量未知词；后验仍然集中在主题 $0$ 上（后验标志为假），但高 $idf$ 词元比例超过了 $\\tau$（逆文档频率标志为真）。\n- 测试 $3$ 和测试 $4$（强度泄露和全部未知）会在已知主题上引起接近均匀的弥散后验（后验标志为真），并且具有很高的高 $idf$ 词元比例（逆文档频率标志为真）。\n- 测试 $5$（仅有常用词）会产生一个接近均匀的后验，因为这些词无法区分主题（后验标志为真），但不包含任何高 $idf$ 词元（逆文档频率标志为假）。\n\n程序实现的算法步骤：\n- 硬编码 $\\phi$、$\\pi$、训练文档、阈值 $\\gamma$、$t_{\\text{high}}$、$\\tau$ 以及五个测试文档。\n- 通过计算每个词在多少个训练文档中至少出现一次来计算 $df(w)$，然后通过给定的定义计算 $idf(w)$。\n- 对于每个测试文档，计算词袋计数，为每个 $z \\in \\{0,1\\}$ 评估 $\\log \\tilde{p}(z \\mid d)$，进行归一化以获得 $p(z \\mid d)$，计算 $A_{\\text{post}}(d)$，并在 $A_{\\text{post}}(d) \\ge \\gamma$ 时进行标记。同时，计算 $f_{\\text{high}}(d)$ 作为 $idf(w) \\ge t_{\\text{high}}$ 的词元比例，并在 $f_{\\text{high}}(d) \\ge \\tau$ 时进行标记。\n- 按指定顺序将每个文档的结果聚合为 $[posterior\\_flag, idf\\_flag]$ 布尔值对，并打印包含这些对的单行列表。\n\n此过程直接植根于词袋文档的分类先验和多项式似然定义，以及逆文档频率的定义，并在对数域中进行仔细的数值计算以确保稳定性。", "answer": "[[False, False], [False, True], [True, True], [True, True], [True, False]]", "id": "3179937"}]}