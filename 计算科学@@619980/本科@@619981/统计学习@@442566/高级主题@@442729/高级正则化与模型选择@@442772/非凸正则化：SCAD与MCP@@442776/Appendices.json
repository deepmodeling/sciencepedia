{"hands_on_practices": [{"introduction": "在实现复杂的算法之前，首先理解其背后的数学结构至关重要。本练习将深入剖析 Minimax 凹点惩罚 (MCP) 估计器，揭示一个系数在何种精确条件下被置为零、被收缩或不受惩罚。通过从一阶最优性条件出发推导出 MCP 的分段解形式，您将对非凸正则化如何实现其“稀疏”和“近似无偏”的特性建立起坚实的理论基础。[@problem_id:3153449]", "problem": "考虑一个线性模型中单个预测变量的单变量最小二乘坐标更新，其中响应向量和预测变量已经中心化，从而截距项解耦。设中心化后的预测变量由向量表示，其元素为 $x_{1}, \\dots, x_{n}$，中心化后的响应由 $y_{1}, \\dots, y_{n}$ 表示。分别定义局部二次曲率和相关性为 $H = \\frac{1}{n} \\sum_{i=1}^{n} x_{i}^{2}$ 和 $z = \\frac{1}{n} \\sum_{i=1}^{n} x_{i} y_{i}$。考虑使用极小极大凹惩罚（MCP）的惩罚最小二乘法，其中对于 $t \\geq 0$，惩罚项的导数为 $p_{\\lambda,\\gamma}'(t) = \\big(\\lambda - \\frac{t}{\\gamma}\\big)_{+}$，且 $p_{\\lambda,\\gamma}(0) = 0$，调整参数 $\\lambda  0$，凹度参数 $\\gamma  1$。从这些定义出发，利用一阶最优性条件和次微分条件，推导坐标级最小化子关于 $z$ 和 $H$ 的精确分段形式，并确定 $|z|$ 上的两个阈值水平，这两个阈值划分了以下三种情况：解恰好为零、解非零但被收缩、解未被惩罚。\n\n现在假设预测变量被一个正常数因子 $s  0$ 缩放，使得对所有 $i$ 都有 $\\tilde{x}_{i} = s x_{i}$，从而产生变换后的量 $\\tilde{H}$ 和 $\\tilde{z}$。推导 $|\\tilde{z}|$ 上的两个阈值水平如何作为 $s$、$H$、$\\lambda$ 和 $\\gamma$ 的函数而变化。你的最终答案应该是包含 $|\\tilde{z}|$ 的下阈值和上阈值的有序对，以闭式形式表示。最终答案中不要提供任何额外的评论。", "solution": "问题要求推导使用极小极大凹惩罚（MCP）的惩罚最小二乘问题的坐标级最小化子，确定划分求解区间的阈值，并分析当预测变量被缩放时这些阈值如何变化。\n\n首先，我们验证问题陈述的有效性。\n问题在统计学习和优化的既定框架内提供了一个清晰且自洽的设定。\n已知条件是：\n- 中心化预测向量：$x_1, \\dots, x_n$。\n- 中心化响应向量：$y_1, \\dots, y_n$。\n- 局部二次曲率：$H = \\frac{1}{n} \\sum_{i=1}^{n} x_{i}^{2}$。\n- 局部相关性：$z = \\frac{1}{n} \\sum_{i=1}^{n} x_{i} y_{i}$。\n- 对于变量 $t \\ge 0$ 的 MCP 惩罚导数：$p_{\\lambda,\\gamma}'(t) = (\\lambda - \\frac{t}{\\gamma})_{+}$，其中 $(\\cdot)_{+}$ 是取正部函数。\n- 零点的 MCP 惩罚：$p_{\\lambda,\\gamma}(0) = 0$。\n- 调整参数：$\\lambda  0$。\n- 凹度参数：$\\gamma  1$。\n- 缩放后的预测变量：$\\tilde{x}_{i} = s x_{i}$，其中 $s  0$。\n- 变换后的量：$\\tilde{H}$ 和 $\\tilde{z}$。\n\n该问题具有科学依据，提法恰当，客观，并包含足够的信息以获得唯一解。它没有违反任何无效性标准。因此，该问题是有效的，我们继续进行求解。\n\n设 $\\beta$ 为单变量回归系数。要最小化的惩罚最小二乘目标函数是损失函数在当前估计值附近的二次近似。这可以表示为：\n$$ L(\\beta) = \\frac{1}{2n} \\sum_{i=1}^{n} (y_i - x_i \\beta)^2 + p_{\\lambda, \\gamma}(|\\beta|) $$\n展开平方项：\n$$ \\frac{1}{2n} \\sum_{i=1}^{n} (y_i^2 - 2 y_i x_i \\beta + x_i^2 \\beta^2) = \\frac{1}{2n}\\sum_{i=1}^{n} y_i^2 - \\beta \\left(\\frac{1}{n}\\sum_{i=1}^{n} x_i y_i\\right) + \\frac{1}{2}\\beta^2 \\left(\\frac{1}{n} \\sum_{i=1}^{n} x_i^2\\right) $$\n忽略常数项 $\\frac{1}{2n}\\sum y_i^2$（因为它不影响关于 $\\beta$ 的最小化），并代入 $H$ 和 $z$ 的定义，要最小化的目标函数为：\n$$ f(\\beta) = \\frac{1}{2} H \\beta^2 - z \\beta + p_{\\lambda, \\gamma}(|\\beta|) $$\n为了找到最小化子 $\\hat{\\beta}$，我们使用次微分的一阶最优性条件，该条件表明 $0$ 必须在 $f$ 于 $\\hat{\\beta}$ 处的次微分中：\n$$ 0 \\in \\partial f(\\hat{\\beta}) = H \\hat{\\beta} - z + \\partial p_{\\lambda, \\gamma}(|\\hat{\\beta}|) $$\n这等价于：\n$$ z - H \\hat{\\beta} \\in \\partial p_{\\lambda, \\gamma}(|\\hat{\\beta}|) $$\n为了继续，我们分析惩罚项 $g(\\beta) = p_{\\lambda, \\gamma}(|\\beta|)$ 的次微分。惩罚函数的导数给定为 $p_{\\lambda, \\gamma}'(t) = (\\lambda - t/\\gamma)_+$，其中 $t \\ge 0$。\n- 如果 $\\beta  0$，则 $\\partial g(\\beta) = \\{p_{\\lambda, \\gamma}'(\\beta)\\}$。\n- 如果 $\\beta  0$，则 $\\partial g(\\beta) = \\{-p_{\\lambda, \\gamma}'(|\\beta|)\\}$。\n- 如果 $\\beta = 0$，次微分是区间 $[-\\alpha, \\alpha]$，其中 $\\alpha = \\lim_{t \\to 0^+} p_{\\lambda, \\gamma}'(t) = \\lambda$。所以，$\\partial g(0) = [-\\lambda, \\lambda]$。\n\n我们根据 $z$ 的值在不同区间分析解 $\\hat{\\beta}$。\n\n情况 1：$\\hat{\\beta} = 0$。\n最优性条件变为 $z \\in \\partial p_{\\lambda, \\gamma}(0)$，这意味着 $z \\in [-\\lambda, \\lambda]$。因此，如果 $|z| \\le \\lambda$，解为 $\\hat{\\beta} = 0$。这确定了 $|z|$ 上的第一个阈值为 $\\lambda$。\n\n情况 2：$\\hat{\\beta} \\ne 0$。\n这在 $|z|  \\lambda$ 时发生。最优性条件是 $z - H \\hat{\\beta} = \\text{sgn}(\\hat{\\beta}) p_{\\lambda, \\gamma}'(|\\hat{\\beta}|)$。\n对于 $p_{\\lambda, \\gamma}'(|\\hat{\\beta}|) = (\\lambda - |\\hat{\\beta}|/\\gamma)_+$，我们有两种子情况：\n\n子情况 2a：$0  |\\hat{\\beta}| \\le \\lambda\\gamma$。此时，$p_{\\lambda, \\gamma}'(|\\hat{\\beta}|) = \\lambda - |\\hat{\\beta}|/\\gamma  0$。\n最优性条件是 $z - H \\hat{\\beta} = \\text{sgn}(\\hat{\\beta})(\\lambda - |\\hat{\\beta}|/\\gamma) = \\text{sgn}(\\hat{\\beta})\\lambda - \\hat{\\beta}/\\gamma$。\n重新整理项以求解 $\\hat{\\beta}$：\n$$ z - \\text{sgn}(\\hat{\\beta})\\lambda = \\hat{\\beta}(H - 1/\\gamma) $$\n假设 $H  1/\\gamma$，这是 MCP 确保得到良好估计量的一个标准条件：\n$$ \\hat{\\beta} = \\frac{z - \\text{sgn}(\\hat{\\beta})\\lambda}{H - 1/\\gamma} $$\n由于我们处于 $\\hat{\\beta} \\ne 0$ 的情况，$\\hat{\\beta}$ 的符号必须与 $z$ 的符号相同（否则分子和 $\\hat{\\beta}$ 将具有相反的符号，这对于 $|z|  \\lambda$ 是一个矛盾）。所以，$\\text{sgn}(\\hat{\\beta}) = \\text{sgn}(z)$。\n$$ \\hat{\\beta} = \\frac{z - \\text{sgn}(z)\\lambda}{H - 1/\\gamma} = \\frac{\\text{sgn}(z)(|z|-\\lambda)}{H - 1/\\gamma} $$\n该解在 $0  |\\hat{\\beta}| \\le \\lambda\\gamma$ 的条件下有效。我们已经有 $|z|\\lambda$，这确保了 $|\\hat{\\beta}|  0$。上界给出：\n$$ |\\hat{\\beta}| = \\frac{|z|-\\lambda}{H - 1/\\gamma} \\le \\lambda\\gamma $$\n$$ |z| - \\lambda \\le \\lambda\\gamma(H - 1/\\gamma) = \\lambda\\gamma H - \\lambda $$\n$$ |z| \\le \\lambda\\gamma H $$\n所以，这个“收缩”解适用于 $\\lambda  |z| \\le \\lambda\\gamma H$。\n\n子情况 2b：$|\\hat{\\beta}|  \\lambda\\gamma$。此时，$p_{\\lambda, \\gamma}'(|\\hat{\\beta}|) = 0$。\n最优性条件简化为 $z - H \\hat{\\beta} = 0$。\n这给出了该子问题的普通最小二乘解：\n$$ \\hat{\\beta} = z/H $$\n如果解的量级满足此子情况的条件，即 $|\\hat{\\beta}|  \\lambda\\gamma$，则该解有效。\n$$ |z/H|  \\lambda\\gamma \\implies |z|  \\lambda\\gamma H $$\n这是“未惩罚”的区间。\n\n$\\hat{\\beta}$ 作为 $z$ 的函数的分段最小化子总结如下：\n$$ \\hat{\\beta}(z) = \\begin{cases} 0  \\text{if } |z| \\le \\lambda \\\\ \\frac{z - \\text{sgn}(z)\\lambda}{H - 1/\\gamma}  \\text{if } \\lambda  |z| \\le \\lambda\\gamma H \\\\ z/H  \\text{if } |z|  \\lambda\\gamma H \\end{cases} $$\n分隔这三个区间的 $|z|$ 的两个阈值水平是 $\\lambda$ 和 $\\lambda\\gamma H$。\n- 分隔零解和收缩解区间的下阈值是 $\\lambda$。\n- 分隔收缩解和未惩罚解区间的上阈值是 $\\lambda\\gamma H$。\n\n现在，我们考虑问题的第二部分。预测变量被 $s  0$ 缩放：$\\tilde{x}_{i} = s x_{i}$。\n变换后的量是：\n$$ \\tilde{H} = \\frac{1}{n} \\sum_{i=1}^{n} \\tilde{x}_{i}^2 = \\frac{1}{n} \\sum_{i=1}^{n} (s x_{i})^2 = s^2 \\left(\\frac{1}{n} \\sum_{i=1}^{n} x_{i}^2\\right) = s^2 H $$\n$$ \\tilde{z} = \\frac{1}{n} \\sum_{i=1}^{n} \\tilde{x}_{i} y_i = \\frac{1}{n} \\sum_{i=1}^{n} (s x_{i}) y_i = s \\left(\\frac{1}{n} \\sum_{i=1}^{n} x_i y_i\\right) = s z $$\n现在坐标级更新是针对新系数 $\\tilde{\\beta}$。目标函数使用新的量 $\\tilde{H}$、$\\tilde{z}$ 以及应用于 $|\\tilde{\\beta}|$ 的相同惩罚函数：\n$$ \\tilde{f}(\\tilde{\\beta}) = \\frac{1}{2} \\tilde{H} \\tilde{\\beta}^2 - \\tilde{z} \\tilde{\\beta} + p_{\\lambda, \\gamma}(|\\tilde{\\beta}|) $$\n这个最小化问题的结构与原始问题相同，只是 $H$ 被 $\\tilde{H}$ 替换，$z$ 被 $\\tilde{z}$ 替换。因此，我们可以通过将 $\\tilde{H}$ 代入我们之前推导出的阈值表达式，直接找到新变量 $|\\tilde{z}|$ 的阈值。\n\n$|\\tilde{z}|$ 的下阈值将零解区间（其中 $\\hat{\\tilde{\\beta}}=0$）与非零解区间分开。与原始问题类比，该阈值就是 $\\lambda$。它不依赖于 $H$ 或缩放因子 $s$。\n\n$|\\tilde{z}|$ 的上阈值将收缩解区间与未惩罚解区间分开。类比可知，该阈值为 $\\lambda\\gamma\\tilde{H}$。我们用原始参数和缩放因子 $s$ 来表示它：\n$$ \\text{上阈值} = \\lambda\\gamma\\tilde{H} = \\lambda\\gamma(s^2 H) = s^2 \\lambda \\gamma H $$\n因此，$|\\tilde{z}|$ 的两个阈值水平是 $\\lambda$ 和 $s^2 \\lambda \\gamma H$。通过 $s$ 进行缩放不影响下阈值，但将上阈值缩放了 $s^2$ 倍。\n\n由 $|\\tilde{z}|$ 的下阈值和上阈值组成的有序对是 $(\\lambda, s^2 \\lambda \\gamma H)$。", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\lambda  s^2 \\lambda \\gamma H\n\\end{pmatrix}\n}\n$$", "id": "3153449"}, {"introduction": "在理解了 MCP 的基本工作原理后，我们将其与另一种流行的非凸惩罚——平滑削边绝对离差 (SCAD) 进行直接比较。本编码练习将通过一个具有高度相关预测变量的挑战性场景，来揭示这两种惩罚在变量选择行为上的差异，以及凹度参数（$a$ 或 $\\gamma$）如何调节这种行为。这项实践对于培养在不同情境下选择合适惩罚项的直觉至关重要。[@problem_id:3153524]", "problem": "要求您构建并分析一个统计学习模拟，该模拟比较使用平滑裁剪绝对偏差 (Smoothly Clipped Absolute Deviation, SCAD) 和极小极大凹惩罚 (Minimax Concave Penalty, MCP) 的非凸正则化。具体目标是研究当存在两个高度相关的预测变量时，变量选择行为如何随着正则化参数的变化而变化，并解释凹度参数的作用。您的程序必须生成单行、确定性的输出，并且无需任何外部输入即可运行。\n\n考虑一个具有两个预测变量的线性回归模型。设 $n$ 表示样本数量。通过对标准正态随机变量 $z_1, z_2 \\sim \\mathcal{N}(0, 1)$ 进行采样，并构造 $x_1 = z_1$ 和 $x_2 = \\rho z_1 + \\sqrt{1 - \\rho^2}\\,z_2$，生成相关性为 $\\rho \\approx 0.99$ 的预测变量 $(x_1, x_2)$。设真实的回归系数为 $\\beta_1^{\\star}$ 和 $\\beta_2^{\\star}$，并生成响应 $y = \\beta_1^{\\star} x_1 + \\beta_2^{\\star} x_2 + \\varepsilon$，其中 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2)$ 是独立噪声。将预测变量标准化，使其均值为零，且对于每个预测变量 $j \\in \\{1, 2\\}$，样本内单位尺度由 $(1/n)\\sum_{i=1}^n x_{ij}^2 = 1$ 定义。将响应中心化，使其均值为零。在整个过程中，使用固定的随机种子以保证可复现性。\n\n将惩罚最小二乘估计量定义为以下目标函数在 $\\beta \\in \\mathbb{R}^2$ 上的最小化子：\n$$\n\\frac{1}{2n} \\sum_{i=1}^n \\left(y_i - x_{i1}\\beta_1 - x_{i2}\\beta_2\\right)^2 + \\sum_{j=1}^2 p_{\\lambda}(|\\beta_j|),\n$$\n其中 $p_{\\lambda}(t)$ 是 SCAD 惩罚或 MCP 惩罚，两者都由正则化参数 $\\lambda  0$ 和凹度参数 $a  0$ 参数化（$a$ 的具体约束如下文所述）。对于 $t \\ge 0$ 和 $a  2$，SCAD 惩罚函数 $p_{\\lambda}^{\\text{SCAD}}(t)$ 分段定义如下：\n$$\np_{\\lambda}^{\\text{SCAD}}(t) =\n\\begin{cases}\n\\lambda t,  0 \\le t \\le \\lambda, \\\\\n\\dfrac{-t^2 + 2a\\lambda t - \\lambda^2}{2(a - 1)},  \\lambda  t \\le a\\lambda, \\\\\n\\dfrac{(a + 1)\\lambda^2}{2},  t  a\\lambda,\n\\end{cases}\n$$\n而对于 $t \\ge 0$ 和 $a  1$，MCP 惩罚函数 $p_{\\lambda}^{\\text{MCP}}(t)$ 定义如下：\n$$\np_{\\lambda}^{\\text{MCP}}(t) =\n\\begin{cases}\n\\lambda t - \\dfrac{t^2}{2a},  0 \\le t \\le a\\lambda, \\\\\n\\dfrac{a\\lambda^2}{2},  t  a\\lambda.\n\\end{cases}\n$$\n实现一个使用坐标下降法的优化算法。在每次坐标更新时，利用标准化设计将问题简化为一维最小化，并使用必要的最优性条件，从第一性原理推导由惩罚函数 $p_{\\lambda}(\\cdot)$ 所蕴含的坐标更新法则。不要使用预构建的求解器。确保您的算法在满足数值收敛准则时终止。\n\n您的程序必须运行以下固定的模拟设置：\n- 使用 $n = 200$ 个样本。\n- 使用 $\\rho = 0.99$ 和 $\\sigma = 0.20$。\n- 使用真实系数 $\\beta_1^{\\star} = 1.0$ 和 $\\beta_2^{\\star} = 1.0$。\n\n对每种惩罚和参数设置，按如下方式定义选择决策：计算出惩罚估计量 $\\hat{\\beta}$ 后，通过检查 $|\\hat{\\beta}_1|  \\tau$ 来确定是否选择了预测变量 1，通过检查 $|\\hat{\\beta}_2|  \\tau$ 来确定是否选择了预测变量 2，其中 $\\tau = 10^{-6}$。使用位掩码将选择结果编码为单个整数，其中最低有效位代表预测变量 1，下一个位代表预测变量 2：\n- 如果两个预测变量均未被选中，则输出 $0$。\n- 如果只选中了预测变量 1，则输出 $1$。\n- 如果只选中了预测变量 2，则输出 $2$。\n- 如果两个预测变量都被选中，则输出 $3$。\n\n测试套件：\n在以下六个测试用例上运行算法，这些用例改变了惩罚类型、正则化参数 $\\lambda$ 和凹度参数 $a$：\n1. MCP，其中 $\\lambda = 0.01$ 和 $a = 3.0$（理想情况：非常轻的正则化）\n2. MCP，其中 $\\lambda = 0.25$ 和 $a = 3.0$（中等正则化，典型凹度）\n3. MCP，其中 $\\lambda = 0.25$ 和 $a = 1.5$（中等正则化，更强的凹度）\n4. SCAD，其中 $\\lambda = 0.25$ 和 $a = 3.7$（中等正则化，典型凹度）\n5. SCAD，其中 $\\lambda = 0.25$ 和 $a = 10.0$（中等正则化，较弱的凹度）\n6. SCAD，其中 $\\lambda = 3.00$ 和 $a = 3.7$（强正则化边界）\n\n您的程序应生成单行输出，其中包含所有六个测试用例的结果，以逗号分隔的列表形式包含在方括号内，并按上述顺序列出（例如，$[3,1,3,2,0,0]$）。最终输出必须是此格式的单行文本。\n\n除了生成数值结果外，您还需要在解决方案中从标准化的一维子问题出发推导坐标更新法则，并使用这些法则从第一性原理出发，解释凹度参数 $a$ 如何影响收缩和选择，特别是在存在高度相关预测变量的情况下。此问题不涉及物理单位或角度，也不需要百分比；所有报告的数值必须是如上指定的纯数字。", "solution": "该问题是有效的，因为它在科学上基于统计学习理论，问题设定良好，有足够的数据和明确的目标，并且表述客观。我们将着手提供一个解决方案。\n\n问题的核心是为线性模型 $y = X\\beta + \\varepsilon$ 求解一个惩罚最小二乘问题。需要最小化的目标函数是：\n$$\nL(\\beta) = \\frac{1}{2n} \\|y - X\\beta\\|_2^2 + \\sum_{j=1}^p p_{\\lambda}(|\\beta_j|)\n$$\n其中 $p=2$，$p_{\\lambda}(\\cdot)$ 是平滑裁剪绝对偏差 (SCAD) 或极小极大凹惩罚 (MCP)，并且设计矩阵 $X$ 的列经过标准化，使得对于 $j \\in \\{1, 2\\}$，有 $\\frac{1}{n}\\sum_{i=1}^n x_{ij}^2 = 1$。我们将使用坐标下降法来找到估计量 $\\hat{\\beta}$。\n\n### 坐标下降子问题推导\n\n坐标下降法一次只针对单个系数 $\\beta_k$ 最小化目标函数，同时将所有其他系数 $\\beta_j$ ($j \\neq k$) 固定在它们的当前值。作为 $\\beta_k$ 的函数，目标函数可以写成：\n$$\nf(\\beta_k) = \\frac{1}{2n} \\sum_{i=1}^n \\left( (y_i - \\sum_{j \\neq k} x_{ij}\\beta_j) - x_{ik}\\beta_k \\right)^2 + p_{\\lambda}(|\\beta_k|) + C\n$$\n其中 $C$ 包含不依赖于 $\\beta_k$ 的项。设偏残差为 $r_{i,(-k)} = y_i - \\sum_{j \\neq k} x_{ij}\\beta_j$。展开平方项，我们得到：\n$$\nf(\\beta_k) = \\frac{1}{2n} \\left( \\sum_{i=1}^n r_{i,(-k)}^2 - 2\\beta_k \\sum_{i=1}^n x_{ik}r_{i,(-k)} + \\beta_k^2 \\sum_{i=1}^n x_{ik}^2 \\right) + p_{\\lambda}(|\\beta_k|) + C\n$$\n使用标准化条件 $\\frac{1}{n}\\sum_{i=1}^n x_{ik}^2 = 1$，表达式得以简化。设 $z_k = \\frac{1}{n} \\sum_{i=1}^n x_{ik}r_{i,(-k)}$，这是第 $k$ 个预测变量与偏残差向量的点积，除以 $1/n$。关于 $\\beta_k$ 最小化 $f(\\beta_k)$ 等价于最小化：\n$$\n\\tilde{f}(\\beta_k) = \\frac{1}{2}\\beta_k^2 - z_k\\beta_k + p_{\\lambda}(|\\beta_k|)\n$$\n对二次项进行配方，这等价于最小化：\n$$\ng(\\beta_k) = \\frac{1}{2}(\\beta_k - z_k)^2 + p_{\\lambda}(|\\beta_k|)\n$$\n这是一个一维的惩罚估计问题。解 $\\hat{\\beta}_k$ 是通过对 $z_k$ 应用一个阈值算子得到的。\n\n### 阈值算子的推导\n\n我们使用凸分析中的必要最优性条件来找到 $g(\\beta_k)$ 的最小化子 $\\hat{\\beta}_k$，该条件指出零向量必须在最小值点的次梯度中：$0 \\in \\partial g(\\hat{\\beta}_k)$。次梯度为 $\\partial g(\\beta_k) = \\beta_k - z_k + \\partial p_{\\lambda}(|\\beta_k|)$。因此，最优性条件是 $z_k - \\hat{\\beta}_k \\in \\partial p_{\\lambda}(|\\hat{\\beta}_k|)$。\n\n让我们假设 $z_k  0$，这意味着解 $\\hat{\\beta}_k \\ge 0$。对于 $\\hat{\\beta}_k  0$，条件变为 $z_k - \\hat{\\beta}_k = p'_{\\lambda}(\\hat{\\beta}_k)$，其中 $p'_{\\lambda}(t)$ 是惩罚函数在 $t0$ 时的导数。如果解是 $\\hat{\\beta}_k = 0$，则条件是 $|z_k| \\le \\lambda$，因为在 $t=0$ 处，$\\partial p_{\\lambda}(|t|)$ 的次梯度是 $[-\\lambda, \\lambda]$。\n\n**1. 极小极大凹惩罚 (MCP)**\n对于 $t \\ge 0$ 和 $a  1$，MCP 的导数为 $p'_{\\lambda}(t) = (\\lambda - t/a)_+ = \\max(0, \\lambda - t/a)$。\n- 如果 $|z_k| \\le \\lambda$，解是 $\\hat{\\beta}_k = 0$。\n- 如果 $z_k  \\lambda$，我们寻找一个解 $\\hat{\\beta}_k  0$。\n    - 如果 $0  \\hat{\\beta}_k \\le a\\lambda$：$z_k - \\hat{\\beta}_k = \\lambda - \\hat{\\beta}_k/a \\implies \\hat{\\beta}_k(1 - 1/a) = z_k - \\lambda \\implies \\hat{\\beta}_k = \\frac{a(z_k - \\lambda)}{a-1}$。这在 $0  \\frac{a(z_k - \\lambda)}{a-1} \\le a\\lambda$ 时成立，简化后为 $\\lambda  z_k \\le a\\lambda$。\n    - 如果 $\\hat{\\beta}_k  a\\lambda$：$z_k - \\hat{\\beta}_k = 0 \\implies \\hat{\\beta}_k = z_k$。这在 $z_k  a\\lambda$ 时成立。\n\n综合并推广到任意 $z_k$，MCP 的更新法则是：\n$$\n\\hat{\\beta}_k \\leftarrow \\begin{cases}\n0,  |z_k| \\le \\lambda \\\\\n\\dfrac{a(|z_k| - \\lambda)}{a-1} \\mathrm{sgn}(z_k),  \\lambda  |z_k| \\le a\\lambda \\\\\nz_k,  |z_k|  a\\lambda\n\\end{cases}\n$$\n\n**2. 平滑裁剪绝对偏差 (SCAD) 惩罚**\n对于 $t \\ge 0$ 和 $a  2$，SCAD 的导数为：\n$p'_{\\lambda}(t) = \\lambda \\mathbb{I}(t \\le \\lambda) + \\frac{(a\\lambda - t)_+}{a-1} \\mathbb{I}(t  \\lambda)$。\n- 如果 $|z_k| \\le \\lambda$，解是 $\\hat{\\beta}_k = 0$。\n- 如果 $z_k  \\lambda$，我们寻找一个解 $\\hat{\\beta}_k  0$。\n    - 如果 $0  \\hat{\\beta}_k \\le \\lambda$：$z_k - \\hat{\\beta}_k = \\lambda \\implies \\hat{\\beta}_k = z_k - \\lambda$。这是标准的软阈值解，发现在 $\\lambda  z_k \\le 2\\lambda$ 时有效。\n    - 如果 $\\lambda  \\hat{\\beta}_k \\le a\\lambda$：$z_k - \\hat{\\beta}_k = \\frac{a\\lambda - \\hat{\\beta}_k}{a-1} \\implies \\hat{\\beta}_k(a-2) = z_k(a-1) - a\\lambda \\implies \\hat{\\beta}_k = \\frac{(a-1)z_k - a\\lambda}{a-2}$。这在 $2\\lambda  z_k \\le a\\lambda$ 时成立。\n    - 如果 $\\hat{\\beta}_k  a\\lambda$：$z_k - \\hat{\\beta}_k = 0 \\implies \\hat{\\beta}_k = z_k$。这在 $z_k  a\\lambda$ 时成立。\n\n综合并推广，SCAD 的更新法则是：\n$$\n\\hat{\\beta}_k \\leftarrow \\begin{cases}\n0,  |z_k| \\le \\lambda \\\\\n(|z_k| - \\lambda) \\mathrm{sgn}(z_k),  \\lambda  |z_k| \\le 2\\lambda \\\\\n\\dfrac{(a-1)z_k - a\\lambda\\,\\mathrm{sgn}(z_k)}{a-2},  2\\lambda  |z_k| \\le a\\lambda \\\\\nz_k,  |z_k|  a\\lambda\n\\end{cases}\n$$\n\n### 凹度参数 $a$ 的作用\n\n参数 $a$ 控制惩罚函数的凹度。\n- **极限行为**：当 $a \\to \\infty$ 时，MCP 和 SCAD 惩罚及其对应的阈值算子都收敛于 LASSO（$L_1$ 惩罚）的惩罚和算子。对于 LASSO，更新是软阈值：$\\hat{\\beta}_k = \\mathrm{sgn}(z_k)(|z_k|-\\lambda)_+$。较大的 $a$ 值使得惩罚在更广的系数值范围内表现得像 LASSO。\n- **偏差减小**：对于幅度 $|z_k|  \\lambda$ 的系数，LASSO 总是将它们向零收缩，从而引入偏差。相比之下，对于 MCP 和 SCAD，代表惩罚率的惩罚函数导数会随着系数幅度的增加而减小。对于足够大的系数（$|\\hat{\\beta}_k|  a\\lambda$），惩罚率变为零，估计变得无偏（$\\hat{\\beta}_k = z_k$）。较小的 $a$ 值（更强的凹度，更接近 MCP 的 $a1$ 和 SCAD 的 $a2$ 的下界）导致这种向无偏估计的过渡发生在较小的系数值上。为大系数提供近似无偏估计的这一特性是非凸惩罚的一个关键优势。\n- **相关预测变量的选择**：众所周知，LASSO 在处理一组高度相关的预测变量时不稳定。它倾向于从组中任意选择一个变量，并将其他变量收缩到零。非凸惩罚可以缓解这个问题。在我们的问题中，预测变量 $x_1$ 和 $x_2$ 高度相关（$\\rho=0.99$），并且两个真实系数都是非零的。LASSO（或具有较大 $a$ 值的 SCAD/MCP）将难以同时选择两者。例如，一旦 $\\beta_1$ 进入模型，它解释了与 $x_2$ 共享的大部分方差，降低了偏残差与 $x_2$ 的相关性，使得 $\\beta_2$ 很难超过惩罚阈值 $\\lambda$。\n- 较小的 $a$ 值（更强的凹度）增强了同时选择两个预测变量的能力。对大系数的惩罚会迅速减弱。假设 $\\hat{\\beta}_1$ 变得很大，将其保留在模型中的成本（惩罚项）达到饱和。然后，目标函数主要集中于拟合剩余的残差。这使得模型更容易包含 $\\hat{\\beta}_2$（如果它能解释剩余残差的显著部分），而不会因对 $\\hat{\\beta}_1$ 的持续增加的惩罚而受到“惩罚”。因此，减小 $a$ 应该有利于得到一个两个真实非零系数的高度相关预测变量都被选中的解（输出代码 $3$）。相反，较大的 $a$ 使惩罚更像 LASSO，有利于只选择一个预测变量（输出代码 $1$ 或 $2$）。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Constructs and analyzes a simulation comparing non-convex regularization\n    using SCAD and MCP penalties for a linear model with correlated predictors.\n    \"\"\"\n\n    # --- Simulation and Model Parameters ---\n    n = 200\n    rho = 0.99\n    sigma = 0.20\n    beta_star = np.array([1.0, 1.0])\n    random_seed = 42\n    selection_threshold = 1e-6\n    \n    # --- Test Suite ---\n    test_cases = [\n        {'penalty': 'MCP', 'lambda': 0.01, 'a': 3.0},\n        {'penalty': 'MCP', 'lambda': 0.25, 'a': 3.0},\n        {'penalty': 'MCP', 'lambda': 0.25, 'a': 1.5},\n        {'penalty': 'SCAD', 'lambda': 0.25, 'a': 3.7},\n        {'penalty': 'SCAD', 'lambda': 0.25, 'a': 10.0},\n        {'penalty': 'SCAD', 'lambda': 3.00, 'a': 3.7},\n    ]\n\n    # --- Data Generation ---\n    def generate_data(n, rho, sigma, beta_star, seed):\n        rs = np.random.RandomState(seed)\n        z1 = rs.randn(n)\n        z2 = rs.randn(n)\n        x1 = z1\n        x2 = rho * z1 + np.sqrt(1 - rho**2) * z2\n        X = np.column_stack((x1, x2))\n        \n        noise = rs.randn(n) * sigma\n        y = X @ beta_star + noise\n        \n        # Preprocessing\n        y_centered = y - np.mean(y)\n        X_std = np.zeros_like(X)\n        X_std[:, 0] = (X[:, 0] - np.mean(X[:, 0])) / np.std(X[:, 0])\n        X_std[:, 1] = (X[:, 1] - np.mean(X[:, 1])) / np.std(X[:, 1])\n        \n        return X_std, y_centered\n\n    X, y = generate_data(n, rho, sigma, beta_star, random_seed)\n\n    # --- Thresholding Operators ---\n    def mcp_threshold(z, lambda_val, a_val):\n        abs_z = np.abs(z)\n        if abs_z = lambda_val:\n            return 0.0\n        elif abs_z > a_val * lambda_val:\n            return z\n        else: # lambda_val  abs_z = a_val * lambda_val\n            return np.sign(z) * a_val * (abs_z - lambda_val) / (a_val - 1.0)\n\n    def scad_threshold(z, lambda_val, a_val):\n        abs_z = np.abs(z)\n        if abs_z = lambda_val:\n            return 0.0\n        elif abs_z = 2.0 * lambda_val:\n            return np.sign(z) * (abs_z - lambda_val)\n        elif abs_z = a_val * lambda_val:\n            return ((a_val - 1.0) * z - np.sign(z) * a_val * lambda_val) / (a_val - 2.0)\n        else: # abs_z > a_val * lambda_val\n            return z\n\n    # --- Coordinate Descent Solver ---\n    def coordinate_descent(X, y, penalty, lambda_val, a_val, max_iter=1000, tol=1e-8):\n        n_samples, n_features = X.shape\n        beta = np.zeros(n_features)\n        \n        X_T_y_over_n = X.T @ y / n_samples\n        X_T_X_over_n = X.T @ X / n_samples\n        \n        threshold_op = mcp_threshold if penalty == 'MCP' else scad_threshold\n        \n        for _ in range(max_iter):\n            beta_old = beta.copy()\n            \n            # Update beta_1\n            z_0 = X_T_y_over_n[0] - X_T_X_over_n[0, 1] * beta[1]\n            beta[0] = threshold_op(z_0, lambda_val, a_val)\n            \n            # Update beta_2\n            z_1 = X_T_y_over_n[1] - X_T_X_over_n[1, 0] * beta[0]\n            beta[1] = threshold_op(z_1, lambda_val, a_val)\n            \n            if np.max(np.abs(beta - beta_old))  tol:\n                break\n                \n        return beta\n\n    # --- Main Loop ---\n    results = []\n    for case in test_cases:\n        penalty = case['penalty']\n        lambda_val = case['lambda']\n        a_val = case['a']\n        \n        beta_hat = coordinate_descent(X, y, penalty, lambda_val, a_val)\n        \n        sel1 = 1 if abs(beta_hat[0]) > selection_threshold else 0\n        sel2 = 1 if abs(beta_hat[1]) > selection_threshold else 0\n        \n        outcome = sel1 + 2 * sel2\n        results.append(outcome)\n\n    # --- Final Output ---\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```", "id": "3153524"}, {"introduction": "现在，我们将从孤立的分析转向一个完整的建模工作流程。本练习不仅要求您实现 SCAD 估计器，还将引导您应用一种常见而强大的技术：在所选变量上使用普通最小二乘法 (OLS) 重新拟合模型，以减小收缩偏差。通过量化由此带来的改进，您将亲身体验这种两阶段方法的实际优势，并加深对非凸正则化在实践中如何平衡变量选择和预测准确性的理解。[@problem_id:3153499]", "problem": "在统计学习中，考虑带有高斯噪声的线性模型。令 $n$ 表示观测数量，$p$ 表示特征数量。设计矩阵用 $X \\in \\mathbb{R}^{n \\times p}$ 表示，参数向量用 $\\beta^\\star \\in \\mathbb{R}^p$ 表示，响应用 $y \\in \\mathbb{R}^n$ 表示。数据根据模型 $y = X \\beta^\\star + \\varepsilon$ 生成，其中 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I_n)$，且方差参数 $\\sigma^2$ 已知。惩罚最小二乘的目标是通过最小化由非凸惩罚项增广的经验风险来估计 $\\beta^\\star$。平滑裁剪绝对偏差 (Smoothly Clipped Absolute Deviation, SCAD) 惩罚由其对 $t \\ge 0$ 的导数 $p'_\\lambda(t)$ 定义，其中调整参数 $\\lambda  0$，形状参数 $a  2$：\n$$\np'_\\lambda(t) =\n\\begin{cases}\n\\lambda,  0 \\le t \\le \\lambda, \\\\\n\\dfrac{a \\lambda - t}{a - 1},  \\lambda  t \\le a \\lambda, \\\\\n0,  t  a \\lambda.\n\\end{cases}\n$$\nSCAD 惩罚最小二乘估计量求解以下问题\n$$\n\\hat{\\beta}^{\\text{SCAD}} \\in \\arg\\min_{\\beta \\in \\mathbb{R}^p} \\left\\{ \\dfrac{1}{2n} \\lVert y - X \\beta \\rVert_2^2 + \\sum_{j=1}^p p_\\lambda(|\\beta_j|) \\right\\}.\n$$\n通过 SCAD 进行选择后（即，在计算 $\\hat{\\beta}^{\\text{SCAD}}$ 并识别出 $\\hat{\\beta}^{\\text{SCAD}}_j \\ne 0$ 的支持集索引后），可以对所选支持集使用普通最小二乘法 (Ordinary Least Squares, OLS) 重新拟合模型，以减少收缩偏倚。具体而言，定义所选支持集 $S = \\{ j : \\hat{\\beta}^{\\text{SCAD}}_j \\ne 0 \\}$ 并通过以下方式重新拟合\n$$\n\\hat{\\beta}^{\\text{refit}}_S \\in \\arg\\min_{b \\in \\mathbb{R}^{|S|}} \\left\\{ \\dfrac{1}{2n} \\left\\lVert y - X_S b \\right\\rVert_2^2 \\right\\}, \\quad \\hat{\\beta}^{\\text{refit}}_{S^c} = 0,\n$$\n其中 $X_S$ 是 $X$ 的子矩阵，仅包含由 $S$ 索引的列。\n\n您的任务是编写一个完整、可运行的程序，该程序能够：\n- 从第一性原理（线性模型、平方损失和 SCAD 惩罚定义）出发，使用循环坐标下降法实现 SCAD 惩罚最小二乘。$X$ 的列必须经过中心化和缩放，使得每一列的经验均值为 $0$，平方范数为 $n$，以确保每个坐标的二次子问题具有单位曲率。\n- 在 SCAD 估计量选择的支持集上，通过无惩罚的最小二乘法执行模型重新拟合。\n- 使用以下指标量化估计精度的提高和收缩偏倚的减少：\n    1. 估计量 $\\hat{\\beta}$ 的参数均方误差 (MSE)：\n       $$\n       \\text{MSE}(\\hat{\\beta}) = \\dfrac{1}{p} \\lVert \\hat{\\beta} - \\beta^\\star \\rVert_2^2.\n       $$\n    2. 一个偏倚代理指标，用于衡量在真实支持集 $S^\\star = \\{ j : \\beta^\\star_j \\ne 0 \\}$ 上沿真实信号方向的平均收缩：\n       $$\n       B(\\hat{\\beta}) = \\dfrac{1}{|S^\\star|} \\sum_{j \\in S^\\star} \\operatorname{sign}(\\beta^\\star_j)\\, (\\beta^\\star_j - \\hat{\\beta}_j).\n       $$\n       较大的正值表示较大的收缩偏倚。报告通过重新拟合实现的该代理指标的减少量，定义为\n       $$\n       \\Delta B = B(\\hat{\\beta}^{\\text{SCAD}}) - B(\\hat{\\beta}^{\\text{refit}}).\n       $$\n- 对于每个测试用例，返回一个数对 $[R, \\Delta B]$，其中 $R = \\text{MSE}(\\hat{\\beta}^{\\text{refit}}) / \\text{MSE}(\\hat{\\beta}^{\\text{SCAD}})$。\n\n坐标下降更新必须从优化单个坐标（而所有其他坐标固定）的子问题推导得出，并使用上述 SCAD 导数所蕴含的 Karush–Kuhn–Tucker (KKT) 条件。该算法必须在标准化的设计矩阵上运行，其中每列的平方范数均为 $n$，以便每个坐标的更新求解的是一个具有单位二次项系数的单变量问题。\n\n每个测试用例的数据生成规则：\n- 生成 $X$，其特征可以是独立的标准正态特征，也可以是列间具有自回归相关性的特征。对于相关设计，使用参数为 $\\rho$ 的自回归相关性，即总体协方差 $\\Sigma \\in \\mathbb{R}^{p \\times p}$ 的元素为 $\\Sigma_{ij} = \\rho^{|i-j|}$。然后从 $\\mathcal{N}(0, \\Sigma)$ 中独立抽样各行。\n- 将 $X$ 的列标准化，使其经验均值为 $0$，平方范数为 $n$。\n- 构建恰好有 $k$ 个非零项的 $\\beta^\\star$。从所有特征中均匀随机地选择支持集。对于非零项，从指定的幅度区间内均匀抽样其大小，并分配独立的随机符号 $\\pm 1$。\n- 生成 $y = X \\beta^\\star + \\varepsilon$，其中 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I_n)$，然后将 $y$ 中心化，使其经验均值为 $0$。\n\n边界情况处理：\n- 如果 SCAD 选择的支持集为空，则定义 $\\hat{\\beta}^{\\text{refit}} = 0$（零向量）。在这种情况下，如果 $\\text{MSE}(\\hat{\\beta}^{\\text{SCAD}}) = 0$，则返回 $R = 1$，否则返回计算出的比率。对于偏倚代理指标，按规定计算 $\\Delta B$；如果 $|S^\\star| = 0$（在测试套件中不会发生，因为总是 $k \\ge 1$），则定义 $B(\\hat{\\beta}) = 0$。\n\n测试套件：\n- 案例 1（理想路径，独立设计）：$n = 200$，$p = 60$，$k = 8$，幅度在 $[1.5, 2.5]$ 内均匀分布，$\\sigma = 0.6$，$\\lambda = 0.25$，$a = 3.7$，独立的标准正态 $X$，随机种子 $123$。\n- 案例 2（边界情况：大惩罚项，独立设计）：$n = 200$，$p = 60$，$k = 8$，幅度在 $[1.5, 2.5]$ 内均匀分布，$\\sigma = 0.6$，$\\lambda = 4.0$，$a = 3.7$，独立的标准正态 $X$，随机种子 $123$。\n- 案例 3（相关设计，中等惩罚项）：$n = 250$，$p = 80$，$k = 10$，幅度在 $[1.3, 2.2]$ 内均匀分布，$\\sigma = 0.8$，$\\lambda = 0.30$，$a = 3.7$，自回归相关性 $\\rho = 0.5$，随机种子 $321$。\n- 案例 4（近似无惩罚机制，独立设计）：$n = 300$，$p = 40$，$k = 12$，幅度在 $[1.0, 1.8]$ 内均匀分布，$\\sigma = 1.0$，$\\lambda = 10^{-6}$，$a = 3.7$，独立的标准正态 $X$，随机种子 $42$。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含一个由方括号括起来的、以逗号分隔的数对列表，每个数对对应一个测试用例。每个数对的形式为 $[R, \\Delta B]$，其中 $R$ 和 $\\Delta B$ 表示为浮点数。例如：$[[0.75,0.20],[1.00,0.00]]$。", "solution": "我们从带有高斯噪声的线性模型开始。令 $X \\in \\mathbb{R}^{n \\times p}$、$\\beta^\\star \\in \\mathbb{R}^p$ 和 $y \\in \\mathbb{R}^n$ 满足 $y = X \\beta^\\star + \\varepsilon$，其中 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I_n)$。经验平方误差为 $\\lVert y - X \\beta \\rVert_2^2 / (2n)$。平滑裁剪绝对偏差 (SCAD) 惩罚通过其对 $t \\ge 0$ 的导数 $p'_\\lambda(t)$ 定义，其中调整参数 $\\lambda  0$，形状参数 $a  2$：\n$$\np'_\\lambda(t) =\n\\begin{cases}\n\\lambda,  0 \\le t \\le \\lambda, \\\\\n\\dfrac{a \\lambda - t}{a - 1},  \\lambda  t \\le a \\lambda, \\\\\n0,  t  a \\lambda.\n\\end{cases}\n$$\n要最小化的惩罚准则为\n$$\nQ(\\beta) = \\dfrac{1}{2n} \\lVert y - X \\beta \\rVert_2^2 + \\sum_{j=1}^p p_\\lambda(|\\beta_j|).\n$$\n\n基于原理的坐标下降更新推导，通过求解每个坐标 $j$ 的一维优化子问题（同时保持其他坐标固定）来进行。为使子问题具有单位曲率，需将设计矩阵标准化，使得 $X$ 的每一列经验均值为 $0$，平方范数等于 $n$，即对所有 $j$ 都有 $\\lVert X_{\\cdot j} \\rVert_2^2 = n$。在此标准化下，对于当前估计值 $\\beta$，定义偏残差 $r = y - X \\beta$。坐标 $j$ 的一维子问题是最小化\n$$\nq_j(\\theta) = \\dfrac{1}{2n} \\left\\lVert r + X_{\\cdot j} \\beta_j - X_{\\cdot j} \\theta \\right\\rVert_2^2 + p_\\lambda(|\\theta|).\n$$\n展开二次项并使用 $\\lVert X_{\\cdot j} \\rVert_2^2 = n$ 可得\n$$\nq_j(\\theta) = \\dfrac{1}{2} (\\theta - z_j)^2 + p_\\lambda(|\\theta|) + \\text{constant}, \\quad z_j = \\beta_j + \\dfrac{1}{n} X_{\\cdot j}^\\top r.\n$$\n因此，每个坐标的更新简化为单变量问题\n$$\n\\min_{\\theta \\in \\mathbb{R}} \\left\\{ \\dfrac{1}{2} (\\theta - z)^2 + p_\\lambda(|\\theta|) \\right\\}\n$$\n其中 $z = z_j$。对于 $\\theta \\ne 0$ 的 Karush–Kuhn–Tucker (KKT) 最优性条件是\n$$\n0 = \\theta - z + p'_\\lambda(|\\theta|)\\, \\operatorname{sign}(\\theta).\n$$\n使用 $p'_\\lambda(\\cdot)$ 的分段形式，我们求解三种情况：\n- 情况 1 ($|\\theta| \\le \\lambda$)：条件变为 $\\theta - z + \\lambda\\, \\operatorname{sign}(\\theta) = 0$，其解为软阈值 $\\theta = \\operatorname{sign}(z)\\, \\max(|z| - \\lambda, 0)$。可行性要求 $|\\theta| \\le \\lambda$，这等价于 $|z| \\le 2 \\lambda$；当 $|z| \\le \\lambda$ 时，解为 $\\theta = 0$。\n- 情况 2 ($\\lambda  |\\theta| \\le a \\lambda$)：条件变为 $\\theta - z + \\dfrac{a \\lambda - |\\theta|}{a - 1}\\, \\operatorname{sign}(\\theta) = 0$。对于符号相同的 $z$ 和 $\\theta$，求解可得\n$$\n\\theta = \\dfrac{(a - 1) z - a \\lambda\\, \\operatorname{sign}(z)}{a - 2},\n$$\n该解在 $2 \\lambda  |z| \\le a \\lambda$ 时有效，并产生位于 $(\\lambda, a \\lambda]$ 区间内的 $|\\theta|$。\n- 情况 3 ($|\\theta|  a \\lambda$)：此时 $p'_\\lambda(|\\theta|) = 0$，条件简化为 $\\theta - z = 0$，得到 $\\theta = z$，在 $|z|  a \\lambda$ 时有效。\n\n这几种情况定义了在循环坐标下降中使用的 SCAD 阈值算子。\n\n算法设计：\n- 通过将每列中心化至经验均值 $0$ 并进行缩放，使每列的平方范数等于 $n$，来标准化 $X$。将 $y$ 中心化至经验均值 $0$。\n- 将 $\\hat{\\beta}^{\\text{SCAD}}$ 初始化为零向量，并设 $r = y$。\n- 循环迭代坐标 $j = 1, \\dots, p$。对每个 $j$，计算 $z_j = \\hat{\\beta}^{\\text{SCAD}}_j + (X_{\\cdot j}^\\top r)/n$，使用上面推导的 SCAD 阈值算子更新 $\\hat{\\beta}^{\\text{SCAD}}_j$，并更新残差 $r \\leftarrow r - X_{\\cdot j} (\\hat{\\beta}^{\\text{SCAD}}_j - \\text{旧值})$。当一次迭代中所有坐标的最大绝对变化量低于容差或达到最大迭代次数时停止。\n- 选择支持集 $S = \\{ j : \\hat{\\beta}^{\\text{SCAD}}_j \\ne 0 \\}$（在数值上，将小幅值阈值化为零）。\n- 在 $X_S$ 上通过普通最小二乘法 (OLS) 重新拟合以获得 $\\hat{\\beta}^{\\text{refit}}_S$，并设置 $\\hat{\\beta}^{\\text{refit}}_{S^c} = 0$。使用一个最小二乘求解器，在必要时通过伪逆来处理可能存在的病态子问题。\n\n指标与预期效果：\n- 对 $\\hat{\\beta}^{\\text{SCAD}}$ 和 $\\hat{\\beta}^{\\text{refit}}$ 计算参数均方误差 $\\text{MSE}(\\hat{\\beta}) = \\lVert \\hat{\\beta} - \\beta^\\star \\rVert_2^2 / p$，并报告比率 $R = \\text{MSE}(\\hat{\\beta}^{\\text{refit}}) / \\text{MSE}(\\hat{\\beta}^{\\text{SCAD}})$。$R  1$ 的值表示重新拟合带来了改进。\n- 在真实支持集 $S^\\star$ 上计算偏倚代理指标：$B(\\hat{\\beta}) = \\frac{1}{|S^\\star|} \\sum_{j \\in S^\\star} \\operatorname{sign}(\\beta^\\star_j) (\\beta^\\star_j - \\hat{\\beta}_j)$。这个量度量了沿每个真实信号符号方向的平均收缩；SCAD 倾向于收缩中等信号（情况 1 和 2），而重新拟合则消除了所选支持集上的收缩。报告 $\\Delta B = B(\\hat{\\beta}^{\\text{SCAD}}) - B(\\hat{\\beta}^{\\text{refit}})$，当重新拟合减少了收缩偏倚时，该值通常为正。\n\n测试套件覆盖范围：\n- 案例 1 考察了一个中等信号、独立设计的情况，其中重新拟合应能同时减少偏倚和均方误差。\n- 案例 2 采用一个非常大的 $\\lambda$，导致支持集为空或接近为空；因此重新拟合不会带来改善 ($R \\approx 1$, $\\Delta B \\approx 0$）。\n- 案例 3 引入了自回归相关性 ($\\rho = 0.5$)，这使得选择变得复杂；重新拟合仍然减少了所选支持集上的收缩，尽管选择错误可能会限制均方误差的增益。\n- 案例 4 使用接近 $0$ 的 $\\lambda$，使得 SCAD 接近 OLS；重新拟合提供的额外改进极小 ($R \\approx 1$)，偏倚减少也微不足道。\n\n程序应实现所有描述的步骤，根据指定案例生成数据，为每个案例计算 $R$ 和 $\\Delta B$，并以规定的确切格式打印包含这些数对列表的单行输出。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef standardize_design(X):\n    \"\"\"\n    Center each column of X and scale so that each column has squared norm equal to n.\n    Returns standardized X and column means and scales (not used further).\n    \"\"\"\n    n = X.shape[0]\n    col_means = X.mean(axis=0, keepdims=True)\n    Xc = X - col_means\n    # Scale so that ||X[:, j]||^2 == n\n    col_norms = np.linalg.norm(Xc, axis=0)\n    # Prevent division by zero for any zero columns\n    safe_norms = np.where(col_norms == 0.0, 1.0, col_norms)\n    scales = safe_norms / np.sqrt(n)\n    Xs = Xc / scales\n    return Xs, col_means, scales\n\ndef scad_threshold(z, lam, a):\n    \"\"\"\n    SCAD thresholding operator for the univariate problem:\n    minimize 0.5 * (theta - z)^2 + p_lambda(|theta|), under standardization.\n    \"\"\"\n    az = abs(z)\n    if az = lam:\n        return 0.0\n    elif az = 2.0 * lam:\n        return np.sign(z) * (az - lam)\n    elif az = a * lam:\n        return ((a - 1.0) * z - np.sign(z) * a * lam) / (a - 2.0)\n    else:\n        return z\n\ndef scad_coordinate_descent(X, y, lam, a, max_iters=1000, tol=1e-6):\n    \"\"\"\n    Cyclic coordinate descent for SCAD-penalized least squares with standardized X (columns centered, ||col||^2 = n).\n    Minimizes (1/(2n)) ||y - Xb||^2 + sum_j p_lambda(|b_j|).\n    \"\"\"\n    n, p = X.shape\n    beta = np.zeros(p)\n    r = y.copy()  # residual y - X @ beta, initially y since beta=0\n    for it in range(max_iters):\n        max_delta = 0.0\n        for j in range(p):\n            xj = X[:, j]\n            old_bj = beta[j]\n            # z_j = beta_j + (x_j^T r)/n\n            # r current is y - X @ beta, so xj.T @ r is xj.T @ (y - X @ beta)\n            # which is xj.T @ (y - X_not_j @ beta_not_j - xj @ beta_j)\n            # z_j should be xj.T @ (y - X_not_j @ beta_not_j) / n\n            # z_j = (xj.T @ r) / n + (xj.T @ xj) / n * beta_j = (xj.T @ r) / n + beta_j because of standardization\n            z = old_bj + (xj @ r) / n\n            new_bj = scad_threshold(z, lam, a)\n            delta = new_bj - old_bj\n            if delta != 0.0:\n                beta[j] = new_bj\n                r -= xj * delta  # update residual to y - X @ beta\n                adelta = abs(delta)\n                if adelta > max_delta:\n                    max_delta = adelta\n        if max_delta  tol:\n            break\n    return beta\n\ndef refit_ols_on_support(X, y, beta_hat, eps=1e-8):\n    \"\"\"\n    Refit unpenalized least squares on the support where |beta_hat| > eps.\n    Returns a full-length coefficient vector with zeros outside the selected support.\n    \"\"\"\n    support = np.where(np.abs(beta_hat) > eps)[0]\n    p = X.shape[1]\n    beta_refit = np.zeros(p)\n    if support.size == 0:\n        return beta_refit\n    XS = X[:, support]\n    # Least squares solution using pseudoinverse via lstsq\n    bS, *_ = np.linalg.lstsq(XS, y, rcond=None)\n    beta_refit[support] = bS\n    return beta_refit\n\ndef parameter_mse(beta_hat, beta_true):\n    \"\"\"\n    Mean squared error in parameter space: (1/p) * ||beta_hat - beta_true||^2\n    \"\"\"\n    p = beta_true.shape[0]\n    return float(np.sum((beta_hat - beta_true) ** 2) / p)\n\ndef bias_proxy(beta_hat, beta_true, eps=1e-12):\n    \"\"\"\n    Bias proxy on true support: average signed shrinkage along true signal direction.\n    B = (1/|S*|) sum_{j in S*} sign(beta_true_j) * (beta_true_j - beta_hat_j)\n    \"\"\"\n    support_true = np.where(np.abs(beta_true) > eps)[0]\n    if support_true.size == 0:\n        return 0.0\n    signs = np.sign(beta_true[support_true])\n    diffs = beta_true[support_true] - beta_hat[support_true]\n    return float(np.mean(signs * diffs))\n\ndef generate_design(n, p, design_type, rho, rng):\n    \"\"\"\n    Generate design matrix X (n x p).\n    design_type: 'independent' or 'correlated' (AR(1) with parameter rho).\n    \"\"\"\n    if design_type == \"independent\":\n        X = rng.standard_normal((n, p))\n    elif design_type == \"correlated\":\n        # AR(1) covariance Sigma_{ij} = rho^{|i-j|}\n        idx = np.arange(p)\n        # Toeplitz construction: Sigma_{ij} = rho^{|i-j|}\n        Sigma = rho ** np.abs(idx[:, None] - idx[None, :])\n        # Cholesky factor (ensure PD with tiny jitter if needed)\n        # For numerical stability in edge cases:\n        jitter = 1e-12\n        try:\n            L = np.linalg.cholesky(Sigma)\n        except np.linalg.LinAlgError:\n            L = np.linalg.cholesky(Sigma + jitter * np.eye(p))\n        Z = rng.standard_normal((n, p))\n        X = Z @ L.T\n    else:\n        raise ValueError(\"Unknown design_type\")\n    return X\n\ndef generate_beta_true(p, k, amp_range, rng):\n    \"\"\"\n    Generate true beta with exactly k nonzeros.\n    Nonzero magnitudes uniform in amp_range, signs random.\n    \"\"\"\n    beta_true = np.zeros(p)\n    support = rng.choice(p, size=k, replace=False)\n    mags = rng.uniform(amp_range[0], amp_range[1], size=k)\n    signs = rng.choice([-1.0, 1.0], size=k)\n    beta_true[support] = mags * signs\n    return beta_true\n\ndef run_case(n, p, k, amp_range, sigma, lam, a, design_type, rho, seed):\n    rng = np.random.default_rng(seed)\n    X_raw = generate_design(n, p, design_type, rho, rng)\n    X, _, _ = standardize_design(X_raw)\n    beta_true = generate_beta_true(p, k, amp_range, rng)\n    noise = rng.normal(0.0, sigma, size=n)\n    y = X @ beta_true + noise\n    # Center y to mean zero\n    y = y - np.mean(y)\n\n    # SCAD estimator\n    beta_scad = scad_coordinate_descent(X, y, lam=lam, a=a, max_iters=1000, tol=1e-6)\n    # Refit OLS on SCAD support\n    beta_refit = refit_ols_on_support(X, y, beta_scad, eps=1e-8)\n\n    # Metrics\n    mse_scad = parameter_mse(beta_scad, beta_true)\n    mse_refit = parameter_mse(beta_refit, beta_true)\n    R = 1.0 if mse_scad == 0.0 else float(mse_refit / mse_scad)\n\n    B_scad = bias_proxy(beta_scad, beta_true, eps=1e-12)\n    B_refit = bias_proxy(beta_refit, beta_true, eps=1e-12)\n    delta_B = float(B_scad - B_refit)\n    return [R, delta_B]\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1: happy path, independent design\n        dict(n=200, p=60, k=8, amp_range=(1.5, 2.5), sigma=0.6, lam=0.25, a=3.7,\n             design_type=\"independent\", rho=0.0, seed=123),\n        # Case 2: boundary (large lambda), independent design\n        dict(n=200, p=60, k=8, amp_range=(1.5, 2.5), sigma=0.6, lam=4.0, a=3.7,\n             design_type=\"independent\", rho=0.0, seed=123),\n        # Case 3: correlated design, moderate penalty\n        dict(n=250, p=80, k=10, amp_range=(1.3, 2.2), sigma=0.8, lam=0.30, a=3.7,\n             design_type=\"correlated\", rho=0.5, seed=321),\n        # Case 4: near-unpenalized regime, independent design\n        dict(n=300, p=40, k=12, amp_range=(1.0, 1.8), sigma=1.0, lam=1e-6, a=3.7,\n             design_type=\"independent\", rho=0.0, seed=42),\n    ]\n\n    results = []\n    for case in test_cases:\n        result = run_case(\n            n=case[\"n\"], p=case[\"p\"], k=case[\"k\"],\n            amp_range=case[\"amp_range\"], sigma=case[\"sigma\"],\n            lam=case[\"lam\"], a=case[\"a\"],\n            design_type=case[\"design_type\"], rho=case[\"rho\"], seed=case[\"seed\"]\n        )\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    # Produce a single line: list of pairs [R, DeltaB] for each test case.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3153499"}]}