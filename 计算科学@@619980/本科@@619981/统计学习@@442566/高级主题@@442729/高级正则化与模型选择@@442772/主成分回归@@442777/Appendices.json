{"hands_on_practices": [{"introduction": "在主成分回归中，选择合适的成分数量是关键一步，但标准的交叉验证方法计算量巨大。本练习将引导你推导一个高效计算留一法交叉验证误差（即预测残差平方和，PRESS）的公式 [@problem_id:1951651]。通过理解“帽子矩阵”在其中的作用，你将掌握一种适用于众多线性模型的通用优化技巧，从而极大地提高模型评估的效率。", "problem": "一个材料工程团队正在研究一种新型高熵合金的性能。他们的目标是基于一组 $p$ 个定量预测变量，为该合金的断裂韧性（用 $y$ 表示）建立一个预测模型。这些预测变量由向量 $\\mathbf{x} \\in \\mathbb{R}^p$ 表示，其中包括组成元素的浓度以及各种热机械加工参数。\n\n该团队收集了一个包含 $n$ 个合金样本的数据集。设 $n \\times p$ 矩阵 $\\mathbf{X}$ 包含所有样本的预测变量值，而 $n \\times 1$ 向量 $\\mathbf{y}$ 包含相应的断裂韧性测量值。为简便起见，假设矩阵 $\\mathbf{X}$ 的各列和向量 $\\mathbf{y}$ 都已经过中心化处理，均值为零。\n\n由于合金设计的性质，许多预测变量预计会高度相关。为了解决这种多重共线性问题，该团队决定使用主成分回归（PCR），这是一个两阶段的过程。首先，对预测变量矩阵 $\\mathbf{X}$ 进行主成分分析（PCA），以获得一组称为主成分的新的正交变量。其次，将响应 $\\mathbf{y}$ 对这些主成分的一个子集进行回归。\n\n使用前 $k$ 个主成分的PCR模型定义如下：\n1. 主成分得分计算为 $\\mathbf{Z}_k = \\mathbf{X}\\mathbf{V}_k$，其中 $\\mathbf{V}_k$ 是一个 $p \\times k$ 矩阵，其列是前 $k$ 个主成分载荷向量（即 $\\mathbf{X}$ 的样本协方差矩阵的特征向量）。\n2. 对 $\\mathbf{y}$ 和 $\\mathbf{Z}_k$ 进行线性回归，以获得拟合值 $\\hat{\\mathbf{y}}^{(k)}$。\n\nPCR中的一个关键步骤是选择最优的主成分数量 $k$。一种常见的方法是选择能使通过交叉验证估计的预测误差最小化的 $k$。该团队计划使用留一法交叉验证（LOOCV）。性能度量指标是预测残差平方和（PRESS），定义为 $\\text{PRESS}(k) = \\sum_{i=1}^{n} (y_i - \\hat{y}_{(-i)}^{(k)})^2$，其中 $\\hat{y}_{(-i)}^{(k)}$ 是一个包含 $k$ 个主成分的PCR模型对第 $i$ 个观测值的预测，而该模型是使用除第 $i$ 个观测值之外的所有数据训练得到的。\n\n尽管这个过程定义明确，但将PCR模型重新拟合 $n$ 次（对每个被留出的观测值拟合一次）的计算效率很低。你的任务是推导出一个高效的 $\\text{PRESS}(k)$ 解析表达式，以避免这种重复拟合。\n\n将 $\\text{PRESS}(k)$ 统计量表示成一种形式，这种形式仅需对包含 $n$ 个观测值的完整数据集进行一次PCR模型拟合即可计算。你的最终表达式应包含与PCR模型相关的普通残差 $e_i^{(k)} = y_i - \\hat{y}_i^{(k)}$ 和相应投影矩阵（或“帽子矩阵”）的对角元素 $h_{ii}^{(k)}$。", "solution": "目标是为具有 $k$ 个主成分的主成分回归（PCR）模型，找到一个计算高效的预测残差平方和（PRESS）统计量的公式。\n\n首先，我们回顾一个对于任何形式为 $\\hat{\\mathbf{y}} = \\mathbf{H}\\mathbf{y}$ 的线性模型都适用的普遍结果，其中 $\\mathbf{H}$ 是投影矩阵或“帽子矩阵”。在不包含第 $i$ 个观测值的情况下拟合模型所得到的预测值 $\\hat{y}_{(-i)}$，可以与在所有数据上拟合模型得到的结果关联起来。第 $i$ 个观测值的留一法交叉验证（LOOCV）残差由以下著名公式给出：\n$$ y_i - \\hat{y}_{(-i)} = \\frac{y_i - \\hat{y}_i}{1 - h_{ii}} $$\n其中 $\\hat{y}_i$ 是完整模型对第 $i$ 个观测值的拟合值，$e_i = y_i - \\hat{y}_i$ 是普通残差，而 $h_{ii}$ 是帽子矩阵 $\\mathbf{H}$ 的第 $i$ 个对角元素。\n\nPRESS统计量是这些LOOCV残差的平方和：\n$$ \\text{PRESS} = \\sum_{i=1}^{n} (y_i - \\hat{y}_{(-i)})^2 = \\sum_{i=1}^{n} \\left( \\frac{y_i - \\hat{y}_i}{1 - h_{ii}} \\right)^2 = \\sum_{i=1}^{n} \\left( \\frac{e_i}{1-h_{ii}} \\right)^2 $$\n为了将此应用于我们的PCR问题，我们需要确定具有 $k$ 个主成分的PCR模型的特定帽子矩阵 $\\mathbf{H}^{(k)}$，并找到其对角元素 $h_{ii}^{(k)}$。\n\n在PCR中，模型是使用主成分得分 $\\mathbf{Z}_k = \\mathbf{X}\\mathbf{V}_k$ 作为预测变量来构建的。将中心化的响应 $\\mathbf{y}$ 对 $\\mathbf{Z}_k$ 进行线性回归，得到估计的系数：\n$$ \\hat{\\boldsymbol{\\theta}}_k = (\\mathbf{Z}_k^T \\mathbf{Z}_k)^{-1} \\mathbf{Z}_k^T \\mathbf{y} $$\n拟合值 $\\hat{\\mathbf{y}}^{(k)}$ 接着由下式给出：\n$$ \\hat{\\mathbf{y}}^{(k)} = \\mathbf{Z}_k \\hat{\\boldsymbol{\\theta}}_k = \\mathbf{Z}_k (\\mathbf{Z}_k^T \\mathbf{Z}_k)^{-1} \\mathbf{Z}_k^T \\mathbf{y} $$\n将此与一般形式 $\\hat{\\mathbf{y}}^{(k)} = \\mathbf{H}^{(k)} \\mathbf{y}$ 进行比较，我们可以确定具有 $k$ 个主成分的PCR的帽子矩阵为：\n$$ \\mathbf{H}^{(k)} = \\mathbf{Z}_k (\\mathbf{Z}_k^T \\mathbf{Z}_k)^{-1} \\mathbf{Z}_k^T $$\n得分矩阵 $\\mathbf{Z}_k$ 的列是得分向量 $\\mathbf{z}_1, \\mathbf{z}_2, \\dots, \\mathbf{z}_k$。主成分的一个关键性质是这些得分向量相互正交：对于 $j \\neq m$，有 $\\mathbf{z}_j^T \\mathbf{z}_m = 0$。这意味着矩阵 $\\mathbf{Z}_k^T \\mathbf{Z}_k$ 是一个 $k \\times k$ 的对角矩阵：\n$$ \\mathbf{Z}_k^T \\mathbf{Z}_k = \\text{diag}(\\mathbf{z}_1^T\\mathbf{z}_1, \\mathbf{z}_2^T\\mathbf{z}_2, \\dots, \\mathbf{z}_k^T\\mathbf{z}_k) = \\text{diag}(\\|\\mathbf{z}_1\\|^2, \\|\\mathbf{z}_2\\|^2, \\dots, \\|\\mathbf{z}_k\\|^2) $$\n其中 $\\|\\mathbf{z}_j\\|^2 = \\sum_{i=1}^{n} z_{ij}^2$ 是第 $j$ 个得分向量的欧几里得范数的平方。\n\n其逆矩阵也是对角的：\n$$ (\\mathbf{Z}_k^T \\mathbf{Z}_k)^{-1} = \\text{diag}(\\|\\mathbf{z}_1\\|^{-2}, \\|\\mathbf{z}_2\\|^{-2}, \\dots, \\|\\mathbf{z}_k\\|^{-2}) $$\n将此代回 $\\mathbf{H}^{(k)}$ 的表达式，我们可以看到帽子矩阵是一系列秩为一的投影矩阵之和：\n$$ \\mathbf{H}^{(k)} = \\sum_{j=1}^{k} \\frac{\\mathbf{z}_j \\mathbf{z}_j^T}{\\|\\mathbf{z}_j\\|^2} $$\n我们关心的是这个矩阵的对角元素 $h_{ii}^{(k)}$。外积矩阵 $\\mathbf{z}_j \\mathbf{z}_j^T$ 的第 $(i,i)$ 个元素就是 $z_{ij}^2$，其中 $z_{ij}$ 是向量 $\\mathbf{z}_j$ 的第 $i$ 个元素。因此，$\\mathbf{H}^{(k)}$ 的对角元素为：\n$$ h_{ii}^{(k)} = \\left( \\sum_{j=1}^{k} \\frac{\\mathbf{z}_j \\mathbf{z}_j^T}{\\|\\mathbf{z}_j\\|^2} \\right)_{ii} = \\sum_{j=1}^{k} \\frac{(\\mathbf{z}_j \\mathbf{z}_j^T)_{ii}}{\\|\\mathbf{z}_j\\|^2} = \\sum_{j=1}^{k} \\frac{z_{ij}^2}{\\sum_{l=1}^{n} z_{lj}^2} $$\n现在我们已经具备了所有要素。对于一个有 $k$ 个主成分的PCR模型，其PRESS统计量可以通过首先将模型拟合到完整数据集以获得普通残差 $e_i^{(k)} = y_i - \\hat{y}_i^{(k)}$ 和主成分得分 $z_{ij}$ 来计算。然后，使用推导出的公式将这些值组合起来。\n\n$\\text{PRESS}(k)$ 的最终表达式为：\n$$ \\text{PRESS}(k) = \\sum_{i=1}^{n} \\left( \\frac{y_i - \\hat{y}_i^{(k)}}{1 - h_{ii}^{(k)}} \\right)^2 = \\sum_{i=1}^{n} \\left( \\frac{e_i^{(k)}}{1-h_{ii}^{(k)}} \\right)^2 $$\n这个公式允许在对完整数据集仅执行一次PCA和回归之后，便可高效地计算任何 $k$ 值的PRESS统计量。", "answer": "$$\\boxed{\\sum_{i=1}^{n} \\left( \\frac{e_i^{(k)}}{1-h_{ii}^{(k)}} \\right)^{2}}$$", "id": "1951651"}, {"introduction": "除了交叉验证，我们还可以利用一些理论准则来指导模型选择。本练习将介绍经典的 Mallows' $C_p$ 统计量，它通过对模型复杂度施加惩罚来提供对预测误差的无偏估计 [@problem_id:3143703]。通过推导并编程实现该准则，你将更深刻地理解统计学习中核心的偏差-方差权衡问题。", "problem": "考虑线性模型 $y \\in \\mathbb{R}^n$，其中 $y = f + \\varepsilon$，$\\varepsilon \\sim \\mathcal{N}(0,\\sigma^2 I_n)$ 是一个均值为零的高斯噪声项，其方差 $\\sigma^2  0$ 已知。统计学习中一类常见的估计量是线性估计量，形式为 $\\hat{y} = S y$，其中 $S \\in \\mathbb{R}^{n \\times n}$ 是一个仅依赖于设计矩阵 $X \\in \\mathbb{R}^{n \\times m}$ 的确定性矩阵。在主成分回归 (PCR) 中，设设计矩阵的奇异值分解 (SVD) 为 $X = U D V^\\top$，其中 $U \\in \\mathbb{R}^{n \\times r}$ 具有标准正交列，$D \\in \\mathbb{R}^{r \\times r}$ 是一个对角矩阵，其对角线元素非负，$V \\in \\mathbb{R}^{m \\times r}$ 具有标准正交列，且 $r = \\min(n,m)$。对于一个固定的整数 $k \\in \\{0,1,\\dots,r\\}$，使用前 $k$ 个主成分的 PCR 预测器定义为 $\\hat{y}^{(k)} = U_k U_k^\\top y$，其中 $U_k \\in \\mathbb{R}^{n \\times k}$ 包含 $U$ 的前 $k$ 列（按 $X$ 的奇异值降序排列）。该估计量的自由度为 $\\mathrm{df}(k) = k$，因为 $\\mathrm{trace}(U_k U_k^\\top) = k$。\n\n您的任务是：\n\n- 从线性模型和线性估计量的定义出发，推导期望平方预测误差 $\\mathbb{E}\\!\\left[\\lVert f - \\hat{y}\\rVert_2^2\\right]$ 的一个无偏估计量，该估计量需用可观测的量来表示，并对任意固定矩阵 $S$ 均有效。仅使用线性高斯模型的核心事实、二次型期望的性质以及上述定义。将您的结果特例化到 PCR 的情况 $\\hat{y}^{(k)} = U_k U_k^\\top y$，并将所得准则完全用 $y$、$U_k$、$\\sigma^2$ 和 $n$ 来表示。然后，解释为什么在选择主成分数量时，最小化此准则是合理的。\n\n- 实现一个程序，对下面的每个测试用例，为每个整数 $k \\in \\{0,1,\\dots,r\\}$ 计算 $\\hat{y}^{(k)} = U_k U_k^\\top y$，构建您推导出的特例化准则，并返回使其最小化的整数 $k$。如果出现平局，则返回最小的那个 $k$。使用 $X$ 的 SVD 来获得 $U$，并使其列按奇异值降序排列。依赖标准的线性代数例程来确保数值稳定性。您的最终输出必须是包含三个选定整数（每个测试用例一个）的单行，形式为逗号分隔的列表并用方括号括起来。\n\n测试套件规范：\n\n- 测试用例 1 (一般情况)：\n  - 维度：$n = 8$，$m = 5$。\n  - 设计矩阵 $X_1 \\in \\mathbb{R}^{8 \\times 5}$ 按行给出：\n    - $[\\,2,\\,-1,\\,0,\\,3,\\,1\\,]$,\n    - $[\\,1,\\,0,\\,4,\\,-2,\\,2\\,]$,\n    - $[\\,3,\\,1,\\,-1,\\,0,\\,-1\\,]$,\n    - $[\\,0,\\,2,\\,1,\\,1,\\,0\\,]$,\n    - $[\\,1,\\,-2,\\,3,\\,0,\\,2\\,]$,\n    - $[\\, -1,\\,1,\\,0,\\,-1,\\,3\\,]$,\n    - $[\\,2,\\,0,\\,-2,\\,2,\\,-1\\,]$,\n    - $[\\,0,\\,3,\\,-1,\\,1,\\,1\\,]$。\n  - 响应向量 $y_1 \\in \\mathbb{R}^8$：\n    - $[\\,4.0,\\,-1.0,\\,3.0,\\,0.0,\\,2.0,\\,-2.0,\\,1.0,\\,0.5\\,]$。\n  - 噪声方差：$\\sigma^2 = 0.5$。\n\n- 测试用例 2 (边缘情况，响应与 $X_2$ 的列空间正交)：\n  - 维度：$n = 6$，$m = 4$。\n  - 设计矩阵 $X_2 \\in \\mathbb{R}^{6 \\times 4}$ 按行给出：\n    - $[\\,1,\\,2,\\,0,\\,-1\\,]$,\n    - $[\\,0,\\,-1,\\,3,\\,2\\,]$,\n    - $[\\,2,\\,0,\\,1,\\,-2\\,]$,\n    - $[\\, -1,\\,1,\\,0,\\,1\\,]$,\n    - $[\\,3,\\,-2,\\,1,\\,0\\,]$,\n    - $[\\,0,\\,1,\\,-1,\\,2\\,]$。\n  - 从 $X_2$ 的 SVD 构建 $U_2$。令 $z \\in \\mathbb{R}^6$ 为 $[\\,1.0,\\,-2.0,\\,0.5,\\,3.0,\\,-1.0,\\,0.0\\,]$。定义 $y_2 = z - U_2 U_2^\\top z$，这确保了 $y_2$ 位于 $X_2$ 列空间的正交补空间中。\n  - 噪声方差：$\\sigma^2 = 0.2$。\n\n- 测试用例 3 (大噪声惩罚下的边界行为)：\n  - 维度：$n = 10$，$m = 7$。\n  - 设计矩阵 $X_3 \\in \\mathbb{R}^{10 \\times 7}$ 按行给出：\n    - $[\\,1,\\,0,\\,1,\\,-1,\\,2,\\,0,\\,3\\,]$,\n    - $[\\,0,\\,1,\\,2,\\,0,\\,-1,\\,1,\\,0\\,]$,\n    - $[\\,1,\\,-1,\\,0,\\,2,\\,0,\\,1,\\,-2\\,]$,\n    - $[\\,2,\\,0,\\,-1,\\,1,\\,1,\\,0,\\,0\\,]$,\n    - $[\\,0,\\,2,\\,1,\\,-2,\\,0,\\,1,\\,1\\,]$,\n    - $[\\, -1,\\,1,\\,0,\\,1,\\,2,\\,-1,\\,0\\,]$,\n    - $[\\,0,\\,0,\\,1,\\,0,\\,1,\\,2,\\,-1\\,]$,\n    - $[\\,1,\\,2,\\,0,\\,-1,\\,0,\\,1,\\,1\\,]$,\n    - $[\\,2,\\,-1,\\,1,\\,0,\\,0,\\,0,\\,2\\,]$,\n    - $[\\,0,\\,1,\\,-2,\\,1,\\,1,\\,0,\\,0\\,]$。\n  - 响应向量 $y_3 \\in \\mathbb{R}^{10}$：\n    - $[\\,0.2,\\,-0.1,\\,0.5,\\,0.0,\\,-0.3,\\,0.1,\\,0.0,\\,0.2,\\,-0.2,\\,0.3\\,]$。\n  - 噪声方差：$\\sigma^2 = 10.0$。\n\n计算要求和输出：\n- 对于每个测试用例 $i \\in \\{1,2,3\\}$，计算 $X_i$ 的 SVD 以获得 $U_i$（其列按奇异值降序排列）。对于每个 $k \\in \\{0,1,\\dots,r_i\\}$，其中 $r_i = \\min(n,m)$ 是第 $i$ 个用例的秩，构建 $\\hat{y}_i^{(k)} = U_{i,k} U_{i,k}^\\top y_i$ 并评估您推导出的特例化准则。选择使该准则最小化的整数 $k$，若有平局则选择最小的 $k$。\n\n- 您的程序应生成单行输出，其中包含结果，形式为逗号分隔的列表并用方括号括起来（例如，$[\\,\\text{result}_1,\\text{result}_2,\\text{result}_3\\,]$）。每个 $\\text{result}_i$ 必须是为测试用例 $i$ 选择的整数 $k$。", "solution": "该问题要求推导线性估计量期望平方预测误差的无偏估计量，将其特例化到主成分回归 (PCR)，并实现一个程序为几个测试用例找到最优成分数 $k$。\n\n### 第一部分：无偏风险估计量的推导\n\n给定一个线性模型 $y = f + \\varepsilon$，其中 $y, f, \\varepsilon \\in \\mathbb{R}^n$，且 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I_n)$，方差 $\\sigma^2$ 已知。$f$ 的一个线性估计量由 $\\hat{y} = S y$ 给出，其中 $S \\in \\mathbb{R}^{n \\times n}$ 是一个确定性矩阵。\n\n我们感兴趣的量是期望平方预测误差，也称为期望预测误差 (EPE) 或风险，定义为 $\\mathbb{E}\\!\\left[\\lVert f - \\hat{y}\\rVert_2^2\\right]$。我们寻求使用可观测数据 ($y, S, \\sigma^2$) 对此量进行无偏估计。\n\n让我们展开 EPE：\n$$\n\\mathbb{E}\\!\\left[\\lVert f - \\hat{y}\\rVert_2^2\\right] = \\mathbb{E}\\!\\left[\\lVert f - S y\\rVert_2^2\\right]\n$$\n代入 $y = f + \\varepsilon$：\n$$\n\\mathbb{E}\\!\\left[\\lVert f - S(f + \\varepsilon)\\rVert_2^2\\right] = \\mathbb{E}\\!\\left[\\lVert (I - S)f - S\\varepsilon\\rVert_2^2\\right]\n$$\n展开平方 $L_2$-范数，即向量与自身的内积：\n$$\n= \\mathbb{E}\\!\\left[\\left((I - S)f - S\\varepsilon\\right)^\\top \\left((I - S)f - S\\varepsilon\\right)\\right]\n$$\n$$\n= \\mathbb{E}\\!\\left[f^\\top (I-S)^\\top(I-S)f - 2 f^\\top (I-S)^\\top S\\varepsilon + \\varepsilon^\\top S^\\top S \\varepsilon \\right]\n$$\n利用期望的线性性质，我们可以分配期望算子。由于 $f$ 和 $S$ 是确定性的，它们被视为常数。\n$$\n= f^\\top (I-S)^\\top(I-S)f - 2 f^\\top (I-S)^\\top S \\mathbb{E}[\\varepsilon] + \\mathbb{E}[\\varepsilon^\\top S^\\top S \\varepsilon]\n$$\n由于 $\\mathbb{E}[\\varepsilon] = 0$，交叉项消失。表达式简化为：\n$$\n= \\lVert (I - S)f \\rVert_2^2 + \\mathbb{E}[\\varepsilon^\\top S^\\top S \\varepsilon]\n$$\n第一项 $\\lVert (I-S)f \\rVert_2^2 = \\lVert f - \\mathbb{E}[\\hat{y}] \\rVert_2^2$ 是估计量的偏差平方。第二项是方差。对于均值为 $\\mu$、协方差为 $\\Sigma$ 的随机向量 $z$ 和矩阵 $A$，我们有性质 $\\mathbb{E}[z^\\top A z] = \\mathrm{trace}(A\\Sigma) + \\mu^\\top A \\mu$。在这里，$z=\\varepsilon$，$\\mu=0$，$\\Sigma=\\sigma^2 I_n$，且 $A=S^\\top S$。因此：\n$$\n\\mathbb{E}[\\varepsilon^\\top S^\\top S \\varepsilon] = \\mathrm{trace}(S^\\top S (\\sigma^2 I_n)) + 0 = \\sigma^2 \\mathrm{trace}(S^\\top S)\n$$\n所以，EPE 为：\n$$\n\\mathbb{E}\\!\\left[\\lVert f - \\hat{y}\\rVert_2^2\\right] = \\lVert (I - S)f \\rVert_2^2 + \\sigma^2 \\mathrm{trace}(S^\\top S)\n$$\n这个表达式依赖于不可观测的真实函数 $f$。为了构建一个估计量，我们将其与一个可观测量联系起来：训练误差，或残差平方和 (RSS)，定义为 $\\mathrm{RSS} = \\lVert y - \\hat{y} \\rVert_2^2$。让我们计算它的期望：\n$$\n\\mathbb{E}[\\mathrm{RSS}] = \\mathbb{E}[\\lVert y - Sy \\rVert_2^2] = \\mathbb{E}[\\lVert (I-S)(f+\\varepsilon) \\rVert_2^2] = \\mathbb{E}[\\lVert (I-S)f + (I-S)\\varepsilon \\rVert_2^2]\n$$\n与之前类似地展开：\n$$\n= \\lVert (I-S)f \\rVert_2^2 + \\mathbb{E}[\\varepsilon^\\top(I-S)^\\top(I-S)\\varepsilon]\n$$\n再次使用二次型期望的性质，其中 $A = (I-S)^\\top(I-S)$：\n$$\n\\mathbb{E}[\\varepsilon^\\top(I-S)^\\top(I-S)\\varepsilon] = \\sigma^2 \\mathrm{trace}((I-S)^\\top(I-S))\n$$\n使用迹的线性和循环性质：\n$$\n\\mathrm{trace}((I-S)^\\top(I-S)) = \\mathrm{trace}(I - S - S^\\top + S^\\top S) = n - 2\\mathrm{trace}(S) + \\mathrm{trace}(S^\\top S)\n$$\n所以我们有：\n$$\n\\mathbb{E}[\\mathrm{RSS}] = \\lVert (I-S)f \\rVert_2^2 + \\sigma^2 (n - 2\\mathrm{trace}(S) + \\mathrm{trace}(S^\\top S))\n$$\n现在我们有两个关于两个未知量 $\\lVert (I - S)f \\rVert_2^2$ 和 EPE 的方程。我们可以用 $\\mathbb{E}[\\mathrm{RSS}]$ 来求解 EPE。从最后一个方程中，我们分离出偏差平方项：\n$$\n\\lVert (I - S)f \\rVert_2^2 = \\mathbb{E}[\\mathrm{RSS}] - \\sigma^2 (n - 2\\mathrm{trace}(S) + \\mathrm{trace}(S^\\top S))\n$$\n将此代入我们的 EPE 表达式中：\n$$\n\\mathrm{EPE} = \\left( \\mathbb{E}[\\mathrm{RSS}] - \\sigma^2 (n - 2\\mathrm{trace}(S) + \\mathrm{trace}(S^\\top S)) \\right) + \\sigma^2 \\mathrm{trace}(S^\\top S)\n$$\n$$\n\\mathrm{EPE} = \\mathbb{E}[\\mathrm{RSS}] - n\\sigma^2 + 2\\sigma^2\\mathrm{trace}(S) = \\mathbb{E}[\\mathrm{RSS} - n\\sigma^2 + 2\\sigma^2\\mathrm{trace}(S)]\n$$\n这表明统计量 $C = \\mathrm{RSS} - n\\sigma^2 + 2\\sigma^2\\mathrm{trace}(S)$ 是 EPE 的一个无偏估计量，因为 $\\mathbb{E}[C] = \\mathrm{EPE}$。该估计量完全用可观测量表示：RSS $\\lVert y - Sy \\rVert_2^2$、维度 $n$、噪声方差 $\\sigma^2$ 以及算子矩阵 $S$ 的迹。这是 Mallows' $C_p$ 统计量的一个推广。\n\n### 第二部分：主成分回归 (PCR) 的特例化\n\n在 PCR 中，估计量是 $\\hat{y}^{(k)} = U_k U_k^\\top y$，这意味着算子矩阵是 $S_k = U_k U_k^\\top$。矩阵 $S_k$ 将数据投影到由前 $k$ 个主成分方向张成的子空间上。作为一个投影矩阵，$S_k$ 是对称的 ($S_k^\\top = S_k$) 和幂等的 ($S_k^2 = S_k$)。\n\n我们通过代入 $S = S_k$ 来特例化一般准则 $C = \\lVert y - Sy \\rVert_2^2 - n\\sigma^2 + 2\\sigma^2\\mathrm{trace}(S)$。\n给定 $k$ 的 RSS 项是 $\\mathrm{RSS}(k) = \\lVert y - U_k U_k^\\top y \\rVert_2^2$。\n迹项是模型的自由度，对于投影矩阵，它等于其投影到的子空间的维度。正如问题中所给出的，并且可以使用迹的循环性质来验证：\n$$\n\\mathrm{trace}(S_k) = \\mathrm{trace}(U_k U_k^\\top) = \\mathrm{trace}(U_k^\\top U_k) = \\mathrm{trace}(I_k) = k\n$$\n其中 $I_k$ 是 $k \\times k$ 单位矩阵。\n\n将这些代入无偏风险估计量的一般公式，我们得到 PCR 的特例化准则，我们将其表示为 $C(k)$：\n$$\nC(k) = \\lVert y - U_k U_k^\\top y \\rVert_2^2 - n\\sigma^2 + 2k\\sigma^2\n$$\n这就是所求的准则，完全用 $y$、$U_k$、$\\sigma^2$ 和 $n$ 来表示。\n\n### 第三部分：最小化该准则的理由\n\n模型选择的目标是从一个模型族中选择一个在新的、未见过的数据上表现最好的模型。EPE，$\\mathbb{E}\\!\\left[\\lVert f - \\hat{y}\\rVert_2^2\\right]$，是衡量这种泛化性能的典型度量。由于我们已经为每个可能的模型复杂度 $k$ 推导出了 EPE 的一个无偏估计量 $C(k)$，因此选择使 $C(k)$ 最小化的 $k$ 值是最小化估计预测误差的一个直接且有原则的策略。\n\n通过分析其组成部分，可以更好地理解准则 $C(k)$。由于项 $-n\\sigma^2$ 相对于 $k$ 是常数，最小化 $C(k)$ 等价于最小化：\n$$\n\\mathrm{RSS}(k) + 2k\\sigma^2\n$$\n这个表达式清晰地说明了偏差-方差权衡。\n-   $\\mathrm{RSS}(k) = \\lVert y - \\hat{y}^{(k)} \\rVert_2^2$ 是训练误差。随着 $k$ 的增加，模型变得更加复杂和灵活，投影到更大的子空间上。这使其能够更紧密地拟合训练数据 $y$，因此 $\\mathrm{RSS}(k)$ 是 $k$ 的一个非增函数。该项与模型的偏差有关。一个简单的模型（小 $k$）可能过于刚性，导致高偏差和大的 RSS。\n-   $2k\\sigma^2$ 是一个惩罚项，它随模型复杂度 $k$ 线性增加。它代表了拟合数据中噪声的成本。一个更复杂的模型（大 $k$）将有更多参数，并可能开始拟合存在于 $y$ 中的随机波动 $\\varepsilon$，这种现象被称为过拟合。这会导致预测的高方差。惩罚项抵消了 RSS 总是偏爱更复杂模型的倾向。\n\n通过最小化这两项的和，我们寻求一个能达到最佳平衡的 $k$ 值：一个足够复杂以捕捉 $f$ 中潜在结构，但又不过于复杂以至于对噪声 $\\varepsilon$ 过拟合的模型。这种方法提供了一种有原则的方式来驾驭偏差-方差权衡，并选择一个具有良好期望泛化性能的模型。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the three test cases for PCR model selection.\n    \"\"\"\n\n    def compute_best_k(X, y, sigma_sq):\n        \"\"\"\n        Computes the optimal number of principal components k for a single test case.\n        \n        Args:\n            X (np.ndarray): The design matrix (n, m).\n            y (np.ndarray): The response vector (n,).\n            sigma_sq (float): The known noise variance.\n\n        Returns:\n            int: The integer k that minimizes the selection criterion.\n        \"\"\"\n        n, m = X.shape\n        r = min(n, m)\n\n        # Compute the \"thin\" SVD of X. U will have shape (n, r).\n        # numpy's svd returns U, s, Vh where s are the singular values in\n        # descending order, which is what the problem requires.\n        try:\n            U, s, Vt = np.linalg.svd(X, full_matrices=False)\n        except np.linalg.LinAlgError:\n            # Handle cases where SVD might fail, though unlikely with given data.\n            # A fallback could be to use a pseudo-inverse or handle a rank-deficient matrix.\n            # For this problem, the matrices are well-behaved.\n            # If X is all zeros, U might be arbitrary.\n            if np.all(X == 0):\n                U = np.zeros((n, r))\n            else:\n                raise\n\n        criteria = []\n        for k in range(r + 1):\n            if k == 0:\n                # The projection is onto the zero vector, so y_hat is the zero vector.\n                # U_0 is an n x 0 matrix, U_0 U_0^T is the zero matrix.\n                rss = np.sum(y**2)\n            else:\n                # U_k contains the first k columns of U\n                Uk = U[:, :k]\n                \n                # Project y onto the column space of Uk\n                # y_hat_k = Uk @ Uk.T @ y\n                y_hat = Uk @ (Uk.T @ y)\n                \n                # Calculate Residual Sum of Squares (RSS)\n                rss = np.sum((y - y_hat)**2)\n            \n            # The criterion to minimize is RSS(k) + 2*k*sigma^2.\n            # The term -n*sigma^2 from the full C(k) is constant w.r.t k\n            # and can be ignored for minimization.\n            criterion = rss + 2 * k * sigma_sq\n            criteria.append(criterion)\n\n        # Find the index k that minimizes the criterion.\n        # np.argmin() returns the first occurrence of the minimum value,\n        # which respects the tie-breaking rule.\n        best_k = np.argmin(criteria)\n        return best_k\n\n    # --- Test Case 1 ---\n    X1 = np.array([\n        [ 2, -1,  0,  3,  1],\n        [ 1,  0,  4, -2,  2],\n        [ 3,  1, -1,  0, -1],\n        [ 0,  2,  1,  1,  0],\n        [ 1, -2,  3,  0,  2],\n        [-1,  1,  0, -1,  3],\n        [ 2,  0, -2,  2, -1],\n        [ 0,  3, -1,  1,  1]\n    ])\n    y1 = np.array([4.0, -1.0, 3.0, 0.0, 2.0, -2.0, 1.0, 0.5])\n    sigma_sq1 = 0.5\n    k1 = compute_best_k(X1, y1, sigma_sq1)\n\n    # --- Test Case 2 ---\n    X2 = np.array([\n        [ 1,  2,  0, -1],\n        [ 0, -1,  3,  2],\n        [ 2,  0,  1, -2],\n        [-1,  1,  0,  1],\n        [ 3, -2,  1,  0],\n        [ 0,  1, -1,  2]\n    ])\n    z2 = np.array([1.0, -2.0, 0.5, 3.0, -1.0, 0.0])\n    sigma_sq2 = 0.2\n    # Construct y2 as specified\n    U2, _, _ = np.linalg.svd(X2, full_matrices=False)\n    y2 = z2 - U2 @ (U2.T @ z2) \n    k2 = compute_best_k(X2, y2, sigma_sq2)\n\n    # --- Test Case 3 ---\n    X3 = np.array([\n        [ 1,  0,  1, -1,  2,  0,  3],\n        [ 0,  1,  2,  0, -1,  1,  0],\n        [ 1, -1,  0,  2,  0,  1, -2],\n        [ 2,  0, -1,  1,  1,  0,  0],\n        [ 0,  2,  1, -2,  0,  1,  1],\n        [-1,  1,  0,  1,  2, -1,  0],\n        [ 0,  0,  1,  0,  1,  2, -1],\n        [ 1,  2,  0, -1,  0,  1,  1],\n        [ 2, -1,  1,  0,  0,  0,  2],\n        [ 0,  1, -2,  1,  1,  0,  0]\n    ])\n    y3 = np.array([0.2, -0.1, 0.5, 0.0, -0.3, 0.1, 0.0, 0.2, -0.2, 0.3])\n    sigma_sq3 = 10.0\n    k3 = compute_best_k(X3, y3, sigma_sq3)\n\n    results = [k1, k2, k3]\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3143703"}, {"introduction": "主成分回归最大的陷阱在于混淆了“解释方差”与“预测能力”。本练习将通过一个动手编程任务，让你直观地对比两种常用的成分选择方法：基于解释方差比例的无监督方法，和基于交叉验证误差的有监督方法 [@problem_id:3160814]。通过精心设计的模拟场景，你将亲身体会到为何高方差的主成分不一定具有高预测价值——这是每一个数据分析师都必须掌握的关键一课。", "problem": "您的任务是实现主成分回归 (PCR)，并比较两种选择主成分数量 $k$ 的标准：通过目标累积解释方差阈值（例如 $\\tau = 0.95$）选择 $k$，与通过最小化K折交叉验证 (CV) 均方误差 (MSE) 选择 $k$。您的实现必须基于核心定义和经过充分检验的公式：主成分分析 (PCA)、奇异值分解 (SVD)、最小二乘线性回归以及K折交叉验证。\n\n基本原理：\n- 主成分分析 (PCA) 将一个中心化的数据矩阵 $X \\in \\mathbb{R}^{n \\times p}$ 转换为按方差排序的正交主成分。如果 $X$ 的瘦奇异值分解 (SVD) 为 $X = U S V^\\top$，那么主轴是 $V$ 的列，奇异值是 $S$ 的对角线元素，样本协方差的特征值与 $S^2$ 成正比。前 $k$ 个主成分的解释方差比是 $S^2$ 的前 $k$ 个元素的累积和除以 $S^2$ 的所有元素之和。\n- 主成分回归 (PCR) 使用前 $k$ 个主成分的得分来拟合响应变量 $y \\in \\mathbb{R}^n$ 的线性模型。为了包含截距项，需要对训练集上的 $X$ 和 $y$ 进行中心化，通过最小二乘法将 $y$ 回归到前 $k$ 个主成分得分列上，最后将 $y$ 的训练均值加回到预测值中。\n- K折交叉验证 (CV) 将索引 $\\{1,\\dots,n\\}$ 划分为 $K$ 个折，在 $K-1$ 个折上进行训练，在留出的折上进行验证，并计算所有折的验证均方误差 (MSE) 的平均值。在大小为 $m$ 的验证集上，预测值 $\\hat{y}$ 相对于真实值 $y$ 的 MSE 为 $\\frac{1}{m} \\sum_{i=1}^m (y_i - \\hat{y}_i)^2$。\n\n定义和任务：\n1. 按如下方式实现 PCR：\n   - 通过减去列均值来中心化 $X$，通过减去其均值来中心化训练集上的 $y$。\n   - 对中心化的训练预测变量计算瘦 SVD $X_{\\text{train}} = U S V^\\top$。\n   - 使用 $V$ 的前 $k$ 列构成主成分得分 $Z_{\\text{train}} = X_{\\text{train}} V_{[:,1:k]}$。\n   - 通过最小二乘法拟合 $Z_{\\text{train}}$ 上的系数。对测试集进行预测时，使用训练均值中心化测试集 $X$，将其投影到相同的 $V_{[:,1:k]}$ 上，并将 $y$ 的训练均值加回到中心化的预测值中。\n2. 按解释方差选择 $k$：计算中心化后的完整 $X$ 的奇异值 $S$，形成 $S^2$，并选择最小的 $k$，使得 $S^2$ 的前 $k$ 个元素的累积比率与 $S^2$ 所有元素之和的比值至少为 $\\tau$。\n3. 按 CV MSE 选择 $k$：对于每个 $k \\in \\{1,2,\\dots,p\\}$，计算使用 $k$ 个主成分的 PCR 的平均 K折 CV MSE，并选择使其最小化的 $k$。如果出现平局，选择最小的 $k$。\n4. 对于每种情景，报告元组 $[k_{\\text{var}}, k_{\\text{cv}}, \\text{MSE}(k_{\\text{var}}), \\text{MSE}(k_{\\text{cv}})]$，其中 $k_{\\text{var}}$ 是按解释方差选择的结果，$k_{\\text{cv}}$ 是按 CV MSE 选择的结果。为保持一致性，将 MSE 值四舍五入到 $4$ 位小数。\n\n数据生成模型：\n- 设 $\\boldsymbol{\\lambda} \\in \\mathbb{R}^p$ 是一个目标特征值向量。生成样本 $x_i \\in \\mathbb{R}^p$ 为 $x_i = g_i \\operatorname{diag}(\\sqrt{\\boldsymbol{\\lambda}})$，其中 $g_i \\sim \\mathcal{N}(\\mathbf{0}, I_p)$ 独立同分布。这将产生协方差为 $\\operatorname{diag}(\\boldsymbol{\\lambda})$ 且主轴等于坐标轴（单位旋转）的数据。\n- 通过一个向量 $\\beta_{\\text{rot}} \\in \\mathbb{R}^p$ 定义主成分基中的系数向量 $\\beta \\in \\mathbb{R}^p$，该向量指定了每个主成分对响应的贡献。由于旋转是单位阵，直接使用 $\\beta = \\beta_{\\text{rot}}$。\n- 生成响应为 $y_i = x_i^\\top \\beta + \\epsilon_i$，其中 $\\epsilon_i \\sim \\mathcal{N}(0, \\sigma_\\epsilon^2)$ 独立同分布。\n\n测试套件：\n使用 $K=5$ 的K折交叉验证实现上述过程。对主轴使用单位旋转，并考虑以下四种情景。对于每种情景，将用于数据生成和折分配的随机种子设置为指定值。\n\n- 情景 1（信号与最高方差分量对齐）：\n  - $n = 200$, $p = 8$\n  - $\\boldsymbol{\\lambda} = (9, 3, 1, 0.5, 0.3, 0.2, 0.1, 0.05)$\n  - $\\beta_{\\text{rot}} = (1, 0, 0, 0, 0, 0, 0, 0)$\n  - $\\sigma_\\epsilon = 0.2$\n  - $\\tau = 0.95$\n  - seed $= 42$\n- 情景 2（信号与最低方差分量对齐）：\n  - $n = 200$, $p = 8$\n  - $\\boldsymbol{\\lambda} = (9, 3, 1, 0.5, 0.3, 0.2, 0.1, 0.05)$\n  - $\\beta_{\\text{rot}} = (0, 0, 0, 0, 0, 0, 0, 1)$\n  - $\\sigma_\\epsilon = 0.2$\n  - $\\tau = 0.95$\n  - seed $= 7$\n- 情景 3（平坦的方差谱，高噪声，多分量信号）：\n  - $n = 300$, $p = 12$\n  - $\\boldsymbol{\\lambda} = (1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)$\n  - $\\beta_{\\text{rot}} = (1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0)$\n  - $\\sigma_\\epsilon = 1.5$\n  - $\\tau = 0.95$\n  - seed $= 123$\n- 情景 4（阈值为1，低噪声，信号在前两个分量上）：\n  - $n = 120$, $p = 10$\n  - $\\boldsymbol{\\lambda} = (5, 4, 3, 2, 1.5, 1, 0.8, 0.6, 0.4, 0.2)$\n  - $\\beta_{\\text{rot}} = (1, 1, 0, 0, 0, 0, 0, 0, 0, 0)$\n  - $\\sigma_\\epsilon = 0.1$\n  - $\\tau = 1.0$\n  - seed $= 99$\n\n不涉及角度单位。不涉及物理单位。最终输出为数值，并且必须按以下方式报告为整数和浮点数的列表。\n\n最终输出格式：\n- 您的程序应生成一行输出，其中包含一个用方括号括起来的逗号分隔列表。每个情景贡献一个列表 $[k_{\\text{var}}, k_{\\text{cv}}, \\text{MSE}(k_{\\text{var}}), \\text{MSE}(k_{\\text{cv}})]$，其中 MSE 条目四舍五入到4位小数。例如，输出应类似于 $[[1,4,0.1234,0.0567],[\\dots],\\dots]$。", "solution": "该问题要求实现主成分回归 (PCR)，并比较两种选择最优主成分数量 $k$ 的不同方法。第一种方法基于累积解释方差阈值 $\\tau$ 来选择 $k$。第二种方法通过最小化由K折交叉验证 (CV) 估计的均方误差 (MSE) 来选择 $k$。该比较在四种具有指定数据生成参数的模拟情景下进行。\n\n### 1. 数据生成模型\n\n数据由一个线性模型生成。对于指定的样本数 $n$ 和预测变量数 $p$，预测变量矩阵 $X \\in \\mathbb{R}^{n \\times p}$ 的构造使其总体协方差矩阵为对角矩阵，其对角线元素由向量 $\\boldsymbol{\\lambda} \\in \\mathbb{R}^p$ 给出。具体来说，$X$ 的每一行 $x_i^\\top$ 生成如下：\n$$ x_i = G_i \\operatorname{diag}(\\sqrt{\\boldsymbol{\\lambda}}) $$\n其中 $G_i$ 是一个由 $p$ 个来自标准正态分布 $\\mathcal{N}(0, 1)$ 的独立样本组成的行向量。这等价于生成一个矩阵 $G \\in \\mathbb{R}^{n \\times p}$，其元素是独立同分布的 $\\mathcal{N}(0, 1)$ 随机变量，并设置 $X = G \\operatorname{diag}(\\sqrt{\\boldsymbol{\\lambda}})$。该数据分布的总体主轴是标准基向量（一个单位旋转）。\n\n响应向量 $y \\in \\mathbb{R}^n$ 根据线性模型生成：\n$$ y_i = x_i^\\top \\beta + \\epsilon_i $$\n其中 $\\beta \\in \\mathbb{R}^p$ 是真实回归系数向量，$\\epsilon_i \\sim \\mathcal{N}(0, \\sigma_\\epsilon^2)$ 是独立同分布的误差项。系数向量 $\\beta$ 直接由 $\\beta = \\beta_{\\text{rot}}$ 给出，其中 $\\beta_{\\text{rot}}$ 指定了沿每个主成分轴的信号强度。\n\n### 2. 主成分回归 (PCR) 算法\n\nPCR 是一个两阶段过程，首先使用主成分分析 (PCA) 降低预测变量的维度，然后在得到的主成分上执行线性回归。针对训练集 $(X_{\\text{train}}, y_{\\text{train}})$ 和测试集 $X_{\\text{test}}$ 指定的程序如下：\n\n1.  **数据中心化**：训练数据被中心化以使其均值为零。测试数据使用训练数据的均值进行中心化，以防止信息泄露。\n    $$ \\bar{x}_{\\text{train}} = \\frac{1}{n_{\\text{train}}} \\sum_{i=1}^{n_{\\text{train}}} x_{\\text{train},i}, \\quad \\bar{y}_{\\text{train}} = \\frac{1}{n_{\\text{train}}} \\sum_{i=1}^{n_{\\text{train}}} y_{\\text{train},i} $$\n    $$ X_{\\text{c,train}} = X_{\\text{train}} - \\mathbf{1}\\bar{x}_{\\text{train}}^\\top, \\quad y_{\\text{c,train}} = y_{\\text{train}} - \\bar{y}_{\\text{train}} $$\n    $$ X_{\\text{c,test}} = X_{\\text{test}} - \\mathbf{1}\\bar{x}_{\\text{train}}^\\top $$\n\n2.  **主成分提取**：通过计算中心化训练预测变量矩阵的奇异值分解 (SVD) 来找到主轴：\n    $$ X_{\\text{c,train}} = U S V^\\top $$\n    $V \\in \\mathbb{R}^{p \\times p}$ 的列是主轴（或载荷向量）。\n\n3.  **得分投影**：将训练数据投影到前 $k$ 个主轴上，以获得得分矩阵 $Z_{\\text{train},k} \\in \\mathbb{R}^{n_{\\text{train}} \\times k}$：\n    $$ Z_{\\text{train},k} = X_{\\text{c,train}} V_k $$\n    其中 $V_k \\in \\mathbb{R}^{p \\times k}$ 包含 $V$ 的前 $k$ 列。\n\n4.  **基于得分的回归**：使用普通最小二乘法拟合一个线性模型，将中心化的响应 $y_{\\text{c,train}}$ 回归到得分 $Z_{\\text{train},k}$ 上，得到系数 $\\hat{\\theta}_k \\in \\mathbb{R}^k$：\n    $$ \\hat{\\theta}_k = (Z_{\\text{train},k}^\\top Z_{\\text{train},k})^{-1} Z_{\\text{train},k}^\\top y_{\\text{c,train}} $$\n\n5.  **预测**：对测试集进行预测时，首先将中心化的测试数据 $X_{\\text{c,test}}$ 投影到相同的 $k$ 个主轴上以获得测试得分 $Z_{\\text{test},k}$，然后应用拟合的系数 $\\hat{\\theta}_k$，最后加回训练响应的均值：\n    $$ Z_{\\text{test},k} = X_{\\text{c,test}} V_k $$\n    $$ \\hat{y}_{\\text{test}} = Z_{\\text{test},k} \\hat{\\theta}_k + \\bar{y}_{\\text{train}} $$\n\n### 3. 主成分选择 I：累积解释方差\n\n该方法根据预测变量中被捕获的方差比例来选择主成分数量 $k$。它应用于完整数据集 $X$。\n\n1.  中心化完整数据矩阵：$X_c = X - \\mathbf{1}\\bar{x}^\\top$。\n2.  计算其 SVD：$X_c = U S V^\\top$。$S$ 的对角线元素是奇异值，$s_1 \\ge s_2 \\ge \\dots \\ge s_p \\ge 0$。\n3.  第 $j$ 个主成分解释的方差与 $s_j^2$ 成正比。前 $k$ 个主成分的累积解释方差比 (CEVR) 为：\n    $$ \\text{CEVR}(k) = \\frac{\\sum_{j=1}^k s_j^2}{\\sum_{j=1}^p s_j^2} $$\n4.  给定一个阈值 $\\tau$，主成分数量 $k_{\\text{var}}$ 被选为满足该阈值的最小 $k$：\n    $$ k_{\\text{var}} = \\min \\{ k \\in \\{1, 2, \\dots, p\\} \\mid \\text{CEVR}(k) \\ge \\tau \\} $$\n\n### 4. 主成分选择 II：K折交叉验证\n\n该方法通过直接估计每个可能的 $k$ 值下 PCR 模型的预测误差来选择 $k$。\n\n1.  将包含 $n$ 个样本的数据集划分为 $K$ 个大小约相等的不相交的折。对于此问题，$K=5$。\n2.  对于每个候选的主成分数量 $k \\in \\{1, 2, \\dots, p\\}$：\n    a. 一个外层循环遍历每个折 $j=1, \\dots, K$。\n    b. 在每次迭代中，第 $j$ 个折被指定为验证集 $(X_{\\text{val}}, y_{\\text{val}})$，其余 $K-1$ 个折构成训练集 $(X_{\\text{train}}, y_{\\text{train}})$。\n    c. 在 $(X_{\\text{train}}, y_{\\text{train}})$ 上训练一个包含 $k$ 个主成分的 PCR 模型。\n    d. 使用训练好的模型对验证集 $X_{\\text{val}}$ 进行预测，得到 $\\hat{y}_{\\text{val}}$。\n    e. 计算该折的均方误差 (MSE)：$\\text{MSE}_j(k) = \\frac{1}{n_{\\text{val}}} \\sum_{i \\in \\text{fold }j} (y_{\\text{val},i} - \\hat{y}_{\\text{val},i})^2$。\n3.  遍历所有折后，对于 $k$ 个主成分的交叉验证 MSE 是各折特定 MSE 的平均值：\n    $$ \\text{CV-MSE}(k) = \\frac{1}{K} \\sum_{j=1}^K \\text{MSE}_j(k) $$\n4.  最优主成分数量 $k_{\\text{cv}}$ 是最小化此平均误差的那个。平局时选择最小的 $k$。\n    $$ k_{\\text{cv}} = \\operatorname{argmin}_{k \\in \\{1, \\dots, p\\}} \\text{CV-MSE}(k) $$\n\n### 5. 实现与评估\n\n对于每个情景，我们首先生成数据 $(X, y)$。然后我们应用两种选择标准。基于方差的方法得到 $k_{\\text{var}}$。对所有 $k \\in \\{1, \\dots, p\\}$ 运行交叉验证过程，产生一个 CV-MSE 值列表。从此列表中，我们确定 $k_{\\text{cv}}$ 为最小化者。最终报告的值是 $k_{\\text{var}}$、$k_{\\text{cv}}$、为 $k_{\\text{var}}$ 预先计算的 CV-MSE（即 $\\text{CV-MSE}(k_{\\text{var}})$）以及最小 CV-MSE（即 $\\text{CV-MSE}(k_{\\text{cv}})$）。这提供了对两种所选模型的预测性能的直接比较。随机种子用于确保数据生成和交叉验证中折分配的可复现性。", "answer": "```python\nimport numpy as np\n\ndef generate_data(n, p, lambdas, beta_rot, sigma_eps, seed):\n    \"\"\"Generates data for a given scenario.\"\"\"\n    rng = np.random.default_rng(seed)\n    # Generate standard normal data\n    G = rng.normal(size=(n, p))\n    # Scale columns to achieve target covariance\n    sqrt_lambdas = np.sqrt(lambdas)\n    X = G * sqrt_lambdas\n    \n    # Generate response\n    # Since principal axes are standard basis, beta = beta_rot\n    beta = beta_rot\n    epsilon = rng.normal(scale=sigma_eps, size=n)\n    y = X @ beta + epsilon\n    \n    return X, y\n\ndef pcr_predict(X_train, y_train, X_test, k):\n    \"\"\"\n    Fits a PCR model on training data and predicts on test data.\n    \"\"\"\n    # 1. Centering\n    x_mean = np.mean(X_train, axis=0)\n    y_mean = np.mean(y_train)\n    \n    X_train_c = X_train - x_mean\n    y_train_c = y_train - y_mean\n    X_test_c = X_test - x_mean\n    \n    # 2. PCA on training data\n    # The problem specifies thin SVD X = U S V^T.\n    # np.linalg.svd returns U, s, Vt where s is a 1D array of singular values.\n    try:\n        _, s, Vt = np.linalg.svd(X_train_c, full_matrices=False)\n    except np.linalg.LinAlgError:\n         # In rare cases with highly collinear data, SVD can fail.\n         # This shouldn't happen with the specified data generation, but as a fallback:\n         return np.full(X_test.shape[0], y_mean) # Predict the mean\n         \n    V = Vt.T\n    \n    # 3. Project onto first k components\n    V_k = V[:, :k]\n    Z_train_k = X_train_c @ V_k\n    \n    # 4. Fit linear model on scores\n    # Using lstsq for numerical stability\n    theta_k, _, _, _ = np.linalg.lstsq(Z_train_k, y_train_c, rcond=None)\n    \n    # 5. Predict on test data\n    Z_test_k = X_test_c @ V_k\n    y_pred_c = Z_test_k @ theta_k\n    y_pred = y_pred_c + y_mean\n    \n    return y_pred\n\ndef select_k_variance(X, tau):\n    \"\"\"\n    Selects k based on cumulative explained variance.\n    \"\"\"\n    # Center the data\n    X_c = X - np.mean(X, axis=0)\n    \n    # SVD\n    _, s, _ = np.linalg.svd(X_c, full_matrices=False)\n    \n    # Explained variance ratios\n    s_squared = s**2\n    total_variance = np.sum(s_squared)\n    \n    if total_variance == 0:\n        return 1\n\n    cumulative_variance_ratio = np.cumsum(s_squared) / total_variance\n    \n    # Find smallest k such that ratio = tau\n    k_var_candidates = np.where(cumulative_variance_ratio = tau)[0]\n    if len(k_var_candidates) == 0:\n        # This occurs if tau=1.0 and there's numerical imprecision\n        k_var = X.shape[1]\n    else:\n        k_var = k_var_candidates[0] + 1 # +1 for 1-based indexing\n        \n    return k_var\n\ndef select_k_cv(X, y, p, K, seed):\n    \"\"\"\n    Selects k by K-fold Cross-Validation and returns CV MSE for all k.\n    \"\"\"\n    n = X.shape[0]\n    rng = np.random.default_rng(seed)\n    indices = np.arange(n)\n    rng.shuffle(indices)\n    \n    folds = np.array_split(indices, K)\n    \n    cv_mses = []\n    \n    for k in range(1, p + 1):\n        fold_mses = []\n        for j in range(K):\n            val_indices = folds[j]\n            train_indices = np.concatenate([folds[i] for i in range(K) if i != j])\n            \n            X_train, y_train = X[train_indices], y[train_indices]\n            X_val, y_val = X[val_indices], y[val_indices]\n            \n            y_pred = pcr_predict(X_train, y_train, X_val, k)\n            \n            mse = np.mean((y_val - y_pred)**2)\n            fold_mses.append(mse)\n            \n        avg_mse = np.mean(fold_mses)\n        cv_mses.append(avg_mse)\n        \n    cv_mses = np.array(cv_mses)\n    # Get k that minimizes CV MSE, breaking ties by choosing smallest k.\n    # np.argmin() naturally does this.\n    k_cv = np.argmin(cv_mses) + 1 # +1 for 1-based indexing\n    \n    return k_cv, cv_mses\n\ndef solve():\n    \"\"\"\n    Main function to run the scenarios and print results.\n    \"\"\"\n    scenarios = [\n        {'n': 200, 'p': 8, 'lambdas': np.array([9, 3, 1, 0.5, 0.3, 0.2, 0.1, 0.05]), \n         'beta_rot': np.array([1, 0, 0, 0, 0, 0, 0, 0]), 'sigma_eps': 0.2, 'tau': 0.95, 'seed': 42},\n        {'n': 200, 'p': 8, 'lambdas': np.array([9, 3, 1, 0.5, 0.3, 0.2, 0.1, 0.05]), \n         'beta_rot': np.array([0, 0, 0, 0, 0, 0, 0, 1]), 'sigma_eps': 0.2, 'tau': 0.95, 'seed': 7},\n        {'n': 300, 'p': 12, 'lambdas': np.ones(12), \n         'beta_rot': np.array([1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'sigma_eps': 1.5, 'tau': 0.95, 'seed': 123},\n        {'n': 120, 'p': 10, 'lambdas': np.array([5, 4, 3, 2, 1.5, 1, 0.8, 0.6, 0.4, 0.2]), \n         'beta_rot': np.array([1, 1, 0, 0, 0, 0, 0, 0, 0, 0]), 'sigma_eps': 0.1, 'tau': 1.0, 'seed': 99}\n    ]\n    \n    K = 5\n    results = []\n\n    for params in scenarios:\n        X, y = generate_data(params['n'], params['p'], params['lambdas'], \n                             params['beta_rot'], params['sigma_eps'], params['seed'])\n        \n        # Method 1: Variance Explained Threshold\n        k_var = select_k_variance(X, params['tau'])\n        \n        # Method 2: Cross-Validation\n        k_cv, cv_mses_all_k = select_k_cv(X, y, params['p'], K, params['seed'])\n        \n        # Get MSEs for the chosen k values\n        mse_for_k_var = cv_mses_all_k[k_var - 1]\n        mse_for_k_cv = cv_mses_all_k[k_cv - 1]\n        \n        results.append([k_var, k_cv, mse_for_k_var, mse_for_k_cv])\n        \n    # Format output as specified\n    formatted_results = []\n    for res in results:\n        # Rounding is handled by the format specifier\n        s = f\"[{res[0]},{res[1]},{res[2]:.4f},{res[3]:.4f}]\"\n        formatted_results.append(s)\n        \n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n\n```", "id": "3160814"}]}