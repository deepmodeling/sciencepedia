{"hands_on_practices": [{"introduction": "该练习旨在让你确定一个关键的正则化阈值 $\\lambda_{\\max}$，当正则化参数大于此值时，组套索模型将产生一个完全稀疏的零解。通过运用基本的 Karush-Kuhn-Tucker (KKT) 条件，你将推导出计算 $\\lambda_{\\max}$ 的通用公式，并将其应用于一个具体的数据集。这个练习对于理解正则化路径的起点和高效实现路径算法至关重要。[@problem_id:3126744]", "problem": "考虑组最小绝对收缩与选择算子 (group lasso) 框架下带有分组预测变量的线性模型。数据由一个响应向量 $Y \\in \\mathbb{R}^{n}$ 和一个设计矩阵 $X \\in \\mathbb{R}^{n \\times p}$ 组成，该矩阵的列被划分为 $G$ 个不相交的组。将参数向量记为 $\\beta = (\\beta_{1}^{\\top}, \\dots, \\beta_{G}^{\\top})^{\\top}$，其中每个 $\\beta_{g} \\in \\mathbb{R}^{p_{g}}$ 对应于包含 $p_{g}$ 列的组 $g$，且 $X = [X_{1} \\,\\, X_{2} \\,\\, \\dots \\,\\, X_{G}]$，其中 $X_{g} \\in \\mathbb{R}^{n \\times p_{g}}$。group lasso 估计量通过最小化以下凸目标函数得到\n$$\n\\frac{1}{2n} \\left\\| Y - \\sum_{g=1}^{G} X_{g} \\beta_{g} \\right\\|_{2}^{2} \\;+\\; \\lambda \\sum_{g=1}^{G} w_{g} \\left\\| \\beta_{g} \\right\\|_{2},\n$$\n其中 $\\lambda \\geq 0$ 是正则化参数，而 $w_{g} > 0$ 是给定的组权重。\n\n从凸优化的第一性原理出发，特别是非光滑凸函数的次梯度最优性 (Karush-Kuhn-Tucker (KKT) 条件) 和欧几里得范数的次微分，推导使零向量 $\\hat{\\beta} = 0$ 满足 group lasso 问题最优性条件的最小正则化值 $\\lambda_{\\max}$ 的闭式表达式。然后，对于下面的具体数据集，计算 $\\lambda_{\\max}$ 的数值。\n\n使用 $n = 5$，$G = 3$，以及以下的 $Y$ 和 $X$ 的分组列：\n- $Y = (2, \\,-1, \\,0, \\,3, \\,-2)^{\\top}$。\n- 组 $g=1$ 有 $p_{1} = 2$ 列：\n$$\nx_{1} = (1, \\,0, \\,1, \\,0, \\,1)^{\\top}, \\quad x_{2} = (0, \\,1, \\,0, \\,1, \\,0)^{\\top},\n$$\n所以 $X_{1} = [x_{1} \\,\\, x_{2}]$。\n- 组 $g=2$ 有 $p_{2} = 1$ 列：\n$$\nx_{3} = (1, \\,1, \\,1, \\,1, \\,1)^{\\top},\n$$\n所以 $X_{2} = [x_{3}]$。\n- 组 $g=3$ 有 $p_{3} = 1$ 列：\n$$\nx_{4} = (2, \\,-1, \\,0, \\,1, \\,2)^{\\top},\n$$\n所以 $X_{3} = [x_{4}]$。\n\n令组权重为 $w_{1} = \\sqrt{2}$，$w_{2} = 1$ 和 $w_{3} = 1$。你的最终答案必须是此数据集的 $\\lambda_{\\max}$ 所对应的单个实数或精确闭式表达式。无需四舍五入。最终答案表示为不带单位的数值。", "solution": "该问题被评估为有效。它在科学上基于正则化线性模型（group lasso）和凸优化的既定理论。该问题是适定的，具有完整且一致的给定条件，允许为 $\\lambda_{\\max}$ 推导出唯一且有意义的解。所用术语是客观和正式的。\n\ngroup lasso 估计量 $\\hat{\\beta}$ 是以下凸优化问题的解：\n$$\n\\min_{\\beta} L(\\beta) = \\min_{\\beta_{1}, \\dots, \\beta_{G}} \\left\\{ \\frac{1}{2n} \\left\\| Y - \\sum_{g=1}^{G} X_{g} \\beta_{g} \\right\\|_{2}^{2} \\;+\\; \\lambda \\sum_{g=1}^{G} w_{g} \\left\\| \\beta_{g} \\right\\|_{2} \\right\\}\n$$\n目标函数 $L(\\beta)$ 是一个可微凸函数（最小二乘损失）和一个不可微但凸的函数（group lasso 惩罚项）之和。Karush-Kuhn-Tucker (KKT) 条件为点 $\\hat{\\beta}$ 是一个最小值点提供了充要条件。这些条件表明，零向量必须属于目标函数在最小值点处的次微分，即 $0 \\in \\partial L(\\hat{\\beta})$。\n\n$L(\\beta)$ 的次微分由下式给出：\n$$\n\\partial L(\\beta) = \\nabla \\left( \\frac{1}{2n} \\left\\| Y - X\\beta \\right\\|_{2}^{2} \\right) + \\lambda \\sum_{g=1}^{G} w_{g} \\partial \\left\\| \\beta_{g} \\right\\|_{2}\n$$\n最小二乘项的梯度是：\n$$\n\\nabla \\left( \\frac{1}{2n} \\left\\| Y - X\\beta \\right\\|_{2}^{2} \\right) = -\\frac{1}{n} X^{\\top} (Y - X\\beta)\n$$\n我们感兴趣的是找到 $\\lambda_{\\max}$，即当解为零向量 $\\hat{\\beta} = 0$ 时 $\\lambda \\ge 0$ 的最小值。将 $\\hat{\\beta}=0$ 代入梯度项，得到：\n$$\n-\\frac{1}{n} X^{\\top} (Y - X \\cdot 0) = -\\frac{1}{n} X^{\\top} Y\n$$\n欧几里得范数 $\\|\\beta_g\\|_2$ 在 $\\beta_g = 0$ 处的次微分是 $\\mathbb{R}^{p_g}$ 中的闭单位球：\n$$\n\\partial \\|\\beta_g\\|_2 |_{\\beta_g = 0} = \\{ v_g \\in \\mathbb{R}^{p_g} \\mid \\|v_g\\|_2 \\le 1 \\}\n$$\n为使 $\\hat{\\beta}=0$ 成为解，KKT 条件 $0 \\in \\partial L(0)$ 必须成立。这意味着对于每个组 $g=1, \\dots, G$，必须存在次梯度 $v_g$ 满足 $\\|v_g\\|_2 \\le 1$，使得：\n$$\n0 = -\\frac{1}{n} X^{\\top}Y + \\lambda \\begin{pmatrix} w_1 v_1 \\\\ \\vdots \\\\ w_G v_G \\end{pmatrix}\n$$\n这个向量方程可以分解为针对每个组的一组方程：\n$$\n0 = -\\frac{1}{n} X_g^{\\top}Y + \\lambda w_g v_g \\quad \\text{for } g = 1, \\dots, G\n$$\n对 $v_g$ 进行整理得到：\n$$\nv_g = \\frac{X_g^{\\top}Y}{n \\lambda w_g}\n$$\n这些次梯度有效的条件是 $\\|v_g\\|_2 \\le 1$。应用此约束：\n$$\n\\left\\| \\frac{X_g^{\\top}Y}{n \\lambda w_g} \\right\\|_{2} \\le 1\n$$\n因为 $\\lambda > 0$ 且 $w_g > 0$，我们可以写成：\n$$\n\\frac{\\|X_g^{\\top}Y\\|_2}{n \\lambda w_g} \\le 1\n$$\n这意味着对于每个组 $g$，$\\lambda$ 必须满足：\n$$\n\\lambda \\ge \\frac{\\|X_g^{\\top}Y\\|_2}{n w_g}\n$$\n为使 $\\hat{\\beta}=0$ 成为一个有效解，这个不等式必须对所有组同时成立。因此，$\\lambda$ 必须大于或等于这些下界的最大值。使 $\\hat{\\beta}=0$ 成为解的最小 $\\lambda$ 值就是这个最大值。这定义了 $\\lambda_{\\max}$：\n$$\n\\lambda_{\\max} = \\max_{g \\in \\{1, \\dots, G\\}} \\left\\{ \\frac{\\|X_g^{\\top}Y\\|_2}{n w_g} \\right\\}\n$$\n现在，我们为给定的数据集计算这个值。\n给定条件是：\n$n = 5$\n$Y = (2, -1, 0, 3, -2)^{\\top}$\n组 1：$p_1=2$，$w_1 = \\sqrt{2}$，$X_1 = \\begin{pmatrix} 1  0 \\\\ 0  1 \\\\ 1  0 \\\\ 0  1 \\\\ 1  0 \\end{pmatrix}$\n组 2：$p_2=1$，$w_2 = 1$，$X_2 = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 1 \\\\ 1 \\end{pmatrix}$\n组 3：$p_3=1$，$w_3 = 1$，$X_3 = \\begin{pmatrix} 2 \\\\ -1 \\\\ 0 \\\\ 1 \\\\ 2 \\end{pmatrix}$\n\n我们为每个组计算项 $\\frac{\\|X_g^{\\top}Y\\|_2}{n w_g}$。\n\n对于组 $g=1$：\n$X_1^{\\top}Y = \\begin{pmatrix} 1  0  1  0  1 \\\\ 0  1  0  1  0 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ -1 \\\\ 0 \\\\ 3 \\\\ -2 \\end{pmatrix} = \\begin{pmatrix} 1(2)+0(-1)+1(0)+0(3)+1(-2) \\\\ 0(2)+1(-1)+0(0)+1(3)+0(-2) \\end{pmatrix} = \\begin{pmatrix} 2-2 \\\\ -1+3 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 2 \\end{pmatrix}$\n$\\|X_1^{\\top}Y\\|_2 = \\sqrt{0^2 + 2^2} = 2$\n组 1 的项是 $\\frac{\\|X_1^{\\top}Y\\|_2}{n w_1} = \\frac{2}{5\\sqrt{2}} = \\frac{\\sqrt{2}}{5}$。\n\n对于组 $g=2$：\n$X_2^{\\top}Y = \\begin{pmatrix} 1  1  1  1  1 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ -1 \\\\ 0 \\\\ 3 \\\\ -2 \\end{pmatrix} = 2 - 1 + 0 + 3 - 2 = 2$\n由于该组只有一个预测变量，结果是一个标量。$\\|X_2^{\\top}Y\\|_2 = |2| = 2$。\n组 2 的项是 $\\frac{\\|X_2^{\\top}Y\\|_2}{n w_2} = \\frac{2}{5 \\cdot 1} = \\frac{2}{5}$。\n\n对于组 $g=3$：\n$X_3^{\\top}Y = \\begin{pmatrix} 2  -1  0  1  2 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ -1 \\\\ 0 \\\\ 3 \\\\ -2 \\end{pmatrix} = 2(2) + (-1)(-1) + 0(0) + 1(3) + 2(-2) = 4 + 1 + 0 + 3 - 4 = 4$\n该组也只有一个预测变量，所以结果是一个标量。$\\|X_3^{\\top}Y\\|_2 = |4| = 4$。\n组 3 的项是 $\\frac{\\|X_3^{\\top}Y\\|_2}{n w_3} = \\frac{4}{5 \\cdot 1} = \\frac{4}{5}$。\n\n最后，我们通过取这些值的最大值来找到 $\\lambda_{\\max}$：\n$$\n\\lambda_{\\max} = \\max\\left\\{ \\frac{\\sqrt{2}}{5}, \\frac{2}{5}, \\frac{4}{5} \\right\\}\n$$\n为了比较这些值，我们注意到 $\\sqrt{2}  2  4$。因此，$\\frac{\\sqrt{2}}{5}  \\frac{2}{5}  \\frac{4}{5}$。\n最大值是 $\\frac{4}{5}$。", "answer": "$$\n\\boxed{\\frac{4}{5}}\n$$", "id": "3126744"}, {"introduction": "在模型稀疏性概念的基础上，本练习将引导你在一个简化的正交设定下，推导出组套索问题的完整解路径。你将运用第一性原理，确定当正则化参数 $\\lambda$ 减小时，系数分组被激活进入模型的精确顺序。这项动手分析为你清晰、循序渐进地揭示了组套索方法核心的逐组变量选择机制。[@problem_id:3126799]", "problem": "考虑一个无噪声线性模型中的组套索（Group Lasso）估计量。设设计矩阵为 $X \\in \\mathbb{R}^{6 \\times 6}$，且 $X = I_{6}$，其中 $I_{6}$ 是 $6 \\times 6$ 的单位矩阵。将系数向量 $\\beta \\in \\mathbb{R}^{6}$ 划分为三个不重叠的组：$G_{1} = \\{1\\}$、$G_{2} = \\{2,3\\}$ 和 $G_{3} = \\{4,5,6\\}$。设观测响应为 $y \\in \\mathbb{R}^{6}$，且 $y = (5, 1, -2, 3, 0, 0)^{\\top}$。考虑组套索优化问题\n$$\n\\hat{\\beta}(\\lambda) \\in \\arg\\min_{\\beta \\in \\mathbb{R}^{6}} \\left\\{ \\frac{1}{2} \\| y - X \\beta \\|_{2}^{2} + \\lambda \\sum_{g=1}^{3} \\| \\beta_{G_{g}} \\|_{2} \\right\\},\n$$\n其中 $\\lambda \\geq 0$ 是正则化参数，$\\| \\cdot \\|_{2}$ 表示欧几里得范数。假设所有组的权重均为1。\n\n从 Karush–Kuhn–Tucker (KKT) 条件所定义的最优性基本定义出发，并且不使用任何预先推导出的求解公式，解析地确定 $\\hat{\\beta}(\\lambda)$ 作为 $\\lambda$ 的函数，并为导致不同稀疏模式的每个 $\\lambda$ 范围验证 KKT 条件。利用 $X$ 的正交性和分组结构进行第一性原理推导。\n\n具体来说：\n- 推导此问题的 KKT 条件，并用它们来刻画组 $G_{g}$ 何时是活动的（非零）或非活动的（零）。\n- 当 $\\lambda$ 从一个较大的值减小到 $0$ 时，计算 $\\hat{\\beta}(\\lambda)$ 的分段形式，并以此确定组 $G_{1}$、$G_{2}$ 和 $G_{3}$ 进入模型的顺序。\n- 识别并陈述每个组变为活动状态时的临界正则化参数的精确值。\n\n最后，报告第二个进入模型的组变为活动状态时的第二个临界正则化参数的精确值。请以精确值的形式表达最终答案，无需四舍五入，且不涉及单位。", "solution": "该问题是有效的。这是一个来自统计学习领域的适定（well-posed）优化问题，提供了所有必要的数据和定义。它具有科学依据、客观且自洽。\n\n组套索优化问题由下式给出\n$$ \\hat{\\beta}(\\lambda) \\in \\arg\\min_{\\beta \\in \\mathbb{R}^{6}} \\left\\{ \\frac{1}{2} \\| y - X \\beta \\|_{2}^{2} + \\lambda \\sum_{g=1}^{3} \\| \\beta_{G_{g}} \\|_{2} \\right\\} $$\n设计矩阵是单位矩阵 $X = I_{6}$。这简化了残差平方和（RSS）项：\n$$ \\frac{1}{2} \\| y - I_{6} \\beta \\|_{2}^{2} = \\frac{1}{2} \\| y - \\beta \\|_{2}^{2} = \\frac{1}{2} \\sum_{i=1}^{6} (y_i - \\beta_i)^2 $$\n这些组是不重叠的。这使得目标函数可以分解为针对每个组 $G_g$ 的独立优化问题之和：\n$$ L(\\beta) = \\sum_{g=1}^{3} \\left( \\frac{1}{2} \\| y_{G_g} - \\beta_{G_g} \\|_{2}^{2} + \\lambda \\| \\beta_{G_g} \\|_{2} \\right) $$\n其中 $\\beta_{G_g}$ 和 $y_{G_g}$ 分别是 $\\beta$ 和 $y$ 中对应于组 $G_g$ 内索引的子向量。我们可以通过分别最小化每个组的目标函数来找到全局最小值 $\\hat{\\beta}$。\n\n令 $L_g(\\beta_{G_g}) = \\frac{1}{2} \\| y_{G_g} - \\beta_{G_g} \\|_{2}^{2} + \\lambda \\| \\beta_{G_g} \\|_{2}$。其中 $\\frac{1}{2} \\| y_{G_g} - \\beta_{G_g} \\|_{2}^{2}$ 项是可微的，而 $\\lambda \\| \\beta_{G_g} \\|_{2}$ 是凸的，但在 $\\beta_{G_g}=0$ 处不可微。对于这个凸优化问题，Karush–Kuhn–Tucker (KKT) 条件提供了最优性的充要条件。$L_g$ 的次梯度在最小化点 $\\hat{\\beta}_{G_g}$ 处必须包含零向量：\n$$ \\nabla_{\\beta_{G_g}} \\left( \\frac{1}{2} \\| y_{G_g} - \\hat{\\beta}_{G_g} \\|_{2}^{2} \\right) + \\lambda \\cdot \\partial (\\| \\hat{\\beta}_{G_g} \\|_{2}) \\ni 0 $$\nRSS 项相对于 $\\beta_{G_g}$ 的梯度是 $-(y_{G_g} - \\hat{\\beta}_{G_g})$。欧几里得范数 $\\partial (\\| \\beta_{G_g} \\|_{2})$ 的次微分是：\n$$ \\partial (\\| \\beta_{G_g} \\|_{2}) = \\begin{cases} \\{ \\frac{\\beta_{G_g}}{\\|\\beta_{G_g}\\|_{2}} \\}  \\text{if } \\beta_{G_g} \\neq 0 \\\\ \\{ v_g \\in \\mathbb{R}^{|G_g|} : \\|v_g\\|_{2} \\le 1 \\}  \\text{if } \\beta_{G_g} = 0 \\end{cases} $$\n因此，组 $g$ 的 KKT 条件是：\n$$ -(y_{G_g} - \\hat{\\beta}_{G_g}) + \\lambda v_g = 0 \\quad \\text{where } v_g \\in \\partial (\\| \\hat{\\beta}_{G_g} \\|_{2}) $$\n我们针对解 $\\hat{\\beta}_{G_g}$ 分析两种情况：\n\n情况 1：该组是活动的，$\\hat{\\beta}_{G_g} \\neq 0$。\n在这种情况下，$v_g = \\frac{\\hat{\\beta}_{G_g}}{\\|\\hat{\\beta}_{G_g}\\|_{2}}$。KKT 条件变为：\n$$ -(y_{G_g} - \\hat{\\beta}_{G_g}) + \\lambda \\frac{\\hat{\\beta}_{G_g}}{\\|\\hat{\\beta}_{G_g}\\|_{2}} = 0 $$\n整理各项以求解 $y_{G_g}$：\n$$ y_{G_g} = \\hat{\\beta}_{G_g} + \\lambda \\frac{\\hat{\\beta}_{G_g}}{\\|\\hat{\\beta}_{G_g}\\|_{2}} = \\hat{\\beta}_{G_g} \\left( 1 + \\frac{\\lambda}{\\|\\hat{\\beta}_{G_g}\\|_{2}} \\right) $$\n这表明 $\\hat{\\beta}_{G_g}$ 必须与 $y_{G_g}$ 共线。对两边取欧几里得范数：\n$$ \\|y_{G_g}\\|_{2} = \\left\\| \\hat{\\beta}_{G_g} \\left( 1 + \\frac{\\lambda}{\\|\\hat{\\beta}_{G_g}\\|_{2}} \\right) \\right\\|_{2} = \\|\\hat{\\beta}_{G_g}\\|_{2} \\left( 1 + \\frac{\\lambda}{\\|\\hat{\\beta}_{G_g}\\|_{2}} \\right) = \\|\\hat{\\beta}_{G_g}\\|_{2} + \\lambda $$\n求解估计系数的范数：\n$$ \\|\\hat{\\beta}_{G_g}\\|_{2} = \\|y_{G_g}\\|_{2} - \\lambda $$\n由于范数必须为正，此解仅在 $\\|y_{G_g}\\|_{2} - \\lambda > 0$ 时有效，即 $\\lambda  \\|y_{G_g}\\|_{2}$。如果此条件成立，我们可以将 $\\|\\hat{\\beta}_{G_g}\\|_{2}$ 代回到 $y_{G_g}$ 的方程中：\n$$ y_{G_g} = \\hat{\\beta}_{G_g} \\left( 1 + \\frac{\\lambda}{\\|y_{G_g}\\|_{2} - \\lambda} \\right) = \\hat{\\beta}_{G_g} \\left( \\frac{\\|y_{G_g}\\|_{2} - \\lambda + \\lambda}{\\|y_{G_g}\\|_{2} - \\lambda} \\right) = \\hat{\\beta}_{G_g} \\left( \\frac{\\|y_{G_g}\\|_{2}}{\\|y_{G_g}\\|_{2} - \\lambda} \\right) $$\n求解 $\\hat{\\beta}_{G_g}$：\n$$ \\hat{\\beta}_{G_g} = y_{G_g} \\left( \\frac{\\|y_{G_g}\\|_{2} - \\lambda}{\\|y_{G_g}\\|_{2}} \\right) = \\left( 1 - \\frac{\\lambda}{\\|y_{G_g}\\|_{2}} \\right) y_{G_g} $$\n这就是块软阈值算子。\n\n情况 2：该组是非活动的，$\\hat{\\beta}_{G_g} = 0$。\n在这种情况下，KKT 条件是 $-y_{G_g} + \\lambda v_g = 0$，对于某个满足 $\\|v_g\\|_2 \\le 1$ 的次梯度向量 $v_g$。这意味着 $y_{G_g} = \\lambda v_g$。对两边取范数：\n$$ \\|y_{G_g}\\|_2 = \\|\\lambda v_g\\|_2 = \\lambda \\|v_g\\|_2 $$\n因为 $\\|v_g\\|_2 \\le 1$，所以当且仅当 $\\|y_{G_g}\\|_2 \\le \\lambda$ 时，此式成立。\n\n组活动性总结：一个组 $G_g$ 变为活动的（即 $\\hat{\\beta}_{G_g}$ 变为非零）当且仅当 $\\lambda  \\|y_{G_g}\\|_2$。$\\lambda_{crit}^{(g)} = \\|y_{G_g}\\|_2$ 是组 $G_g$ 的临界正则化参数值。\n\n现在，我们为给定的数据计算这些临界值：\n$y = (5, 1, -2, 3, 0, 0)^{\\top}$，$G_1 = \\{1\\}$，$G_2 = \\{2,3\\}$，$G_3 = \\{4,5,6\\}$。\n\n对于组 $G_1 = \\{1\\}$：\n$y_{G_1} = (5)$。\n$\\|y_{G_1}\\|_2 = |5| = 5$。\n因此，$\\lambda_{crit}^{(1)} = 5$。\n\n对于组 $G_2 = \\{2,3\\}$：\n$y_{G_2} = (1, -2)^{\\top}$。\n$\\|y_{G_2}\\|_2 = \\sqrt{1^2 + (-2)^2} = \\sqrt{1+4} = \\sqrt{5}$。\n因此，$\\lambda_{crit}^{(2)} = \\sqrt{5}$。\n\n对于组 $G_3 = \\{4,5,6\\}$：\n$y_{G_3} = (3, 0, 0)^{\\top}$。\n$\\|y_{G_3}\\|_2 = \\sqrt{3^2 + 0^2 + 0^2} = 3$。\n因此，$\\lambda_{crit}^{(3)} = 3$。\n\n为了确定当 $\\lambda$ 从一个大数值减小时各组进入模型的顺序，我们将临界参数按降序排列：\n1. $\\lambda_1 = 5$ (对于组 $G_1$)\n2. $\\lambda_2 = 3$ (对于组 $G_3$)\n3. $\\lambda_3 = \\sqrt{5} \\approx 2.236$ (对于组 $G_2$)\n\n解路径如下：\n- 对于 $\\lambda \\ge 5$，所有组都是非活动的（$\\hat{\\beta}=0$）。\n- 在 $\\lambda=5$ 时，组 $G_1$ 变为活动状态。这是第一个临界参数，$G_1$ 是第一个进入模型的组。\n- 对于 $3 \\le \\lambda  5$，只有组 $G_1$ 是活动的。\n- 在 $\\lambda=3$ 时，组 $G_3$ 变为活动状态。这是第二个临界参数，$G_3$ 是第二个进入模型的组。\n- 对于 $\\sqrt{5} \\le \\lambda  3$，组 $G_1$ 和 $G_3$ 是活动的。\n- 在 $\\lambda=\\sqrt{5}$ 时，组 $G_2$ 变为活动状态。这是第三个临界参数，$G_2$ 是第三个进入模型的组。\n- 对于 $0 \\le \\lambda  \\sqrt{5}$，所有组都是活动的。\n\n问题要求的是第二个进入模型的组变为活动状态时的第二个临界正则化参数的精确值。根据我们的分析，第二个进入的组是 $G_3$，这发生在 $\\lambda$ 减小到 $3$ 以下时。\n\n因此，第二个临界参数是 $3$。\n\n我们来计算在 $3 \\le \\lambda  5$ 范围内的解 $\\hat{\\beta}(\\lambda)$ 来明确地验证这一点。\n- 组 $G_1$ 是活动的：$\\hat{\\beta}_{G_1} = \\hat{\\beta}_1 = (1 - \\lambda/5) y_1 = (1 - \\lambda/5) \\cdot 5 = 5-\\lambda$。\n- 组 $G_2$ 是非活动的：$\\|y_{G_2}\\|_2=\\sqrt{5}  3 \\le \\lambda$。所以 $\\hat{\\beta}_{G_2}=(0,0)^{\\top}$。\n- 组 $G_3$ 是非活动的：$\\|y_{G_3}\\|_2=3 \\le \\lambda$。所以 $\\hat{\\beta}_{G_3}=(0,0,0)^{\\top}$。\n在边界 $\\lambda=3$ 处：$\\hat{\\beta}_1=2$，所有其他系数均为零。对于任何略小于 $3$ 的 $\\lambda$，比如说 $\\lambda = 3-\\epsilon$（其中 $\\epsilon > 0$ 是一个小数），组 $G_3$ 变为活动的：\n- $\\hat{\\beta}_{G_3} = (1 - \\lambda/3) y_{G_3} = (1 - (3-\\epsilon)/3)(3,0,0)^{\\top} = (\\epsilon/3)(3,0,0)^{\\top} = (\\epsilon,0,0)^{\\top} \\neq 0$。\n这证实了 $\\lambda=3$ 确实是第二个组（$G_3$）变为活动状态时的临界值。", "answer": "$$\\boxed{3}$$", "id": "3126799"}, {"introduction": "从理论走向实践，本练习要求你通过模拟研究来探讨一个关键的实际问题：当预定义的变量分组与数据的真实潜在结构不匹配时，组套索的性能会受到多大影响？你将实现一个近端梯度求解器，并量化因分组错误导致的估计和预测精度损失。此任务不仅突显了领域知识在定义分组时的重要性，也展现了该方法的稳健性。[@problem_id:3126821]", "problem": "您的任务是通过模拟研究在线性回归设定中，错误指定的组对组套索（Group Lasso）估计量的影响，并量化由此产生的性能损失。请在以下纯数学框架下进行操作。\n\n考虑无截距项的线性模型：给定一个设计矩阵 $\\mathbf{X} \\in \\mathbb{R}^{n \\times p}$ 和系数 $\\boldsymbol{\\beta}^{\\star} \\in \\mathbb{R}^{p}$，响应为 $\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta}^{\\star} + \\boldsymbol{\\varepsilon}$，其中噪声 $\\boldsymbol{\\varepsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\sigma^{2}\\mathbf{I}_{n})$。组套索估计量 $\\widehat{\\boldsymbol{\\beta}}$ 求解以下凸优化问题：\n$$\n\\min_{\\boldsymbol{\\beta} \\in \\mathbb{R}^{p}} \\;\\; \\frac{1}{2n}\\lVert \\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta} \\rVert_{2}^{2} \\;+\\; \\lambda \\sum_{g=1}^{m} w_{g} \\lVert \\boldsymbol{\\beta}_{G_{g}} \\rVert_{2},\n$$\n其中 $\\{G_{1},\\dots,G_{m}\\}$ 是将 $\\{1,\\dots,p\\}$ 划分为 $m$ 个组的一个划分，$\\boldsymbol{\\beta}_{G_{g}}$ 表示由组 $G_{g}$ 索引的 $\\boldsymbol{\\beta}$ 的子向量，而 $w_{g} = \\sqrt{|G_{g}|}$ 是组权重。\n\n您的任务是编写一个完整的程序，该程序能够：\n- 根据上述模型模拟数据 $\\mathbf{X}$ 和 $\\mathbf{y}$，其中 $\\boldsymbol{\\beta}^{\\star}$ 的真实支撑集是连续的，并且可以完全包含在一个组内（“正确指定的组”），或者跨越两个相邻的组（“错误指定的组”）。\n- 通过一种有原则的一阶方法求解组套索优化问题，该方法将光滑损失与非光滑惩罚项分开，并对组惩罚项使用适当的近端算子。\n- 在系数估计误差和预测误差两个方面，量化与正确指定的组相比，因错误指定组而导致的性能损失。\n\n您可以假定的基本知识：\n- 欧几里得范数的定义以及凸函数和次梯度的基本性质。\n- 二次损失 $\\frac{1}{2n}\\lVert \\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta} \\rVert_{2}^{2}$ 关于 $\\boldsymbol{\\beta}$ 的梯度等于 $\\frac{1}{n}\\mathbf{X}^{\\top}(\\mathbf{X}\\boldsymbol{\\beta} - \\mathbf{y})$。\n- 对于一个具有利普希茨连续梯度项和可分离非光滑项的复合凸目标函数，使用近端梯度迭代是合适的。\n\n数据生成过程：\n- 按如下方式构造具有相关列的 $\\mathbf{X}$。设 $\\rho \\in [0,1)$，并从独立的标准正态分布中抽取 $\\mathbf{Z} \\in \\mathbb{R}^{n \\times p}$。通过以下方式递归定义各列：\n$$\n\\mathbf{x}_{1} = \\mathbf{z}_{1}, \\quad \\mathbf{x}_{j} = \\rho \\,\\mathbf{x}_{j-1} + \\sqrt{1-\\rho^{2}}\\,\\mathbf{z}_{j} \\;\\; \\text{for} \\;\\; j \\in \\{2,\\dots,p\\}.\n$$\n然后将每列中心化，使其经验均值为 $0$，并进行缩放，使其经验二阶矩为 $1$，即对每个 $j$ 强制满足 $\\frac{1}{n}\\sum_{i=1}^{n} x_{ij}^{2} = 1$。\n- 设 $\\boldsymbol{\\beta}^{\\star}$ 只有一个长度为 $L$ 的连续非零块，从索引 $s$ 开始（索引从0开始），其中每个非零元素都等于一个固定的大小 $b_{0} > 0$，所有其他元素都等于 $0$。\n- 生成 $\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta}^{\\star} + \\boldsymbol{\\varepsilon}$，其中 $\\boldsymbol{\\varepsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\sigma^{2}\\mathbf{I}_{n})$ 且独立于 $\\mathbf{X}$。\n\n组的设定：\n- 错误指定的组：将索引划分为大小固定为 $g_{\\text{mis}} = 5$ 的统一相邻组，形式为 $G^{\\text{mis}}_{1} = \\{0,1,2,3,4\\}$，$G^{\\text{mis}}_{2} = \\{5,6,7,8,9\\}$，依此类推。如果 $p$ 不是 $5$ 的倍数，最后一组可能会更小。\n- 正确指定的组：形成基本大小为 $g_{\\text{cor}} = 5$ 的组，但修改划分，使整个真实非零块 $\\{s, s+1, \\dots, s+L-1\\}$ 作为一个单独的组 $G^{\\text{cor}}_{k}$ 完整出现（即使这偏离了统一划分），剩余的索引被划分为大小最多为 $5$ 的相邻组，不分割任何索引。\n\n每个测试案例需要计算的性能指标：\n- 估计误差：$E(\\widehat{\\boldsymbol{\\beta}}) = \\lVert \\widehat{\\boldsymbol{\\beta}} - \\boldsymbol{\\beta}^{\\star} \\rVert_{2}^{2}$。\n- 样本内预测误差：$P(\\widehat{\\boldsymbol{\\beta}}) = \\frac{1}{n} \\lVert \\mathbf{X}\\widehat{\\boldsymbol{\\beta}} - \\mathbf{X}\\boldsymbol{\\beta}^{\\star} \\rVert_{2}^{2}$。\n- 在每个指标上，因错误指定导致的性能损失定义为错误指定值与正确指定值之间的差。\n\n算法要求：\n- 使用近端梯度法，其恒定步长基于二次损失梯度的有效利普希茨常数 $L_{\\nabla}$。您必须从 $\\mathbf{X}$ 和 $n$ 计算 $L_{\\nabla}$，然后选择一个满足 $0  \\tau \\le 1/L_{\\nabla}$ 的步长 $\\tau$。\n- 对权重为 $w_{g} = \\sqrt{|G_{g}|}$ 的加权组$\\ell_2$惩罚项 $\\sum_{g} w_{g} \\lVert \\boldsymbol{\\beta}_{G_{g}} \\rVert_{2}$ 使用精确的近端算子。\n- 从 $\\boldsymbol{0}$ 初始化，并进行迭代，直到满足相对容差或达到最大迭代次数。\n\n测试套件：\n在以下五个测试案例上实现并运行您的程序。每个元组为 $(\\text{seed}, n, p, \\rho, s, L, b_{0}, \\sigma, \\lambda)$，所有索引都从0开始。\n- 案例 1: $(0, 120, 30, 0.3, 6, 6, 2.0, 0.5, 0.2)$。\n- 案例 2: $(1, 120, 30, 0.3, 6, 6, 0.5, 0.5, 0.2)$。\n- 案例 3: $(2, 120, 30, 0.3, 6, 6, 3.0, 0.0, 0.1)$。\n- 案例 4: $(3, 60, 80, 0.7, 12, 6, 2.0, 0.5, 0.25)$。\n- 案例 5 (边界情况，错误指定分组下无分割): $(4, 120, 30, 0.3, 5, 5, 2.0, 0.5, 0.2)$。\n\n实现细节：\n- 对每个案例，按照规定构造 $\\mathbf{X}$、$\\boldsymbol{\\beta}^{\\star}$ 和 $\\mathbf{y}$，并用给定的随机种子控制所有随机性。\n- 在同一数据上求解组套索问题两次：一次使用错误指定的组，一次使用正确指定的组（两种情况下的权重均为 $w_{g} = \\sqrt{|G_{g}|}$）。\n- 为两次运行计算 $E(\\widehat{\\boldsymbol{\\beta}})$ 和 $P(\\widehat{\\boldsymbol{\\beta}})$，然后计算性能损失如下：\n$$\n\\Delta E = E(\\widehat{\\boldsymbol{\\beta}}_{\\text{mis}}) - E(\\widehat{\\boldsymbol{\\beta}}_{\\text{cor}}), \\quad \\Delta P = P(\\widehat{\\boldsymbol{\\beta}}_{\\text{mis}}) - P(\\widehat{\\boldsymbol{\\beta}}_{\\text{cor}}).\n$$\n\n最终输出格式：\n- 您的程序应生成单行输出，包含一个长度为 $5$ 的列表，其中每个元素是对应测试案例的一个双元素列表 $[\\Delta E, \\Delta P]$。浮点数应精确打印小数点后 $6$ 位。例如：$[[0.123456,0.234567],[\\dots],\\dots]$。\n- 不应打印任何额外文本。\n\n本问题不涉及角度单位。没有需要报告的物理单位。所有百分比（如有）必须表示为小数，但此处不需要。", "solution": "该问题要求进行一项模拟研究，以量化当预定义的组相对于底层信号的真实结构被错误指定时，组套索（Group Lasso）估计量的性能损失。这涉及生成合成数据，为组套索优化问题实现一个鲁棒的求解器，并在两种不同的分组假设下比较结果。\n\n线性模型由 $\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta}^{\\star} + \\boldsymbol{\\varepsilon}$ 给出，其中 $\\mathbf{X} \\in \\mathbb{R}^{n \\times p}$ 是设计矩阵，$\\boldsymbol{\\beta}^{\\star} \\in \\mathbb{R}^{p}$ 是真实系数向量，$\\boldsymbol{\\varepsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\sigma^{2}\\mathbf{I}_{n})$ 是一个独立同分布的高斯噪声向量。\n\n组套索估计量 $\\widehat{\\boldsymbol{\\beta}}$ 通过求解以下凸优化问题得到：\n$$\n\\min_{\\boldsymbol{\\beta} \\in \\mathbb{R}^{p}} \\;\\; \\underbrace{\\frac{1}{2n}\\lVert \\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta} \\rVert_{2}^{2}}_{f(\\boldsymbol{\\beta})} \\;+\\; \\underbrace{\\lambda \\sum_{g=1}^{m} w_{g} \\lVert \\boldsymbol{\\beta}_{G_{g}} \\rVert_{2}}_{g(\\boldsymbol{\\beta})}\n$$\n该目标函数是一个光滑、可微的损失项 $f(\\boldsymbol{\\beta})$（均方误差）和一个非光滑、凸的惩罚项 $g(\\boldsymbol{\\beta})$ 的复合。惩罚项鼓励组级别的稀疏性，即驱使整个子向量 $\\boldsymbol{\\beta}_{G_{g}}$ 变为零。组 $\\{G_{1},\\dots,G_{m}\\}$ 构成了变量索引 $\\{1,\\dots,p\\}$ 的一个划分，权重设为 $w_{g} = \\sqrt{|G_{g}|}$ 以抵消 $\\ell_2$ 范数对组大小的依赖性。\n\n**数据生成**\n对于每个测试案例，模拟首先根据指定的过程构建数据 $(\\mathbf{X}, \\mathbf{y}, \\boldsymbol{\\beta}^{\\star})$。\n1.  **设计矩阵 $\\mathbf{X}$**：为了引入多重共线性（真实世界数据中的一个常见特征），$\\mathbf{X}$ 的列是使用自回归过程生成的。首先，从独立的 $\\mathcal{N}(0, 1)$ 分布中抽取一个矩阵 $\\mathbf{Z} \\in \\mathbb{R}^{n \\times p}$。然后，$\\mathbf{X}$ 的列（表示为 $\\mathbf{x}_{j}$）被定义为 $\\mathbf{x}_{1} = \\mathbf{z}_{1}$ 和 $\\mathbf{x}_{j} = \\rho \\,\\mathbf{x}_{j-1} + \\sqrt{1-\\rho^{2}}\\,\\mathbf{z}_{j}$（对于 $j=2,\\dots,p$）。这创建了一个类托普利茨（Toeplitz-like）相关结构，其中相邻列的相关性更高。最后，每列都被中心化以具有零均值，并被缩放以使其经验二阶矩为1，即 $\\frac{1}{n}\\lVert\\mathbf{x}_j\\rVert_2^2 = 1$。\n2.  **真实系数 $\\boldsymbol{\\beta}^{\\star}$**：真实信号被构造成具有组稀疏性。向量 $\\boldsymbol{\\beta}^{\\star}$ 在各处都为零，除了一个从索引 $s$ 开始、长度为 $L$ 的连续块，其中非零项的值均为 $b_0$。\n3.  **响应向量 $\\mathbf{y}$**：响应通过线性模型 $\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta}^{\\star} + \\boldsymbol{\\varepsilon}$ 生成，其中噪声 $\\boldsymbol{\\varepsilon}$ 从一个方差为 $\\sigma^2$ 的独立同分布高斯分布中抽取。\n\n**组的设定**\n研究的核心在于比较两种组划分：\n1.  **错误指定的组 ($G^{\\text{mis}}$)**：将索引简单、统一地划分为大小固定为 $g_{\\text{mis}}=5$ 的相邻组。这种结构与 $\\boldsymbol{\\beta}^{\\star}$ 的真实支撑集无关，因此可能会将非零块分割到多个组中。\n2.  **正确指定的组 ($G^{\\text{cor}}$)**：一个“神谕”般的划分，其中非零块 $\\{s, s+1, \\dots, s+L-1\\}$ 是已知的，并被定义为一个单独的组。剩余的索引（不在支撑集中的索引）被划分为大小最多为 $5$ 的相邻组。这代表了理想情景，即模型所假设的组结构与信号的真实底层结构相匹配。\n\n**通过近端梯度法进行优化**\n组套索目标函数的复合性质使其适用于近端梯度法（也称为迭代收缩阈值算法或ISTA）。该算法迭代地对光滑部分 $f(\\boldsymbol{\\beta})$ 执行梯度下降步，然后对非光滑部分 $g(\\boldsymbol{\\beta})$ 执行相应的近端映射步。第 $k+1$ 次迭代的更新规则是：\n$$\n\\boldsymbol{\\beta}^{(k+1)} = \\text{prox}_{\\tau g}\\left(\\boldsymbol{\\beta}^{(k)} - \\tau \\nabla f(\\boldsymbol{\\beta}^{(k)})\\right)\n$$\n- **梯度步**：损失项的梯度是 $\\nabla f(\\boldsymbol{\\beta}) = \\frac{1}{n}\\mathbf{X}^{\\top}(\\mathbf{X}\\boldsymbol{\\beta} - \\mathbf{y})$。\n- **步长 $\\tau$**：为保证收敛，步长必须满足 $0  \\tau \\le 1/L_{\\nabla}$，其中 $L_{\\nabla}$ 是 $\\nabla f$ 的利普希茨常数。常数 $L_{\\nabla}$ 是海森矩阵 $\\nabla^2 f(\\boldsymbol{\\beta}) = \\frac{1}{n}\\mathbf{X}^{\\top}\\mathbf{X}$ 的最大特征值，可以计算为 $L_{\\nabla} = \\frac{1}{n} \\sigma_{\\max}^2(\\mathbf{X})$。选择步长为 $\\tau = 1/L_{\\nabla}$。\n- **近端步**：组套索惩罚项 $g(\\boldsymbol{\\beta}) = \\sum_g \\lambda w_g \\lVert\\boldsymbol{\\beta}_{G_g}\\rVert_2$ 的近端算子在各组间是可分的。对于单个组 $G_g$，该算子是块软阈值操作：\n$$\n\\text{prox}_{\\tau\\lambda w_g \\lVert \\cdot \\rVert_2}(\\mathbf{u}) = \\mathbf{u} \\left(1 - \\frac{\\tau\\lambda w_g}{\\lVert \\mathbf{u} \\rVert_2}\\right)_{+}\n$$\n其中 $(\\cdot)_+ = \\max(0, \\cdot)$。此操作将组子向量 $\\mathbf{u}$ 向原点收缩，如果其范数低于阈值 $\\tau\\lambda w_g$，则将其精确地设置为零。整体近端算子将此操作按块应用于其参数的相应子向量。算法以 $\\boldsymbol{\\beta}^{(0)} = \\mathbf{0}$ 初始化，并迭代直到 $\\boldsymbol{\\beta}$ 的相对变化低于指定的容差。\n\n**性能评估**\n对于每个测试案例，组套索问题被求解两次：一次使用错误指定的组得到 $\\widehat{\\boldsymbol{\\beta}}_{\\text{mis}}$，一次使用正确指定的组得到 $\\widehat{\\boldsymbol{\\beta}}_{\\text{cor}}$。性能通过两个指标进行评估：\n1.  **估计误差**：$E(\\widehat{\\boldsymbol{\\beta}}) = \\lVert \\widehat{\\boldsymbol{\\beta}} - \\boldsymbol{\\beta}^{\\star} \\rVert_{2}^{2}$，它衡量估计系数与真实系数的接近程度。\n2.  **样本内预测误差**：$P(\\widehat{\\boldsymbol{\\beta}}) = \\frac{1}{n} \\lVert \\mathbf{X}\\widehat{\\boldsymbol{\\beta}} - \\mathbf{X}\\boldsymbol{\\beta}^{\\star} \\rVert_{2}^{2}$，它衡量在预测无噪声信号分量时的误差。\n\n然后，因错误指定导致的性能损失被计算为这些指标的差值：$\\Delta E = E(\\widehat{\\boldsymbol{\\beta}}_{\\text{mis}}) - E(\\widehat{\\boldsymbol{\\beta}}_{\\text{cor}})$ 和 $\\Delta P = P(\\widehat{\\boldsymbol{\\beta}}_{\\text{mis}}) - P(\\widehat{\\boldsymbol{\\beta}}_{\\text{cor}})$。正的损失值表明，错误指定的分组导致了更差的性能，这正如预期。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import linalg\n\ndef solve():\n    \"\"\"\n    Simulates the effect of mis-specified groups on the Group Lasso estimator.\n\n    For each test case, it:\n    1. Generates correlated data (X, y) based on a true sparse signal (beta_star).\n    2. Defines two group structures: 'mis-specified' (uniform) and 'correctly specified' (oracle).\n    3. Solves the Group Lasso problem for both group structures using a proximal gradient method.\n    4. Computes estimation and prediction errors for both solutions.\n    5. Calculates the performance loss (mis-specified error - correct error).\n    6. Formats and prints the results as specified.\n    \"\"\"\n    \n    # Constants for the proximal gradient solver\n    MAX_ITER = 10000\n    TOL = 1e-6\n\n    def generate_data(seed, n, p, rho, s, L, b0, sigma):\n        \"\"\"Generates data (X, y, beta_star) according to the problem specification.\"\"\"\n        rng = np.random.default_rng(seed)\n\n        # Generate correlated design matrix X\n        Z = rng.standard_normal((n, p))\n        X = np.zeros((n, p))\n        X[:, 0] = Z[:, 0]\n        for j in range(1, p):\n            X[:, j] = rho * X[:, j - 1] + np.sqrt(1 - rho**2) * Z[:, j]\n\n        # Center and scale X to have mean 0 and second moment 1\n        X -= X.mean(axis=0, keepdims=True)\n        scales = np.sqrt(np.mean(X**2, axis=0))\n        scales[scales == 0] = 1.0  # Avoid division by zero\n        X /= scales\n\n        # Generate true coefficient vector beta_star\n        beta_star = np.zeros(p)\n        if L > 0:\n            beta_star[s : s + L] = b0\n\n        # Generate response vector y\n        epsilon = rng.standard_normal(n) * sigma\n        y = X @ beta_star + epsilon\n\n        return X, y, beta_star\n\n    def generate_mis_groups(p, group_size=5):\n        \"\"\"Generates mis-specified groups (uniform partition).\"\"\"\n        groups = []\n        for i in range(0, p, group_size):\n            groups.append(list(range(i, min(i + group_size, p))))\n        return groups\n\n    def generate_cor_groups(p, s, L, group_size=5):\n        \"\"\"Generates correctly specified groups (aligning with true support).\"\"\"\n        if L == 0:\n            return generate_mis_groups(p, group_size)\n\n        groups = []\n        \n        def partition_indices(indices, size):\n            return [indices[i:i + size] for i in range(0, len(indices), size)]\n\n        # Indices before the support block\n        pre_indices = list(range(0, s))\n        if pre_indices:\n            groups.extend(partition_indices(pre_indices, group_size))\n\n        # The support block as a single group\n        groups.append(list(range(s, s + L)))\n\n        # Indices after the support block\n        post_indices = list(range(s + L, p))\n        if post_indices:\n            groups.extend(partition_indices(post_indices, group_size))\n            \n        return groups\n\n    def group_lasso_solver(X, y, lambda_reg, groups):\n        \"\"\"Solves the Group Lasso problem using a proximal gradient method.\"\"\"\n        n, p = X.shape\n        \n        # Calculate Lipschitz constant and step size\n        C = (X.T @ X) / n\n        eigenvalues = linalg.eigvalsh(C)\n        L_nabla = eigenvalues[-1] if len(eigenvalues) > 0 else 1.0\n        \n        tau = 1.0 / L_nabla if L_nabla > 0 else 1.0\n        \n        # Pre-compute weights and thresholds for the proximity operator\n        weights = np.array([np.sqrt(len(g)) for g in groups])\n        thresholds = tau * lambda_reg * weights\n        \n        # Initialize beta and start iteration\n        beta = np.zeros(p)\n        for _ in range(MAX_ITER):\n            beta_old = beta.copy()\n            \n            # Gradient descent step on the smooth part\n            grad = (X.T @ (X @ beta - y)) / n\n            z = beta - tau * grad\n            \n            # Proximal step on the non-smooth part (group-wise soft-thresholding)\n            beta_new = np.zeros(p)\n            for i, g in enumerate(groups):\n                z_g = z[g]\n                norm_z_g = np.linalg.norm(z_g)\n                \n                if norm_z_g > thresholds[i]:\n                    factor = 1 - thresholds[i] / norm_z_g\n                    beta_new[g] = z_g * factor\n                else:\n                    beta_new[g] = 0.0\n            \n            beta = beta_new\n            \n            # Check for convergence\n            rel_diff = np.linalg.norm(beta - beta_old) / (np.linalg.norm(beta_old) + 1e-8)\n            if rel_diff  TOL:\n                break\n                \n        return beta\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (0, 120, 30, 0.3, 6, 6, 2.0, 0.5, 0.2),\n        (1, 120, 30, 0.3, 6, 6, 0.5, 0.5, 0.2),\n        (2, 120, 30, 0.3, 6, 6, 3.0, 0.0, 0.1),\n        (3, 60, 80, 0.7, 12, 6, 2.0, 0.5, 0.25),\n        (4, 120, 30, 0.3, 5, 5, 2.0, 0.5, 0.2),\n    ]\n\n    results = []\n    for case in test_cases:\n        seed, n, p, rho, s, L, b0, sigma, lambda_reg = case\n        \n        X, y, beta_star = generate_data(seed, n, p, rho, s, L, b0, sigma)\n        \n        # Run for mis-specified groups\n        groups_mis = generate_mis_groups(p, group_size=5)\n        beta_hat_mis = group_lasso_solver(X, y, lambda_reg, groups_mis)\n        \n        # Run for correctly specified groups\n        groups_cor = generate_cor_groups(p, s, L, group_size=5)\n        beta_hat_cor = group_lasso_solver(X, y, lambda_reg, groups_cor)\n        \n        # Calculate performance metrics\n        y_true_signal = X @ beta_star\n\n        E_mis = np.sum((beta_hat_mis - beta_star)**2)\n        P_mis = np.sum((X @ beta_hat_mis - y_true_signal)**2) / n\n        \n        E_cor = np.sum((beta_hat_cor - beta_star)**2)\n        P_cor = np.sum((X @ beta_hat_cor - y_true_signal)**2) / n\n\n        # Calculate performance loss\n        delta_E = E_mis - E_cor\n        delta_P = P_mis - P_cor\n        \n        results.append([delta_E, delta_P])\n\n    # Final print statement in the exact required format.\n    output_str_parts = []\n    for dE, dP in results:\n        dE_str = f\"{dE:.6f}\"\n        dP_str = f\"{dP:.6f}\"\n        output_str_parts.append(f\"[{dE_str},{dP_str}]\")\n        \n    final_output = f\"[{','.join(output_str_parts)}]\"\n    print(final_output)\n\nsolve()\n```", "id": "3126821"}]}