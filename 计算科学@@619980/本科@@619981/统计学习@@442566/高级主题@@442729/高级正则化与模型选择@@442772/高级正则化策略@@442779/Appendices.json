{"hands_on_practices": [{"introduction": "标准的 $L_2$ 正则化（岭回归）将系数缩小到零，但这并非总是最合理的先验知识。本练习将探讨如何通过设计一个“零空间惩罚项”来将正则化目标从零点转移到一个代表特定线性不变性的子空间。你将推导该广义岭回归的解，并量化这种结构化正则化对模型有效自由度和泛化能力的影响。[@problem_id:3096585]", "problem": "给定一个线性预测模型，其参数向量为 $w \\in \\mathbb{R}^d$，输入矩阵为 $X \\in \\mathbb{R}^{n \\times d}$，响应为 $y \\in \\mathbb{R}^n$。考虑一个二次目标函数，它结合了经验风险最小化与两个正则化项：一个各向同性 $\\ell_2$ 惩罚项和一个通过矩阵 $A \\in \\mathbb{R}^{k \\times d}$ 强制实现线性不变性的零空间惩罚项。该目标函数为\n$$\nJ(w) \\;=\\; \\frac{1}{n}\\,\\lVert y - Xw \\rVert_2^2 \\;+\\; \\lambda\\,\\lVert w \\rVert_2^2 \\;+\\; \\mu\\,\\lVert Aw \\rVert_2^2,\n$$\n其中 $\\lambda \\ge 0$ 和 $\\mu \\ge 0$ 是超参数。零空间惩罚项鼓励 $w$ 接近 $A$ 的零空间，从而惩罚对指定线性不变性的偏离。\n\n您的任务是：\n- 仅从最小二乘最小化和矩阵微积分的基本定义出发，推导 $J(w)$ 的最小化子 $w^\\star$ 的正规方程，并以线性系统解的形式获得 $w^\\star$ 的闭式解。然后推导相关的帽子矩阵 $H$（它将训练响应 $y$ 映射到其拟合值 $\\hat{y} = X w^\\star$）以及有效自由度 $\\mathrm{df} = \\mathrm{trace}(H)$。\n- 实现一个程序，该程序求解 $w^\\star$，并针对几个指定的超参数和约束矩阵情况，计算所提供测试集上的有效自由度和样本外均方误差。\n\n使用以下固定的训练和测试数据：\n- 训练设计矩阵 $X_{\\mathrm{train}} \\in \\mathbb{R}^{6 \\times 3}$，\n$$\nX_{\\mathrm{train}} \\;=\\;\n\\begin{pmatrix}\n1  0  2 \\\\\n0  1  -1 \\\\\n2  1  0 \\\\\n1  -1  1 \\\\\n3  0  -2 \\\\\n0  2  1\n\\end{pmatrix}.\n$$\n- 训练响应 $y_{\\mathrm{train}} \\in \\mathbb{R}^{6}$，\n$$\ny_{\\mathrm{train}} \\;=\\;\n\\begin{pmatrix}\n0.1 \\\\\n2.8 \\\\\n6.2 \\\\\n-1.0 \\\\\n7.7 \\\\\n3.1\n\\end{pmatrix}.\n$$\n- 测试设计矩阵 $X_{\\mathrm{test}} \\in \\mathbb{R}^{4 \\times 3}$，\n$$\nX_{\\mathrm{test}} \\;=\\;\n\\begin{pmatrix}\n1  1  0 \\\\\n0  1  2 \\\\\n2  -1  1 \\\\\n1  0  -1\n\\end{pmatrix}.\n$$\n- 无噪声的测试响应 $y_{\\mathrm{test}} \\in \\mathbb{R}^{4}$，由真实参数 $w_{\\mathrm{true}} \\in \\mathbb{R}^3$（其分量为 $w_{\\mathrm{true}} = (2,\\,2,\\,-1)$）生成：\n$$\ny_{\\mathrm{test}} \\;=\\; X_{\\mathrm{test}}\\, w_{\\mathrm{true}} \\;=\\;\n\\begin{pmatrix}\n4 \\\\\n0 \\\\\n1 \\\\\n3\n\\end{pmatrix}.\n$$\n\n对于所有测试用例，使用 $\\lambda = 0.1$。定义以下约束设置的测试套件，每个设置由一对 $(\\mu, A)$ 指定：\n- 情况 1：$\\mu = 0$，$A = \\begin{pmatrix} 0  0  0 \\end{pmatrix}$。\n- 情况 2：$\\mu = 50$，$A = \\begin{pmatrix} 1  -1  0 \\end{pmatrix}$，这通过惩罚对由 $Aw = 0$ 定义的零空间的偏离来鼓励不变性 $w_1 = w_2$。\n- 情况 3：$\\mu = 50$，$A = \\begin{pmatrix} 0  1  1 \\end{pmatrix}$，这通过惩罚对由 $Aw = 0$ 定义的零空间的偏离来鼓励不变性 $w_2 = -w_3$。\n\n对于每种情况：\n- 令 $n = 6$ 和 $d = 3$。构造对称正定矩阵\n$$\nM \\;=\\; \\frac{1}{n} X_{\\mathrm{train}}^\\top X_{\\mathrm{train}} \\;+\\; \\lambda I_d \\;+\\; \\mu\\, A^\\top A,\n$$\n以及右端项\n$$\nb \\;=\\; \\frac{1}{n} X_{\\mathrm{train}}^\\top y_{\\mathrm{train}}.\n$$\n- 通过求解线性系统 $M\\,w^\\star = b$ 来计算 $w^\\star$。\n- 计算帽子矩阵\n$$\nH \\;=\\; \\frac{1}{n}\\, X_{\\mathrm{train}}\\, M^{-1}\\, X_{\\mathrm{train}}^\\top,\n$$\n和有效自由度 $\\mathrm{df} = \\mathrm{trace}(H)$。\n- 计算测试均方误差\n$$\n\\mathrm{MSE}_{\\mathrm{test}} \\;=\\; \\frac{1}{m}\\, \\lVert X_{\\mathrm{test}}\\, w^\\star - y_{\\mathrm{test}} \\rVert_2^2,\n$$\n其中 $m = 4$。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表。该列表应按 $[\\mathrm{MSE}_1, \\mathrm{df}_1, \\mathrm{MSE}_2, \\mathrm{df}_2, \\mathrm{MSE}_3, \\mathrm{df}_3]$ 的顺序排列，其中索引 $i$ 对应于上面的情况 $i$。\n- 在打印行中，将所有浮点输出四舍五入到 $6$ 位小数。\n- 要求的最终输出是单行，例如 $[r_1,r_2,r_3,r_4,r_5,r_6]$，并按指定顺序排列。\n\n注意：不涉及角度，也不需要物理单位。所有数值量都应作为标准浮点运算中的实数进行计算。每个测试用例的答案必须按上述规定以浮点数形式报告。", "solution": "我们从线性最小二乘和矩阵微积分的基本设置开始。设 $X \\in \\mathbb{R}^{n \\times d}$，$y \\in \\mathbb{R}^n$，$w \\in \\mathbb{R}^d$。考虑目标函数\n$$\nJ(w) \\;=\\; \\frac{1}{n}\\,\\lVert y - Xw \\rVert_2^2 \\;+\\; \\lambda\\,\\lVert w \\rVert_2^2 \\;+\\; \\mu\\,\\lVert Aw \\rVert_2^2,\n$$\n其中 $\\lambda \\ge 0$，$\\mu \\ge 0$，且 $A \\in \\mathbb{R}^{k \\times d}$。该目标函数等于一个严格凸的二次函数（前提是正则化确保正定性）和非负惩罚项之和。唯一的最小化子通过将梯度设置为零得到。\n\n使用经过充分检验的二次函数梯度的矩阵微积分恒等式，即 $\\nabla_w \\lVert y - Xw \\rVert_2^2 = -2 X^\\top (y - Xw)$，以及 $\\nabla_w \\lVert w \\rVert_2^2 = 2 w$，和 $\\nabla_w \\lVert A w \\rVert_2^2 = 2 A^\\top A w$，我们计算 $J(w)$ 的梯度：\n$$\n\\nabla_w J(w) \\;=\\; -\\frac{2}{n} X^\\top (y - Xw) \\;+\\; 2 \\lambda w \\;+\\; 2 \\mu A^\\top A w.\n$$\n令 $\\nabla_w J(w) = 0$ 并除以 $2$ 得到正规方程\n$$\n\\left(\\frac{1}{n} X^\\top X + \\lambda I_d + \\mu A^\\top A\\right) w \\;=\\; \\frac{1}{n} X^\\top y.\n$$\n定义对称正定矩阵\n$$\nM \\;=\\; \\frac{1}{n} X^\\top X + \\lambda I_d + \\mu A^\\top A,\n$$\n和右端项\n$$\nb \\;=\\; \\frac{1}{n} X^\\top y.\n$$\n然后，通过求解线性系统得到唯一的最小化子\n$$\nM w^\\star \\;=\\; b.\n$$\n除了线性系统解之外，不需要任何快捷公式；一种数值上稳定的方法是使用线性求解器，而不是显式地求矩阵的逆。\n\n接下来，对于拟合值 $\\hat{y} = X w^\\star$，代入 $w^\\star = M^{-1} b$ 得到\n$$\n\\hat{y} \\;=\\; X M^{-1} \\left( \\frac{1}{n} X^\\top y \\right) \\;=\\; \\left( \\frac{1}{n} X M^{-1} X^\\top \\right) y.\n$$\n因此，帽子矩阵为\n$$\nH \\;=\\; \\frac{1}{n} X M^{-1} X^\\top.\n$$\n有效自由度是线性平滑器模型灵活性的一个标准度量，由下式给出\n$$\n\\mathrm{df} \\;=\\; \\mathrm{trace}(H).\n$$\n零空间惩罚项通过加上半正定矩阵 $\\mu A^\\top A$ 来修正 $M$。这会使 $w$ 在与 $A$ 的零空间正交的方向上的分量收缩，从而有效减少这些方向上的方差。当真实参数 $w_{\\mathrm{true}}$ 接近 $A$ 的零空间时，这个惩罚项通过减少方差而不引入太多额外偏差来改善泛化能力。相反，如果 $w_{\\mathrm{true}}$ 严重违反约束 $A w = 0$，该惩罚项会引入偏差，可能损害样本外性能。\n\n计算每个测试用例所需量的算法步骤：\n- 输入 $X_{\\mathrm{train}} \\in \\mathbb{R}^{n \\times d}$，$y_{\\mathrm{train}} \\in \\mathbb{R}^n$，$X_{\\mathrm{test}} \\in \\mathbb{R}^{m \\times d}$，$y_{\\mathrm{test}} \\in \\mathbb{R}^{m}$，超参数 $\\lambda$ 和 $\\mu$，以及约束矩阵 $A \\in \\mathbb{R}^{k \\times d}$。\n- 构造 $S = \\frac{1}{n} X_{\\mathrm{train}}^\\top X_{\\mathrm{train}}$，$M = S + \\lambda I_d + \\mu A^\\top A$ 和 $b = \\frac{1}{n} X_{\\mathrm{train}}^\\top y_{\\mathrm{train}}$。\n- 使用线性求解器求解 $M w^\\star = b$ 以获得 $w^\\star$。\n- 通过求解 $M C = X_{\\mathrm{train}}^\\top$ 来计算 $C = M^{-1} X_{\\mathrm{train}}^\\top$ 而不进行显式求逆；等价地，$C$ 是一个矩阵右端项的解。然后 $H = \\frac{1}{n} X_{\\mathrm{train}} C$ 且 $\\mathrm{df} = \\mathrm{trace}(H)$。\n- 计算 $\\mathrm{MSE}_{\\mathrm{test}} = \\frac{1}{m} \\lVert X_{\\mathrm{test}} w^\\star - y_{\\mathrm{test}} \\rVert_2^2$。\n- 将浮点输出四舍五入到 $6$ 位小数，并按指定顺序 $[\\mathrm{MSE}_1, \\mathrm{df}_1, \\mathrm{MSE}_2, \\mathrm{df}_2, \\mathrm{MSE}_3, \\mathrm{df}_3]$ 打印。\n\n对于提供的测试套件：\n- 情况 1 使用 $\\mu = 0$ 和 $A = (0, 0, 0)$，这简化为 $\\lambda = 0.1$ 的岭回归。这给出了一个小于 $d = 3$ 的基准 $\\mathrm{df}$ 和一个特定的样本外 $\\mathrm{MSE}_{\\mathrm{test}}$。\n- 情况 2 使用 $\\mu = 50$ 和 $A = (1, -1, 0)$，强烈鼓励 $w_1 \\approx w_2$。由于真实情况满足 $w_1 = w_2$，这个惩罚项应该会减少估计量在 $w_1 - w_2$ 方向上的方差，并通常会改善 $\\mathrm{MSE}_{\\mathrm{test}}$，同时进一步减少 $\\mathrm{df}$。\n- 情况 3 使用 $\\mu = 50$ 和 $A = (0, 1, 1)$，强烈鼓励 $w_2 \\approx - w_3$，这与 $w_2 + w_3 = 1$ 的真实情况 $w_{\\mathrm{true}}$ 相冲突。这会引入偏差，可能导致 $\\mathrm{MSE}_{\\mathrm{test}}$ 膨胀，同时由于强惩罚项，$\\mathrm{df}$ 也会有类似的减少。\n\n指定的程序使用线性系统求解器以数值稳定的方式精确地实现了这些计算，确保结果按要求的格式汇总，并按要求四舍五入到 6 位小数。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    # Fixed training data (n=6, d=3)\n    X_train = np.array([\n        [1.0, 0.0, 2.0],\n        [0.0, 1.0, -1.0],\n        [2.0, 1.0, 0.0],\n        [1.0, -1.0, 1.0],\n        [3.0, 0.0, -2.0],\n        [0.0, 2.0, 1.0]\n    ], dtype=float)\n    y_train = np.array([0.1, 2.8, 6.2, -1.0, 7.7, 3.1], dtype=float)\n\n    # Fixed test data (m=4, d=3)\n    X_test = np.array([\n        [1.0, 1.0, 0.0],\n        [0.0, 1.0, 2.0],\n        [2.0, -1.0, 1.0],\n        [1.0, 0.0, -1.0]\n    ], dtype=float)\n    # Ground-truth w_true = (2, 2, -1), so y_test is noise-free\n    y_test = np.array([4.0, 0.0, 1.0, 3.0], dtype=float)\n\n    n, d = X_train.shape\n    m = X_test.shape[0]\n    lam = 0.1  # lambda\n\n    # Test suite of cases: (mu, A)\n    test_cases = [\n        (0.0, np.array([[0.0, 0.0, 0.0]], dtype=float)),     # Case 1\n        (50.0, np.array([[1.0, -1.0, 0.0]], dtype=float)),   # Case 2\n        (50.0, np.array([[0.0, 1.0, 1.0]], dtype=float)),    # Case 3\n    ]\n\n    # Precompute S and b components\n    Xt = X_train.T\n    S = (Xt @ X_train) / n\n    b = (Xt @ y_train) / n\n    I = np.eye(d)\n\n    results = []\n\n    for mu, A in test_cases:\n        # Form M = S + lam*I + mu * A^T A\n        AT_A = A.T @ A\n        M = S + lam * I + mu * AT_A\n\n        # Solve M w = b\n        w_star = np.linalg.solve(M, b)\n\n        # Compute H = (1/n) * X * M^{-1} * X^T without explicit inverse:\n        # Solve M * C = X^T - C = M^{-1} X^T\n        C = np.linalg.solve(M, Xt)  # shape (d, n)\n        H = (X_train @ C) / n\n        df = np.trace(H)\n\n        # Test MSE\n        y_pred_test = X_test @ w_star\n        mse_test = np.mean((y_pred_test - y_test) ** 2)\n\n        # Append rounded results in the specified order\n        results.extend([mse_test, df])\n\n    # Format results to 6 decimal places, no spaces\n    formatted = [f\"{val:.6f}\" for val in results]\n    print(f\"[{','.join(formatted)}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3096585"}, {"introduction": "正则化的思想可以被推广到更复杂的问题结构中，例如时间序列分析。许多动态系统的参数会随时间平滑演变，而不是剧烈跳变。本练习将指导你应用拉普拉斯正则化来对时变线性模型的系数施加平滑性约束，这需要你构建并求解一个大型的块三对角线性系统，并最终用它来识别模型参数发生显著变化的“机制转换”点。[@problem_id:3096605]", "problem": "考虑一个由离散时间 $t \\in \\{1,\\dots,T\\}$ 索引的时变线性回归模型。在每个时间 $t$，我们观察到一个设计矩阵 $X_t \\in \\mathbb{R}^{n_t \\times p}$ 和一个响应向量 $y_t \\in \\mathbb{R}^{n_t}$。假设数据由一个带加性高斯噪声的线性模型生成：$y_t = X_t w_t^\\star + \\varepsilon_t$，其中 $w_t^\\star \\in \\mathbb{R}^p$ 是时间 $t$ 的真实系数向量，而 $\\varepsilon_t \\sim \\mathcal{N}(0, \\sigma^2 I_{n_t})$ 在时间和样本上是独立的。我们希望通过求解一个正则化的最小二乘问题来估计系数序列 $\\{w_t\\}_{t=1}^T$，该问题通过拉普拉斯平滑惩罚项来鼓励系数在时间上的平滑性。具体来说，定义经验风险\n$$\n\\mathcal{R}(w_1,\\dots,w_T) = \\sum_{t=1}^T \\lVert y_t - X_t w_t \\rVert_2^2 + \\lambda \\sum_{t=2}^T \\lVert w_t - w_{t-1} \\rVert_2^2,\n$$\n其中 $\\lambda \\ge 0$ 是一个正则化参数。该惩罚项耦合了相邻的时间点，鼓励较小的离散时间梯度，同时允许在数据需要时发生变化。当离散系数差分的欧几里得范数满足 $\\lVert \\widehat{w}_t - \\widehat{w}_{t-1} \\rVert_2 \\ge \\tau$ 时，我们定义在时间索引 $t \\in \\{2,\\dots,T\\}$ 发生了一次机制转换。其中 $\\tau  0$ 是一个用户选择的阈值，$\\widehat{w}_t$ 是估计出的系数。\n\n您的任务是实现一个程序，对于下述的每个测试用例，该程序在给定高斯模型下生成合成数据，通过求解上述优化问题来计算正则化估计量（该优化问题使用从高斯似然和给定惩罚项导出的一阶最优性条件），然后通过对估计出的离散时间梯度的范数进行阈值化来检测机制转换。\n\n从以下基本原则开始：\n- 在独立同分布噪声下，高斯对数似然与残差平方和的负值成正比。\n- 正则化估计量通过最小化残差平方和加上一个平滑惩罚项来获得。\n- 最小化一个可微凸函数的一阶最优性条件要求其在最小值点的梯度为零。\n- 和的梯度是梯度的和，而欧几里得范数的平方的梯度在其自变量上是线性的。\n\n应用这些原则，推导出刻画最小值点的线性系统，并用算法地使用它来获得估计值 $\\widehat{w}_1,\\dots,\\widehat{w}_T$。然后计算离散差分 $d_t = \\lVert \\widehat{w}_t - \\widehat{w}_{t-1} \\rVert_2$（对于 $t \\in \\{2,\\dots,T\\}$），并且当且仅当 $d_t \\ge \\tau$ 时，宣告在时间 $t$ 发生了一次机制转换。每个测试用例的最终输出是检测到的机制转换索引（在 $\\{2,\\dots,T\\}$ 中的整数）列表，按升序排序。\n\n每个测试用例的数据生成协议：\n- 固定一个伪随机种子 $s$。\n- 对于每个 $t \\in \\{1,\\dots,T\\}$，从独立标准正态分布中抽取 $X_t$ 的元素，即每个元素服从 $\\mathcal{N}(0,1)$ 分布。\n- 根据指定的变更时间和机制系数组向量，构造分段常数的真实系数序列 $\\{w_t^\\star\\}$。\n- 从独立同分布 $\\mathcal{N}(0,\\sigma^2)$ 中抽取噪声 $\\varepsilon_t$，并设置 $y_t = X_t w_t^\\star + \\varepsilon_t$。\n- 通过最小化 $\\mathcal{R}$ 来求解 $\\{\\widehat{w}_t\\}$。\n- 计算 $d_t$ 并进行阈值化以检测机制转换。\n\n您的程序必须为以下测试套件实现上述过程。每个用例都是独立的，并且必须使用其自己的种子。对于每个用例，$n_t = n$ 对所有 $t$ 成立。\n\n- 案例 A (具有明确机制转换的顺利路径):\n  - $s = 31415$, $T = 12$, $p = 3$, $n = 60$, $\\sigma = 0.1$, $\\lambda = 8.0$, $\\tau = 0.9$.\n  - 变更时间：$[5, 9]$，意味着在 $t = 5$ 和 $t = 9$ 时发生变化。\n  - 按顺序排列的机制系数向量：$w^{(1)} = [1.0, 0.0, 0.0]^\\top$, $w^{(2)} = [1.0, 1.5, 0.0]^\\top$, $w^{(3)} = [0.0, 1.5, -1.0]^\\top$.\n\n- 案例 B (具有小幅度变化的过平滑边缘情况):\n  - $s = 27182$, $T = 12$, $p = 3$, $n = 60$, $\\sigma = 0.2$, $\\lambda = 50.0$, $\\tau = 0.4$.\n  - 变更时间：$[7]$，意味着仅在 $t = 7$ 时发生变化。\n  - 按顺序排列的机制系数向量：$w^{(1)} = [0.0, 0.0, 0.0]^\\top$, $w^{(2)} = [0.0, 0.25, 0.0]^\\top$.\n\n- 案例 C (无变化的基线):\n  - $s = 16180$, $T = 10$, $p = 4$, $n = 60$, $\\sigma = 0.1$, $\\lambda = 5.0$, $\\tau = 0.5$.\n  - 变更时间：$[]$，意味着无变化。\n  - 按顺序排列的机制系数向量：$w^{(1)} = [0.5, -0.5, 0.0, 0.0]^\\top$.\n\n- 案例 D (边界邻近的变化):\n  - $s = 14142$, $T = 8$, $p = 2$, $n = 60$, $\\sigma = 0.1$, $\\lambda = 8.0$, $\\tau = 1.0$.\n  - 变更时间：$[2, 7]$，意味着在 $t = 2$ 和 $t = 7$ 时发生变化。\n  - 按顺序排列的机制系数向量：$w^{(1)} = [-1.0, 0.5]^\\top$, $w^{(2)} = [1.0, 0.5]^\\top$, $w^{(3)} = [1.0, -1.5]^\\top$.\n\n变更时间的解释：设变更时间为 $[c_1, c_2, \\dots, c_K]$，其中 $K \\ge 0$。则对于 $t \\in \\{1,\\dots,c_1 - 1\\}$，$w_t^\\star = w^{(1)}$；对于 $t \\in \\{c_1,\\dots,c_2 - 1\\}$，$w_t^\\star = w^{(2)}$，以此类推，对于 $t \\in \\{c_K,\\dots,T\\}$，$w_t^\\star = w^{(K+1)}$。如果 $K = 0$，则对于所有 $t$，$w_t^\\star = w^{(1)}$。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的、无空格的、逗号分隔的列表的列表，其中第 $k$ 个内部列表是第 $k$ 个测试用例检测到的变更索引的升序列表。例如，如果四个案例生成的检测到的变更列表为 $L_1, L_2, L_3, L_4$，则输出必须是 `[L_1,L_2,L_3,L_4]` 的精确格式，其中每个 $L_j$ 打印为 `[i_1,i_2,...]`，并且该行中任何地方都没有空格。", "solution": "该问题是有效的。这是一个统计学习中的适定问题，基于正则化线性回归和凸优化的原理。所有参数和过程都已指定，从而允许一个唯一且可验证的解。\n\n任务是在一个时变线性模型中，通过最小化一个正则化目标函数来估计系数向量序列 $\\{w_t\\}_{t=1}^T$，然后基于估计出的系数来识别机制转换。\n\n需要最小化的目标函数是正则化的经验风险：\n$$\n\\mathcal{R}(w_1,\\dots,w_T) = \\sum_{t=1}^T \\lVert y_t - X_t w_t \\rVert_2^2 + \\lambda \\sum_{t=2}^T \\lVert w_t - w_{t-1} \\rVert_2^2\n$$\n参数 $\\{w_1, \\dots, w_T\\}$（每个都在 $\\mathbb{R}^p$ 中）是优化问题的变量。函数 $\\mathcal{R}$ 是范数平方和，这使其成为所有系数的拼接向量的二次函数。由于它是凸函数的和，$\\mathcal{R}$ 是凸的。在给定的数据生成协议下，其中 $n_t > p$ 并且 $X_t$ 的条目来自连续分布，矩阵 $X_t^\\top X_t$ 以概率1是正定的。这保证了 $\\mathcal{R}$ 是严格凸的，因此存在唯一的最小化子 $\\{\\widehat{w}_1, \\dots, \\widehat{w}_T\\}$。\n\n通过将 $\\mathcal{R}$ 关于每个 $w_k$ 的梯度设置为零来找到最小化子。这是一阶最优性条件。让我们计算 $\\mathcal{R}$ 关于单个系数向量 $w_k$（对于 $k \\in \\{1,\\dots,T\\}$）的偏导数。\n\n最小二乘数据拟合项关于 $w_k$ 的梯度是：\n$$\n\\frac{\\partial}{\\partial w_k} \\left( \\sum_{t=1}^T \\lVert y_t - X_t w_t \\rVert_2^2 \\right) = \\frac{\\partial}{\\partial w_k} \\lVert y_k - X_k w_k \\rVert_2^2 = 2(X_k^\\top X_k w_k - X_k^\\top y_k)\n$$\n\n时间平滑惩罚项的梯度取决于 $w_k$ 是在边界（$k=1$ 或 $k=T$）还是在内部（$k \\in \\{2, \\dots, T-1\\}$）。\n\n对于一个内部时间点 $k \\in \\{2, \\dots, T-1\\}$，$w_k$ 出现在两个惩罚项中：\n$$\n\\frac{\\partial}{\\partial w_k} \\left( \\lambda \\lVert w_k - w_{k-1} \\rVert_2^2 + \\lambda \\lVert w_{k+1} - w_k \\rVert_2^2 \\right) = 2\\lambda(w_k - w_{k-1}) - 2\\lambda(w_{k+1} - w_k) = 2\\lambda(2w_k - w_{k-1} - w_{k+1})\n$$\n\n对于第一个时间点 $k=1$：\n$$\n\\frac{\\partial}{\\partial w_1} \\left( \\lambda \\lVert w_2 - w_1 \\rVert_2^2 \\right) = -2\\lambda(w_2 - w_1) = 2\\lambda(w_1 - w_2)\n$$\n\n对于最后一个时间点 $k=T$：\n$$\n\\frac{\\partial}{\\partial w_T} \\left( \\lambda \\lVert w_T - w_{T-1} \\rVert_2^2 \\right) = 2\\lambda(w_T - w_{T-1})\n$$\n\n将总偏导数 $\\frac{\\partial \\mathcal{R}}{\\partial w_k}$ 设置为零向量并除以 2，得到一个线性方程组：\n\n对于 $k=1$：\n$$(X_1^\\top X_1) w_1 - X_1^\\top y_1 + \\lambda (w_1 - w_2) = 0 \\implies (X_1^\\top X_1 + \\lambda I_p) w_1 - \\lambda I_p w_2 = X_1^\\top y_1$$\n\n对于 $k \\in \\{2, \\dots, T-1\\}$：\n$$(X_k^\\top X_k) w_k - X_k^\\top y_k + \\lambda (2w_k - w_{k-1} - w_{k+1}) = 0 \\implies -\\lambda I_p w_{k-1} + (X_k^\\top X_k + 2\\lambda I_p) w_k - \\lambda I_p w_{k+1} = X_k^\\top y_k$$\n\n对于 $k=T$：\n$$(X_T^\\top X_T) w_T - X_T^\\top y_T + \\lambda (w_T - w_{T-1}) = 0 \\implies -\\lambda I_p w_{T-1} + (X_T^\\top X_T + \\lambda I_p) w_T = X_T^\\top y_T$$\n\n这组 $T$ 个耦合向量方程可以表示为一个单一的大型线性系统 $\\mathcal{H} W = \\mathcal{B}$，其中 $W = [w_1^\\top, \\dots, w_T^\\top]^\\top \\in \\mathbb{R}^{Tp}$ 是所有系数向量的拼接。右侧项是 $\\mathcal{B} = [(X_1^\\top y_1)^\\top, \\dots, (X_T^\\top y_T)^\\top]^\\top \\in \\mathbb{R}^{Tp}$。系统矩阵 $\\mathcal{H} \\in \\mathbb{R}^{Tp \\times Tp}$ 是一个对称块三对角矩阵：\n$$\n\\mathcal{H} = \\begin{pmatrix}\nA_1   C                   \\\\\nC     A_2    C              \\\\\n      \\ddots  \\ddots  \\ddots \\\\\n              C       A_{T-1}  C \\\\\n                      C       A_T\n\\end{pmatrix}\n$$\n其中块是 $p \\times p$ 矩阵，定义如下：\n- 对角块：\n  - $A_1 = X_1^\\top X_1 + \\lambda I_p$\n  - $A_k = X_k^\\top X_k + 2\\lambda I_p$ for $k \\in \\{2, \\dots, T-1\\}$\n  - $A_T = X_T^\\top X_T + \\lambda I_p$\n- 非对角块：\n  - $C = -\\lambda I_p$\n\n求解算法如下：\n1.  对于每个测试用例，根据指定的协议生成数据 $(X_t, y_t)$ (其中 $t=1, \\dots, T$)。这包括使用提供的伪随机种子、维度、噪声水平，以及构造真实的分段常数系数序列 $w_t^\\star$。\n2.  构造如上推导的矩阵 $\\mathcal{H}$ 和向量 $\\mathcal{B}$。\n3.  求解线性系统 $\\mathcal{H} W = \\mathcal{B}$ 以获得估计系数的扁平化向量 $W = \\widehat{W}$。\n4.  将 $\\widehat{W}$ 重塑为估计系数向量序列 $\\{\\widehat{w}_t\\}_{t=1}^T$。\n5.  对于每个时间 $t \\in \\{2, \\dots, T\\}$，计算离散时间差分范数 $d_t = \\lVert \\widehat{w}_t - \\widehat{w}_{t-1} \\rVert_2$。\n6.  如果 $d_t \\ge \\tau$，则在时间 $t$ 检测到一次机制转换。该测试用例的最终输出是所有此类 $t$ 的排序列表。\n为每个提供的测试用例实现此过程。", "answer": "```python\nimport numpy as np\n\ndef process_case(s, T, p, n, sigma, lambda_reg, tau, change_times, regime_vectors):\n    \"\"\"\n    Generates synthetic data, solves the regularized regression problem,\n    and detects regime shifts for a single test case.\n    \"\"\"\n    # 1. Setup random number generator for reproducibility\n    rng = np.random.default_rng(s)\n\n    # 2. Generate the true piecewise-constant coefficient sequence w_star\n    w_star_T = np.zeros((T, p))\n    # Use searchsorted to find which regime each time t belongs to.\n    # The problem uses 1-based indexing for time, so we check against np.arange(1, T + 1).\n    regime_indices = np.searchsorted(change_times, np.arange(1, T + 1), side='right')\n    for t_idx in range(T):\n        regime_idx = regime_indices[t_idx]\n        w_star_T[t_idx, :] = regime_vectors[regime_idx]\n\n    # 3. Generate synthetic data (X_t, y_t) for t=1...T\n    X_T = []\n    y_T = []\n    for t_idx in range(T):\n        X_t = rng.normal(loc=0.0, scale=1.0, size=(n, p))\n        epsilon_t = rng.normal(loc=0.0, scale=sigma, size=n)\n        y_t = X_t @ w_star_T[t_idx, :] + epsilon_t\n        X_T.append(X_t)\n        y_T.append(y_t)\n\n    # 4. Assemble the block-tridiagonal system matrix H and the vector B\n    total_dim = T * p\n    H = np.zeros((total_dim, total_dim))\n    B = np.zeros(total_dim)\n    I_p = np.eye(p)\n\n    # Populate diagonal blocks of H and the vector B\n    for t_idx in range(T):\n        t = t_idx + 1 # 1-based time index\n        start, end = t_idx * p, (t_idx + 1) * p\n        \n        # RHS vector B\n        B[start:end] = X_T[t_idx].T @ y_T[t_idx]\n        \n        # Diagonal blocks of H\n        XtX = X_T[t_idx].T @ X_T[t_idx]\n        if t == 1 or t == T:\n            H[start:end, start:end] = XtX + lambda_reg * I_p\n        else: # 2 = t = T-1\n            H[start:end, start:end] = XtX + 2 * lambda_reg * I_p\n\n    # Populate off-diagonal blocks of H\n    off_diag_block = -lambda_reg * I_p\n    for t_idx in range(T - 1):\n        start1, end1 = t_idx * p, (t_idx + 1) * p\n        start2, end2 = (t_idx + 1) * p, (t_idx + 2) * p\n        H[start1:end1, start2:end2] = off_diag_block\n        H[start2:end2, start1:end1] = off_diag_block\n        \n    # 5. Solve the linear system H * W = B\n    W_hat_flat = np.linalg.solve(H, B)\n    W_hat_T = W_hat_flat.reshape((T, p))\n    \n    # 6. Detect regime shifts by thresholding norms of differences\n    shifts = []\n    for t_idx in range(1, T): # Corresponds to t=2...T\n        w_curr = W_hat_T[t_idx, :]\n        w_prev = W_hat_T[t_idx - 1, :]\n        diff_norm = np.linalg.norm(w_curr - w_prev)\n        if diff_norm >= tau:\n            # Append the 1-based time index\n            shifts.append(t_idx + 1)\n            \n    return shifts\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the final result.\n    \"\"\"\n    test_cases = [\n        # Case A\n        {'s': 31415, 'T': 12, 'p': 3, 'n': 60, 'sigma': 0.1, 'lambda_reg': 8.0, 'tau': 0.9,\n         'change_times': [5, 9], \n         'regime_vectors': [np.array([1.0, 0.0, 0.0]), np.array([1.0, 1.5, 0.0]), np.array([0.0, 1.5, -1.0])]},\n        # Case B\n        {'s': 27182, 'T': 12, 'p': 3, 'n': 60, 'sigma': 0.2, 'lambda_reg': 50.0, 'tau': 0.4,\n         'change_times': [7], \n         'regime_vectors': [np.array([0.0, 0.0, 0.0]), np.array([0.0, 0.25, 0.0])]},\n        # Case C\n        {'s': 16180, 'T': 10, 'p': 4, 'n': 60, 'sigma': 0.1, 'lambda_reg': 5.0, 'tau': 0.5,\n         'change_times': [], \n         'regime_vectors': [np.array([0.5, -0.5, 0.0, 0.0])]},\n        # Case D\n        {'s': 14142, 'T': 8, 'p': 2, 'n': 60, 'sigma': 0.1, 'lambda_reg': 8.0, 'tau': 1.0,\n         'change_times': [2, 7], \n         'regime_vectors': [np.array([-1.0, 0.5]), np.array([1.0, 0.5]), np.array([1.0, -1.5])]},\n    ]\n    \n    results = []\n    for case in test_cases:\n        detected_shifts = process_case(**case)\n        results.append(detected_shifts)\n    \n    # Format the output string precisely as required: [[...],[...],...] with no spaces.\n    inner_parts = []\n    for res_list in results:\n        # For each list, create the string representation like '[5,9]'\n        inner_str = '[' + ','.join(map(str, res_list)) + ']'\n        inner_parts.append(inner_str)\n    \n    final_output = '[' + ','.join(inner_parts) + ']'\n    print(final_output)\n\nsolve()\n```", "id": "3096605"}, {"introduction": "在深度学习领域，Dropout 是一种广泛应用的随机正则化技术，它在训练时以一定概率随机“丢弃”神经元。表面上看，这似乎只是一种启发式技巧，但其背后有深刻的数学原理。本练习将引导你通过分析平方损失下的期望风险，揭示 Dropout 与一种数据自适应的 $L_2$ 正则化之间的等价关系，从而加深对这种随机正则化方法的理解。[@problem_id:3096661]", "problem": "考虑一个监督学习问题，其输入向量为 $\\mathbf{x} \\in \\mathbb{R}^d$，标量目标为 $y \\in \\mathbb{R}$。设带有平方损失的经验风险为 $\\frac{1}{n}\\sum_{i=1}^n \\left(y_i - f(\\mathbf{x}_i)\\right)^2$，其中 $f$ 是一个模型。考虑一个具有 $h$ 个隐藏单元的单隐藏层网络，其隐藏激活为 $\\mathbf{a}(\\mathbf{x}) = \\phi(W\\mathbf{x}) \\in \\mathbb{R}^h$，输出为 $f(\\mathbf{x}) = \\mathbf{v}^\\top \\mathbf{a}(\\mathbf{x})$，其中 $\\phi$ 是一个逐点非线性函数，$W \\in \\mathbb{R}^{h \\times d}$ 是输入到隐藏层的权重，$\\mathbf{v} \\in \\mathbb{R}^h$ 是隐藏层到输出层的权重。权重衰减（Weight decay）是指向经验风险中添加一个显式的欧几里得范数（$\\ell_2$）惩罚项 $\\lambda \\|\\mathbf{v}\\|_2^2$，其中 $\\lambda  0$。Dropout 是指在训练期间应用于激活值的乘性伯努利掩码。具体来说，假设使用反向缩放（inverted scaling）将 dropout 应用于隐藏激活，保留概率为 $p \\in (0,1]$：$\\tilde{\\mathbf{a}}(\\mathbf{x}) = \\left(\\mathbf{m}/p\\right) \\odot \\mathbf{a}(\\mathbf{x})$，其中 $\\mathbf{m} \\in \\{0,1\\}^h$ 的分量 $m_j \\sim \\mathrm{Bernoulli}(p)$ 相互独立，$\\odot$ 表示逐元素乘法。在 dropout 下的输出是 $\\tilde{f}(\\mathbf{x}) = \\mathbf{v}^\\top \\tilde{\\mathbf{a}}(\\mathbf{x})$。假设 dropout 掩码在单元和样本之间是标准独立的，并考虑关于 dropout 随机性的期望经验风险。此外，为了解释浅层网络中正则化效应的可解释性，我们对隐藏层在当前参数周围进行局部线性化，使得在样本 $\\{\\mathbf{x}_i\\}_{i=1}^n$ 上，每个隐藏激活 $a_j(\\mathbf{x}_i)$ 相对于 dropout 噪声可以被视为一个固定值，并且 $\\phi(W\\mathbf{x})$ 关于 $\\mathbf{x}$ 的雅可比矩阵是有界的，从而使得 dropout 噪声中的二阶项主导正则化效应。\n\n基于这些假设，并从经验风险、平方损失和带有反向缩放的 dropout 的构造出发，分析带有隐藏单元 dropout 的期望经验风险与无 dropout 风险的比较，以及这种比较与显式欧几里得范数（$\\ell_2$）权重衰减的关系。然后确定以下哪些陈述是正确的：\n\nA. 在隐藏单元上使用保留概率为 $p$ 的反向 dropout 时，只要 dropout 掩码是独立的并且隐藏激活相对于 dropout 被视为固定的，那么带有平方损失的期望经验风险就等于无 dropout 的经验风险加上一个形式为 $\\frac{1-p}{p} \\sum_{j=1}^h v_j^2 \\cdot \\frac{1}{n}\\sum_{i=1}^n a_j(\\mathbf{x}_i)^2$ 的附加项。\n\nB. 如果 $\\lambda = \\frac{1-p}{p}$，那么对于任何数据集和架构，系数为 $\\lambda$ 的显式权重衰减都与隐藏单元 dropout 完全等价，从而产生相同的优化轨迹和解。\n\nC. 由隐藏单元反向缩放 dropout 引起的正则化取决于隐藏激活的经验二阶矩，因此是数据依赖的，实际上它充当了一种自适应欧几里得范数（$\\ell_2$）惩罚，对连接到高方差隐藏单元的权重进行更强的收缩。\n\nD. 在相同假设下，隐藏单元 dropout 会对 $\\mathbf{v}$ 产生一个欧几里得范数（$\\ell_1$）惩罚，而不是一个欧几里得范数（$\\ell_2$）惩罚。\n\nE. 如果在一个具有平方损失的线性回归模型 $f(\\mathbf{x}) = \\mathbf{w}^\\top \\mathbf{x}$ 中，将保留概率为 $p$ 且使用反向缩放的 dropout 应用于输入，那么期望经验风险等于无 dropout 风险加上 $\\frac{1-p}{p} \\sum_{j=1}^d w_j^2 \\cdot \\frac{1}{n}\\sum_{i=1}^n x_{ij}^2$，其中 $x_{ij}$ 是 $\\mathbf{x}_i$ 的第 $j$ 个特征。", "solution": "用户希望对一个关于神经网络中 dropout 正则化性质的问题进行严格的验证和求解。\n\n### 问题验证\n\n**步骤 1：提取已知条件**\n\n*   **输入/输出：** 输入向量 $\\mathbf{x} \\in \\mathbb{R}^d$，标量目标 $y \\in \\mathbb{R}$。\n*   **数据集：** $n$ 个独立同分布的样本 $(\\mathbf{x}_i, y_i)$。\n*   **经验风险：** $L_{emp} = \\frac{1}{n}\\sum_{i=1}^n \\left(y_i - f(\\mathbf{x}_i)\\right)^2$ (平方损失)。\n*   **模型：** 单隐藏层网络。\n    *   隐藏层激活：$\\mathbf{a}(\\mathbf{x}) = \\phi(W\\mathbf{x}) \\in \\mathbb{R}^h$。\n    *   输出：$f(\\mathbf{x}) = \\mathbf{v}^\\top \\mathbf{a}(\\mathbf{x})$。\n    *   参数：$W \\in \\mathbb{R}^{h \\times d}$，$\\mathbf{v} \\in \\mathbb{R}^h$。\n*   **权重衰减：** 向经验风险添加的惩罚项 $\\lambda \\|\\mathbf{v}\\|_2^2$，其中 $\\lambda  0$。\n*   **Dropout：**\n    *   应用于隐藏激活 $\\mathbf{a}(\\mathbf{x})$。\n    *   Dropout 掩码：$\\mathbf{m} \\in \\{0,1\\}^h$，其独立分量 $m_j \\sim \\mathrm{Bernoulli}(p)$。\n    *   保留概率：$p \\in (0,1]$。\n    *   反向缩放：$\\tilde{\\mathbf{a}}(\\mathbf{x}) = \\left(\\mathbf{m}/p\\right) \\odot \\mathbf{a}(\\mathbf{x})$。\n    *   带 dropout 的输出：$\\tilde{f}(\\mathbf{x}) = \\mathbf{v}^\\top \\tilde{\\mathbf{a}}(\\mathbf{x})$。\n*   **假设：**\n    1.  目标是分析关于 dropout 随机性的经验风险的期望 $\\mathbb{E}_{\\mathbf{m}}[L_{emp}(\\tilde{f})]$。\n    2.  Dropout 掩码 $\\mathbf{m}$ 在单元和样本之间是独立的。\n    3.  在分析中，隐藏激活 $a_j(\\mathbf{x}_i)$ 相对于 dropout 噪声被视为固定值。\n    4.  问题背景中提到的局部线性化和二阶项的主导作用，为假设 3 提供了理由。\n\n**步骤 2：使用提取的已知条件进行验证**\n\n*   **科学基础扎实：** 该问题牢固地植根于统计学习和深度学习理论。Dropout 和权重衰减是标准的正则化技术。通过分析 dropout 下的期望损失来理解其正则化效果是该领域一个经典且成熟的程序。\n*   **定义明确：** 该问题要求进行具体的推导并基于该推导评估陈述。所提供的假设（例如，将激活视为相对于 dropout 噪声是固定的）是此类分析的标准假设，并使问题可解。\n*   **客观：** 问题使用精确的数学定义进行陈述，避免了任何主观或模糊的语言。\n\n**步骤 3：结论与行动**\n\n问题陈述科学合理、定义明确且客观。它提供了一套进行严格数学分析所需的完整定义和假设。该问题是**有效的**。我们开始求解。\n\n### 推导\n\n目标是计算 dropout 下的期望经验风险 $\\mathbb{E}_{\\mathbf{m}}[\\tilde{L}_{emp}]$，并将其与带有权重衰减的标准经验风险进行比较。带 dropout 的经验风险是 $\\tilde{L}_{emp} = \\frac{1}{n}\\sum_{i=1}^n (y_i - \\tilde{f}(\\mathbf{x}_i))^2$。\n\n根据期望的线性性质，我们可以先分析单个样本 $i$ 的期望损失，然后求平均。\n样本 $i$ 的期望损失为 $\\mathbb{E}_{\\mathbf{m}}[(y_i - \\tilde{f}(\\mathbf{x}_i))^2]$。\n令 $Z = \\tilde{f}(\\mathbf{x}_i)$。由于 $y_i$ 相对于 dropout 随机性 $\\mathbf{m}$ 是一个常数，我们可以使用性质 $\\mathbb{E}[(c - Z)^2] = (c - \\mathbb{E}[Z])^2 + \\mathrm{Var}(Z)$。\n这里，$c = y_i$，期望和方差都是关于 $\\mathbf{m}$ 的。\n\n1.  **计算 $\\mathbb{E}_{\\mathbf{m}}[\\tilde{f}(\\mathbf{x}_i)]$：**\n    带 dropout 的输出是 $\\tilde{f}(\\mathbf{x}_i) = \\mathbf{v}^\\top \\tilde{\\mathbf{a}}(\\mathbf{x}_i) = \\sum_{j=1}^h v_j \\tilde{a}_j(\\mathbf{x}_i)$。\n    受 dropout 影响的激活是 $\\tilde{a}_j(\\mathbf{x}_i) = \\frac{m_j}{p} a_j(\\mathbf{x}_i)$，其中 $m_j \\sim \\mathrm{Bernoulli}(p)$。\n    $m_j$ 的期望是 $\\mathbb{E}[m_j] = p$。\n    将 $v_j$ 和 $a_j(\\mathbf{x}_i)$ 视为相对于 $\\mathbf{m}$ 的常数：\n    $$ \\mathbb{E}_{\\mathbf{m}}[\\tilde{f}(\\mathbf{x}_i)] = \\mathbb{E}_{\\mathbf{m}} \\left[ \\sum_{j=1}^h v_j \\frac{m_j}{p} a_j(\\mathbf{x}_i) \\right] = \\sum_{j=1}^h v_j \\frac{\\mathbb{E}[m_j]}{p} a_j(\\mathbf{x}_i) = \\sum_{j=1}^h v_j \\frac{p}{p} a_j(\\mathbf{x}_i) = \\sum_{j=1}^h v_j a_j(\\mathbf{x}_i) = f(\\mathbf{x}_i) $$\n    这证实了由于使用了反向缩放，dropout 下的期望输出就是原始的、无 dropout 的输出。\n\n2.  **计算 $\\mathrm{Var}_{\\mathbf{m}}[\\tilde{f}(\\mathbf{x}_i)]$：**\n    $$ \\mathrm{Var}_{\\mathbf{m}}[\\tilde{f}(\\mathbf{x}_i)] = \\mathrm{Var}_{\\mathbf{m}} \\left[ \\sum_{j=1}^h v_j \\tilde{a}_j(\\mathbf{x}_i) \\right] $$\n    对于不同的单元 $j$，dropout 掩码 $m_j$ 是独立的。因此，随机变量 $\\tilde{a}_j(\\mathbf{x}_i)$ 是独立的。对于独立随机变量的和，和的方差等于方差的和：\n    $$ \\mathrm{Var}_{\\mathbf{m}}[\\tilde{f}(\\mathbf{x}_i)] = \\sum_{j=1}^h \\mathrm{Var}_{\\mathbf{m}}[v_j \\tilde{a}_j(\\mathbf{x}_i)] = \\sum_{j=1}^h v_j^2 \\mathrm{Var}_{\\mathbf{m}}[\\tilde{a}_j(\\mathbf{x}_i)] $$\n    现在我们计算单个受 dropout 影响的激活的方差：\n    $$ \\mathrm{Var}_{\\mathbf{m}}[\\tilde{a}_j(\\mathbf{x}_i)] = \\mathrm{Var}_{\\mathbf{m}}\\left[\\frac{m_j}{p} a_j(\\mathbf{x}_i)\\right] = \\left(\\frac{a_j(\\mathbf{x}_i)}{p}\\right)^2 \\mathrm{Var}(m_j) $$\n    伯努利变量的方差是 $\\mathrm{Var}(m_j) = p(1-p)$。\n    $$ \\mathrm{Var}_{\\mathbf{m}}[\\tilde{a}_j(\\mathbf{x}_i)] = \\frac{a_j(\\mathbf{x}_i)^2}{p^2} p(1-p) = \\frac{1-p}{p} a_j(\\mathbf{x}_i)^2 $$\n    将其代回 $\\tilde{f}(\\mathbf{x}_i)$ 的方差表达式中：\n    $$ \\mathrm{Var}_{\\mathbf{m}}[\\tilde{f}(\\mathbf{x}_i)] = \\sum_{j=1}^h v_j^2 \\left( \\frac{1-p}{p} a_j(\\mathbf{x}_i)^2 \\right) = \\frac{1-p}{p} \\sum_{j=1}^h v_j^2 a_j(\\mathbf{x}_i)^2 $$\n\n3.  **组合期望损失：**\n    将各部分组合起来，样本 $i$ 的期望损失是：\n    $$ \\mathbb{E}_{\\mathbf{m}}[(y_i - \\tilde{f}(\\mathbf{x}_i))^2] = (y_i - \\mathbb{E}_{\\mathbf{m}}[\\tilde{f}(\\mathbf{x}_i)])^2 + \\mathrm{Var}_{\\mathbf{m}}[\\tilde{f}(\\mathbf{x}_i)] = (y_i - f(\\mathbf{x}_i))^2 + \\frac{1-p}{p} \\sum_{j=1}^h v_j^2 a_j(\\mathbf{x}_i)^2 $$\n\n4.  **计算期望经验风险：**\n    最后，我们在所有 $n$ 个样本上取平均：\n    $$ \\mathbb{E}_{\\mathbf{m}}[\\tilde{L}_{emp}] = \\frac{1}{n} \\sum_{i=1}^n \\mathbb{E}_{\\mathbf{m}}[(y_i - \\tilde{f}(\\mathbf{x}_i))^2] $$\n    $$ \\mathbb{E}_{\\mathbf{m}}[\\tilde{L}_{emp}] = \\frac{1}{n} \\sum_{i=1}^n \\left( (y_i - f(\\mathbf{x}_i))^2 + \\frac{1-p}{p} \\sum_{j=1}^h v_j^2 a_j(\\mathbf{x}_i)^2 \\right) $$\n    $$ \\mathbb{E}_{\\mathbf{m}}[\\tilde{L}_{emp}] = \\left( \\frac{1}{n} \\sum_{i=1}^n (y_i - f(\\mathbf{x}_i))^2 \\right) + \\frac{1}{n} \\sum_{i=1}^n \\left( \\frac{1-p}{p} \\sum_{j=1}^h v_j^2 a_j(\\mathbf{x}_i)^2 \\right) $$\n    重新排列第二项中的求和顺序：\n    $$ \\mathbb{E}_{\\mathbf{m}}[\\tilde{L}_{emp}] = \\underbrace{\\frac{1}{n} \\sum_{i=1}^n (y_i - f(\\mathbf{x}_i))^2}_{\\text{无 dropout 经验风险}} + \\underbrace{\\frac{1-p}{p} \\sum_{j=1}^h v_j^2 \\left( \\frac{1}{n} \\sum_{i=1}^n a_j(\\mathbf{x}_i)^2 \\right)}_{\\text{正则化项}} $$\n\n### 逐项分析\n\n**A. 在隐藏单元上使用保留概率为 $p$ 的反向 dropout 时，只要 dropout 掩码是独立的并且隐藏激活相对于 dropout 被视为固定的，那么带有平方损失的期望经验风险就等于无 dropout 的经验风险加上一个形式为 $\\frac{1-p}{p} \\sum_{j=1}^h v_j^2 \\cdot \\frac{1}{n}\\sum_{i=1}^n a_j(\\mathbf{x}_i)^2$ 的附加项。**\n\n我们推导出的结果是，带 dropout 的期望经验风险是无 dropout 的经验风险与一个正则化项 $\\frac{1-p}{p} \\sum_{j=1}^h v_j^2 \\left(\\frac{1}{n}\\sum_{i=1}^n a_j(\\mathbf{x}_i)^2\\right)$ 的和。这与该选项中陈述的形式完全匹配。\n**结论：正确。**\n\n**B. 如果 $\\lambda = \\frac{1-p}{p}$，那么对于任何数据集和架构，系数为 $\\lambda$ 的显式权重衰减都与隐藏单元 dropout 完全等价，从而产生相同的优化轨迹和解。**\n\n权重衰减的总目标函数是：$L_{WD} = \\left(\\frac{1}{n}\\sum_i (y_i - f(\\mathbf{x}_i))^2\\right) + \\lambda \\sum_{j=1}^h v_j^2$。\nDropout 的期望目标函数是：$\\mathbb{E}_{\\mathbf{m}}[\\tilde{L}_{emp}] = \\left(\\frac{1}{n}\\sum_i (y_i - f(\\mathbf{x}_i))^2\\right) + \\sum_{j=1}^h \\left(\\frac{1-p}{p} \\left(\\frac{1}{n}\\sum_i a_j(\\mathbf{x}_i)^2\\right)\\right) v_j^2$。\n要使这两者等价，我们需要对每个 $j$ 都有 $\\lambda = \\frac{1-p}{p} \\left(\\frac{1}{n}\\sum_i a_j(\\mathbf{x}_i)^2\\right)$。这并不是一个单一的标量 $\\lambda$。每个权重 $v_j$ 的缩放因子取决于相应激活 $a_j$ 的经验二阶矩。这个因子是数据依赖的，并且在训练过程中随着权重 $W$ 的演变而变化。因此，dropout 不等同于标准的权重衰减。此外，使用 dropout 进行训练在每一步优化的是一个随机目标，而不是期望目标，这会导致不同的梯度更新和优化轨迹。因此，完全等价的说法是错误的。\n**结论：不正确。**\n\n**C. 由隐藏单元反向缩放 dropout 引起的正则化取决于隐藏激活的经验二阶矩，因此是数据依赖的，实际上它充当了一种自适应欧几里得范数（$\\ell_2$）惩罚，对连接到高方差隐藏单元的权重进行更强的收缩。**\n\n从我们的推导中，权重 $v_j$ 的正则化项与 $v_j^2$ 成正比，并按因子 $\\frac{1-p}{p} \\left(\\frac{1}{n}\\sum_{i=1}^n a_j(\\mathbf{x}_i)^2\\right)$ 进行缩放。项 $\\frac{1}{n}\\sum_{i=1}^n a_j(\\mathbf{x}_i)^2$ 是第 $j$ 个隐藏单元激活在数据集上的经验二阶矩。由于 $a_j(\\mathbf{x}_i)$ 依赖于输入数据 $\\mathbf{x}_i$，因此正则化是数据依赖的。它对那些相应隐藏单元 $a_j$ 具有较大平均平方激活的权重 $v_j$ 施加更强的惩罚（即更大的有效 $\\lambda_j$）。大的二阶矩通常与高方差相关（因为 $\\mathrm{Var}(X) = \\mathbb{E}[X^2] - (\\mathbb{E}[X])^2 \\geq 0$），所以定性描述是准确的。这正是一种自适应 $\\ell_2$ 惩罚的定义。\n**结论：正确。**\n\n**D. 在相同假设下，隐藏单元 dropout 会对 $\\mathbf{v}$ 产生一个欧几里得范数（$\\ell_1$）惩罚，而不是一个欧几里得范数（$\\ell_2$）惩罚。**\n\n正则化项是 $\\frac{1-p}{p} \\sum_{j=1}^h v_j^2 \\left( \\frac{1}{n} \\sum_{i=1}^n a_j(\\mathbf{x}_i)^2 \\right)$。这是一个权重的平方 $v_j^2$ 的加权和。基于平方值的惩罚是 $\\ell_2$ 惩罚（例如，$\\|\\mathbf{v}\\|_2^2 = \\sum_j v_j^2$）。$\\ell_1$ 惩罚将基于绝对值的和，即 $\\sum_j c_j |v_j|$。所产生的惩罚显然是一种 $\\ell_2$ 正则化，而不是 $\\ell_1$。\n**结论：不正确。**\n\n**E. 如果在一个具有平方损失的线性回归模型 $f(\\mathbf{x}) = \\mathbf{w}^\\top \\mathbf{x}$ 中，将保留概率为 $p$ 且使用反向缩放的 dropout 应用于输入，那么期望经验风险等于无 dropout 风险加上 $\\frac{1-p}{p} \\sum_{j=1}^d w_j^2 \\cdot \\frac{1}{n}\\sum_{i=1}^n x_{ij}^2$，其中 $x_{ij}$ 是 $\\mathbf{x}_i$ 的第 $j$ 个特征。**\n\n这种情况是这个一般问题的一个特例。我们可以将线性回归模型映射到我们的单隐藏层网络，方法是设置隐藏单元数 $h$ 为输入维度 $d$，隐藏层到输出层的权重 $\\mathbf{v}$ 为线性权重 $\\mathbf{w}$，隐藏激活 $\\mathbf{a}(\\mathbf{x})$ 为输入特征 $\\mathbf{x}$（即 $\\phi$ 是恒等函数，并且 $W$ 是单位矩阵，假设 $h=d$）。Dropout 应用于输入 $\\mathbf{x}$，这对应于在此类比中将其应用于“激活”。\n直接代入我们的一般结果：\n$$ \\mathbb{E}_{\\mathbf{m}}[L_{emp}] = (\\text{无 dropout 风险}) + \\frac{1-p}{p} \\sum_{j=1}^d w_j^2 \\left( \\frac{1}{n} \\sum_{i=1}^n x_{ij}^2 \\right) $$\n这个表达式与该选项中的陈述完全匹配。\n**结论：正确。**", "answer": "$$\\boxed{ACE}$$", "id": "3096661"}]}