{"hands_on_practices": [{"introduction": "理论是实践的基石。在深入研究自适应Lasso的算法实现之前，通过一个简单的双变量例子来分析其核心思想至关重要。这个练习将引导你运用Karush-Kuhn–Tucker (KKT) 条件，亲手推导并比较标准Lasso和自适应Lasso在特征选择上的差异，从而直观地理解自适应权重是如何帮助模型做出更明智的决策的 [@problem_id:3095616]。", "problem": "考虑一个线性回归模型，其中有 $n=2$ 个观测值和 $p=2$ 个预测变量，设计矩阵 $X \\in \\mathbb{R}^{2 \\times 2}$ 的列经过标准化，使得样本格拉姆矩阵满足\n$$\n\\frac{1}{n} X^{\\top} X = \\begin{pmatrix} 1 & \\rho \\\\ \\rho & 1 \\end{pmatrix},\n$$\n相关水平为 $\\rho = 0.95$。一个明确的构造是\n$$\nX_1 = \\begin{pmatrix} \\sqrt{2} \\\\ 0 \\end{pmatrix}, \\quad X_2 = \\begin{pmatrix} \\sqrt{2} \\rho \\\\ \\sqrt{2} \\sqrt{1-\\rho^2} \\end{pmatrix},\n$$\n使得 $\\frac{1}{n} X_1^{\\top} X_1 = 1$, $\\frac{1}{n} X_2^{\\top} X_2 = 1$, 且 $\\frac{1}{n} X_1^{\\top} X_2 = \\rho$。设响应向量为 $y = X \\beta^{\\star}$，其中真实系数向量为\n$$\n\\beta^{\\star} = \\begin{pmatrix} 1 \\\\ 0.2 \\end{pmatrix}.\n$$\n将标准 $\\ell_1$ lasso 估计量定义为以下凸问题的最优解\n$$\n\\min_{\\beta \\in \\mathbb{R}^2} \\left\\{ \\frac{1}{2n} \\| y - X \\beta \\|_2^2 + \\lambda \\sum_{j=1}^2 |\\beta_j| \\right\\},\n$$\n并将自适应 lasso 估计量定义为\n$$\n\\min_{\\beta \\in \\mathbb{R}^2} \\left\\{ \\frac{1}{2n} \\| y - X \\beta \\|_2^2 + \\lambda \\sum_{j=1}^2 w_j |\\beta_j| \\right\\},\n$$\n其中权重为 $w_j = \\frac{1}{|\\tilde{\\beta}_j|^{\\gamma}}$，固定 $\\gamma = 1$，$\\tilde{\\beta}$ 是在 $(X,y)$ 上计算的普通最小二乘估计量。\n\n从通过 Karush–Kuhn–Tucker (KKT) 条件定义的凸最优性出发，推导在标准 lasso 和自适应 lasso 中，最优解具有特征 1 激活（系数为正）且特征 2 被排除（即 $\\beta_1 > 0$ 且 $\\beta_2 = 0$）的条件。使用这些条件计算保证特征 2 被排除而特征 1 保持激活的最小正则化参数值 $\\lambda_{\\mathrm{L}}$（标准 lasso）和 $\\lambda_{\\mathrm{AL}}$（自适应 lasso）。最后，计算比率\n$$\nR = \\frac{\\lambda_{\\mathrm{AL}}}{\\lambda_{\\mathrm{L}}}.\n$$\n将最终比率 $R$ 四舍五入到四位有效数字。", "solution": "用户希望通过比较标准 lasso 和自适应 lasso 方法来解决一个统计学习问题。\n\n### 第 1 步：提取已知条件\n-   观测值数量, $n=2$。\n-   预测变量数量, $p=2$。\n-   设计矩阵 $X \\in \\mathbb{R}^{2 \\times 2}$，其列已标准化。\n-   样本格拉姆矩阵：$\\frac{1}{n} X^{\\top} X = \\begin{pmatrix} 1 & \\rho \\\\ \\rho & 1 \\end{pmatrix}$。\n-   相关性：$\\rho = 0.95$。\n-   响应向量：$y = X \\beta^{\\star}$。\n-   真实系数向量：$\\beta^{\\star} = \\begin{pmatrix} 1 \\\\ 0.2 \\end{pmatrix}$。\n-   标准 lasso 目标函数：$L_{\\mathrm{L}}(\\beta) = \\frac{1}{2n} \\| y - X \\beta \\|_2^2 + \\lambda \\sum_{j=1}^2 |\\beta_j|$。\n-   自适应 lasso 目标函数：$L_{\\mathrm{AL}}(\\beta) = \\frac{1}{2n} \\| y - X \\beta \\|_2^2 + \\lambda \\sum_{j=1}^2 w_j |\\beta_j|$。\n-   自适应 lasso 权重：$w_j = \\frac{1}{|\\tilde{\\beta}_j|^{\\gamma}}$，其中 $\\gamma = 1$，$\\tilde{\\beta}$ 是普通最小二乘 (OLS) 估计量。\n\n### 第 2 步：使用提取的已知条件进行验证\n这个问题是凸优化应用于统计学习的一个明确定义的练习。\n-   **科学依据：** 该问题使用了 lasso 和自适应 lasso 的标准定义，它们是高维统计中的基本技术。其设置基于一个标准线性模型。\n-   **定义明确：** 所有必要的数据和定义都已提供，以推导出所要求量的唯一解。该问题是自洽的。\n-   **目标：** 问题陈述由精确的数学定义组成，并要求一个定量的结果。没有歧义或主观性。\n\n### 第 3 步：结论和行动\n问题有效。我们开始求解。\n\nKarush-Kuhn-Tucker (KKT) 条件为凸问题的最优性提供了充要条件。对于形如 $F(\\beta) = f(\\beta) + g(\\beta)$ 的一般目标函数，其中 $f(\\beta)$ 可微，$g(\\beta)$ 是凸函数，解 $\\hat{\\beta}$ 的最优性条件是 $0 \\in \\nabla f(\\hat{\\beta}) + \\partial g(\\hat{\\beta})$，其中 $\\partial g(\\hat{\\beta})$ 是 $g$ 在 $\\hat{\\beta}$ 处的次梯度。\n\n在我们的设定中，可微部分是损失函数 $f(\\beta) = \\frac{1}{2n} \\|y - X\\beta\\|_2^2$，其梯度为 $\\nabla f(\\beta) = -\\frac{1}{n}X^{\\top}(y - X\\beta)$。不可微部分是惩罚项。\n\n我们需要分析解向量 $\\hat{\\beta}$ 具有形式 $\\hat{\\beta} = (\\hat{\\beta}_1, 0)^{\\top}$ 且 $\\hat{\\beta}_1 > 0$ 的情况。\n\n首先，我们计算 $\\frac{1}{n}X^{\\top}y$ 这一项。由于模型是无噪声的，$y = X\\beta^{\\star}$，我们有：\n$$\n\\frac{1}{n} X^{\\top}y = \\frac{1}{n} X^{\\top}X\\beta^{\\star} = \\begin{pmatrix} 1 & \\rho \\\\ \\rho & 1 \\end{pmatrix} \\begin{pmatrix} \\beta^{\\star}_1 \\\\ \\beta^{\\star}_2 \\end{pmatrix}\n$$\n代入 $\\beta^{\\star}_1=1$, $\\beta^{\\star}_2=0.2$, 和 $\\rho=0.95$：\n$$\n\\frac{1}{n} X^{\\top}y = \\begin{pmatrix} 1 & 0.95 \\\\ 0.95 & 1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0.2 \\end{pmatrix} = \\begin{pmatrix} 1 \\cdot 1 + 0.95 \\cdot 0.2 \\\\ 0.95 \\cdot 1 + 1 \\cdot 0.2 \\end{pmatrix} = \\begin{pmatrix} 1 + 0.19 \\\\ 0.95 + 0.2 \\end{pmatrix} = \\begin{pmatrix} 1.19 \\\\ 1.15 \\end{pmatrix}\n$$\n因此，预测变量与响应的相关性为 $\\frac{1}{n}X_1^{\\top}y = 1.19$ 和 $\\frac{1}{n}X_2^{\\top}y = 1.15$。\n\n**标准 Lasso ($\\ell_1$ lasso)**\n\n惩罚项为 $g(\\beta) = \\lambda \\sum_{j=1}^2 |\\beta_j|$。对于一个解 $\\hat{\\beta} = (\\hat{\\beta}_1, 0)^{\\top}$ 且 $\\hat{\\beta}_1 > 0$，惩罚项的次梯度为 $(\\lambda \\cdot \\text{sign}(\\hat{\\beta}_1), \\lambda s_2)^{\\top} = (\\lambda, \\lambda s_2)^{\\top}$，其中 $s_2 \\in [-1, 1]$。\n\nKKT 条件是：\n1.  对于特征 $j=1$：$-\\frac{1}{n}X_1^{\\top}(y - X\\hat{\\beta}) + \\lambda = 0$。\n2.  对于特征 $j=2$：$|-\\frac{1}{n}X_2^{\\top}(y - X\\hat{\\beta})| \\le \\lambda$。\n\n让我们对 $\\hat{\\beta} = (\\hat{\\beta}_1, 0)^{\\top}$ 展开这些条件。\n1.  根据第一个条件：\n    $$\n    -\\frac{1}{n}X_1^{\\top}y + \\frac{1}{n}X_1^{\\top}X_1\\hat{\\beta}_1 + \\lambda = 0\n    $$\n    使用 $\\frac{1}{n}X_1^{\\top}X_1 = 1$ 和 $\\frac{1}{n}X_1^{\\top}y = 1.19$，我们得到：\n    $$\n    -1.19 + \\hat{\\beta}_1 + \\lambda = 0 \\implies \\hat{\\beta}_1 = 1.19 - \\lambda\n    $$\n    $\\hat{\\beta}_1 > 0$ 的假设要求 $1.19 - \\lambda > 0$，这意味着 $\\lambda < 1.19$。\n\n2.  根据第二个条件：\n    $$\n    |-\\frac{1}{n}X_2^{\\top}y + \\frac{1}{n}X_2^{\\top}X_1\\hat{\\beta}_1| \\le \\lambda\n    $$\n    使用 $\\frac{1}{n}X_2^{\\top}y = 1.15$，$\\frac{1}{n}X_2^{\\top}X_1 = \\rho = 0.95$ 和 $\\hat{\\beta}_1$ 的表达式：\n    $$\n    |-1.15 + 0.95(1.19 - \\lambda)| \\le \\lambda\n    $$\n    $$\n    |-1.15 + 1.1305 - 0.95\\lambda| \\le \\lambda\n    $$\n    $$\n    |-0.0195 - 0.95\\lambda| \\le \\lambda\n    $$\n    对于 $\\lambda > 0$，绝对值内的表达式是负的。所以我们可以写成：\n    $$\n    0.0195 + 0.95\\lambda \\le \\lambda\n    $$\n    $$\n    0.0195 \\le \\lambda - 0.95\\lambda = 0.05\\lambda\n    $$\n    $$\n    \\lambda \\ge \\frac{0.0195}{0.05} = 0.39\n    $$\n    保证特征 2 被排除的正则化参数的最小值为 $\\lambda_{\\mathrm{L}} = 0.39$。该值满足条件 $\\lambda < 1.19$。\n\n**自适应 Lasso**\n\n首先，我们必须计算 OLS 估计量 $\\tilde{\\beta}$ 以找到权重 $w_j$。\n$$\n\\tilde{\\beta} = (X^{\\top}X)^{-1}X^{\\top}y = \\left(\\frac{1}{n}X^{\\top}X\\right)^{-1}\\left(\\frac{1}{n}X^{\\top}y\\right)\n$$\n由于 $y = X\\beta^{\\star}$，在这个无噪声设置中，OLS 估计量是无偏且精确的：\n$$\n\\tilde{\\beta} = \\left(\\frac{1}{n}X^{\\top}X\\right)^{-1}\\left(\\frac{1}{n}X^{\\top}X\\beta^{\\star}\\right) = \\beta^{\\star} = \\begin{pmatrix} 1 \\\\ 0.2 \\end{pmatrix}\n$$\n权重为 $w_j = 1/|\\tilde{\\beta}_j|^{\\gamma}$ 且 $\\gamma=1$：\n$$\nw_1 = \\frac{1}{|\\tilde{\\beta}_1|} = \\frac{1}{|1|} = 1\n$$\n$$\nw_2 = \\frac{1}{|\\tilde{\\beta}_2|} = \\frac{1}{|0.2|} = 5\n$$\n自适应 lasso 是一个加权 lasso 问题，其惩罚项为 $g(\\beta) = \\lambda \\sum_{j=1}^2 w_j |\\beta_j|$。对于 $\\hat{\\beta} = (\\hat{\\beta}_1, 0)^{\\top}$ 且 $\\hat{\\beta}_1 > 0$，KKT 条件为：\n1.  对于特征 $j=1$：$-\\frac{1}{n}X_1^{\\top}(y - X\\hat{\\beta}) + \\lambda w_1 = 0$。\n2.  对于特征 $j=2$：$|-\\frac{1}{n}X_2^{\\top}(y - X\\hat{\\beta})| \\le \\lambda w_2$。\n\n让我们展开这些条件：\n1.  根据第一个条件，当 $w_1 = 1$ 时：\n    $$\n    -\\frac{1}{n}X_1^{\\top}y + \\frac{1}{n}X_1^{\\top}X_1\\hat{\\beta}_1 + \\lambda \\cdot 1 = 0\n    $$\n    $$\n    -1.19 + \\hat{\\beta}_1 + \\lambda = 0 \\implies \\hat{\\beta}_1 = 1.19 - \\lambda\n    $$\n    这与标准 lasso 的情况相同，因为 $w_1=1$。条件 $\\hat{\\beta}_1 > 0$ 意味着 $\\lambda < 1.19$。\n\n2.  根据第二个条件，当 $w_2 = 5$ 时：\n    $$\n    |-\\frac{1}{n}X_2^{\\top}y + \\frac{1}{n}X_2^{\\top}X_1\\hat{\\beta}_1| \\le \\lambda w_2\n    $$\n    $$\n    |-1.15 + 0.95(1.19 - \\lambda)| \\le 5\\lambda\n    $$\n    $$\n    |-0.0195 - 0.95\\lambda| \\le 5\\lambda\n    $$\n    对于 $\\lambda > 0$，这变成：\n    $$\n    0.0195 + 0.95\\lambda \\le 5\\lambda\n    $$\n    $$\n    0.0195 \\le (5 - 0.95)\\lambda = 4.05\\lambda\n    $$\n    $$\n    \\lambda \\ge \\frac{0.0195}{4.05}\n    $$\n    自适应 lasso 的正则化参数最小值为 $\\lambda_{\\mathrm{AL}} = \\frac{0.0195}{4.05}$。\n\n**比率计算**\n\n最后，我们计算比率 $R = \\lambda_{\\mathrm{AL}} / \\lambda_{\\mathrm{L}}$：\n$$\nR = \\frac{\\lambda_{\\mathrm{AL}}}{\\lambda_{\\mathrm{L}}} = \\frac{0.0195 / 4.05}{0.39}\n$$\n我们可以通过注意到 $\\lambda_{\\mathrm{L}} = 0.39 = \\frac{0.0195}{0.05}$ 来简化这个计算：\n$$\nR = \\frac{0.0195 / 4.05}{0.0195 / 0.05} = \\frac{0.05}{4.05} = \\frac{5}{405} = \\frac{1}{81}\n$$\n作为小数，$R = \\frac{1}{81} \\approx 0.012345679...$。四舍五入到四位有效数字得到 $0.01235$。", "answer": "$$\n\\boxed{0.01235}\n$$", "id": "3095616"}, {"introduction": "在某些特定场景下，标准Lasso可能无法识别出真实的特征，尤其是在预测变量高度相关且真实系数符号相反（即“符号抵消”）时。本练习旨在通过模拟一个典型的“符号抵消”案例，并结合KKT条件进行分析，来揭示这一现象的内在机理。你将通过编码实践，验证自适应Lasso如何凭借其权重机制成功克服标准Lasso的局限性 [@problem_id:3095620]。", "problem": "考虑一个由多元正态设计生成的、包含预测变量和响应变量的线性模型。令 $n$ 表示观测数量，$p$ 表示预测变量的数量。我们在一个具有高度相关预测变量的环境中研究Lasso和自适应Lasso估计量，其中真实信号表现出符号抵消。您的任务是：从第一性原理出发推导最优性条件，实现遵守这些条件的求解器，然后设计一个模拟实验，以证明在存在符号抵消的情况下，自适应Lasso可能会选择被Lasso舍弃的特征。\n\n基本原理：\n- 使用标准线性回归模型 $y = X \\beta^{\\star} + \\varepsilon$，其中 $X \\in \\mathbb{R}^{n \\times p}$ 在给定抽样下被视为非随机的，$y \\in \\mathbb{R}^{n}$，$\\beta^{\\star} \\in \\mathbb{R}^{p}$，且 $\\varepsilon \\in \\mathbb{R}^{n}$ 的均值为零，方差有限。\n- 使用平方损失函数 $L(\\beta) = \\frac{1}{2n} \\lVert y - X \\beta \\rVert_{2}^{2}$，该函数对于 $\\beta$ 是凸函数。\n- 使用绝对值函数的次梯度定义：对于任意标量 $u$，其次梯度是任意满足以下条件的 $g \\in [-1,1]$：当 $u \\neq 0$ 时，$g = \\operatorname{sign}(u)$；当 $u = 0$ 时，$g$ 是区间 $[-1,1]$ 内的任意值。\n- 使用凸问题的一阶最优性条件和Karush–Kuhn–Tucker (KKT) 条件。\n\n需使用并从中推导的定义：\n- Lasso估计量求解 $\\min_{\\beta \\in \\mathbb{R}^{p}} \\left\\{ \\frac{1}{2n} \\lVert y - X \\beta \\rVert_{2}^{2} + \\lambda \\sum_{j=1}^{p} \\lvert \\beta_{j} \\rvert \\right\\}$，其中 $\\lambda \\ge 0$。\n- 自适应Lasso估计量是一种加权Lasso，求解 $\\min_{\\beta \\in \\mathbb{R}^{p}} \\left\\{ \\frac{1}{2n} \\lVert y - X \\beta \\rVert_{2}^{2} + \\lambda \\sum_{j=1}^{p} w_{j} \\lvert \\beta_{j} \\rvert \\right\\}$，其中 $w_{j} = \\frac{1}{\\lvert \\widehat{\\beta}_{j}^{\\text{init}} \\rvert^{\\gamma} + \\varepsilon_{w}}$，对于某个 $\\gamma > 0$、一个小的 $\\varepsilon_{w} > 0$ 以及一个初始估计量 $\\widehat{\\beta}^{\\text{init}}$（例如普通最小二乘法估计量）。\n\n要求的推导和算法设计：\n1. 从 $L(\\beta)$ 的凸性和绝对值的次梯度定义出发，推导Lasso和自适应Lasso的KKT条件。特别地，令 $r(\\beta) = y - X \\beta$ 和 $c_{j}(\\beta) = \\frac{1}{n} X_{j}^{\\top} r(\\beta)$ 表示与预测变量 $j$ 的残差相关性。请展示关于 $c_{j}(\\beta)$、$\\lambda$ 和 $w_{j}$ 的最优性充要条件。\n2. 设计一个坐标下降算法，通过对每个坐标更新进行软阈值处理来强制满足这些KKT条件。请从第一性原理出发，使用最小二乘偏残差和次梯度刻画，除了关于一维二次最小化的标准事实外，不要假设任何预先推导的快捷公式。\n3. 构建一个模拟实验，其中 $p=2$ 个预测变量 $(X_{1}, X_{2})$ 的相关性 $\\rho$ 接近于1，并将真实系数设置为 $\\beta^{\\star} = (1,-1)$ 以产生符号抵消。从均值为零、协方差矩阵为 $\\Sigma = \\begin{pmatrix} 1 & \\rho \\\\ \\rho & 1 \\end{pmatrix}$ 的多元正态分布中生成 $X$，并用小的高斯噪声生成 $y = X \\beta^{\\star} + \\varepsilon$。将 $X$ 的每一列标准化，使其经验方差为1，并对 $y$ 进行中心化，然后针对指定的 $\\lambda$ 和 $\\gamma$ 拟合Lasso和自适应Lasso。\n4. 使用KKT条件解释，为什么当 $\\rho$ 非常接近1时，初始残差相关性 $c_{j}(0)$ 大约可以为 $1 - \\rho$，这个值可能小于 $\\lambda$，从而使得 $\\beta = 0$ 的Lasso解是KKT可行的。然后解释为什么基于一个大幅值初始估计量的自适应Lasso（其权重为 $w_{j}$）可以充分降低有效阈值 $\\lambda w_{j}$，使得某些坐标变为非零，从而选择被Lasso舍弃的特征。\n\n测试套件和输出规范：\n- 实现算法并运行以下四个测试用例，每个用例由 $(n,\\rho,\\beta_{1}^{\\star},\\beta_{2}^{\\star},\\sigma,\\lambda,\\gamma,\\text{seed})$ 定义：\n    - 用例 1: $(200, 0.99, 1.0, -1.0, 0.01, 0.05, 1.0, 0)$\n    - 用例 2: $(200, 0.95, 1.0, -1.0, 0.01, 0.005, 1.0, 1)$\n    - 用例 3: $(200, 0.99, 1.0, -1.0, 0.01, 1.0, 1.0, 2)$\n    - 用例 4: $(200, 0.99, 1.0, -1.0, 0.01, 0.05, 2.0, 3)$\n- 对于每个用例，计算：\n    - 一个布尔值，指示自适应Lasso是否选择了比Lasso严格更多的非零系数，即 $\\#\\{j: \\widehat{\\beta}_{j}^{\\text{Ada}} \\neq 0\\} > \\#\\{j: \\widehat{\\beta}_{j}^{\\text{Lasso}} \\neq 0\\}$ 是否成立。\n    - Lasso解在 $j = 1,2$ 上的最大KKT违反正度，计算方式为当 $\\widehat{\\beta}_{j} \\neq 0$ 时对等式条件的最大非负违反，或当 $\\widehat{\\beta}_{j} = 0$ 时对不等式条件的最大非负违反，使用残差相关性和阈值进行计算。\n    - 类似地，使用加权阈值计算的自适应Lasso解的最大KKT违反正度。\n    - Lasso和自适应Lasso选择的变量数量。\n- 您的程序应生成单行输出，其中包含一个逗号分隔的列表形式的结果，并用方括号括起来，不含空格。列表中的每个元素对应一个测试用例，其本身也是一个格式为 $[\\text{布尔值}, \\text{浮点数}, \\text{浮点数}, \\text{整数}, \\text{整数}]$ 的列表。例如，最后一行应类似于 $[[\\text{b}_{1},\\text{v}_{1}^{\\text{L}},\\text{v}_{1}^{\\text{A}},\\text{k}_{1}^{\\text{L}},\\text{k}_{1}^{\\text{A}}],[\\text{b}_{2},\\text{v}_{2}^{\\text{L}},\\text{v}_{2}^{\\text{A}},\\text{k}_{2}^{\\text{L}},\\text{k}_{2}^{\\text{A}}],[\\text{b}_{3},\\text{v}_{3}^{\\text{L}},\\text{v}_{3}^{\\text{A}},\\text{k}_{3}^{\\text{L}},\\text{k}_{3}^{\\text{A}}],[\\text{b}_{4},\\text{v}_{4}^{\\text{L}},\\text{v}_{4}^{\\text{A}},\\text{k}_{4}^{\\text{L}},\\text{k}_{4}^{\\text{A}}]]$。\n此问题不涉及物理单位；将所有数值报告为纯浮点数或整数。不使用角度。", "solution": "该问题要求推导Lasso和自适应Lasso估计量的最优性条件，设计一个坐标下降算法来求解这些估计量，并通过一个模拟研究来展示自适应Lasso在特定场景下优于Lasso的情况。\n\n### 1. KKT条件的推导\n\n给定线性模型 $y = X \\beta^{\\star} + \\varepsilon$ 和平方误差损失函数 $L(\\beta) = \\frac{1}{2n} \\lVert y - X \\beta \\rVert_{2}^{2}$。该损失函数是凸且连续可微的。其梯度为：\n$$\n\\nabla_{\\beta} L(\\beta) = \\frac{1}{n} X^{\\top} (X \\beta - y) = -\\frac{1}{n} X^{\\top} (y - X \\beta)\n$$\n\n**Lasso估计量**\n\nLasso估计量 $\\widehat{\\beta}^{\\text{Lasso}}$ 最小化目标函数：\n$$\nJ_{\\text{Lasso}}(\\beta) = L(\\beta) + \\lambda \\sum_{j=1}^{p} \\lvert \\beta_{j} \\rvert = \\frac{1}{2n} \\lVert y - X \\beta \\rVert_{2}^{2} + \\lambda \\lVert \\beta \\rVert_{1}\n$$\n目标函数是一个凸的可微函数 ($L(\\beta)$) 和一个凸的不可微函数（$\\ell_1$范数惩罚项）的和。一个向量 $\\widehat{\\beta}$ 是 $J_{\\text{Lasso}}(\\beta)$ 的最小化子，当且仅当零向量属于其次微分 $\\partial J_{\\text{Lasso}}(\\widehat{\\beta})$。根据次微分的和法则：\n$$\n\\partial J_{\\text{Lasso}}(\\beta) = \\{ \\nabla_{\\beta} L(\\beta) \\} + \\lambda \\cdot \\partial \\left( \\sum_{j=1}^{p} \\lvert \\beta_{j} \\rvert \\right)\n$$\n$\\ell_1$范数的次微分是其各分量次微分的笛卡尔积，即 $\\partial \\lVert \\beta \\rVert_{1} = \\partial \\lvert \\beta_1 \\rvert \\times \\cdots \\times \\partial \\lvert \\beta_p \\rvert$。对于单个分量 $\\beta_j$，绝对值函数 $\\partial \\lvert \\beta_j \\rvert$ 的次梯度由任意标量 $g_j$ 给出，满足：\n$$\ng_j = \\begin{cases} \\operatorname{sign}(\\beta_j) & \\text{若 } \\beta_j \\neq 0 \\\\ v, \\text{ 对于任意 } v \\in [-1, 1] & \\text{若 } \\beta_j = 0 \\end{cases}\n$$\n一阶最优性条件是 $0 \\in \\partial J_{\\text{Lasso}}(\\widehat{\\beta})$。这意味着必须存在一个次梯度向量 $g = (g_1, \\dots, g_p)^{\\top}$，其中 $g_j \\in \\partial \\lvert \\widehat{\\beta}_j \\rvert$，使得：\n$$\n\\nabla_{\\beta} L(\\widehat{\\beta}) + \\lambda g = 0\n$$\n考虑该向量方程的第 $j$ 个分量：\n$$\n-\\frac{1}{n} X_{j}^{\\top} (y - X \\widehat{\\beta}) + \\lambda g_j = 0\n$$\n令 $c_j(\\widehat{\\beta}) = \\frac{1}{n} X_{j}^{\\top} (y - X \\widehat{\\beta})$ 表示第 $j$ 个预测变量与在 $\\widehat{\\beta}$ 处的残差的相关性，该条件变为 $c_j(\\widehat{\\beta}) = \\lambda g_j$。我们根据 $\\widehat{\\beta}_j$ 的值对此进行分析：\n1.  如果 $\\widehat{\\beta}_j \\neq 0$，则 $g_j = \\operatorname{sign}(\\widehat{\\beta}_j)$。条件变为 $c_j(\\widehat{\\beta}) = \\lambda \\operatorname{sign}(\\widehat{\\beta}_j)$。\n2.  如果 $\\widehat{\\beta}_j = 0$，则 $g_j \\in [-1, 1]$。条件 $c_j(\\widehat{\\beta}) = \\lambda g_j$ 意味着 $\\lvert c_j(\\widehat{\\beta}) \\rvert \\le \\lambda$。\n\n这些是Lasso估计量的Karush-Kuhn-Tucker (KKT) 条件。\n\n**自适应Lasso估计量**\n\n自适应Lasso的目标函数是：\n$$\nJ_{\\text{Ada}}(\\beta) = \\frac{1}{2n} \\lVert y - X \\beta \\rVert_{2}^{2} + \\lambda \\sum_{j=1}^{p} w_{j} \\lvert \\beta_{j} \\rvert\n$$\n其中 $w_j > 0$ 是固定权重。推导过程与Lasso情况完全相同，只是每个分量 $\\beta_j$ 的惩罚项被 $w_j$ 缩放。第 $j$ 个分量的最优性条件是：\n$$\n-\\frac{1}{n} X_{j}^{\\top} (y - X \\widehat{\\beta}) + \\lambda w_j g_j = 0\n$$\n其中 $g_j \\in \\partial \\lvert \\widehat{\\beta}_j \\rvert$。这导出了自适应Lasso的KKT条件：\n1.  如果 $\\widehat{\\beta}_j \\neq 0$，则 $c_j(\\widehat{\\beta}) = \\lambda w_j \\operatorname{sign}(\\widehat{\\beta}_j)$。\n2.  如果 $\\widehat{\\beta}_j = 0$，则 $\\lvert c_j(\\widehat{\\beta}) \\rvert \\le \\lambda w_j$。\n\n### 2. 坐标下降算法设计\n\n坐标下降算法一次只对单个坐标 $\\beta_j$ 进行优化，同时保持所有其他坐标 $\\beta_{k \\neq j}$ 固定。对于Lasso的目标函数，我们希望求解：\n$$\n\\min_{\\beta_j} J(\\beta_1, \\dots, \\beta_j, \\dots, \\beta_p)\n$$\n让我们分离出依赖于 $\\beta_j$ 的项：\n$$\nf(\\beta_j) = \\frac{1}{2n} \\left\\lVert y - \\sum_{k \\neq j} X_k \\beta_k - X_j \\beta_j \\right\\rVert_2^2 + \\lambda |\\beta_j| + C\n$$\n其中 $C = \\lambda \\sum_{k \\neq j} |\\beta_k|$ 对于 $\\beta_j$ 是常数。令偏残差为 $r_{(-j)} = y - \\sum_{k \\neq j} X_k \\beta_k$。二次项展开为：\n$$\n\\lVert r_{(-j)} - X_j \\beta_j \\rVert_2^2 = \\lVert r_{(-j)} \\rVert_2^2 - 2 \\beta_j X_j^{\\top} r_{(-j)} + \\beta_j^2 \\lVert X_j \\rVert_2^2\n$$\n问题指定将 $X$ 标准化，使得每列的经验方差为1。假设各列已中心化，这意味着 $\\frac{1}{n}\\sum_i X_{ij}^2=1$（使用总体方差公式进行标准化），因此 $\\lVert X_j \\rVert_2^2 = n$。目标函数变为：\n$$\nf(\\beta_j) = \\frac{1}{2n} (n \\beta_j^2 - 2 \\beta_j X_j^{\\top} r_{(-j)}) + \\lambda |\\beta_j| + C' = \\frac{1}{2}\\beta_j^2 - \\left( \\frac{1}{n} X_j^{\\top} r_{(-j)} \\right) \\beta_j + \\lambda |\\beta_j| + C'\n$$\n令 $z_j = \\frac{1}{n}X_j^{\\top}r_{(-j)}$。这是偏残差对 $X_j$ 进行简单回归的系数。关于 $\\beta_j$ 的次梯度是 $\\beta_j - z_j + \\lambda g_j$，其中 $g_j \\in \\partial |\\beta_j|$。将次梯度设为零，$\\beta_j - z_j + \\lambda g_j = 0$，得到：\n- 如果 $\\beta_j>0$，$g_j=1 \\implies \\beta_j = z_j - \\lambda$。这仅在 $z_j > \\lambda$ 时有效。\n- 如果 $\\beta_j<0$，$g_j=-1 \\implies \\beta_j = z_j + \\lambda$。这仅在 $z_j < -\\lambda$ 时有效。\n- 如果 $|z_j| \\le \\lambda$，满足条件的唯一方式是 $\\beta_j=0$，此时 $g_j = z_j / \\lambda \\in [-1,1]$。\n\n这就得到了软阈值算子 $S_{\\lambda}(\\cdot)$：\n$$\n\\widehat{\\beta}_j = S_{\\lambda}(z_j) = \\operatorname{sign}(z_j) \\max(\\lvert z_j \\rvert - \\lambda, 0)\n$$\n项 $z_j$ 可以高效计算。注意 $r_{(-j)} = y - X\\beta^{\\text{old}} + X_j\\beta_j^{\\text{old}} = r^{\\text{old}} + X_j\\beta_j^{\\text{old}}$。所以，\n$$\nz_j = \\frac{1}{n}X_j^{\\top}(r^{\\text{old}} + X_j\\beta_j^{\\text{old}}) = \\frac{1}{n}X_j^{\\top}r^{\\text{old}} + \\frac{\\lVert X_j \\rVert_2^2}{n}\\beta_j^{\\text{old}} = \\frac{1}{n}X_j^{\\top}r^{\\text{old}} + \\beta_j^{\\text{old}}\n$$\n坐标下降算法遍历 $j=1,\\dots,p$，使用软阈值规则，基于所有其他系数的最新值来更新每个 $\\beta_j$。对于自适应Lasso，更新规则是相同的，只是阈值 $\\lambda$ 被替换为每个坐标 $j$ 的特定阈值 $\\lambda w_j$。\n\n### 3. 模拟与现象解释\n\n该模拟构建了 $p=2$ 个具有高正相关 $\\rho$ 的预测变量，真实系数为 $\\beta^{\\star}=(1, -1)^{\\top}$。这种“符号抵消”对预测变量与响应变量之间的关系有关键影响。\n\n**使用KKT条件解释：**\n\n1.  **Lasso的低初始相关性：** Lasso算法在初始化时（即在 $\\beta=0$ 处）的行为由零解的KKT条件决定：对所有 $j$ 都有 $\\lvert c_j(0) \\rvert \\le \\lambda$。项 $c_j(0) = \\frac{1}{n}X_j^\\top y$ 是预测变量 $j$ 与响应变量之间的初始相关性。\n    对于 $y = X_1\\beta_1^{\\star} + X_2\\beta_2^{\\star} + \\varepsilon$，并将 $X$ 的列标准化为单位方差和零均值后：\n    $$\n    c_1(0) = \\frac{1}{n}X_1^\\top y = \\frac{1}{n}X_1^\\top(X_1\\beta_1^\\star + X_2\\beta_2^\\star + \\varepsilon) = \\left(\\frac{1}{n} X_1^\\top X_1\\right)\\beta_1^\\star + \\left(\\frac{1}{n} X_1^\\top X_2\\right)\\beta_2^\\star + \\frac{1}{n}X_1^\\top \\varepsilon\n    $$\n    对于大的 $n$，$\\frac{1}{n}X_1^\\top X_1 \\approx 1$ 且 $\\frac{1}{n}X_1^\\top X_2 \\approx \\rho$。当 $\\beta_1^\\star=1$ 和 $\\beta_2^\\star=-1$ 时，我们得到：\n    $$\n    c_1(0) \\approx 1 \\cdot (1) + \\rho \\cdot (-1) = 1 - \\rho\n    $$\n    类似地，$c_2(0) \\approx \\rho - 1$。因此，$\\lvert c_j(0) \\rvert \\approx 1-\\rho$。当 $\\rho$ 非常接近1时（例如 $\\rho=0.99$），这个相关性非常小（例如 $0.01$）。如果我们选择一个比这个值大的正则化参数 $\\lambda$（例如 $\\lambda=0.05$），KKT条件 $\\lvert c_j(0) \\rvert \\le \\lambda$ 就会被满足。因此，Lasso的坐标下降算法将不会从原点移动，从而得到最终估计 $\\widehat{\\beta}^{\\text{Lasso}} = 0$。\n\n2.  **自适应Lasso的优势：** 自适应Lasso使用权重 $w_j = 1/(\\lvert \\widehat{\\beta}_{j}^{\\text{init}} \\rvert^{\\gamma} + \\varepsilon_{w})$。关键在于初始估计量 $\\widehat{\\beta}^{\\text{init}}$ 的行为，这里建议使用OLS估计量。在高度共线性的情况下，已知OLS估计量即使是无偏的，也具有非常高的方差。OLS估计量的方差与 $(1-\\rho^2)^{-1}$ 成正比，当 $\\rho \\to 1$ 时会急剧增大。\n    虽然 $\\widehat{\\beta}^{\\text{init}}$ 的期望值是 $\\beta^{\\star}=(1, -1)^{\\top}$，但在有限样本中的一次特定实现可能会由于这种高方差而得到一个“大幅值”的估计。例如，一个估计的绝对值可能远大于1。一个大的 $\\lvert \\widehat{\\beta}_{j}^{\\text{init}} \\rvert$ 值会导致一个非常小的权重 $w_j$。\n    这个小权重将该坐标的有效惩罚阈值降低到 $\\lambda_{\\text{eff},j} = \\lambda w_j$。这个新的有效阈值有可能小于初始残差相关性，即 $\\lambda w_j < \\lvert c_j(0) \\rvert \\approx 1-\\rho$。\n    当这种情况发生时，零系数的KKT条件被违反（$\\lvert c_j(0) \\rvert \\not\\le \\lambda w_j$）。自适应Lasso的坐标下降算法随后会将 $\\beta_j$ 更新为一个非零值。因此，自适应Lasso通过基于一个初始（可能是高方差的）估计来调整其惩罚，能够克服初始的低边际相关性，并选择标准Lasso所舍弃的特征。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the simulation test suite for Lasso vs. Adaptive Lasso.\n    \"\"\"\n    test_cases = [\n        # (n, rho, beta1_star, beta2_star, sigma, lambda, gamma, seed)\n        (200, 0.99, 1.0, -1.0, 0.01, 0.05, 1.0, 0),\n        (200, 0.95, 1.0, -1.0, 0.01, 0.005, 1.0, 1),\n        (200, 0.99, 1.0, -1.0, 0.01, 1.0, 1.0, 2),\n        (200, 0.99, 1.0, -1.0, 0.01, 0.05, 2.0, 3)\n    ]\n    \n    results = []\n\n    for n, rho, beta1_star, beta2_star, sigma, lam, gamma, seed in test_cases:\n        \n        # Set seed for reproducibility\n        np.random.seed(seed)\n        \n        # 1. Generate Data\n        p = 2\n        beta_star = np.array([beta1_star, beta2_star])\n        cov = np.array([[1, rho], [rho, 1]])\n        X = np.random.multivariate_normal(mean=np.zeros(p), cov=cov, size=n)\n        epsilon = np.random.normal(0, sigma, size=n)\n        y = X @ beta_star + epsilon\n\n        # 2. Preprocess Data\n        # Center y\n        y = y - np.mean(y)\n        # Standardize X to have mean 0 and emp. variance 1 (using N in denominator)\n        X_mean = np.mean(X, axis=0)\n        X_std = np.std(X, axis=0, ddof=0)\n        X = (X - X_mean) / X_std\n\n        # Verify standardization: ||X_j||^2 should be n\n        # This simplifies the coordinate descent update derivation\n        # np.sum(X**2, axis=0) is approx [n, n]\n\n        def soft_threshold(z, t):\n            return np.sign(z) * np.maximum(np.abs(z) - t, 0)\n\n        def coordinate_descent(X, y, weights, lam, max_iter=1000, tol=1e-8):\n            n, p = X.shape\n            beta = np.zeros(p)\n            thresholds = lam * weights\n            \n            for _ in range(max_iter):\n                beta_old = beta.copy()\n                for j in range(p):\n                    # Efficiently calculate z_j = beta_j + (1/n) * X_j^T * r\n                    # where r is current residual y - X*beta\n                    r = y - X @ beta\n                    z_j = beta[j] + (X[:, j].T @ r) / n\n                    beta[j] = soft_threshold(z_j, thresholds[j])\n                \n                if np.linalg.norm(beta - beta_old)  tol:\n                    break\n            return beta\n\n        # 3. Fit Lasso\n        weights_lasso = np.ones(p)\n        beta_lasso = coordinate_descent(X, y, weights_lasso, lam)\n\n        # 4. Fit Adaptive Lasso\n        eps_w = 1e-6\n        # Initial estimator: OLS\n        # Use np.linalg.lstsq for numerical stability\n        beta_init = np.linalg.lstsq(X, y, rcond=None)[0]\n        \n        weights_ada = 1 / (np.abs(beta_init)**gamma + eps_w)\n        beta_ada = coordinate_descent(X, y, weights_ada, lam)\n        \n        # 5. Compute evaluation metrics\n        \n        # Count non-zero coefficients\n        count_tol = 1e-6\n        k_lasso = np.sum(np.abs(beta_lasso) > count_tol)\n        k_ada = np.sum(np.abs(beta_ada) > count_tol)\n        \n        # Boolean: AdaLasso selects more\n        ada_selects_more = k_ada > k_lasso\n        \n        def calculate_kkt_violation(X, y, beta, weights, lam):\n            n, p = X.shape\n            r = y - X @ beta\n            c = (X.T @ r) / n\n            thresholds = lam * weights\n            violations = np.zeros(p)\n            \n            for j in range(p):\n                if np.abs(beta[j]) > count_tol:\n                    # Condition: c_j = lambda * w_j * sign(beta_j)\n                    violations[j] = np.abs(c[j] - thresholds[j] * np.sign(beta[j]))\n                else:\n                    # Condition: |c_j| = lambda * w_j\n                    violations[j] = np.maximum(0, np.abs(c[j]) - thresholds[j])\n            return np.max(violations)\n            \n        kkt_violation_lasso = calculate_kkt_violation(X, y, beta_lasso, weights_lasso, lam)\n        kkt_violation_ada = calculate_kkt_violation(X, y, beta_ada, weights_ada, lam)\n        \n        case_result = [ada_selects_more, kkt_violation_lasso, kkt_violation_ada, int(k_lasso), int(k_ada)]\n        results.append(case_result)\n\n    # Format output\n    # Convert bools to lowercase strings for JSON-like output\n    # Format floats and ints as requested\n    def format_val(v):\n        if isinstance(v, bool) or isinstance(v, np.bool_):\n            return str(v).lower()\n        if isinstance(v, (int, np.integer)):\n            return str(v)\n        if isinstance(v, (float, np.floating)):\n            # Using general format for precision\n            return f\"{v:.6g}\" # Use general format for robustness\n        return str(v)\n\n    result_str = ','.join([\n        '[' + ','.join([format_val(v) for v in res]) + ']' \n        for res in results\n    ])\n    \n    print(f\"[{result_str}]\".replace(\" \", \"\"))\n\n\n# This block is for execution if the file is run as a script.\n# The user's system will run the solve() function and capture the output.\n# if __name__ == '__main__':\n#     solve()\n```", "id": "3095620"}, {"introduction": "掌握一个算法的最好方式就是亲手实现它。这个综合性练习将挑战你从零开始，完整地构建一个自适应Lasso求解器，包括推导坐标下降的更新规则、实现内外嵌套的循环结构，以及在合成数据上进行测试。完成这个练习后，你将对自适应Lasso的每一个技术细节都有深刻的理解，并具备将其应用于实际问题的能力 [@problem_id:3111876]。", "problem": "要求您实现一个自适应最小绝对收缩和选择算子 (adaptive LASSO) 求解器，该求解器使用嵌套算法结构：一个外层循环，根据当前系数估计更新坐标权重；一个内层循环，对相应的加权 LASSO 子问题运行循环坐标下降直至收敛。您的程序必须实现该方法，将其应用于指定的合成线性回归测试用例，并输出表征变量选择和系数精度的量化指标。\n\n从经验风险最小化在线性回归中的平方损失以及正则化概念的核心定义开始：\n\n1. 数据由一个设计矩阵 $X \\in \\mathbb{R}^{n \\times p}$ 和一个响应向量 $y \\in \\mathbb{R}^{n}$ 组成。线性模型假设 $y \\approx X \\beta$，其中 $\\beta \\in \\mathbb{R}^{p}$ 是一个待估计的未知参数向量。\n\n2. 需要最小化的损失是经验平方误差，定义为\n$$\nL(\\beta) \\equiv \\frac{1}{2n} \\left\\| y - X \\beta \\right\\|_2^2.\n$$\n\n3. 具有固定非负权重 $w_j$ 的加权最小绝对收缩和选择算子 (weighted LASSO) 估计器使用惩罚目标函数\n$$\nQ(\\beta; \\lambda, w) \\equiv \\frac{1}{2n} \\left\\| y - X \\beta \\right\\|_2^2 + \\lambda \\sum_{j=1}^p w_j \\left| \\beta_j \\right|,\n$$\n其中 $\\lambda \\ge 0$ 是一个正则化参数。\n\n4. 自适应 LASSO 使用当前估计值 $\\hat{\\beta}$ 通过以下公式更新权重\n$$\nw_j = \\frac{1}{\\left( \\left| \\hat{\\beta}_j \\right| + \\varepsilon \\right)^\\gamma},\n$$\n其中 $\\gamma \\ge 0$ 且有一个小的 $\\varepsilon  0$ 以确保在 $\\hat{\\beta}_j = 0$ 时的数值稳定性。\n\n基于这些基本定义和加权 $\\ell_1$ 惩罚的次梯度最优性条件，推导内层循环坐标下降求解器的坐标更新法则，该求解器一次仅针对单个坐标 $\\beta_j$ 最小化 $Q(\\beta; \\lambda, w)$，同时保持所有其他坐标固定。您的推导必须从上述定义出发，通过具有非光滑正则化的凸优化的平稳性（次梯度）条件进行。不要假设任何已知的更新公式。\n\n实现要求：\n\n- 通过中心化至零均值和缩放使其欧几里得范数的平方等于 $n$ 来标准化 $X$ 的每个特征列，并将 $y$ 中心化至零均值。单独处理截距项，不对其进行惩罚。\n\n- 实现内层循环坐标下降，该算法重复更新每个坐标 $\\beta_j$ 直至收敛（收敛由坐标的最大绝对变化量低于给定容差来衡量）。使用由次梯度最优性条件所蕴含的、推导出的坐标最小化器。\n\n- 实现外层自适应循环，该循环从初始估计 $\\hat{\\beta}^{(0)}$ 开始，根据指数为 $\\gamma$ 的自适应规则更新权重 $w_j$，并通过内层坐标下降求解加权 LASSO 子问题以获得 $\\hat{\\beta}^{(1)}$，如此往复，执行固定次数的外层迭代。在权重公式中使用严格大于零的 $\\varepsilon$，以避免除以零。\n\n- 对于初始估计 $\\hat{\\beta}^{(0)}$，使用在标准化（带中心化）预测变量上通过数值稳定的线性最小二乘程序计算出的普通最小二乘解。如果问题是欠定的，则使用该程序返回的最小范数最小二乘解。\n\n- 在标准化数据上估计系数后，将系数映射回原始特征尺度以评估准确性和支撑集恢复。根据中心化模型和原始列均值显式定义估计的截距项。\n\n- 对于变量选择评估，如果一个系数在原始尺度上的绝对值超过阈值 $10^{-3}$，则宣布该系数被选中（非零）。\n\n测试套件：\n\n实现您的求解器，并在以下四个合成测试用例上运行它。在每个用例中，使用共同因子结构生成具有相关高斯列的 $X$，生成一个前 $k$ 个条目非零的稀疏真实 $\\beta^\\star$，然后生成 $y = X \\beta^\\star + \\varepsilon$，其中 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I_n)$ 是独立高斯噪声。对于每个测试用例，使用给定的随机种子以确保可复现性。如果提供了振幅列表，则按顺序将其用于 $\\beta^\\star$ 的前 $k$ 个非零条目，否则使用截断至长度 $k$ 的默认振幅 $[2.0, -1.8, 1.5, -1.2, 1.0, -0.8]$。\n\n- 测试用例 1 (理想情况，中等相关性):\n  - $(n, p, k) = (\\,80,\\, 30,\\, 5\\,)$，\n  - $\\rho = 0.3$，\n  - $\\sigma = 0.4$，\n  - $\\lambda = 0.12$，\n  - $\\gamma = 1.0$，\n  - 外层迭代次数 $= 3$，\n  - 随机种子 $= 42$，\n  - 振幅：默认。\n\n- 测试用例 2 (边界条件，$\\gamma = 0$ 时在标准化尺度上退化为普通 LASSO):\n  - 与测试用例 1 数据相同 (通过相同的种子使用相同的 $X, y$)，\n  - $\\lambda = 0.12$，\n  - $\\gamma = 0.0$，\n  - 外层迭代次数 $= 3$，\n  - 振幅：默认。\n\n- 测试用例 3 (高相关性，$p  n$):\n  - $(n, p, k) = (\\,50,\\, 100,\\, 6\\,)$，\n  - $\\rho = 0.8$，\n  - $\\sigma = 0.5$，\n  - $\\lambda = 0.18$，\n  - $\\gamma = 1.5$，\n  - 外层迭代次数 $= 5$，\n  - 随机种子 $= 123$，\n  - 振幅：默认。\n\n- 测试用例 4 (具有小非零振幅的边缘情况):\n  - $(n, p, k) = (\\,100,\\, 40,\\, 6\\,)$，\n  - $\\rho = 0.4$，\n  - $\\sigma = 0.6$，\n  - $\\lambda = 0.10$，\n  - $\\gamma = 2.0$，\n  - 外层迭代次数 $= 5$，\n  - 随机种子 $= 777$，\n  - 振幅: $[\\,0.25,\\,-0.22,\\,0.20,\\,-0.18,\\,0.15,\\,-0.12\\,]$。\n\n您的程序必须为每个测试用例输出列表 $[\\,\\mathrm{TP}, \\mathrm{TN}, \\mathrm{FP}, \\mathrm{FN}, \\mathrm{err}\\,]$，其中：\n- $\\mathrm{TP}$ 是真阳性数量（$\\beta^\\star_j \\ne 0$ 且估计系数的绝对值超过阈值的索引），\n- $\\mathrm{TN}$ 是真阴性数量（$\\beta^\\star_j = 0$ 且估计系数的绝对值小于或等于阈值的索引），\n- $\\mathrm{FP}$ 是假阳性数量，\n- $\\mathrm{FN}$ 是假阴性数量，\n- $\\mathrm{err}$ 是原始特征尺度下的欧几里得范数 $\\left\\| \\hat{\\beta} - \\beta^\\star \\right\\|_2$，四舍五入到 $4$ 位小数。\n\n最终输出格式：\n\n您的程序应生成单行输出，其中包含一个逗号分隔的列表，该列表由四个用例各自的结果列表组成，并用方括号括起来。例如，一个语法上有效的输出看起来像\n$$\n\\left[ [\\,\\cdot, \\cdot, \\cdot, \\cdot, \\cdot\\,], [\\,\\cdot, \\cdot, \\cdot, \\cdot, \\cdot\\,], [\\,\\cdot, \\cdot, \\cdot, \\cdot, \\cdot\\,], [\\,\\cdot, \\cdot, \\cdot, \\cdot, \\cdot\\,] \\right]\n$$\n点号的位置是数值条目。此问题中没有物理单位。不涉及角度。如果出现比例，必须以小数而非百分比报告，但在此问题中，您必须按上述规定报告整数和一个浮点数。", "solution": "用户希望使用嵌套坐标下降算法实现一个自适应 LASSO 求解器。第一步是验证问题陈述。\n\n### 第 1 步：提取给定信息\n- **模型**：线性模型 $y \\approx X \\beta$，其中 $X \\in \\mathbb{R}^{n \\times p}$，$y \\in \\mathbb{R}^{n}$，$\\beta \\in \\mathbb{R}^{p}$。\n- **损失函数**：经验平方误差 $L(\\beta) = \\frac{1}{2n} \\left\\| y - X \\beta \\right\\|_2^2$。\n- **加权 LASSO 目标**：$Q(\\beta; \\lambda, w) = \\frac{1}{2n} \\left\\| y - X \\beta \\right\\|_2^2 + \\lambda \\sum_{j=1}^p w_j \\left| \\beta_j \\right|$，其中正则化参数 $\\lambda \\ge 0$，权重 $w_j \\ge 0$。\n- **自适应权重**：权重通过 $w_j = \\frac{1}{\\left( \\left| \\hat{\\beta}_j \\right| + \\varepsilon \\right)^\\gamma}$ 更新，其中 $\\hat{\\beta}$ 是当前系数估计，$\\gamma \\ge 0$，$\\varepsilon  0$。\n- **数据标准化**：\n    - 将 $X$ 的列中心化至零均值。\n    - 缩放中心化后的 $X$ 的列，使其欧几里得范数的平方为 $n$。\n    - 将响应向量 $y$ 中心化至零均值。\n- **算法结构**：\n    - **外层循环**：迭代固定次数。在每次迭代中，根据当前估计 $\\hat{\\beta}$ 更新权重 $w_j$，然后求解得到的加权 LASSO 子问题。\n    - **内层循环**：一个循环坐标下降算法，用于求解给定权重集的加权 LASSO 子问题。当任何坐标的最大绝对变化量低于某个容差时收敛。\n- **初始化**：初始系数估计 $\\hat{\\beta}^{(0)}$ 是在标准化数据上的普通最小二乘 (OLS) 解。对于欠定系统 ($p  n$)，使用最小范数最小二乘解。\n- **系数反标准化**：在标准化数据上得到最终估计 $\\hat{\\beta}_s$ 后，将其映射回原始特征尺度得到 $\\hat{\\beta}$。\n- **变量选择标准**：如果 $|\\hat{\\beta}_j|  10^{-3}$，则认为系数 $\\hat{\\beta}_j$ 被选中（非零）。\n- **测试用例**：定义了四个合成测试用例，参数为 $(n, p, k, \\rho, \\sigma, \\lambda, \\gamma, \\text{外层迭代次数, 随机种子, 振幅})$，其中 $k$ 是真实非零系数的数量，$\\rho$ 是特征之间的相关性。\n- **数据生成**：使用共同因子模型 $X_{ij} = \\sqrt{\\rho} f_i + \\sqrt{1-\\rho} Z_{ij}$ 生成特征 $X$，其中 $f_i, Z_{ij}$ 是独立同分布的标准正态变量。响应为 $y = X \\beta^\\star + \\varepsilon$，其中 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I_n)$。\n- **输出指标**：对于每个测试用例，输出必须是列表 $[\\mathrm{TP}, \\mathrm{TN}, \\mathrm{FP}, \\mathrm{FN}, \\mathrm{err}]$，其中 TP, TN, FP, FN 是变量选择的计数，$\\mathrm{err} = \\| \\hat{\\beta} - \\beta^\\star \\|_2$。\n\n### 第 2 步：使用提取的给定信息进行验证\n根据验证标准评估问题。\n\n- **科学性**：该问题牢固地植根于统计学习和凸优化的理论。自适应 LASSO、坐标下降以及正则化和交叉验证等概念都是标准的、成熟的课题。数据生成模型是创建具有受控相关性结构的合成数据集的标准方法。整个问题在科学上是合理的。\n- **适定性**：该问题是适定的。目标函数是凸函数，保证了全局最小值的存在。对于 $\\ell_1$ 惩罚，不一定保证解的唯一性，但坐标下降算法会收敛到一个最小值。算法和数据生成的所有参数都已明确提供，使得问题自洽且允许确定性的求解路径。\n- **客观性**：问题使用精确的数学定义和客观的语言陈述。没有主观或基于意见的陈述。评估标准是定量的且定义清晰。\n\n该问题没有表现出任何无效性缺陷：\n1.  **科学/事实不健全**：未发现违规。\n2.  **非形式化/不相关**：问题高度形式化，并与其所述主题直接相关。\n3.  **不完整/矛盾的设置**：提供了所有必要的参数和定义。没有矛盾。\n4.  **不切实际/不可行**：条件和数据是合成的，但在高维统计的现实场景范围内。\n5.  **不适定/结构不良**：问题结构良好，能导向有意义的解。\n6.  **伪深刻/琐碎**：问题需要对一种现代统计算法进行非平凡的实现，包括从第一性原理进行推导。它既不琐碎也不是同义反复。\n7.  **超出科学可验证性**：通过实现指定的算法并在指定的数据上运行，结果是完全可验证的。\n\n### 第 3 步：结论与行动\n问题是 **有效的**。将提供一个解决方案。\n\n---\n\n### 坐标下降更新法则的推导\n\n内层循环的核心是加权 LASSO 目标函数的坐标最小化：\n$$\nQ(\\beta; \\lambda, w) = \\frac{1}{2n} \\left\\| y - X \\beta \\right\\|_2^2 + \\lambda \\sum_{j=1}^p w_j \\left| \\beta_j \\right|\n$$\n该算法在标准化数据上运行。设 $X_s$ 为标准化的设计矩阵（列已中心化并缩放至平方范数为 $n$），$y_c$ 为中心化的响应向量。在此标准化空间中，系数 $\\beta_s$ 的目标函数为：\n$$\nQ(\\beta_s) = \\frac{1}{2n} \\left\\| y_c - X_s \\beta_s \\right\\|_2^2 + \\lambda \\sum_{j=1}^p w_j \\left| \\beta_{s,j} \\right|\n$$\n坐标下降一次仅针对单个系数 $\\beta_{s,j}$ 优化此函数，同时将所有其他系数 $\\beta_{s,k}$ (对于 $k \\neq j$) 固定在其当前值。我们分离出 $Q(\\beta_s)$ 中依赖于 $\\beta_{s,j}$ 的项：\n$$\nQ(\\beta_{s,j}) = \\frac{1}{2n} \\left\\| y_c - \\sum_{k \\neq j} X_{s,k} \\beta_{s,k} - X_{s,j} \\beta_{s,j} \\right\\|_2^2 + \\lambda w_j |\\beta_{s,j}| + \\text{const}\n$$\n设 $r_s^{(j)} = y_c - \\sum_{k \\neq j} X_{s,k} \\beta_{s,k}$ 为去除预测变量 $j$ 的贡献后的部分残差向量。表达式变为：\n$$\nQ(\\beta_{s,j}) = \\frac{1}{2n} \\left\\| r_s^{(j)} - X_{s,j} \\beta_{s,j} \\right\\|_2^2 + \\lambda w_j |\\beta_{s,j}| + \\text{const}\n$$\n展开平方范数项：\n$$\n\\left\\| r_s^{(j)} - X_{s,j} \\beta_{s,j} \\right\\|_2^2 = (r_s^{(j)})^T r_s^{(j)} - 2 \\beta_{s,j} (X_{s,j})^T r_s^{(j)} + \\beta_{s,j}^2 (X_{s,j})^T X_{s,j}\n$$\n根据指定的标准化，$(X_{s,j})^T X_{s,j} = \\|X_{s,j}\\|_2^2 = n$。代入此式，目标简化为关于 $\\beta_{s,j}$ 的一维函数：\n$$\nQ(\\beta_{s,j}) = \\frac{1}{2n} \\left( n \\beta_{s,j}^2 - 2 \\beta_{s,j} (X_{s,j})^T r_s^{(j)} \\right) + \\lambda w_j |\\beta_{s,j}| + \\text{const}\n$$\n$$\nQ(\\beta_{s,j}) = \\frac{1}{2} \\beta_{s,j}^2 - \\beta_{s,j} \\frac{(X_{s,j})^T r_s^{(j)}}{n} + \\lambda w_j |\\beta_{s,j}| + \\text{const}\n$$\n这是一个凸函数。为了找到其最小值，我们使用次梯度最优性条件，该条件表明在最小值点，$0$ 必须在 $Q$ 的次梯度中。$\\partial_{\\beta_{s,j}} Q(\\beta_{s,j})$ 的次梯度是：\n$$\n\\partial_{\\beta_{s,j}} Q(\\beta_{s,j}) = \\beta_{s,j} - \\frac{(X_{s,j})^T r_s^{(j)}}{n} + \\lambda w_j \\partial|\\beta_{s,j}|\n$$\n其中 $\\partial|\\beta_{s,j}|$ 是绝对值函数的次梯度：如果 $\\beta_{s,j} \\neq 0$，则为 $\\mathrm{sgn}(\\beta_{s,j})$；如果 $\\beta_{s,j} = 0$，则为区间 $[-1, 1]$。设 $\\rho_j = \\frac{1}{n} (X_{s,j})^T r_s^{(j)}$。最优性条件是 $0 \\in \\beta_{s,j} - \\rho_j + \\lambda w_j \\partial|\\beta_{s,j}|$，或 $\\rho_j - \\beta_{s,j} \\in \\lambda w_j \\partial|\\beta_{s,j}|$。\n\n我们分三种情况分析此条件：\n1.  如果 $\\beta_{s,j}  0$，那么 $\\partial|\\beta_{s,j}| = \\{1\\}$。条件是 $\\rho_j - \\beta_{s,j} = \\lambda w_j$，得出 $\\beta_{s,j} = \\rho_j - \\lambda w_j$。这仅在 $\\rho_j - \\lambda w_j  0$ 时成立，即 $\\rho_j  \\lambda w_j$。\n2.  如果 $\\beta_{s,j}  0$，那么 $\\partial|\\beta_{s,j}| = \\{-1\\}$。条件是 $\\rho_j - \\beta_{s,j} = -\\lambda w_j$，得出 $\\beta_{s,j} = \\rho_j + \\lambda w_j$。这仅在 $\\rho_j + \\lambda w_j  0$ 时成立，即 $\\rho_j  -\\lambda w_j$。\n3.  如果 $\\beta_{s,j} = 0$，那么 $\\partial|\\beta_{s,j}| = [-1, 1]$。条件是 $\\rho_j \\in [-\\lambda w_j, \\lambda w_j]$，或 $|\\rho_j| \\le \\lambda w_j$。\n\n将这三种情况结合起来，得到 $\\beta_{s,j}$ 的解，即一个软阈值操作：\n$$\n\\hat{\\beta}_{s,j} = \\mathrm{sgn}(\\rho_j) \\max(0, |\\rho_j| - \\lambda w_j) =: S(\\rho_j, \\lambda w_j)\n$$\n其中 $\\rho_j = \\frac{1}{n} (X_{s,j})^T (y_c - \\sum_{k \\neq j} X_{s,k} \\beta_{s,k})$。这是算法内层循环中使用的坐标更新法则。\n\n### 算法实现\n\n整个过程涉及数据生成、标准化和嵌套循环结构。\n\n1.  **数据生成与标准化**：对于每个测试用例，我们按规定生成数据 $(X, y, \\beta^\\star)$。然后计算均值 $\\bar{X}_j$ 和 $\\bar{y}$，以及每列的标准差（与范数相关）$s_j = \\|X_j - \\bar{X}_j\\|_2$。数据被转换为 $(X_s, y_c)$，其中 $y_c = y - \\bar{y}$，而 $X_s$ 的列为 $(X_j - \\bar{X}_j)\\sqrt{n}/s_j$。\n\n2.  **初始化**：自适应方案的初始系数 $\\hat{\\beta}_s^{(0)}$ 通过求解普通最小二乘问题 $y_c = X_s \\beta_s$ 获得。我们使用一个数值稳定的程序，如 `numpy.linalg.lstsq`。\n\n3.  **外层循环**：此循环运行指定的 `outer_iterations` 次数。在迭代 $t = 1, 2, \\ldots$ 中：\n    a.  **权重更新**：根据前一个系数估计 $\\hat{\\beta}_s^{(t-1)}$ 计算自适应权重：$w_j = (|\\hat{\\beta}_{s,j}^{(t-1)}| + \\varepsilon)^{-\\gamma}$。\n    b.  **内层循环（加权 LASSO 求解器）**：调用坐标下降算法，通过最小化 $Q(\\beta_s; \\lambda, w)$ 来求解 $\\hat{\\beta}_s^{(t)}$。此循环过程如下：\n        i.  用 $\\beta_s = \\hat{\\beta}_s^{(t-1)}$ 进行初始化。\n        ii. 重复循环遍历坐标 $j=1, \\ldots, p$，使用推导出的软阈值法则更新每个 $\\beta_{s,j}$。为提高效率，我们维护完整残差 $r_s = y_c - X_s \\beta_s$ 并在每个坐标步骤后更新它，这是一个 $O(n)$ 操作。\n        iii. 当一个完整周期内所有系数的最大绝对变化量低于一个小的容差（例如 $10^{-8}$）时，循环终止。\n\n4.  **反标准化与评估**：一旦获得最终的标准化系数 $\\hat{\\beta}_s$，它们将被转换回原始尺度：$\\hat{\\beta}_j = \\hat{\\beta}_{s,j} \\sqrt{n} / s_j$。然后将这些系数与真实系数 $\\beta^\\star$ 进行比较，以计算所需的指标：真阳性 (TP)、真阴性 (TN)、假阳性 (FP)、假阴性 (FN) 以及欧几里得误差 $\\|\\hat{\\beta} - \\beta^\\star\\|_2$。", "answer": "```python\nimport numpy as np\n\ndef generate_data(n, p, k, rho, sigma, seed, amplitudes):\n    \"\"\"\n    Generates synthetic data for a linear model with correlated features.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    f = rng.normal(size=(n, 1))\n    Z = rng.normal(size=(n, p))\n    X = np.sqrt(rho) * f + np.sqrt(1 - rho) * Z\n\n    beta_star = np.zeros(p)\n    beta_star[:k] = amplitudes[:k]\n\n    epsilon_noise = rng.normal(scale=sigma, size=n)\n    y = X @ beta_star + epsilon_noise\n\n    return X, y, beta_star\n\ndef soft_threshold(rho, C):\n    \"\"\"\n    Soft-thresholding operator.\n    \"\"\"\n    if rho > C:\n        return rho - C\n    elif rho  -C:\n        return rho + C\n    else:\n        return 0.0\n\ndef solve():\n    \"\"\"\n    Main function to run the adaptive LASSO solver on the test suite.\n    \"\"\"\n    default_amplitudes = [2.0, -1.8, 1.5, -1.2, 1.0, -0.8]\n    test_cases = [\n        {'n': 80, 'p': 30, 'k': 5, 'rho': 0.3, 'sigma': 0.4, 'lambda_': 0.12, 'gamma': 1.0, 'outer_iter': 3, 'seed': 42, 'amplitudes': default_amplitudes},\n        {'n': 80, 'p': 30, 'k': 5, 'rho': 0.3, 'sigma': 0.4, 'lambda_': 0.12, 'gamma': 0.0, 'outer_iter': 3, 'seed': 42, 'amplitudes': default_amplitudes},\n        {'n': 50, 'p': 100, 'k': 6, 'rho': 0.8, 'sigma': 0.5, 'lambda_': 0.18, 'gamma': 1.5, 'outer_iter': 5, 'seed': 123, 'amplitudes': default_amplitudes},\n        {'n': 100, 'p': 40, 'k': 6, 'rho': 0.4, 'sigma': 0.6, 'lambda_': 0.10, 'gamma': 2.0, 'outer_iter': 5, 'seed': 777, 'amplitudes': [0.25, -0.22, 0.20, -0.18, 0.15, -0.12]},\n    ]\n    \n    results = []\n    \n    # Algorithm parameters\n    epsilon = 1e-8\n    inner_tol = 1e-8\n    max_inner_iter = 1000\n    selection_threshold = 1e-3\n\n    for case in test_cases:\n        n, p = case['n'], case['p']\n        \n        # Data generation depends on the seed. Case 2 reuses data from Case 1.\n        if case['seed'] == 42 and case['gamma'] == 0.0:\n            # This is Case 2, re-use data from Case 1\n            pass # X, y, beta_star are already set from previous iteration\n        else:\n             X, y, beta_star = generate_data(n, p, case['k'], case['rho'], case['sigma'], case['seed'], case['amplitudes'])\n\n\n        # 1. Standardize data\n        y_mean = y.mean()\n        X_mean = X.mean(axis=0)\n        y_c = y - y_mean\n        X_c = X - X_mean\n\n        X_c_norms = np.linalg.norm(X_c, axis=0)\n        non_const_cols = X_c_norms > 1e-12\n        \n        X_s = np.zeros_like(X_c)\n        scales = np.ones(p)\n        if np.any(non_const_cols):\n             scales[non_const_cols] = X_c_norms[non_const_cols] / np.sqrt(n)\n             X_s[:, non_const_cols] = X_c[:, non_const_cols] / scales[non_const_cols, np.newaxis].T\n\n\n        # 2. Initial OLS estimate\n        beta_s, _, _, _ = np.linalg.lstsq(X_s, y_c, rcond=None)\n\n        # 3. Adaptive LASSO outer loop\n        for _ in range(case['outer_iter']):\n            # a. Update weights\n            weights = (np.abs(beta_s) + epsilon)**(-case['gamma'])\n\n            # b. Inner coordinate descent loop\n            beta_s_cd = np.copy(beta_s)\n            \n            for _ in range(max_inner_iter):\n                max_change = 0.0\n                for j in range(p):\n                    if not non_const_cols[j]: continue\n                    \n                    beta_sj_old = beta_s_cd[j]\n                    \n                    dot_product = np.dot(X_s[:, j], y_c - np.dot(X_s, beta_s_cd) + X_s[:, j] * beta_sj_old)\n                    rho_j = dot_product / n\n                    \n                    beta_s_cd[j] = soft_threshold(rho_j, case['lambda_'] * weights[j])\n                    \n                    change = abs(beta_s_cd[j] - beta_sj_old)\n                    if change > max_change:\n                        max_change = change\n                \n                if max_change  inner_tol:\n                    break\n            beta_s = beta_s_cd\n        \n        # 4. Un-standardize coefficients\n        beta_final = np.zeros(p)\n        if np.any(non_const_cols):\n            beta_final[non_const_cols] = beta_s[non_const_cols] / scales[non_const_cols]\n\n        # 5. Evaluate metrics\n        support_star = np.abs(beta_star) > 0\n        support_hat = np.abs(beta_final) > selection_threshold\n        \n        TP = np.sum(support_hat  support_star)\n        TN = np.sum(~support_hat  ~support_star)\n        FP = np.sum(support_hat  ~support_star)\n        FN = np.sum(~support_hat  support_star)\n        \n        err = np.linalg.norm(beta_final - beta_star)\n        \n        results.append([int(TP), int(TN), int(FP), int(FN), round(err, 4)])\n\n    print(str(results).replace(\" \", \"\"))\n\nsolve()\n```", "id": "3111876"}]}