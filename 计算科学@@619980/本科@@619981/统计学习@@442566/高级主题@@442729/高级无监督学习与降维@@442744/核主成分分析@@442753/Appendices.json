{"hands_on_practices": [{"introduction": "任何降维技术的一个关键功能是能够将新的、未见过的数据点投影到已学习的低维空间中。本练习将引导你通过一步步的计算，来揭示核主成分分析（KPCA）如何处理这种“样本外”扩展 [@problem_id:3136620]。这个过程将重点阐明核技巧和正确的数据中心化在投影新数据点时所扮演的关键角色。", "problem": "考虑一个包含 $n=3$ 个点的训练集，其核函数 $k(\\cdot,\\cdot)$ 是对称半正定的。令 $K \\in \\mathbb{R}^{3 \\times 3}$ 表示未中心化的格拉姆矩阵（Gram matrix），其元素为 $K_{ij} = k(x_i, x_j)$，具体如下\n$$\nK \\;=\\; \\begin{pmatrix}\n2  1  1 \\\\\n1  2  1 \\\\\n1  1  2\n\\end{pmatrix}.\n$$\n令 $H = I - \\frac{1}{n}\\mathbf{1}\\mathbf{1}^{\\top}$ 为中心化矩阵，其中 $\\mathbf{1} \\in \\mathbb{R}^3$ 是全1向量。中心化的格拉姆矩阵为 $K_c = H K H$。$K_c$ 的非零特征对已知为\n$$\n\\lambda_1 \\,=\\, 1,\\quad u_1 \\,=\\, \\frac{1}{\\sqrt{2}}\\begin{pmatrix}1\\\\-1\\\\0\\end{pmatrix},\\qquad\n\\lambda_2 \\,=\\, 1,\\quad u_2 \\,=\\, \\frac{1}{\\sqrt{6}}\\begin{pmatrix}1\\\\1\\\\-2\\end{pmatrix},\n$$\n其中 $u_1$ 和 $u_2$ 是标准正交的，剩下的特征值等于 $0$，其特征向量与 $\\mathbf{1}$ 成比例。\n\n观测到一个新点 $x_{\\star}$，其与训练点的核函数计算结果如下\n$$\nk_{\\star} \\;=\\; \\begin{pmatrix}\nk(x_1, x_{\\star}) \\\\ k(x_2, x_{\\star}) \\\\ k(x_3, x_{\\star})\n\\end{pmatrix} \\;=\\; \\begin{pmatrix} 3 \\\\ 1 \\\\ 1 \\end{pmatrix}.\n$$\n\n使用核主成分分析（KPCA），在正确考虑了使用训练集进行的特征空间中心化的标准样本外扩展（out-of-sample extension）下，计算 $x_{\\star}$ 的第一主坐标 $z_1(x_{\\star})$。将您的最终答案表示为单个精确值。不需要四舍五入，也不涉及单位。", "solution": "任务是使用核主成分分析（KPCA）计算新数据点 $x_{\\star}$ 的第一主坐标 $z_1(x_{\\star})$。该坐标是 $x_{\\star}$ 的中心化特征向量在特征空间中第一主成分方向上的投影。\n\n令 $\\Phi: \\mathcal{X} \\to \\mathcal{F}$ 是与核函数 $k$ 相关联的特征映射，使得 $k(x, y) = \\Phi(x)^{\\top}\\Phi(y)$。训练数据点为 $\\{x_1, x_2, x_3\\}$。\n\n训练数据在特征空间中的均值由下式给出：\n$$ \\bar{\\Phi} = \\frac{1}{n} \\sum_{i=1}^n \\Phi(x_i) $$\n对于我们的问题，$n=3$，因此 $\\bar{\\Phi} = \\frac{1}{3} \\sum_{i=1}^3 \\Phi(x_i)$。\n\n新点 $x_{\\star}$ 的中心化特征向量是 $\\tilde{\\Phi}(x_{\\star}) = \\Phi(x_{\\star}) - \\bar{\\Phi}$。\n\n在KPCA中，主成分（用 $v_k \\in \\mathcal{F}$ 表示）是特征空间协方差矩阵 $C = \\frac{1}{n} \\sum_{i=1}^n (\\Phi(x_i) - \\bar{\\Phi})(\\Phi(x_i) - \\bar{\\Phi})^{\\top}$ 的特征向量。特征向量 $v_k$ 被归一化为单位长度，即 $\\|v_k\\|^2 = 1$。$x_{\\star}$ 的第 $k$ 个主坐标是 $\\tilde{\\Phi}(x_{\\star})$ 在 $v_k$ 上的投影：\n$$ z_k(x_{\\star}) = v_k^{\\top} \\tilde{\\Phi}(x_{\\star}) = v_k^{\\top} (\\Phi(x_{\\star}) - \\bar{\\Phi}) $$\n\n主成分向量 $v_k$ 可以表示为中心化训练点的线性组合，但也可以用原始（未中心化）的特征向量 $\\Phi(x_i)$ 来表示。展开系数是中心化格拉姆矩阵 $K_c$ 的特征向量的分量。具体来说，对于第 $k$ 个主成分：\n$$ v_k = \\frac{1}{\\sqrt{\\lambda_k}} \\sum_{i=1}^n (\\alpha^{(k)})_i \\Phi(x_i) $$\n这里，$\\lambda_k$ 是中心化格拉姆矩阵 $K_c = H K H$ 的第 $k$ 个特征值，而 $\\alpha^{(k)}$ 是对应的归一化为 $\\|\\alpha^{(k)}\\|=1$ 的特征向量。因子 $\\frac{1}{\\sqrt{\\lambda_k}}$ 确保了 $\\|v_k\\|^2 = 1$。这种形式依赖于这样一个性质：对于 $K_c$ 的非零特征值所对应的特征向量，有 $\\sum_i (\\alpha^{(k)})_i = 0$，这简化了 $v_k$ 的表达式。\n\n现在，我们可以写出 $z_1(x_{\\star})$ 的表达式：\n$$ z_1(x_{\\star}) = v_1^{\\top} (\\Phi(x_{\\star}) - \\bar{\\Phi}) = \\left(\\frac{1}{\\sqrt{\\lambda_1}} \\sum_{i=1}^n (\\alpha^{(1)})_i \\Phi(x_i)\\right)^{\\top} \\left(\\Phi(x_{\\star}) - \\frac{1}{n}\\sum_{j=1}^n \\Phi(x_j)\\right) $$\n展开点积：\n$$ z_1(x_{\\star}) = \\frac{1}{\\sqrt{\\lambda_1}} \\left( \\sum_{i=1}^n (\\alpha^{(1)})_i \\Phi(x_i)^{\\top}\\Phi(x_{\\star}) - \\frac{1}{n} \\sum_{i=1}^n \\sum_{j=1}^n (\\alpha^{(1)})_i \\Phi(x_i)^{\\top}\\Phi(x_j) \\right) $$\n使用核技巧 $k(x, y) = \\Phi(x)^{\\top}\\Phi(y)$ 和格拉姆矩阵的定义 $K_{ij}=k(x_i,x_j)$，我们得到：\n$$ z_1(x_{\\star}) = \\frac{1}{\\sqrt{\\lambda_1}} \\left( \\sum_{i=1}^n (\\alpha^{(1)})_i k(x_i, x_{\\star}) - \\frac{1}{n} \\sum_{i=1}^n \\sum_{j=1}^n (\\alpha^{(1)})_i K_{ij} \\right) $$\n这个表达式可以用矩阵和向量表示法紧凑地写出。令 $k_{\\star}$ 是元素为 $(k_{\\star})_i = k(x_i, x_{\\star})$ 的列向量，$\\mathbf{1}$ 是全1列向量。\n$$ z_1(x_{\\star}) = \\frac{1}{\\sqrt{\\lambda_1}} \\left( (\\alpha^{(1)})^{\\top} k_{\\star} - \\frac{1}{n} (\\alpha^{(1)})^{\\top} K \\mathbf{1} \\right) $$\n我们已知以下值：\n$n=3$。\n$\\lambda_1 = 1$。\n$K_c$ 的特征向量是 $\\alpha^{(1)} = u_1 = \\frac{1}{\\sqrt{2}}\\begin{pmatrix}1\\\\-1\\\\0\\end{pmatrix}$。\n新点的核函数计算值为 $k_{\\star} = \\begin{pmatrix} 3 \\\\ 1 \\\\ 1 \\end{pmatrix}$。\n未中心化的格拉姆矩阵是 $K = \\begin{pmatrix} 2  1  1 \\\\ 1  2  1 \\\\ 1  1  2 \\end{pmatrix}$。\n\n我们继续计算 $z_1(x_{\\star})$ 表达式中的两项。\n\n第一项：$(\\alpha^{(1)})^{\\top} k_{\\star}$\n$$ (\\alpha^{(1)})^{\\top} k_{\\star} = \\frac{1}{\\sqrt{2}}\\begin{pmatrix}1  -1  0\\end{pmatrix} \\begin{pmatrix} 3 \\\\ 1 \\\\ 1 \\end{pmatrix} = \\frac{1}{\\sqrt{2}} (1 \\cdot 3 + (-1) \\cdot 1 + 0 \\cdot 1) = \\frac{1}{\\sqrt{2}}(3 - 1) = \\frac{2}{\\sqrt{2}} = \\sqrt{2} $$\n\n第二项：$\\frac{1}{n} (\\alpha^{(1)})^{\\top} K \\mathbf{1}$\n首先，我们计算乘积 $K\\mathbf{1}$：\n$$ K\\mathbf{1} = \\begin{pmatrix} 2  1  1 \\\\ 1  2  1 \\\\ 1  1  2 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 2+1+1 \\\\ 1+2+1 \\\\ 1+1+2 \\end{pmatrix} = \\begin{pmatrix} 4 \\\\ 4 \\\\ 4 \\end{pmatrix} = 4\\mathbf{1} $$\n现在，我们计算 $(\\alpha^{(1)})^{\\top} K \\mathbf{1}$：\n$$ (\\alpha^{(1)})^{\\top} (4\\mathbf{1}) = 4 (\\alpha^{(1)})^{\\top} \\mathbf{1} = 4 \\left( \\frac{1}{\\sqrt{2}}\\begin{pmatrix}1  -1  0\\end{pmatrix} \\right) \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix} = \\frac{4}{\\sqrt{2}}(1 \\cdot 1 - 1 \\cdot 1 + 0 \\cdot 1) = \\frac{4}{\\sqrt{2}}(0) = 0 $$\n这一项为零是意料之中的。特征向量 $\\alpha^{(1)}$ 对应于 $K_c = HKH$ 的一个非零特征值 $\\lambda_1=1$。这意味着 $\\alpha^{(1)}$ 在 $H$ 的列空间中，即与 $\\mathbf{1}$ 正交的向量空间。由于 $K\\mathbf{1}$ 是 $\\mathbf{1}$ 的倍数，点积 $(\\alpha^{(1)})^{\\top} K\\mathbf{1}$ 必须为零。\n\n因此，$z_1(x_{\\star})$ 表达式中的第二项是 $\\frac{1}{3} \\cdot 0 = 0$。\n\n最后，我们将这些值代回到 $z_1(x_{\\star})$ 的方程中：\n$$ z_1(x_{\\star}) = \\frac{1}{\\sqrt{1}} (\\sqrt{2} - 0) = \\sqrt{2} $$\n$x_{\\star}$ 的第一主坐标是 $\\sqrt{2}$。", "answer": "$$\\boxed{\\sqrt{2}}$$", "id": "3136620"}, {"introduction": "掌握了投影的基础后，我们可以探索KPCA一个更强大的应用：数据去噪。本练习将演示KPCA如何识别嘈杂数据中潜在的非线性结构，并引出“预映射问题”（pre-image problem）这一重要概念 [@problem_id:3136635]。通过将数据投影到主成分流形上，然后再将其重构回原始输入空间，我们可以有效地滤除噪声，恢复数据背后的真实模式。", "problem": "您必须实现一个完整的程序，构建一个玩具数据集，其中核主成分分析（KPCA）的第一个主成分对应于一个已知的非线性函数，然后使用原像近似来恢复该函数。该已知非线性函数为平方函数，定义为 $f(x) = x^{2}$。数据集由二维点 $(x,y)$ 组成，其中 $y = f(x) + \\varepsilon$，$\\varepsilon$ 是加性高斯噪声。您将使用径向基函数（RBF）核，并在 KPCA 中保留单个主成分，这将产生一条一维非线性主曲线，预期其将遵循 $y \\approx f(x)$ 的图像。然后，您将计算每个训练点投影到该第一主成分上之后所得的原像，以获得 $y \\approx f(x)$ 的去噪估计。\n\n基本出发点和定义：\n- 核主成分分析（KPCA）在一个数据集 $\\{x_{i}\\}_{i=1}^{n}$ 上通过一个由特征映射 $\\phi(\\cdot)$ 导出的半正定核 $k(x,z) = \\langle \\phi(x), \\phi(z) \\rangle$ 以及中心化核矩阵 $K_{c}$ 来定义。中心化核由 $K_{c} = H K H$ 给出，其中 $H = I_{n} - \\frac{1}{n}\\mathbf{1}\\mathbf{1}^{\\top}$ 且 $K_{ij} = k(x_{i}, x_{j})$。\n- 径向基函数（RBF）核定义为 $k(x,z) = \\exp\\!\\left(-\\frac{\\lVert x - z \\rVert^{2}}{2\\sigma^{2}}\\right)$，其中 $\\sigma > 0$ 是带宽参数。\n- KPCA 主成分通过求解特征值问题 $K_{c} v_{\\ell} = \\lambda_{\\ell} v_{\\ell}$ 获得，并进行排序，使得 $\\lambda_{1} \\ge \\lambda_{2} \\ge \\cdots \\ge \\lambda_{n} \\ge 0$，其中 $v_{\\ell}$ 被归一化为单位欧几里得范数。对于一个训练点 $x_{k}$，其在主成分 $\\ell$ 上的坐标是 $y_{\\ell}(x_{k}) = \\sqrt{\\lambda_{\\ell}}\\, v_{\\ell,k}$。\n- 训练点 $x_{k}$ 在特征空间中的秩-$m$ 重建可以写作 $\\hat{\\Phi}(x_{k}) = \\sum_{i=1}^{n} a_{i}^{(k)} \\Phi_{c}(x_{i})$，其中 $a_{i}^{(k)} = \\sum_{\\ell=1}^{m} v_{\\ell,i} v_{\\ell,k}$。在本问题中，您必须使用 $m = 1$，因此 $a_{i}^{(k)} = v_{1,i} v_{1,k}$。\n- 原像问题旨在寻找一个 $\\hat{x}$，使得 $\\phi(\\hat{x})$ 在特征空间中尽可能接近 $\\hat{\\Phi}(x_{k})$。对于 RBF 核，一个经过充分检验的原像不动点近似方法是\n$$\n\\hat{x}^{(t+1)} = \\frac{\\sum_{j=1}^{n} \\gamma_{j}^{(t)} x_{j}}{\\sum_{j=1}^{n} \\gamma_{j}^{(t)}}, \n\\quad \\text{其中} \\quad \n\\gamma_{j}^{(t)} = a_{j}^{(k)} \\exp\\!\\left(-\\frac{\\lVert \\hat{x}^{(t)} - x_{j} \\rVert^{2}}{2\\sigma^{2}}\\right).\n$$\n您应以 $\\hat{x}^{(0)} = x_{k}$ 进行初始化，并迭代直至收敛。\n\n程序要求：\n1. 数据生成：对于每个测试用例，生成 $n$ 个独立样本 $x_{i} \\sim \\mathrm{Uniform}([-1,1])$，设置 $y_{i}^{\\mathrm{true}} = f(x_{i}) = x_{i}^{2}$，并观测 $y_{i} = y_{i}^{\\mathrm{true}} + \\varepsilon_{i}$，其中 $\\varepsilon_{i} \\sim \\mathcal{N}(0, \\sigma_{\\mathrm{noise}}^{2})$。构建数据点 $X_{i} = (x_{i}, y_{i}) \\in \\mathbb{R}^{2}$。\n2. 核计算：对二维输入 $X_{i} \\in \\mathbb{R}^{2}$ 使用 RBF 核 $k(x,z) = \\exp\\!\\left(-\\frac{\\lVert x - z \\rVert^{2}}{2\\sigma^{2}}\\right)$。计算 $n \\times n$ 的核矩阵 $K$ 并将其中心化以获得 $K_{c}$。\n3. KPCA：求解 $K_{c} v_{\\ell} = \\lambda_{\\ell} v_{\\ell}$ 并选择具有最大特征值 $\\lambda_{1}$ 的第一个主成分 $(\\lambda_{1}, v_{1})$。使用 $m=1$。\n4. 重建系数：对于每个训练索引 $k \\in \\{1,\\dots,n\\}$，构建 $a^{(k)} \\in \\mathbb{R}^{n}$，其元素为 $a_{i}^{(k)} = v_{1,i} v_{1,k}$。\n5. 原像近似：对于每个 $k$，通过不动点迭代计算 $\\hat{x}_{k} \\in \\mathbb{R}^{2}$\n$$\n\\hat{x}_{k}^{(t+1)} = \\frac{\\sum_{j=1}^{n} \\gamma_{j}^{(t)} X_{j}}{\\sum_{j=1}^{n} \\gamma_{j}^{(t)}}, \n\\quad \\gamma_{j}^{(t)} = a_{j}^{(k)} \\exp\\!\\left(-\\frac{\\lVert \\hat{x}_{k}^{(t)} - X_{j} \\rVert^{2}}{2\\sigma^{2}}\\right),\n$$\n以 $\\hat{x}_{k}^{(0)} = X_{k}$ 初始化，迭代直至 $\\lVert \\hat{x}_{k}^{(t+1)} - \\hat{x}_{k}^{(t)} \\rVert_{2}  \\varepsilon$ 或达到最大迭代次数 $T$。使用容差 $\\varepsilon = 10^{-8}$ 和 $T = 200$。\n6. 函数恢复与评估：使用 $\\hat{x}_{k}$ 的第二个坐标作为 $\\hat{y}_{k}$，即您对 $f(x_{k})$ 的估计。计算去噪前和去噪后的均方根误差（RMSE）：\n$$\n\\mathrm{RMSE}_{\\mathrm{before}} = \\sqrt{\\frac{1}{n}\\sum_{k=1}^{n} \\left(y_{k} - y_{k}^{\\mathrm{true}}\\right)^{2}}, \n\\quad \n\\mathrm{RMSE}_{\\mathrm{after}} = \\sqrt{\\frac{1}{n}\\sum_{k=1}^{n} \\left(\\hat{y}_{k} - y_{k}^{\\mathrm{true}}\\right)^{2}}.\n$$\n\n测试套件：\n在以下三个测试用例上运行您的程序，每个用例由 $(n, \\sigma, \\sigma_{\\mathrm{noise}}, s)$ 指定，其中 $s$ 是用于 $x_i$ 采样和高斯噪声的随机种子。\n- 用例 1：$(n, \\sigma, \\sigma_{\\mathrm{noise}}, s) = (80, 0.15, 0.05, 0)$。\n- 用例 2（边界情况，无噪声）：$(n, \\sigma, \\sigma_{\\mathrm{noise}}, s) = (60, 0.12, 0.0, 1)$。\n- 用例 3（更小带宽，更高噪声）：$(n, \\sigma, \\sigma_{\\mathrm{noise}}, s) = (80, 0.05, 0.10, 2)$。\n\n最终输出格式：\n- 对于每个用例，输出数对 $[\\mathrm{RMSE}_{\\mathrm{before}}, \\mathrm{RMSE}_{\\mathrm{after}}]$，并四舍五入到 6 位小数。\n- 您的程序应生成单行输出，其中包含结果，格式为方括号括起来的逗号分隔列表，例如 `[[a,b],[c,d],[e,f]]`，不含空格。\n\n角度单位、物理单位和百分比不适用于此问题；所有量均为无量纲实数。请遵循上述精确定义以确保科学真实性，不要使用任何未从所述基础推导出的即席启发式方法或捷径。", "solution": "用户提供了一个在统计学习领域中定义明确的计算问题。任务是实现核主成分分析（KPCA）以对合成数据集进行去噪，并评估其性能。该问题具有科学依据，内部一致，并包含解决所需的所有必要信息。我将逐步进行解答。\n\n核心目标是证明 KPCA 能够识别非线性数据流形——在本例中为抛物线——并且通过将含噪数据投影到这个学习到的流形上，然后在原始空间中重建这些点（即原像问题），我们可以有效地减少噪声。\n\n### 1. 数据生成\n\n第一步是根据问题规范生成合成数据集。对于每个测试用例，我们给定数据点数量 $n$、噪声标准差 $\\sigma_{\\mathrm{noise}}$ 和用于可复现性的随机种子 $s$。我们从区间 $[-1, 1]$ 上的均匀分布中生成 $n$ 个独立样本 $x_i$。真实的底层函数是 $f(x) = x^2$。“真实”数据点为 $(x_i, y_i^{\\mathrm{true}})$，其中 $y_i^{\\mathrm{true}} = x_i^2$。然后我们通过添加高斯噪声来创建观测数据集：观测点为 $X_i = (x_i, y_i) \\in \\mathbb{R}^2$，其中 $y_i = y_i^{\\mathrm{true}} + \\varepsilon_i$，$\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma_{\\mathrm{noise}}^2)$。这个数据集 $\\{X_i\\}_{i=1}^n$ 构成了一个带噪声的抛物线。\n\n### 2. 核矩阵构建与中心化\n\nKPCA 在一个由核函数隐式定义的高维特征空间中进行操作。指定的核函数是径向基函数（RBF）核：\n$$k(x, z) = \\exp\\left(-\\frac{\\lVert x - z \\rVert^2}{2\\sigma^2}\\right)$$\n其中 $\\sigma$ 是带宽参数。我们计算 $n \\times n$ 的格拉姆矩阵 $K$，其中每个元素 $K_{ij} = k(X_i, X_j)$，对应于我们在 $\\mathbb{R}^2$ 中的 $n$ 个数据点。\n\nPCA 要求数据以原点为中心。在特征空间中，这通过中心化核矩阵来实现。中心化核矩阵 $K_c$ 由下式给出：\n$$K_c = H K H$$\n其中 $H$ 是中心化矩阵 $H = I_n - \\frac{1}{n}\\mathbf{1}\\mathbf{1}^\\top$。这里，$I_n$ 是 $n \\times n$ 的单位矩阵，$\\mathbf{1}$ 是一个包含 $n$ 个 1 的列向量。此操作确保映射到特征空间的数据 $\\phi(X_i)$ 的均值为零。\n\n### 3. 核主成分提取\n\n特征空间中的主成分通过求解中心化核矩阵的特征值问题来找到：\n$$K_c v_\\ell = \\lambda_\\ell v_\\ell$$\n特征值 $\\lambda_\\ell$ 与每个主成分所捕获的方差成正比，而特征向量 $v_\\ell$（归一化为 $\\|\\cdot\\|_2=1$）包含了用映射数据点来定义主轴的系数。我们将特征值按降序排序，$\\lambda_1 \\ge \\lambda_2 \\ge \\dots \\ge \\lambda_n \\ge 0$。由于我们的数据主要沿着一条一维曲线（抛物线）分布，我们期望对应于最大特征值 $\\lambda_1$ 的第一个主成分能捕获这种主导结构。因此，问题指导我们只使用第一个主成分，这意味着我们只保留特征向量 $v_1$ 和特征值 $\\lambda_1$。\n\n### 4. 通过不动点迭代进行原像重建\n\n将数据投影到特征空间的第一个主成分上之后，我们的目标是找到在原始输入空间中对应的点。这就是“原像问题”。给定特征空间中的一个投影，我们寻求输入空间中的一个点 $\\hat{x}$，使得 $\\phi(\\hat{x})$ 接近这个投影。问题指定了一种不动点迭代方法来近似这个原像。\n\n一个点 $\\phi_c(X_k)$ 在第一个主成分上的投影由其重建给出，这涉及到系数 $a_i^{(k)}$。对于秩为1的重建（$m=1$），这些系数是：\n$$a_i^{(k)} = v_{1,i} v_{1,k}$$\n其中 $v_{1,i}$ 是第一个特征向量 $v_1$ 的第 $i$ 个元素。这些系数定义了所有训练点的中心化特征向量 $\\phi_c(X_i)$ 如何被加权以形成 $\\phi_c(X_k)$ 的重建。\n\n对于每个训练点 $X_k$，我们寻求其原像 $\\hat{x}_k$。我们用原始数据点初始化搜索，$\\hat{x}_k^{(0)} = X_k$，并使用以下更新规则进行迭代，对于 $t=0, 1, 2, \\dots$：\n$$\\hat{x}_k^{(t+1)} = \\frac{\\sum_{j=1}^{n} \\gamma_j^{(t)} X_j}{\\sum_{j=1}^{n} \\gamma_j^{(t)}}$$\n权重 $\\gamma_j^{(t)}$ 定义为：\n$$\\gamma_j^{(t)} = a_j^{(k)} \\exp\\left(-\\frac{\\lVert \\hat{x}_k^{(t)} - X_j \\rVert^2}{2\\sigma^2}\\right)$$\n这个迭代是一个加权平均更新。权重 $\\gamma_j^{(t)}$ 结合了两种效应：来自 PCA 投影的结构信息（通过 $a_j^{(k)}$）和基于局部性的加权（通过 RBF 核项），后者给予更接近当前原像估计 $\\hat{x}_k^{(t)}$ 的训练点 $X_j$ 更大的影响。迭代持续进行，直到估计值的变化可以忽略不计，即 $\\lVert \\hat{x}_k^{(t+1)} - \\hat{x}_k^{(t)} \\rVert_2  \\varepsilon = 10^{-8}$，或者达到最大迭代次数 $T=200$。\n\n### 5. 去噪性能评估\n\n对每个 $X_k$ 进行原像计算的结果是一个去噪点 $\\hat{x}_k = (\\hat{x}_{k,1}, \\hat{x}_{k,2})$。第二个坐标，我们可以称之为 $\\hat{y}_k = \\hat{x}_{k,2}$，是我们对原始带噪声的 $y_k$ 的去噪估计。\n\n为了量化性能，我们计算去噪过程前后 $y$ 坐标的均方根误差（RMSE）。“去噪前”的 RMSE 衡量了观测到的含噪数据相对于真实函数的误差：\n$$\\mathrm{RMSE}_{\\mathrm{before}} = \\sqrt{\\frac{1}{n}\\sum_{k=1}^{n} (y_k - y_k^{\\mathrm{true}})^2}$$\n“去噪后”的 RMSE 衡量了我们重建估计的误差：\n$$\\mathrm{RMSE}_{\\mathrm{after}} = \\sqrt{\\frac{1}{n}\\sum_{k=1}^{n} (\\hat{y}_k - y_k^{\\mathrm{true}})^2}$$\n$\\mathrm{RMSE}_{\\mathrm{after}}  \\mathrm{RMSE}_{\\mathrm{before}}$ 表示去噪成功。我们将对指定的三个测试用例执行这整个过程。", "answer": "```python\nimport numpy as np\nfrom scipy.linalg import eigh\nfrom scipy.spatial.distance import pdist, squareform\n\ndef run_kpca_denoising(n, sigma, sigma_noise, seed):\n    \"\"\"\n    Performs KPCA-based denoising for a single test case.\n\n    Args:\n        n (int): Number of data points.\n        sigma (float): RBF kernel bandwidth.\n        sigma_noise (float): Standard deviation of Gaussian noise.\n        seed (int): Random seed for reproducibility.\n\n    Returns:\n        list: A list containing [RMSE_before, RMSE_after].\n    \"\"\"\n    # 1. Data generation\n    rng = np.random.default_rng(seed)\n    x_coords = rng.uniform(-1, 1, n)\n    y_true = x_coords**2\n    noise = rng.normal(0, sigma_noise, n)\n    y_coords = y_true + noise\n    X = np.vstack((x_coords, y_coords)).T  # n x 2 matrix\n\n    # 6. Evaluation (before)\n    # The noise added is (y_coords - y_true), so RMSE_before is just the RMS of the noise\n    rmse_before = np.sqrt(np.mean((y_coords - y_true)**2))\n\n    # 2. Kernel computation\n    sq_dists = squareform(pdist(X, 'sqeuclidean'))\n    K = np.exp(-sq_dists / (2 * sigma**2))\n\n    # Center the kernel matrix\n    H = np.eye(n) - np.ones((n, n)) / n\n    Kc = H @ K @ H\n\n    # 3. KPCA (Eigen-decomposition)\n    # eigh returns eigenvalues in ascending order\n    eigenvalues, eigenvectors = eigh(Kc)\n    \n    # Select the first principal component (corresponding to the largest eigenvalue)\n    # lambda_1 = eigenvalues[-1] # Not directly needed for pre-image\n    v1 = eigenvectors[:, -1]\n\n    # 5. Pre-image approximation\n    X_hat = np.zeros_like(X)\n    T_max = 200\n    epsilon = 1e-8\n    \n    for k in range(n):\n        # 4. Reconstruction coefficients for point k\n        a_k = v1 * v1[k]\n\n        # Initialize pre-image iteration\n        x_hat_t = X[k, :] \n        \n        for _ in range(T_max):\n            # Compute distances from current estimate to all training points\n            dists_sq_t = np.sum((x_hat_t - X)**2, axis=1)\n            \n            # Compute gamma weights\n            gamma_t = a_k * np.exp(-dists_sq_t / (2 * sigma**2))\n            \n            # Compute next estimate\n            numerator = np.sum(gamma_t[:, np.newaxis] * X, axis=0)\n            denominator = np.sum(gamma_t)\n            \n            # Add small epsilon for numerical stability\n            if np.abs(denominator)  1e-12:\n                denominator += 1e-12\n                \n            x_hat_t_plus_1 = numerator / denominator\n            \n            # Check for convergence\n            if np.linalg.norm(x_hat_t_plus_1 - x_hat_t)  epsilon:\n                x_hat_t = x_hat_t_plus_1\n                break\n                \n            x_hat_t = x_hat_t_plus_1\n        \n        X_hat[k, :] = x_hat_t\n\n    # 6. Evaluation (after)\n    y_hat = X_hat[:, 1]\n    rmse_after = np.sqrt(np.mean((y_hat - y_true)**2))\n\n    return [round(rmse_before, 6), round(rmse_after, 6)]\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    test_cases = [\n        # (n, sigma, sigma_noise, seed)\n        (80, 0.15, 0.05, 0),\n        (60, 0.12, 0.0, 1),\n        (80, 0.05, 0.10, 2),\n    ]\n\n    results = []\n    for case in test_cases:\n        n, sigma, sigma_noise, seed = case\n        result = run_kpca_denoising(n, sigma, sigma_noise, seed)\n        results.append(result)\n\n    # Format output string manually to avoid spaces\n    # str(list) adds spaces, e.g., '[0.1, 0.2]'. We need '[0.1,0.2]'.\n    results_str = [str(r).replace(\" \", \"\") for r in results]\n    print(f\"[{','.join(results_str)}]\")\n\nsolve()\n```", "id": "3136635"}, {"introduction": "KPCA的威力源于核函数的选择，但不恰当的核函数可能会误导分析，使其放大噪声而非捕捉真实信号。本练习介绍了一些必要的诊断工具，如核对齐（kernel alignment）和碎石图分析（scree plot analysis），来批判性地评估所选核函数的性能 [@problem_id:3136607]。通过学习这些诊断方法，你将能够确保你的KPCA模型结果是可靠且有意义的，从而避免该方法的常见陷阱。", "problem": "您的任务是分析核主成分分析（KPCA）找到的首个主成分是否会因核选择不当而被噪声主导。该分析必须从主成分分析（PCA）和再生核希尔伯特空间（RKHS）理论的基本原理出发，并实现为一个完整的、可运行的程序。\n\n从以下基础出发：\n- PCA 寻找使数据方差最大化的标准正交方向，这些方向由协方差矩阵的特征向量给出。\n- 在 KPCA 中，数据通过与半正定核 $k(\\cdot, \\cdot)$ 相关联的特征映射 $\\Phi(\\cdot)$ 映射到特征空间，然后通过核技巧在该特征空间中执行 PCA。\n- Gram 矩阵 $K \\in \\mathbb{R}^{n \\times n}$ 的元素为 $K_{ij} = k(x_i, x_j)$，在特征空间中进行中心化对应于通过 $K_c = H K H$ 对 Gram 矩阵进行中心化，其中 $H = I - \\frac{1}{n} \\mathbf{1} \\mathbf{1}^\\top$。\n\n问题目标：\n1. 通过计算给定核的中心化 Gram 矩阵 $K_c$ 并通过特征分解得到其特征值 $\\lambda_1 \\ge \\lambda_2 \\ge \\cdots \\ge \\lambda_n$ 来实现 KPCA。在解释碎石图时，仅使用谱的非负部分：定义 $\\lambda_i^{+} = \\max(\\lambda_i, 0)$ 并计算首个主成分占比\n$$\ns = \\frac{\\max_i \\lambda_i^{+}}{\\sum_{i=1}^n \\lambda_i^{+}},\n$$\n约定当 $\\sum_i \\lambda_i^{+} = 0$ 时，$s = 0$。\n2. 计算用于 KPCA 的核与从生成数据的潜在标量 $z$ 导出的真实“信号核” $K_{\\text{sig}}$ 之间的中心化对齐度。在中心化核上使用归一化的 Frobenius 内积：\n$$\nA = \\frac{\\langle K_c, K_{\\text{sig},c} \\rangle_F}{\\|K_c\\|_F \\, \\|K_{\\text{sig},c}\\|_F},\n$$\n其中 $K_{\\text{sig},c} = H K_{\\text{sig}} H$，$\\langle A, B \\rangle_F = \\mathrm{trace}(A^\\top B)$，以及 $\\|A\\|_F = \\sqrt{\\langle A, A \\rangle_F}$。如果分母中的任何一个范数为零，则定义 $A = 0$。\n3. 根据以下规则诊断 KPCA 的首个主成分是否捕获了噪声：如果 $s \\ge \\tau_s$ 且 $A \\le \\tau_A$，则声明“捕获噪声”，阈值设为 $\\tau_s = 0.6$ 和 $\\tau_A = 0.2$。\n\n数据模型：\n- 对于每个测试用例，生成 $n$ 个独立的潜在变量 $z_i \\sim \\mathrm{Uniform}([-1, 1])$。\n- 观测值是二维向量 $x_i \\in \\mathbb{R}^2$，构造如下\n$$\nx_i = \\begin{bmatrix} z_i \\\\ 0 \\end{bmatrix} + \\begin{bmatrix} \\epsilon^{(x)}_i \\\\ \\epsilon^{(y)}_i \\end{bmatrix},\n$$\n其中 $\\epsilon^{(x)}_i \\sim \\mathcal{N}(0, \\sigma_x^2)$ 和 $\\epsilon^{(y)}_i \\sim \\mathcal{N}(0, \\sigma_y^2)$ 相互独立。\n- 真实的信号核是潜在变量上的线性核：$K_{\\text{sig}}(i,j) = z_i z_j$。\n\n待在观测数据上测试的核：\n- 线性核：$k(x, x') = x^\\top x'$。\n- 多项式核：$k(x, x') = (x^\\top x' + c)^d$，其中次数 $d \\in \\mathbb{N}$，偏移量 $c \\in \\mathbb{R}$。\n- 径向基函数 (RBF) 核：$k(x, x') = \\exp\\big(-\\gamma \\|x - x'\\|_2^2\\big)$，其中带宽参数 $\\gamma  0$。\n\n测试套件：\n提供一个单一程序，用于生成数据并对以下四个测试用例进行诊断评估。在每个用例中，使用固定的随机种子以保证可复现性。\n\n- 用例 1 (理想路径，信号主导且使用适当的核)：\n    - $n = 200$，$\\sigma_x = 0.05$，$\\sigma_y = 0.20$，核：线性核。\n- 用例 2 (不良路径，不当核放大了噪声)：\n    - $n = 200$，$\\sigma_x = 0.05$，$\\sigma_y = 1.00$，核：多项式核，参数 $d = 5$ 和 $c = 1.0$。\n- 用例 3 (不良路径，RBF 核过窄导致近似对角占优)：\n    - $n = 200$，$\\sigma_x = 0.05$，$\\sigma_y = 0.50$，核：RBF 核，参数 $\\gamma = 15.0$。\n- 用例 4 (边界情况，RBF 核过宽导致近似常数核)：\n    - $n = 60$，$\\sigma_x = 0.05$，$\\sigma_y = 1.00$，核：RBF 核，参数 $\\gamma = 0.001$。\n\n所需输出：\n- 对于每个用例，计算上文定义的碎石图首个主成分占比 $s$ 和中心化对齐度 $A$。\n- 为每个用例输出一个布尔值，指示“捕获噪声”条件是否成立：$s \\ge 0.6$ 且 $A \\le 0.2$。\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的、以逗号分隔的结果列表（例如，“[True,False,True,False]”）。\n\n本问题不涉及物理单位或角度。所有结果均为无量纲。", "solution": "该问题要求分析核主成分分析（KPCA），以确定在选择不当核时，其首个主成分是否会被噪声主导。这将通过从基本原理实现 KPCA，将其应用于具有已知信号和噪声结构的合成数据集，并评估一个特定的诊断规则来完成。该分析涉及统计学习、线性代数和再生核希尔伯特空间（RKHS）理论的概念。\n\n每个测试用例的总体流程如下：\n1.  生成一个包含 $n$ 个二维点 $x_i \\in \\mathbb{R}^2$ 的合成数据集。数据由一维潜在信号 $z_i$ 和加性高斯噪声构成。\n2.  在观测数据 $x_i$ 上使用指定的核函数（线性、多项式或 RBF）构建 Gram 矩阵 $K$。\n3.  对 Gram 矩阵 $K$ 进行中心化以获得 $K_c$，这等同于在特征空间中对数据进行中心化。\n4.  在潜在变量 $z_i$ 上使用线性核构建真实信号 Gram 矩阵 $K_{\\text{sig}}$，并将其中心化以得到 $K_{\\text{sig},c}$。\n5.  计算两个诊断指标：\n    a. 首个主成分占比 $s$，它衡量特征空间中最大主成分的主导程度。\n    b. 中心化对齐度 $A$，它衡量所选核捕获的结构（$K_c$）与真实信号结构（$K_{\\text{sig},c}$）之间的相似性。\n6.  应用诊断规则：如果首个主成分高度主导（$s \\ge 0.6$）但核与真实信号的对齐度很差（$A \\le 0.2$），我们则声明首个主成分捕获了噪声。\n\n**1. KPCA 的理论基础**\n\n主成分分析（PCA）识别一组能够捕获数据集中最大方差的标准正交轴。在 KPCA 中，这是对通过映射 $\\Phi: \\mathbb{R}^d \\to \\mathcal{F}$ 映射到高维特征空间 $\\mathcal{F}$ 的数据点执行的。假设数据已中心化为 $\\Phi(x_i) - \\bar{\\Phi}$，该空间中的协方差矩阵为 $C_{\\Phi} = \\frac{1}{n} \\sum_{i=1}^n (\\Phi(x_i) - \\bar{\\Phi})(\\Phi(x_i) - \\bar{\\Phi})^\\top$。\n\nKPCA 的核心是“核技巧”，它避免了对 $\\Phi(x_i)$ 的显式计算。特征空间中的内积由核函数给出：$k(x_i, x_j) = \\langle \\Phi(x_i), \\Phi(x_j) \\rangle_{\\mathcal{F}}$。在 $\\mathcal{F}$ 中的 PCA 特征值问题 $C_{\\Phi} v = \\lambda v$ 可以重构为 $n \\times n$ Gram 矩阵 $K$ 的特征值问题，其中 $K_{ij} = k(x_i, x_j)$。\n\n在 $\\mathcal{F}$ 中对数据进行中心化是通过变换 Gram 矩阵来实现的。中心化的 Gram 矩阵是 $K_c = H K H$，其中 $H = I - \\frac{1}{n} \\mathbf{1} \\mathbf{1}^\\top$ 是几何中心化矩阵，$I$ 是 $n \\times n$ 的单位矩阵，$\\mathbf{1}$ 是一个包含 $n$ 个 1 的列向量。$K_c$ 的特征值 $\\lambda'_i$ 与特征空间协方差矩阵 $C_{\\Phi}$ 的特征值 $\\lambda_i$ 成正比（具体来说，$\\lambda'_i = n \\lambda_i$）。问题陈述将 $K_c$ 的特征值称为 $\\lambda_i$，我们将遵循此约定。\n\n**2. 数据生成与核矩阵**\n\n对于每个具有参数 $n$、$\\sigma_x$ 和 $\\sigma_y$ 的测试用例，我们按以下方式生成数据：\n-   潜在信号：$z_i \\sim \\mathrm{Uniform}([-1, 1])$，其中 $i = 1, \\dots, n$。\n-   观测数据：$x_i = \\begin{bmatrix} z_i \\\\ 0 \\end{bmatrix} + \\begin{bmatrix} \\epsilon^{(x)}_i \\\\ \\epsilon^{(y)}_i \\end{bmatrix}$，其中 $\\epsilon^{(x)}_i \\sim \\mathcal{N}(0, \\sigma_x^2)$ 且 $\\epsilon^{(y)}_i \\sim \\mathcal{N}(0, \\sigma_y^2)$。第一个维度是信号加噪声，而第二个维度是纯噪声。\n\n根据观测数据 $X = \\{x_1, \\dots, x_n\\}$，我们基于指定的核计算 Gram 矩阵 $K$：\n-   **线性核**：$K_{ij} = x_i^\\top x_j$。\n-   **多项式核**：$K_{ij} = (x_i^\\top x_j + c)^d$。\n-   **RBF 核**：$K_{ij} = \\exp(-\\gamma \\|x_i - x_j\\|_2^2)$。\n\n真实的信号核是潜在变量上的线性核：$K_{\\text{sig},ij} = z_i z_j$。该矩阵代表了我们旨在恢复的理想协方差结构。\n\n**3. 诊断指标**\n\n**首个主成分占比 ($s$):**\n该指标量化了方差在首个主成分中的集中程度。在计算中心化 Gram 矩阵 $K_c$ 后，我们找到其特征值 $\\lambda_1 \\ge \\lambda_2 \\ge \\dots \\ge \\lambda_n$。由于 $K_c$ 是对称的，其所有特征值都是实数。我们只考虑谱的非负部分，定义 $\\lambda_i^+ = \\max(\\lambda_i, 0)$。占比 $s$ 为：\n$$\ns = \\frac{\\max_i \\lambda_i^{+}}{\\sum_{i=1}^n \\lambda_i^{+}}\n$$\n高 $s$ 值表示某个成分主导了特征空间方差。如果分母为零，我们约定 $s=0$。\n\n**中心化对齐度 ($A$):**\n该指标在考虑中心化后，衡量用于分析的核 ($K$) 与真实底层信号结构 ($K_{\\text{sig}}$) 的符合程度。它是中心化矩阵 $K_c$ 和 $K_{\\text{sig},c}$（被视为 $\\mathbb{R}^{n \\times n}$ 中的向量）之间的余弦相似度，使用 Frobenius 内积计算：\n$$\nA = \\frac{\\langle K_c, K_{\\text{sig},c} \\rangle_F}{\\|K_c\\|_F \\, \\|K_{\\text{sig},c}\\|_F}\n$$\n其中 $\\langle A, B \\rangle_F = \\mathrm{trace}(A^\\top B)$ 且 $\\|A\\|_F = \\sqrt{\\langle A, A \\rangle_F}$。对齐度 $A$ 的取值范围在 $[-1, 1]$ 内。接近 1 的值表示高度相似。如果分母中的任何一个范数为零，我们约定 $A=0$。\n\n**4. 测试用例分析**\n\n-   **用例 1 (线性核，低噪声)：** 线性核适用于数据的线性信号结构。我们预期会有很高的对齐度 $A$。噪声较低，因此信号应是方差的主要来源。首个主成分占比 $s$ 将会很显著但不会过高，因为仍然存在噪声。条件 $s \\ge 0.6$ 且 $A \\le 0.2$ 预计不会成立。\n\n-   **用例 2 (多项式核，高噪声)：** 噪声 $\\sigma_y$ 很大，并且使用了高阶 ($d=5$) 多项式核。该核会展开诸如 $(x_i^\\top x_j + c)^d = (x_{i1} x_{j1} + x_{i2} x_{j2} + c)^d$ 的项。项 $x_{i2}x_{j2}$ 只包含噪声。将其提升到 5 次方会放大与噪声相关的结构，使其在核矩阵中占主导地位。这将导致与信号核的对齐度 $A$ 很低。如果这个噪声结构本身是低秩的（例如，由少数几个大的噪声值主导），它可能会在 $K_c$ 中产生一个大的首个特征值，从而得到一个高的 $s$。这个用例旨在触发“捕获噪声”诊断。\n\n-   **用例 3 (窄 RBF 核)：** 大的带宽参数 $\\gamma=15.0$ 使得 RBF 核 $\\exp(-\\gamma \\|x_i - x_j\\|^2)$ 高度局部化。核函数的值仅对于非常接近的点对 $(x_i, x_j)$ 才不可忽略。点之间的邻近性可能由噪声而非信号引起。如果偶然有几个数据点形成一个与其余点分离的紧密簇，核矩阵将通过该块对应的大值来反映这一点。这会产生一个对应于此伪簇的主导特征向量，导致高的 $s$。由于这种结构是由噪声驱动的，与信号的对齐度 $A$ 将会很低。这是另一个预计会触发诊断的场景。\n\n-   **用例 4 (宽 RBF 核)：** 非常小的 $\\gamma=0.001$ 使得核对距离不敏感。$k(x_i, x_j) \\approx 1 - \\gamma \\|x_i - x_j\\|^2$。Gram 矩阵 $K$ 变得接近一个全为 1 的秩一矩阵，$K \\approx \\mathbf{1}\\mathbf{1}^\\top$。中心化后，$K_c = H K H \\approx H (\\mathbf{1}\\mathbf{1}^\\top) H = \\mathbf{0}$，因为 $H\\mathbf{1} = \\mathbf{0}$。$K_c$ 的特征值将非常接近于零，导致小的 $\\|K_c\\|_F$ 和可能很小或不稳定的 $s$。对齐度 $A$ 可能很小。诊断预计不会被触发，但这个用例测试了在这种极限情况下的数值行为。该问题被正确定位为一个边界情况。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the KPCA noise analysis on the specified test cases.\n    \"\"\"\n    \n    # Define test cases as a list of dictionaries.\n    test_cases = [\n        {\n            \"name\": \"Case 1: Happy path (linear kernel)\",\n            \"n\": 200, \"sigma_x\": 0.05, \"sigma_y\": 0.20,\n            \"kernel_type\": \"linear\", \"kernel_params\": {},\n            \"seed\": 0\n        },\n        {\n            \"name\": \"Case 2: Bad path (inappropriate polynomial kernel)\",\n            \"n\": 200, \"sigma_x\": 0.05, \"sigma_y\": 1.00,\n            \"kernel_type\": \"poly\", \"kernel_params\": {\"d\": 5, \"c\": 1.0},\n            \"seed\": 1\n        },\n        {\n            \"name\": \"Case 3: Bad path (narrow RBF kernel)\",\n            \"n\": 200, \"sigma_x\": 0.05, \"sigma_y\": 0.50,\n            \"kernel_type\": \"rbf\", \"kernel_params\": {\"gamma\": 15.0},\n            \"seed\": 2\n        },\n        {\n            \"name\": \"Case 4: Edge case (wide RBF kernel)\",\n            \"n\": 60, \"sigma_x\": 0.05, \"sigma_y\": 1.00,\n            \"kernel_type\": \"rbf\", \"kernel_params\": {\"gamma\": 0.001},\n            \"seed\": 3\n        }\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        result = run_kpca_diagnostic(\n            n=case[\"n\"],\n            sigma_x=case[\"sigma_x\"],\n            sigma_y=case[\"sigma_y\"],\n            kernel_type=case[\"kernel_type\"],\n            kernel_params=case[\"kernel_params\"],\n            seed=case[\"seed\"]\n        )\n        results.append(result)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef run_kpca_diagnostic(n, sigma_x, sigma_y, kernel_type, kernel_params, seed):\n    \"\"\"\n    Performs the KPCA diagnostic for a single case.\n    \"\"\"\n    # Set seed for reproducibility of this specific case\n    np.random.seed(seed)\n    \n    # 1. Data Generation\n    z = np.random.uniform(-1, 1, n)\n    eps_x = np.random.normal(0, sigma_x, n)\n    eps_y = np.random.normal(0, sigma_y, n)\n    \n    x_coords = z + eps_x\n    y_coords = eps_y\n    X = np.stack([x_coords, y_coords], axis=1)\n\n    # 2. Kernel Matrix Construction\n    K = compute_kernel_matrix(X, kernel_type, kernel_params)\n\n    # 3. Signal Kernel Construction\n    K_sig = np.outer(z, z)\n\n    # 4. Centering\n    H = np.eye(n) - (1/n) * np.ones((n, n))\n    K_c = H @ K @ H\n    K_sig_c = H @ K_sig @ H\n\n    # 5. Compute Top-Component Share (s)\n    eigenvalues = np.linalg.eigh(K_c)[0]\n    eigenvalues_plus = np.maximum(eigenvalues, 0)\n    sum_eig = np.sum(eigenvalues_plus)\n    \n    if sum_eig > 1e-12: # Use a tolerance for floating point comparison\n        max_eig = np.max(eigenvalues_plus)\n        s = max_eig / sum_eig\n    else:\n        s = 0.0\n\n    # 6. Compute Centered Alignment (A)\n    norm_Kc_F = np.linalg.norm(K_c, 'fro')\n    norm_K_sig_c_F = np.linalg.norm(K_sig_c, 'fro')\n    \n    denominator = norm_Kc_F * norm_K_sig_c_F\n    if denominator > 1e-12:\n        # Since Kc and K_sig_c are symmetric, tr(A^T B) = sum(A * B)\n        inner_prod_F = np.sum(K_c * K_sig_c)\n        A = inner_prod_F / denominator\n    else:\n        A = 0.0\n\n    # 7. Apply Diagnostic Rule\n    tau_s = 0.6\n    tau_A = 0.2\n    noise_captured = (s >= tau_s) and (A = tau_A)\n    \n    return noise_captured\n\ndef compute_kernel_matrix(X, kernel_type, params):\n    \"\"\"\n    Computes the Gram matrix for the given data and kernel.\n    \"\"\"\n    if kernel_type == \"linear\":\n        return X @ X.T\n    \n    elif kernel_type == \"poly\":\n        d = params[\"d\"]\n        c = params[\"c\"]\n        return (X @ X.T + c)**d\n    \n    elif kernel_type == \"rbf\":\n        gamma = params[\"gamma\"]\n        # Compute pairwise squared Euclidean distances without scipy\n        # (a-b)^2 = a^2 - 2ab + b^2\n        sq_norms = np.sum(X**2, axis=1)\n        dist_sq = sq_norms[:, np.newaxis] + sq_norms[np.newaxis, :] - 2 * (X @ X.T)\n        # Ensure non-negativity due to potential float errors\n        dist_sq = np.maximum(dist_sq, 0)\n        return np.exp(-gamma * dist_sq)\n    \n    else:\n        raise ValueError(f\"Unknown kernel type: {kernel_type}\")\n\nsolve()\n```", "id": "3136607"}]}