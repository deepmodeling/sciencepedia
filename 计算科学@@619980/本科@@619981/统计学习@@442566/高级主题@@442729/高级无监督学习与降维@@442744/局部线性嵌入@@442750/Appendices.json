{"hands_on_practices": [{"introduction": "第一个动手实践将指导你从基本原理出发，完整实现局部线性嵌入（LLE）算法。通过亲手构建算法，你将深入理解其核心机制：寻找近邻、计算重构权重以及构建最终的嵌入。这项练习 [@problem_id:3141684] 还将挑战你去探究不同的数据缩放方法如何影响嵌入结果，从而让你深刻认识到为什么像特征标准化这样的预处理步骤对于流形学习算法至关重要。", "problem": "实现一个完整的程序，从第一性原理出发，为给定的点云构建局部线性嵌入（LLE），并用它来定量比较不同输入缩放下的嵌入。您的程序不得依赖任何外部机器学习库。它必须只使用线性代数、欧几里得几何和特征分解。按如下纯数学术语进行操作。\n\n用作基本基础的定义：\n- 给定一个点云，包含 $N$ 个 $D$ 维样本，排列成矩阵 $X \\in \\mathbb{R}^{N \\times D}$，一个样本的 $k$ 个最近邻是那些欧几里得距离最小的样本（距离相等时可通过索引顺序打破）。对于任意向量 $x,y \\in \\mathbb{R}^{D}$，欧几里得距离为 $\\lVert x - y \\rVert_{2}$。\n- 对于每个样本 $x_i \\in \\mathbb{R}^{D}$，其邻居索引为 $\\mathcal{N}(i)$，定义局部最小二乘重构问题，以找到重构权重 $w_{ij}$ 来最小化平方误差，同时满足权重和为一的约束。使用由邻居差分构建的局部数据矩阵来表述此问题。\n- 从局部重构中组合出全局成本，并通过求解一个从权重导出的特征分解问题来恢复低维嵌入 $Y \\in \\mathbb{R}^{N \\times d}$。使用与最小特征值相关联的非平凡特征向量（不包括与平凡常数模式相关联的那个）。\n- 比较两个仅在刚性变换和均匀缩放意义下定义的嵌入的标准工具是正交 Procrustes 分析。为了比较 $Y_1, Y_2 \\in \\mathbb{R}^{N \\times d}$，将每个嵌入中心化至零均值，归一化至单位 Frobenius 范数，通过奇异值分解（SVD）计算最优正交对齐，并报告对齐后的残差平方和作为差异度。这会得出一个非负实数；数值越小表示越接近。\n\n需要探索的不变性概念：\n- 均匀全局缩放：对于一个标量 $s \\in \\mathbb{R}_{+}$，将 $X$ 替换为 $s X$ 会使所有成对距离乘以相同的因子。研究这将如何影响邻居集、局部重构和最终嵌入。\n- 各向异性特征缩放：对于一个具有不等正元素的对角矩阵 $A \\in \\mathbb{R}^{D \\times D}$，将 $X$ 替换为 $X A$ 会对每个特征进行不同程度的重新缩放。研究这将如何影响邻居集、局部重构和最终嵌入。\n- 按特征标准化：通过使用在 $N$ 个样本上按特征计算的经验均值和方差，将每个特征标准化为零均值和单位方差，从而将 $X$ 变换为 $\\tilde{X}$。\n\n您的实现的算法要求：\n- 使用完整数据集上的精确欧几里得距离实现 $k$-最近邻。\n- 对每个点 $x_i$，提出并求解用其 $k$ 个邻居重构 $x_i$ 的约束最小二乘问题，权重和约束为一。为确保局部 Gram 矩阵病态时的数值稳定性，添加一个 Tikhonov 正则化项，其大小与该矩阵的迹乘以一个小的标量 $\\epsilon$ 成正比。\n- 从权重中组合出全局矩阵，并将维度为 $d$ 的嵌入计算为由重构算子产生的对称半正定矩阵的 $d$ 个最小非平凡特征值所对应的特征向量。\n- 使用线性代数和奇异值分解实现上述的 Procrustes 对齐。\n\n使用以下固定的数据和参数作为测试套件。所有角度都是无量纲的；没有物理单位。所有数值必须被视为精确的标量。\n\n各测试的共享参数：\n- 邻居数 $k = 8$。\n- 目标嵌入维度 $d = 2$。\n- 正则化参数 $\\epsilon = 10^{-3}$，通过将单位矩阵缩放局部 Gram 矩阵的迹的 $\\epsilon$ 倍来使用。\n- 每个布尔测试的 Procrustes 差异度阈值如下所示。\n\n数据集：\n1) 三维空间中的二维弯曲流形。\n- 构建一个 $m = 11$ 的规则网格，使得 $u$ 和 $v$ 各自取遍 $[0,1]$ 区间内的 $m$ 个等间距值，即 $\\{0, \\frac{1}{m-1}, \\ldots, 1\\}$。将所有 $N = m^2$ 个点对 $(u,v)$ 按字典序堆叠成一个矩阵 $U \\in \\mathbb{R}^{N \\times 2}$。\n- 通过 $x = \\begin{bmatrix} u  v  u^2 + v^2 \\end{bmatrix}$ 将每个 $(u,v)$ 映射到 $\\mathbb{R}^{3}$ 中的一个点，形成 $X_{\\text{base}} \\in \\mathbb{R}^{N \\times 3}$。\n- 定义三个变换后的版本：\n  a) 全局缩放：$X_{\\text{glob}} = s X_{\\text{base}}$，其中 $s = 7$。\n  b) 强各向异性特征缩放：$X_{\\text{aniso}} = X_{\\text{base}} A$，其中 $A = \\mathrm{diag}(100, 0.01, 50)$。\n  c) 按特征标准化：$\\mathrm{Std}(X)$ 表示按特征标准化为零均值和单位方差。计算 $X_{\\text{std-base}} = \\mathrm{Std}(X_{\\text{base}})$ 和 $X_{\\text{std-aniso}} = \\mathrm{Std}(X_{\\text{aniso}})$。\n\n2) 三维空间中近似平衡的线性流形。\n- 使用与上面相同的 $U$，通过 $x = \\begin{bmatrix} u  v  \\frac{u+v}{2} \\end{bmatrix}$ 定义 $X_{\\text{bal}} \\in \\mathbb{R}^{N \\times 3}$ 及其标准化版本 $X_{\\text{std-bal}} = \\mathrm{Std}(X_{\\text{bal}})$。\n\n对每个数据集 $X$，计算 LLE 嵌入 $Y \\in \\mathbb{R}^{N \\times d}$。\n\n要计算的定量比较：\n- 令 $\\Delta(\\cdot,\\cdot)$ 表示两个相同数据集大小的嵌入之间的 Procrustes 差异度。\n- 计算以下五个布尔测试结果：\n  1) $b_1$：在弯曲流形上对均匀全局缩放的不变性。要求 $b_1 = [\\Delta(Y_{\\text{base}}, Y_{\\text{glob}})  10^{-6}]$。\n  2) $b_2$：在弯曲流形上对各向异性特征缩放的敏感性。要求 $b_2 = [\\Delta(Y_{\\text{base}}, Y_{\\text{aniso}})  2 \\times 10^{-2}]$。\n  3) $b_3$：在弯曲流形上，标准化能消除各向异性，只留下微小残差。要求 $b_3 = [\\Delta(Y_{\\text{std-base}}, Y_{\\text{std-aniso}})  2 \\times 10^{-2}]$。\n  4) $b_4$：对于弯曲流形，与经过严重各向异性缩放的输入相比，对原始输入进行标准化能改善其与原始嵌入的接近程度。要求 $b_4 = [\\Delta(Y_{\\text{base}}, Y_{\\text{std-base}})  \\Delta(Y_{\\text{base}}, Y_{\\text{aniso}})]$。\n  5) $b_5$：在近似平衡的线性流形上，标准化不会显著改变嵌入。要求 $b_5 = [\\Delta(Y_{\\text{bal}}, Y_{\\text{std-bal}})  5 \\times 10^{-3}]$。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的、逗号分隔的五个布尔结果列表，顺序为 $[b_1, b_2, b_3, b_4, b_5]$。例如，一个有效的输出行可能看起来像 $[\\mathrm{True},\\mathrm{False},\\mathrm{True},\\mathrm{True},\\mathrm{True}]$，但其中的布尔值应为您的程序计算出的实际值。\n\n注意：\n- 所有计算必须是确定性的。不要使用随机性。\n- 不涉及角度。此问题中没有物理单位。", "solution": "用户要求从第一性原理出发实现局部线性嵌入（LLE）算法，以研究其对不同特征缩放变换的敏感性。该实现必须遵守一组特定的算法和参数定义。\n\n整个过程分为三个主要部分：LLE 算法本身、一种用于比较嵌入几何形状的方法（Procrustes 分析），以及在生成的数据集上进行的一系列定量测试。\n\n**1. 局部线性嵌入（LLE）算法**\n\nLLE 算法通过保留局部线性重构属性来构建高维数据集的低维嵌入。对于给定的数据集 $X \\in \\mathbb{R}^{N \\times D}$，该算法分三步进行。\n\n**第 1 步：邻居识别**\n对每个数据点 $x_i$，我们识别其 $k$ 个最近邻。邻域 $\\mathcal{N}(i)$ 由具有最小欧几里得距离 $\\lVert x_i - x_j \\rVert_2$ 的 $k$ 个点 $x_j$ ($j \\neq i$) 组成。为确保确定性结果，任何距离相等的情况都通过偏向索引较小的点 $j$ 来解决。\n\n**第 2 步：局部重构权重计算**\nLLE 的核心假设是每个点 $x_i$ 都可以通过其邻居的线性组合很好地近似。我们寻求一组重构权重 $W_{ij}$，以最小化总重构误差，并满足两个约束条件：如果 $x_j$ 不是 $x_i$ 的邻居，则 $W_{ij} = 0$；并且每个点的权重总和必须为一，即 $\\sum_{j \\in \\mathcal{N}(i)} W_{ij} = 1$。这导致了针对每个点 $x_i$ 的约束最小二乘问题：\n$$ \\min_{W} \\left\\| x_i - \\sum_{j \\in \\mathcal{N}(i)} W_{ij} x_j \\right\\|^2 \\quad \\text{subject to} \\quad \\sum_{j \\in \\mathcal{N}(i)} W_{ij} = 1 $$\n该问题对输入数据的旋转、平移和均匀缩放具有不变性。通过代入约束条件，目标可以重写为找到点 $x_i$ 的权重 $w \\in \\mathbb{R}^k$，以最小化：\n$$ \\left\\| \\sum_{j=1}^k w_j (x_i - x_{\\eta_j}) \\right\\|^2 = w^T G_i w $$\n其中 $\\{x_{\\eta_j}\\}_{j=1}^k$ 是 $x_i$ 的邻居，而 $G_i$ 是局部 Gram 矩阵。$G_i$ 的元素是 $(G_i)_{jk} = (x_i-x_{\\eta_j})^T(x_i-x_{\\eta_k})$。最优权重 $w$ 的解可以通过求解线性系统 $G_i z = \\mathbf{1}$（其中 $\\mathbf{1}$ 是全一向量）然后归一化 $w = z / \\sum z_l$ 来找到。\n\n为处理病态的 Gram 矩阵，添加了 Tikhonov 正则化项。问题指定添加一个与 Gram 矩阵的迹成比例的项，这使得正则化本身具有尺度不变性。修改后的 Gram 矩阵 $G'_i$ 是：\n$$ G'_i = G_i + \\epsilon \\cdot \\mathrm{tr}(G_i) \\cdot I_k $$\n其中 $I_k$ 是 $k \\times k$ 单位矩阵，$\\epsilon = 10^{-3}$ 是一个小的标量。然后我们求解 $G'_i z = \\mathbf{1}$ 来找到权重 $z$。\n\n**第 3 步：全局嵌入计算**\n在计算出 $N \\times N$ 的权重矩阵 $W$（对于非邻居，$W_{ij}=0$）之后，我们寻求一个保留这些局部重构权重的低维嵌入 $Y \\in \\mathbb{R}^{N \\times d}$。这是通过最小化嵌入成本函数来实现的：\n$$ \\Phi(Y) = \\sum_{i=1}^N \\left\\| y_i - \\sum_{j=1}^N W_{ij} y_j \\right\\|^2 $$\n这个二次型可以表示为 $\\Phi(Y) = \\mathrm{Tr}(Y^T M Y)$，其中 $M = (I-W)^T(I-W)$。矩阵 $M$ 是对称且半正定的。为了在避免平凡的坍缩解（$Y=0$）的同时最小化 $\\Phi(Y)$，我们对 $Y$ 施加中心化和单位协方差约束。解由 $M$ 对应于其最小特征值的特征向量给出。$M$ 的最小特征值总是零（或数值上接近零），其对应的特征向量是全一向量。这代表一个平凡的常数嵌入，应被舍弃。所需的 $d$ 维嵌入 $Y$ 由与接下来 $d$ 个最小特征值（即第 2 到第 $d+1$ 个特征值）相关联的特征向量构成。\n\n**2. 用于嵌入比较的正交 Procrustes 分析**\n\n为了定量比较两个仅在刚性变换（旋转、反射）和均匀缩放意义下定义的嵌入 $Y_1, Y_2 \\in \\mathbb{R}^{N \\times d}$，我们使用正交 Procrustes 分析。差异度 $\\Delta(Y_1, Y_2)$ 是最优对齐后的最小平方差和。步骤如下：\n1.  **中心化**：平移 $Y_1$ 和 $Y_2$，使其质心位于原点。\n2.  **归一化**：缩放两个中心化后的矩阵，使其 Frobenius 范数为单位一，即 $\\|Y\\|_F = \\sqrt{\\sum_{i,j} Y_{ij}^2} = 1$。\n3.  **对齐**：通过对矩阵 $C = Y_1^T Y_2$ 进行奇异值分解（SVD），找到最小化 $\\|Y_1 R - Y_2\\|_F^2$ 的最优正交矩阵 $R$。如果 $C = U \\Sigma V^T$，则最优旋转为 $R=UV^T$。\n4.  **计算差异度**：最小平方残差，或称 Procrustes 差异度，可以高效地计算为 $\\Delta = \\|Y_1\\|_F^2 + \\|Y_2\\|_F^2 - 2 \\sum_i \\sigma_i$，其中 $\\sigma_i$ 是 $C$ 的奇异值。归一化后，这简化为 $\\Delta = 2(1 - \\sum_i \\sigma_i)$。较小的差异度表示嵌入之间有更高的相似性。\n\n**3. 数据集与定量测试**\n\n该分析在两个基础数据集的几种变换上进行，使用的参数为 $k=8$, $d=2$, 和 $\\epsilon=10^{-3}$。这些测试旨在探究 LLE 的已知属性：\n- $b_1$：测试对均匀全局缩放的不变性。理论上 LLE 是不变的，所以这应为真。\n- $b_2$：测试对各向异性特征缩放的敏感性。这是 LLE 的一个已知弱点，因此差异度应该很大。\n- $b_3$：测试按特征标准化是否能抵消各向异性的影响。对 $X_{\\text{base}}$ 和 $X_{\\text{aniso}}$ 的特征进行标准化会得到相同的数据集，因此它们的嵌入应该相同，差异度接近于零。\n- $b_4$：比较标准化与强各向异性缩放的效果。标准化预计是一种破坏性小得多的变换。\n- $b_5$：检查标准化对近似平衡的线性流形的影响。预计变化会很小，导致较小的差异度。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import eigh, svd\n\ndef solve():\n    \"\"\"\n    Implements Locally Linear Embedding (LLE) from first principles and uses it to\n    compare embeddings under different input scalings.\n    \"\"\"\n\n    # -----------------------------------------------------\n    # Algorithm Implementation based on Problem Description\n    # -----------------------------------------------------\n\n    def standardize(X):\n        \"\"\"Standardizes each feature of X to have zero mean and unit variance.\"\"\"\n        mean = np.mean(X, axis=0)\n        std = np.std(X, axis=0)\n        # Avoid division by zero for constant features\n        std[np.isclose(std, 0)] = 1.0\n        return (X - mean) / std\n\n    def find_neighbors(X, k):\n        \"\"\"Finds the k-nearest neighbors for each point in X.\"\"\"\n        N = X.shape[0]\n        # Calculate squared Euclidean distances to avoid sqrt\n        sum_X_sq = np.sum(X**2, axis=1)\n        dist_sq_matrix = np.add.outer(sum_X_sq, sum_X_sq) - 2 * (X @ X.T)\n        dist_sq_matrix[dist_sq_matrix  0] = 0.0 # for numerical stability\n        \n        # Argsort on distances to find nearest neighbors\n        # kind='stable' ensures ties are broken by index order as required\n        neighbor_indices = np.argsort(dist_sq_matrix, axis=1, kind='stable')\n        \n        # Return indices of k-nearest neighbors.\n        # Index 0 is the point itself, so we take from 1 to k+1.\n        return neighbor_indices[:, 1:k+1]\n\n    def compute_weights(X, neighbor_indices, k, epsilon):\n        \"\"\"Computes the LLE reconstruction weights for each point.\"\"\"\n        N = X.shape[0]\n        W = np.zeros((N, N))\n        \n        for i in range(N):\n            neighbors = neighbor_indices[i]\n            \n            # Local data matrix of neighbor differences\n            C_i = X[i] - X[neighbors]  # Uses broadcasting\n            \n            # Local Gram matrix\n            G_i = C_i @ C_i.T\n            \n            # Tikhonov regularization as specified\n            trace_G = np.trace(G_i)\n            reg_val = epsilon * trace_G\n            G_i += reg_val * np.identity(k)\n            \n            # Solve for weights by solving G_i * z = 1\n            ones = np.ones(k)\n            try:\n                z = np.linalg.solve(G_i, ones)\n            except np.linalg.LinAlgError:\n                # Fallback to pseudo-inverse if singular despite regularization\n                z = np.linalg.pinv(G_i) @ ones\n\n            # Normalize weights to sum to 1\n            w_i = z / np.sum(z)\n            \n            # Populate sparse weight matrix W\n            W[i, neighbors] = w_i\n            \n        return W\n\n    def compute_embedding(W, N, d):\n        \"\"\"Computes the low-dimensional embedding from the weight matrix.\"\"\"\n        # Construct the cost matrix M = (I-W)^T(I-W)\n        I = np.identity(N)\n        M = (I - W).T @ (I - W)\n        \n        # Find eigenvectors for the d smallest non-trivial eigenvalues.\n        # eigh returns eigenvalues in ascending order.\n        # We need eigenvectors for indices 1 up to and including d.\n        # This corresponds to the 2nd to (d+1)-th smallest eigenvalues.\n        _, eigvecs = eigh(M, subset_by_index=[1, d])\n        \n        return eigvecs\n\n    def procrustes_disparity(Y1, Y2):\n        \"\"\"Computes the orthogonal Procrustes disparity between two embeddings.\"\"\"\n        # 1. Center the embeddings to have zero mean.\n        Y1_c = Y1 - np.mean(Y1, axis=0)\n        Y2_c = Y2 - np.mean(Y2, axis=0)\n\n        # 2. Normalize to unit Frobenius norm.\n        norm1 = np.linalg.norm(Y1_c, 'fro')\n        norm2 = np.linalg.norm(Y2_c, 'fro')\n        Y1_norm = Y1_c / norm1 if norm1 > 1e-10 else Y1_c\n        Y2_norm = Y2_c / norm2 if norm2 > 1e-10 else Y2_c\n\n        # 3. Compute optimal alignment via SVD of the cross-covariance matrix.\n        M = Y1_norm.T @ Y2_norm\n        _, s, _ = svd(M)\n        \n        # 4. Compute disparity using the singular values.\n        # Disparity = ||Y1_norm||^2 + ||Y2_norm||^2 - 2 * tr(Sigma)\n        #           = 1 + 1 - 2 * sum(s) = 2 * (1 - sum(s))\n        disparity = 2.0 - 2.0 * np.sum(s)\n        return disparity\n\n    def run_lle(X, k, d, epsilon):\n        \"\"\"A wrapper function to run the full LLE pipeline.\"\"\"\n        neighbor_indices = find_neighbors(X, k)\n        W = compute_weights(X, neighbor_indices, k, epsilon)\n        N = X.shape[0]\n        Y = compute_embedding(W, N, d)\n        return Y\n\n    # ------------------\n    # Main Logic\n    # ------------------\n    \n    # Shared parameters from the problem description\n    k = 8\n    d = 2\n    epsilon = 1e-3\n    m = 11\n\n    # 1. Generate Datasets\n    u_vals = np.linspace(0.0, 1.0, m)\n    v_vals = np.linspace(0.0, 1.0, m)\n    uu, vv = np.meshgrid(u_vals, v_vals, indexing='ij')\n    U = np.stack([uu.ravel(), vv.ravel()], axis=1)\n\n    # Curved manifold datasets\n    X_base = np.column_stack((U[:, 0], U[:, 1], U[:, 0]**2 + U[:, 1]**2))\n    X_glob = 7.0 * X_base\n    A = np.diag([100.0, 0.01, 50.0])\n    X_aniso = X_base @ A\n    X_std_base = standardize(X_base)\n    X_std_aniso = standardize(X_aniso)\n\n    # Balanced linear manifold datasets\n    X_bal = np.column_stack((U[:, 0], U[:, 1], (U[:, 0] + U[:, 1]) / 2.0))\n    X_std_bal = standardize(X_bal)\n\n    datasets = {\n        \"base\": X_base, \"glob\": X_glob, \"aniso\": X_aniso,\n        \"std-base\": X_std_base, \"std-aniso\": X_std_aniso,\n        \"bal\": X_bal, \"std-bal\": X_std_bal,\n    }\n\n    # 2. Compute LLE Embeddings for all datasets\n    embeddings = {name: run_lle(X, k, d, epsilon) for name, X in datasets.items()}\n\n    # 3. Perform Quantitative Comparisons\n    Y_base, Y_glob, Y_aniso = embeddings[\"base\"], embeddings[\"glob\"], embeddings[\"aniso\"]\n    Y_std_base, Y_std_aniso = embeddings[\"std-base\"], embeddings[\"std-aniso\"]\n    Y_bal, Y_std_bal = embeddings[\"bal\"], embeddings[\"std-bal\"]\n\n    # Test 1: Invariance to uniform global scaling\n    d1 = procrustes_disparity(Y_base, Y_glob)\n    b1 = bool(d1  1e-6)\n\n    # Test 2: Sensitivity to anisotropic feature scaling\n    d2 = procrustes_disparity(Y_base, Y_aniso)\n    b2 = bool(d2 > 2e-2)\n\n    # Test 3: Standardization cancels anisotropy\n    d3 = procrustes_disparity(Y_std_base, Y_std_aniso)\n    b3 = bool(d3  2e-2)\n\n    # Test 4: Standardization vs. Anisotropic Scaling\n    d4_std = procrustes_disparity(Y_base, Y_std_base)\n    b4 = bool(d4_std  d2)\n    \n    # Test 5: Standardization on a balanced manifold\n    d5 = procrustes_disparity(Y_bal, Y_std_bal)\n    b5 = bool(d5  5e-3)\n\n    results = [b1, b2, b3, b4, b5]\n    \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3141684"}, {"introduction": "一个稳健的算法必须能够优雅地处理不完美的数据。这个实践 [@problem_id:3141726] 聚焦于 LLE 的一个关键失效模式，该模式在输入数据包含完全重复的点时出现，这会导致局部权重计算在数学上变得病态。你将学习通过检查局部协方差矩阵及其特征值来诊断这种退化情况，并动手实现和评估像数据抖动和对角加载这样的标准正则化技术，以确保你的算法稳健可靠。", "problem": "给定一个数据矩阵 $X \\in \\mathbb{R}^{N \\times D}$，其行向量为 $x_{i} \\in \\mathbb{R}^{D}$。对于每个索引 $i$，考虑 $x_{i}$ 在欧几里得范数下的 $K$ 个最近邻，平局情况由行索引升序打破。通过堆叠所选邻居 $j$ 的行向量 $x_{j} - x_{i}$，定义邻居-差分矩阵 $Z_{i} \\in \\mathbb{R}^{K \\times D}$。局部线性嵌入的构造将局部协方差 $C_{i} \\in \\mathbb{R}^{K \\times K}$ 定义为 $C_{i} = Z_{i} Z_{i}^{\\top}$。你将分析 $X$ 中完全重复的行如何使 $C_{i}$ 退化，并评估两种基于抖动（jittering）的补救方法。\n\n基本基础：使用以下核心定义和事实。\n- 向量 $v \\in \\mathbb{R}^{D}$ 的欧几里得范数是 $\\lVert v \\rVert_{2} = \\sqrt{\\sum_{d=1}^{D} v_{d}^{2}}$。\n- 对于任何矩阵 $A$，其秩满足 $\\mathrm{rank}(A A^{\\top}) = \\mathrm{rank}(A)$，且 $A A^{\\top}$ 是对称半正定矩阵。\n- 对于一个对称半正定矩阵 $M$，其特征值为实数且非负。当 $\\lambda_{\\min}(M)  0$ 时，2-范数条件数为 $\\kappa_{2}(M) = \\lambda_{\\max}(M) / \\lambda_{\\min}(M)$。\n- 如果两个数据点完全重复，则它们的欧几里得距离为 $0$，相减得到零向量。\n\n你必须从这些原则和定义出发，实现以下内容，不使用本问题陈述中作为提示给出的任何捷径公式。\n\n计算规范。\n- 最近邻：对于给定的 $i$，计算所有 $j \\neq i$ 的距离 $\\lVert x_{j} - x_{i} \\rVert_{2}$，按距离递增排序，然后按 $j$ 排序，并取前 $K$ 个索引。\n- 局部协方差：对于所选的邻居，通过堆叠 $x_{j} - x_{i}$ 形成 $Z_{i}$，然后设置 $C_{i} = Z_{i} Z_{i}^{\\top}$。\n- 奇异性检测：设 $\\tau = 10^{-12}$。如果 $C_{i}$ 的最小特征值小于或等于 $\\tau$，则判定其为奇异矩阵，否则为非奇异矩阵。\n\n要评估的抖动策略。\n- $Z_{i}$ 中的各向同性行抖动：在形成 $C_{i}$ 之前，向 $Z_{i}$ 的每个元素添加均值为 $0$、标准差为 $\\varepsilon$ 的独立高斯噪声。使用 $\\varepsilon = 10^{-3}$ 和固定的随机种子 $0$ 以保证可复现性。\n- $C_{i}$ 的对角线加载（岭）：给定一个目标条件数 $\\kappa_{\\mathrm{target}}  1$，找到最小的非负标量 $r_{\\min}$，使得 $C_{i} + r I$ 的2-范数条件数最多为 $\\kappa_{\\mathrm{target}}$。将 $r_{\\min}$ 表示为四舍五入到 $6$ 位小数的数值。\n\n测试套件。\n使用以下固定的数据集和参数。所有点都在 $\\mathbb{R}^{4}$ 中。\n- 情况 $1$（理想情况，无重复点）：$X_{A}$ 有 $N = 6$ 行，\n  $x_{0} = [0, 0, 0, 0]$,\n  $x_{1} = [1, 0, 0, 0]$,\n  $x_{2} = [0, 1, 0, 0]$,\n  $x_{3} = [0, 0, 1, 0]$,\n  $x_{4} = [0, 0, 0, 1]$,\n  $x_{5} = [1, 1, 1, 1]$。\n  在 $i = 5$ 处，使用 $K = 4$ 进行评估。根据阈值为 $\\tau$ 的奇异性规则，输出一个布尔值，指示 $C_{5}$ 是否为非奇异矩阵。\n- 情况 $2$（单个完全重复的邻居）：$X_{B}$ 是通过在 $X_{A}$ 后附加一个 $x_{5}$ 的副本扩展而来，因此 $N = 7$ 且\n  $x_{6} = [1, 1, 1, 1]$。\n  在 $i = 5$ 处，使用 $K = 4$ 进行评估。根据相同的规则，输出一个布尔值，指示 $C_{5}$ 是否为奇异矩阵。\n- 情况 $3$（所有邻居与中心点相同）：$X_{C}$ 有 $N = 5$ 行，全部等于 $[0, 0, 0, 0]$。在 $i = 0$ 处，使用 $K = 4$ 进行评估。在 $Z_{0}$ 中应用各向同性行抖动，使用 $\\varepsilon = 10^{-3}$ 和种子 $0$，从抖动后的 $Z_{0}$ 形成 $C_{0}$，并根据相同的规则输出一个布尔值，指示得到的 $C_{0}$ 是否为非奇异矩阵。\n- 情况 $4$（为达到目标条件数所需的岭）：重用 $X_{B}$，在 $i = 5$ 处，使用 $K = 4$。设 $\\kappa_{\\mathrm{target}} = 10^{6}$。计算最小的非负 $r_{\\min}$，使得 $C_{5} + r I$ 的2-范数条件数最多为 $\\kappa_{\\mathrm{target}}$。将此 $r_{\\min}$ 作为浮点数输出，四舍五入到 $6$ 位小数。\n\n最终输出格式。\n你的程序应生成一行输出，其中包含情况 $1$ 到 $4$ 的结果，按顺序排列，作为一个用方括号括起来的逗号分隔列表（例如，$[b_{1},b_{2},b_{3},r]$，其中 $b_{1}$、$b_{2}$ 和 $b_{3}$ 是布尔值，$r$ 是一个四舍五入到 $6$ 位小数的浮点数）。不应打印任何其他文本。", "solution": "该问题要求分析局部线性嵌入（LLE）算法中使用的局部协方差矩阵 $C_i$。我们将研究其性质，特别是由于数据点重复而可能导致的奇异性，并评估两种用于数值稳定性的方法：抖动和对角线加载（岭正则化）。\n\n核心流程如下。给定一个数据矩阵 $X \\in \\mathbb{R}^{N \\times D}$ 和一个目标点 $x_i$，我们首先确定其 $K$ 个最近邻。这些邻居是通过将所有其他点 $x_j$（$j \\neq i$）根据欧几里得距离 $\\lVert x_j - x_i \\rVert_2$ 进行升序排序来确定的。距离上的任何平局都通过按行索引 $j$ 升序排序来解决。\n\n从这 $K$ 个邻居（其索引我们表示为 $j_1, \\dots, j_K$），我们构建邻居-差分矩阵 $Z_i \\in \\mathbb{R}^{K \\times D}$。$Z_i$ 的第 $k$ 行是向量 $x_{j_k} - x_i$。\n\n然后，局部协方差矩阵 $C_i \\in \\mathbb{R}^{K \\times K}$ 被定义为格拉姆矩阵 $C_i = Z_i Z_i^\\top$。$C_i$ 的每个元素 $(a, b)$ 是第 $a$ 个和第 $b$ 个差分向量的点积：$(C_i)_{ab} = (x_{j_a} - x_i) \\cdot (x_{j_b} - x_i)$。\n\n根据其构造，$C_i$ 是对称且半正定的。其秩由 $Z_i$ 的秩决定，即由邻居-差分向量集合 $\\{x_{j_k} - x_i\\}_{k=1}^K$ 张成的向量空间的维度。$Z_i$ 的秩至多为 $\\min(K, D)$。如果秩小于 $K$，则 $K \\times K$ 矩阵 $C_i$ 是奇异的，这意味着它至少有一个零特征值。为数值计算目的，如果其最小特征值 $\\lambda_{\\min}(C_i)$ 满足 $\\lambda_{\\min}(C_i) \\le \\tau$，我们将 $C_i$ 归类为奇异矩阵，其中阈值给定为 $\\tau = 10^{-12}$。\n\n### 情况 1：无重复点（非奇异情况）\n给定数据集 $X_A \\in \\mathbb{R}^{6 \\times 4}$，要求我们评估点 $x_5 = [1, 1, 1, 1]$ 在 $K=4$ 时的 $C_5$。\n\n首先，我们计算从 $x_5$ 到所有其他点 $x_j$（$j \\in \\{0, 1, 2, 3, 4\\}$）的欧几里得距离的平方：\n- $\\lVert x_0 - x_5 \\rVert_2^2 = \\lVert [-1, -1, -1, -1] \\rVert_2^2 = 4$\n- $\\lVert x_1 - x_5 \\rVert_2^2 = \\lVert [0, -1, -1, -1] \\rVert_2^2 = 3$\n- $\\lVert x_2 - x_5 \\rVert_2^2 = \\lVert [-1, 0, -1, -1] \\rVert_2^2 = 3$\n- $\\lVert x_3 - x_5 \\rVert_2^2 = \\lVert [-1, -1, 0, -1] \\rVert_2^2 = 3$\n- $\\lVert x_4 - x_5 \\rVert_2^2 = \\lVert [-1, -1, -1, 0] \\rVert_2^2 = 3$\n\n四个最小的距离是相同的（$\\sqrt{3}$），对应于点 $x_1, x_2, x_3, x_4$。由于索引已经是升序的，这些就是 $K=4$ 个最近邻。邻居-差分矩阵 $Z_5$ 由行 $x_j - x_5$ 构成：\n$$Z_5 = \\begin{pmatrix} 0  -1  -1  -1 \\\\ -1  0  -1  -1 \\\\ -1  -1  0  -1 \\\\ -1  -1  -1  0 \\end{pmatrix}$$\n局部协方差矩阵 $C_5 = Z_5 Z_5^\\top$ 是：\n$$C_5 = \\begin{pmatrix} 3  2  2  2 \\\\ 2  3  2  2 \\\\ 2  2  3  2 \\\\ 2  2  2  3 \\end{pmatrix}$$\n该矩阵具有 $2J + I$ 的形式，其中 $J$ 是全一矩阵，$I$ 是单位矩阵。一个 $k \\times k$ 的形式为 $aJ+bI$ 的矩阵的特征值是 $a \\cdot k + b$（重数为 $1$）和 $b$（重数为 $k-1$）。对于 $C_5$，我们有 $k=4$，$a=2$，和 $b=1$。特征值是 $2 \\cdot 4 + 1 = 9$ 和 $1$（重数为 $3$）。\n最小特征值是 $\\lambda_{\\min}(C_5) = 1$。由于 $1  \\tau=10^{-12}$，该矩阵是非奇异的。此情况的结果是 `True`。\n\n### 情况 2：单个完全重复的邻居（奇异情况）\n数据集 $X_B$ 是通过在 $X_A$ 后增加 $x_6 = x_5 = [1, 1, 1, 1]$ 得到的。我们重新为 $x_5$ 评估，参数为 $K=4$。\n$x_5$ 到其重复点 $x_6$ 的距离是 $\\lVert x_6 - x_5 \\rVert_2 = 0$。这是可能的最小距离，所以 $x_6$ 是最近的邻居。剩下的三个邻居是 $x_1, x_2, x_3$，每个的距离都是 $\\sqrt{3}$。\n邻居-差分矩阵 $Z_5$ 是：\n$$Z_5 = \\begin{pmatrix} x_6-x_5 \\\\ x_1-x_5 \\\\ x_2-x_5 \\\\ x_3-x_5 \\end{pmatrix} = \\begin{pmatrix} 0  0  0  0 \\\\ 0  -1  -1  -1 \\\\ -1  0  -1  -1 \\\\ -1  -1  0  -1 \\end{pmatrix}$$\n由于 $Z_5$ 的第一行是零向量，这些行是线性相关的。因此，$\\mathrm{rank}(Z_5)  4$。格拉姆矩阵 $C_5 = Z_5 Z_5^\\top$ 因此必须是奇异的，其秩为 $\\mathrm{rank}(C_5) = \\mathrm{rank}(Z_5)  4$。这意味着它的最小特征值恰好是 $\\lambda_{\\min}(C_5)=0$。由于 $0 \\le \\tau=10^{-12}$，该矩阵是奇异的。此情况的结果是 `True`。\n\n### 情况 3：使用抖动进行正则化\n数据集 $X_C$ 由 $N=5$ 个相同的点 $[0, 0, 0, 0]$ 组成。我们为 $x_0$ 评估，参数为 $K=4$。\n邻居是 $x_1, x_2, x_3, x_4$。到每个点的距离都是 $0$。差分向量 $x_j - x_0$ 都是零向量。初始的邻居-差分矩阵 $Z_0$ 是一个 $4 \\times 4$ 的零矩阵：\n$$Z_0 = \\mathbf{0}_{4 \\times 4}$$\n该矩阵是完全奇异的，秩为 $0$。为解决这个问题，我们应用各向同性抖动。一个噪声矩阵，其元素从高斯分布 $\\mathcal{N}(0, \\varepsilon^2)$（$\\varepsilon = 10^{-3}$）中独立抽取，被加到 $Z_0$ 上以形成新矩阵 $Z'_0$。由于 $Z'_0$ 的元素是从一个连续概率分布中抽取的，该矩阵将以概率 $1$ 可逆。其秩将为 $4$。\n新的协方差矩阵 $C_0 = Z'_0 (Z'_0)^\\top$ 也将有秩 $4$，因此是非奇异的。它的最小特征值 $\\lambda_{\\min}(C_0)$ 将严格为正。考虑到噪声的非零方差，几乎可以肯定 $\\lambda_{\\min}(C_0)$ 将大于小容差 $\\tau = 10^{-12}$。因此，抖动后的矩阵是非奇异的。此情况的结果是 `True`。\n\n### 情况 4：使用对角线加载进行正则化\n我们重用情况 2 中的奇异矩阵 $C_5$。其特征值为 $\\{0, 1, 1, 7\\}$。我们需要找到最小的非负标量 $r_{\\min}$，使得正则化矩阵 $C'_5 = C_5 + rI$ 的条件数 $\\kappa_2(C'_5) \\le \\kappa_{\\mathrm{target}}$，其中 $\\kappa_{\\mathrm{target}} = 10^6$。\n$C'_5$ 的特征值是通过将 $C_5$ 的特征值平移 $r$ 得到的。对于 $r \\ge 0$，它们是 $\\{r, 1+r, 1+r, 7+r\\}$。最小和最大特征值分别是 $\\lambda'_{\\min} = r$ 和 $\\lambda'_{\\max} = 7+r$。\n条件数（对于 $r  0$）是 $\\kappa_2(C'_5) = \\frac{\\lambda'_{\\max}}{\\lambda'_{\\min}} = \\frac{7+r}{r}$。\n我们解这个不等式：\n$$\\frac{7+r}{r} \\le \\kappa_{\\mathrm{target}}$$\n因为 $r$ 必须为正（否则 $\\kappa_2$ 未定义或为无穷大），我们可以乘以 $r$：\n$$7 + r \\le r \\cdot \\kappa_{\\mathrm{target}} \\implies 7 \\le r (\\kappa_{\\mathrm{target}} - 1)$$\n这给出了 $r$ 的条件：\n$$r \\ge \\frac{7}{\\kappa_{\\mathrm{target}} - 1}$$\n$r$ 可以取的最小非负值是 $r_{\\min} = \\frac{7}{\\kappa_{\\mathrm{target}} - 1}$。代入 $\\kappa_{\\mathrm{target}} = 10^6$：\n$$r_{\\min} = \\frac{7}{10^6 - 1} = \\frac{7}{999999} \\approx 7.000007000007 \\times 10^{-6}$$\n将此值四舍五入到 $6$ 位小数，得到 $0.000007$。", "answer": "```python\nimport numpy as np\n\ndef find_neighbors_and_form_CZ(X, i, K):\n    \"\"\"\n    Finds K nearest neighbors for X[i] and computes C_i and Z_i.\n    Ties are broken by ascending row index.\n    \"\"\"\n    x_i = X[i, :]\n    num_points = X.shape[0]\n    \n    distances = []\n    for j in range(num_points):\n        if i == j:\n            continue\n        dist = np.linalg.norm(X[j, :] - x_i)\n        distances.append((dist, j))\n    \n    # Sort by distance, then by index for tie-breaking\n    distances.sort()\n    \n    # Get the K nearest neighbor indices\n    neighbor_indices = [j for dist, j in distances[:K]]\n    \n    # Form the Z_i matrix. The shape should be K x D\n    Z_i = np.zeros((K, X.shape[1]))\n    for k, j in enumerate(neighbor_indices):\n        Z_i[k, :] = X[j, :] - x_i\n    \n    # Form the C_i matrix\n    C_i = Z_i @ Z_i.T\n    \n    return C_i, Z_i\n\ndef solve():\n    \"\"\"\n    Solves the four test cases specified in the problem.\n    \"\"\"\n    # Define datasets\n    X_A = np.array([\n        [0., 0., 0., 0.],\n        [1., 0., 0., 0.],\n        [0., 1., 0., 0.],\n        [0., 0., 1., 0.],\n        [0., 0., 0., 1.],\n        [1., 1., 1., 1.]\n    ], dtype=float)\n\n    X_B = np.array([\n        [0., 0., 0., 0.],\n        [1., 0., 0., 0.],\n        [0., 1., 0., 0.],\n        [0., 0., 1., 0.],\n        [0., 0., 0., 1.],\n        [1., 1., 1., 1.],\n        [1., 1., 1., 1.]\n    ], dtype=float)\n\n    X_C = np.zeros((5, 4), dtype=float)\n    \n    tau = 1e-12\n    results = []\n\n    # Case 1: Happy path, no duplicates. Check for nonsingular.\n    C5_A, _ = find_neighbors_and_form_CZ(X_A, i=5, K=4)\n    eigenvalues_A = np.linalg.eigh(C5_A)[0]\n    lambda_min_A = eigenvalues_A[0]\n    results.append(lambda_min_A > tau)\n\n    # Case 2: Single exact duplicate neighbor. Check for singular.\n    C5_B, _ = find_neighbors_and_form_CZ(X_B, i=5, K=4)\n    eigenvalues_B = np.linalg.eigh(C5_B)[0]\n    lambda_min_B = eigenvalues_B[0]\n    results.append(lambda_min_B = tau)\n\n    # Case 3: All neighbors identical. Jitter and check for nonsingular.\n    epsilon = 1e-3\n    seed = 0\n    _, Z0_C = find_neighbors_and_form_CZ(X_C, i=0, K=4)\n    \n    np.random.seed(seed)\n    noise = np.random.normal(scale=epsilon, size=Z0_C.shape)\n    Z0_jittered = Z0_C + noise\n    \n    C0_jittered = Z0_jittered @ Z0_jittered.T\n    eigenvalues_C = np.linalg.eigh(C0_jittered)[0]\n    lambda_min_C = eigenvalues_C[0]\n    results.append(lambda_min_C > tau)\n    \n    # Case 4: Ridge needed for a target condition number.\n    kappa_target = 10**6\n    # Re-use eigenvalues from Case 2.\n    lambda_min_B_case4 = eigenvalues_B[0]\n    lambda_max_B_case4 = eigenvalues_B[-1]\n    \n    # We need to find the smallest r >= 0 such that:\n    # (lambda_max + r) / (lambda_min + r) = kappa_target\n    # This leads to: r >= (lambda_max - kappa_target * lambda_min) / (kappa_target - 1)\n    if kappa_target > 1:\n        r_min_candidate = (lambda_max_B_case4 - kappa_target * lambda_min_B_case4) / (kappa_target - 1)\n        r_min = max(0.0, r_min_candidate)\n    else:\n        # This case is not relevant for the problem, but for completeness:\n        # If target condition number = 1, it's generally impossible unless a matrix is scalar multiple of identity.\n        r_min = np.inf \n\n    results.append(r_min)\n    \n    # Format final output string as per requirements\n    formatted_results = []\n    for item in results:\n        if isinstance(item, bool):\n            formatted_results.append(str(item))\n        elif isinstance(item, float):\n            # Format the float to 6 decimal places for the output string\n            formatted_results.append(f\"{item:.6f}\")\n        else:\n            formatted_results.append(str(item))\n            \n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3141726"}, {"introduction": "除了实现标准算法，机器学习的一项关键技能是知道如何根据不同需求对其进行修改。最后一个实践 [@problem_id:3141689] 介绍了一种 LLE 的正则化变体，其目标不仅仅是精确地重构数据点，还要控制重构权重的分布。通过在优化问题中加入一个惩罚项，你将探索一个基本的权衡（trade-off）概念——在重构保真度与模型简洁性之间取得平衡，这是现代统计学习中的一个核心主题。", "problem": "您的任务是研究局部线性嵌入（Locally Linear Embedding, LLE）的一种变体，该变体旨在平衡局部重构精度与重构权重的分散程度。给定一个位于 $\\mathbb{R}^D$ 中的点集 $\\{ \\mathbf{x}_i \\}_{i=1}^n$。对于每个点 $\\mathbf{x}_i$，根据欧几里得距离选择 $k$ 个最近邻。令邻域索引集为 $\\mathcal{N}(i)$，并定义局部偏移矩阵 $\\mathbf{Z}_i \\in \\mathbb{R}^{D \\times k}$，其列向量为 $\\mathbf{x}_j - \\mathbf{x}_i$，其中 $j \\in \\mathcal{N}(i)$。利用权重 $\\mathbf{w}_i \\in \\mathbb{R}^k$ 从其邻域重构 $\\mathbf{x}_i$，这些权重满足仿射约束 $\\sum_{j \\in \\mathcal{N}(i)} w_{ij} = 1$。重构误差向量等于 $\\mathbf{Z}_i \\mathbf{w}_i$，其平方范数 $\\| \\mathbf{Z}_i \\mathbf{w}_i \\|_2^2$ 用于量化局部重构精度。权重的分散程度由 $\\sum_{j \\in \\mathcal{N}(i)} w_{ij}^2$ 捕捉，在仿射约束下，当权重在选定的邻域点之间均匀分布时，该项达到最小值。在此变体中，您必须计算权重，通过惩罚 $\\sum_{j \\in \\mathcal{N}(i)} w_{ij}^2$ 来平衡重构精度与权重分散程度，同时强制执行仿射约束 $\\sum_{j \\in \\mathcal{N}(i)} w_{ij} = 1$。对于给定的惩罚参数 $\\lambda \\ge 0$，确定每个 $i$ 的权重 $\\mathbf{w}_i$，使得在满足仿射约束的条件下，局部重构平方范数与惩罚项之和最小。\n\n您的程序必须针对下面指定的每个测试用例执行以下步骤：\n- 对于数据集中的每个点 $\\mathbf{x}_i$，根据欧几里得距离选择恰好 $k$ 个最近邻（不包括 $\\mathbf{x}_i$ 本身）。\n- 对于每个点 $\\mathbf{x}_i$，构建局部偏移矩阵 $\\mathbf{Z}_i$，其 $k$ 个列向量为 $\\mathbf{x}_j - \\mathbf{x}_i$，其中 $j \\in \\mathcal{N}(i)$。\n- 对于给定的 $\\lambda$，计算权重 $\\mathbf{w}_i$，以最小化重构误差的平方范数 $\\| \\mathbf{Z}_i \\mathbf{w}_i \\|_2^2$ 与惩罚项 $\\lambda \\sum_{j \\in \\mathcal{N}(i)} w_{ij}^2$ 之和，同时满足约束 $\\sum_{j \\in \\mathcal{N}(i)} w_{ij} = 1$。\n- 计算每个点的重构误差 $e_i = \\| \\mathbf{Z}_i \\mathbf{w}_i \\|_2^2$ 和每个点的权重分散度 $s_i = \\sum_{j \\in \\mathcal{N}(i)} w_{ij}^2$。\n- 对所有点进行聚合，以获得平均重构误差 $\\bar{e} = \\frac{1}{n} \\sum_{i=1}^n e_i$ 和平均权重分散度 $\\bar{s} = \\frac{1}{n} \\sum_{i=1}^n s_i$。\n\n您必须确保科学真实性和数值稳定性。如果您的方法所需的任何线性系统或矩阵逆因几何原因（例如当 $k  D$ 时）是奇异或病态的，您必须使用一种数值稳健的方法，以在仿射约束下产生有效解。在所有内部计算中，角度必须以弧度为单位进行解释。\n\n测试套件：\n定义以下确定性数据集和参数集。不允许任何随机性。\n\n- 数据集 $\\mathcal{A}$ 位于 $\\mathbb{R}^2$ 中：$n = 8$ 个点位于单位圆上，角度为 $\\theta_i = \\frac{2\\pi i}{8}$，其中 $i \\in \\{0,1,2,3,4,5,6,7\\}$，因此 $\\mathbf{x}_i = (\\cos \\theta_i, \\sin \\theta_i)$。\n- 数据集 $\\mathcal{B}$ 位于 $\\mathbb{R}^3$ 中：$n = 10$ 个点沿一条直线分布，带有小的确定性扰动。令 $t_i = \\frac{i}{9}$，其中 $i \\in \\{0,1,2,3,4,5,6,7,8,9\\}$。定义 $\\epsilon = 0.01$ 和 $\\mathbf{x}_i = (t_i, 2 t_i, -t_i) + \\epsilon \\cdot (\\sin(10 t_i), \\cos(7 t_i), \\sin(5 t_i))$。\n- 数据集 $\\mathcal{C}$ 位于 $\\mathbb{R}^2$ 中：$n = 6$ 个点形成一个带有中点的非退化矩形：$\\mathbf{x}_1 = (0,0)$，$\\mathbf{x}_2 = (2,0)$，$\\mathbf{x}_3 = (2,1)$，$\\mathbf{x}_4 = (0,1)$，$\\mathbf{x}_5 = (1,0.5)$，$\\mathbf{x}_6 = (1,1.5)$。\n\n测试用例如下：\n- 用例 1：数据集 $\\mathcal{A}$，$k = 3$，$\\lambda = 0$。\n- 用例 2：数据集 $\\mathcal{A}$，$k = 3$，$\\lambda = 0.1$。\n- 用例 3：数据集 $\\mathcal{A}$，$k = 3$，$\\lambda = 10$。\n- 用例 4：数据集 $\\mathcal{B}$，$k = 2$，$\\lambda = 0$。\n- 用例 5：数据集 $\\mathcal{B}$，$k = 2$，$\\lambda = 1$。\n- 用例 6：数据集 $\\mathcal{C}$，$k = 3$，$\\lambda = 50$。\n\n您的程序必须为每个用例计算 $(\\bar{e}, \\bar{s})$，并生成一行输出，其中包含结果，格式为无空格的逗号分隔列表的列表，每个内部列表的顺序为 $[\\bar{e},\\bar{s}]$，并且两个值都四舍五入到 $6$ 位小数。例如，输出格式必须类似于 $[[a_1,b_1],[a_2,b_2],[a_3,b_3],[a_4,b_4],[a_5,b_5],[a_6,b_6]]$，其中 $a_i$ 和 $b_i$ 被计算出的十进制值替换。", "solution": "用户提供的问题是统计学习领域中一个有效且适定的练习，具体涉及局部线性嵌入（LLE）中权重计算步骤的一个正则化变体。该问题具有科学依据，需要解决一个约束二次优化问题，并提供了确定性的、可验证的测试用例。我们将进行完整的推导和求解。\n\n问题的核心是为每个数据点 $\\mathbf{x}_i \\in \\mathbb{R}^D$ 找到一组权重 $\\mathbf{w}_i \\in \\mathbb{R}^k$，用其 $k$ 个最近邻来重构 $\\mathbf{x}_i$。这些权重通过最小化一个目标函数来确定，该函数平衡了两个相互竞争的目标：最小化局部重构误差和最小化权重的分散程度。这受一个仿射约束的制约，以确保权重具有平移不变性。\n\n令 $\\mathbf{x}_i$ 的 $k$ 个最近邻的索引集为 $\\mathcal{N}(i)$。局部偏移矩阵 $\\mathbf{Z}_i \\in \\mathbb{R}^{D \\times k}$ 的列向量由 $\\mathbf{z}_{ij} = \\mathbf{x}_j - \\mathbf{x}_i$ 给出，其中每个 $j \\in \\mathcal{N}(i)$。从这些偏移量重构零向量由 $\\sum_{j \\in \\mathcal{N}(i)} w_{ij}(\\mathbf{x}_j - \\mathbf{x}_i) = \\mathbf{Z}_i \\mathbf{w}_i$ 给出。\n\n对于每个点 $\\mathbf{x}_i$，优化问题是找到权重向量 $\\mathbf{w}_i \\in \\mathbb{R}^k$，该向量解出：\n$$\n\\min_{\\mathbf{w}_i} \\left( \\| \\mathbf{Z}_i \\mathbf{w}_i \\|_2^2 + \\lambda \\sum_{j \\in \\mathcal{N}(i)} w_{ij}^2 \\right) \\quad \\text{subject to} \\quad \\sum_{j \\in \\mathcal{N}(i)} w_{ij} = 1\n$$\n其中 $\\lambda \\ge 0$ 是一个正则化参数。\n\n我们可以用矩阵表示法来表达目标函数。第一项，即重构误差的平方，是 $\\| \\mathbf{Z}_i \\mathbf{w}_i \\|_2^2 = (\\mathbf{Z}_i \\mathbf{w}_i)^T(\\mathbf{Z}_i \\mathbf{w}_i) = \\mathbf{w}_i^T \\mathbf{Z}_i^T \\mathbf{Z}_i \\mathbf{w}_i$。令 $\\mathbf{C}_i = \\mathbf{Z}_i^T \\mathbf{Z}_i \\in \\mathbb{R}^{k \\times k}$ 为邻域偏移量的局部格拉姆矩阵。该矩阵是对称半正定的。其 $(p,q)$ 项是点积 $(\\mathbf{x}_p - \\mathbf{x}_i)^T (\\mathbf{x}_q - \\mathbf{x}_i)$，其中 $p, q \\in \\mathcal{N}(i)$。\n\n第二项，即对权重分散度的惩罚，是 $\\lambda \\sum_{j \\in \\mathcal{N}(i)} w_{ij}^2 = \\lambda \\| \\mathbf{w}_i \\|_2^2 = \\lambda \\mathbf{w}_i^T \\mathbf{I}_k \\mathbf{w}_i$，其中 $\\mathbf{I}_k$ 是 $k \\times k$ 的单位矩阵。\n\n仿射约束可以写成 $\\mathbf{1}^T \\mathbf{w}_i = 1$，其中 $\\mathbf{1}$ 是一个包含 $k$ 个 1 的列向量。\n\n结合这些项，问题是一个约束二次规划：\n$$\n\\min_{\\mathbf{w}_i} \\mathcal{L}(\\mathbf{w}_i) = \\mathbf{w}_i^T (\\mathbf{C}_i + \\lambda \\mathbf{I}_k) \\mathbf{w}_i \\quad \\text{subject to} \\quad \\mathbf{1}^T \\mathbf{w}_i = 1\n$$\n令 $\\mathbf{G}_i = \\mathbf{C}_i + \\lambda \\mathbf{I}_k$。矩阵 $\\mathbf{G}_i$ 是对称的。对于 $\\lambda  0$，它是严格正定的，保证了唯一解。对于 $\\lambda = 0$，$\\mathbf{G}_i = \\mathbf{C}_i$ 是半正定的，并且可能是奇异的，特别是当邻居数 $k$ 大于数据维度 $D$ 时。\n\n我们使用拉格朗日乘子法来解决这个问题。拉格朗日函数为：\n$$\n\\mathcal{J}(\\mathbf{w}_i, \\mu) = \\mathbf{w}_i^T \\mathbf{G}_i \\mathbf{w}_i - \\mu (\\mathbf{1}^T \\mathbf{w}_i - 1)\n$$\n其中 $\\mu$ 是拉格朗日乘子。为了找到最小值，我们将关于 $\\mathbf{w}_i$ 的梯度设为零：\n$$\n\\nabla_{\\mathbf{w}_i} \\mathcal{J} = 2 \\mathbf{G}_i \\mathbf{w}_i - \\mu \\mathbf{1} = \\mathbf{0}\n$$\n这给出了平稳性条件 $2 \\mathbf{G}_i \\mathbf{w}_i = \\mu \\mathbf{1}$。\n\n如果 $\\mathbf{G}_i$ 是可逆的（当 $\\lambda  0$ 时保证如此），我们可以写出 $\\mathbf{w}_i = \\frac{\\mu}{2} \\mathbf{G}_i^{-1} \\mathbf{1}$。为了找到乘子 $\\mu$，我们将其代入约束 $\\mathbf{1}^T \\mathbf{w}_i = 1$ 中：\n$$\n\\mathbf{1}^T \\left( \\frac{\\mu}{2} \\mathbf{G}_i^{-1} \\mathbf{1} \\right) = 1 \\implies \\frac{\\mu}{2} \\left( \\mathbf{1}^T \\mathbf{G}_i^{-1} \\mathbf{1} \\right) = 1\n$$\n解出项 $\\frac{\\mu}{2}$ 得到 $\\frac{\\mu}{2} = \\frac{1}{\\mathbf{1}^T \\mathbf{G}_i^{-1} \\mathbf{1}}$。\n将此代回 $\\mathbf{w}_i$ 的表达式，我们得到最优权重：\n$$\n\\mathbf{w}_i = \\frac{\\mathbf{G}_i^{-1} \\mathbf{1}}{\\mathbf{1}^T \\mathbf{G}_i^{-1} \\mathbf{1}}\n$$\n从数值角度来看，显式计算矩阵逆 $\\mathbf{G}_i^{-1}$ 效率低下且可能不稳定。更好的方法是首先求解线性系统 $\\mathbf{G}_i \\mathbf{v}_i = \\mathbf{1}$ 以得到向量 $\\mathbf{v}_i = \\mathbf{G}_i^{-1} \\mathbf{1}$。然后，通过对 $\\mathbf{v}_i$ 进行归一化来找到权重：\n$$\n\\mathbf{w}_i = \\frac{\\mathbf{v}_i}{\\mathbf{1}^T \\mathbf{v}_i} = \\frac{\\mathbf{v}_i}{\\sum_{j=1}^k v_{ij}}\n$$\n该过程必须对 $\\mathbf{G}_i$ 是奇异的情况（即当 $\\lambda = 0$ 且 $\\mathbf{C}_i$ 秩亏时）保持稳健。在这种情况下，系统 $\\mathbf{G}_i\\mathbf{v}_i = \\mathbf{1}$ 可能没有解或有无穷多解。问题陈述要求一种稳健的方法。标准程序是找到 $\\mathbf{v}_i$ 的最小范数最小二乘解。这对应于使用 Moore-Penrose 伪逆，$\\mathbf{v}_i = \\mathbf{G}_i^{\\dagger} \\mathbf{1}$。现代线性代数求解器，如 `numpy.linalg.lstsq`，旨在有效处理这种情况。\n\n一旦为点 $\\mathbf{x}_i$ 找到最优权重 $\\mathbf{w}_i$，我们计算所需的度量：\n- 每点重构误差：$e_i = \\| \\mathbf{Z}_i \\mathbf{w}_i \\|_2^2 = \\mathbf{w}_i^T \\mathbf{C}_i \\mathbf{w}_i$。\n- 每点权重分散度：$s_i = \\sum_{j \\in \\mathcal{N}(i)} w_{ij}^2 = \\| \\mathbf{w}_i \\|_2^2$。\n\n然后将这些值在数据集中的所有 $n$ 个点上取平均，以获得最终输出 $\\bar{e} = \\frac{1}{n} \\sum_{i=1}^n e_i$ 和 $\\bar{s} = \\frac{1}{n} \\sum_{i=1}^n s_i$。\n\n总体算法如下：\n1. 对于 $n$ 个点中的每一个 $\\mathbf{x}_i$，$i=1, \\dots, n$：\n    a. 计算到所有其他点 $\\mathbf{x}_j$ ($j \\ne i$) 的欧几里得距离。\n    b. 识别 $k$ 个最近邻的索引 $\\mathcal{N}(i)$。\n    c. 形成 $D \\times k$ 偏移矩阵 $\\mathbf{Z}_i$，其列为 $\\mathbf{x}_j - \\mathbf{x}_i$，其中 $j \\in \\mathcal{N}(i)$。\n    d. 计算 $k \\times k$ 格拉姆矩阵 $\\mathbf{C}_i = \\mathbf{Z}_i^T \\mathbf{Z}_i$。\n    e. 形成正则化矩阵 $\\mathbf{G}_i = \\mathbf{C}_i + \\lambda \\mathbf{I}_k$。\n    f. 使用稳健的最小二乘求解器求解 $\\mathbf{G}_i \\mathbf{v}_i = \\mathbf{1}$ 中的 $\\mathbf{v}_i$。\n    g. 归一化以找到权重：$\\mathbf{w}_i = \\mathbf{v}_i / \\sum_j v_{ij}$。\n    h. 计算 $e_i = \\mathbf{w}_i^T \\mathbf{C}_i \\mathbf{w}_i$ 和 $s_i = \\mathbf{w}_i^T \\mathbf{w_i}$。\n2. 从收集的 $e_i$ 和 $s_i$ 值计算平均值 $\\bar{e}$ 和 $\\bar{s}$。\n3. 对所有测试用例重复此过程，并按规定格式化输出。", "answer": "```python\nimport numpy as np\n\ndef get_dataset_A():\n    \"\"\"Generates n=8 points on a unit circle.\"\"\"\n    n = 8\n    thetas = 2 * np.pi * np.arange(n) / n\n    return np.array([np.cos(thetas), np.sin(thetas)]).T\n\ndef get_dataset_B():\n    \"\"\"Generates n=10 points on a perturbed line in R^3.\"\"\"\n    n = 10\n    epsilon = 0.01\n    t = np.arange(n) / (n - 1.0)\n    base_line = np.array([t, 2*t, -t]).T\n    perturbations = epsilon * np.array([np.sin(10*t), np.cos(7*t), np.sin(5*t)]).T\n    return base_line + perturbations\n\ndef get_dataset_C():\n    \"\"\"Generates n=6 points forming a non-degenerate rectangle with midpoints.\"\"\"\n    return np.array([\n        [0.0, 0.0],\n        [2.0, 0.0],\n        [2.0, 1.0],\n        [0.0, 1.0],\n        [1.0, 0.5],\n        [1.0, 1.5]\n    ])\n\ndef compute_lle_variant(X, k, lambda_val):\n    \"\"\"\n    Computes the mean reconstruction error and weight spread for the LLE variant.\n    \"\"\"\n    n, D = X.shape\n    all_e_i = []\n    all_s_i = []\n\n    for i in range(n):\n        # Find k nearest neighbors\n        x_i = X[i]\n        \n        # Calculate squared Euclidean distances to all other points\n        # Using squared distances for finding neighbors is equivalent and faster\n        dists_sq = np.sum((X - x_i)**2, axis=1)\n        \n        # Exclude the point itself\n        dists_sq[i] = np.inf\n        \n        # Get indices of the k nearest neighbors\n        neighbor_indices = np.argsort(dists_sq)[:k]\n        \n        # Form the local offset matrix Z_i (D x k)\n        neighbors = X[neighbor_indices]\n        Z_i = (neighbors - x_i).T\n        \n        # Compute the local Gram matrix C_i (k x k)\n        C_i = Z_i.T @ Z_i\n        \n        # Form the regularized Gram matrix G_i = C_i + lambda * I\n        G_i = C_i + lambda_val * np.eye(k)\n        \n        # Solve the linear system G_i * v_i = 1 for v_i\n        # Using lstsq is robust against singularity, which occurs when lambda=0 and k > D\n        ones_k = np.ones(k)\n        v_i = np.linalg.lstsq(G_i, ones_k, rcond=None)[0]\n        \n        # Compute the weights w_i by normalizing v_i\n        # The sum can be zero in pathological cases, handle division by zero\n        sum_v_i = np.sum(v_i)\n        if np.isclose(sum_v_i, 0):\n             # This indicates a failure to find a valid reconstruction under the \n             # sum-to-one constraint. This shouldn't happen for the given problems.\n             # A uniform weight distribution might be a fallback.\n             w_i = np.full(k, 1.0/k)\n        else:\n             w_i = v_i / sum_v_i\n\n        # Compute per-point reconstruction error e_i\n        e_i = w_i.T @ C_i @ w_i\n        all_e_i.append(e_i)\n        \n        # Compute per-point weight spread s_i\n        s_i = w_i.T @ w_i\n        all_s_i.append(s_i)\n\n    # Compute mean error and spread\n    mean_e = np.mean(all_e_i)\n    mean_s = np.mean(all_s_i)\n    \n    return [round(mean_e, 6), round(mean_s, 6)]\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print results.\n    \"\"\"\n    datasets = {\n        'A': get_dataset_A(),\n        'B': get_dataset_B(),\n        'C': get_dataset_C()\n    }\n    \n    test_cases = [\n        ('A', 3, 0.0),\n        ('A', 3, 0.1),\n        ('A', 3, 10.0),\n        ('B', 2, 0.0),\n        ('B', 2, 1.0),\n        ('C', 3, 50.0)\n    ]\n    \n    results = []\n    for dataset_key, k, lambda_val in test_cases:\n        X = datasets[dataset_key]\n        result = compute_lle_variant(X, k, lambda_val)\n        results.append(result)\n\n    # Format output string\n    # e.g., [[0.1,0.2],[0.3,0.4]]\n    # Using f-strings to represent floats without trailing zeros if they are integers\n    # but the rounding to 6 decimal places should make this consistent\n    output_str = '[' + ','.join([f\"[{e:.6f},{s:.6f}]\" for e, s in results]) + ']'\n    print(output_str)\n\nsolve()\n```", "id": "3141689"}]}