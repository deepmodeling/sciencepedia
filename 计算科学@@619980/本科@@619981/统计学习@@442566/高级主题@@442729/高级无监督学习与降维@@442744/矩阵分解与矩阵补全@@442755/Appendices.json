{"hands_on_practices": [{"introduction": "此练习将带您深入探讨核范数最小化的核心，这是一种最常见的矩阵填充凸优化方法。您将实现一个基于奇异值阈值（SVT）算子的近端点算法。通过亲手编写这个基础算法，您将具体地理解正则化是如何直接促进低秩解的产生，以及正则化参数 $\\lambda$ 如何权衡数据保真度与矩阵的秩 [@problem_id:3168247]。", "problem": "要求您设计并分析一种使用核范数正则化器的矩阵补全近端方法。您的目标是实现一个由近端点法则驱动的迭代方案，然后研究正则化权重的选择如何影响迭代过程中矩阵秩的演变。您编写的程序必须通过奇异值分解 (SVD) 机制计算核范数的近端算子，并评估不同参数值下的秩收缩轨迹。\n\n起点与定义：\n- 设 $M \\in \\mathbb{R}^{m \\times n}$ 为一个部分观测矩阵，其关联的观测掩码为 $\\Omega \\subseteq \\{1,\\dots,m\\} \\times \\{1,\\dots,n\\}$。定义线性观测算子 $P_{\\Omega}:\\mathbb{R}^{m \\times n} \\to \\mathbb{R}^{m \\times n}$ 为：若 $(i,j)\\in\\Omega$，则 $(P_{\\Omega}(X))_{ij} = X_{ij}$；否则 $(P_{\\Omega}(X))_{ij}=0$。定义互补算子 $P_{\\Omega^c}(X) = X - P_{\\Omega}(X)$。矩阵 $X$ 的核范数，记为 $\\|X\\|_*$，是其所有奇异值之和。\n- 一个正常、闭、凸函数 $f:\\mathbb{R}^{m \\times n}\\to\\mathbb{R}\\cup\\{+\\infty\\}$ 在点 $Y$ 处，参数为 $t>0$ 的近端算子定义为\n$$\n\\operatorname{prox}_{t f}(Y) = \\arg\\min_{X\\in\\mathbb{R}^{m\\times n}} \\left\\{ f(X) + \\frac{1}{2t}\\|X - Y\\|_F^2 \\right\\}.\n$$\n- 考虑复合目标函数 $F(X) = \\frac{1}{2}\\|P_{\\Omega}(X - M)\\|_F^2 + \\lambda \\|X\\|_*$，其中 $\\lambda>0$。\n\n任务：\n- 仅使用上述定义以及凸函数和 SVD 的标准性质，推导出一个迭代方案。该方案在每次迭代 $k$ 中，首先对已观测条目强制执行数据一致性，然后在所得点上应用核范数的近端算子。具体地，对于固定的 $\\lambda$，从 $X^0 = 0$ 开始，对 $k=0,1,2,\\dots,K-1$ 执行以下两个步骤：\n  1. 数据一致性注入以形成 $Y^k$：若 $(i,j)\\in\\Omega$，则设置 $Y^k_{ij} = M_{ij}$；否则设置 $Y^k_{ij} = X^k_{ij}$（即，$Y^k = P_{\\Omega}(M) + P_{\\Omega^c}(X^k)$）。\n  2. 近端步骤以获得 $X^{k+1} = \\operatorname{prox}_{\\lambda \\|\\cdot\\|_*}(Y^k)$，该步骤通过从第一性原理推导的奇异值阈值化进行计算。\n- 在每次迭代 $k$ 中，计算 $X^{k}$ 的秩，其定义为严格大于一个固定数值容差的奇异值数量（您必须选择并声明一个合理的容差）。\n\n研究内容：\n- 实现上述迭代方案，并对每个 $\\lambda$ 的选择，记录秩序列 $\\{\\operatorname{rank}(X^{1}), \\operatorname{rank}(X^{2}), \\dots, \\operatorname{rank}(X^{K})\\}$，其中 $K$ 是总迭代次数。\n\n数据生成协议（确定性）：\n- 设置 $m=20$ 和 $n=15$。\n- 使用正交因子和奇异值构建一个秩为 $r_{\\text{true}}=3$ 的真实低秩矩阵 $X_{\\text{true}}\\in\\mathbb{R}^{20\\times 15}$。使用固定的奇异值 $10$、$5$ 和 $3$，并通过对具有固定伪随机种子的高斯随机矩阵应用 Gram–Schmidt 过程（通过标准的 QR 分解）来确定性地生成正交列。\n- 使用相同的固定伪随机种子，以概率 $p=0.5$ 独立采样每个条目是否被观测，从而生成观测掩码。\n- 对于 $(i,j)\\in\\Omega$，设置 $M_{ij} = X_{\\text{true},ij} + \\varepsilon_{ij}$，其中 $\\varepsilon_{ij}$ 是均值为 $0$、标准差为 $0.01$ 的独立高斯噪声；对于 $(i,j)\\notin\\Omega$，设置 $M_{ij}=0$，从而形成观测矩阵 $M$。噪声生成使用相同的固定种子。\n\n测试套件：\n- 使用 $K=12$ 次迭代和秩容差 $10^{-8}$。\n- 评估三种 $\\lambda$ 值下的秩轨迹：$\\lambda=0.0$（边界情况）、$\\lambda=1.0$（中等正则化）和 $\\lambda=5.0$（强正则化）。\n\n要求的最终输出格式：\n- 您的程序必须生成单行输出，其中包含三个秩轨迹（对应三个 $\\lambda$ 值），格式为用方括号括起来的逗号分隔列表的列表，例如 $[[r_{1,1},\\dots,r_{1,12}],[r_{2,1},\\dots,r_{2,12}],[r_{3,1},\\dots,r_{3,12}]]$，其中 $r_{j,k}$ 是一个整数秩。不应打印任何其他文本。\n\n注：\n- 此问题不涉及角度和物理单位。\n- 实现必须是自包含的，并通过使用固定的伪随机种子来确保确定性。\n- 您必须通过从近端算子的定义和 SVD 的性质推导出的奇异值阈值化来计算 $\\operatorname{prox}_{\\lambda \\|\\cdot\\|_*}$；不要假设或使用任何预先提供的公式。", "solution": "已对用户提供的问题陈述进行了分析和验证。\n\n### 步骤 1：提取已知信息\n- **矩阵与算子**：\n    - $M \\in \\mathbb{R}^{m \\times n}$：一个部分观测矩阵。\n    - $\\Omega \\subseteq \\{1,\\dots,m\\} \\times \\{1,\\dots,n\\}$：一个观测掩码。\n    - $P_{\\Omega}(X)$：线性算子，其中若 $(i,j)\\in\\Omega$，则 $(P_{\\Omega}(X))_{ij} = X_{ij}$，否则为 $0$。\n    - $P_{\\Omega^c}(X) = X - P_{\\Omega}(X)$。\n    - $\\|X\\|_*$：矩阵 X 的核范数，即其奇异值之和。\n- **近端算子定义**：\n    - 对于一个正常、闭、凸函数 f，$\\operatorname{prox}_{t f}(Y) = \\arg\\min_{X\\in\\mathbb{R}^{m \\times n}} \\left\\{ f(X) + \\frac{1}{2t}\\|X - Y\\|_F^2 \\right\\}$。\n- **目标函数**：\n    - $F(X) = \\frac{1}{2}\\|P_{\\Omega}(X - M)\\|_F^2 + \\lambda \\|X\\|_*$，其中 $\\lambda>0$。\n- **迭代方案**：\n    - 初始化：$X^0 = 0$。\n    - 对 $k=0,1,2,\\dots,K-1$：\n        1. **数据一致性注入**：$Y^k = P_{\\Omega}(M) + P_{\\Omega^c}(X^k)$。\n        2. **近端步骤**：$X^{k+1} = \\operatorname{prox}_{\\lambda \\|\\cdot\\|_*}(Y^k)$。\n- **任务**：\n    - 在每次迭代中计算秩序列 $\\{\\operatorname{rank}(X^{1}), \\operatorname{rank}(X^{2}), \\dots, \\operatorname{rank}(X^{K})\\}$。秩定义为大于指定容差的奇异值数量。\n- **数据生成协议**：\n    - 矩阵维度：$m=20$, $n=15$。\n    - 真实秩：$r_{\\text{true}}=3$。\n    - 真实奇异值：$\\{10, 5, 3\\}$。\n    - 真实矩阵构建：$X_{\\text{true}} = U \\operatorname{diag}(\\{10,5,3\\}) V^T$，其中 U 和 V 具有通过对使用固定种子的高斯随机矩阵进行 QR 分解而生成的正交列。\n    - 观测概率：对每个条目为 $p=0.5$，使用固定种子。\n    - 观测矩阵：$M = P_{\\Omega}(X_{\\text{true}} + \\varepsilon)$，其中 ε 是均值为 0、标准差为 0.01 的高斯噪声，使用固定种子生成。\n- **测试套件**：\n    - 迭代次数：$K=12$。\n    - 秩容差：$10^{-8}$。\n    - 正则化参数：$\\lambda \\in \\{0.0, 1.0, 5.0\\}$。\n- **输出格式**：\n    - 单行 `[[r_{1,1},\\dots,r_{1,12}],[r_{2,1},\\dots,r_{2,12}],[r_{3,1},\\dots,r_{3,12}]]`。\n\n### 步骤 2：使用提取的已知信息进行验证\n- **科学依据**：该问题植根于凸优化这一成熟领域，特别是用于矩阵补全的近端方法。核范数、近端算子和奇异值分解 (SVD) 等概念是该领域的基础。所描述的算法是用于核范数最小化的一个著名迭代方案。\n- **适定性**：该问题定义清晰。它提供了所有必要的参数、一个明确的迭代公式、一个确定性的数据生成协议（通过固定种子），以及精确的输出格式。这确保了可以计算出唯一且有意义的解。\n- **客观性**：该问题以形式化的数学语言陈述，没有歧义、主观性或基于观点的论断。\n\n问题陈述是自包含、一致的，并遵循既定的科学原则。未发现任何缺陷。\n\n### 步骤 3：结论与行动\n该问题是**有效的**。将提供完整解答。\n\n### 解法推导与方法\n\n该迭代方案的核心是计算核范数的近端算子，$X^{k+1} = \\operatorname{prox}_{\\lambda \\|\\cdot\\|_*}(Y^k)$。问题陈述的符号表示建议将其解释为函数 $f(X) = \\lambda \\|X\\|_*$ 在参数 $t=1$ 时的近端算子。根据所提供的近端算子定义，这对应于求解以下优化问题：\n$$\nX^{k+1} = \\arg\\min_{X \\in \\mathbb{R}^{m \\times n}} \\left\\{ \\lambda\\|X\\|_* + \\frac{1}{2}\\|X - Y^k\\|_F^2 \\right\\}\n$$\n这个问题有一个基于 $Y^k$ 的奇异值分解 (SVD) 的闭式解。设 $Y^k$ 的 SVD 为 $Y^k = U \\Sigma V^T$，其中 $U \\in \\mathbb{R}^{m \\times r}$ 和 $V \\in \\mathbb{R}^{n \\times r}$ 是具有正交列的矩阵，$\\Sigma = \\operatorname{diag}(\\sigma_1, \\sigma_2, \\dots, \\sigma_r)$ 是由正奇异值 $\\sigma_i > 0$ 构成的对角矩阵，且 $r = \\operatorname{rank}(Y^k)$。\n\n核范数和 Frobenius 范数都是酉不变的。这意味着对于任何正交矩阵 $A$ 和 $B$，都有 $\\|Z\\|_* = \\|A^T Z B\\|_*$ 和 $\\|Z\\|_F = \\|A^T Z B\\|_F$。将此性质应用于 $A=U$ 和 $B=V$，优化问题可以根据变换后的变量 $\\hat{X} = U^T X V$ 重写为：\n$$\n\\arg\\min_{\\hat{X} \\in \\mathbb{R}^{m \\times n}} \\left\\{ \\lambda\\|\\hat{X}\\|_* + \\frac{1}{2}\\|\\hat{X} - U^T Y^k V\\|_F^2 \\right\\}\n$$\n由于 $Y^k = U \\Sigma V^T$，我们有 $U^T Y^k V = \\Sigma$。问题简化为：\n$$\n\\arg\\min_{\\hat{X} \\in \\mathbb{R}^{m \\times n}} \\left\\{ \\lambda\\|\\hat{X}\\|_* + \\frac{1}{2}\\|\\hat{X} - \\Sigma\\|_F^2 \\right\\}\n$$\n可以证明（例如，通过 von Neumann 迹不等式），对于一个固定的对角矩阵 $\\Sigma$，当 $\\hat{X}$ 也是一个对角矩阵时达到最小值。设 $\\hat{X} = D = \\operatorname{diag}(d_1, d_2, \\dots, d_r)$。$D$ 的奇异值就是 $|d_i|$。为了最小化 $\\|\\hat{X}\\|_*$，我们应该选择 $d_i \\ge 0$。问题于是解耦为一组独立的标量最小化问题，每个奇异值对应一个：\n$$\n\\min_{d_i \\ge 0} \\left\\{ \\lambda d_i + \\frac{1}{2}(d_i - \\sigma_i)^2 \\right\\} \\quad \\text{for } i=1, \\dots, r\n$$\n目标函数是关于 $d_i$ 的一个简单二次函数。它对 $d_i$ 的导数是 $\\lambda + d_i - \\sigma_i$。令导数为零得到一个候选解 $d_i = \\sigma_i - \\lambda$。由于我们要求 $d_i \\ge 0$，最优解是 $d_i = \\max(0, \\sigma_i - \\lambda)$。这个操作被称为软阈值算子，记为 $S_\\lambda(\\sigma_i) = (\\sigma_i - \\lambda)_+$。\n\n因此，最优的对角矩阵是 $\\hat{X}_{\\text{opt}} = S_\\lambda(\\Sigma) = \\operatorname{diag}(S_\\lambda(\\sigma_1), \\dots, S_\\lambda(\\sigma_r))$。为了找到最终解 $X^{k+1}$，我们变换回原始坐标系：\n$$\nX^{k+1} = U \\hat{X}_{\\text{opt}} V^T = U S_\\lambda(\\Sigma) V^T\n$$\n这个过程被称为奇异值阈值化 (SVT)。\n\n完整的算法如下：\n1.  **初始化**：根据指定的确定性协议生成真实矩阵 $X_{\\text{true}}$、观测矩阵 $M$ 和观测掩码 $\\Omega$。设置 $X^0 = 0$。\n2.  **迭代**：对每个指定的 $\\lambda$ 值，以及对 $k=0, \\dots, K-1$：\n    a.  **数据更新**：形成矩阵 $Y^k = M + P_{\\Omega^c}(X^k)$。此步骤保留了来自 $M$ 的已知条目，同时对未知条目使用当前估计 $X^k$。\n    b.  **SVD**：计算 $Y^k$ 的 SVD：$Y^k = U \\Sigma V^T$。\n    c.  **阈值化**：对奇异值应用软阈值算子：$\\Sigma_{\\text{thresh}} = S_\\lambda(\\Sigma)$，其中对角线条目为 $(\\Sigma_{\\text{thresh}})_{ii} = \\max(0, \\Sigma_{ii} - \\lambda)$。\n    d.  **重构**：形成下一个迭代结果 $X^{k+1} = U \\Sigma_{\\text{thresh}} V^T$。\n    e.  **秩计算**：将 $\\operatorname{rank}(X^{k+1})$ 计算为 $\\Sigma_{\\text{thresh}}$ 中大于容差 $10^{-8}$ 的对角线条目数量。\n3.  **存储结果**：对于每个 $\\lambda$，存储计算出的秩序列 $\\{\\operatorname{rank}(X^1), \\dots, \\operatorname{rank}(X^{12})\\}$。最终输出将是一个包含这三个序列的列表。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements the proximal point algorithm for matrix completion and\n    evaluates rank trajectories for different regularization weights.\n    \"\"\"\n    # Parameters from the problem statement\n    m, n = 20, 15\n    r_true = 3\n    singular_values_true = np.array([10.0, 5.0, 3.0])\n    p_obs = 0.5\n    noise_std = 0.01\n    K = 12\n    rank_tol = 1e-8\n    lambda_vals = [0.0, 1.0, 5.0]\n    seed = 0\n\n    # Data generation protocol (deterministic)\n    rng = np.random.default_rng(seed)\n\n    # 1. Construct ground-truth low-rank matrix X_true\n    # Generate random matrices and use QR to get orthonormal columns\n    U_rand = rng.standard_normal((m, r_true))\n    U, _ = np.linalg.qr(U_rand)\n    \n    V_rand = rng.standard_normal((n, r_true))\n    V, _ = np.linalg.qr(V_rand)\n    V_t = V.T\n    \n    X_true = U @ np.diag(singular_values_true) @ V_t\n\n    # 2. Generate observation mask Omega\n    omega_mask = rng.random((m, n))  p_obs\n    omega_mask_c = ~omega_mask\n\n    # 3. Form the observed matrix M\n    noise = rng.normal(loc=0.0, scale=noise_std, size=(m, n))\n    M = np.zeros((m, n))\n    M[omega_mask] = (X_true + noise)[omega_mask]\n\n    all_rank_trajectories = []\n\n    # Investigation loop over lambda values\n    for lambda_val in lambda_vals:\n        rank_trajectory = []\n        X = np.zeros((m, n))\n\n        # Iterative scheme\n        for _ in range(K):\n            # 1. Data-consistency injection\n            Y = M + X * omega_mask_c\n\n            # 2. Proximal step\n            # Compute SVD of Y\n            try:\n                U_svd, s_svd, Vh_svd = np.linalg.svd(Y, full_matrices=False)\n            except np.linalg.LinAlgError:\n                # Handle cases where SVD might fail, though unlikely here\n                s_svd = np.array([])\n                U_svd = np.zeros((m, 0))\n                Vh_svd = np.zeros((0, n))\n\n            # Apply soft-thresholding to singular values\n            s_thresh = np.maximum(0, s_svd - lambda_val)\n            \n            # Reconstruct the matrix X_next\n            # Create a diagonal matrix of the correct size for multiplication\n            Sigma_thresh = np.zeros((len(s_thresh), len(s_thresh)))\n            np.fill_diagonal(Sigma_thresh, s_thresh)\n            \n            X_next = U_svd @ Sigma_thresh @ Vh_svd\n\n            # Compute rank of X_next\n            rank = np.sum(s_thresh > rank_tol)\n            rank_trajectory.append(rank)\n\n            # Update X for the next iteration\n            X = X_next\n            \n        all_rank_trajectories.append(rank_trajectory)\n\n    # Final print statement in the exact required format\n    inner_strs = [f\"[{','.join(map(str, r))}]\" for r in all_rank_trajectories]\n    final_output = f\"[{','.join(inner_strs)}]\"\n    print(final_output)\n\nsolve()\n```", "id": "3168247"}, {"introduction": "现实世界中的矩阵填充问题往往伴随着宝贵的旁路信息。此练习将探讨如何利用图拉普拉斯正则化项，将一个网络（在此案例中为交通传感器网络）的拓扑结构融入矩阵分解模型中。通过实现一个交替最小化求解器，您将学习如何对潜在因子施加结构性先验，从而获得更准确、更有意义的填充结果，尤其是在处理数据稀疏的项时 [@problem_id:3145702]。", "problem": "您将处理一个受交通传感器网络启发的矩阵补全任务。考虑一个传感器-时间数据矩阵 $X \\in \\mathbb{R}^{n \\times T}$，其中缺失条目由一个二元掩码 $M \\in \\{0,1\\}^{n \\times T}$ 指示，这里 $M_{i,t} = 1$ 表示 $X_{i,t}$ 是观测值，而 $M_{i,t} = 0$ 表示它是缺失值。您将使用低秩分解模型来补全该矩阵，并评估通过图正则化器引入道路网络拓扑结构如何影响峰值时间预测的准确性。\n\n基本原理和定义：\n- 通过低秩分解进行矩阵补全假设 $X \\approx U V^{\\top}$，其中 $U \\in \\mathbb{R}^{n \\times r}$ 且 $V \\in \\mathbb{R}^{T \\times r}$，$r$ 是所选的秩。\n- 传感器上的道路网络拓扑由一个无向加权邻接矩阵 $W \\in \\mathbb{R}_{\\ge 0}^{n \\times n}$ 编码，其中 $W_{i,j} = W_{j,i}$ 且 $W_{i,i} = 0$。度矩阵为 $D = \\mathrm{diag}(W \\mathbf{1})$，图拉普拉斯矩阵为 $L = D - W$。\n- 该补全问题被构建为最小化带惩罚的经验风险\n$$\n\\min_{U,V} \\;\\; \\frac{1}{2} \\left\\| M \\odot \\left(X - U V^{\\top}\\right) \\right\\|_{F}^{2} + \\frac{\\lambda_{\\mathrm{U}}}{2}\\,\\mathrm{tr}\\!\\left(U^{\\top} L U\\right) + \\frac{\\lambda}{2}\\left(\\|U\\|_{F}^{2} + \\|V\\|_{F}^{2}\\right),\n$$\n其中 $\\odot$ 表示哈达玛积，$\\lambda_{\\mathrm{U}} \\ge 0$ 控制与图拓扑的对齐程度，而 $\\lambda  0$ 是一个岭正则化器。当 $\\lambda_{\\mathrm{U}} = 0$ 时，这简化为不带图正则化的标准低秩矩阵分解。\n\n任务：\n- 实现一个交替最小化求解器，对于固定的 $r$、$\\lambda_{\\mathrm{U}}$ 和 $\\lambda$，该求解器交替更新 $U$ 和 $V$，直到达到固定的迭代次数。使用下面指定的确定性初始化和确定性数据生成。\n- 补全矩阵得到 $\\widehat{X} = U V^{\\top}$ 后，为每个传感器 $i \\in \\{1,\\dots,n\\}$ 定义其峰值时间索引，对于真实矩阵为 $p_i^{\\star} = \\arg\\max_{t \\in \\{1,\\dots,T\\}} X_{i,t}$，对于补全矩阵为 $\\widehat{p}_i = \\arg\\max_{t \\in \\{1,\\dots,T\\}} \\widehat{X}_{i,t}$。如果出现平局，取最小的索引（$\\arg\\max$ 的默认行为）。\n- 定义总峰值索引误差为 $E(\\widehat{X}; X) = \\sum_{i=1}^{n} \\left| \\widehat{p}_i - p_i^{\\star} \\right|$。\n- 对于下方的每个测试用例，计算图正则化分解相对于非正则化基线的峰值预测改进量，结果为整数：\n$$\n\\Delta = E\\big(\\widehat{X}_{\\text{baseline}}; X\\big) - E\\big(\\widehat{X}_{\\text{graph}}; X\\big).\n$$\n正值的 $\\Delta$ 表示图正则化提高了峰值预测的准确性。\n\n数据生成（确定性且通用）：\n- 维度：$n = 5$, $T = 8$, $r = 2$。\n- 真实因子：\n  - 传感器因子 $U^{\\star} \\in \\mathbb{R}^{n \\times r}$：对于 $i = 1,\\dots,n$，设位置 $p_i = \\frac{i-1}{n-1}$。将第一列设置为 $U^{\\star}_{i,1} = p_i$，对于所有 $i$，第二列设置为 $U^{\\star}_{i,2} = 0.5$。\n  - 时间因子 $V^{\\star} \\in \\mathbb{R}^{T \\times r}$：对于 $t = 1,\\dots,T$，设时间索引 $\\tau_t = t-1$。设置 $V^{\\star}_{t,1} = \\exp\\!\\left(-\\frac{1}{2} \\left(\\frac{\\tau_t - 5}{1.0}\\right)^{2}\\right)$ 和 $V^{\\star}_{t,2} = 0.2 \\cdot \\frac{\\tau_t}{T}$。\n- 带微小噪声的真实矩阵：$X = U^{\\star} (V^{\\star})^{\\top} + \\epsilon$，其中 $\\epsilon$ 的条目是使用伪随机数生成器（种子为 $s_{\\mathrm{truth}} = 1$）从标准差为 $0.01$ 的零均值高斯分布中独立抽取的。\n- 基础观测掩码 $M^{\\mathrm{base}} \\in \\{0,1\\}^{n \\times T}$：使用伪随机数生成器（种子为 $s_{\\mathrm{mask}} = 0$）在 $\\left[0,1\\right)$ 范围内独立抽取均匀随机数，如果抽取的数值至少为 $0.4$，则设置 $M^{\\mathrm{base}}_{i,t} = 1$，否则 $M^{\\mathrm{base}}_{i,t} = 0$（因此期望的观测比例为 $0.6$）。\n- 算法的确定性初始化：在每次运行开始时，使用伪随机数生成器（种子为 $s_{\\mathrm{init}} = 123$）以标准差为 $0.1$ 的独立零均值高斯分布条目来初始化 $U$ 和 $V$。\n\n道路网络图：\n- $n$ 个传感器上的路径图：$W \\in \\mathbb{R}^{n \\times n}$，其中对于 $i \\in \\{1,\\dots,n-1\\}$ 有 $W_{i,i+1} = W_{i+1,i} = 1$，否则 $W_{i,j} = 0$。\n- 空图：$W = 0$（因此 $L = 0$）。\n\n算法设置：\n- 使用 $\\lambda = 0.1$，迭代次数 $N_{\\mathrm{iter}} = 50$。\n- 对于基线，设置 $\\lambda_{\\mathrm{U}} = 0$；对于图正则化运行，设置 $\\lambda_{\\mathrm{U}} = 5.0$。\n\n测试套件：\n- 案例 $1$（理想路径）：$W$ 等于路径图，$M = M^{\\mathrm{base}}$。\n- 案例 $2$（冷启动传感器）：$W$ 等于路径图，$M$ 等于 $M^{\\mathrm{base}}$，但第三整行设为零，即对于所有 $t \\in \\{1,\\dots,T\\}$，$M_{3,t} = 0$。\n- 案例 $3$（完全观测）：$W$ 等于路径图，$M$ 是形状为 $n \\times T$ 的全一矩阵。\n- 案例 $4$（无图信息）：$W = 0$，$M = M^{\\mathrm{base}}$。\n\n程序要求：\n- 实现一个在更新 $V$ 和 $U$ 之间交替进行的交替最小化求解器：\n  - $V$ 的更新必须为每个时间索引求解一个岭正则化加权最小二乘问题，其权重来自 $M$ 的相应列。\n  - $U$ 的更新必须在给定 $V$ 的情况下，精确地最小化关于 $U$ 的目标函数，这需要求解一个通过图拉普拉斯矩阵 $L$ 耦合所有传感器潜在向量的线性系统。\n- 对于每个测试用例，从相同的初始化开始运行基线和图正则化求解器，补全矩阵以得到 $\\widehat{X}$，计算总峰值索引误差 $E(\\widehat{X}; X)$，并报告整数改进量 $\\Delta$。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含按案例 $1$ 到 $4$ 的顺序排列的结果，格式为方括号内的逗号分隔列表，例如 $\\left[\\Delta_1,\\Delta_2,\\Delta_3,\\Delta_4\\right]$。", "solution": "该问题要求实现一种基于低秩分解的图正则化矩阵补全算法。其目标是评估图正则化器对预测模拟传感器网络中峰值时间事件准确性的影响。解法是通过交替最小化找到的。\n\n需要最小化的目标函数是带惩罚的经验风险：\n$$\nJ(U, V) = \\frac{1}{2} \\left\\| M \\odot \\left(X - U V^{\\top}\\right) \\right\\|_{F}^{2} + \\frac{\\lambda_{\\mathrm{U}}}{2}\\,\\mathrm{tr}\\!\\left(U^{\\top} L U\\right) + \\frac{\\lambda}{2}\\left(\\|U\\|_{F}^{2} + \\|V\\|_{F}^{2}\\right)\n$$\n在此，$X \\in \\mathbb{R}^{n \\times T}$ 是数据矩阵，$M \\in \\{0,1\\}^{n \\times T}$ 是观测掩码，$U \\in \\mathbb{R}^{n \\times r}$ 和 $V \\in \\mathbb{R}^{T \\times r}$ 是潜在因子，$L \\in \\mathbb{R}^{n \\times n}$ 是编码传感器拓扑的图拉普拉斯矩阵，而 $\\lambda_{\\mathrm{U}}, \\lambda$ 是正则化参数。项 $\\mathrm{tr}\\!\\left(U^{\\top} L U\\right)$ 鼓励相连传感器的潜在因子相似。\n\n该优化问题在 $U$ 和 $V$ 上不是联合凸的。然而，它是双凸的，意味着对于固定的 $V$，该问题在 $U$ 上是凸的，对于固定的 $U$，该问题在 $V$ 上是凸的。这种结构启发我们采用交替最小化方法，即我们迭代地求解一个因子矩阵，同时保持另一个因子矩阵固定。\n\n### 交替最小化算法\n\n我们初始化 $U$ 和 $V$，然后在固定的迭代次数内，在两个主要步骤之间交替进行。\n\n#### 1. 更新 V（固定 U）\n对于固定的 $U$，依赖于 $V$ 的目标函数项为：\n$$\nJ(V) = \\frac{1}{2} \\left\\| M \\odot \\left(X - U V^{\\top}\\right) \\right\\|_{F}^{2} + \\frac{\\lambda}{2} \\|V\\|_{F}^{2} = \\frac{1}{2} \\sum_{t=1}^{T} \\sum_{i=1}^{n} M_{i,t} (X_{i,t} - \\mathbf{u}_i^{\\top} \\mathbf{v}_t)^2 + \\frac{\\lambda}{2} \\sum_{t=1}^{T} \\|\\mathbf{v}_t\\|_2^2\n$$\n其中 $\\mathbf{u}_i$ 是 $U$ 的第 $i$ 行，$\\mathbf{v}_t$ 是 $V$ 的第 $t$ 行。该问题按时间索引 $t$ 解耦。对于每个 $t \\in \\{1, \\dots, T\\}$，我们最小化：\n$$\nJ_t(\\mathbf{v}_t) = \\frac{1}{2} \\sum_{i=1}^{n} M_{i,t} (X_{i,t} - \\mathbf{u}_i^{\\top} \\mathbf{v}_t)^2 + \\frac{\\lambda}{2} \\|\\mathbf{v}_t\\|_2^2\n$$\n这是一个岭正则化加权最小二乘问题。设 $M_t = \\mathrm{diag}(M_{1,t}, \\dots, M_{n,t})$ 是时间 $t$ 的权重对角矩阵。关于 $\\mathbf{v}_t$ 的问题等价于最小化 $\\frac{1}{2}\\|\\sqrt{M_t}(\\mathbf{x}_t - U\\mathbf{v}_t)\\|_2^2 + \\frac{\\lambda}{2}\\|\\mathbf{v}_t\\|_2^2$，其中 $\\mathbf{x}_t$ 是 $X$ 的第 $t$ 列。对 $\\mathbf{v}_t$ 求梯度并令其为零，得到正规方程：\n$$\n\\left( U^{\\top} M_t U + \\lambda I_r \\right) \\mathbf{v}_t = U^{\\top} M_t \\mathbf{x}_t\n$$\n其中 $I_r$ 是 $r \\times r$ 的单位矩阵。更新后的 $\\mathbf{v}_t$ 通过求解这个 $r \\times r$ 线性系统得到。这可以更明确地表示为：设 $\\mathcal{I}_t = \\{ i \\mid M_{i,t} = 1 \\}$ 是在时间 $t$ 观测到的传感器集合。设 $U_{\\mathcal{I}_t}$ 是 $U$ 的子矩阵，其行由 $\\mathcal{I}_t$ 索引。方程为：\n$$\n\\left( U_{\\mathcal{I}_t}^{\\top} U_{\\mathcal{I}_t} + \\lambda I_r \\right) \\mathbf{v}_t = U_{\\mathcal{I}_t}^{\\top} \\mathbf{x}_{\\mathcal{I}_t, t}\n$$\n\n#### 2. 更新 U（固定 V）\n对于固定的 $V$，依赖于 $U$ 的目标函数项为：\n$$\nJ(U) = \\frac{1}{2} \\left\\| M \\odot \\left(X - U V^{\\top}\\right) \\right\\|_{F}^{2} + \\frac{\\lambda_{\\mathrm{U}}}{2}\\,\\mathrm{tr}\\!\\left(U^{\\top} L U\\right) + \\frac{\\lambda}{2}\\|U\\|_{F}^{2}\n$$\n与 V 的更新不同，该问题不会按传感器索引 $i$ 解耦，因为图正则化器 $\\mathrm{tr}(U^{\\top} L U) = \\sum_{i,j} L_{ij} \\mathbf{u}_i^{\\top} \\mathbf{u}_j$ 耦合了 $U$ 的行。我们必须同时求解 $U$ 的所有行。对单行 $\\mathbf{u}_i$ 求梯度并令其为零，得到：\n$$\n\\left( \\sum_{t=1}^{T} M_{i,t} \\mathbf{v}_t \\mathbf{v}_t^{\\top} + \\lambda I_r \\right) \\mathbf{u}_i + \\lambda_{\\mathrm{U}} \\sum_{j=1}^{n} L_{ij} \\mathbf{u}_j = \\sum_{t=1}^{T} M_{i,t} X_{i,t} \\mathbf{v}_t\n$$\n这为 $n$ 个向量 $\\mathbf{u}_1, \\dots, \\mathbf{u}_n$ 得到一个包含 $n$ 个耦合线性方程的系统。我们可以将其表示为单个大型线性系统。设 $\\mathbf{u} = \\mathrm{vec}(U) \\in \\mathbb{R}^{nr}$ 是通过堆叠 $U$ 的行得到的向量。该系统形如 $\\mathcal{A} \\mathbf{u} = \\mathbf{b}$，其中 $\\mathcal{A}$ 是一个 $nr \\times nr$ 的块矩阵，$\\mathbf{b}$ 是一个长度为 $nr$ 的向量。$\\mathcal{A}$ 的第 $(i,j)$ 个块（一个 $r \\times r$ 矩阵）由以下公式给出：\n$$\n\\mathcal{A}_{ij} = \\delta_{ij} \\left( \\sum_{t=1}^{T} M_{i,t} \\mathbf{v}_t \\mathbf{v}_t^{\\top} + \\lambda I_r \\right) + \\lambda_{\\mathrm{U}} L_{ij} I_r\n$$\n其中 $\\delta_{ij}$ 是克罗内克 δ。向量 $\\mathbf{b}$ 的第 $i$ 个块是 $\\mathbf{b}_i = \\sum_{t=1}^{T} M_{i,t} X_{i,t} \\mathbf{v}_t$。矩阵 $\\mathcal{A}$ 是对称正定的（因为 $L$ 是半正定的且 $\\lambda  0$），确保了 $U$ 的唯一解。更新后的 $U$ 通过求解这个 $nr \\times nr$ 线性系统并重塑结果向量得到。\n\n### 评估\n在运行交替最小化固定次数的迭代后，补全的矩阵为 $\\widehat{X} = UV^{\\top}$。通过将预测的峰值时间索引 $\\widehat{p}_i = \\arg\\max_t \\widehat{X}_{i,t}$ 与真实的峰值时间索引 $p_i^{\\star} = \\arg\\max_t X_{i,t}$ 进行比较来评估性能。总峰值索引误差为 $E(\\widehat{X}; X) = \\sum_{i=1}^{n} | \\widehat{p}_i - p_i^{\\star} |$。最终报告的指标是图正则化带来的改进量 $\\Delta$，定义为基线模型（$\\lambda_{\\mathrm{U}}=0$）和图正则化模型（$\\lambda_{\\mathrm{U}}0$）之间的误差差值：$\\Delta = E_{\\text{baseline}} - E_{\\text{graph}}$。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\n# language: Python\n# version: 3.12\n# libraries:\n#   - name: numpy\n#     version: 1.23.5\n#   - name: scipy\n#     version: 1.11.4\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to orchestrate the matrix completion task for all test cases.\n    \"\"\"\n    # Problem Parameters\n    n, T, r = 5, 8, 2\n    lambda_reg = 0.1\n    n_iter = 50\n    lambda_U_graph = 5.0\n    s_truth, s_mask, s_init = 1, 0, 123\n\n    def alternating_minimization(X, M, L, lambda_U, n, T, r, lambda_reg, n_iter, seed_init):\n        \"\"\"\n        Implements the alternating minimization solver for graph-regularized matrix completion.\n        \"\"\"\n        rng_init = np.random.default_rng(seed=seed_init)\n        U = rng_init.normal(0, 0.1, size=(n, r))\n        V = rng_init.normal(0, 0.1, size=(T, r))\n\n        for _ in range(n_iter):\n            # 1. V-update (holding U fixed)\n            # This part solves a separate ridge regression problem for each time point t.\n            for t in range(T):\n                observed_sensors = M[:, t].astype(bool)\n                if not np.any(observed_sensors):\n                    # No observations at this time point, impute with zero vector\n                    V[t, :] = 0.0\n                    continue\n                \n                U_obs = U[observed_sensors, :]\n                X_obs = X[observed_sensors, t]\n                \n                # A_t = U_obs.T @ U_obs + lambda * I\n                A_t = U_obs.T.dot(U_obs) + lambda_reg * np.identity(r)\n                # b_t = U_obs.T @ X_obs_t\n                b_t = U_obs.T.dot(X_obs)\n                \n                # Solve A_t v_t = b_t\n                V[t, :] = np.linalg.solve(A_t, b_t)\n\n            # 2. U-update (holding V fixed)\n            # This part solves one large linear system for all of U.\n            A_u = lambda_U * np.kron(L, np.identity(r))\n            b_u = np.zeros(n * r)\n\n            for i in range(n):\n                observed_times = M[i, :].astype(bool)\n                if not np.any(observed_times):\n                    # Cold-start sensor: no observations\n                    C_i = lambda_reg * np.identity(r)\n                    b_i = np.zeros(r)\n                else:\n                    V_obs = V[observed_times, :]\n                    X_obs = X[i, observed_times]\n                    \n                    # C_i = V_obs.T @ V_obs + lambda * I\n                    C_i = V_obs.T.dot(V_obs) + lambda_reg * np.identity(r)\n                    # b_i = V_obs.T @ X_obs_i\n                    b_i = V_obs.T.dot(X_obs)\n                \n                # Fill the block matrix A_u and vector b_u\n                A_u[i*r:(i+1)*r, i*r:(i+1)*r] += C_i\n                b_u[i*r:(i+1)*r] = b_i\n            \n            # Solve A_u u = b_u and reshape to get U\n            u_vec = np.linalg.solve(A_u, b_u)\n            U = u_vec.reshape((n, r))\n        \n        return U.dot(V.T)\n\n    # --- Data Generation ---\n    # Ground truth factors U_star, V_star\n    p_i = np.linspace(0, 1, n)\n    U_star = np.zeros((n, r))\n    U_star[:, 0] = p_i\n    U_star[:, 1] = 0.5\n    \n    tau_t = np.arange(T, dtype=float)\n    V_star = np.zeros((T, r))\n    V_star[:, 0] = np.exp(-0.5 * ((tau_t - 5.0) / 1.0)**2)\n    V_star[:, 1] = 0.2 * tau_t / T\n    \n    # Ground truth matrix X with Gaussian noise\n    rng_truth = np.random.default_rng(seed=s_truth)\n    epsilon = rng_truth.normal(0, 0.01, size=(n, T))\n    X_true = U_star.dot(V_star.T) + epsilon\n    \n    # Base observation mask\n    rng_mask = np.random.default_rng(seed=s_mask)\n    M_base = rng_mask.uniform(0, 1, size=(n, T)) >= 0.4\n    \n    # --- Graph and Test Case Definitions ---\n    # Path graph\n    W_path = np.zeros((n, n))\n    for i in range(n - 1):\n        W_path[i, i + 1] = 1\n        W_path[i + 1, i] = 1\n    \n    # Null graph\n    W_null = np.zeros((n, n))\n\n    def get_laplacian(W):\n        D = np.diag(np.sum(W, axis=1))\n        return D - W\n\n    L_path = get_laplacian(W_path)\n    L_null = get_laplacian(W_null)\n\n    # Function to modify mask for Case 2\n    def mod_m_case2(m):\n        m_mod = m.copy()\n        m_mod[2, :] = 0  # Sensor 3 is a cold-start sensor\n        return m_mod\n\n    test_cases = [\n        {'L': L_path, 'M': M_base},\n        {'L': L_path, 'M': mod_m_case2(M_base)},\n        {'L': L_path, 'M': np.ones((n, T), dtype=bool)},\n        {'L': L_null, 'M': M_base},\n    ]\n\n    p_star = np.argmax(X_true, axis=1)\n    results = []\n\n    # --- Main Loop ---\n    for case in test_cases:\n        M, L = case['M'], case['L']\n        \n        # Run baseline model (lambda_U = 0)\n        X_hat_base = alternating_minimization(X_true, M, L, lambda_U=0.0, \n                                              n=n, T=T, r=r, lambda_reg=lambda_reg, \n                                              n_iter=n_iter, seed_init=s_init)\n        p_hat_base = np.argmax(X_hat_base, axis=1)\n        E_base = np.sum(np.abs(p_hat_base - p_star))\n\n        # Run graph-regularized model\n        X_hat_graph = alternating_minimization(X_true, M, L, lambda_U=lambda_U_graph,\n                                               n=n, T=T, r=r, lambda_reg=lambda_reg, \n                                               n_iter=n_iter, seed_init=s_init)\n        p_hat_graph = np.argmax(X_hat_graph, axis=1)\n        E_graph = np.sum(np.abs(p_hat_graph - p_star))\n\n        delta = E_base - E_graph\n        results.append(int(delta))\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3145702"}, {"introduction": "选择合适的正则化器是统计建模的关键环节。本练习设置了一项比较研究，旨在对比两种流行的低秩正则化器：迹范数（trace norm）和最大范数（max norm）。通过一个可以控制底层数据结构“尖峰度”（spikiness）的模拟实验，您将研究不同正则化器在不同条件下的性能表现，从而深入了解每种方法的统计特性和实际优势 [@problem_id:3145784]。", "problem": "您必须编写一个完整、可运行的程序，模拟在不同奇异向量尖峰度水平下，两种不同正则化器作用下的矩阵补全过程，并通过对已观测与未观测条目进行训练/测试集划分，来评估哪种正则化器能更好地控制过拟合。该程序的教学背景是统计学习中的矩阵分解和矩阵补全。程序必须实现以下设定并产生指定的输出。\n\n从以下核心定义和原理开始。考虑一个秩为 $r$ 的目标矩阵 $X^{\\star} \\in \\mathbb{R}^{m \\times n}$。其部分条目在带有加性噪声的情况下被观测到，目标是从这些带噪观测中估计 $X^{\\star}$。观测索引集表示为 $\\Omega \\subseteq \\{1,\\dots,m\\} \\times \\{1,\\dots,n\\}$，经验风险是在观测条目上的均方误差。一种常见方法是采用低秩矩阵分解 $X = U V^{\\top}$，其中 $U \\in \\mathbb{R}^{m \\times r}$ 且 $V \\in \\mathbb{R}^{n \\times r}$，然后进行正则化以控制复杂度。本文考虑两种正则化器：\n\n1. 迹范数（也称核范数）正则化，通过一个基于分解的、控制 $U$ 和 $V$ 的弗罗贝尼乌斯范数的惩罚项来实现。\n\n2. 最大范数正则化，通过控制 $U$ 和 $V$ 的最大行 $\\ell_{2}$ 范数的约束来实现。\n\n程序必须通过构造 $X^{\\star} = A B^{\\top}$ 来模拟 $X^{\\star}$ 奇异向量中的尖峰度，其中 $A \\in \\mathbb{R}^{m \\times r}$ 和 $B \\in \\mathbb{R}^{n \\times r}$ 是通过对随机矩阵的行范数进行缩放以引入预设尖峰度水平而生成的。设尖峰度参数 $s \\in [0,1]$ 控制行相干性。具体来说，构造一个权重向量 $w^{A} \\in \\mathbb{R}^{m}$，其中 $w^{A}_{1} = 1 + s \\cdot (m - 1)$ 且对于所有 $i \\in \\{2,\\dots,m\\}$ 有 $w^{A}_{i} = 1$，然后将其归一化，使其算术平均值为1，即用 $w^{A} \\cdot \\frac{m}{\\sum_{i=1}^{m} w^{A}_{i}}$ 替换 $w^{A}$。类似地，构造 $w^{B} \\in \\mathbb{R}^{n}$，其中 $w^{B}_{1} = 1 + s \\cdot (n - 1)$ 且对于所有 $j \\in \\{2,\\dots,n\\}$ 有 $w^{B}_{j} = 1$，然后将其归一化，使其算术平均值为1。通过从标准正态分布中独立抽取每个条目，并将 $A$ 的行按 $\\sqrt{w^{A}_{i}}$、$B$ 的行按 $\\sqrt{w^{B}_{j}}$ 进行缩放，来生成 $A$ 和 $B$。最后，缩放 $X^{\\star}$ 以使 $\\lVert X^{\\star} \\rVert_{F}$ 等于 $\\sqrt{m n}$。\n\n观测值的生成如下。对于给定的观测比例 $p \\in (0,1)$，采样一个掩码 $W \\in \\{0,1\\}^{m \\times n}$，其条目为独立的伯努利分布（参数为 $p$）（即 $W_{ij} \\sim \\mathrm{Bernoulli}(p)$）。设噪声标准差为 $\\sigma  0$。观测数据矩阵 $Y$ 定义为 $Y_{ij} = W_{ij} \\cdot \\left( X^{\\star}_{ij} + \\varepsilon_{ij} \\right)$，其中 $\\varepsilon_{ij} \\sim \\mathcal{N}(0, \\sigma^{2})$ 且相互独立。在训练中，只有 $W_{ij} = 1$ 的条目通过与 $W$ 进行逐元素相乘被包含在损失中。\n\n您必须实现并训练两个形式为 $U V^{\\top}$（固定秩 $r$）的估计量 $X_{\\mathrm{trace}}$ 和 $X_{\\mathrm{max}}$，使用基于梯度的优化方法：\n\n- 迹范数代理估计量：最小化观测条目上的平方误差和加上一个与 $\\lVert U \\rVert_{F}^{2} + \\lVert V \\rVert_{F}^{2}$ 成比例的惩罚项，使用固定的正则化参数 $\\lambda  0$ 以及固定的学习率和迭代次数。\n\n- 最大范数代理估计量：最小化观测条目上的平方误差和，约束条件为 $U$ 和 $V$ 的最大行 $\\ell_{2}$ 范数受固定参数 $\\alpha  0$ 的限制。实现方式为：对观测条目上的平方误差执行梯度下降步骤，并在每步之后，将 $U$ 和 $V$ 的每一行投影到以原点为中心、半径为 $\\alpha$ 的闭 $\\ell_{2}$ 球上。\n\n为确保公平比较，在每个测试用例开始时，为两个估计量使用相同的随机矩阵初始化 $U$ 和 $V$。训练后，计算每个估计量的泛化误差，即在未观测条目上（即掩码 $W$ 的补集上）的均方误差，使用无噪声的 $X^{\\star}$ 作为基准真相。将一个测试用例的最终决策定义为一个布尔值，当且仅当最大范数代理估计量取得了严格小于迹范数代理估计量的泛化误差时，该值为真。\n\n测试套件。您的程序必须运行以下三个测试用例，无需任何外部输入，每个用例都由整数和浮点数完全指定：\n\n- 用例 1：$(m,n,r,s,p,\\sigma) = (30,30,2,0.0,0.4,0.1)$，随机性种子为 $42$。\n\n- 用例 2：$(m,n,r,s,p,\\sigma) = (30,30,2,0.9,0.4,0.1)$，随机性种子为 $43$。\n\n- 用例 3：$(m,n,r,s,p,\\sigma) = (30,30,2,0.9,0.1,0.1)$，随机性种子为 $44$。\n\n在所有测试用例中使用相同的固定超参数：正则化强度 $\\lambda = 0.08$，最大行范数界 $\\alpha = 0.9$，一个恒定的学习率（选择一个合理的较小正值），以及一个固定的迭代次数（选择一个合理的正整数，大到足以在实践中确保收敛）。确保优化过程中的数值稳定性。\n\n最终输出格式。您的程序应产生单行输出，包含这三个用例的布尔值列表，指明每个用例中最大范数代理估计量是否比迹范数代理估计量具有严格更小的泛化误差。该行必须是用方括号括起来的逗号分隔列表，不含空格，例如 $[{\\tt True},{\\tt False},{\\tt True}]$。不应打印任何其他文本。", "solution": "该问题要求对两种用于低秩矩阵补全的正则化方法进行比较分析：一种是迹范数代理方法，另一种是最大范数代理方法。该分析通过一项模拟研究进行，该研究改变了基准真相矩阵奇异向量的“尖峰度”以及观测条目的比例。\n\n### 1. 数据生成模型\n\n模拟的基础是生成一个具有受控属性的、秩为 $r$ 的合成基准真相矩阵 $X^{\\star} \\in \\mathbb{R}^{m \\times n}$。\n\n**1.1. 尖峰奇异向量的构造**\n奇异向量的结构由一个尖峰度参数 $s \\in [0,1]$ 控制。$s=0$ 的值对应于非尖峰情况，此时行范数期望是均匀的；而 $s  0$ 则会引入一个尖峰，使得因子矩阵的第一行的范数比其他行更大。这种行范数的不均匀性是矩阵补全理论中的一个关键概念——非均匀相干性的一个代理。\n\n目标矩阵构造为 $X^{\\star} = A B^{\\top}$，其中 $A \\in \\mathbb{R}^{m \\times r}$ 且 $B \\in \\mathbb{R}^{n \\times r}$。过程如下：\n\n1.  **定义权重向量**：定义两个权重向量 $w^{A} \\in \\mathbb{R}^{m}$ 和 $w^{B} \\in \\mathbb{R}^{n}$，以控制矩阵 $A$ 和 $B$ 的行范数。对于一个尖峰度参数 $s$，未归一化的权重为：\n    $$\n    w^{A}_{1} = 1 + s \\cdot (m - 1), \\quad w^{A}_{i} = 1 \\text{ for } i \\in \\{2,\\dots,m\\}\n    $$\n    $$\n    w^{B}_{1} = 1 + s \\cdot (n - 1), \\quad w^{B}_{j} = 1 \\text{ for } j \\in \\{2,\\dots,n\\}\n    $$\n\n2.  **归一化权重**：然后将这些向量归一化，使其算术平均值为1。$w^A$ 的归一化因子是 $\\frac{m}{\\sum_{i=1}^{m} w^{A}_{i}}$，$w^B$ 的归一化因子类似。求和结果为 $\\sum_{i=1}^{m} w^{A}_{i} = (1 + s(m-1)) + (m-1) = m + s(m-1)$。因此，归一化后的权重是：\n    $$\n    w^{A}_{\\text{norm}} = w^A \\cdot \\frac{m}{m + s(m-1)}\n    $$\n    $$\n    w^{B}_{\\text{norm}} = w^B \\cdot \\frac{n}{n + s(n-1)}\n    $$\n\n3.  **生成并缩放因子**：首先从标准正态分布中抽取矩阵 $A$ 和 $B$ 的条目，我们称之为 $\\tilde{A}$ 和 $\\tilde{B}$。然后，将每一行乘以对应归一化权重的平方根进行缩放：\n    $$\n    A_{i,:} = \\tilde{A}_{i,:} \\cdot \\sqrt{w^{A}_{\\text{norm},i}} \\quad \\text{for } i=1,\\dots,m\n    $$\n    $$\n    B_{j,:} = \\tilde{B}_{j,:} \\cdot \\sqrt{w^{B}_{\\text{norm},j}} \\quad \\text{for } j=1,\\dots,n\n    $$\n    现在，$A$ 的第 $i$ 行的期望 $\\ell_2$ 范数平方与 $w^{A}_{\\text{norm},i}$ 成正比。\n\n4.  **构造并归一化目标矩阵**：基准真相矩阵为 $X^{\\star} = AB^{\\top}$。最后，对 $X^{\\star}$ 进行缩放，使其弗罗贝尼乌斯范数为 $\\sqrt{mn}$，这确保了其条目的均方根为1。\n    $$\n    X^{\\star} \\leftarrow X^{\\star} \\cdot \\frac{\\sqrt{mn}}{\\lVert X^{\\star} \\rVert_{F}}\n    $$\n\n**1.2. 观测模型**\n$X^{\\star}$ 的一部分条目被观测到，并被加性高斯噪声所破坏。\n- 生成一个观测掩码 $W \\in \\{0,1\\}^{m \\times n}$，其中每个条目 $W_{ij}$ 是一个独立的伯努利随机变量，参数为 $p$，即 $W_{ij} \\sim \\mathrm{Bernoulli}(p)$。$W_{ij}=1$ 表示一个观测条目。\n- 观测数据矩阵 $Y \\in \\mathbb{R}^{m \\times n}$ 是通过提取 $X^{\\star}$ 中与掩码 $W$ 对应的条目，并加上独立同分布的噪声 $\\varepsilon_{ij} \\sim \\mathcal{N}(0, \\sigma^2)$ 而形成的。问题将 $Y_{ij}$ 定义为 $Y_{ij} = W_{ij} \\cdot (X^{\\star}_{ij} + \\varepsilon_{ij})$。为了优化，更方便的做法是定义完整的含噪矩阵 $Y_{full} = X^{\\star} + \\mathcal{E}$，其中 $\\mathcal{E}$ 是噪声项矩阵，然后使用掩码 $W$ 来为损失函数选择条目。\n\n### 2. 估计量与优化算法\n\n目标是使用低秩分解 $X = UV^{\\top}$（其中 $U \\in \\mathbb{R}^{m \\times r}$ 且 $V \\in \\mathbb{R}^{n \\times r}$）来估计 $X^{\\star}$。比较了两种正则化方案。两者都使用基于梯度的方法进行训练，固定学习率 $\\eta=0.005$，迭代 $2000$ 次。选择这些超参数是考虑到问题规模的合理性。\n\n**2.1. 迹范数代理估计量 ($X_{\\mathrm{trace}}$)**\n一个矩阵的迹范数是其奇异值之和。秩最小化问题的一个常见凸松弛是最小化迹范数。对于分解 $X=UV^\\top$，惩罚项 $\\lambda(\\lVert U \\rVert_F^2 + \\lVert V \\rVert_F^2)$ 作为迹范数惩罚项 $\\lambda \\lVert X \\rVert_*$ 的一个非凸代理。需要最小化的目标函数是观测条目上的正则化平方误差和：\n$$\n\\mathcal{L}_{\\mathrm{trace}}(U, V) = \\frac{1}{2}\\left\\| W \\odot (UV^{\\top} - Y_{full}) \\right\\|_{F}^{2} + \\frac{\\lambda}{2} (\\left\\| U \\right\\|_{F}^{2} + \\left\\| V \\right\\|_{F}^{2})\n$$\n其中 $\\odot$ 表示逐元素（哈达玛）积，$\\lambda=0.08$ 是正则化参数。\n\n优化通过梯度下降进行。目标函数关于 $U$ 和 $V$ 的梯度是：\n$$\n\\nabla_{U} \\mathcal{L}_{\\mathrm{trace}} = (W \\odot (UV^{\\top} - Y_{full})) V + \\lambda U\n$$\n$$\n\\nabla_{V} \\mathcal{L}_{\\mathrm{trace}} = (W \\odot (UV^{\\top} - Y_{full}))^{\\top} U + \\lambda V\n$$\n更新规则是 $U \\leftarrow U - \\eta \\nabla_{U} \\mathcal{L}_{\\mathrm{trace}}$ 和 $V \\leftarrow V - \\eta \\nabla_{V} \\mathcal{L}_{\\mathrm{trace}}$。\n\n**2.2. 最大范数代理估计量 ($X_{\\mathrm{max}}$)**\n最大范数正则化通过约束因子矩阵 $U$ 和 $V$ 的行的最大 $\\ell_2$ 范数来控制模型的复杂度。这种方法已被证明是有效的，尤其是在数据表现出非均匀性（尖峰度）时。优化问题是：\n$$\n\\text{minimize} \\quad \\mathcal{L}(U, V) = \\frac{1}{2}\\left\\| W \\odot (UV^{\\top} - Y_{full}) \\right\\|_{F}^{2}\n$$\n$$\n\\text{subject to} \\quad \\max_{i} \\|U_{i,:}\\|_{2} \\le \\alpha \\quad \\text{and} \\quad \\max_{j} \\|V_{j,:}\\|_{2} \\le \\alpha\n$$\n其中 $U_{i,:}$ 是 $U$ 的第 $i$ 行，$V_{j,:}$ 是 $V$ 的第 $j$ 行，$\\alpha=0.9$ 是最大行范数界。\n\n这个带约束的问题通过投影梯度下降（PGD）来解决。每次迭代包括两个步骤：\n1.  **梯度步**：通过在无约束损失函数 $\\mathcal{L}(U,V)$ 的负梯度方向上移动一步来更新 $U$ 和 $V$。设中间更新为 $\\tilde{U}$ 和 $\\tilde{V}$。\n    $$\n    \\nabla_{U} \\mathcal{L} = (W \\odot (UV^{\\top} - Y_{full})) V \\quad \\Rightarrow \\quad \\tilde{U} = U - \\eta \\nabla_{U} \\mathcal{L}\n    $$\n    $$\n    \\nabla_{V} \\mathcal{L} = (W \\odot (UV^{\\top} - Y_{full}))^{\\top} U \\quad \\Rightarrow \\quad \\tilde{V} = V - \\eta \\nabla_{V} \\mathcal{L}\n    $$\n2.  **投影步**：将 $\\tilde{U}$ 和 $\\tilde{V}$ 的每一行投影回半径为 $\\alpha$ 的闭 $\\ell_2$ 球上。对于 $\\tilde{U}$ 的每一行 $u_i$：\n    $$\n    U_{i,:} \\leftarrow u_i \\cdot \\min\\left(1, \\frac{\\alpha}{\\|u_i\\|_2}\\right)\n    $$\n    同样的投影也应用于 $\\tilde{V}$ 的每一行 $v_j$。如果一行的范数为零，它将保持为零。\n\n### 3. 评估与比较\n\n每个估计量的性能通过其泛化误差来衡量，该误差定义为在*未观测*条目集（表示为 $\\Omega^c = \\{(i,j) | W_{ij}=0\\}$）上的均方误差（MSE）。对于一个估计矩阵 $\\hat{X}=UV^\\top$，其泛化误差为：\n$$\n\\text{MSE}_{\\text{unobs}}(\\hat{X}) = \\frac{\\sum_{(i,j) \\in \\Omega^c} (\\hat{X}_{ij} - X^{\\star}_{ij})^2}{|\\Omega^c|} = \\frac{\\left\\| (1-W) \\odot (\\hat{X} - X^{\\star}) \\right\\|_F^2}{\\sum_{i,j}(1-W_{ij})}\n$$\n对于每个测试用例，最终决策是一个布尔值，当最大范数估计量取得的泛化误差严格小于迹范数估计量时，该值为 `True`（即 $\\text{MSE}_{\\text{unobs}}(X_{\\mathrm{max}})  \\text{MSE}_{\\text{unobs}}(X_{\\mathrm{trace}})$），否则为 `False`。理论结果表明，最大范数正则化对尖峰度应具有更强的鲁棒性，本模拟旨在检验这一点。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the matrix completion simulation for all test cases.\n    \"\"\"\n    \n    # Define fixed hyperparameters for the optimization algorithms.\n    # A learning rate of 0.005 and 2000 iterations were chosen as reasonable\n    # values for convergence for the given problem size and parameters.\n    LEARNING_RATE = 0.005\n    ITERATIONS = 2000\n    LAMBDA_REG = 0.08  # Regularization for trace-norm surrogate\n    ALPHA_BOUND = 0.9   # Max row-norm bound for max-norm surrogate\n\n    test_cases = [\n        # (m, n, r, s, p, sigma, seed)\n        (30, 30, 2, 0.0, 0.4, 0.1, 42),\n        (30, 30, 2, 0.9, 0.4, 0.1, 43),\n        (30, 30, 2, 0.9, 0.1, 0.1, 44),\n    ]\n\n    results = []\n    \n    for m, n, r, s, p, sigma, seed in test_cases:\n        rng = np.random.default_rng(seed)\n\n        # 1. Generate Ground Truth Matrix X_star\n        # Generate weight vectors w_A and w_B for spikiness\n        w_A = np.ones(m)\n        if m > 1:\n            w_A[0] = 1 + s * (m - 1)\n        w_A *= m / np.sum(w_A)\n        \n        w_B = np.ones(n)\n        if n > 1:\n            w_B[0] = 1 + s * (n - 1)\n        w_B *= n / np.sum(w_B)\n\n        # Generate factor matrices A and B and scale rows\n        A = rng.standard_normal(size=(m, r)) * np.sqrt(w_A[:, np.newaxis])\n        B = rng.standard_normal(size=(n, r)) * np.sqrt(w_B[:, np.newaxis])\n        \n        X_star = A @ B.T\n        \n        # Normalize X_star to have Frobenius norm sqrt(mn)\n        norm_X_star = np.linalg.norm(X_star, 'fro')\n        if norm_X_star > 1e-9: # Avoid division by zero\n            X_star *= np.sqrt(m * n) / norm_X_star\n\n        # 2. Generate Observed Data\n        # Generate mask W for observed entries\n        W = rng.binomial(1, p, size=(m, n)).astype(np.float64)\n        \n        # Generate noise and the full noisy matrix Y_full\n        noise = rng.normal(0, sigma, size=(m, n))\n        Y_full = X_star + noise\n\n        # 3. Initialize Factor Matrices U and V for optimization\n        U_init = rng.standard_normal(size=(m, r))\n        V_init = rng.standard_normal(size=(n, r))\n\n        # 4. Train Trace-Norm Surrogate Estimator\n        U_trace, V_trace = U_init.copy(), V_init.copy()\n        for _ in range(ITERATIONS):\n            X_pred = U_trace @ V_trace.T\n            grad_loss_part = W * (X_pred - Y_full)\n            \n            grad_U = grad_loss_part @ V_trace + LAMBDA_REG * U_trace\n            grad_V = grad_loss_part.T @ U_trace + LAMBDA_REG * V_trace\n            \n            U_trace -= LEARNING_RATE * grad_U\n            V_trace -= LEARNING_RATE * grad_V\n\n        # 5. Train Max-Norm Surrogate Estimator\n        U_max, V_max = U_init.copy(), V_init.copy()\n        for _ in range(ITERATIONS):\n            X_pred = U_max @ V_max.T\n            grad_loss_part = W * (X_pred - Y_full)\n\n            # Gradient step (without regularization term)\n            grad_U = grad_loss_part @ V_max\n            grad_V = grad_loss_part.T @ U_max\n            \n            U_max -= LEARNING_RATE * grad_U\n            V_max -= LEARNING_RATE * grad_V\n\n            # Projection step\n            row_norms_U = np.linalg.norm(U_max, axis=1)\n            scales_U = np.minimum(1.0, ALPHA_BOUND / (row_norms_U + 1e-9))\n            U_max *= scales_U[:, np.newaxis]\n\n            row_norms_V = np.linalg.norm(V_max, axis=1)\n            scales_V = np.minimum(1.0, ALPHA_BOUND / (row_norms_V + 1e-9))\n            V_max *= scales_V[:, np.newaxis]\n\n        # 6. Evaluate Generalization Error\n        W_unobs = 1 - W\n        num_unobs = np.sum(W_unobs)\n\n        if num_unobs == 0:\n            # This case should not happen with p  1 and the given sizes,\n            # but is handled for robustness.\n            mse_trace, mse_max = 0.0, 0.0\n        else:\n            X_trace = U_trace @ V_trace.T\n            error_trace = np.sum(W_unobs * (X_trace - X_star)**2)\n            mse_trace = error_trace / num_unobs\n            \n            X_max = U_max @ V_max.T\n            error_max = np.sum(W_unobs * (X_max - X_star)**2)\n            mse_max = error_max / num_unobs\n        \n        # 7. Compare and store result\n        results.append(mse_max  mse_trace)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, [r.title() for r in results]))}]\")\n\nif __name__ == '__main__':\n    solve()\n```", "id": "3145784"}]}