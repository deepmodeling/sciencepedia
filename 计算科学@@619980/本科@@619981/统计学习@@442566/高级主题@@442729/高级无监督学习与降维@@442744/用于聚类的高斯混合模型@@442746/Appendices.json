{"hands_on_practices": [{"introduction": "在混合高斯模型中，分类的本质是确定决策边界。通过推导一个简单一维案例的决策边界，我们可以直观地理解模型参数（特别是混合权重 $\\pi_k$）如何影响一个簇的结束和另一个簇的开始。这个练习对于理解在做出分类决策时，先验信念（权重）和数据似然之间的相互作用至关重要 [@problem_id:3122648]。", "problem": "考虑一个用于一维（$1$-D）特征空间聚类的、具有 $2$ 个分量的高斯混合模型（GMM）。设该模型的各分量均值为 $\\mu_1$ 和 $\\mu_2$，两个分量具有相同的协方差（在一维情况下等于方差）$\\sigma^2$，以及满足 $\\pi_1 + \\pi_2 = 1$ 和 $\\pi_k \\in (0,1)$（对于每个分量索引 $k \\in \\{1,2\\}$）的混合权重（先验）$\\pi_1$ 和 $\\pi_2$。数据生成密度为 $p(x) = \\pi_1 \\,\\mathcal{N}(x \\mid \\mu_1, \\sigma^2) + \\pi_2 \\,\\mathcal{N}(x \\mid \\mu_2, \\sigma^2)$，其中 $\\mathcal{N}(x \\mid \\mu, \\sigma^2)$ 表示一维高斯密度。贝叶斯决策规则将点 $x$ 分配给后验概率（正比于 $\\pi_k \\,\\mathcal{N}(x \\mid \\mu_k, \\sigma^2)$）较大的分量。两个分量之间的决策边界是后验概率相等的位置 $x^\\star$。\n\n您的任务是：\n- 仅从高斯密度和贝叶斯决策规则的定义出发，从第一性原理推导由 $\\pi_1 \\,\\mathcal{N}(x^\\star \\mid \\mu_1, \\sigma^2) = \\pi_2 \\,\\mathcal{N}(x^\\star \\mid \\mu_2, \\sigma^2)$ 定义的决策边界 $x^\\star$ 的方程，并求解该方程，用 $\\mu_1$、$\\mu_2$、$\\sigma^2$、$\\pi_1$ 和 $\\pi_2$ 表示 $x^\\star$。\n- 定义中点 $m = (\\mu_1 + \\mu_2)/2$ 和位移 $\\Delta = x^\\star - m$。分析 $\\Delta$ 的符号如何依赖于混合权重，并将 $\\Delta$ 的符号与决策边界是向 $\\mu_1$ 还是向 $\\mu_2$ 移动联系起来。\n\n实现一个程序，对每个测试用例计算三元组 $[x^\\star, \\Delta, s]$，其中 $x^\\star$ 是决策边界的位置，$\\Delta$ 是与中点的位移，而 $s$ 是一个整数符号指示符，定义如下：\n- 如果 $\\Delta > 0$（边界向 $\\mu_2$ 移动），则 $s = 1$。\n- 如果 $\\Delta  0$（边界向 $\\mu_1$ 移动），则 $s = -1$。\n- 如果 $\\Delta = 0$（无移动），则 $s = 0$。\n\n使用以下测试套件，该套件保持协方差相同并改变混合权重以探究决策边界的移动：\n- 测试用例 1：$\\mu_1 = -1$, $\\mu_2 = 1$, $\\sigma^2 = 0.25$, $\\pi_1 = 0.5$, $\\pi_2 = 0.5$。\n- 测试用例 2：$\\mu_1 = -1$, $\\mu_2 = 1$, $\\sigma^2 = 0.25$, $\\pi_1 = 0.7$, $\\pi_2 = 0.3$。\n- 测试用例 3：$\\mu_1 = -1$, $\\mu_2 = 1$, $\\sigma^2 = 0.25$, $\\pi_1 = 0.01$, $\\pi_2 = 0.99$。\n\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果，每个测试用例一个三元组，顺序为：$[[x^\\star_1,\\Delta_1,s_1],[x^\\star_2,\\Delta_2,s_2],[x^\\star_3,\\Delta_3,s_3]]$。不应打印任何额外文本。", "solution": "用户提供了一个有效的、适定的、基于统计学习理论的问题。任务是推导一个双分量一维高斯混合模型（GMM）的决策边界并分析其性质。\n\n### 第1部分：决策边界 $x^\\star$ 的推导\n\n问题将决策边界 $x^\\star$ 定义为两个分量的后验概率相等的点。在 GMM 的背景下，分量 $k$ 的后验概率正比于先验概率（混合权重）$\\pi_k$ 与类条件密度 $\\mathcal{N}(x \\mid \\mu_k, \\sigma^2)$ 的乘积。因此，决策边界 $x^\\star$ 满足以下方程：\n$$\n\\pi_1 \\,\\mathcal{N}(x^\\star \\mid \\mu_1, \\sigma^2) = \\pi_2 \\,\\mathcal{N}(x^\\star \\mid \\mu_2, \\sigma^2)\n$$\n一维高斯概率密度函数（PDF）由下式给出：\n$$\n\\mathcal{N}(x \\mid \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right)\n$$\n将此定义代入边界方程中：\n$$\n\\pi_1 \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x^\\star-\\mu_1)^2}{2\\sigma^2}\\right) = \\pi_2 \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x^\\star-\\mu_2)^2}{2\\sigma^2}\\right)\n$$\n归一化常数 $\\frac{1}{\\sqrt{2\\pi\\sigma^2}}$ 为正且在等式两侧共有，因此可以约掉：\n$$\n\\pi_1 \\exp\\left(-\\frac{(x^\\star-\\mu_1)^2}{2\\sigma^2}\\right) = \\pi_2 \\exp\\left(-\\frac{(x^\\star-\\mu_2)^2}{2\\sigma^2}\\right)\n$$\n为求解 $x^\\star$，我们对等式两边取自然对数。这是一个保持等式成立的单调变换。\n$$\n\\ln\\left( \\pi_1 \\exp\\left(-\\frac{(x^\\star-\\mu_1)^2}{2\\sigma^2}\\right) \\right) = \\ln\\left( \\pi_2 \\exp\\left(-\\frac{(x^\\star-\\mu_2)^2}{2\\sigma^2}\\right) \\right)\n$$\n利用对数的性质 $\\ln(ab) = \\ln(a) + \\ln(b)$ 和 $\\ln(e^z) = z$，我们得到：\n$$\n\\ln(\\pi_1) - \\frac{(x^\\star-\\mu_1)^2}{2\\sigma^2} = \\ln(\\pi_2) - \\frac{(x^\\star-\\mu_2)^2}{2\\sigma^2}\n$$\n重新整理各项，将对数部分和二次项部分分组：\n$$\n\\ln(\\pi_1) - \\ln(\\pi_2) = \\frac{(x^\\star-\\mu_1)^2}{2\\sigma^2} - \\frac{(x^\\star-\\mu_2)^2}{2\\sigma^2}\n$$\n使用 $\\ln(a) - \\ln(b) = \\ln(a/b)$ 并将两边乘以 $2\\sigma^2$：\n$$\n2\\sigma^2 \\ln\\left(\\frac{\\pi_1}{\\pi_2}\\right) = (x^\\star-\\mu_1)^2 - (x^\\star-\\mu_2)^2\n$$\n我们展开右侧的二次项：\n$$\n(x^\\star)^2 - 2x^\\star\\mu_1 + \\mu_1^2 - \\left((x^\\star)^2 - 2x^\\star\\mu_2 + \\mu_2^2\\right)\n$$\n$$\n= (x^\\star)^2 - 2x^\\star\\mu_1 + \\mu_1^2 - (x^\\star)^2 + 2x^\\star\\mu_2 - \\mu_2^2\n$$\n$(x^\\star)^2$ 项相互抵消，留下一个关于 $x^\\star$ 的线性方程：\n$$\n= 2x^\\star(\\mu_2 - \\mu_1) + (\\mu_1^2 - \\mu_2^2)\n$$\n使用平方差公式 $\\mu_1^2 - \\mu_2^2 = (\\mu_1 - \\mu_2)(\\mu_1 + \\mu_2) = -(\\mu_2 - \\mu_1)(\\mu_1 + \\mu_2)$，上式简化为：\n$$\n= 2x^\\star(\\mu_2 - \\mu_1) - (\\mu_2 - \\mu_1)(\\mu_1 + \\mu_2)\n$$\n将其代回我们的方程中：\n$$\n2\\sigma^2 \\ln\\left(\\frac{\\pi_1}{\\pi_2}\\right) = 2x^\\star(\\mu_2 - \\mu_1) - (\\mu_2 - \\mu_1)(\\mu_1 + \\mu_2)\n$$\n假设均值不相同（$\\mu_1 \\neq \\mu_2$），我们可以将两边除以 $2(\\mu_2 - \\mu_1)$：\n$$\n\\frac{\\sigma^2}{\\mu_2 - \\mu_1} \\ln\\left(\\frac{\\pi_1}{\\pi_2}\\right) = x^\\star - \\frac{\\mu_1 + \\mu_2}{2}\n$$\n求解 $x^\\star$ 得到决策边界的最终表达式：\n$$\nx^\\star = \\frac{\\mu_1 + \\mu_2}{2} + \\frac{\\sigma^2}{\\mu_2 - \\mu_1} \\ln\\left(\\frac{\\pi_1}{\\pi_2}\\right)\n$$\n\n### 第2部分：位移 $\\Delta$ 的分析\n\n问题定义了中点 $m = (\\mu_1 + \\mu_2)/2$ 和位移 $\\Delta = x^\\star - m$。使用我们推导出的 $x^\\star$ 表达式：\n$$\n\\Delta = \\left( \\frac{\\mu_1 + \\mu_2}{2} + \\frac{\\sigma^2}{\\mu_2 - \\mu_1} \\ln\\left(\\frac{\\pi_1}{\\pi_2}\\right) \\right) - \\frac{\\mu_1 + \\mu_2}{2}\n$$\n$$\n\\Delta = \\frac{\\sigma^2}{\\mu_2 - \\mu_1} \\ln\\left(\\frac{\\pi_1}{\\pi_2}\\right)\n$$\n$\\Delta$ 的符号决定了决策边界相对于中点 $m$ 的移动。由于方差 $\\sigma^2$ 始终为正，$\\Delta$ 的符号取决于 $(\\mu_2 - \\mu_1)$ 和 $\\ln(\\pi_1/\\pi_2)$ 的符号。\n对于给定的测试用例，$\\mu_1 = -1$ 且 $\\mu_2 = 1$，因此 $\\mu_2 - \\mu_1 = 2  0$。在这种情况下，$\\Delta$ 的符号完全由 $\\ln(\\pi_1/\\pi_2)$ 的符号决定。\n\n- 如果 $\\pi_1 = \\pi_2$，则 $\\pi_1/\\pi_2 = 1$ 且 $\\ln(1) = 0$。这导致 $\\Delta = 0$。决策边界 $x^\\star$ 恰好位于中点 $m$。这对应于符号指示符 $s=0$。\n- 如果 $\\pi_1  \\pi_2$，则 $\\pi_1/\\pi_2  1$ 且 $\\ln(\\pi_1/\\pi_2)  0$。这导致 $\\Delta  0$。边界 $x^\\star = m + \\Delta$ 从中点向正方向移动，即朝向 $\\mu_2$。这对应于 $s=1$。分量1的较高先验概率将其决策边界推离其均值 $\\mu_1$。\n- 如果 $\\pi_1  \\pi_2$，则 $\\pi_1/\\pi_2  1$ 且 $\\ln(\\pi_1/\\pi_2)  0$。这导致 $\\Delta  0$。边界 $x^\\star = m + \\Delta$ 从中点向负方向移动，即朝向 $\\mu_1$。这对应于 $s=-1$。分量2的较高先验概率将其决策边界推离其均值 $\\mu_2$。\n\n这些结果与问题陈述中提供的符号指示符 $s$ 的定义一致。现在可以实现推导出的公式来解决测试用例。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (mu1, mu2, sigma_sq, pi1, pi2)\n        (-1.0, 1.0, 0.25, 0.5, 0.5),\n        (-1.0, 1.0, 0.25, 0.7, 0.3),\n        (-1.0, 1.0, 0.25, 0.01, 0.99),\n    ]\n\n    results = []\n    for case in test_cases:\n        mu1, mu2, sigma_sq, pi1, pi2 = case\n\n        # Calculate the midpoint m\n        m = (mu1 + mu2) / 2.0\n        \n        # The decision boundary x_star is derived from the condition:\n        # pi1 * N(x|mu1, sigma_sq) = pi2 * N(x|mu2, sigma_sq)\n        # This simplifies to the following equation, as derived in the solution:\n        # x_star = m + (sigma_sq / (mu2 - mu1)) * log(pi1 / pi2)\n        \n        # We can assume mu1 != mu2, as per the test cases.\n        # The priors pi_k are in (0,1), so division by pi2 is safe.\n        log_ratio = np.log(pi1 / pi2)\n        \n        x_star = m + (sigma_sq / (mu2 - mu1)) * log_ratio\n        \n        # The displacement delta is defined as x_star - m.\n        delta = x_star - m\n        \n        # The sign indicator s is the sign of delta.\n        # np.sign(x) returns 1.0 for x>0, -1.0 for x0, and 0.0 for x=0.\n        # We cast the result to an integer.\n        s = int(np.sign(delta))\n        \n        # Store the computed triple [x_star, delta, s].\n        results.append([x_star, delta, s])\n\n    # The required output format is a single line with no spaces:\n    # [[x_1,d_1,s_1],[x_2,d_2,s_2],[x_3,d_3,s_3]]\n    # Converting the list of lists to a string and removing spaces achieves this.\n    final_output = str(results).replace(\" \", \"\")\n    \n    # Final print statement in the exact required format.\n    print(final_output)\n\nsolve()\n```", "id": "3122648"}, {"introduction": "与将每个数据点分配到单个簇的硬聚类算法不同，混合高斯模型提供了一种软性的、概率性的分配。本练习将引入香农熵 (Shannon entropy) 作为一个强大工具，来量化这些分配的不确定性。通过计算熵，我们可以识别出模型不确定性高的簇重叠区域，从而对聚类结构有更细致的理解 [@problem_id:3122627]。", "problem": "您的任务是实现一个程序，该程序使用每个数据点的成分责任（component responsibilities）的香农熵来量化高斯混合模型聚类中的分配不确定性。请从基本原理出发：使用贝叶斯定理从高斯混合模型中获取后验分配概率（即责任），并使用离散分布的香农熵定义。实现必须是数值稳定的且科学上可靠的。\n\n假设一个在 $d$ 维空间中具有 $K$ 个成分的高斯混合模型。设该混合模型具有非负的混合比例 $\\pi_k$（满足 $\\sum_{k=1}^K \\pi_k = 1$）、成分均值 $\\mu_k \\in \\mathbb{R}^d$ 以及正定协方差矩阵 $\\Sigma_k \\in \\mathbb{R}^{d \\times d}$，其中 $k \\in \\{1,\\dots,K\\}$。成分 $k$ 的多元正态概率密度函数 (PDF) 为\n$$\n\\mathcal{N}(x \\mid \\mu_k, \\Sigma_k) = \\frac{1}{(2\\pi)^{d/2} |\\Sigma_k|^{1/2}} \\exp\\left(-\\frac{1}{2}(x-\\mu_k)^\\top \\Sigma_k^{-1} (x-\\mu_k)\\right).\n$$\n使用贝叶斯定理，将每个数据点 $x_i$ 的后验分配概率（责任）$r_{ik}$ 构建为给定 $x_i$ 时成分 $k$ 的后验概率。该概率由成分先验 $\\pi_k$ 和成分似然 $\\mathcal{N}(x_i \\mid \\mu_k, \\Sigma_k)$ 推导得出。对于每个点 $x_i$，使用自然对数计算其在 $K$ 个成分上的分配分布的香农熵，\n$$\nH(x_i) = -\\sum_{k=1}^K r_{ik} \\log r_{ik}.\n$$\n解释：当多个成分在 $x_i$ 处具有可比较的后验概率时，会出现较大的 $H(x_i)$，表示存在重叠；当单个成分占主导地位时，会出现较小的 $H(x_i)$。\n\n对于下方的每个测试用例，计算其熵 $H(x_i)$ 超过给定阈值 $\\tau$ 的点的比例。将此比例表示为小数（而非百分比）。您的程序必须按照所述，通过贝叶斯定理和多元正态PDF实现责任的计算，使用自然对数计算熵，并汇总所有测试用例的最终结果。\n\n测试套件规范：\n- 用例 1（一维，两个良好分离的成分）：\n  - $d = 1$, $K = 2$, $\\pi = [0.5, 0.5]$。\n  - $\\mu_1 = -3$, $\\mu_2 = 3$。\n  - $\\Sigma_1 = [0.5]$, $\\Sigma_2 = [0.5]$。\n  - 点 $x_i \\in \\{-4, -3, -2, 2, 3, 4\\}$。\n  - 阈值 $\\tau = 0.3$。\n- 用例 2（一维，两个中度重叠的成分）：\n  - $d = 1$, $K = 2$, $\\pi = [0.5, 0.5]$。\n  - $\\mu_1 = 0$, $\\mu_2 = 1$。\n  - $\\Sigma_1 = [1]$, $\\Sigma_2 = [1]$。\n  - 点 $x_i \\in \\{-0.2, 0.5, 0.8, 1.5\\}$。\n  - 阈值 $\\tau = 0.5$。\n- 用例 3（二维，沿一个轴重叠）：\n  - $d = 2$, $K = 2$, $\\pi = [0.5, 0.5]$。\n  - $\\mu_1 = [0, 0]$, $\\mu_2 = [3, 0]$。\n  - $\\Sigma_1 = \\mathrm{diag}([1, 4])$, $\\Sigma_2 = \\mathrm{diag}([1, 4])$。\n  - 点 $x_i \\in \\{[0, 0], [1.5, 0], [3, 0], [1.5, 2], [1.5, -2]\\}$。\n  - 阈值 $\\tau = 0.4$。\n- 用例 4（二维，作为相同成分边界条件的均匀责任）：\n  - $d = 2$, $K = 3$, $\\pi = [\\frac{1}{3}, \\frac{1}{3}, \\frac{1}{3}]$。\n  - $\\mu_1 = [0, 0]$, $\\mu_2 = [0, 0]$, $\\mu_3 = [0, 0]$。\n  - $\\Sigma_1 = I$, $\\Sigma_2 = I$, $\\Sigma_3 = I$, 其中 $I$ 是 $2 \\times 2$ 单位矩阵。\n  - 点 $x_i \\in \\{[2, 2], [0, 0], [-1, 1], [3, -3]\\}$。\n  - 阈值 $\\tau = 1.0$。\n\n输出要求：\n- 对于每个用例，计算满足 $H(x_i) > \\tau$ 的点的比例。\n- 您的程序应生成单行输出，其中包含这四个比例，以逗号分隔的列表形式并用方括号括起（例如，$[r_1, r_2, r_3, r_4]$），其中每个 $r_j$ 都是一个十进制数。\n\n假设与约束：\n- 所有 $\\log$ 计算均使用自然对数。\n- 在熵计算中通过避免 $\\log 0$ 来确保数值稳定性；这可以通过将责任值裁剪至一个小的正常数来处理，使其不为 $0$。\n- 不涉及物理单位。", "solution": "该问题要求实现一个函数，用于计算高斯混合模型 (GMM) 中数据点的分配不确定性，该不确定性通过成分责任的香农熵来量化。解决方案必须从第一性原理推导，包括贝叶斯定理和多元正态分布的定义。\n\n首先，我们建立理论基础。GMM 是一种概率模型，它假设所有数据点都由有限数量的、参数未知的高斯分布的混合生成。对于一个在 $d$ 维空间中具有 $K$ 个成分的 GMM，该模型由混合比例 $\\pi_k$、成分均值 $\\mu_k \\in \\mathbb{R}^d$ 和成分协方差矩阵 $\\Sigma_k \\in \\mathbb{R}^{d \\times d}$ 定义，其中 $k \\in \\{1, \\dots, K\\}$。混合比例为非负数且总和为一：$\\sum_{k=1}^K \\pi_k = 1$。每个协方差矩阵 $\\Sigma_k$ 都要求是正定的。\n\n在点 $x \\in \\mathbb{R}^d$ 处，单个多元正态成分 $k$ 的概率密度函数 (PDF) 由下式给出：\n$$\n\\mathcal{N}(x \\mid \\mu_k, \\Sigma_k) = \\frac{1}{(2\\pi)^{d/2} |\\Sigma_k|^{1/2}} \\exp\\left(-\\frac{1}{2}(x-\\mu_k)^\\top \\Sigma_k^{-1} (x-\\mu_k)\\right)\n$$\n此处，$|\\Sigma_k|$ 是协方差矩阵 $\\Sigma_k$ 的行列式，而项 $(x-\\mu_k)^\\top \\Sigma_k^{-1} (x-\\mu_k)$ 是点 $x$ 和均值 $\\mu_k$ 之间的马氏距离的平方。\n\n问题的核心是为每个数据点 $x_i$ 确定它由成分 $k$ 生成的后验概率。这个后验概率被称为责任（responsibility），记作 $r_{ik}$。使用贝叶斯定理，责任的计算如下：\n$$\nr_{ik} = P(k \\mid x_i) = \\frac{P(x_i \\mid k) P(k)}{P(x_i)} = \\frac{\\pi_k \\mathcal{N}(x_i \\mid \\mu_k, \\Sigma_k)}{\\sum_{j=1}^K \\pi_j \\mathcal{N}(x_i \\mid \\mu_j, \\Sigma_j)}\n$$\n在此表达式中，$\\pi_k$ 作为成分 $k$ 的先验概率 $P(k)$，而 $\\mathcal{N}(x_i \\mid \\mu_k, \\Sigma_k)$ 是似然 $P(x_i \\mid k)$。分母 $\\sum_{j=1}^K \\pi_j \\mathcal{N}(x_i \\mid \\mu_j, \\Sigma_j)$ 是数据点 $x_i$ 的边际概率，通常称为证据 (evidence)，它作为一个归一化常数，确保 $\\sum_{k=1}^K r_{ik} = 1$。\n\n直接计算似然和证据在数值上可能不稳定，尤其是在高维情况下，或当指数项导致下溢 (underflow) 时。一种标准且稳健的技术是使用对数概率进行计算。我们将点 $x_i$ 和成分 $k$ 的加权似然的对数定义为 $L_{ik}$：\n$$\nL_{ik} = \\log\\left(\\pi_k \\mathcal{N}(x_i \\mid \\mu_k, \\Sigma_k)\\right) = \\log(\\pi_k) + \\log\\left(\\mathcal{N}(x_i \\mid \\mu_k, \\Sigma_k)\\right)\n$$\n对数概率密度函数 (log-PDF) 为：\n$$\n\\log\\left(\\mathcal{N}(x_i \\mid \\mu_k, \\Sigma_k)\\right) = -\\frac{d}{2}\\log(2\\pi) - \\frac{1}{2}\\log|\\Sigma_k| - \\frac{1}{2}(x_i-\\mu_k)^\\top \\Sigma_k^{-1} (x_i-\\mu_k)\n$$\n责任可以用 $L_{ik}$ 表示为 $r_{ik} = \\frac{\\exp(L_{ik})}{\\sum_{j=1}^K \\exp(L_{ij})}$。为了防止指数函数中的数值上溢/下溢，我们使用 log-sum-exp 技巧。令 $L_{\\text{max}, i} = \\max_{j} L_{ij}$。那么，\n$$\nr_{ik} = \\frac{\\exp(L_{ik} - L_{\\text{max}, i})}{\\sum_{j=1}^K \\exp(L_{ij} - L_{\\text{max}, i})}\n$$\n此计算是数值稳定的，因为现在最大的指数是 $0$。\n\n一旦为数据点 $x_i$ 计算出责任 $\\{r_{ik}\\}_{k=1}^K$，它们就构成了关于这些成分的一个离散概率分布。此分配的不确定性通过香non熵来量化，使用自然对数：\n$$\nH(x_i) = -\\sum_{k=1}^K r_{ik} \\log r_{ik}\n$$\n当责任是均匀的（例如，对所有 $k$ 都有 $r_{ik} = 1/K$）时，熵达到最大值，表示高度不确定性。当一个成分的责任为 $1$ 而所有其他成分的责任都为 $0$ 时，熵达到最小值（为 $0$），表示确定的分配。在计算中，如果 $r_{ik}=0$，则项 $r_{ik} \\log r_{ik}$ 取为 $0$，这对应于极限 $\\lim_{p \\to 0^+} p \\log p = 0$。\n\n最后一步是处理每个给定的测试用例。对于每个用例，我们遍历所有提供的数据点 $\\{x_i\\}$。对每个点，我们计算其熵 $H(x_i)$。然后我们统计有多少个点的熵大于指定的阈值 $\\tau$。该用例的结果是这些点占总点数的比例。\n\n实现将遵循这一逻辑。将创建一个函数，根据给定的 GMM 参数为一组点计算熵。该函数将为每个点和成分计算对数加权似然，应用 log-sum-exp 技巧来求得责任，然后为每个点的责任分布计算香non熵。主脚本将对每个测试用例使用此函数，并计算所需的比例。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes assignment uncertainty in GMM clustering using Shannon entropy.\n    \"\"\"\n\n    def compute_entropies(X, pis, mus, sigmas):\n        \"\"\"\n        Computes the Shannon entropy of responsibilities for each data point.\n\n        Args:\n            X (np.ndarray): Data points, shape (N, d).\n            pis (np.ndarray): Mixing proportions, shape (K,).\n            mus (np.ndarray): Component means, shape (K, d).\n            sigmas (np.ndarray): Component covariance matrices, shape (K, d, d).\n\n        Returns:\n            np.ndarray: Shannon entropy for each data point, shape (N,).\n        \"\"\"\n        N, d = X.shape\n        K = len(pis)\n\n        # log_weighted_likelihoods will store log(pi_k * N(x_i | mu_k, sigma_k))\n        log_weighted_likelihoods = np.zeros((N, K))\n\n        for k in range(K):\n            pi_k = pis[k]\n            mu_k = mus[k]\n            sigma_k = sigmas[k]\n            \n            # Pre-compute inverse and log-determinant of covariance matrix\n            inv_sigma_k = np.linalg.inv(sigma_k)\n            det_sigma_k = np.linalg.det(sigma_k)\n            log_det_sigma_k = np.log(det_sigma_k)\n\n            # Log of the normalization constant of the multivariate normal PDF\n            log_norm_const = -0.5 * (d * np.log(2. * np.pi) + log_det_sigma_k)\n\n            # Compute Mahalanobis distance for all points to the k-th center\n            X_minus_mu = X - mu_k\n            # (X_minus_mu @ inv_sigma_k) gives shape (N, d)\n            # The element-wise product followed by sum is equivalent to (x-mu)T.inv(S).(x-mu) for each row\n            mahalanobis_sq = np.sum((X_minus_mu @ inv_sigma_k) * X_minus_mu, axis=1)\n\n            # Log-likelihood for each point for component k\n            log_pdf = log_norm_const - 0.5 * mahalanobis_sq\n            \n            # Log of the joint probability P(x, k) = P(x|k)P(k)\n            log_weighted_likelihoods[:, k] = np.log(pi_k) + log_pdf\n\n        # Log-sum-exp trick for numerical stability to compute responsibilities\n        log_sum_exp_offset = np.max(log_weighted_likelihoods, axis=1, keepdims=True)\n        exp_terms = np.exp(log_weighted_likelihoods - log_sum_exp_offset)\n        sum_exp_terms = np.sum(exp_terms, axis=1, keepdims=True)\n        \n        responsibilities = exp_terms / sum_exp_terms\n        \n        # Compute Shannon entropy, handling the case where r_ik = 0.\n        # The term r*log(r) - 0 as r - 0.\n        # np.where is used to avoid log(0) which results in -inf, and 0*-inf = nan.\n        entropy_terms = np.where(responsibilities  0, responsibilities * np.log(responsibilities), 0)\n        entropies = -np.sum(entropy_terms, axis=1)\n        \n        return entropies\n\n    test_cases = [\n        # Case 1 (one-dimensional, well-separated)\n        {\n            \"points\": np.array([[-4.], [-3.], [-2.], [2.], [3.], [4.]]),\n            \"pis\": np.array([0.5, 0.5]),\n            \"mus\": np.array([[-3.], [3.]]),\n            \"sigmas\": np.array([[[0.5]], [[0.5]]]),\n            \"tau\": 0.3\n        },\n        # Case 2 (one-dimensional, overlapping)\n        {\n            \"points\": np.array([[-0.2], [0.5], [0.8], [1.5]]),\n            \"pis\": np.array([0.5, 0.5]),\n            \"mus\": np.array([[0.], [1.]]),\n            \"sigmas\": np.array([[[1.]], [[1.]]]),\n            \"tau\": 0.5\n        },\n        # Case 3 (two-dimensional, overlap along one axis)\n        {\n            \"points\": np.array([[0., 0.], [1.5, 0.], [3., 0.], [1.5, 2.], [1.5, -2.]]),\n            \"pis\": np.array([0.5, 0.5]),\n            \"mus\": np.array([[0., 0.], [3., 0.]]),\n            \"sigmas\": np.array([[[1., 0.], [0., 4.]], [[1., 0.], [0., 4.]]]),\n            \"tau\": 0.4\n        },\n        # Case 4 (two-dimensional, identical components)\n        {\n            \"points\": np.array([[2., 2.], [0., 0.], [-1., 1.], [3., -3.]]),\n            \"pis\": np.array([1./3., 1./3., 1./3.]),\n            \"mus\": np.array([[0., 0.], [0., 0.], [0., 0.]]),\n            \"sigmas\": np.array([[[1., 0.], [0., 1.]], [[1., 0.], [0., 1.]], [[1., 0.], [0., 1.]]]),\n            \"tau\": 1.0\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        X = case[\"points\"]\n        pis = case[\"pis\"]\n        mus = case[\"mus\"]\n        sigmas = case[\"sigmas\"]\n        tau = case[\"tau\"]\n\n        entropies = compute_entropies(X, pis, mus, sigmas)\n        \n        num_points_above_threshold = np.sum(entropies  tau)\n        fraction = num_points_above_threshold / len(X)\n        results.append(fraction)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3122627"}, {"introduction": "在实际应用中，真实的簇数通常是未知的。这个高级练习通过使用贝叶斯信息准则 (Bayesian Information Criterion, BIC) 在单簇和双簇模型之间做出选择，解决了模型选择这一关键问题。通过模拟，我们将研究数据量和簇之间的分离程度如何影响我们做出正确选择的能力，这是任何实际聚类任务的关键考量 [@problem_id:3122550]。", "problem": "要求您设计并实现一个完整的、可运行的程序，以经验性地量化在使用贝叶斯信息准则 (BIC) 可靠分离高斯混合模型中两个紧密间隔的组分时所需的样本量。背景是统计学习，重点是用于聚类的高斯混合模型。您的推导和算法设计所使用的基本原理是高斯模型的最大似然估计和作为对数边缘似然的大样本近似的贝叶斯信息准则。\n\n考虑一个在 $d$ 维空间中的双组分多元高斯分布混合模型，其参数为：组分均值 $\\mu_1,\\mu_2 \\in \\mathbb{R}^d$，组分协方差矩阵 $\\Sigma_1,\\Sigma_2 \\in \\mathbb{R}^{d \\times d}$（均为正定矩阵），以及混合权重 $\\pi_1,\\pi_2 \\in (0,1)$ 且 $\\pi_1 + \\pi_2 = 1$。数据由从该混合模型中进行的 $n$ 次独立抽样组成。这些组分几乎相同，即欧几里得距离 $\\|\\mu_1 - \\mu_2\\|$ 远小于协方差的主导尺度，该尺度由 $\\sqrt{\\lambda_{\\max}(\\Sigma)}$ 量化，其中 $\\lambda_{\\max}(\\Sigma)$ 表示协方差矩阵的最大特征值。\n\n您的程序必须为每个测试用例执行以下任务：\n\n1. 在 $\\mathbb{R}^d$ 空间中，从一个具有相等混合权重 $\\pi_1 = \\pi_2 = \\frac{1}{2}$、相同协方差矩阵 $\\Sigma_1 = \\Sigma_2 = \\Sigma$、以及沿第一坐标轴分离的均值 $\\mu_1 = \\left(-\\frac{\\delta}{2}, 0, \\dots, 0\\right)$ 和 $\\mu_2 = \\left(\\frac{\\delta}{2}, 0, \\dots, 0\\right)$ 的双组分高斯混合模型中模拟 $n$ 个独立样本，其中 $\\delta = \\|\\mu_1 - \\mu_2\\|$ 是分离幅度。\n\n2. 通过最大似然法对每个模拟数据集拟合两个竞争模型：\n   - 一个单一多元高斯模型，其均值 $\\mu$ 和协方差 $\\Sigma$ 从完整数据集中估计得出。\n   - 一个双组分高斯混合模型，其参数 $(\\pi_1,\\pi_2,\\mu_1,\\mu_2,\\Sigma_1,\\Sigma_2)$ 使用期望最大化 (EM) 算法进行估计。\n\n3. 为每个拟合模型计算贝叶斯信息准则 (BIC)。对于一个具有最大化对数似然 $\\ell(\\hat{\\theta})$ 和 $p$ 个自由参数，并拟合于 $n$ 个观测值的模型，BIC 定义为 $ \\mathrm{BIC} = -2 \\, \\ell(\\hat{\\theta}) + p \\, \\log(n) $。对于 $d$ 维的单一高斯模型，参数数量为 $p_{\\text{single}} = d + \\frac{d(d+1)}{2}$。对于 $d$ 维的双组分全协方差高斯混合模型，参数数量为 $p_{\\text{mix}} = 2d + 2 \\cdot \\frac{d(d+1)}{2} + (2-1) = 2d + d(d+1) + 1$。\n\n4. 如果双组分模型的 BIC 严格小于单高斯模型的 BIC，即 $\\mathrm{BIC}_{\\text{mix}}  \\mathrm{BIC}_{\\text{single}}$，则定义为双组分模型的一次“获胜”。对于给定的 $n$，重复模拟和拟合过程 $T$ 次，以估计获胜的比例。如果获胜比例至少达到可靠性阈值 $\\tau$（以小数表示），则声明对于给定的测试用例，$n$ 实现了可靠分离。\n\n5. 对于每个测试用例，在固定的候选样本量集合中搜索，以找到实现可靠分离的最小样本量 $n$。如果没有候选样本量能实现可靠分离，则该测试用例返回 $-1$。\n\n使用以下测试套件来探测不同程度的组分邻近度，并覆盖从非常困难到较容易的分离情景。所有量必须完全按照规定处理。\n\n- 所有用例的通用设置：维度 $d = 2$，协方差矩阵 $\\Sigma = s^2 I_2$，其中 $s = 1.0$，因此 $\\sqrt{\\lambda_{\\max}(\\Sigma)} = s = 1.0$，相等的混合权重 $\\pi_1 = \\pi_2 = \\frac{1}{2}$，试验次数 $T = 12$，可靠性阈值 $\\tau = 0.75$，候选样本量 $n \\in \\{60, 120, 240, 360\\}$。\n- 用例 A (极小分离)：$\\delta = 0.10$。\n- 用例 B (小分离)：$\\delta = 0.20$。\n- 用例 C (相对于 $s$ 的中等分离)：$\\delta = 0.30$。\n\n您的程序必须为双组分混合模型实现一个统计上稳健的期望最大化过程，稳健地计算两个模型的对数似然，应用上述参数数量来计算 BIC，并执行重复试验的可靠性评估。允许采取数值稳定性措施，例如在协方差估计中添加小的对角正则化项。不允许使用外部数据或用户输入。\n\n最终输出格式：您的程序应生成一行输出，其中包含三个用例实现可靠分离的最小样本量，形式为一个用方括号括起来的逗号分隔列表，例如 $[n_A,n_B,n_C]$，其中每个 $n_\\cdot$ 是来自候选集的一个整数，或者如果没有任何候选满足可靠性阈值，则为 $-1$。", "solution": "该问题要求进行一项经验性研究，以探究需要多大的样本量 $n$ 才能使用贝叶斯信息准则 (BIC) 可靠地区分双组分高斯混合模型 (GMM) 与更简单的单组分高斯模型。该混合模型的组分被指定为紧密分布，这使得模型选择任务变得不平凡。\n\n该方法涉及一系列计算实验。对于由特定组分均值间分离距离 $\\delta$ 定义的每个测试用例，我们从给定的候选集合 $\\{60, 120, 240, 360\\}$ 中搜索最小样本量，该样本量能让更复杂的双组分模型在 $T=12$ 次独立试验中以至少 $\\tau = 0.75$ 的可靠性被 BIC 所偏好。\n\n**1. 数据模拟**\n\n在每次试验中，会在一个 $d=2$ 维空间中模拟一个包含 $n$ 个点的数据集。数据从一个具有以下参数的双组分 GMM 中抽取：\n- 混合权重：$\\pi_1 = \\pi_2 = 0.5$。\n- 组分均值：$\\mu_1 = \\left(-\\frac{\\delta}{2}, 0\\right)^T$ 和 $\\mu_2 = \\left(\\frac{\\delta}{2}, 0\\right)^T$。参数 $\\delta$ 控制组分间的分离程度。\n- 组分协方差：$\\Sigma_1 = \\Sigma_2 = \\Sigma = s^2 I_2$，其中 $s=1.0$，$I_2$ 是 $2 \\times 2$ 的单位矩阵。这意味着组分是各向同性且相同的球形组分。\n\n为了生成一个大小为 $n$ 的样本，大约 $n/2$ 个点从高斯分布 $\\mathcal{N}(\\mu_1, \\Sigma)$ 中抽取，另外 $n/2$ 个点从 $\\mathcal{N}(\\mu_2, \\Sigma)$ 中抽取。在任何给定的试验中，来自每个组分的精确点数由 $n$ 次概率为 $0.5$ 的独立伯努利抽样决定。\n\n**2. 模型拟合与评估**\n\n使用最大似然估计 (MLE) 对每个模拟数据集拟合两个模型。\n\n**模型 A：单变量高斯模型**\n将单个高斯分布 $\\mathcal{N}(\\mu, \\Sigma_{\\text{single}})$ 拟合到包含 $n$ 个点的整个数据集。MLE 参数是样本均值 $\\hat{\\mu}$ 和样本协方差矩阵 $\\hat{\\Sigma}_{\\text{single}}$：\n$$\n\\hat{\\mu} = \\frac{1}{n} \\sum_{i=1}^n x_i\n$$\n$$\n\\hat{\\Sigma}_{\\text{single}} = \\frac{1}{n} \\sum_{i=1}^n (x_i - \\hat{\\mu})(x_i - \\hat{\\mu})^T\n$$\n该模型的最大化对数似然 $\\ell_{\\text{single}}$ 可以使用闭式表达式计算：\n$$\n\\ell_{\\text{single}} = -\\frac{n}{2} \\left( d \\log(2\\pi) + \\log|\\det(\\hat{\\Sigma}_{\\text{single}})| + d \\right)\n$$\n在维度 $d=2$ 下，该模型的自由参数数量为 $p_{\\text{single}} = d + \\frac{d(d+1)}{2} = 2 + \\frac{2(3)}{2} = 5$。\n\n**模型 B：双组分高斯混合模型**\n将一个双组分 GMM 拟合到数据。由于最大似然估计没有闭式解，参数 $\\theta = \\{\\pi_1, \\pi_2, \\mu_1, \\mu_2, \\Sigma_1, \\Sigma_2\\}$ 通过期望最大化 (EM) 算法进行估计。\n\n- **初始化**：EM 算法的性能对初始参数选择敏感。为了获得稳健的初始化，先运行几次简单的 k-均值聚类算法 ($k=2$)。得到的簇的均值、比例和协方差被用作 EM 算法的起始点。\n\n- **期望步骤 (E-Step)**：给定当前的参数估计 $\\theta^{(t)}$，计算数据点 $x_i$ 由组分 $k$ 生成的后验概率（或称“责任”）：\n$$\n\\gamma_{ik}^{(t+1)} = \\frac{\\pi_k^{(t)} \\mathcal{N}(x_i | \\mu_k^{(t)}, \\Sigma_k^{(t)})}{\\sum_{j=1}^2 \\pi_j^{(t)} \\mathcal{N}(x_i | \\mu_j^{(t)}, \\Sigma_j^{(t)})}\n$$\n\n- **最大化步骤 (M-Step)**：使用计算出的责任更新参数，以最大化期望对数似然：\n$$\nN_k^{(t+1)} = \\sum_{i=1}^n \\gamma_{ik}^{(t+1)}\n$$\n$$\n\\pi_k^{(t+1)} = \\frac{N_k^{(t+1)}}{n}\n$$\n$$\n\\mu_k^{(t+1)} = \\frac{1}{N_k^{(t+1)}} \\sum_{i=1}^n \\gamma_{ik}^{(t+1)} x_i\n$$\n$$\n\\Sigma_k^{(t+1)} = \\frac{1}{N_k^{(t+1)}} \\sum_{i=1}^n \\gamma_{ik}^{(t+1)} (x_i - \\mu_k^{(t+1)})(x_i - \\mu_k^{(t+1)})^T\n$$\n为了确保数值稳定性并防止奇异性，每次更新后向协方差矩阵添加一个小的正则化项 ($\\epsilon I_d$，其中 $\\epsilon = 10^{-6}$)。\n\n- **收敛**：迭代执行 E-step 和 M-step，直到总对数似然 $\\ell_{\\text{mix}} = \\sum_{i=1}^n \\log\\left( \\sum_{k=1}^2 \\pi_k \\mathcal{N}(x_i | \\mu_k, \\Sigma_k) \\right)$ 的变化量低于一个小的容差值。\n\n在维度 $d=2$ 下，这个全协方差 GMM 的自由参数数量为 $p_{\\text{mix}} = 2d + 2 \\cdot \\frac{d(d+1)}{2} + (2-1) = 4 + 6 + 1 = 11$。\n\n**3. 通过 BIC 进行模型选择**\n\n使用贝叶斯信息准则比较两个拟合的模型。对于一个有 $p$ 个参数和最大化对数似然 $\\ell$ 的模型，BIC 为：\n$$\n\\mathrm{BIC} = -2 \\ell + p \\log(n)\n$$\nBIC 值较低的模型更优。因此，如果 $\\mathrm{BIC}_{\\text{mix}}  \\mathrm{BIC}_{\\text{single}}$，则认为双组分 GMM “获胜”。这个条件可以表示为对数似然增益的一个下界：\n$$\n2(\\ell_{\\text{mix}} - \\ell_{\\text{single}}) > (p_{\\text{mix}} - p_{\\text{single}})\\log(n)\n$$\n右侧代表对 GMM 增加的复杂度的惩罚，GMM 比单高斯模型多出 $\\Delta p = p_{\\text{mix}} - p_{\\text{single}} = 6$ 个参数。\n\n**4. 经验性搜索最小样本量**\n\n对于每个测试用例（即每个 $\\delta$ 值），程序按升序遍历候选样本量 $n \\in \\{60, 120, 240, 360\\}$。对于每个 $n$，它执行 $T=12$ 次模拟与拟合试验。它计算 GMM 获胜的次数（即 $\\mathrm{BIC}_{\\text{mix}}  \\mathrm{BIC}_{\\text{single}}$）。如果获胜比例至少达到可靠性阈值 $\\tau=0.75$（即 12 次中至少 9 次获胜），则该样本量 $n$ 被认为是足以实现可靠分离的。程序返回第一个（也就是最小的）满足此标准的 $n$。如果没有候选 $n$ 满足阈值，则返回 $-1$。对三个指定的 $\\delta$ 值中的每一个都重复此整个过程。", "answer": "```python\nimport numpy as np\nfrom scipy.stats import multivariate_normal\nfrom scipy.special import logsumexp\n\ndef fit_gmm_em(data, d, k, max_iter=100, tol=1e-4):\n    \"\"\"\n    Fits a k-component Gaussian Mixture Model using the EM algorithm.\n    \"\"\"\n    n, _ = data.shape\n    reg_cov = 1e-6\n\n    # Initialization using k-means\n    try:\n        # 1. Randomly initialize centers\n        centers = data[np.random.choice(n, k, replace=False)]\n        \n        # 2. A few k-means iterations\n        for _ in range(10):\n            distances = np.linalg.norm(data[:, np.newaxis, :] - centers[np.newaxis, :, :], axis=2)\n            labels = np.argmin(distances, axis=1)\n            \n            new_centers = np.array([data[labels == j].mean(axis=0) if np.sum(labels == j) > 0 else centers[j] for j in range(k)])\n\n            if np.allclose(centers, new_centers):\n                break\n            centers = new_centers\n\n        mus = centers\n        labels = np.argmin(np.linalg.norm(data[:, np.newaxis, :] - mus, axis=2), axis=1) # final assignment\n        \n        pis = np.array([np.mean(labels == j) for j in range(k)])\n        \n        sigmas = []\n        for j in range(k):\n            cluster_points = data[labels == j]\n            if len(cluster_points)  d:\n                # Not enough points, use global covariance\n                cov_j = np.cov(data, rowvar=False, bias=True)\n            else:\n                cov_j = np.cov(cluster_points, rowvar=False, bias=True)\n            sigmas.append(cov_j + reg_cov * np.identity(d))\n        sigmas = np.array(sigmas)\n\n    except (ValueError, np.linalg.LinAlgError):\n        return -np.inf\n\n    ll_old = -np.inf\n    for _ in range(max_iter):\n        # E-step\n        try:\n            log_probs = np.zeros((n, k))\n            for j in range(k):\n                log_probs[:, j] = np.log(pis[j]) + multivariate_normal.logpdf(data, mean=mus[j], cov=sigmas[j])\n        except (ValueError, np.linalg.LinAlgError):\n            return -np.inf\n\n        log_sum_probs = logsumexp(log_probs, axis=1)\n        log_responsibilities = log_probs - log_sum_probs[:, np.newaxis]\n        responsibilities = np.exp(log_responsibilities)\n\n        # M-step\n        nk = np.sum(responsibilities, axis=0)\n        \n        # Check for collapsed components\n        if np.any(nk  1e-9):\n            return -np.inf\n\n        pis = nk / n\n        mus = np.dot(responsibilities.T, data) / nk[:, np.newaxis]\n        \n        for j in range(k):\n            diff = data - mus[j]\n            weighted_diff = diff * np.sqrt(responsibilities[:, j])[:, np.newaxis]\n            sigmas[j] = (weighted_diff.T @ weighted_diff) / nk[j] + reg_cov * np.identity(d)\n        \n        # Convergence check\n        ll_new = np.sum(log_sum_probs)\n        if ll_new - ll_old  tol and ll_new > -np.inf:\n            break\n        ll_old = ll_new\n\n    if not np.isfinite(ll_old):\n        return -np.inf\n        \n    return ll_old\n\ndef log_likelihood_single_gaussian(data, d):\n    \"\"\"\n    Computes the maximized log-likelihood for a single Gaussian model.\n    \"\"\"\n    n, _ = data.shape\n    if n  2:\n        return -np.inf\n    \n    mean = np.mean(data, axis=0)\n    cov = np.cov(data, rowvar=False, bias=True)\n    cov += 1e-6 * np.identity(d) # Regularization\n\n    try:\n        sign, logdet = np.linalg.slogdet(cov)\n        if sign = 0:\n            return -np.inf\n    except np.linalg.LinAlgError:\n        return -np.inf\n\n    ll = -0.5 * n * (d * np.log(2 * np.pi) + logdet + d)\n    return ll\n\ndef find_minimal_n(delta, d, cov_matrix, T, tau, n_candidates):\n    \"\"\"\n    Searches for the minimal sample size n that achieves reliable separation.\n    \"\"\"\n    mu1 = np.zeros(d)\n    mu1[0] = -delta / 2\n    mu2 = np.zeros(d)\n    mu2[0] = delta / 2\n    \n    p_single = d + d * (d + 1) / 2\n    p_mix = 2 * d + d * (d + 1) + 1 # for k=2 components\n\n    for n in sorted(n_candidates):\n        win_count = 0\n        for _ in range(T):\n            # 1. Simulate data\n            n1 = np.random.binomial(n, 0.5)\n            n2 = n - n1\n            data1 = np.random.multivariate_normal(mu1, cov_matrix, size=n1)\n            data2 = np.random.multivariate_normal(mu2, cov_matrix, size=n2)\n            data = np.vstack((data1, data2)) if n1 > 0 and n2 > 0 else (data1 if n1 > 0 else data2)\n            np.random.shuffle(data)\n\n            # 2. Fit single Gaussian model\n            ll_single = log_likelihood_single_gaussian(data, d)\n            if not np.isfinite(ll_single):\n                continue\n            bic_single = -2 * ll_single + p_single * np.log(n)\n\n            # 3. Fit two-component GMM\n            ll_mix = fit_gmm_em(data, d, k=2)\n            if not np.isfinite(ll_mix):\n                continue\n            bic_mix = -2 * ll_mix + p_mix * np.log(n)\n\n            # 4. Compare BIC\n            if bic_mix  bic_single:\n                win_count += 1\n        \n        # 5. Check reliability\n        win_fraction = win_count / T\n        if win_fraction >= tau:\n            return n\n            \n    return -1\n\ndef solve():\n    # Set a seed for reproducibility.\n    np.random.seed(42)\n\n    # Common settings from problem statement\n    d = 2\n    s = 1.0\n    cov_matrix = np.identity(d) * (s**2)\n    T = 12\n    tau = 0.75\n    n_candidates = [60, 120, 240, 360]\n    \n    # Test cases\n    test_cases = [\n        {'name': 'Case A', 'delta': 0.10},\n        {'name': 'Case B', 'delta': 0.20},\n        {'name': 'Case C', 'delta': 0.30},\n    ]\n\n    results = []\n    for case in test_cases:\n        delta = case['delta']\n        min_n = find_minimal_n(delta, d, cov_matrix, T, tau, n_candidates)\n        results.append(min_n)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```", "id": "3122550"}]}