{"hands_on_practices": [{"introduction": "像主成分分析（PCA）这样的线性方法在处理位于非线性流形上的数据时常常会失效。本练习将通过一个经典的“尖点”曲线，让您动手实践并对此进行验证，您将比较PCA和Isomap（一种近似流形内在几何的算法）的效果。通过实现这两种算法并使用量化指标进行评估，您将具体理解为何需要以及如何进行非线性降维 [@problem_id:3144257]。", "problem": "考虑由参数方程 $p(t) = (t^2, t^3)$ (其中 $t \\in [-1,1]$) 定义的平面曲线。这个集合是 $\\mathbb{R}^2$ 的一个一维子集，在 $t=0$ 处有一个尖点。您将分析非线性降维过程，这些过程试图将该子集映射到一个一维目标空间，同时保留局部邻域结构和连续性。目标是构建数据，定义内在（测地）度量的近似，使用两种算法生成嵌入，并使用局部利普希茨估计和邻域保持分数来量化失真。\n\n基本基础：\n- 令 $M \\subset \\mathbb{R}^2$ 为在 $N$ 个点上采样的集合 $\\{p(t) : t \\in [-1,1]\\}$。两个点 $x, x' \\in \\mathbb{R}^2$ 之间的环境欧几里得距离为 $\\|x - x'\\|_2$。\n- 在样本上使用环境欧几里得距离构建一个 $k$-最近邻（kNN）图，其无向边的权重等于邻居之间的欧几里得距离。令 $k_{\\text{graph}}$ 表示图的连通性参数（邻居数）。\n- $M$ 上样本 $i$ 和 $j$ 之间的内在测地距离 $d_M(i,j)$ 通过此 kNN 图中的最短路径距离来近似，其中边的权重等于环境欧几里得距离。\n- 经典多维标度（MDS）通过对给定对称距离矩阵 $D$ 中距离的平方进行双中心化，并提取格拉姆矩阵的前 $d$ 个特征对，将点嵌入到目标维度 $d$。具体来说，令 $H = I - \\frac{1}{N}\\mathbf{1}\\mathbf{1}^\\top$ 和 $B = -\\frac{1}{2}H(D^{\\circ 2})H$，其中 $D^{\\circ 2}$ 是逐元素平方。如果 $\\lambda_1 \\ge \\lambda_2 \\ge \\cdots$ 是 $B$ 的特征值，对应的标准正交特征向量为 $v_1, v_2, \\dots$，则使用严格为正的特征值，嵌入到维度 $d$ 的结果由 $Y = [\\sqrt{\\lambda_1} v_1, \\dots, \\sqrt{\\lambda_d} v_d]$ 给出。\n- 主成分分析（PCA）通过将中心化数据 $X \\in \\mathbb{R}^{N \\times 2}$ 投影到样本协方差矩阵的主特征向量上，实现到 $d=1$ 维度的嵌入。\n\n失真量化：\n- 定义一个映射 $f: M \\to \\mathbb{R}$，它为每个采样点生成一维嵌入坐标。对于固定的局部邻域大小 $k_{\\text{local}}$，为每个索引 $i$ 定义内在邻居集合 $N_{k_{\\text{local}}}^M(i)$，其为对于 $j \\ne i$ 具有最小 $d_M(i,j)$ 的 $k_{\\text{local}}$ 个索引。$i$ 处的局部利普希茨估计定义为\n$$\nL_i = \\max_{j \\in N_{k_{\\text{local}}}^M(i)} \\frac{|f(i) - f(j)|}{d_M(i,j)}.\n$$\n使用均值 $L_{\\text{avg}} = \\frac{1}{N}\\sum_{i=1}^N L_i$ 和定义为 $\\{L_i\\}_{i=1}^N$ 的第 $95$ 百分位的上尾描述符 $L_{95}$ 进行聚合。较大的值表示局部拉伸和潜在的撕裂，即 $M$ 中的邻近点被映射到相距很远的位置。\n- 令 $N_{k_{\\text{local}}}^Y(i)$ 为对于 $j \\ne i$ 具有最小嵌入空间距离 $|f(i) - f(j)|$ 的 $k_{\\text{local}}$ 个索引。定义邻域保持分数为\n$$\nQ = \\frac{1}{N} \\sum_{i=1}^N \\frac{|N_{k_{\\text{local}}}^M(i) \\cap N_{k_{\\text{local}}}^Y(i)|}{k_{\\text{local}}}.\n$$\n$[0,1]$ 范围内的 $Q$ 值越接近 $1$，表示局部邻域保持（连续性）越好。\n\n待测试算法：\n- 等距映射（Isomap-like）：通过 kNN 图上的最短路径计算 $d_M$，然后对 $d_M$ 应用经典多维标度（MDS）以获得到 $d=1$ 维度的嵌入。将所得坐标函数表示为 $f_{\\text{iso}}$。\n- 主成分分析（PCA）：计算中心化环境坐标 $X$ 的主成分，并将数据投影到该主成分上，以获得一维嵌入。将所得坐标函数表示为 $f_{\\text{pca}}$。\n\n在一个完整的、可运行的程序中要实现的任务：\n1. 通过在 $[-1,1]$ 中均匀取值 $t$ 并映射 $p(t) = (t^2, t^3)$，从尖点曲线上采样 $N$ 个点。\n2. 使用采样点之间的环境欧几里得距离，构建参数为 $k_{\\text{graph}}$ 的无向加权 kNN 图。\n3. 使用 kNN 图上的最短路径计算近似的内在测地距离矩阵 $d_M$。\n4. 生成一维嵌入：\n   - 对于 Isomap-like：对 $d_M$ 应用经典多维标度（MDS）以获得 $f_{\\text{iso}}$。\n   - 对于 PCA：投影到主成分上以获得 $f_{\\text{pca}}$。\n5. 对于每个嵌入，使用来自 $d_M$ 的 $k_{\\text{local}}$ 个内在邻居计算 $Q$、$L_{\\text{avg}}$ 和 $L_{95}$。\n\n测试套件：\n- 所有情况均使用 $N=200$ 和 $k_{\\text{local}}=6$。评估以下四个参数情况，它们分别探究了理想路径、边界情况和极端情况：\n  1. 方法 Isomap-like，使用 $k_{\\text{graph}}=8$（理想路径）。\n  2. 方法 Isomap-like，使用 $k_{\\text{graph}}=4$（边界：图更稀疏，但仍连通）。\n  3. 方法 Isomap-like，使用 $k_{\\text{graph}}=30$（极端情况：在尖点附近可能发生“短路”）。\n  4. 方法 PCA，使用 $k_{\\text{graph}}=8$（线性基准；$k_{\\text{graph}}$ 仅用于在度量中定义 $d_M$）。\n对于每种情况，程序必须计算并返回浮点数三元组 $[Q, L_{\\text{avg}}, L_{95}]$。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含结果，格式为四个三元组的逗号分隔列表，每个三元组本身是包含三个浮点数的列表，并用方括号括起来。打印的浮点数必须在小数点后有六位数字。例如，输出应如下所示：\n$[[q_1,\\ell_1^{\\text{avg}},\\ell_1^{95}],[q_2,\\ell_2^{\\text{avg}},\\ell_2^{95}],[q_3,\\ell_3^{\\text{avg}},\\ell_3^{95}],[q_4,\\ell_4^{\\text{avg}},\\ell_4^{95}]]$\n所有数字均以十进制浮点数形式打印（无单位）。", "solution": "该问题要求在一个合成数据集上，分析两种降维算法：主成分分析（PCA）和等距映射（Isomap）的一个变体。该数据集是对平面曲线 $p(t) = (t^2, t^3)$（其中 $t \\in [-1,1]$）的采样，该曲线形成一个带尖点的一维流形。这些算法生成的一维嵌入的质量将通过邻域保持分数和局部利普希茨估计进行量化。\n\n解决方案系统地遵循问题陈述中概述的任务进行。\n\n**1. 数据生成与 k-最近邻（kNN）图的构建**\n\n首先，我们生成数据。通过在区间 $[-1, 1]$ 内创建一个包含 $N$ 个等间距 $t$ 值的向量，然后计算相应的 $(x,y)$ 坐标，从流形 $M = \\{p(t) = (t^2, t^3) \\mid t \\in [-1,1]\\}$ 中采样 $N=200$ 个点。令该数据矩阵为 $X \\in \\mathbb{R}^{N \\times 2}$。\n\n接下来，对于给定的连通性参数 $k_{\\text{graph}}$，我们构建一个加权的无向 kNN 图。图的顶点是 $N$ 个采样点。两点之间是否建立边取决于它们在环境空间 $\\mathbb{R}^2$ 中的邻近程度。边 $(i, j)$ 的权重是欧几里得距离 $\\|x_i - x_j\\|_2$。如果点 $i$ 或点 $j$ 在对方的 $k_{\\text{graph}}$ 个最近邻之内，则连接它们，从而构建一个无向图。首先计算所有点对的欧几里得距离矩阵。然后，对每个点 $i$，确定其 $k_{\\text{graph}}$ 个最近邻。对于每个这样的邻域关系，在邻接矩阵中添加边，并确保矩阵是对称的以表示一个无向图。不存在的边被赋予无限大的权重。\n\n**2. 内在测地距离近似**\n\n两点 $x_i, x_j \\in M$ 之间的内在测地距离 $d_M(i,j)$ 是它们沿流形的最短路径长度。这通过在构建的 kNN 图上计算所有点对的最短路径距离来近似。为此，可以使用 Floyd-Warshall 算法或从每个节点运行 Dijkstra 算法。此计算产生测地距离矩阵 $d_M \\in \\mathbb{R}^{N \\times N}$，它作为 Isomap-like 嵌入的输入，并作为计算失真度量的基准真相。\n\n**3. 嵌入算法**\n\n使用两种方法将 $N$ 个点嵌入到一维空间 $\\mathbb{R}^1$ 中。得到的嵌入是一个向量 $f \\in \\mathbb{R}^N$，其中 $f(i)$ 是第 $i$ 个点的坐标。\n\n- **Isomap-like（在 $d_M$ 上使用经典 MDS）**：该方法是 Isomap 算法的核心，旨在找到一个能保持测地距离的嵌入。它将经典多维标度（MDS）应用于测地距离矩阵 $d_M$。过程如下：\n    1.  从距离矩阵 $D = d_M$ 计算逐元素平方的距离矩阵 $D^{\\circ 2}$。\n    2.  对 $D^{\\circ 2}$ 应用双中心化变换以获得格拉姆矩阵 $B = -\\frac{1}{2} H (D^{\\circ 2}) H$，其中 $H = I - \\frac{1}{N}\\mathbf{1}\\mathbf{1}^\\top$ 是中心化矩阵。\n    3.  对对称矩阵 $B$ 进行特征分解，得到特征值 $\\lambda_1 \\ge \\lambda_2 \\ge \\dots$ 和相应的标准正交特征向量 $v_1, v_2, \\dots$。\n    4.  使用最大的严格为正的特征值 $\\lambda_1$ 及其特征向量 $v_1$ 构建一维嵌入：$f_{\\text{iso}} = \\sqrt{\\lambda_1} v_1$。\n\n- **主成分分析（PCA）**：PCA 是一种线性降维技术，它将数据投影到最大方差的方向上。\n    1.  通过减去每个特征的均值来中心化原始数据 $X \\in \\mathbb{R}^{N \\times 2}$：$X_{\\text{centered}} = X - \\bar{X}$。\n    2.  计算样本协方差矩阵 $C = \\frac{1}{N-1}X_{\\text{centered}}^\\top X_{\\text{centered}}$。\n    3.  对 $2 \\times 2$ 矩阵 $C$ 进行特征分解。与最大特征值对应的特征向量 $u_1$ 是第一主成分。\n    4.  通过将中心化数据投影到该主成分上获得一维嵌入：$f_{\\text{pca}} = X_{\\text{centered}} u_1$。\n\n**4. 失真量化**\n\n对于每个嵌入 $f$，我们使用预先计算的内在距离矩阵 $d_M$ 和局部邻域大小 $k_{\\text{local}}=6$ 来量化其质量。\n\n- **邻域保持分数 ($Q$)**：该度量评估局部邻域的保持程度。对于每个点 $i$，我们确定其在内在空间中的 $k_{\\text{local}}$ 个最近邻 $N_{k_{\\text{local}}}^M(i)$ 和在嵌入空间中的 $k_{\\text{local}}$ 个最近邻 $N_{k_{\\text{local}}}^Y(i)$。该分数是所有点上这两个邻域集合之间重叠部分的平均比例：\n$$Q = \\frac{1}{N} \\sum_{i=1}^N \\frac{|N_{k_{\\text{local}}}^M(i) \\cap N_{k_{\\text{local}}}^Y(i)|}{k_{\\text{local}}}$$\n接近 $1$ 的 $Q$ 值意味着局部结构得到了很好的保持。\n\n- **局部利普希茨估计 ($L_i$, $L_{\\text{avg}}$, $L_{95}$)**：这些度量用于衡量流形的局部拉伸情况。对于每个点 $i$，局部利普希茨估计是在其内在邻域上，嵌入坐标变化与内在距离变化之比的最大值：\n$$L_i = \\max_{j \\in N_{k_{\\text{local}}}^M(i)} \\frac{|f(i) - f(j)|}{d_M(i,j)}$$\n对于一个完美的等距嵌入，所有的 $L_i=1$。较大的 $L_i$ 值表示流形上的近点在嵌入中被映射到很远的地方，这表明出现了“撕裂”。我们计算集合 $\\{L_i\\}_{i=1}^N$ 的均值（$L_{\\text{avg}}$）和第 $95$ 百分位（$L_{95}$）来总结这些估计的分布。\n\n程序为四个指定的测试用例实现了这些步骤，并为每个用例计算三元组 $[Q, L_{\\text{avg}}, L_{95}]$。\n- **PCA 与 Isomap 对比**：PCA 是线性的，预计无法“展开”尖点曲线，而是会将其两个分支相互折叠。这将导致在原点附近的邻域保持性很差（$Q \\ll 1$）。Isomap 专为此类任务设计，应该表现更好。\n- **Isomap 与 $k_{\\text{graph}}$**：$k_{\\text{graph}}$ 的选择对 Isomap 至关重要。一个小的 $k_{\\text{graph}}$（例如 $4$）有造成图不连通的风险。一个大的 $k_{\\text{graph}}$（例如 $30$）可能导致“短路”错误，即 kNN 图由于欧几里得距离小而错误地连接了尖点两侧的点，这违反了流形的内在拓扑结构。这将降低测地距离近似和最终嵌入的质量。一个中间值（例如 $8$）预计会产生最好的结果。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.sparse.csgraph import shortest_path\nfrom scipy.spatial.distance import pdist, squareform\n\ndef solve():\n    \"\"\"\n    Solves the manifold learning problem specified in the prompt.\n    \"\"\"\n\n    def run_isomap(d_M, N):\n        \"\"\"\n        Performs Isomap-like embedding using Classical MDS.\n        \"\"\"\n        # Element-wise square of the distance matrix\n        D2 = d_M**2\n        \n        # Double-centering\n        H = np.eye(N) - (1/N) * np.ones((N, N))\n        B = -0.5 * H @ D2 @ H\n        \n        # Eigendecomposition of the Gram matrix\n        # np.linalg.eigh is used for symmetric matrices and returns sorted eigenvalues\n        eigvals, eigvecs = np.linalg.eigh(B)\n        \n        # Select the top eigenpair (largest eigenvalue)\n        # eigh sorts in ascending order, so we take the last one.\n        lambda_1 = eigvals[-1]\n        v_1 = eigvecs[:, -1]\n        \n        # Handle potential small negative eigenvalues due to numerical precision\n        if lambda_1  0:\n            lambda_1 = 0\n            \n        # Return 1D embedding\n        return np.sqrt(lambda_1) * v_1\n\n    def run_pca(X, N):\n        \"\"\"\n        Performs PCA for 1D embedding.\n        \"\"\"\n        # Center the data\n        X_centered = X - np.mean(X, axis=0)\n        \n        # Compute covariance matrix\n        cov_mat = (X_centered.T @ X_centered) / (N - 1)\n        \n        # Eigendecomposition of the covariance matrix\n        eigvals, eigvecs = np.linalg.eigh(cov_mat)\n        \n        # Select the principal eigenvector (corresponding to the largest eigenvalue)\n        # eigh sorts in ascending order, so we take the last one.\n        u_1 = eigvecs[:, -1]\n        \n        # Project centered data onto the principal component\n        return X_centered @ u_1\n    \n    def compute_metrics(d_M, f_embedding, N, k_local):\n        \"\"\"\n        Computes the Q, L_avg, and L_95 metrics.\n        \"\"\"\n        # Find intrinsic and embedding space neighborhoods\n        intrinsic_neighbors = np.argsort(d_M, axis=1)[:, 1:k_local + 1]\n        \n        embedding_dists = np.abs(f_embedding[:, np.newaxis] - f_embedding)\n        embedding_neighbors = np.argsort(embedding_dists, axis=1)[:, 1:k_local + 1]\n\n        # Compute Neighborhood Preservation Score (Q)\n        q_sum = 0\n        for i in range(N):\n            intersection_size = len(np.intersect1d(intrinsic_neighbors[i], embedding_neighbors[i]))\n            q_sum += intersection_size / k_local\n        Q = q_sum / N\n\n        # Compute Local Lipschitz Estimates (L)\n        L_vals = np.zeros(N)\n        for i in range(N):\n            neighbors_i = intrinsic_neighbors[i]\n            d_M_local = d_M[i, neighbors_i]\n            d_f_local = np.abs(f_embedding[i] - f_embedding[neighbors_i])\n            \n            # np.divide handles division by zero (or inf) correctly for this case.\n            # If d_M_local is 0, it would be an error, but it won't be for j != i.\n            # If d_M_local is inf (disconnected graph), ratio becomes 0, which is fine.\n            ratios = np.divide(d_f_local, d_M_local, out=np.zeros_like(d_f_local), where=d_M_local!=0)\n            L_vals[i] = np.max(ratios)\n\n        L_avg = np.mean(L_vals)\n        L_95 = np.percentile(L_vals, 95)\n        \n        return [Q, L_avg, L_95]\n\n    # Global parameters\n    N = 200\n    k_local = 6\n    \n    # Define the four test cases\n    test_cases = [\n        {\"method\": \"isomap\", \"k_graph\": 8},\n        {\"method\": \"isomap\", \"k_graph\": 4},\n        {\"method\": \"isomap\", \"k_graph\": 30},\n        {\"method\": \"pca\", \"k_graph\": 8}\n    ]\n\n    # Generate the dataset from the cusp curve\n    t_vals = np.linspace(-1, 1, N)\n    X = np.vstack((t_vals**2, t_vals**3)).T\n\n    # Pre-compute Euclidean distances once\n    euclidean_dists = squareform(pdist(X, 'euclidean'))\n    \n    final_results = []\n    for case in test_cases:\n        method = case[\"method\"]\n        k_graph = case[\"k_graph\"]\n        \n        # 1. Construct kNN graph and compute geodesic distance matrix d_M\n        # Adjacency matrix weighted by distance, infinite for non-edges\n        adj_mat = np.full((N, N), np.inf)\n        np.fill_diagonal(adj_mat, 0)\n        \n        # Build a symmetric kNN graph (mutual kNN)\n        for i in range(N):\n            # Find k_graph nearest neighbors (excluding self)\n            neighbors = np.argsort(euclidean_dists[i, :])[1:k_graph + 1]\n            for j in neighbors:\n                adj_mat[i, j] = euclidean_dists[i, j]\n                adj_mat[j, i] = euclidean_dists[j, i] # ensure symmetry\n        \n        d_M = shortest_path(csgraph=adj_mat, directed=False)\n        \n        # Check for graph connectivity, problem statement implies it stays connected\n        if np.any(np.isinf(d_M)):\n            raise RuntimeError(f\"Graph is not connected for k_graph={k_graph}\")\n\n        # 2. Produce the 1D embedding\n        if method == \"isomap\":\n            f_embedding = run_isomap(d_M, N)\n        elif method == \"pca\":\n            f_embedding = run_pca(X, N)\n        \n        # 3. Compute and store metrics for the current case\n        metrics = compute_metrics(d_M, f_embedding, N, k_local)\n        final_results.append(metrics)\n    \n    # Format the final output string\n    result_str_parts = []\n    for res in final_results:\n        q, l_avg, l_95 = res\n        part = f\"[{q:.6f},{l_avg:.6f},{l_95:.6f}]\"\n        result_str_parts.append(part)\n        \n    final_output = f\"[{','.join(result_str_parts)}]\"\n    print(final_output)\n\nsolve()\n```", "id": "3144257"}, {"introduction": "除了数据可视化，流形的几何结构还可以作为机器学习任务的强大先验。本练习将探讨图拉普拉斯算子（作为流形微分性质的离散近似）如何在半监督回归中发挥作用。通过完成相关推导并解决一个具体示例，您将了解即使在标签数据极少的情况下，标签信息也能在数据流形上平滑地传播 [@problem_id:3144216]。", "problem": "考虑一个嵌入在 $\\mathbb{R}^{2}$ 中的紧致光滑一维流形 $\\mathcal{M}$，从中沿一条光滑曲线抽取 $n=3$ 个样本 $\\{x_{1},x_{2},x_{3}\\}$。构建一个具有对称正权重 $w_{ij}=w_{ji}$ 的无向加权图，并定义加权邻接矩阵 $W=[w_{ij}]$、度矩阵 $D=\\operatorname{diag}(d_{1},d_{2},d_{3})$（其中 $d_{i}=\\sum_{j=1}^{3}w_{ij}$）以及组合图拉普拉斯算子 $L=D-W$。假设非局部图能量 $\\sum_{i,j=1}^{3}w_{ij}(f_{i}-f_{j})^{2}$ 近似于在 $\\mathcal{M}$ 上进行 Tikhonov 平滑时使用的狄利克雷能量 $\\int_{\\mathcal{M}}\\|\\nabla f\\|^{2}\\,\\mathrm{d}\\mu$，其中 $f:\\{x_{1},x_{2},x_{3}\\}\\to\\mathbb{R}$ 是在采样点上的一个函数。\n\n给定一个具体的图，其权重为 $w_{12}=1$，$w_{23}=2$ 和 $w_{13}=0$。设 $y_{1}=1$，$y_{3}=0$ 是两个标签，$y_{2}$ 未知。考虑通过最小化以下目标函数来进行带流形先验的半监督回归：\n$$\nJ(f)=\\frac{1}{2}\\sum_{i\\in\\mathcal{L}}(f_{i}-y_{i})^{2}+\\frac{\\gamma}{2}\\sum_{i,j=1}^{3}w_{ij}(f_{i}-f_{j})^{2}+\\frac{\\lambda}{2}\\sum_{i=1}^{3}f_{i}^{2},\n$$\n其中 $\\mathcal{L}=\\{1,3\\}$ 是已标记索引集，$\\gamma0$ 控制流形平滑度，$\\lambda0$ 是一个 Tikhonov 岭参数。取 $\\gamma=1$ 和 $\\lambda=1$。令 $M=\\operatorname{diag}(m_{1},m_{2},m_{3})$ 为对角选择矩阵，如果 $i\\in\\mathcal{L}$，则 $m_{i}=1$，否则 $m_{i}=0$。\n\n任务：\n- 从 $W$、$D$ 和 $L$ 的定义出发，推导连接图二次型和成对差异能量的精确代数恒等式，从而建立 Tikhonov 平滑的离散对应形式。\n- 使用目标函数 $J(f)$ 且仅使用上述基本定义，推导确定唯一最小化子 $f^{\\star}$ 的一阶最优性条件。\n- 针对给定的图、标签和参数，计算 $f^{\\star}=(f_{1}^{\\star},f_{2}^{\\star},f_{3}^{\\star})$ 的具体值。\n\n以精确有理数形式给出每个分量的最终 $f^{\\star}$。不要四舍五入。", "solution": "该问题是适定的且科学上合理的，设置在基于图的半监督学习的标准框架内。我们将按要求依次完成这三个任务。\n\n令 $f = (f_1, f_2, f_3)^T \\in \\mathbb{R}^3$ 为图节点上的函数值向量。问题给出了邻接矩阵的权重 $w_{12}=1$，$w_{23}=2$ 和 $w_{13}=0$。由于图是无向的，权重是对称的，即 $w_{ij}=w_{ji}$。\n\n### 任务1：成对能量与图拉普拉斯二次型之间的恒等式\n\n第一个任务是建立连接成对差异能量 $\\sum_{i,j=1}^{3}w_{ij}(f_{i}-f_{j})^{2}$ 和图拉普拉斯二次型 $f^T L f$ 的代数恒等式。\n\n图拉普拉斯算子定义为 $L=D-W$，其中 $W=[w_{ij}]$ 是加权邻接矩阵，$D=\\operatorname{diag}(d_{1},d_{2},d_{3})$ 是度矩阵，其元素为 $d_i = \\sum_{j=1}^{3} w_{ij}$。\n\n我们展开成对差异能量项：\n$$\n\\sum_{i,j=1}^{3} w_{ij}(f_i - f_j)^2 = \\sum_{i,j=1}^{3} w_{ij}(f_i^2 - 2f_i f_j + f_j^2)\n$$\n我们可以将其分为三个部分：\n$$\n\\sum_{i,j=1}^{3} w_{ij}f_i^2 - 2\\sum_{i,j=1}^{3} w_{ij}f_i f_j + \\sum_{i,j=1}^{3} w_{ij}f_j^2\n$$\n我们用矩阵表示法分析每个部分。\n第一部分可以通过提出 $f_i^2$ 来重写：\n$$\n\\sum_{i=1}^{3} f_i^2 \\left(\\sum_{j=1}^{3} w_{ij}\\right) = \\sum_{i=1}^{3} d_i f_i^2 = f^T D f\n$$\n第二部分与邻接矩阵 $W$ 相关的二次型直接相关：\n$$\n-2\\sum_{i,j=1}^{3} w_{ij}f_i f_j = -2 f^T W f\n$$\n对于第三部分，我们可以交换求和索引 $i$ 和 $j$：\n$$\n\\sum_{i,j=1}^{3} w_{ij}f_j^2 = \\sum_{j,i=1}^{3} w_{ji}f_i^2\n$$\n由于权重是对称的（$w_{ij}=w_{ji}$），这变为：\n$$\n\\sum_{i,j=1}^{3} w_{ij}f_i^2 = \\sum_{i=1}^{3} f_i^2 \\left(\\sum_{j=1}^{3} w_{ij}\\right) = \\sum_{i=1}^{3} d_i f_i^2 = f^T D f\n$$\n将所有三个部分组合起来，我们得到：\n$$\n\\sum_{i,j=1}^{3} w_{ij}(f_i - f_j)^2 = f^T D f - 2 f^T W f + f^T D f = 2 f^T D f - 2 f^T W f\n$$\n在左边提出 $2f^T$，在右边提出 $f$：\n$$\n2 f^T (D - W) f = 2 f^T L f\n$$\n因此，精确的代数恒等式是：\n$$\n\\sum_{i,j=1}^{3}w_{ij}(f_{i}-f_{j})^{2} = 2 f^T L f\n$$\n这表明图拉普拉斯二次型是成对能量和的一半，这是平方 $H^1$ 半范数（狄利克雷能量）的离散模拟。\n\n### 任务2：一阶最优性条件\n\n第二个任务是推导目标函数 $J(f)$ 的唯一最小化子 $f^{\\star}$ 的一阶最优性条件。\n目标函数为：\n$$\nJ(f)=\\frac{1}{2}\\sum_{i\\in\\mathcal{L}}(f_{i}-y_{i})^{2}+\\frac{\\gamma}{2}\\sum_{i,j=1}^{3}w_{ij}(f_{i}-f_{j})^{2}+\\frac{\\lambda}{2}\\sum_{i=1}^{3}f_{i}^{2}\n$$\n我们可以将此函数重写为矩阵形式。令 $y$ 为 $\\mathbb{R}^3$ 中的一个向量，如果 $i \\in \\mathcal{L}$，则 $y_i$ 是给定的标签，否则为 $0$。给定 $y_1=1$ 和 $y_3=0$，我们定义 $y = (1, 0, 0)^T$。选择矩阵 $M=\\operatorname{diag}(m_1, m_2, m_3)$ 中，如果 $i \\in \\mathcal{L}=\\{1,3\\}$ 则 $m_i=1$，否则 $m_i=0$，因此 $M = \\operatorname{diag}(1,0,1)$。\n第一项是：\n$$\n\\frac{1}{2}\\sum_{i\\in\\mathcal{L}}(f_{i}-y_{i})^{2} = \\frac{1}{2}\\sum_{i=1}^{3}m_{i}(f_{i}-y_{i})^{2} = \\frac{1}{2}(f-y)^T M (f-y)\n$$\n注意 $M=M^T$ 且 $M^2=M$。\n使用任务1中的恒等式，第二项是：\n$$\n\\frac{\\gamma}{2}\\sum_{i,j=1}^{3}w_{ij}(f_{i}-f_{j})^{2} = \\frac{\\gamma}{2}(2 f^T L f) = \\gamma f^T L f\n$$\n第三项是标准的岭回归惩罚项：\n$$\n\\frac{\\lambda}{2}\\sum_{i=1}^{3}f_{i}^{2} = \\frac{\\lambda}{2} f^T f = \\frac{\\lambda}{2} f^T I f\n$$\n其中 $I$ 是 $3 \\times 3$ 的单位矩阵。\n完整的矩阵形式的目标函数是：\n$$\nJ(f) = \\frac{1}{2}(f-y)^T M (f-y) + \\gamma f^T L f + \\frac{\\lambda}{2} f^T I f\n$$\n$J(f)$ 是一个严格凸函数，因为它是半正定矩阵 $M$ 和 $L$ 以及一个正定矩阵 $\\lambda I$（因为 $\\lambda0$）所对应的二次型之和。因此，存在唯一的最小化子 $f^\\star$。为了找到它，我们计算 $J(f)$ 关于 $f$ 的梯度，并将其设为零向量。\n为了求梯度，我们将 $J(f)$ 写成标准的二次型形式：\n$J(f) = \\frac{1}{2} f^T(M + 2\\gamma L + \\lambda I)f - f^T M y + \\frac{1}{2}y^T M y$。\n其梯度为 $\\nabla_f J(f) = (M + 2\\gamma L + \\lambda I)f - My$。\n\n另一种方法是逐项求导，这有助于验证：\n$\\nabla_f \\left( \\frac{1}{2}(f-y)^T M (f-y) \\right) = M(f-y)$。这是正确的。\n$\\nabla_f \\left( \\gamma f^T L f \\right) = 2 \\gamma L f$。这是因为 $L$ 是对称的。\n$\\nabla_f \\left( \\frac{\\lambda}{2} f^T I f \\right) = \\lambda I f$。这是正确的。\n所以梯度是：\n$\\nabla_f J(f) = M(f-y) + 2\\gamma Lf + \\lambda I f$。\n令 $\\nabla_f J(f^\\star) = 0$：\n$$\nM(f^\\star - y) + 2\\gamma L f^\\star + \\lambda I f^\\star = 0\n$$\n$$\n(M + 2\\gamma L + \\lambda I) f^\\star = M y\n$$\n这就是确定唯一最小化子 $f^\\star$ 的一阶最优性条件。\n\n### 任务3：具体值的计算\n\n最后一个任务是计算给定参数 $\\gamma=1$ 和 $\\lambda=1$ 时的 $f^\\star = (f_1^\\star, f_2^\\star, f_3^\\star)^T$。\n首先，构建矩阵 $W$、$D$ 和 $L$。\n权重为 $w_{12}=1$，$w_{23}=2$，$w_{13}=0$。\n邻接矩阵 $W$ 是：\n$$\nW = \\begin{pmatrix} 0  1  0 \\\\ 1  0  2 \\\\ 0  2  0 \\end{pmatrix}\n$$\n度为 $d_1 = w_{12}+w_{13} = 1$，$d_2 = w_{21}+w_{23} = 1+2=3$，$d_3 = w_{31}+w_{32} = 0+2=2$。\n度矩阵 $D$ 是：\n$$\nD = \\begin{pmatrix} 1  0  0 \\\\ 0  3  0 \\\\ 0  0  2 \\end{pmatrix}\n$$\n图拉普拉斯算子 $L=D-W$ 是：\n$$\nL = \\begin{pmatrix} 1  -1  0 \\\\ -1  3  -2 \\\\ 0  -2  2 \\end{pmatrix}\n$$\n对于 $\\mathcal{L}=\\{1,3\\}$ 的选择矩阵是 $M=\\operatorname{diag}(1,0,1)$：\n$$\nM = \\begin{pmatrix} 1  0  0 \\\\ 0  0  0 \\\\ 0  0  1 \\end{pmatrix}\n$$\n标签向量是 $y=(y_1, 0, y_3)^T=(1, 0, 0)^T$。\n当 $\\gamma=1, \\lambda=1$ 时，最优性条件是 $(M + 2L + I)f^\\star = My$。\n我们来计算矩阵 $A = M + 2L + I$：\n$$\nA = \\begin{pmatrix} 1  0  0 \\\\ 0  0  0 \\\\ 0  0  1 \\end{pmatrix} + 2\\begin{pmatrix} 1  -1  0 \\\\ -1  3  -2 \\\\ 0  -2  4 \\end{pmatrix} + \\begin{pmatrix} 1  0  0 \\\\ 0  1  0 \\\\ 0  0  1 \\end{pmatrix}\n$$\n$$\nA = \\begin{pmatrix} 1  0  0 \\\\ 0  0  0 \\\\ 0  0  1 \\end{pmatrix} + \\begin{pmatrix} 2  -2  0 \\\\ -2  6  -4 \\\\ 0  -4  4 \\end{pmatrix} + \\begin{pmatrix} 1  0  0 \\\\ 0  1  0 \\\\ 0  0  1 \\end{pmatrix} = \\begin{pmatrix} 4  -2  0 \\\\ -2  7  -4 \\\\ 0  -4  6 \\end{pmatrix}\n$$\n现在，计算右侧向量 $b = My$：\n$$\nb = \\begin{pmatrix} 1  0  0 \\\\ 0  0  0 \\\\ 0  0  1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}\n$$\n我们必须求解线性系统 $Af^\\star=b$：\n$$\n\\begin{pmatrix} 4  -2  0 \\\\ -2  7  -4 \\\\ 0  -4  6 \\end{pmatrix} \\begin{pmatrix} f_1^\\star \\\\ f_2^\\star \\\\ f_3^\\star \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}\n$$\n这给出了一个三元线性方程组：\n1. $4f_1^\\star - 2f_2^\\star = 1$\n2. $-2f_1^\\star + 7f_2^\\star - 4f_3^\\star = 0$\n3. $-4f_2^\\star + 6f_3^\\star = 0$\n\n由方程(3)得，$6f_3^\\star = 4f_2^\\star$，所以 $f_3^\\star = \\frac{4}{6}f_2^\\star = \\frac{2}{3}f_2^\\star$。\n由方程(1)得，$4f_1^\\star = 1+2f_2^\\star$，所以 $f_1^\\star = \\frac{1}{4} + \\frac{1}{2}f_2^\\star$。\n将这些代入方程(2)：\n$$\n-2\\left(\\frac{1}{4} + \\frac{1}{2}f_2^\\star\\right) + 7f_2^\\star - 4\\left(\\frac{2}{3}f_2^\\star\\right) = 0\n$$\n$$\n-\\frac{1}{2} - f_2^\\star + 7f_2^\\star - \\frac{8}{3}f_2^\\star = 0\n$$\n$$\n6f_2^\\star - \\frac{8}{3}f_2^\\star = \\frac{1}{2}\n$$\n$$\n\\left(\\frac{18}{3} - \\frac{8}{3}\\right)f_2^\\star = \\frac{1}{2}\n$$\n$$\n\\frac{10}{3}f_2^\\star = \\frac{1}{2}\n$$\n$$\nf_2^\\star = \\frac{1}{2} \\cdot \\frac{3}{10} = \\frac{3}{20}\n$$\n现在，我们求出 $f_1^\\star$ 和 $f_3^\\star$：\n$$\nf_1^\\star = \\frac{1}{4} + \\frac{1}{2}f_2^\\star = \\frac{1}{4} + \\frac{1}{2}\\left(\\frac{3}{20}\\right) = \\frac{1}{4} + \\frac{3}{40} = \\frac{10}{40} + \\frac{3}{40} = \\frac{13}{40}\n$$\n$$\nf_3^\\star = \\frac{2}{3}f_2^\\star = \\frac{2}{3}\\left(\\frac{3}{20}\\right) = \\frac{2}{20} = \\frac{1}{10}\n$$\n解是 $f^\\star = (\\frac{13}{40}, \\frac{3}{20}, \\frac{1}{10})^T$。\n\n作为验证，我们重新检查一阶条件的推导。\n对 $f_k$ 求偏导：\n$\\frac{\\partial J}{\\partial f_k} = m_k(f_k-y_k) + \\frac{\\gamma}{2} \\sum_{i,j} w_{ij} ( \\delta_{ik} - \\delta_{jk} ) 2(f_i-f_j) + \\lambda f_k = 0$。\n中间项为：\n$\\gamma \\sum_{i,j} w_{ij} (\\delta_{ik}(f_i-f_j) - \\delta_{jk}(f_i-f_j)) = \\gamma (\\sum_j w_{kj}(f_k-f_j) - \\sum_i w_{ik}(f_i-f_k))$。\n$= \\gamma (\\sum_j w_{kj}(f_k-f_j) + \\sum_i w_{ik}(f_k-f_i)) = 2\\gamma \\sum_j w_{kj}(f_k-f_j)$（因为 $w_{ik}=w_{ki}$）。\n$2\\gamma (\\sum_j w_{kj}f_k - \\sum_j w_{kj}f_j) = 2\\gamma(d_k f_k - (Wf)_k) = 2\\gamma(Lf)_k$。\n所以 $\\frac{\\partial J}{\\partial f_k} = m_k(f_k-y_k) + 2\\gamma(Lf)_k + \\lambda f_k$。\n这证实了我们的梯度计算是正确的：$\\nabla_f J = M(f-y) + 2\\gamma L f + \\lambda I f$。\n基于此推导的解 $f^\\star = (\\frac{13}{40}, \\frac{3}{20}, \\frac{1}{10})$ 是正确的。\n\n最终值：\n$f_1^\\star = \\frac{13}{40}$\n$f_2^\\star = \\frac{3}{20} = \\frac{6}{40}$\n$f_3^\\star = \\frac{1}{10} = \\frac{4}{40}$\n我们来检验方程组：\n1. $4(\\frac{13}{40}) - 2(\\frac{6}{40}) = \\frac{52-12}{40} = \\frac{40}{40} = 1$。正确。\n2. $-2(\\frac{13}{40}) + 7(\\frac{6}{40}) - 4(\\frac{4}{40}) = \\frac{-26+42-16}{40} = \\frac{0}{40} = 0$。正确。\n3. $-4(\\frac{6}{40}) + 6(\\frac{4}{40}) = \\frac{-24+24}{40} = 0$。正确。\n该解在算术上是正确的。\n\n要求最终答案为有理数形式。\n$f_1^\\star = 13/40$, $f_2^\\star = 3/20$, $f_3^\\star = 1/10$。", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{13}{40}  \\frac{3}{20}  \\frac{1}{10}\n\\end{pmatrix}\n}\n$$", "id": "3144216"}, {"introduction": "图拉普拉斯算子的谱包含了关于流形内在几何的丰富信息。本练习将介绍热核指纹（Heat Kernel Signature, HKS），它是一种多尺度的描述子，如同流形上每个点的“指纹”。您将通过比较不同形状的热核指纹来实现一个对称性检测算法，从而一窥谱方法如何揭示数据基本的几何特性 [@problem_id:3144160]。", "problem": "你需要推导并实现一种基于热核特征 (Heat Kernel Signature, HKS) 的诊断方法，用于检测由加权图近似的离散流形的内在对称性。该诊断方法必须基于黎曼流形上的热方程以及 Laplace-Beltrami 算子的谱性质。目标是确定对于每个给定的形状，是否存在一个将其映射到自身的非平凡等距变换，这在离散设置中对应于一个非平凡的图自同构。你的程序必须利用此原理，通过检查从热流中派生的逐点多尺度特征的相等性，来判断形状是否表现出内部对称性。\n\n基本要求：\n- 从紧黎曼流形 $(M, g)$ 上的热方程开始：标量热 $u(t, x)$ 满足 $\\partial_t u(t, x) = -\\Delta_M u(t, x)$，其中 $\\Delta_M$ 是 Laplace-Beltrami 算子。\n- 热核 $K_t(x, y)$ 是热方程的基本解，从热核在时间 $t  0$ 提取的逐点特征被用于对称性检测。\n- 在通过带对称邻接矩阵 $W$ 的加权图进行的离散近似中，使用对称归一化图拉普拉斯算子 $L$ 作为 $\\Delta_M$ 的一致替代，并通过组合 $L$ 在多个时间点的谱分量来计算得到的逐点多尺度热核特征。\n\n诊断要求：\n- 对于每个形状，使用几个严格为正的扩散时间，在每个节点上构建一个逐点的多尺度热核特征。定义一个用于比较两个特征的数值容差。当且仅当存在至少两个不同节点，其特征在数值容差范围内相等时，判定一个形状具有非平凡对称性。该标准将“等距变换保持 Laplace-Beltrami 算子不变，因此也保持逐点热核特征不变”这一原理付诸实施。\n\n验证形状和参数（测试套件）：\n- 案例 1（理想情况）：一个具有 $n = 16$ 个节点和均匀边权重 $1.0$ 的循环图。已知该形状具有非平凡的旋转和反射对称性。\n- 案例 2（边界条件）：一个具有 $n = 15$ 个节点和均匀边权重 $1.0$ 的路径图。该形状具有关于其中心的非平凡反射对称性。\n- 案例 3（边缘情况）：一个具有 $n = 15$ 个节点的路径图，其边权重均为 $1.0$，但节点 $0$ 和 $1$ 之间的一条边的权重被扰动为 $0.5$。此扰动破坏了反射对称性，导致没有非平凡的自同构。\n\n实现约束：\n- 使用对称归一化图拉普拉斯算子 $L = I - D^{-1/2} W D^{-1/2}$，其中 $D$ 是对角度矩阵 $D_{ii} = \\sum_j W_{ij}$，$I$ 是单位矩阵。\n- 使用扩散时间 $t \\in \\{0.01, 0.05, 0.1\\}$。\n- 在判断特征相等时，使用相对容差 $r_{\\text{tol}} = 10^{-5}$ 和绝对容差 $a_{\\text{tol}} = 10^{-8}$。\n\n输出规格：\n- 对于三个案例中的每一个，输出一个布尔值，指示诊断是否检测到非平凡对称性（即，是否存在至少两个不同节点的特征在容差范围内相等）。\n- 你的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，按上述案例的顺序列出结果。例如，输出应类似于 $[b_1,b_2,b_3]$，其中每个 $b_i$ 为 $\\text{True}$ 或 $\\text{False}$。\n\n本问题不涉及物理单位或角度单位。所有比较必须使用指定的容差进行纯数值比较。程序必须是自包含的，并且无需任何用户输入或外部文件即可产生指定的输出。", "solution": "该问题要求开发并实现一种诊断方法，用于检测离散流形中的内在对称性，这些流形由加权图表示。该诊断方法将基于热核特征（Heat Kernel Signature, HKS），这是谱形状分析中一个成熟的工具。其核心原理是，流形上的等距变换（对应于图中的自同构）会保持局部的热扩散性质，而这些性质可由 HKS 捕捉。\n\n解决方案分几步进行：首先，我们形式化流形上连续热方程与其在图上的离散对应物之间的联系。其次，我们在离散设置中定义热核特征。第三，我们建立用于对称性检测的诊断标准。最后，我们将此方法应用于指定的测试案例。\n\n**理论基础：从连续流形到离散图**\n\n在紧黎曼流形 $(M, g)$ 上，热的扩散由热方程控制：\n$$\n\\frac{\\partial u(t, x)}{\\partial t} = -\\Delta_M u(t, x)\n$$\n其中 $u(t, x)$ 是点 $x \\in M$ 在时间 $t  0$ 的温度，$\\Delta_M$ 是 Laplace-Beltrami 算子，一个将标准拉普拉斯算子推广到弯曲空间的二阶微分算子。此方程的基本解是热核 $K_t(x, y)$，它描述了在时间 $t$ 内热量从点 $y$ 流动到点 $x$ 的过程。\n\n等距变换 $\\phi: M \\to M$ 是一个保距映射。一个基本结论是，等距变换与 Laplace-Beltrami 算子是可交换的。此性质意味着热核在等距变换下是不变的，即对于所有 $x, y \\in M$，有 $K_t(x, y) = K_t(\\phi(x), \\phi(y))$。一个直接的推论是，热核的对角线值 $K_t(x, x)$ 也是保持不变的：$K_t(x, x) = K_t(\\phi(x), \\phi(x))$。如果存在一个非平凡的等距变换，使得对于 $x \\neq y$ 有 $\\phi(x) = y$，那么它们的对角线热核值必然相同：$K_t(x, x) = K_t(y, y)$。这个值 $K_t(x, x)$，被称为点 $x$ 处的热核特征（HKS），是几何形状的一个强大的局部描述子。\n\n在离散设置中，流形由一个加权图 $G = (V, E)$ 近似，其中 $V$ 是 $n$ 个节点（顶点）的集合，$E$ 是边的集合。几何信息被编码在对称的加权邻接矩阵 $W$ 中，其中如果节点 $i$ 和 $j$ 相连，则 $W_{ij}  0$，否则 $W_{ij} = 0$。Laplace-Beltrami 算子 $\\Delta_M$ 的类似物是图拉普拉斯算子。按照规定，我们使用对称归一化图拉普拉斯算子，定义为：\n$$\nL = I - D^{-1/2} W D^{-1/2}\n$$\n此处，$I$ 是 $n \\times n$ 的单位矩阵，$D$ 是对角度矩阵，其元素为 $D_{ii} = \\sum_{j=1}^{n} W_{ij}$。矩阵 $D^{-1/2}$ 是一个对角矩阵，其元素对于 $D_{ii} \\neq 0$ 为 $(D^{-1/2})_{ii} = 1/\\sqrt{D_{ii}}$。\n\n**离散热核特征 (HKS)**\n\n图上的离散热方程是一个常微分方程组：\n$$\n\\frac{d\\mathbf{u}(t)}{dt} = -L\\mathbf{u}(t)\n$$\n其中 $\\mathbf{u}(t) \\in \\mathbb{R}^n$ 是在时间 $t$ 各节点热量值的向量。其解由矩阵指数给出：$\\mathbf{u}(t) = e^{-Lt} \\mathbf{u}(0)$。矩阵 $H_t = e^{-Lt}$ 是离散热算子，其元素 $K_t(i, j) = (H_t)_{ij}$ 构成了离散热核。\n\n为计算此矩阵指数，我们使用对称矩阵 $L$ 的谱分解。设 $\\{\\lambda_k\\}_{k=1}^n$ 是 $L$ 的特征值，$\\{\\phi_k\\}_{k=1}^n$ 是对应的标准正交特征向量。特征分解为 $L = \\Phi \\Lambda \\Phi^T$，其中 $\\Lambda$ 是由特征值 $\\lambda_k$ 构成的对角矩阵，$\\Phi$ 是其列为特征向量 $\\phi_k$ 的正交矩阵。热算子则为：\n$$\nH_t = e^{-Lt} = \\Phi e^{-\\Lambda t} \\Phi^T\n$$\n其中 $e^{-\\Lambda t}$ 是对角元素为 $e^{-\\lambda_k t}$ 的对角矩阵。\n\n节点 $i$ 在时间 $t$ 的 HKS 是热核的对角元素 $K_t(i, i)$，可计算为：\n$$\nh_i(t) = K_t(i, i) = \\sum_{k=1}^{n} e^{-\\lambda_k t} (\\phi_k(i))^2\n$$\n其中 $\\phi_k(i)$ 是特征向量 $\\phi_k$ 的第 $i$ 个分量。为了创建一个更鲁棒的描述子，我们在多个时间尺度 $t \\in \\{t_1, t_2, \\ldots, t_m\\}$ 上评估 HKS。这为每个节点 $i$ 生成一个多尺度特征向量：\n$$\n\\mathbf{h}_i = [h_i(t_1), h_i(t_2), \\ldots, h_i(t_m)]^T\n$$\n对于本问题，指定的时间尺度为 $t \\in \\{0.01, 0.05, 0.1\\}$。\n\n**对称性检测算法**\n\n图上的非平凡自同构是一个保持邻接结构的节点排列。如果这样一个自同构将节点 $i$ 映射到节点 $j$（其中 $i \\neq j$），那么它们的局部几何结构是相同的，因此，它们的 HKS 向量也必须相同，即 $\\mathbf{h}_i = \\mathbf{h}_j$。因此，诊断流程如下：\n\n1.  给定一个由邻接矩阵 $W$ 定义的图，计算对称归一化拉普拉斯算子 $L = I - D^{-1/2} W D^{-1/2}$。\n2.  计算 $L$ 的完整特征分解，以获得其特征值 $\\lambda_k$ 和特征向量 $\\phi_k$。\n3.  对于每个节点 $i \\in \\{1, \\ldots, n\\}$，使用扩散时间集 $t \\in \\{0.01, 0.05, 0.1\\}$ 计算其多尺度 HKS 向量 $\\mathbf{h}_i$。\n4.  如果存在至少一对不同的节点 $(i, j)$（$i \\neq j$），它们的特征向量在指定的数值容差内相等，则检测到非平凡对称性。两个向量 $\\mathbf{a}$ 和 $\\mathbf{b}$ 的相等性比较是逐元素进行的，对于所有元素 $k$ 满足条件 $|\\mathbf{a}_k - \\mathbf{b}_k| \\leq a_{\\text{tol}} + r_{\\text{tol}} \\cdot |\\mathbf{b}_k|$。问题指定了相对容差 $r_{\\text{tol}} = 10^{-5}$ 和绝对容差 $a_{\\text{tol}} = 10^{-8}$。\n5.  如果找到这样一对节点，则结果为 `True`。否则，在检查完所有不同的节点对后仍未找到，则结果为 `False`。\n\n**在测试案例上的应用**\n\n该算法被应用于三个指定的图结构。\n\n-   **案例 1：循环图（$n = 16$）。**该图是高度对称的。在旋转操作下，所有节点在拓扑上是等价的。我们预期会找到许多具有相同 HKS 向量的节点对，从而得到 `True` 的结果。\n-   **案例 2：路径图（$n = 15$）。**该图具有围绕其中心节点（索引为 $7$ 的节点）的反射对称性。例如，节点 $0$ 与节点 $14$ 对称等价，节点 $1$ 与节点 $13$ 对称等价，以此类推。我们预期会找到像 $(0, 14)$ 这样具有相同 HKS 向量的节点对，从而得到 `True` 的结果。\n-   **案例 3：受扰动的路径图（$n = 15$）。**节点 $0$ 和 $1$ 之间的边权重从 $1.0$ 更改为 $0.5$。此扰动破坏了反射对称性。与节点 $14$ 相比，节点 $0$ 现在处于一个不同的局部环境中，这种差异将通过热扩散过程传播。我们预期不会有两个不同的节点具有相同的 HKS 向量，从而得到 `False` 的结果。\n\n实现部分将为每个案例构建邻接矩阵，执行基于 HKS 的对称性检测算法，并报告布尔值结果。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef detect_symmetry(W, times, rtol, atol):\n    \"\"\"\n    Detects intrinsic symmetry in a graph using the Heat Kernel Signature (HKS).\n\n    Args:\n        W (np.ndarray): The symmetric weighted adjacency matrix of the graph.\n        times (np.ndarray): A vector of diffusion times t.\n        rtol (float): The relative tolerance for comparing signatures.\n        atol (float): The absolute tolerance for comparing signatures.\n\n    Returns:\n        bool: True if a non-trivial symmetry is detected, False otherwise.\n    \"\"\"\n    n = W.shape[0]\n    if n = 1:\n        return False\n\n    # 1. Compute the symmetric normalized graph Laplacian L = I - D^(-1/2) W D^(-1/2)\n    degrees = np.sum(W, axis=1)\n    \n    # Handle isolated nodes to avoid division by zero, though not expected in these cases.\n    d_inv_sqrt_vec = np.zeros(n)\n    non_zero_degrees = degrees > 1e-12\n    d_inv_sqrt_vec[non_zero_degrees] = 1.0 / np.sqrt(degrees[non_zero_degrees])\n    D_inv_sqrt = np.diag(d_inv_sqrt_vec)\n    \n    I = np.identity(n)\n    L = I - D_inv_sqrt @ W @ D_inv_sqrt\n\n    # 2. Compute eigenvalues and eigenvectors of L\n    # np.linalg.eigh is used for symmetric matrices. It returns sorted eigenvalues.\n    eigenvalues, eigenvectors = np.linalg.eigh(L)\n\n    # 3. Compute the Heat Kernel Signature (HKS) for all nodes\n    # h_i(t) = sum_k exp(-lambda_k * t) * (phi_k(i))^2\n    # This can be vectorized for efficiency.\n    \n    # phi_k(i)^2 matrix: (n_nodes x n_eigenvectors)\n    eigenvectors_sq = np.square(eigenvectors)\n    \n    # exp(-lambda_k * t) matrix: (n_eigenvalues x n_times)\n    exp_matrix = np.exp(-np.outer(eigenvalues, times))\n    \n    # HKS matrix: (n_nodes x n_times)\n    # hks_matrix[i, j] = HKS of node i at time times[j]\n    hks_matrix = eigenvectors_sq @ exp_matrix\n\n    # 4. Compare signatures of all distinct pairs of nodes\n    for i in range(n):\n        for j in range(i + 1, n):\n            # Use np.allclose for element-wise comparison within tolerance\n            if np.allclose(hks_matrix[i], hks_matrix[j], rtol=rtol, atol=atol):\n                # Found two distinct nodes with the same signature, so a symmetry exists.\n                return True\n\n    # 5. If no such pair is found, no non-trivial symmetry is detected.\n    return False\n\ndef solve():\n    \"\"\"\n    Main function to run the symmetry detection for the three specified test cases.\n    \"\"\"\n    # Define parameters from the problem statement\n    diffusion_times = np.array([0.01, 0.05, 0.1])\n    r_tol = 1e-5\n    a_tol = 1e-8\n\n    # --- Case 1: Cycle graph (n=16) ---\n    n1 = 16\n    W1 = np.diag(np.ones(n1 - 1), k=1) + np.diag(np.ones(n1 - 1), k=-1)\n    W1[0, n1 - 1] = 1.0\n    W1[n1 - 1, 0] = 1.0\n\n    # --- Case 2: Path graph (n=15) ---\n    n2 = 15\n    W2 = np.diag(np.ones(n2 - 1), k=1) + np.diag(np.ones(n2 - 1), k=-1)\n\n    # --- Case 3: Perturbed path graph (n=15) ---\n    n3 = 15\n    W3 = np.diag(np.ones(n3 - 1), k=1) + np.diag(np.ones(n3 - 1), k=-1)\n    W3[0, 1] = 0.5\n    W3[1, 0] = 0.5\n    \n    test_cases = [W1, W2, W3]\n    \n    results = []\n    for W in test_cases:\n        has_symmetry = detect_symmetry(W, diffusion_times, r_tol, a_tol)\n        results.append(has_symmetry)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(str(r).capitalize() for r in results)}]\")\n\nsolve()\n\n```", "id": "3144160"}]}