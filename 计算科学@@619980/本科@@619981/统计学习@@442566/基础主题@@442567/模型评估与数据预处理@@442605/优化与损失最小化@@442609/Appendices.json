{"hands_on_practices": [{"introduction": "标准的平方损失（或最小二乘法）在理论上很方便，但在实践中对异常值（outliers）非常敏感。本练习将引导您探索 Huber 损失函数，它是一种更稳健的替代方案，通过结合平方损失和绝对值损失的优点来减小异常值的影响。通过实现迭代重加权最小二乘（IRLS）算法，您将亲手调节 Huber 损失的参数 $\\delta$，直观地理解模型在统计效率和稳健性之间的权衡。 [@problem_id:3153932]", "problem": "考虑一个单变量线性回归模型，其预测变量为 $x \\in \\mathbb{R}$，响应变量为 $y \\in \\mathbb{R}$，假设函数为 $h_{\\boldsymbol{\\beta}}(x) = \\beta_0 + \\beta_1 x$，参数为 $\\boldsymbol{\\beta} = (\\beta_0, \\beta_1)$。任务是通过在一个固定数据集上最小化平均损失来执行经验风险最小化（ERM）。仅使用良定义的凸损失函数，并确保优化的数值稳定性。数据集 $\\mathcal{D}$ 明确给出为以下 $n=9$ 个数据对 $(x_i, y_i)$（$i = 1, \\dots, n$）：\n$$(0, 1.1),\\ (1, 2.9),\\ (2, 5.2),\\ (3, 7.0),\\ (4, 8.9),\\ (5, 11.2),\\ (2.5, 20.0),\\ (4.0, -5.0),\\ (10.0, 35.0).$$\n最后三个点是故意设置的离群点（两个具有极端残差，一个在 $x$ 上具有高杠杆值）。设经验风险为 $R(\\boldsymbol{\\beta}) = \\frac{1}{n} \\sum_{i=1}^n \\ell(r_i)$，其中 $r_i = y_i - h_{\\boldsymbol{\\beta}}(x_i)$ 是残差，$\\ell$ 是一个凸损失函数。\n\n定义平方损失 $\\ell_{\\mathrm{sq}}(r) = \\tfrac{1}{2} r^2$ 和带有阈值 $\\delta > 0$ 的 Huber 损失 $L_\\delta(r)$ 如下：\n$$\nL_\\delta(r) =\n\\begin{cases}\n\\tfrac{1}{2} r^2,  \\text{if } |r| \\le \\delta, \\\\\n\\delta \\left(|r| - \\tfrac{1}{2} \\delta\\right),  \\text{if } |r| > \\delta.\n\\end{cases}\n$$\n您的程序必须：\n- 通过使用数值稳定的方法求解相应的一阶最优性条件，计算 $R(\\boldsymbol{\\beta})$ 在 $\\ell_{\\mathrm{sq}}$ 损失下的最小化子 $\\boldsymbol{\\beta}^{\\star}_{\\mathrm{sq}}$。\n- 对于每个指定的 $\\delta$ 值，使用基于原则的凸优化方法（如迭代重加权最小二乘法，IRLS），计算 $R(\\boldsymbol{\\beta})$ 在 $L_\\delta$ 损失下的最小化子 $\\boldsymbol{\\beta}^{\\star}_{\\mathrm{Huber}}(\\delta)$，并确保检查收敛标准。\n\n在您的解决方案中，从概念上解释当 $\\delta$ 变化时的稳健性权衡：改变 $\\delta$ 如何改变离群点的影响以及与平方损失最小化的关系。\n\n测试套件规范：\n- 使用给定的数据集 $\\mathcal{D}$。\n- 针对以下阈值评估 Huber 最小化子：$\\delta \\in \\{0.5, 1.0, 3.0, 10^6\\}$。\n- $\\delta = 10^6$ 的情况是近似平方损失行为的一个边界情况。\n- $\\delta = 0.5$ 的情况是一个强调稳健性的边缘情况，在大多数残差上表现出近线性的行为。\n- $\\delta = 1.0$ 的情况是一个适中的“理想路径”情况。\n- $\\delta = 3.0$ 的情况是一个较大的阈值，它降低了稳健性，但仍然限制了极端影响。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，顺序如下：\n$$[\\beta_{0,\\mathrm{sq}}, \\beta_{1,\\mathrm{sq}}, \\beta_{0,\\mathrm{Huber}}(0.5), \\beta_{1,\\mathrm{Huber}}(0.5), \\beta_{0,\\mathrm{Huber}}(1.0), \\beta_{1,\\mathrm{Huber}}(1.0), \\beta_{0,\\mathrm{Huber}}(3.0), \\beta_{1,\\mathrm{Huber}}(3.0), \\beta_{0,\\mathrm{Huber}}(10^6), \\beta_{1,\\mathrm{Huber}}(10^6)].$$\n所有数值条目必须是实数。不涉及物理单位。", "solution": "在尝试任何解决方案之前，需对问题陈述进行验证。\n\n### 步骤1：提取已知条件\n- **模型**：单变量线性回归假设 $h_{\\boldsymbol{\\beta}}(x) = \\beta_0 + \\beta_1 x$。\n- **参数**：$\\boldsymbol{\\beta} = (\\beta_0, \\beta_1)$。\n- **任务**：执行经验风险最小化（ERM）。\n- **数据集** $\\mathcal{D}$：一组 $n=9$ 个数据对 $(x_i, y_i)$：\n  $$(0, 1.1), (1, 2.9), (2, 5.2), (3, 7.0), (4, 8.9), (5, 11.2), (2.5, 20.0), (4.0, -5.0), (10.0, 35.0).$$\n- **经验风险**：$R(\\boldsymbol{\\beta}) = \\frac{1}{n} \\sum_{i=1}^n \\ell(r_i)$，其中残差为 $r_i = y_i - h_{\\boldsymbol{\\beta}}(x_i)$。\n- **损失函数**：\n    1.  平方损失：$\\ell_{\\mathrm{sq}}(r) = \\tfrac{1}{2} r^2$。\n    2.  带阈值 $\\delta > 0$ 的 Huber 损失：\n        $$\n        L_\\delta(r) =\n        \\begin{cases}\n        \\tfrac{1}{2} r^2,  \\text{if } |r| \\le \\delta, \\\\\n        \\delta \\left(|r| - \\tfrac{1}{2} \\delta\\right),  \\text{if } |r| > \\delta.\n        \\end{cases}\n        $$\n- **计算要求**：\n    1.  通过求解一阶最优性条件，使用数值稳定的方法计算平方损失最小化子 $\\boldsymbol{\\beta}^{\\star}_{\\mathrm{sq}}$。\n    2.  对于 $\\delta \\in \\{0.5, 1.0, 3.0, 10^6\\}$，使用迭代重加权最小二乘法（IRLS）计算 Huber 损失最小化子 $\\boldsymbol{\\beta}^{\\star}_{\\mathrm{Huber}}(\\delta)$。\n- **概念要求**：解释当 $\\delta$ 变化时的稳健性权衡。\n- **输出格式**：一个单行的实数列表：$[\\beta_{0,\\mathrm{sq}}, \\beta_{1,\\mathrm{sq}}, \\beta_{0,\\mathrm{Huber}}(0.5), \\beta_{1,\\mathrm{Huber}}(0.5), \\dots, \\beta_{0,\\mathrm{Huber}}(10^6), \\beta_{1,\\mathrm{Huber}}(10^6)]$。\n\n### 步骤2：使用提取的已知条件进行验证\n根据既定标准对问题进行评估：\n- **科学依据**：该问题在统计学习、稳健统计学和凸优化理论方面有扎实的基础。线性回归、平方损失、Huber 损失和经验风险最小化都是标准的、成熟的概念。\n- **良定性**：平方损失和 Huber 损失的目标函数都是关于参数 $\\boldsymbol{\\beta}$ 的凸函数。只要设计矩阵是满秩的（由于 $x_i$ 的值不完全相同，所以确实如此），就存在唯一的最小化子。该问题是自洽的，并提供了所有必要的信息。\n- **客观性**：该问题使用精确的数学语言陈述，不包含任何主观或模糊的术语。\n\n未发现任何缺陷。该问题不违反任何科学原理，不是不完整或矛盾的，计算上是可行的，并且是良定的。包含离群点和一系列 $\\delta$ 值构成了对稳健回归原理的有意义的测试，而非简单的练习。\n\n### 步骤3：结论与行动\n该问题是 **有效的**。将构建一个基于原则的解决方案。\n\n### 解决方案推导\n\n问题的核心是找到参数向量 $\\boldsymbol{\\beta} = [\\beta_0, \\beta_1]^T$，以最小化数据集 $\\mathcal{D}$ 上的平均损失。这可以用矩阵表示法来表述。设响应向量为 $\\mathbf{y} \\in \\mathbb{R}^n$，设计矩阵为 $\\mathbf{X} \\in \\mathbb{R}^{n \\times 2}$，其中 $n=9$。\n$$\n\\mathbf{y} = \\begin{pmatrix} 1.1 \\\\ 2.9 \\\\ 5.2 \\\\ 7.0 \\\\ 8.9 \\\\ 11.2 \\\\ 20.0 \\\\ -5.0 \\\\ 35.0 \\end{pmatrix}, \\quad\n\\mathbf{X} = \\begin{pmatrix} 1  0 \\\\ 1  1 \\\\ 1  2 \\\\ 1  3 \\\\ 1  4 \\\\ 1  5 \\\\ 1  2.5 \\\\ 1  4.0 \\\\ 1  10.0 \\end{pmatrix}\n$$\n残差向量为 $\\mathbf{r} = \\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta}$。\n\n**1. 平方损失下的最小化（普通最小二乘法）**\n\n平方损失的经验风险为：\n$$R_{\\mathrm{sq}}(\\boldsymbol{\\beta}) = \\frac{1}{n} \\sum_{i=1}^n \\frac{1}{2} (y_i - (\\beta_0 + \\beta_1 x_i))^2 = \\frac{1}{2n} \\|\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta}\\|_2^2$$\n最小化 $R_{\\mathrm{sq}}(\\boldsymbol{\\beta})$ 等价于最小化残差向量的欧几里得范数的平方 $\\|\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta}\\|_2^2$。这是经典的普通最小二乘法（OLS）问题。通过将关于 $\\boldsymbol{\\beta}$ 的梯度设为零，可以找到一阶最优性条件：\n$$\\nabla_{\\boldsymbol{\\beta}} \\left( \\frac{1}{2n} (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})^T (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta}) \\right) = -\\frac{1}{n} \\mathbf{X}^T (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta}) = \\mathbf{0}$$\n这可以简化为著名的正规方程组：\n$$\\mathbf{X}^T \\mathbf{X} \\boldsymbol{\\beta} = \\mathbf{X}^T \\mathbf{y}$$\n最优参数 $\\boldsymbol{\\beta}^{\\star}_{\\mathrm{sq}}$ 的解由下式给出：\n$$\\boldsymbol{\\beta}^{\\star}_{\\mathrm{sq}} = (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{y}$$\n为了数值稳定性，应避免直接计算逆矩阵 $(\\mathbf{X}^T \\mathbf{X})^{-1}$。相反，可以求解正规方程组的线性系统。一种更稳健的方法，也是数值库中的标准做法，是使用 $\\mathbf{X}$ 的矩阵分解（如 QR 或 SVD 分解）来求解最小二乘问题。例如，如果 $\\mathbf{X} = \\mathbf{Q}\\mathbf{R}$，问题就简化为通过回代法求解上三角系统 $\\mathbf{R}\\boldsymbol{\\beta} = \\mathbf{Q}^T\\mathbf{y}$。\n\n**2. Huber 损失下的最小化（稳健回归）**\n\nHuber 损失的经验风险为：\n$$R_{\\mathrm{Huber}}(\\boldsymbol{\\beta}) = \\frac{1}{n} \\sum_{i=1}^n L_\\delta(y_i - \\mathbf{x}_i^T \\boldsymbol{\\beta})$$\n其中 $\\mathbf{x}_i^T$ 是 $\\mathbf{X}$ 的第 $i$ 行。由于 $L_\\delta$ 是一个凸函数，$R_{\\mathrm{Huber}}$ 也是凸函数，并且存在一个全局最小值。一阶最优性条件是 $\\nabla_{\\boldsymbol{\\beta}} R_{\\mathrm{Huber}}(\\boldsymbol{\\beta}) = \\mathbf{0}$。$L_\\delta(r)$ 的导数是 Huber 影响函数 $\\psi_\\delta(r) = L'_\\delta(r)$：\n$$\n\\psi_\\delta(r) = \\mathrm{clip}(r, -\\delta, \\delta) =\n\\begin{cases}\nr,  \\text{if } |r| \\le \\delta \\\\\n\\delta \\cdot \\mathrm{sgn}(r),  \\text{if } |r| > \\delta\n\\end{cases}\n$$\n风险的梯度为：\n$$\\nabla_{\\boldsymbol{\\beta}} R_{\\mathrm{Huber}} = \\frac{1}{n} \\sum_{i=1}^n - \\psi_\\delta(y_i - \\mathbf{x}_i^T\\boldsymbol{\\beta}) \\mathbf{x}_i = \\mathbf{0}$$\n这给出了非线性方程组 $\\sum_{i=1}^n \\psi_\\delta(r_i) \\mathbf{x}_i = \\mathbf{0}$。为了求解这个方程组，我们使用迭代重加权最小二乘法（IRLS）算法。我们定义一个权重函数 $w(r) = \\psi_\\delta(r)/r：$\n$$\nw(r) =\n\\begin{cases}\n1,  \\text{if } |r| \\le \\delta \\\\\n\\delta/|r|,  \\text{if } |r| > \\delta\n\\end{cases}\n$$\n使用这个权重，条件变为 $\\sum_{i=1}^n w(r_i) r_i \\mathbf{x}_i = \\mathbf{0}$，可以写成 $\\sum_{i=1}^n w_i (y_i - \\mathbf{x}_i^T \\boldsymbol{\\beta}) \\mathbf{x}_i = \\mathbf{0}$，其中 $w_i = w(r_i)$。这可以重新排列成一个加权最小二乘形式：\n$$(\\mathbf{X}^T \\mathbf{W}(\\boldsymbol{\\beta}) \\mathbf{X}) \\boldsymbol{\\beta} = \\mathbf{X}^T \\mathbf{W}(\\boldsymbol{\\beta}) \\mathbf{y}$$\n这里，$\\mathbf{W}(\\boldsymbol{\\beta})$ 是一个由权重 $w_i$ 构成的对角矩阵，这些权重本身通过残差 $r_i$ 依赖于 $\\boldsymbol{\\beta}$。这种依赖关系表明需要一个迭代解：\n\n**IRLS 算法：**\n1.  **初始化**：选择一个初始参数估计 $\\boldsymbol{\\beta}^{(0)}$。OLS 解 $\\boldsymbol{\\beta}^{\\star}_{\\mathrm{sq}}$ 是一个合适的起点。\n2.  **迭代**：对 $k=0, 1, 2, \\dots$ 进行迭代，直到收敛：\n    a.  **计算残差**：$r_i^{(k)} = y_i - \\mathbf{x}_i^T \\boldsymbol{\\beta}^{(k)}$。\n    b.  **计算权重**：$w_i^{(k+1)} = w(r_i^{(k)})$。形成对角权重矩阵 $\\mathbf{W}^{(k+1)}$。\n    c.  **求解加权最小二乘**：通过求解加权最小二乘问题来更新参数估计：\n        $$\\boldsymbol{\\beta}^{(k+1)} = \\arg\\min_{\\boldsymbol{\\beta}} \\sum_{i=1}^n w_i^{(k+1)} (y_i - \\mathbf{x}_i^T\\boldsymbol{\\beta})^2$$\n        解通过求解以下线性系统给出：\n        $$(\\mathbf{X}^T \\mathbf{W}^{(k+1)} \\mathbf{X}) \\boldsymbol{\\beta}^{(k+1)} = \\mathbf{X}^T \\mathbf{W}^{(k+1)} \\mathbf{y}$$\n3.  **检查收敛性**：当参数向量的变化，例如 $\\|\\boldsymbol{\\beta}^{(k+1)} - \\boldsymbol{\\beta}^{(k)}\\|_2$，低于预定义的容差时，过程停止。\n\n**3. 概念性解释：稳健性与 $\\delta$ 的作用**\n\n平方损失和 Huber 损失之间的关键区别在于它们对大残差（离群点）的处理方式。\n- **平方损失**：数据点 $i$ 对损失函数梯度的贡献与其残差 $r_i$ 成正比。如果一个离群点导致了大的残差，它会对拟合模型施加相应大的“拉力”。这使得 OLS 回归对离群点高度敏感。在 IRLS 框架中，这相当于将所有权重 $w_i$ 设为 1，而不管残差的大小。\n\n- **Huber 损失**：残差的影响是有界的。对于残差 $|r_i| > \\delta$，其影响被限制在一个常数值 $\\pm\\delta$。在 IRLS 算法中，这通过权重反映出来。对于一个 $|r_i| \\le \\delta$ 的正常点，权重为 $w_i=1$，与 OLS 中一样。对于一个 $|r_i| > \\delta$ 的离群点，权重变为 $w_i = \\delta/|r_i|  1$。随着残差大小的增加，该权重减小，从而有效地降低了离群点对参数估计的影响。\n\n参数 $\\delta$ 控制着统计效率和稳健性之间的权衡：\n- **大的 $\\delta$ (例如, $\\delta=10^6$)**：阈值 $\\delta$ 非常大，以至于数据集中的所有残差都将满足 $|r_i| \\le \\delta$。在这种情况下，对于所有数据点，Huber 损失 $L_\\delta(r)$ 变得与平方损失 $\\frac{1}{2}r^2$ 完全相同。因此，IRLS 算法中的所有权重 $w_i$ 都将为 1，并且 Huber 最小化子 $\\boldsymbol{\\beta}^{\\star}_{\\mathrm{Huber}}(\\delta)$ 将收敛于 OLS 最小化子 $\\boldsymbol{\\beta}^{\\star}_{\\mathrm{sq}}$。\n- **小的 $\\delta$ (例如, $\\delta=0.5$)**：阈值非常低。除了最小的那些残差外，大多数残差都将满足 $|r_i|  \\delta$。对于这些点，损失实际上是线性的 ($|r|$)，而不是二次的 ($r^2$)。由此产生的拟合是高度稳健的，因为大离群点的影响被严重削减。其行为接近于最小绝对偏差（L1）回归。\n- **中等的 $\\delta$**：适度选择的 $\\delta$ 提供了一种平衡。它使得拟合能够从大部分数据（假定为“正常点”）的平方损失的高效率中受益，同时保护拟合免受少数具有大残差的“离群点”的扭曲。$\\delta$ 的值实际上定义了我们所认为的离群点。\n\n总之，通过改变 $\\delta$，可以在非稳健但高效的 OLS 回归与对极端数据点不那么敏感的高度稳健回归之间进行平滑插值。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes OLS and Huber regression parameters for a given dataset.\n    \"\"\"\n    # Define the dataset from the problem statement\n    data = np.array([\n        [0.0, 1.1],\n        [1.0, 2.9],\n        [2.0, 5.2],\n        [3.0, 7.0],\n        [4.0, 8.9],\n        [5.0, 11.2],\n        [2.5, 20.0],\n        [4.0, -5.0],\n        [10.0, 35.0]\n    ])\n\n    x_data = data[:, 0]\n    y_data = data[:, 1]\n    \n    # Construct the design matrix X with an intercept term\n    X = np.vstack([np.ones(len(x_data)), x_data]).T\n\n    # Test suite specification\n    delta_values = [0.5, 1.0, 3.0, 10**6]\n    \n    # List to store all computed beta values\n    all_beta_values = []\n\n    # --- Part 1: Squared Loss Minimization (OLS) ---\n    # Use numpy.linalg.lstsq for a numerically stable solution\n    beta_sq, _, _, _ = np.linalg.lstsq(X, y_data, rcond=None)\n    all_beta_values.extend(beta_sq)\n\n    # --- Part 2: Huber Loss Minimization (IRLS) ---\n    \n    def solve_huber_irls(X_mat, y_vec, delta, tol=1e-8, max_iter=100):\n        \"\"\"\n        Solves for Huber regression parameters using Iteratively Reweighted Least Squares (IRLS).\n        \n        Args:\n            X_mat (np.ndarray): Design matrix.\n            y_vec (np.ndarray): Response vector.\n            delta (float): Huber loss threshold.\n            tol (float): Convergence tolerance.\n            max_iter (int): Maximum number of iterations.\n            \n        Returns:\n            np.ndarray: The estimated parameter vector beta.\n        \"\"\"\n        # Initialize beta with the OLS solution\n        beta, _, _, _ = np.linalg.lstsq(X_mat, y_vec, rcond=None)\n        \n        for _ in range(max_iter):\n            beta_old = beta.copy()\n            \n            # 1. Compute residuals\n            residuals = y_vec - X_mat @ beta\n            abs_residuals = np.abs(residuals)\n            \n            # 2. Compute weights\n            # Start with weights of 1 for all points\n            weights = np.ones_like(y_vec)\n            # Identify outliers (where |r|  delta)\n            outlier_idx = abs_residuals  delta\n            # Update weights for outliers. This is safe from division by zero\n            # because abs_residuals[outlier_idx]  delta  0.\n            weights[outlier_idx] = delta / abs_residuals[outlier_idx]\n\n            # 3. Solve weighted least squares problem\n            # This is done by transforming the system and using a standard lstsq solver\n            # sqrt_W = np.sqrt(np.diag(weights))\n            # X_w = sqrt_W @ X\n            # y_w = sqrt_W @ y\n            # A more efficient way without forming the diagonal matrix:\n            sqrt_w_vec = np.sqrt(weights)\n            X_w = X_mat * sqrt_w_vec[:, np.newaxis]\n            y_w = y_vec * sqrt_w_vec\n            \n            beta, _, _, _ = np.linalg.lstsq(X_w, y_w, rcond=None)\n            \n            # 4. Check for convergence\n            if np.linalg.norm(beta - beta_old)  tol:\n                break\n                \n        return beta\n\n    # Loop through each delta value and compute the Huber regression parameters\n    for delta in delta_values:\n        beta_huber = solve_huber_irls(X, y_data, delta)\n        all_beta_values.extend(beta_huber)\n\n    # Final print statement in the exact required format\n    print(f\"[{','.join(f'{val:.8f}' for val in all_beta_values)}]\")\n\nsolve()\n```", "id": "3153932"}, {"introduction": "在特征选择等高维问题中，像 LASSO 这样的凸惩罚项虽然有效，但可能会对大系数产生不必要的偏差。本练习将带您进入非凸优化的前沿领域，探索如平滑剪裁绝对偏差（SCAD）和极小极大凹惩罚（MCP）等更先进的正则化方法，它们旨在提供更稀疏且偏差更小的解。您将实现坐标下降算法，并观察非凸性如何导致多重局部最小值，从而揭示初始值选择对最终模型性能的深刻影响。 [@problem_id:3153982]", "problem": "考虑统计学习中的惩罚回归问题，其目标是最小化一个由均方误差数据保真项和非凸正则化项组成的经验风险。令 $X \\in \\mathbb{R}^{n \\times p}$ 表示设计矩阵，$y \\in \\mathbb{R}^n$ 表示响应向量，$b \\in \\mathbb{R}^p$ 表示系数向量。待最小化的经验风险为\n$$\n\\mathcal{L}(b) \\triangleq \\frac{1}{2n}\\lVert y - X b \\rVert_2^2 + \\sum_{j=1}^p \\mathcal{P}(b_j;\\lambda,\\theta),\n$$\n其中正则化项 $\\mathcal{P}$ 是平滑裁剪绝对偏差 (Smoothly Clipped Absolute Deviation, SCAD) 惩罚项（调节参数 $\\lambda  0$，形状参数 $a  2$），或者是最小最大凹惩罚项 (Minimax Concave Penalty, MCP)（调节参数 $\\lambda  0$，形状参数 $\\gamma  1$）。SCAD 和 MCP 都是非凸惩罚项，它们在鼓励稀疏性的同时，相对于凸惩罚项能减少偏差。\n\n从基本定义出发，实现一个坐标下降算法，该算法在保持所有其他坐标固定的情况下，每次循环最小化 $\\mathcal{L}(b)$ 的一个坐标 $b_j$。该算法必须：\n- 将每个特征列 $X_{\\cdot j}$ 标准化，使其均值为零并满足 $\\lVert X_{\\cdot j} \\rVert_2^2 / n = 1$，同时将 $y$ 中心化至零均值，以使单坐标更新具有良好的尺度。\n- 对 $b$ 使用两种不同的初始化：零向量和通过 Moore-Penrose 伪逆得到的普通最小二乘解。\n- 对于每次坐标更新，推导并实现由 SCAD 或 MCP 下的一维子问题的一阶最优性条件所蕴含的精确闭式坐标级最小化器。不要依赖通用的黑盒优化程序；相反，应使用由平稳性方程和次梯度条件产生的显式分段阈值映射。\n\n程序必须为下面指定的每个测试用例计算以下量：\n1. 从零初始化和最小二乘初始化得到的系数向量之间的欧几里得范数差，即 $\\lVert b^{(0)} - b^{(\\mathrm{LS})} \\rVert_2$。\n2. 两种初始化下最小化目标函数值之间的绝对差，即 $\\left|\\mathcal{L}\\big(b^{(0)}\\big) - \\mathcal{L}\\big(b^{(\\mathrm{LS})}\\big)\\right|$。\n\n您的实现必须按照指示使用具有固定随机种子的确定性数据生成。当任何坐标的最大绝对变化低于容差或达到最大迭代次数时，坐标下降应终止。使用以下测试套件来探索不同的情景，包括“理想路径”、可能导致多个局部最小值的强共线性以及边界条件：\n\n- 测试用例 1 (SCAD, 正交设计, 理想路径):\n  - 设置 $n=50, p=3$。通过随机种子为 0 的 $\\mathrm{QR}$ 分解，获得一个 $\\mathbb{R}^{n \\times n}$ 中的随机正交矩阵，取其前 $p$ 列生成 $X$，然后缩放各列以使 $\\lVert X_{\\cdot j} \\rVert_2^2/n = 1$。令 $b^\\star = [2.5, -1.0, 0.0]^\\top$。生成 $y = X b^\\star$（无噪声），然后按规定中心化 $y$ 和标准化 $X$。使用 SCAD，参数为 $\\lambda = 0.5$ 和 $a = 3.7$。使用两种初始化运行坐标下降，并计算所要求的两个量。\n- 测试用例 2 (SCAD, 完全共线性, 局部最小值敏感性):\n  - 设置 $n=60, p=2$。使用随机种子 1，生成一个具有独立标准正态分布项的向量 $u \\in \\mathbb{R}^n$。令 $X = [u, u]$（两个相同的列），然后中心化并缩放以满足 $\\lVert X_{\\cdot j} \\rVert_2^2/n = 1$。生成 $y = 3 u$，然后中心化 $y$。使用 SCAD，参数为 $\\lambda = 1.5$ 和 $a = 3.7$。使用两种初始化运行坐标下降，并计算所要求的两个量。\n- 测试用例 3 (MCP, 边界情况 $\\lambda = 0$):\n  - 设置 $n=80, p=4$。使用随机种子 2，生成具有独立标准正态分布项的 $X$，然后中心化并缩放各列以满足 $\\lVert X_{\\cdot j} \\rVert_2^2/n = 1$。令 $b^\\star = [1.0, -2.0, 0.0, 0.5]^\\top$。生成 $y = X b^\\star$（无噪声），然后中心化 $y$。使用 MCP，参数为 $\\lambda = 0$ 和 $\\gamma = 3.0$。使用两种初始化运行坐标下降，并计算所要求的两个量。\n- 测试用例 4 (MCP, 完全共线性, 强非凸性):\n  - 设置 $n=60, p=2$。使用随机种子 3，生成一个具有独立标准正态分布项的向量 $u \\in \\mathbb{R}^n$。令 $X = [u, u]$（两个相同的列），然后中心化并缩放以满足 $\\lVert X_{\\cdot j} \\rVert_2^2/n = 1$。生成 $y = 2 u$，然后中心化 $y$。使用 MCP，参数为 $\\lambda = 1.2$ 和 $\\gamma = 1.5$。使用两种初始化运行坐标下降，并计算所要求的两个量。\n\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果，每个元素对应一个测试用例，其本身是一个包含两个浮点数的列表 $[\\lVert b^{(0)} - b^{(\\mathrm{LS})} \\rVert_2, \\left|\\mathcal{L}\\big(b^{(0)}\\big) - \\mathcal{L}\\big(b^{(\\mathrm{LS})}\\big)\\right|]$。例如，输出必须如下所示：\n$$\n\\texttt{[[x_1,y_1],[x_2,y_2],[x_3,y_3],[x_4,y_4]]}\n$$\n其中 $x_i$ 和 $y_i$ 以十进制浮点数表示。", "solution": "该问题要求实现一个坐标下降算法，以解决带有非凸正则化项（特别是平滑裁剪绝对偏差 (SCAD) 或最小最大凹惩罚项 (MCP)）的惩罚回归问题。解决方案必须从第一性原理推导得出，包括数据标准化和坐标级更新规则。\n\n要最小化的目标函数是经验风险 $\\mathcal{L}(b)$：\n$$\n\\mathcal{L}(b) = \\frac{1}{2n}\\lVert y - X b \\rVert_2^2 + \\sum_{j=1}^p \\mathcal{P}(b_j;\\lambda,\\theta)\n$$\n其中 $X \\in \\mathbb{R}^{n \\times p}$ 是设计矩阵，$y \\in \\mathbb{R}^n$ 是响应向量，$b \\in \\mathbb{R}^p$ 是系数向量，$\\mathcal{P}$ 是带有调节参数 $\\lambda$ 和形状参数 $\\theta$（对于 SCAD 是 $a$，对于 MCP 是 $\\gamma$）的惩罚函数。\n\n首先，根据问题陈述，必须对数据进行标准化。响应向量 $y$ 被中心化，使其均值为零。设计矩阵的每一列 $j$，$X_{\\cdot j}$，被中心化使其均值为零，然后进行缩放，使其欧几里得范数的平方除以 $n$ 等于 1，即 $\\frac{1}{n}\\lVert X_{\\cdot j} \\rVert_2^2 = 1$。这种标准化确保了惩罚项能公平地应用于所有系数，并简化了坐标更新的推导过程。\n\n该算法的核心是坐标下降，它在保持所有其他系数 $b_k$（$k \\neq j$）固定的情况下，迭代地最小化关于单个系数 $b_j$ 的目标函数。用于更新 $b_j$ 的一维子问题是：\n$$\n\\arg\\min_{b_j} \\mathcal{L}(b_1, \\dots, b_j, \\dots, b_p)\n$$\n我们分离出依赖于 $b_j$ 的项。残差可以写为 $y - Xb = (y - \\sum_{k\\neq j} X_{\\cdot k} b_k) - X_{\\cdot j} b_j$。令 $r_j = y - \\sum_{k\\neq j} X_{\\cdot k} b_k$ 为部分残差。子问题变为：\n$$\n\\arg\\min_{b_j} \\left\\{ \\frac{1}{2n} \\lVert r_j - X_{\\cdot j} b_j \\rVert_2^2 + \\mathcal{P}(b_j) \\right\\}\n$$\n展开平方范数并使用标准化 $\\lVert X_{\\cdot j} \\rVert_2^2 = n$，我们得到：\n$$\n\\arg\\min_{b_j} \\left\\{ \\frac{1}{2n} (\\lVert r_j \\rVert_2^2 - 2b_j X_{\\cdot j}^\\top r_j + b_j^2 n) + \\mathcal{P}(b_j) \\right\\}\n$$\n忽略常数项并进行化简，这等价于：\n$$\n\\arg\\min_{b_j} \\left\\{ \\frac{1}{2} b_j^2 - \\frac{1}{n}(X_{\\cdot j}^\\top r_j)b_j + \\mathcal{P}(b_j) \\right\\}\n$$\n令 $z_j = \\frac{1}{n} X_{\\cdot j}^\\top r_j$。该问题等价于最小化 $\\frac{1}{2}(b_j - z_j)^2 + \\mathcal{P}(b_j)$，这是一个近端算子问题。为了高效计算，$z_j$ 可以从完全残差 $\\tilde{r} = y - X b^{\\text{old}}$ 计算得出，即 $z_j = \\frac{1}{n} X_{\\cdot j}^\\top \\tilde{r} + b_j^{\\text{old}}$。\n\n这个一维最小化问题的解由一个依赖于惩罚项 $\\mathcal{P}$ 的阈值函数给出。我们推导 SCAD 和 MCP 的闭式最小化器。\n\n**SCAD 阈值化：**\nSCAD 惩罚项由其导数定义。为了找到最小化器，我们求解一阶条件 $b_j - z_j + \\mathcal{P}'(b_j) = 0$。对于非凸的 SCAD 惩罚项，这可能有多个解。标准的坐标下降算法使用以下已被广泛接受的阈值函数，它会选择适当的最小化器：\n$$\nb_j^{\\text{new}} = S_{\\text{SCAD}}(z_j; \\lambda, a) = \\begin{cases}\n\\text{sgn}(z_j) \\max(0, |z_j|-\\lambda)  \\text{if } |z_j| \\le 2\\lambda \\\\\n\\frac{(a-1)z_j - \\text{sgn}(z_j)a\\lambda}{a-2}  \\text{if } 2\\lambda  |z_j| \\le a\\lambda \\\\\nz_j  \\text{if } |z_j| > a\\lambda\n\\end{cases}\n$$\n其中 $a > 2$ 是形状参数。\n\n**MCP 阈值化：**\n类似地，对于 MCP，其阈值算子由其一阶条件推导得出，并由下式给出：\n$$\nb_j^{\\text{new}} = S_{\\text{MCP}}(z_j; \\lambda, \\gamma) = \\begin{cases}\n0  \\text{if } |z_j| \\le \\lambda \\\\\n\\text{sgn}(z_j) \\frac{\\gamma(|z_j| - \\lambda)}{\\gamma - 1}  \\text{if } \\lambda  |z_j| \\le \\gamma\\lambda \\\\\nz_j  \\text{if } |z_j| > \\gamma\\lambda\n\\end{cases}\n$$\n其中 $\\gamma > 1$ 是形状参数。\n\n算法按以下步骤进行：\n1.  为给定的测试用例生成数据 $(X, y)$。\n2.  标准化 $X$ 并中心化 $y$。\n3.  初始化两个系数向量：$b^{(0)}$ 为零向量，$b^{(\\mathrm{LS})}$ 为通过标准化后的 $X$ 的 Moore-Penrose 伪逆计算得到的普通最小二乘解。\n4.  对于每种初始化，运行坐标下降算法：\n    a. 循环遍历坐标 $j=1, \\dots, p$。\n    b. 计算 $z_j = \\frac{1}{n}X_{\\cdot j}^\\top(y - Xb) + b_j$。\n    c. 使用适当的阈值函数（$S_{\\text{SCAD}}$ 或 $S_{\\text{MCP}}$）更新 $b_j$。\n    d. 重复此过程，直到任何坐标的最大绝对变化低于容差（例如，$10^{-8}$）或达到最大迭代次数。\n5.  收敛后，计算最终的目标函数值 $\\mathcal{L}(b^{(0)})$ 和 $\\mathcal{L}(b^{(\\mathrm{LS})})$，其中 $b^{(0)}$ 和 $b^{(\\mathrm{LS})}$ 表示从各自初始化得到的收敛向量。\n6.  计算所要求的两个量：最终系数向量之间的欧几里得范数差 $\\lVert b^{(0)} - b^{(\\mathrm{LS})} \\rVert_2$，以及它们的目标函数值之间的绝对差 $\\left|\\mathcal{L}(b^{(0)}) - \\mathcal{L}(b^{(\\mathrm{LS})})\\right|$。\n\n对所有四个测试用例重复此过程，这些测试用例旨在探究算法在不同条件下的行为，包括正交设计、完全共线性（可能暴露多个局部最小值）以及零惩罚的边界情况。SCAD 和 MCP 的非凸性意味着最终解可能对起始点敏感，通过比较零初始化和 OLS 初始化的结果，可以显式地测试这一现象。", "answer": "```python\nimport numpy as np\n\ndef scad_penalty(t, lambda_, a):\n    \"\"\"Computes the SCAD penalty value.\"\"\"\n    abs_t = np.abs(t)\n    if abs_t = lambda_:\n        return lambda_ * abs_t\n    elif abs_t = a * lambda_:\n        return -(abs_t**2 - 2 * a * lambda_ * abs_t + lambda_**2) / (2 * (a - 1))\n    else:\n        return (a + 1) * lambda_**2 / 2\n\ndef mcp_penalty(t, lambda_, gamma):\n    \"\"\"Computes the MCP penalty value.\"\"\"\n    abs_t = np.abs(t)\n    if abs_t = gamma * lambda_:\n        return lambda_ * abs_t - abs_t**2 / (2 * gamma)\n    else:\n        return 0.5 * gamma * lambda_**2\n\ndef calculate_loss(X, y, b, penalty_type, lambda_, shape_param):\n    \"\"\"Calculates the objective function value.\"\"\"\n    n = X.shape[0]\n    mse = (0.5 / n) * np.sum((y - X @ b)**2)\n    \n    total_penalty = 0.0\n    if penalty_type == 'scad':\n        a = shape_param\n        if lambda_ > 0:\n            for val in b:\n                total_penalty += scad_penalty(val, lambda_, a)\n    elif penalty_type == 'mcp':\n        gamma = shape_param\n        if lambda_ > 0:\n            for val in b:\n                total_penalty += mcp_penalty(val, lambda_, gamma)\n\n    return mse + total_penalty\n\ndef scad_threshold(z, lambda_, a):\n    \"\"\"Computes the SCAD thresholding operator.\"\"\"\n    abs_z = np.abs(z)\n    if abs_z = 2 * lambda_:\n        return np.sign(z) * max(0, abs_z - lambda_)\n    elif abs_z = a * lambda_:\n        return ((a - 1) * z - np.sign(z) * a * lambda_) / (a - 2)\n    else:\n        return z\n\ndef mcp_threshold(z, lambda_, gamma):\n    \"\"\"Computes the MCP thresholding operator.\"\"\"\n    abs_z = np.abs(z)\n    if abs_z = lambda_:\n        return 0.0\n    elif abs_z = gamma * lambda_:\n        return np.sign(z) * gamma * (abs_z - lambda_) / (gamma - 1)\n    else:\n        return z\n\ndef coordinate_descent(X, y, b_init, penalty_type, lambda_, shape_param, tol=1e-8, max_iter=1000):\n    \"\"\"Performs coordinate descent for nonconvex penalized regression.\"\"\"\n    n, p = X.shape\n    b = b_init.copy()\n    \n    threshold_func = None\n    if penalty_type == 'scad':\n        threshold_func = lambda z: scad_threshold(z, lambda_, shape_param)\n    elif penalty_type == 'mcp':\n        threshold_func = lambda z: mcp_threshold(z, lambda_, shape_param)\n    else:\n        raise ValueError(\"Unknown penalty type\")\n\n    for _ in range(max_iter):\n        b_old = b.copy()\n        for j in range(p):\n            # Calculate z_j efficiently\n            # z_j = (1/n) * X_j^T * r_j where r_j is partial residual\n            # z_j = (1/n) * X_j^T * (y - sum_{k!=j} X_k*b_k)\n            # which is z_j = (1/n) * X_j^T * (y - X*b_old + X_j*b_old_j)\n            # z_j = b_old[j] + (1/n) * X[:, j].T @ (y - X @ b)\n            # Note: b is being updated in the inner loop.\n            residual = y - X @ b\n            z_j = b[j] + (X[:, j].T @ residual) / n\n            \n            b[j] = threshold_func(z_j)\n\n        if np.max(np.abs(b - b_old))  tol:\n            break\n            \n    return b\n\ndef run_case(n, p, seed, penalty_type, lambda_, shape_param, b_star, X_gen_type, y_gen_val):\n    \"\"\"Generates data, runs CD, and computes metrics for one test case.\"\"\"\n    rng = np.random.default_rng(seed)\n\n    # Generate data\n    if X_gen_type == 'ortho':\n        Q, _ = np.linalg.qr(rng.standard_normal(size=(n, n)))\n        X = Q[:, :p]\n    elif X_gen_type == 'collinear':\n        u = rng.standard_normal(size=n)\n        X = np.tile(u, (p, 1)).T\n    else: # 'random'\n        X = rng.standard_normal(size=(n, p))\n    \n    # Standardize X\n    X_mean = X.mean(axis=0)\n    X_centered = X - X_mean\n    X_scales = np.linalg.norm(X_centered, axis=0) / np.sqrt(n)\n    # Handle columns with zero norm to avoid division by zero\n    X_scales[X_scales == 0] = 1.0\n    X_std = X_centered / X_scales\n\n    # Generate y\n    if y_gen_val == 'b_star':\n        y = X @ b_star # Use original X to generate y from true b_star\n    else:\n        # For collinear cases, u is from original un-standardized X\n        u = X[:, 0]\n        y = y_gen_val * u\n    \n    # Center y\n    y_mean = y.mean()\n    y_centered = y - y_mean\n    \n    # Initializations\n    b_init_zero = np.zeros(p)\n    b_init_ls = np.linalg.pinv(X_std) @ y_centered\n\n    # Run coordinate descent from both initializations\n    b_final_zero = coordinate_descent(X_std, y_centered, b_init_zero, penalty_type, lambda_, shape_param)\n    b_final_ls = coordinate_descent(X_std, y_centered, b_init_ls, penalty_type, lambda_, shape_param)\n\n    # Calculate losses\n    loss_zero = calculate_loss(X_std, y_centered, b_final_zero, penalty_type, lambda_, shape_param)\n    loss_ls = calculate_loss(X_std, y_centered, b_final_ls, penalty_type, lambda_, shape_param)\n\n    # Compute final metrics\n    norm_diff = np.linalg.norm(b_final_zero - b_final_ls)\n    loss_diff = np.abs(loss_zero - loss_ls)\n    \n    return [norm_diff, loss_diff]\n\ndef solve():\n    \"\"\"Main function to run all test cases and print results.\"\"\"\n    test_cases = [\n        # Test Case 1\n        {'n': 50, 'p': 3, 'seed': 0, 'penalty_type': 'scad', 'lambda_': 0.5, 'shape_param': 3.7,\n         'b_star': np.array([2.5, -1.0, 0.0]), 'X_gen_type': 'ortho', 'y_gen_val': 'b_star'},\n        # Test Case 2\n        {'n': 60, 'p': 2, 'seed': 1, 'penalty_type': 'scad', 'lambda_': 1.5, 'shape_param': 3.7,\n         'b_star': None, 'X_gen_type': 'collinear', 'y_gen_val': 3.0},\n        # Test Case 3\n        {'n': 80, 'p': 4, 'seed': 2, 'penalty_type': 'mcp', 'lambda_': 0.0, 'shape_param': 3.0,\n         'b_star': np.array([1.0, -2.0, 0.0, 0.5]), 'X_gen_type': 'random', 'y_gen_val': 'b_star'},\n        # Test Case 4\n        {'n': 60, 'p': 2, 'seed': 3, 'penalty_type': 'mcp', 'lambda_': 1.2, 'shape_param': 1.5,\n         'b_star': None, 'X_gen_type': 'collinear', 'y_gen_val': 2.0}\n    ]\n\n    results = []\n    for case in test_cases:\n        norm_diff, loss_diff = run_case(**case)\n        results.append(f\"[{norm_diff:.10f},{loss_diff:.10f}]\")\n        \n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3153982"}, {"introduction": "许多强大的机器学习模型，例如使用核技巧的模型，其性能严重依赖于无法通过训练数据直接学习的超参数（hyperparameters）。本练习将指导您推导核岭回归（Kernel Ridge Regression）的闭式解，这是一个在再生核希尔伯特空间（RKHS）中进行线性回归的优雅框架。更重要的是，您将学习如何通过最小化留一法交叉验证（LOOCV）误差来自动选择最优的正则化参数 $\\lambda$，这是在实践中进行模型选择和调优的一项核心技能。 [@problem_id:3153909]", "problem": "考虑一个监督学习问题，其训练输入为 $\\{x_{i}\\}_{i=1}^{n}$，实值输出为 $y \\in \\mathbb{R}^{n}$。令 $k(\\cdot,\\cdot)$ 是一个正半定核，其Gram矩阵 $K \\in \\mathbb{R}^{n \\times n}$ 定义为 $K_{ij} = k(x_{i},x_{j})$。核岭回归通过最小化以下形式的正则化经验风险，在相关的再生核希尔伯特空间（RKHS）中拟合一个函数 $f$\n$$\n\\sum_{i=1}^{n} \\left( y_{i} - f(x_{i}) \\right)^{2} + \\lambda \\| f \\|_{\\mathcal{H}}^{2},\n$$\n其中 $\\lambda  0$ 是一个正则化参数，$\\| f \\|_{\\mathcal{H}}$ 是RKHS范数。根据表示定理，最小化器可以表示为 $f(\\cdot) = \\sum_{i=1}^{n} a_{i} k(x_{i}, \\cdot)$，其中系数 $a \\in \\mathbb{R}^{n}$。令 $\\hat{f} \\in \\mathbb{R}^{n}$ 表示在训练输入处的拟合值向量，即 $\\hat{f}_{i} = f(x_{i})$。\n\n1. 从正则化经验风险和表示形式 $f(\\cdot) = \\sum_{i=1}^{n} a_{i} k(x_{i}, \\cdot)$ 出发，推导最优系数向量 $\\hat{a}$ 关于 $K$、$\\lambda$ 和 $y$ 的闭式表达式，然后以闭式形式表示训练输入处的拟合值向量 $\\hat{f}$。\n\n2. 留一法交叉验证（LOOCV）通过在 $n-1$ 个点上重复训练模型并预测被留下的点来估计泛化误差。对于拟合值是 $y$ 的线性函数的方法，即 $\\hat{f} = H(\\lambda) y$（其中平滑矩阵 $H(\\lambda)$ 不依赖于 $y$），LOOCV均方误差可以根据残差和 $H(\\lambda)$ 的对角线元素来表示。考虑第1部分得到的核岭回归平滑矩阵 $H(\\lambda)$，对于 $n=2$ 的情况，当核矩阵为\n$$\nK = \\begin{pmatrix} 1  1 \\\\ 1  1 \\end{pmatrix}\n$$\n且输出为\n$$\ny = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}\n$$\n时，推导LOOCV均方误差作为 $\\lambda$ 的显式函数。\n\n3. 使用第2部分得到的表达式，确定使该数据集的LOOCV均方误差最小化的 $\\lambda \\geq 0$ 的值。请提供精确值，不要四舍五入；不涉及物理单位。\n\n4. 分析第2部分中数据集的LOOCV均方误差作为 $\\lambda$ 的函数的凸性。计算关于 $\\lambda$ 的二阶导数，并确定该函数为凸函数时 $\\lambda$ 的范围。从第一性原理证明你的结论。\n\n你最终报告的答案应该是使第3部分中指定数据集的LOOCV均方误差最小化的单个 $\\lambda$ 值。", "solution": "该问题是有效的，因为它在科学上基于统计学习理论，是适定的，并且提供了所有必要的信息。\n\n**第1部分：最优系数和拟合值的推导**\n\n目标是最小化核岭回归的正则化经验风险：\n$$L(f) = \\sum_{i=1}^{n} ( y_{i} - f(x_{i}) )^{2} + \\lambda \\| f \\|_{\\mathcal{H}}^{2}$$\n根据表示定理，最小化器 $f$ 的形式为 $f(\\cdot) = \\sum_{j=1}^{n} a_{j} k(x_{j}, \\cdot)$，其中系数向量 $a \\in \\mathbb{R}^{n}$。\n\n函数在训练输入 $x_i$ 处的值由下式给出：\n$$f(x_i) = \\sum_{j=1}^{n} a_j k(x_j, x_i) = \\sum_{j=1}^{n} K_{ij} a_j = (Ka)_i$$\n以向量形式，拟合值向量 $\\hat{f}$ 为 $\\hat{f} = Ka$。\n\nRKHS范数的平方 $\\| f \\|_{\\mathcal{H}}^{2}$ 为：\n$$\\| f \\|_{\\mathcal{H}}^{2} = \\left\\langle \\sum_{i=1}^{n} a_{i} k(x_{i}, \\cdot), \\sum_{j=1}^{n} a_{j} k(x_{j}, \\cdot) \\right\\rangle_{\\mathcal{H}} = \\sum_{i=1}^{n} \\sum_{j=1}^{n} a_{i} a_{j} \\langle k(x_{i}, \\cdot), k(x_{j}, \\cdot) \\rangle_{\\mathcal{H}}$$\n使用再生性质 $\\langle g, k(x, \\cdot) \\rangle_{\\mathcal{H}} = g(x)$，我们有 $\\langle k(x_i, \\cdot), k(x_j, \\cdot) \\rangle_{\\mathcal{H}} = k(x_i, x_j) = K_{ij}$。因此：\n$$\\| f \\|_{\\mathcal{H}}^{2} = \\sum_{i=1}^{n} \\sum_{j=1}^{n} a_{i} a_{j} K_{ij} = a^{T} K a$$\n\n将这些代入目标函数，我们得到一个关于系数向量 $a$ 的函数：\n$$L(a) = (y - Ka)^{T}(y - Ka) + \\lambda a^{T}Ka$$\n为了找到最优系数，我们计算 $L(a)$ 关于 $a$ 的梯度并将其设为零。\n$$L(a) = y^T y - 2y^T K a + a^T K^T K a + \\lambda a^T K a$$\n由于 $K$ 是一个Gram矩阵，所以它是对称的 ($K^T = K$)。\n$$L(a) = y^T y - 2(Ky)^T a + a^T(K^2 + \\lambda K)a$$\n关于 $a$ 的梯度是：\n$$\\nabla_a L(a) = -2Ky + 2(K^2 + \\lambda K)a$$\n将梯度设为零，得到最优性的一阶条件：\n$$(K^2 + \\lambda K)a = Ky \\implies K(K + \\lambda I)a = Ky$$\n其中 $I$ 是 $n \\times n$ 的单位矩阵。对于 $\\lambda  0$，矩阵 $K + \\lambda I$ 是严格正定的，因此是可逆的。然而，如果 $K$是奇异的，这个方程不能唯一确定 $a$。任何在 $K$ 的零空间中的向量都可以加到一个 $a$ 的解上，而不会改变左侧或预测值 $\\hat{f}=Ka$。\n\n尽管 $a$ 不唯一，但拟合值向量 $\\hat{f}$ 是唯一的。设 $v$ 是 $K (K+\\lambda I)$ 零空间中的任意向量。由于 $K+\\lambda I$ 是可逆的，这等价于 $K$ 的零空间。那么 $\\hat{f} = K(a+v) = Ka+Kv = Ka$。预测值不变。我们可以通过将一阶条件左乘 $K(K+\\lambda I)^{-1}$ 来推导 $\\hat{f}$ 的唯一表达式：\n$$K(K+\\lambda I)^{-1} [K(K+\\lambda I)a] = K(K+\\lambda I)^{-1}[Ky]$$\n这可以简化为：\n$$K a = K(K+\\lambda I)^{-1}y$$\n因此，拟合值的闭式表达式是：\n$$\\hat{f} = K(K + \\lambda I)^{-1}y$$\n一个能够产生正确拟合值的系数向量 $\\hat{a}$ 的标准选择是：\n$$\\hat{a} = (K + \\lambda I)^{-1}y$$\n这个特定的 $\\hat{a}$ 选择是最小化一个略有不同但相关的目标函数时会得到的解，并且是文献中报告的标准解。\n\n**第2部分：特定情况下的LOOCV均方误差**\n\n对于拟合值由 $\\hat{f} = H(\\lambda)y$ 给出的线性平滑器，留一法交叉验证（LOOCV）均方误差为：\n$$LOOCV(\\lambda) = \\frac{1}{n} \\sum_{i=1}^{n} \\left( \\frac{y_i - \\hat{f}_i}{1 - H_{ii}(\\lambda)} \\right)^2$$\n从第1部分可知，平滑矩阵为 $H(\\lambda) = K(K+\\lambda I)^{-1}$。给定 $n=2$， $y = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}$，以及 $K = \\begin{pmatrix} 1  1 \\\\ 1  1 \\end{pmatrix}$。\n\n首先，我们计算 $(K+\\lambda I)^{-1}$：\n$$K + \\lambda I = \\begin{pmatrix} 1+\\lambda  1 \\\\ 1  1+\\lambda \\end{pmatrix}$$\n行列式为 $\\det(K+\\lambda I) = (1+\\lambda)^2 - 1 = 1 + 2\\lambda + \\lambda^2 - 1 = \\lambda^2 + 2\\lambda = \\lambda(\\lambda+2)$。\n逆矩阵为：\n$$(K+\\lambda I)^{-1} = \\frac{1}{\\lambda(\\lambda+2)} \\begin{pmatrix} 1+\\lambda  -1 \\\\ -1  1+\\lambda \\end{pmatrix}$$\n现在，我们计算平滑矩阵 $H(\\lambda)$：\n$$H(\\lambda) = K(K+\\lambda I)^{-1} = \\begin{pmatrix} 1  1 \\\\ 1  1 \\end{pmatrix} \\frac{1}{\\lambda(\\lambda+2)} \\begin{pmatrix} 1+\\lambda  -1 \\\\ -1  1+\\lambda \\end{pmatrix}$$\n$$H(\\lambda) = \\frac{1}{\\lambda(\\lambda+2)} \\begin{pmatrix} (1+\\lambda)-1  -1+(1+\\lambda) \\\\ (1+\\lambda)-1  -1+(1+\\lambda) \\end{pmatrix} = \\frac{1}{\\lambda(\\lambda+2)} \\begin{pmatrix} \\lambda  \\lambda \\\\ \\lambda  \\lambda \\end{pmatrix} = \\frac{1}{\\lambda+2} \\begin{pmatrix} 1  1 \\\\ 1  1 \\end{pmatrix}$$\n对角线元素为 $H_{11} = H_{22} = \\frac{1}{\\lambda+2}$。\n\n接下来，我们计算拟合值 $\\hat{f}$ 和残差 $r = y - \\hat{f}$：\n$$\\hat{f} = H(\\lambda)y = \\frac{1}{\\lambda+2} \\begin{pmatrix} 1  1 \\\\ 1  1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix} = \\frac{1}{\\lambda+2} \\begin{pmatrix} 3 \\\\ 3 \\end{pmatrix}$$\n残差为：\n$$r_1 = y_1 - \\hat{f}_1 = 1 - \\frac{3}{\\lambda+2} = \\frac{\\lambda+2-3}{\\lambda+2} = \\frac{\\lambda-1}{\\lambda+2}$$\n$$r_2 = y_2 - \\hat{f}_2 = 2 - \\frac{3}{\\lambda+2} = \\frac{2(\\lambda+2)-3}{\\lambda+2} = \\frac{2\\lambda+1}{\\lambda+2}$$\n项 $1-H_{ii}$ 为 $1 - \\frac{1}{\\lambda+2} = \\frac{\\lambda+1}{\\lambda+2}$。\n\n现在我们可以写出LOOCV误差：\n$$LOOCV(\\lambda) = \\frac{1}{2} \\left[ \\left( \\frac{r_1}{1-H_{11}} \\right)^2 + \\left( \\frac{r_2}{1-H_{22}} \\right)^2 \\right]$$\n$$LOOCV(\\lambda) = \\frac{1}{2} \\left[ \\left( \\frac{(\\lambda-1)/(\\lambda+2)}{(\\lambda+1)/(\\lambda+2)} \\right)^2 + \\left( \\frac{(2\\lambda+1)/(\\lambda+2)}{(\\lambda+1)/(\\lambda+2)} \\right)^2 \\right] = \\frac{1}{2} \\left[ \\left( \\frac{\\lambda-1}{\\lambda+1} \\right)^2 + \\left( \\frac{2\\lambda+1}{\\lambda+1} \\right)^2 \\right]$$\n$$LOOCV(\\lambda) = \\frac{1}{2(\\lambda+1)^2} \\left[ (\\lambda-1)^2 + (2\\lambda+1)^2 \\right]$$\n$$LOOCV(\\lambda) = \\frac{1}{2(\\lambda+1)^2} \\left[ (\\lambda^2-2\\lambda+1) + (4\\lambda^2+4\\lambda+1) \\right] = \\frac{5\\lambda^2+2\\lambda+2}{2(\\lambda+1)^2}$$\n\n**第3部分：最小化LOOCV均方误差**\n\n为了找到使 $LOOCV(\\lambda)$ 最小化的 $\\lambda \\ge 0$ 的值，我们计算其关于 $\\lambda$ 的导数并将其设为零。\n令 $L(\\lambda) = LOOCV(\\lambda) = \\frac{5\\lambda^2+2\\lambda+2}{2(\\lambda+1)^2}$。使用商法则：\n$$L'(\\lambda) = \\frac{(10\\lambda+2) \\cdot 2(\\lambda+1)^2 - (5\\lambda^2+2\\lambda+2) \\cdot 4(\\lambda+1)}{4(\\lambda+1)^4}$$\n分子和分母同时除以 $2(\\lambda+1)$ (对于 $\\lambda \\neq -1$):\n$$L'(\\lambda) = \\frac{(10\\lambda+2)(\\lambda+1) - 2(5\\lambda^2+2\\lambda+2)}{2(\\lambda+1)^3} = \\frac{(10\\lambda^2+12\\lambda+2) - (10\\lambda^2+4\\lambda+4)}{2(\\lambda+1)^3}$$\n$$L'(\\lambda) = \\frac{8\\lambda-2}{2(\\lambda+1)^3} = \\frac{4\\lambda-1}{(\\lambda+1)^3}$$\n将导数设为零：\n$$L'(\\lambda) = 0 \\implies 4\\lambda-1=0 \\implies \\lambda=\\frac{1}{4}$$\n由于我们考虑 $\\lambda \\ge 0$，分母 $(\\lambda+1)^3$ 是正的。 $L'(\\lambda)$ 的符号由 $4\\lambda-1$ 决定。对于 $0 \\le \\lambda  1/4$，$L'(\\lambda)  0$，所以 $L(\\lambda)$ 是递减的。对于 $\\lambda > 1/4$，$L'(\\lambda) > 0$，所以 $L(\\lambda)$ 是递增的。因此，$\\lambda=1/4$ 是 $\\lambda \\ge 0$ 时的全局最小值。\n\n**第4部分：凸性分析**\n\n为了分析 $L(\\lambda)$ 的凸性，我们计算其二阶导数 $L''(\\lambda)$。\n$$L'(\\lambda) = \\frac{4\\lambda-1}{(\\lambda+1)^3}$$\n再次使用商法则：\n$$L''(\\lambda) = \\frac{4 \\cdot (\\lambda+1)^3 - (4\\lambda-1) \\cdot 3(\\lambda+1)^2}{((\\lambda+1)^3)^2} = \\frac{4(\\lambda+1) - 3(4\\lambda-1)}{(\\lambda+1)^4}$$\n$$L''(\\lambda) = \\frac{4\\lambda+4 - 12\\lambda+3}{(\\lambda+1)^4} = \\frac{7-8\\lambda}{(\\lambda+1)^4}$$\n当一个函数的二阶导数非负时，该函数是凸的。对于 $\\lambda \\ge 0$，分母 $(\\lambda+1)^4$ 是正的。因此，$L''(\\lambda)$ 的符号由分子 $7-8\\lambda$ 的符号决定。\n$$L''(\\lambda) \\ge 0 \\iff 7-8\\lambda \\ge 0 \\iff 7 \\ge 8\\lambda \\iff \\lambda \\le \\frac{7}{8}$$\nLOOCV均方误差函数 $L(\\lambda)$ 在区间 $[0, 7/8]$ 上是凸的。我们的解 $\\lambda=1/4$ 位于这个区间内，这符合最小值的预期。对于 $\\lambda > 7/8$，该函数是凹的。\n问题要求的是最小化LOOCV均方误差的 $\\lambda$ 值，这已在第3部分中找到。", "answer": "$$\\boxed{\\frac{1}{4}}$$", "id": "3153909"}]}