{"hands_on_practices": [{"introduction": "我们从偏差-方差权衡最经典的例证开始：比较普通最小二乘法 (OLS) 与岭回归。这项练习要求您构建一个具有高度共线性的场景，从而亲眼见证少量由正则化引入的偏差如何能够大幅降低估计量的方差。理解这一点是领会为何像岭回归和 LASSO 这样的方法在现代机器学习中如此强大的关键。[@problem_id:3118692]", "problem": "考虑一个具有两个预测变量的固定设计线性回归模型，其中响应向量根据 $y = X \\beta + \\varepsilon$ 生成。这里 $X \\in \\mathbb{R}^{n \\times 2}$ 是一个非随机的设计矩阵，$\\beta \\in \\mathbb{R}^{2}$ 是真实的系数向量，$\\varepsilon \\in \\mathbb{R}^{n}$ 是一个噪声向量，其分量独立且满足 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2} I_{n})$。将普通最小二乘（OLS）估计量和岭估计量定义为\n$$\\hat{\\beta}_{\\mathrm{OLS}} = (X^{\\top} X)^{-1} X^{\\top} y,$$\n$$\\hat{\\beta}_{\\mathrm{ridge}}(\\lambda) = (X^{\\top} X + \\lambda I_{2})^{-1} X^{\\top} y,$$\n其中 $\\lambda \\ge 0$ 是岭惩罚参数，而 $I_{2}$ 是 $2 \\times 2$ 的单位矩阵。使用以下基本定义和事实：\n- 参数 $\\theta$ 的估计量 $\\hat{\\theta}$ 的偏差为 $\\mathrm{Bias}(\\hat{\\theta}) = \\mathbb{E}[\\hat{\\theta}] - \\theta$。\n- 在 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2} I_{n})$ 条件下，线性估计量 $A y$ 的方差为 $\\mathrm{Var}(A y) = \\sigma^{2} A A^{\\top}$。\n- 对于 OLS 估计量，$\\mathbb{E}[\\hat{\\beta}_{\\mathrm{OLS}}] = \\beta$ 且 $\\mathrm{Var}(\\hat{\\beta}_{\\mathrm{OLS}}) = \\sigma^{2} (X^{\\top} X)^{-1}$。\n- 对于岭估计量，$\\mathbb{E}[\\hat{\\beta}_{\\mathrm{ridge}}(\\lambda)] = (X^{\\top} X + \\lambda I_{2})^{-1} X^{\\top} X \\beta$ 且 $\\mathrm{Var}(\\hat{\\beta}_{\\mathrm{ridge}}(\\lambda)) = \\sigma^{2} (X^{\\top} X + \\lambda I_{2})^{-1} X^{\\top} X (X^{\\top} X + \\lambda I_{2})^{-1}$。\n\n你的任务是构建一个确定性的共线性设计 $X$ 家族，以分离共线性对估计量方差的影响，然后量化 OLS 与岭估计的偏差和方差。具体而言：\n\n1. 构造 $X$ 使得格拉姆矩阵 $G = X^{\\top} X$ 具有特征分解 $G = V \\mathrm{diag}(s_{1}, s_{2}) V^{\\top}$，其中标准正交特征向量为 $v_{1} = \\frac{1}{\\sqrt{2}}(1, 1)^{\\top}$ 和 $v_{2} = \\frac{1}{\\sqrt{2}}(1, -1)^{\\top}$，对于一个给定的相关水平 $\\rho \\in [0, 1)$，特征值为 $s_{1} = n(1+\\rho)$ 和 $s_{2} = n(1-\\rho)$。这确保了当 $\\rho$ 接近 $1$ 时存在共线性。使用偶数 $n$，并通过 $\\mathbb{R}^{n}$ 中的标准正交列来构造 $X$，使得 $X^{\\top} X$ 精确匹配指定的特征结构。取真实系数向量 $\\beta = (1, 1)^{\\top}$ 和噪声方差 $\\sigma^{2} = 1$。\n\n2. 对于一个给定的 $(n, \\rho, \\lambda)$ 三元组，计算：\n   - 岭偏差向量的欧几里得范数平方，$\\|\\mathrm{Bias}(\\hat{\\beta}_{\\mathrm{ridge}}(\\lambda))\\|_{2}^{2}$，其中\n     $$\\mathrm{Bias}(\\hat{\\beta}_{\\mathrm{ridge}}(\\lambda)) = \\left((X^{\\top} X + \\lambda I_{2})^{-1} X^{\\top} X - I_{2}\\right)\\beta.$$\n   - 岭方差的迹与 OLS 方差的迹之比：\n     $$R_{\\mathrm{var}}(X, \\lambda) = \\frac{\\mathrm{tr}\\left(\\mathrm{Var}(\\hat{\\beta}_{\\mathrm{ridge}}(\\lambda))\\right)}{\\mathrm{tr}\\left(\\mathrm{Var}(\\hat{\\beta}_{\\mathrm{OLS}})\\right)} = \\frac{\\mathrm{tr}\\left(\\sigma^{2} (X^{\\top} X + \\lambda I_{2})^{-1} X^{\\top} X (X^{\\top} X + \\lambda I_{2})^{-1}\\right)}{\\mathrm{tr}\\left(\\sigma^{2} (X^{\\top} X)^{-1}\\right)}.$$\n\n3. 设计并评估以下 $(n, \\rho, \\lambda)$ 测试套件以探究不同的情景：\n   - 情况 A（高共线性，偏差接近于零的方差缩减）：$n = 1000$, $\\rho = 0.999$, $\\lambda = 1$。\n   - 情况 B（边界情况，无正则化）：$n = 1000$, $\\rho = 0.999$, $\\lambda = 0$。\n   - 情况 C（中等共线性）：$n = 500$, $\\rho = 0.9$, $\\lambda = 1$。\n   - 情况 D（低共线性）：$n = 500$, $\\rho = 0.0, \\lambda = 1$。\n\n4. 你的程序必须是一个单一、完整的脚本，它：\n   - 为每个测试用例精确地构造项目 1 中指定的 $X$。\n   - 为每个测试用例计算项目 2 中的量。\n   - 生成一行输出，包含一个用方括号括起来的逗号分隔列表，其顺序为 $[\\|\\mathrm{Bias}\\|^{2}_{\\mathrm{A}}, R_{\\mathrm{var},\\mathrm{A}}, \\|\\mathrm{Bias}\\|^{2}_{\\mathrm{B}}, R_{\\mathrm{var},\\mathrm{B}}, \\|\\mathrm{Bias}\\|^{2}_{\\mathrm{C}}, R_{\\mathrm{var},\\mathrm{C}}, \\|\\mathrm{Bias}\\|^{2}_{\\mathrm{D}}, R_{\\mathrm{var},\\mathrm{D}}]$。\n\n所有量都是没有物理单位的纯数。不涉及角度。确保所有矩阵计算都使用指定的公式进行。输出必须是可复现的，并且除了上述 $X$ 的确定性构造外，不得依赖任何外部随机性。", "solution": "该问题定义明确、有科学依据且内部一致。它提供了一整套定义、参数和约束，以推导出一个唯一的、确定性的解。所有待计算的量都可以表示为格拉姆矩阵 $G = X^{\\top} X$ 的函数，该矩阵的完整特征分解已给出。“构造 $X$”的指令是一个概念上的手段，以确保这样的矩阵存在，这由奇异值分解定理保证。然而，由于最终的量只依赖于 $G=X^\\top X$，我们可以直接使用其指定的属性进行计算，从而绕过对 $n \\times 2$ 矩阵 $X$ 的显式构造。\n\n我们的步骤如下：\n1.  推导岭偏差向量的欧几里得范数平方 $\\|\\mathrm{Bias}(\\hat{\\beta}_{\\mathrm{ridge}}(\\lambda))\\|_{2}^{2}$ 的解析公式。\n2.  推导方差比 $R_{\\mathrm{var}}(X, \\lambda)$ 的解析公式。\n3.  将这些公式应用于四个指定的测试用例。\n\n格拉姆矩阵定义为 $G = X^{\\top} X = V \\Lambda V^{\\top}$，其中 $\\Lambda = \\mathrm{diag}(s_{1}, s_{2})$ 是特征值的对角矩阵，而 $V = [v_1, v_2]$ 是相应特征向量的正交矩阵。\n特征值由 $s_{1} = n(1+\\rho)$ 和 $s_{2} = n(1-\\rho)$ 给出。\n特征向量为 $v_{1} = \\frac{1}{\\sqrt{2}}(1, 1)^{\\top}$ 和 $v_{2} = \\frac{1}{\\sqrt{2}}(1, -1)^{\\top}$，构成的矩阵为 $V = \\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1  1 \\\\ 1  -1 \\end{pmatrix}$。\n真实系数向量为 $\\beta = (1, 1)^{\\top}$，噪声方差为 $\\sigma^2 = 1$。\n\n**1. 偏差范数平方的推导**\n\n岭估计量的偏差由下式给出：\n$$ \\mathrm{Bias}(\\hat{\\beta}_{\\mathrm{ridge}}(\\lambda)) = \\mathbb{E}[\\hat{\\beta}_{\\mathrm{ridge}}(\\lambda)] - \\beta = \\left( (X^{\\top} X + \\lambda I_{2})^{-1} X^{\\top} X - I_{2} \\right) \\beta $$\n代入 $G = X^{\\top} X = V \\Lambda V^{\\top}$ 和 $I_{2} = V V^{\\top}$：\n$$ \\mathrm{Bias}(\\cdot) = \\left( (V \\Lambda V^{\\top} + \\lambda V V^{\\top})^{-1} V \\Lambda V^{\\top} - V V^{\\top} \\right) \\beta $$\n$$ = \\left( (V (\\Lambda + \\lambda I_{2}) V^{\\top})^{-1} V \\Lambda V^{\\top} - V V^{\\top} \\right) \\beta $$\n$$ = \\left( V (\\Lambda + \\lambda I_{2})^{-1} V^{\\top} V \\Lambda V^{\\top} - V V^{\\top} \\right) \\beta $$\n使用 $V^{\\top}V = I_{2}$：\n$$ = \\left( V (\\Lambda + \\lambda I_{2})^{-1} \\Lambda V^{\\top} - V V^{\\top} \\right) \\beta = V \\left( (\\Lambda + \\lambda I_{2})^{-1} \\Lambda - I_{2} \\right) V^{\\top} \\beta $$\n中间的矩阵项简化为：\n$$ (\\Lambda + \\lambda I_{2})^{-1} \\Lambda - I_{2} = \\begin{pmatrix} \\frac{s_1}{s_1+\\lambda}  0 \\\\ 0  \\frac{s_2}{s_2+\\lambda} \\end{pmatrix} - \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix} = \\begin{pmatrix} -\\frac{\\lambda}{s_1+\\lambda}  0 \\\\ 0  -\\frac{\\lambda}{s_2+\\lambda} \\end{pmatrix} = -\\lambda(\\Lambda+\\lambda I_{2})^{-1} $$\n因此，偏差向量为 $\\mathrm{Bias}(\\cdot) = -\\lambda V (\\Lambda+\\lambda I_{2})^{-1} V^{\\top} \\beta$。\n为了求其范数平方 $\\|\\mathrm{Bias}(\\cdot)\\|_{2}^{2}$，我们首先将 $\\beta$ 变换到特征向量基上：\n$$ V^{\\top}\\beta = \\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1  1 \\\\ 1  -1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\frac{1}{\\sqrt{2}}\\begin{pmatrix} 2 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} \\sqrt{2} \\\\ 0 \\end{pmatrix} $$\n这表明 $\\beta$ 与第一个特征向量 $v_1$ 对齐，因为 $\\beta = \\sqrt{2}v_1$。\n由于 $V$ 是一个正交矩阵，它保持欧几里得范数不变。因此，我们有：\n$$ \\|\\mathrm{Bias}(\\cdot)\\|_{2}^{2} = \\|-\\lambda V (\\Lambda+\\lambda I_{2})^{-1} V^{\\top} \\beta\\|_{2}^{2} = \\|-\\lambda (\\Lambda+\\lambda I_{2})^{-1} V^{\\top} \\beta\\|_{2}^{2} $$\n$$ = \\left\\| -\\lambda \\begin{pmatrix} \\frac{1}{s_1+\\lambda}  0 \\\\ 0  \\frac{1}{s_2+\\lambda} \\end{pmatrix} \\begin{pmatrix} \\sqrt{2} \\\\ 0 \\end{pmatrix} \\right\\|_{2}^{2} = \\left\\| \\begin{pmatrix} -\\frac{\\lambda\\sqrt{2}}{s_1+\\lambda} \\\\ 0 \\end{pmatrix} \\right\\|_{2}^{2} = \\left(-\\frac{\\lambda\\sqrt{2}}{s_1+\\lambda}\\right)^2 $$\n这就得到了偏差范数平方的最终公式：\n$$ \\|\\mathrm{Bias}(\\hat{\\beta}_{\\mathrm{ridge}}(\\lambda))\\|_{2}^{2} = \\frac{2\\lambda^2}{(s_1+\\lambda)^2} $$\n\n**2. 方差比的推导**\n\n方差比为 $R_{\\mathrm{var}} = \\frac{\\mathrm{tr}(\\mathrm{Var}(\\hat{\\beta}_{\\mathrm{ridge}}))}{\\mathrm{tr}(\\mathrm{Var}(\\hat{\\beta}_{\\mathrm{OLS}}))}$。给定 $\\sigma^2=1$，我们分别分析分子和分母。\n\n分母，OLS 方差的迹：\n$$ \\mathrm{tr}(\\mathrm{Var}(\\hat{\\beta}_{\\mathrm{OLS}})) = \\mathrm{tr}(\\sigma^2 (X^{\\top}X)^{-1}) = \\mathrm{tr}(G^{-1}) = \\mathrm{tr}((V\\Lambda V^\\top)^{-1}) = \\mathrm{tr}(V\\Lambda^{-1}V^\\top) $$\n利用迹的循环性质，$\\mathrm{tr}(ABC) = \\mathrm{tr}(CAB)$：\n$$ \\mathrm{tr}(V\\Lambda^{-1}V^\\top) = \\mathrm{tr}(V^\\top V\\Lambda^{-1}) = \\mathrm{tr}(\\Lambda^{-1}) = \\frac{1}{s_1} + \\frac{1}{s_2} $$\n\n分子，岭方差的迹：\n$$ \\mathrm{tr}(\\mathrm{Var}(\\hat{\\beta}_{\\mathrm{ridge}})) = \\mathrm{tr}(\\sigma^2 (G+\\lambda I)^{-1} G (G+\\lambda I)^{-1}) $$\n$$ = \\mathrm{tr}( (V(\\Lambda+\\lambda I)V^\\top)^{-1} (V\\Lambda V^\\top) (V(\\Lambda+\\lambda I)V^\\top)^{-1} ) $$\n$$ = \\mathrm{tr}( V(\\Lambda+\\lambda I)^{-1} V^{\\top} V\\Lambda V^{\\top} V(\\Lambda+\\lambda I)^{-1} V^{\\top} ) $$\n$$ = \\mathrm{tr}( V (\\Lambda+\\lambda I)^{-1} \\Lambda (\\Lambda+\\lambda I)^{-1} V^{\\top} ) $$\n再次利用迹的循环性质：\n$$ = \\mathrm{tr}( (\\Lambda+\\lambda I)^{-1} \\Lambda (\\Lambda+\\lambda I)^{-1} ) = \\mathrm{tr}\\left( \\begin{pmatrix} \\frac{s_1}{(s_1+\\lambda)^2}  0 \\\\ 0  \\frac{s_2}{(s_2+\\lambda)^2} \\end{pmatrix} \\right) = \\frac{s_1}{(s_1+\\lambda)^2} + \\frac{s_2}{(s_2+\\lambda)^2} $$\n\n结合分子和分母，得到方差比的表达式：\n$$ R_{\\mathrm{var}}(X, \\lambda) = \\frac{\\frac{s_1}{(s_1+\\lambda)^2} + \\frac{s_2}{(s_2+\\lambda)^2}}{\\frac{1}{s_1} + \\frac{1}{s_2}} $$\n\n**3. 测试用例的计算**\n\n现在我们将这些公式应用于给定的测试用例，代入 $s_1 = n(1+\\rho)$ 和 $s_2 = n(1-\\rho)$。\n\n**情况 A**：$(n, \\rho, \\lambda) = (1000, 0.999, 1)$。\n$s_1 = 1000(1.999) = 1999$, $s_2 = 1000(0.001) = 1$。\n$\\|\\mathrm{Bias}\\|^2 = \\frac{2(1^2)}{(1999+1)^2} = \\frac{2}{2000^2} = 5 \\times 10^{-7}$。\n$R_{\\mathrm{var}} = \\frac{1999/(1999+1)^2 + 1/(1+1)^2}{1/1999 + 1/1} = \\frac{1999/4000000 + 1/4}{2000/1999} \\approx 0.25037$。\n\n**情况 B**：$(n, \\rho, \\lambda) = (1000, 0.999, 0)$。\n这对应于 OLS 估计量。\n$\\|\\mathrm{Bias}\\|^2 = \\frac{2(0^2)}{(1999+0)^2} = 0$，因为 OLS 是无偏的。\n$R_{\\mathrm{var}} = \\frac{\\mathrm{tr}(\\mathrm{Var}(\\hat{\\beta}_{\\mathrm{OLS}}))}{\\mathrm{tr}(\\mathrm{Var}(\\hat{\\beta}_{\\mathrm{OLS}}))} = 1$。\n\n**情况 C**：$(n, \\rho, \\lambda) = (500, 0.9, 1)$。\n$s_1 = 500(1.9) = 950$, $s_2 = 500(0.1) = 50$。\n$\\|\\mathrm{Bias}\\|^2 = \\frac{2(1^2)}{(950+1)^2} = \\frac{2}{951^2} \\approx 2.211 \\times 10^{-6}$。\n$R_{\\mathrm{var}} = \\frac{950/(950+1)^2 + 50/(50+1)^2}{1/950 + 1/50} \\approx 0.9630$。\n\n**情况 D**：$(n, \\rho, \\lambda) = (500, 0.0, 1)$。\n$s_1 = 500(1) = 500$, $s_2 = 500(1) = 500$。\n$\\|\\mathrm{Bias}\\|^2 = \\frac{2(1^2)}{(500+1)^2} = \\frac{2}{501^2} \\approx 7.968 \\times 10^{-6}$。\n$R_{\\mathrm{var}} = \\frac{500/(500+1)^2 + 500/(500+1)^2}{1/500 + 1/500} = \\frac{2 \\cdot 500/501^2}{2/500} = \\frac{500^2}{501^2} \\approx 0.9960$。\n\n实现将编码这些推导出的解析公式。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the squared bias norm and variance ratio for OLS vs. Ridge estimators\n    under different scenarios of collinearity.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (n, rho, lambda), description\n        (1000, 0.999, 1), # Case A: high collinearity, variance reduction\n        (1000, 0.999, 0), # Case B: boundary case, no regularization (OLS)\n        (500, 0.9, 1),    # Case C: moderate collinearity\n        (500, 0.0, 1),    # Case D: low collinearity (orthogonal design)\n    ]\n\n    results = []\n    for n, rho, lmbda in test_cases:\n        # The problem is structured such that all calculations depend only on the\n        # eigenvalues of the Gram matrix G = X'X, not on X itself.\n        # We calculate these eigenvalues directly.\n        s1 = float(n) * (1.0 + rho)\n        s2 = float(n) * (1.0 - rho)\n\n        # 1. Compute the squared Euclidean norm of the ridge bias vector.\n        # The analytical formula derived in the solution is:\n        # ||Bias||^2 = 2 * lambda^2 / (s1 + lambda)^2\n        # This simplification arises because the true beta vector is aligned with\n        # the first principal component of the design.\n        sq_bias_norm = 2.0 * lmbda**2 / (s1 + lmbda)**2\n\n        # 2. Compute the ratio of the trace of the ridge variance to the trace of OLS variance.\n        # The analytical formula derived in the solution is:\n        # R_var = (s1/(s1+l)^2 + s2/(s2+l)^2) / (1/s1 + 1/s2)\n        if lmbda == 0:\n            # For OLS (lambda=0), the ridge estimator is the OLS estimator,\n            # so the ratio of their variance traces is exactly 1.\n            r_var = 1.0\n        else:\n            # Denominator: Trace of OLS variance (scaled by sigma^2 = 1)\n            # The problem constraint rho in [0, 1) ensures s1 > 0 and s2 >= 0.\n            # If rho is close to 1, s2 is close to 0, which is the source of high variance.\n            # If rho  1, s2 > 0, so ols_var_trace is finite.\n            ols_var_trace = (1.0 / s1) + (1.0 / s2)\n\n            # Numerator: Trace of Ridge variance (scaled by sigma^2 = 1)\n            ridge_var_trace = s1 / (s1 + lmbda)**2 + s2 / (s2 + lmbda)**2\n\n            r_var = ridge_var_trace / ols_var_trace\n                \n        results.append(sq_bias_norm)\n        results.append(r_var)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```", "id": "3118692"}, {"introduction": "接下来，我们从贝叶斯的视角来探讨偏差-方差权衡。这个问题引入了分层模型，在其中我们可以跨越相关组别“借力” (borrowing strength)。您将推导出部分汇集 (partial pooling) 或称收缩 (shrinkage) 的估计量，并发现它虽然向着一个共同的均值有偏，但其方差低于独立估计每个组均值的方法。这个练习揭示了贝叶斯收缩与频率派正则化之间的深刻联系。[@problem_id:3118672]", "problem": "一位研究人员正在使用层次正态-正态模型对分组数据进行建模。对于组 $g$，他们观测到 $n_{g}$ 个样本 $y_{g1}, y_{g2}, \\dots, y_{g n_{g}}$，这些样本遵循数据生成过程 $y_{gi} \\mid \\theta_{g} \\sim \\mathcal{N}(\\theta_{g}, \\sigma^{2})$，且特定于组的参数遵循先验分布 $\\theta_{g} \\sim \\mathcal{N}(\\mu, \\tau^{2})$，其中 $\\mu$、$\\sigma^{2}$ 和 $\\tau^{2}$ 是已知常数，且 $\\sigma^{2} > 0$ 和 $\\tau^{2} > 0$。假设在以 $\\theta_{g}$ 为条件的每个组内，样本是独立同分布的 (i.i.d.)。\n\n考虑组均值 $\\theta_{g}$ 的两种估计量：\n- 无池化 (No pooling)：$\\hat{\\theta}_{g}^{\\mathrm{NP}} = \\bar{y}_{g}$，其中 $\\bar{y}_{g} = \\frac{1}{n_{g}} \\sum_{i=1}^{n_{g}} y_{gi}$。\n- 部分池化 (Partial pooling)：在层次先验和高斯似然下，最小化后验期望平方损失的估计量。\n\n使用估计量的偏差、方差和一致性的核心定义，并从高斯似然和先验出发，执行以下操作：\n- 从贝叶斯准则推导出部分池化估计量的闭式解，并计算其条件偏差 $E(\\hat{\\theta}_{g} \\mid \\theta_{g}) - \\theta_{g}$ 和条件方差 $\\mathrm{Var}(\\hat{\\theta}_{g} \\mid \\theta_{g})$。\n- 计算无池化估计量的条件偏差和方差。\n- 根据您推导出的估计量公式，解释部分池化如何相对于无池化减少方差，并分析当 $n_{g} \\to \\infty$ 时两种估计量的一致性（依概率收敛）。\n\n最后，将估计量 $\\hat{\\theta}_{g}$ 的条件均方误差 (MSE) 定义为 $\\mathrm{MSE}(\\hat{\\theta}_{g} \\mid \\theta_{g}) = E\\!\\left[(\\hat{\\theta}_{g} - \\theta_{g})^{2} \\mid \\theta_{g}\\right]$。推导、简化并以单一闭式解析表达式的形式，给出完全用 $n_{g}$、$\\sigma^{2}$、$\\tau^{2}$、$\\mu$ 和 $\\theta_{g}$ 表示的差值\n$$\\Delta = \\mathrm{MSE}(\\hat{\\theta}_{g}^{\\mathrm{PP}} \\mid \\theta_{g}) - \\mathrm{MSE}(\\hat{\\theta}_{g}^{\\mathrm{NP}} \\mid \\theta_{g})$$\n您的最终答案必须是 $\\Delta$ 的这个精确符号表达式。", "solution": "问题陈述经过严格验证，确认有效。这是一个在贝叶斯统计学中提法得当、有科学依据的问题，没有不一致、模糊不清或事实错误之处。推导所请求的量所需的所有信息均已提供。\n\n我们首先分析组均值 $\\theta_g$ 的两种估计量。除非另有说明，所有期望 $E[\\cdot]$ 和方差 $\\mathrm{Var}(\\cdot)$ 都是以 $\\theta_g$ 的真实值为条件的。\n\n**无池化估计量：$\\hat{\\theta}_{g}^{\\mathrm{NP}}$**\n\n无池化估计量定义为组 $g$ 的样本均值：$\\hat{\\theta}_{g}^{\\mathrm{NP}} = \\bar{y}_{g} = \\frac{1}{n_{g}} \\sum_{i=1}^{n_{g}} y_{gi}$。数据 $y_{gi}$ 从 $y_{gi} \\mid \\theta_{g} \\sim \\mathcal{N}(\\theta_{g}, \\sigma^{2})$ 中抽取。由于在以 $\\theta_g$ 为条件下样本是独立同分布的 (i.i.d.)，样本均值 $\\bar{y}_{g}$ 的抽样分布也是正态的。\n\n$\\bar{y}_{g}$ 的条件期望是：\n$$E[\\bar{y}_{g} \\mid \\theta_{g}] = E\\left[\\frac{1}{n_{g}} \\sum_{i=1}^{n_{g}} y_{gi} \\mid \\theta_{g}\\right] = \\frac{1}{n_{g}} \\sum_{i=1}^{n_{g}} E[y_{gi} \\mid \\theta_{g}] = \\frac{1}{n_{g}} \\sum_{i=1}^{n_{g}} \\theta_{g} = \\theta_{g}$$\n$\\bar{y}_{g}$ 的条件方差是：\n$$\\mathrm{Var}(\\bar{y}_{g} \\mid \\theta_{g}) = \\mathrm{Var}\\left(\\frac{1}{n_{g}} \\sum_{i=1}^{n_{g}} y_{gi} \\mid \\theta_{g}\\right) = \\frac{1}{n_{g}^{2}} \\sum_{i=1}^{n_{g}} \\mathrm{Var}(y_{gi} \\mid \\theta_{g}) = \\frac{1}{n_{g}^{2}} (n_{g}\\sigma^{2}) = \\frac{\\sigma^{2}}{n_{g}}$$\n因此，抽样分布为 $\\bar{y}_{g} \\mid \\theta_{g} \\sim \\mathcal{N}(\\theta_{g}, \\frac{\\sigma^{2}}{n_{g}})$。\n\n$\\hat{\\theta}_{g}^{\\mathrm{NP}}$ 的条件偏差是：\n$$E[\\hat{\\theta}_{g}^{\\mathrm{NP}} \\mid \\theta_{g}] - \\theta_{g} = E[\\bar{y}_{g} \\mid \\theta_{g}] - \\theta_{g} = \\theta_{g} - \\theta_{g} = 0$$\n无池化估计量是条件无偏的。\n\n$\\hat{\\theta}_{g}^{\\mathrm{NP}}$ 的条件方差是：\n$$\\mathrm{Var}(\\hat{\\theta}_{g}^{\\mathrm{NP}} \\mid \\theta_{g}) = \\mathrm{Var}(\\bar{y}_{g} \\mid \\theta_{g}) = \\frac{\\sigma^{2}}{n_{g}}$$\n\n**部分池化估计量：$\\hat{\\theta}_{g}^{\\mathrm{PP}}$**\n\n最小化后验期望平方损失的部分池化估计量是后验均值 $E[\\theta_{g} \\mid y_{g1}, \\dots, y_{gn_{g}}]$。我们通过应用贝叶斯准则来找到它：$p(\\theta_{g} \\mid \\mathbf{y}_{g}) \\propto p(\\mathbf{y}_{g} \\mid \\theta_{g}) p(\\theta_{g})$。样本均值 $\\bar{y}_{g}$ 是 $\\theta_{g}$ 的充分统计量。\n$\\bar{y}_{g}$ 的似然是 $p(\\bar{y}_{g} \\mid \\theta_{g}) \\sim \\mathcal{N}(\\theta_{g}, \\frac{\\sigma^{2}}{n_{g}})$。\n$\\theta_{g}$ 的先验是 $p(\\theta_{g}) \\sim \\mathcal{N}(\\mu, \\tau^{2})$。\n\n后验分布与这两个高斯密度的乘积成正比：\n$$p(\\theta_{g} \\mid \\bar{y}_{g}) \\propto \\exp\\left(-\\frac{(\\bar{y}_{g} - \\theta_{g})^{2}}{2\\sigma^{2}/n_{g}}\\right) \\exp\\left(-\\frac{(\\theta_{g} - \\mu)^{2}}{2\\tau^{2}}\\right)$$\n这是一个共轭模型，所以 $\\theta_g$ 的后验分布也是一个正态分布，$p(\\theta_{g} \\mid \\bar{y}_{g}) \\sim \\mathcal{N}(\\mu_{\\text{post}}, \\sigma_{\\text{post}}^{2})$。后验精度是先验精度和数据精度的和：\n$$\\frac{1}{\\sigma_{\\text{post}}^{2}} = \\frac{1}{\\tau^{2}} + \\frac{1}{\\sigma^{2}/n_{g}} = \\frac{1}{\\tau^{2}} + \\frac{n_{g}}{\\sigma^{2}}$$\n后验均值 $\\mu_{\\text{post}}$ 是先验均值和数据均值的精度加权平均：\n$$\\mu_{\\text{post}} = \\sigma_{\\text{post}}^{2} \\left(\\frac{\\mu}{\\tau^{2}} + \\frac{\\bar{y}_{g}}{\\sigma^{2}/n_{g}}\\right) = \\left(\\frac{1}{\\tau^{2}} + \\frac{n_{g}}{\\sigma^{2}}\\right)^{-1} \\left(\\frac{\\mu}{\\tau^{2}} + \\frac{n_{g}\\bar{y}_{g}}{\\sigma^{2}}\\right)$$\n估计量是 $\\hat{\\theta}_{g}^{\\mathrm{PP}} = \\mu_{\\text{post}}$。简化表达式：\n$$\\hat{\\theta}_{g}^{\\mathrm{PP}} = \\frac{\\frac{\\mu}{\\tau^{2}} + \\frac{n_{g}\\bar{y}_{g}}{\\sigma^{2}}}{\\frac{1}{\\tau^{2}} + \\frac{n_{g}}{\\sigma^{2}}} = \\frac{\\sigma^{2}\\mu + n_{g}\\tau^{2}\\bar{y}_{g}}{\\sigma^{2} + n_{g}\\tau^{2}}$$\n这可以表示为加权平均：\n$$\\hat{\\theta}_{g}^{\\mathrm{PP}} = \\frac{n_{g}\\tau^{2}}{\\sigma^{2} + n_{g}\\tau^{2}}\\bar{y}_{g} + \\frac{\\sigma^{2}}{\\sigma^{2} + n_{g}\\tau^{2}}\\mu = w\\bar{y}_{g} + (1-w)\\mu$$\n其中权重为 $w = \\frac{n_{g}\\tau^{2}}{\\sigma^{2} + n_{g}\\tau^{2}}$。\n\n$\\hat{\\theta}_{g}^{\\mathrm{PP}}$ 的条件偏差通过以 $\\theta_g$ 为条件取期望来计算：\n$$E[\\hat{\\theta}_{g}^{\\mathrm{PP}} \\mid \\theta_{g}] = E[w\\bar{y}_{g} + (1-w)\\mu \\mid \\theta_{g}] = w E[\\bar{y}_{g} \\mid \\theta_{g}] + (1-w)\\mu = w\\theta_{g} + (1-w)\\mu$$\n偏差 $= E[\\hat{\\theta}_{g}^{\\mathrm{PP}} \\mid \\theta_{g}] - \\theta_{g} = w\\theta_{g} + (1-w)\\mu - \\theta_{g} = (w-1)\\theta_{g} + (1-w)\\mu = (1-w)(\\mu - \\theta_{g})$。\n代入 $1-w = \\frac{\\sigma^{2}}{\\sigma^{2} + n_{g}\\tau^{2}}$，偏差为：\n$$\\text{Bias}(\\hat{\\theta}_{g}^{\\mathrm{PP}} \\mid \\theta_{g}) = \\frac{\\sigma^{2}}{\\sigma^{2} + n_{g}\\tau^{2}}(\\mu - \\theta_{g})$$\n这个估计量偏向于先验均值 $\\mu$，这种效应被称为收缩 (shrinkage)。\n\n$\\hat{\\theta}_{g}^{\\mathrm{PP}}$ 的条件方差是：\n$$\\mathrm{Var}(\\hat{\\theta}_{g}^{\\mathrm{PP}} \\mid \\theta_{g}) = \\mathrm{Var}(w\\bar{y}_{g} + (1-w)\\mu \\mid \\theta_{g}) = w^{2}\\mathrm{Var}(\\bar{y}_{g} \\mid \\theta_{g}) = w^{2} \\frac{\\sigma^{2}}{n_{g}}$$\n代入 $w$ 的表达式：\n$$\\mathrm{Var}(\\hat{\\theta}_{g}^{\\mathrm{PP}} \\mid \\theta_{g}) = \\left(\\frac{n_{g}\\tau^{2}}{\\sigma^{2} + n_{g}\\tau^{2}}\\right)^{2} \\frac{\\sigma^{2}}{n_{g}}$$\n\n**比较与一致性分析**\n\n方差减小：部分池化估计量的方差为 $\\mathrm{Var}(\\hat{\\theta}_{g}^{\\mathrm{PP}} \\mid \\theta_{g}) = w^{2} \\mathrm{Var}(\\hat{\\theta}_{g}^{\\mathrm{NP}} \\mid \\theta_{g})$。由于 $\\sigma^{2} > 0$ 且 $\\tau^{2} > 0$，权重 $w = \\frac{n_{g}\\tau^{2}}{\\sigma^{2} + n_{g}\\tau^{2}}$ 满足 $0  w  1$。因此，$w^{2}  1$，这证明了部分池化估计量的条件方差严格小于无池化估计量。这种方差的减小是通过将估计值“收缩”到先验均值 $\\mu$ 来实现的，使其不易受数据 $\\bar{y}_{g}$ 中抽样噪声的影响。这是偏差-方差权衡的一个例子；我们引入偏差以减小方差。\n\n一致性：如果一个估计量随着样本量的增加依概率收敛于真实参数，则该估计量是一致的。一个充分条件是其偏差和方差都收敛于零。\n\n对于 $\\hat{\\theta}_{g}^{\\mathrm{NP}}$，当 $n_{g} \\to \\infty$ 时：\n- 偏差始终为 $0$。\n- $\\mathrm{Var}(\\hat{\\theta}_{g}^{\\mathrm{NP}} \\mid \\theta_{g}) = \\frac{\\sigma^{2}}{n_{g}} \\to 0$。\n因此，$\\hat{\\theta}_{g}^{\\mathrm{NP}}$ 是 $\\theta_{g}$ 的一致估计量。\n\n对于 $\\hat{\\theta}_{g}^{\\mathrm{PP}}$，当 $n_{g} \\to \\infty$ 时：\n- 偏差：$\\lim_{n_{g}\\to\\infty} \\frac{\\sigma^{2}}{\\sigma^{2} + n_{g}\\tau^{2}}(\\mu - \\theta_{g}) = 0$。\n- 方差：$\\lim_{n_{g}\\to\\infty} \\left(\\frac{n_{g}\\tau^{2}}{\\sigma^{2} + n_{g}\\tau^{2}}\\right)^{2} \\frac{\\sigma^{2}}{n_{g}} = \\lim_{n_{g}\\to\\infty} \\left(\\frac{\\tau^{2}}{\\sigma^{2}/n_{g} + \\tau^{2}}\\right)^{2} \\frac{\\sigma^{2}}{n_{g}} = \\left(\\frac{\\tau^{2}}{\\tau^{2}}\\right)^{2} \\cdot 0 = 0$。\n由于偏差和方差都收敛于 $0$，$\\hat{\\theta}_{g}^{\\mathrm{PP}}$ 也是 $\\theta_{g}$ 的一致估计量。随着 $n_{g}$ 的增加，权重 $w$ 趋近于 $1$，这意味着 $\\hat{\\theta}_{g}^{\\mathrm{PP}}$ 收敛于 $\\hat{\\theta}_{g}^{\\mathrm{NP}}$。数据压倒了先验。\n\n**均方误差 (MSE) 差异**\n\n条件 MSE 定义为 $\\mathrm{MSE}(\\hat{\\theta}_{g} \\mid \\theta_{g}) = (E[\\hat{\\theta}_{g} \\mid \\theta_{g}] - \\theta_{g})^{2} + \\mathrm{Var}(\\hat{\\theta}_{g} \\mid \\theta_{g})$。\n\n对于无池化估计量：\n$$\\mathrm{MSE}(\\hat{\\theta}_{g}^{\\mathrm{NP}} \\mid \\theta_{g}) = (0)^{2} + \\frac{\\sigma^{2}}{n_{g}} = \\frac{\\sigma^{2}}{n_{g}}$$\n\n对于部分池化估计量：\n$$\\mathrm{MSE}(\\hat{\\theta}_{g}^{\\mathrm{PP}} \\mid \\theta_{g}) = \\left(\\frac{\\sigma^{2}}{\\sigma^{2} + n_{g}\\tau^{2}}(\\mu - \\theta_{g})\\right)^{2} + \\left(\\frac{n_{g}\\tau^{2}}{\\sigma^{2} + n_{g}\\tau^{2}}\\right)^{2} \\frac{\\sigma^{2}}{n_{g}}$$\n$$= \\frac{\\sigma^{4}(\\mu - \\theta_{g})^{2}}{(n_{g}\\tau^{2} + \\sigma^{2})^{2}} + \\frac{n_{g}^{2}\\tau^{4}\\sigma^{2}}{n_{g}(n_{g}\\tau^{2} + \\sigma^{2})^{2}} = \\frac{\\sigma^{4}(\\mu - \\theta_{g})^{2} + n_{g}\\tau^{4}\\sigma^{2}}{(n_{g}\\tau^{2} + \\sigma^{2})^{2}}$$\n\n我们现在计算差值 $\\Delta = \\mathrm{MSE}(\\hat{\\theta}_{g}^{\\mathrm{PP}} \\mid \\theta_{g}) - \\mathrm{MSE}(\\hat{\\theta}_{g}^{\\mathrm{NP}} \\mid \\theta_{g})$：\n$$\\Delta = \\frac{\\sigma^{4}(\\mu - \\theta_{g})^{2} + n_{g}\\tau^{4}\\sigma^{2}}{(n_{g}\\tau^{2} + \\sigma^{2})^{2}} - \\frac{\\sigma^{2}}{n_{g}}$$\n使用公分母 $n_{g}(n_{g}\\tau^{2} + \\sigma^{2})^{2}$：\n$$ \\Delta = \\frac{n_{g}(\\sigma^{4}(\\mu - \\theta_{g})^{2} + n_{g}\\tau^{4}\\sigma^{2}) - \\sigma^{2}(n_{g}\\tau^{2} + \\sigma^{2})^{2}}{n_{g}(n_{g}\\tau^{2} + \\sigma^{2})^{2}} $$\n展开分子：\n$$ \\text{Numerator} = n_{g}\\sigma^{4}(\\mu - \\theta_{g})^{2} + n_{g}^{2}\\tau^{4}\\sigma^{2} - \\sigma^{2}(n_{g}^{2}\\tau^{4} + 2n_{g}\\tau^{2}\\sigma^{2} + \\sigma^{4}) $$\n$$ = n_{g}\\sigma^{4}(\\mu - \\theta_{g})^{2} + n_{g}^{2}\\tau^{4}\\sigma^{2} - n_{g}^{2}\\tau^{4}\\sigma^{2} - 2n_{g}\\tau^{2}\\sigma^{4} - \\sigma^{6} $$\n$n_{g}^{2}\\tau^{4}\\sigma^{2}$ 项相互抵消：\n$$ \\text{Numerator} = n_{g}\\sigma^{4}(\\mu - \\theta_{g})^{2} - 2n_{g}\\tau^{2}\\sigma^{4} - \\sigma^{6} $$\n提出因子 $\\sigma^{4}$：\n$$ \\text{Numerator} = \\sigma^{4}[n_{g}(\\mu - \\theta_{g})^{2} - 2n_{g}\\tau^{2} - \\sigma^{2}] $$\n因此，MSE 差异的最终表达式为：\n$$ \\Delta = \\frac{\\sigma^{4}[n_{g}(\\mu - \\theta_{g})^{2} - 2n_{g}\\tau^{2} - \\sigma^{2}]}{n_{g}(n_{g}\\tau^{2} + \\sigma^{2})^{2}} $$", "answer": "$$\\boxed{\\frac{\\sigma^{4}\\left[n_{g}(\\mu - \\theta_{g})^{2} - 2n_{g}\\tau^{2} - \\sigma^{2}\\right]}{n_{g}(n_{g}\\tau^{2} + \\sigma^{2})^{2}}}$$", "id": "3118672"}, {"introduction": "最后，我们将这个概念推广到模型复杂度本身不固定的非参数模型中。这项练习比较了两种用于构建分类器的密度估计方法——直方图法和核密度估计。您将发现，平滑参数（例如直方图的组距 $h_n$ 或核密度估计的带宽）的选择直接控制着偏差与方差的权衡，并决定了最优的学习速率，这是统计学习理论中的一个基本概念。[@problem_id:3118697]", "problem": "考虑标签为 $Y \\in \\{0,1\\}$、特征为 $X \\in \\mathbb{R}$ 的二元分类问题。假设类别先验概率相等，即 $\\pi_{0} = \\pi_{1} = \\frac{1}{2}$。设类别条件密度 $f_{0}(x)$ 和 $f_{1}(x)$ 在一个固定内点 $x_{0} \\in \\mathbb{R}$ 的邻域内是二次连续可微、严格为正，并且其导数有界。贝叶斯分类器为 $g^{\\ast}(x) = \\mathbf{1}\\{\\eta(x) \\geq \\frac{1}{2}\\}$，其中后验概率为 $\\eta(x) = \\frac{\\pi_{1} f_{1}(x)}{\\pi_{0} f_{0}(x) + \\pi_{1} f_{1}(x)}$，贝叶斯风险由 $g^{\\ast}$ 达到。\n\n给定 $n$ 个独立同分布的训练样本 $\\{(X_{i}, Y_{i})\\}_{i=1}^{n}$。考虑两种通过估计对数似然比 $L(x) = \\ln\\big(\\pi_{1} f_{1}(x)\\big) - \\ln\\big(\\pi_{0} f_{0}(x)\\big)$ 并在 $0$ 处进行阈值判决来进行分类的置入式分类器。这两种类别条件密度的估计量是：\n\n1. 直方图规则：将 $\\mathbb{R}$ 划分为宽度为 $h_{n}$ 且以原点为基准的区间（bin），对于每个类别 $k \\in \\{0,1\\}$，通过将在包含 $x_{0}$ 的区间内类别 $k$ 的样本数，除以类别 $k$ 的样本总量与 $h_{n}$ 的乘积，来估计 $f_{k}(x_{0})$。\n\n2. 核密度置入法：对于每个类别 $k \\in \\{0,1\\}$，使用核密度估计量来估计 $f_{k}(x_{0})$，该估计量使用一个对称的二阶核函数 $K$，满足 $\\int K(u) \\, du = 1$、$\\int u K(u) \\, du = 0$、有限二阶矩 $\\mu_{2}(K) = \\int u^{2} K(u) \\, du$ 和有限粗糙度 $R(K) = \\int K(u)^{2} \\, du$，并使用带宽 $h_{n}$。\n\n在固定点 $x_{0}$ 处进行分析，并处理从每个密度估计量获得的置入式对数似然比估计量 $\\widehat{L}(x_{0})$。从偏差和方差的定义以及无意识统计学家法则出发，对每种方法，推导点态均方误差（MSE）的主导项贡献，其中MSE定义为 $\\operatorname{MSE}(\\widehat{L}(x_{0})) = \\big(E[\\widehat{L}(x_{0})] - L(x_{0})\\big)^{2} + \\operatorname{Var}(\\widehat{L}(x_{0}))$，并用区间宽度或核带宽 $h_{n}$ 以及样本量 $n$ 来表示。然后，忽略不影响关于 $n$ 的渐近率的常数，确定能够最小化 $\\widehat{L}(x_{0})$ 主导项点态MSE的 $h_{n}$ 的渐近最优率（作为 $n$ 的函数），针对以下情况：\n\n(a) 直方图规则，以及\n\n(b) 使用对称二阶核函数 $K$ 的核密度置入法。\n\n最后，使用 LaTeX 的 $\\mathrm{pmatrix}$ 环境，将这两个最优率按（直方图，核）的顺序表示为一个单行矩阵。您的答案必须是单个闭式解析表达式。无需四舍五入。解释这些选择是否意味着贝叶斯风险一致性，但最终报告的答案必须仅为这两个率的行矩阵。", "solution": "用户要求通过推导能够最小化对数似然比估计量点态均方误差的平滑参数 $h_n$ 的最优率，来分析两种置入式分类器。\n\n首先验证问题。\n**步骤 1：提取已知条件**\n- 二元分类：$Y \\in \\{0,1\\}$。\n- 特征空间：$X \\in \\mathbb{R}$。\n- 类别先验概率：$\\pi_{0} = \\pi_{1} = \\frac{1}{2}$。\n- 类别条件密度：$f_{0}(x)$、$f_{1}(x)$ 在一个固定点 $x_{0}$ 附近是二次连续可微、严格为正，且导数有界。\n- 对数似然比：$L(x) = \\ln\\big(\\pi_{1} f_{1}(x)\\big) - \\ln\\big(\\pi_{0} f_{0}(x)\\big)$。在先验概率相等的情况下，即为 $L(x) = \\ln(f_{1}(x)) - \\ln(f_{0}(x))$。\n- $L(x_0)$ 的估计量：$\\widehat{L}(x_{0}) = \\ln(\\hat{f}_{1}(x_{0})) - \\ln(\\hat{f}_{0}(x_{0}))$。\n- 样本：$n$ 个独立同分布样本 $\\{(X_{i}, Y_{i})\\}_{i=1}^{n}$。类别 $k$ 中的样本数量为 $N_k$，且 $E[N_k] = n \\pi_k = n/2$。\n- 估计量 1（直方图）：对于将 $\\mathbb{R}$ 划分为宽度为 $h_n$ 且以原点为基准的区间，$\\hat{f}_{k}(x_{0}) = \\frac{N_{k}(x_{0})}{N_{k} h_{n}}$，其中 $N_k(x_0)$ 是包含 $x_0$ 的区间中类别为 $k$ 的样本数。\n- 估计量 2（核密度）：$\\hat{f}_{k}(x_{0}) = \\frac{1}{N_{k} h_{n}} \\sum_{i: Y_{i}=k} K\\left(\\frac{x_{0} - X_{i}}{h_{n}}\\right)$。\n- 核函数 $K$：对称，$\\int K(u) \\, du = 1$，$\\int u K(u) \\, du = 0$，$\\mu_{2}(K) = \\int u^{2} K(u) \\, du  \\infty$，$R(K) = \\int K(u)^{2} \\, du  \\infty$。\n- 目标：对每种方法，找出 $\\widehat{L}(x_0)$ 的主导项点态均方误差 (MSE)，定义为 $\\operatorname{MSE}(\\widehat{L}(x_{0})) = (E[\\widehat{L}(x_{0})] - L(x_{0}))^{2} + \\operatorname{Var}(\\widehat{L}(x_{0}))$。然后确定 $h_n$ 作为 $n$ 的函数的渐近最优率。\n\n**步骤 2：使用提取的已知条件进行验证**\n该问题是非参数统计估计和分类理论中的一个标准练习。偏差、方差、均方误差、密度估计（直方图和核方法）以及偏差-方差权衡等概念是该领域的核心。关于密度和核函数的假设是标准的。该问题具有科学依据、问题明确、客观，并包含足够的信息以得到关于渐近率的唯一解。因此，该问题被认为是**有效的**。\n\n**步骤 3：结论与行动**\n问题有效。将提供详细的解决方案。\n\n**均方误差的推导**\n\n在 $x_0$ 点的对数似然比估计量为 $\\widehat{L}(x_0) = \\ln(\\hat{f}_1(x_0)) - \\ln(\\hat{f}_0(x_0))$。估计量 $\\hat{f}_0(x_0)$ 和 $\\hat{f}_1(x_0)$ 是从不相交的数据子集（分别为 $Y_i=0$ 和 $Y_i=1$ 的样本）构建的，因此它们是独立的随机变量。\n对于大的 $n$，类别样本量 $N_k$ 集中在其均值 $n/2$ 附近。为了进行渐近分析，我们视 $N_k$ 固定为 $n/2$，这不影响主导项的率。\n\n$\\widehat{L}(x_0)$ 的均方误差是其偏差平方与方差之和。我们使用 delta 方法（泰勒展开）来近似这些量。令 $G(u, v) = \\ln v - \\ln u$。那么 $\\widehat{L}(x_0) = G(\\hat{f}_0(x_0), \\hat{f}_1(x_0))$。真实值为 $L(x_0) = G(f_0(x_0), f_1(x_0))$。\n\n$G$ 的偏导数为 $\\frac{\\partial G}{\\partial u} = -\\frac{1}{u}$ 和 $\\frac{\\partial G}{\\partial v} = \\frac{1}{v}$。\n$\\widehat{L}(x_0)$ 的方差主导项为：\n$$ \\operatorname{Var}(\\widehat{L}(x_{0})) \\approx \\left(-\\frac{1}{f_{0}(x_{0})}\\right)^{2} \\operatorname{Var}(\\hat{f}_{0}(x_{0})) + \\left(\\frac{1}{f_{1}(x_{0})}\\right)^{2} \\operatorname{Var}(\\hat{f}_{1}(x_{0})) $$\n对于偏差，需要二阶展开：\n$$ E[\\widehat{L}(x_0)] - L(x_0) \\approx \\frac{\\operatorname{Bias}(\\hat{f}_1(x_0))}{f_1(x_0)} - \\frac{\\operatorname{Bias}(\\hat{f}_0(x_0))}{f_0(x_0)} $$\n这里我们忽略泰勒展开中的高阶项，因为密度估计量的偏差通常比其方差的阶数更高。\n因此，偏差的平方为：\n$$ (\\operatorname{Bias}(\\widehat{L}(x_0)))^2 \\approx \\left(\\frac{\\operatorname{Bias}(\\hat{f}_1(x_0))}{f_1(x_0)} - \\frac{\\operatorname{Bias}(\\hat{f}_0(x_0))}{f_0(x_0)}\\right)^2 $$\n\n我们需要确定每个密度估计量的主导项偏差和方差。\n\n**(a) 直方图规则**\n\n设包含 $x_0$ 的区间为 $B(x_0)$，其宽度为 $h_n$。设 $N_k(x_0)$ 是该区间内类别为 $k$ 的点的数量。$\\hat{f}_{k}(x_{0}) = \\frac{N_{k}(x_{0})}{N_{k} h_{n}}$。\n$N_k(x_0)$ 服从二项分布 $\\operatorname{Bin}(N_k, p_k)$，其中 $p_k = \\int_{B(x_0)} f_k(x) dx$。\n期望为 $E[\\hat{f}_{k}(x_{0})] = \\frac{N_k p_k}{N_k h_n} = \\frac{1}{h_n} \\int_{B(x_0)} f_k(x) dx$。\n我们将 $f_k(x)$ 在 $x_0$ 附近展开：$f_k(x) = f_k(x_0) + f_k'(x_0)(x-x_0) + O((x-x_0)^2)$。\n由于区间是固定的（“以原点为基准”），$x_0$ 在其区间内的位置是固定的。设该区间为 $[x_0-ah_n, x_0+bh_n)$，其中 $a+b=1$。\n$$ E[\\hat{f}_k(x_0)] = \\frac{1}{h_n} \\int_{x_0-ah_n}^{x_0+bh_n} \\left(f_k(x_0) + f_k'(x_0)(x-x_0) + \\dots\\right) dx $$\n$$ = f_k(x_0) + \\frac{f_k'(x_0)}{h_n} \\left[\\frac{(x-x_0)^2}{2}\\right]_{x_0-ah_n}^{x_0+bh_n} + O(h_n^2) = f_k(x_0) + \\frac{f_k'(x_0) h_n}{2} (b^2-a^2) + O(h_n^2) $$\n$$ = f_k(x_0) + \\frac{f_k'(x_0) h_n}{2} (b-a) + O(h_n^2) $$\n主导项偏差为 $\\operatorname{Bias}(\\hat{f}_{k}(x_{0})) \\approx C_{k,1} h_n$，其中 $C_{k,1} = \\frac{f_k'(x_0)}{2}(b-a)$。除非 $x_0$ 正好位于区间的中心（$a=b=1/2$）或 $f_k'(x_0)=0$，否则该项不为零。对于一个一般的固定点 $x_0$，偏差的阶为 $h_n$。\n\n方差为 $\\operatorname{Var}(\\hat{f}_{k}(x_{0})) = \\frac{p_k(1-p_k)}{N_k h_n^2}$。当 $h_n \\to 0$ 时，$p_k \\approx f_k(x_0)h_n \\to 0$。\n$$ \\operatorname{Var}(\\hat{f}_{k}(x_{0})) \\approx \\frac{p_k}{N_k h_n^2} \\approx \\frac{f_k(x_0)h_n}{N_k h_n^2} = \\frac{f_k(x_0)}{N_k h_n} \\approx \\frac{2 f_k(x_0)}{n h_n} $$\n方差的主导项的阶为 $(nh_n)^{-1}$。\n\n现在我们组合 $\\widehat{L}(x_0)$ 的均方误差：\n$$ (\\operatorname{Bias}(\\widehat{L}(x_0)))^2 \\approx \\left(\\frac{C_{1,1} h_n}{f_1(x_0)} - \\frac{C_{0,1} h_n}{f_0(x_0)}\\right)^2 = A_1 h_n^2 $$\n其中 $A_1$ 是一个依赖于 $f_k(x_0)$、$f_k'(x_0)$ 以及 $x_0$ 在其区间内位置的常数。\n$$ \\operatorname{Var}(\\widehat{L}(x_0)) \\approx \\frac{1}{f_0(x_0)^2} \\frac{2f_0(x_0)}{nh_n} + \\frac{1}{f_1(x_0)^2} \\frac{2f_1(x_0)}{nh_n} = \\frac{A_2}{nh_n} $$\n其中 $A_2 = 2 \\left(\\frac{1}{f_0(x_0)} + \\frac{1}{f_1(x_0)}\\right)$。\n均方误差为 $\\operatorname{MSE}(\\widehat{L}(x_0)) \\approx A_1 h_n^2 + \\frac{A_2}{nh_n}$。\n为了关于 $h_n$ 最小化该式，我们求导并令其为零：\n$$ \\frac{d}{dh_n}\\operatorname{MSE} = 2A_1 h_n - \\frac{A_2}{nh_n^2} = 0 \\implies h_n^3 = \\frac{A_2}{2nA_1} $$\n因此，$h_n$ 的最优率为 $h_n \\propto n^{-1/3}$。\n\n**(b) 核密度置入法**\n\n对于核密度估计量，偏差的计算是标准的：\n$$ E[\\hat{f}_k(x_0)] = \\int K(u) f_k(x_0-uh_n) du $$\n使用 $f_k(x_0-uh_n)$ 的泰勒展开和核函数 $K$ 的性质：\n$$ E[\\hat{f}_k(x_0)] = f_k(x_0) \\int K(u)du - h_n f_k'(x_0) \\int uK(u)du + \\frac{h_n^2}{2} f_k''(x_0) \\int u^2K(u)du + O(h_n^3) $$\n$$ = f_k(x_0) - 0 + \\frac{1}{2}h_n^2 f_k''(x_0) \\mu_2(K) + O(h_n^3) $$\n主导项偏差为 $\\operatorname{Bias}(\\hat{f}_{k}(x_{0})) \\approx \\frac{1}{2}\\mu_2(K)f_k''(x_0) h_n^2$。偏差的阶为 $h_n^2$。\n\n方差的计算也是标准的：\n$$ \\operatorname{Var}(\\hat{f}_k(x_0)) = \\frac{1}{N_k h_n^2} \\operatorname{Var}\\left(K\\left(\\frac{x_0-X}{h_n}\\right)\\right) \\approx \\frac{1}{N_k h_n^2} E\\left[K\\left(\\frac{x_0-X}{h_n}\\right)^2\\right] $$\n$$ E\\left[K\\left(\\frac{x_0-X}{h_n}\\right)^2\\right] = \\int K\\left(\\frac{x_0-x}{h_n}\\right)^2 f_k(x) dx \\approx h_n f_k(x_0) \\int K(u)^2 du = h_n f_k(x_0) R(K) $$\n所以，$\\operatorname{Var}(\\hat{f}_k(x_0)) \\approx \\frac{h_n f_k(x_0) R(K)}{N_k h_n^2} = \\frac{f_k(x_0)R(K)}{N_k h_n} \\approx \\frac{2f_k(x_0)R(K)}{n h_n}$。\n方差的阶为 $(nh_n)^{-1}$。\n\n现在我们组合 $\\widehat{L}(x_0)$ 的均方误差：\n$$ (\\operatorname{Bias}(\\widehat{L}(x_0)))^2 \\approx \\left(\\frac{\\frac{1}{2}\\mu_2(K)f_1''(x_0)h_n^2}{f_1(x_0)} - \\frac{\\frac{1}{2}\\mu_2(K)f_0''(x_0)h_n^2}{f_0(x_0)}\\right)^2 = B_1 h_n^4 $$\n其中 $B_1$ 是一个常数。关键在于偏差的平方的阶为 $h_n^4$。\n方差表达式与直方图情况类似，但常数不同：\n$$ \\operatorname{Var}(\\widehat{L}(x_0)) \\approx \\frac{B_2}{nh_n} $$\n其中 $B_2 = 2R(K)\\left(\\frac{1}{f_0(x_0)} + \\frac{1}{f_1(x_0)}\\right)$。\n均方误差为 $\\operatorname{MSE}(\\widehat{L}(x_0)) \\approx B_1 h_n^4 + \\frac{B_2}{nh_n}$。\n为了最小化，求导并令其为零：\n$$ \\frac{d}{dh_n}\\operatorname{MSE} = 4B_1 h_n^3 - \\frac{B_2}{nh_n^2} = 0 \\implies h_n^5 = \\frac{B_2}{4nB_1} $$\n因此，$h_n$ 的最优率为 $h_n \\propto n^{-1/5}$。\n\n**贝叶斯风险一致性**\n对于这两种方法，选择最优的 $h_n$ 会导致 $\\widehat{L}(x_0)$ 的均方误差在 $n \\to \\infty$ 时趋于零。\n对于直方图，$\\operatorname{MSE}(\\widehat{L}(x_0)) = O(h_n^2) = O((n^{-1/3})^2) = O(n^{-2/3})$。\n对于核估计量，$\\operatorname{MSE}(\\widehat{L}(x_0)) = O(h_n^4) = O((n^{-1/5})^4) = O(n^{-4/5})$。\n由于均方误差收敛到零，估计量 $\\widehat{L}(x_0)$ 是均方一致的，这意味着它依概率收敛到真实值 $L(x_0)$。对数似然比估计量的这种点态一致性是所得分类器 $\\hat{g}_n(x) = \\mathbf{1}\\{\\widehat{L}(x) \\geq 0\\}$ 一致性的基本要求。要实现贝叶斯风险一致性，即 $R(\\hat{g}_n) \\to R(g^*)$，通常需要更强的条件，例如密度估计在 $X$ 的支撑集上的一致收敛。然而，计算出的 $h_n$ 的最优率正是为这些类型的估计量产生最快可能点态收敛的率，并且在标准的正则性条件下，它们足以建立贝叶斯风险一致性。\n\n最终答案只需要 $h_n$ 的率。\n(a) 直方图规则：$n^{-1/3}$。\n(b) 核密度置入法：$n^{-1/5}$。\n这些以行矩阵的形式呈现。", "answer": "$$\n\\boxed{\\begin{pmatrix} n^{-\\frac{1}{3}}  n^{-\\frac{1}{5}} \\end{pmatrix}}\n$$", "id": "3118697"}]}