{"hands_on_practices": [{"introduction": "参数的标准误不仅仅是一个冰冷的数字，它更揭示了我们估计可靠性的深刻信息。这个练习将引导你深入剖析截距项 $\\beta_0$ 标准误的计算公式，让你亲身体会实验设计——特别是预测变量 $x$ 的取值范围和中心——如何显著影响我们对模型参数的信心。通过这个练习，你将理解为什么在远离数据中心的地方进行预测（即外推）会极大地增加不确定性，并学会如何通过巧妙的实验设计来获得更稳健的估计 [@problem_id:3176623]。", "problem": "一位研究人员拟合了一个形式为 $y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i$ 的简单线性回归模型，其中 $\\varepsilon_i \\stackrel{\\text{i.i.d.}}{\\sim} \\mathcal{N}(0,\\sigma^2)$（独立同分布的正态分布，均值为 $0$，方差为 $\\sigma^2$）。在经典线性模型下，普通最小二乘法 (OLS) 估计量是无偏的，其抽样变异性由设计矩阵和 $\\sigma^2$ 决定。\n\n为了研究当预测变量范围极小时，截距估计量的不确定性如何表现，我们考虑三种实验设计，每种设计的样本量均为 $n = 30$，且误差方差 $\\sigma^2$ 相同：\n\n- 设计 A：$x_i$ 在 $[9.8, 10.2]$ 上均匀分布。\n- 设计 B：$x_i$ 在 $[0.0, 0.4]$ 上均匀分布。\n- 设计 C：$x_i$ 在 $[-0.2, 0.2]$ 上均匀分布。\n\n假设在不同设计下，拟合模型产生的残差标准差相似（因此截距不确定性的差异是由 $x$ 的设计驱动的）。您为每种设计计算了截距 $\\beta_0$ 的 $95\\%$ 置信区间。\n\n在这种情况下，以下哪个陈述最能描述截距的置信区间何时会宽到不具信息量，并正确解释了截距估计量的标准误 $\\operatorname{SE}(\\hat{\\beta}_0)$ 如何依赖于 $\\bar{x}$ 和 $\\sum_{i=1}^{n} (x_i - \\bar{x})^2$？\n\nA. 当预测变量范围极小时，如果 $|\\bar{x}|$ 很大，截距的置信区间可能会非常宽，因为从远离 $x=0$ 的数据进行外推会使 $\\operatorname{SE}(\\hat{\\beta}_0)$ 膨胀。具体来说，$\\operatorname{SE}(\\hat{\\beta}_0)$ 随着 $|\\bar{x}|$ 的增加而增加，并随着 $\\sum_{i=1}^{n} (x_i - \\bar{x})^2$ 的增加而减少。因此，设计 A 产生的截距区间比设计 B 和 C 宽得多。\n\nB. 截距的标准误随着 $\\sum_{i=1}^{n} (x_i - \\bar{x})^2$ 的增加而增加，并且基本上与 $\\bar{x}$ 无关。因此，除非误差是异方差的，否则极小的预测变量范围会直接收窄截距的区间。\n\nC. 极小的预测变量范围总是会收窄截距的置信区间，因为 $x$ 中较小的变异性会减少整体不确定性；较大的 $|\\bar{x}|$ 由于在较大的 $x$ 处具有更高的杠杆作用而使截距稳定。\n\nD. 截距的标准误约等于残差标准差除以 $\\sqrt{n}$，而与 $\\bar{x}$ 或 $x_i$ 的散布无关。因此，如果残差变异性相似，所有三种设计都应产生相似的截距区间。", "solution": "用户希望我扮演一位严谨的STEM教授来解决一个统计学问题。我将首先验证问题陈述，然后基于第一性原理推导解决方案，最后评估每个选项。\n\n### 问题验证\n\n**步骤 1：提取已知条件**\n-   **模型：** 简单线性回归, $y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i$。\n-   **误差项：** $\\varepsilon_i \\stackrel{\\text{i.i.d.}}{\\sim} \\mathcal{N}(0,\\sigma^2)$ (独立同分布的正态分布，均值为 $0$，方差为 $\\sigma^2$)。\n-   **估计量：** 普通最小二乘法 (OLS)。\n-   **样本量：** 对所有设计均为 $n = 30$。\n-   **误差方差：** 在所有设计中 $\\sigma^2$ 相同。\n-   **假设：** 在所有设计中残差标准差相似。\n-   **设计 A：** $x_i$ 在 $[9.8, 10.2]$ 上均匀分布。\n-   **设计 B：** $x_i$ 在 $[0.0, 0.4]$ 上均匀分布。\n-   **设计 C：** $x_i$ 在 $[-0.2, 0.2]$ 上均匀分布。\n-   **任务：** 描述截距 $\\beta_0$ 的置信区间的特征，并解释其标准误 $\\operatorname{SE}(\\hat{\\beta}_0)$ 对预测变量均值 $\\bar{x}$ 和离散程度 $\\sum_{i=1}^{n} (x_i - \\bar{x})^2$ 的依赖性。\n\n**步骤 2：使用提取的已知条件进行验证**\n1.  **科学依据：** 该问题牢固地建立在经典线性回归模型框架内，这是统计学的基石。模型、假设和概念（标准误、置信区间）都是标准的，并且在数学上是合理的。\n2.  **适定性：** 问题提供了足够的信息来确定截距估计量标准误的函数形式，并定性地比较其在三种指定设计中的大小。可以得出一个独特概念性结论。\n3.  **客观性：** 语言清晰、精确，没有主观陈述。设计是客观指定的。\n4.  **完整性：** 问题是自洽的。所有必要的组成部分（$n$、模型形式、误差结构以及预测变量设计的具体细节）都已提供，足以分析截距的不确定性。假设残差标准差相似简化了比较，使其按预期集中在设计矩阵上。\n\n**步骤 3：结论和行动**\n问题陈述有效。这是一个在回归分析中表述良好的概念性问题。我现在将进行解题推导。\n\n### 解题推导\n\n分析的核心是 OLS 截距估计量 $\\hat{\\beta}_0$ 的方差公式，以及由此得到的标准误。在简单线性回归模型中，OLS 估计量由以下公式给出：\n$$ \\hat{\\beta}_1 = \\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^{n} (x_i - \\bar{x})^2} $$\n$$ \\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1 \\bar{x} $$\n其中 $\\bar{x} = \\frac{1}{n}\\sum_{i=1}^{n} x_i$ 且 $\\bar{y} = \\frac{1}{n}\\sum_{i=1}^{n} y_i$。\n\n为了找到 $\\hat{\\beta}_0$ 的标准误，我们必须首先推导其方差。我们使用方差的性质以及在模型假设下 $\\operatorname{Cov}(\\bar{y}, \\hat{\\beta}_1) = 0$ 的事实。\n$$ \\operatorname{Var}(\\hat{\\beta}_0) = \\operatorname{Var}(\\bar{y} - \\hat{\\beta}_1 \\bar{x}) $$\n$$ \\operatorname{Var}(\\hat{\\beta}_0) = \\operatorname{Var}(\\bar{y}) + \\bar{x}^2 \\operatorname{Var}(\\hat{\\beta}_1) - 2\\bar{x}\\operatorname{Cov}(\\bar{y}, \\hat{\\beta}_1) $$\n所需的组成部分是：\n-   $\\operatorname{Var}(\\bar{y}) = \\operatorname{Var}\\left(\\frac{1}{n}\\sum_{i=1}^n y_i\\right) = \\frac{1}{n^2}\\sum_{i=1}^n \\operatorname{Var}(y_i) = \\frac{n\\sigma^2}{n^2} = \\frac{\\sigma^2}{n}$。\n-   $\\operatorname{Var}(\\hat{\\beta}_1) = \\frac{\\sigma^2}{\\sum_{i=1}^{n} (x_i - \\bar{x})^2}$。\n-   $\\operatorname{Cov}(\\bar{y}, \\hat{\\beta}_1) = 0$。这是一个标准结果；中心化的预测变量与响应变量的样本均值不相关。\n\n将这些代入 $\\hat{\\beta}_0$ 的方差表达式中：\n$$ \\operatorname{Var}(\\hat{\\beta}_0) = \\frac{\\sigma^2}{n} + \\bar{x}^2 \\frac{\\sigma^2}{\\sum_{i=1}^{n} (x_i - \\bar{x})^2} $$\n$$ \\operatorname{Var}(\\hat{\\beta}_0) = \\sigma^2 \\left( \\frac{1}{n} + \\frac{\\bar{x}^2}{\\sum_{i=1}^{n} (x_i - \\bar{x})^2} \\right) $$\n\n标准误 $\\operatorname{SE}(\\hat{\\beta}_0)$ 是估计方差的平方根，其中 $\\sigma^2$ 被其估计量 $\\hat{\\sigma}^2$（残差方差，常写作 $s^2$ 或 $RSE^2$）所替代。\n$$ \\operatorname{SE}(\\hat{\\beta}_0) = \\hat{\\sigma} \\sqrt{\\frac{1}{n} + \\frac{\\bar{x}^2}{\\sum_{i=1}^{n} (x_i - \\bar{x})^2}} $$\n$\\beta_0$ 的 $95\\%$ 置信区间的宽度与 $\\operatorname{SE}(\\hat{\\beta}_0)$ 成正比：$2 \\times t_{n-2, 0.975} \\times \\operatorname{SE}(\\hat{\\beta}_0)$。由于假设 $n$ 和 $\\hat{\\sigma}$ 在不同设计中是恒定的，因此宽度由平方根下的项决定。\n\n这个公式揭示了两个关键的依赖关系：\n1.  由于 $\\bar{x}^2$ 项的存在，标准误随着 $|\\bar{x}|$ 的增加而增加。这反映了从数据中心（$\\bar{x}$）外推到 y 轴截距（即 $x=0$ 处）所引入的不确定性。\n2.  标准误随着预测变量的离散程度 $\\sum_{i=1}^{n} (x_i - \\bar{x})^2$ 的增加而减小。更大的离散程度提供了更稳定的回归线。\n\n问题陈述中提到，所有三种设计的预测变量范围都是“极小的”。让我们来分析这些设计：\n-   所有设计都有 $n=30$ 和一个 $0.4$ 的预测变量范围。由于数据点是均匀间隔的，离差平方和 $S_{xx} = \\sum_{i=1}^{n} (x_i - \\bar{x})^2$ 对于所有三种设计将是相同且很小的。\n-   关键的区别在于预测变量的均值 $\\bar{x}$：\n    -   **设计 A:** $x_i \\in [9.8, 10.2] \\implies \\bar{x}_A = 10.0$。\n    -   **设计 B:** $x_i \\in [0.0, 0.4] \\implies \\bar{x}_B = 0.2$。\n    -   **设计 C:** $x_i \\in [-0.2, 0.2] \\implies \\bar{x}_C = 0.0$。\n\n现在我们比较每种设计的 $\\frac{\\bar{x}^2}{\\sum (x_i - \\bar{x})^2}$ 项：\n-   **设计 A:** $\\frac{(10.0)^2}{S_{xx}} = \\frac{100}{S_{xx}}$。由于 $S_{xx}$ 很小，这一项非常大。\n-   **设计 B:** $\\frac{(0.2)^2}{S_{xx}} = \\frac{0.04}{S_{xx}}$。这一项比设计 A 的小得多。\n-   **设计 C:** $\\frac{(0.0)^2}{S_{xx}} = 0$。不确定性被最小化，且 $\\operatorname{SE}(\\hat{\\beta}_0) = \\hat{\\sigma}/\\sqrt{n}$。\n\n结论：在预测变量范围极小（即 $S_{xx}$ 很小）的情况下，截距的不确定性主要由外推距离 $|\\bar{x}|$ 主导。设计 A 涉及到从以 $\\bar{x}=10.0$ 为中心的数据向 $x=0$ 进行大规模外推，导致标准误极大，从而使得 $\\beta_0$ 的置信区间非常宽，不具信息量。设计 B 和 C 的数据更接近 $x=0$，因此它们的置信区间会窄得多。具体来说，设计 A 将产生比 B 和 C 宽得多的区间。\n\n### 逐项分析\n\n**A. 当预测变量范围极小时，如果 $|\\bar{x}|$ 很大，截距的置信区间可能会非常宽，因为从远离 $x=0$ 的数据进行外推会使 $\\operatorname{SE}(\\hat{\\beta}_0)$ 膨胀。具体来说，$\\operatorname{SE}(\\hat{\\beta}_0)$ 随着 $|\\bar{x}|$ 的增加而增加，并随着 $\\sum_{i=1}^{n} (x_i - \\bar{x})^2$ 的增加而减少。因此，设计 A 产生的截距区间比设计 B 和 C 宽得多。**\n这个陈述与我们的推导完全一致。它正确地指出了当 $|\\bar{x}|$ 很大时外推的作用，正确地陈述了 $\\operatorname{SE}(\\hat{\\beta}_0)$ 对 $|\\bar{x}|$ 和 $\\sum_{i=1}^{n} (x_i - \\bar{x})^2$ 的数学依赖关系，并正确地比较了三种设计。\n**结论：正确**\n\n**B. 截距的标准误随着 $\\sum_{i=1}^{n} (x_i - \\bar{x})^2$ 的增加而增加，并且基本上与 $\\bar{x}$ 无关。因此，除非误差是异方差的，否则极小的预测变量范围会直接收窄截距的区间。**\n这个陈述在多个方面都是不正确的。首先，$\\operatorname{SE}(\\hat{\\beta}_0)$ 随着 $\\sum_{i=1}^{n} (x_i - \\bar{x})^2$ 的增加而*减小*，因为该项出现在分母中。其次，它强烈依赖于 $\\bar{x}$，除非在 $\\bar{x} \\approx 0$ 的特殊情况下。\n**结论：不正确**\n\n**C. 极小的预测变量范围总是会收窄截距的置信区间，因为 $x$ 中较小的变异性会减少整体不确定性；较大的 $|\\bar{x}|$ 由于在较大的 $x$ 处具有更高的杠杆作用而使截距稳定。**\n这个陈述有根本性的缺陷。极小的预测变量范围（即小的 $\\sum(x_i - \\bar{x})^2$）会*增加*斜率的不确定性，这反过来又会增加截距的不确定性。此外，更大的 $|\\bar{x}|$ 会因外推的需要而极大地*破坏*截距估计的稳定性，正如方差公式分子中的 $\\bar{x}^2$ 项所示。\n**结论：不正确**\n\n**D. 截距的标准误约等于残差标准差除以 $\\sqrt{n}$，而与 $\\bar{x}$ 或 $x_i$ 的散布无关。因此，如果残差变异性相似，所有三种设计都应产生相似的截距区间。**\n这个陈述将一个特例描述为普遍规则。公式 $\\operatorname{SE}(\\hat{\\beta}_0) \\approx \\hat{\\sigma}/\\sqrt{n}$ 仅在 $\\frac{\\bar{x}^2}{\\sum(x_i - \\bar{x})^2}$ 项可忽略不计时才有效，这要求 $\\bar{x} \\approx 0$。这对设计 C 成立，但对设计 A 则完全不成立。因此，声称所有设计都将有相似的区间的说法是不正确的。\n**结论：不正确**", "answer": "$$\\boxed{A}$$", "id": "3176623"}, {"introduction": "在实际应用中，我们关心的科学问题往往不局限于单个回归系数的效应，而是涉及多个系数之间的比较或组合。这个练习将带你从计算单个参数的标准误，推广到处理一个更普遍的情形：多个系数的线性组合，例如 $f(\\beta) = x_{a}^{\\top}\\beta - x_{b}^{\\top}\\beta$。通过这个练习，你将学会如何利用 OLS 估计量 $\\hat{\\beta}$ 的协方差矩阵来精确计算这种线性组合的标准误，这是进行复杂假设检验（如比较不同预测因子效应大小）的一项核心技能 [@problem_id:3176582]。", "problem": "考虑一个带有截距项和两个预测变量的线性回归模型，其矩阵形式写作 $y = X\\beta + \\varepsilon$，其中 $y \\in \\mathbb{R}^{n}$，$X \\in \\mathbb{R}^{n \\times p}$ 且 $p=3$，$\\beta \\in \\mathbb{R}^{3}$，以及 $\\varepsilon \\in \\mathbb{R}^{n}$。假设误差满足 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2} I_{n})$。令 $\\hat{\\beta}$ 为普通最小二乘（OLS）估计量。已知 $n=25$，残差平方和 $\\text{SSE} = 220$，且矩阵 $(X^{\\top}X)^{-1}$ 等于\n$$\n\\begin{pmatrix}\n0.50  0.02  -0.01 \\\\\n0.02  0.20  0.00 \\\\\n-0.01  0.00  0.10\n\\end{pmatrix}.\n$$\n考虑在两个协变量行向量 $x_{a}^{\\top} = \\begin{pmatrix} 1  2  -1 \\end{pmatrix}$ 和 $x_{b}^{\\top} = \\begin{pmatrix} 1  0  3 \\end{pmatrix}$ 上的预测差泛函 $f(\\beta) = x_{a}^{\\top}\\beta - x_{b}^{\\top}\\beta$。请从所述假设下 OLS 估计量分布的基本原理出发，仔细推导以 $\\hat{\\beta}$ 的协方差和 $(X^{\\top}X)^{-1}$ 表示的标准误，从而为 $f(\\beta)$ 构建一个双侧 $95\\%$ 置信区间。在数值计算中，使用自由度为 $n-p$、置信水平为 $0.95$ 的双侧学生 t 分布临界值 $t_{0.975,\\,22} = 2.074$。明确指出置信区间的中心和半宽，然后仅报告半宽。将您报告的半宽四舍五入到四位有效数字。", "solution": "用户想要为回归系数的线性组合构建一个 $95\\%$ 置信区间，并报告其半宽。\n\n### 第 1 步：提取已知条件\n- 线性模型：$y = X\\beta + \\varepsilon$\n- $y \\in \\mathbb{R}^{n}$, $X \\in \\mathbb{R}^{n \\times p}$, $\\beta \\in \\mathbb{R}^{p}$, $\\varepsilon \\in \\mathbb{R}^{n}$\n- 观测数量：$n=25$\n- 参数数量（包括截距）：$p=3$\n- 误差分布假设：$\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2} I_{n})$\n- 残差平方和：$\\text{SSE} = 220$\n- 矩阵 $(X^{\\top}X)^{-1}$:\n$$\n(X^{\\top}X)^{-1} = \n\\begin{pmatrix}\n0.50  0.02  -0.01 \\\\\n0.02  0.20  0.00 \\\\\n-0.01  0.00  0.10\n\\end{pmatrix}\n$$\n- 目标泛函：$f(\\beta) = x_{a}^{\\top}\\beta - x_{b}^{\\top}\\beta$\n- 协变量向量：$x_{a}^{\\top} = \\begin{pmatrix} 1  2  -1 \\end{pmatrix}$ 和 $x_{b}^{\\top} = \\begin{pmatrix} 1  0  3 \\end{pmatrix}$\n- 置信水平：$95\\%$\n- 临界值：$t_{0.975,\\,22} = 2.074$ （自由度为 $n-p = 25-3=22$ 的学生 t 分布）\n\n### 第 2 步：使用提取的已知条件进行验证\n该问题在线性回归理论的框架内定义明确。构建置信区间所需的所有必要组成部分都已提供。这些假设（误差的正态性）对于此类问题是标准的。矩阵 $(X^{\\top}X)^{-1}$ 是对称的，这是此类矩阵的一个必要属性。该问题具有科学依据、良定且客观。它不违反任何无效性标准。\n\n### 第 3 步：结论和行动\n该问题是**有效的**。将提供完整解答。\n\n### 置信区间和半宽的推导\n$\\beta$ 的普通最小二乘（OLS）估计量是 $\\hat{\\beta} = (X^{\\top}X)^{-1}X^{\\top}y$。在给定假设 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2} I_{n})$ 下，估计量 $\\hat{\\beta}$ 也服从正态分布。具体来说，其分布为：\n$$\n\\hat{\\beta} \\sim \\mathcal{N}\\left(\\beta, \\sigma^{2}(X^{\\top}X)^{-1}\\right)\n$$\n目标泛函为 $f(\\beta) = x_{a}^{\\top}\\beta - x_{b}^{\\top}\\beta$。我们可以将其表示为 $\\beta$ 各分量的线性组合：\n$$\nf(\\beta) = (x_{a} - x_{b})^{\\top}\\beta\n$$\n让我们定义一个向量 $c = x_{a} - x_{b}$。那么 $f(\\beta) = c^{\\top}\\beta$。$f(\\beta)$ 的自然估计量是 $f(\\hat{\\beta}) = c^{\\top}\\hat{\\beta}$。\n\n由于 $\\hat{\\beta}$ 服从正态分布，其分量的任何线性组合（例如 $c^{\\top}\\hat{\\beta}$）也服从正态分布。\n该估计量的期望值为：\n$$\nE[f(\\hat{\\beta})] = E[c^{\\top}\\hat{\\beta}] = c^{\\top}E[\\hat{\\beta}] = c^{\\top}\\beta = f(\\beta)\n$$\n这表明 $f(\\hat{\\beta})$ 是 $f(\\beta)$ 的无偏估计量。\n该估计量的方差为：\n$$\n\\text{Var}(f(\\hat{\\beta})) = \\text{Var}(c^{\\top}\\hat{\\beta}) = c^{\\top}\\text{Cov}(\\hat{\\beta})c = c^{\\top}\\left(\\sigma^{2}(X^{\\top}X)^{-1}\\right)c = \\sigma^{2}c^{\\top}(X^{\\top}X)^{-1}c\n$$\n因此，我们估计量的分布为：\n$$\nf(\\hat{\\beta}) \\sim \\mathcal{N}\\left(f(\\beta), \\sigma^{2}c^{\\top}(X^{\\top}X)^{-1}c\\right)\n$$\n方差 $\\sigma^{2}$ 通常是未知的，必须从数据中估计。$\\sigma^{2}$ 的无偏估计量是均方误差（MSE），由下式给出：\n$$\n\\hat{\\sigma}^{2} = \\frac{\\text{SSE}}{n-p}\n$$\n其中 $n-p$ 是残差的自由度。一个标准结论是，量 $\\frac{(n-p)\\hat{\\sigma}^{2}}{\\sigma^{2}} = \\frac{\\text{SSE}}{\\sigma^{2}}$ 服从自由度为 $n-p$ 的卡方分布，即 $\\chi_{n-p}^{2}$。\n\n为了构建置信区间，我们构造一个服从学生 t 分布的枢轴量。标准化随机变量\n$$\nZ = \\frac{f(\\hat{\\beta}) - f(\\beta)}{\\sqrt{\\sigma^{2}c^{\\top}(X^{\\top}X)^{-1}c}} \\sim \\mathcal{N}(0, 1)\n$$\n是一个标准正态变量。如果我们用其估计值 $\\hat{\\sigma}^{2}$ 替换未知的 $\\sigma^{2}$，我们得到 t 统计量：\n$$\nT = \\frac{f(\\hat{\\beta}) - f(\\beta)}{\\sqrt{\\hat{\\sigma}^{2}c^{\\top}(X^{\\top}X)^{-1}c}} = \\frac{f(\\hat{\\beta}) - f(\\beta)}{\\text{SE}(f(\\hat{\\beta}))}\n$$\n其中 $\\text{SE}(f(\\hat{\\beta})) = \\sqrt{\\hat{\\sigma}^{2}c^{\\top}(X^{\\top}X)^{-1}c}$ 是估计量 $f(\\hat{\\beta})$ 的标准误。该统计量 $T$ 服从自由度为 $n-p$ 的学生 t 分布。\n$$\nT \\sim t_{n-p}\n$$\n$f(\\beta)$ 的双侧 $(1-\\alpha)$ 置信区间是使用 t 分布的临界值 $t_{\\alpha/2, n-p}$ 构建的，使得 $P(|T| > t_{\\alpha/2, n-p}) = \\alpha$。在我们的例子中，置信水平为 $95\\%$，所以 $\\alpha = 0.05$ 且 $\\alpha/2 = 0.025$。相关的临界值由其上分位数表示，$t_{1-\\alpha/2, n-p} = t_{0.975, n-p}$。该区间由下式给出：\n$$\nf(\\hat{\\beta}) \\pm t_{1-\\alpha/2, n-p} \\times \\text{SE}(f(\\hat{\\beta}))\n$$\n置信区间的中心是 $f(\\hat{\\beta})$，半宽是 $W = t_{1-\\alpha/2, n-p} \\times \\text{SE}(f(\\hat{\\beta}))$。题目要求计算这个半宽。\n\n现在，我们用给定的值进行计算。\n首先，计算向量 $c$:\n$$\nc^{\\top} = x_{a}^{\\top} - x_{b}^{\\top} = \\begin{pmatrix} 1  2  -1 \\end{pmatrix} - \\begin{pmatrix} 1  0  3 \\end{pmatrix} = \\begin{pmatrix} 0  2  -4 \\end{pmatrix}\n$$\n所以，$c = \\begin{pmatrix} 0 \\\\ 2 \\\\ -4 \\end{pmatrix}$。\n\n接下来，计算误差方差的估计值 $\\hat{\\sigma}^{2}$:\n$$\n\\hat{\\sigma}^{2} = \\frac{\\text{SSE}}{n-p} = \\frac{220}{25-3} = \\frac{220}{22} = 10\n$$\n接下来，计算二次型 $c^{\\top}(X^{\\top}X)^{-1}c$:\n$$\nc^{\\top}(X^{\\top}X)^{-1} = \\begin{pmatrix} 0  2  -4 \\end{pmatrix} \\begin{pmatrix}\n0.50  0.02  -0.01 \\\\\n0.02  0.20  0.00 \\\\\n-0.01  0.00  0.10\n\\end{pmatrix}\n$$\n$$\n= \\begin{pmatrix} (0)(0.50)+(2)(0.02)+(-4)(-0.01)  (0)(0.02)+(2)(0.20)+(-4)(0.00)  (0)(-0.01)+(2)(0.00)+(-4)(0.10) \\end{pmatrix}\n$$\n$$\n= \\begin{pmatrix} 0+0.04+0.04  0+0.40+0  0+0-0.40 \\end{pmatrix} = \\begin{pmatrix} 0.08  0.40  -0.40 \\end{pmatrix}\n$$\n现在，乘以 $c$:\n$$\nc^{\\top}(X^{\\top}X)^{-1}c = \\begin{pmatrix} 0.08  0.40  -0.40 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 2 \\\\ -4 \\end{pmatrix}\n$$\n$$\n= (0.08)(0) + (0.40)(2) + (-0.40)(-4) = 0 + 0.80 + 1.60 = 2.40\n$$\n现在，我们可以计算 $f(\\hat{\\beta})$ 的标准误的平方：\n$$\n\\text{SE}(f(\\hat{\\beta}))^{2} = \\hat{\\sigma}^{2} c^{\\top}(X^{\\top}X)^{-1}c = (10)(2.40) = 24\n$$\n标准误是：\n$$\n\\text{SE}(f(\\hat{\\beta})) = \\sqrt{24}\n$$\n自由度为 $n-p=22$。$95\\%$ 置信区间的临界值由题给为 $t_{0.975, 22} = 2.074$。\n\n置信区间的半宽是：\n$$\nW = t_{0.975, 22} \\times \\text{SE}(f(\\hat{\\beta})) = 2.074 \\times \\sqrt{24}\n$$\n数值上，$\\sqrt{24} \\approx 4.898979...$\n$$\nW = 2.074 \\times 4.898979... \\approx 10.158683...\n$$\n将此结果四舍五入到四位有效数字得到 $10.16$。\n置信区间的中心是 $f(\\hat{\\beta}) = c^{\\top}\\hat{\\beta}$，我们不需要计算它。半宽是所要求的值。\n半宽是 $10.16$。", "answer": "$$\n\\boxed{10.16}\n$$", "id": "3176582"}, {"introduction": "经典的统计公式，如标准误的解析解，通常建立在一系列严格的假设之上，而这些假设在真实世界数据中可能并不成立。本练习是一个动手编程任务，旨在挑战你超越这些经典公式，去实现并应用像自助法 (bootstrap) 和刀切法 (jackknife) 这样的现代重采样方法。通过在不同情景下（尤其是在存在高杠杆点等棘手情况下）比较这些计算方法与传统解析方法的结果，你将对稳健统计推断的原理和实践获得深刻的理解 [@problem_id:3176572]。", "problem": "考虑经典的线性回归设定，其中观测响应向量 $y \\in \\mathbb{R}^n$ 被建模为 $y = X \\beta + \\varepsilon$，其中设计矩阵为 $X \\in \\mathbb{R}^{n \\times p}$，系数向量为 $\\beta \\in \\mathbb{R}^p$，噪声向量为 $\\varepsilon \\in \\mathbb{R}^n$。假设 $\\varepsilon$ 具有零均值和有限方差，并使用普通最小二乘 (OLS) 估计。在本问题中，您将使用三种方法——经典线性模型假设下的解析方法、通过留一法 (leave-one-out) 计算的刀切法 (jackknife) 和非参数自助法 (nonparametric bootstrap)——来估计单个系数 $\\hat{\\beta}_j$ 的标准误 (SE)，然后为杠杆敏感 (leverage-sensitive) 的设计构建并比较 $95\\%$ 置信区间 (CI)。\n\n您必须实现一个程序，该程序：\n- 推导普通最小二乘 (OLS) 系数估计值，其中普通最小二乘 (OLS) 意味着选择 $\\hat{\\beta}$ 以最小化残差平方和。\n- 使用经典线性模型假设和根据残差估计的误差方差，获得所选坐标 $\\hat{\\beta}_j$ 的解析标准误 (SE)。使用自由度为 $n - p$ 的学生t分布 (Student's $t$ distribution) 构建一个双侧 $95\\%$ 置信区间 (CI)。\n- 通过依次系统地排除每个观测值并汇总留一法估计值，计算 $\\hat{\\beta}_j$ 的刀切法标准误 (SE)。使用与解析置信区间中相同的基于t分布的分位数，构建一个双侧 $95\\%$ 置信区间 (CI)。\n- 通过对 $(X, y)$ 的行进行有放回重抽样（指定次数为 $B$ 次）来计算自助法标准误 (bootstrap SE) 和一个百分位 $95\\%$ 置信区间 (CI)，为每个自助样本重新估计 $\\hat{\\beta}_j$，并取 $2.5\\%$ 和 $97.5\\%$ 处的经验分位数。\n\n您必须关注杠杆敏感的设计（即，其中 $X$ 的某些行对拟合产生异常高影响的设计）。实验是完全指定且确定性的：您的程序必须使用提供的种子和参数生成 $y$，然后报告每个测试用例所要求的指标。\n\n您必须使用的基本原理：\n- 经典线性模型下的普通最小二乘 (OLS) 估计量。\n- 回归设计中的杠杆率概念（$X$ 中能够强烈影响拟合的行）。\n- 刀切法和自助法的重抽样原理。\n\n请勿在问题陈述中使用或引用简便公式。您的实现应从这些原理出发，符合逻辑地进行。\n\n测试套件：\n- 案例 A（平衡设计，低杠杆）：\n  - 设 $n = 40, p = 2$，并让 $x$ 在 $[-2, 2]$ 上均匀分布 $40$ 个点。定义 $X = [\\mathbf{1}, x]$（一个截距和单个预测变量）。\n  - 真实系数 $\\beta^\\star = [1.0, 2.0]$。\n  - 噪声是独立同正态分布的，标准差为 $\\sigma = 0.5$。\n  - 使用随机种子 $123$ 生成 $y = X \\beta^\\star + \\varepsilon$。\n  - 目标索引 $j = 1$（$x$ 上的斜率）。\n  - 自助法重复次数 $B = 1000$，自助法种子为 $321$。\n\n- 案例 B（一个极端杠杆点，维度相同）：\n  - 设 $n = 40, p = 2$，并让 $x$ 在 $[-1, 1]$ 上均匀分布 $40$ 个点，但将第一个元素设置为 $10.0$。定义 $X = [\\mathbf{1}, x]$。\n  - 真实系数 $\\beta^\\star = [1.0, 2.0]$。\n  - 噪声是独立同正态分布的，标准差为 $\\sigma = 0.5$。\n  - 使用随机种子 $456$ 生成 $y = X \\beta^\\star + \\varepsilon$。\n  - 目标索引 $j = 1$（$x$ 上的斜率）。\n  - 自助法重复次数 $B = 1000$，自助法种子为 $654$。\n\n- 案例 C（存在杠杆点的近似共线性，包括截距在内的三个预测变量）：\n  - 设 $n = 12, p = 3$。让 $x_1$ 在 $[-1, 1]$ 上均匀分布 $12$ 个点，并定义 $x_2 = 0.98 x_1 + 0.02 \\sin(k)$，其中 $k = 0, 1, \\dots, 11$，然后设置 $x_1[11] = 5.0$ 和 $x_2[11] = 4.9$。定义 $X = [\\mathbf{1}, x_1, x_2]$。\n  - 真实系数 $\\beta^\\star = [0.5, 1.5, -0.5]$。\n  - 噪声是独立同正态分布的，标准差为 $\\sigma = 0.8$。\n  - 使用随机种子 $789$ 生成 $y = X \\beta^\\star + \\varepsilon$。\n  - 目标索引 $j = 2$（$x_2$ 上的系数）。\n  - 自助法重复次数 $B = 2000$，自助法种子为 $987$。\n\n对于每个测试用例，按顺序计算并返回：\n1. $\\hat{\\beta}_j$ 的解析标准误 (SE)（一个浮点数）。\n2. $\\hat{\\beta}_j$ 的自助法标准误 (SE)（一个浮点数）。\n3. $\\hat{\\beta}_j$ 的刀切法标准误 (SE)（一个浮点数）。\n4. 解析置信区间宽度（一个浮点数）。\n5. 自助法百分位置信区间宽度（一个浮点数）。\n6. 刀切法置信区间宽度（一个浮点数）。\n7. 解析置信区间是否包含真实系数 $\\beta^\\star_j$？（一个布尔值）。\n8. 自助法百分位置信区间是否包含 $\\beta^\\star_j$？（一个布尔值）。\n9. 刀切法置信区间是否包含 $\\beta^\\star_j$？（一个布尔值）。\n\n您的程序应生成单行输出，其中包含一个逗号分隔的列表，该列表包含三个用方括号括起来的、按测试用例排列的列表，严格按照测试套件的顺序。例如，最终打印的行必须如下所示：\n$[[r_{A,1}, r_{A,2}, \\dots, r_{A,9}], [r_{B,1}, \\dots, r_{B,9}], [r_{C,1}, \\dots, r_{C,9}]]$，\n其中所有 $r_{*,*}$ 都是为每个案例计算的浮点数或布尔值。不需要单位；将量表示为无量纲的浮点数和布尔值。", "solution": "该问题被评估为有效，其科学基础扎根于统计回归理论，问题设定良好，所有必要的参数和条件都已指定，并且其表述是客观的。我们将继续推导必要的公式并构建解决方案。\n\n### 1. 普通最小二乘 (OLS) 估计\n\n经典线性回归模型由 $y = X \\beta + \\varepsilon$ 给出，其中 $y \\in \\mathbb{R}^n$ 是响应向量，$X \\in \\mathbb{R}^{n \\times p}$ 是设计矩阵，$\\beta \\in \\mathbb{R}^p$ 是系数向量，$\\varepsilon \\in \\mathbb{R}^n$ 是噪声向量。普通最小二乘 (OLS) 估计量，记为 $\\hat{\\beta}$，是使残差平方和 (RSS) 最小化的 $\\beta$ 值：\n$$\nS(\\beta) = \\|y - X\\beta\\|^2_2 = (y - X\\beta)^\\top (y - X\\beta)\n$$\n为了找到最小值，我们对 $S(\\beta)$ 关于 $\\beta$ 求导，并令梯度为零：\n$$\n\\frac{\\partial S(\\beta)}{\\partial \\beta} = -2X^\\top(y - X\\beta) = 0\n$$\n这导出了正规方程：\n$$\n(X^\\top X) \\hat{\\beta} = X^\\top y\n$$\n假设矩阵 $X^\\top X$ 是可逆的（即，$X$ 具有满列秩），则唯一的 OLS 解为：\n$$\n\\hat{\\beta} = (X^\\top X)^{-1} X^\\top y\n$$\n该估计量在 $y$ 上是线性的。为了数值稳定性，最好直接求解线性方程组，而不是计算 $X^\\top X$ 的逆矩阵。\n\n### 2. 解析标准误和置信区间\n\n在经典线性模型 (CLM) 假设下，误差 $\\varepsilon_i$ 不相关且具有恒定方差 $\\sigma^2$，即 $\\text{Cov}(\\varepsilon) = \\sigma^2 I_n$，其中 $I_n$ 是 $n \\times n$ 的单位矩阵。OLS 估计量 $\\hat{\\beta}$ 的协方差矩阵是：\n$$\n\\text{Cov}(\\hat{\\beta}) = \\text{Cov}((X^\\top X)^{-1} X^\\top y) = (X^\\top X)^{-1} X^\\top \\text{Cov}(y) X (X^\\top X)^{-1}\n$$\n由于 $\\text{Cov}(y) = \\text{Cov}(X\\beta + \\varepsilon) = \\text{Cov}(\\varepsilon) = \\sigma^2 I_n$，这可以简化为：\n$$\n\\text{Cov}(\\hat{\\beta}) = \\sigma^2 (X^\\top X)^{-1}\n$$\n单个系数估计 $\\hat{\\beta}_j$（对于 $j \\in \\{0, 1, \\dots, p-1\\}$）的方差是该矩阵的第 $j$ 个对角元素：\n$$\n\\text{Var}(\\hat{\\beta}_j) = \\sigma^2 \\left[(X^\\top X)^{-1}\\right]_{jj}\n$$\n在实践中，真实的误差方差 $\\sigma^2$ 是未知的，必须从数据中估计。$\\sigma^2$ 的一个无偏估计量是：\n$$\n\\hat{\\sigma}^2 = \\frac{1}{n-p} \\sum_{i=1}^n e_i^2 = \\frac{\\text{RSS}}{n-p}\n$$\n其中 $e = y - X\\hat{\\beta}$ 是残差。分母 $n-p$ 代表残差的自由度。\n$\\hat{\\beta}_j$ 的估计标准误 (SE) 是估计方差的平方根：\n$$\n\\text{SE}_{\\text{analytic}}(\\hat{\\beta}_j) = \\sqrt{\\hat{\\sigma}^2 \\left[(X^\\top X)^{-1}\\right]_{jj}} = \\hat{\\sigma} \\sqrt{\\left[(X^\\top X)^{-1}\\right]_{jj}}\n$$\n假设误差也服从正态分布，则量 $\\frac{\\hat{\\beta}_j - \\beta_j}{\\text{SE}(\\hat{\\beta}_j)}$ 服从自由度为 $n-p$ 的学生t分布 (Student's $t$ distribution)。关于 $\\beta_j$ 的一个双侧 $(1-\\alpha) \\cdot 100\\%$ 置信区间 (CI) 构建如下：\n$$\n\\text{CI}_{\\text{analytic}} = \\left[ \\hat{\\beta}_j - t_{1-\\alpha/2, n-p} \\cdot \\text{SE}_{\\text{analytic}}(\\hat{\\beta}_j), \\quad \\hat{\\beta}_j + t_{1-\\alpha/2, n-p} \\cdot \\text{SE}_{\\text{analytic}}(\\hat{\\beta}_j) \\right]\n$$\n其中 $t_{1-\\alpha/2, n-p}$ 是 $t_{n-p}$ 分布的 $(1-\\alpha/2)$-分位数。对于 $95\\%$ 置信区间，$\\alpha = 0.05$。该区间的宽度为 $2 \\cdot t_{0.975, n-p} \\cdot \\text{SE}_{\\text{analytic}}(\\hat{\\beta}_j)$。\n\n### 3. 刀切法标准误和置信区间\n\n刀切法是一种重抽样技术，它通过每次系统地排除一个观测值来估计估计量的方差。设 $\\hat{\\beta}$ 是来自大小为 $n$ 的完整数据集的估计值。\n1. 对于每个 $i = 1, \\dots, n$，通过从 $X$ 和 $y$ 中移除第 $i$ 行来形成第 $i$ 个留一数据集，记为 $(X_{(-i)}, y_{(-i)})$。\n2. 为这 $n$ 个数据集中的每一个计算 OLS 估计：$\\hat{\\beta}_{(-i)} = (X_{(-i)}^\\top X_{(-i)})^{-1} X_{(-i)}^\\top y_{(-i)}$。\n3. 我们关注目标系数，从而得到 $n$ 个估计值 $\\{\\hat{\\beta}_{j,(-1)}, \\dots, \\hat{\\beta}_{j,(-n)}\\}$。\n4. $\\hat{\\beta}_j$ 的刀切法方差估计由下式给出：\n$$\n\\widehat{\\text{Var}}_{\\text{jack}}(\\hat{\\beta}_j) = \\frac{n-1}{n} \\sum_{i=1}^n (\\hat{\\beta}_{j,(-i)} - \\bar{\\beta}_j)^2\n$$\n其中 $\\bar{\\beta}_j = \\frac{1}{n} \\sum_{i=1}^n \\hat{\\beta}_{j,(-i)}$ 是留一法估计值的均值。\n5. 刀切法标准误是该方差的平方根：\n$$\n\\text{SE}_{\\text{jack}}(\\hat{\\beta}_j) = \\sqrt{\\widehat{\\text{Var}}_{\\text{jack}}(\\hat{\\beta}_j)}\n$$\n按照规定，刀切法置信区间是围绕全样本估计值 $\\hat{\\beta}_j$ 对称构建的，使用与解析方法相同的 $t$-分位数：\n$$\n\\text{CI}_{\\text{jack}} = \\left[ \\hat{\\beta}_j - t_{1-\\alpha/2, n-p} \\cdot \\text{SE}_{\\text{jack}}(\\hat{\\beta}_j), \\quad \\hat{\\beta}_j + t_{1-\\alpha/2, n-p} \\cdot \\text{SE}_{\\text{jack}}(\\hat{\\beta}_j) \\right]\n$$\n其宽度为 $2 \\cdot t_{0.975, n-p} \\cdot \\text{SE}_{\\text{jack}}(\\hat{\\beta}_j)$。\n\n### 4. 自助法标准误和置信区间\n\n非参数自助法通过从原始数据中有放回地重抽样来生成重复数据集。这种方法不依赖于经典线性模型假设。\n1. 指定自助法重复次数 $B$。\n2. 对于每次重复 $b = 1, \\dots, B$：\n    a. 通过从原始数据 $(X, y)$ 中有放回地抽取 $n$ 行，创建一个自助样本 $(X^*_b, y^*_b)$。\n    b. 在此自助样本上计算 OLS 估计：$\\hat{\\beta}^*_b = (X^{*\\top}_b X^*_b)^{-1} X^{*\\top}_b y^*_b$。请注意，某些自助样本可能会导致 $X^{*\\top}_b X^*_b$ 矩阵是奇异的，尤其是在高杠杆设计中。在这种情况下，可以使用广义逆（例如 Moore-Penrose 伪逆）来找到最小范数的最小二乘解。\n3. 这个过程产生 $B$ 个自助系数向量的集合，$\\{\\hat{\\beta}^*_1, \\dots, \\hat{\\beta}^*_B\\}$。考虑目标系数的分布，$\\{\\hat{\\beta}^*_{j,1}, \\dots, \\hat{\\beta}^*_{j,B}\\}$。\n4. 自助法标准误是这些自助估计值的样本标准差：\n$$\n\\text{SE}_{\\text{boot}}(\\hat{\\beta}_j) = \\sqrt{\\frac{1}{B-1} \\sum_{b=1}^B (\\hat{\\beta}^*_{j,b} - \\bar{\\beta}^*_j)^2}, \\text{ 其中 } \\bar{\\beta}^*_j = \\frac{1}{B} \\sum_{b=1}^B \\hat{\\beta}^*_{j,b}\n$$\n5. 百分位自助置信区间是直接从自助估计值的经验分布中构建的。$(1-\\alpha) \\cdot 100\\%$ 百分位置信区间由排序后的自助重复样本的 $(\\alpha/2)$ 和 $(1-\\alpha/2)$ 分位数给出。对于 $95\\%$ 置信区间：\n$$\n\\text{CI}_{\\text{percentile}} = [q_{0.025}, q_{0.975}]\n$$\n其中 $q_{p}$ 是集合 $\\{\\hat{\\beta}^*_{j,1}, \\dots, \\hat{\\beta}^*_{j,B}\\}$ 的第 $p$ 个经验分位数。该区间的宽度为 $q_{0.975} - q_{0.025}$。如果自助分布是偏斜的，该方法具有生成非对称区间的优点，这种情况在存在影响点时经常发生。\n\n程序现在将为每个指定的测试用例实现这三种方法。", "answer": "```python\nimport numpy as np\nfrom scipy.stats import t\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print results.\n    \"\"\"\n\n    test_cases = [\n        # Case A: Balanced design, low leverage\n        {\n            \"n\": 40, \"p\": 2,\n            \"x_def\": lambda n: (np.c_[np.ones(n), np.linspace(-2, 2, n)], None),\n            \"beta_star\": np.array([1.0, 2.0]),\n            \"sigma\": 0.5,\n            \"y_seed\": 123,\n            \"target_j\": 1,\n            \"B\": 1000,\n            \"bootstrap_seed\": 321\n        },\n        # Case B: One extreme leverage point\n        {\n            \"n\": 40, \"p\": 2,\n            \"x_def\": lambda n: (\n                np.c_[np.ones(n), (lambda x: (x.__setitem__(0, 10.0), x)[1])(np.linspace(-1, 1, n))], None\n            ),\n            \"beta_star\": np.array([1.0, 2.0]),\n            \"sigma\": 0.5,\n            \"y_seed\": 456,\n            \"target_j\": 1,\n            \"B\": 1000,\n            \"bootstrap_seed\": 654\n        },\n        # Case C: Near-collinearity with a leverage point\n        {\n            \"n\": 12, \"p\": 3,\n            \"x_def\": lambda n: (\n                (lambda x1, x2: (\n                    (x1.__setitem__(n-1, 5.0), x2.__setitem__(n-1, 4.9)),\n                    np.c_[np.ones(n), x1, x2]\n                )[1])(np.linspace(-1, 1, n), 0.98 * np.linspace(-1, 1, n) + 0.02 * np.sin(np.arange(n))),\n                None\n            ),\n            \"beta_star\": np.array([0.5, 1.5, -0.5]),\n            \"sigma\": 0.8,\n            \"y_seed\": 789,\n            \"target_j\": 2,\n            \"B\": 2000,\n            \"bootstrap_seed\": 987\n        }\n    ]\n\n    all_results = []\n    for case in test_cases:\n        results = run_case(case)\n        all_results.append(results)\n\n    # Convert boolean to lower case string representation for printing\n    formatted_results = []\n    for case_res in all_results:\n        str_res = []\n        for v in case_res:\n            if isinstance(v, float):\n                str_res.append(f\"{v:.6f}\")\n            elif isinstance(v, bool) or isinstance(v, np.bool_):\n                str_res.append(str(v).lower())\n            else:\n                str_res.append(str(v))\n        formatted_results.append(f\"[{','.join(str_res)}]\")\n\n    print(f\"[{','.join(formatted_results)}]\")\n\ndef get_ols_estimate(X, y):\n    \"\"\"\n    Computes OLS estimates using np.linalg.lstsq for numerical stability.\n    \"\"\"\n    beta_hat, _, _, _ = np.linalg.lstsq(X, y, rcond=None)\n    return beta_hat\n\ndef run_case(params):\n    \"\"\"\n    Runs a single test case and computes all required metrics.\n    \"\"\"\n    n, p = params[\"n\"], params[\"p\"]\n    X, _ = params[\"x_def\"](n)\n    beta_star = params[\"beta_star\"]\n    sigma = params[\"sigma\"]\n    y_seed = params[\"y_seed\"]\n    target_j = params[\"target_j\"]\n    B = params[\"B\"]\n    bootstrap_seed = params[\"bootstrap_seed\"]\n    \n    # Generate response vector y\n    rng_y = np.random.default_rng(y_seed)\n    epsilon = rng_y.normal(0, sigma, n)\n    y = X @ beta_star + epsilon\n\n    # --- Full Sample OLS ---\n    beta_hat = get_ols_estimate(X, y)\n    beta_hat_j = beta_hat[target_j]\n\n    # --- 1. Analytic Method ---\n    residuals = y - X @ beta_hat\n    rss = residuals.T @ residuals\n    sigma_hat_sq = rss / (n - p)\n    sigma_hat = np.sqrt(sigma_hat_sq)\n    \n    # Using np.linalg.inv is acceptable here as X.T @ X is small.\n    # For larger p, solving a system would be better.\n    try:\n        C = np.linalg.inv(X.T @ X)\n        analytic_se = sigma_hat * np.sqrt(C[target_j, target_j])\n        \n        df = n - p\n        t_crit = t.ppf(0.975, df)\n        analytic_ci_half_width = t_crit * analytic_se\n        analytic_ci = [beta_hat_j - analytic_ci_half_width, beta_hat_j + analytic_ci_half_width]\n        analytic_ci_width = 2 * analytic_ci_half_width\n        analytic_contains_true = (analytic_ci[0] = beta_star[target_j] = analytic_ci[1])\n    except np.linalg.LinAlgError:\n        # Handle cases where X is rank deficient from the start\n        analytic_se = np.nan\n        analytic_ci_width = np.nan\n        analytic_contains_true = False\n\n    # --- 2. Bootstrap Method ---\n    rng_boot = np.random.default_rng(bootstrap_seed)\n    bootstrap_betas_j = []\n    \n    for _ in range(B):\n        indices = rng_boot.choice(n, size=n, replace=True)\n        X_boot, y_boot = X[indices], y[indices]\n        \n        # lstsq handles potential singularity in bootstrap samples\n        beta_boot = get_ols_estimate(X_boot, y_boot)\n        bootstrap_betas_j.append(beta_boot[target_j])\n\n    bootstrap_betas_j = np.array(bootstrap_betas_j)\n    \n    bootstrap_se = np.std(bootstrap_betas_j, ddof=1)\n    \n    # Percentile CI\n    bootstrap_ci = np.quantile(bootstrap_betas_j, [0.025, 0.975])\n    bootstrap_ci_width = bootstrap_ci[1] - bootstrap_ci[0]\n    bootstrap_contains_true = (bootstrap_ci[0] = beta_star[target_j] = bootstrap_ci[1])\n\n    # --- 3. Jackknife Method ---\n    jackknife_betas_j = []\n    for i in range(n):\n        X_jack = np.delete(X, i, axis=0)\n        y_jack = np.delete(y, i, axis=0)\n        \n        beta_jack = get_ols_estimate(X_jack, y_jack)\n        jackknife_betas_j.append(beta_jack[target_j])\n    \n    jackknife_betas_j = np.array(jackknife_betas_j)\n    beta_j_bar_jack = np.mean(jackknife_betas_j)\n    \n    jackknife_var = ((n - 1) / n) * np.sum((jackknife_betas_j - beta_j_bar_jack)**2)\n    jackknife_se = np.sqrt(jackknife_var)\n    \n    # CI using t-critical value from analytic method\n    jackknife_ci_half_width = t_crit * jackknife_se\n    jackknife_ci = [beta_hat_j - jackknife_ci_half_width, beta_hat_j + jackknife_ci_half_width]\n    jackknife_ci_width = 2 * jackknife_ci_half_width\n    jackknife_contains_true = (jackknife_ci[0] = beta_star[target_j] = jackknife_ci[1])\n    \n    return [\n        analytic_se, bootstrap_se, jackknife_se,\n        analytic_ci_width, bootstrap_ci_width, jackknife_ci_width,\n        analytic_contains_true, bootstrap_contains_true, jackknife_contains_true\n    ]\n\n\nif __name__ == '__main__':\n    solve()\n\n```", "id": "3176572"}]}