{"hands_on_practices": [{"introduction": "我们常希望估计量是无偏的，但这并非总是最优选择。在统计学中，著名的“偏差-方差权衡”告诉我们，有时引入少量偏差可以显著降低估计量的方差，从而获得更低的总误差。这个练习将引导你通过一个具体的岭回归（Ridge Regression）实例，亲手推导一个有偏估计量的偏差和方差，并找出最小化均方误差的最优正则化参数 $\\lambda$。通过这个分析计算过程，你将对这一核心概念有更深刻的理解。[@problem_id:3155654]", "problem": "考虑一个监督学习线性模型，其固定设计矩阵为 $X \\in \\mathbb{R}^{3 \\times 2}$，未知参数向量为 $\\beta \\in \\mathbb{R}^{2}$，响应向量为 $Y \\in \\mathbb{R}^{3}$，满足\n$$\nY = X \\beta + \\varepsilon,\n$$\n其中 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2} I_{3})$ 且 $I_{3}$ 表示 $3 \\times 3$ 单位矩阵。目标参数是由 $\\mu = X \\beta$ 定义的平均响应向量 $\\mu \\in \\mathbb{R}^{3}$。考虑 $\\beta$ 的岭回归收缩估计量，\n$$\n\\hat{\\beta}_{\\lambda} = (X^{\\top} X + \\lambda I_{2})^{-1} X^{\\top} Y,\n$$\n其中收缩参数 $\\lambda \\geq 0$，以及导出的平均响应估计量\n$$\n\\hat{\\mu}_{\\lambda} = X \\hat{\\beta}_{\\lambda}。\n$$\n\n请从第一性原理出发，使用上述期望、方差和线性模型的定义。除了这些定义之外，不要假定任何其他结果。你的任务是：\n1. 推导 $\\hat{\\mu}_{\\lambda}$ 相对于 $\\mu$ 的偏差向量，表示为 $\\lambda$ 和 $X$ 的函数，并推导 $\\hat{\\mu}_{\\lambda}$ 的协方差矩阵，表示为 $\\lambda$、$X$ 和 $\\sigma^{2}$ 的函数。\n2. 使用你的推导，对于固定的设计矩阵\n$$\nX = \\begin{pmatrix}\n1  0 \\\\\n0  1 \\\\\n0  0\n\\end{pmatrix}，\n$$\n将期望平方误差 $E\\!\\left[\\|\\hat{\\mu}_{\\lambda} - \\mu\\|^{2}\\right]$ 表示为 $\\lambda$ 的函数。\n3. 对于 $\\beta = \\begin{pmatrix} 2 \\\\ -1 \\end{pmatrix}$ 和 $\\sigma^{2} = 3$，确定使 $E\\!\\left[\\|\\hat{\\mu}_{\\lambda} - \\mu\\|^{2}\\right]$ 最小的 $\\lambda \\geq 0$ 的值。请提供精确值作为最终答案，无需四舍五入。", "solution": "该问题是有效的，因为它具有科学依据、提法明确且客观。它包含正则化线性模型理论中的标准推导。我们将按顺序完成这三个任务。\n\n线性模型由 $Y = X \\beta + \\varepsilon$ 给出，其中 $Y \\in \\mathbb{R}^{3}$，$X \\in \\mathbb{R}^{3 \\times 2}$，$\\beta \\in \\mathbb{R}^{2}$，误差项 $\\varepsilon \\in \\mathbb{R}^{3}$ 服从多元正态分布 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2} I_{3})$。这意味着 $E[\\varepsilon] = 0$ 且 $\\text{Cov}(\\varepsilon) = E[\\varepsilon \\varepsilon^{\\top}] = \\sigma^{2} I_{3}$。平均响应向量是 $\\mu = X \\beta$。$\\beta$ 的岭回归估计量和导出的 $\\mu$ 的估计量由以下公式给出：\n$$\n\\hat{\\beta}_{\\lambda} = (X^{\\top} X + \\lambda I_{2})^{-1} X^{\\top} Y\n$$\n$$\n\\hat{\\mu}_{\\lambda} = X \\hat{\\beta}_{\\lambda} = X (X^{\\top} X + \\lambda I_{2})^{-1} X^{\\top} Y\n$$\n其中 $\\lambda \\geq 0$。\n\n**1. $\\hat{\\mu}_{\\lambda}$ 的偏差向量和协方差矩阵的推导**\n\n首先，我们推导估计量 $\\hat{\\mu}_{\\lambda}$ 的偏差。偏差定义为 $\\text{Bias}(\\hat{\\mu}_{\\lambda}) = E[\\hat{\\mu}_{\\lambda}] - \\mu$。\n为了求出 $\\hat{\\mu}_{\\lambda}$ 的期望，我们使用期望算子的线性性。响应向量 $Y$ 的期望是：\n$$\nE[Y] = E[X \\beta + \\varepsilon] = X \\beta + E[\\varepsilon] = X \\beta + 0 = X \\beta\n$$\n因为 $X$ 和 $\\beta$ 被认为是固定的（非随机的）。\n现在我们计算 $\\hat{\\mu}_{\\lambda}$ 的期望：\n$$\nE[\\hat{\\mu}_{\\lambda}] = E[X (X^{\\top} X + \\lambda I_{2})^{-1} X^{\\top} Y]\n$$\n矩阵项相对于期望是常数，所以我们可以将其提出：\n$$\nE[\\hat{\\mu}_{\\lambda}] = X (X^{\\top} X + \\lambda I_{2})^{-1} X^{\\top} E[Y] = X (X^{\\top} X + \\lambda I_{2})^{-1} X^{\\top} (X \\beta)\n$$\n将此代入偏差的定义中：\n$$\n\\text{Bias}(\\hat{\\mu}_{\\lambda}) = X (X^{\\top} X + \\lambda I_{2})^{-1} X^{\\top} X \\beta - \\mu = X (X^{\\top} X + \\lambda I_{2})^{-1} X^{\\top} X \\beta - X \\beta\n$$\n从左边提出 $X$，从右边提出 $\\beta$：\n$$\n\\text{Bias}(\\hat{\\mu}_{\\lambda}) = X \\left[ (X^{\\top} X + \\lambda I_{2})^{-1} X^{\\top} X - I_{2} \\right] \\beta\n$$\n为了化简括号中的表达式，令 $A = X^{\\top} X$。表达式变为 $(A + \\lambda I_{2})^{-1} A - I_{2}$。我们可以写作 $I_{2} = (A + \\lambda I_{2})^{-1} (A + \\lambda I_{2})$。\n$$\n(A + \\lambda I_{2})^{-1} A - (A + \\lambda I_{2})^{-1} (A + \\lambda I_{2}) = (A + \\lambda I_{2})^{-1} (A - (A + \\lambda I_{2})) = (A + \\lambda I_{2})^{-1} (-\\lambda I_{2}) = -\\lambda (A + \\lambda I_{2})^{-1}\n$$\n将 $A = X^{\\top} X$ 代回，偏差向量为：\n$$\n\\text{Bias}(\\hat{\\mu}_{\\lambda}) = X \\left[ -\\lambda (X^{\\top} X + \\lambda I_{2})^{-1} \\right] \\beta = -\\lambda X (X^{\\top} X + \\lambda I_{2})^{-1} \\beta\n$$\n接下来，我们推导协方差矩阵 $\\text{Cov}(\\hat{\\mu}_{\\lambda})$。我们使用性质：对于一个随机向量 $Z$ 和一个常数矩阵 $M$，有 $\\text{Cov}(MZ) = M \\text{Cov}(Z) M^{\\top}$。\n令 $H_{\\lambda} = X (X^{\\top} X + \\lambda I_{2})^{-1} X^{\\top}$。则 $\\hat{\\mu}_{\\lambda} = H_{\\lambda} Y$。\n$$\n\\text{Cov}(\\hat{\\mu}_{\\lambda}) = \\text{Cov}(H_{\\lambda} Y) = H_{\\lambda} \\text{Cov}(Y) H_{\\lambda}^{\\top}\n$$\n$Y$ 的协方差为：\n$$\n\\text{Cov}(Y) = \\text{Cov}(X \\beta + \\varepsilon) = \\text{Cov}(\\varepsilon) = \\sigma^{2} I_{3}\n$$\n将此代入 $\\hat{\\mu}_{\\lambda}$ 的协方差表达式中：\n$$\n\\text{Cov}(\\hat{\\mu}_{\\lambda}) = H_{\\lambda} (\\sigma^{2} I_{3}) H_{\\lambda}^{\\top} = \\sigma^{2} H_{\\lambda} H_{\\lambda}^{\\top}\n$$\n矩阵 $H_{\\lambda}$ 是对称的，因为 $(X^{\\top}X + \\lambda I_2)$ 是对称的，因此其逆矩阵也是对称的。\n$H_{\\lambda}^{\\top} = (X (X^{\\top} X + \\lambda I_{2})^{-1} X^{\\top})^{\\top} = (X^{\\top})^{\\top} ((X^{\\top} X + \\lambda I_{2})^{-1})^{\\top} X^{\\top} = X (X^{\\top} X + \\lambda I_{2})^{-1} X^{\\top} = H_{\\lambda}$。\n所以，$\\text{Cov}(\\hat{\\mu}_{\\lambda}) = \\sigma^{2} H_{\\lambda} H_{\\lambda} = \\sigma^{2} H_{\\lambda}^{2}$。\n代入 $H_{\\lambda}$ 的定义：\n$$\n\\text{Cov}(\\hat{\\mu}_{\\lambda}) = \\sigma^{2} \\left( X (X^{\\top} X + \\lambda I_{2})^{-1} X^{\\top} \\right)^{2} = \\sigma^{2} X (X^{\\top} X + \\lambda I_{2})^{-1} X^{\\top} X (X^{\\top} X + \\lambda I_{2})^{-1} X^{\\top}\n$$\n\n**2. 针对特定 $X$ 的期望平方误差**\n\n$\\hat{\\mu}_{\\lambda}$ 的期望平方误差，或称均方误差 (MSE)，是 $E[\\|\\hat{\\mu}_{\\lambda} - \\mu\\|^{2}]$。这可以分解为偏差的平方范数和协方差矩阵的迹（即总方差）之和。\n$E[\\|\\hat{\\mu}_{\\lambda} - \\mu\\|^{2}] = E[(\\hat{\\mu}_{\\lambda} - \\mu)^{\\top}(\\hat{\\mu}_{\\lambda} - \\mu)] = \\|\\text{Bias}(\\hat{\\mu}_{\\lambda})\\|^{2} + \\text{Tr}(\\text{Cov}(\\hat{\\mu}_{\\lambda}))$。\n\n我们给定了具体的设计矩阵：\n$$\nX = \\begin{pmatrix} 1  0 \\\\ 0  1 \\\\ 0  0 \\end{pmatrix}\n$$\n首先，我们计算 $X^{\\top}X$：\n$$\nX^{\\top} X = \\begin{pmatrix} 1  0  0 \\\\ 0  1  0 \\end{pmatrix} \\begin{pmatrix} 1  0 \\\\ 0  1 \\\\ 0  0 \\end{pmatrix} = \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix} = I_{2}\n$$\n现在，我们使用 $X^{\\top}X = I_{2}$ 来具体化偏差和协方差的表达式。\n\n对于偏差项：\n$$\n\\text{Bias}(\\hat{\\mu}_{\\lambda}) = -\\lambda X (I_{2} + \\lambda I_{2})^{-1} \\beta = -\\lambda X ((1+\\lambda)I_{2})^{-1} \\beta = -\\lambda X \\frac{1}{1+\\lambda} I_{2} \\beta = -\\frac{\\lambda}{1+\\lambda} X \\beta = -\\frac{\\lambda}{1+\\lambda} \\mu\n$$\n偏差的平方范数是：\n$$\n\\|\\text{Bias}(\\hat{\\mu}_{\\lambda})\\|^{2} = \\left\\| -\\frac{\\lambda}{1+\\lambda} \\mu \\right\\|^{2} = \\frac{\\lambda^{2}}{(1+\\lambda)^{2}} \\|\\mu\\|^{2}\n$$\n我们注意到，对于这个特定的 $X$，$\\mu = X\\beta = \\begin{pmatrix} \\beta_{1} \\\\ \\beta_{2} \\\\ 0 \\end{pmatrix}$。因此，$\\|\\mu\\|^{2} = \\beta_{1}^{2} + \\beta_{2}^{2} = \\|\\beta\\|^{2}$。\n$$\n\\|\\text{Bias}(\\hat{\\mu}_{\\lambda})\\|^{2} = \\frac{\\lambda^{2}}{(1+\\lambda)^{2}} \\|\\beta\\|^{2}\n$$\n\n对于方差项，我们首先化简协方差矩阵的表达式：\n$$\n\\text{Cov}(\\hat{\\mu}_{\\lambda}) = \\sigma^{2} X(I_{2} + \\lambda I_{2})^{-1}I_{2}(I_{2} + \\lambda I_{2})^{-1}X^{\\top} = \\sigma^{2} X \\left(\\frac{1}{1+\\lambda}I_{2}\\right) I_{2} \\left(\\frac{1}{1+\\lambda}I_{2}\\right) X^{\\top} = \\frac{\\sigma^{2}}{(1+\\lambda)^{2}} X X^{\\top}\n$$\n我们来计算 $XX^{\\top}$：\n$$\nXX^{\\top} = \\begin{pmatrix} 1  0 \\\\ 0  1 \\\\ 0  0 \\end{pmatrix} \\begin{pmatrix} 1  0  0 \\\\ 0  1  0 \\end{pmatrix} = \\begin{pmatrix} 1  0  0 \\\\ 0  1  0 \\\\ 0  0  0 \\end{pmatrix}\n$$\n方差项是协方差矩阵的迹：\n$$\n\\text{Tr}(\\text{Cov}(\\hat{\\mu}_{\\lambda})) = \\text{Tr}\\left( \\frac{\\sigma^{2}}{(1+\\lambda)^{2}} X X^{\\top} \\right) = \\frac{\\sigma^{2}}{(1+\\lambda)^{2}} \\text{Tr}(XX^{\\top})\n$$\n$XX^{\\top}$ 的迹是 $1+1+0=2$。\n$$\n\\text{Tr}(\\text{Cov}(\\hat{\\mu}_{\\lambda})) = \\frac{2\\sigma^{2}}{(1+\\lambda)^{2}}\n$$\n结合偏差项和方差项，期望平方误差为：\n$$\nE[\\|\\hat{\\mu}_{\\lambda} - \\mu\\|^{2}] = \\frac{\\lambda^{2}}{(1+\\lambda)^{2}} \\|\\beta\\|^{2} + \\frac{2\\sigma^{2}}{(1+\\lambda)^{2}} = \\frac{\\lambda^{2}\\|\\beta\\|^{2} + 2\\sigma^{2}}{(1+\\lambda)^{2}}\n$$\n\n**3. 最小化期望平方误差**\n\n我们已知 $\\beta = \\begin{pmatrix} 2 \\\\ -1 \\end{pmatrix}$ 和 $\\sigma^{2} = 3$。首先，我们计算 $\\|\\beta\\|^{2}$：\n$$\n\\|\\beta\\|^{2} = 2^{2} + (-1)^{2} = 4 + 1 = 5\n$$\n将 $\\|\\beta\\|^{2}=5$ 和 $\\sigma^{2}=3$ 代入期望平方误差表达式中：\n$$\nE[\\|\\hat{\\mu}_{\\lambda} - \\mu\\|^{2}] = \\frac{5\\lambda^{2} + 2(3)}{(1+\\lambda)^{2}} = \\frac{5\\lambda^{2} + 6}{(1+\\lambda)^{2}}\n$$\n令 $f(\\lambda) = \\frac{5\\lambda^{2} + 6}{(1+\\lambda)^{2}}$。我们想要找到使 $f(\\lambda)$ 最小的 $\\lambda \\geq 0$ 的值。为此，我们求 $f(\\lambda)$ 关于 $\\lambda$ 的导数并令其为零。使用微分的商法则 $[u/v]' = (u'v - uv')/v^2$：\n$$\nf'(\\lambda) = \\frac{(10\\lambda)(1+\\lambda)^{2} - (5\\lambda^{2} + 6)(2(1+\\lambda))}{((1+\\lambda)^{2})^{2}}\n$$\n对于 $\\lambda \\geq 0$，$1+\\lambda \\neq 0$，所以我们可以通过用 $1+\\lambda$ 除分子和分母来化简：\n$$\nf'(\\lambda) = \\frac{10\\lambda(1+\\lambda) - 2(5\\lambda^{2} + 6)}{(1+\\lambda)^{3}} = \\frac{10\\lambda^{2} + 10\\lambda - 10\\lambda^{2} - 12}{(1+\\lambda)^{3}} = \\frac{10\\lambda - 12}{(1+\\lambda)^{3}}\n$$\n令导数为零以求临界点：\n$$\nf'(\\lambda) = 0 \\implies 10\\lambda - 12 = 0 \\implies 10\\lambda = 12 \\implies \\lambda = \\frac{12}{10} = \\frac{6}{5}\n$$\n临界点是 $\\lambda = \\frac{6}{5}$。该值满足约束条件 $\\lambda \\geq 0$。为了确认这是一个最小值，我们可以考察导数的符号。对于 $\\lambda \\geq 0$，分母 $(1+\\lambda)^{3}$ 是正的。$f'(\\lambda)$ 的符号由分子 $10\\lambda - 12$ 决定。\n- 对于 $0 \\leq \\lambda  \\frac{6}{5}$，$10\\lambda - 12  0$，所以 $f'(\\lambda)  0$。函数 $f(\\lambda)$ 是递减的。\n- 对于 $\\lambda > \\frac{6}{5}$，$10\\lambda - 12 > 0$，所以 $f'(\\lambda) > 0$。函数 $f(\\lambda)$ 是递增的。\n由于函数在 $\\lambda=\\frac{6}{5}$ 之前递减，之后递增，因此 $\\lambda = \\frac{6}{5}$ 是 $\\lambda \\geq 0$ 上的全局最小值。\n\n使期望平方误差最小的 $\\lambda$ 的值是 $\\frac{6}{5}$。", "answer": "$$\n\\boxed{\\frac{6}{5}}\n$$", "id": "3155654"}, {"introduction": "当数据不再“行为良好”时，标准估计量可能会彻底失效。例如，当数据分布呈现“重尾”（heavy tails）特性，即包含极端异常值时，样本均值就不再是中心位置的可靠估计。本练习将介绍一种稳健的替代方案——均值中位数（Median-of-Means）估计量。你将通过编写代码，进行蒙特卡洛模拟，直观地比较它与样本均值在重尾噪声下的表现，从而理解为特定数据特性选择合适估计量的重要性。[@problem_id:3155655]", "problem": "考虑从一个重尾噪声模型中生成的独立同分布 (i.i.d.) 数据点 $X_1, X_2, \\dots, X_n$，该模型定义为 $X_i = \\mu + Z_i$，其中 $Z_i \\sim \\text{Student-}t_\\nu$ 且 $\\nu \\in (1,2)$。设感兴趣的参数为位置参数 $\\mu$，并固定 $\\mu = 1.0$ 和 $\\nu = 1.5$。样本均值统计量为 $\\bar{X} = \\frac{1}{n} \\sum_{i=1}^n X_i$，它是 $\\mu$ 的标准估计量，但已知对重尾分布很敏感。按如下方式定义均值中位数估计量：对于给定的 $n$，选择 $k = \\lfloor \\sqrt{n} \\rfloor$ 个大小相等的组，每组大小为 $m = \\lfloor n/k \\rfloor$。将前 $k \\cdot m$ 个观测值划分为 $k$ 个不相交的组 $G_1, G_2, \\dots, G_k$，其中对所有 $j$ 都有 $|G_j| = m$，计算每个组的均值 $M_j = \\frac{1}{m} \\sum_{i \\in G_j} X_i$，然后定义估计量 $\\widehat{\\mu}_{\\mathrm{MoM}} = \\operatorname{median}(M_1, M_2, \\dots, M_k)$。对于均值中位数估计量，应忽略前 $k \\cdot m$ 个观测值之外的任何剩余观测值。样本均值 $\\bar{X}$ 应使用所有 $n$ 个观测值来计算。\n\n你的任务是实现一个程序，对于一组给定的样本量 $n$ 的测试套件，通过蒙特卡洛模拟比较 $\\bar{X}$ 和 $\\widehat{\\mu}_{\\mathrm{MoM}}$ 在 $\\mu$ 周围的经验集中度。对于每个 $n$，执行 $T$ 次独立试验，其中 $T = 2000$，并在每次试验中从指定的重尾模型中生成一个大小为 $n$ 的新的独立同分布样本。对于每个估计量，计算其相对于 $\\mu$ 的经验平均绝对误差：\n$$\\mathrm{MAE}_{\\mathrm{mean}}(n) = \\frac{1}{T} \\sum_{t=1}^T \\left| \\bar{X}^{(t)} - \\mu \\right|, \\quad \\mathrm{MAE}_{\\mathrm{MoM}}(n) = \\frac{1}{T} \\sum_{t=1}^T \\left| \\widehat{\\mu}_{\\mathrm{MoM}}^{(t)} - \\mu \\right|.$$\n对于每个 $n$，报告比率\n$$R(n) = \\frac{\\mathrm{MAE}_{\\mathrm{mean}}(n)}{\\mathrm{MAE}_{\\mathrm{MoM}}(n)}.$$\n较大的 $R(n)$ 值表示均值中位数估计量相对于样本均值具有更好的集中度。使用固定的伪随机数生成器种子 $12345$，以确保结果是可复现的。\n\n测试套件：\n- 待评估的样本量 $n$：$n \\in \\{1, 10, 30, 100, 300, 1000\\}$。\n- 位置参数：$\\mu = 1.0$。\n- 自由度：$\\nu = 1.5$。\n- 试验次数：$T = 2000$。\n\n边缘情况覆盖：\n- 当 $n = 1$ 时，有 $k = 1$ 和 $m = 1$，因此 $\\widehat{\\mu}_{\\mathrm{MoM}}$ 精确等于 $\\bar{X}$，并且 $R(1)$ 应等于 $1$。\n\n你的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果，顺序与测试套件中的 $n$ 值一致。将每个浮点数四舍五入到 $6$ 位小数。例如，输出格式必须与 $[r_1,r_2,\\dots,r_6]$ 完全一样，其中每个 $r_j$ 对应测试套件中匹配的 $n$ 的 $R(n)$ 值。", "solution": "该问题被评估为有效。它在科学上基于统计估计的原理，定义和约束清晰完整，问题适定，并且其表述是客观的。该问题探讨了稳健统计学中的一个重要概念：重尾分布的位置参数估计，在这种情况下，像样本均值这样的标准估计量表现不佳。样本均值与均值中位数估计量之间的比较是该领域一个标准且富有启发性的练习。所有参数、程序和评估指标都得到了明确规定，没有歧义，从而确保了解决方案的唯一性和可验证性。\n\n### 理论框架\n\n该问题要求比较一个分布的位置参数 $\\mu$ 的两个估计量。该分布由 $X_i = \\mu + Z_i$ 定义，其中噪声项 $Z_i$ 服从自由度为 $\\nu$ 的学生t分布。自由度的具体值为 $\\nu = 1.5$。学生t分布的一个关键特性是其矩依赖于 $\\nu$。具体来说，其 $k$ 阶矩是有限的当且仅当 $k  \\nu$。\n\n对于给定的 $\\nu = 1.5$，它位于区间 $(1, 2)$ 内，因此具有以下性质：\n1.  由于 $1  \\nu$，该分布的均值是良定义且有限的。其期望值为 $E[X_i] = E[\\mu + Z_i] = \\mu + E[Z_i] = \\mu + 0 = \\mu$。\n2.  由于 $2 \\ge \\nu$，该分布的方差是无穷大的。\n\n基础数据分布的无穷大方差对样本均值 $\\bar{X} = \\frac{1}{n} \\sum_{i=1}^n X_i$ 的行为有深远的影响。标准的中心极限定理指出，随着样本量 $n$ 的增长，样本均值的分布会趋近于正态分布，但这要求基础分布具有有限方差。由于这个条件不满足，$\\bar{X}$ 的分布不会收敛于高斯分布。相反，其分布仍然是重尾的，这意味着即使对于大的 $n$，$\\bar{X}$ 的极端值也会以不可忽略的概率出现。因此，在这种情况下，样本均值不是 $\\mu$ 的一个稳健估计量，因为它对重尾噪声产生的频繁的大幅值观测值（离群点）高度敏感。\n\n### 均值中位数估计量\n\n均值中位数 (MoM) 估计量 $\\widehat{\\mu}_{\\mathrm{MoM}}$ 是一种稳健的替代方法，旨在在存在重尾分布时表现良好。其构造过程包含两个阶段：\n1.  **求均值：** 将数据划分为 $k = \\lfloor \\sqrt{n} \\rfloor$ 个较小的组，每组大小为 $m = \\lfloor n/k \\rfloor$。对每个组 $j \\in \\{1, \\dots, k\\}$ 计算样本均值 $M_j$。组内求均值仍然会产生一个重尾变量，但它是对 $\\mu$ 的一个估计。\n2.  **求中位数：** 最终的估计量 $\\widehat{\\mu}_{\\mathrm{MoM}}$ 是这 $k$ 个组均值的中位数：$\\widehat{\\mu}_{\\mathrm{MoM}} = \\operatorname{median}(M_1, M_2, \\dots, M_k)$。\n\nMoM 估计量的稳健性源于中位数算子。中位数对极端值的量级不敏感。如果少数几个组均值 $M_j$ 异常大或小（由于一个或多个来自学生t分布的离群点落入该组），只要被污染的组均值数量少于一半，它们就不会显著影响中位数。这一特性为抵御重尾分布的影响提供了强有力的防御，从而得到一个更紧密地集中在真实参数 $\\mu$ 周围的估计量。\n\n### 蒙特卡洛模拟\n\n为了经验性地评估和比较 $\\bar{X}$ 和 $\\widehat{\\mu}_{\\mathrm{MoM}}$ 的性能，我们采用蒙特卡洛模拟。对于测试套件中的每个样本量 $n$，我们执行大量的试验，次数为 $T = 2000$。在每次试验 $t \\in \\{1, \\dots, T\\}$ 中：\n1.  从指定的自由度为 $\\nu=1.5$、位置为 $\\mu=1.0$ 的学生t分布中生成一个包含 $n$ 个独立同分布点的新数据集。\n2.  使用所有 $n$ 个点计算样本均值 $\\bar{X}^{(t)}$。\n3.  根据其定义计算 MoM 估计值 $\\widehat{\\mu}_{\\mathrm{MoM}}^{(t)}$。\n4.  记录每个估计量的绝对误差 $|\\bar{X}^{(t)} - \\mu|$ 和 $|\\widehat{\\mu}_{\\mathrm{MoM}}^{(t)} - \\mu|$。\n\n在所有 $T$ 次试验之后，通过对所有试验的绝对误差求平均来计算每个估计量的平均绝对误差 (MAE)：\n$$ \\mathrm{MAE}_{\\mathrm{mean}}(n) = \\frac{1}{T} \\sum_{t=1}^T \\left| \\bar{X}^{(t)} - \\mu \\right| $$\n$$ \\mathrm{MAE}_{\\mathrm{MoM}}(n) = \\frac{1}{T} \\sum_{t=1}^T \\left| \\widehat{\\mu}_{\\mathrm{MoM}}^{(t)} - \\mu \\right| $$\n\n最终用于比较的度量是比率 $R(n) = \\frac{\\mathrm{MAE}_{\\mathrm{mean}}(n)}{\\mathrm{MAE}_{\\mathrm{MoM}}(n)}$。$R(n) > 1$ 的值表示 MoM 估计量具有更小的 MAE，因此比样本均值更好地集中在 $\\mu$ 周围。理论预测，随着 $n$ 的增加，MoM 估计量的优越性将变得更加明显，导致 $R(n)$ 呈现增长趋势。\n\n对于边缘情况 $n=1$，我们有 $k = \\lfloor\\sqrt{1}\\rfloor = 1$ 和 $m = \\lfloor 1/1 \\rfloor = 1$。MoM 过程使用一个大小为一的组，因此 $M_1 = X_1$。估计量为 $\\widehat{\\mu}_{\\mathrm{MoM}} = \\operatorname{median}(M_1) = X_1$。样本均值也为 $\\bar{X} = X_1$。由于两个估计量完全相同，它们的 MAE 也将相同，因此 $R(1)$ 必须等于 $1$。\n\n### 算法实现\n\n该模拟将使用 Python 的 `numpy` 和 `scipy` 库来实现。\n1.  使用固定的种子 $12345$ 初始化一个伪随机数生成器，以保证可复现性。\n2.  定义常量：$\\mu = 1.0$，$\\nu = 1.5$，$T = 2000$。\n3.  遍历测试套件中的每个样本量 $n$ in $\\{1, 10, 30, 100, 300, 1000\\}$。\n4.  对于每个 $n$，将两个估计量的绝对误差累加器初始化为零。\n5.  开始蒙特卡洛循环， $t$ 从 $1$ 到 $T$：\n    a. 使用 `scipy.stats.t.rvs(df=nu, loc=mu, size=n, random_state=rng)` 生成一个大小为 $n$ 的样本。\n    b. 计算完整样本的样本均值 $\\bar{X}$。\n    c. 计算 MoM 参数：$k = \\lfloor \\sqrt{n} \\rfloor$ 和 $m = \\lfloor n/k \\rfloor$。根据设计，处理 $n=0$ 时 $k=0$ 的情况，尽管给定的测试套件中不要求。如果 $k>0$，则继续。\n    d. 确定用于 MoM 的数据点数量：`num_pts_mom = k * m`。\n    e. 将样本截断为前 `num_pts_mom` 个点。\n    f. 将此截断的样本重塑为形状为 $(k, m)$ 的矩阵。\n    g. 计算每行的均值以获得 $k$ 个组均值。\n    h. 计算组均值的中位数以获得 $\\widehat{\\mu}_{\\mathrm{MoM}}$。\n    i. 计算绝对误差 $|\\bar{X} - \\mu|$ 和 $|\\widehat{\\mu}_{\\mathrm{MoM}} - \\mu|$，并将它们加到各自的累加器中。\n6.  蒙特卡洛循环结束后，通过将累积误差除以 $T$ 来计算两个估计量的 MAE。\n7.  计算比率 $R(n) = \\mathrm{MAE}_{\\mathrm{mean}}(n) / \\mathrm{MAE}_{\\mathrm{MoM}}(n)$。\n8.  存储计算出的比率。\n9.  遍历完所有 $n$ 后，将比率列表格式化为指定的输出字符串。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import t as student_t\n\ndef solve():\n    \"\"\"\n    Performs a Monte Carlo simulation to compare the mean absolute error of the\n    sample mean and the median-of-means estimator for a heavy-tailed distribution.\n    \"\"\"\n    \n    # Problem parameters\n    mu = 1.0\n    nu = 1.5\n    T = 2000\n    seed = 12345\n    \n    # Test suite of sample sizes\n    test_cases = [1, 10, 30, 100, 300, 1000]\n\n    # Initialize the random number generator for reproducibility\n    rng = np.random.default_rng(seed)\n\n    results = []\n    \n    for n in test_cases:\n        # Accumulators for absolute errors for each estimator\n        total_abs_error_mean = 0.0\n        total_abs_error_mom = 0.0\n\n        for _ in range(T):\n            # 1. Generate a fresh i.i.d. sample of size n\n            samples = student_t.rvs(df=nu, loc=mu, scale=1, size=n, random_state=rng)\n\n            # 2. Calculate the sample mean estimator\n            x_bar = np.mean(samples)\n            \n            # 3. Calculate the median-of-means estimator\n            if n == 0:\n                # Handle n=0 case, although not in test suite.\n                # Estimators are undefined, but we can set to a default.\n                mu_mom = np.nan \n            else:\n                k = int(np.floor(np.sqrt(n)))\n                if k == 0:\n                    # This branch is hit for n=1, 2, 3 where k=1, but if n was 0, k=0.\n                    # Or if n=0, sqrt(n)=0 floor(0)=0.\n                    # As k=0, m is division by zero. Let's handle this case for completeness.\n                    # For n in {1,2,3}, k=1, so m = floor(n/1) = n.\n                    # num_pts=n, reshape(1,n). mean across axis=1 is just the row.\n                    # median of one value is the value itself.\n                    # Effectively, for k=1, mu_mom is just mean of first m=n points.\n                    # This logic should be robust. k will not be 0 for n>=1.\n                     mu_mom = np.nan\n                else: \n                     m = int(np.floor(n / k))\n                \n                if m > 0: # Ensure groups are non-empty\n                    num_points_for_mom = k * m\n                    \n                    # Partition the first k*m observations\n                    # and reshape into k groups of size m\n                    groups = samples[:num_points_for_mom].reshape(k, m)\n                    \n                    # Compute mean for each group\n                    group_means = np.mean(groups, axis=1)\n                    \n                    # Compute the median of the group means\n                    mu_mom = np.median(group_means)\n                else:\n                    # This case happens for small n where floor(n/k) = 0.\n                    # E.g., for n=2, k=1, m=2. No problem.\n                    # E.g., for n=3, k=1, m=3. No problem.\n                    # No n in the test suite causes this.\n                    # If it happened, MoM is not well-defined.\n                    mu_mom = np.nan\n\n            # 4. Calculate absolute errors and accumulate\n            total_abs_error_mean += np.abs(x_bar - mu)\n            if not np.isnan(mu_mom):\n                total_abs_error_mom += np.abs(mu_mom - mu)\n\n        # 5. Compute Mean Absolute Error (MAE) for both estimators\n        mae_mean = total_abs_error_mean / T\n        mae_mom = total_abs_error_mom / T\n\n        # 6. Compute the ratio R(n)\n        # For n=1, k=1, m=1, so mu_mom = x_bar and ratio should be 1.\n        if mae_mom == 0:\n            # Avoid division by zero, though unlikely in this context\n            ratio = 1.0 if mae_mean == 0 else np.inf\n        else:\n            ratio = mae_mean / mae_mom\n            \n        results.append(ratio)\n\n    # Final print statement in the exact required format.\n    formatted_results = [f\"{r:.6f}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3155655"}, {"introduction": "如果一个估计量的性质（如偏差）难以通过数学公式直接推导，我们该怎么办？计算统计学为此提供了强大的工具。本练习将介绍两种强大的重采样技术——刀切法（Jackknife）和自助法（Bootstrap），它们可以直接从数据中估计出统计量的偏差。你将亲手实现这些方法，为一个非线性估计量 $\\log(\\bar{X})$ 进行偏差校正，从而掌握在理论分析不可行时评估和改进估计量的实用技能。[@problem_id:3155706]", "problem": "考虑一个由独立同分布 (i.i.d.) 的正观测值 $X_1, X_2, \\dots, X_n$ 组成的数据集。我们感兴趣的参数是总体均值的对数，记为 $\\theta = \\log\\left(E[X]\\right)$。统计量是数据的函数，对于数据集 $D$，此处记为 $T(D)$；而估计量是旨在估计某个参数的统计量；其在某个数据集上的实现值称为估计值。在本问题中，使用非线性估计量 $\\hat{\\theta} = T(D) = \\log\\left(\\bar{X}\\right)$，其中 $\\bar{X}$ 是样本均值。$\\hat{\\theta}$ 对 $\\theta$ 的偏差定义为 $B(\\hat{\\theta}) = E[\\hat{\\theta}] - \\theta$。您的任务是使用刀切法 (jackknife method) 估计偏差，并计算基于自助法 (bootstrap) 的偏差校正，然后在不同的数据生成场景下比较这两种方法。\n\n从上述核心定义出发，实现以下操作，不依赖任何预先推导或简化的公式：\n- 使用基于留一法 (leave-one-out) 样本的刀切法估计 $\\hat{\\theta}$ 的偏差。\n- 使用非参数自助法（从观测数据的经验分布中进行有放回重抽样）来估计 $\\hat{\\theta}$ 的偏差，并构建一个自助法偏差校正估计量。\n- 对于每个数据集，使用已知的数据生成分布计算真实参数值 $\\theta$，并将刀切法偏差校正估计值和自助法偏差校正估计值与 $\\theta$ 进行比较。\n\n您必须完全按照下方的测试套件中的规定生成数据集，并使用给定的随机种子以确保可复现性。所有角度（如有）必须视为无量纲实数；不涉及物理单位。所有输出必须是实数或整数，以普通十进制表示法表示。\n\n测试套件：\n- 情况 1（理想路径，中等样本量，轻度偏斜）：\n  - 分布：速率 $\\lambda = 1.0$ 的指数分布，因此 $E[X] = 1/\\lambda$ 且 $\\theta = \\log(1/\\lambda) = -\\log(\\lambda)$。\n  - 样本量：$n = 30$。\n  - 自助法重复次数：$B = 500$。\n  - 随机种子：$123$。\n- 情况 2（边界条件，极小样本）：\n  - 分布：速率 $\\lambda = 2.0$ 的指数分布。\n  - 样本量：$n = 3$。\n  - 自助法重复次数：$B = 1000$。\n  - 随机种子：$456$。\n- 情况 3（理想路径，中等样本量，较重度偏斜）：\n  - 分布：基础正态参数为 $\\mu = 0.0$, $\\sigma = 1.0$ 的对数正态分布，因此 $E[X] = \\exp(\\mu + \\sigma^2/2)$ 且 $\\theta = \\log(E[X]) = \\mu + \\sigma^2/2$。\n  - 样本量：$n = 50$。\n  - 自助法重复次数：$B = 1000$。\n  - 随机种子：$789$。\n- 情况 4（边缘情况，重尾）：\n  - 分布：基础正态参数为 $\\mu = -0.5$, $\\sigma = 2.0$ 的对数正态分布。\n  - 样本量：$n = 20$。\n  - 自助法重复次数：$B = 2000$。\n  - 随机种子：$321$。\n\n对于每种情况，执行以下操作：\n- 使用给定的种子从指定分布生成数据 $X_1, \\dots, X_n$。\n- 计算全样本估计值 $\\hat{\\theta} = \\log(\\bar{X})$。\n- 从留一法数据集计算刀切法偏差估计，并计算相应的刀切法偏差校正估计量。\n- 使用 $B$ 次重抽样计算自助法偏差估计，并计算相应的自助法偏差校正估计量。\n- 根据该情况的已知分布参数计算真实参数 $\\theta$。\n- 通过比较绝对误差，计算哪种校正更接近真实参数。\n\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果。每个元素对应一种情况，并且本身必须是遵循以下顺序的方括号列表：\n$[\\hat{\\theta}, \\theta, b_{\\text{jack}}, b_{\\text{boot}}, \\hat{\\theta}_{\\text{jack,corr}}, \\hat{\\theta}_{\\text{boot,corr}}, I_{\\text{boot-better}}]$,\n其中 $b_{\\text{jack}}$ 是刀切法偏差估计，$b_{\\text{boot}}$ 是自助法偏差估计，$\\hat{\\theta}_{\\text{jack,corr}}$ 是刀切法偏差校正估计值，$\\hat{\\theta}_{\\text{boot,corr}}$ 是自助法偏差校正估计值，而 $I_{\\text{boot-better}}$ 是一个整数指示符，如果自助法偏差校正估计值相对于 $\\theta$ 的绝对误差小于刀切法偏差校正估计值，则其值为 $1$，否则为 $0$。例如，最终输出应类似于 $[[\\dots],[\\dots],[\\dots],[\\dots]]$，所有值均为十进制或整数形式，不含百分号，也没有附加文本。", "solution": "用户提供了一个计算统计学领域的问题，要求实现并比较两种基于重抽样的偏差估计技术：刀切法 (jackknife) 和非参数自助法 (nonparametric bootstrap)。\n\n### 1. 问题验证\n\n问题陈述已经过严格验证，并被确定为一个有效的科学问题。\n\n*   **提取的已知条件**：该问题为四个不同的测试用例提供了所有必要参数，包括数据生成分布（指数分布、对数正态分布）、它们的参数（$\\lambda$、$\\mu$、$\\sigma$）、样本量（$n$）、自助法重复次数（$B$）以及用于可复现性的随机种子。它明确定义了感兴趣的参数 $\\theta = \\log(E[X])$、估计量 $\\hat{\\theta} = \\log(\\bar{X})$ 以及所需的输出。\n\n*   **验证结论**：\n    1.  **有科学依据**：该问题基于统计推断的基本且公认的原则。刀切法和自助法是估计估计量属性（如偏差和方差）的经典方法。估计量 $\\hat{\\theta} = \\log(\\bar{X})$ 是一个展示样本矩非线性变换中偏差的典型例子。由于对数函数是严格凹函数，偏差由 Jensen's inequality 引起。具体来说，$E[\\log(\\bar{X})] \\le \\log(E[\\bar{X}]) = \\log(E[X]) = \\theta$。因此，预计会存在负偏差，而这些方法旨在估计此偏差。该问题在科学上是合理的。\n    2.  **适定的**：该问题是完全指定的。对于每种情况，分布、参数、样本量和随机种子的组合唯一确定了要生成的数据集。计算刀切法和自助法估计值的过程是标准且无歧义的。每个测试用例都存在唯一、稳定且有意义的数值解。\n    3.  **客观的**：该问题使用精确、形式化的数学和统计语言陈述。各项任务是客观的计算过程，最终的比较基于一个清晰的定量标准（绝对误差）。\n\n该问题通过了所有标准，是有效的。后续章节将详细介绍求解方法。\n\n### 2. 方法论\n\n解决方案将通过为每个测试用例系统地执行所需的计算来实现。\n\n#### 2.1. 真实参数计算 ($\\theta$)\n真实参数 $\\theta = \\log(E[X])$ 是根据指定分布的已知性质计算的。\n*   对于速率参数为 $\\lambda$ 的指数分布，其期望值为 $E[X] = 1/\\lambda$。因此，$\\theta = \\log(1/\\lambda) = -\\log(\\lambda)$。\n*   对于基础正态分布参数为 $\\mu$ 和 $\\sigma$ 的对数正态分布，其期望值为 $E[X] = \\exp(\\mu + \\sigma^2/2)$。因此，$\\theta = \\log(E[X]) = \\mu + \\sigma^2/2$。\n\n#### 2.2. 数据生成和全样本估计 ($\\hat{\\theta}$)\n对于每种情况，使用给定的随机种子从指定的分布中生成一个大小为 $n$ 的数据集 $D = \\{X_1, \\dots, X_n\\}$。然后计算全样本估计值 $\\hat{\\theta} = \\log(\\bar{X})$，其中 $\\bar{X} = \\frac{1}{n}\\sum_{i=1}^n X_i$。\n\n#### 2.3. 刀切法偏差估计\n刀切法通过在留一法子样本上系统地重新计算统计量来估计偏差。\n1.  对于每个观测值 $i \\in \\{1, \\dots, n\\}$，通过从原始数据集 $D$ 中移除 $X_i$ 来创建一个子样本 $D_{(-i)}$。\n2.  计算该子样本的样本均值 $\\bar{X}_{(-i)} = \\frac{1}{n-1}\\sum_{j \\ne i} X_j$。这可以高效地计算为 $\\bar{X}_{(-i)} = \\frac{n\\bar{X} - X_i}{n-1}$。\n3.  计算“留一法”估计值 $\\hat{\\theta}_{(-i)} = \\log(\\bar{X}_{(-i)})$。\n4.  刀切法偏差估计 $b_{\\text{jack}}$ 由以下公式给出：\n    $$b_{\\text{jack}} = (n-1) \\left( \\left( \\frac{1}{n} \\sum_{i=1}^n \\hat{\\theta}_{(-i)} \\right) - \\hat{\\theta} \\right)$$\n5.  刀切法偏差校正估计值则为 $\\hat{\\theta}_{\\text{jack,corr}} = \\hat{\\theta} - b_{\\text{jack}}$。\n\n#### 2.4. 自助法偏差估计\n非参数自助法通过从数据的经验分布中模拟估计量的抽样分布来估计偏差。\n1.  生成 $B$ 个自助法样本 $D^{\\ast 1}, \\dots, D^{\\ast B}$。每个样本 $D^{\\ast b}$ 是通过从原始数据集 $D$ 中有放回地抽取 $n$ 个观测值而创建的。\n2.  对于每个自助法样本 $D^{\\ast b}$，计算统计量的自助法重复制：$\\hat{\\theta}^{\\ast b} = \\log(\\bar{X}^{\\ast b})$，其中 $\\bar{X}^{\\ast b}$ 是样本 $D^{\\ast b}$ 的均值。\n3.  自助法偏差估计 $b_{\\text{boot}}$ 是自助法重复制的平均值与原始全样本估计值之间的差：\n    $$b_{\\text{boot}} = \\left( \\frac{1}{B} \\sum_{b=1}^B \\hat{\\theta}^{\\ast b} \\right) - \\hat{\\theta}$$\n4.  自助法偏差校正估计值为 $\\hat{\\theta}_{\\text{boot,corr}} = \\hat{\\theta} - b_{\\text{boot}}$。这可以写成：\n    $$\\hat{\\theta}_{\\text{boot,corr}} = 2\\hat{\\theta} - \\frac{1}{B} \\sum_{b=1}^B \\hat{\\theta}^{\\ast b}$$\n\n#### 2.5. 比较和最终输出\n对于每个测试用例，计算每个校正后估计值相对于真实参数 $\\theta$ 的绝对误差：\n*   误差（刀切法）：$\\text{err}_{\\text{jack}} = |\\hat{\\theta}_{\\text{jack,corr}} - \\theta|$\n*   误差（自助法）：$\\text{err}_{\\text{boot}} = |\\hat{\\theta}_{\\text{boot,corr}} - \\theta|$\n\n如果 $\\text{err}_{\\text{boot}}  \\text{err}_{\\text{jack}}$，则指示变量 $I_{\\text{boot-better}}$ 设置为 $1$，否则设置为 $0$。每种情况的最终结果是一个包含七个值的有序列表：$[\\hat{\\theta}, \\theta, b_{\\text{jack}}, b_{\\text{boot}}, \\hat{\\theta}_{\\text{jack,corr}}, \\hat{\\theta}_{\\text{boot,corr}}, I_{\\text{boot-better}}]$。所有四种情况的结果被汇总成一个列表的列表作为最终输出。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem by computing jackknife and bootstrap bias estimates\n    for the estimator log(sample mean) across four test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"dist\": \"exp\",\n            \"params\": {\"lambda\": 1.0},\n            \"n\": 30,\n            \"B\": 500,\n            \"seed\": 123\n        },\n        {\n            \"dist\": \"exp\",\n            \"params\": {\"lambda\": 2.0},\n            \"n\": 3,\n            \"B\": 1000,\n            \"seed\": 456\n        },\n        {\n            \"dist\": \"lognormal\",\n            \"params\": {\"mu\": 0.0, \"sigma\": 1.0},\n            \"n\": 50,\n            \"B\": 1000,\n            \"seed\": 789\n        },\n        {\n            \"dist\": \"lognormal\",\n            \"params\": {\"mu\": -0.5, \"sigma\": 2.0},\n            \"n\": 20,\n            \"B\": 2000,\n            \"seed\": 321\n        }\n    ]\n\n    all_results = []\n\n    for case in test_cases:\n        n = case[\"n\"]\n        B = case[\"B\"]\n        seed = case[\"seed\"]\n        dist = case[\"dist\"]\n        params = case[\"params\"]\n\n        # Use a random number generator for reproducibility.\n        rng = np.random.default_rng(seed)\n\n        # 1. Generate data and compute true theta.\n        if dist == \"exp\":\n            rate = params[\"lambda\"]\n            X = rng.exponential(scale=1.0/rate, size=n)\n            theta_true = -np.log(rate)\n        elif dist == \"lognormal\":\n            mu, sigma = params[\"mu\"], params[\"sigma\"]\n            X = rng.lognormal(mean=mu, sigma=sigma, size=n)\n            theta_true = mu + (sigma**2) / 2.0\n            \n        # 2. Compute the full-sample estimate.\n        x_bar = np.mean(X)\n        theta_hat = np.log(x_bar)\n\n        # 3. Compute jackknife bias estimate and corrected estimator.\n        # Efficiently calculate leave-one-out means\n        x_bar_j = (n * x_bar - X) / (n - 1)\n        theta_hat_j = np.log(x_bar_j)\n        \n        # Jackknife bias\n        theta_hat_j_mean = np.mean(theta_hat_j)\n        b_jack = (n - 1) * (theta_hat_j_mean - theta_hat)\n        \n        # Jackknife bias-corrected estimate\n        theta_hat_jack_corr = theta_hat - b_jack\n\n        # 4. Compute bootstrap bias estimate and corrected estimator.\n        bootstrap_replicates = rng.choice(X, size=(B, n), replace=True)\n        x_bar_b = np.mean(bootstrap_replicates, axis=1)\n        theta_hat_b = np.log(x_bar_b)\n        \n        # Bootstrap bias\n        theta_hat_b_mean = np.mean(theta_hat_b)\n        b_boot = theta_hat_b_mean - theta_hat\n        \n        # Bootstrap bias-corrected estimate\n        theta_hat_boot_corr = theta_hat - b_boot\n        \n        # 5. Compare the corrected estimators.\n        err_jack = np.abs(theta_hat_jack_corr - theta_true)\n        err_boot = np.abs(theta_hat_boot_corr - theta_true)\n        I_boot_better = 1 if err_boot  err_jack else 0\n\n        # Assemble the results for the current case.\n        case_result = [\n            theta_hat, \n            theta_true, \n            b_jack, \n            b_boot, \n            theta_hat_jack_corr, \n            theta_hat_boot_corr, \n            I_boot_better\n        ]\n        all_results.append(case_result)\n\n    # Format the final output string exactly as required.\n    result_strings = []\n    for res in all_results:\n        # Convert each number in the list to its string representation.\n        # The integer indicator will be correctly formatted as an integer string.\n        s = \"[\" + \",\".join(map(str, res)) + \"]\"\n        result_strings.append(s)\n    \n    final_output = \"[\" + \",\".join(result_strings) + \"]\"\n    print(final_output)\n\nsolve()\n```", "id": "3155706"}]}