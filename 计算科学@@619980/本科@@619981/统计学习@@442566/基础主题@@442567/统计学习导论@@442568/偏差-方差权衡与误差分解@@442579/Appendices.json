{"hands_on_practices": [{"introduction": "我们将从一个基础练习开始，该练习将偏差-方差权衡变得具体可感。通过分析一个简单的惩罚回归模型，您将推导出正则化参数 $\\lambda$ 如何在引入偏差的同时降低方差。这个过程将使您能够找到最小化预测误差的最佳平衡点。", "problem": "考虑具有独立同分布噪声的非参数回归模型：$y_i = f_0(x_i) + \\varepsilon_i$，其中 $i = 1,\\dots,n$，$x_i = i/n$ 在 $[0,1]$ 中等间距分布，且 $\\varepsilon_i \\sim \\mathcal{N}(0,\\sigma^2)$。设估计量是通过最小化带有对二阶导数的粗糙度惩罚的经验风险得到的：在形如 $f(x) = \\theta \\,\\varphi(x)$ 的函数中，选择 $\\hat{f}$ 来最小化\n$$\n\\frac{1}{n} \\sum_{i=1}^n \\big(y_i - f(x_i)\\big)^2 \\;+\\; \\lambda \\int_0^1 \\big(f''(x)\\big)^2 \\, dx,\n$$\n其中 $\\lambda \\ge 0$ 是一个调节参数，$\\varphi(x) = \\sqrt{2}\\,\\sin(2\\pi k x)$，$k$ 为一个固定的正整数。假设 $n$ 是 $k$ 的偶数倍，因此 $(1/n)\\sum_{i=1}^n \\varphi(x_i)^2 = 1$ 精确成立。\n\n1) 从点态偏差、方差和均方误差的定义出发，推导在任意点 $x_0 \\in [0,1]$ 处，$\\hat{f}(x_0)$ 的点态偏差和方差作为 $\\lambda$ 的函数的显式公式。你的推导应从第一性原理出发，通过求解 $\\hat{\\theta}$ 的惩罚最小二乘问题，将 $\\hat{f}(x_0)$ 表示为数据的线性估计量，然后计算期望和方差。\n\n2) 将问题具体化到已知信号 $f_0(x) = \\beta \\sin(2\\pi k x)$，其中 $\\beta \\in \\mathbb{R}$，并取 $x_0$ 为 $f_0$ 的最大曲率位置（例如，任何满足 $\\sin(2\\pi k x_0) = 1$ 的 $x_0$）。显式计算曲率权重 $r := \\int_0^1 \\big(\\varphi''(x)\\big)^2\\,dx$，并使用第 (1) 部分的公式将点态均方误差 $\\mathbb{E}\\big[(\\hat{f}(x_0) - f_0(x_0))^2\\big]$ 写成 $\\lambda$ 的函数。\n\n3) 确定使第 (2) 部分中 $x_0$ 处的点态均方误差最小化的 $\\lambda$ 值。将你的最终答案表示为关于 $n$、$\\sigma^2$、$\\beta$ 和 $k$ 的闭式表达式。不需要进行数值计算。只提供最终表达式。不包含任何单位。\n\n答案格式要求：你的最终答案必须是单一的闭式解析表达式。如果你进行了任何近似，请不要说明；提供精确表达式。不要对你的答案进行四舍五入。", "solution": "该问题要求找到最优的调节参数 $\\lambda$，以最小化惩罚最小二乘估计量的点态均方误差 (MSE)。我们将按要求分三部分解决此问题。首先，我们推导估计量的点态偏差和方差的一般表达式。其次，我们将这些表达式具体化到给定的信号和评估点，并计算 MSE。第三，我们针对 $\\lambda$ 最小化此 MSE。\n\n估计量 $\\hat{f}(x)$ 的形式为 $f(x) = \\theta \\varphi(x)$，其中 $\\varphi(x) = \\sqrt{2}\\,\\sin(2\\pi k x)$。参数 $\\hat{\\theta}$ 通过最小化目标函数得到：\n$$\nL(\\theta) = \\frac{1}{n} \\sum_{i=1}^n \\big(y_i - \\theta \\varphi(x_i)\\big)^2 \\;+\\; \\lambda \\int_0^1 \\big((\\theta\\varphi(x))''\\big)^2 \\, dx\n$$\n我们可以重写惩罚项。令 $r := \\int_0^1 \\big(\\varphi''(x)\\big)^2 \\, dx$。目标函数变为：\n$$\nL(\\theta) = \\frac{1}{n} \\sum_{i=1}^n \\big(y_i - \\theta \\varphi(x_i)\\big)^2 \\;+\\; \\lambda \\theta^2 r\n$$\n这是关于 $\\theta$ 的二次函数。为求最小值，我们对 $\\theta$ 求导并将结果设为零：\n$$\n\\frac{dL}{d\\theta} = \\frac{1}{n} \\sum_{i=1}^n 2\\big(y_i - \\theta \\varphi(x_i)\\big)(-\\varphi(x_i)) \\;+\\; 2 \\lambda \\theta r = 0\n$$\n$$\n-\\frac{1}{n} \\sum_{i=1}^n y_i \\varphi(x_i) + \\frac{\\theta}{n} \\sum_{i=1}^n \\varphi(x_i)^2 + \\lambda \\theta r = 0\n$$\n给定条件 $\\frac{1}{n} \\sum_{i=1}^n \\varphi(x_i)^2 = 1$。将其代入方程可得：\n$$\n-\\frac{1}{n} \\sum_{i=1}^n y_i \\varphi(x_i) + \\theta + \\lambda \\theta r = 0\n$$\n求解 $\\theta$ 的估计量 $\\hat{\\theta}$：\n$$\n\\hat{\\theta}(1 + \\lambda r) = \\frac{1}{n} \\sum_{i=1}^n y_i \\varphi(x_i) \\implies \\hat{\\theta} = \\frac{\\frac{1}{n} \\sum_{i=1}^n y_i \\varphi(x_i)}{1 + \\lambda r}\n$$\n函数在点 $x_0$ 处的估计量为 $\\hat{f}(x_0) = \\hat{\\theta} \\varphi(x_0)$。\n\n**1) 点态偏差和方差**\n\n估计量 $\\hat{f}(x_0)$ 的点态偏差定义为 $\\text{Bias}(\\hat{f}(x_0)) = \\mathbb{E}[\\hat{f}(x_0)] - f_0(x_0)$。期望是关于噪声分布计算的。由于 $\\mathbb{E}[\\varepsilon_i] = 0$，我们有 $\\mathbb{E}[y_i] = \\mathbb{E}[f_0(x_i) + \\varepsilon_i] = f_0(x_i)$。\n$$\n\\mathbb{E}[\\hat{f}(x_0)] = \\mathbb{E}\\left[ \\frac{\\frac{1}{n} \\sum_{i=1}^n y_i \\varphi(x_i)}{1 + \\lambda r} \\varphi(x_0) \\right] = \\frac{\\varphi(x_0)}{1 + \\lambda r} \\frac{1}{n} \\sum_{i=1}^n \\mathbb{E}[y_i] \\varphi(x_i) = \\frac{\\varphi(x_0)}{1 + \\lambda r} \\frac{1}{n} \\sum_{i=1}^n f_0(x_i) \\varphi(x_i)\n$$\n我们定义离散内积 $\\langle g, h \\rangle_n = \\frac{1}{n} \\sum_{i=1}^n g(x_i)h(x_i)$。那么 $\\mathbb{E}[\\hat{f}(x_0)] = \\frac{\\langle f_0, \\varphi \\rangle_n}{1 + \\lambda r} \\varphi(x_0)$。\n偏差为：\n$$\n\\text{Bias}(\\hat{f}(x_0)) = \\frac{\\langle f_0, \\varphi \\rangle_n}{1 + \\lambda r} \\varphi(x_0) - f_0(x_0)\n$$\n点态方差为 $\\text{Var}(\\hat{f}(x_0)) = \\text{Var}(\\hat{\\theta} \\varphi(x_0)) = \\varphi(x_0)^2 \\text{Var}(\\hat{\\theta})$。噪声变量 $\\varepsilon_i$ 是独立的，因此 $y_i$ 也是独立的，其方差为 $\\text{Var}(y_i) = \\sigma^2$。\n$$\n\\text{Var}(\\hat{\\theta}) = \\text{Var}\\left( \\frac{\\frac{1}{n} \\sum_{i=1}^n y_i \\varphi(x_i)}{1 + \\lambda r} \\right) = \\frac{1}{(1 + \\lambda r)^2} \\text{Var}\\left(\\frac{1}{n} \\sum_{i=1}^n y_i \\varphi(x_i)\\right)\n$$\n$$\n\\text{Var}(\\hat{\\theta}) = \\frac{1}{n^2 (1 + \\lambda r)^2} \\sum_{i=1}^n \\varphi(x_i)^2 \\text{Var}(y_i) = \\frac{\\sigma^2}{n^2 (1 + \\lambda r)^2} \\sum_{i=1}^n \\varphi(x_i)^2\n$$\n使用给定条件 $\\sum_{i=1}^n \\varphi(x_i)^2 = n$，我们得到：\n$$\n\\text{Var}(\\hat{\\theta}) = \\frac{n \\sigma^2}{n^2 (1 + \\lambda r)^2} = \\frac{\\sigma^2}{n (1 + \\lambda r)^2}\n$$\n因此，估计量的点态方差为：\n$$\n\\text{Var}(\\hat{f}(x_0)) = \\varphi(x_0)^2 \\text{Var}(\\hat{\\theta}) = \\frac{\\sigma^2 \\varphi(x_0)^2}{n (1 + \\lambda r)^2}\n$$\n\n**2) 特定情况下的均方误差**\n\n首先，我们计算曲率权重 $r$：\n$$\n\\varphi(x) = \\sqrt{2}\\,\\sin(2\\pi k x)\n$$\n$$\n\\varphi'(x) = \\sqrt{2}\\,(2\\pi k)\\,\\cos(2\\pi k x)\n$$\n$$\n\\varphi''(x) = -\\sqrt{2}\\,(2\\pi k)^2\\,\\sin(2\\pi k x)\n$$\n$$\nr = \\int_0^1 \\big(\\varphi''(x)\\big)^2 \\, dx = \\int_0^1 \\left(-\\sqrt{2}\\,(2\\pi k)^2\\,\\sin(2\\pi k x)\\right)^2 \\, dx = 2(2\\pi k)^4 \\int_0^1 \\sin^2(2\\pi k x) \\, dx\n$$\n使用恒等式 $\\sin^2(\\alpha) = \\frac{1 - \\cos(2\\alpha)}{2}$：\n$$\nr = 2(16\\pi^4 k^4) \\int_0^1 \\frac{1 - \\cos(4\\pi k x)}{2} \\, dx = 16\\pi^4 k^4 \\left[ x - \\frac{\\sin(4\\pi k x)}{4\\pi k} \\right]_0^1 = 16\\pi^4 k^4 (1 - 0) = 16\\pi^4 k^4\n$$\n现在，我们将问题具体化到信号 $f_0(x) = \\beta \\sin(2\\pi k x) = \\frac{\\beta}{\\sqrt{2}}\\varphi(x)$。离散内积为：\n$$\n\\langle f_0, \\varphi \\rangle_n = \\frac{1}{n} \\sum_{i=1}^n \\left(\\frac{\\beta}{\\sqrt{2}}\\varphi(x_i)\\right) \\varphi(x_i) = \\frac{\\beta}{\\sqrt{2}} \\left( \\frac{1}{n} \\sum_{i=1}^n \\varphi(x_i)^2 \\right) = \\frac{\\beta}{\\sqrt{2}} \\cdot 1 = \\frac{\\beta}{\\sqrt{2}}\n$$\n我们在满足 $\\sin(2\\pi k x_0) = 1$ 的点 $x_0$ 处进行评估。在该点，$\\varphi(x_0) = \\sqrt{2}\\sin(2\\pi k x_0) = \\sqrt{2}$ 且 $f_0(x_0) = \\beta\\sin(2\\pi k x_0) = \\beta$。\n将这些代入偏差和方差公式：\n$$\n\\text{Bias}(\\hat{f}(x_0)) = \\frac{\\beta/\\sqrt{2}}{1 + \\lambda r} (\\sqrt{2}) - \\beta = \\frac{\\beta}{1 + \\lambda r} - \\beta = \\frac{\\beta - \\beta(1+\\lambda r)}{1 + \\lambda r} = -\\frac{\\beta \\lambda r}{1 + \\lambda r}\n$$\n$$\n\\text{Var}(\\hat{f}(x_0)) = \\frac{\\sigma^2 (\\sqrt{2})^2}{n (1 + \\lambda r)^2} = \\frac{2\\sigma^2}{n (1 + \\lambda r)^2}\n$$\n点态均方误差为 $\\text{MSE}(\\hat{f}(x_0)) = (\\text{Bias}(\\hat{f}(x_0)))^2 + \\text{Var}(\\hat{f}(x_0))$：\n$$\n\\text{MSE}(\\lambda) = \\left(-\\frac{\\beta \\lambda r}{1 + \\lambda r}\\right)^2 + \\frac{2\\sigma^2}{n (1 + \\lambda r)^2} = \\frac{\\beta^2 \\lambda^2 r^2}{(1 + \\lambda r)^2} + \\frac{2\\sigma^2}{n (1 + \\lambda r)^2} = \\frac{n\\beta^2 \\lambda^2 r^2 + 2\\sigma^2}{n(1 + \\lambda r)^2}\n$$\n\n**3) $\\lambda$ 的最优值**\n\n为求得最小化 MSE 的 $\\lambda$ 值，我们将 $\\text{MSE}(\\lambda)$ 对 $\\lambda$ 求导并令其为零。\n$$\n\\frac{d}{d\\lambda}\\text{MSE}(\\lambda) = \\frac{d}{d\\lambda} \\left[ \\frac{n\\beta^2 \\lambda^2 r^2 + 2\\sigma^2}{n(1 + \\lambda r)^2} \\right]\n$$\n使用商法则，并注意到因子 $1/n$ 是一个常数：\n$$\n\\frac{d}{d\\lambda}\\text{MSE}(\\lambda) = \\frac{1}{n} \\frac{(2n\\beta^2 \\lambda r^2)(1+\\lambda r)^2 - (n\\beta^2 \\lambda^2 r^2 + 2\\sigma^2)(2(1+\\lambda r)r)}{(1+\\lambda r)^4} = 0\n$$\n对于 $\\lambda \\ge 0$，我们可以除以 $2(1+\\lambda r) \\neq 0$：\n$$\n(n\\beta^2 \\lambda r^2)(1+\\lambda r) - r(n\\beta^2 \\lambda^2 r^2 + 2\\sigma^2) = 0\n$$\n$$\nn\\beta^2 \\lambda r^2 + n\\beta^2 \\lambda^2 r^3 - n\\beta^2 \\lambda^2 r^3 - 2\\sigma^2 r = 0\n$$\n$$\nn\\beta^2 \\lambda r^2 - 2\\sigma^2 r = 0\n$$\n假设 $r \\neq 0$（因为 $k$ 是正整数，所以成立）且 $\\beta \\neq 0$：\n$$\nn\\beta^2 \\lambda r = 2\\sigma^2 \\implies \\lambda = \\frac{2\\sigma^2}{n\\beta^2 r}\n$$\n二阶导数检验确认这是一个最小值。代入 $r = 16\\pi^4 k^4$ 的值：\n$$\n\\lambda = \\frac{2\\sigma^2}{n\\beta^2 (16\\pi^4 k^4)} = \\frac{\\sigma^2}{8 n \\beta^2 \\pi^4 k^4}\n$$\n这就是调节参数 $\\lambda$ 的最优值。", "answer": "$$\\boxed{\\frac{\\sigma^2}{8 n \\beta^2 \\pi^4 k^4}}$$", "id": "3180624"}, {"introduction": "在正则化概念的基础上，本练习旨在解决多元回归中一个常见的问题：多重共线性。您将研究岭回归如何通过主动引入少量偏差，来有效抑制由相关预测变量引起的大幅方差膨胀，从而提高整体预测精度。这个练习[@problem_id:3180600]将量化地展示正则化在稳定模型方面的强大作用。", "problem": "考虑标准线性模型 $Y = X \\beta + \\varepsilon$，其中 $X \\in \\mathbb{R}^{n \\times p}$，真实参数向量为 $\\beta \\in \\mathbb{R}^{p}$，噪声为 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2} I_{n})$。对于固定的正则化强度 $\\lambda > 0$，岭估计量为 $\\hat{\\beta}_{\\lambda} = (X^{\\top} X + \\lambda I_{p})^{-1} X^{\\top} Y$。对于新的输入 $x_{0} \\in \\mathbb{R}^{p}$，定义预测为 $\\hat{f}_{\\lambda}(x_{0}) = x_{0}^{\\top} \\hat{\\beta}_{\\lambda}$。在 $x_{0}$ 处对条件均值的均方预测误差为 $\\mathrm{MSE}(x_{0}) = \\mathbb{E}\\big[(\\hat{f}_{\\lambda}(x_{0}) - x_{0}^{\\top} \\beta)^{2}\\big]$，其中期望仅对训练噪声 $\\varepsilon$ 计算。\n\n您将比较两种设计，它们具有相同的样本量 $n$ 和噪声水平 $\\sigma^{2}$，且 $p = 2$：\n- 设计 A (正交)：$X^{\\top} X = n I_{2}$。\n- 设计 B (高度相关)：$X^{\\top} X = n \\begin{pmatrix} 1  \\rho \\\\ \\rho  1 \\end{pmatrix}$，相关性为 $\\rho \\in (0, 1)$。\n\n仅使用模型定义、岭估计量定义以及期望和方差的定义，首先推导出每种设计中预测偏差 $\\mathrm{Bias}(x_{0}) = \\mathbb{E}[\\hat{f}_{\\lambda}(x_{0})] - x_{0}^{\\top} \\beta$ 和预测方差 $\\mathrm{Var}(x_{0}) = \\mathrm{Var}(\\hat{f}_{\\lambda}(x_{0}))$ 的闭式表达式。然后，对于特定值 $n = 100$，$\\sigma^{2} = 1$，$\\lambda = 1$，$\\rho = 0.9$，$\\beta = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$ 和 $x_{0} = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$，计算比率\n$$\nR \\;=\\; \\frac{\\mathrm{MSE}_{B}(x_{0})}{\\mathrm{MSE}_{A}(x_{0})},\n$$\n其中 $\\mathrm{MSE}_{A}(x_{0})$ 和 $\\mathrm{MSE}_{B}(x_{0})$ 分别表示设计 A 和设计 B 下的均方预测误差。此处 $\\mathrm{MSE}(x_{0})$ 指的是预测条件均值 $x_{0}^{\\top} \\beta$ 的误差，因此等于偏差平方与方差之和。\n\n提供您的最终数值 $R$，四舍五入到四位有效数字。", "solution": "### 一般偏差和方差的推导\n首先，我们推导预测 $\\hat{f}_{\\lambda}(x_{0})$ 的偏差和方差的一般表达式。\n\n预测器的期望为：\n$$\n\\mathbb{E}[\\hat{f}_{\\lambda}(x_{0})] = \\mathbb{E}[x_{0}^{\\top} \\hat{\\beta}_{\\lambda}] = x_{0}^{\\top} \\mathbb{E}[\\hat{\\beta}_{\\lambda}]\n$$\n由于 $\\mathbb{E}[Y] = \\mathbb{E}[X \\beta + \\varepsilon] = X \\beta$，估计量的期望为：\n$$\n\\mathbb{E}[\\hat{\\beta}_{\\lambda}] = \\mathbb{E}[(X^{\\top} X + \\lambda I_{p})^{-1} X^{\\top} Y] = (X^{\\top} X + \\lambda I_{p})^{-1} X^{\\top} (X \\beta)\n$$\n那么预测的偏差为：\n$$\n\\mathrm{Bias}(x_{0}) = x_{0}^{\\top} (X^{\\top} X + \\lambda I_{p})^{-1} X^{\\top} X \\beta - x_{0}^{\\top} \\beta = x_{0}^{\\top} \\left[ (X^{\\top} X + \\lambda I_{p})^{-1} X^{\\top} X - I_p \\right] \\beta\n$$\n使用恒等式 $I_p = (X^{\\top} X + \\lambda I_{p})^{-1} (X^{\\top} X + \\lambda I_p)$，我们简化方括号中的项：\n$$\n(X^{\\top} X + \\lambda I_{p})^{-1} X^{\\top} X - (X^{\\top} X + \\lambda I_{p})^{-1} (X^{\\top} X + \\lambda I_p) = -\\lambda (X^{\\top} X + \\lambda I_{p})^{-1}\n$$\n所以，偏差为：\n$$\n\\mathrm{Bias}(x_{0}) = -\\lambda x_{0}^{\\top} (X^{\\top} X + \\lambda I_{p})^{-1} \\beta\n$$\n预测的方差为：\n$$\n\\mathrm{Var}(x_{0}) = \\mathrm{Var}(\\hat{f}_{\\lambda}(x_{0})) = \\mathrm{Var}(x_{0}^{\\top} \\hat{\\beta}_{\\lambda}) = x_{0}^{\\top} \\mathrm{Var}(\\hat{\\beta}_{\\lambda}) x_{0}\n$$\n估计量的方差为：\n$$\n\\mathrm{Var}(\\hat{\\beta}_{\\lambda}) = \\mathrm{Var}((X^{\\top} X + \\lambda I_{p})^{-1} X^{\\top} Y) = (X^{\\top} X + \\lambda I_{p})^{-1} X^{\\top} \\mathrm{Var}(Y) (X^{\\top})^{\\top} ((X^{\\top} X + \\lambda I_{p})^{-1})^{\\top}\n$$\n使用 $\\mathrm{Var}(Y) = \\sigma^2 I_n$ 以及 $(X^{\\top} X + \\lambda I_{p})^{-1}$ 是对称矩阵这一事实：\n$$\n\\mathrm{Var}(\\hat{\\beta}_{\\lambda}) = \\sigma^{2} (X^{\\top} X + \\lambda I_{p})^{-1} X^{\\top} X (X^{\\top} X + \\lambda I_{p})^{-1}\n$$\n所以，预测方差为：\n$$\n\\mathrm{Var}(x_{0}) = \\sigma^{2} x_{0}^{\\top} (X^{\\top} X + \\lambda I_{p})^{-1} X^{\\top} X (X^{\\top} X + \\lambda I_{p})^{-1} x_{0}\n$$\n\n### 设计 A (正交) 的分析\n对于设计 A，$X^{\\top} X = n I_{2}$。\n项 $(X^{\\top} X + \\lambda I_{2})$ 变为 $n I_{2} + \\lambda I_{2} = (n+\\lambda)I_2$。其逆矩阵为 $\\frac{1}{n+\\lambda}I_2$。\n\n**设计 A 的偏差平方**：\n$$\n(\\mathrm{Bias}_{A}(x_{0}))^2 = \\left( -\\lambda x_{0}^{\\top} \\left(\\frac{1}{n+\\lambda}I_2\\right) \\beta \\right)^2 = \\frac{\\lambda^2}{(n+\\lambda)^2} (x_{0}^{\\top} \\beta)^2\n$$\n根据给定值，$x_{0}^{\\top} \\beta = \\begin{pmatrix} 1  -1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = 1 \\cdot 1 + (-1) \\cdot 1 = 0$。\n因此，$(\\mathrm{Bias}_{A}(x_{0}))^2 = 0$。\n\n**设计 A 的方差**：\n$$\n\\mathrm{Var}_{A}(x_{0}) = \\sigma^{2} x_{0}^{\\top} \\left(\\frac{1}{n+\\lambda}I_2\\right) (n I_2) \\left(\\frac{1}{n+\\lambda}I_2\\right) x_{0} = \\frac{n \\sigma^2}{(n+\\lambda)^2} x_{0}^{\\top} x_{0}\n$$\n根据给定值，$x_{0}^{\\top} x_{0} = \\begin{pmatrix} 1  -1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} = 1^2 + (-1)^2 = 2$。\n所以，$\\mathrm{Var}_{A}(x_{0}) = \\frac{100 \\cdot 1}{(100+1)^2} \\cdot 2 = \\frac{200}{101^2} = \\frac{200}{10201}$。\n\n**设计 A 的均方误差**：\n$\\mathrm{MSE}_{A}(x_{0}) = (\\mathrm{Bias}_{A}(x_{0}))^2 + \\mathrm{Var}_{A}(x_{0}) = 0 + \\frac{200}{10201} = \\frac{200}{10201}$。\n\n### 设计 B (相关) 的分析\n对于设计 B，$X^{\\top} X = n \\begin{pmatrix} 1  \\rho \\\\ \\rho  1 \\end{pmatrix}$。让我们求这个矩阵的特征分解。\n矩阵 $\\begin{pmatrix} 1  \\rho \\\\ \\rho  1 \\end{pmatrix}$ 的特征值为 $1+\\rho$ 和 $1-\\rho$。\n对应的未归一化特征向量为 $\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$ 和 $\\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$。\n所以，$X^{\\top} X$ 的特征值为 $\\lambda_1 = n(1+\\rho)$ 和 $\\lambda_2 = n(1-\\rho)$。\n归一化特征向量为 $v_1 = \\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$ 和 $v_2 = \\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$。\n给定的向量为 $\\beta = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\sqrt{2} v_1$ 和 $x_0 = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} = \\sqrt{2} v_2$。它们与特征向量成比例。\n\n设 $U = \\begin{pmatrix} v_1  v_2 \\end{pmatrix}$ 为特征向量矩阵。则 $X^{\\top}X = U D U^{\\top}$，其中 $D = \\mathrm{diag}(\\lambda_1, \\lambda_2)$。\n则 $X^{\\top}X + \\lambda I_2 = U D U^{\\top} + \\lambda U U^{\\top} = U(D+\\lambda I_2)U^{\\top}$。\n其逆矩阵为 $(X^{\\top}X + \\lambda I_2)^{-1} = U(D+\\lambda I_2)^{-1}U^{\\top}$。\n\n**设计 B 的偏差平方**：\n$$\n\\mathrm{Bias}_{B}(x_{0}) = -\\lambda x_{0}^{\\top} U(D+\\lambda I_2)^{-1}U^{\\top} \\beta\n$$\n我们有 $x_0 = \\sqrt{2} v_2$ 和 $\\beta = \\sqrt{2} v_1$。\n$U^{\\top}x_0 = \\sqrt{2} U^{\\top}v_2 = \\sqrt{2} \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$，以及 $U^{\\top}\\beta = \\sqrt{2} U^{\\top}v_1 = \\sqrt{2} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$。\n$$\n\\mathrm{Bias}_{B}(x_{0}) = -\\lambda (\\sqrt{2}\\begin{pmatrix} 0  1 \\end{pmatrix}) (D+\\lambda I_2)^{-1} (\\sqrt{2}\\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}) = -2\\lambda \\begin{pmatrix} 0  1 \\end{pmatrix} \\begin{pmatrix} \\frac{1}{\\lambda_1+\\lambda}  0 \\\\ 0  \\frac{1}{\\lambda_2+\\lambda} \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}\n$$\n$$\n\\mathrm{Bias}_{B}(x_{0}) = -2\\lambda \\begin{pmatrix} 0  \\frac{1}{\\lambda_2+\\lambda} \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = 0\n$$\n因此，$(\\mathrm{Bias}_{B}(x_{0}))^2 = 0$。\n\n**设计 B 的方差**：\n$$\n\\mathrm{Var}_{B}(x_{0}) = \\sigma^2 x_{0}^{\\top} U(D+\\lambda I_2)^{-1}U^{\\top} U D U^{\\top} U(D+\\lambda I_2)^{-1}U^{\\top} x_{0}\n$$\n这可以简化为：\n$$\n\\mathrm{Var}_{B}(x_{0}) = \\sigma^2 x_{0}^{\\top} U D(D+\\lambda I_2)^{-2} U^{\\top} x_{0}\n$$\n使用 $U^{\\top}x_0 = \\sqrt{2} \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$：\n$$\n\\mathrm{Var}_{B}(x_{0}) = \\sigma^2 (\\sqrt{2}\\begin{pmatrix} 0  1 \\end{pmatrix}) \\begin{pmatrix} \\frac{\\lambda_1}{(\\lambda_1+\\lambda)^2}  0 \\\\ 0  \\frac{\\lambda_2}{(\\lambda_2+\\lambda)^2} \\end{pmatrix} (\\sqrt{2}\\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}) = 2\\sigma^2 \\frac{\\lambda_2}{(\\lambda_2+\\lambda)^2}\n$$\n现在，代入数值：\n$\\lambda_2 = n(1-\\rho) = 100(1-0.9) = 100(0.1) = 10$。\n$\\sigma^2 = 1$，$\\lambda=1$。\n$$\n\\mathrm{Var}_{B}(x_{0}) = 2(1) \\frac{10}{(10+1)^2} = \\frac{20}{11^2} = \\frac{20}{121}\n$$\n\n**设计 B 的均方误差**：\n$\\mathrm{MSE}_{B}(x_{0}) = (\\mathrm{Bias}_{B}(x_{0}))^2 + \\mathrm{Var}_{B}(x_{0}) = 0 + \\frac{20}{121} = \\frac{20}{121}$。\n\n### 比率 R 的计算\n比率 $R$ 是两个 MSE 值的商。\n$$\nR = \\frac{\\mathrm{MSE}_{B}(x_{0})}{\\mathrm{MSE}_{A}(x_{0})} = \\frac{20/121}{200/10201} = \\frac{20}{121} \\times \\frac{10201}{200} = \\frac{10201}{10 \\times 121} = \\frac{10201}{1210}\n$$\n数值四舍五入到四位有效数字：\n$R \\approx 8.4305785...$\n四舍五入到四位有效数字得到 $8.431$。", "answer": "$$\n\\boxed{8.431}\n$$", "id": "3180600"}, {"introduction": "我们最后的练习将模型的复杂度从全局转向局部，探索单一调整参数的局限性。通过一个动手实践的模拟练习[@problem_id:3180581]，您将看到一个函数的光滑度是如何变化的，以及为什么根据数据的不同区域调整模型的灵活性（在此例中是核带宽）能够实现更优的局部偏差-方差平衡，并获得卓越的预测性能。", "problem": "考虑一个非参数回归设定，其中输入 $x \\in [0,1]$ 从 $[0,1]$ 上的均匀分布中独立同分布地抽取，输出 $y$ 由 $y = f(x) + \\varepsilon$ 给出，其中 $\\varepsilon$ 是独立的噪声，服从均值为 $0$、方差为 $\\sigma^2$ 的正态分布。目标函数 $f(x)$ 表现出空间变化的平滑度，对所有 $x \\in [0,1]$ 定义为\n$$\nf(x) = \\sin(2\\pi x) + 0.5 \\exp\\!\\big(-200(x-0.75)^2\\big).\n$$\n目标是设计一个模拟，以研究核回归中空间自适应带宽选择如何平衡局部偏差和方差，并与单一的全局带宽进行比较。\n\n仅从期望平方误差的定义和上述模型假设出发，推导并实现一个算法，在固定的评估点 $x_{\\text{smooth}} = 0.05$ 和 $x_{\\text{sharp}} = 0.75$ 处，估计 $f(x)$ 的两个估计器的均方误差（MSE）、偏差平方和方差：\n- 一个全局带宽的 Nadaraya–Watson 估计器，使用高斯核 $K(u) = \\exp\\!\\big(-u^2/2\\big)$，其单一带宽 $h$ 通过在候选带宽集上最小化留一法交叉验证（LOO-CV）误差来选择。\n- 一个空间自适应的 Nadaraya–Watson 估计器，在每个评估点 $x^\\star$ 处，通过最小化 $x^\\star$ 周围的局部化 LOO-CV 误差来选择一个局部带宽 $h(x^\\star)$。使用一个引导宽度为 $w_{\\text{pilot}}$ 的高斯加权窗口来局部化交叉验证目标。\n\n在评估点 $x^\\star$ 处，带宽为 $h$ 的 Nadaraya–Watson 估计器定义为\n$$\n\\widehat{f}_h(x^\\star) = \\frac{\\sum_{i=1}^n K\\!\\left(\\frac{x^\\star - x_i}{h}\\right) y_i}{\\sum_{i=1}^n K\\!\\left(\\frac{x^\\star - x_i}{h}\\right)},\n$$\n其中 $(x_i,y_i)$ 是训练样本。对于全局带宽选择，在训练输入 $x_j$ 处，带宽为 $h$ 的 LOO-CV 预测为\n$$\n\\widehat{y}_{-j,h}(x_j) = \\frac{\\sum_{i \\neq j} K\\!\\left(\\frac{x_j - x_i}{h}\\right) y_i}{\\sum_{i \\neq j} K\\!\\left(\\frac{x_j - x_i}{h}\\right)},\n$$\n全局 LOO-CV 目标是所有 $j$ 的平方残差的平均值。对于在评估点 $x^\\star$ 处的空间自适应选择，定义局部化权重\n$$\nw_j(x^\\star) = \\exp\\!\\left(-\\frac{1}{2}\\left(\\frac{x_j - x^\\star}{w_{\\text{pilot}}}\\right)^2\\right),\n$$\n并最小化 $x^\\star$ 附近的加权平方残差平均值，\n$$\nL(h; x^\\star) = \\frac{\\sum_{j=1}^n w_j(x^\\star)\\left(y_j - \\widehat{y}_{-j,h}(x_j)\\right)^2}{\\sum_{j=1}^n w_j(x^\\star)}.\n$$\n\n你必须通过蒙特卡洛模拟来估计 $x_{\\text{smooth}}$ 和 $x_{\\text{sharp}}$ 处的 MSE、偏差平方和方差：对于每个参数集，抽取 $n$ 个训练样本，拟合两个估计器，并在 $R$ 次独立重复实验中记录 $\\widehat{f}(x^\\star)$。使用以下定义：\n- 在 $x^\\star$ 处的偏差平方：$\\left(\\mathbb{E}[\\widehat{f}(x^\\star)] - f(x^\\star)\\right)^2$，\n- 在 $x^\\star$ 处的方差：$\\mathbb{E}\\left[\\left(\\widehat{f}(x^\\star) - \\mathbb{E}[\\widehat{f}(x^\\star)]\\right)^2\\right]$，\n- 在 $x^\\star$ 处的均方误差：$\\mathbb{E}\\left[\\left(\\widehat{f}(x^\\star) - f(x^\\star)\\right)^2\\right]$，\n其中期望通过对 $R$ 次重复实验的平均值来近似。\n\n对于每个参数集，报告在两个评估点上局部 MSE 的改善，定义为 $I(x^\\star) = \\text{MSE}_{\\text{global}}(x^\\star) - \\text{MSE}_{\\text{adaptive}}(x^\\star)$，其中 $x^\\star \\in \\{x_{\\text{smooth}}, x_{\\text{sharp}}\\}$。正值表示自适应估计器相对于全局估计器减少了 MSE。\n\n为以下参数值 $(n, \\sigma, \\mathcal{H}, w_{\\text{pilot}}, R, \\text{seed})$ 的测试套件实现模拟，其中 $\\mathcal{H}$ 是候选带宽集：\n- 测试用例 1：$(150, 0.1, \\{0.02, 0.04, 0.08, 0.16\\}, 0.15, 150, 12345)$。\n- 测试用例 2：$(60, 0.1, \\{0.02, 0.04, 0.08, 0.16\\}, 0.15, 150, 23456)$。\n- 测试用例 3：$(150, 0.4, \\{0.02, 0.04, 0.08, 0.16\\}, 0.15, 150, 34567)$。\n- 测试用例 4：$(150, 0.1, \\{0.01, 0.02, 0.04\\}, 0.10, 120, 45678)$。\n\n你的程序应产生单行输出，包含一个用方括号括起来的逗号分隔列表。该列表必须按测试用例排序，在每个测试用例内，按评估点 $(x_{\\text{smooth}}, x_{\\text{sharp}})$ 排序。每个条目必须是四舍五入到六位小数的浮点数。例如，输出必须是以下形式：\n$$\n[\\Delta_1(x_{\\text{smooth}}), \\Delta_1(x_{\\text{sharp}}), \\Delta_2(x_{\\text{smooth}}), \\Delta_2(x_{\\text{sharp}}), \\Delta_3(x_{\\text{smooth}}), \\Delta_3(x_{\\text{sharp}}), \\Delta_4(x_{\\text{smooth}}), \\Delta_4(x_{\\text{sharp}})],\n$$\n其中 $\\Delta_k(x^\\star)$ 表示测试用例 $k$ 的 MSE 改善 $I(x^\\star)$。此问题不涉及物理单位或角度，所有输出必须是指定的浮点数。程序必须完全自包含，不得需要任何用户输入或外部文件。使用上面定义的高斯核，并确保所有计算在数值上是稳定的（例如，避免在留一法分母中除以零）。", "solution": "模拟的结构如下：\n1.  **数据生成**：对于 $R$ 次蒙特卡洛重复实验中的每一次，生成一个训练集 $\\{(x_i, y_i)\\}_{i=1}^n$。输入 $x_i$ 从均匀分布 $U[0, 1]$ 中抽取。输出根据模型 $y_i = f(x_i) + \\varepsilon_i$ 生成，其中 $\\varepsilon_i$ 从正态分布 $N(0, \\sigma^2)$ 中抽样。\n\n2.  **带宽选择**：每次重复实验中的关键步骤是从候选集 $\\mathcal{H}$ 中选择带宽参数 $h$。\n    *   **留一法交叉验证 (LOO-CV) 预计算**：为了效率，对于每个候选带宽 $h \\in \\mathcal{H}$，我们首先为所有训练点 $x_j$, $j=1, \\dots, n$ 计算 LOO-CV 预测。在 $x_j$ 处的 LOO-CV 预测由下式给出\n    $$\n    \\widehat{y}_{-j,h}(x_j) = \\frac{\\sum_{i \\neq j} K\\left(\\frac{x_j - x_i}{h}\\right) y_i}{\\sum_{i \\neq j} K\\left(\\frac{x_j - x_i}{h}\\right)}\n    $$\n    其中 $K(u) = \\exp(-u^2/2)$ 是高斯核。直接实现计算成本很高。我们可以使用全和来重新表述它。设 $S_y(x_j, h) = \\sum_{i=1}^n K\\left(\\frac{x_j - x_i}{h}\\right) y_i$ 和 $S_1(x_j, h) = \\sum_{i=1}^n K\\left(\\frac{x_j - x_i}{h}\\right)$。由于 $K(0)=1$，留一法预测器可以高效地计算为：\n    $$\n    \\widehat{y}_{-j,h}(x_j) = \\frac{S_y(x_j, h) - y_j}{S_1(x_j, h) - 1}\n    $$\n    此计算将对所有 $j \\in \\{1,\\dots,n\\}$ 和所有 $h \\in \\mathcal{H}$ 执行。\n\n    *   **全局带宽选择**：选择全局带宽 $h_{\\text{global}}$ 以最小化整个训练集上的均方 LOO-CV 误差：\n    $$\n    h_{\\text{global}} = \\arg\\min_{h \\in \\mathcal{H}} \\left\\{ \\frac{1}{n} \\sum_{j=1}^n \\left(y_j - \\widehat{y}_{-j,h}(x_j)\\right)^2 \\right\\}\n    $$\n\n    *   **空间自适应带宽选择**：对于每个评估点 $x^\\star$，通过最小化加权的 LOO-CV 误差来选择一个局部带宽 $h_{\\text{adaptive}}(x^\\star)$。权重强调靠近 $x^\\star$ 的训练点。目标函数是：\n    $$\n    L(h; x^\\star) = \\frac{\\sum_{j=1}^n w_j(x^\\star)\\left(y_j - \\widehat{y}_{-j,h}(x_j)\\right)^2}{\\sum_{j=1}^n w_j(x^\\star)}, \\quad \\text{其中} \\quad w_j(x^\\star) = \\exp\\left(-\\frac{1}{2}\\left(\\frac{x_j - x^\\star}{w_{\\text{pilot}}}\\right)^2\\right)\n    $$\n    局部带宽则为 $h_{\\text{adaptive}}(x^\\star) = \\arg\\min_{h \\in \\mathcal{H}} L(h; x^\\star)$。此过程分别对 $x^\\star = x_{\\text{smooth}} = 0.05$ 和 $x^\\star = x_{\\text{sharp}} = 0.75$ 执行。\n\n3.  **预测**：对于给定的重复实验，一旦选择了带宽，就使用 Nadaraya-Watson 估计器计算两个评估点上的函数估计值：\n    $$\n    \\widehat{f}_h(x^\\star) = \\frac{\\sum_{i=1}^n K\\left(\\frac{x^\\star - x_i}{h}\\right) y_i}{\\sum_{i=1}^n K\\left(\\frac{x^\\star - x_i}{h}\\right)}\n    $$\n    对于全局估计器，我们对 $x_{\\text{smooth}}$ 和 $x_{\\text{sharp}}$ 都使用 $h = h_{\\text{global}}$。对于自适应估计器，在 $x_{\\text{smooth}}$ 处预测时使用 $h = h_{\\text{adaptive}}(x_{\\text{smooth}})$，在 $x_{\\text{sharp}}$ 处预测时使用 $h = h_{\\text{adaptive}}(x_{\\text{sharp}})$。为了数值稳定性，在分母上加上一个小的常数以防止除以零。\n\n4.  **误差估计**：数据生成、带宽选择和预测的过程重复 $R$ 次。这为每个估计器在每个评估点上产生 $R$ 个估计值。设 $\\{\\widehat{f}^{(r)}(x^\\star)\\}_{r=1}^R$ 为估计值的集合。均方误差 (MSE) 随后通过对这些重复实验的平均值来近似：\n    $$\n    \\text{MSE}(x^\\star) = \\mathbb{E}\\left[\\left(\\widehat{f}(x^\\star) - f(x^\\star)\\right)^2\\right] \\approx \\frac{1}{R} \\sum_{r=1}^R \\left(\\widehat{f}^{(r)}(x^\\star) - f(x^\\star)\\right)^2\n    $$\n    这将为全局和自适应估计器在两个评估点上进行计算。\n\n5.  **最终输出**：每个测试用例的最终结果是在每个点上 MSE 的改善，定义为 $I(x^\\star) = \\text{MSE}_{\\text{global}}(x^\\star) - \\text{MSE}_{\\text{adaptive}}(x^\\star)$。正值表示自适应方法提供了更低的 MSE。\n\n实现将使用 `numpy` 进行高效的向量化计算，这对于模拟的可行性至关重要。使用带种子的随机数生成器确保每个测试用例结果的可复现性。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements a Monte Carlo simulation to compare global vs. spatially adaptive\n    bandwidth selection for Nadaraya-Watson kernel regression.\n    \"\"\"\n    test_cases = [\n        (150, 0.1, [0.02, 0.04, 0.08, 0.16], 0.15, 150, 12345),\n        (60, 0.1, [0.02, 0.04, 0.08, 0.16], 0.15, 150, 23456),\n        (150, 0.4, [0.02, 0.04, 0.08, 0.16], 0.15, 150, 34567),\n        (150, 0.1, [0.01, 0.02, 0.04], 0.10, 120, 45678),\n    ]\n\n    all_results = []\n    \n    # Define the true function\n    def f_true(x):\n        return np.sin(2 * np.pi * x) + 0.5 * np.exp(-200 * (x - 0.75)**2)\n\n    # Define the Gaussian kernel\n    def gaussian_kernel(u):\n        return np.exp(-0.5 * u**2)\n\n    # Evaluation points\n    x_eval = np.array([0.05, 0.75]).reshape(-1, 1) # smooth, sharp\n    f_at_eval = f_true(x_eval)\n\n    # Numerical stability constant\n    STABILITY_EPS = 1e-12\n\n    for case in test_cases:\n        n, sigma, h_candidates, w_pilot, R, seed = case\n        h_candidates = np.array(h_candidates)\n        num_h = len(h_candidates)\n\n        rng = np.random.default_rng(seed)\n\n        preds_global = np.zeros((R, 2))\n        preds_adaptive = np.zeros((R, 2))\n\n        for r in range(R):\n            # 1. Generate data\n            x_train = rng.uniform(0, 1, size=(n, 1))\n            noise = rng.normal(0, sigma, size=(n, 1))\n            y_train = f_true(x_train) + noise\n\n            # 2. Pre-compute LOO predictions for all candidate bandwidths\n            # Pairwise differences for LOO\n            x_diff_loo = x_train - x_train.T\n            all_y_hat_loo = np.zeros((n, num_h))\n\n            for i, h in enumerate(h_candidates):\n                # Kernel matrix for LOO\n                K_mat_loo = gaussian_kernel(x_diff_loo / h)\n                \n                # Full sums S_y and S_1\n                S_y = K_mat_loo @ y_train\n                S_1 = K_mat_loo.sum(axis=1, keepdims=True)\n                \n                # Efficient LOO calculation\n                loo_denom = S_1 - 1.0\n                # Handle numerically unstable denominators\n                loo_denom[np.abs(loo_denom)  STABILITY_EPS] = STABILITY_EPS\n                \n                y_hat_loo_h = (S_y - y_train) / loo_denom\n                all_y_hat_loo[:, i] = y_hat_loo_h.flatten()\n\n            # 3. Global bandwidth selection\n            squared_errors_loo = (y_train - all_y_hat_loo)**2\n            global_cv_scores = squared_errors_loo.mean(axis=0)\n            best_h_idx_global = np.argmin(global_cv_scores)\n            h_global = h_candidates[best_h_idx_global]\n\n            # 4. Global prediction\n            diff_pred_global = x_eval - x_train.T\n            K_pred_global = gaussian_kernel(diff_pred_global / h_global)\n            pred_denom_g = K_pred_global.sum(axis=1, keepdims=True)\n            pred_denom_g[pred_denom_g  STABILITY_EPS] = STABILITY_EPS\n            f_hat_global = (K_pred_global @ y_train) / pred_denom_g\n            preds_global[r, :] = f_hat_global.flatten()\n\n            # 5. Spatially adaptive bandwidth selection and prediction\n            for j, x_star in enumerate(x_eval):\n                # Localization weights\n                weights_loc = np.exp(-0.5 * ((x_train - x_star) / w_pilot)**2)\n                weights_sum = weights_loc.sum()\n                \n                # Localized CV scores\n                weighted_sq_errors = weights_loc * squared_errors_loo\n                local_cv_scores = weighted_sq_errors.sum(axis=0) / weights_sum\n                \n                best_h_idx_adaptive = np.argmin(local_cv_scores)\n                h_adaptive = h_candidates[best_h_idx_adaptive]\n                \n                # Adaptive prediction\n                diff_pred_adaptive = x_star - x_train.T\n                K_pred_adaptive = gaussian_kernel(diff_pred_adaptive / h_adaptive)\n                pred_denom_a = K_pred_adaptive.sum()\n                if pred_denom_a  STABILITY_EPS: pred_denom_a = STABILITY_EPS\n                f_hat_adaptive = (K_pred_adaptive @ y_train) / pred_denom_a\n                preds_adaptive[r, j] = f_hat_adaptive.flatten()[0]\n\n        # 6. Calculate MSEs and improvement\n        mse_global = np.mean((preds_global - f_at_eval.T)**2, axis=0)\n        mse_adaptive = np.mean((preds_adaptive - f_at_eval.T)**2, axis=0)\n\n        improvement = mse_global - mse_adaptive\n        all_results.extend(improvement)\n\n    # Final print statement\n    formatted_results = [f\"{val:.6f}\" for val in all_results]\n    print(f\"[{','.join(formatted_results)}]\")\n\n\nsolve()\n```", "id": "3180581"}]}