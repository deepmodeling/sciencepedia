{"hands_on_practices": [{"introduction": "本练习通过一个简单的位置模型，深入探讨了预测和推断之间的根本差异。你将探索不同的损失函数（$\\ell_2$、$\\ell_1$ 和 Huber 损失）如何能够产生相同的点估计，但却对该估计的置信度（即不确定性）有截然不同的描述。这个练习 [@problem_id:3148943] 将加深你的理解：选择损失函数不仅是一个预测问题，更是一种对数据生成过程的隐含假设，而这正是统计推断的核心。", "problem": "给你一个学习任务，其中模型是一个没有输入特征、仅含截距的预测器 $f(x;\\beta)=\\beta$，观测值 $\\{y_i\\}_{i=1}^n$ 是实数值。考虑用三种不同的经验损失来估计 $\\beta$：\n- 平方损失：$\\ell_2(y,\\beta)=\\frac{1}{2}(y-\\beta)^2$。\n- 绝对损失：$\\ell_1(y,\\beta)=|y-\\beta|$。\n- Huber 损失，阈值为 $\\delta>0$：\n$$\n\\rho_\\delta(r)=\n\\begin{cases}\n\\frac{1}{2}r^2, & \\text{若 } |r|\\le \\delta, \\\\\n\\delta|r|-\\frac{1}{2}\\delta^2, & \\text{若 } |r|>\\delta,\n\\end{cases}\n$$\n应用于残差 $r=y-\\beta$。\n对于每个损失函数 $\\ell$，设经验风险为 $R_n^\\ell(\\beta)=\\frac{1}{n}\\sum_{i=1}^n \\ell(y_i,\\beta)$。\n\n使用的基本定义：\n- 经验风险最小化（ERM）选择 $\\hat{\\beta}$ 以最小化特定损失下的 $R_n^\\ell(\\beta)$。\n- 在位置模型中，对于平方损失，ERM 是样本均值；对于绝对损失，它是样本中位数；对于 Huber 损失，ERM 求解 $\\sum_{i=1}^n \\psi_\\delta(y_i-\\beta)=0$，其中 Huber 分数函数是\n$$\n\\psi_\\delta(r)=\n\\begin{cases}\nr, & \\text{若 } |r|\\le \\delta, \\\\\n\\delta\\,\\mathrm{sign}(r), & \\text{若 } |r|>\\delta,\n\\end{cases}\n$$\n其导数为 $\\psi'_\\delta(r)=\\mathbf{1}\\{|r|\\le \\delta\\}$。\n- 对 $\\beta$ 的推断取决于与损失函数相关的假设噪声模型：\n  1. 在与平方损失对应的高斯模型下，位置参数的最大似然估计量是样本均值，其渐近标准误为 $\\sqrt{\\sigma^2/n}$，其中 $\\sigma^2$ 是方差；使用最大似然估计 $\\hat{\\sigma}^2=\\frac{1}{n}\\sum_{i=1}^n (y_i-\\hat{\\beta})^2$ 可得到插件标准误 $\\mathrm{SE}_{\\mathrm{Gauss}}=\\sqrt{\\hat{\\sigma}^2/n}$。\n  2. 在与绝对损失对应的拉普拉斯模型下，其尺度参数为 $b$，位置的费雪信息为 $I(\\mu)=1/b^2$，得到 $\\mathrm{SE}_{\\mathrm{Laplace}}=b/\\sqrt{n}$；使用最大似然估计 $\\hat{b}=\\frac{1}{n}\\sum_{i=1}^n |y_i-\\hat{\\beta}|$ 可得到插件标准误 $\\mathrm{SE}_{\\mathrm{Laplace}}=\\hat{b}/\\sqrt{n}$。\n  3. 对于 Huber M-估计量，在一般条件下，其渐近方差由三明治公式 $\\mathrm{Var}(\\hat{\\beta})=\\frac{B}{nA^2}$ 给出，其中 $A=\\mathbb{E}[\\psi'_\\delta(\\varepsilon)]$ 且 $B=\\mathbb{E}[\\psi_\\delta(\\varepsilon)^2]$。经验插件估计为 $A_{\\mathrm{hat}}=\\frac{1}{n}\\sum_{i=1}^n \\psi'_\\delta(r_i)$ 和 $B_{\\mathrm{hat}}=\\frac{1}{n}\\sum_{i=1}^n \\psi_\\delta(r_i)^2$，得到 $\\mathrm{SE}_{\\mathrm{Huber}}=\\sqrt{B_{\\mathrm{hat}}}/(A_{\\mathrm{hat}}\\sqrt{n})$。\n\n任务。构建并分析一个案例，其中同一个经验风险最小化器 $\\hat{\\beta}$ 能同时在三种损失下产生良好的经验预测，但关于 $\\beta$ 的推断却因特定于损失的解释和曲率而异。使用以下有限样本，所有样本都关于 $0$ 对称，因此均值、中位数和 Huber 的 ERM 都在 $\\hat{\\beta}=0$ 处重合，但它们的标准误不同。对每个测试用例，计算：\n- 在平方损失下的 ERM $\\hat{\\beta}_{2}$、绝对损失下的 ERM $\\hat{\\beta}_{1}$ 和 Huber 损失下的 ERM $\\hat{\\beta}_{H}$（通过数值求解 $\\sum \\psi_\\delta(y_i-\\beta)=0$）。\n- 一个共享的最小化器 $\\hat{\\beta}_{\\mathrm{shared}}=\\hat{\\beta}_2$ 和一个布尔值检查 $E$，该检查确认 $\\hat{\\beta}_2$、$\\hat{\\beta}_1$ 和 $\\hat{\\beta}_H$ 在数值容差 $10^{-9}$ 内相等。\n- 在每种损失下，$\\hat{\\beta}_{\\mathrm{shared}}$ 处的经验风险及其与各自 ERM 所能达到的最小经验风险的比率：\n$$\n\\mathrm{ratio}_{\\ell}=\\frac{R_n^\\ell(\\hat{\\beta}_{\\mathrm{shared}})}{\\min_\\beta R_n^\\ell(\\beta)},\n$$\n对于 $\\ell\\in\\{\\ell_2,\\ell_1,\\rho_\\delta\\}$。\n- 在 $\\hat{\\beta}_{\\mathrm{shared}}$ 处评估的插件标准误 $\\mathrm{SE}_{\\mathrm{Gauss}}$、$\\mathrm{SE}_{\\mathrm{Laplace}}$ 和 $\\mathrm{SE}_{\\mathrm{Huber}}$，以及推断离散度\n$$\nS=\\frac{\\max\\{\\mathrm{SE}_{\\mathrm{Gauss}},\\mathrm{SE}_{\\mathrm{Laplace}},\\mathrm{SE}_{\\mathrm{Huber}}\\}}{\\min\\{\\mathrm{SE}_{\\mathrm{Gauss}},\\mathrm{SE}_{\\mathrm{Laplace}},\\mathrm{SE}_{\\mathrm{Huber}}\\}}.\n$$\n解释：即使预测同样好（比率等于 $1$），由于损失特定的曲率和隐含的噪声模型，标准误也可能存在显著差异。\n\n测试套件。使用以下参数集：\n- 测试用例 1（重尾对称样本，稳健阈值）：$y=[-50,-30,-20,-10,-5,-2,-1,0,1,2,5,10,20,30,50]$，$\\delta=2$。\n- 测试用例 2（相同样本，类二次阈值）：$y=[-50,-30,-20,-10,-5,-2,-1,0,1,2,5,10,20,30,50]$，$\\delta=100$。\n- 测试用例 3（轻尾对称样本）：$y=[-2,-1,-1,0,0,0,1,1,2]$，$\\delta=1$。\n\n输出规范。你的程序应生成单行输出，其中包含用方括号括起来的逗号分隔的结果列表。对于每个测试用例，按顺序输出以下八个值：\n1. $E$ (布尔值), \n2. $\\mathrm{ratio}_{\\ell_2}$ (浮点数), \n3. $\\mathrm{ratio}_{\\ell_1}$ (浮点数), \n4. $\\mathrm{ratio}_{\\rho_\\delta}$ (浮点数), \n5. $\\mathrm{SE}_{\\mathrm{Gauss}}$ (浮点数), \n6. $\\mathrm{SE}_{\\mathrm{Laplace}}$ (浮点数), \n7. $\\mathrm{SE}_{\\mathrm{Huber}}$ (浮点数), \n8. $S$ (浮点数). \n按顺序聚合三个测试用例的结果；例如，最终输出应类似于\n$[\\text{case1\\_v1},\\dots,\\text{case1\\_v8},\\text{case2\\_v1},\\dots,\\text{case3\\_v8}]$。", "solution": "所提供的问题是有效的。这是一个在计算统计学领域中表述清晰的练习，具有科学依据且客观。它引导用户在经验风险最小化（ERM）的背景下，使用不同的损失函数来分析预测与推断之间的关系。\n\n该问题研究了一个仅含截距的模型 $f(x;\\beta) = \\beta$，使用数据 $\\{y_i\\}_{i=1}^n$。这是一个估计位置参数的基本问题。我们被要求比较由三种不同损失函数派生出的三个 $\\beta$ 的估计量：平方误差 $\\ell_2(y,\\beta)=\\frac{1}{2}(y-\\beta)^2$、绝对误差 $\\ell_1(y,\\beta)=|y-\\beta|$ 和 Huber 损失 $\\rho_\\delta(y-\\beta)$。\n\n该问题的构建方式使得数据样本关于 $0$ 对称。这是一个刻意的设计选择。对于对称样本，样本均值（$\\ell_2$ 的 ERM）、样本中位数（$\\ell_1$ 的 ERM）和 Huber M-估计量（$\\rho_\\delta$ 的 ERM）都在对称点重合，在本例中即为 $\\beta=0$。这可以被正式证明：\n- 一组关于 $0$ 对称的数的样本均值为 $0$。\n- 一组关于 $0$ 对称的数的样本中位数为 $0$。\n- Huber M-估计量 $\\hat{\\beta}_H$ 是 $\\sum_{i=1}^n \\psi_\\delta(y_i - \\beta) = 0$ 的解。分数函数 $\\psi_\\delta(r)$ 是奇函数，即 $\\psi_\\delta(-r) = -\\psi_\\delta(r)$。对于一个关于 $0$ 对称的样本 $\\{y_i\\}$，和 $\\sum_i \\psi_\\delta(y_i - 0) = \\sum_i \\psi_\\delta(y_i)$ 等于 $0$，因为样本中每存在一个 $y_i$，也存在一个 $-y_i$，它们贡献的 $\\psi_\\delta(y_i)$ 和 $\\psi_\\delta(-y_i)$ 会相互抵消。因此，$\\hat{\\beta}_H=0$ 是唯一解。\n\n这种设置确保了对于所有三种方法，估计的参数 $\\hat{\\beta}$ 都是相同的（$0$）。因此，预测模型 $f(x) = \\hat{\\beta}$ 在所有情况下都是相同的。在这一共同的最小化器上评估的经验风险 $R_n^\\ell(\\hat{\\beta}_{\\mathrm{shared}})$ 因此是可达到的最小风险，使得比率 $\\mathrm{ratio}_{\\ell}$ 全部等于 $1$。这表明，从纯粹的预测角度（最小化经验风险）来看，这三种损失函数在这些特定数据集上导致了相同的结果。\n\n然而，问题的核心在于将这种预测等价性与推断上的差异进行对比。关于 $\\beta$ 的推断，特别是其不确定性，由估计量 $\\hat{\\beta}$ 的标准误来体现。标准误与（对数似然或）风险函数在其最小值处的曲率成反比。一个更尖锐的谷底（更高的曲率）意味着更精确的估计和更小的标准误。问题提供了与每个损失函数相关的插件标准误公式，反映了与每种损失相关的隐含分布假设：\n1.  **平方损失 ($\\ell_2$) 与高斯噪声**：标准误 $\\mathrm{SE}_{\\mathrm{Gauss}}=\\sqrt{\\hat{\\sigma}^2/n}$，其中 $\\hat{\\sigma}^2=\\frac{1}{n}\\sum_{i=1}^n (y_i-\\hat{\\beta})^2$，源自假设高斯噪声的最大似然理论。样本方差 $\\hat{\\sigma}^2$ 对离群值高度敏感，因此对于重尾数据，此标准误可能很大。\n\n2.  **绝对损失 ($\\ell_1$) 与拉普拉斯噪声**：标准误 $\\mathrm{SE}_{\\mathrm{Laplace}}=\\hat{b}/\\sqrt{n}$，其中 $\\hat{b}=\\frac{1}{n}\\sum_{i=1}^n |y_i-\\hat{\\beta}|$，对应于拉普拉斯噪声模型。平均绝对偏差 $\\hat{b}$ 对离群值的敏感度低于方差，使得该估计量更为稳健。\n\n3.  **Huber 损失与稳健 M-估计**：标准误由三明治公式 $\\mathrm{SE}_{\\mathrm{Huber}}=\\sqrt{B_{\\mathrm{hat}}}/(A_{\\mathrm{hat}}\\sqrt{n})$ 给出，其中 $A_{\\mathrm{hat}}=\\frac{1}{n}\\sum_{i=1}^n \\psi'_\\delta(r_i)$ 且 $B_{\\mathrm{hat}}=\\frac{1}{n}\\sum_{i=1}^n \\psi_\\delta(r_i)^2$。这个公式更具通用性，不假设噪声模型与损失完全匹配。$A_{\\mathrm{hat}}$ 衡量平均曲率，而 $B_{\\mathrm{hat}}$ 衡量分数的方差。参数 $\\delta$ 控制着权衡：对于大的 $\\delta$，Huber 损失近似于 $\\ell_2$ 损失，而对于小的 $\\delta$，它对于大残差的行为更像 $\\ell_1$ 损失，从而赋予其稳健性。\n\n我们现在继续对每个测试用例进行计算。在所有用例中，$\\hat{\\beta}_2 = \\hat{\\beta}_1 = \\hat{\\beta}_H = 0$，因此 $\\hat{\\beta}_{\\mathrm{shared}}=0$，布尔值检查 $E$ 为真，且所有风险比率均为 $1$。分析将集中在标准误上。\n\n**测试用例 1**：$y=[-50,-30,-20,-10,-5,-2,-1,0,1,2,5,10,20,30,50]$，$\\delta=2$。\n该样本对称且重尾。当 $\\hat{\\beta}_{\\mathrm{shared}}=0$ 且 $n=15$ 时：\n- $\\mathrm{SE}_{\\mathrm{Gauss}}$：$\\hat{\\sigma}^2 = \\frac{1}{15}\\sum y_i^2 = 524$。$\\mathrm{SE}_{\\mathrm{Gauss}} = \\sqrt{524/15} \\approx 5.910$。\n- $\\mathrm{SE}_{\\mathrm{Laplace}}$：$\\hat{b} = \\frac{1}{15}\\sum |y_i| = 236/15 \\approx 15.733$。$\\mathrm{SE}_{\\mathrm{Laplace}} = \\hat{b}/\\sqrt{15} \\approx 4.062$。\n- $\\mathrm{SE}_{\\mathrm{Huber}}$：当 $\\delta=2$ 时，只有绝对值 $\\le 2$ 的残差被二次处理。$A_{\\mathrm{hat}} = \\frac{1}{15}\\sum \\mathbf{1}\\{|y_i|\\le 2\\} = 5/15 = 1/3$。$B_{\\mathrm{hat}} = \\frac{1}{15}\\sum \\psi_2(y_i)^2 = 50/15 = 10/3$。$\\mathrm{SE}_{\\mathrm{Huber}} = \\sqrt{10/3}/((1/3)\\sqrt{15}) = \\sqrt{2} \\approx 1.414$。\n推断离散度为 $S = 5.910 / 1.414 \\approx 4.179$。这种巨大的离散度突显了假设的噪声模型对推断的深远影响。具有小 $\\delta$ 的 Huber 估计量的稳健性从其显著更小的标准误中可见一斑。\n\n**测试用例 2**：与用例 1 相同的 $y$，但 $\\delta=100$。\n由于所有数据点 $|y_i|$ 都小于 $\\delta=100$，因此对于此样本，Huber 损失的行为与平方损失完全相同。\n- $\\mathrm{SE}_{\\mathrm{Gauss}}$ 和 $\\mathrm{SE}_{\\mathrm{Laplace}}$ 不变：分别约为 $5.910$ 和 $4.062$。\n- $\\mathrm{SE}_{\\mathrm{Huber}}$：$A_{\\mathrm{hat}} = \\frac{1}{15}\\sum \\mathbf{1}\\{|y_i|\\le 100\\} = 1$。$B_{\\mathrm{hat}} = \\frac{1}{15}\\sum \\psi_{100}(y_i)^2 = \\frac{1}{15}\\sum y_i^2 = 524$。$\\mathrm{SE}_{\\mathrm{Huber}} = \\sqrt{524}/(1 \\cdot \\sqrt{15}) = \\mathrm{SE}_{\\mathrm{Gauss}} \\approx 5.910$。\n离散度为 $S = 5.910/4.062 \\approx 1.455$。正如预期的那样，大的 $\\delta$ 使 Huber 估计量的行为类似于均值，其标准误估计也收敛于基于高斯模型的估计。\n\n**测试用例 3**：$y=[-2,-1,-1,0,0,0,1,1,2]$，$\\delta=1$。\n这个样本是轻尾的（扁平峰态）。这里 $n=9$。\n- $\\mathrm{SE}_{\\mathrm{Gauss}}$：$\\hat{\\sigma}^2 = \\frac{1}{9}\\sum y_i^2 = 12/9 = 4/3$。$\\mathrm{SE}_{\\mathrm{Gauss}} = \\sqrt{(4/3)/9} = \\sqrt{4/27} \\approx 0.385$。\n- $\\mathrm{SE}_{\\mathrm{Laplace}}$：$\\hat{b} = \\frac{1}{9}\\sum |y_i| = 8/9$。$\\mathrm{SE}_{\\mathrm{Laplace}} = (8/9)/\\sqrt{9} = 8/27 \\approx 0.296$。\n- $\\mathrm{SE}_{\\mathrm{Huber}}$：当 $\\delta=1$ 时，$A_{\\mathrm{hat}} = \\frac{1}{9}\\sum \\mathbf{1}\\{|y_i|\\le 1\\} = 7/9$。$B_{\\mathrm{hat}} = \\frac{1}{9}\\sum \\psi_1(y_i)^2 = 6/9 = 2/3$。$\\mathrm{SE}_{\\mathrm{Huber}} = \\sqrt{2/3}/((7/9)\\sqrt{9}) = \\sqrt{6}/7 \\approx 0.350$。\n离散度为 $S = 0.385/0.296 \\approx 1.299$。对于这个表现良好、轻尾的数据，所有三种方法的标准误都非常接近，表明底层建模假设之间的冲突较小。\n\n总之，这些测试用例有效地证明了，尽管不同的损失函数可能在精心整理的数据上产生相同的点估计，从而具有相同的预测性能，但推断结论（即估计的不确定性）可能会有很大不同。这凸显了损失函数的关键作用，它不仅在于找到一个“最佳”参数，还在于定义其精度的度量。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import root_scalar\n\ndef solve():\n    \"\"\"\n    Solves the problem for the given test cases, computes the required metrics,\n    and prints the formatted output.\n    \"\"\"\n\n    # --- Helper Functions ---\n\n    def loss_huber_vec(r, delta):\n        \"\"\"Vectorized Huber loss function.\"\"\"\n        abs_r = np.abs(r)\n        mask = abs_r = delta\n        loss = np.zeros_like(r, dtype=float)\n        loss[mask] = 0.5 * r[mask]**2\n        loss[~mask] = delta * abs_r[~mask] - 0.5 * delta**2\n        return loss\n\n    def huber_score_vec(r, delta):\n        \"\"\"Vectorized Huber score function.\"\"\"\n        score = np.clip(r, -delta, delta)\n        return score\n\n    def huber_score_deriv_vec(r, delta):\n        \"\"\"Vectorized derivative of the Huber score function.\"\"\"\n        return (np.abs(r) = delta).astype(float)\n\n    def calculate_metrics(y, delta):\n        \"\"\"\n        Calculates all required metrics for a given data sample y and delta.\n        \"\"\"\n        y_np = np.array(y, dtype=float)\n        n = len(y_np)\n\n        # 1. Compute ERMs\n        # ERM for squared loss (mean)\n        beta2 = np.mean(y_np)\n        \n        # ERM for absolute loss (median)\n        beta1 = np.median(y_np)\n        \n        # ERM for Huber loss (numerical root finding)\n        huber_obj_func = lambda b: np.sum(huber_score_vec(y_np - b, delta))\n        # Bracket for the root finder. The minimum must be within the data range.\n        bracket = [np.min(y_np), np.max(y_np)]\n        if bracket[0] == bracket[1]: # All data points are the same\n            bracket[0] -= 1\n            bracket[1] += 1\n        sol = root_scalar(huber_obj_func, bracket=bracket, method='brentq')\n        betaH = sol.root\n\n        # 2. Shared minimizer and equality check\n        beta_shared = beta2\n        E = np.allclose([beta1, betaH], beta_shared, atol=1e-9, rtol=0)\n        \n        # 3. Empirical risks and ratios\n        # Since data is symmetric, all ERMs are 0. Ratios will be 1.0.\n        # Minimal risks\n        min_risk2 = np.mean(0.5 * (y_np - beta2)**2)\n        min_risk1 = np.mean(np.abs(y_np - beta1))\n        min_riskH = np.mean(loss_huber_vec(y_np - betaH, delta))\n        \n        # Risks at shared beta\n        shared_risk2 = np.mean(0.5 * (y_np - beta_shared)**2)\n        shared_risk1 = np.mean(np.abs(y_np - beta_shared))\n        shared_riskH = np.mean(loss_huber_vec(y_np - beta_shared, delta))\n        \n        ratio2 = shared_risk2 / min_risk2 if min_risk2  0 else 1.0\n        ratio1 = shared_risk1 / min_risk1 if min_risk1  0 else 1.0\n        ratioH = shared_riskH / min_riskH if min_riskH  0 else 1.0\n        \n        # 4. Standard Errors at beta_shared\n        r_shared = y_np - beta_shared\n        \n        # SE_Gauss\n        sigma2_hat = np.mean(r_shared**2)\n        se_gauss = np.sqrt(sigma2_hat / n)\n        \n        # SE_Laplace\n        b_hat = np.mean(np.abs(r_shared))\n        se_laplace = b_hat / np.sqrt(n)\n        \n        # SE_Huber\n        A_hat = np.mean(huber_score_deriv_vec(r_shared, delta))\n        B_hat = np.mean(huber_score_vec(r_shared, delta)**2)\n        \n        if A_hat  0:\n            se_huber = np.sqrt(B_hat) / (A_hat * np.sqrt(n))\n        else: # Should not happen with given data\n            se_huber = np.inf\n            \n        # 5. Inference Spread\n        ses = [se_gauss, se_laplace, se_huber]\n        S = np.max(ses) / np.min(ses) if np.min(ses)  0 else np.inf\n        \n        return [E, ratio2, ratio1, ratioH, se_gauss, se_laplace, se_huber, S]\n\n    # --- Test Cases ---\n    test_cases = [\n        ([-50, -30, -20, -10, -5, -2, -1, 0, 1, 2, 5, 10, 20, 30, 50], 2.0),\n        ([-50, -30, -20, -10, -5, -2, -1, 0, 1, 2, 5, 10, 20, 30, 50], 100.0),\n        ([-2, -1, -1, 0, 0, 0, 1, 1, 2], 1.0)\n    ]\n\n    # --- Main Logic ---\n    all_results = []\n    for y_data, delta_val in test_cases:\n        case_results = calculate_metrics(y_data, delta_val)\n        all_results.extend(case_results)\n\n    # --- Final Output ---\n    # Python's default str() for bools ('True', 'False') is acceptable.\n    formatted_results = [f\"{val}\" for val in all_results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3148943"}, {"introduction": "在基础概念之上，本练习将处理一个在应用回归中常见的场景：响应变量变换。通过使用 Box-Cox 变换，你将看到一个旨在改善模型拟合以提高预测性能（例如，通过稳定方差）的技术，如何使在原始尺度上解释模型系数的推断任务变得复杂。这个动手实践 [@problem_id:3148926] 将指导你为预测值和边际效应实现偏差校正，揭示在变换尺度和原始数据尺度之间转换时所需的微妙而关键的调整。", "problem": "本题要求您对使用响应变换稳定方差时，模型预测与模型推断进行比较。考虑应用于严格为正的响应的 Box–Cox 变换族。对于任意实数参数 $\\lambda$，该变换定义如下：\n$$\nT_{\\lambda}(y) =\n\\begin{cases}\n\\frac{y^{\\lambda}-1}{\\lambda},  \\lambda \\neq 0, \\\\\n\\log(y),  \\lambda = 0,\n\\end{cases}\n$$\n其逆映射 $g_{\\lambda}(z) = T_{\\lambda}^{-1}(z)$ 由下式给出：\n$$\ng_{\\lambda}(z) =\n\\begin{cases}\n(\\lambda z + 1)^{1/\\lambda},  \\lambda \\neq 0, \\\\\n\\exp(z),  \\lambda = 0.\n\\end{cases}\n$$\n假设在变换后的尺度上存在一个线性回归模型：\n$$\nZ \\equiv T_{\\lambda}(Y) \\mid X \\sim \\text{Normal}(\\mu, \\sigma^2),\n$$\n其中 $\\mu = X^{\\top}\\beta$，$\\sigma^2  0$ 是变换后尺度上的条件方差。您的任务是通过一个程序来计算以下量，这些量对比了在原始尺度 $Y$ 上的预测和推断：\n- 对于预测：计算朴素的反向变换预测 $g_{\\lambda}(\\mu)$ 和通过二阶展开来解释反向变换偏差的偏差校正预测。\n- 对于推断：计算单个回归量 $x_j$ 对原始尺度条件均值的边际效应，首先通过对 $g_{\\lambda}$ 使用链式法则进行朴素计算，然后使用二阶展开进行考虑方差的校正。\n\n使用以下基本依据：\n- 上文给出的 Box–Cox 定义。\n- 一个二次可微函数 $g$ 在 $m$ 附近的二阶泰勒展开：\n$$\ng(Z) \\approx g(m) + g'(m)(Z-m) + \\tfrac{1}{2}g''(m)(Z-m)^2,\n$$\n并且，对于一个随机变量 $Z$，若其满足 $\\mathbb{E}[Z]=m$ 和 $\\operatorname{Var}(Z)=s^2$，则可推导出\n$$\n\\mathbb{E}[g(Z)] \\approx g(m) + \\tfrac{1}{2} g''(m) s^2.\n$$\n- 微分的链式法则。\n\n基于这些依据，且不引入任何未经证明的额外公式，为每个带有参数 $(\\lambda,\\mu,\\sigma^2,\\beta_j)$ 的测试用例推导出以下程序输出：\n1. 原始尺度上的朴素反向变换预测，即忽略方差时 $\\mathbb{E}[Y \\mid X]$ 的值：一个等于 $g_{\\lambda}(\\mu)$ 的浮点数。\n2. 使用二阶展开的原始尺度上的偏差校正预测：一个等于 $g_{\\lambda}(\\mu) + \\tfrac{1}{2} g_{\\lambda}''(\\mu)\\,\\sigma^2$ 的浮点数。\n3. $x_j$ 对原始尺度条件均值的朴素边际效应，仅使用链式法则和系数 $\\beta_j$：一个等于 $\\beta_j\\, g_{\\lambda}'(\\mu)$ 的浮点数。\n4. 使用二阶展开的、$x_j$ 对原始尺度条件均值的考虑方差的校正边际效应：一个等于 $\\beta_j \\big(g_{\\lambda}'(\\mu) + \\tfrac{1}{2} g_{\\lambda}'''(\\mu)\\, \\sigma^2\\big)$ 的浮点数。\n\n实现细节与约束：\n- 对于 $\\lambda = 0$ 的情况，根据其定义 $g_{0}(z)=\\exp(z)$ 处理。对于 $g_{\\lambda}$ 在 $\\lambda=0$ 处的导数，使用指数函数的相应导数。\n- 对于所有其他实数 $\\lambda$，逆函数 $g_{\\lambda}(z)$ 仅在 $\\lambda z + 1  0$ 时有定义；所有提供的测试用例在求值点 $z=\\mu$ 处均满足此条件。\n- 不涉及物理单位。所有输出均为实值浮点数。\n- 不使用角度。\n- 分数和小数均可接受；请勿使用百分号。\n\n测试套件：\n为以下四个参数集提供结果，每个参数集写为 $(\\lambda,\\mu,\\sigma^2,\\beta_j)$：\n- 案例 1：$(0,\\, 0.0,\\, 0.04,\\, 0.2)$。\n- 案例 2：$(0.5,\\, 1.5,\\, 0.36,\\, 1.0)$。\n- 案例 3：$(1.0,\\, 3.0,\\, 2.0,\\, 2.0)$。\n- 案例 4：$(-0.5,\\, 1.0,\\, 0.25,\\, 0.5)$。\n\n最终输出格式：\n您的程序应生成单行输出，其中包含四个案例的结果，形式为列表的列表。每个内部列表对应一个案例，并按 $[\\text{朴素预测}, \\text{偏差校正预测}, \\text{朴素边际效应}, \\text{偏差校正边际效应}]$ 的顺序排列。\n例如，程序必须打印一个形如\n$[[a_{11},a_{12},a_{13},a_{14}],[a_{21},a_{22},a_{23},a_{24}],[a_{31},a_{32},a_{33},a_{34}],[a_{41},a_{42},a_{43},a_{44}]]$\n的字符串，其中填入上述测试套件的数值条目，且不含任何额外文本。", "solution": "我们从指定的基本定义和原理出发。Box–Cox 逆变换 $g_{\\lambda}(z)$ 为\n$$\ng_{\\lambda}(z) =\n\\begin{cases}\n(\\lambda z + 1)^{1/\\lambda},  \\lambda \\neq 0, \\\\\n\\exp(z),  \\lambda = 0.\n\\end{cases}\n$$\n我们假设变换后的响应 $Z = T_{\\lambda}(Y)$ 满足 $Z \\mid X \\sim \\text{Normal}(\\mu, \\sigma^2)$，其中 $\\mu = X^{\\top}\\beta$ 且 $\\sigma^2  0$。\n\n目标 1 (预测)：我们的目标是计算 $\\mathbb{E}[Y \\mid X] = \\mathbb{E}[g_{\\lambda}(Z) \\mid X]$。朴素的反向变换忽略了 $Z$ 的随机性，并设定\n$$\n\\text{朴素预测} = g_{\\lambda}(\\mu).\n$$\n为解释非线性引起的反向变换偏差，我们使用 $g_{\\lambda}(Z)$ 在 $m=\\mu$ 附近的二阶泰勒展开：\n$$\ng_{\\lambda}(Z) \\approx g_{\\lambda}(\\mu) + g_{\\lambda}'(\\mu)(Z-\\mu) + \\tfrac{1}{2} g_{\\lambda}''(\\mu) (Z-\\mu)^2.\n$$\n在给定 $X$ 的条件下取条件期望，并利用 $\\mathbb{E}[Z-\\mu \\mid X]=0$ 和 $\\mathbb{E}[(Z-\\mu)^2 \\mid X] = \\sigma^2$，得到二阶 delta 方法近似\n$$\n\\mathbb{E}[Y \\mid X] = \\mathbb{E}[g_{\\lambda}(Z) \\mid X] \\approx g_{\\lambda}(\\mu) + \\tfrac{1}{2} g_{\\lambda}''(\\mu)\\, \\sigma^2.\n$$\n因此，偏差校正的预测为\n$$\n\\text{偏差校正的预测} \\approx g_{\\lambda}(\\mu) + \\tfrac{1}{2} g_{\\lambda}''(\\mu)\\, \\sigma^2.\n$$\n\n目标 2 (推断)：单个回归量 $x_j$ 对原始尺度条件均值的边际效应是偏导数 $\\dfrac{\\partial}{\\partial x_j} \\mathbb{E}[Y \\mid X]$。首先，将 $\\mathbb{E}[Y \\mid X]$ 朴素地处理为 $\\approx g_{\\lambda}(\\mu)$，根据链式法则和 $\\mu = X^{\\top}\\beta$，我们得到\n$$\n\\text{朴素边际效应} = \\dfrac{\\partial}{\\partial x_j} g_{\\lambda}(\\mu) = g_{\\lambda}'(\\mu) \\cdot \\dfrac{\\partial \\mu}{\\partial x_j} = g_{\\lambda}'(\\mu) \\cdot \\beta_j.\n$$\n接下来，通过对条件均值使用相同的二阶近似来引入方差，\n$$\n\\mathbb{E}[Y \\mid X] \\approx g_{\\lambda}(\\mu) + \\tfrac{1}{2} g_{\\lambda}''(\\mu)\\, \\sigma^2,\n$$\n我们对 $x_j$ 求导并再次使用链式法则。由于在此设置中 $\\sigma^2$ 被视为相对于 $x_j$ 的常数，我们得到\n$$\n\\dfrac{\\partial}{\\partial x_j} \\mathbb{E}[Y \\mid X] \\approx g_{\\lambda}'(\\mu)\\, \\beta_j + \\tfrac{1}{2} g_{\\lambda}'''(\\mu)\\, \\sigma^2\\, \\beta_j = \\beta_j\\left(g_{\\lambda}'(\\mu) + \\tfrac{1}{2} g_{\\lambda}'''(\\mu)\\, \\sigma^2\\right).\n$$\n因此，\n$$\n\\text{偏差校正的边际效应} \\approx \\beta_j\\left(g_{\\lambda}'(\\mu) + \\tfrac{1}{2} g_{\\lambda}'''(\\mu)\\, \\sigma^2\\right).\n$$\n\n我们现在计算上面所需的 $g_{\\lambda}$ 的导数。对于 $\\lambda \\neq 0$，$g_{\\lambda}$ 可以写成 $g_{\\lambda}(z) = (\\lambda z + 1)^{K}$，其中 $K = 1/\\lambda$。对一个仿射函数的幂函数使用重复微分，对于任意实数 $k$：\n$$\n\\dfrac{d}{dz} (a z + b)^k = k a (a z + b)^{k-1}, \\quad\n\\dfrac{d^2}{dz^2} (a z + b)^k = k (k-1) a^2 (a z + b)^{k-2}, \\quad\n\\dfrac{d^3}{dz^3} (a z + b)^k = k (k-1) (k-2) a^3 (a z + b)^{k-3}.\n$$\n令 $a=\\lambda$，$b=1$ 和 $k=K=1/\\lambda$，我们得到对于 $\\lambda \\neq 0$ 的情况：\n$$\ng_{\\lambda}'(z) = (\\lambda z + 1)^{\\frac{1}{\\lambda}-1},\n$$\n$$\ng_{\\lambda}''(z) = \\left(\\frac{1}{\\lambda}-1\\right)\\lambda\\, (\\lambda z + 1)^{\\frac{1}{\\lambda}-2},\n$$\n$$\ng_{\\lambda}'''(z) = \\left(\\frac{1}{\\lambda}-1\\right)\\left(\\frac{1}{\\lambda}-2\\right)\\lambda^2\\, (\\lambda z + 1)^{\\frac{1}{\\lambda}-3}.\n$$\n对于 $\\lambda = 0$，我们有 $g_{0}(z) = \\exp(z)$，因此\n$$\ng_{0}'(z) = \\exp(z), \\quad g_{0}''(z) = \\exp(z), \\quad g_{0}'''(z) = \\exp(z).\n$$\n\n程序算法设计：\n- 对于每个案例 $(\\lambda,\\mu,\\sigma^2,\\beta_j)$，使用适当的分支（$\\lambda=0$ 或 $\\lambda \\neq 0$）计算 $g_{\\lambda}(\\mu)$、$g_{\\lambda}''(\\mu)$ 以及 $g_{\\lambda}'(\\mu)$、$g_{\\lambda}'''(\\mu)$。\n- 计算每个案例所需的四个输出：\n  - 朴素预测：$g_{\\lambda}(\\mu)$。\n  - 偏差校正的预测：$g_{\\lambda}(\\mu) + \\tfrac{1}{2} g_{\\lambda}''(\\mu)\\, \\sigma^2$。\n  - 朴素边际效应：$\\beta_j\\, g_{\\lambda}'(\\mu)$。\n  - 偏差校正的边际效应：$\\beta_j\\left(g_{\\lambda}'(\\mu) + \\tfrac{1}{2} g_{\\lambda}'''(\\mu)\\, \\sigma^2\\right)$。\n- 将所有测试案例的输出按规定顺序组合成一个列表的列表，并在单行上打印，不带任何额外文本。\n\n测试套件覆盖范围：\n- 案例 1 使用 $\\lambda = 0$（对数变换），用于测试特殊分支，并说明反向变换偏差如何通过指数函数依赖于 $\\sigma^2$。\n- 案例 2 使用 $\\lambda = 0.5$ 以及中等大小的 $\\mu$ 和 $\\sigma^2$，用于在典型的方差稳定化场景下测试通用的 Box–Cox 导数。\n- 案例 3 使用 $\\lambda = 1.0$（在相差一个平移的情况下为恒等变换），此时 $g_{\\lambda}''(\\mu)=0$ 且 $g_{\\lambda}'''(\\mu)=0$，导致偏差校正为零，边际效应等于 $\\beta_j$，这是一个有用的边界检查。\n- 案例 4 使用 $\\lambda = -0.5$ 且满足 $\\lambda \\mu + 1  0$，用于测试负参数下的行为，并确认在求值点处对定义域的正确处理。\n\n相对于模型预测的模型推断解释：\n- 原始尺度上的预测得益于变换尺度上的方差稳定化，但在反向变换时需要进行偏差校正以近似 $\\mathbb{E}[Y \\mid X]$；当 $g_{\\lambda}$ 在 $\\mu$ 处是凸函数时（例如指数函数的情况），朴素的 $g_{\\lambda}(\\mu)$ 会低估原始均值。\n- 原始尺度上的推断很复杂，因为边际效应通过 $g_{\\lambda}'(\\mu)$ 和 $g_{\\lambda}'''(\\mu)$ 依赖于 $\\mu$ 和 $\\sigma^2$；因此，变换尺度上的单个系数 $\\beta_j$ 并不会转化为原始尺度上的恒定效应，而考虑方差的推断会引入额外的曲率项。\n\n程序直接实现了这些推导，以生成指定测试套件所需的数值输出。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nimport math\n\ndef g_lambda(z: float, lam: float) - float:\n    \"\"\"Inverse Box-Cox transformation g_lambda(z).\"\"\"\n    if abs(lam)  1e-12:\n        return math.exp(z)\n    base = lam * z + 1.0\n    # Domain check is assumed valid at evaluation point by problem statement.\n    return math.pow(base, 1.0 / lam)\n\ndef g1_lambda(z: float, lam: float) - float:\n    \"\"\"First derivative g'(z) for the inverse Box-Cox.\"\"\"\n    if abs(lam)  1e-12:\n        return math.exp(z)\n    base = lam * z + 1.0\n    return math.pow(base, (1.0 / lam) - 1.0)\n\ndef g2_lambda(z: float, lam: float) - float:\n    \"\"\"Second derivative g''(z) for the inverse Box-Cox.\"\"\"\n    if abs(lam)  1e-12:\n        return math.exp(z)\n    base = lam * z + 1.0\n    return ((1.0 / lam) - 1.0) * lam * math.pow(base, (1.0 / lam) - 2.0)\n\ndef g3_lambda(z: float, lam: float) - float:\n    \"\"\"Third derivative g'''(z) for the inverse Box-Cox.\"\"\"\n    if abs(lam)  1e-12:\n        return math.exp(z)\n    base = lam * z + 1.0\n    return ((1.0 / lam) - 1.0) * ((1.0 / lam) - 2.0) * (lam ** 2) * math.pow(base, (1.0 / lam) - 3.0)\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each tuple is (lambda, mu, sigma2, beta_j)\n    test_cases = [\n        (0.0, 0.0, 0.04, 0.2),    # Case 1\n        (0.5, 1.5, 0.36, 1.0),    # Case 2\n        (1.0, 3.0, 2.0, 2.0),     # Case 3\n        (-0.5, 1.0, 0.25, 0.5),   # Case 4\n    ]\n\n    results = []\n    for lam, mu, sigma2, beta_j in test_cases:\n        g = g_lambda(mu, lam)\n        g1 = g1_lambda(mu, lam)\n        g2 = g2_lambda(mu, lam)\n        g3 = g3_lambda(mu, lam)\n\n        naive_pred = g\n        bias_corr_pred = g + 0.5 * g2 * sigma2\n\n        naive_effect = beta_j * g1\n        bias_corr_effect = beta_j * (g1 + 0.5 * g3 * sigma2)\n\n        results.append([naive_pred, bias_corr_pred, naive_effect, bias_corr_effect])\n\n    # Final print statement in the exact required format.\n    # Print a single line JSON-like list of lists with full-precision floats.\n    def fmt(x):\n        # Use repr-like formatting for better precision consistency\n        return format(x, '.15g')\n    line = \"[\" + \",\".join(\"[\" + \",\".join(fmt(v) for v in row) + \"]\" for row in results) + \"]\"\n    print(line)\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3148926"}, {"introduction": "最后的这个练习将进入概率分类的领域，使用逻辑回归来对比两种不同类型的不确定性。你将实现一个 MAP（最大后验）估计，并研究在特定点上的预测不确定性（通过熵来衡量）与模型参数的整体推断不确定性（通过协方差矩阵的迹来衡量）之间的关系。通过这个练习 [@problem_id:3149003]，你会发现像正则化这样的操作可能对这两种不确定性产生相反的影响，从而让你对模型构建中固有的权衡有更深刻的认识。", "problem": "在一个统计学习的二元分类场景中，您需要仔细比较模型预测和模型推断。考虑使用 logistic 连接函数的伯努利条件模型：对于给定的特征向量 $x \\in \\mathbb{R}^d$ 和参数向量 $\\theta \\in \\mathbb{R}^d$，可观测的标签 $y \\in \\{0,1\\}$ 通过条件概率 $p(y=1 \\mid x,\\theta) = \\sigma(\\theta^\\top x)$ 进行建模，其中 logistic sigmoid 函数 $\\sigma(z)$ 由基本关系 $\\sigma(z) = \\frac{1}{1+\\exp(-z)}$ 定义。对于一个拟合参数 $\\hat{\\theta}$，在点 $x$ 处的预测熵由香non熵给出，以自然单位 (奈特) 计算，即 $H(x) = -\\hat{p}(y=1 \\mid x)\\ln \\hat{p}(y=1 \\mid x) - \\hat{p}(y=0 \\mid x)\\ln \\hat{p}(y=0 \\mid x)$。\n\n为了估计参数向量，请在精度为 $\\lambda  0$ 的零均值各向同性高斯先验下，使用最大后验 (MAP) 估计，这意味着先验密度正比于 $\\exp\\!\\left(-\\frac{\\lambda}{2}\\|\\theta\\|_2^2\\right)$。训练数据为数据对 $(x_i,y_i)$，其中 $i=1,\\dots,n$，且 $y_i \\in \\{0,1\\}$ 和 $x_i \\in \\mathbb{R}^d$。$\\theta$ 的后验正比于似然与先验的乘积。仅从独立样本的伯努利似然、logistic 连接函数和高斯先验的核心定义出发，推导出对数后验的梯度和 Hessian 矩阵。然后，实现一个迭代二阶方法来找到对数后验的一个驻点 $\\hat{\\theta}$ (即 MAP 估计)。在 $\\hat{\\theta}$ 处使用对数后验的局部二次近似，将推断协方差矩阵 $\\widehat{\\Sigma}$ 定义为 $\\hat{\\theta}$ 处的负 Hessian 矩阵的逆。您必须报告的推断方差度量是 $\\widehat{\\Sigma}$ 的迹，记为 $\\operatorname{tr}(\\widehat{\\Sigma})$，它聚合了逐参数的不确定性。对于在查询点 $x^\\star$ 的预测，计算 $\\hat{p}(y=1 \\mid x^\\star) = \\sigma(\\hat{\\theta}^\\top x^\\star)$ 及其熵 $H(x^\\star)$ (使用自然对数)。总而言之，对于每个测试用例，您必须输出由预测熵和推断方差迹组成的对，即 $[H(x^\\star), \\operatorname{tr}(\\widehat{\\Sigma})]$。\n\n您的程序必须在没有任何外部输入的情况下实现以下内容：\n\n- 使用固定的训练集，其中 $n = 10$ 和 $d = 3$ (包括一个截距项)。特征向量的第一个分量为 $1$，作为截距。正类 ($y_i = 1$) 有五个点，其特征 $(x_{i,1}, x_{i,2}, x_{i,3})$ 分别为 $(1, 2.0, 2.0)$, $(1, 2.5, 1.5)$, $(1, 1.7, 2.3)$, $(1, 2.2, 1.8)$, $(1, 1.8, 2.1)$。负类 ($y_i = 0$) 有五个点，其特征 $(x_{i,1}, x_{i,2}, x_{i,3})$ 分别为 $(1, -2.0, -2.0)$, $(1, -2.5, -1.5)$, $(1, -1.7, -2.3)$, $(1, -2.2, -1.8)$, $(1, -1.8, -2.1)$。\n\n- 仅从定义出发，通过一个使用对数后验的梯度和 Hessian 矩阵的迭代二阶上升过程，正确地推导和实现 MAP 估计。通过在每次迭代中检查参数更新的范数，并在其低于一个小的容差时停止，来确保收敛到一个驻点。\n\n- 在 MAP 估计 $\\hat{\\theta}$ 处，使用局部二次近似来形成推断协方差 $\\widehat{\\Sigma}$，即 $\\hat{\\theta}$ 处负 Hessian 矩阵的逆，并计算 $\\operatorname{tr}(\\widehat{\\Sigma})$。\n\n- 对于每个查询，计算预测概率 $\\hat{p}(y=1 \\mid x^\\star)$ 和预测熵 $H(x^\\star)$，使用自然对数 (奈特)。\n\n通过适当地选择先验精度 $\\lambda$ 和查询点 $x^\\star$，设计并分析预测熵增加而推断方差减少的条件。以下四个测试用例旨在探究不同方面：\n\n- 测试用例 A (理想情况：低正则化，置信预测)：先验精度 $\\lambda = 0.1$，查询向量 $x^\\star$ 的分量为 $1$, $2.5$, $2.0$。\n\n- 测试用例 B (增加正则化以收缩参数)：先验精度 $\\lambda = 20.0$，查询向量 $x^\\star$ 的分量为 $1$, $2.5$, $2.0$。\n\n- 测试用例 C (边缘情况：非截距项特征为零)：先验精度 $\\lambda = 0.1$，查询向量 $x^\\star$ 的分量为 $1$, $0.0$, $0.0$。\n\n- 测试用例 D (边界压力：强正则化，大查询幅度)：先验精度 $\\lambda = 50.0$，查询向量 $x^\\star$ 的分量为 $1$, $6.0$, $6.0$。\n\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果，每个结果本身都是一个双元素浮点数列表 $[H(x^\\star), \\operatorname{tr}(\\widehat{\\Sigma})]$，按此顺序排列。例如，打印的输出必须看起来像 $[[h_1,v_1],[h_2,v_2],[h_3,v_3],[h_4,v_4]]$，其中 $h_j$ 和 $v_j$ 是由您的程序计算出的实际数值。", "solution": "该问题要求在贝叶斯 logistic 回归框架内比较模型预测和模型推断。这涉及到推导并实现模型参数的最大后验 (MAP) 估计，然后计算两个不同的度量：一个预测不确定性度量（熵）和一个推断不确定性度量（参数协方差矩阵的迹）。\n\n解决方案分四个阶段进行：\n1.  根据指定的似然和先验，构建对数后验函数。\n2.  推导优化所必需的对数后验的梯度和 Hessian 矩阵。\n3.  指定用于寻找 MAP 估计 $\\hat{\\theta}$ 的二阶优化算法 (Newton-Raphson)。\n4.  定义待计算的预测和推断不确定性度量。\n\n**1. 对数后验公式**\n\n对于给定的特征向量 $x_i \\in \\mathbb{R}^d$，二元标签 $y_i \\in \\{0, 1\\}$ 的模型是伯努利分布，其概率为 $p_i = p(y_i=1 \\mid x_i, \\theta) = \\sigma(\\theta^\\top x_i)$，其中 $\\sigma(z) = (1+e^{-z})^{-1}$ 是 logistic sigmoid 函数。\n\n单个观测 $(x_i, y_i)$ 的似然为 $p(y_i \\mid x_i, \\theta) = p_i^{y_i} (1-p_i)^{1-y_i}$。对数似然为 $\\ln p(y_i \\mid x_i, \\theta) = y_i \\ln p_i + (1-y_i) \\ln(1-p_i)$。使用恒等式 $\\ln p_i = \\ln \\sigma(\\theta^\\top x_i) = -\\ln(1+e^{-\\theta^\\top x_i})$ 和 $\\ln(1-p_i) = \\ln(1-\\sigma(\\theta^\\top x_i)) = -\\theta^\\top x_i - \\ln(1+e^{-\\theta^\\top x_i})$，我们可以将对数似然重写为：\n$$\n\\ln p(y_i \\mid x_i, \\theta) = y_i(\\theta^\\top x_i) - \\ln(1+e^{\\theta^\\top x_i})\n$$\n对于一个包含 $n$ 个独立样本的数据集 $D = \\{(x_i, y_i)\\}_{i=1}^n$，总对数似然是所有样本的总和：\n$$\nLL(\\theta) = \\sum_{i=1}^n \\left( y_i(\\theta^\\top x_i) - \\ln(1+e^{\\theta^\\top x_i}) \\right)\n$$\n问题指定参数向量 $\\theta$ 上的先验为零均值各向同性高斯先验，其精度为 $\\lambda  0$。先验密度 $p(\\theta) \\propto \\exp\\left(-\\frac{\\lambda}{2}\\|\\theta\\|_2^2\\right)$。对数先验（不考虑加性常数）为：\n$$\n\\ln p(\\theta) = -\\frac{\\lambda}{2} \\theta^\\top \\theta\n$$\n我们旨在为 MAP 估计最大化的对数后验函数 $\\mathcal{L}(\\theta)$，是对数似然和对数先验的和：\n$$\n\\mathcal{L}(\\theta) = \\ln p(\\theta \\mid D) \\propto LL(\\theta) + \\ln p(\\theta) = \\left( \\sum_{i=1}^n \\left( y_i(\\theta^\\top x_i) - \\ln(1+e^{\\theta^\\top x_i}) \\right) \\right) - \\frac{\\lambda}{2} \\theta^\\top \\theta\n$$\n\n**2. 对数后验的梯度和 Hessian 矩阵**\n\n为了优化 $\\mathcal{L}(\\theta)$，我们计算它关于 $\\theta$ 的梯度和 Hessian 矩阵。\n\n对数后验的梯度是：\n$$\n\\nabla_{\\theta} \\mathcal{L}(\\theta) = \\sum_{i=1}^n \\nabla_{\\theta}\\left( y_i(\\theta^\\top x_i) - \\ln(1+e^{\\theta^\\top x_i}) \\right) - \\nabla_{\\theta}\\left(\\frac{\\lambda}{2} \\theta^\\top \\theta\\right)\n$$\n使用链式法则，$\\nabla_{\\theta}\\ln(1+e^{\\theta^\\top x_i}) = \\frac{e^{\\theta^\\top x_i}}{1+e^{\\theta^\\top x_i}} x_i = \\sigma(\\theta^\\top x_i)x_i = p_i x_i$。单个样本对数似然的梯度是 $(y_i - p_i)x_i$。对数先验的梯度是 $-\\lambda\\theta$。\n对所有样本求和，完整的梯度是：\n$$\n\\nabla_{\\theta} \\mathcal{L}(\\theta) = \\sum_{i=1}^n (y_i - \\sigma(\\theta^\\top x_i)) x_i - \\lambda\\theta\n$$\n在矩阵表示法中，设 $X$ 为 $n \\times d$ 的设计矩阵，$y$ 为标签向量，$p$ 为概率向量，则梯度为 $g(\\theta) = X^\\top(y-p) - \\lambda\\theta$。\n\nHessian 矩阵是二阶导数矩阵，$\\nabla^2_{\\theta} \\mathcal{L}(\\theta)$。单个样本对数似然项的二阶导数是：\n$$\n\\nabla^2_{\\theta} \\left((y_i - p_i)x_i \\right) = -x_i (\\nabla_{\\theta} p_i)^\\top = -x_i \\left( \\sigma'(\\theta^\\top x_i) x_i \\right)^\\top = - \\sigma(\\theta^\\top x_i)(1-\\sigma(\\theta^\\top x_i)) x_i x_i^\\top\n$$\n对数先验项的 Hessian 矩阵是 $-\\lambda I$，其中 $I$ 是 $d \\times d$ 的单位矩阵。\n对数后验的完整 Hessian 矩阵是：\n$$\nH(\\theta) = \\nabla^2_{\\theta} \\mathcal{L}(\\theta) = - \\sum_{i=1}^n \\sigma(\\theta^\\top x_i)(1-\\sigma(\\theta^\\top x_i)) x_i x_i^\\top - \\lambda I\n$$\n在矩阵表示法中，这是 $H(\\theta) = -X^\\top W X - \\lambda I$，其中 $W$ 是一个 $n \\times n$ 的对角矩阵，其元素为 $W_{ii} = p_i(1-p_i)$。由于 $p_i(1-p_i) \\ge 0$，矩阵 $X^\\top W X$ 是半正定的。对于 $\\lambda  0$，Hessian 矩阵 $H(\\theta)$ 是严格负定的，这意味着对数后验 $\\mathcal{L}(\\theta)$ 是一个严格凹函数。这保证了存在唯一的全局最大值，可以高效地找到。\n\n**3. 通过 Newton-Raphson 方法进行优化**\n\n我们使用一个迭代二阶方法，特别是 Newton-Raphson 算法，来找到最大化 $\\mathcal{L}(\\theta)$ 的 MAP 估计 $\\hat{\\theta}$。从一个初始猜测 $\\theta_0$（例如，零向量）开始，参数通过迭代更新：\n$$\n\\theta_{k+1} = \\theta_k - [H(\\theta_k)]^{-1} g(\\theta_k)\n$$\n其中 $g(\\theta_k)$ 和 $H(\\theta_k)$ 是在当前估计 $\\theta_k$ 处计算的梯度和 Hessian 矩阵。更新步长 $\\Delta\\theta_k = -[H(\\theta_k)]^{-1} g(\\theta_k)$ 通过求解线性系统 $H(\\theta_k)\\Delta\\theta_k = -g(\\theta_k)$ 得到。迭代持续进行，直到更新步长的范数 $\\|\\Delta\\theta_k\\|_2$ 小于一个小的容差。得到的参数向量即为 MAP 估计 $\\hat{\\theta}$。\n\n**4. 推断和预测度量**\n\n收敛到 $\\hat{\\theta}$ 后，我们计算两个指定的量：\n\n*   **推断方差：** 参数估计中的不确定性由后验协方差矩阵捕捉。后验分布在其众数 $\\hat{\\theta}$ 处的局部高斯近似是使用 Hessian 矩阵形成的。该近似的协方差矩阵是在众数处求值的负 Hessian 矩阵的逆：\n    $$\n    \\widehat{\\Sigma} = [-H(\\hat{\\theta})]^{-1} = [X^\\top \\widehat{W} X + \\lambda I]^{-1}\n    $$\n    其中 $\\widehat{W}$ 是在 $\\hat{\\theta}$ 处计算的权重矩阵。该矩阵是由先验正则化的观测费雪信息矩阵。总参数不确定性的度量是该协方差矩阵的迹，$\\operatorname{tr}(\\widehat{\\Sigma})$。\n\n*   **预测熵：** 对于一个新的查询点 $x^\\star$，模型的预测是概率 $\\hat{p}(y=1 \\mid x^\\star) = \\sigma(\\hat{\\theta}^\\top x^\\star)$。这个单一预测的不确定性由相应伯努利分布的香non熵量化，以奈特为单位度量：\n    $$\n    H(x^\\star) = - \\hat{p}^\\star \\ln(\\hat{p}^\\star) - (1-\\hat{p}^\\star)\\ln(1-\\hat{p}^\\star)\n    $$\n    其中 $\\hat{p}^\\star = \\hat{p}(y=1 \\mid x^\\star)$。当预测最不确定时（$\\hat{p}^\\star = 0.5$），熵最大化为 $H=\\ln(2) \\approx 0.693$；当预测完全确定时（$\\hat{p}^\\star = 0$ 或 $\\hat{p}^\\star = 1$），熵为 $0$。\n\n以下程序为指定的数据集和测试用例实现了这整个过程。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef calculate_entropy(p):\n    \"\"\"Calculates the Shannon entropy for a Bernoulli probability p.\"\"\"\n    if p = 1e-12 or p >= 1 - 1e-12:\n        return 0.0\n    return -(p * np.log(p) + (1 - p) * np.log(1 - p))\n\ndef compute_map_estimate(X, y, lam, tol=1e-9, max_iter=100):\n    \"\"\"\n    Computes the MAP estimate for logistic regression using Newton's method.\n    \n    Args:\n        X (np.ndarray): Design matrix of shape (n, d).\n        y (np.ndarray): Target vector of shape (n,).\n        lam (float): Precision of the Gaussian prior.\n        tol (float): Convergence tolerance.\n        max_iter (int): Maximum number of iterations.\n        \n    Returns:\n        tuple: A tuple containing:\n            - theta_hat (np.ndarray): The MAP parameter estimate of shape (d,).\n            - hessian_at_map (np.ndarray): The Hessian of the log-posterior at theta_hat.\n    \"\"\"\n    n, d = X.shape\n    theta = np.zeros(d)\n\n    for i in range(max_iter):\n        # Calculate linear predictors and probabilities\n        z = X @ theta\n        p = 1 / (1 + np.exp(-z))\n\n        # Calculate gradient of the log-posterior\n        grad = X.T @ (y - p) - lam * theta\n\n        # Calculate Hessian of the log-posterior\n        W_diag = p * (1 - p)\n        # Efficiently compute X.T @ W @ X without forming a dense W\n        Hess = -X.T @ (W_diag[:, np.newaxis] * X) - lam * np.identity(d)\n\n        # Solve the Newton step: H * delta = -g\n        delta_theta = np.linalg.solve(Hess, -grad)\n\n        # Update parameters\n        theta += delta_theta\n        \n        # Check for convergence\n        if np.linalg.norm(delta_theta)  tol:\n            break\n    \n    # After convergence, one final calculation of the Hessian at the final theta\n    z_final = X @ theta\n    p_final = 1 / (1 + np.exp(-z_final))\n    W_diag_final = p_final * (1 - p_final)\n    hessian_at_map = -X.T @ (W_diag_final[:, np.newaxis] * X) - lam * np.identity(d)\n    \n    return theta, hessian_at_map\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem, running all test cases.\n    \"\"\"\n    # Define the fixed training set\n    X_train = np.array([\n        # Positive class (y=1)\n        [1.0, 2.0, 2.0],\n        [1.0, 2.5, 1.5],\n        [1.0, 1.7, 2.3],\n        [1.0, 2.2, 1.8],\n        [1.0, 1.8, 2.1],\n        # Negative class (y=0)\n        [1.0, -2.0, -2.0],\n        [1.0, -2.5, -1.5],\n        [1.0, -1.7, -2.3],\n        [1.0, -2.2, -1.8],\n        [1.0, -1.8, -2.1]\n    ])\n    y_train = np.array([1, 1, 1, 1, 1, 0, 0, 0, 0, 0])\n\n    # Define the prescribed test cases\n    test_cases = [\n        # (lambda, x_star)\n        (0.1, np.array([1.0, 2.5, 2.0])), # Case A\n        (20.0, np.array([1.0, 2.5, 2.0])), # Case B\n        (0.1, np.array([1.0, 0.0, 0.0])), # Case C\n        (50.0, np.array([1.0, 6.0, 6.0]))  # Case D\n    ]\n\n    results = []\n    for lam, x_star in test_cases:\n        # 1. Compute MAP estimate and Hessian at MAP\n        theta_hat, hessian_at_map = compute_map_estimate(X_train, y_train, lam)\n        \n        # 2. Compute inferential variance trace\n        # Inferential covariance is the inverse of the negative Hessian\n        neg_hessian = -hessian_at_map\n        try:\n            sigma_hat = np.linalg.inv(neg_hessian)\n            tr_sigma = np.trace(sigma_hat)\n        except np.linalg.LinAlgError:\n            tr_sigma = float('inf') # Should not happen given problem structure\n            \n        # 3. Compute predictive entropy\n        z_star = x_star @ theta_hat\n        p_star = 1 / (1 + np.exp(-z_star))\n        entropy = calculate_entropy(p_star)\n\n        # Store the pair of results\n        results.append([entropy, tr_sigma])\n    \n    # Format the final output string exactly as specified\n    formatted_results = [f\"[{h:.6f},{v:.6f}]\" for h, v in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3149003"}]}