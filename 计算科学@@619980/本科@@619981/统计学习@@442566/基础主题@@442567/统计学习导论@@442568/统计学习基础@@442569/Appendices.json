{"hands_on_practices": [{"introduction": "在评估分类模型的性能时，仅看准确率是远远不够的。本次实践将引导你从基本定义出发，推导召回率 (Recall)、精确率 (Precision) 和 $F_1$ 分数等关键指标之间的内在联系。通过这个练习，你将深刻理解这些指标是如何相互关联，并共同依赖于数据的底层分布，特别是类别不平衡问题中的流行率 $\\pi$ ([@problem_id:3094141])。", "problem": "一个二元分类器在一个由 $N$ 个独立样本组成的数据集上进行评估，这些样本来自一个患病率（真正例的比例）为 $\\pi \\in [0,1]$ 的总体。该分类器被调整以达到目标召回率 $R \\in [0,1]$ 和目标特异性 $S \\in [0,1]$，每个指标都精确地解释为它们在该数据集上的总体对应值。仅依赖患病率、召回率、特异性、精确率和 F1-分数（F1-score）的基本定义，按以下步骤进行：\n\n- 使用召回率和特异性的定义，结合患病率，用 $N$、$ \\pi$、$R$ 和 $S$ 表示混淆矩阵的计数：真正例 ($TP$)、假正例 ($FP$)、真反例 ($TN$) 和假反例 ($FN$)。\n\n- 从这些计数和精确率的定义中，推导出精确率 $P$。\n\n- 使用 F1-分数（F1-score）作为精确率和召回率的调和平均数的定义，或等价地用混淆矩阵的计数来表示，推导出一个仅以 $\\pi$、$R$ 和 $S$ 为函数的 F1-分数的封闭形式表达式。\n\n- 陈述为使这些量有明确定义且计数为非负并可解释为数据集计数，对 $N$、$ \\pi$、$R$ 和 $S$ 所需的可行性约束。\n\n你的最终答案必须是仅以 $\\pi$、$R$ 和 $S$ 为函数的 F1-分数的单一、简化的解析表达式。不需要进行数值评估，也不需要四舍五入。", "solution": "问题陈述被评估为有效。它在科学上基于统计学习和分类指标的基本原理。该问题提法恰当、客观，并包含足够的信息以推导出唯一的解析解。\n\n推导过程如下。\n\n首先，我们定义在大小为 $N$、患病率为 $\\pi$ 的数据集中实际正例和实际反例的总数。\n实际正例数 $P_{actual}$ 由 $P_{actual} = N\\pi$ 给出。\n实际反例数 $N_{actual}$ 由 $N_{actual} = N(1-\\pi)$ 给出。\n\n接下来，我们使用召回率 ($R$) 和特异性 ($S$) 的定义来表示混淆矩阵的四个计数：真正例 ($TP$)、假反例 ($FN$)、真反例 ($TN$) 和假正例 ($FP$)。\n\n召回率，或称真正例率，是实际正例中被正确识别的比例。其定义为：\n$$R = \\frac{TP}{P_{actual}} = \\frac{TP}{TP + FN}$$\n由此，我们可以用 $N$、$\\pi$ 和 $R$ 来表示 $TP$：\n$$TP = R \\cdot P_{actual} = R N\\pi$$\n假反例的数量是未被识别为正例的实际正例：\n$$FN = P_{actual} - TP = N\\pi - R N\\pi = N\\pi(1-R)$$\n\n特异性，或称真反例率，是实际反例中被正确识别的比例。其定义为：\n$$S = \\frac{TN}{N_{actual}} = \\frac{TN}{TN + FP}$$\n由此，我们可以用 $N$、$\\pi$ 和 $S$ 来表示 $TN$：\n$$TN = S \\cdot N_{actual} = S N(1-\\pi)$$\n假正例的数量是未被识别为反例的实际反例：\n$$FP = N_{actual} - TN = N(1-\\pi) - S N(1-\\pi) = N(1-\\pi)(1-S)$$\n\n用给定的参数总结混淆矩阵的计数：\n- $TP = N\\pi R$\n- $FN = N\\pi(1-R)$\n- $TN = N(1-\\pi)S$\n- $FP = N(1-\\pi)(1-S)$\n\n接下来，我们推导精确率 ($P$) 的表达式。精确率，或称阳性预测值，是预测为正例中实际为正例的比例。其定义为：\n$$P = \\frac{TP}{TP + FP}$$\n代入 $TP$ 和 $FP$ 的表达式：\n$$P = \\frac{N\\pi R}{N\\pi R + N(1-\\pi)(1-S)}$$\n假设 $N \\neq 0$，我们可以约去分子和分母中的 $N$：\n$$P = \\frac{\\pi R}{\\pi R + (1-\\pi)(1-S)}$$\n\n现在，我们推导 F1-分数 ($F_1$) 的封闭形式表达式。$F_1$-分数定义为精确率和召回率的调和平均数。一个更直接的、用混淆矩阵计数表示的定义是：\n$$F_1 = \\frac{2TP}{2TP + FP + FN}$$\n这种形式在计算上是稳健的，因为它避免了当精确率未定义时（即，如果 $TP+FP=0$）可能出现的除以零的问题。\n代入 $TP$、$FP$ 和 $FN$ 的表达式：\n$$F_1 = \\frac{2(N\\pi R)}{2(N\\pi R) + N(1-\\pi)(1-S) + N\\pi(1-R)}$$\n再次假设 $N \\neq 0$，我们约去所有项中的 $N$：\n$$F_1 = \\frac{2\\pi R}{2\\pi R + (1-\\pi)(1-S) + \\pi(1-R)}$$\n为简化，我们展开并合并分母中的项：\n$$ \\text{分母} = 2\\pi R + (1 - S - \\pi + \\pi S) + (\\pi - \\pi R) $$\n$$ \\text{分母} = (2\\pi R - \\pi R) + (-\\pi + \\pi) + 1 - S + \\pi S $$\n$$ \\text{分母} = \\pi R + 1 - S + \\pi S $$\n这可以因式分解以对包含 $\\pi$ 的项进行分组：\n$$ \\text{分母} = \\pi(R+S) + 1 - S $$\n因此，F1-分数的最终简化表达式是：\n$$F_1 = \\frac{2\\pi R}{\\pi(R+S) + 1 - S}$$\n\n最后，我们陈述可行性约束。\n给定的参数 $\\pi, R, S$ 定义在区间 $[0,1]$ 内。\n样本数 $N$ 必须是一个非负整数，通常 $N \\in \\mathbb{Z}^+$。\n为了使问题在真实数据集上可解释，计数 $TP$、$FP$、$TN$ 和 $FN$ 必须是非负整数。\n- 因为 $N \\ge 0$ 且 $\\pi, R, S \\in [0,1]$，所以非负性得到保证。\n- 为使计数为整数，必须满足以下条件：\n  1. 实际正例数 $N\\pi$ 必须是整数。\n  2. 实际反例数 $N(1-\\pi)$ 必须是整数。（如果 $N$ 是整数，这由条件1所隐含）。\n  3. 真正例数 $TP = (N\\pi)R$ 必须是整数。\n  4. 真反例数 $TN = N(1-\\pi)S$ 必须是整数。\n只要分母不为零，$F_1$ 的派生表达式就是明确定义的。分母 $\\pi(R+S) + 1 - S$ 为零当且仅当 $\\pi=0$ 且 $S=1$。这对应于没有正例且分类器不做任何正向预测（$FP=0$）的场景。在这种特殊情况下，$TP$、$FP$ 和 $FN$ 均为 $0$，按照惯例导致 $F_1$ 分数为 $0$。在这种情况下，我们的公式会产生不确定形式 $\\frac{0}{0}$，但当 $\\pi \\to 0$ 时，对于任何 $S  1$，其极限正确地计算为 $0$。", "answer": "$$\\boxed{\\frac{2\\pi R}{\\pi(R+S) + 1 - S}}$$", "id": "3094141"}, {"introduction": "偏差-方差权衡 (Bias-Variance Trade-off) 是统计学习领域的中心议题，构成了模型选择的核心理论基础。本练习将通过分析一个多项式回归模型，让你亲手推导预测误差的完整表达式，并将其分解为不可约误差、偏差和方差三部分。这个推导过程将使模型复杂度 $k$ 与样本量 $n$ 之间的抽象权衡关系，变得具体且可量化 ([@problem_id:3121938])。", "problem": "考虑一个在区间 $[-1,1]$ 上的一维回归问题。设 $x$ 从区间 $[-1,1]$ 上的均匀分布中抽取，响应由 $y = f^{\\star}(x) + \\varepsilon$ 生成，其中 $\\varepsilon$ 是独立噪声，满足 $\\mathbb{E}[\\varepsilon \\mid x] = 0$ 和 $\\operatorname{Var}(\\varepsilon \\mid x) = \\sigma^{2}$。假设未知的回归函数 $f^{\\star}$ 在均匀测度下，对于区间 $[-1,1]$ 上的一个标准正交多项式基 $\\{\\phi_{j}\\}_{j \\ge 0}$ 存在一个 $L^{2}$ 展开，即 $f^{\\star}(x) = \\sum_{j=0}^{\\infty} \\theta_{j} \\phi_{j}(x)$，且 $\\sum_{j=0}^{\\infty} \\theta_{j}^{2}  \\infty$，其中 $\\int_{-1}^{1} \\phi_{j}(x)\\phi_{\\ell}(x)\\,\\frac{dx}{2} = \\mathbf{1}\\{j=\\ell\\}$。\n\n您收集了一个大小为 $n$ 的训练样本 $\\{(x_{i},y_{i})\\}_{i=1}^{n}$，其中 $x_{i}$ 是确定性选择的，使得前 $k+1$ 个基函数相对于设计点上的离散均匀测度是经验性标准正交的，并且与所有更高阶的基函数正交：\n\n$$\n\\frac{1}{n}\\sum_{i=1}^{n} \\phi_{j}(x_{i})\\,\\phi_{\\ell}(x_{i}) = \n\\begin{cases}\n1,  j=\\ell \\le k,\\\\\n0,  j \\ne \\ell,\\; j,\\ell \\le k,\\\\\n0,  \\ell \\le k  j.\n\\end{cases}\n$$\n\n您通过在特征 $\\{\\phi_{0},\\dots,\\phi_{k}\\}$ 上使用普通最小二乘法来拟合一个 $k$ 次多项式最小二乘预测器 $\\widehat{f}_{k}(x) = \\sum_{j=0}^{k} \\widehat{\\beta}_{j}\\,\\phi_{j}(x)$。\n\n仅使用第一性原理（即条件期望和方差的定义、上述的标准正交关系以及最小二乘法的正规方程），推导对于从相同数据生成过程中独立抽取的一个新测试点 $(x,y)$ 的期望积分平方预测误差的偏差-方差分解：\n\n$$\n\\mathcal{R}_{k,n} \\equiv \\mathbb{E}\\big[(y - \\widehat{f}_{k}(x))^{2}\\big],\n$$\n\n其中期望是针对训练噪声、训练输入、测试输入 $x$ 和测试噪声计算的。将您的最终结果表示为 $k$、$n$、$\\sigma^{2}$ 和系数 $\\{\\theta_{j}\\}_{j \\ge 0}$ 的封闭形式函数。您的最终答案必须是一个单一的封闭形式解析表达式。无需四舍五入。", "solution": "该问题被评估为有效，因为它具有科学依据、问题设定良好且客观。它代表了统计学习中的一个标准理论练习。因此，我们可以进行推导。\n\n需要分析的量是期望积分平方预测误差，由下式给出\n$$ \\mathcal{R}_{k,n} \\equiv \\mathbb{E}\\big[(y - \\widehat{f}_{k}(x))^{2}\\big] $$\n总期望 $\\mathbb{E}$ 是对所有随机性来源求的：测试点的输入 $x$ 及其相关噪声 $\\varepsilon$，以及训练数据中的噪声，我们将其表示为 $\\{\\varepsilon_{i}\\}_{i=1}^{n}$。训练输入 $\\{x_i\\}_{i=1}^n$ 是确定性的。拟合模型 $\\widehat{f}_k$ 依赖于训练数据 $\\{(x_i, y_i)\\}_{i=1}^n$，因此也依赖于训练噪声 $\\{\\varepsilon_i\\}_{i=1}^n$。\n\n我们首先应用全期望定律，以训练数据 $\\mathcal{D} = \\{(x_i, y_i)\\}_{i=1}^n$ 和测试输入 $x$ 为条件。响应 $y$ 由 $y = f^{\\star}(x)+\\varepsilon$ 给出。\n$$ \\mathcal{R}_{k,n} = \\mathbb{E}_{\\mathcal{D}, x} \\left[ \\mathbb{E}_{\\varepsilon | \\mathcal{D}, x} \\left[ (f^{\\star}(x) + \\varepsilon - \\widehat{f}_{k}(x))^2 \\mid \\mathcal{D}, x \\right] \\right] $$\n展开平方项，我们得到：\n$$ (f^{\\star}(x) - \\widehat{f}_{k}(x))^2 + 2\\varepsilon(f^{\\star}(x) - \\widehat{f}_{k}(x)) + \\varepsilon^2 $$\n交叉项的条件期望为零，因为测试噪声 $\\varepsilon$ 独立于 $\\mathcal{D}$ 和 $x$，并且 $\\mathbb{E}[\\varepsilon | x] = 0$：\n$$ \\mathbb{E}_{\\varepsilon | \\mathcal{D}, x} \\left[ 2\\varepsilon(f^{\\star}(x) - \\widehat{f}_{k}(x)) \\mid \\mathcal{D}, x \\right] = 2(f^{\\star}(x) - \\widehat{f}_{k}(x))\\mathbb{E}_{\\varepsilon | x}[\\varepsilon] = 0 $$\n$\\varepsilon^2$ 项的条件期望是噪声的条件方差，因为其均值为零：\n$$ \\mathbb{E}_{\\varepsilon | \\mathcal{D}, x} [\\varepsilon^2 \\mid \\mathcal{D}, x] = \\mathbb{E}_{\\varepsilon|x}[\\varepsilon^2 | x] = \\operatorname{Var}(\\varepsilon|x) + (\\mathbb{E}[\\varepsilon|x])^2 = \\sigma^2 + 0^2 = \\sigma^2 $$\n将这些代回，风险变为：\n$$ \\mathcal{R}_{k,n} = \\mathbb{E}_{\\mathcal{D}, x} \\left[ (f^{\\star}(x) - \\widehat{f}_{k}(x))^2 + \\sigma^2 \\right] = \\mathbb{E}_{\\mathcal{D}, x} \\left[ (f^{\\star}(x) - \\widehat{f}_{k}(x))^2 \\right] + \\sigma^2 $$\n项 $\\sigma^2$ 是不可约误差。余下的项是平均积分平方误差 (MISE)。我们通过引入平均预测器函数 $\\bar{f}_k(x) = \\mathbb{E}_{\\mathcal{D}}[\\widehat{f}_k(x)]$ 将其分解为偏差和方差分量。\n$$ \\mathbb{E}_{\\mathcal{D}, x} \\left[ (f^{\\star}(x) - \\widehat{f}_{k}(x))^2 \\right] = \\mathbb{E}_{\\mathcal{D}, x} \\left[ (f^{\\star}(x) - \\bar{f}_k(x) + \\bar{f}_k(x) - \\widehat{f}_{k}(x))^2 \\right] $$\n展开平方项，并注意到交叉项 $\\mathbb{E}_{\\mathcal{D}}[(f^{\\star}(x) - \\bar{f}_k(x))(\\bar{f}_k(x) - \\widehat{f}_{k}(x))]$ 因为 $\\mathbb{E}_{\\mathcal{D}}[\\bar{f}_k(x) - \\widehat{f}_{k}(x)] = 0$ 而消失，我们得到标准分解：\n$$ \\mathcal{R}_{k,n} = \\underbrace{\\mathbb{E}_{x}\\left[(f^{\\star}(x) - \\bar{f}_k(x))^2\\right]}_{\\text{积分平方偏差}} + \\underbrace{\\mathbb{E}_{\\mathcal{D},x}\\left[(\\widehat{f}_{k}(x) - \\bar{f}_k(x))^2\\right]}_{\\text{积分方差}} + \\sigma^2 $$\n我们现在推导每一项。这需要找到普通最小二乘 (OLS) 系数 $\\widehat{\\beta}_j$。OLS 估计器最小化 $\\sum_{i=1}^n (y_i - \\sum_{j=0}^k \\beta_j \\phi_j(x_i))^2$。正规方程为：\n$$ \\sum_{j=0}^{k} \\widehat{\\beta}_j \\left(\\frac{1}{n}\\sum_{i=1}^{n} \\phi_j(x_i) \\phi_{\\ell}(x_i)\\right) = \\frac{1}{n}\\sum_{i=1}^{n} y_i \\phi_{\\ell}(x_i), \\quad \\text{for } \\ell=0, \\dots, k $$\n使用给定的关于设计点 $\\{x_i\\}$ 的经验性标准正交条件，左侧简化为 $\\widehat{\\beta}_{\\ell}$。因此，对于 $j=0, \\dots, k$：\n$$ \\widehat{\\beta}_{j} = \\frac{1}{n} \\sum_{i=1}^{n} y_i \\phi_j(x_i) $$\n接下来，我们求平均预测器 $\\bar{f}_k(x) = \\sum_{j=0}^k \\mathbb{E}_{\\mathcal{D}}[\\widehat{\\beta}_j] \\phi_j(x)$。期望是针对训练噪声计算的。由于 $y_i = f^{\\star}(x_i) + \\varepsilon_i$ 且 $\\mathbb{E}[\\varepsilon_i|x_i]=0$，我们有 $\\mathbb{E}_{\\mathcal{D}}[y_i] = f^{\\star}(x_i)$。\n$$ \\mathbb{E}_{\\mathcal{D}}[\\widehat{\\beta}_j] = \\frac{1}{n} \\sum_{i=1}^{n} \\mathbb{E}_{\\mathcal{D}}[y_i] \\phi_j(x_i) = \\frac{1}{n} \\sum_{i=1}^{n} f^{\\star}(x_i) \\phi_j(x_i) $$\n代入展开式 $f^{\\star}(x_i) = \\sum_{\\ell=0}^{\\infty} \\theta_{\\ell} \\phi_{\\ell}(x_i)$：\n$$ \\mathbb{E}_{\\mathcal{D}}[\\widehat{\\beta}_j] = \\frac{1}{n} \\sum_{i=1}^{n} \\left(\\sum_{\\ell=0}^{\\infty} \\theta_{\\ell} \\phi_{\\ell}(x_i)\\right) \\phi_j(x_i) = \\sum_{\\ell=0}^{\\infty} \\theta_{\\ell} \\left(\\frac{1}{n}\\sum_{i=1}^{n} \\phi_{\\ell}(x_i) \\phi_j(x_i)\\right) $$\n使用所提供的对于 $j \\le k$ 的经验性标准正交条件，括号中的项在 $\\ell \\le k$ 时为 $\\mathbf{1}\\{j=\\ell\\}$，在 $\\ell > k$ 时为 0。因此，对于 $j \\le k$：\n$$ \\mathbb{E}_{\\mathcal{D}}[\\widehat{\\beta}_j] = \\theta_j $$\n平均预测器是 $f^{\\star}$ 在所选函数空间上的投影：\n$$ \\bar{f}_k(x) = \\sum_{j=0}^{k} \\theta_j \\phi_j(x) $$\n现在我们可以计算偏差项。期望 $\\mathbb{E}_x$ 对应于在 $[-1,1]$ 上对 $\\frac{dx}{2}$ 的积分。\n$$ \\text{Bias}^2 = \\mathbb{E}_{x}\\left[ \\left( f^{\\star}(x) - \\bar{f}_k(x) \\right)^2 \\right] = \\mathbb{E}_{x}\\left[ \\left( \\sum_{j=0}^{\\infty} \\theta_j \\phi_j(x) - \\sum_{j=0}^{k} \\theta_j \\phi_j(x) \\right)^2 \\right] $$\n$$ = \\mathbb{E}_{x}\\left[ \\left( \\sum_{j=k+1}^{\\infty} \\theta_j \\phi_j(x) \\right)^2 \\right] = \\sum_{j=k+1}^{\\infty} \\sum_{\\ell=k+1}^{\\infty} \\theta_j \\theta_{\\ell} \\mathbb{E}_{x}[\\phi_j(x)\\phi_{\\ell}(x)] $$\n使用总体标准正交性，$\\mathbb{E}_{x}[\\phi_j(x)\\phi_{\\ell}(x)] = \\int_{-1}^1 \\phi_j(x)\\phi_{\\ell}(x)\\frac{dx}{2} = \\mathbf{1}\\{j=\\ell\\}$，偏差项为：\n$$ \\text{积分平方偏差} = \\sum_{j=k+1}^{\\infty} \\theta_j^2 $$\n接下来，我们计算方差项。\n$$ \\text{Variance} = \\mathbb{E}_{\\mathcal{D},x}\\left[ (\\widehat{f}_{k}(x) - \\bar{f}_k(x))^2 \\right] = \\mathbb{E}_{\\mathcal{D}} \\left[ \\mathbb{E}_{x} \\left[ \\left( \\sum_{j=0}^{k} (\\widehat{\\beta}_j - \\theta_j)\\phi_j(x) \\right)^2 \\right] \\right] $$\n由于总体标准正交性，关于 $x$ 的内层期望得以简化：\n$$ \\mathbb{E}_{x} \\left[ \\sum_{j=0}^{k} \\sum_{\\ell=0}^{k} (\\widehat{\\beta}_j - \\theta_j)(\\widehat{\\beta}_{\\ell} - \\theta_{\\ell}) \\phi_j(x)\\phi_{\\ell}(x) \\right] = \\sum_{j=0}^{k} (\\widehat{\\beta}_j - \\theta_j)^2 $$\n积分方差则是对训练数据的期望：\n$$ \\text{Integrated Variance} = \\mathbb{E}_{\\mathcal{D}}\\left[ \\sum_{j=0}^{k} (\\widehat{\\beta}_j - \\theta_j)^2 \\right] = \\sum_{j=0}^{k} \\mathbb{E}_{\\mathcal{D}}\\left[ (\\widehat{\\beta}_j - \\theta_j)^2 \\right] = \\sum_{j=0}^{k} \\operatorname{Var}_{\\mathcal{D}}(\\widehat{\\beta}_j) $$\n因为 $\\mathbb{E}_{\\mathcal{D}}[\\widehat{\\beta}_j] = \\theta_j$。我们计算系数的方差。\n$$ \\widehat{\\beta}_j - \\theta_j = \\left( \\frac{1}{n}\\sum_{i=1}^n y_i \\phi_j(x_i) \\right) - \\left( \\frac{1}{n}\\sum_{i=1}^n f^{\\star}(x_i) \\phi_j(x_i) \\right) = \\frac{1}{n}\\sum_{i=1}^n \\varepsilon_i \\phi_j(x_i) $$\n噪声 $\\{\\varepsilon_i\\}$ 是独立的，方差为 $\\sigma^2$，而 $\\phi_j(x_i)$ 是确定性常数。\n$$ \\operatorname{Var}_{\\mathcal{D}}(\\widehat{\\beta}_j) = \\operatorname{Var}\\left( \\frac{1}{n}\\sum_{i=1}^n \\varepsilon_i \\phi_j(x_i) \\right) = \\frac{1}{n^2} \\sum_{i=1}^n \\operatorname{Var}(\\varepsilon_i \\phi_j(x_i)) = \\frac{1}{n^2} \\sum_{i=1}^n (\\phi_j(x_i))^2 \\operatorname{Var}(\\varepsilon_i) $$\n$$ = \\frac{\\sigma^2}{n^2} \\sum_{i=1}^n (\\phi_j(x_i))^2 = \\frac{\\sigma^2}{n} \\left(\\frac{1}{n}\\sum_{i=1}^n \\phi_j(x_i)\\phi_j(x_i)\\right) $$\n使用对于 $j \\le k$ 的经验性标准正交性，括号中的项为 1。\n$$ \\operatorname{Var}_{\\mathcal{D}}(\\widehat{\\beta}_j) = \\frac{\\sigma^2}{n} $$\n积分方差是 $j=0, \\dots, k$ 的总和：\n$$ \\text{积分方差} = \\sum_{j=0}^{k} \\frac{\\sigma^2}{n} = \\frac{(k+1)\\sigma^2}{n} $$\n最后，我们将这三个分量组合起来，得到总期望误差 $\\mathcal{R}_{k,n}$。\n$$ \\mathcal{R}_{k,n} = \\text{积分平方偏差} + \\text{积分方差} + \\text{不可约误差} $$\n$$ \\mathcal{R}_{k,n} = \\sum_{j=k+1}^{\\infty} \\theta_j^2 + \\frac{(k+1)\\sigma^2}{n} + \\sigma^2 $$\n这个表达式代表了期望预测误差的完整偏差-方差分解。", "answer": "$$\\boxed{\\sigma^2 + \\frac{\\sigma^2(k+1)}{n} + \\sum_{j=k+1}^{\\infty} \\theta_j^2}$$", "id": "3121938"}, {"introduction": "为了防止模型过拟合，我们需要一种方法来衡量和控制其“学习能力”，即模型容量 (Model Capacity)。本练习将通过一个经典的阈值分类器案例，让你具体计算 Vapnik-Chervonenkis (VC) 维，这是衡量模型复杂度的基石。通过这个实践，你将看到增长函数 (Growth Function) 和 VC 维等理论工具如何转化为关于模型泛化能力和所需样本量的实际保证 ([@problem_id:3122009])。", "problem": "考虑实数轴上的二元分类问题，其假设类为阈值分类器。对于每个阈值 $t \\in \\mathbb{R}$，定义分类器 $h_t : \\mathbb{R} \\to \\{0,1\\}$ 为 $h_t(x) = \\mathbb{I}\\{x \\ge t\\}$，其中 $\\mathbb{I}\\{\\cdot\\}$ 是指示函数。假设数据从 $\\mathbb{R} \\times \\{0,1\\}$ 上的一个未知分布中独立同分布地抽取，并且该分布对于此假设类是可实现的：存在一个未知的 $t_{\\star} \\in \\mathbb{R}$，使得 $Y = \\mathbb{I}\\{X \\ge t_{\\star}\\}$ 几乎必然成立。\n\n从增长函数和 Vapnik–Chervonenkis (VC) 维的定义出发，并仅使用基本的概率工具和独立性，完成以下任务：\n\n1. 计算 $\\mathbb{R}$ 上阈值分类器类的增长函数 $\\Pi_{\\mathcal{H}}(n)$，其中 $\\Pi_{\\mathcal{H}}(n)$ 定义为，在 $\\mathbb{R}$ 中所有包含 $n$ 个不同点的数据集上，假设类 $\\mathcal{H}$ 中的分类器能够在这些点上产生的不同 $\\{0,1\\}$ 标注方式的最大数量。\n\n2. 确定 $\\mathcal{H}$ 的 Vapnik–Chervonenkis 维 $d_{\\mathrm{VC}}$。\n\n3. 推导一个非渐近的可实现情况下的样本量条件，该条件确保经验风险最小化具有以下泛化性质：以至少 $1 - \\delta$ 的概率，任何在 $m$ 个独立同分布样本上达到零经验误差的 $\\mathcal{H}$ 中的分类器，其真实误差至多为 $\\epsilon$。用 $\\epsilon$、$\\delta$ 和增长函数明确表示此条件，然后使用你计算出的 $\\Pi_{\\mathcal{H}}(\\cdot)$ 将其具体化。\n\n以 $\\Pi_{\\mathcal{H}}(n)$ 和 $d_{\\mathrm{VC}}$ 这两个量的顺序，将你的最终答案格式化为单行。不需要进行数值四舍五入，也不涉及任何物理单位。", "solution": "此问题将首先被验证，若有效，则按要求分三部分解决：计算增长函数、确定 Vapnik-Chervonenkis (VC) 维，以及为可实现情况推导样本量条件。\n\n### 问题验证\n\n**步骤 1：提取已知条件**\n- **假设类**：$\\mathcal{H} = \\{h_t : \\mathbb{R} \\to \\{0,1\\} \\mid h_t(x) = \\mathbb{I}\\{x \\ge t\\}, t \\in \\mathbb{R}\\}$。这些分类器是实数轴上的阈值分类器。\n- **数据分布**：数据点 $(X, Y)$ 从 $\\mathbb{R} \\times \\{0,1\\}$ 上的一个未知分布中独立同分布 (i.i.d.) 地抽取。\n- **可实现性假设**：存在一个真实阈值 $t_{\\star} \\in \\mathbb{R}$，使得标签由规则 $Y = \\mathbb{I}\\{X \\ge t_{\\star}\\}$ 几乎必然地生成。\n- **任务 1**：计算增长函数 $\\Pi_{\\mathcal{H}}(n) = \\max_{x_1, \\dots, x_n \\in \\mathbb{R}} |\\mathcal{H}|_{\\{x_1, \\dots, x_n\\}}|$，其中 $|\\mathcal{H}|_{\\{x_1, \\dots, x_n\\}}|$ 是假设类 $\\mathcal{H}$ 中的分类器在点集 $\\{x_1, \\dots, x_n\\}$ 上能产生的不同标注方式的数量。\n- **任务 2**：确定 VC 维，$d_{\\mathrm{VC}} = \\max\\{n \\in \\mathbb{N} \\mid \\Pi_{\\mathcal{H}}(n) = 2^n\\}$。\n- **任务 3**：推导关于样本量 $m$ 的一个条件，保证以至少 $1 - \\delta$ 的概率，任何经验误差为零的分类器 $\\hat{h} \\in \\mathcal{H}$ 的真实误差至多为 $\\epsilon$。\n\n**步骤 2：使用提取的已知条件进行验证**\n- **科学或事实不准确**：该问题是统计学习理论中一个标准的、基础的练习。阈值分类器、增长函数和 VC 维等概念在数学上和科学上都是合理的。没有发现缺陷。\n- **无法形式化或不相关**：该问题表述规范，并且与统计学习的基础直接相关。没有发现缺陷。\n- **设置不完整或矛盾**：该问题是自洽的。所有必要的定义（假设类、增长函数、VC 维）和假设（可实现性、独立同分布数据）都已提供，并且相互一致。没有发现缺陷。\n- **不切实际或不可行**：该设置是一个简化但经典的教学模型，用于学习理论的教学目的。它在物理上或科学上并非不合理。没有发现缺陷。\n- **不适定或结构不良**：该问题是适定的。待计算的量（$\\Pi_{\\mathcal{H}}(n)$ 和 $d_{\\mathrm{VC}}$）由问题定义唯一确定。样本复杂度界的推导是一个标准过程。没有发现缺陷。\n- **伪深刻、琐碎或同义反复**：虽然这是一个经典的例子，但解决它需要对基本定义有清晰的理解和正确的应用。它并非琐碎或人为构造。没有发现缺陷。\n- **超出科学可验证范围**：这些论断在数学上是可推导和可验证的。没有发现缺陷。\n\n**步骤 3：结论与行动**\n此问题有效。将提供完整解答。\n\n### 解答\n\n**第 1 部分：计算增长函数 $\\Pi_{\\mathcal{H}}(n)$**\n\n增长函数 $\\Pi_{\\mathcal{H}}(n)$ 是假设类 $\\mathcal{H}$ 能够标注一个包含 $n$ 个点的数据集的最大方式数。令 $S = \\{x_1, x_2, \\dots, x_n\\}$ 为 $\\mathbb{R}$ 中任意 $n$ 个不同点的集合。为了分析这些二分，我们可以不失一般性地将这些点排序：$x_{(1)}  x_{(2)}  \\dots  x_{(n)}$。\n\n分类器 $h_t(x) = \\mathbb{I}\\{x \\ge t\\}$ 将大于或等于阈值 $t$ 的点标注为 $1$，将小于 $t$ 的点标注为 $0$。对于这些有序点，其标签向量为 $(h_t(x_{(1)}), h_t(x_{(2)}), \\dots, h_t(x_{(n)}))$。具体的标注方式取决于阈值 $t$ 相对于点 $x_{(i)}$ 的位置。\n\n这 $n$ 个不同的点将实数轴划分为 $n+1$ 个开区间。我们分析将 $t$ 放置在这些区域中每一个时所产生的标注。\n1.  如果 $t > x_{(n)}$，那么对所有 $i \\in \\{1, \\dots, n\\}$，都有 $x_{(i)}  t$。因此，对所有 $i$，$h_t(x_{(i)}) = 0$。标签向量为 $(0, 0, \\dots, 0)$。\n2.  如果 $x_{(n-1)}  t \\le x_{(n)}$，那么 $h_t(x_{(n)}) = 1$，并且对所有 $i  n$，都有 $x_{(i)}  t$，所以 $h_t(x_{(i)}) = 0$。标签向量为 $(0, 0, \\dots, 0, 1)$。\n3.  如果 $x_{(n-2)}  t \\le x_{(n-1)}$，那么 $h_t(x_{(n-1)}) = 1$ 且 $h_t(x_{(n)}) = 1$。对所有 $i  n-1$，都有 $x_{(i)}  t$，所以 $h_t(x_{(i)}) = 0$。标签向量为 $(0, 0, \\dots, 1, 1)$。\n...\nk. 一般地，对于 $k \\in \\{1, \\dots, n\\}$，如果我们放置阈值使得 $x_{(k-1)}  t \\le x_{(k)}$（约定 $x_{(0)} = -\\infty$），那么对所有 $i \\ge k$，有 $h_t(x_{(i)}) = 1$；对所有 $i  k$，有 $h_t(x_{(i)}) = 0$。\n...\nn+1. 如果 $t \\le x_{(1)}$，那么对所有 $i \\in \\{1, \\dots, n\\}$，都有 $x_{(i)} \\ge t$。因此，对所有 $i$，$h_t(x_{(i)}) = 1$。标签向量为 $(1, 1, \\dots, 1)$。\n\n让我们列出对有序点 $(x_{(1)}, \\dots, x_{(n)})$ 生成的不同标注：\n- $(0, 0, \\dots, 0, 0)$\n- $(0, 0, \\dots, 0, 1)$\n- $(0, 0, \\dots, 1, 1)$\n- ...\n- $(0, 1, \\dots, 1, 1)$\n- $(1, 1, \\dots, 1, 1)$\n\n有 $n$ 种标注方式至少包含一个 $0$ 和一个 $1$，再加上全零向量和全一向量。这总共给出了 $n+1$ 种不同的标注方式。例如，对于任何 $t \\in (x_{(n-j)}, x_{(n-j+1)}]$，会生成恰好有 $j$ 个 $1$ 的标注 $(0, \\dots, 0, 1, \\dots, 1)$。由于所有的二分都是“某个点右侧的所有点都被标注为 1”的形式，我们无法在有序点上生成例如 $(1, 0, \\dots)$ 这样的标注。\n\n无论这 $n$ 个不同点的具体位置如何，不同二分的数量都是 $n+1$。因此，在所有大小为 $n$ 的点集上的最大数量也是 $n+1$。\n$$ \\Pi_{\\mathcal{H}}(n) = n+1 $$\n\n**第 2 部分：确定 Vapnik-Chervonenkis 维 $d_{\\mathrm{VC}}$**\n\nVC 维 $d_{\\mathrm{VC}}$ 定义为假设类 $\\mathcal{H}$ 能够打散一个包含 $n$ 个点的集合的最大整数 $n$，这意味着它可以为该集合生成所有 $2^n$ 种可能的标注。这等价于找到满足 $\\Pi_{\\mathcal{H}}(n) = 2^n$ 的最大整数 $n$。\n\n使用我们刚刚计算的增长函数，我们需要找到满足以下条件的最大整数 $n$：\n$$ n+1 = 2^n $$\n我们来测试一些小的整数 $n$ 值：\n- 当 $n=1$ 时：$1+1 = 2$ 且 $2^1 = 2$。等式成立。因此，$\\mathcal{H}$ 可以打散 1 个点。确实，对于任意点 $x_1$，标注 $\\{1\\}$ 可由 $h_{x_1}$ 产生，而标注 $\\{0\\}$ 可由 $h_{x_1+\\delta}$（对于任意 $\\delta>0$）产生。\n- 当 $n=2$ 时：$2+1 = 3$ 且 $2^2 = 4$。等式不成立 ($3  4$)。如第 1 部分所示，对于任意两点 $x_1  x_2$，无法生成标注 $(1,0)$，因为如果 $h_t(x_1)=1$，则 $t \\le x_1$，这意味着 $t  x_2$，所以 $h_t(x_2)$ 也必须是 $1$。\n- 当 $n > 2$ 时：可以通过归纳法证明，对于 $n > 1$，$n+1  2^n$。\n\n使 $\\Pi_{\\mathcal{H}}(n) = 2^n$ 成立的最大整数 $n$ 是 $n=1$。\n$$ d_{\\mathrm{VC}} = 1 $$\n\n**第 3 部分：推导样本量条件**\n\n我们处于可实现设定中，并寻求一个样本量 $m$，使得以至少 $1-\\delta$ 的概率，任何经验风险为零（$R_S(\\hat{h}) = 0$）的假设 $\\hat{h} \\in \\mathcal{H}$ 的真实风险至多为 $\\epsilon$（$R(\\hat{h}) \\le \\epsilon$）。这是一个标准的可能近似正确 (PAC) 学习保证。\n\n该条件可以表述为对“坏”事件概率的界定，即存在一个与样本一致但真实误差很高的假设：\n$$ P(\\exists h \\in \\mathcal{H} \\text{ such that } R_S(h)=0 \\text{ and } R(h) > \\epsilon) \\le \\delta $$\n令 $H_{bad} = \\{h \\in \\mathcal{H} \\mid R(h) > \\epsilon\\}$。我们关心的事件是，我们的大小为 $m$ 的样本 $S$ 被 $H_{bad}$ 中至少一个假设完美标注。\n\n界定此概率的一个标准方法是应用联合界。对（可能无限的）集合 $H_{bad}$ 进行朴素的联合界是不可行的。取而代之的是，将界应用于假设类 $\\mathcal{H}$ 能够标注特定样本点 $x_1, \\dots, x_m$ 的有限多种不同方式上。这种标注的数量至多为 $\\Pi_{\\mathcal{H}}(m)$。对于任何固定的“坏”假设 $h \\in H_{bad}$，它与大小为 $m$ 的随机样本一致的概率是 $(1-R(h))^m  (1-\\epsilon)^m$。一个结合了这些思想的简化论证，得出了关于坏事件概率的以下界：\n$$ P(\\text{bad event}) \\le \\Pi_{\\mathcal{H}}(m)(1-\\epsilon)^m $$\n虽然一个完全严格的证明需要更高级的技术，如对称化，但这个界是一个常见的起点，并抓住了本质的权衡关系。我们将此上界设为小于或等于 $\\delta$：\n$$ \\Pi_{\\mathcal{H}}(m)(1-\\epsilon)^m \\le \\delta $$\n为了找到关于 $m$ 的条件，我们可以对两边取自然对数：\n$$ \\ln(\\Pi_{\\mathcal{H}}(m)) + m \\ln(1-\\epsilon) \\le \\ln(\\delta) $$\n使用著名不等式 $\\ln(1-x) \\le -x$（对于 $x \\in [0,1)$），我们有 $\\ln(1-\\epsilon) \\le -\\epsilon$。代入这个不等式可以得到一个更简单（尽管稍微宽松）的条件：\n$$ \\ln(\\Pi_{\\mathcal{H}}(m)) - m\\epsilon \\le \\ln(\\delta) $$\n重新整理这个不等式以表达关于 $m$ 的条件，得到：\n$$ m\\epsilon \\ge \\ln(\\Pi_{\\mathcal{H}}(m)) + \\ln\\left(\\frac{1}{\\delta}\\right) $$\n$$ m \\ge \\frac{1}{\\epsilon} \\left( \\ln(\\Pi_{\\mathcal{H}}(m)) + \\ln\\left(\\frac{1}{\\delta}\\right) \\right) $$\n这是一个用增长函数 $\\Pi_{\\mathcal{H}}(\\cdot)$、$\\epsilon$ 和 $\\delta$ 表示的一般样本量条件。\n\n现在，我们使用阈值分类器类的增长函数 $\\Pi_{\\mathcal{H}}(m) = m+1$ 来具体化这个结果：\n$$ m \\ge \\frac{1}{\\epsilon} \\left( \\ln(m+1) + \\ln\\left(\\frac{1}{\\delta}\\right) \\right) $$\n这个隐式不等式就是所求的样本量条件。它表明，对于固定的 $\\epsilon$ 和 $\\delta$，所需的样本量 $m$ 与其自身成对数增长，导致总样本复杂度大致与 $\\frac{1}{\\epsilon}\\ln(\\frac{1}{\\epsilon\\delta})$ 成正比。", "answer": "$$\n\\boxed{n+1, 1}\n$$", "id": "3122009"}]}