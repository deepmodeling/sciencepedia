{"hands_on_practices": [{"introduction": "本练习将探讨参数化与非参数化模型之间的一个根本区别：它们对特征缩放的敏感度。通过比较对标准化数据应用的普通最小二乘（OLS）模型与Nadaraya-Watson核回归，你将亲身观察到为什么像缩放这样的预处理步骤不仅是常规操作，而且至关重要，特别是对于基于距离的非参数化方法。这个实践将巩固你对模型内部机制——拟合系数与计算距离——如何决定其行为的理解。[@problem_id:3155821]", "problem": "您将实现一个原则性的比较，对比参数化方法和非参数化方法，以研究特征缩放对可解释性和预测性能的影响。参数化方法是应用于标准化预测变量的普通最小二乘法（OLS）线性回归，非参数化方法是使用高斯核和固定带宽的 Nadaraya–Watson 核回归。您必须推导出一个关于输入特征缩放的可测试性能漂移度量，并将其实现为一个完整的程序。\n\n数据生成过程：\n- 设预测变量的随机向量为 $X = (X_{1}, X_{2})$，其中 $X_{1} \\sim \\mathrm{Uniform}[-2,2]$ 和 $X_{2} \\sim \\mathrm{Uniform}[-1,1]$ 相互独立。\n- 设噪声为 $\\varepsilon \\sim \\mathcal{N}(0,\\sigma^{2})$，独立于 $X$，其中 $\\sigma = 0.5$。\n- 响应定义为\n$$\nY \\;=\\; 3 X_{1} \\;-\\; 2 X_{2} \\;+\\; 0.3 \\sin(3 X_{1}) \\;+\\; \\varepsilon.\n$$\n\n抽样：\n- 使用固定种子 $7$ 的伪随机数生成器生成一个大小为 $n_{\\text{train}} = 200$ 的训练样本和一个大小为 $n_{\\text{test}} = 100$ 的测试样本，以确保结果是确定性的。\n\n缩放协议：\n- 对于一个缩放因子 $s > 0$，定义缩放后的预测变量为 $X^{(s)} = s \\cdot X$，该操作逐分量应用于训练和测试预测变量。\n\n参数化方法（标准化 OLS）：\n- 对于每个 $s$，使用训练集统计数据计算标准化预测变量 $Z^{(s)} = \\mathrm{standardize}(X^{(s)})$，其中标准化定义为 $Z^{(s)}_{j} = \\frac{X^{(s)}_{j} - \\mu^{(s)}_{j,\\text{train}}}{\\sigma^{(s)}_{j,\\text{train}}}$，适用于每个预测变量索引 $j \\in \\{1,2\\}$。$\\mu^{(s)}_{j,\\text{train}}$ 和 $\\sigma^{(s)}_{j,\\text{train}}$ 是在缩放后的训练集上计算的样本均值和样本标准差。拟合一个带截距的 OLS 模型，用 $Z^{(s)}$ 预测 $Y$。令 $\\widehat{\\beta}^{(s)} \\in \\mathbb{R}^{2}$ 表示标准化预测变量的斜率系数向量（不包括截距）。定义相对于基线 $s=1$ 的参数化系数漂移为\n$$\nd_{\\text{param}}(s) \\;=\\; \\big\\| \\widehat{\\beta}^{(s)} - \\widehat{\\beta}^{(1)} \\big\\|_{2}.\n$$\n\n非参数化方法（高斯核回归）：\n- 对于每个 $s$，在缩放后的训练集上拟合一个 Nadaraya–Watson 估计器，不进行任何标准化。使用各向同性的高斯核，固定带宽 $h = 0.5$。对于一个测试点 $x \\in \\mathbb{R}^{2}$，其预测值为\n$$\n\\widehat{m}^{(s)}(x) \\;=\\; \\frac{\\sum_{i=1}^{n_{\\text{train}}} \\exp\\!\\left(-\\frac{\\|x - x^{(s)}_{i}\\|_{2}^{2}}{2 h^{2}}\\right) \\, y_{i}}{\\sum_{i=1}^{n_{\\text{train}}} \\exp\\!\\left(-\\frac{\\|x - x^{(s)}_{i}\\|_{2}^{2}}{2 h^{2}}\\right)},\n$$\n其中 $x^{(s)}_{i}$ 是缩放后的训练预测变量，$y_{i}$ 是对应的训练响应。如果在任何测试点，分母小于数值阈值 $\\tau = 10^{-12}$，则将 $\\widehat{m}^{(s)}(x)$ 定义为训练响应的均值，以确保数值稳定性。在缩放后的测试集上，计算均方误差（MSE）\n$$\n\\mathrm{MSE}_{\\text{kernel}}(s) \\;=\\; \\frac{1}{n_{\\text{test}}} \\sum_{j=1}^{n_{\\text{test}}} \\Big( y^{\\text{test}}_{j} - \\widehat{m}^{(s)}(x^{(s),\\text{test}}_{j}) \\Big)^{2}.\n$$\n定义相对于基线 $s=1$ 的非参数化性能漂移为\n$$\nd_{\\text{kernel}}(s) \\;=\\; \\big| \\mathrm{MSE}_{\\text{kernel}}(s) - \\mathrm{MSE}_{\\text{kernel}}(1) \\big|.\n$$\n\n测试套件：\n- 使用缩放集 $S = \\{0.5, 1.0, 2.0, 10.0\\}$。\n- 对于每个 $s \\in S$，计算如上定义的 $d_{\\text{param}}(s)$ 和 $d_{\\text{kernel}}(s)$。\n\n要求的最终输出格式：\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，结果的顺序与 $S$ 的元素相对应：\n$$\n\\big[ d_{\\text{param}}(0.5), \\; d_{\\text{kernel}}(0.5), \\; d_{\\text{param}}(1.0), \\; d_{\\text{kernel}}(1.0), \\; d_{\\text{param}}(2.0), \\; d_{\\text{kernel}}(2.0), \\; d_{\\text{param}}(10.0), \\; d_{\\text{kernel}}(10.0) \\big].\n$$\n每个条目必须是一个实数（浮点数）。不应打印任何额外文本。", "solution": "该问题陈述被评估为具有科学依据、定义明确且客观。它为统计学习中一个明确定义的计算实验提供了一套完整且一致的定义和参数。任务是比较特征缩放对参数化模型（基于标准化数据的普通最小二乘法）和非参数化模型（Nadaraya–Watson 核回归）的影响。这将通过实现指定的模型并分别计算其参数和性能的漂移度量来完成。\n\n### 理论基础\n\n这个问题的核心在于参数化模型和非参数化模型处理输入特征方式的根本区别。\n\n1.  **参数化模型：通过标准化实现缩放不变性**\n\n    指定的参数化方法是应用于标准化预测变量的普通最小二乘法（OLS）回归。对于给定的预测特征 $X_j$ 和一个缩放因子 $s > 0$，缩放后的特征为 $X_j^{(s)} = s X_j$。标准化过程涉及使用在训练数据上计算的样本均值 $\\mu[ \\cdot ]$ 和样本标准差 $\\sigma[ \\cdot ]$ 对特征进行中心化和缩放。\n\n    设 $\\mu_{j,\\text{train}} = \\mu[X_{j,\\text{train}}]$ 和 $\\sigma_{j,\\text{train}} = \\sigma[X_{j,\\text{train}}]$。缩放后训练数据的均值和标准差为：\n    $$\n    \\mu^{(s)}_{j,\\text{train}} = \\mu[s X_{j,\\text{train}}] = s \\cdot \\mu[X_{j,\\text{train}}] = s \\mu_{j,\\text{train}}\n    $$\n    $$\n    \\sigma^{(s)}_{j,\\text{train}} = \\sigma[s X_{j,\\text{train}}] = s \\cdot \\sigma[X_{j,\\text{train}}] = s \\sigma_{j,\\text{train}}\n    $$\n    这些缩放性质是均值和标准差算子的基本特性。缩放后数据的标准化预测变量 $Z^{(s)}_j$ 则为：\n    $$\n    Z^{(s)}_j = \\frac{X^{(s)}_j - \\mu^{(s)}_{j,\\text{train}}}{\\sigma^{(s)}_{j,\\text{train}}} = \\frac{s X_j - s \\mu_{j,\\text{train}}}{s \\sigma_{j,\\text{train}}} = \\frac{s (X_j - \\mu_{j,\\text{train}})}{s \\sigma_{j,\\text{train}}} = \\frac{X_j - \\mu_{j,\\text{train}}}{\\sigma_{j,\\text{train}}} = Z^{(1)}_j\n    $$\n    这表明，对于任何缩放因子 $s > 0$，标准化预测变量 $Z^{(s)}_j$ 在数学上都是相同的。由于 OLS 模型是根据这些标准化预测变量的设计矩阵来预测固定的响应向量 $Y$，并且该设计矩阵对 $s$ 是不变的，因此得到的 OLS 斜率系数向量 $\\widehat{\\beta}^{(s)}$ 也必须是不变的。因此，理论上，对于所有 $s$，参数化系数漂移 $d_{\\text{param}}(s) = \\| \\widehat{\\beta}^{(s)} - \\widehat{\\beta}^{(1)} \\|_{2}$ 都应精确为 $0$。任何非零结果都可归因于浮点精度限制。\n\n2.  **非参数化模型：通过距离度量产生的缩放敏感性**\n\n    非参数化方法是 Nadaraya–Watson 核估计器。其对一个点 $x$ 的预测取决于训练响应 $y_i$ 的加权平均值，其中权重由 $x$ 与训练点 $x_i$ 之间距离的核函数确定。指定的各向同性高斯核使用欧几里得距离。\n\n    当特征按因子 $s$ 缩放时，一个测试点 $x$ 变为 $s x$，训练点 $x_i$ 变为 $s x_i$。核函数指数中的平方欧几里得距离项变为：\n    $$\n    \\| s x - s x_i \\|_2^2 = \\| s(x - x_i) \\|_2^2 = s^2 \\| x - x_i \\|_2^2\n    $$\n    因此，具有固定带宽 $h$ 的核函数会受到缩放的影响：\n    $$\n    K_h(s x, s x_i) = \\exp\\left(-\\frac{\\| s x - s x_i \\|_{2}^{2}}{2 h^{2}}\\right) = \\exp\\left(-\\frac{s^2 \\| x - x_i \\|_{2}^{2}}{2 h^{2}}\\right) = \\exp\\left(-\\frac{\\| x - x_i \\|_{2}^{2}}{2 (h/s)^{2}}\\right)\n    $$\n    这表明，将数据按因子 $s$ 缩放，等效于使用原始未缩放的数据和一个有效带宽 $h_{\\text{eff}} = h/s$。带宽参数 $h$ 关键地控制着估计器的偏差-方差权衡。较小的带宽会导致更灵活（低偏差，高方差）的拟合，而较大的带宽则会产生更平滑（高偏差，低方差）的拟合。由于缩放因子 $s$ 直接调节有效带宽，它改变了模型的复杂性及其拟合底层函数的能力。通常，这将改变模型在测试集上的预测性能，该性能由均方误差（MSE）衡量。因此，对于 $s \\neq 1$，非参数化性能漂移 $d_{\\text{kernel}}(s) = | \\mathrm{MSE}_{\\text{kernel}}(s) - \\mathrm{MSE}_{\\text{kernel}}(1) |$ 预计将不为零。\n\n### 计算步骤\n\n分析将按以下步骤进行：\n\n1.  **数据生成**：使用固定的随机种子 $7$，生成一个大小为 $n_{\\text{train}} = 200$ 的训练集和一个大小为 $n_{\\text{test}} = 100$ 的测试集。预测变量 $X_1$ 和 $X_2$ 从各自的均匀分布中抽取，响应 $Y$ 根据指定的数据生成过程计算得出，该过程包括正弦非线性和高斯噪声。\n\n2.  **遍历缩放因子进行迭代分析**：对于集合 $S = \\{0.5, 1.0, 2.0, 10.0\\}$ 中的每个缩放因子 $s$：\n    *   通过将训练和测试预测变量 $X_{\\text{train}}$ 和 $X_{\\text{test}}$ 按分量乘以 $s$ 来进行缩放，从而产生 $X^{(s)}_{\\text{train}}$ 和 $X^{(s)}_{\\text{test}}$。\n    *   **参数化分析**：\n        *   为 $X^{(s)}_{\\text{train}}$ 的每个特征计算样本均值和样本标准差（分母为 $n-1$，即 `ddof=1`）。\n        *   使用这些统计数据对 $X^{(s)}_{\\text{train}}$ 和 $X^{(s)}_{\\text{test}}$ 进行标准化。\n        *   拟合一个带截距的 OLS 模型，用标准化后的训练预测变量来预测 $Y_{\\text{train}}$。存储两个斜率系数，构成向量 $\\widehat{\\beta}^{(s)}$。\n    *   **非参数化分析**：\n        *   使用缩放后的训练数据 $(X^{(s)}_{\\text{train}}, Y_{\\text{train}})$ 和固定带宽 $h=0.5$ 来拟合 Nadaraya–Watson 估计器。\n        *   对缩放后的测试集 $X^{(s)}_{\\text{test}}$ 中的每个点进行预测。实施数值稳定性检查（将分母与 $\\tau=10^{-12}$ 进行比较）。\n        *   计算并存储预测值与真实测试响应 $Y_{\\text{test}}$ 之间的均方误差 $\\mathrm{MSE}_{\\text{kernel}}(s)$。\n\n3.  **漂移计算**：\n    *   在遍历 $S$ 中所有缩放因子后，检索对应于 $s=1.0$ 的基线结果，即 $\\widehat{\\beta}^{(1)}$ 和 $\\mathrm{MSE}_{\\text{kernel}}(1)$。\n    *   对于每个 $s \\in S$，计算漂移度量：\n        *   $d_{\\text{param}}(s) = \\| \\widehat{\\beta}^{(s)} - \\widehat{\\beta}^{(1)} \\|_{2}$。\n        *   $d_{\\text{kernel}}(s) = | \\mathrm{MSE}_{\\text{kernel}}(s) - \\mathrm{MSE}_{\\text{kernel}}(1) |$。\n\n4.  **最终输出**：将计算出的漂移值按指定顺序整理成一个列表并打印。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements a comparison between a parametric (standardized OLS) and a \n    non-parametric (Nadaraya-Watson) method to study the effect of \n    feature rescaling.\n    \"\"\"\n    \n    # Problem parameters\n    n_train = 200\n    n_test = 100\n    noise_sigma = 0.5\n    seed = 7\n    h = 0.5\n    tau = 1e-12\n    scaling_factors = [0.5, 1.0, 2.0, 10.0]\n\n    # Set seed for reproducibility\n    rng = np.random.default_rng(seed)\n\n    # --- Data Generation ---\n    # Training data\n    X1_train = rng.uniform(-2, 2, size=n_train)\n    X2_train = rng.uniform(-1, 1, size=n_train)\n    X_train = np.stack([X1_train, X2_train], axis=1)\n    eps_train = rng.normal(0, noise_sigma, size=n_train)\n    Y_train = 3 * X1_train - 2 * X2_train + 0.3 * np.sin(3 * X1_train) + eps_train\n\n    # Test data\n    X1_test = rng.uniform(-2, 2, size=n_test)\n    X2_test = rng.uniform(-1, 1, size=n_test)\n    X_test = np.stack([X1_test, X2_test], axis=1)\n    eps_test = rng.normal(0, noise_sigma, size=n_test)\n    Y_test = 3 * X1_test - 2 * X2_test + 0.3 * np.sin(3 * X1_test) + eps_test\n    \n    Y_train_mean = np.mean(Y_train)\n\n    # Storage for results\n    beta_coeffs = {}\n    kernel_mses = {}\n\n    for s in scaling_factors:\n        # --- Rescaling ---\n        X_train_s = s * X_train\n        X_test_s = s * X_test\n\n        # --- Parametric Method (Standardized OLS) ---\n        mu_s = np.mean(X_train_s, axis=0)\n        # Use ddof=1 for sample standard deviation as per statistical convention\n        sigma_s = np.std(X_train_s, axis=0, ddof=1)\n        \n        # Handle case where a feature has zero standard deviation\n        sigma_s[sigma_s == 0] = 1.0\n\n        Z_train_s = (X_train_s - mu_s) / sigma_s\n        Z_train_s_intercept = np.hstack([np.ones((n_train, 1)), Z_train_s])\n        \n        # Solve OLS using least squares\n        coeffs_s, _, _, _ = np.linalg.lstsq(Z_train_s_intercept, Y_train, rcond=None)\n        beta_coeffs[s] = coeffs_s[1:]  # Store only slope coefficients\n\n        # --- Non-Parametric Method (Nadaraya-Watson) ---\n        Y_pred_kernel = np.zeros(n_test)\n        \n        for j in range(n_test):\n            x_test_point = X_test_s[j]\n            \n            # Calculate squared Euclidean distances from the test point to all training points\n            dists_sq = np.sum((X_train_s - x_test_point)**2, axis=1)\n            \n            # Compute Gaussian kernel weights\n            weights = np.exp(-dists_sq / (2 * h**2))\n            \n            # Calculate numerator and denominator for the prediction\n            numerator = np.sum(weights * Y_train)\n            denominator = np.sum(weights)\n\n            if denominator  tau:\n                Y_pred_kernel[j] = Y_train_mean\n            else:\n                Y_pred_kernel[j] = numerator / denominator\n        \n        # Calculate MSE for the kernel regression\n        mse_s = np.mean((Y_test - Y_pred_kernel)**2)\n        kernel_mses[s] = mse_s\n\n    # --- Drift Calculation ---\n    results = []\n    beta_baseline = beta_coeffs[1.0]\n    mse_baseline = kernel_mses[1.0]\n\n    for s in scaling_factors:\n        # Parametric coefficient drift\n        d_param = np.linalg.norm(beta_coeffs[s] - beta_baseline)\n        \n        # Non-parametric performance drift\n        d_kernel = np.abs(kernel_mses[s] - mse_baseline)\n        \n        results.append(d_param)\n        results.append(d_kernel)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3155821"}, {"introduction": "当模型被要求在训练数据范围之外进行预测时，它们的表现会如何？本实践练习通过在一个地理空间数据集上比较参数化的线性趋势模型和非参数化的高斯过程（GP）模型，来解决外推这一关键问题。你将实现这两种方法来量化预测不确定性，从而揭示参数化模型的强结构性假设如何导致自信（但可能错误）的外推，而灵活的GP模型则能恰当地在未知区域显示增加的不确定性。[@problem_id:3155828]", "problem": "给定一个由传感器位置和标量读数组成的小型地理空间数据集。您将比较两个模型，以量化在没有传感器读数的指定位置的预测不确定性。这两个模型是：一个关于纬度和经度的参数化线性趋势模型，以及一个非参数高斯过程 (GP) 克里金模型。您的任务是，对于每个测试位置，计算在每个模型下潜在信号（不包括观测噪声）的后验方差，并将结果组合成一行输出，格式如下文所述。\n\n基本基础：\n- 参数化模型定义：一个有限维线性模型由一个假定的函数形式和一个有限的参数向量定义。考虑带有高斯噪声的普通最小二乘 (OLS) 线性模型，其中位于纬度 $lat$ 和经度 $lon$ 的位置的响应建模为 $y = \\beta_{0} + \\beta_{1} \\, lat + \\beta_{2} \\, lon + \\varepsilon$，其中 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2})$，参数为 $\\beta_{0}, \\beta_{1}, \\beta_{2}$。\n- 非参数模型定义：高斯过程 (GP) 在一个无限维函数空间中，对函数施加一个由均值函数和协方差核定义的分布。对于位置 $x = (lat, lon)$，先验为 $f \\sim \\mathcal{GP}(0, k(\\cdot, \\cdot))$，观测值遵循 $y = f(x) + \\eta$，其中 $\\eta \\sim \\mathcal{N}(0, \\tau^{2})$。\n\n可用作起点的经过充分检验的公式和事实：\n- 在带有高斯噪声的 OLS 线性模型中，参数估计量的抽样分布为 $ \\hat{\\boldsymbol{\\beta}} \\sim \\mathcal{N}(\\boldsymbol{\\beta}, \\sigma^{2} (X^{\\top} X)^{-1}) $，其中 $X$ 是设计矩阵，其行 $x_{i}^{\\top} = [1, lat_{i}, lon_{i}]$，噪声方差的无偏估计量为 $\\hat{\\sigma}^{2} = \\mathrm{RSS} / (n - p)$，其中 $\\mathrm{RSS}$ 是残差平方和，$n$ 是观测数量，$p$ 是参数数量。\n- 在带有观测噪声的高斯过程回归中，训练函数值和测试函数值的联合分布是多元高斯分布，通过条件化可以得到一个高斯后验，其方差仅取决于核函数和位置。\n\n您的程序必须使用以下数据集和模型超参数。所有角度都以度为单位。坐标是纬度和经度（度），核函数中的距离直接以度计算（不转换为弧度）。共有 $8$ 个传感器，其坐标和标量读数如下：\n- 传感器 1：$lat = -9$， $lon = -18$， $y = 9.7$。\n- 传感器 2：$lat = -6$， $lon = 12$， $y = 4.8$。\n- 传感器 3：$lat = 0$， $lon = 0$， $y = 9.7$。\n- 传感器 4：$lat = 4$， $lon = -10$， $y = 14.5$。\n- 传感器 5：$lat = 9$， $lon = 19$， $y = 10.3$。\n- 传感器 6：$lat = -3$， $lon = 5$， $y = 7.3$。\n- 传感器 7：$lat = 7$， $lon = -4$， $y = 14.4$。\n- 传感器 8：$lat = -10$， $lon = 15$， $y = 2.4$。\n\n参数化模型：\n- 使用线性趋势模型 $y = \\beta_{0} + \\beta_{1} \\, lat + \\beta_{2} \\, lon + \\varepsilon$，其中 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2})$。\n- 通过普通最小二乘 (OLS) 拟合模型，使用 $\\hat{\\sigma}^{2} = \\mathrm{RSS}/(n-p)$ 估计 $\\hat{\\sigma}^{2}$，其中 $n = 8$，$p = 3$。\n- 对于一个特征向量为 $x_{*}^{\\top} = [1, lat_{*}, lon_{*}]$ 的测试位置，计算参数化模型下潜在信号（均值函数）的后验方差为 $ \\mathrm{Var}_{\\text{param}}(x_{*}) = \\hat{\\sigma}^{2} \\, x_{*}^{\\top} (X^{\\top} X)^{-1} x_{*}$, 其中 $X$ 是训练传感器的 $n \\times p$ 设计矩阵。\n\n非参数 GP 克里金模型：\n- 使用零均值高斯过程 (GP) 先验和一个各向异性平方指数 (高斯) 核：\n$$\nk\\big((lat, lon), (lat', lon')\\big) = s^{2} \\exp\\left( -\\tfrac{1}{2} \\left[ \\left( \\frac{lat - lat'}{\\ell_{\\mathrm{lat}}} \\right)^{2} + \\left( \\frac{lon - lon'}{\\ell_{\\mathrm{lon}}} \\right)^{2} \\right] \\right).\n$$\n- 超参数固定为 $s^{2} = 9$，$\\ell_{\\mathrm{lat}} = 6$，$\\ell_{\\mathrm{lon}} = 10$，观测噪声方差 $\\tau^{2} = 0.25$。\n- 对于一个测试位置 $x_{*}$，潜在信号（不包括观测噪声）的后验方差为\n$$\n\\mathrm{Var}_{\\text{GP}}(x_{*}) = k(x_{*}, x_{*}) - \\boldsymbol{k}_{*}^{\\top} (K + \\tau^{2} I)^{-1} \\boldsymbol{k}_{*},\n$$\n其中 $K$ 是使用 $k$ 在训练传感器位置上计算的 $n \\times n$ 核矩阵，$\\boldsymbol{k}_{*}$ 是 $x_{*}$ 与每个训练传感器位置之间的 $n \\times 1$ 核向量。\n\n测试套件：\n为以下 $4$ 个测试位置（均以度为单位）计算 $\\mathrm{Var}_{\\text{param}}(x_{*})$ 和 $\\mathrm{Var}_{\\text{GP}}(x_{*})$：\n- 测试 1：$lat_{*} = 20$， $lon_{*} = 35$ （远在训练区域之外）。\n- 测试 2：$lat_{*} = 1$， $lon_{*} = 2$ （靠近训练区域中心）。\n- 测试 3：$lat_{*} = -10$， $lon_{*} = 15$ （恰好在一个训练传感器的位置）。\n- 测试 4：$lat_{*} = 12$， $lon_{*} = -18$ （在训练区域之外但靠近）。\n\n最终输出格式：\n您的程序应生成一行输出，其中包含 $8$ 个浮点数结果，以逗号分隔的列表形式包含在方括号中，顺序完全如下：\n$$\n[\\mathrm{Var}_{\\text{param}}(x_{1}), \\mathrm{Var}_{\\text{GP}}(x_{1}), \\mathrm{Var}_{\\text{param}}(x_{2}), \\mathrm{Var}_{\\text{GP}}(x_{2}), \\mathrm{Var}_{\\text{param}}(x_{3}), \\mathrm{Var}_{\\text{GP}}(x_{3}), \\mathrm{Var}_{\\text{param}}(x_{4}), \\mathrm{Var}_{\\text{GP}}(x_{4})].\n$$\n\n角度单位说明：\n所有角度均以度为单位。在核函数距离计算中一致使用度。无需转换为弧度。\n\n注意：问题要求的是每个模型下潜在信号的不确定性，而不是带噪声观测的方差。因此，不要将观测噪声方差加到预测方差中。", "solution": "我们从线性回归和高斯过程 (GP) 回归的核心定义出发，从基本原理比较参数化和非参数化的不确定性量化。我们将推导两种模型下新位置潜在信号的后验方差，然后实现相应的算法。\n\n参数化模型（有限维线性模型）：\n- 定义：参数化模型指定了一个有限维参数向量和一个固定的函数形式。对于具有纬度 $lat$ 和经度 $lon$ 的地理空间坐标，考虑线性模型\n$$\ny_{i} = \\beta_{0} + \\beta_{1} \\, lat_{i} + \\beta_{2} \\, lon_{i} + \\varepsilon_{i}, \\quad \\varepsilon_{i} \\sim \\mathcal{N}(0, \\sigma^{2}),\n$$\n其中 $i = 1, \\ldots, n$。\n- 设计矩阵：将预测变量堆叠成一个设计矩阵 $X \\in \\mathbb{R}^{n \\times p}$，其中 $p=3$，每一行为 $x_{i}^{\\top} = [1, lat_{i}, lon_{i}]$。\n- 估计：普通最小二乘 (OLS) 估计量最小化残差平方和 $\\mathrm{RSS} = \\sum_{i=1}^{n} (y_{i} - x_{i}^{\\top} \\hat{\\boldsymbol{\\beta}})^{2}$。在高斯噪声假设下，估计量的抽样分布为 $ \\hat{\\boldsymbol{\\beta}} \\sim \\mathcal{N}(\\boldsymbol{\\beta}, \\sigma^{2} (X^{\\top} X)^{-1})$，噪声方差的无偏估计量为 $ \\hat{\\sigma}^{2} = \\mathrm{RSS} / (n - p)$。\n- 潜在信号的预测均值和不确定性：一个特征向量为 $x_{*}^{\\top} = [1, lat_{*}, lon_{*}]$ 的新点的潜在信号（均值函数）是 $m_{*} = x_{*}^{\\top} \\boldsymbol{\\beta}$。估计量 $x_{*}^{\\top} \\hat{\\boldsymbol{\\beta}}$ 的方差为\n$$\n\\mathrm{Var}\\left( x_{*}^{\\top} \\hat{\\boldsymbol{\\beta}} \\right) = \\sigma^{2} \\, x_{*}^{\\top} (X^{\\top} X)^{-1} x_{*}.\n$$\n用 $\\hat{\\sigma}^{2}$ 替换 $\\sigma^{2}$，得到参数化模型下潜在信号的实用后验方差估计：\n$$\n\\mathrm{Var}_{\\text{param}}(x_{*}) = \\hat{\\sigma}^{2} \\, x_{*}^{\\top} (X^{\\top} X)^{-1} x_{*}.\n$$\n注意，这是均值函数估计的不确定性，不包括新观测的不可约减噪声 $\\sigma^{2}$。\n\n参数化模型的算法设计：\n- 将 1、$lat_{i}$ 和 $lon_{i}$ 连接起来，为所有传感器构建 $X$。\n- 求解正规方程得到 $\\hat{\\boldsymbol{\\beta}}$，使用 $ \\hat{\\boldsymbol{\\beta}} = (X^{\\top} X)^{-1} X^{\\top} \\boldsymbol{y}$。\n- 计算残差 $\\boldsymbol{r} = \\boldsymbol{y} - X \\hat{\\boldsymbol{\\beta}}$ 和 $\\hat{\\sigma}^{2} = \\boldsymbol{r}^{\\top} \\boldsymbol{r} / (n - p)$。\n- 对于每个测试位置，构建 $x_{*}$ 并计算 $ \\mathrm{Var}_{\\text{param}}(x_{*}) = \\hat{\\sigma}^{2} \\, x_{*}^{\\top} (X^{\\top} X)^{-1} x_{*}$。在数值上，通过求解 $(X^{\\top} X) \\boldsymbol{w} = x_{*}$ 并计算 $x_{*}^{\\top} \\boldsymbol{w}$ 来避免显式矩阵求逆。\n\n非参数模型（高斯过程克里金）：\n- 定义：高斯过程 (GP) 在函数 $f$ 上施加一个分布，其中 $f \\sim \\mathcal{GP}(0, k(\\cdot, \\cdot))$。观测值遵循 $y_{i} = f(x_{i}) + \\eta_{i}$，其中 $\\eta_{i} \\sim \\mathcal{N}(0, \\tau^{2})$。\n- 核函数：使用以度为单位的各向异性平方指数核，其振幅为 $s^{2}$，长度尺度为 $\\ell_{\\mathrm{lat}}$ 和 $\\ell_{\\mathrm{lon}}$：\n$$\nk(x, x') = s^{2} \\exp\\left( -\\tfrac{1}{2} \\left[ \\left( \\frac{lat - lat'}{\\ell_{\\mathrm{lat}}} \\right)^{2} + \\left( \\frac{lon - lon'}{\\ell_{\\mathrm{lon}}} \\right)^{2} \\right] \\right).\n$$\n- 多元高斯分布的条件化法则：在训练输入处的 $\\boldsymbol{f}$ 和在测试输入处的 $f_{*}$ 的联合先验是高斯分布\n$$\n\\begin{bmatrix}\n\\boldsymbol{f} \\\\\nf_{*}\n\\end{bmatrix}\n\\sim\n\\mathcal{N}\\left(\n\\begin{bmatrix}\n\\boldsymbol{0} \\\\\n0\n\\end{bmatrix},\n\\begin{bmatrix}\nK  \\boldsymbol{k}_{*} \\\\\n\\boldsymbol{k}_{*}^{\\top}  k(x_{*}, x_{*})\n\\end{bmatrix}\n\\right),\n$$\n对于观测值 $\\boldsymbol{y} = \\boldsymbol{f} + \\boldsymbol{\\eta}$，给定 $\\boldsymbol{y}$ 时 $f_{*}$ 的后验方差为\n$$\n\\mathrm{Var}_{\\text{GP}}(x_{*}) = k(x_{*}, x_{*}) - \\boldsymbol{k}_{*}^{\\top} (K + \\tau^{2} I)^{-1} \\boldsymbol{k}_{*}.\n$$\n关键是，后验方差仅取决于核函数、传感器位置（以及 $\\tau^{2}$），而不取决于观测值 $\\boldsymbol{y}$。\n- 数值稳定性：通过 Cholesky 分解 $K_{\\tau} = K + \\tau^{2} I = L L^{\\top}$ 来计算 $(K + \\tau^{2} I)^{-1} \\boldsymbol{k}_{*}$，求解 $L \\boldsymbol{v} = \\boldsymbol{k}_{*}$，并计算 $ \\mathrm{Var}_{\\text{GP}}(x_{*}) = k(x_{*}, x_{*}) - \\boldsymbol{v}^{\\top} \\boldsymbol{v}$。如果需要，向对角线添加一个小的扰动项 $\\epsilon$ 以确保正定性。\n\nGP 模型的算法设计：\n- 使用 $k$ 从训练坐标计算 $n \\times n$ 核矩阵 $K$。\n- 构造 $K_{\\tau} = K + \\tau^{2} I + \\epsilon I$，其中 $\\epsilon$ 是一个小数。\n- 对于每个测试位置，计算 $\\boldsymbol{k}_{*}$ 和 $k(x_{*}, x_{*}) = s^{2}$，执行基于 Cholesky 的求解得到 $\\boldsymbol{v}$，并计算 $ \\mathrm{Var}_{\\text{GP}}(x_{*}) = s^{2} - \\boldsymbol{v}^{\\top} \\boldsymbol{v}$。\n\n测试覆盖理由：\n- 测试 1 远在训练区域之外，以检验外推行为。参数化不确定性根据 $x_{*}^{\\top} (X^{\\top} X)^{-1} x_{*}$ 增长，而当 $x_{*}$ 远离传感器时，GP 方差接近先验方差 $s^{2}$。\n- 测试 2 靠近中心，检验两种方法都有更多信息时的内插情况。\n- 测试 3 与一个训练传感器位置重合，测试在存在噪声方差 $\\tau^{2}$ 的情况下，GP 在训练输入处的方差减小情况，以及参数化模型在该输入处非零的均值函数不确定性。\n- 测试 4 在训练区域之外但很近，用于探测边界行为。\n\n实现细节：\n- 所有角度量都以度为单位；核函数距离使用按长度尺度缩放的度数差。\n- 最终输出是按指定顺序排列的包含 $8$ 个浮点数的单个列表。方差不需要物理单位，角度单位固定为度。\n\n基于这些原理和算法，程序为每个测试位置计算 $\\mathrm{Var}_{\\text{param}}(x_{*})$ 和 $\\mathrm{Var}_{\\text{GP}}(x_{*})$，并以所需的单行格式打印它们。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef build_design_matrix(lats, lons):\n    \"\"\"\n    Construct the design matrix X for the linear model y = b0 + b1*lat + b2*lon + noise.\n    \"\"\"\n    n = len(lats)\n    X = np.column_stack([np.ones(n), np.array(lats, dtype=float), np.array(lons, dtype=float)])\n    return X\n\ndef fit_ols_params(X, y):\n    \"\"\"\n    Fit OLS parameters and estimate sigma^2 using unbiased estimator RSS/(n-p).\n    Returns beta_hat and sigma2_hat.\n    \"\"\"\n    # Solve normal equations without explicit inversion for numerical stability\n    XtX = X.T @ X\n    Xty = X.T @ y\n    beta_hat = np.linalg.solve(XtX, Xty)\n    residuals = y - X @ beta_hat\n    n, p = X.shape\n    rss = float(residuals.T @ residuals)\n    sigma2_hat = rss / (n - p)\n    return beta_hat, sigma2_hat, XtX\n\ndef parametric_variance_at(XtX, sigma2_hat, lat_star, lon_star):\n    \"\"\"\n    Compute Var_param(x*) = sigma2_hat * x_*^T (XtX)^-1 x_*\n    without explicitly inverting XtX by solving (XtX) w = x_*.\n    \"\"\"\n    x_star = np.array([1.0, float(lat_star), float(lon_star)])\n    # Solve (XtX) w = x_star\n    w = np.linalg.solve(XtX, x_star)\n    var_param = sigma2_hat * float(x_star.T @ w)\n    return var_param\n\ndef se_kernel(lat1, lon1, lat2, lon2, s2, ell_lat, ell_lon):\n    \"\"\"\n    Anisotropic squared exponential kernel in degrees.\n    \"\"\"\n    dlat = (lat1 - lat2) / ell_lat\n    dlon = (lon1 - lon2) / ell_lon\n    return s2 * np.exp(-0.5 * (dlat**2 + dlon**2))\n\ndef build_kernel_matrix(coords, s2, ell_lat, ell_lon):\n    \"\"\"\n    Build the kernel matrix K for training coordinates.\n    coords: list of (lat, lon)\n    \"\"\"\n    n = len(coords)\n    K = np.empty((n, n), dtype=float)\n    for i in range(n):\n        lat_i, lon_i = coords[i]\n        for j in range(n):\n            lat_j, lon_j = coords[j]\n            K[i, j] = se_kernel(lat_i, lon_i, lat_j, lon_j, s2, ell_lat, ell_lon)\n    return K\n\ndef gp_posterior_variance(K, coords, lat_star, lon_star, s2, ell_lat, ell_lon, tau2, jitter=1e-9):\n    \"\"\"\n    Compute GP posterior variance of the latent signal at x*:\n    Var_GP(x*) = k(x*,x*) - k_*^T (K + tau2*I)^-1 k_*\n    Uses Cholesky factorization for numerical stability.\n    \"\"\"\n    n = len(coords)\n    # Compute k_* vector\n    k_star = np.empty(n, dtype=float)\n    for i in range(n):\n        lat_i, lon_i = coords[i]\n        k_star[i] = se_kernel(lat_star, lon_star, lat_i, lon_i, s2, ell_lat, ell_lon)\n    # k(x*, x*) for SE kernel is s2\n    k_xx = s2\n    # Add noise variance and jitter to K\n    K_tilde = K + (tau2 + jitter) * np.eye(n)\n    # Cholesky factorization\n    try:\n        L = np.linalg.cholesky(K_tilde)\n    except np.linalg.LinAlgError:\n        # Increase jitter if needed\n        K_tilde = K + (tau2 + 1e-6) * np.eye(n)\n        L = np.linalg.cholesky(K_tilde)\n    # Solve L v = k_star\n    v = np.linalg.solve(L, k_star)\n    var_gp = float(k_xx - v.T @ v)\n    # Numerical safety: variance should be non-negative\n    if var_gp  0 and var_gp > -1e-10:\n        var_gp = 0.0\n    return var_gp\n\ndef solve():\n    # Training sensors (lat, lon, y)\n    sensors = [\n        (-9.0, -18.0, 9.7),\n        (-6.0,  12.0, 4.8),\n        ( 0.0,   0.0, 9.7),\n        ( 4.0, -10.0,14.5),\n        ( 9.0,  19.0,10.3),\n        (-3.0,   5.0, 7.3),\n        ( 7.0,  -4.0,14.4),\n        (-10.0, 15.0, 2.4),\n    ]\n    lats = [s[0] for s in sensors]\n    lons = [s[1] for s in sensors]\n    ys   = [s[2] for s in sensors]\n    y = np.array(ys, dtype=float)\n\n    # Design matrix and OLS fit\n    X = build_design_matrix(lats, lons)\n    beta_hat, sigma2_hat, XtX = fit_ols_params(X, y)\n\n    # GP hyperparameters\n    s2 = 9.0\n    ell_lat = 6.0  # degrees\n    ell_lon = 10.0 # degrees\n    tau2 = 0.25\n\n    # Build kernel matrix for training coords\n    coords = [(lat, lon) for lat, lon, _ in sensors]\n    K = build_kernel_matrix(coords, s2, ell_lat, ell_lon)\n\n    # Test cases: list of (lat*, lon*)\n    test_cases = [\n        (20.0,  35.0),   # Test 1: far outside\n        ( 1.0,   2.0),   # Test 2: near center\n        (-10.0, 15.0),   # Test 3: exactly a training sensor\n        (12.0, -18.0),   # Test 4: outside near region\n    ]\n\n    results = []\n    for lat_star, lon_star in test_cases:\n        var_param = parametric_variance_at(XtX, sigma2_hat, lat_star, lon_star)\n        var_gp = gp_posterior_variance(K, coords, lat_star, lon_star, s2, ell_lat, ell_lon, tau2)\n        results.append(var_param)\n        results.append(var_gp)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3155828"}, {"introduction": "单个模型很少是完美的。这项高级实践深入探讨了集成技术，比较了一种参数化方法（通过AIC权重进行模型平均）和一种非参数化方法（装袋法，bagging）。通过蒙特卡洛模拟，你将研究如何通过管理偏差-方差权衡来结合多个模型以改善预测。本练习展示了两种范式如何解决模型选择的不确定性，并突显了聚合在统计学习中的力量。[@problem_id:3155854]", "problem": "编写一个完整的程序，使用蒙特卡洛模拟，比较一个基于赤池信息准则 (Akaike Information Criterion, AIC) 权重的参数模型平均预测器与一个基于自助聚合 (bootstrap-aggregated) $k$-近邻的非参数 bagged 预测器，并在一个固定的预测点量化偏差和方差。设定为带有加性高斯噪声的单变量回归。假设以下核心定义为基本基础：(i) 线性回归的高斯似然，(ii) 回归参数和噪声方差的最大似然估计，(iii) 用于模型比较的赤池信息准则 (AIC)，(iv) 作为对从数据生成分布中抽样的经验近似的自助法重抽样，(v) $k$-近邻回归规则，以及 (vi) 估计量的偏差和方差的定义。仅使用这些基本元素来推导您的程序。\n\n您的程序应基于第一性原理实现以下内容。\n\n1) 数据生成过程和预测目标。对于每个独立重复实验 $r \\in \\{1,\\dots,R\\}$，为 $i \\in \\{1,\\dots,n\\}$ 独立抽取 $x_{i}^{(r)} \\sim \\mathrm{Uniform}[0,1]$，定义一个根据测试用例指定的确定性回归函数 $f(\\cdot)$，生成响应\n$$\ny_{i}^{(r)} = f\\!\\left(x_{i}^{(r)}\\right) + \\varepsilon_{i}^{(r)},\n$$\n其中 $\\varepsilon_{i}^{(r)} \\sim \\mathcal{N}(0,\\sigma^{2})$ 在 $i$ 和 $r$ 之间独立，然后在固定点 $x_{0}$ 计算预测器。\n\n2) 参数模型集与 AIC 权重平均。考虑由带截距项的 $d \\in \\{1,2\\}$ 次多项式回归给出的候选参数模型。对于一个固定的 $d$，均值函数为\n$$\nm_{d}(x;\\boldsymbol{\\beta}) = \\sum_{j=0}^{d} \\beta_{j}\\, x^{j},\n$$\n参数为 $\\boldsymbol{\\beta} \\in \\mathbb{R}^{d+1}$，误差为方差是 $\\sigma^{2}$ 的高斯误差。对于每次重复实验，通过最大似然（在假定高斯噪声下等价于普通最小二乘法）拟合每个模型，以获得 $\\widehat{\\boldsymbol{\\beta}}_{d}$ 和残差平方和。令 $\\widehat{\\sigma}^{2}_{d}$ 表示 $\\sigma^{2}$ 的最大似然估计，令 $k_{d}$ 为模型中估计参数的数量，其包括回归系数和噪声方差，因此 $k_{d} = (d+1) + 1 = d + 2$。对于使用最大似然估计的高斯线性回归，在拟合参数下的对数似然为\n$$\n\\log L_{d} = -\\frac{n}{2}\\Big(\\log(2\\pi) + \\log(\\widehat{\\sigma}^{2}_{d}) + 1\\Big).\n$$\n赤池信息准则为\n$$\n\\mathrm{AIC}_{d} = 2\\,k_{d} - 2\\,\\log L_{d}.\n$$\n计算 AIC 差值 $\\Delta_{d} = \\mathrm{AIC}_{d} - \\min_{d' \\in \\{1,2\\}} \\mathrm{AIC}_{d'}$ 和 AIC 权重\n$$\nw_{d} = \\frac{\\exp\\!\\left(-\\tfrac{1}{2}\\Delta_{d}\\right)}{\\sum_{d' \\in \\{1,2\\}} \\exp\\!\\left(-\\tfrac{1}{2}\\Delta_{d'}\\right)}.\n$$\n对于每次重复实验，在 $x_{0}$ 处构建参数模型平均预测器，\n$$\n\\widehat{f}_{\\mathrm{P}}^{(r)}(x_{0}) = \\sum_{d \\in \\{1,2\\}} w_{d}\\, m_{d}\\!\\left(x_{0};\\widehat{\\boldsymbol{\\beta}}_{d}\\right).\n$$\n\n3) 非参数 $k$-近邻与 bagging。对于 $k \\in \\mathbb{N}$，在 $x_{0}$ 处的非参数 $k$-近邻预测器使用其协变量在欧几里得距离上最接近 $x_{0}$ 的 $k$ 个响应值的平均值。对于每次重复实验，计算：\n- 使用所有 $n$ 个训练观测值的单样本 $k$-近邻预测器 $\\widehat{f}_{\\mathrm{N,single}}^{(r)}(x_{0})$。\n- bagged $k$-近邻预测器 $\\widehat{f}_{\\mathrm{N,bag}}^{(r)}(x_{0})$，通过从训练数据中有放回地抽取 $B$ 个自助重抽样样本（每个样本大小为 $n$），在每个重抽样样本 $b \\in \\{1,\\dots,B\\}$ 上计算一个 $k$-近邻预测 $\\widehat{f}_{b}(x_{0})$，并求平均，\n$$\n\\widehat{f}_{\\mathrm{N,bag}}^{(r)}(x_{0}) = \\frac{1}{B}\\sum_{b=1}^{B} \\widehat{f}_{b}(x_{0}).\n$$\n\n4) 在 $x_{0}$ 处的偏差和方差量化。设真实目标为 $f(x_{0})$。在 $R$ 次重复实验中，对于每种方法 $M \\in \\{\\mathrm{P}, \\mathrm{N,single}, \\mathrm{N,bag}\\}$，估计\n- 偏差：\n$$\n\\widehat{\\mathrm{Bias}}_{M} = \\frac{1}{R}\\sum_{r=1}^{R}\\widehat{f}_{M}^{(r)}(x_{0}) - f(x_{0}),\n$$\n- 和方差：\n$$\n\\widehat{\\mathrm{Var}}_{M} = \\frac{1}{R-1}\\sum_{r=1}^{R}\\left(\\widehat{f}_{M}^{(r)}(x_{0}) - \\frac{1}{R}\\sum_{r'=1}^{R}\\widehat{f}_{M}^{(r')}(x_{0})\\right)^{2}.\n$$\n此外，通过比率量化 bagging 为非参数方法带来的方差缩减\n$$\n\\rho = \\frac{\\widehat{\\mathrm{Var}}_{\\mathrm{N,bag}}}{\\widehat{\\mathrm{Var}}_{\\mathrm{N,single}}}.\n$$\n\n5) 测试套件。您的程序必须运行以下测试用例，每个用例由元组 $(\\text{seed}, n, \\sigma, x_{0}, k, B, R, f)$ 定义，其中 $f$ 指定确定性回归函数：\n- 用例 1：$(\\;12345,\\;50,\\;1.0,\\;0.5,\\;7,\\;200,\\;400,\\;f(x)=2+3x\\;)$。\n- 用例 2：$(\\;54321,\\;60,\\;0.5,\\;0.3,\\;9,\\;200,\\;400,\\;f(x)=\\sin(2\\pi x)\\;)$。\n- 用例 3：$(\\;20231105,\\;20,\\;2.0,\\;0.2,\\;5,\\;300,\\;300,\\;f(x)=x^{3}-x\\;)$。\n\n6) 输出。对于每个用例，按以下顺序生成一个包含 7 个浮点数的列表：\n$[\\widehat{\\mathrm{Bias}}_{\\mathrm{P}},\\;\\widehat{\\mathrm{Var}}_{\\mathrm{P}},\\;\\widehat{\\mathrm{Bias}}_{\\mathrm{N,single}},\\;\\widehat{\\mathrm{Var}}_{\\mathrm{N,single}},\\;\\widehat{\\mathrm{Bias}}_{\\mathrm{N,bag}},\\;\\widehat{\\mathrm{Var}}_{\\mathrm{N,bag}},\\;\\rho]$。\n您的程序应生成单行输出，其中包含每个用例结果的列表，这些列表以逗号分隔并包含在方括号内（例如，$[\\,[\\cdot],\\,[\\cdot],\\,[\\cdot]\\,]$）。报告每个浮点数，四舍五入到 $6$ 位小数。不应打印任何其他文本。\n\n所有量都是纯数学量；不涉及物理单位。当角度出现在三角函数内部时，单位是弧度。所有比率和平均值都以小数形式表示，而不是百分比。", "solution": "用户提供的问题是一个在统计学习领域中明确定义的计算任务，因此是有效的。它基于已建立的统计理论，具有科学依据，自包含所有必要的参数和定义，并且是客观的。该任务是编写一个程序，执行蒙特卡洛模拟，以比较一个参数模型平均预测器与一个非参数 bagged 预测器。比较基于在固定点估计每个预测器的偏差和方差。\n\n该解决方案通过遵循问题陈述中概述的步骤来实现。总体结构是一个主模拟循环，该循环迭代 $R$ 次重复实验，以近似预测器的抽样分布。\n\n**1. 模拟框架**\n程序的核心是一个运行 $R$ 次重复实验的循环。对于每次重复实验 $r \\in \\{1,\\dots,R\\}$，都会根据指定的数据生成过程生成一个新的数据集。在此数据集上，我们训练和评估三种不同的预测器：\n1.  一个参数模型平均预测器，$\\widehat{f}_{\\mathrm{P}}^{(r)}(x_0)$。\n2.  一个标准的非参数 $k$-近邻预测器，$\\widehat{f}_{\\mathrm{N,single}}^{(r)}(x_0)$。\n3.  一个 bagged (自助聚合) 的非参数 $k$-近邻预测器，$\\widehat{f}_{\\mathrm{N,bag}}^{(r)}(x_0)$。\n\n每种方法在目标点 $x_0$ 的预测值都会被存储下来。在所有 $R$ 次重复实验完成后，这些存储的预测值将用于估计每个预测器的偏差和方差。\n\n**2. 数据生成**\n对于每次重复实验 $r$，都会创建一个大小为 $n$ 的训练数据集 $\\{ (x_i^{(r)}, y_i^{(r)}) \\}_{i=1}^n$。过程如下：\n-   协变量 $x_i^{(r)}$ 从均匀分布中独立抽取，$x_{i}^{(r)} \\sim \\mathrm{Uniform}[0,1]$。\n-   响应 $y_i^{(r)}$ 通过模型 $y_{i}^{(r)} = f(x_{i}^{(r)}) + \\varepsilon_{i}^{(r)}$ 生成，其中 $f(\\cdot)$ 是为测试用例指定的真实确定性回归函数，而 $\\varepsilon_{i}^{(r)}$ 是从高斯分布中抽取的独立噪声项，$\\varepsilon_{i}^{(r)} \\sim \\mathcal{N}(0, \\sigma^2)$。\n\n**3. 参数模型平均预测器 ($\\widehat{f}_{\\mathrm{P}}$)**\n该预测器是通过对一组候选多项式模型的预测进行平均来构建的，权重由它们的赤池信息准则 (AIC) 分数决定。候选模型是 $d \\in \\{1, 2\\}$ 次的多项式回归，其均值函数为 $m_{d}(x;\\boldsymbol{\\beta}) = \\sum_{j=0}^{d} \\beta_{j}\\, x^{j}$。\n\n对于每次重复实验和每个次数 $d$：\n-   构建大小为 $n \\times (d+1)$ 的设计矩阵 $\\mathbf{X}_d$，其中第 $i$ 行为 $[1, x_i, \\dots, x_i^d]$。\n-   通过解决普通最小二乘问题来估计回归系数 $\\widehat{\\boldsymbol{\\beta}}_d$，这在假定高斯噪声下等价于最大似然估计。这是通过找到最小化残差平方和 (RSS) $\\|\\boldsymbol{y} - \\mathbf{X}_d \\boldsymbol{\\beta}\\|_2^2$ 的 $\\widehat{\\boldsymbol{\\beta}}_d$ 来实现的。\n-   计算误差方差的最大似然估计：$\\widehat{\\sigma}^2_d = \\frac{1}{n} \\mathrm{RSS}_d = \\frac{1}{n} \\sum_{i=1}^n (y_i - m_d(x_i; \\widehat{\\boldsymbol{\\beta}}_d))^2$。\n-   确定模型中估计参数的数量，$k_d = (d+1) + 1 = d+2$，其中包括 $d+1$ 个回归系数和方差参数 $\\sigma^2$。\n-   计算最大化后的对数似然：$\\log L_d = -\\frac{n}{2}\\big(\\log(2\\pi) + \\log(\\widehat{\\sigma}^2_d) + 1\\big)$。\n-   计算模型的 AIC：$\\mathrm{AIC}_d = 2k_d - 2\\log L_d$。\n\n在计算 $\\mathrm{AIC}_1$ 和 $\\mathrm{AIC}_2$ 之后：\n-   确定最小 AIC 值，$\\mathrm{AIC}_{\\min} = \\min(\\mathrm{AIC}_1, \\mathrm{AIC}_2)$。\n-   计算 AIC 差值，$\\Delta_d = \\mathrm{AIC}_d - \\mathrm{AIC}_{\\min}$。\n-   计算每个模型的赤池权重：$w_d = \\frac{\\exp(-\\frac{1}{2}\\Delta_d)}{\\sum_{d' \\in \\{1,2\\}} \\exp(-\\frac{1}{2}\\Delta_{d'})}$。这些权重代表每个模型成为真实数据生成过程的最佳近似的相对可能性。\n-   该次重复实验在点 $x_0$ 的最终模型平均预测是各个模型预测的加权平均：$\\widehat{f}_{\\mathrm{P}}^{(r)}(x_0) = \\sum_{d \\in \\{1,2\\}} w_d \\, m_d(x_0; \\widehat{\\boldsymbol{\\beta}}_d)$。\n\n**4. 非参数预测器 ($\\widehat{f}_{\\mathrm{N,single}}$ 和 $\\widehat{f}_{\\mathrm{N,bag}}$)**\n这些预测器基于 $k$-近邻 (kNN) 算法。\n\n-   **单样本 kNN 预测器 ($\\widehat{f}_{\\mathrm{N,single}}$)**：对于给定的重复实验，此预测器直接在训练数据 $\\{(x_i^{(r)}, y_i^{(r)})\\}_{i=1}^n$ 上计算。我们确定 $k$ 个训练点的集合 $\\mathcal{N}_k(x_0)$，这些点的协变量 $x_i$ 最接近 $x_0$。预测是相应响应值的平均值：$\\widehat{f}_{\\mathrm{N,single}}^{(r)}(x_0) = \\frac{1}{k} \\sum_{i \\in \\mathcal{N}_k(x_0)} y_i^{(r)}$。\n\n-   **Bagged kNN 预测器 ($\\widehat{f}_{\\mathrm{N,bag}}$)**：Bagging (自助聚合) 是一种方差缩减技术。对于每次重复实验，应用以下过程：\n    -   从原始训练数据中生成 $B$ 个自助重抽样样本。每个重抽样样本都是通过有放回地抽取 $n$ 个数据点创建的。\n    -   对于每个自助重抽样样本 $b \\in \\{1, \\dots, B\\}$，训练一个 kNN 预测器并计算其在 $x_0$ 处的预测，记为 $\\widehat{f}_b(x_0)$。\n    -   bagged 预测是这些单个预测的平均值：$\\widehat{f}_{\\mathrm{N,bag}}^{(r)}(x_0) = \\frac{1}{B} \\sum_{b=1}^{B} \\widehat{f}_b(x_0)$。通过对在数据的轻微扰动版本上训练的许多模型进行平均，bagging 通常会降低像 kNN 这样的不稳定预测器的方差。\n\n**5. 偏差和方差估计**\n完成 $R$ 次重复实验后，我们对三种方法中的每一种都有 $R$ 个预测，形成了经验抽样分布。设 $\\{\\widehat{f}_M^{(r)}(x_0)\\}_{r=1}^R$ 为方法 $M \\in \\{\\mathrm{P}, \\mathrm{N,single}, \\mathrm{N,bag}\\}$ 的预测集合。\n\n-   方法 $M$ 的平均预测为 $\\overline{\\widehat{f}_M(x_0)} = \\frac{1}{R} \\sum_{r=1}^R \\widehat{f}_M^{(r)}(x_0)$。\n-   **偏差**被估计为平均预测与真实值 $f(x_0)$ 之间的差异：\n    $$ \\widehat{\\mathrm{Bias}}_{M} = \\overline{\\widehat{f}_M(x_0)} - f(x_0) $$\n-   **方差**使用预测的样本方差进行估计，并进行贝塞尔校正：\n    $$ \\widehat{\\mathrm{Var}}_{M} = \\frac{1}{R-1} \\sum_{r=1}^R \\left(\\widehat{f}_{M}^{(r)}(x_0) - \\overline{\\widehat{f}_M(x_0)}\\right)^2 $$\n-   最后，由 bagging 带来的**方差缩减比率**通过 $\\rho = \\frac{\\widehat{\\mathrm{Var}}_{\\mathrm{N,bag}}}{\\widehat{\\mathrm{Var}}_{\\mathrm{N,single}}}$ 来量化。$\\rho  1$ 的值表示 bagging 成功地减少了 kNN 预测器的方差。\n\n该程序实现了这整个模拟过程，遍历指定的测试用例，并按要求格式化每个用例的七个性能指标。", "answer": "```python\nimport numpy as np\n\ndef run_simulation(seed, n, sigma, x0, k, B, R, f_func):\n    \"\"\"\n    Runs a single Monte Carlo simulation for a given test case.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    \n    # Storage for predictions from each replicate\n    preds_p = np.zeros(R)\n    preds_ns = np.zeros(R)\n    preds_nb = np.zeros(R)\n\n    for r in range(R):\n        # 1. Data-generating process\n        x_train = rng.uniform(0, 1, n)\n        epsilon = rng.normal(0, sigma, n)\n        y_train = f_func(x_train) + epsilon\n\n        # 2. Parametric model-averaged predictor\n        aics = []\n        betas = []\n        degrees = [1, 2]\n        \n        for d in degrees:\n            # Fit polynomial regression of degree d\n            X_mat = np.vander(x_train, d + 1, increasing=True)\n            beta_hat, res, _, _ = np.linalg.lstsq(X_mat, y_train, rcond=None)\n            \n            # Note: np.linalg.lstsq returns the sum of squared residuals, not RSS.\n            # If n > d+1, res is RSS. If n = d+1, res is an empty array.\n            # We assume n > max(d)+1, which holds for test cases.\n            rss = res[0] if len(res) > 0 else 0.0\n            sigma2_hat_ml = rss / n\n            \n            # The MLE for sigma^2 can be 0 if RSS is 0 (perfect fit).\n            # This is unlikely with noise but must be handled for numerical stability.\n            if sigma2_hat_ml = 0:\n                # Assign a huge AIC to penalize this model\n                aic = 1e9\n            else:\n                logL = -n/2 * (np.log(2 * np.pi) + np.log(sigma2_hat_ml) + 1)\n                k_d = d + 2  # (d+1) coefficients + 1 variance parameter\n                aic = 2 * k_d - 2 * logL\n            \n            aics.append(aic)\n            betas.append(beta_hat)\n\n        aics = np.array(aics)\n        delta_aics = aics - np.min(aics)\n        exp_term = np.exp(-0.5 * delta_aics)\n        weights = exp_term / np.sum(exp_term)\n\n        # Calculate model-averaged prediction at x0\n        f_hat_p = 0\n        for i, d in enumerate(degrees):\n            x0_poly_vec = np.vander(np.array([x0]), d + 1, increasing=True)[0]\n            pred_d = x0_poly_vec @ betas[i]\n            f_hat_p += weights[i] * pred_d\n        \n        preds_p[r] = f_hat_p\n\n        # 3. Non-parametric k-nearest neighbors predictors\n        \n        # Helper function for kNN prediction\n        def knn_predict(x_target, x_data, y_data, k_neighbors):\n            distances = np.abs(x_data - x_target)\n            # Find indices of the k nearest neighbors\n            neighbor_indices = np.argsort(distances)[:k_neighbors]\n            return np.mean(y_data[neighbor_indices])\n\n        # Single-sample kNN predictor\n        preds_ns[r] = knn_predict(x0, x_train, y_train, k)\n\n        # Bagged kNN predictor\n        bootstrap_preds = np.zeros(B)\n        for b in range(B):\n            # Create a bootstrap resample\n            bootstrap_indices = rng.choice(n, size=n, replace=True)\n            x_boot = x_train[bootstrap_indices]\n            y_boot = y_train[bootstrap_indices]\n            bootstrap_preds[b] = knn_predict(x0, x_boot, y_boot, k)\n        \n        preds_nb[r] = np.mean(bootstrap_preds)\n\n    # 4. Bias and variance quantification\n    f_true_x0 = f_func(x0)\n    \n    # Parametric method\n    mean_p = np.mean(preds_p)\n    bias_p = mean_p - f_true_x0\n    var_p = np.var(preds_p, ddof=1)\n    \n    # Non-parametric single-sample method\n    mean_ns = np.mean(preds_ns)\n    bias_ns = mean_ns - f_true_x0\n    var_ns = np.var(preds_ns, ddof=1)\n\n    # Non-parametric bagged method\n    mean_nb = np.mean(preds_nb)\n    bias_nb = mean_nb - f_true_x0\n    var_nb = np.var(preds_nb, ddof=1)\n\n    # Variance reduction ratio\n    rho = var_nb / var_ns if var_ns > 0 else 0\n\n    return [bias_p, var_p, bias_ns, var_ns, bias_nb, var_nb, rho]\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print results.\n    \"\"\"\n    # 5. Test suite\n    test_cases = [\n        (12345, 50, 1.0, 0.5, 7, 200, 400, lambda x: 2 + 3 * x),\n        (54321, 60, 0.5, 0.3, 9, 200, 400, lambda x: np.sin(2 * np.pi * x)),\n        (20231105, 20, 2.0, 0.2, 5, 300, 300, lambda x: x**3 - x)\n    ]\n\n    all_results_str = []\n    for case in test_cases:\n        results = run_simulation(*case)\n        # Format results to 6 decimal places\n        results_str = f\"[{','.join(f'{r:.6f}' for r in results)}]\"\n        all_results_str.append(results_str)\n\n    # 6. Output\n    print(f\"[{','.join(all_results_str)}]\")\n\nsolve()\n```", "id": "3155854"}]}