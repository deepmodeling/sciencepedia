{"hands_on_practices": [{"introduction": "许多半监督学习方法通过一个正则化项来利用未标记数据，该正则化项鼓励模型在其预测中表现出高置信度。熵最小化就是这样一种技术，它旨在将决策边界推向数据稀疏的区域，这一想法被称为“聚类假设”。然而，当这个基本假设被违反时会发生什么呢？这个练习 [@problem_id:3162680] 挑战你识别一种情况，其中真实的决策边界恰好位于数据最密集的区域，从而导致熵最小化将模型推离最优解，这突显了在应用半监督方法之前理解其基本假设的重要性。", "problem": "给定一个半监督分类场景，其中有一个从二元分布中抽取的小型标记数据集，以及一个从相同输入边缘分布中抽取的大型未标记数据集。考虑一种添加未标记数据正则化的方法，该方法鼓励低预测不确定性，通常通过最小化未标记输入上预测分布的期望熵来实现。设一个在 $K$ 个类别上的类别分布 $q(y \\mid x)$ 的熵定义为 $H(q(y \\mid x)) = - \\sum_{k=1}^{K} q_k(y \\mid x) \\log q_k(y \\mid x)$，并回顾聚类假设：决策边界应该位于边缘输入密度 $p(x)$ 较低的区域。同时回顾贝叶斯决策规则：一个预测为 $\\hat{y}(x) = \\arg\\max_{y} p(y \\mid x)$ 的分类器会最小化期望的 $0$-$1$ 损失。\n\n哪个选项正确地构建了一个科学上一致的数据集，并解释了为什么对未标记数据进行熵最小化会使学习到的决策边界偏离贝叶斯决策规则，特别是由于违反了聚类假设，即后验概率 $p(y \\mid x)$ 在高边缘密度区域发生变化？\n\nA. 输入 $x \\in \\mathbb{R}^{2}$ 从单个各向同性高斯分布 $p(x) = \\mathcal{N}(x; 0, \\sigma^{2} I_{2})$ 中抽取，其中 $\\sigma^{2} > 0$。标签在每个半空间是确定性的：如果 $x_{1} \\ge 0$，则 $p(y=1 \\mid x) = 1$；如果 $x_{1}  0$，则 $p(y=1 \\mid x) = 0$ (因此 $p(y=0 \\mid x) = 1 - p(y=1 \\mid x)$)。贝叶斯决策边界是垂直线 $x_{1} = 0$，它穿过 $x = 0$ 附近的最高密度区域。熵最小化鼓励对高密度的未标记输入进行一致置信的预测，因此可能将边界推向低密度尾部，以避免在 $x_{1} = 0$ 附近的预测不确定，从而产生一个与贝叶斯规则不一致的边界，并导致在一个半空间上产生大的分类错误。\n\nB. 输入 $x \\in \\mathbb{R}^{2}$ 从 $x_{1}$ 轴上两个分离良好的高斯分布的混合中抽取：$p(x) = \\tfrac{1}{2}\\,\\mathcal{N}(x; (-m, 0), \\sigma^{2} I_{2}) + \\tfrac{1}{2}\\,\\mathcal{N}(x; (m, 0), \\sigma^{2} I_{2})$，其中 $m \\gg \\sigma > 0$。标签遵循聚类成员关系：在左侧聚类上 $p(y=0 \\mid x)$ 很高，在右侧聚类上 $p(y=1 \\mid x)$ 很高。贝叶斯边界位于聚类之间（靠近 $x_{1} = 0$），这是一个低密度区域。熵最小化将边界与这个密度谷对齐，从而改善分类效果。\n\nC. 输入 $x \\in \\mathbb{R}^{2}$ 具有一个径向双峰边缘分布：$r = \\lVert x \\rVert_{2}$ 的分布在 $r = r_{1}$ 和 $r = r_{2}$ 处有两个峰，其中 $r_{1}  r_{2}$，并且在这些半径之间的环形区域密度较低。标签是径向阈值化的：对于 $r \\approx r_{1}$，$p(y=0 \\mid x)$ 很高；对于 $r \\approx r_{2}$，$p(y=1 \\mid x)$ 很高。贝叶斯决策边界是环之间的低密度环形区域，因此熵最小化鼓励在低密度间隙中形成边界，从而支持正确的决策。\n\nD. 输入 $x \\in \\mathbb{R}^{2}$ 从一个以 0 为中心的重尾但单峰的分布中抽取，例如乘积拉普拉斯分布 $p(x) \\propto \\exp(-\\lvert x_{1} \\rvert / b - \\lvert x_{2} \\rvert / b)$，其中 $b > 0$。标签是径向的：如果 $\\lVert x \\rVert_{2} \\le R$，则 $p(y=1 \\mid x) = 1$，否则 $p(y=1 \\mid x) = 0$，其中 $R > 0$。贝叶斯边界是圆 $\\lVert x \\rVert_{2} = R$，它位于一个中等密度区域，但不在 $x = 0$ 的峰值附近。在这里，熵最小化通常不会将边界推离贝叶斯解，因为后验概率翻转并非发生在最高密度点；这里没有对聚类假设的强烈违反。\n\n选择最佳选项，并根据贝叶斯决策规则、熵和聚类假设的定义，通过推理来证明您的选择。", "solution": "用户要求分析一个关于半监督学习的统计学习问题，具体来说是当聚类假设被违反时，熵最小化正则化可能失败的情况。我将首先验证问题陈述，然后进行详细的解答。\n\n### 问题验证\n\n**步骤1：提取已知条件**\n\n-   **场景**：半监督分类。\n-   **数据**：一个小型标记数据集和一个大型未标记数据集，两者均从相同的输入边缘分布 $p(x)$ 中抽取。标记集来自一个二元分布。\n-   **方法**：一种未标记数据正则化器，鼓励低预测不确定性，通过最小化未标记输入上预测分布 $q(y \\mid x)$ 的期望熵来实现。\n-   **熵定义**：对于一个在 $K$ 个类别上的类别分布 $q(y \\mid x)$，熵为 $H(q(y \\mid x)) = - \\sum_{k=1}^{K} q_k(y \\mid x) \\log q_k(y \\mid x)$。\n-   **聚类假设**：决策边界应该位于边缘输入密度 $p(x)$ 较低的区域。\n-   **贝叶斯决策规则**：预测 $\\hat{y}(x) = \\arg\\max_{y} p(y \\mid x)$ 的分类器最小化期望的0-1损失。\n-   **问题**：找出哪个选项正确地构建了一个数据集，并解释了为什么对未标记数据进行熵最小化会使学习到的决策边界偏离贝叶斯决策规则，原因是违反了聚类假设（即后验概率 $p(y \\mid x)$ 在高边缘密度 $p(x)$ 区域发生变化）。\n\n**步骤2：使用提取的已知条件进行验证**\n\n1.  **科学基础**：该问题陈述牢固地植根于统计学习理论的既定概念。半监督学习、熵最小化（一种不确定性正则化）、聚类假设和贝叶斯决策规则都是标准且定义明确的主题。数据分布与聚类假设之间的不匹配可能导致半监督方法失败，这是该领域一个已知且重要的问题。\n2.  **问题明确**：问题提得很好。它要求在四个场景中找出一个最佳选项，以说明一个特定的、描述清晰的现象。可以通过基于所提供定义的逻辑推导来确定正确答案的存在。\n3.  **客观性**：使用的语言精确、技术化且客观。它避免了任何主观或模糊的术语。\n4.  **完整性和一致性**：问题是自洽的。它提供了评估选项所需的所有必要定义（$H(q(y \\mid x))$、贝叶斯规则、聚类假设）和背景。没有内部矛盾。\n5.  **其他缺陷**：该问题没有表现出任何其他缺陷。它并非微不足道，因为它需要理解边缘数据密度 $p(x)$、后验概率 $p(y \\mid x)$ 以及熵正则化器行为之间的相互作用。\n\n**步骤3：结论与行动**\n\n问题陈述是**有效的**。我将继续推导解决方案并评估每个选项。\n\n### 解决方案推导\n\n问题的核心在于理解在特定场景下，监督损失的目标与熵正则化项的目标之间的冲突。\n\n-   监督损失项（例如，对标记数据的交叉熵）鼓励学习到的预测分布 $q(y \\mid x)$ 与真实后验概率 $p(y \\mid x)$ 相匹配。这类模型的决策边界理想情况下会收敛到贝叶斯决策边界，即点集 $p(y=1 \\mid x) = p(y=0 \\mid x) = 0.5$（对于二元问题）。\n-   熵正则化项 $\\mathbb{E}_{x \\sim p_{unlabeled}(x)}[H(q(y \\mid x))]$ 鼓励模型对其对未标记数据的预测充满信心（低熵）。如果 $q(y \\mid x)$ 接近一个独热向量（例如，$q(y=1 \\mid x)$ 接近 $0$ 或 $1$），则预测是低熵的。熵在模型最不确定的地方最大化，即 $q(y=1 \\mid x) = 0.5$。\n-   **聚类假设**假定决策边界（不确定性应最高的地方）位于 $p(x)$ 的低密度区域。如果这一假设成立，那么边界附近几乎没有未标记点，因此熵正则化器在那里的影响不强，两个目标能够很好地对齐。\n-   当决策边界位于 $p(x)$ 的高密度区域时，就**违反了聚类假设**。在这种情况下，许多未标记点都位于边界附近。熵正则化器会强烈惩罚模型在这些点上的不确定性，将模型的决策边界推离这个高密度区域，移至一个低密度区域，即使这与（稀缺的）标记数据所指示的真实贝叶斯决策边界相矛盾。\n\n我们需要找到描述这种失效模式的选项。\n\n**逐项分析**\n\n**A. 输入 $x \\in \\mathbb{R}^{2}$ 从单个各向同性高斯分布 $p(x) = \\mathcal{N}(x; 0, \\sigma^{2} I_{2})$ 中抽取，其中 $\\sigma^{2} > 0$。标签在每个半空间是确定性的：如果 $x_{1} \\ge 0$，则 $p(y=1 \\mid x) = 1$；如果 $x_{1}  0$，则 $p(y=1 \\mid x) = 0$。贝叶斯决策边界是垂直线 $x_{1} = 0$，它穿过 $x=0$ 附近的最高密度区域。熵最小化鼓励对高密度的未标记输入进行一致置信的预测，因此可能将边界推向低密度尾部，以避免在 $x_{1} = 0$ 附近的预测不确定，从而产生一个与贝叶斯规则不一致的边界，并导致在一个半空间上产生大的分类错误。**\n\n-   **数据分布**：边缘密度 $p(x)$ 是一个以原点 $x=(0,0)$ 为中心的单峰高斯分布。最高密度在中心。\n-   **贝叶斯边界**：真实后验概率 $p(y \\mid x)$ 在直线 $x_1 = 0$ 处从 $0$ 跳到 $1$。因此，贝叶斯最优决策边界是 $x_1 = 0$。\n-   **聚类假设**：这条边界 $x_1=0$ 直接穿过数据分布的众数（mode）$x=(0,0)$。这是对聚类假设的严重违反，因为决策边界穿过了边缘密度最高的区域。\n-   **熵最小化的效果**：大量的未标记点将集中在原点附近，因此也靠近真实边界 $x_1 = 0$。试图对这个边界建模的分类器，对于靠近 $x_1=0$ 的点，必然会有很高的不确定性（预测概率接近 $0.5$）。作用于该区域密集未标记数据的熵正则化器，将产生强烈的激励，迫使模型将其决策边界“移动”到数据点较少的地方，即高斯分布的低密度尾部。例如，模型可能会学习一个边界 $x_1=c$（其中 $|c|$ 很大），或者一个远离原点的弯曲边界。这种修改为绝大多数未标记点实现了低熵，但将边界置于远离真实边界的位置，导致泛化性能不佳。\n-   **结论**：该选项正确地描述了一个违反聚类假设的数据集，并提供了一个科学上合理的解释，说明了为什么熵最小化会导致一个次优的分类器，将学习到的边界推离贝叶斯最优边界。**正确**。\n\n**B. 输入 $x \\in \\mathbb{R}^{2}$ 从 $x_{1}$ 轴上两个分离良好的高斯分布的混合中抽取：$p(x) = \\tfrac{1}{2}\\,\\mathcal{N}(x; (-m, 0), \\sigma^{2} I_{2}) + \\tfrac{1}{2}\\,\\mathcal{N}(x; (m, 0), \\sigma^{2} I_{2})$，其中 $m \\gg \\sigma > 0$。标签遵循聚类成员关系：在左侧聚类上 $p(y=0 \\mid x)$ 很高，在右侧聚类上 $p(y=1 \\mid x)$ 很高。贝叶斯边界位于聚类之间（靠近 $x_{1} = 0$），这是一个低密度区域。熵最小化将边界与这个密度谷对齐，从而改善分类效果。**\n\n-   **数据分布**：边缘密度 $p(x)$ 在两个聚类中很高，而在它们之间的区域（一个密度“谷”）很低。\n-   **贝叶斯边界**：分隔两个类别的边界位于靠近 $x_1=0$ 的谷中。\n-   **聚类假设**：决策边界位于一个低密度区域。聚类假设被*满足*了。\n-   **熵最小化的效果**：熵最小化通过鼓励模型将决策边界放在低密度谷中来帮助模型，这与贝叶斯最优解是一致的。这是半监督学习*效果良好*的典型例子。\n-   **结论**：该选项描述了熵最小化有益的一种情况，这与问题要求说明的现象相反。**不正确**。\n\n**C. 输入 $x \\in \\mathbb{R}^{2}$ 具有一个径向双峰边缘分布：$r = \\lVert x \\rVert_{2}$ 的分布在 $r = r_{1}$ 和 $r = r_{2}$ 处有两个峰，其中 $r_{1}  r_{2}$，并且在这些半径之间的环形区域密度较低。标签是径向阈值化的：对于 $r \\approx r_{1}$，$p(y=0 \\mid x)$ 很高；对于 $r \\approx r_{2}$，$p(y=1 \\mid x)$ 很高。贝叶斯决策边界是环之间的低密度环形区域，因此熵最小化鼓励在低密度间隙中形成边界，从而支持正确的决策。**\n\n-   **数据分布**：数据形成两个同心的高密度环，它们之间有一个低密度间隙。\n-   **贝叶斯边界**：决策边界将是位于低密度环形区域中分隔两个环的一个圆。\n-   **聚类假设**：决策边界位于一个低密度区域。聚类假设被*满足*了。\n-   **熵最小化的效果**：与选项 B 类似，熵最小化会把决策边界推向低密度间隙，这与贝叶斯最优解一致。这是另一个半监督学习预计会成功的例子。\n-   **结论**：该选项描述了熵最小化有益而非有害的一种情况。**不正确**。\n\n**D. 输入 $x \\in \\mathbb{R}^{2}$ 从一个以 0 为中心的重尾但单峰的分布中抽取，例如乘积拉普拉斯分布 $p(x) \\propto \\exp(-\\lvert x_{1} \\rvert / b - \\lvert x_{2} \\rvert / b)$，其中 $b > 0$。标签是径向的：如果 $\\lVert x \\rVert_{2} \\le R$，则 $p(y=1 \\mid x) = 1$，否则 $p(y=1 \\mid x) = 0$，其中 $R > 0$。贝叶斯边界是圆 $\\lVert x \\rVert_{2} = R$，它位于一个中等密度区域，但不在 $x = 0$ 的峰值附近。在这里，熵最小化通常不会将边界推离贝叶斯解，因为后验概率翻转并非发生在最高密度点；这里没有对聚类假设的强烈违反。**\n\n-   **数据分布**：单峰分布，密度在原点达到峰值。\n-   **贝叶斯边界**：边界是一个半径为 $R$ 的圆。\n-   **聚类假设**：边界 $\\|x\\|_2 = R$（对于 $R>0$）不通过最大密度点（$x=0$）。然而，它确实穿过一个可以有相当大密度的区域，所以聚类假设仍然被违反，尽管可能不像选项 A 那样严重，因为在选项 A 中边界穿过了众数。\n-   **熵最小化的效果**：该选项本身就*反驳了*这是一个好的失效模式的例子。它声称熵最小化“通常不会将边界推离”。尽管所提供的推理很简短，但该选项的结论是这种情况不能说明所期望的失效机制。问题要求一个选项*构建一个数据集并解释为什么*失效会发生。这个选项构建了一个数据集，然后论证失效不显著。\n-   **结论**：这个选项没有完成解释边界为何被推离的核心任务。它明确地陈述了相反的情况。**不正确**。\n\n**结论**\n\n选项 A 提供了对题干所述场景的最清晰、最准确的例子。它构建了一个数据分布和一个分类任务，导致了对聚类假设的强烈违反，其中贝叶斯最优决策边界穿过了数据密度最高的区域。它接着正确地解释了在这种情况下，熵最小化正则化器会与监督目标冲突，并将学习到的决策边界推向一个远离最优边界的低密度区域，从而降低性能。", "answer": "$$\\boxed{A}$$", "id": "3162680"}, {"introduction": "在概念上理解了半监督学习的潜在陷阱之后，让我们通过数学分析来深入探究其内部机制。自训练（Self-training）是一种直观的半监督学习方法，模型使用自己的高置信度预测为未标记数据生成伪标签，并以此进行再训练。这个练习 [@problem_id:3162593] 提供了一个简化的模型，让你能够精确地推导出更新方程并分析其迭代过程的动态行为，从而对算法如何利用未标记数据获得更深刻的定量理解。", "problem": "考虑一个统计学习中的半监督自训练过程，该过程结合了监督学习（有标签数据）、非监督学习（无标签数据）和半监督学习（共同利用有标签和无标签数据）的思想。一个一维特征 $x \\in \\mathbb{R}$ 由两个类别 $C_{1}$ 或 $C_{2}$ 中的一个生成，其类条件分布为 $x \\mid C_{k} \\sim \\mathcal{N}(\\mu_{k}, \\sigma^{2})$（其中 $k \\in \\{1,2\\}$），方差 $\\sigma^{2}$ 相等且已知，类先验概率相等，即 $P(C_{1}) = P(C_{2}) = \\frac{1}{2}$。给定来自每个类别的小的有标签数据集，它们产生初始样本均值估计 $m_{1}^{(0)}$ 和 $m_{2}^{(0)}$。同时还给定一个从上述混合分布中独立抽取的大量无标签样本。\n\n在第 $t$ 次迭代时，自训练过程定义如下：\n- 决策阈值 $\\tau^{(t)}$ 被设置为当前类均值估计的中点：$\\tau^{(t)} = \\frac{1}{2}\\big(m_{1}^{(t)} + m_{2}^{(t)}\\big)$。\n- 每个无标签点 $x$ 通过一个插件式分类器被赋予伪标签：如果 $x \\le \\tau^{(t)}$ 则分配为 $C_{1}$，如果 $x > \\tau^{(t)}$ 则分配为 $C_{2}$。\n- 类均值估计通过先前估计值与伪标签分区均值的凸组合进行更新：\n$$\nm_{1}^{(t+1)} = \\lambda\\, m_{1}^{(t)} + (1-\\lambda)\\, \\mathbb{E}\\!\\left[X \\,\\middle|\\, X \\le \\tau^{(t)}\\right], \\quad\nm_{2}^{(t+1)} = \\lambda\\, m_{2}^{(t)} + (1-\\lambda)\\, \\mathbb{E}\\!\\left[X \\,\\middle|\\, X > \\tau^{(t)}\\right],\n$$\n其中 $X$ 表示服从无标签混合分布的随机变量，$\\lambda \\in (0,1)$ 是一个固定权重，反映了有标签数据的相对影响。\n\n从条件期望的定义和正态分布的性质出发，用正态概率密度函数 (PDF) $\\phi$ 和累积分布函数 (CDF) $\\Phi$ 表示并推导以下内容：\n1. 在给定的具有相等先验概率的混合模型下，$\\mathbb{E}\\!\\left[X \\,\\middle|\\, X \\le \\tau\\right]$ 和 $\\mathbb{E}\\!\\left[X \\,\\middle|\\, X > \\tau\\right]$ 的闭式表达式。\n2. 相应的 $m_{1}^{(t+1)}$、$m_{2}^{(t+1)}$ 和 $\\tau^{(t+1)}$ 的迭代更新方程。\n\n最后，特别地，考虑 $\\mu_{1} = -\\mu$、$\\mu_{2} = \\mu$（其中 $\\mu > 0$）的对称情况。确定满足 $\\tau^{\\star} = \\frac{1}{2}\\big(m_{1}^{\\star} + m_{2}^{\\star}\\big)$ 和 $m_{k}^{\\star} = \\lambda\\, m_{k}^{\\star} + (1-\\lambda)\\, \\mathbb{E}\\!\\left[X \\,\\middle|\\, \\text{在 } \\tau^{\\star} \\text{ 处的伪标签指示为 } C_{k} \\right]$（$k \\in \\{1,2\\}$）的不动点阈值 $\\tau^{\\star}$，并给出其精确值。无需四舍五入。最终答案必须是一个实数。", "solution": "用户希望我解决一个关于半监督自训练算法的问题。\n\n### 问题验证\n\n**步骤 1：提取已知条件**\n\n-   **特征空间**：一维特征 $x \\in \\mathbb{R}$。\n-   **类别**：$C_{1}$ 和 $C_{2}$。\n-   **类条件分布**：$x \\mid C_{k} \\sim \\mathcal{N}(\\mu_{k}, \\sigma^{2})$，其中 $k \\in \\{1,2\\}$。\n-   **方差**：两个类别的方差 $\\sigma^{2}$ 相等且已知。\n-   **类先验概率**：$P(C_{1}) = P(C_{2}) = \\frac{1}{2}$。\n-   **初始数据**：小的有标签集提供初始均值估计 $m_{1}^{(0)}$ 和 $m_{2}^{(0)}$。一个大的无标签样本从指定的混合分布中抽取。\n-   **第 $t$ 次迭代时的自训练过程**：\n    -   决策阈值：$\\tau^{(t)} = \\frac{1}{2}\\big(m_{1}^{(t)} + m_{2}^{(t)}\\big)$。\n    -   伪标签规则：如果 $x \\le \\tau^{(t)}$ 则分配为类别 $C_{1}$，如果 $x > \\tau^{(t)}$ 则分配为 $C_{2}$。\n    -   均值更新方程：\n        $$\n        m_{1}^{(t+1)} = \\lambda\\, m_{1}^{(t)} + (1-\\lambda)\\, \\mathbb{E}\\!\\left[X \\,\\middle|\\, X \\le \\tau^{(t)}\\right]\n        $$\n        $$\n        m_{2}^{(t+1)} = \\lambda\\, m_{2}^{(t)} + (1-\\lambda)\\, \\mathbb{E}\\!\\left[X \\,\\middle|\\, X > \\tau^{(t)}\\right]\n        $$\n    -   权重参数：$\\lambda \\in (0,1)$。\n    -   随机变量 $X$：从无标签混合分布中抽取。\n-   **任务**：\n    1.  用标准正态 PDF $\\phi$ 和 CDF $\\Phi$ 推导 $\\mathbb{E}\\!\\left[X \\,\\middle|\\, X \\le \\tau\\right]$ 和 $\\mathbb{E}\\!\\left[X \\,\\middle|\\, X > \\tau\\right]$ 的闭式表达式。\n    2.  推导 $m_{1}^{(t+1)}$、$m_{2}^{(t+1)}$ 和 $\\tau^{(t+1)}$ 的迭代更新方程。\n    3.  对于对称情况 $\\mu_{1} = -\\mu$ 和 $\\mu_{2} = \\mu$ ($\\mu > 0$)，确定不动点阈值 $\\tau^{\\star}$ 的精确值。\n\n**步骤 2：使用提取的已知条件进行验证**\n\n-   **有科学依据**：该问题在统计学习领域是公认的，涉及半监督学习中自训练的一种变体。相关概念（高斯混合模型、条件期望、迭代估计）都是标准的，并且在科学上是合理的。\n-   **适定性**：问题定义清晰，包含了推导所要求量和分析不动点所需的所有必要组成部分。在指定的对称性下，不动点阈值存在唯一解是合理的。\n-   **客观性**：问题使用形式化数学语言陈述，没有任何主观或模糊的术语。\n-   **结论**：该问题未违反任何无效性标准。它是统计学习理论中一个有效、可解的问题。\n\n**步骤 3：裁定与行动**\n\n问题有效。我将开始解题。\n\n### 解法\n\n随机变量 $X$ 从两个正态分布的混合中抽取。其概率密度函数 (PDF) $f_X(x)$ 由全概率定律给出：\n$$f_X(x) = P(C_1) f_{X|C_1}(x) + P(C_2) f_{X|C_2}(x)$$\n设 $\\phi_{\\mu, \\sigma^2}(x)$ 是正态分布 $\\mathcal{N}(\\mu, \\sigma^2)$ 的 PDF。我们有 $\\phi_{\\mu, \\sigma^2}(x) = \\frac{1}{\\sigma} \\phi\\left(\\frac{x-\\mu}{\\sigma}\\right)$，其中 $\\phi(z)$ 是标准正态 PDF。给定 $P(C_1)=P(C_2)=\\frac{1}{2}$，混合 PDF 为：\n$$f_X(x) = \\frac{1}{2} \\phi_{\\mu_1, \\sigma^2}(x) + \\frac{1}{2} \\phi_{\\mu_2, \\sigma^2}(x)$$\n$X$ 的累积分布函数 (CDF) 是 $F_X(\\tau) = P(X \\le \\tau) = \\int_{-\\infty}^{\\tau} f_X(x) dx$。设 $\\Phi(z)$ 是标准正态 CDF。\n$$F_X(\\tau) = \\frac{1}{2} \\Phi\\left(\\frac{\\tau-\\mu_1}{\\sigma}\\right) + \\frac{1}{2} \\Phi\\left(\\frac{\\tau-\\mu_2}{\\sigma}\\right)$$\n\n**1. 条件期望的闭式表达式**\n\n我们首先推导正态变量 $Y \\sim \\mathcal{N}(\\mu, \\sigma^2)$ 的部分期望表达式。\n$$ \\int_{-\\infty}^{a} y \\, \\phi_{\\mu, \\sigma^2}(y) dy = \\int_{-\\infty}^{a} y \\frac{1}{\\sigma}\\phi\\left(\\frac{y-\\mu}{\\sigma}\\right) dy $$\n令 $z = \\frac{y-\\mu}{\\sigma}$，则 $y = \\mu + \\sigma z$ 且 $dy=\\sigma dz$。积分上限变为 $z_a = \\frac{a-\\mu}{\\sigma}$。\n$$ \\int_{-\\infty}^{z_a} (\\mu + \\sigma z) \\phi(z) dz = \\mu \\int_{-\\infty}^{z_a} \\phi(z) dz + \\sigma \\int_{-\\infty}^{z_a} z \\phi(z) dz $$\n第一个积分是 $\\mu \\Phi(z_a)$。对于第二个积分，因为 $\\frac{d}{dz}\\phi(z) = -z\\phi(z)$，我们有 $\\int z\\phi(z)dz = -\\phi(z)$。\n$$ \\mu \\Phi(z_a) + \\sigma [-\\phi(z)]_{-\\infty}^{z_a} = \\mu \\Phi(z_a) - \\sigma \\phi(z_a) $$\n所以，$\\int_{-\\infty}^{a} y \\, \\phi_{\\mu, \\sigma^2}(y) dy = \\mu \\Phi\\left(\\frac{a-\\mu}{\\sigma}\\right) - \\sigma \\phi\\left(\\frac{a-\\mu}{\\sigma}\\right)$。\n\n条件期望 $\\mathbb{E}[X \\mid X \\le \\tau]$ 由下式给出：\n$$ \\mathbb{E}[X \\mid X \\le \\tau] = \\frac{\\int_{-\\infty}^{\\tau} x f_X(x) dx}{P(X \\le \\tau)} = \\frac{\\frac{1}{2} \\int_{-\\infty}^{\\tau} x \\phi_{\\mu_1, \\sigma^2}(x) dx + \\frac{1}{2} \\int_{-\\infty}^{\\tau} x \\phi_{\\mu_2, \\sigma^2}(x) dx}{\\frac{1}{2} \\Phi\\left(\\frac{\\tau-\\mu_1}{\\sigma}\\right) + \\frac{1}{2} \\Phi\\left(\\frac{\\tau-\\mu_2}{\\sigma}\\right)} $$\n使用部分期望公式并化简，我们得到：\n$$ \\mathbb{E}[X \\mid X \\le \\tau] = \\frac{\\left[\\mu_1 \\Phi\\left(\\frac{\\tau-\\mu_1}{\\sigma}\\right) - \\sigma \\phi\\left(\\frac{\\tau-\\mu_1}{\\sigma}\\right)\\right] + \\left[\\mu_2 \\Phi\\left(\\frac{\\tau-\\mu_2}{\\sigma}\\right) - \\sigma \\phi\\left(\\frac{\\tau-\\mu_2}{\\sigma}\\right)\\right]}{\\Phi\\left(\\frac{\\tau-\\mu_1}{\\sigma}\\right) + \\Phi\\left(\\frac{\\tau-\\mu_2}{\\sigma}\\right)} $$\n\n对于第二个条件期望 $\\mathbb{E}[X \\mid X > \\tau]$，我们使用 $\\mathbb{E}[Y] = \\int_{-\\infty}^{\\infty} y \\phi_{\\mu,\\sigma^2}(y)dy = \\mu$ 这个事实。\n$$ \\int_{\\tau}^{\\infty} y \\phi_{\\mu, \\sigma^2}(y) dy = \\mathbb{E}[Y] - \\int_{-\\infty}^{\\tau} y \\phi_{\\mu, \\sigma^2}(y) dy = \\mu - \\left[\\mu \\Phi\\left(\\frac{\\tau-\\mu}{\\sigma}\\right) - \\sigma \\phi\\left(\\frac{\\tau-\\mu}{\\sigma}\\right)\\right] $$\n$$ = \\mu \\left(1 - \\Phi\\left(\\frac{\\tau-\\mu}{\\sigma}\\right)\\right) + \\sigma \\phi\\left(\\frac{\\tau-\\mu}{\\sigma}\\right) $$\n条件期望为：\n$$ \\mathbb{E}[X \\mid X > \\tau] = \\frac{\\int_{\\tau}^{\\infty} x f_X(x) dx}{P(X > \\tau)} = \\frac{\\frac{1}{2} \\int_{\\tau}^{\\infty} x \\phi_{\\mu_1, \\sigma^2}(x) dx + \\frac{1}{2} \\int_{\\tau}^{\\infty} x \\phi_{\\mu_2, \\sigma^2}(x) dx}{\\frac{1}{2} P(X > \\tau|C_1) + \\frac{1}{2} P(X > \\tau|C_2)} $$\n$$ \\mathbb{E}[X \\mid X > \\tau] = \\frac{\\left[\\mu_1\\left(1-\\Phi\\left(\\frac{\\tau-\\mu_1}{\\sigma}\\right)\\right) + \\sigma\\phi\\left(\\frac{\\tau-\\mu_1}{\\sigma}\\right)\\right] + \\left[\\mu_2\\left(1-\\Phi\\left(\\frac{\\tau-\\mu_2}{\\sigma}\\right)\\right) + \\sigma\\phi\\left(\\frac{\\tau-\\mu_2}{\\sigma}\\right)\\right]}{\\left(1-\\Phi\\left(\\frac{\\tau-\\mu_1}{\\sigma}\\right)\\right) + \\left(1-\\Phi\\left(\\frac{\\tau-\\mu_2}{\\sigma}\\right)\\right)} $$\n\n为简化记法，令 $z_k = \\frac{\\tau-\\mu_k}{\\sigma}$，$\\Phi_k = \\Phi(z_k)$，且 $\\phi_k = \\phi(z_k)$。\n$$ \\mathbb{E}[X \\mid X \\le \\tau] = \\frac{\\mu_1 \\Phi_1 + \\mu_2 \\Phi_2 - \\sigma(\\phi_1 + \\phi_2)}{\\Phi_1 + \\Phi_2} $$\n$$ \\mathbb{E}[X \\mid X > \\tau] = \\frac{\\mu_1(1-\\Phi_1) + \\mu_2(1-\\Phi_2) + \\sigma(\\phi_1 + \\phi_2)}{2 - \\Phi_1 - \\Phi_2} $$\n\n**2. 迭代更新方程**\n\n将第 1 部分的表达式代入给定的递推关系，即可得到迭代更新方程。第 $t$ 次迭代时的阈值为 $\\tau^{(t)} = \\frac{1}{2}(m_{1}^{(t)} + m_{2}^{(t)})$。期望是关于真实混合分布计算的，因此我们使用真实参数 $\\mu_k$ 和 $\\sigma$。\n令 $z_k^{(t)} = \\frac{\\tau^{(t)}-\\mu_k}{\\sigma}$，$\\Phi_k^{(t)} = \\Phi(z_k^{(t)})$，且 $\\phi_k^{(t)} = \\phi(z_k^{(t)})$。\n\n$m_1$ 的更新方程为：\n$$ m_{1}^{(t+1)} = \\lambda\\, m_{1}^{(t)} + (1-\\lambda)\\,\\left[\\frac{\\mu_1 \\Phi_1^{(t)} + \\mu_2 \\Phi_2^{(t)} - \\sigma(\\phi_1^{(t)} + \\phi_2^{(t)})}{\\Phi_1^{(t)} + \\Phi_2^{(t)}}\\right] $$\n$m_2$ 的更新方程为：\n$$ m_{2}^{(t+1)} = \\lambda\\, m_{2}^{(t)} + (1-\\lambda)\\,\\left[\\frac{\\mu_1(1-\\Phi_1^{(t)}) + \\mu_2(1-\\Phi_2^{(t)}) + \\sigma(\\phi_1^{(t)} + \\phi_2^{(t)})}{2 - \\Phi_1^{(t)} - \\Phi_2^{(t)}}\\right] $$\n阈值的更新方程为：\n$$ \\tau^{(t+1)} = \\frac{1}{2} \\left( m_{1}^{(t+1)} + m_{2}^{(t+1)} \\right) $$\n\n**3. 对称情况下的不动点阈值**\n\n我们考虑对称情况，即 $\\mu_1 = -\\mu$ 和 $\\mu_2 = \\mu$，其中 $\\mu > 0$。\n一个不动点 $(m_1^{\\star}, m_2^{\\star}, \\tau^{\\star})$ 满足去掉所有时间上标后的更新方程。由于 $\\lambda \\in (0,1)$，均值的条件简化为：\n$$ m_{1}^{\\star} = \\mathbb{E}[X \\mid X \\le \\tau^{\\star}] $$\n$$ m_{2}^{\\star} = \\mathbb{E}[X \\mid X > \\tau^{\\star}] $$\n阈值由下式给出：\n$$ \\tau^{\\star} = \\frac{1}{2}\\left(m_{1}^{\\star} + m_{2}^{\\star}\\right) $$\n将均值方程代入阈值方程，得到 $\\tau^{\\star}$ 的自洽条件：\n$$ \\tau^{\\star} = \\frac{1}{2}\\left( \\mathbb{E}[X \\mid X \\le \\tau^{\\star}] + \\mathbb{E}[X \\mid X > \\tau^{\\star}] \\right) $$\n我们定义函数 $H(\\tau) = \\frac{1}{2}\\left( \\mathbb{E}[X \\mid X \\le \\tau] + \\mathbb{E}[X \\mid X > \\tau] \\right)$。我们寻找满足 $\\tau^{\\star} = H(\\tau^{\\star})$ 的不动点 $\\tau^{\\star}$。\n\n在对称情况下，混合 PDF $f_X(x)$ 是一个偶函数：\n$$ f_X(-x) = \\frac{1}{2} \\phi_{-\\mu, \\sigma^2}(-x) + \\frac{1}{2} \\phi_{\\mu, \\sigma^2}(-x) = \\frac{1}{2\\sigma}\\phi\\left(\\frac{-x-(-\\mu)}{\\sigma}\\right) + \\frac{1}{2\\sigma}\\phi\\left(\\frac{-x-\\mu}{\\sigma}\\right) $$\n$$ = \\frac{1}{2\\sigma}\\phi\\left(\\frac{-(x-\\mu)}{\\sigma}\\right) + \\frac{1}{2\\sigma}\\phi\\left(\\frac{-(x+\\mu)}{\\sigma}\\right) = \\frac{1}{2\\sigma}\\phi\\left(\\frac{x-\\mu}{\\sigma}\\right) + \\frac{1}{2\\sigma}\\phi\\left(\\frac{x+\\mu}{\\sigma}\\right) $$\n$$ = \\frac{1}{2} \\phi_{\\mu, \\sigma^2}(x) + \\frac{1}{2} \\phi_{-\\mu, \\sigma^2}(x) = f_X(x) $$\n现在，我们来分析函数 $H(\\tau)$。考虑 $H(-\\tau)$：\n$$ H(-\\tau) = \\frac{1}{2}\\left( \\mathbb{E}[X \\mid X \\le -\\tau] + \\mathbb{E}[X \\mid X > -\\tau] \\right) $$\n我们来计算各个分量。令 $Y = -X$。$Y$ 的 PDF 是 $f_Y(y) = f_X(-y) = f_X(y)$，所以 $Y$ 和 $X$ 有相同的分布。\n$$ \\mathbb{E}[X \\mid X \\le -\\tau] = \\mathbb{E}[-Y \\mid -Y \\le -\\tau] = \\mathbb{E}[-Y \\mid Y \\ge \\tau] = -\\mathbb{E}[Y \\mid Y \\ge \\tau] = -\\mathbb{E}[X \\mid X \\ge \\tau] $$\n$$ \\mathbb{E}[X \\mid X > -\\tau] = \\mathbb{E}[-Y \\mid -Y > -\\tau] = \\mathbb{E}[-Y \\mid Y  \\tau] = -\\mathbb{E}[Y \\mid Y  \\tau] = -\\mathbb{E}[X \\mid X  \\tau] $$\n注意，对于连续变量，$\\mathbb{E}[X \\mid X  \\tau] = \\mathbb{E}[X \\mid X \\le \\tau]$ 且 $\\mathbb{E}[X \\mid X \\ge \\tau] = \\mathbb{E}[X \\mid X > \\tau]$。\n所以，我们证明了 $\\mathbb{E}[X \\mid X \\le -\\tau] = -\\mathbb{E}[X \\mid X > \\tau]$ 和 $\\mathbb{E}[X \\mid X > -\\tau] = -\\mathbb{E}[X \\mid X \\le \\tau]$。\n将这些代入 $H(-\\tau)$ 的表达式中：\n$$ H(-\\tau) = \\frac{1}{2}\\left( -\\mathbb{E}[X \\mid X > \\tau] - \\mathbb{E}[X \\mid X \\le \\tau] \\right) = - \\frac{1}{2}\\left( \\mathbb{E}[X \\mid X \\le \\tau] + \\mathbb{E}[X \\mid X > \\tau] \\right) = -H(\\tau) $$\n因此，$H(\\tau)$ 是一个奇函数。\n不动点方程是 $\\tau = H(\\tau)$。对于 $\\tau=0$，我们有 $H(0) = -H(-0) = -H(0)$，这意味着 $2H(0)=0$，所以 $H(0)=0$。\n因此，$\\tau^{\\star} = 0$ 是不动点方程的一个解。\n\n问题要求的是‘那个’不动点阈值，暗示了唯一性。令 $g(\\tau) = H(\\tau) - \\tau$。$g(\\tau)$ 的根就是不动点。由于 $H(\\tau)$ 是奇函数，$g(\\tau)$ 也是一个奇函数：$g(-\\tau) = H(-\\tau) - (-\\tau) = -H(\\tau) + \\tau = -g(\\tau)$。如果存在一个非零解 $\\tau_0 \\ne 0$ 使得 $g(\\tau_0)=0$，那么可以立即推断出 $g(-\\tau_0) = -g(\\tau_0) = 0$，这意味着 $-\\tau_0$ 也是一个解。非零解的存在将意味着非唯一性。要使解如问题措辞所暗示的那样是唯一的，唯一的可能性就是不存在非零解。这使得 $\\tau^{\\star}=0$ 成为唯一的不动点。\n\n不动点阈值是一个单一的值，并且考虑到问题的对称性，唯一的候选者就是对称中心。\n\n最终答案：不动点阈值 $\\tau^{\\star}$ 是 $0$。", "answer": "$$\\boxed{0}$$", "id": "3162593"}, {"introduction": "最后，我们将理论付诸实践，完成一个基于图的半监督学习编码挑战，这是一种特别适用于具有关系结构数据的强大范式。在这个练习 [@problem_id:3162685] 中，你将模拟社交网络中“错误信息”的传播，这是一个关键的现实世界应用。你需要实现从构建加权图、处理有影响力的“中心”节点，到执行迭代式标签传播算法的整个流程，亲身体验信念如何在网络中扩散和稳定，并探索不同设计选择（如边权重和中心节点处理策略）对结果的影响。", "problem": "给定一个基于图的半监督学习 (SSL) 设置，应用于一个无向社交网络。其核心原理是使用带重启的随机游走来模拟标量“信念”分数在图上的扩散过程，该过程由一小组种子标签驱动。你的任务是实现一个程序，该程序根据邻接结构和节点特征构建一个加权图，应用不同的边加权方案，处理中心节点的存在，并执行迭代式标签传播直至收敛。最终输出必须汇总所提供测试套件中指定节点的分数。\n\n基本原理：基于图、相似性度量和行随机转移矩阵的定义进行工作。扩散过程是一个离散时间带重启的随机游走，这是统计学习和马尔可夫过程中一个经过充分检验的原理。\n\n数据集规格：\n- 节点由 $0,1,2,3,4,5,6,7$ 索引。\n- 无向边由节点对集合 $\\{(1,0),(1,2),(1,3),(1,4),(1,5),(1,6),(1,7),(0,2),(2,5),(3,4),(4,6),(5,6)\\}$ 给出，这意味着如果 $(i,j)$ 在集合中，则节点 $i$ 和 $j$ 是双向连接的。\n- 节点特征是$4$维实数向量，具体如下：\n  - 节点 $0$: $[1,1,0,0]$\n  - 节点 $1$: $[1,1,1,0]$\n  - 节点 $2$: $[1,0,0,1]$\n  - 节点 $3$: $[1,0,0,0]$\n  - 节点 $4$: $[0,1,1,0]$\n  - 节点 $5$: $[1,1,0,1]$\n  - 节点 $6$: $[0,0,1,1]$\n  - 节点 $7$: $[0,1,0,0]$\n- 种子标签（解释为初始错误信息信念）由 $Y \\in \\mathbb{R}^8$ 给出，其中 $Y_0 = 1$，$Y_5 = 1$，对于所有其他节点 $i$，$Y_i = 0$。\n\n边加权：\n- 对于一条无向边 $(i,j)$，分配权重 $W_{ij} = s(i,j)$，其中 $s(i,j)$ 使用以下两种相似性函数之一根据节点特征计算得出：\n  - Jaccard 相似性，用于二元隶属特征：\n    $$s_{\\text{J}}(i,j) = \\frac{|A_i \\cap A_j|}{|A_i \\cup A_j|},$$\n    其中 $A_i = \\{k : \\text{节点 } i \\text{ 在索引 } k \\text{ 处的特征} > 0\\}$，$|\\cdot|$ 表示集合的基数。如果 $|A_i \\cup A_j| = 0$，则定义 $s_{\\text{J}}(i,j) = 0$。\n  - 余弦相似性，用于实数向量：\n    $$s_{\\text{C}}(i,j) = \\frac{x_i \\cdot x_j}{\\|x_i\\|_2 \\, \\|x_j\\|_2},$$\n    其中 $x_i \\in \\mathbb{R}^4$ 是节点 $i$ 的特征向量，$\\cdot$ 表示欧几里得内积，$\\|\\cdot\\|_2$ 是欧几里得范数。如果分母为 $0$，则定义 $s_{\\text{C}}(i,j) = 0$。\n- 如果 $(i,j)$ 不是一条边，则设置 $W_{ij} = 0$。由于图是无向的，请确保 $W_{ij} = W_{ji}$。\n\n中心节点敏感度：\n- 将节点 $i$ 的未加权度 $d_i$ 定义为给定集合中通过边连接的邻居数量（即与节点 $i$ 关联的边的数量）。将任何满足 $d_i > \\tau$（其中阈值 $\\tau = 5$）的节点定义为中心节点。对于此图，节点 $1$ 是一个中心节点。\n- 在归一化之前，应用以下中心节点策略之一：\n  - \"none\"：保持 $W$ 不变。\n  - \"downweight\"：对于每个中心节点 $h$，将其所有关联权重乘以一个因子 $\\gamma \\in (0,1)$，即对于所有与 $h$ 连接的 $j$，设置 $W_{h,j} \\leftarrow \\gamma W_{h,j}$ 和 $W_{j,h} \\leftarrow \\gamma W_{j,h}$。\n  - \"remove\"：对于每个中心节点 $h$，对于所有 $j$，设置 $W_{h,j} \\leftarrow 0$ 和 $W_{j,h} \\leftarrow 0$（移除所有与 $h$ 关联的边）。\n\n转移矩阵：\n- 通过行归一化从 $W$ 生成行随机转移矩阵 $P \\in \\mathbb{R}^{8 \\times 8}$：\n  $$P_{ij} = \\begin{cases}\n  \\frac{W_{ij}}{\\sum_{k=0}^{7} W_{ik}},  \\text{if } \\sum_{k=0}^{7} W_{ik} > 0, \\\\\n  0,  \\text{otherwise.}\n  \\end{cases}$$\n\n带重启的扩散：\n- 通过带重启的随机游走规则来演化信念向量 $F^{(t)} \\in \\mathbb{R}^8$，重启参数为 $\\alpha \\in (0,1)$：\n  $$F^{(t+1)} = \\alpha P F^{(t)} + (1 - \\alpha) Y,$$\n  初始化为 $F^{(0)} = Y$。使用 $\\alpha = 0.85$。\n- 迭代直至收敛，收敛条件定义为 $\\max_i |F^{(t+1)}_i - F^{(t)}_i|  \\varepsilon$（容差 $\\varepsilon = 10^{-12}$），或直至达到最大迭代次数 $10000$ 次，以先到者为准。\n\n测试套件：\n计算最终收敛的错误信息分数 $F^*$，并按顺序报告索引为 $[2,3,4,6,7]$ 的节点的值，针对以下四个参数集中的每一个：\n1. 权重计算 $s_{\\text{J}}$ (Jaccard)，中心节点策略 \"none\"。\n2. 权重计算 $s_{\\text{C}}$ (cosine)，中心节点策略 \"none\"。\n3. 权重计算 $s_{\\text{J}}$ (Jaccard)，中心节点策略 \"downweight\"，$\\gamma = 0.3$。\n4. 权重计算 $s_{\\text{C}}$ (cosine)，中心节点策略 \"remove\"。\n\n最终输出格式：\n你的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表。列表中的每个元素本身是针对节点 $[2,3,4,6,7]$ 的五个分数的列表，分数四舍五入到 $6$ 位小数，顺序与测试用例相同。例如：\n$$[[f_{2},f_{3},f_{4},f_{6},f_{7}],[\\ldots],[\\ldots],[\\ldots]]$$\n不应打印任何额外文本。", "solution": "用户指定了一个基于图的半监督学习问题。任务是计算社交网络图上“信念”分数的扩散。该过程涉及构建一个加权图，应用不同的边加权和中心节点处理策略，并运行一个迭代式标签传播算法直至收敛。\n\n该问题在计算上是明确定义的，其科学依据是图上随机游走的原理，并且所有必要的数据和参数都已提供。因此，该问题被认为是有效的。\n\n解决方案针对四个指定的测试用例中的每一个，系统地执行以下步骤：\n\n1.  **图与特征表示**：网络由一个节点集 $V = \\{0, 1, \\dots, 7\\}$ 和一个无向边集 $E$ 定义。我们使用邻接矩阵 $A \\in \\{0, 1\\}^{8 \\times 8}$ 来表示图的拓扑结构，其中如果节点 $i$ 和 $j$ 之间有边连接，则 $A_{ij} = 1$，否则 $A_{ij} = 0$。节点特征存储在矩阵 $X \\in \\mathbb{R}^{8 \\times 4}$ 中，其中第 $i$ 行对应于节点 $i$ 的特征向量 $x_i$。\n\n2.  **边加权**：构建一个权重矩阵 $W \\in \\mathbb{R}^{8 \\times 8}$。对于每个由边连接的节点对 $(i, j)$（即 $A_{ij} = 1$），根据其特征向量 $x_i$ 和 $x_j$ 的相似性计算权重 $W_{ij} = W_{ji}$。如果不存在边，则 $W_{ij} = 0$。使用两种相似性函数：\n    *   **Jaccard 相似性 ($s_{\\text{J}}$)**：该度量适用于二元或基于集合的特征。对于特征向量 $x_i$ 和 $x_j$，我们定义集合 $A_k = \\{l : (x_k)_l > 0\\}$，其中包含非零特征的索引。相似性是这些集合的交集大小与并集大小的比率：\n        $$W_{ij} = s_{\\text{J}}(i, j) = \\frac{|A_i \\cap A_j|}{|A_i \\cup A_j|}$$\n    *   **余弦相似性 ($s_{\\text{C}}$)**：该度量衡量内积空间中两个非零向量之间夹角的余弦值。它通过向量的点积除以其欧几里得范数的乘积来计算：\n        $$W_{ij} = s_{\\text{C}}(i, j) = \\frac{x_i \\cdot x_j}{\\|x_i\\|_2 \\|x_j\\|_2}$$\n\n3.  **中心节点敏感度调整**：问题将中心节点定义为未加权度 $d_i > \\tau = 5$ 的节点。度 $d_i$ 是连接到节点 $i$ 的边的数量。对于这个特定的图，节点 $1$ 的度为 $d_1 = 7$，使其成为唯一的中心节点。然后根据指定的策略修改权重矩阵 $W$：\n    *   `\"none\"`：不对 $W$ 进行任何更改。\n    *   `\"downweight\"`：对于中心节点 $h=1$，所有关联的边权重 $W_{1,j}$ 和 $W_{j,1}$ 均乘以一个因子 $\\gamma = 0.3$。\n    *   `\"remove\"`：对于中心节点 $h=1$，所有关联的边权重 $W_{1,j}$ 和 $W_{j,1}$ 均设置为 $0$。\n\n4.  **转移矩阵构建**：（可能被修改的）权重矩阵 $W$ 被转换为一个行随机转移矩阵 $P \\in \\mathbb{R}^{8 \\times 8}$。每个元素 $P_{ij}$ 表示在随机游走的单步中从节点 $i$ 转移到节点 $j$ 的概率。这是通过对 $W$ 进行行归一化来实现的：\n    $$P_{ij} = \\frac{W_{ij}}{\\sum_{k=0}^{7} W_{ik}}$$\n    如果某行的和 $\\sum_k W_{ik}$ 为零（例如，对于孤立节点），则 $P$ 中对应的行由全零组成。\n\n5.  **带重启的迭代扩散**：算法的核心是信念向量 $F^{(t)} \\in \\mathbb{R}^8$ 的迭代更新，其中 $F_i^{(t)}$ 是节点 $i$ 在第 $t$ 次迭代时的信念分数。从初始信念向量 $F^{(0)} = Y$（其中 $Y$ 是种子标签向量）开始，分数根据以下规则演化：\n    $$F^{(t+1)} = \\alpha P F^{(t)} + (1 - \\alpha) Y$$\n    在这里，$\\alpha=0.85$ 是重启参数。该方程模拟了这样一个过程：在每一步中，一个节点上的信念单元有 $\\alpha$ 的概率传播到其邻居（与 $P$ 中的权重成比例），并有 $1-\\alpha$ 的概率从初始种子分布 $Y$ “重启”。这个重启项确保了种子节点的影响被持续地注入到网络中。\n\n6.  **收敛**：迭代过程持续进行，直到信念向量稳定下来。当任何节点的分数在连续迭代之间的最大绝对变化小于容差阈值 $\\varepsilon = 10^{-12}$ 时，即达到收敛：\n    $$\\max_{i} |F^{(t+1)}_i - F^{(t)}_i|  \\varepsilon$$\n    设置最大迭代次数 $10000$ 次作为安全措施。收敛后的向量表示为 $F^*$，代表最终的稳态信念分布。\n\n7.  **结果汇总**：对于四个测试配置中的每一个，计算收敛的信念分数 $F^*$。然后提取指定目标节点 $[2, 3, 4, 6, 7]$ 的分数，四舍五入到 $6$ 位小数，并编译成最终的输出格式。", "answer": "```python\nimport numpy as np\n\ndef calculate_jaccard(v1, v2):\n    \"\"\"\n    Calculates the Jaccard similarity between two binary feature vectors.\n    \"\"\"\n    set1 = {i for i, val in enumerate(v1) if val > 0}\n    set2 = {i for i, val in enumerate(v2) if val > 0}\n    \n    intersection_size = len(set1.intersection(set2))\n    union_size = len(set1.union(set2))\n    \n    if union_size == 0:\n        return 0.0\n    return intersection_size / union_size\n\ndef calculate_cosine(v1, v2):\n    \"\"\"\n    Calculates the Cosine similarity between two real-valued vectors.\n    \"\"\"\n    dot_product = np.dot(v1, v2)\n    norm_product = np.linalg.norm(v1) * np.linalg.norm(v2)\n    \n    if norm_product == 0:\n        return 0.0\n    return dot_product / norm_product\n\ndef run_diffusion(P, Y, alpha, epsilon, max_iter):\n    \"\"\"\n    Performs the iterative diffusion with restart until convergence.\n    \"\"\"\n    F = Y.copy()\n    for _ in range(max_iter):\n        F_old = F.copy()\n        # F_new = alpha * P @ F_old + (1 - alpha) * Y\n        F = alpha * np.dot(P, F_old) + (1 - alpha) * Y\n        \n        if np.max(np.abs(F - F_old))  epsilon:\n            break\n            \n    return F\n\ndef solve():\n    \"\"\"\n    Main function to solve the semi-supervised learning problem for all test cases.\n    \"\"\"\n    # Dataset Specification\n    num_nodes = 8\n    edges = [(1,0),(1,2),(1,3),(1,4),(1,5),(1,6),(1,7),(0,2),(2,5),(3,4),(4,6),(5,6)]\n    features = np.array([\n        [1, 1, 0, 0],  # Node 0\n        [1, 1, 1, 0],  # Node 1\n        [1, 0, 0, 1],  # Node 2\n        [1, 0, 0, 0],  # Node 3\n        [0, 1, 1, 0],  # Node 4\n        [1, 1, 0, 1],  # Node 5\n        [0, 0, 1, 1],  # Node 6\n        [0, 1, 0, 0]   # Node 7\n    ])\n    Y = np.array([0.0] * num_nodes)\n    Y[0] = 1.0\n    Y[5] = 1.0\n\n    # Parameters\n    alpha = 0.85\n    epsilon = 1e-12\n    max_iter = 10000\n    tau = 5\n    gamma = 0.3\n    target_nodes = [2, 3, 4, 6, 7]\n\n    # Test Suite\n    test_cases = [\n        {'weight_func': 'jaccard', 'hub_strat': 'none'},\n        {'weight_func': 'cosine', 'hub_strat': 'none'},\n        {'weight_func': 'jaccard', 'hub_strat': 'downweight'},\n        {'weight_func': 'cosine', 'hub_strat': 'remove'},\n    ]\n\n    # Build base adjacency matrix\n    A = np.zeros((num_nodes, num_nodes))\n    for i, j in edges:\n        A[i, j] = 1\n        A[j, i] = 1\n\n    all_results = []\n    \n    for params in test_cases:\n        # 1. Build Weight Matrix W\n        W = np.zeros((num_nodes, num_nodes))\n        weight_calculator = calculate_jaccard if params['weight_func'] == 'jaccard' else calculate_cosine\n        \n        for i in range(num_nodes):\n            for j in range(i + 1, num_nodes):\n                if A[i, j] > 0:\n                    sim = weight_calculator(features[i], features[j])\n                    W[i, j] = W[j, i] = sim\n\n        # 2. Apply Hub Strategy\n        degrees = np.sum(A, axis=1)\n        hub_indices = np.where(degrees > tau)[0]\n        \n        if params['hub_strat'] != 'none':\n            for h in hub_indices:\n                neighbors = np.where(A[h, :] > 0)[0]\n                for j in neighbors:\n                    if params['hub_strat'] == 'downweight':\n                        W[h, j] *= gamma\n                        W[j, h] *= gamma\n                    elif params['hub_strat'] == 'remove':\n                        W[h, j] = 0\n                        W[j, h] = 0\n\n        # 3. Build Transition Matrix P\n        row_sums = np.sum(W, axis=1)\n        P = np.zeros_like(W)\n        non_zero_rows = row_sums > 0\n        P[non_zero_rows] = W[non_zero_rows] / row_sums[non_zero_rows, np.newaxis]\n\n        # 4. Run Diffusion\n        F_final = run_diffusion(P, Y, alpha, epsilon, max_iter)\n\n        # 5. Collect results\n        case_results = [F_final[i] for i in target_nodes]\n        all_results.append(case_results)\n\n    # Format output string\n    formatted_results = []\n    for res_list in all_results:\n        formatted_list = [f\"{x:.6f}\" for x in res_list]\n        formatted_results.append(f\"[{','.join(formatted_list)}]\")\n    \n    final_output_str = f\"[{','.join(formatted_results)}]\"\n    print(final_output_str)\n\nsolve()\n```", "id": "3162685"}]}