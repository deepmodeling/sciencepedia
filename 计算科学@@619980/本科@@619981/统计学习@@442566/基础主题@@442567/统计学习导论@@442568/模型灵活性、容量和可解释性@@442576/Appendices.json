{"hands_on_practices": [{"introduction": "模型“容量”或“灵活性”的抽象概念，通常可以通过其使用的参数或特征数量来具体衡量。这个练习将指导你亲自推导多项式模型中的特征数量，并利用这个量化的指标来解释一个典型的过拟合案例。通过这种方式，你将把模型容量这一抽象概念与实际的泛化性能直接联系起来 [@problem_id:3148660]。", "problem": "一个团队正在为一个回归任务比较多项式核模型，该任务有 $p=20$ 个输入变量和 $n=500$ 个独立同分布的样本。多项式核对应于一个特征空间中的线性模型，该特征空间由原始输入中总次数最高为 $d$ 的所有单项式组成。该团队观察到，次数 $d=2$ 和次数 $d=4$ 的模型都达到了基本为零的训练误差，而测试均方误差对于 $d=2$ 约为 $0.08$，对于 $d=4$ 约为 $0.20$。\n\n从计数和统计学习的第一性原理出发，完成以下任务。\n\n1) 仅使用单项式的定义，即一个形如 $x_{1}^{\\alpha_{1}} x_{2}^{\\alpha_{2}} \\cdots x_{p}^{\\alpha_{p}}$ 的乘积，其中非负整数指数 $\\alpha_{j}$ 的总次数满足 $\\sum_{j=1}^{p} \\alpha_{j} \\le d$，推导出 $p$ 个变量中总次数最多为 $d$ 的不同单项式数量的封闭形式表达式。你的推导不能假设任何已有的计数公式，并且应基于一个有效的组合论证。\n\n2) 对于 $p=20$，$d=2$ 和 $d=4$，计算你在第（1）部分中得到的表达式的值。\n\n3) 在统计学习理论中，一个经过充分检验的事实是：对于具有范数约束的 $M$ 维有界特征空间中的线性预测器，基于诸如 Rademacher 复杂度等复杂性度量的数据无关泛化界与 $\\sqrt{M/n}$ 成正比。使用这种比例关系作为代理，计算此问题中泛化差距（次数 $d=4$ 除以次数 $d=2$）的预测比率 $r$。将 $r$ 表示为一个无单位的数字，并将你的答案四舍五入到三位有效数字。\n\n4) 利用第（2）部分中的计数结果，简要解释特征维数的变化与观察到的测试误差增加有何关系，以及这对模型的灵活性、容量和可解释性意味着什么。\n\n最终答案只需提交 $r$ 的数值。", "solution": "该问题经评估是有效的，因为它在科学上基于统计学习理论，问题提出得当，客观，并包含求解所需的所有信息。\n\n该问题要求完成四个任务：1) 推导单项式数量的公式，2) 对特定参数计算该公式的值，3) 基于给定的比例定律计算泛化差距的比率，以及 4) 对结果提供概念性解释。最终答案指定为仅提交第（3）部分的数值。\n\n**第1部分：单项式数量的推导**\n\n我们需要求出 $p$ 个变量 $x_1, x_2, \\dots, x_p$ 中，总次数最多为 $d$ 的不同单项式的数量。单项式的形式为 $x_{1}^{\\alpha_{1}} x_{2}^{\\alpha_{2}} \\cdots x_{p}^{\\alpha_{p}}$，其中指数 $\\alpha_j$ 是非负整数。条件是总次数 $\\sum_{j=1}^{p} \\alpha_{j}$ 小于或等于 $d$。\n$$ \\sum_{j=1}^{p} \\alpha_{j} \\le d, \\quad \\alpha_j \\in \\{0, 1, 2, \\dots\\} $$\n这个问题等价于计算上述不等式的非负整数解的数量。为了将这个不等式转换成一个更易于处理的等式，我们引入一个非负整数“松弛”变量 $\\alpha_{p+1}$。我们将 $\\alpha_{p+1}$ 定义为：\n$$ \\alpha_{p+1} = d - \\sum_{j=1}^{p} \\alpha_{j} $$\n由于 $\\sum_{j=1}^{p} \\alpha_{j} \\le d$，可以保证 $\\alpha_{p+1} \\ge 0$。原来的不等式现在完全等价于求下列方程的非负整数解的数量：\n$$ \\alpha_1 + \\alpha_2 + \\cdots + \\alpha_p + \\alpha_{p+1} = d $$\n这是一个经典的组合问题，可以按要求使用“隔板法”（stars and bars）来解决。想象我们有 $d$ 个相同的物品（星，$\\star$）要分配到 $p+1$ 个不同的箱子中（即变量 $\\alpha_1, \\dots, \\alpha_{p+1}$）。我们可以将 $d$ 个星排成一排，并用 $p$ 个隔板（$|$）将它们分成 $p+1$ 组，以此来表示这种排列。第 $j$ 组中星的数量对应于 $\\alpha_j$ 的值。\n\n例如，有 $d=3$ 个星和 $p=2$ 个变量（意味着有 $p+1=3$ 个箱子），排列 $\\star|\\star\\star|$ 对应于 $\\alpha_1=1$，$\\alpha_2=2$，以及松弛变量 $\\alpha_3=0$。这对应于次数为 $3$ 的单项式 $x_1^1 x_2^2$。排列 $| \\star | \\star\\star$ 对应于 $\\alpha_1=0$，$\\alpha_2=1$，以及 $\\alpha_3=2$。这意味着 $\\alpha_1+\\alpha_2 = 1 \\le 3$，代表次数为 $1$ 的单项式 $x_2^1$。\n\n为了计算这种排列的数量，我们考虑一个总共有 $d+p$ 个位置的序列。我们需要从这些位置中选择 $p$ 个来放置隔板（剩下的 $d$ 个位置将由星填充）。这样做的方法数由二项式系数“($d+p$) 选 $p$”给出。\n\n令 $M(p, d)$ 为单项式的数量。从 $d+p$ 个总位置中选择 $p$ 个位置放置隔板的方法数是：\n$$ M(p, d) = \\binom{p+d}{p} $$\n或者，我们可以选择 $d$ 个位置放置星，这给出了等价的表达式：\n$$ M(p, d) = \\binom{p+d}{d} = \\frac{(p+d)!}{d! p!} $$\n这就是在 $p$ 个变量中总次数最多为 $d$ 的不同单项式数量的封闭形式表达式。这个计数包括了常数项（次数为 $0$ 的单项式），它对应于解 $\\alpha_1 = \\alpha_2 = \\dots = \\alpha_p = 0$。\n\n**第2部分：表达式求值**\n\n给定 $p=20$ 个变量，我们需要为次数 $d=2$ 和 $d=4$ 的模型计算特征数量。\n\n对于 $d=2$，令特征数量为 $M_2$。\n$$ M_2 = M(20, 2) = \\binom{20+2}{2} = \\binom{22}{2} $$\n$$ M_2 = \\frac{22!}{2!(22-2)!} = \\frac{22!}{2!20!} = \\frac{22 \\times 21}{2 \\times 1} = 11 \\times 21 = 231 $$\n\n对于 $d=4$，令特征数量为 $M_4$。\n$$ M_4 = M(20, 4) = \\binom{20+4}{4} = \\binom{24}{4} $$\n$$ M_4 = \\frac{24!}{4!(24-4)!} = \\frac{24!}{4!20!} = \\frac{24 \\times 23 \\times 22 \\times 21}{4 \\times 3 \\times 2 \\times 1} $$\n由于 $4 \\times 3 \\times 2 \\times 1 = 24$，我们可以简化为：\n$$ M_4 = 23 \\times 22 \\times 21 = 506 \\times 21 = 10626 $$\n所以，次数为2的模型的特征数量是 $M_2 = 231$，次数为4的模型的特征数量是 $M_4 = 10626$。\n\n**第3部分：泛化差距比率的计算**\n\n我们被告知，对于一个 $M$ 维特征空间中的线性预测器，一个典型的数据无关泛化界与 $\\sqrt{M/n}$ 成正比。泛化差距 $G$ 是测试误差与训练误差之间的差值。我们被要求使用这种比例关系作为代理。\n令 $G_d$ 为次数为 $d$ 的模型的泛化差距。\n$$ G_d \\propto \\sqrt{\\frac{M_d}{n}} $$\n这意味着 $G_d = C \\sqrt{M_d/n}$，其中 $C$ 是一个比例常数，它依赖于与此比率无关的因素，例如权重的范数约束和数据分布的属性。\n\n泛化差距的预测比率 $r$ 是：\n$$ r = \\frac{G_4}{G_2} = \\frac{C \\sqrt{M_4/n}}{C \\sqrt{M_2/n}} $$\n常数 $C$ 和样本大小 $n$ 被消掉了：\n$$ r = \\sqrt{\\frac{M_4}{M_2}} $$\n使用第2部分计算的值：\n$$ r = \\sqrt{\\frac{10626}{231}} $$\n首先，我们简化分数：\n$$ \\frac{10626}{231} = 46 $$\n因此，比率 $r$ 是：\n$$ r = \\sqrt{46} $$\n将其表示为一个四舍五入到三位有效数字的数：\n$$ r \\approx 6.7823... $$\n四舍五入到三位有效数字得到 $r \\approx 6.78$。\n\n**第4部分：对观察到的测试误差增加的解释**\n\n特征数量 $M$ 是模型容量或灵活性的直接度量。具有更大 $M$ 的模型可以表示更复杂的函数类别。\n在这个问题中，我们有 $p=20$ 个输入和 $n=500$ 个样本。\n- 次数 $d=2$ 的模型有 $M_2=231$ 个特征。由于 $M_2  n$，这是一个可以实现良好拟合但不能保证完美插值的区域。\n- 次数 $d=4$ 的模型有 $M_4=10626$ 个特征。这里，$M_4 \\gg n$，使模型处于一个严重过参数化的区域。\n\n两个模型都达到“基本为零的训练误差”这一观察结果可以用它们的高容量来解释。对于500个数据点，$d=2$ 的模型有231个特征，已经非常灵活。而 $d=4$ 的模型处于过参数化区域（$M_4 > n$），其参数数量多于数据点，这通常使其能够插值训练数据，从而将训练误差降至零。\n\n然而，测试误差是衡量模型对新的、未见过数据的泛化能力的指标。它受到偏差-方差权衡的影响。\n- **灵活性和容量**：从 $d=2$ 变为 $d=4$，特征空间维度从 $M_2=231$ 增加到 $M_4=10626$，模型容量急剧增加。\n- **过拟合和方差**：虽然增加的容量有助于减少训练误差（并可能减少模型偏差），但代价是方差大大增加。$d=4$ 的模型拥有海量特征，它不仅拟合了训练数据中的潜在信号，还拟合了该样本特有的随机噪声。这就是过拟合。模型变得对训练集的特殊性高度敏感。\n- **测试误差**：测试均方误差从 $d=2$ 的 $0.08$ 增加到 $d=4$ 的 $0.20$，是过拟合的典型症状。$d=4$ 模型增加的方差主导了任何潜在的偏差减少，导致泛化性能变差。对于这个问题和数据集大小，$d=2$ 的模型代表了偏差和方差之间更好的平衡。\n- **可解释性**：可解释性与模型复杂度成反比。一个基于 $M_2=231$ 个特征的线性模型已经非常难以解释。而一个基于 $M_4=10626$ 个特征的模型，这些特征涉及最多四个原始变量的复杂乘积，在所有实际应用中都是一个“黑箱”。人类不可能通过分析数万个系数的单个效应来理解模型的决策过程。相比之下，更简单的 $d=2$ 模型更具可解释性。\n\n总之，从 $d=2$ 到 $d=4$ 的转变极大地增加了模型容量，导致了严重的过拟合，表现为由于高方差而导致的测试误差增加，以及模型可解释性的急剧下降。", "answer": "$$\n\\boxed{6.78}\n$$", "id": "3148660"}, {"introduction": "在选择模型的复杂度时，我们不必局限于固定的模型结构，而是可以采用自适应的方法。这项练习将展示一种强大的、由数据驱动的策略来逐步增加模型的灵活性。通过在模型误差最大的地方迭代地添加“节点”(knots)，你将学会如何构建一个能够贴合数据局部复杂性的回归样条模型，从而获得对模型灵活性如何运作的直观感受 [@problem_id:3157197]。", "problem": "您需要为一个回归样条模型实现一个自适应节点放置的算法程序，并报告模型复杂度和误差的演变。请从以下基本概念开始：普通最小二乘 (OLS) 回归最小化残差平方和，分段多项式通过截断幂基表示，样本的残差是观测值与拟合值之间的差。除了这些基本定义之外，您不得使用任何快捷公式。\n\n考虑一个一维输入域，样本为 $\\{(x_i, y_i)\\}_{i=1}^n$，其中 $x_i \\in [0,1]$ 且 $y_i \\in \\mathbb{R}$。一个带有节点 $\\{t_j\\}_{j=1}^K$ 的 $m$ 次分段多项式，可以使用截断幂基表示为函数 $1, x, x^2, \\dots, x^m$ 以及每个节点 $t_j$ 对应的 $(x - t_j)_+^m$ 的线性组合，其中 $(u)_+ = \\max(u, 0)$。对于三次样条 ($m = 3$)，这产生一个模型\n$$\n\\hat{y}(x) = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\beta_3 x^3 + \\sum_{j=1}^K \\gamma_j (x - t_j)_+^3.\n$$\n在 OLS 中，选择系数以最小化\n$$\n\\sum_{i=1}^n \\left( y_i - \\hat{y}(x_i) \\right)^2.\n$$\n将残差定义为 $r_i = y_i - \\hat{y}(x_i)$，均方根误差 (RMSE) 定义为\n$$\n\\mathrm{RMSE} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^n r_i^2}.\n$$\n将拟合模型的自由度 (df) 定义为估计系数的数量，对于三次截断幂基，该值为 $4 + K$（$1, x, x^2, x^3$ 各一个，每个节点一个）。\n\n实现以下多分辨率节点添加算法：\n- 初始化时没有节点 ($K = 0$)，使用 OLS 对基 $\\{1, x, x^2, x^3\\}$ 拟合三次模型。\n- 在每次迭代 $k = 1, 2, \\dots$ 中，计算当前残差 $\\{r_i\\}_{i=1}^n$，找出使 $|r_{i^\\star}|$ 最大化的索引 $i^\\star$（通过选择最小的索引来打破平局），并在 $t_k = x_{i^\\star}$ 处提出一个新节点。\n- 如果 $t_k$ 已经在节点集中，则跳到下一个尚未使用的最大 $|r_i|$ 处；如果所有 $x_i$ 都已经是节点，则终止。\n- 添加节点 $t_k$，重新拟合包含新基函数 $(x - t_k)_+^3$ 的模型，然后记录更新后的 $df$ 和 $\\mathrm{RMSE}$。\n- 持续此过程，直到达到指定的最大节点数，或者直到添加一个节点会使基函数数量超过 $n$ (以避免退化)，即如果 $4 + K \\ge n$，则终止。\n\n每个测试用例的数据生成必须遵循：\n- 在 $[0,1]$ 中均匀抽取 $n$ 个点 $x_i$。\n- 使用确定性函数 $f(x) = \\sin(2\\pi x) + \\frac{1}{2} x$。\n- 生成噪声 $\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma^2)$ 并设置 $y_i = f(x_i) + \\varepsilon_i$。\n- 为保证可复现性，使用指定的随机种子。\n\n您的程序必须实现此算法，并为每个测试用例报告从迭代 $k=0$（无节点）到最终迭代的 $df$ 和 $\\mathrm{RMSE}$ 值序列。每个测试用例的输出必须是一个包含两个列表的列表：$df$ 值列表和 $\\mathrm{RMSE}$ 值列表，两者均按迭代排序。将所有测试用例的结果聚合到一个列表中。\n\n测试套件：\n- 案例 1: $n = 100$, $\\sigma = 0.1$, 最大节点数 $= 5$, 种子 $= 0$ (一般情况)。\n- 案例 2: $n = 50$, $\\sigma = 0$, 最大节点数 $= 3$, 种子 $= 1$ (无噪声边界情况)。\n- 案例 3: $n = 200$, $\\sigma = 0.3$, 最大节点数 $= 0$, 种子 $= 2$ (零迭代边界情况)。\n- 案例 4: $n = 30$, $\\sigma = 0.05$, 最大节点数 $= 25$, 种子 $= 3$ (包含许多潜在节点的压力测试；确保根据需要通过 $df \\ge n$ 规则终止)。\n\n最终输出格式：\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果，每个元素是按顺序排列的一个测试用例的列表对：\n$[ [\\text{df\\_sequence\\_case1}, \\text{rmse\\_sequence\\_case1}], [\\text{df\\_sequence\\_case2}, \\text{rmse\\_sequence\\_case2}], [\\text{df\\_sequence\\_case3}, \\text{rmse\\_sequence\\_case3}], [\\text{df\\_sequence\\_case4}, \\text{rmse\\_sequence\\_case4}] ]$.\n所有数字必须是标准的浮点数或整数字面量，不得打印任何附加文本。", "solution": "### 问题验证\n\n**步骤 1：提取给定信息**\n\n问题提供了以下数据、定义和程序：\n-   **数据样本**: $\\{(x_i, y_i)\\}_{i=1}^n$ 其中 $x_i \\in [0,1]$ 且 $y_i \\in \\mathbb{R}$。\n-   **模型**: 使用截断幂基表示的三次分段样条 ($m=3$)。模型函数为 $\\hat{y}(x) = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\beta_3 x^3 + \\sum_{j=1}^K \\gamma_j (x - t_j)_+^3$，其中 $\\{t_j\\}_{j=1}^K$ 是节点，$(u)_+ = \\max(u, 0)$。\n-   **拟合目标**: 使用普通最小二乘法 (OLS) 最小化残差平方和，即最小化 $\\sum_{i=1}^n \\left( y_i - \\hat{y}(x_i) \\right)^2$。\n-   **度量指标**:\n    -   残差: $r_i = y_i - \\hat{y}(x_i)$。\n    -   均方根误差 (RMSE): $\\mathrm{RMSE} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^n r_i^2}$。\n    -   自由度 (df): $df = 4 + K$。\n-   **节点添加算法**:\n    1.  **初始化**: 从 $K=0$ 个节点开始。拟合全局三次模型。\n    2.  **迭代**: 对于 $k=1, 2, \\dots$:\n        -   计算残差 $\\{r_i\\}$。\n        -   找到使 $|r_{i^\\star}|$ 最大化的索引 $i^\\star$。平局通过选择最小的索引来打破。\n        -   提出一个新节点 $t_k = x_{i^\\star}$。\n        -   如果 $t_k$ 已经是一个节点，则使用下一个尚未使用的最大 $|r_i|$ 对应的 $x_i$。\n        -   添加节点 $t_k$，重新拟合模型，并记录新的 $df$ 和 $\\mathrm{RMSE}$。\n-   **终止条件**: 算法在满足以下任一条件时终止：\n    1.  达到指定的最大节点数。\n    2.  添加一个新节点将导致基函数数量大于或等于样本数量，即 $4 + (K_{current}+1) \\ge n$。\n    3.  所有唯一的样本位置 $x_i$ 都已被用作节点。\n-   **数据生成**:\n    -   $x_i \\sim U([0,1])$ 对于 $i=1, \\dots, n$。\n    -   真实函数: $f(x) = \\sin(2\\pi x) + \\frac{1}{2} x$。\n    -   观测值: $y_i = f(x_i) + \\varepsilon_i$，其中 $\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma^2)$。\n    -   为保证可复现性，提供了特定的随机种子。\n-   **测试套件**:\n    -   案例 1: $n = 100$, $\\sigma = 0.1$, 最大节点数 $= 5$, 种子 $= 0$。\n    -   案例 2: $n = 50$, $\\sigma = 0$, 最大节点数 $= 3$, 种子 $= 1$。\n    -   案例 3: $n = 200$, $\\sigma = 0.3$, 最大节点数 $= 0$, 种子 $= 2$。\n    -   案例 4: $n = 30$, $\\sigma = 0.05$, 最大节点数 $= 25$, 种子 $= 3$。\n\n**步骤 2：使用提取的给定信息进行验证**\n\n-   **科学依据**：该问题牢固地植根于统计学习和非参数回归领域。回归样条、截断幂基、普通最小二乘法和前向逐步模型选择都是标准的、成熟的概念。该程序是一种有效且具体的自适应模型拟合方法。\n-   **适定性**：该问题是适定的。数据生成过程被完全指定。算法是确定性的，包括初始化、迭代、节点选择中的平局打破以及终止的明确规则。这确保了对于给定的测试用例，存在唯一的模型序列和相应的度量。终止条件 $4+K \\ge n$ 防止了 OLS 拟合中的线性系统变得欠定，确保了在每一步都能找到有意义的解。\n-   **客观性**：该问题以精确、客观的数学和算法语言陈述。没有主观性陈述或含糊之处。\n\n问题陈述没有表现出任何缺陷，如科学上不健全、不完整、矛盾或不可行。它是一个在应用数学中定义明确的计算问题。\n\n**步骤 3：结论与行动**\n\n该问题是**有效的**。将提供一个解决方案。\n\n### 解决方案\n\n该问题要求实现一个前向逐步算法来构建回归样条模型。其核心思想是在现有模型拟合最差的位置迭代地添加节点，从而有针对性地增加模型的灵活性。\n\n指定的模型是一个带有 $K$ 个节点的三次样条，可以表示为基函数的线性组合：\n$$\n\\hat{y}(x) = \\sum_{j=0}^{3} \\beta_j x^j + \\sum_{j=1}^{K} \\gamma_j (x - t_j)_+^3\n$$\n这是一个关于系数 $(\\beta_0, \\beta_1, \\beta_2, \\beta_3, \\gamma_1, \\dots, \\gamma_K)$ 的线性模型。给定一组数据点 $\\{(x_i, y_i)\\}_{i=1}^n$ 和一组节点 $\\{t_j\\}_{j=1}^K$，我们可以构建一个 $n \\times (4+K)$ 的设计矩阵 $\\mathbf{X}$。$\\mathbf{X}$ 的每一行 $i$ 对应一个数据点 $x_i$，每一列对应一个基函数。具体来说，第 $i$ 行是：\n$$\n\\mathbf{X}_{i,:} = [1, x_i, x_i^2, x_i^3, (x_i - t_1)_+^3, \\dots, (x_i - t_K)_+^3]\n$$\n所有系数的向量是 $\\mathbf{b} = [\\beta_0, \\beta_1, \\beta_2, \\beta_3, \\gamma_1, \\dots, \\gamma_K]^T$。预测值的向量则由 $\\hat{\\mathbf{y}} = \\mathbf{X}\\mathbf{b}$ 给出。\n\n根据普通最小二乘法 (OLS) 的原理，最优系数向量 $\\mathbf{b}$ 是使残差平方和 (SSR) 最小化的向量，SSR 定义为 $SSR = ||\\mathbf{y} - \\hat{\\mathbf{y}}||_2^2 = ||\\mathbf{y} - \\mathbf{X}\\mathbf{b}||_2^2$。这是一个标准的线性代数问题，其解可以通过数值稳定的方法（如 QR 分解）找到，例如 `numpy.linalg.lstsq` 中所实现的。\n\n算法流程如下：\n\n1.  **初始化 ($k=0$)**: 我们从一个空的节点集开始，$K=0$。模型是一个简单的全局三次多项式 $\\hat{y}(x) = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\beta_3 x^3$。设计矩阵 $\\mathbf{X}$ 有 4 列。我们求解初始系数，计算残差 $r_i = y_i - \\hat{y}(x_i)$，然后计算初始自由度 ($df=4$) 和 RMSE。这些初始值构成了我们结果序列中的第一个条目。\n\n2.  **迭代添加节点 (对于 $k = 1, 2, \\dots$)**:\n    a.  **终止检查**: 在添加新节点之前，我们检查两个终止条件。首先，如果已达到最大节点数，则过程停止。其次，为防止系统欠定，我们检查添加另一个节点是否会使基函数的数量 ($4 + K_{current} + 1$) 大于或等于数据点的数量 $n$。如果是，则终止。\n    b.  **节点选择**: 自适应策略的核心在于选择下一个节点。我们根据当前模型的拟合计算绝对残差 $|r_i|$。与最大绝对残差相对应的位置 $x_{i^\\star}$ 被选为新的候选节点。这种贪心策略将下一个节点放置在模型误差最大的地方。问题规定， $|r_i|$ 的平局应通过选择索引 $i$ 最小的观测值来打破。此外，我们必须确保新节点不是现有节点的副本。我们按照相应 $|r_i|$ 的降序搜索 $x_i$，直到找到一个唯一的节点位置。如果所有潜在位置 $x_i$ 都已在节点集中，则算法终止。\n    c.  **模型重新拟合**: 一旦选定一个新节点 $t_k$，就将其添加到节点集中。设计矩阵 $\\mathbf{X}$ 会增加一个代表新基函数 $(x_i - t_k)_+^3$ 的新列。然后，使用 OLS 将整个包含 $4+K$ 个系数的模型重新拟合到数据上。\n    d.  **记录度量**: 重新拟合后，计算更新后的自由度 ($df=4+K$) 和新的（通常更低的）RMSE，并将它们附加到各自的序列中。\n\n这个迭代过程会一直持续，直到满足其中一个终止条件。该程序生成了一系列复杂度递增 ($df$) 的模型及其相应的拟合优度 (RMSE)，从而揭示了模型复杂度和预测误差之间的权衡。对于每个测试用例，我们将按照规定生成数据并执行此算法，收集每一步（包括初始的 $K=0$ 步）的 `df` 和 `RMSE`。", "answer": "```python\nimport numpy as np\n\ndef adaptive_knot_placement(n: int, sigma: float, max_knots: int, seed: int) - list:\n    \"\"\"\n    Implements the adaptive knot placement algorithm for regression splines.\n\n    Args:\n        n (int): Number of data points.\n        sigma (float): Standard deviation of the Gaussian noise.\n        max_knots (int): Maximum number of knots to add.\n        seed (int): Random seed for reproducibility.\n\n    Returns:\n        list: A list containing two lists: the sequence of degrees of freedom (df)\n              and the sequence of Root Mean Squared Errors (RMSE).\n    \"\"\"\n    # 1. Data Generation\n    rng = np.random.default_rng(seed)\n    x = rng.uniform(0, 1, n)\n    f_x = np.sin(2 * np.pi * x) + 0.5 * x\n    epsilon = rng.normal(0, sigma, n)\n    y = f_x + epsilon\n\n    # Initialization\n    knots = []\n    df_sequence = []\n    rmse_sequence = []\n\n    # 2. Initial Fit (k=0, no knots)\n    K = 0\n    # Design matrix: columns for 1, x, x^2, x^3\n    X_poly = np.power(x[:, np.newaxis], np.arange(4))\n    X = X_poly\n\n    # Solve OLS\n    coeffs = np.linalg.lstsq(X, y, rcond=None)[0]\n    y_hat = X @ coeffs\n    residuals = y - y_hat\n    rmse = np.sqrt(np.mean(residuals**2))\n    df = 4 + K\n    \n    df_sequence.append(df)\n    rmse_sequence.append(rmse)\n\n    # 3. Iterative Knot Addition\n    for k in range(1, max_knots + 1):\n        # Termination condition: model would become degenerate\n        if 4 + len(knots) + 1 = n:\n            break\n\n        # Find new knot\n        # Sort indices by descending |residual|, with ascending index as tie-breaker\n        sort_keys = (np.arange(n), -np.abs(residuals))\n        sorted_indices = np.lexsort(sort_keys)\n        \n        new_knot = None\n        knot_found = False\n        knot_set = set(knots) # Use a set for efficient checking\n        for idx in sorted_indices:\n            candidate_knot = x[idx]\n            if candidate_knot not in knot_set:\n                new_knot = candidate_knot\n                knot_found = True\n                break\n        \n        # Termination condition: no new unique knots can be added\n        if not knot_found:\n            break\n\n        # Add knot and prepare for refitting\n        knots.append(new_knot)\n        K = len(knots)\n\n        # Re-build design matrix\n        new_basis_col = np.maximum(0, x - new_knot)**3\n        X = np.hstack([X, new_basis_col[:, np.newaxis]])\n\n        # Refit model\n        coeffs = np.linalg.lstsq(X, y, rcond=None)[0]\n        y_hat = X @ coeffs\n        residuals = y - y_hat\n        rmse = np.sqrt(np.mean(residuals**2))\n        df = 4 + K\n\n        df_sequence.append(df)\n        rmse_sequence.append(rmse)\n        \n    return [df_sequence, rmse_sequence]\n\ndef solve():\n    \"\"\"\n    Runs the test suite and prints the final results in the specified format.\n    \"\"\"\n    test_cases = [\n        # (n, sigma, max_knots, seed)\n        (100, 0.1, 5, 0),\n        (50, 0.0, 3, 1),\n        (200, 0.3, 0, 2),\n        (30, 0.05, 25, 3),\n    ]\n\n    all_results = []\n    for case in test_cases:\n        n, sigma, max_knots, seed = case\n        result = adaptive_knot_placement(n, sigma, max_knots, seed)\n        all_results.append(result)\n\n    # Format the final output string to be a compact list representation\n    # e.g., [[[df1s],[rmse1s]],[[df2s],[rmse2s]]]\n    # repr() gives a standard representation, and .replace(\" \", \"\") removes whitespace.\n    output_str = repr(all_results).replace(\" \", \"\")\n    print(output_str)\n\nsolve()\n```", "id": "3157197"}, {"introduction": "正则化是管理模型灵活性与可解释性之间权衡的主要工具。本练习不仅探讨$L_2$正则化如何控制模型复杂度以改善泛化能力，更关注其如何稳定模型的系数。通过追踪特征系数随正则化强度变化的路径，你将深入理解模型稳定性及其对解释可靠性的影响 [@problem_id:3148628]。", "problem": "您的任务是分析逻辑回归系数的符号如何随着正则化强度的变化而变化，并将此行为与模型的灵活性、容量和可解释性联系起来。考虑在经验风险最小化（ERM）框架下，使用平方欧几里得范数（也称为L2）惩罚项的二元分类逻辑回归。模型参数是一个权重向量 $w \\in \\mathbb{R}^d$ 和一个截距 $b \\in \\mathbb{R}$，预测值为 $p_i = \\sigma(z_i)$，其中 $z_i = w^\\top x_i + b$，$\\sigma(t) = \\frac{1}{1 + e^{-t}}$。对于一个数据集 $\\{(x_i, y_i)\\}_{i=1}^n$（其中 $y_i \\in \\{0,1\\}$），给定正则化参数 $\\lambda \\ge 0$ 的正则化目标函数为\n$$\nJ_\\lambda(w,b) = \\frac{1}{n} \\sum_{i=1}^n \\left( \\log\\left(1 + e^{z_i}\\right) - y_i z_i \\right) + \\frac{\\lambda}{2} \\|w\\|_2^2,\n$$\n其中 $\\|w\\|_2$ 是欧几里得范数，截距 $b$ 不受惩罚。\n\n将使用的定义：\n- 带有容差 $\\varepsilon  0$ 的系数符号由函数 $S_\\varepsilon:\\mathbb{R}\\to\\{-1,0,1\\}$ 定义，\n$$\nS_\\varepsilon(u) =\n\\begin{cases}\n-1,  \\text{若 } u  -\\varepsilon, \\\\\n0,  \\text{若 } |u| \\le \\varepsilon, \\\\\n1,  \\text{若 } u  \\varepsilon.\n\\end{cases}\n$$\n- 给定一个递增排序的正则化参数网格 $\\Lambda = \\{\\lambda_1, \\lambda_2, \\dots, \\lambda_m\\}$，对于特征索引 $j \\in \\{1, \\dots, d\\}$，整个路径上的符号变化计数为\n$$\nC_j = \\sum_{k=1}^{m-1} \\mathbf{1}\\left\\{ S_\\varepsilon\\big(w_j(\\lambda_k)\\big) \\ne S_\\varepsilon\\big(w_j(\\lambda_{k+1})\\big) \\right\\},\n$$\n其中 $\\mathbf{1}\\{\\cdot\\}$ 表示指示函数，$w_j(\\lambda)$ 是在正则化参数为 $\\lambda$ 时拟合的特征 $j$ 的系数。\n- 路径上每个特征的稳定性是没有符号变化的相邻对的比例：\n$$\n\\text{stab}_j = \\frac{1}{m-1} \\sum_{k=1}^{m-1} \\mathbf{1}\\left\\{ S_\\varepsilon\\big(w_j(\\lambda_k)\\big) = S_\\varepsilon\\big(w_j(\\lambda_{k+1})\\big) \\right\\}。\n$$\n- 总体稳定性指数是所有特征稳定性的平均值：\n$$\n\\text{Stability} = \\frac{1}{d} \\sum_{j=1}^d \\text{stab}_j.\n$$\n\n从逻辑模型、其损失函数和正则化惩罚项的基本定义出发，推导并实现一个正则化目标 $J_\\lambda(w,b)$ 的求解器，该求解器使用基于梯度的优化方法，为每个 $\\lambda \\in \\Lambda$ 拟合 $(w(\\lambda), b(\\lambda))$。然后，为下述每个测试案例计算每个特征 $j$ 的 $C_j$ 和总体稳定性 Stability。\n\n正则化网格固定为 $\\Lambda = \\{0.001, 0.01, 0.1, 1.0, 10.0, 100.0\\}$，容差为 $\\varepsilon = 10^{-6}$，并且截距 $b$ 不得被惩罚。\n\n测试套件（三个案例）以确保覆盖范围：\n- 案例A（通用，信号清晰）：使用种子 $0$ 生成数据。设 $n=120$, $d=3$。从标准正态分布中独立抽取 $X \\in \\mathbb{R}^{n \\times d}$ 的元素。定义真实参数 $w^* = (3, -2, 0)$ 和 $b^* = 0$。对每个 $i$，计算 $z_i^* = x_i^\\top w^* + b^*$，如果 $z_i^* \\ge 0$ 则设 $y_i = 1$，否则设 $y_i = 0$。\n- 案例B（高度共线性，归因模糊）：使用种子 $1$ 生成数据。设 $n=100$, $d=3$。从标准正态分布中抽取 $x^{(1)} \\in \\mathbb{R}^n$。创建 $x^{(2)} = x^{(1)} + 0.01 \\cdot \\xi$，其中 $\\xi$ 是具有相同种子序列的标准正态分布。独立地从标准正态分布中抽取 $x^{(3)}$。通过堆叠 $(x^{(1)}, x^{(2)}, x^{(3)})$ 形成 $X$。定义 $w^* = (1, -1, 0)$, $b^* = 0$。对每个 $i$，如果 $x_i^{(1)} - x_i^{(2)} \\ge 0$ 则设 $y_i = 1$，否则设 $y_i = 0$。\n- 案例C（低信噪比，正则化主导）：使用种子 $2$ 生成数据。设 $n=200$, $d=3$。将 $X$ 抽取为标准正态分布。定义 $w^* = (0.2, -0.2, 0)$ 和 $b^* = 0$。从具有相同种子的标准正态分布中抽取独立噪声 $\\eta \\in \\mathbb{R}^n$。计算 $z_i^* = x_i^\\top w^* + b^* + 0.1 \\eta_i$，如果 $z_i^* \\ge 0$ 则设 $y_i = 1$，否则设 $y_i = 0$。\n\n程序要求：\n- 通过使用带有精确梯度的基于梯度的优化器最小化 $J_\\lambda(w,b)$，为每个 $\\lambda \\in \\Lambda$ 拟合 $(w(\\lambda), b(\\lambda))$；确保截距 $b$ 不被惩罚。\n- 对每个测试案例，计算 $d=3$ 个特征在 $\\Lambda$ 上的符号变化计数列表 $[C_1, C_2, C_3]$，以及如上定义的总体稳定性 Stability，表示为小数点后保留三位的小数。\n- 最终输出格式：您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，每个元素对应一个测试案例，并且本身是一个形式为 $[[C_1,C_2,C_3],\\text{Stability}]$ 的双元素列表。例如，最终打印的行必须看起来像 $[[[c_{A1},c_{A2},c_{A3}],s_A],[[c_{B1},c_{B2},c_{B3}],s_B],[[c_{C1},c_{C2},c_{C3}],s_C]]$，所有 $c$ 值为整数，所有 $s$ 值为保留三位小数的小数。\n\n不涉及物理单位或角度，也不应使用百分比。所有数值输出必须是指定格式的整数或小数。必须完全按照描述生成数据集以确保可复现性。实现不得要求任何外部输入，并且必须按原样运行。", "solution": "用户希望分析逻辑回归系数在不同L2正则化强度下的稳定性。这涉及到实现一个逻辑回归求解器，生成三个不同的数据集，对一系列正则化参数（$\\lambda$）进行模型拟合，然后量化系数符号在该正则化路径上的稳定性。\n\n### 1. 数学公式\n\n问题是最小化逻辑回归的正则化目标函数：\n$$\nJ_\\lambda(w,b) = \\frac{1}{n} \\sum_{i=1}^n \\left( \\log\\left(1 + e^{w^\\top x_i + b}\\right) - y_i (w^\\top x_i + b) \\right) + \\frac{\\lambda}{2} \\|w\\|_2^2\n$$\n该目标函数是凸函数，确保对于任何 $\\lambda  0$，基于梯度的优化方法都能找到唯一的全局最小值。为了使用此类优化器，我们需要 $J_\\lambda$ 相对于参数 $w$ 和 $b$ 的梯度。\n\n令 $z_i = w^\\top x_i + b$。sigmoid函数为 $\\sigma(z_i) = p_i = \\frac{1}{1 + e^{-z_i}}$。\n单个数据点的损失项相对于 $z_i$ 的梯度为：\n$$\n\\frac{\\partial}{\\partial z_i} \\left( \\log(1 + e^{z_i}) - y_i z_i \\right) = \\frac{e^{z_i}}{1 + e^{z_i}} - y_i = \\sigma(z_i) - y_i = p_i - y_i\n$$\n使用链式法则，我们可以找到相对于 $w$ 的每个分量和 $b$ 的梯度：\n$$\n\\frac{\\partial z_i}{\\partial w_j} = x_{ij} \\quad \\text{和} \\quad \\frac{\\partial z_i}{\\partial b} = 1\n$$\n目标函数 $J_\\lambda$ 相对于权重 $w_j$ 的梯度是：\n$$\n\\nabla_{w_j} J_\\lambda = \\frac{1}{n} \\sum_{i=1}^n \\frac{\\partial J_\\lambda}{\\partial z_i} \\frac{\\partial z_i}{\\partial w_j} + \\frac{\\partial}{\\partial w_j} \\left( \\frac{\\lambda}{2} \\|w\\|_2^2 \\right) = \\frac{1}{n} \\sum_{i=1}^n (p_i - y_i)x_{ij} + \\lambda w_j\n$$\n以向量形式，相对于整个权重向量 $w$ 的梯度是：\n$$\n\\nabla_w J_\\lambda = \\frac{1}{n} X^\\top (p - y) + \\lambda w\n$$\n其中 $p$ 是预测概率向量，$y$ 是真实标签向量。\n\n截距 $b$ 不受惩罚。其梯度是：\n$$\n\\nabla_b J_\\lambda = \\frac{1}{n} \\sum_{i=1}^n (p_i - y_i)\n$$\n\n这些精确梯度将被提供给一个拟牛顿优化算法（L-BFGS-B），以找到每个指定网格中的 $\\lambda$ 所对应的最优 $(w(\\lambda), b(\\lambda))$。\n\n### 2. 实现策略\n\n该解决方案使用Python实现，遵循指定的环境。每个测试案例的总体流程如下：\n\n1.  **数据生成**：对于三个测试案例（A、B、C）中的每一个，根据问题的规则生成一个特定的数据集 $(X, y)$，使用 `numpy.random.default_rng` 并指定种子以确保可复现性。\n\n2.  **模型拟合**：一个循环遍历固定的正则化网格 $\\Lambda = \\{0.001, 0.01, 0.1, 1.0, 10.0, 100.0\\}$。\n    *   对于每个 $\\lambda \\in \\Lambda$，将目标函数 $J_\\lambda(w,b)$ 及其梯度传递给 `scipy.optimize.minimize`，并使用 'L-BFGS-B' 方法。待优化的参数是连接 $w$ 和 $b$ 的单个向量。\n    *   实现使用了“热启动”：为某个 $\\lambda_k$ 找到的解 $(w, b)$ 被用作在下一个值 $\\lambda_{k+1}$ 进行优化时的初始猜测。这提高了效率，因为相邻 $\\lambda$ 值的解通常很接近。\n    *   通过使用 `log-sum-exp` 技巧处理损失项和使用 `scipy.special.expit` 作为稳定的sigmoid函数来确保数值稳定性。\n\n3.  **指标计算**：在为所有 $\\lambda$ 值拟合模型后，分析得到的系数路径（一个 $w$ 向量序列）。\n    *   使用给定的符号函数 $S_\\varepsilon(u)$ 和 $\\varepsilon = 10^{-6}$ 来确定每个系数 $w_j(\\lambda)$ 的符号。\n    *   通过比较路径中相邻 $\\lambda$ 值的系数符号，计算每个特征 $j$ 的符号变化次数 $C_j$。\n    *   `Stability` 指数计算为每个特征稳定性的平均值 $\\text{stab}_j$，其中 $\\text{stab}_j$ 是 $w_j$ 符号不发生变化的相邻 $\\lambda$ 对的比例。这等价于 $\\text{stab}_j = 1 - C_j / (m-1)$，其中 $m$ 是 $\\lambda$ 网格中的点数。\n\n4.  **输出格式化**：将每个测试案例的结果（包括符号变化计数列表 $[C_1, C_2, C_3]$ 和总体 `Stability`（四舍五入到小数点后三位））格式化为精确的字符串表示，不含多余空格，如规范所示。最终输出是包含这些结果列表的单行。\n\n### 3. 测试案例的预期行为\n\n*   **案例A（信号清晰）**：由于特征1和2具有强的底层系数，它们的符号应该被正确识别并保持稳定，直到高正则化将它们压缩到零。特征3没有信号，所以其系数应保持在零附近。我们预期符号变化计数较低，稳定性较高。\n*   **案例B（高度共线性）**：特征1和2几乎相同（$x^{(2)} \\approx x^{(1)}$），而结果取决于它们的差（$x^{(1)} - x^{(2)}$）。这造成了模糊性，因为许多满足 $w_1 \\approx -w_2$ 的 $w_1$ 和 $w_2$ 组合都可以产生良好的拟合效果。L2惩罚将选择一个唯一的解，但系数路径可能不如案例A平滑，可能导致 $w_1$ 和 $w_2$ 的不稳定性增加和更多的符号变化。\n*   **案例C（低信噪比）**：真实系数很小，并且在线性预测器中加入了显著的噪声。模型可能难以从噪声中区分出微弱的信号，尤其是在低正则化时。随着 $\\lambda$ 的增加，惩罚项将迅速主导，将已经很小的系数压缩到零。这种情况预计将表现出最大的不稳定性，稳定性得分最低。", "answer": "```python\nimport numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.special import expit\n\ndef solve():\n    \"\"\"\n    Solves the logistic regression regularization path problem for three test cases.\n    \"\"\"\n\n    # --- Constants from the problem statement ---\n    LAMBDA_GRID = np.array([0.001, 0.01, 0.1, 1.0, 10.0, 100.0])\n    EPSILON = 1e-6\n\n    # --- Nested Helper Functions ---\n    def _generate_data(case_spec):\n        \"\"\"Generates dataset (X, y) based on the case specification.\"\"\"\n        case_id = case_spec['id']\n        seed = case_spec['seed']\n        n = case_spec['n']\n        d = case_spec['d']\n        rng = np.random.default_rng(seed)\n\n        if case_id == 'A':\n            w_star = np.array(case_spec['w_star'])\n            b_star = case_spec['b_star']\n            X = rng.standard_normal(size=(n, d))\n            z_star = X @ w_star + b_star\n            y = (z_star = 0).astype(int)\n        elif case_id == 'B':\n            # Collinear features\n            x1 = rng.standard_normal(size=n)\n            xi = rng.standard_normal(size=n)\n            x2 = x1 + 0.01 * xi\n            x3 = rng.standard_normal(size=n)\n            X = np.stack([x1, x2, x3], axis=1)\n            y = ((x1 - x2) = 0).astype(int)\n        elif case_id == 'C':\n            w_star = np.array(case_spec['w_star'])\n            b_star = case_spec['b_star']\n            noise_std = case_spec['noise_std']\n            X = rng.standard_normal(size=(n, d))\n            eta = rng.standard_normal(size=n)\n            z_star = X @ w_star + b_star + noise_std * eta\n            y = (z_star = 0).astype(int)\n        \n        return X, y\n\n    def _objective_and_grad(params, X, y, lambda_val):\n        \"\"\"Computes the objective function J and its gradient.\"\"\"\n        n, d = X.shape\n        w = params[:d]\n        b = params[d]\n\n        z = X @ w + b\n\n        # Numerically stable calculation of log-likelihood part\n        log_1_plus_exp_z = np.maximum(0, z) + np.log(1 + np.exp(-np.abs(z)))\n        loss = np.mean(log_1_plus_exp_z - y * z)\n\n        # L2 regularization term (only on w)\n        reg_term = (lambda_val / 2.0) * np.dot(w, w)\n        J = loss + reg_term\n\n        # Gradient calculation\n        p = expit(z)\n        p_minus_y = p - y\n        grad_w = (1.0 / n) * X.T @ p_minus_y + lambda_val * w\n        grad_b = np.mean(p_minus_y)\n        grad = np.append(grad_w, grad_b)\n        \n        return J, grad\n\n    def _s_epsilon_vec(u_vec, epsilon):\n        \"\"\"Vectorized sign function with tolerance.\"\"\"\n        return np.select([u_vec  -epsilon, u_vec  epsilon], [-1, 1], default=0)\n\n    def _calculate_metrics(w_path, m, d, epsilon):\n        \"\"\"Calculates sign-change counts and stability.\"\"\"\n        w_path_np = np.array(w_path)\n        sign_path = _s_epsilon_vec(w_path_np, epsilon)\n        \n        # Sign-change count C_j\n        sign_diffs = np.diff(sign_path, axis=0)\n        sign_changes = (sign_diffs != 0).astype(int)\n        c_counts = np.sum(sign_changes, axis=0)\n        \n        # Stability\n        num_transitions = m - 1\n        if num_transitions == 0:\n            return c_counts, 1.0\n        \n        stab_j = (num_transitions - c_counts) / num_transitions\n        overall_stability = np.mean(stab_j)\n        \n        return c_counts, overall_stability\n\n    # --- Main Logic ---\n    test_cases = [\n        {'id': 'A', 'seed': 0, 'n': 120, 'd': 3, 'w_star': [3, -2, 0], 'b_star': 0},\n        {'id': 'B', 'seed': 1, 'n': 100, 'd': 3},\n        {'id': 'C', 'seed': 2, 'n': 200, 'd': 3, 'w_star': [0.2, -0.2, 0], 'b_star': 0, 'noise_std': 0.1},\n    ]\n\n    all_results_str = []\n\n    for case_spec in test_cases:\n        X, y = _generate_data(case_spec)\n        d = X.shape[1]\n        \n        w_path = []\n        # Initial guess for the optimization (cold start for the first lambda)\n        params_0 = np.zeros(d + 1)\n        \n        for lambda_val in LAMBDA_GRID:\n            result = minimize(\n                fun=_objective_and_grad,\n                x0=params_0,\n                args=(X, y, lambda_val),\n                method='L-BFGS-B',\n                jac=True\n            )\n            # Use the current solution as a warm start for the next iteration\n            params_0 = result.x\n            w_path.append(result.x[:d])\n\n        c_counts, stability = _calculate_metrics(w_path, len(LAMBDA_GRID), d, EPSILON)\n        \n        c_counts_list = list(c_counts.astype(int))\n        \n        # Format the result string for this case\n        result_str = f\"[[{','.join(map(str, c_counts_list))}],{stability:.3f}]\"\n        all_results_str.append(result_str)\n\n    # Final print statement in the exact required format\n    print(f\"[{','.join(all_results_str)}]\")\n\nsolve()\n```", "id": "3148628"}]}