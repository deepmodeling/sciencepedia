{"hands_on_practices": [{"introduction": "在实践中，预测变量与响应变量之间的关系并非总是线性的。对特征进行非线性变换，例如Box-Cox变换，通常可以使这种关系更接近线性，同时稳定方差并使数据更符合正态分布的假设。这个练习 [@problem_id:3160298] 将指导你通过编程实现Box-Cox变换这一强大的工具，来处理偏态数据，并量化评估不同变换参数对数据分布和模型拟合优度的影响。", "problem": "您的任务是编程研究非线性单调变换如何影响单个正值预测变量的偏度和线性模型拟合的质量。您将对预测变量（特征、输入）应用 Box-Cox 变换族，并在一系列变换参数网格上比较结果。\n\n任务的基本依据：\n- 定义正输入的 Box-Cox 变换如下。对于任何 $x \\in \\mathbb{R}_{+}$ 和参数 $\\lambda \\in \\mathbb{R}$，定义\n$$\ng_{\\lambda}(x) = \n\\begin{cases}\n\\dfrac{x^{\\lambda}-1}{\\lambda}  \\text{if } \\lambda \\ne 0, \\\\\n\\log(x)  \\text{if } \\lambda = 0.\n\\end{cases}\n$$\n- 一个实值样本 $z_{1},\\dots,z_{n}$ 的样本偏度 $\\gamma_{1}(z)$ 定义为\n$$\n\\gamma_{1}(z) = \\frac{\\frac{1}{n}\\sum_{i=1}^{n}\\left(z_{i}-\\bar{z}\\right)^{3}}{\\left(\\frac{1}{n}\\sum_{i=1}^{n}\\left(z_{i}-\\bar{z}\\right)^{2}\\right)^{3/2}},\n\\quad \\bar{z}=\\frac{1}{n}\\sum_{i=1}^{n} z_{i}.\n$$\n- 对于一个带截距的线性模型，\n$$\ny_{i} = \\beta_{0} + \\beta_{1} z_{i} + \\varepsilon_{i}, \\quad \\varepsilon_{i} \\stackrel{\\text{iid}}{\\sim} \\mathcal{N}(0,\\sigma^{2}),\n$$\n普通最小二乘法（OLS）拟合产生残差 $r_{i}$ 和残差平方和 $\\text{RSS}=\\sum_{i=1}^{n} r_{i}^{2}$。在高斯误差模型下，其中 $\\hat{\\sigma}^{2}=\\text{RSS}/n$，最大化对数似然为\n$$\n\\ell = -\\frac{n}{2}\\left(\\log\\left(2\\pi \\hat{\\sigma}^{2}\\right)+1\\right).\n$$\n\n您的程序必须为每个指定的测试用例执行以下操作：\n1. 数据生成。通过以下方式生成一个正预测变量 $X_{i}$ 和一个响应 $Y_{i}$\n$$\nX_{i} \\sim \\text{LogNormal}(\\mu,\\sigma_{X}^{2}), \\quad \nY_{i} = \\beta_{0} + \\beta_{1}\\, g_{\\lambda^{\\star}}(X_{i}) + \\varepsilon_{i}, \\quad \\varepsilon_{i} \\sim \\mathcal{N}(0,\\sigma_{\\varepsilon}^{2}),\n$$\n对于 $i=1,\\dots,n$，使用指定的随机种子以保证可复现性。所有随机抽取必须是独立同分布的。\n\n2. 变换网格。考虑候选集\n$$\n\\Lambda = \\left\\{-2.0,-1.0,-0.5,-0.25,0.0,0.25,0.5,1.0,2.0\\right\\}.\n$$\n\n3. 偏度均衡。对于每个 $\\lambda \\in \\Lambda$，计算 $Z_{i}^{(\\lambda)}=g_{\\lambda}(X_{i})$ 并计算绝对偏度 $|\\gamma_{1}(Z^{(\\lambda)})|$。令 $\\hat{\\lambda}_{\\text{skew}}$ 为 $\\Lambda$ 上的任意一个最小化子。如果出现平局，选择按升序扫描 $\\Lambda$ 时遇到的第一个最小化子。\n\n4. 模型拟合优化。对于每个 $\\lambda \\in \\Lambda$，拟合带截距的 OLS 模型 $Y_{i}=\\beta_{0}^{(\\lambda)}+\\beta_{1}^{(\\lambda)} Z_{i}^{(\\lambda)}+\\varepsilon_{i}^{(\\lambda)}$，计算残差平方和 $\\text{RSS}(\\lambda)$ 及相应的最大化对数似然\n$$\n\\ell(\\lambda) = -\\frac{n}{2}\\left(\\log\\left(2\\pi \\cdot \\frac{\\text{RSS}(\\lambda)}{n}\\right) + 1\\right).\n$$\n令 $\\hat{\\lambda}_{\\text{fit}}$ 为 $\\Lambda$ 上 $\\ell(\\lambda)$ 的任意一个最大化子。如果出现平局，选择按升序扫描 $\\Lambda$ 时遇到的第一个最大化子。\n\n5. 相对于恒等变换的改进。定义恒等变换为 $\\lambda=1.0$。令 $\\Delta_{\\text{skew}} = \\ell(\\hat{\\lambda}_{\\text{skew}}) - \\ell(1.0)$。定义一个布尔改进指示符 $I$，当且仅当 $\\Delta_{\\text{skew}} > 0$ 时为真。\n\n6. 每个测试用例的输出。对于每个测试用例，输出列表 $[\\hat{\\lambda}_{\\text{skew}}, \\hat{\\lambda}_{\\text{fit}}, I, \\Delta_{\\text{skew}}]$，其中两个 $\\lambda$ 值和 $\\Delta_{\\text{skew}}$ 必须四舍五入到四位小数。布尔值 $I$ 必须是 true 或 false。\n\n测试套件：\n使用以下四个测试用例。对于每个用例，使用指定的参数 $(\\text{seed},n,\\mu,\\sigma_{X},\\beta_{0},\\beta_{1},\\lambda^{\\star},\\sigma_{\\varepsilon})$：\n- 用例 1: $(0,\\,400,\\,0.0,\\,1.0,\\,1.0,\\,2.0,\\,0.0,\\,1.0)$。\n- 用例 2: $(1,\\,400,\\,0.0,\\,0.25,\\,-0.5,\\,1.5,\\,1.0,\\,1.0)$。\n- 用例 3: $(2,\\,400,\\,0.0,\\,1.25,\\,0.0,\\,1.0,\\,0.5,\\,1.5)$。\n- 用例 4: $(3,\\,400,\\,0.5,\\,1.0,\\,0.5,\\,1.0,\\,-0.5,\\,1.0)$。\n\n不涉及角度单位。本问题中没有物理单位。\n\n最终输出格式：\n您的程序应生成单行输出，包含一个由四个用例结果组成的逗号分隔列表，每个结果本身是一个形如 $[\\hat{\\lambda}_{\\text{skew}},\\hat{\\lambda}_{\\text{fit}},I,\\Delta_{\\text{skew}}]$ 的列表，并且外层列表用方括号括起来。例如，包含两个用例的输出应如下所示：\n$[[\\cdot,\\cdot,\\cdot,\\cdot],[\\cdot,\\cdot,\\cdot,\\cdot]]$。\n每个内部列表中的所有三个实值条目在文本输出中都必须四舍五入到四位小数。", "solution": "The user wants to investigate the effects of Box-Cox transformations on a predictor variable. I will first validate the problem statement.\n\n### Step 1: Extract Givens\n\n- **Box–Cox Transformation**: For a positive input $x \\in \\mathbb{R}_{+}$ and a parameter $\\lambda \\in \\mathbb{R}$, the transformation is defined as:\n$$\ng_{\\lambda}(x) = \n\\begin{cases}\n\\dfrac{x^{\\lambda}-1}{\\lambda}  \\text{if } \\lambda \\ne 0, \\\\\n\\log(x)  \\text{if } \\lambda = 0.\n\\end{cases}\n$$\n\n- **Sample Skewness**: For a sample $z = \\{z_{1}, \\dots, z_{n}\\}$, the sample skewness is:\n$$\n\\gamma_{1}(z) = \\frac{\\frac{1}{n}\\sum_{i=1}^{n}\\left(z_{i}-\\bar{z}\\right)^{3}}{\\left(\\frac{1}{n}\\sum_{i=1}^{n}\\left(z_{i}-\\bar{z}\\right)^{2}\\right)^{3/2}}, \\quad \\text{where } \\bar{z}=\\frac{1}{n}\\sum_{i=1}^{n} z_{i}.\n$$\n\n- **Linear Model and Log-Likelihood**: For a simple linear regression model $y_{i} = \\beta_{0} + \\beta_{1} z_{i} + \\varepsilon_{i}$ with i.i.d. Gaussian errors $\\varepsilon_{i} \\sim \\mathcal{N}(0,\\sigma^{2})$, the maximized log-likelihood under OLS is:\n$$\n\\ell = -\\frac{n}{2}\\left(\\log\\left(2\\pi \\hat{\\sigma}^{2}\\right)+1\\right),\n$$\nwhere $\\hat{\\sigma}^{2}=\\text{RSS}/n$ is the maximum likelihood estimate of the error variance, with $\\text{RSS}$ being the residual sum of squares.\n\n- **Data Generation Model**:\n$$\nX_{i} \\sim \\text{LogNormal}(\\mu,\\sigma_{X}^{2}), \\quad \nY_{i} = \\beta_{0} + \\beta_{1}\\, g_{\\lambda^{\\star}}(X_{i}) + \\varepsilon_{i}, \\quad \\varepsilon_{i} \\sim \\mathcal{N}(0,\\sigma_{\\varepsilon}^{2})\n$$\nfor $i=1,\\dots,n$, with a specified random seed.\n\n- **Candidate Transformation Parameters**:\n$$\n\\Lambda = \\left\\{-2.0,-1.0,-0.5,-0.25,0.0,0.25,0.5,1.0,2.0\\right\\}\n$$\n\n- **Procedure**:\n    1.  **Skewness Equalization**: Find $\\hat{\\lambda}_{\\text{skew}} \\in \\Lambda$ that minimizes the absolute sample skewness $|\\gamma_{1}(g_{\\lambda}(X))|$.\n    2.  **Model-Fit Optimization**: Find $\\hat{\\lambda}_{\\text{fit}} \\in \\Lambda$ that maximizes the log-likelihood $\\ell(\\lambda)$ of the model $Y \\sim g_{\\lambda}(X)$.\n    3.  **Tie-breaking**: For both optimizations, the first value encountered in an ascending scan of $\\Lambda$ is chosen.\n    4.  **Improvement over Identity**: Calculate $\\Delta_{\\text{skew}} = \\ell(\\hat{\\lambda}_{\\text{skew}}) - \\ell(1.0)$ and a boolean indicator $I = (\\Delta_{\\text{skew}} > 0)$.\n\n- **Output**: For each test case, a list $[\\hat{\\lambda}_{\\text{skew}}, \\hat{\\lambda}_{\\text{fit}}, I, \\Delta_{\\text{skew}}]$ with floating-point numbers rounded to four decimal places.\n\n- **Test Cases**:\n    - Case 1: $(\\text{seed},n,\\mu,\\sigma_{X},\\beta_{0},\\beta_{1},\\lambda^{\\star},\\sigma_{\\varepsilon}) = (0, 400, 0.0, 1.0, 1.0, 2.0, 0.0, 1.0)$.\n    - Case 2: $(\\text{seed},n,\\mu,\\sigma_{X},\\beta_{0},\\beta_{1},\\lambda^{\\star},\\sigma_{\\varepsilon}) = (1, 400, 0.0, 0.25, -0.5, 1.5, 1.0, 1.0)$.\n    - Case 3: $(\\text{seed},n,\\mu,\\sigma_{X},\\beta_{0},\\beta_{1},\\lambda^{\\star},\\sigma_{\\varepsilon}) = (2, 400, 0.0, 1.25, 0.0, 1.0, 0.5, 1.5)$.\n    - Case 4: $(\\text{seed},n,\\mu,\\sigma_{X},\\beta_{0},\\beta_{1},\\lambda^{\\star},\\sigma_{\\varepsilon}) = (3, 400, 0.5, 1.0, 0.5, 1.0, -0.5, 1.0)$.\n\n### Step 2: Validate Using Extracted Givens\n\nThe problem is coherent and well-defined.\n- **Scientifically Grounded**: The problem is built upon fundamental and standard concepts in statistical modeling and data analysis, including the Box-Cox transformation, linear regression, skewness, and likelihood-based model evaluation. The data simulation protocol is a common approach for studying statistical methods.\n- **Well-Posed**: The sequence of operations is clearly specified. The use of a fixed random seed ensures reproducibility. The search for optimal $\\lambda$ values is conducted over a finite, discrete set $\\Lambda$, and the explicit tie-breaking rule guarantees a unique solution for each test case.\n- **Objective**: All definitions and tasks are stated with mathematical precision, leaving no room for subjective interpretation.\n- **Flaw Analysis**:\n    1.  **Scientific/Factual Unsoundness**: None. All formulas and concepts are standard in statistics.\n    2.  **Non-Formalizable/Irrelevant**: The problem is entirely formalizable and highly relevant to its stated topic in statistical learning.\n    3.  **Incomplete/Contradictory**: The problem specification is complete, providing all necessary parameters, data generation procedures, formulas, and criteria for evaluation. There are no contradictions.\n    4.  **Unrealistic/Infeasible**: The specified parameters are reasonable for a simulation study. The generation of $X$ from a LogNormal distribution ensures $X>0$, making the Box-Cox transformation well-defined.\n    5.  **Ill-Posed/Poorly Structured**: None. The problem structure is logical and leads to a unique, meaningful solution.\n    6.  **Pseudo-Profound/Trivial**: The problem requires the integration of several non-trivial statistical computations into a simulation framework to explore a conceptually important trade-off in feature engineering. It is a substantive task.\n    7.  **Outside Scientific Verifiability**: The results are computationally deterministic and verifiable given the random seeds.\n\n### Step 3: Verdict and Action\nThe problem is valid. I will proceed to construct the solution.\n\nThe solution will be implemented as a single Python script. For each test case defined in the problem, the script will execute the following sequence of steps:\n1.  Initialize a random number generator with the specified seed for reproducibility.\n2.  Generate the predictor data $X$ from a LogNormal distribution, $\\text{LogNormal}(\\mu, \\sigma_X^2)$, and the noise term $\\varepsilon$ from a Normal distribution, $\\mathcal{N}(0, \\sigma_\\varepsilon^2)$. The parameters $\\mu$, $\\sigma_X$, and $\\sigma_\\varepsilon$ are provided in each test case.\n3.  Construct the response variable $Y$ using the true underlying relationship: $Y_{i} = \\beta_{0} + \\beta_{1}\\, g_{\\lambda^{\\star}}(X_{i}) + \\varepsilon_{i}$.\n4.  Iterate through the candidate set of transformation parameters $\\Lambda$. For each $\\lambda \\in \\Lambda$:\n    a. Apply the Box-Cox transformation to the predictor: $Z^{(\\lambda)} = g_{\\lambda}(X)$.\n    b. Compute the sample skewness of the transformed predictor, $\\gamma_{1}(Z^{(\\lambda)})$, using the provided formula. The absolute value of this skewness is stored.\n    c. Fit a simple linear regression model of $Y$ on $Z^{(\\lambda)}$ using Ordinary Least Squares (OLS) to obtain coefficient estimates.\n    d. From the OLS fit, calculate the Residual Sum of Squares, $\\text{RSS}(\\lambda)$.\n    e. Use $\\text{RSS}(\\lambda)$ to compute the maximized log-likelihood, $\\ell(\\lambda)$, according to the given formula.\n5.  After iterating through all $\\lambda \\in \\Lambda$, determine $\\hat{\\lambda}_{\\text{skew}}$ by finding the $\\lambda$ that minimizes the stored absolute skewness values. The specified tie-breaking rule (first in ascending order) is handled by using `numpy.argmin`.\n6.  Similarly, determine $\\hat{\\lambda}_{\\text{fit}}$ by finding the $\\lambda$ that maximizes the stored log-likelihood values. The tie-breaking rule is handled by using `numpy.argmax`.\n7.  Calculate the improvement metric $\\Delta_{\\text{skew}} = \\ell(\\hat{\\lambda}_{\\text{skew}}) - \\ell(1.0)$. The value $\\ell(1.0)$ corresponds to the log-likelihood for the model using the identity-like transformation.\n8.  Define the boolean indicator $I$ as true if $\\Delta_{\\text{skew}} > 0$, and false otherwise.\n9.  Assemble the final result for the test case, which is the list $[\\hat{\\lambda}_{\\text{skew}}, \\hat{\\lambda}_{\\text{fit}}, I, \\Delta_{\\text{skew}}]$, ensuring numerical values are rounded to four decimal places.\n10. After processing all test cases, the results are formatted into a single string as a list of lists and printed to standard output.\nHelper functions will be defined for the Box-Cox transformation and other repeated calculations to ensure code clarity and correctness. The `scipy.stats.skew` function will be used for computing skewness as it correctly implements the biased estimator specified in the problem statement.", "answer": "```python\nimport numpy as np\nfrom scipy.stats import skew\n\ndef solve():\n    \"\"\"\n    Main function to run the simulation for all test cases and print the final result.\n    \"\"\"\n    # Define the test cases as per the problem statement.\n    # Format: (seed, n, mu, sigma_X, beta_0, beta_1, lambda_star, sigma_epsilon)\n    test_cases = [\n        (0, 400, 0.0, 1.0, 1.0, 2.0, 0.0, 1.0),\n        (1, 400, 0.0, 0.25, -0.5, 1.5, 1.0, 1.0),\n        (2, 400, 0.0, 1.25, 0.0, 1.0, 0.5, 1.5),\n        (3, 400, 0.5, 1.0, 0.5, 1.0, -0.5, 1.0),\n    ]\n\n    # Candidate set for the transformation parameter lambda.\n    Lambda = np.array([-2.0, -1.0, -0.5, -0.25, 0.0, 0.25, 0.5, 1.0, 2.0])\n\n    all_results = []\n\n    def box_cox_transform(x, lambda_val):\n        \"\"\"\n        Applies the Box-Cox transformation to the input data x.\n        \"\"\"\n        if lambda_val == 0.0:\n            return np.log(x)\n        else:\n            return (np.power(x, lambda_val) - 1.0) / lambda_val\n\n    for case in test_cases:\n        seed, n, mu, sigma_x, beta_0, beta_1, lambda_star, sigma_eps = case\n        \n        # Initialize random number generator for reproducibility.\n        rng = np.random.default_rng(seed)\n        \n        # Step 1: Data generation.\n        # X is from a LogNormal distribution, ensuring it's positive.\n        X = rng.lognormal(mean=mu, sigma=sigma_x, size=n)\n        # Epsilon is from a Normal distribution.\n        eps = rng.normal(loc=0.0, scale=sigma_eps, size=n)\n        \n        # The true transformed predictor and the response variable Y.\n        X_transformed_true = box_cox_transform(X, lambda_star)\n        Y = beta_0 + beta_1 * X_transformed_true + eps\n        \n        abs_skewness_list = []\n        log_likelihood_list = []\n        \n        # Step 2-4: Iterate through the transformation grid.\n        for lambda_val in Lambda:\n            # Transform the predictor X using the current lambda.\n            Z = box_cox_transform(X, lambda_val)\n            \n            # Compute absolute skewness. scipy.stats.skew with bias=True (default)\n            # matches the formula provided in the problem.\n            current_abs_skew = np.abs(skew(Z))\n            abs_skewness_list.append(current_abs_skew)\n            \n            # Fit an OLS model: Y = b0 + b1*Z.\n            # np.polyfit returns coefficients [b1, b0].\n            b1_fit, b0_fit = np.polyfit(Z, Y, 1)\n            \n            # Calculate predicted Y values and residuals.\n            Y_pred = b0_fit + b1_fit * Z\n            \n            # Compute residual sum of squares (RSS).\n            rss = np.sum((Y - Y_pred)**2)\n            \n            # The MLE for variance is RSS/n.\n            sigma2_hat = rss / float(n)\n            \n            # Compute the maximized log-likelihood.\n            # A check for non-positive variance to avoid math errors, though unlikely here.\n            if sigma2_hat > 0:\n                log_lik = -n / 2.0 * (np.log(2.0 * np.pi * sigma2_hat) + 1.0)\n            else:\n                log_lik = -np.inf\n            log_likelihood_list.append(log_lik)\n\n        # Find the lambda that minimizes absolute skewness.\n        # np.argmin respects the tie-breaking rule (first occurrence).\n        idx_skew = np.argmin(abs_skewness_list)\n        hat_lambda_skew = Lambda[idx_skew]\n        \n        # Find the lambda that maximizes log-likelihood.\n        # np.argmax also respects the tie-breaking rule.\n        idx_fit = np.argmax(log_likelihood_list)\n        hat_lambda_fit = Lambda[idx_fit]\n        \n        # Step 5: Improvement over identity.\n        # Find the log-likelihood for the identity transformation (lambda = 1.0).\n        idx_1 = np.where(Lambda == 1.0)[0][0]\n        log_lik_at_1 = log_likelihood_list[idx_1]\n        \n        # Get the log-likelihood at the skewness-minimizing lambda.\n        log_lik_at_skew = log_likelihood_list[idx_skew]\n        \n        Delta_skew = log_lik_at_skew - log_lik_at_1\n        I = Delta_skew > 0.0\n        \n        # Step 6: Assemble the output for the current case.\n        case_result = [\n            hat_lambda_skew, \n            hat_lambda_fit, \n            bool(I),\n            Delta_skew\n        ]\n        all_results.append(case_result)\n\n    # Format the final output string as a list of lists.\n    result_strings = []\n    for res in all_results:\n        # Format: [float, float, bool, float] with specified rounding and boolean case.\n        s = f\"[{res[0]:.4f},{res[1]:.4f},{str(res[2]).lower()},{res[3]:.4f}]\"\n        result_strings.append(s)\n    \n    # Final print statement must be on a single line in the exact specified format.\n    print(f\"[{','.join(result_strings)}]\")\n\nsolve()\n```", "id": "3160298"}, {"introduction": "在拥有多个预测变量的模型中，一个常见且关键的问题是多重共线性，即预测变量之间高度相关。这种情况会使标准线性回归模型的系数变得不稳定且难以解释。这个练习 [@problem_id:3160401] 通过一个完美的共线性例子，让你亲手验证普通最小二乘法（OLS）在这种情境下的不确定性，并展示像岭回归（Ridge Regression）这样的正则化方法如何通过引入一个微小的惩罚项，来获得唯一且更稳健的系数估计。", "problem": "考虑一个数据构造，用以阐明预测变量、特征和输入在线性模型（包含正则化和非正则化两种情况）中的作用。假设有 $n$ 个样本，一个基础特征 $x_1 \\in \\mathbb{R}^n$ 定义为 $x_1 = [-2,-1,0,1,2]^\\top$，并定义一个完全共线的第二个特征 $x_2 = 3 x_1$。设目标值是无噪声的，由 $y = 6 x_1$ 给出，并取截距为 $0$。根据需要将特征列堆叠起来，定义设计矩阵 $X \\in \\mathbb{R}^{n \\times p}$。\n\n本问题的基础知识包括：\n- 普通最小二乘法 (OLS) 的定义：最小化残差平方和，当唯一解存在时，等价于求解正规方程 $X^\\top X \\beta = X^\\top y$。\n- 岭回归的定义：最小化带有 $\\ell_2$ 惩罚项的残差平方和，等价于求解 $(X^\\top X + \\lambda I)\\beta = X^\\top y$（其中 $\\lambda \\ge 0$，$I$ 是单位矩阵）。\n- 摩尔-彭若斯伪逆 (MPP) 的定义：当 $X^\\top X$ 是奇异矩阵时，最小范数最小二乘解为 $\\beta = X^+ y$，其中 $X^+$ 表示伪逆矩阵。\n- 可识别性原则：如果存在唯一解，则参数是可识别的；否则，可能存在无穷多个参数向量 $\\beta$ 产生相同的预测值 $X\\beta$。\n\n任务：\n1. 仅使用基础特征 $x_1$ ($p = 1$)，计算 OLS 系数 $\\beta_1$。\n2. 同时使用两个具有完全共线性 $x_2 = 3 x_1$ 的特征 $[x_1, x_2]$ ($p = 2$)，利用摩尔-彭若斯伪逆计算非正则化的最小二乘系数 $(\\beta_1, \\beta_2)$。\n3. 对于共线双特征情况，计算在以下正则化强度 $\\lambda$ 下的岭回归系数 $(\\beta_1, \\beta_2)$：$0$、$0.1$、$1$、$10$ 和 $1000000$。当 $\\lambda = 0$ 时，使用摩尔-彭若斯伪逆。\n4. 讨论可识别性和系数路径：推导在这种特定的秩为1的设定下岭回归解的形式，并解释比率 $\\beta_2 / \\beta_1$ 如何随 $\\lambda$ 的变化而变化。\n\n数值细节：\n- 所有计算均为纯数学计算，无单位。\n- 报告的每个系数必须四舍五入到 $6$ 位小数。\n\n测试套件：\n- 情况1：使用单个预测变量 $x_1$ 的 OLS (非正则化)，返回 $[\\beta_1]$。\n- 情况2：使用预测变量 $[x_1, x_2]$ 的 OLS (使用 MPP 的非正则化)，返回 $[\\beta_1, \\beta_2]$。\n- 情况3：使用预测变量 $[x_1, x_2]$ 且 $\\lambda = 0$ 的岭回归，通过 MPP 返回 $[\\beta_1, \\beta_2]$。\n- 情况4：使用预测变量 $[x_1, x_2]$ 且 $\\lambda = 0.1$ 的岭回归，返回 $[\\beta_1, \\beta_2]$。\n- 情况5：使用预测变量 $[x_1, x_2]$ 且 $\\lambda = 1$ 的岭回归，返回 $[\\beta_1, \\beta_2]$。\n- 情况6：使用预测变量 $[x_1, x_2]$ 且 $\\lambda = 10$ 的岭回归，返回 $[\\beta_1, \\beta_2]$。\n- 情况7：使用预测变量 $[x_1, x_2]$ 且 $\\lambda = 1000000$ 的岭回归，返回 $[\\beta_1, \\beta_2]$。\n\n最终输出格式：\n您的程序应生成单行输出，其中包含一个由方括号括起来的逗号分隔列表。列表中的每个元素都是一个内层方括号列表，包含一个测试用例的四舍五入后的系数，并按测试套件的顺序列出。例如，输出应如下所示：\n$[[c_{1,1},c_{1,2},\\dots],[c_{2,1},c_{2,2}],\\dots]$\n且必须精确地以单行形式打印，不含任何额外文本。", "solution": "问题验证如下。\n\n已知条件：\n- 样本数量：$n = 5$。\n- 基础特征向量：$x_1 = [-2, -1, 0, 1, 2]^\\top \\in \\mathbb{R}^n$。\n- 第二个特征向量：$x_2 = 3 x_1$。\n- 目标向量：$y = 6 x_1$。\n- 截距：$0$。\n- 设计矩阵：$X \\in \\mathbb{R}^{n \\times p}$，由特征列构成。\n- 普通最小二乘法 (OLS) 定义：最小化残差平方和，求解 $X^\\top X \\beta = X^\\top y$。\n- 岭回归定义：最小化残差平方和加上一个 $\\ell_2$ 惩罚项，求解 $(X^\\top X + \\lambda I)\\beta = X^\\top y$（其中 $\\lambda \\ge 0$）。\n- 摩尔-彭若斯伪逆 (MPP)：对于奇异矩阵 $X^\\top X$，最小范数最小二乘解为 $\\beta = X^+ y$。\n- 任务1：对于 $p=1$，使用特征 $x_1$ 计算 OLS 系数 $\\beta_1$。\n- 任务2：对于 $p=2$，使用特征 $[x_1, x_2]$，通过 MPP 计算 OLS 系数 $(\\beta_1, \\beta_2)$。\n- 任务3：对于 $p=2$，计算 $\\lambda \\in \\{0, 0.1, 1, 10, 1000000\\}$ 时的岭回归系数 $(\\beta_1, \\beta_2)$。\n- 任务4：讨论可识别性以及系数比率 $\\beta_2 / \\beta_1$ 随 $\\lambda$ 变化的表现。\n- 数值精度：将系数四舍五入到 $6$ 位小数。\n\n验证：\n- **科学依据：**该问题是统计学习和线性代数中的一个标准练习，涉及 OLS、岭回归、多重共线性和参数可识别性等基本概念。所有原理都是公认的。\n- **适定性：**该问题提供了所有必要的数据和定义。对于奇异情况，使用摩尔-彭若斯伪逆的指令确保了每个任务都有一个唯一定义的解。因此，该问题是适定的。\n- **客观性：**问题以精确的数学语言陈述，没有主观性或歧义。\n\n结论：该问题有效。\n\n我们开始求解。\n\n首先，我们定义给定的向量并计算一些初步的量。\n基础特征向量为 $x_1 = [-2, -1, 0, 1, 2]^\\top$。\n样本数量为 $n=5$。\n第二个特征为 $x_2 = 3x_1 = [-6, -3, 0, 3, 6]^\\top$。\n目标向量为 $y = 6x_1 = [-12, -6, 0, 6, 12]^\\top$。\n\n我们计算以下内积：\n$x_1^\\top x_1 = (-2)^2 + (-1)^2 + 0^2 + 1^2 + 2^2 = 4 + 1 + 0 + 1 + 4 = 10$。\n$x_1^\\top y = x_1^\\top (6x_1) = 6(x_1^\\top x_1) = 6(10) = 60$。\n\n任务1：使用单个预测变量 $x_1$ 的 OLS\n设计矩阵为 $X = x_1$，这是一个 $5 \\times 1$ 的矩阵。OLS 系数 $\\beta_1$ 通过求解正规方程 $(X^\\top X) \\beta_1 = X^\\top y$ 得到。\n$$\n\\beta_1 = (x_1^\\top x_1)^{-1} (x_1^\\top y) = (10)^{-1} (60) = 6\n$$\n单特征模型的系数为 $\\beta_1 = 6$。\n\n任务2：使用预测变量 $[x_1, x_2]$ 并利用摩尔-彭若斯伪逆的 OLS\n设计矩阵为 $X = [x_1, x_2] = [x_1, 3x_1]$。各列完全共线，因此 $X$ 的秩为1。矩阵 $X^\\top X$ 将是奇异的。\n$$\nX^\\top X = \\begin{bmatrix} x_1^\\top x_1  x_1^\\top x_2 \\\\ x_2^\\top x_1  x_2^\\top x_2 \\end{bmatrix} = \\begin{bmatrix} x_1^\\top x_1  x_1^\\top (3x_1) \\\\ (3x_1)^\\top x_1  (3x_1)^\\top (3x_1) \\end{bmatrix} = \\begin{bmatrix} x_1^\\top x_1  3(x_1^\\top x_1) \\\\ 3(x_1^\\top x_1)  9(x_1^\\top x_1) \\end{bmatrix} = \\begin{bmatrix} 10  30 \\\\ 30  90 \\end{bmatrix}\n$$\n$X^\\top X$ 的行列式为 $10 \\times 90 - 30 \\times 30 = 0$，证实了其奇异性。正规方程的右侧为：\n$$\nX^\\top y = \\begin{bmatrix} x_1^\\top y \\\\ x_2^\\top y \\end{bmatrix} = \\begin{bmatrix} x_1^\\top y \\\\ (3x_1)^\\top y \\end{bmatrix} = \\begin{bmatrix} 60 \\\\ 3(60) \\end{bmatrix} = \\begin{bmatrix} 60 \\\\ 180 \\end{bmatrix}\n$$\n方程组 $X^\\top X \\beta = X^\\top y$ 有无穷多解，满足 $10\\beta_1 + 30\\beta_2 = 60$，可简化为 $\\beta_1 + 3\\beta_2 = 6$。摩尔-彭若斯伪逆提供最小 $\\ell_2$ 范数解。该解向量 $\\beta=[\\beta_1, \\beta_2]^\\top$ 必须与约束的零空间正交，这意味着它必须与系数向量 $[1, 3]^\\top$ 成比例。因此，对于某个标量 $k$，有 $\\beta=k[1, 3]^\\top = [k, 3k]^\\top$。\n将此代入约束方程：\n$$\nk + 3(3k) = 6 \\implies 10k = 6 \\implies k = 0.6\n$$\n因此，最小范数解为 $\\beta_1 = 0.6$ 和 $\\beta_2 = 3(0.6) = 1.8$。\n\n任务3：针对不同 $\\lambda$ 值的岭回归\n岭回归解由 $\\beta_\\lambda = (X^\\top X + \\lambda I)^{-1} X^\\top y$ 给出。需要求逆的矩阵是：\n$$\nX^\\top X + \\lambda I = \\begin{bmatrix} 10  30 \\\\ 30  90 \\end{bmatrix} + \\begin{bmatrix} \\lambda  0 \\\\ 0  \\lambda \\end{bmatrix} = \\begin{bmatrix} 10+\\lambda  30 \\\\ 30  90+\\lambda \\end{bmatrix}\n$$\n对于 $\\lambda > 0$，该矩阵是可逆的。其行列式为 $(10+\\lambda)(90+\\lambda) - 900 = 100\\lambda + \\lambda^2 = \\lambda(\\lambda+100)$。\n其逆矩阵为：\n$$\n(X^\\top X + \\lambda I)^{-1} = \\frac{1}{\\lambda(\\lambda+100)} \\begin{bmatrix} 90+\\lambda  -30 \\\\ -30  10+\\lambda \\end{bmatrix}\n$$\n岭回归系数则为：\n$$\n\\beta_\\lambda = \\begin{bmatrix} \\beta_{\\lambda,1} \\\\ \\beta_{\\lambda,2} \\end{bmatrix} = \\frac{1}{\\lambda(\\lambda+100)} \\begin{bmatrix} 90+\\lambda  -30 \\\\ -30  10+\\lambda \\end{bmatrix} \\begin{bmatrix} 60 \\\\ 180 \\end{bmatrix}\n$$\n计算各分量：\n$\\beta_{\\lambda,1} = \\frac{1}{\\lambda(\\lambda+100)} [60(90+\\lambda) - 30(180)] = \\frac{60\\lambda}{\\lambda(\\lambda+100)} = \\frac{60}{\\lambda+100}$。\n$\\beta_{\\lambda,2} = \\frac{1}{\\lambda(\\lambda+100)} [-30(60) + 180(10+\\lambda)] = \\frac{180\\lambda}{\\lambda(\\lambda+100)} = \\frac{180}{\\lambda+100}$。\n\n- 对于 $\\lambda=0$：题目要求使用 MPP，因此结果与任务2相同：$(\\beta_1, \\beta_2) = (0.6, 1.8)$。\n- 对于 $\\lambda=0.1$：$(\\beta_1, \\beta_2) = (\\frac{60}{100.1}, \\frac{180}{100.1}) \\approx (0.599401, 1.798202)$。\n- 对于 $\\lambda=1$：$(\\beta_1, \\beta_2) = (\\frac{60}{101}, \\frac{180}{101}) \\approx (0.594059, 1.782178)$。\n- 对于 $\\lambda=10$：$(\\beta_1, \\beta_2) = (\\frac{60}{110}, \\frac{180}{110}) \\approx (0.545455, 1.636364)$。\n- 对于 $\\lambda=1000000$：$(\\beta_1, \\beta_2) = (\\frac{60}{1000100}, \\frac{180}{1000100}) \\approx (0.000060, 0.000180)$。\n\n任务4：关于可识别性和系数路径的讨论\n- 可识别性：在 OLS 情况（$\\lambda=0$）下，由于特征 $x_1$ 和 $x_2$ 共线，矩阵 $X^\\top X$ 是奇异的。这导致正规方程组有无穷多解，所有解都位于直线 $\\beta_1 + 3\\beta_2 = 6$ 上。由于参数向量 $\\beta$ 不存在唯一解，因此参数是不可识别的。然而，对于所有解，预测值 $\\hat{y} = X\\beta$ 都是唯一的，因为 $\\hat{y} = \\beta_1 x_1 + \\beta_2 x_2 = \\beta_1 x_1 + \\beta_2 (3x_1) = (\\beta_1 + 3\\beta_2)x_1 = 6x_1 = y$。岭回归通过添加项 $\\lambda I$（其中 $\\lambda > 0$），使得矩阵 $(X^\\top X + \\lambda I)$ 可逆，从而确保了 $\\beta_\\lambda$ 的唯一解，并使得参数在任何正正则化强度下都是可识别的。\n- 系数路径：我们推导出的岭回归解是 $\\beta_\\lambda = (\\frac{60}{\\lambda+100}, \\frac{180}{\\lambda+100})$。系数的比率为：\n$$\n\\frac{\\beta_{\\lambda,2}}{\\beta_{\\lambda,1}} = \\frac{180/(\\lambda+100)}{60/(\\lambda+100)} = \\frac{180}{60} = 3\n$$\n对于所有 $\\lambda > 0$，这个比率是常数，等于 $3$。当 $\\lambda \\to 0^+$ 时，岭回归解收敛于 $(0.6, 1.8)$，即最小范数 OLS 解，其比率为 $1.8/0.6=3$。当 $\\lambda \\to \\infty$ 时，两个系数都向 $0$ 收缩，但它们的比率保持为 $3$。系数路径是一条从最小范数解 $(0.6, 1.8)$ 到原点 $(0,0)$ 的直线。这是因为预测变量 $x_1$ 和 $x_2$ 完全相关，在特征空间中定义了一个由向量 $[1, 3]^\\top$ 张成的一维子空间。数据方差仅存在于这个方向上。岭回归对系数进行惩罚，在这种情况下，它沿着数据协方差矩阵 $X^\\top X$ 的主成分方向收缩系数。$X^\\top X$ 的主特征向量与 $[1, 3]^\\top$ 成比例，这决定了系数的固定比率。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the linear regression problem under different conditions as specified.\n    \"\"\"\n    # Define base data using floating-point numbers for precision.\n    x1_vec = np.array([-2.0, -1.0, 0.0, 1.0, 2.0])\n    x1 = x1_vec.reshape(-1, 1)\n    y = 6.0 * x1\n\n    all_results = []\n\n    # Case 1: OLS with a single predictor x1 (p=1)\n    # beta_1 = (X1.T @ X1)^-1 @ X1.T @ y\n    X1 = x1\n    beta_case1 = np.linalg.inv(X1.T @ X1) @ (X1.T @ y)\n    all_results.append(beta_case1.flatten())\n\n    # Define data for two-predictor cases (p=2)\n    x2 = 3.0 * x1\n    X2 = np.hstack((x1, x2))\n    \n    # Case 2: OLS with predictors [x1, x2] using Moore-Penrose Pseudoinverse\n    # beta = pinv(X2) @ y\n    beta_mpp = np.linalg.pinv(X2) @ y\n    all_results.append(beta_mpp.flatten())\n\n    # Case 3: Ridge with lambda = 0, using MPP as specified\n    # This is identical to Case 2.\n    all_results.append(beta_mpp.flatten())\n\n    # Cases 4-7: Ridge with predictors [x1, x2] and lambda > 0\n    lambdas = [0.1, 1.0, 10.0, 1000000.0]\n    p = X2.shape[1]\n    identity_matrix = np.identity(p)\n    X2T_X2 = X2.T @ X2\n    X2T_y = X2.T @ y\n\n    for lam in lambdas:\n        # beta_ridge = (X2.T @ X2 + lam * I)^-1 @ X2.T @ y\n        beta_ridge = np.linalg.inv(X2T_X2 + lam * identity_matrix) @ X2T_y\n        all_results.append(beta_ridge.flatten())\n    \n    # Format the final output string as per requirements.\n    # Each coefficient is formatted to 6 decimal places.\n    formatted_results = []\n    for res_vector in all_results:\n        # Using f-string formatting handles rounding and ensures 6 decimal places.\n        s_res = [f\"{x:.6f}\" for x in res_vector]\n        formatted_results.append(f\"[{','.join(s_res)}]\")\n    \n    final_output_string = f\"[{','.join(formatted_results)}]\"\n    print(final_output_string)\n\nsolve()\n```", "id": "3160401"}, {"introduction": "模型的性能在很大程度上取决于其在不同数据环境下的泛化能力，一个常见的挑战是“领域漂移”（domain shift），即测试数据的分布与训练数据不同。天真地应用特征处理方法，例如标准的Z分数归一化，在面对数据测量方式或分布发生根本性变化时可能会失效。通过一个货币单位变化的实际场景 [@problem_id:3160364]，本练习让你通过比较不同归一化策略，深刻理解构建在不同数据情境下保持语义一致性的稳健特征的重要性。", "problem": "您将处理一个一维监督学习问题，其中单个实值特征表示产品价格，预测目标是在以特定货币表示时，该价格的线性函数。训练域使用美元（USD）。在测试时会出现域偏移，因为相同的现实世界量是以欧元（EUR）记录的，且汇率已知。您的任务是实现并评估三种不同的特征归一化策略，以在该域偏移下产生一致的预测。\n\n基本基础和定义：\n- 预测器是一个函数 $f:\\mathbb{R}\\to\\mathbb{R}$，它将输入特征 $x$ 映射到预测输出 $y$。我们考虑平方损失 $L(y,\\hat{y})=(y-\\hat{y})^{2}$。\n- 在使用平方损失对线性函数进行经验风险最小化时，将 $y$ 回归到单个输入 $z$ 上的普通最小二乘法解的斜率为 $\\hat{w}=\\dfrac{\\sum (z-\\bar{z})(y-\\bar{y})}{\\sum (z-\\bar{z})^{2}}$，截距为 $\\hat{c}=\\bar{y}-\\hat{w}\\,\\bar{z}$，其中上划线表示样本均值。\n- 对于一个样本均值为 $\\mu$、样本标准差为 $\\sigma>0$ 的特征 $x$，其Z-score标准化定义为 $z=(x-\\mu)/\\sigma$。\n\n数据生成机制：\n- 真实目标值是美元价格的线性函数：$y=\\theta_{0}+\\theta_{1}\\,x_{\\mathrm{USD}}$。\n- 训练数据以美元收集。在测试时，观测到的特征是 $x_{\\mathrm{EUR}}=r\\cdot x_{\\mathrm{USD}}$，其中 $r>0$ 是汇率（欧元/美元）。标签始终遵循上述基于美元的规律。\n\n所有情况下均需使用的训练过程：\n1. 使用总体定义（即，对所有 $n$ 个样本求平均，分母为 $n$），计算美元特征 $x_{\\mathrm{USD}}$ 的训练均值 $\\mu_{\\mathrm{A}}$ 和标准差 $\\sigma_{\\mathrm{A}}$。\n2. 将训练特征标准化为 $z_{\\mathrm{A}}=(x_{\\mathrm{USD}}-\\mu_{\\mathrm{A}})/\\sigma_{\\mathrm{A}}$。\n3. 通过在 $(z_{\\mathrm{A}},y)$ 上使用普通最小二乘法来拟合线性预测器 $\\hat{y}=\\hat{w}\\,z+\\hat{c}$。\n\n在测试时，对于一个观测到的欧元特征向量 $x_{\\mathrm{EUR}}$，评估三种策略来构建用于推理的标准化输入 $z$：\n- 策略 T1（在观测值上使用训练统计数据）：$z_{\\mathrm{T1}}=(x_{\\mathrm{EUR}}-\\mu_{\\mathrm{A}})/\\sigma_{\\mathrm{A}}$。\n- 策略 T2（按域标准化）：从 $x_{\\mathrm{EUR}}$ 本身计算 $\\mu_{\\mathrm{B}}$ 和 $\\sigma_{\\mathrm{B}}$（使用总体定义），然后设置 $z_{\\mathrm{T2}}=(x_{\\mathrm{EUR}}-\\mu_{\\mathrm{B}})/\\sigma_{\\mathrm{B}}$。\n- 策略 T3（考虑货币因素后使用训练统计数据）：$z_{\\mathrm{T3}}=(x_{\\mathrm{EUR}}/r-\\mu_{\\mathrm{A}})/\\sigma_{\\mathrm{A}}$。\n\n对于每种策略，生成预测 $\\hat{y}=\\hat{w}\\,z+\\hat{c}$，并评估测试集上的均方根误差（RMSE），其定义为 $\\sqrt{\\dfrac{1}{m}\\sum_{i=1}^{m}(\\hat{y}_{i}-y_{i})^{2}}$，其中 $m$ 是测试样本的数量，$y_{i}=\\theta_{0}+\\theta_{1}\\,x_{\\mathrm{USD},i}$。\n\n使用以下固定的数值规范：\n- 真实参数：$\\theta_{0}=4.0$，$\\theta_{1}=1.2$。\n- 训练美元特征值：$[\\,12.0,\\,25.0,\\,40.0,\\,55.0,\\,70.0\\,]$。\n- 训练标签：$y=\\theta_{0}+\\theta_{1}\\,x_{\\mathrm{USD}}$，根据上述值精确计算。\n- 测试套件（四种情况），每种情况都给出一个汇率 $r$ 和一个美元价格列表 $x_{\\mathrm{USD}}$；观测到的测试特征为 $x_{\\mathrm{EUR}}=r\\cdot x_{\\mathrm{USD}}$：\n  1. 情况 1（一般域偏移）：$r=0.9$, $x_{\\mathrm{USD}}=[\\,15.0,\\,22.5,\\,60.0,\\,80.0\\,]$。\n  2. 情况 2（无域偏移）：$r=1.0$, $x_{\\mathrm{USD}}=[\\,10.0,\\,30.0,\\,50.0\\,]$。\n  3. 情况 3（缩放下的分布匹配）：$r=1.15$, $x_{\\mathrm{USD}}=[\\,12.0,\\,25.0,\\,40.0,\\,55.0,\\,70.0\\,]$。\n  4. 情况 4（强域偏移）：$r=0.5$, $x_{\\mathrm{USD}}=[\\,5.0,\\,15.0,\\,75.0,\\,120.0\\,]$。\n\n您的程序应该：\n- 完全按照规定实现训练过程。\n- 对于每个测试用例，按顺序计算策略 T1、T2 和 T3 的 RMSE。\n- 输出中不需要物理单位；所有值均以实数报告。\n\n最终输出格式：\n- 您的程序应生成一行输出，其中包含一个用方括号括起来的逗号分隔列表，其结果按以下顺序排列：\n  $[\\,\\mathrm{RMSE}_{\\mathrm{T1}}^{(1)},\\mathrm{RMSE}_{\\mathrm{T2}}^{(1)},\\mathrm{RMSE}_{\\mathrm{T3}}^{(1)},\\mathrm{RMSE}_{\\mathrm{T1}}^{(2)},\\mathrm{RMSE}_{\\mathrm{T2}}^{(2)},\\mathrm{RMSE}_{\\mathrm{T3}}^{(2)},\\mathrm{RMSE}_{\\mathrm{T1}}^{(3)},\\mathrm{RMSE}_{\\mathrm{T2}}^{(3)},\\mathrm{RMSE}_{\\mathrm{T3}}^{(3)},\\mathrm{RMSE}_{\\mathrm{T1}}^{(4)},\\mathrm{RMSE}_{\\mathrm{T2}}^{(4)},\\mathrm{RMSE}_{\\mathrm{T3}}^{(4)}\\,]$，其中上标 $^{(k)}$ 表示测试用例编号 $k\\in\\{1,2,3,4\\}$。", "solution": "此问题已经过验证。\n\n### 步骤 1：提取已知信息\n- **预测器函数**：$f:\\mathbb{R}\\to\\mathbb{R}$，将输入特征 $x$ 映射到预测输出 $\\hat{y}$。\n- **损失函数**：平方损失，$L(y,\\hat{y})=(y-\\hat{y})^{2}$。\n- **单个回归量 $z$ 的普通最小二乘法 (OLS) 解**：\n  - 斜率：$\\hat{w}=\\dfrac{\\sum (z-\\bar{z})(y-\\bar{y})}{\\sum (z-\\bar{z})^{2}}$\n  - 截距：$\\hat{c}=\\bar{y}-\\hat{w}\\,\\bar{z}$\n- **Z-score 标准化**：$z=(x-\\mu)/\\sigma$，使用样本均值 $\\mu$ 和样本标准差 $\\sigma>0$。问题指定使用总体定义（均值和方差的分母为 $n$）。\n- **真实数据生成过程**：真实目标 $y$ 是美元价格的线性函数：$y=\\theta_{0}+\\theta_{1}\\,x_{\\mathrm{USD}}$。\n- **域偏移机制**：训练数据使用 $x_{\\mathrm{USD}}$。测试数据使用 $x_{\\mathrm{EUR}}=r\\cdot x_{\\mathrm{USD}}$，其中 $r>0$ 是一个已知汇率。目标 $y$ 始终由 $x_{\\mathrm{USD}}$ 决定。\n- **训练过程**：\n  1.  计算训练特征 $x_{\\mathrm{USD}}$ 的训练均值 $\\mu_{\\mathrm{A}}$ 和总体标准差 $\\sigma_{\\mathrm{A}}$（来自 $n$ 个样本）。\n  2.  标准化训练特征：$z_{\\mathrm{A}}=(x_{\\mathrm{USD}}-\\mu_{\\mathrm{A}})/\\sigma_{\\mathrm{A}}$。\n  3.  通过对 $(z_{\\mathrm{A}},y)$ 进行 OLS 拟合线性预测器 $\\hat{y}=\\hat{w}\\,z+\\hat{c}$。\n- **测试时推理策略**：对于一个观测到的测试特征向量 $x_{\\mathrm{EUR}}$，输入 $z$ 通过以下方式构建：\n  - **T1（在观测值上使用训练统计数据）**：$z_{\\mathrm{T1}}=(x_{\\mathrm{EUR}}-\\mu_{\\mathrm{A}})/\\sigma_{\\mathrm{A}}$。\n  - **T2（按域标准化）**：从 $x_{\\mathrm{EUR}}$ 计算均值 $\\mu_{\\mathrm{B}}$ 和总体标准差 $\\sigma_{\\mathrm{B}}$，然后 $z_{\\mathrm{T2}}=(x_{\\mathrm{EUR}}-\\mu_{\\mathrm{B}})/\\sigma_{\\mathrm{B}}$。\n  - **T3（考虑货币因素后使用训练统计数据）**：$z_{\\mathrm{T3}}=(x_{\\mathrm{EUR}}/r-\\mu_{\\mathrm{A}})/\\sigma_{\\mathrm{A}}$。\n- **评估指标**：测试集上的均方根误差 (RMSE)，$\\sqrt{\\dfrac{1}{m}\\sum_{i=1}^{m}(\\hat{y}_{i}-y_{i})^{2}}$，其中 $m$ 是测试集大小。\n- **数值规范**：\n  - 真实参数：$\\theta_{0}=4.0$，$\\theta_{1}=1.2$。\n  - 训练数据 $x_{\\mathrm{USD}}$：$[\\,12.0,\\,25.0,\\,40.0,\\,55.0,\\,70.0\\,]$，$n=5$。\n  - 训练标签：$y=\\theta_{0}+\\theta_{1}\\,x_{\\mathrm{USD}}$。\n  - 测试用例：\n    1.  $r=0.9$, $x_{\\mathrm{USD}}=[\\,15.0,\\,22.5,\\,60.0,\\,80.0\\,]$。\n    2.  $r=1.0$, $x_{\\mathrm{USD}}=[\\,10.0,\\,30.0,\\,50.0\\,]$。\n    3.  $r=1.15$, $x_{\\mathrm{USD}}=[\\,12.0,\\,25.0,\\,40.0,\\,55.0,\\,70.0\\,]$。\n    4.  $r=0.5$, $x_{\\mathrm{USD}}=[\\,5.0,\\,15.0,\\,75.0,\\,120.0\\,]$。\n\n### 步骤 2：使用提取的已知信息进行验证\n该问题在科学上基于线性回归和统计学习的原理。域偏移、特征标准化和经验风险最小化等概念是该领域的标准概念。该设定是一个简化但概念上合理的现实世界场景模型，即在一个上下文（例如，一种货币）中训练的模型必须应用于另一个上下文中。该问题是适定的：所有数据、参数和程序都已明确定义，不存在矛盾。为训练和测试提供的特征向量都包含不同的元素，确保标准差 $\\sigma$ 始终大于零，使得 Z-score 标准化是明确定义的。该问题并非无足轻重；它需要仔细实现指定的程序，并突出了关于模型在协变量偏移下的鲁棒性的基本概念。预期结果是可通过计算验证的。\n\n### 步骤 3：结论与操作\n此问题有效。将提供一个合理的解答。\n\n### 解答\n\n该问题要求我们训练一个线性回归模型，并使用三种不同的特征归一化策略来评估其在特定类型的域偏移下的性能。\n\n**1. 训练阶段**\n\n训练过程涉及拟合一个线性模型 $\\hat{y} = \\hat{w}z + \\hat{c}$，其中 $z$ 是标准化特征。\n\n首先，我们定义训练数据。给定的特征为 $x_{\\mathrm{A, USD}} = [\\,12.0,\\, 25.0,\\, 40.0,\\, 55.0,\\, 70.0\\,]$。\n真实参数为 $\\theta_{0}=4.0$ 和 $\\theta_{1}=1.2$。训练标签 $y_{\\mathrm{A}}$ 由真实模型生成，$y_i = \\theta_{0} + \\theta_{1} x_{\\mathrm{A, USD}, i}$：\n$y_{\\mathrm{A}} = [\\,4.0 + 1.2 \\cdot 12.0,\\, \\dots,\\, 4.0 + 1.2 \\cdot 70.0\\,] = [\\,18.4,\\, 34.0,\\, 52.0,\\, 70.0,\\, 88.0\\,]$。\n\n接下来，我们使用总体定义（分母 $n=5$）计算训练特征的统计数据：\n均值为 $\\mu_{\\mathrm{A}} = \\frac{1}{5}(12.0 + 25.0 + 40.0 + 55.0 + 70.0) = \\frac{202.0}{5} = 40.4$。\n方差为 $\\sigma_{\\mathrm{A}}^2 = \\frac{1}{5}\\sum_{i=1}^{5}(x_{\\mathrm{A, USD}, i} - \\mu_{\\mathrm{A}})^2 = \\frac{1}{5}((12.0-40.4)^2 + \\dots + (70.0-40.4)^2) = \\frac{2133.2}{5} = 426.64$。\n标准差为 $\\sigma_{\\mathrm{A}} = \\sqrt{426.64} \\approx 20.6552656$。\n\n训练特征被标准化：$z_{\\mathrm{A}} = (x_{\\mathrm{A, USD}}-\\mu_{\\mathrm{A}})/\\sigma_{\\mathrm{A}}$。\n真实标签 $y_{\\mathrm{A}}$ 与标准化特征 $z_{\\mathrm{A}}$ 之间的关系是完全线性的：\n$y_{\\mathrm{A}} = \\theta_{0} + \\theta_{1}x_{\\mathrm{A, USD}} = \\theta_{0} + \\theta_{1}(\\sigma_{\\mathrm{A}} z_{\\mathrm{A}} + \\mu_{\\mathrm{A}}) = (\\theta_{0} + \\theta_{1}\\mu_{\\mathrm{A}}) + (\\theta_{1}\\sigma_{\\mathrm{A}})z_{\\mathrm{A}}$。\n由于 OLS 提供了最佳线性无偏估计量，并且数据完全在一条直线上，因此拟合的参数将是精确的。模型为 $\\hat{y} = \\hat{w}z_{\\mathrm{A}} + \\hat{c}$。\n估计的斜率为 $\\hat{w} = \\theta_{1}\\sigma_{\\mathrm{A}} = 1.2 \\cdot \\sqrt{426.64} \\approx 24.7863187$。\n估计的截距为 $\\hat{c} = \\theta_{0} + \\theta_{1}\\mu_{\\mathrm{A}} = 4.0 + 1.2 \\cdot 40.4 = 4.0 + 48.48 = 52.48$。\n\n这些参数 $(\\hat{w}, \\hat{c})$ 是固定的，并用于所有测试用例的推理。\n\n**2. 测试阶段**\n\n对于每个测试用例，我们给出一个汇率 $r$ 和一组测试价格 $x_{\\mathrm{B, USD}}$。观测到的特征是 $x_{\\mathrm{B, EUR}} = r \\cdot x_{\\mathrm{B, USD}}$，真实标签是 $y_{\\mathrm{B}} = \\theta_{0} + \\theta_{1}x_{\\mathrm{B, USD}}$。\n\n**策略 T1（在观测值上使用训练统计数据）：**\n用于预测的输入：$z_{\\mathrm{T1}} = (x_{\\mathrm{B, EUR}} - \\mu_{\\mathrm{A}})/\\sigma_{\\mathrm{A}}$。\n预测：$\\hat{y}_{\\mathrm{T1}} = \\hat{w} z_{\\mathrm{T1}} + \\hat{c}$。\n该策略天真地将训练统计数据应用于来自偏移域的特征。它未能考虑货币变化和特征分布的潜在变化。\n\n**策略 T2（按域标准化）：**\n首先，从测试特征 $x_{\\mathrm{B, EUR}}$ 计算统计数据：均值 $\\mu_{\\mathrm{B}}$ 和标准差 $\\sigma_{\\mathrm{B}}$。\n用于预测的输入：$z_{\\mathrm{T2}} = (x_{\\mathrm{B, EUR}} - \\mu_{\\mathrm{B}})/\\sigma_{\\mathrm{B}}$。\n预测：$\\hat{y}_{\\mathrm{T2}} = \\hat{w} z_{\\mathrm{T2}} + \\hat{c}$。\n该策略适应了测试分布的前两个矩，但忽略了域之间的已知关系。\n\n**策略 T3（考虑货币因素后使用训练统计数据）：**\n首先，反转货币换算：$x_{\\mathrm{B, USD\\_reconstructed}} = x_{\\mathrm{B, EUR}} / r$。根据定义，这完美地恢复了原始的 $x_{\\mathrm{B, USD}}$。\n用于预测的输入：$z_{\\mathrm{T3}} = (x_{\\mathrm{B, USD\\_reconstructed}} - \\mu_{\\mathrm{A}})/\\sigma_{\\mathrm{A}} = (x_{\\mathrm{B, USD}} - \\mu_{\\mathrm{A}})/\\sigma_{\\mathrm{A}}$。\n预测：$\\hat{y}_{\\mathrm{T3}} = \\hat{w} z_{\\mathrm{T3}} + \\hat{c}$。\n让我们分析这个预测：\n$\\hat{y}_{\\mathrm{T3}} = (\\theta_{1}\\sigma_{\\mathrm{A}})\\left(\\frac{x_{\\mathrm{B, USD}} - \\mu_{\\mathrm{A}}}{\\sigma_{\\mathrm{A}}}\\right) + (\\theta_{0} + \\theta_{1}\\mu_{\\mathrm{A}})$\n$\\hat{y}_{\\mathrm{T3}} = \\theta_{1}(x_{\\mathrm{B, USD}} - \\mu_{\\mathrm{A}}) + \\theta_{0} + \\theta_{1}\\mu_{\\mathrm{A}}$\n$\\hat{y}_{\\mathrm{T3}} = \\theta_{1}x_{\\mathrm{B, USD}} - \\theta_{1}\\mu_{\\mathrm{A}} + \\theta_{0} + \\theta_{1}\\mu_{\\mathrm{A}} = \\theta_{0} + \\theta_{1}x_{\\mathrm{B, USD}} = y_{\\mathrm{B}}$。\n策略 T3 的预测与真实标签完全相同。因此，T3 的 RMSE 在所有情况下都将为 $0$。这表明，如果域偏移是一个已知的、可逆的变换，那么在特征工程之前应用逆变换，对于一个无噪声的线性模型可以产生完美的预测。\n\n现在我们计算所有策略和测试用例的 RMSE。\n\n**情况 1：$r=0.9$, $x_{\\mathrm{B, USD}}=[\\,15.0,\\,22.5,\\,60.0,\\,80.0\\,]$**\n$x_{\\mathrm{B, EUR}} = [\\,13.5,\\, 20.25,\\, 54.0,\\, 72.0\\,]$\n$y_{\\mathrm{B}} = [\\,22.0,\\, 31.0,\\, 76.0,\\, 100.0\\,]$\n- T1：RMSE $\\approx 11.2345$\n- T2：RMSE $\\approx 9.0733$\n- T3：RMSE $= 0.0$\n\n**情况 2：$r=1.0$, $x_{\\mathrm{B, USD}}=[\\,10.0,\\,30.0,\\,50.0\\,]$**\n这里，$r=1.0$，因此 $x_{\\mathrm{B, EUR}} = x_{\\mathrm{B, USD}}$。这代表数据分布发生了变化，但没有尺度变化。\n$x_{\\mathrm{B, EUR}} = [\\,10.0,\\, 30.0,\\, 50.0\\,]$\n$y_{\\mathrm{B}} = [\\,16.0,\\, 40.0,\\, 64.0\\,]$\n- T1：$z_{\\mathrm{T1}} = (x_{\\mathrm{B, USD}} - \\mu_{\\mathrm{A}})/\\sigma_{\\mathrm{A}}$。这与策略 T3 中的变换相同。如 T3 所示，这将产生完美的预测。RMSE $= 0.0$。\n- T2：该策略使用测试集本身的均值和标准差，这与 $\\mu_{\\mathrm{A}}$ 和 $\\sigma_{\\mathrm{A}}$ 不同。因此，它将导致非零误差。RMSE $\\approx 3.7919$。\n- T3：由于 $r=1.0$，$z_{\\mathrm{T3}} = (x_{\\mathrm{B, EUR}}/1.0 - \\mu_{\\mathrm{A}})/\\sigma_{\\mathrm{A}} = z_{\\mathrm{T1}}$。RMSE $= 0.0$。\n\n**情况 3：$r=1.15$, $x_{\\mathrm{B, USD}}=[\\,12.0,\\,25.0,\\,40.0,\\,55.0,\\,70.0\\,]$**\n测试集 $x_{\\mathrm{B, USD}}$ 与训练集 $x_{\\mathrm{A, USD}}$ 相同。\n$x_{\\mathrm{B, EUR}} = r \\cdot x_{\\mathrm{A, USD}}$\n- T1：将产生误差，因为它将缩放后的值视为原始值。RMSE $\\approx 5.7008$。\n- T2：让我们分析这种情况。$\\mu_{\\mathrm{B}} = \\mathrm{mean}(r \\cdot x_{\\mathrm{A, USD}}) = r \\mu_{\\mathrm{A}}$。$\\sigma_{\\mathrm{B}} = \\mathrm{std}(r \\cdot x_{\\mathrm{A, USD}}) = r \\sigma_{\\mathrm{A}}$。\n  $z_{\\mathrm{T2}} = (x_{\\mathrm{B, EUR}} - \\mu_{\\mathrm{B}})/\\sigma_{\\mathrm{B}} = (r x_{\\mathrm{A, USD}} - r \\mu_{\\mathrm{A}})/(r \\sigma_{\\mathrm{A}}) = (x_{\\mathrm{A, USD}} - \\mu_{\\mathrm{A}})/\\sigma_{\\mathrm{A}} = z_{\\mathrm{A}}$。\n  测试集的标准化输入与训练集的标准化输入相同。由于底层的美元价格以及因此的标签也相同，预测值 $\\hat{y}_{\\mathrm{T2}}$ 将与训练标签 $y_{\\mathrm{A}} = y_{\\mathrm{B}}$ 相同。因此，误差为零。RMSE $= 0.0$。\n- T3：与往常一样，RMSE $= 0.0$。\n\n**情况 4：$r=0.5$, $x_{\\mathrm{B, USD}}=[\\,5.0,\\,15.0,\\,75.0,\\,120.0\\,]$**\n$x_{\\mathrm{B, EUR}} = [\\,2.5,\\, 7.5,\\, 37.5,\\, 60.0\\,]$\n$y_{\\mathrm{B}} = [\\,10.0,\\, 22.0,\\, 94.0,\\, 148.0\\,]$\n- T1：RMSE $\\approx 25.1017$\n- T2：RMSE $\\approx 18.0664$\n- T3：RMSE $= 0.0$\n\n结果阐明了一个关键原则：明确地处理已知的域偏移来源（如货币换算）优于通用的适应策略（如按域重新标准化）或完全忽略偏移。策略 T3 正确地反转了已知的变换，在这种理想化的无噪声环境中表现稳健并实现了完美性能。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and evaluates three feature-normalization strategies\n    for a linear regression model under a currency-based domain shift.\n    \"\"\"\n    \n    # Define ground truth parameters and training data\n    theta_0 = 4.0\n    theta_1 = 1.2\n    x_train_usd = np.array([12.0, 25.0, 40.0, 55.0, 70.0])\n\n    # Define the four test cases\n    test_cases = [\n        # Case 1 (general domain shift)\n        {\"r\": 0.9, \"x_usd\": np.array([15.0, 22.5, 60.0, 80.0])},\n        # Case 2 (no domain shift)\n        {\"r\": 1.0, \"x_usd\": np.array([10.0, 30.0, 50.0])},\n        # Case 3 (distributional match under scaling)\n        {\"r\": 1.15, \"x_usd\": np.array([12.0, 25.0, 40.0, 55.0, 70.0])},\n        # Case 4 (strong domain shift)\n        {\"r\": 0.5, \"x_usd\": np.array([5.0, 15.0, 75.0, 120.0])},\n    ]\n\n    # --- Training Phase ---\n    # The problem specifies using population definitions for mean and std dev.\n    # np.mean() and np.std(ddof=0) compute these.\n    mu_A = np.mean(x_train_usd)\n    sigma_A = np.std(x_train_usd) # default ddof=0 is population std dev\n\n    # The training labels are generated by a perfect linear model without noise.\n    # y = theta_0 + theta_1 * x_usd\n    # The feature x_usd is related to the standardized feature z by x_usd = sigma_A * z + mu_A.\n    # Substituting, we get y = theta_0 + theta_1 * (sigma_A * z + mu_A)\n    # y = (theta_0 + theta_1 * mu_A) + (theta_1 * sigma_A) * z\n    # Since the OLS fit on this noiseless data will be perfect, the learned coefficients\n    # for the model y_hat = c_hat + w_hat * z will be exactly:\n    c_hat = theta_0 + theta_1 * mu_A\n    w_hat = theta_1 * sigma_A\n\n    results = []\n\n    # --- Testing Phase ---\n    for case in test_cases:\n        r = case[\"r\"]\n        x_test_usd = case[\"x_usd\"]\n        \n        # Observed features in the test domain (EUR)\n        x_test_eur = r * x_test_usd\n        \n        # Ground-truth labels, always based on USD price\n        y_test_true = theta_0 + theta_1 * x_test_usd\n        \n        # --- Strategy T1 (train-statistics on observed) ---\n        z_t1 = (x_test_eur - mu_A) / sigma_A\n        y_hat_t1 = w_hat * z_t1 + c_hat\n        rmse_t1 = np.sqrt(np.mean((y_hat_t1 - y_test_true)**2))\n        results.append(rmse_t1)\n        \n        # --- Strategy T2 (per-domain standardization) ---\n        mu_B = np.mean(x_test_eur)\n        sigma_B = np.std(x_test_eur) # ddof=0 for population std dev\n        \n        # Handle the case of zero standard deviation, although not expected here\n        if sigma_B > 1e-9:\n            z_t2 = (x_test_eur - mu_B) / sigma_B\n        else:\n            z_t2 = np.zeros_like(x_test_eur)\n            \n        y_hat_t2 = w_hat * z_t2 + c_hat\n        rmse_t2 = np.sqrt(np.mean((y_hat_t2 - y_test_true)**2))\n        results.append(rmse_t2)\n        \n        # --- Strategy T3 (currency-aware then train-statistics) ---\n        # Reconstruct the feature in the original domain (USD)\n        x_test_usd_reconstructed = x_test_eur / r\n        z_t3 = (x_test_usd_reconstructed - mu_A) / sigma_A\n        y_hat_t3 = w_hat * z_t3 + c_hat\n        rmse_t3 = np.sqrt(np.mean((y_hat_t3 - y_test_true)**2))\n        results.append(rmse_t3)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3160364"}]}