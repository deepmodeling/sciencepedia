{"hands_on_practices": [{"introduction": "贝叶斯分类器是统计决策理论的基石，它为我们提供了在给定数据下做出最优分类决策的理论框架。本练习将指导你从基本原理出发，针对一个由高斯分布描述的分类问题，推导出贝叶斯最优决策规则，从而揭示概率分布与最优决策边界之间的深刻联系，并引出充分统计量这一核心概念。[@problem_id:3166549]", "problem": "考虑一个二元分类问题，其类别标签为 $Y \\in \\{0,1\\}$，观测值为 $X \\in \\mathbb{R}$。类别先验概率为 $\\pi_{0} = \\mathbb{P}(Y=0)$ 和 $\\pi_{1} = \\mathbb{P}(Y=1)$，且 $\\pi_{0} + \\pi_{1} = 1$。假设类条件密度属于一个以均值为索引、具有公共已知方差的单参数自然指数族。具体来说，假设对于参数 $\\mu_{0}, \\mu_{1} \\in \\mathbb{R}$ 和一个已知的 $\\sigma^{2} > 0$，有\n$$\nX \\mid Y=y \\sim \\mathcal{N}(\\mu_{y}, \\sigma^{2})\n$$\n你将分析其充分性，并用充分统计量推导出贝叶斯最优决策。\n\n仅使用概率论和统计决策理论中的基本定义（例如，充分性的因子分解定理以及将贝叶斯分类器定义为最大化后验概率的决策），完成以下任务：\n\n1. 证明该密度族可以写成自然指数族的形式，找出一个关于 $\\mu$ 的充分统计量 $T(X)$，并解释为什么在该单参数族的标准正则性条件下，$T(X)$ 是最小充分统计量。\n\n2. 确定在每个类别 $Y=y$ 下，$T(X)$ 的分布。\n\n3. 在0-1损失下，通过比较后验概率推导出贝叶斯最优决策规则，并证明该规则对 $X$ 的依赖仅通过充分统计量 $T(X)$。\n\n4. 具体到以下数值指定的案例：$\\mu_{0} = 0$, $\\mu_{1} = 3$, $\\sigma^{2} = 1$, $\\pi_{0} = \\frac{13}{20}$, $\\pi_{1} = \\frac{7}{20}$。对于此案例，计算贝叶斯最优阈值 $t^{\\ast}$，使得决策规则为：当且仅当 $T(X) > t^{\\ast}$ 时，预测 $Y=1$，否则预测 $Y=0$。将最终阈值表示为单个封闭形式的解析表达式。不要进行四舍五入。", "solution": "所述问题具有科学依据、提法恰当且客观。它植根于统计决策理论的基本原理，由一系列逻辑关联的任务组成，可以根据所提供的信息推导出唯一且有意义的解。因此，该问题是有效的，我将提供完整的解答。\n\n解答分为四个部分，对应问题描述中的四个任务。\n\n**第一部分：指数族形式与最小充分性**\n\n给定观测值 $X \\in \\mathbb{R}$ 的类条件密度是高斯分布：$X \\mid Y=y \\sim \\mathcal{N}(\\mu_{y}, \\sigma^{2})$。我们考虑一个参数为 $\\mu$、已知方差为 $\\sigma^2 > 0$ 的此类密度。其概率密度函数 (PDF) 为\n$$\nf(x; \\mu) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left( -\\frac{(x-\\mu)^2}{2\\sigma^2} \\right)\n$$\n为了证明它属于单参数自然指数族，我们必须将其写成标准形式 $f(x; \\eta) = h(x) \\exp(\\eta T(x) - A(\\eta))$。我们展开指数中的项：\n$$\n-\\frac{(x-\\mu)^2}{2\\sigma^2} = -\\frac{x^2 - 2x\\mu + \\mu^2}{2\\sigma^2} = \\frac{\\mu}{\\sigma^2}x - \\frac{\\mu^2}{2\\sigma^2} - \\frac{x^2}{2\\sigma^2}\n$$\n将此代回 PDF 表达式，我们可以将各项重新整理如下：\n$$\nf(x; \\mu) = \\left[ \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{x^2}{2\\sigma^2}\\right) \\right] \\exp\\left( \\left(\\frac{\\mu}{\\sigma^2}\\right)x - \\frac{\\mu^2}{2\\sigma^2} \\right)\n$$\n该表达式与自然指数族的形式相匹配。通过比较各项，我们确定：\n- 充分统计量：$T(x) = x$。\n- 自然参数：$\\eta = \\eta(\\mu) = \\frac{\\mu}{\\sigma^2}$。\n- 基测度：$h(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{x^2}{2\\sigma^2}\\right)$。\n- 对数配分函数（累积量生成函数）：$A(\\eta) = \\frac{\\mu^2}{2\\sigma^2}$。为了将其表示为 $\\eta$ 的函数，我们对自然参数的关系进行逆运算：$\\mu = \\eta\\sigma^2$。代入后得到 $A(\\eta) = \\frac{(\\eta\\sigma^2)^2}{2\\sigma^2} = \\frac{\\eta^2 \\sigma^4}{2\\sigma^2} = \\frac{1}{2}\\eta^2\\sigma^2$。\n\n因此，参数 $\\mu$ 的一个充分统计量是 $T(X) = X$。\n\n关于最小充分性，单参数自然指数族的一个标准结果指出，如果自然参数空间 $\\mathcal{H} = \\{\\eta(\\mu) \\mid \\mu \\in \\mathbb{R}\\}$ 包含一个开区间，则对应的统计量 $T(X)$ 是最小充分的。在本例中，原始参数 $\\mu$ 属于 $\\mathbb{R}$。由于 $\\sigma^2$ 是一个固定的正常数，自然参数 $\\eta = \\mu/\\sigma^2$ 也覆盖了整个实数轴，即 $\\mathcal{H} = \\mathbb{R}$。因为 $\\mathbb{R}$ 是一个开集，所以正则性条件得到满足，我们得出结论：$T(X) = X$ 是 $\\mu$ 的一个最小充分统计量。\n\n**第二部分：充分统计量的分布**\n\n正如第一部分所确定的，$\\mu$ 的一个充分统计量是 $T(X) = X$。问题要求在类别标签 $Y=y$ 的条件下 $T(X)$ 的分布。根据定义，这也就是在 $Y=y$ 的条件下 $X$ 的分布。\n问题陈述直接给出了这些分布：\n$$\nX \\mid Y=y \\sim \\mathcal{N}(\\mu_{y}, \\sigma^{2})\n$$\n因此，充分统计量 $T(X)$ 的分布为：\n- 对于类别 $Y=0$：$T(X) \\mid (Y=0) \\sim \\mathcal{N}(\\mu_{0}, \\sigma^{2})$\n- 对于类别 $Y=1$：$T(X) \\mid (Y=1) \\sim \\mathcal{N}(\\mu_{1}, \\sigma^{2})$\n\n**第三部分：贝叶斯最优决策规则的推导**\n\n对于一个使用0-1损失的二元分类问题，贝叶斯最优决策规则旨在最小化错分概率。这等价于将一个观测值 $x$ 分配给具有最大后验概率的类别。决策规则是当且仅当以下条件成立时预测 $Y=1$：\n$$\n\\mathbb{P}(Y=1 \\mid X=x) > \\mathbb{P}(Y=0 \\mid X=x)\n$$\n使用贝叶斯定理，$\\mathbb{P}(Y=y \\mid X=x) = \\frac{p(x \\mid Y=y)\\mathbb{P}(Y=y)}{p(x)}$，其中 $p(x \\mid Y=y)$ 是类条件密度，$\\mathbb{P}(Y=y) = \\pi_y$ 是类别先验概率。该不等式变为：\n$$\n\\frac{p(x \\mid Y=1)\\pi_{1}}{p(x)} > \\frac{p(x \\mid Y=0)\\pi_{0}}{p(x)}\n$$\n由于 $p(x) > 0$，我们可以将其简化为一个涉及似然和先验的比较：\n$$\np(x \\mid Y=1)\\pi_{1} > p(x \\mid Y=0)\\pi_{0}\n$$\n根据通过因子分解定理得到的充分性定义，任何似然比 $\\frac{p(x|\\theta_1)}{p(x|\\theta_0)}$ 对数据 $x$ 的依赖仅通过充分统计量 $T(x)$。在这里，相关参数是 $\\mu_1$ 和 $\\mu_0$。因此，决策规则必须是 $T(X)=X$ 的函数。\n\n为了推导该规则的显式形式，我们对不等式两边取自然对数（一个严格单调递增函数）：\n$$\n\\ln(p(x \\mid Y=1)) + \\ln(\\pi_{1}) > \\ln(p(x \\mid Y=0)) + \\ln(\\pi_{0})\n$$\n正态分布 $\\mathcal{N}(\\mu, \\sigma^2)$ 的对数密度为 $\\ln(p(x; \\mu)) = -\\frac{(x-\\mu)^2}{2\\sigma^2} - \\frac{1}{2}\\ln(2\\pi\\sigma^2)$。将此代入不等式：\n$$\n-\\frac{(x-\\mu_{1})^2}{2\\sigma^2} - \\frac{1}{2}\\ln(2\\pi\\sigma^2) + \\ln(\\pi_{1}) > -\\frac{(x-\\mu_{0})^2}{2\\sigma^2} - \\frac{1}{2}\\ln(2\\pi\\sigma^2) + \\ln(\\pi_{0})\n$$\n项 $-\\frac{1}{2}\\ln(2\\pi\\sigma^2)$ 被消去。整理各项，我们得到：\n$$\n\\ln\\left(\\frac{\\pi_{1}}{\\pi_{0}}\\right) > \\frac{(x-\\mu_{1})^2 - (x-\\mu_{0})^2}{2\\sigma^2}\n$$\n两边乘以 $2\\sigma^2$ 并展开分子中的平方项：\n$$\n2\\sigma^2 \\ln\\left(\\frac{\\pi_{1}}{\\pi_{0}}\\right) > (x^2 - 2x\\mu_{1} + \\mu_{1}^2) - (x^2 - 2x\\mu_{0} + \\mu_{0}^2)\n$$\n$$\n2\\sigma^2 \\ln\\left(\\frac{\\pi_{1}}{\\pi_{0}}\\right) > -2x\\mu_{1} + 2x\\mu_{0} + \\mu_{1}^2 - \\mu_{0}^2\n$$\n我们分离出包含 $x$ 的项：\n$$\n2x(\\mu_{1} - \\mu_{0}) > \\mu_{1}^2 - \\mu_{0}^2 - 2\\sigma^2 \\ln\\left(\\frac{\\pi_{1}}{\\pi_{0}}\\right) = \\mu_{1}^2 - \\mu_{0}^2 + 2\\sigma^2 \\ln\\left(\\frac{\\pi_{0}}{\\pi_{1}}\\right)\n$$\n假设 $\\mu_{1} \\neq \\mu_{0}$，我们可以两边除以 $2(\\mu_{1} - \\mu_{0})$。如果 $\\mu_{1} > \\mu_{0}$，不等号方向保持不变：\n$$\nx > \\frac{\\mu_{1}^2 - \\mu_{0}^2}{2(\\mu_{1} - \\mu_{0})} + \\frac{2\\sigma^2}{2(\\mu_{1} - \\mu_{0})} \\ln\\left(\\frac{\\pi_{0}}{\\pi_{1}}\\right)\n$$\n化简第一项得到：\n$$\nx > \\frac{(\\mu_{1} - \\mu_{0})(\\mu_{1} + \\mu_{0})}{2(\\mu_{1} - \\mu_{0})} + \\frac{\\sigma^2}{\\mu_{1} - \\mu_{0}} \\ln\\left(\\frac{\\pi_{0}}{\\pi_{1}}\\right)\n$$\n$$\nx > \\frac{\\mu_{0} + \\mu_{1}}{2} + \\frac{\\sigma^2}{\\mu_{1} - \\mu_{0}} \\ln\\left(\\frac{\\pi_{0}}{\\pi_{1}}\\right)\n$$\n贝叶斯最优规则是当且仅当 $T(X) = X$ 超过此阈值时预测 $Y=1$。这明确地表明该决策对 $X$ 的依赖仅通过 $T(X)$。\n\n**第四部分：阈值的数值计算**\n\n我们得到给定的数值：$\\mu_{0} = 0$, $\\mu_{1} = 3$, $\\sigma^{2} = 1$, $\\pi_{0} = \\frac{13}{20}$ 及 $\\pi_{1} = \\frac{7}{20}$。决策规则是如果 $T(X) > t^{\\ast}$ 则预测 $Y=1$，其中 $t^{\\ast}$ 是在第三部分中推导出的阈值。\n$$\nt^{\\ast} = \\frac{\\mu_{0} + \\mu_{1}}{2} + \\frac{\\sigma^2}{\\mu_{1} - \\mu_{0}}\\ln\\left(\\frac{\\pi_{0}}{\\pi_{1}}\\right)\n$$\n我们将给定值代入此表达式。首先，我们计算各个组成部分：\n- $\\mu_{0} + \\mu_{1} = 0 + 3 = 3$\n- $\\mu_{1} - \\mu_{0} = 3 - 0 = 3$\n- $\\sigma^2 = 1$\n- $\\frac{\\pi_{0}}{\\pi_{1}} = \\frac{13/20}{7/20} = \\frac{13}{7}$\n\n将这些值代入 $t^{\\ast}$ 的公式：\n$$\nt^{\\ast} = \\frac{3}{2} + \\frac{1}{3}\\ln\\left(\\frac{13}{7}\\right)\n$$\n这就是贝叶斯最优决策阈值的最终封闭形式解析表达式。", "answer": "$$\n\\boxed{\\frac{3}{2} + \\frac{1}{3} \\ln\\left(\\frac{13}{7}\\right)}\n$$", "id": "3166549"}, {"introduction": "中心极限定理是估计样本均值等简单统计量分布的强大工具，但在实际应用中，我们往往更关心这些统计量的非线性函数的性质。本练习将介绍“德尔塔方法”（Delta Method），这是一种用于近似推断复杂估计量分布的关键技术，它对于在各种场景下构建置信区间和进行假设检验至关重要。[@problem_id:3166562]", "problem": "一个学习系统观测到由模型 $Y_{i} = \\theta + \\varepsilon_{i}$ 生成的 $n$ 个独立同分布 (i.i.d.) 样本 $\\{Y_{i}\\}_{i=1}^{n}$，其中 $\\theta \\in \\mathbb{R}$ 是一个固定但未知的参数，$\\varepsilon_{i}$ 是均值为 $0$ 且方差有限的 i.i.d. 噪声项。$\\theta$ 的估计量是样本均值 $\\hat{\\theta} = \\frac{1}{n} \\sum_{i=1}^{n} Y_{i}$。考虑非线性泛函 $\\psi(\\theta) = \\ln\\!\\big(1 + \\theta^{4}\\big)$。\n\n从基本原理出发——具体来说，是方差的定义和中心极限定理（CLT），该定理指出 $\\sqrt{n}\\,(\\hat{\\theta} - \\theta)$ 依分布收敛于一个均值为 $0$、方差等于 $\\operatorname{Var}(\\varepsilon_{1})$ 的正态随机变量，再结合一阶泰勒展开——推导 $\\psi(\\hat{\\theta})$ 围绕 $\\psi(\\theta)$ 的大样本（渐近）分布。\n\n然后，在保持 $n$ 和 $\\theta$ 不变的情况下，评估在以下两种情形中，这个渐近分布如何依赖于噪声分布：\n- 情形 (i)：$\\varepsilon_{i} \\sim \\mathcal{N}(0, \\sigma^{2})$，其中 $\\sigma^{2} = 1.44$。\n- 情形 (ii)：$\\varepsilon_{i} \\sim \\text{Laplace}(0, b)$，其中尺度参数 $b = 0.80$（因此 $\\operatorname{Var}(\\varepsilon_{1}) = 2 b^{2}$）。\n\n取 $n = 200$ 和 $\\theta = 1.20$。使用由CLT和泰勒展开证明的一阶Delta方法，计算情形(ii)与情形(i)中 $\\psi(\\hat{\\theta})$ 的渐近方差之比。将你的最终答案四舍五入到五位有效数字。无需单位。", "solution": "该问题陈述清晰，内容自洽，并且科学地基于统计学习和概率论的原理。我们可以继续进行推导和求解。\n\n学习系统观测到来自模型 $Y_{i} = \\theta + \\varepsilon_{i}$ 的 $n$ 个独立同分布 (i.i.d.) 样本 $\\{Y_{i}\\}_{i=1}^{n}$。参数 $\\theta$ 通过样本均值 $\\hat{\\theta} = \\frac{1}{n} \\sum_{i=1}^{n} Y_{i}$ 进行估计。噪声项 $\\varepsilon_{i}$ 是 i.i.d. 的，其 $E[\\varepsilon_{i}] = 0$ 且方差有限，我们记为 $V_{\\varepsilon} = \\operatorname{Var}(\\varepsilon_{1})$。\n\n首先，我们确定估计量 $\\hat{\\theta}$ 的性质。$\\hat{\\theta}$ 的期望是：\n$$ E[\\hat{\\theta}] = E\\left[\\frac{1}{n} \\sum_{i=1}^{n} Y_{i}\\right] = \\frac{1}{n} \\sum_{i=1}^{n} E[Y_{i}] = \\frac{1}{n} \\sum_{i=1}^{n} E[\\theta + \\varepsilon_{i}] = \\frac{1}{n} \\sum_{i=1}^{n} (\\theta + 0) = \\frac{1}{n} (n\\theta) = \\theta $$\n$\\hat{\\theta}$ 的方差是：\n$$ \\operatorname{Var}(\\hat{\\theta}) = \\operatorname{Var}\\left(\\frac{1}{n} \\sum_{i=1}^{n} Y_{i}\\right) = \\frac{1}{n^{2}} \\sum_{i=1}^{n} \\operatorname{Var}(Y_{i}) = \\frac{1}{n^{2}} \\sum_{i=1}^{n} \\operatorname{Var}(\\theta + \\varepsilon_{i}) = \\frac{1}{n^{2}} \\sum_{i=1}^{n} \\operatorname{Var}(\\varepsilon_{i}) = \\frac{n V_{\\varepsilon}}{n^{2}} = \\frac{V_{\\varepsilon}}{n} $$\n问题陈述指出，根据中心极限定理（CLT），标准化的估计量依分布收敛于一个正态分布：\n$$ \\sqrt{n}(\\hat{\\theta} - \\theta) \\xrightarrow{d} \\mathcal{N}(0, V_{\\varepsilon}) $$\n其中 $\\xrightarrow{d}$ 表示依分布收敛。\n\n我们感兴趣的是非线性泛函 $\\psi(\\hat{\\theta})$ 的渐近分布，其中 $\\psi(\\theta) = \\ln(1 + \\theta^{4})$。为了找到这个分布，我们使用Delta方法，该方法基于 $\\psi(\\hat{\\theta})$ 在真实参数 $\\theta$ 附近的一阶泰勒展开。对于接近 $\\theta$ 的 $\\hat{\\theta}$（当 $n$ 很大时成立），我们有：\n$$ \\psi(\\hat{\\theta}) \\approx \\psi(\\theta) + \\psi'(\\theta) (\\hat{\\theta} - \\theta) $$\n其中 $\\psi'(\\theta)$ 是 $\\psi(\\theta)$ 的一阶导数在 $\\theta$ 处的值。该导数为：\n$$ \\psi'(\\theta) = \\frac{d}{d\\theta}\\ln(1 + \\theta^{4}) = \\frac{1}{1 + \\theta^{4}} \\cdot \\frac{d}{d\\theta}(1 + \\theta^{4}) = \\frac{4\\theta^{3}}{1 + \\theta^{4}} $$\n整理泰勒展开式，我们得到：\n$$ \\psi(\\hat{\\theta}) - \\psi(\\theta) \\approx \\psi'(\\theta) (\\hat{\\theta} - \\theta) $$\n为了分析渐近分布，我们用 $\\sqrt{n}$ 来缩放这个差值：\n$$ \\sqrt{n}(\\psi(\\hat{\\theta}) - \\psi(\\theta)) \\approx \\psi'(\\theta) \\left( \\sqrt{n}(\\hat{\\theta} - \\theta) \\right) $$\n从CLT可知，我们知道右侧括号中项的渐近分布。由于对于固定的 $\\theta$，$\\psi'(\\theta)$ 是一个常数，我们可以应用 Slutsky 定理。$\\sqrt{n}(\\psi(\\hat{\\theta}) - \\psi(\\theta))$ 的渐近分布是常数 $\\psi'(\\theta)$ 与一个正态随机变量 $Z \\sim \\mathcal{N}(0, V_{\\varepsilon})$ 的乘积的分布。如果 $Z \\sim \\mathcal{N}(0, \\sigma^{2})$，那么 $cZ \\sim \\mathcal{N}(0, c^{2}\\sigma^{2})$。\n因此，大样本分布为：\n$$ \\sqrt{n}(\\psi(\\hat{\\theta}) - \\psi(\\theta)) \\xrightarrow{d} \\mathcal{N}\\left(0, [\\psi'(\\theta)]^{2} V_{\\varepsilon}\\right) $$\n这个结果意味着，对于大的 $n$，$\\psi(\\hat{\\theta})$ 近似服从均值为 $\\psi(\\theta)$、方差为 $\\frac{[\\psi'(\\theta)]^{2} V_{\\varepsilon}}{n}$ 的正态分布。这个方差是估计量 $\\psi(\\hat{\\theta})$ 的渐近方差，我们记为 $\\operatorname{Var}_{asym}(\\psi(\\hat{\\theta}))$。\n$$ \\operatorname{Var}_{asym}(\\psi(\\hat{\\theta})) = \\frac{[\\psi'(\\theta)]^{2} \\operatorname{Var}(\\varepsilon_{1})}{n} = \\frac{1}{n} \\left(\\frac{4\\theta^{3}}{1 + \\theta^{4}}\\right)^{2} V_{\\varepsilon} $$\n问题要求计算在两种不同情形下 $\\psi(\\hat{\\theta})$ 的渐近方差之比。设 $\\operatorname{Var}_{asym}^{(i)}(\\psi(\\hat{\\theta}))$ 和 $\\operatorname{Var}_{asym}^{(ii)}(\\psi(\\hat{\\theta}))$ 分别为情形(i)和情形(ii)中的渐近方差。噪声方差分别为 $V_{\\varepsilon}^{(i)}$ 和 $V_{\\varepsilon}^{(ii)}$。\n$$ \\operatorname{Var}_{asym}^{(i)}(\\psi(\\hat{\\theta})) = \\frac{1}{n} \\left(\\frac{4\\theta^{3}}{1 + \\theta^{4}}\\right)^{2} V_{\\varepsilon}^{(i)} $$\n$$ \\operatorname{Var}_{asym}^{(ii)}(\\psi(\\hat{\\theta})) = \\frac{1}{n} \\left(\\frac{4\\theta^{3}}{1 + \\theta^{4}}\\right)^{2} V_{\\varepsilon}^{(ii)} $$\n比率为：\n$$ \\frac{\\operatorname{Var}_{asym}^{(ii)}(\\psi(\\hat{\\theta}))}{\\operatorname{Var}_{asym}^{(i)}(\\psi(\\hat{\\theta}))} = \\frac{\\frac{1}{n} \\left(\\frac{4\\theta^{3}}{1 + \\theta^{4}}\\right)^{2} V_{\\varepsilon}^{(ii)}}{\\frac{1}{n} \\left(\\frac{4\\theta^{3}}{1 + \\theta^{4}}\\right)^{2} V_{\\varepsilon}^{(i)}} = \\frac{V_{\\varepsilon}^{(ii)}}{V_{\\varepsilon}^{(i)}} $$\n估计量 $\\psi(\\hat{\\theta})$ 的渐近方差之比简化为基础噪声分布的方差之比。具体的 $n=200$ 和 $\\theta=1.20$ 的值在此计算中是不需要的，因为包含它们的公共因子被约掉了。\n\n现在我们计算每种情形下的噪声方差。\n\n情形 (i)：噪声项是高斯的，$\\varepsilon_{i} \\sim \\mathcal{N}(0, \\sigma^{2})$，其中 $\\sigma^{2} = 1.44$。\n方差是直接给出的：\n$$ V_{\\varepsilon}^{(i)} = \\sigma^{2} = 1.44 $$\n\n情形 (ii)：噪声项服从Laplace分布，$\\varepsilon_{i} \\sim \\text{Laplace}(0, b)$，尺度参数 $b = 0.80$。该分布的方差给定为 $2b^{2}$。\n方差是：\n$$ V_{\\varepsilon}^{(ii)} = 2b^{2} = 2(0.80)^{2} = 2(0.64) = 1.28 $$\n\n最后，我们计算比率：\n$$ \\frac{V_{\\varepsilon}^{(ii)}}{V_{\\varepsilon}^{(i)}} = \\frac{1.28}{1.44} $$\n这个分数可以简化：\n$$ \\frac{1.28}{1.44} = \\frac{128}{144} = \\frac{16 \\times 8}{16 \\times 9} = \\frac{8}{9} $$\n作为小数，这是 $0.88888...$。四舍五入到五位有效数字，我们得到 $0.88889$。", "answer": "$$\\boxed{0.88889}$$", "id": "3166562"}, {"introduction": "许多现代机器学习算法，例如包含“随机失活”（Dropout）的算法，其本身就具有随机性。为了深刻理解并改进这些算法，我们需要从概率论的角度分析它们的行为。这个实践练习将深入探讨 Dropout 的核心机制，运用期望和方差等基本概念来分析它对训练损失和梯度更新的影响，从而让你对这一无处不在的正则化技术有更严谨的认识。[@problem_id:3166636]", "problem": "考虑在线性回归设置中，下述用于随机特征掩码的概率模型，该模型通常被称为 dropout。设特征维度为 $d$，随机特征向量 $X \\in \\mathbb{R}^d$ 的坐标是独立的，其中 $X_j \\sim \\mathcal{N}(0,\\sigma_j^2)$，对于 $j \\in \\{1,\\dots,d\\}$。定义对角协方差矩阵 $\\Sigma = \\operatorname{diag}(\\sigma_1^2,\\dots,\\sigma_d^2)$。设独立的掩码变量 $D_j \\sim \\operatorname{Bernoulli}(p)$，其中 $p \\in [0,1]$ 为固定值，且 $D_j$ 在 $j$ 上是独立的。设掩码后的特征向量为 $X' = D \\odot X$，其中 $\\odot$ 表示 Hadamard（逐元素）积。设标签为 $y = w_*^\\top X + \\epsilon$，其中 $\\epsilon \\sim \\mathcal{N}(0,\\sigma_\\epsilon^2)$ 与 $X$ 和 $D$ 独立。考虑一个固定的当前参数向量 $w \\in \\mathbb{R}^d$ 和固定的学习率 $\\eta > 0$。单个掩码观测的训练损失为 $L = \\tfrac{1}{2}(y - w^\\top X')^2$，单步随机梯度下降 (SGD) 更新为 $w_{\\text{new}} = w - \\eta \\nabla_w L = w + \\eta (y - w^\\top X') X'$。\n\n从期望、方差、独立性的核心定义以及正态分布的已知矩出发，推导下列数量的闭式表达式：\n- 期望 $\\mathbb{E}[L]$，作为 $p$、$w_*$、$w$、$(\\sigma_j^2)_{j=1}^d$ 和 $\\sigma_\\epsilon^2$ 的函数。\n- 使用全方差定律计算方差 $\\operatorname{Var}(L)$。\n- 期望更新向量 $\\mathbb{E}[\\Delta w]$，其中 $\\Delta w = w_{\\text{new}} - w$。\n- 逐坐标方差 $\\operatorname{Var}(\\Delta w_k)$，对于 $k \\in \\{1,\\dots,d\\}$。\n\n您的程序必须精确实现这些推导出的表达式，并为下面这套参数值测试集计算所要求的量。对于每个测试，$d=3$ 且所有向量都在 $\\mathbb{R}^3$ 中：\n\n- 测试 1 (一般情况): $p=0.5$, $w_* = [1.0,-0.5,2.0]$, $w = [0.8,-0.2,1.5]$, $(\\sigma_1,\\sigma_2,\\sigma_3) = [1.0,2.0,0.5]$, $\\sigma_\\epsilon = 0.3$, $\\eta=0.1$。\n- 测试 2 (无 dropout，无标签噪声): $p=1.0$, $w_* = [1.0,1.0,1.0]$, $w = [0.0,0.0,0.0]$, $(\\sigma_1,\\sigma_2,\\sigma_3) = [1.0,1.0,1.0]$, $\\sigma_\\epsilon = 0.0$, $\\eta=0.05$。\n- 测试 3 (完全 dropout，非零标签噪声): $p=0.0$, $w_* = [0.5,-1.0,1.5]$, $w = [1.0,0.0,-0.5]$, $(\\sigma_1,\\sigma_2,\\sigma_3) = [0.3,1.5,2.0]$, $\\sigma_\\epsilon = 0.7$, $\\eta=0.2$。\n- 测试 4 (一个零方差特征): $p=0.7$, $w_* = [2.0,-1.0,0.0]$, $w = [1.5,-0.5,0.5]$, $(\\sigma_1,\\sigma_2,\\sigma_3) = [0.0,1.0,2.0]$, $\\sigma_\\epsilon = 0.0$, $\\eta=0.15$。\n\n您的程序应生成单行输出，其中包含用方括号括起来的逗号分隔列表形式的结果。对于每个测试用例，按以下顺序输出一个包含四个元素的列表：$[\\mathbb{E}[L], \\operatorname{Var}(L), \\mathbb{E}[\\Delta w], \\operatorname{Var}(\\Delta w)]$，其中 $\\mathbb{E}[\\Delta w]$ 和 $\\operatorname{Var}(\\Delta w)$ 是包含逐坐标值的三元素列表。因此，最终打印的输出必须是一个包含四个列表（每个测试用例一个）的列表，并采用确切指定的结构。所有数值答案必须是浮点数或浮点数列表，不带百分号，也没有单位。程序必须是自包含的，并且不需要任何输入。", "solution": "该问题是适定的、有科学依据的、客观的，并包含唯一解所需的所有信息。我们开始推导所要求的四个量。\n\n该模型涉及三个随机性来源：特征向量 $X$、掩码向量 $D$ 和噪声 $\\epsilon$。它们是相互独立的。我们首先列出基础随机变量的关键性质，这些性质将在整个推导过程中使用。\n\n-   特征 $X_j \\sim \\mathcal{N}(0, \\sigma_j^2)$ 是独立的零均值正态变量。它们的矩为：$\\mathbb{E}[X_j]=0$，$\\mathbb{E}[X_j^2]=\\sigma_j^2$，$\\mathbb{E}[X_j^3]=0$，$\\mathbb{E}[X_j^4]=3\\sigma_j^4$。对于 $j \\neq k$，$\\mathbb{E}[X_j X_k] = \\mathbb{E}[X_j]\\mathbb{E}[X_k]=0$。\n-   掩码变量 $D_j \\sim \\operatorname{Bernoulli}(p)$ 是独立的。它们的矩为：$\\mathbb{E}[D_j]=p$，对于任何整数 $k \\ge 1$，$\\mathbb{E}[D_j^k]=p$，因为 $D_j \\in \\{0,1\\}$，且 $\\operatorname{Var}(D_j) = p(1-p)$。\n-   观测噪声 $\\epsilon \\sim \\mathcal{N}(0, \\sigma_\\epsilon^2)$ 的矩为 $\\mathbb{E}[\\epsilon]=0$ 和 $\\mathbb{E}[\\epsilon^2]=\\sigma_\\epsilon^2$。\n-   掩码后的特征为 $X'_j = D_j X_j$。由于 $D_j$ 和 $X_j$ 的独立性：\n    -   $\\mathbb{E}[X'_j] = \\mathbb{E}[D_j]\\mathbb{E}[X_j] = p \\cdot 0 = 0$。\n    -   $\\mathbb{E}[(X'_j)^2] = \\mathbb{E}[D_j^2 X_j^2] = \\mathbb{E}[D_j^2]\\mathbb{E}[X_j^2] = p\\sigma_j^2$。\n    -   对于 $j \\neq k$，$\\mathbb{E}[X'_j X'_k] = \\mathbb{E}[D_jX_j D_kX_k] = \\mathbb{E}[D_j]\\mathbb{E}[X_j]\\mathbb{E}[D_k]\\mathbb{E}[X_k]=0$。\n\n令 $\\Sigma = \\operatorname{diag}(\\sigma_1^2, \\dots, \\sigma_d^2)$ 为 $X$ 的对角协方差矩阵。\n\n### 期望损失 $\\mathbb{E}[L]$ 的推导\n损失为 $L = \\frac{1}{2}(y - w^\\top X')^2$。代入 $y = w_*^\\top X + \\epsilon$ 和 $X' = D \\odot X$：\n$$\nL = \\frac{1}{2} (w_*^\\top X + \\epsilon - w^\\top X')^2\n$$\n根据期望的线性性，我们有：\n$$\n\\mathbb{E}[L] = \\frac{1}{2} \\mathbb{E}[ (w_*^\\top X)^2 + \\epsilon^2 + (w^\\top X')^2 + 2(w_*^\\top X)\\epsilon - 2(w_*^\\top X)(w^\\top X') - 2\\epsilon(w^\\top X') ]\n$$\n我们计算每一项的期望：\n-   $\\mathbb{E}[\\epsilon^2]=\\sigma_\\epsilon^2$。\n-   包含单个因子 $\\epsilon$ 或单个非平方因子 $X_j$ 或 $X'_j$ 的项的期望为 0，因为 $\\mathbb{E}[\\epsilon]=0$ 且 $\\mathbb{E}[X]=\\mathbb{E}[X']=0$，并且所有变量都是独立的。因此，$\\mathbb{E}[(w_*^\\top X)\\epsilon]=0$ 且 $\\mathbb{E}[\\epsilon(w^\\top X')]=0$。\n-   $\\mathbb{E}[(w_*^\\top X)^2] = \\mathbb{E}[(\\sum_j w_{*j} X_j)^2] = \\sum_j w_{*j}^2 \\mathbb{E}[X_j^2] = \\sum_j w_{*j}^2 \\sigma_j^2 = w_*^\\top \\Sigma w_*$。\n-   $\\mathbb{E}[(w^\\top X')^2] = \\mathbb{E}[(\\sum_j w_j X'_j)^2] = \\sum_j w_j^2 \\mathbb{E}[(X'_j)^2] = \\sum_j w_j^2 p \\sigma_j^2 = p(w^\\top \\Sigma w)$。\n-   $\\mathbb{E}[(w_*^\\top X)(w^\\top X')] = \\mathbb{E}[(\\sum_j w_{*j} X_j)(\\sum_k w_k X'_k)] = \\sum_j w_{*j} w_j \\mathbb{E}[X_j X'_j] = \\sum_j w_{*j} w_j \\mathbb{E}[X_j D_j X_j] = \\sum_j w_{*j} w_j \\mathbb{E}[D_j]\\mathbb{E}[X_j^2] = \\sum_j w_{*j} w_j p \\sigma_j^2 = p(w_*^\\top \\Sigma w)$。\n\n将这些项合并，得到期望损失：\n$$\n\\mathbb{E}[L] = \\frac{1}{2} \\left( w_*^\\top \\Sigma w_* + \\sigma_\\epsilon^2 + p(w^\\top \\Sigma w) - 2p(w_*^\\top \\Sigma w) \\right)\n$$\n\n### 损失方差 $\\operatorname{Var}(L)$ 的推导\n我们使用全方差定律：$\\operatorname{Var}(L) = \\mathbb{E}[\\operatorname{Var}(L|D)] + \\operatorname{Var}(\\mathbb{E}[L|D])$。\n给定掩码 $D$，令 $w_D = w \\odot D$。损失为 $L = \\frac{1}{2}Z^2$，其中 $Z = (w_* - w_D)^\\top X + \\epsilon$。$Z$ 是独立正态变量的和，因此它服从正态分布，均值为 $\\mathbb{E}[Z]=0$，方差为 $\\sigma_Z^2 = \\operatorname{Var}(Z) = \\operatorname{Var}((w_* - w_D)^\\top X) + \\operatorname{Var}(\\epsilon) = (w_*-w_D)^\\top \\Sigma (w_*-w_D) + \\sigma_\\epsilon^2$。\n损失的条件期望为 $\\mathbb{E}[L|D] = \\frac{1}{2}\\mathbb{E}[Z^2|D] = \\frac{1}{2}\\sigma_Z^2$。\n条件方差为 $\\operatorname{Var}(L|D) = \\operatorname{Var}(\\frac{1}{2}Z^2|D) = \\frac{1}{4}\\operatorname{Var}(Z^2|D)$。因为 $Z/\\sigma_Z \\sim \\mathcal{N}(0,1)$，所以 $(Z/\\sigma_Z)^2 \\sim \\chi^2_1$，其方差为 $2$。因此 $\\operatorname{Var}(Z^2) = \\sigma_Z^4 \\operatorname{Var}((Z/\\sigma_Z)^2) = 2\\sigma_Z^4$。所以，$\\operatorname{Var}(L|D) = \\frac{1}{2}(\\sigma_Z^2)^2$。\n\n令 $S_D = (w_*-w_D)^\\top \\Sigma (w_*-w_D) = \\sum_j (w_{*j} - w_j D_j)^2 \\sigma_j^2$。则 $\\sigma_Z^2 = S_D + \\sigma_\\epsilon^2$。\n全方差的两个分量是：\n1.  $\\operatorname{Var}(\\mathbb{E}[L|D]) = \\operatorname{Var}(\\frac{1}{2}(S_D + \\sigma_\\epsilon^2)) = \\frac{1}{4}\\operatorname{Var}(S_D)$。令 $A_j = (w_{*j}-w_j D_j)^2\\sigma_j^2$。因为 $D_j$ 是独立的，所以 $A_j$ 是独立的。$S_D = \\sum_j A_j$。\n    $\\operatorname{Var}(S_D) = \\sum_j \\operatorname{Var}(A_j)$。\n    $A_j = \\sigma_j^2(w_{*j}^2 - 2w_{*j}w_j D_j + w_j^2 D_j^2) = \\sigma_j^2(w_{*j}^2 + (w_j^2 - 2w_{*j}w_j)D_j)$ 因为 $D_j^2=D_j$。\n    $\\operatorname{Var}(A_j) = \\sigma_j^4(w_j^2 - 2w_{*j}w_j)^2 \\operatorname{Var}(D_j) = \\sigma_j^4(w_j^2-2w_{*j}w_j)^2 p(1-p)$。\n    所以，$\\operatorname{Var}(\\mathbb{E}[L|D]) = \\frac{p(1-p)}{4} \\sum_j \\sigma_j^4(w_j^2 - 2w_{*j}w_j)^2$。\n2.  $\\mathbb{E}[\\operatorname{Var}(L|D)] = \\mathbb{E}[\\frac{1}{2}(\\sigma_Z^2)^2] = \\frac{1}{2}\\mathbb{E}[(S_D + \\sigma_\\epsilon^2)^2] = \\frac{1}{2}(\\mathbb{E}[S_D^2] + 2\\sigma_\\epsilon^2 \\mathbb{E}[S_D] + \\sigma_\\epsilon^4)$。\n    $\\mathbb{E}[S_D^2] = \\operatorname{Var}(S_D) + (\\mathbb{E}[S_D])^2$。\n    $\\mathbb{E}[S_D] = \\mathbb{E}[\\sum_j A_j] = \\sum_j \\mathbb{E}[A_j] = \\sum_j \\sigma_j^2(w_{*j}^2 - 2pw_{*j}w_j + pw_j^2) = w_*^\\top\\Sigma w_* - 2p w_*^\\top\\Sigma w + p w^\\top\\Sigma w$。\n    注意到 $\\mathbb{E}[S_D] + \\sigma_\\epsilon^2 = 2\\mathbb{E}[L]$。\n    $\\mathbb{E}[\\operatorname{Var}(L|D)] = \\frac{1}{2}(\\operatorname{Var}(S_D) + (\\mathbb{E}[S_D])^2 + 2\\sigma_\\epsilon^2\\mathbb{E}[S_D] + \\sigma_\\epsilon^4) = \\frac{1}{2}(\\operatorname{Var}(S_D) + (\\mathbb{E}[S_D] + \\sigma_\\epsilon^2)^2) = \\frac{1}{2}\\operatorname{Var}(S_D) + 2(\\mathbb{E}[L])^2$。\n\n合并各项：$\\operatorname{Var}(L) = (\\frac{1}{2}\\operatorname{Var}(S_D) + 2(\\mathbb{E}[L])^2) + \\frac{1}{4}\\operatorname{Var}(S_D) = 2(\\mathbb{E}[L])^2 + \\frac{3}{4}\\operatorname{Var}(S_D)$。\n$$\n\\operatorname{Var}(L) = 2(\\mathbb{E}[L])^2 + \\frac{3p(1-p)}{4} \\sum_{j=1}^d \\sigma_j^4(w_j^2 - 2w_{*j}w_j)^2\n$$\n\n### 期望更新向量 $\\mathbb{E}[\\Delta w]$ 的推导\n更新向量为 $\\Delta w = \\eta(y - w^\\top X')X'$。我们逐坐标计算其期望。\n$$\n\\mathbb{E}[\\Delta w_k] = \\eta \\mathbb{E}[(w_*^\\top X + \\epsilon - w^\\top X') X'_k] = \\eta (\\mathbb{E}[(w_*^\\top X)X'_k] + \\mathbb{E}[\\epsilon X'_k] - \\mathbb{E}[(w^\\top X')X'_k])\n$$\n-   $\\mathbb{E}[\\epsilon X'_k] = \\mathbb{E}[\\epsilon]\\mathbb{E}[X'_k] = 0$。\n-   $\\mathbb{E}[(w_*^\\top X)X'_k] = \\mathbb{E}[(\\sum_j w_{*j} X_j)(D_k X_k)] = \\sum_j w_{*j} \\mathbb{E}[X_j D_k X_k]$。只有 $j=k$ 的项不为零，得到 $w_{*k}\\mathbb{E}[D_k X_k^2] = w_{*k} \\mathbb{E}[D_k]\\mathbb{E}[X_k^2] = w_{*k} p \\sigma_k^2$。\n-   $\\mathbb{E}[(w^\\top X')X'_k] = \\mathbb{E}[(\\sum_j w_j X'_j)X'_k] = \\sum_j w_j \\mathbb{E}[X'_j X'_k]$。只有 $j=k$ 的项不为零，得到 $w_k \\mathbb{E}[(X'_k)^2] = w_k p \\sigma_k^2$。\n合并得到：\n$$\n\\mathbb{E}[\\Delta w_k] = \\eta (w_{*k}p\\sigma_k^2 - w_k p\\sigma_k^2) = \\eta p \\sigma_k^2 (w_{*k} - w_k)\n$$\n\n### 逐坐标更新方差 $\\operatorname{Var}(\\Delta w_k)$ 的推导\n$\\operatorname{Var}(\\Delta w_k) = \\mathbb{E}[(\\Delta w_k)^2] - (\\mathbb{E}[\\Delta w_k])^2$.\n我们计算 $\\mathbb{E}[(\\Delta w_k)^2] = \\eta^2 \\mathbb{E}[ (y-w^\\top X')^2 (X'_k)^2 ]$。\n$$\n\\mathbb{E}[(\\Delta w_k)^2] = \\eta^2 \\mathbb{E}[ ((w_*-w_D)^\\top X+\\epsilon)^2 (D_k X_k)^2 ]\n$$\n$D_k^2 = D_k$, 所以 $(X'_k)^2 = D_k X_k^2$.\n$$\n\\mathbb{E}[(\\Delta w_k)^2] = \\eta^2 \\mathbb{E}[ \\mathbb{E}[((w_*-w_D)^\\top X+\\epsilon)^2 D_k X_k^2 | D] ]\n= \\eta^2 \\mathbb{E}_D[ D_k \\mathbb{E}_{X,\\epsilon}[((w_*-w_D)^\\top X+\\epsilon)^2 X_k^2 | D] ]\n$$\n内层期望是 $\\mathbb{E}[Z^2 X_k^2|D]$，其中 $Z=(w_*-w_D)^\\top X + \\epsilon$。$\\mathbb{E}[Z^2 X_k^2|D] = \\mathbb{E}[Z^2|D]\\mathbb{E}[X_k^2|D] + 2(\\operatorname{Cov}(Z,X_k|D))^2$。\n$\\mathbb{E}[Z^2|D] = \\sigma_Z^2 = S_D + \\sigma_\\epsilon^2$。$\\mathbb{E}[X_k^2|D]=\\sigma_k^2$。\n$\\operatorname{Cov}(Z,X_k|D) = \\mathbb{E}[ZX_k|D] = \\mathbb{E}[((w_*-w_D)^\\top X)X_k|D] = (w_{*k}-w_k D_k)\\sigma_k^2$。\n所以内层期望是 $(S_D+\\sigma_\\epsilon^2)\\sigma_k^2 + 2((w_{*k}-w_k D_k)\\sigma_k^2)^2$。\n取关于$D$的期望：\n$\\mathbb{E}[(\\Delta w_k)^2] = \\eta^2 \\mathbb{E}_D[D_k ( (S_D+\\sigma_\\epsilon^2)\\sigma_k^2 + 2((w_{*k}-w_k D_k)\\sigma_k^2)^2 ) ]$\n$= \\eta^2 (\\mathbb{E}_D[D_k S_D]\\sigma_k^2 + p\\sigma_\\epsilon^2\\sigma_k^2 + 2\\sigma_k^4\\mathbb{E}_D[D_k(w_{*k}-w_k D_k)^2])$\n$\\mathbb{E}_D[D_k(w_{*k}-w_k D_k)^2] = \\mathbb{E}_D[D_k(w_{*k}-w_k)^2] = p(w_{*k}-w_k)^2$。\n$\\mathbb{E}_D[D_k S_D] = \\mathbb{E}_D[D_k \\sum_j (w_{*j}-w_j D_j)^2 \\sigma_j^2] = p(w_{*k}-w_k)^2\\sigma_k^2 + \\sum_{j\\neq k}\\sigma_j^2 \\mathbb{E}_D[D_k(w_{*j}-w_j D_j)^2]$\n$= p(w_{*k}-w_k)^2\\sigma_k^2 + p\\sum_{j\\neq k}\\sigma_j^2 (w_{*j}^2 - 2pw_{*j}w_j + pw_j^2)$。\n$\\mathbb{E}[(\\Delta w_k)^2] = \\eta^2 p [ ( (w_{*k}-w_k)^2\\sigma_k^2 + \\sum_{j\\neq k} \\dots )\\sigma_k^2 + \\sigma_\\epsilon^2\\sigma_k^2 + 2\\sigma_k^4(w_{*k}-w_k)^2 ]$\n$= \\eta^2 p [ 3(w_{*k}-w_k)^2\\sigma_k^4 + \\sigma_\\epsilon^2\\sigma_k^2 + \\sigma_k^2\\sum_{j\\neq k}(w_{*j}^2 - 2pw_{*j}w_j + pw_j^2)\\sigma_j^2 ]$\n$\\operatorname{Var}(\\Delta w_k) = \\mathbb{E}[(\\Delta w_k)^2] - (\\mathbb{E}[\\Delta w_k])^2 = \\mathbb{E}[(\\Delta w_k)^2] - \\eta^2 p^2\\sigma_k^4(w_{*k}-w_k)^2$\n$= \\eta^2 p [ (3-p)(w_{*k}-w_k)^2\\sigma_k^4 + \\sigma_\\epsilon^2\\sigma_k^2 + \\sigma_k^2\\sum_{j\\neq k}(w_{*j}^2 - 2pw_{*j}w_j + pw_j^2)\\sigma_j^2 ]$\n此表达式与python代码中实现的公式相符。\n$$\n\\operatorname{Var}(\\Delta w_k) = \\eta^2 p \\left( (3-p)(w_{*k}-w_k)^2\\sigma_k^4 + \\sigma_k^2\\sigma_\\epsilon^2 + \\sigma_k^2 \\sum_{j\\neq k} (w_{*j}^2 - 2pw_{*j}w_j + pw_j^2)\\sigma_j^2 \\right)\n$$", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem for the given test suite of parameters.\n    Implements the derived closed-form expressions for the requested quantities.\n    \"\"\"\n    \n    test_cases = [\n        {'p': 0.5, 'w_star': [1.0, -0.5, 2.0], 'w': [0.8, -0.2, 1.5], 'sigmas': [1.0, 2.0, 0.5], 'sigma_eps': 0.3, 'eta': 0.1},\n        {'p': 1.0, 'w_star': [1.0, 1.0, 1.0], 'w': [0.0, 0.0, 0.0], 'sigmas': [1.0, 1.0, 1.0], 'sigma_eps': 0.0, 'eta': 0.05},\n        {'p': 0.0, 'w_star': [0.5, -1.0, 1.5], 'w': [1.0, 0.0, -0.5], 'sigmas': [0.3, 1.5, 2.0], 'sigma_eps': 0.7, 'eta': 0.2},\n        {'p': 0.7, 'w_star': [2.0, -1.0, 0.0], 'w': [1.5, -0.5, 0.5], 'sigmas': [0.0, 1.0, 2.0], 'sigma_eps': 0.0, 'eta': 0.15},\n    ]\n\n    results = []\n    \n    for params in test_cases:\n        p = params['p']\n        w_star = np.array(params['w_star'])\n        w = np.array(params['w'])\n        sigmas = np.array(params['sigmas'])\n        sigma_eps = params['sigma_eps']\n        eta = params['eta']\n        \n        d = len(w_star)\n        sigma_sq = sigmas**2\n        sigma_eps_sq = sigma_eps**2\n        \n        # --- 1. Expectation of the Loss: E[L] ---\n        w_star_S_w_star = np.sum(w_star**2 * sigma_sq)\n        w_S_w = np.sum(w**2 * sigma_sq)\n        w_star_S_w = np.sum(w_star * w * sigma_sq)\n        \n        exp_L = 0.5 * (w_star_S_w_star + sigma_eps_sq + p * (w_S_w - 2 * w_star_S_w))\n\n        # --- 2. Variance of the Loss: Var(L) ---\n        C_j_sq = sigma_sq**2 * (w**2 - 2 * w_star * w)**2\n        var_S_D = p * (1 - p) * np.sum(C_j_sq)\n        var_L = 2 * exp_L**2 + 0.75 * var_S_D\n\n        # --- 3. Expected Update Vector: E[Δw] ---\n        v = w_star - w\n        exp_delta_w = eta * p * sigma_sq * v\n        \n        # --- 4. Per-coordinate Update Variance: Var(Δw_k) ---\n        var_delta_w = np.zeros(d)\n        if p > 0: # If p=0, variance is 0\n            S_j_terms = (w_star**2 - 2 * p * w_star * w + p * w**2) * sigma_sq\n            sum_S_j = np.sum(S_j_terms)\n            for k in range(d):\n                v_k = w_star[k] - w[k]\n                sigma_k_sq = sigma_sq[k]\n                sigma_k_4 = sigma_k_sq**2\n\n                sum_S_j_neq_k = sum_S_j - S_j_terms[k]\n                \n                term1 = (3 - p) * v_k**2 * sigma_k_4\n                term2 = sigma_k_sq * sigma_eps_sq\n                term3 = sigma_k_sq * sum_S_j_neq_k\n                \n                var_delta_w[k] = (eta**2 * p) * (term1 + term2 + term3)\n\n        result_case = [\n            exp_L,\n            var_L,\n            exp_delta_w.tolist(),\n            var_delta_w.tolist()\n        ]\n        results.append(result_case)\n\n    # Format and print the final output exactly as required.\n    # The default str() for lists includes spaces, which is standard.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3166636"}]}