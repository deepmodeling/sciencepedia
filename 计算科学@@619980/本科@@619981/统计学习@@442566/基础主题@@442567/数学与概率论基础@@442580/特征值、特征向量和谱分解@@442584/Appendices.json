{"hands_on_practices": [{"introduction": "谱分解不仅是一个抽象的数学概念，更是一种强大的诊断工具，能帮助我们深入理解数据。我们将探索的第一个应用是诊断多重共线性，即特征之间高度相关的情况，这会破坏线性模型的稳定性。通过这个练习，你将学习如何利用相关矩阵的特征值来揭示这些隐藏的依赖关系，并精确定位导致问题的特征，这是构建稳健统计模型的关键一步。[@problem_id:3117789]", "problem": "给定一系列表示为实数矩阵的多元数据集，要求您使用谱分解，通过经验相关矩阵的小特征值来识别近似共线的特征，从而诊断多重共线性，然后根据相应的特征向量提出要舍弃的特征。此任务的背景是统计学习，其中特征多重共线性会降低模型的可解释性和数值稳定性。该任务要求从核心定义和经过充分检验的事实出发进行形式化推导，然后进行算法实现。\n\n从以下基本概念开始：\n- 数据集是一个矩阵 $X \\in \\mathbb{R}^{n \\times p}$，包含 $n$ 个观测值和 $p$ 个特征，其列为 $x_{1}, \\dots, x_{p}$。\n- 特征 $j$ 的经验均值为 $\\bar{x}_{j} = \\frac{1}{n} \\sum_{i=1}^{n} X_{ij}$。\n- 特征 $j$ 的无偏经验方差为 $s_{j}^{2} = \\frac{1}{n-1} \\sum_{i=1}^{n} \\left(X_{ij} - \\bar{x}_{j}\\right)^{2}$。经验标准差为 $s_{j} = \\sqrt{s_{j}^{2}}$。\n- 标准化设计矩阵为 $Z \\in \\mathbb{R}^{n \\times p}$，对于所有 $s_{j} > 0$ 的特征，其元素为 $Z_{ij} = \\frac{X_{ij} - \\bar{x}_{j}}{s_{j}}$。\n- 经验相关矩阵定义为\n$$\nR = \\frac{1}{n - 1} Z^{\\top} Z \\in \\mathbb{R}^{p \\times p}.\n$$\n这个矩阵 $R$ 是对称、半正定的，并且可以进行谱分解 $R = V \\Lambda V^{\\top}$，其中 $V \\in \\mathbb{R}^{p \\times p}$ 的列是标准正交的（即特征向量），$\\Lambda = \\mathrm{diag}(\\lambda_{1}, \\dots, \\lambda_{p})$ 包含相应的特征值。\n- 在主成分分析（PCA）的背景下，每个特征值 $\\lambda_{k}$ 等于 $Z$ 沿特征向量 $v_{k}$ 方向的方差。小特征值表示方差小的方向，这对应于特征之间近似冗余的线性关系。\n\n您的程序必须为每个测试用例实现以下步骤：\n1. 使用上述定义将每个特征标准化为零均值和单位方差。如果任何特征的经验标准差 $s_{j} = 0$，则立即标记为待舍弃，并将其从后续的谱分析中排除。在计算 $s_{j}^{2}$ 和 $R$ 时，均使用无偏分母 $n-1$。\n2. 对剩余特征计算经验相关矩阵 $R = \\frac{1}{n-1} Z^{\\top} Z$。\n3. 计算谱分解 $R = V \\Lambda V^{\\top}$，得到特征值 $\\lambda_{k}$ 和特征向量 $v_{k}$。\n4. 给定一个小特征值阈值 $\\tau > 0$，找出所有满足 $\\lambda_{k} < \\tau$ 的索引 $k$。对于每个这样的 $k$，找到在 $v_{k}$ 上具有最大绝对载荷 $|v_{k,j}|$ 的特征索引 $j^{\\star}$，并建议舍弃特征 $j^{\\star}$。如果在数值容差 $\\epsilon$ 内有多个特征并列获得最大载荷，则选择这些并列特征中索引最大的一个来确定性地打破平局。\n5. 将步骤 4 中所有建议舍弃的特征与步骤 1 中检测到的任何零方差特征汇总在一起，去除重复项，并将结果列表按升序排序。索引必须相对于 $X$ 的原始列使用从零开始的索引进行报告。\n\n科学真实性与适用性：此过程基于一个事实，即相关（或协方差）矩阵的小特征值源于特征空间中方差接近于零的方向，这与特征间的近线性依赖关系一致，而后者是统计学习中多重共线性的一个标志。\n\n此问题不涉及角度单位和物理单位。不需要百分比。\n\n测试套件：\n对于每个测试用例，程序必须使用提供的 $X$、阈值 $\\tau$ 和平局容差 $\\epsilon$。矩阵如下：\n\n- 测试用例 1（近似重复的特征；理想路径）：\n$$\nX^{(1)} =\n\\begin{bmatrix}\n0  & 0.00 + 0.00  & 1.00 \\\\\n1  & 1.00 + 0.05  & -0.50 \\\\\n2  & 2.00 - 0.02  & 0.70 \\\\\n3  & 3.00 + 0.10  & -1.20 \\\\\n4  & 4.00 - 0.05  & 0.30 \\\\\n5  & 5.00 + 0.03  & 0.80 \\\\\n6  & 6.00 - 0.08  & -0.90 \\\\\n7  & 7.00 + 0.02  & 1.10 \\\\\n8  & 8.00 + 0.00  & -0.40 \\\\\n9  & 9.00 - 0.07  & 0.20\n\\end{bmatrix}\n\\quad\n\\tau^{(1)} = 0.05\n\\quad\n\\epsilon^{(1)} = 10^{-12}.\n$$\n解释：第二个特征约等于第一个特征加上少量噪声；第三个特征不相关。\n\n- 测试用例 2（近似独立的特征；未检测到多重共线性的边界条件）：\n$$\nX^{(2)} =\n\\begin{bmatrix}\n0  & 1.10  & 0.50 \\\\\n1  & -0.90  & 0.30 \\\\\n2  & 0.70  & -0.60 \\\\\n3  & -0.30  & 0.70 \\\\\n4  & 0.20  & -0.80 \\\\\n5  & -0.40  & 0.10 \\\\\n6  & 0.80  & 0.20 \\\\\n7  & -0.50  & -0.30 \\\\\n8  & 0.60  & 0.40 \\\\\n9  & -0.70  & -0.20 \\\\\n10  & 0.90  & 0.00 \\\\\n11  & -1.00  & 0.50\n\\end{bmatrix}\n\\quad\n\\tau^{(2)} = 0.10\n\\quad\n\\epsilon^{(2)} = 10^{-12}.\n$$\n\n- 测试用例 3（一个特征是另外两个特征的近似线性组合；带有少量噪声的边缘情况）：\n$$\nX^{(3)} =\n\\begin{bmatrix}\n2.00  & -1.00  & 2.00 + (-1.00) + 0.01 \\\\\n3.00  & -1.10  & 3.00 + (-1.10) - 0.02 \\\\\n4.00  & -0.90  & 4.00 + (-0.90) + 0.00 \\\\\n5.50  & -1.20  & 5.50 + (-1.20) + 0.03 \\\\\n6.10  & -0.80  & 6.10 + (-0.80) - 0.01 \\\\\n7.00  & -1.05  & 7.00 + (-1.05) + 0.02 \\\\\n8.20  & -1.00  & 8.20 + (-1.00) - 0.02 \\\\\n9.00  & -0.95  & 9.00 + (-0.95) + 0.01\n\\end{bmatrix}\n\\quad\n\\tau^{(3)} = 0.10\n\\quad\n\\epsilon^{(3)} = 10^{-12}.\n$$\n\n- 测试用例 4（四个特征，存在两个近似依赖关系和中度噪声；压力测试）：\n设特征为\n$$\nx_{1} = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\\\ 4 \\\\ 5 \\\\ 6 \\\\ 7 \\\\ 8 \\\\ 9 \\end{bmatrix},\n\\quad\nx_{2} = \\begin{bmatrix} 9.00 \\\\ 7.95 \\\\ 7.00 \\\\ 6.05 \\\\ 5.00 \\\\ 3.95 \\\\ 3.00 \\\\ 2.05 \\\\ 1.00 \\end{bmatrix},\n\\quad\nx_{3} = \\begin{bmatrix} 1.50 \\\\ 0.00 \\\\ 1.70 \\\\ 0.20 \\\\ 1.40 \\\\ 0.30 \\\\ 1.80 \\\\ 0.10 \\\\ 1.60 \\end{bmatrix},\n$$\n和\n$$\nx_{4} = 0.50 \\, x_{1} + 0.50 \\, x_{3} + \\begin{bmatrix} 0.01 \\\\ -0.02 \\\\ 0.00 \\\\ 0.03 \\\\ -0.01 \\\\ 0.02 \\\\ -0.02 \\\\ 0.01 \\\\ 0.00 \\end{bmatrix}.\n$$\n则\n$$\nX^{(4)} = \\begin{bmatrix} x_{1} & x_{2} & x_{3} & x_{4} \\end{bmatrix}\n\\quad\n\\tau^{(4)} = 0.08\n\\quad\n\\epsilon^{(4)} = 10^{-12}.\n$$\n\n最终输出规范：\n- 对于每个测试用例，根据上述汇总规则，输出一个建议舍弃的、从零开始的特征索引列表。\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，列表中的每个条目本身是对应测试用例的整数列表。例如，一个包含四个测试用例的输出可能看起来像 $[ [0,2], [], [3], [1] ]$，但实际输出字符串中不应包含空格。确切的格式要求是形如 $[[\\dots],[\\dots],[\\dots],[\\dots]]$ 的单行字符串，其中不含任何空格。", "solution": "用户提供的问题被评估为有效。它在科学上基于多元统计学和线性代数的原理，问题设定良好（well-posed），具有清晰且确定性的算法流程，并且其表述是客观的。\n\n解决方案源于对相关矩阵谱分解的几何解释。多重共线性意味着数据集中的某个特征向量可以被其他特征向量的线性组合近似表示。问题中指定的算法形式化了一个检测和解决此类依赖关系的过程。\n\n设数据集由矩阵 $X \\in \\mathbb{R}^{n \\times p}$ 表示，包含 $n$ 个观测值和 $p$ 个特征，记为列 $x_1, \\dots, x_p$。诊断多重共线性的第一步是标准化数据。对于每个特征 $j \\in \\{1, \\dots, p\\}$，计算经验均值 $\\bar{x}_j$ 和无偏经验标准差 $s_j$：\n$$\n\\bar{x}_{j} = \\frac{1}{n} \\sum_{i=1}^{n} X_{ij}\n$$\n$$\ns_{j} = \\sqrt{\\frac{1}{n-1} \\sum_{i=1}^{n} \\left(X_{ij} - \\bar{x}_{j}\\right)^{2}}\n$$\n对于 $s_j = 0$ 的特征 $j$，它在所有观测值中都是恒定的，不携带任何方差。这样的特征是无信息量的，应立即标记为待移除。对于所有 $s_j > 0$ 的特征，我们创建一个标准化特征向量 $z_j$，其元素为 $Z_{ij} = \\frac{X_{ij} - \\bar{x}_{j}}{s_{j}}$。这些标准化向量构成了标准化设计矩阵 $Z \\in \\mathbb{R}^{n \\times p'}$ 的列，其中 $p'$ 是非恒定特征的数量。\n\n经验相关矩阵 $R \\in \\mathbb{R}^{p' \\times p'}$ 由标准化数据计算得出：\n$$\nR = \\frac{1}{n - 1} Z^{\\top} Z\n$$\n根据其构造，$R$ 是一个对称半正定矩阵。其对角元素 $R_{jj}$ 均等于 $1$，其非对角元素 $R_{jk}$ 表示特征 $j$ 和 $k$ 之间的样本相关性。\n\n该诊断程序的基石是 $R$ 的谱分解：\n$$\nR = V \\Lambda V^{\\top}\n$$\n其中 $V$ 是一个正交矩阵，其列 $v_1, \\dots, v_{p'}$ 是 $R$ 的特征向量，$\\Lambda = \\mathrm{diag}(\\lambda_1, \\dots, \\lambda_{p'})$ 是一个对角矩阵，包含相应的非负特征值，按非递减顺序排列。\n\n每个特征向量 $v_k$ 代表特征空间中的一个主轴，而相应的特征值 $\\lambda_k$ 代表标准化数据投影到该轴上的方差。特征之间的近线性依赖关系会转化为特征空间中数据方差极小的一个方向。因此，一个非常小的特征值 $\\lambda_k \\approx 0$ 表明存在多重共线性。相应的特征向量 $v_k$ 揭示了这种线性关系的本质。具体来说，如果 $\\lambda_k \\approx 0$，那么标准化特征的线性组合 $Z v_k$ 的方差接近于零。这意味着：\n$$\nZ v_k = \\sum_{j=1}^{p'} Z_{:,j} v_{k,j} \\approx 0\n$$\n其中 $v_{k,j}$ 是特征向量 $v_k$ 的第 $j$ 个分量。这个方程表示 $Z$ 的列之间存在近线性依赖关系。\n\n指定的算法将此原理付诸实践。它识别所有低于给定阈值 $\\tau > 0$ 的特征值 $\\lambda_k$。对于每个这样的小特征值，检查其关联的特征向量 $v_k$。特征向量的分量 $v_{k,j}$ 是原始特征在该线性依赖关系中的“载荷”。为了解决多重共线性问题，必须移除其中一个相关的特征。这里采用一种常见的启发式方法，即识别对该依赖关系贡献最大的特征——也就是具有最大绝对载荷 $|v_{k,j}|$ 的那个。设该特征索引为 $j^{\\star}$。如果多个特征表现出相同的最大载荷（在数值容差 $\\epsilon$ 内），则通过选择索引最大的特征来打破平局。这个特征 $j^{\\star}$ 被建议剔除。\n\n最终要舍弃的特征集合是零方差特征（初始识别）集合与基于小特征值分析建议剔除的特征集合的并集。这些索引经过去除重复项并按升序排序，以提供一个确定性的最终建议。\n\n算法流程如下：\n1. 对于输入矩阵 $X$，使用 $n-1$ 作为分母计算每个特征列的标准差。识别标准差为 $0$ 的列的索引集合。这些是首批待移除的候选项。\n2. 从 $X$ 中滤除零方差列，形成一个新矩阵 $X'$。保留从 $X'$ 的列索引到 $X$ 的原始列索引的映射。如果 $X'$ 的列数小于或等于 $1$，则分析停止，仅报告零方差索引。\n3. 标准化 $X'$ 以获得矩阵 $Z$，使其每列的均值为 $0$，标准差为 $1$。\n4. 计算相关矩阵 $R = \\frac{1}{n-1} Z^{\\top} Z$。\n5. 对 $R$ 进行特征分解，得到特征值 $\\lambda_k$ 和特征向量 $v_k$。\n6. 使用步骤 1 中的索引初始化一个待舍弃特征的集合。\n7. 遍历每个特征值 $\\lambda_k$。如果 $\\lambda_k < \\tau$：\n    a. 提取对应的特征向量 $v_k$。\n    b. 找到最大绝对载荷 $m = \\max_j |v_{k,j}|$。\n    c. 在简化的特征集中，识别所有满足 $|v_{k,j'}|$ 与 $m$ 的差值在容差 $\\epsilon$ 内的索引 $j'$（即 $m - |v_{k,j'}| \\le \\epsilon$）。\n    d. 从这些并列的索引中，选择最大的一个，记为 $j'_{\\text{drop}}$。\n    e. 将这个简化后的索引 $j'_{\\text{drop}}$ 映射回其在 $X$ 中的原始索引，并将其添加到待舍弃特征的集合中。\n8. 将最终的索引集合转换为一个排序后的列表，并作为给定测试用例的结果返回。对所有测试用例重复此过程。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the multicollinearity diagnosis problem for a suite of test cases.\n    \"\"\"\n\n    # Test case 1: Near-duplicate features\n    X1 = np.array([\n        [0.0, 0.00, 1.00], [1.0, 1.00 + 0.05, -0.50], [2.0, 2.00 - 0.02, 0.70],\n        [3.0, 3.00 + 0.10, -1.20], [4.0, 4.00 - 0.05, 0.30], [5.0, 5.00 + 0.03, 0.80],\n        [6.0, 6.00 - 0.08, -0.90], [7.0, 7.00 + 0.02, 1.10], [8.0, 8.00 + 0.00, -0.40],\n        [9.0, 9.00 - 0.07, 0.20]\n    ])\n    tau1 = 0.05\n    eps1 = 1e-12\n\n    # Test case 2: Approximately independent features\n    X2 = np.array([\n        [0.0, 1.10, 0.50], [1.0, -0.90, 0.30], [2.0, 0.70, -0.60], [3.0, -0.30, 0.70],\n        [4.0, 0.20, -0.80], [5.0, -0.40, 0.10], [6.0, 0.80, 0.20], [7.0, -0.50, -0.30],\n        [8.0, 0.60, 0.40], [9.0, -0.70, -0.20], [10.0, 0.90, 0.00], [11.0, -1.00, 0.50]\n    ])\n    tau2 = 0.10\n    eps2 = 1e-12\n\n    # Test case 3: One feature is a near-linear combination of two others\n    X3 = np.array([\n        [2.00, -1.00, 2.00 + (-1.00) + 0.01], [3.00, -1.10, 3.00 + (-1.10) - 0.02],\n        [4.00, -0.90, 4.00 + (-0.90) + 0.00], [5.50, -1.20, 5.50 + (-1.20) + 0.03],\n        [6.10, -0.80, 6.10 + (-0.80) - 0.01], [7.00, -1.05, 7.00 + (-1.05) + 0.02],\n        [8.20, -1.00, 8.20 + (-1.00) - 0.02], [9.00, -0.95, 9.00 + (-0.95) + 0.01],\n    ])\n    tau3 = 0.10\n    eps3 = 1e-12\n\n    # Test case 4: Four features with two near-dependencies\n    x1_4 = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9]).reshape(-1, 1)\n    x2_4 = np.array([9.00, 7.95, 7.00, 6.05, 5.00, 3.95, 3.00, 2.05, 1.00]).reshape(-1, 1)\n    x3_4 = np.array([1.50, 0.00, 1.70, 0.20, 1.40, 0.30, 1.80, 0.10, 1.60]).reshape(-1, 1)\n    delta4 = np.array([0.01, -0.02, 0.00, 0.03, -0.01, 0.02, -0.02, 0.01, 0.00]).reshape(-1, 1)\n    x4_4 = 0.5 * x1_4 + 0.5 * x3_4 + delta4\n    X4 = np.hstack([x1_4, x2_4, x3_4, x4_4])\n    tau4 = 0.08\n    eps4 = 1e-12\n\n    test_cases = [\n        (X1, tau1, eps1),\n        (X2, tau2, eps2),\n        (X3, tau3, eps3),\n        (X4, tau4, eps4),\n    ]\n\n    all_results = []\n\n    for X, tau, epsilon in test_cases:\n        n, p = X.shape\n        drops_to_propose = set()\n\n        if n < 2:\n            # Cannot compute variance, add all features to drop if p > 0\n            if p > 0:\n                drops_to_propose.update(range(p))\n            all_results.append(sorted(list(drops_to_propose)))\n            continue\n\n        # Step 1: Standardize and handle zero-variance features\n        stds = np.std(X, axis=0, ddof=1)\n        zero_var_indices = np.where(stds == 0)[0]\n        drops_to_propose.update(zero_var_indices)\n\n        non_zero_var_mask = stds > 0\n        original_indices_map = np.arange(p)[non_zero_var_mask]\n        \n        # If 1 or 0 features remain, no multicollinearity to analyze\n        if len(original_indices_map) <= 1:\n            all_results.append(sorted(list(drops_to_propose)))\n            continue\n        \n        X_filtered = X[:, non_zero_var_mask]\n        means_filtered = np.mean(X_filtered, axis=0)\n        stds_filtered = stds[non_zero_var_mask]\n\n        Z = (X_filtered - means_filtered) / stds_filtered\n\n        # Step 2: Compute the empirical correlation matrix R\n        R = (Z.T @ Z) / (n - 1)\n        \n        # Step 3: Compute the spectral decomposition\n        # np.linalg.eigh is for symmetric matrices, returns sorted eigenvalues\n        eigenvalues, eigenvectors = np.linalg.eigh(R)\n\n        # Step 4: Identify features to drop based on small eigenvalues\n        small_eigenvalue_indices = np.where(eigenvalues < tau)[0]\n        \n        for k in small_eigenvalue_indices:\n            eigenvector = eigenvectors[:, k]\n            abs_loadings = np.abs(eigenvector)\n            max_abs_loading = np.max(abs_loadings)\n            \n            # Identify indices of features tied for the largest loading\n            # A feature's loading is considered 'tied' if it's within epsilon of the max\n            tie_indices_filtered = np.where(max_abs_loading - abs_loadings <= epsilon)[0]\n            \n            # Break tie by choosing the highest index\n            drop_candidate_filtered_idx = np.max(tie_indices_filtered)\n            \n            # Map back to original feature index\n            original_idx = original_indices_map[drop_candidate_filtered_idx]\n            drops_to_propose.add(original_idx)\n\n        # Step 5: Aggregate, deduplicate, and sort\n        final_drops = sorted(list(drops_to_propose))\n        all_results.append(final_drops)\n\n    # Final print statement in the exact required format\n    print(str(all_results).replace(\" \", \"\"))\n\nsolve()\n```", "id": "3117789"}, {"introduction": "在数据分析中，主成分分析（PCA）常用于降维，其基本思想是保留数据中方差最大的方向。然而，这些高方差方向是否总是对特定任务（如分类）最重要？本练习将挑战这一直觉。我们将通过理论分析和编码实践，探讨一个关键情景：当对分类至关重要的信息恰好位于低方差方向时，盲目应用PCA反而会损害分类器的性能。这项练习旨在培养你在特征工程中的批判性思维，强调在数据转换时必须考虑最终目标，并理解“降维”与“为特定任务保留信息”之间的区别。[@problem_id:3117809]", "problem": "考虑两个类别，它们被建模为具有相同先验概率和共享的对称正定协方差矩阵的多元正态分布。设共享的协方差矩阵通过其谱分解表示为一个标准正交特征基，其特征对为 $\\{(\\lambda_i,\\mathbf{u}_i)\\}_{i=1}^p$，并按 $\\lambda_1 \\ge \\lambda_2 \\ge \\cdots \\ge \\lambda_p > 0$ 排序。设均值差为 $\\Delta\\boldsymbol{\\mu} = \\boldsymbol{\\mu}_1 - \\boldsymbol{\\mu}_0$，并定义其在协方差特征基中的坐标为 $c_i = \\mathbf{u}_i^\\top \\Delta\\boldsymbol{\\mu}$，其中 $i \\in \\{1,\\dots,p\\}$。包含 $k$ 个主成分的主成分分析（PCA）去噪，对应于将数据投影到由 $\\{\\mathbf{u}_1,\\dots,\\mathbf{u}_k\\}$ 张成的子空间上，然后在该子空间中进行分类。\n\n仅从以下经过充分检验的事实和核心定义出发：\n- 对于具有共享协方差和相同先验的两个多元正态类别，线性判别分析（LDA）决策规则是贝叶斯最优的。\n- 在给定协方差矩阵下，两个均值之间的马氏距离是决定贝叶斯误差的关键量。\n- 谱分解将协方差矩阵表示为一个具有非负特征值的标准正交基，并允许向主子空间进行正交投影，\n\n推导最优错分概率如何依赖于马氏距离，首先在全空间中，然后在经过 PCA 投影到前 $k$ 个特征向量上之后。利用此结果，根据 $\\{(\\lambda_i,c_i)\\}$ 精确地说明 PCA 去噪在何时会因移除具有判别性的低方差方向而损害分类性能。\n\n您的程序必须对下面的每个测试用例，计算由 PCA 去噪引起的最优错分概率的变化量，该变化量定义为\n- PCA 处理后的最优错分概率减去全空间中的最优错分概率，\n\n并以实数形式返回此变化量。正值表示 PCA 去噪增加了误差（损害了分类性能），零表示没有变化，负值则表示有所改善。您必须：\n- 将 PCA 视为保留与 $k$ 个最大特征值 $\\{\\lambda_1,\\dots,\\lambda_k\\}$ 相关联的前 $k$ 个特征向量 $\\{\\mathbf{u}_1,\\dots,\\mathbf{u}_k\\}$，\n- 假设 $k \\in \\{0,1,\\dots,p\\}$，\n- 完全在特征基中使用给定的 $(\\lambda_i)$ 和 $(c_i)$ 进行计算，\n- 将所有最终数值输出四舍五入到 $6$ 位小数。\n\n测试套件（每个用例是一个三元组 $(\\boldsymbol{\\lambda},\\mathbf{c},k)$，其中 $\\boldsymbol{\\lambda}$ 是列表 $(\\lambda_1,\\dots,\\lambda_p)$，$\\mathbf{c}$ 是列表 $(c_1,\\dots,c_p)$）：\n- 用例 1 （移除了具有判别性的低方差方向）：$\\boldsymbol{\\lambda} = (\\,10.0,\\,3.0,\\,1.0,\\,0.2\\,)$，$\\mathbf{c} = (\\,0.0,\\,0.0,\\,0.0,\\,1.0\\,)$，$k = 1$。\n- 用例 2 （无损害，因为信号位于顶层方向）：$\\boldsymbol{\\lambda} = (\\,10.0,\\,3.0,\\,1.0,\\,0.2\\,)$，$\\mathbf{c} = (\\,1.0,\\,0.0,\\,0.0,\\,0.0\\,)$，$k = 1$。\n- 用例 3 （各向同性协方差；丢弃一些信号分量导致轻微损害）：$\\boldsymbol{\\lambda} = (\\,2.0,\\,2.0,\\,2.0,\\,2.0\\,)$，$\\mathbf{c} = (\\,1.0,\\,0.5,\\,0.3,\\,0.2\\,)$，$k = 2$。\n- 用例 4 （边界情况；保留所有分量，因此无变化）：$\\boldsymbol{\\lambda} = (\\,5.0,\\,1.0,\\,0.5\\,)$，$\\mathbf{c} = (\\,0.4,\\,0.6,\\,0.8\\,)$，$k = 3$。\n- 用例 5 （边界情况；保留 0 个分量，因此分类变为随机猜测）：$\\boldsymbol{\\lambda} = (\\,4.0,\\,1.0\\,)$，$\\mathbf{c} = (\\,1.0,\\,0.0\\,)$，$k = 0$。\n\n您的程序应生成单行输出，其中包含一个方括号括起来的逗号分隔列表的结果（例如，$[r_1,r_2,\\dots,r_5]$），其中 $r_j$ 是第 $j$ 个测试用例的最优错分概率的变化量，四舍五入到 $6$ 位小数。", "solution": "我们考虑两个多元正态类别，它们具有相同的先验概率、共享的协方差以及均值 $\\boldsymbol{\\mu}_0$ 和 $\\boldsymbol{\\mu}_1$。设均值差为 $\\Delta\\boldsymbol{\\mu} = \\boldsymbol{\\mu}_1 - \\boldsymbol{\\mu}_0$。将共享的协方差矩阵记为 $\\boldsymbol{\\Sigma}$，其谱分解为 $\\boldsymbol{\\Sigma} = \\sum_{i=1}^p \\lambda_i \\mathbf{u}_i \\mathbf{u}_i^\\top$，其中 $\\{\\mathbf{u}_i\\}_{i=1}^p$ 是一个标准正交基，且对所有 $i$ 都有 $\\lambda_i > 0$。定义坐标 $c_i = \\mathbf{u}_i^\\top \\Delta\\boldsymbol{\\mu}$。\n\n基于原理的推导：\n- 对于具有相同协方差和相同先验概率的两个多元正态分布，线性判别分析（LDA）规则是贝叶斯最优的。判别方向与 $\\boldsymbol{\\Sigma}^{-1} \\Delta\\boldsymbol{\\mu}$ 成正比。决定贝叶斯误差的关键标量是在 $\\boldsymbol{\\Sigma}$ 下两个均值之间的马氏距离，定义为\n$$\nd^2 = \\Delta\\boldsymbol{\\mu}^\\top \\boldsymbol{\\Sigma}^{-1} \\Delta\\boldsymbol{\\mu}.\n$$\n- 使用 $\\boldsymbol{\\Sigma}$ 的谱分解，我们有 $\\boldsymbol{\\Sigma}^{-1} = \\sum_{i=1}^p \\lambda_i^{-1} \\mathbf{u}_i \\mathbf{u}_i^\\top$。因此，\n$$\nd^2 = \\Delta\\boldsymbol{\\mu}^\\top \\left( \\sum_{i=1}^p \\lambda_i^{-1} \\mathbf{u}_i \\mathbf{u}_i^\\top \\right) \\Delta\\boldsymbol{\\mu} = \\sum_{i=1}^p \\lambda_i^{-1} \\left( \\mathbf{u}_i^\\top \\Delta\\boldsymbol{\\mu} \\right)^2 = \\sum_{i=1}^p \\frac{c_i^2}{\\lambda_i}.\n$$\n- 在此设置下，对于相等的先验概率，贝叶斯错分概率是 $d$ 的单调递减函数，由标准正态累积分布函数（CDF）在 $-d/2$ 处的值给出：\n$$\nP_{\\text{err, full}} = \\Phi\\left( -\\frac{d}{2} \\right),\n$$\n其中 $\\Phi$ 是标准正态累积分布函数。\n\n现在考虑保留前 $k$ 个特征向量 $\\{\\mathbf{u}_1,\\dots,\\mathbf{u}_k\\}$ 的主成分分析（PCA）去噪。设到该子空间的正交投影算子为 $\\mathbf{P}_k = \\sum_{i=1}^k \\mathbf{u}_i \\mathbf{u}_i^\\top$。在投影空间中，有效均值差为 $\\mathbf{P}_k \\Delta\\boldsymbol{\\mu}$，有效协方差为限制在 $k$ 维子空间中的 $\\mathbf{P}_k \\boldsymbol{\\Sigma} \\mathbf{P}_k = \\sum_{i=1}^k \\lambda_i \\mathbf{u}_i \\mathbf{u}_i^\\top$。该子空间中相应的马氏距离变为\n$$\nd_k^2 = (\\mathbf{P}_k \\Delta\\boldsymbol{\\mu})^\\top \\left( \\mathbf{P}_k \\boldsymbol{\\Sigma} \\mathbf{P}_k \\right)^{-1} (\\mathbf{P}_k \\Delta\\boldsymbol{\\mu}) = \\sum_{i=1}^k \\frac{c_i^2}{\\lambda_i}.\n$$\n因此，经过 PCA 去噪后的最优错分概率为\n$$\nP_{\\text{err, PCA}(k)} = \\Phi\\left( -\\frac{d_k}{2} \\right).\n$$\n由 PCA 去噪引起的误差变化为\n$$\n\\Delta P_{\\text{err}}(k) = P_{\\text{err, PCA}(k)} - P_{\\text{err, full}} = \\Phi\\left( -\\frac{1}{2}\\sqrt{\\sum_{i=1}^k \\frac{c_i^2}{\\lambda_i}} \\right) - \\Phi\\left( -\\frac{1}{2}\\sqrt{\\sum_{i=1}^p \\frac{c_i^2}{\\lambda_i}} \\right).\n$$\n从这个表达式可以立即得出：\n- 如果所有判别能量都位于被丢弃的低方差方向（即对于 $i \\le k$ 有 $c_i = 0$，而对于某个 $j > k$ 有 $c_j \\ne 0$），那么 $d_k = 0$ 且 $P_{\\text{err, PCA}(k)} = \\Phi(0) = 0.5$，而 $P_{\\text{err, full}} < 0.5$；因此 $\\Delta P_{\\text{err}}(k) > 0$，PCA 损害了分类性能。\n- 如果所有判别能量都位于前 $k$ 个方向内（即对于 $i > k$ 有 $c_i = 0$），那么 $d_k = d$ 且 $\\Delta P_{\\text{err}}(k) = 0$；PCA 不会改变性能。\n- 如果一些判别能量位于前 $k$ 个方向之外，那么 $0 \\le d_k < d$ 且 $\\Delta P_{\\text{err}}(k) > 0$；PCA 通过移除有用的方向而损害了分类性能。当被移除的方向具有较小的 $\\lambda_i$（低方差）但不可忽略的 $|c_i|$（高判别性内容）时，损害通常更大，因为它们通过 $c_i^2/\\lambda_i$ 对 $d^2$ 有很强的贡献。\n\n用于计算的算法设计：\n- 对于每个测试用例，计算 $d^2 = \\sum_{i=1}^p c_i^2/\\lambda_i$ 和 $d_k^2 = \\sum_{i=1}^k c_i^2/\\lambda_i$（约定 $d_0^2 = 0$）。\n- 计算 $P_{\\text{err, full}} = \\Phi(-\\sqrt{d^2}/2)$ 和 $P_{\\text{err, PCA}(k)} = \\Phi(-\\sqrt{d_k^2}/2)$，使用 $\\Phi(x) = \\tfrac{1}{2}\\left(1 + \\operatorname{erf}\\!\\left(\\tfrac{x}{\\sqrt{2}}\\right)\\right)$。\n- 输出 $\\Delta P_{\\text{err}}(k) = P_{\\text{err, PCA}(k)} - P_{\\text{err, full}}$，四舍五入到 $6$ 位小数。\n\n对所提供用例的定性预期：\n- 用例 1：严重损害，因为所有信号都位于方差最小的方向，而当 $k = 1$ 时该方向被丢弃。\n- 用例 2：无损害，因为信号完全位于被 PCA 保留的顶层方向。\n- 用例 3：轻微损害，因为各向同性协方差意味着每个被丢弃的分量都会移除成比例的判别内容。\n- 用例 4：无损害，因为 $k = p$，所以没有分量被丢弃。\n- 用例 5：对于给定的设置，损害最大，因为 $k = 0$ 导致错误率为随机猜测水平 $0.5$。\n\n程序实现了这些计算，并将四舍五入后的变化量作为单个方括号列表打印出来。", "answer": "```python\nimport numpy as np\nimport math\n\ndef normal_cdf(x: float) -> float:\n    # Standard normal CDF using the error function\n    return 0.5 * (1.0 + math.erf(x / math.sqrt(2.0)))\n\ndef bayes_error_change(lambdas, c, k) -> float:\n    lambdas = np.array(lambdas, dtype=float)\n    c = np.array(c, dtype=float)\n    # Ensure inputs are consistent\n    assert lambdas.ndim == 1 and c.ndim == 1 and lambdas.shape == c.shape\n    p = lambdas.size\n    assert 0 <= k <= p\n\n    # Compute squared Mahalanobis distances\n    inv_terms = (c ** 2) / lambdas\n    d2_full = float(np.sum(inv_terms))\n    d2_k = float(np.sum(inv_terms[:k])) if k > 0 else 0.0\n\n    # Convert to Bayes errors using Phi(-d/2)\n    err_full = normal_cdf(-math.sqrt(d2_full) / 2.0) if d2_full > 0.0 else 0.5\n    err_k = normal_cdf(-math.sqrt(d2_k) / 2.0) if d2_k > 0.0 else 0.5\n\n    # Change in error (post-PCA minus full)\n    return err_k - err_full\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each case: (lambdas, c, k)\n    test_cases = [\n        # Case 1\n        ([10.0, 3.0, 1.0, 0.2], [0.0, 0.0, 0.0, 1.0], 1),\n        # Case 2\n        ([10.0, 3.0, 1.0, 0.2], [1.0, 0.0, 0.0, 0.0], 1),\n        # Case 3\n        ([2.0, 2.0, 2.0, 2.0], [1.0, 0.5, 0.3, 0.2], 2),\n        # Case 4\n        ([5.0, 1.0, 0.5], [0.4, 0.6, 0.8], 3),\n        # Case 5\n        ([4.0, 1.0], [1.0, 0.0], 0),\n    ]\n\n    results = []\n    for lambdas, c, k in test_cases:\n        delta_err = bayes_error_change(lambdas, c, k)\n        results.append(f\"{delta_err:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3117809"}, {"introduction": "谱方法的应用远不止于分析协方差矩阵和线性关系，它还能通过图论的视角揭示数据中复杂的非线性结构。在最后的这个练习中，你将根据数据点构建一个相似度图，并分析其图拉普拉斯矩阵的特征向量。这项被称为谱嵌入的技术，能够揭示那些线性不可分的隐藏簇。通过实现一个半监督学习算法，你将学习如何将少量已知标签传播到整个数据集，这为你打开了通往流形学习和现代聚类技术的大门，展示了特征值和特征向量在发现数据内在几何结构方面的强大能力。[@problem_id:3117759]", "problem": "给定您一些小型的标记和未标记数据集，旨在说明统计学习中的谱方法。目标是使用余弦相似度从样本构建一个加权图，建立一个图拉普拉斯算子，并使用基于拉普拉斯算子特征向量的谱嵌入，通过嵌入空间中的最近原型法进行半监督分类。\n\n出发点：仅使用相似图的基本定义和对称矩阵的谱性质。具体来说：\n- 两个向量 $\\mathbf{x}_i \\in \\mathbb{R}^d$ 和 $\\mathbf{x}_j \\in \\mathbb{R}^d$ 之间的余弦相似度定义为 $\\mathrm{cos}(\\theta_{ij}) = \\dfrac{\\mathbf{x}_i^\\top \\mathbf{x}_j}{\\|\\mathbf{x}_i\\|\\|\\mathbf{x}_j\\|}$，并约定如果任一范数为零，则相似度设为 $0$。\n- 给定 $i \\neq j$ 时的非负对称权重 $w_{ij}$ 和 $w_{ii} = 0$，定义度 $d_i = \\sum_{j=1}^n w_{ij}$ 和未归一化的图拉普拉斯算子 $L = D - W$，其中 $D = \\mathrm{diag}(d_1,\\dots,d_n)$ 且 $W = [w_{ij}]$。\n- 对于任意非零向量 $\\mathbf{f} \\in \\mathbb{R}^n$，$L$ 的瑞利商为 $R(\\mathbf{f}) = \\dfrac{\\mathbf{f}^\\top L \\mathbf{f}}{\\mathbf{f}^\\top \\mathbf{f}}$。对称矩阵 $L$ 拥有一组与实特征值相关的特征向量构成的正交归一基，特征值排序为 $0 = \\lambda_1 \\le \\lambda_2 \\le \\dots \\le \\lambda_n$。在正交性约束下 $R(\\mathbf{f})$ 的最小化子对应于 $L$ 的特征向量。\n\n任务：\n- 对于每个数据集，使用余弦相似度在 $n$ 个样本上构建一个全连接相似图，但将负值裁剪为零：对于 $i \\neq j$，定义 $w_{ij} = \\max\\{0, \\mathrm{cos}(\\theta_{ij})\\}$ 且 $w_{ii} = 0$。\n- 构建未归一化的拉普拉斯算子 $L = D - W$。\n- 按如下方式计算 $k$ 维谱嵌入。设 $k$ 为已知的类别数。计算 $L$ 的所有特征对，将特征值按升序排序，舍弃对应于最小特征值的特征向量，然后取接下来的 $k$ 个特征向量作为嵌入矩阵 $Y \\in \\mathbb{R}^{n \\times k}$ 的列。样本 $i$ 的嵌入坐标是 $Y$ 的第 $i$ 行 $\\mathbf{y}_i^\\top$。\n- 半监督分类：对于每个类别 $c \\in \\{0,\\dots,k-1\\}$，计算嵌入空间中的类别原型，即类别为 $c$ 的所有标记样本的 $\\mathbf{y}_i$ 的算术平均值。然后将每个未标记样本分配给其原型在欧几里得距离上最近的类别。如果距离相等，则选择最小的类别索引来打破僵局。\n- 计算未标记子集上的准确率，即预测类别与提供的真实类别相等的未标记样本所占的比例。准确率以 $[0,1]$ 区间内的小数表示。\n\n为以下测试套件实现上述过程。在每个案例中，$X$ 是数据矩阵，其行代表样本，$y$ 是标签向量，其中 $-1$ 表示未标记，$y^{\\mathrm{true}}$ 是真实标签向量。所有数值均为实数，应视为无量纲。\n\n测试案例 A（两个潜在类别，在 $\\mathbb{R}^3$ 中有中等分离度）：\n- 样本数 $n = 8$，特征数 $d = 3$，类别数 $k = 2$。\n- 数据矩阵 $X_A$，其行向量为\n  $\\big(1.0, 0.0, 0.0\\big)$、\n  $\\big(0.9, 0.1, 0.0\\big)$、\n  $\\big(0.95, 0.0, 0.1\\big)$、\n  $\\big(0.85, 0.2, 0.1\\big)$、\n  $\\big(0.0, 1.0, 0.0\\big)$、\n  $\\big(0.1, 0.9, 0.0\\big)$、\n  $\\big(0.0, 0.95, 0.1\\big)$、\n  $\\big(0.2, 0.85, 0.1\\big)$。\n- 真实标签 $y^{\\mathrm{true}}_A = [0, 0, 0, 0, 1, 1, 1, 1]$。\n- 半监督标签 $y_A = [0, -1, -1, -1, 1, -1, -1, -1]$。\n\n测试案例 B（三个潜在类别，在 $\\mathbb{R}^2$ 中大约相隔 $120^\\circ$）：\n- 样本数 $n = 9$，特征数 $d = 2$，类别数 $k = 3$。\n- 设 $s = \\sqrt{3} / 2 \\approx 0.8660254$。\n- 数据矩阵 $X_B$，其行向量为\n  $\\big(1.0, 0.0\\big)$、\n  $\\big(0.95, 0.1\\big)$、\n  $\\big(0.9, -0.1\\big)$、\n  $\\big(-0.5, s\\big)$、\n  $\\big(-0.6, 0.8\\big)$、\n  $\\big(-0.4, 0.9\\big)$、\n  $\\big(-0.5, -s\\big)$、\n  $\\big(-0.6, -0.8\\big)$、\n  $\\big(-0.4, -0.9\\big)$。\n- 真实标签 $y^{\\mathrm{true}}_B = [0, 0, 0, 1, 1, 1, 2, 2, 2]$。\n- 半监督标签 $y_B = [0, -1, -1, 1, -1, -1, 2, -1, -1]$。\n\n测试案例 C（两个潜在类别，在 $\\mathbb{R}^3$ 中大约相隔 $30^\\circ$）：\n- 样本数 $n = 8$，特征数 $d = 3$，类别数 $k = 2$。\n- 设 $c = \\cos(\\pi/6) \\approx 0.8660254$ 且 $t = \\sin(\\pi/6) = 0.5$。\n- 数据矩阵 $X_C$，其行向量为\n  $\\big(1.0, 0.0, 0.0\\big)$、\n  $\\big(0.95, 0.1, 0.0\\big)$、\n  $\\big(1.0, 0.05, 0.0\\big)$、\n  $\\big(0.9, 0.2, 0.0\\big)$、\n  $\\big(c, t, 0.0\\big)$、\n  $\\big(0.8, 0.55, 0.0\\big)$、\n  $\\big(0.9, 0.45, 0.0\\big)$、\n  $\\big(0.85, 0.52, 0.05\\big)$。\n- 真实标签 $y^{\\mathrm{true}}_C = [0, 0, 0, 0, 1, 1, 1, 1]$。\n- 半监督标签 $y_C = [0, -1, -1, -1, 1, -1, -1, -1]$。\n\n程序要求：\n- 对于每个测试案例，完全按照描述构建 $W$、$D$ 和 $L$；通过舍弃对应于最小特征值的特征向量并按升序取接下来的 $k$ 个特征向量来计算谱嵌入 $Y$；在嵌入空间中计算类别原型，并使用欧几里得距离和指定的打破僵局规则，通过最近原型法对未标记点进行分类；然后计算未标记数据的准确率，结果为 $[0,1]$ 区间内的小数，并四舍五入到六位小数。\n- 您的程序应生成单行输出，其中包含测试案例 A、测试案例 B 和测试案例 C 的三个准确率，按此顺序排列，形式为用方括号括起来的逗号分隔列表，例如 $[0.500000,1.000000,0.666667]$。\n\n程序没有额外的输入，也不允许使用外部数据源。所有计算都是无量纲的，因此不需要物理单位。三角常数中的角度以弧度为单位。请确保所有数值常数在适用时均使用上述值实现。", "solution": "该问题要求实现一个基于谱图理论的半监督分类算法。该程序将应用于三个不同的测试案例，并需要报告每个数据集未标记部分的准确率。该方法可分解为五个主要步骤：图构建、拉普拉斯算子构建、谱嵌入、分类和评估。\n\n### 步骤 1：邻接矩阵构建\n首先，我们通过构建一个加权邻接矩阵 $W \\in \\mathbb{R}^{n \\times n}$ 来对 $n$ 个数据样本（表示为向量 $\\mathbf{x}_1, \\dots, \\mathbf{x}_n \\in \\mathbb{R}^d$）之间的关系进行建模。该图是全连接的。两个不同样本 $\\mathbf{x}_i$ 和 $\\mathbf{x}_j$ 之间的权重 $w_{ij}$ 源自它们的余弦相似度，该相似度衡量了它们之间夹角的余弦值：\n$$\n\\mathrm{cos}(\\theta_{ij}) = \\frac{\\mathbf{x}_i^\\top \\mathbf{x}_j}{\\|\\mathbf{x}_i\\|_2 \\|\\mathbf{x}_j\\|_2}\n$$\n问题指定了一个约定，即如果任一向量的范数为零，则相似度为 $0$。根据问题的指示，负的相似度值被裁剪为零。权重矩阵的对角线元素被设置为零。因此，权重定义如下：\n$$\nw_{ij} = \\begin{cases} \\max\\{0, \\mathrm{cos}(\\theta_{ij})\\} & \\text{if } i \\neq j \\\\ 0 & \\text{if } i = j \\end{cases}\n$$\n得到的矩阵 $W$ 是对称的，且其元素非负。\n\n### 步骤 2：图拉普拉斯算子构建\n从权重矩阵 $W$，我们构建未归一化的图拉普拉斯算子 $L$。这需要度矩阵 $D$，它是一个对角矩阵，其对角线上的元素 $D_{ii} = d_i$ 是连接到顶点 $i$ 的所有边的权重之和：\n$$\nd_i = \\sum_{j=1}^n w_{ij}\n$$\n未归一化的图拉普拉斯算子由下式给出：\n$$\nL = D - W\n$$\n$L$ 是一个对称半正定矩阵，这是后续谱分析的一个关键性质。\n\n### 步骤 3：谱嵌入\n谱方法的核心在于分析图拉普拉斯算子 $L$ 的特征系统。由于 $L$ 是实对称的，它有 $n$ 个完整的实特征值 $0 = \\lambda_1 \\le \\lambda_2 \\le \\dots \\le \\lambda_n$ 和一组对应的特征向量正交归一基 $\\mathbf{v}_1, \\mathbf{v}_2, \\dots, \\mathbf{v}_n$。与最小非零特征值相对应的特征向量（称为 Fiedler 向量）捕捉了图的全局结构，有效地将其划分为簇。\n\n问题指定构建一个 $k$ 维嵌入，其中 $k$ 是类别数。我们计算 $L$ 的所有特征对。与最小特征值 $\\lambda_1 = 0$ 相关联的特征向量 $\\mathbf{v}_1$ 被舍弃，因为它是一个常数向量（对于连通图而言），不包含用于聚类的信息。嵌入矩阵 $Y \\in \\mathbb{R}^{n \\times k}$ 由接下来的 $k$ 个特征向量作为其列构成：\n$$\nY = [\\mathbf{v}_2, \\mathbf{v}_3, \\dots, \\mathbf{v}_{k+1}]\n$$\n$Y$ 的第 $i$ 行，表示为 $\\mathbf{y}_i^\\top$，作为原始样本 $\\mathbf{x}_i$ 新的 $k$ 维坐标向量。这个从 $\\mathbb{R}^d$到 $\\mathbb{R}^k$ 的映射就是谱嵌入。\n\n### 步骤 4：半监督分类\n数据嵌入到低维空间后，我们进行分类。数据集为一小部分样本提供了标签，其余样本未标记。对于每个类别 $c \\in \\{0, \\dots, k-1\\}$，我们通过计算属于该类别的所有标记样本的嵌入向量的算术平均值，来得到一个类别原型 $\\mathbf{p}_c$。设 $I_c$ 是标记为类别 $c$ 的样本索引集。该原型为：\n$$\n\\mathbf{p}_c = \\frac{1}{|I_c|} \\sum_{i \\in I_c} \\mathbf{y}_i\n$$\n然后，通过将每个未标记样本 $j$ 分配到嵌入空间中最近的原型所属的类别来进行分类。距离度量是欧几里得距离。预测标签 $\\hat{y}_j$ 为：\n$$\n\\hat{y}_j = \\arg\\min_{c \\in \\{0, \\dots, k-1\\}} \\|\\mathbf{y}_j - \\mathbf{p}_c\\|_2\n$$\n距离上的相等情况通过选择具有最小索引的类别来打破。\n\n### 步骤 5：准确率评估\n最后一步是评估此半监督分类器的性能。准确率计算为预测标签 $\\hat{y}_j$ 与提供的真实标签 $y^{\\mathrm{true}}_j$ 相匹配的未标记样本所占的比例。设 $S_U$ 为未标记样本的索引集。准确率为：\n$$\n\\text{Accuracy} = \\frac{1}{|S_U|} \\sum_{j \\in S_U} \\mathbb{I}(\\hat{y}_j = y^{\\mathrm{true}}_j)\n$$\n其中 $\\mathbb{I}(\\cdot)$ 是指示函数，如果其参数为真，则为 $1$，否则为 $0$。此过程被系统地应用于所提供的三个测试案例。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n# from scipy import ...\n\ndef compute_accuracy(X, y, y_true, k):\n    \"\"\"\n    Computes semi-supervised classification accuracy using spectral methods.\n    \n    Args:\n        X (np.ndarray): Data matrix (n_samples, n_features).\n        y (np.ndarray): Label vector (-1 for unlabeled).\n        y_true (np.ndarray): Ground-truth label vector.\n        k (int): Number of classes.\n\n    Returns:\n        float: Accuracy on the unlabeled subset.\n    \"\"\"\n    n = X.shape[0]\n\n    # Step 1: Construct the Adjacency Matrix W\n    # Normalize rows of X to compute cosine similarity more efficiently.\n    norms = np.linalg.norm(X, axis=1, keepdims=True)\n    X_normalized = np.divide(X, norms, out=np.zeros_like(X, dtype=float), where=norms!=0)\n    \n    # Cosine similarity matrix is the dot product of normalized vectors.\n    cos_sim_matrix = X_normalized @ X_normalized.T\n    \n    # Build W by clipping negative values and setting diagonal to zero.\n    W = np.maximum(0, cos_sim_matrix)\n    np.fill_diagonal(W, 0)\n    \n    # Step 2: Construct the Graph Laplacian L\n    d = np.sum(W, axis=1)\n    D = np.diag(d)\n    L = D - W\n    \n    # Step 3: Compute the Spectral Embedding Y\n    # eigh returns eigenvalues in ascending order and corresponding eigenvectors.\n    eigenvalues, eigenvectors = np.linalg.eigh(L)\n    \n    # Discard the eigenvector for the smallest eigenvalue and take the next k.\n    Y = eigenvectors[:, 1:k+1]\n    \n    # Step 4: Semi-supervised Classification\n    labeled_mask = (y != -1)\n    unlabeled_mask = ~labeled_mask\n    \n    # Compute class prototypes in the embedding space.\n    prototypes = []\n    for c in range(k):\n        class_mask = (y == c)\n        class_embeddings = Y[class_mask, :]\n        \n        # This check is for robustness; problem data guarantees labeled samples for each class.\n        if class_embeddings.shape[0] > 0:\n            prototype = np.mean(class_embeddings, axis=0)\n            prototypes.append(prototype)\n        else:\n            # Handle case where a class has no labeled samples (not in this problem)\n            # A possible strategy is to place the prototype at the origin.\n            prototypes.append(np.zeros(k))\n\n    prototypes = np.array(prototypes)\n    \n    # Classify unlabeled samples by nearest prototype.\n    unlabeled_embeddings = Y[unlabeled_mask, :]\n    predictions = []\n    for emb in unlabeled_embeddings:\n        # Calculate Euclidean distance to all prototypes\n        distances = np.linalg.norm(prototypes - emb, axis=1)\n        # argmin breaks ties by choosing the smallest index, as required.\n        predicted_class = np.argmin(distances)\n        predictions.append(predicted_class)\n    \n    predictions = np.array(predictions)\n    \n    # Step 5: Calculate Accuracy\n    true_labels_unlabeled = y_true[unlabeled_mask]\n    \n    correct_count = np.sum(predictions == true_labels_unlabeled)\n    total_unlabeled = len(true_labels_unlabeled)\n    \n    if total_unlabeled == 0:\n        accuracy = 1.0 #Convention for no unlabeled points\n    else:\n        accuracy = correct_count / total_unlabeled\n        \n    return accuracy\n\ndef solve():\n    \"\"\"\n    Main function to define test cases, run the algorithm, and print results.\n    \"\"\"\n    # Define constants for test cases B and C\n    s_b = np.sqrt(3) / 2\n    c_c = np.cos(np.pi / 6)\n    t_c = np.sin(np.pi / 6)\n\n    test_cases = [\n        {\n            \"X\": np.array([\n                [1.0, 0.0, 0.0], [0.9, 0.1, 0.0], [0.95, 0.0, 0.1], [0.85, 0.2, 0.1],\n                [0.0, 1.0, 0.0], [0.1, 0.9, 0.0], [0.0, 0.95, 0.1], [0.2, 0.85, 0.1]\n            ]),\n            \"y\": np.array([0, -1, -1, -1, 1, -1, -1, -1]),\n            \"y_true\": np.array([0, 0, 0, 0, 1, 1, 1, 1]),\n            \"k\": 2\n        },\n        {\n            \"X\": np.array([\n                [1.0, 0.0], [0.95, 0.1], [0.9, -0.1],\n                [-0.5, s_b], [-0.6, 0.8], [-0.4, 0.9],\n                [-0.5, -s_b], [-0.6, -0.8], [-0.4, -0.9]\n            ]),\n            \"y\": np.array([0, -1, -1, 1, -1, -1, 2, -1, -1]),\n            \"y_true\": np.array([0, 0, 0, 1, 1, 1, 2, 2, 2]),\n            \"k\": 3\n        },\n        {\n            \"X\": np.array([\n                [1.0, 0.0, 0.0], [0.95, 0.1, 0.0], [1.0, 0.05, 0.0], [0.9, 0.2, 0.0],\n                [c_c, t_c, 0.0], [0.8, 0.55, 0.0], [0.9, 0.45, 0.0], [0.85, 0.52, 0.05]\n            ]),\n            \"y\": np.array([0, -1, -1, -1, 1, -1, -1, -1]),\n            \"y_true\": np.array([0, 0, 0, 0, 1, 1, 1, 1]),\n            \"k\": 2\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        accuracy = compute_accuracy(case[\"X\"], case[\"y\"], case[\"y_true\"], case[\"k\"])\n        results.append(accuracy)\n\n    # Format output to six decimal places as specified.\n    formatted_results = [f\"{r:.6f}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3117759"}]}