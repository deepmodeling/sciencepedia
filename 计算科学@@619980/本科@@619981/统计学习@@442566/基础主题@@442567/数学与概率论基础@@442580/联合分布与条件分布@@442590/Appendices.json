{"hands_on_practices": [{"introduction": "要想掌握联合分布与条件分布，第一步是理解其基本计算方法。本练习提供了一个简单的离散随机变量场景，让你实践如何求解归一化常数、计算边缘分布，并最终推导出条件概率质量函数（PMF）。这是后续所有更复杂工作的基石。[@problem_id:2512]", "problem": "设 $X$ 和 $Y$ 是两个离散随机变量。$X$ 的样本空间是 $S_X = \\{1, 2\\}$，$Y$ 的样本空间是 $S_Y = \\{1, 2, 3\\}$。$X$ 和 $Y$ 的联合概率质量函数 (PMF) 由下式给出\n$$p_{X,Y}(x,y) = cxy$$\n对于 $(x,y) \\in S_X \\times S_Y$，其他情况下 $p_{X,Y}(x,y)=0$。在此表达式中，$c$ 是一个归一化常数。\n\n你的任务是求出在给定 $x=1$ 的条件下，当 $y=3$ 时条件概率质量函数 $p_{Y|X}(y|x)$ 的值。", "solution": "我们有联合概率质量函数\n$$p_{X,Y}(x,y)=c\\,x\\,y,\\quad x\\in\\{1,2\\},\\;y\\in\\{1,2,3\\}.$$\n归一化可得\n$$1=\\sum_{x=1}^2\\sum_{y=1}^3p_{X,Y}(x,y)\n=\\;c\\sum_{x=1}^2x\\sum_{y=1}^3y\n=c\\bigl(1+2\\bigr)\\bigl(1+2+3\\bigr)\n=c\\cdot3\\cdot6\n=18c,$$\n因此\n$$c=\\frac1{18}.$$\n接下来，$X$ 在 $x=1$ 处的边缘概率为\n$$p_X(1)=\\sum_{y=1}^3p_{X,Y}(1,y)\n=\\sum_{y=1}^3\\frac1{18}\\cdot1\\cdot y\n=\\frac1{18}(1+2+3)\n=\\frac{6}{18}\n=\\frac13.$$\n在 $(1,3)$ 处的联合概率是\n$$p_{X,Y}(1,3)=\\frac1{18}\\cdot1\\cdot3=\\frac3{18}=\\frac16.$$\n因此，条件概率是\n$$p_{Y\\mid X}(3\\mid1)\n=\\frac{p_{X,Y}(1,3)}{p_X(1)}\n=\\frac{\\frac16}{\\frac13}\n=\\frac12.$$", "answer": "$$\\boxed{\\frac{1}{2}}$$", "id": "2512"}, {"introduction": "掌握了基本计算后，我们可以将其应用于统计学习中的实际问题。本练习将展示高斯判别分析（GDA）如何利用类条件分布 $p(\\mathbf{x}|y)$ 和贝叶斯定理来构建分类决策边界。通过这个例子，你将理解为何掌握这些概率分布对于构建和解释生成式模型至关重要。[@problem_id:3134073]", "problem": "考虑一个二元分类问题，其中特征向量为 $\\mathbf{x} \\in \\mathbb{R}^{2}$，类别标签为 $y \\in \\{1,2\\}$。在高斯判别分析 (GDA) 中，我们假设类条件分布是多元正态分布，并且联合分布可分解为 $p(\\mathbf{x}, y) = p(y)p(\\mathbf{x}|y)$。假设您从以下生成模型中模拟数据（无需进行模拟；提供此规范仅为推导提供依据）：\n\n- 类先验概率：$p(y=1) = \\pi_{1} = 0.6$ 和 $p(y=2) = \\pi_{2} = 0.4$。\n- 类条件分布：$p(\\mathbf{x}|y=k) = \\mathcal{N}(\\boldsymbol{\\mu}_{k}, \\boldsymbol{\\Sigma}_{k})$，对于 $k \\in \\{1,2\\}$，其参数为\n$$\n\\boldsymbol{\\mu}_{1} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}, \\quad \\boldsymbol{\\Sigma}_{1} = \\begin{pmatrix} 2  1 \\\\ 1  2 \\end{pmatrix}, \\qquad\n\\boldsymbol{\\mu}_{2} = \\begin{pmatrix} -1 \\\\ 1 \\end{pmatrix}, \\quad \\boldsymbol{\\Sigma}_{2} = \\begin{pmatrix} 1  0 \\\\ 0  3 \\end{pmatrix}.\n$$\n\n仅从联合分布、条件分布和贝叶斯法则的基本定义出发，推导比较后验概率 $p(y=1|\\mathbf{x})$ 和 $p(y=2|\\mathbf{x})$ 的决策规则，并证明该规则会产生一个二次决策边界。然后，使用指定的参数，明确计算判别边界函数 $g(\\mathbf{x})$，其零水平集 $g(\\mathbf{x}) = 0$ 用于分隔两个类别。其中 $g(\\mathbf{x})$ 是对数后验概率差 $g(\\mathbf{x}) = \\ln p(y=1|\\mathbf{x}) - \\ln p(y=2|\\mathbf{x})$，不考虑一个对两类都相同的加法常数。\n\n将最终的 $g(\\mathbf{x})$ 表示为关于 $x_{1}$ 和 $x_{2}$ 的单个解析表达式（不带等式或不等式），并尽可能化简。无需进行数值四舍五入。", "solution": "目标是找到分隔两个类别的决策边界。该边界由后验概率相等的点集 $\\mathbf{x}$ 定义：$p(y=1|\\mathbf{x}) = p(y=2|\\mathbf{x})$。这等价于对数后验概率相等，即 $\\ln p(y=1|\\mathbf{x}) = \\ln p(y=2|\\mathbf{x})$，或者它们的差为零：$g(\\mathbf{x}) = \\ln p(y=1|\\mathbf{x}) - \\ln p(y=2|\\mathbf{x}) = 0$。\n\n根据贝叶斯法则，后验概率为 $p(y=k|\\mathbf{x}) = \\frac{p(\\mathbf{x}|y=k)p(y=k)}{p(\\mathbf{x})}$。\n对数后验概率为 $\\ln p(y=k|\\mathbf{x}) = \\ln p(\\mathbf{x}|y=k) + \\ln p(y=k) - \\ln p(\\mathbf{x})$。\n\n判别函数 $g(\\mathbf{x})$ 为：\n$$g(\\mathbf{x}) = (\\ln p(\\mathbf{x}|y=1) + \\ln p(y=1) - \\ln p(\\mathbf{x})) - (\\ln p(\\mathbf{x}|y=2) + \\ln p(y=2) - \\ln p(\\mathbf{x}))$$\n$$g(\\mathbf{x}) = \\ln p(\\mathbf{x}|y=1) - \\ln p(\\mathbf{x}|y=2) + \\ln \\pi_1 - \\ln \\pi_2$$\n项 $\\ln p(\\mathbf{x})$ 被消掉了，这与问题陈述中在定义判别分数时忽略对两类都相同的加法常数相对应。\n\n类条件分布 $p(\\mathbf{x}|y=k)$ 是一个维度 $d=2$ 的多元正态分布：\n$$p(\\mathbf{x}|y=k) = \\frac{1}{(2\\pi)^{d/2}|\\boldsymbol{\\Sigma}_k|^{1/2}} \\exp\\left(-\\frac{1}{2}(\\mathbf{x}-\\boldsymbol{\\mu}_k)^T \\boldsymbol{\\Sigma}_k^{-1} (\\mathbf{x}-\\boldsymbol{\\mu}_k)\\right)$$\n取自然对数得到：\n$$\\ln p(\\mathbf{x}|y=k) = -\\frac{d}{2}\\ln(2\\pi) - \\frac{1}{2}\\ln|\\boldsymbol{\\Sigma}_k| - \\frac{1}{2}(\\mathbf{x}-\\boldsymbol{\\mu}_k)^T \\boldsymbol{\\Sigma}_k^{-1} (\\mathbf{x}-\\boldsymbol{\\mu}_k)$$\n项 $(\\mathbf{x}-\\boldsymbol{\\mu}_k)^T \\boldsymbol{\\Sigma}_k^{-1} (\\mathbf{x}-\\boldsymbol{\\mu}_k)$ 可以展开为 $\\mathbf{x}^T\\boldsymbol{\\Sigma}_k^{-1}\\mathbf{x} - 2\\boldsymbol{\\mu}_k^T\\boldsymbol{\\Sigma}_k^{-1}\\mathbf{x} + \\boldsymbol{\\mu}_k^T\\boldsymbol{\\Sigma}_k^{-1}\\boldsymbol{\\mu}_k$。这是一个关于 $\\mathbf{x}$ 的二次函数。决策边界是通过将两个这样的对数概率之差设为一个常数来确定的，这会得到一个关于 $\\mathbf{x}$ 各分量的二次方程。除非协方差矩阵相等（$\\boldsymbol{\\Sigma}_1 = \\boldsymbol{\\Sigma}_2$），在这种情况下，二次项 $\\mathbf{x}^T\\boldsymbol{\\Sigma}^{-1}\\mathbf{x}$ 会抵消，从而得到一个线性边界。由于本题中 $\\boldsymbol{\\Sigma}_1 \\neq \\boldsymbol{\\Sigma}_2$，因此边界是二次的。\n\n我们为每个类别定义判别分数，省略公共常数 $-\\frac{d}{2}\\ln(2\\pi)$：\n$$\\delta_k(\\mathbf{x}) = -\\frac{1}{2}\\ln|\\boldsymbol{\\Sigma}_k| - \\frac{1}{2}(\\mathbf{x}-\\boldsymbol{\\mu}_k)^T \\boldsymbol{\\Sigma}_k^{-1} (\\mathbf{x}-\\boldsymbol{\\mu}_k) + \\ln\\pi_k$$\n所求函数为 $g(\\mathbf{x}) = \\delta_1(\\mathbf{x}) - \\delta_2(\\mathbf{x})$。\n\n首先，我们根据给定的参数计算必要的组成部分：\n-   行列式：$|\\boldsymbol{\\Sigma}_1| = (2)(2)-(1)(1) = 3$。$|\\boldsymbol{\\Sigma}_2| = (1)(3)-(0)(0) = 3$。由于 $|\\boldsymbol{\\Sigma}_1|=|\\boldsymbol{\\Sigma}_2|$，项 $-\\frac{1}{2}\\ln|\\boldsymbol{\\Sigma}_k|$ 将在 $g(\\mathbf{x})$ 中抵消。\n-   逆协方差矩阵：\n    $$\\boldsymbol{\\Sigma}_1^{-1} = \\frac{1}{3}\\begin{pmatrix} 2  -1 \\\\ -1  2 \\end{pmatrix} = \\begin{pmatrix} \\frac{2}{3}  -\\frac{1}{3} \\\\ -\\frac{1}{3}  \\frac{2}{3} \\end{pmatrix}$$\n    $$\\boldsymbol{\\Sigma}_2^{-1} = \\frac{1}{3}\\begin{pmatrix} 3  0 \\\\ 0  1 \\end{pmatrix} = \\begin{pmatrix} 1  0 \\\\ 0  \\frac{1}{3} \\end{pmatrix}$$\n-   先验概率的对数：$\\ln \\pi_1 = \\ln(0.6)$，$\\ln \\pi_2 = \\ln(0.4)$。它们的差是 $\\ln(0.6) - \\ln(0.4) = \\ln(\\frac{0.6}{0.4}) = \\ln(\\frac{3}{2})$。\n\n现在，我们展开二次型。\n对于类别 1，其中 $\\mathbf{x} = \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix}$ 且 $\\boldsymbol{\\mu}_1 = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$：\n$$(\\mathbf{x}-\\boldsymbol{\\mu}_1)^T \\boldsymbol{\\Sigma}_1^{-1} (\\mathbf{x}-\\boldsymbol{\\mu}_1) = \\frac{2}{3}x_1^2 - \\frac{2}{3}x_1x_2 + \\frac{2}{3}x_2^2 - \\frac{4}{3}x_1 + \\frac{2}{3}x_2 + \\frac{2}{3}$$\n对 $\\delta_1(\\mathbf{x})$ 的贡献是此值的 $-\\frac{1}{2}$：\n$$-\\frac{1}{2}(\\mathbf{x}-\\boldsymbol{\\mu}_1)^T \\boldsymbol{\\Sigma}_1^{-1} (\\mathbf{x}-\\boldsymbol{\\mu}_1) = -\\frac{1}{3}x_1^2 + \\frac{1}{3}x_1x_2 - \\frac{1}{3}x_2^2 + \\frac{2}{3}x_1 - \\frac{1}{3}x_2 - \\frac{1}{3}$$\n\n对于类别 2，其中 $\\boldsymbol{\\mu}_2 = \\begin{pmatrix} -1 \\\\ 1 \\end{pmatrix}$：\n$$(\\mathbf{x}-\\boldsymbol{\\mu}_2)^T \\boldsymbol{\\Sigma}_2^{-1} (\\mathbf{x}-\\boldsymbol{\\mu}_2) = x_1^2 + \\frac{1}{3}x_2^2 + 2x_1 - \\frac{2}{3}x_2 + \\frac{4}{3}$$\n对 $\\delta_2(\\mathbf{x})$ 的贡献是此值的 $-\\frac{1}{2}$：\n$$-\\frac{1}{2}(\\mathbf{x}-\\boldsymbol{\\mu}_2)^T \\boldsymbol{\\Sigma}_2^{-1} (\\mathbf{x}-\\boldsymbol{\\mu}_2) = -\\frac{1}{2}x_1^2 - \\frac{1}{6}x_2^2 - x_1 + \\frac{1}{3}x_2 - \\frac{2}{3}$$\n\n现在我们计算 $g(\\mathbf{x}) = \\delta_1(\\mathbf{x}) - \\delta_2(\\mathbf{x})$。$\\ln|\\boldsymbol{\\Sigma}_k|$ 项抵消了。\n$$g(\\mathbf{x}) = \\left(-\\frac{1}{2}(\\mathbf{x}-\\boldsymbol{\\mu}_1)^T \\boldsymbol{\\Sigma}_1^{-1} (\\mathbf{x}-\\boldsymbol{\\mu}_1) + \\ln\\pi_1\\right) - \\left(-\\frac{1}{2}(\\mathbf{x}-\\boldsymbol{\\mu}_2)^T \\boldsymbol{\\Sigma}_2^{-1} (\\mathbf{x}-\\boldsymbol{\\mu}_2) + \\ln\\pi_2\\right)$$\n代入展开后的形式：\n$$g(\\mathbf{x}) = \\left(-\\frac{1}{3}x_1^2 + \\frac{1}{3}x_1x_2 - \\frac{1}{3}x_2^2 + \\frac{2}{3}x_1 - \\frac{1}{3}x_2 - \\frac{1}{3}\\right) - \\left(-\\frac{1}{2}x_1^2 - \\frac{1}{6}x_2^2 - x_1 + \\frac{1}{3}x_2 - \\frac{2}{3}\\right) + \\ln\\left(\\frac{3}{2}\\right)$$\n我们按 $x_1$ 和 $x_2$ 的幂次合并同类项：\n-   $x_1^2$ 项：$(-\\frac{1}{3} + \\frac{1}{2})x_1^2 = \\frac{1}{6}x_1^2$\n-   $x_2^2$ 项：$(-\\frac{1}{3} + \\frac{1}{6})x_2^2 = -\\frac{1}{6}x_2^2$\n-   $x_1x_2$ 项：$\\frac{1}{3}x_1x_2$\n-   $x_1$ 项：$(\\frac{2}{3} - (-1))x_1 = \\frac{5}{3}x_1$\n-   $x_2$ 项：$(-\\frac{1}{3} - \\frac{1}{3})x_2 = -\\frac{2}{3}x_2$\n-   常数项：$(-\\frac{1}{3} - (-\\frac{2}{3})) + \\ln(\\frac{3}{2}) = \\frac{1}{3} + \\ln(\\frac{3}{2})$\n\n合并所有项，得到判别函数 $g(\\mathbf{x})$ 的最终表达式：\n$$g(\\mathbf{x}) = \\frac{1}{6}x_1^2 - \\frac{1}{6}x_2^2 + \\frac{1}{3}x_1x_2 + \\frac{5}{3}x_1 - \\frac{2}{3}x_2 + \\frac{1}{3} + \\ln\\left(\\frac{3}{2}\\right)$$", "answer": "$$\\boxed{\\frac{1}{6}x_{1}^{2} - \\frac{1}{6}x_{2}^{2} + \\frac{1}{3}x_{1}x_{2} + \\frac{5}{3}x_{1} - \\frac{2}{3}x_{2} + \\frac{1}{3} + \\ln\\left(\\frac{3}{2}\\right)}$$", "id": "3134073"}, {"introduction": "最后的练习将探讨一个微妙但至关重要的概念：对撞机偏误（collider bias），即条件化如何能在原本独立的变量间引入依赖关系。通过分析一个关于特征选择的假设情景，你将看到看似无害的数据筛选如何引入虚假的相关性。对于任何数据科学家来说，这都是避免统计谬误、审慎推理数据生成过程的重要一课。[@problem_id:3134074]", "problem": "在一个统计学习任务中，一个数据整理流水线使用一个基于结果的过滤器来仅保留“有希望的”案例。假设两个二元特征 $X \\in \\{0,1\\}$ 和 $Y \\in \\{0,1\\}$ 分别表示两个独立的探测器是否对一个项目触发，其中 $\\mathbb{P}(X=1)=\\mathbb{P}(Y=1)=\\frac{1}{2}$，并且 $X$ 和 $Y$ 是独立的，即对于所有 $x,y \\in \\{0,1\\}$，都有 $\\mathbb{P}(X=x, Y=y)=\\mathbb{P}(X=x)\\mathbb{P}(Y=y)$。该流水线的过滤变量 $Z \\in \\{0,1\\}$ 被确定性地定义为 $Z=\\mathbf{1}\\{X+Y \\geq 1\\}$，这意味着如果至少一个探测器触发，则该项目被保留。只有 $Z=1$ 的项目被包含在训练集中。\n\n从联合分布、条件概率 $\\mathbb{P}(A \\mid B)=\\frac{\\mathbb{P}(A \\cap B)}{\\mathbb{P}(B)}$ 和独立性的定义出发，推导所有 $x,y \\in \\{0,1\\}$ 的条件联合分布 $\\mathbb{P}(X=x, Y=y \\mid Z=1)$。通过比较 $\\mathbb{P}(X=1, Y=1 \\mid Z=1)$ 与 $\\mathbb{P}(X=1 \\mid Z=1)\\mathbb{P}(Y=1 \\mid Z=1)$，用此证明在给定 $Z=1$ 的条件下，$X$ 和 $Y$ 不是条件独立的。然后，计算条件相关性 $\\operatorname{Corr}(X,Y \\mid Z=1)$，其中 $\\operatorname{Corr}(X,Y \\mid Z=1)=\\frac{\\operatorname{Cov}(X,Y \\mid Z=1)}{\\sqrt{\\operatorname{Var}(X \\mid Z=1)\\operatorname{Var}(Y \\mid Z=1)}}$，$\\operatorname{Cov}(X,Y \\mid Z=1)=\\mathbb{E}[XY \\mid Z=1]-\\mathbb{E}[X \\mid Z=1]\\mathbb{E}[Y \\mid Z=1]$，以及 $\\operatorname{Var}(X \\mid Z=1)=\\mathbb{E}[X^{2} \\mid Z=1]-\\left(\\mathbb{E}[X \\mid Z=1]\\right)^{2}$。\n\n简要解释为什么以结果过滤器 $Z$（它是 $X$ 和 $Y$ 的函数）为条件会引入 $X$ 和 $Y$ 之间的依赖性，即使 $X$ 和 $Y$ 是边缘独立的，并将此与基于结果保留项目的特征选择的潜在陷阱联系起来。给出 $\\operatorname{Corr}(X,Y \\mid Z=1)$ 的最终数值答案。无需四舍五入。", "solution": "首先，我们建立二元特征 $X$ 和 $Y$ 的边缘概率和联合概率。我们已知 $\\mathbb{P}(X=1) = \\frac{1}{2}$ 和 $\\mathbb{P}(Y=1) = \\frac{1}{2}$。由于 $X$ 和 $Y$ 是二元变量，因此可得 $\\mathbb{P}(X=0) = 1 - \\mathbb{P}(X=1) = 1 - \\frac{1}{2} = \\frac{1}{2}$ 和 $\\mathbb{P}(Y=0) = 1 - \\mathbb{P}(Y=1) = 1 - \\frac{1}{2} = \\frac{1}{2}$。\n\n问题陈述 $X$ 和 $Y$ 是独立的，意味着对于所有 $x, y \\in \\{0, 1\\}$，都有 $\\mathbb{P}(X=x, Y=y) = \\mathbb{P}(X=x)\\mathbb{P}(Y=y)$。我们可以计算出完整的联合分布：\n$$ \\mathbb{P}(X=0, Y=0) = \\mathbb{P}(X=0)\\mathbb{P}(Y=0) = \\frac{1}{2} \\times \\frac{1}{2} = \\frac{1}{4} $$\n$$ \\mathbb{P}(X=0, Y=1) = \\mathbb{P}(X=0)\\mathbb{P}(Y=1) = \\frac{1}{2} \\times \\frac{1}{2} = \\frac{1}{4} $$\n$$ \\mathbb{P}(X=1, Y=0) = \\mathbb{P}(X=1)\\mathbb{P}(Y=0) = \\frac{1}{2} \\times \\frac{1}{2} = \\frac{1}{4} $$\n$$ \\mathbb{P}(X=1, Y=1) = \\mathbb{P}(X=1)\\mathbb{P}(Y=1) = \\frac{1}{2} \\times \\frac{1}{2} = \\frac{1}{4} $$\n这些概率的总和是 $\\frac{1}{4} + \\frac{1}{4} + \\frac{1}{4} + \\frac{1}{4} = 1$，正如预期。\n\n过滤变量 $Z$ 定义为 $Z = \\mathbf{1}\\{X+Y \\geq 1\\}$。这意味着如果至少一个探测器触发，则 $Z=1$，否则 $Z=0$。事件 $Z=1$ 是不相交事件 $\\{X=0, Y=1\\}$、$\\{X=1, Y=0\\}$ 和 $\\{X=1, Y=1\\}$ 的并集。或者，事件 $Z=1$ 是事件 $\\{X=0, Y=0\\}$ 的补集。我们可以计算其概率：\n$$ \\mathbb{P}(Z=1) = \\mathbb{P}(X+Y \\geq 1) = 1 - \\mathbb{P}(X+Y  1) = 1 - \\mathbb{P}(X=0, Y=0) = 1 - \\frac{1}{4} = \\frac{3}{4} $$\n\n现在，我们使用条件概率公式 $\\mathbb{P}(A \\mid B) = \\frac{\\mathbb{P}(A \\cap B)}{\\mathbb{P}(B)}$ 来推导条件联合分布 $\\mathbb{P}(X=x, Y=y \\mid Z=1)$。\n$$ \\mathbb{P}(X=x, Y=y \\mid Z=1) = \\frac{\\mathbb{P}(\\{X=x, Y=y\\} \\cap \\{Z=1\\})}{\\mathbb{P}(Z=1)} $$\n如果 $x+y  1$（即 $(x,y)=(0,0)$），事件 $\\{X=x, Y=y\\}$ 与 $\\{Z=1\\}$ 不相容，所以它们的交集是空集，概率为 $0$。\n如果 $x+y \\geq 1$，事件 $\\{X=x, Y=y\\}$ 意味着 $Z=1$，所以交集 $\\{X=x, Y=y\\} \\cap \\{Z=1\\}$ 就是 $\\{X=x, Y=y\\}$。\n因此，对于所有满足 $x+y \\geq 1$ 的 $(x,y)$：\n$$ \\mathbb{P}(X=x, Y=y \\mid Z=1) = \\frac{\\mathbb{P}(X=x, Y=y)}{\\mathbb{P}(Z=1)} $$\n我们计算条件联合概率：\n$$ \\mathbb{P}(X=0, Y=0 \\mid Z=1) = 0 $$\n$$ \\mathbb{P}(X=0, Y=1 \\mid Z=1) = \\frac{\\mathbb{P}(X=0, Y=1)}{\\mathbb{P}(Z=1)} = \\frac{1/4}{3/4} = \\frac{1}{3} $$\n$$ \\mathbb{P}(X=1, Y=0 \\mid Z=1) = \\frac{\\mathbb{P}(X=1, Y=0)}{\\mathbb{P}(Z=1)} = \\frac{1/4}{3/4} = \\frac{1}{3} $$\n$$ \\mathbb{P}(X=1, Y=1 \\mid Z=1) = \\frac{\\mathbb{P}(X=1, Y=1)}{\\mathbb{P}(Z=1)} = \\frac{1/4}{3/4} = \\frac{1}{3} $$\n\n为了证明在给定 $Z=1$ 的条件下 $X$ 和 $Y$ 不是条件独立的，我们必须证明对于至少一对 $(x,y)$，有 $\\mathbb{P}(X=x, Y=y \\mid Z=1) \\neq \\mathbb{P}(X=x \\mid Z=1)\\mathbb{P}(Y=y \\mid Z=1)$。让我们使用 $(x,y)=(1,1)$。首先，我们求出条件边缘概率：\n$$ \\mathbb{P}(X=1 \\mid Z=1) = \\sum_{y \\in \\{0,1\\}} \\mathbb{P}(X=1, Y=y \\mid Z=1) = \\mathbb{P}(X=1, Y=0 \\mid Z=1) + \\mathbb{P}(X=1, Y=1 \\mid Z=1) = \\frac{1}{3} + \\frac{1}{3} = \\frac{2}{3} $$\n根据对称性，\n$$ \\mathbb{P}(Y=1 \\mid Z=1) = \\sum_{x \\in \\{0,1\\}} \\mathbb{P}(X=x, Y=1 \\mid Z=1) = \\mathbb{P}(X=0, Y=1 \\mid Z=1) + \\mathbb{P}(X=1, Y=1 \\mid Z=1) = \\frac{1}{3} + \\frac{1}{3} = \\frac{2}{3} $$\n现在我们将 $\\mathbb{P}(X=1, Y=1 \\mid Z=1)$ 与条件边缘概率的乘积进行比较：\n$$ \\mathbb{P}(X=1, Y=1 \\mid Z=1) = \\frac{1}{3} $$\n$$ \\mathbb{P}(X=1 \\mid Z=1)\\mathbb{P}(Y=1 \\mid Z=1) = \\frac{2}{3} \\times \\frac{2}{3} = \\frac{4}{9} $$\n由于 $\\frac{1}{3} \\neq \\frac{4}{9}$，我们证明了在给定 $Z=1$ 的条件下，$X$ 和 $Y$ 不是条件独立的。\n\n接下来，我们计算条件相关性 $\\operatorname{Corr}(X,Y \\mid Z=1)$。我们需要计算条件期望、方差和协方差。\n伯努利变量的条件期望是其取值为 $1$ 的条件概率。\n$$ \\mathbb{E}[X \\mid Z=1] = \\mathbb{P}(X=1 \\mid Z=1) = \\frac{2}{3} $$\n$$ \\mathbb{E}[Y \\mid Z=1] = \\mathbb{P}(Y=1 \\mid Z=1) = \\frac{2}{3} $$\n对于一个伯努利变量，$X^2=X$，所以 $\\mathbb{E}[X^2 \\mid Z=1] = \\mathbb{E}[X \\mid Z=1] = \\frac{2}{3}$。\n$X$ 的条件方差是：\n$$ \\operatorname{Var}(X \\mid Z=1) = \\mathbb{E}[X^2 \\mid Z=1] - \\left(\\mathbb{E}[X \\mid Z=1]\\right)^2 = \\frac{2}{3} - \\left(\\frac{2}{3}\\right)^2 = \\frac{2}{3} - \\frac{4}{9} = \\frac{6}{9} - \\frac{4}{9} = \\frac{2}{9} $$\n根据对称性，$\\operatorname{Var}(Y \\mid Z=1) = \\frac{2}{9}$。\n\n乘积 $XY$ 也是一个伯努利变量，当且仅当 $X=1$ 且 $Y=1$ 时，其值为 $1$。\n$$ \\mathbb{E}[XY \\mid Z=1] = \\mathbb{P}(XY=1 \\mid Z=1) = \\mathbb{P}(X=1, Y=1 \\mid Z=1) = \\frac{1}{3} $$\n条件协方差是：\n$$ \\operatorname{Cov}(X,Y \\mid Z=1) = \\mathbb{E}[XY \\mid Z=1] - \\mathbb{E}[X \\mid Z=1]\\mathbb{E}[Y \\mid Z=1] = \\frac{1}{3} - \\left(\\frac{2}{3}\\right)\\left(\\frac{2}{3}\\right) = \\frac{1}{3} - \\frac{4}{9} = \\frac{3}{9} - \\frac{4}{9} = -\\frac{1}{9} $$\n最后，条件相关性是：\n$$ \\operatorname{Corr}(X,Y \\mid Z=1) = \\frac{\\operatorname{Cov}(X,Y \\mid Z=1)}{\\sqrt{\\operatorname{Var}(X \\mid Z=1)\\operatorname{Var}(Y \\mid Z=1)}} = \\frac{-1/9}{\\sqrt{(\\frac{2}{9})(\\frac{2}{9})}} = \\frac{-1/9}{2/9} = -\\frac{1}{2} $$\n\n两个独立的变量在以一个共同效应为条件时变得相关的现象被称为 Berkson悖论 或 对撞偏倚。在这里，$Z$ 是一个“对撞因子”，因为它的值由 $X$ 和 $Y$ 共同决定。我们选择了一个 $Z=1$ 的子群体，即至少一个探测器触发的群体。在这个选定的群体中，关于一个变量的信息会提供关于另一个变量的信息。例如，如果我们处于 $Z=1$ 的群体中，并且观察到 $X=0$，我们就可以确定地知道 $Y$ 必须为 $1$ 才能满足条件 $X+Y \\geq 1$。这就产生了一种依赖关系。计算出的负相关性 $\\left(-\\frac{1}{2}\\right)$ 表明，在经过筛选的数据集中，如果一个探测器触发了，那么另一个探测器也触发的可能性要小于基线条件概率。具体来说，$\\mathbb{P}(Y=1 \\mid X=1, Z=1) = \\frac{\\mathbb{P}(X=1, Y=1 \\mid Z=1)}{\\mathbb{P}(X=1 \\mid Z=1)} = \\frac{1/3}{2/3} = \\frac{1}{2}$，这小于 $\\mathbb{P}(Y=1 \\mid Z=1) = \\frac{2}{3}$。在统计学习中，如果通过对作为结果函数（或结果本身）的变量进行筛选来执行特征选择或数据整理，这可能会在总体中独立的预测变量之间引入虚假的相关性。这可能导致不正确的模型设定和关于预测变量之间关系的错误科学结论。", "answer": "$$\\boxed{-\\frac{1}{2}}$$", "id": "3134074"}]}