{"hands_on_practices": [{"introduction": "向量为表示高维数据提供了一种强有力的手段。在系统生物学等领域，一个细胞的复杂状态（由成千上万个基因的表达水平表征）可以被建模为高维空间中的一个点。本练习 [@problem_id:1477133] 将引导你计算两个此类状态向量之间的欧几里得距离，展示一个简单的几何概念如何能够量化复杂的生物学变化。", "problem": "在系统生物学中，一个生物细胞的状态可以被建模为高维“状态空间”中的一个向量，其中向量的每个分量代表一个特定基因的表达水平。细胞条件的改变，例如疾病的发生，对应于这个状态向量的一个位移。\n\n一位研究人员正在研究一个涉及四个关键基因的特定细胞通路：Gene Alpha、Gene Beta、Gene Gamma 和 Gene Delta。这些基因的表达水平被用来定义一个四维状态向量。\n\n对于一个健康细胞，测量其归一化表达水平，形成“健康状态向量”$V_H$。该向量的分量，对应于 (Alpha, Beta, Gamma, Delta)，由下式给出：\n$$ V_H = (7.5, 11.2, 5.1, 14.3) $$\n\n对于一个处于患病状态的细胞，测量同一组基因的表达水平，形成“患病状态向量”$V_D$：\n$$ V_D = (9.1, 6.8, 5.9, 10.4) $$\n\n表达水平以任意单位 (a.u.) 给出。为了量化细胞状态的总变化，请计算在这个四维基因表达空间中，健康状态和患病状态之间位移的大小。\n\n你的最终答案应以任意单位表示，并四舍五入到三位有效数字。", "solution": "在四维基因表达空间中，健康和患病细胞状态之间的位移是向量 $V_{H}$ 和 $V_{D}$ 之间的欧几里得距离。将位移向量记为 $\\Delta V$，定义为 $\\Delta V = V_{D} - V_{H}$。其大小由下式给出\n$$\n\\|\\Delta V\\| = \\sqrt{\\sum_{i=1}^{4} (V_{D,i} - V_{H,i})^{2}}.\n$$\n计算各分量的差值：\n$$\n\\Delta V = (9.1 - 7.5,\\; 6.8 - 11.2,\\; 5.9 - 5.1,\\; 10.4 - 14.3) = (1.6,\\; -4.4,\\; 0.8,\\; -3.9).\n$$\n将每个分量平方然后求和：\n$$\n1.6^{2} = 2.56,\\quad (-4.4)^{2} = 19.36,\\quad 0.8^{2} = 0.64,\\quad (-3.9)^{2} = 15.21,\n$$\n$$\n2.56 + 19.36 + 0.64 + 15.21 = 37.77.\n$$\n取平方根以获得其大小：\n$$\n\\|\\Delta V\\| = \\sqrt{37.77} \\approx 6.14573.\n$$\n四舍五入到三位有效数字（以任意单位计），结果为 $6.15$。", "answer": "$$\\boxed{6.15}$$", "id": "1477133"}, {"introduction": "矩阵是表示线性变换和方程组的基础。在代谢工程中，化学计量矩阵可以编码网络中代谢物和反应之间的关系。通过执行矩阵与向量的乘法，正如你将在本实践问题 [@problem_id:1477176] 中所做的那样，你可以模拟网络的动态并预测代谢物浓度的变化率，这是系统生物学中的一项关键任务。", "problem": "在系统生物学中，细胞内代谢物浓度的动态变化可以通过基于反应化学计量的线性系统来建模。考虑一个假设的微生物体内一个简化的代谢网络，该网络产生一种有价值的化合物 C。该网络涉及三种关键的内源代谢物：A、B 和 C。它们的相互作用由五个反应描述，每个反应都有特定的速率或通量。\n\n该系统的化学计量关系由化学计量矩阵 $S$ 表示。$S$ 的行对应于代谢物（按 A、B、C 的顺序），列对应于五个反应通量（按 $v_1, v_2, v_3, v_4, v_5$ 的顺序）。化学计量矩阵 $S$ 如下所示：\n$$\nS = \\begin{pmatrix}\n1  0  -1  0  -1 \\\\\n0  1  -2  0  0 \\\\\n0  0  1  -1  0\n\\end{pmatrix}\n$$\n在这里，正值表示代谢物的生成，负值表示消耗，零表示不参与反应。\n\n在特定时刻，该生物体的代謝状态由一个通量向量 $\\mathbf{v}$ 来表征，其中每个分量代表相应反应的速率。测得的通量由以下向量给出：\n$$\n\\mathbf{v} = \\begin{pmatrix} 10 \\\\ 12 \\\\ 4 \\\\ 3 \\\\ 2 \\end{pmatrix}\n$$\n所有通量的单位均为毫摩尔/克干重/小时 (mmol gDW$^{-1}$ h$^{-1}$)。\n\n请计算代表代谢物 A、B 和 C 浓度净变化率的向量。将您的答案表示为单个行矩阵，其数值单位为 mmol gDW$^{-1}$ h$^{-1}$。", "solution": "将代谢物浓度变化率与反应通量联系起来的基本方程由矩阵-向量乘积给出：\n$$\n\\frac{d\\mathbf{c}}{dt} = S\\mathbf{v}\n$$\n其中 $\\frac{d\\mathbf{c}}{dt}$ 是代谢物浓度变化率的向量，$S$ 是化学计量矩阵，$\\mathbf{v}$ 是通量向量。\n\n浓度变化向量，我们可以表示为 $\\mathbf{r} = \\begin{pmatrix} r_A  r_B  r_C \\end{pmatrix}^T$，是通过将给定的矩阵 $S$ 与向量 $\\mathbf{v}$ 相乘来计算的。\n\n给定的矩阵是：\n$$\nS = \\begin{pmatrix}\n1  0  -1  0  -1 \\\\\n0  1  -2  0  0 \\\\\n0  0  1  -1  0\n\\end{pmatrix}\n\\quad \\text{和} \\quad\n\\mathbf{v} = \\begin{pmatrix} 10 \\\\ 12 \\\\ 4 \\\\ 3 \\\\ 2 \\end{pmatrix}\n$$\n\n我们执行矩阵-向量乘法：\n$$\n\\begin{pmatrix} r_A \\\\ r_B \\\\ r_C \\end{pmatrix} = \\begin{pmatrix}\n1  0  -1  0  -1 \\\\\n0  1  -2  0  0 \\\\\n0  0  1  -1  0\n\\end{pmatrix}\n\\begin{pmatrix} 10 \\\\ 12 \\\\ 4 \\\\ 3 \\\\ 2 \\end{pmatrix}\n$$\n\n让我们计算结果向量的每个分量。\n\n对于第一个分量，即代谢物 A 的变化率 ($r_A$)：\n$$\nr_A = (1 \\times 10) + (0 \\times 12) + (-1 \\times 4) + (0 \\times 3) + (-1 \\times 2)\n$$\n$$\nr_A = 10 + 0 - 4 + 0 - 2 = 4\n$$\n\n对于第二个分量，即代谢物 B 的变化率 ($r_B$)：\n$$\nr_B = (0 \\times 10) + (1 \\times 12) + (-2 \\times 4) + (0 \\times 3) + (0 \\times 2)\n$$\n$$\nr_B = 0 + 12 - 8 + 0 + 0 = 4\n$$\n\n对于第三个分量，即代谢物 C 的变化率 ($r_C$)：\n$$\nr_C = (0 \\times 10) + (0 \\times 12) + (1 \\times 4) + (-1 \\times 3) + (0 \\times 2)\n$$\n$$\nr_C = 0 + 0 + 4 - 3 + 0 = 1\n$$\n\n因此，代谢物 A、B 和 C 浓度净变化率的向量是 $\\begin{pmatrix} 4 \\\\ 4 \\\\ 1 \\end{pmatrix}$。这些速率的单位与通量单位相同，即 mmol gDW$^{-1}$ h$^{-1}$。\n\n问题要求将答案表示为单个行矩阵。\n$$\n\\begin{pmatrix} 4  4  1 \\end{pmatrix}\n$$", "answer": "$$\\boxed{\\begin{pmatrix} 4  4  1 \\end{pmatrix}}$$", "id": "1477176"}, {"introduction": "除了基本运算，矩阵在机器学习中先进的数据预处理技术中也扮演着核心角色。白化变换能够去除特征间的相关性并使其具有单位方差，这对于优化许多算法的性能至关重要。这个高级练习 [@problem_id:3192878] 要求你实现并比较两种重要的白化方法——主成分分析（PCA）白化和零相位成分分析（ZCA）白化，以理解它们对下游分类任务的影响。", "problem": "在统计学习领域，您将面临一个基于数据集的比较任务，该任务聚焦于向量和矩阵。目标是实现并对比主成分分析（PCA）白化和零相位成分分析（ZCA）白化，然后量化这两种方法对下游岭回归（ridge regression）产生的线性分类间隔的差异。所有算法定义必须源于核心的线性代数事实和标准的统计学习公式。您的程序必须是一个完整、可运行的 Python 程序，执行以下步骤并产生可客观测试的结果。\n\n设数据矩阵为 $X \\in \\mathbb{R}^{n \\times d}$，其中行代表样本，列代表特征。定义中心化数据矩阵 $X_c = X - \\mathbf{1}\\mu^\\top$，其中 $\\mu \\in \\mathbb{R}^d$ 是样本均值向量，$\\mathbf{1} \\in \\mathbb{R}^{n}$ 是全为1的向量。根据经过充分检验的公式定义经验协方差矩阵\n$$\n\\Sigma = \\frac{1}{n} X_c^\\top X_c,\n$$\n该矩阵是对称且半正定的。协方差矩阵的特征分解由下式给出\n$$\n\\Sigma = U \\Lambda U^\\top,\n$$\n其中 $U \\in \\mathbb{R}^{d \\times d}$ 是正交矩阵，$\\Lambda \\in \\mathbb{R}^{d \\times d}$ 是对角线上元素非负的对角矩阵。为了在存在小特征值或零特征值时稳定白化过程，引入一个小的正则化参数 $\\delta > 0$ 并定义\n$$\n\\Lambda_\\delta = \\Lambda + \\delta I_d, \\quad \\Lambda_\\delta^{-1/2} = \\operatorname{diag}\\left(\\frac{1}{\\sqrt{\\lambda_i + \\delta}}\\right).\n$$\n在 $X_c$ 上定义 PCA 白化和 ZCA 白化变换如下\n$$\nX_{\\text{PCA}} = X_c U \\Lambda_\\delta^{-1/2}, \\quad X_{\\text{ZCA}} = X_c U \\Lambda_\\delta^{-1/2} U^\\top.\n$$\n\n对于线性分类，我们考虑岭回归（RR），这是一个标准的线性模型，通过最小化带惩罚的平方损失来拟合权重向量 $w \\in \\mathbb{R}^d$ 和截距 $b \\in \\mathbb{R}$。设标签为 $y \\in \\{-1, +1\\}^n$，设计矩阵为 $Z = [X \\ \\mathbf{1}] \\in \\mathbb{R}^{n \\times (d+1)}$。RR 的目标函数是\n$$\n\\min_{w, b} \\ \\|y - X w - b \\mathbf{1}\\|_2^2 + \\lambda \\|w\\|_2^2,\n$$\n其中 $\\lambda > 0$ 是正则化强度，截距 $b$ 不受惩罚。使用著名的正规方程推导，通过将梯度设置为零可获得唯一最小化解，得到\n$$\n\\theta^\\star = \\begin{bmatrix} w^\\star \\\\ b^\\star \\end{bmatrix} = \\left(Z^\\top Z + R\\right)^{-1} Z^\\top y,\n$$\n其中 $R = \\operatorname{diag}(\\lambda, \\ldots, \\lambda, 0) \\in \\mathbb{R}^{(d+1) \\times (d+1)}$ 将惩罚施加在特征权重上，而不是截距上。对于每个样本 $i$，几何间隔定义为\n$$\nm_i = \\frac{y_i \\left(w^{\\star\\top} x_i + b^\\star \\right)}{\\|w^\\star\\|_2},\n$$\n我们通过平均间隔 $\\bar{m} = \\frac{1}{n} \\sum_{i=1}^{n} m_i$ 和最小间隔 $m_{\\min} = \\min_i m_i$ 来总结分类器性能。\n\n您的任务是实现上述定义以完成以下操作：\n- 对数据进行中心化，\n- 计算协方差及其特征分解，\n- 应用带有特征值正则化 $\\delta$ 的 PCA 白化和 ZCA 白化，\n- 在每个白化数据集上训练一个岭回归分类器，并计算几何间隔，\n- 通过报告以下值来量化 ZCA 和 PCA 白化之间的差异\n$$\n\\Delta \\bar{m} = \\bar{m}_{\\text{ZCA}} - \\bar{m}_{\\text{PCA}}, \\quad \\Delta m_{\\min} = m_{\\min,\\text{ZCA}} - m_{\\min,\\text{PCA}}.\n$$\n\n测试套件。实现并评估以下三个测试用例。在问题说明中指定的角度必须以度为单位进行解释；请确保在代码中正确转换为弧度。\n\n- 测试用例 1（一般各向异性、旋转协方差；正常路径）：\n  - 维度 $d = 2$；每个类别的样本数 $n_+ = 80$, $n_- = 80$，所以 $n = 160$。\n  - 类别均值：$\\mu_+ = [2, 0]^\\top$, $\\mu_- = [-2, 0]^\\top$。\n  - 基础协方差：$\\operatorname{diag}(3, 0.2)$。\n  - 旋转角度：$30^\\circ$。设旋转矩阵为\n    $$\n    R(\\theta) = \\begin{bmatrix} \\cos\\theta  -\\sin\\theta \\\\ \\sin\\theta  \\cos\\theta \\end{bmatrix}\n    $$\n    其中 $\\theta = 30^\\circ$ 转换为弧度，并为两个类别设置 $\\Sigma_{\\text{rot}} = R(\\theta) \\operatorname{diag}(3, 0.2) R(\\theta)^\\top$。\n  - 白化正则化 $\\delta = 10^{-5}$，岭回归强度 $\\lambda = 10^{-1}$。\n\n- 测试用例 2（近奇异协方差；边界条件）：\n  - 维度 $d = 2$；每个类别的样本数 $n_+ = 100$, $n_- = 100$，所以 $n = 200$。\n  - 类别均值：$\\mu_+ = [0.5, -0.5]^\\top$, $\\mu_- = [-0.5, 0.5]^\\top$。\n  - 基础协方差：$\\operatorname{diag}(10, 10^{-6})$。\n  - 旋转角度：$75^\\circ$。使用 $R(\\theta)$，其中 $\\theta = 75^\\circ$ 转换为弧度；设置 $\\Sigma_{\\text{rot}} = R(\\theta) \\operatorname{diag}(10, 10^{-6}) R(\\theta)^\\top$。\n  - 白化正则化 $\\delta = 10^{-3}$，岭回归强度 $\\lambda = 10^{-1}$。\n\n- 测试用例 3（已白化的数据；一个重要的边缘情况，预计 PCA 和 ZCA 的效果将在此对齐）：\n  - 维度 $d = 2$；总样本数 $n = 400$。\n  - 数据从具有单位协方差的标准正态分布中采样：$x_i \\sim \\mathcal{N}(0, I_2)$，标签由 $y_i = \\operatorname{sign}(x_{i,1})$ 确定性地定义，当 $x_{i,1} = 0$ 时，通过设置 $y_i = +1$ 来打破平局。\n  - 白化正则化 $\\delta = 10^{-5}$，岭回归强度 $\\lambda = 10^{-1}$。\n\n实现细节：\n- 使用固定的随机数生成器种子，以确保所有测试用例的可复现性。\n- 对于每个测试用例，通过堆叠两个类别（如果适用）来形成组合数据集，按所述进行中心化，使用指定的 $\\delta$ 计算白化变换，使用封闭形式的正规方程求解 $(w^\\star, b^\\star)$，并为 PCA 白化和 ZCA 白化数据计算 $\\bar{m}$ 和 $m_{\\min}$。\n- 最终输出必须表示为单行，其中包含一个用方括号括起来的逗号分隔列表。每个元素按顺序对应一个测试用例，并且必须是双元素列表 $[\\Delta \\bar{m}, \\Delta m_{\\min}]$，两个条目均为浮点数。\n- 您的程序应生成单行输出，其中包含用方括号括起来的逗号分隔列表形式的结果（例如，“[[a,b],[c,d],[e,f]]”）。程序不需要打印任何物理单位、角度或百分比；测试套件中指定的角度仅用于构建旋转矩阵，并且必须在内部转换为弧度。", "solution": "问题陈述经过严格验证，确认有效。它在科学上基于线性代数和统计学习的既定原则，问题定义清晰，提供了所有必要的参数和定义，并且其表述是客观的。\n\n该任务是一项计算练习，旨在基于标准且无矛盾的公式，实现并比较两种白化技术及其对下游分类任务的影响。\n\n解决方案首先为三个指定的测试用例实现一个数据生成方案。然后，应用一个通用流程来处理生成的数据，包括中心化、白化、训练分类器和计算性能指标。\n\n**1. 数据生成与问题设置**\n\n使用固定的随机种子以确保结果的可复现性。对于每个测试用例，根据规范生成一个数据矩阵 $X \\in \\mathbb{R}^{n \\times d}$ 和一个相应的标签向量 $y \\in \\{-1, +1\\}^n$。\n\n对于测试用例 1 和 2，数据从两个高斯分布的混合中抽取。\n- 设参数为维度 $d$、每个类别的样本数 $n_+$ 和 $n_-$、类别均值 $\\mu_+$ 和 $\\mu_-$、一个基础对角协方差矩阵 $\\Sigma_{\\text{base}}$ 以及一个旋转角度 $\\theta$。总样本数为 $n = n_+ + n_-$。\n- 旋转矩阵构造为 $R(\\theta) = \\begin{bmatrix} \\cos\\theta  -\\sin\\theta \\\\ \\sin\\theta  \\cos\\theta \\end{bmatrix}$，其中 $\\theta$ 从度数转换为弧度。\n- 两个类别的共享协方差矩阵为 $\\Sigma_{\\text{rot}} = R(\\theta) \\Sigma_{\\text{base}} R(\\theta)^\\top$。\n- $n_+$ 个样本从分布 $\\mathcal{N}(\\mu_+, \\Sigma_{\\text{rot}})$ 中抽取，并被赋予标签 $+1$。\n- $n_-$ 个样本从分布 $\\mathcal{N}(\\mu_-, \\Sigma_{\\text{rot}})$ 中抽取，并被赋予标签 $-1$。\n- 最终的数据矩阵 $X$ 通过堆叠这些样本形成，标签向量 $y$ 通过连接这些标签形成。\n\n对于测试用例 3，数据从标准正态分布中抽取。\n- $n$ 个样本 $x_i$ 从 $\\mathcal{N}(0, I_d)$ 中抽取，其中 $d$ 是维度，$I_d$ 是 $d \\times d$ 的单位矩阵。\n- 标签由第一个特征的符号确定：$y_i = \\operatorname{sign}(x_{i,1})$。根据规定，通过设置 $y_i = +1$ 来处理 $x_{i,1} = 0$ 的情况。\n\n**2. 数据中心化与协方差估计**\n\n给定一个数据矩阵 $X \\in \\mathbb{R}^{n \\times d}$，处理流程的第一步是对数据进行中心化。\n- 样本均值向量 $\\mu \\in \\mathbb{R}^d$ 计算为 $\\mu = \\frac{1}{n} \\sum_{i=1}^n x_i$，其中 $x_i$ 是 $X$ 的行向量。\n- 然后计算中心化数据矩阵 $X_c = X - \\mathbf{1}\\mu^\\top$，其中 $\\mathbf{1} \\in \\mathbb{R}^n$ 是一个全为1的列向量。\n- 经验协方差矩阵 $\\Sigma \\in \\mathbb{R}^{d \\times d}$ 使用公式 $\\Sigma = \\frac{1}{n} X_c^\\top X_c$ 从中心化数据计算得出。\n\n**3. 特征分解与白化变换**\n\n白化需要协方差矩阵的特征分解。\n- 由于 $\\Sigma$ 是一个对称半正定矩阵，其特征分解由 $\\Sigma = U \\Lambda U^\\top$ 给出，其中 $U$ 是特征向量组成的正交矩阵，$\\Lambda$ 是由相应的实数、非负特征值 $\\lambda_i$ 组成的对角矩阵。\n- 为了确保数值稳定性，特别是当某些特征值接近于零时，引入一个正则化参数 $\\delta > 0$。正则化后的特征值矩阵为 $\\Lambda_\\delta = \\Lambda + \\delta I_d$。\n- 该矩阵的逆平方根计算为 $\\Lambda_\\delta^{-1/2} = \\operatorname{diag}\\left(\\frac{1}{\\sqrt{\\lambda_i + \\delta}}\\right)$。\n\n使用这些组件，将两种白化变换应用于中心化数据 $X_c$：\n- **PCA 白化**：数据被投影到主轴（$U$ 的列）上并进行缩放。变换后的数据为 $X_{\\text{PCA}} = X_c U \\Lambda_\\delta^{-1/2}$。这个新数据的协方差近似为单位矩阵。\n- **ZCA 白化**：此变换将 PCA 白化后的数据旋转回去，使其尽可能接近原始数据。变换后的数据为 $X_{\\text{ZCA}} = X_c U \\Lambda_\\delta^{-1/2} U^\\top$。\n\n**4. 岭回归与间隔计算**\n\n对于 PCA 白化数据（$X_{\\text{PCA}}$）和 ZCA 白化数据（$X_{\\text{ZCA}}$），我们都训练一个岭回归分类器。设白化后的数据通常表示为 $X_w \\in \\mathbb{R}^{n \\times d}$。\n- 岭回归模型通过最小化目标函数 $\\|y - X_w w - b \\mathbf{1}\\|_2^2 + \\lambda \\|w\\|_2^2$ 来找到一个权重向量 $w \\in \\mathbb{R}^d$ 和一个截距 $b \\in \\mathbb{R}$，其中 $\\lambda > 0$ 是正则化强度。\n- 为了解决这个问题，我们构造一个增广设计矩阵 $Z = [X_w \\ \\mathbf{1}] \\in \\mathbb{R}^{n \\times (d+1)}$ 和一个相应的参数向量 $\\theta = [w^\\top \\ b]^\\top$。\n- 惩罚仅应用于权重 $w$，而不应用于截距 $b$。这被编码在惩罚矩阵 $R = \\operatorname{diag}(\\lambda, \\ldots, \\lambda, 0) \\in \\mathbb{R}^{(d+1) \\times (d+1)}$ 中。\n- 最优参数向量 $\\theta^\\star$ 通过正规方程找到：$\\theta^\\star = (Z^\\top Z + R)^{-1} Z^\\top y$。这个线性系统被数值求解以得到 $\\theta^\\star = [w^{\\star\\top} \\ b^\\star]^\\top$。\n\n一旦找到最优分类器 $(w^\\star, b^\\star)$，就计算每个样本 $i$ 的几何间隔。该间隔度量了样本到决策超平面 $w^{\\star\\top}x + b^\\star = 0$ 的带符号归一化距离。\n- 对于白化数据集 $X_w$ 中的每个样本 $x_i$，其间隔为 $m_i = \\frac{y_i (w^{\\star\\top} x_i + b^\\star)}{\\|w^\\star\\|_2}$。\n- 从这些间隔中计算出两个汇总统计量：平均间隔 $\\bar{m} = \\frac{1}{n} \\sum_{i=1}^n m_i$ 和最小间隔 $m_{\\min} = \\min_i m_i$。\n\n**5. 最终比较**\n\n整个过程为 PCA 白化数据（$\\bar{m}_{\\text{PCA}}, m_{\\min,\\text{PCA}}$）和 ZCA 白化数据（$\\bar{m}_{\\text{ZCA}}, m_{\\min,\\text{ZCA}}$）分别产出一个平均间隔和最小间隔。最后一步是按要求计算这些指标的差异：\n- $\\Delta \\bar{m} = \\bar{m}_{\\text{ZCA}} - \\bar{m}_{\\text{PCA}}$\n- $\\Delta m_{\\min} = m_{\\min,\\text{ZCA}} - m_{\\min,\\text{PCA}}$\n\n为每个测试用例报告这两个值，从而对两种白化方法对后续线性分类器几何间隔的影响进行定量比较。", "answer": "```python\nimport numpy as np\n# No other libraries are permitted by the problem statement.\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print the results.\n    \"\"\"\n    # Use a fixed random seed for reproducibility.\n    np.random.seed(42)\n\n    def run_case(params):\n        \"\"\"\n        Executes a single test case, from data generation to final metric calculation.\n        \"\"\"\n        # Step 1: Data Generation\n        case_type = params[\"type\"]\n        d = params[\"d\"]\n        lambda_val = params[\"lambda\"]\n        delta = params[\"delta\"]\n\n        if case_type == \"gaussian_mixture\":\n            n_pos = params[\"n_pos\"]\n            n_neg = params[\"n_neg\"]\n            n = n_pos + n_neg\n            mu_pos = np.array(params[\"mu_pos\"])\n            mu_neg = np.array(params[\"mu_neg\"])\n            sigma_base = np.diag(params[\"sigma_base_diag\"])\n            theta_deg = params[\"theta_deg\"]\n            \n            theta_rad = np.deg2rad(theta_deg)\n            c, s = np.cos(theta_rad), np.sin(theta_rad)\n            rot_matrix = np.array([[c, -s], [s, c]])\n            sigma_rot = rot_matrix @ sigma_base @ rot_matrix.T\n\n            X_pos = np.random.multivariate_normal(mu_pos, sigma_rot, n_pos)\n            X_neg = np.random.multivariate_normal(mu_neg, sigma_rot, n_neg)\n\n            X = np.vstack([X_pos, X_neg])\n            y = np.hstack([np.ones(n_pos), -np.ones(n_neg)])\n\n        elif case_type == \"std_normal\":\n            n = params[\"n\"]\n            X = np.random.multivariate_normal(np.zeros(d), np.identity(d), n)\n            y = np.sign(X[:, 0])\n            y[y == 0] = 1.0 # Handle the tie-breaking case\n        else:\n            raise ValueError(\"Unknown test case type\")\n\n        # Step 2: Centering and Covariance\n        mu = np.mean(X, axis=0)\n        X_c = X - mu\n        \n        # Ensure n is integer for division\n        n_samples = X_c.shape[0]\n        Sigma = (X_c.T @ X_c) / n_samples\n        \n        # Step 3: Eigendecomposition and Whitening Transforms\n        eigvals, U = np.linalg.eigh(Sigma)\n        Lambda_delta_inv_sqrt = np.diag(1.0 / np.sqrt(eigvals + delta))\n\n        # PCA whitening\n        X_pca = X_c @ U @ Lambda_delta_inv_sqrt\n        # ZCA whitening\n        X_zca = X_pca @ U.T\n\n        def train_and_get_margins(X_w, y_labels, reg_lambda):\n            \"\"\"\n            Trains a ridge regression classifier and computes geometric margins.\n            \"\"\"\n            n_w, d_w = X_w.shape\n            \n            # Form the design matrix Z = [X_w, 1]\n            Z = np.hstack([X_w, np.ones((n_w, 1))])\n            \n            # Form the regularization matrix R\n            R_diag = np.full(d_w + 1, reg_lambda)\n            R_diag[-1] = 0.0 # No penalty on the intercept\n            R = np.diag(R_diag)\n            \n            # Solve the normal equations: (Z^T Z + R) theta = Z^T y\n            A = Z.T @ Z + R\n            b_vec = Z.T @ y_labels\n            theta_star = np.linalg.solve(A, b_vec)\n            \n            w_star = theta_star[:-1]\n            b_star = theta_star[-1]\n            \n            w_norm = np.linalg.norm(w_star)\n\n            # Avoid division by zero if w_star is all zeros\n            if w_norm == 0:\n                margins = np.zeros(n_w)\n            else:\n                # Calculate geometric margins\n                margins = (y_labels * (X_w @ w_star + b_star)) / w_norm\n            \n            m_bar = np.mean(margins)\n            m_min = np.min(margins)\n            \n            return m_bar, m_min\n\n        # Step 4: Train on each whitened set and get margins\n        m_bar_pca, m_min_pca = train_and_get_margins(X_pca, y, lambda_val)\n        m_bar_zca, m_min_zca = train_and_get_margins(X_zca, y, lambda_val)\n\n        # Step 5: Compute differences\n        delta_m_bar = m_bar_zca - m_bar_pca\n        delta_m_min = m_min_zca - m_min_pca\n        \n        return [delta_m_bar, delta_m_min]\n\n    test_cases_params = [\n        # Test case 1\n        {\n            \"type\": \"gaussian_mixture\", \"d\": 2, \"n_pos\": 80, \"n_neg\": 80,\n            \"mu_pos\": [2, 0], \"mu_neg\": [-2, 0], \"sigma_base_diag\": [3, 0.2],\n            \"theta_deg\": 30, \"delta\": 1e-5, \"lambda\": 1e-1\n        },\n        # Test case 2\n        {\n            \"type\": \"gaussian_mixture\", \"d\": 2, \"n_pos\": 100, \"n_neg\": 100,\n            \"mu_pos\": [0.5, -0.5], \"mu_neg\": [-0.5, 0.5], \"sigma_base_diag\": [10, 1e-6],\n            \"theta_deg\": 75, \"delta\": 1e-3, \"lambda\": 1e-1\n        },\n        # Test case 3\n        {\n            \"type\": \"std_normal\", \"d\": 2, \"n\": 400, \"delta\": 1e-5, \"lambda\": 1e-1\n        }\n    ]\n\n    results = [run_case(params) for params in test_cases_params]\n\n    # Format output as specified: [[a,b],[c,d],[e,f]]\n    inner_strings = [f\"[{res[0]},{res[1]}]\" for res in results]\n    final_output = f\"[{','.join(inner_strings)}]\"\n    \n    print(final_output)\n\nsolve()\n```", "id": "3192878"}]}