{"hands_on_practices": [{"introduction": "在理想的统计世界中，我们假设可以完美地测量所有变量。然而，在现实世界中，数据往往带有测量误差。本练习将探讨当预测变量存在测量误差时，它如何影响我们对变量间关系的估计。通过运用协方差和方差的基本性质，我们将推导出一种被称为“衰减偏误”的现象，并从数学上揭示测量误差为何会系统性地削弱我们观察到的效应大小。[@problem_id:3119226]", "problem": "考虑一个实值统计学习场景，其中单个预测变量存在经典测量误差。令未观测到的真实值为 $X^{\\text{true}}$，观测到的预测变量为 $X^{\\text{obs}} = X^{\\text{true}} + U$，其中 $U$ 是测量误差。响应变量由线性模型 $Y = \\alpha + \\beta X^{\\text{true}} + \\varepsilon$ 生成。假设 $E[U] = 0$，$E[\\varepsilon] = 0$，且 $U$ 与 $X^{\\text{true}}$ 和 $\\varepsilon$ 均独立，所有变量都具有有限二阶矩。定义 $\\sigma_{X}^{2} = \\operatorname{Var}(X^{\\text{true}})$，$\\sigma_{U}^{2} = \\operatorname{Var}(U)$，以及 $\\sigma_{\\varepsilon}^{2} = \\operatorname{Var}(\\varepsilon)$。\n\n假设一位分析者使用普通最小二乘法 (OLS) 对 $Y$ 和 $X^{\\text{obs}}$ 进行回归，并忽略了测量误差的存在。令 $\\hat{\\beta}_{\\text{naive}}$ 表示通过对 $Y$ 和 $X^{\\text{obs}}$ 进行回归得到的总体 OLS 斜率。仅使用期望、方差、协方差和相关性的核心定义以及给定的独立性假设，推导：\n\n1. 衰减因子 $\\lambda = \\hat{\\beta}_{\\text{naive}} / \\beta$，用 $\\sigma_{X}^{2}$ 和 $\\sigma_{U}^{2}$ 表示。\n2. 相关性中的乘性偏差比 $R = \\operatorname{Corr}(X^{\\text{obs}}, Y) / \\operatorname{Corr}(X^{\\text{true}}, Y)$，用 $\\sigma_{X}^{2}$ 和 $\\sigma_{U}^{2}$ 表示并尽可能简化。\n\n将你的最终答案表示为一个包含两个封闭形式表达式 $(\\lambda, R)$ 的单行矩阵。无需四舍五入。", "solution": "该问题陈述是统计学习和计量经济学中一个良定且经典的问题，具体涉及线性回归模型中预测变量存在测量误差所带来的后果。所有必要的组成部分和假设都已通过标准模型定义显式或隐式地给出。模型 $Y = \\alpha + \\beta X^{\\text{true}} + \\varepsilon$ 是一个标准线性模型，其基本假设是误差项 $\\varepsilon$ 与预测变量 $X^{\\text{true}}$ 不相关。因此，我们将在标准假设 $\\operatorname{Cov}(X^{\\text{true}}, \\varepsilon) = 0$ 下进行推导。所有推导都将基于给定的条件以及期望、方差、协方差和相关性的基本定义。\n\n令各量如问题陈述中定义：\n- 真实模型: $Y = \\alpha + \\beta X^{\\text{true}} + \\varepsilon$\n- 测量模型: $X^{\\text{obs}} = X^{\\text{true}} + U$\n- 方差: $\\sigma_{X}^{2} = \\operatorname{Var}(X^{\\text{true}})$，$\\sigma_{U}^{2} = \\operatorname{Var}(U)$，$\\sigma_{\\varepsilon}^{2} = \\operatorname{Var}(\\varepsilon)$\n- 假设: $E[U] = 0$，$E[\\varepsilon] = 0$。$U$ 与 $X^{\\text{true}}$ 和 $\\varepsilon$ 独立。推而广之，我们也假设 $\\varepsilon$ 与 $X^{\\text{true}}$ 独立。独立性意味着协方差为零。因此，我们有 $\\operatorname{Cov}(X^{\\text{true}}, U) = 0$，$\\operatorname{Cov}(U, \\varepsilon) = 0$ 以及 $\\operatorname{Cov}(X^{\\text{true}}, \\varepsilon) = 0$。\n\n首先，我们推导衰减因子 $\\lambda = \\hat{\\beta}_{\\text{naive}} / \\beta$。\n响应变量 $Y$ 对预测变量 $Z$ 回归的总体普通最小二乘 (OLS) 斜率系数由公式 $\\operatorname{Cov}(Z, Y) / \\operatorname{Var}(Z)$ 给出。在这个问题中，分析者对 $Y$ 和观测到的预测变量 $X^{\\text{obs}}$ 进行回归，因此朴素 OLS 斜率为：\n$$\n\\hat{\\beta}_{\\text{naive}} = \\frac{\\operatorname{Cov}(X^{\\text{obs}}, Y)}{\\operatorname{Var}(X^{\\text{obs}})}\n$$\n我们必须计算分子和分母。\n\n分母是观测到的预测变量的方差，即 $\\operatorname{Var}(X^{\\text{obs}})$。\n使用定义 $X^{\\text{obs}} = X^{\\text{true}} + U$：\n$$\n\\operatorname{Var}(X^{\\text{obs}}) = \\operatorname{Var}(X^{\\text{true}} + U)\n$$\n使用属性 $\\operatorname{Var}(A+B) = \\operatorname{Var}(A) + \\operatorname{Var}(B) + 2\\operatorname{Cov}(A,B)$ 以及 $X^{\\text{true}}$ 和 $U$ 独立的假设（因此 $\\operatorname{Cov}(X^{\\text{true}}, U) = 0$）：\n$$\n\\operatorname{Var}(X^{\\text{obs}}) = \\operatorname{Var}(X^{\\text{true}}) + \\operatorname{Var}(U) = \\sigma_{X}^{2} + \\sigma_{U}^{2}\n$$\n\n分子是观测到的预测变量与响应变量之间的协方差，即 $\\operatorname{Cov}(X^{\\text{obs}}, Y)$。\n代入 $X^{\\text{obs}}$ 和 $Y$ 的表达式：\n$$\n\\operatorname{Cov}(X^{\\text{obs}}, Y) = \\operatorname{Cov}(X^{\\text{true}} + U, \\alpha + \\beta X^{\\text{true}} + \\varepsilon)\n$$\n利用协方差的双线性性质，并注意到与常数 ($\\alpha$) 的协方差为零：\n$$\n\\operatorname{Cov}(X^{\\text{obs}}, Y) = \\operatorname{Cov}(X^{\\text{true}} + U, \\beta X^{\\text{true}} + \\varepsilon) = \\operatorname{Cov}(X^{\\text{true}}, \\beta X^{\\text{true}}) + \\operatorname{Cov}(X^{\\text{true}}, \\varepsilon) + \\operatorname{Cov}(U, \\beta X^{\\text{true}}) + \\operatorname{Cov(U, \\varepsilon)}\n$$\n我们计算每一项：\n- $\\operatorname{Cov}(X^{\\text{true}}, \\beta X^{\\text{true}}) = \\beta \\operatorname{Cov}(X^{\\text{true}}, X^{\\text{true}}) = \\beta \\operatorname{Var}(X^{\\text{true}}) = \\beta \\sigma_{X}^{2}$\n- $\\operatorname{Cov}(X^{\\text{true}}, \\varepsilon) = 0$ (标准模型假设)\n- $\\operatorname{Cov}(U, \\beta X^{\\text{true}}) = \\beta \\operatorname{Cov}(U, X^{\\text{true}}) = 0$ (因为 $U$ 和 $X^{\\text{true}}$ 独立)\n- $\\operatorname{Cov}(U, \\varepsilon) = 0$ (因为 $U$ 和 $\\varepsilon$ 独立)\n\n将这些项相加得到分子：\n$$\n\\operatorname{Cov}(X^{\\text{obs}}, Y) = \\beta \\sigma_{X}^{2}\n$$\n\n现在，我们可以组合出 $\\hat{\\beta}_{\\text{naive}}$ 的表达式：\n$$\n\\hat{\\beta}_{\\text{naive}} = \\frac{\\beta \\sigma_{X}^{2}}{\\sigma_{X}^{2} + \\sigma_{U}^{2}}\n$$\n衰减因子 $\\lambda$ 是这个朴素斜率与真实斜率 $\\beta$ 的比值：\n$$\n\\lambda = \\frac{\\hat{\\beta}_{\\text{naive}}}{\\beta} = \\frac{\\frac{\\beta \\sigma_{X}^{2}}{\\sigma_{X}^{2} + \\sigma_{U}^{2}}}{\\beta} = \\frac{\\sigma_{X}^{2}}{\\sigma_{X}^{2} + \\sigma_{U}^{2}}\n$$\n\n其次，我们推导相关性中的乘性偏差比 $R = \\operatorname{Corr}(X^{\\text{obs}}, Y) / \\operatorname{Corr}(X^{\\text{true}}, Y)$。\n相关系数的定义是 $\\operatorname{Corr}(A, B) = \\frac{\\operatorname{Cov}(A, B)}{\\sqrt{\\operatorname{Var}(A)\\operatorname{Var}(B)}}$。\n\n我们首先计算 $\\operatorname{Corr}(X^{\\text{obs}}, Y)$。我们已经求得 $\\operatorname{Cov}(X^{\\text{obs}}, Y) = \\beta \\sigma_{X}^{2}$ 和 $\\operatorname{Var}(X^{\\text{obs}}) = \\sigma_{X}^{2} + \\sigma_{U}^{2}$。我们需要求出 $\\operatorname{Var}(Y)$：\n$$\n\\operatorname{Var}(Y) = \\operatorname{Var}(\\alpha + \\beta X^{\\text{true}} + \\varepsilon) = \\operatorname{Var}(\\beta X^{\\text{true}} + \\varepsilon)\n$$\n$$\n\\operatorname{Var}(Y) = \\operatorname{Var}(\\beta X^{\\text{true}}) + \\operatorname{Var}(\\varepsilon) + 2\\operatorname{Cov}(\\beta X^{\\text{true}}, \\varepsilon) = \\beta^2 \\operatorname{Var}(X^{\\text{true}}) + \\sigma_{\\varepsilon}^{2} + 2\\beta \\operatorname{Cov}(X^{\\text{true}}, \\varepsilon)\n$$\n使用假设 $\\operatorname{Cov}(X^{\\text{true}}, \\varepsilon) = 0$：\n$$\n\\operatorname{Var}(Y) = \\beta^2 \\sigma_{X}^{2} + \\sigma_{\\varepsilon}^{2}\n$$\n因此，观测到的预测变量与响应变量之间的相关性为：\n$$\n\\operatorname{Corr}(X^{\\text{obs}}, Y) = \\frac{\\operatorname{Cov}(X^{\\text{obs}}, Y)}{\\sqrt{\\operatorname{Var}(X^{\\text{obs}})\\operatorname{Var}(Y)}} = \\frac{\\beta \\sigma_{X}^{2}}{\\sqrt{(\\sigma_{X}^{2} + \\sigma_{U}^{2})(\\beta^2 \\sigma_{X}^{2} + \\sigma_{\\varepsilon}^{2})}}\n$$\n\n接下来，我们计算 $\\operatorname{Corr}(X^{\\text{true}}, Y)$。我们首先需要 $\\operatorname{Cov}(X^{\\text{true}}, Y)$：\n$$\n\\operatorname{Cov}(X^{\\text{true}}, Y) = \\operatorname{Cov}(X^{\\text{true}}, \\alpha + \\beta X^{\\text{true}} + \\varepsilon) = \\operatorname{Cov}(X^{\\text{true}}, \\beta X^{\\text{true}}) + \\operatorname{Cov}(X^{\\text{true}}, \\varepsilon) = \\beta \\sigma_{X}^{2}\n$$\n方差为 $\\operatorname{Var}(X^{\\text{true}}) = \\sigma_{X}^{2}$ 和 $\\operatorname{Var}(Y) = \\beta^2 \\sigma_{X}^{2} + \\sigma_{\\varepsilon}^{2}$。\n所以，真实相关性为：\n$$\n\\operatorname{Corr}(X^{\\text{true}}, Y) = \\frac{\\operatorname{Cov}(X^{\\text{true}}, Y)}{\\sqrt{\\operatorname{Var}(X^{\\text{true}})\\operatorname{Var(Y)}}} = \\frac{\\beta \\sigma_{X}^{2}}{\\sqrt{\\sigma_{X}^{2}(\\beta^2 \\sigma_{X}^{2} + \\sigma_{\\varepsilon}^{2})}}\n$$\n\n最后，我们计算比率 $R$：\n$$\nR = \\frac{\\operatorname{Corr}(X^{\\text{obs}}, Y)}{\\operatorname{Corr}(X^{\\text{true}}, Y)} = \\frac{\\frac{\\beta \\sigma_{X}^{2}}{\\sqrt{(\\sigma_{X}^{2} + \\sigma_{U}^{2})(\\beta^2 \\sigma_{X}^{2} + \\sigma_{\\varepsilon}^{2})}}}{\\frac{\\beta \\sigma_{X}^{2}}{\\sqrt{\\sigma_{X}^{2}(\\beta^2 \\sigma_{X}^{2} + \\sigma_{\\varepsilon}^{2})}}}\n$$\n项 $\\beta \\sigma_{X}^{2}$ 和 $\\sqrt{\\beta^2 \\sigma_{X}^{2} + \\sigma_{\\varepsilon}^{2}}$ 从分子和分母中约去，剩下：\n$$\nR = \\frac{1/\\sqrt{\\sigma_{X}^{2} + \\sigma_{U}^{2}}}{1/\\sqrt{\\sigma_{X}^{2}}} = \\frac{\\sqrt{\\sigma_{X}^{2}}}{\\sqrt{\\sigma_{X}^{2} + \\sigma_{U}^{2}}} = \\sqrt{\\frac{\\sigma_{X}^{2}}{\\sigma_{X}^{2} + \\sigma_{U}^{2}}}\n$$\n这就是相关性中偏差比的简化表达式。注意到 $R = \\sqrt{\\lambda}$。\n两个所求的表达式是 $\\lambda = \\frac{\\sigma_{X}^{2}}{\\sigma_{X}^{2} + \\sigma_{U}^{2}}$ 和 $R = \\sqrt{\\frac{\\sigma_{X}^{2}}{\\sigma_{X}^{2} + \\sigma_{U}^{2}}}$。", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{\\sigma_{X}^{2}}{\\sigma_{X}^{2} + \\sigma_{U}^{2}} & \\sqrt{\\frac{\\sigma_{X}^{2}}{\\sigma_{X}^{2} + \\sigma_{U}^{2}}}\n\\end{pmatrix}\n}\n$$", "id": "3119226"}, {"introduction": "在理解了抽象的偏误推导之后，让我们通过一个具体的计算实例来加深理解。辛普森悖论是一个惊人的统计现象，它表明一个未被观察到的混淆变量可以完全逆转我们所观察到的相关性方向。在这个动手编程练习中，你将亲手生成一个展示辛普森悖论的数据集，并利用模型选择准则来验证，一旦我们正确地将混淆变量纳入模型，这个悖论便会迎刃而解。[@problem_id:3119190]", "problem": "您必须编写一个完整的程序，在两个实值随机变量的子组特定关联背景下，构建展示辛普森悖论 (Simpson's paradox) 的合成数据集，然后在省略与包含混淆组指示符的情况下评估模型选择。所有计算都必须从期望、方差、协方差和相关性的基本定义，以及作为残差平方和最小化器的普通最小二乘估计出发。\n\n定义基础：\n- 对于具有有限二阶矩的实值随机变量 $X$ 和 $Y$，期望为 $E[X]$，方差为 $\\operatorname{Var}(X) = E\\left[(X - E[X])^2\\right]$，协方差为 $\\operatorname{Cov}(X,Y) = E\\left[(X - E[X])(Y - E[Y])\\right]$，皮尔逊相关性 (Pearson correlation) 为 $\\operatorname{Corr}(X,Y) = \\dfrac{\\operatorname{Cov}(X,Y)}{\\sqrt{\\operatorname{Var}(X)\\operatorname{Var}(Y)}}$。\n- 对于数据集 $\\{(x_i,y_i)\\}_{i=1}^n$，通过将期望替换为样本均值并使用相应的中心化和来获得样本模拟量。使用除数为 $n-1$ 的无偏样本协方差和方差；相关性是样本协方差与样本标准差乘积的比值。\n- 普通最小二乘 (Ordinary Least Squares, OLS) 拟合就回归参数最小化 $\\sum_{i=1}^n (y_i - \\hat{y}_i)^2$。残差平方和为 $\\mathrm{RSS} = \\sum_{i=1}^n (y_i - \\hat{y}_i)^2$。对于模型选择，使用赤池信息准则 (Akaike Information Criterion, AIC)，在高斯误差下，其定义（不计加法常数）为 $\\mathrm{AIC}^\\star = n \\log(\\mathrm{RSS}/n) + 2k$，其中 $k$ 是估计参数的数量，$\\mathrm{AIC}^\\star$ 较小的模型更优。\n\n数据生成过程：\n- 存在 2 个由 $g \\in \\{0,1\\}$ 索引的子组，样本量分别为 $n_0$ 和 $n_1$。对于每个组 $g$，生成 $X \\mid G=g \\sim \\mathcal{N}(\\mu_{x,g}, \\sigma_x^2)$，并根据线性模型 $Y = \\alpha_g + \\beta X + \\varepsilon$（其中 $\\varepsilon \\sim \\mathcal{N}(0,\\sigma_\\varepsilon^2)$ 且与 $X$ 无关）生成 $Y \\mid X,G=g$。\n- 这种构造产生了 $X$ 和 $Y$ 之间的组内线性关联，其具有共同的斜率 $\\beta$ 但可能具有不同的截距 $\\alpha_0$ 和 $\\alpha_1$，从而在 $(X,Y)$ 平面中造成了组间的混淆偏移。通过适当选择 $(\\mu_{x,0},\\mu_{x,1},\\alpha_0,\\alpha_1,\\beta,\\sigma_x,\\sigma_\\varepsilon)$，$X$ 和 $Y$ 之间的边际关联（在合并组之后）可以与组内关联的符号相反，从而展示了辛普森悖论。\n\n每个数据集需要执行的计算：\n1. 对于每个组 $g \\in \\{0,1\\}$，计算无偏样本协方差 $\\widehat{\\operatorname{Cov}}_g(X,Y)$，并使用 $\\operatorname{sign}(z)$ 函数确定其符号。该函数定义为：如果 $z > \\tau$，则为 $+1$；如果 $z  -\\tau$，则为 $-1$；否则为 $0$，容差 $\\tau = 10^{-10}$。\n2. 在合并数据上，计算无偏样本皮尔逊相关性 $\\widehat{r} = \\widehat{\\operatorname{Corr}}(X,Y)$。\n3. 定义一个反转指示符 $R$，如果两个子组的协方差具有相同的非零符号，且 $\\widehat{r}$ 的符号与之相反，则 $R$ 等于 $1$，否则等于 $0$。\n4. 拟合两个 OLS 模型：\n   - 模型 $\\mathcal{M}_0$：$Y$ 仅对截距和 $X$ 进行回归。\n   - 模型 $\\mathcal{M}_1$：$Y$ 对截距、$X$ 和二元组指示符 $G$ 进行回归。\n   对于每个模型，在完整样本上计算 $\\mathrm{RSS}$，然后计算 $\\mathrm{AIC}^\\star = n \\log(\\mathrm{RSS}/n) + 2k$，其中对于 $\\mathcal{M}_0$，$k=2$；对于 $\\mathcal{M}_1$，$k=3$；且 $n = n_0 + n_1$。选择 $\\mathrm{AIC}^\\star$ 较小的模型，并报告其索引（$\\mathcal{M}_0$ 为 $0$，$\\mathcal{M}_1$ 为 $1$）。\n5. 为每个数据集报告三元组 $[\\widehat{r}_{\\text{rounded}}, R, M]$，其中 $\\widehat{r}_{\\text{rounded}}$ 是 $\\widehat{r}$ 四舍五入到 4 位小数的结果，$R \\in \\{0,1\\}$ 是反转指示符，$M \\in \\{0,1\\}$ 是所选模型的索引。\n\n测试套件：\n使用以下 5 组参数集。每个案例都是一个元组 $(\\text{seed}, n_0, n_1, \\mu_{x,0}, \\mu_{x,1}, \\beta, \\alpha_0, \\alpha_1, \\sigma_x, \\sigma_\\varepsilon)$。\n\n- 案例 A（经典的辛普森反转，组内正相关，边际负相关）：$(12345, 200, 200, 4.0, 1.0, 1.0, -3.0, 3.0, 0.6, 0.6)$。\n- 案例 B（组内负相关，边际正相关）：$(54321, 200, 200, 4.0, 1.0, -1.0, 3.0, -3.0, 0.6, 0.6)$。\n- 案例 C（边界情况，通过设计使得边际关联接近于零，使用大 $n$ 以保证稳定性）：$(102938, 4000, 4000, 1.5, -1.5, 1.0, -1.666666667, 1.666666667, 0.5, 0.5)$。\n- 案例 D（小样本边缘情况，强混淆，结果可能存在噪声）：$(777, 5, 5, 4.0, 1.0, 1.0, -3.0, 3.0, 0.6, 0.6)$。\n- 案例 E（极小的组内方差以强调组间反转）：$(24680, 100, 100, 5.0, 1.0, 1.0, -4.0, 4.0, 0.05, 0.05)$。\n\n最终输出格式：\n您的程序应生成单行输出，其中包含按 A 到 E 顺序排列的 5 个结果三元组的列表，列表用方括号括起，元素间用逗号分隔，无空格。例如，输出应如下所示：\n$[[r_A,R_A,M_A],[r_B,R_B,M_B],[r_C,R_C,M_C],[r_D,R_D,M_D],[r_E,R_E,M_E]]$\n其中每个 $r_\\cdot$ 是一个四舍五入到 4 位小数的浮点数，每个 $R_\\cdot$ 和 $M_\\cdot$ 是 $\\{0,1\\}$ 中的整数。", "solution": "用户想要解决一个统计问题，该问题涉及模拟辛普森悖论和执行模型选择。\n\n### 步骤 1：提取已知信息\n- **主题**：统计学习，特别是期望、方差、协方差、相关性等概念及其在回归和模型选择中的应用。\n- **任务**：生成展示辛普森悖论的合成数据集，并使用赤池信息准则 (AIC) 比较两个回归模型。\n- **核心定义**：\n    - 期望 $E[X]$，方差 $\\operatorname{Var}(X) = E\\left[(X - E[X])^2\\right]$，协方差 $\\operatorname{Cov}(X,Y) = E\\left[(X - E[X])(Y - E[Y])\\right]$，以及皮尔逊相关性 $\\operatorname{Corr}(X,Y) = \\frac{\\operatorname{Cov}(X,Y)}{\\sqrt{\\operatorname{Var}(X)\\operatorname{Var}(Y)}}$。\n    - 样本统计量将使用除数为 $n-1$ 的无偏估计量。\n    - 普通最小二乘 (OLS) 最小化残差平方和 (RSS)，$\\mathrm{RSS} = \\sum_{i=1}^n (y_i - \\hat{y}_i)^2$。\n    - 模型选择标准是 AIC 的一个变体，$\\mathrm{AIC}^\\star = n \\log(\\mathrm{RSS}/n) + 2k$，其中 $n$ 是样本量，$k$ 是参数数量。\n- **数据生成过程**：\n    - 两个子组 $g \\in \\{0,1\\}$，样本量为 $n_0, n_1$。\n    - 预测变量 $X$ 以组为条件生成：$X \\mid G=g \\sim \\mathcal{N}(\\mu_{x,g}, \\sigma_x^2)$。\n    - 响应变量 $Y$ 由一个具有依赖于组的截距的线性模型生成：$Y = \\alpha_g + \\beta X + \\varepsilon$，其中 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma_\\varepsilon^2)$。\n- **要求的计算**：\n    1.  对每个子组 $g \\in \\{0,1\\}$，计算无偏样本协方差 $\\widehat{\\operatorname{Cov}}_g(X,Y)$ 的符号，使用容差 $\\tau = 10^{-10}$。\n    2.  在完整的合并数据集上计算样本皮尔逊相关性 $\\widehat{r}$。\n    3.  计算反转指示符 $R$，如果子组协方差共享相同的非零符号且合并相关性符号相反，则为 $1$，否则为 $0$。\n    4.  对合并数据拟合两个 OLS 模型：$\\mathcal{M}_0: Y \\sim 1+X$（忽略组）和 $\\mathcal{M}_1: Y \\sim 1+X+G$（包括组指示符 $G$）。选择 $\\mathrm{AIC}^\\star$ 较低的模型，并报告其索引 $M \\in \\{0,1\\}$。\n    5.  对于每个参数集，报告三元组 $[\\widehat{r}_{\\text{rounded}}, R, M]$。\n- **测试案例**：为模拟提供了五个参数集 $(\\text{seed}, n_0, n_1, \\mu_{x,0}, \\mu_{x,1}, \\beta, \\alpha_0, \\alpha_1, \\sigma_x, \\sigma_\\varepsilon)$。\n- **输出格式**：包含五个案例结果的单行 `[[r_A,R_A,M_A],[r_B,R_B,M_B],...]`。\n\n### 步骤 2：使用提取的已知信息进行验证\n- **科学/事实合理性**：该问题在科学上是合理的。它是一个标准的统计模拟练习，基于线性回归、混淆和模型选择（辛普森悖论、OLS、AIC）的公认原则。数据生成过程是用于创建混淆数据的有效统计模型。\n- **适定性**：该问题是适定的。数据生成过程被完全指定，包括用于可复现性的随机种子。计算任务定义明确。每个测试案例都可以获得唯一的数值结果。\n- **客观性**：该问题使用客观、形式化的数学和统计语言陈述。所有定义和程序都是精确的，没有主观性。\n- **完整性和一致性**：提供了所有必要的信息（参数、公式、程序）。问题陈述中没有矛盾之处。无偏估计量的使用和指定的 AIC 公式与标准统计实践一致。\n- **其他缺陷**：在模拟背景下，该问题并非微不足道、隐喻性或不切实际。它直接针对指定的主题，是理解统计混淆的一个有价值的练习。\n\n### 步骤 3：结论和行动\n该问题是 **有效的**。将提供一个完整的解决方案。\n\n### 解法\n\n该问题要求我们模拟展示辛普森悖论的数据集。辛普森悖论是一种统计现象，即在数据的子组中观察到的趋势在数据汇总时发生逆转。然后，我们将使用一个标准的模型选择准则来确定，考虑子组结构是否能产生更好的统计模型。\n\n**1. 概念框架：辛普森悖论与混淆**\n\n辛普森悖论的出现是由于存在混淆变量。在这个问题中，组指示符 $G$ 就是混淆变量。它是一个与预测变量 $X$ 和结果变量 $Y$ 都相关的变量。数据生成方式如下：\n- 在每个组 $g$ *内部*，$X$ 和 $Y$ 之间的关系是线性的，具有共同的斜率 $\\beta$：$Y = \\alpha_g + \\beta X + \\varepsilon$。$\\beta$ 的符号决定了真实的、潜在的关联方向。\n- 各组在 $X$ 的分布（通过均值 $\\mu_{x,g}$）和 $Y$ 的基线水平（通过截距 $\\alpha_g$）上有所不同。\n通过仔细选择参数 $(\\mu_{x,0}, \\mu_{x,1})$ 和 $(\\alpha_0, \\alpha_1)$，我们可以创造一种情况，即连接两个组级数据云中心的趋势与每个云内部的趋势方向相反。当数据被合并且组结构被忽略时，这种虚假的组间趋势可能会压倒真实的组内趋势，导致汇总（或边际）相关性的符号与 $\\beta$ 相反。\n\n**2. 数据生成与分析步骤**\n\n对于每个测试案例，我们执行以下操作序列：\n\n**a. 数据模拟：**\n使用提供的参数 $(\\text{seed}, n_0, n_1, \\mu_{x,0}, \\mu_{x,1}, \\beta, \\alpha_0, \\alpha_1, \\sigma_x, \\sigma_\\varepsilon)$，我们生成对应于组 $g=0$ 和 $g=1$ 的两个数据集。\n- 对于组 $g=0$，我们抽取 $n_0$ 个 $X_0 \\sim \\mathcal{N}(\\mu_{x,0}, \\sigma_x^2)$ 的样本。然后，我们计算相应的 $Y_0$ 值，即 $Y_0 = \\alpha_0 + \\beta X_0 + \\varepsilon_0$，其中 $\\varepsilon_0 \\sim \\mathcal{N}(0, \\sigma_\\varepsilon^2)$。\n- 类似地，对于组 $g=1$，我们抽取 $n_1$ 个 $X_1 \\sim \\mathcal{N}(\\mu_{x,1}, \\sigma_x^2)$ 的样本，并计算 $Y_1 = \\alpha_1 + \\beta X_1 + \\varepsilon_1$，其中 $\\varepsilon_1 \\sim \\mathcal{N}(0, \\sigma_\\varepsilon^2)$。\n\n**b. 子组与合并关联分析：**\n- **组内协方差：** 对于每个组 $g \\in \\{0,1\\}$，我们计算无偏样本协方差，它衡量该组内的线形关联方向和强度：\n  $$ \\widehat{\\operatorname{Cov}}_g(X,Y) = \\frac{1}{n_g-1} \\sum_{i=1}^{n_g} (x_{i,g} - \\bar{x}_g)(y_{i,g} - \\bar{y}_g) $$\n  然后我们确定它的符号，$\\operatorname{sign}(\\widehat{\\operatorname{Cov}}_g(X,Y))$，使用指定的数值容差 $\\tau=10^{-10}$。在一个成功的模拟中，两个符号都应与真实斜率参数 $\\beta$ 的符号相匹配。\n\n- **合并相关性：** 我们将两个数据集合并成一个大小为 $n=n_0+n_1$ 的单一合并样本。然后我们计算皮尔逊样本相关系数 $\\widehat{r}$：\n  $$ \\widehat{r} = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum_{i=1}^n (x_i - \\bar{x})^2 \\sum_{i=1}^n (y_i - \\bar{y})^2}} $$\n  这衡量了忽略组结构的边际线性关联。\n\n- **反转指示符 ($R$)：** 我们正式地检查悖论。如果观察到悖论，即组内关联一致且非零，而合并关联方向相反，则指示符 $R$ 设置为 $1$。形式上：如果 $\\operatorname{sign}(\\widehat{\\operatorname{Cov}}_0) = \\operatorname{sign}(\\widehat{\\operatorname{Cov}}_1) \\neq 0$ 且 $\\operatorname{sign}(\\widehat{r}) = -\\operatorname{sign}(\\widehat{\\operatorname{Cov}}_0)$，则 $R=1$。否则 $R=0$。\n\n**c. 通过 OLS 和 AIC 进行模型比较：**\n我们使用普通最小二乘法 (OLS) 对合并数据拟合两个线性模型。OLS 找到使残差平方和 (RSS) 最小化的参数估计。\n\n- **模型 $\\mathcal{M}_0$（简单模型）：** $Y_i = b_0 + b_1 X_i + e_i$。该模型将 $Y$ 对 $X$ 和一个截距进行回归，忽略了混淆组变量 $G$。估计的参数数量为 $k_0 = 2$。\n- **模型 $\\mathcal{M}_1$（混淆变量模型）：** $Y_i = b'_0 + b'_1 X_i + b'_2 G_i + e'_i$。该模型将二元组指示符 $G$（例如，组 $0$ 为 $G_i=0$，组 $1$ 为 $G_i=1$）作为预测变量。这等同于拟合一个对每个组具有共同斜率但不同截距的模型，这与我们的数据生成过程相匹配。参数数量为 $k_1 = 3$。\n\n对于每个模型 $\\mathcal{M}_j$，我们计算其 $\\mathrm{RSS}_j$，然后计算其赤池信息准则分数：\n$$ \\mathrm{AIC}^\\star_j = n \\log(\\mathrm{RSS}_j/n) + 2k_j $$\n$\\mathrm{AIC}^\\star$ 平衡了模型拟合度（较低的 $\\mathrm{RSS}$ 更好）与模型复杂性（较低的 $k$ 更好）。$\\mathrm{AIC}^\\star$ 值较小的模型更优。我们预计，当辛普森悖论存在时，$\\mathcal{M}_1$ 将提供显著更好的拟合（$\\mathrm{RSS}_1$ 低得多），这会超过其因增加一个参数而受到的惩罚，因此将被选中。我们报告所选模型的索引 $M$。\n\n最后，对于每个测试案例，我们组装所需的三元组：$[\\widehat{r}_{\\text{rounded}}, R, M]$。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Constructs synthetic datasets exhibiting Simpson’s paradox, evaluates model selection\n    with and without a confounding variable, and reports the results.\n    \"\"\"\n    test_cases = [\n        # (seed, n0, n1, mu_x0, mu_x1, beta, alpha0, alpha1, sigma_x, sigma_e)\n        (12345, 200, 200, 4.0, 1.0, 1.0, -3.0, 3.0, 0.6, 0.6),\n        (54321, 200, 200, 4.0, 1.0, -1.0, 3.0, -3.0, 0.6, 0.6),\n        (102938, 4000, 4000, 1.5, -1.5, 1.0, -1.666666667, 1.666666667, 0.5, 0.5),\n        (777, 5, 5, 4.0, 1.0, 1.0, -3.0, 3.0, 0.6, 0.6),\n        (24680, 100, 100, 5.0, 1.0, 1.0, -4.0, 4.0, 0.05, 0.05),\n    ]\n\n    results = []\n\n    def sign_func(z, tau=1e-10):\n        if z > tau:\n            return 1\n        elif z  -tau:\n            return -1\n        else:\n            return 0\n\n    for case in test_cases:\n        seed, n0, n1, mu_x0, mu_x1, beta, alpha0, alpha1, sigma_x, sigma_e = case\n\n        rng = np.random.default_rng(seed)\n\n        # Generate data for both groups\n        X0 = rng.normal(loc=mu_x0, scale=sigma_x, size=n0)\n        Y0 = alpha0 + beta * X0 + rng.normal(loc=0, scale=sigma_e, size=n0)\n\n        X1 = rng.normal(loc=mu_x1, scale=sigma_x, size=n1)\n        Y1 = alpha1 + beta * X1 + rng.normal(loc=0, scale=sigma_e, size=n1)\n\n        # 1. Compute subgroup unbiased sample covariance and its sign\n        cov0 = np.cov(X0, Y0, ddof=1)[0, 1]\n        cov1 = np.cov(X1, Y1, ddof=1)[0, 1]\n        sign_cov0 = sign_func(cov0)\n        sign_cov1 = sign_func(cov1)\n\n        # Pool the data\n        X_pooled = np.concatenate((X0, X1))\n        Y_pooled = np.concatenate((Y0, Y1))\n        n = n0 + n1\n\n        # 2. Compute pooled sample Pearson correlation\n        r_pooled = np.corrcoef(X_pooled, Y_pooled)[0, 1]\n        sign_r_pooled = sign_func(r_pooled)\n\n        # 3. Compute reversal indicator R\n        R = 0\n        if sign_cov0 != 0 and sign_cov0 == sign_cov1 and sign_r_pooled == -sign_cov0:\n            R = 1\n\n        # 4. Fit OLS models and compute AIC\n        # Model M0: Y ~ 1 + X\n        X_mat0 = np.vstack([np.ones(n), X_pooled]).T\n        # Solve (X'X)b = X'y for b, which is more stable than inv(X'X)\n        try:\n            b0 = np.linalg.solve(X_mat0.T @ X_mat0, X_mat0.T @ Y_pooled)\n            Y_hat0 = X_mat0 @ b0\n            RSS0 = np.sum((Y_pooled - Y_hat0)**2)\n            k0 = 2\n            AIC0 = n * np.log(RSS0 / n) + 2 * k0\n        except np.linalg.LinAlgError:\n            AIC0 = np.inf\n\n\n        # Model M1: Y ~ 1 + X + G\n        G_indicator = np.concatenate([np.zeros(n0), np.ones(n1)])\n        X_mat1 = np.vstack([np.ones(n), X_pooled, G_indicator]).T\n        try:\n            b1 = np.linalg.solve(X_mat1.T @ X_mat1, X_mat1.T @ Y_pooled)\n            Y_hat1 = X_mat1 @ b1\n            RSS1 = np.sum((Y_pooled - Y_hat1)**2)\n            k1 = 3\n            AIC1 = n * np.log(RSS1 / n) + 2 * k1\n        except np.linalg.LinAlgError:\n            AIC1 = np.inf\n        \n        # Select model with smaller AIC\n        M = 1 if AIC1  AIC0 else 0\n\n        # 5. Formulate the result triple\n        r_rounded = round(r_pooled, 4)\n        results.append([r_rounded, R, M])\n\n    # Format the final output string as specified\n    output_str = '[' + ','.join([f'[{r},{res_R},{res_M}]' for r, res_R, res_M in results]) + ']'\n    print(output_str)\n\nsolve()\n```", "id": "3119190"}, {"introduction": "前面的练习揭示了违反回归基本假设所导致的偏误。现在，我们将介绍一种强大的修正方法：工具变量 (Instrumental Variables, IV)。当核心假设 $\\operatorname{Cov}(X, \\varepsilon) = 0$ 不成立时（即存在内生性），IV 方法即便在这种情况下也能提供无偏估计。这个更深入的练习将引导你推导内生性产生的条件，并计算 IV 估计量的渐近方差，让你一窥因果推断领域的精妙之处。[@problem_id:3119168]", "problem": "一位数据科学家正在研究存在遗漏变量内生性时线性回归估计量的偏差和变异性，并提议使用工具变量（IV）来纠正它。对于单个回归量，考虑观测值 $\\{(Y_{i}, X_{i}, Z_{i}, U_{i})\\}_{i=1}^{n}$ 的以下数据生成过程：\n$$\nY_{i} \\;=\\; \\beta\\, X_{i} \\;+\\; \\varepsilon_{i}, \\quad\nX_{i} \\;=\\; \\gamma\\, Z_{i} \\;+\\; \\delta\\, U_{i} \\;+\\; \\nu_{i}, \\quad\n\\varepsilon_{i} \\;=\\; \\rho\\, U_{i} \\;+\\; \\eta_{i},\n$$\n其中 $i=1,\\dots,n$，且所有变量都已中心化，使得 $E[Y_{i}]=E[X_{i}]=E[Z_{i}]=E[U_{i}]=E[\\nu_{i}]=E[\\eta_{i}]=0$。假设集合 $\\{Z_{i},U_{i},\\nu_{i},\\eta_{i}\\}$ 在样本内和样本间均联合独立，且具有有限二阶矩\n$$\n\\operatorname{Var}(Z_{i}) \\;=\\; \\sigma_{Z}^{2}, \\quad \\operatorname{Var}(U_{i}) \\;=\\; \\sigma_{U}^{2}, \\quad \\operatorname{Var}(\\nu_{i}) \\;=\\; \\sigma_{\\nu}^{2}, \\quad \\operatorname{Var}(\\eta_{i}) \\;=\\; \\sigma_{\\eta}^{2}.\n$$\n研究人员使用普通最小二乘法（OLS）和工具变量法（IV），其中IV法使用 $Z_{i}$ 作为 $X_{i}$ 的工具变量。目标是识别产生内生性 $\\operatorname{Cov}(X_{i}, \\varepsilon_{i}) \\neq 0$ 的条件，并展示IV法如何恢复无偏性和有效的变异性。\n\n仅使用期望、方差、协方差和相关性的核心定义（例如，$\\operatorname{Cov}(A,B)=E[AB]-E[A]E[B]$）以及基于大数定律和中心极限定理的标准大样本论证，完成以下任务：\n\n1.  用上述结构参数和矩推导 $\\operatorname{Cov}(X_{i}, \\varepsilon_{i})$，并精确说明在何种参数条件下 $\\operatorname{Cov}(X_{i}, \\varepsilon_{i}) \\neq 0$。\n2.  利用推导出的协方差，解释 $\\beta$ 的单回归量OLS斜率估计量是无偏还是有偏，并将其期望与 $\\operatorname{Cov}(X_{i}, \\varepsilon_{i})$ 和 $\\operatorname{Var}(X_{i})$ 联系起来。\n3.  将 $Z_{i}$ 视为工具变量，假设工具变量的相关性和外生性在总体中成立。推导 $\\beta$ 的恰好识别IV估计量的大样本（渐近）方差，并将其完全用 $n$、$\\gamma$、$\\sigma_{Z}^{2}$、$\\sigma_{U}^{2}$、$\\sigma_{\\eta}^{2}$ 和 $\\rho$ 表示。\n\n提供第3部分大样本方差的闭式表达式作为最终答案。无需四舍五入；仅报告符号表达式。", "solution": "该问题陈述被评估为有效。它提出了一个计量经济学中关于遗漏变量偏差和工具变量估计的标准、定义明确的问题。模型和假设具有科学依据、内部一致，并且足以推导出所要求的量。任务是客观且可形式化的。\n\n我们首先按顺序处理问题的三个部分。\n\n### 第1部分：协方差和内生性条件\n\n第一个任务是推导回归量 $X_i$ 和误差项 $\\varepsilon_i$ 之间的协方差，并确定该协方差不为零（从而产生内生性问题）的条件。\n\n两个随机变量 $A$ 和 $B$ 之间的协方差定义为 $\\operatorname{Cov}(A, B) = E[AB] - E[A]E[B]$。问题陈述中说明所有变量都已中心化，意味着它们的期望值为零。具体来说，$E[X_i] = 0$ 且 $E[\\varepsilon_i] = 0$。因此，协方差简化为：\n$$\n\\operatorname{Cov}(X_{i}, \\varepsilon_{i}) = E[X_i \\varepsilon_i] - E[X_i]E[\\varepsilon_i] = E[X_i \\varepsilon_i] - (0)(0) = E[X_i \\varepsilon_i]\n$$\n为了计算这个期望，我们代入 $X_i$ 和 $\\varepsilon_i$ 的结构方程：\n$$\nX_{i} = \\gamma Z_{i} + \\delta U_{i} + \\nu_{i}\n$$\n$$\n\\varepsilon_{i} = \\rho U_{i} + \\eta_{i}\n$$\n乘积 $X_i \\varepsilon_i$ 为：\n$$\nX_i \\varepsilon_i = (\\gamma Z_{i} + \\delta U_{i} + \\nu_{i})(\\rho U_{i} + \\eta_{i}) = \\gamma \\rho Z_i U_i + \\gamma Z_i \\eta_i + \\delta \\rho U_i^2 + \\delta U_i \\eta_i + \\rho \\nu_i U_i + \\nu_i \\eta_i\n$$\n根据期望的线性性质，我们有：\n$$\nE[X_i \\varepsilon_i] = \\gamma \\rho E[Z_i U_i] + \\gamma E[Z_i \\eta_i] + \\delta \\rho E[U_i^2] + \\delta E[U_i \\eta_i] + \\rho E[\\nu_i U_i] + E[\\nu_i \\eta_i]\n$$\n问题假设随机变量集合 $\\{Z_{i},U_{i},\\nu_{i},\\eta_{i}\\}$ 是联合独立的。对于此集合中任意两个不同的、零均值的独立变量 $A$ 和 $B$，$E[AB] = E[A]E[B] = 0 \\cdot 0 = 0$。这意味着：\n$$\nE[Z_i U_i] = 0, \\quad E[Z_i \\eta_i] = 0, \\quad E[U_i \\eta_i] = 0, \\quad E[\\nu_i U_i] = 0, \\quad E[\\nu_i \\eta_i] = 0\n$$\n代入这些结果，期望简化为：\n$$\nE[X_i \\varepsilon_i] = \\delta \\rho E[U_i^2]\n$$\n$U_i$ 的方差给定为 $\\operatorname{Var}(U_i) = \\sigma_U^2$。由于 $E[U_i]=0$，我们有 $\\operatorname{Var}(U_i) = E[U_i^2] - (E[U_i])^2 = E[U_i^2]$。因此，$E[U_i^2] = \\sigma_U^2$。\n所以协方差为：\n$$\n\\operatorname{Cov}(X_{i}, \\varepsilon_{i}) = \\delta \\rho \\sigma_U^2\n$$\n如果 $\\operatorname{Cov}(X_i, \\varepsilon_i) \\neq 0$，则存在内生性。假设 $\\sigma_U^2 > 0$（即 $U_i$ 不是一个常数），此条件成立当且仅当 $\\delta \\neq 0$ 和 $\\rho \\neq 0$ 同时成立。\n这些条件有明确的经济学解释：\n1.  $\\delta \\neq 0$：未观测到的混淆变量 $U_i$ 对回归量 $X_i$ 有因果效应。\n2.  $\\rho \\neq 0$：未观测到的混淆变量 $U_i$ 通过误差项 $\\varepsilon_i$ 对结果 $Y_i$ 有因果效应。\n\n当这两个条件都成立时，$U_i$ 是 $X_i$ 和 $Y_i$ 的共同原因，导致 $X_i$ 和 $\\varepsilon_i$ 之间出现伪相关。\n\n### 第2部分：OLS 估计量偏差\n\n在单回归量模型 $Y_i = \\beta X_i + \\varepsilon_i$ 中，$\\beta$ 的普通最小二乘（OLS）估计量由下式给出：\n$$\n\\hat{\\beta}_{OLS} = \\frac{\\sum_{i=1}^{n} X_i Y_i}{\\sum_{i=1}^{n} X_i^2}\n$$\n代入 $Y_i$ 的真实模型：\n$$\n\\hat{\\beta}_{OLS} = \\frac{\\sum_{i=1}^{n} X_i (\\beta X_i + \\varepsilon_i)}{\\sum_{i=1}^{n} X_i^2} = \\frac{\\beta \\sum_{i=1}^{n} X_i^2 + \\sum_{i=1}^{n} X_i \\varepsilon_i}{\\sum_{i=1}^{n} X_i^2} = \\beta + \\frac{\\sum_{i=1}^{n} X_i \\varepsilon_i}{\\sum_{i=1}^{n} X_i^2}\n$$\n为分析其性质，我们考虑其在大样本（$n \\to \\infty$）中的行为。根据大数定律，样本均值依概率收敛到其对应的总体期望。我们可以将 $\\hat{\\beta}_{OLS}$ 写成样本均值的形式：\n$$\n\\hat{\\beta}_{OLS} = \\beta + \\frac{\\frac{1}{n}\\sum_{i=1}^{n} X_i \\varepsilon_i}{\\frac{1}{n}\\sum_{i=1}^{n} X_i^2}\n$$\n当 $n \\to \\infty$ 时，我们有：\n$$\n\\frac{1}{n}\\sum_{i=1}^{n} X_i \\varepsilon_i \\xrightarrow{p} E[X_i \\varepsilon_i] = \\operatorname{Cov}(X_i, \\varepsilon_i)\n$$\n$$\n\\frac{1}{n}\\sum_{i=1}^{n} X_i^2 \\xrightarrow{p} E[X_i^2] = \\operatorname{Var}(X_i)\n$$\n根据连续映射定理，$\\hat{\\beta}_{OLS}$ 的概率极限是：\n$$\n\\operatorname{plim}_{n \\to \\infty} \\hat{\\beta}_{OLS} = \\beta + \\frac{\\operatorname{Cov}(X_i, \\varepsilon_i)}{\\operatorname{Var}(X_i)}\n$$\nOLS估计量是一致的（渐近无偏的）当且仅当 $\\operatorname{plim} \\hat{\\beta}_{OLS} = \\beta$，这要求 $\\operatorname{Cov}(X_i, \\varepsilon_i) = 0$。从第1部分我们知道，该协方差为 $\\delta \\rho \\sigma_U^2$。如果存在内生性（$\\delta \\neq 0$ 且 $\\rho \\neq 0$），则 $\\operatorname{Cov}(X_i, \\varepsilon_i) \\neq 0$，OLS估计量是不一致的且是渐近有偏的。渐近偏差由 $\\frac{\\delta \\rho \\sigma_U^2}{\\operatorname{Var}(X_i)}$ 项给出。\n\n### 第3部分：IV 估计量的渐近方差\n\n使用 $Z_i$ 作为 $X_i$ 的工具变量时，$\\beta$ 的工具变量（IV）估计量为：\n$$\n\\hat{\\beta}_{IV} = \\frac{\\sum_{i=1}^{n} Z_i Y_i}{\\sum_{i=1}^{n} Z_i X_i}\n$$\n为了找到其渐近方差，我们首先确定其一致性。代入 $Y_i = \\beta X_i + \\varepsilon_i$：\n$$\n\\hat{\\beta}_{IV} = \\beta + \\frac{\\sum_{i=1}^{n} Z_i \\varepsilon_i}{\\sum_{i=1}^{n} Z_i X_i} = \\beta + \\frac{\\frac{1}{n}\\sum_{i=1}^{n} Z_i \\varepsilon_i}{\\frac{1}{n}\\sum_{i=1}^{n} Z_i X_i}\n$$\n$\\hat{\\beta}_{IV}$ 的一致性取决于两个关键条件：\n1.  **工具变量相关性**: $\\operatorname{Cov}(Z_i, X_i) \\neq 0$。\n2.  **工具变量外生性**: $\\operatorname{Cov}(Z_i, \\varepsilon_i) = 0$。\n\n让我们使用模型结构来验证这些条件。\n$\\operatorname{Cov}(Z_i, X_i) = E[Z_i X_i] = E[Z_i(\\gamma Z_i + \\delta U_i + \\nu_i)] = \\gamma E[Z_i^2] + \\delta E[Z_i U_i] + E[Z_i \\nu_i] = \\gamma \\sigma_Z^2 + 0 + 0 = \\gamma \\sigma_Z^2$。\n相关性要求 $\\gamma \\neq 0$。\n$\\operatorname{Cov}(Z_i, \\varepsilon_i) = E[Z_i \\varepsilon_i] = E[Z_i(\\rho U_i + \\eta_i)] = \\rho E[Z_i U_i] + E[Z_i \\eta_i] = 0 + 0 = 0$。\n由于 $Z_i$ 与 $U_i$ 和 $\\eta_i$ 独立，外生性成立。\n\n取 $\\hat{\\beta}_{IV}$ 的概率极限：\n$$\n\\operatorname{plim}_{n \\to \\infty} \\hat{\\beta}_{IV} = \\beta + \\frac{\\operatorname{plim}\\frac{1}{n}\\sum Z_i \\varepsilon_i}{\\operatorname{plim}\\frac{1}{n}\\sum Z_i X_i} = \\beta + \\frac{\\operatorname{Cov}(Z_i, \\varepsilon_i)}{\\operatorname{Cov}(Z_i, X_i)} = \\beta + \\frac{0}{\\gamma \\sigma_Z^2} = \\beta\n$$\n因此，IV估计量是 $\\beta$ 的一致估计量。\n\n现在，我们推导渐近方差。考虑表达式 $\\sqrt{n}(\\hat{\\beta}_{IV} - \\beta)$：\n$$\n\\sqrt{n}(\\hat{\\beta}_{IV} - \\beta) = \\sqrt{n} \\left( \\frac{\\frac{1}{n}\\sum Z_i \\varepsilon_i}{\\frac{1}{n}\\sum Z_i X_i} \\right) = \\frac{\\frac{1}{\\sqrt{n}}\\sum Z_i \\varepsilon_i}{\\frac{1}{n}\\sum Z_i X_i}\n$$\n分母依概率收敛到一个常数：$\\frac{1}{n}\\sum Z_i X_i \\xrightarrow{p} E[Z_i X_i] = \\gamma \\sigma_Z^2$。\n对于分子，我们对随机变量 $W_i = Z_i \\varepsilon_i$ 应用中心极限定理。其均值为 $E[W_i] = E[Z_i \\varepsilon_i] = 0$。其方差为：\n$\\operatorname{Var}(W_i) = E[W_i^2] - (E[W_i])^2 = E[(Z_i \\varepsilon_i)^2] = E[Z_i^2 \\varepsilon_i^2]$。\n因为 $Z_i$ 独立于 $\\varepsilon_i = \\rho U_i + \\eta_i$，我们可以分离期望：\n$$\n\\operatorname{Var}(Z_i \\varepsilon_i) = E[Z_i^2] E[\\varepsilon_i^2]\n$$\n我们有 $E[Z_i^2] = \\operatorname{Var}(Z_i) = \\sigma_Z^2$。对于 $\\varepsilon_i$，我们有 $E[\\varepsilon_i^2] = \\operatorname{Var}(\\varepsilon_i)$，因为 $E[\\varepsilon_i]=0$。\n$$\n\\operatorname{Var}(\\varepsilon_i) = \\operatorname{Var}(\\rho U_i + \\eta_i)\n$$\n由于 $U_i$ 和 $\\eta_i$ 是独立的，它们和的方差等于它们方差的和：\n$$\n\\operatorname{Var}(\\rho U_i + \\eta_i) = \\operatorname{Var}(\\rho U_i) + \\operatorname{Var}(\\eta_i) = \\rho^2 \\operatorname{Var}(U_i) + \\operatorname{Var}(\\eta_i) = \\rho^2 \\sigma_U^2 + \\sigma_\\eta^2\n$$\n所以，$\\operatorname{Var}(Z_i \\varepsilon_i) = \\sigma_Z^2 (\\rho^2 \\sigma_U^2 + \\sigma_\\eta^2)$。\n根据中心极限定理，分子项的极限分布为：\n$$\n\\frac{1}{\\sqrt{n}}\\sum_i Z_i \\varepsilon_i \\xrightarrow{d} N(0, \\operatorname{Var}(Z_i \\varepsilon_i)) = N(0, \\sigma_Z^2 (\\rho^2 \\sigma_U^2 + \\sigma_\\eta^2))\n$$\n使用斯卢茨基（Slutsky）定理，$\\sqrt{n}(\\hat{\\beta}_{IV} - \\beta)$ 的极限分布是：\n$$\n\\sqrt{n}(\\hat{\\beta}_{IV} - \\beta) \\xrightarrow{d} \\frac{N(0, \\sigma_Z^2 (\\rho^2 \\sigma_U^2 + \\sigma_\\eta^2))}{\\gamma \\sigma_Z^2} = N\\left(0, \\frac{\\sigma_Z^2 (\\rho^2 \\sigma_U^2 + \\sigma_\\eta^2)}{(\\gamma \\sigma_Z^2)^2}\\right)\n$$\n这个极限分布的方差，记为 $V_{IV}$，是：\n$$\nV_{IV} = \\frac{\\sigma_Z^2 (\\rho^2 \\sigma_U^2 + \\sigma_\\eta^2)}{\\gamma^2 (\\sigma_Z^2)^2} = \\frac{\\rho^2 \\sigma_U^2 + \\sigma_\\eta^2}{\\gamma^2 \\sigma_Z^2}\n$$\n估计量 $\\hat{\\beta}_{IV}$ 的渐近方差，记为 $\\operatorname{AVar}(\\hat{\\beta}_{IV})$，是这个极限分布的方差除以 $n$：\n$$\n\\operatorname{AVar}(\\hat{\\beta}_{IV}) = \\frac{1}{n} V_{IV} = \\frac{\\rho^2 \\sigma_U^2 + \\sigma_\\eta^2}{n \\gamma^2 \\sigma_Z^2}\n$$\n这就是IV估计量的大样本方差的最终表达式。", "answer": "$$\n\\boxed{\\frac{\\rho^2 \\sigma_U^2 + \\sigma_\\eta^2}{n \\gamma^2 \\sigma_Z^2}}\n$$", "id": "3119168"}]}