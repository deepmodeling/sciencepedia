{"hands_on_practices": [{"introduction": "为了将线性无关和秩的抽象概念与统计实践联系起来，我们首先通过一个基本练习来打下坚实的基础。在这个练习中，我们将亲手构建一个具有完美多重共线性的场景，以直接观察其对模型参数估计的影响。通过计算并分析一个特意设计为秩亏缺的矩阵的零空间，我们将揭示为什么设计矩阵的秩对于获得唯一且可解释的模型参数至关重要 [@problem_id:3140085]。", "problem": "考虑一个用于统计学习的线性回归设计矩阵 $X \\in \\mathbb{R}^{4 \\times 5}$，其列向量记为 $c_{1}, c_{2}, c_{3}, c_{4}, c_{5} \\in \\mathbb{R}^{4}$，如下所示：\n$$\nc_{1} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 2 \\\\ -1 \\end{pmatrix},\\quad\nc_{2} = \\begin{pmatrix} 0 \\\\ 1 \\\\ 3 \\\\ 2 \\end{pmatrix},\\quad\nc_{4} = \\begin{pmatrix} 2 \\\\ -1 \\\\ 0 \\\\ 3 \\end{pmatrix},\n$$\n并通过精确求和定义另外两个特征：\n$$\nc_{3} = c_{1} + c_{2},\\quad c_{5} = c_{2} + c_{4}.\n$$\n令 $X = [\\,c_{1}\\; c_{2}\\; c_{3}\\; c_{4}\\; c_{5}\\,]$。\n\n仅使用适用于统计学习的线性代数基本定义（线性无关、秩、零空间和单位矩阵的定义），完成以下任务：\n\n- 验证各列之间指定的线性相关性，并确定 $c_{1}, c_{2}, c_{4}$ 是否线性无关。\n- 从基本原理出发，确定 $X$ 的秩，并计算零空间 $\\mathcal{N}(X) = \\{\\,w \\in \\mathbb{R}^{5} : Xw = 0\\,\\}$ 的一组基。\n- 在具有零均值噪声 $\\varepsilon$ 的线性模型 $y = X\\beta + \\varepsilon$ 的背景下，讨论任何非平凡零空间对参数可解释性的影响，并从概念上解释将单位矩阵 $I_{5}$ 的一个正常数倍加到一个格拉姆型矩阵上与可识别性的关系。\n\n你的最终答案必须是给出 $X$ 的零空间基的一个解析表达式（你可以将基向量表示为单个矩阵的列）。最终答案框中请勿包含解释性文本。", "solution": "该问题经评估有效，因其在数学和科学上是合理的、自洽的、适定的，并且与线性代数和统计学习的指定主题直接相关。我们可以进行完整解答。\n\n设计矩阵为 $X \\in \\mathbb{R}^{4 \\times 5}$，其列向量为 $c_1, c_2, c_3, c_4, c_5 \\in \\mathbb{R}^4$。\n列向量给出如下：\n$c_{1} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 2 \\\\ -1 \\end{pmatrix}$，$c_{2} = \\begin{pmatrix} 0 \\\\ 1 \\\\ 3 \\\\ 2 \\end{pmatrix}$，$c_{4} = \\begin{pmatrix} 2 \\\\ -1 \\\\ 0 \\\\ 3 \\end{pmatrix}$。\n其余列由线性组合定义：\n$c_{3} = c_{1} + c_{2}$\n$c_{5} = c_{2} + c_{4}$\n\n首先，我们验证向量 $c_1, c_2, c_4$ 是否线性无关。根据定义，一组向量是线性无关的，当且仅当它们等于零向量的线性组合只有所有系数都为零的平凡组合。我们寻求标量 $a_1, a_2, a_4 \\in \\mathbb{R}$ 对以下方程的解：\n$$a_1 c_1 + a_2 c_2 + a_4 c_4 = 0$$\n该向量方程对应于一个齐次线性方程组：\n$$\n\\begin{pmatrix} 1   0   2 \\\\ 0   1   -1 \\\\ 2   3   0 \\\\ -1  2  3 \\end{pmatrix}\n\\begin{pmatrix} a_1 \\\\ a_2 \\\\ a_4 \\end{pmatrix} =\n\\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix}\n$$\n我们对系数矩阵进行高斯消元：\n$$\n\\begin{pmatrix} 1   0   2 \\\\ 0   1   -1 \\\\ 2   3   0 \\\\ -1  2  3 \\end{pmatrix}\n\\xrightarrow[R_4 \\to R_4 + R_1]{R_3 \\to R_3 - 2R_1}\n\\begin{pmatrix} 1   0   2 \\\\ 0   1   -1 \\\\ 0   3   -4 \\\\ 0   2   5 \\end{pmatrix}\n\\xrightarrow[R_4 \\to R_4 - 2R_2]{R_3 \\to R_3 - 3R_2}\n\\begin{pmatrix} 1   0   2 \\\\ 0   1   -1 \\\\ 0   0   -1 \\\\ 0   0   7 \\end{pmatrix}\n\\xrightarrow{R_4 \\to R_4 + 7R_3}\n\\begin{pmatrix} 1   0   2 \\\\ 0   1   -1 \\\\ 0   0   -1 \\\\ 0   0   0 \\end{pmatrix}\n$$\n行阶梯形矩阵有 $3$ 个主元。这表明该方程组的唯一解是平凡解 $a_1 = 0$, $a_2 = 0$, $a_4 = 0$。因此，向量 $c_1, c_2, c_4$ 是线性无关的。\n\n接下来，我们确定 $X$ 的秩。矩阵的秩是其列空间的维数，这等于最大线性无关列向量的数目。$X$ 的列空间是 $\\mathcal{C}(X) = \\text{span}\\{c_1, c_2, c_3, c_4, c_5\\}$。使用给定的 $c_3$ 和 $c_5$ 的定义：\n$$\\mathcal{C}(X) = \\text{span}\\{c_1, c_2, c_1+c_2, c_4, c_2+c_4\\}$$\n由于 $c_3$ 和 $c_5$ 是集合中其他向量的线性组合，它们不增加生成空间的维数。因此，生成集可以简化为：\n$$\\mathcal{C}(X) = \\text{span}\\{c_1, c_2, c_4\\}$$\n正如我们刚刚证明的，$\\{c_1, c_2, c_4\\}$ 是一个线性无关集，因此它构成了 $X$ 列空间的一组基。列空间的维数是其基中向量的数目，即 3。因此，$X$ 的秩为 $\\text{rank}(X) = 3$。\n\n现在，我们计算零空间 $\\mathcal{N}(X)$ 的一组基。零空间是所有满足 $Xw = 0$ 的向量 $w \\in \\mathbb{R}^5$ 的集合。令 $w = (w_1, w_2, w_3, w_4, w_5)^T$。条件 $Xw = 0$ 可以写成 $X$ 各列的线性组合：\n$$w_1 c_1 + w_2 c_2 + w_3 c_3 + w_4 c_4 + w_5 c_5 = 0$$\n代入 $c_3$ 和 $c_5$ 的定义：\n$$w_1 c_1 + w_2 c_2 + w_3 (c_1 + c_2) + w_4 c_4 + w_5 (c_2 + c_4) = 0$$\n我们按线性无关向量 $c_1, c_2, c_4$ 对各项进行分组：\n$$(w_1 + w_3)c_1 + (w_2 + w_3 + w_5)c_2 + (w_4 + w_5)c_4 = 0$$\n由于 $c_1, c_2, c_4$ 是线性无关的，它们的系数必须全为零：\n\\begin{align*} w_1 + w_3 = 0 \\\\ w_2 + w_3 + w_5 = 0 \\\\ w_4 + w_5 = 0 \\end{align*}\n这是一个包含 5 个未知数的 3 个方程的方程组。自由变量的数量为 $5 - 3 = 2$，这对应于 $X$ 的零度。这与秩-零度定理 $\\text{rank}(X) + \\text{nullity}(X) = 5$ 一致。我们选择 $w_3$ 和 $w_5$ 作为自由参数。令 $w_3 = s$ 和 $w_5 = t$，其中 $s, t \\in \\mathbb{R}$ 为任意实数。\n由第一个方程得：$w_1 = -w_3 = -s$。\n由第三个方程得：$w_4 = -w_5 = -t$。\n由第二个方程得：$w_2 = -w_3 - w_5 = -s - t$。\n通解向量 $w$ 是：\n$$w = \\begin{pmatrix} w_1 \\\\ w_2 \\\\ w_3 \\\\ w_4 \\\\ w_5 \\end{pmatrix} = \\begin{pmatrix} -s \\\\ -s-t \\\\ s \\\\ -t \\\\ t \\end{pmatrix} = s \\begin{pmatrix} -1 \\\\ -1 \\\\ 1 \\\\ 0 \\\\ 0 \\end{pmatrix} + t \\begin{pmatrix} 0 \\\\ -1 \\\\ 0 \\\\ -1 \\\\ 1 \\end{pmatrix}$$\n零空间的一组基由将一个自由参数设为 1 另一个设为 0 所对应的向量构成。\n令 $v_1 = (-1, -1, 1, 0, 0)^T$（当 $s=1, t=0$ 时）和 $v_2 = (0, -1, 0, -1, 1)^T$（当 $s=0, t=1$ 时）。\n集合 $\\{v_1, v_2\\}$ 是 $\\mathcal{N}(X)$ 的一组基。\n\n在线性模型 $y = X\\beta + \\varepsilon$ 的背景下，$X$ 的非平凡零空间（即 $\\mathcal{N}(X) \\neq \\{0\\}$）表示预测变量之间存在完全多重共线性。这对参数的可解释性有严重影响。普通最小二乘法（OLS）寻求一个最小化残差平方和的系数向量 $\\beta$，这需要求解正规方程 $(X^T X)\\beta = X^T y$。如果 $X$ 的列是线性相关的，那么格拉姆矩阵 $X^T X$ 就是奇异的，无法求逆。因此，$\\beta$ 不存在唯一解。\n如果 $\\hat{\\beta}$ 是任意一个特解，那么对于任意向量 $w \\in \\mathcal{N}(X)$，向量 $\\beta^* = \\hat{\\beta} + w$ 也是一个解，因为 $X\\beta^* = X(\\hat{\\beta} + w) = X\\hat{\\beta} + Xw = X\\hat{\\beta} + 0 = X\\hat{\\beta}$。这意味着无穷多个不同的系数向量 $\\beta$ 会产生完全相同的预测。这使得识别每个预测变量的独特“效应”成为不可能。例如，使用基向量 $v_1$，我们看到将 $\\beta_1$ 和 $\\beta_2$ 减少某个量 $k$ 的同时将 $\\beta_3$ 增加 $k$，会得到一个在观测上完全相同的模型。各个系数 $\\beta_j$ 是不可识别的。\n\n将单位矩阵 $\\lambda I_5$（其中 $\\lambda  0$）的一个正常数倍加到格拉姆矩阵 $X^T X$ 上，是岭回归的核心机制。岭回归的解为 $\\hat{\\beta}_{\\text{ridge}} = (X^T X + \\lambda I_5)^{-1} X^T y$。这个过程通过保证矩阵 $(X^T X + \\lambda I_5)$ 即使在 $X^T X$ 是奇异的情况下也是可逆的，从而确保了可识别性。\n从概念上讲，这是可行的，因为 $X^T X$ 是一个半正定矩阵，意味着其所有特征值 $\\mu_i$ 都是非负的（$\\mu_i \\ge 0$）。由于 $X$ 的列线性相关，$X^T X$ 是奇异的，这意味着它的至少一个特征值恰好为 0。设 $v$ 是 $X^T X$ 的一个特征向量，对应的特征值为 $\\mu$。那么 $(X^T X + \\lambda I_5)v = (X^T X)v + (\\lambda I_5)v = \\mu v + \\lambda v = (\\mu + \\lambda)v$。这表明正则化矩阵 $(X^T X + \\lambda I_5)$ 的特征值是 $(\\mu_i + \\lambda)$。由于 $\\mu_i \\ge 0$ 且 $\\lambda  0$，所有的特征值 $(\\mu_i + \\lambda)$ 都是严格为正的。一个方阵可逆当且仅当其所有特征值都非零。通过将所有特征值移位为严格正数，矩阵 $(X^T X + \\lambda I_5)$ 变得可逆，从而为 $\\beta$ 提供了一个唯一解。这种正则化在数学上解决了多重共线性问题，允许对一组唯一的（尽管有偏的）参数进行稳定估计。", "answer": "$$\\boxed{\\begin{pmatrix} -1  0 \\\\ -1  -1 \\\\ 1  0 \\\\ 0  -1 \\\\ 0  1 \\end{pmatrix}}$$", "id": "3140085"}, {"introduction": "在理解了线性依赖如何导致参数不唯一后，一个自然的问题是：我们如何量化这种共线性的严重程度？本练习将我们从“是/否”存在线性依赖的问题，带入到“依赖程度有多大”的量化分析中。我们将推导并计算方差膨胀因子（VIF），这是一个关键的诊断工具，并揭示其与格拉姆矩阵 $(X^{\\top} X)^{-1}$ 对角线元素之间的深刻数学联系 [@problem_id:3140092]。", "problem": "在统计学习中使用的线性回归中，考虑模型 $y = X\\beta + \\varepsilon$，其中 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2} I_{n})$，$I_{n}$ 是 $n \\times n$ 单位矩阵，$X$ 是一个 $n \\times p$ 的设计矩阵，有 $p=3$ 个预测变量，并且 $X$ 的列已经过均值中心化和单位方差标准化处理。假设样本量为 $n=120$，格拉姆矩阵为\n$$\nX^{\\top} X = \\begin{pmatrix}\n120  108  24 \\\\\n108  120  12 \\\\\n24  12  120\n\\end{pmatrix}.\n$$\n仅使用基本定义，从普通最小二乘 (OLS) 估计量及其抽样方差，以及方差膨胀因子 (VIF) 的定义出发——VIF是在相关预测变量下OLS系数的抽样方差与在具有相同边际尺度的正交预测变量下其应有的抽样方差之比。在标准化预测变量的设定下，推导出一个连接VIF与 $(X^{\\top} X)^{-1}$ 对角线元素的关系式，然后计算由给定的 $X^{\\top} X$ 所隐含的三个预测变量的最大VIF。将您的最终答案四舍五入到四位有效数字。最终答案以无单位的单个数字表示。", "solution": "首先评估问题的有效性。所有给定条件按要求逐字提取。\n- 模型：$y = X\\beta + \\varepsilon$\n- 误差分布：$\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2} I_{n})$\n- 单位矩阵：$I_{n}$ 是 $n \\times n$ 单位矩阵。\n- 设计矩阵：$X$ 是一个 $n \\times p$ 矩阵。\n- 预测变量数量：$p=3$。\n- 样本量：$n=120$。\n- 标准化：$X$ 的列经过均值中心化和单位方差标准化。\n- 格拉姆矩阵：$X^{\\top} X = \\begin{pmatrix} 120  108  24 \\\\ 108  120  12 \\\\ 24  12  120 \\end{pmatrix}$。\n- VIF 定义：在相关预测变量下OLS系数的抽样方差与在具有相同边际尺度的正交预测变量下其应有的抽样方差之比。\n- 任务：推导VIF与 $(X^{\\top} X)^{-1}$ 对角线元素之间的关系，然后计算最大的VIF。\n- 四舍五入：将最终答案四舍五入到四位有效数字。\n\n该问题是有效的。这是一个回归分析中的标准、适定问题，回归分析是统计学习的一个主题。所提供的数据是内部一致的。预测变量被标准化为单位方差的条件意味着格拉姆矩阵 $X^{\\top} X$ 的对角线元素为 $X_j^{\\top}X_j$。问题提供的对角线元素为 $120$，这等于样本量 $n=120$。这对应于此背景下标准化的一种常见定义，即 $\\frac{1}{n} \\sum_{i=1}^n X_{ij}^2 = 1$，因此 $X_j^{\\top}X_j = \\sum_{i=1}^n X_{ij}^2 = n$。给定的格拉姆矩阵也是对称和正定的，符合要求。\n\n按要求，解答过程分为两部分：首先是推导，然后是计算。\n\n**第一部分：VIF关系的推导**\n\n系数向量 $\\beta$ 的普通最小二乘 (OLS) 估计量由下式给出：\n$$\n\\hat{\\beta} = (X^{\\top} X)^{-1} X^{\\top} y\n$$\n将设计矩阵 $X$ 视为固定，$\\hat{\\beta}$ 的抽样方差-协方差矩阵推导如下：\n$$\n\\text{Var}(\\hat{\\beta}) = \\text{Var}((X^{\\top} X)^{-1} X^{\\top} y) = (X^{\\top} X)^{-1} X^{\\top} \\text{Var}(y) (X^{\\top} X^{-1} X^{\\top})^{\\top}\n$$\n根据模型假设 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2} I_{n})$，响应向量 $y$ 的方差为 $\\text{Var}(y) = \\text{Var}(X\\beta + \\varepsilon) = \\text{Var}(\\varepsilon) = \\sigma^2 I_n$。将此代入 $\\text{Var}(\\hat{\\beta})$ 的表达式中：\n$$\n\\text{Var}(\\hat{\\beta}) = (X^{\\top} X)^{-1} X^{\\top} (\\sigma^2 I_n) X ((X^{\\top} X)^{-1})^{\\top}\n$$\n由于 $X^{\\top} X$ 是对称的，其逆矩阵也是对称的。因此，$((X^{\\top} X)^{-1})^{\\top} = (X^{\\top} X)^{-1}$。\n$$\n\\text{Var}(\\hat{\\beta}) = \\sigma^2 (X^{\\top} X)^{-1} X^{\\top} I_n X (X^{\\top} X)^{-1} = \\sigma^2 (X^{\\top} X)^{-1} (X^{\\top} X) (X^{\\top} X)^{-1} = \\sigma^2 (X^{\\top} X)^{-1}\n$$\n单个系数估计量 $\\hat{\\beta}_j$ 的抽样方差是该协方差矩阵的第 $j$ 个对角线元素：\n$$\n\\text{Var}(\\hat{\\beta}_j) = \\sigma^2 \\left( (X^{\\top} X)^{-1} \\right)_{jj}\n$$\n这是VIF比率中的分子。\n\n对于分母，我们考虑一个假设情况，即预测变量是正交的，但具有相同的“边际尺度”。对于标准化的预测变量，尺度由列范数定义。如前所述，标准化意味着 $X_j^{\\top}X_j = n$。一个具有相同边际尺度的正交设计矩阵，我们记为 $X_{ortho}$，将满足 $(X_{ortho, j})^{\\top} (X_{ortho, k}) = 0$ 对 $j \\neq k$ 成立，以及 $(X_{ortho, j})^{\\top} (X_{ortho, j}) = n$。这导致一个对角格拉姆矩阵：\n$$\nX_{ortho}^{\\top} X_{ortho} = \\text{diag}(n, n, \\dots, n) = n I_p\n$$\n这个格拉姆矩阵的逆是：\n$$\n(X_{ortho}^{\\top} X_{ortho})^{-1} = (n I_p)^{-1} = \\frac{1}{n} I_p\n$$\n在这种理想化的正交情况下，第 $j$ 个系数估计量（记为 $\\hat{\\beta}_{j, ortho}$）的抽样方差将是：\n$$\n\\text{Var}(\\hat{\\beta}_{j, ortho}) = \\sigma^2 \\left( (X_{ortho}^{\\top} X_{ortho})^{-1} \\right)_{jj} = \\sigma^2 \\left( \\frac{1}{n} I_p \\right)_{jj} = \\frac{\\sigma^2}{n}\n$$\n第 $j$ 个预测变量的方差膨胀因子 VIF$_j$ 定义为这两个方差的比值：\n$$\n\\text{VIF}_j = \\frac{\\text{Var}(\\hat{\\beta}_j)}{\\text{Var}(\\hat{\\beta}_{j, ortho})} = \\frac{\\sigma^2 \\left( (X^{\\top} X)^{-1} \\right)_{jj}}{\\sigma^2 / n}\n$$\n$\\sigma^2$ 项相互抵消，从而得到标准化预测变量所需的关系式：\n$$\n\\text{VIF}_j = n \\left( (X^{\\top} X)^{-1} \\right)_{jj}\n$$\n\n**第二部分：最大VIF的计算**\n\n我们必须计算给定格拉姆矩阵 $A = X^{\\top} X$ 的逆矩阵的对角线元素：\n$$\nA = \\begin{pmatrix}\n120  108  24 \\\\\n108  120  12 \\\\\n24  12  120\n\\end{pmatrix}\n$$\n我们使用矩阵求逆公式 $A^{-1} = \\frac{1}{\\det(A)}\\text{adj}(A)$，其中 $\\text{adj}(A)$ 是伴随矩阵（代数余子式矩阵的转置）。$A^{-1}$ 的对角线元素由 $(A^{-1})_{jj} = \\frac{C_{jj}}{\\det(A)}$ 给出，其中 $C_{jj}$ 是第 $(j,j)$ 个代数余子式。\n\n首先，我们计算 $A$ 的行列式：\n$$\n\\det(A) = 120(120 \\cdot 120 - 12 \\cdot 12) - 108(108 \\cdot 120 - 12 \\cdot 24) + 24(108 \\cdot 12 - 120 \\cdot 24)\n$$\n$$\n\\det(A) = 120(14400 - 144) - 108(12960 - 288) + 24(1296 - 2880)\n$$\n$$\n\\det(A) = 120(14256) - 108(12672) + 24(-1584) = 1710720 - 1368576 - 38016 = 304128\n$$\n接下来，我们计算对角线上的代数余子式，$C_{jj} = (-1)^{j+j}M_{jj} = M_{jj}$，其中 $M_{jj}$ 是通过移除第 $j$ 行和第 $j$ 列得到的子矩阵的行列式。\n$$\nC_{11} = \\det \\begin{pmatrix} 120  12 \\\\ 12  120 \\end{pmatrix} = 120^2 - 12^2 = 14400 - 144 = 14256\n$$\n$$\nC_{22} = \\det \\begin{pmatrix} 120  24 \\\\ 24  120 \\end{pmatrix} = 120^2 - 24^2 = 14400 - 576 = 13824\n$$\n$$\nC_{33} = \\det \\begin{pmatrix} 120  108 \\\\ 108  120 \\end{pmatrix} = 120^2 - 108^2 = 14400 - 11664 = 2736\n$$\n现在我们求逆矩阵 $A^{-1}$ 的对角线元素：\n$$\n(A^{-1})_{11} = \\frac{C_{11}}{\\det(A)} = \\frac{14256}{304128}\n$$\n$$\n(A^{-1})_{22} = \\frac{C_{22}}{\\det(A)} = \\frac{13824}{304128}\n$$\n$$\n(A^{-1})_{33} = \\frac{C_{33}}{\\det(A)} = \\frac{2736}{304128}\n$$\n使用推导出的公式 $\\text{VIF}_j = n \\left( (X^{\\top} X)^{-1} \\right)_{jj}$，其中 $n=120$：\n$$\n\\text{VIF}_1 = 120 \\times \\frac{14256}{304128} = \\frac{1710720}{304128}\n$$\n为了简化，注意到 $A = 12 B$，其中 $B = \\begin{pmatrix} 10  9  2 \\\\ 9  10  1 \\\\ 2  1  10 \\end{pmatrix}$。我们有 $(A^{-1})_{11} = \\frac{14256}{304128} = \\frac{99 \\cdot 144}{176 \\cdot 1728} = \\frac{99 \\cdot 144}{176 \\cdot 12 \\cdot 144} = \\frac{99}{2112} = \\frac{9}{192} = \\frac{3}{64}$。\n所以，$\\text{VIF}_1 = 120 \\times \\frac{3}{64} = \\frac{30 \\times 3}{16} = \\frac{90}{16} = \\frac{45}{8} = 5.625$。\n\n对于其他 VIF：\n$(A^{-1})_{22} = \\frac{13824}{304128} = \\frac{96 \\cdot 144}{176 \\cdot 1728} = \\frac{96}{2112} = \\frac{6}{132} = \\frac{1}{22}$。\n$\\text{VIF}_2 = 120 \\times \\frac{1}{22} = \\frac{60}{11} \\approx 5.4545...$\n\n$(A^{-1})_{33} = \\frac{2736}{304128} = \\frac{19 \\cdot 144}{176 \\cdot 1728} = \\frac{19}{2112}$。\n$\\text{VIF}_3 = 120 \\times \\frac{19}{2112} = \\frac{10 \\times 19}{176} = \\frac{190}{176} = \\frac{95}{88} \\approx 1.0795...$\n\n比较这三个值：\n$\\text{VIF}_1 = 5.625$\n$\\text{VIF}_2 = 5.\\overline{45}$\n$\\text{VIF}_3 \\approx 1.0795$\n最大的VIF是 $\\text{VIF}_1 = 5.625$。问题要求将答案四舍五入到四位有效数字。数字5.625已经有四位有效数字。", "answer": "$$\\boxed{5.625}$$", "id": "3140092"}, {"introduction": "最后，我们将通过一个更高级的算法实践，探索如何在模型构建过程中主动选择并维持特征的线性无关性。正交匹配追踪（OMP）算法贪婪地选择特征来稀疏地逼近一个信号，为我们提供了一个绝佳的范例，展示了如何逐步构建一个线性无关的特征集。通过亲手实现这个算法，我们将实时观察所选特征子集的秩如何演变，并理解为什么在每一步都保持满秩对于算法的稳定性和模型的有效性是至关重要的 [@problem_id:3140087]。", "problem": "你将实现正交匹配追踪 (Orthogonal Matching Pursuit, OMP) 算法，并通过实验研究选择最大绝对相关性的列如何构建一个线性无关集，直到遇到数值或结构上的相关性。重点在于线性无关性、矩阵的秩和单位矩阵，这些都置于统计学习中使用的标准最小二乘投影框架内。\n\n将用到的基本概念：\n- 线性无关的定义：如果对于矩阵 $X \\in \\mathbb{R}^{n \\times p}$，方程 $X c = 0$ 的唯一解是 $c = 0$，那么该矩阵的列是线性无关的。\n- 矩阵的秩：$\\operatorname{rank}(X)$ 等于矩阵 $X$ 的列空间的维度。\n- 最小二乘法中的正交投影：将向量 $y \\in \\mathbb{R}^{n}$ 正交投影到所选列 $X_S$ 的张成空间上，可以最小化 $\\lVert y - X_S \\beta \\rVert_2$，并产生一个与 $\\operatorname{span}(X_S)$ 正交的残差。\n- 单位矩阵 $I_n$ 的秩为 $\\operatorname{rank}(I_n) = n$，其列向量相互正交且线性无关。\n\n需要实现的 OMP 过程描述：\n- 初始化残差 $r^{(0)} = y$，支撑集 $S^{(0)} = \\emptyset$。\n- 对于迭代 $t = 1, 2, \\dots$：\n  - 计算相关性 $c = X^\\top r^{(t-1)}$。\n  - 在不在 $S^{(t-1)}$ 中的索引里，选择使 $|c_j|$ 最大化的索引 $j^\\star$。如果存在多个最大值，选择最小的索引。将 $j^\\star$ 添加到支撑集中，得到 $S^{(t)}$。\n  - 通过对所选列进行最小二乘拟合来重新计算系数 $\\hat{\\beta}_{S^{(t)}}$，即最小化 $\\lVert y - X_{S^{(t)}} \\beta \\rVert_2$。\n  - 更新残差 $r^{(t)} = y - X_{S^{(t)}} \\hat{\\beta}_{S^{(t)}}$。\n  - 每次选择后，使用标准的奇异值分解容差计算数值秩 $\\operatorname{rank}(X_{S^{(t)}})$。\n  - 如果达到预设的最大迭代次数 $K$ 或对于固定阈值 $\\tau$ 满足 $\\lVert r^{(t)} \\rVert_2 \\leq \\tau$，则停止迭代。\n\n你的程序必须：\n- 实现上述 OMP 算法，当出现绝对相关性相等时，通过选择最小索引来打破平局。\n- 每次选择后，记录：\n  - 所选索引 $j^\\star$（使用基于0的索引）。\n  - 当前秩 $\\operatorname{rank}(X_{S^{(t)}})$，作为一个整数。\n- 在每个测试用例结束时，返回：\n  - 按选择顺序列出的所选索引列表。\n  - 每次选择后的秩列表。\n  - 一个布尔标志，指示所选集合的所有前缀是否都具有满列秩，即在每次执行的迭代中，$\\operatorname{rank}(X_{S^{(t)}}) = |S^{(t)}|$ 是否都成立。\n\n终止阈值和范数：\n- 使用欧几里得范数。如果 $\\lVert r^{(t)} \\rVert_2 \\leq \\tau$（其中 $\\tau = 10^{-12}$），则提前停止。\n- 使用奇异值分解程序提供的带有其标准容差的默认数值秩。\n\n测试套件：\n在以下三个测试用例上实现 OMP 过程。在所有情况下，列索引均使用基于0的索引，并按规定设置最大迭代次数 $K$。\n\n$1.$ 单位矩阵字典（理想情况）：\n- $X^{(1)} = I_5$。\n- $y^{(1)} = [\\,2,\\,-1,\\,0,\\,3,\\,0\\,]^\\top$。\n- $K^{(1)} = 3$。\n\n$2.$ 重复列（字典中存在结构性相关）：\n- 令列向量 $x_0, x_1, x_2, x_3, x_4 \\in \\mathbb{R}^4$ 为\n  $$x_0 = [\\,1,\\,0,\\,0,\\,0\\,]^\\top,\\quad x_1 = [\\,0,\\,1,\\,0,\\,0\\,]^\\top,\\quad x_2 = [\\,0,\\,1,\\,0,\\,0\\,]^\\top,$$\n  $$x_3 = [\\,0,\\,0,\\,1,\\,0\\,]^\\top,\\quad x_4 = [\\,0,\\,0,\\,0,\\,1\\,]^\\top.$$\n  将这些列向量堆叠起来形成矩阵 $X^{(2)} \\in \\mathbb{R}^{4 \\times 5}$。\n- $y^{(2)} = [\\,1,\\,2,\\,0,\\,0\\,]^\\top$。\n- $K^{(2)} = 3$。\n\n$3.$ 近似共线列（数值相关性边缘情况）：\n- 令列向量 $x_0, x_1, x_2 \\in \\mathbb{R}^3$ 为\n  $$x_0 = [\\,1,\\,0,\\,0\\,]^\\top,\\quad x_1 = [\\,1,\\,10^{-16},\\,0\\,]^\\top,\\quad x_2 = [\\,0,\\,0,\\,1\\,]^\\top.$$\n  将这些列向量堆叠起来形成矩阵 $X^{(3)} \\in \\mathbb{R}^{3 \\times 3}$。\n- $y^{(3)} = x_1 = [\\,1,\\,10^{-16},\\,0\\,]^\\top$。\n- $K^{(3)} = 2$。\n\n最终输出格式：\n- 你的程序应生成单行输出，其中包含用方括号括起来的逗号分隔列表形式的结果。\n- 对于每个测试用例 $i \\in \\{1,2,3\\}$，输出一个嵌套列表 $[\\text{indices}^{(i)}, \\text{ranks}^{(i)}, \\text{independent\\_all}^{(i)}]$，其中：\n  - $\\text{indices}^{(i)}$ 是所选索引的列表（整数，基于0），\n  - $\\text{ranks}^{(i)}$ 是每次选择后的秩的列表（整数），\n  - $\\text{independent\\_all}^{(i)}$ 是一个布尔值。\n- 顶层列表必须按 $[\\,\\text{case }1,\\,\\text{case }2,\\,\\text{case }3\\,]$ 的顺序排列。\n- 所需格式示例（非实际答案）：$[[[\\,\\cdot\\,],[\\,\\cdot\\,],\\text{True}],[[\\,\\cdot\\,],[\\,\\cdot\\,],\\text{True}],[[\\,\\cdot\\,],[\\,\\cdot\\,],\\text{False}]]$。\n\n本问题不涉及物理单位或角度。所有数值输出都应按上述要求以整数、浮点数或布尔值的形式提供。", "solution": "该问题要求实现正交匹配追踪 (OMP) 算法，以分析从字典矩阵 $X$ 中选出的列向量的线性无关性。分析在三个特定的测试用例上进行，这些用例旨在探究该算法在理想条件、结构性相关（重复列）和数值相关（近似共线列）下的行为。\n\nOMP 算法是一种用于稀疏信号近似的贪心迭代方法。其目标是找到一个稀疏系数向量 $\\beta$，使得字典矩阵 $X$ 的列的线性组合能最好地解释信号 $y$，即 $y \\approx X\\beta$。\n\n该算法按以下步骤进行：\n1.  **初始化**：以一个空支撑集 $S^{(0)} = \\emptyset$ 开始，初始残差等于信号本身，即 $r^{(0)} = y$。\n2.  **迭代 $t$**：\n    a. **匹配步骤**：在 $X$ 中找到与当前残差 $r^{(t-1)}$ 最相关的列 $x_j$（其中 $j \\notin S^{(t-1)}$）。这是通过找到使内积的绝对值最大化的索引 $j^\\star$ 来实现的：\n    $$\n    j^\\star = \\arg\\max_{j \\notin S^{(t-1)}} | \\langle x_j, r^{(t-1)} \\rangle | = \\arg\\max_{j \\notin S^{(t-1)}} | (X^\\top r^{(t-1)})_j |\n    $$\n    如果最大绝对相关性出现平局，问题规定选择最小的索引。\n    b. **支撑集更新**：将新选出的索引添加到支撑集中：$S^{(t)} = S^{(t-1)} \\cup \\{j^\\star\\}$。\n    c. **正交化步骤**：通过求解一个最小二乘问题来找到新的系数向量 $\\hat{\\beta}_{S^{(t)}}$，该问题将原始信号 $y$ 投影到当前支撑集 $X_{S^{(t)}}$ 中列向量所张成的子空间上：\n    $$\n    \\hat{\\beta}_{S^{(t)}} = \\arg\\min_{\\beta} \\lVert y - X_{S^{(t)}} \\beta \\rVert_2\n    $$\n    d. **残差更新**：将残差更新为 $y$ 与子空间 $\\operatorname{span}(X_{S^{(t)}})$ 正交的分量：\n    $$\n    r^{(t)} = y - X_{S^{(t)}} \\hat{\\beta}_{S^{(t)}}\n    $$\n    根据构造，这个新的残差 $r^{(t)}$ 与所有先前选择的列正交，即 $(X_{S^{(t)}})^\\top r^{(t)} = 0$。\n\n这个问题的核心是监控所选列的线性无关性。如果一组列向量构成的矩阵具有满列秩，则这组列向量是线性无关的。因此，在每次迭代 $t$ 中，我们检查是否满足 $\\operatorname{rank}(X_{S^{(t)}}) = |S^{(t)}| = t$。如果此条件在任何步骤中不成立，则表示新添加的列是先前所选列的线性组合（或在数值上接近线性组合）。我们将使用通过奇异值分解 (SVD) 计算的数值秩。\n\n当达到最大迭代次数 $K$ 或残差的范数低于阈值 $\\tau = 10^{-12}$ 时，算法终止，后者表示信号 $y$ 已被充分重构。\n\n我们现在分析三个指定的测试用例。\n\n**测试用例 1：单位矩阵字典**\n-   $X^{(1)} = I_5$，一个标准正交字典。\n-   $y^{(1)} = [\\,2,\\,-1,\\,0,\\,3,\\,0\\,]^\\top$。\n-   $K^{(1)} = 3$。\n由于 $X^{(1)}$ 是单位矩阵，其列向量相互正交。在每一步 $t$ 中，相关性向量为 $c = (X^{(1)})^\\top r^{(t-1)} = r^{(t-1)}$。算法将贪心地选择当前残差中绝对值最大的分量所对应的索引。\n-   **迭代 $t=1$**：$r^{(0)} = [\\,2,\\,-1,\\,0,\\,3,\\,0\\,]^\\top$。最大绝对值为 $3$，在索引 $3$ 处。因此，选择 $j_1^\\star=3$。$S^{(1)}=\\{3\\}$。矩阵 $X_{S^{(1)}} = [x_3]$ 的秩为 $1$。\n-   **迭代 $t=2$**：新的残差 $r^{(1)}$ 将是 $y^{(1)}$ 将其第 $3$ 个分量置零后的结果：$r^{(1)} = [\\,2,\\,-1,\\,0,\\,0,\\,0\\,]^\\top$。最大绝对值为 $2$，在索引 $0$ 处。因此，选择 $j_2^\\star=0$。$S^{(2)}=\\{3,0\\}$。矩阵 $X_{S^{(2)}}=[x_3, x_0]$ 的秩为 $2$。\n-   **迭代 $t=3$**：残差 $r^{(2)}$ 将是 $r^{(1)}$ 将其第 $0$ 个分量置零后的结果：$r^{(2)} = [\\,0,\\,-1,\\,0,\\,0,\\,0\\,]^\\top$。最大绝对值为 $1$，在索引 $1$ 处。因此，选择 $j_3^\\star=1$。$S^{(3)}=\\{3,0,1\\}$。$X_{S^{(3)}}=[x_3, x_0, x_1]$ 的秩为 $3$。\n在整个过程中，秩等于所选列的数量，因此 `independent_all` 标志保持为 `True`。\n\n**测试用例 2：重复列**\n-   $X^{(2)} \\in \\mathbb{R}^{4 \\times 5}$，其中 $x_1 = x_2 = [\\,0,\\,1,\\,0,\\,0\\,]^\\top$。\n-   $y^{(2)} = [\\,1,\\,2,\\,0,\\,0\\,]^\\top = 1 \\cdot x_0 + 2 \\cdot x_1$。\n-   $K^{(2)} = 3$。\n这个案例测试结构性相关。\n-   **迭代 $t=1$**：$r^{(0)} = y^{(2)}$。相关性为 $c = (X^{(2)})^\\top y^{(2)} = [\\,1,\\,2,\\,2,\\,0,\\,0\\,]^\\top$。最大绝对相关性出现平局（$|c_1|=|c_2|=2$）。平局打破规则选择最小的索引，因此 $j_1^\\star=1$。$S^{(1)}=\\{1\\}$。秩为 $1$。残差更新为 $r^{(1)} = y^{(2)} - 2x_1 = [\\,1,\\,0,\\,0,\\,0\\,]^\\top = x_0$。\n-   **迭代 $t=2$**：与 $r^{(1)}=x_0$ 的相关性为 $c = (X^{(2)})^\\top x_0 = [\\,1,\\,0,\\,0,\\,0,\\,0\\,]^\\top$。可用索引中的最大相关性在索引 $0$ 处。因此，选择 $j_2^\\star=0$。$S^{(2)}=\\{1,0\\}$。$X_{S^{(2)}} = [x_1, x_0]$ 的秩为 $2$。新的残差为 $r^{(2)} = y^{(2)} - (2x_1 + 1x_0) = 0$。\n由于 $\\lVert r^{(2)} \\rVert_2 = 0 \\leq \\tau$，算法终止。它成功地使用一个线性无关集 $\\{x_1, x_0\\}$ 重构了信号，并且没有继续选择相关的列 $x_2$。`independent_all` 标志保持为 `True`。\n\n**测试用例 3：近似共线列**\n-   $X^{(3)} \\in \\mathbb{R}^{3 \\times 3}$，其中 $x_0 = [\\,1,\\,0,\\,0\\,]^\\top$ 且 $x_1 = [\\,1,\\,10^{-16},\\,0\\,]^\\top$。\n-   $y^{(3)} = x_1 = [\\,1,\\,10^{-16},\\,0\\,]^\\top$。\n-   $K^{(3)} = 2$。\n这个案例测试数值相关性。\n-   **迭代 $t=1$**：$r^{(0)} = y^{(3)} = x_1$。相关性为 $c = (X^{(3)})^\\top x_1 = [\\,x_0^\\top x_1, \\,x_1^\\top x_1, \\,x_2^\\top x_1\\,]^\\top = [\\,1, \\,1+(10^{-16})^2, \\,0\\,]^\\top$。最大相关性在索引 $1$ 处。因此，选择 $j_1^\\star=1$。$S^{(1)}=\\{1\\}$。秩为 $1$。用 $x_1$ 拟合 $y^{(3)}=x_1$ 的最小二乘解为 $\\hat{\\beta}_1=1$。新的残差为 $r^{(1)} = y^{(3)} - 1 \\cdot x_1 = 0$。\n由于 $\\lVert r^{(1)} \\rVert_2 = 0 \\leq \\tau$，算法终止。它用单个向量完美地表示了信号。算法不会进行第二次迭代，从而避免了选择近似共线的向量 $x_0$，否则会创建一个病态的子矩阵 $X_{\\{1,0\\}}$。`independent_all` 标志保持为 `True`。\n\n在这三种情况下，算法都构建了一个线性无关的列集合。对于案例2和3，由于信号在达到最大迭代次数之前被完美重构，算法终止，从而预先阻止了会引入线性相关（结构性或数值性）的列的选择。这展示了 OMP 在信号在给定字典中具有稀疏表示的情况下的一个关键特性。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run OMP on all test cases and print results.\n    \"\"\"\n\n    def custom_str(obj):\n        \"\"\"Custom string conversion to remove spaces.\"\"\"\n        if isinstance(obj, bool):\n            return str(obj).lower()\n        elif isinstance(obj, (list, tuple)):\n            if not obj:\n                return \"[]\"\n            return f\"[{','.join(custom_str(item) for item in obj)}]\"\n        return str(obj)\n\n    def run_omp(X, y, K, tau=1e-12):\n        \"\"\"\n        Implements the Orthogonal Matching Pursuit algorithm.\n\n        Args:\n            X (np.ndarray): The dictionary matrix of shape (n, p).\n            y (np.ndarray): The target vector of shape (n,).\n            K (int): The maximum number of iterations.\n            tau (float): The residual norm threshold for early stopping.\n\n        Returns:\n            list: [selected_indices, ranks, independent_all]\n        \"\"\"\n        n, p = X.shape\n        r = y.copy()\n        S = []  # Support set of selected indices\n        \n        selected_indices = []\n        ranks = []\n        independent_all = True\n        \n        for t in range(1, K + 1):\n            # Compute correlations\n            correlations = X.T @ r\n            \n            # Mask out already selected indices\n            abs_correlations = np.abs(correlations)\n            abs_correlations[S] = -1.0 # Use a value guaranteed to not be the max\n            \n            # Find index with max absolute correlation, with tie-breaking\n            max_corr_val = np.max(abs_correlations)\n            if max_corr_val = 0: # All columns have been selected or remaining correlations are zero\n                break\n            \n            candidate_indices = np.where(abs_correlations == max_corr_val)[0]\n            j_star = np.min(candidate_indices) # Tie-breaking: smallest index\n\n            # Update support set and recorded lists\n            S.append(j_star)\n            selected_indices.append(j_star)\n            \n            # Form submatrix and compute its rank\n            X_S = X[:, S]\n            current_rank = np.linalg.matrix_rank(X_S)\n            ranks.append(current_rank)\n            \n            # Check for linear independence\n            if current_rank != t:\n                independent_all = False\n\n            # Solve least squares and update residual\n            # Use rcond=None to adopt the new default behavior of np.linalg.lstsq\n            beta_S, _, _, _ = np.linalg.lstsq(X_S, y, rcond=None)\n            r = y - X_S @ beta_S\n            \n            # Check for termination due to small residual\n            if np.linalg.norm(r) = tau:\n                break\n                \n        return [selected_indices, ranks, independent_all]\n\n    # Test Case 1: Identity dictionary\n    X1 = np.eye(5)\n    y1 = np.array([2.0, -1.0, 0.0, 3.0, 0.0])\n    K1 = 3\n\n    # Test Case 2: Duplicate columns\n    X2 = np.array([\n        [1.0, 0.0, 0.0, 0.0, 0.0],\n        [0.0, 1.0, 1.0, 0.0, 0.0],\n        [0.0, 0.0, 0.0, 1.0, 0.0],\n        [0.0, 0.0, 0.0, 0.0, 1.0]\n    ]).T\n    y2 = np.array([1.0, 2.0, 0.0, 0.0])\n    K2 = 3\n\n    # Test Case 3: Nearly collinear columns\n    X3 = np.array([\n        [1.0, 1.0, 0.0],\n        [0.0, 1e-16, 0.0],\n        [0.0, 0.0, 1.0]\n    ]).T\n    y3 = np.array([1.0, 1e-16, 0.0])\n    K3 = 2\n\n    test_cases = [\n        (X1, y1, K1),\n        (X2, y2, K2),\n        (X3, y3, K3)\n    ]\n\n    results = []\n    for X, y, K in test_cases:\n        result = run_omp(X, y, K)\n        # Manually convert python bool to lowercase as per weird example format\n        result[2] = True if result[2] else False\n        results.append(result)\n\n    # Format the final output string without spaces and with lowercase booleans\n    print(custom_str(results))\n\n# solve() # This is commented out to prevent execution in this context. The code itself is the answer.\n```", "id": "3140087"}]}