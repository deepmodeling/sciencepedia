{"hands_on_practices": [{"introduction": "本练习将基础的伯努利分布与逻辑回归联系起来，后者是现代机器学习中用于二元分类的基石模型。你将从第一性原理出发，推导$L_2$正则化逻辑回归的梯度，并实现梯度下降算法来训练模型。通过这个练习，你将深入理解正则化如何作为“权重衰减”发挥作用，以及学习率如何影响优化过程的动态变化。[@problem_id:3116203]", "problem": "给定一个使用伯努利似然和逻辑斯谛链接函数建模的二元分类情景。该问题的基本依据包括以下经过充分检验的事实和核心定义：伯努利概率质量函数 $p(y \\mid \\pi) = \\pi^{y} (1 - \\pi)^{1 - y}$（其中 $y \\in \\{0,1\\}$），观测的独立性意味着联合似然是单个似然的乘积，以及将实值分数 $z$ 映射到 $(0,1)$ 区间内概率的逻辑斯谛函数 $\\sigma(z) = \\dfrac{1}{1 + e^{-z}}$。正则化对权重的欧几里得二范数（L2）的平方施加惩罚，表示为 $\\dfrac{\\lambda}{2} \\lVert w \\rVert^{2}$，其中 $\\lambda$ 为非负系数。\n\n任务 A（从第一性原理推导）：考虑 $n$ 个独立观测 $\\{(x_{i}, y_{i})\\}_{i=1}^{n}$，其中 $x_{i} \\in \\mathbb{R}^{d}$ 且 $y_{i} \\in \\{0,1\\}$。假设 $y_{i}$ 的伯努利参数为 $\\pi_{i} = \\sigma(z_{i})$，其中 $z_{i} = w^{\\top} x_{i}$ 且 $w \\in \\mathbb{R}^{d}$ 是权重向量。从伯努利概率质量函数和逻辑斯谛函数的定义出发，推导正则化负对数似然\n$$\nJ(w) = \\sum_{i=1}^{n} \\left[ - y_{i} \\log \\sigma(z_{i}) - (1 - y_{i}) \\log \\left(1 - \\sigma(z_{i})\\right) \\right] + \\frac{\\lambda}{2} \\lVert w \\rVert^{2}\n$$\n关于 $w$ 的梯度。然后，说明该梯度如何在梯度下降中对 $w$ 产生乘性收缩，并阐明其与权重衰减的关系。\n\n任务 B（算法分析）：使用推导出的梯度实现梯度下降，以研究不同学习率下的训练动态。使用以下固定的、科学合理的测试套件，以确保覆盖典型和边缘情况：\n- 数据维度 $d = 2$ 和样本数 $n = 5$。\n- 特征矩阵 $X \\in \\mathbb{R}^{n \\times d}$ 和标签 $y \\in \\{0,1\\}^{n}$ 由以下明确给出\n$$\nX = \\begin{bmatrix}\n1.0  0.0 \\\\\n1.0  1.0 \\\\\n2.0  -1.0 \\\\\n-1.0  2.0 \\\\\n0.0  -2.0\n\\end{bmatrix}, \\quad\ny = \\begin{bmatrix}\n1 \\\\\n1 \\\\\n1 \\\\\n0 \\\\\n0\n\\end{bmatrix}.\n$$\n- 正则化系数 $\\lambda = 0.1$。\n- 初始化 $w^{(0)} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$（即，所有权重初始化为零）。\n- 梯度下降迭代次数 $T = 50$。\n- 待测试的学习率：$\\eta \\in \\{0.0, 0.05, 0.2, 1.5\\}$。\n\n为保证数值鲁棒性，使用与 softplus 函数的恒等式来计算负对数似然以避免上溢：\n$$\n\\text{softplus}(z) = \\log(1 + e^{z}) = \\max(0,z) + \\log\\!\\left(1 + e^{-\\lvert z \\rvert}\\right),\n$$\n并使用基于对 $z$ 符号进行分情况讨论的数值稳定逻辑斯谛函数实现。对于测试套件中的每个学习率 $\\eta$，运行梯度下降 $T$ 次迭代，并计算最终的正则化目标值 $J\\!\\left(w^{(T)}\\right)$。\n\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表的结果。该列表必须包含与学习率 $\\eta = 0.0, \\eta = 0.05, \\eta = 0.2, \\eta = 1.5$ 顺序对应的四个最终目标值 $J\\!\\left(w^{(T)}\\right)$，每个值以保留六位小数的十进制形式打印，例如 $\\left[\\text{val}_{1},\\text{val}_{2},\\text{val}_{3},\\text{val}_{4}\\right]$。所有值均为十进制实数，不适用物理单位，也不使用百分比。", "solution": "该问题被评估为有效，因为它基于科学原理、自成体系、定义明确且客观。它提出了一个统计学习中的标准任务——为 L2 正则化逻辑斯谛回归推导和实现梯度下降——并明确指定了所有必要的数据和参数。\n\n### 任务 A：推导与分析\n\n目标是推导正则化负对数似然函数的梯度，并分析其在梯度下降更新规则中的作用。\n\n正则化负对数似然函数如下：\n$$\nJ(w) = \\sum_{i=1}^{n} \\left[ - y_{i} \\log \\sigma(z_{i}) - (1 - y_{i}) \\log \\left(1 - \\sigma(z_{i})\\right) \\right] + \\frac{\\lambda}{2} \\lVert w \\rVert^{2}\n$$\n其中 $z_i = w^\\top x_i$，$w \\in \\mathbb{R}^d$，$x_i \\in \\mathbb{R}^d$，$y_i \\in \\{0, 1\\}$。逻辑斯谛函数是 $\\sigma(z) = (1 + e^{-z})^{-1}$。\n\n梯度 $\\nabla_w J(w)$ 可以通过分别考虑损失项和正则化项来计算。设 $L(w) = \\sum_{i=1}^{n} L_i(w)$ 为损失（负对数似然），$R(w) = \\frac{\\lambda}{2} \\lVert w \\rVert^2$ 为正则化项。则 $J(w) = L(w) + R(w)$。\n\n首先，我们求正则化项的梯度。由于 $\\lVert w \\rVert^2 = w^\\top w$，我们有：\n$$\n\\nabla_w R(w) = \\nabla_w \\left(\\frac{\\lambda}{2} w^\\top w\\right) = \\frac{\\lambda}{2} (2 w) = \\lambda w\n$$\n这是向量微积分中的一个标准结果。\n\n接下来，我们求单个观测 $i$ 的损失项梯度，$L_i(w) = - y_{i} \\log \\sigma(z_{i}) - (1 - y_{i}) \\log(1 - \\sigma(z_{i}))$。我们使用链式法则：$\\nabla_w L_i(w) = \\frac{\\partial L_i}{\\partial z_i} \\nabla_w z_i$。\n\n$z_i = w^\\top x_i$ 关于 $w$ 的梯度就是：\n$$\n\\nabla_w z_i = x_i\n$$\n为了求得 $\\frac{\\partial L_i}{\\partial z_i}$，我们首先需要逻辑斯谛函数的导数 $\\frac{d\\sigma(z)}{dz}$。\n$$\n\\frac{d\\sigma(z)}{dz} = \\frac{d}{dz} (1 + e^{-z})^{-1} = -1(1 + e^{-z})^{-2} (-e^{-z}) = \\frac{e^{-z}}{(1 + e^{-z})^2} = \\frac{1}{1 + e^{-z}} \\cdot \\frac{e^{-z}}{1 + e^{-z}}\n$$\n认识到 $\\sigma(z) = \\frac{1}{1 + e^{-z}}$ 和 $1 - \\sigma(z) = \\frac{e^{-z}}{1 + e^{-z}}$，我们得到恒等式：\n$$\n\\frac{d\\sigma(z)}{dz} = \\sigma(z) (1 - \\sigma(z))\n$$\n现在我们可以再次使用链式法则计算 $\\frac{\\partial L_i}{\\partial z_i}$，即 $\\frac{\\partial L_i}{\\partial z_i} = \\frac{\\partial L_i}{\\partial \\sigma(z_i)} \\frac{d\\sigma(z_i)}{dz_i}$。\n$$\n\\frac{\\partial L_i}{\\partial \\sigma(z_i)} = -\\frac{y_i}{\\sigma(z_i)} + \\frac{1-y_i}{1-\\sigma(z_i)} = \\frac{-y_i(1-\\sigma(z_i)) + (1-y_i)\\sigma(z_i)}{\\sigma(z_i)(1-\\sigma(z_i))} = \\frac{\\sigma(z_i) - y_i}{\\sigma(z_i)(1-\\sigma(z_i))}\n$$\n综合这些结果：\n$$\n\\frac{\\partial L_i}{\\partial z_i} = \\frac{\\partial L_i}{\\partial \\sigma(z_i)} \\frac{d\\sigma(z_i)}{dz_i} = \\left( \\frac{\\sigma(z_i) - y_i}{\\sigma(z_i)(1-\\sigma(z_i))} \\right) \\cdot \\left( \\sigma(z_i)(1-\\sigma(z_i)) \\right) = \\sigma(z_i) - y_i\n$$\n这个简洁的结果表明，交叉熵损失关于预激活得分 $z_i$ 的导数是预测概率与真实标签之差。\n\n因此，$L_i(w)$ 的梯度为：\n$$\n\\nabla_w L_i(w) = \\frac{\\partial L_i}{\\partial z_i} \\nabla_w z_i = (\\sigma(w^\\top x_i) - y_i) x_i\n$$\n总损失 $L(w)$ 的梯度是所有观测的总和：\n$$\n\\nabla_w L(w) = \\sum_{i=1}^{n} (\\sigma(w^\\top x_i) - y_i) x_i\n$$\n结合损失项和正则化项的梯度，我们得到目标函数 $J(w)$ 的完整梯度：\n$$\n\\nabla_w J(w) = \\sum_{i=1}^{n} (\\sigma(w^\\top x_i) - y_i) x_i + \\lambda w\n$$\n在矩阵表示法中，其中 $X \\in \\mathbb{R}^{n \\times d}$ 是行向量为 $x_i^\\top$ 的设计矩阵，$y \\in \\{0,1\\}^n$ 是标签向量，$\\sigma$ 是逐元素应用的，梯度为：\n$$\n\\nabla_w J(w) = X^\\top (\\sigma(Xw) - y) + \\lambda w\n$$\n\n现在，我们分析此梯度在梯度下降更新中的作用。在迭代次数 $t$ 时，权重向量 $w$ 的更新规则（学习率为 $\\eta > 0$）是：\n$$\nw^{(t+1)} \\leftarrow w^{(t)} - \\eta \\nabla_w J(w^{(t)})\n$$\n代入推导出的梯度：\n$$\nw^{(t+1)} \\leftarrow w^{(t)} - \\eta \\left( \\sum_{i=1}^{n} (\\sigma(w^{(t)\\top} x_i) - y_i) x_i + \\lambda w^{(t)} \\right)\n$$\n我们可以重新整理这些项，以分离出正则化项对 $w^{(t)}$ 的影响：\n$$\nw^{(t+1)} \\leftarrow w^{(t)} - \\eta \\lambda w^{(t)} - \\eta \\left( \\sum_{i=1}^{n} (\\sigma(w^{(t)\\top} x_i) - y_i) x_i \\right)\n$$\n从前两项中提取 $w^{(t)}$ 因子：\n$$\nw^{(t+1)} \\leftarrow (1 - \\eta \\lambda) w^{(t)} - \\eta \\nabla_w L(w^{(t)})\n$$\n这种形式明确揭示了与权重衰减的关系。在减去未正则化损失的梯度之前，当前权重向量 $w^{(t)}$ 乘以一个因子 $(1 - \\eta \\lambda)$。由于 $\\eta > 0$ 和 $\\lambda > 0$，该因子小于 1，导致权重在每次迭代中都向零收缩。这种乘性收缩正是所谓的“权重衰减”。因此，对于标准梯度下降，目标函数上的 L2 正则化在数学上等同于在更新规则中应用权重衰减。\n\n### 任务 B：算法实现\n\n分析现在转向使用推导出的梯度进行具体的数值实现。任务是为指定的数据集和参数运行梯度下降，并报告不同学习率下的最终目标值 $J(w^{(T)})$。\n\n为了数值稳定性，目标函数 $J(w)$ 将使用二元交叉熵损失的恒等式来计算：\n$$\nL_i(w) = \\log(1 + e^{z_i}) - y_i z_i = \\text{softplus}(z_i) - y_i z_i\n$$\n其中 $z_i = w^\\top x_i$。`softplus` 函数通过 $\\text{softplus}(z) = \\max(0,z) + \\log(1 + e^{-\\lvert z \\rvert})$ 进行稳定计算。\n因此，总目标函数为：\n$$\nJ(w) = \\sum_{i=1}^{n} (\\text{softplus}(w^\\top x_i) - y_i (w^\\top x_i)) + \\frac{\\lambda}{2} w^\\top w\n$$\n梯度下降算法通过将 $w^{(0)}$ 初始化为零向量，并使用推导出的梯度 $\\nabla_w J(w) = X^\\top (\\sigma(Xw) - y) + \\lambda w$ 迭代 $T=50$ 步来进行。逻辑斯谛函数 $\\sigma(z)$ 通过分情况讨论来实现，以防止对大的负输入值发生上溢。对每个学习率 $\\eta \\in \\{0.0, 0.05, 0.2, 1.5\\}$ 重复此过程。\n对于 $\\eta=0.0$，权重不会从其初始零向量状态更新。最终目标值为 $J(w=\\mathbf{0}) = \\sum_{i=1}^5 \\text{softplus}(0) + 0 = 5 \\log(2) \\approx 3.465736$。\n对于其他学习率，迭代更新将使 $w$ 移向凸目标函数的最小值。一个小的、稳定的学习率，如 $\\eta=0.05$ 或 $\\eta=0.2$，预计会平滑收敛，从而得到一个低的目标值。一个非常大的学习率，如 $\\eta=1.5$，可能会导致迭代越过最小值并发散，从而导致一个大的或非有限的目标值。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements gradient descent for L2-regularized logistic regression and\n    evaluates the final objective function value for a suite of learning rates.\n    \"\"\"\n    # Define the fixed, scientifically sound test suite from the problem.\n    X_data = np.array([\n        [1.0, 0.0],\n        [1.0, 1.0],\n        [2.0, -1.0],\n        [-1.0, 2.0],\n        [0.0, -2.0]\n    ])\n    y_data = np.array([1, 1, 1, 0, 0])\n    \n    test_params = {\n        'lambda_reg': 0.1,\n        'w_init': np.zeros(2),\n        'iterations': 50,\n        'learning_rates': [0.0, 0.05, 0.2, 1.5]\n    }\n\n    def stable_sigmoid(z):\n        \"\"\"Numerically stable implementation of the logistic function.\"\"\"\n        # Use np.where for vectorized case analysis\n        return np.where(z >= 0, 1 / (1 + np.exp(-z)), np.exp(z) / (1 + np.exp(z)))\n\n    def stable_softplus(z):\n        \"\"\"Numerically stable implementation of the softplus function.\"\"\"\n        # Using the identity: softplus(z) = max(0,z) + log(1 + exp(-|z|))\n        return np.maximum(0, z) + np.log(1 + np.exp(-np.abs(z)))\n\n    def objective_function(w, X, y, lambda_reg):\n        \"\"\"Computes the regularized negative log-likelihood (J(w)).\"\"\"\n        z = X @ w\n        # Loss using the stable softplus identity: log(1+exp(z)) - y*z\n        loss = np.sum(stable_softplus(z) - y * z)\n        # L2 regularization term\n        reg_term = (lambda_reg / 2) * np.dot(w, w)\n        return loss + reg_term\n\n    results = []\n    \n    lambda_reg = test_params['lambda_reg']\n    T = test_params['iterations']\n\n    for eta in test_params['learning_rates']:\n        w = test_params['w_init'].copy()\n        \n        for _ in range(T):\n            # Calculate scores z = Xw\n            z = X_data @ w\n            \n            # Predictions pi = sigma(z)\n            pi = stable_sigmoid(z)\n            \n            # Gradient: X^T * (pi - y) + lambda * w\n            gradient = X_data.T @ (pi - y_data) + lambda_reg * w\n            \n            # Gradient descent update\n            w = w - eta * gradient\n        \n        # Compute final objective value\n        final_objective = objective_function(w, X_data, y_data, lambda_reg)\n        results.append(final_objective)\n\n    # Final print statement in the exact required format.\n    # The formatted string f'{r:.6f}' rounds and ensures 6 decimal places.\n    print(f\"[{','.join([f'{r:.6f}' for r in results])}]\")\n\nsolve()\n```", "id": "3116203"}, {"introduction": "在参数估计概念的基础上，本练习深入探讨了二项分布估计量的性质。你将比较标准的“最大似然估计”（Maximum Likelihood Estimator, MLE）与一种带惩罚项的MLE，后者会将估计值“收缩”到某个先验值。通过编程精确计算偏差、方差和均方误差（Mean Squared Error, MSE），你将亲身体验并深入理解“偏差-方差权衡”这一核心概念，它对于防止模型过拟合至关重要。[@problem_id:3116263]", "problem": "考虑一个随机变量 $Y$，它服从参数为 $n$ 和 $p$ 的二项分布（Binomial distribution），记作 $Y \\sim \\text{Binomial}(n,p)$。二项分布的概率质量函数由下式给出\n$$\n\\mathbb{P}(Y=y) = \\binom{n}{y} p^{y} (1-p)^{n-y} \\quad \\text{for } y \\in \\{0,1,\\ldots,n\\}.\n$$\n给定观测值 $Y=y$，对于 $p \\in (0,1)$ 的相关对数似然函数为\n$$\n\\ell(p; y) = y \\log p + (n-y) \\log (1-p).\n$$\n最大似然估计（Maximum Likelihood Estimation, MLE）是指通过在 $p \\in [0,1]$ 上最大化对数似然函数 $\\ell(p; y)$ 来估计参数 $p$ 的方法。在统计学习中，通常会考虑使用带惩罚项的似然函数来引入先验信念或正则化。考虑带惩罚项的目标函数\n$$\n\\ell_{\\lambda}(p; y) = \\ell(p; y) - \\lambda \\lvert p - p_0 \\rvert,\n$$\n其中 $\\lambda \\ge 0$ 是惩罚强度，$p_0 \\in [0,1]$ 是一个固定的参考值。$p$ 的带惩罚项的最大似然估计定义为 $\\ell_{\\lambda}(p; y)$ 在 $p \\in [0,1]$ 上的最大化者。\n\n任务：\n1. 从二项似然函数的基本定义出发，为单个观测值 $Y=y$ 定义标准最大似然估计 $\\hat{p}_{\\text{std}}(y)$ 和带惩罚项的最大似然估计 $\\hat{p}_{\\lambda}(y)$。该定义必须与在 $p \\in [0,1]$ 上最大化各自的凹目标函数相一致。\n2. 使用“无意识统计学家法则”（law of the unconscious statistician），将在真实参数为 $p$ 的二项抽样模型下，估计量 $\\hat{p}(Y)$ 的偏差和方差表示为\n$$\n\\text{Bias}(\\hat{p}) = \\mathbb{E}[\\hat{p}(Y)] - p, \\quad \\text{Var}(\\hat{p}) = \\mathbb{E}[\\hat{p}(Y)^2] - \\left(\\mathbb{E}[\\hat{p}(Y)]\\right)^2,\n$$\n其中期望是关于 $Y \\sim \\text{Binomial}(n,p)$ 的分布计算的。通过对 $y \\in \\{0,\\ldots,n\\}$ 求和来精确计算这些量（无需模拟）：\n$$\n\\mathbb{E}[\\hat{p}(Y)] = \\sum_{y=0}^{n} \\hat{p}(y) \\binom{n}{y} p^{y} (1-p)^{n-y}, \\quad \\mathbb{E}[\\hat{p}(Y)^2] = \\sum_{y=0}^{n} \\hat{p}(y)^2 \\binom{n}{y} p^{y} (1-p)^{n-y}.\n$$\n3. 通过计算标准最大似然估计 $\\hat{p}_{\\text{std}}(Y)$ 和带惩罚项的最大似然估计 $\\hat{p}_{\\lambda}(Y)$ 在以下测试集中的偏差和方差，分析由惩罚强度 $\\lambda$ 引起的偏差-方差权衡。不涉及物理单位，所有量都应表示为实数（浮点数）。不要使用百分号；所有概率都必须处理为 $[0,1]$ 区间内的小数。\n\n测试集：\n- 情况 A：$n=20$, $p=0.4$, $p_0=0.5$, $\\Lambda = \\{0.0, 0.5, 2.0\\}$。\n- 情况 B：$n=20$, $p=0.05$, $p_0=0.2$, $\\Lambda = \\{0.0, 1.0, 4.0\\}$。\n- 情况 C：$n=20$, $p=0.95$, $p_0=0.8$, $\\Lambda = \\{0.0, 1.0, 4.0\\}$。\n\n计算要求：\n- 对于每种情况，对于每个 $y \\in \\{0,\\ldots,n\\}$，通过在 $p \\in [0,1]$ 上最大化相应的目标函数来计算 $\\hat{p}_{\\text{std}}(y)$ 和 $\\hat{p}_{\\lambda}(y)$。标准最大似然估计通过最大化 $\\ell(p; y)$ 获得；带惩罚项的最大似然估计通过最大化 $\\ell_{\\lambda}(p; y)$ 获得。带惩罚项的目标函数是凹的，但在 $p=p_0$ 处不可导（非光滑），因此在 $p \\in [0,1]$ 上使用数值稳定的单峰搜索方法。\n- 使用二项概率质量函数，为每个估计量计算偏差（$\\text{Bias}$）、方差（$\\text{Var}$）和均方误差（$\\text{MSE} = \\text{Bias}^2 + \\text{Var}$）。\n\n最终输出规范：\n- 您的程序应生成单行输出，其中包含所有结果，形式为方括号内以逗号分隔的列表。\n- 对于每种情况（按 A, B, C 的顺序），首先输出标准最大似然估计的三个浮点数：$[\\text{Bias}_{\\text{std}}, \\text{Var}_{\\text{std}}, \\text{MSE}_{\\text{std}}]$，然后对于指定集合 $\\Lambda$ 中的每个 $\\lambda$（按给定顺序），输出带惩罚项估计量的三个对应浮点数：$[\\text{Bias}_{\\lambda}, \\text{Var}_{\\lambda}, \\text{MSE}_{\\lambda}]$。\n- 数字应格式化为小数。例如，输出结构为：\n$$\n[\\text{A-std-bias}, \\text{A-std-var}, \\text{A-std-mse}, \\text{A-}\\lambda_1\\text{-bias}, \\text{A-}\\lambda_1\\text{-var}, \\text{A-}\\lambda_1\\text{-mse}, \\ldots, \\text{C-}\\lambda_3\\text{-mse}].\n$$\n确保列表顺序与情况的顺序（A, B, C）完全匹配，并且在每种情况下，$\\lambda$ 的顺序与给定顺序一致。", "solution": "该问题要求对二项分布 $Y \\sim \\text{Binomial}(n, p)$ 的参数 $p$ 的带惩罚项的最大似然估计（MLE）进行统计分析。我们的任务是计算和比较标准最大似然估计和带惩罚项的最大似然估计的偏差、方差和均方误差（MSE）。该分析将针对一组特定的测试用例进行。\n\n### 步骤 1：估计量的定义\n\n首先，我们为给定的观测值 $Y=y$ 正式定义估计量。\n\n**1.1. 标准最大似然估计（MLE）**\n\n标准最大似然估计（记作 $\\hat{p}_{\\text{std}}(y)$）是在有效参数空间 $p \\in [0,1]$ 上使对数似然函数 $\\ell(p; y)$ 最大化的 $p$ 值。对数似然函数由下式给出：\n$$\n\\ell(p; y) = y \\log p + (n-y) \\log (1-p)\n$$\n对于 $p \\in (0,1)$，该函数是严格凹的。其唯一的最大化者可通过将其关于 $p$ 的导数设为零来找到：\n$$\n\\frac{\\partial \\ell(p; y)}{\\partial p} = \\frac{y}{p} - \\frac{n-y}{1-p} = 0\n$$\n求解 $p$ 可得 $y(1-p) = (n-y)p$，化简为 $y = np$，得出 $p = y/n$。\n对于边界情况 $y=0$ 和 $y=n$，对数似然函数分别为 $n \\log(1-p)$ 和 $n \\log p$。它们分别在 $p=0$ 和 $p=1$ 处最大化。因此，最大似然估计统一由以下公式给出：\n$$\n\\hat{p}_{\\text{std}}(y) = \\frac{y}{n} \\quad \\text{ for } y \\in \\{0, 1, \\ldots, n\\}\n$$\n作为一个随机变量，该估计量为 $\\hat{p}_{\\text{std}}(Y) = Y/n$。众所周知，该估计量是无偏的，即 $\\mathbb{E}[\\hat{p}_{\\text{std}}(Y)] = \\mathbb{E}[Y/n] = (np)/n = p$。\n\n**1.2. 带惩罚项的最大似然估计**\n\n带惩罚项的最大似然估计（记作 $\\hat{p}_{\\lambda}(y)$）在 $p \\in [0,1]$ 上最大化带惩罚项的目标函数 $\\ell_{\\lambda}(p; y)$：\n$$\n\\ell_{\\lambda}(p; y) = \\ell(p; y) - \\lambda \\lvert p - p_0 \\rvert\n$$\n其中 $\\lambda \\ge 0$ 是惩罚强度，$p_0 \\in [0,1]$ 是一个参考值。目标函数 $\\ell_{\\lambda}(p; y)$ 是一个严格凹函数 $\\ell(p; y)$ 和一个凹函数 $-\\lambda|p-p_0|$ 的和。因此，$\\ell_{\\lambda}(p; y)$ 是严格凹的，这保证了对于任何给定的 $y, n, \\lambda, p_0$，都存在唯一的最大化者。\n\n绝对值项 $|p - p_0|$ 使得目标函数在 $p=p_0$ 处不可微。虽然可以通过分析 $\\ell_{\\lambda}(p; y)$ 的次梯度来推导出解析解，但问题建议使用数值方法。由于目标函数是凹的，因此是单峰的，我们可以采用鲁棒的数值优化算法（如三分搜索或黄金分割搜索）来找到最大化者。\n\n惩罚项的作用是将标准估计 $\\hat{p}_{\\text{std}}(y) = y/n$ “收缩”到参考值 $p_0$。因此，最大化者 $\\hat{p}_{\\lambda}(y)$ 必须位于 $\\hat{p}_{\\text{std}}(y)$ 和 $p_0$ 之间的闭区间内。此属性允许我们将数值优化的搜索空间限制在 $[\\min(y/n, p_0), \\max(y/n, p_0)]$，这提高了数值稳定性和效率，特别是通过避免对数未定义的边界 $p=0$ 和 $p=1$（除非 $y/n$ 本身就是这些边界之一）。\n\n### 步骤 2：偏差、方差和 MSE 的计算\n\n估计量 $\\hat{p}(Y)$ 的统计特性是相对于真实的数据生成分布 $Y \\sim \\text{Binomial}(n, p)$ 进行评估的。我们计算偏差、方差和 MSE。\n\n任何函数 $g(Y)$ 的期望是通过对 $Y$ 的所有可能结果求和来计算的，并按其概率加权：\n$$\n\\mathbb{E}[g(Y)] = \\sum_{y=0}^{n} g(y) \\mathbb{P}(Y=y) = \\sum_{y=0}^{n} g(y) \\binom{n}{y} p^{y} (1-p)^{n-y}\n$$\n利用这一点，我们可以计算估计量 $\\hat{p}(Y)$ 的一阶矩和二阶矩：\n- 一阶矩：$\\mathbb{E}[\\hat{p}(Y)] = \\sum_{y=0}^{n} \\hat{p}(y) \\mathbb{P}(Y=y)$\n- 二阶矩：$\\mathbb{E}[\\hat{p}(Y)^2] = \\sum_{y=0}^{n} \\hat{p}(y)^2 \\mathbbP(Y=y)$\n\n然后，偏差、方差和 MSE 定义如下：\n- **偏差**：$\\text{Bias}(\\hat{p}) = \\mathbb{E}[\\hat{p}(Y)] - p$\n- **方差**：$\\text{Var}(\\hat{p}) = \\mathbb{E}[\\hat{p}(Y)^2] - \\left(\\mathbb{E}[\\hat{p}(Y)]\\right)^2$\n- **均方误差 (MSE)**：$\\text{MSE}(\\hat{p}) = \\text{Bias}(\\hat{p})^2 + \\text{Var}(\\hat{p})$\n\n### 步骤 3：计算过程\n\n获取测试集结果的总体过程如下：\n1.  对于由 $(n, p, p_0, \\Lambda)$ 定义的每个测试用例（A, B, C）：\n    a.  为 $y \\in \\{0, 1, \\ldots, n\\}$ 生成概率质量函数（PMF）值 $\\mathbb{P}(Y=y)$。\n    b.  首先，计算标准最大似然估计 $\\hat{p}_{\\text{std}}(Y)$ 的各项指标。估计值为 $\\hat{p}_{\\text{std}}(y) = y/n$。\n    c.  然后，对于集合 $\\Lambda$ 中的每个惩罚强度 $\\lambda$：\n        i.   通过在约束区间 $[\\min(y/n, p_0), \\max(y/n, p_0)]$ 上使用三分搜索对 $\\ell_{\\lambda}(p; y)$ 进行数值最大化，从而为所有 $y \\in \\{0, 1, \\ldots, n\\}$ 确定带惩罚项的估计值 $\\hat{p}_{\\lambda}(y)$。$\\lambda=0.0$ 的情况用于重现标准最大似然估计，以提供一致性检查。\n        ii.  使用计算出的估计值 $\\{\\hat{p}(y)\\}_{y=0}^n$ 和概率质量函数（PMF），计算 $\\mathbb{E}[\\hat{p}(Y)]$ 和 $\\mathbb{E}[\\hat{p}(Y)^2]$。\n        iii. 从这些矩，计算偏差、方差和 MSE。\n2.  将所有计算出的指标按指定顺序汇总到一个列表中。\n\n这种结构化方法使我们能够系统地评估 L1 式惩罚对估计量性能的影响，从而探索偏差-方差权衡。我们预计，随着 $\\lambda$ 的增加，偏差通常会增加（除非 $p_0$ 恰好等于 $p$），而方差会减小。平衡这两者的均方误差（MSE）可能会在 $\\lambda$ 较小时减小，然后再次增加。", "answer": "```python\nimport numpy as np\nfrom scipy.special import comb\n\ndef solve():\n    \"\"\"\n    Solves the problem of computing bias, variance, and MSE for standard and \n    penalized MLEs of a Binomial parameter.\n    \"\"\"\n    \n    test_cases = [\n        {'n': 20, 'p': 0.4, 'p0': 0.5, 'lambdas': [0.0, 0.5, 2.0]},\n        {'n': 20, 'p': 0.05, 'p0': 0.2, 'lambdas': [0.0, 1.0, 4.0]},\n        {'n': 20, 'p': 0.95, 'p0': 0.8, 'lambdas': [0.0, 1.0, 4.0]},\n    ]\n\n    def get_penalized_mle(y, n, lam, p0, tol=1e-12):\n        \"\"\"\n        Computes the penalized MLE for p for a given observation y.\n        \"\"\"\n        p_std = y / n\n        \n        # Handle the case where lambda is zero, which is the standard MLE.\n        if lam == 0.0:\n            return p_std\n\n        # Objective function to be maximized.\n        def objective(p, y, n, lam, p0):\n            if p < tol and y > 0:\n                return -np.inf\n            if p > 1 - tol and y < n:\n                return -np.inf\n\n            # Use x*log(y) logic to avoid 0*log(0) = NaN\n            log_p_term = 0 if y == 0 else y * np.log(p)\n            log_1_p_term = 0 if n == y else (n - y) * np.log(1 - p)\n            \n            log_lik = log_p_term + log_1_p_term\n            penalty = lam * abs(p - p0)\n            return log_lik - penalty\n\n        # The maximizer is known to lie between the standard MLE and p0.\n        low = min(p_std, p0)\n        high = max(p_std, p0)\n\n        # If the interval is trivial.\n        if abs(high - low) < tol:\n            return low\n\n        # Ternary search for the maximum of a unimodal function.\n        for _ in range(100): # 100 iterations are sufficient for high precision.\n            if high - low < tol:\n                break\n            m1 = low + (high - low) / 3\n            m2 = high - (high - low) / 3\n            if objective(m1, y, n, lam, p0) < objective(m2, y, n, lam, p0):\n                low = m1\n            else:\n                high = m2\n        \n        return (low + high) / 2\n\n    results = []\n    \n    for case in test_cases:\n        n, p_true, p0, lambdas = case['n'], case['p'], case['p0'], case['lambdas']\n        \n        ys = np.arange(n + 1)\n        # Pre-compute binomial probabilities\n        pmf = np.array([comb(n, y) * (p_true**y) * ((1 - p_true)**(n - y)) for y in ys])\n\n        # --- Standard MLE ---\n        phat_std_values = ys / n\n        \n        E_phat_std = np.sum(phat_std_values * pmf)\n        E_phat_std_sq = np.sum(phat_std_values**2 * pmf)\n        \n        bias_std = E_phat_std - p_true\n        var_std = E_phat_std_sq - E_phat_std**2\n        mse_std = bias_std**2 + var_std\n        \n        results.extend([bias_std, var_std, mse_std])\n        \n        # --- Penalized MLE for each lambda ---\n        for lam in lambdas:\n            phat_pen_values = np.array([get_penalized_mle(y, n, lam, p0) for y in ys])\n            \n            E_phat_pen = np.sum(phat_pen_values * pmf)\n            E_phat_pen_sq = np.sum(phat_pen_values**2 * pmf)\n\n            bias_pen = E_phat_pen - p_true\n            var_pen = E_phat_pen_sq - E_phat_pen**2\n            mse_pen = bias_pen**2 + var_pen\n            \n            results.extend([bias_pen, var_pen, mse_pen])\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3116263"}, {"introduction": "最后一个练习将泊松分布应用于计数数据建模，这是从生物学到金融学等众多领域的常见任务。你将亲手实现“迭代重加权最小二乘”（Iteratively Reweighted Least Squares, IRLS）算法来拟合泊松回归模型，并进一步探索一个关键的现实挑战：模型设定错误。通过生成具有“过度离散”（over-dispersion）的数据并用一个未能考虑到此特性的模型进行拟合，你将量化这种设定错误对预测精度的影响，从而理解简单的泊松模型在何种情况下可能是不够的。[@problem_id:3116193]", "problem": "您将执行一个离散数据建模任务，该任务基于泊松分布和负二项分布的核心定义，以及用于预测的条件期望的基本性质。目标是构建合成数据集，其中计数的条件均值由特征的对数线性函数决定，但真实的数据生成过程表现出与负二项模型一致的过度离散现象。然后，您必须在错误的方差假设下拟合一个泊松回归模型，并量化其相对于神谕预测器的样本外预测误差。\n\n请使用以下原则作为您推导和实现的起点：\n- 率参数为 $\\lambda$ 的泊松分布，其概率质量函数为 $P(N = k \\mid \\lambda) = \\exp(-\\lambda)\\lambda^{k}/k!$（对于 $k \\in \\{0,1,2,\\dots\\}$），均值为 $E[N \\mid \\lambda] = \\lambda$，方差为 $\\operatorname{Var}(N \\mid \\lambda) = \\lambda$。\n- 负二项分布可以表示为泊松-伽马混合分布：如果 $\\Lambda \\sim \\operatorname{Gamma}(\\text{shape}=\\theta,\\text{scale}=\\mu/\\theta)$ 且 $N \\mid \\Lambda \\sim \\operatorname{Poisson}(\\Lambda)$，则 $N$ 服从负二项分布，其均值为 $E[N] = \\mu$，方差为 $\\operatorname{Var}(N) = \\mu + \\mu^{2}/\\theta$。当 $\\theta$ 为有限值时，这捕捉了相对于泊松模型的过度离散现象。\n- 在平方误差损失下进行预测时，条件期望 $E[N \\mid X]$ 是最小化均方预测误差的神谕预测器。\n\n您的程序必须：\n1. 对于每个指定的测试用例 $t$，根据以下过程生成一个训练数据集 $\\{(x_{i}, n_{i})\\}_{i=1}^{n_{\\text{train}}}$ 和一个独立的测试数据集 $\\{(x_{j}^{\\ast}, n_{j}^{\\ast})\\}_{j=1}^{n_{\\text{test}}}$。\n   - 特征：设 $p$ 为非截距项特征的数量。通过将截距项 $x_{i,0} = 1$ 与 $p$ 个独立的标准正态坐标 $x_{i,1},\\dots,x_{i,p} \\sim \\mathcal{N}(0,1)$ 拼接起来，构建 $x_{i} \\in \\mathbb{R}^{p+1}$。在测试集上对 $x_{j}^{\\ast}$ 执行同样的操作。\n   - 均值函数：设 $\\beta \\in \\mathbb{R}^{p+1}$ 为一个固定值。定义条件均值 $\\mu_{i} = \\exp(\\beta^{\\top} x_{i})$ 和 $\\mu_{j}^{\\ast} = \\exp(\\beta^{\\top} x_{j}^{\\ast})$。\n   - 真实计数（负二项分布）：对每个 $i$，抽取一个潜在率 $\\lambda_{i} \\sim \\operatorname{Gamma}(\\text{shape}=\\theta, \\text{scale}=\\mu_{i}/\\theta)$，然后抽取计数 $n_{i} \\sim \\operatorname{Poisson}(\\lambda_{i})$。对每个 $j$，抽取 $\\lambda_{j}^{\\ast} \\sim \\operatorname{Gamma}(\\text{shape}=\\theta, \\text{scale}=\\mu_{j}^{\\ast}/\\theta)$，然后抽取 $n_{j}^{\\ast} \\sim \\operatorname{Poisson}(\\lambda_{j}^{\\ast})$。\n   - 在每个用例中，必须通过为训练集使用提供的种子 $s$ 和为测试集使用种子 $s+1000$ 来确保随机性是可复现的。\n2. 使用规范对数链接函数，通过最大似然法将泊松回归模型拟合到训练数据。具体来说，将 $n_{i}$ 视为 $n_{i} \\sim \\operatorname{Poisson}(\\tilde{\\mu}_{i})$，其中 $\\tilde{\\mu}_{i} = \\exp(\\tilde{\\beta}^{\\top} x_{i})$，并通过求解泊松对数似然函数所隐含的一阶最优性条件来计算最大似然估计 $\\hat{\\beta}$。您必须实现一个基于从梯度和 Hessian 矩阵推导出的迭代重加权最小二乘法的数值方法；不允许使用外部的广义线性模型 (GLM) 库。允许使用标准库或指定的数值库中的线性代数程序。\n3. 在测试集上，计算以下量：\n   - 泊松拟合预测器 $\\hat{\\mu}_{j} = \\exp(\\hat{\\beta}^{\\top} x_{j}^{\\ast})$ 及其均方误差 $\\operatorname{MSE}_{\\text{P}} = \\frac{1}{n_{\\text{test}}}\\sum_{j=1}^{n_{\\text{test}}} (n_{j}^{\\ast} - \\hat{\\mu}_{j})^{2}$。\n   - 神谕预测器 $\\mu_{j}^{\\ast} = \\exp(\\beta^{\\top} x_{j}^{\\ast})$ 及其均方误差 $\\operatorname{MSE}_{\\text{O}} = \\frac{1}{n_{\\text{test}}}\\sum_{j=1}^{n_{\\text{test}}} (n_{j}^{\\ast} - \\mu_{j}^{\\ast})^{2}$。\n   - 误设误差比 $R = \\operatorname{MSE}_{\\text{P}} / \\operatorname{MSE}_{\\text{O}}$。\n4. 生成最终输出，为单行文本，包含对应于以下测试用例的比率列表 $[R_{1}, R_{2}, R_{3}]$，四舍五入到六位小数，不含其他文本。\n\n测试套件规范：\n- 用例 1（中度过度离散，正常情况）：$n_{\\text{train}} = 500$, $n_{\\text{test}} = 5000$, $p = 3$, $\\beta = [0.2, 0.5, -0.3, 0.1]$（第一个元素是截距），$\\theta = 2.0$，种子 $s = 2025$。\n- 用例 2（接近泊松分布的方差，边界条件）：$n_{\\text{train}} = 500$, $n_{\\text{test}} = 5000$, $p = 3$, $\\beta = [0.2, 0.5, -0.3, 0.1]$, $\\theta = 50.0$，种子 $s = 2026$。\n- 用例 3（高度过度离散且训练集较小，边缘情况）：$n_{\\text{train}} = 200$, $n_{\\text{test}} = 5000$, $p = 3$, $\\beta = [0.2, 0.5, -0.3, 0.1]$, $\\theta = 0.5$，种子 $s = 2027$。\n\n最终输出格式：\n- 您的程序应生成一行输出，其中包含一个用方括号括起来的逗号分隔的比率列表，并四舍五入到六位小数，例如 `[1.234567,1.000001,2.500000]`。", "solution": "提出的问题要求构建一个模拟研究，以评估一个误设的泊松回归模型的预测性能。真实的数据生成过程遵循负二项分布，该分布相对于泊松模型表现出过度离散。评估是通过比较拟合模型的均方误差（MSE）与神谕预测器的均方误差来进行的，后者可以访问真实的底层数据生成参数。该解决方案包括三个主要阶段：数据生成、通过迭代重加权最小二乘法（IRLS）进行模型拟合以及性能评估。\n\n### 1. 数据生成过程\n\n对于每个测试用例，我们生成一个训练集 $\\{(x_{i}, n_{i})\\}_{i=1}^{n_{\\text{train}}}$ 和一个测试集 $\\{(x_{j}^{\\ast}, n_{j}^{\\ast})\\}_{j=1}^{n_{\\text{test}}}$。\n\n**特征**：特征向量 $x_i \\in \\mathbb{R}^{p+1}$ 是通过将一个截距项 $x_{i,0} = 1$ 与 $p$ 个从标准正态分布中独立抽取的特征坐标 $x_{i,k} \\sim \\mathcal{N}(0, 1)$（对于 $k=1, \\dots, p$）拼接而成的。训练集和测试集的设计矩阵分别表示为 $X_{\\text{train}}$ 和 $X_{\\text{test}}$。\n\n**计数**：计数数据 $n_i$ 是使用指定的泊松-伽马混合公式从负二项分布中生成的。此过程正确地建模了一个条件均值 $\\mu = E[N \\mid X]$，同时引入了过度离散。\n首先，对于给定的特征向量 $x_i$，条件均值 $\\mu_i$ 由一个对数线性模型定义：\n$$ \\mu_i = \\exp(\\beta^{\\top} x_i) $$\n其中 $\\beta$ 是真实回归系数向量。\n\n然后，负二项计数分两步生成：\n1.  从一个伽马分布中抽取一个潜在率变量 $\\lambda_i$：\n    $$ \\lambda_i \\sim \\operatorname{Gamma}(\\text{shape}=a, \\text{scale}=s) $$\n    其中形状参数为 $a = \\theta$，尺度参数为 $s = \\mu_i / \\theta$。此伽马分布的均值为 $E[\\lambda_i] = a \\cdot s = \\theta (\\mu_i / \\theta) = \\mu_i$。\n2.  从一个以潜在率 $\\lambda_i$ 为参数的泊松分布中抽取观测到的计数 $n_i$：\n    $$ n_i \\mid \\lambda_i \\sim \\operatorname{Poisson}(\\lambda_i) $$\n$n_i$ 的边际分布是负二项分布，其均值为 $E[n_i] = E[E[n_i \\mid \\lambda_i]] = E[\\lambda_i] = \\mu_i$，方差为 $\\operatorname{Var}(n_i) = E[\\operatorname{Var}(n_i \\mid \\lambda_i)] + \\operatorname{Var}(E[n_i \\mid \\lambda_i]) = E[\\lambda_i] + \\operatorname{Var}(\\lambda_i) = \\mu_i + \\theta (\\mu_i / \\theta)^2 = \\mu_i + \\mu_i^2 / \\theta$。项 $\\mu_i^2 / \\theta$ 表示过度离散；当 $\\theta \\to \\infty$ 时，方差接近均值 $\\mu_i$，负二项分布收敛于泊松分布。\n\n随机种子 $s$ 和 $s+1000$ 分别用于训练集和测试集，以确保生成数据的可复现性。\n\n### 2. 通过迭代重加权最小二乘法（IRLS）进行泊松回归\n\n将一个泊松回归模型拟合到训练数据 $\\{(x_i, n_i)\\}$。这个模型是误设的，因为它假设 $n_i \\sim \\operatorname{Poisson}(\\tilde{\\mu}_i)$ 且 $\\operatorname{Var}(n_i) = \\tilde{\\mu}_i$，而实际上数据是过度离散的。模型的链接函数是对数函数：$\\tilde{\\mu}_i = \\exp(x_i^\\top \\tilde{\\beta})$。参数向量 $\\tilde{\\beta}$ 通过最大化泊松对数似然函数来估计，对于大小为 $n_{\\text{train}}$ 的整个训练集，该函数为：\n$$ \\ell(\\tilde{\\beta}) = \\sum_{i=1}^{n_{\\text{train}}} \\left( n_i \\log(\\tilde{\\mu}_i) - \\tilde{\\mu}_i - \\log(n_i!) \\right) = \\sum_{i=1}^{n_{\\text{train}}} \\left( n_i (x_i^\\top \\tilde{\\beta}) - \\exp(x_i^\\top \\tilde{\\beta}) - \\log(n_i!) \\right) $$\n最大似然估计 $\\hat{\\beta}$ 是通过求解 $\\nabla_{\\tilde{\\beta}} \\ell(\\tilde{\\beta}) = 0$ 找到的。这是通过一种基于 Newton-Raphson 的算法来完成的，该算法被称为 Fisher 得分法，或在此背景下称为迭代重加权最小二乘法（IRLS）。\n\n对数似然的梯度（得分向量）是：\n$$ g(\\tilde{\\beta}) = \\nabla_{\\tilde{\\beta}} \\ell(\\tilde{\\beta}) = \\sum_{i=1}^{n_{\\text{train}}} (n_i - \\tilde{\\mu}_i) x_i = X_{\\text{train}}^\\top (n - \\tilde{\\mu}) $$\nHessian 矩阵是：\n$$ H(\\tilde{\\beta}) = \\nabla_{\\tilde{\\beta}}^2 \\ell(\\tilde{\\beta}) = -\\sum_{i=1}^{n_{\\text{train}}} \\tilde{\\mu}_i x_i x_i^\\top = -X_{\\text{train}}^\\top W X_{\\text{train}} $$\n其中 $W$ 是一个对角权重矩阵，其元素为 $W_{ii} = \\tilde{\\mu}_i$。在第 $k$ 次迭代中的 Newton-Raphson 更新是：\n$$ \\tilde{\\beta}^{(k+1)} = \\tilde{\\beta}^{(k)} - [H(\\tilde{\\beta}^{(k)})]^{-1} g(\\tilde{\\beta}^{(k)}) = \\tilde{\\beta}^{(k)} + (X_{\\text{train}}^\\top W^{(k)} X_{\\text{train}})^{-1} X_{\\text{train}}^\\top (n - \\tilde{\\mu}^{(k)}) $$\n这可以重新排列成 IRLS 形式。通过定义一个“工作响应”向量 $z^{(k)}$，其元素为：\n$$ z_i^{(k)} = (x_i^\\top \\tilde{\\beta}^{(k)}) + \\frac{n_i - \\tilde{\\mu}_i^{(k)}}{\\tilde{\\mu}_i^{(k)}} $$\n更新步骤就变成了一个加权最小二乘解：\n$$ \\tilde{\\beta}^{(k+1)} = (X_{\\text{train}}^\\top W^{(k)} X_{\\text{train}})^{-1} X_{\\text{train}}^\\top W^{(k)} z^{(k)} $$\n算法过程如下：\n1. 初始化 $\\tilde{\\beta}^{(0)}$，例如初始化为零向量。\n2. 对于 $k = 0, 1, 2, \\dots$ 直到收敛：\n   a. 计算线性预测器 $\\eta^{(k)} = X_{\\text{train}} \\tilde{\\beta}^{(k)}$。\n   b. 计算均值 $\\tilde{\\mu}^{(k)} = \\exp(\\eta^{(k)})$。\n   c. 构成权重矩阵 $W^{(k)} = \\operatorname{diag}(\\tilde{\\mu}_i^{(k)})$。\n   d. 计算工作响应 $z^{(k)} = \\eta^{(k)} + (n - \\tilde{\\mu}^{(k)}) / \\tilde{\\mu}^{(k)}$。\n   e. 求解加权最小二乘系统以得到 $\\tilde{\\beta}^{(k+1)}$。\n3. 收敛后的向量即为最大似然估计，表示为 $\\hat{\\beta}$。\n\n### 3. 性能评估\n\n预测性能在独立的测试集上进行评估。在平方误差损失下，最优预测器（神谕）是给定特征的计数的条件期望，即真实均值函数 $\\mu_j^{\\ast} = \\exp(\\beta^\\top x_j^\\ast)$。\n\n计算以下量：\n- **神谕预测器**：$\\mu_j^{\\ast} = \\exp(\\beta^\\top x_j^\\ast)$。\n- **神谕均方误差**：$\\operatorname{MSE}_{\\text{O}} = \\frac{1}{n_{\\text{test}}}\\sum_{j=1}^{n_{\\text{test}}} (n_j^{\\ast} - \\mu_j^{\\ast})^2$。这衡量了由负二项过程的内在随机性（方差）导致的不可约误差。\n- **泊松拟合预测器**：$\\hat{\\mu}_j = \\exp(\\hat{\\beta}^\\top x_j^{\\ast})$，使用从训练集估计出的 $\\hat{\\beta}$。\n- **泊松拟合均方误差**：$\\operatorname{MSE}_{\\text{P}} = \\frac{1}{n_{\\text{test}}}\\sum_{j=1}^{n_{\\text{test}}} (n_j^{\\ast} - \\hat{\\mu}_j)^2$。该误差既包括不可约误差，也包括使用一个估计的、误设的模型带来的额外误差。\n- **误设误差比**：$R = \\operatorname{MSE}_{\\text{P}} / \\operatorname{MSE}_{\\text{O}}$。该比率量化了预测准确性的下降程度。接近 1 的值表示误设模型的性能几乎与神谕预测器一样好，而大于 1 的值则表示性能有所损失。\n\n对问题陈述中指定的三个测试用例中的每一个都计算此比率。", "answer": "```python\nimport numpy as np\n\ndef fit_poisson_regression(X, n, max_iter=50, tol=1e-8):\n    \"\"\"\n    Fits a Poisson regression model using Iteratively Reweighted Least Squares (IRLS).\n\n    Args:\n        X (np.ndarray): The design matrix (n_samples, n_features).\n        n (np.ndarray): The vector of observed counts (n_samples,).\n        max_iter (int): Maximum number of iterations for the IRLS algorithm.\n        tol (float): Convergence tolerance for the change in the beta vector norm.\n\n    Returns:\n        np.ndarray: The estimated coefficient vector beta_hat.\n    \"\"\"\n    # Initialize beta coefficients to zero\n    beta = np.zeros(X.shape[1])\n    \n    for k in range(max_iter):\n        beta_old = beta.copy()\n        \n        # Calculate linear predictor and mean\n        eta = X @ beta\n        mu = np.exp(eta)\n        \n        # Calculate weights and working response\n        # W is a diagonal matrix, but we can use element-wise multiplication for efficiency\n        weights = mu\n        z = eta + (n - mu) / mu\n        \n        # Solve the weighted least squares system: beta = (X'WX)^-1 * X'Wz\n        # Using np.linalg.solve for numerical stability\n        A = X.T @ (weights[:, np.newaxis] * X)\n        b = X.T @ (weights * z)\n        \n        try:\n            beta = np.linalg.solve(A, b)\n        except np.linalg.LinAlgError:\n            # If matrix is singular, return the last valid beta\n            # This might happen in edge cases with sparse data\n            return beta_old\n            \n        # Check for convergence\n        if np.linalg.norm(beta - beta_old) < tol:\n            break\n            \n    return beta\n\ndef generate_data(n_samples, p, beta, theta, seed):\n    \"\"\"\n    Generates synthetic data according to the Negative Binomial process.\n\n    Args:\n        n_samples (int): The number of data points to generate.\n        p (int): The number of non-intercept features.\n        beta (np.ndarray): The true coefficient vector.\n        theta (float): The dispersion parameter for the Gamma distribution.\n        seed (int): The random seed for reproducibility.\n\n    Returns:\n        tuple[np.ndarray, np.ndarray, np.ndarray]: X, n, mu_true\n            - X: design matrix\n            - n: generated counts\n            - mu_true: true conditional means\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    \n    # Generate features X: intercept + N(0,1) features\n    X0 = np.ones((n_samples, 1))\n    X_features = rng.standard_normal(size=(n_samples, p))\n    X = np.hstack((X0, X_features))\n    \n    # Calculate true conditional mean mu\n    mu_true = np.exp(X @ beta)\n    \n    # Generate counts from Negative Binomial (Poisson-Gamma mixture)\n    # 1. Draw lambda from Gamma(shape=theta, scale=mu/theta)\n    shape = theta\n    scale = mu_true / theta\n    lambdas = rng.gamma(shape, scale, size=n_samples)\n    \n    # 2. Draw n from Poisson(lambda)\n    n = rng.poisson(lambdas)\n    \n    return X, n, mu_true\n\ndef run_simulation_case(n_train, n_test, p, beta_list, theta, seed):\n    \"\"\"\n    Runs the full simulation for one test case.\n\n    Args:\n        n_train (int): Size of the training set.\n        n_test (int): Size of the test set.\n        p (int): Number of non-intercept features.\n        beta_list (list): The true coefficient vector as a list.\n        theta (float): The dispersion parameter.\n        seed (int): The base random seed.\n\n    Returns:\n        float: The misspecification error ratio R = MSE_P / MSE_O.\n    \"\"\"\n    beta_true = np.array(beta_list)\n    \n    # 1. Generate training and test data\n    X_train, n_train, _ = generate_data(n_train, p, beta_true, theta, seed)\n    X_test, n_test, mu_oracle = generate_data(n_test, p, beta_true, theta, seed + 1000)\n    \n    # 2. Fit Poisson regression model on training data\n    beta_hat = fit_poisson_regression(X_train, n_train)\n    \n    # 3. Evaluate on the test set\n    # Predictions from the fitted misspecified Poisson model\n    mu_hat_p = np.exp(X_test @ beta_hat)\n    \n    # Mean Squared Error for the Poisson-fit model\n    mse_p = np.mean((n_test - mu_hat_p)**2)\n    \n    # Mean Squared Error for the oracle predictor\n    mse_o = np.mean((n_test - mu_oracle)**2)\n    \n    # Calculate the misspecification error ratio\n    ratio = mse_p / mse_o\n    \n    return ratio\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the results.\n    \"\"\"\n    test_cases = [\n        # (n_train, n_test, p, beta, theta, seed)\n        (500, 5000, 3, [0.2, 0.5, -0.3, 0.1], 2.0, 2025), # Case 1\n        (500, 5000, 3, [0.2, 0.5, -0.3, 0.1], 50.0, 2026), # Case 2\n        (200, 5000, 3, [0.2, 0.5, -0.3, 0.1], 0.5, 2027), # Case 3\n    ]\n    \n    results = []\n    for case in test_cases:\n        ratio = run_simulation_case(*case)\n        results.append(ratio)\n    \n    # Format the results to exactly six decimal places\n    formatted_results = [f\"{r:.6f}\" for r in results]\n    \n    # Print the final output in the required format\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n\n```", "id": "3116193"}]}