{"hands_on_practices": [{"introduction": "理论概念在应用于现实世界数据时，其鲁棒性至关重要。此练习通过一个简化的场景，探讨了当特征矩阵受到特定结构性扰动（即秩一扰动）时，模型的关键属性会如何变化。通过这个练习，你将亲手计算这种扰动对特征矩阵谱范数和二次损失函数的影响，从而直观地理解数据误差如何影响模型。更重要的是，你将学习如何精确地调整正则化参数，以抵消这种特定的数据损坏，从而深刻体会正则化在增强模型稳定性方面的作用 [@problem_id:3146460]。", "problem": "考虑一个监督学习场景，其中特征矩阵为 $X \\in \\mathbb{R}^{2 \\times 2}$，响应向量为 $y \\in \\mathbb{R}^{2}$。您将分析特征的秩一扰动如何影响谱范数和二次损失，然后推导出一个正则化选择，以减轻其主导方向上的扰动。\n\n使用以下基本定义作为您的出发点：\n- 矩阵 $A$ 的谱范数 $\\|A\\|_{2}$ 是 $A$ 的最大奇异值，等价于 $\\|A\\|_{2} = \\sqrt{\\lambda_{\\max}(A^{\\top}A)}$，其中 $\\lambda_{\\max}(\\cdot)$ 表示最大特征值。\n- 对于权重 $w \\in \\mathbb{R}^{2}$，经验二次损失为 $L(w) = \\frac{1}{2}\\|Xw - y\\|_{2}^{2}$。\n- 普通最小二乘法 (OLS) 求解 $\\min_{w} \\frac{1}{2}\\|Xw - y\\|_{2}^{2}$。\n- 岭回归求解 $\\min_{w} \\frac{1}{2}\\|Xw - y\\|_{2}^{2} + \\frac{\\lambda}{2}\\|w\\|_{2}^{2}$，其中 $\\lambda > 0$。\n\n设干净数据为\n$$\nX = \\begin{pmatrix} 3  0 \\\\ 0  1 \\end{pmatrix}, \\quad y = \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix}.\n$$\n假设特征被形式为 $\\Delta X = u v^{\\top}$ 的秩一扰动所损坏，其中\n$$\nu = \\begin{pmatrix} a \\\\ 0 \\end{pmatrix}, \\quad v = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}, \\quad a \\geq 0,\n$$\n使得 $\\Delta X = \\begin{pmatrix} a  0 \\\\ 0  0 \\end{pmatrix}$，被扰动的特征矩阵为 $X_{\\text{cor}} = X + \\Delta X$。\n\n任务：\n1. 仅使用谱范数的定义，计算由扰动引起的谱范数的精确变化量，即求 $\\|X_{\\text{cor}}\\|_{2} - \\|X\\|_{2}$，并表示为关于 $a$ 的表达式。\n2. 设 $w_{\\text{OLS}}$ 表示在干净数据 $(X,y)$ 上训练得到的 OLS 解。当在被扰动的特征上评估 $w_{\\text{OLS}}$ 时，计算经验二次损失的精确变化量，即求 $L_{\\text{cor}}(w_{\\text{OLS}}) - L(w_{\\text{OLS}})$，并表示为关于 $a$ 的表达式，其中 $L_{\\text{cor}}(w) = \\frac{1}{2}\\|X_{\\text{cor}}w - y\\|_{2}^{2}$。\n3. 在干净数据上训练岭回归，得到作为 $\\lambda$ 函数的 $w_{\\lambda}$。推导出一个正则化参数 $\\lambda$ 的值（作为 $a$ 的函数），当在被扰动的特征上评估 $w_{\\lambda}$ 时，该值能完全抵消残差向量第一个坐标中的扰动，即选择 $\\lambda$ 使得 $(X_{\\text{cor}}w_{\\lambda} - y)$ 的第一个元素恰好为零。\n\n将任务 1-3 的最终结果以关于 $a$ 的精确表达式形式，按顺序（谱范数变化量、二次损失变化量、以及用于抵消扰动的正则化参数）在一个单行矩阵中报告。无需四舍五入。最终答案不带单位。", "solution": "该问题陈述是适定的、数学上一致的，并且基于线性代数和统计学习的原理。我们将按指定顺序解决这三个任务。\n\n设给定的矩阵和向量为：\n$$\nX = \\begin{pmatrix} 3  0 \\\\ 0  1 \\end{pmatrix}, \\quad y = \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix}\n$$\n扰动由 $\\Delta X = \\begin{pmatrix} a  0 \\\\ 0  0 \\end{pmatrix}$（其中 $a \\geq 0$）给出。被扰动的矩阵是：\n$$\nX_{\\text{cor}} = X + \\Delta X = \\begin{pmatrix} 3  0 \\\\ 0  1 \\end{pmatrix} + \\begin{pmatrix} a  0 \\\\ 0  0 \\end{pmatrix} = \\begin{pmatrix} 3+a  0 \\\\ 0  1 \\end{pmatrix}\n$$\n\n**任务1：谱范数的变化**\n\n矩阵 $A$ 的谱范数定义为 $\\|A\\|_{2} = \\sqrt{\\lambda_{\\max}(A^{\\top}A)}$。\n\n首先，我们计算干净矩阵 $X$ 的谱范数。\n$$\nX^{\\top}X = \\begin{pmatrix} 3  0 \\\\ 0  1 \\end{pmatrix} \\begin{pmatrix} 3  0 \\\\ 0  1 \\end{pmatrix} = \\begin{pmatrix} 9  0 \\\\ 0  1 \\end{pmatrix}\n$$\n这个对角矩阵的特征值是其对角线元素，$\\lambda_1 = 9$ 和 $\\lambda_2 = 1$。最大特征值是 $\\lambda_{\\max}(X^{\\top}X) = 9$。\n因此，$X$ 的谱范数是：\n$$\n\\|X\\|_{2} = \\sqrt{9} = 3\n$$\n\n接下来，我们计算被扰动的矩阵 $X_{\\text{cor}}$ 的谱范数。\n$$\nX_{\\text{cor}}^{\\top}X_{\\text{cor}} = \\begin{pmatrix} 3+a  0 \\\\ 0  1 \\end{pmatrix} \\begin{pmatrix} 3+a  0 \\\\ 0  1 \\end{pmatrix} = \\begin{pmatrix} (3+a)^2  0 \\\\ 0  1 \\end{pmatrix}\n$$\n特征值为 $(3+a)^2$ 和 $1$。由于 $a \\geq 0$，我们有 $3+a \\geq 3$，因此 $(3+a)^2 \\geq 9$。最大特征值是 $\\lambda_{\\max}(X_{\\text{cor}}^{\\top}X_{\\text{cor}}) = (3+a)^2$。\n因此，$X_{\\text{cor}}$ 的谱范数是：\n$$\n\\|X_{\\text{cor}}\\|_{2} = \\sqrt{(3+a)^2} = |3+a| = 3+a\n$$\n谱范数的变化量是：\n$$\n\\|X_{\\text{cor}}\\|_{2} - \\|X\\|_{2} = (3+a) - 3 = a\n$$\n\n**任务2：二次损失的变化**\n\n首先，我们求解干净数据 $(X, y)$ 的普通最小二乘 (OLS) 解 $w_{\\text{OLS}}$。OLS 解最小化 $L(w) = \\frac{1}{2}\\|Xw - y\\|_{2}^{2}$，并由 $w_{\\text{OLS}} = (X^{\\top}X)^{-1}X^{\\top}y$ 给出。由于 $X$ 是可逆的，这可以简化为 $w_{\\text{OLS}} = X^{-1}y$。\n$$\nX^{-1} = \\begin{pmatrix} \\frac{1}{3}  0 \\\\ 0  1 \\end{pmatrix}\n$$\n$$\nw_{\\text{OLS}} = \\begin{pmatrix} \\frac{1}{3}  0 \\\\ 0  1 \\end{pmatrix} \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}\n$$\n该解在干净数据上的损失为：\n$$\nL(w_{\\text{OLS}}) = \\frac{1}{2}\\|Xw_{\\text{OLS}} - y\\|_{2}^{2} = \\frac{1}{2}\\left\\|\\begin{pmatrix} 3  0 \\\\ 0  1 \\end{pmatrix}\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} - \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix}\\right\\|_{2}^{2} = \\frac{1}{2}\\left\\|\\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix} - \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix}\\right\\|_{2}^{2} = \\frac{1}{2}\\|\\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}\\|_{2}^{2} = 0\n$$\n现在，我们在被扰动的数据上评估 $w_{\\text{OLS}}$ 的损失，$L_{\\text{cor}}(w_{\\text{OLS}}) = \\frac{1}{2}\\|X_{\\text{cor}}w_{\\text{OLS}} - y\\|_{2}^{2}$。\n$$\nL_{\\text{cor}}(w_{\\text{OLS}}) = \\frac{1}{2}\\left\\|\\begin{pmatrix} 3+a  0 \\\\ 0  1 \\end{pmatrix}\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} - \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix}\\right\\|_{2}^{2} = \\frac{1}{2}\\left\\|\\begin{pmatrix} 3+a \\\\ 1 \\end{pmatrix} - \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix}\\right\\|_{2}^{2} = \\frac{1}{2}\\left\\|\\begin{pmatrix} a \\\\ 0 \\end{pmatrix}\\right\\|_{2}^{2} = \\frac{1}{2}(a^2) = \\frac{a^2}{2}\n$$\n经验二次损失的变化量是：\n$$\nL_{\\text{cor}}(w_{\\text{OLS}}) - L(w_{\\text{OLS}}) = \\frac{a^2}{2} - 0 = \\frac{a^2}{2}\n$$\n\n**任务3：用于抵消扰动的正则化参数**\n\n我们首先求出在干净数据上训练得到的岭回归解 $w_{\\lambda}$，其由 $w_{\\lambda} = (X^{\\top}X + \\lambda I)^{-1}X^{\\top}y$ 给出。\n我们已经计算出 $X^{\\top}X = \\begin{pmatrix} 9  0 \\\\ 0  1 \\end{pmatrix}$。我们还需要 $X^{\\top}y$：\n$$\nX^{\\top}y = \\begin{pmatrix} 3  0 \\\\ 0  1 \\end{pmatrix} \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 9 \\\\ 1 \\end{pmatrix}\n$$\n现在我们构建要求逆的项：\n$$\nX^{\\top}X + \\lambda I = \\begin{pmatrix} 9  0 \\\\ 0  1 \\end{pmatrix} + \\begin{pmatrix} \\lambda  0 \\\\ 0  \\lambda \\end{pmatrix} = \\begin{pmatrix} 9+\\lambda  0 \\\\ 0  1+\\lambda \\end{pmatrix}\n$$\n其逆矩阵是：\n$$\n(X^{\\top}X + \\lambda I)^{-1} = \\begin{pmatrix} \\frac{1}{9+\\lambda}  0 \\\\ 0  \\frac{1}{1+\\lambda} \\end{pmatrix}\n$$\n于是岭回归解为：\n$$\nw_{\\lambda} = \\begin{pmatrix} \\frac{1}{9+\\lambda}  0 \\\\ 0  \\frac{1}{1+\\lambda} \\end{pmatrix} \\begin{pmatrix} 9 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} \\frac{9}{9+\\lambda} \\\\ \\frac{1}{1+\\lambda} \\end{pmatrix}\n$$\n我们需要找到 $\\lambda$，使得残差向量 $(X_{\\text{cor}}w_{\\lambda} - y)$ 的第一个元素为零。设该残差为 $r_{\\text{cor}}$。\n$$\nr_{\\text{cor}} = X_{\\text{cor}}w_{\\lambda} - y = \\begin{pmatrix} 3+a  0 \\\\ 0  1 \\end{pmatrix} \\begin{pmatrix} \\frac{9}{9+\\lambda} \\\\ \\frac{1}{1+\\lambda} \\end{pmatrix} - \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} (3+a)\\frac{9}{9+\\lambda} \\\\ \\frac{1}{1+\\lambda} \\end{pmatrix} - \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} \\frac{9(3+a)}{9+\\lambda} - 3 \\\\ \\frac{1}{1+\\lambda} - 1 \\end{pmatrix}\n$$\n将第一个元素设为零：\n$$\n\\frac{9(3+a)}{9+\\lambda} - 3 = 0\n$$\n$$\n9(3+a) = 3(9+\\lambda)\n$$\n两边除以 3：\n$$\n3(3+a) = 9+\\lambda\n$$\n$$\n9 + 3a = 9 + \\lambda\n$$\n求解 $\\lambda$ 可得：\n$$\n\\lambda = 3a\n$$\n这个 $\\lambda$ 值确保了在残差的第一个坐标中，扰动的影响被抵消。对于任何 $a>0$，岭回归的条件 $\\lambda > 0$ 都得到满足。如果 $a=0$，则 $\\lambda=0$，这对应于残差已经为零的 OLS 情况。", "answer": "$$\n\\boxed{\\begin{pmatrix} a  \\frac{a^2}{2}  3a \\end{pmatrix}}\n$$", "id": "3146460"}, {"introduction": "在掌握了基本概念后，我们将目光投向一个在现代深度学习中无处不在的技术——批量归一化（Batch Normalization, BN）。BN旨在稳定神经网络的训练过程，但其具体效果如何？我们可以运用矩阵范数和二次型作为强大的分析工具来一探究竟。此练习要求你分析BN变换如何重塑特征的协方差结构，通过计算协方差矩阵的谱范数来量化特征空间的缩放效应，并利用二次型来考察模型在特定方向上的方差变化。通过这项实践，你将学会如何利用这些数学工具来诊断和监控复杂的机器学习训练动态，从而将抽象理论与前沿实践联系起来 [@problem_id:3146421]。", "problem": "考虑一个小批量特征矩阵 $X \\in \\mathbb{R}^{n \\times d}$，其行向量为 $x_i^\\top \\in \\mathbb{R}^d$。在统计学习中，批归一化 (Batch Normalization, BN) 使用批次统计数据对每个特征进行变换。设按列计算的样本均值为 $\\mu \\in \\mathbb{R}^d$，按列计算的样本方差为 $v \\in \\mathbb{R}^d$，其中对于每个特征索引 $j \\in \\{1,\\dots,d\\}$，\n$$\n\\mu_j = \\frac{1}{n}\\sum_{i=1}^n X_{ij}, \\quad v_j = \\frac{1}{n}\\sum_{i=1}^n \\left( X_{ij} - \\mu_j \\right)^2.\n$$\n给定参数 $\\gamma \\in \\mathbb{R}^d$、$\\beta \\in \\mathbb{R}^d$ 和一个小的正常数 $\\varepsilon \\in \\mathbb{R}$，BN 生成一个输出 $Y \\in \\mathbb{R}^{n \\times d}$，其元素为\n$$\nY_{ij} = \\gamma_j \\cdot \\frac{X_{ij} - \\mu_j}{\\sqrt{v_j + \\varepsilon}} + \\beta_j.\n$$\n设样本协方差估计量为\n$$\n\\hat{\\Sigma}(Z) = \\frac{1}{n}\\left(Z - \\mathbf{1}\\bar{z}^\\top\\right)^\\top \\left(Z - \\mathbf{1}\\bar{z}^\\top\\right),\n$$\n其中 $\\bar{z} \\in \\mathbb{R}^d$ 是 $Z$ 的按列均值，$\\mathbf{1} \\in \\mathbb{R}^n$ 是全为 1 的向量。对于一个对称矩阵 $A \\in \\mathbb{R}^{d \\times d}$，定义谱范数\n$$\n\\|A\\|_2 = \\max_{\\|u\\|_2 = 1} \\|Au\\|_2,\n$$\n以及二次型\n$$\nq_A(u) = u^\\top A u.\n$$\n您的任务是，对于每个给定的训练快照，通过 BN 计算 $Y$，然后计算 $\\hat{\\Sigma}_{\\text{BN}} = \\hat{\\Sigma}(Y)$，并通过 $\\|\\hat{\\Sigma}_{\\text{BN}}\\|_2$ 在不同快照间的变化来分析训练的稳定性。此外，对于一个固定的方向 $u \\in \\mathbb{R}^d$，通过计算以下比率来比较 BN 对二次型的影响\n$$\nr = \\frac{u^\\top \\hat{\\Sigma}_{\\text{BN}} u}{u^\\top \\hat{\\Sigma}(X) u}.\n$$\n\n从上述定义出发，且不假定任何快捷恒等式，实现一个程序，为每个快照计算：\n- $\\|\\hat{\\Sigma}_{\\text{BN}}\\|_2$ 的值，\n- 一个相对于前一个快照的布尔稳定性指标，定义为\n$$\n\\text{stable} = \n\\begin{cases}\n\\text{True},  \\text{对于第一个快照},\\\\\n\\left( \\left| \\|\\hat{\\Sigma}_{\\text{BN}}^{(t)}\\|_2 - \\|\\hat{\\Sigma}_{\\text{BN}}^{(t-1)}\\|_2 \\right| \\le \\tau \\right),  \\text{对于 } t \\ge 2,\n\\end{cases}\n$$\n其中 $\\tau \\in \\mathbb{R}_{>0}$ 是一个给定的阈值，\n- 上面定义的比率 $r$。\n\n使用以下包含三个快照的测试套件，每个快照都有 $n = 6$ 个样本和 $d = 3$ 个特征，并使用相同的方向 $u$ 和稳定性阈值 $\\tau$：\n- 快照 1：\n  - $X^{(1)} = \\begin{bmatrix}\n  1.2  -0.3  2.0 \\\\\n  0.9  -0.1  2.2 \\\\\n  1.1  -0.4  1.9 \\\\\n  1.0  0.0  2.1 \\\\\n  1.3  -0.2  2.3 \\\\\n  0.8  -0.1  2.0\n  \\end{bmatrix}$，\n  - $\\gamma^{(1)} = \\begin{bmatrix} 0.9  1.1  1.0 \\end{bmatrix}$，\n  - $\\beta^{(1)} = \\begin{bmatrix} 0.1  -0.05  0.0 \\end{bmatrix}$，\n  - $\\varepsilon^{(1)} = 10^{-5}$。\n- 快照 2（第二个特征近乎恒定）：\n  - $X^{(2)} = \\begin{bmatrix}\n  2.0  5.0  -1.0 \\\\\n  2.1  5.0  -0.9 \\\\\n  1.9  5.0  -1.1 \\\\\n  2.2  5.0  -1.0 \\\\\n  2.0  5.0  -1.2 \\\\\n  2.1  5.0  -1.0\n  \\end{bmatrix}$，\n  - $\\gamma^{(2)} = \\begin{bmatrix} 1.0  0.5  1.2 \\end{bmatrix}$，\n  - $\\beta^{(2)} = \\begin{bmatrix} 0.0  0.0  0.1 \\end{bmatrix}$，\n  - $\\varepsilon^{(2)} = 10^{-4}$。\n- 快照 3（第三个特征上存在极端缩放）：\n  - $X^{(3)} = \\begin{bmatrix}\n  -0.5  1.0  0.2 \\\\\n  -0.6  0.9  0.3 \\\\\n  -0.4  1.1  0.4 \\\\\n  -0.5  1.05  0.25 \\\\\n  -0.55  0.95  0.35 \\\\\n  -0.45  1.02  0.28\n  \\end{bmatrix}$，\n  - $\\gamma^{(3)} = \\begin{bmatrix} 0.7  1.0  2.5 \\end{bmatrix}$，\n  - $\\beta^{(3)} = \\begin{bmatrix} -0.1  0.0  0.0 \\end{bmatrix}$，\n  - $\\varepsilon^{(3)} = 10^{-5}$。\n\n使用固定的方向和阈值：\n- $u = \\begin{bmatrix} 1.0  -0.5  0.2 \\end{bmatrix}$，\n- $\\tau = 0.15$。\n\n您的程序必须：\n1. 使用上述定义实现 BN 变换。\n2. 为每个快照计算 $\\hat{\\Sigma}(X)$ 和 $\\hat{\\Sigma}_{\\text{BN}} = \\hat{\\Sigma}(Y)$。\n3. 仅使用谱范数作为最大奇异值的定义以及关于对称矩阵的事实来计算 $\\|\\hat{\\Sigma}_{\\text{BN}}\\|_2$。\n4. 为每个快照计算稳定性布尔值和比率 $r$。\n\n最终输出格式：\n您的程序应生成一行输出，其中包含一个用方括号括起来的逗号分隔列表，每个快照的结果是一个列表 $[\\|\\hat{\\Sigma}_{\\text{BN}}\\|_2, \\text{stable}, r]$。例如，打印的输出应类似于 $[[\\text{float},\\text{boolean},\\text{float}],\\dots]$。所有量都是无量纲的，不需要单位。", "solution": "用户要求分析批归一化 (BN) 变换对跨多个快照的特征数据的样本协方差矩阵的影响。我将首先验证问题陈述，然后提供一个原则性的、分步的解决方案。\n\n### 问题验证\n\n**第 1 步：提取已知条件**\n问题提供了以下定义和数据：\n-   一个特征矩阵 $X \\in \\mathbb{R}^{n \\times d}$。\n-   按列样本均值：$\\mu_j = \\frac{1}{n}\\sum_{i=1}^n X_{ij}$。\n-   按列样本方差：$v_j = \\frac{1}{n}\\sum_{i=1}^n \\left( X_{ij} - \\mu_j \\right)^2$。\n-   BN 参数：$\\gamma \\in \\mathbb{R}^d$, $\\beta \\in \\mathbb{R}^d$, $\\varepsilon \\in \\mathbb{R}_{>0}$。\n-   BN 输出：$Y_{ij} = \\gamma_j \\cdot \\frac{X_{ij} - \\mu_j}{\\sqrt{v_j + \\varepsilon}} + \\beta_j$。\n-   样本协方差估计量：$\\hat{\\Sigma}(Z) = \\frac{1}{n}\\left(Z - \\mathbf{1}\\bar{z}^\\top\\right)^\\top \\left(Z - \\mathbf{1}\\bar{z}^\\top\\right)$，其中 $\\bar{z}$ 是 $Z$ 的按列均值。\n-   对称矩阵 $A$ 的谱范数：$\\|A\\|_2 = \\max_{\\|u\\|_2 = 1} \\|Au\\|_2$。\n-   二次型：$q_A(u) = u^\\top A u$。\n-   每个快照的任务：\n    1.  计算 $\\|\\hat{\\Sigma}_{\\text{BN}}\\|_2$，其中 $\\hat{\\Sigma}_{\\text{BN}} = \\hat{\\Sigma}(Y)$。\n    2.  计算一个稳定性指标：$\\text{stable} = (\\text{t=1 时为 True})$ 或 $(\\left| \\|\\hat{\\Sigma}_{\\text{BN}}^{(t)}\\|_2 - \\|\\hat{\\Sigma}_{\\text{BN}}^{(t-1)}\\|_2 \\right| \\le \\tau \\text{ 当 } t \\ge 2)$。\n    3.  计算比率 $r = \\frac{u^\\top \\hat{\\Sigma}_{\\text{BN}} u}{u^\\top \\hat{\\Sigma}(X) u}$。\n-   提供了三个快照的数值数据 ($X, \\gamma, \\beta, \\varepsilon$)，其中 $n=6, d=3$。\n-   给定了固定的方向向量 $u = \\begin{bmatrix} 1.0  -0.5  0.2 \\end{bmatrix}$ 和阈值 $\\tau = 0.15$。\n\n**第 2 步：验证已知条件**\n根据既定标准对问题进行评估：\n1.  **科学严谨性**：均值、方差、批归一化和样本协方差的定义是统计学习领域的标准定义。样本方差和协方差的定义一致使用因子 $1/n$。谱范数的定义是正确的。问题在科学和数学上是严谨的。\n2.  **适定性**：问题是适定的。提供了所有必要的数据和定义来计算唯一的数值解。任务被明确无误地指定。\n3.  **客观性**：问题以精确、客观的数学语言陈述，没有主观性断言。\n\n**第 3 步：结论**\n问题是**有效的**。它是自洽的、数学上严谨的，并基于应用于机器学习的线性代数和统计学的既定原则。现在将推导一个完整的解决方案。\n\n### 求解推导\n\n对每个快照的分析涉及一系列计算。我们将为通用快照概述方法论，然后将其应用于所提供的数据。\n\n**1. 批归一化变换**\n给定一个小批量特征矩阵 $X \\in \\mathbb{R}^{n \\times d}$，参数 $\\gamma, \\beta \\in \\mathbb{R}^d$ 和 $\\varepsilon \\in \\mathbb{R}_{>0}$：\n\na. **计算批次统计量**：首先，我们计算 $X$ 的按列样本均值 $\\mu \\in \\mathbb{R}^d$ 和方差 $v \\in \\mathbb{R}^d$。\n$$ \\mu = \\frac{1}{n} X^\\top \\mathbf{1}_n \\quad (\\text{或 } \\mu_j = \\frac{1}{n} \\sum_{i=1}^n X_{ij}) $$\n$$ v_j = \\frac{1}{n} \\sum_{i=1}^n (X_{ij} - \\mu_j)^2 $$\n\nb. **归一化与变换**：我们使用这些统计量来计算输出矩阵 $Y \\in \\mathbb{R}^{n \\times d}$。为了高效计算，该变换可以用矩阵运算表示。令 $\\mathbf{1}_n$ 为 $n$ 维的全 1 向量。\n-   中心化矩阵为 $X_c = X - \\mathbf{1}_n \\mu^\\top$。\n-   每个元素的归一化步骤是 $\\hat{X}_{ij} = \\frac{X_{ij} - \\mu_j}{\\sqrt{v_j + \\varepsilon}}$。\n-   最终的仿射变换是 $Y_{ij} = \\gamma_j \\hat{X}_{ij} + \\beta_j$。\n以矩阵形式表示：\n$$ Y = (X - \\mathbf{1}_n \\mu^\\top) \\text{diag}(\\frac{1}{\\sqrt{v_1+\\varepsilon}}, \\dots, \\frac{1}{\\sqrt{v_d+\\varepsilon}}) \\text{diag}(\\gamma_1, \\dots, \\gamma_d) + \\mathbf{1}_n \\beta^\\top $$\n其中 $\\text{diag}(\\cdot)$ 创建一个对角矩阵。在实践中，这是通过广播 (broadcasting) 实现的。\n\n**2. 协方差矩阵计算**\n问题将一个矩阵 $Z \\in \\mathbb{R}^{n \\times d}$ 的样本协方差估计量定义为 $\\hat{\\Sigma}(Z) = \\frac{1}{n} Z_c^\\top Z_c$，其中 $Z_c$ 是中心化矩阵 $Z_c = Z - \\mathbf{1}_n \\bar{z}^\\top$ 且 $\\bar{z}$ 是 $Z$ 的按列均值。\n\na. **输入的协方差**：我们计算 $\\hat{\\Sigma}(X)$。均值 $\\mu$ 已知。中心化矩阵为 $X_c = X - \\mathbf{1}_n \\mu^\\top$。\n$$ \\hat{\\Sigma}(X) = \\frac{1}{n} X_c^\\top X_c $$\n\nb. **BN 输出的协方差**：我们计算 $\\hat{\\Sigma}_{\\text{BN}} = \\hat{\\Sigma}(Y)$。这需要首先计算 $Y$ 的均值，我们称之为 $\\bar{y}$。请注意，我们必须直接从 $Y$ 计算它，而不是依赖 BN 的任何理论属性。\n$$ \\bar{y} = \\frac{1}{n} Y^\\top \\mathbf{1}_n $$\n然后，我们构建中心化的输出矩阵 $Y_c = Y - \\mathbf{1}_n \\bar{y}^\\top$。\n$$ \\hat{\\Sigma}_{\\text{BN}} = \\frac{1}{n} Y_c^\\top Y_c $$\n\n**3. 谱范数计算**\n对称矩阵 $A$ 的谱范数 $\\|A\\|_2$ 是其最大的奇异值，也等于其最大绝对值的特征值。由于 $\\hat{\\Sigma}_{\\text{BN}}$ 是一个样本协方差矩阵，它是对称且半正定的，这意味着其特征值非负。\n$$ \\|\\hat{\\Sigma}_{\\text{BN}}\\|_2 = \\lambda_{\\max}(\\hat{\\Sigma}_{\\text{BN}}) = \\sigma_{\\max}(\\hat{\\Sigma}_{\\text{BN}}) $$\n我们将计算 $\\hat{\\Sigma}_{\\text{BN}}$ 的奇异值并取其最大值。\n\n**4. 稳定性指标**\n对于每个快照 $t \\in \\{1, 2, 3\\}$，我们计算稳定性指标。设 $s^{(t)} = \\|\\hat{\\Sigma}_{\\text{BN}}^{(t)}\\|_2$。\n-   对于 $t=1$：$\\text{stable}^{(1)} = \\text{True}$。\n-   对于 $t \\ge 2$：$\\text{stable}^{(t)} = \\left| s^{(t)} - s^{(t-1)} \\right| \\le \\tau$。\n我们使用给定的阈值 $\\tau = 0.15$。\n\n**5. 二次型比率**\n使用给定的向量 $u \\in \\mathbb{R}^d$，我们计算比率 $r$。\na. **X 的二次型**：$q_X = u^\\top \\hat{\\Sigma}(X) u$。\nb. **Y 的二次型**：$q_Y = u^\\top \\hat{\\Sigma}_{\\text{BN}} u$。\nc. **比率**：$r = \\frac{q_Y}{q_X}$。\n该比率量化了 BN 变换如何改变在 $u$ 方向上的方差。对于给定的数据，分母不为零，因此该比率是良定义的。\n\n对三个快照中的每一个重复这一系列操作，并保留前一个快照的谱范数以计算当前快照的稳定性指标。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem by processing three snapshots of training data.\n    For each snapshot, it applies Batch Normalization, computes covariance matrices,\n    and calculates stability metrics as per the problem description.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    # Each case is a dictionary containing X, gamma, beta, and epsilon.\n    test_cases = [\n        {\n            \"X\": np.array([\n                [1.2, -0.3, 2.0],\n                [0.9, -0.1, 2.2],\n                [1.1, -0.4, 1.9],\n                [1.0,  0.0, 2.1],\n                [1.3, -0.2, 2.3],\n                [0.8, -0.1, 2.0]\n            ]),\n            \"gamma\": np.array([0.9, 1.1, 1.0]),\n            \"beta\": np.array([0.1, -0.05, 0.0]),\n            \"epsilon\": 1e-5\n        },\n        {\n            \"X\": np.array([\n                [2.0, 5.0, -1.0],\n                [2.1, 5.0, -0.9],\n                [1.9, 5.0, -1.1],\n                [2.2, 5.0, -1.0],\n                [2.0, 5.0, -1.2],\n                [2.1, 5.0, -1.0]\n            ]),\n            \"gamma\": np.array([1.0, 0.5, 1.2]),\n            \"beta\": np.array([0.0, 0.0, 0.1]),\n            \"epsilon\": 1e-4\n        },\n        {\n            \"X\": np.array([\n                [-0.5,  1.0,   0.2],\n                [-0.6,  0.9,   0.3],\n                [-0.4,  1.1,   0.4],\n                [-0.5,  1.05,  0.25],\n                [-0.55, 0.95,  0.35],\n                [-0.45, 1.02,  0.28]\n            ]),\n            \"gamma\": np.array([0.7, 1.0, 2.5]),\n            \"beta\": np.array([-0.1, 0.0, 0.0]),\n            \"epsilon\": 1e-5\n        }\n    ]\n\n    # Fixed parameters\n    u = np.array([1.0, -0.5, 0.2])\n    tau = 0.15\n\n    results = []\n    prev_spectral_norm_bn = None\n\n    def calculate_covariance_matrix(Z):\n        \"\"\"\n        Computes the sample covariance matrix for a given data matrix Z.\n        Sigma_hat(Z) = (1/n) * (Z - 1*z_bar^T)^T * (Z - 1*z_bar^T)\n        \"\"\"\n        n = Z.shape[0]\n        # Calculate column-wise mean z_bar\n        z_bar = np.mean(Z, axis=0)\n        # Center the matrix Z\n        Z_centered = Z - z_bar\n        # Compute the covariance matrix\n        cov_Z = (1/n) * Z_centered.T @ Z_centered\n        return cov_Z\n\n    for i, case in enumerate(test_cases):\n        X = case[\"X\"]\n        gamma = case[\"gamma\"]\n        beta = case[\"beta\"]\n        epsilon = case[\"epsilon\"]\n        n = X.shape[0]\n\n        # 1. Batch Normalization Transformation\n        # a. Compute batch statistics (mean and variance)\n        mu = np.mean(X, axis=0)\n        # np.var uses 1/n divisor by default (ddof=0), which matches the problem spec.\n        var = np.var(X, axis=0)\n        \n        # b. Normalize and transform X to get Y\n        X_normalized = (X - mu) / np.sqrt(var + epsilon)\n        Y = gamma * X_normalized + beta\n\n        # 2. Covariance Matrix Computation\n        # a. Covariance of input X\n        sigma_X = calculate_covariance_matrix(X)\n        \n        # b. Covariance of BN output Y\n        sigma_bn = calculate_covariance_matrix(Y)\n\n        # 3. Spectral Norm Computation\n        # The spectral norm is the largest singular value. For a symmetric PSD matrix,\n        # it is also the largest eigenvalue. We compute singular values.\n        singular_values = np.linalg.svd(sigma_bn, compute_uv=False)\n        spectral_norm_bn = singular_values[0]\n\n        # 4. Stability Indicator\n        if i == 0:\n            # For the first snapshot, stability is True by definition.\n            is_stable = True\n        else:\n            # For subsequent snapshots, compare with the previous norm.\n            is_stable = np.abs(spectral_norm_bn - prev_spectral_norm_bn) = tau\n        \n        # Store for the next iteration\n        prev_spectral_norm_bn = spectral_norm_bn\n\n        # 5. Quadratic Form Ratio\n        # q_A(u) = u^T * A * u\n        q_X = u.T @ sigma_X @ u\n        q_BN = u.T @ sigma_bn @ u\n        \n        # Handle potential division by zero, though not expected for this data\n        if q_X == 0:\n            # If the variance in direction u is zero for X, the ratio is ill-defined.\n            # We can set it to a placeholder like infinity or NaN depending on context.\n            # For this problem, we assume q_X will be non-zero.\n            r = np.inf if q_BN != 0 else np.nan\n        else:\n            r = q_BN / q_X\n\n        # Store the results for this snapshot\n        results.append([spectral_norm_bn, is_stable, r])\n\n    # Final print statement in the exact required format.\n    # Convert boolean to Python's True/False string representation\n    formatted_results = []\n    for res in results:\n        # Format: [float, boolean, float]\n        formatted_results.append(f\"[{res[0]},{str(res[1])},{res[2]}]\")\n\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3146421"}, {"introduction": "最后，我们从分析转向设计，这是将理论知识转化为实用技能的关键一步。在许多机器学习应用中，我们需要主动控制模型的某些行为，例如限制其在特定数据分布下的输出方差。这项挑战要求你设计一个完整的算法，通过调整正则化参数 $\\lambda$ 来确保一个二次型 $w^{\\top} \\hat{\\Sigma} w$ 不超过预设的容忍度 $\\tau$。此问题的核心难点在于，对于大规模数据，精确计算协方差矩阵 $\\hat{\\Sigma}$ 及其谱范数成本过高。因此，你将实现一种高效的现代数值方法——随机SVD（Randomized SVD）——来估计谱范数，并基于该估计值设计一个既能提出合理参数又能保证约束最终得到满足的“安全”调优策略。这项综合性练习将引导你整合理论界、数值估计和算法逻辑，构建一个反映机器学习工程师在现实世界中所面临挑战的鲁棒控制系统 [@problem_id:3146495]。", "problem": "考虑一个中心化的数据矩阵 $X \\in \\mathbb{R}^{n \\times d}$，其经验协方差估计量 $\\hat{\\Sigma} \\in \\mathbb{R}^{d \\times d}$ 定义为\n$$\n\\hat{\\Sigma} = \\frac{1}{n} X^\\top X,\n$$\n其中 $X$ 的每一列都已进行均值中心化。设 $w \\in \\mathbb{R}^d$ 为一个给定的权重向量，$\\tau \\in \\mathbb{R}_{\\ge 0}$ 为一个预设的容差。矩阵 $A$ 的谱范数，记作 $\\|A\\|_2$，是 $A$ 的最大奇异值。对于对称半正定矩阵（例如协方差矩阵），$\\|A\\|_2$ 等于其最大特征值。一个二次型由 $w^\\top \\hat{\\Sigma} w$ 给出。在统计学习中，控制 $w^\\top \\hat{\\Sigma} w$ 可以解释为在经验协方差下限制由模型权重引起的方差。\n\n您的任务是设计一个完整、可运行的程序，该程序能够：\n- 使用基于奇异值分解（SVD）的随机化值域查找器，构建谱范数 $\\|\\hat{\\Sigma}\\|_2$ 的估计量，其中随机化值域查找器使用带有过采样参数和固定幂迭代次数的高斯草图。\n- 使用估计的谱范数，通过以下规则提出并安全地调整一个正则化参数 $\\lambda \\in \\mathbb{R}_{\\ge 0}$ 来缩放权重\n$$\nw_\\lambda = \\frac{1}{1 + \\lambda} \\, w,\n$$\n从而使得到的二次型满足\n$$\nw_\\lambda^\\top \\hat{\\Sigma} \\, w_\\lambda \\le \\tau.\n$$\n您的推导和算法必须从核心定义和事实出发：协方差矩阵的性质、谱范数和二次型；以及将随机化SVD作为一种经过充分检验的估计主导奇异值的数值方法的普适思想。您不得依赖任何未从这些基础推导出的简化公式。估计量和调整规则必须确保不等式 $w_\\lambda^\\top \\hat{\\Sigma} \\, w_\\lambda \\le \\tau$ 对实际的经验协方差成立，即使在谱范数估计量不完美的情况下也是如此；如果基于估计量的初始提议不满足该不等式，您的算法必须安全地调整 $\\lambda$ 以实现它。\n\n实现以下四个案例的测试套件。在每个案例中，必须使用指定的随机种子和预设的生成模型确定性地构建 $X$。然后计算 $\\hat{\\Sigma}$，通过目标秩 $k = 1$、过采样 $p$ 和幂迭代 $q$ 的随机化SVD估计 $\\|\\hat{\\Sigma}\\|_2$，调整 $\\lambda$ 以满足不等式，并报告结果。\n\n测试案例 1（满秩对角协方差）：\n- 维度：$n = 200$, $d = 5$。\n- 随机种子：$s = 42$。\n- 通过抽取具有独立标准正态分布条目的 $Z \\in \\mathbb{R}^{n \\times d}$ 并设置 $X = Z \\, D^{1/2}$ 来构建 $X$，其中 $D = \\operatorname{diag}(3.0, 2.0, 1.0, 0.5, 0.1)$ 且 $D^{1/2} = \\operatorname{diag}(\\sqrt{3.0}, \\sqrt{2.0}, \\sqrt{1.0}, \\sqrt{0.5}, \\sqrt{0.1})$；然后对 $X$ 的每一列进行均值中心化。\n- 权重向量：$w = (1.0, -0.5, 0.75, 0.0, -1.25)$。\n- 容差：$\\tau = 3.5$。\n- 随机化SVD参数：目标秩 $k = 1$，过采样 $p = 4$，幂迭代 $q = 2$。\n\n测试案例 2（低秩协方差）：\n- 维度：$n = 100$, $d = 6$。\n- 随机种子：$s = 7$。\n- 通过使用相同的种子抽取具有独立标准正态分布条目的 $Z \\in \\mathbb{R}^{n \\times 2}$ 和 $B \\in \\mathbb{R}^{2 \\times d}$，并设置 $X = Z B$ 来构建 $X$；然后对 $X$ 的每一列进行均值中心化。\n- 权重向量：$w = (0.5, -0.5, 0.5, -0.5, 0.5, -0.5)$。\n- 容差：$\\tau = 1.0$。\n- 随机化SVD参数：目标秩 $k = 1$，过采样 $p = 4$，幂迭代 $q = 2$。\n\n测试案例 3（严格的容差）：\n- 维度：$n = 50$, $d = 4$。\n- 随机种子：$s = 0$。\n- 通过抽取具有独立标准正态分布条目的 $Z \\in \\mathbb{R}^{n \\times d}$ 并设置 $X = Z \\, D^{1/2}$ 来构建 $X$，其中 $D = \\operatorname{diag}(2.5, 1.5, 0.3, 0.1)$；然后对 $X$ 的每一列进行均值中心化。\n- 权重向量：$w = (2.0, 1.0, -1.0, 0.5)$。\n- 容差：$\\tau = 0.05$。\n- 随机化SVD参数：目标秩 $k = 1$，过采样 $p = 4$，幂迭代 $q = 2$。\n\n测试案例 4（零权重）：\n- 维度：$n = 60$, $d = 5$。\n- 随机种子：$s = 123$。\n- 通过抽取具有独立标准正态分布条目的 $Z \\in \\mathbb{R}^{n \\times d}$ 并设置 $X = Z \\, D^{1/2}$ 来构建 $X$，其中 $D = \\operatorname{diag}(1.0, 0.8, 0.6, 0.4, 0.2)$；然后对 $X$ 的每一列进行均值中心化。\n- 权重向量：$w = (0.0, 0.0, 0.0, 0.0, 0.0)$。\n- 容差：$\\tau = 1.0$。\n- 随机化SVD参数：目标秩 $k = 1$，过采样 $p = 4$，幂迭代 $q = 2$。\n\n算法要求：\n- 在形成 $\\hat{\\Sigma}$ 之前，对 $X$ 的列进行中心化处理。\n- 实现一个随机化值域查找器来估计 $\\|\\hat{\\Sigma}\\|_2$：抽取一个高斯草图矩阵 $\\Omega \\in \\mathbb{R}^{d \\times (k+p)}$，形成 $Y = \\hat{\\Sigma} \\Omega$，重复应用 $q$ 次幂迭代 $Y \\leftarrow \\hat{\\Sigma} Y$，通过 QR 分解计算 $Y$ 列向量的标准正交基 $Q$，然后估计 $Q^\\top \\hat{\\Sigma} Q$ 的最大奇异值。\n- 使用谱范数估计和缩放规则 $w_\\lambda = \\frac{1}{1+\\lambda} w$ 来提出 $\\lambda$。如果该提议不满足 $w_\\lambda^\\top \\hat{\\Sigma} w_\\lambda \\le \\tau$，则安全地增加 $\\lambda$，以使不等式对实际的二次型成立。\n\n最终输出格式：\n- 对于每个测试案例，输出列表 $[\\text{norm\\_est}, \\text{lambda}, \\text{qf\\_final}, \\text{satisfied}]$，其中 $\\text{norm\\_est}$ 是 $\\|\\hat{\\Sigma}\\|_2$ 的浮点估计值，$\\text{lambda}$ 是调整后的正则化参数（浮点数），$\\text{qf\\_final}$ 是最终的二次型 $w_\\lambda^\\top \\hat{\\Sigma} w_\\lambda$（浮点数），$\\text{satisfied}$ 是一个布尔值，指示 $w_\\lambda^\\top \\hat{\\Sigma} w_\\lambda \\le \\tau$ 是否成立。\n- 您的程序应生成单行输出，其中包含四个案例的结果，以逗号分隔的列表形式封装在方括号中，且不含空格；例如，形式为 $[[a_1,b_1,c_1,d_1],[a_2,b_2,c_2,d_2],[a_3,b_3,c_3,d_3],[a_4,b_4,c_4,d_4]]$。", "solution": "该问题要求设计并实现一个算法来调整权重向量 $w \\in \\mathbb{R}^d$ 的正则化参数 $\\lambda \\in \\mathbb{R}_{\\ge 0}$。该调整必须确保一个特定的二次型（代表一种方差度量）被给定的容差 $\\tau \\in \\mathbb{R}_{\\ge 0}$ 所界定。权重向量根据规则 $w_\\lambda = \\frac{1}{1 + \\lambda} w$ 进行缩放，需要满足的约束是 $w_\\lambda^\\top \\hat{\\Sigma} w_\\lambda \\le \\tau$，其中 $\\hat{\\Sigma} = \\frac{1}{n} X^\\top X$ 是中心化数据 $X \\in \\mathbb{R}^{n \\times d}$ 的经验协方差矩阵。调整过程必须使用通过随机化值域查找算法获得的协方差矩阵谱范数 $\\|\\hat{\\Sigma}\\|_2$ 的估计值，并且必须包含一个安全机制以保证约束得到满足。\n\n我们的推导分三个阶段进行：首先，我们分析约束以找到所需的确切正则化；其次，我们基于谱范数估计推导出一个正则化参数的提议值；第三，我们将这些结合成一个完整且安全的调整算法。\n\n**1. 正则化约束分析**\n\n问题的核心是以下不等式：\n$$\nw_\\lambda^\\top \\hat{\\Sigma} \\, w_\\lambda \\le \\tau\n$$\n代入缩放后权重向量的定义 $w_\\lambda = \\frac{1}{1 + \\lambda} w$，我们得到：\n$$\n\\left(\\frac{1}{1 + \\lambda} w\\right)^\\top \\hat{\\Sigma} \\left(\\frac{1}{1 + \\lambda} w\\right) \\le \\tau\n$$\n由于 $\\lambda \\ge 0$，项 $(1 + \\lambda)$ 是一个正标量。我们可以将其提取出来：\n$$\n\\frac{1}{(1 + \\lambda)^2} w^\\top \\hat{\\Sigma} w \\le \\tau\n$$\n我们定义未缩放的二次型为 $Q_0 = w^\\top \\hat{\\Sigma} w$。不等式简化为：\n$$\n\\frac{Q_0}{(1 + \\lambda)^2} \\le \\tau\n$$\n可以立即确定两种平凡情况：\n- 如果初始二次型已经满足约束，即 $Q_0 \\le \\tau$，则不需要正则化。我们可以设置 $\\lambda = 0$。\n- 如果权重向量是零向量，$w=0$，则 $Q_0 = 0$。由于容差是非负的，$\\tau \\ge 0$，约束 $0 \\le \\tau$ 总是满足。因此，$\\lambda=0$ 是合适的选择。\n\n如果 $Q_0  \\tau$，我们必须找到一个 $\\lambda  0$。重排不等式得到：\n$$\n(1 + \\lambda)^2 \\ge \\frac{Q_0}{\\tau}\n$$\n由于 $1 + \\lambda  0$，我们可以对两边取平方根：\n$$\n1 + \\lambda \\ge \\sqrt{\\frac{Q_0}{\\tau}} \\implies \\lambda \\ge \\sqrt{\\frac{Q_0}{\\tau}} - 1\n$$\n为了用最小的正则化量满足约束，我们应该选择最小的有效 $\\lambda$。这给了我们所需的确切最小值：\n$$\n\\lambda_{exact} = \\sqrt{\\frac{Q_0}{\\tau}} - 1\n$$\n该值作为我们的“安全”后备方案。\n\n**2. 通过谱范数估计提出 $\\lambda$**\n\n问题要求我们使用协方差矩阵谱范数 $\\|\\hat{\\Sigma}\\|_2$ 的估计值来*提出*一个 $\\lambda$ 值。二次型 $Q_0$ 和谱范数 $\\|\\hat{\\Sigma}\\|_2$ 之间的联系是通过瑞利商的性质建立的。对于任意向量 $w$，我们有不等式：\n$$\nw^\\top \\hat{\\Sigma} w \\le \\lambda_{max}(\\hat{\\Sigma}) \\|w\\|_2^2\n$$\n其中 $\\lambda_{max}(\\hat{\\Sigma})$ 是 $\\hat{\\Sigma}$ 的最大特征值，由于 $\\hat{\\Sigma}$ 是对称半正定的，这等于其谱范数 $\\|\\hat{\\Sigma}\\|_2$。\n\n设 $\\tilde{\\lambda}_{max}$ 是我们对 $\\|\\hat{\\Sigma}\\|_2$ 的估计。我们可以构造 $Q_0$ 的一个估计上界：$Q_0 \\le \\tilde{\\lambda}_{max} \\|w\\|_2^2$。一种保守的方法是选择一个对此上界满足约束的 $\\lambda$：\n$$\n\\frac{1}{(1 + \\lambda)^2} \\left(\\tilde{\\lambda}_{max} \\|w\\|_2^2\\right) \\le \\tau\n$$\n与之前同样的方式求解 $\\lambda$，得到一个提议值：\n$$\n\\lambda_{prop} = \\max\\left(0, \\sqrt{\\frac{\\tilde{\\lambda}_{max} \\|w\\|_2^2}{\\tau}} - 1\\right)\n$$\n这个提议取决于估计 $\\tilde{\\lambda}_{max}$ 的质量以及瑞利商不等式中的松弛程度。\n\n**3. 完整且安全的调整算法**\n\n我们现在将这些组件合成为一个鲁棒的算法，该算法会提出 $\\lambda$，然后验证其有效性，并在必要时进行修正。\n\n1.  **初始化**：从中心化的数据计算经验协方差矩阵 $\\hat{\\Sigma} = \\frac{1}{n} X^\\top X$。计算未缩放的二次型 $Q_0 = w^\\top \\hat{\\Sigma} w$。\n\n2.  **平凡情况检查**：如果 $Q_0 \\le \\tau$，约束已经满足。设置最终参数 $\\lambda_{final} = 0$。\n\n3.  **需要正则化**：如果 $Q_0  \\tau$：\n    a. **估计范数**：使用指定的随机化算法计算估计值 $\\tilde{\\lambda}_{max} \\approx \\|\\hat{\\Sigma}\\|_2$。\n    b. **提出 $\\lambda$**：计算提议的参数 $\\lambda_{prop} = \\max\\left(0, \\sqrt{\\frac{\\tilde{\\lambda}_{max} \\|w\\|_2^2}{\\tau}} - 1\\right)$。\n    c. **安全检查**：用提议的参数评估二次型，$Q_{prop} = \\frac{Q_0}{(1 + \\lambda_{prop})^2}$。\n    d. **确定 $\\lambda$**：\n        i. 如果 $Q_{prop} \\le \\tau$，该提议是安全且充分的。设置 $\\lambda_{final} = \\lambda_{prop}$。\n        ii. 如果 $Q_{prop}  \\tau$，该提议不充分。这可能是因为 $\\tilde{\\lambda}_{max}$ 是一个低估值或者瑞利商界限很松。我们回到确切的最小值：$\\lambda_{final} = \\sqrt{\\frac{Q_0}{\\tau}} - 1$。\n\n4.  **最终结果**：最终的缩放权重是 $w_{\\lambda_{final}} = \\frac{1}{1+\\lambda_{final}}w$，最终的二次型是 $w_{\\lambda_{final}}^\\top \\hat{\\Sigma} w_{\\lambda_{final}}$，保证小于或等于 $\\tau$。\n\n**随机化谱范数估计**\n\n$\\tilde{\\lambda}_{max} = \\|\\hat{\\Sigma}\\|_2$ 的估计是使用带有幂迭代的随机化值域查找器进行的。给定矩阵 $\\hat{\\Sigma} \\in \\mathbb{R}^{d \\times d}$、目标秩 $k$、过采样参数 $p$ 和幂迭代次数 $q$：\n\n1.  **草图绘制**：抽取一个随机高斯矩阵 $\\Omega \\in \\mathbb{R}^{d \\times (k+p)}$。\n2.  **值域近似**：形成草图矩阵 $Y_0 = \\hat{\\Sigma} \\Omega$。\n3.  **幂迭代**：增强草图矩阵与 $\\hat{\\Sigma}$ 的主导特征空间的对齐。对于 $i=1, \\dots, q$，更新 $Y_i = \\hat{\\Sigma} Y_{i-1}$。设最终矩阵为 $Y = Y_q$。\n4.  **正交化**：通过QR分解计算由 $Y$ 的列所张成的子空间的标准正交基：$Y = QR$。$Q \\in \\mathbb{R}^{d \\times (k+p)}$ 的列构成了这个基。\n5.  **投影**：形成小的投影矩阵 $B = Q^\\top \\hat{\\Sigma} Q \\in \\mathbb{R}^{(k+p) \\times (k+p)}$。$B$ 的谱性质近似于 $\\hat{\\Sigma}$ 的谱性质。\n6.  **估计**：$\\hat{\\Sigma}$ 的谱范数被估计为 $B$ 的谱范数：$\\tilde{\\lambda}_{max} = \\|B\\|_2$。由于 $B$ 是对称半正定的，其谱范数是它的最大特征值，可以通过计算 $B$ 的奇异值并取最大值来找到。\n\n这个有原则且鲁棒的程序确保了所有问题要求都得到满足，包括关键的安全保证。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n\n    def process_case(n, d, seed, X_params, w, tau, rsvd_params):\n        \"\"\"\n        Executes a single test case from data generation to final result calculation.\n\n        Args:\n            n (int): Number of samples.\n            d (int): Number of features.\n            seed (int): Random seed for reproducibility.\n            X_params (dict): Parameters for generating the data matrix X.\n            w (list): The weight vector.\n            tau (float): The tolerance for the quadratic form.\n            rsvd_params (dict): Parameters for the randomized SVD.\n\n        Returns:\n            list: A list containing [norm_est, lambda_final, qf_final, satisfied].\n        \"\"\"\n        rng = np.random.default_rng(seed)\n\n        # 1. Construct data matrix X and center it\n        if X_params[\"type\"] == \"diag\":\n            Z = rng.standard_normal(size=(n, d))\n            D_sqrt = np.diag(np.sqrt(X_params[\"D_diag\"]))\n            X = Z @ D_sqrt\n        elif X_params[\"type\"] == \"low_rank\":\n            rank = X_params[\"rank\"]\n            Z = rng.standard_normal(size=(n, rank))\n            B = rng.standard_normal(size=(rank, d))\n            X = Z @ B\n        \n        X_centered = X - X.mean(axis=0)\n\n        # 2. Compute empirical covariance\n        Sigma_hat = (1/n) * (X_centered.T @ X_centered)\n\n        # 3. Estimate spectral norm using randomized range finder\n        k, p, q = rsvd_params['k'], rsvd_params['p'], rsvd_params['q']\n        l = k + p\n        \n        # Use the same RNG sequence for all random operations within a case\n        Omega = rng.standard_normal(size=(d, l))\n        \n        Y = Sigma_hat @ Omega\n        for _ in range(q):\n            Y = Sigma_hat @ Y\n        \n        Q, _ = np.linalg.qr(Y)\n        \n        B = Q.T @ Sigma_hat @ Q\n        \n        # Spectral norm of B is its largest singular value\n        s_values_B = np.linalg.svd(B, compute_uv=False)\n        norm_est = s_values_B[0] if s_values_B.size > 0 else 0.0\n\n        # 4. Propose and tune lambda\n        w_vec = np.array(w)\n        Q0 = w_vec.T @ Sigma_hat @ w_vec\n\n        lambda_final = 0.0\n        \n        if Q0 > tau:\n            w_norm_sq = np.sum(w_vec**2)\n            \n            # This check is mostly for logical completeness; Q0 > tau implies w is not a zero vector.\n            if np.isclose(w_norm_sq, 0):\n                 lambda_final = 0.0\n            else:\n                # Propose lambda based on the spectral norm estimate\n                Q0_est = norm_est * w_norm_sq\n                \n                # The proposal must be non-negative. tau must be > 0 here.\n                lambda_prop = max(0.0, np.sqrt(Q0_est / tau) - 1.0)\n                \n                # Safety check: see if the proposed lambda is sufficient\n                Q_prop = Q0 / (1.0 + lambda_prop)**2\n                \n                if Q_prop = tau:\n                    lambda_final = lambda_prop\n                else:\n                    # Safety fallback: use the exact value derived from the true Q0\n                    lambda_final = np.sqrt(Q0 / tau) - 1.0\n\n        # 5. Calculate final quadratic form and check satisfaction\n        qf_final = Q0 / (1.0 + lambda_final)**2\n        satisfied = qf_final = tau\n\n        return [norm_est, lambda_final, qf_final, bool(satisfied)]\n\n    test_cases = [\n        # Case 1\n        {\n            \"n\": 200, \"d\": 5, \"seed\": 42,\n            \"X_params\": {\"type\": \"diag\", \"D_diag\": [3.0, 2.0, 1.0, 0.5, 0.1]},\n            \"w\": [1.0, -0.5, 0.75, 0.0, -1.25],\n            \"tau\": 3.5,\n            \"rsvd_params\": {\"k\": 1, \"p\": 4, \"q\": 2}\n        },\n        # Case 2\n        {\n            \"n\": 100, \"d\": 6, \"seed\": 7,\n            \"X_params\": {\"type\": \"low_rank\", \"rank\": 2},\n            \"w\": [0.5, -0.5, 0.5, -0.5, 0.5, -0.5],\n            \"tau\": 1.0,\n            \"rsvd_params\": {\"k\": 1, \"p\": 4, \"q\": 2}\n        },\n        # Case 3\n        {\n            \"n\": 50, \"d\": 4, \"seed\": 0,\n            \"X_params\": {\"type\": \"diag\", \"D_diag\": [2.5, 1.5, 0.3, 0.1]},\n            \"w\": [2.0, 1.0, -1.0, 0.5],\n            \"tau\": 0.05,\n            \"rsvd_params\": {\"k\": 1, \"p\": 4, \"q\": 2}\n        },\n        # Case 4\n        {\n            \"n\": 60, \"d\": 5, \"seed\": 123,\n            \"X_params\": {\"type\": \"diag\", \"D_diag\": [1.0, 0.8, 0.6, 0.4, 0.2]},\n            \"w\": [0.0, 0.0, 0.0, 0.0, 0.0],\n            \"tau\": 1.0,\n            \"rsvd_params\": {\"k\": 1, \"p\": 4, \"q\": 2}\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        results.append(process_case(**case))\n\n    # Format the final output string exactly as required:\n    # a list of lists, with no spaces and standard string representations.\n    outer_list = []\n    for res_list in results:\n        # Convert boolean to standard capitalized string representation, e.g., True\n        inner_list = [str(item) for item in res_list]\n        outer_list.append('[' + ','.join(inner_list) + ']')\n    final_output = '[' + ','.join(outer_list) + ']'\n    \n    print(final_output)\n\nsolve()\n```", "id": "3146495"}]}