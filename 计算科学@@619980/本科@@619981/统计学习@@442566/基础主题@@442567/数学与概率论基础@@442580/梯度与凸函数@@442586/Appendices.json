{"hands_on_practices": [{"introduction": "在机器学习中，许多重要的目标函数（例如支持向量机中的合页损失）虽然是凸的，但在某些点上并不可微。本练习将引导你探索“次梯度”这一核心概念，它是梯度思想在不可微函数上的自然推广。通过证明一个基本性质并亲手计算一个具体例子中的次梯度，你将掌握分析和优化更广泛一类凸函数的关键工具，为处理现实世界中的复杂模型打下坚实基础 [@problem_id:3125992]。", "problem": "考虑函数复合 $f(w) = g(Aw)$，其中 $g:\\mathbb{R}^m \\to \\mathbb{R}$ 是凸函数，$A:\\mathbb{R}^d \\to \\mathbb{R}^m$ 是线性映射。使用凸性定义和线性映射的性质作为基本依据。你必须从第一性原理出发：凸性定义指出，如果对于一个函数 $h$ 定义域内的所有 $x, y$ 以及所有的 $t \\in [0,1]$，\n$$\nh(tx + (1-t)y) \\leq t\\,h(x) + (1-t)\\,h(y),\n$$ \n都成立，则函数 $h$ 是凸的；并且线性意味着对于所有的 $x,y$ 和所有的 $t \\in [0,1]$ 都有 $A(tx + (1-t)y) = t\\,A x + (1-t)\\,A y$。\n\n任务 1：在 $g$ 是凸函数且 $A$ 是线性映射的假设下，证明复合函数 $f(w) = g(Aw)$ 是凸的。你的证明必须仅依赖于凸性和线性的定义，不得使用任何快捷公式。\n\n任务 2：构建一个适用于统计学习的具体数据集和一个凸分段线性函数 $g$，并分析 $f$ 的次梯度。设数据集包含 $\\mathbb{R}^2$ 中的 $n=2$ 个带标签的样本：\n- 特征向量 $x_1 = (1,0)$ 和 $x_2 = (0,1)$。\n- 标签 $y_1 = 1$ 和 $y_2 = -1$。\n\n通过堆叠行向量 $y_i x_i^\\top$ 来定义线性映射 $A$，即\n$$\nA = \\begin{pmatrix} 1  0 \\\\ 0  -1 \\end{pmatrix}.\n$$\n通过合页损失定义凸分段线性函数 $g:\\mathbb{R}^2 \\to \\mathbb{R}$\n$$\ng(z) = \\sum_{i=1}^{2} \\max\\big(0,\\, 1 - z_i\\big),\n$$\n其中 $z = (z_1,z_2) \\in \\mathbb{R}^2$。注意到 $g$ 是仿射函数的逐点最大值之和，因此是凸和分段线性的。\n\n使用这些定义，设 $f(w) = g(Aw)$，其中 $w \\in \\mathbb{R}^2$。对于任意 $w$，定义向量 $z = Aw$，并按坐标为 $g$ 定义如下的次梯度选择规则：\n- 如果 $z_i  1$，设 $s_i = -1$。\n- 如果 $z_i  1$，设 $s_i = 0$。\n- 如果 $z_i = 1$，从次微分区间 $[-1,0]$ 中选择典型代表 $s_i = -\\tfrac{1}{2}$。\n\n使用此选择规则，通过\n$$\n\\partial f(w) \\ni A^\\top s,\n$$\n计算 $f$ 在 $w$ 处的一个次梯度，其中 $s = (s_1,s_2)$ 如上定义。\n\n任务 3：在指定的测试用例上数值验证凸性，并计算边界和内部情况的次梯度。使用以下测试套件：\n- 使用 $f$ 的不等式进行凸性检查：\n  $$\n  f(t w_1 + (1-t) w_2) \\le t\\, f(w_1) + (1-t)\\, f(w_2),\n  $$\n  在以下情况求值：\n  - 情况 1：$w_1 = (0,0)$，$w_2 = (2,-2)$，$t = \\tfrac{1}{3}$。\n  - 情况 2：$w_1 = (0,0)$，$w_2 = (2,-2)$，$t = 0$。\n  - 情况 3：$w_1 = (0,0)$，$w_2 = (2,-2)$，$t = 1$。\n- 在以下点计算次梯度：\n  - 点 a：$w_a = (0,0)$。\n  - 点 b：$w_b = (2,-2)$。\n  - 点 c：$w_c = (1,-1)$。\n\n对于每个点 $w_a$、$w_b$ 和 $w_c$，根据上述规则计算 $f(w)$（作为一个实数）和所选的次梯度 $A^\\top s$（作为一个二维向量）。\n\n最终输出格式要求：你的程序应产生单行输出，其中包含用方括号括起来的逗号分隔的结果列表。顺序必须是：\n$$\n\\big[\\, \\text{convexity\\_case\\_1},\\ \\text{convexity\\_case\\_2},\\ \\text{convexity\\_case\\_3},\\ f(w_a),\\ \\partial f(w_a),\\ f(w_b),\\ \\partial f(w_b),\\ f(w_c),\\ \\partial f(w_c) \\,\\big],\n$$\n其中，凸性检查情况是布尔值，每个 $f(w)$ 是一个实数，每个 $\\partial f(w)$ 是一个包含两个实数的列表。不涉及物理单位、角度单位或百分比；所有输出都是无单位的实数或布尔值。", "solution": "特此确认问题陈述的有效性。该问题在科学上基于凸分析原理及其在统计学习中的应用。这是一个适定的问题，所有组成部分都得到了精确定义，是客观且自洽的。这些任务逻辑结构清晰，从一个理论证明开始，随后进行数值验证和计算，这是数学和工程学中的一种标准教学方法。\n\n**任务 1：复合函数凸性的证明**\n\n我们被要求证明函数 $f(w) = g(Aw)$ 是凸的，已知 $g: \\mathbb{R}^m \\to \\mathbb{R}$ 是一个凸函数，$A: \\mathbb{R}^d \\to \\mathbb{R}^m$ 是一个线性映射。证明必须从凸性和线性的第一性原理出发。\n\n设 $w_1, w_2$ 为 $f$ 定义域 $\\mathbb{R}^d$ 中的任意两个向量。设 $t$ 为区间 $[0, 1]$ 内的任意标量。为了证明 $f$ 是凸的，我们必须证明以下不等式成立：\n$$\nf(t w_1 + (1-t) w_2) \\le t f(w_1) + (1-t) f(w_2)\n$$\n\n我们从不等式左侧开始，使用 $f(w)$ 的定义进行计算：\n$$\nf(t w_1 + (1-t) w_2) = g(A(t w_1 + (1-t) w_2))\n$$\n已知 $A$ 是一个线性映射。根据线性的定义，对于任意向量 $x, y$ 和标量 $t \\in [0, 1]$，有 $A(tx + (1-t)y) = t Ax + (1-t) Ay$。将此性质应用于我们的表达式，我们得到：\n$$\ng(A(t w_1 + (1-t) w_2)) = g(t A w_1 + (1-t) A w_2)\n$$\n为简化表示，我们定义 $\\mathbb{R}^m$ 中的两个向量：$z_1 = A w_1$ 和 $z_2 = A w_2$。表达式变为：\n$$\ng(t z_1 + (1-t) z_2)\n$$\n现在，我们使用函数 $g$ 是凸的这一给定性质。根据 $g$ 的凸性定义，对于其定义域中的任意 $z_1, z_2$ 和任意 $t \\in [0, 1]$，我们有：\n$$\ng(t z_1 + (1-t) z_2) \\le t g(z_1) + (1-t) g(z_2)\n$$\n将 $z_1 = A w_1$ 和 $z_2 = A w_2$ 的定义代回：\n$$\nt g(z_1) + (1-t) g(z_2) = t g(A w_1) + (1-t) g(A w_2)\n$$\n最后，我们再次使用 $f$ 的定义，$f(w) = g(Aw)$，将此表达式与 $f$ 关联起来：\n$$\nt g(A w_1) + (1-t) g(A w_2) = t f(w_1) + (1-t) f(w_2)\n$$\n通过连接这一系列表达式和不等式，我们证明了：\n$$\nf(t w_1 + (1-t) w_2) = g(t Aw_1 + (1-t) Aw_2) \\le t g(Aw_1) + (1-t) g(Aw_2) = t f(w_1) + (1-t) f(w_2)\n$$\n因此，$f(t w_1 + (1-t) w_2) \\le t f(w_1) + (1-t) f(w_2)$，这就完成了函数 $f(w) = g(Aw)$ 是凸函数的证明。\n\n**任务 2 和 3：数值验证与次梯度计算**\n\n我们已知线性映射 $A$ 和凸函数 $g$ 的具体形式。让我们明确地写出函数 $f(w)$ 的表达式。\n权重向量为 $w = \\begin{pmatrix} w_x \\\\ w_y \\end{pmatrix} \\in \\mathbb{R}^2$。\n线性映射为 $A = \\begin{pmatrix} 1  0 \\\\ 0  -1 \\end{pmatrix}$。\n变换 $z = Aw$ 为：\n$$\nz = \\begin{pmatrix} z_1 \\\\ z_2 \\end{pmatrix} = \\begin{pmatrix} 1  0 \\\\ 0  -1 \\end{pmatrix} \\begin{pmatrix} w_x \\\\ w_y \\end{pmatrix} = \\begin{pmatrix} w_x \\\\ -w_y \\end{pmatrix}\n$$\n函数 $g(z)$ 是合页损失 $g(z) = \\max(0, 1-z_1) + \\max(0, 1-z_2)$。\n将 $z_1=w_x$ 和 $z_2=-w_y$ 代入 $g(z)$，我们得到 $f(w)$ 的显式形式：\n$$\nf(w) = \\max(0, 1-w_x) + \\max(0, 1+w_y)\n$$\n\n**凸性检查**\n我们对 $w_1 = (0,0)$ 和 $w_2 = (2,-2)$ 检查不等式 $f(t w_1 + (1-t) w_2) \\le t f(w_1) + (1-t) f(w_2)$。\n首先，计算函数在端点处的值：\n$f(w_1) = f(0,0) = \\max(0, 1-0) + \\max(0, 1+0) = 1 + 1 = 2$。\n$f(w_2) = f(2,-2) = \\max(0, 1-2) + \\max(0, 1+(-2)) = \\max(0,-1) + \\max(0,-1) = 0 + 0 = 0$。\n\n情况 1：$t = \\frac{1}{3}$。\n插值点为 $w_{mix} = \\frac{1}{3}w_1 + \\frac{2}{3}w_2 = \\frac{2}{3}(2,-2) = (\\frac{4}{3}, -\\frac{4}{3})$。\n左侧：$f(w_{mix}) = \\max(0, 1-\\frac{4}{3}) + \\max(0, 1+(-\\frac{4}{3})) = \\max(0, -\\frac{1}{3}) + \\max(0, -\\frac{1}{3}) = 0$。\n右侧：$t f(w_1) + (1-t) f(w_2) = \\frac{1}{3}(2) + \\frac{2}{3}(0) = \\frac{2}{3}$。\n不等式为 $0 \\le \\frac{2}{3}$，这是成立的。\n\n情况 2：$t = 0$。\n不等式变为 $f(w_2) \\le f(w_2)$，即 $0 \\le 0$，这是一个成立的陈述。\n\n情况 3：$t = 1$。\n不等式变为 $f(w_1) \\le f(w_1)$，即 $2 \\le 2$，这是一个成立的陈述。\n\n**次梯度计算**\n$f(w)$ 的一个次梯度使用链式法则 $\\partial f(w) \\ni A^\\top s$ 进行计算，其中 $s \\in \\partial g(z)$ 且 $z = Aw$。\n给定 $A = \\begin{pmatrix} 1  0 \\\\ 0  -1 \\end{pmatrix}$，其转置是 $A^\\top = A$。\n次梯度为 $\\partial f(w) \\ni A^\\top s = \\begin{pmatrix} 1  0 \\\\ 0  -1 \\end{pmatrix} \\begin{pmatrix} s_1 \\\\ s_2 \\end{pmatrix} = \\begin{pmatrix} s_1 \\\\ -s_2 \\end{pmatrix}$。\n$s=(s_1, s_2)$ 的分量由 $z=(w_x, -w_y)$ 根据以下规则确定：\n- 如果 $z_i  1$，$s_i = -1$。\n- 如果 $z_i  1$，$s_i = 0$。\n- 如果 $z_i = 1$，$s_i = -1/2$。\n\n点 a: $w_a = (0,0)$。\n$f(w_a) = 2$。\n$z = Aw_a = (0,0)$，因此 $z_1 = 0  1$ 且 $z_2 = 0  1$。\n这得到 $s_1 = -1$ 和 $s_2 = -1$。\n$\\partial f(w_a) \\ni \\begin{pmatrix} s_1 \\\\ -s_2 \\end{pmatrix} = \\begin{pmatrix} -1 \\\\ -(-1) \\end{pmatrix} = \\begin{pmatrix} -1 \\\\ 1 \\end{pmatrix}$。\n\n点 b: $w_b = (2,-2)$。\n$f(w_b) = 0$。\n$z = Aw_b = (2, -(-2)) = (2,2)$，因此 $z_1 = 2  1$ 且 $z_2 = 2  1$。\n这得到 $s_1 = 0$ 和 $s_2 = 0$。\n$\\partial f(w_b) \\ni \\begin{pmatrix} s_1 \\\\ -s_2 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ -0 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$。\n\n点 c: $w_c = (1,-1)$。\n$f(w_c) = 0$。\n$z = Aw_c = (1, -(-1)) = (1,1)$，因此 $z_1 = 1$ 且 $z_2 = 1$。两者都位于拐点处。\n规则指定 $s_1 = -1/2$ 和 $s_2 = -1/2$。\n$\\partial f(w_c) \\ni \\begin{pmatrix} s_1 \\\\ -s_2 \\end{pmatrix} = \\begin{pmatrix} -1/2 \\\\ -(-1/2) \\end{pmatrix} = \\begin{pmatrix} -0.5 \\\\ 0.5 \\end{pmatrix}$。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Performs the convexity checks and subgradient computations as per the problem statement.\n    \"\"\"\n\n    # Define the base function f(w) based on the problem derivation\n    def f(w):\n        \"\"\"Computes f(w) = max(0, 1-w_x) + max(0, 1+w_y).\"\"\"\n        w = np.asarray(w)\n        return np.maximum(0, 1 - w[0]) + np.maximum(0, 1 + w[1])\n\n    # Define the subgradient computation function\n    def compute_subgradient(w):\n        \"\"\"Computes a subgradient of f at w using the specified rule.\"\"\"\n        w = np.asarray(w)\n        z1 = w[0]\n        z2 = -w[1]\n\n        # Determine s1 based on z1\n        if z1  1:\n            s1 = -1.0\n        elif z1 > 1:\n            s1 = 0.0\n        else:  # z1 == 1\n            s1 = -0.5\n\n        # Determine s2 based on z2\n        if z2  1:\n            s2 = -1.0\n        elif z2 > 1:\n            s2 = 0.0\n        else:  # z2 == 1\n            s2 = -0.5\n\n        # Compute subgradient of f(w) using the chain rule: A^T * s\n        # A^T is [[1, 0], [0, -1]]\n        subgrad_f = np.array([s1, -s2])\n        return subgrad_f.tolist()\n\n    # --- Task 3: Numerical Verification and Computation ---\n    \n    results = []\n\n    # Convexity checks\n    w1 = np.array([0.0, 0.0])\n    w2 = np.array([2.0, -2.0])\n    \n    # Case 1: t = 1/3\n    t1 = 1.0 / 3.0\n    w_mix1 = t1 * w1 + (1 - t1) * w2\n    lhs1 = f(w_mix1)\n    rhs1 = t1 * f(w1) + (1 - t1) * f(w2)\n    results.append(lhs1 = rhs1)\n\n    # Case 2: t = 0\n    t2 = 0.0\n    w_mix2 = t2 * w1 + (1 - t2) * w2\n    lhs2 = f(w_mix2)\n    rhs2 = t2 * f(w1) + (1 - t2) * f(w2)\n    results.append(lhs2 = rhs2)\n    \n    # Case 3: t = 1\n    t3 = 1.0\n    w_mix3 = t3 * w1 + (1 - t3) * w2\n    lhs3 = f(w_mix3)\n    rhs3 = t3 * f(w1) + (1 - t3) * f(w2)\n    results.append(lhs3 = rhs3)\n\n    # Subgradient computations\n    # Point a: w_a = (0,0)\n    w_a = (0.0, 0.0)\n    results.append(f(w_a))\n    results.append(compute_subgradient(w_a))\n\n    # Point b: w_b = (2,-2)\n    w_b = (2.0, -2.0)\n    results.append(f(w_b))\n    results.append(compute_subgradient(w_b))\n\n    # Point c: w_c = (1,-1)\n    w_c = (1.0, -1.0)\n    results.append(f(w_c))\n    results.append(compute_subgradient(w_c))\n    \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3125992"}, {"introduction": "在处理大规模数据集时，仅仅找到损失函数的最小值是不够的，我们还必须“快速”地找到它。本练习聚焦于优化算法中的一个里程碑式成果——Nesterov 加速梯度法，它显著提升了算法的收敛速度。你将通过编程实践，亲眼见证其理论上的 $O(1/t^2)$ 收敛率，并通过一个“失败案例”深刻理解为什么梯度利普希茨连续性等理论假设并非空谈，而是决定算法性能的实际关键 [@problem_id:3126019]。", "problem": "您的任务是从统计学习中的梯度和凸性角度，实现并实证评估一种针对凸函数的加速一阶优化方法。目标是在平滑性假设下验证理论上预期的收敛速率，并在关键的平滑性假设被违反时诊断其失效情况。您的程序必须实现一个单一的过程，对所有测试用例运行相同的加速方法，并为每个用例返回一个布尔决策。\n\n基本原理和假设：\n- 一个函数 $f:\\mathbb{R}^d\\to\\mathbb{R}$ 是凸函数，如果对于所有的 $\\mathbf{x},\\mathbf{y}\\in\\mathbb{R}^d$ 和 $\\theta\\in[0,1]$，都有 $f(\\theta \\mathbf{x}+(1-\\theta)\\mathbf{y})\\le \\theta f(\\mathbf{x})+(1-\\theta) f(\\mathbf{y})$。\n- 一个可微函数的梯度是 $L$-Lipschitz 的，如果存在 $L\\ge 0$ 使得对于所有的 $\\mathbf{x},\\mathbf{y}$，都有 $\\|\\nabla f(\\mathbf{x})-\\nabla f(\\mathbf{y})\\|_2\\le L\\|\\mathbf{x}-\\mathbf{y}\\|_2$。这蕴含了下降引理（descent lemma）：$f(\\mathbf{y})\\le f(\\mathbf{x})+\\nabla f(\\mathbf{x})^\\top(\\mathbf{y}-\\mathbf{x})+\\frac{L}{2}\\|\\mathbf{y}-\\mathbf{x}\\|_2^2$。\n- Nesterov 加速梯度（NAG）是一种加速一阶方法，在 $L$-Lipschitz 梯度假设下，就目标函数次优性 $f(\\mathbf{x}_t)-f^\\star$ 而言，它能达到最优的最坏情况收敛速率 $O(1/t^2)$，其中 $t$ 表示迭代索引，$f^\\star$ 是最小值。\n\n您的任务：\n- 实现一个单版本的 Nesterov 加速梯度方法，它使用步长 $1/L$，其中 $L$ 会在每个测试用例中提供给您的程序。使用标准的恒定步长 $1/L$ 和经典的 Nesterov 外插方案，该方案维护一个辅助序列和一个动量参数；将相同的实现应用于所有测试用例。您的实现必须纯粹通过评估梯度来运作，不得使用任何线搜索、回溯或超出所提供 $L$ 值的预言机（oracle）。\n- 对于每个测试用例，从提供的初始点 $\\mathbf{x}_0$ 开始，运行加速方法指定的迭代次数 $T$。记录目标值的序列 $f(\\mathbf{x}_t)$。\n- 为实证评估观察到的速率是否与 $O(1/t^2)$ 一致，请执行以下操作：\n  1. 令 $e_t = f(\\mathbf{x}_t) - f^\\star$，其中每个测试用例的 $f^\\star$ 是已知的，并在下面提供。对于 $t=1,2,\\dots,T$，构造序列 $s_t = t^2 e_t$。\n  2. 考虑最后一段迭代窗口 $t\\in\\{\\lfloor 0.6T\\rfloor,\\dots,T\\}$。在此窗口内，计算 $\\log e_t$ 对 $\\log t$ 回归的最小二乘斜率 $\\hat{\\alpha}$。同时计算有界性比率 $R = \\max_{t \\text{ in window}} s_t \\,/\\, \\min_{t \\text{ in window}} s_t$。\n  3. 返回一个布尔决策，当且仅当两个条件同时成立时为真：$\\hat{\\alpha}\\le -1.8$ 和 $R \\le 10$。如果序列中任何时候出现非有限的目标值，则返回假。\n- 您的程序必须汇总所有测试用例的这些决策，并将它们作为逗号分隔的 Python 列表打印在单行中，例如 $[\\text{True},\\text{False},\\text{True}]$。\n\n实现该方法并在以下测试套件上运行它：\n\n- 测试用例 $\\mathbf{A}$ (一维平滑二次函数，理想路径):\n  - 维度 $d=1$。\n  - 函数 $f(x) = \\tfrac{1}{2}\\, a x^2$，$a=10$，因此梯度为 $\\nabla f(x) = a x$。这是凸函数，且 $L=a=10$。\n  - 最小值点 $x^\\star=0$ 且 $f^\\star=0$。\n  - 初始点 $x_0=5$。\n  - 使用 $L=10$ 和 $T=300$ 次迭代。\n\n- 测试用例 $\\mathbf{B}$ (具有不同曲率的高维平滑二次函数):\n  - 维度 $d=5$。\n  - 函数 $f(\\mathbf{x}) = \\tfrac{1}{2}\\,\\mathbf{x}^\\top A \\mathbf{x}$，$A=\\mathrm{diag}(1,3,5,10,20)$，因此梯度为 $\\nabla f(\\mathbf{x})=A\\mathbf{x}$。这是凸函数，且 $L=\\lambda_{\\max}(A)=20$。\n  - 最小值点 $\\mathbf{x}^\\star=\\mathbf{0}$ 且 $f^\\star=0$。\n  - 初始点 $\\mathbf{x}_0=[5,-4,3,-2,1]^\\top$。\n  - 使用 $L=20$ 和 $T=400$ 次迭代。\n\n- 测试用例 $\\mathbf{C}$ (凸函数但梯度非全局 Lipschitz；失效诊断):\n  - 维度 $d=5$。\n  - 函数 $f(\\mathbf{x})=\\sum_{i=1}^d |x_i|^{3/2}$。此函数是凸函数且连续可微，其梯度分量为 $\\nabla_i f(\\mathbf{x})=\\tfrac{3}{2}\\,\\mathrm{sign}(x_i)\\,|x_i|^{1/2}$，但其梯度在 $\\mathbb{R}^d$ 上不是 Lipschitz 的，因为其 Jacobian 范数在 $\\mathbf{x}=\\mathbf{0}$ 附近是无界的。\n  - 最小值点 $\\mathbf{x}^\\star=\\mathbf{0}$ 且 $f^\\star=0$。\n  - 初始点 $\\mathbf{x}_0=[5,-4,3,-2,1]^\\top$。\n  - 强制加速方法使用相同的常数 $L=20$ 并运行 $T=400$ 次迭代。\n  - 预期结果是，根据上述决策规则，观察到的行为将与 $O(1/t^2)$ 不匹配。\n\n数值要求：\n- 不涉及角度。\n- 不涉及物理单位。\n- 您程序的最终输出必须是单行文本，其中包含测试用例 $\\mathbf{A}$、$\\mathbf{B}$ 和 $\\mathbf{C}$ 的三个布尔决策，以方括号括起来的逗号分隔列表形式表示，例如 $[\\text{True},\\text{True},\\text{False}]$。\n\n您的程序必须是完全自包含的，且不得读取任何输入。", "solution": "用户提供的问题已经过严格验证，被认为是定义明确、有科学依据且内部一致的。所有必要的参数和定义都已提供，足以得出一个完整且唯一的解决方案。该问题要求在三个不同的测试用例上实现并进行 Nesterov 加速梯度（NAG）方法的实证分析，旨在突出梯度 Lipschitz 连续性对于实现加速收敛速率的重要性。\n\n### 原理与算法设计\n\nNesterov 加速梯度是一种一阶优化算法，它改进了标准梯度下降法在一类凸函数上的收敛速率。对于一个具有 $L$-Lipschitz 连续梯度（即 $L$-平滑）的凸函数 $f$，标准梯度下降法在目标函数次优性 $f(\\mathbf{x}_t) - f^\\star$ 上的收敛速率为 $O(1/t)$。NAG 引入了一个“动量”项，该项结合了先前步骤的信息以加速收敛，从而为此类函数达到了 $O(1/t^2)$ 的最优速率。\n\n其核心思想是，不在当前迭代点 $\\mathbf{x}_t$ 处计算梯度，而是在一个外插点 $\\mathbf{y}_t$ 处计算，该点代表了沿近期进展方向的“激进”一步。更新规则结合了标准的梯度步和动量步。我们将使用一种常见且有效的 NAG 实现，它维护两个迭代序列 $\\mathbf{x}_t$ 和 $\\mathbf{y}_t$，并按以下步骤进行：\n\n设初始点为 $\\mathbf{x}_0$。设置辅助序列初始点 $\\mathbf{y}_0 = \\mathbf{x}_0$。对于迭代 $t=0, 1, 2, \\dots, T-1$：\n\n1.  **梯度步：** 主序列通过从辅助点 $\\mathbf{y}_t$ 开始，以步长 $\\eta = 1/L$ 进行一次梯度下降来更新。\n    $$\n    \\mathbf{x}_{t+1} = \\mathbf{y}_t - \\frac{1}{L} \\nabla f(\\mathbf{y}_t)\n    $$\n\n2.  **动量步：** 辅助序列通过从新点 $\\mathbf{x}_{t+1}$ 沿最近一步 $(\\mathbf{x}_{t+1} - \\mathbf{x}_t)$ 的方向向前投影来更新。\n    $$\n    \\mathbf{y}_{t+1} = \\mathbf{x}_{t+1} + \\beta_t (\\mathbf{x}_{t+1} - \\mathbf{x}_t)\n    $$\n    动量参数 $\\beta_t$ 必须谨慎选择。对于我们从 0 开始计数的迭代器 $t$，一个保证 $O(1/t^2)$ 速率的标准方案是 $\\beta_t = \\frac{t}{t+3}$。\n\n我们分析其收敛性的是迭代序列 $\\{\\mathbf{x}_t\\}_{t=1}^T$。\n\n### 实证速率验证\n\n理论收敛速率 $f(\\mathbf{x}_t) - f^\\star = O(1/t^2)$ 意味着对于某个常数 $C$ 和较大的 $t$，误差 $e_t = f(\\mathbf{x}_t) - f^\\star$ 的行为类似于 $C/t^2$。我们按规定使用两个条件对此进行实证验证：\n\n1.  **幂律衰减率：** 取对数，我们得到 $\\log e_t \\approx \\log C - 2 \\log t$。这表明 $\\log e_t$ 和 $\\log t$ 之间存在线性关系，斜率为 -2。我们计算迭代后期（$t \\in \\{\\lfloor 0.6T\\rfloor, \\dots, T\\}$）这种关系的最小二乘斜率 $\\hat{\\alpha}$。条件 $\\hat{\\alpha} \\le -1.8$ 检查观察到的速率是否与理论预测一致，允许微小的有限迭代偏差。\n\n2.  **渐近有界性：** 当 $t \\to \\infty$ 时，表达式 $s_t = t^2 e_t$ 应趋近于常数 $C$。我们通过计算同一分析窗口内的比率 $R = \\max(s_t) / \\min(s_t)$ 来检查该序列是否“渐近有界”。一个小的比率表明 $s_t$正在稳定到一个近乎恒定的值。条件 $R \\le 10$ 为这种行为提供了宽松的容忍度。\n\n一个测试用例被认为展示了加速率，当且仅当两个条件都满足。\n\n### 测试用例分析\n\n-   **测试用例 A 和 B：** 这两个用例涉及二次函数 $f(x) = \\frac{1}{2}ax^2$ 和 $f(\\mathbf{x}) = \\frac{1}{2}\\mathbf{x}^\\top A \\mathbf{x}$。它们的梯度 $\\nabla f(x) = ax$ 和 $\\nabla f(\\mathbf{x}) = A\\mathbf{x}$ 是线性的，因此是全局 Lipschitz 连续的。所提供的 Lipschitz 常数（A 为 $L=10$，B 为 $L=20$）是正确的。由于这些函数完美满足 NAG 的假设，我们预期该方法将达到其理论上的 $O(1/t^2)$ 速率。实证分析应确认这一点，从而对这两个用例都得出 `True` 的决策。\n\n-   **测试用例 C：** 函数是 $f(\\mathbf{x}) = \\sum_i |x_i|^{3/2}$。这个函数是凸的，但其梯度分量 $\\nabla_i f(\\mathbf{x}) = \\frac{3}{2}\\mathrm{sign}(x_i)|x_i|^{1/2}$ 不是全局 Lipschitz 连续的。Hessian 矩阵的对角元素 $\\frac{\\partial^2 f}{\\partial x_i^2} = \\frac{3}{4}|x_i|^{-1/2}$ 在 $x_i \\to 0$ 时是无界的。这意味着函数的局部平滑性在原点附近急剧恶化。NAG 达到 $O(1/t^2)$ 速率的保证不成立。使用基于常数 $L=20$ 的固定步长是不合适的；随着迭代点接近原点，局部 Lipschitz 常数可能会变得远大于 20，使得步长 $1/20$ 过大，从而导致收敛缓慢或不稳定。因此，预计观察到的收敛速率将显著慢于 $O(1/t^2)$。实证测试旨在检测这种失效情况，我们预期会得到 `False` 的决策。\n\n实现将精确遵循此逻辑，对所有情况执行相同的 NAG 算法，并应用指定的决策规则。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the optimization problem across all test cases and print the results.\n    \"\"\"\n\n    def run_nesterov_accelerated_gradient(func_f, grad_f, x0, L, T):\n        \"\"\"\n        Implements Nesterov's Accelerated Gradient (NAG) method.\n\n        Args:\n            func_f: The objective function.\n            grad_f: The gradient of the objective function.\n            x0: The initial point.\n            L: The Lipschitz constant of the gradient.\n            T: The number of iterations.\n\n        Returns:\n            A list of objective values for iterations 1 to T, or None if a non-finite value is encountered.\n        \"\"\"\n        x_k = np.array(x0, dtype=np.float64)\n        y_k = np.array(x0, dtype=np.float64)\n        x_prev_k = np.array(x0, dtype=np.float64)\n        \n        objectives = []\n\n        for k in range(T): # Corresponds to iterations t=1, ..., T\n            x_prev_k = x_k\n            \n            # Gradient evaluation and update step for x\n            grad_val = grad_f(y_k)\n            if not np.all(np.isfinite(grad_val)):\n                return None\n            \n            x_k = y_k - (1.0 / L) * grad_val\n            \n            # Store objective value\n            obj_val = func_f(x_k)\n            if not np.isfinite(obj_val):\n                return None\n            objectives.append(obj_val)\n            \n            # Momentum update step for y\n            momentum_coeff = k / (k + 3.0)\n            y_k = x_k + momentum_coeff * (x_k - x_prev_k)\n            \n        return objectives\n\n    def analyze_convergence(objectives, f_star, T):\n        \"\"\"\n        Analyzes the convergence rate based on the sequence of objective values.\n\n        Args:\n            objectives: A list of T objective values.\n            f_star: The known minimum value of the function.\n            T: The total number of iterations.\n\n        Returns:\n            A boolean decision based on the specified criteria.\n        \"\"\"\n        if objectives is None:\n            return False\n\n        # Calculate errors e_t = f(x_t) - f_star for t=1,...,T\n        e_t = np.array([obj - f_star for obj in objectives])\n\n        # Define analysis window: iterations floor(0.6*T) to T\n        start_t = int(np.floor(0.6 * T))\n        if start_t > T:\n            return False \n        \n        window_indices = np.arange(start_t - 1, T)\n        if len(window_indices)  2:\n            return False\n\n        t_vals = window_indices + 1\n        e_vals = e_t[window_indices]\n        \n        # Filter out non-positive errors to avoid issues with log\n        positive_mask = e_vals > 0\n        if np.sum(positive_mask)  2: # Need at least two points for regression\n            return False\n\n        t_log = np.log(t_vals[positive_mask])\n        e_log = np.log(e_vals[positive_mask])\n\n        # Condition 1: Check the slope of log-log plot\n        try:\n            slope, _ = np.polyfit(t_log, e_log, 1)\n        except np.linalg.LinAlgError:\n            return False # Should not happen with valid data\n        \n        cond1 = slope = -1.8\n\n        # Condition 2: Check the boundedness ratio of s_t = t^2 * e_t\n        s_t = (t_vals**2) * e_vals\n        s_t_valid = s_t[positive_mask]\n        \n        min_s_t = np.min(s_t_valid)\n        if min_s_t = 0:\n            return False\n            \n        max_s_t = np.max(s_t_valid)\n        R = max_s_t / min_s_t\n        cond2 = R = 10.0\n\n        return cond1 and cond2\n\n    # --- Test Cases Definition ---\n    # Case A\n    a_A = 10.0\n    func_A = lambda x: 0.5 * a_A * x**2\n    grad_A = lambda x: a_A * x\n    \n    # Case B\n    A_B = np.diag([1.0, 3.0, 5.0, 10.0, 20.0])\n    func_B = lambda x: 0.5 * x.T @ A_B @ x\n    grad_B = lambda x: A_B @ x\n\n    # Case C\n    # Note: np.sign(0) is 0, which gives the correct gradient at the origin.\n    func_C = lambda x: np.sum(np.power(np.abs(x), 1.5))\n    grad_C = lambda x: 1.5 * np.sign(x) * np.power(np.abs(x), 0.5)\n\n    test_cases = [\n        {\n            \"id\": \"A\",\n            \"func\": func_A, \"grad\": grad_A,\n            \"x0\": np.array([5.0]), \"L\": 10.0, \"T\": 300, \"f_star\": 0.0\n        },\n        {\n            \"id\": \"B\",\n            \"func\": func_B, \"grad\": grad_B,\n            \"x0\": np.array([5.0, -4.0, 3.0, -2.0, 1.0]), \"L\": 20.0, \"T\": 400, \"f_star\": 0.0\n        },\n        {\n            \"id\": \"C\",\n            \"func\": func_C, \"grad\": grad_C,\n            \"x0\": np.array([5.0, -4.0, 3.0, -2.0, 1.0]), \"L\": 20.0, \"T\": 400, \"f_star\": 0.0\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        objectives = run_nesterov_accelerated_gradient(\n            case[\"func\"], case[\"grad\"], case[\"x0\"], case[\"L\"], case[\"T\"]\n        )\n        decision = analyze_convergence(objectives, case[\"f_star\"], case[\"T\"])\n        results.append(decision)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3126019"}, {"introduction": "许多先进的统计学习模型，如 LASSO 回归，其优化目标是一个“复合函数”：一部分是光滑的损失函数，另一部分是不可微的正则项。面对这类问题，常规的梯度下降法不再适用。本练习将向你介绍一种功能强大的现代优化算法——近端梯度法（Proximal Gradient Method），它专为解决此类复合优化问题而设计。通过实现该算法并分析其在不同条件下的收敛行为，你将学会如何有效处理不可微性，并理解问题的底层结构（如强凸性）如何决定优化的最终效率 [@problem_id:3126043]。", "problem": "考虑统计学习中的一个复合优化问题，其目标函数是一个光滑凸项与一个凸非光滑正则化项之和。设 $f(w) = g(w) + h(w)$，其中 $w \\in \\mathbb{R}^n$，$g$ 是可微的凸函数，其梯度是 Lipschitz 连续的，而 $h$ 是凸函数，可能非光滑但可计算邻近算子。具体而言，您将研究以下情况：\n$$\ng(w) = \\tfrac{1}{2}\\lVert A w - b \\rVert_2^2,\\qquad h(w) = \\lambda \\lVert w \\rVert_1,\n$$\n并以下述基本定义和事实为出发点：\n- 光滑项的梯度为 $\\nabla g(w) = A^\\top (A w - b)$。\n- 如果存在 $L \\ge 0$ 使得对于所有 $u,v$ 都有 $\\lVert \\nabla g(u) - \\nabla g(v) \\rVert_2 \\le L \\lVert u - v \\rVert_2$ 成立，则称 $g$ 的梯度是 $L$-Lipschitz 的。对于上述二次函数 $g$，最小的有效 $L$ 等于 $A^\\top A$ 的谱范数，即 $L = \\lambda_{\\max}(A^\\top A)$。\n- 如果对于所有 $u,v$ 都有 $g(v) \\ge g(u) + \\nabla g(u)^\\top (v-u) + \\tfrac{\\mu}{2} \\lVert v-u \\rVert_2^2$ 成立，则称函数 $g$ 是 $\\mu$-强凸的。对于上述二次函数 $g$，最大的有效 $\\mu$ 等于 $A^\\top A$ 的最小特征值，即 $\\mu = \\lambda_{\\min}(A^\\top A)$，该值严格为正当且仅当 $A$ 是满列秩的。\n- 步长为 $\\alpha  0$ 时 $h$ 的邻近映射为 $\\mathrm{prox}_{\\alpha h}(v) = \\arg\\min_{u \\in \\mathbb{R}^n} \\left\\{ h(u) + \\tfrac{1}{2\\alpha} \\lVert u - v \\rVert_2^2 \\right\\}$。\n\n您将实现一个固定步长为 $\\alpha = 1/L$ 的邻近梯度法，其每次迭代如下：\n$$\nw^{k+1} = \\mathrm{prox}_{\\alpha h}\\big(w^k - \\alpha \\nabla g(w^k)\\big).\n$$\n对于 $h(w) = \\lambda \\lVert w \\rVert_1$，其邻近映射是坐标级的软阈值操作，这是一种闭式运算。\n\n您的任务：\n1. 对每个测试用例，计算 $L = \\lambda_{\\max}(A^\\top A)$ 和 $\\mu = \\lambda_{\\min}(A^\\top A)$。\n2. 从给定的初始点 $w^0$ 开始，实现步长为 $\\alpha = 1/L$ 的邻近梯度法。\n3. 通过运行算法进行大量迭代直至接近稳态，从而获得一个高精度的参考解 $w^\\star$，并令 $f^\\star = f(w^\\star)$。\n4. 按如下方式分析收敛性：\n   - 如果 $\\mu  0$，通过计算迭代序列尾部片段上各比率的中位数来估计经验线性因子：\n     $$\n     r_k = \\frac{\\lVert w^{k+1} - w^\\star \\rVert_2}{\\lVert w^{k} - w^\\star \\rVert_2},\n     $$\n     （跳过分母为零的索引）。将此中位数表示为 $q_{\\text{emp}}$。同时，计算理论上预测的最坏情况因子 $q_{\\text{th}}$，该因子源于对步长为 $\\alpha=1/L$、g 具有 $L$-Lipschitz 连续性和 $\\mu$-强凸性的邻近梯度法的标准分析。您的程序应报告经验因子是否在小容差范围内不超过理论预测值，结果编码为一个布尔值。\n   - 如果 $\\mu = 0$，设置 $q_{\\text{th}} = -1.0$ 和 $q_{\\text{emp}} = -1.0$ 作为哨兵值，并通过构造尾部序列 $c_k = k \\cdot (f(w^k) - f^\\star)$ 来验证其次线性行为，检查该序列在大多数步骤上是否在小数值容差内基本不增；将此结果报告为一个布尔值。\n\n为确保科学真实性和可测试性而设定的实现细节：\n- 使用软阈值公式计算 $\\ell_1$-范数的邻近映射。\n- 全程使用欧几里得范数 $\\lVert \\cdot \\rVert_2$。\n- 为近似 $w^\\star$，从给定的 $w^0$ 开始，将方法迭代 $N_\\star = 20000$ 步。\n- 为了进行收敛性诊断，从相同的初始点 $w^0$ 运行 $K = 800$ 次迭代，并仅使用最后 $T = 200$ 步来计算经验因子或进行次线性检查。\n- 对于强凸性检查，当 $q_{\\text{emp}} \\le q_{\\text{th}} + \\varepsilon_{\\text{lin}}$ 成立时（其中 $\\varepsilon_{\\text{lin}} = 10^{-2}$），视为成功。\n- 对于 $\\mu = 0$ 时的次线性检查，如果至少有比例为 $p = 0.95$ 的连续尾部差分满足 $c_{k+1} \\le c_k + \\varepsilon_{\\text{sub}}$（其中 $\\varepsilon_{\\text{sub}} = 10^{-9}$），则认为序列 $\\{c_k\\}$ 是基本不增的。\n- 如果从特征值计算出的 $\\mu$ 小于一个数值下限 $\\delta = 10^{-12}$，则在决定是 $\\mu=0$ 分支还是 $\\mu  0$ 分支时，将其视为零。\n\n测试套件：\n提供以下三个案例的结果，每个案例由 $(A,b,\\lambda,w^0)$ 指定，并带有明确的数值。所有数字都是无量纲的。\n- 案例 1 (良态且强凸)：\n  $$\n  A = \\begin{bmatrix}\n  3  0\\\\\n  0  2\\\\\n  0  0\n  \\end{bmatrix},\\quad\n  b = \\begin{bmatrix}\n  1\\\\\n  -2\\\\\n  0\n  \\end{bmatrix},\\quad\n  \\lambda = 0.2,\\quad\n  w^0 = \\begin{bmatrix} 2 \\\\ -1 \\end{bmatrix}.\n  $$\n- 案例 2 (秩亏，非强凸)：\n  $$\n  A = \\begin{bmatrix}\n  1  0\\\\\n  1  0\\\\\n  0  0\n  \\end{bmatrix},\\quad\n  b = \\begin{bmatrix}\n  1\\\\\n  0\\\\\n  0\n  \\end{bmatrix},\\quad\n  \\lambda = 0.1,\\quad\n  w^0 = \\begin{bmatrix} 0.5 \\\\ 0.5 \\end{bmatrix}.\n  $$\n- 案例 3 (病态且强凸)：\n  $$\n  A = \\begin{bmatrix}\n  10  0\\\\\n  0  1\\\\\n  0  0\n  \\end{bmatrix},\\quad\n  b = \\begin{bmatrix}\n  3\\\\\n  -1\\\\\n  0\n  \\end{bmatrix},\\quad\n  \\lambda = 0.05,\\quad\n  w^0 = \\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix}.\n  $$\n\n所需输出与格式：\n- 对于上述每个案例，按顺序计算并输出包含五个值的序列\n  $$\n  \\big[L,\\ \\mu,\\ q_{\\text{th}},\\ q_{\\text{emp}},\\ \\text{check}\\big],\n  $$\n  其中 $L$ 和 $\\mu$ 如前定义，$q_{\\text{th}}$ 是 $\\mu  0$ 时理论预测的最坏情况线性因子（当 $\\mu = 0$ 时为 $-1.0$），$q_{\\text{emp}}$ 是如上定义的经验因子（当 $\\mu = 0$ 时为 $-1.0$），而 $\\text{check}$ 是如上定义的布尔值。\n- 您的程序应生成单行输出，其中包含所有三个案例的结果，这些结果被串联成一个用方括号括起来的逗号分隔列表，顺序如下：\n  $$\n  \\big[L_1,\\mu_1,q_{\\text{th},1},q_{\\text{emp},1},\\text{check}_1,\\ L_2,\\mu_2,q_{\\text{th},2},q_{\\text{emp},2},\\text{check}_2,\\ L_3,\\mu_3,q_{\\text{th},3},q_{\\text{emp},3},\\text{check}_3\\big].\n  $$", "solution": "该问题要求分析 LASSO 类型问题的近端梯度法的收敛性。问题定义明确，所有必要的参数和步骤都已清晰说明，足以进行唯一且完整的求解。\n\n待优化的目标函数为 $f(w) = g(w) + h(w)$，其中：\n-   光滑凸损失项：$g(w) = \\frac{1}{2}\\lVert A w - b \\rVert_2^2$\n-   凸非光滑正则化项：$h(w) = \\lambda \\lVert w \\rVert_1$\n\n近端梯度法的迭代公式为：\n$$\nw^{k+1} = \\mathrm{prox}_{\\alpha h}\\left(w^k - \\alpha \\nabla g(w^k)\\right)\n$$\n其中 $\\alpha  0$ 是步长。\n\n迭代所需的关键组件包括：\n1.  **$g(w)$ 的梯度**：$\\nabla g(w) = A^\\top(A w - b)$。\n2.  **$h(w)$ 的邻近映射**：对于 $h(w) = \\lambda \\lVert w \\rVert_1$，其邻近算子是软阈值函数，对向量 $v$ 的每个分量 $v_i$ 进行操作：\n    $$\n    [\\mathrm{prox}_{\\alpha h}(v)]_i = \\mathrm{sign}(v_i) \\max(|v_i| - \\alpha \\lambda, 0)\n    $$\n\n算法的收敛速率取决于 $g(w)$ 的 Lipschitz 常数 $L = \\lambda_{\\max}(A^\\top A)$ 和强凸性常数 $\\mu = \\lambda_{\\min}(A^\\top A)$。问题指定采用固定步长 $\\alpha = 1/L$。\n\n每个测试用例的求解过程如下：\n1.  **计算常数**：根据矩阵 $A$ 计算 $A^\\top A$ 的特征值，从而得到 $L$ 和 $\\mu$。若 $\\mu  10^{-12}$，则将其视为 0。\n\n2.  **计算参考解**：从 $w^0$ 开始，运行算法 $N_\\star = 20000$ 次迭代，将最终结果作为高精度解 $w^\\star$，并计算 $f^\\star = f(w^\\star)$。\n\n3.  **生成分析序列**：再次从 $w^0$ 开始，运行算法 $K = 800$ 次迭代，保存所有迭代点 $\\{w^k\\}$。\n\n4.  **分析收敛性**：根据 $\\mu$ 的值进行区分。\n\n    -   **强凸情况 ($\\mu  0$)**：理论预测迭代点会线性收敛。最坏情况下的理论收敛因子为 $q_{\\text{th}} = 1 - \\mu/L$。我们通过计算最后 $T=200$ 次迭代中收敛比率 $r_k = \\lVert w^{k+1} - w^\\star \\rVert_2 / \\lVert w^{k} - w^\\star \\rVert_2$ 的中位数，得到经验收敛因子 $q_{\\text{emp}}$。如果 $q_{\\text{emp}} \\le q_{\\text{th}} + 10^{-2}$，则检查通过。\n\n    -   **非强凸情况 ($\\mu = 0$)**：理论预测函数值次线性收敛，即 $f(w^k) - f^\\star = \\mathcal{O}(1/k)$。我们验证序列 $c_k = k \\cdot (f(w^k) - f^\\star)$ 在最后 $T=200$ 次迭代中是否基本不增（即 $c_{k+1} \\le c_k + 10^{-9}$）。如果该条件在至少 95% 的步骤中成立，则检查通过。对于此情况，按规定设置 $q_{\\text{th}} = -1.0$ 和 $q_{\\text{emp}} = -1.0$。\n\n此流程将应用于所有三个测试用例，并汇总结果。", "answer": "```python\nimport numpy as np\n\n# Global constants from the problem statement\nN_STAR = 20000\nK = 800\nT = 200\nDELTA = 1e-12\nEPSILON_LIN = 1e-2\nEPSILON_SUB = 1e-9\nP = 0.95\n\ndef g(w, A, b):\n    \"\"\"Computes the value of the smooth term g(w).\"\"\"\n    return 0.5 * np.linalg.norm(A @ w - b)**2\n\ndef h(w, lambda_reg):\n    \"\"\"Computes the value of the non-smooth term h(w).\"\"\"\n    return lambda_reg * np.linalg.norm(w, 1)\n\ndef f_obj(w, A, b, lambda_reg):\n    \"\"\"Computes the value of the objective function f(w).\"\"\"\n    return g(w, A, b) + h(w, lambda_reg)\n\ndef grad_g(w, A, b):\n    \"\"\"Computes the gradient of the smooth term g(w).\"\"\"\n    return A.T @ (A @ w - b)\n\ndef prox_l1(v, step_size, lambda_reg):\n    \"\"\"Computes the proximal operator for the L1 norm (soft-thresholding).\"\"\"\n    return np.sign(v) * np.maximum(0, np.abs(v) - step_size * lambda_reg)\n\ndef run_proximal_gradient(w0, A, b, lambda_reg, alpha, n_iters):\n    \"\"\"Runs the proximal gradient algorithm for a given number of iterations.\"\"\"\n    w = np.copy(w0)\n    w_history = [np.copy(w)]\n    for _ in range(n_iters):\n        gradient = grad_g(w, A, b)\n        w_update = w - alpha * gradient\n        w = prox_l1(w_update, alpha, lambda_reg)\n        w_history.append(np.copy(w))\n    return w_history\n\ndef analyze_convergence(iterates, w_star, L, mu, A, b, lambda_reg):\n    \"\"\"Analyzes the convergence of the iterates based on mu.\"\"\"\n    if mu > 0:\n        # Strongly convex case\n        q_th = 1.0 - mu / L\n        \n        # Tail iterates from w^{K-T} to w^K (T+1 iterates)\n        tail_iterates = iterates[K - T : K + 1]\n        ratios = []\n        for i in range(T):\n            w_k = tail_iterates[i]\n            w_k_plus_1 = tail_iterates[i+1]\n            \n            denom = np.linalg.norm(w_k - w_star)\n            if denom > 1e-15:  # Avoid division by zero\n                num = np.linalg.norm(w_k_plus_1 - w_star)\n                ratios.append(num / denom)\n        \n        q_emp = np.median(ratios) if ratios else 0.0\n        check = q_emp = q_th + EPSILON_LIN\n        \n    else:\n        # Convex, but not strongly convex case\n        q_th = -1.0\n        q_emp = -1.0\n        \n        f_star = f_obj(w_star, A, b, lambda_reg)\n        \n        success_count = 0\n        num_comparisons = T\n        \n        # Check c_{k+1} = c_k + eps for k from K-T to K-1\n        # The iterates list has K+1 elements (0 to K).\n        # We need f(w^k) and f(w^{k+1}) where k runs from K-T to K-1.\n        # This corresponds to indices K-T to K in the iterates list.\n        for idx in range(K - T, K):\n            k = idx # The iteration number is the index in this context\n            f_k = f_obj(iterates[k], A, b, lambda_reg)\n            f_k_plus_1 = f_obj(iterates[k+1], A, b, lambda_reg)\n\n            # k in c_k should be the iteration number. Iteration 1 is iterates[1].\n            # So for iterates[k], it is iteration k.\n            # But the problem says construct c_k = k * ... \n            # This is ambiguous if k starts from 0 or 1.\n            # Let's assume k is the iteration counter, starting from 1.\n            # So for iterates[idx], the iteration number is idx.\n            # Let's use idx as k, assuming first iter is k=1.\n            \n            c_k = idx * (f_k - f_star)\n            c_k_plus_1 = (idx + 1) * (f_k_plus_1 - f_star)\n            \n            if c_k_plus_1 = c_k + EPSILON_SUB:\n                success_count += 1\n                \n        check = (success_count / num_comparisons) >= P\n\n    return q_th, q_emp, check\n\ndef solve_case(A_list, b_list, lambda_reg, w0_list):\n    \"\"\"Solves a single test case from the problem statement.\"\"\"\n    A = np.array(A_list, dtype=float)\n    b = np.array(b_list, dtype=float)\n    w0 = np.array(w0_list, dtype=float)\n\n    # 1. Compute L and mu\n    AtA = A.T @ A\n    eigenvalues = np.linalg.eigvalsh(AtA)\n    L = np.max(eigenvalues) if eigenvalues.size > 0 else 0.0\n    mu_raw = np.min(eigenvalues) if eigenvalues.size > 0 else 0.0\n    mu = mu_raw if mu_raw >= DELTA else 0.0\n    \n    alpha = 1.0 / L if L > 0 else 1.0\n\n    # 2. Compute reference solution w_star\n    w_star_history = run_proximal_gradient(w0, A, b, lambda_reg, alpha, N_STAR)\n    w_star = w_star_history[-1]\n\n    # 3. Generate iterates for analysis\n    iterates = run_proximal_gradient(w0, A, b, lambda_reg, alpha, K)\n    \n    # 4. Analyze convergence\n    q_th, q_emp, check = analyze_convergence(iterates, w_star, L, mu, A, b, lambda_reg)\n    \n    return [L, mu, q_th, q_emp, str(check).lower()]\n\ndef solve():\n    \"\"\"Main function to run all test cases and print results.\"\"\"\n    test_cases = [\n        # Case 1 (well-conditioned and strongly convex)\n        (\n            [[3, 0], [0, 2], [0, 0]], \n            [1, -2, 0], \n            0.2, \n            [2, -1]\n        ),\n        # Case 2 (rank-deficient, not strongly convex)\n        (\n            [[1, 0], [1, 0], [0, 0]], \n            [1, 0, 0], \n            0.1, \n            [0.5, 0.5]\n        ),\n        # Case 3 (ill-conditioned and strongly convex)\n        (\n            [[10, 0], [0, 1], [0, 0]], \n            [3, -1, 0], \n            0.05, \n            [2, 1]\n        ),\n    ]\n\n    all_results = []\n    for case in test_cases:\n        A, b, lambda_reg, w0 = case\n        result = solve_case(A, b, lambda_reg, w0)\n        all_results.extend(result)\n        \n    # Format the final output string\n    formatted_results = []\n    for item in all_results:\n        if isinstance(item, float):\n            formatted_results.append(f\"{item:.10f}\".rstrip('0').rstrip('.'))\n        else:\n            formatted_results.append(str(item))\n\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3126043"}]}