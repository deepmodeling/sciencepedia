{"hands_on_practices": [{"introduction": "在实践中，通过肉眼观察“肘部点”来确定最佳聚类数 $k$ 往往带有主观性。为了让选择过程更加客观和可复现，我们可以将其形式化为一个算法。本练习将指导您实现一个基于簇内平方和（WCSS）下降速率变化的自动化“肘部”检测算法，并在一系列不同分离度的合成数据集上验证其有效性，从而帮助您从“凭感觉”过渡到“有方法”。[@problem_id:3107505]", "problem": "您的任务是设计并实现一个算法，通过分析簇内平方和（WCSS）来选择数据集中的聚类数量。对于给定的聚类数 $k$ 和数据矩阵 $X \\in \\mathbb{R}^{n \\times d}$，簇内平方和（WCSS）定义为每个数据点到其所属簇的质心的平方欧几里得距离之和。该度量源于 $k$-means 目标，即将 $d$ 维空间中的 $n$ 个点划分到 $k$ 个簇中，以最小化各点到其簇质心的平方距离之和。众所周知，随着聚类数 $k$ 的增加，目标值 $W(k)$ 是非递增的，因为每增加一个簇都可以对数据进行更精细的划分。\n\n您的算法必须从 $k$-means 聚类的定义出发，使用多次随机初始化来计算整数 $k \\in \\{1,2,\\dots,K_{\\max}\\}$ 的 WCSS 函数 $W(k)$ 以减少对初始化的敏感性，然后评估比率\n$$\nR(k) \\equiv \\frac{W(k-1) - W(k)}{W(k) - W(k+1)}\n$$\n对于整数 $k \\in \\{2,3,\\dots,K_{\\max}-1\\}$。选择规则如下：\n- 选择满足 $R(k) \\ge \\gamma$ 的最小整数 $k$，其中 $\\gamma > 0$ 是一个给定的阈值。\n- 如果没有 $k$ 满足该不等式，则选择在 $\\{2,\\dots,K_{\\max}-1\\}$ 中使 $R(k)$ 最大化的整数 $k$；如果存在平局，则选择平局中最小的 $k$。\n\n您必须在分离度递增的合成高斯混合数据上验证该算法。数据生成方式如下：\n- 设混合成分的真实数量为 $m_{\\text{true}} = 3$，维度 $d = 2$，每个簇的样本大小为 $n_c$（因此总样本量 $n = 3 n_c$）。\n- 对于给定的分离参数 $s \\ge 0$，定义簇均值为\n$$\n\\mu_1 = (-s, 0), \\quad \\mu_2 = (s, 0), \\quad \\mu_3 = (0, s),\n$$\n并为每个簇独立采样 $n_c$ 个点，这些点来自一个多元正态分布，其均值为 $\\mu_j$，协方差矩阵为 $\\sigma^2 I_2$，其中 $\\sigma = 1$，$I_2$ 是 $2 \\times 2$ 的单位矩阵。\n- 合并所有簇的样本以形成数据集 $X$。\n\n实现要求：\n- 使用标准的 Lloyd 算法进行 $k$-means 计算来计算 $W(k)$，并通过 $r$ 次随机重启来缓解局部最小值问题。对于每个 $k$，返回 $r$ 次运行中最小的 WCSS。\n- 如果在更新过程中某个簇变为空，将其质心重新初始化为一个随机数据点。\n- 通过使用固定的伪随机数生成器种子来确保可复现性。\n- 在计算 $R(k)$ 时，如果分母 $W(k) - W(k+1)$ 在数值上为非正值或低于一个小的容差（例如，小于 $10^{-12}$），将 $R(k)$ 视为 $+\\infty$，以反映增加聚类不再减少 WCSS 的平稳状态。\n\n测试套件：\n实现您的算法以生成输出，针对以下参数集，每个参数集代表了不同的分离度和阈值情况：\n- 情况 1：$n_c = 200$, $s = 4.0$, $\\gamma = 1.3$, $K_{\\max} = 8$, $r = 10$。\n- 情况 2：$n_c = 200$, $s = 2.5$, $\\gamma = 1.2$, $K_{\\max} = 8$, $r = 10$。\n- 情况 3：$n_c = 200$, $s = 1.0$, $\\gamma = 1.2$, $K_{\\max} = 8$, $r = 10$。\n- 情况 4：$n_c = 200$, $s = 0.0$, $\\gamma = 1.2$, $K_{\\max} = 8$, $r = 10$。\n\n您的程序必须为每种情况生成合成数据，运行上述选择算法，并以整数形式输出每种情况所选择的 $k$ 值。\n\n最终输出格式：\n您的程序应生成单行输出，包含四个案例所选整数的逗号分隔列表，并用方括号括起来，例如 `[k_1,k_2,k_3,k_4]`。不涉及物理单位。不使用角度。所有数值输出均表示为纯整数，不带附加文本。", "solution": "该问题要求设计并实现一个算法，用于确定给定数据集的最佳聚类数 $k$。这是无监督学习中的一个基本问题，称为模型选择。所指定的方法是一种启发式方法，它基于簇内平方和（WCSS）的行为。WCSS 是一个常用指标，用于评估由 $k$-means 算法生成的聚类质量。\n\n首先，我们对问题的各个组成部分进行形式化。\n\n**$k$-means 聚类与簇内平方和（WCSS）**\n\n给定一个数据集 $X = \\{x_1, x_2, \\dots, x_n\\}$，其中每个数据点 $x_i \\in \\mathbb{R}^d$，$k$-means 算法旨在将这 $n$ 个点划分到 $k$ 个不相交的集合或簇 $C_1, C_2, \\dots, C_k$ 中，以便最小化 WCSS。WCSS 记为 $W(k)$，是每个点与其所属簇的质心之间的平方欧几里得距离之和。其数学定义如下：\n$$\nW(k) = \\sum_{j=1}^{k} \\sum_{x_i \\in C_j} \\|x_i - \\mu_j\\|^2\n$$\n其中 $\\mu_j = \\frac{1}{|C_j|} \\sum_{x_i \\in C_j} x_i$ 是簇 $C_j$ 中各点的质心（均值）。\n\n最小化此目标的标准迭代过程是 Lloyd 算法：\n1.  **初始化**：选择 $k$ 个初始质心，通常通过随机选择 $k$ 个数据点来实现。\n2.  **分配步骤**：将每个数据点 $x_i$ 分配到离它最近的质心 $\\mu_j$ 所对应的簇 $C_j$。即 $j = \\arg\\min_{l \\in \\{1,\\dots,k\\}} \\|x_i - \\mu_l\\|^2$。\n3.  **更新步骤**：将每个质心 $\\mu_j$ 重新计算为分配给簇 $C_j$ 的所有数据点的均值。\n重复这两个步骤，直到簇分配不再改变。在每次迭代中，$W(k)$ 的值保证是非递增的。然而，该算法可能会收敛到目标函数的局部最小值。为缓解此问题，通常会使用不同的随机初始化多次运行整个过程（$r$ 次重启），并选择产生最低 $W(k)$ 的解。\n\n随着聚类数 $k$ 的增加，WCSS $W(k)$ 是一个非递增函数。在 $k=n$ 的极端情况下，每个点自成一簇，WCSS 变为 $0$。$W(k)$ 与 $k$ 的关系图通常显示出初始的急剧下降，随后趋于平缓，形成一个类似“肘部”的形状。最佳聚类数通常被启发式地确定为这个“肘点”，在此点之后，增加更多的簇在 WCSS 减少方面带来的收益递减。\n\n**选择启发法**\n\n该问题定义了一个具体的量化规则来定位这个肘点。该规则涉及计算 $k \\in \\{2, 3, \\dots, K_{\\max}-1\\}$ 的比率 $R(k)$：\n$$\nR(k) \\equiv \\frac{W(k-1) - W(k)}{W(k) - W(k+1)}\n$$\n分子 $W(k-1) - W(k)$ 表示将聚类数从 $k-1$ 增加到 $k$ 时 WCSS 的减少量。分母 $W(k) - W(k+1)$ 表示随后从 $k$ 增加到 $k+1$ 时的减少量。$R(k)$ 的值较大表明，增加第 $k$ 个簇所带来的好处远大于增加第 $(k+1)$ 个簇所带来的好处。这正是“肘点”的条件：WCSS 与 $k$ 的曲线在点 $k$ 之前的斜率远大于之后的斜率。\n\n选择规则定义如下：\n1.  选择满足 $R(k) \\ge \\gamma$ 的最小整数 $k \\in \\{2, \\dots, K_{\\max}-1\\}$，其中 $\\gamma > 0$ 是一个指定的阈值。\n2.  如果不存在这样的 $k$，则选择在 $\\{2, \\dots, K_{\\max}-1\\}$ 范围内使比率 $R(k)$ 最大化的 $k$。如果最大值存在平局，则选择最小的 $k$。\n\n对于分母有一种特殊情况需要处理：如果 $W(k) - W(k+1)$ 在数值上接近于零（小于或等于容差 $10^{-12}$），我们将 $R(k)$ 解释为 $+\\infty$。这能正确捕捉 WCSS 已达到平稳状态的情况，使得 $k$ 成为肘点的有力候选。\n\n**算法流程**\n\n对于每个测试用例，实现将遵循以下步骤：\n\n1.  **数据生成**：对于给定的分离度 $s$、每个簇的样本量 $n_c$ 和维度 $d=2$，生成一个包含 $n=3n_c$ 个点的数据集。数据从三个高斯分布的混合模型中抽取，其均值分别为 $\\mu_1 = (-s, 0)$、$\\mu_2 = (s, 0)$ 和 $\\mu_3 = (0, s)$，并共享一个各向同性的协方差矩阵 $\\sigma^2 I_2$（其中 $\\sigma=1$）。使用固定的随机数生成器种子以确保可复现性。\n\n2.  **WCSS 曲线计算**：对于从 $1$ 到 $K_{\\max}$ 的每个整数 $k$：\n    a. 将变量 `min_wcss` 初始化为无穷大。\n    b. 独立执行 $r$ 次 Lloyd 的 $k$-means 算法。对于每次运行：\n        i.   通过从 $X$ 中随机选择 $k$ 个不同的数据点来初始化 $k$ 个质心。\n        ii.  迭代执行分配和更新步骤，直到收敛（即簇分配稳定）。\n        iii. 一个关键细节是处理空簇。如果在更新步骤中，某个簇没有任何分配的点，则通过从 $X$ 中选择一个新的随机数据点来重新初始化其质心。这确保了活动簇的数量保持为 $k$。\n        iv.  收敛后，计算最终划分的 WCSS。\n        v.   更新 `min_wcss = min(min_wcss, current_wcss)`。\n    c. 将得到的 `min_wcss` 存储为值 $W(k)$。\n\n3.  **最优 $k$ 选择**：\n    a. 使用计算出的序列 $W(1), W(2), \\dots, W(K_{\\max})$，根据指定公式计算 $k \\in \\{2, \\dots, K_{\\max}-1\\}$ 的比率 $R(k)$，包括对近零分母的特殊处理。\n    b. 从 $k=2$ 迭代到 $K_{\\max}-1$，找到第一个满足 $R(k) \\ge \\gamma$ 的 $k$。如果找到，则此 $k$ 即为结果。\n    c. 如果循环完成仍未找到这样的 $k$，则找到使 $R(k)$ 最大化的 $k$。平局决胜规则（选择最小的 $k$）可以通过查找最大值索引的标准方法自然处理。此时该 $k$ 即为结果。\n\n将此完整流程应用于问题描述中定义的四个参数集中的每一个。", "answer": "```python\nimport numpy as np\n\ndef run_kmeans(X, k, num_restarts, max_iter, rng):\n    \"\"\"\n    Runs the k-means algorithm with multiple restarts and returns the best result.\n    \n    Args:\n        X (np.ndarray): The data matrix, shape (n_samples, n_features).\n        k (int): The number of clusters.\n        num_restarts (int): The number of times to run k-means with different initializations.\n        max_iter (int): The maximum number of iterations for a single run.\n        rng (np.random.Generator): The random number generator for reproducibility.\n        \n    Returns:\n        float: The minimum WCSS found across all restarts.\n    \"\"\"\n    n_samples, n_features = X.shape\n    min_wcss = np.inf\n\n    for _ in range(num_restarts):\n        # Initialize centroids by choosing k random points from X without replacement\n        initial_indices = rng.choice(n_samples, k, replace=False)\n        centroids = X[initial_indices]\n        \n        prev_assignments = np.zeros(n_samples, dtype=int) - 1\n\n        for iteration in range(max_iter):\n            # Assignment step\n            # Calculate squared Euclidean distances from each point to each centroid\n            # Shape: (n_samples, k)\n            dist_sq = np.sum((X[:, np.newaxis, :] - centroids[np.newaxis, :, :]) ** 2, axis=2)\n            assignments = np.argmin(dist_sq, axis=1)\n\n            # Check for convergence\n            if np.array_equal(assignments, prev_assignments):\n                break\n            prev_assignments = assignments\n\n            # Update step\n            new_centroids = np.zeros((k, n_features))\n            counts = np.zeros(k, dtype=int)\n            \n            # Efficiently sum points for each cluster\n            np.add.at(new_centroids, assignments, X)\n            counts = np.bincount(assignments, minlength=k)\n            \n            non_empty_mask = counts > 0\n            empty_mask = ~non_empty_mask\n            \n            # Calculate means for non-empty clusters\n            new_centroids[non_empty_mask] /= counts[non_empty_mask, np.newaxis]\n            \n            # Handle empty clusters by re-initializing centroids to random data points\n            num_empty = np.sum(empty_mask)\n            if num_empty > 0:\n                random_points_indices = rng.choice(n_samples, num_empty, replace=True)\n                new_centroids[empty_mask] = X[random_points_indices]\n            \n            centroids = new_centroids\n        \n        # Calculate WCSS for the converged clustering\n        current_wcss = 0.0\n        for j in range(k):\n            cluster_points = X[assignments == j]\n            if len(cluster_points) > 0:\n                current_wcss += np.sum((cluster_points - centroids[j]) ** 2)\n        \n        if current_wcss  min_wcss:\n            min_wcss = current_wcss\n            \n    return min_wcss\n\ndef solve():\n    \"\"\"\n    Main function to run the cluster selection algorithm for all test cases.\n    \"\"\"\n    # Fixed seed for reproducibility\n    rng = np.random.default_rng(seed=12345)\n    max_iter_kmeans = 100 # Set a reasonable max iteration count for Lloyd's\n    den_tolerance = 1e-12\n\n    # (n_c, s, gamma, K_max, r)\n    test_cases = [\n        (200, 4.0, 1.3, 8, 10),\n        (200, 2.5, 1.2, 8, 10),\n        (200, 1.0, 1.2, 8, 10),\n        (200, 0.0, 1.2, 8, 10),\n    ]\n\n    results = []\n    \n    for case_params in test_cases:\n        n_c, s, gamma, K_max, r = case_params\n        \n        # 1. Generate synthetic data\n        n_features = 2\n        means = np.array([[-s, 0], [s, 0], [0, s]])\n        cov = np.identity(n_features) * 1.0  # sigma=1\n        \n        data_parts = []\n        for i in range(3):\n            data_parts.append(rng.multivariate_normal(means[i], cov, size=n_c))\n        X = np.vstack(data_parts)\n        \n        # 2. Compute WCSS curve\n        wcss_values = []\n        for k_val in range(1, K_max + 1):\n            wcss = run_kmeans(X, k_val, r, max_iter_kmeans, rng)\n            wcss_values.append(wcss)\n            \n        # 3. Apply the selection rule\n        k_search_range = range(2, K_max)\n        r_values = {}\n        \n        for k_val in k_search_range:\n            # W(k-1) is at index k-2, W(k) at k-1, W(k+1) at k\n            w_km1 = wcss_values[k_val - 2]\n            w_k = wcss_values[k_val - 1]\n            w_kp1 = wcss_values[k_val]\n            \n            numerator = w_km1 - w_k\n            denominator = w_k - w_kp1\n            \n            if denominator = den_tolerance:\n                r_values[k_val] = np.inf\n            else:\n                r_values[k_val] = numerator / denominator\n\n        # Find smallest k such that R(k) >= gamma\n        selected_k = -1\n        for k_val in sorted(r_values.keys()):\n            if r_values[k_val] >= gamma:\n                selected_k = k_val\n                break\n        \n        # If no such k exists, find k that maximizes R(k)\n        if selected_k == -1:\n            if not r_values: # This can happen if K_max  3\n                 # Problem constraints ensure K_max is at least 8, so r_values is non-empty.\n                 # This branch is for robustness, but not expected to be hit.\n                 # As no k in the search range can be chosen, an error value might be appropriate.\n                 # However, based on the problem, we must choose from the range.\n                 # With K_max=8, the range is [2, 7], so there is always something to choose.\n                pass\n            \n            # Find the k corresponding to the maximum R value.\n            # max() on dict.items() with a key lambda is one way.\n            # np.argmax on a list of values is more direct.\n            k_options = list(r_values.keys())\n            r_option_values = [r_values[k] for k in k_options]\n            \n            # np.argmax returns the first index of the maximum, which handles tie-breaking\n            # (select smallest k) correctly.\n            best_k_idx = np.argmax(r_option_values)\n            selected_k = k_options[best_k_idx]\n\n        results.append(selected_k)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```", "id": "3107505"}, {"introduction": "在使用基于距离的聚类算法（如 $k$-means）时，一个常被忽视却至关重要的步骤是特征缩放。由于 $k$-means 旨在最小化欧几里得距离的平方和，数值范围或方差较大的特征会不成比例地主导距离计算，可能掩盖其他特征中包含的真实结构。本练习将通过构建巧妙的假想情景，让您亲手验证特征归一化如何能够揭示或混淆聚类结构，从而深刻理解数据预处理在聚类分析中的关键作用。[@problem_id:3107563]", "problem": "要求您实现一个实验，量化在欧几里得$k$-均值方法中选择簇数量时，特征级单位方差归一化如何影响簇内平方和（WCSS）曲线中肘部的检测。该实验必须实现为一个完整、可运行的程序。关注的关键对象是簇内平方和（WCSS）以及特征缩放对欧几里得距离的影响。\n\n使用以下基本原理：\n- 欧几里得$k$-均值算法旨在最小化簇内平方和（WCSS），其定义为 $$W(k) \\;=\\; \\sum_{i=1}^{n} \\left\\| x_i - \\mu_{\\mathrm{cluster}(i)} \\right\\|_2^2,$$ 其中 $x_i \\in \\mathbb{R}^d$ 是第 $i$ 个数据向量，$\\mu_{\\mathrm{cluster}(i)}$ 是 $x_i$ 所属簇的质心，$n$ 是样本数，$d$ 是特征数，$k$ 是簇的数量。\n- 特征级单位方差归一化是指使用以下公式转换每个特征 $j \\in \\{1,\\dots,d\\}$：$$x_{ij}' \\;=\\; \\frac{x_{ij} - \\bar{x}_j}{s_j},$$ 其中 $\\bar{x}_j$ 是特征 $j$ 的样本均值，$s_j$ 是其样本标准差。如果 $s_j = 0$，则对所有样本保持该特征不变，即使用 $x_{ij}' = x_{ij}$。\n\n您必须实现以下步骤：\n1. 使用带有多次随机重启的 Lloyd 算法来实现欧几里得 $k$-均值，以确保稳健性。对于每个 $k$，返回最小化的 $W(k)$。\n2. 按如下方式实现肘部检测。考虑点集 $p_k = (k, W(k))$，其中 $k \\in \\{1,2,\\dots,K_{\\max}\\}$。将肘部索引 $\\hat{k}$ 定义为整数 $k \\in \\{2,\\dots,K_{\\max}-1\\}$，该整数使得点 $p_k$ 到经过 $p_1$ 和 $p_{K_{\\max}}$ 的直线的垂直距离最大化。您的程序必须使用基本的欧几里得几何精确计算此距离。\n\n构建三个合成数据集，每个数据集都具有指定的参数和随机种子，以形成一个测试套件。对于每个数据集，计算两次肘部索引：一次使用原始（未缩放）特征，另一次使用归一化为单位方差的特征。这些数据集是：\n\n- 测试用例1（两个特征，归一化揭示肘部）：\n  - 维度 $d = 2$。\n  - 真实簇数 $c = 3$。\n  - 每簇样本数 $n_c = 60$（总计 $n = 180$）。\n  - 随机种子 $= 123$。\n  - 簇构造：特征1是标准差为 $\\sigma_x = 12$ 的独立高斯噪声。特征2的簇均值位于 $-6$、 $0$ 和 $6$，簇内高斯噪声标准差为 $\\sigma_y = 0.6$。\n\n- 测试用例2（十个特征，归一化通过放大多噪声特征而掩盖了肘部）：\n  - 维度 $d = 10$。\n  - 真实簇数 $c = 3$。\n  - 每簇样本数 $n_c = 50$（总计 $n = 150$）。\n  - 随机种子 $= 321$。\n  - 簇构造：特征1携带簇信号，均值位于 $-12$、 $0$ 和 $12$，簇内高斯噪声标准差为 $\\sigma_{\\mathrm{sig}} = 0.5$。其余 $9$ 个特征是标准差为 $\\sigma_{\\mathrm{noise}} = 5$ 且均值为零的独立高斯噪声。\n\n- 测试用例3（包含零方差特征的边缘案例）：\n  - 维度 $d = 3$。\n  - 真实簇数 $c = 3$。\n  - 每簇样本数 $n_c = 40$（总计 $n = 120$）。\n  - 随机种子 $= 777$。\n  - 簇构造：特征1对所有样本均为常数 $0$（零方差）。特征2的簇均值位于 $-4$、 $0$ 和 $4$，簇内高斯噪声标准差为 $\\sigma_y = 0.7$。特征3是标准差为 $\\sigma_z = 1$ 且均值为零的独立高斯噪声。\n\n对于每个测试用例，使用如下的最大簇数 $K_{\\max}$：\n- 测试用例1：$K_{\\max} = 8$。\n- 测试用例2：$K_{\\max} = 8$。\n- 测试用例3：$K_{\\max} = 7$。\n\n实现要求：\n- 对于每个测试用例，完全按照规定生成数据。\n- 对于每个测试用例，计算两次肘部索引 $\\hat{k}$（原始和归一化）。\n- 为 $k$-均值使用 $5$ 次随机重启，每次重启最多迭代 $300$ 次。\n- 仅使用欧几里得距离和标准算术运算。\n- 完全按照规定处理零方差特征。\n\n最终输出规范：\n- 您的程序应生成单行输出，包含按以下顺序排列的六个肘部索引：$[\\hat{k}_{1,\\mathrm{raw}}, \\hat{k}_{1,\\mathrm{norm}}, \\hat{k}_{2,\\mathrm{raw}}, \\hat{k}_{2,\\mathrm{norm}}, \\hat{k}_{3,\\mathrm{raw}}, \\hat{k}_{3,\\mathrm{norm}}]$。\n- 输出必须是单行的 Python 列表字面量，整数之间用逗号分隔，并用方括号括起来（例如：`[2,3,3,2,2,3]`）。\n\n不涉及物理单位或角度。所有数值答案均为整数。确保程序是自包含的，在给定指定种子的情况下是确定性的，并且仅使用指定的库。", "solution": "该问题经评估有效。这是一个适定、有科学依据且客观的计算问题，属于统计学习领域。所有参数、定义和步骤都得到了明确的规定，从而能够得出唯一且可复现的解。\n\n解决方案通过实施指定的实验来推进，该实验涉及几个相互关联的组成部分：合成数据生成、特征归一化、$k$-均值算法的稳健实现以及用于肘部检测的几何方法。\n\n**1. 基本原理**\n\n问题的核心在于聚类算法和数据预处理的交叉点。欧几里得$k$-均值算法旨在最小化簇内平方和（WCSS），这是一个对特征尺度敏感的目标函数。\n\n- **簇内平方和（WCSS）：**对于给定的簇数 $k$，WCSS定义为\n$$W(k) \\;=\\; \\sum_{j=1}^{k} \\sum_{x_i \\in C_j} \\left\\| x_i - \\mu_j \\right\\|_2^2$$\n其中 $C_j$ 是第 $j$ 个簇中数据点的集合，$\\mu_j$ 是簇 $C_j$ 的质心，$\\| \\cdot \\|_2$ 是欧几里得范数。由于欧几里得范数对每个维度的差值平方求和，因此具有较大数据范围或方差的特征会对总距离产生不成比例的影响，可能掩盖其他特征中的潜在结构。\n\n- **特征归一化：**为缓解此问题，通常会对特征进行缩放。本题指定了单位方差归一化（标准化）。对于每个特征 $j$，变换公式为\n$$x_{ij}' \\;=\\; \\frac{x_{ij} - \\bar{x}_j}{s_j}$$\n其中 $\\bar{x}_j$ 是特征 $j$ 的样本均值，$s_j$ 是其样本标准差。样本标准差计算公式为 $s_j = \\sqrt{\\frac{1}{n-1}\\sum_{i=1}^n(x_{ij}-\\bar{x}_j)^2}$。此变换使每个特征的均值为 $0$，标准差为 $1$。问题规定，如果一个特征的方差为零（$s_j = 0$），则应保持其不变，即 $x_{ij}' = x_{ij}$。\n\n**2. 算法实现**\n\n整个实验被封装在一个程序中，该程序为三个测试用例中的每一个系统地执行所需的步骤。\n\n- **数据生成：**对于每个测试用例，使用带种子的随机数生成器（`numpy.random.default_rng`）根据精确的规范生成一个合成数据集，以确保可复现性。使用参数（维度 $d$、真实簇数 $c$、每簇样本数 $n_c$、均值和标准差）来构建聚类结构先验已知的数据矩阵。\n\n- **$k$-均值算法：**基于 Lloyd 算法开发了一个稳健的欧几里得 $k$-均值实现。\n    - **初始化：**为了开始 Lloyd 算法的一次迭代，通过从数据集中随机选择 $k$ 个不同的数据点（Forgy 方法）来选择 $k$ 个初始质心。\n    - **迭代：**该算法分两步进行：\n        1. **分配步骤：**每个数据点 $x_i$ 被分配到与其最近的质心相对应的簇，从而最小化 $\\| x_i - \\mu_j \\|_2^2$。这可以通过平方欧几里得距离高效计算。\n        2. **更新步骤：**每个簇的质心 $\\mu_j$ 被重新计算为分配给该簇的所有数据点的平均值。如果某个簇变为空，则保留其前一次迭代的质心位置，以保持稳定性和恒定的簇数。\n    - **收敛：**当质心位置在迭代之间不再变化或达到最大迭代次数（$300$）时，迭代过程停止。\n    - **稳健性：**为了避免 $k$-均值中常见的局部最优解，整个 Lloyd 算法使用不同的随机初始化运行 $5$ 次（重启）。在这些重启中产生最小 WCSS（即 $W(k)$）的聚类结果被选为该 $k$ 值的结果。\n\n- **肘部检测：**WCSS 曲线中的“肘部”是确定最优簇数的一种启发式方法。本题定义了一种确定性的方法来定位它。\n    - 通过为每个 $k \\in \\{1, 2, \\dots, K_{\\max}\\}$ 运行 $k$-均值算法，生成一系列点 $p_k = (k, W(k))$。\n    - 定义一条穿过第一个点 $p_1 = (1, W(1))$ 和最后一个点 $p_{K_{\\max}} = (K_{\\max}, W(K_{\\max}))$ 的直线 $L$。\n    - 对于每个中间点 $p_k$（其中 $k \\in \\{2, \\dots, K_{\\max}-1\\}$），计算其到直线 $L$ 的垂直距离。从点 $p_0$ 到由点 $p_a$ 和 $p_b$ 定义的直线的距离可以使用平行四边形面积公式求得：\n    $$ \\text{distance} = \\frac{|\\det(\\vec{v}, \\vec{w})|}{\\|\\vec{v}\\|} = \\frac{|v_x w_y - v_y w_x|}{\\sqrt{v_x^2 + v_y^2}} $$\n    其中 $\\vec{v} = p_b - p_a$ 且 $\\vec{w} = p_0 - p_a$。\n    - 肘部索引 $\\hat{k}$ 是使该垂直距离最大化的 $k$ 值。\n\n- **实验流程：**对于三个测试用例中的每一个，整个过程执行两次：一次在原始未缩放的数据上，一次在应用特征级单位方差归一化后的数据上。收集得到的六个肘部索引（$\\hat{k}_{1,\\mathrm{raw}}, \\hat{k}_{1,\\mathrm{norm}}, \\hat{k}_{2,\\mathrm{raw}}, \\hat{k}_{2,\\mathrm{norm}}, \\hat{k}_{3,\\mathrm{raw}}, \\hat{k}_{3,\\mathrm{norm}}$）并作为最终输出呈现。该实验旨在展示归一化如何能够揭示隐藏的簇结构（测试用例1），或通过放大噪声来掩盖它们（测试用例2），同时也能正确处理零方差特征等边缘情况（测试用例3）。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.spatial.distance import cdist\n\ndef solve():\n    \"\"\"\n    Implements and runs the experiment to quantify the effect of feature normalization\n    on elbow detection in k-means clustering.\n    \"\"\"\n\n    def generate_data(case_id, d, c, n_c, rng):\n        \"\"\"Generates synthetic dataset for a given test case.\"\"\"\n        n_total = c * n_c\n        \n        if case_id == 1:\n            # Case 1: Normalization reveals the elbow\n            # Feature 1: High-variance noise\n            # Feature 2: Low-variance signal\n            x1 = rng.normal(loc=0, scale=12, size=n_total)\n            x2 = np.zeros(n_total)\n            means = [-6, 0, 6]\n            sigma_y = 0.6\n            for i, mean in enumerate(means):\n                start, end = i * n_c, (i + 1) * n_c\n                x2[start:end] = rng.normal(loc=mean, scale=sigma_y, size=n_c)\n            X = np.stack((x1, x2), axis=1)\n            \n        elif case_id == 2:\n            # Case 2: Normalization obscures the elbow\n            # Feature 1: Signal\n            # Features 2-10: Moderate-variance noise\n            x_sig = np.zeros(n_total)\n            means = [-12, 0, 12]\n            sigma_sig = 0.5\n            for i, mean in enumerate(means):\n                start, end = i * n_c, (i + 1) * n_c\n                x_sig[start:end] = rng.normal(loc=mean, scale=sigma_sig, size=n_c)\n            \n            x_noise = rng.normal(loc=0, scale=5, size=(n_total, d - 1))\n            X = np.concatenate((x_sig.reshape(-1, 1), x_noise), axis=1)\n\n        elif case_id == 3:\n            # Case 3: Edge case with a zero-variance feature\n            x1 = np.zeros(n_total) # Zero variance\n            x2 = np.zeros(n_total)\n            means = [-4, 0, 4]\n            sigma_y = 0.7\n            for i, mean in enumerate(means):\n                start, end = i * n_c, (i + 1) * n_c\n                x2[start:end] = rng.normal(loc=mean, scale=sigma_y, size=n_c)\n            \n            x3 = rng.normal(loc=0, scale=1, size=n_total)\n            X = np.stack((x1, x2, x3), axis=1)\n        \n        return X\n\n    def normalize_features(X):\n        \"\"\"Normalizes features to unit variance as specified.\"\"\"\n        X_norm = X.copy()\n        means = np.mean(X, axis=0)\n        # Use ddof=1 for sample standard deviation\n        stds = np.std(X, axis=0, ddof=1)\n        \n        for j in range(X.shape[1]):\n            if stds[j] > 1e-12: # Check for non-zero standard deviation\n                X_norm[:, j] = (X[:, j] - means[j]) / stds[j]\n            # If stds[j] is zero, the feature is left unchanged as per instructions\n        return X_norm\n\n    def _kmeans_single_run(X, k, max_iter, rng):\n        \"\"\"Performs a single run of Lloyd's algorithm for k-means.\"\"\"\n        n_samples = X.shape[0]\n        \n        # Forgy initialization: Choose k random distinct points as initial centroids\n        initial_indices = rng.choice(n_samples, size=k, replace=False)\n        centroids = X[initial_indices]\n        \n        for _ in range(max_iter):\n            # Assignment step: an O(N*k*d) operation, vectorized\n            distances_sq = cdist(X, centroids, 'sqeuclidean')\n            labels = np.argmin(distances_sq, axis=1)\n            \n            # Update step: Compute new centroids\n            new_centroids = np.copy(centroids)\n            for j in range(k):\n                points_in_cluster = X[labels == j]\n                if len(points_in_cluster) > 0:\n                    new_centroids[j] = np.mean(points_in_cluster, axis=0)\n                # Else: cluster is empty, retain previous centroid\n            \n            # Convergence check\n            if np.allclose(centroids, new_centroids):\n                break\n            \n            centroids = new_centroids\n        \n        # Final WCSS calculation\n        wcss = 0.0\n        for j in range(k):\n            points_in_cluster = X[labels == j]\n            if len(points_in_cluster) > 0:\n                dist_sq = np.sum((points_in_cluster - centroids[j])**2)\n                wcss += dist_sq\n                \n        return wcss\n\n    def kmeans_multirestart(X, k, n_restarts, max_iter, parent_rng):\n        \"\"\"Runs k-means with multiple random restarts and returns the best WCSS.\"\"\"\n        min_wcss = np.inf\n        \n        # Special case k=1\n        if k == 1:\n            centroid = np.mean(X, axis=0)\n            return np.sum((X - centroid)**2)\n            \n        # Generate seeds for each restart from the parent RNG for reproducibility\n        restart_seeds = parent_rng.integers(low=0, high=2**32 - 1, size=n_restarts)\n        \n        for i in range(n_restarts):\n            rng_restart = np.random.default_rng(restart_seeds[i])\n            wcss = _kmeans_single_run(X, k, max_iter, rng_restart)\n            if wcss  min_wcss:\n                min_wcss = wcss\n        return min_wcss\n        \n    def find_elbow(wcss_values, K_max):\n        \"\"\"Finds the elbow point using the perpendicular distance method.\"\"\"\n        points = np.array([(k, wcss) for k, wcss in enumerate(wcss_values, 1)])\n        \n        p1 = points[0]\n        p_Kmax = points[K_max - 1]\n        \n        line_vec = p_Kmax - p1\n        line_vec_norm_sq = np.sum(line_vec**2)\n        \n        if line_vec_norm_sq == 0:\n            return -1 # Should not happen in this problem\n            \n        distances = []\n        # k ranges from 2 to K_max - 1\n        for i in range(1, K_max - 1):\n            p_k = points[i]\n            point_vec = p_k - p1\n            \n            # Perpendicular distance using cross-product magnitude equivalent in 2D\n            numerator = np.abs(line_vec[0] * point_vec[1] - line_vec[1] * point_vec[0])\n            distance = numerator / np.sqrt(line_vec_norm_sq)\n            distances.append(distance)\n        \n        if not distances:\n            return -1 # K_max is too small\n\n        # +2 because distances index is 0..N-1, corresponds to k=2..K_max-1\n        elbow_k = np.argmax(distances) + 2\n        return int(elbow_k)\n\n    # --- Main Execution ---\n    test_cases = [\n        {'case_id': 1, 'd': 2, 'c': 3, 'n_c': 60, 'seed': 123, 'K_max': 8},\n        {'case_id': 2, 'd': 10, 'c': 3, 'n_c': 50, 'seed': 321, 'K_max': 8},\n        {'case_id': 3, 'd': 3, 'c': 3, 'n_c': 40, 'seed': 777, 'K_max': 7}\n    ]\n    \n    k_means_restarts = 5\n    k_means_max_iter = 300\n    \n    results = []\n    for case in test_cases:\n        # Create a single RNG for all randomness within a test case (data + kmeans)\n        # This ensures that both raw and norm runs use the same sequence of random inits\n        case_rng = np.random.default_rng(case['seed'])\n        \n        X = generate_data(\n            case_id=case['case_id'], d=case['d'], c=case['c'], \n            n_c=case['n_c'], rng=case_rng\n        )\n        K_max = case['K_max']\n        \n        # 1. Run on raw data\n        wcss_raw = []\n        for k in range(1, K_max + 1):\n            wcss = kmeans_multirestart(X, k, k_means_restarts, k_means_max_iter, case_rng)\n            wcss_raw.append(wcss)\n        elbow_raw = find_elbow(wcss_raw, K_max)\n        results.append(elbow_raw)\n        \n        # 2. Run on normalized data\n        X_norm = normalize_features(X)\n        wcss_norm = []\n        for k in range(1, K_max + 1):\n            wcss = kmeans_multirestart(X_norm, k, k_means_restarts, k_means_max_iter, case_rng)\n            wcss_norm.append(wcss)\n        elbow_norm = find_elbow(wcss_norm, K_max)\n        results.append(elbow_norm)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```", "id": "3107563"}, {"introduction": "“肘部法则”是否总是有效？答案是否定的，它的成功依赖于数据在几何上满足某些隐含的假设。本练习将引导您进行一次富有启发性的探索，首先构建一个肘部法则的“对抗样本”——一组均匀分布在圆上的点，您会发现其 WCSS 曲线平坦无奇，无法提供明确的 $k$ 值。接着，您将通过一个简单的几何变形（将圆形压成椭圆）来打破这种对称性，并观察到一个清晰的“肘部”如何重新出现，从而深入理解肘部法则生效的几何本质。[@problem_id:3107514]", "problem": "你的任务是评估簇内平方和如何随着聚类数量的变化而变化，并构建一个对于基于视觉“肘部”选择聚类数量具有对抗性的数据集。考虑最小化总簇内欧几里得距离平方和的经典划分方法。设数据集为有限点集 $\\{x_i\\}_{i=1}^n \\subset \\mathbb{R}^d$。对于给定的整数 $k \\ge 1$，将簇内平方和 $W(k)$ 定义为，在所有将点分配到 $k$ 个聚类以及所有选择 $k$ 个质心的方案中，每个点到其所属聚类质心的平方距离之和的最小值。即，\n$$\nW(k) \\equiv \\min_{\\text{partitions into }k\\text{ clusters}} \\sum_{c=1}^k \\sum_{x_i \\in C_c} \\|x_i - \\mu_c\\|_2^2,\n$$\n其中 $C_c$ 表示第 $c$ 个聚类，$\\mu_c$ 是 $C_c$ 的质心。\n\n你的程序必须实现一个算法，使用标准的 Lloyd 迭代优化（通常称为 $k$-均值）并进行谨慎的初始化来近似 $W(k)$。你随后必须通过在球面上放置点来构建一个对抗性数据集，该数据集在指定的 $k$ 值范围内使 $W(k)$ 曲线变得平坦，并提出和实现该数据集的一个确定性形变，该形变能重新引入一个可见的肘部，并解释肘部出现的原因。\n\n使用二维设置 $\\mathbb{R}^2$ 和单位圆进行球面构建。具体来说：\n- 通过将点放置在角度 $\\theta_j = \\frac{2\\pi j}{n}$ (其中 $j = 0, 1, \\dots, n - 1$，以弧度计) 并通过 $x_j = (r \\cos \\theta_j, r \\sin \\theta_j)$ 映射到笛卡尔坐标，在半径 $r = 1$ 的单位圆上生成 $n$ 个均匀分布的点。\n- 为近似给定数据集的 $W(k)$，实现带有 $k$-means++ 风格种子和多次重启的 Lloyd 算法。使用平方欧几里得距离 $\\|\\cdot\\|_2^2$。当需要时，通过将质心重新初始化到最远的点来稳健地处理空簇。在给定固定随机种子的情况下，该算法必须是确定性的。\n\n定义以下量化标准：\n1. 平坦线检测。对于连续整数范围 $\\{k_{\\min}, k_{\\min} + 1, \\dots, k_{\\max}\\}$，计算序列 $W(k)$ 和归一化的连续下降量\n$$\n\\Delta(k) = \\frac{W(k) - W(k+1)}{W(1)} \\quad \\text{for } k \\in \\{k_{\\min}, \\dots, k_{\\max}-1\\}.\n$$\n如果对于给定的阈值 $\\tau \\in (0,1)$，$\\max_{k} \\Delta(k) \\le \\tau$ 成立，则宣布曲线“在该范围内平坦化”。\n\n2. 通过离散曲率进行肘部检测。对于整数 $k \\in \\{2, \\dots, K-1\\}$，定义离散二阶差分\n$$\nD(k) = W(k-1) - 2 W(k) + W(k+1).\n$$\n返回在 $k \\in \\{2, \\dots, K-1\\}$ 上使 $D(k)$ 最大化的肘部位置 $k^\\star$。若有平局，必须通过选择最小的 $k$ 来打破。\n\n构建对抗性数据集（圆上的点）和一个通过在垂直方向上以因子 $s \\in (0,1)$ 进行各向异性缩放而获得的形变数据集，即，将每个点 $(x, y)$ 映射到 $(x, s y)$，其中 $s = 0.5$。\n\n你的程序必须为下面指定的测试套件产生输出。在任何使用到的地方，角度都应以弧度解释。\n\n测试套件：\n- 测试用例 1 (对抗性平坦线检查)：\n    - 数据集：半径 $r = 1$ 的单位圆上的 $n = 360$ 个点。\n    - 评估所有整数 $k \\in \\{1, 2, \\dots, 12\\}$ 的 $W(k)$。\n    - 平坦线检测范围：$k_{\\min} = 6$, $k_{\\max} = 12$。\n    - 阈值：$\\tau = 0.025$。\n    - 输出：一个布尔值，指示曲线在 $\\{6, 7, \\dots, 12\\}$ 范围内是否根据上述标准平坦化。\n\n- 测试用例 2 (形变后的肘部)：\n    - 数据集：取半径 $r = 1$ 的相同 $n = 360$ 个圆上点，然后通过垂直缩放因子 $s = 0.5$ 进行形变。\n    - 评估所有整数 $k \\in \\{1, 2, \\dots, 12\\}$ 的 $W(k)$。\n    - 使用离散二阶差分 $D(k)$ 在 $\\{2, 3, \\dots, 11\\}$ 范围内如上所述选择肘部 $k^\\star$。\n    - 输出：整数 $k^\\star$。\n\n- 测试用例 3 (边界范围非平坦性检查)：\n    - 数据集：半径 $r = 1$ 的单位圆上的 $n = 360$ 个点。\n    - 评估所有整数 $k \\in \\{1, 2, \\dots, 12\\}$ 的 $W(k)$。\n    - 平坦线检测范围：$k_{\\min} = 2$, $k_{\\max} = 5$。\n    - 阈值：$\\tau = 0.03$。\n    - 输出：一个布尔值，指示曲线在 $\\{2, 3, 4, 5\\}$ 范围内是否根据上述标准平坦化。\n\n你的程序应生成单行输出，其中包含用方括号括起来的逗号分隔列表形式的结果（例如，`[result1,result2,result3]`）。这三个结果必须分别按顺序对应于测试用例 1、2 和 3 的输出，并且类型必须为布尔型、整型和布尔型。", "solution": "当前任务要求对簇内平方和（记为 $W(k)$）作为聚类数量 $k$ 的函数进行严格检验。此分析旨在通过构建一个使“肘部法则”失效的数据集以及一个使其成功的数据集修改版，来展示这一选择最优 $k$ 值的常用启发式方法的局限性。\n\n### 原理 1：簇内平方和与 $k$-均值聚类\n\n一个数据集由 $d$ 维欧几里得空间 $\\mathbb{R}^d$ 中的 $n$ 个点 $\\{x_i\\}_{i=1}^n$ 集合给出。基于划分的聚类的目标是将这些点分组成 $k$ 个不同的、非空的集合或簇 $\\{C_1, C_2, \\dots, C_k\\}$，这些簇共同构成一个完备且互斥的划分。\n\n此任务的标准目标函数是总簇内平方和（$WSS$），它衡量了聚类的紧凑程度。对于一个给定的划分，它是每个点到其所属簇质心的欧几里得距离平方之和。簇 $C_c$ 的质心 $\\mu_c$ 是其算术平均值：$\\mu_c = \\frac{1}{|C_c|} \\sum_{x_i \\in C_c} x_i$。\n\n量 $W(k)$ 被定义为在所有将数据划分为 $k$ 个簇的可能划分中，可能的最小 $WSS$ 值：\n$$\nW(k) \\equiv \\min_{\\{C_c\\}_{c=1}^k} \\sum_{c=1}^k \\sum_{x_i \\in C_c} \\|x_i - \\mu_c\\|_2^2\n$$\n找到此函数的真正全局最小值是一个 NP-难问题。因此，采用迭代启发式算法来寻找一个好的局部最小值。其中最常用的是 Lloyd 算法，通常称为 $k$-均值算法。\n\n该算法按以下步骤进行：\n1.  **初始化**：选择 $k$ 个初始质心。明智的选择对获得好的结果至关重要。$k$-means++ 种子策略是一种行之有效的方法，它倾向于选择分布良好的初始质心，从而提高算法的质量和收敛速度。\n2.  **迭代**：重复以下两个步骤直至收敛。\n    a.  **分配步骤**：将每个数据点 $x_i$ 分配到与最近质心 $\\mu_c$ 对应的簇 $C_c$ 中，即 $\\|x_i - \\mu_c\\|_2^2$ 最小化的簇。\n    b.  **更新步骤**：将每个簇的质心 $\\mu_c$ 重新计算为分配给该簇的所有点的均值。\n3.  **终止**：当聚类分配在迭代之间不再改变时，算法收敛。\n\n由于该算法对初始条件敏感，标准做法是使用不同的随机初始化（重启）运行多次，并选择产生最小 $W(k)$ 的聚类结果。\n\n### 原理 2：“肘部法则”及其对抗性案例\n\n函数 $W(k)$ 是 $k$ 的单调递减函数。随着 $k$ 的增加，点被划分到更小、更多的簇中，这不可避免地减少了到各自质心的平方距离之和。$W(1)$ 是数据相对于全局均值的总平方和，如果所有点都不同，则 $W(n) = 0$。\n\n肘部法是一种视觉启发式方法，用于估计 $k$ 的一个“好”值。人们绘制 $W(k)$ 关于 $k$ 的图，并寻找一个“肘部”点，在该点 $W(k)$ 的下降速率急剧减缓。其直觉是，该点代表了模型复杂度（更多的簇）和解释力（更低的 $WSS$）之间的权衡。\n\n该方法的一个**对抗性数据集**是指不产生明显肘部的数据集。一个典型的例子是具有高度对称性分布的点集，例如在球面上均匀分布的点。在指定的 $\\mathbb{R}^2$ 情况下，我们使用半径 $r=1$ 的单位圆上的 $n=360$ 个点，其坐标为 $(r \\cos \\theta_j, r \\sin \\theta_j)$，角度为 $\\theta_j = \\frac{2\\pi j}{n}$。\n\n这个数据集的旋转对称性意味着没有内在“正确”的聚类数量（除了 $k=1$ 或 $k=n$）。$k$ 个质心的最优放置将趋向于一个内接在圆内的正 $k$-边形。随着 $k$ 的增加，$W(k)$ 的减少非常平滑和渐进。没有任何一点，在增加一个聚类后会提供不成比例的巨大好处，因此 $W(k)$ 曲线缺乏明显的肘部。这可以通过归一化的连续下降量 $\\Delta(k) = \\frac{W(k) - W(k+1)}{W(1)}$ 来量化。对于圆形数据，该值预计会很小，并且在 $k$ 的范围内变化缓慢，导致曲线根据问题的标准显得“平坦化”。\n\n### 原理 3：通过各向异性缩放重新引入结构\n\n为了恢复一个显著的肘部，必须打破数据集的对称性。问题提出了一个各向异性缩放变换，将每个点 $(x, y)$ 映射到 $(x, s y)$，缩放因子 $s = 0.5$。这将单位圆变换为一个沿 $x$ 轴的半长轴为 $1$、沿 $y$ 轴的半短轴为 $0.5$ 的椭圆。\n\n这种形变对点的分布有关键影响。虽然点保持了它们在圆上的顺序，但连续点之间的欧几里得距离发生了变化。点在椭圆曲率高的两端 $(\\pm 1, 0)$ 附近变得密集，而在曲率低的顶部和底部 $(0, \\pm 0.5)$ 附近变得稀疏。\n\n这创造了一个清晰的结构特征：两组密集的点。$k=2$ 的聚类可以通过在每个高密度区域放置一个质心来有效捕获这种结构。这导致从 $k=1$（单个质心在原点）移动到 $k=2$ 时，$WSS$ 有非常显著的减少。对于 $k>2$，随后的 $WSS$ 减少不那么剧烈。这种急剧的初始下降之后是更渐进的减少，在 $k=2$ 处形成了一个突出的肘部。\n\n这个视觉上的肘部可以通过找到离散二阶差分 $D(k) = W(k-1) - 2W(k) + W(k+1)$ 的最大值来定量检测。这个值是二阶导数的近似，并且在像肘部这样的高凸性点处值很大。因此，我们预期对于形变后的数据集，$k^\\star = \\arg\\max_k D(k)$ 会是 $2$。\n\n### 算法实现策略\n\n解决方案将作为一个 Python 程序实现，其结构如下：\n\n1.  **数据生成**：一个函数将生成两个所需的数据集：单位圆上的 $n=360$ 个点以及这些点的各向异性缩放版本。\n2.  **$k$-均值算法**：一个核心函数将实现 $k$-均值算法。\n    -   它将使用多次重启来确保高质量的解。\n    -   每次运行都将使用 $k$-means++ 种子方法进行初始化，以实现确定性和稳健的行为，由一个主随机种子控制。\n    -   迭代过程将通过将空簇的质心重新播种到距离任何现有非空质心最远的数据点来处理空簇。\n3.  **$W(k)$ 计算**：一个包装函数将通过重复调用 $k$-均值函数来计算给定数据集在指定 $k$ 值范围内的整个 $W(k)$ 曲线。\n4.  **测试用例评估**：\n    -   **平坦线检查**：一个函数将通过计算圆形数据集在指定范围内的归一化下降量来实现标准 $\\max_{k} \\Delta(k) \\le \\tau$。\n    -   **肘部检测**：一个函数将计算形变数据集的离散二阶差分 $D(k)$，并找到使该值最大化的 $k^\\star$，平局则通过选择较小的 $k$ 来打破。\n5.  **主执行**：脚本的主要部分将协调这些组件来执行三个测试用例，收集结果（两个布尔值和一个整数），并以指定格式打印它们。使用 `numpy` 和 `scipy.spatial.distance.cdist` 将确保高效的向量化计算。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.spatial.distance import cdist\n\n# Global seed for full determinism of the stochastic k-means algorithm.\nRANDOM_SEED = 42\n\ndef _kmeans_plusplus_init(data, k, rng):\n    \"\"\"\n    Initializes k-means centroids using the k-means++ algorithm for a single run.\n    This ensures deterministic initialization given a random number generator.\n    \"\"\"\n    n_samples, n_features = data.shape\n    centroids = np.zeros((k, n_features))\n\n    # 1. Choose the first centroid uniformly at random from the data points.\n    first_idx = rng.choice(n_samples)\n    centroids[0] = data[first_idx]\n\n    # 2. For each subsequent centroid, choose it with probability proportional to D(x)^2.\n    for i in range(1, k):\n        # Calculate the squared distance from each point to the nearest centroid.\n        dist_sq = cdist(data, centroids[:i, :], 'sqeuclidean')\n        min_dist_sq = np.min(dist_sq, axis=1)\n\n        # Create a probability distribution and choose the next centroid.\n        sum_dist_sq = np.sum(min_dist_sq)\n        if sum_dist_sq == 0:  # Edge case: all points are centroids\n            probs = None # Uniform probability\n        else:\n            probs = min_dist_sq / sum_dist_sq\n        \n        next_idx = rng.choice(n_samples, p=probs)\n        centroids[i] = data[next_idx]\n\n    return centroids\n\ndef _single_kmeans_run(data, k, max_iter, rng):\n    \"\"\"\n    Performs a single run of Lloyd's k-means algorithm.\n    \"\"\"\n    centroids = _kmeans_plusplus_init(data, k, rng)\n\n    for _ in range(max_iter):\n        # Assignment step: assign each point to the nearest centroid.\n        dist_sq = cdist(data, centroids, 'sqeuclidean')\n        labels = np.argmin(dist_sq, axis=1)\n\n        # Update step: recalculate centroids as the mean of assigned points.\n        new_centroids = np.zeros_like(centroids)\n        counts = np.bincount(labels, minlength=k)\n        \n        non_empty_indices = np.where(counts > 0)[0]\n        empty_indices = np.where(counts == 0)[0]\n\n        for j in non_empty_indices:\n            new_centroids[j] = data[labels == j].mean(axis=0)\n\n        # Robustly handle empty clusters as per the problem description.\n        if len(empty_indices) > 0:\n            active_centroids = new_centroids[non_empty_indices]\n            \n            if len(active_centroids) > 0:\n                dists_to_active = cdist(data, active_centroids, 'sqeuclidean')\n                min_dists_sq = np.min(dists_to_active, axis=1)\n                \n                # To handle multiple empty clusters, find a set of distinct farthest points.\n                farthest_indices = np.argsort(min_dists_sq)[-len(empty_indices):][::-1]\n\n                for i, empty_idx in enumerate(empty_indices):\n                    new_centroids[empty_idx] = data[farthest_indices[i]]\n            else:\n                 # This case (all clusters empty) is unlikely with this problem's parameters.\n                 # If it occurs, re-initialize all centroids.\n                 new_centroids = _kmeans_plusplus_init(data, k, rng)\n\n        # Check for convergence.\n        if np.allclose(centroids, new_centroids):\n            break\n        \n        centroids = new_centroids\n\n    # Calculate final WSS.\n    final_dist_sq = cdist(data, centroids, 'sqeuclidean')\n    final_labels = np.argmin(final_dist_sq, axis=1)\n    wss = np.sum(final_dist_sq[np.arange(len(data)), final_labels])\n    \n    return wss\n\ndef _kmeans(data, k, num_restarts, seed_rng):\n    \"\"\"\n    Runs k-means with multiple restarts and returns the best WSS.\n    \"\"\"\n    best_wss = np.inf\n    # Spawn new independent random number generators for each restart for reproducibility.\n    child_seeds = seed_rng.spawn(num_restarts)\n    \n    for i in range(num_restarts):\n        run_rng = np.random.default_rng(child_seeds[i])\n        wss = _single_kmeans_run(data, k, max_iter=100, rng=run_rng)\n        if wss  best_wss:\n            best_wss = wss\n            \n    return best_wss\n\ndef _generate_points(n, r, s=None):\n    \"\"\"\n    Generates n points on a circle, optionally applying anisotropic scaling.\n    \"\"\"\n    angles = np.linspace(0, 2 * np.pi, n, endpoint=False)\n    points = np.zeros((n, 2))\n    points[:, 0] = r * np.cos(angles)\n    points[:, 1] = r * np.sin(angles)\n    \n    if s is not None:\n        points[:, 1] *= s\n        \n    return points\n\ndef _compute_w_curve(data, k_range, num_restarts=10, rng=None):\n    \"\"\"\n    Computes the W(k) curve for a range of k values.\n    \"\"\"\n    if rng is None:\n        # Create a top-level generator for seeding the k-means runs.\n        rng = np.random.default_rng(RANDOM_SEED)\n\n    w_values = []\n    for k in k_range:\n        if k == 1:\n            centroid = np.mean(data, axis=0)\n            wss = np.sum(cdist(data, centroid.reshape(1, -1), 'sqeuclidean'))\n            w_values.append(wss)\n        else:\n            w_values.append(_kmeans(data, k, num_restarts, rng))\n    return w_values\n\ndef _check_flatline(w_values, k_min, k_max, tau):\n    \"\"\"\n    Checks if the W(k) curve is \"flatlined\" over a given range.\n    \"\"\"\n    w1 = w_values[0] # Corresponds to W(1)\n    max_delta = 0.0\n    for k in range(k_min, k_max):\n        # w_values is 0-indexed, so W(k) is at index k-1.\n        wk = w_values[k - 1]\n        wk_plus_1 = w_values[k]\n        delta_k = (wk - wk_plus_1) / w1\n        if delta_k > max_delta:\n            max_delta = delta_k\n    return max_delta = tau\n\ndef _find_elbow(w_values, K):\n    \"\"\"\n    Finds the elbow location k* using the discrete second difference.\n    \"\"\"\n    # D(k) is defined for k in {2, ..., K-1}\n    # w_values[i] corresponds to W(i+1)\n    D_values = []\n    for k in range(2, K):\n        w_km1 = w_values[k - 2]  # W(k-1)\n        w_k = w_values[k - 1]    # W(k)\n        w_kp1 = w_values[k]      # W(k+1)\n        Dk = w_km1 - 2 * w_k + w_kp1\n        D_values.append(Dk)\n        \n    # argmax breaks ties by choosing the first occurrence (smallest k).\n    # The calculated D_values correspond to k = 2, 3, ..., so we add 2 to the index.\n    best_k_idx = np.argmax(D_values)\n    return best_k_idx + 2\n\ndef solve():\n    # Define parameters from the problem statement.\n    n = 360\n    r = 1.0\n    s_deform = 0.5\n    k_max_eval = 12\n    k_range = list(range(1, k_max_eval + 1))\n    \n    # Create a master random number generator to ensure all parts of the\n    # calculation are deterministic from a single seed.\n    main_rng = np.random.default_rng(RANDOM_SEED)\n\n    # Generate datasets.\n    points_circle = _generate_points(n, r)\n    points_deformed = _generate_points(n, r, s=s_deform)\n\n    # Pre-calculate W(k) curves to avoid re-computation for tests 1 and 3.\n    w_circle = _compute_w_curve(points_circle, k_range, rng=main_rng)\n    w_deformed = _compute_w_curve(points_deformed, k_range, rng=main_rng)\n    \n    # --- Test Case 1: Adversarial flatline check ---\n    k_min1, k_max1, tau1 = 6, 12, 0.025\n    result1 = _check_flatline(w_circle, k_min1, k_max1, tau1)\n\n    # --- Test Case 2: Elbow after deformation ---\n    # elbow detection range is k in {2, ..., K-1}, and K=12 for the curve.\n    result2 = _find_elbow(w_deformed, k_max_eval)\n\n    # --- Test Case 3: Boundary-range non-flatness check ---\n    k_min3, k_max3, tau3 = 2, 5, 0.03\n    result3 = _check_flatline(w_circle, k_min3, k_max3, tau3)\n\n    # Combine results and print in the specified format.\n    results = [result1, result2, result3]\n    # The print must exactly match the format '[result1,result2,result3]'\n    # Python's str() for bool is 'True'/'False', which is correct.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3107514"}]}