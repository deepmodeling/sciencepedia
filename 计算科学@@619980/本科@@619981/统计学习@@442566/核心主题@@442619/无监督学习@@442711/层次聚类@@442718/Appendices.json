{"hands_on_practices": [{"introduction": "层次聚类，特别是像 Ward 这样的方法，对特征的尺度非常敏感。由于 Ward 方法旨在最小化簇内方差，数值范围较大的特征会对聚类结构产生不成比例的影响。本练习 [@problem_id:3129004] 将引导你亲手探索这一关键概念，通过比较原始数据与标准化数据在聚类过程中的合并高度，让你掌握数据预处理中至关重要的一课。", "problem": "您将获得一组数据矩阵，并需要研究特征缩放对使用Ward最小方差准则的凝聚式层次聚类的影响。本研究的基础是簇内离散度的定义以及由合并两个簇所引起的簇内平方和的增量。设 $X \\in \\mathbb{R}^{n \\times d}$ 表示一个包含 $n$ 个样本和 $d$ 个特征的数据矩阵。将簇 $C \\subset \\{1,\\dots,n\\}$ 在特征 $j$ 上的均值定义为 $\\mu_{C,j} = \\frac{1}{|C|}\\sum_{i \\in C} x_{ij}$。对于两个不相交的簇 $A$ 和 $B$，Ward 合并高度（因合并 $A$ 和 $B$ 而导致的簇内平方和增量）为\n$$\n\\Delta(A,B;X) = \\frac{|A|\\,|B|}{|A|+|B|}\\sum_{j=1}^d \\left( \\mu_{A,j} - \\mu_{B,j} \\right)^2.\n$$\n特征标准化按列定义，即减去特征均值并除以总体标准差。对于特征 $j$，设 $\\bar{x}_j = \\frac{1}{n}\\sum_{i=1}^n x_{ij}$ 且 $\\sigma_j = \\sqrt{\\frac{1}{n}\\sum_{i=1}^n (x_{ij}-\\bar{x}_j)^2}$。标准化数据矩阵 $Z \\in \\mathbb{R}^{n \\times d}$ 定义为：当 $\\sigma_j  0$ 时，$z_{ij} = \\frac{x_{ij}-\\bar{x}_j}{\\sigma_j}$；当 $\\sigma_j = 0$ 时，对所有 $i$，$z_{ij} = 0$（常数特征列无法重新缩放至单位方差，在中心化后设为零）。\n\n任务：\n- 对每个给定的测试矩阵 $X$，在原始矩阵 $X$ 上使用Ward方法执行凝聚式层次聚类，以获得合并序列。使用此合并序列，为每个合并步骤 $t$ 计算原始合并高度 $\\Delta_t^{\\text{raw}} = \\Delta(A_t,B_t;X)$ 和在同一簇对上计算的标准化合并高度 $\\Delta_t^{\\text{std}} = \\Delta(A_t,B_t;Z)$，其中 $Z$ 是 $X$ 的标准化版本（使用上述规则），而 $(A_t,B_t)$ 是根据原始数据Ward过程在步骤 $t$ 合并的簇。\n- 通过计算标量来量化由标准化引起的合并高度变化\n$$\nS_{\\text{total}} = \\sum_{t=1}^{n-1} \\left| \\Delta_t^{\\text{std}} - \\Delta_t^{\\text{raw}} \\right|.\n$$\n- 识别驱动这些变化的变量。对于每个合并步骤 $t$，将高度差异分解为按特征的贡献：\n$$\nc_{j}(t) = \\frac{|A_t|\\,|B_t|}{|A_t|+|B_t|}\\left[\\left(\\mu_{A_t,j}^{\\text{std}} - \\mu_{B_t,j}^{\\text{std}}\\right)^2 - \\left(\\mu_{A_t,j}^{\\text{raw}} - \\mu_{B_t,j}^{\\text{raw}}\\right)^2\\right],\n$$\n其中 $\\mu_{C,j}^{\\text{raw}}$ 和 $\\mu_{C,j}^{\\text{std}}$ 分别是在 $X$ 和 $Z$ 上计算的簇均值。对每个特征 $j$，聚合所有合并步骤的绝对贡献：\n$$\nS_j = \\sum_{t=1}^{n-1} \\left| c_j(t) \\right|.\n$$\n将驱动变量集合定义为使 $S_j$ 达到最大值的索引 $j$（包括所有平局情况），并以从零开始的升序索引报告。\n\n您的程序必须为以下数据矩阵测试套件实现上述计算。每个矩阵都已明确指定。请确保数值合理性和内部一致性，并使用下方的确切值。\n\n测试套件：\n- 测试 $1$（$n=6, d=3$）：\n$$\nX^{(1)} = \\begin{bmatrix}\n100    0    0.10 \\\\\n102    0.50    0.20 \\\\\n98    -0.20    0.00 \\\\\n0    5    -0.10 \\\\\n3    4.50    -0.30 \\\\\n-2    5.20    0.00\n\\end{bmatrix}.\n$$\n- 测试 $2$（$n=5, d=3$），第一列为常数：\n$$\nX^{(2)} = \\begin{bmatrix}\n10    -1    50 \\\\\n10    0    51 \\\\\n10    1    49 \\\\\n10    2    48 \\\\\n10    -2    52\n\\end{bmatrix}.\n$$\n- 测试 $3$（$n=8, d=4$），尺度差异巨大：\n$$\nX^{(3)} = \\begin{bmatrix}\n100.0    0.0    0.10    10000.0 \\\\\n101.5    -0.5    0.05    10002.0 \\\\\n98.5    0.3    0.15    9998.0 \\\\\n100.8    -0.2    0.12    10001.0 \\\\\n0.0    5.0    -0.10    10050.0 \\\\\n-1.5    4.5    -0.05    10052.0 \\\\\n2.0    5.2    -0.12    10049.0 \\\\\n-0.8    4.8    -0.08    10051.5\n\\end{bmatrix}.\n$$\n- 测试 $4$（$n=3, d=2$），最小案例：\n$$\nX^{(4)} = \\begin{bmatrix}\n0    0 \\\\\n10    0 \\\\\n0    10\n\\end{bmatrix}.\n$$\n\n输出规范：\n- 对每个测试矩阵 $X^{(k)}$，计算 $S_{\\text{total}}^{(k)}$ 和驱动变量索引集 $J^{(k)} = \\{ j : S_j \\text{ 为最大值} \\}$。\n- 将每个 $S_{\\text{total}}^{(k)}$ 四舍五入到 $6$ 位小数。\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表。每个测试用例的结果本身也是一个列表，形式为 $[S_{\\text{total}}^{(k)},[j_1,j_2,\\dots]]$，其中特征索引从零开始且按升序排列。例如：$[[s_1,[j_{1,1},j_{1,2}]], [s_2,[j_{2,1}]], [s_3,[j_{3,1},j_{3,2},j_{3,3}]], [s_4,[j_{4,1}]]]$。输出字符串中不允许有任何空格。\n\n约束和说明：\n- 在 $\\mathbb{R}^d$ 中使用欧几里得几何，并仅基于簇内平方和使用Ward最小方差准则。合并序列必须通过对原始数据矩阵 $X$ 应用Ward方法获得，并且标准化高度必须在相同的合并序列上评估。\n- 所有计算均为纯数值计算，不涉及物理单位。角度不适用。\n- 确保对 $\\sigma_j = 0$ 的常数特征列进行稳健处理；根据上述定义，这些列在中心化后变为零，并且对标准化差异没有贡献。", "solution": "经评估，用户提供的问题是**有效的**。这是一个定义明确且自成体系的统计学习计算任务，其基础是层次聚类和数据标准化的既定原则。问题陈述没有科学上的不健全、模糊或矛盾之处。\n\n### 基于原则的解决方案设计\n\n该问题要求分析特征标准化如何影响使用Ward方法的凝聚式层次聚类中的合并高度。解决方案围绕以下核心原则和计算步骤构建。\n\n1.  **特征标准化**：第一步是将原始数据矩阵 $X \\in \\mathbb{R}^{n \\times d}$ 转换为标准化矩阵 $Z \\in \\mathbb{R}^{n \\times d}$。这是许多机器学习算法中常见的预处理步骤，以确保尺度较大的特征不会主导分析。对每个特征 $j$ 指定的转换为 $z_{ij} = (x_{ij} - \\bar{x}_j) / \\sigma_j$，其中 $\\bar{x}_j$ 是特征均值，$\\sigma_j$ 是总体标准差。此过程将每个特征中心化至均值为 $0$，并将其缩放至标准差为 $1$。一个关键细节是处理 $\\sigma_j = 0$ 的常数特征。对于此类特征，问题规定对所有样本 $i$，标准化值 $z_{ij}$ 均设为 $0$。这是合乎逻辑的，因为常数特征不提供区分样本的信息，在中心化（减去其自身均值）后，它会变成一个全为零的列。\n\n2.  **Ward层次聚类**：聚类过程在**原始数据矩阵** $X$ 上执行。Ward最小方差法是凝聚式聚类的一种形式。它从将每个数据点视为其自身的簇开始，并在每一步合并能导致总簇内平方和 (ESS) 增量最小的簇对。总 ESS 是每个点与其所属簇的质心之间距离的平方和。合并两个簇 $A$ 和 $B$ 时 ESS 的增量由Ward合并高度的公式给出：\n    $$\n    \\Delta(A,B;X) = \\frac{|A|\\,|B|}{|A|+|B|}\\sum_{j=1}^d \\left( \\mu_{A,j} - \\mu_{B,j} \\right)^2\n    $$\n    其中 $|C|$ 是簇 $C$ 的大小，$\\mu_{C,j}$ 是簇 $C$ 在特征 $j$ 上的均值。此公式表示待合并簇的质心之间的欧几里得距离平方，并按一个取决于它们大小的因子进行加权。应用此方法会产生一个包含 $n-1$ 次合并的序列 $(A_t, B_t)_{t=1}^{n-1}$，该序列定义了层次结构。\n\n3.  **合并高度的比较分析**：分析的核心是比较在原始数据 $X$ 与标准化数据 $Z$ 上计算的合并高度。关键在于，合并序列是根据原始数据 $X$ 一次性确定的。对于这个固定序列中的每个合并步骤 $t$，计算两个量：\n    -   原始合并高度，$\\Delta_t^{\\text{raw}} = \\Delta(A_t, B_t; X)$。\n    -   标准化合并高度，$\\Delta_t^{\\text{std}} = \\Delta(A_t, B_t; Z)$。这是使用相同的公式计算的，但簇均值是根据标准化数据 $Z$ 推导出来的。\n\n4.  **变化的量化**：标准化的影响通过两个聚合指标来量化：\n    -   $S_{\\text{total}} = \\sum_{t=1}^{n-1} | \\Delta_t^{\\text{std}} - \\Delta_t^{\\text{raw}} |$：该标量衡量了整个聚类过程中合并高度的总绝对变化。\n    -   $S_j = \\sum_{t=1}^{n-1} | c_j(t) |$：该指标按特征分解变化。项 $c_j(t)$ 分离出特征 $j$ 在合并步骤 $t$ 对高度差异的贡献。将所有合并的绝对贡献相加，可以衡量特征 $j$ 对高度差异的总影响。具有较大 $S_j$ 的特征，其缩放会显著改变簇间距离，因此被识别为变化的“驱动变量”。驱动变量集合定义为与 $S_j$ 最大值相对应的特征索引。\n\n### 算法实现\n\n对每个提供的测试矩阵，通过以下步骤实现解决方案：\n\n1.  读取大小为 $n \\times d$ 的输入矩阵 $X$。\n2.  计算标准化矩阵 $Z$。对每列 $j$，计算均值 $\\bar{x}_j$ 和总体标准差 $\\sigma_j$。如果 $\\sigma_j  0$，则按规定计算 $z_{ij}$。如果 $\\sigma_j = 0$，则将 $Z$ 的第 $j$ 列设为零。\n3.  使用 `scipy.cluster.hierarchy.linkage` 和 `method='ward'` 在 $X$ 上执行Ward层次聚类。这将返回一个 $(n-1) \\times 4$ 的链接矩阵 $L$，该矩阵编码了合并序列。\n4.  初始化 $S_{\\text{total}} = 0$ 和一个长度为 $d$ 的零向量 $S_j$。\n5.  按照链接矩阵 $L$ 中的描述，遍历每次合并 $t=1, \\dots, n-1$：\n    a. 识别此步骤中正在合并的两个簇 $A_t$ 和 $B_t$。这通过从链接矩阵中递归地追溯簇索引到原始数据点来完成。\n    b. 计算大小 $|A_t|$ 和 $|B_t|$ 以及系数 $w_t = \\frac{|A_t|\\,|B_t|}{|A_t|+|B_t|}$。\n    c. 使用原始数据 ($\\mu_{A_t}^{\\text{raw}}, \\mu_{B_t}^{\\text{raw}}$) 和标准化数据 ($\\mu_{A_t}^{\\text{std}}, \\mu_{B_t}^{\\text{std}}$) 计算两个簇的质心向量。\n    d. 计算原始和标准化的合并高度：\n       $\\Delta_t^{\\text{raw}} = w_t \\sum_{j=1}^d (\\mu_{A_t,j}^{\\text{raw}} - \\mu_{B_t,j}^{\\text{raw}})^2$\n       $\\Delta_t^{\\text{std}} = w_t \\sum_{j=1}^d (\\mu_{A_t,j}^{\\text{std}} - \\mu_{B_t,j}^{\\text{std}})^2$\n    e. 更新总变化：$S_{\\text{total}} \\leftarrow S_{\\text{total}} + |\\Delta_t^{\\text{std}} - \\Delta_t^{\\text{raw}}|$。\n    f. 计算按特征的贡献：$c_j(t) = w_t [(\\mu_{A_t,j}^{\\text{std}} - \\mu_{B_t,j}^{\\text{std}})^2 - (\\mu_{A_t,j}^{\\text{raw}} - \\mu_{B_t,j}^{\\text{raw}})^2]$。\n    g. 对所有 $j=1, \\dots, d$，更新按特征的聚合变化：$S_j \\leftarrow S_j + |c_j(t)|$。\n6.  遍历所有合并后，在向量 $S_j$ 中找到最大值。\n7.  识别所有满足 $S_j$ 等于此最大值的特征索引 $j$（使用一个小的容差进行浮点数比较）。这些是驱动变量。\n8.  按照规定格式化最终结果，包括四舍五入后的 $S_{\\text{total}}$ 和排序后的驱动变量索引列表。\n\n这个系统化的过程确保所有量都根据其定义进行计算，从而得到正确且可验证的最终答案。", "answer": "```python\nimport numpy as np\nfrom scipy.cluster.hierarchy import linkage\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for the given test suite.\n    \"\"\"\n    test_cases = [\n        np.array([\n            [100, 0, 0.10],\n            [102, 0.50, 0.20],\n            [98, -0.20, 0.00],\n            [0, 5, -0.10],\n            [3, 4.50, -0.30],\n            [-2, 5.20, 0.00]\n        ], dtype=float),\n        np.array([\n            [10, -1, 50],\n            [10, 0, 51],\n            [10, 1, 49],\n            [10, 2, 48],\n            [10, -2, 52]\n        ], dtype=float),\n        np.array([\n            [100.0, 0.0, 0.10, 10000.0],\n            [101.5, -0.5, 0.05, 10002.0],\n            [98.5, 0.3, 0.15, 9998.0],\n            [100.8, -0.2, 0.12, 10001.0],\n            [0.0, 5.0, -0.10, 10050.0],\n            [-1.5, 4.5, -0.05, 10052.0],\n            [2.0, 5.2, -0.12, 10049.0],\n            [-0.8, 4.8, -0.08, 10051.5]\n        ], dtype=float),\n        np.array([\n            [0, 0],\n            [10, 0],\n            [0, 10]\n        ], dtype=float)\n    ]\n\n    results = [_solve_one_case(X) for X in test_cases]\n    \n    # Format the output string to remove spaces\n    print(str(results).replace(\" \", \"\"))\n\ndef _solve_one_case(X):\n    \"\"\"\n    Solves the problem for a single data matrix X.\n    \"\"\"\n    n, d = X.shape\n\n    # 1. Standardize data matrix X to get Z\n    means = np.mean(X, axis=0)\n    stds = np.std(X, axis=0)  # Population std dev (ddof=0 is default)\n    \n    # Use np.divide for safe division, setting output to 0 where std is 0\n    Z = np.divide(X - means, stds, out=np.zeros_like(X, dtype=float), where=stds!=0)\n\n    # 2. Perform Ward's clustering on raw data X\n    L = linkage(X, method='ward', metric='euclidean')\n\n    # 3. Initialize collectors for S_total and S_j\n    S_total = 0.0\n    S_j = np.zeros(d, dtype=float)\n    \n    # Cache for cluster point indices\n    # clusters[i] stores the list of original point indices for cluster i\n    clusters = {i: [i] for i in range(n)}\n    \n    # 4. Iterate through the n-1 merges from the linkage matrix\n    for t in range(n - 1):\n        # Identify merged clusters by their indices\n        c1_idx, c2_idx = int(L[t, 0]), int(L[t, 1])\n        \n        # Retrieve the original data point indices for each cluster\n        A_points = clusters[c1_idx]\n        B_points = clusters[c2_idx]\n        \n        # Create the new merged cluster and add it to the cache\n        new_cluster_idx = n + t\n        clusters[new_cluster_idx] = A_points + B_points\n        \n        nA, nB = len(A_points), len(B_points)\n        coeff = (nA * nB) / (nA + nB)\n        \n        # Calculate cluster means (centroids) for raw and standardized data\n        mu_A_raw = np.mean(X[A_points, :], axis=0)\n        mu_B_raw = np.mean(X[B_points, :], axis=0)\n        \n        mu_A_std = np.mean(Z[A_points, :], axis=0)\n        mu_B_std = np.mean(Z[B_points, :], axis=0)\n        \n        # Calculate merge heights (increase in ESS)\n        delta_raw = coeff * np.sum((mu_A_raw - mu_B_raw)**2)\n        delta_std = coeff * np.sum((mu_A_std - mu_B_std)**2)\n        \n        # Accumulate total change\n        S_total += np.abs(delta_std - delta_raw)\n        \n        # Calculate feature-wise contributions to the height difference\n        diff_sq_std = (mu_A_std - mu_B_std)**2\n        diff_sq_raw = (mu_A_raw - mu_B_raw)**2\n        c_t = coeff * (diff_sq_std - diff_sq_raw)\n        \n        # Accumulate absolute feature-wise contributions\n        S_j += np.abs(c_t)\n        \n    # 5. Identify driving variables (indices of max S_j)\n    max_Sj = np.max(S_j)\n    # Use a tolerance for floating-point comparison\n    driving_vars = np.where(np.isclose(S_j, max_Sj))[0].tolist()\n    \n    return [round(S_total, 6), driving_vars]\n\nsolve()\n```", "id": "3129004"}, {"introduction": "“相似性”的概念是聚类的核心，而你选择的距离度量正是对它的定义。本练习 [@problem_id:3129024] 对比了两种基本的度量：衡量直线距离的欧几里得距离，以及衡量数据向量夹角的余弦距离。通过观察这些不同的距离定义如何导致不同的聚类结构，你将为自己的数据分析任务培养出选择合适度量的深刻直觉。", "problem": "给定两种定义实向量空间中观测值（表示为向量）之间相异性的方法：欧几里得距离和余弦距离。在层次聚类中，树状图是一个有根树，通过基于连接规则连续合并簇而获得；在本问题中，连接规则为平均连接。树状图会导出叶节点之间的共表型距离，定义为两个叶节点在树中首次连接的高度。树状图对原始相异性的拟合优度通常通过原始成对相异性与共表型距离之间的皮尔逊积矩相关系数 (PPMCC) 来评估。此外，给定同一叶节点集上的两个树状图，它们之间的 Robinson-Foulds (RF) 距离根据其簇划分来定义，并量化了它们的结构差异。\n\n从以下基本概念出发：\n- 度量空间是一个对 $(\\mathcal{X}, d)$，其中 $d$ 是一个满足非负性、不可辨识者同一性、对称性和三角不等式的距离函数。\n- 凝聚式层次聚类过程通过从单例簇开始，根据连接规则迭代地合并具有最小相异性的两个簇来构建一个有根二叉树。在平均连接中，两个簇之间的相异性是这两个簇中元素之间成对距离的平均值。\n- 两个叶节点之间的共表型距离是它们在树状图中首次被连接的高度。共表型相关系数是原始成对距离向量与树状图导出的共表型距离向量之间的 PPMCC，两者均按相同的压缩顺序排列。\n- 两个具有相同叶节点集的有根树之间的 Robinson-Foulds 距离是非平凡簇划分（所有内部节点的叶节点集，不包括单例和全集）集合之间对称差的大小。\n\n任务：\n1. 对于下述每个测试用例，使用具有平均连接的凝聚式层次聚类构建两个树状图：一个来自欧几里得距离，一个来自余弦距离。\n2. 计算每个树状图相对于其原始相异性向量（欧几里得树状图对应欧几里得距离，余弦树状图对应余弦距离）的共表型相关系数。\n3. 计算在同一叶节点集上构建的欧几里得树状图和余弦树状图之间的 Robinson-Foulds 距离。\n4. 对于每个测试用例，返回一个包含三个值的列表：欧几里得共表型相关系数（浮点数）、余弦共表型相关系数（浮点数）和 Robinson-Foulds 距离（整数）。\n\n角度单位说明：凡出现角度，均以度为单位指定。您的程序在生成向量时（如果需要）必须在内部将给定的角度视为度，尽管实际计算可能会转换为弧度。\n\n测试套件（每个测试用例是 $\\mathbb{R}^2$ 中的一组点）：\n- 测试用例 A（方向上分离的簇）：\n  - $R_1$ 射线上的点：$[1, 0]$, $[2, 0]$, $[3, 0]$。\n  - $R_2$ 射线上的点：$[\\tfrac{1}{2}, \\tfrac{\\sqrt{3}}{2}]$, $[1, \\sqrt{3}]$, $[\\tfrac{3}{2}, \\tfrac{3\\sqrt{3}}{2}]$。\n- 测试用例 B（特征缩放失真）：\n  - 取测试用例 A 中的所有点，并将每个点的第一个坐标乘以 $10$。\n- 测试用例 C（具有大幅度差异的近共线射线）：\n  - 令 $\\theta_A = 44^\\circ$ 且 $\\theta_B = 46^\\circ$。\n  - 定义单位方向向量 $u_A = [\\cos(\\theta_A), \\sin(\\theta_A)]$ 和 $u_B = [\\cos(\\theta_B), \\sin(\\theta_B)]$。\n  - 点：$1 \\cdot u_A$, $2 \\cdot u_A$, $4 \\cdot u_A$, $10 \\cdot u_B$, $11 \\cdot u_B$, $12 \\cdot u_B$。\n\n输出规范：\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果。每个测试用例对应的元素本身必须是 $[c_{\\text{eucl}}, c_{\\text{cos}}, r]$ 形式的列表，其中 $c_{\\text{eucl}}$ 是欧几里得共表型相关系数（浮点数），$c_{\\text{cos}}$ 是余弦共表型相关系数（浮点数），$r$ 是 Robinson-Foulds 距离（整数）。例如，一个包含三个测试用例的输出应如下所示：$[[c_1^{\\text{eucl}}, c_1^{\\text{cos}}, r_1],[c_2^{\\text{eucl}}, c_2^{\\text{cos}}, r_2],[c_3^{\\text{eucl}}, c_3^{\\text{cos}}, r_3]]$。", "solution": "用户提供的问题已经过验证，并被确定为一个有效、适定的科学问题。其定义、数据和任务清晰、自成一体，并基于已建立的统计学习和数值计算原理。所有必要信息均已提供。\n\n### 基于原则的解决方案\n\n目标是比较使用两种不同相异性度量——欧几里得距离和余弦距离——在三个不同数据集上应用凝聚式层次聚类所产生的结果。比较使用两个定量指标进行：共表型相关系数 (CPCC)，它衡量树状图对原始相异性的保真度；以及 Robinson-Foulds (RF) 距离，它衡量两个树状图之间的拓扑差异。\n\n对于每个测试用例，解决方案系统地按以下步骤进行：\n\n1.  **计算相异性矩阵**：\n    对于每个测试用例，我们从 $\\mathbb{R}^2$ 中的 $N$ 个点集开始，表示为一个 $N \\times 2$ 的矩阵 $X$。计算两个独立的成对相异性矩阵。\n    -   **欧几里得距离**：这是两个向量之差的标准 $L_2$ 范数。对于点 $p_i = (p_{i1}, p_{i2})$ 和 $p_j = (p_{j1}, p_{j2})$，欧几里得距离为 $d_E(p_i, p_j) = \\sqrt{(p_{i1} - p_{j1})^2 + (p_{i2} - p_{j2})^2}$。该度量对向量在空间中的幅度和绝对位置都敏感。\n    -   **余弦距离**：该度量衡量两个向量之间夹角的余弦值，使其对向量的幅度不敏感（即，用一个正常数缩放向量不会改变它们的余弦距离）。它定义为 $d_C(p_i, p_j) = 1 - \\frac{p_i \\cdot p_j}{\\|p_i\\| \\|p_j\\|}$，其中 $\\|p\\|$ 表示 $p$ 的欧几里得范数。余弦距离为 $0$ 意味着向量指向同一方向，而距离为 $1$ 意味着它们正交。\n\n    这些相异性矩阵以压缩的一维数组格式计算，这是像 SciPy 这样的科学计算库所要求的。该格式存储了对称相异性矩阵的上三角部分。\n\n2.  **凝聚式层次聚类**：\n    使用计算出的相异性矩阵，我们执行层次聚类。选择的算法是具有“平均”连接准则的凝聚式算法。此过程首先将 $N$ 个点中的每一个都视为其自身的簇。在随后的每一步中，将具有最小相异性的两个簇合并。对于平均连接，两个簇 $C_a$ 和 $C_b$ 之间的相异性定义为这两个簇中点之间所有成对相异性的平均值：\n    $$d(C_a, C_b) = \\frac{1}{|C_a||C_b|} \\sum_{p_i \\in C_a, p_j \\in C_b} d(p_i, p_j)$$\n    这个过程重复 $N-1$ 次，直到所有点都包含在一个单一的簇中。结果是一个树状图，一种表示簇的嵌套合并的树形结构。聚类算法的输出是一个连接矩阵，表示为 $Z$，它编码了合并的序列和每次合并发生时的相异性（高度）。我们生成两个这样的矩阵：$Z_{\\text{Eucl}}$ 来自欧几里得距离，而 $Z_{\\text{cos}}$ 来自余弦距离。\n\n3.  **共表型相关系数 (CPCC) 计算**：\n    CPCC 评估树状图保留数据点原始成对相异性的程度。首先，我们从树状图计算共表型距离矩阵。两点 $p_i$ 和 $p_j$ 之间的共表型距离 $d_{\\text{coph}}(p_i, p_j)$ 是它们在树状图中首次合并到同一个簇时的高度。这个高度对应于被合并的两个子簇之间的连接距离。\n    然后，CPCC 被计算为原始相异性矩阵的元素与共表型距离矩阵相应元素之间的皮尔逊积矩相关系数。设 $d$ 为原始相异性向量，$d_{\\text{coph}}$ 为共表型距离向量，两者均采用压缩形式。CPCC，$c$，是：\n    $$c = \\frac{\\sum_{i", "answer": "```python\nimport numpy as np\nfrom scipy.spatial.distance import pdist\nfrom scipy.cluster.hierarchy import linkage, cophenet\n\ndef get_splits(Z: np.ndarray) - set:\n    \"\"\"\n    Computes the set of nontrivial splits from a linkage matrix.\n\n    A split is the set of leaf indices in a cluster formed at an internal node.\n    Nontrivial splits exclude singletons and the full set of leaves.\n\n    Args:\n        Z: The linkage matrix from scipy.cluster.hierarchy.linkage.\n\n    Returns:\n        A set of frozensets, where each frozenset contains the integer indices\n        of the leaves in a nontrivial split.\n    \"\"\"\n    N = Z.shape[0] + 1\n    # The clusters dictionary maps a node index to the set of leaf indices under it.\n    # Leaf nodes (0 to N-1) are initialized with their own index.\n    # Internal nodes (N to 2N-2) are added during the iteration.\n    clusters = {i: frozenset({i}) for i in range(N)}\n    splits = set()\n    \n    # Each row in Z represents an internal node and a merge operation.\n    for i in range(N - 1):\n        # Indices of the two clusters being merged.\n        c1_idx = int(Z[i, 0])\n        c2_idx = int(Z[i, 1])\n        \n        # Retrieve the leaf sets for the clusters being merged.\n        leaves1 = clusters[c1_idx]\n        leaves2 = clusters[c2_idx]\n        \n        # Form the new cluster by taking the union of the leaf sets.\n        new_cluster_leaves = leaves1.union(leaves2)\n        \n        # The new cluster corresponds to a new internal node.\n        # Its index is N + i.\n        new_cluster_idx = N + i\n        clusters[new_cluster_idx] = new_cluster_leaves\n        \n        # A split is nontrivial if it is not a singleton (guaranteed by merge)\n        # and not the full set of all leaves.\n        if len(new_cluster_leaves)  N:\n            splits.add(new_cluster_leaves)\n            \n    return splits\n\ndef solve_case(points: np.ndarray) - list:\n    \"\"\"\n    Performs clustering and evaluation for a single test case.\n\n    Args:\n        points: A NumPy array of shape (N, D) representing the data points.\n\n    Returns:\n        A list containing [c_eucl, c_cos, rf_dist].\n    \"\"\"\n    # Euclidean-based analysis\n    dist_eucl = pdist(points, 'euclidean')\n    Z_eucl = linkage(dist_eucl, method='average')\n    c_eucl, _ = cophenet(Z_eucl, dist_eucl)\n    splits_eucl = get_splits(Z_eucl)\n    \n    # Cosine-based analysis\n    dist_cos = pdist(points, 'cosine')\n    # Handle potential NaN from pdist if a zero-vector is present. This is not\n    # expected for the given test cases but is good practice.\n    if np.any(np.isnan(dist_cos)):\n        dist_cos = np.nan_to_num(dist_cos)\n    Z_cos = linkage(dist_cos, method='average')\n    c_cos, _ = cophenet(Z_cos, dist_cos)\n    splits_cos = get_splits(Z_cos)\n    \n    # Robinson-Foulds distance calculation\n    rf_dist = len(splits_eucl.symmetric_difference(splits_cos))\n    \n    return [c_eucl, c_cos, rf_dist]\n\ndef solve():\n    \"\"\"\n    Main function to define test cases, run the analysis, and print the results.\n    \"\"\"\n    # Test Case A: Directionally separated clusters\n    case_a = np.array([\n        [1.0, 0.0], [2.0, 0.0], [3.0, 0.0],\n        [0.5, np.sqrt(3)/2], [1.0, np.sqrt(3)], [1.5, 3*np.sqrt(3)/2]\n    ])\n\n    # Test Case B: Feature scaling distortion\n    case_b = case_a.copy()\n    case_b[:, 0] *= 10\n\n    # Test Case C: Nearly collinear rays with large magnitude differences\n    theta_A_deg = 44.0\n    theta_B_deg = 46.0\n    theta_A_rad = np.deg2rad(theta_A_deg)\n    theta_B_rad = np.deg2rad(theta_B_deg)\n    u_A = np.array([np.cos(theta_A_rad), np.sin(theta_A_rad)])\n    u_B = np.array([np.cos(theta_B_rad), np.sin(theta_B_rad)])\n    case_c = np.array([\n        1.0 * u_A, 2.0 * u_A, 4.0 * u_A,\n        10.0 * u_B, 11.0 * u_B, 12.0 * u_B\n    ])\n\n    test_cases = [case_a, case_b, case_c]\n    \n    results = []\n    for case in test_cases:\n        result = solve_case(case)\n        results.append(result)\n\n    # Format the output string to match the specified format without extra spaces.\n    case_strings = []\n    for res in results:\n        # Format each list [float, float, int] into a string \"[f,f,i]\"\n        case_strings.append(f\"[{res[0]:.10f},{res[1]:.10f},{res[2]}]\")\n    \n    final_output = f\"[{','.join(case_strings)}]\"\n    print(final_output)\n\nsolve()\n```", "id": "3129024"}, {"introduction": "在聚类分析中，最常见的问题或许就是确定“正确”的簇数量。本练习 [@problem_id:3129027] 介绍了一种强大且广泛使用的技术来回答这个问题：轮廓系数（silhouette score）。你将学习如何计算该系数——它衡量了每个数据点与其所在簇的匹配程度以及与其他簇的分离程度——并利用其平均值来从你的层次结构中客观地选择一个最优的簇数量 $k$。", "problem": "给定欧几里得平面中的有限点集，您需要使用平均连接法执行凝聚式层次聚类，然后在多个切割水平上通过轮廓系数评估聚类质量。您的任务是实现一个程序，对于每个指定的数据集和一个给定的最大簇数，计算跨切割水平的平均轮廓系数，并选择使该平均值最大化的簇数。\n\n需要使用的基本原理和定义：\n- 欧几里得距离：对于点 $x_{i} \\in \\mathbb{R}^{2}$ 和 $x_{j} \\in \\mathbb{R}^{2}$，定义距离 $d(i,j) = \\lVert x_{i} - x_{j} \\rVert_{2}$。\n- 使用平均连接的凝聚式层次聚类：从单例簇开始，重复合并其成员之间平均成对距离最小的一对簇，直到只剩下一个簇为止。这将生成一个二叉树状图。\n- 切割树状图：对于整数 $k \\geq 2$，通过在树状图中选择一个水平切割水平，从而产生 $k$ 个连通分量，即可获得一个产生恰好 $k$ 个簇的“切割”。使用直接强制生成恰好 $k$ 个簇的标准。\n- 点 $i$ 的轮廓系数：对于点集 $\\{x_{1},\\dots,x_{n}\\}$ 上的聚类，划分为不相交的非空簇 $\\{C_{1},\\dots,C_{k}\\}$，令 $C(i)$ 表示包含点 $i$ 的簇。定义\n  - 簇内相异性 $a(i)$ 为 $d(i,j)$ 在所有 $j \\in C(i)$ 且 $j \\neq i$ 上的平均值。如果 $\\lvert C(i) \\rvert = 1$，则设 $a(i) = 0$。\n  - 对于任何其他簇 $C' \\neq C(i)$，定义从 $i$ 到 $C'$ 的跨簇平均相异性为 $d(i,j)$ 在所有 $j \\in C'$ 上的平均值。令 $b(i)$ 为这些平均值在所有 $C' \\neq C(i)$ 上的最小值。\n  - 点 $i$ 的轮廓系数为\n    $$ s(i) = \\begin{cases}\n    0,  \\text{若 } \\lvert C(i) \\rvert = 1, \\\\\n    \\dfrac{b(i) - a(i)}{\\max\\{a(i), b(i)\\}},  \\text{其他情况, 约定当 } a(i)=b(i)=0 \\text{ 时 } s(i)=0.\n    \\end{cases} $$\n- $k$ 切割水平下的平均轮廓系数：$\\bar{s}(k) = \\dfrac{1}{n} \\sum_{i=1}^{n} s(i)$，其中 $n$ 是点的数量。\n- 选择规则：对于每个数据集，评估所有在 $\\{2,3,\\dots, \\min\\{k_{\\max}, n\\}\\}$ 中的整数 $k$ 的 $\\bar{s}(k)$。令 $k^{\\star}$ 为任何一个达到最大平均轮廓系数的 $k$。如果出现绝对容差 $\\varepsilon = 10^{-12}$ 内的平局，选择最小的那个 $k$。\n\n嵌入程序内的输入（不允许外部输入）：\n- 您必须使用欧几里得距离和平均连接进行层次聚类，并在每次切割时产生恰好 $k$ 个簇。\n- 包含三个数据集的测试套件，每个数据集由一个平面坐标列表和一个参数 $k_{\\max}$ 指定：\n  - 数据集 A（良好分离的双簇结构），有 $n = 12$ 个点和 $k_{\\max} = 5$：\n    - 点：\n      $(0,0)$, $(0,1)$, $(1,0)$, $(1,1)$, $(0.5,0)$, $(0,0.5)$,\n      $(5,5)$, $(5,6)$, $(6,5)$, $(6,6)$, $(5.5,5)$, $(5,5.5)$。\n  - 数据集 B（良好分离的三簇结构），有 $n = 15$ 个点和 $k_{\\max} = 6$：\n    - 点：\n      $(-6,0)$, $(-6,0.2)$, $(-6,-0.2)$, $(-5.8,0)$, $(-6.2,0)$,\n      $(0,0)$, $(0,0.2)$, $(0,-0.2)$, $(0.2,0)$, $(-0.2,0)$,\n      $(6,0)$, $(6,0.2)$, $(6,-0.2)$, $(5.8,0)$, $(6.2,0)$。\n  - 数据集 C（退化的相同点），有 $n = 5$ 个点和 $k_{\\max} = 4$：\n    - 点：\n      $(10,-3)$, $(10,-3)$, $(10,-3)$, $(10,-3)$, $(10,-3)$。\n\n程序要求：\n- 对于每个数据集，使用上述定义计算所有整数 $k \\in \\{2,\\dots,\\min\\{k_{\\max}, n\\}\\}$ 的 $\\bar{s}(k)$，然后根据容差为 $\\varepsilon = 10^{-12}$ 的平局打破规则选择 $k^{\\star}$。\n- 您的程序应生成单行输出，其中包含为每个数据集选择的 $k^{\\star}$，按 A、B、C 的顺序，作为逗号分隔的列表并用方括号括起来。例如，一个有效的输出格式如 `[2,3,2]`。\n- 答案值为整数。\n\n注意：不涉及物理单位或角度。所有用于打破平局的数值比较必须使用指定的绝对容差 $\\varepsilon = 10^{-12}$。确保您的实现与上述定义一致。", "solution": "用户提供的问题是有效的。它在科学上基于统计学习的既定原则，特别是凝聚式层次聚类和使用轮廓系数进行聚类验证。所有术语，如欧几里得距离 ($d(i,j)$)、平均连接和轮廓分数 ($s(i)$)，都得到了数学上的明确定义。该问题是自包含的，提供了所有必要的数据和约束，包括数据集、聚类参数、要评估的簇范围 ($k$)，以及精确的平局打破规则。该计算任务是可行的，并为每个数据集得出一个唯一的、可验证的解。\n\n解决方案是通过对每个提供的数据集遵循一个结构化的、多步骤的过程来实现的。\n\n**步骤 1：凝聚式层次聚类**\n\n对于每个数据集，我们首先计算所有点之间的成对距离。问题指定使用欧几里得距离，$d(i,j) = \\lVert x_{i} - x_{j} \\rVert_{2}$。这些距离被组织成一个压缩距离矩阵。\n\n接下来，我们使用平均连接标准执行凝聚式层次聚类。此方法从将每个数据点视为一个单例簇开始。然后，它迭代地合并两个簇 $C_A$ 和 $C_B$，这两个簇具有最小的平均成对距离，定义为：\n$$\nD(C_A, C_B) = \\frac{1}{|C_A| |C_B|} \\sum_{i \\in C_A} \\sum_{j \\in C_B} d(i,j)\n$$\n这个过程持续进行，直到所有点都包含在一个单一的簇中，从而产生一个称为树状图的二叉树结构。每个数据集只执行一次此聚类过程。使用 `scipy.spatial.distance.pdist` 函数进行距离计算，并使用 `scipy.cluster.hierarchy.linkage`（`method='average'`）创建树状图。\n\n**步骤 2：树状图切割和簇分配**\n\n对于每个具有 $n$ 个点和给定最大簇数 $k_{\\max}$ 的数据集，我们评估 $k \\in \\{2, 3, \\dots, \\min\\{k_{\\max}, n\\}\\}$ 的聚类配置。对于此范围内的每个整数 $k$，我们“切割”树状图以将数据划分为恰好 $k$ 个簇。这通过识别层次结构中顶部的 $n-k$ 次合并并将其余点分配到所产生的 $k$ 个分支来实现。为此，使用了 `scipy.cluster.hierarchy.fcluster` 函数和 `criterion='maxclust'` 选项。这样，对于每个 $k$ 值，我们都能得到所有 $n$ 个点的簇标签集合。\n\n**步骤 3：轮廓分数计算**\n\n对于每个划分为 $k$ 个簇的分区，我们计算平均轮廓分数 $\\bar{s}(k)$，以评估其质量。单个点 $i$ 的轮廓分数，表示为 $s(i)$，衡量该点与其分配的簇的契合程度，相比于邻近簇。其计算方法如下：\n\n1.  **簇内相异性, $a(i)$**：这是点 $i$ 到其自身簇 $C(i)$ 内所有其他点 $j$ 的平均距离。\n    $$ a(i) = \\frac{1}{|C(i)| - 1} \\sum_{j \\in C(i), j \\neq i} d(i,j) $$\n    根据问题定义，如果点 $i$ 位于一个单例簇中 ($|C(i)| = 1$)，则 $a(i) = 0$。\n\n2.  **簇间相异性, $b(i)$**：对于每个其他簇 $C'$ ($C' \\neq C(i)$)，我们计算点 $i$ 到 $C'$ 中所有点的平均距离。$b(i)$ 是这些值在所有其他簇上的最小值。\n    $$ b(i) = \\min_{C' \\neq C(i)} \\left\\{ \\frac{1}{|C'|} \\sum_{j \\in C'} d(i,j) \\right\\} $$\n\n3.  **轮廓系数 $s(i)$**：点 $i$ 的轮廓系数由以下公式给出：\n    $$ s(i) = \\begin{cases} 0,  \\text{若 } |C(i)| = 1 \\\\ \\dfrac{b(i) - a(i)}{\\max\\{a(i), b(i)\\}},  \\text{其他情况} \\end{cases} $$\n    如果 $a(i) = b(i) = 0$，则使用一个特殊约定，即定义 $s(i)$ 为 $0$。这种情况与包含相同点的数据集（如数据集 C）相关。\n\n$k$-簇分区的平均轮廓分数 $\\bar{s}(k)$ 是所有 $n$ 个数据点的 $s(i)$ 的平均值：\n$$ \\bar{s}(k) = \\frac{1}{n} \\sum_{i=1}^{n} s(i) $$\n\n**步骤 4：最优簇数选择**\n\n在计算完所有有效 $k$ 的 $\\bar{s}(k)$ 之后，我们选择最优的簇数 $k^{\\star}$。选择规则要求找到使平均轮廓分数最大化的 $k$。为了处理可能的平局，问题指定了一个两步规则：\n1.  首先，确定实现的最大分数，$S_{\\max} = \\max_{k} \\{\\bar{s}(k)\\}$。\n2.  然后，识别所有 $k$ 值的集合，其分数在绝对容差 $\\varepsilon = 10^{-12}$ 内与 $S_{\\max}$ 相等。如果 $|\\bar{s}(k') - S_{\\max}| \\leq \\varepsilon$，则分数 $\\bar{s}(k')$ 被视为平局。\n3.  从这个“并列最大”的 $k$ 值集合中，选择最小的一个。\n\n此过程确保了每个数据集都有一个唯一且确定的 $k^{\\star}$。该逻辑通过先计算所有分数，然后找到最大值，最后按 $k$ 的递增顺序遍历分数，以找到第一个落在最大值容差范围内的 $k$ 来实现。\n\n对于数据集 A（两个良好分离的组），预计在 $k=2$ 时获得最高的轮廓分数。对于数据集 B（三个良好分离的组），预计 $k=3$ 是最优的。对于数据集 C（所有点都相同），所有距离都为 $0$，导致对于所有点和所有聚类，都有 $a(i) = 0$ 和 $b(i) = 0$。因此，对于所有 $k$，都有 $s(i)=0$ 和 $\\bar{s}(k)=0$。平局打破规则将选择测试的最小 $k$，即 $2$。", "answer": "```python\nimport numpy as np\nfrom scipy.cluster.hierarchy import linkage, fcluster\nfrom scipy.spatial.distance import pdist, squareform\n\ndef calculate_average_silhouette(dist_matrix, labels, n):\n    \"\"\"\n    Calculates the average silhouette score for a given clustering.\n    Follows the definitions specified in the problem statement.\n    \"\"\"\n    if n == 0:\n        return 0.0\n\n    unique_labels, label_indices, label_counts = np.unique(labels, return_inverse=True, return_counts=True)\n    num_clusters = len(unique_labels)\n\n    if num_clusters = 1:\n        return 0.0\n\n    silhouettes = np.zeros(n)\n    \n    for i in range(n):\n        my_label_idx = label_indices[i]\n        my_label = unique_labels[my_label_idx]\n        my_cluster_size = label_counts[my_label_idx]\n\n        # Per problem definition, s(i) = 0 for singleton clusters.\n        if my_cluster_size == 1:\n            silhouettes[i] = 0.0\n            continue\n        \n        # Calculate a(i): mean distance to other points in the same cluster.\n        in_cluster_mask = (labels == my_label)\n        in_cluster_mask[i] = False\n        a_i = np.sum(dist_matrix[i, in_cluster_mask]) / (my_cluster_size - 1)\n\n        # Calculate b(i): min mean distance to points in any other cluster.\n        b_i = np.inf\n        for j, other_label in enumerate(unique_labels):\n            if j == my_label_idx:\n                continue\n            \n            other_cluster_mask = (labels == other_label)\n            mean_dist = np.mean(dist_matrix[i, other_cluster_mask])\n            b_i = min(b_i, mean_dist)\n\n        # Calculate s(i).\n        denominator = max(a_i, b_i)\n        if denominator == 0:\n            # Handles the convention where s(i)=0 if a(i)=b(i)=0.\n            silhouettes[i] = 0.0\n        else:\n            silhouettes[i] = (b_i - a_i) / denominator\n            \n    return np.mean(silhouettes)\n\ndef find_optimal_k(points, k_max):\n    \"\"\"\n    Performs hierarchical clustering and finds the optimal k based on silhouette score.\n    \"\"\"\n    n = len(points)\n    epsilon = 1e-12\n\n    if n = 1:\n        # According to problem constraints (k>=2), n must be at least 2.\n        # This case should not be reached.\n        return None\n\n    # Step 1: Perform hierarchical clustering with average linkage.\n    condensed_dist_matrix = pdist(points, 'euclidean')\n    linkage_matrix = linkage(condensed_dist_matrix, method='average')\n    \n    # We need the full n x n distance matrix for silhouette calculations.\n    dist_matrix = squareform(condensed_dist_matrix)\n\n    # Step 2: Evaluate average silhouette for each k.\n    silhouette_scores = {}\n    k_range = range(2, min(k_max, n-1) + 1 if n > 1 else 2) # Range is up to n-1 clusters for n points\n    \n    # Adjust range to be at most n\n    k_range = range(2, min(k_max, n) + 1)\n\n\n    if not k_range:\n        return None\n\n    for k in k_range:\n        labels = fcluster(linkage_matrix, t=k, criterion='maxclust')\n        avg_silhouette = calculate_average_silhouette(dist_matrix, labels, n)\n        silhouette_scores[k] = avg_silhouette\n\n    # Step 3: Select the optimal k using the specified tie-breaking rule.\n    if not silhouette_scores:\n        return None\n        \n    # Find the true maximum score\n    max_score = -np.inf\n    if silhouette_scores:\n        max_score = max(silhouette_scores.values())\n\n    # Find the smallest k that is tied for the maximum score\n    best_k = -1\n    for k in sorted(silhouette_scores.keys()):\n        score = silhouette_scores[k]\n        if abs(score - max_score) = epsilon:\n            best_k = k\n            break # Found the smallest k in the tie-set, so we can stop.\n    \n    return best_k\n\ndef solve():\n    \"\"\"\n    Main function to run the analysis on all test cases and print the final result.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Dataset A\n        {\n            \"points\": np.array([\n                (0,0), (0,1), (1,0), (1,1), (0.5,0), (0,0.5),\n                (5,5), (5,6), (6,5), (6,6), (5.5,5), (5,5.5)\n            ]),\n            \"k_max\": 5\n        },\n        # Dataset B\n        {\n            \"points\": np.array([\n                (-6,0), (-6,0.2), (-6,-0.2), (-5.8,0), (-6.2,0),\n                (0,0), (0,0.2), (0,-0.2), (0.2,0), (-0.2,0),\n                (6,0), (6,0.2), (6,-0.2), (5.8,0), (6.2,0)\n            ]),\n            \"k_max\": 6\n        },\n        # Dataset C\n        {\n            \"points\": np.array([\n                (10,-3), (10,-3), (10,-3), (10,-3), (10,-3)\n            ]),\n            \"k_max\": 4\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        k_star = find_optimal_k(case[\"points\"], case[\"k_max\"])\n        results.append(k_star)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3129027"}]}