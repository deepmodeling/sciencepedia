{"hands_on_practices": [{"introduction": "在层次聚类中，我们通常期望合并高度（即合并时簇之间的距离）是单调递增的。然而，某些联结方法，如质心联结法，可能会违反这一直观属性。本练习通过一个简单的三点二维数据集，让你亲手计算并验证质心联结法如何导致“反转”（inversion）现象，即后一次合并的高度反而低于前一次，从而加深你对该方法内在机制及其潜在缺陷的理解。[@problem_id:3140573]", "problem": "考虑在$\\mathbb{R}^{2}$中使用欧几里得度量的质心连接层次凝聚聚类（HAC）。质心连接将两个簇之间的距离定义为其算术平均值（质心）之间的欧几里得距离。给定$3$个数据点\n$$\n\\mathbf{x}_{1}=\\left(-\\frac{1}{2},\\,0\\right),\\quad\n\\mathbf{x}_{2}=\\left(\\frac{1}{2},\\,0\\right),\\quad\n\\mathbf{x}_{3}=\\left(0,\\,\\frac{\\sqrt{13}}{4}\\right).\n$$\n从单例簇 $\\{\\mathbf{x}_{1}\\}$、$\\{\\mathbf{x}_{2}\\}$ 和 $\\{\\mathbf{x}_{3}\\}$ 开始，执行质心连接的 HAC。在每一步中，合并质心距离最小的一对簇，并将所选的质心距离记录为该步骤的合并高度。精确计算：\n- 第一个合并高度 $h_{1}$，\n- 第一步合并后簇的质心，\n- 第二个合并高度 $h_{2}$，\n并数值验证 $h_2  h_1$。", "solution": "该问题是有效的，因为它是一个适定且有科学依据的练习，旨在应用质心连接的层次凝聚聚类（HAC）算法，这是统计学习中的一个标准课题。它提供了所有必要的数据和定义，以得出一个唯一且可验证的解。\n\n过程始于$3$个单例簇，$C_{1}=\\{\\mathbf{x}_{1}\\}$、$C_{2}=\\{\\mathbf{x}_{2}\\}$和$C_{3}=\\{\\mathbf{x}_{3}\\}$，其中数据点为：\n$$\n\\mathbf{x}_{1}=\\left(-\\frac{1}{2},\\,0\\right),\\quad\n\\mathbf{x}_{2}=\\left(\\frac{1}{2},\\,0\\right),\\quad\n\\mathbf{x}_{3}=\\left(0,\\,\\frac{\\sqrt{13}}{4}\\right)\n$$\n在质心连接中，两个簇之间的距离是它们质心之间的欧几里得距离。对于单例簇，质心就是点本身。设 $\\mu_i$ 是簇 $C_i$ 的质心。初始时，$\\mu_{1}=\\mathbf{x}_{1}$，$\\mu_{2}=\\mathbf{x}_{2}$，以及 $\\mu_{3}=\\mathbf{x}_{3}$。\n\n首先，我们计算初始质心之间的成对欧几里得距离的平方：\n$C_{1}$ 和 $C_{2}$ 之间的距离平方为：\n$$\nd(C_{1}, C_{2})^{2} = ||\\mu_{1} - \\mu_{2}||^{2} = ||\\mathbf{x}_{1} - \\mathbf{x}_{2}||^{2} = \\left\\|\\left(-\\frac{1}{2}-\\frac{1}{2},\\,0-0\\right)\\right\\|^{2} = \\|(-1, 0)\\|^{2} = (-1)^{2} + 0^{2} = 1\n$$\n$C_{1}$ 和 $C_{3}$ 之间的距离平方为：\n$$\nd(C_{1}, C_{3})^{2} = ||\\mu_{1} - \\mu_{3}||^{2} = ||\\mathbf{x}_{1} - \\mathbf{x}_{3}||^{2} = \\left\\|\\left(-\\frac{1}{2}-0,\\,0-\\frac{\\sqrt{13}}{4}\\right)\\right\\|^{2} = \\left(-\\frac{1}{2}\\right)^{2} + \\left(-\\frac{\\sqrt{13}}{4}\\right)^{2} = \\frac{1}{4} + \\frac{13}{16} = \\frac{4}{16} + \\frac{13}{16} = \\frac{17}{16}\n$$\n$C_{2}$ 和 $C_{3}$ 之间的距离平方为：\n$$\nd(C_{2}, C_{3})^{2} = ||\\mu_{2} - \\mu_{3}||^{2} = ||\\mathbf{x}_{2} - \\mathbf{x}_{3}||^{2} = \\left\\|\\left(\\frac{1}{2}-0,\\,0-\\frac{\\sqrt{13}}{4}\\right)\\right\\|^{2} = \\left(\\frac{1}{2}\\right)^{2} + \\left(-\\frac{\\sqrt{13}}{4}\\right)^{2} = \\frac{1}{4} + \\frac{13}{16} = \\frac{4}{16} + \\frac{13}{16} = \\frac{17}{16}\n$$\n对应的距离是 $d(C_{1}, C_{2}) = \\sqrt{1} = 1$，以及 $d(C_{1}, C_{3}) = d(C_{2}, C_{3}) = \\sqrt{\\frac{17}{16}} = \\frac{\\sqrt{17}}{4}$。\n为了找到最小距离，我们比较 $1$ 和 $\\frac{\\sqrt{17}}{4}$。将两个值平方得到 $1^{2}=1$ 和 $\\left(\\frac{\\sqrt{17}}{4}\\right)^{2}=\\frac{17}{16}$。因为 $1  \\frac{17}{16}$，所以我们有 $1  \\frac{\\sqrt{17}}{4}$。\n因此，最小距离是 $d(C_{1}, C_{2}) = 1$。\n\n第一步是合并最近的簇 $C_{1}$ 和 $C_{2}$。合并高度 $h_{1}$ 就是这个最小距离。\n$$\nh_{1} = 1\n$$\n设新簇为 $C_{12} = C_{1} \\cup C_{2} = \\{\\mathbf{x}_{1}, \\mathbf{x}_{2}\\}$。这个新簇的质心 $\\mu_{12}$ 是其构成点的算术平均值：\n$$\n\\mu_{12} = \\frac{\\mathbf{x}_{1} + \\mathbf{x}_{2}}{2} = \\frac{1}{2} \\left[ \\left(-\\frac{1}{2}, 0\\right) + \\left(\\frac{1}{2}, 0\\right) \\right] = \\frac{1}{2} (0, 0) = (0, 0)\n$$\n合并后簇的质心是原点 $(0,0)$。\n\n算法进入第二步。我们现在有两个簇：$C_{12}$ 和 $C_{3}$。下一次合并必须在这两个簇之间进行。第二次合并的高度 $h_{2}$ 是它们质心 $\\mu_{12}$ 和 $\\mu_{3}$ 之间的距离。\n$$\nh_{2} = d(C_{12}, C_{3}) = ||\\mu_{12} - \\mu_{3}|| = ||(0, 0) - \\left(0, \\frac{\\sqrt{13}}{4}\\right)|| = \\left\\|\\left(0, -\\frac{\\sqrt{13}}{4}\\right)\\right\\|\n$$\n计算其模长：\n$$\nh_{2} = \\sqrt{0^{2} + \\left(-\\frac{\\sqrt{13}}{4}\\right)^{2}} = \\sqrt{\\frac{13}{16}} = \\frac{\\sqrt{13}}{4}\n$$\n现在我们验证发生了反转，即 $h_{2}  h_{1}$。\n我们需要比较 $h_2 = \\frac{\\sqrt{13}}{4}$ 和 $h_1 = 1$。\n不等式是 $\\frac{\\sqrt{13}}{4}  1$。\n两边乘以 $4$，我们得到 $\\sqrt{13}  4$。\n由于两边都是正数，我们可以将它们平方得到 $13  16$。这个不等式是成立的，这证实了 $h_{2}  h_{1}$。\n质心连接法产生了一个非单调的树状图，这是该连接类型的一个已知特征。\n\n最后，我们计算反转幅度 $\\Delta$，定义为 $\\Delta=h_{2}-h_{1}$。\n$$\n\\Delta = \\frac{\\sqrt{13}}{4} - 1\n$$\n为了将其写成单一表达式，我们使用一个公分母：\n$$\n\\Delta = \\frac{\\sqrt{13} - 4}{4}\n$$\n这就是反转幅度的精确闭式表达式。", "answer": "$$\n\\boxed{\\frac{\\sqrt{13}-4}{4}}\n$$", "id": "3140573"}, {"introduction": "理论知识需要通过实践来检验，本练习将引导你从手动计算转向编程实践，以探索不同联结方法在面对实际数据中常见的“离群点”时的表现。你将通过编程模拟，直观地观察到单一联结法因其“链式效应”而对离群点异常敏感，并将其与完整、平均和质心联结法的稳健性进行对比。这个练习对于理解如何根据数据特性选择合适的联结方法至关重要。[@problem_id:3140585]", "problem": "给定一个在欧几里得度量下的平面上的基础数据集，该数据集包含两个紧密的簇。第一个簇 $A$ 由五个点组成：$(-0.2,-0.2)$、$(-0.2,0.2)$、$(0.2,-0.2)$、$(0.2,0.2)$ 和 $(0,0)$。第二个簇 $B$ 是将簇 $A$ 中的每个点沿 $x$ 轴平移 $4$ 个单位得到的，因此 $B$ 由点 $(3.8,-0.2)$、$(3.8,0.2)$、$(4.2,-0.2)$、$(4.2,0.2)$ 和 $(4,0)$ 组成。所有距离均为欧几里得距离，并以无单位坐标表示。\n\n从这个基础数据集开始，您将按如下方式添加离群点。对于给定的标量距离 $R \\gt 0$ 和整数数量 $s \\ge 1$，您需要添加 $s$ 个离群点，这些点位于坐标 $(-R,y_i)$ 处，其中 $y_i$ 是以 $0$ 为中心、步长为 $0.2$ 的均匀间隔偏移量，具体为 $y_i = 0.2\\,(i - (s-1)/2)$，其中 $i \\in \\{0,1,\\dots,s-1\\}$。当 $R$ 很大时，这种构造确保每个添加的离群点与簇 $A$ 的质心距离约为 $R$，且远离簇 $B$。\n\n您必须编写一个完整、可运行的程序，该程序：\n- 构建基础数据集并计算两两之间的欧几里得距离。\n- 计算由基础数据集导出的完全图的最小生成树 (MST)，使用欧几里得距离作为边权重。令 $W_0$ 表示此基础 MST 中的最大边权重。\n- 对于每个测试用例，将指定的离群点添加到基础数据集中，重新计算 MST，并令 $W_1$ 表示新 MST 中的最大边权重。计算增量 $\\Delta W = W_1 - W_0$。同时计算 $m_{\\text{new}}$，即至少涉及一个新增离群点节点的 MST 边数。\n- 对每个数据集（基础数据集和带离群点的数据集），使用四种连接方法（单连接、全连接、平均连接和质心连接）进行层次凝聚聚类。对于每种方法，提取最终的合并高度 $H$（连接结果中最后一次合并记录的高度）。对于每个测试用例，计算相对于基础数据集的最终合并高度的增量，即 $\\Delta H_{\\text{single}}$、$\\Delta H_{\\text{complete}}$、$\\Delta H_{\\text{average}}$ 和 $\\Delta H_{\\text{centroid}}$。\n\n推理的基本依据：使用连接方法的定义，以及在边权重由两两距离给出的度量空间中，单连接聚类与最小生成树上的 Kruskal 算法之间的著名等价关系。\n\n您的程序必须实现计算，并为以下 $(R,s)$ 值的测试套件生成结果：\n- $(1.0,1)$：一个放置在相对靠近簇 $A$ 的离群点。\n- $(4.0,2)$：两个离群点，其放置距离与簇质心之间的间距相当。\n- $(8.0,1)$：一个远离两个簇的离群点。\n- $(3.0,3)$：三个处于中等距离的离群点。\n\n对于每个测试用例，您的程序应按以下顺序输出一个包含六个值的列表：\n- $\\Delta H_{\\text{single}}$\n- $\\Delta H_{\\text{complete}}$\n- $\\Delta H_{\\text{average}}$\n- $\\Delta H_{\\text{centroid}}$\n- $\\Delta W$\n- $m_{\\text{new}}$\n\n所有实数值输出必须表示为十进制浮点数，计数 $m_{\\text{new}}$ 必须表示为整数。\n\n最终输出格式：您的程序应生成单行输出，其中包含所有测试用例的结果，格式为一个由各测试用例列表组成的逗号分隔列表，并用方括号括起来，不含空格。例如，其形式应为 $[[v_{1,1},v_{1,2},\\dots,v_{1,6}],[v_{2,1},\\dots,v_{2,6}],\\dots]$，其中 $v_{i,j}$ 表示第 $i$ 个测试用例的第 $j$ 个值。", "solution": "该问题要求对离群点在层次聚类和最小生成树 (MST) 结构上的影响进行计算分析。解决方案涉及生成一个基础数据集，添加指定的离群点，然后针对几个测试用例计算关键指标的变化。\n\n方法论步骤如下：\n\n1.  **数据集构建**：\n    基础数据集由二维平面上的两个不同簇 $A$ 和 $B$ 构成。簇 $A$ 由五个点 $\\{(-0.2,-0.2), (-0.2,0.2), (0.2,-0.2), (0.2,0.2), (0,0)\\}$ 定义。簇 $B$ 是簇 $A$ 沿正 $x$ 轴平移 $4$ 个单位得到的，生成的点为 $\\{(3.8,-0.2), (3.8,0.2), (4.2,-0.2), (4.2,0.2), (4,0)\\}$。基础数据集中的总点数为 $10$。对于每个由一对 $(R, s)$ 定义的测试用例，其中 $R  0$ 是距离， $s \\ge 1$ 是整数数量，会生成 $s$ 个离群点。这些离群点放置在坐标 $(-R, y_i)$ 处，其垂直偏移量 $y_i$ 由公式 $y_i = 0.2 \\times (i - (s-1)/2)$ 定义，其中 $i \\in \\{0, 1, \\dots, s-1\\}$。这种构造将离群点放置在簇 $A$ 的左侧。\n\n2.  **核心分析程序**：\n    对于任何给定的点集，都会计算一套标准指标。此程序首先应用于基础数据集，然后应用于每个增强数据集（基础数据集 + 离群点）。\n    -   **两两距离**：分析首先计算数据集中所有点之间的两两欧几里得距离矩阵。该距离矩阵作为完全图的边权重，其中点是顶点。\n    -   **最小生成树 (MST)**：使用标准算法（如 Prim 算法或 Kruskal 算法）计算此加权完全图的 MST。我们确定 MST 中存在的最大边权重 $W$。\n    -   **层次凝聚聚类**：使用四种不同的连接方法（单连接、全连接、平均连接和质心连接）执行层次聚类。对于每种方法，结果是一个描述合并过程的连接矩阵。提取的关键指标是最终合并高度 $H$，即最后两个簇合并形成包含所有数据点的单个簇时的距离或相异度值。\n\n3.  **理论联系：单连接与 MST**：\n    图论和聚类中的一个基本原理指出，单连接层次聚类等同于构建一个 MST。具体来说，单连接聚类中的合并序列对应于 Kruskal 算法构建 MST 时添加边的序列。一个直接的推论是，单连接聚类中的最终合并高度 $H_{\\text{single}}$ 必须等于 MST 中最长边的权重 $W$。这为计算提供了一个关键的自洽性检验，因为我们预期在所有测试用例中都会发现 $\\Delta H_{\\text{single}} = \\Delta W$。\n\n4.  **差异分析**：\n    问题的核心是量化因引入离群点而导致的指标变化。\n    -   **基线计算**：首先，对基础数据集运行分析程序，以获得最大 MST 边权重 ($W_0$) 和四种连接方法各自的最终合并高度 ($H_{\\text{single,0}}$、$H_{\\text{complete,0}}$、$H_{\\text{average,0}}$ 和 $H_{\\text{centroid,0}}$) 的基线值。\n    -   **测试用例计算**：对于每个测试用例 $(R,s)$，将指定的离群点添加到基础数据集中。然后对这个新的、更大的数据集运行相同的分析程序，以计算新值：$W_1$、$H_{\\text{single,1}}$、$H_{\\text{complete,1}}$、$H_{\\text{average,1}}$ 和 $H_{\\text{centroid,1}}$。\n    -   **变化计算**：增量（用 $\\Delta$ 表示）计算为新值与基线值之间的差：\n        $$ \\Delta H_{\\text{method}} = H_{\\text{method,1}} - H_{\\text{method,0}} $$\n        $$ \\Delta W = W_1 - W_0 $$\n    -   **MST 边数**：计算一个附加指标 $m_{\\text{new}}$。这是新 MST 中至少有一个端点对应于新增离群点的边的数量。该指标量化了离群点如何被整合到数据的连接结构中。\n\n5.  **实现**：\n    所述程序在一个 Python 程序中实现。`numpy` 库用于数值运算和数组管理。`scipy` 库提供了科学计算所需的高级函数：用于距离计算的 `scipy.spatial.distance.pdist` 和 `scipy.spatial.distance.squareform`，用于 MST 的 `scipy.sparse.csgraph.minimum_spanning_tree`，以及用于层次聚类的 `scipy.cluster.hierarchy.linkage`。需要注意的是，对于‘centroid’连接方法，`linkage` 函数需要原始数据点作为输入，而对于‘single’、‘complete’和‘average’方法，它作用于压缩后的距离矩阵。程序遍历指定的测试套件，为每个案例计算六个所需的输出值，并将它们格式化为指定的字符串格式。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.spatial.distance import pdist, squareform\nfrom scipy.sparse.csgraph import minimum_spanning_tree\nfrom scipy.cluster.hierarchy import linkage\n\ndef solve():\n    \"\"\"\n    Solves the problem by analyzing the impact of outliers on hierarchical \n    clustering and Minimum Spanning Trees (MST).\n    \"\"\"\n\n    def analyze_dataset(points):\n        \"\"\"\n        Computes clustering and MST metrics for a given set of points.\n\n        Args:\n            points (np.ndarray): A NumPy array of shape (n, 2) representing n points.\n\n        Returns:\n            A tuple containing:\n            - H_single (float): Final merge height for single linkage.\n            - H_complete (float): Final merge height for complete linkage.\n            - H_average (float): Final merge height for average linkage.\n            - H_centroid (float): Final merge height for centroid linkage.\n            - W (float): Maximum edge weight in the MST.\n            - mst (csr_matrix): The computed Minimum Spanning Tree.\n        \"\"\"\n        n_points = points.shape[0]\n        if n_points  2:\n            return 0.0, 0.0, 0.0, 0.0, 0.0, None\n\n        # Compute pairwise Euclidean distances\n        condensed_dist = pdist(points, 'euclidean')\n        dist_matrix = squareform(condensed_dist)\n\n        # Compute Minimum Spanning Tree (MST)\n        mst = minimum_spanning_tree(dist_matrix)\n        W = mst.max()\n\n        # Perform hierarchical clustering\n        # Single linkage\n        Z_single = linkage(condensed_dist, method='single')\n        H_single = Z_single[-1, 2]\n\n        # Complete linkage\n        Z_complete = linkage(condensed_dist, method='complete')\n        H_complete = Z_complete[-1, 2]\n\n        # Average linkage\n        Z_average = linkage(condensed_dist, method='average')\n        H_average = Z_average[-1, 2]\n\n        # Centroid linkage (requires original data points, not distance matrix)\n        Z_centroid = linkage(points, method='centroid')\n        H_centroid = Z_centroid[-1, 2]\n\n        return H_single, H_complete, H_average, H_centroid, W, mst\n\n    # --- Step 1: Define and analyze the base dataset ---\n    # Cluster A\n    cluster_A = np.array([\n        [-0.2, -0.2],\n        [-0.2,  0.2],\n        [ 0.2, -0.2],\n        [ 0.2,  0.2],\n        [ 0.0,  0.0]\n    ])\n\n    # Cluster B (translation of A by 4 units on x-axis)\n    cluster_B = cluster_A + np.array([4.0, 0.0])\n\n    base_dataset = np.vstack([cluster_A, cluster_B])\n    n_base_points = base_dataset.shape[0]\n\n    # Calculate baseline metrics\n    H_single_0, H_complete_0, H_average_0, H_centroid_0, W_0, _ = analyze_dataset(base_dataset)\n\n    # --- Step 2: Process each test case ---\n    test_cases = [\n        (1.0, 1),\n        (4.0, 2),\n        (8.0, 1),\n        (3.0, 3)\n    ]\n\n    all_results = []\n    for R, s in test_cases:\n        # Generate outlier points\n        y_offsets = 0.2 * (np.arange(s) - (s - 1) / 2.0)\n        outliers = np.zeros((s, 2))\n        outliers[:, 0] = -R\n        outliers[:, 1] = y_offsets\n\n        # Create the new dataset with outliers\n        new_dataset = np.vstack([base_dataset, outliers])\n\n        # Analyze the new dataset\n        H_single_1, H_complete_1, H_average_1, H_centroid_1, W_1, new_mst = analyze_dataset(new_dataset)\n        \n        # Compute the change in metrics (delta values)\n        delta_H_single = H_single_1 - H_single_0\n        delta_H_complete = H_complete_1 - H_complete_0\n        delta_H_average = H_average_1 - H_average_0\n        delta_H_centroid = H_centroid_1 - H_centroid_0\n        delta_W = W_1 - W_0\n\n        # Compute m_new: number of MST edges involving an outlier\n        # Outlier indices start at n_base_points\n        rows, cols = new_mst.nonzero()\n        m_new = np.sum((rows >= n_base_points) | (cols >= n_base_points))\n        \n        # Collect results for the current test case\n        case_results = [\n            delta_H_single,\n            delta_H_complete,\n            delta_H_average,\n            delta_H_centroid,\n            delta_W,\n            int(m_new)\n        ]\n        all_results.append(case_results)\n\n    # --- Step 3: Format the final output string ---\n    result_strings = []\n    for res_list in all_results:\n        # Convert all items to their string representation\n        # The last value m_new is already an int.\n        stringified_list = [str(item) for item in res_list]\n        result_strings.append(f\"[{','.join(stringified_list)}]\")\n        \n    final_output = f\"[{','.join(result_strings)}]\"\n    print(final_output)\n\nsolve()\n```", "id": "3140585"}, {"introduction": "在比较了不同联结方法对明显离群点的反应后，我们将进一步探讨一个更细微的问题：当簇的边界因重尾分布而变得模糊时，各种方法表现如何？本练习聚焦于比较在实践中同样受欢迎的完整联结法和平均联结法。你将通过生成和分析服从$\\mathrm{Student}\\text{-}t(\\nu)$分布的合成数据，量化并理解为何完整联结法的“最远邻”规则能更好地抵抗跨簇尾部点的干扰，而平均联结法则倾向于平滑这些极端距离。[@problem_id:3140593]", "problem": "你的任务是实现一个层次凝聚聚类实验，以比较不同连接方法在重尾合成数据上的表现。目标是以一种可复现的方式，量化完全连接法相较于平均连接法如何抵抗跨簇尾部的过早合并。平均连接法会平滑距离，可能导致更早的合并。你的程序必须是最终答案部分指定的完整、可运行的程序。\n\n从核心定义开始，使用以下基本概念：\n- 一个使用欧几里得距离的度量空间，其中对于任何点 $x,y \\in \\mathbb{R}^2$，距离为 $d(x,y)=\\sqrt{(x_1-y_1)^2+(x_2-y_2)^2}$。\n- 层次凝聚聚类（HAC），它在指定的簇间距离下迭代地合并两个最近的簇。簇间距离取决于连接方法：\n  - 完全连接法：对于簇 $A$ 和 $B$，距离为 $D_{\\text{complete}}(A,B)=\\max_{x\\in A,y\\in B} d(x,y)$。\n  - 平均连接法：对于簇 $A$ 和 $B$，距离为 $D_{\\text{average}}(A,B)=\\frac{1}{|A||B|}\\sum_{x\\in A}\\sum_{y\\in B} d(x,y)$。\n- 从自由度为 $\\nu$ 的学生t分布（表示为 $\\mathrm{Student}\\text{-}t(\\nu)$）生成的重尾合成数据，在两个坐标上独立采样，并按因子 $\\sigma$ 进行缩放。\n\n对于每个测试用例，按如下方式在 $\\mathbb{R}^2$ 中生成两个簇：\n1. 为左簇抽取 $n$ 个点，为右簇抽取 $n$ 个点。每个点的坐标都独立地从 $\\mathrm{Student}\\text{-}t(\\nu)$ 分布中采样，然后乘以 $\\sigma$。\n2. 将左簇通过向量 $(-\\Delta/2, 0)$ 进行平移，右簇通过 $(\\Delta/2, 0)$ 进行平移，其中 $\\Delta$ 是中心间距。\n3. 用标签 $0$ 标记左簇的点，用标签 $1$ 标记右簇的点。\n4. 使用固定的随机种子以确保可复现性。\n\n对于每种连接方法（完全连接和平均连接），在欧几里得度量下执行HAC。跟踪树状图中首次出现包含标签 $0$ 和 $1$ 的簇的合并高度。将这些高度分别表示为 $h_{\\text{complete}}$ 和 $h_{\\text{average}}$。每个测试用例关注的量是浮点数 $h_{\\text{complete}} - h_{\\text{average}}$。正值表示完全连接法比平均连接法在更高的树状图高度上更能抵抗跨标签的尾部合并。\n\n将上述过程实现为一个程序，并在以下测试套件上运行它，该套件为每个用例指定了 $n$、$\\nu$、$\\sigma$、$\\Delta$ 和随机种子：\n- 测试用例 1（一般重尾分离）：$n=80$, $\\nu=3$, $\\sigma=0.8$, $\\Delta=5.0$, seed $=2023$。\n- 测试用例 2（中心更近且尾部更重）：$n=80$, $\\nu=2$, $\\sigma=1.0$, $\\Delta=2.0$, seed $=7$。\n- 测试用例 3（大分离，中度重尾）：$n=60$, $\\nu=5$, $\\sigma=0.8$, $\\Delta=8.0$, seed $=99$。\n- 测试用例 4（近高斯尾）：$n=100$, $\\nu=25$, $\\sigma=1.0$, $\\Delta=4.0$, seed $=123$。\n\n不涉及物理单位或角度单位。所有输出都是无单位的实数。你的程序应生成单行输出，其中包含用方括号括起来的逗号分隔列表形式的结果，严格按照测试用例的顺序，例如 $[r_1,r_2,r_3,r_4]$，其中每个 $r_i$ 是为测试用例 $i$ 计算的 $h_{\\text{complete}} - h_{\\text{average}}$。输出必须是浮点数。", "solution": "所述问题是有效的。这是一个定义明确的计算实验，基于统计学习的既定原则，特别是层次凝聚聚类（HAC）。该问题是自包含的，所有必要的参数、定义和程序都已明确指定。它是科学上合理的、客观的，并且由于使用了固定的随机种子，每个测试用例都会产生唯一、可验证的结果。\n\n目标是量化完全连接法和平均连接法在应用于具有两个不同重尾簇的数据集时的行为差异。核心假设是，完全连接法对簇之间的最大距离敏感，因此在防止尾部交织的簇过早合并方面，会比平均连接法更鲁棒，后者会平均掉这些大的距离。\n\n解决方案是通过对每个测试用例遵循一系列严格定义的步骤来实现的。\n\n**步骤 1：合成数据生成**\n\n对于每个测试用例，我们在 $\\mathbb{R}^2$ 中生成一个由两个不同簇组成的数据集。总点数为 $2n$。\n每个点 $(x_1, x_2)$ 的生成过程如下：\n1.  从自由度为 $\\nu$ 的学生t分布（表示为 $\\mathrm{Student}\\text{-}t(\\nu)$）中抽取两个独立的随机变量 $z_1$ 和 $z_2$。选择该分布是因为其重尾特性，特别是对于较小的 $\\nu$ 值。当 $\\nu \\to \\infty$ 时，$\\mathrm{Student}\\text{-}t(\\nu)$ 分布收敛于标准正态分布 $\\mathcal{N}(0,1)$。\n2.  这些变量通过一个因子 $\\sigma$ 进行缩放，得到坐标对 $(\\sigma z_1, \\sigma z_2)$。\n3.  我们为“左”簇生成 $n$ 个这样的点，为“右”簇生成 $n$ 个点。\n4.  为了分离这两个簇，左簇的点通过向量 $(-\\Delta/2, 0)$ 进行平移，右簇的点通过 $(\\Delta/2, 0)$ 进行平移。这使得两个簇的名义中心在第一个坐标轴上相距 $\\Delta$。\n5.  源自左簇的点被赋予标签 $0$，源自右簇的点被赋予标签 $1$。这个基准真相（ground truth）用于评估聚类结果。\n通过为每个测试用例初始化具有特定种子的随机数生成器，整个过程变得确定性和可复现。\n\n**步骤 2：层次凝聚聚类 (HAC)**\n\nHAC 是一种构建簇层次结构的迭代算法。它从将 $2n$ 个数据点中的每一个都视为一个单例簇开始。在随后的每一步中，两个最近的簇被合并成一个新的、更大的簇。这个过程一直持续到只剩下一个包含所有数据点的簇为止。合并的序列形成一个称为树状图的二叉树结构。在树状图中两个簇合并的“高度”对应于发生合并时的簇间距离。\n\n如何度量两个非单例簇之间距离的选择由连接方法决定。问题指定欧几里得距离 $d(x,y)=\\sqrt{(x_1-y_1)^2+(x_2-y_2)^2}$ 作为单个点 $x, y \\in \\mathbb{R}^2$ 之间的基本度量。\n\n**步骤 3：连接方法**\n\n簇间距离使用两种不同的连接准则计算：\n\n1.  **完全连接法**：两个簇 $A$ 和 $B$ 之间的距离定义为 $A$ 中任意点与 $B$ 中任意点之间的最大距离。\n    $$D_{\\text{complete}}(A,B)=\\max_{x\\in A,y\\in B} d(x,y)$$\n    这种“最远邻”方法是保守的。要使两个簇在此度量下被认为是“近”的，它们的所有点必须相对接近于另一个簇的所有点。此属性使完全连接法对异常值敏感，并通常产生更紧凑、球形的簇。如果两个主簇的尾部包含相距很远的点，预计它会抵抗合并这两个簇。\n\n2.  **平均连接法**：簇 $A$ 和 $B$ 之间的距离是来自每个簇的点之间所有成对距离的平均值。\n    $$D_{\\text{average}}(A,B)=\\frac{1}{|A||B|}\\sum_{x\\in A}\\sum_{y\\in B} d(x,y)$$\n    此方法提供了对簇距离更“中心”的度量，有效地平均了少数远距离点的影响。它比完全连接法对异常值不那么敏感，但如果簇在平均距离上很近，即使它们的边界没有很好地分离，也可能导致它们合并。\n\n**步骤 4：量化合并点**\n\n对于每个测试用例和每种连接方法（完全连接和平均连接），我们执行HAC。我们感兴趣的是确定算法首次合并来自两个原始基准真相簇（标签 $0$ 和标签 $1$）的点的精确时刻——由树状图合并高度表示。\n\n程序如下：\n1.  初始化 $2n$ 个簇，每个数据点一个，并存储它们的基准真值标签（$0$ 或 $1$）。\n2.  从最低高度到最高高度，遍历HAC算法的合并过程。\n3.  每次合并都会形成一个新簇。我们通过取合并的两个较小簇的标签集的并集，来确定这个新簇中存在的基准真值标签集。\n4.  当一个新形成的簇首次同时包含标签 $0$ 和标签 $1$ 时，我们记录相应的合并高度。对于完全连接法，该高度为 $h_{\\text{complete}}$；对于平均连接法，为 $h_{\\text{average}}$。\n5.  该测试用例最终关注的量是差值 $h_{\\text{complete}} - h_{\\text{average}}$。该差值为正值证实了以下假设：在这种类型的数据上，完全连接法相比平均连接法，在更大的距离阈值上保持了两个初始簇之间的分离。\n\n将此完整协议系统地应用于指定的全部 $4$ 个测试用例，产生 $4$ 个数值结果。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import t\nfrom scipy.cluster import hierarchy\n\ndef solve():\n    \"\"\"\n    Main function to run the hierarchical clustering experiment on the test suite.\n    \"\"\"\n    # Test cases specify n (points per cluster), nu (degrees of freedom for t-dist),\n    # sigma (scaling factor), Delta (inter-center separation), and a random seed.\n    test_cases = [\n        # (n, nu, sigma, Delta, seed)\n        (80, 3, 0.8, 5.0, 2023), # Test case 1\n        (80, 2, 1.0, 2.0, 7),    # Test case 2\n        (60, 5, 0.8, 8.0, 99),   # Test case 3\n        (100, 25, 1.0, 4.0, 123),  # Test case 4\n    ]\n\n    results = []\n    for n, nu, sigma, delta, seed in test_cases:\n        # Generate the synthetic data for the current test case.\n        X, labels = generate_data(n, nu, sigma, delta, seed)\n\n        # Calculate the first cross-label merge height for complete linkage.\n        h_complete = find_first_cross_label_merge_height(X, labels, 'complete')\n\n        # Calculate the first cross-label merge height for average linkage.\n        h_average = find_first_cross_label_merge_height(X, labels, 'average')\n\n        # The quantity of interest is the difference in merge heights.\n        result = h_complete - h_average\n        results.append(result)\n\n    # Print the final results in the specified format.\n    print(f\"[{','.join(f'{r:.8f}' for r in results)}]\")\n\ndef generate_data(n, nu, sigma, delta, seed):\n    \"\"\"\n    Generates two heavy-tailed clusters in 2D space.\n\n    Args:\n        n (int): Number of points per cluster.\n        nu (float): Degrees of freedom for the Student's t-distribution.\n        sigma (float): Scaling factor for the distribution.\n        delta (float): Separation distance between cluster centers.\n        seed (int): Seed for the random number generator.\n\n    Returns:\n        tuple[np.ndarray, np.ndarray]:\n            - A NumPy array of shape (2*n, 2) containing the coordinates of all points.\n            - A NumPy array of shape (2*n,) containing the ground-truth labels (0 or 1).\n    \"\"\"\n    rng = np.random.default_rng(seed)\n\n    # Generate points for the left cluster from the Student's t-distribution.\n    # The rvs function is used with a generator for reproducible randomness.\n    left_cluster = t.rvs(df=nu, size=(n, 2), random_state=rng) * sigma\n    left_cluster[:, 0] -= delta / 2\n\n    # Generate points for the right cluster.\n    right_cluster = t.rvs(df=nu, size=(n, 2), random_state=rng) * sigma\n    right_cluster[:, 0] += delta / 2\n\n    # Combine points and create corresponding labels.\n    X = np.vstack([left_cluster, right_cluster])\n    labels = np.array([0] * n + [1] * n)\n\n    return X, labels\n\ndef find_first_cross_label_merge_height(X, labels, method):\n    \"\"\"\n    Performs HAC and finds the first merge height that combines points from different labels.\n\n    Args:\n        X (np.ndarray): The data points, shape (N, 2).\n        labels (np.ndarray): The ground-truth labels, shape (N,).\n        method (str): The linkage method ('complete' or 'average').\n\n    Returns:\n        float: The dendrogram height of the first cross-label merge.\n    \"\"\"\n    N = X.shape[0]\n\n    # Perform hierarchical agglomerative clustering using the specified method.\n    # The linkage matrix Z encodes the dendrogram.\n    Z = hierarchy.linkage(X, method=method, metric='euclidean')\n\n    # `cluster_labels` will store the set of original labels for each cluster.\n    # Initially, clusters 0 to N-1 are the original points.\n    cluster_labels = {i: {labels[i]} for i in range(N)}\n\n    # Iterate through each merge recorded in the linkage matrix Z.\n    # Each row `i` represents a merge creating a new cluster with index `N + i`.\n    for i in range(Z.shape[0]):\n        # Get the indices of the two clusters being merged.\n        # Indices  N are original points, indices >= N are newly formed clusters.\n        id1, id2 = int(Z[i, 0]), int(Z[i, 1])\n\n        # Retrieve the sets of original labels for the merging clusters.\n        labels1 = cluster_labels[id1]\n        labels2 = cluster_labels[id2]\n\n        # The new cluster's labels are the union of the merged clusters' labels.\n        new_labels = labels1.union(labels2)\n\n        # The new cluster's index is N + i. We store its label set.\n        cluster_id = N + i\n        cluster_labels[cluster_id] = new_labels\n\n        # Check if the new cluster is \"impure\", i.e., contains both original labels.\n        if 0 in new_labels and 1 in new_labels:\n            # If so, this is the first cross-label merge.\n            # The height of this merge is given in the 3rd column of Z.\n            merge_height = Z[i, 2]\n            return merge_height\n\n    # This part should not be reached in a valid scenario with two labels.\n    # It would imply all points had the same label, which is not the case.\n    return np.inf\n\n# Execute the main function.\nsolve()\n```", "id": "3140593"}]}