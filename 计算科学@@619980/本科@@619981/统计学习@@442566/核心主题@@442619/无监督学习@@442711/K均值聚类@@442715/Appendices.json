{"hands_on_practices": [{"introduction": "K-means 算法的核心是 Lloyd 算法，但其性能并非一目了然。本练习将引导你从第一性原理出发，推导其计算复杂度，并亲手构建数据集来观察其在不同情况下的收敛速度。通过理论分析与编程实践的结合 [@problem_id:3134960]，你将深刻理解算法的效率及其对数据结构的敏感性，这对于在实际应用中做出明智的决策至关重要。", "problem": "考虑用于 $k$-均值聚类的 Lloyd 算法。该算法迭代执行两个步骤：一个分配步骤，使用 $\\mathbb{R}^d$ 中的欧几里得距离将每个数据点映射到最近的质心；以及一个更新步骤，将每个质心重新计算为其当前簇内所有点的算术平均值。令 $n$ 表示点的数量，$k$ 表示簇的数量，$d$ 表示维度，$t$ 表示直到收敛（定义为分配不再发生变化）所需的迭代次数。分析的基本依据应为以下经过充分检验的事实：在每个分配步骤中，对于 $n$ 个点中的每一个，都会使用欧几里得范数计算其到 $\\mathbb{R}^d$ 中 $k$ 个质心中每一个的距离；在每个更新步骤中，每个质心都通过计算分配给它的点的均值来更新。\n\n任务：\n- 从这些事实以及欧几里得距离和算术平均值的定义出发，推导 Lloyd 算法单次迭代的计算复杂度，并利用此推导来证明整个算法直到收敛的总渐近复杂度为 $O(n k d t)$。\n- 按照上述描述实现用于 $k$-均值聚类的 Lloyd 算法，使用 $\\mathbb{R}^d$ 中的欧几里得距离，并基于分配不变来判断收敛。如果在更新过程中有任何簇变为空，则确定性地将该质心重新初始化为当前距离其分配的质心具有最大欧几里得距离的数据点（这样可以保持进展并避免未定义的均值）。\n- 构建并在以下确定性测试套件上运行。下面提到的所有角度都必须以弧度为单位解释。\n\n测试套件：\n1. $\\mathbb{R}^2$ 中的对抗性圆形数据集，旨在展示缓慢的收敛行为。其中 $k=2$，质心初始化为单位圆上的两个相邻点，以创建一个狭窄的初始决策边界，该边界会随着质心的移动而迁移。对于每个 $n \\in \\{16,32,64\\}$，在单位圆上以角度 $\\theta_i = \\frac{2\\pi i}{n}$（其中 $i = 0,1,\\dots,n-1$）均匀间隔地构造 $n$ 个点，其笛卡尔坐标表示为 $(\\cos\\theta_i,\\sin\\theta_i)$。将质心初始化为角度为 $\\theta_0 = 0$ 和 $\\theta_1 = \\frac{2\\pi}{n}$ 的点。\n2. $\\mathbb{R}^2$ 中一个分隔良好的“理想路径”数据集，旨在展示快速收敛。其中 $n=200$，$k=2$，由两个各向同性的高斯团块形成：通过为伪随机数生成器设定种子，并从一个中心在 $(0,0)$、每个坐标方差为 $1$ 的正态分布中提取一个固定序列，确定性地采样 $100$ 个点；以及中心在 $(10,10)$、具有相同方差的另外 $100$ 个点。为保持确定性，将质心初始化为数据集顺序中的前两个点。\n3. $\\mathbb{R}^1$ 中一个具有重复相同值的边缘情况数据集，其中 $n=9$，$k=3$，所有点都等于 $0$。将质心初始化为 $-1$、$0$ 和 $1$。\n\n对于每个测试用例，运行 Lloyd 算法直至收敛，并记录：\n- 迭代次数 $t$。\n- 乘积 $n \\cdot k \\cdot d \\cdot t$（作为一个整数），这是一个标量，反映了复杂度推导所隐含的渐近操作计数因子。\n\n你的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表。每个测试用例对应一个二元列表 $[t, n \\cdot k \\cdot d \\cdot t]$，列表顺序与测试套件的顺序相同，例如 $[[t_1, n_1 k_1 d_1 t_1],[t_2, n_2 k_2 d_2 t_2],[t_3, n_3 k_3 d_3 t_3],[t_4, n_4 k_4 d_4 t_4]]$。\n\n注意：不涉及任何物理单位。所有角度均以弧度为单位指定。要求的输出是整数或整数列表，最终输出格式必须严格遵守上述单行方括号列表的形式。", "solution": "用户提供的问题经评估是有效的，因为它科学地基于成熟的k-均值聚类理论，其确定性测试用例使其成为一个适定的问题，并且问题是客观的，不含矛盾或含糊之处。它提出了一个算法分析和实现方面的标准但非平凡的任务。\n\n### 第1部分：计算复杂度推导\n\nLloyd 算法的总计算复杂度是迭代次数 $t$ 与单次迭代复杂度的乘积。一次迭代包括两个连续的步骤：分配步骤和更新步骤。我们根据所提供的先验事实来推导每一步的复杂度。\n\n令 $n$ 为数据点的数量，$k$ 为簇的数量，$d$ 为数据空间 $\\mathbb{R}^d$ 的维度。\n\n**分配步骤复杂度**\n\n问题陈述，在分配步骤中，“对于 $n$ 个点中的每一个，都会使用欧几里得范数计算其到 $\\mathbb{R}^d$ 中 $k$ 个质心中每一个的距离”。\n\n1.  **距离计算**：一个数据点 $\\mathbf{x} = (x_1, \\dots, x_d)$ 和一个质心 $\\mathbf{c} = (c_1, \\dots, c_d)$ 之间的欧几里得距离由 $D(\\mathbf{x}, \\mathbf{c}) = \\sqrt{\\sum_{i=1}^{d} (x_i - c_i)^2}$ 给出。要计算这个值，我们执行：\n    *   $d$ 次减法 ($x_i - c_i$)。\n    *   $d$ 次乘法（平方）。\n    *   $d-1$ 次加法（求和）。\n    *   $1$ 次平方根运算。\n    算术运算的总数为 $d+d+(d-1)+1 = 3d$。因此，单次距离计算的复杂度与维度 $d$ 成正比，即 $O(d)$。注意，为了找到*最近*的质心，我们可以使用欧几里得距离的平方，$D^2(\\mathbf{x}, \\mathbf{c}) = \\sum_{i=1}^{d} (x_i - c_i)^2$，这样可以避免计算成本更高的平方根运算，但其复杂度仍为 $O(d)$。\n\n2.  **总分配成本**：对于 $n$ 个数据点中的每一个，我们都计算它到所有 $k$ 个质心的距离。这需要进行 $n \\times k$ 次距离计算。因此，所有距离计算的总成本为 $n \\times k \\times O(d) = O(nkd)$。\n\n3.  **寻找最小值**：对于每个数据点，在计算其到 $k$ 个质心的距离后，我们必须找到这 $k$ 个值中的最小值来确定分配。这需要 $k-1$ 次比较，这是一个复杂度为 $O(k)$ 的操作。由于必须对所有 $n$ 个点执行此操作，因此寻找所有最小值的总复杂度为 $n \\times O(k) = O(nk)$。\n\n4.  **分配步骤总复杂度**：分配步骤的总复杂度是距离计算和寻找最小值成本的总和：$O(nkd) + O(nk)$。由于 $d \\ge 1$， $O(nkd)$ 项占主导地位。因此，分配步骤的复杂度为 $O(nkd)$。\n\n**更新步骤复杂度**\n\n问题陈述，在更新步骤中，“每个质心都通过计算分配给它的点的均值来更新”。\n\n1.  **均值计算**：假设簇 $j$ 包含 $n_j$ 个点，记为 $\\{\\mathbf{x}_{j,1}, \\dots, \\mathbf{x}_{j, n_j}\\}$。新的质心 $\\mathbf{c}_j'$ 是它们的算术平均值：$\\mathbf{c}_j' = \\frac{1}{n_j} \\sum_{i=1}^{n_j} \\mathbf{x}_{j,i}$。\n    *   要计算总和 $\\sum_{i=1}^{n_j} \\mathbf{x}_{j,i}$，我们需要对 $n_j$ 个维度为 $d$ 的向量求和。这需要 $(n_j - 1) \\times d$ 次加法。复杂度为 $O(n_j d)$。\n    *   要计算最终的均值，我们将得到的和向量除以标量 $n_j$。这涉及 $d$ 次除法，复杂度为 $O(d)$ 的操作。\n    *   更新簇 $j$ 的一个质心的总复杂度为 $O(n_j d) + O(d) = O(n_j d)$。\n\n2.  **总更新成本**：我们必须对所有 $k$ 个簇执行此更新。总复杂度是每个簇成本的总和：$\\sum_{j=1}^{k} O(n_j d)$。这可以写为 $O(d \\sum_{j=1}^{k} n_j)$。\n    由于 $n$ 个数据点中的每一个都被精确地分配给一个簇，所以所有簇中点的数量之和等于点的总数：$\\sum_{j=1}^{k} n_j = n$。\n    因此，更新步骤的复杂度为 $O(nd)$。\n\n处理空簇的特殊规则要求找到与其分配的质心距离最大的数据点。这涉及对 $n$ 个预先计算的距离进行扫描，这是一个 $O(n)$ 的操作。这个成本仅在某个簇为空时产生，并且被标准更新过程的 $O(nd)$ 成本所主导（因为 $d \\ge 1$），因此它不会改变更新步骤的渐近复杂度。\n\n**总复杂度**\n\nLloyd 算法单次迭代的复杂度是分配步骤和更新步骤复杂度的总和：\n$$ C_{\\text{iteration}} = C_{\\text{assignment}} + C_{\\text{update}} = O(nkd) + O(nd) $$\n由于 $k \\ge 1$，$O(nkd)$ 项主导了 $O(nd)$。因此，单次迭代的复杂度为 $O(nkd)$。\n\n如果算法运行 $t$ 次迭代直到收敛，那么总计算复杂度为：\n$$ C_{\\text{total}} = t \\times C_{\\text{iteration}} = t \\times O(nkd) = O(nkdt) $$\n这就证明了整个算法的渐近复杂度为 $O(nkdt)$。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.spatial.distance import cdist\n\ndef lloyds_algorithm(data, k, initial_centroids):\n    \"\"\"\n    Performs k-means clustering using Lloyd's algorithm.\n\n    Args:\n        data (np.ndarray): The dataset of shape (n, d).\n        k (int): The number of clusters.\n        initial_centroids (np.ndarray): The initial centroids of shape (k, d).\n\n    Returns:\n        int: The number of iterations 't' until convergence.\n    \"\"\"\n    n, d = data.shape\n    centroids = np.copy(initial_centroids)\n    # Initialize assignments with a sentinel value that won't match any valid assignment\n    assignments = np.full(n, -1, dtype=int)\n    \n    t = 0\n    while True:\n        # --- Assignment Step ---\n        # Compute squared Euclidean distances from each point to each centroid\n        # cdist is efficient for this. 'sqeuclidean' avoids sqrt calculation.\n        dist_sq = cdist(data, centroids, 'sqeuclidean')\n        \n        # Assign each point to the closest centroid\n        # np.argmin handles ties by picking the first occurrence.\n        new_assignments = np.argmin(dist_sq, axis=1)\n        \n        # --- Convergence Check ---\n        # If assignments have not changed, the algorithm has converged.\n        if np.array_equal(new_assignments, assignments):\n            break\n            \n        assignments = new_assignments\n        \n        # --- Update Step ---\n        \n        # Per problem spec, find the point to use for reinitializing empty clusters.\n        # This is the point with the largest distance to ITS OWN assigned centroid.\n        # We use the distances and assignments from the current step to find this point.\n        dists_to_assigned_centroids = dist_sq[np.arange(n), assignments]\n        farthest_point_idx = np.argmax(dists_to_assigned_centroids)\n        reinit_point = data[farthest_point_idx]\n        \n        new_centroids = np.zeros_like(centroids)\n        for j in range(k):\n            # Select all points assigned to the current cluster\n            cluster_points = data[assignments == j]\n            \n            # If the cluster is not empty, compute the mean\n            if len(cluster_points) > 0:\n                new_centroids[j] = np.mean(cluster_points, axis=0)\n            # If the cluster is empty, reinitialize its centroid\n            else:\n                new_centroids[j] = reinit_point\n\n        centroids = new_centroids\n        t += 1 # Increment iteration count after a full (assign + update) cycle\n\n    return t\n\ndef solve():\n    \"\"\"\n    Generates test cases, runs Lloyd's algorithm, and prints the results.\n    \"\"\"\n    test_cases = []\n    \n    # Test Suite Part 1: Adversarial circle datasets\n    k1 = 2\n    d1 = 2\n    for n1 in [16, 32, 64]:\n        thetas = 2 * np.pi * np.arange(n1) / n1\n        points = np.column_stack([np.cos(thetas), np.sin(thetas)])\n        \n        # Initial centroids at angles 0 and 2*pi/n\n        c_theta_0 = 0.0\n        c_theta_1 = 2 * np.pi / n1\n        initial_centroids = np.array([\n            [np.cos(c_theta_0), np.sin(c_theta_0)],\n            [np.cos(c_theta_1), np.sin(c_theta_1)]\n        ])\n        test_cases.append({\n            \"name\": f\"Circle n={n1}\",\n            \"data\": points, \"k\": k1, \"d\": d1, \"n\": n1,\n            \"initial_centroids\": initial_centroids\n        })\n\n    # Test Suite Part 2: Well-separated Gaussian blobs\n    n2 = 200\n    k2 = 2\n    d2 = 2\n    np.random.seed(0) # For deterministic results\n    blob1 = np.random.normal(loc=0.0, scale=1.0, size=(100, d2))\n    blob2 = np.random.normal(loc=10.0, scale=1.0, size=(100, d2))\n    points2 = np.vstack([blob1, blob2])\n    # Initialize centroids to the first two points of the dataset\n    initial_centroids2 = points2[:k2]\n    test_cases.append({\n        \"name\": \"Gaussians\",\n        \"data\": points2, \"k\": k2, \"d\": d2, \"n\": n2,\n        \"initial_centroids\": initial_centroids2\n    })\n    \n    # Test Suite Part 3: Edge-case with identical points\n    n3 = 9\n    k3 = 3\n    d3 = 1\n    points3 = np.zeros((n3, d3))\n    initial_centroids3 = np.array([[-1.0], [0.0], [1.0]])\n    test_cases.append({\n        \"name\": \"Edge-case\",\n        \"data\": points3, \"k\": k3, \"d\": d3, \"n\": n3,\n        \"initial_centroids\": initial_centroids3\n    })\n    \n    results = []\n    for case in test_cases:\n        t = lloyds_algorithm(case[\"data\"], case[\"k\"], case[\"initial_centroids\"])\n        n, k, d = case[\"n\"], case[\"k\"], case[\"d\"]\n        product = n * k * d * t\n        results.append([t, product])\n    \n    # The final print statement must produce the exact required format.\n    # str() on a list of lists creates a string like '[[...], [...]]'\n    # .replace(\" \", \"\") removes spaces to match the required dense format.\n    print(str(results).replace(\" \", \"\"))\n\nsolve()\n```", "id": "3134960"}, {"introduction": "现实世界的数据集通常包含数值和类别等混合特征，而标准的 K-means 算法无法直接处理。本练习聚焦于特征工程这一关键步骤，指导你如何使用独热编码 (one-hot encoding) 来转化类别数据，并探索缩放因子 $\\alpha$ 如何影响最终的聚类结果。通过亲手操作 [@problem_id:3134973]，你将量化地理解预处理选择对基于距离的算法的重要性，并掌握在混合数据上成功应用 K-means 的核心技巧。", "problem": "考虑一个数据集，其中包含具有一个实值特征和一个分类特征的点。该分类特征可取三个值之一：A、B 或 C。您将使用独热编码来表示分类特征，并对每个虚拟变量应用一个缩放因子 $\\,\\alpha\\,$。也就是说，对于类别 A，使用独热向量 $[1,0,0]$；对于类别 B，使用 $[0,1,0]$；对于类别 C，使用 $[0,0,1]$，然后将此向量乘以 $\\,\\alpha\\,$。实值特征保持不缩放。一个点的完整编码特征向量是实值特征（第一分量）和经 $\\,\\alpha\\,$ 缩放的独热向量（剩余分量）的串联。\n\n使用的基本定义：\n- 在 $\\mathbb{R}^d$ 中，两个向量 $\\,\\mathbf{x}\\,$ 和 $\\,\\mathbf{y}\\,$ 之间的欧几里得距离由 $\\,\\|\\mathbf{x}-\\mathbf{y}\\|_2\\,$ 给出，而欧几里得距离的平方为 $\\,\\|\\mathbf{x}-\\mathbf{y}\\|_2^2\\,$。\n- 对于具有质心 $\\,\\{\\boldsymbol{\\mu}_j\\}_{j=1}^k\\,$ 和将每个点索引 $\\,i\\,$ 映射到其簇的分配函数 $\\,c(i)\\in\\{1,\\dots,k\\}\\,$ 的 $\\,k\\,$ 个簇，$\\,k$-means 目标函数 $\\,J\\,$ 由下式给出\n$$\nJ \\;=\\; \\sum_{i} \\big\\| \\mathbf{x}_i - \\boldsymbol{\\mu}_{c(i)} \\big\\|_2^2.\n$$\n簇的质心定义为分配给该簇的特征向量的算术平均值。\n\n您的任务是分析分类独热虚拟变量的缩放因子 $\\,\\alpha\\,$ 在单次 Lloyd 迭代（分配步骤后跟质心重新计算，但不重复）下如何影响欧几里得距离和 $\\,k$-means 目标函数 $\\,J\\,$。\n\n使用以下包含 $\\,6\\,$ 个点的数据集。每个点都有一个实值分量和一个分类标签：\n- 点 $\\,0\\,$：实值 $\\,0.0\\,$，类别 A。\n- 点 $\\,1\\,$：实值 $\\,0.3\\,$，类别 A。\n- 点 $\\,2\\,$：实值 $\\,0.9\\,$，类别 A。\n- 点 $\\,3\\,$：实值 $\\,1.2\\,$，类别 B。\n- 点 $\\,4\\,$：实值 $\\,0.6\\,$，类别 C。\n- 点 $\\,5\\,$：实值 $\\,1.0\\,$，类别 C。\n\n使用独热顺序 $[A,B,C]$ 对类别进行编码。\n\n对于测试套件中每个指定的 $\\,\\alpha\\,$ 值，执行以下操作：\n1. 使用上述方案（先是实值特征，然后是经 $\\,\\alpha\\,$ 缩放的独热向量）构建编码数据集。\n2. 将 $\\,k=2\\,$ 个质心分别初始化为点 $\\,0\\,$ 和点 $\\,5\\,$ 的编码向量。\n3. 使用欧几里得距离的平方将每个点分配给最近的质心，以避免不必要的平方根；如果距离完全相等，则通过选择索引较小的质心来打破平局。\n4. 将质心重新计算为编码特征空间中分配给每个簇的点的均值。\n5. 将 $\\,k$-means 目标函数 $\\,J\\,$ 计算为每个点到其重新计算的簇质心的欧几里得距离平方和。\n6. 计算点 $\\,1\\,$ 和点 $\\,4\\,$ 的编码向量之间的欧几里得距离 $\\,\\|\\mathbf{x}_1 - \\mathbf{x}_4\\|_2\\,$。\n\n$\\,\\alpha\\,$ 的测试套件：\n- $\\,\\alpha = 0.0\\,$，\n- $\\,\\alpha = 0.5\\,$，\n- $\\,\\alpha = 1.0\\,$，\n- $\\,\\alpha = 1.5\\,$。\n\n您的程序应输出一行，其中包含测试套件的结果，形式为一对浮点数 $[d, J]$ 的列表。其中，对于给定的 $\\,\\alpha\\,$，$\\,d\\,$ 是欧几里得距离 $\\,\\|\\mathbf{x}_1 - \\mathbf{x}_4\\|_2\\,$，而 $\\,J\\,$ 是在该 $\\,\\alpha\\,$ 值下经过单次 Lloyd 迭代后对应的 $\\,k$-means 目标函数值。最终输出格式必须是单行形式\n$$\n[[d_0,J_0],[d_1,J_1],[d_2,J_2],[d_3,J_3]]\n$$\n对应于给定顺序的 $\\,\\alpha\\,$ 值。将每个浮点数表示为四舍五入到六位小数。此问题不应读取任何外部输入，也不涉及任何物理单位或角度。", "solution": "用户希望我解决一个 k-means 聚类问题。\n\n### 步骤 1：验证问题陈述\n我将首先根据指定规则验证问题陈述。\n\n#### 步骤 1.1：提取已知条件\n- **数据集**：6 个点，每个点都有一个实值特征和一个分类特征（A、B 或 C）。\n    - 点 0：实值 $0.0$，类别 A。\n    - 点 1：实值 $0.3$，类别 A。\n    - 点 2：实值 $0.9$，类别 A。\n    - 点 3：实值 $1.2$，类别 B。\n    - 点 4：实值 $0.6$，类别 C。\n    - 点 5：实值 $1.0$，类别 C。\n- **特征编码**：\n    - 具有实值 $r$ 和一个类别的点被编码成一个向量。\n    - 分类特征按 [A, B, C] 顺序进行独热编码。\n        - A: $[1,0,0]$\n        - B: $[0,1,0]$\n        - C: $[0,0,1]$\n    - 独热向量按因子 $\\alpha$ 进行缩放。\n    - 最终特征向量是实值特征和缩放后的独热向量的串联：$[r, \\alpha \\cdot \\text{独热向量}]$。\n- **距离度量**：用于分配的欧几里得距离平方 $\\|\\mathbf{x}-\\mathbf{y}\\|_2^2$。最终距离 $\\|\\mathbf{x}_1-\\mathbf{x}_4\\|_2$ 是欧几里得距离。\n- **k-means 算法**：\n    - $k=2$。\n    - 目标函数：$J = \\sum_{i} \\| \\mathbf{x}_i - \\boldsymbol{\\mu}_{c(i)} \\|_2^2$。\n    - 初始质心：点 0 和点 5 的编码向量。\n    - 过程：单次 Lloyd 迭代（一次分配步骤，一次质心重新计算步骤）。\n    - 打破平局：如果到质心的距离相等，则分配给索引较小的质心。\n- **对每个 $\\alpha$ 的任务**：\n    1. 构建编码数据集。\n    2. 初始化质心。\n    3. 将点分配给最近的质心。\n    4. 重新计算质心。\n    5. 计算最终的 k-means 目标函数 $J$。\n    6. 计算距离 $d = \\|\\mathbf{x}_1 - \\mathbf{x}_4\\|_2$。\n- **测试套件**：$\\alpha \\in \\{0.0, 0.5, 1.0, 1.5\\}$。\n- **输出格式**：一个 $[d, J]$ 对的列表，`[[d_0,J_0],[d_1,J_1],[d_2,J_2],[d_3,J_3]]`，数值四舍五入到六位小数。\n\n#### 步骤 1.2：使用提取的已知条件进行验证\n- **科学依据**：该问题基于机器学习和线性代数中的标准概念，特别是 k-means 聚类、独热编码和欧几里得距离。定义和程序都是标准且正确的。\n- **定义明确**：问题已完全指定。数据集、编码方案、初始条件和算法步骤都已明确定义，没有歧义。对于每个 $\\alpha$ 值，都存在唯一的解。\n- **客观性**：问题陈述以精确、客观的语言编写。它要求进行特定的、可验证的计算。\n- 该问题不违反任何无效性标准。它是统计学习领域内一个定义明确的计算任务。\n\n#### 步骤 1.3：结论和行动\n问题有效。我将继续进行解答。\n\n### 步骤 2：解答\n解决方案是针对通用 $\\alpha$ 的分步计算，然后将其应用于测试套件中的特定值。\n\n#### 2.1 特征向量编码\n一个具有实值 $r$ 和一个类别的点表示为一个 $4$ 维向量 $\\mathbf{x} = [r, \\alpha v_1, \\alpha v_2, \\alpha v_3]$，其中 $[v_1, v_2, v_3]$ 是按 [A, B, C] 顺序排列的类别的独热向量。\n\n数据集编码如下：\n- $\\mathbf{x}_0 = [0.0, \\alpha, 0, 0]$\n- $\\mathbf{x}_1 = [0.3, \\alpha, 0, 0]$\n- $\\mathbf{x}_2 = [0.9, \\alpha, 0, 0]$\n- $\\mathbf{x}_3 = [1.2, 0, \\alpha, 0]$\n- $\\mathbf{x}_4 = [0.6, 0, 0, \\alpha]$\n- $\\mathbf{x}_5 = [1.0, 0, 0, \\alpha]$\n\n#### 2.2 单次 Lloyd 迭代\n\n**初始质心**\n两个初始质心 $\\boldsymbol{\\mu}_1^{(0)}$ 和 $\\boldsymbol{\\mu}_2^{(0)}$ 被初始化为点 $0$ 和点 $5$ 的编码向量。\n- $\\boldsymbol{\\mu}_1^{(0)} = \\mathbf{x}_0 = [0.0, \\alpha, 0, 0]$\n- $\\boldsymbol{\\mu}_2^{(0)} = \\mathbf{x}_5 = [1.0, 0, 0, \\alpha]$\n\n**分配步骤**\n使用欧几里得距离平方，将每个点 $\\mathbf{x}_i$ 分配给对应于更近质心的簇。\n\n- 对于 $\\mathbf{x}_0, \\mathbf{x}_1, \\mathbf{x}_3, \\mathbf{x}_4, \\mathbf{x}_5$，分配与 $\\alpha$ 无关：\n    - $\\mathbf{x}_0$: $\\|\\mathbf{x}_0 - \\boldsymbol{\\mu}_1^{(0)}\\|_2^2 = 0$。分配给簇 $1$。\n    - $\\mathbf{x}_1$: $\\|\\mathbf{x}_1 - \\boldsymbol{\\mu}_1^{(0)}\\|_2^2 = (0.3)^2 = 0.09$。$\\|\\mathbf{x}_1 - \\boldsymbol{\\mu}_2^{(0)}\\|_2^2 = (-0.7)^2 + \\alpha^2 + (-\\alpha)^2 = 0.49 + 2\\alpha^2$。分配给簇 $1$。\n    - $\\mathbf{x}_3$: $\\|\\mathbf{x}_3 - \\boldsymbol{\\mu}_1^{(0)}\\|_2^2 = (1.2)^2 + (-\\alpha)^2 + \\alpha^2 = 1.44 + 2\\alpha^2$。$\\|\\mathbf{x}_3 - \\boldsymbol{\\mu}_2^{(0)}\\|_2^2 = (0.2)^2 + \\alpha^2 + (-\\alpha)^2 = 0.04 + 2\\alpha^2$。分配给簇 $2$。\n    - $\\mathbf{x}_4$: $\\|\\mathbf{x}_4 - \\boldsymbol{\\mu}_1^{(0)}\\|_2^2 = (0.6)^2 + (-\\alpha)^2 + \\alpha^2 = 0.36 + 2\\alpha^2$。$\\|\\mathbf{x}_4 - \\boldsymbol{\\mu}_2^{(0)}\\|_2^2 = (-0.4)^2 = 0.16$。分配给簇 $2$。\n    - $\\mathbf{x}_5$: $\\|\\mathbf{x}_5 - \\boldsymbol{\\mu}_2^{(0)}\\|_2^2 = 0$。分配给簇 $2$。\n\n- 对于 $\\mathbf{x}_2 = [0.9, \\alpha, 0, 0]$，分配取决于 $\\alpha$：\n    - $\\|\\mathbf{x}_2 - \\boldsymbol{\\mu}_1^{(0)}\\|_2^2 = (0.9)^2 = 0.81$。\n    - $\\|\\mathbf{x}_2 - \\boldsymbol{\\mu}_2^{(0)}\\|_2^2 = (0.9-1.0)^2 + \\alpha^2 + (-\\alpha)^2 = (-0.1)^2 + 2\\alpha^2 = 0.01 + 2\\alpha^2$。\n    - 我们比较 $0.81$ 和 $0.01 + 2\\alpha^2$。如果 $0.81 \\le 0.01 + 2\\alpha^2$，则分配给簇 1，否则分配给簇 2。\n    - $0.81 \\le 0.01 + 2\\alpha^2 \\implies 0.80 \\le 2\\alpha^2 \\implies 0.4 \\le \\alpha^2$。\n    - 因此，如果 $\\alpha^2 \\ge 0.4$，$\\mathbf{x}_2$ 被分配给簇 1；如果 $\\alpha^2  0.4$，则分配给簇 2。\n\n测试值为 $\\alpha \\in \\{0.0, 0.5, 1.0, 1.5\\}$。\n- 对于 $\\alpha=0.0$, $\\alpha^2 = 0.0  0.4$。\n- 对于 $\\alpha=0.5$, $\\alpha^2 = 0.25  0.4$。\n- 对于 $\\alpha=1.0$, $\\alpha^2 = 1.0 \\ge 0.4$。\n- 对于 $\\alpha=1.5$, $\\alpha^2 = 2.25 \\ge 0.4$。\n\n这为簇分配定义了两种情况。\n\n**更新步骤（质心重新计算）**\n\n- **情况 1：$\\alpha^2  0.4$（对于 $\\alpha=0.0, 0.5$）**\n    - 簇 1：$\\{\\mathbf{x}_0, \\mathbf{x}_1\\}$\n    - 簇 2：$\\{\\mathbf{x}_2, \\mathbf{x}_3, \\mathbf{x}_4, \\mathbf{x}_5\\}$\n    - 新质心：\n        - $\\boldsymbol{\\mu}_1^{(1)} = \\frac{1}{2}(\\mathbf{x}_0 + \\mathbf{x}_1) = \\frac{1}{2}[0.3, 2\\alpha, 0, 0] = [0.15, \\alpha, 0, 0]$\n        - $\\boldsymbol{\\mu}_2^{(1)} = \\frac{1}{4}(\\mathbf{x}_2+\\mathbf{x}_3+\\mathbf{x}_4+\\mathbf{x}_5) = \\frac{1}{4}[3.7, \\alpha, \\alpha, 2\\alpha] = [0.925, 0.25\\alpha, 0.25\\alpha, 0.5\\alpha]$\n\n- **情况 2：$\\alpha^2 \\ge 0.4$（对于 $\\alpha=1.0, 1.5$）**\n    - 簇 1：$\\{\\mathbf{x}_0, \\mathbf{x}_1, \\mathbf{x}_2\\}$\n    - 簇 2：$\\{\\mathbf{x}_3, \\mathbf{x}_4, \\mathbf{x}_5\\}$\n    - 新质心：\n        - $\\boldsymbol{\\mu}_1^{(1)} = \\frac{1}{3}(\\mathbf{x}_0+\\mathbf{x}_1+\\mathbf{x}_2) = \\frac{1}{3}[1.2, 3\\alpha, 0, 0] = [0.4, \\alpha, 0, 0]$\n        - $\\boldsymbol{\\mu}_2^{(1)} = \\frac{1}{3}(\\mathbf{x}_3+\\mathbf{x}_4+\\mathbf{x}_5) = \\frac{1}{3}[2.8, 0, \\alpha, 2\\alpha] = [\\frac{2.8}{3}, 0, \\frac{\\alpha}{3}, \\frac{2\\alpha}{3}]$\n\n#### 2.3 计算所需输出\n\n**欧几里得距离 $d = \\|\\mathbf{x}_1 - \\mathbf{x}_4\\|_2$**\n这个距离只取决于编码，而不取决于聚类结果。\n- $\\mathbf{x}_1 - \\mathbf{x}_4 = [0.3-0.6, \\alpha-0, 0-0, 0-\\alpha] = [-0.3, \\alpha, 0, -\\alpha]$\n- $d^2 = \\|\\mathbf{x}_1 - \\mathbf{x}_4\\|_2^2 = (-0.3)^2 + \\alpha^2 + 0^2 + (-\\alpha)^2 = 0.09 + 2\\alpha^2$\n- $d = \\sqrt{0.09 + 2\\alpha^2}$\n\n**k-means 目标函数 $J$**\n目标函数 $J$ 是从每个点到其新质心 $\\boldsymbol{\\mu}_{c(i)}^{(1)}$ 的距离平方和。\n\n- **情况 1（$J_1$ 针对 $\\alpha^2  0.4$）：**\n    - 簇 1 的距离平方和：$\\|\\mathbf{x}_0 - \\boldsymbol{\\mu}_1^{(1)}\\|_2^2 + \\|\\mathbf{x}_1 - \\boldsymbol{\\mu}_1^{(1)}\\|_2^2 = (0.0-0.15)^2 + (0.3-0.15)^2 = 0.0225 + 0.0225 = 0.045$。\n    - 簇 2 的距离平方和：这是各分量方差之和。更直接的计算得出 $\\sum_{i \\in C_2} \\|\\mathbf{x}_i - \\boldsymbol{\\mu}_2^{(1)}\\|_2^2 = 0.1875 + 2.5\\alpha^2$。\n    - $J_1 = 0.045 + 0.1875 + 2.5\\alpha^2 = 0.2325 + 2.5\\alpha^2$。\n\n- **情况 2（$J_2$ 针对 $\\alpha^2 \\ge 0.4$）：**\n    - 簇 1 的距离平方和：$\\|\\mathbf{x}_0 - \\boldsymbol{\\mu}_1^{(1)}\\|_2^2 + \\|\\mathbf{x}_1 - \\boldsymbol{\\mu}_1^{(1)}\\|_2^2 + \\|\\mathbf{x}_2 - \\boldsymbol{\\mu}_1^{(1)}\\|_2^2 = (0.0-0.4)^2 + (0.3-0.4)^2 + (0.9-0.4)^2 = 0.16 + 0.01 + 0.25 = 0.42$。\n    - 簇 2 的距离平方和：$\\sum_{i \\in C_2} \\|\\mathbf{x}_i - \\boldsymbol{\\mu}_2^{(1)}\\|_2^2 = \\frac{1}{3}(0.56 + 4\\alpha^2)$。\n    - $J_2 = 0.42 + \\frac{0.56 + 4\\alpha^2}{3}$。\n\n#### 2.4 测试套件的最终计算\n\n- **对于 $\\alpha = 0.0$：**（情况 1）\n    - $d = \\sqrt{0.09 + 2(0.0)^2} = \\sqrt{0.09} = 0.3$。\n    - $J = 0.2325 + 2.5(0.0)^2 = 0.2325$。\n    - 结果：$[0.300000, 0.232500]$\n\n- **对于 $\\alpha = 0.5$：**（情况 1）\n    - $d = \\sqrt{0.09 + 2(0.5)^2} = \\sqrt{0.09 + 0.5} = \\sqrt{0.59} \\approx 0.768115$。\n    - $J = 0.2325 + 2.5(0.5)^2 = 0.2325 + 0.625 = 0.8575$。\n    - 结果：$[0.768115, 0.857500]$\n\n- **对于 $\\alpha = 1.0$：**（情况 2）\n    - $d = \\sqrt{0.09 + 2(1.0)^2} = \\sqrt{2.09} \\approx 1.445683$。\n    - $J = 0.42 + \\frac{0.56 + 4(1.0)^2}{3} = 0.42 + \\frac{4.56}{3} = 0.42 + 1.52 = 1.94$。\n    - 结果：$[1.445683, 1.940000]$\n\n- **对于 $\\alpha = 1.5$：**（情况 2）\n    - $d = \\sqrt{0.09 + 2(1.5)^2} = \\sqrt{0.09 + 4.5} = \\sqrt{4.59} \\approx 2.142429$。\n    - $J = 0.42 + \\frac{0.56 + 4(1.5)^2}{3} = 0.42 + \\frac{0.56 + 9}{3} = 0.42 + \\frac{9.56}{3} \\approx 0.42 + 3.186667 = 3.606667$。\n    - 结果：$[2.142429, 3.606667]$", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Performs a single Lloyd iteration for k-means clustering on a mixed-type dataset\n    for various scaling factors of the categorical features.\n    \"\"\"\n    # Define the dataset of points with a real value and a categorical label.\n    raw_data = [\n        (0.0, 'A'), (0.3, 'A'), (0.9, 'A'),\n        (1.2, 'B'), (0.6, 'C'), (1.0, 'C')\n    ]\n\n    # Map categories to one-hot vectors based on the order [A, B, C].\n    categories = {'A': np.array([1, 0, 0]), 'B': np.array([0, 1, 0]), 'C': np.array([0, 0, 1])}\n    \n    # Test suite of scaling factors for the one-hot encoded features.\n    alphas = [0.0, 0.5, 1.0, 1.5]\n\n    final_results = []\n\n    for alpha in alphas:\n        # Step 1: Build the encoded dataset.\n        # The feature vector is [real_value, alpha * one_hot_vector].\n        encoded_data = []\n        for r_val, cat in raw_data:\n            scaled_one_hot = alpha * categories[cat]\n            point = np.concatenate(([r_val], scaled_one_hot))\n            encoded_data.append(point)\n        X = np.array(encoded_data)\n\n        # Step 2: Initialize k=2 centroids using point 0 and point 5.\n        initial_centroids = np.array([X[0], X[5]])\n\n        # Step 3: Assign each point to the nearest centroid.\n        # Tie-breaking: assign to the centroid with the smaller index (0).\n        assignments = []\n        for point in X:\n            dist_sq_0 = np.sum((point - initial_centroids[0])**2)\n            dist_sq_1 = np.sum((point - initial_centroids[1])**2)\n            if dist_sq_0 = dist_sq_1:\n                assignments.append(0)\n            else:\n                assignments.append(1)\n        assignments = np.array(assignments)\n\n        # Step 4: Recompute the centroids as the mean of assigned points.\n        cluster_0_points = X[assignments == 0]\n        cluster_1_points = X[assignments == 1]\n        \n        new_centroids = np.zeros_like(initial_centroids)\n        # Ensure clusters are not empty before computing mean.\n        if len(cluster_0_points)  0:\n            new_centroids[0] = np.mean(cluster_0_points, axis=0)\n        if len(cluster_1_points)  0:\n            new_centroids[1] = np.mean(cluster_1_points, axis=0)\n\n        # Step 5: Compute the k-means objective J.\n        # J is the sum of squared Euclidean distances to the new centroids.\n        J = 0.0\n        if len(cluster_0_points)  0:\n            J += np.sum((cluster_0_points - new_centroids[0])**2)\n        if len(cluster_1_points)  0:\n            J += np.sum((cluster_1_points - new_centroids[1])**2)\n\n        # Step 6: Compute the Euclidean distance between encoded points 1 and 4.\n        d = np.linalg.norm(X[1] - X[4])\n\n        final_results.append([d, J])\n\n    # Format the final output string as specified.\n    # [[d_0,J_0],[d_1,J_1],...] with values rounded to six decimal places.\n    result_strings = []\n    for d, J in final_results:\n        result_strings.append(f\"[{d:.6f},{J:.6f}]\")\n    \n    output_string = f\"[{','.join(result_strings)}]\"\n    print(output_string)\n\nsolve()\n```", "id": "3134973"}, {"introduction": "标准的 K-means 算法平等地对待每一个数据点，但在某些应用场景中，不同数据点的重要性可能天差地别。本练习将引导你探索加权 K-means 这一重要扩展，你将从目标函数出发，推导出质心更新的加权规则。此外，你还将通过编程实践来验证一个核心思想：为数据点赋予整数权重等价于在数据集中复制这些点 [@problem_id:3134938]，从而深刻理解权重如何影响聚类中心的“引力”。", "problem": "设计并实现一个程序，在 $k$-均值聚类的背景下，形式化并测试点多重性的影响。从核心定义出发，推导如何通过整数倍复制数据点或通过非负权重来表示为数据点分配多重性。然后实现相应的算法，以验证这种等价性，并测量当权重变化时质心的漂移。\n\n基本基础：\n- 在一个有限数据集 $\\{x_i\\}_{i=1}^n \\subset \\mathbb{R}^d$ 上使用标准的 $k$-均值目标函数，其定义为最小化簇内平方距离之和，其中硬分配 $r_{ik} \\in \\{0,1\\}$ 且对于每个 $i$ 满足 $\\sum_{k=1}^K r_{ik} = 1$。\n- 使用平方欧几里得范数和多元微积分的标准原理，包括将梯度设置为零以找到最小值点。\n- 使用整数多重性对应于将一个数据点重复整数次的解释。\n\n任务：\n- 从带有非负权重 $\\{w_i\\}_{i=1}^n$ 的目标函数出发，推导在分配固定的情况下最小化该目标的质心更新规则。使用上述基本基础证明每一步，不得使用任何预先给出的最终公式或捷径。解释为什么当距离为平方欧几里得距离时，硬聚类分配不依赖于 $\\{w_i\\}$。\n- 仅使用定义和求和的线性性质，证明带有整数权重 $\\{w_i\\}$ 的加权目标函数，与在每个 $x_i$ 被复制 $w_i$ 次的数据集上的非加权目标函数完全等价。并由此得出结论：在这种情况下，质心更新规则是相同的。\n- 实现确定性过程，在给定数据集、整数权重和初始质心的情况下，执行单步的 $k$-均值分配，然后进行质心更新。分别在加权和复制-非加权两种形式下实现。使用平方欧几里得距离，且不要随机化任何方面。\n\n测试套件和要求输出：\n实现以下四个测试用例。对每一个用例，计算指定的结果。程序必须将这四个结果汇总到单行输出中，形式为一个用方括号括起来的逗号分隔列表，不含多余空格。\n\n- 测试用例1（$\\mathbb{R}^1$ 中的单簇等价性）：数据 $x_1 = 0$, $x_2 = 2$，整数权重为 $w_1 = 3$, $w_2 = 1$。分别通过加权更新计算一次质心，以及通过将 $x_1$ 复制三次、$x_2$ 复制一次后取非加权平均值计算一次质心。返回一个布尔值，表示两个质心之间的绝对差是否小于 $\\varepsilon = 10^{-12}$。\n\n- 测试用例2（$\\mathbb{R}^2$ 中的单簇等价性，含零权重边缘情况）：数据 $x_1 = (0,0)$, $x_2 = (2,2)$, $x_3 = (10,10)$，整数权重为 $w_1 = 1$, $w_2 = 1$, $w_3 = 0$。分别通过加权更新计算一次质心，以及根据权重进行复制（注意零权重点被复制零次）后计算一次质心。返回一个布尔值，表示两个质心之差的欧几里得范数是否小于 $\\varepsilon = 10^{-12}$。\n\n- 测试用例3（在 $\\mathbb{R}^2$ 中使用 $K = 2$ 的单次 $k$-均值迭代等价性）：数据点\n$x_1 = (0,0)$, $x_2 = (1,0)$, $x_3 = (0,1)$, $x_4 = (5,5)$, $x_5 = (6,5)$, $x_6 = (5,6)$\n整数权重为\n$w_1 = 1$, $w_2 = 2$, $w_3 = 1$, $w_4 = 1$, $w_5 = 1$, $w_6 = 3$。\n使用固定的初始质心 $\\mu_1^{(0)} = (0,0)$ 和 $\\mu_2^{(0)} = (5,5)$。执行一次分配步骤，通过最小化与当前质心的平方欧几里得距离（注意此步骤不依赖于 $\\{w_i\\}$），然后执行一次质心更新。分别使用原始数据的加权更新执行一次，和在每个 $x_i$ 被复制 $w_i$ 次的数据集上使用非加权更新执行一次。返回一个布尔值，表示对应质心之差的欧几里得范数是否均小于 $\\varepsilon = 10^{-12}$。\n\n- 测试用例4（当单个权重变化时的质心漂移，$K=1$）：考虑 $n=5$ 个点 $(0,0)$ 的副本，每个副本的单位权重为 $1$，以及一个位于 $(3,0)$ 的离群点，其权重 $w$ 在集合 $\\{0,1,2,5,10\\}$ 中变化。对于每个指定的 $w$，计算 $K=1$ 时的更新质心（这简化为所有点的加权平均值），然后计算该质心到原点的欧几里得距离。返回这五个距离的列表，每个距离四舍五入到 $6$ 位小数。\n\n最终输出格式：\n- 程序必须生成单行输出，包含一个由方括号括起来的逗号分隔列表，不含空格。列表元素必须按顺序为：测试用例1的布尔值，测试用例2的布尔值，测试用例3的布尔值，以及测试用例4的列表（其本身也由方括号括起来，条目用逗号分隔，不含空格）。例如，结构必须是 $[b_1,b_2,b_3,[d_1,d_2,d_3,d_4,d_5]]$，其中 $b_j$ 是布尔值，$d_j$ 是格式化为精确 $6$ 位小数的浮点数。\n\n实现约束：\n- 全程使用平方欧几里得距离。\n- 所有整数权重均为非负数。\n- 在进行相等性检查时，使用 $\\varepsilon = 10^{-12}$ 的严格容差。\n- 不允许使用随机性、外部输入、文件或网络访问。", "solution": "用户提供了一个问题，要求对加权 $k$-均值聚类进行形式化、推导和实现。该问题定义明确、科学上合理且内部一致。\n\n### **问题验证**\n\n**步骤1：提取的已知信息**\n- **非加权 $k$-均值目标：** 对于数据集 $\\{x_i\\}_{i=1}^n \\subset \\mathbb{R}^d$，最小化 $J = \\sum_{i=1}^n \\sum_{k=1}^K r_{ik} \\|x_i - \\mu_k\\|^2$。\n- **分配：** $r_{ik} \\in \\{0,1\\}$ 且 $\\sum_{k=1}^K r_{ik} = 1$。\n- **度量：** 平方欧几里得范数。\n- **优化原理：** 通过将目标函数的梯度设为零来寻找最小值点。\n- **加权 $k$-均值目标：** 使用非负权重 $\\{w_i\\}_{i=1}^n$ 对非加权目标的扩展。\n- **多重性解释：** 在非加权公式中，整数权重 $w_i$ 等同于将数据点 $x_i$ 复制 $w_i$ 次。\n- **任务1：** 推导加权目标的质心更新规则。解释为什么聚类分配规则与权重无关。\n- **任务2：** 证明对于整数权重，加权目标等同于在复制数据集上的非加权目标。\n- **任务3：** 为加权和复制-非加权两种方法实现单次 $k$-均值迭代的算法。\n- **测试用例：** 提供了四个具体的测试用例，包含数据、权重、初始条件和预期输出，以验证理论等价性并测量质心漂移。所有浮点数比较必须使用 $\\varepsilon = 10^{-12}$ 的容差。测试用例4要求四舍五入到6位小数。\n- **输出格式：** 一个单行字符串 `[b_1,b_2,b_3,[d_1,d_2,d_3,d_4,d_5]]`，表示四个测试用例的结果。\n\n**步骤2：使用提取的已知信息进行验证**\n根据以下标准，该问题被评估为有效：\n- **科学上合理：** 该问题植根于统计学习的基本原理，特别是 $k$-均值算法，并使用多元微积分的标准优化技术。加权 $k$-均值是该算法的一个标准扩展。\n- **适定性：** 所有任务都已明确定义，所有测试用例都提供了必要的数据和初始条件，以产生一个唯一的、可验证的解。\n- **客观性：** 问题以精确、无歧义的数学和算法术语陈述。\n\n**步骤3：结论与行动**\n问题有效。将提供完整的解决方案。\n\n---\n\n### **理论推导**\n\n本节提供问题陈述所要求的数学推导。\n\n**1. 加权质心更新规则的推导**\n\n加权 $k$-均值的目标是找到一组质心 $\\{\\mu_k\\}_{k=1}^K$ 和分配 $\\{r_{ik}\\}$，以最小化簇内平方距离之和，其中每个点的贡献由权重 $w_i$ 进行缩放。目标函数 $J$ 为：\n$$\nJ(\\{\\mu_k\\}, \\{r_{ik}\\}) = \\sum_{i=1}^n \\sum_{k=1}^K r_{ik} w_i \\|x_i - \\mu_k\\|^2\n$$\n$k$-均值算法通过迭代方式最小化此函数。在质心更新步骤（M步）中，分配 $\\{r_{ik}\\}$ 是固定的，我们寻求找到最小化 $J$ 的质心 $\\{\\mu_k\\}$。由于不同簇的项是独立的，我们可以分别为每个簇最小化目标。\n\n让我们专注于更新单个质心 $\\mu_k$。我们只需要考虑目标函数中依赖于 $\\mu_k$ 的部分，我们将其表示为 $J_k$：\n$$\nJ_k = \\sum_{i=1}^n r_{ik} w_i \\|x_i - \\mu_k\\|^2\n$$\n这里，$x_i \\in \\mathbb{R}^d$ 且 $\\mu_k \\in \\mathbb{R}^d$。平方欧几里得范数为 $\\|x_i - \\mu_k\\|^2 = (x_i - \\mu_k)^T(x_i - \\mu_k)$。为了找到最小值，我们计算 $J_k$ 关于 $\\mu_k$ 的梯度并将其设为零。\n$$\n\\nabla_{\\mu_k} J_k = \\nabla_{\\mu_k} \\sum_{i=1}^n r_{ik} w_i (x_i - \\mu_k)^T(x_i - \\mu_k)\n$$\n利用梯度算子的线性和恒等式 $\\nabla_v (u-v)^T(u-v) = -2(u-v)$，我们得到：\n$$\n\\nabla_{\\mu_k} J_k = \\sum_{i=1}^n r_{ik} w_i \\nabla_{\\mu_k} (\\|x_i - \\mu_k\\|^2) = \\sum_{i=1}^n r_{ik} w_i [-2(x_i - \\mu_k)]\n$$\n将梯度设为零以找到最优的 $\\mu_k$：\n$$\n-2 \\sum_{i=1}^n r_{ik} w_i (x_i - \\mu_k) = 0\n$$\n假设分配给簇 $k$ 的点不全都是零权重，我们可以除以 $-2$：\n$$\n\\sum_{i=1}^n r_{ik} w_i x_i - \\sum_{i=1}^n r_{ik} w_i \\mu_k = 0\n$$\n$$\n\\left(\\sum_{i=1}^n r_{ik} w_i\\right) \\mu_k = \\sum_{i=1}^n r_{ik} w_i x_i\n$$\n求解 $\\mu_k$，我们得到质心更新规则：\n$$\n\\mu_k = \\frac{\\sum_{i=1}^n r_{ik} w_i x_i}{\\sum_{i=1}^n r_{ik} w_i}\n$$\n这表明一个簇的最优质心是分配给该簇的数据点的加权平均值。如果分母为零（即没有带正权重的点被分配到该簇），则质心未定义，通常保持不变或重新初始化。\n\n**分配步骤中权重无关性的解释：**\n在分配步骤（E步）中，质心 $\\{\\mu_k\\}$ 是固定的。对于每个数据点 $x_i$，我们必须将其分配到能最小化其对总目标 $J$ 贡献的簇。点 $x_i$ 的贡献是 $\\sum_{k=1}^K r_{ik} w_i \\|x_i - \\mu_k\\|^2$。由于只有一个 $r_{ik}$ 为 $1$（其他为 $0$），该贡献变为 $w_i \\|x_i - \\mu_c\\|^2$，其中 $c$ 是所分配簇的索引。\n\n为了对给定的点 $x_i$ 最小化此项，我们必须选择能最小化 $w_i \\|x_i - \\mu_c\\|^2$ 的簇索引 $c$。\n- 如果 $w_i  0$，权重 $w_i$ 是一个正常数因子。最小化 $w_i \\|x_i - \\mu_c\\|^2$ 等价于最小化 $\\|x_i - \\mu_c\\|^2$（或其平方根，即欧几里得距离）。\n- 如果 $w_i = 0$，点 $x_i$ 对目标的贡献始终为零，无论其如何分配。因此，其分配是任意的，对目标函数或后续的质心更新没有影响。\n\n在任何一种情况下，将一个点分配给一个簇的决策规则仅取决于找到欧几里得距离上最近的质心，而与权重 $w_i$ 的具体值无关。\n\n**2. 整数权重等价性的证明**\n\n我们现在将证明，对于一组非负整数权重 $\\{w_i\\}_{i=1}^n$，加权 $k$-均值目标与在每个点 $x_i$ 被复制 $w_i$ 次的数据集上的标准（非加权）$k$-均值目标是相同的。\n\n设原始数据集为 $X = \\{x_i\\}_{i=1}^n$。设整数权重为 $W = \\{w_i\\}_{i=1}^n$，其中 $w_i \\in \\{0, 1, 2, \\dots\\}$。构造一个新的、复制的数据集 $X' = \\{x'_j\\}_{j=1}^{N'}$，其中 $N' = \\sum_{i=1}^n w_i$。这个新数据集包含每个点 $x_i$ 的 $w_i$ 个副本。我们可以定义一个映射 $\\phi: \\{1, \\dots, N'\\} \\to \\{1, \\dots, n\\}$，使得 $x'_j = x_{\\phi(j)}$。映射到给定 $i$ 的索引 $j$ 的集合（即原像 $\\phi^{-1}(i)$）的大小为 $|\\phi^{-1}(i)| = w_i$。\n\n复制数据集 $X'$ 的非加权 $k$-均值目标是：\n$$\nJ'_{unweighted} = \\sum_{j=1}^{N'} \\sum_{k=1}^K r'_{jk} \\|x'_j - \\mu_k\\|^2\n$$\n其中 $r'_{jk} \\in \\{0,1\\}$ 是 $X'$ 中各点的硬分配。\n\n对于复制数据集中的任何点 $x'_j$，其位置是 $x_{\\phi(j)}$。$x'_j$ 的聚类分配是通过找到欧几里得距离最近的质心 $\\mu_k$ 来确定的。由于给定点 $x_i$ 的所有副本都在同一位置，它们都将被分配到同一个簇。也就是说，对于任何满足 $\\phi(j_1) = \\phi(j_2) = i$ 的 $j_1, j_2$，我们都将有 $r'_{j_1,k} = r'_{j_2,k} = r_{ik}$，对所有 $k=1, \\dots, K$ 成立。\n\n我们现在可以通过根据原始点 $x_i$ 对 $j$ 的求和进行分组，来重写目标 $J'_{unweighted}$：\n$$\nJ'_{unweighted} = \\sum_{i=1}^n \\sum_{j \\in \\phi^{-1}(i)} \\left( \\sum_{k=1}^K r'_{jk} \\|x'_j - \\mu_k\\|^2 \\right)\n$$\n在关于 $j \\in \\phi^{-1}(i)$ 的内层求和中，我们知道 $x'_j = x_i$ 且 $r'_{jk} = r_{ik}$。将这些代入：\n$$\nJ'_{unweighted} = \\sum_{i=1}^n \\sum_{j \\in \\phi^{-1}(i)} \\left( \\sum_{k=1}^K r_{ik} \\|x_i - \\mu_k\\|^2 \\right)\n$$\n对于集合 $\\phi^{-1}(i)$ 中的所有 $j$，项 $\\sum_{k=1}^K r_{ik} \\|x_i - \\mu_k\\|^2$ 是一个常数。该集合的大小是 $w_i$。因此，对 $j \\in \\phi^{-1}(i)$ 的求和可以简化为乘以该集合的大小：\n$$\n\\sum_{j \\in \\phi^{-1}(i)} \\left( \\dots \\right) = w_i \\left( \\sum_{k=1}^K r_{ik} \\|x_i - \\mu_k\\|^2 \\right)\n$$\n将此代回 $J'_{unweighted}$ 的表达式中：\n$$\nJ'_{unweighted} = \\sum_{i=1}^n w_i \\left( \\sum_{k=1}^K r_{ik} \\|x_i - \\mu_k\\|^2 \\right) = \\sum_{i=1}^n \\sum_{k=1}^K r_{ik} w_i \\|x_i - \\mu_k\\|^2\n$$\n这正是加权目标函数 $J_{weighted}$。由于目标函数完全相同，它们关于 $\\{\\mu_k\\}$ 的最小化必须产生相同的解。因此，从复制数据集上的非加权目标推导出的质心更新规则，等价于原始数据集上的加权更新规则，这证实了质心更新是一致的结论。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Formalizes, implements, and tests weighted k-means clustering.\n    - Derives weighted centroid updates.\n    - Proves equivalence with duplicated-unweighted data for integer weights.\n    - Runs four test cases to validate theory and measure centroid drift.\n    \"\"\"\n    epsilon = 1e-12\n\n    # Helper function to format results for the final print statement.\n    def format_result(item):\n        if isinstance(item, list):\n            # Format the list of floats to 6 decimal places, no spaces.\n            return f\"[{','.join(f'{x:.6f}' for x in item)}]\"\n        else:\n            # For booleans, the default str() is used.\n            return str(item).lower()\n\n    # --- Test Case 1 ---\n    # Single-cluster equivalence in R^1\n    x1_data = np.array([0., 2.])\n    w1_data = np.array([3, 1])\n    \n    # Weighted approach\n    centroid_weighted_1 = np.sum(x1_data * w1_data) / np.sum(w1_data)\n    \n    # Duplicated-unweighted approach\n    x1_dup = np.repeat(x1_data, w1_data)\n    centroid_dup_1 = np.mean(x1_dup)\n    \n    result_1 = np.abs(centroid_weighted_1 - centroid_dup_1)  epsilon\n\n    # --- Test Case 2 ---\n    # Single-cluster equivalence in R^2 with a zero-weight edge case\n    x2_data = np.array([[0., 0.], [2., 2.], [10., 10.]])\n    w2_data = np.array([1, 1, 0])\n    \n    # Weighted approach\n    # Note: Handles sum of weights being zero if all weights are zero\n    sum_weights_2 = np.sum(w2_data)\n    if sum_weights_2  0:\n        centroid_weighted_2 = np.sum(x2_data * w2_data[:, np.newaxis], axis=0) / sum_weights_2\n    else: # Should not happen in this test case\n        centroid_weighted_2 = np.zeros(x2_data.shape[1])\n        \n    # Duplicated-unweighted approach\n    w2_data_int = w2_data.astype(int)\n    x2_dup = np.repeat(x2_data, w2_data_int, axis=0)\n    \n    if x2_dup.shape[0]  0:\n        centroid_dup_2 = np.mean(x2_dup, axis=0)\n    else:\n        centroid_dup_2 = np.zeros(x2_data.shape[1])\n    \n    result_2 = np.linalg.norm(centroid_weighted_2 - centroid_dup_2)  epsilon\n\n    # --- Test Case 3 ---\n    # Single k-means iteration equivalence in R^2 with K=2\n    x3_data = np.array([[0.,0.], [1.,0.], [0.,1.], [5.,5.], [6.,5.], [5.,6.]])\n    w3_data = np.array([1, 2, 1, 1, 1, 3])\n    mu3_initial = np.array([[0.,0.], [5.,5.]])\n\n    # Assignment step (common for both methods, independent of weights)\n    # Using numpy broadcasting to compute all squared distances at once\n    dist_sq = np.sum((x3_data[:, np.newaxis, :] - mu3_initial[np.newaxis, :, :])**2, axis=2)\n    assignments = np.argmin(dist_sq, axis=1)\n\n    # Weighted update\n    mu3_weighted_new = np.zeros_like(mu3_initial)\n    for k in range(2):\n        mask = (assignments == k)\n        cluster_weights = w3_data[mask]\n        sum_cluster_weights = np.sum(cluster_weights)\n        if sum_cluster_weights  0:\n            mu3_weighted_new[k] = np.sum(x3_data[mask] * cluster_weights[:, np.newaxis], axis=0) / sum_cluster_weights\n        else: # If cluster is empty\n            mu3_weighted_new[k] = mu3_initial[k]\n    \n    # Duplicated-unweighted update\n    w3_data_int = w3_data.astype(int)\n    x3_dup = np.repeat(x3_data, w3_data_int, axis=0)\n    assignments_dup = np.repeat(assignments, w3_data_int, axis=0)\n    \n    mu3_dup_new = np.zeros_like(mu3_initial)\n    for k in range(2):\n        mask = (assignments_dup == k)\n        if np.any(mask):\n            mu3_dup_new[k] = np.mean(x3_dup[mask], axis=0)\n        else:\n            mu3_dup_new[k] = mu3_initial[k]\n\n    diff1 = np.linalg.norm(mu3_weighted_new[0] - mu3_dup_new[0])\n    diff2 = np.linalg.norm(mu3_weighted_new[1] - mu3_dup_new[1])\n    result_3 = (diff1  epsilon) and (diff2  epsilon)\n\n    # --- Test Case 4 ---\n    # Centroid drift as a single weight varies\n    base_points = np.array([[0., 0.]] * 5)\n    base_weights = np.array([1] * 5)\n    outlier_point = np.array([[3., 0.]])\n    outlier_weights = [0, 1, 2, 5, 10]\n    \n    distances = []\n    for w in outlier_weights:\n        # Full dataset and weights\n        all_points = np.vstack((base_points, outlier_point))\n        all_weights = np.append(base_weights, w)\n        \n        # Compute centroid (weighted mean)\n        sum_weights_4 = np.sum(all_weights)\n        if sum_weights_4  0:\n            centroid = np.sum(all_points * all_weights[:, np.newaxis], axis=0) / sum_weights_4\n        else:\n            centroid = np.zeros(2)\n\n        # Compute Euclidean distance from the origin\n        distance = np.linalg.norm(centroid)\n        distances.append(distance)\n        \n    result_4 = distances\n    \n    # -- Aggregate and Print Results --\n    final_results = [result_1, result_2, result_3, result_4]\n    \n    # The print statement must produce a single line in the specified format.\n    print(f\"[{','.join(map(format_result, final_results))}]\")\n\nsolve()\n\n```", "id": "3134938"}]}