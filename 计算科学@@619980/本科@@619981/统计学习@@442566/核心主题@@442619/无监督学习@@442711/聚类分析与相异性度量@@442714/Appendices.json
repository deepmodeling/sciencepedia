{"hands_on_practices": [{"introduction": "我们从一个直观的问题开始：当数据簇不是漂亮的球形时会发生什么？本练习将引导你构建两个拉长的椭圆形高斯分布数据簇，并使用基于密度的聚类算法 DBSCAN 进行分析。你将亲眼见证，标准的欧氏距离 $d_E$ 会因为其各向同性的特性而错误地将两个独立的簇合并。随后，我们将切换到马氏距离 $d_M$，它能够根据数据的协方差结构进行自适应调整，从而成功地分离出正确的簇。这个实践旨在揭示一个核心原则：选择的距离度量必须与数据的内在几何形状相匹配。[@problem_id:3109620]", "problem": "给定一项任务，要求您实现一个完整的程序，该程序构建包含两个细长高斯簇的合成数据，使用基于噪声应用的基于密度的空间聚类（DBSCAN）算法进行聚类，并评估将相异性度量从欧几里得距离切换到适应于样本协方差的 Mahalanobis 距离的效果。该程序应完全自包含，并为一个小型测试套件生成一行聚合结果的输出。\n\n其基本基础包括以下经过充分检验的定义和事实：\n- 均值向量为 $\\mu \\in \\mathbb{R}^2$、协方差矩阵为 $\\Sigma \\in \\mathbb{R}^{2 \\times 2}$ 的二元正态分布由概率密度函数 $p(x) = \\frac{1}{2\\pi \\sqrt{\\det(\\Sigma)}} \\exp\\left( -\\frac{1}{2}(x - \\mu)^\\top \\Sigma^{-1} (x - \\mu) \\right)$ 指定，其中 $x \\in \\mathbb{R}^2$。\n- 两点 $x, y \\in \\mathbb{R}^2$ 之间的欧几里得距离是 $d_E(x,y) = \\|x - y\\|_2 = \\sqrt{(x - y)^\\top (x - y)}$。\n- 给定一个对称正定协方差矩阵 $\\Sigma$，两点 $x, y \\in \\mathbb{R}^2$ 之间的 Mahalanobis 距离是 $d_M(x,y) = \\sqrt{(x - y)^\\top \\Sigma^{-1} (x - y)}$。如果通过 Cholesky 分解得到 $\\Sigma = L L^\\top$（其中 $L$ 为下三角矩阵），则 $d_M(x,y) = \\|L^{-1}(x - y)\\|_2$，这意味着 Mahalanobis 距离等于经过线性白化变换 $z = L^{-1} x$ 后的欧几里得距离。\n- DBSCAN 由参数 $\\varepsilon > 0$（邻域半径）和 $m \\in \\mathbb{N}$（最小点数）定义。对于一个数据集 $\\{x_i\\}_{i=1}^n \\subset \\mathbb{R}^2$ 和一个相异性度量 $d(\\cdot,\\cdot)$，将 $x_i$ 的 $\\varepsilon$-邻域定义为 $N_\\varepsilon(x_i) = \\{x_j : d(x_i, x_j) \\le \\varepsilon\\}$。如果 $|N_\\varepsilon(x_i)| \\ge m$，则点 $x_i$ 是一个核心点。通过使用密度可达性从核心点迭代扩展来形成簇，将核心点和边界点（位于某个核心点邻域内的点）分配给同一个簇，而未被分配到任何簇的点则被标记为噪声。\n\n问题要求：\n1. 在 $\\mathbb{R}^2$ 中构建两个细长的高斯簇，它们共享一个通过特征分解设计的协方差矩阵。使用标准正交特征向量 $v_1 = \\frac{1}{\\sqrt{2}}(1,1)^\\top$ 和 $v_2 = \\frac{1}{\\sqrt{2}}(-1,1)^\\top$，以及特征值 $\\lambda_1$（较大，沿 $v_1$ 方向产生拉伸）和 $\\lambda_2$（较小，沿 $v_2$ 方向产生薄的分布）。令 $Q = [v_1 \\ v_2] \\in \\mathbb{R}^{2 \\times 2}$，并设置 $\\Sigma = Q \\operatorname{diag}(\\lambda_1, \\lambda_2) Q^\\top$。将两个簇的均值沿低方差方向 $v_2$ 对称地放置在 $\\mu_1 = -d \\, v_2$ 和 $\\mu_2 = +d \\, v_2$ 处，其中 $d > 0$ 控制半分离幅度。\n2. 使用给定的定义从头开始实现 DBSCAN，不依赖外部聚类库。同时使用 $d_E$ 和 $d_M$，在 Mahalanobis 的情况下，您必须使用无偏样本协方差估计器 $$\\hat{\\Sigma} = \\frac{1}{n - 1} \\sum_{i=1}^n (x_i - \\bar{x})(x_i - \\bar{x})^\\top,$$ 从整个数据集中估计协方差，其中 $\\bar{x} = \\frac{1}{n}\\sum_{i=1}^n x_i$。在进行 Cholesky 分解之前，通过向 $\\hat{\\Sigma}$ 添加一个小的脊（ridge）$\\lambda I$（其中 $\\lambda = 10^{-8}$）来确保数值稳定性。\n3. 对于每个测试用例，运行两次 DBSCAN：一次使用欧几里得距离（基线），一次使用通过白化变换适应于 $\\hat{\\Sigma}$ 的 Mahalanobis 距离。在每个测试用例的两次运行中使用相同的 $\\varepsilon$ 和 $m$。对于每次运行，报告发现的簇的整数数量（不包括标记为-1的噪声）。通过一个布尔值来评估分离度的改进，该布尔值指示在该测试用例中，Mahalanobis 运行是否比欧几里得运行发现了严格更多的簇。\n\n测试套件：\n- 测试用例 1（理想情况）：每个簇 $n_{\\text{per}} = 400$ 个点（总共 $n = 800$），$\\lambda_1 = 100$，$\\lambda_2 = 0.25$，$d = 3$，$\\varepsilon = 7.0$，$m = 10$，随机种子 $= 123$。\n- 测试用例 2（边界条件）：$n_{\\text{per}} = 400$，$\\lambda_1 = 100$，$\\lambda_2 = 0.25$，$d = 3$，$\\varepsilon = 5.5$，$m = 10$，随机种子 $= 321$。\n- 测试用例 3（分离度较小的边缘情况）：$n_{\\text{per}} = 300$，$\\lambda_1 = 100$，$\\lambda_2 = 0.25$，$d = 2$，$\\varepsilon = 5.0$，$m = 10$，随机种子 $= 42$。\n\n您的程序应生成一行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果，每个测试用例贡献一个形如 $[\\text{clusters\\_euclidean}, \\text{clusters\\_mahalanobis}, \\text{improved}]$ 的子列表。例如，输出格式为 $$[[c_{E,1}, c_{M,1}, b_1],[c_{E,2}, c_{M,2}, b_2],[c_{E,3}, c_{M,3}, b_3]],$$ 其中 $c_{E,i}$ 和 $c_{M,i}$ 是整数，$b_i$ 是布尔值。此问题不涉及任何物理单位、角度单位或百分比表示。", "solution": "用户提供的问题已经过分析，并被确定为**有效**。该问题在科学上是合理的、定义明确、客观，并包含获得唯一、可验证解所需的所有必要信息。这是统计学习中的一个标准练习，旨在展示像 Mahalanobis 距离这样的自适应距离度量在处理非球形数据分布的聚类算法中的效用。\n\n### 基于原理的解决方案设计\n\n解决方案主要分三个阶段进行：合成数据生成、DBSCAN 聚类算法（使用两种不同距离度量）的实现与应用，以及结果评估。\n\n#### 1. 合成数据生成\n\n该问题要求创建一个包含两个不同、细长的簇的数据集。这是通过从两个二元高斯分布 $\\mathcal{N}(\\mu_1, \\Sigma)$ 和 $\\mathcal{N}(\\mu_2, \\Sigma)$ 中采样来实现的。\n\n1.  **协方差矩阵 $\\Sigma$**：这些簇被设计成细长且旋转的形状。这通过共享协方差矩阵的特征分解 $\\Sigma = Q \\Lambda Q^\\top$ 来定义。\n    -   特征向量 $v_1 = \\frac{1}{\\sqrt{2}}(1,1)^\\top$ 和 $v_2 = \\frac{1}{\\sqrt{2}}(-1,1)^\\top$ 构成了正交旋转矩阵 $Q = [v_1 \\ v_2]$ 的列。这些向量定义了簇的主轴。\n    -   特征值 $\\lambda_1$（大）和 $\\lambda_2$（小）是沿这些主轴的方差。$\\lambda_1$ 的大值导致沿 $v_1$ 方向的拉伸，而小的 $\\lambda_2$ 确保簇沿 $v_2$ 方向很薄。特征值矩阵为 $\\Lambda = \\operatorname{diag}(\\lambda_1, \\lambda_2)$。\n\n2.  **簇均值 $\\mu_1, \\mu_2$**：两个簇沿其窄轴 $v_2$ 分开。均值向量对称地放置在原点两侧，位置为 $\\mu_1 = -d \\cdot v_2$ 和 $\\mu_2 = d \\cdot v_2$，其中 $d$ 是给定的分离参数。簇中心之间的总欧几里得距离为 $2d$。\n\n对于每个测试用例，我们从这两个分布中各生成 $n_{\\text{per}}$ 个数据点，从而得到一个总大小为 $n = 2 n_{\\text{per}}$ 的数据集。指定的随机种子确保了生成数据的可复现性。\n\n#### 2. DBSCAN 实现与应用\n\n我们从基本原理出发实现基于噪声应用的基于密度的空间聚类（DBSCAN）算法。DBSCAN 根据点的密度来识别簇。\n\n-   **核心概念**：该算法由两个参数控制：邻域半径 $\\varepsilon$ 和最小点数 $m$。如果一个点的 $\\varepsilon$-邻域内至少包含 $m$ 个点（包括其自身），则该点为**核心点**。**边界点**不是核心点，但位于某个核心点的邻域内。**噪声点**既不是核心点也不是边界点。簇是通过连接相互邻域内的核心点而形成的，所有边界点都被分配到其附近核心点所在的簇。\n\n-   **实现**：我们的实现会遍历所有点。如果一个未访问的点被发现是核心点，则启动一个新簇，并通过基于队列的扩展过程找到所有密度可达的点。为提高效率，在开始主循环之前，我们预先计算所有点对之间的距离矩阵。未被分配到任何簇的点被标记为噪声（-1）。\n\n我们对每个测试用例在两种场景下应用此 DBSCAN 实现：\n\n**a. 基线：欧几里得距离 ($d_E$)**\n第一次运行使用标准的欧几里得距离 $d_E(x,y) = \\|x-y\\|_2$。对于细长的簇，这种度量可能会产生误导。位于同一簇两端的两个点之间的距离可能大于位于不同但相邻簇中的两个点之间的距离。这常常导致 DBSCAN 错误地将不同的细长簇合并，特别是当 $\\varepsilon$ 的选择与簇间分离度相当或更大时。\n\n**b. 自适应：Mahalanobis 距离 ($d_M$)**\n第二次运行通过使用 Mahalanobis 距离来解决欧几里得度量的缺点，该距离考虑了数据的协方差。给定一个协方差矩阵 $S$，距离为 $d_M(x,y) = \\sqrt{(x-y)^\\top S^{-1}(x-y)}$。该度量有效地在一个“白化”的空间中测量距离，在这个空间中，数据具有单位协方差矩阵（即是球形的）。\n\n解决方案采用一种高效的方法来应用此度量，正如问题陈述所建议的：\n1.  **估计协方差**：使用无偏估计器从整个数据集中估计样本协方差矩阵 $\\hat{\\Sigma}$：$\\hat{\\Sigma} = \\frac{1}{n-1}\\sum_{i=1}^n (x_i - \\bar{x})(x_i - \\bar{x})^\\top$。\n2.  **正则化与分解**：为了数值稳定性，向 $\\hat{\\Sigma}$ 中加入一个小的脊（ridge）$\\lambda I$（其中 $\\lambda=10^{-8}$）。得到的矩阵使用 Cholesky 分解进行分解：$\\hat{\\Sigma}_{\\text{reg}} = L L^\\top$，其中 $L$ 是一个下三角矩阵。\n3.  **白化变换**：将整个数据集 $X$ 变换为一个新的数据集 $Z$，使得在 $X$ 上的 Mahalanobis 距离对应于在 $Z$ 上的欧几里得距离。变换为 $z_i^\\top = x_i^\\top (L^{-1})^\\top$。\n4.  **聚类**：然后使用标准的欧几里得距离在白化数据 $Z$ 上运行 DBSCAN。变换后点 $z_i$ 和 $z_j$ 之间的距离是 $\\|z_i - z_j\\|_2 = \\|L^{-1}(x_i - x_j)\\|_2$，这正是原始点 $x_i$ 和 $x_j$ 之间的 Mahalanobis 距离。\n\n经过此变换后，原始空间中的细长簇在白化空间中变为球形，只要 $\\varepsilon$ 选择得当，就很容易被像 DBSCAN 这样的基于密度的算法分离开。\n\n#### 3. 评估\n对于每个测试用例，找到的簇的数量是通过计算 DBSCAN 分配的唯一的正簇标签的数量来确定的（标记为 -1 的噪声被排除）。总输出为：\n-   $c_E$：使用欧几里得距离找到的簇的数量。\n-   $c_M$：使用 Mahalanobis 距离找到的簇的数量。\n-   `improved`：一个布尔标志，如果 $c_M > c_E$ 则为 `True`，否则为 `False`。\n\n该评估直接量化了在给定的聚类问题中使用数据自适应距离度量的优势。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import cholesky, inv\n\n# Global constants for DBSCAN labels\nUNVISITED = 0\nNOISE = -1\n\ndef _range_query(n_points, dist_matrix, p_idx, eps):\n    \"\"\"Finds all points within eps distance of point p_idx using a precomputed distance matrix.\"\"\"\n    return np.where(dist_matrix[p_idx] = eps)[0]\n\ndef _expand_cluster(n_points, dist_matrix, labels, p_idx, neighbor_indices, cluster_id, eps, m):\n    \"\"\"Expands a cluster from a core point.\"\"\"\n    labels[p_idx] = cluster_id\n    \n    # Use a list as a queue, processing with an index to avoid pop(0)\n    queue = list(neighbor_indices)\n    head_idx = 0\n    while head_idx  len(queue):\n        current_p_idx = queue[head_idx]\n        head_idx += 1\n\n        if labels[current_p_idx] == NOISE:\n            labels[current_p_idx] = cluster_id  # Change noise to border point\n        \n        if labels[current_p_idx] == UNVISITED:\n            labels[current_p_idx] = cluster_id\n            \n            # Find neighbors of the current point\n            current_neighbor_indices = _range_query(n_points, dist_matrix, current_p_idx, eps)\n            \n            if len(current_neighbor_indices) >= m:\n                # This is a core point, add its neighbors to the queue\n                for neighbor_idx in current_neighbor_indices:\n                    # Add only points that are unvisited or noise\n                    if labels[neighbor_idx] in [UNVISITED, NOISE]:\n                        queue.append(neighbor_idx)\n\ndef dbscan(X, eps, m):\n    \"\"\"\n    Performs DBSCAN clustering from first principles.\n\n    Args:\n        X (np.ndarray): Data points, shape (n_samples, n_features).\n        eps (float): The radius of a neighborhood.\n        m (int): The minimum number of points required to form a dense region.\n\n    Returns:\n        np.ndarray: Array of cluster labels for each point. Noise is labeled -1.\n    \"\"\"\n    n_points = X.shape[0]\n    \n    # Pre-compute pairwise Euclidean distance matrix for efficiency\n    dist_matrix = np.sqrt(np.sum((X[:, np.newaxis, :] - X[np.newaxis, :, :]) ** 2, axis=-1))\n    \n    labels = np.full(n_points, UNVISITED, dtype=int)\n    cluster_id = 0\n    \n    for i in range(n_points):\n        if labels[i] != UNVISITED:\n            continue\n            \n        neighbor_indices = _range_query(n_points, dist_matrix, i, eps)\n        \n        if len(neighbor_indices)  m:\n            labels[i] = NOISE\n        else:\n            cluster_id += 1\n            _expand_cluster(n_points, dist_matrix, labels, i, neighbor_indices, cluster_id, eps, m)\n            \n    return labels\n\ndef generate_data(n_per, lambda1, lambda2, d, seed):\n    \"\"\"Generates two elongated Gaussian clusters.\"\"\"\n    np.random.seed(seed)\n    \n    v1 = np.array([1, 1]) / np.sqrt(2)\n    v2 = np.array([-1, 1]) / np.sqrt(2)\n    \n    Q = np.column_stack([v1, v2])\n    Lambda = np.diag([lambda1, lambda2])\n    Sigma = Q @ Lambda @ Q.T\n    \n    mu1 = -d * v2\n    mu2 = d * v2\n    \n    cluster1 = np.random.multivariate_normal(mu1, Sigma, n_per)\n    cluster2 = np.random.multivariate_normal(mu2, Sigma, n_per)\n    \n    return np.vstack([cluster1, cluster2])\n\ndef run_test_case(n_per, lambda1, lambda2, d, eps, m, seed):\n    \"\"\"Runs a single test case for the problem.\"\"\"\n    X = generate_data(n_per, lambda1, lambda2, d, seed)\n    n = X.shape[0]\n\n    # 1. Euclidean DBSCAN\n    labels_euclidean = dbscan(X, eps, m)\n    # Number of clusters is the max label, excluding noise (-1)\n    clusters_euclidean = len(np.unique(labels_euclidean[labels_euclidean > 0]))\n\n    # 2. Mahalanobis DBSCAN via whitening transform\n    # Estimate sample covariance\n    mu_hat = np.mean(X, axis=0)\n    Sigma_hat = np.cov(X, rowvar=False)\n    \n    # Regularize for numerical stability\n    ridge = 1e-8\n    Sigma_hat_reg = Sigma_hat + ridge * np.eye(X.shape[1])\n    \n    # Cholesky decomposition and whitening\n    L = cholesky(Sigma_hat_reg, lower=True)\n    L_inv = inv(L)\n    \n    # Transform data into whitened space\n    # z_i^T = (L^{-1} x_i^T)^T = x_i (L^{-1})^T\n    Z = X @ L_inv.T\n    \n    # Run DBSCAN on whitened data (equivalent to Mahalanobis on original data)\n    labels_mahalanobis = dbscan(Z, eps, m)\n    clusters_mahalanobis = len(np.unique(labels_mahalanobis[labels_mahalanobis > 0]))\n    \n    # 3. Evaluate improvement\n    improved = clusters_mahalanobis > clusters_euclidean\n    \n    return [clusters_euclidean, clusters_mahalanobis, improved]\n\ndef solve():\n    \"\"\"Main function to run test suite and print results.\"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (n_per, lambda1, lambda2, d, eps, m, seed)\n        (400, 100, 0.25, 3, 7.0, 10, 123),\n        (400, 100, 0.25, 3, 5.5, 10, 321),\n        (300, 100, 0.25, 2, 5.0, 10, 42),\n    ]\n\n    results = []\n    for case in test_cases:\n        n_per, lambda1, lambda2, d, eps, m, seed = case\n        result = run_test_case(n_per, lambda1, lambda2, d, eps, m, seed)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    case_strings = [f\"[{c_e},{c_m},{str(b).lower()}]\" for c_e, c_m, b in results]\n    print(f\"[{','.join(case_strings)}]\")\n\nsolve()\n\n```", "id": "3109620"}, {"introduction": "在选择了合适的距离度量后，我们还需面对算法内部的参数选择。本练习聚焦于层次聚类，并探讨在存在异常值的情况下，不同的链接准则（单链接、全链接和平均链接）表现如何。通过在一个包含明确异常值的数据集上运行层次聚类，并追踪异常值被并入主数据簇的“合并高度”，我们可以量化地评估每种链接方法的稳健性。这个实践将为你提供关于为什么全链接方法因其隔离异常值的倾向而备受青睐的具体见解。[@problem_id:3109639]", "problem": "你需要编写一个完整、可运行的程序。对于给定的一组二维点云，该程序需在欧几里得范数（也称为 $L_2$ 范数）下计算层次凝聚聚类，并定量评估不同连接准则对离群点的敏感性。程序必须使用以下基本定义和事实作为其推理基础：\n\n- $\\mathbb{R}^2$ 上的欧几里得范数 $L_2$ 将点 $x = (x_1, x_2)$ 和 $y = (y_1, y_2)$ 之间的差异性定义为 $d(x,y) = \\sqrt{(x_1 - y_1)^2 + (x_2 - y_2)^2}$。\n- 层次凝聚聚类从 $n$ 个单例簇开始，根据选定的连接准则，反复合并簇间差异性最小的两个簇，直到只剩下一个簇为止。\n- 连接准则定义了点集 $A$ 和 $B$（均为非空）之间的簇间差异性：\n    - 单连接：$D_{\\text{single}}(A,B) = \\min_{x \\in A, y \\in B} d(x,y)$。\n    - 完全连接：$D_{\\text{complete}}(A,B) = \\max_{x \\in A, y \\in B} d(x,y)$。\n    - 平均连接：$D_{\\text{average}}(A,B) = \\frac{1}{|A||B|} \\sum_{x \\in A} \\sum_{y \\in B} d(x,y)$。\n- 层次聚类过程会产生一系列在不同高度 $h_1 \\le h_2 \\le \\dots \\le h_{n-1}$ 发生的合并，其中每个 $h_k$ 是第 $k$ 次合并发生时的差异性；这些高度定义了树状图。\n\n对于每个数据集，将给出标记离群点的明确索引。定义离群点集为 $O \\subset \\{0,1,\\dots,n-1\\}$，内点集为 $I = \\{0,1,\\dots,n-1\\} \\setminus O$。对于给定的连接准则，将离群点隔离高度 $H$ 定义为：某个包含至少一个离群点的簇与一个仅包含内点的簇首次发生合并时的树状图高度。也就是说，如果在某个步骤中，两个合并高度为 $h$ 的簇 $C_a$ 和 $C_b$ 满足 $\\left( C_a \\cap O \\ne \\emptyset \\right)$ 和 $\\left( C_b \\subseteq I \\right)$（反之亦然），并且这是满足该条件的最早步骤，则 $H = h$。\n\n为了提供一个比较尺度，将内点紧凑度尺度 $S$ 定义为所有不同内点之间成对欧几里得距离的中位数，即\n$$\nS = \\text{median}\\left( \\{ d(x_i, x_j) : i,j \\in I, i \\ne j \\} \\right).\n$$\n对于每个连接准则，计算隔离比率 $R = H / S$。该比率量化了相对于内点内部尺度，离群点被合并到内点群体中的相对延迟程度。\n\n你的任务是：\n- 实现基于 $L_2$ 范数的单连接、完全连接和平均连接的层次凝聚聚类。\n- 对于测试套件中的每个数据集，计算每种连接方式的隔离高度 $H$、尺度 $S$ 和隔离比率 $R$。\n- 通过为每个数据集生成一个布尔标志来证明“在 $L_2$ 范数下，完全连接能够隔离离群点”这一论断。当且仅当完全连接下的隔离比率严格大于单连接和平均连接下的隔离比率时，该标志为真。\n\n测试套件：\n使用以下数据集。每个数据集是一个按顺序给出的 $\\mathbb{R}^2$ 中的点列表，以及一个明确的离群点索引列表 $O$。\n\n- 测试用例 1（理想情况；一个极端离群点）：\n    - 点：$(0.0,0.0)$, $(0.2,-0.1)$, $(-0.1,0.3)$, $(0.3,0.1)$, $(-0.2,-0.2)$, $(0.1,-0.3)$, $(20.0,0.0)$\n    - 离群点索引：$[6]$\n- 测试用例 2（两个紧凑的内点群组加一个极端离群点）：\n    - 点：$(0.0,0.0)$, $(0.1,0.0)$, $(0.0,0.1)$, $(-0.1,0.0)$, $(1.0,1.0)$, $(1.1,1.0)$, $(1.0,1.1)$, $(0.9,1.0)$, $(10.0,10.0)$\n    - 离群点索引：$[8]$\n- 测试用例 3（中等距离的离群点）：\n    - 点：$(0.0,0.0)$, $(0.2,0.0)$, $(-0.1,0.2)$, $(0.3,-0.1)$, $(-0.2,-0.1)$, $(0.1,-0.2)$, $(3.0,3.0)$\n    - 离群点索引：$[6]$\n- 测试用例 4（两个邻近的离群点）：\n    - 点：$(0.0,0.0)$, $(0.2,-0.1)$, $(-0.1,0.3)$, $(0.3,0.1)$, $(-0.2,-0.2)$, $(0.1,-0.3)$, $(20.0,0.0)$, $(20.0,0.5)$\n    - 离群点索引：$[6,7]$\n\n最终输出格式要求：\n你的程序应生成单行输出，将所有测试用例的结果汇总到一个列表中。对于每个测试用例，输出一个子列表，按以下顺序包含四个元素：单连接的隔离高度 $H$、完全连接的隔离高度 $H$、平均连接的隔离高度 $H$ 以及前述的布尔标志。总输出必须是包含这些按测试用例组织的子列表的顶层列表，形式为单行，元素以逗号分隔，并用方括号括起来，例如\n$$\n[[H_{\\text{single}},H_{\\text{complete}},H_{\\text{average}},\\text{flag}],\\dots]\n$$\n所有数值结果必须是不带单位的实数，布尔值必须是 true 或 false。精确的打印表示形式中不应包含空格。", "solution": "问题陈述已经过严格验证，并被认为是有效的。它在科学上基于统计学习的原理，特别是聚类分析。关于层次凝聚聚类、欧几里得距离和连接准则（单连接、完全连接、平均连接）的定义都是标准且数学上精确的。其目标——计算每种连接方法的离群点隔离高度并进行比较——是定义明确的，并且所有必要的数据和条件都已提供。\n\n任务是实现层次凝聚聚类，以分析不同连接准则对离群点的敏感性。对于每个给定的数据集（包含 $\\mathbb{R}^2$ 中的点云和指定的离群点索引集 $O$），我们必须计算单连接、完全连接和平均连接的离群点隔离高度 $H$。最后，我们确定一个布尔标志，以指示完全连接是否在比其他两种方法严格更大的合并高度上隔离离群点。\n\n解决方案的核心是正确实现层次凝聚聚类算法，并对生成的树状图进行后续分析。对于每个测试用例，算法按以下步骤进行：\n\n1.  **数据表示**：输入点表示为一个 $n \\times 2$ 的矩阵，其中 $n$ 是点的数量。离群点索引集表示为 $O$，内点索引集表示为 $I = \\{0, 1, \\dots, n-1\\} \\setminus O$。\n\n2.  **距离计算**：任意两点 $x, y \\in \\mathbb{R}^2$ 之间的差异性是它们的欧几里得（$L_2$）距离 $d(x,y)$。我们预先计算所有 $n$ 个点之间的成对距离，生成一个压缩距离矩阵。这可以通过使用 `scipy.spatial.distance.pdist` 高效完成。\n\n3.  **层次聚类**：对于单连接、完全连接和平均连接这三种连接准则中的每一种，我们都执行层次凝聚聚类。`scipy.cluster.hierarchy.linkage` 函数非常适合此任务。它接收压缩距离矩阵和连接方法作为输入，并生成一个连接矩阵 $Z$。矩阵 $Z$ 的大小为 $(n-1) \\times 4$，编码了完整的合并层次结构。每一行 $Z_k$ 对应第 $k$ 次合并（按距离排序），并包含 `[index_1, index_2, distance, new_cluster_size]`。其中的 `distance`是两个合并簇之间的差异性，对应于树状图中的合并高度。\n\n4.  **离群点隔离高度（$H$）的确定**：核心任务是找到特定的合并高度 $H$，它被定义为包含至少一个离群点的簇与仅包含内点的簇首次合并时的高度。为了找到 $H$，我们必须检查连接矩阵 $Z$ 提供的合并序列。\n    - 我们初始化 $n$ 个簇，每个簇包含一个点。在整个合并过程中，必须跟踪每个簇的构成（即它所包含的原始点索引集）。我们可以为此使用一个集合列表，例如 `cluster_contents`，其中 `cluster_contents[j]` 存储簇 $j$ 的索引。对于 $j  n$，`cluster_contents[j]` 就是 $\\{j\\}$。\n    - 我们从 $k=0$ 到 $n-2$ 遍历连接矩阵 $Z$ 的行。每一行 $Z_k$ 代表在高度 $h_k$ 合并由索引 `idx1` 和 `idx2` 标识的两个簇。这次合并形成的新簇被分配索引 $n+k$。\n    - 对于每次合并，我们检索要合并的簇的内容，$C_1 = \\text{cluster\\_contents}[\\text{idx1}]$ 和 $C_2 = \\text{cluster\\_contents}[\\text{idx2}]$。\n    - 然后我们检查它们相对于离群点集 $O$ 的状态：如果 $C \\cap O \\neq \\emptyset$，则簇 $C$ 是一个“含离群点簇”；如果 $C \\cap O = \\emptyset$，则簇 $C$ 是一个“纯内点簇”。\n    - 隔离合并的条件是，合并的两个簇中，一个是含离群点簇，另一个是纯内点簇。即，$(C_1 \\cap O \\neq \\emptyset \\text{ and } C_2 \\cap O = \\emptyset)$ 或 $(C_1 \\cap O = \\emptyset \\text{ and } C_2 \\cap O \\neq \\emptyset)$。\n    - 由于 $Z$ 的行是按合并高度升序排列的，满足此条件的第一次合并的高度 $h_k$ 就是离群点隔离高度 $H$。\n    - 检查完条件后，我们通过创建新的合并簇来更新我们的跟踪结构：$\\text{cluster\\_contents}[n+k] = C_1 \\cup C_2$。\n\n5.  **比较分析**：对三种连接准则中的每一种都重复此过程，得到 $H_{\\text{single}}$、$H_{\\text{complete}}$ 和 $H_{\\text{average}}$。问题要求生成一个布尔标志，当且仅当完全连接的隔离比率 $R = H/S$ 严格大于其他两种时，该标志为真。由于对于任何给定的数据集，内点紧凑度尺度 $S$ 是一个正常数，因此比较 $R_{\\text{complete}}  R_{\\text{single}}$ 等价于 $H_{\\text{complete}}  H_{\\text{single}}$。因此，该标志是通过逻辑表达式 $(H_{\\text{complete}}  H_{\\text{single}}) \\land (H_{\\text{complete}}  H_{\\text{average}})$ 的结果计算得出的。\n\n6.  **最终输出**：将每个测试用例计算出的值 $[H_{\\text{single}}, H_{\\text{complete}}, H_{\\text{average}}, \\text{flag}]$ 收集起来，并按照问题规定格式化为单个字符串。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.spatial.distance import pdist\nfrom scipy.cluster.hierarchy import linkage\n\ndef solve():\n    \"\"\"\n    Solves the hierarchical clustering problem for a suite of test cases.\n    For each case, it computes outlier isolation heights for single, complete,\n    and average linkage, and determines a flag comparing them.\n    \"\"\"\n    test_cases = [\n        {\n            \"points\": np.array([\n                (0.0, 0.0), (0.2, -0.1), (-0.1, 0.3), (0.3, 0.1), \n                (-0.2, -0.2), (0.1, -0.3), (20.0, 0.0)\n            ]),\n            \"outliers\": [6]\n        },\n        {\n            \"points\": np.array([\n                (0.0, 0.0), (0.1, 0.0), (0.0, 0.1), (-0.1, 0.0), \n                (1.0, 1.0), (1.1, 1.0), (1.0, 1.1), (0.9, 1.0), \n                (10.0, 10.0)\n            ]),\n            \"outliers\": [8]\n        },\n        {\n            \"points\": np.array([\n                (0.0, 0.0), (0.2, 0.0), (-0.1, 0.2), (0.3, -0.1), \n                (-0.2, -0.1), (0.1, -0.2), (3.0, 3.0)\n            ]),\n            \"outliers\": [6]\n        },\n        {\n            \"points\": np.array([\n                (0.0, 0.0), (0.2, -0.1), (-0.1, 0.3), (0.3, 0.1), \n                (-0.2, -0.2), (0.1, -0.3), (20.0, 0.0), (20.0, 0.5)\n            ]),\n            \"outliers\": [6, 7]\n        }\n    ]\n\n    all_results = []\n    \n    for case in test_cases:\n        points = case[\"points\"]\n        outlier_indices = set(case[\"outliers\"])\n        \n        case_results = []\n        for linkage_method in ['single', 'complete', 'average']:\n            n = points.shape[0]\n            \n            # Perform hierarchical clustering using the specified linkage method\n            # The metric 'euclidean' is implicitly used by pdist\n            dist_matrix = pdist(points, 'euclidean')\n            Z = linkage(dist_matrix, method=linkage_method)\n            \n            # Track the original point indices within each cluster\n            cluster_contents = [{i} for i in range(n)]\n            \n            isolation_height = -1.0\n            \n            # Iterate through the merges in the order they occur\n            for i in range(n - 1):\n                # Z[i, 0] and Z[i, 1] are the indices of the clusters being merged\n                # Z[i, 2] is the merge height (distance)\n                idx1, idx2, height, _ = Z[i]\n                idx1, idx2 = int(idx1), int(idx2)\n                \n                # Get the set of original points for each of the two merging clusters\n                clust1_content = cluster_contents[idx1]\n                clust2_content = cluster_contents[idx2]\n                \n                # Check if a cluster contains any outliers\n                is_outlier1 = bool(clust1_content.intersection(outlier_indices))\n                is_outlier2 = bool(clust2_content.intersection(outlier_indices))\n                \n                # The isolation merge is the first where one cluster has outliers\n                # and the other does not.\n                if is_outlier1 != is_outlier2:\n                    isolation_height = height\n                    break # Found the first such merge, so this is H\n                \n                # If not an isolation merge, create the new cluster for subsequent steps.\n                # The new cluster's index is n + i.\n                new_cluster_content = clust1_content.union(clust2_content)\n                cluster_contents.append(new_cluster_content)\n            \n            case_results.append(isolation_height)\n\n        H_single, H_complete, H_average = case_results\n        \n        # The flag is true iff H_complete is strictly greater than the other two.\n        flag = (H_complete > H_single) and (H_complete > H_average)\n        \n        all_results.append([H_single, H_complete, H_average, flag])\n\n    # Format the final output string as per requirements:\n    # no spaces, lowercase booleans, nested lists.\n    result_str_parts = []\n    for res in all_results:\n        flag_str = 'true' if res[3] else 'false'\n        part = f\"[{res[0]},{res[1]},{res[2]},{flag_str}]\"\n        result_str_parts.append(part)\n    \n    final_output = f\"[{','.join(result_str_parts)}]\"\n    print(final_output)\n\nsolve()\n```", "id": "3109639"}, {"introduction": "现在，我们将视野扩展到一种常见但特殊的数据类型——时间序列，其中“形状”往往比“绝对值”更重要。本练习对比了对数值大小敏感的欧氏距离 $d_{\\text{E}}$ 和对形状敏感的相关性距离 $d_{\\text{C}}$。更重要的是，本练习引入了一种正式的聚类质量评估方法——轮廓系数（Silhouette Score），并用它来衡量在数据中添加一个不相关的噪声特征后，这两种度量的表现有多稳健。通过这个练习，你不仅会学到适用于特定数据类型的新度量，还将掌握如何量化地验证和比较你的聚类结果。[@problem_id:3109546]", "problem": "考虑使用两种相异性度量对时间序列数据进行聚类：欧几里得距离和相关距离。假设一个数据集由多个样本组成，每个样本表示为一个实值向量。两个向量 $x \\in \\mathbb{R}^d$ 和 $y \\in \\mathbb{R}^d$ 之间的欧几里得距离定义为 $$d_{\\text{E}}(x,y) = \\sqrt{\\sum_{k=1}^{d} (x_k - y_k)^2}.$$ 两个向量 $x$ 和 $y$ 之间的相关距离定义为 $$d_{\\text{C}}(x,y) = 1 - \\rho(x,y),$$ 其中 $\\rho(x,y)$ 是 $x$ 和 $y$ 坐标之间的皮尔逊相关系数 (PCC)，其公式为 $$\\rho(x,y) = \\frac{\\sum_{k=1}^{d} (x_k - \\bar{x})(y_k - \\bar{y})}{\\sqrt{\\sum_{k=1}^{d} (x_k - \\bar{x})^2} \\sqrt{\\sum_{k=1}^{d} (y_k - \\bar{y})^2}},$$ 其中 $\\bar{x}$ 和 $\\bar{y}$ 分别表示 $x$ 和 $y$ 坐标的算术平均值。\n\n聚类有效性将通过轮廓分数进行量化。对于一个包含 $n$ 个样本的数据集和给定的聚类标签，样本 $i$ 的轮廓值可以根据距离矩阵 $D \\in \\mathbb{R}^{n \\times n}$ 从第一性原理定义如下。设 $a(i)$ 为样本 $i$ 与其所在簇中所有其他样本之间的平均相异性。设 $b(i)$ 为样本 $i$ 与不包含 $i$ 的所有其他簇中样本的平均相异性的最小值。样本 $i$ 的轮廓值为 $$s(i) = \\begin{cases} \\dfrac{b(i) - a(i)}{\\max\\{a(i), b(i)\\}},  \\text{if } \\max\\{a(i), b(i)\\}  0, \\\\ 0,  \\text{otherwise.} \\end{cases}$$ 总体轮廓分数为 $\\dfrac{1}{n} \\sum_{i=1}^{n} s(i)$。\n\n研究对于时间序列聚类，在欧几里得距离和相关距离下，附加一个无关噪声特征如何影响轮廓分数。每个时间序列样本将通过在时间索引序列上评估一个基准信号并添加观测噪声来构建。无关噪声特征将是附加到每个样本向量的额外坐标，独立地从零均值高斯分布中抽取，并且与定义聚类的结构无关。三角函数的角度必须以弧度为单位进行解释。\n\n您的程序必须从第一性原理出发实现以下内容，并产生所需的输出：\n- 为下述三个测试案例生成合成数据集。对于每个案例，生成数据集的两个版本：一个不含无关噪声特征的基线版本，以及一个为每个样本向量附加了单个无关噪声特征的增强版本。\n- 对于每个数据集版本和每种相异性度量（$d_{\\text{E}}$ 和 $d_{\\text{C}}$），计算完整的成对距离矩阵，然后使用提供的聚类标签计算轮廓分数。\n- 对于每个测试案例，输出四个浮点数值：$d_{\\text{E}}$ 下的基线轮廓分数、$d_{\\text{E}}$ 下的增强轮廓分数、$d_{\\text{C}}$ 下的基线轮廓分数以及 $d_{\\text{C}}$ 下的增强轮廓分数。\n\n数据生成细节：\n- 时间索引集：对于时间序列长度 $T$，令 $t \\in \\{0, 1, \\dots, T-1\\}$ 并定义基准角序列 $a_t = 2\\pi t / T$。\n- 观测噪声是独立同分布的，服从均值为 $0$、标准差为 $\\sigma$ 的高斯噪声，加到每个时间坐标上。\n- 对于增强数据集，每个样本的无关噪声特征 $z$ 独立地从高斯分布 $\\mathcal{N}(0, \\tau^2)$ 中抽取，并作为额外坐标附加到样本向量上。\n- 聚类标签是按构造方式分配的，必须按给定方式使用；不运行任何聚类算法。\n\n测试套件：\n- 案例 $1$（基于形状的分离，“理想情况”）：两簇具有相位差的正弦波。参数：每簇样本数 $24$，时间序列长度 $T=60$，观测噪声标准差 $\\sigma=0.2$，第二簇的相位差 $\\phi=\\pi/2$，无关噪声特征标准差 $\\tau=5.0$，随机种子 $42$。簇 $0$：$\\sin(a_t)$，簇 $1$：$\\sin(a_t + \\phi)$。\n- 案例 $2$（基于尺度的分离，相关性不变结构）：两簇仅在幅度缩放上不同的正弦波。参数：每簇样本数 $24$，时间序列长度 $T=60$，观测噪声标准差 $\\sigma=0.2$，簇 $1$ 的幅度缩放因子为 $\\alpha=2.0$，无关噪声特征标准差 $\\tau=5.0$，随机种子 $43$。簇 $0$：$\\sin(a_t)$，簇 $1$：$\\alpha \\sin(a_t)$。\n- 案例 $3$（短序列的边界情况）：与案例 1 相同，但时间序列长度较短。参数：每簇样本数 $24$，时间序列长度 $T=8$，观测噪声标准差 $\\sigma=0.2$，第二簇的相位差 $\\phi=\\pi/2$，无关噪声特征标准差 $\\tau=10.0$，随机种子 $44$。簇 $0$：$\\sin(a_t)$，簇 $1$：$\\sin(a_t + \\phi)$。\n\n要求的最终输出格式：\n- 您的程序应生成单行输出，包含一个用方括号括起来的逗号分隔列表。该列表必须按以下顺序包含 $12$ 个浮点数：\n  - 案例 $1$：$[\\text{基线 } d_{\\text{E}}, \\text{增强 } d_{\\text{E}}, \\text{基线 } d_{\\text{C}}, \\text{增强 } d_{\\text{C}}]$，\n  - 案例 $2$：$[\\text{基线 } d_{\\text{E}}, \\text{增强 } d_{\\text{E}}, \\text{基线 } d_{\\text{C}}, \\text{增强 } d_{\\text{C}}]$，\n  - 案例 $3$：$[\\text{基线 } d_{\\text{E}}, \\text{增强 } d_{\\text{E}}, \\text{基线 } d_{\\text{C}}, \\text{增强 } d_{\\text{C}}]$。\n例如，单行输出必须类似于 $[\\text{r}_1,\\text{r}_2,\\dots,\\text{r}_{12}]$，其中 $\\text{r}_1$ 到 $\\text{r}_{12}$ 遵循上述顺序。不应打印任何附加文本。", "solution": "该问题已经过验证，被认为是自洽的、有科学依据且定义明确的。所有定义、参数和过程在统计学习和信号处理领域都是标准的。因此，我们可以开始进行解答。\n\n目标是研究两种相异性度量，即欧几里得距离 ($d_{\\text{E}}$) 和相关距离 ($d_{\\text{C}}$)，在时间序列聚类背景下对无关噪声特征的鲁棒性。分析将在三个合成数据集上进行，聚类质量将使用从第一性原理计算的轮廓分数来衡量。\n\n每个测试案例的总体流程如下：\n1.  根据指定的参数生成一个由时间序列样本组成的合成数据集，创建两个簇。这构成了“基线”数据集。\n2.  通过向基线数据集中的每个样本附加一个单一的、高方差的无关噪声特征来创建“增强”数据集。\n3.  对于基线和增强数据集，分别使用欧几里得距离 ($D_{\\text{E}}$) 和相关距离 ($D_{\\text{C}}$) 计算 $n \\times n$ 的成对距离矩阵。\n4.  使用这些距离矩阵和已知的聚类标签，为四种组合（基线/$d_{\\text{E}}$、增强/$d_{\\text{E}}$、基线/$d_{\\text{C}}$、增强/$d_{\\text{C}}$）中的每一种计算总体轮廓分数。\n\n**1. 数据生成**\n\n对于每个测试案例，我们生成 $n = 48$ 个样本，每个簇 $24$ 个样本。时间序列长度用 $T$ 表示。样本在基线情况下是 $\\mathbb{R}^T$ 中的向量，在增强情况下是 $\\mathbb{R}^{T+1}$ 中的向量。\n时间索引为 $t \\in \\{0, 1, \\dots, T-1\\}$，由此定义基准角序列为 $a_t = 2\\pi t / T$。\n样本向量是通过评估一个基准信号，添加观测噪声，以及在增强情况下附加一个无关噪声特征来生成的。\n-   **基线样本**：一个向量 $x \\in \\mathbb{R}^T$，其分量为 $x_t = \\text{signal}(a_t) + \\epsilon_t$，其中 $\\epsilon_t \\sim \\mathcal{N}(0, \\sigma^2)$ 是独立同分布的高斯噪声。\n-   **增强样本**：一个向量 $x_{\\text{aug}} \\in \\mathbb{R}^{T+1}$，通过将一个噪声值 $z \\sim \\mathcal{N}(0, \\tau^2)$ 附加到基线样本上创建：$x_{\\text{aug}} = [x_0, x_1, \\dots, x_{T-1}, z]$。\n\n**2. 相异性度量**\n\n-   **欧几里得距离 ($d_{\\text{E}}$)**：定义为 $d_{\\text{E}}(x,y) = \\sqrt{\\sum_{k=1}^{d} (x_k - y_k)^2}$，其中 $d$ 是向量维度（$d=T$ 或 $d=T+1$）。该度量对时间序列的形状和幅度的差异都很敏感。当添加一个无关特征 $z$ 时，距离的平方会包含项 $(z_x - z_y)^2$。如果该特征的方差 $\\tau^2$ 与原始信号中的变化相比很大，那么该项可能会在距离计算中占主导地位，从而可能掩盖潜在的聚类结构。\n\n-   **相关距离 ($d_{\\text{C}}$)**：定义为 $d_{\\text{C}}(x,y) = 1 - \\rho(x,y)$，其中 $\\rho(x,y)$ 是皮尔逊相关系数 (PCC)。PCC 的公式为\n    $$\\rho(x,y) = \\frac{\\sum_{k=1}^{d} (x_k - \\bar{x})(y_k - \\bar{y})}{\\sqrt{\\sum_{k=1}^{d} (x_k - \\bar{x})^2} \\sqrt{\\sum_{k=1}^{d} (y_k - \\bar{y})^2}}$$\n    相关性衡量两个向量分量之间的线性关系。它对每个向量各自的仿射变换（即，平移一个常数和乘以一个正常数）是不变的。这使其对时间序列的“形状”敏感，但对其基线水平或幅度不敏感。添加一个噪声特征会给 PCC 的分子和分母中的求和项增加一个新的分量，从而扰动相关性。这种扰动的幅度取决于原始向量的维度；一个噪声维度对于高维向量的影响比对低维向量的影响要小。\n\n**3. 轮廓分数计算**\n\n轮廓分数是根据给定的 $n \\times n$ 距离矩阵 $D$ 和一个包含 $n$ 个聚类标签的数组计算的。对于每个样本 $i$：\n-   设 $C(i)$ 为包含样本 $i$ 的簇。\n-   $a(i)$：样本 $i$ 与同一簇内所有其他样本的平均相异性。\n    $$a(i) = \\frac{1}{|C(i)| - 1} \\sum_{j \\in C(i), j \\neq i} D_{ij}$$\n-   $b(i)$：样本 $i$ 与任何其他单个簇中所有样本的最小平均相异性。\n    $$b(i) = \\min_{k \\neq C(i)} \\left\\{ \\frac{1}{|C_k|} \\sum_{j \\in C_k} D_{ij} \\right\\}$$\n-   样本 $i$ 的轮廓值是 $s(i) = (b(i) - a(i)) / \\max\\{a(i), b(i)\\}$，如果分母为 $0$，则 $s(i)=0$。\n-   总体轮廓分数是所有 $s(i)$ 值的平均值：$S = \\frac{1}{n} \\sum_{i=1}^{n} s(i)$。\n\n**4. 测试案例分析**\n\n-   **案例 1**：基于形状的分离 ($T=60$, $\\phi=\\pi/2$, $\\tau=5.0$)。簇 $0$ 的形状为 $\\sin(a_t)$，簇 $1$ 的形状为 $\\sin(a_t + \\pi/2) = \\cos(a_t)$。\n    -   *基线*：由于形状不同，预计 $d_{\\text{E}}$ 和 $d_{\\text{C}}$ 都能有效分离簇，从而得到较高的轮廓分数。\n    -   *增强*：噪声特征 $z$ 的标准差 $\\tau=5.0$，远大于信号幅度（$1.0$）和观测噪声（$\\sigma=0.2$）。$d_{\\text{E}}$ 中的项 $(z_x-z_y)^2$ 很可能会占主导地位，严重削弱欧几里得距离识别原始结构的能力。$d_{\\text{E}}$ 的轮廓分数应该会显著下降。对于 $d_{\\text{C}}$，无关特征只是 $T+1=61$ 个维度中的一个。其影响应该是可察觉的，但不会是灾难性的。因此，预计 $d_{\\text{C}}$ 会更具鲁棒性。\n\n-   **案例 2**：基于尺度的分离 ($T=60$, $\\alpha=2.0$, $\\tau=5.0$)。簇 $0$ 是 $\\sin(a_t)$，簇 $1$ 是 $2.0 \\sin(a_t)$。\n    -   *基线*：形状相同，但幅度不同。$d_{\\text{E}}$ 对幅度敏感，能有效分离这些簇。相比之下，$d_{\\text{C}}$ 是尺度不变的，因此 $\\rho(\\sin(a_t), 2\\sin(a_t)) \\approx 1$（忽略噪声），得出 $d_{\\text{C}} \\approx 0$。因此，$d_{\\text{C}}$ 无法区分这些簇，其轮廓分数应该接近于 $0$。\n    -   *增强*：无关特征会像案例 1 那样降低 $d_{\\text{E}}$ 的分数。对于 $d_{\\text{C}}$，分离效果已经很差；噪声特征不太可能改善它。\n\n-   **案例 3**：短序列的边界情况 ($T=8$, $\\phi=\\pi/2$, $\\tau=10.0$)。这是一个像案例 1 一样的基于形状的分离，但时间序列非常短。\n    -   *基线*：仅有 $8$ 个点，正弦和余弦的形状被粗略采样，观测噪声具有更大的相对影响。预计两种度量的基线轮廓分数都将低于案例 1。\n    -   *增强*：无关噪声特征具有非常高的标准差（$\\tau=10.0$），确保 $d_{\\text{E}}$ 会灾难性地失败。对于 $d_{\\text{C}}$，无关特征现在是 $T+1=9$ 个维度中的 $1$ 个。它对相关性计算的相对贡献远大于案例 1（其中是 $61$ 个维度中的 $1$ 个）。因此，我们预计 $d_{\\text{C}}$ 也会显著退化，其鲁棒性远不如案例 1。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.spatial.distance import cdist\n\ndef compute_silhouette_score_from_dist_matrix(D, labels):\n    \"\"\"\n    Computes the silhouette score from a precomputed distance matrix and labels.\n    Implemented from first principles as per the problem description.\n    \"\"\"\n    n_samples = len(labels)\n    if n_samples = 1:\n        return 0.0\n\n    unique_labels = np.unique(labels)\n    if len(unique_labels) = 1:\n        return 0.0\n\n    s_values = []\n    for i in range(n_samples):\n        my_label = labels[i]\n        \n        # Calculate a(i): average intra-cluster distance\n        is_in_my_cluster = (labels == my_label)\n        # Exclude sample i itself\n        is_in_my_cluster[i] = False\n        num_other_in_cluster = np.sum(is_in_my_cluster)\n\n        if num_other_in_cluster == 0:\n            a_i = 0.0\n        else:\n            a_i = np.sum(D[i, is_in_my_cluster]) / num_other_in_cluster\n\n        # Calculate b(i): minimum average inter-cluster distance\n        other_cluster_means = []\n        for other_label in unique_labels:\n            if other_label == my_label:\n                continue\n            is_in_other_cluster = (labels == other_label)\n            b_i_k = np.mean(D[i, is_in_other_cluster])\n            other_cluster_means.append(b_i_k)\n\n        if not other_cluster_means:\n             b_i = 0.0\n        else:\n             b_i = np.min(other_cluster_means)\n\n        # Calculate silhouette for sample i\n        denominator = max(a_i, b_i)\n        if denominator == 0:\n            s_i = 0.0\n        else:\n            s_i = (b_i - a_i) / denominator\n        s_values.append(s_i)\n\n    return np.mean(s_values)\n\ndef run_analysis(params):\n    \"\"\"\n    Generates data for a single test case and computes the four required silhouette scores.\n    \"\"\"\n    n_per_cluster = params['n_per_cluster']\n    T = params['T']\n    sigma = params['sigma']\n    phi = params['phi']\n    alpha = params['alpha']\n    tau = params['tau']\n    seed = params['seed']\n    \n    n_samples = 2 * n_per_cluster\n    rng = np.random.default_rng(seed)\n\n    # Generate data\n    t_indices = np.arange(T)\n    a_t = 2 * np.pi * t_indices / T\n    \n    baseline_data = np.zeros((n_samples, T))\n    labels = np.array([0] * n_per_cluster + [1] * n_per_cluster)\n\n    # Define base signals for the two clusters\n    signal_0 = np.sin(a_t)\n    if params['case_id'] == 2:\n        signal_1 = alpha * np.sin(a_t)\n    else: # Cases 1 and 3\n        signal_1 = np.sin(a_t + phi)\n\n    # Populate dataset with noisy samples\n    for i in range(n_samples):\n        obs_noise = rng.normal(0, sigma, size=T)\n        if labels[i] == 0:\n            baseline_data[i, :] = signal_0 + obs_noise\n        else:\n            baseline_data[i, :] = signal_1 + obs_noise\n            \n    # Create augmented dataset\n    z_noise = rng.normal(0, tau, size=(n_samples, 1))\n    augmented_data = np.hstack((baseline_data, z_noise))\n    \n    # --- Baseline Dataset Analysis ---\n    # Euclidean distance\n    dist_e_base = cdist(baseline_data, baseline_data, 'euclidean')\n    s_e_base = compute_silhouette_score_from_dist_matrix(dist_e_base, labels)\n    \n    # Correlation distance\n    dist_c_base = cdist(baseline_data, baseline_data, 'correlation')\n    s_c_base = compute_silhouette_score_from_dist_matrix(dist_c_base, labels)\n\n    # --- Augmented Dataset Analysis ---\n    # Euclidean distance\n    dist_e_aug = cdist(augmented_data, augmented_data, 'euclidean')\n    s_e_aug = compute_silhouette_score_from_dist_matrix(dist_e_aug, labels)\n\n    # Correlation distance\n    # np.corrcoef may produce NaNs if a vector has 0 variance. cdist handles this.\n    dist_c_aug = cdist(augmented_data, augmented_data, 'correlation')\n    s_c_aug = compute_silhouette_score_from_dist_matrix(dist_c_aug, labels)\n    \n    return [s_e_base, s_e_aug, s_c_base, s_c_aug]\n    \ndef solve():\n    \"\"\"\n    Main function to run all test cases and print results.\n    \"\"\"\n    test_cases = [\n        {'case_id': 1, 'n_per_cluster': 24, 'T': 60, 'sigma': 0.2, 'phi': np.pi/2, 'alpha': None, 'tau': 5.0, 'seed': 42},\n        {'case_id': 2, 'n_per_cluster': 24, 'T': 60, 'sigma': 0.2, 'phi': None, 'alpha': 2.0, 'tau': 5.0, 'seed': 43},\n        {'case_id': 3, 'n_per_cluster': 24, 'T': 8, 'sigma': 0.2, 'phi': np.pi/2, 'alpha': None, 'tau': 10.0, 'seed': 44}\n    ]\n\n    all_results = []\n    for params in test_cases:\n        results_for_case = run_analysis(params)\n        all_results.extend(results_for_case)\n\n    # Format the final list of 12 floats for printing\n    print(f\"[{','.join(map(str, all_results))}]\")\n\nsolve()\n```", "id": "3109546"}]}