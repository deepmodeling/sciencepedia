{"hands_on_practices": [{"introduction": "要真正掌握留一法交叉验证（LOOCV），我们从其数学核心开始。这个练习要求您将LOOCV框架应用于最简单的预测模型：一个总是预测样本均值的模型。通过推导LOOCV均方误差（MSE）的封闭形式表达式，您将揭示交叉验证误差与数据内在方差之间的基本关系，从而为后续学习奠定坚实的理论基础。[@problem_id:1912461]", "problem": "在统计学习领域，交叉验证是一种基本技术，用于评估统计分析的结果在独立数据集上的泛化能力。一个常见的变体是留一法交叉验证 (LOOCV)。\n\n考虑一个包含 $n$ 个观测值的数据集：$y_1, y_2, \\ldots, y_n$。我们希望评估一个非常简单的预测模型：对于任意给定的训练集，该模型对新数据点的预测值就是该训练集中观测值的算术平均值。\n\n此数据集的 LOOCV 过程包含 $n$ 次迭代。在第 $i$ 次迭代中（对于 $i=1, \\ldots, n$），第 $i$ 个观测值 $y_i$ 被作为验证集，剩下的 $n-1$ 个观测值被用作训练集。模型在这 $n-1$ 个观测值上进行训练，并对被留出的观测值 $y_i$ 作出预测。然后计算预测值与实际值 $y_i$ 之间的平方误差。\n\n你的任务是推导 LOOCV 均方误差 (MSE) 的通用闭式表达式，该均方误差是所有 $n$ 次迭代中平方误差的平均值。请用观测值数量 $n$ 和整个数据集的样本方差 $s^2$ 来表示你的最终答案。样本方差定义为 $s^2 = \\frac{1}{n-1} \\sum_{i=1}^{n} (y_i - \\bar{y})^2$，其中 $\\bar{y}$ 是所有 $n$ 个观测值的样本均值。", "solution": "我们考虑的预测模型输出的是训练集的算术平均值。设全样本均值为 $\\bar{y} = \\frac{1}{n}\\sum_{j=1}^{n} y_{j}$。在第 $i$ 折 LOOCV 中，训练集排除了 $y_i$，因此留一均值为\n$$\n\\bar{y}_{-i} = \\frac{1}{n-1}\\sum_{\\substack{j=1 \\\\ j\\neq i}}^{n} y_{j} = \\frac{n\\bar{y} - y_{i}}{n-1}.\n$$\n对留出的 $y_i$ 的预测误差为\n$$\ny_{i} - \\bar{y}_{-i} = y_{i} - \\frac{n\\bar{y} - y_{i}}{n-1} = \\frac{n(y_{i} - \\bar{y})}{n-1}.\n$$\n因此，第 $i$ 折的平方误差为\n$$\n\\left(y_{i} - \\bar{y}_{-i}\\right)^{2} = \\frac{n^{2}}{(n-1)^{2}}(y_{i} - \\bar{y})^{2}.\n$$\nLOOCV 均方误差 (MSE) 是这些平方误差在 $i=1,\\ldots,n$ 上的平均值：\n$$\n\\text{LOOCV MSE} = \\frac{1}{n}\\sum_{i=1}^{n} \\left(y_{i} - \\bar{y}_{-i}\\right)^{2} = \\frac{1}{n}\\cdot \\frac{n^{2}}{(n-1)^{2}} \\sum_{i=1}^{n} (y_{i} - \\bar{y})^{2}.\n$$\n根据样本方差的定义 $s^{2} = \\frac{1}{n-1}\\sum_{i=1}^{n} (y_{i} - \\bar{y})^{2}$，我们代入 $\\sum_{i=1}^{n} (y_{i} - \\bar{y})^{2} = (n-1)s^{2}$ 得到\n$$\n\\text{LOOCV MSE} = \\frac{1}{n}\\cdot \\frac{n^{2}}{(n-1)^{2}} \\cdot (n-1) s^{2} = \\frac{n}{n-1} s^{2}.\n$$\n因此，对于以均值为预测值的模型，其 LOOCV 均方误差为 $\\frac{n}{n-1}s^{2}$。", "answer": "$$\\boxed{\\frac{n}{n-1}s^{2}}$$", "id": "1912461"}, {"introduction": "在建立了理论基础之后，让我们通过一个分步计算的例子将其付诸实践。本问题使用一个小的、假设的材料科学数据集，来演示针对K最近邻（k-NN）分类器（此处$k=1$）的LOOCV流程。通过手动遍历每个数据点——将其留出，找到其最近的邻居，并检验预测结果——您将对LOOCV如何在分类场景中评估模型性能获得具体、过程化的理解。[@problem_id:90086]", "problem": "在计算材料科学领域，机器学习模型被越来越多地用于预测材料性质并加速新材料的发现。一个常见的任务是基于一组计算或实验特征对材料进行分类。\n\n考虑一个简化的分类问题，将二维材料分为“平庸绝缘体”（类别 0）或“拓扑绝缘体”（类别 1）。分类基于两个特征：$f_1$，每个原子的剥离能（单位：eV/atom），和 $f_2$，电子带隙（单位：eV）。我们将使用 1-最近邻（1-NN）分类器来完成此任务。\n\n1-NN 算法通过将新数据点分配为其在训练数据中单个最近邻的类别来对其进行分类。“邻近度”由距离度量确定，我们将为此使用标准的欧几里得距离。对于特征空间中的两个点 $\\mathbf{p}=(p_1, p_2)$ 和 $\\mathbf{q}=(q_1, q_2)$，欧几里得距离为 $d(\\mathbf{p}, \\mathbf{q}) = \\sqrt{(p_1-q_1)^2 + (p_2-q_2)^2}$。\n\n为了在一个小数据集上评估该分类器的性能，而不将其拆分为独立的训练集和测试集，我们采用留一法交叉验证（LOOCV）。在 LOOCV 中，对于一个包含 $N$ 个点的数据集，我们执行 $N$ 次迭代。在每次迭代 $i$ 中，第 $i$ 个数据点被作为测试样本留出，模型在余下的 $N-1$ 个数据点上进行训练。然后将被留出点的预测结果与其真实类别进行比较。总体准确率是正确分类的点的比例。\n\n给定以下包含五种二维材料的数据集：\n\n| 材料 ID | 特征 $f_1$ | 特征 $f_2$ | 类别标签 |\n| :--- | :---: | :---: | :---: |\n| A | 1.0 | 1.0 | 0 |\n| B | 2.0 | 2.0 | 0 |\n| C | 5.0 | 5.0 | 1 |\n| D | 6.0 | 4.0 | 1 |\n| E | 2.5 | 2.5 | 1 |\n\n如果到最近邻的距离出现相等的情况，则选择在数据集表格中出现得更早的邻居（即 A 在 B 之前，B 在 C 之前，依此类推）。\n\n计算该 1-NN 分类器在此数据集上的留一法交叉验证（LOOCV）准确率。", "solution": "目标是计算 1-最近邻（1-NN）分类器在给定的包含 $N=5$ 种材料的数据集上的留一法交叉验证（LOOCV）准确率。准确率定义为：\n$$\n\\text{Accuracy} = \\frac{\\text{Number of Correctly Classified Samples}}{\\text{Total Number of Samples}}\n$$\n\n我们将执行 $N=5$ 次迭代，数据集中每种材料一次。在每次迭代中，我们留出一种材料作为测试点，并将其余四种用作训练集。我们从训练集中找到该测试点的最近邻，并将该邻居的类别分配给测试点。\n\n为了找到最近邻，我们需要计算欧几里得距离。请注意，比较欧几里得距离的平方 $d^2 = (p_1-q_1)^2 + (p_2-q_2)^2$ 与比较距离本身是等效的，并且可以避免处理平方根。\n\n**迭代 1: 留出材料 A**\n-   测试点: A = (1.0, 1.0), 真实类别 = 0\n-   训练集: {B, C, D, E}\n-   A点到各点的距离平方:\n    -   $d^2(A, B) = (1.0 - 2.0)^2 + (1.0 - 2.0)^2 = (-1.0)^2 + (-1.0)^2 = 1.0 + 1.0 = 2.0$\n    -   $d^2(A, C) = (1.0 - 5.0)^2 + (1.0 - 5.0)^2 = (-4.0)^2 + (-4.0)^2 = 16.0 + 16.0 = 32.0$\n    -   $d^2(A, D) = (1.0 - 6.0)^2 + (1.0 - 4.0)^2 = (-5.0)^2 + (-3.0)^2 = 25.0 + 9.0 = 34.0$\n    -   $d^2(A, E) = (1.0 - 2.5)^2 + (1.0 - 2.5)^2 = (-1.5)^2 + (-1.5)^2 = 2.25 + 2.25 = 4.5$\n-   最小距离平方为 2.0，对应于材料 B。\n-   最近邻是 B。B 的类别是 0。\n-   A 的预测类别：0。\n-   结果：正确（真实：0，预测：0）。\n\n**迭代 2: 留出材料 B**\n-   测试点: B = (2.0, 2.0), 真实类别 = 0\n-   训练集: {A, C, D, E}\n-   B点到各点的距离平方:\n    -   $d^2(B, A) = (2.0 - 1.0)^2 + (2.0 - 1.0)^2 = (1.0)^2 + (1.0)^2 = 1.0 + 1.0 = 2.0$\n    -   $d^2(B, C) = (2.0 - 5.0)^2 + (2.0 - 5.0)^2 = (-3.0)^2 + (-3.0)^2 = 9.0 + 9.0 = 18.0$\n    -   $d^2(B, D) = (2.0 - 6.0)^2 + (2.0 - 4.0)^2 = (-4.0)^2 + (-2.0)^2 = 16.0 + 4.0 = 20.0$\n    -   $d^2(B, E) = (2.0 - 2.5)^2 + (2.0 - 2.5)^2 = (-0.5)^2 + (-0.5)^2 = 0.25 + 0.25 = 0.5$\n-   最小距离平方为 0.5，对应于材料 E。\n-   最近邻是 E。E 的类别是 1。\n-   B 的预测类别：1。\n-   结果：错误（真实：0，预测：1）。\n\n**迭代 3: 留出材料 C**\n-   测试点: C = (5.0, 5.0), 真实类别 = 1\n-   训练集: {A, B, D, E}\n-   C点到各点的距离平方:\n    -   $d^2(C, A) = (5.0 - 1.0)^2 + (5.0 - 1.0)^2 = (4.0)^2 + (4.0)^2 = 16.0 + 16.0 = 32.0$\n    -   $d^2(C, B) = (5.0 - 2.0)^2 + (5.0 - 2.0)^2 = (3.0)^2 + (3.0)^2 = 9.0 + 9.0 = 18.0$\n    -   $d^2(C, D) = (5.0 - 6.0)^2 + (5.0 - 4.0)^2 = (-1.0)^2 + (1.0)^2 = 1.0 + 1.0 = 2.0$\n    -   $d^2(C, E) = (5.0 - 2.5)^2 + (5.0 - 2.5)^2 = (2.5)^2 + (2.5)^2 = 6.25 + 6.25 = 12.5$\n-   最小距离平方为 2.0，对应于材料 D。\n-   最近邻是 D。D 的类别是 1。\n-   C 的预测类别：1。\n-   结果：正确（真实：1，预测：1）。\n\n**迭代 4: 留出材料 D**\n-   测试点: D = (6.0, 4.0), 真实类别 = 1\n-   训练集: {A, B, C, E}\n-   D点到各点的距离平方:\n    -   $d^2(D, A) = (6.0 - 1.0)^2 + (4.0 - 1.0)^2 = (5.0)^2 + (3.0)^2 = 25.0 + 9.0 = 34.0$\n    -   $d^2(D, B) = (6.0 - 2.0)^2 + (4.0 - 2.0)^2 = (4.0)^2 + (2.0)^2 = 16.0 + 4.0 = 20.0$\n    -   $d^2(D, C) = (6.0 - 5.0)^2 + (4.0 - 5.0)^2 = (1.0)^2 + (-1.0)^2 = 1.0 + 1.0 = 2.0$\n    -   $d^2(D, E) = (6.0 - 2.5)^2 + (4.0 - 2.5)^2 = (3.5)^2 + (1.5)^2 = 12.25 + 2.25 = 14.5$\n-   最小距离平方为 2.0，对应于材料 C。\n-   最近邻是 C。C 的类别是 1。\n-   D 的预测类别：1。\n-   结果：正确（真实：1，预测：1）。\n\n**迭代 5: 留出材料 E**\n-   测试点: E = (2.5, 2.5), 真实类别 = 1\n-   训练集: {A, B, C, D}\n-   E点到各点的距离平方:\n    -   $d^2(E, A) = (2.5 - 1.0)^2 + (2.5 - 1.0)^2 = (1.5)^2 + (1.5)^2 = 2.25 + 2.25 = 4.5$\n    -   $d^2(E, B) = (2.5 - 2.0)^2 + (2.5 - 2.0)^2 = (0.5)^2 + (0.5)^2 = 0.25 + 0.25 = 0.5$\n    -   $d^2(E, C) = (2.5 - 5.0)^2 + (2.5 - 5.0)^2 = (-2.5)^2 + (-2.5)^2 = 6.25 + 6.25 = 12.5$\n    -   $d^2(E, D) = (2.5 - 6.0)^2 + (2.5 - 4.0)^2 = (-3.5)^2 + (-1.5)^2 = 12.25 + 2.25 = 14.5$\n-   最小距离平方为 0.5，对应于材料 B。\n-   最近邻是 B。B 的类别是 0。\n-   E 的预测类别：0。\n-   结果：错误（真实：1，预测：0）。\n\n**最终准确率计算**\n我们总结结果如下：\n-   A: 正确分类\n-   B: 错误分类\n-   C: 正确分类\n-   D: 正确分类\n-   E: 错误分类\n\n正确分类的样本总数为 3。\n样本总数为 5。\n\nLOOCV 准确率为：\n$$\n\\text{Accuracy} = \\frac{3}{5}\n$$", "answer": "$$ \\boxed{\\frac{3}{5}} $$", "id": "90086"}, {"introduction": "为什么LOOCV对于构建稳健的模型如此关键？这个编码练习通过展示LOOCV在实际模型选择中的作用来回答这个问题。您将使用精心构建的数据集（包括一个含有离群点的数据集），比较LOOCV与具有误导性的训练误差在模型选择上的差异。这个实践将揭示LOOCV如何为模型的泛化能力提供更真实的评估，从而指导您选择更具鲁棒性的模型。[@problem_id:3139300]", "problem": "您将为$k$近邻（k-NN）分类器实现和分析留一法交叉验证（LOOCV），该分析将在旨在揭示单个极端离群点对模型选择影响的人工构造数据集上进行。其目的是从基本原理出发，证明为什么LOOCV可以选择一个更大的$k$值来对抗离群点，而训练重代入误差则倾向于选择$k=1$。请从以下基本定义开始。\n\n假设有一个包含$N$个带标签点的数据集$\\{(x_i, y_i)\\}_{i=1}^N$，其中$x_i \\in \\mathbb{R}^2$且$y_i \\in \\{0,1\\}$。$k$近邻分类器通过以下步骤为一个点预测标签：\n- 计算到所有候选邻居的欧几里得距离（在留一法交叉验证中排除查询点本身），\n- 选择距离最小的$k$个点（如果出现距离相等的情况，则通过较小的原始索引来确定性地打破平局），\n- 返回这$k$个邻居中获得多数票的类别（对于奇数$k$，这可以避免票数平局；如果罕见的平局仍然存在，则选择其邻居距离总和较小的类别来打破平局，如果仍然平局，则选择较小的类别标签）。\n\n定义预测器$\\hat{f}$在数据集上的经验$0\\text{-}1$损失为$$\\frac{1}{N}\\sum_{i=1}^N \\mathbb{I}\\{\\hat{f}(x_i) \\neq y_i\\},$$其中$\\mathbb{I}\\{\\cdot\\}$是指示函数。定义训练重代入误差为经验$0\\text{-}1$损失，其计算方式为：使用包含$x_i$的同一数据集对每个$x_i$进行分类，并允许$x_i$被选为其自身的邻居之一。定义留一法交叉验证（LOOCV）误差为经验$0\\text{-}1$损失，其计算方式为：使用从训练集中排除了$x_i$的模型对每个$x_i$进行分类。\n\n您的程序必须：\n1. 严格遵循上述规则，实现$k$近邻分类、$\\mathbb{R}^2$中的欧几里得距离、训练重代入误差和留一法交叉验证误差的计算。\n2. 对于下面测试套件中的每个数据集以及候选值$k \\in \\{1,3\\}$，计算：\n   - 最小化LOOCV误差的$k$值，如果多个$k$值产生相同的最小误差，则使用“选择最小的$k$”规则打破平局，\n   - 最小化训练重代入误差的$k$值，使用相同的打破平局规则。\n3. 生成单行输出，其中按顺序包含每个数据集的结果，格式为一个由方括号括起来的逗号分隔列表，其中每个数据集的结果本身是一个双元素列表$[k_{\\text{LOOCV}}, k_{\\text{train}}]$。例如，输出应如下所示：$$[[k_1^{\\text{LOOCV}},k_1^{\\text{train}}],[k_2^{\\text{LOOCV}},k_2^{\\text{train}}],[k_3^{\\text{LOOCV}},k_3^{\\text{train}}]].$$\n\n测试套件（所有坐标都在$\\mathbb{R}^2$中，无物理单位）：\n- 情况A（一个位于簇中心的极端离群点）：\n  - 类别 $0$ 的点：$(1,0)$, $(-1,0)$, $(0,1)$, $(0,-1)$。\n  - 类别 $1$ 的离群点：$(0,0)$。\n  - 候选 $k$ 值：$\\{1,3\\}$。\n- 情况B（无离群点；分离良好的簇）：\n  - 类别 $0$ 的点：$(1,0)$, $(-1,0)$, $(0,1)$, $(0,-1)$。\n  - 类别 $1$ 的点：$(10,0)$, $(-10,0)$, $(0,10)$, $(0,-10)$。\n  - 候选 $k$ 值：$\\{1,3\\}$。\n- 情况C（一个远离所有其他点但与其中一个簇共享标签的极端离群点）：\n  - 类别 $0$ 的点：$(1,0)$, $(-1,0)$, $(0,1)$, $(0,-1)$, $(100,100)$。\n  - 类别 $1$ 的点：$(3,0)$, $(3,1)$, $(4,0)$, $(4,1)$。\n  - 候选 $k$ 值：$\\{1,3\\}$。\n\n您的程序应生成单行输出，其中包含一个由方括号括起来的逗号分隔列表的结果，不含空格，格式完全如下$$[[k_A^{\\text{LOOCV}},k_A^{\\text{train}}],[k_B^{\\text{LOOCV}},k_B^{\\text{train}}],[k_C^{\\text{LOOCV}},k_C^{\\text{train}}]].$$输出中的数字必须是整数。", "solution": "该问题要求对$k$近邻（$k$-NN）分类器的训练重代入误差和留一法交叉验证（LOOCV）误差进行比较分析。我们将根据指定规则实现分类器和误差度量，并将其应用于三个不同的数据集，候选$k$值为$\\{1, 3\\}$。目标是确定每种误差度量所选择的最优$k$值，分别表示为$k_{\\text{train}}$和$k_{\\text{LOOCV}}$。\n\n基本定义如下。数据集是$N$个点的集合$\\{(x_i, y_i)\\}_{i=1}^N$，其中$x_i \\in \\mathbb{R}^2$且$y_i \\in \\{0,1\\}$。对于一个查询点$x$的预测$\\hat{f}(x)$，是通过对$k$个最近的训练点进行多数票决来做出的，其中距离为欧几里得距离。距离的平局通过点的较小原始索引来打破。训练重代入误差$E_{\\text{train}}$是在完整训练集上的$0\\text{-}1$损失，允许一个点成为其自身的邻居。LOOCV误差$E_{\\text{LOOCV}}$是通过对使用排除了$(x_i, y_i)$的数据集训练的模型来预测每个点$x_i$的误差进行平均计算得出的$0\\text{-}1$损失。每种度量对应的最优$k$值是使其误差最小化的那个$k$；平局则通过选择较小的$k$来打破。\n\n我们将系统地分析每种情况。\n\n**情况A：一个位于簇中心的极端离群点。**\n数据集包含$N=5$个点。\n类别0：$P_0=(1,0)$, $P_1=(-1,0)$, $P_2=(0,1)$, $P_3=(0,-1)$。\n类别1：$P_4=(0,0)$。\n\n**训练重代入误差分析（$E_{\\text{train}}$）：**\n对于$k=1$：当对训练集中的一个点$x_i$进行分类时，其最近的邻居总是它自己，距离为0。因此，预测标签$\\hat{f}(x_i)$就是它自己的标签$y_i$。这导致零个错误分类。\n$E_{\\text{train}}(k=1) = \\frac{0}{5} = 0$。\n对于$k=3$：我们为每个点找到3个最近的邻居，包括该点本身。\n- 对于任何类别0的点（例如，$P_0=(1,0)$），其邻居是它本身（$P_0$，类别0，距离0）、离群点$P_4$（$P_4$，类别1，距离1）以及另一个类别0的点（$P_2$或$P_3$，距离$\\sqrt{2}$）。这些邻居的标签是$\\{0, 1, 0\\}$，所以多数票决结果是类别0。预测是正确的。这对$P_0, P_1, P_2, P_3$都成立。\n- 对于离群点$P_4=(0,0)$（类别1），其邻居是它本身（$P_4$，类别1，距离0）和任意两个类别0的点，它们都等距，距离为1。根据索引打破平局的规则，我们选择$P_0$和$P_1$。邻居的标签是$\\{1, 0, 0\\}$。多数票决结果是类别0。预测为0，但真实标签为1。这是一个错误分类。\n对于$k=3$的总错误分类数为1。\n$E_{\\text{train}}(k=3) = \\frac{1}{5} = 0.2$。\n比较误差，$E_{\\text{train}}(k=1) = 0  E_{\\text{train}}(k=3) = 0.2$。因此，$k_{\\text{train}} = 1$。\n\n**留一法交叉验证误差分析（$E_{\\text{LOOCV}}$）：**\n对于$k=1$：我们使用其余的$N-1$个点对每个点$x_i$进行分类。\n- 对于任何类别0的点（例如，$P_0=(1,0)$），其在剩余集合$\\{P_1, P_2, P_3, P_4\\}$中的最近邻居是离群点$P_4=(0,0)$（类别1），距离为1。预测为类别1，而真实标签为0。这是一个错误分类。所有4个类别0的点都以这种方式被错误分类。\n- 对于离群点$P_4=(0,0)$（类别1），其在集合$\\{P_0, P_1, P_2, P_3\\}$中的最近邻居是它们中的任何一个（所有点距离都为1）。根据索引打破平局，我们选择$P_0=(1,0)$（类别0）。预测为类别0，而真实标签为1。这是一个错误分类。\n对于$k=1$的总错误分类数为5。\n$E_{\\text{LOOCV}}(k=1) = \\frac{5}{5} = 1.0$。\n对于$k=3$：\n- 对于任何类别0的点（例如，$P_0=(1,0)$），其在剩余集合$\\{P_1, P_2, P_3, P_4\\}$中的3个最近邻居是$P_4$（类别1，距离1）、$P_2$（类别0，距离$\\sqrt{2}$）和$P_3$（类别0，距离$\\sqrt{2}$）。标签是$\\{1, 0, 0\\}$。多数票决结果是类别0。预测是正确的。这对所有4个类别0的点都成立。\n- 对于离群点$P_4=(0,0)$（类别1），其在集合$\\{P_0, P_1, P_2, P_3\\}$中的3个最近邻居是这些点中的任意三个（所有点距离都为1）。根据索引打破平局，我们选择$P_0, P_1, P_2$，它们都属于类别0。标签是$\\{0, 0, 0\\}$。预测为类别0，而真实标签为1。这是一个错误分类。\n对于$k=3$的总错误分类数为1。\n$E_{\\text{LOOCV}}(k=3) = \\frac{1}{5} = 0.2$。\n比较误差，$E_{\\text{LOOCV}}(k=3) = 0.2  E_{\\text{LOOCV}}(k=1) = 1.0$。因此，$k_{\\text{LOOCV}} = 3$。\n情况A的结果是$[k_{\\text{LOOCV}}, k_{\\text{train}}] = [3, 1]$。\n\n**情况B：无离群点；分离良好的簇。**\n数据集包含$N=8$个点。\n类别0：$\\{(1,0), (-1,0), (0,1), (0,-1)\\}$。\n类别1：$\\{(10,0), (-10,0), (0,10), (0,-10)\\}$。\n遵循相同的方法：\n$E_{\\text{train}}(k=1) = 0$。对于$k=3$，外部簇（类别1）中的每个点都会被错误分类，因为除了它本身之外，它的两个最近邻居都来自内部簇（类别0），导致4个错误。$E_{\\text{train}}(k=3) = \\frac{4}{8} = 0.5$。所以$k_{\\text{train}} = 1$。\n$E_{\\text{LOOCV}}(k=1)$：每个类别1的点的最近邻居是一个类别0的点，导致4个错误。每个类别0的点的最近邻居是另一个类别0的点，导致0个错误。$E_{\\text{LOOCV}}(k=1) = \\frac{4}{8} = 0.5$。\n$E_{\\text{LOOCV}}(k=3)$：每个类别1的点的三个最近邻居都是类别0的点，导致4个错误。每个类别0的点的三个最近邻居是其他类别0的点，导致0个错误。$E_{\\text{LOOCV}}(k=3) = \\frac{4}{8} = 0.5$。\n误差持平：$E_{\\text{LOOCV}}(k=1) = E_{\\text{LOOCV}}(k=3) = 0.5$。根据打破平局的规则，我们选择最小的$k$。因此，$k_{\\text{LOOCV}} = 1$。\n情况B的结果是$[k_{\\text{LOOCV}}, k_{\\text{train}}] = [1, 1]$。\n\n**情况C：一个远离所有其他点的极端离群点。**\n数据集包含$N=9$个点。\n类别0：$\\{(1,0), (-1,0), (0,1), (0,-1), (100,100)\\}$。\n类别1：$\\{(3,0), (3,1), (4,0), (4,1)\\}$。\n遵循相同的方法：\n$E_{\\text{train}}(k=1) = 0$。对于$k=3$，只有离群点$(100,100)$被错误分类，因为除了它本身之外，它的两个最近邻居都来自类别1的簇。$E_{\\text{train}}(k=3) = \\frac{1}{9}$。所以$k_{\\text{train}} = 1$。\n$E_{\\text{LOOCV}}(k=1)$：当离群点$(100,100)$被留出时，它的最近邻居是一个类别1的点，导致一个错误。所有其他点都被正确分类。$E_{\\text{LOOCV}}(k=1) = \\frac{1}{9}$。\n$E_{\\text{LOOCV}}(k=3)$：当离群点$(100,100)$被留出时，它的三个最近邻居都是类别1的点，导致一个错误。所有其他点都被正确分类。$E_{\\text{LOOCV}}(k=3) = \\frac{1}{9}$。\n误差持平：$E_{\\text{LOOCV}}(k=1) = E_{\\text{LOOCV}}(k=3) = \\frac{1}{9}$。根据打破平局的规则，我们选择最小的$k$。因此，$k_{\\text{LOOCV}} = 1$。\n情况C的结果是$[k_{\\text{LOOCV}}, k_{\\text{train}}] = [1, 1]$。\n\n从每种情况收集最终结果。\n- 情况A：$[3, 1]$\n- 情况B：$[1, 1]$\n- 情况C：$[1, 1]$\n这些结果展示了关键的洞见：训练重代入误差将总是轻易地选择$k=1$（因为其误差始终为0），这是该误差度量的一个病态特征。LOOCV提供了对泛化误差更可靠的估计。在情况A中，它正确地识别出需要一个更大的$k=3$来抵抗中心离群点。在情况B和C中，数据的几何结构使得$k=1$更优或与$k=3$持平，根据打破平局的规则，最终选择了$k=1$。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and analyzes LOOCV vs. training error for k-NN on three datasets.\n    \"\"\"\n\n    test_cases = [\n        # Case A: one extreme outlier centered in the cluster\n        {\n            \"class_0_points\": np.array([[1, 0], [-1, 0], [0, 1], [0, -1]]),\n            \"class_1_points\": np.array([[0, 0]]),\n        },\n        # Case B: no outlier; well-separated clusters\n        {\n            \"class_0_points\": np.array([[1, 0], [-1, 0], [0, 1], [0, -1]]),\n            \"class_1_points\": np.array([[10, 0], [-10, 0], [0, 10], [0, -10]]),\n        },\n        # Case C: an extreme outlier far from all other points\n        {\n            \"class_0_points\": np.array([[1, 0], [-1, 0], [0, 1], [0, -1], [100, 100]]),\n            \"class_1_points\": np.array([[3, 0], [3, 1], [4, 0], [4, 1]]),\n        }\n    ]\n    \n    candidate_k_values = [1, 3]\n    final_results = []\n\n    for case_data in test_cases:\n        # Prepare dataset with original indices for tie-breaking\n        points = []\n        labels = []\n        \n        index = 0\n        for p in case_data[\"class_0_points\"]:\n            points.append(p)\n            labels.append(0)\n            index += 1\n        for p in case_data[\"class_1_points\"]:\n            points.append(p)\n            labels.append(1)\n            index += 1\n\n        points = np.array(points)\n        labels = np.array(labels)\n        full_dataset = list(zip(range(len(points)), points, labels))\n\n        # --- k-NN Implementation ---\n        def euclidean_distance(p1, p2):\n            return np.sqrt(np.sum((p1 - p2)**2))\n\n        def k_nn_predict(query_point, training_data, k):\n            if not training_data:\n                # Undefined, but for this problem, training set is never empty\n                return 0 \n            \n            distances = []\n            for idx, train_point, label in training_data:\n                dist = euclidean_distance(query_point, train_point)\n                distances.append((dist, idx, label))\n            \n            # Sort by distance, then by original index for tie-breaking\n            distances.sort(key=lambda x: (x[0], x[1]))\n            \n            neighbors = distances[:k]\n            \n            neighbor_labels = [label for _, _, label in neighbors]\n            \n            # Majority vote\n            count_0 = neighbor_labels.count(0)\n            count_1 = neighbor_labels.count(1)\n\n            if count_0 > count_1:\n                return 0\n            elif count_1 > count_0:\n                return 1\n            else: # Vote tie-break (unlikely for odd k here, but for completeness)\n                sum_dist_0 = sum(dist for dist, _, label in neighbors if label == 0)\n                sum_dist_1 = sum(dist for dist, _, label in neighbors if label == 1)\n                if sum_dist_0  sum_dist_1:\n                    return 0\n                elif sum_dist_1  sum_dist_0:\n                    return 1\n                else: # Tie in sum of distances, choose smaller class label\n                    return 0\n        \n        # --- Error Calculation ---\n        \n        # Training Resubstitution Error\n        train_errors = {}\n        for k in candidate_k_values:\n            misclassifications = 0\n            for i in range(len(points)):\n                query_point = points[i]\n                true_label = labels[i]\n                # In training error, the point itself is part of the training set\n                # For k-NN, if k>=1, the nearest neighbor to a point is itself.\n                # If k=1, error is always 0.\n                if k == 1:\n                    predicted_label = true_label\n                else:\n                    predicted_label = k_nn_predict(query_point, full_dataset, k)\n                    \n                if predicted_label != true_label:\n                    misclassifications += 1\n            train_errors[k] = misclassifications / len(points)\n        \n        # LOOCV Error\n        loocv_errors = {}\n        for k in candidate_k_values:\n            misclassifications = 0\n            for i in range(len(points)):\n                query_point = points[i]\n                true_label = labels[i]\n                \n                # Create LOOCV training set by excluding point i\n                loocv_train_set = full_dataset[:i] + full_dataset[i+1:]\n                \n                predicted_label = k_nn_predict(query_point, loocv_train_set, k)\n                \n                if predicted_label != true_label:\n                    misclassifications += 1\n            loocv_errors[k] = misclassifications / len(points)\n\n        # --- Model Selection (Find best k) ---\n        \n        # Tie-breaking rule: choose smallest k\n        min_loocv_error = float('inf')\n        best_k_loocv = -1\n        for k in sorted(loocv_errors.keys()):\n            if loocv_errors[k]  min_loocv_error:\n                min_loocv_error = loocv_errors[k]\n                best_k_loocv = k\n        \n        min_train_error = float('inf')\n        best_k_train = -1\n        # For k=1 training error is always 0, so it will always be selected\n        # if 1 is a candidate k.\n        for k in sorted(train_errors.keys()):\n            if train_errors[k]  min_train_error:\n                min_train_error = train_errors[k]\n                best_k_train = k\n                \n        final_results.append([best_k_loocv, best_k_train])\n\n    # Format the final output string\n    output_str = \"[\" + \",\".join([f\"[{k_l},{k_t}]\" for k_l, k_t in final_results]) + \"]\"\n    print(output_str)\n\nsolve()\n```", "id": "3139300"}]}