{"hands_on_practices": [{"introduction": "最基础的决策边界是线性边界，例如二维空间中的直线或高维空间中的超平面。本练习将抽象的判别函数方程 $g(\\mathbf{x}) = \\mathbf{w}^{\\top}\\mathbf{x} + b = 0$ 置于一个具体的优化问题中。通过实践，你将学习如何将几何约束（例如，边界必须穿过特定点）和分类要求（例如，满足一定的间隔）转化为代数方程和不等式，并在此基础上找到最优的分类器参数 [@problem_id:3116612]。", "problem": "在统计学习中，线性判别函数定义为 $g(\\mathbf{x}) = \\mathbf{w}^{\\top}\\mathbf{x} + b$，其中 $\\mathbf{x} \\in \\mathbb{R}^{2}$ 是特征向量，$\\mathbf{w} \\in \\mathbb{R}^{2}$ 是权重向量，$b \\in \\mathbb{R}$ 是偏置。决策边界是满足 $g(\\mathbf{x}) = 0$ 的点集。考虑一个二元线性分类器的约束优化问题，该分类器必须满足两类约束：\n- 锚点约束，要求决策边界通过两个指定点 $\\mathbf{p}_{1} = (0,0)$ 和 $\\mathbf{p}_{2} = (2,2)$，这施加了约束 $\\mathbf{w}^{\\top}\\mathbf{p}_{i} + b = 0$（对于 $i \\in \\{1,2\\}$）。\n- 对两个带标签的样本 $(\\mathbf{x}_{+}, y_{+})$ 和 $(\\mathbf{x}_{-}, y_{-})$ 的单位间隔分类约束，其中 $y_{+} = +1$，$y_{-} = -1$，$\\mathbf{x}_{+} = (4,3)$，$\\mathbf{x}_{-} = (2,5)$，这施加了约束 $y_{j}(\\mathbf{w}^{\\top}\\mathbf{x}_{j} + b) \\geq 1$（对于 $j \\in \\{+, -\\}$）。\n\n从上述定义出发，仅使用判别函数以及线性和等式约束中可行性的基本原理，判断该约束集是否可行（即，是否存在满足所有约束的 $\\mathbf{w}$ 和 $b$）。如果可行，考虑在给定约束条件下最小化权重向量的欧几里得范数的平方 $\\|\\mathbf{w}\\|^{2}$ 的优化问题。推导在这些约束下 $\\|\\mathbf{w}\\|^{2}$ 的最小值。将最终答案表示为一个实数。无需四舍五入。", "solution": "用户提供了一个在线性分类器背景下的约束优化问题。任务是首先确定约束集的可行性，如果可行，则求出权重向量的欧几里得范数平方 $\\|\\mathbf{w}\\|^{2}$ 的最小值。\n\n该问题由以下要素定义：\n- 一个线性判别函数 $g(\\mathbf{x}) = \\mathbf{w}^{\\top}\\mathbf{x} + b$，其中 $\\mathbf{x} \\in \\mathbb{R}^{2}$，$\\mathbf{w} \\in \\mathbb{R}^{2}$，$b \\in \\mathbb{R}$。\n- 一个由点集 $g(\\mathbf{x}) = 0$ 定义的决策边界。\n- 对参数 $\\mathbf{w} = \\begin{pmatrix} w_1 \\\\ w_2 \\end{pmatrix}$ 和 $b$ 的一组约束。\n\n首先，我们必须验证问题陈述。该问题是统计学习理论领域一个定义明确的数学任务。它有科学依据、客观，并提供了继续进行所需的所有必要信息。它要求确定可行性，这是分析约束系统的标准部分。因此，该问题是有效的，我们可以继续进行求解。\n\n求解过程包括两个主要部分：分析约束的可行性，然后解决优化问题。\n\n**第一部分：约束分析与可行性**\n\n约束分为两类：锚点约束（等式）和分类约束（不等式）。\n\n1.  **锚点约束：** 这些约束要求决策边界通过两个指定点 $\\mathbf{p}_{1} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$ 和 $\\mathbf{p}_{2} = \\begin{pmatrix} 2 \\\\ 2 \\end{pmatrix}$。这意味着这些点必须满足决策边界方程 $\\mathbf{w}^{\\top}\\mathbf{x} + b = 0$。\n\n    对于 $\\mathbf{p}_{1} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$：\n    $$ \\mathbf{w}^{\\top}\\mathbf{p}_{1} + b = w_1(0) + w_2(0) + b = 0 $$\n    这立即得出条件 $b = 0$。\n\n    对于 $\\mathbf{p}_{2} = \\begin{pmatrix} 2 \\\\ 2 \\end{pmatrix}$，并使用 $b=0$ 的结果：\n    $$ \\mathbf{w}^{\\top}\\mathbf{p}_{2} + 0 = w_1(2) + w_2(2) = 0 $$\n    $$ 2w_1 + 2w_2 = 0 \\implies w_1 + w_2 = 0 \\implies w_2 = -w_1 $$\n    因此，锚点约束将三个自由参数（$w_1$、$w_2$、$b$）减少到单个自由度。偏置 $b$ 必须为 $0$，权重向量必须是 $\\mathbf{w} = \\begin{pmatrix} w_1 \\\\ -w_1 \\end{pmatrix}$ 的形式，其中 $w_1 \\in \\mathbb{R}$ 是某个标量。决策边界由 $w_1 x_1 - w_1 x_2 = 0$ 定义。如果 $w_1 \\neq 0$，则可简化为 $x_1-x_2=0$，这正如预期的那样，是穿过两个锚点的直线。\n\n2.  **单位间隔分类约束：** 这些约束由不等式 $y_{j}(\\mathbf{w}^{\\top}\\mathbf{x}_{j} + b) \\geq 1$ 给出，适用于两个带标签的样本。我们将从锚点约束得到的结果（$b=0$ 和 $\\mathbf{w} = \\begin{pmatrix} w_1 \\\\ -w_1 \\end{pmatrix}$）代入这些不等式。\n\n    对于正样本 $(\\mathbf{x}_{+}, y_{+})$，其中 $\\mathbf{x}_{+} = \\begin{pmatrix} 4 \\\\ 3 \\end{pmatrix}$ 和 $y_{+} = +1$：\n    $$ (+1) \\left( \\mathbf{w}^{\\top}\\mathbf{x}_{+} + 0 \\right) \\geq 1 $$\n    $$ \\begin{pmatrix} w_1 & -w_1 \\end{pmatrix} \\begin{pmatrix} 4 \\\\ 3 \\end{pmatrix} \\geq 1 $$\n    $$ 4w_1 - 3w_1 \\geq 1 $$\n    $$ w_1 \\geq 1 $$\n\n    对于负样本 $(\\mathbf{x}_{-}, y_{-})$，其中 $\\mathbf{x}_{-} = \\begin{pmatrix} 2 \\\\ 5 \\end{pmatrix}$ 和 $y_{-} = -1$：\n    $$ (-1) \\left( \\mathbf{w}^{\\top}\\mathbf{x}_{-} + 0 \\right) \\geq 1 $$\n    $$ (-1) \\left( \\begin{pmatrix} w_1 & -w_1 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ 5 \\end{pmatrix} \\right) \\geq 1 $$\n    $$ (-1) (2w_1 - 5w_1) \\geq 1 $$\n    $$ (-1) (-3w_1) \\geq 1 $$\n    $$ 3w_1 \\geq 1 \\implies w_1 \\geq \\frac{1}{3} $$\n\n3.  **可行性结论：**\n    对唯一剩下的参数 $w_1$ 的完整约束集是：\n    - $w_1 \\geq 1$\n    - $w_1 \\geq \\frac{1}{3}$\n\n    这两个条件的交集是区间 $[1, \\infty)$。由于这个区间是非空的（例如，$w_1 = 2$ 是一个有效选择），所以约束集是可行的。存在满足所有给定条件的参数 $\\mathbf{w}$ 和 $b$。\n\n**第二部分：$\\|\\mathbf{w}\\|^{2}$ 的最小化**\n\n确定了可行性之后，我们现在继续解决优化问题：在推导出的约束条件下，最小化 $\\|\\mathbf{w}\\|^{2}$。\n\n目标函数是 $\\mathbf{w}$ 的欧几里得范数的平方：\n$$ \\|\\mathbf{w}\\|^{2} = w_1^2 + w_2^2 $$\n使用来自锚点约束的关系 $w_2 = -w_1$，我们可以仅用 $w_1$ 来表示目标函数：\n$$ \\|\\mathbf{w}\\|^{2} = w_1^2 + (-w_1)^2 = 2w_1^2 $$\n\n因此，优化问题简化为一个一维问题：\n最小化 $f(w_1) = 2w_1^2$\n约束条件为 $w_1 \\geq 1$。\n\n函数 $f(w_1) = 2w_1^2$ 是一个抛物线，其顶点（全局最小值）在 $w_1=0$ 处。在定义域 $w_1 > 0$ 上，该函数是严格递增的。$w_1$ 的可行域是区间 $[1, \\infty)$。由于函数在整个区间上是递增的，其在此定义域上的最小值将出现在最左侧的边界点，即 $w_1 = 1$。\n\n因此，目标函数的最小值通过在 $w_1=1$ 处计算 $f(w_1)$ 的值得到：\n$$ \\min(\\|\\mathbf{w}\\|^{2}) = f(1) = 2(1)^2 = 2 $$\n\n这个最小值在参数 $w_1=1$ 时达到，这意味着 $w_2 = -1$ 且 $b=0$。最优权重向量是 $\\mathbf{w}^{*} = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$。其范数平方的最小值是 $\\|\\mathbf{w}^{*}\\|^{2} = 1^2 + (-1)^2 = 2$。\n该解满足所有约束：\n- $w_1 = 1 \\geq 1$ (激活约束)。\n- $w_1 = 1 \\geq \\frac{1}{3}$。\n因此，最小值为 $2$。", "answer": "$$\\boxed{2}$$", "id": "3116612"}, {"introduction": "然而，现实世界的数据往往不是线性可分的，这要求我们使用更灵活的决策边界。本练习引入多项式判别函数，它能够生成复杂的曲线边界来分离诸如相互缠绕的螺旋线等非线性数据集。你的任务是找出能够成功分离这些数据所需的“最小多项式次数”，这让你亲身体验模型选择的核心思想：在模型的表达能力与复杂性之间找到平衡 [@problem_id:3116684]。", "problem": "考虑平面上的二元分类问题，其决策规则由一个判别函数定义。判别函数 $g(\\mathbf{x})$ 将 $\\mathbf{x}\\in\\mathbb{R}^2$ 映射到 $\\mathbb{R}$，并导出由 $g(\\mathbf{x})=0$ 给出的决策边界。满足 $g(\\mathbf{x})\\ge 0$ 的点被分配到类别 $+1$，而满足 $g(\\mathbf{x})<0$ 的点被分配到类别 $-1$。你将研究有界阶数的多项式判别函数，并确定恢复分隔两条相互缠绕的螺旋线的光滑边界所需的最小阶数。\n\n推导的基本依据：\n- 判别函数 $g(\\mathbf{x})$ 通过其零水平集 $g(\\mathbf{x})=0$ 定义决策边界。\n- 使用平方损失的经验风险最小化通过最小化 $\\sum_{i=1}^{n} \\left(g(\\mathbf{x}_i)-y_i\\right)^2$ 来拟合判别函数，其中 $y_i\\in\\{-1,+1\\}$ 是标签。\n- 多项式基函数产生一个有限维线性模型。设 $\\alpha=(\\alpha_1,\\alpha_2)$ 是一个非负整数的多重索引，且 $|\\alpha|=\\alpha_1+\\alpha_2$。定义单项式 $\\mathbf{x}^\\alpha=x_1^{\\alpha_1}x_2^{\\alpha_2}$。一个最高阶数不超过 $m$ 的多项式判别函数为 $g_m(\\mathbf{x})=\\sum_{|\\alpha|\\le m} c_\\alpha \\mathbf{x}^\\alpha$，其中 $c_\\alpha\\in\\mathbb{R}$ 是系数。\n- 使用一个小的岭参数的正则化最小二乘法可以稳定拟合：对于一个小的 $\\lambda>0$，最小化 $\\sum_{i=1}^{n} \\left(g_m(\\mathbf{x}_i)-y_i\\right)^2 + \\lambda \\sum_{|\\alpha|\\le m} c_\\alpha^2$。\n\n数据生成：\n- 两条相互缠绕的阿基米德螺旋线在极坐标中通过 $r=a\\,\\theta$ 参数化，其中角度 $\\theta$ 以弧度为单位。对于类别 $+1$，从 $[0, 2\\pi T]$ 上的均匀分布中抽样 $\\theta$，并设置 $(x,y)=(r\\cos\\theta,r\\sin\\theta)$，其中 $r=a\\,\\theta$。对于类别 $-1$，使用偏移了 $\\pi$ 的角度，即 $\\theta'=\\theta+\\pi$，以及 $(x',y')=(r'\\cos\\theta',r'\\sin\\theta')$，其中 $r'=a\\,\\theta$。向两个坐标添加标准差为 $\\sigma$ 的独立高斯噪声。通过将 $x$ 和 $y$ 坐标都除以最大半径 $r_{\\max}=a\\cdot 2\\pi T$ 来归一化特征，以使输入位于可比较的范围内。角度必须以弧度为单位。\n\n研究设计：\n- 对每个数据集，使用固定的训练集比例 $f_{\\text{train}}=0.7$ 和验证集比例 $f_{\\text{val}}=0.3$ 将样本分割为训练集和验证集。对每个数据集使用固定的随机种子以确保可复现性。\n- 对每个候选多项式阶数 $m\\in\\{1,2,\\dots,M_{\\max}\\}$，使用训练集和岭参数 $\\lambda=10^{-6}$ 通过正则化最小二乘法拟合 $g_m(\\mathbf{x})$，然后计算验证准确率，即预测标签 $\\operatorname{sign}(g_m(\\mathbf{x}))$ 与真实标签相匹配的验证点所占的比例。当某个验证点的 $g_m(\\mathbf{x})=0$ 时，将其预测视为 $+1$。\n- 将数据集的最小阶数定义为候选集中使得验证准确率至少达到指定阈值 $\\tau$ 的最小 $m$。如果没有任何阶数达到该阈值，则返回 $-1$。\n\n测试套件：\n- 使用以下参数集，其中每个元组为 $(N,T,a,\\sigma,\\tau,M_{\\max},\\text{seed})$：\n    - 案例 $1$ (理想路径): $(600,3,0.5,0.05,0.85,10,0)$。\n    - 案例 $2$ (低噪声，较少圈数): $(600,2,0.7,0.0,0.90,10,1)$。\n    - 案例 $3$ (更多圈数，中等噪声): $(600,5,0.3,0.12,0.80,12,2)$。\n    - 案例 $4$ (边缘案例，高噪声，较少样本): $(300,4,0.6,0.25,0.75,12,3)$。\n\n要求输出：\n- 你的程序应生成单行输出，其中包含四个测试案例的最小阶数，按顺序排列，格式为方括号内以逗号分隔的列表（例如，$[m_1,m_2,m_3,m_4]$）。每个 $m_i$ 必须是整数，其中 $-1$ 表示在该案例中，候选集中的任何多项式阶数均未达到阈值。", "solution": "用户希望确定判别函数所需的最小多项式阶数，以便准确分类属于两条相互缠绕的阿基米德螺旋线的数据点。这是一个统计学习背景下的模型选择问题。解决方案涉及针对一组给定参数进行系统的计算实验。\n\n### 1. 问题形式化与数据生成\n\n该任务是 $\\mathbb{R}^2$ 中的一个二元分类问题。两个类别被标记为 $y=+1$ 和 $y=-1$。数据点基于两条相互缠绕的阿基米德螺旋线生成。\n\n阿基米德螺旋线在极坐标 $(r, \\phi)$ 中由方程 $r = a\\phi$ 描述。对于给定的参数集 $(N, T, a, \\sigma)$，我们为每个类别生成 $N/2$ 个点。\n\n- **类别 $+1$**：我们从区间 $[0, 2\\pi T]$ 上的均匀分布中抽样 $N/2$ 个角度 $\\theta_i$。每个点在笛卡尔坐标系中的表示为：\n$$\n\\mathbf{x}_{i,+} = \\begin{pmatrix} r_i\\cos\\theta_i \\\\ r_i\\sin\\theta_i \\end{pmatrix} + \\boldsymbol{\\epsilon}_i, \\quad \\text{其中 } r_i = a\\theta_i\n$$\n项 $\\boldsymbol{\\epsilon}_i$ 表示添加到每个坐标的独立高斯噪声，它从均值为 $0$、标准差为 $\\sigma$ 的正态分布中抽取，即 $\\boldsymbol{\\epsilon}_i \\sim \\mathcal{N}(\\mathbf{0}, \\sigma^2 I)$，其中 $I$ 是 $2 \\times 2$ 的单位矩阵。\n\n- **类别 $-1$**：对于用于类别 $+1$ 的每个角度 $\\theta_i$，类别 $-1$ 的相应点是使用一个偏移后的角度 $\\theta'_i = \\theta_i + \\pi$ 和半径 $r'_i = a\\theta_i$ 生成的。请注意，半径是原始角度 $\\theta_i$ 的函数。其笛卡尔坐标为：\n$$\n\\mathbf{x}_{i,-} = \\begin{pmatrix} r'_i\\cos\\theta'_i \\\\ r'_i\\sin\\theta'_i \\end{pmatrix} + \\boldsymbol{\\epsilon}'_i = \\begin{pmatrix} a\\theta_i\\cos(\\theta_i + \\pi) \\\\ a\\theta_i\\sin(\\theta_i + \\pi) \\end{pmatrix} + \\boldsymbol{\\epsilon}'_i = -\\begin{pmatrix} a\\theta_i\\cos\\theta_i \\\\ a\\theta_i\\sin\\theta_i \\end{pmatrix} + \\boldsymbol{\\epsilon}'_i\n$$\n这种构造意味着，在添加噪声之前，类别 $-1$ 的点与类别 $+1$ 的点关于原点是完全点对称的。\n\n- **归一化**：生成所有 $N$ 个点后，将其坐标除以最大可能半径 $r_{\\max} = a \\cdot 2\\pi T$ 进行归一化，以确保特征值落在一致的范围内。\n\n### 2. 多项式判别函数\n\n分类是使用一个最高阶数不超过 $m$ 的多项式判别函数 $g_m(\\mathbf{x})$ 来执行的。对于一个点 $\\mathbf{x} = (x_1, x_2)^T \\in \\mathbb{R}^2$，该判别函数是单项式的线性组合：\n$$\ng_m(\\mathbf{x}) = \\sum_{|\\alpha| \\le m} c_\\alpha \\mathbf{x}^\\alpha = \\sum_{\\alpha_1+\\alpha_2 \\le m} c_{(\\alpha_1,\\alpha_2)} x_1^{\\alpha_1} x_2^{\\alpha_2}\n$$\n其中 $c_\\alpha$ 是实值系数。这可以表示为线性模型的向量形式，$g_m(\\mathbf{x}) = \\boldsymbol{\\phi}_m(\\mathbf{x})^T \\mathbf{c}$，其中 $\\boldsymbol{\\phi}_m(\\mathbf{x})$ 是单项式基函数的向量（例如，当 $m=2$ 时，$\\boldsymbol{\\phi}_2(\\mathbf{x}) = (1, x_1, x_2, x_1^2, x_1x_2, x_2^2)^T$），而 $\\mathbf{c}$ 是相应系数 $c_\\alpha$ 的向量。\n\n决策边界是满足 $g_m(\\mathbf{x})=0$ 的点集。决策规则根据判别函数的符号分配类别标签：\n$$\n\\text{class}(\\mathbf{x}) = \\begin{cases} +1 & \\text{if } g_m(\\mathbf{x}) \\ge 0 \\\\ -1 & \\text{if } g_m(\\mathbf{x}) < 0 \\end{cases}\n$$\n\n### 3. 通过正则化最小二乘法进行模型训练\n\n系数向量 $\\mathbf{c}$ 是通过使用正则化最小二乘法（岭回归）将模型拟合到训练集 $\\{(\\mathbf{x}_i, y_i)\\}_{i=1}^{n_{\\text{train}}}$ 来确定的。该方法最小化判别函数输出与真实标签之间的平方误差之和，再加上一个关于系数幅度的惩罚项：\n$$\n\\min_{\\mathbf{c}} \\sum_{i=1}^{n_{\\text{train}}} (g_m(\\mathbf{x}_i) - y_i)^2 + \\lambda \\sum_{|\\alpha| \\le m} c_\\alpha^2\n$$\n其中 $\\lambda = 10^{-6}$ 是正则化参数。用矩阵形式表示为：\n$$\n\\min_{\\mathbf{c}} \\|\\mathbf{\\Phi}\\mathbf{c} - \\mathbf{y}\\|^2_2 + \\lambda \\|\\mathbf{c}\\|^2_2\n$$\n其中 $\\mathbf{\\Phi}$ 是设计矩阵，其行向量为 $\\boldsymbol{\\phi}_m(\\mathbf{x}_i)^T$，而 $\\mathbf{y}$ 是标签向量。最优系数 $\\hat{\\mathbf{c}}$ 的解析解由正规方程给出：\n$$\n\\hat{\\mathbf{c}} = (\\mathbf{\\Phi}^T\\mathbf{\\Phi} + \\lambda I)^{-1}\\mathbf{\\Phi}^T\\mathbf{y}\n$$\n其中 $I$ 是适当维度的单位矩阵。\n\n### 4. 模型选择与评估\n\n核心任务是找到能够达到期望分类性能的最小多项式阶数 $m^*$。其步骤如下：\n1.  对每个测试案例，生成数据并将其分割为训练集（$f_{\\text{train}}=0.7$ 的样本）和验证集（$f_{\\text{val}}=0.3$）。在用固定的随机种子打乱数据集后进行分割，以确保可复现性。\n2.  对于从 $1$ 到 $M_{\\max}$ 的每个候选阶数 $m$：\n    a. 为训练数据构建多项式特征矩阵 $\\mathbf{\\Phi}_{\\text{train}}$。\n    b. 使用岭回归公式计算系数向量 $\\hat{\\mathbf{c}}$ 来训练模型。\n    c. 在验证集上评估训练好的模型。对于每个验证点 $\\mathbf{x}_j$，计算预测标签 $\\hat{y}_j = \\operatorname{sign}(g_m(\\mathbf{x}_j))$。\n    d. 计算验证准确率，即正确分类的点的比例：$\\text{Accuracy} = \\frac{1}{n_{\\text{val}}} \\sum_{j=1}^{n_{\\text{val}}} \\mathbb{I}(\\hat{y}_j = y_j)$，其中 $\\mathbb{I}(\\cdot)$ 是指示函数。\n3.  所需的最小阶数 $m^*$ 是搜索范围内使得验证准确率大于或等于指定阈值 $\\tau$ 的最小 $m$。\n4.  如果在 $\\{1, \\dots, M_{\\max}\\}$ 范围内的所有阶数 $m$ 都未能达到准确率阈值 $\\tau$，则结果报告为 $-1$。\n\n对测试套件中提供的每个参数集执行此过程，以确定相应的最小阶数。\n\n### 代码实现\n```python\nimport numpy as np\n\ndef generate_polynomial_features(X, degree):\n    \"\"\"\n    Generates a design matrix for polynomial features up to a given degree.\n\n    Args:\n        X (np.ndarray): Input data of shape (n_samples, 2).\n        degree (int): The maximum degree of the polynomial.\n\n    Returns:\n        np.ndarray: The design matrix of shape (n_samples, n_features).\n    \"\"\"\n    n_samples = X.shape[0]\n    x1 = X[:, 0]\n    x2 = X[:, 1]\n\n    num_features = (degree + 1) * (degree + 2) // 2\n    phi = np.empty((n_samples, num_features))\n\n    feature_idx = 0\n    for m in range(degree + 1):\n        for p1 in range(m + 1):\n            p2 = m - p1\n            phi[:, feature_idx] = (x1**p1) * (x2**p2)\n            feature_idx += 1\n    return phi\n\ndef run_case(N, T, a, sigma, tau, M_max, seed):\n    \"\"\"\n    Runs a single test case to find the minimal polynomial degree.\n\n    Args:\n        N (int): Total number of samples.\n        T (float): Number of turns for the spiral.\n        a (float): Spiral constant (r = a*theta).\n        sigma (float): Standard deviation of Gaussian noise.\n        tau (float): Accuracy threshold.\n        M_max (int): Maximum polynomial degree to test.\n        seed (int): Random seed for reproducibility.\n\n    Returns:\n        int: The minimal polynomial degree, or -1 if threshold is not met.\n    \"\"\"\n    # 1. Data Generation\n    rng = np.random.default_rng(seed)\n    n_per_class = N // 2\n    \n    # Generate angles for class +1\n    theta = rng.uniform(0, 2 * np.pi * T, size=n_per_class)\n    r = a * theta\n    \n    # Class +1 points\n    x_plus = r * np.cos(theta)\n    y_plus = r * np.sin(theta)\n    \n    # Class -1 points (point-symmetric to class +1)\n    x_minus = -x_plus\n    y_minus = -y_plus\n    \n    X_plus = np.stack((x_plus, y_plus), axis=1)\n    X_minus = np.stack((x_minus, y_minus), axis=1)\n    \n    # Add noise\n    X_plus += rng.normal(0, sigma, size=X_plus.shape)\n    X_minus += rng.normal(0, sigma, size=X_minus.shape)\n    \n    X = np.vstack((X_plus, X_minus))\n    y = np.hstack((np.ones(n_per_class), -np.ones(n_per_class)))\n    \n    # Normalization\n    r_max = a * 2 * np.pi * T\n    if r_max > 0:\n        X /= r_max\n        \n    # 2. Data Splitting (70% train, 30% validation)\n    indices = np.arange(N)\n    rng.shuffle(indices)\n    \n    train_size = int(0.7 * N)\n    train_idx = indices[:train_size]\n    val_idx = indices[train_size:]\n    \n    X_train, y_train = X[train_idx], y[train_idx]\n    X_val, y_val = X[val_idx], y[val_idx]\n\n    lambda_reg = 1e-6\n    \n    # 3. Model Selection Loop\n    for m in range(1, M_max + 1):\n        # Feature Engineering\n        phi_train = generate_polynomial_features(X_train, m)\n        phi_val = generate_polynomial_features(X_val, m)\n\n        # Training (Ridge Regression)\n        d = phi_train.shape[1]\n        I = np.eye(d)\n        \n        try:\n            # Solve normal equations: (Phi^T * Phi + lambda * I) * c = Phi^T * y\n            A = phi_train.T @ phi_train + lambda_reg * I\n            b = phi_train.T @ y_train\n            coeffs = np.linalg.solve(A, b)\n        except np.linalg.LinAlgError:\n            # Failsafe for singular matrix, though unlikely with ridge\n            continue \n\n        # Validation\n        g_val = phi_val @ coeffs\n        \n        # Predictions: sign(g), with sign(0) -> +1\n        # (g_val >= 0) -> bool, * 2 - 1 maps True to 1 and False to -1\n        y_pred = (g_val >= 0) * 2 - 1\n        \n        accuracy = np.mean(y_pred == y_val)\n        \n        if accuracy >= tau:\n            return m\n            \n    return -1\n\ndef solve():\n    \"\"\"\n    Main function to execute all test cases and print results.\n    \"\"\"\n    test_cases = [\n        (600, 3, 0.5, 0.05, 0.85, 10, 0),\n        (600, 2, 0.7, 0.0, 0.90, 10, 1),\n        (600, 5, 0.3, 0.12, 0.80, 12, 2),\n        (300, 4, 0.6, 0.25, 0.75, 12, 3),\n    ]\n\n    results = []\n    for params in test_cases:\n        result = run_case(*params)\n        results.append(result)\n\n    # Print results in the specified format\n    print(f\"[{','.join(map(str, results))}]\")\n\n# solve() # This is commented out to not execute during rendering\n```", "answer": "[5,3,7,9]", "id": "3116684"}, {"introduction": "在找到一个决策边界后，分析其内在属性也同样重要，例如它对训练数据微小变化的敏感度。本练习将探讨分类器在面对“对抗性”样本时的鲁棒性，你将分析一个标签的翻转如何从根本上改变二次分类器决策边界的形状或拓扑结构。通过这个过程，你将更深刻地理解不同模型稳定性的差异，以及决策边界的精细行为 [@problem_id:3116628]。", "problem": "考虑在实数轴上的二元分类问题，有三个训练样本，其输入为 $x_{1}=-1$、$x_{2}=0$ 和 $x_{3}=1$，对应的标签为 $y_{1}=-1$、$y_{2}=+1$ 和 $y_{3}=+1$。分类器的判别函数是一个实值函数 $f(x)$，其决策边界是满足 $f(x)=0$ 的点集，正决策区域为 $\\{x:\\,f(x)\\ge 0\\}$。\n\n两个分类器在这些数据上根据以下规则进行训练：\n\n- 线性最小二乘分类器 $f_{\\mathrm{lin}}(x)=w x + b$ 是通过最小化 $\\sum_{i=1}^{3}\\bigl(y_{i}-\\bigl(w x_{i}+b\\bigr)\\bigr)^{2}$（关于 $w$ 和 $b$）得到的。定义每个训练点的间隔为 $m_{i}=|f_{\\mathrm{lin}}(x_{i})|$。一个攻击者被允许仅在间隔不超过阈值 $\\tau=\\frac{7}{10}$ 的训练点上翻转标签。\n\n- 在多项式特征空间中的一个2次核分类器（等价于一个二次判别式）$f_{\\mathrm{ker}}(x)=a x^{2}+b x + c$ 是通过对标签进行精确插值得到的，即在没有正则化的情况下强制 $f_{\\mathrm{ker}}(x_{i})=y_{i}$ 对 $i=1,2,3$ 成立。$f_{\\mathrm{ker}}$ 在一维空间中决策边界的拓扑结构由 $f_{\\mathrm{ker}}(x)=0$ 的不同实根的数量决定：两个不同的实根对应两个边界点（且正决策区域可能不连通），一个重实根对应单个边界点，没有实根则对应边界的缺失。\n\n从给定的标签开始，确定将核分类器决策边界的拓扑结构从具有两个不同实根变为没有实根所需的最少攻击性标签翻转次数，翻转仅限于间隔满足 $m_{i}\\le \\tau$ 的允许训练点。请以单个整数形式提供你的答案。无需四舍五入。", "solution": "该问题要求找到改变核分类器决策边界拓扑结构所需的最小攻击性标签翻转次数。该过程包括三个主要步骤：首先，通过分析线性最小二乘分类器来确定哪些训练点的标签容易被翻转；其次，描述初始核分类器的特性；第三，检验允许的翻转对核分类器决策边界的影响。\n\n首先，我们确定线性最小二乘分类器 $f_{\\mathrm{lin}}(x)=w x + b$ 的参数。该分类器在数据点 $(x_1, y_1) = (-1, -1)$、$(x_2, y_2) = (0, 1)$ 和 $(x_3, y_3) = (1, 1)$ 上进行训练。参数 $w$ 和 $b$ 通过最小化平方误差和 $J(w, b) = \\sum_{i=1}^{3} (y_i - (w x_i + b))^2$ 来找到。\n成本函数为：\n$$J(w,b) = (y_1 - (wx_1+b))^2 + (y_2 - (wx_2+b))^2 + (y_3 - (wx_3+b))^2$$\n$$J(w,b) = (-1 - (-w+b))^2 + (1 - (0+b))^2 + (1 - (w+b))^2$$\n$$J(w,b) = (-1+w-b)^2 + (1-b)^2 + (1-w-b)^2$$\n为了最小化 $J(w, b)$，我们将其关于 $w$ 和 $b$ 的偏导数设为0。\n$$\\frac{\\partial J}{\\partial b} = 2(-1+w-b)(-1) + 2(1-b)(-1) + 2(1-w-b)(-1) = 0$$\n$$ (1-w+b) + (b-1) + (w+b-1) = 0 $$\n$$ 3b - 1 = 0 \\implies b = \\frac{1}{3} $$\n$$\\frac{\\partial J}{\\partial w} = 2(-1+w-b)(1) + 2(1-w-b)(-1) = 0$$\n$$ (-1+w-b) - (1-w-b) = 0 $$\n$$ 2w - 2 = 0 \\implies w = 1 $$\n因此，线性最小二乘分类器是 $f_{\\mathrm{lin}}(x) = x + \\frac{1}{3}$。\n\n接下来，我们为每个训练点计算间隔 $m_i = |f_{\\mathrm{lin}}(x_i)|$。\n对于 $x_1 = -1$：$f_{\\mathrm{lin}}(-1) = -1 + \\frac{1}{3} = -\\frac{2}{3}$。间隔为 $m_1 = |-\\frac{2}{3}| = \\frac{2}{3}$。\n对于 $x_2 = 0$：$f_{\\mathrm{lin}}(0) = 0 + \\frac{1}{3} = \\frac{1}{3}$。间隔为 $m_2 = \\frac{1}{3}$。\n对于 $x_3 = 1$：$f_{\\mathrm{lin}}(1) = 1 + \\frac{1}{3} = \\frac{4}{3}$。间隔为 $m_3 = \\frac{4}{3}$。\n\n攻击者只有在间隔 $m_i$ 不超过阈值 $\\tau = \\frac{7}{10}$ 时才能翻转标签 $y_i$。\n$m_1 = \\frac{2}{3} \\approx 0.667$。由于 $\\frac{2}{3}  \\frac{7}{10}$，标签 $y_1$ 可以被翻转。\n$m_2 = \\frac{1}{3} \\approx 0.333$。由于 $\\frac{1}{3}  \\frac{7}{10}$，标签 $y_2$ 可以被翻转。\n$m_3 = \\frac{4}{3} \\approx 1.333$。由于 $\\frac{4}{3} > \\frac{7}{10}$，标签 $y_3$ 不能被翻转。\n因此，在 $x_1=-1$ 和 $x_2=0$ 处的标签是可翻转的。\n\n现在我们分析初始的2次核分类器 $f_{\\mathrm{ker}}(x) = ax^2 + bx + c$。它通过对初始标签 $y_1 = -1$、$y_2 = 1$、$y_3 = 1$ 进行精确插值来确定。\n插值条件是：\n$f_{\\mathrm{ker}}(-1) = a(-1)^2 + b(-1) + c = a - b + c = -1$\n$f_{\\mathrm{ker}}(0) = a(0)^2 + b(0) + c = c = 1$\n$f_{\\mathrm{ker}}(1) = a(1)^2 + b(1) + c = a + b + c = 1$\n将 $c=1$ 代入第一个和第三个方程得到：\n$a - b + 1 = -1 \\implies a - b = -2$\n$a + b + 1 = 1 \\implies a + b = 0$\n将这两个方程相加得到 $2a = -2$，所以 $a = -1$。将 $a=-1$ 代入 $a+b=0$ 得到 $b=1$。\n初始的核分类器是 $f_{\\mathrm{ker}}(x) = -x^2 + x + 1$。\n其决策边界的拓扑结构由 $f_{\\mathrm{ker}}(x) = 0$ 的根决定。不同实根的数量取决于判别式 $\\Delta = b^2 - 4ac$。\n对于初始分类器，$\\Delta = (1)^2 - 4(-1)(1) = 1 + 4 = 5$。由于 $\\Delta > 0$，存在两个不同的实根，正如问题中所述。\n\n目标是使用最少次数的允许翻转，将拓扑结构从“两个不同的实根”($\\Delta > 0$)变为“没有实根”($\\Delta  0$)。让我们测试单次翻转的情形。\n\n情况1：翻转 $x_1$ 处的标签。原始标签是 $y_1=-1$；新标签是 $y'_1 = 1$。标签集变为 $(y'_1, y_2, y_3) = (1, 1, 1)$。我们通过对这些新标签进行插值来找到新的核分类器 $f'_{\\mathrm{ker}}(x) = a'x^2+b'x+c'$。\n$a' - b' + c' = 1$\n$c' = 1$\n$a' + b' + c' = 1$\n代入 $c'=1$ 得到：\n$a' - b' + 1 = 1 \\implies a' - b' = 0$\n$a' + b' + 1 = 1 \\implies a' + b' = 0$\n解这个方程组得到 $a'=0$ 和 $b'=0$。\n新的分类器是 $f'_{\\mathrm{ker}}(x) = 0 \\cdot x^2 + 0 \\cdot x + 1 = 1$。决策边界由 $f'_{\\mathrm{ker}}(x) = 0$ 定义，即方程 $1=0$。该方程没有解，因此没有实根。这对应于所期望的“没有实根”的拓扑结构。\n这个改变仅用一次翻转就实现了。\n\n为完整起见，让我们检查另一种单次翻转的可能性。\n情况2：翻转 $x_2$ 处的标签。原始标签是 $y_2=1$；新标签是 $y''_2 = -1$。标签集变为 $(y_1, y''_2, y_3) = (-1, -1, 1)$。我们找到新的分类器 $f''_{\\mathrm{ker}}(x) = a''x^2+b''x+c''$。\n$a'' - b'' + c'' = -1$\n$c'' = -1$\n$a'' + b'' + c'' = 1$\n代入 $c''=-1$ 得到：\n$a'' - b'' - 1 = -1 \\implies a'' - b'' = 0 \\implies a''=b''$\n$a'' + b'' - 1 = 1 \\implies a'' + b'' = 2$\n将 $a''=b''$ 代入第二个方程得到 $2a'' = 2$，所以 $a''=1$ 和 $b''=1$。\n新的分类器是 $f''_{\\mathrm{ker}}(x) = x^2 + x - 1$。\n判别式是 $\\Delta'' = (1)^2 - 4(1)(-1) = 1+4=5$。由于 $\\Delta'' > 0$，拓扑结构仍然是“两个不同的实根”。这次翻转没有达到目标。\n\n由于单次翻转（在 $x_1$ 处）足以将拓扑结构改变为“没有实根”，并且1是可能的最小非零翻转次数，因此所需的最少翻转次数是1。", "answer": "$$\\boxed{1}$$", "id": "3116628"}]}