{"hands_on_practices": [{"introduction": "第一个练习是一个概念性探索，旨在揭示两种模型类型之间的根本关系。我们将研究一些具体情景，它们表明不同的生成模型参数化，例如具有不同先验 $\\pi_y$ 和类条件密度 $p(x|y)$ 的模型，可能产生完全相同的后验概率 $p(y|x)$，从而拥有相同的决策边界。这个练习阐明了为何模型的生成属性（如参数可辨识性）可能不影响其判别性能，帮助你从根本上理解这两种范式之间的区别 [@problem_id:3124837]。", "problem": "考虑一个二元分类问题，其类别标签为 $y \\in \\{0,1\\}$，特征为实值 $x \\in \\mathbb{R}$。令 $\\pi_y = \\mathbb{P}(y)$ 表示类别先验，令 $p(x \\mid y)$ 表示类别条件密度。回顾基本定义：\n- 参数统计模型的可辨识性（identifiability）意味着从参数到其所导出的分布的映射是单射（一对一）的。如果两个不同的参数值导出了相同的可观测分布，则这些参数是不可辨识的。\n- 贝叶斯(Bayes)法则意味着 $p(y \\mid x) = \\dfrac{\\pi_y \\, p(x \\mid y)}{\\sum_{y' \\in \\{0,1\\}} \\pi_{y'} \\, p(x \\mid y')}$，且最小化 $0$-$1$ 损失的贝叶斯决策规则会将样本指派给后验概率较大的类别。\n\n我们将分析两种具体的情境。\n\n情境 $\\mathrm{S1}$（类内分量重叠的混合模型）：假设\n- $\\pi_0 = \\pi_1 = \\tfrac{1}{2}$，\n- 对于类别 $y=0$，其类别条件密度是一个双分量高斯混合模型，混合权重参数为 $w \\in (0,1)$，\n$$\np(x \\mid y=0; w) \\;=\\; w \\, \\mathcal{N}(x; 0, 1) \\;+\\; (1-w) \\, \\mathcal{N}(x; 0, 1),\n$$\n- 对于类别 $y=1$，$p(x \\mid y=1) \\;=\\; \\mathcal{N}(x; 1, 1)$，\n其中 $\\mathcal{N}(x;\\mu,\\sigma^2)$ 表示均值为 $\\mu$、方差为 $\\sigma^2$ 的高斯密度。\n\n情境 $\\mathrm{S2}$（两个产生相同判别行为的不同生成模型）：考虑两个模型 $\\mathrm{G1}$ 和 $\\mathrm{G2}$，每个模型都具有类别条件高斯密度，且模型内部的方差相等：\n- 模型 $\\mathrm{G1}$：$\\pi_1^{(1)} = \\tfrac{1}{2}$，$\\pi_0^{(1)} = \\tfrac{1}{2}$，$p^{(1)}(x \\mid y=1) = \\mathcal{N}(x; 1, 1)$，$p^{(1)}(x \\mid y=0) = \\mathcal{N}(x; -1, 1)$。\n- 模型 $\\mathrm{G2}$：$\\pi_1^{(2)} = \\dfrac{e^{2}}{1+e^{2}}$，$\\pi_0^{(2)} = \\dfrac{1}{1+e^{2}}$，$p^{(2)}(x \\mid y=1) = \\mathcal{N}(x; 3, 2)$，$p^{(2)}(x \\mid y=0) = \\mathcal{N}(x; -1, 2)$。\n\n通过选择所有正确选项来回答以下多项选择题。\n\n下列哪些陈述是正确的？\n\nA. 在情境 $\\mathrm{S1}$ 中，生成参数 $w$ 从 $\\{(x_i,y_i)\\}_{i=1}^n$ 中是不可辨识的，因为所有 $w \\in (0,1)$ 都导出相同的 $p(x \\mid y=0)$，因此也导出相同的 $p(y \\mid x)$。所以，即使有无限数据也无法恢复 $w$。\n\nB. 在情境 $\\mathrm{S2}$ 中，对于任意 $x \\in \\mathbb{R}$，模型 $\\mathrm{G1}$ 下的后验概率 $p^{(1)}(y=1 \\mid x)$ 等于模型 $\\mathrm{G2}$ 下的后验概率 $p^{(2)}(y=1 \\mid x)$，尽管 $\\{\\pi_y^{(1)}, p^{(1)}(x \\mid y)\\}$ 和 $\\{\\pi_y^{(2)}, p^{(2)}(x \\mid y)\\}$ 两组参数不同。\n\nC. 如果两种不同的生成参数化对所有 $x \\in \\mathbb{R}$ 都产生相同的 $p(y \\mid x)$，那么它们也必定对每个 $y$ 具有相同的类别条件密度 $p(x \\mid y)$ 和相同的先验 $\\pi_y$。换言之，判别可辨识性意味着生成可辨识性。\n\nD. 因为在这两种情境中类别条件密度都有重叠，所以贝叶斯最优决策边界不是唯一确定的；仅凭重叠就使得决策边界变得模糊不清。", "solution": "我们从第一性原理出发：可辨识性的定义和贝叶斯法则 $p(y \\mid x) = \\dfrac{\\pi_y \\, p(x \\mid y)}{\\sum_{y' \\in \\{0,1\\}} \\pi_{y'} \\, p(x \\mid y')}$。\n\n情境 $\\mathrm{S1}$ 的分析：\n- 对于类别 $y=0$，混合模型为\n$$\np(x \\mid y=0; w) \\;=\\; w \\, \\mathcal{N}(x; 0, 1) \\;+\\; (1-w) \\, \\mathcal{N}(x; 0, 1).\n$$\n根据线性性质，由于两个分量是相同的密度，混合模型简化为\n$$\np(x \\mid y=0; w) \\;=\\; \\mathcal{N}(x; 0, 1) \\quad \\text{对于所有 } w \\in (0,1)。\n$$\n因此，对于所有 $w$，可观测分布 $p(x \\mid y=0; w)$ 都是相同的，同样地，联合分布 $p(x,y)$ 和后验分布 $p(y \\mid x)$ 也都与 $w$ 无关。因此，$w$ 是不可辨识的。这是一种结构性不可辨识性：存在一个不可数集族的参数值，它们都导出相同的数据分布。\n- 由于 $\\pi_0 = \\pi_1 = \\tfrac{1}{2}$ 且 $p(x \\mid y=1)$ 固定为 $\\mathcal{N}(x; 1, 1)$，贝叶斯法则给出的后验概率 $p(y \\mid x)$ 完全不依赖于 $w$。因此，即使有无限数据，也无法恢复 $w$。\n\n关于 $\\mathrm{S1}$ 的结论：陈述 A 是正确的。\n\n情境 $\\mathrm{S2}$ 的分析：\n我们通过对数优势（log-odds）$\\ell(x) = \\log \\dfrac{p(y=1 \\mid x)}{p(y=0 \\mid x)}$ 来比较 $p^{(1)}(y=1 \\mid x)$ 和 $p^{(2)}(y=1 \\mid x)$。根据贝叶斯法则，\n$$\n\\ell(x) \\;=\\; \\log \\frac{\\pi_1 \\, p(x \\mid y=1)}{\\pi_0 \\, p(x \\mid y=0)} \\;=\\; \\log \\frac{\\pi_1}{\\pi_0} \\;+\\; \\log \\frac{p(x \\mid y=1)}{p(x \\mid y=0)}.\n$$\n对于方差均为 $\\sigma^2$ 的高斯类别条件分布，直接展开可以表明 $\\log \\dfrac{p(x \\mid y=1)}{p(x \\mid y=0)}$ 是 $x$ 的一个仿射函数。我们对每个模型进行显式推导。\n\n- 模型 $\\mathrm{G1}$：$\\pi_1^{(1)} = \\tfrac{1}{2}$，$\\pi_0^{(1)} = \\tfrac{1}{2}$，$p^{(1)}(x \\mid y=1) = \\mathcal{N}(x;1,1)$，$p^{(1)}(x \\mid y=0) = \\mathcal{N}(x;-1,1)$。则\n$$\n\\ell_1(x) \\;=\\; \\log \\frac{\\tfrac{1}{2}}{\\tfrac{1}{2}} \\;+\\; \\log \\frac{\\mathcal{N}(x;1,1)}{\\mathcal{N}(x;-1,1)}\n\\;=\\; 0 \\;-\\; \\frac{(x-1)^2 - (x+1)^2}{2 \\cdot 1}\n\\;=\\; -\\frac{(x^2 - 2x + 1) - (x^2 + 2x + 1)}{2}\n\\;=\\; -\\frac{-4x}{2}\n\\;=\\; 2x.\n$$\n因此 $p^{(1)}(y=1 \\mid x) \\;=\\; \\dfrac{1}{1 + e^{-\\ell_1(x)}} \\;=\\; \\dfrac{1}{1 + e^{-2x}}$。\n\n- 模型 $\\mathrm{G2}$：$\\pi_1^{(2)} = \\dfrac{e^{2}}{1+e^{2}}$，$\\pi_0^{(2)} = \\dfrac{1}{1+e^{2}}$，$p^{(2)}(x \\mid y=1) = \\mathcal{N}(x;3,2)$，$p^{(2)}(x \\mid y=0) = \\mathcal{N}(x;-1,2)$。则\n$$\n\\ell_2(x) \\;=\\; \\log \\frac{\\pi_1^{(2)}}{\\pi_0^{(2)}} \\;+\\; \\log \\frac{\\mathcal{N}(x;3,2)}{\\mathcal{N}(x;-1,2)}\n\\;=\\; \\log \\left( \\frac{\\tfrac{e^{2}}{1+e^{2}}}{\\tfrac{1}{1+e^{2}}} \\right)\n\\;-\\; \\frac{(x-3)^2 - (x+1)^2}{2 \\cdot 2}.\n$$\n计算各项：\n$$\n\\log \\left( \\frac{e^{2}}{1} \\right) \\;=\\; 2, \\quad\n(x-3)^2 - (x+1)^2 \\;=\\; (x^2 - 6x + 9) - (x^2 + 2x + 1) \\;=\\; -8x + 8.\n$$\n因此\n$$\n\\ell_2(x) \\;=\\; 2 \\;-\\; \\frac{-8x + 8}{4}\n\\;=\\; 2 \\;-\\; (-2x + 2)\n\\;=\\; 2x.\n$$\n因此 $p^{(2)}(y=1 \\mid x) \\;=\\; \\dfrac{1}{1 + e^{-2x}}$，这对于每个 $x \\in \\mathbb{R}$ 都与模型 $\\mathrm{G1}$ 下的后验概率相同，尽管 $\\mathrm{G1}$ 和 $\\mathrm{G2}$ 的先验概率和类别条件密度不同。\n\n关于 $\\mathrm{S2}$ 的结论：陈述 B 是正确的。\n\n对陈述 C 和 D 的影响：\n- 陈述 C 声称，对于所有 $x$ 都有相同的 $p(y \\mid x)$ 会强制要求生成模型的组件 $p(x \\mid y)$ 和 $\\pi_y$ 也相同。上面的显式反例（模型 $\\mathrm{G1}$ 和 $\\mathrm{G2}$）表明这是错误的：我们对所有 $x$ 都有相同的 $p(y \\mid x)$，但 $\\{\\pi_y, p(x \\mid y)\\}$ 却不同。因此 C 是不正确的。\n- 陈述 D 声称 $p(x \\mid y=0)$ 和 $p(x \\mid y=1)$ 的重叠意味着贝叶斯决策边界不唯一。这是不正确的。在二元分类情况下，贝叶斯决策边界是由满足 $\\pi_1 p(x \\mid y=1) = \\pi_0 p(x \\mid y=0)$（等价于 $p(y=1 \\mid x) = \\tfrac{1}{2}$）的 $x$ 的集合定义的。在模型 $\\mathrm{G1}$ 中，我们发现 $\\ell_1(x) = 2x$，所以决策边界是满足 $\\ell_1(x) = 0$ 的唯一点，即 $x=0$。在模型 $\\mathrm{G2}$ 中，$\\ell_2(x) = 2x$ 也得到唯一的决策边界 $x=0$。重叠会影响错误率，但本身不会导致贝叶斯规则的模糊性；后验概率是明确定义的，决策边界也是如此，除非出现退化情况，即在某个区间上完全相等，而这种情况在这里没有发生。因此 D 是不正确的。\n\n各选项分析总结：\n- A：正确。在 $\\mathrm{S1}$ 中，对于所有 $w$，$p(x \\mid y=0; w)$ 都相同，因此 $w$ 是不可辨识的，且 $p(y \\mid x)$ 不变。\n- B：正确。在 $\\mathrm{S2}$ 中，两个模型都得到 $\\ell(x) = 2x$，因此对所有 $x$ 都有相同的 $p(y=1 \\mid x)$。\n- C：不正确。$\\mathrm{G1}$ 和 $\\mathrm{G2}$ 是一个反例：$p(y \\mid x)$ 相同，但 $\\{\\pi_y, p(x \\mid y)\\}$ 不同。\n- D：不正确。重叠并不意味着贝叶斯边界不唯一；在这里，两个模型中的边界都是唯一的 $x=0$。", "answer": "$$\\boxed{AB}$$", "id": "3124837"}, {"introduction": "判别模型通常很强大，但当我们的部分数据缺失时会发生什么？本练习模拟了一个数据缺失与类别标签相关的场景，一个朴素的判别模型在这种情况下表现不佳。通过推导和计算，你将看到生成模型如何通过对包括缺失指示变量 $M$ 在内的整个数据生成过程（即 $p(x, y, M)$）进行显式建模，来纠正这种偏差并提供更准确的推断 [@problem_id:3124923]。", "problem": "给定一个二元分类场景，其中包含一个实值特征和一个显式的缺失机制。设类别标签为伯努利随机变量 $Y \\in \\{0,1\\}$，其先验概率为 $P(Y=1)=\\pi$ 和 $P(Y=0)=1-\\pi$。设 $X \\in \\mathbb{R}$ 是一个可能缺失的特征，设 $M \\in \\{0,1\\}$ 是缺失指示符，其中 $M=1$ 表示 $X$ 缺失，$M=0$ 表示 $X$ 已观测到。假设数据生成过程如下：\n- 联合分布可分解为 $p(y,x,m) = p(y)\\,p(x \\mid y)\\,p(m \\mid y)$，即缺失仅取决于类别，并且在给定类别的情况下，与特征条件独立（相对于 $X$ 而言是随机缺失 (MAR)）。\n- 缺失机制按类别定义为 $P(M=1 \\mid Y=1)=\\alpha_1$ 和 $P(M=1 \\mid Y=0)=\\alpha_0$，其中 $0 \\lt \\alpha_0 \\lt 1$ 且 $0 \\lt \\alpha_1 \\lt 1$。\n\n一个忽略 $M$ 的判别模型试图直接从观测到的数据对中学习 $p(y \\mid x)$。当在预测时 $X$ 缺失且 $M$ 被忽略时，由于没有可用于条件化的观测特征值，此类模型会退化为预测先验概率 $P(Y=1)=\\pi$。而生成模型使用完整的分解式 $p(y)\\,p(x \\mid y)\\,p(m \\mid y)$，即使在 $X$ 未被观测到的情况下，也允许通过 $p(m \\mid y)$ 从缺失指示符中更新关于 $Y$ 的信念。\n\n任务 A（推导）。仅从贝叶斯定理和全概率定律出发：\n- 用 $\\pi$、$\\alpha_0$ 和 $\\alpha_1$ 推导 $P(Y=1 \\mid M=1)$ 和 $P(Y=1 \\mid M=0)$。\n- 将忽略 $M$ 的判别预测（当 $X$ 缺失时）定义为 $\\widehat{P}_{\\text{disc}}(Y=1 \\mid M=1)=\\pi$。将 $M=1$ 的偏差定义为 $b_1 = P(Y=1 \\mid M=1) - \\pi$，并类似地将 $M=0$ 的偏差定义为 $b_0 = P(Y=1 \\mid M=0) - \\pi$。用 $\\pi$、$\\alpha_0$ 和 $\\alpha_1$ 表示 $b_1$ 和 $b_0$。\n- 考虑一种训练方法，该方法丢弃特征缺失的样本（即，限制在 $M=0$ 的情况下），然后在这个子集上拟合一个用于 $p(y \\mid x)$ 的判别模型，同时忽略 $M$。在上述 MAR 假设下，推导 $P(Y=1 \\mid X=x, M=0)$ 的对数几率与 $P(Y=1 \\mid X=x)$ 的对数几率之间的关系。证明它们之间相差一个仅依赖于 $\\alpha_0$ 和 $\\alpha_1$ 的加性常数偏移。将此偏移表示为 $s$，并用 $\\alpha_0$ 和 $\\alpha_1$ 表示它。\n\n任务 B（计算）。实现一个程序，给定 $(\\pi,\\alpha_0,\\alpha_1)$，计算以下三个量：\n- $b_1$，即上文定义的 $M=1$ 的偏差。\n- $b_0$，即上文定义的 $M=0$ 的偏差。\n- $s$，即上文定义的常数对数几率偏移。\n\n使用以下参数三元组 $(\\pi,\\alpha_0,\\alpha_1)$ 的测试套件：\n- 测试用例 1：$(\\pi,\\alpha_0,\\alpha_1) = (0.5, 0.2, 0.8)$。\n- 测试用例 2：$(\\pi,\\alpha_0,\\alpha_1) = (0.4, 0.3, 0.3)$。\n- 测试用例 3：$(\\pi,\\alpha_0,\\alpha_1) = (0.3, 0.01, 0.99)$。\n\n你的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，顺序为 $[b_1^{(1)}, b_0^{(1)}, s^{(1)}, b_1^{(2)}, b_0^{(2)}, s^{(2)}, b_1^{(3)}, b_0^{(3)}, s^{(3)}]$，其中上标表示测试用例索引。输出中的每个数字必须精确到小数点后 $6$ 位。不应打印任何其他文本。", "solution": "任务 A：推导\n\n首先，我们推导在给定缺失状态 $M$ 的情况下类别 $Y=1$ 的后验概率。我们将使用贝叶斯定理 $P(A \\mid B) = \\frac{P(B \\mid A)P(A)}{P(B)}$ 和全概率定律 $P(B) = \\sum_i P(B \\mid A_i)P(A_i)$。\n\n推导 $P(Y=1 \\mid M=1)$：\n后验概率 $P(Y=1 \\mid M=1)$ 由贝叶斯定理给出：\n$$P(Y=1 \\mid M=1) = \\frac{P(M=1 \\mid Y=1) P(Y=1)}{P(M=1)}$$\n分子中的项已知为 $P(M=1 \\mid Y=1) = \\alpha_1$ 和 $P(Y=1) = \\pi$。分母 $P(M=1)$ 是缺失的边际概率，我们使用全概率定律在类别 $Y=0$ 和 $Y=1$ 上对其进行展开：\n$$P(M=1) = P(M=1 \\mid Y=1)P(Y=1) + P(M=1 \\mid Y=0)P(Y=0)$$\n代入已知量 $P(Y=0)=1-\\pi$ 和 $P(M=1 \\mid Y=0)=\\alpha_0$：\n$$P(M=1) = \\alpha_1 \\pi + \\alpha_0 (1-\\pi)$$\n结合这些结果，得到后验概率的表达式：\n$$P(Y=1 \\mid M=1) = \\frac{\\alpha_1 \\pi}{\\alpha_1 \\pi + \\alpha_0 (1-\\pi)}$$\n\n推导 $P(Y=1 \\mid M=0)$：\n此推导遵循相同的逻辑。我们首先确定观测到特征（$M=0$）的概率：\n$P(M=0 \\mid Y=1) = 1 - P(M=1 \\mid Y=1) = 1 - \\alpha_1$\n$P(M=0 \\mid Y=0) = 1 - P(M=1 \\mid Y=0) = 1 - \\alpha_0$\n使用贝叶斯定理：\n$$P(Y=1 \\mid M=0) = \\frac{P(M=0 \\mid Y=1) P(Y=1)}{P(M=0)}$$\n分子是 $(1 - \\alpha_1) \\pi$。分母是观测到特征的边际概率：\n$$P(M=0) = P(M=0 \\mid Y=1)P(Y=1) + P(M=0 \\mid Y=0)P(Y=0) = (1 - \\alpha_1) \\pi + (1 - \\alpha_0) (1-\\pi)$$\n因此，后验概率为：\n$$P(Y=1 \\mid M=0) = \\frac{(1 - \\alpha_1) \\pi}{(1 - \\alpha_1) \\pi + (1 - \\alpha_0) (1-\\pi)}$$\n\n推导偏差 $b_1$ 和 $b_0$：\n偏差 $b_1$ 定义为 $b_1 = P(Y=1 \\mid M=1) - \\pi$。代入推导出的 $P(Y=1 \\mid M=1)$ 表达式：\n$$b_1 = \\frac{\\alpha_1 \\pi}{\\alpha_1 \\pi + \\alpha_0 (1-\\pi)} - \\pi = \\pi \\left( \\frac{\\alpha_1}{\\alpha_1 \\pi + \\alpha_0 (1-\\pi)} - 1 \\right)$$\n$$b_1 = \\pi \\left( \\frac{\\alpha_1 - (\\alpha_1 \\pi + \\alpha_0 (1-\\pi))}{\\alpha_1 \\pi + \\alpha_0 (1-\\pi)} \\right) = \\pi \\left( \\frac{\\alpha_1(1-\\pi) - \\alpha_0(1-\\pi)}{\\alpha_1 \\pi + \\alpha_0 (1-\\pi)} \\right)$$\n$$b_1 = \\frac{\\pi (1-\\pi) (\\alpha_1 - \\alpha_0)}{\\alpha_1 \\pi + \\alpha_0 (1-\\pi)}$$\n该表达式表明，偏差 $b_1$ 为零当且仅当 $\\alpha_1 = \\alpha_0$，即当缺失与类别完全独立时。\n\n偏差 $b_0$ 定义为 $b_0 = P(Y=1 \\mid M=0) - \\pi$。代入推导出的 $P(Y=1 \\mid M=0)$ 表达式：\n$$b_0 = \\frac{(1 - \\alpha_1) \\pi}{(1 - \\alpha_1) \\pi + (1 - \\alpha_0) (1-\\pi)} - \\pi = \\pi \\left( \\frac{1 - \\alpha_1}{(1 - \\alpha_1) \\pi + (1 - \\alpha_0) (1-\\pi)} - 1 \\right)$$\n$$b_0 = \\pi \\left( \\frac{(1 - \\alpha_1) - ((1 - \\alpha_1) \\pi + (1 - \\alpha_0) (1-\\pi))}{(1 - \\alpha_1) \\pi + (1 - \\alpha_0) (1-\\pi)} \\right)$$\n$$b_0 = \\pi \\left( \\frac{(1 - \\alpha_1)(1-\\pi) - (1 - \\alpha_0)(1-\\pi)}{(1 - \\alpha_1) \\pi + (1 - \\alpha_0) (1-\\pi)} \\right)$$\n$$b_0 = \\frac{\\pi (1-\\pi) (\\alpha_0 - \\alpha_1)}{(1 - \\alpha_1) \\pi + (1 - \\alpha_0) (1-\\pi)}$$\n\n推导对数几率偏移 $s$：\n概率为 $p$ 的事件的对数几率是 $\\text{logit}(p) = \\log\\left(\\frac{p}{1-p}\\right)$。我们需要比较 $P(Y=1 \\mid X=x)$ 的对数几率与 $P(Y=1 \\mid X=x, M=0)$ 的对数几率。\n$P(Y=1 \\mid X=x)$ 的几率是：\n$$\\frac{P(Y=1 \\mid X=x)}{P(Y=0 \\mid X=x)} = \\frac{p(x \\mid Y=1)P(Y=1)}{p(x \\mid Y=0)P(Y=0)} = \\frac{p(x \\mid Y=1)\\pi}{p(x \\mid Y=0)(1-\\pi)}$$\n取自然对数得到对数几率：\n$$\\text{log-odds}(P(Y=1 \\mid X=x)) = \\log\\left(\\frac{p(x \\mid Y=1)}{p(x \\mid Y=0)}\\right) + \\log\\left(\\frac{\\pi}{1-\\pi}\\right)$$\n$P(Y=1 \\mid X=x, M=0)$ 的几率是：\n$$\\frac{P(Y=1 \\mid X=x, M=0)}{P(Y=0 \\mid X=x, M=0)} = \\frac{p(Y=1, x, M=0)}{p(Y=0, x, M=0)}$$\n使用给定的分解式 $p(y,x,m) = p(y)p(x \\mid y)p(m \\mid y)$：\n$$ \\frac{p(Y=1)p(x \\mid Y=1)p(M=0 \\mid Y=1)}{p(Y=0)p(x \\mid Y=0)p(M=0 \\mid Y=0)} = \\frac{\\pi \\, p(x \\mid Y=1) \\, (1-\\alpha_1)}{(1-\\pi) \\, p(x \\mid Y=0) \\, (1-\\alpha_0)} $$\n$$ \\frac{P(Y=1 \\mid X=x, M=0)}{P(Y=0 \\mid X=x, M=0)} = \\left(\\frac{p(x \\mid Y=1)\\pi}{p(x \\mid Y=0)(1-\\pi)}\\right) \\left(\\frac{1-\\alpha_1}{1-\\alpha_0}\\right)$$\n取自然对数得到观测数据子集的对数几率：\n$$ \\text{log-odds}(P(Y=1 \\mid X=x, M=0)) = \\log\\left(\\frac{p(x \\mid Y=1)\\pi}{p(x \\mid Y=0)(1-\\pi)}\\right) + \\log\\left(\\frac{1-\\alpha_1}{1-\\alpha_0}\\right) $$\n$$ \\text{log-odds}(P(Y=1 \\mid X=x, M=0)) = \\text{log-odds}(P(Y=1 \\mid X=x)) + \\log\\left(\\frac{1-\\alpha_1}{1-\\alpha_0}\\right) $$\n问题将偏移 $s$ 定义为这个加性常数。因此：\n$$s = \\log\\left(\\frac{1-\\alpha_1}{1-\\alpha_0}\\right)$$\n这个偏移代表了在指定的 MAR 机制下，当模型仅在具有观测特征的数据子集上训练时，在对数几率尺度上引入的偏差。该偏差仅取决于两个类别之间观测率的差异。\n\n这些推导完成了任务 A。这些公式现在将用于任务 B 的计算。", "answer": "```python\n# 完整且可运行的 Python 3 代码。\n# 导入必须符合指定的执行环境。\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    根据给定的二元分类缺失数据模型，为测试用例计算偏差和对数几率偏移。\n    \"\"\"\n    # 定义问题陈述中的测试用例。\n    # 每个元组是 (pi, alpha_0, alpha_1)。\n    test_cases = [\n        (0.5, 0.2, 0.8),\n        (0.4, 0.3, 0.3),\n        (0.3, 0.01, 0.99),\n    ]\n\n    # 用于存储所有测试用例结果的列表。\n    results = []\n\n    for case in test_cases:\n        pi, alpha_0, alpha_1 = case\n\n        # --- 计算 b_1，即 M=1 时的偏差 ---\n        # 公式: b_1 = (pi * (1-pi) * (alpha_1 - alpha_0)) / (alpha_1 * pi + alpha_0 * (1-pi))\n        # 此公式是良定的，因为分母 P(M=1) > 0。\n        b1_numerator = pi * (1.0 - pi) * (alpha_1 - alpha_0)\n        b1_denominator = alpha_1 * pi + alpha_0 * (1.0 - pi)\n        b1 = b1_numerator / b1_denominator\n\n        # --- 计算 b_0，即 M=0 时的偏差 ---\n        # 公式: b_0 = (pi * (1-pi) * (alpha_0 - alpha_1)) / ((1-alpha_1) * pi + (1-alpha_0) * (1-pi))\n        # 此公式是良定的，因为分母 P(M=0) > 0。\n        b0_numerator = pi * (1.0 - pi) * (alpha_0 - alpha_1)\n        b0_denominator = (1.0 - alpha_1) * pi + (1.0 - alpha_0) * (1.0 - pi)\n        b0 = b0_numerator / b0_denominator\n\n        # --- 计算 s，即对数几率偏移 ---\n        # 公式: s = log((1-alpha_1) / (1-alpha_0))\n        # np.log 是自然对数。\n        # 由于 0  alpha_0, alpha_1  1，该对数的参数是良定且为正的。\n        s = np.log((1.0 - alpha_1) / (1.0 - alpha_0))\n\n        results.extend([b1, b0, s])\n\n    # 完全按照要求格式化最终输出字符串。\n    # 每个数字四舍五入到小数点后 6 位。\n    output_str = f\"[{','.join(f'{r:.6f}' for r in results)}]\"\n    print(output_str)\n\n# 执行主函数。\nsolve()\n```", "id": "3124923"}, {"introduction": "特征选择是构建高效机器学习模型的关键步骤。这个编程实践练习要求你实现并对比两种截然不同的特征选择方法：一种植根于朴素贝叶斯（Naive Bayes）的生成式假设，另一种基于逻辑回归（logistic regression）的判别能力。通过在多个合成数据集上比较它们的特征排序结果，包括处理相关特征的情况，你将深入理解它们各自的底层假设如何影响其在实践中的行为 [@problem_id:3124940]。", "problem": "给定一个二元分类场景，其中有随机变量 $X \\in \\mathbb{R}^d$ 和 $Y \\in \\{0,1\\}$。我们考虑两类模型：一类是指定类条件分布 $p(x \\mid y)$ 的生成模型，另一类是指定条件类别概率 $p(y \\mid x)$ 的判别模型。您的任务是实现两种特征选择程序，并在几个合成测试用例上对它们进行比较。\n\n此任务的基础是条件概率和贝叶斯法则的定义、用于参数估计的最大似然原理，以及经过充分检验的逻辑回归模型对 $p(y \\mid x)$ 的表述。生成式程序将通过特征对类条件对数似然的经验期望的贡献来为特征评分，而判别式程序将通过对数条件概率相对于每个特征的梯度绝对值的经验平均值来为特征评分。\n\n使用的定义：\n- 函数 $f(X,Y)$ 在数据集 $\\{(x_i,y_i)\\}_{i=1}^n$ 上的经验期望为 $\\frac{1}{n}\\sum_{i=1}^n f(x_i,y_i)$。\n- 在特征独立的朴素贝叶斯假设下，类条件高斯模型形式为 $p(x \\mid y=k) = \\prod_{j=1}^d \\mathcal{N}(x_j \\mid \\mu_{k,j}, \\sigma_{k,j}^2)$，其中 $\\mu_{k,j}$ 和 $\\sigma_{k,j}^2$ 是类别 $k \\in \\{0,1\\}$ 的特征 $j$ 特定于该类的均值和方差，通过最大似然估计 (MLE) 进行估计。\n- 逻辑回归将 $p(y=1 \\mid x)$ 建模为 $\\sigma(w^\\top x + b)$，其中 $\\sigma(z) = \\frac{1}{1+\\exp(-z)}$ 是逻辑函数，$w \\in \\mathbb{R}^d$ 是权重向量，$b \\in \\mathbb{R}$ 是偏置，通过最大似然估计 (MLE) 进行估计，可选择性地使用 $\\ell_2$ 正则化（也称为岭惩罚）以提高数值稳定性。\n\n您的程序必须实现：\n- 特征 $j$ 的生成式特征分数：$s^{\\text{gen}}_j = \\mathbb{E}\\left[\\log p(x_j \\mid y)\\right]$，其中期望是在经验数据集上计算的，而 $p(x_j \\mid y)$ 是通过对仅限于类别 $y$ 的训练数据进行 MLE 学习得到参数的一维高斯分布 $\\mathcal{N}(x_j \\mid \\mu_{y,j}, \\sigma_{y,j}^2)$。为了数值稳定性，您可以将方差的下限设置为一个小的正值 $10^{-6}$。\n- 特征 $j$ 的判别式特征分数：$s^{\\text{disc}}_j = \\mathbb{E}\\left[\\left|\\frac{\\partial}{\\partial x_j}\\log p(y \\mid x)\\right|\\right]$，在通过 MLE 找到的拟合逻辑回归参数 $(w,b)$ 处进行评估（对 $w$ 使用大小为 $\\lambda = 0.1$ 的 $\\ell_2$ 惩罚以确保优化问题是良态的）。梯度是相对于输入特征 $x_j$ 计算的。\n\n对于这两种程序，通过分数降序选择前 $k$ 个特征，若分数相同，则优先选择较小的特征索引。报告每个测试用例所选的特征索引列表。\n\n数据生成：\n所有测试用例均使用固定的伪随机种子 $42$ 以确保可复现性。每个案例都是一个具有 $d=6$ 个特征的二元分类问题。除非另有说明，否则使用独立的高斯特征。对于类别 $y=0$ 和类别 $y=1$，每个特征的均值和方差在下面指定。对于每个案例，从类别 $y=0$ 的分布中抽取 $n_0$ 个样本，从类别 $y=1$ 的分布中抽取 $n_1$ 个样本，将它们连接起来形成数据集 $\\{(x_i,y_i)\\}_{i=1}^{n}$，其中 $n=n_0+n_1$，并按指定设置 $k$。不涉及角度，也没有物理单位。\n\n- 案例 1（平衡，两个信息特征）：\n  - $n_0 = 200$, $n_1 = 200$, $d = 6$, $k=2$。\n  - 类别 0：特征 0 的均值为 -2，方差为 1；特征 1 的均值为 2，方差为 1；特征 2,3,4,5 的均值为 0，方差为 3。\n  - 类别 1：特征 0 的均值为 2，方差为 1；特征 1 的均值为 -2，方差为 1；特征 2,3,4,5 的均值为 0，方差为 3。\n\n- 案例 2（平衡，一个低方差高信息量特征）：\n  - $n_0 = 300$, $n_1 = 300$, $d = 6$, $k=2$。\n  - 类别 0：特征 0 的均值为 -1，方差为 1；特征 2 的均值为 -1，方差为 0.2；特征 1,3,4,5 的均值为 0，方差为 2。\n  - 类别 1：特征 0 的均值为 1，方差为 1；特征 2 的均值为 1，方差为 0.2；特征 1,3,4,5 的均值为 0，方差为 2。\n\n- 案例 3（类别不平衡，两个信息特征）：\n  - $n_0 = 60$, $n_1 = 240$, $d = 6$, $k=2$。\n  - 类别 0：特征 0 的均值为 -1，方差为 1；特征 3 的均值为 -1，方差为 1；特征 1,2,4,5 的均值为 0，方差为 5。\n  - 类别 1：特征 0 的均值为 1，方差为 1；特征 3 的均值为 1，方差为 1；特征 1,2,4,5 的均值为 0，方差为 5。\n\n- 案例 4（平衡，两个冗余相关的信息特征）：\n  - $n_0 = 250$, $n_1 = 250$, $d = 6$, $k=2$。\n  - 为每个样本构建一个潜在标量 $z$，其中 $z \\sim \\mathcal{N}(\\mu_y, 1)$，$\\mu_0 = -2$ 且 $\\mu_1 = 2$。\n  - 设置特征 0 为 $x_0 = z + \\epsilon_0$，特征 1 为 $x_1 = z + \\epsilon_1$，其中 $\\epsilon_0 \\sim \\mathcal{N}(0, 0.1^2)$ 且 $\\epsilon_1 \\sim \\mathcal{N}(0, 0.1^2)$。\n  - 对于两个类别，特征 2,3,4,5 是独立的噪声，均值为 0，方差为 9。\n\n实现约束：\n- 使用最大似然估计 (MLE) 拟合生成模型的 $\\mu_{k,j}$ 和 $\\sigma_{k,j}^2$。\n- 通过最小化带大小为 $\\lambda = 0.1$ 的 $\\ell_2$ 惩罚的正则化负对数似然来拟合逻辑回归参数 $(w,b)$，使用数值稳定的优化器。为判别式拟合标准化特征，方法是减去在所有训练样本上计算的每个特征的全局均值，并除以其全局标准差（非类别特定）。\n- 对于生成式分数，将 $s^{\\text{gen}}_j$ 精确计算为 $\\log \\mathcal{N}(x_{i,j} \\mid \\mu_{y_i,j}, \\sigma_{y_i,j}^2)$ 的经验均值。\n- 对于判别式分数，将 $s^{\\text{disc}}_j$ 计算为在拟合的 $(w,b)$ 处 $\\left|\\frac{\\partial}{\\partial x_j}\\log p(y_i \\mid x_i)\\right|$ 的经验均值。\n\n测试套件和答案规范：\n- 为指定的四个案例实现上述过程，使用固定的种子 $42$。\n- 对每个案例，输出在生成式评分和判别式评分下排名前 $k$ 的特征索引。\n- 最终输出必须是单行，包含一个用方括号括起来的逗号分隔列表，其中每个元素对应一个案例，并且本身是一个双元素列表 $[G,D]$。这里 $G$ 是生成式选择的 $k$ 个整数的列表，$D$ 是判别式选择的 $k$ 个整数的列表。例如，包含两个案例的输出行看起来像这样：`[[[0,1],[0,1]],[[2,0],[2,0]]]`。打印输出中不得包含任何空白字符。", "solution": "此问题要求实现并比较两种不同的特征选择方法：一种是生成式方法，另一种是判别式方法。解决方案涉及为多个测试用例生成合成数据，应用两种选择算法，并报告排名最高的特征。\n\n### 1. 生成式特征选择\n\n生成式方法对特征的类条件概率分布 $p(x \\mid y)$ 进行建模。由此，可以使用贝叶斯法则推导出后验概率 $p(y \\mid x)$，这也需要对类先验概率 $p(y)$ 进行建模。指定的生成模型是具有高斯类条件分布的朴素贝叶斯分类器。该模型基于一个强假设，即特征在给定类别标签的条件下是相互独立的。\n\n对于给定的类别 $k \\in \\{0, 1\\}$，其分布为：\n$$p(x \\mid y=k) = \\prod_{j=1}^d p(x_j \\mid y=k) = \\prod_{j=1}^d \\mathcal{N}(x_j \\mid \\mu_{k,j}, \\sigma_{k,j}^2)$$\n其中 $x_j$ 是第 $j$ 个特征，$\\mathcal{N}(x \\mid \\mu, \\sigma^2)$ 是高斯分布的概率密度函数。\n\n**参数估计**：每个类别 $k$ 和特征 $j$ 的参数 $(\\mu_{k,j}, \\sigma_{k,j}^2)$ 使用最大似然估计 (MLE) 从训练数据中估计。对于属于类别 $k$ 的一组样本 $\\{x_{i,j}\\}_{i \\mid y_i=k}$，MLE 估计值是样本均值和样本方差：\n$$\\hat{\\mu}_{k,j} = \\frac{1}{n_k} \\sum_{i: y_i=k} x_{i,j}$$\n$$\\hat{\\sigma}_{k,j}^2 = \\frac{1}{n_k} \\sum_{i: y_i=k} (x_{i,j} - \\hat{\\mu}_{k,j})^2$$\n其中 $n_k$ 是类别 $k$ 中的样本数。为了数值稳定性，估计的方差下限设置为一个小的正值 $\\epsilon = 10^{-6}$。\n\n**特征评分**：特征 $j$ 的分数，表示为 $s^{\\text{gen}}_j$，定义为该特征的类条件对数似然的经验期望：\n$$s^{\\text{gen}}_j = \\mathbb{E}\\left[\\log p(x_j \\mid y)\\right] = \\frac{1}{n} \\sum_{i=1}^n \\log p(x_{i,j} \\mid y_i)$$\n此分数是使用每个样本 $(x_i, y_i)$ 的估计参数 $(\\hat{\\mu}_{y_i,j}, \\hat{\\sigma}_{y_i,j}^2)$ 计算的。对于单个观测值 $x_{i,j}$，给定其类别 $y_i$ 的对数似然为：\n$$\\log \\mathcal{N}(x_{i,j} \\mid \\hat{\\mu}_{y_i,j}, \\hat{\\sigma}_{y_i,j}^2) = -\\frac{1}{2} \\log(2\\pi\\hat{\\sigma}_{y_i,j}^2) - \\frac{(x_{i,j} - \\hat{\\mu}_{y_i,j})^2}{2\\hat{\\sigma}_{y_i,j}^2}$$\n此分数衡量了特征 $j$ 的数据在平均程度上与所学的类条件高斯模型的符合程度。产生更高平均对数似然的特征被认为更具信息量。\n\n### 2. 判别式特征选择\n\n判别式方法直接对后验概率 $p(y \\mid x)$ 进行建模，从而绕过了对特征分布 $p(x \\mid y)$ 进行建模的需要。指定的模型是逻辑回归。\n\n逻辑回归模型将类别 1 的概率定义为：\n$$p(y=1 \\mid x) = \\sigma(w^\\top x' + b) = \\frac{1}{1 + \\exp(-(w^\\top x' + b))}$$\n其中 $w \\in \\mathbb{R}^d$ 是权重向量，$b \\in \\mathbb{R}$ 是偏置项，$\\sigma(\\cdot)$ 是 logistic sigmoid 函数。输入向量 $x'$ 表示经过标准化（减去全局均值并除以所有数据中每个特征的全局标准差）后的特征。\n\n**参数估计**：参数 $(w, b)$ 是通过最小化正则化负对数似然（也称为带 $\\ell_2$ 惩罚的交叉熵损失）来找到的。要最小化的目标函数是：\n$$L(w,b) = -\\sum_{i=1}^n \\left[ y_i \\log p(y_i=1 \\mid x_i) + (1-y_i) \\log p(y_i=0 \\mid x_i) \\right] + \\frac{\\lambda}{2} \\|w\\|_2^2$$\n其中 $\\lambda=0.1$ 是正则化强度。这是一个凸优化问题，可以使用 L-BFGS 等数值方法高效求解。\n\n**特征评分**：特征 $j$ 的分数 $s^{\\text{disc}}_j$ 是对数条件概率相对于原始（未标准化）特征 $x_j$ 的梯度绝对值的经验平均值：\n$$s^{\\text{disc}}_j = \\mathbb{E}\\left[\\left|\\frac{\\partial}{\\partial x_j}\\log p(y \\mid x)\\right|\\right] = \\frac{1}{n} \\sum_{i=1}^n \\left|\\frac{\\partial}{\\partial x_j}\\log p(y_i \\mid x_i)\\right|$$\n我们使用链式法则推导梯度。令 $z_i = w^\\top x'_i + b$。单个样本的对数似然为 $l_i = \\log p(y_i | x_i)$。相对于线性激活值 $z_i$ 的梯度是 $\\frac{\\partial l_i}{\\partial z_i} = y_i - \\sigma(z_i)$。由于 $x'_{i,j} = (x_{i,j} - \\mu^{\\text{glob}}_j) / \\sigma^{\\text{glob}}_j$，我们有 $\\frac{\\partial z_i}{\\partial x_j} = \\frac{w_j}{\\sigma^{\\text{glob}}_j}$。因此，相对于原始特征的梯度是：\n$$\\frac{\\partial l_i}{\\partial x_j} = \\frac{\\partial l_i}{\\partial z_i} \\frac{\\partial z_i}{\\partial x_j} = (y_i - \\sigma(z_i)) \\frac{w_j}{\\sigma^{\\text{glob}}_j}$$\n特征 $j$ 的分数变为：\n$$s^{\\text{disc}}_j = \\frac{1}{n} \\sum_{i=1}^n \\left| (y_i - \\sigma(z_i)) \\frac{w_j}{\\sigma^{\\text{glob}}_j} \\right| = \\left|\\frac{w_j}{\\sigma^{\\text{glob}}_j}\\right| \\left(\\frac{1}{n} \\sum_{i=1}^n |y_i - \\sigma(z_i)|\\right)$$\n由于项 $\\frac{1}{n} \\sum_i |y_i - \\sigma(z_i)|$ 对所有特征 $j$ 都是常数，因此特征的排序完全由标准化特征所学的权重 $w_j$ 的绝对值除以原始特征的标准差 $\\sigma^{\\text{glob}}_j$ 决定。此度量量化了每个特征对模型预测的影响。\n\n### 3. 特征排序与选择\n\n对于这两种方法，在计算完所有特征 $j=1, \\dots, d$ 的分数 $\\{s_j\\}$ 后，特征按其分数的降序排列。为确保排序的确定性，平局通过优先选择索引较小的特征来打破。然后从这个排序列表中选择前 $k$ 个特征。在计算上，这是通过对键 $(\\text{-score}, \\text{index})$ 进行字典序排序来实现的。", "answer": "```python\nimport numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.special import expit, xlogy\n\ndef generate_data(case_params, rng):\n    \"\"\"为给定的测试用例生成合成数据。\"\"\"\n    d = case_params['d']\n    n0, n1 = case_params['n0'], case_params['n1']\n    n = n0 + n1\n    X = np.zeros((n, d))\n    y = np.concatenate([np.zeros(n0), np.ones(n1)])\n\n    if case_params['id'] in [1, 2, 3]:\n        means0, var0 = case_params['class0_params']\n        means1, var1 = case_params['class1_params']\n        std0, std1 = np.sqrt(var0), np.sqrt(var1)\n        \n        X[:n0, :] = rng.normal(loc=means0, scale=std0, size=(n0, d))\n        X[n0:, :] = rng.normal(loc=means1, scale=std1, size=(n1, d))\n    \n    elif case_params['id'] == 4:\n        mu0, mu1 = -2, 2\n        \n        # 生成潜在变量 z\n        z0 = rng.normal(loc=mu0, scale=1, size=n0)\n        z1 = rng.normal(loc=mu1, scale=1, size=n1)\n        z = np.concatenate([z0, z1])\n        \n        # 从 z 生成相关的特征 x0, x1\n        epsilon0 = rng.normal(loc=0, scale=0.1, size=n)\n        epsilon1 = rng.normal(loc=0, scale=0.1, size=n)\n        X[:, 0] = z + epsilon0\n        X[:, 1] = z + epsilon1\n        \n        # 生成独立的噪声特征\n        X[:, 2:] = rng.normal(loc=0, scale=3, size=(n, d - 2))\n        \n    return X, y\n\ndef get_top_k_indices(scores, k):\n    \"\"\"根据分数选择前 k 个特征索引，并处理平分情况。\"\"\"\n    indices = np.arange(len(scores))\n    # 按分数（降序）排序，然后按索引（升序）排序以打破平局\n    sorted_indices = np.lexsort((indices, -scores))\n    return sorted_indices[:k]\n    \ndef generative_selector(X, y, k):\n    \"\"\"计算生成式特征分数并选择前 k 个。\"\"\"\n    n, d = X.shape\n    \n    X0 = X[y == 0]\n    X1 = X[y == 1]\n    n0, n1 = len(X0), len(X1)\n\n    # 每个类别的参数的 MLE\n    mu0 = np.mean(X0, axis=0) if n0 > 0 else np.zeros(d)\n    var0 = np.var(X0, axis=0) if n0 > 1 else np.ones(d)\n    mu1 = np.mean(X1, axis=0) if n1 > 0 else np.zeros(d)\n    var1 = np.var(X1, axis=0) if n1 > 1 else np.ones(d)\n    \n    # 应用方差下限\n    var0 = np.maximum(var0, 1e-6)\n    var1 = np.maximum(var1, 1e-6)\n    \n    mus = np.array([mu0, mu1])\n    variances = np.array([var0, var1])\n    \n    # 计算对数似然的经验期望\n    log_likelihoods = np.zeros(d)\n    for i in range(n):\n        yi = int(y[i])\n        xi = X[i, :]\n        mu_yi = mus[yi, :]\n        var_yi = variances[yi, :]\n        \n        log_pdf_i = -0.5 * np.log(2 * np.pi * var_yi) - ((xi - mu_yi)**2) / (2 * var_yi)\n        log_likelihoods += log_pdf_i\n        \n    scores = log_likelihoods / n\n\n    return get_top_k_indices(scores, k)\n\ndef discriminative_selector(X, y, k, lambda_reg):\n    \"\"\"计算判别式特征分数并选择前 k 个。\"\"\"\n    n, d = X.shape\n    \n    # 标准化特征（全局）\n    mu_glob = np.mean(X, axis=0)\n    std_glob = np.std(X, axis=0)\n    std_glob[std_glob == 0] = 1.0  # 避免除以零\n    \n    X_std = (X - mu_glob) / std_glob\n    \n    # 逻辑回归的目标函数\n    def objective_function(params, X_s, y_l, lambda_r):\n        w = params[:-1]\n        b = params[-1]\n        z = X_s @ w + b\n        y_hat = expit(z)\n        \n        # 正则化负对数似然（成本）\n        nll = -np.sum(xlogy(y_l, y_hat) + xlogy(1 - y_l, 1 - y_hat))\n        l2_penalty = 0.5 * lambda_r * np.sum(w**2)\n        cost = nll + l2_penalty\n        \n        # 梯度\n        error = y_hat - y_l\n        grad_w = X_s.T @ error + lambda_r * w\n        grad_b = np.sum(error)\n        grad = np.concatenate((grad_w, [grad_b]))\n        return cost, grad\n\n    initial_params = np.zeros(d + 1)\n    res = minimize(\n        objective_function,\n        initial_params,\n        args=(X_std, y, lambda_reg),\n        jac=True,\n        method='L-BFGS-B'\n    )\n    \n    w_opt = res.x[:-1]\n    \n    # 特征分数与 |w_j / std_j| 成正比\n    scores = np.abs(w_opt / std_glob)\n    \n    return get_top_k_indices(scores, k)\n\ndef solve():\n    \"\"\"运行所有测试用例并打印结果的主函数。\"\"\"\n    test_cases = [\n        {\n            'id': 1, 'n0': 200, 'n1': 200, 'd': 6, 'k': 2,\n            'class0_params': (np.array([-2, 2, 0, 0, 0, 0]), np.array([1, 1, 3, 3, 3, 3])),\n            'class1_params': (np.array([2, -2, 0, 0, 0, 0]), np.array([1, 1, 3, 3, 3, 3]))\n        },\n        {\n            'id': 2, 'n0': 300, 'n1': 300, 'd': 6, 'k': 2,\n            'class0_params': (np.array([-1, 0, -1, 0, 0, 0]), np.array([1, 2, 0.2, 2, 2, 2])),\n            'class1_params': (np.array([1, 0, 1, 0, 0, 0]), np.array([1, 2, 0.2, 2, 2, 2]))\n        },\n        {\n            'id': 3, 'n0': 60, 'n1': 240, 'd': 6, 'k': 2,\n            'class0_params': (np.array([-1, 0, 0, -1, 0, 0]), np.array([1, 5, 5, 1, 5, 5])),\n            'class1_params': (np.array([1, 0, 0, 1, 0, 0]), np.array([1, 5, 5, 1, 5, 5]))\n        },\n        {\n            'id': 4, 'n0': 250, 'n1': 250, 'd': 6, 'k': 2\n            # 特殊的生成逻辑在 generate_data 函数内部处理\n        }\n    ]\n    \n    rng = np.random.default_rng(42)\n    lambda_reg = 0.1\n    \n    results_str_list = []\n    \n    for case in test_cases:\n        X, y = generate_data(case, rng)\n        k = case['k']\n        \n        gen_indices = generative_selector(X, y, k)\n        disc_indices = discriminative_selector(X, y, k, lambda_reg)\n        \n        gen_indices_str = f\"[{','.join(map(str, gen_indices))}]\"\n        disc_indices_str = f\"[{','.join(map(str, disc_indices))}]\"\n        \n        results_str_list.append(f\"[{gen_indices_str},{disc_indices_str}]\")\n        \n    print(f\"[{','.join(results_str_list)}]\")\n\nsolve()\n```", "id": "3124940"}]}