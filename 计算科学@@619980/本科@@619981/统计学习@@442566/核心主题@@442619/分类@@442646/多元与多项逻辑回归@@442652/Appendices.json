{"hands_on_practices": [{"introduction": "当从二元分类扩展到多类别问题时，一个直观的想法是为每个类别训练一个独立的“一对多”（One-vs-Rest, OvR）逻辑回归分类器。然而，这种方法可能导致概率不一致的问题。本练习将通过一个具体的计算例子，揭示“一对多”策略的内在缺陷，并展示为何使用Softmax函数的多项逻辑回归能够提供一个数学上更严谨、结果更一致的解决方案。[@problem_id:3151582]", "problem": "本题要求您构建并分析一个具体的多类分类场景，以突显多个二元逻辑回归的“一对多”（OvR）方案与使用 softmax 链接函数的单个多项逻辑回归之间的差异。您的任务是实现伯努利分布和分类分布的规范逆链接函数所隐含的概率映射，将它们应用于指定的线性模型，并量化 OvR 方案如何产生与归一化概率不一致的重叠决策区域，而 softmax 模型则强制执行归一化。\n\n使用的基本定义：\n- 对于伯努利分布，使用规范逆链接函数，它将线性预测器映射到 $(0,1)$ 范围内的有效概率。\n- 对于具有 $K$ 个类的分类分布，使用由指数族表示法导出的规范逆链接函数，该函数产生满足 $\\sum_{k=1}^K p_k=1$ 的归一化概率向量 $(p_1,\\dots,p_K)$。\n\n设置：\n- 考虑 $K=3$ 个类和特征向量 $x \\in \\mathbb{R}^2$，并增加一个截距项，即使用 $\\bar{x} = \\left(1,x_1,x_2\\right)^\\top$。\n- 为“一对多”模型（每个模型都是针对其余类的二元逻辑回归）定义三个特定于类的权重向量，作为矩阵 $W_{\\text{ovr}} \\in \\mathbb{R}^{3 \\times 3}$ 的行：\n  $$\n  W_{\\text{ovr}} =\n  \\begin{bmatrix}\n  1.0  1.5  -0.1 \\\\\n  1.0  -0.1  1.5 \\\\\n  0.6  0.7  0.7\n  \\end{bmatrix}.\n  $$\n  对于类 $k \\in \\{1,2,3\\}$，OvR 线性预测器为 $z_k^{\\text{ovr}} = w_k^\\top \\bar{x}$，其中 $w_k^\\top$ 是 $W_{\\text{ovr}}$ 的第 $k$ 行。\n- 定义一个具有相同线性得分的多项（softmax）模型，使用一个与 $W_{\\text{ovr}}$ 相等的权重矩阵 $W_{\\text{soft}} \\in \\mathbb{R}^{3 \\times 3}$：\n  $$\n  W_{\\text{soft}} = W_{\\text{ovr}}.\n  $$\n  对于类 $k \\in \\{1,2,3\\}$，softmax 线性得分为 $s_k = \\beta_k^\\top \\bar{x}$，其中 $\\beta_k^\\top$ 是 $W_{\\text{soft}}$ 的第 $k$ 行。\n\n需对每个测试输入 $\\bar{x}$ 执行的计算：\n1. 使用应用于 $(z_1^{\\text{ovr}},z_2^{\\text{ovr}},z_3^{\\text{ovr}})$ 的伯努利规范逆链接函数计算 OvR 类概率 $(p_1^{\\text{ovr}},p_2^{\\text{ovr}},p_3^{\\text{ovr}})$。令 $S_{\\text{ovr}}=\\sum_{k=1}^3 p_k^{\\text{ovr}}$。同时计算 $N_{0.5}$（$p_k^{\\text{ovr}} \\ge 0.5$ 的 OvR 类的数量），以及一个重叠指示器 $I_{\\text{ovr}}$，如果至少有两个类同时满足 $p_k^{\\text{ovr}} \\ge 0.5$，则该指示器为 $1$，否则为 $0$。\n2. 使用应用于 $(s_1,s_2,s_3)$ 的分类规范逆链接函数计算 softmax 概率 $(p_1^{\\text{soft}},p_2^{\\text{soft}},p_3^{\\text{soft}})$，并令 $S_{\\text{soft}}=\\sum_{k=1}^3 p_k^{\\text{soft}}$。\n\n测试套件：\n- 使用以下五个特征向量（每个 $x$ 以 $[x_1,x_2]$ 形式给出），选择这些向量是为了测试一般重叠情况、中度负值区域、混合符号强证据以及两个极端边缘情况：\n  - $x^{(1)} = [0.4, 0.4]$\n  - $x^{(2)} = [-1.0, -1.0]$\n  - $x^{(3)} = [3.0, -3.0]$\n  - $x^{(4)} = [-5.0, -5.0]$\n  - $x^{(5)} = [10.0, 10.0]$\n- 对于每个 $x^{(i)}$，构建 $\\bar{x}^{(i)} = \\left(1,x_1^{(i)},x_2^{(i)}\\right)^\\top$ 并评估上述量值。\n\n数值稳定性：\n- 使用伯努利和分类规范逆链接函数的数值稳定实现，以处理大幅值线性预测器而不会出现上溢。\n\n要求的最终输出：\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，该列表的第 $i$ 个元素本身也是一个列表\n  $$\n  \\left[\\,\\text{round}(S_{\\text{ovr}},6),\\,\\text{round}(S_{\\text{soft}},6),\\,N_{0.5},\\,\\mathbf{1}\\{I_{\\text{ovr}}=1\\}\\,\\right],\n  $$\n  其中 $S_{\\text{ovr}}$ 和 $S_{\\text{soft}}$ 四舍五入到 6 位小数，$N_{0.5}$ 是一个整数，而 $\\mathbf{1}\\{I_{\\text{ovr}}=1\\}$ 是一个表示为 True 或 False 的布尔指示器。这五个元素必须按照上述测试套件的顺序报告。例如，输出格式必须如下所示：\n  $$\n  [[...],[...],[...],[...],[...]]\n  $$\n  不含空格。", "solution": "The user's request is a valid computational problem.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n\n-   **Classes**: $K=3$ classes.\n-   **Feature Vectors**: Input vectors $x \\in \\mathbb{R}^2$ are augmented with an intercept term to form $\\bar{x} = (1, x_1, x_2)^\\top$.\n-   **One-vs-Rest (OVR) Model**:\n    -   Weight matrix:\n      $$\n      W_{\\text{ovr}} =\n      \\begin{bmatrix}\n      1.0  1.5  -0.1 \\\\\n      1.0  -0.1  1.5 \\\\\n      0.6  0.7  0.7\n      \\end{bmatrix}.\n      $$\n    -   Linear predictor for class $k \\in \\{1, 2, 3\\}$: $z_k^{\\text{ovr}} = w_k^\\top \\bar{x}$, where $w_k^\\top$ is the $k$-th row of $W_{\\text{ovr}}$.\n-   **Multinomial (Softmax) Model**:\n    -   Weight matrix: $W_{\\text{soft}} = W_{\\text{ovr}}$.\n    -   Linear score for class $k \\in \\{1, 2, 3\\}$: $s_k = \\beta_k^\\top \\bar{x}$, where $\\beta_k^\\top$ is the $k$-th row of $W_{\\text{soft}}$.\n-   **Inverse Link Functions**:\n    -   **Bernoulli**: The canonical inverse link mapping a linear predictor to a probability in $(0, 1)$.\n    -   **Categorical**: The canonical inverse link producing a normalized probability vector $(p_1, \\dots, p_K)$ such that $\\sum_{k=1}^K p_k = 1$.\n-   **Computations for each $\\bar{x}$**:\n    1.  **OVR**: Compute probabilities $(p_1^{\\text{ovr}}, p_2^{\\text{ovr}}, p_3^{\\text{ovr}})$, their sum $S_{\\text{ovr}} = \\sum_{k=1}^3 p_k^{\\text{ovr}}$, the count $N_{0.5}$ of classes where $p_k^{\\text{ovr}} \\ge 0.5$, and an overlap indicator $I_{\\text{ovr}}$ (equal to $1$ if $N_{0.5} \\ge 2$, else $0$).\n    2.  **Softmax**: Compute probabilities $(p_1^{\\text{soft}}, p_2^{\\text{soft}}, p_3^{\\text{soft}})$ and their sum $S_{\\text{soft}} = \\sum_{k=1}^3 p_k^{\\text{soft}}$.\n-   **Test Suite**: A set of five feature vectors $x^{(i)} = [x_1^{(i)}, x_2^{(i)}]$:\n    -   $x^{(1)} = [0.4, 0.4]$\n    -   $x^{(2)} = [-1.0, -1.0]$\n    -   $x^{(3)} = [3.0, -3.0]$\n    -   $x^{(4)} = [-5.0, -5.0]$\n    -   $x^{(5)} = [10.0, 10.0]$\n-   **Output Format**: For each test vector, produce a list $[\\text{round}(S_{\\text{ovr}}, 6), \\text{round}(S_{\\text{soft}}, 6), N_{0.5}, \\mathbf{1}\\{I_{\\text{ovr}}=1\\}]$, where the last element is a boolean (`True` or `False`).\n\n**Step 2: Validate Using Extracted Givens**\n\n-   **Scientifically Grounded**: The problem is well-grounded in the theory of generalized linear models (GLMs). The specified canonical inverse link for a Bernoulli distribution is the logistic (sigmoid) function, and for a categorical distribution, it is the softmax function. The comparison of one-vs-rest and multinomial logistic regression is a standard and important topic in statistical learning.\n-   **Well-Posed**: The problem is well-posed. All parameters, data, and required computations are explicitly defined, leading to a unique, stable, and meaningful numerical solution.\n-   **Objective**: The problem is stated objectively using precise mathematical notation and terminology, without any subjective or ambiguous language.\n-   **Conclusion**: The problem is free of scientific unsoundness, incompleteness, contradictions, and other flaws listed in the validation criteria.\n\n**Step 3: Verdict and Action**\n\n-   **Verdict**: The problem is **valid**.\n-   **Action**: A complete solution will be provided.\n\n### Solution Derivation\n\nThis problem requires a comparison of two common multiclass classification strategies: a one-vs-rest (OVR) scheme using multiple independent binary classifiers and a joint multinomial (softmax) regression model. The core of the task is to apply the correct probability mapping functions to the linear scores produced by the given weight matrices.\n\n**1. Canonical Inverse Link Functions**\n\n-   **Bernoulli/OVR**: The canonical inverse link for the Bernoulli distribution is the logistic function, often called the sigmoid function, $\\sigma(z)$. It maps a linear predictor $z \\in \\mathbb{R}$ to a probability in $(0, 1)$.\n    $$\n    p = \\sigma(z) = \\frac{1}{1 + e^{-z}}\n    $$\n    In the OVR scheme, this function is applied independently to each class-specific linear predictor $z_k^{\\text{ovr}}$.\n\n-   **Categorical/Softmax**: The canonical inverse link for the categorical distribution is the softmax function. It maps a vector of linear scores $s = (s_1, \\dots, s_K)^\\top$ to a normalized probability vector $p = (p_1, \\dots, p_K)^\\top$ where each $p_k \\in (0, 1)$ and $\\sum_{k=1}^K p_k = 1$.\n    $$\n    p_k = \\frac{e^{s_k}}{\\sum_{j=1}^K e^{s_j}}\n    $$\n    The softmax function ensures that the resulting probabilities form a valid distribution over the $K$ classes.\n\n**2. Model Computations**\n\nFor each input feature vector $x^{(i)} = [x_1^{(i)}, x_2^{(i)}]^\\top$, we first augment it to $\\bar{x}^{(i)} = [1, x_1^{(i)}, x_2^{(i)}]^\\top$. The linear scores for both models are computed using the same weight matrix $W = W_{\\text{ovr}} = W_{\\text{soft}}$. The vector of scores/predictors is given by $z^{(i)} = W\\bar{x}^{(i)}$.\n\nLet's denote the score vector elements as $z_1, z_2, z_3$.\n\n-   **OVR Probabilities**: The probabilities for the OVR model are calculated by applying the sigmoid function independently to each score:\n    $$\n    p_k^{\\text{ovr}} = \\sigma(z_k) = \\frac{1}{1 + e^{-z_k}} \\quad \\text{for } k \\in \\{1, 2, 3\\}\n    $$\n    The sum $S_{\\text{ovr}} = \\sum_{k=1}^3 p_k^{\\text{ovr}}$ is generally not equal to $1$. We then count $N_{0.5} = \\sum_{k=1}^3 \\mathbf{1}\\{p_k^{\\text{ovr}} \\ge 0.5\\}$ and set the overlap indicator $I_{\\text{ovr}}$ to be true if $N_{0.5} \\ge 2$.\n\n-   **Softmax Probabilities**: The probabilities for the softmax model are calculated jointly:\n    $$\n    p_k^{\\text{soft}} = \\frac{e^{z_k}}{e^{z_1} + e^{z_2} + e^{z_3}} \\quad \\text{for } k \\in \\{1, 2, 3\\}\n    $$\n    By construction, the sum $S_{\\text{soft}} = \\sum_{k=1}^3 p_k^{\\text{soft}}$ will always be equal to $1$.\n\n**3. Numerical Stability**\n\nTo prevent numerical overflow/underflow with large magnitude inputs to the exponential function, stable implementations are required.\n\n-   **Stable Sigmoid $\\sigma(z)$**:\n    -   If $z \\ge 0$, use $1 / (1 + e^{-z})$. This avoids $e$ raised to a large positive power.\n    -   If $z  0$, use an algebraically equivalent form $e^z / (1 + e^z)$. This again avoids $e$ raised to a large positive power.\n\n-   **Stable Softmax**: The \"log-sum-exp\" trick is used. Let $c = \\max_k(z_k)$. The softmax probabilities can be rewritten as:\n    $$\n    p_k^{\\text{soft}} = \\frac{e^{z_k - c}}{\\sum_{j=1}^K e^{z_j - c}}\n    $$\n    This transformation centers the exponents around $0$, preventing overflow from large positive scores while maintaining numerical precision.\n\n**4. Walkthrough for Test Case $x^{(1)} = [0.4, 0.4]$**\n\n1.  **Augment vector**: $\\bar{x}^{(1)} = [1, 0.4, 0.4]^\\top$.\n2.  **Compute linear scores**:\n    $$\n    z = W \\bar{x}^{(1)} =\n    \\begin{bmatrix}\n    1.0  1.5  -0.1 \\\\\n    1.0  -0.1  1.5 \\\\\n    0.6  0.7  0.7\n    \\end{bmatrix}\n    \\begin{bmatrix}\n    1.0 \\\\\n    0.4 \\\\\n    0.4\n    \\end{bmatrix}\n    =\n    \\begin{bmatrix}\n    1.0 + 0.6 - 0.04 \\\\\n    1.0 - 0.04 + 0.6 \\\\\n    0.6 + 0.28 + 0.28\n    \\end{bmatrix}\n    =\n    \\begin{bmatrix}\n    1.56 \\\\\n    1.56 \\\\\n    1.16\n    \\end{bmatrix}\n    $$\n3.  **OVR Calculation**:\n    -   $p_1^{\\text{ovr}} = \\sigma(1.56) \\approx 0.826384$\n    -   $p_2^{\\text{ovr}} = \\sigma(1.56) \\approx 0.826384$\n    -   $p_3^{\\text{ovr}} = \\sigma(1.16) \\approx 0.761352$\n    -   $S_{\\text{ovr}} = 0.826384 + 0.826384 + 0.761352 = 2.414120$\n    -   All three probabilities are $\\ge 0.5$, so $N_{0.5} = 3$.\n    -   Since $N_{0.5} \\ge 2$, the overlap indicator is `True`.\n\n4.  **Softmax Calculation**:\n    -   Using scores $z_1=1.56, z_2=1.56, z_3=1.16$.\n    -   $e^{1.56} \\approx 4.7588$, $e^{1.56} \\approx 4.7588$, $e^{1.16} \\approx 3.1899$.\n    -   Sum of exponentials $\\approx 12.7075$.\n    -   $p_1^{\\text{soft}} = 4.7588 / 12.7075 \\approx 0.374468$\n    -   $p_2^{\\text{soft}} = 4.7588 / 12.7075 \\approx 0.374468$\n    -   $p_3^{\\text{soft}} = 3.1899 / 12.7075 \\approx 0.251064$\n    -   $S_{\\text{soft}} = 0.374468 + 0.374468 + 0.251064 = 1.000000$.\n\n5.  **Result for $x^{(1)}$**:\n    $[\\text{round}(2.414120, 6), \\text{round}(1.0, 6), 3, \\text{True}] \\rightarrow [2.41412, 1.0, 3, \\text{True}]$.\n\nThis process is repeated for all five test vectors to generate the final output. The OVR scheme yields unnormalized probabilities that sum to $2.41$ and classifies the point into all three classes (since all $p_k>0.5$), highlighting its ambiguity. The softmax model provides a valid, normalized probability distribution, assigning the highest (but not overwhelming) probability to classes $1$ and $2$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes and compares probabilities from one-vs-rest (OVR) and softmax\n    models for a series of test cases.\n    \"\"\"\n\n    # Define the weight matrix for both OVR and Softmax models.\n    W = np.array([\n        [1.0, 1.5, -0.1],\n        [1.0, -0.1, 1.5],\n        [0.6, 0.7, 0.7]\n    ])\n\n    # Define the test suite of feature vectors.\n    test_cases_x = [\n        np.array([0.4, 0.4]),\n        np.array([-1.0, -1.0]),\n        np.array([3.0, -3.0]),\n        np.array([-5.0, -5.0]),\n        np.array([10.0, 10.0])\n    ]\n\n    def stable_sigmoid(z):\n        \"\"\"Numerically stable implementation of the sigmoid function.\"\"\"\n        # Condition to avoid overflow in exp for negative z\n        # and to maintain precision for positive z.\n        # np.where(condition, x, y) is used for element-wise conditional evaluation.\n        return np.where(\n            z >= 0,\n            1.0 / (1.0 + np.exp(-z)),\n            np.exp(z) / (1.0 + np.exp(z))\n        )\n\n    def stable_softmax(s):\n        \"\"\"Numerically stable implementation of the softmax function.\"\"\"\n        # Subtract the max for numerical stability (log-sum-exp trick).\n        s_max = np.max(s)\n        s_shifted = s - s_max\n        exps = np.exp(s_shifted)\n        return exps / np.sum(exps)\n\n    results_for_print = []\n\n    for x in test_cases_x:\n        # Augment the feature vector with an intercept term (x_0 = 1).\n        x_aug = np.insert(x, 0, 1.0)\n\n        # Compute the linear scores/predictors z = W * x_aug.\n        # Since W is (3, 3) and x_aug is (3,), the result is a (3,) vector.\n        z = W @ x_aug\n\n        # 1. One-vs-Rest (OVR) computations\n        p_ovr = stable_sigmoid(z)\n        S_ovr = np.sum(p_ovr)\n        \n        # Count classes where probability is >= 0.5\n        N_05 = np.sum(p_ovr >= 0.5)\n        \n        # Overlap indicator: True if at least two classes meet the threshold\n        I_ovr = (N_05 >= 2)\n\n        # 2. Softmax computations\n        p_soft = stable_softmax(z)\n        S_soft = np.sum(p_soft) # This will be 1.0 by definition.\n\n        # Format the sub-result as a string to avoid spaces when printing\n        # the final list of lists.\n        # Rounding S_ovr and S_soft to 6 decimal places as required.\n        s_ovr_rounded = f\"{S_ovr:.6f}\"\n        s_soft_rounded = f\"{S_soft:.6f}\"\n        \n        sub_result_str = f\"[{s_ovr_rounded},{s_soft_rounded},{N_05},{I_ovr}]\"\n        results_for_print.append(sub_result_str)\n\n    # Final print statement in the exact required format.\n    # Joining the string-formatted sub-results with a comma.\n    print(f\"[{','.join(results_for_print)}]\")\n\nsolve()\n\n```", "id": "3151582"}, {"introduction": "在理论上理解Softmax函数的优雅之后，我们面临着在计算机上实现它的实际挑战。直接根据公式 $\\frac{\\exp(s_k)}{\\sum_j \\exp(s_j)}$ 进行计算，当线性得分 $s_k$ 的数值很大或很小时，很容易遭遇浮点数的上溢或下溢问题，导致计算失败。本练习将引导你实现并对比两种计算方法，从而掌握在科学计算中至关重要的“log-sum-exp”技巧，确保模型实现的鲁棒性。[@problem_id:3151616]", "problem": "您将处理一个计算任务，该任务源于多项逻辑回归模型的归一化步骤。对于一个特征向量 $x \\in \\mathbb{R}^p$ 和 $K$ 个类别特定的参数向量 $\\beta_j \\in \\mathbb{R}^p$，线性预测变量为 $\\eta_j = x^\\top \\beta_j$，其中 $j \\in \\{1,\\dots,K\\}$。softmax 函数中的归一化常数需要计算 $\\log \\sum_{j=1}^K \\exp(\\eta_j)$。这个计算在实数算术中是明确定义的，但在有限精度的浮点算术中，当 $\\lVert \\eta \\rVert$ 很大时，可能会出现数值不稳定的情况。\n\n从多项逻辑回归预测变量以及对数和指数函数的基本定义出发，请为量 $\\log \\sum_{j=1}^K \\exp(\\eta_j)$ 实现两种计算方法：\n- 一种直接方法，即先应用指数和求和运算，然后应用对数运算，不进行任何特殊的数值范围处理。\n- 一种数值稳定的方法，通过一个在精确算术中保持等价性的、有数学依据的变换来计算相同的值，同时避免上溢或下溢。\n\n然后，对下面提供的每个测试用例，执行以下操作：\n1. 使用 $\\eta_j = x^\\top \\beta_j$ 计算向量 $\\eta \\in \\mathbb{R}^K$。\n2. 计算欧几里得范数 $\\lVert \\eta \\rVert_2$。\n3. 计算 $\\log \\sum_{j=1}^K \\exp(\\eta_j)$ 的直接结果和数值稳定结果。\n4. 使用一个可靠的参考实现来近似 $\\log \\sum_{j=1}^K \\exp(\\eta_j)$ 的数学正确值。\n5. 对每种方法，判断其结果是否为有限数。当结果为有限数时，计算其相对于参考值的绝对误差。如果结果不是有限数，则将误差报告为 $+\\infty$。\n\n您的程序必须使用以下测试套件。每个测试定义了 $p$、$K$、一个特征向量 $x \\in \\mathbb{R}^p$ 和一个目标向量 $t \\in \\mathbb{R}^K$。通过设置以下公式来构建参数 $\\beta_j \\in \\mathbb{R}^p$，使得对于每个 $j$ 都有 $x^\\top \\beta_j = t_j$：\n$$\n\\beta_j \\;=\\; \\frac{t_j}{x^\\top x}\\, x \\quad \\text{对于每个 } j \\in \\{1,\\dots,K\\}.\n$$\n此构造保证了在精确算术中 $x^\\top \\beta_j = t_j$。\n\n测试套件：\n- 案例 A（中等值，正常路径）：\n  - $p = 3$, $K = 3$\n  - $x = [0.5,\\,-1.0,\\,2.0]$\n  - $t = [-1.2,\\,0.3,\\,2.1]$\n- 案例 B（非常大的正值，直接方法容易上溢）：\n  - $p = 3$, $K = 3$\n  - $x = [1.0,\\,1.0,\\,1.0]$\n  - $t = [1000.0,\\,1001.0,\\,999.0]$\n- 案例 C（非常大的负值，直接方法容易下溢）：\n  - $p = 3$, $K = 3$\n  - $x = [1.0,\\,1.0,\\,1.0]$\n  - $t = [-1000.0,\\,-1001.0,\\,-999.0]$\n- 案例 D（混合极端值，具有挑战性的动态范围）：\n  - $p = 3$, $K = 3$\n  - $x = [1.0,\\,1.0,\\,1.0]$\n  - $t = [-1000.0,\\,0.0,\\,1000.0]$\n\n对于每个案例，按顺序输出一个包含五个值的序列：\n- $\\lVert \\eta \\rVert_2$，以浮点数形式表示，\n- 一个布尔值，指示直接结果是否为有限数，\n- 一个布尔值，指示稳定结果是否为有限数，\n- 直接结果相对于参考值的绝对误差，以浮点数形式表示（如果不是有限数，则使用 $+\\infty$），\n- 稳定结果相对于参考值的绝对误差，以浮点数形式表示（如果不是有限数，则使用 $+\\infty$）。\n\n最终输出格式：\n您的程序应生成单行输出，其中包含按 A、B、C、D 顺序串联的所有案例的结果，形式为一个用方括号括起来的逗号分隔列表，例如“[r1,r2,r3,...,r20]”。所有布尔值必须显示为不带引号的 True 或 False。不应打印任何额外的文本或空格。", "solution": "问题要求实现并比较计算 log-sum-exp 函数的两种方法，该函数定义为 $L(\\eta) = \\log \\sum_{j=1}^K \\exp(\\eta_j)$，其中 $\\eta \\in \\mathbb{R}^K$ 是来自多项逻辑回归模型的线性预测变量向量。其目的是处理浮点运算中出现的数值不稳定性问题。\n\n### 步骤 1：提取已知信息\n- **任务**：为计算 $L(\\eta) = \\log \\sum_{j=1}^K \\exp(\\eta_j)$ 实现一种直接方法和一种数值稳定的方法。\n- **线性预测变量**：$\\eta_j = x^\\top \\beta_j$，其中 $j \\in \\{1, \\dots, K\\}$，$x \\in \\mathbb{R}^p$ 且 $\\beta_j \\in \\mathbb{R}^p$。\n- **参数构造**：对于给定的测试向量 $x$ 和 $t$，参数 $\\beta_j$ 按 $\\beta_j = \\frac{t_j}{x^\\top x} x$ 构造。如下代入所示，这种构造确保了对每个 $j$ 都有 $\\eta_j = t_j$：\n$$ x^\\top \\beta_j = x^\\top \\left( \\frac{t_j}{x^\\top x} x \\right) = \\frac{t_j}{x^\\top x} (x^\\top x) = t_j $$\n该恒等式在 $x^\\top x \\neq 0$（即 $x$ 不是零向量）的情况下成立。所有提供的测试用例都使用非零的 $x$ 向量。因此，对于每个测试用例，线性预测变量向量 $\\eta$ 正是给定的目标向量 $t$。\n- **测试用例**：\n    - **A**：$p=3, K=3, x=[0.5, -1.0, 2.0], t=[-1.2, 0.3, 2.1]$\n    - **B**：$p=3, K=3, x=[1.0, 1.0, 1.0], t=[1000.0, 1001.0, 999.0]$\n    - **C**：$p=3, K=3, x=[1.0, 1.0, 1.0], t=[-1000.0, -1001.0, -999.0]$\n    - **D**：$p=3, K=3, x=[1.0, 1.0, 1.0], t=[-1000.0, 0.0, 1000.0]$\n- **每个案例的必需输出**：一个包含 5 个值的序列：\n    1.  欧几里得范数 $\\|\\eta\\|_2$。\n    2.  布尔值：直接结果是否为有限数？\n    3.  布尔值：稳定结果是否为有限数？\n    4.  直接结果的绝对误差（或 $+\\infty$）。\n    5.  稳定结果的绝对误差（或 $+\\infty$）。\n- **参考值**：使用一个可靠的参考实现来近似正确值。\n\n### 步骤 2：使用提取的已知信息进行验证\n对问题陈述进行严格评估：\n- **科学依据**：该问题解决了 log-sum-exp 计算中众所周知的数值不稳定性问题，这是多项逻辑回归和其他机器学习模型中使用的 softmax 函数的基本组成部分。用于实现稳定性的数学变换是标准且正确的。该问题牢固地植根于数值分析和统计学习理论。\n- **适定性**：问题是明确的。每个测试用例的所有输入（$x, t, p, K$）都已指定。要计算的量有明确定义。导致 $\\eta = t$ 的 $\\beta_j$ 的构造是明确的。评估指标（范数、有限性、绝对误差）是精确的。每个计算都存在唯一解。\n- **客观性**：问题以客观的数学语言陈述。它要求基于数值输入和既定算法的计算解决方案，不含主观解释。\n- **完整性和一致性**：问题是自包含的。所有必要的公式和数据都已提供。设置中没有矛盾之处。\n- **无其他缺陷**：问题并非无足轻重（它展示了一个关键的数值问题），不是隐喻性的，并且与所述主题直接相关。它可以通过计算来验证。\n\n### 步骤 3：结论与行动\n该问题是**有效的**。将提供一个合理的解决方案。\n\n### 解法推导\n\n对于给定的向量 $\\eta = (\\eta_1, \\eta_2, \\dots, \\eta_K)$，我们需要计算 $L(\\eta) = \\log \\sum_{j=1}^K \\exp(\\eta_j)$。\n\n**方法 1：直接计算**\n此方法按公式书写的方式实现。\n1.  对每个 $j \\in \\{1, \\dots, K\\}$，计算 $v_j = \\exp(\\eta_j)$。\n2.  计算和 $S = \\sum_{j=1}^K v_j$。\n3.  计算最终结果 $R_{\\text{direct}} = \\log(S)$。\n\n这种方法在数值上是不稳定的。对于标准的双精度浮点数，当 $z \\gtrsim 709.78$ 时，$\\exp(z)$ 会上溢至 $+\\infty$。如果任何一个 $\\eta_j$ 很大，相应的 $\\exp(\\eta_j)$ 会变成 $+\\infty$，导致和为无穷大，结果也为无穷大。反之，当 $z \\lesssim -745.13$ 时，$\\exp(z)$ 会下溢至 $0$。如果所有的 $\\eta_j$ 值都是很大的负数，那么和中的每一项都可能下溢到 $0.0$，使得和为 $0.0$。对数 $\\log(0.0)$ 则是 $-\\infty$。\n\n**方法 2：数值稳定的计算（Log-Sum-Exp 技巧）**\n此方法基于一个数学恒等式，该恒等式将数值重新定位到一个数值安全的范围内。设 $m = \\max_{j} \\eta_j$。我们可以通过提出因子 $\\exp(m)$ 来重写表达式：\n$$ L(\\eta) = \\log \\left( \\sum_{j=1}^K \\exp(\\eta_j) \\right) $$\n$$ = \\log \\left( \\exp(m) \\sum_{j=1}^K \\frac{\\exp(\\eta_j)}{\\exp(m)} \\right) $$\n利用对数和指数的性质 $\\log(a \\cdot b) = \\log(a) + \\log(b)$ 和 $\\exp(a)/\\exp(b) = \\exp(a-b)$，我们得到：\n$$ = \\log(\\exp(m)) + \\log \\left( \\sum_{j=1}^K \\exp(\\eta_j - m) \\right) $$\n$$ = m + \\log \\left( \\sum_{j=1}^K \\exp(\\eta_j - m) \\right) $$\n这给出了稳定的计算步骤：\n1.  找到向量中的最大值：$m = \\max(\\eta_1, \\dots, \\eta_K)$。\n2.  计算移位后的向量 $\\eta_j' = \\eta_j - m$。\n3.  对每个 $j$ 计算 $v_j' = \\exp(\\eta_j')$。\n4.  计算和 $S' = \\sum_{j=1}^K v_j'$。\n5.  计算最终结果 $R_{\\text{stable}} = m + \\log(S')$。\n\n这种方法是稳定的，因为指数函数的参数 $\\eta_j' = \\eta_j - m$ 总是小于或等于 $0$。最大参数为 $0$，导致 $\\exp(0)=1$。这可以防止上溢。此外，由于和 $S'$ 中至少有一项恰好为 $1$，因此可以保证和至少为 $1$。这可以防止对数的参数因所有项下溢而变为 $0$，从而避免了结果为 $-\\infty$。\n\n**每个测试用例的计算步骤**\n对于由 $x$ 和 $t$ 定义的每个案例：\n1.  设置 $\\eta = t$。\n2.  计算欧几里得范数 $\\|\\eta\\|_2 = \\sqrt{\\sum_{j=1}^K \\eta_j^2}$。\n3.  使用直接方法计算 $R_{\\text{direct}}$。\n4.  使用稳定方法计算 $R_{\\text{stable}}$。\n5.  使用 `scipy.special.logsumexp` 获取参考值 $R_{\\text{ref}}$，它是稳定方法的一个专业实现版本。\n6.  判断 $R_{\\text{direct}}$ 和 $R_{\\text{stable}}$ 是否为有限数。\n7.  计算绝对误差 $E_{\\text{direct}} = |R_{\\text{direct}} - R_{\\text{ref}}|$ 和 $E_{\\text{stable}} = |R_{\\text{stable}} - R_{\\text{ref}}|$。如果某个结果不是有限数，其对应的误差报告为 $+\\infty$。\n8.  收集并格式化五个所需的值：$(\\|\\eta\\|_2, \\text{isfinite}(R_{\\text{direct}}), \\text{isfinite}(R_{\\text{stable}}), E_{\\text{direct}}, E_{\\text{stable}})$。所有测试用例的结果将被连接成一个单一列表作为最终输出。", "answer": "```python\nimport numpy as np\nfrom scipy.special import logsumexp\n\ndef solve():\n    \"\"\"\n    Solves the numerical stability problem for the log-sum-exp function.\n    \"\"\"\n    \n    # Each test case is defined by p, K, x, and t.\n    # We can directly use t as eta, as per the problem's construction.\n    test_cases = {\n        'A': {\n            'p': 3, 'K': 3,\n            'x': np.array([0.5, -1.0, 2.0]),\n            't': np.array([-1.2, 0.3, 2.1]),\n        },\n        'B': {\n            'p': 3, 'K': 3,\n            'x': np.array([1.0, 1.0, 1.0]),\n            't': np.array([1000.0, 1001.0, 999.0]),\n        },\n        'C': {\n            'p': 3, 'K': 3,\n            'x': np.array([1.0, 1.0, 1.0]),\n            't': np.array([-1000.0, -1001.0, -999.0]),\n        },\n        'D': {\n            'p': 3, 'K': 3,\n            'x': np.array([1.0, 1.0, 1.0]),\n            't': np.array([-1000.0, 0.0, 1000.0]),\n        }\n    }\n\n    all_results = []\n    \n    # The problem asks to process cases in order A, B, C, D.\n    case_order = ['A', 'B', 'C', 'D']\n    \n    for case_key in case_order:\n        case_data = test_cases[case_key]\n        t = case_data['t']\n        \n        # As per the problem description, eta_j = t_j.\n        eta = t.astype(np.float64) # Use float64 for precision.\n        \n        # 1. Compute the vector eta (already done) and its Euclidean norm.\n        norm_eta = np.linalg.norm(eta)\n        \n        # 2. Compute the direct result.\n        # This implementation follows the naive formula log(sum(exp(eta))).\n        # It is prone to overflow/underflow.\n        with np.errstate(over='ignore'): # Suppress overflow warnings for this calculation\n            exp_eta_direct = np.exp(eta)\n            sum_exp_direct = np.sum(exp_eta_direct)\n            res_direct = np.log(sum_exp_direct)\n            \n        # 3. Compute the numerically stable result.\n        # This uses the log-sum-exp trick: m + log(sum(exp(eta - m))).\n        m = np.max(eta)\n        res_stable = m + np.log(np.sum(np.exp(eta - m)))\n        \n        # 4. Use a reliable reference implementation.\n        # scipy.special.logsumexp provides a robust implementation.\n        ref_val = logsumexp(eta)\n        \n        # 5. Determine finiteness and compute absolute errors.\n        is_finite_direct = np.isfinite(res_direct)\n        is_finite_stable = np.isfinite(res_stable)\n        \n        err_direct = np.abs(res_direct - ref_val) if is_finite_direct else np.inf\n        err_stable = np.abs(res_stable - ref_val) if is_finite_stable else np.inf\n\n        # Append the five required values for the current case.\n        all_results.extend([\n            norm_eta,\n            is_finite_direct,\n            is_finite_stable,\n            err_direct,\n            err_stable\n        ])\n\n    # Final print statement in the exact required format.\n    # Convert bools to 'True'/'False' strings without quotes.\n    # Format floats to a consistent representation.\n    formatted_results = []\n    for r in all_results:\n        if isinstance(r, bool):\n            formatted_results.append(str(r))\n        else:\n            # Using repr() to get a standard floating point representation\n            formatted_results.append(repr(r))\n            \n    # The requested format is a comma-separated list inside square brackets.\n    # map(str,...) is sufficient and clean.\n    print(f\"[{','.join(map(str, all_results))}]\")\n\nsolve()\n```", "id": "3151616"}]}