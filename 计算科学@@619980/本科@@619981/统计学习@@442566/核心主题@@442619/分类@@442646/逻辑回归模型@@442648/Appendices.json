{"hands_on_practices": [{"introduction": "使用逻辑回归的一项关键技能是解释模型系数。与线性回归中系数代表结果本身的變化不同，逻辑回归的系数衡量的是事件发生的*对数几率* (log-odds) 的变化。第一个练习 [@problem_id:1931449] 提供了一个直接且基础的计算，旨在帮助你掌握这一核心概念，展示预测变量的变化如何转化为成功对数几率的变化。", "problem": "一个研究小组正在研究影响考生在一项以难度著称的专业认证考试中成功通过的因素。他们从一个考生样本中收集数据，记录了他们学习的小时数（$x$）以及他们是通过（$Y=1$）还是未通过（$Y=0$）考试。\n\n他们拟合了一个简单的逻辑回归模型来预测通过的概率，$p = P(Y=1|x)$。该模型由以下计算通过概率对数优势比的方程给出：\n$$ \\ln\\left(\\frac{p}{1-p}\\right) = \\beta_0 + \\beta_1 x $$\n将模型与数据拟合后，估计出的系数为 $\\hat{\\beta}_0 = -5.2$ 和 $\\hat{\\beta}_1 = 0.115$。\n\n一名已经学习了一段时间的考生决定再多学习7个小时。根据拟合的模型，该考生通过考试的对数优势比会发生什么变化？将你的答案表示为一个四舍五入到三位有效数字的数值。", "solution": "我们得到了一个关于对数优势比的简单逻辑回归模型：\n$$\\ln\\left(\\frac{p}{1-p}\\right) = \\beta_{0} + \\beta_{1} x.$$\n如果学习时间从 $x$ 增加到 $x + \\Delta x$，对数优势比的变化为\n$$\\Delta \\left[\\ln\\left(\\frac{p}{1-p}\\right)\\right] = \\left[\\beta_{0} + \\beta_{1}(x+\\Delta x)\\right] - \\left[\\beta_{0} + \\beta_{1}x\\right] = \\beta_{1}\\Delta x.$$\n使用估计出的斜率 $\\hat{\\beta}_{1} = 0.115$ 和 $\\Delta x = 7$，变化为\n$$\\Delta \\left[\\ln\\left(\\frac{p}{1-p}\\right)\\right] = \\hat{\\beta}_{1} \\cdot 7 = 0.115 \\cdot 7 = 0.805.$$\n四舍五入到三位有效数字，结果是 $0.805$。", "answer": "$$\\boxed{0.805}$$", "id": "1931449"}, {"introduction": "现实世界的数据常常包含分类预测变量，例如订阅等级或产品类型，这些变量无法直接代入数学方程。我们使用一种称为“虚拟变量编码” (dummy variable encoding) 的技术来数值化地表示这些类别。这个练习 [@problem_id:1931482] 将指导你如何为一个分类预测变量正确地建立逻辑回归模型方程，这是构建准确且可解释模型的重要一步。", "problem": "一位数据科学家正在构建一个模型，用于预测一项基于订阅的软件服务的客户流失情况。我们关注的结果是一个二元变量 $Y$，其中当客户流失（取消订阅）时 $Y=1$，未流失时 $Y=0$。唯一可用的预测变量是客户的 `Subscription Tier`（订阅等级），这是一个具有三个不同水平的分类变量：'Basic'、'Standard' 和 'Premium'。\n\n这位数据科学家决定使用逻辑回归模型来估计流失概率 $p = P(Y=1)$。为了纳入该分类预测变量，他们使用了哑变量编码，并选择 'Basic' 等级作为参考类别。\n\n设 $p$ 为客户流失的概率。根据客户的订阅等级，下列哪个方程正确地表示了客户流失的对数优势比（log-odds）$\\ln\\left(\\frac{p}{1-p}\\right)$ 的逻辑回归模型？\n\nA. $\\ln\\left(\\frac{p}{1-p}\\right) = \\beta_0 + \\beta_1 X_{\\text{Standard}} + \\beta_2 X_{\\text{Premium}}$，其中，对于 'Standard' 等级，$X_{\\text{Standard}} = 1$，否则为 0；对于 'Premium' 等级，$X_{\\text{Premium}} = 1$，否则为 0。\n\nB. $\\ln\\left(\\frac{p}{1-p}\\right) = \\beta_0 + \\beta_1 Z$，其中，对于 'Basic' 等级 $Z=1$，对于 'Standard' 等级 $Z=2$，对于 'Premium' 等级 $Z=3$。\n\nC. $p = \\beta_0 + \\beta_1 X_{\\text{Standard}} + \\beta_2 X_{\\text{Premium}}$，其中，对于 'Standard' 等级，$X_{\\text{Standard}} = 1$，否则为 0；对于 'Premium' 等级，$X_{\\text{Premium}} = 1$，否则为 0。\n\nD. $\\ln\\left(\\frac{p}{1-p}\\right) = \\beta_0 + \\beta_1 X_{\\text{Basic}} + \\beta_2 X_{\\text{Standard}} + \\beta_3 X_{\\text{Premium}}$，其中，对于相应的等级，$X_{\\text{level}} = 1$，否则为 0。\n\nE. $\\ln\\left(\\frac{p}{1-p}\\right) = \\beta_0 + \\beta_1 X_{\\text{Premium}}$，其中，如果等级是 'Premium'，$X_{\\text{Premium}} = 1$，否则为 0。", "solution": "我们使用逻辑回归对二元结果 $Y \\in \\{0,1\\}$ 进行建模，该模型将 $p=P(Y=1)$ 的对数优势比（logit）指定为预测变量的线性函数：\n$$\n\\ln\\left(\\frac{p}{1-p}\\right)=\\eta=\\beta_{0}+\\text{(预测变量的线性组合)}.\n$$\n对于一个有三个水平的分类预测变量，并使用参考编码，当 $L=3$ 时，我们引入 $L-1=2$ 个哑变量。以 'Basic' 等级为参考，定义\n- 如果等级是 'Standard'，$X_{\\text{Standard}}=1$，否则为 $0$，\n- 如果等级是 'Premium'，$X_{\\text{Premium}}=1$，否则为 $0$，\n这样对于 'Basic' 等级，两个指示变量都为 $0$。\n\n那么，正确的逻辑回归模型是\n$$\n\\ln\\left(\\frac{p}{1-p}\\right)=\\beta_{0}+\\beta_{1}X_{\\text{Standard}}+\\beta_{2}X_{\\text{Premium}}.\n$$\n通过代入，可得：\n- 对于 'Basic' 等级：$X_{\\text{Standard}}=0$, $X_{\\text{Premium}}=0$，所以 $\\ln\\left(\\frac{p}{1-p}\\right)=\\beta_{0}$。\n- 对于 'Standard' 等级：$X_{\\text{Standard}}=1$, $X_{\\text{Premium}}=0$，所以 $\\ln\\left(\\frac{p}{1-p}\\right)=\\beta_{0}+\\beta_{1}$。\n- 对于 'Premium' 等级：$X_{\\text{Standard}}=0$, $X_{\\text{Premium}}=1$，所以 $\\ln\\left(\\frac{p}{1-p}\\right)=\\beta_{0}+\\beta_{2}$。\n这与以 'Basic' 等级为参考的标准哑变量编码相匹配。\n\n评估各个选项：\n- A 选项完全匹配上述的哑变量逻辑回归模型设定，因此是正确的。\n- B 选项使用单一数值代码 $Z \\in \\{1,2,3\\}$，并假设在有序代码之间存在线性效应，这不等同于以 'Basic' 为参考的哑变量编码，并且在类别之间强加了不合理的线性关系。\n- C 选项对 $p$ 而不是对数优势比进行线性建模；这不是逻辑回归。\n- D 选项包含了三个哑变量和一个截距项；由于 $X_{\\text{Basic}}+X_{\\text{Standard}}+X_{\\text{Premium}}=1$，这会导致完全多重共线性（哑变量陷阱），因此它不是一个有效的模型设定。\n- E 选项只为 'Premium' 等级使用了一个哑变量，将 'Standard' 和 'Basic' 等级合并在了一起；这没有通过完整的哑变量编码来表示一个以 'Basic' 为参考的三水平因子。\n\n因此，正确选项是 A。", "answer": "$$\\boxed{A}$$", "id": "1931482"}, {"introduction": "虽然统计软件能自动拟合模型，但真正的深刻理解源于了解其内部工作原理。这个高级练习 [@problem_id:3142117] 要求你从基本原理出发，构建一个逻辑回归模型，并实现牛顿-拉弗森 (Newton-Raphson) 算法进行优化。你还将计算和比较不同的“伪R方” ($R^2$) 指标，从而深入了解我们如何衡量逻辑模型的解释能力。", "problem": "要求您从第一性原理出发，实现一个二元逻辑回归模型，并比较两种可释方差的度量：Tjur 判别系数和 McFadden 伪决定系数。请从以下统计学习的基本原理开始：\n- 二元结果被建模为条件独立的伯努利随机变量，其成功概率为 $p_i \\in (0,1)$。\n- 逻辑回归模型通过逻辑函数指定协变量与成功概率之间的关联，使得 $p_i = \\sigma(\\eta_i)$，其中 $\\sigma(t) = \\frac{1}{1 + e^{-t}}$，线性预测器为 $\\eta_i = \\mathbf{x}_i^{\\top}\\boldsymbol{\\beta}$，$\\boldsymbol{\\beta}$ 是一个包含截距的未知参数向量。\n- 最大似然估计 (MLE) 原理选择能使伯努利模型和逻辑关联所蕴含的对数似然最大化的 $\\boldsymbol{\\beta}$。\n\n任务：\n1. 从伯努利似然和逻辑关联的定义出发（不使用其他预先推导的公式），推导以 $\\boldsymbol{\\beta}$ 表示的对数似然，并展示如何获得 Newton–Raphson 迭代法来计算最大似然估计 $\\widehat{\\boldsymbol{\\beta}}$。然后，实现一个 Newton–Raphson 求解器，该求解器：\n   - 默认在设计矩阵中添加一列全为 1 的截距项。\n   - 迭代直至参数增量的欧几里得范数小于容差 $10^{-12}$ 或达到 $100$ 次迭代。\n2. 找到 $\\widehat{\\boldsymbol{\\beta}}$ 后，计算所有观测值的拟合概率 $\\widehat{p}_i = \\sigma(\\mathbf{x}_i^{\\top}\\widehat{\\boldsymbol{\\beta}})$。\n3. 从第一性原理出发，将 Tjur 判别系数定义为所有 $y_i = 1$ 的案例中 $\\widehat{p}_i$ 的均值与所有 $y_i = 0$ 的案例中 $\\widehat{p}_i$ 的均值之差。同样，从第一性原理出发，使用完整模型和空模型（仅含截距）的最大化对数似然来定义 McFadden 伪决定系数，并使用自然对数进行计算。\n4. 使用以下测试套件，每个案例提供一个包含单个预测变量（程序必须自动添加截距）的设计矩阵和一个二元响应向量。对每个案例，计算 Tjur 和 McFadden 两种度量。\n   - 案例 A（无明显信号）：预测变量值 $x$ 和标签 $y$\n     - $x = \\big[0,0,0,0,0,0,1,1,1,1,1,1\\big]$\n     - $y = \\big[0,1,0,1,0,1,0,1,0,1,0,1\\big]$\n   - 案例 B（中等信号）：预测变量值 $x$ 和标签 $y$\n     - $x = \\big[\\underbrace{0,\\dots,0}_{10\\ \\text{times}},\\underbrace{1,\\dots,1}_{10\\ \\text{times}}\\big]$\n     - $y = \\big[1,0,0,1,0,0,0,0,0,0,\\ 1,1,1,1,1,1,1,1,0,0\\big]$\n   - 案例 C（较强但非完美信号）：预测变量值 $x$ 和标签 $y$\n     - $x = \\big[\\underbrace{0,\\dots,0}_{12\\ \\text{times}},\\underbrace{1,\\dots,1}_{8\\ \\text{times}}\\big]$\n     - $y = \\big[1,0,0,0,0,0,0,0,0,1,0,1,\\ 1,1,1,1,1,1,1,0\\big]$\n5. 最终输出格式：您的程序应生成单行输出，包含一个用方括号括起来的逗号分隔列表。该列表必须按顺序包含案例 A、案例 B、案例 C 的 Tjur 和 McFadden 度量，所有值均为浮点数并四舍五入到六位小数，即：\n   - 输出顺序：$\\big[\\text{Tjur}_A,\\text{McFadden}_A,\\text{Tjur}_B,\\text{McFadden}_B,\\text{Tjur}_C,\\text{McFadden}_C\\big]$。\n   - 格式示例：$\\big[0.123456,0.234567,0.345678,0.456789,0.567890,0.678901\\big]$。\n\n本问题不涉及物理量或角度，因此不需要单位。所有数值答案都应按指定顺序打印为十进制浮点数，并四舍五入到小数点后六位。", "solution": "问题陈述经评估有效。它在科学上基于统计学习的原理，问题定义明确、客观且内部一致。它提供了推导和实现二元逻辑回归模型、计算指定的可释方差度量，并将其应用于所提供测试案例的所有必要信息。该任务是计算统计学中一个标准的、有一定难度的练习。我们可以开始求解。\n\n解决方案的结构如下：首先，我们推导逻辑回归模型的数学基础，包括对数似然函数和 Newton-Raphson 优化算法。其次，我们定义两种可释方差的度量：Tjur 判别系数和 McFadden 伪决定系数。最后，我们描述将这些原理应用于测试案例的实现过程。\n\n**1. 逻辑回归模型及其最大似然估计的推导**\n\n二元逻辑回归模型用于根据预测向量 $\\mathbf{x}_i \\in \\mathbb{R}^k$ 对 $i=1, \\dots, N$ 个观测值的二元结果变量 $y_i \\in \\{0, 1\\}$ 进行建模。\n\n**对数似然函数**\n每个结果 $y_i$ 被建模为独立伯努利随机变量 $Y_i \\sim \\text{Bernoulli}(p_i)$ 的一次实现，其中 $p_i = P(Y_i=1 | \\mathbf{x}_i)$。其概率质量函数为 $P(Y_i=y_i) = p_i^{y_i}(1-p_i)^{1-y_i}$。逻辑回归模型通过 logit 关联函数将概率 $p_i$ 与预测变量 $\\mathbf{x}_i$ 联系起来。概率的对数优势比（或 logit）被建模为预测变量的线性函数：\n$$ \\text{logit}(p_i) = \\log\\left(\\frac{p_i}{1-p_i}\\right) = \\mathbf{x}_i^{\\top}\\boldsymbol{\\beta} $$\n其中 $\\boldsymbol{\\beta} \\in \\mathbb{R}^k$ 是模型参数的向量。线性部分 $\\eta_i = \\mathbf{x}_i^{\\top}\\boldsymbol{\\beta}$ 被称为线性预测器。\n通过对 logit 函数求逆，我们得到概率 $p_i$ 是 sigmoid 函数 $\\sigma(\\cdot)$ 应用于线性预测器的结果：\n$$ p_i = \\sigma(\\eta_i) = \\sigma(\\mathbf{x}_i^{\\top}\\boldsymbol{\\beta}) = \\frac{1}{1 + e^{-\\mathbf{x}_i^{\\top}\\boldsymbol{\\beta}}} $$\n鉴于观测值的独立性，整个数据集 $(\\mathbf{y}, \\mathbf{X})$ 的似然函数，其中 $\\mathbf{y} = (y_1, \\dots, y_N)^{\\top}$，$\\mathbf{X}$ 是以 $\\mathbf{x}_i^{\\top}$ 为行的设计矩阵，是各个概率的乘积：\n$$ L(\\boldsymbol{\\beta}) = \\prod_{i=1}^N p_i^{y_i} (1-p_i)^{1-y_i} $$\n在计算上，处理对数似然 $\\ell(\\boldsymbol{\\beta}) = \\log L(\\boldsymbol{\\beta})$ 更为方便：\n$$ \\ell(\\boldsymbol{\\beta}) = \\sum_{i=1}^N \\left[ y_i \\log p_i + (1-y_i) \\log(1-p_i) \\right] $$\n我们可以利用关系 $\\log\\left(\\frac{p_i}{1-p_i}\\right) = \\mathbf{x}_i^{\\top}\\boldsymbol{\\beta}$ 和 $\\log(1-p_i) = -\\log(1+e^{\\mathbf{x}_i^{\\top}\\boldsymbol{\\beta}})$，将其直接用 $\\boldsymbol{\\beta}$ 表示。\n$$ \\ell(\\boldsymbol{\\beta}) = \\sum_{i=1}^N \\left[ y_i(\\mathbf{x}_i^{\\top}\\boldsymbol{\\beta}) - \\log(1+e^{\\mathbf{x}_i^{\\top}\\boldsymbol{\\beta}}) \\right] $$\n这就是需要最大化以找到最大似然估计 (MLE) $\\widehat{\\boldsymbol{\\beta}}$ 的对数似然函数。\n\n**用于最大化的 Newton–Raphson 方法**\nMLE $\\widehat{\\boldsymbol{\\beta}}$ 通过求解 $\\nabla \\ell(\\boldsymbol{\\beta}) = \\mathbf{0}$ 找到。由于此方程是非线性的，我们使用迭代数值方法 Newton-Raphson。最大化 $\\ell(\\boldsymbol{\\beta})$ 的更新规则是：\n$$ \\boldsymbol{\\beta}^{(t+1)} = \\boldsymbol{\\beta}^{(t)} - \\left[ \\mathbf{H}(\\boldsymbol{\\beta}^{(t)}) \\right]^{-1} \\nabla \\ell(\\boldsymbol{\\beta}^{(t)}) $$\n其中 $\\nabla \\ell(\\boldsymbol{\\beta})$ 是对数似然的梯度向量，$\\mathbf{H}(\\boldsymbol{\\beta})$ 是其 Hessian 矩阵。\n\n**对数似然的梯度**\n梯度 $\\nabla \\ell(\\boldsymbol{\\beta})$ 是一个由偏导数 $\\frac{\\partial \\ell}{\\partial \\beta_j}$ 组成的向量，其中 $j=1, \\dots, k$。\n$$ \\frac{\\partial \\ell}{\\partial \\beta_j} = \\frac{\\partial}{\\partial \\beta_j} \\sum_{i=1}^N \\left[ y_i(\\mathbf{x}_i^{\\top}\\boldsymbol{\\beta}) - \\log(1+e^{\\mathbf{x}_i^{\\top}\\boldsymbol{\\beta}}) \\right] = \\sum_{i=1}^N \\left[ y_i x_{ij} - \\frac{\\partial}{\\partial \\beta_j} \\log(1+e^{\\mathbf{x}_i^{\\top}\\boldsymbol{\\beta}}) \\right] $$\n使用链式法则，$\\frac{\\partial}{\\partial \\beta_j} \\log(1+e^{\\mathbf{x}_i^{\\top}\\boldsymbol{\\beta}}) = \\frac{e^{\\mathbf{x}_i^{\\top}\\boldsymbol{\\beta}}}{1+e^{\\mathbf{x}_i^{\\top}\\boldsymbol{\\beta}}} \\cdot x_{ij} = p_i x_{ij}$。\n$$ \\frac{\\partial \\ell}{\\partial \\beta_j} = \\sum_{i=1}^N (y_i - p_i) x_{ij} $$\n在矩阵形式中，设 $\\mathbf{p}$ 是概率 $p_i$ 的向量，则梯度为：\n$$ \\nabla \\ell(\\boldsymbol{\\beta}) = \\mathbf{X}^{\\top}(\\mathbf{y} - \\mathbf{p}) $$\n\n**对数似然的 Hessian 矩阵**\nHessian 矩阵 $\\mathbf{H}$ 的元素为 $H_{jl} = \\frac{\\partial^2 \\ell}{\\partial \\beta_l \\partial \\beta_j}$。\n$$ H_{jl} = \\frac{\\partial}{\\partial \\beta_l} \\sum_{i=1}^N (y_i - p_i) x_{ij} = \\sum_{i=1}^N -x_{ij} \\frac{\\partial p_i}{\\partial \\beta_l} $$\n我们需要 $\\frac{\\partial p_i}{\\partial \\beta_l} = \\frac{d\\sigma(\\eta_i)}{d\\eta_i} \\frac{\\partial \\eta_i}{\\partial \\beta_l}$。sigmoid 函数的导数是 $\\sigma'(\\eta) = \\sigma(\\eta)(1-\\sigma(\\eta)) = p_i(1-p_i)$。\n因此，$\\frac{\\partial p_i}{\\partial \\beta_l} = p_i(1-p_i)x_{il}$。将其代回：\n$$ H_{jl} = - \\sum_{i=1}^N x_{ij} x_{il} p_i(1-p_i) $$\n在矩阵形式中，令 $\\mathbf{W}$ 为对角矩阵，其对角元素为 $W_{ii} = p_i(1-p_i)$。Hessian 矩阵是：\n$$ \\mathbf{H}(\\boldsymbol{\\beta}) = -\\mathbf{X}^{\\top}\\mathbf{W}\\mathbf{X} $$\nNewton-Raphson 更新变为：\n$$ \\boldsymbol{\\beta}^{(t+1)} = \\boldsymbol{\\beta}^{(t)} - (-\\mathbf{X}^{\\top}\\mathbf{W}^{(t)}\\mathbf{X})^{-1} (\\mathbf{X}^{\\top}(\\mathbf{y} - \\mathbf{p}^{(t)})) = \\boldsymbol{\\beta}^{(t)} + (\\mathbf{X}^{\\top}\\mathbf{W}^{(t)}\\mathbf{X})^{-1} \\mathbf{X}^{\\top}(\\mathbf{y} - \\mathbf{p}^{(t)}) $$\n这是迭代重加权最小二乘 (IRLS) 算法的更新规则。\n\n**2. 可释方差的度量**\n\n**Tjur 判别系数 ($D$)**\n该系数衡量了两种结果类别的拟合概率分布的分离程度。从第一性原理出发，它是 $y_i=1$ 的观测值的平均拟合概率与 $y_i=0$ 的观测值的平均拟合概率之差。\n$$ D = \\mathbb{E}[\\widehat{p} | Y=1] - \\mathbb{E}[\\widehat{p} | Y=0] $$\n样本估计值 $\\widehat{D}$ 是根据拟合概率 $\\widehat{p}_i = \\sigma(\\mathbf{x}_i^{\\top}\\widehat{\\boldsymbol{\\beta}})$ 计算得出的：\n$$ \\widehat{D} = \\frac{\\sum_{i=1}^N y_i \\widehat{p}_i}{\\sum_{i=1}^N y_i} - \\frac{\\sum_{i=1}^N (1-y_i) \\widehat{p}_i}{\\sum_{i=1}^N (1-y_i)} $$\nD 值接近 1 表示完美分离，而接近 0 表示没有分离。\n\n**McFadden 伪 R 方 ($R^2_{\\text{McF}}$)**\n这个度量类似于线性回归中的决定系数 ($R^2$)。它比较了拟合模型的对数似然 $\\ell_M = \\ell(\\widehat{\\boldsymbol{\\beta}})$ 与仅含截距的空模型的对数似然 $\\ell_0 = \\ell(\\widehat{\\boldsymbol{\\beta}}_0)$。空模型代表了一个基准，其中预测变量没有影响，成功概率是一个常数，由样本均值 $\\bar{y}$ 估计。\n作为模型拟合优度的度量，对数似然总是非正的。完美拟合会得到对数似然为 $0$。McFadden 的 $R^2$ 定义为：\n$$ R^2_{\\text{McF}} = 1 - \\frac{\\ell_M}{\\ell_0} $$\n该值被解释为预测变量相比于空模型在模型拟合度上带来的比例改进。其取值范围从 0（无改进）到一个理论上小于 1 的最大值。\n\n**3. 实现策略**\n\n实现将包括一个 Python 脚本。\n1.  将创建一个函数 `newton_raphson_solver`，用于为给定的设计矩阵 $\\mathbf{X}$ 和响应向量 $\\mathbf{y}$ 找到最大似然估计 $\\widehat{\\boldsymbol{\\beta}}$。它将从 $\\boldsymbol{\\beta}=\\mathbf{0}$ 开始，迭代应用 Newton-Raphson 更新规则，直到参数增量的欧几里得范数低于 $10^{-12}$ 或达到 $100$ 次迭代。\n2.  对于每个测试案例，我们将通过取给定的预测变量向量 $x$ 并在其前面添加一列全为 1 的截距项来构建一个“完整”设计矩阵。\n3.  我们还将构建一个“空”设计矩阵，它只是一个全为 1 的列，以拟合仅含截距的模型。\n4.  完整模型和空模型都将使用 `newton_raphson_solver` 进行拟合，以获得 $\\widehat{\\boldsymbol{\\beta}}_{\\text{full}}$ 和 $\\widehat{\\boldsymbol{\\beta}}_{\\text{null}}$。\n5.  将计算完整模型的拟合概率 $\\widehat{\\mathbf{p}}$。Tjur 的 $D$ 将根据这些概率计算。\n6.  将计算最大化后的完整模型 ($\\ell_M$) 和最大化后的空模型 ($\\ell_0$) 的对数似然。\n7.  McFadden 的 $R^2$ 将使用公式 $1 - \\ell_M / \\ell_0$ 计算。\n8.  对所有三个测试案例重复此过程，并将结果收集并按指定格式打印。将使用 `scipy.special.expit` 函数以实现一个数值稳定的 sigmoid 函数。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import expit\n\ndef solve():\n    \"\"\"\n    Main function to run the logistic regression analysis for all test cases.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (\n            np.array([0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1]),\n            np.array([0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1])\n        ),\n        (\n            np.array([0]*10 + [1]*10),\n            np.array([1,0,0,1,0,0,0,0,0,0, 1,1,1,1,1,1,1,1,0,0])\n        ),\n        (\n            np.array([0]*12 + [1]*8),\n            np.array([1,0,0,0,0,0,0,0,0,1,0,1, 1,1,1,1,1,1,1,0])\n        )\n    ]\n\n    results = []\n    for x, y in test_cases:\n        tjur_d, mcfadden_r2 = compute_metrics(x, y)\n        results.append(round(tjur_d, 6))\n        results.append(round(mcfadden_r2, 6))\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef newton_raphson_solver(X, y, tol=1e-12, max_iter=100):\n    \"\"\"\n    Finds the MLE for logistic regression parameters using Newton-Raphson.\n\n    Args:\n        X (np.ndarray): Design matrix of shape (n_samples, n_features).\n        y (np.ndarray): Response vector of shape (n_samples,).\n        tol (float): Convergence tolerance for the parameter increment norm.\n        max_iter (int): Maximum number of iterations.\n\n    Returns:\n        np.ndarray: The estimated parameter vector beta.\n    \"\"\"\n    # Initialize beta to zeros\n    beta = np.zeros(X.shape[1])\n    \n    for _ in range(max_iter):\n        # Linear predictor\n        eta = X @ beta\n        \n        # Probabilities using a numerically stable sigmoid (expit)\n        p = expit(eta)\n        \n        # Diagonal weight matrix W with weights w_i = p_i * (1 - p_i)\n        # Add a small epsilon for numerical stability in case p is 0 or 1\n        w = p * (1 - p) + 1e-15\n        W = np.diag(w)\n        \n        # Gradient of the log-likelihood\n        gradient = X.T @ (y - p)\n        \n        # Hessian of the log-likelihood\n        hessian = -X.T @ W @ X\n        \n        # Newton-Raphson step\n        try:\n            step = -np.linalg.inv(hessian) @ gradient\n        except np.linalg.LinAlgError:\n            # In case of singularity, break and return current best estimate.\n            # This can happen with perfect separation, but not in test cases.\n            break\n\n        # Update beta\n        beta = beta + step\n        \n        # Check for convergence\n        if np.linalg.norm(step)  tol:\n            break\n            \n    return beta\n\ndef calculate_log_likelihood(p, y):\n    \"\"\"\n    Calculates the log-likelihood of a Bernoulli model.\n\n    Args:\n        p (np.ndarray): Vector of success probabilities.\n        y (np.ndarray): Vector of binary outcomes.\n\n    Returns:\n        float: The total log-likelihood.\n    \"\"\"\n    # Add a small epsilon to prevent log(0)\n    eps = 1e-15\n    p_clipped = np.clip(p, eps, 1 - eps)\n    return np.sum(y * np.log(p_clipped) + (1 - y) * np.log(1 - p_clipped))\n\ndef compute_metrics(x, y):\n    \"\"\"\n    Computes Tjur's D and McFadden's R^2 for a given dataset.\n\n    Args:\n        x (np.ndarray): Single-predictor vector.\n        y (np.ndarray): Response vector.\n\n    Returns:\n        tuple: A tuple containing (tjur_d, mcfadden_r2).\n    \"\"\"\n    # 1. Prepare design matrices for full and null models\n    X_full = np.c_[np.ones(x.shape[0]), x]\n    X_null = np.ones((y.shape[0], 1))\n    \n    # 2. Fit both models to get parameter estimates\n    beta_full = newton_raphson_solver(X_full, y)\n    beta_null = newton_raphson_solver(X_null, y)\n    \n    # 3. Compute Tjur's coefficient of discrimination\n    p_hat_full = expit(X_full @ beta_full)\n    \n    p_if_y1 = p_hat_full[y == 1]\n    p_if_y0 = p_hat_full[y == 0]\n    \n    mean_p1 = np.mean(p_if_y1) if len(p_if_y1) > 0 else 0\n    mean_p0 = np.mean(p_if_y0) if len(p_if_y0) > 0 else 0\n    tjur_d = mean_p1 - mean_p0\n\n    # 4. Compute McFadden's pseudo-R-squared\n    # Log-likelihood of the full model\n    ll_full = calculate_log_likelihood(p_hat_full, y)\n    \n    # Log-likelihood of the null model\n    p_hat_null = expit(X_null @ beta_null)\n    ll_null = calculate_log_likelihood(p_hat_null, y)\n    \n    # Handle the case where ll_null is zero to avoid division by zero\n    if ll_null == 0:\n        mcfadden_r2 = 1.0 if ll_full == 0 else 0.0\n    else:\n        mcfadden_r2 = 1 - (ll_full / ll_null)\n        \n    return tjur_d, mcfadden_r2\n\nif __name__ == '__main__':\n    solve()\n\n```", "id": "3142117"}]}