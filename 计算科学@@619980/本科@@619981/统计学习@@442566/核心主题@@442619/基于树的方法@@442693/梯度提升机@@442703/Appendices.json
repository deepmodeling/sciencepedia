{"hands_on_practices": [{"introduction": "这个实践深入探讨了梯度提升算法的核心：更新规则。通过实现并比较一阶（仅梯度）更新与二阶（牛顿）更新 [@problem_id:3125494]，你将深刻理解计算效率和收敛速度之间的权衡。本练习阐明了损失函数曲率的不同近似方法如何影响学习过程，这是机器学习优化的一个关键概念。", "problem": "您将实现并比较梯度提升机 (GBM) 中用于二元分类的两种更新规则，这两种规则均在逻辑斯蒂损失下进行：一种是一阶纯梯度更新，另一种是二阶 Newton 更新。您的任务是，在固定的浅层基学习器下，计算每次提升迭代中经验风险的相对减少量，并评估两种更新规则的计算开销与收益。\n\n基本设置。考虑独立同分布的数据点 $\\{(x_i, y_i)\\}_{i=1}^N$，其中 $x_i \\in \\mathbb{R}$ 且 $y_i \\in \\{0, 1\\}$。设模型的当前得分（logit）为 $F_i \\in \\mathbb{R}$，预测概率为 $\\sigma(F_i) = \\frac{1}{1 + e^{-F_i}}$，其中 $\\sigma$ 是逻辑斯蒂 sigmoid 函数。在二元逻辑斯蒂损失下的经验风险为\n$$\nR(F) \\;=\\; \\sum_{i=1}^N \\left( - y_i \\log \\sigma(F_i) - (1-y_i)\\log(1-\\sigma(F_i)) \\right).\n$$\n\n在每次提升迭代中，拟合并将一个决策树桩基学习器添加到模型得分 $F$ 中。该树桩由单个特征 $x$ 上的一个固定阈值 $\\tau$ 定义：\n- 左叶节点索引集：$\\mathcal{L} = \\{ i : x_i \\le \\tau \\}$，\n- 右叶节点索引集：$\\mathcal{R} = \\{ i : x_i  \\tau \\}$，\n其常数输出 $v_{\\mathcal{L}}$ 和 $v_{\\mathcal{R}}$ 分别被加到 $i \\in \\mathcal{L}$ 和 $i \\in \\mathcal{R}$ 的 $F_i$ 上。迭代后的新模型得分为 $F_i \\leftarrow F_i + \\nu\\, v_{\\text{leaf}(i)}$，其中 $\\nu \\in (0,1]$ 是给定的缩减率（学习率）。\n\n固定树桩阈值规则。每个数据集的阈值 $\\tau$ 只固定一次，设为 $\\{x_i\\}_{i=1}^N$ 的中位数，计算方法如下：如果 $N$ 是奇数，$\\tau$ 是排序后 $\\{x_i\\}$ 的中间元素；如果 $N$ 是偶数，$\\tau$ 是排序后 $\\{x_i\\}$ 中间两个元素的平均值。所有提升迭代都重复使用相同的 $\\tau$。\n\n更新规则。设 $p_i = \\sigma(F_i)$，$R$ 相对于 $F_i$ 的梯度（一阶导数）为\n$$\ng_i \\;=\\; \\frac{\\partial R}{\\partial F_i} \\;=\\; p_i - y_i,\n$$\nHessian 矩阵的对角线元素（二阶导数）为\n$$\nh_i \\;=\\; \\frac{\\partial^2 R}{\\partial F_i^2} \\;=\\; p_i (1 - p_i).\n$$\n您必须为叶节点值 $v_{\\mathcal{L}}$ 和 $v_{\\mathcal{R}}$ 实现两种变体：\n- 纯梯度 GBM：在每个叶节点上通过最小二乘法拟合负梯度，因此\n$$\nv_{\\mathcal{S}} \\;=\\; - \\frac{1}{|\\mathcal{S}|} \\sum_{i \\in \\mathcal{S}} g_i, \\quad \\mathcal{S} \\in \\{\\mathcal{L}, \\mathcal{R}\\}.\n$$\n- Newton GBM：每个叶节点使用一个二阶（对角）Newton 步，因此\n$$\nv_{\\mathcal{S}} \\;=\\; - \\frac{\\sum_{i \\in \\mathcal{S}} g_i}{\\sum_{i \\in \\mathcal{S}} h_i}, \\quad \\mathcal{S} \\in \\{\\mathcal{L}, \\mathcal{R}\\},\n$$\n约定如果分母为 $0$，则 $v_{\\mathcal{S}} = 0$。\n\n每轮的相对减少量。设 $R_{\\text{prev}}$ 和 $R_{\\text{new}}$ 分别表示一次提升迭代前后的经验风险。相对减少量定义为\n$$\n\\Delta R / R \\;=\\; \\frac{R_{\\text{prev}} - R_{\\text{new}}}{R_{\\text{prev}}}.\n$$\n记录每一轮的这个值。\n\n计算开销度量。为了以可复现的方式评估开销，仅根据以下规则计算每轮更新中所使用的浮点加/减法和乘法运算：\n- 排除的运算：$\\exp(\\cdot)$ 的求值、除法、比较、索引以及任何纯粹用于度量（例如计算 $R(F)$）的计算均不计数。\n- 每个样本的导数：\n  - 计算 $g_i = p_i - y_i$ 消耗 1 次加法。\n  - 仅对于 Newton GBM，计算 $h_i = p_i (1 - p_i)$ 消耗 1 次加法（用于 $1 - p_i$）和 1 次乘法。\n- 每个叶节点的归约：\n  - 在大小为 $|\\mathcal{S}|$ 的叶节点上求和 $\\sum_{i \\in \\mathcal{S}} g_i$ 消耗 $|\\mathcal{S}|$ 次加法。\n  - 仅对于 Newton GBM，在该叶节点上求和 $\\sum_{i \\in \\mathcal{S}} h_i$ 消耗 $|\\mathcal{S}|$ 次加法。\n- 模型得分更新：\n  - 对于每个更新的样本 $i$，更新 $F_i \\leftarrow F_i + \\nu\\, v_{\\text{leaf}(i)}$ 消耗 1 次乘法和 1 次加法。\n\n对于一个大小为 $N$ 的数据集，具有固定的分区大小 $|\\mathcal{L}|$ 和 $|\\mathcal{R}|$，这会产生每轮的运算计数：\n- 纯梯度 GBM：计算 $g_i$ 需要 $N$ 次加法；求和 $g_i$ 需要 $|\\mathcal{L}| + |\\mathcal{R}| = N$ 次加法；更新需要 $2N$ 次运算；总计 $4N$ 次计数的运算。\n- Newton GBM：计算 $g_i$ 需要 $N$ 次加法；计算 $h_i$ 需要 $2N$ 次运算（每个样本一次加法和一次乘法）；求和 $g_i$ 需要 $N$ 次加法；求和 $h_i$ 需要 $N$ 次加法；更新需要 $2N$ 次运算；总计 $7N$ 次计数的运算。\n如果一个叶节点为空，其对归约和更新的贡献为零，并且其叶节点值定义为 $0$。\n\n性能总结。在 $M$ 轮之后，计算：\n- 纯梯度 GBM 每轮的相对减少量 $\\Delta R / R$ 列表。\n- Newton GBM 每轮的相对减少量 $\\Delta R / R$ 列表。\n- 每种方法的一个标量“单位开销增益”，定义为\n$$\n\\text{gain\\_per\\_kops} \\;=\\; \\frac{R(F^{(0)}) - R(F^{(M)})}{\\text{ops\\_total}/1000},\n$$\n其中 $\\text{ops\\_total}$ 是该方法在所有 $M$ 轮中计数的运算总和。\n\n初始条件。对所有 $i$ 使用 $F_i^{(0)} = 0$，因此 $p_i^{(0)} = \\sigma(0) = 0.5$。\n\n测试套件。您的程序必须为以下参数集实现上述逻辑。对于每种情况，按规定计算中位数阈值 $\\tau$，并在所有提升迭代中重复使用它。\n\n- 情况 1：$X = [-2.0, -1.0, 0.0, 1.0, 2.0]$，$y = [0, 0, 0, 1, 1]$，$M = 5$，$\\nu = 0.5$。\n- 情况 2：$X = [0.0, 0.0, 0.0, 0.0]$，$y = [0, 1, 0, 1]$，$M = 8$，$\\nu = 0.3$。\n- 情况 3：$X = [-1.5, -1.0, -0.5, 0.0, 0.5, 1.0, 1.5]$，$y = [0, 0, 0, 1, 1, 1, 1]$，$M = 12$，$\\nu = 0.1$。\n- 情况 4：$X = [-10.0, 0.0, 10.0]$，$y = [0, 1, 1]$，$M = 1$，$\\nu = 0.8$。\n\n最终输出格式。您的程序必须生成单行输出，其中包含所有情况的结果，形式为用方括号括起来的逗号分隔列表。对于每种情况，输出一个包含四个元素的列表：\n- 纯梯度 GBM 每轮的 $\\Delta R/R$ 列表，四舍五入到 6 位小数。\n- Newton GBM 每轮的 $\\Delta R/R$ 列表，四舍五入到 6 位小数。\n- 纯梯度法的每 1000 次运算增益，四舍五入到 6 位小数。\n- Newton 法的每 1000 次运算增益，四舍五入到 6 位小数。\n\n因此，整体输出必须是以下形式的单行字符串：\n$[ \\text{case1}, \\text{case2}, \\text{case3}, \\text{case4} ]$，\n其中每个 $\\text{casek}$ 是一个格式化为如下的列表：\n$[ [\\delta_{1}^{\\text{grad}}, \\ldots, \\delta_{M}^{\\text{grad}}], [\\delta_{1}^{\\text{newt}}, \\ldots, \\delta_{M}^{\\text{newt}}], G_{\\text{grad}}, G_{\\text{newt}} ]$，\n所有浮点值都四舍五入到 6 位小数。不应打印任何其他文本。不使用角度；不涉及物理单位。百分比必须以小数形式表示（不带百分号）。", "solution": "首先对问题陈述进行严格的验证过程。\n\n### 步骤 1：提取给定条件\n- **数据**：一组 $N$ 个数据点 $\\{(x_i, y_i)\\}_{i=1}^N$，包含特征 $x_i \\in \\mathbb{R}$ 和二元标签 $y_i \\in \\{0, 1\\}$。\n- **模型得分**：$F_i$，样本 $i$ 的 logit。\n- **预测**：预测概率由逻辑斯蒂 sigmoid 函数给出，$p_i = \\sigma(F_i) = \\frac{1}{1 + e^{-F_i}}$。\n- **损失函数**：经验风险是总二元逻辑斯蒂损失：$R(F) = \\sum_{i=1}^N \\left( - y_i \\log \\sigma(F_i) - (1-y_i)\\log(1-\\sigma(F_i)) \\right)$。\n- **基学习器**：具有固定阈值 $\\tau$ 的决策树桩。\n- **叶节点分区**：数据被划分为左叶节点 $\\mathcal{L} = \\{ i : x_i \\le \\tau \\}$ 和右叶节点 $\\mathcal{R} = \\{ i : x_i  \\tau \\}$。\n- **阈值规则**：$\\tau$ 是 $\\{x_i\\}_{i=1}^N$ 的中位数，对于奇数 $N$ 计算为中间元素，对于偶数 $N$ 计算为两个中间元素的平均值。该阈值计算一次后在所有提升迭代中重复使用。\n- **模型更新规则**：$F_i \\leftarrow F_i + \\nu \\cdot v_{\\text{leaf}(i)}$，其中 $\\nu$ 是缩减率（学习率）。\n- **导数**：梯度为 $g_i = p_i - y_i$，Hessian 矩阵的对角线元素为 $h_i = p_i (1 - p_i)$。\n- **叶节点值规则**：\n    - **纯梯度法**：$v_{\\mathcal{S}} = - \\frac{1}{|\\mathcal{S}|} \\sum_{i \\in \\mathcal{S}} g_i$ 对于叶节点 $\\mathcal{S}$。如果叶节点为空，$v_{\\mathcal{S}} = 0$。\n    - **Newton 法**：$v_{\\mathcal{S}} = - \\frac{\\sum_{i \\in \\mathcal{S}} g_i}{\\sum_{i \\in \\mathcal{S}} h_i}$。如果分母为 $0$，$v_{\\mathcal{S}} = 0$。\n- **性能度量**：\n    - **每轮相对减少量**：$\\Delta R / R = \\frac{R_{\\text{prev}} - R_{\\text{new}}}{R_{\\text{prev}}}$。\n    - **计算开销**：每轮特定浮点运算的抽象计数：纯梯度法为 $4N$，Newton 法为 $7N$。\n    - **单位开销增益**：$\\text{gain\\_per\\_kops} = \\frac{R(F^{(0)}) - R(F^{(M)})}{\\text{ops\\_total}/1000}$。\n- **初始条件**：初始模型得分为 $F_i^{(0)} = 0$ 对所有 $i$。\n- **测试套件**：提供了四个具体的测试用例，每个用例都有数据 $(X, y)$、迭代次数 $M$ 和缩减率 $\\nu$。\n\n### 步骤 2：使用提取的给定条件进行验证\n- **科学依据**：该问题在统计学习和数值优化的原理上有充分的依据。梯度提升、逻辑斯蒂损失和 Newton 法是标准的、成熟的概念。所提供的公式是正确的。\n- **适定性**：问题被完全指定。它提供了所有必要的数据、初始条件、所有步骤的明确数学公式，以及所需输出度量的清晰定义。系统的演化是确定性的，会导向一个唯一且有意义的解。\n- **客观性**：问题以精确、客观的数学语言陈述，没有歧义或主观论断。\n\n### 步骤 3：结论与行动\n该问题是有效的。这是一个基于可靠科学原理的、定义明确的计算任务。将开发一个解决方案。\n\n### 求解推导与算法\n该任务要求实现并比较梯度提升机 (GBM) 用于二元分类的两种变体。比较基于收敛速度（通过每轮经验风险的相对减少量衡量）和一个定义的 `gain per overhead`（单位开销增益）度量。\n\n首先，我们按规定建立 GBM 算法的组件。对于由数据 $(X, y)$、迭代次数 $M$ 和缩减率 $\\nu$ 定义的每个测试用例：\n\n1.  **初始化**：\n    - 数据点数量为 $N = |X|$。\n    - 根据指定的中位数规则从 $X$ 计算固定的树桩阈值 $\\tau$。这个 $\\tau$ 定义了左、右叶节点索引集 $\\mathcal{L}$ 和 $\\mathcal{R}$，它们在所有提升迭代中保持不变。\n    - 两个模型，一个用于纯梯度方法（`grad`），一个用于 Newton 方法（`newt`），被初始化为得分 $F_{\\text{grad}}^{(0)} = F_{\\text{newt}}^{(0)} = \\vec{0}$。\n    - 初始预测概率为 $p_i^{(0)} = \\sigma(0) = 0.5$ 对所有 $i \\in \\{1, \\dots, N\\}$。\n    - 初始经验风险为 $R(F^{(0)}) = \\sum_{i=1}^N (-\\log(0.5)) = N \\log(2)$。\n\n2.  **提升迭代**：对于从 $1$ 到 $M$ 的每一轮 $m$，我们为 `grad` 和 `newt` 模型都执行一次更新。对于一个具有得分 $F$ 的通用模型：\n    a. 计算当前经验风险 $R_{\\text{prev}} = R(F)$。\n    b. 为所有样本 $i$ 计算当前概率 $p_i = \\sigma(F_i)$。\n    c. 计算负梯度向量（伪残差）：$g_i = p_i - y_i$。\n    d. **叶节点值计算**：两种方法的核心区别在此。\n        - **纯梯度法**：叶节点值 $v_{\\mathcal{S}}$ 是该叶节点中负伪残差的平均值，实际上是通过最小二乘法（均值）拟合负梯度：\n          $$ v_{\\mathcal{S}} = - \\frac{1}{|\\mathcal{S}|} \\sum_{i \\in \\mathcal{S}} g_i $$\n          如果叶节点 $\\mathcal{S}$ 为空 ($|\\mathcal{S}|=0$)，其值为 $v_{\\mathcal{S}} = 0$。\n        - **Newton 法**：叶节点值使用二阶信息。计算 Hessian 矩阵的对角线元素 $h_i = p_i(1-p_i)$。然后，叶节点值是一个二阶（Newton）步：\n          $$ v_{\\mathcal{S}} = - \\frac{\\sum_{i \\in \\mathcal{S}} g_i}{\\sum_{i \\in \\mathcal{S}} h_i} $$\n          如果 $\\sum_{i \\in \\mathcal{S}} h_i = 0$，则 $v_{\\mathcal{S}} = 0$。\n    e. **模型得分更新**：所有样本的得分通过加上相应的叶节点值（由缩减率参数 $\\nu$ 缩放）来更新：\n        $$ F_i \\leftarrow F_i + \\nu \\cdot v_{\\text{leaf}(i)} $$\n    f. 计算新的经验风险 $R_{\\text{new}} = R(F_{\\text{new}})$。\n    g. 计算并存储该轮风险的相对减少量：$\\Delta R / R = (R_{\\text{prev}} - R_{\\text{new}}) / R_{\\text{prev}}$。\n\n3.  **性能总结**：在 $M$ 轮之后，为两种方法计算最终度量。\n    - 总风险减少量为 $R(F^{(0)}) - R(F^{(M)})$。\n    - `grad` 方法的总运算计数 $\\text{ops\\_total}$ 为 $M \\times 4N$，`newt` 方法为 $M \\times 7N$。\n    - `gain_per_kops` 计算为 $\\frac{R(F^{(0)}) - R(F^{(M)})}{\\text{ops\\_total}/1000}$。\n\n4.  **实现**：该算法使用 `numpy` 库在 Python 中实现，以进行高效的向量和矩阵运算。创建单独的函数来计算中位数阈值、sigmoid 函数、逻辑斯蒂风险，以及为给定方法运行主 GBM 模拟循环。一个主循环遍历四个测试用例，为每个用例调用模拟逻辑，计算最终度量，并按规定格式化输出字符串。通过在指数计算前裁剪大的得分值以防止溢出，以及通过将概率裁剪到不完全等于 $0$ 或 $1$ 来防止 `log(0)` 错误，从而考虑了数值稳定性。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef get_median_threshold(X: np.ndarray) - float:\n    \"\"\"Computes the median of X as per the problem's rules.\"\"\"\n    if len(X) == 0:\n        return 0.0\n    X_sorted = np.sort(X)\n    N = len(X_sorted)\n    if N % 2 == 1:\n        tau = X_sorted[N // 2]\n    else:\n        tau = (X_sorted[N // 2 - 1] + X_sorted[N // 2]) / 2.0\n    return float(tau)\n\ndef sigmoid(F: np.ndarray) - np.ndarray:\n    \"\"\"Computes the logistic sigmoid function with overflow protection.\"\"\"\n    F_clipped = np.clip(F, -500, 500)\n    return 1.0 / (1.0 + np.exp(-F_clipped))\n\ndef logistic_risk(F: np.ndarray, y: np.ndarray) - float:\n    \"\"\"Computes the empirical risk (total logistic loss) with numerical stability.\"\"\"\n    p = sigmoid(F)\n    epsilon = 1e-12\n    p_clipped = np.clip(p, epsilon, 1.0 - epsilon)\n    risk = -np.sum(y * np.log(p_clipped) + (1 - y) * np.log(1.0 - p_clipped))\n    return float(risk)\n\ndef run_gbm_variant(\n    X: np.ndarray,\n    y: np.ndarray,\n    M: int,\n    nu: float,\n    leaf_indices_L: np.ndarray,\n    leaf_indices_R: np.ndarray,\n    method: str\n) - tuple[list[float], np.ndarray]:\n    \"\"\"Runs the GBM simulation for one variant (grad or newton).\"\"\"\n    N = len(y)\n    F = np.zeros(N, dtype=np.float64)\n    rel_decreases = []\n\n    for _ in range(M):\n        R_prev = logistic_risk(F, y)\n\n        p = sigmoid(F)\n        g = p - y\n\n        g_L = g[leaf_indices_L]\n        g_R = g[leaf_indices_R]\n\n        if method == 'grad':\n            v_L = -np.mean(g_L) if len(g_L) > 0 else 0.0\n            v_R = -np.mean(g_R) if len(g_R) > 0 else 0.0\n        else: # newton\n            h = p * (1 - p)\n            h_L = h[leaf_indices_L]\n            h_R = h[leaf_indices_R]\n            \n            sum_g_L = np.sum(g_L)\n            sum_h_L = np.sum(h_L)\n            sum_g_R = np.sum(g_R)\n            sum_h_R = np.sum(h_R)\n\n            v_L = -sum_g_L / sum_h_L if sum_h_L > 1e-9 else 0.0\n            v_R = -sum_g_R / sum_h_R if sum_h_R > 1e-9 else 0.0\n        \n        F[leaf_indices_L] += nu * v_L\n        F[leaf_indices_R] += nu * v_R\n\n        R_new = logistic_risk(F, y)\n        \n        rel_decr = (R_prev - R_new) / R_prev if R_prev > 1e-9 else 0.0\n        rel_decreases.append(rel_decr)\n\n    return rel_decreases, F\n\ndef process_case(case_data: dict) - str:\n    \"\"\"Processes a single test case and returns the formatted string result.\"\"\"\n    X = case_data['X']\n    y = case_data['y']\n    M = case_data['M']\n    nu = case_data['nu']\n    \n    N = len(X)\n    \n    tau = get_median_threshold(X)\n    indices = np.arange(N)\n    leaf_L_indices = indices[X = tau]\n    leaf_R_indices = indices[X > tau]\n\n    rel_decr_grad, F_final_grad = run_gbm_variant(X, y, M, nu, leaf_L_indices, leaf_R_indices, 'grad')\n    rel_decr_newt, F_final_newt = run_gbm_variant(X, y, M, nu, leaf_L_indices, leaf_R_indices, 'newton')\n\n    R0 = logistic_risk(np.zeros(N), y)\n    R_final_grad = logistic_risk(F_final_grad, y)\n    R_final_newt = logistic_risk(F_final_newt, y)\n\n    ops_total_grad = M * 4 * N\n    ops_total_newt = M * 7 * N\n\n    gain_grad = (R0 - R_final_grad) / (ops_total_grad / 1000.0) if ops_total_grad > 0 else 0.0\n    gain_newt = (R0 - R_final_newt) / (ops_total_newt / 1000.0) if ops_total_newt > 0 else 0.0\n\n    grad_list_str = f\"[{','.join([f'{v:.6f}' for v in rel_decr_grad])}]\"\n    newt_list_str = f\"[{','.join([f'{v:.6f}' for v in rel_decr_newt])}]\"\n    \n    case_result_str = (\n        f\"[{grad_list_str},\"\n        f\"{newt_list_str},\"\n        f\"{gain_grad:.6f},\"\n        f\"{gain_newt:.6f}]\"\n    )\n    \n    return case_result_str\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the final output.\n    \"\"\"\n    test_cases = [\n        {'X': np.array([-2.0, -1.0, 0.0, 1.0, 2.0]), 'y': np.array([0, 0, 0, 1, 1]), 'M': 5, 'nu': 0.5},\n        {'X': np.array([0.0, 0.0, 0.0, 0.0]), 'y': np.array([0, 1, 0, 1]), 'M': 8, 'nu': 0.3},\n        {'X': np.array([-1.5, -1.0, -0.5, 0.0, 0.5, 1.0, 1.5]), 'y': np.array([0, 0, 0, 1, 1, 1, 1]), 'M': 12, 'nu': 0.1},\n        {'X': np.array([-10.0, 0.0, 10.0]), 'y': np.array([0, 1, 1]), 'M': 1, 'nu': 0.8}\n    ]\n\n    all_results_str = [process_case(case) for case in test_cases]\n    \n    final_output = f\"[{','.join(all_results_str)}]\"\n    print(final_output)\n\nsolve()\n```", "id": "3125494"}, {"introduction": "现实世界的数据集通常充满噪声并包含不相关的特征。这个实践模拟了这样一个场景，让你观察梯度提升机在这些条件下的表现 [@problem_id:3125513]。通过追踪模型选择哪些特征进行分裂，你将凭经验验证学习率 $\\nu$ 的正则化效果，并体会到 GBM 执行特征选择的内在能力，这使其成为一个在实际应用中非常强大的算法。", "problem": "考虑在梯度提升机（GBM）中使用的二元分裂回归树，其中不纯度由平方误差和（SSE）衡量。此问题的基础是统计学习中的最小二乘风险最小化原则，该原则将预测的经验风险定义为 $$ \\mathcal{R}(F) = \\sum_{i=1}^{n} \\left(y_i - F(x_i)\\right)^2, $$ 其中 $F$ 是一个预测器，$x_i \\in \\mathbb{R}^d$ 是特征向量，$y_i \\in \\mathbb{R}$ 是目标值。最小二乘法的梯度提升通过迭代地将一个弱学习器 $h_m(x)$ 拟合到负梯度（即残差）来进行，对于最小二乘法，残差等于 $$ r_i^{(m)} = y_i - F_{m-1}(x_i), $$ 并通过以下方式更新预测器 $$ F_m(x) = F_{m-1}(x) + \\nu \\cdot h_m(x), $$ 其中 $\\nu \\in (0,1]$ 是控制步长的学习率（正则化参数）。\n\n在二元分裂回归树（决策树桩）中，给定第 $m$ 次迭代的残差 $\\{r_i\\}_{i=1}^n$，在特征 $j$ 和阈值 $\\tau$ 上的分裂将数据划分为左节点和右节点。对于任何集合 $S$（其中 $|S| = N$），使 SSE 最小化的节点预测是平均残差 $\\bar{r}_S = \\frac{1}{N} \\sum_{i \\in S} r_i$，该节点的 SSE 为 $$ \\mathrm{SSE}(S) = \\sum_{i \\in S} \\left(r_i - \\bar{r}_S\\right)^2。$$ 对于一个候选分裂 $(j,\\tau)$，不纯度下降为 $$ \\Delta(j,\\tau) = \\mathrm{SSE}(\\text{parent}) - \\left(\\mathrm{SSE}(\\text{left}) + \\mathrm{SSE}(\\text{right})\\right), $$ 并且选择的分裂是在所有特征和有效阈值中使 $\\Delta(j,\\tau)$ 最大化的分裂。\n\n假设数据生成过程有 $r$ 个相关特征 $\\{x_1,\\dots,x_r\\}$ 和 $p$ 个注入到特征集中的无关特征 $\\{z_1,\\dots,z_p\\}$。目标值由相关特征加噪声生成。在最小二乘提升下，无关特征在期望上与残差统计独立，因此它们的期望不纯度下降低于相关特征。通过 $\\nu$ 进行的正则化调节了更新的幅度，影响结构化残差分量被移除的速度以及后续分裂追逐噪声的频率。根据经验，在固定的迭代次数 $m$ 内，较小的 $\\nu$ 倾向于减少对无关特征进行分裂的频率和影响。\n\n您的任务是编写一个完整、可运行的程序，该程序：\n- 模拟一个包含 $n$ 个样本、 $r$ 个相关特征和 $p$ 个无关特征的数据集。从标准正态分布中独立生成相关特征 $\\{x_1,x_2\\}$，并从标准正态分布中独立生成 $\\{z_k\\}_{k=1}^p$，所有特征相互独立。通过一个与树分裂对齐的分段常数规则定义目标值：\n  $$ y = 2 \\cdot \\mathbb{I}[x_1  0] + 1.5 \\cdot \\mathrm{sign}(x_2) + \\epsilon, $$\n  其中 $\\epsilon \\sim \\mathcal{N}(0, 0.5^2)$，$\\mathrm{sign}(u) \\in \\{-1,0,1\\}$ 并约定 $\\mathrm{sign}(0)=0$。仅使用定义的首 $r=2$ 个相关特征；所有额外的 $p$ 个特征都是无关的。\n- 实现使用决策树桩的最小二乘 GBM，进行 $m$ 次迭代，学习率为 $\\nu$。在每次迭代中：\n  1. 计算残差 $r_i^{(m)} = y_i - F_{m-1}(x_i)$。\n  2. 对于每个特征 $j \\in \\{1,\\dots,r+p\\}$，考虑所有位于连续排序的唯一特征值之间的中点作为阈值。对于每个阈值，使用 SSE 定义计算 $\\Delta(j,\\tau)$，并为每个特征选择最佳阈值。\n  3. 选择在所有特征中具有最大不纯度下降 $\\Delta^\\star$ 的特征-阈值对 $(j^\\star, \\tau^\\star)$。将左右叶节点的预测定义为相应节点中的平均残差，并用 $\\nu$ 乘以该树桩的叶节点预测值来更新 $F_m(x)$。\n  4. 跟踪 $j^\\star$ 是否是注入的无关特征 $\\{z_k\\}$ 之一，并累积总不纯度下降 $\\Delta^\\star$ 以及归因于无关特征分裂的部分（当 $j^\\star$ 是无关特征时）。\n- 对于每个测试用例，返回两个量：\n  1. 无关特征被选中用于分裂的迭代次数所占的比例，即 $$ \\text{usage\\_fraction} = \\frac{\\#\\{\\text{iterations with } j^\\star \\in \\{z_k\\}\\}}{m}。$$\n  2. 归因于无关特征分裂的累积不纯度下降与所有分裂的总累积不纯度下降之比，即 $$ \\text{impurity\\_share\\_irrelevant} = \\frac{\\sum_{\\text{irrelevant splits}} \\Delta^\\star}{\\sum_{\\text{all splits}} \\Delta^\\star}。$$\n如果分数中的分母为零，则返回 $0$ 作为比率。\n\n使用以下测试套件来评估行为的不同方面：\n- 案例 A（无无关特征的基线）：$n=300$, $r=2$, $p=0$, $m=20$, $\\nu=0.1$，固定随机种子为 $1$。\n- 案例 B（中等数量的无关特征，中等学习率）：$n=300$, $r=2$, $p=10$, $m=20$, $\\nu=0.1$，固定随机种子为 $2$。\n- 案例 C（中等数量的无关特征，较小学习率）：$n=300$, $r=2$, $p=10$, $m=20$, $\\nu=0.01$，固定随机种子为 $3$。\n- 案例 D（边缘情况，单次迭代）：$n=300$, $r=2$, $p=10$, $m=1, \\nu=0.1$，固定随机种子为 $4$。\n- 案例 E（大量无关特征，更多迭代次数）：$n=300$, $r=2$, $p=50$, $m=30$, $\\nu=0.1$，固定随机种子为 $5$。\n\n您的程序应生成单行输出，其中包含用方括号括起来的逗号分隔列表形式的结果。每个测试用例的结果应为一个包含两个浮点数的列表，顺序为 $[\\text{usage\\_fraction}, \\text{impurity\\_share\\_irrelevant}]$。例如，最终输出格式必须为 $$ \\big[ [u_1, s_1], [u_2, s_2], \\dots \\big], $$ 并精确地以单行形式打印，如 \"[[u1,s1],[u2,s2],...]\"。不涉及物理单位或角度；所有比率均以小数表示。", "solution": "用户提供的问题已根据指定标准进行了分析和验证。\n\n### 第一步：提取已知条件\n- **模型框架**：使用二元分裂回归树（决策树桩）的梯度提升机（GBM）。\n- **优化原则**：最小二乘风险最小化，经验风险为 $\\mathcal{R}(F) = \\sum_{i=1}^{n} (y_i - F(x_i))^2$。\n- **提升算法**：迭代地将弱学习器 $h_m(x)$ 拟合到负梯度（残差）$r_i^{(m)} = y_i - F_{m-1}(x_i)$。\n- **模型更新规则**：$F_m(x) = F_{m-1}(x) + \\nu \\cdot h_m(x)$，其中 $\\nu$ 是学习率。\n- **弱学习器细节**：\n    - **分裂准则**：最大化由平方误差和（SSE）测量的的不纯度下降 $\\Delta(j,\\tau)$。\n    - **节点不纯度**：对于包含数据索引集 $S$ 的节点，$\\mathrm{SSE}(S) = \\sum_{i \\in S} (r_i - \\bar{r}_S)^2$，其中 $\\bar{r}_S$ 是节点中的平均残差。\n    - **不纯度下降**：$\\Delta(j,\\tau) = \\mathrm{SSE}(\\text{parent}) - (\\mathrm{SSE}(\\text{left}) + \\mathrm{SSE}(\\text{right}))$。\n    - **分裂阈值**：连续排序的唯一特征值之间的中点。\n- **数据生成**：\n    - $n$ 个样本，$r$ 个相关特征 $\\{x_j\\}_{j=1}^r$，$p$ 个无关特征 $\\{z_k\\}_{k=1}^p$。\n    - $x_j \\sim \\mathcal{N}(0, 1)$ 和 $z_k \\sim \\mathcal{N}(0, 1)$，所有特征相互独立。\n    - 目标值：$y = 2 \\cdot \\mathbb{I}[x_1  0] + 1.5 \\cdot \\mathrm{sign}(x_2) + \\epsilon$，其中 $\\epsilon \\sim \\mathcal{N}(0, 0.5^2)$，$r=2$，且 $\\mathrm{sign}(0)=0$。\n- **任务**：对于一组测试用例，实现 GBM 并计算两个指标：\n    1. `usage_fraction`：分裂发生在无关特征上的迭代比例。\n    2. `impurity_share_irrelevant`：由无关特征分裂产生的累积不纯度下降与总累积不纯度下降之比。\n- **测试用例**：\n    - A: $n=300, r=2, p=0, m=20, \\nu=0.1$, 种子=1。\n    - B: $n=300, r=2, p=10, m=20, \\nu=0.1$, 种子=2。\n    - C: $n=300, r=2, p=10, m=20, \\nu=0.01$, 种子=3。\n    - D: $n=300, r=2, p=10, m=1, \\nu=0.1$, 种子=4。\n    - E: $n=300, r=2, p=50, m=30, \\nu=0.1$, 种子=5。\n\n### 第二步：使用提取的已知条件进行验证\n- **科学性**：问题描述了标准的最小二乘梯度提升算法（LS-Boost）。残差作为负梯度、弱学习器、SSE 不纯度以及通过学习率进行正则化等概念是统计学习理论的基础。该设置在科学上是合理的。\n- **良构性**：问题是良构的。数据生成过程、算法步骤、参数和期望输出都已明确指定。对于每个给定的固定随机种子，测试用例都存在一个确定的解。一个微小的遗漏是初始模型 $F_0(x)$，但对于最小二乘回归，标准惯例是用最小化 SSE 的常数模型进行初始化，即目标变量的均值 $F_0(x) = \\bar{y}$。这是一个标准且无争议的假设。\n- **客观性**：问题以精确、客观的数学和算法语言陈述，没有主观论断。\n\n### 第三步：结论与行动\n该问题是**有效的**。它是一个在机器学习领域中明确定义的计算任务，需要忠实地实现一个标准算法。现在将开始解题过程。\n\n### 解法\n该问题要求实现一个以决策树桩为弱学习器的梯度提升机（GBM），以分析该算法在存在无关特征时的行为。我们将构建一个遵循指定数据生成过程和 GBM 算法的模拟。\n\n**1. 数据生成**\n对于每个具有参数 $n$、$r$、$p$ 和随机种子的测试用例，我们如下生成数据：\n- 共创建 $r+p$ 个特征。前 $r=2$ 个是相关的，随后的 $p$ 个是无关的。\n- 大小为 $n \\times (r+p)$ 的特征矩阵 $\\mathbf{X}$ 的值从标准正态分布 $\\mathcal{N}(0, 1)$ 中抽取。\n- 大小为 $n$ 的目标向量 $\\mathbf{y}$ 根据以下规则生成：\n$$y_i = 2 \\cdot \\mathbb{I}[x_{i,1}  0] + 1.5 \\cdot \\mathrm{sign}(x_{i,2}) + \\epsilon_i$$\n其中 $x_{i,1}$ 和 $x_{i,2}$ 是第 $i$ 个样本的两个相关特征，$\\mathbb{I}[\\cdot]$ 是指示函数，$\\epsilon_i \\sim \\mathcal{N}(0, 0.5^2)$ 是一个噪声项。\n\n**2. 梯度提升算法**\nGBM 是迭代构建的。\n\n**初始化**：必须选择初始模型 $F_0(\\mathbf{x})$。对于最小二乘损失函数，最优的常数预测是观测目标值的均值。因此，我们为所有样本 $i=1, \\dots, n$ 初始化模型为：\n$$F_0(\\mathbf{x}_i) = \\bar{y} = \\frac{1}{n} \\sum_{i=1}^n y_i$$\n\n**迭代**：模型进行 $m$ 次迭代更新。对于每次迭代 $k=1, \\dots, m$：\n\n**a. 计算残差**：SSE 损失函数关于模型预测 $F_{k-1}(\\mathbf{x}_i)$ 的负梯度是残差：\n$$r_i^{(k)} = y_i - F_{k-1}(\\mathbf{x}_i)$$\n\n**b. 拟合决策树桩**：训练一个决策树桩 $h_k(\\mathbf{x})$ 来预测残差 $r_i^{(k)}$。这涉及在所有特征和所有可能的阈值上找到最佳的二元分裂。一个分裂由一个特征索引 $j$ 和一个阈值 $\\tau$ 定义。分裂的质量由不纯度下降衡量，对于 SSE，即方差减少。\n\n包含一组残差索引 $S$ 的节点的不纯度是 $\\mathrm{SSE}(S) = \\sum_{i \\in S} (r_i - \\bar{r}_S)^2$。将父节点 $S_P$ 分裂为左节点 $S_L$ 和右节点 $S_R$ 的不纯度下降是 $\\Delta = \\mathrm{SSE}(S_P) - (\\mathrm{SSE}(S_L) + \\mathrm{SSE}(S_R))$。这可以更高效地计算。设 $T_S = \\sum_{i \\in S} r_i$ 且 $N_S = |S|$。SSE 可以写成 $\\mathrm{SSE}(S) = \\sum_{i \\in S} r_i^2 - T_S^2/N_S$。不纯度下降简化为：\n$$\\Delta = \\left( \\frac{T_L^2}{N_L} + \\frac{T_R^2}{N_R} \\right) - \\frac{T_P^2}{N_P}$$\n在我们的情况中，每次分裂的父节点都包含所有 $n$ 个样本。我们搜索使这个量 $\\Delta^\\star$ 最大化的特征 $j^\\star$ 和阈值 $\\tau^\\star$。\n\n搜索最优分裂 $(j^\\star, \\tau^\\star)$ 的过程是遍历每个特征 $j \\in \\{1, \\dots, r+p\\}$。对于每个特征，我们考虑其连续唯一排序值的中点作为阈值。一个寻找特征最佳阈值的有效方法是，将特征值与相应的残差一起排序，并使用运行总和来为每个潜在分裂点计算 $T_L$、$T_R$、$N_L$ 和 $N_R$。\n\n**c. 更新模型**：一旦找到最佳分裂 $(j^\\star, \\tau^\\star)$，决策树桩 $h_k(\\mathbf{x})$ 就被定义了。它在两个叶子中的预测是常数值，等于每个叶子中残差的均值：\n$$h_k(\\mathbf{x}) = \\begin{cases} \\bar{r}_{L}  \\text{if } x_j \\le \\tau^\\star \\\\ \\bar{r}_{R}  \\text{if } x_j  \\tau^\\star \\end{cases}$$\n然后，集成模型以大小为 $\\nu$ 的步长进行更新：\n$$F_k(\\mathbf{x}) = F_{k-1}(\\mathbf{x}) + \\nu \\cdot h_k(\\mathbf{x})$$\n\n**3. 性能指标**\n在 $m$ 次迭代中，我们跟踪选择的分裂特征 $j^\\star$ 和相应的不纯度下降 $\\Delta^\\star$。\n- 我们计算无关特征被选择的次数，$N_{\\text{irrelevant}} = \\sum_{k=1}^m \\mathbb{I}[j_k^\\star  r]$。\n- 我们累积总不纯度下降，$\\Delta_{\\text{total}} = \\sum_{k=1}^m \\Delta_k^\\star$。\n- 我们累积由无关分裂产生的不纯度下降，$\\Delta_{\\text{irrelevant}} = \\sum_{k=1}^m \\Delta_k^\\star \\cdot \\mathbb{I}[j_k^\\star  r]$。\n\n最后，我们计算所需的指标：\n- $\\text{usage\\_fraction} = N_{\\text{irrelevant}} / m$\n- $\\text{impurity\\_share\\_irrelevant} = \\Delta_{\\text{irrelevant}} / \\Delta_{\\text{total}}$（约定如果 $\\Delta_{\\text{total}} = 0$ 则为 $0$）。\n\n该实现将处理每个测试用例及其特定的参数和随机种子，得出两个指定的浮点数值。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the Gradient Boosting Machine simulation problem.\n    \"\"\"\n    test_cases = [\n        # (n, r, p, m, nu, seed)\n        (300, 2, 0, 20, 0.1, 1),   # Case A: baseline\n        (300, 2, 10, 20, 0.1, 2),  # Case B: moderate irrelevant, moderate nu\n        (300, 2, 10, 20, 0.01, 3), # Case C: moderate irrelevant, small nu\n        (300, 2, 10, 1, 0.1, 4),   # Case D: edge case, single iteration\n        (300, 2, 50, 30, 0.1, 5),  # Case E: many irrelevant, more iterations\n    ]\n\n    results = []\n    for case in test_cases:\n        n, r, p, m, nu, seed = case\n        \n        # Set random seed for reproducibility\n        rng = np.random.default_rng(seed)\n\n        # 1. Data Generation\n        num_features = r + p\n        X = rng.normal(size=(n, num_features))\n\n        # Target variable generation based on the first r=2 features\n        x1 = X[:, 0]\n        x2 = X[:, 1]\n        epsilon = rng.normal(loc=0, scale=0.5, size=n)\n        y = 2 * (x1 > 0) + 1.5 * np.sign(x2) + epsilon\n\n        # 2. GBM Implementation\n        # Initialization\n        F = np.full(n, np.mean(y))\n        \n        irrelevant_split_count = 0\n        total_impurity_decrease = 0.0\n        irrelevant_impurity_decrease = 0.0\n\n        for _ in range(m):\n            # Compute residuals\n            residuals = y - F\n            \n            # Find the best split\n            best_gain = -1.0\n            best_split_info = None\n\n            sum_residuals_parent = np.sum(residuals)\n            # This term is constant for the current iteration across all splits\n            parent_impurity_term = (sum_residuals_parent**2) / n\n\n            for j in range(num_features):\n                feature_values = X[:, j]\n                \n                sorted_indices = np.argsort(feature_values)\n                sorted_residuals = residuals[sorted_indices]\n                sorted_feature_values = feature_values[sorted_indices]\n                \n                running_sum_left = 0.0\n                \n                for i in range(n - 1):\n                    n_left = i + 1\n                    running_sum_left += sorted_residuals[i]\n                    \n                    if sorted_feature_values[i] == sorted_feature_values[i+1]:\n                        continue\n                    \n                    n_right = n - n_left\n                    \n                    sum_residuals_left = running_sum_left\n                    sum_residuals_right = sum_residuals_parent - sum_residuals_left\n                    \n                    gain = (sum_residuals_left**2) / n_left + \\\n                           (sum_residuals_right**2) / n_right - \\\n                           parent_impurity_term\n                    \n                    if gain > best_gain:\n                        best_gain = gain\n                        threshold = (sorted_feature_values[i] + sorted_feature_values[i+1]) / 2.0\n                        best_split_info = {\n                            \"feature_idx\": j,\n                            \"threshold\": threshold,\n                            \"gain\": gain,\n                            \"mean_res_left\": sum_residuals_left / n_left,\n                            \"mean_res_right\": sum_residuals_right / n_right,\n                        }\n            \n            if best_split_info is None:\n                break # No valid split found\n\n            # Update tracking variables\n            total_impurity_decrease += best_split_info[\"gain\"]\n            if best_split_info[\"feature_idx\"] >= r:\n                irrelevant_split_count += 1\n                irrelevant_impurity_decrease += best_split_info[\"gain\"]\n                \n            # Update the model F\n            feature_idx = best_split_info[\"feature_idx\"]\n            threshold = best_split_info[\"threshold\"]\n            \n            left_mask = X[:, feature_idx] = threshold\n            \n            F[left_mask] += nu * best_split_info[\"mean_res_left\"]\n            F[~left_mask] += nu * best_split_info[\"mean_res_right\"]\n\n        # 3. Calculate final metrics\n        usage_fraction = irrelevant_split_count / m if m > 0 else 0.0\n        \n        if total_impurity_decrease > 1e-9: # Use a small tolerance for floating point\n            impurity_share_irrelevant = irrelevant_impurity_decrease / total_impurity_decrease\n        else:\n            impurity_share_irrelevant = 0.0\n            \n        results.append([usage_fraction, impurity_share_irrelevant])\n\n    # Format the final output string exactly as required\n    result_strings = [f\"[{u},{s}]\" for u, s in results]\n    print(f\"[{','.join(result_strings)}]\")\n\nsolve()\n```", "id": "3125513"}, {"introduction": "学习率 $\\nu$ 和树的数量 $M$ 是 GBM 中两个最关键的超参数，并且它们紧密地交织在一起。本练习通过检验一个假设来探索它们之间的关系：如果乘积 $M\\nu$ 保持不变，学习过程将遵循相似的路径 [@problem_id:3125556]。这提供了一个实用的视角来审视更抽象的提升理论（作为一种函数梯度下降），为超参数调优提供了宝贵的见解。", "problem": "要求您设计并实现一个程序，利用函数梯度下降原理和平方误差损失下的经验风险最小化，来经验性地研究梯度提升机（GBM）中学习率 $\\nu$ 与基学习器（树）数量 $M$ 之间的相互作用。目标是保持乘积 $M \\nu$ 恒定，并比较不同 $\\nu$ 值下的训练损失轨迹 $R(f_m)$，以检验以下假设：当 $M \\nu$ 固定时，作为累积阶段 $t = m \\nu$ 的函数，训练损失会遵循相似的路径。\n\n请从以下基本概念出发：\n- 平方误差损失下的回归问题的经验风险最小化：对于一个模型 $f$ 和数据 $\\{(x_i, y_i)\\}_{i=1}^n$，经验风险为 $R(f) = \\frac{1}{n} \\sum_{i=1}^n L(y_i, f(x_i))$，其中损失函数为 $L(y, \\hat{y}) = \\frac{1}{2} (y - \\hat{y})^2$。\n- 函数梯度下降：这是一种迭代方法，通过遵循 $R$ 的负函数梯度来更新函数 $f_m$ 以减小 $R(f_m)$，该梯度在受限的函数类别（此处为决策树桩）中进行近似。\n\n您将实现用于回归的 GBM，并遵循以下约束：\n- 基学习器是决策树桩（深度为 1 的回归树）：选择单个特征索引 $j$ 和阈值 $\\tau$，如果 $x_j \\le \\tau$ 则预测一个常数 $c_\\ell$，否则预测 $c_r$。每个树桩必须通过最小二乘法拟合，以最小化当前伪残差的平方误差。对于任何固定的分裂，最优的区域预测值必须是该区域内残差的均值，并且分裂的选择必须最小化所有区域的平方误差总和。所有阈值必须选择为相邻排序后特征值的中点。\n- 初始模型必须对所有 $x$ 都为 $f_0(x) = 0$。\n\n需要检验的假设是：对于固定的 $T = M \\nu$，当 $m$ 足够大且 $\\nu$ 足够小以模拟连续时间上的梯度流时，作为 $t = m \\nu$ 的函数的训练损失轨迹 $R(f_m)$ 在不同 $\\nu$ 方案下是相似的。您必须通过将每个轨迹插值到 $[0, T]$ 上的一个公共时间值网格上，并计算该网格上轨迹间的最大绝对偏差来操作化地定义相似性。\n\n请实现以下实验设置。\n\n数据生成：\n- 所有特征必须是 $[0, 1]$ 上的独立同分布的均匀随机变量。为确保可复现性，请为每个测试用例显式设置随机种子。\n- 标签 $y$ 必须由特征的光滑函数或分段函数生成，并加上均值为 $0$、标准差为 $\\sigma$ 的独立高斯噪声。\n\n模型和损失：\n- 在概念推导中使用平方误差损失 $L(y, \\hat{y}) = \\frac{1}{2} (y - \\hat{y})^2$，并在每个阶段报告 $R(f_m) = \\frac{1}{n} \\sum_{i=1}^n (y_i - f_m(x_i))^2$ 作为经验轨迹。\n\n实验参数：\n- 固定累积阶段范围 $T = M \\nu = 2.0$。\n- 使用学习率 $\\nu \\in \\{0.2, 0.1, 0.05, 0.01\\}$，并为每个选择设置 $M = T / \\nu$ 以保持 $M \\nu$ 恒定。\n- 将每个损失轨迹插值到一个在 $[0, T]$ 区间内包含 $201$ 个点的均匀时间值网格上。\n\n相似性度量：\n- 对于给定的数据集和四种 $(\\nu, M)$ 设置，计算公共网格上的插值损失曲线。然后计算所有插值曲线对之间的最大绝对偏差，即 $\\max_{a  b} \\max_{t \\in \\text{grid}} |R_a(t) - R_b(t)|$。\n\n测试套件：\n- 提供三个独立的测试用例，涵盖不同的情况：\n    1. 理想情况：$n = 200$, $d = 2$, seed $= 0$, $\\sigma = 0.1$, $y = \\sin(2 \\pi x_1) + x_2^2 + \\varepsilon$，其中 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2)$，且 $x_1, x_2 \\sim \\text{Uniform}(0, 1)$。\n    2. 更高噪声的情况：$n = 200$, $d = 2$, seed $= 1$, $\\sigma = 0.5$, $y = \\sin(2 \\pi x_1) + x_2^2 + \\varepsilon$，特征分布同上。\n    3. 分段目标函数：$n = 200$, $d = 1$, seed $= 2$, $\\sigma = 0.05$, $y = \\mathbb{I}\\{x_1  0.5\\} + \\varepsilon$，其中 $\\mathbb{I}\\{\\cdot\\}$ 是指示函数。\n\n输出规格：\n- 对每个测试用例，计算等于上述定义的最大绝对偏差的单个浮点数。\n- 您的程序应生成单行输出，其中包含这三个结果，四舍五入到六位小数，并以逗号分隔列表的形式用方括号括起来（例如，“[r1,r2,r3]”）。\n\n不应读取任何用户输入。所有随机种子、数据集大小和参数必须完全按照规定进行硬编码。给定所述种子，程序必须是完全自包含且确定性的。不涉及角度；不涉及物理单位。", "solution": "本解答为回归问题提供了一个梯度提升机（GBM）的详细解释和实现，旨在当乘积 $M\\nu$ 保持恒定时，经验性地研究学习率 $\\nu$ 和估计器数量 $M$ 之间的关系。该框架植根于函数梯度下降的原理。\n\n### 1. 理论框架：函数梯度下降\n\n梯度提升机是一类集成学习方法，它通过顺序添加弱学习器来构建一个强大的预测模型。其核心思想可以解释为在函数空间中执行梯度下降。\n\n目标是找到一个函数 $f(x)$，使其在训练数据 $\\{(x_i, y_i)\\}_{i=1}^n$ 上的经验风险最小化，经验风险定义为损失的平均值：\n$$\nR(f) = \\frac{1}{n} \\sum_{i=1}^n L(y_i, f(x_i))\n$$\n对于本问题，指定的损失函数是按 $\\frac{1}{2}$ 因子缩放的平方误差损失：\n$$\nL(y, \\hat{y}) = \\frac{1}{2} (y - \\hat{y})^2\n$$\nGBM 算法是迭代的。它从一个初始模型 $f_0(x)$ 开始，并顺序地更新它：\n$$\nf_m(x) = f_{m-1}(x) + v_m(x)\n$$\n其中 $v_m(x)$ 是一个被选择用来使风险 $R(f)$ “最速下降”的函数。在函数梯度下降中，对于每个数据点 $i$，理想的更新 $v_m(x_i)$ 与损失函数关于函数值 $f(x_i)$ 的负梯度成正比，该梯度在当前模型 $f_{m-1}(x_i)$ 处计算。\n\n对于单个数据点 $(x_i, y_i)$，负梯度为：\n$$\nr_{im} = - \\left[ \\frac{\\partial L(y_i, F)}{\\partial F} \\right]_{F=f_{m-1}(x_i)} = - \\frac{1}{2} \\cdot 2 \\cdot (f_{m-1}(x_i) - y_i) \\cdot (-1) = y_i - f_{m-1}(x_i)\n$$\n这些量 $r_{im}$ 被称为第 $m$ 次迭代的伪残差。理想的更新函数 $v_m(x)$ 应满足对所有 $i$ 都有 $v_m(x_i) = r_{im}$。然而，我们将更新函数限制在一个受限的“弱学习器”类别中，在本例中是决策树桩。\n\n因此，在每个阶段 $m$，我们拟合一个基学习器 $h_m(x)$ 到当前的伪残差上。也就是说，我们解决以下优化问题：\n$$\nh_m = \\arg\\min_{h \\in \\mathcal{H}} \\sum_{i=1}^n (r_{im} - h(x_i))^2\n$$\n其中 $\\mathcal{H}$ 是所有可能的决策树桩的类别。\n\n一旦找到最佳的基学习器 $h_m$，模型就通过一个缩减步长进行更新：\n$$\nf_m(x) = f_{m-1}(x) + \\nu h_m(x)\n$$\n这里，$\\nu \\in (0, 1]$ 是学习率，它控制每个弱学习器对最终模型的贡献。\n\n### 2. 算法实现\n\n这里具体实现的算法遵循以下步骤：\n\n1.  **初始化**：按照规定，初始模型设置为 $f_0(x) = 0$。数据点数量为 $n$，特征数量为 $d$。\n\n2.  **迭代**：对于 $m = 1, 2, \\dots, M$：\n    a.  **计算伪残差**：对每个样本 $i=1, \\dots, n$，计算伪残差 $r_{im} = y_i - f_{m-1}(x_i)$。\n    b.  **拟合基学习器**：通过最小化平方误差和，将一个决策树桩 $h_m(x)$ 拟合到训练集 $\\{(x_i, r_{im})\\}_{i=1}^n$ 上。\n    c.  **更新模型**：更新集成模型：对所有 $i$，$f_m(x_i) = f_{m-1}(x_i) + \\nu h_m(x_i)$。\n    d.  **记录损失**：计算并存储训练风险 $R(f_m) = \\frac{1}{n} \\sum_{i=1}^n (y_i - f_m(x_i))^2$。\n\n### 3. 基学习器：决策树桩\n\n决策树桩是深度为 1 的回归树。它沿一个特征轴用一次切割来划分特征空间。\n-   一个树桩由一个特征索引 $j \\in \\{1, \\dots, d\\}$、一个阈值 $\\tau$ 以及两个对应区域的常数值定义：左叶节点的 $c_\\ell$（$x_j \\le \\tau$）和右叶节点的 $c_r$（$x_j  \\tau$）。\n-   为了找到给定伪残差集 $\\{r_i\\}$ 的最佳树桩，我们遍历所有可能的特征 $j$ 和所有有效的阈值 $\\tau$。\n-   对于每个特征 $j$，潜在的阈值 $\\tau$ 是该特征排序后相邻值的中点。\n-   对于给定的分裂 $(j, \\tau)$，数据被划分为左集合 $I_\\ell = \\{i \\mid x_{ij} \\le \\tau\\}$ 和右集合 $I_r = \\{i \\mid x_{ij}  \\tau\\}$。\n-   对于此分裂，最小化平方误差的最优预测值 $c_\\ell$ 和 $c_r$ 是每个区域内残差的均值：\n    $$\n    c_\\ell = \\frac{1}{|I_\\ell|} \\sum_{i \\in I_\\ell} r_i \\quad \\text{和} \\quad c_r = \\frac{1}{|I_r|} \\sum_{i \\in I_r} r_i\n    $$\n-   分裂的质量通过平方误差总和（SSE）来衡量：\n    $$\n    \\text{SSE}(j, \\tau) = \\sum_{i \\in I_\\ell} (r_i - c_\\ell)^2 + \\sum_{i \\in I_r} (r_i - c_r)^2\n    $$\n-   算法选择产生最小 SSE 的特征 $j^*$ 和阈值 $\\tau^*$。最终的树桩 $h(x)$ 由 $(j^*, \\tau^*, c_\\ell^*, c_r^*)$ 定义。一种高效的实现方法是按特征值对数据进行排序，并使用累加和来计算所有可能分裂的 SSE，每个特征的时间复杂度为 $O(n \\log n)$。\n\n### 4. 实验设计与分析\n\n该实验旨在检验这样一个假设：对于固定的累积“时间” $T = M\\nu$，只要 $\\nu$ 足够小且 $M$ 足够大，训练损失轨迹就与 $\\nu$ 和 $M$ 的具体选择无关。这反映了离散提升更新收敛到连续梯度流的现象。\n\n-   **数据生成**：对于每个测试用例，使用固定的随机种子，根据指定的底层函数、特征分布和噪声水平生成大小为 $n=200$ 的数据集，以确保可复现性。\n-   **参数方案**：总“提升时间”固定为 $T = 2.0$。我们测试四种学习率：$\\nu \\in \\{0.2, 0.1, 0.05, 0.01\\}$。相应的提升迭代次数为 $M = T/\\nu$，即 $\\{10, 20, 40, 200\\}$。\n-   **损失轨迹**：对于每对 $(\\nu, M)$，训练 GBM，并在每一步 $m=0, \\dots, M$ 记录训练风险 $R(f_m)$。这得到一个损失值序列。我们将这些值映射到类时变量 $t_m = m \\nu$，从而得到一组点 $(t_m, R(f_m))$。\n-   **轨迹比较**：为了比较四个损失轨迹，必须在同一个公共网格上对它们进行评估。我们为 $t \\in [0, T]$ 定义一个包含 $201$ 个点的均匀网格。每个经验损失轨迹都使用线性插值到这个网格上。\n-   **相似性度量**：轨迹之间的差异性由最大绝对偏差来量化。我们计算所有插值曲线对的此偏差，并报告总体的最大值：\n    $$\n    \\text{MaxDev} = \\max_{a, b \\in \\{\\nu_1, \\dots, \\nu_4\\}, a  b} \\left( \\max_{t \\in \\text{grid}} |R_{\\text{interp}, a}(t) - R_{\\text{interp}, b}(t)| \\right)\n    $$\n这个过程对三个不同的测试用例重复进行，以在不同的数据生成过程下评估该假设。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the GBM experiment and produce the final output.\n    \"\"\"\n\n    class DecisionStump:\n        \"\"\"\n        A regression tree of depth 1 (a decision stump).\n        \"\"\"\n        def __init__(self):\n            self.feature_index = None\n            self.threshold = None\n            self.left_value = None\n            self.right_value = None\n            self.min_sse = float('inf')\n\n        def fit(self, X, y):\n            \"\"\"\n            Finds the best split (feature and threshold) to minimize SSE.\n            \"\"\"\n            n_samples, n_features = X.shape\n            if n_samples = 1:\n                return\n\n            for j in range(n_features):\n                feature_values = X[:, j]\n                sorted_indices = np.argsort(feature_values)\n                y_sorted = y[sorted_indices]\n\n                # Efficiently calculate SSE using running sums\n                sum_y_right = np.sum(y_sorted)\n                sum_sq_y_right = np.sum(y_sorted**2)\n                n_right = n_samples\n                \n                sum_y_left = 0.0\n                sum_sq_y_left = 0.0\n                n_left = 0\n\n                for i in range(n_samples - 1):\n                    val = y_sorted[i]\n                    n_left += 1\n                    n_right -= 1\n                    sum_y_left += val\n                    sum_y_right -= val\n                    sum_sq_y_left += val**2\n                    sum_sq_y_right -= val**2\n\n                    # Only consider splits between unique feature values\n                    if feature_values[sorted_indices[i]] == feature_values[sorted_indices[i+1]]:\n                        continue\n\n                    # SSE = sum(y^2) - (sum(y))^2 / n\n                    sse_left = sum_sq_y_left - (sum_y_left**2) / n_left\n                    sse_right = sum_sq_y_right - (sum_y_right**2) / n_right\n                    total_sse = sse_left + sse_right\n\n                    if total_sse  self.min_sse:\n                        self.min_sse = total_sse\n                        self.feature_index = j\n                        self.threshold = (feature_values[sorted_indices[i]] + feature_values[sorted_indices[i+1]]) / 2.0\n                        self.left_value = sum_y_left / n_left\n                        self.right_value = sum_y_right / n_right\n            \n            # If no split was found (e.g., all X values are identical), predict the mean\n            if self.feature_index is None:\n                self.left_value = np.mean(y)\n                self.right_value = np.mean(y)\n\n\n        def predict(self, X):\n            \"\"\"\n            Makes predictions using the fitted stump.\n            \"\"\"\n            if self.feature_index is None:\n                return np.full(X.shape[0], self.left_value)\n\n            predictions = np.zeros(X.shape[0])\n            left_indices = X[:, self.feature_index] = self.threshold\n            right_indices = ~left_indices\n            predictions[left_indices] = self.left_value\n            predictions[right_indices] = self.right_value\n            return predictions\n\n    def generate_data(n, d, seed, sigma, target_func_type):\n        \"\"\"\n        Generates data for a specific test case.\n        \"\"\"\n        rng = np.random.default_rng(seed)\n        X = rng.uniform(0, 1, size=(n, d))\n        noise = rng.normal(0, sigma, size=n)\n        \n        if target_func_type == 'smooth':\n            y = np.sin(2 * np.pi * X[:, 0]) + X[:, 1]**2 + noise\n        elif target_func_type == 'piecewise':\n            y = (X[:, 0] > 0.5).astype(float) + noise\n        else:\n            raise ValueError(\"Unknown target function type.\")\n            \n        return X, y\n\n    test_cases = [\n        {'n': 200, 'd': 2, 'seed': 0, 'sigma': 0.1, 'target_func_type': 'smooth'},\n        {'n': 200, 'd': 2, 'seed': 1, 'sigma': 0.5, 'target_func_type': 'smooth'},\n        {'n': 200, 'd': 1, 'seed': 2, 'sigma': 0.05, 'target_func_type': 'piecewise'},\n    ]\n\n    results = []\n    \n    for case_params in test_cases:\n        X, y = generate_data(**case_params)\n        \n        T = 2.0\n        nu_values = [0.2, 0.1, 0.05, 0.01]\n        t_grid = np.linspace(0, T, 201)\n        \n        interpolated_curves = []\n\n        for nu in nu_values:\n            M = int(np.round(T / nu))\n            \n            # This array will store the model's predictions f_m(x_i)\n            F = np.zeros_like(y)\n            losses = []\n            \n            # Initial model f_0 = 0, so initial loss is mean of y^2\n            initial_loss = np.mean((y - F)**2) # Correctly calculates for F=0\n            losses.append(initial_loss)\n            \n            for _ in range(M):\n                residuals = y - F\n                stump = DecisionStump()\n                stump.fit(X, residuals)\n                h_m_predictions = stump.predict(X)\n                F += nu * h_m_predictions\n                current_loss = np.mean((y - F)**2)\n                losses.append(current_loss)\n            \n            # Create time points for the loss trajectory\n            t_m = np.arange(M + 1) * nu\n            \n            # Interpolate the loss curve onto the common grid\n            interp_loss = np.interp(t_grid, t_m, losses)\n            interpolated_curves.append(interp_loss)\n        \n        # Calculate the maximum absolute deviation across all pairs of curves\n        max_dev = 0.0\n        num_curves = len(interpolated_curves)\n        for i in range(num_curves):\n            for j in range(i + 1, num_curves):\n                dev = np.max(np.abs(interpolated_curves[i] - interpolated_curves[j]))\n                if dev > max_dev:\n                    max_dev = dev\n        \n        results.append(max_dev)\n\n    # Format the final output string\n    output_str = f\"[{','.join([f'{r:.6f}' for r in results])}]\"\n    print(output_str)\n\nsolve()\n```", "id": "3125556"}]}