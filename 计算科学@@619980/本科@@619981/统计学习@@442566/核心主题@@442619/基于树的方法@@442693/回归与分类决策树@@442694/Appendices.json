{"hands_on_practices": [{"introduction": "决策树的强大之处在于它能捕捉线性模型无法处理的非线性数据关系。第一个实践将引导你处理一个经典案例——异或（XOR）问题，你将亲手构建一个决策树并将其性能与线性分类器进行对比，从而直观地理解树模型的独特优势。[@problem_id:3113048]", "problem": "构建一个程序，该程序模拟具有异或（XOR）结构的二元分类数据，并比较深度为 $2$ 的决策树分类器与线性最小二乘分类器的性能。数据生成过程定义如下。从单位区间上的连续均匀分布中独立抽取特征 $x_1$ 和 $x_2$，即 $x_1 \\sim \\text{Uniform}(0,1)$ 和 $x_2 \\sim \\text{Uniform}(0,1)$。对于给定的阈值 $a \\in (0,1)$ 和 $b \\in (0,1)$，通过异或规则定义标签\n$$\nY = \\mathbf{1}\\big((x_1 > a) \\oplus (x_2 > b)\\big),\n$$\n其中 $\\mathbf{1}(\\cdot)$ 是指示函数，$\\oplus$ 表示逻辑异或。以比率 $\\eta \\in [0,1/2)$ 的对称标签噪声独立地破坏标签，即以概率 $\\eta$ 翻转 $Y$。\n\n从基本原理实现两个分类器：\n\n- 线性最小二乘分类器：选择系数 $\\mathbf{w} \\in \\mathbb{R}^3$ 以最小化训练集上线性得分 $f(\\mathbf{x}) = w_0 + w_1 x_1 + w_2 x_2$ 与 $\\{0,1\\}$ 中的二元标签之间的经验均方误差。通过 $\\hat{Y} = \\mathbf{1}(f(\\mathbf{x}) \\ge 0.5)$进行分类。报告在一个独立抽取的测试集上的错分率。\n\n- 一个最大深度为 $2$ 的决策树分类器，使用轴对齐分裂，并采用基尼不纯度贪心地构建。在任何包含标签多重集的节点上，将基尼不纯度定义为\n$$\nG = 1 - \\sum_{k \\in \\{0,1\\}} p_k^2,\n$$\n其中 $p_k$ 是该节点上类别 $k$ 的经验比例。对于在特征 $j \\in \\{1,2\\}$ 和阈值 $t$ 上的候选分裂，将节点划分为左子节点 $\\{x_j \\le t\\}$ 和右子节点 $\\{x_j > t\\}$，并选择使子节点不纯度的加权平均值最小化的分裂。将候选阈值限制在节点上观测到的连续排序唯一特征值之间的中点。当达到最大深度 $2$ 或节点为纯时，停止分裂。在叶节点上预测多数类，若平局则偏向类别 $0$。如果多个分裂产生相同的不纯度，则通过选择较小的特征索引来打破平局；如果仍然平局，则选择较小的阈值。\n\n对于下面的每个测试用例，您必须：\n\n- 使用指定的参数 $(a,b,\\eta)$ 和用于抽样的独立随机种子，生成一个大小为 $n_{\\text{train}}$ 的训练集和一个大小为 $n_{\\text{test}}$ 的独立测试集。使用提供的确切种子以确保可复现性。\n\n- 仅使用训练集来训练两个模型。\n\n- 将每个模型的测试错分率计算为测试集上 $\\hat{Y} \\ne Y$ 的经验比例。\n\n- 对于每个测试用例，以实数形式返回对 $[\\text{error}_{\\text{linear}}, \\text{error}_{\\text{tree}}]$。\n\n测试套件（每行表示 $(n_{\\text{train}}, n_{\\text{test}}, a, b, \\eta, \\text{seed}_{\\text{train}}, \\text{seed}_{\\text{test}})$）：\n\n- 案例 1：$(400, 5000, 0.5, 0.5, 0.0, 42, 4242)$。\n- 案例 2：$(400, 5000, 0.2, 0.8, 0.0, 1, 11)$。\n- 案例 3：$(2000, 5000, 0.5, 0.5, 0.1, 7, 77)$。\n- 案例 4：$(30, 5000, 0.5, 0.5, 0.0, 2024, 2025)$。\n\n最终输出格式：您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，列表中的每个元素本身是对应于各个案例的双元素列表 $[\\text{error}_{\\text{linear}}, \\text{error}_{\\text{tree}}]$。例如，输出应如下所示\n$$\n[[e_1^{\\text{lin}}, e_1^{\\text{tree}}],[e_2^{\\text{lin}}, e_2^{\\text{tree}}],[e_3^{\\text{lin}}, e_3^{\\text{tree}}],[e_4^{\\text{lin}}, e_4^{\\text{tree}}]]\n$$\n其中每个 $e_i^{\\cdot}$ 均为实数。不应打印其他文本。", "solution": "该问题要求在一个表现出异或（XOR）结构的合成数据集上，实现并比较两种不同的分类算法。该问题陈述的有效性已得到确认，因为它在科学上基于统计学习理论，通过特定的数据生成协议和确定性的算法定义而定义明确，并且其表述是客观的。我们将继续提供完整的解决方案。\n\n问题的核心在于数据生成过程。特征 $x_1$ 和 $x_2$ 从区间 $[0,1]$ 上的独立均匀分布中抽取。二元标签 $Y$ 由这些特征是否分别超过给定阈值 $a$ 和 $b$ 的异或运算定义：$Y = \\mathbf{1}\\big((x_1 > a) \\oplus (x_2 > b)\\big)$。此规则将单位正方形特征空间划分为四个象限，标签在 $0$ 和 $1$ 之间交替。这种结构不是线性可分的，对线性模型构成了经典的挑战。通过以指定的概率 $\\eta$ 翻转真实标签 $Y$ 来引入标签噪声。\n\n我们将构建和评估两种分类器：线性最小二乘分类器和深度为 $2$ 的决策树。\n\n**1. 线性最小二乘分类器**\n\n该方法将线性回归模型重新用于分类任务。模型假设特征与得分函数之间存在线性关系，$f(\\mathbf{x}) = w_0 + w_1 x_1 + w_2 x_2$。对于一个训练集 $\\{(\\mathbf{x}_i, y_i)\\}_{i=1}^{n_{\\text{train}}}$，其中 $\\mathbf{x}_i = (x_{i1}, x_{i2})$ 且 $y_i \\in \\{0, 1\\}$，选择系数 $\\mathbf{w} = (w_0, w_1, w_2)^T$ 来最小化经验均方误差（MSE）：\n$$\n\\text{MSE}(\\mathbf{w}) = \\frac{1}{n_{\\text{train}}} \\sum_{i=1}^{n_{\\text{train}}} (y_i - f(\\mathbf{x}_i))^2\n$$\n这是一个标准的普通最小二乘法（OLS）问题。通过定义一个大小为 $n_{\\text{train}} \\times 3$ 的设计矩阵 $\\mathbf{X}_b$，其中第 $i$ 行为 $(1, x_{i1}, x_{i2})$，目标可以写成向量形式，即最小化 $\\|\\mathbf{y} - \\mathbf{X}_b \\mathbf{w}\\|_2^2$。通过求解正规方程找到最优系数向量 $\\hat{\\mathbf{w}}$，从而得到闭式解：\n$$\n\\hat{\\mathbf{w}} = (\\mathbf{X}_b^T \\mathbf{X}_b)^{-1} \\mathbf{X}_b^T \\mathbf{y}\n$$\n其中假设 $\\mathbf{X}_b^T \\mathbf{X}_b$ 是可逆的。一旦从训练数据中确定了 $\\hat{\\mathbf{w}}$，就可以通过对线性得分进行阈值处理来对新数据点进行预测。问题指定了 $0.5$ 的阈值：\n$$\n\\hat{Y} = \\mathbf{1}(f(\\mathbf{x}) \\ge 0.5) = \\mathbf{1}(\\hat{w}_0 + \\hat{w}_1 x_1 + \\hat{w}_2 x_2 \\ge 0.5)\n$$\n\n**2. 决策树分类器**\n\n决策树分类器是一种非线性模型，它将特征空间划分为多个超矩形，并为每个超矩形分配一个类别标签。决策树从根节点开始自上而下地贪心构建。在每个节点，算法寻找最佳的轴对齐分裂，将数据分成两个子节点。\n\n“最佳”分裂被定义为能最大程度减少不纯度的分裂。这里使用的不纯度度量是基尼不纯度，对于一个包含标签多重集的节点，其经验类别比例为 $p_k$（$k \\in \\{0,1\\}$），定义为：\n$$\nG = 1 - \\sum_{k \\in \\{0,1\\}} p_k^2 = 1 - (p_0^2 + p_1^2)\n$$\n基尼不纯度为 $0$ 表示一个纯节点（所有样本都属于同一个类别）。对于在特征 $j$ 和阈值 $t$ 上的候选分裂，父节点 $S$ 处的数据被划分为左子节点 $S_L = \\{\\mathbf{x_i} \\in S \\mid x_{ij} \\le t\\}$ 和右子节点 $S_R = \\{\\mathbf{x_i} \\in S \\mid x_{ij} > t\\}$。分裂的质量由子节点的基尼不纯度的加权平均值来衡量：\n$$\nI_{\\text{split}} = \\frac{|S_L|}{|S|} G(S_L) + \\frac{|S_R|}{|S|} G(S_R)\n$$\n算法会穷举搜索所有特征 $j \\in \\{1,2\\}$ 和所有有效的候选阈值 $t$，以找到最小化 $I_{\\text{split}}$ 的分裂。候选阈值被限制在当前节点上观测到的特征的连续唯一排序值之间的中点。\n\n树的构建过程遵循特定的规则：\n- **最大深度**：一旦树达到深度 $2$，就停止分裂。深度为 $0$ 的树是一个单独的根节点；深度为 $2$ 的树有一个根节点、其子节点以及其孙子节点（必须是叶节点）。\n- **纯度**：如果节点是纯的，该节点的分裂也会停止。\n- **预测**：在叶节点上，预测的类别是该节点中样本的多数类。平局则偏向类别 $0$。\n- **分裂的平局打破规则**：如果多个分裂产生相同的最小基尼不纯度，首先选择特征索引较小的分裂（$x_1$ 优先于 $x_2$）来打破平局；如果仍然平局，则选择阈值较小的分裂。\n\n**评估**\n\n对于每个指定的测试用例，我们将使用提供的参数 $(a,b,\\eta)$ 和随机种子生成一个大小为 $n_{\\text{train}}$ 的训练集和一个大小为 $n_{\\text{test}}$ 的独立测试集。两个模型都将在训练数据上进行训练。它们的性能将通过计算测试集上的错分率（即不正确预测的比例）来评估。每个用例的最终输出将是错分率对 $[\\text{error}_{\\text{linear}}, \\text{error}_{\\text{tree}}]$。", "answer": "```python\nimport numpy as np\n\ndef generate_data(n_samples, a, b, eta, seed):\n    \"\"\"\n    Generates synthetic XOR data with label noise.\n    \n    Args:\n        n_samples (int): Number of data points to generate.\n        a (float): Threshold for feature x1.\n        b (float): Threshold for feature x2.\n        eta (float): Symmetric label noise rate.\n        seed (int): Random seed for reproducibility.\n        \n    Returns:\n        tuple[np.ndarray, np.ndarray]: A tuple containing features (X) and labels (Y).\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    X = rng.uniform(0, 1, size=(n_samples, 2))\n    x1, x2 = X[:, 0], X[:, 1]\n    \n    # True labels based on the XOR rule\n    y_true = np.logical_xor(x1 > a, x2 > b).astype(int)\n    \n    # Introduce symmetric label noise\n    noise_mask = rng.random(n_samples)  eta\n    y_noisy = y_true.copy()\n    y_noisy[noise_mask] = 1 - y_noisy[noise_mask]\n    \n    return X, y_noisy\n\nclass LinearLeastSquaresClassifier:\n    \"\"\"\n    A linear classifier trained by minimizing mean squared error.\n    \"\"\"\n    def __init__(self):\n        self.w = None\n\n    def fit(self, X_train, y_train):\n        \"\"\"\n        Fits the linear model using the normal equations.\n        \n        Args:\n            X_train (np.ndarray): Training features.\n            y_train (np.ndarray): Training labels.\n        \"\"\"\n        X_b = np.c_[np.ones(X_train.shape[0]), X_train]\n        try:\n            # Solve (X_b^T X_b) w = X_b^T y\n            XtX = X_b.T @ X_b\n            XtX_inv = np.linalg.inv(XtX)\n            self.w = XtX_inv @ X_b.T @ y_train\n        except np.linalg.LinAlgError:\n            # Use pseudo-inverse if matrix is singular\n            self.w = np.linalg.pinv(X_b) @ y_train\n\n    def predict(self, X):\n        \"\"\"\n        Predicts labels for new data.\n        \n        Args:\n            X (np.ndarray): Features of data to predict.\n            \n        Returns:\n            np.ndarray: Predicted binary labels {0, 1}.\n        \"\"\"\n        if self.w is None:\n            raise RuntimeError(\"Model has not been trained yet.\")\n        X_b = np.c_[np.ones(X.shape[0]), X]\n        scores = X_b @ self.w\n        return (scores >= 0.5).astype(int)\n\nclass DecisionTreeClassifier:\n    \"\"\"\n    A decision tree classifier with Gini impurity and max depth.\n    \"\"\"\n    class Node:\n        def __init__(self, feature_index=None, threshold=None, left=None, right=None, *, value=None):\n            self.feature_index = feature_index\n            self.threshold = threshold\n            self.left = left\n            self.right = right\n            self.value = value\n        \n        def is_leaf(self):\n            return self.value is not None\n\n    def __init__(self, max_depth=2):\n        self.max_depth = max_depth\n        self.root = None\n        self.n_classes_ = 2 # Fixed for this problem\n\n    def _gini(self, y):\n        \"\"\"Calculates Gini impurity.\"\"\"\n        if y.size == 0:\n            return 0.0\n        p = np.bincount(y, minlength=self.n_classes_) / y.size\n        return 1 - np.sum(p**2)\n\n    def _majority_vote(self, y):\n        \"\"\"Predicts class, breaking ties in favor of 0.\"\"\"\n        counts = np.bincount(y, minlength=self.n_classes_)\n        return 0 if counts[0] >= counts[1] else 1\n\n    def _find_best_split(self, X, y):\n        \"\"\"Finds the best split for a node.\"\"\"\n        n_samples, n_features = X.shape\n        if n_samples = 1:\n            return None\n\n        parent_gini = self._gini(y)\n        best_gini = parent_gini\n        best_split = None\n\n        for feature_idx in range(n_features):\n            unique_vals = np.unique(X[:, feature_idx])\n            if unique_vals.size = 1:\n                continue\n            \n            thresholds = (unique_vals[:-1] + unique_vals[1:]) / 2.0\n\n            for t in thresholds:\n                left_indices = X[:, feature_idx] = t\n                right_indices = ~left_indices\n                \n                y_left, y_right = y[left_indices], y[right_indices]\n                \n                if y_left.size == 0 or y_right.size == 0:\n                    continue\n\n                p_left = y_left.size / n_samples\n                p_right = y_right.size / n_samples\n                \n                weighted_gini = p_left * self._gini(y_left) + p_right * self._gini(y_right)\n\n                # Tie-breaking logic as per problem description\n                if weighted_gini  best_gini:\n                    best_gini = weighted_gini\n                    best_split = (feature_idx, t)\n                elif weighted_gini == best_gini:\n                    if best_split is not None:\n                        if feature_idx  best_split[0]:\n                            best_split = (feature_idx, t)\n                        elif feature_idx == best_split[0] and t  best_split[1]:\n                            best_split = (feature_idx, t)\n\n        return best_split\n\n    def _build_tree(self, X, y, depth):\n        \"\"\"Recursively builds the decision tree.\"\"\"\n        is_pure = len(np.unique(y)) == 1\n        is_max_depth = depth >= self.max_depth\n\n        if is_pure or is_max_depth:\n            leaf_value = self._majority_vote(y)\n            return self.Node(value=leaf_value)\n\n        best_split = self._find_best_split(X, y)\n        if best_split is None:\n            leaf_value = self._majority_vote(y)\n            return self.Node(value=leaf_value)\n            \n        feature_idx, threshold = best_split\n        left_mask = X[:, feature_idx] = threshold\n        right_mask = ~left_mask\n        \n        left_child = self._build_tree(X[left_mask, :], y[left_mask], depth + 1)\n        right_child = self._build_tree(X[right_mask, :], y[right_mask], depth + 1)\n        \n        return self.Node(feature_idx, threshold, left_child, right_child)\n\n    def fit(self, X, y):\n        \"\"\"Builds the decision tree from training data.\"\"\"\n        y_int = y.astype(int)\n        self.root = self._build_tree(X, y_int, 0)\n    \n    def _predict_single(self, x, node):\n        \"\"\"Traverses the tree for a single prediction.\"\"\"\n        if node.is_leaf():\n            return node.value\n        \n        if x[node.feature_index] = node.threshold:\n            return self._predict_single(x, node.left)\n        else:\n            return self._predict_single(x, node.right)\n            \n    def predict(self, X):\n        \"\"\"Predicts labels for new data.\"\"\"\n        if self.root is None:\n            raise RuntimeError(\"Model has not been trained yet.\")\n        return np.array([self._predict_single(x, self.root) for x in X])\n\ndef solve():\n    test_cases = [\n        (400, 5000, 0.5, 0.5, 0.0, 42, 4242),\n        (400, 5000, 0.2, 0.8, 0.0, 1, 11),\n        (2000, 5000, 0.5, 0.5, 0.1, 7, 77),\n        (30, 5000, 0.5, 0.5, 0.0, 2024, 2025),\n    ]\n\n    results = []\n    for params in test_cases:\n        n_train, n_test, a, b, eta, seed_train, seed_test = params\n\n        # Generate data\n        X_train, y_train = generate_data(n_train, a, b, eta, seed_train)\n        X_test, y_test = generate_data(n_test, a, b, eta, seed_test)\n\n        # Linear Least-Squares Classifier\n        linear_model = LinearLeastSquaresClassifier()\n        linear_model.fit(X_train, y_train)\n        y_pred_linear = linear_model.predict(X_test)\n        error_linear = np.mean(y_pred_linear != y_test)\n        \n        # Decision Tree Classifier\n        tree_model = DecisionTreeClassifier(max_depth=2)\n        tree_model.fit(X_train, y_train)\n        y_pred_tree = tree_model.predict(X_test)\n        error_tree = np.mean(y_pred_tree != y_test)\n        \n        results.append([error_linear, error_tree])\n    \n    # Format the final output string exactly as requested\n    output_str = \"[\" + \",\".join([f\"[{e_lin},{e_tree}]\" for e_lin, e_tree in results]) + \"]\"\n    print(output_str)\n\nsolve()\n```", "id": "3113048"}, {"introduction": "决策树虽然功能强大，但其贪心算法的本质使其容易过拟合，尤其是在面对小数据子集中的伪相关性时。本练习将构建一个这样的“陷阱”场景，展示决策树如何被误导，并让你亲自验证一个简单的正则化技巧——设置最小叶节点样本数——是如何有效防止模型学习这些虚假模式的。[@problem_id:3112969]", "problem": "考虑使用由经验风险最小化驱动的贪婪分裂的决策树进行二元分类。对于预测单一类别的叶节点，其经验风险由错分率定义。决策桩（decision stump）是一棵深度为$1$的决策树，它在单个特征阈值上进行一次分裂。设训练集为$\\{(x_i, y_i)\\}_{i=1}^N$，其中$x_i \\in \\mathbb{R}^2$且$y_i \\in \\{0,1\\}$，并且所有训练样本的权重均为单位权重。在特征$j \\in \\{0,1\\}$上以阈值$\\tau$进行分裂，会将数据划分为一个左子节点$\\{i : x_{i,j} \\le \\tau\\}$和一个右子节点$\\{i : x_{i,j}  \\tau\\}$。决策桩的经验风险是子节点错分率的加权和。最小子节点权重约束要求每个子节点必须至少包含$w_{\\min}$的总样本权重，在单位权重下，这等同于每个子节点至少有$w_{\\min}$个样本。贪婪决策桩选择在满足最小子节点权重约束的条件下，使经验风险最小化的特征和阈值。如果没有有效的分裂能降低父节点的经验风险，则该决策桩默认不进行分裂，并预测多数类别。\n\n你将构建一个科学上合理的情景，其中小亚组中出现虚假相关性，可能误导贪婪分裂。数据生成过程如下，所有随机变量均独立：\n\n- 训练集大小$N = 200$，测试集大小$M = 5000$。\n- 对每个样本，特征$x_0$从$x_{0} \\sim \\mathcal{N}(0,1)$中抽取。标签由$y = \\mathbb{1}\\{x_{0} + \\epsilon  0\\}$生成，其中$\\epsilon \\sim \\mathcal{N}(0, \\sigma^2)$，$\\sigma = 3.0$；由于噪声较高，这使得$x_0$成为一个弱信息特征。\n- 对于训练集，特征$x_1$从$x_{1} \\sim \\mathrm{Uniform}(-2,2)$中抽取，但对于带正标签的训练样本中一个大小为$s = 15$的小亚组，其$x_{1}$被设置为极端值$10$。这在一个小亚组中创建了一个虚假相关性，贪婪算法可以通过在$x_1$上使用高阈值来隔离该亚组从而利用此相关性。\n- 对于测试集，特征$x_1$对所有样本均从$x_{1} \\sim \\mathrm{Uniform}(-2,2)$中抽取，没有极端值，因此虚假模式不具有泛化性。\n\n定义一个预测该叶节点内多数类别的叶节点的经验风险为该叶节点中不属于多数类别的样本所占的比例。对于一次分裂，经验风险是所有子节点中的错分总数除以$N$。贪婪决策桩对每个特征，检查连续排序的唯一特征值之间的中点作为阈值。它选择能产生最小经验风险并满足两个子节点的最小子节点权重约束$w_{\\min}$的分裂；平局时选择较小的特征索引，然后选择较小的阈值。如果没有有效的分裂能降低父节点的经验风险，则决策桩不进行分裂并预测整体的多数类别。\n\n任务：\n- 实现所述的数据生成和带有最小子节点权重约束的决策桩学习。\n- 在训练集上训练决策桩，并通过检查所选特征是否为$x_1$来测试学到的分裂是否是虚假的。如果决策桩不分裂或选择了$x_0$，则视为避免了虚假分裂。\n\n测试套件：\n- 使用上述固定的训练和测试分布，其中$N = 200$，$M = 5000$，$s = 15$。\n- 评估以下最小子节点权重$w_{\\min}$的值：\n  1. $w_{\\min} = 1$（无有效约束）。\n  2. $w_{\\min} = 15$（边界等于虚假亚组的大小）。\n  3. $w_{\\min} = 16$（略高于虚假亚组的大小）。\n  4. $w_{\\min} = 500$（过大以至于无法进行任何分裂）。\n\n对于每种情况，你的程序必须输出一个布尔值，指示学到的决策桩是否避免了虚假分裂（如上定义）。最终输出格式必须是单行包含一个布尔值列表，按测试套件的精确顺序列出四个布尔值，并用方括号括起来，以逗号分隔，例如，$[b_1,b_2,b_3,b_4]$，其中每个$b_i$为$\\mathrm{True}$或$\\mathrm{False}$。", "solution": "## 问题验证\n\n### 步骤1：提取给定信息\n- **模型**：用于二元分类的决策桩（深度为1的决策树）。\n- **数据**：训练集$\\{(x_i, y_i)\\}_{i=1}^N$，其中$x_i \\in \\mathbb{R}^2$且$y_i \\in \\{0,1\\}$。所有样本权重为$1$。\n- **目标**：基于最小化经验风险的贪婪分裂选择。\n- **分裂规则**：在特征$j$上以阈值$\\tau$进行分裂，会创建一个左子节点$\\{i : x_{i,j} \\le \\tau\\}$和一个右子节点$\\{i : x_{i,j}  \\tau\\}$。\n- **叶节点风险**：叶节点的经验风险是其错分率，即该叶节点中不属于多数类别的样本所占的比例。\n- **决策桩风险**：决策桩的经验风险是两个子节点中的错分总数除以总样本数$N$。\n- **约束**：最小子节点权重约束$w_{\\min}$要求每个子节点至少包含$w_{\\min}$个样本。\n- **候选阈值**：连续排序的唯一特征值之间的中点。\n- **平局处理**：如果多个分裂产生相同的最小风险，则选择特征索引较小的那个。如果特征索引也相同，则选择阈值较小的那个。\n- **不分裂条件**：如果没有有效的分裂能降低父节点的经验风险，则决策桩不进行分裂并预测整体的多数类别。\n- **训练数据生成（$N=200$）**：\n    - $x_0 \\sim \\mathcal{N}(0,1)$。\n    - $\\epsilon \\sim \\mathcal{N}(0, \\sigma^2)$，其中$\\sigma=3.0$。\n    - $y = \\mathbb{1}\\{x_0 + \\epsilon  0\\}$，其中$\\mathbb{1}$是指示函数。\n    - 对于大多数样本，$x_1 \\sim \\mathrm{Uniform}(-2,2)$。\n    - 一个大小为$s=15$的带正标签（$y=1$）的训练样本亚组，其$x_1$被设置为$10$。\n- **测试数据生成（$M=5000$）**：\n    - $x_0 \\sim \\mathcal{N}(0,1)$。\n    - $\\epsilon \\sim \\mathcal{N}(0, 3.0^2)$。\n    - $y = \\mathbb{1}\\{x_0 + \\epsilon  0\\}$。\n    - 对所有样本，$x_1 \\sim \\mathrm{Uniform}(-2,2)$（无虚假模式）。\n- **任务**：对于一组$w_{\\min}$值，确定训练出的决策桩是否避免了对特征$x_1$的虚假分裂。避免虚假分裂定义为选择特征$x_0$或完全不进行分裂。\n- **测试套件**：$w_{\\min} \\in \\{1, 15, 16, 500\\}$。\n\n### 步骤2：使用提取的给定信息进行验证\n- **科学依据**：该问题在统计学习原理方面有充分的依据。它描述了一个标准算法（带贪婪训练的决策桩），并使用一个合成数据生成过程来研究一种常见的病理现象：对虚假相关性的过拟合。使用最小子节点大小（`w_min`）是一种标准的正则化技术，用于防止此类过拟合。\n- **问题明确**：该问题被高精度地指定了。所有参数（$N, M, s, \\sigma$）、算法程序（贪婪搜索、风险计算、阈值选择）、约束（$w_{\\min}$）和平局处理规则都已明确定义。这确保了对于给定的随机种子，该过程会产生一个唯一的、确定性的结果。\n- **客观性**：问题是以客观的数学语言陈述的。任务是实现指定的算法并报告其行为，这是一个纯粹的计算练习，不含主观性。\n\n该问题没有违反任何无效性标准。它是一个定义明确、科学合理的计算统计学问题。\n\n### 步骤3：结论与行动\n问题有效。将提供一个解决方案。\n\n## 解决方案\n\n该问题要求我们实现一个决策桩学习算法，并在一个为产生虚假相关性而专门构建的数据集上测试其行为。目标是观察`最小子节点权重`约束$w_{\\min}$如何影响算法对这种虚假模式的易感性。\n\n### 1. 原理与实验设计\n问题的核心在于一个真实（但弱）的信息特征$x_0$和一个虚假“完美”的特征$x_1$之间的冲突。\n- **特征$x_0$**：该特征通过方程$y = \\mathbb{1}\\{x_0 + \\epsilon  0\\}$与标签$y$有因果关系。然而，高噪声方差（$\\sigma^2=9.0$）使得这种关系很弱，意味着在$x_0$上的分裂可能只会带来经验风险的适度降低。\n- **特征$x_1$**：该特征通常不提供信息。然而，一个大小为$s=15$的训练样本小亚组（标签为$y=1$）被人为地分配了一个极端值$x_1=10$。贪婪算法可以发现在$x_1$上的一个分裂（例如，在阈值$\\tau$介于$2$和$10$之间），从而完美地将这$15$个样本隔离到一个零错分的“纯”子节点中。这可能导致训练集上整体经验风险的大幅降低，使其成为一个极具吸引力但却是虚假的分裂。\n- **通过$w_{\\min}$进行正则化**：最小子节点权重约束$w_{\\min}$是一种正则化形式。通过要求每个子节点包含最少数量的样本，我们可以禁止隔离非常小的亚组的分裂。如果$w_{\\min}$被设置得大于虚假亚组的大小（$s=15$），贪婪算法将被阻止做出这个局部最优但全局较差的选择，这可能迫使它选择更稳健的特征$x_0$或根本不分裂。\n\n### 2. 数据生成\n首先，我们实现大小为$N=200$的训练集的数据生成过程。为保证可复现性，使用固定的随机种子。\n1.  从标准正态分布$x_0 \\sim \\mathcal{N}(0,1)$生成特征$x_0$。\n2.  从均值为$0$、标准差为$\\sigma=3.0$的正态分布$\\epsilon \\sim \\mathcal{N}(0, 3.0^2)$生成噪声$\\epsilon$。\n3.  计算二元标签$y = \\mathbb{1}\\{x_0 + \\epsilon  0\\}$。\n4.  从均匀分布$x_1 \\sim \\mathrm{Uniform}(-2,2)$生成特征$x_1$。\n5.  识别所有$y=1$的样本索引。从这个集合中，随机选择$s=15$个索引，并将其对应的$x_1$值设置为$10.0$。这注入了虚假相关性。\n6.  将$x_0$和$x_1$组合成一个特征矩阵$X$。\n\n### 3. 决策桩算法\n通过找到满足$w_{\\min}$约束且最小化总错分数的单次分裂（一个特征$j$和一个阈值$\\tau$）来训练决策桩。\n1.  **计算父节点风险**：首先，计算根节点（即不进行分裂时）的错分计数。这是整个数据集中少数类的计数。该值作为要超越的初始`best_misclass`分数。\n2.  **遍历所有分裂**：\n    - 对每个特征$j \\in \\{0, 1\\}$：\n        - 确定候选阈值的集合。这些是特征$x_j$的连续唯一值之间的中点。\n        - 对每个阈值$\\tau$（按升序）：\n            a.  **划分数据**：将样本分为左集合（$x_j \\le \\tau$）和右集合（$x_j  \\tau$）。\n            b.  **检查约束**：计算左集合（$n_{left}$）和右集合（$n_{right}$）中的样本数。如果$n_{left}  w_{\\min}$或$n_{right}  w_{\\min}$，则这是一个无效分裂；继续下一个阈值。\n            c.  **计算风险**：对于有效分裂，计算总错分计数。这是左子节点和右子节点中错分数的总和。子节点的错分计数是该子节点内少数类的样本数。\n            d.  **更新最佳分裂**：将当前分裂的错分计数与迄今为止找到的`best_misclass`进行比较。如果当前计数严格更小（``），则用这个新计数更新`best_misclass`，并将当前特征$j$和阈值$\\tau$记录为最佳分裂。严格不等式和循环顺序（先特征0后特征1；先小阈值）正确地实现了指定的平局处理规则。\n3.  **返回结果**：检查所有有效分裂后，返回最佳分裂的特征索引。如果没有分裂优于父节点风险，则返回初始特征索引$-1$，表示没有进行分裂。\n\n### 4. 测试案例分析\n对测试套件$\\{1, 15, 16, 500\\}$中的每个$w_{\\min}$值执行该过程。\n-   **案例1：$w_{\\min} = 1$**：这个约束是微不足道的。在$x_1$上的虚假分裂创建了一个大小为$15$的子节点，这$\\ge 1$。这个分裂非常有吸引力，因为它创建了一个纯节点，导致风险显著降低。预计算法会选择特征$x_1$。结果应为`False`（未避免虚假分裂）。\n-   **案例2：$w_{\\min} = 15$**：虚假分裂创建了一个大小恰好为$15$的子节点。由于约束是$n_{child} \\ge w_{\\min}$，这个分裂仍然有效（$15 \\ge 15$）。预计算法仍会选择特征$x_1$。结果应为`False`。\n-   **案例3：$w_{\\min} = 16$**：现在虚假分裂是无效的，因为它大小为$15$的子节点不满足$16$的最小权重（$15  16$）。算法被迫忽略这个“陷阱”，必须寻找替代分裂。它要么选择在弱信息特征$x_0$上进行分裂（如果能降低风险），要么根本不分裂。无论哪种情况，都不会选择特征$x_1$。结果应为`True`（避免了虚假分裂）。\n-   **案例4：$w_{\\min} = 500$**：总样本数为$N=200$。不可能创建两个都满足至少有$500$个样本的约束的子节点。因此，不存在有效的分裂。算法将不会分裂。由于没有选择特征$x_1$，结果为`True`。\n\n该实现将生成一个与这四个结果相对应的布尔值列表。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements the data generation and decision stump learning to test the effect\n    of the minimum child weight constraint on avoiding spurious splits.\n    \"\"\"\n    # Define problem parameters\n    N = 200\n    s = 15\n    sigma = 3.0\n    w_min_cases = [1, 15, 16, 500]\n\n    # Set a fixed seed for reproducibility of the random dataset\n    np.random.seed(42)\n\n    # --- Data Generation --\n    # This block creates the single training set used for all test cases.\n    \n    # Feature x_0 is drawn from a standard normal distribution\n    x0 = np.random.randn(N)\n    # The label y is determined by x_0 plus high-variance noise\n    epsilon = np.random.normal(0, sigma, N)\n    y_train = (x0 + epsilon > 0).astype(int)\n\n    # Feature x_1 is mostly uniform noise\n    x1 = np.random.uniform(-2, 2, N)\n    \n    # Identify indices of positive-labeled samples to inject the spurious pattern\n    positive_indices = np.where(y_train == 1)[0]\n    \n    # Set x_1 to an extreme value for a small subgroup of 's' positive samples.\n    # This creates a spurious correlation that a greedy algorithm might exploit.\n    if len(positive_indices) >= s:\n        spurious_indices = np.random.choice(positive_indices, size=s, replace=False)\n        x1[spurious_indices] = 10.0\n    else:\n        # This case is unlikely with N=200 but makes the code more robust.\n        x1[positive_indices] = 10.0\n\n    X_train = np.column_stack((x0, x1))\n    \n    results = []\n    for w_min in w_min_cases:\n        # --- Decision Stump Training ---\n        n_samples = X_train.shape[0]\n\n        # Calculate the misclassification count of the parent node (no split scenario).\n        # This is the number of samples in the minority class.\n        n_pos_parent = np.sum(y_train)\n        parent_misclass = min(n_pos_parent, n_samples - n_pos_parent)\n\n        best_misclass = parent_misclass\n        best_split = {'feature': -1, 'threshold': np.inf}\n\n        # Iterate through features (j=0 for x_0, j=1 for x_1)\n        for j in range(X_train.shape[1]):\n            feature_values = X_train[:, j]\n            \n            # Candidate thresholds are midpoints of unique sorted feature values.\n            unique_vals = np.unique(feature_values)\n            if len(unique_vals)  2:\n                continue\n            thresholds = (unique_vals[:-1] + unique_vals[1:]) / 2.0\n\n            # Evaluate each potential split\n            for tau in thresholds:\n                # Partition data into left and right children\n                left_mask = feature_values = tau\n                \n                n_left = np.sum(left_mask)\n                n_right = n_samples - n_left\n\n                # Verify the minimum child weight constraint\n                if n_left  w_min or n_right  w_min:\n                    continue\n\n                # Calculate misclassifications in the left child\n                y_left = y_train[left_mask]\n                misclass_left = min(np.sum(y_left), n_left - np.sum(y_left))\n                \n                # Calculate misclassifications in the right child\n                right_mask = ~left_mask\n                y_right = y_train[right_mask]\n                misclass_right = min(np.sum(y_right), n_right - np.sum(y_right))\n\n                current_misclass = misclass_left + misclass_right\n\n                # A split is chosen only if it strictly reduces the misclassification count.\n                # The loop order (j=0 then j=1; tau ascending) ensures that ties are\n                # broken by smaller feature index, then smaller threshold.\n                if current_misclass  best_misclass:\n                    best_misclass = current_misclass\n                    best_split = {'feature': j, 'threshold': tau}\n        \n        selected_feature = best_split['feature']\n        \n        # The split is considered non-spurious if the selected feature is not x_1 (index 1).\n        # This includes cases where x_0 is chosen or no split is made (feature = -1).\n        avoids_spurious = (selected_feature != 1)\n        results.append(avoids_spurious)\n\n    # Print the final list of booleans in the specified format\n    print(f\"[{','.join(map(str, [r.item() for r in results]))}]\".replace(\" \", \"\"))\n\nsolve()\n```", "id": "3112969"}, {"introduction": "模型的实际性能在很大程度上取决于输入数据的质量。本练习将探讨特征变量中的测量误差对回归树稳定性的影响，你将量化噪声是如何影响模型分裂点定位的精确性，并最终如何影响其整体预测准确度的。[@problem_id:3113009]", "problem": "本题要求您形式化并量化预测变量中的加性测量误差如何影响一维回归树中的分裂阈值估计及最终的预测结果。考虑一个在平方损失下训练的深度为一的回归树（即在一个标量特征上进行单次分裂）。请从以下基础设定开始：\n- 在平方损失下的经验风险最小化，其中任何区域内的最优常数预测等于该区域内结果的样本均值。\n- 对于由观测特征上的阈值引导的、将索引划分为两个区域（左区域和右区域）的任何划分，其平方误差和等于每个区域内各样本点与该区域样本均值之差的平方和的总和。\n\n建立如下的数据生成过程。对于每个训练样本 $i$：\n- 独立地从 $\\operatorname{Uniform}(0,1)$ 分布中抽取真实特征 $X_i$。\n- 通过具有单一真实边界 $\\tau^\\star$ 的阶跃函数 $f(x)$ 来定义确定性结果，其公式如下\n$$\nf(x) = \\begin{cases}\n0,  x \\le \\tau^\\star, \\\\\n1,  x  \\tau^\\star,\n\\end{cases}\n\\quad \\text{with} \\quad \\tau^\\star = 0.5.\n$$\n令 $Y_i = f(X_i)$。\n- 抽取独立的测量误差 $\\epsilon_i \\sim \\mathcal{N}(0,\\sigma^2)$，并构成观测特征\n$$\nW_i = \\min\\left(1, \\max\\left(0, X_i + \\epsilon_i\\right)\\right),\n$$\n也就是说，$W_i$ 是 $X_i + \\epsilon_i$ 被裁剪到区间 $[0,1]$ 后的值。\n\n训练目标。对于一个候选阈值 $\\tau$，定义其引导的左区域 $\\{i: W_i \\le \\tau\\}$ 和右区域 $\\{i: W_i  \\tau\\}$。令 $c_L(\\tau)$ 为左区域中 $\\{Y_i\\}$ 的样本均值，$c_R(\\tau)$ 为右区域中 $\\{Y_i\\}$ 的样本均值。经验风险即为总平方误差和\n$$\n\\operatorname{SSE}(\\tau) = \\sum_{i: W_i \\le \\tau} \\left(Y_i - c_L(\\tau)\\right)^2 + \\sum_{i: W_i  \\tau} \\left(Y_i - c_R(\\tau)\\right)^2.\n$$\n将估计阈值 $\\hat{\\tau}$ 定义为能够产生非空左、右区域的所有分裂中 $\\operatorname{SSE}(\\tau)$ 的任意一个最小化器。树所使用的相应叶节点预测为 $c_L(\\hat{\\tau})$ 和 $c_R(\\hat{\\tau})$。\n\n下游预测评估。考虑一个由 $[0,1]$ 区间内 $m$ 个等距点 $x_j$ 组成的测试网格（其中 $m$ 很大）。该树产生的预测为\n$$\n\\hat{f}(x) = \\begin{cases}\nc_L(\\hat{\\tau}),  x \\le \\hat{\\tau}, \\\\\nc_R(\\hat{\\tau}),  x  \\hat{\\tau}.\n\\end{cases}\n$$\n定义该网格上的均方预测误差为\n$$\n\\operatorname{MSE} = \\frac{1}{m} \\sum_{j=1}^{m} \\left(\\hat{f}(x_j) - f(x_j)\\right)^2.\n$$\n\n您的程序必须：\n1. 按照描述生成训练数据，使用固定的随机种子以确保可复现性。\n2. 对于每个测试用例 $(\\sigma,n)$，通过在连续的已排序观测特征 $\\{W_i\\}$ 之间的阈值上最小化 $\\operatorname{SSE}(\\tau)$ 来估计 $\\hat{\\tau}$，以保证两个区域都非空。使用上文定义的确切经验风险目标。根据 $\\hat{\\tau}$ 引导的训练分配计算 $c_L(\\hat{\\tau})$ 和 $c_R(\\hat{\\tau})$。\n3. 计算绝对阈值误差 $e_\\tau = |\\hat{\\tau} - \\tau^\\star|$。\n4. 在一个由 $[0,1]$ 区间内 $m=10001$ 个均匀分布的点组成的测试网格上计算均方预测误差 $\\operatorname{MSE}$（无物理单位）。\n5. 报告每个测试用例的结果对 $[e_\\tau,\\operatorname{MSE}]$。\n\n测试套件。使用以下五个参数集，其设计旨在覆盖标准情况、不同噪声水平以及小样本边缘情况：\n- 案例 A: $(\\sigma,n) = (0.0, 200)$。\n- 案例 B: $(\\sigma,n) = (0.05, 200)$。\n- 案例 C: $(\\sigma,n) = (0.2, 200)$。\n- 案例 D: $(\\sigma,n) = (0.2, 40)$。\n- 案例 E: $(\\sigma,n) = (0.5, 200)$。\n\n最终输出格式。您的程序应生成单行输出，其中包含五个测试用例的结果，格式为一个由方括号括起来的、逗号分隔的列表的列表。第 $k$ 个内部列表必须是与案例 $k$（按 A、B、C、D、E 的顺序）对应的结果对 $[e_\\tau,\\operatorname{MSE}]$。例如，打印的字符串必须如下所示\n$$\n[\\,[e_{\\tau,A},\\operatorname{MSE}_A],\\,[e_{\\tau,B},\\operatorname{MSE}_B],\\,[e_{\\tau,C},\\operatorname{MSE}_C],\\,[e_{\\tau,D},\\operatorname{MSE}_D],\\,[e_{\\tau,E},\\operatorname{MSE}_E]\\,].\n$$\n所有数值答案必须是浮点数。不得打印任何额外文本。", "solution": "用户提供的问题陈述被判定为有效。它在科学上基于统计学习理论，具体探讨了测量误差对回归树模型的影响。该问题是适定的，具有明确定义的数据生成过程、目标函数和评估指标，确保可以计算出唯一且有意义的解。其设置是客观、完整且计算上可行的。\n\n解决方案遵循问题规范，按以下步骤进行。\n\n### 1. 理论框架与目标函数\n\n核心任务是为深度为一的回归树找到一个最优分裂阈值 $\\hat{\\tau}$，以最小化平方误差和 (SSE)。该树根据观测特征 $W$ 将数据划分为左区域 $\\{i: W_i \\le \\tau\\}$ 和右区域 $\\{i: W_i  \\tau\\}$。区域内任何点的预测值是该区域内结果 $Y_i$ 的样本均值。\n\n设 $c_L(\\tau)$ 和 $c_R(\\tau)$ 分别为由阈值 $\\tau$ 引导的左、右区域中 $Y_i$ 的样本均值。目标是找到最小化经验风险的 $\\hat{\\tau}$：\n$$\n\\hat{\\tau} = \\arg\\min_{\\tau} \\operatorname{SSE}(\\tau) = \\arg\\min_{\\tau} \\left( \\sum_{i: W_i \\le \\tau} (Y_i - c_L(\\tau))^2 + \\sum_{i: W_i  \\tau} (Y_i - c_R(\\tau))^2 \\right)\n$$\n对 $\\hat{\\tau}$ 的搜索被限制在能产生非空左、右划分的值域内。\n\n一个计算上高效的、用于计算给定区域（例如，包含 $n_L$ 个点的左区域）SSE 的公式可以从方差的定义中导出：\n$$\n\\sum_{i \\in L} (Y_i - c_L)^2 = \\sum_{i \\in L} Y_i^2 - \\frac{1}{n_L} \\left( \\sum_{i \\in L} Y_i \\right)^2\n$$\n由于结果变量 $Y_i$ 是二元的（0 或 1），我们有 $Y_i^2 = Y_i$。这简化了表达式。设 $S_L = \\sum_{i \\in L} Y_i$ 和 $S_R = \\sum_{i \\in R} Y_i$ 分别为左、右区域中结果的总和，其样本数分别为 $n_L$ 和 $n_R$。对于给定的分裂，SSE 变为：\n$$\n\\operatorname{SSE}(\\tau) = \\left(S_L - \\frac{S_L^2}{n_L}\\right) + \\left(S_R - \\frac{S_R^2}{n_R}\\right)\n$$\n该公式用于高效地计算每个候选分裂的 SSE。\n\n### 2. 算法实现\n\n为了找到最优阈值 $\\hat{\\tau}$，我们采用一个标准的高效算法：\n\n1.  数据生成：对于每个测试用例 $(\\sigma, n)$，我们生成 $n$ 个数据点。真实特征 $X_i$ 从 $\\operatorname{Uniform}(0,1)$ 分布中抽取。结果 $Y_i$ 由阶跃函数 $Y_i = \\mathbf{1}(X_i  \\tau^\\star)$ 决定，其中 $\\tau^\\star = 0.5$。测量噪声 $\\epsilon_i \\sim \\mathcal{N}(0, \\sigma^2)$ 被加到 $X_i$ 上形成带噪声的特征，然后将其裁剪到 $[0,1]$ 区间以产生观测特征 $W_i$。固定的随机种子确保了可复现性。\n\n2.  最优分裂搜索：a. 训练数据对 $(W_i, Y_i)$ 根据观测特征 $W_i$ 进行排序。b. 算法遍历所有可能的分裂点。候选阈值 $\\tau$ 是已排序 $W_i$ 的连续唯一值之间的中点。这确保了数据的每个不同划分都被恰好考虑一次。c. 为了使搜索高效，我们使用运行和。我们从第一个迭代到第 $(n-1)$ 个已排序的数据点。在每一步 $j$，我们考虑在点 $j$ 之后进行分裂。我们维护左、右划分的计数 ($n_L, n_R$) 和结果总和 ($S_L, S_R$)。当我们将单个数据点从右划分移动到左划分时，这些值可以在 $O(1)$ 时间内更新。d. 对于每个候选分裂，我们使用上述简化公式计算总 $\\operatorname{SSE}$。选择产生最小 $\\operatorname{SSE}$ 的分裂。相应的阈值 $\\hat{\\tau}$ 以及左、右均值预测 $c_L(\\hat{\\tau})$ 和 $c_R(\\hat{\\tau})$ 被存储起来。\n\n3.  评估：a. 阈值误差：阈值估计的绝对误差计算为 $e_\\tau = |\\hat{\\tau} - \\tau^\\star|$。b. 均方预测误差 (MSE)：生成的树模型 $\\hat{f}(x)$ 的性能在一个由 $[0,1]$ 区间内 $m=10001$ 个均匀分布的点组成的精细网格上进行评估。对于每个 $x_j$，如果 $x_j \\le \\hat{\\tau}$，模型预测 $\\hat{f}(x_j) = c_L(\\hat{\\tau})$，否则预测 $\\hat{f}(x_j) = c_R(\\hat{\\tau})$。$\\operatorname{MSE}$ 计算为这些预测值与真实函数值 $f(x_j)$ 之间的平均平方差：\n    $$\n    \\operatorname{MSE} = \\frac{1}{m} \\sum_{j=1}^{m} (\\hat{f}(x_j) - f(x_j))^2\n    $$\n\n对问题陈述中指定的所有五个测试用例重复此过程，并收集得到的结果对 $[e_\\tau, \\operatorname{MSE}]$。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the regression tree problem for a suite of test cases.\n\n    For each case (sigma, n), it performs the following steps:\n    1. Generates training data (W, Y) with measurement error sigma.\n    2. Finds the optimal split threshold `tau_hat` for a depth-1 regression tree\n       by minimizing the Sum of Squared Errors (SSE).\n    3. Computes the absolute threshold error `e_tau`.\n    4. Computes the Mean Squared Prediction Error (MSE) on a fine test grid.\n    5. Collects and prints the results in the specified format.\n    \"\"\"\n\n    # Test cases as per the problem statement\n    test_cases = [\n        (0.0, 200),   # Case A: No noise\n        (0.05, 200),  # Case B: Low noise\n        (0.2, 200),   # Case C: High noise\n        (0.2, 40),    # Case D: High noise, small sample\n        (0.5, 200),   # Case E: Very high noise\n    ]\n\n    results = []\n    tau_star = 0.5\n    m_grid = 10001\n    \n    # Use a fixed random seed for reproducibility\n    rng = np.random.default_rng(42)\n\n    for sigma, n in test_cases:\n        # 1. Generate training data\n        X = rng.uniform(0, 1, size=n)\n        Y = (X > tau_star).astype(float)\n        epsilon = rng.normal(0, sigma, size=n)\n        W = np.clip(X + epsilon, 0, 1)\n\n        # 2. Find the best split by minimizing SSE\n        sort_indices = np.argsort(W)\n        W_sorted = W[sort_indices]\n        Y_sorted = Y[sort_indices]\n\n        # Initialize tracking variables for the best split\n        min_sse = float('inf')\n        best_tau = None\n        best_c_L = None\n        best_c_R = None\n        \n        # Precompute total sum for efficient updates\n        total_sum_y = np.sum(Y_sorted)\n\n        # Running sum for the left partition\n        sum_y_left = 0.0\n        \n        # Iterate through all possible split points (between sorted W values)\n        # The loop ensures both left and right partitions are non-empty.\n        for i in range(n - 1):\n            sum_y_left += Y_sorted[i]\n            n_left = i + 1\n\n            # To avoid redundant calculations, only consider splits at unique W values.\n            # If W_sorted[i] == W_sorted[i+1], splitting between them is meaningless.\n            if W_sorted[i] == W_sorted[i+1]:\n                continue\n            \n            n_right = n - n_left\n            sum_y_right = total_sum_y - sum_y_left\n            \n            # Use the efficient formula for SSE of a binary target: sum(p(1-p)) per node.\n            # SSE = sum(y^2) - (sum(y))^2/N. For y in {0,1}, sum(y^2) = sum(y).\n            # So, SSE = sum(y) - (sum(y))^2/N.\n            sse_L = sum_y_left - (sum_y_left**2) / n_left\n            sse_R = sum_y_right - (sum_y_right**2) / n_right\n            current_sse = sse_L + sse_R\n            \n            if current_sse  min_sse:\n                min_sse = current_sse\n                # The threshold is the midpoint between two consecutive W values\n                best_tau = (W_sorted[i] + W_sorted[i+1]) / 2.0\n                best_c_L = sum_y_left / n_left\n                best_c_R = sum_y_right / n_right\n\n        # Handle case where all W are identical (unlikely but possible)\n        if best_tau is None:\n            # Fallback: No split is made. The prediction is the global mean.\n            # This case will have very high SSE on the training set,\n            # but is the only option if no split can be made.\n            # For this problem's evaluation, we can set tau to mid-range\n            # and c_L, c_R to the global mean, though it won't be optimal.\n            # A split is required by the problem, so this indicates an issue in very\n            # extreme cases not covered by the test suite. We assume a split is found.\n            # The current setup will error if no split is found.\n            pass\n\n        # 3. Compute absolute threshold error\n        e_tau = abs(best_tau - tau_star)\n\n        # 4. Compute Mean Squared Prediction Error (MSE) on a test grid\n        x_grid = np.linspace(0, 1, m_grid)\n        y_true = (x_grid > tau_star).astype(float)\n        \n        y_pred = np.where(x_grid = best_tau, best_c_L, best_c_R)\n        \n        mse = np.mean((y_pred - y_true)**2)\n        \n        # 5. Store the results for this case\n        results.append([e_tau, mse])\n    \n    # Final print statement in the exact required format.\n    # The map(str, ...) on a list of lists correctly formats each inner list\n    # as a string, e.g., '[0.123, 0.456]', including spaces as per Python's default.\n    print(f\"[{','.join(map(str, results))}]\".replace(\" \", \"\"))\n\nsolve()\n```", "id": "3113009"}]}