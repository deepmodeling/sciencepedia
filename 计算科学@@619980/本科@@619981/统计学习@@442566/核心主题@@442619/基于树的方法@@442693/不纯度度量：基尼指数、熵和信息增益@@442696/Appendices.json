{"hands_on_practices": [{"introduction": "基尼不纯度（Gini impurity）和信息增益（Information Gain，基于熵）是决策树中最常用的两种分裂标准。虽然它们的目标相似——最大化节点的纯度——但它们的数学性质略有不同，这会导致它们在特定情况下做出不同的选择。本练习将引导你通过一个编码任务，直接比较这两种标准在处理具有许多稀有类别的分类特征时的表现，从而揭示它们在灵敏度和对小样本纯度变化的响应之间的权衡。 [@problem_id:3112936]", "problem": "您需要为一个决策树分类器中的类别型特征实现并比较两种分裂标准下的二元划分：香农熵（信息增益）和基尼不纯度（基尼下降）。该任务侧重于在存在许多稀有类别的情况下这些标准的行为，以及在捕获信息和对小计数值过拟合之间的权衡。您的程序必须为每个提供的测试用例，确定在每种标准下类别的最优二元划分，计算训练分裂的质量指标，并在已知真实类别概率的情况下评估期望泛化精度。\n\n定义与基础：\n- 假设有一个具有 $K$ 个类别的单一类别型特征。对于类别 $k \\in \\{1,\\dots,K\\}$，给定训练样本数 $n_k$、正类（$y=1$）的观测计数 $c_k$ 以及 $y=1$ 的真实潜在概率 $p_k$。\n- 类别 $k$ 的经验正类率为 $\\hat{p}_k = c_k / n_k$。\n- 父节点总样本数为 $N = \\sum_{k=1}^{K} n_k$，父节点的经验率为 $\\hat{p}_{\\mathrm{parent}} = \\left(\\sum_{k=1}^{K} c_k\\right)/N$。\n- 概率为 $p$ 的二元分布的香农熵为 $H(p) = -p\\log(p) - (1-p)\\log(1-p)$，基尼不纯度为 $G(p) = 2p(1-p)$。熵的计算使用自然对数；请注意，对数的底只是对 $H(p)$ 进行常数缩放，这不会改变使增益最大化的分裂。\n- 对于将 $K$ 个类别二元划分为左子集 $\\mathcal{L}$ 和右子集 $\\mathcal{R}$ 的任意分裂，其计数分别为 $N_{\\mathcal{L}}$ 和 $N_{\\mathcal{R}}$，经验率分别为 $\\hat{p}_{\\mathcal{L}}$ 和 $\\hat{p}_{\\mathcal{R}}$，条件熵为\n$$\nH_{\\mathrm{cond}} = \\frac{N_{\\mathcal{L}}}{N} H(\\hat{p}_{\\mathcal{L}}) + \\frac{N_{\\mathcal{R}}}{N} H(\\hat{p}_{\\mathcal{R}}),\n$$\n信息增益为\n$$\n\\mathrm{IG} = H(\\hat{p}_{\\mathrm{parent}}) - H_{\\mathrm{cond}}.\n$$\n类似地，条件基尼不纯度为\n$$\nG_{\\mathrm{cond}} = \\frac{N_{\\mathcal{L}}}{N} G(\\hat{p}_{\\mathcal{L}}) + \\frac{N_{\\mathcal{R}}}{N} G(\\hat{p}_{\\mathcal{R}}),\n$$\n基尼下降为\n$$\n\\mathrm{GD} = G(\\hat{p}_{\\mathrm{parent}}) - G_{\\mathrm{cond}}.\n$$\n\n二元划分搜索协议：\n- 将类别按经验率 $\\hat{p}_k$ 升序排列以获得一个类别序列。将注意力限制在索引 $t \\in \\{1,\\dots,K-1\\}$ 处的前缀分裂上：左子节点包含排序后序列中的前 $t$ 个类别，右子节点包含剩余的 $K-t$ 个类别。当不纯度是一个可分离的、Schur-凹函数时，此协议对于二元分类是可采纳的，它避免了对集合划分的指数级搜索，同时对于包括香non熵和基尼不纯度在内的广泛不纯度度量，能够保留最优分裂。\n- 对于每个 $t$，计算信息增益 $\\mathrm{IG}(t)$ 和基尼下降 $\\mathrm{GD}(t)$。设 $t^{\\star}_{H}$ 是使 $\\mathrm{IG}(t)$ 最大化的索引，$t^{\\star}_{G}$ 是使 $\\mathrm{GD}(t)$ 最大化的索引。如果出现平局，选择最小的 $t$。\n- 使用训练数据定义单次分裂分类器：对于每个子节点，基于 $\\hat{p}_{\\mathcal{L}}$ 和 $\\hat{p}_{\\mathcal{R}}$ 预测多数类（当率至少为 $0.5$ 时预测为 $1$，否则预测为 $0$；通过预测 $1$ 来打破平局）。在已知真实概率 $p_k$ 的情况下，期望泛化精度计算如下\n$$\n\\mathrm{Acc} = \\frac{1}{N}\\left(\\sum_{k \\in \\mathcal{L}} n_k \\cdot \\bigl(\\mathbf{1}[\\hat{p}_{\\mathcal{L}} \\ge 0.5] \\cdot p_k + \\mathbf{1}[\\hat{p}_{\\mathcal{L}}  0.5] \\cdot (1-p_k)\\bigr) + \\sum_{k \\in \\mathcal{R}} n_k \\cdot \\bigl(\\mathbf{1}[\\hat{p}_{\\mathcal{R}} \\ge 0.5] \\cdot p_k + \\mathbf{1}[\\hat{p}_{\\mathcal{R}}  0.5] \\cdot (1-p_k)\\bigr)\\right),\n$$\n其中 $\\mathbf{1}[\\cdot]$ 表示指示函数。该公式评估每个类别在其真实概率下的期望正确性，并按 $n_k$ 进行加权。\n\n您的程序必须实现上述协议，并为每个测试用例生成以下输出：\n- 由香农熵选择的索引 $t^{\\star}_{H}$。\n- 由基尼不纯度选择的索引 $t^{\\star}_{G}$。\n- 最大信息增益 $\\mathrm{IG}(t^{\\star}_{H})$，以浮点数形式表示。\n- 最大基尼下降 $\\mathrm{GD}(t^{\\star}_{G})$，以浮点数形式表示。\n- 各自单次分裂分类器的期望泛化精度 $\\mathrm{Acc}_{H}$ 和 $\\mathrm{Acc}_{G}$，以 $[0,1]$ 范围内的浮点数形式表示。\n\n测试套件：\n对于每个测试用例，数组均以排序前类别的顺序呈现。您的程序必须在评估分裂之前按经验率 $\\hat{p}_k$ 进行排序。\n\n- 测试用例 1（许多稀有类别，训练数据表明纯度很高，但真实率适中）：\n    - $n = [200,200,200,200,5,5,5,5,5,5,5,5]$\n    - $c = [130,70,120,80,5,5,5,5,5,5,5,5]$\n    - $p_{\\mathrm{true}} = [0.6,0.4,0.6,0.4,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5]$\n\n- 测试用例 2（稀有类别在小计数值下确实具有信息量；比较标准敏感度）：\n    - $n = [100,100,100,100,10,10,10,10,10,10,10,10,10,10]$\n    - $c = [55,45,60,40,9,9,9,9,9,9,9,9,9,9]$\n    - $p_{\\mathrm{true}} = [0.55,0.45,0.60,0.40,0.95,0.95,0.95,0.95,0.95,0.95,0.95,0.95,0.95,0.95]$\n\n- 测试用例 3（边界情况，所有类别的经验率和真实率均为 0.5）：\n    - $n = [50,50,50,50,50,50]$\n    - $c = [25,25,25,25,25,25]$\n    - $p_{\\mathrm{true}} = [0.5,0.5,0.5,0.5,0.5,0.5]$\n\n最终输出格式：\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表。列表中的每个元素对应一个测试用例，其本身也是一个形式为\n$[t^{\\star}_{H}, t^{\\star}_{G}, \\mathrm{IG}(t^{\\star}_{H}), \\mathrm{GD}(t^{\\star}_{G}), \\mathrm{Acc}_{H}, \\mathrm{Acc}_{G}]$\n的列表。例如，打印的输出应如下所示：\n[[tH1,tG1,IG1,GD1,AccH1,AccG1],[tH2,tG2,IG2,GD2,AccH2,AccG2],[tH3,tG3,IG3,GD3,AccH3,AccG3]]。\n所有输出必须是小数或整数；不要使用百分比。", "solution": "该问题要求为一个类别型特征实现决策树分裂算法，并比较两种常见的分裂标准：信息增益（源于香农熵）和基尼下降（源于基尼不纯度）。该过程包括找到特征类别的最优二元划分以使这些标准最大化，然后评估所得到的单次分裂分类器的泛化性能。\n\n解决方案的结构如下：\n首先，对于每个测试用例，我们必须准备数据。类别由其样本数（$n_k$）、正类计数（$c_k$）和真实正类概率（$p_k$）定义。用于分裂的主要变量是经验正类率 $\\hat{p}_k = c_k / n_k$。指定分裂协议的核心是按此经验率 $\\hat{p}_k$ 升序排列类别。这将最优二元划分的搜索空间从指数数量级的集合划分减少到线性数量级（$K-1$）的前缀分裂，其中 $K$ 是类别数量。在二元响应的背景下，这种简化保证了对于像香农熵和基尼不纯度这样的不纯度函数能够保留最优分裂。\n\n其次，我们计算任何分裂之前父节点（包含所有数据）的不纯度。设 $N = \\sum_k n_k$ 为总样本量，$C = \\sum_k c_k$ 为总正类计数。父节点的经验率为 $\\hat{p}_{\\mathrm{parent}} = C/N$。父节点的不纯度使用提供的香农熵公式 $H(p) = -p\\log(p) - (1-p)\\log(1-p)$ 和基尼不纯度公式 $G(p) = 2p(1-p)$ 计算。当 $p=0$ 或 $p=1$ 时，熵为 $0$。\n\n第三，我们基于已排序的类别遍历所有可能的前缀分裂。索引为 $t \\in \\{1, \\dots, K-1\\}$ 的分裂将类别划分为一个包含前 $t$ 个类别的左子节点 $\\mathcal{L}$ 和一个包含剩余 $K-t$ 个类别的右子节点 $\\mathcal{R}$。对于每个潜在的分裂，我们计算子节点的属性：\n- 左子节点 $\\mathcal{L}$：总计数 $N_{\\mathcal{L}} = \\sum_{k=1}^{t} n_{s,k}$，正类计数 $C_{\\mathcal{L}} = \\sum_{k=1}^{t} c_{s,k}$，其中下标 $s$ 表示排序后的顺序。经验率为 $\\hat{p}_{\\mathcal{L}} = C_{\\mathcal{L}} / N_{\\mathcal{L}}$。\n- 右子节点 $\\mathcal{R}$：总计数 $N_{\\mathcal{R}} = N - N_{\\mathcal{L}}$，正类计数 $C_{\\mathcal{R}} = C - C_{\\mathcal{L}}$。经验率为 $\\hat{p}_{\\mathcal{R}} = C_{\\mathcal{R}} / N_{\\mathcal{R}}$。\n\n利用这些值，我们计算分裂的条件不纯度，即子节点不纯度的加权平均值：\n$$\nH_{\\mathrm{cond}}(t) = \\frac{N_{\\mathcal{L}}}{N} H(\\hat{p}_{\\mathcal{L}}) + \\frac{N_{\\mathcal{R}}}{N} H(\\hat{p}_{\\mathcal{R}})\n$$\n$$\nG_{\\mathrm{cond}}(t) = \\frac{N_{\\mathcal{L}}}{N} G(\\hat{p}_{\\mathcal{L}}) + \\frac{N_{\\mathcal{R}}}{N} G(\\hat{p}_{\\mathcal{R}})\n$$\n分裂的质量随后通过从父节点到子节点的不纯度减少量来量化：\n- 信息增益：$\\mathrm{IG}(t) = H(\\hat{p}_{\\mathrm{parent}}) - H_{\\mathrm{cond}}(t)$\n- 基尼下降：$\\mathrm{GD}(t) = G(\\hat{p}_{\\mathrm{parent}}) - G_{\\mathrm{cond}}(t)$\n\n我们找到使 $\\mathrm{IG}(t)$ 最大化的分裂索引 $t^{\\star}_{H}$ 和使 $\\mathrm{GD}(t)$ 最大化的分裂索引 $t^{\\star}_{G}$。对于最大值出现平局的情况，问题规定选择最小的索引 $t$。这通过从 $t=1$ 迭代到 $K-1$ 并在仅当发现严格更大的不纯度减少量时才更新最优分裂来处理。\n\n第四，一旦两种标准下的最优分裂被确定，我们评估相应的单次分裂分类器的期望泛化精度。每个子节点（$\\mathcal{L}$ 或 $\\mathcal{R}$）的预测由其经验率决定：如果率 $\\ge 0.5$ 则预测为类别 $1$，否则预测为类别 $0$。然后使用已知的真实概率 $p_k$ 计算精度。精度公式是所有类别上正确预测概率的加权平均值：\n$$\n\\mathrm{Acc} = \\frac{1}{N}\\left(\\sum_{k \\in \\mathcal{L}} n_k \\cdot P(\\text{correct}|k) + \\sum_{k \\in \\mathcal{R}} n_k \\cdot P(\\text{correct}|k)\\right)\n$$\n其中，如果包含类别 $k$ 的节点的预测为 $1$，则 $P(\\text{correct}|k)$ 为 $p_k$；如果预测为 $0$，则为 $1-p_k$。此过程分别对由信息增益找到的分裂（$\\mathrm{Acc}_H$）和由基尼下降找到的分裂（$\\mathrm{Acc}_G$）执行。\n\n整个过程被封装在一个函数中，该函数处理每个测试用例并返回六个指定的结果：$t^{\\star}_{H}$、$t^{\\star}_{G}$、$\\mathrm{IG}(t^{\\star}_{H})$、$\\mathrm{GD}(t^{\\star}_{G})$、$\\mathrm{Acc}_{H}$ 和 $\\mathrm{Acc}_{G}$。该实现使用 `numpy` 进行高效的数组操作，特别是用于计算累积和，以加速对分裂的迭代。", "answer": "完整且可运行的 Python 3 代码如下。导入必须符合指定的执行环境。\n```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for all test cases.\n    \"\"\"\n\n    def entropy(p):\n        \"\"\"Calculates Shannon entropy for a binary distribution.\"\"\"\n        if p == 0 or p == 1:\n            return 0.0\n        return -p * np.log(p) - (1 - p) * np.log(1 - p)\n\n    def gini(p):\n        \"\"\"Calculates Gini impurity for a binary distribution.\"\"\"\n        return 2 * p * (1 - p)\n\n    def solve_case(n_orig, c_orig, p_true_orig):\n        \"\"\"\n        Solves the problem for a single test case.\n        \"\"\"\n        n_orig = np.array(n_orig, dtype=float)\n        c_orig = np.array(c_orig, dtype=float)\n        p_true_orig = np.array(p_true_orig, dtype=float)\n        \n        # 1. Calculate p_hat and combine data\n        p_hat_orig = np.divide(c_orig, n_orig, out=np.zeros_like(c_orig, dtype=float), where=n_orig != 0)\n        \n        K = len(n_orig)\n\n        # Combine into a list of tuples to sort: (p_hat, n, c, p_true)\n        # Using a secondary sort key (original index) for stability, though not required by a problem\n        # but good practice.\n        indexed_data = list(zip(p_hat_orig, n_orig, c_orig, p_true_orig, range(K)))\n        sorted_indexed_data = sorted(indexed_data)\n        \n        if not sorted_indexed_data:\n            # Handle empty input case, though not in test suite\n            return [1, 1, 0.0, 0.0, 0.5, 0.5]\n\n        # Unzip into sorted arrays\n        p_hat_s, n_s, c_s, p_true_s, _ = map(np.array, zip(*sorted_indexed_data))\n\n        # 2. Parent node stats\n        N_total = n_s.sum()\n        C_total = c_s.sum()\n        \n        if N_total == 0:\n            return [1, 1, 0.0, 0.0, 0.5, 0.5]\n\n        p_hat_parent = C_total / N_total\n        H_parent = entropy(p_hat_parent)\n        G_parent = gini(p_hat_parent)\n\n        # Handle case with only one category where no split is possible\n        if K == 1:\n            pred = 1 if p_hat_parent >= 0.5 else 0\n            correct_mass = 0\n            if pred == 1:\n                correct_mass = (n_s * p_true_s).sum()\n            else:\n                correct_mass = (n_s * (1 - p_true_s)).sum()\n            acc = correct_mass / N_total if N_total > 0 else 0.5\n            return [1, 1, 0.0, 0.0, acc, acc]\n            \n        # 3. Iterate through splits\n        n_cumsum = np.cumsum(n_s)\n        c_cumsum = np.cumsum(c_s)\n\n        max_ig = -np.inf\n        t_star_h = 1\n        max_gd = -np.inf\n        t_star_g = 1\n\n        for i in range(K - 1):\n            t = i + 1\n\n            N_L = n_cumsum[i]\n            C_L = c_cumsum[i]\n            p_hat_L = C_L / N_L if N_L > 0 else 0.0\n\n            N_R = N_total - N_L\n            C_R = C_total - C_L\n            p_hat_R = C_R / N_R if N_R > 0 else 0.0\n            \n            H_cond = (N_L / N_total) * entropy(p_hat_L) + (N_R / N_total) * entropy(p_hat_R)\n            G_cond = (N_L / N_total) * gini(p_hat_L) + (N_R / N_total) * gini(p_hat_R)\n            \n            ig = H_parent - H_cond\n            gd = G_parent - G_cond\n\n            # First value found becomes the max\n            if i == 0:\n                max_ig = ig\n                max_gd = gd\n            \n            if ig > max_ig:\n                max_ig = ig\n                t_star_h = t\n            \n            if gd > max_gd:\n                max_gd = gd\n                t_star_g = t\n\n        # 4. Calculate accuracies\n        def calculate_accuracy(t_star):\n            # Left child\n            N_L = n_cumsum[t_star - 1]\n            C_L = c_cumsum[t_star - 1]\n            p_hat_L = C_L / N_L if N_L > 0 else 0.0\n            pred_L = 1 if p_hat_L >= 0.5 else 0\n\n            # Right child\n            N_R = N_total - N_L\n            C_R = C_total - C_L\n            p_hat_R = C_R / N_R if N_R > 0 else 0.0\n            pred_R = 1 if p_hat_R >= 0.5 else 0\n\n            total_correct_mass = 0.0\n            \n            # Left node contribution\n            n_L_cats = n_s[:t_star]\n            p_true_L_cats = p_true_s[:t_star]\n            if pred_L == 1:\n                total_correct_mass += np.sum(n_L_cats * p_true_L_cats)\n            else:\n                total_correct_mass += np.sum(n_L_cats * (1 - p_true_L_cats))\n            \n            # Right node contribution\n            n_R_cats = n_s[t_star:]\n            p_true_R_cats = p_true_s[t_star:]\n            if pred_R == 1:\n                total_correct_mass += np.sum(n_R_cats * p_true_R_cats)\n            else:\n                total_correct_mass += np.sum(n_R_cats * (1 - p_true_R_cats))\n            \n            return total_correct_mass / N_total\n\n        Acc_H = calculate_accuracy(t_star_h)\n        Acc_G = calculate_accuracy(t_star_g)\n\n        return [t_star_h, t_star_g, max_ig, max_gd, Acc_H, Acc_G]\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"n\": [200, 200, 200, 200, 5, 5, 5, 5, 5, 5, 5, 5],\n            \"c\": [130, 70, 120, 80, 5, 5, 5, 5, 5, 5, 5, 5],\n            \"p_true\": [0.6, 0.4, 0.6, 0.4, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5]\n        },\n        {\n            \"n\": [100, 100, 100, 100, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10],\n            \"c\": [55, 45, 60, 40, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9],\n            \"p_true\": [0.55, 0.45, 0.60, 0.40, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]\n        },\n        {\n            \"n\": [50, 50, 50, 50, 50, 50],\n            \"c\": [25, 25, 25, 25, 25, 25],\n            \"p_true\": [0.5, 0.5, 0.5, 0.5, 0.5, 0.5]\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        result = solve_case(case[\"n\"], case[\"c\"], case[\"p_true\"])\n        results.append(f\"[{','.join(f'{x:.7f}' for x in result)}]\")\n\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3112936"}, {"introduction": "在构建决策树时，贪婪地最大化信息增益可能会导致模型学习到训练数据中的偶然模式，从而产生过拟合，尤其是在处理高维或稀疏数据（如文本）时。本练习通过一个直观的文本分类场景，展示了一个稀有特征如何因在小样本中偶然地完美划分数据而获得极高的信息增益。通过亲手计算和比较，你将体会到这种过拟合的风险，并学习如何应用最小支持度（minimum support）这类简单而有效的约束来构建更稳健的模型。 [@problem_id:3131370]", "problem": "一个二元文本分类任务有两个类别，标记为 $y \\in \\{+1,-1\\}$。训练集包含 $N=20$ 个文档，其中 $N_{+}=10$ 个为正类，$N_{-}=10$ 个为负类。考虑两个候选二元特征，它们对应于文档中两个词的出现与否：罕见词“zephyr”和更常见的词“team”。在训练集中观察到的文档级共现计数如下：\n- 对于“zephyr”：在包含该词的 $n_{z}=4$ 个文档中，$4$ 个为正类，$0$ 个为负类；在不包含该词的 $N-n_{z}=16$ 个文档中，$6$ 个为正类，$10$ 个为负类。\n- 对于“team”：在包含该词的 $n_{t}=10$ 个文档中，$6$ 个为正类，$4$ 个为负类；在不包含该词的 $N-n_{t}=10$ 个文档中，$4$ 个为正类，$6$ 个为负类。\n\n仅使用信息论和统计学习中决策树分裂的标准定义，首先计算根据“zephyr”的出现与否进行分裂的信息增益（IG），然后计算根据“team”的出现与否进行分裂的信息增益，两者都以比特（即以 2 为底的对数）为单位。基于这些计算，论证为何罕见词在训练数据上看起来是一个强大的分裂器，以及为什么这可能反映了在文本环境中的过拟合。\n\n为缓解过拟合，施加一个最小支持度约束，要求任何选定的分裂词必须至少出现在 $s=5$ 个训练文档中。在此约束下，从两个候选中确定最佳可用分裂，并计算其信息增益（以比特为单位）。将在最小支持度约束 $s=5$ 下的最佳可用分裂的信息增益（四舍五入到 $4$ 位有效数字）作为你的最终答案报告。", "solution": "要解决这个问题，我们首先需要计算在两种情况下分裂前后的熵，然后计算信息增益。信息增益 (IG) 的定义是父节点的熵减去子节点熵的加权平均值：\n$$ IG(D, A) = H(D) - \\sum_{v \\in \\text{Values}(A)} \\frac{|D_v|}{|D|} H(D_v) $$\n其中 $H(D)$ 是数据集 $D$ 的熵，对于二元分类，其计算公式为 $H(p_+, p_-) = -p_+ \\log_2(p_+) - p_- \\log_2(p_-)$。\n\n**1. 计算父节点的熵**\n\n整个数据集包含 $N=20$ 个文档，其中 $N_+=10$ 个正类和 $N_-=10$ 个负类。因此，正类和负类的比例分别是 $p_+ = p_- = 10/20 = 0.5$。\n父节点的熵为：\n$$ H(\\text{父节点}) = -0.5 \\log_2(0.5) - 0.5 \\log_2(0.5) = - \\log_2(0.5) = 1 \\text{ 比特} $$\n\n**2. 计算特征 \"zephyr\" 的信息增益**\n\n- **子节点1 (包含 \"zephyr\"):** 包含 $n_z=4$ 个文档，其中 $4$ 个正类，$0$ 个负类。这个节点是纯的。\n  - $p_+ = 4/4 = 1, p_- = 0/4 = 0$\n  - 熵 $H(\\text{含zephyr}) = -1 \\log_2(1) - 0 = 0$ 比特。\n\n- **子节点2 (不含 \"zephyr\"):** 包含 $16$ 个文档，其中 $6$ 个正类，$10$ 个负类。\n  - $p_+ = 6/16 = 3/8, p_- = 10/16 = 5/8$\n  - 熵 $H(\\text{不含zephyr}) = -(\\frac{3}{8} \\log_2(\\frac{3}{8}) + \\frac{5}{8} \\log_2(\\frac{5}{8})) \\approx 0.9544$ 比特。\n\n- **信息增益:**\n$$ IG(\\text{zephyr}) = H(\\text{父节点}) - \\left[ \\frac{4}{20} H(\\text{含zephyr}) + \\frac{16}{20} H(\\text{不含zephyr}) \\right] $$\n$$ IG(\\text{zephyr}) = 1 - \\left[ \\frac{4}{20} \\cdot 0 + \\frac{16}{20} \\cdot 0.9544 \\right] = 1 - 0.8 \\cdot 0.9544 \\approx 1 - 0.7635 = 0.2365 \\text{ 比特} $$\n\n**3. 计算特征 \"team\" 的信息增益**\n\n- **子节点1 (包含 \"team\"):** 包含 $n_t=10$ 个文档，其中 $6$ 个正类，$4$ 个负类。\n  - $p_+ = 6/10 = 0.6, p_- = 4/10 = 0.4$\n  - 熵 $H(\\text{含team}) = - (0.6 \\log_2(0.6) + 0.4 \\log_2(0.4)) \\approx 0.9710$ 比特。\n\n- **子节点2 (不含 \"team\"):** 包含 $10$ 个文档，其中 $4$ 个正类，$6$ 个负类。\n  - $p_+ = 4/10 = 0.4, p_- = 6/10 = 0.6$\n  - 熵 $H(\\text{不含team}) = - (0.4 \\log_2(0.4) + 0.6 \\log_2(0.6)) \\approx 0.9710$ 比特。\n\n- **信息增益:**\n$$ IG(\\text{team}) = H(\\text{父节点}) - \\left[ \\frac{10}{20} H(\\text{含team}) + \\frac{10}{20} H(\\text{不含team}) \\right] $$\n$$ IG(\\text{team}) = 1 - \\left[ 0.5 \\cdot 0.9710 + 0.5 \\cdot 0.9710 \\right] = 1 - 0.9710 = 0.0290 \\text{ 比特} $$\n\n**论证与过拟合**\n\n比较信息增益，$IG(\\text{zephyr}) \\approx 0.2365$ 远大于 $IG(\\text{team}) \\approx 0.0290$。因此，仅从训练数据来看，\"zephyr\" 是一个更强大的分裂特征。它之所以强大，是因为它创建了一个完全纯净的子节点（所有含 \"zephyr\" 的文档都是正类）。然而，这种完美的分裂是基于一个非常小的样本（仅4个文档）。在文本分类中，罕见词由于随机性，可能偶然出现在一类文档中。模型学习到“如果文档包含‘zephyr’，则为正类”这样的规则，很可能是在学习训练数据中的噪声，而不是一个具有泛化能力的真实模式。这正是过拟合的典型表现。\n\n**4. 应用最小支持度约束**\n\n约束要求分裂词至少出现在 $s=5$ 个文档中。\n- 特征 \"zephyr\" 的支持度为 $n_z = 4$。由于 $4  5$，该特征不符合约束，**不能**被用作分裂。\n- 特征 \"team\" 的支持度为 $n_t = 10$。由于 $10 \\ge 5$，该特征符合约束，**可以**被使用。\n\n在约束下，唯一可用的分裂是基于 \"team\"。因此，最佳可用分裂的信息增益就是 $IG(\\text{team})$。\n\n**最终答案**\n\n最佳可用分裂的信息增益为 $IG(\\text{team}) \\approx 0.0290494$。四舍五入到4位有效数字，结果是 $0.02905$。", "answer": "$$\n\\boxed{0.02905}\n$$", "id": "3131370"}, {"introduction": "决策树本身是为处理离散特征而设计的，因此在应用于连续型特征时，必须先将其转化为离散的分割点。一个常见的预处理步骤是将连续特征“分箱”（binning），但分箱策略的选择会直接影响模型能够发现的模式和最终的信息增益。本练习将让你动手实现两种经典的离散化策略——等宽分箱（equal-width）和等频分箱（equal-frequency），并比较它们在不同数据分布下对寻找最佳分割点和最大化信息增益的影响。 [@problem_id:3131419]", "problem": "给定一个二元分类数据集，包含一个连续预测变量 $X \\in \\mathbb{R}$ 和一个二元类别标签 $Y \\in \\{0,1\\}$。考虑将 $X$ 离散化为 $K$ 个箱的两种方案：等宽分箱和等频分箱。任务是量化离散化方案的选择如何影响基于离散化后的箱进行单次分裂所能达到的最大信息增益（IG）。\n\n从不纯度的基本定义入手，即基于类别标签分布的香农熵。定义通过在相邻箱之间设置阈值将数据集划分为两个子集所实现的不纯度降低。不要假定任何预先推导好的公式；需要从概率和熵的基本原理推导出所需的量。\n\n对于固定的箱数 $K$，为每种离散化方案执行以下步骤：\n- 根据方案将每个样本分配到 $K$ 个箱中的一个。\n- 考虑所有在箱索引 $t$ 和 $t+1$ 之间设置阈值的分裂，其中 $t \\in \\{0,1,\\dots,K-2\\}$。对于每次分裂，左子节点包含所有索引 $\\leq t$ 的箱中的样本，右子节点包含所有索引 $\\geq t+1$ 的箱中的样本。导致子集为空的分裂，其信息增益必须视为零。\n- 基于经验类别标签分布的熵，计算每个合格分裂的信息增益，即从父节点到子节点的不纯度降低。\n- 返回该方案在所有合格阈值下的最大信息增益。\n\n离散化方案：\n- 等宽分箱：将范围 $[\\min(X), \\max(X)]$ 划分为 $K$ 个等宽的箱。所有值 $x = \\max(X)$ 必须分配给最高的箱索引。如果 $\\min(X) = \\max(X)$，则将所有样本分配到一个箱中。\n- 等频分箱：根据样本的排名将其分配到箱中，使得各箱的大小最多相差一。如果 $N$ 是样本数，$r_i \\in \\{0,1,\\dots,N-1\\}$ 是 $x_i$ 在 $X$ 升序排列中的排名，则将 $x_i$ 分配到箱索引 $\\left\\lfloor \\frac{K \\cdot r_i}{N} \\right\\rfloor$。\n\n您的程序必须实现这两种离散化方案，并为每个测试用例计算一对浮点数 $[IG_{\\text{EW}}, IG_{\\text{EF}}]$，其中 $IG_{\\text{EW}}$ 是等宽分箱下的最大信息增益，$IG_{\\text{EF}}$ 是等频分箱下的最大信息增益。计算熵时使用以2为底的对数。如果分裂产生一个空子节点，其信息增益定义为 $0$。如果父节点是纯的或 $K \\leq 1$，则信息增益必须为 $0$。\n\n测试套件：\n- 案例1（理想情况，单调关系）：$X = [0.05,0.10,0.12,0.20,0.25,0.33,0.40,0.48,0.51,0.58,0.60,0.66,0.72,0.79,0.85,0.90,0.93,0.96,0.98,0.99]$, $Y = [0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1]$, $K = 4$。\n- 案例2（纯父节点）：$X = [3.2,5.1,2.0,7.8,9.0,1.1,4.4,6.6,8.8,0.5]$, $Y = [1,1,1,1,1,1,1,1,1,1]$, $K = 4$。\n- 案例3（预测变量值相同）：$X = [1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]$, $Y = [0,1,0,1,0,1,0,1,0,1,0,1]$, $K = 5$。\n- 案例4a（偏斜且不平衡，箱数较少）：$X = [0.2,0.3,0.1,0.4,0.2,0.5,0.6,0.8,1.0,1.2,9.0,9.2,9.5,9.6,9.8,10.0,8.7,8.9,9.1,9.3,3.0,3.5,3.8,4.2,4.5,5.0,5.2,5.5,6.0,7.0]$, $Y = [0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,1,1,1,1,1]$, $K = 2$。\n- 案例4b（与案例4a相同，箱数较多）：$X$ 和 $Y$ 与案例4a相同, $K = 6$。\n\n最终输出格式：\n- 您的程序应生成单行输出，包含一个由列表组成的逗号分隔列表，每个内部列表为 $[IG_{\\text{EW}}, IG_{\\text{EF}}]$，四舍五入到六位小数，并用方括号括起来。例如：$[[a,b],[c,d],[e,f],[g,h],[i,j]]$。不应打印任何额外文本。", "solution": "该问题要求对两种数据离散化方案——等宽分箱和等频分箱——进行比较分析，通过评估它们对连续预测变量单次分裂所能达到的最大信息增益的影响。解决方案将从信息论的基本原理出发进行构建。\n\n### 基本原理：熵与信息增益\n\n**1. 香农熵**\n\n衡量一组分类数据不纯度的核心概念是香农熵。对于一个包含来自二元类别标签 $Y \\in \\{0, 1\\}$ 的样本的数据集 $D$，设 $N$ 为样本总数。设 $N_0$ 为标签为 $0$ 的样本数，$N_1$ 为标签为 $1$ 的样本数，使得 $N = N_0 + N_1$。每个类别的经验概率（比例）为 $p_0 = N_0/N$ 和 $p_1 = N_1/N$。\n\n数据集 $D$ 的香农熵，记为 $H(D)$，是其不纯度或不确定性的度量，定义为：\n$$\nH(D) = -\\sum_{c \\in \\{0,1\\}} p_c \\log_2(p_c) = -p_0 \\log_2(p_0) - p_1 \\log_2(p_1)\n$$\n按照惯例，我们定义 $0 \\log_2(0) \\equiv 0$。如果一个数据集是纯的（即所有样本都属于同一个类别），那么其中一个比例为 $1$，另一个为 $0$，导致熵为 $H(D) = -1 \\log_2(1) - 0 \\log_2(0) = 0$。对于二元分类问题，最大熵为 $1$，这发生在类别完全平衡时（$p_0 = p_1 = 0.5$）。\n\n**2. 信息增益**\n\n信息增益（IG）量化了将数据集 $D$ 划分成子集所实现的熵减少量。一次分裂将大小为 $N$ 的父数据集 $D$ 划分为两个子数据集，一个大小为 $N_L$ 的左子集 $D_L$ 和一个大小为 $N_R$ 的右子集 $D_R$，其中 $N = N_L + N_R$。\n\n子集的总熵是它们各自熵的加权平均值：\n$$\nH_{\\text{children}}(D_L, D_R) = \\frac{N_L}{N} H(D_L) + \\frac{N_R}{N} H(D_R)\n$$\n此次分裂的信息增益是父节点熵与子节点加权平均熵之差：\n$$\nIG(D, \\text{split}) = H(D) - H_{\\text{children}}(D_L, D_R)\n$$\n更高的IG表示一次更有效的分裂，因为它产生的子节点比父节点更纯（不确定性更低）。问题规定，如果一次分裂导致某个子节点为空（$N_L=0$ 或 $N_R=0$），其IG定义为 $0$。\n\n### 离散化与分裂算法\n\n首先将连续预测变量 $X$ 转换为具有 $K$ 个箱的分类属性。然后在两个相邻的箱之间进行分裂。分裂阈值 $t \\in \\{0, 1, \\dots, K-2\\}$ 将数据划分，使得左子节点 $D_L$ 包含所有被分配到索引小于或等于 $t$ 的箱中的样本，而右子节点 $D_R$ 包含所有被分配到索引大于或等于 $t+1$ 的箱中的样本。目标是找到能最大化信息增益的分裂 $t$。\n\n对于给定的离散化方案，总体算法如下：\n1.  验证平凡条件：如果箱数 $K \\leq 1$ 或初始数据集 $D$ 是纯的（$H(D)=0$），则最大IG为 $0$。\n2.  计算父节点的熵 $H(D)$。\n3.  根据所选方案将连续特征 $X$ 离散化为 $K$ 个箱，为每个样本分配一个箱索引。\n4.  初始化变量 $\\text{max\\_IG} = 0$。\n5.  遍历从 $0$ 到 $K-2$ 的所有可能的分裂阈值 $t$。\n    a.  将数据集划分为 $D_L$（箱索引 $\\leq t$）和 $D_R$（箱索引 $> t$）。\n    b.  如果任一子集为空，则此次分裂的IG为 $0$。否则，使用上述公式计算 $IG(D, \\text{split})$。\n    c.  更新 $\\text{max\\_IG} = \\max(\\text{max\\_IG}, IG(D, \\text{split}))$。\n6.  返回 $\\text{max\\_IG}$。\n\n### 离散化方案\n\n**1. 等宽分箱**\n\n该方案将 $X$ 的范围划分为 $K$ 个等宽的区间。\n- 设 $\\min(X)$ 和 $\\max(X)$ 为预测变量的最小值和最大值。如果 $\\min(X) = \\max(X)$，所有样本都被分配到单个箱（索引为 $0$）中，无法进行有意义的分裂，因此IG为 $0$。\n- 否则，每个箱的宽度计算为 $w = (\\max(X) - \\min(X)) / K$。\n- 对于值为 $x_i$ 的样本，确定其箱索引。根据规定，任何值为 $x_i = \\max(X)$ 的样本都被分配到最后一个箱，即索引 $K-1$。对于任何其他值为 $x_i  \\max(X)$ 的样本，其箱索引由 $\\lfloor (x_i - \\min(X)) / w \\rfloor$ 给出。\n\n**2. 等频分箱**\n\n该方案旨在将等量的样本放入每个箱中。\n- 首先根据样本的 $X$ 值进行排序。设 $N$ 为样本总数。每个样本被赋予一个排名 $r_i \\in \\{0, 1, \\dots, N-1\\}$，其中排名 $0$ 对应 $X$ 值最小的样本。\n- 为了一致地处理 $X$ 值中可能存在的并列情况，使用稳定排序算法，该算法会保留并列元素的原始相对顺序。\n- 然后使用提供的公式计算排名为 $r_i$ 的样本的箱索引：$\\lfloor (K \\cdot r_i) / N \\rfloor$。这将 $N$ 个已排序的样本划分到 $K$ 个箱中，每个箱包含 $\\lfloor N/K \\rfloor$ 或 $\\lceil N/K \\rceil$ 个样本。\n\n通过为两种方案和每个测试用例实施这一完整流程，我们可以系统地计算并比较可实现的最大信息增益。", "answer": "```python\nimport numpy as np\n\ndef calculate_entropy(y: np.ndarray) -> float:\n    \"\"\"Computes the Shannon entropy for a binary label array.\"\"\"\n    n = len(y)\n    if n == 0:\n        return 0.0\n\n    # Assumes y contains 0s and 1s\n    p1 = np.sum(y) / n\n    p0 = 1.0 - p1\n\n    if p0 == 0.0 or p1 == 0.0:\n        return 0.0\n\n    return -p0 * np.log2(p0) - p1 * np.log2(p1)\n\ndef get_max_ig(X: np.ndarray, Y: np.ndarray, K: int, mode: str) -> float:\n    \"\"\"\n    Computes the maximum Information Gain for a given discretization scheme.\n\n    Args:\n        X: Continuous predictor variable.\n        Y: Binary class labels (0 or 1).\n        K: Number of bins.\n        mode: Discretization scheme ('equal_width' or 'equal_frequency').\n\n    Returns:\n        The maximum Information Gain.\n    \"\"\"\n    N = len(X)\n    if K = 1 or N == 0:\n        return 0.0\n\n    parent_entropy = calculate_entropy(Y)\n    if parent_entropy == 0.0:\n        return 0.0\n\n    # Step 1: Discretize X into bin indices\n    bin_indices = np.zeros(N, dtype=int)\n    if mode == 'equal_width':\n        min_x, max_x = np.min(X), np.max(X)\n        if min_x == max_x:\n            # All values are the same, they go to one bin. No split possible.\n            bin_indices.fill(0)\n        else:\n            width = (max_x - min_x) / K\n            # Assign bins, with special handling for max_x\n            is_max = (X == max_x)\n            # Use np.floor and then correct for max_x to avoid floating point issues\n            # on boundaries.\n            bin_indices = np.floor((X - min_x) / width).astype(int)\n            # Clamp bin indices to be within [0, K-1]. np.floor on max_x will give K.\n            bin_indices = np.clip(bin_indices, 0, K - 1)\n            # Ensure max_x is in the last bin as per rule\n            bin_indices[is_max] = K - 1\n            \n    elif mode == 'equal_frequency':\n        # Stable sort is required for ties, as per problem description\n        # argsort with kind 'mergesort' is stable\n        sorted_indices = np.argsort(X, kind='mergesort')\n        ranks = np.empty_like(sorted_indices)\n        ranks[sorted_indices] = np.arange(N)\n        bin_indices = np.floor(K * ranks / N).astype(int)\n    \n    else:\n        raise ValueError(\"Invalid discretization mode.\")\n\n    # Step 2: Iterate through possible splits to find max IG\n    max_ig = 0.0\n    for t in range(K - 1):\n        left_mask = (bin_indices = t)\n        right_mask = (bin_indices > t)\n\n        y_left = Y[left_mask]\n        y_right = Y[right_mask]\n\n        n_left = len(y_left)\n        n_right = len(y_right)\n\n        if n_left == 0 or n_right == 0:\n            # As per problem, splits resulting in an empty child have IG=0\n            ig = 0.0\n        else:\n            entropy_left = calculate_entropy(y_left)\n            entropy_right = calculate_entropy(y_right)\n            \n            weighted_child_entropy = (n_left / N) * entropy_left + (n_right / N) * entropy_right\n            ig = parent_entropy - weighted_child_entropy\n        \n        if ig > max_ig:\n            max_ig = ig\n            \n    return max_ig\n\ndef solve():\n    \"\"\"\n    Main function to run test cases and print results.\n    \"\"\"\n    test_cases = [\n        # Case 1\n        (\n            np.array([0.05,0.10,0.12,0.20,0.25,0.33,0.40,0.48,0.51,0.58,0.60,0.66,0.72,0.79,0.85,0.90,0.93,0.96,0.98,0.99]),\n            np.array([0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1]),\n            4\n        ),\n        # Case 2\n        (\n            np.array([3.2,5.1,2.0,7.8,9.0,1.1,4.4,6.6,8.8,0.5]),\n            np.array([1,1,1,1,1,1,1,1,1,1]),\n            4\n        ),\n        # Case 3\n        (\n            np.array([1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]),\n            np.array([0,1,0,1,0,1,0,1,0,1,0,1]),\n            5\n        ),\n        # Case 4a\n        (\n            np.array([0.2,0.3,0.1,0.4,0.2,0.5,0.6,0.8,1.0,1.2,9.0,9.2,9.5,9.6,9.8,10.0,8.7,8.9,9.1,9.3,3.0,3.5,3.8,4.2,4.5,5.0,5.2,5.5,6.0,7.0]),\n            np.array([0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,1,1,1,1,1]),\n            2\n        ),\n        # Case 4b\n        (\n            np.array([0.2,0.3,0.1,0.4,0.2,0.5,0.6,0.8,1.0,1.2,9.0,9.2,9.5,9.6,9.8,10.0,8.7,8.9,9.1,9.3,3.0,3.5,3.8,4.2,4.5,5.0,5.2,5.5,6.0,7.0]),\n            np.array([0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,1,1,1,1,1]),\n            6\n        )\n    ]\n    \n    results = []\n    for x_data, y_data, k_bins in test_cases:\n        ig_ew = get_max_ig(x_data, y_data, k_bins, 'equal_width')\n        ig_ef = get_max_ig(x_data, y_data, k_bins, 'equal_frequency')\n        results.append([ig_ew, ig_ef])\n\n    # Format output as specified\n    formatted_results = ','.join([f'[{v[0]:.6f},{v[1]:.6f}]' for v in results])\n    print(f\"[{formatted_results}]\")\n\nsolve()\n\n```", "id": "3131419"}]}