{"hands_on_practices": [{"introduction": "提升算法的核心思想是积跬步以至千里，将“弱”学习器组合成强大的模型。然而，基学习器的“弱”并非没有下限；其表达能力直接决定了集成模型的最终潜力。本练习将通过经典的异或（XOR）问题，直观地展示决策树桩（深度为1的决策树）在面对非线性可分数据时的局限性，并揭示为何选择一个具备足够表达能力的基学习器是成功构建模型的关键第一步 [@problem_id:3105953]。", "problem": "考虑标签在 $\\{-1,+1\\}$ 中、特征在 $\\{0,1\\}^d$ 中的二元分类问题。任务是设计并分析一个在两种轴对齐基学习器类别上运行的提升程序：深度为 $1$ 的决策桩和深度为 $2$ 的决策树。分析必须基于经验风险最小化和提升方法中弱学习器概念的基本定义，而不是依赖于问题陈述中的快捷公式。程序必须实现一个用于二元分类的、带有指数损失的分阶段加性提升程序，其中在每一轮选择一个基学习器，以最小化关于当前训练样本分布的加权经验分类误差。决策桩定义为一棵深度为 $1$ 的树，它在阈值 $0.5$ 处对单个特征进行分裂，并为两个叶节点分配 $\\{-1,+1\\}$ 中的恒定标签。深度为 $2$ 的决策树定义为一棵具有两次连续轴对齐分裂的树，分裂发生在阈值 $0.5$ 处，可能针对不同的特征，最终产生四个叶节点，每个叶节点都分配一个 $\\{-1,+1\\}$ 中的恒定标签。\n\n在您的程序中构建并包含以下数据集测试套件。每个数据集都由一个明确的点列表和相应的标签给出，以整数形式表示，但解释为特征的 $\\{0,1\\}^d$ 元素和标签的 $\\{-1,+1\\}$ 元素：\n\n- 测试用例 $1$ （二维中的类似异或的对抗模式）：特征维度 $d=2$。点为 $x_1=(0,0)$, $x_2=(0,1)$, $x_3=(1,0)$, $x_4=(1,1)$，标签为 $y_1=-1$, $y_2=+1$, $y_3=+1$, $y_4=-1$。\n- 测试用例 $2$ （一维中的线性可分模式）：特征维度 $d=1$。点为 $x_1=(0)$, $x_2=(0)$, $x_3=(1)$, $x_4=(1)$，标签为 $y_1=-1$, $y_2=-1, y_3=+1, y_4=+1$。\n- 测试用例 $3$ （矛盾的重复样本）：特征维度 $d=2$。点为 $x_1=(0,0)$, $x_2=(0,0)$, $x_3=(1,1)$, $x_4=(1,1)$，标签为 $y_1=-1, y_2=+1, y_3=+1, y_4=-1$。\n\n对于每个数据集，运行两个提升实验：一个使用决策桩（深度 $1$）作为基学习器类别，另一个使用深度为 $2$ 的决策树作为基学习器类别。在两个实验中，都通过对指数损失进行分阶段加性建模来推进，并在每一轮选择一个基分类器，以最小化当前的加权经验分类误差。继续添加基学习器，直到训练集上的经验分类误差变为零，此时所用的轮数记录为 $T$。如果因为在某个阶段没有基学习器能将加权误差降低到 $0.5$ 以下，或者函数类别无法表示这些标签，导致程序无法实现零经验分类误差，则记录 $T=+\\infty$。\n\n您的程序必须实现这些过程，并作为其最终输出，生成一个包含在方括号内的、以逗号分隔的列表的单行。该列表必须包含六个结果，按以下顺序排列：$[T_{1,\\text{stump}},T_{1,\\text{depth2}},T_{2,\\text{stump}},T_{2,\\text{depth2}},T_{3,\\text{stump}},T_{3,\\text{depth2}}]$，其中 $T_{k,\\text{stump}}$ 是在数据集 $k$ 上使用决策桩所需的轮数，而 $T_{k,\\text{depth2}}$ 是在数据集 $k$ 上使用深度为 $2$ 的决策树所需的轮数。对于有限的 $T$，值必须是整数；对于不可能的情况，值为 $+\\infty$，打印为编程语言所识别的浮点数表示（例如，在 Python 中，这被打印为 $\\text{inf}$）。本问题不涉及任何物理单位、角度单位或百分比；所有输出都是无量纲的整数或浮点无穷大符号。程序必须是完全自包含的，并且不需要任何输入。", "solution": "问题要求设计和分析一个用于二元分类的提升算法。解决方案将首先建立算法的理论基础，然后定义特定的基学习器类别，最后将该算法应用于三个提供的测试用例，以确定达到零经验误差所需的轮数 $T$。\n\n### 1. 理论基础：使用指数损失的分阶段加性提升\n\n提升方法的目标是构建一个强分类器 $F(\\mathbf{x})$，作为简单或“弱”学习器 $h_t(\\mathbf{x})$ 的加权和。对于标签为 $y \\in \\{-1, +1\\}$ 的二元分类问题，最终的预测由这个和的符号给出：$\\hat{y} = \\text{sign}(F(\\mathbf{x}))$。分类器是以分阶段的方式迭代构建的。从一个空分类器 $F_0(\\mathbf{x}) = 0$ 开始，在每一轮 $t=1, 2, \\dots, T$，我们添加一个新的弱学习器 $h_t$ 及其对应的权重 $\\alpha_t$：\n$$F_t(\\mathbf{x}) = F_{t-1}(\\mathbf{x}) + \\alpha_t h_t(\\mathbf{x})$$\n\n在每个阶段选择 $h_t$ 和 $\\alpha_t$ 的原则是最小化在训练数据 $\\{(\\mathbf{x}_i, y_i)\\}_{i=1}^N$ 上的经验损失函数。问题指定使用指数损失，对于分类器 $F_t$，其定义为：\n$$R_{\\text{emp}}(F_t) = \\sum_{i=1}^N \\exp(-y_i F_t(\\mathbf{x}_i))$$\n\n将分阶段更新规则代入损失函数，我们寻求最小化：\n$$R_{\\text{emp}}(F_t) = \\sum_{i=1}^N \\exp(-y_i (F_{t-1}(\\mathbf{x}_i) + \\alpha_t h_t(\\mathbf{x}_i))) = \\sum_{i=1}^N \\exp(-y_i F_{t-1}(\\mathbf{x}_i)) \\exp(-y_i \\alpha_t h_t(\\mathbf{x}_i))$$\n\n让我们将第 $t$ 轮开始时每个样本 $i$ 的权重定义为 $w_i^{(t)} = \\exp(-y_i F_{t-1}(\\mathbf{x}_i))$。这些权重由前一轮 $t-1$ 预先确定。因此，第 $t$ 轮的最小化问题变为：\n$$(\\alpha_t, h_t) = \\arg\\min_{\\alpha, h} \\sum_{i=1}^N w_i^{(t)} \\exp(-y_i \\alpha h(\\mathbf{x}_i))$$\n\n这个最小化过程通常分两步进行：\n$1$.  **找到最佳弱学习器 $h_t$**：问题指示我们选择能够最小化当前加权经验分类误差的基分类器。这是 AdaBoost 框架中一个常用且合理的启发式方法。项 $\\exp(-y_i \\alpha h(\\mathbf{x}_i))$ 可以根据 $h(\\mathbf{x}_i)$ 是否正确分类 $y_i$ 来分开。由于 $y_i, h(\\mathbf{x}_i) \\in \\{-1, +1\\}$，$y_i h(\\mathbf{x}_i)=1$ 表示正确分类，$y_i h(\\mathbf{x}_i)=-1$ 表示错误分类。对于一个固定的 $\\alpha > 0$，这个和可以写成：\n    $$\\sum_{i:y_i=h(\\mathbf{x}_i)} w_i^{(t)} e^{-\\alpha} + \\sum_{i:y_i \\neq h(\\mathbf{x}_i)} w_i^{(t)} e^{\\alpha}$$\n    在 $h$ 上最小化此表达式等价于最小化错误分类的加权和 $\\sum_{i=1}^N w_i^{(t)} \\mathbb{I}(y_i \\neq h(\\mathbf{x}_i))$，其中 $\\mathbb{I}(\\cdot)$ 是指示函数。将初始权重 $w_i^{(t)}$ 归一化使其和为 $1$（我们称之为 $W_i^{(t)}$）后，这等价于找到最小化加权分类误差的学习器 $h_t$：\n    $$h_t = \\arg\\min_h \\sum_{i=1}^N W_i^{(t)} \\mathbb{I}(y_i \\neq h(\\mathbf{x}_i)) = \\arg\\min_h \\epsilon_t(h)$$\n\n$2$.  **找到最优系数 $\\alpha_t$**：一旦确定了 $h_t$ 及其误差 $\\epsilon_t$，我们通过求解 $\\frac{\\partial R_{\\text{emp}}}{\\partial \\alpha} = 0$ 来找到最优的 $\\alpha_t$。这会得到众所周知的公式：\n    $$\\alpha_t = \\frac{1}{2} \\ln \\left(\\frac{1 - \\epsilon_t}{\\epsilon_t}\\right)$$\n    为了使 $\\alpha_t$ 为实数且为正，弱学习器必须优于随机猜测，即 $\\epsilon_t  0.5$。如果找不到这样的学习器，提升过程无法继续。\n\n$3$.  **更新权重**：找到 $\\alpha_t$ 和 $h_t$ 后，更新下一轮 $t+1$ 的样本权重，以给予被 $h_t$ 错误分类的样本更多重要性：\n    $$W_i^{(t+1)} \\propto W_i^{(t)} \\exp(-y_i \\alpha_t h_t(\\mathbf{x}_i))$$\n    然后将这些权重重新归一化，使其和为 $1$。\n\n当组合分类器 $F_T(\\mathbf{x}) = \\sum_{t=1}^T \\alpha_t h_t(\\mathbf{x})$ 达到零经验分类误差，即对所有 $i=1, \\dots, N$ 都有 $\\text{sign}(F_T(\\mathbf{x}_i)) = y_i$ 时，程序终止。\n\n### 2. 基学习器类别\n\n特征是二元的，$\\mathbf{x} \\in \\{0,1\\}^d$，分裂发生在阈值 $0.5$ 处。这将分裂简化为测试特征 $x_j$ 是 $0$ 还是 $1$。\n\n-   **决策桩（深度 1）**：决策桩是只有一个分裂的决策树。它由一个用于分裂的特征 $j \\in \\{0, \\dots, d-1\\}$ 和两个叶节点值 $c_0, c_1 \\in \\{-1,+1\\}$ 定义。分类规则是如果 $x_j=0$，则 $h(\\mathbf{x}) = c_0$；如果 $x_j=1$，则 $h(\\mathbf{x}) = c_1$。为了找到最好的决策桩，我们遍历所有特征 $j$。对于每个 $j$，我们找到最优的叶节点值 $c_0$ 和 $c_1$，以最小化数据两个分区上的加权误差。\n\n-   **决策树（深度 2）**：这种树有一个根分裂和两个子分裂。它由一个根分裂特征 $j_1$ 定义，对于每个分支（$x_{j_1}=0$ 和 $x_{j_1}=1$），有一个后续的分裂特征（分别为 $j_{2,0}$ 和 $j_{2,1}$）和相应的叶节点标签。这会创建 $4$ 个叶节点。通过遍历所有可能的根分裂特征 $j_1$ 来找到最佳的深度为 $2$ 的树。对于 $j_1$ 的每种选择，数据被分区，在每个子集上，我们找到最佳的决策桩来形成树的第二层。选择能产生最小总加权误差的 $j_1$ 和两个子决策桩的组合。\n\n### 3. 测试用例分析\n\n当 $\\sum_{i=1}^N \\mathbb{I}(\\text{sign}(F_T(\\mathbf{x}_i)) \\neq y_i) = 0$ 时，程序终止并报告 $T$。如果不可能达到，则 $T=+\\infty$。\n\n**测试用例 1：类似异或的模式**\n-   数据：$\\mathbf{x}_1=(0,0), y_1=-1$; $\\mathbf{x}_2=(0,1), y_2=+1$; $\\mathbf{x}_3=(1,0), y_3=+1$; $\\mathbf{x}_4=(1,1), y_4=-1$。\n-   **实验 1（决策桩）**：在第 $t=1$ 轮，权重是均匀的：对于 $i=1, \\dots, 4$，有 $W_i^{(1)} = 1/4$。\n    -   考虑一个在特征 $j=0$ 上分裂的决策桩。数据被划分为 $\\{(\\mathbf{x}_1, y_1), (\\mathbf{x}_2, y_2)\\}$ 和 $\\{(\\mathbf{x}_3, y_3), (\\mathbf{x}_4, y_4)\\}$。在每个分区中，标签都是 $\\{-1, +1\\}$，且权重相等。最佳的叶节点标签将正确分类一个点并错误分类另一个点，每个分区贡献 $1/4$ 的误差。总加权误差为 $\\epsilon_1 = 1/4 + 1/4 = 0.5$。\n    -   通过对称性，在特征 $j=1$ 上的分裂也得到最小误差 $\\epsilon_1 = 0.5$。\n    -   由于最佳弱学习器的加权误差不严格小于 $0.5$，提升准则未被满足。程序停止。\n    -   结果：$T_{1,\\text{stump}} = +\\infty$。\n-   **实验 2（深度 2 决策树）**：在第 $t=1$ 轮，权重均匀，我们搜索最佳的深度为 $2$ 的树。\n    -   一棵深度为 $2$ 的树可以完美地分类异或模式。例如，首先在特征 $j_1=0$ 上分裂。左分支包含 $\\{(\\mathbf{x}_1, y_1), (\\mathbf{x}_2, y_2)\\}$，右分支包含 $\\{(\\mathbf{x}_3, y_3), (\\mathbf{x}_4, y_4)\\}$。然后，在特征 $j_2=1$ 上分裂两个分支。这将四个点中的每一个都隔离到自己的叶节点中。\n    -   叶节点 $1$：$x_0=0, x_1=0 \\implies$ 点 $\\mathbf{x}_1$。标签可设为 $y_1=-1$。\n    -   叶节点 $2$：$x_0=0, x_1=1 \\implies$ 点 $\\mathbf{x}_2$。标签可设为 $y_2=+1$。\n    -   叶节点 $3$：$x_0=1, x_1=0 \\implies$ 点 $\\mathbf{x}_3$。标签可设为 $y_3=+1$。\n    -   叶节点 $4$：$x_0=1, x_1=1 \\implies$ 点 $\\mathbf{x}_4$。标签可设为 $y_4=-1$。\n    -   这棵树 $h_1$ 完美地分类了所有训练数据，所以它的加权误差为 $\\epsilon_1=0$。\n    -   当 $\\epsilon_1=0$ 时，$\\alpha_1 = \\infty$。最终的分类器 $F_1(\\mathbf{x}) = \\alpha_1 h_1(\\mathbf{x})$ 的预测将是 $\\text{sign}(F_1(\\mathbf{x}_i)) = h_1(\\mathbf{x}_i) = y_i$。一轮之后，总经验误差为 $0$。\n    -   结果：$T_{1,\\text{depth2}} = 1$。\n\n**测试用例 2：线性可分模式**\n-   数据：$\\mathbf{x}_1=(0), y_1=-1$; $\\mathbf{x}_2=(0), y_2=-1$; $\\mathbf{x}_3=(1), y_3=+1$; $\\mathbf{x}_4=(1), y_4=+1$。\n-   **实验 3（决策桩）**：在 $t=1$ 时，权重为 $W_i^{(1)} = 1/4$。只有一个特征 $j=0$。\n    -   在 $j=0$ 上的决策桩将数据划分为 $\\{(\\mathbf{x}_1, y_1), (\\mathbf{x}_2, y_2)\\}$ 和 $\\{(\\mathbf{x}_3, y_3), (\\mathbf{x}_4, y_4)\\}$。\n    -   对于 $x_0=0$ 的分区，两个标签都是 $-1$。最优的叶节点标签是 $c_0=-1$，误差为 $0$。\n    -   对于 $x_0=1$ 的分区，两个标签都是 $+1$。最优的叶节点标签是 $c_1=+1$，误差为 $0$。\n    -   得到的决策桩是该数据的完美分类器，实现加权误差 $\\epsilon_1=0$。如上例所示，这将导致在一轮后以零误差终止。\n    -   结果：$T_{2,\\text{stump}} = 1$。\n-   **实验 4（深度 2 决策树）**：深度为 $2$ 的决策树类别包含了所有深度为 $1$ 的决策桩。由于决策桩提供了一个误差 $\\epsilon_1=0$ 的完美分类器，算法将会找到它（或一个等效的深度为 2 的树）并在一轮内终止。\n    -   结果：$T_{2,\\text{depth2}} = 1$。\n\n**测试用例 3：矛盾的重复样本**\n-   数据：$\\mathbf{x}_1=(0,0), y_1=-1$; $\\mathbf{x}_2=(0,0), y_2=+1$; $\\mathbf{x}_3=(1,1), y_3=+1$; $\\mathbf{x}_4=(1,1), y_4=-1$。\n-   **实验 5 和 6（决策桩和深度 2 决策树）**：这个数据集存在一个根本性问题。特征向量 $(0,0)$ 与两个不同的标签 $-1$ 和 $+1$ 相关联。特征向量 $(1,1)$ 也是如此。\n    -   任何确定性分类器 $f(\\mathbf{x})$ 对于给定的输入必须产生单一的输出。因此，对于 $\\mathbf{x}=(0,0)$，$f((0,0))$ 可以是 $-1$ 或 $+1$，但不能同时是两者。它将不可避免地错误分类 $(\\mathbf{x}_1, y_1)$ 或 $(\\mathbf{x}_2, y_2)$。同样的逻辑也适用于 $\\mathbf{x}=(1,1)$。\n    -   这意味着没有任何确定性分类器，无论其复杂性如何（决策桩、深度为 $2$ 的树，或任何其他类型），能够在该数据集上实现零经验误差。所需的终止条件，即零误差，是无法达到的。\n    -   问题陈述中提到，如果函数类别无法表示这些标签，则 $T=+\\infty$。这里正是这种情况。\n    -   结果：$T_{3,\\text{stump}} = +\\infty$ 且 $T_{3,\\text{depth2}} = +\\infty$。\n\n总之，预期的结果是 $[+\\infty, 1, 1, 1, +\\infty, +\\infty]$。", "answer": "```python\nimport numpy as np\n\ndef has_contradictions(X, y):\n    \"\"\"\n    Checks if the dataset contains contradictory examples, i.e.,\n    identical feature vectors with different labels.\n    \"\"\"\n    try:\n        # Use a string representation for hashable rows\n        # This is more robust for floating point issues as well, though not needed here.\n        # np.unique with axis=0 is also a good option for fully numeric data.\n        seen = {}\n        for i in range(X.shape[0]):\n            key = tuple(X[i])\n            if key in seen and seen[key] != y[i]:\n                return True\n            seen[key] = y[i]\n        return False\n    except TypeError: # Fallback for non-hashable types, not expected here\n        unique_X, inverse_indices = np.unique(X, axis=0, return_inverse=True)\n        for i in range(len(unique_X)):\n            indices = np.where(inverse_indices == i)[0]\n            if len(indices) > 1 and len(np.unique(y[indices])) > 1:\n                return True\n        return False\n\ndef find_best_stump(X, y, W):\n    \"\"\"\n    Finds the best decision stump for the given data and weights.\n    A stump is a tree of depth 1.\n    \"\"\"\n    N, d = X.shape\n    best_stump = {'error': float('inf')}\n\n    for j in range(d):\n        # Data points where feature j is 0 or 1\n        indices0 = np.where(X[:, j] == 0)[0]\n        indices1 = np.where(X[:, j] == 1)[0]\n        \n        # Determine best leaf value c0 for partition 0\n        w_plus0 = np.sum(W[indices0][y[indices0] == 1])\n        w_minus0 = np.sum(W[indices0][y[indices0] == -1])\n        c0 = 1 if w_plus0 >= w_minus0 else -1\n        error0 = w_minus0 if c0 == 1 else w_plus0\n\n        # Determine best leaf value c1 for partition 1\n        w_plus1 = np.sum(W[indices1][y[indices1] == 1])\n        w_minus1 = np.sum(W[indices1][y[indices1] == -1])\n        c1 = 1 if w_plus1 >= w_minus1 else -1\n        error1 = w_minus1 if c1 == 1 else w_plus1\n\n        total_error = error0 + error1\n\n        if total_error  best_stump['error']:\n            best_stump['error'] = total_error\n            best_stump['feature'] = j\n            best_stump['c0'] = c0\n            best_stump['c1'] = c1\n            \n    # Generate predictions for the best stump found\n    preds = np.zeros(N)\n    j = best_stump['feature']\n    c0 = best_stump['c0']\n    c1 = best_stump['c1']\n    indices0 = np.where(X[:, j] == 0)[0]\n    indices1 = np.where(X[:, j] == 1)[0]\n    preds[indices0] = c0\n    preds[indices1] = c1\n    \n    return best_stump, best_stump['error'], preds\n\ndef find_best_stump_for_subset(X, y, W, subset_indices):\n    \"\"\"\n    Helper function to find the best stump on a subset of data.\n    Returns info, error contribution on subset, and predictions for subset.\n    \"\"\"\n    if len(subset_indices) == 0:\n        return {}, 0.0, np.array([])\n\n    _, d = X.shape\n    best_stump = {'error': float('inf')}\n    \n    # We must iterate over all possible features for the split\n    for j in range(d):\n        # Further partition the subset\n        sub_indices0 = subset_indices[X[subset_indices, j] == 0]\n        sub_indices1 = subset_indices[X[subset_indices, j] == 1]\n        \n        w_plus0 = np.sum(W[sub_indices0][y[sub_indices0] == 1])\n        w_minus0 = np.sum(W[sub_indices0][y[sub_indices0] == -1])\n        c0 = 1 if w_plus0 >= w_minus0 else -1\n        error0 = w_minus0 if c0 == 1 else w_plus0\n\n        w_plus1 = np.sum(W[sub_indices1][y[sub_indices1] == 1])\n        w_minus1 = np.sum(W[sub_indices1][y[sub_indices1] == -1])\n        c1 = 1 if w_plus1 >= w_minus1 else -1\n        error1 = w_minus1 if c1 == 1 else w_plus1\n        \n        total_error = error0 + error1\n        \n        if total_error  best_stump['error']:\n            best_stump['error'] = total_error\n            best_stump['split_feature'] = j\n            best_stump['c0'] = c0\n            best_stump['c1'] = c1\n            \n    # Generate predictions for the points in subset_indices\n    preds_subset = np.zeros(len(subset_indices))\n    j = best_stump['split_feature']\n    c0 = best_stump['c0']\n    c1 = best_stump['c1']\n    \n    # Map from subset indices to relative indices for preds_subset\n    original_to_subset_map = {orig_idx: i for i, orig_idx in enumerate(subset_indices)}\n    \n    sub_indices0 = subset_indices[X[subset_indices, j] == 0]\n    sub_indices1 = subset_indices[X[subset_indices, j] == 1]\n\n    for idx in sub_indices0:\n        preds_subset[original_to_subset_map[idx]] = c0\n    for idx in sub_indices1:\n        preds_subset[original_to_subset_map[idx]] = c1\n\n    return best_stump, best_stump['error'], preds_subset\n\n\ndef find_best_depth2_tree(X, y, W):\n    \"\"\"\n    Finds the best decision tree of depth 2.\n    \"\"\"\n    N, d = X.shape\n    best_tree = {'error': float('inf')}\n\n    for j1 in range(d):\n        # Root split feature\n        indices0 = np.where(X[:, j1] == 0)[0]\n        indices1 = np.where(X[:, j1] == 1)[0]\n        \n        stump0, error0, preds0 = find_best_stump_for_subset(X, y, W, indices0)\n        stump1, error1, preds1 = find_best_stump_for_subset(X, y, W, indices1)\n        \n        total_error = error0 + error1\n        \n        if total_error  best_tree['error']:\n            best_tree['error'] = total_error\n            best_tree['j1'] = j1\n            best_tree['stump0_info'] = stump0\n            best_tree['stump1_info'] = stump1\n            \n            # Reconstruct full prediction vector\n            preds = np.zeros(N)\n            preds[indices0] = preds0\n            preds[indices1] = preds1\n            best_tree['preds'] = preds\n\n    return best_tree, best_tree['error'], best_tree.get('preds')\n\ndef run_boosting(X, y, learner_class, max_rounds=100):\n    \"\"\"\n    Runs the boosting procedure.\n    \"\"\"\n    if has_contradictions(X, y):\n        return float('inf')\n\n    N = X.shape[0]\n    W = np.ones(N) / N\n    F = np.zeros(N) # Stores the cumulative score F(x_i)\n\n    for t in range(1, max_rounds + 1):\n        if learner_class == 'stump':\n            _, error, h_preds = find_best_stump(X, y, W)\n        elif learner_class == 'depth2':\n            _, error, h_preds = find_best_depth2_tree(X, y, W)\n        else:\n            raise ValueError(\"Unknown learner class\")\n\n        # AdaBoost condition for weak learner\n        if error >= 0.5:\n            return float('inf')\n\n        # Handle perfect learner case to avoid division by zero\n        # A small epsilon is used for numerical stability.\n        if error  np.finfo(float).eps:\n            alpha = 1e9 # A large finite number simulating infinity\n        else:\n            alpha = 0.5 * np.log((1 - error) / error)\n\n        F += alpha * h_preds\n        \n        # Check for termination condition: zero empirical error\n        misclassified = np.sign(F) != y\n        if not np.any(misclassified):\n            return t\n            \n        # Update weights\n        W = W * np.exp(-alpha * y * h_preds)\n        W /= np.sum(W) # Normalize\n\n    # If loop finishes without reaching zero error, it's considered a failure.\n    return float('inf')\n\ndef solve():\n    # Test case 1 (XOR-like adversarial pattern)\n    X1 = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n    y1 = np.array([-1, 1, 1, -1])\n\n    # Test case 2 (Linearly separable pattern)\n    X2 = np.array([[0], [0], [1], [1]])\n    y2 = np.array([-1, -1, 1, 1])\n\n    # Test case 3 (Contradictory duplicates)\n    X3 = np.array([[0, 0], [0, 0], [1, 1], [1, 1]])\n    y3 = np.array([-1, 1, 1, -1])\n\n    test_cases = [\n        (X1, y1),\n        (X2, y2),\n        (X3, y3)\n    ]\n\n    results = []\n    for X, y in test_cases:\n        t_stump = run_boosting(X, y, 'stump')\n        t_depth2 = run_boosting(X, y, 'depth2')\n        results.extend([t_stump, t_depth2])\n    \n    # Format the results as string, handling inf\n    str_results = [str(r) if r != float('inf') else 'inf' for r in results]\n    int_results = [int(r) if r != 'inf' else 'inf' for r in str_results]\n\n    final_output_str = [str(r) for r in int_results]\n\n    print(f\"[{','.join(final_output_str)}]\")\n\nsolve()\n```", "id": "3105953"}, {"introduction": "选定了基学习器后，我们面临下一个问题：如何有效地将它们组合起来？在每一轮迭代中完全“相信”并全量添加新的基学习器，往往会导致模型过快地拟合训练数据中的噪声，从而陷入过拟合的陷阱。本练习将引导你亲手实现梯度提升机，并探索其中最重要的正则化参数——缩减率（shrinkage）$\\nu$ 的作用，通过实验直观感受它如何通过控制学习步长来提升模型的泛化能力 [@problem_id:3125539]。", "problem": "考虑一个监督回归问题，其训练数据为 $\\{(\\mathbf{x}_i, y_i)\\}_{i=1}^n$，其中 $\\mathbf{x}_i \\in \\mathbb{R}^d$ 且 $y_i \\in \\mathbb{R}$。目标是以分阶段的方式，在一类加性函数 $f$ 上最小化经验风险 $R(f)$。此过程从一个基本点出发：使用均方误差 (MSE) 损失的经验风险最小化由以下目标函数决定\n$$\nR(f) = \\frac{1}{n}\\sum_{i=1}^n \\left(y_i - f(\\mathbf{x}_i)\\right)^2.\n$$\n在使用函数空间梯度下降的分阶段加性建模中，在每个阶段 $m$，选择一个弱学习器 $h_m$ 来近似于在当前模型 $f_{m-1}$ 处评估的损失函数的负梯度，并通过以下方式进行更新\n$$\nf_m = f_{m-1} + \\nu \\, h_m,\n$$\n其中 $\\nu \\in (0, 1]$ 是一个分阶段收缩参数，起到隐式正则化的作用。\n\n您必须实现使用平方误差损失和轴对齐回归桩作为弱学习器的梯度提升。一个回归桩 $h(\\mathbf{x})$ 的定义是：选择一个特征索引 $j \\in \\{1, \\dots, d\\}$ 和一个阈值 $t \\in \\mathbb{R}$，当 $x_j  t$ 时预测一个常数 $a$，否则预测一个常数 $b$。对于一个目标向量 $\\mathbf{r} \\in \\mathbb{R}^n$ 的平方误差拟合，对于任何固定的 $(j,t)$，最优常数是阈值两侧目标值的均值。\n\n给定一个固定的训练集，包含 $n = 14$ 个在 $d = 2$ 维空间中的点：\n$$\n\\mathbf{X} = \\big[(-2.0,-1.0),\\, (-2.0,1.0),\\, (-1.0,-1.0),\\, (-1.0,1.0),\\, (0.0,-1.0),\\, (0.0,1.0),\\, (1.0,-1.0),\\, (1.0,1.0),\\, (2.0,-1.0),\\, (2.0,1.0),\\, (-3.0,0.0),\\, (3.0,0.0),\\, (0.5,-0.5),\\, (-0.5,0.5)\\big],\n$$\n以及一个用于目标的确定性规则\n$$\ny = 2 \\cdot \\mathbb{1}\\{x_1  0\\} \\;-\\; \\mathbb{1}\\{x_2  0\\} \\;+\\; 0.3\\,x_1 \\;+\\; 0.1\\,x_2,\n$$\n该规则必须用于从 $\\mathbf{X}$ 中精确地（无随机性）计算 $\\mathbf{y} \\in \\mathbb{R}^{14}$。初始化 $f_0(\\mathbf{x}) \\equiv 0$，并在每个阶段 $m = 1, 2, \\dots, M$（其中 $M$ 为固定值），计算伪残差\n$$\nr_i^{(m)} = y_i - f_{m-1}(\\mathbf{x}_i),\n$$\n通过最小化训练点上的平方误差，为 $\\{(\\mathbf{x}_i, r_i^{(m)})\\}_{i=1}^n$ 拟合一个回归桩 $h_m$，并更新 $f_m = f_{m-1} + \\nu h_m$。\n\n研究作为隐式正则化的分阶段收缩 $\\nu$：将提升阶段的数量固定为 $M = 12$，并对每个指定的 $\\nu$ 值，评估欧几里得预测范数\n$$\n\\|f_M\\|_2 \\;=\\; \\left(\\sum_{i=1}^n f_M(\\mathbf{x}_i)^2\\right)^{1/2}\n$$\n和训练损失\n$$\nR(f_M) \\;=\\; \\frac{1}{n}\\sum_{i=1}^n \\big(y_i - f_M(\\mathbf{x}_i)\\big)^2.\n$$\n\n您的实现必须通过考虑所有特征 $j \\in \\{1,\\dots,d\\}$ 和所有由所选特征在训练样本中连续不同值之间的中点形成的阈值 $t$ 来构建回归桩。对于任何候选的 $(j,t)$，最优左侧常数 $a$ 是索引满足 $x_{ij}  t$ 的目标值的均值，最优右侧常数 $b$ 是索引满足 $x_{ij} \\ge t$ 的目标值的均值。在所有特征的所有候选者中，选择那个能最小化训练数据上残差平方和的回归桩。如果一个特征没有有效的分割（所有值都相等），则跳过它；如果所有特征都没有有效的分割，则使用等于目标值全局均值的常数预测器作为回归桩。\n\n测试套件：\n- 精确使用五个收缩值 $\\nu \\in \\{0.01,\\, 0.1,\\, 0.3,\\, 0.7,\\, 1.0\\}$。\n- 对所有测试，将阶段数固定为 $M = 12$。\n- 对每个 $\\nu$，计算对 $\\left(\\|f_M\\|_2,\\, R(f_M)\\right)$，结果四舍五入到6位小数。\n\n答案规格：\n- 按照给定顺序，为每个 $\\nu$ 输出一个对 $\\left[\\|f_M\\|_2, R(f_M)\\right]$，形式为包含两个浮点数的列表。\n- 您的程序应生成单行输出，包含一个用方括号括起来的逗号分隔列表形式的结果，其中每个对都用自己的方括号括起来，且没有空格。例如，输出应如下所示\n$$\n\\big[[\\|f_M\\|_2^{(1)}, R(f_M)^{(1)}],[\\|f_M\\|_2^{(2)}, R(f_M)^{(2)}],\\dots\\big],\n$$\n具体呈现为单行字符串，如 \"[[n1,l1],[n2,l2],[n3,l3],[n4,l4],[n5,l5]]\"，其中每个 $n_k$ 和 $l_k$ 都是四舍五入到6位小数的浮点数。", "solution": "用户要求实现一个用于特定回归问题的梯度提升算法，该算法利用回归桩作为弱学习器。目标是评估收缩参数 $\\nu$ 对最终模型预测范数和训练损失的影响。此过程始于对问题陈述的形式验证，该陈述被认为是科学上合理、适定且完整的。\n\n### 使用平方误差损失的梯度提升原理\n\n梯度提升是一种集成学习方法，它通过一系列弱学习器以分阶段的方式构建一个强大的预测模型。其核心思想是将优化问题视为函数空间中的一种梯度下降。\n\n目标是找到一个函数 $f(\\mathbf{x})$，以最小化由损失函数 $L(y, f(\\mathbf{x}))$ 定义的经验风险。对于本问题，指定的损失是均方误差 (MSE)，经验风险由下式给出：\n$$\nR(f) = \\frac{1}{n} \\sum_{i=1}^{n} L(y_i, f(\\mathbf{x}_i)) = \\frac{1}{n}\\sum_{i=1}^{n} \\left(y_i - f(\\mathbf{x}_i)\\right)^2\n$$\n其中 $\\{(\\mathbf{x}_i, y_i)\\}_{i=1}^n$ 是训练集，包含 $n=14$ 个样本且 $\\mathbf{x}_i \\in \\mathbb{R}^2$。\n\n算法流程如下：\n\n1.  **初始化**：模型被初始化为一个常数值。本问题指定初始模型为 $f_0(\\mathbf{x}) \\equiv 0$。因此，在训练数据上的预测向量为 $\\mathbf{f}_0 = \\mathbf{0} \\in \\mathbb{R}^n$。\n\n2.  **分阶段加性建模**：对于每个阶段 $m = 1, 2, \\dots, M$，模型被迭代更新。在泛函梯度下降的背景下，我们寻求添加一个函数 $h_m$，使当前模型 $f_{m-1}$ 朝着损失函数负梯度的方向移动。关于函数值 $f(\\mathbf{x}_i)$ 的负梯度分量为：\n    $$\n    r_i^{(m)} = -\\left[\\frac{\\partial L(y_i, F)}{\\partial F}\\right]_{F=f_{m-1}(\\mathbf{x}_i)} = -\\left[\\frac{\\partial}{\\partial F} \\frac{1}{2}(y_i - F)^2\\right]_{F=f_{m-1}(\\mathbf{x}_i)} = y_i - f_{m-1}(\\mathbf{x}_i)\n    $$\n    这些量 $r_i^{(m)}$ 称为第 $m$ 阶段的伪残差。\n\n3.  **弱学习器拟合**：一个弱学习器，在本例中是一个回归桩 $h_m(\\mathbf{x})$，被拟合以预测伪残差。这通过解决以下最小二乘问题完成：\n    $$\n    h_m = \\arg\\min_{h} \\sum_{i=1}^n \\left(r_i^{(m)} - h(\\mathbf{x}_i)\\right)^2\n    $$\n\n4.  **模型更新**：通过添加由收缩参数 $\\nu$ 缩放的新弱学习器来更新完整模型：\n    $$\n    f_m(\\mathbf{x}) = f_{m-1}(\\mathbf{x}) + \\nu h_m(\\mathbf{x})\n    $$\n    收缩参数 $\\nu \\in (0, 1]$ 减少了每个独立弱学习器的影响，这是一种防止过拟合的正则化形式。\n\n此过程重复固定的阶段数，即 $M=12$ 次。\n\n### 拟合回归桩\n\n回归桩是只有一个分裂的决策树。它由一个特征索引 $j \\in \\{1, \\dots, d\\}$、一个阈值 $t \\in \\mathbb{R}$ 和两个常数输出值 $a$ 和 $b$ 定义。其函数形式为：\n$$\nh(\\mathbf{x}; j, t, a, b) = a \\cdot \\mathbb{1}\\{x_j  t\\} + b \\cdot \\mathbb{1}\\{x_j \\ge t\\}\n$$\n其中 $\\mathbb{1}\\{\\cdot\\}$ 是指示函数。\n\n为了为给定的一组伪残差 $\\{r_i^{(m)}\\}_{i=1}^n$ 找到最优的回归桩 $h_m$，执行以下过程：\n1.  对于每个特征 $j \\in \\{1, 2\\}$：\n    a.  识别训练数据 $\\mathbf{X}$ 中存在的特征 $j$ 的唯一值集合 $\\{v_1, v_2, \\dots, v_k\\}$。\n    b.  通过取每对连续唯一值的中点来构建一组候选阈值 $\\{t_s\\}_{s=1}^{k-1}$：$t_s = (v_s + v_{s+1})/2$。\n    c.  对于每个候选 $(j, t_s)$：\n        i.   将样本索引划分为两个集合：$I_{left} = \\{i \\mid x_{ij}  t_s\\}$ 和 $I_{right} = \\{i \\mid x_{ij} \\ge t_s\\}$。\n        ii.  对于此分裂，最小化误差平方和的最优常数 $a$ 和 $b$ 是每个分区内残差的均值：\n             $$\n             a = \\frac{1}{|I_{left}|} \\sum_{i \\in I_{left}} r_i^{(m)}, \\quad b = \\frac{1}{|I_{right}|} \\sum_{i \\in I_{right}} r_i^{(m)}\n             $$\n        iii. 计算此回归桩的误差平方和 (SSE)：\n             $$\n             SSE(j, t_s) = \\sum_{i \\in I_{left}} (r_i^{(m)} - a)^2 + \\sum_{i \\in I_{right}} (r_i^{(m)} - b)^2\n             $$\n2.  最佳回归桩 $(j^*, t^*, a^*, b^*)$ 是在所有可能的特征 $j$ 和阈值 $t_s$ 中产生最小 SSE 的那个。如果任何特征都无法进行有效分割（例如，如果所有样本都具有相同的特征向量），则回归桩默认为一个等于残差全局均值的常数预测器。\n\n### 执行计划\n\n对于每个指定的收缩值 $\\nu \\in \\{0.01, 0.1, 0.3, 0.7, 1.0\\}$，实现遵循以下步骤：\n1.  **数据准备**：使用给定的特征矩阵 $\\mathbf{X}$ 和确定性规则计算目标值 $y_i$：\n    $$\n    y_i = 2 \\cdot \\mathbb{1}\\{x_{i1}  0\\} - \\mathbb{1}\\{x_{i2}  0\\} + 0.3x_{i1} + 0.1x_{i2}\n    $$\n2.  **提升循环**：\n    - 将预测向量 $\\mathbf{f}_0$ 初始化为全零。\n    - 对于 $m = 1, \\dots, 12$：\n        a. 计算伪残差 $\\mathbf{r}^{(m)} = \\mathbf{y} - \\mathbf{f}_{m-1}$。\n        b. 将一个最优回归桩 $h_m$ 拟合到数据对 $(\\mathbf{X}, \\mathbf{r}^{(m)})$。\n        c. 从拟合的回归桩在训练数据 $\\mathbf{X}$ 上获得预测值 $\\mathbf{h}_m$。\n        d. 更新模型预测：$\\mathbf{f}_m = \\mathbf{f}_{m-1} + \\nu \\mathbf{h}_m$。\n3.  **评估**：经过 $M=12$ 个阶段后，最终的预测向量为 $\\mathbf{f}_M$。计算以下两个量：\n    - **欧几里得预测范数**：$\\|f_M\\|_2 = \\sqrt{\\sum_{i=1}^n f_M(\\mathbf{x}_i)^2} = \\|\\mathbf{f}_M\\|_2$。\n    - **训练损失 (MSE)**：$R(f_M) = \\frac{1}{n} \\sum_{i=1}^n (y_i - f_M(\\mathbf{x}_i))^2$。\n4.  **输出格式化**：将得到的对 $(\\|f_M\\|_2, R(f_M))$ 四舍五入到6位小数并存储。最终输出是一个包含每个 $\\nu$ 值对应结果对的列表。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements Gradient Boosting with Regression Stumps for a specified regression task.\n    \"\"\"\n    \n    # --- Problem Givens ---\n    X_data = np.array([\n        [-2.0, -1.0], [-2.0, 1.0], [-1.0, -1.0], [-1.0, 1.0],\n        [0.0, -1.0], [0.0, 1.0], [1.0, -1.0], [1.0, 1.0],\n        [2.0, -1.0], [2.0, 1.0], [-3.0, 0.0], [3.0, 0.0],\n        [0.5, -0.5], [-0.5, 0.5]\n    ])\n    n_samples, n_features = X_data.shape\n    M = 12\n    nu_values = [0.01, 0.1, 0.3, 0.7, 1.0]\n\n    def compute_y(X):\n        \"\"\"Computes the target values based on the given deterministic rule.\"\"\"\n        x1 = X[:, 0]\n        x2 = X[:, 1]\n        y = (2.0 * (x1 > 0) - 1.0 * (x2  0) + 0.3 * x1 + 0.1 * x2)\n        return y\n\n    class RegressionStump:\n        \"\"\"A regression stump weak learner.\"\"\"\n        def __init__(self):\n            self.feature_index = None\n            self.threshold = None\n            self.left_value = None\n            self.right_value = None\n\n        def fit(self, X, y):\n            \"\"\"\n            Finds the best split (feature and threshold) for the stump.\n            The best split is the one that minimizes the sum of squared errors.\n            \"\"\"\n            n_samples, n_features = X.shape\n            best_sse = float('inf')\n            \n            # Default prediction is the global mean if no split is found\n            global_mean = np.mean(y)\n\n            for j in range(n_features):\n                feature_values = X[:, j]\n                unique_values = np.unique(feature_values)\n\n                if len(unique_values) = 1:\n                    continue  # Cannot split on this feature\n\n                thresholds = (unique_values[:-1] + unique_values[1:]) / 2.0\n\n                for t in thresholds:\n                    left_indices = np.where(feature_values  t)[0]\n                    right_indices = np.where(feature_values >= t)[0]\n                    \n                    # This check is technically redundant with the len(unique_values)>1 check\n                    if len(left_indices) == 0 or len(right_indices) == 0:\n                        continue\n\n                    y_left, y_right = y[left_indices], y[right_indices]\n                    \n                    mean_left = np.mean(y_left)\n                    mean_right = np.mean(y_right)\n                    \n                    sse = np.sum((y_left - mean_left)**2) + np.sum((y_right - mean_right)**2)\n\n                    if sse  best_sse:\n                        best_sse = sse\n                        self.feature_index = j\n                        self.threshold = t\n                        self.left_value = mean_left\n                        self.right_value = mean_right\n            \n            # If no split improved upon the initial 'inf' sse, it means no valid split was found.\n            # In this case, use the global mean.\n            if self.feature_index is None:\n                self.left_value = global_mean\n                self.right_value = global_mean\n\n        def predict(self, X):\n            \"\"\"Makes predictions using the fitted stump.\"\"\"\n            if self.feature_index is None:\n                # No split was found, predict the global mean for all samples.\n                return np.full(X.shape[0], self.left_value)\n            \n            feature_values = X[:, self.feature_index]\n            return np.where(feature_values  self.threshold, self.left_value, self.right_value)\n\n    # --- Main Execution Logic ---\n    y_true = compute_y(X_data)\n    results = []\n\n    for nu in nu_values:\n        f_m_preds = np.zeros(n_samples)  # f_0(x) = 0\n\n        for _ in range(M):\n            residuals = y_true - f_m_preds\n            \n            stump = RegressionStump()\n            stump.fit(X_data, residuals)\n            \n            h_m_preds = stump.predict(X_data)\n            \n            f_m_preds += nu * h_m_preds\n\n        # After M stages, f_m_preds holds the final predictions f_M\n        norm_fM = np.linalg.norm(f_m_preds)\n        loss_fM = np.mean((y_true - f_m_preds)**2)\n\n        results.append([round(norm_fM, 6), round(loss_fM, 6)])\n\n    # Format the final output string as specified\n    formatted_pairs = [f\"[{r[0]},{r[1]}]\" for r in results]\n    print(f\"[{','.join(formatted_pairs)}]\")\n\nsolve()\n```", "id": "3125539"}, {"introduction": "理论模型往往假设数据是“干净”的，但现实世界的数据集常常混杂着噪声和异常值。对于这些异常点，标准的平方误差损失（$L_2$ loss）会给予过高的权重，可能导致整个模型“跑偏”。本练习将带你直面这一挑战，通过在一个含有重尾噪声的模拟数据集上，比较使用 $L_2$ 损失和鲁棒的 Huber 损失训练出的梯度提升模型，从而深刻理解为何选择正确的损失函数是提升模型稳健性的关键 [@problem_id:3125607]。", "problem": "要求您实现一个小型的独立实验，比较在观测噪声具有重尾分布时，梯度提升回归在两种不同损失函数下的行为。您将使用经验风险最小化和分步加性建模的框架。实验设置如下。\n\n假设输入为标量 $x \\in \\mathbb{R}$，真实回归函数为 $f^*(x) = 2x$。观测值根据 $y = f^*(x) + \\epsilon$ 生成，其中 $\\epsilon$ 服从自由度为 $\\nu$ 的学生t分布 (Student's $t$ distribution)，并在 $\\nu  2$ 时进行缩放以使方差为单位1。具体来说，如果 $t \\sim t_\\nu$ 是一个标准学生t分布变量，定义 $\\epsilon = t \\cdot \\sqrt{\\frac{\\nu - 2}{\\nu}}$，这样当 $\\nu  2$ 时，$\\mathrm{Var}(\\epsilon) = 1$。\n\n您将构建两个梯度提升回归器，它们仅在损失函数的选择上有所不同：\n- 最小二乘损失 $L_2$：$\\ell(y, F) = \\frac{1}{2}(y - F)^2$。\n- 带阈值 $\\delta  0$ 的Huber损失：\n  $$\\ell(y,F) = \\begin{cases}\n  \\frac{1}{2}(y - F)^2,  \\text{if } |y - F| \\le \\delta,\\\\\n  \\delta |y - F| - \\frac{1}{2}\\delta^2,  \\text{otherwise.}\n  \\end{cases}$$\n\n两个模型都必须实现为分步加性模型 $F_m(x) = F_{m-1}(x) + \\eta \\, h_m(x)$，其中 $m = 1,2,\\dots,M$，$\\eta \\in (0,1]$ 是给定的学习率，$h_m$ 是一个决策树桩 (decision stump) 弱学习器，它将 $x$ 映射到两个区域 $(-\\infty, s]$ 和 $(s, \\infty)$，并根据当前伪残差上的最小二乘法确定区域特定的常数预测值。初始预测值 $F_0$ 必须在常数范围内最小化所选损失的经验风险：\n- 对于 $L_2$ 损失，这是 $y$ 的经验均值。\n- 对于带阈值 $\\delta$ 的Huber损失，最优常数 $c$ 满足 $\\sum_{i=1}^n \\psi(y_i - c) = 0$，其中如果 $|r| \\le \\delta$，则 $\\psi(r) = r$，否则 $\\psi(r) = \\delta \\,\\mathrm{sign}(r)$。您必须通过求解一维凸函数求根问题 $\\sum_{i=1}^n \\psi(y_i - c) = 0$ 来计算这个 $c$，使用一种能保证找到解的区间法 (bracketing method)。\n\n在每次提升迭代 $m$ 中，您必须：\n- 计算每个样本关于预测的梯度 $g_i^{(m)} = \\frac{\\partial \\ell(y_i, F_{m-1}(x_i))}{\\partial F}$，在当前模型 $F_{m-1}$ 处进行评估。对于 $L_2$ 损失，这等于 $F_{m-1}(x_i) - y_i$。对于Huber损失，它等于 $-\\psi(y_i - F_{m-1}(x_i))$，使用与上面相同的 $\\psi$。\n- 将伪残差定义为负梯度 $r_i^{(m)} = - g_i^{(m)}$。\n- 通过最小化所有单阈值分裂的经验平方误差，将决策树桩 $h_m$ 拟合到目标 $\\{r_i^{(m)}\\}_{i=1}^n$。对于固定的分裂阈值 $s$，最优的区域常数是分裂两侧目标的经验均值。您必须选择能在两个区域上产生最小平方误差总和的分裂。\n\n您必须为每种损失评估以下两种统计量：\n- $M$ 次迭代后的最终中位数绝对误差：$\\mathrm{MedAE} = \\mathrm{median}_i\\left(|y_i - F_M(x_i)|\\right)$。\n- 梯度幅值中位数在所有提升迭代中的平均值，定义为\n  $$\\overline{G} = \\frac{1}{M} \\sum_{m=1}^M \\mathrm{median}_i\\left(|g_i^{(m)}|\\right),$$\n  其中 $g_i^{(m)}$ 如上定义，对于每个 $m$，中位数是在样本索引 $i \\in \\{1,\\dots,n\\}$ 上计算的。\n\n实现上述过程并在以下测试套件上运行。对于每个测试用例，从 $[-1, 1]$ 上的均匀分布中独立同分布地生成 $n$ 个输入 $x_i$，并使用指定的自由度 $\\nu$ 按前述方式生成输出。为了可复现性，请使用指定的随机种子。使用相同的数据集来训练两种损失的模型，并报告在该数据集上的指标。两种损失使用相同的弱学习器类别和学习率。\n\n测试套件参数集，每个集为 $(\\nu, \\delta, M, \\eta, n, \\text{seed})$：\n- 案例 1：$(\\nu=\\;3.0,\\; \\delta=\\;1.0,\\; M=\\;30,\\; \\eta=\\;0.1,\\; n=\\;256,\\; \\text{seed}=\\;12345)$。\n- 案例 2：$(\\nu=\\;10.0,\\; \\delta=\\;1.0,\\; M=\\;30,\\; \\eta=\\;0.1,\\; n=\\;256,\\; \\text{seed}=\\;12346)$。\n- 案例 3：$(\\nu=\\;2.5,\\; \\delta=\\;0.5,\\; M=\\;30,\\; \\eta=\\;0.1,\\; n=\\;256,\\; \\text{seed}=\\;12347)$。\n\n您的程序必须产生单行输出，包含一个列表的列表，每个子列表对应一个测试用例，且每个子列表等于 \n$[\\mathrm{MedAE}_{L_2}, \\mathrm{MedAE}_{\\text{Huber}}, \\overline{G}_{L_2}, \\overline{G}_{\\text{Huber}}]$，四舍五入到六位小数。确切的输出格式必须是单行字符串，形式为 \n$[[a_{11},a_{12},a_{13},a_{14}],[a_{21},a_{22},a_{23},a_{24}],[a_{31},a_{32},a_{33},a_{34}]]$ \n，其中不含任何空格。\n\n您必须使用的基本原理：\n- 函数 $F$ 的经验风险最小化：最小化 $\\frac{1}{n} \\sum_{i=1}^n \\ell(y_i, F(x_i))$。\n- 分步加性建模：$F_m = F_{m-1} + \\eta h_m$，其中选择 $h_m$ 以降低经验风险。\n- 负梯度拟合：在迭代 $m$ 时，将 $h_m$ 拟合到目标 $r_i^{(m)} = -\\left.\\frac{\\partial \\ell(y_i, F(x_i))}{\\partial F}\\right|_{F=F_{m-1}}$。\n- 区域上的最小二乘最优常数：该区域上目标的经验均值可以最小化平方误差之和。\n\n不应假设任何其他公式。您必须在实现中从这些原理推导出所有其他步骤。本问题中没有物理单位。不使用角度。不使用百分比。\n\n您的程序应生成单行输出，包含一个用方括号括起来的、由逗号分隔的列表的列表，例如 $[[r_{11},r_{12},r_{13},r_{14}],[r_{21},r_{22},r_{23},r_{24}],[r_{31},r_{32},r_{33},r_{34}]]$.", "solution": "该问题要求实现并比较两种梯度提升回归模型，一个使用标准的最小二乘 ($L_2$) 损失，另一个使用更稳健的Huber损失。该比较在一个观测噪声呈现重尾特性的环境中进行，这种环境通过学生t分布 (Student's t-distribution) 进行模拟。实现基于经验风险最小化和分步加性建模的原理。\n\n模型的一般结构是一个分步加性函数，形式如下：\n$$F_m(x) = F_{m-1}(x) + \\eta \\, h_m(x)$$\n其中 $m=1, 2, \\dots, M$ 是提升迭代次数，$\\eta \\in (0, 1]$ 是学习率，$h_m(x)$ 是一个称为弱学习器的简单函数。整体模型 $F_M(x)$ 是对真实函数 $f^*(x)$ 的一个近似。该过程从一个初始模型 $F_0(x)$ 开始，它是一个常数值，被选择用来最小化所选损失函数的经验风险。\n\n在每次迭代 $m$ 中，梯度提升算法通过添加一个指向损失曲面上最速下降方向的弱学习器来寻求改进模型。这个方向由损失函数关于当前模型预测值的负梯度给出。每个样本的负梯度被称为伪残差：\n$$r_i^{(m)} = -\\left[\\frac{\\partial \\ell(y_i, F(x_i))}{\\partial F}\\right]_{F=F_{m-1}}$$\n然后，弱学习器 $h_m(x)$ 被用来拟合这些伪残差，通常是通过最小化平方误差来实现。\n\n指定的弱学习器是决策树桩 (decision stump)，它在分裂点 $s$ 处将输入空间 $\\mathbb{R}$ 划分为两个区域，$(-\\infty, s]$ 和 $(s, \\infty)$。树桩在每个区域产生一个恒定的预测值。为了将树桩 $h_m$ 拟合到伪残差 $\\{r_i^{(m)}\\}_{i=1}^n$，我们寻找分裂点 $s$ 和两个常数预测值 $c_1$ 和 $c_2$，以最小化平方误差之和：\n$$\\sum_{i: x_i \\le s} (r_i^{(m)} - c_1)^2 + \\sum_{i: x_i  s} (r_i^{(m)} - c_2)^2$$\n对于任何给定的分裂 $s$，最优常数是各自区域中伪残差的均值。通过迭代所有可能的分裂点并选择导致最小平方误差总和的那个，可以找到最佳分裂点 $s$。一个高效的算法可以在 $O(n \\log n)$ 时间内找到最优分裂，方法是首先按特征 $x$ 对数据进行排序，然后使用累加和来计算每个潜在分裂的平方误差之和。\n\n每种损失函数的具体细节如下：\n\n**1. 最小二乘 ($L_2$) 损失**\n损失函数为 $\\ell(y, F) = \\frac{1}{2}(y - F)^2$。\n- **初始预测 $F_0$**：为了最小化经验风险 $\\frac{1}{n} \\sum_{i=1}^n \\frac{1}{2}(y_i - c)^2$，我们将关于 $c$ 的导数设为零：$\\sum_i -(y_i - c) = 0$。这得到了最优常数 $c = \\frac{1}{n}\\sum_{i=1}^n y_i$，即目标的样本均值。因此，$F_0(x) = \\bar{y}$。\n- **伪残差**：关于预测 $F$ 在 $F_{m-1}(x_i)$ 处的梯度是 $\\frac{\\partial \\ell}{\\partial F} = F - y$。因此，伪残差为：\n$$r_i^{(m)} = -(F_{m-1}(x_i) - y_i) = y_i - F_{m-1}(x_i)$$\n这些就是模型在第 $m-1$ 次迭代时的普通残差。\n\n**2. Huber损失**\n带阈值 $\\delta  0$ 的Huber损失函数定义为：\n$$\\ell(y,F) = \\begin{cases}\n  \\frac{1}{2}(y - F)^2,  \\text{if } |y - F| \\le \\delta \\\\\n  \\delta |y - F| - \\frac{1}{2}\\delta^2,  \\text{otherwise.}\n  \\end{cases}$$\n对于小误差，该损失函数呈二次方特性；对于大误差，则呈线性特性，这使其对异常值的敏感度低于 $L_2$ 损失。\n- **初始预测 $F_0$**：最优常数 $c$ 最小化经验风险 $\\frac{1}{n} \\sum_{i=1}^n \\ell(y_i, c)$。最优性条件是总损失关于 $c$ 的导数为零。这导出了方程 $\\sum_{i=1}^n \\psi(y_i - c) = 0$，其中 $\\psi$ 是Huber损失对其参数 $r=y-F$ 的导数：\n$$\\psi(r) = \\frac{\\partial \\ell}{\\partial r} = \\begin{cases}\n  r,  \\text{if } |r| \\le \\delta \\\\\n  \\delta \\cdot \\mathrm{sign}(r),  \\text{otherwise.}\n  \\end{cases}$$\n函数 $g(c) = \\sum_{i=1}^n \\psi(y_i - c)$ 是单调且连续的，因此可以使用二分法在合适的区间（例如 $[\\min(y), \\max(y)]$）上可靠地找到其根。\n- **伪残差**：关于预测 $F$ 在 $F_{m-1}(x_i)$ 处的梯度是 $\\frac{\\partial \\ell}{\\partial F} = -\\psi(y - F)$。因此，伪残差为：\n$$r_i^{(m)} = -(-\\psi(y_i - F_{m-1}(x_i))) = \\psi(y_i - F_{m-1}(x_i))$$\n这意味着残差 $y_i - F_{m-1}(x_i)$ 会通过裁剪函数 $\\psi$ 来为弱学习器生成目标。对大残差的这种抑制是Huber方法稳健性的核心。\n\n对于指定的每个测试用例，执行以下步骤：\n1.  生成一个包含 $n$ 个点 $(x_i, y_i)$ 的数据集。输入 $x_i$ 从 Uniform$(-1, 1)$ 分布中抽取。输出按 $y_i = 2x_i + \\epsilon_i$ 生成，其中噪声 $\\epsilon_i$ 从自由度为 $\\nu$ 的学生t分布中采样，并按 $\\sqrt{(\\nu-2)/\\nu}$ 进行缩放以确保在 $\\nu  2$ 时方差为1。\n2.  在该数据集上，使用学习率 $\\eta$ 训练两个梯度提升模型，迭代 $M$ 次：一个使用 $L_2$ 损失，另一个使用Huber损失。\n3.  为每个模型计算两个指标：\n    - 最终中位数绝对误差：$\\mathrm{MedAE} = \\mathrm{median}_i(|y_i - F_M(x_i)|)$。\n    - 平均中位数梯度幅值：$\\overline{G} = \\frac{1}{M} \\sum_{m=1}^M \\mathrm{median}_i(|g_i^{(m)}|)$，其中 $g_i^{(m)}$ 是在第 $m$ 次迭代时每个样本的梯度。\n\n该实现将这些原理封装到一个主函数中，该主函数会遍历所有测试用例，还有一些函数用于针对给定损失运行提升算法，以及一些辅助工具用于拟合决策树桩和为Huber损失寻找初始常数。", "answer": "```python\nimport numpy as np\nfrom scipy.stats import t as t_dist\nfrom scipy.optimize import bisect\n\nclass DecisionStump:\n    \"\"\"A decision stump for 1D regression.\"\"\"\n    def __init__(self, split_val, left_pred, right_pred):\n        self.split_val = split_val\n        self.left_pred = left_pred\n        self.right_pred = right_pred\n\n    def predict(self, x):\n        \"\"\"Predicts the output for a given input array x.\"\"\"\n        if self.split_val is None:\n            # If no split, predict the same value for all inputs.\n            return np.full_like(x, self.left_pred, dtype=float)\n        \n        predictions = np.full_like(x, self.right_pred, dtype=float)\n        predictions[x = self.split_val] = self.left_pred\n        return predictions\n\ndef fit_stump(x, r):\n    \"\"\"\n    Fits a decision stump to predict residuals r from feature x.\n    The stump is chosen to minimize the sum of squared errors.\n    \"\"\"\n    n = len(x)\n    if n == 0:\n        return DecisionStump(None, 0.0, 0.0)\n\n    sort_idx = np.argsort(x)\n    x_sorted, r_sorted = x[sort_idx], r[sort_idx]\n\n    best_sse = np.inf\n    best_split_val = None\n    best_left_pred = None\n    best_right_pred = None\n\n    r_sum_total = np.sum(r_sorted)\n    r2_sum_total = np.sum(r_sorted**2)\n\n    left_sum_r = 0.0\n    left_sum_r2 = 0.0\n\n    for i in range(n - 1):\n        left_sum_r += r_sorted[i]\n        left_sum_r2 += r_sorted[i]**2\n        n_left = i + 1\n        \n        # Avoid splitting between identical x values\n        if x_sorted[i] == x_sorted[i+1]:\n            continue\n\n        n_right = n - n_left\n        right_sum_r = r_sum_total - left_sum_r\n        right_sum_r2 = r2_sum_total - left_sum_r2\n\n        sse_left = left_sum_r2 - (left_sum_r**2) / n_left\n        sse_right = right_sum_r2 - (right_sum_r**2) / n_right\n        total_sse = sse_left + sse_right\n\n        if total_sse  best_sse:\n            best_sse = total_sse\n            best_split_val = (x_sorted[i] + x_sorted[i+1]) / 2.0\n            best_left_pred = left_sum_r / n_left\n            best_right_pred = right_sum_r / n_right\n    \n    if best_split_val is None:\n        # All x values are the same, no split is possible.\n        pred = r.mean()\n        return DecisionStump(None, pred, pred)\n    \n    return DecisionStump(best_split_val, best_left_pred, best_right_pred)\n\ndef find_huber_initial_constant(y, delta):\n    \"\"\"Finds the optimal initial constant for Huber loss via root-finding.\"\"\"\n    def huber_obj_grad(c):\n        residuals = y - c\n        return np.sum(np.clip(residuals, -delta, delta))\n\n    # The optimal constant is guaranteed to be within the range of y\n    y_min, y_max = np.min(y), np.max(y)\n    if y_min == y_max:\n        return y_min\n        \n    return bisect(huber_obj_grad, y_min, y_max)\n\ndef run_gbm(x, y, M, eta, loss_type, delta):\n    \"\"\"Runs a gradient boosting machine for regression.\"\"\"\n    n = len(y)\n    \n    # Initialize model F_0\n    if loss_type == 'l2':\n        F = np.full(n, np.mean(y))\n    elif loss_type == 'huber':\n        f0 = find_huber_initial_constant(y, delta)\n        F = np.full(n, f0)\n    else:\n        raise ValueError(\"Unknown loss type\")\n        \n    all_median_gradients = []\n\n    for _ in range(M):\n        # Compute pseudo-residuals (negative gradients)\n        if loss_type == 'l2':\n            gradients = F - y\n            residuals = -gradients\n        elif loss_type == 'huber':\n            residuals_unclipped = y - F\n            residuals = np.clip(residuals_unclipped, -delta, delta)\n            gradients = -residuals\n        \n        all_median_gradients.append(np.median(np.abs(gradients)))\n\n        # Fit a weak learner (decision stump) to the pseudo-residuals\n        stump = fit_stump(x, residuals)\n        \n        # Update the model\n        F += eta * stump.predict(x)\n        \n    # Calculate final metrics\n    med_ae = np.median(np.abs(y - F))\n    avg_med_grad = np.mean(all_median_gradients)\n    \n    return med_ae, avg_med_grad\n\n\ndef solve():\n    \"\"\"Main function to run the specified test suite and print results.\"\"\"\n    test_cases = [\n        # (nu,      delta, M,  eta,   n,   seed)\n        (3.0,     1.0,   30, 0.1, 256, 12345),\n        (10.0,    1.0,   30, 0.1, 256, 12346),\n        (2.5,     0.5,   30, 0.1, 256, 12347)\n    ]\n    \n    all_case_results = []\n    \n    for nu, delta, M, eta, n, seed in test_cases:\n        rng = np.random.default_rng(seed)\n        \n        # Generate data\n        x = rng.uniform(low=-1.0, high=1.0, size=n)\n        \n        # Generate noise from scaled Student's t-distribution\n        t_samples = t_dist.rvs(df=nu, size=n, random_state=rng)\n        # Scale to unit variance (valid since nu > 2 for all cases)\n        noise = t_samples * np.sqrt((nu - 2.0) / nu)\n        \n        # Generate observations\n        y = 2.0 * x + noise\n\n        # Run L2 model\n        l2_medae, l2_avg_g = run_gbm(x, y, M, eta, 'l2', delta=None)\n        \n        # Run Huber model\n        huber_medae, huber_avg_g = run_gbm(x, y, M, eta, 'huber', delta=delta)\n        \n        all_case_results.append([l2_medae, huber_medae, l2_avg_g, huber_avg_g])\n\n    # Format the output string\n    output_parts = []\n    for case_res in all_case_results:\n        formatted_res = ','.join([f\"{val:.6f}\" for val in case_res])\n        output_parts.append(f\"[{formatted_res}]\")\n    \n    final_output = f\"[{','.join(output_parts)}]\"\n    print(final_output)\n\nsolve()\n```", "id": "3125607"}]}