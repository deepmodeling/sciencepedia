{"hands_on_practices": [{"introduction": "决策树的构建过程是在每个节点上递归地寻找“最佳”划分。但我们如何定义“最佳”？这个练习将带你深入探讨两种最核心的划分标准：基尼不纯度（Gini impurity）和信息熵（Shannon entropy）。通过从第一性原理出发推导杂质减少量，你将能定量地比较这两种标准在处理类别不平衡数据时的敏感性差异，从而理解它们在实践中可能导致的不同行为。[@problem_id:3166111]", "problem": "给定一个类别为 $\\{0,1\\}$ 的二元分类问题。设类先验概率为 $P(Y=1)=\\pi$ 和 $P(Y=0)=1-\\pi$，其中 $\\pi \\in [0,1]$。考虑一个基于二元特征 $X \\in \\{0,1\\}$ 的候选二元分裂，其类条件概率为 $P(X=1 \\mid Y=1)=a$ 和 $P(X=1 \\mid Y=0)=b$，其中 $a \\in [0,1]$ 且 $b \\in [0,1]$。该分裂将父节点划分为两个子节点：$X=1$ 为左子节点，$X=0$ 为右子节点。\n\n作为基本依据，请使用以下定义：\n- 对于一个正类别概率为 $p$ 的节点，其基尼不纯度 (Gini impurity) 为 $G(p)=2p(1-p)$。\n- 对于一个正类别概率为 $p$ 的节点，其香农熵 (Shannon entropy)（以比特为单位）为 $H(p)=-p\\log_2 p -(1-p)\\log_2(1-p)$，并约定 $0\\log_2 0$ 取值为 $0$。\n- 对于任意不纯度函数 $I(\\cdot)$，将一个正类别概率为 $p$ 的父节点分裂为两个子节点时，其不纯度减少量（对于熵而言也称为信息增益）为 $I(p)-w_L I(p_L)-w_R I(p_R)$，其中 $w_L$ 和 $w_R$ 分别是进入左、右子节点的样本比例，而 $p_L$ 和 $p_R$ 分别是左、右子节点内部的正类别概率。\n\n根据这些定义，推导 $w_L$、$w_R$、$p_L$ 和 $p_R$ 关于 $\\pi$、$a$ 和 $b$ 的表达式，然后计算基尼不纯度和香农熵的不纯度减少量。对于固定的 $(a,b)$ 对和给定的 $\\pi$，将基尼不纯度和香农熵的不纯度减少量分别记为 $R_G(\\pi;a,b)$ 和 $R_H(\\pi;a,b)$。\n\n您的任务是编写一个完整、可运行的程序，该程序：\n- 使用端点值 $\\pi=0.01$ 和 $\\pi=0.50$ 来评估分裂质量对稀有类别的敏感性，具体方法是计算敏感性比率\n  $$S_G(a,b)=\\frac{R_G(0.01;a,b)}{R_G(0.50;a,b)} \\quad \\text{和} \\quad S_H(a,b)=\\frac{R_H(0.01;a,b)}{R_H(0.50;a,b)}.$$\n- 如果分母 $R_G(0.50;a,b)$ 为 $0$，则定义 $S_G(a,b)=0.0$。如果分母 $R_H(0.50;a,b)$ 为 $0$，则定义 $S_H(a,b)=0.0$。此约定确保在退化情况下输出有明确定义。\n\n仅使用所提供的定义来实现以上要求，不要引用任何其他预先推导的结果。\n\n测试套件：\n- 使用以下五个 $(a,b)$ 对作为不同的测试用例，以探究不同的信号机制和边界行为：\n  - 用例一（强分裂）：$(a,b)=(0.90,0.10)$。\n  - 用例二（弱分裂）：$(a,b)=(0.60,0.40)$。\n  - 用例三（无信号边界）：$(a,b)=(0.50,0.50)$。\n  - 用例四（高真阳性率和中等假阳性率）：$(a,b)=(0.99,0.49)$。\n  - 用例五（负类别中的罕见触发）：$(a,b)=(0.30,0.01)$。\n\n要求的最终输出格式：\n- 您的程序必须输出单行，其中包含一个扁平化的浮点数列表，共 $10$ 个数字，顺序如下\n  $$[S_G(a_1,b_1), S_H(a_1,b_1), S_G(a_2,b_2), S_H(a_2,b_2), \\dots, S_G(a_5,b_5), S_H(a_5,b_5)],$$\n  其中 $(a_1,b_1),\\dots,(a_5,b_5)$ 遵循上述测试套件的顺序。\n- 每个数字必须以十进制形式打印。不涉及物理单位或角度。\n- 列表必须以逗号分隔，并用方括号括起来，不得包含额外的空白或文本。\n\n您的程序必须是自包含的，不需要任何输入，并且确定性地执行。", "solution": "**中间量的推导：**\n\n该问题在标准贝叶斯概率框架内定义。设 $\\pi = P(Y=1)$ 为正类别的先验概率。特征 $X$ 是二元的，其类条件概率给定为 $P(X=1 \\mid Y=1) = a$ 和 $P(X=1 \\mid Y=0) = b$。分裂根据 $X$ 的值对数据进行划分，$X=1$ 进入左子节点，$X=0$ 进入右子节点。\n\n1.  **父节点的正类别概率 $p$**：\n    父节点包含分裂前的所有样本。因此，父节点中正类别的概率就是正类别的整体先验概率。\n    $$p = P(Y=1) = \\pi$$\n\n2.  **子节点比例 $w_L$ 和 $w_R$**：\n    这些是观测到 $X=1$（左子节点）和 $X=0$（右子节点）的边际概率。我们使用全概率公式来找到它们。\n    -   $w_L$ 是进入左子节点（$X=1$）的样本比例：\n        $$w_L = P(X=1) = P(X=1 \\mid Y=1)P(Y=1) + P(X=1 \\mid Y=0)P(Y=0)$$\n        代入给定值：\n        $$w_L = a\\pi + b(1-\\pi)$$\n    -   $w_R$ 是进入右子节点（$X=0$）的样本比例：\n        $$w_R = P(X=0) = 1 - P(X=1) = 1 - w_L$$\n        或者，使用 $X=0$ 的类条件概率，即 $P(X=0 \\mid Y=1) = 1-a$ 和 $P(X=0 \\mid Y=0) = 1-b$：\n        $$w_R = P(X=0 \\mid Y=1)P(Y=1) + P(X=0 \\mid Y=0)P(Y=0) = (1-a)\\pi + (1-b)(1-\\pi)$$\n        很容易验证 $w_L + w_R = (a\\pi + b(1-\\pi)) + ((1-a)\\pi + (1-b)(1-\\pi)) = \\pi(a+1-a) + (1-\\pi)(b+1-b) = \\pi + (1-\\pi) = 1$。\n\n3.  **子节点的正类别概率 $p_L$ 和 $p_R$**：\n    这些是后验概率，使用贝叶斯定理计算。\n    -   $p_L$ 是在 $X=1$ 的条件下，左子节点中正类别的概率。\n        $$p_L = P(Y=1 \\mid X=1) = \\frac{P(X=1 \\mid Y=1)P(Y=1)}{P(X=1)}$$\n        代入先前推导出的表达式：\n        $$p_L = \\frac{a\\pi}{w_L} = \\frac{a\\pi}{a\\pi + b(1-\\pi)}$$\n        这仅在 $w_L > 0$ 时有定义。如果 $w_L=0$，则左子节点为空，其对不纯度减少量的贡献为零。\n    -   $p_R$ 是在 $X=0$ 的条件下，右子节点中正类别的概率。\n        $$p_R = P(Y=1 \\mid X=0) = \\frac{P(X=0 \\mid Y=1)P(Y=1)}{P(X=0)}$$\n        代入先前推导出的表达式：\n        $$p_R = \\frac{(1-a)\\pi}{w_R} = \\frac{(1-a)\\pi}{(1-a)\\pi + (1-b)(1-\\pi)}$$\n        这仅在 $w_R > 0$ 时有定义。如果 $w_R=0$，则右子节点为空。\n\n**不纯度减少量的计算：**\n\n有了这些量，我们可以使用提供的公式计算任何不纯度函数 $I(\\cdot)$ 的不纯度减少量：\n$$R_I(\\pi;a,b) = I(p) - [w_L I(p_L) + w_R I(p_R)]$$\n其中 $p=\\pi$。\n\n-   **对于基尼不纯度**：\n    基尼不纯度为 $G(p) = 2p(1-p)$。减少量为：\n    $$R_G(\\pi;a,b) = G(\\pi) - [w_L G(p_L) + w_R G(p_R)]$$\n    $$R_G(\\pi;a,b) = 2\\pi(1-\\pi) - \\left[ w_L \\cdot 2p_L(1-p_L) + w_R \\cdot 2p_R(1-p_R) \\right]$$\n\n-   **对于香农熵**：\n    香non熵为 $H(p) = -p\\log_2 p - (1-p)\\log_2(1-p)$。减少量，也称为信息增益，是：\n    $$R_H(\\pi;a,b) = H(\\pi) - [w_L H(p_L) + w_R H(p_R)]$$\n    $$R_H(\\pi;a,b) = \\left( -\\pi\\log_2 \\pi - (1-\\pi)\\log_2(1-\\pi) \\right) - \\left[ w_L H(p_L) + w_R H(p_R) \\right]$$\n\n这些表达式在提供的程序中直接实现。程序为测试套件中的每个 $(a, b)$ 对计算 $\\pi=0.01$ 和 $\\pi=0.50$ 时的这些减少量值，然后计算所需的敏感性比率 $S_G$ 和 $S_H$。分母为零的特殊情况（当 $a=b$ 时发生，使得分裂不提供信息）按规定通过将比率设置为 $0.0$ 来处理。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Calculates sensitivity ratios for Gini and Entropy impurity reductions based on a binary split.\n    \"\"\"\n\n    def shannon_entropy(p):\n        \"\"\"\n        Calculates Shannon entropy H(p) = -p*log2(p) - (1-p)*log2(1-p).\n        Handles p=0 and p=1 cases where H(p)=0.\n        \"\"\"\n        if p == 0 or p >= 1:\n            return 0.0\n        return -p * np.log2(p) - (1 - p) * np.log2(1 - p)\n\n    def gini_impurity(p):\n        \"\"\"\n        Calculates Gini impurity G(p) = 2*p*(1-p).\n        \"\"\"\n        return 2 * p * (1 - p)\n\n    def calculate_reductions(pi, a, b):\n        \"\"\"\n        Calculates the impurity reduction for Gini and Shannon entropy.\n\n        Args:\n            pi (float): The prior probability of class 1, P(Y=1).\n            a (float): The class-conditional probability P(X=1 | Y=1).\n            b (float): The class-conditional probability P(X=1 | Y=0).\n\n        Returns:\n            tuple: A tuple containing (Gini reduction, Shannon entropy reduction).\n        \"\"\"\n        # If a=b, feature X is independent of class Y, so impurity reduction is 0.\n        if np.isclose(a, b):\n            return 0.0, 0.0\n        \n        # Parent node positive class probability is pi.\n        p_parent = pi\n\n        # Calculate proportions of samples in left (X=1) and right (X=0) children.\n        w_L = a * pi + b * (1 - pi)\n        w_R = 1.0 - w_L\n\n        # Calculate parent node impurity.\n        parent_gini = gini_impurity(p_parent)\n        parent_entropy = shannon_entropy(p_parent)\n\n        # Calculate weighted average of child node impurities.\n        gini_children = 0.0\n        entropy_children = 0.0\n\n        # Contribution from left child (X=1).\n        if w_L > 0:\n            p_L = (a * pi) / w_L\n            gini_children += w_L * gini_impurity(p_L)\n            entropy_children += w_L * shannon_entropy(p_L)\n\n        # Contribution from right child (X=0).\n        if w_R > 0:\n            p_R = ((1 - a) * pi) / w_R\n            gini_children += w_R * gini_impurity(p_R)\n            entropy_children += w_R * shannon_entropy(p_R)\n        \n        # Impurity reduction is parent impurity minus weighted child impurity.\n        reduction_gini = parent_gini - gini_children\n        reduction_entropy = parent_entropy - entropy_children\n        \n        return reduction_gini, reduction_entropy\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (0.90, 0.10), # Case one (strong split)\n        (0.60, 0.40), # Case two (weak split)\n        (0.50, 0.50), # Case three (no signal boundary)\n        (0.99, 0.49), # Case four (high true positive and moderate false positive)\n        (0.30, 0.01), # Case five (rare trigger among negatives)\n    ]\n\n    pi_rare = 0.01\n    pi_balanced = 0.50\n\n    results = []\n    for a, b in test_cases:\n        # Calculate reductions for the rare class (imbalanced) scenario.\n        R_G_rare, R_H_rare = calculate_reductions(pi_rare, a, b)\n        \n        # Calculate reductions for the balanced class scenario.\n        R_G_balanced, R_H_balanced = calculate_reductions(pi_balanced, a, b)\n\n        # Calculate sensitivity ratios S_G and S_H.\n        # Handle the case where the denominator is zero.\n        S_G = 0.0 if np.isclose(R_G_balanced, 0) else R_G_rare / R_G_balanced\n        S_H = 0.0 if np.isclose(R_H_balanced, 0) else R_H_rare / R_H_balanced\n        \n        results.extend([S_G, S_H])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3166111"}, {"introduction": "理解了单个划分的机制后，下一步自然是构建完整的随机森林。这个练习要求你从零开始实现随机森林算法，包括其关键组成部分：自助采样（bootstrapping）、节点处的特征子集随机选择，以及基于基尼不纯度减少的特征重要性计算。通过在包含恒定特征和完全重复特征等边缘情况的数据集上进行测试，你将亲身体验该算法的稳健性，并揭示特征重要性度量在这些特殊情况下的行为。[@problem_id:3166180]", "problem": "我们要求您从零开始实现一个用于二分类的随机森林，该随机森林基于分类与回归树（CART），使用基尼不纯度作为分裂准则，并计算定义为平均不纯度下降的归一化特征重要性分数。您的实现必须处理某些特征为常数（零方差）的边缘情况。然后，您将在三个指定的测试用例上运行该实现，并报告一个紧凑的数值摘要。\n\n定义和要求：\n- 决策树是递归生成的。在每个具有数据索引 $\\mathcal{I}$ 的内部节点，您必须：\n  1) 在每个节点，独立地从 $d$ 个可用特征中随机选择一个大小为 $m$ 的特征子集（无放回）。\n  2) 对于每个选定的特征 $j$，考虑所有候选阈值，这些阈值由 $\\mathcal{I}$ 中样本在该特征上的排序后唯一值之间的中点构成。如果一个特征只有一个唯一值，则该节点无法基于此特征进行分裂。\n  3) 对于一个候选阈值 $\\tau$，将 $x_{ij} \\le \\tau$ 的索引集定义为左子节点，将 $x_{ij} > \\tau$ 的索引集定义为右子节点。计算此次分裂的加权基尼不纯度，并选择能最大化不纯度下降的特征和阈值。如果最佳的不纯度下降为非正数，则将该节点设为叶节点。\n- 对于任何索引集为 $\\mathcal{I}$ 且类别标签为 $y_i \\in \\{0,1\\}$ 的节点，其基尼不纯度为 $G(\\mathcal{I}) = 1 - \\sum_{k \\in \\{0,1\\}} p_k^2$，其中 $p_k$ 是 $\\mathcal{I}$ 中各类的比例。\n- 对于使用特征 $j$ 和阈值 $\\tau$ 将 $\\mathcal{I}$ 分裂为左子集 $\\mathcal{L}$ 和右子集 $\\mathcal{R}$ 的情况，该分裂贡献的不纯度下降为\n$$\\Delta(\\mathcal{I}, j, \\tau) = G(\\mathcal{I}) - \\frac{|\\mathcal{L}|}{|\\mathcal{I}|} G(\\mathcal{L}) - \\frac{|\\mathcal{R}|}{|\\mathcal{I}|} G(\\mathcal{R}).$$\n- 包含 $T$ 棵树的随机森林是通过在原始训练集上进行大小为 $N$ 的自助采样（有放回抽样）来训练每棵树，并应用大小为 $m$ 的节点级随机特征子空间选择来构建的。\n- 对于单棵树中的特征 $j$，其重要性是在所有选择特征 $j$ 作为最佳分裂特征的内部节点上，将 $\\Delta(\\mathcal{I}, j, \\tau)$ 按该节点的样本数 $|\\mathcal{I}|$ 进行加权后的总和。将所有树的此总和进行累加。设 $S$ 为所有特征的这些累加的、经样本加权的下降量之总和。如果 $S > 0$，则将特征 $j$ 的归一化重要性定义为 $I_j = \\frac{\\text{aggregated decrease for } j}{S}$，使得 $\\sum_{j=1}^{d} I_j = 1$。如果 $S = 0$，则对所有 $j$ 定义 $I_j = 0$。\n- 除了当没有有效分裂能产生正的不纯度下降时的隐式停止条件外，对树的深度没有其他限制。\n\n实现约束：\n- 您必须完全按照上述规定，使用纯数值运算来实现程序。不允许使用外部机器学习库。\n- 所有随机性必须使用指定的种子以保证结果可复现。\n\n数据集和测试套件：\n您将在三种情况下运行您的实现。在每种情况下，按照规定构建特征矩阵 $X \\in \\mathbb{R}^{N \\times d}$ 和标签 $y \\in \\{0,1\\}^N$，然后使用指定的参数训练一个随机森林，并计算归一化的特征重要性 $\\{I_j\\}_{j=1}^d$。\n\n情况1（包含一个信息特征、一个常数特征和一个噪声特征）：\n- 数据生成种子：$42$。\n- $N = 200$, $d = 3$。\n- 为 $i = 1,\\dots,200$，独立生成 $x_{i1} \\sim \\mathcal{N}(0,1)$。\n- 对所有 $i$，设置 $x_{i2} = 0$（一个常数特征）。\n- 对所有 $i$，独立生成 $x_{i3} \\sim \\mathcal{N}(0,1)$。\n- 如果 $x_{i1} > 0$，则定义标签 $y_i = 1$，否则 $y_i = 0$。\n- 随机森林参数：树的数量 $T = 50$，节点级特征子集大小 $m = 2$，森林随机性种子 $2024$。\n- 计算归一化重要性 $(I_1, I_2, I_3)$。\n\n情况2（一个完全重复的信息特征和一个噪声特征）：\n- 数据生成种子：$123$。\n- $N = 200$, $d = 3$。\n- 为 $i = 1,\\dots,200$，独立生成 $x_{i1} \\sim \\mathcal{N}(0,1)$。\n- 对所有 $i$，设置 $x_{i2} = x_{i1}$（一个完全重复的副本）。\n- 独立生成 $x_{i3} \\sim \\mathcal{N}(0,1)$。\n- 如果 $x_{i1} > 0$，则定义标签 $y_i = 1$，否则 $y_i = 0$。\n- 随机森林参数：树的数量 $T = 50$，节点级特征子集大小 $m = 1$，森林随机性种子 $2025$。\n- 计算归一化重要性 $(I_1, I_2, I_3)$，并计算重复份额统计量 $S_{\\text{dup}} = \\frac{I_1 + I_2}{I_1 + I_2 + I_3}$。\n\n情况3（所有特征均为常数）：\n- 数据生成种子：$7$。\n- $N = 100$, $d = 2$。\n- 对所有 $i$，设置 $x_{i1} = 0$ 和 $x_{i2} = 3$（两个特征均为常数）。\n- 使用给定的种子，将标签 $y_i$ 生成为独立的伯努利分布，其中 $P(y_i = 1) = 0.5$。\n- 随机森林参数：树的数量 $T = 10$，节点级特征子集大小 $m = 2$，森林随机性种子 $99$。\n- 计算归一化重要性 $(I_1, I_2)$ 及其总和 $Q = I_1 + I_2$。\n\n数值报告：\n- 对于情况1，报告三个浮点数 $I_1, I_2, I_3$。\n- 对于情况2，报告四个浮点数 $S_{\\text{dup}}, I_1, I_2, I_3$。\n- 对于情况3，报告三个浮点数 $Q, I_1, I_2$。\n- 将报告的每个浮点数四舍五入到六位小数。\n\n最终输出格式：\n您的程序应生成单行输出，其中包含所有报告结果，格式为方括号括起来的逗号分隔列表，顺序如下：\n$[I_1^{(1)}, I_2^{(1)}, I_3^{(1)}, S_{\\text{dup}}^{(2)}, I_1^{(2)}, I_2^{(2)}, I_3^{(2)}, Q^{(3)}, I_1^{(3)}, I_2^{(3)}]$,\n其中上标表示情况的索引。没有物理单位。不使用角度。所有比例必须以小数形式报告，而不是百分比。", "solution": "用户指定了从零开始实现一个用于二分类的随机森林分类器的任务。该实现必须遵循一套关于分类与回归树（CART）、基尼不纯度、节点分裂以及基于平均不纯度下降计算特征重要性的特定方法的精确定义。该实现将通过三个不同的测试用例进行验证，并且必须以特定格式报告一组数值结果。\n\n### 问题验证\n\n首先，我将根据所需标准验证问题陈述。\n\n**第一步：提取已知条件**\n\n- **算法：** 一个由`T`棵用于二分类（`y \\in \\{0,1\\}`）的分类与回归树（CART）组成的随机森林。\n- **树的构建（递归）：**\n    - 在每个节点 `\\mathcal{I}`，从 `d` 个总特征中随机选择 `m` 个特征（无放回）。\n    - 对每个选定的特征 `j`，找到最优分裂。\n    - **阈值 (`\\tau`)：** `\\mathcal{I}` 中样本在特征 `j` 上的排序后唯一值之间的中点。\n    - **分裂规则：** 左子节点 `\\mathcal{L} = \\{i \\in \\mathcal{I} | x_{ij} \\le \\tau\\}`，右子节点 `\\mathcal{R} = \\{i \\in \\mathcal{I} | x_{ij} > \\tau\\}`。\n    - **停止条件：** 如果最佳的不纯度下降为非正数，或无法进行有效分裂（例如，节点处所有特征均为常数），则该节点成为叶节点。\n- **分裂准则（基尼不纯度）：**\n    - **节点 `\\mathcal{I}` 的不纯度：** `G(\\mathcal{I}) = 1 - \\sum_{k \\in \\{0,1\\}} p_k^2`，其中 `p_k` 是类别 `k` 的比例。\n    - **不纯度下降（基尼增益）：** `\\Delta(\\mathcal{I}, j, \\tau) = G(\\mathcal{I}) - \\frac{|\\mathcal{L}|}{|\\mathcal{I}|} G(\\mathcal{L}) - \\frac{|\\mathcal{R}|}{|\\mathcal{I}|} G(\\mathcal{R})`。选择 `\\Delta` 最大的分裂。\n- **集成方法（随机森林）：**\n    - `T` 棵树中的每一棵都在从大小为 `N` 的训练数据中进行大小为 `N` 的自助采样（有放回抽样）得到的样本上进行训练。\n- **特征重要性：**\n    - 对于单棵树和特征 `j`，重要性是所有以 `j` 为最佳分裂特征的节点上 `|\\mathcal{I}| \\cdot \\Delta(\\mathcal{I}, j, \\tau)` 的总和。\n    - 特征 `j` 的总重要性是所有 `T` 棵树的该值聚合后的总和。\n    - **归一化：** 设 `S` 为所有特征的聚合重要性之和。如果 `S > 0`，归一化重要性 `I_j` 为 `\\frac{\\text{aggregated decrease for } j}{S}`；如果 `S = 0`，则 `I_j = 0`。\n- **测试用例与参数：**\n    - **情况1：** 数据种子=`42`, `N=200`, `d=3`。`x_1 \\sim \\mathcal{N}(0,1)`, `x_2=0`, `x_3 \\sim \\mathcal{N}(0,1)`。若 `x_1 > 0` 则 `y=1`，否则为 `0`。随机森林参数：`T=50`, `m=2`, 森林种子=`2024`。报告：`(I_1, I_2, I_3)`。\n    - **情况2：** 数据种子=`123`, `N=200`, `d=3`。`x_1 \\sim \\mathcal{N}(0,1)`, `x_2=x_1`, `x_3 \\sim \\mathcal{N}(0,1)`。若 `x_1 > 0` 则 `y=1`，否则为 `0`。随机森林参数：`T=50`, `m=1`, 森林种子=`2025`。报告：`(S_{\\text{dup}}, I_1, I_2, I_3)`，其中 `S_{\\text{dup}} = \\frac{I_1 + I_2}{I_1 + I_2 + I_3}`。\n    - **情况3：** 数据种子=`7`, `N=100`, `d=2`。`x_1=0`, `x_2=3`。`y \\sim \\text{Bernoulli}(0.5)`。随机森林参数：`T=10`, `m=2`, 森林种子=`99`。报告：`(Q, I_1, I_2)`，其中 `Q = I_1 + I_2`。\n- **输出：** 一个包含 `10` 个浮点数的单行逗号分隔列表，四舍五入到六位小数，并用方括号括起来。\n\n**第二步：使用提取的已知条件进行验证**\n\n- **科学依据：** 问题描述了标准的随机森林算法。CART 方法论、基尼不纯度准则以及用于特征重要性的平均不纯度下降（MDI）都是统计学习中公认的概念。所提供的定义在数学上和算法上都是正确的。\n- **适定性：** 问题陈述得非常严谨。所有参数（N、d、T、m）、数据生成过程和随机种子都已提供，这确保了结果是唯一的、确定性的和可计算的。边缘情况（常数特征、无正增益）也得到了明确定义。\n- **客观性：** 语言正式、精确，没有任何主观或模糊的术语。\n\n该问题未违反任何无效性标准。它是一个定义明确、科学合理的计算任务。\n\n**第三步：结论与行动**\n\n问题有效。我将继续进行实现和求解。\n\n### 算法实现\n\n解决方案将由一组函数构成，这些函数共同实现指定的随机森林算法。\n\n1.  **`gini_impurity(y)`**：一个辅助函数，用于计算给定标签数组 `y` 的基尼不纯度。\n2.  **`find_best_split(X, y, idxs, feature_subset)`**：此函数遍历给定的特征子集。对于每个特征，它评估所有有效的阈值（唯一值的中点），以找到最大化基尼增益的分裂。它返回一个包含找到的最佳分裂细节（特征索引、阈值、增益和子节点索引）的字典，如果未找到有效分裂，则返回 `None`。\n3.  **`grow_tree(X, y, idxs, m, d, rng, tree_importances)`**：一个递归函数，用于构建单棵决策树。在每一步（节点），它会检查停止条件（例如，纯节点）。如果不是叶节点，它会随机选择 `m` 个特征，调用 `find_best_split` 来确定最优分裂，记录对特征重要性的贡献（`|\\mathcal{I}| \\cdot \\Delta`），然后对左、右子节点递归调用自身。\n4.  **`run_rf_case(X, y, T, m, forest_seed)`**：此函数负责为单个测试用例协调整个随机森林的训练过程。它使用森林种子初始化一个随机数生成器。然后迭代 `T` 次，每次创建一个数据的自助采样样本，并使用 `grow_tree` 生成一棵树。它汇总所有树的特征重要性，并返回最终的归一化重要性分数。\n5.  **`solve()`**：主函数，负责为三个测试用例准备数据，使用适当的参数调用 `run_rf_case`，计算所需的摘要统计量（`S_{\\text{dup}}`, `Q`），并以指定格式打印最终的合并结果。\n\n使用 `numpy.random.default_rng` 仔细管理随机性，以确保数据生成和算法所有随机方面（自助采样和特征选择）的可复现性。特别注意边缘情况，例如处理无法分裂的常数特征，以及仅在总不纯度下降为正时才对重要性进行归一化。", "answer": "```python\nimport numpy as np\n\ndef gini_impurity(y):\n    \"\"\"Computes the Gini impurity of a set of labels.\"\"\"\n    n_samples = len(y)\n    if n_samples == 0:\n        return 0.0\n    _, counts = np.unique(y, return_counts=True)\n    proportions = counts / n_samples\n    return 1.0 - np.sum(proportions**2)\n\ndef find_best_split(X, y, idxs, feature_subset):\n    \"\"\"Finds the best split for a node by iterating through features and thresholds.\"\"\"\n    X_node, y_node = X[idxs], y[idxs]\n    n_node = len(y_node)\n    \n    if n_node = 1:\n        return None\n\n    g_parent = gini_impurity(y_node)\n    best_gain = -1.0\n    best_split_info = None\n\n    for j in feature_subset:\n        feature_values = X_node[:, j]\n        unique_vals = np.unique(feature_values)\n\n        if len(unique_vals) = 1:\n            continue\n\n        thresholds = (unique_vals[:-1] + unique_vals[1:]) / 2.0\n\n        for tau in thresholds:\n            left_mask = feature_values = tau\n            right_mask = ~left_mask\n\n            y_left, y_right = y_node[left_mask], y_node[right_mask]\n\n            if len(y_left) == 0 or len(y_right) == 0:\n                continue\n\n            g_left = gini_impurity(y_left)\n            g_right = gini_impurity(y_right)\n            n_left, n_right = len(y_left), len(y_right)\n            \n            w_left = n_left / n_node\n            w_right = n_right / n_node\n            \n            gain = g_parent - (w_left * g_left + w_right * g_right)\n\n            if gain > best_gain:\n                best_gain = gain\n                best_split_info = {\n                    'feature': j, \n                    'threshold': tau, \n                    'gain': gain,\n                    'left_idxs': idxs[left_mask],\n                    'right_idxs': idxs[right_mask]\n                }\n                \n    return best_split_info\n\ndef grow_tree(X, y, idxs, m, d, rng, tree_importances):\n    \"\"\"Recursively grows a single decision tree and accumulates feature importances.\"\"\"\n    if len(np.unique(y[idxs])) == 1:\n        return\n\n    m_node = min(m, d)\n    feature_subset = rng.choice(d, size=m_node, replace=False)\n    \n    best_split = find_best_split(X, y, idxs, feature_subset)\n    \n    if best_split is None or best_split['gain'] = 0:\n        return\n\n    j = best_split['feature']\n    gain = best_split['gain']\n    n_samples = len(idxs)\n    tree_importances[j] += n_samples * gain\n    \n    left_idxs = best_split['left_idxs']\n    right_idxs = best_split['right_idxs']\n    \n    grow_tree(X, y, left_idxs, m, d, rng, tree_importances)\n    grow_tree(X, y, right_idxs, m, d, rng, tree_importances)\n\ndef run_rf_case(X, y, T, m, forest_seed):\n    \"\"\"Runs the Random Forest algorithm for a given case and computes feature importances.\"\"\"\n    N, d = X.shape\n    forest_rng = np.random.default_rng(forest_seed)\n    total_importances = np.zeros(d)\n\n    for _ in range(T):\n        bootstrap_idxs = forest_rng.choice(N, size=N, replace=True)\n        X_sample, y_sample = X[bootstrap_idxs], y[bootstrap_idxs]\n        \n        tree_importances = np.zeros(d)\n        \n        # Grow a tree on the bootstrap sample\n        initial_idxs = np.arange(len(y_sample))\n        grow_tree(X_sample, y_sample, initial_idxs, m, d, forest_rng, tree_importances)\n        \n        total_importances += tree_importances\n\n    S = np.sum(total_importances)\n    if S > 0:\n        normalized_importances = total_importances / S\n    else:\n        normalized_importances = np.zeros(d)\n        \n    return normalized_importances\n\ndef solve():\n    \"\"\"Generates data for three cases, runs the RF, and reports the results.\"\"\"\n    all_results = []\n\n    # Case 1\n    data_seed, N, d = 42, 200, 3\n    T, m, forest_seed = 50, 2, 2024\n    rng_case1 = np.random.default_rng(data_seed)\n    X1 = np.zeros((N, d))\n    X1[:, 0] = rng_case1.normal(size=N)\n    X1[:, 1] = 0.0\n    X1[:, 2] = rng_case1.normal(size=N)\n    y1 = (X1[:, 0] > 0).astype(int)\n    importances1 = run_rf_case(X1, y1, T, m, forest_seed)\n    all_results.extend(importances1)\n\n    # Case 2\n    data_seed, N, d = 123, 200, 3\n    T, m, forest_seed = 50, 1, 2025\n    rng_case2 = np.random.default_rng(data_seed)\n    X2 = np.zeros((N, d))\n    X2[:, 0] = rng_case2.normal(size=N)\n    X2[:, 1] = X2[:, 0]\n    X2[:, 2] = rng_case2.normal(size=N)\n    y2 = (X2[:, 0] > 0).astype(int)\n    importances2 = run_rf_case(X2, y2, T, m, forest_seed)\n    I1_2, I2_2, I3_2 = importances2\n    denom = I1_2 + I2_2 + I3_2\n    S_dup = (I1_2 + I2_2) / denom if denom > 0 else 0.0\n    all_results.extend([S_dup, I1_2, I2_2, I3_2])\n\n    # Case 3\n    data_seed, N, d = 7, 100, 2\n    T, m, forest_seed = 10, 2, 99\n    rng_case3 = np.random.default_rng(data_seed)\n    X3 = np.zeros((N, d))\n    X3[:, 0] = 0.0\n    X3[:, 1] = 3.0\n    y3 = rng_case3.integers(0, 2, size=N)\n    importances3 = run_rf_case(X3, y3, T, m, forest_seed)\n    I1_3, I2_3 = importances3\n    Q = I1_3 + I2_3\n    all_results.extend([Q, I1_3, I2_3])\n\n    # Format and print the final output\n    formatted_results = [f\"{r:.6f}\" for r in all_results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3166180"}, {"introduction": "从算法的理想世界走向应用建模的现实世界，我们会遇到各种挑战，其中“目标泄漏”（target leakage）是一个特别隐蔽且危险的陷阱。这个练习模拟了一个金融信贷场景，其中一个特征无意中包含了关于贷款违约结果的未来信息。通过训练一个决策树桩集成模型并分析其特征重要性，你将看到模型如何轻易地被这些泄漏信息“欺骗”，并认识到特征重要性不仅可以评估特征贡献，还可以作为诊断工具来发现数据中存在的问题。[@problem_id:2386893]", "problem": "您正在对一个信贷投资组合中的二元贷款违约结果进行建模。对于每个观测值 $i \\in \\{1,\\dots,n\\}$，设二元目标为 $y_i \\in \\{0,1\\}$，其中 $y_i = 1$ 表示违约。您将生成具有经济可解释性的合成协变量，然后添加一个微妙的、在结果产生后会泄露关于 $y_i$ 信息的协变量。然后，您必须量化一个决策桩（单次分裂决策树森林）集成模型如何按重要性对协变量进行排序，并报告每个测试用例中最重要的协变量的从零开始的索引。索引必须以整数形式报告。\n\n数据生成过程：\n- 设基础协变量的数量为 $p_b = 5$。对每个观测值 $i$，从一个均值为零、协方差矩阵为 $\\Sigma(\\rho) \\in \\mathbb{R}^{p_b \\times p_b}$ 的多元正态分布中抽取一个基础特征向量 $\\mathbf{x}_i \\in \\mathbb{R}^{p_b}$，该协方差矩阵定义为\n$$\n\\Sigma(\\rho) = (1-\\rho) I_{p_b} + \\rho \\mathbf{1}\\mathbf{1}^\\top,\n$$\n其中 $I_{p_b}$ 是大小为 $p_b$ 的单位矩阵，$\\mathbf{1}$ 是 $p_b$ 维的全一向量。标量 $\\rho \\in (-\\frac{1}{p_b-1},1)$ 控制基础特征之间的共同相关性。\n- 独立于 $\\mathbf{x}_i$ 抽取一个特异性宏观因子 $m_i \\sim \\mathcal{N}(0,1)$。\n- 通过一个逻辑指数定义一个潜在得分 $s_i$：\n$$\ns_i = \\beta_0 + \\sum_{j=1}^{p_b} \\beta_j x_{i,j} + \\gamma m_i,\n$$\n系数固定为 $\\beta_0 = -0.5$，$\\beta_1 = 0.8$，$\\beta_2 = -1.0$，$\\beta_3 = 0.6$，$\\beta_4 = 0.0$，$\\beta_5 = 0.5$ 以及 $\\gamma = 0.7$。\n- 通过逻辑函数定义违约概率 $p_i$：\n$$\np_i = \\frac{1}{1 + e^{-s_i}}.\n$$\n- 对每个 $i$ 独立地抽取二元结果 $y_i \\sim \\text{Bernoulli}(p_i)$。\n- 定义一个泄露目标信息的结果后协变量 $z_i$ 为\n$$\nz_i = \\lambda \\, y_i + \\delta_i,\n$$\n其中 $\\delta_i \\sim \\mathcal{N}(0,\\sigma^2)$ 与所有其他变量独立。参数 $\\lambda \\in \\mathbb{R}$ 控制泄露的幅度，而 $\\sigma \\ge 0$ 控制掩盖泄露的噪声量。\n- 为了建模，您将如下构建特征向量 $\\tilde{\\mathbf{x}}_i$。如果测试用例标志 $\\text{include\\_leak} = 1$，则设置\n$$\n\\tilde{\\mathbf{x}}_i = \\big[x_{i,1}, x_{i,2}, x_{i,3}, x_{i,4}, x_{i,5}, z_i\\big] \\in \\mathbb{R}^{p},\n$$\n此时 $p = p_b + 1$，且泄露协变量位于从零开始的索引 $p_b$ 处。如果 $\\text{include\\_leak} = 0$，则设置\n$$\n\\tilde{\\mathbf{x}}_i = \\big[x_{i,1}, x_{i,2}, x_{i,3}, x_{i,4}, x_{i,5}\\big] \\in \\mathbb{R}^{p},\n$$\n此时 $p = p_b$。\n\n模型与重要性：\n- 考虑一个由 $T=200$ 个决策桩（单次分裂决策树）组成的集成模型。对于每棵树 $t \\in \\{1,\\dots,T\\}$：\n  - 通过从 $\\{1,\\dots,n\\}$ 中有放回地抽样索引，抽取一个大小为 $n$ 的自助样本（bootstrap sample）。\n  - 令 $m_{\\text{try}} = \\lceil \\sqrt{p} \\rceil$，并从 $\\{0,\\dots,p-1\\}$ 中均匀随机地选择 $m_{\\text{try}}$ 个不同的特征用于考虑分裂。\n  - 对于每个选定的特征 $j$，考虑形式为 $x_{j} \\le \\tau$ 的分裂，其中阈值 $\\tau$ 取自自助样本中该特征排序后观测值的连续值之间的中点，排除那些会导致子节点为空的阈值。令 $G(S)$ 表示一组二元标签 $S$ 的基尼不纯度（Gini impurity）：\n  $$\n  G(S) = 1 - \\sum_{c \\in \\{0,1\\}} \\left(\\frac{n_c}{|S|}\\right)^2,\n  $$\n  其中 $n_c$ 是类别 $c$ 在 $S$ 中的计数。对于一个标签多重集为 $S$ 的父节点，分裂为左子节点 $S_L$ 和右子节点 $S_R$，定义不纯度减少量为\n  $$\n  \\Delta G = G(S) - \\left(\\frac{|S_L|}{|S|}G(S_L) + \\frac{|S_R|}{|S|}G(S_R)\\right).\n  $$\n  - 在所考虑的特征和阈值中，选择能使 $\\Delta G$ 最大化的特征 $j^\\star$ 和阈值 $\\tau^\\star$。通过在 $(j^\\star,\\tau^\\star)$ 处进行分裂来生成一个决策桩。\n  - 将所选分裂实现的不纯度减少量 $\\Delta G^\\star$ 归因于特征 $j^\\star$。\n- 定义特征 $j$ 的重要性为在 $T$ 棵树上归因于它的不纯度减少量之和：\n$$\nI_j = \\sum_{t=1}^{T} \\Delta G_t \\cdot \\mathbf{1}\\{j_t^\\star = j\\}.\n$$\n- 对每个测试用例，计算使 $I_j$ 最大化的索引 $j_{\\max} \\in \\{0,\\dots,p-1\\}$。如果出现平局，则取达到最大值的最小索引。\n\n测试套件：\n为保证可复现性，每个测试用例的数据生成和集成模型构建过程均使用一个独立的随机种子 $s$。使用以下四个测试用例，每个用例由元组 $(n,\\sigma,\\lambda,\\rho,\\text{include\\_leak}, s)$ 指定：\n- 用例 A: $(3000, 0.1, 0.9, 0.2, 1, 11)$。\n- 用例 B: $(3000, 0.3, 0.6, 0.2, 1, 12)$。\n- 用例 C: $(1200, 0.1, 0.9, 0.2, 0, 13)$。\n- 用例 D: $(3000, 0.6, 0.4, 0.2, 1, 14)$。\n\n要求的程序行为和输出：\n- 对于每个测试用例，根据上述过程生成数据，训练上述集成模型，计算特征重要性 $\\{I_j\\}_{j=0}^{p-1}$，并返回最重要特征的从零开始的索引 $j_{\\max}$。\n- 您的程序必须产生单行输出，其中包含四个索引，以逗号分隔的列表形式并用方括号括起来，顺序与测试用例相同，例如 [$i_1$,$i_2$,$i_3$,$i_4$]。不应打印任何额外文本。", "solution": "所提出的问题是计算统计学和机器学习领域一个有效且适定的练习，具体涉及基于树的集成模型中特征重要性的评估。数据生成过程被严格定义，并在金融计量经济学的标准模型中有其科学依据。任务是使用决策桩集成模型中的基尼不纯度减少量来量化特征重要性，并识别出最具影响力的特征，特别是在存在“泄露”协变量的情况下。该问题是客观、自洽且算法上明确的，允许一个唯一、可复现的解。\n\n对于每个测试用例，求解方法分两个主要阶段进行：数据生成和包含重要性计算的模型训练。所有数学实体，包括变量、参数和数值，都按要求使用 LaTeX 表示。\n\n**1. 数据生成过程**\n\n对于每个指定的测试用例，根据以下随机过程生成一个大小为 $n$ 的合成数据集。使用随机种子 $s$ 来确保可复现性。\n\n- **基础协变量**：对每个观测值 $i \\in \\{1, \\dots, n\\}$，从一个多元正态分布 $\\mathcal{N}(\\mathbf{0}, \\Sigma(\\rho))$ 中抽取一组 $p_b = 5$ 个基础协变量，记为向量 $\\mathbf{x}_i \\in \\mathbb{R}^{p_b}$。协方差矩阵 $\\Sigma(\\rho)$ 定义为\n$$\n\\Sigma(\\rho) = (1-\\rho) I_{p_b} + \\rho \\mathbf{1}\\mathbf{1}^\\top\n$$\n其中 $I_{p_b}$ 是 $p_b \\times p_b$ 的单位矩阵，$\\mathbf{1}$ 是一个 $p_b$ 维的全一向量。参数 $\\rho$ 控制这些基础特征之间的等相关性。\n\n- **潜在得分与违约概率**：独立抽取一个特异性因子 $m_i \\sim \\mathcal{N}(0,1)$。一个潜在得分 $s_i$ 被构建为基础协变量和宏观因子的线性组合：\n$$\ns_i = \\beta_0 + \\sum_{j=1}^{p_b} \\beta_j x_{i,j} + \\gamma m_i\n$$\n系数固定为 $\\beta_0 = -0.5$，$\\boldsymbol{\\beta}_{1..p_b} = [0.8, -1.0, 0.6, 0.0, 0.5]$，以及 $\\gamma = 0.7$。请注意，$x_{i,4}$ 的系数为 $\\beta_4 = 0.0$，这使得该特征在构造上相对于潜在得分是无信息的。然后使用标准逻辑函数将潜在得分转换为违约概率 $p_i$：\n$$\np_i = \\frac{1}{1 + e^{-s_i}}\n$$\n\n- **二元结果**：二元目标变量 $y_i \\in \\{0, 1\\}$ 表示未违约（$0$）或违约（$1$），从一个以生成概率为参数的伯努利分布中抽取，$y_i \\sim \\text{Bernoulli}(p_i)$。\n\n- **泄露协变量**：生成一个结果后协变量 $z_i$ 来模拟来自目标变量的信息泄露。其定义为：\n$$\nz_i = \\lambda \\, y_i + \\delta_i\n$$\n其中 $\\delta_i$ 是从正态分布 $\\mathcal{N}(0, \\sigma^2)$ 中抽取的噪声项。参数 $\\lambda$ 控制泄露的强度，而 $\\sigma$ 控制噪声水平。$|\\lambda|$ 与 $\\sigma$ 的高比率意味着 $z_i$ 和 $y_i$ 之间存在强大且易于检测的联系。\n\n- **最终特征矩阵**：组装完整的特征矩阵 $\\tilde{\\mathbf{X}}$。如果 `include_leak` 标志为 $1$，则特征集为 $\\tilde{\\mathbf{x}}_i = [x_{i,1}, \\dots, x_{i,5}, z_i] \\in \\mathbb{R}^{6}$。泄露特征 $z_i$ 位于最后一个位置（从零开始的索引为 $5$）。如果标志为 $0$，则仅使用基础协变量，$\\tilde{\\mathbf{x}}_i = [x_{i,1}, \\dots, x_{i,5}] \\in \\mathbb{R}^{5}$。特征的数量用 $p$ 表示。\n\n**2. 特征重要性量化**\n\n每个特征的重要性通过训练一个由 $T=200$ 个决策桩组成的集成模型来确定。决策桩是只有一个分裂的决策树。\n\n- **集成模型构建**：对于集成模型中的 $T$ 个决策桩中的每一个：\n    1. 通过从完整数据集 $(\\tilde{\\mathbf{X}}, \\mathbf{y})$ 中有放回地抽样，创建一个大小为 $n$ 的自助样本。\n    2. 随机选择一个由 $m_{\\text{try}} = \\lceil \\sqrt{p} \\rceil$ 个不同特征组成的子集。\n    3. 对于每个选定的特征，找到最优分裂。分裂由一个特征 $j$ 和一个阈值 $\\tau$ 定义。分裂的质量由基尼不纯度减少量 $\\Delta G$ 来衡量。一组标签 $S$ 的基尼不纯度由下式给出：\n    $$\n    G(S) = 1 - \\sum_{c \\in \\{0,1\\}} \\left(\\frac{n_c}{|S|}\\right)^2\n    $$\n    其中 $n_c$ 是类别 $c$ 在集合 $S$ 中的计数。不纯度减少量是父节点的不纯度与两个子节点不纯度的加权平均值之间的差。\n    4. 选择能够产生最大不纯度减少量 $\\Delta G^\\star$ 的特征 $j^\\star$ 和阈值 $\\tau^\\star$ 用于决策桩的分裂。潜在的阈值是自助样本中特征的连续唯一排序值的中点。\n\n- **重要性聚合**：特征 $j$ 的重要性，记为 $I_j$，计算为它在集成模型中所有树上所贡献的不纯度减少量之和：\n$$\nI_j = \\sum_{t=1}^{T} \\Delta G_t \\cdot \\mathbf{1}\\{j_t^\\star = j\\}\n$$\n其中 $\\mathbf{1}\\{\\cdot\\}$ 是指示函数。\n\n- **结果**：最后，对于每个测试用例，识别出具有最高重要性得分的特征的从零开始的索引，$j_{\\max} = \\arg\\max_j I_j$。平局通过选择最小的索引来解决。该索引是该用例的输出。该过程用 Python 实现，遵循指定的库和随机种子，以确保结果可验证。", "answer": "```python\nimport numpy as np\nfrom math import ceil, sqrt\n\ndef _gini_impurity(y):\n    \"\"\"Calculates the Gini impurity of a set of binary labels.\"\"\"\n    n_samples = len(y)\n    if n_samples == 0:\n        return 0.0\n    n1_count = np.sum(y)\n    p1 = n1_count / n_samples\n    p0 = 1.0 - p1\n    return 1.0 - (p0**2 + p1**2)\n\ndef _find_best_split(X_boot, y_boot, feature_indices):\n    \"\"\"Finds the best split for a single decision stump.\"\"\"\n    n_samples = len(y_boot)\n    if n_samples = 1:\n        return -1, -1.0\n\n    best_gain = -1.0\n    best_feature = -1\n\n    gini_parent = _gini_impurity(y_boot)\n    n1_total = np.sum(y_boot)\n    \n    for j in feature_indices:\n        feature_values = X_boot[:, j]\n        \n        unique_vals = np.unique(feature_values)\n        if len(unique_vals)  2:\n            continue\n            \n        # Efficiently find the best split for feature j\n        sorted_indices = np.argsort(feature_values)\n        y_sorted = y_boot[sorted_indices]\n        x_sorted = feature_values[sorted_indices]\n        \n        y_cumsum = np.cumsum(y_sorted)\n\n        split_points = np.where(x_sorted[:-1] != x_sorted[1:])[0]\n\n        for i in split_points:\n            n_left = i + 1\n            n_right = n_samples - n_left\n\n            n1_left = y_cumsum[i]\n            n1_right = n1_total - n1_left\n\n            gini_left = 1.0 - ((n1_left / n_left)**2 + ((n_left - n1_left) / n_left)**2)\n            gini_right = 1.0 - ((n1_right / n_right)**2 + ((n_right - n1_right) / n_right)**2)\n            \n            gain = gini_parent - (n_left / n_samples * gini_left + n_right / n_samples * gini_right)\n\n            if gain > best_gain:\n                best_gain = gain\n                best_feature = j\n                \n    return best_feature, best_gain\n\ndef solve():\n    \"\"\"\n    Main function to run the simulation for all test cases and print the results.\n    \"\"\"\n    test_cases = [\n        # (n, sigma, lambda, rho, include_leak, seed)\n        (3000, 0.1, 0.9, 0.2, 1, 11),\n        (3000, 0.3, 0.6, 0.2, 1, 12),\n        (1200, 0.1, 0.9, 0.2, 0, 13),\n        (3000, 0.6, 0.4, 0.2, 1, 14),\n    ]\n\n    results = []\n\n    for n, sigma, lam, rho, include_leak, seed in test_cases:\n        # Set seed for reproducibility for the entire test case\n        np.random.seed(seed)\n\n        # 1. Data Generation\n        p_b = 5\n        betas = np.array([0.8, -1.0, 0.6, 0.0, 0.5])\n        beta_0 = -0.5\n        gamma = 0.7\n        \n        # Covariance matrix\n        cov_matrix = (1 - rho) * np.identity(p_b) + rho * np.ones((p_b, p_b))\n        \n        # Base features\n        X_base = np.random.multivariate_normal(np.zeros(p_b), cov_matrix, n)\n        \n        # Macro factor\n        m = np.random.randn(n)\n        \n        # Latent score\n        s = beta_0 + X_base @ betas + gamma * m\n        \n        # Default probability\n        p_default = 1.0 / (1.0 + np.exp(-s))\n        \n        # Binary outcome\n        y = np.random.binomial(1, p_default, n)\n        \n        # Assemble final feature matrix\n        if include_leak == 1:\n            delta = np.random.normal(0, sigma, n)\n            z = lam * y + delta\n            X = np.c_[X_base, z]\n        else:\n            X = X_base\n        \n        n_samples, p = X.shape\n\n        # 2. Ensemble Training and Importance Calculation\n        T = 200\n        m_try = ceil(sqrt(p))\n        importances = np.zeros(p)\n        \n        all_indices = np.arange(n_samples)\n\n        for _ in range(T):\n            # Bootstrap sample\n            boot_indices = np.random.choice(all_indices, size=n_samples, replace=True)\n            X_boot, y_boot = X[boot_indices], y[boot_indices]\n            \n            # Feature subsampling\n            feature_indices = np.random.choice(p, size=m_try, replace=False)\n            \n            # Find best split for this stump\n            best_feature, best_gain = _find_best_split(X_boot, y_boot, feature_indices)\n            \n            if best_feature != -1:\n                importances[best_feature] += best_gain\n\n        # 3. Find the most important feature index\n        j_max = np.argmax(importances)\n        results.append(j_max)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2386893"}]}