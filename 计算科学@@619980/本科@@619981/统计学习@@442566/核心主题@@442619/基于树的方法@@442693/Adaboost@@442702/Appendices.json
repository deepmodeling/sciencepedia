{"hands_on_practices": [{"introduction": "理论知识需要通过实践来巩固。这项练习要求您在一个精心构建的小型数据集上实现AdaBoost算法，该数据集旨在诱导加权误差序列产生振荡。通过亲手实现并调整学习率（或称为“收缩率”）$\\nu$，您将直观地观察到该参数如何抑制振荡、稳定训练过程，并最终影响分类间隔的增长，从而加深对AdaBoost动态行为的理解。[@problem_id:3095513]", "problem": "要求您实现并分析一个版本的自适应增强（AdaBoost），它将运行在一个特意构建的、会在加权错误序列中引起振荡的小型数据集上。您的程序必须是一个完整、可运行的实现，遵循以下规范，并以要求的格式输出单行结果。\n\n背景和理论基础：\n- 在标签为 $\\{-1,+1\\}$ 的二元分类框架内进行。\n- 在增强的第 $t$ 轮，存在一个覆盖 $N$ 个训练样本的权重分布 $D_t$。\n- 弱学习器 $h_t$ 将输入映射到 $\\{-1,+1\\}$，其加权分类误差为 $\\varepsilon_t = \\sum_{i=1}^N D_t(i)\\,\\mathbb{1}\\{y_i \\neq h_t(x_i)\\}$。\n- 每一轮从一个固定的字典中选择一个能最小化加权误差的弱学习器 $h_t$。每轮的权重更新必须通过最小化逐轮指数损失 $\\sum_{i=1}^N D_t(i)\\,\\exp(-\\alpha_t y_i h_t(x_i))$ 来推导，该最小化是相对于系数 $\\alpha_t$ 进行的；此最小化过程定义了归一化因子 $Z_t$ 以及归一化后对 $D_{t+1}$ 的更新。您还必须考虑一个收缩参数 $\\nu \\in (0,1]$，它将选定的 $\\alpha_t$ 乘以因子 $\\nu$ 以抑制振荡。不允许使用其他任何启发式方法或修改。\n\n数据集和弱学习器字典：\n- 精确使用以下包含 $N=12$ 个点的数据集，这些点位于 $\\mathbb{R}^2$ 中，标签为 $y \\in \\{-1,+1\\}$：\n  - 四个 $(x_1,x_2)=(+1,-1)$ 的副本，其 $y=-1$。\n  - 四个 $(x_1,x_2)=(-1,+1)$ 的副本，其 $y=-1$。\n  - 两个 $(x_1,x_2)=(+1,+1)$ 的副本，其 $y=+1$。\n  - 两个 $(x_1,x_2)=(-1,-1)$ 的副本，其 $y=-1$。\n- 弱学习器字典仅包含以下两个固定的决策树桩：\n  - $h^{(1)}(x) = \\mathrm{sign}(x_1)$。\n  - $h^{(2)}(x) = \\mathrm{sign}(x_2)$。\n- 在每个增强轮次 $t$，选择加权误差 $\\varepsilon_t$ 较小的弱学习器。如果出现平局，选择索引较小的那个（即选择 $h^{(1)}$）。\n\n程序要求：\n- 从均匀分布 $D_1(i)=1/N$ 开始，实现带收缩参数 $\\nu \\in (0,1]$ 的 AdaBoost 算法，共运行 $T$ 轮。\n- 在每一轮 $t$：\n  - 计算两个弱学习器的加权误差并选择 $h_t$。\n  - 通过最小化给定选定 $h_t$ 的逐轮指数损失来确定系数 $\\alpha_t$。然后应用收缩，得到 $\\nu\\alpha_t$ 作为实际的更新幅度。\n  - 使用收缩后的系数通过指数更新来更新分布 $D_{t+1}$，并用该更新所隐含的因子 $Z_t$ 对其进行归一化。\n  - 累加评分函数 $F_T(x) = \\sum_{s=1}^t \\nu\\alpha_s h_s(x)$ 以跟踪间隔。\n- 在 $T$ 轮结束后，计算以下量值：\n  - 乘积 $\\prod_{t=1}^T Z_t$。\n  - 最小间隔 $\\min_{i \\in \\{1,\\dots,N\\}} y_i F_T(x_i)$。\n  - $(\\varepsilon_t)_{t=1}^T$ 的一阶差分序列中符号变化的次数，即在 $\\Delta_t = \\varepsilon_{t+1} - \\varepsilon_t$（其中 $t=1,\\dots,T-1$）中，计数符号变化时忽略任何 $\\Delta_t=0$ 的情况。\n  - 最大振荡幅度 $\\max_{t=1,\\dots,T-1} |\\varepsilon_{t+1} - \\varepsilon_t|$。\n\n测试套件：\n- 您的程序必须针对以下五个参数设置运行算法，每个都被视为一个独立的测试用例：\n  1. $T=12$, $\\nu=1.0$。\n  2. $T=12$, $\\nu=0.5$。\n  3. $T=12$, $\\nu=0.2$。\n  4. $T=1$, $\\nu=1.0$。\n  5. $T=30$, $\\nu=0.05$。\n- 对于每个测试用例，计算一个包含四个条目的结果列表，顺序如下：$[\\prod_{t=1}^T Z_t,\\ \\min_i y_i F_T(x_i),\\ \\text{符号变化次数},\\ \\text{最大振荡幅度}]$。\n\n数值和格式要求：\n- 所有浮点数输出必须使用标准四舍五入到该精度下最接近的可表示值，精确到 $6$ 位小数。\n- 所有整数必须是精确整数。\n- 您的程序应生成单行输出，其中包含五个按测试用例排列的列表，这些列表以逗号分隔，并用方括号括起来，例如：\n  - 输出格式: $[[r_{11},r_{12},r_{13},r_{14}],[r_{21},r_{22},r_{23},r_{24}],\\dots,[r_{51},r_{52},r_{53},r_{54}]]$\n其中 $r_{jk}$ 表示测试用例 $j$ 的第 $k$ 个结果。\n\n科学真实性和推导期望：\n- 您必须从加权误差、指数损失和归一化因子 $Z_t$ 的基本定义，以及选择最小化逐轮指数损失的系数的规则出发。不要在问题陈述中引入无关的公式或捷径。\n- 数据集和弱学习器字典是特意选择的，以便随着数据的不同部分受到越来越大的重视，加权误差 $\\varepsilon_t$ 会在不同轮次间振荡。您的实现和输出将量化这些振荡如何影响 $Z_t$ 和间隔的增长，以及收缩（由 $\\nu$ 控制）如何抑制振荡并改变结果。\n\n无用户输入：\n- 代码必须是完全自包含的，并且不得从输入或文件中读取数据。", "solution": "所提出的问题是有效的。它在科学上基于统计学习的原理，特别是 AdaBoost 算法，并且问题定义良好，具有一套完整且一致的定义、数据和目标。任务是实现一个带有收缩参数的 AdaBoost 变体，将其应用于指定的数据集，并分析其行为，特别是在增强轮次中加权误差的振荡情况。\n\n### AdaBoost 更新的理论基础\n\nAdaBoost 算法的核心是训练样本的迭代重新加权。在每一轮 $t$，我们寻找一个弱学习器 $h_t$ 及其对应的系数 $\\alpha_t$，以最小化当前加权数据分布 $D_t$ 上的指数损失函数。\n\n逐轮指数损失由下式给出：\n$$L(\\alpha_t) = \\sum_{i=1}^N D_t(i)\\,\\exp(-\\alpha_t y_i h_t(x_i))$$\n其中 $y_i \\in \\{-1, +1\\}$ 是真实标签，$h_t(x_i) \\in \\{-1, +1\\}$ 是所选弱学习器的预测。项 $y_i h_t(x_i)$ 对于正确分类为 $+1$，对于错误分类为 $-1$。\n\n为了找到最优的 $\\alpha_t$，我们将 $L(\\alpha_t)$ 对 $\\alpha_t$ 求导，并令结果为零：\n$$\\frac{\\partial L}{\\partial \\alpha_t} = \\sum_{i=1}^N D_t(i)\\,\\exp(-\\alpha_t y_i h_t(x_i))(-y_i h_t(x_i)) = 0$$\n我们可以将总和划分为正确分类和错误分类的样本。令 $\\varepsilon_t = \\sum_{i=1}^N D_t(i)\\,\\mathbb{1}\\{y_i \\neq h_t(x_i)\\}$ 为 $h_t$ 的加权误差。正确分类样本的权重之和为 $1-\\varepsilon_t$。方程变为：\n$$-\\sum_{i: y_i=h_t(x_i)} D_t(i)\\,e^{-\\alpha_t} + \\sum_{i: y_i\\neq h_t(x_i)} D_t(i)\\,e^{\\alpha_t} = 0$$\n$$-(1-\\varepsilon_t)e^{-\\alpha_t} + \\varepsilon_t e^{\\alpha_t} = 0$$\n$$\\varepsilon_t e^{\\alpha_t} = (1-\\varepsilon_t) e^{-\\alpha_t}$$\n$$e^{2\\alpha_t} = \\frac{1-\\varepsilon_t}{\\varepsilon_t}$$\n这就得出了系数 $\\alpha_t$ 的著名公式，前提是 $\\varepsilon_t \\in (0, 1)$：\n$$\\alpha_t = \\frac{1}{2} \\ln\\left(\\frac{1-\\varepsilon_t}{\\varepsilon_t}\\right)$$\n问题引入了一个收缩参数 $\\nu \\in (0, 1]$，它用于抑制更新。因此，更新的有效系数是 $\\nu\\alpha_t$。\n\n下一轮的权重 $D_{t+1}$ 通过减少正确分类样本的权重和增加错误分类样本的权重来更新。未归一化的权重为：\n$$D'_{t+1}(i) = D_t(i) \\exp(-\\nu\\alpha_t y_i h_t(x_i))$$\n为确保 $D_{t+1}$ 是一个有效的概率分布，我们用一个因子 $Z_t$ 进行归一化，该因子是未归一化权重的总和：\n$$Z_t = \\sum_{i=1}^N D'_{t+1}(i) = \\sum_{i=1}^N D_t(i) \\exp(-\\nu\\alpha_t y_i h_t(x_i))$$\n因此，最终的权重更新规则是：\n$$D_{t+1}(i) = \\frac{D_t(i) \\exp(-\\nu\\alpha_t y_i h_t(x_i))}{Z_t}$$\n\n### 算法流程\n\n实现过程如下：\n\n1.  **数据表示**：数据集包含 $N=12$ 个点，但只有 $4$ 个唯一的 $(x, y)$ 对。为了提高效率，我们对这些唯一的数据点进行操作，并为每个点关联一个计数：\n    -   A类：$(+1, -1), y=-1$，计数：$4$\n    -   B类：$(-1, +1), y=-1$，计数：$4$\n    -   C类：$(+1, +1), y=+1$，计数：$2$\n    -   D类：$(-1, -1), y=-1$，计数：$2$\n    每种唯一数据点类型的总初始权重是其计数除以 $N=12$。\n2.  **弱学习器**：两个弱学习器是 $h^{(1)}(x) = \\mathrm{sign}(x_1)$ 和 $h^{(2)}(x) = \\mathrm{sign}(x_2)$。它们对每个唯一数据点的预测和正确性都预先计算好。\n    -   $h^{(1)}$ 对 A 类点分类错误。\n    -   $h^{(2)}$ 对 B 类点分类错误。\n3.  **初始化**：从 $N=12$ 个点上的均匀权重分布开始，这分别对应于唯一数据点类型 A、B、C、D 的权重 $4/12、4/12、2/12、2/12$。\n4.  **增强迭代**：对于从 $1$ 到 $T$ 的每一轮 $t$：\n    a.  计算每个弱学习器的加权误差：$\\varepsilon_t^{(j)} = \\sum_{k \\in \\{A,B,C,D\\}} w_t(k) \\cdot \\mathbb{1}\\{y_k \\neq h^{(j)}(x_k)\\}$。此处，$w_t(k)$ 是数据点类型 $k$ 的总权重。\n    b.  选择具有最小加权误差 $\\varepsilon_t$ 的学习器 $h_t$。平局决胜规则规定，如果 $\\varepsilon_t^{(1)} = \\varepsilon_t^{(2)}$，则选择 $h^{(1)}$。\n    c.  计算 $\\alpha_t = \\frac{1}{2} \\ln((1-\\varepsilon_t)/\\varepsilon_t)$。为了数值稳定性，将 $\\varepsilon_t$ 裁剪到严格介于 $0$ 和 $1$ 之间。\n    d.  使用收缩后的系数 $\\nu\\alpha_t$ 计算归一化因子 $Z_t$。\n    e.  根据推导出的更新规则，为每种唯一数据点类型更新权重 $w_{t+1}$。\n    f.  存储 $\\varepsilon_t$、$Z_t$ 以及选定的收缩系数 $\\nu\\alpha_t$ 和学习器 $h_t$ 以供后处理。\n5.  **最终计算**：在 $T$ 轮之后：\n    a.  **$Z_t$ 的乘积**：计算 $\\Pi_{t=1}^T Z_t$。\n    b.  **最小间隔**：最终分类器是 $H(x) = \\mathrm{sign}(F_T(x))$，其中 $F_T(x) = \\sum_{t=1}^T \\nu\\alpha_t h_t(x)$。点 $x_i$ 的间隔是 $y_i F_T(x_i)$。我们找出所有唯一数据点中的最小间隔。\n    c.  **误差振荡分析**：计算差分序列 $\\Delta_t = \\varepsilon_{t+1} - \\varepsilon_t$（其中 $t=1, \\dots, T-1$）。\n        i.  通过计算 $\\Delta_t$ 序列中正负值之间的切换次数来找到符号变化的次数，忽略任何 $\\Delta_t=0$ 的情况。\n        ii. 最大振荡幅度是 $\\max_{t=1, \\dots, T-1} |\\Delta_t|$。\n    对于 $T \\le 1$ 的情况，振荡指标定义为 $0$。\n\n将此结构化流程应用于问题陈述中定义的五个测试用例中的每一个。所有浮点结果均四舍五入到六位小数。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the AdaBoost simulation for all test cases and print the results.\n    \"\"\"\n    test_cases = [\n        # (T, nu)\n        (12, 1.0),\n        (12, 0.5),\n        (12, 0.2),\n        (1, 1.0),\n        (30, 0.05),\n    ]\n\n    results = []\n    for T, nu in test_cases:\n        result = run_adaboost(T, nu)\n        results.append(result)\n\n    # Format the final output string as a list of lists.\n    # The str() of a list is its string representation, e.g., '[1, 2, 3]'.\n    # We join these string representations with commas and enclose in brackets.\n    output_str = f\"[{','.join(map(str, results))}]\"\n    print(output_str)\n\ndef run_adaboost(T, nu):\n    \"\"\"\n    Implements the specified AdaBoost algorithm for T rounds with shrinkage nu.\n    \n    Args:\n        T (int): The number of boosting rounds.\n        nu (float): The shrinkage parameter.\n\n    Returns:\n        list: A list containing the four required metrics:\n              [product of Z_t, min margin, sign changes, max oscillation amplitude].\n    \"\"\"\n    # Dataset definition (compact form for unique points)\n    # Types: A, B, C, D\n    # x_unique = np.array([[+1, -1], [-1, +1], [+1, +1], [-1, -1]])\n    y_unique = np.array([-1, -1, 1, -1])\n    counts = np.array([4, 4, 2, 2])\n    N = counts.sum()\n\n    # Weak learners' predictions on unique points\n    # h1(x) = sign(x1), h2(x) = sign(x2)\n    h1_preds = np.array([1, -1, 1, -1])\n    h2_preds = np.array([-1, 1, 1, -1])\n\n    # Pre-calculate y_i * h(x_i) for each learner\n    y_h1 = y_unique * h1_preds  # Result: [-1, 1, 1, 1] -> h1 misclassifies type A\n    y_h2 = y_unique * h2_preds  # Result: [1, -1, 1, 1] -> h2 misclassifies type B\n\n    # Initialize weights based on counts\n    weights = counts / N\n\n    # Storage for per-round quantities\n    all_Z_t = []\n    all_err_t = []\n    \n    # Store coefficients and chosen learners to compute final score function F_T\n    alphas_nu = []\n    hs_preds = []\n\n    # Epsilon for numerical stability to avoid log(0) or division by zero\n    epsilon = 1e-15\n\n    for _ in range(T):\n        # Calculate weighted errors for each learner\n        err1 = np.sum(weights[y_h1 == -1])\n        err2 = np.sum(weights[y_h2 == -1])\n\n        # Select learner based on minimum error (with tie-breaking)\n        if err1 = err2:\n            err_t = err1\n            y_h_t = y_h1\n            hs_preds.append(h1_preds)\n        else:\n            err_t = err2\n            y_h_t = y_h2\n            hs_preds.append(h2_preds)\n\n        # Clip error to avoid numerical issues\n        err_t = np.clip(err_t, epsilon, 1.0 - epsilon)\n        \n        # Calculate alpha\n        alpha_t = 0.5 * np.log((1 - err_t) / err_t)\n        \n        # Store shrunk alpha\n        alphas_nu.append(nu * alpha_t)\n\n        # Update weights\n        w_update_exp = np.exp(-nu * alpha_t * y_h_t)\n        Z_t = np.sum(weights * w_update_exp)\n        weights = weights * w_update_exp / Z_t\n\n        # Store round results\n        all_Z_t.append(Z_t)\n        all_err_t.append(err_t)\n\n    # 1. Product of Z_t\n    prod_Z = np.prod(all_Z_t)\n\n    # 2. Minimum margin\n    F_T = np.zeros(len(y_unique))\n    for i in range(T):\n        F_T += alphas_nu[i] * hs_preds[i]\n    margins = y_unique * F_T\n    min_margin = np.min(margins)\n\n    # For T = 1, oscillation metrics are 0\n    if T = 1:\n        sign_changes = 0\n        max_osc_amp = 0.0\n    else:\n        # 3. Number of sign changes in error differences\n        err_diffs = np.diff(all_err_t)\n        # Filter out differences that are effectively zero\n        nonzero_diffs = err_diffs[np.abs(err_diffs) > epsilon]\n        \n        if len(nonzero_diffs)  2:\n            sign_changes = 0\n        else:\n            signs = np.sign(nonzero_diffs)\n            sign_changes = np.sum(np.diff(signs) != 0)\n        \n        # 4. Maximum oscillation amplitude\n        max_osc_amp = np.max(np.abs(err_diffs))\n\n    # Round final results to 6 decimal places, keeping integers as ints\n    return [\n        round(prod_Z, 6),\n        round(min_margin, 6),\n        int(sign_changes),\n        round(max_osc_amp, 6)\n    ]\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3095513"}, {"introduction": "AdaBoost的优势在于它能集中处理“困难”样本，但这在面对噪声数据或异常值时也可能成为其弱点。本练习将模拟一个关键的脆弱性：单个持续被错误分类的样本（可视为一种对抗性样本）如何导致其权重爆炸性增长，从而主导整个学习过程。通过模拟标准AdaBoost更新并与一种带有权重裁剪的防御机制进行对比，您将量化这种权重放大效应，并评估一种简单而实用的增强算法鲁棒性的方法。[@problem_id:3095556]", "problem": "自适应提升（AdaBoost）用于二元分类，它通过多轮迭代维护一个样本权重的分布，以使后续的弱学习器重点关注困难样本。考虑在第 $t$ 轮中，样本标签为 $y_i \\in \\{-1, +1\\}$，弱假设为 $h_t(x_i) \\in \\{-1, +1\\}$。该算法开始时，对于 $N$ 个样本，权重是均匀的，$w_1(i) = 1/N$。在每一轮，加权错误被定义为被错误分类样本的权重之和，即 $\\varepsilon_t = \\sum_{i=1}^N w_t(i) \\,\\mathbf{1}\\{y_i \\neq h_t(x_i)\\}$。弱学习器的贡献由一个系数 $\\alpha_t$ 进行缩放，并且权重分布通过指数规则进行更新：\n$$\nw_{t+1}(i) \\propto w_t(i)\\,\\exp\\!\\big(-\\alpha_t\\,y_i\\,h_t(x_i)\\big),\n$$\n随后进行归一化，以使 $\\sum_{i=1}^N w_{t+1}(i) = 1$。当一个样本受到对抗性扰动，导致 $h_t(x)$ 在多轮中符号翻转时，乘法因子会反复放大其权重。本问题要求您量化这种放大效应，并评估一种通过在归一化之前裁剪权重来削减极端损失贡献的防御方法。\n\n从上述核心定义出发，实现一个模拟器。在给定固定数据集和每轮预定的弱假设输出的情况下，该模拟器计算目标样本权重在两种机制下的增长情况：\n- 标准 AdaBoost 更新；以及\n- 一种削减损失的防御方法，其中归一化前的候选权重 $\\tilde{w}_{t+1}(i) = w_t(i)\\,\\exp\\!\\big(-\\alpha_t\\,y_i\\,h_t(x_i)\\big)$ 被一个固定的上限 $\\Lambda \\in (0,1)$ 裁剪为 $\\tilde{w}_{t+1}^{\\mathrm{clip}}(i) = \\min\\{\\tilde{w}_{t+1}(i), \\Lambda\\}$，然后重新归一化以使总和为 1。\n\n您的程序必须：\n- 使用包含 $N=4$ 个样本的数据集，其标签为 $y = [+1, +1, -1, -1]$。\n- 对于下方的每个测试用例，模拟 $T$ 轮，根据当前权重计算 $\\varepsilon_t$，设置 $\\alpha_t = \\tfrac{1}{2}\\ln\\!\\big(\\tfrac{1-\\varepsilon_t}{\\varepsilon_t}\\big)$，并按所述方式更新权重。\n- 对于一个目标索引 $s$，生成标准更新下的增长因子 $g_{\\mathrm{std}} = \\dfrac{w_{T+1}(s)}{w_1(s)}$ 和裁剪更新下的增长因子 $g_{\\mathrm{clip}} = \\dfrac{w^{\\mathrm{clip}}_{T+1}(s)}{w_1(s)}$（其中 $w_1(s) = 1/N$）。如果 $\\varepsilon_t$ 在数值上等于 $0$ 或 $1$，则分别将其视为 $\\varepsilon_t = 10^{-12}$ 或 $\\varepsilon_t = 1 - 10^{-12}$ 以保持数值稳定性。\n\n测试套件规格（每个用例提供 $T$、每轮对所有 $i$ 的假设输出 $h_t(x_i)$、裁剪上限 $\\Lambda$ 和目标索引 $s$）：\n\n- 用例 1（理想路径，偶尔的对抗性翻转）：$T=5$, $s=0$, $\\Lambda=0.2$，各轮次\n  - $t=1$: $[+1,+1,-1,+1]$\n  - $t=2$: $[+1,-1,-1,-1]$\n  - $t=3$: $[-1,+1,-1,-1]$\n  - $t=4$: $[+1,+1,+1,-1]$\n  - $t=5$: $[+1,+1,-1,+1]$\n- 用例 2（早期轮次中对目标进行严重的对抗性翻转）：$T=6$, $s=0$, $\\Lambda=0.2$，各轮次\n  - $t=1$: $[-1,+1,-1,-1]$\n  - $t=2$: $[-1,+1,-1,-1]$\n  - $t=3$: $[-1,+1,-1,-1]$\n  - $t=4$: $[-1,+1,-1,-1]$\n  - $t=5$: $[+1,+1,-1,+1]$\n  - $t=6$: $[+1,-1,-1,-1]$\n- 用例 3（目标保持正确；其他样本偶尔错误）：$T=4$, $s=0$, $\\Lambda=0.2$，各轮次\n  - $t=1$: $[+1,+1,-1,+1]$\n  - $t=2$: $[+1,-1,-1,-1]$\n  - $t=3$: $[+1,+1,+1,-1]$\n  - $t=4$: $[+1,+1,-1,+1]$\n- 用例 4（每轮有两个错误，加权错误接近边界值）：$T=6$, $s=0$, $\\Lambda=0.2$，各轮次\n  - $t=1$: $[-1,+1,+1,-1]$\n  - $t=2$: $[+1,-1,+1,-1]$\n  - $t=3$: $[-1,+1,-1,+1]$\n  - $t=4$: $[+1,-1,-1,+1]$\n  - $t=5$: $[-1,+1,+1,-1]$\n  - $t=6$: $[+1,-1,+1,-1]$\n\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表的结果，顺序为 $[g_{\\mathrm{std},1},g_{\\mathrm{clip},1},g_{\\mathrm{std},2},g_{\\mathrm{clip},2},g_{\\mathrm{std},3},g_{\\mathrm{clip},3},g_{\\mathrm{std},4},g_{\\mathrm{clip},4}]$，每个值四舍五入到六位小数。不涉及物理单位。不适用角度。不应使用百分比；所有量都应表示为实数。", "solution": "该问题是有效的。它在科学上基于统计学习的原理，特别是 AdaBoost 算法。它定义明确，提供了一套完整且一致的定义、数据和参数，以执行确定性模拟。目标清晰，需要一个虽非微不足道但可行的实现。\n\n解决方案将针对一个固定的数据集和预定的弱学习器输出，在一系列轮次中模拟 AdaBoost 算法。我们将计算样本权重在两种不同更新规则下的演变：标准 AdaBoost 更新和一种包含了权重裁剪作为对抗权重放大机制的修改版本。\n\nAdaBoost 的核心原理是通过组合弱学习器来顺序地构建一个强分类器。它通过在训练样本上维护一个权重分布来实现这一点。在每一轮中，训练一个新的弱学习器以最小化加权错误，从而有效地关注先前学习器错误分类的样本。然后，根据新的弱学习器是否正确分类每个样本来更新其权重。\n\n首先，我们建立初始状态。数据集有 $N=4$ 个样本，标签为 $y = [+1, +1, -1, -1]$。过程从第 $t=1$ 轮开始，采用均匀权重分布，其中每个样本 $i$ 的权重为 $w_1(i) = 1/N = 1/4 = 0.25$。我们将运行两个并行模拟，一个用于标准算法，一个用于裁剪变体。两者都以相同的初始权重开始。\n\n模拟迭代进行 $T$ 轮，从 $t=1$ 到 $t=T$。在每一轮 $t$ 中，我们对标准模拟和裁剪模拟都执行以下步骤，注意每次模拟的权重 $w_t^{\\mathrm{std}}(i)$ 和 $w_t^{\\mathrm{clip}}(i)$ 将在第一轮后独立演变。\n\n1.  **计算加权错误 ($\\varepsilon_t$)**：当前弱假设 $h_t$ 的加权错误是错误分类样本的权重之和。如果 $y_i \\neq h_t(x_i)$，或者等效地，如果乘积 $y_i h_t(x_i) = -1$，则样本 $i$ 被错误分类。\n    $$\n    \\varepsilon_t = \\sum_{i=1}^N w_t(i) \\,\\mathbf{1}\\{y_i \\neq h_t(x_i)\\}\n    $$\n    这个计算是分别对标准模拟和裁剪模拟使用它们各自的当前权重进行的，从而产生 $\\varepsilon_t^{\\mathrm{std}}$ 和 $\\varepsilon_t^{\\mathrm{clip}}$。为防止除以零或对数错误，如果 $\\varepsilon_t$ 在数值上为 $0$ 或 $1$，则分别将其限制在 $10^{-12}$ 或 $1 - 10^{-12}$。\n\n2.  **计算假设权重 ($\\alpha_t$)**：弱学习器 $h_t$ 对最终分类器的贡献由其权重 $\\alpha_t$ 决定。这是根据加权错误 $\\varepsilon_t$ 计算的。\n    $$\n    \\alpha_t = \\frac{1}{2}\\ln\\left(\\frac{1-\\varepsilon_t}{\\varepsilon_t}\\right)\n    $$\n    一个比随机猜测更好的学习器将有 $\\varepsilon_t  0.5$，这导致 $\\alpha_t > 0$。这个计算也是对每个模拟单独进行的，产生 $\\alpha_t^{\\mathrm{std}}$ 和 $\\alpha_t^{\\mathrm{clip}}$。\n\n3.  **更新样本权重**：该算法的核心在于更新样本权重。更新规则会乘性地增加错误分类样本的权重，并减少正确分类样本的权重。下一轮的未归一化权重 $\\tilde{w}_{t+1}(i)$ 是：\n    $$\n    \\tilde{w}_{t+1}(i) = w_t(i)\\,\\exp\\!\\big(-\\alpha_t\\,y_i\\,h_t(x_i)\\big)\n    $$\n    由于对于正确分类，$y_i h_t(x_i)$ 为 $+1$，对于错误分类为 $-1$，因此该规则简化为将权重乘以 $e^{\\alpha_t}$（对于错误）和乘以 $e^{-\\alpha_t}$（对于正确预测）。\n\n4.  **应用裁剪并归一化**：\n    *   **标准模拟**：未归一化的权重 $\\tilde{w}_{t+1}^{\\mathrm{std}}(i)$ 通过除以它们的总和 $Z_t^{\\mathrm{std}} = \\sum_{j=1}^N \\tilde{w}_{t+1}^{\\mathrm{std}}(j)$ 来进行归一化，以确保它们为下一轮形成一个有效的概率分布：\n        $$\n        w_{t+1}^{\\mathrm{std}}(i) = \\frac{\\tilde{w}_{t+1}^{\\mathrm{std}}(i)}{Z_t^{\\mathrm{std}}}\n        $$\n    *   **裁剪模拟**：未归一化的权重 $\\tilde{w}_{t+1}^{\\mathrm{clip}}(i)$ 首先被裁剪到一个最大值 $\\Lambda$。这一步限制了任何单个样本的影响。\n        $$\n        \\tilde{w}_{t+1}^{\\mathrm{post\\_clip}}(i) = \\min\\{\\tilde{w}_{t+1}^{\\mathrm{clip}}(i), \\Lambda\\}\n        $$\n        这些裁剪后的权重然后通过它们的总和 $Z_t^{\\mathrm{clip}} = \\sum_{j=1}^N \\tilde{w}_{t+1}^{\\mathrm{post\\_clip}}(j)$ 进行归一化：\n        $$\n        w_{t+1}^{\\mathrm{clip}}(i) = \\frac{\\tilde{w}_{t+1}^{\\mathrm{post\\_clip}}(i)}{Z_t^{\\mathrm{clip}}}\n        $$\n\n这个过程重复进行 $T$ 轮。在最后一轮之后，我们获得权重分布 $w_{T+1}^{\\mathrm{std}}$ 和 $w_{T+1}^{\\mathrm{clip}}$。\n\n最后，对于索引为 $s$ 的目标样本，我们计算增长因子。增长因子衡量样本权重从其初始均匀值 $w_1(s) = 1/N$ 开始的放大程度。\n$$\ng_{\\mathrm{std}} = \\frac{w_{T+1}^{\\mathrm{std}}(s)}{w_1(s)} \\quad \\text{和} \\quad g_{\\mathrm{clip}} = \\frac{w_{T+1}^{\\mathrm{clip}}(s)}{w_1(s)}\n$$\n这些值量化了裁剪防御在减轻目标样本权重爆炸方面的有效性。实现将为每个指定的测试用例执行这些步骤。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Simulates AdaBoost and a clipped-weight variant to compute weight growth factors.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1\n        (5,  # T\n         [[1, 1, -1, 1], [1, -1, -1, -1], [-1, 1, -1, -1], [1, 1, 1, -1], [1, 1, -1, 1]],  # h_t\n         0.2,  # Lambda\n         0),  # s\n        # Case 2\n        (6,  # T\n         [[-1, 1, -1, -1], [-1, 1, -1, -1], [-1, 1, -1, -1], [-1, 1, -1, -1], [1, 1, -1, 1], [1, -1, -1, -1]],  # h_t\n         0.2,  # Lambda\n         0),  # s\n        # Case 3\n        (4,  # T\n         [[1, 1, -1, 1], [1, -1, -1, -1], [1, 1, 1, -1], [1, 1, -1, 1]],  # h_t\n         0.2,  # Lambda\n         0),  # s\n        # Case 4\n        (6,  # T\n         [[-1, 1, 1, -1], [1, -1, 1, -1], [-1, 1, -1, 1], [1, -1, -1, 1], [-1, 1, 1, -1], [1, -1, 1, -1]],  # h_t\n         0.2,  # Lambda\n         0)  # s\n    ]\n\n    # Dataset specification\n    y = np.array([1, 1, -1, -1])\n    N = 4\n    \n    results = []\n    \n    for case in test_cases:\n        T, h_rounds_list, Lambda, s = case\n        h_rounds = np.array(h_rounds_list)\n\n        # Initialize weights for both standard and clipped simulations\n        w_std = np.full(N, 1.0 / N)\n        w_clip = np.full(N, 1.0 / N)\n\n        for t in range(T):\n            h_t = h_rounds[t]\n\n            # --- Standard AdaBoost update ---\n            # 1. Calculate weighted error\n            is_misclassified_std = (y != h_t)\n            eps_std = np.sum(w_std[is_misclassified_std])\n            eps_std = np.clip(eps_std, 1e-12, 1.0 - 1e-12)\n            \n            # 2. Calculate hypothesis weight\n            alpha_std = 0.5 * np.log((1.0 - eps_std) / eps_std)\n            \n            # 3. Update example weights\n            exponents_std = -alpha_std * y * h_t\n            w_tilde_std = w_std * np.exp(exponents_std)\n            \n            # 4. Normalize\n            w_std = w_tilde_std / np.sum(w_tilde_std)\n\n            # --- Clipped AdaBoost update ---\n            # 1. Calculate weighted error\n            is_misclassified_clip = (y != h_t)\n            eps_clip = np.sum(w_clip[is_misclassified_clip])\n            eps_clip = np.clip(eps_clip, 1e-12, 1.0 - 1e-12)\n\n            # 2. Calculate hypothesis weight\n            alpha_clip = 0.5 * np.log((1.0 - eps_clip) / eps_clip)\n            \n            # 3. Update example weights (unnormalized)\n            exponents_clip = -alpha_clip * y * h_t\n            w_tilde_clip = w_clip * np.exp(exponents_clip)\n\n            # 4. Apply clipping and then normalize\n            w_tilde_clip_clipped = np.minimum(w_tilde_clip, Lambda)\n            sum_clipped = np.sum(w_tilde_clip_clipped)\n            if sum_clipped > 0:\n                w_clip = w_tilde_clip_clipped / sum_clipped\n            else: # Handle case where all weights are clipped to 0\n                w_clip = np.full(N, 1.0 / N)\n\n        # Calculate final growth factors\n        w1_s = 1.0 / N\n        g_std = w_std[s] / w1_s\n        g_clip = w_clip[s] / w1_s\n        \n        results.append(f\"{g_std:.6f}\")\n        results.append(f\"{g_clip:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3095556"}, {"introduction": "在掌握了AdaBoost对孤立噪声点的敏感性之后，我们将挑战一个更系统化的问题：对抗性标签噪声。这项高级练习将引导您从第一性原理出发，为标准指数损失下的对抗策略进行推导，并设计一种基于Huberized指数损失的鲁棒AdaBoost变体。通过实现这两种算法并比较它们在对抗环境下的表现，您将把算法的鲁棒性与损失函数的数学性质直接联系起来，从而深入理解如何通过修改理论基础来构建更具韧性的机器学习模型。[@problem_id:3095542]", "problem": "您需要研究对抗性标签噪声对自适应提升（AdaBoost）算法的影响，并实现一个使用Huber化指数损失的鲁棒变体。您必须从第一性原理出发，推导出关键的更新步骤，然后在单个程序中实现这两种方法，以比较它们在对抗性条件下的行为。\n\n设定为标签集在 $\\{-1,+1\\}$ 内的二元分类问题。设 $\\{(x_i,y_i)\\}_{i=1}^n$ 为一个训练集，其中 $x_i \\in \\mathbb{R}$ 且 $y_i \\in \\{-1,+1\\}$。考虑一个分步加性模型 $F_t(x) = F_{t-1}(x) + \\alpha_t h_t(x)$，其中 $F_0(x) \\equiv 0$，$h_t$ 是一个输出在 $\\{-1,+1\\}$ 内的决策树桩，$\\alpha_t \\in \\mathbb{R}$ 是一个步长。\n\n在每一轮 $t$，我们根据所选凸代理损失函数关于间隔 $z_i = y_i F_{t-1}(x_i)$ 的负导数，定义一个在训练样本上的经验分布 $D_t$。对于指数损失 $\\phi(z) = \\exp(-z)$，这会得到 $D_t(i) \\propto \\exp(-y_i F_{t-1}(x_i))$。对于下面定义的Huber化指数损失 $\\phi_\\delta(z)$，该分布类似地使用 $-\\phi_\\delta'(z_i)$。\n\n您将研究一个对抗者，在每一轮 $t$，该对抗者被允许翻转最多比例为 $\\rho \\in [0,1/2]$ 的标签，以最大化该轮在指数损失下的归一化因子。形式上，在学习器使用干净标签和分布 $D_t$ 选择 $h_t$ 后，对抗者输出一个标签向量 $y^{(t)} \\in \\{-1,+1\\}^n$，使得 $y^{(t)}$ 与干净标签 $y$ 之间的汉明距离最多为 $\\lfloor \\rho n \\rfloor$，并且对于该轮固定的分类器 $h_t$ 和分布 $D_t$，$y^{(t)}$ 能最大化归一化因子\n$$\nZ_t(\\alpha) \\;=\\; \\sum_{i=1}^n D_t(i)\\,\\exp\\!\\big(-\\alpha\\, y^{(t)}_i\\, h_t(x_i)\\big)\n$$\n通过选择翻转哪些标签来实现。然后，学习器根据所选的损失函数和对抗性标签 $y^{(t)}$ 来选择 $\\alpha_t$，并更新 $F_t$。\n\n您必须：\n\n- 从第一性原理出发，从经验风险与凸代理以及分步加性模型的定义开始，推导出以下部分。\n\n  1) 对于指数损失，展示该轮的归一化因子 $Z_t$ 如何依赖于加权分类误差 $\\varepsilon_t = \\sum_{i=1}^n D_t(i)\\,\\mathbf{1}[y^{(t)}_i \\neq h_t(x_i)]$（对于固定的分布 $D_t$ 和分类器 $h_t$）。利用这一点，确定在最多可以翻转 $\\lfloor \\rho n \\rfloor$ 个标签的约束下，对抗者的最优翻转策略，并且学习器会通过可能翻转 $h_t$ 的符号来强制执行弱学习条件，以确保 $\\varepsilon_t \\leq 1/2$。\n\n  2) 对于指数损失，推导出在方向 $h_t$ 上使用对抗性标签 $y^{(t)}$ 最小化经验指数风险的步长 $\\alpha_t$。\n\n  3) 提出一个带参数 $\\delta > 0$ 的Huber化指数损失，定义如下\n  $$\n  \\phi_\\delta(z) =\n  \\begin{cases}\n  \\exp(-z),  \\text{if } z \\geq -\\delta, \\\\\n  \\exp(\\delta) - \\exp(\\delta)(z+\\delta),  \\text{if } z  -\\delta.\n  \\end{cases}\n  $$\n  证明 $-\\phi_\\delta'(z)$ 是一个截断的重要性权重，并推导出在最小化 $\\sum_{i=1}^n \\phi_\\delta\\!\\big(y^{(t)}_i (F_{t-1}(x_i) + \\alpha h_t(x_i))\\big)$ 关于 $\\alpha$ 时，$\\alpha_t$ 的一维最优性条件。提供一个数值稳定的过程（例如，在导数上使用区间法加二分法）来找到 $\\alpha_t$。\n\n- 实现共享同一个弱学习器的两种算法：\n\n  a) 一个标准的AdaBoost变体，在每一轮中使用指数损失从干净标签 $y$ 计算 $D_t(i) \\propto \\exp(-y_i F_{t-1}(x_i))$，在 $(x_i,y_i)$ 上训练一个加权决策树桩 $h_t$，让对抗者通过最优地翻转最多 $\\lfloor \\rho n \\rfloor$ 个标签来生成 $y^{(t)}$，以最大化该轮 $h_t$ 在 $D_t$ 下的归一化因子，然后使用 $y^{(t)}$ 计算指数损失的 $\\alpha_t$，并更新 $F_t$。\n\n  b) 一个鲁棒的变体，它使用带参数 $\\delta > 0$ 的Huber化指数损失从干净标签 $y$ 计算分布 $D_t(i) \\propto -\\phi_\\delta'(y_i F_{t-1}(x_i))$，在 $(x_i,y_i)$ 上训练相同的加权树桩，让同一个对抗者翻转标签生成 $y^{(t)}$（对抗者仍然以指数归一化因子为目标，而不是Huber化的那个），通过最小化沿 $h_t$ 方向使用 $y^{(t)}$ 的Huber化经验风险来计算 $\\alpha_t$，然后更新 $F_t$。\n\n- 使用一个固定的、确定性的数据集，大小为 $n=60$，由在区间 $[-1,1]$ 上等距分布的 $x_i$ 和 $y_i = \\operatorname{sign}(x_i)$ 定义，并约定 $\\operatorname{sign}(0)=+1$。使用一个由实线上的阈值和 $\\{-1,+1\\}$ 中的极性定义的决策树桩类，其预测规则为 $h(x) = s \\cdot \\operatorname{sign}(x - \\theta)$，其中平局使用 $+1$。所有实验都训练 $T=20$ 轮。\n\n- 对抗者在每一轮都按如下方式操作：给定当前分布 $D_t$（根据干净标签 $y$ 和所选损失计算）和选定的树桩 $h_t$（使用干净标签训练），对抗者输出 $y^{(t)}$，通过翻转 $y$ 中最多 $\\lfloor \\rho n \\rfloor$ 个条目，这些条目被选择以最大化该轮的指数归一化因子，同时受到学习器将通过在需要时翻转 $h_t$ 的符号来强制执行弱学习条件以确保加权误差最多为 $1/2$ 的约束。因此，对抗者不得将加权误差推高到 $1/2$ 以上。\n\n- 为了评估，在 $T$ 轮之后，必须在干净标签 $y$ 上使用最终分类器 $\\operatorname{sign}(F_T(x))$ 计算经验分类误差，其中平局算作 $+1$。将误差报告为 $[0,1]$ 内的实数。\n\n实现一个单一程序，运行以下参数三元组 $(\\rho, \\text{loss}, \\delta)$ 的测试套件，并输出 $T$ 轮后得到的最终干净标签误差（浮点数）：\n\n- 测试用例 1: $(0.0,\\ \\text{exp},\\ 0.0)$\n- 测试用例 2: $(0.3,\\ \\text{exp},\\ 0.0)$\n- 测试用例 3: $(0.3,\\ \\text{huber},\\ 0.5)$\n- 测试用例 4: $(0.45,\\ \\text{huber},\\ 0.5)$\n- 测试用例 5: $(0.45,\\ \\text{exp},\\ 0.0)$\n\n您的程序应生成单行输出，其中包含五个结果，以逗号分隔并用方括号括起来（例如，$[0.0000,0.1000,0.0500,0.1000,0.2000]$）。不允许有其他输出。所有数字必须打印为小数点后恰好有四位的小数。\n\n所有角度（如有）都以弧度为单位，但不需要进行角度计算。不涉及物理单位。每个测试用例的最终答案是 $[0,1]$ 内的浮点数。", "solution": "该问题要求对标准和鲁棒的AdaBoost在对抗性标签噪声下的更新规则进行理论推导，然后进行比较性实现。\n\n### 第1步：推导\n\n设训练集为 $\\{(x_i, y_i)\\}_{i=1}^n$，其中 $y_i \\in \\{-1, +1\\}$。分步加性模型为 $F_t(x) = F_{t-1}(x) + \\alpha_t h_t(x)$，其中 $F_0(x) = 0$。第 $t$ 轮的样本权重由一个分布 $D_t(i)$ 给出，该分布从一个凸损失函数 $\\phi$ 推导而来。令 $z_i = y_i F_{t-1}(x_i)$ 为样本 $i$ 关于干净标签 $y_i$ 的间隔。权重为 $D_t(i) \\propto -\\phi'(z_i)$。在弱学习器 $h_t$ 使用干净数据 $(x, y)$ 和权重 $D_t$ 进行训练后，一个对抗者引入对抗性标签 $y^{(t)}$。\n\n#### 1. 对抗者的最优策略\n\n对抗者的目标是通过翻转最多 $k = \\lfloor \\rho n \\rfloor$ 个标签来最大化指数损失的归一化因子 $Z_t(\\alpha)$。学习器使用步长 $\\alpha_t > 0$（因为强制执行了弱学习条件）。\n$$\nZ_t(\\alpha) = \\sum_{i=1}^n D_t(i)\\,\\exp(-\\alpha\\, y^{(t)}_i\\, h_t(x_i))\n$$\n乘积 $y^{(t)}_i h_t(x_i)$ 对于 $h_t$ 在对抗性标签 $y_i^{(t)}$ 上的正确分类为 $+1$，对于不正确分类为 $-1$。我们可以将求和分为正确分类和错误分类的样本：\n$$\nZ_t(\\alpha) = \\sum_{i: y^{(t)}_i = h_t(x_i)} D_t(i)\\,e^{-\\alpha} + \\sum_{i: y^{(t)}_i \\neq h_t(x_i)} D_t(i)\\,e^{\\alpha}\n$$\n令 $\\varepsilon_t = \\sum_{i=1}^n D_t(i)\\,\\mathbf{1}[y^{(t)}_i \\neq h_t(x_i)]$ 为 $h_t$ 关于对抗性标签 $y^{(t)}$ 和分布 $D_t$ 的加权分类误差。由于 $\\sum_{i=1}^n D_t(i) = 1$，正确分类样本的权重之和为 $1-\\varepsilon_t$。归一化因子变为：\n$$\nZ_t(\\alpha) = (1-\\varepsilon_t)e^{-\\alpha} + \\varepsilon_t e^{\\alpha}\n$$\n为了在固定的 $\\alpha > 0$ 下最大化 $Z_t(\\alpha)$，我们分析其关于 $\\varepsilon_t$ 的导数：\n$$\n\\frac{\\partial Z_t(\\alpha)}{\\partial \\varepsilon_t} = e^{\\alpha} - e^{-\\alpha}\n$$\n对于 $\\alpha > 0$，我们有 $e^{\\alpha} > 1$ 且 $e^{-\\alpha}  1$，所以 $e^{\\alpha} - e^{-\\alpha} > 0$。因此，$Z_t(\\alpha)$ 是 $\\varepsilon_t$ 的一个单调递增函数。对抗者的最优策略是最大化加权误差 $\\varepsilon_t$。\n\n对抗者从干净标签 $y$ 开始，通过翻转标签来生成 $y^{(t)}$。令 $h_t$ 为学习器选择的弱学习器（在干净标签上强制执行弱学习条件后）。令 $I_{corr} = \\{i \\mid y_i = h_t(x_i)\\}$ 为 $h_t$ 在干净标签上正确分类的点的集合，令 $I_{incorr} = \\{i \\mid y_i \\neq h_t(x_i)\\}$ 为错误分类的点的集合。初始时，误差为 $\\varepsilon_{t,clean} = \\sum_{i \\in I_{incorr}} D_t(i)$。\n- 如果对抗者翻转一个点 $i \\in I_{corr}$ 的标签，该点会变为被错误分类，误差增加 $D_t(i)$。\n- 如果对抗者翻转一个点 $i \\in I_{incorr}$ 的标签，该点会变为被正确分类，误差减少 $D_t(i)$。\n\n为了最大化 $\\varepsilon_t$，对抗者应只翻转 $I_{corr}$ 中点的标签。为实现误差的最大增量，对抗者应贪婪地翻转 $I_{corr}$ 中对应于最大权重 $D_t(i)$ 的点的标签。\n\n对抗者受到两个约束：\n1.  翻转次数最多为 $k = \\lfloor \\rho n \\rfloor$。\n2.  最终的对抗性误差必须满足 $\\varepsilon_t \\le 1/2$。如果 $\\varepsilon_t > 1/2$，学习器会翻转 $h_t$ 的符号，导致新的误差为 $1-\\varepsilon_t  1/2$。这会减少误差，从而挫败对抗者的目标。\n\n因此，对抗者的策略如下：\n1.  识别正确分类点的集合 $I_{corr}$ 及其权重 $D_t(i)$。\n2.  按权重的降序对这些点进行排序。\n3.  遍历排序后的列表，逐个翻转标签，直到已进行 $k$ 次翻转，或者翻转下一个标签会导致总加权误差超过 $1/2$。\n\n#### 2. 指数损失的步长\n\n学习器选择 $\\alpha_t$ 以最小化使用对抗性标签 $y^{(t)}$ 的经验风险。对于指数损失，这等同于最小化 $Z_t(\\alpha)$：\n$$\n\\alpha_t = \\arg\\min_{\\alpha > 0} \\left( (1-\\varepsilon_t)e^{-\\alpha} + \\varepsilon_t e^{\\alpha} \\right)\n$$\n我们将关于 $\\alpha$ 的导数设为零：\n$$\n\\frac{d}{d\\alpha} \\left( (1-\\varepsilon_t)e^{-\\alpha} + \\varepsilon_t e^{\\alpha} \\right) = -(1-\\varepsilon_t)e^{-\\alpha} + \\varepsilon_t e^{\\alpha} = 0\n$$\n$$\n\\varepsilon_t e^{\\alpha} = (1-\\varepsilon_t)e^{-\\alpha} \\implies e^{2\\alpha} = \\frac{1-\\varepsilon_t}{\\varepsilon_t}\n$$\n解出 $\\alpha$ 得到步长：\n$$\n\\alpha_t = \\frac{1}{2} \\ln\\left(\\frac{1 - \\varepsilon_t}{\\varepsilon_t}\\right)\n$$\n这对 $\\varepsilon_t \\in (0, 1)$ 有效。弱学习条件 $\\varepsilon_t \\le 1/2$ 确保了 $\\alpha_t \\ge 0$。\n\n#### 3. Huber化指数损失\n\nHuber化指数损失定义为：\n$$\n\\phi_\\delta(z) =\n\\begin{cases}\n\\exp(-z),  \\text{if } z \\geq -\\delta, \\\\\n\\exp(\\delta) - \\exp(\\delta)(z+\\delta),  \\text{if } z  -\\delta.\n\\end{cases}\n$$\n该函数在 $z=-\\delta$ 处是连续的，因为 $\\exp(-(-\\delta)) = \\exp(\\delta)$ 且 $\\exp(\\delta) - \\exp(\\delta)(-\\delta+\\delta) = \\exp(\\delta)$。其导数为：\n$$\n\\phi'_\\delta(z) =\n\\begin{cases}\n-\\exp(-z),  \\text{if } z > -\\delta, \\\\\n-\\exp(\\delta),  \\text{if } z  -\\delta.\n\\end{cases}\n$$\n导数在 $z=-\\delta$ 处也是连续的，值为 $-\\exp(\\delta)$。用于权重计算的负导数为：\n$$\n-\\phi'_\\delta(z) =\n\\begin{cases}\n\\exp(-z),  \\text{if } z > -\\delta, \\\\\n\\exp(\\delta),  \\text{if } z  -\\delta.\n\\end{cases}\n$$\n这可以紧凑地写为 $-\\phi'_\\delta(z) = \\min(\\exp(-z), \\exp(\\delta))$。这表明 $-\\phi'_\\delta(z)$ 是指数权重 $\\exp(-z)$ 的一个“截断”版本，权重被限制在最大值 $\\exp(\\delta)$。这可以防止少数具有非常大负间隔的点主导权重分布，从而赋予鲁棒性。\n\n为了找到步长 $\\alpha_t$，我们使用Huber化损失和对抗性标签 $y^{(t)}$ 最小化总经验风险：\n$$\nJ(\\alpha) = \\sum_{i=1}^n \\phi_\\delta\\big(y^{(t)}_i (F_{t-1}(x_i) + \\alpha h_t(x_i))\\big)\n$$\n令 $z'_i = y^{(t)}_i F_{t-1}(x_i)$ 和 $m_i = y^{(t)}_i h_t(x_i)$。我们希望最小化 $\\sum_i \\phi_\\delta(z'_i + \\alpha m_i)$。我们将关于 $\\alpha$ 的导数设为零：\n$$\n\\frac{dJ}{d\\alpha} = \\sum_{i=1}^n m_i \\phi'_\\delta(z'_i + \\alpha m_i) = 0\n$$\n由于 $\\phi_\\delta$ 是凸函数，这个条件足以得到最小值。令 $C = \\{i \\mid m_i = 1\\}$ 和 $M = \\{i \\mid m_i = -1\\}$ 分别为 $h_t$ 在标签 $y^{(t)}$ 上正确分类和错误分类的索引集。方程变为：\n$$\n\\sum_{i \\in C} \\phi'_\\delta(z'_i + \\alpha) + \\sum_{i \\in M} (-1) \\phi'_\\delta(z'_i - \\alpha) = 0\n$$\n$$\n\\sum_{i \\in C} (-\\phi'_\\delta(z'_i + \\alpha)) = \\sum_{i \\in M} (-\\phi'_\\delta(z'_i - \\alpha))\n$$\n代入 $-\\phi'_\\delta(z)$ 的表达式：\n$$\n\\sum_{i \\in C} \\min(\\exp(-(z'_i + \\alpha)), \\exp(\\delta)) = \\sum_{i \\in M} \\min(\\exp(-(z'_i - \\alpha)), \\exp(\\delta))\n$$\n这是一个关于 $\\alpha$ 的非线性方程。令需要求零点的函数为\n$$\nG(\\alpha) = \\sum_{i \\in M} \\min(\\exp(-z'_i + \\alpha), \\exp(\\delta)) - \\sum_{i \\in C} \\min(\\exp(-z'_i - \\alpha), \\exp(\\delta))\n$$\n函数 $G(\\alpha)$ 对于 $\\alpha \\ge 0$ 是单调非递减的。我们可以使用像二分法这样的数值方法找到根 $\\alpha_t$。\n\n**$\\alpha_t$ 的数值求解过程（Huber化损失）：**\n1.  如上定义函数 $G(\\alpha)$。\n2.  检查在 $\\alpha=0$ 处的值。如果 $G(0) \\geq 0$，则最小值在 $\\alpha_t=0$ 处，因为增加 $\\alpha$ 不会使 $G(\\alpha)$ 减小。\n3.  如果 $G(0)  0$，则存在一个正根。我们需要找到一个搜索的上界。我们可以从一个猜测开始，例如 $\\alpha_{high}=1.0$，然后将其加倍直到 $G(\\alpha_{high}) > 0$。设下界为 $\\alpha_{low}=0$。\n4.  在区间 $[\\alpha_{low}, \\alpha_{high}]$ 上应用二分法：\n    a. 计算 $\\alpha_{mid} = (\\alpha_{low} + \\alpha_{high})/2$。\n    b. 如果 $G(\\alpha_{mid})  0$，则根在区间的上半部分：设置 $\\alpha_{low} = \\alpha_{mid}$。\n    c. 如果 $G(\\alpha_{mid}) \\geq 0$，则根在区间的下半部分：设置 $\\alpha_{high} = \\alpha_{mid}$。\n5.  重复固定次数的迭代（例如100次）或直到区间宽度足够小。得到的 $\\alpha_{mid}$ 就是我们的 $\\alpha_t$。\n\n这样就完成了所需的推导。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main solver function that runs the test suite and prints the final results.\n    \"\"\"\n    \n    # --- Problem Setup ---\n    n = 60\n    T = 20\n    x = np.linspace(-1, 1, n)\n    y = np.sign(x)\n    y[y == 0] = 1 # Per problem spec, sign(0) = +1\n\n    # Define a consistent sign function for tie-breaking\n    def sign_with_tiebreak(arr):\n        s = np.sign(arr)\n        s[s == 0] = 1\n        return s\n\n    # --- Weak Learner: Decision Stump ---\n    def find_best_stump(x, y, D):\n        \"\"\"Finds the best decision stump (threshold, polarity) for weighted data.\"\"\"\n        best_err = np.inf\n        best_theta = 0\n        best_s = 1\n        \n        # Define potential thresholds\n        thresholds = (x[:-1] + x[1:]) / 2\n        \n        for theta in thresholds:\n            # Polarity s = +1\n            s = 1\n            h_pred = s * sign_with_tiebreak(x - theta)\n            err = D @ (h_pred != y)\n            \n            if err  best_err:\n                best_err = err\n                best_theta = theta\n                best_s = s\n\n            # Polarity s = -1 (error is 1 - err for s=+1)\n            if 1 - err  best_err:\n                best_err = 1 - err\n                best_theta = theta\n                best_s = -s\n                \n        # The learner enforces weak learning by choosing min(err, 1-err),\n        # so best_err is guaranteed to be = 0.5.\n        return best_theta, best_s, best_err\n\n    # --- Adversary ---\n    def adversary_flip(y_clean, h_pred, D, rho, n):\n        \"\"\"Flips labels to maximize weighted error.\"\"\"\n        y_adv = np.copy(y_clean)\n        if rho == 0:\n            return y_adv\n        \n        k = int(np.floor(rho * n))\n        \n        # Initial error on clean labels\n        err_adv = D @ (y_adv != h_pred)\n        \n        # Identify correctly classified points on clean labels\n        # These are the candidates for flipping to increase error\n        corr_indices = np.where(y_clean == h_pred)[0]\n        \n        # Sort these candidates by their weights in descending order\n        candidate_weights = D[corr_indices]\n        sorted_candidates = corr_indices[np.argsort(-candidate_weights)]\n        \n        flips_done = 0\n        for idx in sorted_candidates:\n            if flips_done >= k:\n                break\n            \n            # Check if flipping this label violates the error = 0.5 constraint\n            if err_adv + D[idx] = 0.5:\n                # Perform the flip\n                y_adv[idx] *= -1\n                err_adv += D[idx]\n                flips_done += 1\n            else:\n                # Flipping this would push error > 0.5, so we stop.\n                # The adversary will not do this as the learner would counter it.\n                break\n                \n        return y_adv\n\n    # --- Huberized Alpha Calculation ---\n    def find_alpha_huber(y_adv, F_prev, h_pred, delta, n):\n        \"\"\"Finds alpha_t for Huberized loss using bisection.\"\"\"\n        z_prime = y_adv * F_prev\n        m = y_adv * h_pred\n        \n        exp_delta = np.exp(delta)\n\n        C_indices = np.where(m == 1)[0]\n        M_indices = np.where(m == -1)[0]\n\n        def G(alpha):\n            if alpha  0: return -np.inf # We search for alpha >= 0\n            \n            term_C = np.sum(np.minimum(np.exp(-(z_prime[C_indices] + alpha)), exp_delta))\n            term_M = np.sum(np.minimum(np.exp(-(z_prime[M_indices] - alpha)), exp_delta))\n            \n            return term_M - term_C\n\n        # Check if alpha=0 is already the solution\n        if G(0) >= 0:\n            return 0.0\n\n        # Find an upper bound for the search\n        alpha_low = 0.0\n        alpha_high = 1.0\n        while G(alpha_high)  0:\n            alpha_high *= 2.0\n            if alpha_high > 1e6: # Safety break\n                return 0.0\n\n        # Bisection\n        for _ in range(100): # 100 iterations is more than enough\n            alpha_mid = (alpha_low + alpha_high) / 2\n            if G(alpha_mid)  0:\n                alpha_low = alpha_mid\n            else:\n                alpha_high = alpha_mid\n        \n        return (alpha_low + alpha_high) / 2\n        \n    # --- Main Boosting Algorithm ---\n    def run_boosting(rho, loss_type, delta, x, y, n, T):\n        \"\"\"Runs the boosting algorithm for a given configuration.\"\"\"\n        F = np.zeros(n)\n        exp_delta = np.exp(delta) if delta > 0 else 0\n        \n        for t in range(T):\n            # 1. Compute weights D_t\n            if loss_type == 'exp':\n                w = np.exp(-y * F)\n            elif loss_type == 'huber':\n                z = y * F\n                w = np.minimum(np.exp(-z), exp_delta)\n            \n            D = w / np.sum(w)\n            \n            # 2. Train weak learner h_t on clean data\n            theta, s, _ = find_best_stump(x, y, D)\n            h_pred = s * sign_with_tiebreak(x - theta)\n            \n            # 3. Adversary acts\n            y_adv = adversary_flip(y, h_pred, D, rho, n)\n            \n            # 4. Compute step size alpha_t\n            if loss_type == 'exp':\n                eps_t = D @ (y_adv != h_pred)\n                # Add small epsilon for numerical stability\n                eps_t = np.clip(eps_t, 1e-10, 1 - 1e-10)\n                alpha = 0.5 * np.log((1 - eps_t) / eps_t)\n            elif loss_type == 'huber':\n                alpha = find_alpha_huber(y_adv, F, h_pred, delta, n)\n\n            # 5. Update model\n            F += alpha * h_pred\n        \n        # Evaluate final classifier on clean labels\n        y_hat = sign_with_tiebreak(F)\n        final_err = np.mean(y_hat != y)\n        \n        return final_err\n\n    # --- Run Test Suite ---\n    test_cases = [\n        (0.0, 'exp', 0.0),   # Test case 1\n        (0.3, 'exp', 0.0),   # Test case 2\n        (0.3, 'huber', 0.5), # Test case 3\n        (0.45, 'huber', 0.5),# Test case 4\n        (0.45, 'exp', 0.0)   # Test case 5\n    ]\n\n    results = []\n    for rho, loss, delta in test_cases:\n        error = run_boosting(rho, loss, delta, x, y, n, T)\n        results.append(f\"{error:.4f}\")\n\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3095542"}]}