{"hands_on_practices": [{"introduction": "极限梯度提升（XGBoost）的强大之处在于其独特的正则化目标函数，它使用损失函数的二阶泰勒展开进行优化。这项练习是基础性的，因为它要求您为常见的多分类场景推导出该展开式的核心组成部分——梯度（一阶导数）和海森矩阵（二阶导数）[@problem_id:3120267]。通过完成这个推导并将其应用于一个简单的数据集，您将具体理解损失函数的曲率如何指导模型在每一步的学习过程。", "problem": "考虑 eXtreme Gradient Boosting (XGBoost)，该算法通过将决策树拟合到损失函数的二阶泰勒展开来优化正则化目标。对于具有 $K$ 个类别的多分类问题，XGBoost 使用 softmax 模型，其中样本 $i$ 和类别 $k$ 的类别分数为 $f_{ik}$，概率由 softmax 变换定义为 $p_{ik} = \\frac{\\exp(f_{ik})}{\\sum_{j=1}^{K} \\exp(f_{ij})}$。每个样本的损失是在 softmax 概率下真实类别的负对数似然。\n\n仅从这些定义出发，推导损失函数关于分数 $f_{ik}$ 的每个样本的梯度 $g_{ik}$ 和每个样本的二阶导数对角线元素（Hessian 矩阵的对角线）$h_{ik}$ 的表达式。\n\n然后，在下面的 3 分类玩具数据集上测试您的推导。在单个叶子节点中有 $N=3$ 个训练样本，其类别标签为\n$$\ny_1 = 1,\\quad y_2 = 1,\\quad y_3 = 3.\n$$\n假设当前的提升迭代对所有类别分数使用零初始化，因此对于所有的 $i$ 和 $k$，$f_{ik} = 0$。设 $\\ell_2$ 正则化参数为 $\\lambda = \\frac{1}{2}$。\n\n使用 XGBoost 为类别 $k=2$ 采用的二阶叶权重解，计算最优叶权重\n$$\nw^{\\ast} = -\\frac{\\sum_{i=1}^{N} g_{i2}}{\\sum_{i=1}^{N} h_{i2} + \\lambda}.\n$$\n\n以单个数字的形式报告 $w^{\\ast}$ 的值。无需四舍五入。", "solution": "我们从多分类逻辑（softmax）建模的基本定义开始。对于一个具有分数 $\\mathbf{f}_i = (f_{i1}, \\dots, f_{iK})$ 的样本 $i$，类别 $k$ 的 softmax 概率为\n$$\np_{ik} = \\frac{\\exp(f_{ik})}{\\sum_{j=1}^{K} \\exp(f_{ij})}.\n$$\n对于样本 $i$ 和真实类别 $y_i$，其负对数似然（交叉熵）损失为\n$$\nL_i(\\mathbf{f}_i) = -\\ln\\left(p_{i, y_i}\\right) = -\\sum_{k=1}^{K} \\mathbf{1}[y_i = k] \\ln(p_{ik}),\n$$\n其中 $\\mathbf{1}[\\cdot]$ 是指示函数。\n\n我们使用链式法则推导梯度 $\\frac{\\partial L_i}{\\partial f_{ik}}$。首先注意到 $L_i = -\\sum_{k} y_{ik} \\ln(p_{ik})$，其中 $y_{ik} = \\mathbf{1}[y_i=k]$。$\\ln(p_{ik})$ 关于 $f_{il}$ 的导数使用 softmax 的导数。softmax 的雅可比矩阵是众所周知的，可以如下推导：\n$$\np_{ik} = \\frac{\\exp(f_{ik})}{Z_i},\\quad \\text{其中}\\quad Z_i = \\sum_{j=1}^{K} \\exp(f_{ij}).\n$$\n那么\n$$\n\\frac{\\partial p_{ik}}{\\partial f_{il}} = \\frac{\\exp(f_{ik})}{Z_i}\\left(\\delta_{kl} - \\frac{\\exp(f_{il})}{Z_i}\\right) = p_{ik}(\\delta_{kl} - p_{il}),\n$$\n其中 $\\delta_{kl}$ 是克罗内克 δ。因此，\n$$\n\\frac{\\partial \\ln(p_{ik})}{\\partial f_{il}} = \\frac{1}{p_{ik}}\\frac{\\partial p_{ik}}{\\partial f_{il}} = \\delta_{kl} - p_{il}.\n$$\n所以，$L_i$ 关于 $f_{il}$ 的梯度是\n$$\n\\frac{\\partial L_i}{\\partial f_{il}} = -\\sum_{k=1}^{K} y_{ik}\\left(\\delta_{kl} - p_{il}\\right) = -y_{il} + \\left(\\sum_{k=1}^{K} y_{ik}\\right)p_{il}.\n$$\n由于 $\\sum_{k=1}^{K} y_{ik} = 1$，我们有\n$$\n\\frac{\\partial L_i}{\\partial f_{il}} = p_{il} - y_{il} = p_{il} - \\mathbf{1}[y_i=l].\n$$\n为了清晰起见，将索引 $l$ 重命名为 $k$，每个样本的梯度为\n$$\ng_{ik} = \\frac{\\partial L_i}{\\partial f_{ik}} = p_{ik} - \\mathbf{1}[y_i = k].\n$$\n\n接下来，我们推导 Hessian 矩阵。关于 $\\mathbf{f}_i$ 的 Hessian 矩阵是一个矩阵，其元素为\n$$\n\\frac{\\partial^2 L_i}{\\partial f_{ik}\\,\\partial f_{il}} = \\frac{\\partial}{\\partial f_{il}}\\left(p_{ik} - y_{ik}\\right) = \\frac{\\partial p_{ik}}{\\partial f_{il}} = p_{ik}\\left(\\delta_{kl} - p_{il}\\right).\n$$\n因此，样本 $i$ 的完整 Hessian 矩阵为\n$$\nH_i = \\operatorname{diag}(\\mathbf{p}_i) - \\mathbf{p}_i \\mathbf{p}_i^{\\top},\n$$\n其中 $\\mathbf{p}_i = (p_{i1},\\dots,p_{iK})$。对角线元素为\n$$\n\\frac{\\partial^2 L_i}{\\partial f_{ik}^2} = p_{ik}(1 - p_{ik}).\n$$\n在 XGBoost 的按类别建树过程中，用于类别 $k$ 的二阶项是该类别的对角线元素，因此每个样本的二阶导数对角线元素为\n$$\nh_{ik} = p_{ik}(1 - p_{ik}).\n$$\n\n我们现在在指定的 3 分类数据集上进行测试，该数据集有 $N=3$ 个样本，标签为 $y_1=1$, $y_2=1$, $y_3=3$。当前的分数为对所有 $i$ 和 $k$ 都有 $f_{ik}=0$。在此初始化下，\n$$\np_{ik} = \\frac{\\exp(0)}{\\exp(0)+\\exp(0)+\\exp(0)} = \\frac{1}{3}\\quad \\text{对所有 } i,k.\n$$\n\n计算类别 $k=2$ 的每个样本的梯度：\n- 对于样本 $i=1$，$y_1=1 \\neq 2$，所以 $\\mathbf{1}[y_1=2]=0$ 且 $g_{12} = \\frac{1}{3} - 0 = \\frac{1}{3}$。\n- 对于样本 $i=2$，$y_2=1 \\neq 2$，所以 $g_{22} = \\frac{1}{3} - 0 = \\frac{1}{3}$。\n- 对于样本 $i=3$，$y_3=3 \\neq 2$，所以 $g_{32} = \\frac{1}{3} - 0 = \\frac{1}{3}$。\n\n在叶子节点上聚合类别 2 的梯度：\n$$\nG_2 = \\sum_{i=1}^{3} g_{i2} = \\frac{1}{3} + \\frac{1}{3} + \\frac{1}{3} = 1.\n$$\n\n计算类别 $k=2$ 的每个样本的二阶导数对角线元素：\n$$\nh_{i2} = p_{i2}(1 - p_{i2}) = \\frac{1}{3}\\left(1 - \\frac{1}{3}\\right) = \\frac{1}{3}\\cdot \\frac{2}{3} = \\frac{2}{9}.\n$$\n在叶子节点上聚合类别 2 的 Hessian：\n$$\nH_2 = \\sum_{i=1}^{3} h_{i2} = 3 \\cdot \\frac{2}{9} = \\frac{6}{9} = \\frac{2}{3}.\n$$\n\n当 $\\lambda = \\frac{1}{2}$ 时，类别 2 的二阶最优叶权重为\n$$\nw^{\\ast} = -\\frac{G_2}{H_2 + \\lambda} = -\\frac{1}{\\frac{2}{3} + \\frac{1}{2}} = -\\frac{1}{\\frac{4}{6} + \\frac{3}{6}} = -\\frac{1}{\\frac{7}{6}} = -\\frac{6}{7}.\n$$\n\n这是一个单一的精确有理数，因此无需四舍五入。", "answer": "$$\\boxed{-\\frac{6}{7}}$$", "id": "3120267"}, {"introduction": "在我们掌握了梯度和海森矩阵之后，算法必须决定继续生长树是否值得。这项练习深入探讨了 XGBoost 正则化策略的核心，重点关注模型如何在拟合数据与控制复杂性之间取得平衡[@problem_id:3120320]。通过推导出一个分裂被拒绝的条件，您将学会理解复杂度惩罚项 $\\gamma$ 在防止过拟合和构建更具泛化能力的树方面所起的关键作用。", "problem": "考虑极端梯度提升（XGBoost），该算法通过对当前预测值进行二阶泰勒展开，最小化一个正则化的训练目标来拟合决策树。对于权重为 $w$ 的单个叶节点，假设每次迭代对目标的近似贡献为\n$$\\sum_{i \\in \\text{leaf}} \\big( g_i w + \\tfrac{1}{2} h_i w^2 \\big) + \\tfrac{\\lambda}{2} w^2 + \\gamma,$$\n其中 $g_i$ 和 $h_i$ 分别表示损失函数在数据点 $i$ 处对预测值的一阶和二阶导数，$\\lambda$ 是叶节点权重的 $\\ell_2$ 正则化系数，$\\gamma$ 是每个叶节点的复杂度惩罚。令 $G = \\sum_{i \\in \\text{leaf}} g_i$ 和 $H = \\sum_{i \\in \\text{leaf}} h_i$ 表示该叶节点上的一阶和二阶导数之和。\n\n假设一个候选分裂将一个节点上的数据划分为一个左子节点（其聚合值为 $(G_L, H_L)$）和一个右子节点（其聚合值为 $(G_R, H_R)$），而父节点的聚合值为 $(G_P, H_P)$，其中 $G_P = G_L + G_R$ 且 $H_P = H_L + H_R$。假设 $H_L > 0$，$H_R > 0$ 且 $\\lambda > 0$。\n\n从上述正则化目标出发，且不使用任何预先记下的公式，推导出一个精确的解析表达式，该表达式表示叶节点惩罚 $\\gamma$ 的最小值，使得执行分裂所带来的净目标减少为非正值（即不发生分裂）。然后，使用具体数值 $G_L = 7$，$H_L = 14$，$G_R = -3$，$H_R = 6$ 和 $\\lambda = 1$ 来计算这个最小 $\\gamma$ 值。将最小 $\\gamma$ 的最终数值四舍五入到四位有效数字。你的最终答案必须是一个实数。", "solution": "用户希望我按照以下步骤解决问题：\n1.  **验证问题陈述：** 我将提取所有给定信息，并根据验证标准进行检查。该问题提法明确，在统计学习理论（特别是 XGBoost 的机制）方面有科学依据，并提供了所有必要的信息。这是一个有效的问题。\n2.  **推导最小 gamma 的解析表达式。**\n3.  **用给定的数值计算该表达式。**\n\n以下是详细的解题过程：\n\n### 步骤 1：问题验证\n\n**1.1. 提取给定信息：**\n-   对于权重为 $w$ 的单个叶节点，每次迭代的近似目标贡献为：$\\sum_{i \\in \\text{leaf}} \\big( g_i w + \\tfrac{1}{2} h_i w^2 \\big) + \\tfrac{\\lambda}{2} w^2 + \\gamma$。\n-   $g_i$：损失函数在数据点 $i$ 处对预测值的一阶导数。\n-   $h_i$：损失函数在数据点 $i$ 处对预测值的二阶导数。\n-   $\\lambda$：叶节点权重的 $\\ell_2$ 正则化系数。\n-   $\\gamma$：每个叶节点的复杂度惩罚。\n-   $G = \\sum_{i \\in \\text{leaf}} g_i$。\n-   $H = \\sum_{i \\in \\text{leaf}} h_i$。\n-   一次分裂创建一个左子节点（其聚合值为 $(G_L, H_L)$）和一个右子节点（其聚合值为 $(G_R, H_R)$）。\n-   父节点的聚合值为 $(G_P, H_P)$，其中 $G_P = G_L + G_R$ 且 $H_P = H_L + H_R$。\n-   假设：$H_L > 0$，$H_R > 0$ 且 $\\lambda > 0$。\n-   用于计算的数值：$G_L = 7$，$H_L = 14$，$G_R = -3$，$H_R = 6$ 和 $\\lambda = 1$。\n\n**1.2. 使用提取的信息进行验证：**\n-   **科学性：** 该问题是极端梯度提升（XGBoost）理论中的一个标准推导，XGBoost 是机器学习中一个广泛使用且成熟的算法。目标函数、其泰勒展开和正则化项都是 XGBoost 公式中的经典内容。\n-   **提法明确：** 问题提供了所有必要的定义、约束和数据，以推导出唯一且有意义的解。问题清晰明确，没有歧义。\n-   **客观性：** 问题以精确的数学语言陈述，没有主观性。\n\n**1.3. 结论与行动：**\n该问题是有效的。我将继续推导解答。\n\n### 步骤 2：推导与求解\n\n问题要求解 $\\gamma$ 的最小值，使得分裂不被执行。这种情况发生在分裂后的总目标函数大于或等于不进行分裂时的目标函数。我们必须首先找到给定树结构下的最小化目标值。\n\n设 $\\tilde{\\mathcal{L}}$ 为正在添加的新树的总目标函数。对于一棵有 $T$ 个叶节点的树，这是每个叶节点贡献的总和：\n$$ \\tilde{\\mathcal{L}} = \\sum_{j=1}^{T} \\left[ \\sum_{i \\in I_j} \\left( g_i w_j + \\frac{1}{2} h_i w_j^2 \\right) + \\frac{\\lambda}{2} w_j^2 + \\gamma \\right] $$\n其中 $I_j$ 是叶节点 $j$ 中的数据集，而 $w_j$ 是叶节点 $j$ 的权重。\n\n使用给定的聚合总和 $G_j = \\sum_{i \\in I_j} g_i$ 和 $H_j = \\sum_{i \\in I_j} h_i$，我们可以将单个叶节点 $j$ 的目标重写为：\n$$ \\tilde{\\mathcal{L}}_j = G_j w_j + \\frac{1}{2} H_j w_j^2 + \\frac{\\lambda}{2} w_j^2 + \\gamma = G_j w_j + \\frac{1}{2} (H_j + \\lambda) w_j^2 + \\gamma $$\n树的总目标函数是：\n$$ \\tilde{\\mathcal{L}} = \\sum_{j=1}^{T} \\left[ G_j w_j + \\frac{1}{2} (H_j + \\lambda) w_j^2 \\right] + T\\gamma $$\n对于固定的树结构（即固定的叶节点），此目标函数是关于叶节点权重 $w_j$ 的独立二次函数的总和。我们可以通过最小化每个叶节点对目标的贡献来找到其最优权重 $w_j^*$。我们对 $w_j$ 求导并令其为 $0$：\n$$ \\frac{\\partial \\tilde{\\mathcal{L}}_j}{\\partial w_j} = G_j + (H_j + \\lambda) w_j = 0 $$\n考虑到 $H_L > 0$，$H_R > 0$ 且 $\\lambda > 0$，因此对于任何叶节点 $H_j > 0$，从而 $H_j + \\lambda > 0$。因此，存在唯一的最小值。最优权重 $w_j^*$ 是：\n$$ w_j^* = -\\frac{G_j}{H_j + \\lambda} $$\n将此最优权重代回叶节点 $j$ 的目标函数，得到该叶节点的最小目标值：\n$$ \\tilde{\\mathcal{L}}_j^* = G_j \\left( -\\frac{G_j}{H_j + \\lambda} \\right) + \\frac{1}{2} (H_j + \\lambda) \\left( -\\frac{G_j}{H_j + \\lambda} \\right)^2 + \\gamma $$\n$$ \\tilde{\\mathcal{L}}_j^* = -\\frac{G_j^2}{H_j + \\lambda} + \\frac{1}{2} \\frac{G_j^2}{H_j + \\lambda} + \\gamma = -\\frac{1}{2} \\frac{G_j^2}{H_j + \\lambda} + \\gamma $$\n这个值通常被称为叶节点的“分数”。\n\n现在，我们比较两种情况下的总目标值：\n1.  **不分裂：** 父节点被视为单个叶节点。此时，$T=1$。目标函数为：\n    $$ \\tilde{\\mathcal{L}}_{\\text{no-split}} = -\\frac{1}{2} \\frac{G_P^2}{H_P + \\lambda} + \\gamma $$\n2.  **分裂：** 父节点被分裂为左（$L$）和右（$R$）两个叶节点。此时，$T=2$。总目标是两个新叶节点分数的总和：\n    $$ \\tilde{\\mathcal{L}}_{\\text{split}} = \\left( -\\frac{1}{2} \\frac{G_L^2}{H_L + \\lambda} + \\gamma \\right) + \\left( -\\frac{1}{2} \\frac{G_R^2}{H_R + \\lambda} + \\gamma \\right) = -\\frac{1}{2} \\left( \\frac{G_L^2}{H_L + \\lambda} + \\frac{G_R^2}{H_R + \\lambda} \\right) + 2\\gamma $$\n如果分裂能降低总体目标函数，即如果 $\\tilde{\\mathcal{L}}_{\\text{split}}  \\tilde{\\mathcal{L}}_{\\text{no-split}}$，则分裂是有益的。“净目标减少量”，通常称为增益，是 $\\text{Gain} = \\tilde{\\mathcal{L}}_{\\text{no-split}} - \\tilde{\\mathcal{L}}_{\\text{split}}$。\n\n问题陈述，如果此净目标减少量为非正值，则不发生分裂。因此，我们设定条件 $\\text{Gain} \\le 0$：\n$$ \\tilde{\\mathcal{L}}_{\\text{no-split}} - \\tilde{\\mathcal{L}}_{\\text{split}} \\le 0 $$\n$$ \\left( -\\frac{1}{2} \\frac{G_P^2}{H_P + \\lambda} + \\gamma \\right) - \\left( -\\frac{1}{2} \\left( \\frac{G_L^2}{H_L + \\lambda} + \\frac{G_R^2}{H_R + \\lambda} \\right) + 2\\gamma \\right) \\le 0 $$\n$$ \\frac{1}{2} \\left( \\frac{G_L^2}{H_L + \\lambda} + \\frac{G_R^2}{H_R + \\lambda} - \\frac{G_P^2}{H_P + \\lambda} \\right) - \\gamma \\le 0 $$\n重新整理以求解 $\\gamma$：\n$$ \\gamma \\ge \\frac{1}{2} \\left( \\frac{G_L^2}{H_L + \\lambda} + \\frac{G_R^2}{H_R + \\lambda} - \\frac{G_P^2}{H_P + \\lambda} \\right) $$\n确保不发生分裂的 $\\gamma$ 的最小值是使不等式变为等式的值。使用 $G_P = G_L + G_R$ 和 $H_P = H_L + H_R$：\n$$ \\gamma_{\\text{min}} = \\frac{1}{2} \\left( \\frac{G_L^2}{H_L + \\lambda} + \\frac{G_R^2}{H_R + \\lambda} - \\frac{(G_L + G_R)^2}{H_L + H_R + \\lambda} \\right) $$\n这就是所要求的解析表达式。\n\n现在我们用给定的值计算这个表达式：$G_L = 7$，$H_L = 14$，$G_R = -3$，$H_R = 6$ 和 $\\lambda = 1$。\n首先，计算父节点的聚合值：\n$$ G_P = G_L + G_R = 7 + (-3) = 4 $$\n$$ H_P = H_L + H_R = 14 + 6 = 20 $$\n将这些值代入 $\\gamma_{\\text{min}}$ 的表达式中：\n$$ \\gamma_{\\text{min}} = \\frac{1}{2} \\left( \\frac{7^2}{14 + 1} + \\frac{(-3)^2}{6 + 1} - \\frac{4^2}{20 + 1} \\right) $$\n$$ \\gamma_{\\text{min}} = \\frac{1}{2} \\left( \\frac{49}{15} + \\frac{9}{7} - \\frac{16}{21} \\right) $$\n为了对分数求和，我们找到一个公分母，即 $\\text{lcm}(15, 7, 21) = 105$。\n$$ \\gamma_{\\text{min}} = \\frac{1}{2} \\left( \\frac{49 \\times 7}{15 \\times 7} + \\frac{9 \\times 15}{7 \\times 15} - \\frac{16 \\times 5}{21 \\times 5} \\right) $$\n$$ \\gamma_{\\text{min}} = \\frac{1}{2} \\left( \\frac{343}{105} + \\frac{135}{105} - \\frac{80}{105} \\right) $$\n$$ \\gamma_{\\text{min}} = \\frac{1}{2} \\left( \\frac{343 + 135 - 80}{105} \\right) $$\n$$ \\gamma_{\\text{min}} = \\frac{1}{2} \\left( \\frac{478 - 80}{105} \\right) = \\frac{1}{2} \\left( \\frac{398}{105} \\right) $$\n$$ \\gamma_{\\text{min}} = \\frac{199}{105} $$\n换算成小数，约等于 $1.895238...$。四舍五入到四位有效数字得到 $1.895$。", "answer": "$$\\boxed{1.895}$$", "id": "3120320"}, {"introduction": "前面的练习侧重于寻找最优分裂点的确定性机制，但现实世界中的模型对其训练数据非常敏感。这项动手编程任务将理论计算转向了实际的代码实现，通过使用自助法（bootstrap）重采样来量化学习到的分裂阈值的不确定性，从而探索模型稳定性的概念[@problem_id:3120252]。完成这项练习将使您对模型的行为有一个更深刻、更细致的理解，并认识到评估其稳健性的重要性。", "problem": "您将研究使用极限梯度提升 (XGBoost) 原理构建的一维决策桩中，学习到的分裂阈值的稳定性，并通过自助法重采样 (bootstrap resampling) 来量化不确定性。任务是实现一个完整的程序，该程序在极限梯度提升的第一次迭代中使用逻辑损失拟合一个单深度树桩，记录学习到的阈值，并为该阈值构建一个跨自助法重采样的非参数置信区间。\n\n使用以下基础理论：\n- 二元标签编码为 $y \\in \\{0,1\\}$，模型对分数函数 $f(x)$ 使用逻辑损失，其中 $p(x) = \\sigma(f(x))$，$\\sigma(\\cdot)$ 表示 logistic sigmoid 函数。单个观测值的损失为 $-\\left(y \\log p + (1-y)\\log(1-p)\\right)$。\n- 极限梯度提升 (XGBoost) 通过围绕当前分数 $f(x)$ 对正则化目标函数进行二阶泰勒近似来构建树，使用关于 $f(x)$ 的一阶和二阶导数，并对叶子权重使用参数为 $\\lambda$ 的 $\\ell_2$ 正则化，同时施加叶子分裂惩罚 $\\gamma$。\n- 决策桩由单个特征 $x$ 上的一个标量阈值 $t$ 产生，恰好有两个叶子节点。\n\n要求：\n1) 从第一性原理（从逻辑损失和正则化目标的二阶泰勒近似开始）出发，推导评估任何形式为 $x \\le t$ 与 $x  t$ 的候选树桩分裂质量所需的表达式，使用左右划分上汇总的一阶和二阶导数项以及正则化参数 $\\lambda$ 和 $\\gamma$。从所有训练点的当前模型分数 $f(x) \\equiv 0$ 开始，因此对所有 $x$ 都有 $p(x) = \\sigma(0)$。\n2) 实现一个程序，在给定一个由数据对 $(x_i,y_i)$ 组成的数据集（其中 $x_i \\in \\mathbb{R}$ 且 $y_i \\in \\{0,1\\}$）的情况下，通过以下步骤计算第一个树桩的最佳阈值 $t$：\n   - 按 $x$ 对数据进行排序，\n   - 仅在严格递增的相邻 $x$ 值之间考虑候选分裂点，\n   - 为每个候选分裂点评估二阶近似的正则化目标函数减少量，并\n   - 选择使该减少量最大化的 $t$，该选择受制于一个最小子节点权重约束，该约束定义为每个子节点中二阶导数之和的下界。\n   如果在该约束下不存在有效分裂，则使用观测到的最小和最大 $x$ 值之间的中点作为回退阈值。\n3) 为了评估稳定性，实现自助法重采样：对于一个大小为 $n$ 的给定数据集，有放回地抽取 $B$ 个大小为 $n$ 的自助样本，在每个自助样本上完全按照上述方法重新拟合树桩，并记录阈值 $\\{t^{(b)}\\}_{b=1}^B$。\n4) 将 $\\{t^{(b)}\\}_{b=1}^B$ 的 $2.5$ 和 $97.5$ 分位数作为 $t$ 的经验 $95\\%$ 水平双侧置信区间进行计算。\n5) 使用单一、固定的伪随机种子 $12345$ 来确保确定性结果。\n6) 对于下方的每个数据集，按如下方式人工生成数据：\n   - 独立地从 $\\mathcal{N}(0,1)$ 采样 $x_i$。\n   - 给定一个真实阈值 $\\theta^\\star$，如果 $x_i  \\theta^\\star$ 则设置 $y_i = 1$，否则设置 $y_i = 0$。\n   - 对每个 $i$ 独立地以概率 $p_{\\text{flip}}$ 通过将 $y_i$ 替换为 $1-y_i$ 来翻转标签。\n   所有概率必须表示为 $[0,1]$ 区间内的小数。\n7) 分裂选择的超参数必须是：$\\lambda = 1.0$，$\\gamma = 0.0$，以及最小子节点权重（二阶导数之和）等于 $10^{-6}$。仅在初始提升迭代中，当 $f(x)\\equiv 0$ 时进行计算。\n8) 测试套件。在以下三个数据集上运行您的程序，每个数据集使用 $B=400$ 次自助采样：\n   - 测试A（理想情况）：$n=300$, $\\theta^\\star = 0.25$, $p_{\\text{flip}} = 0.0$。\n   - 测试B（噪声较大）：$n=200$, $\\theta^\\star = -0.20$, $p_{\\text{flip}} = 0.30$。\n   - 测试C（小样本）：$n=40$, $\\theta^\\star = 0.00$, $p_{\\text{flip}} = 0.10$。\n9) 最终输出格式。您的程序应产生单行输出，其中包含三个测试的置信区间，格式为一个包含三个双元素列表的逗号分隔列表，每个列表为 $[L,U]$，其中 $L$ 是下界，$U$ 是上界。将每个边界值四舍五入到六位小数。确切要求的格式是：\n   [[L_A,U_A],[L_B,U_B],[L_C,U_C]]\n其中 $L_A$ 和 $U_A$ 是测试A的边界， $L_B$ 和 $U_B$ 是测试B的边界， $L_C$ 和 $U_C$ 是测试C的边界。不应打印任何额外文本。\n\n注意：\n- 不涉及角度或物理单位。\n- 所有数值概率和分位数必须按小数处理（而不是百分比）。\n- 算法必须纯粹根据上述定义实现；不允许使用任何超出二阶近似与 $\\ell_2$ 正则化和叶子惩罚所隐含范围的启发式捷径。", "solution": "该问题要求实现一个程序，来确定决策桩最优分裂阈值的自助法置信区间。这个决策桩代表了一个使用逻辑损失进行二元分类训练的极限梯度提升（XGBoost）模型中的第一棵树。解决方案分为两部分：首先，从第一性原理推导理论基础；其次，基于该理论进行算法实现。\n\n### 第一部分：理论推导\n\nXGBoost算法的核心是迭代地最小化一个正则化的目标函数。在每次提升迭代 $m$ 中，一个新的树（由其分数函数 $f_m(x)$ 表示）被添加到模型中。在这一步，目标函数通过损失函数在前 $m-1$ 棵树的预测值 $\\hat{y}^{(m-1)}$ 周围的二阶泰勒展开来进行近似。新树 $f_m$ 的目标函数是：\n$$ \\mathcal{O}^{(m)} \\approx \\sum_{i=1}^{n} \\left[ L(y_i, \\hat{y}_i^{(m-1)}) + g_i f_m(x_i) + \\frac{1}{2} h_i f_m^2(x_i) \\right] + \\Omega(f_m) $$\n此处，$L(y_i, \\hat{y})$ 是单个观测值 $(x_i, y_i)$ 的损失函数，$g_i$ 和 $h_i$ 是损失函数关于原始分数预测值 $\\hat{y}$ 的一阶和二阶导数，在当前模型的预测值 $\\hat{y}_i^{(m-1)}$ 处计算：\n$$ g_i = \\left[\\frac{\\partial L(y_i, \\hat{y})}{\\partial \\hat{y}}\\right]_{\\hat{y}=\\hat{y}_i^{(m-1)}}, \\quad h_i = \\left[\\frac{\\partial^2 L(y_i, \\hat{y})}{\\partial \\hat{y}^2}\\right]_{\\hat{y}=\\hat{y}_i^{(m-1)}} $$\n$L(y_i, \\hat{y}_i^{(m-1)})$ 项相对于新树 $f_m$ 是一个常数，可以从优化中省略。正则化项 $\\Omega(f_m)$ 对模型复杂度进行惩罚：\n$$ \\Omega(f_m) = \\gamma T + \\frac{1}{2}\\lambda \\sum_{j=1}^{T} w_j^2 $$\n其中 $T$ 是树中叶子节点的数量，$w_j$ 是分配给叶子节点 $j$ 的分数（权重），$\\gamma$ 和 $\\lambda$ 是正则化参数。\n\n对于一个具有 $T$ 个叶子节点的固定树结构，函数 $f_m(x_i)$ 仅返回实例 $x_i$ 落入的叶子节点 $j$ 的权重 $w_j$。令 $I_j = \\{i | x_i \\text{ 在叶子节点 } j \\text{ 中}\\}$。目标函数可以通过按叶子节点分组重写：\n$$ \\mathcal{O}^{(m)} \\approx \\sum_{j=1}^{T} \\left[ \\left(\\sum_{i \\in I_j} g_i\\right) w_j + \\frac{1}{2} \\left(\\sum_{i \\in I_j} h_i + \\lambda\\right) w_j^2 \\right] + \\gamma T $$\n令 $G_j = \\sum_{i \\in I_j} g_i$ 且 $H_j = \\sum_{i \\in I_j} h_i$。目标函数变成了关于 $w_j$ 的二次函数之和：\n$$ \\mathcal{O}^{(m)} \\approx \\sum_{j=1}^{T} \\left[ G_j w_j + \\frac{1}{2} (H_j + \\lambda) w_j^2 \\right] + \\gamma T $$\n对于一个固定的树结构，对于每个叶子节点 $j$，最小化这个二次函数的最优权重 $w_j^*$ 是通过将其关于 $w_j$ 的导数设为零来找到的：\n$$ \\frac{\\partial}{\\partial w_j} \\left[ G_j w_j + \\frac{1}{2} (H_j + \\lambda) w_j^2 \\right] = G_j + (H_j + \\lambda) w_j = 0 \\implies w_j^* = -\\frac{G_j}{H_j + \\lambda} $$\n将此最优权重代回目标函数，得到给定树结构的最小目标值：\n$$ \\mathcal{O}^* = -\\frac{1}{2} \\sum_{j=1}^{T} \\frac{G_j^2}{H_j + \\lambda} + \\gamma T $$\n树构建算法的目标是找到一个能最小化此值的树结构。这是一个贪心过程。从单个叶子节点开始，我们寻找一个能导致目标函数最大减少量的分裂。将一个父叶子节点 $P$ 分裂成一个左叶子节点 $L$ 和一个右叶子节点 $R$ 所带来的减少量，或称“增益”，是：\n$$ \\text{Gain} = \\mathcal{O}^*_P - (\\mathcal{O}^*_L + \\mathcal{O}^*_R) $$\n$$ \\text{Gain} = \\left(-\\frac{1}{2} \\frac{G_P^2}{H_P + \\lambda} + \\gamma \\cdot 1\\right) - \\left(-\\frac{1}{2} \\frac{G_L^2}{H_L + \\lambda} - \\frac{1}{2} \\frac{G_R^2}{H_R + \\lambda} + \\gamma \\cdot 2\\right) $$\n注意到 $G_P = G_L + G_R$ 和 $H_P = H_L + H_R$，增益简化为：\n$$ \\text{Gain} = \\frac{1}{2} \\left[ \\frac{G_L^2}{H_L + \\lambda} + \\frac{G_R^2}{H_R + \\lambda} - \\frac{(G_L + G_R)^2}{H_L + H_R + \\lambda} \\right] - \\gamma $$\n为了找到给定节点的最佳分裂，我们必须为所有可能的候选分裂评估此增益。对于特定节点的任何分裂，$\\frac{(G_L+G_R)^2}{H_L+H_R+\\lambda}$ 项（与父节点的分数有关）和惩罚项 $\\gamma$ 都是常数。因此，最大化增益等价于最大化分数：\n$$ \\text{Score} = \\frac{G_L^2}{H_L + \\lambda} + \\frac{G_R^2}{H_R + \\lambda} $$\n这就是用来评估候选分裂质量的表达式。\n\n对于这个具体问题，我们对一个标签为 $y \\in \\{0,1\\}$ 的二元分类问题使用逻辑损失。预测概率是 $p = \\sigma(f(x)) = (1+e^{-f(x)})^{-1}$，损失是 $L(y, f) = -[y \\log p + (1-y)\\log(1-p)]$。关于原始分数 $f(x)$ 的导数是：\n$$ g = \\frac{\\partial L}{\\partial f} = p - y $$\n$$ h = \\frac{\\partial^2 L}{\\partial f^2} = \\frac{\\partial p}{\\partial f} = p(1-p) $$\n问题规定我们从初始模型为 $f_0(x) \\equiv 0$ 的第一次提升迭代开始。这意味着对所有数据点 $i$，初始概率预测为 $p_i = \\sigma(0) = 0.5$。因此，第一棵树的梯度和二阶导数（Hessian）是：\n$$ g_i = 0.5 - y_i $$\n$$ h_i = 0.5 \\times (1 - 0.5) = 0.25 $$\n这些 $g_i$ 和 $h_i$ 的表达式被用来计算每个候选分裂的汇总和 $G_L, H_L, G_R, H_R$。\n\n### 第二部分：算法与实现\n\n寻找最优分裂及其置信区间的步骤如下：\n\n1.  **数据模拟**：对于每个测试用例，生成一个大小为 $n$ 的数据集。特征值 $x_i$ 从标准正态分布 $\\mathcal{N}(0,1)$ 中抽取。如果 $x_i  \\theta^\\star$，真实标签 $y_i$ 被设置为 $1$，否则为 $0$。然后以概率 $p_{\\text{flip}}$ 翻转这些标签以引入噪声。\n\n2.  **最优阈值搜索**：实现一个函数来为给定数据集 $(x, y)$ 寻找最佳分裂阈值 $t$。\n    a. 首先，为所有点计算梯度 $g_i = 0.5 - y_i$ 和二阶导数 $h_i = 0.25$。\n    b. 数据点 $(x_i, y_i)$ 根据它们的 $x_i$ 值进行排序。这使得能够对所有可能的分裂点进行高效的线性扫描。\n    c. 算法遍历排序后的相邻 $x$ 值。在每对 $x_{(i)}  x_{(i+1)}$ 的 $(x_{(i)}, x_{(i+1)})$ 之间定义一个候选分裂。阈值是中点 $t = (x_{(i)} + x_{(i+1)})/2$。\n    d. 对于每个候选分裂，数据被划分为一个左集合（$x \\le t$）和一个右集合（$x  t$）。计算梯度和二阶导数的和（$G_L, H_L, G_R, H_R$）。\n    e. 仅当两个子节点中的二阶导数之和都至少为最小子节点权重时，即 $H_L \\ge 10^{-6}$ 和 $H_R \\ge 10^{-6}$，分裂才被认为是有效的。\n    f. 对每个有效分裂，使用 $\\lambda=1.0$ 计算分数 $\\frac{G_L^2}{H_L + \\lambda} + \\frac{G_R^2}{H_R + \\lambda}$。\n    g. 最大化此分数的阈值 $t$ 被选为该数据集的最优阈值。\n    h. 如果找不到有效的分裂（例如，样本中所有的 $x$ 值都相同），则使用一个等于观测到的最小和最大 $x$ 值中点的回退阈值。\n\n3.  **自助法重采样**：为了评估学习到的阈值的稳定性，此过程在 $B=400$ 个自助样本上重复进行。对于每个测试用例生成的数据集，通过从原始数据中有放回地抽样来创建 $B$ 个大小为 $n$ 的新数据集。为每个自助样本计算最优阈值，从而产生一个包含400个阈值的分布 $\\{t^{(b)}\\}_{b=1}^{400}$。\n\n4.  **置信区间计算**：通过找到排序后的自助法阈值的第 $2.5$ 和第 $97.5$ 百分位数来构建阈值的经验 $95\\%$ 置信区间。\n\n对指定的三个测试用例中的每一个都执行这整个过程，使用固定的伪随机数生成器种子 $12345$ 以获得确定性和可复现的结果。最终输出包含每个测试用例计算出的置信区间，格式化为六位小数。", "answer": "```python\nimport numpy as np\n\ndef find_best_threshold(x, y, lambda_, min_child_weight):\n    \"\"\"\n    Finds the best split threshold for a 1D decision stump in XGBoost.\n    \"\"\"\n    n = len(x)\n    if n  2:\n        return (np.min(x) + np.max(x)) / 2.0 if n > 0 else 0.0\n\n    # At the first iteration, f(x)=0, so p=0.5\n    # g = p - y, h = p * (1-p)\n    g = 0.5 - y\n    h_val = 0.25\n\n    # Sort data by feature x for efficient scanning\n    sorted_indices = np.argsort(x)\n    x_sorted = x[sorted_indices]\n    g_sorted = g[sorted_indices]\n    \n    # Initialize with fallback threshold\n    fallback_threshold = (x_sorted[0] + x_sorted[-1]) / 2.0\n    best_threshold = fallback_threshold\n    best_score = -1.0\n    found_valid_split = False\n\n    # Calculate total sums\n    G_total = np.sum(g_sorted)\n    H_total = n * h_val\n\n    # Efficiently scan for the best split\n    G_L, H_L = 0.0, 0.0\n    for i in range(n - 1):\n        G_L += g_sorted[i]\n        H_L += h_val\n\n        # Consider split only between unique x values\n        if x_sorted[i]  x_sorted[i+1]:\n            G_R = G_total - G_L\n            H_R = H_total - H_L\n\n            # Check min_child_weight constraint\n            if H_L >= min_child_weight and H_R >= min_child_weight:\n                found_valid_split = True\n                score = (G_L**2 / (H_L + lambda_)) + (G_R**2 / (H_R + lambda_))\n                if score > best_score:\n                    best_score = score\n                    best_threshold = (x_sorted[i] + x_sorted[i+1]) / 2.0\n    \n    return best_threshold\n\ndef run_test_case(n, theta_star, p_flip, lambda_, min_child_weight, B, rng):\n    \"\"\"\n    Generates data, runs bootstrap, and computes CI for one test case.\n    \"\"\"\n    # 1. Generate synthetic data\n    x = rng.normal(loc=0.0, scale=1.0, size=n)\n    y = (x > theta_star).astype(int)\n    \n    # Apply label noise\n    flip_mask = rng.random(size=n)  p_flip\n    y[flip_mask] = 1 - y[flip_mask]\n\n    # 2. Bootstrap resampling\n    bootstrap_thresholds = []\n    base_indices = np.arange(n)\n    for _ in range(B):\n        bootstrap_indices = rng.choice(base_indices, size=n, replace=True)\n        x_boot = x[bootstrap_indices]\n        y_boot = y[bootstrap_indices]\n        \n        threshold = find_best_threshold(x_boot, y_boot, lambda_, min_child_weight)\n        bootstrap_thresholds.append(threshold)\n\n    # 3. Compute 95% confidence interval\n    lower_bound = np.quantile(bootstrap_thresholds, 0.025)\n    upper_bound = np.quantile(bootstrap_thresholds, 0.975)\n    \n    return [lower_bound, upper_bound]\n\ndef solve():\n    \"\"\"\n    Main solver function to run all test cases and print the final result.\n    \"\"\"\n    # Global parameters\n    SEED = 12345\n    B = 400\n    LAMBDA = 1.0\n    GAMMA = 0.0  # Note: gamma does not affect split selection, only gain value\n    MIN_CHILD_WEIGHT = 1e-6\n    \n    rng = np.random.default_rng(SEED)\n\n    # Test cases from the problem statement\n    test_cases = [\n        {'n': 300, 'theta_star': 0.25, 'p_flip': 0.0},   # Test A\n        {'n': 200, 'theta_star': -0.20, 'p_flip': 0.30}, # Test B\n        {'n': 40, 'theta_star': 0.00, 'p_flip': 0.10},   # Test C\n    ]\n\n    results = []\n    for case in test_cases:\n        ci = run_test_case(\n            n=case['n'],\n            theta_star=case['theta_star'],\n            p_flip=case['p_flip'],\n            lambda_=LAMBDA,\n            min_child_weight=MIN_CHILD_WEIGHT,\n            B=B,\n            rng=rng\n        )\n        results.append(ci)\n\n    # Format output to match the specified format\n    formatted_intervals = [f\"[{l:.6f},{u:.6f}]\" for l, u in results]\n    final_output = f\"[{','.join(formatted_intervals)}]\"\n\n    print(final_output)\n\nsolve()\n\n```", "id": "3120252"}]}