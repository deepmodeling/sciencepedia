{"hands_on_practices": [{"introduction": "支持向量机 (SVM) 的核心思想是找到一个能以最大间隔分两个类别的超平面。对于线性可分的数据，我们可以构建一个“硬间隔”分类器，但在现实世界中，数据往往包含噪声或重叠，这就需要更灵活的“软间隔”分类器。本练习将通过一个动手编程任务，帮助您深入理解正则化参数 $C$ 如何在最大化间隔和最小化分类错误之间进行权衡，并从第一性原理出发，量化地验证当 $C$ 足够大时，软间隔解如何收敛于硬间隔解 [@problem_id:3178313]。", "problem": "您将实现并分析一个在统计学习背景下的二元线性支持向量机（SVM）。您编写的程序必须构建一个线性可分的数据集，计算硬间隔解，然后验证对于足够大的正则化参数 $C$，软间隔解会收敛到硬间隔解，并使用零松弛变量 $\\xi_i=0$ 从 Karush–Kuhn–Tucker (KKT) 条件中推导出一个量化阈值。\n\n将使用的基本和核心定义：\n- 支持向量机（SVM）旨在找到一个线性分类器 $f(\\mathbf{x}) = \\mathbf{w}^\\top \\mathbf{x} + b$，使得决策规则 $\\operatorname{sign}(f(\\mathbf{x}))$ 能够分离各个类别。\n- 对于硬间隔 SVM（线性可分情况），原始优化问题是在约束条件 $y_i(\\mathbf{w}^\\top \\mathbf{x}_i + b) \\ge 1$ 对所有 $i$ 成立的情况下，最小化 $\\frac{1}{2}\\|\\mathbf{w}\\|^2$。\n- 对于软间隔 SVM，原始优化问题是在约束条件 $y_i(\\mathbf{w}^\\top \\mathbf{x}_i + b) \\ge 1 - \\xi_i$ 和 $\\xi_i \\ge 0$ 对所有 $i$ 成立的情况下，最小化 $\\frac{1}{2}\\|\\mathbf{w}\\|^2 + C \\sum_{i=1}^n \\xi_i$。\n- 对偶优化公式使用拉格朗日乘子 $\\alpha_i$ 和由 $Q_{ij} = y_i y_j \\mathbf{x}_i^\\top \\mathbf{x}_j$ 定义的 Gram 矩阵 $Q$。在硬间隔对偶问题中，约束条件为 $\\alpha_i \\ge 0$ 和 $\\sum_{i=1}^n y_i \\alpha_i = 0$。在软间隔对偶问题中，约束条件为 $0 \\le \\alpha_i \\le C$ 和 $\\sum_{i=1}^n y_i \\alpha_i = 0$。\n- Karush–Kuhn–Tucker (KKT) 条件在原始变量和对偶变量之间施加了互补松弛性和平稳性关系。\n\n您的程序必须：\n1. 在 $\\mathbb{R}^2$ 中构建以下固定的、确定性的、线性可分的数据集：\n   - 正类 ($y=+1$): $\\mathbf{x}_1=(2,2)^\\top$, $\\mathbf{x}_2=(2,3)^\\top$, $\\mathbf{x}_3=(3,2)^\\top$, $\\mathbf{x}_4=(3,3)^\\top$。\n   - 负类 ($y=-1$): $\\mathbf{x}_5=(-2,-2)^\\top$, $\\mathbf{x}_6=(-3,-2)^\\top$, $\\mathbf{x}_7=(-2,-3)^\\top$, $\\mathbf{x}_8=(-3,-3)^\\top$。\n   按列表的自然顺序使用标签向量 $y_i \\in \\{-1,+1\\}$。\n2. 求解硬间隔对偶问题以获得最优乘子 $\\alpha_i^{\\mathrm{H}}$，并由此计算硬间隔参数 $\\mathbf{w}_{\\mathrm{H}} = \\sum_{i=1}^n \\alpha_i^{\\mathrm{H}} y_i \\mathbf{x}_i$ 和 $b_{\\mathrm{H}}$。计算 $b_{\\mathrm{H}}$ 时，使用支持向量（即那些 $\\alpha_i^{\\mathrm{H}} > 0$ 的向量），通过间隔条件 $y_i(\\mathbf{w}_{\\mathrm{H}}^\\top \\mathbf{x}_i + b_{\\mathrm{H}}) = 1$ 对所有可用的支持向量取平均值。\n3. 量化一个阈值 $C_\\star$，使得当 $C \\ge C_\\star$ 时，软间隔解达到零松弛 $\\xi_i=0$ 并与硬间隔解重合。您必须使用软间隔原始问题的 KKT 平稳性条件（其中 $\\xi_i=0$）来确定 $C_\\star$，并将其与硬间隔对偶乘子关联起来。不要假设任何快捷公式；从 KKT 条件和可行性的第一性原理推导 $C_\\star$。\n4. 对于软间隔 SVM，求解不同 $C$ 值下的对偶问题，计算得到的参数 $\\mathbf{w}(C)$ 和 $b(C)$，以及松弛变量 $\\xi_i(C) = \\max\\{0, 1 - y_i(\\mathbf{w}(C)^\\top \\mathbf{x}_i + b(C))\\}$。\n5. 通过将 $\\mathbf{w}(C)$ 和 $b(C)$ 与 $\\mathbf{w}_{\\mathrm{H}}$ 和 $b_{\\mathrm{H}}$ 在适当的数值容差内进行比较，并检查所有 $\\xi_i(C)$ 是否在容差范围内等于零，来验证收敛性。\n\n测试套件和覆盖范围：\n- 设 $C_\\star$ 是您推导的阈值。定义以下五个测试用例，以探究不同的区域：\n  1. $C = 0.10$ (非常小的 $C$，预期存在非零松弛)。\n  2. $C = \\frac{1}{2} C_\\star$ (低于阈值)。\n  3. $C = C_\\star$ (在阈值处)。\n  4. $C = 1.10 \\, C_\\star$ (略高于阈值)。\n  5. $C = 1000 \\, C_\\star$ (远高于阈值)。\n- 对于每个测试用例，您的程序必须计算并返回一个包含三个条目的列表作为结果：\n  - 一个布尔值，指示所有松弛变量是否在 $10^{-6}$ 的容差内满足 $\\xi_i(C) = 0$。\n  - 一个布尔值，指示是否满足 $\\|\\mathbf{w}(C) - \\mathbf{w}_{\\mathrm{H}}\\|_2 \\le 10^{-5}$ 和 $|b(C) - b_{\\mathrm{H}}| \\le 10^{-5}$。\n  - $C_\\star$ 的浮点值（每个测试用例都相同）。\n\n最终输出格式：\n- 您的程序应生成一行输出，其中包含一个用方括号括起来的逗号分隔列表。列表中的每个元素对应一个测试用例，并且本身是包含上述三个值的列表。例如：\"[[true_or_false,true_or_false,C_star_value],[...],...]\"。\n- 此问题不涉及任何物理单位、角度或百分比；所有量均为无量纲的实数或布尔值。", "solution": "用户提供了一个有效的问题。\n该问题要求分析硬间隔和软间隔线性支持向量机（SVM）之间的关系。主要目标是求解硬间隔 SVM，然后从第一性原理推导出一个正则化阈值 $C_\\star$，使得对于任何正则化参数 $C \\ge C_\\star$，软间隔 SVM 解都收敛到硬间隔解。这种收敛性需要通过数值方法进行验证。\n\n分析分三个阶段进行：\n1.  对给定的线性可分数据集求解硬间隔对偶问题。\n2.  从 Karush-Kuhn-Tucker (KKT) 条件推导阈值 $C_\\star$。\n3.  实现一个数值解法来求解软间隔对偶问题，并对一组测试值 $C$ 验证其收敛性。\n\n**1. 硬间隔 SVM 解**\n\n数据集被指定为 $\\mathbb{R}^2$ 中的 $n=8$ 个点。\n数据点 $\\mathbf{x}_i$ 及其对应的类别标签 $y_i \\in \\{+1, -1\\}$ 如下：\n$\\mathbf{X} = \\begin{bmatrix} (2, 2)^\\top \\\\ (2, 3)^\\top \\\\ (3, 2)^\\top \\\\ (3, 3)^\\top \\\\ (-2, -2)^\\top \\\\ (-3, -2)^\\top \\\\ (-2, -3)^\\top \\\\ (-3, -3)^\\top \\end{bmatrix}$, $\\mathbf{y} = \\begin{bmatrix} +1 \\\\ +1 \\\\ +1 \\\\ +1 \\\\ -1 \\\\ -1 \\\\ -1 \\\\ -1 \\end{bmatrix}$。\n\n对于硬间隔 SVM，我们寻求在所有 $i$ 满足 $y_i(\\mathbf{w}^\\top\\mathbf{x}_i + b) \\ge 1$ 的约束下最小化 $\\frac{1}{2}\\|\\mathbf{w}\\|^2$。其对应的对偶问题是一个二次规划（QP）问题：\n$$\n\\underset{\\mathbf{\\alpha}}{\\text{maximize}} \\quad L_D(\\mathbf{\\alpha}) = \\sum_{i=1}^n \\alpha_i - \\frac{1}{2} \\sum_{i=1}^n \\sum_{j=1}^n \\alpha_i \\alpha_j y_i y_j (\\mathbf{x}_i^\\top \\mathbf{x}_j)\n$$\n约束条件为：\n$$\n\\sum_{i=1}^n \\alpha_i y_i = 0 \\quad \\text{and} \\quad \\alpha_i \\ge 0 \\quad \\text{for } i=1, \\dots, n.\n$$\n这等价于在相同约束条件下最小化 $\\frac{1}{2}\\mathbf{\\alpha}^\\top Q \\mathbf{\\alpha} - \\mathbf{1}^\\top \\mathbf{\\alpha}$，其中 $Q$ 是 Gram 矩阵，其元素为 $Q_{ij} = y_i y_j (\\mathbf{x}_i^\\top \\mathbf{x}_j)$。\n\n对给定数据集求解此 QP 问题，可得到最优的拉格朗日乘子 $\\mathbf{\\alpha}^{\\mathrm{H}}$。由于数据集的对称性，只有最接近另一类别的点 $\\mathbf{x}_1=(2,2)^\\top$ 和 $\\mathbf{x}_5=(-2,-2)^\\top$ 是支持向量。最优乘子为：\n$$\n\\alpha_1^{\\mathrm{H}} = \\frac{1}{16}, \\quad \\alpha_5^{\\mathrm{H}} = \\frac{1}{16}, \\quad \\text{and} \\quad \\alpha_i^{\\mathrm{H}} = 0 \\text{ for } i \\notin \\{1, 5\\}.\n$$\n\n利用最优乘子，权重向量 $\\mathbf{w}_{\\mathrm{H}}$ 计算如下：\n$$\n\\mathbf{w}_{\\mathrm{H}} = \\sum_{i=1}^n \\alpha_i^{\\mathrm{H}} y_i \\mathbf{x}_i = \\alpha_1^{\\mathrm{H}} y_1 \\mathbf{x}_1 + \\alpha_5^{\\mathrm{H}} y_5 \\mathbf{x}_5\n$$\n$$\n\\mathbf{w}_{\\mathrm{H}} = \\frac{1}{16}(+1)\\begin{pmatrix} 2 \\\\ 2 \\end{pmatrix} + \\frac{1}{16}(-1)\\begin{pmatrix} -2 \\\\ -2 \\end{pmatrix} = \\frac{1}{16}\\begin{pmatrix} 2 \\\\ 2 \\end{pmatrix} + \\frac{1}{16}\\begin{pmatrix} 2 \\\\ 2 \\end{pmatrix} = \\frac{1}{8}\\begin{pmatrix} 2 \\\\ 2 \\end{pmatrix} = \\begin{pmatrix} 0.25 \\\\ 0.25 \\end{pmatrix}.\n$$\n\n偏置项 $b_{\\mathrm{H}}$ 可通过支持向量求得，对于支持向量，间隔条件 $y_k(\\mathbf{w}_{\\mathrm{H}}^\\top \\mathbf{x}_k + b_{\\mathrm{H}}) = 1$ 成立。使用支持向量 $\\mathbf{x}_1$：\n$$\n(+1)\\left(\\begin{pmatrix} 0.25 \\\\ 0.25 \\end{pmatrix}^\\top \\begin{pmatrix} 2 \\\\ 2 \\end{pmatrix} + b_{\\mathrm{H}}\\right) = 1 \\implies (0.5 + 0.5) + b_{\\mathrm{H}} = 1 \\implies 1 + b_{\\mathrm{H}} = 1 \\implies b_{\\mathrm{H}} = 0.\n$$\n使用 $\\mathbf{x}_5$ 会得到相同的结果。因此，硬间隔解为 $\\mathbf{w}_{\\mathrm{H}} = (0.25, 0.25)^\\top$ 和 $b_{\\mathrm{H}}=0$。\n\n**2. 收敛阈值 $C_\\star$ 的推导**\n\n软间隔 SVM 的原始问题是：\n$$\n\\underset{\\mathbf{w}, b, \\mathbf{\\xi}}{\\text{minimize}} \\quad \\frac{1}{2}\\|\\mathbf{w}\\|^2 + C \\sum_{i=1}^n \\xi_i\n$$\n约束条件为 $y_i(\\mathbf{w}^\\top \\mathbf{x}_i + b) \\ge 1 - \\xi_i$ 且 $\\xi_i \\ge 0$ 对 $i=1, \\dots, n$ 成立。\n\n该问题的拉格朗日函数为：\n$$\nL_P(\\mathbf{w}, b, \\mathbf{\\xi}, \\mathbf{\\alpha}, \\mathbf{\\mu}) = \\frac{1}{2}\\|\\mathbf{w}\\|^2 + C \\sum_{i=1}^n \\xi_i - \\sum_{i=1}^n \\alpha_i [y_i(\\mathbf{w}^\\top \\mathbf{x}_i + b) - 1 + \\xi_i] - \\sum_{i=1}^n \\mu_i \\xi_i,\n$$\n其中 $\\alpha_i \\ge 0$ 和 $\\mu_i \\ge 0$ 是拉格朗日乘子。KKT 平稳性条件是：\n1.  $\\nabla_{\\mathbf{w}} L_P = \\mathbf{w} - \\sum_i \\alpha_i y_i \\mathbf{x}_i = \\mathbf{0} \\implies \\mathbf{w} = \\sum_i \\alpha_i y_i \\mathbf{x}_i$\n2.  $\\frac{\\partial L_P}{\\partial b} = -\\sum_i \\alpha_i y_i = 0$\n3.  $\\frac{\\partial L_P}{\\partial \\xi_i} = C - \\alpha_i - \\mu_i = 0$\n\n当且仅当所有松弛变量 $\\xi_i$ 均为零时，软间隔解 $(\\mathbf{w}(C), b(C))$ 收敛于硬间隔解 $(\\mathbf{w}_{\\mathrm{H}}, b_{\\mathrm{H}})$。当所有 $i$ 的 $\\xi_i=0$ 时，软间隔问题的约束条件实际上变成了硬间隔的约束条件。解 $(\\mathbf{w}_{\\mathrm{H}}, b_{\\mathrm{H}})$ 及其对偶变量 $\\alpha_i^{\\mathrm{H}}$ 必须满足软间隔问题的所有 KKT 条件。\n\n条件 (1) 和 (2) 与硬间隔公式相同，并且通过构造已经满足。条件 (3)，即 $C - \\alpha_i - \\mu_i = 0$，结合非负约束 $\\mu_i \\ge 0$，对乘子 $\\alpha_i$ 施加了一个新的要求：\n$$\nC - \\alpha_i = \\mu_i \\ge 0 \\implies C \\ge \\alpha_i.\n$$\n这个不等式必须对硬间隔乘子 $\\alpha_i^{\\mathrm{H}}$ 成立，这样它们才能成为软间隔问题的可行对偶变量（对应于零松弛的原始解）。因此，为了使解重合，我们必须有 $C \\ge \\alpha_i^{\\mathrm{H}}$ 对所有 $i=1, \\dots, n$ 成立。\n保证所有乘子都满足此条件的最小 $C$ 值即为阈值 $C_\\star$：\n$$\nC_\\star = \\max_{i} \\{\\alpha_i^{\\mathrm{H}}\\}.\n$$\n使用硬间隔解，我们发现：\n$$\nC_\\star = \\max\\left\\{\\frac{1}{16}, 0, \\dots, \\frac{1}{16}, \\dots, 0\\right\\} = \\frac{1}{16} = 0.0625.\n$$\n对于任何 $C \\ge C_\\star$，最优的软间隔乘子 $\\alpha_i(C)$ 将与 $\\alpha_i^{\\mathrm{H}}$ 相同，因此 $\\mathbf{w}(C) = \\mathbf{w}_{\\mathrm{H}}$，$b(C) = b_{\\mathrm{H}}$，并且所有 $\\xi_i(C) = 0$。\n\n**3. 算法实现与验证**\n\n实现一个算法来求解对偶 QP 问题，既包括硬间隔情况（通过设置 $C=\\infty$），也包括各种有限 $C$ 值的软间隔情况。\n1.  **求解硬间隔情况**：使用 QP 求解器找到 $\\mathbf{\\alpha}^{\\mathrm{H}}$，并从中计算出 $\\mathbf{w}_{\\mathrm{H}}$、$b_{\\mathrm{H}}$ 和 $C_\\star = \\max(\\mathbf{\\alpha}^{\\mathrm{H}})$。\n2.  **遍历测试用例**：对于每个指定的 $C$ 值（$0.10$、$0.5 C_\\star$、$C_\\star$、$1.1 C_\\star$、$1000 C_\\star$），在约束 $0 \\le \\alpha_i \\le C$ 下求解软间隔对偶 QP 问题，以找到 $\\mathbf{\\alpha}(C)$。\n3.  **计算软间隔参数**：从 $\\mathbf{\\alpha}(C)$ 计算参数 $\\mathbf{w}(C)$ 和 $b(C)$。偏置 $b(C)$ 是通过对所有满足 $0 < \\alpha_i(C) < C$ 的支持向量计算 $y_i - \\mathbf{w}(C)^\\top\\mathbf{x}_i$ 的平均值来确定的。如果不存在这样的点，$b(C)$ 则取为由所有支持向量（$\\alpha_i(C)>0$）确定的可行范围的中点。\n4.  **计算松弛并验证**：通过 $\\xi_i(C) = \\max\\{0, 1 - y_i(\\mathbf{w}(C)^\\top\\mathbf{x}_i + b(C))\\}$ 计算松弛变量。\n5.  **检查条件**：检查两个条件：(a) 所有 $\\xi_i(C)$ 是否在 $10^{-6}$ 的容差内为零，以及 (b) $\\mathbf{w}(C)$ 和 $b(C)$ 是否在 $10^{-5}$ 的容差内与 $\\mathbf{w}_{\\mathrm{H}}$ 和 $b_{\\mathrm{H}}$ 匹配。\n\n指定测试用例的结果应确认，对于 $C \\ge C_\\star$，软间隔解与硬间隔解相同（零松弛，参数匹配），而对于 $C<C_\\star$，解将不同且通常具有非零松弛。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import minimize, Bounds\n\ndef solve():\n    \"\"\"\n    Main function to perform SVM analysis as described in the problem.\n    \"\"\"\n    # 1. Construct the fixed, deterministic, linearly separable dataset.\n    X = np.array([\n        [2, 2], [2, 3], [3, 2], [3, 3],  # Positive class\n        [-2, -2], [-3, -2], [-2, -3], [-3, -3]  # Negative class\n    ])\n    y = np.array([1, 1, 1, 1, -1, -1, -1, -1])\n    n_samples, n_features = X.shape\n\n    # Gram matrix Q_ij = y_i * y_j * x_i^T * x_j\n    Q = np.outer(y, y) * (X @ X.T)\n\n    def solve_svm_dual(C_val):\n        \"\"\"Solves the SVM dual QP problem for a given C.\"\"\"\n        \n        # Objective function to minimize: 0.5 * alpha^T * Q * alpha - 1^T * alpha\n        def objective(alpha):\n            return 0.5 * alpha @ Q @ alpha - np.sum(alpha)\n\n        # Equality constraint: sum(alpha_i * y_i) = 0\n        constraints = [{'type': 'eq', 'fun': lambda alpha: alpha @ y}]\n\n        # Bounds on alpha: 0 = alpha_i = C\n        # For hard-margin, C is effectively infinity.\n        bounds = Bounds(0, C_val)\n\n        # Initial guess for alpha\n        alpha0 = np.zeros(n_samples)\n\n        # Solve the QP problem\n        res = minimize(objective, alpha0, method='SLSQP', bounds=bounds, constraints=constraints)\n        \n        return res.x if res.success else None\n\n    def get_w_b(alpha, C):\n        \"\"\"Computes w and b from the optimal alpha values.\"\"\"\n        # Set small alpha values to zero\n        alpha[alpha  1e-7] = 0\n        \n        # Compute w = sum(alpha_i * y_i * x_i)\n        w = np.sum((alpha * y)[:, np.newaxis] * X, axis=0)\n        \n        # Find support vectors\n        sv_indices = np.where(alpha > 1e-7)[0]\n        \n        # Robustly compute b\n        # Try to find support vectors on the margin (0  alpha  C)\n        margin_sv_indices = np.where((alpha > 1e-7)  (alpha  C - 1e-7))[0]\n        \n        if len(margin_sv_indices) > 0:\n            b = np.mean([y[i] - X[i] @ w for i in margin_sv_indices])\n        elif len(sv_indices) > 0:\n            # Fallback for cases where no SVs are on the margin (e.g., small C)\n            # Use all SVs to find the feasible range for b\n            sv_pos_indices = [i for i in sv_indices if y[i] > 0]\n            sv_neg_indices = [i for i in sv_indices if y[i]  0]\n            \n            b_highs = [1 - X[i] @ w for i in sv_pos_indices]\n            b_lows = [-1 - X[i] @ w for i in sv_neg_indices]\n            \n            b_high = min(b_highs) if b_highs else np.inf\n            b_low = max(b_lows) if b_lows else -np.inf\n            \n            b = (b_low + b_high) / 2.0\n        else: # No support vectors, should not happen for this problem\n            b = 0.0\n\n        return w, b\n\n    # 2. Solve the hard-margin dual problem (C -> infinity)\n    alpha_H = solve_svm_dual(np.inf)\n    if alpha_H is None:\n        raise RuntimeError(\"Hard-margin QP optimization failed.\")\n    \n    w_H, b_H = get_w_b(alpha_H, np.inf)\n\n    # 3. Quantify the threshold value C_star\n    C_star = np.max(alpha_H)\n\n    # 4. Define test cases\n    test_cases = [\n        0.10,\n        0.5 * C_star,\n        C_star,\n        1.10 * C_star,\n        1000 * C_star\n    ]\n\n    all_results = []\n    \n    # Tolerances\n    slack_tol = 1e-6\n    param_tol = 1e-5\n\n    # 5. Loop through test cases, solve soft-margin SVM and verify\n    for C in test_cases:\n        alpha_C = solve_svm_dual(C)\n        if alpha_C is None:\n            # Append a failure state if optimization fails\n            all_results.append([False, False, C_star])\n            continue\n\n        w_C, b_C = get_w_b(alpha_C.copy(), C)\n        \n        # Compute slacks\n        slacks = np.maximum(0, 1 - y * (X @ w_C + b_C))\n        \n        # Check conditions\n        all_slacks_zero = np.all(slacks = slack_tol)\n        params_converged = np.linalg.norm(w_C - w_H) = param_tol and abs(b_C - b_H) = param_tol\n        \n        result = [all_slacks_zero, params_converged, C_star]\n        all_results.append(result)\n\n    # Format output according to problem specification\n    def format_result(res_list):\n        bool1, bool2, c_val = res_list\n        # The example in the prompt uses lowercase booleans\n        return f\"[{str(bool1).lower()},{str(bool2).lower()},{c_val:.6f}]\"\n\n    formatted_results = [format_result(res) for res in all_results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3178313"}, {"introduction": "许多现实世界的数据集在原始输入空间中并非线性可分，这正是支持向量机强大能力的用武之地。“核技巧”(kernel trick) 是解决非线性问题的优雅方案，它允许我们在一个高维特征空间中进行计算，而无需显式地定义该空间中的数据坐标。这种方法通过将内积运算替换为一个核函数来实现，极大地提高了计算效率。本练习将引导您通过编程，直观地比较显式特征映射与隐式核函数两种方法，从而揭示核技巧的精髓 [@problem_id:3178252]。", "problem": "您需要编写一个完整、可运行的程序，从第一性原理出发，演示显式多项式特征增广如何在支持向量机（SVM）分类框架内实现一个二次核，并比较显式和隐式两种实现方式。严格在标签为 $\\{-1,+1\\}$ 的二元分类问题中进行操作。使用硬间隔支持向量机（SVM）公式作为数学基础，并通过带有大惩罚参数 $C$ 的软间隔对偶问题进行数值近似，以确保可行性和数值稳定性。\n\n需要采用的基本原理和定义：\n- 硬间隔 SVM 原始问题旨在通过求解以下问题在特征空间 $\\mathcal{H}$ 中寻找一个分离超平面\n$$\n\\min_{\\mathbf{w}, b}\\ \\frac{1}{2}\\lVert \\mathbf{w} \\rVert^2 \\quad \\text{subject to}\\quad y_i\\left(\\mathbf{w}^\\top \\phi(\\mathbf{x}_i) + b\\right) \\ge 1 \\ \\text{for all } i,\n$$\n其中 $\\phi: \\mathbb{R}^d \\to \\mathcal{H}$ 是一个特征映射，$y_i \\in \\{-1,+1\\}$ 是标签。\n- 软间隔 SVM 对偶问题（用于数值计算）最小化以下凸目标函数\n$$\n\\min_{\\boldsymbol{\\alpha} \\in \\mathbb{R}^n} \\ \\frac{1}{2}\\boldsymbol{\\alpha}^\\top \\mathbf{Q} \\boldsymbol{\\alpha} - \\mathbf{1}^\\top \\boldsymbol{\\alpha} \\quad \\text{subject to}\\quad \\mathbf{y}^\\top \\boldsymbol{\\alpha} = 0,\\ \\ 0 \\le \\alpha_i \\le C,\n$$\n其中 $\\mathbf{Q}_{ij} = y_i y_j K(\\mathbf{x}_i,\\mathbf{x}_j)$, $K$ 是一个半正定核。对于线性核 $K(\\mathbf{x},\\mathbf{z}) = \\mathbf{x}^\\top \\mathbf{z}$，这对应于在原始输入空间中的 SVM。对于非线性核，决策函数为 $f(\\mathbf{x}) = \\sum_{i=1}^n \\alpha_i y_i K(\\mathbf{x}_i,\\mathbf{x}) + b$。\n- 特征空间中的几何间隔为 $\\gamma = 1/\\lVert \\mathbf{w} \\rVert$，其中当使用表示定理时，$\\mathbf{w} = \\sum_{i=1}^n \\alpha_i y_i \\phi(\\mathbf{x}_i)$。\n- 在 $\\mathbb{R}^2$ 中的显式二次特征映射定义为\n$$\n\\phi(\\mathbf{x}) = \\begin{bmatrix} x_1^2 \\\\ x_2^2 \\\\ \\sqrt{2}\\,x_1 x_2 \\end{bmatrix}.\n$$\n您将以隐式方式实现齐次二次核为 $K(\\mathbf{x},\\mathbf{z}) = \\left(\\mathbf{x}^\\top \\mathbf{z}\\right)^2$，并以显式方式通过内积 $\\phi(\\mathbf{x})^\\top \\phi(\\mathbf{z})$ 实现。\n\n您的程序必须执行的任务：\n- 在对偶形式下实现一个软间隔 SVM 求解器，选择惩罚参数 $C = 10^5$ 来近似硬间隔解。用它来计算 $\\boldsymbol{\\alpha}$、偏置 $b$ 和预测值。\n- 对于齐次二次核，通过数值上检查在训练点上的相应 Gram 矩阵是否相等，来比较隐式核 $K(\\mathbf{x},\\mathbf{z}) = (\\mathbf{x}^\\top \\mathbf{z})^2$ 与显式内积 $\\phi(\\mathbf{x})^\\top \\phi(\\mathbf{z})$。\n- 使用为二次核学到的对偶变量，通过 $\\mathbf{w} = \\sum_i \\alpha_i y_i \\phi(\\mathbf{x}_i)$ 在增广特征空间中显式重构 $\\mathbf{w}$，计算几何间隔 $\\gamma = 1/\\lVert \\mathbf{w} \\rVert$，并验证使用显式 $(\\mathbf{w}, b)$ 计算的决策值与隐式核决策函数在训练点上的计算结果是否一致。\n- 通过使用线性核 $K(\\mathbf{x},\\mathbf{z})=\\mathbf{x}^\\top \\mathbf{z}$ 进行训练并检查是否所有训练标签都被正确分类，来评估在原始输入空间中的线性可分性。类似地，评估在二次核下的可分性。\n\n角度单位不适用。不涉及物理单位。所有报告的数值间隔必须是实数（浮点数）。所有布尔值必须是精确的逻辑值。不要输出百分比。\n\n测试套件需覆盖多个方面：\n- 数据集 A（在输入空间中非线性可分，在二次映射后线性可分）：\n  - 输入 $\\mathbf{X}_A = \\left\\{(1,1),\\ (1,-1),\\ (-1,1),\\ (-1,-1)\\right\\}$，标签 $\\mathbf{y}_A = \\{+1,\\ -1,\\ -1,\\ +1\\}$。\n- 数据集 B（在输入空间中已经线性可分）：\n  - 输入 $\\mathbf{X}_B = \\left\\{(2,2),\\ (2,3),\\ (-2,-2),\\ (-2,-3)\\right\\}$，标签 $\\mathbf{y}_B = \\{+1,\\ +1,\\ -1,\\ -1\\}$。\n- 数据集 C（最小的两点异类集合）：\n  - 输入 $\\mathbf{X}_C = \\left\\{(1,0),\\ (-1,0)\\right\\}$，标签 $\\mathbf{y}_C = \\{+1,\\ -1\\}$。\n\n对于每个数据集，按 A、B、C 的顺序，您的程序必须计算并返回以下五个值：\n$1.$ 一个布尔值，表示线性核是否在训练数据上实现完美分类（解释为输入空间中的线性可分性）。\n$2.$ 一个布尔值，表示二次核是否在训练数据上实现完美分类。\n$3.$ 一个布尔值，表示显式 Gram 矩阵 $\\left[\\phi(\\mathbf{x}_i)^\\top \\phi(\\mathbf{x}_j)\\right]_{ij}$ 是否在 $10^{-10}$ 的绝对容差内等于隐式 Gram 矩阵 $\\left[(\\mathbf{x}_i^\\top \\mathbf{x}_j)^2\\right]_{ij}$。\n$4.$ 一个布尔值，表示在增广特征空间中通过 $(\\mathbf{w},b)$ 显式计算的决策值的符号是否与来自隐式核决策函数在训练数据上的决策值符号一致。\n$5.$ 在显式特征空间中为二次核计算的几何间隔 $\\gamma = 1/\\lVert \\mathbf{w} \\rVert$，四舍五入到六位小数。\n\n最终输出格式：\n您的程序应生成一行输出，其中包含数据集 A、B 和 C 的 15 个结果，按顺序排列，作为一个用方括号括起来的逗号分隔列表（例如，“[resultA1,resultA2,resultA3,resultA4,resultA5,resultB1,...,resultC5]”）。布尔值和浮点数必须按此确切顺序和次序出现，间隔值按规定四舍五入到六位小数。不得打印任何其他文本。", "solution": "本题要求对核技巧进行数值演示和验证，特别是在支持向量机（SVM）背景下针对齐次二次核。这涉及到实现一个SVM求解器，比较隐式核函数与显式特征映射，并在多个数据集上评估性能。\n\n其数学基础是用于二元分类的SVM公式。给定一个训练数据集 $\\{\\mathbf{x}_i, y_i\\}_{i=1}^n$，其中输入向量 $\\mathbf{x}_i \\in \\mathbb{R}^d$，类别标签 $y_i \\in \\{-1, +1\\}$，硬间隔SVM旨在特征空间 $\\mathcal{H}$ 中找到一个分离超平面 $(\\mathbf{w}, b)$。从输入空间到特征空间的映射由函数 $\\phi: \\mathbb{R}^d \\to \\mathcal{H}$ 给出。目标是最大化几何间隔 $\\gamma = 1/\\lVert \\mathbf{w} \\rVert$，这等价于求解以下原始优化问题：\n$$\n\\min_{\\mathbf{w}, b} \\frac{1}{2} \\lVert \\mathbf{w} \\rVert^2 \\quad \\text{subject to} \\quad y_i (\\mathbf{w}^\\top \\phi(\\mathbf{x}_i) + b) \\ge 1, \\quad \\forall i=1, \\dots, n.\n$$\n出于计算目的，特别是当特征空间 $\\mathcal{H}$ 维度很高时，求解对偶问题更为实用。程序将通过使用具有非常大惩罚参数 $C = 10^5$ 的软间隔对偶公式来近似硬间隔解。对偶问题是一个凸二次规划（QP）问题，表示为：\n$$\n\\min_{\\boldsymbol{\\alpha}} \\frac{1}{2} \\boldsymbol{\\alpha}^\\top \\mathbf{Q} \\boldsymbol{\\alpha} - \\mathbf{1}^\\top \\boldsymbol{\\alpha} \\quad \\text{subject to} \\quad \\mathbf{y}^\\top \\boldsymbol{\\alpha} = 0, \\quad 0 \\le \\alpha_i \\le C,\n$$\n其中 $\\boldsymbol{\\alpha} = [\\alpha_1, \\dots, \\alpha_n]^\\top$ 是拉格朗日乘子，$\\mathbf{1}$ 是一个 $n$ 维的全一向量，$\\mathbf{Q}$ 是一个 $n \\times n$ 矩阵，其元素为 $\\mathbf{Q}_{ij} = y_i y_j K(\\mathbf{x}_i, \\mathbf{x}_j)$。核函数 $K(\\mathbf{x}, \\mathbf{z}) = \\phi(\\mathbf{x})^\\top \\phi(\\mathbf{z})$ 计算特征空间中的内积，而无需显式计算特征向量 $\\phi(\\mathbf{x})$。这就是核技巧的精髓。\n\n解出最优拉格朗日乘子 $\\boldsymbol{\\alpha}^*$ 后，特征空间中的权重向量 $\\mathbf{w}$ 由表示定理给出：\n$$\n\\mathbf{w} = \\sum_{i=1}^n \\alpha_i^* y_i \\phi(\\mathbf{x}_i).\n$$\n偏置项 $b$ 可以从任何满足 $0  \\alpha_k^*  C$ 的支持向量 $\\mathbf{x}_k$ 计算得出，使用 Karush-Kuhn-Tucker (KKT) 条件 $y_k(\\mathbf{w}^\\top \\phi(\\mathbf{x}_k) + b) = 1$，得到：\n$$\nb = y_k - \\mathbf{w}^\\top \\phi(\\mathbf{x}_k) = y_k - \\sum_{i=1}^n \\alpha_i^* y_i K(\\mathbf{x}_i, \\mathbf{x}_k).\n$$\n为了数值稳健性，$b$ 通常在所有这样的支持向量上取平均值。\n\n问题指定了两个核：线性核 $K(\\mathbf{x}, \\mathbf{z}) = \\mathbf{x}^\\top \\mathbf{z}$（其中 $\\phi(\\mathbf{x}) = \\mathbf{x}$）和齐次二次核 $K(\\mathbf{x}, \\mathbf{z}) = (\\mathbf{x}^\\top \\mathbf{z})^2$。关键任务是验证这个二次核是由显式特征映射 $\\phi: \\mathbb{R}^2 \\to \\mathbb{R}^3$ 实现的，定义如下：\n$$\n\\phi(\\mathbf{x}) = \\phi\\left(\\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix}\\right) = \\begin{bmatrix} x_1^2 \\\\ x_2^2 \\\\ \\sqrt{2} x_1 x_2 \\end{bmatrix}.\n$$\n通过计算此特征空间中的内积来确认等价性：\n$$\n\\phi(\\mathbf{x})^\\top \\phi(\\mathbf{z}) = \\begin{bmatrix} x_1^2 \\\\ x_2^2 \\\\ \\sqrt{2} x_1 x_2 \\end{bmatrix}^\\top \\begin{bmatrix} z_1^2 \\\\ z_2^2 \\\\ \\sqrt{2} z_1 z_2 \\end{bmatrix} = x_1^2 z_1^2 + x_2^2 z_2^2 + 2x_1 x_2 z_1 z_2 = (x_1 z_1 + x_2 z_2)^2 = (\\mathbf{x}^\\top \\mathbf{z})^2.\n$$\n程序将使用 `scipy.optimize.minimize` 实现一个 QP 求解器来找到 $\\boldsymbol{\\alpha}^*$。对于提供的三个数据集中的每一个，它将执行以下五项分析：\n$1.$ **线性可分性**：用线性核训练一个 SVM，并检查它是否完美地分类训练数据。\n$2.$ **二次可分性**：用二次核训练一个 SVM，并检查是否实现完美分类。\n$3.$ **Gram 矩阵等价性**：数值验证来自隐式核的 Gram 矩阵 $[\\mathbf{G}_{\\text{implicit}}]_{ij} = (\\mathbf{x}_i^\\top \\mathbf{x}_j)^2$ 与来自显式特征映射的 Gram 矩阵 $[\\mathbf{G}_{\\text{explicit}}]_{ij} = \\phi(\\mathbf{x}_i)^\\top \\phi(\\mathbf{x}_j)$ 是否相同。\n$4.$ **决策函数等价性**：对于二次核，为所有训练点以两种方式计算决策函数值 $f(\\mathbf{x})$：隐式地通过 $f(\\mathbf{x}) = \\sum_i \\alpha_i^* y_i K(\\mathbf{x}_i, \\mathbf{x}) + b$，和显式地通过 $f(\\mathbf{x}) = \\mathbf{w}^\\top \\phi(\\mathbf{x}) + b$。这些值的符号必须一致。\n$5.$ **几何间隔计算**：使用为二次核显式构造的 $\\mathbf{w}$，计算几何间隔 $\\gamma = 1/\\lVert \\mathbf{w} \\rVert$。如果特征空间中的数据不是线性可分的（如数据集 C），则分离超平面未定义，导致 $\\lVert \\mathbf{w} \\rVert \\to 0$。在这种情况下，间隔被认为是 $0.0$，表示没有分离间隔。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef solve_svm_dual(X, y, C, kernel):\n    \"\"\"\n    Solves the dual soft-margin SVM problem using a QP solver.\n    \"\"\"\n    n_samples, _ = X.shape\n\n    # Construct the Gram matrix\n    K = np.zeros((n_samples, n_samples))\n    for i in range(n_samples):\n        for j in range(n_samples):\n            K[i, j] = kernel(X[i], X[j])\n\n    # Construct the Q matrix for the QP problem\n    Q = np.outer(y, y) * K\n\n    # Define the objective function (to be minimized)\n    def objective(alpha):\n        return 0.5 * np.dot(alpha, np.dot(Q, alpha)) - np.sum(alpha)\n\n    # Define the equality constraint: sum(alpha_i * y_i) = 0\n    constraints = ({'type': 'eq', 'fun': lambda alpha: np.dot(alpha, y)})\n\n    # Define the bounds for alpha_i: 0 = alpha_i = C\n    bounds = [(0, C) for _ in range(n_samples)]\n\n    # Initial guess for alpha\n    alpha_init = np.zeros(n_samples)\n\n    # Solve the QP problem\n    res = minimize(objective, alpha_init, method='SLSQP', bounds=bounds, constraints=constraints)\n    alphas = res.x\n    \n    # Clean up small alpha values\n    alphas[alphas  1e-6] = 0\n\n    # Calculate the bias term 'b'\n    # It's robust to average 'b' over all support vectors.\n    sv_indices = np.where(alphas > 1e-6)[0]\n    \n    if len(sv_indices) > 0:\n        b_values = []\n        for sv_idx in sv_indices:\n            # Prediction for a support vector without bias\n            pred_sv = np.sum(alphas * y * K[:, sv_idx])\n            b_values.append(y[sv_idx] - pred_sv)\n        b = np.mean(b_values)\n    else:\n        # Fallback if no support vectors found (unlikely for these datasets)\n        b = 0.0\n\n    return alphas, b\n\ndef get_decision_values(X_eval, X_train, y_train, alphas, b, kernel):\n    \"\"\"\n    Computes decision values for given data points.\n    \"\"\"\n    n_eval = X_eval.shape[0]\n    n_train = X_train.shape[0]\n    decision_values = np.zeros(n_eval)\n    \n    for i in range(n_eval):\n        s = 0\n        for j in range(n_train):\n            if alphas[j] > 1e-6:\n                s += alphas[j] * y_train[j] * kernel(X_train[j], X_eval[i])\n        decision_values[i] = s + b\n\n    return decision_values\n\ndef solve_case(X, y, C):\n    \"\"\"\n    Performs all 5 required tasks for a single dataset.\n    \"\"\"\n    n_samples, _ = X.shape\n\n    # Kernel function definitions\n    linear_kernel = lambda x, z: np.dot(x, z)\n    quad_kernel = lambda x, z: np.dot(x, z)**2\n\n    # 1. Assess linear separability\n    alphas_lin, b_lin = solve_svm_dual(X, y, C, linear_kernel)\n    dec_vals_lin = get_decision_values(X, X, y, alphas_lin, b_lin, linear_kernel)\n    is_lin_separable = np.all(np.sign(dec_vals_lin) == y)\n\n    # 2. Assess quadratic separability\n    alphas_quad, b_quad = solve_svm_dual(X, y, C, quad_kernel)\n    dec_vals_quad_implicit = get_decision_values(X, X, y, alphas_quad, b_quad, quad_kernel)\n    is_quad_separable = np.all(np.sign(dec_vals_quad_implicit) == y)\n\n    # 3. Verify Gram matrix equality\n    G_implicit = np.array([[quad_kernel(xi, xj) for xj in X] for xi in X])\n    \n    phi_map = lambda x: np.array([x[0]**2, x[1]**2, np.sqrt(2) * x[0] * x[1]])\n    Phi = np.array([phi_map(x_i) for x_i in X])\n    G_explicit = Phi @ Phi.T\n    \n    gram_matrices_equal = np.allclose(G_implicit, G_explicit, atol=1e-10)\n\n    # 4. Verify decision function agreement\n    w_explicit = np.sum(alphas_quad[:, np.newaxis] * y[:, np.newaxis] * Phi, axis=0)\n    dec_vals_quad_explicit = Phi @ w_explicit + b_quad\n    decision_funcs_agree = np.all(np.sign(dec_vals_quad_implicit) == np.sign(dec_vals_quad_explicit))\n    \n    # 5. Compute geometric margin for the quadratic kernel\n    w_norm = np.linalg.norm(w_explicit)\n    # If not separable, the concept of margin is ill-defined. Report 0.\n    # For Dataset C (quadratic), points collapse, w is numerically zero.\n    if not is_quad_separable and w_norm  1e-9:\n        geometric_margin = 0.0\n    else:\n        geometric_margin = 1.0 / w_norm if w_norm > 1e-9 else 0.0\n\n    return [\n        is_lin_separable,\n        is_quad_separable,\n        gram_matrices_equal,\n        decision_funcs_agree,\n        round(geometric_margin, 6)\n    ]\n\ndef solve():\n    \"\"\"\n    Main entry point to run all test cases and print the final result.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Dataset A\n        (np.array([[1.0, 1.0], [1.0, -1.0], [-1.0, 1.0], [-1.0, -1.0]]),\n         np.array([1, -1, -1, 1])),\n        # Dataset B\n        (np.array([[2.0, 2.0], [2.0, 3.0], [-2.0, -2.0], [-2.0, -3.0]]),\n         np.array([1, 1, -1, -1])),\n        # Dataset C\n        (np.array([[1.0, 0.0], [-1.0, 0.0]]),\n         np.array([1, -1]))\n    ]\n    \n    C = 1.0e5\n    final_output_list = []\n    \n    for X, y in test_cases:\n        results = solve_case(X, y, C)\n        \n        # Append results to the final list, formatting as required\n        final_output_list.append(str(results[0]).lower())\n        final_output_list.append(str(results[1]).lower())\n        final_output_list.append(str(results[2]).lower())\n        final_output_list.append(str(results[3]).lower())\n        final_output_list.append(f\"{results[4]:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(final_output_list)}]\")\n\nsolve()\n```", "id": "3178252"}, {"introduction": "我们已经了解了支持向量机优化的目标，但对于大规模数据集，如何高效地找到最优解呢？序列最小最优化 (Sequential Minimal Optimization, SMO) 算法是解决 SVM 对偶问题的标准高效方法。它的核心思想是将一个大的二次规划问题分解为一系列最小的、可以解析求解的子问题——每次只优化两个拉格朗日乘子。通过从头推导一对乘子的更新规则并进行数值计算，您将深入了解 SVM 训练的“底层逻辑”，从而巩固对对偶形式和优化约束的理解 [@problem_id:3178321]。", "problem": "二元软间隔支持向量机 (SVM) 通过在等式约束和箱形约束下最大化对偶目标函数进行训练。设有训练样本对 $\\{(x_{k}, y_{k})\\}_{k=1}^{n}$，其标签 $y_{k} \\in \\{-1, +1\\}$，以及一个半正定核函数 $K(\\cdot,\\cdot)$。对偶变量为 $\\alpha_{k}$，惩罚参数为 $C  0$，对偶问题是在满足所有 $k$ 的约束条件 $0 \\le \\alpha_{k} \\le C$ 以及 $\\sum_{k=1}^{n} \\alpha_{k} y_{k} = 0$ 的情况下，最大化一个以向量 $\\alpha$ 为变量的凹二次目标函数。序列最小最优化 (SMO) 方法更新一对变量 $(\\alpha_{i}, \\alpha_{j})$，同时保持其他坐标固定，确保等式约束和箱形约束仍然得到满足。\n\n仅从上述 SVM 对偶目标和约束条件出发，使用一个限制在仅影响 $\\alpha_{i}$ 和 $\\alpha_{j}$ 的二维子空间内、同时保持等式约束 $\\sum_{k=1}^{n} \\alpha_{k} y_{k} = 0$ 的线搜索方法，推导出 $(\\alpha_{i}, \\alpha_{j})$ 的闭式成对更新规则。该规则是通过沿此可行线优化对偶目标，然后应用箱形约束 $0 \\le \\alpha_{i}, \\alpha_{j} \\le C$ 得出的。用核矩阵元素 $K(x_{i}, x_{i})$、$K(x_{j}, x_{j})$、$K(x_{i}, x_{j})$，标签 $y_{i}, y_{j}$，以及由 $E_{k} = f(x_{k}) - y_{k}$ 定义的当前预测误差 $E_{i}$ 和 $E_{j}$ 来表示你的推导过程，其中 $f$ 是当前的决策函数。\n\n然后，针对以下特定情景（该情景被选为病态的，即二乘二核子矩阵的曲率很小），对一个 SMO 更新步骤进行数值评估：\n- 惩罚项：$C = 1$。\n- 当前变量：$\\alpha_{i} = 0.4$，$\\alpha_{j} = 0.6$。\n- 标签：$y_{i} = +1$，$y_{j} = -1$。\n- 核矩阵元素：$K(x_{i}, x_{i}) = 1$，$K(x_{j}, x_{j}) = 1$，$K(x_{i}, x_{j}) = 0.9999$。\n- 误差：$E_{i} = 0.3$，$E_{j} = -0.4$。\n\n计算在保持等式约束和箱形约束下 $\\alpha_{j}$ 的可行区间的界，执行成对更新，并确定更新后的值 $\\alpha_{i}^{\\text{new}}$ 和 $\\alpha_{j}^{\\text{new}}$。此外，使用你的线搜索推导，提供对偶目标单步改进量 $\\Delta W$（即在此 SMO 步骤中对偶目标的增加量）的闭式表达式，该表达式用曲率参数和实际的裁剪步长表示，并对给定数据进行数值计算。将所有要求的数值输出四舍五入到四位有效数字。\n\n你的最终答案必须是一个单行矩阵，按顺序包含更新后的值 $\\alpha_{i}^{\\text{new}}$、$\\alpha_{j}^{\\text{new}}$ 和对偶目标改进量 $\\Delta W$，四舍五入到四位有效数字，且不带单位。", "solution": "该问题是有效的，因为它是统计学习中一个关于支持向量机 (SVM) 的序列最小最优化 (SMO) 算法的推导和应用的标准、适定的问题。所有提供的信息在科学上都是合理的、自洽且一致的。\n\n### 第 1 部分：SMO 成对更新的推导\n\n软间隔 SVM 的对偶问题是关于对偶变量 $\\alpha_k$ 最大化目标函数 $W(\\alpha)$：\n$$\nW(\\alpha) = \\sum_{k=1}^{n} \\alpha_k - \\frac{1}{2} \\sum_{k=1}^{n} \\sum_{l=1}^{n} \\alpha_k \\alpha_l y_k y_l K(x_k, x_l)\n$$\n约束条件为：\n$$\n\\sum_{k=1}^{n} \\alpha_k y_k = 0\n$$\n$$\n0 \\le \\alpha_k \\le C, \\quad \\text{对于 } k = 1, \\dots, n\n$$\n其中 $K(x_k, x_l)$ 记作 $K_{kl}$。\n\nSMO 选择一对变量，例如 $\\alpha_i$ 和 $\\alpha_j$，进行优化，同时保持所有其他变量 $\\alpha_k$ (对于 $k \\ne i, j$) 固定。设当前（旧）值为 $\\alpha_k^{\\text{old}}$，新值为 $\\alpha_k^{\\text{new}}$。\n\n从等式约束，我们有：\n$$\n\\sum_{k=1}^{n} \\alpha_k^{\\text{new}} y_k = \\alpha_i^{\\text{new}} y_i + \\alpha_j^{\\text{new}} y_j + \\sum_{k \\ne i,j} \\alpha_k^{\\text{old}} y_k = 0\n$$\n由于旧值也满足该约束：\n$$\n\\alpha_i^{\\text{old}} y_i + \\alpha_j^{\\text{old}} y_j + \\sum_{k \\ne i,j} \\alpha_k^{\\text{old}} y_k = 0\n$$\n可得：\n$$\n\\alpha_i^{\\text{new}} y_i + \\alpha_j^{\\text{new}} y_j = \\alpha_i^{\\text{old}} y_i + \\alpha_j^{\\text{old}} y_j = \\gamma\n$$\n其中 $\\gamma$ 在此更新步骤中是一个常数。两边乘以 $y_i$（并利用 $y_i^2 = 1$），我们可以用 $\\alpha_j^{\\text{new}}$ 表示 $\\alpha_i^{\\text{new}}$：\n$$\n\\alpha_i^{\\text{new}} = \\gamma y_i - \\alpha_j^{\\text{new}} y_i y_j\n$$\n令 $s = y_i y_j$。则 $\\alpha_i^{\\text{new}} = \\gamma y_i - s \\alpha_j^{\\text{new}}$。这表明新值必须位于一条直线上。\n\n现在我们沿着这条线优化目标函数 $W$。我们可以将 $W$ 表示为单个变量 $\\alpha_j^{\\text{new}}$ 的函数。为简单起见，我们省略上标 \"new\"。目标函数 $W$ 是 $\\alpha_k$ 的二次函数。将 $\\alpha_i = \\gamma y_i - s \\alpha_j$ 代入 $W$，它变成 $\\alpha_j$ 的二次函数。我们对其二阶导数感兴趣，以找到最大值。$W$ 中涉及 $\\alpha_i$ 和 $\\alpha_j$ 的项是：\n$$\nW_{\\text{sub}} = \\alpha_i + \\alpha_j - \\frac{1}{2} (\\alpha_i^2 K_{ii} + \\alpha_j^2 K_{jj} + 2 \\alpha_i \\alpha_j y_i y_j K_{ij}) + \\dots\n$$\n其中其他项是关于 $\\alpha_i, \\alpha_j$ 的常数项或线性项。$(\\alpha_i, \\alpha_j)$ 中的二次部分是 $-\\frac{1}{2}(\\alpha_i^2 K_{ii} + \\alpha_j^2 K_{jj} + 2 s \\alpha_i \\alpha_j K_{ij})$。代入 $\\alpha_i = \\gamma y_i - s \\alpha_j$：\n$$\n-\\frac{1}{2}((\\gamma y_i - s \\alpha_j)^2 K_{ii} + \\alpha_j^2 K_{jj} + 2s(\\gamma y_i - s \\alpha_j)\\alpha_j K_{ij})\n$$\n$\\alpha_j^2$ 的系数是：\n$$\n-\\frac{1}{2}((-s)^2 K_{ii} + K_{jj} + 2s(-s)K_{ij}) = -\\frac{1}{2}(s^2 K_{ii} + K_{jj} - 2s^2 K_{ij}) = -\\frac{1}{2}(K_{ii} + K_{jj} - 2K_{ij})\n$$\n因为 $s^2 = (y_i y_j)^2 = y_i^2 y_j^2 = 1$。令 $\\eta = K_{ii} + K_{jj} - 2K_{ij}$。$W$ 沿着可行线对 $\\alpha_j$ 的二阶导数是 $\\frac{d^2 W}{d\\alpha_j^2} = -\\eta$。由于核函数 $K$ 是半正定的，相应的 Gram 矩阵也是半正定的，这意味着 $\\eta \\ge 0$。对于 $\\eta  0$，$W$ 是 $\\alpha_j$ 的严格凹二次函数，并有一个唯一的最大值。\n\n为了找到这个最大值的位置，我们将一阶导数设为零。令 $u_k = \\sum_{l=1}^n \\alpha_l y_l K_{lk} = f(x_k) - b$。$W$ 的梯度是 $\\frac{\\partial W}{\\partial \\alpha_k} = 1 - y_k u_k$。$W$ 沿着直线的导数是：\n$$\n\\frac{dW}{d\\alpha_j} = \\frac{\\partial W}{\\partial \\alpha_j} + \\frac{\\partial W}{\\partial \\alpha_i} \\frac{d\\alpha_i}{d\\alpha_j} = (1 - y_j u_j) + (1 - y_i u_i)(-s)\n$$\n其中 $u_i$ 和 $u_j$ 是新 $\\alpha_i, \\alpha_j$ 的函数。令 $\\alpha_k^{\\text{new}} = \\alpha_k^{\\text{old}} + \\Delta\\alpha_k$。则 $\\Delta\\alpha_i = -s \\Delta\\alpha_j$。新的 $u_k$ 值是 $u_k^{\\text{new}} = u_k^{\\text{old}} + \\Delta\\alpha_i y_i K_{ik} + \\Delta\\alpha_j y_j K_{jk}$。\n$u_i^{\\text{new}} = u_i^{\\text{old}} - s \\Delta\\alpha_j y_i K_{ii} + \\Delta\\alpha_j y_j K_{ij} = u_i^{\\text{old}} + \\Delta\\alpha_j y_j (K_{ij} - K_{ii})$。\n$u_j^{\\text{new}} = u_j^{\\text{old}} - s \\Delta\\alpha_j y_i K_{ji} + \\Delta\\alpha_j y_j K_{jj} = u_j^{\\text{old}} + \\Delta\\alpha_j y_j (K_{jj} - K_{ij})$。\n对于线性部分使用旧值，二次部分使用新值，令 $\\frac{dW}{d\\alpha_j} = 0$：\n$0 = \\frac{dW}{d\\alpha_j}|_{\\alpha_j^{\\text{old}}} + \\frac{d^2 W}{d\\alpha_j^2}|_{\\alpha_j^{\\text{old}}} (\\alpha_j^{\\text{new,unc}} - \\alpha_j^{\\text{old}})$。\n在旧值处的导数是 $(1-y_j u_j^{\\text{old}}) - s(1-y_i u_i^{\\text{old}})$。\n使用 $E_k = u_k^{\\text{old}} + b^{\\text{old}} - y_k$，这变为 $y_j(b^{\\text{old}}-E_j^{\\text{old}}) - s y_i(b^{\\text{old}}-E_i^{\\text{old}}) = y_j(b-E_j) - y_j(b-E_i) = y_j(E_i-E_j)$。\n所以，$y_j(E_i-E_j) - \\eta(\\alpha_j^{\\text{new,unc}} - \\alpha_j^{\\text{old}}) = 0$。这给出了 $\\alpha_j$ 的无约束更新：\n$$\n\\alpha_j^{\\text{new, unc}} = \\alpha_j^{\\text{old}} + \\frac{y_j(E_i - E_j)}{\\eta} = \\alpha_j^{\\text{old}} + \\frac{y_j(E_i - E_j)}{K_{ii} + K_{jj} - 2K_{ij}}\n$$\n这个值必须被裁剪以满足箱形约束 $0 \\le \\alpha_i^{\\text{new}} \\le C$ 和 $0 \\le \\alpha_j^{\\text{new}} \\le C$。这些约束为 $\\alpha_j^{\\text{new}}$ 定义了一个可行区间 $[L, H]$。\n如果 $y_i \\ne y_j$ ($s=-1$)，约束是 $\\alpha_i - \\alpha_j = \\alpha_i^{\\text{old}} - \\alpha_j^{\\text{old}}$。界是 $L = \\max(0, \\alpha_j^{\\text{old}} - \\alpha_i^{\\text{old}})$ 和 $H = \\min(C, C + \\alpha_j^{\\text{old}} - \\alpha_i^{\\text{old}})$。\n如果 $y_i = y_j$ ($s=1$)，约束是 $\\alpha_i + \\alpha_j = \\alpha_i^{\\text{old}} + \\alpha_j^{\\text{old}}$。界是 $L = \\max(0, \\alpha_i^{\\text{old}} + \\alpha_j^{\\text{old}} - C)$ 和 $H = \\min(C, \\alpha_i^{\\text{old}} + \\alpha_j^{\\text{old}})$。\n$\\alpha_j$ 的最终更新值是：\n$$\n\\alpha_j^{\\text{new}} = \\begin{cases} H  \\text{如果 } \\alpha_j^{\\text{new, unc}}  H \\\\ \\alpha_j^{\\text{new, unc}}  \\text{如果 } L \\le \\alpha_j^{\\text{new, unc}} \\le H \\\\ L  \\text{如果 } \\alpha_j^{\\text{new, unc}}  L \\end{cases}\n$$\n$\\alpha_i^{\\text{new}}$ 的值然后通过线性约束找到：$\\alpha_i^{\\text{new}} = \\alpha_i^{\\text{old}} + s(\\alpha_j^{\\text{old}} - \\alpha_j^{\\text{new}})$。\n\n### 第 2 部分：目标改进量 $\\Delta W$ 的推导\n\n目标函数的变化量 $\\Delta W = W(\\alpha^{\\text{new}}) - W(\\alpha^{\\text{old}})$，可以通过将 $W$ 在 $\\alpha^{\\text{old}}$ 附近进行二阶泰勒展开得到。变化向量是 $\\Delta\\vec{\\alpha}$，其非零分量为 $\\Delta\\alpha_i$ 和 $\\Delta\\alpha_j$。\n$$\n\\Delta W \\approx \\nabla W(\\alpha^{\\text{old}})^T \\Delta\\vec{\\alpha} + \\frac{1}{2} \\Delta\\vec{\\alpha}^T \\mathbf{H} \\Delta\\vec{\\alpha}\n$$\n其中 $\\mathbf{H}$ 是 $W$ 的 Hessian 矩阵，其元素为 $H_{kl} = -y_k y_l K_{kl}$。由于 $W$ 是二次函数，这个展开是精确的。\n梯度项是：$\\frac{\\partial W}{\\partial \\alpha_i} \\Delta\\alpha_i + \\frac{\\partial W}{\\partial \\alpha_j} \\Delta\\alpha_j$。\n在旧值处，根据 $\\Delta\\alpha_i = -s \\Delta\\alpha_j$ 和 $\\frac{\\partial W}{\\partial \\alpha_k} = y_k(b-E_k)$，该项变为：\n$y_i(b-E_i)(-s\\Delta\\alpha_j) + y_j(b-E_j)\\Delta\\alpha_j = \\Delta\\alpha_j[-y_j(b-E_i) + y_j(b-E_j)] = \\Delta\\alpha_j y_j(E_i-E_j)$。\nHessian 项是 $\\frac{1}{2}(\\Delta\\alpha_i^2 H_{ii} + \\Delta\\alpha_j^2 H_{jj} + 2\\Delta\\alpha_i\\Delta\\alpha_j H_{ij})$\n$= \\frac{1}{2}((-s\\Delta\\alpha_j)^2(-K_{ii}) + (\\Delta\\alpha_j)^2(-K_{jj}) + 2(-s\\Delta\\alpha_j)(\\Delta\\alpha_j)(-sK_{ij}))$\n$= \\frac{1}{2}(\\Delta\\alpha_j)^2 (-K_{ii} - K_{jj} + 2K_{ij}) = -\\frac{1}{2} \\eta (\\Delta\\alpha_j)^2$。\n因此，目标改进量是：\n$$\n\\Delta W = \\Delta \\alpha_j y_j(E_i - E_j) - \\frac{1}{2} \\eta (\\Delta \\alpha_j)^2\n$$\n其中 $\\Delta\\alpha_j = \\alpha_j^{\\text{new}} - \\alpha_j^{\\text{old}}$ 是 $\\alpha_j$ 的实际、裁剪后的变化量。\n\n### 第 3 部分：数值评估\n\n给定数据：\n- $C = 1$。\n- $\\alpha_i^{\\text{old}} = 0.4$, $\\alpha_j^{\\text{old}} = 0.6$。\n- $y_i = +1$, $y_j = -1$。\n- $K_{ii} = 1$, $K_{jj} = 1$, $K_{ij} = 0.9999$。\n- $E_i = 0.3$, $E_j = -0.4$。\n\n1.  **计算参数**：\n    $s = y_i y_j = (+1)(-1) = -1$。\n    由于 $y_i \\ne y_j$，我们使用相应的 $L$ 和 $H$ 的界：\n    $L = \\max(0, \\alpha_j^{\\text{old}} - \\alpha_i^{\\text{old}}) = \\max(0, 0.6 - 0.4) = \\max(0, 0.2) = 0.2$。\n    $H = \\min(C, C + \\alpha_j^{\\text{old}} - \\alpha_i^{\\text{old}}) = \\min(1, 1 + 0.6 - 0.4) = \\min(1, 1.2) = 1$。\n    $\\alpha_j^{\\text{new}}$ 的可行区间是 $[0.2, 1]$。\n    $\\eta = K_{ii} + K_{jj} - 2K_{ij} = 1 + 1 - 2(0.9999) = 2 - 1.9998 = 0.0002$。\n\n2.  **计算未裁剪的更新**：\n    $E_i - E_j = 0.3 - (-0.4) = 0.7$。\n    $\\alpha_j^{\\text{new, unc}} = \\alpha_j^{\\text{old}} + \\frac{y_j(E_i - E_j)}{\\eta} = 0.6 + \\frac{(-1)(0.7)}{0.0002} = 0.6 - \\frac{0.7}{0.0002} = 0.6 - 3500 = -3499.4$。\n\n3.  **裁剪 $\\alpha_j^{\\text{new}}$ 并计算 $\\alpha_i^{\\text{new}}$**：\n    由于 $\\alpha_j^{\\text{new, unc}} = -3499.4  L = 0.2$，我们裁剪该值：\n    $\\alpha_j^{\\text{new}} = L = 0.2$。\n    现在我们求出对应的 $\\alpha_i^{\\text{new}}$：\n    $\\alpha_i^{\\text{new}} = \\alpha_i^{\\text{old}} + s(\\alpha_j^{\\text{old}} - \\alpha_j^{\\text{new}}) = 0.4 + (-1)(0.6 - 0.2) = 0.4 - 0.4 = 0$。\n    四舍五入到四位有效数字，$\\alpha_i^{\\text{new}} = 0.0000$ 且 $\\alpha_j^{\\text{new}} = 0.2000$。\n\n4.  **计算目标改进量 $\\Delta W$**：\n    $\\alpha_j$ 的实际变化量是 $\\Delta\\alpha_j = \\alpha_j^{\\text{new}} - \\alpha_j^{\\text{old}} = 0.2 - 0.6 = -0.4$。\n    使用推导出的 $\\Delta W$ 公式：\n    $\\Delta W = \\Delta \\alpha_j y_j(E_i - E_j) - \\frac{1}{2} \\eta (\\Delta \\alpha_j)^2$\n    $\\Delta W = (-0.4) \\times (-1) \\times (0.7) - \\frac{1}{2} (0.0002) (-0.4)^2$\n    $\\Delta W = (0.4)(0.7) - (0.0001)(0.16)$\n    $\\Delta W = 0.28 - 0.000016 = 0.279984$。\n    四舍五入到四位有效数字，$\\Delta W = 0.2800$。", "answer": "$$\n\\boxed{\\begin{pmatrix} 0.0000  0.2000  0.2800 \\end{pmatrix}}\n$$", "id": "3178321"}]}