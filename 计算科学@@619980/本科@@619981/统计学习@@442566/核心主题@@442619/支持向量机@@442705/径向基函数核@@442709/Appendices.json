{"hands_on_practices": [{"introduction": "径向基函数（RBF）核支持向量机（SVM）的强大功能来自于其能够塑造灵活的非线性决策边界，但这也使其在训练过程中容易出现过拟合。本练习将引导你分析一个典型的过拟合案例，即模型在训练集上表现完美，但在测试集上性能等同于随机猜测。通过这个思想实验，你将深入理解核函数宽度参数$\\gamma$和正则化参数$C$是如何影响模型复杂度和泛化能力的，并学会诊断模型失效的关键原因 [@problem_id:2433181]。", "problem": "一个研究团队正在构建一个支持向量机 (SVM) 分类器，用于根据序列衍生特征（如$k$-mer 频率、预测的二级结构比例和 Pfam 域计数）来预测蛋白质功能。该数据集包含$n=2000$个蛋白质，并被划分为一个类别均衡的分层训练/测试集。一个带有径向基函数 (RBF) 核的 SVM 使用标准特征缩放进行训练。该模型在训练集上达到了$99\\%$的准确率，但在测试集上仅达到了$50\\%$的准确率。\n\n根据带松弛变量的 SVM 最小化一个在最大间隔分离和训练误差之间进行权衡的目标函数的基本定义：\n$$\n\\min_{\\mathbf{w},b,\\boldsymbol{\\xi}} \\ \\frac{1}{2}\\lVert \\mathbf{w}\\rVert^2 + C \\sum_{i=1}^{n} \\xi_i \n\\quad \\text{subject to} \\quad y_i\\left(\\mathbf{w}^\\top \\phi(\\mathbf{x}_i) + b\\right) \\ge 1 - \\xi_i,\\ \\xi_i \\ge 0,\n$$\n以及核技巧通过一个核函数$k(\\mathbf{x},\\mathbf{x}')$替换内积$\\langle \\phi(\\mathbf{x}), \\phi(\\mathbf{x}') \\rangle$，其中 RBF 核定义为：\n$$\nk(\\mathbf{x},\\mathbf{x}') = \\exp\\!\\left(-\\gamma \\lVert \\mathbf{x} - \\mathbf{x}' \\rVert^2\\right),\n$$\n请针对此生物信息学任务，就泛化与训练拟合进行推理。\n\n哪个超参数最可能是造成所观察到的差距的原因，为什么？\n\nA. 正则化参数$C$太大，导致模型严重惩罚训练误差，缩小了间隔，并过拟合了训练数据。\n\nB. 正则化参数$C$太小，导致模型欠拟合；这解释了$99\\%$的训练准确率但$50\\%$的测试准确率。\n\nC. RBF 核宽度参数$\\gamma$太大，导致$k(\\mathbf{x},\\mathbf{x}')$变得高度局部化，决策函数变得过于复杂，实际上是记忆了训练集。\n\nD. RBF 核宽度参数$\\gamma$太小，导致核函数变得过于宽泛且近似线性；这解释了$99\\%$的训练准确率但$50\\%$的测试准确率。", "solution": "对问题陈述进行验证。\n\n**步骤 1：提取已知信息**\n- 机器学习任务：用于蛋白质功能预测的支持向量机 (SVM) 分类器。\n- 特征：$k$-mer 频率、预测的二级结构比例、Pfam 域计数。\n- 数据集大小：$n=2000$ 个蛋白质。\n- 数据划分：分层的训练/测试集划分，类别均衡。\n- 模型：使用径向基函数 (RBF) 核的 SVM，采用标准特征缩放。\n- 性能：训练集准确率$99\\%$，测试集准确率$50\\%$。\n- SVM 目标函数：$\\min_{\\mathbf{w},b,\\boldsymbol{\\xi}} \\ \\frac{1}{2}\\lVert \\mathbf{w}\\rVert^2 + C \\sum_{i=1}^{n} \\xi_i$。\n- SVM 约束条件：$y_i\\left(\\mathbf{w}^\\top \\phi(\\mathbf{x}_i) + b\\right) \\ge 1 - \\xi_i$ 且 $\\xi_i \\ge 0$。\n- 核技巧：内积$\\langle \\phi(\\mathbf{x}), \\phi(\\mathbf{x}') \\rangle$被核函数$k(\\mathbf{x},\\mathbf{x}')$替代。\n- RBF 核定义：$k(\\mathbf{x},\\mathbf{x}') = \\exp\\!\\left(-\\gamma \\lVert \\mathbf{x} - \\mathbf{x}' \\rVert^2\\right)$。\n- 问题：找出最可能导致观察到的性能差距的超参数，并解释原因。\n\n**步骤 2：使用提取的已知信息进行验证**\n该问题具有科学依据，描述了计算生物学和机器学习中的一个标准且现实的场景。SVM 目标函数和 RBF 核的定义在数学上是正确的。观察到的现象——训练准确率（$99\\%$）和测试准确率（$50\\%$）之间的巨大差异——是严重过拟合的典型案例。对于一个类别均衡的二分类任务，$50\\%$ 的测试准确率等同于随机猜测，表明模型完全无法泛化。该问题定义明确，要求基于超参数$C$和$\\gamma$的作用，对这种过拟合的原因进行合乎逻辑的推断。问题陈述是自洽的、客观的且内部一致的。\n\n**步骤 3：结论与行动**\n该问题有效。将推导出解答。\n\n问题的核心是训练准确率（$99\\%$）和测试准确率（$50\\%$）之间的巨大差距。这是一个严重过拟合的教科书式案例，即模型过于完美地学习了训练数据（包括噪声），以至于无法泛化到未见过的数据上。在类别均衡的情况下，$50\\%$ 的测试准确率表明模型对新数据的预测能力不比随机猜测好。我们必须分析两个超参数$C$和$\\gamma$的作用，以确定最可能的原因。\n\n参数$C$是正则化参数。它控制着在最大化间隔和最小化训练集上的分类误差之间的权衡。\n- 大的$C$会对错分的训练样本（即那些$\\xi_i > 0$的样本）施加高惩罚。这会迫使优化器去寻找一个能够尽可能多地正确分类训练样本的决策边界，即使这需要一个复杂的边界和较小的间隔。因此，大的$C$会鼓励过拟合。\n- 小的$C$会施加较小的惩罚，允许更多的训练样本被错分以换取更大的间隔。这会导致一个更简单、“更软”的决策边界，如果$C$太小，可能会导致欠拟合。\n\nRBF 核$k(\\mathbf{x},\\mathbf{x}') = \\exp(-\\gamma \\lVert \\mathbf{x} - \\mathbf{x}' \\rVert^2)$中的参数$\\gamma$定义了单个训练样本的影响范围。\n- 小的$\\gamma$会导致每个支持向量的影响半径较大，因为核函数值随距离增加而缓慢减小。得到的决策边界是平滑的，其行为类似于线性分类器。一个非常小的$\\gamma$可能导致欠拟合。\n- 大的$\\gamma$会导致影响半径非常小。即使对于离支持向量中等距离的点，核函数值也会迅速下降到接近零。这意味着决策函数只受支持向量紧邻区域内的点的影响。得到的决策边界变得高度复杂和非线性，基本上是围绕训练样本的一系列小的决策区域“岛屿”。这使得模型能够“记忆”训练集，导致极端过拟合。\n\n鉴于性能指标——近乎完美的训练准确率和随机猜测水平的测试准确率——模型不仅是过拟合，而且是完全没有学到可泛化的模式。虽然大的$C$通过惩罚训练误差会助长过拟合，但一个非常大的$\\gamma$为观察到的极端“记忆”行为提供了机制。当$\\gamma$很大时，每个训练点都可以成为其自己的支持向量，在其周围创建一个局部化的决策区域。这完美地解释了模型如何在训练集上达到$99\\%$ 的准确率，而对于那些没有落在某个训练点极近位置的测试点却毫无预测能力。因此，一个过大的$\\gamma$是对这种特定的、灾难性的失败模式最直接且有说服力的解释。\n\n选项评估：\n\nA. 正则化参数$C$太大，导致模型严重惩罚训练误差，缩小了间隔，并过拟合了训练数据。\n这个陈述事实上是正确的。大的$C$确实会导致过拟合。然而，它不能像$\\gamma$的效应那样直接地解释性能崩塌至$50\\%$（随机概率）的极端性。它是一个促成因素，但对于这个特定的结果来说，可能不是主要或影响最大的因素。\n\nB. 正则化参数$C$太小，导致模型欠拟合；这解释了$99\\%$的训练准确率但$50\\%$的测试准确率。\n这个陈述是矛盾的。小的$C$会导致欠拟合，这会表现为低的训练准确率，而不是$99\\%$。因此，该选项是**错误的**。\n\nC. RBF 核宽度参数$\\gamma$太大，导致$k(\\mathbf{x},\\mathbf{x}')$变得高度局部化，决策函数变得过于复杂，实际上是记忆了训练集。\n这个陈述准确地描述了大的$\\gamma$的效应。高度的局部化导致模型能够完美拟合训练数据的特定排列，从而得到近乎完美的训练准确率。同样的复杂性导致了泛化能力的完全丧失，产生了不比随机猜测更好的测试准确率。这是对观察到的性能最精确的解释。该选项是**正确的**。\n\nD. RBF 核宽度参数$\\gamma$太小，导致核函数变得过于宽泛且近似线性；这解释了$99\\%$的训练准确率但$50\\%$的测试准确率。\n这个陈述是矛盾的。小的$\\gamma$导致一个更简单、近似线性的模型，如果真实边界是复杂的，这会导致欠拟合。它将无法在复杂数据集上达到$99\\%$ 的训练准确率。因此，该选项是**错误的**。\n\n比较 A 和 C，选项 C 为观察到的极端过拟合情况提供了更强大、更具体的解释。由大$\\gamma$引起的“记忆”效应是泛化能力完全崩塌至随机猜测水平的最可能原因。", "answer": "$$\\boxed{C}$$", "id": "2433181"}, {"introduction": "RBF核函数的核心是欧几里得距离，这意味着它对输入特征的尺度（scale）非常敏感，这是一个在实践中至关重要的特性。在这个练习中，你将探索当不同特征的数值范围存在巨大差异（例如，基因表达数据）且未进行归一化时，RBF核SVM的决策边界会发生怎样的剧烈变化。这项实践旨在揭示为何特征缩放对于所有基于距离的机器学习算法而言，不仅仅是“良好实践”，而往往是获得有意义结果的必要步骤 [@problem_id:2433217]。", "problem": "您正在构建一个二元分类器，用于使用每个样本具有$p$个特征的基因表达谱来区分肿瘤样本与正常样本，样本表示为$\\mathbf{x} \\in \\mathbb{R}^p$。原始数据包含异构的尺度：一些基因的原始计数值在$10^3$到$10^4$的数量级，而另一些基因的归一化值接近 0 或在$[0,1]$范围内。您使用径向基函数 (RBF) 核训练了一个支持向量机 (SVM)，也就是由下式定义的高斯核：\n$$\nk(\\mathbf{x}, \\mathbf{z}) \\;=\\; \\exp\\!\\big(-\\gamma \\,\\lVert \\mathbf{x} - \\mathbf{z} \\rVert^2\\big),\n$$\n其中核参数$\\gamma > 0$和正则化参数$C > 0$是固定的。您在训练前忘记了对特征进行归一化或标准化。\n\n哪个陈述最能描述这对在原始输入空间中学到的决策边界的影响？\n\nA. 决策边界不变，因为核技巧使分类器对特征缩放不敏感。\n\nB. 决策边界被高数量级的基因主导：欧几里得距离由这些坐标控制，使得$k(\\mathbf{x}, \\mathbf{z})$接近 0，除非样本在这些基因上的值极其接近。分类器变得非常局部化，并沿着高尺度方向高度扭曲，同时有效忽略了低尺度的基因。\n\nC. 决策边界简化为单个线性超平面，因为大数量级特征导致 RBF 核在样本间近似为常数。\n\nD. 正则化隐式地重新缩放了特征，因此决策边界的效果与所有基因都事先进行标准化的情况相同。", "solution": "首先，我们必须验证问题陈述的有效性。\n\n该问题描述了一个使用支持向量机 (SVM) 对基因表达数据进行二元分类的任务。给定条件如下：\n- 输入数据：向量$\\mathbf{x} \\in \\mathbb{R}^p$，其中$p$是特征（基因）的数量。\n- 特征尺度：高度异构，一些特征的数量级在$10^3$到$10^4$之间，而另一些则在$[0, 1]$范围内。\n- 模型：带有径向基函数 (RBF) 核的支持向量机。\n- 核定义：$k(\\mathbf{x}, \\mathbf{z}) = \\exp(-\\gamma \\lVert \\mathbf{x} - \\mathbf{z} \\rVert^2)$，其中$\\gamma > 0$是一个固定参数。\n- 正则化：一个固定的参数$C > 0$。\n- 关键条件：在训练前未对特征进行归一化或标准化。\n- 问题：描述这种缺乏缩放对学习到的决策边界的影响。\n\n问题陈述具有科学依据、提法恰当且客观。它描述了在机器学习算法（特别是像核SVM这样基于距离的方法）的实际应用中一个常见且关键的问题。整个设定是自洽的，没有矛盾或歧义。所用术语在计算生物学和机器学习领域是标准的。因此，该问题是有效的，我们可以着手解答。\n\n分析的核心在于 RBF 核的自变量，即两个数据点之间的平方欧几里得距离$\\lVert \\mathbf{x} - \\mathbf{z} \\rVert^2$。该距离计算为所有特征上的总和：\n$$\n\\lVert \\mathbf{x} - \\mathbf{z} \\rVert^2 = \\sum_{i=1}^{p} (x_i - z_i)^2\n$$\n让我们将特征索引集$\\{1, 2, \\dots, p\\}$划分为两个不相交的集合：$I_{high}$表示高数量级特征，$I_{low}$表示低数量级特征。\n$I_{high}$中的特征值在$10^3$到$10^4$的数量级。这些特征中一个很小的相对差异可能导致一个非常大的绝对差异。例如，如果$x_j, z_j \\in I_{high}$仅相差$1\\%$, 比如$x_j=5000$和$z_j=5050$，那么差的平方是$(x_j - z_j)^2 = 50^2 = 2500$。\n相比之下，对于$I_{low}$中值在$[0, 1]$范围内的特征，可能的最大平方差是$(1-0)^2 = 1$。\n\n总的平方欧几里得距离可以写成：\n$$\n\\lVert \\mathbf{x} - \\mathbf{z} \\rVert^2 = \\sum_{j \\in I_{high}} (x_j - z_j)^2 + \\sum_{k \\in I_{low}} (x_k - z_k)^2\n$$\n显而易见，这个和完全由高数量级特征的项主导。相比之下，低数量级特征的贡献在数值上可以忽略不计。对于任意两个在高尺度维度上不是极其接近的不同点$\\mathbf{x}$和$\\mathbf{z}$，项$\\sum_{j \\in I_{high}} (x_j - z_j)^2$将会是一个非常大的正数。因此，整个平方距离$\\lVert \\mathbf{x} - \\mathbf{z} \\rVert^2$将会很大。\n\n现在，考虑核函数$k(\\mathbf{x}, \\mathbf{z}) = \\exp(-\\gamma \\lVert \\mathbf{x} - \\mathbf{z} \\rVert^2)$。由于$\\gamma > 0$且对于大多数点对$(\\mathbf{x}, \\mathbf{z})$，$\\lVert \\mathbf{x} - \\mathbf{z} \\rVert^2$很大，指数$-\\gamma \\lVert \\mathbf{x} - \\mathbf{z} \\rVert^2$将会是一个大幅值的负数。这导致：\n$$\nk(\\mathbf{x}, \\mathbf{z}) \\approx \\exp(-\\text{large positive number}) \\approx 0\n$$\n核函数（衡量一种“相似性”的度量）对于任何一对点，其计算结果都将几乎为零，除非它们彼此异常接近，特别是在对应于高数量级特征的维度上。\n\n对于一个新点$\\mathbf{u}$，决策函数由$f(\\mathbf{u}) = \\sum_{i \\in \\text{SV}} \\alpha_i y_i k(\\mathbf{x}_i, \\mathbf{u}) + b$给出，其中$\\mathbf{x}_i$是支持向量，$y_i \\in \\{-1, 1\\}$是它们的标签，$\\alpha_i$是学习到的权重。每个支持向量$\\mathbf{x}_i$对点$\\mathbf{u}$预测的影响是由核函数值$k(\\mathbf{x}_i, \\mathbf{u})$介导的。由于除非$\\mathbf{u}$位于$\\mathbf{x}_i$的一个非常小的邻域内，否则该值接近于零，因此每个支持向量都有一个高度局部化的影响场。这迫使决策边界成为这些小影响范围的复杂拼凑物。分类器实质上会试图通过创建一个对高尺度特征的微小变化非常敏感的、高度扭曲的边界来“记忆”训练数据，同时忽略低尺度特征中包含的任何信息。\n\n现在我们评估给出的选项：\n\n**A. 决策边界不变，因为核技巧使分类器对特征缩放不敏感。**\n这个陈述根本上是错误的。RBF 核对欧几里得距离的依赖使其对输入特征的尺度高度敏感。对缩放的不变性不是核技巧的一个普遍属性。某些核（如线性核）可能对统一缩放不变，但 RBF 核是需要特征归一化才能得到有意义结果的典型例子。\n结论：**错误**。\n\n**B. 决策边界被高数量级的基因主导：欧几里得距离由这些坐标控制，使得$k(\\mathbf{x}, \\mathbf{z})$接近 0，除非样本在这些基因上的值极其接近。分类器变得非常局部化，并沿着高尺度方向高度扭曲，同时有效忽略了低尺度的基因。**\n这个陈述准确地总结了我们的推导。高数量级的特征主导了欧几里得距离。这使得 RBF 核函数值对于除了高维特征子空间中最近邻之外的所有点都趋近于 0。这种影响的局部化导致了一个高度复杂、扭曲和过拟合的决策边界，它实际上对低数量级的特征是“盲目”的。\n结论：**正确**。\n\n**C. 决策边界简化为单个线性超平面，因为大数量级特征导致 RBF 核在样本间近似为常数。**\n这与正确的效果相反。核函数值确实近似为常数，但这个常数是 0，而不是一个非零常数。一个非零常数核（或者更准确地说，一个所有非对角线元素都相似且接近对角线元素的核矩阵）只会在$\\gamma$极小的情况下出现，这会使 SVM 表现得像一个线性分类器。对于固定的$\\gamma > 0$，大数量级的特征具有相反的效果：它们使核函数变得高度非线性和局部化。\n结论：**错误**。\n\n**D. 正则化隐式地重新缩放了特征，因此决策边界的效果与所有基因都事先进行标准化的情况相同。**\n这个陈述表明了对正则化参数$C$作用的误解。在 SVM 中，$C$控制对误分类训练样本的惩罚。它在间隔大小和训练误差之间进行权衡。它不执行任何形式的特征缩放，无论是显式的还是隐式的。对偶公式中的权重$\\alpha_i$与数据点（支持向量）相关联，而不是与特征相关联。因此，正则化不能弥补特征归一化的缺失。\n结论：**错误**。", "answer": "$$\\boxed{B}$$", "id": "2433217"}, {"introduction": "在理论模型与实际计算之间，常常存在着数值稳定性的挑战，尤其是在处理核矩阵这类大型数值对象时。通过这个编码练习，你将不再局限于理论层面，而是亲手验证极端的$\\gamma$值或数据集中存在的重复点如何导致核矩阵变得“病态”（ill-conditioned），从而使得依赖于它的学习算法在数值上不稳定甚至失效。这项实践将帮助你理解“加噪”（jitter）等正则化技术在保障算法鲁棒性方面的实际作用，是连接抽象数学概念与稳健代码实现的关键一环 [@problem_id:3165648]。", "problem": "您将分析在高斯径向基函数（RBF）核矩阵在极端带宽参数选择下的数值稳定性。对于数据点$\\{x_i\\}_{i=1}^n \\subset \\mathbb{R}^d$，高斯核定义为\n$$\nK_{ij} = \\exp\\!\\left(-\\gamma \\,\\lVert x_i - x_j \\rVert_2^2\\right),\n$$\n其中$\\gamma > 0$是逆长度尺度。在某些配置下，核矩阵可能会变得数值病态，这会导致诸如核岭回归等统计学习算法中的线性代数运算不稳定。您的目标是识别失效模式并测试两种正则化策略：添加一个小的对角抖动（jitter）和通过先验区间限制参数$\\gamma$。\n\n将使用的基本原理和定义：\n- 核矩阵$K$是对称半正定的。线性求解的数值稳定性由谱条件数$\\kappa_2(K) = \\sigma_{\\max}(K) / \\sigma_{\\min}(K)$来表征，其中$\\sigma_{\\max}$和$\\sigma_{\\min}$分别是奇异值分解（SVD）得到的最大和最小奇异值。\n- 为避免在有限精度算术中除以小于机器精度的值，请使用截断的条件数估计器\n$$\n\\kappa_\\delta(K) = \\frac{\\sigma_{\\max}(K)}{\\max\\{\\sigma_{\\min}(K),\\delta\\}},\n$$\n其中$\\delta = 10^{-15}$。以浮点数形式报告$\\log_{10} \\kappa_\\delta(K)$。\n- 通过最小奇异值定义一个近奇异指示器：如果$\\sigma_{\\min}(K)  \\tau$，则矩阵$K$被声明为近奇异，阈值$\\tau = 10^{-12}$。\n- 抖动正则化添加了单位矩阵的倍数：对于$\\epsilon  0$，定义$K_\\epsilon = K + \\epsilon I_n$，这将所有特征值移动了$\\epsilon$。\n- 有界$\\gamma$先验通过$\\gamma_{\\text{clip}} = \\min\\{\\max\\{\\gamma,\\gamma_{\\min}\\},\\gamma_{\\max}\\}$将提议的$\\gamma$值截断到一个区间$[\\gamma_{\\min},\\gamma_{\\max}]$内。\n\n数据：\n- 数据集$\\mathcal{A}$（包含完全重复的点）：$n=6$，$d=2$，点为\n$$\nx_1=(0,0),\\; x_2=(1,0),\\; x_3=(0,1),\\; x_4=(1,1),\\; x_5=(0.1,0.1),\\; x_6=(0.1,0.1).\n$$\n- 数据集$\\mathcal{B}$（所有点都不同）：$n=6$，$d=2$，点为\n$$\nx_1=(0,0),\\; x_2=(1,0),\\; x_3=(0,1),\\; x_4=(1,1),\\; x_5=(0.1,0.1),\\; x_6=(0.9,0.9).\n$$\n\n正则化参数：\n- 近奇异阈值：$\\tau = 10^{-12}$。\n- 条件数截断值：$\\delta = 10^{-15}$。\n- 抖动水平：$\\epsilon = 10^{-6}$。\n- 有界$\\gamma$先验区间：$\\gamma_{\\min} = 10^{-3}$，$\\gamma_{\\max} = 10^{3}$。\n\n测试套件：\n对于每个项目，精确计算指定的量。\n1. 在数据集$\\mathcal{A}$上，使用$\\gamma = 10^{-12}$且无抖动，使用阈值$\\tau$报告近奇异布尔值。\n2. 在数据集$\\mathcal{A}$上，使用$\\gamma = 1$且无抖动，使用阈值$\\tau$报告近奇异布尔值。\n3. 在数据集$\\mathcal{A}$上，使用$\\gamma = 1$和抖动$\\epsilon = 10^{-6}$，以浮点数形式报告$\\log_{10}\\kappa_\\delta(K_\\epsilon)$。\n4. 在数据集$\\mathcal{B}$上，使用$\\gamma = 10^{-12}$且无抖动，以浮点数形式报告$\\log_{10}\\kappa_\\delta(K)$。\n5. 在数据集$\\mathcal{B}$上，对于候选集$\\{\\gamma\\} = \\{10^{-12}, 10^{-3}, 1, 10^{3}, 10^{12}\\}$，应用有界$\\gamma$先验，区间为$[\\gamma_{\\min},\\gamma_{\\max}] = [10^{-3}, 10^{3}]$，无抖动，并在截断后以浮点数形式报告最坏情况（集合中的最大值）的$\\log_{10}\\kappa_\\delta(K)$。\n6. 在数据集$\\mathcal{B}$上，对于相同的候选集$\\{\\gamma\\}$，不应用有界$\\gamma$先验，但为每个核添加抖动$\\epsilon = 10^{-6}$；以浮点数形式报告最坏情况（集合中的最大值）的$\\log_{10}\\kappa_\\delta(K_\\epsilon)$。\n7. 在数据集$\\mathcal{B}$上，使用$\\gamma = 10^{6}$且无抖动，以浮点数形式报告$\\log_{10}\\kappa_\\delta(K)$。\n\n您的程序必须生成单行输出，其中包含一个逗号分隔的列表，用方括号括起来，并严格按以下顺序排列结果：\n[result1,result2,result3,result4,result5,result6,result7]\n所有布尔值必须不带引号，所有浮点数必须以标准 Python 浮点格式打印。不需要用户输入，也不得读取或写入任何文件。", "solution": "该问题要求分析高斯径向基函数（RBF）核矩阵 $K$ 在不同逆长度尺度参数 $\\gamma$ 选择下的数值稳定性。稳定性由矩阵的谱条件数来量化。我们将研究病态条件的两个主要来源以及两种相应的正则化策略。\n\n首先，我们建立基本原理。两个点 $x_i, x_j \\in \\mathbb{R}^d$ 之间的高斯 RBF 核由下式给出\n$$\nK_{ij} = \\exp(-\\gamma \\lVert x_i - x_j \\rVert_2^2)\n$$\n其中 $\\gamma  0$。得到的 $n \\times n$ 矩阵 $K$ 是对称且半正定的。求解涉及 $K$ 的线性系统（例如在支持向量机或核岭回归中）的数值稳定性取决于其谱条件数 $\\kappa_2(K) = \\sigma_{\\max}(K) / \\sigma_{\\min}(K)$，其中 $\\sigma_{\\max}$ 和 $\\sigma_{\\min}$ 是 $K$ 的最大和最小奇异值。由于 $K$ 是对称半正定的，其奇异值就是其特征值。大的条件数表明矩阵接近奇异，并且涉及它的数值计算可能不稳定。问题定义了该度量的一个截断版本，$\\kappa_\\delta(K) = \\sigma_{\\max}(K) / \\max\\{\\sigma_{\\min}(K), \\delta\\}$，并使用一个下限 $\\delta = 10^{-15}$ 来处理低于机器精度的值。\n\n高斯核矩阵的两种主要失效模式是：\n1.  **$\\gamma$ 的选择**：\n    - 当 $\\gamma \\to 0$ 时，指数 $-\\gamma \\lVert x_i - x_j \\rVert_2^2 \\to 0$。因此，每个元素 $K_{ij} \\to \\exp(0) = 1$。核矩阵 $K$ 趋近于全 1 矩阵 $J_n$。该矩阵的秩为 1，有一个等于 $n$ 的特征值和 $n-1$ 个等于 0 的特征值。因此，对于非常小的 $\\gamma$，$K$ 会变得近奇异，导致条件数极大。\n    - 当 $\\gamma \\to \\infty$ 时，对于不同的点 $x_i \\neq x_j$，指数 $-\\gamma \\lVert x_i - x_j \\rVert_2^2 \\to -\\infty$，因此非对角元素 $K_{ij} \\to 0$。对角元素保持为 $K_{ii} = \\exp(0)=1$。因此，$K$ 趋近于单位矩阵 $I_n$，这是完美条件的（$\\kappa_2(I_n) = 1$）。然而，如果点非常接近，非对角线项可能不会足够快地衰减到零，或者如果达到机器精度限制，它们可能在数值上变为零，从而可能孤立点并影响谱。\n\n2.  **数据几何形状**：如果数据集包含重复的点，例如，对于 $i \\neq k$ 有 $x_i = x_k$，那么核矩阵的第 i 行和第 k 行（以及列）将变得相同。这种线性相关性使得对于任何 $\\gamma  0$，矩阵 $K$ 都是奇异的，这意味着其最小奇异值恰好为 0。\n\n为缓解这些问题，提出了两种正则化策略：\n1.  **抖动正则化**：在 $K$ 的对角线上添加一个小的正项 $\\epsilon$，得到 $K_\\epsilon = K + \\epsilon I_n$。这将 $K$ 的所有特征值移动了 $\\epsilon$。如果 $K$ 的特征值为 $\\lambda_1 \\ge \\dots \\ge \\lambda_n \\ge 0$，则 $K_\\epsilon$ 的特征值为 $\\lambda_i + \\epsilon$。这保证了 $K_\\epsilon$ 的最小特征值至少为 $\\epsilon$，从而限制了条件数。\n2.  **有界 $\\gamma$ 先验**：此方法将 $\\gamma$ 限制在一个预定义的区间 $[\\gamma_{\\min}, \\gamma_{\\max}]$ 内。通过截断候选的 $\\gamma$ 值，它防止了使用已知会导致病态条件的极端值。\n\n我们现在根据这些原则评估七个测试用例。\n\n**测试用例 1：在数据集 $\\mathcal{A}$ 上，使用 $\\gamma = 10^{-12}$ 且无抖动，报告近奇异布尔值。**\n数据集 $\\mathcal{A}$ 包含重复点：$x_5 = x_6 = (0.1, 0.1)$。这意味着对于任何 $\\gamma  0$，核矩阵 $K$ 的第 5 行和第 6 行将是相同的。一个具有线性相关行的矩阵是奇异的，意味着其行列式为 0，并且至少有一个零特征值。因此，$\\sigma_{\\min}(K)$ 将为 0（或在有限精度算术中为机器 epsilon 数量级的值）。近奇异阈值为 $\\tau = 10^{-12}$。由于 $\\sigma_{\\min}(K) \\approx 0  10^{-12}$，该矩阵被声明为近奇异。结果是 `True`。\n\n**测试用例 2：在数据集 $\\mathcal{A}$ 上，使用 $\\gamma = 1$ 且无抖动，报告近奇异布尔值。**\n与前一个用例一样，数据集 $\\mathcal{A}$ 中存在重复点 $x_5=x_6$ 确保了核矩阵 $K$ 是奇异的。此属性与 $\\gamma$ 的值无关（只要 $\\gamma  0$）。因此，$\\sigma_{\\min}(K) \\approx 0$，小于阈值 $\\tau=10^{-12}$。该矩阵是近奇异的。结果是 `True`。\n\n**测试用例 3：在数据集 $\\mathcal{A}$ 上，使用 $\\gamma = 1$ 和抖动 $\\epsilon = 10^{-6}$，报告 $\\log_{10}\\kappa_\\delta(K_\\epsilon)$。**\n从测试用例 2 中我们知道，对于数据集 $\\mathcal{A}$ 上的 $\\gamma=1$，核矩阵 $K$ 是奇异的，因此 $\\sigma_{\\min}(K) = 0$。应用抖动正则化形成 $K_\\epsilon = K + \\epsilon I_n$。这将 $K$ 的特征值移动了 $\\epsilon = 10^{-6}$。$K_\\epsilon$ 的最小奇异值（特征值）将约等于 $\\epsilon$。因此，$\\sigma_{\\min}(K_\\epsilon) \\approx 10^{-6}$。最大奇异值 $\\sigma_{\\max}(K_\\epsilon)$ 将是 $\\sigma_{\\max}(K) + \\epsilon$。条件数则为 $\\kappa_\\delta(K_\\epsilon) \\approx (\\sigma_{\\max}(K)+\\epsilon)/\\epsilon$。我们以数值方式计算这个值。对数条件数将约为 $\\log_{10}(\\sigma_{\\max}(K) / 10^{-6}) = \\log_{10}(\\sigma_{\\max}(K)) + 6$。\n\n**测试用例 4：在数据集 $\\mathcal{B}$ 上，使用 $\\gamma = 10^{-12}$ 且无抖动，报告 $\\log_{10}\\kappa_\\delta(K)$。**\n数据集 $\\mathcal{B}$ 包含不同的点，因此不保证矩阵是奇异的。然而，$\\gamma = 10^{-12}$ 非常小。如原理中所述，当 $\\gamma \\to 0$ 时，核矩阵 $K$ 趋近于秩为 1 的全 1 矩阵 $J_6$。该矩阵将是近奇异的，有一个接近 $n=6$ 的大奇异值和 $n-1=5$ 个非常接近 0 的奇异值。最小奇异值 $\\sigma_{\\min}(K)$ 预计将远小于截断参数 $\\delta = 10^{-15}$。因此，截断条件数将为 $\\kappa_\\delta(K) = \\sigma_{\\max}(K) / \\delta$。结果将是 $\\log_{10}(\\sigma_{\\max}(K)) - \\log_{10}(10^{-15}) \\approx \\log_{10}(6) + 15 \\approx 15.778$。\n\n**测试用例 5：在数据集 $\\mathcal{B}$ 上，在应用有界 $\\gamma$ 先验后报告最坏情况的 $\\log_{10}\\kappa_\\delta(K)$。**\n$\\gamma$ 的候选集是 $\\{10^{-12}, 10^{-3}, 1, 10^{3}, 10^{12}\\}$。先验区间是 $[\\gamma_{\\min}, \\gamma_{\\max}] = [10^{-3}, 10^{3}]$。将截断规则 $\\gamma_{\\text{clip}} = \\min\\{\\max\\{\\gamma, \\gamma_{\\min}\\}, \\gamma_{\\max}\\}$ 应用于该集合，得到：\n- $\\gamma=10^{-12} \\to 10^{-3}$\n- $\\gamma=10^{-3} \\to 10^{-3}$\n- $\\gamma=1 \\to 1$\n- $\\gamma=10^{3} \\to 10^{3}$\n- $\\gamma=10^{12} \\to 10^{3}$\n要测试的有效参数集是 $\\{10^{-3}, 1, 10^{3}\\}$。我们为这三个 $\\gamma$ 值中的每一个计算 $\\log_{10}\\kappa_\\delta(K)$，并报告最大值。最小的有效 gamma 值 $\\gamma=10^{-3}$ 预计会产生最高的条件数，因为它最接近 $\\gamma \\to 0$ 的奇异状态。\n\n**测试用例 6：在数据集 $\\mathcal{B}$ 上，报告使用抖动时最坏情况的 $\\log_{10}\\kappa_\\delta(K_\\epsilon)$。**\n我们使用原始的 $\\gamma$ 候选集 $\\{10^{-12}, 10^{-3}, 1, 10^{3}, 10^{12}\\}$，但对每种情况应用抖动 $\\epsilon = 10^{-6}$。抖动可以防止条件数爆炸。\n- 对于 $\\gamma=10^{-12}$，$K \\approx J_6$。$K_\\epsilon$ 的特征值近似为 $\\{6+\\epsilon, \\epsilon, \\epsilon, \\epsilon, \\epsilon, \\epsilon\\}$。所以 $\\kappa(K_\\epsilon) \\approx (6+\\epsilon)/\\epsilon \\approx 6 \\times 10^6$。\n- 对于大的 $\\gamma=10^{12}$，$K \\approx I_6$，所以 $K_\\epsilon \\approx (1+\\epsilon)I_6$，且 $\\kappa(K_\\epsilon) \\approx 1$。\n最坏情况（最大）的条件数预计会出现在最小的 $\\gamma$ 值，即 $\\gamma=10^{-12}$。我们计算五个 $\\log_{10}\\kappa_\\delta(K_\\epsilon)$ 的值并取最大值。\n\n**测试用例 7：在数据集 $\\mathcal{B}$ 上，使用 $\\gamma = 10^6$ 且无抖动，报告 $\\log_{10}\\kappa_\\delta(K)$。**\n这里，$\\gamma=10^6$ 非常大。由于数据集 $\\mathcal{B}$ 中的所有点都是不同的，核矩阵 $K$ 将趋近于单位矩阵 $I_6$。非对角线项 $K_{ij} = \\exp(-10^6 \\lVert x_i - x_j \\rVert_2^2)$ 将非常接近零。该矩阵将是强对角占优的，所有对角线项都为 1。因此，其所有奇异值都将非常接近 1，其条件数将接近 1。得到的 $\\log_{10}\\kappa_\\delta(K)$ 将非常接近 0。\n\n实现将以数值方式计算这些量。", "answer": "[True,True,6.444203173740049,15.778151250383644,3.551351173995738,6.778151250383644,0.0]", "id": "3165648"}]}