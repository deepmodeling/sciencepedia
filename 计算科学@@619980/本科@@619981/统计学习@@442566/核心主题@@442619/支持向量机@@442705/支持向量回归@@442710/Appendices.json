{"hands_on_practices": [{"introduction": "这个练习将带你剥离自动化求解器的复杂性，回归到支持向量回归（SVR）的核心原理。通过为一个极简数据集手动求解SVR的原始优化问题，你将对 $\\epsilon$-不敏感管道、松弛变量以及模型参数的确定过程建立起具体而深刻的理解。这项基础性实践对于构建关于SVR内部工作机制的直观认识至关重要。[@problem_id:3178709]", "problem": "考虑一个包含两个数据点的一维训练集：$(x_1, y_1) = (0, 0)$ 和 $(x_2, y_2) = (1, 2)$。使用 $\\epsilon$-不敏感损失（其中 $\\epsilon = 0.5$）和正则化参数 $C = 1$ 来拟合一个线性支持向量回归（SVR）模型，其预测器为 $f(x) = w x + b$。从 SVR 的基本原始定义出发：在 $\\epsilon$-管道约束条件下，最小化权重正则化项与 $\\epsilon$-不敏感松弛惩罚项之和，不要借助预先推导的快捷公式。将该优化明确地构建为一个二次规划（QP）问题，并基于第一性原理进行精确求解。确定 $w$ 和 $b$ 的精确最优值，并指明哪些训练点是支持向量，根据 Karush–Kuhn–Tucker (KKT) 互补性解释原因。将你最终拟合的参数 $(w,b)$ 以行矩阵的形式报告。无需四舍五入。", "solution": "题目要求我们对一个包含两个点 $(x_1, y_1) = (0, 0)$ 和 $(x_2, y_2) = (1, 2)$ 的训练集拟合一个线性支持向量回归（SVR）模型 $f(x) = w x + b$。给定的 SVR 参数为不敏感参数 $\\epsilon = 0.5$ 和正则化参数 $C=1$。我们需要从 SVR 的基本原始定义出发来解决这个问题。\n\nSVR 原始优化问题的构建旨在最小化模型复杂度（正则化项）和训练误差（损失函数）的组合。模型复杂度由 $\\frac{1}{2} \\|w\\|^2$ 度量，对于一维输入，即为 $\\frac{1}{2} w^2$。误差由 $\\epsilon$-不敏感损失度量，该损失只惩罚大于 $\\epsilon$ 的残差。这是通过为每个数据点 $i$ 引入非负松弛变量 $\\xi_i$ 和 $\\xi_i^*$ 来实现的。需要最小化的目标函数是：\n$$\n\\min_{w, b, \\xi, \\xi^*} \\frac{1}{2} w^2 + C \\sum_{i=1}^{n} (\\xi_i + \\xi_i^*)\n$$\n该最小化问题受一组定义了数据点周围 $\\epsilon$-管道的约束条件限制：\n$$\n\\begin{cases}\ny_i - (w x_i + b) \\le \\epsilon + \\xi_i,  \\text{for } i=1, \\dots, n \\\\\n(w x_i + b) - y_i \\le \\epsilon + \\xi_i^*,  \\text{for } i=1, \\dots, n \\\\\n\\xi_i \\ge 0, \\xi_i^* \\ge 0,  \\text{for } i=1, \\dots, n\n\\end{cases}\n$$\n这里，数据点的数量 $n=2$。让我们代入给定的值：\n- 数据点 1：$(x_1, y_1) = (0, 0)$\n- 数据点 2：$(x_2, y_2) = (1, 2)$\n- 参数：$\\epsilon = 0.5$, $C = 1$\n\n目标函数变为：\n$$\n\\min_{w, b, \\xi_1, \\xi_1^*, \\xi_2, \\xi_2^*} \\frac{1}{2} w^2 + 1 \\cdot (\\xi_1 + \\xi_1^* + \\xi_2 + \\xi_2^*)\n$$\n对于点 1 $((x_1, y_1) = (0, 0))$ 的约束条件是：\n$$\n0 - (w \\cdot 0 + b) \\le 0.5 + \\xi_1 \\quad \\implies \\quad -b \\le 0.5 + \\xi_1\n$$\n$$\n(w \\cdot 0 + b) - 0 \\le 0.5 + \\xi_1^* \\quad \\implies \\quad b \\le 0.5 + \\xi_1^*\n$$\n对于点 2 $((x_2, y_2) = (1, 2))$ 的约束条件是：\n$$\n2 - (w \\cdot 1 + b) \\le 0.5 + \\xi_2 \\quad \\implies \\quad 1.5 \\le w + b + \\xi_2\n$$\n$$\n(w \\cdot 1 + b) - 2 \\le 0.5 + \\xi_2^* \\quad \\implies \\quad w + b \\le 2.5 + \\xi_2^*\n$$\n以及松弛变量的非负约束：\n$$\n\\xi_1, \\xi_1^*, \\xi_2, \\xi_2^* \\ge 0\n$$\n由于松弛变量在目标函数中受到惩罚，我们希望使它们尽可能小。这意味着我们应该首先研究所有松弛变量都为零的情况，即 $\\xi_1 = \\xi_1^* = \\xi_2 = \\xi_2^* = 0$。这对应于寻找一个对所有数据点都完全位于 $\\epsilon$-管道内的函数 $f(x)$。约束条件简化为：\n$$\n-b \\le 0.5 \\quad \\implies \\quad b \\ge -0.5\n$$\n$$\nb \\le 0.5\n$$\n$$\n1.5 \\le w + b\n$$\n$$\nw + b \\le 2.5\n$$\n这些不等式定义了 $(w, b)$ 的一个可行域。在此区域内，目标函数仅为 $\\frac{1}{2} w^2$。为了找到解，我们必须找到该可行域中使 $\\frac{1}{2} w^2$ 最小化的点 $(w, b)$，这等价于最小化 $|w|$。\n\n可行域由 $-0.5 \\le b \\le 0.5$ 和 $1.5 \\le w+b \\le 2.5$ 定义。为了最小化 $|w|$，我们考察约束 $w+b \\ge 1.5$，可以写成 $w \\ge 1.5 - b$。为了使 $w$ 尽可能小（同时假设 $w \\ge 0$，这是合理的，因为直线从 $(0,0)$ 延伸到 $(1,2)$），我们必须使 $b$ 尽可能大。在可行域中，$b$ 的最大值是 $b=0.5$。将此值代入关于 $w$ 的不等式，得到 $w \\ge 1.5 - 0.5 = 1$。因此，$w$ 的最小可能值为 $1$。\n\n我们来检查点 $(w, b) = (1, 0.5)$ 是否在可行域内。\n- $b=0.5$：满足条件 $-0.5 \\le 0.5 \\le 0.5$。\n- $w+b = 1+0.5=1.5$：满足条件 $1.5 \\le 1.5 \\le 2.5$。\n所以，点 $(w, b) = (1, 0.5)$ 在可行域内。在该点，目标函数的值为 $\\frac{1}{2} w^2 = \\frac{1}{2} (1)^2 = 0.5$。\n\n现在，我们必须考虑具有非零松弛变量的解是否可能产生更低的目标值。任何非零松弛都会给目标函数增加一个正的惩罚项。只有当允许一些惩罚能够使 $\\frac{1}{2}w^2$ 项的减少量大于所产生的惩罚时，才可能得到更好的解。\n让我们考虑一个对当前解的小偏离，例如，将 $w$ 减小到 $w' = 1-\\delta$（对于某个小的 $\\delta > 0$）。为了最小化松弛惩罚，我们可能保持 $b=0.5$。那么 $w'+b' = 1.5-\\delta$。约束 $w+b \\ge 1.5$ 被违反。为了满足它，我们需要 $\\xi_2 \\ge \\delta$。最小的惩罚是 $\\xi_2=\\delta$。新的目标函数值将是 $\\frac{1}{2}(1-\\delta)^2 + \\delta = \\frac{1}{2}(1-2\\delta+\\delta^2) + \\delta = 0.5 - \\delta + 0.5\\delta^2 + \\delta = 0.5 + 0.5\\delta^2$。这个值大于 $0.5$。这个推理表明，任何引入松弛惩罚的偏离都会增加目标函数值。因此，最优解确实是 $(w, b) = (1, 0.5)$，且所有松弛变量均为零。\n\n为了正式确认此解并识别支持向量，我们使用 Karush–Kuhn–Tucker (KKT) 条件。如果数据点 $i$ 对应的拉格朗日乘子 $\\alpha_i$ 或 $\\alpha_i^*$ 非零，则该点是支持向量。KKT 互补性条件要求，对于一个激活的约束，其乘子可以非零；对于一个非零的乘子，其约束必须是激活的。\n\n让我们检验我们的解 $(w,b)=(1,0.5)$ 和 $f(x)=x+0.5$：\n- 对于点 1 $(x_1, y_1) = (0, 0)$：\n  - 预测值为 $f(0) = 1(0) + 0.5 = 0.5$。\n  - 残差为 $y_1 - f(0) = 0 - 0.5 = -0.5$。\n  - 绝对残差 $|-0.5| = 0.5$，恰好等于 $\\epsilon$。\n  - 激活的具体约束是 $(w x_1 + b) - y_1 = \\epsilon$。由于此约束是激活的，其关联的拉格朗日乘子 $\\alpha_1^*$ 可以非零。因此，点 1 是一个支持向量。\n\n- 对于点 2 $(x_2, y_2) = (1, 2)$：\n  - 预测值为 $f(1) = 1(1) + 0.5 = 1.5$。\n  - 残差为 $y_2 - f(1) = 2 - 1.5 = 0.5$。\n  - 绝对残差 $|0.5| = 0.5$，也恰好等于 $\\epsilon$。\n  - 激活的具体约束是 $y_2 - (w x_2 + b) = \\epsilon$。由于此约束是激活的，其关联的拉格朗日乘子 $\\alpha_2$ 可以非零。因此，点 2 也是一个支持向量。\n\n两个数据点都恰好位于 $\\epsilon$-管道的边界上。KKT 的驻点条件是 $\\sum(\\alpha_i - \\alpha_i^*) = 0$ 和 $w = \\sum(\\alpha_i - \\alpha_i^*)x_i$。对于我们的解，$\\alpha_1=0$ 且 $\\alpha_2^*=0$，因为每个点只触及了管道边界的一侧。驻点条件变为 $\\alpha_2 - \\alpha_1^* = 0$ 和 $w = \\alpha_2 x_2 = \\alpha_2$。由于 $w=1$，我们得到 $\\alpha_2=1$。这意味着 $\\alpha_1^*=1$。又因为 $C=1$，两个乘子都达到了它们的上界，这与数据点位于边界上（或边界外，但此处并非如此，因为所有 $\\xi_i, \\xi_i^*$ 均为零）的情况是一致的。所有 KKT 条件均得到满足，从而证实 $(w,b)=(1,0.5)$ 是精确的最优解。\n\n最优参数是 $w=1$ 和 $b=0.5$。两个训练点 $(0,0)$ 和 $(1,2)$ 都是支持向量，因为它们位于 $\\epsilon$-管道的边界上，意味着它们的残差的绝对值恰好等于 $\\epsilon$。这一点由它们在对偶问题表述中的非零拉格朗日乘子（$\\alpha_1^*=1$ 和 $\\alpha_2=1$）所证实，根据 KKT 理论，这是成为支持向量的必要条件。", "answer": "$$\n\\boxed{\\begin{pmatrix} 1  0.5 \\end{pmatrix}}\n$$", "id": "3178709"}, {"introduction": "理论知识固然重要，但模型在现实世界中的表现往往取决于超参数的正确选择。本实践将从手动计算过渡到计算机实验，旨在探索关键参数 $\\epsilon$ 的重要作用。你将构建一个场景，以揭示一个常见的陷阱：不当的 $\\epsilon$ 值如何导致模型糟糕的外推能力，并观察到调整该参数后模型预测能力的显著提升。[@problem_id:3178721]", "problem": "你需要通过显式构造和计算来演示，当 epsilon-不敏感管参数指定不当时，带有线性模型的支持向量回归 (SVR) 的外推效果会很差，以及重新调整 epsilon-不敏感参数如何改善学到的斜率和外推误差。在线性支持向量回归 (SVR) 的设定下工作，其中预测器形式为 $f(x) = w x + b$，经验风险基于 epsilon-不敏感损失。使用一维数据集，并通过符合原理的凸优化过程求解原始形式。\n\n使用的基本原理：\n- 经验风险最小化 (ERM) 原则：选择参数 $(w, b)$ 以在训练集 $\\{(x_i, y_i)\\}_{i=1}^n$ 上最小化正则化的经验损失。\n- 支持向量回归 (SVR) 中使用的 epsilon-不敏感损失：对于残差 $r_i = y_i - f(x_i)$，损失为 $L_{\\epsilon}(r_i) = \\max(0, |r_i| - \\epsilon)$。\n- 原始形式的凸正则化目标：最小化正则化项与经验 epsilon-不敏感损失之和。\n\n你的任务：\n1. 使用下面指定的包含 $n = 13$ 个点的固定训练数据集。输入位置为：\n   $x = [-1.5, -1.3, -1.0, -0.7, -0.4, -0.2, 0.0, 0.2, 0.5, 0.8, 1.0, 1.3, 1.5]$。\n   对应的输出由一个真实的线性函数加上微小的确定性噪声生成：\n   对于相同的 $x_i$，有 $y_i = 2 x_i + 1 + \\nu_i$，其中噪声值是固定的\n   $\\nu = [0.02, -0.03, 0.01, 0.05, -0.04, 0.03, 0.0, -0.02, 0.05, -0.01, 0.02, -0.04, 0.03]$，\n   因此 $y$ 是完全确定的。\n\n2. 将要最小化的原始 SVR 目标定义为\n   $$J(w, b) = \\frac{1}{2}\\lambda w^2 + C \\sum_{i=1}^{n} \\max\\big(0, |y_i - (w x_i + b)| - \\epsilon\\big),$$\n   其中 $\\lambda > 0$ 是斜率 $w$ 的正则化权重，$C > 0$ 是经验损失权重，$\\epsilon \\ge 0$ 是 epsilon-不敏感管的半径。你必须在目标中将 $b$ 视为未正则化的（即，对 $b$ 没有惩罚）。\n\n3. 实现一个正确的算法来最小化 $J(w, b)$ 关于 $(w, b)$ 的值。你必须使用一个从上述基本原理出发的、符合原理的凸优化方法。一个有效的方法是使用带有递减步长的次梯度法来处理 epsilon-不敏感损失的不可微性。你的实现必须：\n   - 将 $w$ 和 $b$ 初始化为 $0$。\n   - 基于 $J(w, b)$ 关于 $w$ 和 $b$ 的次梯度进行迭代更新。\n   - 对于 $t = 0, 1, \\dots, T-1$，使用形式为 $\\eta_t = \\eta_0 / \\sqrt{t + 1}$ 的递减步长策略，其中 $\\eta_0 > 0$ 是固定的，迭代次数 $T$ 足够大以确保在给定数据集上收敛。\n   - 使用固定的超参数 $\\lambda = 1.0$，$C = 10.0$，$\\eta_0 = 0.01$ 和 $T = 10000$。\n\n4. 通过在训练域外的单个测试点 $x_{\\text{out}} = 3.0$ 评估训练好的线性 SVR，并将其与真实的基础线性函数 $f^{\\star}(x) = 2x + 1$ 进行比较，从而构建一个外推的反例。将外推误差定义为绝对差：\n   $$E(\\epsilon) = \\big| \\, (w(\\epsilon) \\cdot x_{\\text{out}} + b(\\epsilon)) - (2 \\cdot x_{\\text{out}} + 1) \\, \\big|.$$\n\n5. 通过为以下 epsilon 值的测试套件计算 $(w(\\epsilon), E(\\epsilon))$，展示重新调整 $\\epsilon$ 如何改变学到的斜率 $w(\\epsilon)$ 并减少外推误差 $E(\\epsilon)$：\n   - 一个错误指定的大 epsilon：$\\epsilon = 3.5$（旨在通过使模型塌陷至 $w \\approx 0$ 来产生差的外推效果）。\n   - 一个精心选择的小 epsilon：$\\epsilon = 0.1$（旨在恢复一个接近真实值 $w^{\\star} = 2$ 的斜率，并产生小的外推误差）。\n   - 一个边界情况：$\\epsilon = 0.0$（绝对偏差损失，用于测试鲁棒性）。\n\n6. 你的程序必须产生单行输出，包含六个结果，按上面列出的 epsilon 值排序。该行必须是一个用方括号括起来的逗号分隔列表，顺序完全如下，每个值四舍五入到 $6$ 位小数：\n   $$[w(3.5), E(3.5), w(0.1), E(0.1), w(0.0), E(0.0)]。$$\n\n注意：\n- 完全在纯数学术语下进行；不涉及物理单位。\n- 必须如上文所述，明确定义和使用支持向量回归 (SVR)。\n- 除了基本原理和给定的目标之外，你不能使用任何捷径公式；相反，应从第一性原理出发推导并实现优化更新。\n- 通过遵循给定的数据集和优化设置，确保科学真实性和内部一致性。", "solution": "该问题要求演示支持向量回归 (SVR) 在 epsilon-不敏感管参数 $\\epsilon$ 指定不当时的外推效果会很差，以及选择更合适的 $\\epsilon$ 如何改善性能。这将通过使用次梯度下降法为给定数据集求解原始 SVR 目标来展示。\n\n线性 SVR 模型由 $f(x) = w x + b$ 给出，其中由于输入 $x$ 是一维的，所以 $w$ 和 $b$ 是标量参数。\n\n目标是找到最小化正则化经验风险函数 $J(w, b)$ 的参数 $(w, b)$：\n$$\nJ(w, b) = \\frac{1}{2}\\lambda w^2 + C \\sum_{i=1}^{n} \\max\\big(0, |y_i - (w x_i + b)| - \\epsilon\\big)\n$$\n这里，$\\lambda > 0$ 是正则化参数，$C > 0$ 是损失的惩罚参数，$\\epsilon \\ge 0$ 定义了 epsilon-不敏感管的半径。项 $\\frac{1}{2}\\lambda w^2$ 是一个 Tikhonov 正则化项，它惩罚大的斜率值以防止过拟合，求和项是训练集 $\\{(x_i, y_i)\\}_{i=1}^n$ 上的总经验损失。\n\n目标函数 $J(w,b)$ 是凸函数，因为它是两个凸函数之和：正则化项 $\\frac{1}{2}\\lambda w^2$（它在 $w$ 上是严格凸的）和经验损失项。损失函数 $L_{\\epsilon}(r) = \\max(0, |r| - \\epsilon)$ 是残差 $r$ 的凸函数，并且由于残差 $r_i = y_i - (w x_i + b)$ 是 $(w, b)$ 的仿射函数，所以复合函数 $L_{\\epsilon}(r_i(w, b))$ 在 $(w, b)$ 上也是凸的。\n\n由于存在绝对值和 $\\max$ 算子，目标函数在 $|y_i - (w x_i + b)| = \\epsilon$ 或 $y_i - (w x_i + b) = 0$ 的点是不可微的。因此，我们将使用次梯度法，这是梯度下降法对不可微凸函数的扩展。\n\n在第 $t$ 次迭代时，次梯度下降的更新规则是：\n$$\nw_{t+1} = w_t - \\eta_t g_{w,t}\n$$\n$$\nb_{t+1} = b_t - \\eta_t g_{b,t}\n$$\n其中 $\\eta_t$ 是步长，$(g_{w,t}, g_{b,t})$ 是 $J$ 在 $(w_t, b_t)$ 处的一个次梯度。\n\n函数的和的次梯度是它们次梯度的和。因此，我们可以通过将正则化项的梯度和损失项的次梯度相加来找到 $J(w, b)$ 的次梯度。\n\n1.  **正则化项的梯度：**\n    正则化项 $J_{reg}(w) = \\frac{1}{2}\\lambda w^2$ 关于 $w$ 是可微的。\n    $$\n    \\frac{\\partial J_{reg}}{\\partial w} = \\lambda w\n    $$\n    该项不依赖于 $b$，因此它关于 $b$ 的导数为 $0$。\n\n2.  **损失项的次梯度：**\n    损失项为 $J_{loss}(w, b) = C \\sum_{i=1}^{n} L_{\\epsilon}(r_i)$，其中 $r_i = y_i - (w x_i + b)$。\n    让我们找到 $L_{\\epsilon}(r_i) = \\max(0, |r_i| - \\epsilon)$ 关于 $w$ 和 $b$ 的次梯度。我们使用次微分的链式法则。$L_{\\epsilon}(r_i)$ 关于 $w$ 的一个次梯度可以计算为 $(\\partial_{r_i} L_{\\epsilon}) \\cdot (\\frac{\\partial r_i}{\\partial w})$。\n\n    首先，让我们找到 $L_{\\epsilon}$ 关于残差 $r_i$ 的次梯度。令 $\\partial_{r_i} L_{\\epsilon}$ 表示次微分的一个元素。\n    -   如果 $|r_i| < \\epsilon$，那么 $|r_i| - \\epsilon < 0$，所以 $L_{\\epsilon}(r_i) = 0$。导数为 $0$。\n    -   如果 $r_i > \\epsilon$，那么 $|r_i| = r_i$，所以 $L_{\\epsilon}(r_i) = r_i - \\epsilon$。导数为 $1$。\n    -   如果 $r_i < -\\epsilon$，那么 $|r_i| = -r_i$，所以 $L_{\\epsilon}(r_i) = -r_i - \\epsilon$。导数为 $-1$。\n    -   在不可微点 $|r_i| = \\epsilon$，我们可以从次微分区间中选择任何值。为了算法实现，我们可以选择 $0$。\n\n    关于 $r_i$ 的 $L_{\\epsilon}(r_i)$ 的一个有效次梯度选择，记为 $g_{r_i}$，是：\n    $$\n    g_{r_i} \\in \\partial_{r_i} L_{\\epsilon}(r_i) \\quad \\text{其中} \\quad g_{r_i} = \\begin{cases}\n    1  \\text{如果 } r_i > \\epsilon \\\\\n    -1  \\text{如果 } r_i < -\\epsilon \\\\\n    0  \\text{如果 } |r_i| \\le \\epsilon\n    \\end{cases}\n    $$\n    残差 $r_i$ 关于 $w$ 和 $b$ 的偏导数是：\n    $$\n    \\frac{\\partial r_i}{\\partial w} = -x_i \\quad \\text{和} \\quad \\frac{\\partial r_i}{\\partial b} = -1\n    $$\n    使用链式法则，总损失项关于 $w$ 和 $b$ 的次梯度是：\n    $$\n    g_{J_{loss}, w} = C \\sum_{i=1}^n (g_{r_i} \\cdot \\frac{\\partial r_i}{\\partial w}) = C \\sum_{i=1}^n g_{r_i} (-x_i) = -C \\sum_{i=1}^n g_{r_i} x_i\n    $$\n    $$\n    g_{J_{loss}, b} = C \\sum_{i=1}^n (g_{r_i} \\cdot \\frac{\\partial r_i}{\\partial b}) = C \\sum_{i=1}^n g_{r_i} (-1) = -C \\sum_{i=1}^n g_{r_i}\n    $$\n\n3.  **J(w, b) 的完整次梯度：**\n    将各部分组合起来，完整目标函数 $J(w, b)$ 的一个次梯度 $(g_w, g_b)$ 是：\n    $$\n    g_w = \\lambda w - C \\sum_{i=1}^n g_{r_i} x_i\n    $$\n    $$\n    g_b = - C \\sum_{i=1}^n g_{r_i}\n    $$\n    其中 $g_{r_i}$ 是根据残差 $r_i = y_i - (w x_i + b)$ 和上面定义的 $\\epsilon$ 来确定的。\n\n算法的执行过程是：初始化 $w_0=0$, $b_0=0$，并使用递减步长策略 $\\eta_t = \\eta_0 / \\sqrt{t + 1}$ 迭代应用更新规则 $T$ 步。我们对每个指定的 $\\epsilon \\in \\{3.5, 0.1, 0.0\\}$ 值执行此过程。对于每个得到的模型 $(w(\\epsilon), b(\\epsilon))$，我们计算在 $x_{\\text{out}}=3.0$ 处的外推误差为 $E(\\epsilon) = |(w(\\epsilon) x_{\\text{out}} + b(\\epsilon)) - (2 x_{\\text{out}} + 1)|$。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the SVR problem using subgradient descent for different epsilon values\n    and computes the resulting learned slope and extrapolation error.\n    \"\"\"\n    # Step 1: Define the fixed training dataset and problem parameters.\n    x_train = np.array([-1.5, -1.3, -1.0, -0.7, -0.4, -0.2, 0.0, 0.2, 0.5, 0.8, 1.0, 1.3, 1.5])\n    nu = np.array([0.02, -0.03, 0.01, 0.05, -0.04, 0.03, 0.0, -0.02, 0.05, -0.01, 0.02, -0.04, 0.03])\n    # The output values are generated by a true linear function plus noise.\n    y_train = 2 * x_train + 1 + nu\n\n    # Step 2: Define the SVR and optimization hyperparameters.\n    lambda_reg = 1.0\n    C = 10.0\n    eta0 = 0.01\n    T = 10000\n\n    # Step 3: Define the test case for extrapolation.\n    x_out = 3.0\n    # The true value at the extrapolation point.\n    y_true_out = 2 * x_out + 1\n\n    # Epsilon values to be tested, as specified in the problem.\n    epsilon_values = [3.5, 0.1, 0.0]\n\n    results = []\n\n    # Step 4: Iterate through each epsilon, train the SVR model, and evaluate it.\n    for epsilon in epsilon_values:\n        # Train the SVR model using subgradient descent.\n        w, b = train_svr(x_train, y_train, lambda_reg, C, epsilon, eta0, T)\n\n        # Calculate the model's prediction at the extrapolation point.\n        y_pred_out = w * x_out + b\n\n        # Calculate the extrapolation error.\n        extrapolation_error = abs(y_pred_out - y_true_out)\n\n        # Store the results with the required precision.\n        results.append(f\"{w:.6f}\")\n        results.append(f\"{extrapolation_error:.6f}\")\n\n    # Step 5: Print the final output in the specified format.\n    print(f\"[{','.join(results)}]\")\n\n\ndef train_svr(x, y, lambda_reg, C, epsilon, eta0, T):\n    \"\"\"\n    Implements subgradient descent to find the optimal w and b for SVR.\n\n    Args:\n        x (np.ndarray): Input feature vector.\n        y (np.ndarray): Target value vector.\n        lambda_reg (float): Regularization parameter.\n        C (float): Loss penalty parameter.\n        epsilon (float): Epsilon-insensitive tube radius.\n        eta0 (float): Initial learning rate for the step size schedule.\n        T (int): Number of iterations.\n\n    Returns:\n        tuple[float, float]: The learned parameters (w, b).\n    \"\"\"\n    # Initialize parameters.\n    w = 0.0\n    b = 0.0\n    n = len(x)\n\n    # Perform T iterations of subgradient descent.\n    for t in range(T):\n        # Calculate the diminishing step size for the current iteration.\n        eta_t = eta0 / np.sqrt(t + 1)\n\n        # Calculate the residuals for all data points.\n        residuals = y - (w * x + b)\n\n        # Determine the coefficients for the loss subgradient based on the residuals.\n        # This implements the subgradient g_ri from the theoretical derivation.\n        g_loss_coeffs = np.zeros(n)\n        g_loss_coeffs[residuals > epsilon] = 1.0\n        g_loss_coeffs[residuals  -epsilon] = -1.0\n\n        # Calculate the full subgradient of the objective function J(w, b).\n        # g_w = (gradient of regularization) + (subgradient of loss wrt w)\n        g_w = lambda_reg * w - C * np.sum(g_loss_coeffs * x)\n        # g_b = (subgradient of loss wrt b)\n        g_b = - C * np.sum(g_loss_coeffs)\n\n        # Update the parameters using the subgradient descent rule.\n        w = w - eta_t * g_w\n        b = b - eta_t * g_b\n\n    return w, b\n\nsolve()\n```", "id": "3178721"}, {"introduction": "截距或偏置项 $b$ 是SVR模型中的一个关键组成部分，但其确定过程可能相当微妙。这项高级实践将深入SVR的对偶形式，探究如何根据支持向量及其对应的对偶变量来计算 $b$。通过分析残差分布不对称的案例，你将揭示支持向量在回归线周围的分布如何影响模型的偏置，并对原始问题与对偶问题解之间的相互作用产生更深刻的理解。[@problem_id:3178771]", "problem": "考虑一个使用线性函数类别的支持向量回归问题，其预测函数为 $f(x) = w^\\top x + b$。该学习问题在 $w$ 上使用 $\\varepsilon$-不敏感损失和 $\\ell_2$ 正则化。训练数据由数据对 $(x_i, y_i)$ 组成，其中 $i = 1, \\dots, n$，$x_i \\in \\mathbb{R}^d$ 且 $y_i \\in \\mathbb{R}$。正则化参数为 $C  0$，管道宽度为 $\\varepsilon  0$。其基本原理是使用带凸约束的 $\\varepsilon$-不敏感损失进行经验风险最小化，以及针对带线性约束的凸优化的拉格朗日对偶和 Karush–Kuhn–Tucker (KKT) 条件。目标是研究偏置项 $b$ 如何从 KKT 条件中计算得出，以及当所有支持向量都位于 $\\varepsilon$-管道的同一侧时，它会受到何种影响。\n\n从使用 $\\varepsilon$-不敏感损失的凸优化公式出发，推导线性核情况下的对偶优化问题，并描述如何根据 KKT 条件计算偏置项 $b$。实现一个求解器，该求解器：\n- 针对小规模实例，使用数值优化器求解线性支持向量回归的对偶优化问题，\n- 根据 KKT 关系，计算 $w$，然后仅使用非边界支持向量（即满足 $0  \\alpha_i  C$ 或 $0  \\alpha_i^\\star  C$ 的向量）来计算 $b$；当只有一侧管道有非边界支持向量时，使用相应的一侧来计算 $b$ 并解释其潜在偏差，\n- 如果完全没有非边界支持向量（一种边界情况），则回退到一种基于数据和已学习到的 $w$ 的一致性估计器来计算 $b$。\n\n最后，通过报告每个测试案例中学习到的 $b$ 和平均残差 $\\frac{1}{n}\\sum_{i=1}^n (y_i - f(x_i))$，来说明残差偏度对计算出的 $b$ 的影响。\n\n您的程序必须为以下三个具有 $d = 1$ 和 $n = 10$ 的确定性测试案例实现上述逻辑：\n\n- 测试案例 1（平衡的残差；支持向量位于两侧）：\n  - $x_i$ 值：$[-1.0, -0.8, -0.6, -0.4, -0.2, 0.0, 0.2, 0.4, 0.6, 0.8]$。\n  - 用于合成 $y_i$ 的真实线性模型：$y_i = w_{\\text{true}} x_i + b_{\\text{true}} + \\text{noise}_i$，其中 $w_{\\text{true}} = 1.5$，$b_{\\text{true}} = 0.2$，噪声 $\\text{noise}_i$ 值为 $[0.12, -0.12, 0.08, -0.08, 0.15, -0.15, 0.05, -0.05, 0.11, -0.11]$。\n  - 使用 $C = 1.0$ 和 $\\varepsilon = 0.1$。\n\n- 测试案例 2（单侧残差；所有支持向量位于上侧）：\n  - $x_i$ 值：与测试案例 1 相同。\n  - 真实线性模型：$y_i = w_{\\text{true}} x_i + b_{\\text{true}} + \\text{noise}_i$，其中 $w_{\\text{true}} = 1.0$，$b_{\\text{true}} = 0.0$，以及严格非负的噪声 $\\text{noise}_i$ 值 $[0.2, 0.15, 0.12, 0.18, 0.25, 0.05, 0.14, 0.11, 0.16, 0.22]$。\n  - 使用 $C = 1.0$ 和 $\\varepsilon = 0.1$。\n\n- 测试案例 3（极小 $C$ 值的边界情况；可能没有非边界支持向量）：\n  - $x_i$ 值：与测试案例 1 相同。\n  - 真实线性模型：$y_i = w_{\\text{true}} x_i + b_{\\text{true}} + \\text{noise}_i$，其中 $w_{\\text{true}} = 1.2$，$b_{\\text{true}} = -0.1$，噪声 $\\text{noise}_i$ 值为 $[0.09, -0.09, 0.08, -0.08, 0.07, -0.07, 0.06, -0.06, 0.05, -0.05]$。\n  - 使用 $C = 0.05$ 和 $\\varepsilon = 0.1$。\n\n对于每个测试案例，计算并返回：\n- 学习到的偏置项 $b$，作为一个浮点数，\n- 平均残差 $\\frac{1}{n}\\sum_{i=1}^n (y_i - f(x_i))$，作为一个浮点数。\n\n您的程序应生成一行输出，其中包含一个由方括号括起来的逗号分隔列表（例如，$[b_1,m_1,b_2,m_2,b_3,m_3]$），其中 $b_k$ 是测试案例 $k$ 的偏置项，$m_k$ 是相应的平均残差。不涉及物理单位，所有报告的值必须是实数。", "solution": "支持向量回归（Support Vector Regression, SVR）问题旨在寻找一个函数 $f(x)$，该函数在尽可能“平坦”的同时，能够近似一组训练数据点 $\\{(x_i, y_i)\\}_{i=1}^n$。对于线性情况，函数为 $f(x) = w^\\top x + b$，其中 $x_i \\in \\mathbb{R}^d$，$y_i \\in \\mathbb{R}$，$w \\in \\mathbb{R}^d$，$b \\in \\mathbb{R}$。\n\n该问题被构建为一个使用 $\\varepsilon$-不敏感损失函数的凸优化问题，该损失函数不惩罚与真实值之间距离在 $\\varepsilon  0$ 以内的误差。超出这个“管道”的偏差会受到线性惩罚，由正则化参数 $C  0$ 控制。函数的平坦度由权重向量的 $\\ell_2$-范数 $\\|w\\|^2$ 来衡量。\n\n**原始形式**\n\n为了处理误差，我们为每个数据点 $i$ 引入非负的松弛变量 $\\xi_i$ 和 $\\xi_i^\\star$。原始问题是最小化正则化风险：\n$$\n\\min_{w, b, \\xi, \\xi^\\star} \\quad \\frac{1}{2} \\|w\\|^2 + C \\sum_{i=1}^n (\\xi_i + \\xi_i^\\star)\n$$\n受以下约束：\n$$\n\\begin{align*}\ny_i - (w^\\top x_i + b) \\le \\varepsilon + \\xi_i, \\quad i=1, \\dots, n \\\\\n(w^\\top x_i + b) - y_i \\le \\varepsilon + \\xi_i^\\star, \\quad i=1, \\dots, n \\\\\n\\xi_i, \\xi_i^\\star \\ge 0, \\quad i=1, \\dots, n\n\\end{align*}\n$$\n前两个约束确保预测值 $f(x_i) = w^\\top x_i + b$ 在 $y_i$ 周围的 $\\varepsilon$-管道内（允许有松弛变量）。\n\n**拉格朗日与对偶形式**\n\n为了解决这个带约束的优化问题，我们通过引入非负的拉格朗日乘子 $\\alpha_i, \\alpha_i^\\star$（对应主要约束）和 $\\mu_i, \\mu_i^\\star$（对应松弛变量的非负性约束）来构造拉格朗日函数：\n$$\nL = \\frac{1}{2} \\|w\\|^2 + C \\sum_{i=1}^n (\\xi_i + \\xi_i^\\star) - \\sum_{i=1}^n \\alpha_i (\\varepsilon + \\xi_i - y_i + w^\\top x_i + b) - \\sum_{i=1}^n \\alpha_i^\\star (\\varepsilon + \\xi_i^\\star + y_i - w^\\top x_i - b) - \\sum_{i=1}^n \\mu_i \\xi_i - \\sum_{i=1}^n \\mu_i^\\star \\xi_i^\\star\n$$\n将 $L$ 对原始变量（$w, b, \\xi_i, \\xi_i^\\star$）的偏导数设为零，得到最优性的 Karush-Kuhn-Tucker (KKT) 条件：\n$$\n\\frac{\\partial L}{\\partial w} = w - \\sum_{i=1}^n (\\alpha_i - \\alpha_i^\\star) x_i = 0 \\implies w = \\sum_{i=1}^n (\\alpha_i - \\alpha_i^\\star) x_i\n$$\n$$\n\\frac{\\partial L}{\\partial b} = - \\sum_{i=1}^n (\\alpha_i - \\alpha_i^\\star) = 0 \\implies \\sum_{i=1}^n \\alpha_i = \\sum_{i=1}^n \\alpha_i^\\star\n$$\n$$\n\\frac{\\partial L}{\\partial \\xi_i} = C - \\alpha_i - \\mu_i = 0\n$$\n$$\n\\frac{\\partial L}{\\partial \\xi_i^\\star} = C - \\alpha_i^\\star - \\mu_i^\\star = 0\n$$\n将这些代入拉格朗日函数，得到 Wolfe 对偶优化问题。目标是最大化对偶目标函数，这等价于最小化其负值：\n$$\n\\min_{\\alpha, \\alpha^\\star} \\quad \\frac{1}{2} \\sum_{i=1}^n \\sum_{j=1}^n (\\alpha_i - \\alpha_i^\\star)(\\alpha_j - \\alpha_j^\\star) (x_i^\\top x_j) - \\sum_{i=1}^n y_i(\\alpha_i - \\alpha_i^\\star) + \\varepsilon \\sum_{i=1}^n (\\alpha_i + \\alpha_i^\\star)\n$$\n受以下由 KKT 条件推导出的约束：\n$$\n\\begin{align*}\n\\sum_{i=1}^n (\\alpha_i - \\alpha_i^\\star) = 0 \\\\\n0 \\le \\alpha_i \\le C, \\quad i=1, \\dots, n \\\\\n0 \\le \\alpha_i^\\star \\le C, \\quad i=1, \\dots, n\n\\end{align*}\n这是一个关于 $2n$ 个对偶变量 $\\alpha_i$ 和 $\\alpha_i^\\star$ 的二次规划（Quadratic Programming, QP）问题，可以使用数值优化器求解。\n\n**偏置项 $b$ 的计算**\n\n一旦找到最优的对偶变量 $\\alpha^\\ast, \\alpha^{\\star\\ast}$，权重向量 $w$ 就可以计算为 $w = \\sum_{i=1}^n (\\alpha_i^\\ast - \\alpha_i^{\\star\\ast}) x_i$。偏置项 $b$ 使用 KKT 互补松弛条件来确定：\n$$\n\\begin{align*}\n\\alpha_i (\\varepsilon + \\xi_i - y_i + w^\\top x_i + b) = 0 \\\\\n\\alpha_i^\\star (\\varepsilon + \\xi_i^\\star + y_i - w^\\top x_i - b) = 0 \\\\\n(C - \\alpha_i) \\xi_i = 0 \\\\\n(C - \\alpha_i^\\star) \\xi_i^\\star = 0\n\\end{align*}\n如果 $\\alpha_i  0$ 或 $\\alpha_i^\\star  0$，则数据点 $i$ 是一个**支持向量**。我们特别关注**非边界支持向量**，其对偶变量严格介于 $0$ 和 $C$ 之间。\n\n1.  如果 $0  \\alpha_i^\\ast  C$，则 $\\xi_i = 0$，第一个 KKT 条件意味着 $\\varepsilon - y_i + w^\\top x_i + b = 0$。这给出 $b = y_i - w^\\top x_i - \\varepsilon$。这些点恰好位于 $\\varepsilon$-管道的上边界。\n2.  如果 $0  \\alpha_i^{\\star\\ast}  C$，则 $\\xi_i^\\star = 0$，第二个 KKT 条件意味着 $\\varepsilon + y_i - w^\\top x_i - b = 0$。这给出 $b = y_i - w^\\top x_i + \\varepsilon$。这些点恰好位于 $\\varepsilon$-管道的下边界。\n\n为获得数值稳定性，计算 $b$ 的标准方法是对从所有非边界支持向量得到的值取平均。\n$$\nb = \\text{mean}\\left( \\{y_i - w^\\top x_i - \\varepsilon \\mid 0  \\alpha_i^\\ast  C\\} \\cup \\{y_i - w^\\top x_i + \\varepsilon \\mid 0  \\alpha_i^{\\star\\ast}  C\\} \\right)\n$$\n\n如果所有非边界支持向量都位于管道的一侧（例如，只存在满足 $0  \\alpha_i^\\ast  C$ 的点），这表明残差中存在系统性偏斜。例如，如果所有这些点都在上边界，这意味着模型倾向于欠预测（$y_i  f(x_i)$）。仅基于这些点计算 $b$ 可能会捕捉到这种系统性偏移，导致非零的平均残差 $\\frac{1}{n}\\sum_{i=1}^n (y_i - f(x_i))$，该值量化了模型的整体偏差。\n\n在不存在非边界支持向量的边界情况（所有 $\\alpha_i^\\ast, \\alpha_i^{\\star\\ast}$ 都为 $0$ 或 $C$）下，上述方法失效。一个一致性的 $b$ 的估计器必须满足所有数据点的 KKT 条件。一个有效的 $b$ 必须位于区间 $[b_{\\text{low}}, b_{\\text{high}}]$ 内，其中：\n$$\nb_{\\text{low}} = \\max_{i: \\alpha_i^\\ast  C} (y_i - w^\\top x_i - \\varepsilon) \\quad \\text{和} \\quad b_{\\text{high}} = \\min_{i: \\alpha_i^{\\star\\ast}  C} (y_i - w^\\top x_i + \\varepsilon)\n$$\n当不存在非边界支持向量时，索引集合分别为 $\\{i | \\alpha_i^\\ast=0\\}$ 和 $\\{i | \\alpha_i^{\\star\\ast}=0\\}$。一个稳健的选择是这个可行区间的中点：$b = \\frac{1}{2}(b_{\\text{low}} + b_{\\text{high}})$。\n\n该实现将为每个测试案例求解对偶 QP 问题，计算 $w$，然后根据是否存在非边界支持向量，使用适当的方法计算 $b$。最后，它将报告学习到的 $b$ 和由此产生的平均残差。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef solve_svr(X, y, C, epsilon):\n    \"\"\"\n    Solves a linear Support Vector Regression problem.\n\n    Args:\n        X (np.ndarray): Input data of shape (n_samples, n_features).\n        y (np.ndarray): Target values of shape (n_samples,).\n        C (float): Regularization parameter.\n        epsilon (float): Epsilon-tube width.\n\n    Returns:\n        tuple: A tuple containing the bias `b` (float) and the mean residual (float).\n    \"\"\"\n    n_samples, n_features = X.shape\n\n    # Construct the Gram matrix for the linear kernel.\n    K = X @ X.T\n    \n    # Formulate the dual QP problem for scipy.optimize.minimize.\n    # We want to minimize: 0.5 * beta.T @ Q @ beta + p.T @ beta\n    # where beta is the concatenation of alpha and alpha_star vectors.\n    \n    # Quadratic term matrix Q\n    Q = np.block([\n        [K, -K],\n        [-K, K]\n    ])\n    \n    # Linear term vector p\n    p = np.concatenate([-y + epsilon, y + epsilon])\n    \n    # Objective function and its Jacobian (gradient)\n    def objective(beta):\n        return 0.5 * beta.T @ Q @ beta + p.T @ beta\n        \n    def jacobian(beta):\n        return Q @ beta + p\n        \n    # Define constraints for the optimizer.\n    # Equality constraint: sum(alpha_i - alpha_i_star) = 0\n    constraints = ({\n        'type': 'eq',\n        'fun': lambda beta: np.sum(beta[:n_samples]) - np.sum(beta[n_samples:]),\n        'jac': lambda beta: np.concatenate([np.ones(n_samples), -np.ones(n_samples)])\n    })\n    \n    # Box constraints (bounds) for dual variables: 0 = alpha_i, alpha_i_star = C\n    bounds = [(0, C)] * (2 * n_samples)\n    \n    # Initial guess for the dual variables\n    beta_0 = np.zeros(2 * n_samples)\n    \n    # Solve the QP problem using SLSQP\n    result = minimize(objective, beta_0, jac=jacobian, bounds=bounds, constraints=constraints, method='SLSQP')\n    beta = result.x\n    \n    alpha = beta[:n_samples]\n    alpha_star = beta[n_samples:]\n    \n    # A small tolerance to handle numerical precision issues when checking boundaries.\n    tol = 1e-6\n\n    # Compute the weight vector w. For d=1, w is a scalar.\n    w = np.sum((alpha - alpha_star).reshape(-1, 1) * X, axis=0)\n    \n    # Compute the bias term b using KKT conditions.\n    \n    # Find indices of non-bound support vectors\n    non_bound_sv_indices_upper = np.where((alpha > tol)  (alpha  C - tol))[0]\n    non_bound_sv_indices_lower = np.where((alpha_star > tol)  (alpha_star  C - tol))[0]\n\n    b_values = []\n    \n    # Check for non-bound SVs on the upper boundary\n    if len(non_bound_sv_indices_upper) > 0:\n        b_upper_candidates = y[non_bound_sv_indices_upper] - X[non_bound_sv_indices_upper] @ w - epsilon\n        b_values.extend(b_upper_candidates.tolist())\n        \n    # Check for non-bound SVs on the lower boundary\n    if len(non_bound_sv_indices_lower) > 0:\n        b_lower_candidates = y[non_bound_sv_indices_lower] - X[non_bound_sv_indices_lower] @ w + epsilon\n        b_values.extend(b_lower_candidates.tolist())\n\n    if len(b_values) > 0:\n        # Standard method: average over all non-bound SVs\n        b = np.mean(b_values)\n    else:\n        # Fallback method: no non-bound SVs exist.\n        # Compute b from the interval defined by all points satisfying KKT conditions.\n        s1_indices = np.where(alpha  C - tol)[0]\n        s2_indices = np.where(alpha_star  C - tol)[0]\n        \n        b_low = np.max(y[s1_indices] - X[s1_indices] @ w - epsilon)\n        b_high = np.min(y[s2_indices] - X[s2_indices] @ w + epsilon)\n        \n        b = (b_low + b_high) / 2\n        \n    # Calculate final predictions and the mean residual.\n    f_x = X @ w + b\n    mean_residual = np.mean(y - f_x)\n    \n    return b, mean_residual\n\ndef solve():\n    # Define the test cases from the problem statement.\n    \n    # Test case 1 (balanced residuals)\n    x1 = np.array([-1.0, -0.8, -0.6, -0.4, -0.2, 0.0, 0.2, 0.4, 0.6, 0.8])\n    w_true1, b_true1 = 1.5, 0.2\n    noise1 = np.array([0.12, -0.12, 0.08, -0.08, 0.15, -0.15, 0.05, -0.05, 0.11, -0.11])\n    y1 = w_true1 * x1 + b_true1 + noise1\n    params1 = (x1.reshape(-1, 1), y1, 1.0, 0.1)\n\n    # Test case 2 (one-sided residuals)\n    x2 = x1\n    w_true2, b_true2 = 1.0, 0.0\n    noise2 = np.array([0.2, 0.15, 0.12, 0.18, 0.25, 0.05, 0.14, 0.11, 0.16, 0.22])\n    y2 = w_true2 * x2 + b_true2 + noise2\n    params2 = (x2.reshape(-1, 1), y2, 1.0, 0.1)\n\n    # Test case 3 (boundary case, small C)\n    x3 = x1\n    w_true3, b_true3 = 1.2, -0.1\n    noise3 = np.array([0.09, -0.09, 0.08, -0.08, 0.07, -0.07, 0.06, -0.06, 0.05, -0.05])\n    y3 = w_true3 * x3 + b_true3 + noise3\n    params3 = (x3.reshape(-1, 1), y3, 0.05, 0.1)\n\n    test_cases = [params1, params2, params3]\n\n    results = []\n    for case in test_cases:\n        X, y, C, epsilon = case\n        b, mean_res = solve_svr(X, y, C, epsilon)\n        results.append(b)\n        results.append(mean_res)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3178771"}]}