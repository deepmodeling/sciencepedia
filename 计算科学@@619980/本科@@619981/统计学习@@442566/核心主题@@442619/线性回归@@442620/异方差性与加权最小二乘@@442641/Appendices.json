{"hands_on_practices": [{"introduction": "加权最小二乘法 (WLS) 是解决异方差性的标准方法，其权重与方差成反比。但如果某个观测点的方差接近于零，会发生什么呢？本练习将探讨一个关键陷阱：理论上的“最优”权重 $w_i \\propto 1/\\operatorname{Var}(\\varepsilon_i)$ 会变得极大，导致数值不稳定和估计扭曲。通过这个练习 [@problem_id:3128028]，你将学会诊断该问题，并理解“稳定化权重”如何提供一个实用的解决方案。", "problem": "考虑一个带有异方差误差的线性回归模型，其中响应变量被建模为 $y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i$，对于 $i = 1, \\dots, n$。假设误差满足 $E[\\varepsilon_i] = 0$ 和 $\\operatorname{Var}(\\varepsilon_i) = \\sigma^2 |x_i|$，其中 $\\sigma^2 > 0$ 是一个未知的比例常数。加权最小二乘法 (WLS) 使用正权重 $w_i$ 来最小化 $\\sum_{i=1}^n w_i (y_i - \\beta_0 - \\beta_1 x_i)^2$。当 $\\operatorname{Var}(\\varepsilon_i)$ 与 $|x_i|$ 成正比时，一个自然的选择是取 $w_i$ 与 $1/|x_i|$ 成正比，但这可能会过度放大 $x_i$ 接近 $0$ 的观测值，并扭曲对截距的估计。为缓解此问题，可以考虑使用稳定化权重 $w_i = 1 / (|x_i| + \\delta)$，其中 $\\delta \\ge 0$ 是一个稳定化参数。\n\n从线性模型的定义出发，并根据在异方差情况下，最佳线性无偏估计量是通过误差方差的倒数进行适当加权得到的这一原则，推导在一般对角权重矩阵和由 $\\operatorname{Var}(\\varepsilon_i)=\\sigma^2|x_i|$ 所蕴含的对角误差方差结构下，截距估计量的方差。从第一性原理出发，且不使用问题陈述中提供的快捷公式，解释为什么当某些 $|x_i|$ 非常小时，选择 $w_i = 1/|x_i|$ 会导致截距估计的不稳定性，并说明稳定化选择 $w_i = 1/(|x_i| + \\delta)$ 如何限制接近零的 $x_i$ 的影响。\n\n然后，实现一个程序，对下文指定的每个测试用例，计算以下两个量：\n- WLS 截距估计量方差与普通最小二乘法 (OLS) 截距估计量方差的比率 $r$。其中 OLS 对所有 $i$ 设置 $w_i = 1$，但在评估估计量方差时仍遵循 $\\operatorname{Var}(\\varepsilon_i)=\\sigma^2|x_i|$。\n- 加权正规矩阵 $X^\\top W X$ 的谱条件数 $\\kappa$。其中 $X$ 是设计矩阵，其第一列为全1，第二列为 $x_i$，且 $W = \\operatorname{diag}(w_1, \\dots, w_n)$。\n\n你的程序必须：\n- 使用 $$X = \\begin{bmatrix} 1  x_1 \\\\ \\vdots  \\vdots \\\\ 1  x_n \\end{bmatrix}$$。\n- 使用 $\\Sigma = \\operatorname{diag}(\\sigma^2 |x_1|, \\dots, \\sigma^2 |x_n|)$ 来表示误差方差结构。\n- 仅使用上述模型假设计算WLS下的截距方差。\n- 仅使用上述模型假设计算OLS下的截距方差。\n- 对所有测试用例设置 $\\sigma^2 = 1$。\n- 将报告的每个浮点数四舍五入到六位小数。\n- 生成单行输出，包含方括号括起来的逗号分隔列表形式的结果，其中每个元素对应一个测试用例，本身是 $[r,\\kappa]$ 格式的逗号分隔对，不含空格。\n\n测试套件（每个测试用例指定了 $x_i$ 值的向量和稳定化参数 $\\delta$）：\n1. 正常路径（无零值）：$x = (-2.0, -1.0, -0.5, -0.2, 0.1, 0.3, 0.6, 1.0, 2.0)$, $\\delta = 0.0$。\n2. 边界敏感性（近零值，无精确零值）：$x = (-2.0, -1.0, -0.5, -10^{-8}, 10^{-8}, 0.1, 0.3, 0.6, 1.0, 2.0)$, $\\delta = 0.0$。\n3. 使用精确零值进行稳定化：$x = (-2.0, -1.0, -0.5, -0.2, 0.0, 0.05, 0.1, 0.3, 0.6, 1.0, 2.0)$, $\\delta = 0.1$。\n4. 强稳定化：$x = (-2.0, -1.0, -0.5, -0.2, 0.0, 0.05, 0.1, 0.3, 0.6, 1.0, 2.0)$, $\\delta = 0.5$。\n\n答案规格：\n- 对每个测试用例，按上述描述输出数对 $[r,\\kappa]$。\n- 你的程序应生成单行输出，其中包含所有测试用例的结果，格式为方括号括起来的逗号分隔列表。输出字符串中不得有任何空格。", "solution": "问题陈述是统计学习中的一个有效练习，涉及异方差情况下加权最小二乘 (WLS) 估计量的性质。它要求从第一性原理进行理论推导和解释，并通过数值实现来阐释这些概念。问题的所有组成部分都有科学依据、定义明确且内部一致。\n\n线性模型以矩阵形式给出：$y = X\\beta + \\varepsilon$，其中 $y$ 是一个 $n \\times 1$ 的观测向量，$X$ 是 $n \\times 2$ 的设计矩阵，$\\beta = [\\beta_0, \\beta_1]^\\top$ 是系数向量，$\\varepsilon$ 是 $n \\times 1$ 的误差向量。对误差的假设是 $E[\\varepsilon] = 0$ 和 $\\operatorname{Var}(\\varepsilon) = E[\\varepsilon \\varepsilon^\\top] = \\Sigma$，其中 $\\Sigma$ 是一个对角矩阵，其元素为 $[\\Sigma]_{ii} = \\sigma^2|x_i|$。\n\n加权最小二乘 (WLS) 估计量 $\\hat{\\beta}_{WLS}$ 是通过最小化加权残差平方和 $S(\\beta) = (y - X\\beta)^\\top W (y - X\\beta)$ 来找到的，其中 $W$ 是由正权重 $w_i$ 构成的对角矩阵。该最小化问题的解由以下加权正规方程给出：\n$$ (X^\\top W X) \\hat{\\beta}_{WLS} = X^\\top W y $$\n这得出的估计量为：\n$$ \\hat{\\beta}_{WLS} = (X^\\top W X)^{-1} X^\\top W y $$\n\n该估计量的方差推导如下。由于 $\\hat{\\beta}_{WLS}$ 是 $y$ 的线性函数，其方差为：\n$$ \\operatorname{Var}(\\hat{\\beta}_{WLS}) = \\operatorname{Var}((X^\\top W X)^{-1} X^\\top W y) $$\n利用 $\\beta$ 是一个常数向量以及 $\\operatorname{Var}(y) = \\operatorname{Var}(X\\beta + \\varepsilon) = \\operatorname{Var}(\\varepsilon) = \\Sigma$ 这一事实，我们有：\n$$ \\operatorname{Var}(\\hat{\\beta}_{WLS}) = (X^\\top W X)^{-1} (X^\\top W) \\operatorname{Var}(y) (X^\\top W)^\\top ((X^\\top W X)^{-1})^\\top $$\n$$ \\operatorname{Var}(\\hat{\\beta}_{WLS}) = (X^\\top W X)^{-1} (X^\\top W \\Sigma W^\\top X) (X^\\top W X)^{-1} $$\n由于 $W$ 和 $X^\\top W X$ 是对称的，这可以简化为“三明治”公式：\n$$ \\operatorname{Var}(\\hat{\\beta}_{WLS}) = (X^\\top W X)^{-1} (X^\\top W \\Sigma W X) (X^\\top W X)^{-1} $$\n截距估计量 $\\operatorname{Var}(\\hat{\\beta}_{0, WLS})$ 的方差是这个 $2 \\times 2$ 矩阵的第一个对角元素，即 $(1,1)$ 位置的元素。这个通用公式对任何正权重 $W$ 的选择都有效，并且不局限于 $W \\propto \\Sigma^{-1}$ 的“最优”情况。\n\n现在我们来分析权重的选择。\n对于WLS，能产生最佳线性无偏估计量 (BLUE) 的最优权重与误差方差的倒数成正比，即 $w_i \\propto 1/\\operatorname{Var}(\\varepsilon_i) = 1/(\\sigma^2|x_i|)$。我们选择 $w_i = 1/|x_i|$（这隐含地在权重定义中设置了 $\\sigma^2=1$，这是允许的，因为估计量对权重的常数缩放是不变的）。在这种特殊情况下，权重矩阵为 $W = (1/\\sigma^2)\\Sigma^{-1}$（如果我们包含 $\\sigma^2$ 项的话）。让我们看看这对三明治公式的中间项有何影响：\n$$ X^\\top W \\Sigma W X = X^\\top \\left(\\frac{1}{\\sigma^2}\\Sigma^{-1}\\right) \\Sigma \\left(\\frac{1}{\\sigma^2}\\Sigma^{-1}\\right) X = \\frac{1}{\\sigma^2} X^\\top \\Sigma^{-1} X $$\n以及外侧项：\n$$ X^\\top W X = X^\\top \\left(\\frac{1}{\\sigma^2}\\Sigma^{-1}\\right) X = \\frac{1}{\\sigma^2} X^\\top \\Sigma^{-1} X $$\n将这些代入三明治公式可得：\n$$ \\operatorname{Var}(\\hat{\\beta}_{BLUE}) = \\left(\\frac{1}{\\sigma^2} X^\\top \\Sigma^{-1} X\\right)^{-1} \\left(\\frac{1}{\\sigma^2} X^\\top \\Sigma^{-1} X\\right) \\left(\\frac{1}{\\sigma^2} X^\\top \\Sigma^{-1} X\\right)^{-1} = \\sigma^2 (X^\\top \\Sigma^{-1} X)^{-1} = (X^\\top W X)^{-1} \\sigma^2 $$\n（使用 $W = \\Sigma^{-1}/\\sigma^2$ 将得到 $\\sigma^4(X^\\top \\Sigma^{-1} X)^{-1}$ 等。如果我们设置 $W = \\Sigma^{-1}$，则 $\\operatorname{Var} = (X^\\top\\Sigma^{-1}X)^{-1}$）。当选择 $w_i=1/|x_i|$ 且 $\\sigma^2=1$ 时，我们得到 $W = \\Sigma^{-1}$ 和 $\\operatorname{Var}(\\hat{\\beta})=(X^\\top W X)^{-1}$。\n\n当某个观测值 $x_k$ 非常接近 $0$ 且我们使用理论上的最优权重 $w_i = 1/|x_i|$（即 $\\delta=0$）时，就会出现不稳定性。\n1.  **数值不稳定性**：权重 $w_k = 1/|x_k|$ 会变得极大。正规矩阵 $$X^\\top W X = \\begin{pmatrix} \\sum w_i  \\sum w_i x_i \\\\ \\sum w_i x_i  \\sum w_i x_i^2 \\end{pmatrix}$$ 会变得病态。$\\sum w_i$ 项由 $w_k$ 主导，使得左上角的元素变得巨大，而其他元素的缩放程度不同。例如，$\\sum w_i x_i^2 = \\sum |x_i|$ 并不由第 $k$ 项主导。这种量级上的巨大差异使得矩阵近乎奇异，这可以通过一个非常大的谱条件数 $\\kappa$ 来量化。对这样一个矩阵求逆在数值上是不稳定的，并且对数据的微小扰动高度敏感。\n2.  **估计扭曲**：WLS 目标函数 $\\sum w_i (y_i - \\beta_0 - \\beta_1 x_i)^2$ 由 $i=k$ 这一项主导。为了最小化这个和，回归线被迫极度靠近点 $(x_k, y_k)$，即 $y_k \\approx \\hat{\\beta}_0 + \\hat{\\beta}_1 x_k$。当 $x_k \\to 0$ 时，这意味着 $\\hat{\\beta}_0 \\approx y_k$。截距的估计值几乎完全由单个观测值 $y_k$ 决定，而 $y_k$ 本身的值又受到随机误差的影响。这种对单个数据点影响的极端放大会扭曲截距估计，使其变得不可靠。\n\n带有 $\\delta > 0$ 的稳定化权重 $w_i = 1 / (|x_i| + \\delta)$ 缓解了这个问题。\n当 $x_i \\to 0$ 时，稳定化权重 $w_i \\to 1/\\delta$。因此，权重被 $1/\\delta$ 上界。这防止了任何单个权重变得任意大。因此，没有单个观测值能够主导最小二乘拟合。矩阵 $X^\\top W X$ 中的所有元素都保持有界且量级可比，从而得到一个更小（更好）的条件数和一个更稳定、更稳健的数值解。最终的截距估计值 $\\hat{\\beta}_0$ 将是所有数据点之间的一个平衡折衷，这正是最小二乘原理的初衷。其代价是，这些非最优权重导致的估计量不再是 BLUE，与理论上（但不稳定）的最优情况相比，其方差可能会增加。\n\n在实现部分，我们计算以下量：\n- WLS 截距估计量的方差 $\\operatorname{Var}(\\hat{\\beta}_{0, WLS})$ 是 $(X^\\top W X)^{-1} (X^\\top W \\Sigma W X) (X^\\top W X)^{-1}$ 矩阵的 $(1,1)$ 位置元素，其中 $W=\\operatorname{diag}(1/(|x_i|+\\delta))$ 且 $\\Sigma=\\operatorname{diag}(|x_i|)$（当 $\\sigma^2=1$ 时）。\n- OLS 截距估计量的方差 $\\operatorname{Var}(\\hat{\\beta}_{0, OLS})$ 使用相同的通用三明治公式计算，但对所有 $i$ 权重为 $w_i=1$。因此，我们设置 $W_{OLS}=I$（单位矩阵）。方差是 $(X^\\top X)^{-1} (X^\\top \\Sigma X) (X^\\top X)^{-1}$ 矩阵的 $(1,1)$ 位置元素。尽管 OLS 估计量本身不使用异方差信息，但这种计算方式正确地考虑了真实的异方差误差结构。\n- 比率是 $r = \\operatorname{Var}(\\hat{\\beta}_{0, WLS}) / \\operatorname{Var}(\\hat{\\beta}_{0, OLS})$。\n- 条件数是 $\\kappa = \\kappa(X^\\top W X)$。\n\n程序将对每个指定的测试用例执行这些计算。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the ratio of WLS to OLS intercept variance and the condition number\n    of the weighted normal matrix for several test cases of heteroscedastic linear regression.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\"x\": np.array([-2.0, -1.0, -0.5, -0.2, 0.1, 0.3, 0.6, 1.0, 2.0]), \"delta\": 0.0},\n        {\"x\": np.array([-2.0, -1.0, -0.5, -1e-8, 1e-8, 0.1, 0.3, 0.6, 1.0, 2.0]), \"delta\": 0.0},\n        {\"x\": np.array([-2.0, -1.0, -0.5, -0.2, 0.0, 0.05, 0.1, 0.3, 0.6, 1.0, 2.0]), \"delta\": 0.1},\n        {\"x\": np.array([-2.0, -1.0, -0.5, -0.2, 0.0, 0.05, 0.1, 0.3, 0.6, 1.0, 2.0]), \"delta\": 0.5},\n    ]\n\n    results = []\n    \n    # Set the unknown constant of proportionality to 1 as specified.\n    sigma_squared = 1.0\n\n    for case in test_cases:\n        x_vec = case[\"x\"]\n        delta = case[\"delta\"]\n        n = len(x_vec)\n\n        # Construct the design matrix X\n        X = np.vstack((np.ones(n), x_vec)).T\n\n        # Construct the true error covariance matrix Sigma\n        # Var(epsilon_i) = sigma^2 * |x_i|\n        Sigma = np.diag(sigma_squared * np.abs(x_vec))\n\n        # --- WLS Calculation ---\n        # Construct the WLS weight matrix W\n        # w_i = 1 / (|x_i| + delta)\n        w_vals = 1.0 / (np.abs(x_vec) + delta)\n        W = np.diag(w_vals)\n\n        # Calculate the weighted normal matrix for WLS\n        X_T_W_X = X.T @ W @ X\n        \n        # Calculate the \"sandwich\" middle part for WLS\n        X_T_W_Sigma_W_X = X.T @ W @ Sigma @ W @ X\n\n        # Calculate the variance-covariance matrix for the WLS estimator\n        # Var(beta_hat_WLS) = (X'WX)^-1 (X'W Sigma WX) (X'WX)^-1\n        X_T_W_X_inv = np.linalg.inv(X_T_W_X)\n        var_beta_wls = X_T_W_X_inv @ X_T_W_Sigma_W_X @ X_T_W_X_inv\n\n        # Extract variance of the intercept estimator (beta_0)\n        var_b0_wls = var_beta_wls[0, 0]\n\n        # Calculate the spectral condition number of the weighted normal matrix\n        kappa = np.linalg.cond(X_T_W_X)\n\n        # --- OLS Calculation ---\n        # For OLS, the weights w_i are all 1.\n        # The estimator is (X'X)^-1 X'y, and its variance must be calculated\n        # using the true heteroscedastic covariance matrix Sigma.\n        X_T_X = X.T @ X\n        \n        # OLS \"sandwich\" middle part\n        X_T_Sigma_X = X.T @ Sigma @ X\n\n        # Calculate the variance-covariance matrix for the OLS estimator\n        # Var(beta_hat_OLS) = (X'X)^-1 (X' Sigma X) (X'X)^-1\n        X_T_X_inv = np.linalg.inv(X_T_X)\n        var_beta_ols = X_T_X_inv @ X_T_Sigma_X @ X_T_X_inv\n        \n        # Extract variance of the intercept estimator (beta_0)\n        var_b0_ols = var_beta_ols[0, 0]\n\n        # --- Ratio Calculation ---\n        # Ratio of WLS intercept variance to OLS intercept variance\n        if var_b0_ols == 0:\n            # Handle potential division by zero, though unlikely in these cases\n            r = np.inf if var_b0_wls > 0 else 0.0\n        else:\n            r = var_b0_wls / var_b0_ols\n        \n        results.append((r, kappa))\n\n    # Format the final output string as specified, with 6 decimal places and no spaces\n    output_str = \",\".join([f\"[{r:.6f},{k:.6f}]\" for r, k in results])\n    print(f\"[{output_str}]\")\n\nsolve()\n\n```", "id": "3128028"}, {"introduction": "在现实世界的数据分析中，我们很少能事先知道真实的方差结构。本练习将介绍一种强大的方法——可行加权最小二乘法 (FGLS)，它能直接从数据中估计权重。你将实现一个两阶段的估计过程：首先利用普通最小二乘法 (OLS) 的残差来为方差建模，然后用估计出的权重进行 WLS 回归 [@problem_id:3128024]。这个练习还将让你探索该方法对预测变量中高杠杆点（异常值）的敏感性。", "problem": "给定一个含单个预测变量和异方差误差结构的简单线性回归模型。对于索引为 $i = 1, \\dots, n$ 的观测值，假设数据根据以下公式生成\n$$\ny_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i,\n$$\n其中\n$$\n\\operatorname{Var}(\\varepsilon_i \\mid x_i) = \\sigma^2 \\lvert x_i \\rvert^\\alpha,\n$$\n这里 $\\sigma^2 > 0$ 且 $\\alpha \\in \\mathbb{R}$。您的任务是推导并实现一个可行的加权最小二乘 (WLS) 估计量，通过使用初始普通最小二乘 (OLS) 拟合的残差从数据中估计未知指数 $\\alpha$，然后检验此过程对预测变量 $x$ 中离群值的敏感性。\n\n基本原理：\n- 普通最小二乘 (OLS) 最小化 $\\sum_{i=1}^n (y_i - \\beta_0 - \\beta_1 x_i)^2$ 并求解由一阶最优性条件产生的正规方程。\n- 加权最小二乘 (WLS) 最小化 $\\sum_{i=1}^n w_i (y_i - \\beta_0 - \\beta_1 x_i)^2$，其中权重 $\\{w_i\\}$ 是预先设定的，并求解加权正规方程。对于方差已知的异方差模型，最优权重与条件方差成反比。\n- 如果方差模型未知，但已参数化指定至 $\\alpha$，则一个可行的 WLS 过程会从数据中估计 $\\alpha$，并将该估计值代入权重中。\n\n实现要求：\n1.  使用以下协议为每个测试用例确定性地生成数据。\n    -   令 $x_i$ 从标准正态分布 $N(0,1)$ 中独立抽取，使用指定的伪随机数生成器种子。\n    -   令 $z_i$ 从标准正态分布 $N(0,1)$ 中独立抽取，使用与抽取 $x_i$ 不同的种子，且与 $x_i$ 的抽取独立。\n    -   定义一个稳定化幅度函数 $m(x) = \\max(\\lvert x \\rvert, \\delta)$，其中 $\\delta = 10^{-8}$ 以保证数值稳定性。在所有与方差相关的计算中（包括数据生成、权重构建和辅助回归），使用 $m(x)$ 代替 $\\lvert x \\rvert$，以避免如 $\\log 0$ 之类的未定义操作，并处理负 $\\alpha$ 而不出现奇异点。\n    -   生成误差为 $\\varepsilon_i = \\sigma \\, m(x_i)^{\\alpha/2} z_i$，使得 $\\operatorname{Var}(\\varepsilon_i \\mid x_i) = \\sigma^2 m(x_i)^\\alpha$ 能够稳定地逼近目标形式。\n    -   生成响应为 $y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i$。\n2.  对于每个数据集（无离群值）：\n    -   拟合 OLS 以获得残差 $r_i = y_i - \\hat{\\beta}_0^{\\text{OLS}} - \\hat{\\beta}_1^{\\text{OLS}} x_i$。\n    -   通过对 $\\log(r_i^2)$ 关于截距项和 $\\log(m(x_i))$ 的辅助 OLS 回归来估计 $\\alpha$，即，将 $t_i = \\log(\\max(r_i^2,\\delta))$ 对 $1$ 和 $z_i = \\log(m(x_i))$ 进行回归。将斜率估计值表示为 $\\hat{\\alpha}$。\n    -   构建 WLS 权重 $w_i = m(x_i)^{-\\hat{\\alpha}}$ 并计算 $\\beta_1$ 的 WLS 估计值，表示为 $\\hat{\\beta}_1^{\\text{WLS}}$。\n3.  $x$ 中的离群值敏感性分析：\n    -   创建原始预测变量向量的副本，并将索引 $j = \\lfloor n/2 \\rfloor$ 处的单个条目替换为大小为 $s \\cdot s_x$ 的离群值，其中 $s$ 是给定的离群值尺度，$s_x$ 是原始 $x$ 的样本标准差。\n    -   使用与上述相同的噪声抽取 $\\{z_i\\}$，通过更新后的 $x$ 重新生成 $\\varepsilon_i^{\\text{out}} = \\sigma \\, m(x_i^{\\text{out}})^{\\alpha/2} z_i$，并设置 $y_i^{\\text{out}} = \\beta_0 + \\beta_1 x_i^{\\text{out}} + \\varepsilon_i^{\\text{out}}$。\n    -   重复可行的 WLS 过程以获得 $\\hat{\\alpha}^{\\text{out}}$ 和 $\\hat{\\beta}_1^{\\text{WLS,out}}$。\n    -   报告定义为 $\\Delta_\\alpha = \\lvert \\hat{\\alpha}^{\\text{out}} - \\hat{\\alpha} \\rvert$ 和 $\\Delta_{\\beta_1} = \\lvert \\hat{\\beta}_1^{\\text{WLS,out}} - \\hat{\\beta}_1^{\\text{WLS}} \\rvert$ 的敏感性度量。\n4.  数值报告：\n    -   对于每个测试用例，报告一个包含六个浮点数的列表，四舍五入到四位小数：\n        -   $\\hat{\\alpha}$，\n        -   $\\hat{\\beta}_1^{\\text{WLS}}$，\n        -   $\\hat{\\alpha}^{\\text{out}}$，\n        -   $\\hat{\\beta}_1^{\\text{WLS,out}}$，\n        -   $\\Delta_\\alpha$，\n        -   $\\Delta_{\\beta_1}$。\n5.  随机性与可复现性：\n    -   在所有测试用例中使用相同的基础种子，但通过添加测试套件中指定的固定偏移量来派生不同的种子，以保证确定性和可复现的输出。\n    -   对于测试用例索引 $k$（从0开始），使用种子 $s_x = s_0 + k$ 来生成 $\\{x_i\\}$，使用种子 $s_z = s_0 + 100k$ 来生成 $\\{z_i\\}$，其中 $s_0 = 2757$。\n\n测试套件：\n为以下四个参数集提供结果。对每个参数集，均执行上述完整的无离群值注入和有离群值注入的流程。\n- 测试 1 （理想情况，方差随 $\\lvert x \\rvert$ 增大）：$n = 200$，$\\beta_0 = 1.0$，$\\beta_1 = 2.0$，$\\sigma = 1.0$，$\\alpha = 1.0$，离群值尺度 $s = 25.0$。\n- 测试 2 （同方差边界）：$n = 200$，$\\beta_0 = 0.5$，$\\beta_1 = -1.5$，$\\sigma = 1.0$，$\\alpha = 0.0$，离群值尺度 $s = 25.0$。\n- 测试 3 （方差随 $\\lvert x \\rvert$ 减小）：$n = 200$，$\\beta_0 = 0.0$，$\\beta_1 = 1.0$，$\\sigma = 1.0$，$\\alpha = -1.0$，离群值尺度 $s = 25.0$。\n- 测试 4 （小样本，中度异方差性）：$n = 30$，$\\beta_0 = 1.0$，$\\beta_1 = 0.5$，$\\sigma = 0.5$，$\\alpha = 0.5$，离群值尺度 $s = 50.0$。\n\n最终输出格式：\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果。每个元素对应一个测试用例，本身也是一个列表，顺序为\n$[\\hat{\\alpha}, \\hat{\\beta}_1^{\\text{WLS}}, \\hat{\\alpha}^{\\text{out}}, \\hat{\\beta}_1^{\\text{WLS,out}}, \\Delta_\\alpha, \\Delta_{\\beta_1}]$，\n所有条目均四舍五入到四位小数。例如：\n[$[\\cdots]$, $[\\cdots]$, $[\\cdots]$, $[\\cdots]$]。", "solution": "所提出的问题要求推导并实现一个可行的加权最小二乘 (WLS) 过程，以处理简单线性回归模型中的异方差性。任务的核心是估计控制方差结构的参数，然后分析此估计过程对预测变量中高杠杆离群值的敏感性。\n\n数据从以下模型生成：\n$$\ny_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i, \\quad i = 1, \\dots, n\n$$\n其中误差 $\\varepsilon_i$ 是异方差的，其条件方差结构由下式给出：\n$$\n\\operatorname{Var}(\\varepsilon_i \\mid x_i) = \\sigma^2 \\lvert x_i \\rvert^\\alpha\n$$\n此处，$\\beta_0$ 和 $\\beta_1$ 是回归系数，$\\sigma^2  0$ 是一个方差尺度参数，$\\alpha \\in \\mathbb{R}$ 是一个指数，它决定了误差方差如何随预测变量 $x_i$ 的大小而变化。为了数值稳定性，尤其是在 $x_i=0$ 或 $\\alpha$ 为负的情况下，我们采用一个稳定化幅度函数 $m(x) = \\max(\\lvert x \\rvert, \\delta)$，其中 $\\delta$ 是一个小的正常数，因此操作上的方差模型为 $\\operatorname{Var}(\\varepsilon_i \\mid x_i) = \\sigma^2 m(x_i)^\\alpha$。\n\n在矩阵表示法中，模型为 $\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}$，其中 $\\mathbf{y}$ 是 $n \\times 1$ 的响应向量，$\\mathbf{X}$ 是 $n \\times 2$ 的设计矩阵，其列分别为截距项和预测变量 $x_i$，$\\boldsymbol{\\beta} = [\\beta_0, \\beta_1]^T$ 是系数向量，$\\boldsymbol{\\varepsilon}$ 是 $n \\times 1$ 的误差向量。误差的协方差矩阵，以 $\\mathbf{X}$ 为条件，是 $\\operatorname{Cov}(\\boldsymbol{\\varepsilon} \\mid \\mathbf{X}) = \\boldsymbol{\\Omega}$，这是一个对角矩阵，其元素为 $(\\boldsymbol{\\Omega})_{ii} = \\sigma^2 m(x_i)^\\alpha$。\n\n如果误差方差已知，$\\boldsymbol{\\beta}$ 的最优估计量将是加权最小二乘 (WLS) 估计量，它最小化加权残差平方和 $\\sum_{i=1}^n w_i (y_i - \\beta_0 - \\beta_1 x_i)^2$。最优权重 $w_i$ 与方差成反比，即 $w_i \\propto (\\operatorname{Var}(\\varepsilon_i \\mid x_i))^{-1}$。我们可以设置权重 $w_i = m(x_i)^{-\\alpha}$。WLS 估计量由下式给出：\n$$\n\\hat{\\boldsymbol{\\beta}}^{\\text{WLS}} = (\\mathbf{X}^T \\mathbf{W} \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{W} \\mathbf{y}\n$$\n其中 $\\mathbf{W} = \\operatorname{diag}(w_1, \\dots, w_n)$ 是权重的对角矩阵。\n\n然而，参数 $\\alpha$ 是未知的，必须从数据中估计。这导致了一个称为可行 WLS (或可行广义最小二乘，FGLS) 的多阶段过程。\n\n**步骤1：初始普通最小二乘 (OLS) 估计**\n首先，我们忽略异方差性并计算 $\\boldsymbol{\\beta}$ 的 OLS 估计量：\n$$\n\\hat{\\boldsymbol{\\beta}}^{\\text{OLS}} = (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{y}\n$$\n在异方差性下，OLS 估计量仍然是无偏的，但不再是最佳线性无偏估计量 (BLUE)；其标准误也是有偏的。然后我们计算 OLS 残差，$r_i = y_i - (\\hat{\\beta}_0^{\\text{OLS}} + \\hat{\\beta}_1^{\\text{OLS}} x_i)$，它们可作为不可观测的真实误差 $\\varepsilon_i$ 的代理。\n\n**步骤2：用于估计 $\\alpha$ 的辅助回归**\n可行过程的核心是使用残差来估计未知的方差参数 $\\alpha$。方差模型为 $\\operatorname{Var}(\\varepsilon_i \\mid x_i) = \\sigma^2 m(x_i)^\\alpha$。取自然对数可得到一个线性关系：\n$$\n\\log(\\operatorname{Var}(\\varepsilon_i \\mid x_i)) = \\log(\\sigma^2) + \\alpha \\log(m(x_i))\n$$\n由于 $E[r_i^2 \\mid x_i]$ 近似于 $\\operatorname{Var}(\\varepsilon_i \\mid x_i)$，我们可以建立一个辅助回归模型。根据问题规范，我们将 $t_i = \\log(\\max(r_i^2, \\delta))$ 对一个截距项和 $z_i = \\log(m(x_i))$ 进行回归：\n$$\nt_i = \\gamma_0 + \\gamma_1 z_i + u_i\n$$\n其中 $\\gamma_0$ 是对 $\\log(\\sigma^2)$ 的估计加上一个与 $\\log(\\chi^2_1)$ 噪声期望相关的项，而 $\\gamma_1$ 是对 $\\alpha$ 的估计。我们使用 OLS 拟合这个辅助模型，并将其斜率估计值作为我们对 $\\alpha$ 的估计，表示为 $\\hat{\\alpha}$。\n\n**步骤3：使用估计权重的 WLS**\n利用估计值 $\\hat{\\alpha}$，我们构建可行的权重：\n$$\n\\hat{w}_i = m(x_i)^{-\\hat{\\alpha}}\n$$\n我们构成估计的权重矩阵 $\\hat{\\mathbf{W}} = \\operatorname{diag}(\\hat{w}_1, \\dots, \\hat{w}_n)$ 并计算可行的 WLS 估计量：\n$$\n\\hat{\\boldsymbol{\\beta}}^{\\text{FWLS}} = (\\mathbf{X}^T \\hat{\\mathbf{W}} \\mathbf{X})^{-1} \\mathbf{X}^T \\hat{\\mathbf{W}} \\mathbf{y}\n$$\n我们提取此向量的第二个分量作为我们的最终估计值 $\\hat{\\beta}_1^{\\text{WLS}}$。\n\n**离群值敏感性分析**\n该问题要求分析此过程对预测变量中高杠杆点的敏感性。通过将单个值 $x_j$（在索引 $j = \\lfloor n/2 \\rfloor$ 处）替换为一个大值 $x_j^{\\text{out}} = s \\cdot s_x$ 来引入一个离群值，其中 $s_x$ 是原始预测变量的样本标准差，$s$ 是一个大的缩放因子。这将创建一个新的数据集 $(x_i^{\\text{out}}, y_i^{\\text{out}})$。\n\n然后，在包含离群值的数据集上重复整个可行的 WLS 过程。离群值 $x_j^{\\text{out}}$ 是一个高杠杆点，意味着它有很强的潜力影响回归线。这种影响会传播到整个估计链中：\n1.  初始的 OLS 拟合 $(\\hat{\\boldsymbol{\\beta}}^{\\text{OLS,out}})$ 将被离群值严重扭曲。\n2.  这种扭曲的拟合导致一组扭曲的残差 $r_i^{\\text{out}}$，尤其是在离群值索引 $j$ 处可能有一个非常大的残差。\n3.  大的离群值残差可能会主导辅助回归，导致一个较差的估计值 $\\hat{\\alpha}^{\\text{out}}$。\n4.  被污染的估计值 $\\hat{\\alpha}^{\\text{out}}$ 会导致不适当的权重 $\\hat{w}_i^{\\text{out}}$，这可能无法正确地增加或减少观测值的权重，从而导致一个有偏的最终估计值 $\\hat{\\beta}_1^{\\text{WLS,out}}$。\n\n敏感性通过绝对差 $\\Delta_\\alpha = \\lvert \\hat{\\alpha}^{\\text{out}} - \\hat{\\alpha} \\rvert$ 和 $\\Delta_{\\beta_1} = \\lvert \\hat{\\beta}_1^{\\text{WLS,out}} - \\hat{\\beta}_1^{\\text{WLS}} \\rvert$ 来量化。此分析揭示了自动化多阶段统计过程的一个关键弱点：一个非稳健的初始步骤可能会破坏整个分析，即使后续步骤在理论上是为处理潜在问题（如 WLS 处理异方差性）而设计的。\n\n实现将使用指定的参数和随机种子为每个测试用例确定性地生成数据。然后，它将对原始数据集和注入了离群值的数据集执行所述的可行 WLS 过程，计算所需的估计值，并计算敏感性度量。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print the results.\n    \"\"\"\n\n    test_cases = [\n        # (n, beta_0, beta_1, sigma, alpha, s)\n        (200, 1.0, 2.0, 1.0, 1.0, 25.0),\n        (200, 0.5, -1.5, 1.0, 0.0, 25.0),\n        (200, 0.0, 1.0, 1.0, -1.0, 25.0),\n        (30, 1.0, 0.5, 0.5, 0.5, 50.0),\n    ]\n\n    all_results = []\n    for i, params in enumerate(test_cases):\n        case_result = solve_case(*params, case_idx=i)\n        all_results.append(case_result)\n\n    # Format the final output string as a list of lists.\n    print(f\"[{','.join(map(str, all_results))}]\")\n\ndef solve_case(n, beta0, beta1, sigma, alpha, s, case_idx):\n    \"\"\"\n    Solves a single test case for the Feasible WLS procedure and its\n    sensitivity to an outlier.\n    \"\"\"\n    s0 = 2757\n    delta = 1e-8\n\n    def m_func(x_vec):\n        \"\"\"Stabilized magnitude function.\"\"\"\n        return np.maximum(np.abs(x_vec), delta)\n\n    def run_feasible_wls(x, y):\n        \"\"\"\n        Performs the complete two-stage feasible WLS estimation.\n        \n        Args:\n            x (np.array): Predictor variable vector.\n            y (np.array): Response variable vector.\n\n        Returns:\n            tuple: A tuple containing (alpha_hat, beta1_wls_hat).\n        \"\"\"\n        # Stage 1: OLS to get residuals\n        X_mat = np.vstack([np.ones(len(x)), x]).T\n        try:\n            beta_ols = np.linalg.solve(X_mat.T @ X_mat, X_mat.T @ y)\n        except np.linalg.LinAlgError:\n            # Use pseudo-inverse for singular matrices, though unlikely here\n            beta_ols = np.linalg.pinv(X_mat.T @ X_mat) @ X_mat.T @ y\n        \n        residuals = y - X_mat @ beta_ols\n\n        # Stage 2: Auxiliary regression to estimate alpha\n        log_sq_residuals = np.log(np.maximum(residuals**2, delta))\n        log_m_x = np.log(m_func(x))\n        Z_mat = np.vstack([np.ones(len(log_m_x)), log_m_x]).T\n        \n        try:\n            gamma = np.linalg.solve(Z_mat.T @ Z_mat, Z_mat.T @ log_sq_residuals)\n        except np.linalg.LinAlgError:\n            gamma = np.linalg.pinv(Z_mat.T @ Z_mat) @ Z_mat.T @ log_sq_residuals\n            \n        alpha_hat = gamma[1]\n\n        # Stage 3: WLS with estimated weights\n        weights = m_func(x)**(-alpha_hat)\n        W_mat = np.diag(weights)\n        \n        try:\n            beta_wls = np.linalg.solve(X_mat.T @ W_mat @ X_mat, X_mat.T @ W_mat @ y)\n        except np.linalg.LinAlgError:\n            beta_wls = np.linalg.pinv(X_mat.T @ W_mat @ X_mat) @ (X_mat.T @ W_mat @ y)\n            \n        beta1_wls_hat = beta_wls[1]\n\n        return alpha_hat, beta1_wls_hat\n\n    # --- Data Generation (No Outlier) ---\n    rng_x = np.random.default_rng(seed=s0 + case_idx)\n    rng_z = np.random.default_rng(seed=s0 + 100 * case_idx)\n    \n    x_original = rng_x.normal(loc=0, scale=1, size=n)\n    z_noise = rng_z.normal(loc=0, scale=1, size=n)\n    \n    eps_original = sigma * m_func(x_original)**(alpha / 2) * z_noise\n    y_original = beta0 + beta1 * x_original + eps_original\n\n    alpha_hat, beta1_wls_hat = run_feasible_wls(x_original, y_original)\n\n    # --- Outlier Injection and Analysis ---\n    x_outlier = x_original.copy()\n    outlier_idx = n // 2\n    # Use ddof=1 for sample standard deviation\n    s_x = np.std(x_original, ddof=1)\n    x_outlier[outlier_idx] = s * s_x\n    \n    # Regenerate responses with the new x vector but same noise draws\n    eps_outlier = sigma * m_func(x_outlier)**(alpha / 2) * z_noise\n    y_outlier = beta0 + beta1 * x_outlier + eps_outlier\n\n    alpha_hat_out, beta1_wls_hat_out = run_feasible_wls(x_outlier, y_outlier)\n\n    # --- Calculate Sensitivity Metrics ---\n    delta_alpha = np.abs(alpha_hat_out - alpha_hat)\n    delta_beta1 = np.abs(beta1_wls_hat_out - beta1_wls_hat)\n\n    # --- Package results for printing ---\n    result_list = [\n        round(alpha_hat, 4),\n        round(beta1_wls_hat, 4),\n        round(alpha_hat_out, 4),\n        round(beta1_wls_hat_out, 4),\n        round(delta_alpha, 4),\n        round(delta_beta1, 4)\n    ]\n\n    return result_list\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3128024"}, {"introduction": "WLS 能有效处理异方差问题，但如果数据中同时存在响应变量的异常值 (outliers)，我们该怎么办？这个高级练习将指导你构建一个能同时处理这两种问题的混合稳健估计器。通过将 WLS 与 Huber 损失函数相结合，你将实现一个迭代重加权最小二乘 (IRLS) 算法 [@problem_id:3127972]，使其不仅能应对非恒定的方差，还能抵抗巨大残差带来的影响。", "problem": "考虑一个具有异方差误差的线性模型，其中观测值由 $i \\in \\{1,2,\\dots,n\\}$ 索引。每个响应 $y_i \\in \\mathbb{R}$ 通过 $y_i = x_i^\\top \\beta + \\varepsilon_i$ 与预测变量向量 $x_i \\in \\mathbb{R}^p$ 相关，其中 $\\beta \\in \\mathbb{R}^p$ 未知，$\\varepsilon_i$ 是独立的零均值误差。假设 $\\varepsilon_i$ 的方差与 $1/w_i$ 成正比，其中 $w_i  0$ 是已知的正权重。您将构建一个混合估计量，它结合了加权最小二乘法 (WLS) 和 Huber 损失，以同时处理异方差性和离群值。\n\n基本原理：\n- 加权最小二乘法 (WLS) 的原理是最小化 $\\sum_{i=1}^n w_i \\, r_i^2$，其中 $r_i = y_i - x_i^\\top \\beta$。\n- 具有调整参数 $k  0$ 的 Huber 损失函数定义为\n$$\n\\rho_k(u) = \n\\begin{cases}\n\\frac{1}{2} u^2,  |u| \\le k, \\\\\nk|u| - \\frac{1}{2}k^2,  |u|  k.\n\\end{cases}\n$$\n\n定义两种将 WLS 权重 $w_i$ 与 Huber 损失相结合的策略：\n- 策略 $\\mathsf{S}$ (尺度感知)：在应用 Huber 损失之前，通过 $\\sqrt{w_i}$ 标准化残差，即最小化 $\\sum_{i=1}^n \\rho_k\\!\\left(\\sqrt{w_i}\\,r_i\\right)$。\n- 策略 $\\mathsf{C}$ (恒定阈值)：将 Huber 损失直接应用于带有 WLS 权重的未标准化残差，即最小化 $\\sum_{i=1}^n w_i \\,\\rho_k(r_i)$。\n\n您的任务：\n1. 仅从上述基本定义出发，推导可用于迭代求解 $\\beta$ 的计算上合理的正规方程。您不能假设任何预先推导的快捷公式；相反，应使用所述目标的一阶最优性条件来推导一个可数值实现的迭代重加权最小二乘法 (IRLS) 程序。\n2. 实现一个程序，对于下述每个测试用例，使用您从第一性原理推导出的 IRLS 算法，计算策略 $\\mathsf{S}$ 和策略 $\\mathsf{C}$ 的系数估计值 $\\beta$。使用截距项，因此 $p = 2$，其中 $x_i = [1, x_i^{\\text{raw}}]^\\top$，$x_i^{\\text{raw}}$ 是提供的标量预测值。\n3. 在两种策略中使用相同的 Huber 调整参数 $k$。在策略 $\\mathsf{S}$ 中，在 $\\rho_k$ 的参数中使用 $\\sqrt{w_i}$ 意味着从 $r_i$ 的角度看，存在一个观测特定的有效阈值；在策略 $\\mathsf{C}$ 中，从 $r_i$ 的角度看，该阈值在所有观测中是恒定的。\n4. 使用 IRLS 停止规则：当 $\\beta$ 的最大绝对变化小于容差 $10^{-8}$ 或达到最大迭代次数 $100$ 时停止，以先到者为准。使用与每个策略在第一次迭代时的权重相对应的 WLS 解进行初始化。\n5. 不涉及物理单位或角度，也不需要百分比。\n\n测试套件：\n- 测试用例 1 (同方差，末尾有离群值)：\n  - 预测值 $x^{\\text{raw}}$: $\\{0,1,2,3,4,5,6,7,8,9\\}$。\n  - 响应值 $y$: $\\{2.0, 5.9, 8.1, 10.9, 14.2, 17.1, 20.0, 23.2, 26.1, 60.0\\}$。\n  - 权重 $w$: $\\{1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0\\}$。\n  - 调整参数 $k$: $1.345$。\n- 测试用例 2 (异方差，中间有高权重离群值，尾部有噪声)：\n  - 预测值 $x^{\\text{raw}}$: $\\{0,1,2,3,4,5,6,7,8,9\\}$。\n  - 响应值 $y$: $\\{2.1, 4.9, 7.8, 11.2, 50.0, 16.5, 19.3, 24.0, 23.0, 35.0\\}$。\n  - 权重 $w$: $\\{4.0,4.0,4.0,4.0,4.0,1.0,1.0,0.25,0.25,0.1\\}$。\n  - 调整参数 $k$: $1.345$。\n- 测试用例 3 (边界条件，存在接近零的权重和大的离群值)：\n  - 预测值 $x^{\\text{raw}}$: $\\{0,1,2,3,4,5,6,7,8,9\\}$。\n  - 响应值 $y$: $\\{2.0, 5.0, 8.0, 11.0, 14.0, 17.0, 20.0, 23.0, 200.0, 29.0\\}$。\n  - 权重 $w$: $\\{1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,0.001,1.0\\}$。\n  - 调整参数 $k$: $1.345$。\n\n要求输出：\n- 对于每个测试用例，计算策略 $\\mathsf{S}$ 和策略 $\\mathsf{C}$ 的截距和斜率估计值。将它们表示为 $\\beta^{\\mathsf{S}} = [\\beta^{\\mathsf{S}}_0,\\beta^{\\mathsf{S}}_1]$ 和 $\\beta^{\\mathsf{C}} = [\\beta^{\\mathsf{C}}_0,\\beta^{\\mathsf{C}}_1]$。\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表的结果，每个测试用例按 $[\\beta^{\\mathsf{S}}_0,\\beta^{\\mathsf{S}}_1,\\beta^{\\mathsf{C}}_0,\\beta^{\\mathsf{C}}_1]$ 的顺序贡献一个列表。对于所有三个测试用例，最终输出必须看起来像一个包含三个列表的单一列表，且没有空格，例如：$[[a_1,b_1,c_1,d_1],[a_2,b_2,c_2,d_2],[a_3,b_3,c_3,d_3]]$，其中每个 $a_j,b_j,c_j,d_j$ 都是一个浮点数。如果需要，您可以将浮点数格式化为固定的小数位数。", "solution": "任务是推导并实现一个迭代重加权最小二乘法 (IRLS) 算法，以为具有异方差误差的线性模型找到系数向量 $\\beta$ 的稳健估计。考虑了两种将加权最小二乘法 (WLS) 与 Huber 损失函数相结合的策略。推导将从每种策略的目标函数的一阶最优性条件开始。\n\n线性模型由 $y_i = x_i^\\top \\beta + \\varepsilon_i$ 给出，其中 $i \\in \\{1, 2, \\dots, n\\}$，$y_i \\in \\mathbb{R}$ 是响应，$x_i \\in \\mathbb{R}^p$ 是预测变量向量，$\\beta \\in \\mathbb{R}^p$ 是未知的系数向量，$\\varepsilon_i$ 是独立的随机误差，其均值 $E[\\varepsilon_i] = 0$，方差 $\\text{Var}(\\varepsilon_i) = \\sigma^2/w_i$，其中 $w_i  0$ 是已知的正权重。第 $i$ 个观测的残差表示为 $r_i = y_i - x_i^\\top \\beta$。\n\n具有调整参数 $k  0$ 的 Huber 损失函数定义为：\n$$\n\\rho_k(u) = \n\\begin{cases}\n\\frac{1}{2} u^2,  |u| \\le k \\\\\nk|u| - \\frac{1}{2}k^2,  |u|  k\n\\end{cases}\n$$\n为了找到目标函数的最小值，我们将使其关于 $\\beta$ 的梯度为零。这需要 Huber 损失的导数，即 Huber 影响函数 $\\psi_k(u) = \\rho_k'(u)$:\n$$\n\\psi_k(u) = \n\\begin{cases}\nu,  |u| \\le k \\\\\nk \\cdot \\text{sgn}(u),  |u|  k\n\\end{cases}\n$$\n其中 $\\text{sgn}(u)$ 是符号函数。该影响函数可以紧凑地写为 $\\psi_k(u) = u \\cdot \\min(1, k/|u|)$。IRLS 算法通过将这个非线性函数重写为 $\\psi_k(u) = u \\cdot \\omega(u)$ 来推导，其中 $\\omega(u) = \\psi_k(u)/u$ 是一个权重函数。如果 $u=0$，则该比值取为 1。权重函数为：\n$$\n\\omega(u) = \\frac{\\psi_k(u)}{u} = \n\\begin{cases}\n1,  |u| \\le k \\\\\nk/|u|,  |u|  k\n\\end{cases}\n$$\n\n**策略 $\\mathsf{S}$ (尺度感知) 的推导**\n\n策略 $\\mathsf{S}$ 的目标是最小化：\n$$\nQ_{\\mathsf{S}}(\\beta) = \\sum_{i=1}^n \\rho_k(\\sqrt{w_i}\\,r_i) = \\sum_{i=1}^n \\rho_k(\\sqrt{w_i}\\,(y_i - x_i^\\top \\beta))\n$$\n我们求 $Q_{\\mathsf{S}}(\\beta)$ 关于向量 $\\beta$ 的梯度。对于每个分量 $\\beta_j$，$j \\in \\{1, \\dots, p\\}$：\n$$\n\\frac{\\partial Q_{\\mathsf{S}}}{\\partial \\beta_j} = \\sum_{i=1}^n \\frac{\\partial}{\\partial \\beta_j} \\rho_k(\\sqrt{w_i}\\,r_i)\n$$\n使用链式法则，设 $u_i = \\sqrt{w_i}\\,r_i$：\n$$\n\\frac{\\partial \\rho_k(u_i)}{\\partial \\beta_j} = \\psi_k(u_i) \\frac{\\partial u_i}{\\partial \\beta_j} = \\psi_k(\\sqrt{w_i}\\,r_i) \\cdot \\sqrt{w_i} \\frac{\\partial (y_i - x_i^\\top \\beta)}{\\partial \\beta_j} = \\psi_k(\\sqrt{w_i}\\,r_i) \\cdot \\sqrt{w_i} \\cdot (-x_{ij})\n$$\n一阶最优性条件是 $\\nabla_\\beta Q_{\\mathsf{S}}(\\beta) = 0$，这给出了一个包含 $p$ 个方程的方程组：\n$$\n\\sum_{i=1}^n \\sqrt{w_i} \\, \\psi_k(\\sqrt{w_i}\\,r_i) \\, x_{ij} = 0 \\quad \\text{for } j=1, \\dots, p\n$$\n这些是正规方程。为了构建 IRLS 算法，我们引入权重 $\\omega_{\\mathsf{S}, i} = \\omega(\\sqrt{w_i}\\,r_i) = \\psi_k(\\sqrt{w_i}\\,r_i) / (\\sqrt{w_i}\\,r_i)$。将其代入正规方程：\n$$\n\\sum_{i=1}^n \\sqrt{w_i} \\, (\\omega_{\\mathsf{S}, i} \\cdot \\sqrt{w_i}\\,r_i) \\, x_{ij} = 0 \\implies \\sum_{i=1}^n (w_i \\, \\omega_{\\mathsf{S}, i}) \\, r_i \\, x_{ij} = 0\n$$\n代入 $r_i = y_i - x_i^\\top \\beta$：\n$$\n\\sum_{i=1}^n (w_i \\, \\omega_{\\mathsf{S}, i}) \\, (y_i - x_i^\\top \\beta) \\, x_{ij} = 0\n$$\n以矩阵形式，设 $X$ 为 $n \\times p$ 的设计矩阵，$Y$ 为 $n \\times 1$ 的响应向量，$W_{\\text{IRLS}}^{\\mathsf{S}}$ 为对角线元素为 $w_i \\omega_{\\mathsf{S}, i}$ 的对角矩阵，则方程变为：\n$$\nX^\\top W_{\\text{IRLS}}^{\\mathsf{S}} (Y - X\\beta) = 0 \\implies X^\\top W_{\\text{IRLS}}^{\\mathsf{S}} X \\beta = X^\\top W_{\\text{IRLS}}^{\\mathsf{S}} Y\n$$\n这是一个有效权重为 $w_i^{\\text{eff}} = w_i \\omega_{\\mathsf{S}, i}$ 的 WLS 问题的正规方程。IRLS 算法迭代求解该系统，其中权重 $\\omega_{\\mathsf{S}, i}$ 在每一步中使用当前的 $\\beta$ 估计值进行更新。\n\n**策略 $\\mathsf{C}$ (恒定阈值) 的推导**\n\n策略 $\\mathsf{C}$ 的目标是最小化：\n$$\nQ_{\\mathsf{C}}(\\beta) = \\sum_{i=1}^n w_i \\,\\rho_k(r_i) = \\sum_{i=1}^n w_i \\,\\rho_k(y_i - x_i^\\top \\beta)\n$$\n对 $\\beta_j$ 求偏导数：\n$$\n\\frac{\\partial Q_{\\mathsf{C}}}{\\partial \\beta_j} = \\sum_{i=1}^n w_i \\frac{\\partial}{\\partial \\beta_j} \\rho_k(r_i) = \\sum_{i=1}^n w_i \\, \\psi_k(r_i) \\frac{\\partial r_i}{\\partial \\beta_j} = \\sum_{i=1}^n w_i \\, \\psi_k(r_i) (-x_{ij})\n$$\n将梯度设为零，得到正规方程：\n$$\n\\sum_{i=1}^n w_i \\, \\psi_k(r_i) \\, x_{ij} = 0 \\quad \\text{for } j=1, \\dots, p\n$$\n我们将此策略的 IRLS 权重定义为 $\\omega_{\\mathsf{C}, i} = \\omega(r_i) = \\psi_k(r_i) / r_i$。代入后得到：\n$$\n\\sum_{i=1}^n w_i \\, (\\omega_{\\mathsf{C}, i} \\cdot r_i) \\, x_{ij} = 0\n$$\n这在形式上与策略 $\\mathsf{S}$ 的结果相同，得到 WLS 正规方程 $X^\\top W_{\\text{IRLS}}^{\\mathsf{C}} X \\beta = X^\\top W_{\\text{IRLS}}^{\\mathsf{C}} Y$，其中 $W_{\\text{IRLS}}^{\\mathsf{C}}$ 是对角线元素为 $w_i \\omega_{\\mathsf{C}, i}$ 的对角矩阵。\n\n**迭代重加权最小二乘法 (IRLS) 算法**\n\n两种策略都导出了一个 IRLS 过程。对于给定策略，其算法如下：\n1.  **初始化**：计算初始估计值 $\\beta^{(0)}$。根据要求，我们使用 WLS 解，这等同于为所有 $i$ 设置 IRLS 权重 $\\omega_i = 1$。令 $W$ 为给定权重 $w_i$ 构成的对角矩阵。\n    $$\n    \\beta^{(0)} = (X^\\top W X)^{-1} X^\\top W Y\n    $$\n2.  **迭代**：对于 $t=1, 2, \\dots$，直到收敛：\n    a.  计算残差：$r^{(t-1)} = Y - X\\beta^{(t-1)}$。\n    b.  计算 IRLS 权重 $\\omega_i^{(t)}$。\n        -   对于策略 $\\mathsf{S}$：$\\omega_{\\mathsf{S}, i}^{(t)} = \\omega(\\sqrt{w_i} r_i^{(t-1)})$。\n        -   对于策略 $\\mathsf{C}$：$\\omega_{\\mathsf{C}, i}^{(t)} = \\omega(r_i^{(t-1)})$。\n    c.  构成总权重的对角矩阵 $W_{\\text{total}}^{(t)}$，其对角线元素为 $w_i \\omega_i^{(t)}$。\n    d.  求解 WLS 系统以获得更新的估计值 $\\beta^{(t)}$：\n        $$\n        \\beta^{(t)} = (X^\\top W_{\\text{total}}^{(t)} X)^{-1} X^\\top W_{\\text{total}}^{(t)} Y\n        $$\n3.  **终止**：当 $\\beta$ 各分量的最大绝对变化小于容差（例如 $10^{-8}$）或达到最大迭代次数（例如 $100$）时停止。最终的 $\\beta^{(t)}$ 即为解。\n\n这个基于原理的推导证实，两种稳健回归策略都可以通过迭代求解一个加权最小二乘问题来实现，其中权重根据前一次迭代的残差自适应地重新计算。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef _solve_irls(X, Y, w, k, strategy, tol=1e-8, max_iter=100):\n    \"\"\"\n    Solves for beta using Iteratively Reweighted Least Squares (IRLS).\n\n    Args:\n        X (np.ndarray): Design matrix of size (n, p).\n        Y (np.ndarray): Response vector of size (n,).\n        w (np.ndarray): Pre-defined weights of size (n,).\n        k (float): Huber loss tuning parameter.\n        strategy (str): 'S' for scale-aware or 'C' for constant-threshold.\n        tol (float): Convergence tolerance.\n        max_iter (int): Maximum number of iterations.\n\n    Returns:\n        np.ndarray: Estimated coefficient vector beta of size (p,).\n    \"\"\"\n    n, p = X.shape\n\n    # 1. Initialization: Standard WLS estimate\n    W_initial = np.diag(w)\n    try:\n        beta = np.linalg.solve(X.T @ W_initial @ X, X.T @ W_initial @ Y)\n    except np.linalg.LinAlgError:\n        # Use pseudo-inverse for singular matrices\n        beta = np.linalg.pinv(X.T @ W_initial @ X) @ X.T @ W_initial @ Y\n\n\n    for _ in range(max_iter):\n        beta_old = beta.copy()\n\n        # 2a. Compute residuals\n        r = Y - X @ beta\n\n        # 2b. Compute IRLS weights (omega)\n        if strategy == 'S':\n            # u_i = sqrt(w_i) * r_i\n            u = np.sqrt(w) * r\n        elif strategy == 'C':\n            # u_i = r_i\n            u = r\n        else:\n            raise ValueError(\"Strategy must be 'S' or 'C'\")\n\n        abs_u = np.abs(u)\n        \n        # omega_i = min(1, k / |u_i|)\n        # Use np.divide for safe division by zero\n        omega = np.minimum(1.0, np.divide(k, abs_u, out=np.full_like(abs_u, np.inf), where=abs_u != 0))\n\n        # 2c. Form total weights\n        w_total = w * omega\n        W_total = np.diag(w_total)\n\n        # 2d. Update beta by solving WLS\n        try:\n            beta = np.linalg.solve(X.T @ W_total @ X, X.T @ W_total @ Y)\n        except np.linalg.LinAlgError:\n            beta = np.linalg.pinv(X.T @ W_total @ X) @ X.T @ W_total @ Y\n\n        # 3. Check for convergence\n        if np.max(np.abs(beta - beta_old))  tol:\n            break\n            \n    return beta\n\ndef solve():\n    \"\"\"\n    Main function to run the test cases and print results.\n    \"\"\"\n    test_cases = [\n        {\n            \"x_raw\": np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=float),\n            \"y\": np.array([2.0, 5.9, 8.1, 10.9, 14.2, 17.1, 20.0, 23.2, 26.1, 60.0], dtype=float),\n            \"w\": np.array([1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], dtype=float),\n            \"k\": 1.345\n        },\n        {\n            \"x_raw\": np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=float),\n            \"y\": np.array([2.1, 4.9, 7.8, 11.2, 50.0, 16.5, 19.3, 24.0, 23.0, 35.0], dtype=float),\n            \"w\": np.array([4.0, 4.0, 4.0, 4.0, 4.0, 1.0, 1.0, 0.25, 0.25, 0.1], dtype=float),\n            \"k\": 1.345\n        },\n        {\n            \"x_raw\": np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=float),\n            \"y\": np.array([2.0, 5.0, 8.0, 11.0, 14.0, 17.0, 20.0, 23.0, 200.0, 29.0], dtype=float),\n            \"w\": np.array([1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.001, 1.0], dtype=float),\n            \"k\": 1.345\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        x_raw, y, w, k = case[\"x_raw\"], case[\"y\"], case[\"w\"], case[\"k\"]\n        \n        # Construct design matrix with intercept\n        X = np.c_[np.ones(len(x_raw)), x_raw]\n\n        # Compute beta for Strategy S\n        beta_S = _solve_irls(X, y, w, k, 'S')\n        \n        # Compute beta for Strategy C\n        beta_C = _solve_irls(X, y, w, k, 'C')\n\n        case_results = [\n            beta_S[0], beta_S[1],\n            beta_C[0], beta_C[1]\n        ]\n        results.append(case_results)\n    \n    # Format output as a string representation of a list of lists, with no spaces.\n    print(str(results).replace(' ', ''))\n\nsolve()\n```", "id": "3127972"}]}