{"hands_on_practices": [{"introduction": "普通最小二乘（OLS）估计的一个基本前提是设计矩阵 $X$ 必须具有满列秩，这意味着不存在任何一个预测变量可以被其他预测变量完美地线性表示。当我们将分类变量转换为虚拟（或称独热）指标时，一个常见的错误——“虚拟变量陷阱”——会违反这一假设，导致完全多重共线性。通过这个实践练习 ([@problem_id:3099895])，你将亲手构建出存在共线性的设计矩阵，并学习如何通过简单的修正来解决这个问题，从而加深对模型矩阵构建重要性的理解。", "problem": "考虑线性回归模型 $y = X \\beta + \\varepsilon$，其中设计矩阵为 $X \\in \\mathbb{R}^{n \\times p}$，系数向量为 $\\beta \\in \\mathbb{R}^{p}$，噪声为 $\\varepsilon \\in \\mathbb{R}^{n}$。普通最小二乘法（OLS）的一个核心假设是 $X$ 具有满列秩，这等价于 $X^\\top X$ 可逆。在处理分类预测变量的统计学习中，一种常见的构造方法是为 $k$ 个类别中的每一个类别使用一个截距列和哑（独热）指示列。所谓的“哑变量陷阱”发生在截距与所有 $k$ 个哑指示变量一起被包含时，这会导致 $X$ 的列之间产生线性相关性，从而使得 $X^\\top X$ 不可逆。\n\n仅使用定义和线性代数知识，编写一个程序，该程序对于给定的分类预测变量，以三种方式构造设计矩阵 $X$，并检查每种情况下 $X^\\top X$ 的可逆性：\n- 初始构造：包含一个截距列 $1_n$ 和所有 $k$ 个哑指示列。\n- 补救方法A：去掉截距，但包含所有 $k$ 个哑指示列。\n- 补救方法B：保留截距，但去掉一个哑指示列，选择数据集中频率最低的类别（如果频率相同，则去掉索引最小的类别）。\n\n您的程序必须通过确定 $X^\\top X$ 的数值秩是否等于所构造的 $X$ 的列数 $p$ 来计算可逆性。秩应使用奇异值分解和标准容差以数值稳定的方式进行计算。\n\n请为以下分类赋值测试套件实现上述要求（每个测试用例提供每行的类别索引和类别总数 $k$）：\n- 测试用例1：$n = 8$，$k = 3$，类别索引 $[0,1,2,0,1,2,0,1]$。\n- 测试用例2：$n = 7$，$k = 3$，类别索引 $[0,1,0,1,0,1,1]$。\n- 测试用例3：$n = 6$，$k = 2$，类别索引 $[0,1,0,1,0,1]$。\n\n对于每个测试用例，按以下固定顺序产生三个布尔值结果：\n$[$初始构造可逆性, 补救方法A可逆性, 补救方法B可逆性$]$。\n\n您的程序应生成单行输出，其中包含所有结果，这些结果按测试用例1、2、3的顺序展平为一个列表，形式为用方括号括起来的逗号分隔序列。具体来说，输出必须是\n$[$初始$_1$, 补救A$_1$, 补救B$_1$, 初始$_2$, 补救A$_2$, 补救B$_2$, 初始$_3$, 补救A$_3$, 补救B$_3]$，\n其中每个条目都是一个布尔值。本问题不涉及物理单位或角度单位，也不需要百分比。", "solution": "该问题要求分析线性回归中的“哑变量陷阱”，这是一种由分类预测变量的特定编码方式引起的完全多重共线性现象。我们的任务是针对给定的分类变量构造三个不同的设计矩阵，并判断每种情况下得到的 $X^\\top X$ 矩阵的可逆性。可逆性是存在唯一普通最小二乘（OLS）估计量 $\\hat{\\beta} = (X^\\top X)^{-1} X^\\top y$ 的一个关键条件。矩阵 $X^\\top X$ 可逆，当且仅当设计矩阵 $X$ 具有满列秩。\n\n这里起作用的核心原理是线性无关。一组向量 $\\{v_1, v_2, \\dots, v_p\\}$ 是线性无关的，如果方程 $c_1 v_1 + c_2 v_2 + \\dots + c_p v_p = 0$ 的唯一解是零解 $c_1 = c_2 = \\dots = c_p = 0$。如果存在非零解，则这些向量是线性相关的。一个矩阵具有满列秩，当且仅当其列向量构成一个线性无关集。\n\n我们将为一个具有 $k$ 个水平、在 $n$ 个样本上观测到的分类预测变量分析三种构造方法。分类数据以索引向量的形式提供，其中每个索引都在 $\\{0, 1, \\dots, k-1\\}$ 中。\n\n1.  **初始构造**：设计矩阵 $X_{initial}$ 通过包含一个截距列（全为1的向量，$1_n$）以及所有 $k$ 个哑指示列 $d_0, d_1, \\dots, d_{k-1}$ 而构成。每个列 $d_j$ 是一个向量，如果第 $i$ 个样本属于类别 $j$，则其第 $i$ 个元素为1，否则为0。该矩阵有 $p = k+1$ 列。存在一个基本的线性相关性，因为任何样本都必须且只属于一个类别。因此，哑指示列之和总是等于截距列：\n    $$ \\sum_{j=0}^{k-1} d_j = 1_n $$\n    这可以重写为 $1 \\cdot 1_n - 1 \\cdot d_0 - \\dots - 1 \\cdot d_{k-1} = 0$，这是 $X_{initial}$ 的列的一个等于零向量的非零线性组合。因此，这些列是线性相关的，$rank(X_{initial})  p$，并且 $X_{initial}^\\top X_{initial}$ 绝不可逆。\n\n2.  **补救方法A（无截距）**：设计矩阵 $X_A$ 通过包含所有 $k$ 个哑列但省略截距而构成。该矩阵为 $X_A = [d_0 | d_1 | \\dots | d_{k-1}]$，有 $p=k$ 列。为检查线性无关性，我们考虑方程 $\\sum_{j=0}^{k-1} c_j d_j = 0$。对于属于类别 $j'$ 的任何样本 $i$，该行的方程简化为 $c_{j'} \\cdot 1 = 0$，这意味着 $c_{j'} = 0$。如果从 0 到 $k-1$ 的每个类别在数据中至少出现一次，这个逻辑会迫使所有系数 $c_0, \\dots, c_{k-1}$ 都为零。那么这些列就是线性无关的，并且 $X_A$ 具有满列秩。但是，如果任何类别 $j'$ 未在数据集中出现，其对应的列 $d_{j'}$ 将是一个零向量。零向量与任何其他向量集都是显然线性相关的（例如，$1 \\cdot d_{j'} = 0$），所以 $X_A$ 将不具有满列秩。\n\n3.  **补救方法B（去掉一个哑变量）**：设计矩阵 $X_B$ 通过包含截距和 $k$ 个哑列中的 $k-1$ 个而构成。设被去掉的列为 $d_{j^*}$。该矩阵为 $X_B = [1_n | d_0 | \\dots | d_{j^*-1} | d_{j^*+1} | \\dots | d_{k-1}]$，有 $p=k$ 列。如果 $1_n$ 可以表示为其余 $k-1$ 个哑列的线性组合，则存在线性相关性。其余哑列的和为 $\\sum_{j \\neq j^*} d_j$。由于 $\\sum_{j=0}^{k-1} d_j = 1_n$，我们有 $\\sum_{j \\neq j^*} d_j = 1_n - d_{j^*}$。这个和等于 $1_n$ 当且仅当 $d_{j^*} = 0$。这种情况恰好在被去掉的类别 $j^*$ 有零个观测值时发生。在这种情况下，线性相关性问题没有得到解决。否则，如果被去掉的类别在数据中存在，其余的列将是线性无关的，并且 $X_B$ 将具有满列秩。问题指定去掉频率最低类别的哑变量，这是一种合理的启发式方法，可以避免去掉对应于大群体的列，但如果频率最低的类别频率为零，则无法解决相关性问题。\n\n计算过程是为每个测试用例实现这三种构造方法。对于每个构造的矩阵 $X$，计算其对应的格拉姆矩阵 $M = X^\\top X$。然后通过比较其数值秩与维度 $p$ 来确定 $M$ 的可逆性。秩会按要求使用奇异值分解（SVD）以稳健的方式计算。每个测试用例的三种方法都会记录一个布尔值结果。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef _check_invertibility(X: np.ndarray) - bool:\n    \"\"\"\n    Checks if X.T @ X is invertible by determining if its rank equals its number of columns.\n    Rank is computed using SVD, which is numerically stable.\n    The problem asks to check the invertibility of X^T X by checking its rank.\n    \"\"\"\n    p = X.shape[1]\n    \n    # A matrix with 0 columns has a 0x0 Gram matrix.\n    # The 0x0 matrix is invertible (its determinant is 1).\n    # This case is not reached by the test suite but is handled for correctness.\n    if p == 0:\n        return True\n    \n    # Form the Gram matrix X-transpose-X\n    XTX = X.T @ X\n    \n    # Compute the rank of the Gram matrix.\n    # np.linalg.matrix_rank uses SVD and a tolerance based on machine precision.\n    rank_XTX = np.linalg.matrix_rank(XTX)\n    \n    # The matrix is invertible if and only if it has full rank.\n    return rank_XTX == p\n\ndef solve():\n    \"\"\"\n    Main function to process test cases and generate the final output.\n    \"\"\"\n    test_cases = [\n        # (category_indices, k)\n        (np.array([0, 1, 2, 0, 1, 2, 0, 1]), 3),\n        (np.array([0, 1, 0, 1, 0, 1, 1]), 3),\n        (np.array([0, 1, 0, 1, 0, 1]), 2),\n    ]\n\n    all_results = []\n    \n    for category_indices, k in test_cases:\n        n = len(category_indices)\n        \n        # --- Common setup for each case ---\n        # Create dummy indicator columns for k categories.\n        # The columns of `dummies` correspond to categories 0, 1, ..., k-1.\n        dummies = np.zeros((n, k), dtype=int)\n        dummies[np.arange(n), category_indices] = 1\n        \n        # Create the intercept column (a vector of ones).\n        intercept = np.ones((n, 1), dtype=int)\n\n        # --- Method 1: Initial Construction (with dummy variable trap) ---\n        # Include intercept and all k dummy variables.\n        X_initial = np.hstack((intercept, dummies))\n        all_results.append(_check_invertibility(X_initial))\n\n        # --- Method 2: Remedy A (drop intercept) ---\n        # Include all k dummy variables, no intercept.\n        X_A = dummies\n        all_results.append(_check_invertibility(X_A))\n\n        # --- Method 3: Remedy B (drop one dummy variable) ---\n        # Keep intercept, drop one dummy variable.\n        # Find the least frequent category. np.bincount counts occurrences.\n        # minlength=k ensures all categories get a count, even if it is 0.\n        frequencies = np.bincount(category_indices, minlength=k)\n        \n        # np.argmin() finds the index of the first occurrence of the minimum value,\n        # which satisfies the tie-breaking rule (drop lowest-index category).\n        category_to_drop = np.argmin(frequencies)\n        \n        # In X_initial, column 0 is the intercept. Columns 1 to k are the dummies\n        # for categories 0 to k-1. So, the column for `category_to_drop`\n        # is at index `1 + category_to_drop`.\n        col_index_to_drop = 1 + category_to_drop\n        X_B = np.delete(X_initial, col_index_to_drop, axis=1)\n        all_results.append(_check_invertibility(X_B))\n\n    # Format the final output as a comma-separated string of booleans.\n    # The default string representation of a boolean in Python is \"True\" or \"False\".\n    output_str = f\"[{','.join(map(str, all_results))}]\"\n    print(output_str)\n\nsolve()\n```", "id": "3099895"}, {"introduction": "在回归分析中，并非所有数据点都具有相同的重要性；某些观测值可能对模型的拟合结果产生不成比例的巨大影响。为了识别这些“高杠杆”和“强影响”点，我们使用杠杆值（leverage）和库克距离（Cook's distance）等诊断工具。本练习 ([@problem_id:3099870]) 将指导你通过编程计算这些关键指标，让你直观地理解高杠杆点（具有潜在影响）和强影响点（具有实际影响）之间的区别，这是进行稳健回归诊断的核心技能。", "problem": "您正在普通最小二乘线性回归框架下工作。其基础如下：给定一个满列秩的设计矩阵 $X$ 和一个响应向量 $y$，普通最小二乘估计量通过最小化残差平方和得到，其拟合值是 $y$ 在 $X$ 列空间上的正交投影。投影算子的对角元素量化了每个观测值的杠杆作用，而影响则是通过评估当单个观测值被扰动或移除时，拟合模型会发生多大变化来确定的。在本任务中，您将为每个测试用例在 $X$ 中植入一个高杠杆点，根据与 $X$ 相关的投影算子计算杠杆值 $h_{ii}$ 和拟合残差，然后使用基于残差大小、杠杆值和模型自由度的库克距离 $D_i$ 定义来评估其影响。\n\n操作要求：\n- 对每个测试用例，通过添加截距列来增广 $X$，使得参数总数等于增广后 $X$ 的列数。\n- 使用增广后的 $X$ 拟合普通最小二乘模型并计算残差。\n- 计算杠杆值 $h_{ii}$，其值为与 $X$ 相关的投影算子的对角线元素。\n- 根据残差、杠杆值、参数数量和残差方差的定义计算库克距离 $D_i$。\n- 使用以下检测规则：\n  - 高杠杆值经验法则：如果 $h_{ii}  \\frac{2p}{n}$，则将观测值 $i$ 标记为高杠杆点，其中 $p$ 是增广后 $X$ 的列数，$n$ 是观测值的数量。\n  - 强影响点经验法则：如果 $D_i  \\frac{4}{n}$，则将观测值 $i$ 标记为强影响点。\n- 对每个测试用例，按顺序报告以下六个量：\n  $[$\n  植入索引 $i^\\star$ 的杠杆值 $h_{i^\\star i^\\star}$（四舍五入到六位小数），\n  具有最大杠杆值的观测值的基于 0 的索引，\n  植入索引 $i^\\star$ 的库克距离 $D_{i^\\star}$（四舍五入到六位小数），\n  具有最大库克距离的观测值的基于 0 的索引，\n  一个布尔值，指示根据经验法则植入索引是否为高杠杆点（使用 $True$ 或 $False$），\n  以及一个布尔值，指示根据经验法则植入索引是否为强影响点（使用 $True$ 或 $False$）\n  $]$。\n\n您的程序应生成单行输出，其中包含所有测试用例的结果，格式为逗号分隔的列表，并用方括号括起来，每个测试用例的六个输出也用各自的方括号括起来；例如：$[ [\\dots], [\\dots], [\\dots] ]$，不包含任何额外的空格或文本。\n\n测试套件（每个用例在 $X$ 中植入一个高杠杆行；索引基于 0，所有数值均为实值）：\n- 用例 1（单个预测变量，植入点处有中等大小的残差）：\n  - $n = 10$，植入索引 $i^\\star = 9$。\n  - $X$ 单列的预测变量值（添加截距前）：$[-1.2, 0.3, -0.5, 0.8, -0.1, 0.4, -0.7, 0.2, -0.3, 10.0]$。\n  - 响应 $y$：$[-1.3, 1.55, 0.2, 2.5, 0.85, 1.65, -0.28, 1.32, 0.43, 21.2]$。\n- 用例 2（单个预测变量，植入点处有非常大的残差）：\n  - $n = 10$，植入索引 $i^\\star = 9$。\n  - 预测变量值：$[-1.2, 0.3, -0.5, 0.8, -0.1, 0.4, -0.7, 0.2, -0.3, 10.0]$。\n  - 响应 $y$：$[-1.3, 1.55, 0.2, 2.5, 0.85, 1.65, -0.28, 1.32, 0.43, 36.0]$。\n- 用例 3（两个预测变量，植入点在两个坐标上均为极值）：\n  - $n = 12$，植入索引 $i^\\star = 11$。\n  - $X$ 的两列的预测变量值（添加截距前）：\n    - 第一个预测变量：$[-1.0, 0.2, -0.3, 0.5, -0.1, 0.4, -0.7, 0.0, 0.3, -0.2, 0.6, 8.0]$。\n    - 第二个预测变量：$[0.5, -0.1, 0.2, -0.4, 0.3, -0.2, 0.1, -0.5, 0.4, -0.3, 0.0, -9.0]$。\n  - 响应 $y$：$[-1.5, 0.95, -0.17, 1.68, 0.04, 1.34, -0.64, 0.97, 0.57, 0.48, 1.4, 21.55]$。\n- 用例 4（单个预测变量，植入点与其余点极远，以将杠杆值推向上界 1）：\n  - $n = 8$，植入索引 $i^\\star = 7$。\n  - 预测变量值：$[-0.5, 0.2, -0.1, 0.3, -0.2, 0.1, 0.0, 50.0]$。\n  - 响应 $y$：$[1.51, 2.18, 1.93, 2.29, 1.8, 2.12, 1.99, 52.01]$。\n- 用例 5（单个预测变量，中等高杠杆值，噪声变化）：\n  - $n = 15$，植入索引 $i^\\star = 14$。\n  - 预测变量值：$[-1.0, -0.8, -0.6, -0.4, -0.2, 0.0, 0.2, 0.4, 0.6, 0.8, 1.0, 0.1, -0.3, 0.5, 6.0]$。\n  - 响应 $y$：$[-0.3, -0.5, 0.0, -0.6, -0.1, 0.5, -0.2, 0.6, 0.1, 0.5, 0.0, 0.25, -0.45, 0.85, 2.0]$。\n\n实现说明：\n- 在所有用例中，用一个全为 1 的截距列来增广 $X$。\n- 所有索引均使用基于 0 的索引。\n- 将报告的植入索引的杠杆值和库克距离四舍五入到六位小数。\n- 对布尔值使用 $True$ 或 $False$。\n- 最终输出必须是严格符合格式的单行：一个顶层方括号列表，包含五个方括号子列表，每个子列表包含其测试用例的六个要求值，用逗号分隔，且整行中没有任何空格。", "solution": "用户提供的问题已经过严格验证，并被确定为统计回归诊断领域内一个定义明确、具有科学依据的任务。问题陈述完整、客观且内部一致，可以得出一个唯一且有意义的解。\n\n此问题的核心是计算和解释两个关键的回归诊断指标：杠杆值和库克距离。这些指标用于识别对普通最小二乘（OLS）回归分析结果具有异常大影响的观测值。\n\n### 理论框架\n\n标准多元线性回归模型由下式给出：\n$$\ny = X\\beta + \\epsilon\n$$\n其中 $y$ 是一个 $n \\times 1$ 的观测向量，$X$ 是一个 $n \\times p$ 的满列秩设计矩阵（包含预测变量，通常还有一个截距列），$\\beta$ 是一个 $p \\times 1$ 的未知系数向量，$\\epsilon$ 是一个 $n \\times 1$ 的不相关误差项向量，其均值为 0，方差恒为 $\\sigma^2$。\n\n$\\beta$ 的 OLS 估计量是使残差平方和 $RSS = \\sum_{i=1}^n (y_i - \\hat{y}_i)^2$ 最小化的向量 $\\hat{\\beta}$。该估计量由正规方程组的解给出：\n$$\n\\hat{\\beta} = (X^T X)^{-1} X^T y\n$$\n\n拟合值是 $y$ 在 $X$ 列空间上的正交投影：\n$$\n\\hat{y} = X\\hat{\\beta} = X(X^T X)^{-1} X^T y = H y\n$$\n矩阵 $H = X(X^T X)^{-1} X^T$ 被称为投影矩阵或“帽子矩阵”，因为它将 $y$ 映射到 $\\hat{y}$。\n\n**杠杆值**\n第 $i$ 个观测值的杠杆值，记为 $h_{ii}$，是帽子矩阵 $H$ 的第 $i$ 个对角元素。它量化了观测响应 $y_i$ 对其自身拟合值 $\\hat{y}_i$ 的影响，因为 $\\hat{y}_i = \\sum_{j=1}^n H_{ij} y_j = h_{ii}y_i + \\sum_{j \\neq i} H_{ij} y_j$。杠杆值 $h_{ii}$ 完全由设计矩阵 $X$ 决定，代表了由于其在预测变量空间中的位置而具有影响力的“潜力”。远离预测变量数据云中心的点具有高杠杆值。杠杆值的范围是 $0 \\le h_{ii} \\le 1$，其总和等于参数数量，即 $\\sum_{i=1}^n h_{ii} = p$。一个常见的经验法则是，如果一个观测值的杠杆值超过平均杠杆值的两倍，即 $h_{ii}  2p/n$，则将其标记为高杠杆点。\n\n**库克距离**\n杠杆值衡量的是潜在影响，而库克距离 $D_i$ 衡量的是当移除第 $i$ 个观测值时模型拟合值的实际总体变化。它是观测值杠杆值和其残差大小的综合度量。一个高杠杆值的观测值可能具有影响力，也可能没有；如果其对应的残差也很大，它就变得具有影响力。库克距离定义为：\n$$\nD_i = \\frac{ \\sum_{j=1}^n (\\hat{y}_j - \\hat{y}_{j(i)})^2 }{p \\cdot \\hat{\\sigma}^2}\n$$\n其中 $\\hat{y}_{j(i)}$ 是在不包含观测值 $i$ 的情况下拟合的模型对观测值 $j$ 的拟合值，而 $\\hat{\\sigma}^2$ 是使用所有观测值拟合的模型的均方误差。一个更方便的计算公式是：\n$$\nD_i = \\frac{e_i^2}{p \\cdot \\hat{\\sigma}^2} \\left[ \\frac{h_{ii}}{(1 - h_{ii})^2} \\right]\n$$\n其中 $e_i = y_i - \\hat{y}_i$ 是第 $i$ 个残差，$\\hat{\\sigma}^2 = \\frac{1}{n-p}\\sum_{j=1}^n e_j^2$ 是均方误差（或残差方差）。一个常见的经验法则是，如果 $D_i  4/n$，则认为该观测值具有影响力。\n\n### 算法流程\n\n对于每个测试用例，执行以下流程来计算所需的六个量：\n1.  **构建设计矩阵**：将提供的预测变量值构成一个矩阵，然后在其前面添加一个全为 1 的列以表示模型截距。这样就得到了最终的 $n \\times p$ 设计矩阵 $X$。\n2.  **拟合 OLS 模型**：通过求解线性方程组 $X^T X \\hat{\\beta} = X^T y$ 来计算 OLS 系数估计值 $\\hat{\\beta}$。这种方法比直接计算 $X^T X$ 的逆矩阵在数值上更稳定。\n3.  **计算杠杆值**：计算帽子矩阵 $H = X (X^T X)^{-1} X^T$。然后从 $H$ 的对角线中提取杠杆值 $h_{ii}$。\n4.  **计算残差和均方误差**：计算拟合值 $\\hat{y} = X\\hat{\\beta}$，然后计算残差 $e = y - \\hat{y}$。接着计算均方误差 $\\hat{\\sigma}^2 = \\frac{e^T e}{n-p}$。\n5.  **计算库克距离**：使用杠杆值 $h_{ii}$、残差 $e_i$、参数数量 $p$ 和均方误差 $\\hat{\\sigma}^2$，根据上述公式计算所有观测值的库克距离 $D_i$。\n6.  **提取并报告结果**：\n    - 检索指定植入点 $i^\\star$ 的杠杆值 $h_{i^\\star i^\\star}$ 和库克距离 $D_{i^\\star}$。\n    - 找到具有最大杠杆值和最大库克距离的观测值的索引。\n    - 将植入点的杠杆值 $h_{i^\\star i^\\star}$ 与阈值 $2p/n$进行比较，以确定它是否为高杠杆点。\n    - 将植入点的库克距离 $D_{i^\\star}$ 与阈值 $4/n$ 进行比较，以确定它是否为强影响点。\n    - 将这六个值编译成该测试用例的列表。\n\n对所有测试用例重复此流程，并将最终结果按规定聚合为单个格式化字符串。", "answer": "```python\nimport numpy as np\n\ndef analyze_case(X_raw: np.ndarray, y: np.ndarray, istar: int):\n    \"\"\"\n    Performs leverage and influence analysis for a single OLS regression case.\n\n    Args:\n        X_raw: The raw predictor matrix (without intercept).\n        y: The response vector.\n        istar: The 0-based index of the planted point.\n\n    Returns:\n        A list containing the six required diagnostic quantities.\n    \"\"\"\n    n = len(y)\n    \n    # Ensure X_raw is a 2D array\n    if X_raw.ndim == 1:\n        X_raw = X_raw.reshape(-1, 1)\n\n    # 1. Construct Design Matrix with an intercept\n    X = np.c_[np.ones(n), X_raw]\n    n, p = X.shape\n\n    # 2. Fit OLS Model\n    try:\n        XtX = X.T @ X\n        Xty = X.T @ y\n        beta_hat = np.linalg.solve(XtX, Xty)\n        XtX_inv = np.linalg.inv(XtX)\n    except np.linalg.LinAlgError:\n         # Problem specification guarantees full column rank, so this is a fallback.\n        pinv_X = np.linalg.pinv(X)\n        beta_hat = pinv_X @ y\n        XtX_inv = np.linalg.pinv(XtX)\n\n    # 3. Compute Leverage\n    H = X @ XtX_inv @ X.T\n    h = np.diag(H)\n\n    # 4. Compute Residuals and MSE\n    y_hat = X @ beta_hat\n    e = y - y_hat\n    rss = e.T @ e\n    mse = rss / (n - p)\n\n    # 5. Compute Cook's Distances\n    # Using np.errstate to handle potential division by zero if h_ii = 1.\n    # In such a case, the residual e_i must be 0, leading to a 0/0 form.\n    # The calculated value would be NaN, but this doesn't occur in the test data.\n    with np.errstate(divide='ignore', invalid='ignore'):\n        D = (e**2 / (p * mse)) * (h / (1 - h)**2)\n    D = np.nan_to_num(D, nan=0.0) # Replace any NaN with 0 for argmax consistency\n\n    # 6. Extract and Report Results\n    h_istar = h[istar]\n    max_h_idx = int(np.argmax(h))\n    \n    D_istar = D[istar]\n    max_D_idx = int(np.argmax(D))\n    \n    is_high_leverage = h_istar  (2 * p / n)\n    is_influential = D_istar  (4 / n)\n    \n    return [\n        round(h_istar, 6),\n        max_h_idx,\n        round(D_istar, 6),\n        max_D_idx,\n        is_high_leverage,\n        is_influential,\n    ]\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the final result.\n    \"\"\"\n    test_cases = [\n        # Case 1\n        {\n            \"X_raw\": np.array([-1.2, 0.3, -0.5, 0.8, -0.1, 0.4, -0.7, 0.2, -0.3, 10.0]),\n            \"y\": np.array([-1.3, 1.55, 0.2, 2.5, 0.85, 1.65, -0.28, 1.32, 0.43, 21.2]),\n            \"istar\": 9\n        },\n        # Case 2\n        {\n            \"X_raw\": np.array([-1.2, 0.3, -0.5, 0.8, -0.1, 0.4, -0.7, 0.2, -0.3, 10.0]),\n            \"y\": np.array([-1.3, 1.55, 0.2, 2.5, 0.85, 1.65, -0.28, 1.32, 0.43, 36.0]),\n            \"istar\": 9\n        },\n        # Case 3\n        {\n            \"X_raw\": np.c_[\n                [-1.0, 0.2, -0.3, 0.5, -0.1, 0.4, -0.7, 0.0, 0.3, -0.2, 0.6, 8.0],\n                [0.5, -0.1, 0.2, -0.4, 0.3, -0.2, 0.1, -0.5, 0.4, -0.3, 0.0, -9.0]\n            ],\n            \"y\": np.array([-1.5, 0.95, -0.17, 1.68, 0.04, 1.34, -0.64, 0.97, 0.57, 0.48, 1.4, 21.55]),\n            \"istar\": 11\n        },\n        # Case 4\n        {\n            \"X_raw\": np.array([-0.5, 0.2, -0.1, 0.3, -0.2, 0.1, 0.0, 50.0]),\n            \"y\": np.array([1.51, 2.18, 1.93, 2.29, 1.8, 2.12, 1.99, 52.01]),\n            \"istar\": 7\n        },\n        # Case 5\n        {\n            \"X_raw\": np.array([-1.0, -0.8, -0.6, -0.4, -0.2, 0.0, 0.2, 0.4, 0.6, 0.8, 1.0, 0.1, -0.3, 0.5, 6.0]),\n            \"y\": np.array([-0.3, -0.5, 0.0, -0.6, -0.1, 0.5, -0.2, 0.6, 0.1, 0.5, 0.0, 0.25, -0.45, 0.85, 2.0]),\n            \"istar\": 14\n        }\n    ]\n    \n    all_results = []\n    for case in test_cases:\n        result = analyze_case(case[\"X_raw\"], case[\"y\"], case[\"istar\"])\n        all_results.append(result)\n\n    # Format the final output string exactly as required (no spaces)\n    result_strings = []\n    for res in all_results:\n        # Format: [leverage,max_lev_idx,cook_dist,max_cook_idx,is_high_lev,is_influential]\n        # Example: [0.97278,9,0.000955,2,True,False]\n        res_str = f\"[{res[0]},{res[1]},{res[2]},{res[3]},{str(res[4])},{str(res[5])}]\"\n        result_strings.append(res_str)\n    \n    final_output = f\"[{','.join(result_strings)}]\"\n    print(final_output)\n\nsolve()\n```", "id": "3099870"}, {"introduction": "经典线性回归模型的一个核心假设是同方差性，即所有误差项 $\\epsilon$ 具有相同的方差。然而，在许多实际应用中，误差的方差会随着预测变量的值而变化，这种情况被称为异方差性。本练习 ([@problem_id:3099884]) 介绍了加权最小二乘法（WLS）作为应对异方差性的有效方法，并引导你计算和比较两种标准误：一种是基于模型的，另一种是稳健的“三明治”估计量。通过这个实践，你将学会如何处理非恒定方差问题，并理解在模型假设可能不完全成立时进行可靠统计推断的重要性。", "problem": "考虑一个带有抽样权重或精度权重的线性回归模型，表示为 $Y = X\\beta + \\epsilon$，其中 $E[\\epsilon \\mid X] = 0$。目标是分析在估计中引入特定于观测值的非负权重 $w_i$ 如何影响独立性和同方差性的假设，以及如何在加权情境下计算估计量的基于模型的方差和稳健方差。\n\n从最小化误差平方和的最小二乘法定义出发，将加权最小二乘法（WLS）定义为最小化加权残差平方和。此外，考虑一种不依赖于方差模型正确设定的异方差稳健协方差估计量（通常称为“三明治”估计量）。您的程序必须：\n- 通过最小化加权残差平方和并求解得到的正规方程，从第一性原理推导出 $\\beta$ 的 WLS 估计量。\n- 在经典假设下计算 WLS 估计量的基于模型的协方差，该假设认为加权后的误差是条件同方差的，其方差与 $W^{-1}$ 成正比，其中 $W$ 是权重的对角矩阵。\n- 为 WLS 估计量计算一个“三明治”协方差估计量，该估计量在未知形式的一般异方差下是一致的。\n- 通过计算标准化残差的离散度量来量化加权使残差近似同方差的程度。令 $r_i = y_i - x_i^\\top \\hat{\\beta}$ 表示残差，$z_i = \\sqrt{w_i}\\, r_i$ 表示标准化残差。将离散度量定义为 $D = \\mathrm{std}(|z|)/\\mathrm{mean}(|z|)$，其中 $\\mathrm{std}$ 是样本标准差，$\\mathrm{mean}$ 是样本均值。较小的 $D$ 值表示加权后更接近同方差性。\n\n使用以下测试套件，每个案例提供 $(X, y, w)$，包含 $n = 6$ 个观测值和 $p = 2$ 个预测变量（截距和斜率）。在所有案例中，设计矩阵 $X$ 的行为 $[1, x_i]$，其中 $x_i \\in \\{\\,0, 1, 2, 3, 4, 5\\,\\}$，即 $x_0 = 0$，$x_1 = 1$，$x_2 = 2$，$x_3 = 3$，$x_4 = 4$，$x_5 = 5$。\n\n案例 A（基准，同方差且独立的误差，等权重）：\n- 真实系数：$\\beta = [\\,1,\\,2\\,]$。\n- 误差：$\\epsilon = [\\,0.2,\\,-0.1,\\,0.0,\\,0.1,\\,-0.2,\\,0.0\\,]$。\n- 响应：$y_i = 1 + 2\\,x_i + \\epsilon_i$，对于 $i \\in \\{0,1,2,3,4,5\\}$。\n- 权重：$w = [\\,1,\\,1,\\,1,\\,1,\\,1,\\,1\\,]$。\n\n案例 B（异方差误差，权重等于逆方差，独立性成立）：\n- 真实系数：$\\beta = [\\,1,\\,2\\,]$。\n- 标准化误差模式：$z = [\\,0.5,\\,-0.5,\\,0.5,\\,-0.5,\\,0.5,\\,-0.5\\,]$。\n- 权重：$w = [\\,4.0,\\,4.0,\\,1.0,\\,1.0,\\,0.25,\\,0.25\\,]$。\n- 误差：$\\epsilon_i = z_i / \\sqrt{w_i}$，对于 $i \\in \\{0,1,2,3,4,5\\}$。\n- 响应：$y_i = 1 + 2\\,x_i + \\epsilon_i$。\n\n案例 C（与案例 B 数据相同，权重设定错误，独立性成立）：\n- 使用与案例 B 中相同的 $X$ 和 $y$。\n- 权重：$w = [\\,0.25,\\,0.25,\\,1.0,\\,1.0,\\,4.0,\\,4.0\\,]$。\n\n案例 D（具有极端权重和强杠杆作用的边界案例）：\n- 真实系数：$\\beta = [\\,0,\\,-1\\,]$。\n- 标准化误差模式：$z = [\\,1,\\,-1,\\,1,\\,-1,\\,1,\\,-1\\,]$。\n- 权重：$w = [\\,1000.0,\\,0.001,\\,10.0,\\,0.1,\\,50.0,\\,0.5\\,]$。\n- 误差：$\\epsilon_i = z_i / \\sqrt{w_i}$，对于 $i \\in \\{0,1,2,3,4,5\\}$。\n- 响应：$y_i = 0 - 1\\,x_i + \\epsilon_i = -x_i + \\epsilon_i$。\n\n对于每个案例，计算：\n1. WLS 估计量 $\\hat{\\beta}$。\n2. 在假设 $\\mathrm{Var}(\\epsilon \\mid X) = \\sigma^2 W^{-1}$ 下，$\\hat{\\beta}$ 的基于模型的标准误，其中 $\\hat{\\sigma}^2$ 从加权残差平方和中估计。\n3. $\\hat{\\beta}$ 的三明治（异方差稳健）标准误。\n4. 标准化残差的离散度量 $D$。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含每个案例的七个浮点数列表，顺序为 $[\\,\\hat{\\beta}_0,\\,\\hat{\\beta}_1,\\,\\mathrm{se}_{\\mathrm{model},0},\\,\\mathrm{se}_{\\mathrm{model},1},\\,\\mathrm{se}_{\\mathrm{sand},0},\\,\\mathrm{se}_{\\mathrm{sand},1},\\,D\\,]$。\n- 将四个案例的结果汇总到一个列表中，以逗号分隔的列表形式打印在单行上，并用方括号括起来，例如 $[[\\cdot],[\\cdot],[\\cdot],[\\cdot]]$。\n\n您的程序必须是一个完整的、可运行的程序，没有外部输入。所有计算必须是确定性的，并直接遵循上述定义和假设。在最终输出列表中，将所有数值结果表示为浮点数。", "solution": "该问题要求推导和应用加权最小二乘法（WLS）估计量，以及其系数的两种协方差估计量：一种是基于模型的估计量，另一种是异方差稳健（三明治）估计量。我们将首先从第一性原理推导必要的公式，然后概述所提供测试案例的计算步骤。\n\n标准线性模型由 $Y = X\\beta + \\epsilon$ 给出，其中 $Y$ 是一个 $n \\times 1$ 的观测向量，$X$ 是一个秩为 $p$ 的 $n \\times p$ 设计矩阵，$\\beta$ 是一个 $p \\times 1$ 的未知系数向量，$\\epsilon$ 是一个 $n \\times 1$ 的不可观测误差向量，其条件均值为 $E[\\epsilon \\mid X] = 0$。在普通最小二乘法（OLS）中，我们假设误差不相关且具有恒定方差，即 $\\mathrm{Var}(\\epsilon \\mid X) = \\sigma^2 I_n$，其中 $I_n$ 是 $n \\times n$ 的单位矩阵。\n\nWLS 处理异方差的情况，即误差方差不相等：$\\mathrm{Var}(\\epsilon_i \\mid X) = \\sigma_i^2$。我们引入一组已知的正权重 $w_i$，通常选择与误差方差成反比，即 $w_i \\propto 1/\\sigma_i^2$。\n\nWLS 估计量通过最小化加权残差平方和（WRSS）得到：\n$$\nS(\\beta) = \\sum_{i=1}^{n} w_i (y_i - x_i^\\top \\beta)^2\n$$\n其中 $y_i$ 是第 $i$ 个观测值，$x_i^\\top$ 是矩阵 $X$ 的第 $i$ 行。设 $W$ 是一个 $n \\times n$ 的对角矩阵，其对角线元素为权重 $w_i$。目标函数可以写成矩阵形式：\n$$\nS(\\beta) = (Y - X\\beta)^\\top W (Y - X\\beta)\n$$\n为了找到最小化 $S(\\beta)$ 的估计量 $\\hat{\\beta}$，我们将 $S(\\beta)$ 对 $\\beta$ 求导，并令结果为零。\n$$\n\\frac{\\partial S(\\beta)}{\\partial \\beta} = \\frac{\\partial}{\\partial \\beta} (Y^\\top W Y - 2\\beta^\\top X^\\top W Y + \\beta^\\top X^\\top W X \\beta) = -2X^\\top W Y + 2X^\\top W X \\beta\n$$\n将导数设为零，得到 WLS 正规方程：\n$$\nX^\\top W X \\hat{\\beta} = X^\\top W Y\n$$\n假设矩阵 $X^\\top W X$ 是可逆的（如果 $X$ 是满列秩且所有 $w_i  0$，则该条件成立），我们可以解出 WLS 估计量 $\\hat{\\beta}$：\n$$\n\\hat{\\beta}_{WLS} = (X^\\top W X)^{-1} X^\\top W Y\n$$\n该估计量是 $Y$ 的线性函数，并且在 $E[\\epsilon \\mid X] = 0$ 的条件下是无偏的：$E[\\hat{\\beta} \\mid X] = E[(X^\\top W X)^{-1} X^\\top W (X\\beta + \\epsilon) \\mid X] = \\beta + (X^\\top W X)^{-1} X^\\top W E[\\epsilon \\mid X] = \\beta$。\n\n接下来，我们推导 $\\hat{\\beta}$ 的协方差矩阵。估计量与真实值之间的偏差是：\n$$\n\\hat{\\beta} - \\beta = (X^\\top W X)^{-1} X^\\top W \\epsilon\n$$\n那么 $\\hat{\\beta}$ 的协方差矩阵是：\n$$\n\\mathrm{Var}(\\hat{\\beta} \\mid X) = E[(\\hat{\\beta}-\\beta)(\\hat{\\beta}-\\beta)^\\top \\mid X] = (X^\\top W X)^{-1} X^\\top W E[\\epsilon\\epsilon^\\top \\mid X] W X (X^\\top W X)^{-1}\n$$\n设 $\\Omega = E[\\epsilon\\epsilon^\\top \\mid X]$ 为误差的真实协方差矩阵。$\\hat{\\beta}$ 的协方差矩阵的一般形式是：\n$$\n\\mathrm{Var}(\\hat{\\beta} \\mid X) = (X^\\top W X)^{-1} (X^\\top W \\Omega W X) (X^\\top W X)^{-1}\n$$\n\n标准误的计算取决于对 $\\Omega$ 所作的假设。\n\n**1. 基于模型的协方差估计量**\n此方法假设权重被正确设定，即它们与真实误差方差成反比。这意味着对于某个未知的常数 $\\sigma^2$，有 $\\mathrm{Var}(\\epsilon_i \\mid X) = \\sigma^2/w_i$。以矩阵形式表示，$\\Omega = \\sigma^2 W^{-1}$。将此代入通用协方差公式，会显著简化：\n$$\n\\mathrm{Var}(\\hat{\\beta} \\mid X) = (X^\\top W X)^{-1} X^\\top W (\\sigma^2 W^{-1}) W X (X^\\top W X)^{-1} = \\sigma^2 (X^\\top W X)^{-1}\n$$\n为了使其可操作，我们需要估计 $\\sigma^2$。在假设 $\\Omega = \\sigma^2 W^{-1}$ 下，变换后的误差 $\\epsilon^* = W^{1/2}\\epsilon$ 是同方差的，其方差为 $\\sigma^2 I_n$。$\\sigma^2$ 的一个无偏估计量由回归的加权残差平方和除以自由度 $n-p$ 得出：\n$$\n\\hat{\\sigma}^2 = \\frac{1}{n-p} \\sum_{i=1}^{n} w_i(y_i - x_i^\\top \\hat{\\beta})^2 = \\frac{(Y-X\\hat{\\beta})^\\top W (Y-X\\hat{\\beta})}{n-p}\n$$\n因此，估计的基于模型的协方差矩阵是：\n$$\n\\widehat{\\mathrm{Cov}}_{\\mathrm{model}}(\\hat{\\beta}) = \\hat{\\sigma}^2 (X^\\top W X)^{-1}\n$$\n系数 $\\hat{\\beta}_j$ 的基于模型的标准误是该矩阵对角线元素的平方根。该估计量仅在 $\\Omega = \\sigma^2 W^{-1}$ 的假设成立时才有效。\n\n**2. 三明治（异方差稳健）协方差估计量**\n该估计量对于权重矩阵 $W$ 的错误设定是稳健的。它不假设 $\\Omega = \\sigma^2 W^{-1}$。相反，它直接估计通用协方差公式的中间部分（“肉”）。我们通常假设误差不相关但可能存在异方差，因此 $\\Omega = \\mathrm{diag}(\\sigma_1^2, \\sigma_2^2, ..., \\sigma_n^2)$。“肉”项是 $M = X^\\top W \\Omega W X = \\sum_{i=1}^n w_i^2 \\sigma_i^2 x_i x_i^\\top$。\n为了估计 $M$，我们用它们的经验估计量，即残差平方 $r_i^2 = (y_i - x_i^\\top \\hat{\\beta})^2$ 来替代未知的方差 $\\sigma_i^2$。估计的“肉”矩阵是：\n$$\n\\hat{M} = \\sum_{i=1}^{n} w_i^2 r_i^2 x_i x_i^\\top = X^\\top W \\hat{\\Omega} W X\n$$\n其中 $\\hat{\\Omega} = \\mathrm{diag}(r_1^2, r_2^2, ..., r_n^2)$。那么三明治协方差估计量是：\n$$\n\\widehat{\\mathrm{Cov}}_{\\mathrm{sand}}(\\hat{\\beta}) = (X^\\top W X)^{-1} \\hat{M} (X^\\top W X)^{-1}\n$$\n三明治标准误是该矩阵对角线元素的平方根。它们是“一致的”，因为即使权重 $w_i$ 未能正确地对异方差进行建模，它们也能提供对 $\\hat{\\beta}$ 真实抽样变异性的有效近似。当权重被正确设定时（如案例 B），基于模型的估计量和三明治估计量应产生相似的结果。当权重设定错误时（如案例 C），它们预计会产生差异。\n\n**3. 离散度量 $D$**\n为了评估加权方案在实现同方差性方面的效果，我们检查标准化残差。原始残差是 $r_i = y_i - x_i^\\top \\hat{\\beta}$。标准化残差，对应于转换后的 OLS 问题的残差，是 $z_i = \\sqrt{w_i} r_i$。如果加权成功，$z_i$ 应具有大致恒定的方差。问题定义了一个离散度量 $D$ 来量化这一点：\n$$\nD = \\frac{\\mathrm{std}(|z|)}{\\mathrm{mean}(|z|)}\n$$\n其中 $|z|$ 是标准化残差绝对值的向量，$\\mathrm{std}$ 是样本标准差（使用分母 $n-1$），$\\mathrm{mean}$ 是样本均值。较小的 $D$ 值表示标准化残差的绝对值相对于其平均大小具有较低的变异性，表明加权后更接近同方差性。例如，如果所有 $|z_i|$ 都相同，则 $\\mathrm{std}(|z|)$ 将为 $0$，因此 $D$ 也为 $0$，表示在量值上完全同方差。", "answer": "```python\nimport numpy as np\n\ndef solve_case(X, y, w):\n    \"\"\"\n    Performs WLS estimation and computes required statistics for a single case.\n    \n    Args:\n        X (np.ndarray): Design matrix (n x p).\n        y (np.ndarray): Response vector (n x 1).\n        w (np.ndarray): Weight vector (n x 1).\n\n    Returns:\n        list: A list of 7 floats containing beta_hat, model-based SEs, \n              sandwich SEs, and the dispersion metric D.\n    \"\"\"\n    n, p = X.shape\n    W = np.diag(w)\n\n    # 1. Compute WLS estimator beta_hat\n    # beta_hat = (X^T W X)^-1 X^T W y\n    XT_W = X.T @ W\n    XT_W_X = XT_W @ X\n    inv_XT_W_X = np.linalg.inv(XT_W_X)\n    XT_W_y = XT_W @ y\n    beta_hat = inv_XT_W_X @ XT_W_y\n\n    # Compute residuals\n    residuals = y - X @ beta_hat\n    \n    # 2. Compute model-based standard errors\n    # cov_model = sigma_hat^2 * (X^T W X)^-1\n    # sigma_hat^2 = sum(w_i * r_i^2) / (n - p)\n    wrss = np.sum(w * residuals**2)\n    sigma_sq_hat = wrss / (n - p)\n    cov_model = sigma_sq_hat * inv_XT_W_X\n    se_model = np.sqrt(np.diag(cov_model))\n    \n    # 3. Compute sandwich (heteroskedasticity-consistent) standard errors\n    # cov_sand = (X^T W X)^-1 * (X^T W Omega_hat W X) * (X^T W X)^-1\n    # where Omega_hat = diag(r_i^2)\n    # The \"meat\" is M_hat = sum(w_i^2 * r_i^2 * x_i * x_i^T)\n    meat = np.zeros((p, p))\n    for i in range(n):\n        xi = X[i, :].reshape(p, 1)\n        meat += (w[i]**2) * (residuals[i]**2) * (xi @ xi.T)\n    \n    cov_sand = inv_XT_W_X @ meat @ inv_XT_W_X\n    se_sand = np.sqrt(np.diag(cov_sand))\n    \n    # 4. Compute dispersion metric D\n    # z_i = sqrt(w_i) * r_i\n    # D = std(|z|) / mean(|z|)\n    z = np.sqrt(w) * residuals\n    abs_z = np.abs(z)\n    \n    # Check for mean(|z|) == 0 to avoid division by zero\n    mean_abs_z = np.mean(abs_z)\n    if mean_abs_z == 0:\n        D = 0.0 # If all residuals are zero, variance is zero, dispersion is zero.\n    else:\n        # Use ddof=1 for sample standard deviation\n        std_abs_z = np.std(abs_z, ddof=1)\n        D = std_abs_z / mean_abs_z\n\n    return [\n        beta_hat[0], beta_hat[1],\n        se_model[0], se_model[1],\n        se_sand[0], se_sand[1],\n        D\n    ]\n\ndef solve():\n    \"\"\"\n    Main function to define test cases, run calculations, and print results.\n    \"\"\"\n    x_i = np.array([0, 1, 2, 3, 4, 5], dtype=float)\n    X = np.vstack([np.ones_like(x_i), x_i]).T\n\n    # Case A\n    beta_a = np.array([1.0, 2.0])\n    eps_a = np.array([0.2, -0.1, 0.0, 0.1, -0.2, 0.0])\n    y_a = X @ beta_a + eps_a\n    w_a = np.ones(6)\n\n    # Case B\n    beta_b = np.array([1.0, 2.0])\n    z_b = np.array([0.5, -0.5, 0.5, -0.5, 0.5, -0.5])\n    w_b = np.array([4.0, 4.0, 1.0, 1.0, 0.25, 0.25])\n    eps_b = z_b / np.sqrt(w_b)\n    y_b = X @ beta_b + eps_b\n\n    # Case C\n    y_c = y_b  # same data as Case B\n    w_c = np.array([0.25, 0.25, 1.0, 1.0, 4.0, 4.0])\n\n    # Case D\n    beta_d = np.array([0.0, -1.0])\n    z_d = np.array([1.0, -1.0, 1.0, -1.0, 1.0, -1.0])\n    w_d = np.array([1000.0, 0.001, 10.0, 0.1, 50.0, 0.5])\n    eps_d = z_d / np.sqrt(w_d)\n    y_d = X @ beta_d + eps_d\n\n    test_cases = [\n        (X, y_a, w_a),\n        (X, y_b, w_b),\n        (X, y_c, w_c),\n        (X, y_d, w_d),\n    ]\n\n    all_results = []\n    for X_case, y_case, w_case in test_cases:\n        result = solve_case(X_case, y_case, w_case)\n        all_results.append(result)\n\n    # Format output as a string representing a list of lists.\n    # e.g., \"[[r1, r2, ...], [r1, r2, ...]]\"\n    case_strings = []\n    for case_result in all_results:\n        case_strings.append(f\"[{','.join(f'{x:.7f}' for x in case_result)}]\")\n    \n    print(f\"[{','.join(case_strings)}]\")\n\nsolve()\n```", "id": "3099884"}]}