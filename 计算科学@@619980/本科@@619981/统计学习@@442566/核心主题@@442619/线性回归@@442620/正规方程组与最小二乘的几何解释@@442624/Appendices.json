{"hands_on_practices": [{"introduction": "最小二乘法不仅仅是一个优化问题；它在几何上有一个优美的解释，这个解释是理解其许多性质的关键。这个练习的核心是揭示最小化残差平方和与一个基本的正交性条件是等价的：最优残差向量必须与特征矩阵 $X$ 的所有列向量正交。你将首先从微积分和线性代数的基本原理出发推导出这一结论，然后通过编写代码在数值上验证它，从而将抽象的理论与具体的计算实践联系起来 [@problem_id:3186054]。", "problem": "考虑在欧几里得空间中拟合线性模型的普通最小二乘法（OLS）问题，其中设计矩阵为 $X \\in \\mathbb{R}^{n \\times p}$，其列向量表示为 $x_1, x_2, \\dots, x_p$，响应向量为 $y \\in \\mathbb{R}^n$。对于一个候选系数向量 $\\beta \\in \\mathbb{R}^p$，其残差定义为 $r(\\beta) = y - X\\beta$。从 OLS 选择 $\\hat{\\beta}$ 以最小化残差的平方欧几里得范数（即目标函数 $J(\\beta) = \\frac{1}{2}\\|r(\\beta)\\|_2^2$）这一基本原则出发，请基于多元微积分和线性代数的基本原理，推导为何最小化残差 $r(\\hat{\\beta})$ 与 $X$ 的每个列向量 $x_j$ 正交。你的推导应仅依赖于对 $J(\\beta)$ 进行微分，并使用有限维实向量空间中内积和正交投影的性质，而不应预先援引任何目标公式。\n\n完成推导后，编写一个程序，通过在合成测试用例中计算点积 $x_j^\\top r(\\hat{\\beta})$ 来数值验证该正交条件。对于数值验证，将绝对值小于容差 $\\varepsilon = 10^{-10}$ 的值视为零。\n\n你的程序必须在每个测试用例中执行以下步骤：\n- 使用数值稳定的最小二乘法计算一个系数向量 $\\hat{\\beta}$，以最小化 $\\|y - X\\hat{\\beta}\\|_2$。\n- 计算残差 $r(\\hat{\\beta}) = y - X\\hat{\\beta}$。\n- 计算 $X$ 的所有列向量 $x_j$ 与残差的点积 $x_j^\\top r(\\hat{\\beta})$。\n- 返回一个布尔值，指示所有这些点积的绝对值是否都小于 $\\varepsilon$。\n\n使用以下测试套件。所有条目均为实数。\n\n测试用例 $1$ (超定，满列秩):\n$$\nX_1 = \\begin{bmatrix}\n1  0  2 \\\\\n0  1  -1 \\\\\n1  1  0 \\\\\n2  -1  1 \\\\\n-1  2  3\n\\end{bmatrix},\\quad\ny_1 = \\begin{bmatrix}\n3 \\\\\n-1 \\\\\n2 \\\\\n0 \\\\\n5\n\\end{bmatrix}。\n$$\n\n测试用例 $2$ (方阵，恰定，可逆):\n$$\nX_2 = \\begin{bmatrix}\n2  0  0 \\\\\n0  3  0 \\\\\n0  0  4\n\\end{bmatrix},\\quad\ny_2 = \\begin{bmatrix}\n2 \\\\\n6 \\\\\n8\n\\end{bmatrix}。\n$$\n\n测试用例 $3$ (超定，秩亏，其中 $x_3 = x_1 + x_2$):\n$$\nX_3 = \\begin{bmatrix}\n1  0  1 \\\\\n0  1  1 \\\\\n1  1  2 \\\\\n-1  2  1\n\\end{bmatrix},\\quad\ny_3 = \\begin{bmatrix}\n0 \\\\\n1 \\\\\n3 \\\\\n-2\n\\end{bmatrix}。\n$$\n\n测试用例 $4$ (欠定，列数多于行数):\n$$\nX_4 = \\begin{bmatrix}\n1  0  1  2 \\\\\n0  1  1  -1 \\\\\n1  -1  0  3\n\\end{bmatrix},\\quad\ny_4 = \\begin{bmatrix}\n1 \\\\\n2 \\\\\n-1\n\\end{bmatrix}。\n$$\n\n最终输出格式：你的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表（例如 $[result_1,result_2,result_3,result_4]$），其中每个 $result_k$ 是测试用例 $k$ 的布尔值，表示是否所有点积 $x_j^\\top r(\\hat{\\beta})$ 的绝对值都小于 $\\varepsilon$。\n\n不需要物理单位或角度单位。使用绝对值阈值 $\\varepsilon = 10^{-10}$ 表示所有容差检查。", "solution": "问题陈述经评估有效。它提出了一个线性代数和统计学习中的标准、适定问题，没有任何科学、逻辑或事实上的缺陷。理论推导和数值验证所需的所有数据和条件都已提供。\n\n### 普通最小二乘法中正交条件的推导\n\n普通最小二乘法（OLS）的核心原则是找到一个系数向量 $\\hat{\\beta} \\in \\mathbb{R}^p$ 来最小化残差平方和。这等价于最小化残差向量 $r(\\beta) = y - X\\beta$ 的平方欧几里得范数。需要最小化的目标函数由下式给出：\n$$\nJ(\\beta) = \\frac{1}{2}\\|r(\\beta)\\|_2^2 = \\frac{1}{2}\\|y - X\\beta\\|_2^2\n$$\n在这里，$y \\in \\mathbb{R}^n$ 是响应向量，$X \\in \\mathbb{R}^{n \\times p}$ 是设计矩阵，其列向量为 $x_1, \\dots, x_p$，而 $\\beta \\in \\mathbb{R}^p$ 是系数向量。因子 $\\frac{1}{2}$ 是为了数学上的方便而引入的，它不改变最小值的位置。\n\n为了找到最小化这个标量值函数的向量 $\\hat{\\beta}$，我们采用多元微积分的方法。最小化过程需要找到使 $J(\\beta)$ 关于 $\\beta$ 的梯度为零向量的点。\n\n**步骤 1：使用内积表示目标函数。**\n向量 $v$ 的平方欧几里得范数等价于它与自身的内积，即 $\\|v\\|_2^2 = v^\\top v$。将此应用于我们的目标函数：\n$$\nJ(\\beta) = \\frac{1}{2}(y - X\\beta)^\\top (y - X\\beta)\n$$\n我们使用矩阵转置的性质，特别是 $(AB)^\\top = B^\\top A^\\top$，来展开这个表达式：\n$$\nJ(\\beta) = \\frac{1}{2}(y^\\top - (X\\beta)^\\top)(y - X\\beta) = \\frac{1}{2}(y^\\top - \\beta^\\top X^\\top)(y - X\\beta)\n$$\n分配各项可得：\n$$\nJ(\\beta) = \\frac{1}{2}(y^\\top y - y^\\top X\\beta - \\beta^\\top X^\\top y + \\beta^\\top X^\\top X\\beta)\n$$\n项 $\\beta^\\top X^\\top y$ 是一个 $1 \\times 1$ 矩阵，即一个标量。标量等于其自身的转置。其转置为 $(\\beta^\\top X^\\top y)^\\top = y^\\top (X^\\top)^\\top (\\beta^\\top)^\\top = y^\\top X \\beta$。因此，中间两项是相同的。我们可以将它们合并：\n$$\nJ(\\beta) = \\frac{1}{2}(y^\\top y - 2\\beta^\\top X^\\top y + \\beta^\\top X^\\top X\\beta)\n$$\n\n**步骤 2：对目标函数关于 $\\beta$ 求导。**\n我们计算 $J(\\beta)$ 的梯度，记作 $\\nabla_\\beta J(\\beta)$，它是一个由 $J(\\beta)$ 对 $\\beta$ 的每个分量 $\\beta_k$ 的偏导数组成的向量。我们使用向量微积分中的以下标准结果：\n-   对于向量 $a$，$\\nabla_\\beta (\\beta^\\top a) = a$。\n-   对于对称矩阵 $A$，$\\nabla_\\beta (\\beta^\\top A \\beta) = 2A\\beta$。矩阵 $X^\\top X$ 是对称的，因为 $(X^\\top X)^\\top = X^\\top (X^\\top)^\\top = X^\\top X$。\n\n让我们逐项对 $J(\\beta)$ 进行微分：\n1.  项 $\\frac{1}{2}y^\\top y$ 相对于 $\\beta$ 是常数，所以其梯度是零向量。\n2.  项 $-\\beta^\\top X^\\top y$ 的形式为 $-\\beta^\\top a$，其中 $a = X^\\top y$。其梯度为 $-X^\\top y$。\n3.  项 $\\frac{1}{2}\\beta^\\top (X^\\top X) \\beta$ 的形式为 $\\frac{1}{2}\\beta^\\top A \\beta$，其中 $A = X^\\top X$。其梯度为 $\\frac{1}{2}(2(X^\\top X)\\beta) = (X^\\top X)\\beta$。\n\n综合这些结果，目标函数的梯度为：\n$$\n\\nabla_\\beta J(\\beta) = -(X^\\top y) + (X^\\top X)\\beta\n$$\n\n**步骤 3：将梯度设为零以求最小值。**\n对于凸函数 $J(\\beta)$，$\\hat{\\beta}$ 成为最小化点的必要条件是在 $\\hat{\\beta}$ 处的梯度为零向量：\n$$\n\\nabla_\\beta J(\\hat{\\beta}) = 0\n$$\n代入我们的梯度表达式：\n$$\n(X^\\top X)\\hat{\\beta} - X^\\top y = 0\n$$\n这可以重排为著名的*正规方程*（normal equations）：\n$$\nX^\\top X\\hat{\\beta} = X^\\top y\n$$\n我们可以通过将所有项移到一边来进一步重排此方程：\n$$\nX^\\top y - X^\\top X\\hat{\\beta} = 0\n$$\n提出因子 $X^\\top$ 可得：\n$$\nX^\\top (y - X\\hat{\\beta}) = 0\n$$\n\n**步骤 4：从正交性的角度解释结果。**\n括号内的向量是最优残差向量 $r(\\hat{\\beta}) = y - X\\hat{\\beta}$。因此，该方程表明：\n$$\nX^\\top r(\\hat{\\beta}) = 0\n$$\n我们来分析这个矩阵-向量积的结构。矩阵 $X^\\top$ 的行是 $X$ 的转置列：\n$$\nX^\\top = \\begin{bmatrix}\n-  x_1^\\top  - \\\\\n-  x_2^\\top  - \\\\\n \\vdots  \\\\\n-  x_p^\\top  -\n\\end{bmatrix}\n$$\n乘积 $X^\\top r(\\hat{\\beta})$ 是 $\\mathbb{R}^p$ 中的一个向量，其第 $j$ 个元素是 $X^\\top$ 的第 $j$ 行（即 $x_j^\\top$）与向量 $r(\\hat{\\beta})$ 的内积：\n$$\nX^\\top r(\\hat{\\beta}) = \\begin{bmatrix}\nx_1^\\top r(\\hat{\\beta}) \\\\\nx_2^\\top r(\\hat{\\beta}) \\\\\n\\vdots \\\\\nx_p^\\top r(\\hat{\\beta})\n\\end{bmatrix}\n$$\n条件 $X^\\top r(\\hat{\\beta}) = 0$ 意味着这个结果向量的每个元素都必须为零：\n$$\nx_j^\\top r(\\hat{\\beta}) = 0 \\quad \\text{对于所有 } j = 1, 2, \\dots, p\n$$\n表达式 $x_j^\\top r(\\hat{\\beta})$ 是向量 $x_j$ 和 $r(\\hat{\\beta})$ 之间内积（点积）的定义。当实向量空间中两个向量的内积为零时，根据定义，它们是正交的。\n\n因此，最小化残差平方和的条件直接且必然地意味着，所得到的残差向量 $r(\\hat{\\beta})$ 必须与设计矩阵 $X$ 的每一个列向量 $x_j$ 正交。这也意味着残差向量与 $X$ 的列空间正交，因为这些列向量构成了该子空间的一个基（或生成集）。向量 $X\\hat{\\beta}$ 是 $y$ 在 $X$ 的列空间上的正交投影。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Derives and numerically verifies that the OLS residual vector is orthogonal\n    to the columns of the design matrix.\n    \"\"\"\n    # Define the tolerance for numerical verification.\n    epsilon = 1e-10\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test case 1: Overdetermined, full column rank\n        (np.array([\n            [1.0, 0.0, 2.0],\n            [0.0, 1.0, -1.0],\n            [1.0, 1.0, 0.0],\n            [2.0, -1.0, 1.0],\n            [-1.0, 2.0, 3.0]\n        ]),\n         np.array([3.0, -1.0, 2.0, 0.0, 5.0])),\n\n        # Test case 2: Square, exactly determined, invertible\n        (np.array([\n            [2.0, 0.0, 0.0],\n            [0.0, 3.0, 0.0],\n            [0.0, 0.0, 4.0]\n        ]),\n         np.array([2.0, 6.0, 8.0])),\n        \n        # Test case 3: Overdetermined, rank-deficient\n        (np.array([\n            [1.0, 0.0, 1.0],\n            [0.0, 1.0, 1.0],\n            [1.0, 1.0, 2.0],\n            [-1.0, 2.0, 1.0]\n        ]),\n         np.array([0.0, 1.0, 3.0, -2.0])),\n\n        # Test case 4: Underdetermined, more columns than rows\n        (np.array([\n            [1.0, 0.0, 1.0, 2.0],\n            [0.0, 1.0, 1.0, -1.0],\n            [1.0, -1.0, 0.0, 3.0]\n        ]),\n         np.array([1.0, 2.0, -1.0]))\n    ]\n\n    results = []\n    for X, y in test_cases:\n        # Step 1: Compute the coefficient vector beta_hat that minimizes ||y - X*beta||.\n        # np.linalg.lstsq provides a numerically stable solution and handles all cases\n        # (full rank, rank-deficient, over/underdetermined).\n        # rcond=None is set to use the machine-precision-dependent default and suppress future warnings.\n        beta_hat = np.linalg.lstsq(X, y, rcond=None)[0]\n\n        # Step 2: Compute the residual vector r(beta_hat) = y - X*beta_hat.\n        residual = y - X @ beta_hat\n\n        # Step 3: Compute the dot products of each column of X with the residual vector.\n        # This is efficiently computed as the matrix-vector product X^T * r.\n        dot_products = X.T @ residual\n\n        # Step 4: Verify if the absolute value of all dot products is less than the tolerance.\n        # np.all returns True if all elements in the boolean array are True.\n        are_all_orthogonal = np.all(np.abs(dot_products)  epsilon)\n        \n        results.append(are_all_orthogonal)\n\n    # Final print statement in the exact required format.\n    # The map(str, ...) converts each boolean in `results` to its string representation ('True' or 'False').\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3186054"}, {"introduction": "建立了正交性原理后，我们可以用它来剖析一些统计学中看似矛盾的现象。本练习将探讨“抑制效应” (suppression effect)，这是一种反直觉的情景：当模型中加入另一个变量后，一个预测变量与响应变量之间的关系符号会发生反转。通过将“控制一个变量”解释为一种正交投影，你将能够从几何角度推导并解释这一效应，将一个令人困惑的悖论转化为清晰的几何直觉 [@problem_id:3132937]。", "problem": "考虑一个总体，其中预测变量对 $\\{x_1, x_2\\}$ 和响应变量 $y$ 联合分布且具有有限二阶矩。预测变量经过标准化，使得 $\\operatorname{E}[x_1] = \\operatorname{E}[x_2] = 0$ 且 $\\operatorname{Var}(x_1) = \\operatorname{Var}(x_2) = 1$。假设预测变量呈正相关，对于某个 $0  \\rho  1$ 有 $\\operatorname{Cov}(x_1, x_2) = \\rho$，并且响应变量的均值为零，即 $\\operatorname{E}[y] = 0$。在该总体上的经验研究得出以下协方差：$\\operatorname{Cov}(x_1, y) = 0.2$ 和 $\\operatorname{Cov}(x_2, y) = 0.8$。\n\n我们通过普通最小二乘法 (OLS) 拟合多元线性模型 $y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\varepsilon$，该方法可解释为将 $y$ 投影到由 $\\{x_1, x_2\\}$ 张成的空间上的线性投影，以最小化期望残差平方。从协方差、方差的基本定义以及线性投影的最小二乘最优性（正规方程）出发，推导系数 $\\beta_1$ 作为 $\\rho$ 的函数，然后在 $\\rho = 0.9$ 处计算其值。在推导过程中，请阐明为何 $x_1$ 和 $y$ 之间的正边际关联并不排除在控制了 $x_2$ 后出现负的偏系数 $\\beta_1$（抑制效应），并使用正交投影从几何角度解释这种符号反转。\n\n计算 $\\beta_1$ 在 $\\rho = 0.9$ 时的数值，并将答案四舍五入到四位有效数字。该系数无需单位。", "solution": "该问题要求推导一个多元回归系数，解释抑制效应，并进行数值计算。我们首先验证问题的陈述。\n\n### 问题验证\n该问题在理论统计学和线性模型的框架内是良定义的。\n**已知条件：**\n- 预测变量 $\\{x_1, x_2\\}$ 和响应变量 $y$ 是具有有限二阶矩的随机变量。\n- $\\operatorname{E}[x_1] = 0$, $\\operatorname{E}[x_2] = 0$, $\\operatorname{E}[y] = 0$。\n- $\\operatorname{Var}(x_1) = 1$, $\\operatorname{Var}(x_2) = 1$。\n- $\\operatorname{Cov}(x_1, x_2) = \\rho$，其中 $0  \\rho  1$。\n- $\\operatorname{Cov}(x_1, y) = 0.2$。\n- $\\operatorname{Cov}(x_2, y) = 0.8$。\n- 模型为 $y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\varepsilon$，通过普通最小二乘法 (OLS) 拟合。\n\n**验证：**\n1.  **科学依据：** 该问题使用了期望、方差、协方差和普通最小二乘回归等标准的、公认的概念。所有前提都与统计理论一致。\n2.  **适定性：** 该问题提供了确定 OLS 系数 $\\beta_1$ 和 $\\beta_2$ 所需的所有信息（协方差和方差）。条件 $0  \\rho  1$ 确保了预测变量之间不是完全共线性的，从而保证了解的唯一性。\n3.  **客观性：** 该问题以精确的数学语言陈述，并要求进行形式推导和数值计算。\n\n该问题被认为是有效的和自洽的。\n\n### 系数 $\\beta_1$ 的推导\nOLS 过程旨在最小化期望残差平方，$S = \\operatorname{E}[\\varepsilon^2] = \\operatorname{E}[(y - \\beta_0 - \\beta_1 x_1 - \\beta_2 x_2)^2]$。\n\n首先，我们确定截距 $\\beta_0$。对模型方程取期望：\n$$ \\operatorname{E}[y] = \\operatorname{E}[\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\varepsilon] = \\beta_0 + \\beta_1 \\operatorname{E}[x_1] + \\beta_2 \\operatorname{E}[x_2] + \\operatorname{E}[\\varepsilon] $$\n根据 OLS 的构造，它确保残差的期望值为零，即 $\\operatorname{E}[\\varepsilon] = 0$。利用给定的零均值条件，我们有：\n$$ 0 = \\beta_0 + \\beta_1(0) + \\beta_2(0) + 0 $$\n这意味着 $\\beta_0 = 0$。模型简化为 $y = \\beta_1 x_1 + \\beta_2 x_2 + \\varepsilon$。\n\n对 $S = \\operatorname{E}[(y - \\beta_1 x_1 - \\beta_2 x_2)^2]$ 的最小化可导出正规方程，该方程表明残差向量必须与每个预测变量向量正交。用期望表示，这意味着 $\\operatorname{E}[\\varepsilon x_1] = 0$ 和 $\\operatorname{E}[\\varepsilon x_2] = 0$。\n$$ \\operatorname{E}[(y - \\beta_1 x_1 - \\beta_2 x_2)x_1] = 0 \\implies \\operatorname{E}[yx_1] - \\beta_1 \\operatorname{E}[x_1^2] - \\beta_2 \\operatorname{E}[x_1x_2] = 0 $$\n$$ \\operatorname{E}[(y - \\beta_1 x_1 - \\beta_2 x_2)x_2] = 0 \\implies \\operatorname{E}[yx_2] - \\beta_1 \\operatorname{E}[x_1x_2] - \\beta_2 \\operatorname{E}[x_2^2] = 0 $$\n鉴于所有变量的均值为零，乘积的期望等于协方差，平方的期望等于方差：\n- $\\operatorname{E}[yx_1] = \\operatorname{Cov}(x_1, y) = 0.2$\n- $\\operatorname{E}[yx_2] = \\operatorname{Cov}(x_2, y) = 0.8$\n- $\\operatorname{E}[x_1^2] = \\operatorname{Var}(x_1) = 1$\n- $\\operatorname{E}[x_2^2] = \\operatorname{Var}(x_2) = 1$\n- $\\operatorname{E}[x_1x_2] = \\operatorname{Cov}(x_1, x_2) = \\rho$\n\n将这些值代入正规方程，得到关于 $\\beta_1$ 和 $\\beta_2$ 的以下线性方程组：\n$$ \\begin{cases} (1) \\quad 1 \\cdot \\beta_1 + \\rho \\cdot \\beta_2 = 0.2 \\\\ (2) \\quad \\rho \\cdot \\beta_1 + 1 \\cdot \\beta_2 = 0.8 \\end{cases} $$\n我们可以求解该方程组以得到 $\\beta_1$。从方程 (1) 中，我们将 $\\beta_2$ 表示为 $\\beta_1$ 的函数：\n$$ \\beta_2 = \\frac{0.2 - \\beta_1}{\\rho} $$\n将此代入方程 (2)：\n$$ \\rho \\beta_1 + \\left( \\frac{0.2 - \\beta_1}{\\rho} \\right) = 0.8 $$\n乘以 $\\rho$（非零）以消去分母：\n$$ \\rho^2 \\beta_1 + 0.2 - \\beta_1 = 0.8 \\rho $$\n$$ \\beta_1(\\rho^2 - 1) = 0.8 \\rho - 0.2 $$\n$$ \\beta_1 = \\frac{0.8 \\rho - 0.2}{\\rho^2 - 1} = \\frac{-(0.2 - 0.8 \\rho)}{-(1 - \\rho^2)} $$\n这就得出了 $\\beta_1$ 作为 $\\rho$ 函数的表达式：\n$$ \\beta_1 = \\frac{0.2 - 0.8 \\rho}{1 - \\rho^2} $$\n\n### 抑制效应与几何解释\n$x_1$ 和 $y$ 之间的边际关联由 $\\operatorname{Cov}(x_1, y) = 0.2$ 给出，是正的。然而，偏系数 $\\beta_1$ 可以是负的。这种现象是抑制效应的一种形式。$\\beta_1$ 为负的条件是：\n$$ \\frac{0.2 - 0.8 \\rho}{1 - \\rho^2}  0 $$\n因为 $0  \\rho  1$，分母 $1 - \\rho^2$ 是正的。因此，$\\beta_1$ 的符号由分子决定：\n$$ 0.2 - 0.8 \\rho  0 \\implies 0.2  0.8 \\rho \\implies \\rho > \\frac{0.2}{0.8} = 0.25 $$\n所以，如果 $\\rho > 0.25$，尽管 $x_1$ 和 $y$ 的边际协方差为正，但 $x_1$ 对 $y$ 的偏效应是负的。\n\n**概念解释：**\n系数 $\\beta_1$ 表示在保持 $x_2$ 不变的情况下，$x_1$ 每增加一个单位，$y$ 的期望变化量。预测变量 $x_2$ 与 $y$ 有很强的正相关性（$\\operatorname{Cov}(x_2, y) = 0.8$），并且 $x_1$ 也与 $x_2$ 正相关（$\\rho > 0$）。观测到的 $x_1$ 和 $y$ 之间的部分正相关性是通过 $x_2$ 作为中介的：$x_1$ 的增加与 $x_2$ 的增加相关，而 $x_2$ 的增加又与 $y$ 的强劲增加相关。多元回归“分离”了这种间接效应。如果通过 $x_2$ 的间接正相关性足够强（即 $\\rho$ 足够大），它就可能掩盖了 $x_1$ 和 $y$ 之间潜在的负向直接关联。\n\n**几何解释：**\n在零均值随机变量的向量空间中，OLS 系数 $\\beta_1$ 可以通过 Frisch-Waugh-Lovell 定理所描述的正交投影来理解。系数 $\\beta_1$ 是 $y$ 对 $x_1$ 中正交于 $x_2$ 的部分进行回归的回归系数。令 $x_{1|2}$ 为将 $x_1$ 对 $x_2$ 回归所得的残差：\n$$ x_{1|2} = x_1 - \\frac{\\operatorname{Cov}(x_1, x_2)}{\\operatorname{Var}(x_2)} x_2 = x_1 - \\rho x_2 $$\n那么 $\\beta_1$ 由 $y$ 对 $x_{1|2}$ 的简单回归给出：\n$$ \\beta_1 = \\frac{\\operatorname{Cov}(y, x_{1|2})}{\\operatorname{Var}(x_{1|2})} $$\n分子是：\n$$ \\operatorname{Cov}(y, x_1 - \\rho x_2) = \\operatorname{Cov}(y, x_1) - \\rho \\operatorname{Cov}(y, x_2) = 0.2 - \\rho(0.8) $$\n分母是：\n$$ \\operatorname{Var}(x_1 - \\rho x_2) = \\operatorname{Var}(x_1) - 2\\rho \\operatorname{Cov}(x_1, x_2) + \\rho^2 \\operatorname{Var}(x_2) = 1 - 2\\rho(\\rho) + \\rho^2(1) = 1 - \\rho^2 $$\n因此，$\\beta_1 = \\frac{0.2 - 0.8\\rho}{1 - \\rho^2}$，这证实了我们的推导。从几何上看，$\\beta_1$ 并非由向量 $y$ 到 $x_1$ 的投影（该投影为正）决定，而是由 $y$ 到向量 $x_{1|2}$（$x_1$ 中正交于 $x_2$ 的部分）的投影决定。如果 $\\rho$ 是一个较大的正数，向量 $x_1$ 和 $x_2$ 的方向很接近。向量 $x_{1|2}$ 代表了从 $\\rho x_2$ 变为 $x_1$ 所需的“修正量”。即使 $y$ 到 $x_1$ 的投影是正的，$y$ 到这个“修正”向量上的投影也可能是负的。当 $y$ 受 $x_2$ 的强拉力（由于 $\\operatorname{Cov}(y, x_2) = 0.8$）占主导地位，以至于 $x_1$ 的残差部分所需的向量分量变为负值时，就会发生这种情况。\n\n### 数值计算\n我们计算 $\\rho = 0.9$ 时的 $\\beta_1$ 值：\n$$ \\beta_1 = \\frac{0.2 - 0.8(0.9)}{1 - (0.9)^2} = \\frac{0.2 - 0.72}{1 - 0.81} = \\frac{-0.52}{0.19} $$\n$$ \\beta_1 \\approx -2.736842... $$\n四舍五入到四位有效数字，我们得到 $\\beta_1 = -2.737$。\n结果证实了抑制效应：当相关性很高时（$\\rho = 0.9 > 0.25$），偏系数 $\\beta_1$ 确实是负的。", "answer": "$$\\boxed{-2.737}$$", "id": "3132937"}, {"introduction": "最小二乘法的几何基础不仅揭示了它的强大之处，也暴露了其潜在的弱点。本练习探讨了“病态条件”(ill-conditioning)问题，即当预测变量之间几乎共线时，特征矩阵 $X$ 的列空间会变得近乎“退化”。你将研究这种不良的几何结构如何急剧放大响应向量 $y$ 中的微小变化或噪声，从而导致极不稳定和不可靠的系数估计，这对于任何进行模型拟合的实践者来说都是一个至关重要的警示 [@problem_id:3186018]。", "problem": "给定一个高设计矩阵族和一个以微小角度旋转的目标向量族。你必须量化普通最小二乘法中从目标旋转到参数摆动的几何放大效应，并汇总一小组测试套件的结果。\n\n按如下方式构造设计矩阵 $X(\\varepsilon) \\in \\mathbb{R}^{6 \\times 3}$。设 $X(\\varepsilon)$ 的所有元素均为 $0$，除了前三行，其为对角矩阵，对角线上的元素为 $1$、$1$ 和 $\\varepsilon$：\n$$\nX(\\varepsilon) \\;=\\;\n\\begin{bmatrix}\n1  0  0 \\\\\n0  1  0 \\\\\n0  0  \\varepsilon \\\\\n0  0  0 \\\\\n0  0  0 \\\\\n0  0  0\n\\end{bmatrix}.\n$$\n对于以弧度为单位的旋转角 $\\theta$，通过将向量 $y(0) = [\\,1,\\,0,\\,0,\\,0,\\,0,\\,0\\,]^{\\top}$ 在由第一和第三坐标轴张成的二维子空间中旋转角度 $\\theta$ 来定义一个单位向量 $y(\\theta) \\in \\mathbb{R}^{6}$，同时保持所有其他坐标不变：\n$$\ny(\\theta) \\;=\\; \\begin{bmatrix} \\cos(\\theta) \\\\ 0 \\\\ \\sin(\\theta) \\\\ 0 \\\\ 0 \\\\ 0 \\end{bmatrix}.\n$$\n对于下面测试套件中的每一对 $(\\varepsilon,\\theta)$，令 $\\widehat{\\beta}(\\theta)$ 为最小化最小二乘目标函数 $\\|X(\\varepsilon)\\,\\beta - y(\\theta)\\|_{2}$（其中 $\\beta \\in \\mathbb{R}^{3}$）的最小欧几里得范数解。定义观测增益\n$$\nG(\\varepsilon,\\theta) \\;=\\; \\frac{\\|\\widehat{\\beta}(\\theta) - \\widehat{\\beta}(0)\\|_{2}}{\\|\\,y(\\theta) - y(0)\\|_{2}}.\n$$\n令 $\\kappa_{2}(X(\\varepsilon))$ 表示由欧几里得范数（也称2-范数）导出的 $X(\\varepsilon)$ 的谱条件数，即最大奇异值与最小非零奇异值之比。最后，定义归一化增益\n$$\nQ(\\varepsilon,\\theta) \\;=\\; \\frac{G(\\varepsilon,\\theta)}{\\kappa_{2}(X(\\varepsilon))}.\n$$\n\n任务：\n- 对于下面的每个测试用例，使用你的编程环境提供的精确算术运算计算 $Q(\\varepsilon,\\theta)$，所有角度均以弧度为单位。\n- 你必须按照上述定义计算最小二乘解和谱条件数。不需要此处指定之外的任何额外数据。\n\n测试套件（角度以弧度为单位）：\n- $(\\varepsilon,\\theta) = (10^{-3},\\,10^{-3})$\n- $(\\varepsilon,\\theta) = (10^{-6},\\,10^{-3})$\n- $(\\varepsilon,\\theta) = (1,\\,10^{-3})$\n- $(\\varepsilon,\\theta) = (10^{-3},\\,10^{-12})$\n- $(\\varepsilon,\\theta) = (10^{-12},\\,10^{-6})$\n\n输出规格：\n- 你的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，结果的顺序与上面给出的测试套件的顺序一致，即 $[\\,Q(\\varepsilon_{1},\\theta_{1}),\\,Q(\\varepsilon_{2},\\theta_{2}),\\,Q(\\varepsilon_{3},\\theta_{3}),\\,Q(\\varepsilon_{4},\\theta_{4}),\\,Q(\\varepsilon_{5},\\theta_{5})\\,]$。\n- 列表中的每个元素都必须是浮点数。", "solution": "该问题要求为几对参数 $(\\varepsilon, \\theta)$ 计算一个归一化增益度量 $Q(\\varepsilon, \\theta)$。这涉及到求解普通最小二乘（OLS）解、其对目标向量中扰动的敏感性，以及设计矩阵的条件数。该解法通过一系列解析推导，得到了一个用于计算 $Q(\\varepsilon, \\theta)$ 的简化且数值稳定的公式。\n\n首先，我们确定最小化目标函数 $\\|X(\\varepsilon)\\beta - y(\\theta)\\|_{2}$（其中 $\\beta \\in \\mathbb{R}^{3}$）的普通最小二乘（OLS）估计 $\\widehat{\\beta}(\\theta)$。问题指定我们应找到最小欧几里得范数解。OLS问题的解由正规方程给出：\n$$\n(X(\\varepsilon)^T X(\\varepsilon)) \\widehat{\\beta}(\\theta) = X(\\varepsilon)^T y(\\theta)\n$$\n矩阵 $X(\\varepsilon)^T X(\\varepsilon)$ 计算如下：\n$$\nX(\\varepsilon)^T X(\\varepsilon) =\n\\begin{bmatrix}\n1  0  0  0  0  0 \\\\\n0  1  0  0  0  0 \\\\\n0  0  \\varepsilon  0  0  0\n\\end{bmatrix}\n\\begin{bmatrix}\n1  0  0 \\\\\n0  1  0 \\\\\n0  0  \\varepsilon \\\\\n0  0  0 \\\\\n0  0  0 \\\\\n0  0  0\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n1  0  0 \\\\\n0  1  0 \\\\\n0  0  \\varepsilon^2\n\\end{bmatrix}\n$$\n对于所有测试用例，$\\varepsilon > 0$，这意味着 $\\varepsilon^2 > 0$。因此，$X(\\varepsilon)^T X(\\varepsilon)$ 是一个具有正元素的对角矩阵，使其可逆。这保证了 $\\widehat{\\beta}(\\theta)$ 有唯一解，该解本质上是最小范数解。其逆矩阵为：\n$$\n(X(\\varepsilon)^T X(\\varepsilon))^{-1} =\n\\begin{bmatrix}\n1  0  0 \\\\\n0  1  0 \\\\\n0  0  1/\\varepsilon^2\n\\end{bmatrix}\n$$\n接下来，我们计算项 $X(\\varepsilon)^T y(\\theta)$：\n$$\nX(\\varepsilon)^T y(\\theta) =\n\\begin{bmatrix}\n1  0  0  0  0  0 \\\\\n0  1  0  0  0  0 \\\\\n0  0  \\varepsilon  0  0  0\n\\end{bmatrix}\n\\begin{bmatrix} \\cos(\\theta) \\\\ 0 \\\\ \\sin(\\theta) \\\\ 0 \\\\ 0 \\\\ 0 \\end{bmatrix}\n=\n\\begin{bmatrix}\n\\cos(\\theta) \\\\\n0 \\\\\n\\varepsilon \\sin(\\theta)\n\\end{bmatrix}\n$$\n那么 OLS 解为 $\\widehat{\\beta}(\\theta) = (X(\\varepsilon)^T X(\\varepsilon))^{-1} (X(\\varepsilon)^T y(\\theta))$：\n$$\n\\widehat{\\beta}(\\theta) =\n\\begin{bmatrix}\n1  0  0 \\\\\n0  1  0 \\\\\n0  0  1/\\varepsilon^2\n\\end{bmatrix}\n\\begin{bmatrix}\n\\cos(\\theta) \\\\\n0 \\\\\n\\varepsilon \\sin(\\theta)\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\cos(\\theta) \\\\\n0 \\\\\n\\sin(\\theta)/\\varepsilon\n\\end{bmatrix}\n$$\n为了计算观测增益 $G(\\varepsilon, \\theta)$，我们需要差分向量 $\\widehat{\\beta}(\\theta) - \\widehat{\\beta}(0)$ 和 $y(\\theta) - y(0)$。\n对于 $\\theta=0$，我们有 $y(0) = [\\,1, 0, 0, 0, 0, 0\\,]^T$ 且 $\\widehat{\\beta}(0) = [\\,\\cos(0), 0, \\sin(0)/\\varepsilon\\,]^T = [\\,1, 0, 0\\,]^T$。\n参数向量的差为：\n$$\n\\widehat{\\beta}(\\theta) - \\widehat{\\beta}(0) =\n\\begin{bmatrix}\n\\cos(\\theta) - 1 \\\\\n0 \\\\\n\\sin(\\theta)/\\varepsilon\n\\end{bmatrix}\n$$\n该向量的欧几里得范数为：\n$$\n\\|\\widehat{\\beta}(\\theta) - \\widehat{\\beta}(0)\\|_{2} = \\sqrt{(\\cos(\\theta) - 1)^2 + (\\sin(\\theta)/\\varepsilon)^2}\n$$\n目标向量的差为：\n$$\ny(\\theta) - y(0) =\n\\begin{bmatrix}\n\\cos(\\theta) - 1 \\\\\n0 \\\\\n\\sin(\\theta) \\\\\n0 \\\\\n0 \\\\\n0\n\\end{bmatrix}\n$$\n该向量的欧几里得范数为：\n$$\n\\|y(\\theta) - y(0)\\|_{2} = \\sqrt{(\\cos(\\theta) - 1)^2 + \\sin^2(\\theta)} = \\sqrt{\\cos^2(\\theta) - 2\\cos(\\theta) + 1 + \\sin^2(\\theta)} = \\sqrt{2 - 2\\cos(\\theta)}\n$$\n使用半角恒等式 $1 - \\cos(\\theta) = 2\\sin^2(\\theta/2)$，我们得到：\n$$\n\\|y(\\theta) - y(0)\\|_{2} = \\sqrt{4\\sin^2(\\theta/2)} = 2|\\sin(\\theta/2)|\n$$\n对于测试套件中的小正角 $\\theta$，这可以简化为 $2\\sin(\\theta/2)$。\n观测增益是这些范数的比值：\n$$\nG(\\varepsilon, \\theta) = \\frac{\\|\\widehat{\\beta}(\\theta) - \\widehat{\\beta}(0)\\|_{2}}{\\|y(\\theta) - y(0)\\|_{2}} = \\frac{\\sqrt{(\\cos(\\theta) - 1)^2 + (\\sin(\\theta)/\\varepsilon)^2}}{2|\\sin(\\theta/2)|}\n$$\n接下来，我们计算谱条件数 $\\kappa_2(X(\\varepsilon))$。它是最大奇异值与最小非零奇异值之比。$X(\\varepsilon)$ 的奇异值是 $X(\\varepsilon)^T X(\\varepsilon) = \\text{diag}(1, 1, \\varepsilon^2)$ 的特征值的平方根。特征值为 $1$、$1$ 和 $\\varepsilon^2$。由于所有测试用例中 $\\varepsilon > 0$，所以奇异值为 $\\sigma_1=1$, $\\sigma_2=1$ 和 $\\sigma_3=\\varepsilon$。\n最大奇异值为 $\\sigma_{\\max} = \\max(1, \\varepsilon)$，最小非零奇异值为 $\\sigma_{\\min} = \\min(1, \\varepsilon)$。\n因此，条件数为：\n$$\n\\kappa_2(X(\\varepsilon)) = \\frac{\\max(1, \\varepsilon)}{\\min(1, \\varepsilon)}\n$$\n对于所有测试用例，$\\varepsilon \\le 1$，所以 $\\max(1, \\varepsilon) = 1$ 且 $\\min(1, \\varepsilon) = \\varepsilon$。这可以简化为：\n$$\n\\kappa_2(X(\\varepsilon)) = 1/\\varepsilon \\quad (\\text{对于 } \\varepsilon \\le 1)\n$$\n这包括 $\\varepsilon=1$ 的情况，此时 $\\kappa_2(X(1))=1$。\n\n最后，我们计算归一化增益 $Q(\\varepsilon, \\theta) = G(\\varepsilon, \\theta) / \\kappa_2(X(\\varepsilon))$。对于 $\\varepsilon \\le 1$，这等于：\n$$\nQ(\\varepsilon, \\theta) = \\varepsilon \\cdot G(\\varepsilon, \\theta) = \\varepsilon \\frac{\\sqrt{(\\cos(\\theta) - 1)^2 + (\\sin(\\theta)/\\varepsilon)^2}}{2|\\sin(\\theta/2)|}\n$$\n将因子 $\\varepsilon$ 移入平方根内：\n$$\nQ(\\varepsilon, \\theta) = \\frac{\\sqrt{\\varepsilon^2(\\cos(\\theta) - 1)^2 + \\sin^2(\\theta)}}{2|\\sin(\\theta/2)|}\n$$\n我们使用半角恒等式来简化分子：$\\cos(\\theta) - 1 = -2\\sin^2(\\theta/2)$ 和 $\\sin(\\theta) = 2\\sin(\\theta/2)\\cos(\\theta/2)$。\n$$\n\\text{分子} = \\sqrt{\\varepsilon^2(-2\\sin^2(\\theta/2))^2 + (2\\sin(\\theta/2)\\cos(\\theta/2))^2} = \\sqrt{4\\varepsilon^2\\sin^4(\\theta/2) + 4\\sin^2(\\theta/2)\\cos^2(\\theta/2)}\n$$\n从根式中提出 $4\\sin^2(\\theta/2)$（并回顾 $|\\sin(\\theta/2)| > 0$）：\n$$\n\\text{分子} = 2|\\sin(\\theta/2)|\\sqrt{\\varepsilon^2\\sin^2(\\theta/2) + \\cos^2(\\theta/2)}\n$$\n将此代回 $Q(\\varepsilon, \\theta)$ 的表达式中：\n$$\nQ(\\varepsilon, \\theta) = \\frac{2|\\sin(\\theta/2)|\\sqrt{\\varepsilon^2\\sin^2(\\theta/2) + \\cos^2(\\theta/2)}}{2|\\sin(\\theta/2)|} = \\sqrt{\\varepsilon^2\\sin^2(\\theta/2) + \\cos^2(\\theta/2)}\n$$\n这个最终公式对于给定的输入是简单的、精确的且数值稳定的。它适用于所有测试用例，因为它们都满足 $\\varepsilon \\le 1$。我们将使用这个公式进行最终计算。对于特殊情况 $\\varepsilon=1$，该公式得出 $Q(1, \\theta) = \\sqrt{\\sin^2(\\theta/2) + \\cos^2(\\theta/2)} = \\sqrt{1} = 1$，这是正确的。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the normalized gain Q for a series of test cases based on the\n    derived analytical formula.\n    \"\"\"\n    # Test suite (epsilon, theta) with angles in radians.\n    test_cases = [\n        (1e-3, 1e-3),\n        (1e-6, 1e-3),\n        (1.0, 1e-3),\n        (1e-3, 1e-12),\n        (1e-12, 1e-6),\n    ]\n\n    results = []\n    \n    # The derived analytical solution for Q(epsilon, theta) for epsilon = 1 is:\n    # Q = sqrt(epsilon^2 * sin^2(theta/2) + cos^2(theta/2))\n    # This formula is used for all test cases, as they all satisfy epsilon = 1.\n\n    for eps, theta in test_cases:\n        # Angle for half-angle identities\n        theta_half = theta / 2.0\n        \n        # Calculate sin and cos of the half angle\n        sin_theta_half = np.sin(theta_half)\n        cos_theta_half = np.cos(theta_half)\n        \n        # Apply the derived formula for Q\n        # Q^2 = eps^2 * sin^2(theta/2) + cos^2(theta/2)\n        q_squared = (eps**2) * (sin_theta_half**2) + (cos_theta_half**2)\n        \n        q = np.sqrt(q_squared)\n        \n        results.append(q)\n\n    # Format the output as a comma-separated list of floating-point numbers\n    # enclosed in square brackets.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3186018"}]}