{"hands_on_practices": [{"introduction": "多元回归模型中的系数并不仅仅是预测变量与响应变量之间的简单斜率。它实际上代表了一种*条件*效应。通过这个练习，我们将利用分块矩阵从第一性原理出发，推导出著名的Frisch-Waugh-Lovell定理，揭示每个回归系数是如何量化一个预测变量在“剔除”其他变量影响后的独特贡献的。[@problem_id:3146050]", "problem": "考虑矩阵形式的多元线性回归，其中响应向量为 $y \\in \\mathbb{R}^{n}$，设计矩阵为 $X \\in \\mathbb{R}^{n \\times p}$，分区为 $X = [X_{1}\\;X_{2}]$，其中 $X_{1} \\in \\mathbb{R}^{n \\times 1}$ 是一个我们感兴趣的预测变量，$X_{2} \\in \\mathbb{R}^{n \\times q}$ 包含一个截距项和其他预测变量。普通最小二乘估计定义为在 $\\beta \\in \\mathbb{R}^{p}$ 上残差平方范数 $\\|y - X\\beta\\|_{2}^{2}$ 的最小化子。设到 $X_{2}$ 列空间上的投影矩阵为 $P_{X_{2}} = X_{2}(X_{2}^{\\top}X_{2})^{-1}X_{2}^{\\top}$，相关的残差生成矩阵为 $M_{X_{2}} = I - P_{X_{2}}$。从这些定义和最小二乘准则所蕴含的正规方程出发，推导一个与 $X_{1}$ 相关的普通最小二乘系数的显式表达式，该表达式仅依赖于 $X_{1}$、$y$ 和 $M_{X_{2}}$。\n\n为了在协变量结构重叠的情况下对推导进行压力测试，考虑 $n = 5$ 个观测值，其中\n$x_{1} = \\begin{pmatrix}1 \\\\ 2 \\\\ 3 \\\\ 4 \\\\ 5\\end{pmatrix}$，$z = \\begin{pmatrix}1 \\\\ 3 \\\\ 3 \\\\ 5 \\\\ 5\\end{pmatrix}$，$1_{n} = \\begin{pmatrix}1 \\\\ 1 \\\\ 1 \\\\ 1 \\\\ 1\\end{pmatrix}$，$X_{2} = [1_{n}\\;z]$，以及 $y = \\begin{pmatrix}8 \\\\ 8 \\\\ 10 \\\\ 10 \\\\ 12\\end{pmatrix}$。\n使用您推导的表达式计算 $x_{1}$ 上的系数的数值。无需四舍五入。最后，根据条件效应解释，当 $x_{1}$ 与 $X_{2}$ 的列有显著重叠时，这个系数代表什么。", "solution": "首先通过提取给定信息并评估其科学合理性、完整性和客观性来验证问题。\n\n**步骤 1：提取已知条件**\n- 响应向量：$y \\in \\mathbb{R}^{n}$\n- 设计矩阵：$X \\in \\mathbb{R}^{n \\times p}$，分区为 $X = [X_{1}\\;X_{2}]$\n- 分区：$X_{1} \\in \\mathbb{R}^{n \\times 1}$ (感兴趣的预测变量)，$X_{2} \\in \\mathbb{R}^{n \\times q}$ (截距项和其他预测变量)。预测变量总数为 $p=1+q$。\n- OLS 准则：在 $\\beta \\in \\mathbb{R}^{p}$ 上最小化 $\\|y - X\\beta\\|_{2}^{2}$。\n- $X_2$ 的投影矩阵：$P_{X_{2}} = X_{2}(X_{2}^{\\top}X_{2})^{-1}X_{2}^{\\top}$\n- $X_2$ 的残差生成矩阵：$M_{X_{2}} = I - P_{X_{2}}$\n- 任务 1：推导 $X_1$ 的 OLS 系数 $\\hat{\\beta}_1$ 的显式表达式，用 $X_1$、$y$ 和 $M_{X_2}$ 表示。\n- 任务 2：对于 $n=5$，根据给定数据计算 $\\hat{\\beta}_1$：\n  - $X_{1} = x_{1} = \\begin{pmatrix}1 \\\\ 2 \\\\ 3 \\\\ 4 \\\\ 5\\end{pmatrix}$\n  - $z = \\begin{pmatrix}1 \\\\ 3 \\\\ 3 \\\\ 5 \\\\ 5\\end{pmatrix}$\n  - $1_{n} = \\begin{pmatrix}1 \\\\ 1 \\\\ 1 \\\\ 1 \\\\ 1\\end{pmatrix}$\n  - $X_{2} = [1_{n}\\;z]$\n  - $y = \\begin{pmatrix}8 \\\\ 8 \\\\ 10 \\\\ 10 \\\\ 12\\end{pmatrix}$\n- 任务 3：在条件效应和协变量重叠的背景下解释该系数的含义。\n\n**步骤 2：使用提取的已知条件进行验证**\n该问题定义明确且具有科学依据。它展示了线性模型中的一个标准理论推导，即 Frisch-Waugh-Lovell 定理，随后是一个直接的数值应用。所提供的数据和定义是自洽且一致的。数值示例中的矩阵被指定，以确保必要的矩阵逆 $(X_{2}^{\\top}X_{2})^{-1}$ 和 $(X^{\\top}X)^{-1}$ 存在，从而保证这是一个适定问题。语言客观、正式。未发现任何缺陷。\n\n**步骤 3：结论与行动**\n问题有效。将提供完整解答。\n\n**$\\hat{\\beta}_1$ 表达式的推导**\n\n多元线性回归模型由 $y = X\\beta + \\epsilon$ 给出。普通最小二乘 (OLS) 估计 $\\hat{\\beta}$ 是使残差平方和 $S(\\beta) = \\|y - X\\beta\\|_{2}^{2}$ 最小化的向量。该最小化问题的解满足正规方程：\n$$\nX^{\\top}X\\hat{\\beta} = X^{\\top}y\n$$\n我们根据问题陈述对设计矩阵 $X$ 和系数向量 $\\beta$ 进行分区：\n$$\nX = [X_{1}\\;X_{2}] \\quad \\text{和} \\quad \\beta = \\begin{pmatrix} \\beta_1 \\\\ \\beta_2 \\end{pmatrix}\n$$\n其中 $\\beta_1$ 是对应于单个预测变量 $X_1$ 的标量，$\\beta_2 \\in \\mathbb{R}^q$ 对应于 $X_2$ 中的预测变量。OLS 估计值表示为 $\\hat{\\beta}_1$ 和 $\\hat{\\beta}_2$。\n\n将分块形式代入正规方程，得到一个分块矩阵系统：\n$$\n\\begin{pmatrix} X_1^{\\top} \\\\ X_2^{\\top} \\end{pmatrix} [X_1\\;X_2] \\begin{pmatrix} \\hat{\\beta}_1 \\\\ \\hat{\\beta}_2 \\end{pmatrix} = \\begin{pmatrix} X_1^{\\top}y \\\\ X_2^{\\top}y \\end{pmatrix}\n$$\n$$\n\\begin{pmatrix} X_1^{\\top}X_1  X_1^{\\top}X_2 \\\\ X_2^{\\top}X_1  X_2^{\\top}X_2 \\end{pmatrix} \\begin{pmatrix} \\hat{\\beta}_1 \\\\ \\hat{\\beta}_2 \\end{pmatrix} = \\begin{pmatrix} X_1^{\\top}y \\\\ X_2^{\\top}y \\end{pmatrix}\n$$\n这个分块系统展开为两个方程：\n1. $X_1^{\\top}X_1 \\hat{\\beta}_1 + X_1^{\\top}X_2 \\hat{\\beta}_2 = X_1^{\\top}y$\n2. $X_2^{\\top}X_1 \\hat{\\beta}_1 + X_2^{\\top}X_2 \\hat{\\beta}_2 = X_2^{\\top}y$\n\n我们的目标是找到 $\\hat{\\beta}_1$ 的表达式。我们可以通过求解第二个方程得到 $\\hat{\\beta}_2$，然后将其代入第一个方程来实现。从方程 (2) 出发，假设 $X_2$ 具有满列秩，因此 $(X_2^{\\top}X_2)^{-1}$ 存在：\n$$\nX_2^{\\top}X_2 \\hat{\\beta}_2 = X_2^{\\top}y - X_2^{\\top}X_1 \\hat{\\beta}_1\n$$\n$$\n\\hat{\\beta}_2 = (X_2^{\\top}X_2)^{-1}(X_2^{\\top}y - X_2^{\\top}X_1 \\hat{\\beta}_1)\n$$\n现在，将这个 $\\hat{\\beta}_2$ 的表达式代入方程 (1)：\n$$\nX_1^{\\top}X_1 \\hat{\\beta}_1 + X_1^{\\top}X_2 (X_2^{\\top}X_2)^{-1}(X_2^{\\top}y - X_2^{\\top}X_1 \\hat{\\beta}_1) = X_1^{\\top}y\n$$\n展开各项：\n$$\nX_1^{\\top}X_1 \\hat{\\beta}_1 + X_1^{\\top}X_2(X_2^{\\top}X_2)^{-1}X_2^{\\top}y - X_1^{\\top}X_2(X_2^{\\top}X_2)^{-1}X_2^{\\top}X_1 \\hat{\\beta}_1 = X_1^{\\top}y\n$$\n使用投影矩阵的定义 $P_{X_2} = X_2(X_2^{\\top}X_2)^{-1}X_2^{\\top}$，我们可以简化方程：\n$$\nX_1^{\\top}X_1 \\hat{\\beta}_1 + X_1^{\\top}P_{X_2}y - X_1^{\\top}P_{X_2}X_1 \\hat{\\beta}_1 = X_1^{\\top}y\n$$\n将包含 $\\hat{\\beta}_1$ 的项组合在左侧，其他项放在右侧：\n$$\n(X_1^{\\top}X_1 - X_1^{\\top}P_{X_2}X_1) \\hat{\\beta}_1 = X_1^{\\top}y - X_1^{\\top}P_{X_2}y\n$$\n在两侧提出因子 $X_1^{\\top}$：\n$$\nX_1^{\\top}(I - P_{X_2})X_1 \\hat{\\beta}_1 = X_1^{\\top}(I - P_{X_2})y\n$$\n使用残差生成矩阵的定义 $M_{X_2} = I - P_{X_2}$，我们得到：\n$$\n(X_1^{\\top}M_{X_2}X_1) \\hat{\\beta}_1 = X_1^{\\top}M_{X_2}y\n$$\n由于 $X_1$ 是一个单列向量，$X_1^{\\top}M_{X_2}X_1$ 是一个标量。只要 $X_1$ 不在 $X_2$ 的列空间中（即 $M_{X_2}X_1 \\neq 0$），这个标量就非零，我们就可以解出 $\\hat{\\beta}_1$：\n$$\n\\hat{\\beta}_1 = (X_1^{\\top}M_{X_2}X_1)^{-1} (X_1^{\\top}M_{X_2}y)\n$$\n这就是所求的表达式。\n\n**数值计算**\n\n给定：\n$X_{1} = \\begin{pmatrix}1 \\\\ 2 \\\\ 3 \\\\ 4 \\\\ 5\\end{pmatrix}$，$X_{2} = \\begin{pmatrix} 1  1 \\\\ 1  3 \\\\ 1  3 \\\\ 1  5 \\\\ 1  5 \\end{pmatrix}$，$y = \\begin{pmatrix}8 \\\\ 8 \\\\ 10 \\\\ 10 \\\\ 12\\end{pmatrix}$。\n\n我们需要计算量 $\\tilde{x}_1 = M_{X_2}X_1$ 和 $\\tilde{y} = M_{X_2}y$。这些分别是将 $X_1$ 和 $y$ 对 $X_2$ 进行回归后得到的残差。\n首先，计算 $X_2^{\\top}X_2$ 及其逆矩阵：\n$$\nX_2^{\\top}X_2 = \\begin{pmatrix} 1  1  1  1  1 \\\\ 1  3  3  5  5 \\end{pmatrix} \\begin{pmatrix} 1  1 \\\\ 1  3 \\\\ 1  3 \\\\ 1  5 \\\\ 1  5 \\end{pmatrix} = \\begin{pmatrix} 5  17 \\\\ 17  69 \\end{pmatrix}\n$$\n行列式为 $\\det(X_2^{\\top}X_2) = (5)(69) - (17)(17) = 345 - 289 = 56$。\n逆矩阵为：\n$$\n(X_2^{\\top}X_2)^{-1} = \\frac{1}{56} \\begin{pmatrix} 69  -17 \\\\ -17  5 \\end{pmatrix}\n$$\n向量 $\\tilde{x}_1 = M_{X_2}X_1 = X_1 - P_{X_2}X_1 = X_1 - X_2(X_2^{\\top}X_2)^{-1}X_2^{\\top}X_1$。\n首先，计算 $X_2^{\\top}X_1$：\n$$\nX_2^{\\top}X_1 = \\begin{pmatrix} 1  1  1  1  1 \\\\ 1  3  3  5  5 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 2 \\\\ 3 \\\\ 4 \\\\ 5 \\end{pmatrix} = \\begin{pmatrix} 15 \\\\ 61 \\end{pmatrix}\n$$\n将 $X_1$ 对 $X_2$ 回归的 OLS 系数为 $\\hat{\\gamma}_{x_1} = (X_2^{\\top}X_2)^{-1}X_2^{\\top}X_1$：\n$$\n\\hat{\\gamma}_{x_1} = \\frac{1}{56} \\begin{pmatrix} 69  -17 \\\\ -17  5 \\end{pmatrix} \\begin{pmatrix} 15 \\\\ 61 \\end{pmatrix} = \\frac{1}{56} \\begin{pmatrix} 1035 - 1037 \\\\ -255 + 305 \\end{pmatrix} = \\frac{1}{56} \\begin{pmatrix} -2 \\\\ 50 \\end{pmatrix} = \\begin{pmatrix} -1/28 \\\\ 25/28 \\end{pmatrix}\n$$\n拟合值为 $\\hat{X}_1 = X_2\\hat{\\gamma}_{x_1}$，残差为 $\\tilde{x}_1 = X_1 - \\hat{X}_1$：\n$$\n\\tilde{x}_1 = \\begin{pmatrix} 1 \\\\ 2 \\\\ 3 \\\\ 4 \\\\ 5 \\end{pmatrix} - \\frac{1}{28}\\begin{pmatrix} 1  1 \\\\ 1  3 \\\\ 1  3 \\\\ 1  5 \\\\ 1  5 \\end{pmatrix} \\begin{pmatrix} -1 \\\\ 25 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 2 \\\\ 3 \\\\ 4 \\\\ 5 \\end{pmatrix} - \\frac{1}{28}\\begin{pmatrix} 24 \\\\ 74 \\\\ 74 \\\\ 124 \\\\ 124 \\end{pmatrix} = \\frac{1}{28}\\begin{pmatrix} 28-24 \\\\ 56-74 \\\\ 84-74 \\\\ 112-124 \\\\ 140-124 \\end{pmatrix} = \\frac{1}{28}\\begin{pmatrix} 4 \\\\ -18 \\\\ 10 \\\\ -12 \\\\ 16 \\end{pmatrix}\n$$\n接下来，我们计算 $y$ 的残差，$\\tilde{y} = M_{X_2}y = y - X_2(X_2^{\\top}X_2)^{-1}X_2^{\\top}y$。首先，$X_2^{\\top}y$：\n$$\nX_2^{\\top}y = \\begin{pmatrix} 1  1  1  1  1 \\\\ 1  3  3  5  5 \\end{pmatrix} \\begin{pmatrix} 8 \\\\ 8 \\\\ 10 \\\\ 10 \\\\ 12 \\end{pmatrix} = \\begin{pmatrix} 48 \\\\ 172 \\end{pmatrix}\n$$\n将 $y$ 对 $X_2$ 回归的 OLS 系数为 $\\hat{\\gamma}_{y} = (X_2^{\\top}X_2)^{-1}X_2^{\\top}y$：\n$$\n\\hat{\\gamma}_{y} = \\frac{1}{56} \\begin{pmatrix} 69  -17 \\\\ -17  5 \\end{pmatrix} \\begin{pmatrix} 48 \\\\ 172 \\end{pmatrix} = \\frac{1}{56} \\begin{pmatrix} 3312 - 2924 \\\\ -816 + 860 \\end{pmatrix} = \\frac{1}{56} \\begin{pmatrix} 388 \\\\ 44 \\end{pmatrix} = \\begin{pmatrix} 97/14 \\\\ 11/14 \\end{pmatrix}\n$$\n残差为 $\\tilde{y} = y - X_2\\hat{\\gamma}_y$：\n$$\n\\tilde{y} = \\begin{pmatrix} 8 \\\\ 8 \\\\ 10 \\\\ 10 \\\\ 12 \\end{pmatrix} - \\frac{1}{14}\\begin{pmatrix} 1  1 \\\\ 1  3 \\\\ 1  3 \\\\ 1  5 \\\\ 1  5 \\end{pmatrix} \\begin{pmatrix} 97 \\\\ 11 \\end{pmatrix} = \\begin{pmatrix} 8 \\\\ 8 \\\\ 10 \\\\ 10 \\\\ 12 \\end{pmatrix} - \\frac{1}{14}\\begin{pmatrix} 108 \\\\ 130 \\\\ 130 \\\\ 152 \\\\ 152 \\end{pmatrix} = \\frac{1}{14}\\begin{pmatrix} 112-108 \\\\ 112-130 \\\\ 140-130 \\\\ 140-152 \\\\ 168-152 \\end{pmatrix} = \\frac{1}{14}\\begin{pmatrix} 4 \\\\ -18 \\\\ 10 \\\\ -12 \\\\ 16 \\end{pmatrix}\n$$\n现在我们计算 $\\hat{\\beta}_1$：\n$$\n\\hat{\\beta}_1 = \\frac{\\tilde{x}_1^{\\top}\\tilde{y}}{\\tilde{x}_1^{\\top}\\tilde{x}_1}\n$$\n我们观察到 $\\tilde{y} = \\frac{1}{14}\\begin{pmatrix} 4 \\\\ -18 \\\\ 10 \\\\ -12 \\\\ 16 \\end{pmatrix}$ 和 $\\tilde{x}_1 = \\frac{1}{28}\\begin{pmatrix} 4 \\\\ -18 \\\\ 10 \\\\ -12 \\\\ 16 \\end{pmatrix}$。\n因此，我们有一个直接关系：$\\tilde{y} = 2 \\tilde{x}_1$。\n将此代入 $\\hat{\\beta}_1$ 的公式中：\n$$\n\\hat{\\beta}_1 = \\frac{\\tilde{x}_1^{\\top}(2\\tilde{x}_1)}{\\tilde{x}_1^{\\top}\\tilde{x}_1} = \\frac{2(\\tilde{x}_1^{\\top}\\tilde{x}_1)}{\\tilde{x}_1^{\\top}\\tilde{x}_1} = 2\n$$\n只要 $\\tilde{x}_1$ 不是零向量，这个结果就有效，本例中情况确实如此。该系数的数值为 $2$。\n\n**系数的解释**\n\n系数 $\\hat{\\beta}_1$ 代表预测变量 $X_1$ 对响应 $y$ 的估计条件效应。如推导所示，$\\hat{\\beta}_1$ 是将 $y$ 的残差（在对 $X_2$ 回归后）对 $X_1$ 的残差（在对 $X_2$ 回归后）进行简单回归得到的系数。这意味着 $\\hat{\\beta}_1$ 量化了 $X_1$ 中与 $X_2$ 中预测变量线性无关的部分与 $y$ 中与 $X_2$ 中预测变量线性无关的部分之间的关联。在实践中，它是在保持 $X_2$ 中所有其他预测变量的值不变的情况下，预测变量 $X_1$ 的值增加一个单位时，$y$ 期望值的估计变化量。\n\n当 $X_1$ 与 $X_2$ 的列“显著重叠”时，这意味着存在高度的多重共线性。在这种情况下，$X_1$ 中的大部分变异已经被 $X_2$ 中的预测变量所解释。因此，残差向量 $\\tilde{x}_1 = M_{X_2}X_1$ 的分量将具有很小的量级，其平方范数 $\\|M_{X_2}X_1\\|_2^2 = X_1^{\\top}M_{X_2}X_1$（即 $\\hat{\\beta}_1$ 最终表达式中的分母）将接近于零。\n\n估计量 $\\hat{\\beta}_1$ 的方差由 $\\text{Var}(\\hat{\\beta}_1) = \\sigma^2 (X_1^{\\top}M_{X_2}X_1)^{-1}$ 给出，其中 $\\sigma^2$ 是误差方差。如果分母 $X_1^{\\top}M_{X_2}X_1$ 很小，$\\hat{\\beta}_1$ 的方差就会变得非常大。这意味着该估计是不稳定和不可靠的。大方差意味着大标准误和宽置信区间，表明关于 $\\beta_1$ 真实值存在很大的不确定性。虽然该系数在形式上仍然代表 $X_1$ 的条件效应，但与 $X_2$ 的显著重叠使得模型难以厘清 $X_1$ 对 $y$ 变异的独特贡献。数据根本不包含足够的信息来精确估计这种效应。", "answer": "$$\n\\boxed{2}\n$$", "id": "3146050"}, {"introduction": "标准化预测变量（即中心化和缩放）是许多高级建模技术（如正则化回归）的关键预处理步骤。这个过程不仅仅是数值上的调整，它从根本上改变了问题的几何结构。在本练习中，你将通过代数推导，亲眼见证设计矩阵如何从一个基于协方差的空间转换到一个基于相关性的空间，并揭示原始回归系数与标准化系数之间的精确换算关系。[@problem_id:3146102]", "problem": "给定一个列满秩的设计矩阵 $X \\in \\mathbb{R}^{n \\times p}$，其列是中心化的，使得在平均算子 $\\frac{1}{n} \\sum_{i=1}^{n}$ 下，每个预测变量的样本均值为 $0$。响应向量 $y \\in \\mathbb{R}^{n}$ 也是中心化的。考虑普通最小二乘法（OLS, Ordinary Least Squares）问题：找到一个 $\\hat{\\beta} \\in \\mathbb{R}^{p}$ 来最小化 $\\|y - X \\beta\\|_{2}^{2}$，这会得到正规方程 $X^{\\top} X \\hat{\\beta} = X^{\\top} y$ 和帽子矩阵 $H = X (X^{\\top} X)^{-1} X^{\\top}$。\n\n定义对角缩放矩阵 $S = \\mathrm{diag}(s_{1}, \\dots, s_{p})$，其中 $s_{j}^{2} = \\frac{1}{n} x_{j}^{\\top} x_{j}$，$x_{j}$ 表示 $X$ 的第 $j$ 列。令标准化预测变量矩阵为 $Z = X S^{-1}$，使得在平均算子 $\\frac{1}{n} \\sum_{i=1}^{n}$ 下，$Z$ 的每一列的方差都为 $1$。令响应标准差为 $s_{y}^{2} = \\frac{1}{n} y^{\\top} y$，并定义标准化响应为 $z_{y} = y / s_{y}$。\n\n从 OLS 正规方程出发，仅使用基本线性代数，完成以下任务：\n\n- 根据预测变量间的两两相关性，推导出矩阵 $\\frac{1}{n} Z^{\\top} Z$，并解释为什么它是一个基于相关性的格拉姆矩阵。\n- 将在标准化预测变量尺度下的系数向量 $\\hat{\\gamma}$（由 $X \\hat{\\beta} = Z \\hat{\\gamma}$ 定义）用 $Z^{\\top} Z$ 和 $Z^{\\top} y$ 表示，然后将 $\\hat{\\beta}$ 用 $\\hat{\\gamma}$ 和 $S$ 表示。清楚地说明在 $X$ 的列进行缩放时，$\\hat{\\beta}$ 中条目的单位如何变化。\n- 判断在变换 $X \\mapsto Z$ 下帽子矩阵是否改变，并通过推导标准化预测变量尺度下的帽子矩阵，用代数方法证明你的结论。\n\n令 $R \\in \\mathbb{R}^{p \\times p}$ 表示预测变量相关矩阵，其条目为 $R_{jk} = \\frac{\\frac{1}{n} x_{j}^{\\top} x_{k}}{s_{j} s_{k}}$；令 $r \\in \\mathbb{R}^{p}$ 为预测变量-响应相关性向量，其条目为 $r_{j} = \\frac{\\frac{1}{n} x_{j}^{\\top} y}{s_{j} s_{y}}$。完成推导后，请给出将标准化响应 $z_{y}$ 对标准化预测变量 $Z$ 进行 OLS 回归所得到的系数向量的闭式表达式，该表达式仅用 $R$ 和 $r$ 表示。\n\n你的最终答案必须是单个解析表达式。无需四舍五入。", "solution": "该问题陈述清晰，其科学基础是多元线性回归的原理，并包含足够的信息以获得唯一解。所给的定义和假设在统计学习中是标准的。我将按要求进行推导。\n\n该问题要求基于普通最小二乘（OLS）回归的矩阵形式进行若干推导。我们给定一个中心化的列满秩设计矩阵 $X \\in \\mathbb{R}^{n \\times p}$ 和一个中心化的响应向量 $y \\in \\mathbb{R}^{n}$。\n\n**第1部分：矩阵 $\\frac{1}{n} Z^{\\top} Z$**\n\n首先，我们推导矩阵 $\\frac{1}{n} Z^{\\top} Z$ 的表达式。标准化预测变量矩阵 $Z$ 定义为 $Z = X S^{-1}$，其中 $S$ 是预测变量标准差的对角矩阵，$S = \\mathrm{diag}(s_1, \\dots, s_p)$ 且 $s_j^2 = \\frac{1}{n} x_j^{\\top} x_j$。由于 $S$ 是对角矩阵，其转置等于自身，$(S^{-1})^{\\top} = S^{-1}$。\n\n我们计算 $Z^{\\top} Z$：\n$$\nZ^{\\top} Z = (X S^{-1})^{\\top} (X S^{-1}) = (S^{-1})^{\\top} X^{\\top} X S^{-1} = S^{-1} X^{\\top} X S^{-1}\n$$\n现在我们将此矩阵乘以 $\\frac{1}{n}$：\n$$\n\\frac{1}{n} Z^{\\top} Z = \\frac{1}{n} S^{-1} X^{\\top} X S^{-1}\n$$\n让我们来考察该矩阵的第 $(j,k)$ 个条目。矩阵 $X^{\\top} X$ 的条目为 $(X^{\\top} X)_{jk} = x_j^{\\top} x_k$。矩阵 $S^{-1}$ 是对角矩阵，其条目为 $(S^{-1})_{jj} = 1/s_j$。\n矩阵 $\\frac{1}{n} S^{-1} X^{\\top} X S^{-1}$ 的第 $(j,k)$ 个条目是：\n$$\n\\left(\\frac{1}{n} S^{-1} X^{\\top} X S^{-1}\\right)_{jk} = \\frac{1}{n} (S^{-1})_{jj} (X^{\\top} X)_{jk} (S^{-1})_{kk} = \\frac{1}{n} \\left(\\frac{1}{s_j}\\right) (x_j^{\\top} x_k) \\left(\\frac{1}{s_k}\\right) = \\frac{\\frac{1}{n} x_j^{\\top} x_k}{s_j s_k}\n$$\n这个表达式恰好是预测变量相关矩阵 $R_{jk}$ 的第 $(j,k)$ 个条目的定义。由于 $X$ 的列是中心化的，$\\frac{1}{n} x_j^{\\top} x_k$ 是预测变量 $x_j$ 和 $x_k$ 之间的样本协方差。此外，$s_j = \\sqrt{\\frac{1}{n} x_j^{\\top} x_j}$ 和 $s_k = \\sqrt{\\frac{1}{n} x_k^{\\top} x_k}$ 分别是它们的样本标准差。因此，$R_{jk}$ 是 $x_j$ 和 $x_k$ 之间的样本相关系数。\n\n因此，矩阵 $\\frac{1}{n} Z^{\\top} Z$ 就是预测变量相关矩阵 $R$：\n$$\n\\frac{1}{n} Z^{\\top} Z = R\n$$\n这个矩阵是一个格拉姆矩阵，因为它的条目是内积。具体来说，如果我们在预测变量空间上定义一个内积为 $\\langle u, v \\rangle = \\frac{1}{n} u^{\\top} v$，那么该矩阵的第 $(j,k)$ 个条目就是 $\\langle z_j, z_k \\rangle$，其中 $z_j$ 是 $Z$ 的第 $j$ 列。由于向量 $z_j$ 被标准化为单位方差（即 $\\langle z_j, z_j \\rangle = 1$），这个格拉姆矩阵就是一个相关矩阵。\n\n**第2部分：系数 $\\hat{\\gamma}$ 和 $\\hat{\\beta}$**\n\n题目要求我们表达系数向量 $\\hat{\\gamma}$（通过将 $y$ 对 $Z$ 进行回归得到），并将其与原始系数 $\\hat{\\beta}$ 关联起来。对于标准化预测变量，OLS 问题是找到最小化 $\\|y - Z\\gamma\\|_2^2$ 的 $\\hat{\\gamma}$。相应的正规方程为：\n$$\n(Z^{\\top} Z) \\hat{\\gamma} = Z^{\\top} y\n$$\n由于 $X$ 是列满秩的，且 $S$ 是可逆的，所以 $Z=XS^{-1}$ 也是列满秩的，这保证了 $Z^{\\top} Z$ 是可逆的。求解 $\\hat{\\gamma}$ 可得：\n$$\n\\hat{\\gamma} = (Z^{\\top} Z)^{-1} Z^{\\top} y\n$$\n拟合值之间的关系由 $X \\hat{\\beta} = Z \\hat{\\gamma}$ 给出。代入 $Z = X S^{-1}$：\n$$\nX \\hat{\\beta} = (X S^{-1}) \\hat{\\gamma}\n$$\n由于 $X$ 是列满秩的，矩阵 $X^{\\top}X$ 是可逆的。我们可以用 $X^{\\top}$ 左乘得到 $X^{\\top}X\\hat{\\beta} = X^{\\top}X S^{-1} \\hat{\\gamma}$，然后再乘以 $(X^{\\top}X)^{-1}$ 以分离出 $\\hat{\\beta}$：\n$$\n\\hat{\\beta} = (X^{\\top}X)^{-1}(X^{\\top}X) S^{-1} \\hat{\\gamma} = S^{-1} \\hat{\\gamma}\n$$\n这意味着对于每个分量 $j$，$\\hat{\\beta}_j = \\frac{1}{s_j} \\hat{\\gamma}_j$。\n\n关于 $\\hat{\\beta}_j$ 的单位，令 $[v]$ 表示量 $v$ 的单位。模型方程为 $y \\approx \\sum_j X_{ij} \\beta_j$，这意味着对于求和中的每一项，都有 $[y] = [X_{ij}] [\\beta_j]$。因此，系数 $\\beta_j$ 的单位是 $[\\beta_j] = \\frac{[y]}{[X_{ij}]}$。如果预测变量列 $x_j$ 的单位通过一个缩放因子 $c$ 改变（例如，从米到厘米，$c=100$），则新的预测变量为 $x_j' = c x_j$。新的系数 $\\beta_j'$ 必须满足 $x_j' \\beta_j' = x_j \\beta_j$，所以 $(c x_j) \\beta_j' = x_j \\beta_j$，这意味着 $\\beta_j' = \\frac{1}{c} \\beta_j$。因此，系数 $\\hat{\\beta}_j$ 的缩放与其对应的预测变量列的缩放成反比。\n\n**第3部分：帽子矩阵的不变性**\n\n原始模型的帽子矩阵是 $H = X(X^{\\top}X)^{-1}X^{\\top}$。我们想要确定当我们使用标准化预测变量 $Z$ 时，这个矩阵是否会改变。使用预测变量 $Z$ 的模型的帽子矩阵将是 $H_Z = Z(Z^{\\top}Z)^{-1}Z^{\\top}$。我们将 $Z=XS^{-1}$ 代入此表达式并化简：\n$$\nH_Z = (X S^{-1}) ((X S^{-1})^{\\top} (X S^{-1}))^{-1} (X S^{-1})^{\\top}\n$$\n首先，我们化简中间的逆矩阵项：\n$$\n((X S^{-1})^{\\top} (X S^{-1}))^{-1} = (S^{-1} X^{\\top} X S^{-1})^{-1}\n$$\n利用属性 $(ABC)^{-1} = C^{-1}B^{-1}A^{-1}$ 并注意到 $S$ 是可逆的，我们得到：\n$$\n(S^{-1})^{-1} (X^{\\top}X)^{-1} (S^{-1})^{-1} = S (X^{\\top}X)^{-1} S\n$$\n现在，将此结果代回到 $H_Z$ 的表达式中：\n$$\nH_Z = (X S^{-1}) [S (X^{\\top}X)^{-1} S] (S^{-1})^{\\top} X^{\\top}\n$$\n由于 $S$ 是对角矩阵，$(S^{-1})^{\\top} = S^{-1}$。我们现在可以对各项进行分组：\n$$\nH_Z = X (S^{-1} S) (X^{\\top}X)^{-1} (S S^{-1}) X^{\\top}\n$$\n由于 $S^{-1}S = I$ 和 $SS^{-1} = I$，其中 $I$ 是单位矩阵，表达式化简为：\n$$\nH_Z = X I (X^{\\top}X)^{-1} I X^{\\top} = X(X^{\\top}X)^{-1}X^{\\top} = H\n$$\n因此，帽子矩阵对于设计矩阵各列的可逆线性变换（例如缩放）是不变的。\n\n**第4部分：标准化系数的最终表达式**\n\n最后，题目要求我们找出将标准化响应 $z_y = y/s_y$ 对标准化预测变量 $Z$ 进行回归的系数向量。我们用 $\\hat{\\gamma}_{std}$ 表示这个系数向量。OLS 问题是最小化 $\\|z_y - Z\\gamma_{std}\\|_2^2$，其正规方程为：\n$$\n(Z^{\\top} Z) \\hat{\\gamma}_{std} = Z^{\\top} z_y\n$$\n求解 $\\hat{\\gamma}_{std}$ 可得：\n$$\n\\hat{\\gamma}_{std} = (Z^{\\top} Z)^{-1} (Z^{\\top} z_y)\n$$\n我们需要用相关矩阵 $R$ 和 $r$ 来表示这个表达式。如第1部分所推导，我们有 $Z^{\\top}Z = nR$。\n现在，我们来分析 $Z^{\\top} z_y$ 这一项：\n$$\nZ^{\\top} z_y = Z^{\\top} \\frac{y}{s_y} = \\frac{1}{s_y} Z^{\\top} y\n$$\n让我们考察向量 $Z^{\\top} y$ 的第 $j$ 个分量。它是 $z_j^{\\top} y$。由于 $z_j = x_j / s_j$，我们有：\n$$\n(Z^{\\top} y)_j = z_j^{\\top} y = \\left(\\frac{x_j}{s_j}\\right)^{\\top} y = \\frac{1}{s_j} x_j^{\\top} y\n$$\n预测变量-响应相关性向量 $r$ 的第 $j$ 个分量由 $r_j = \\frac{\\frac{1}{n} x_j^{\\top} y}{s_j s_y}$ 给出。重新整理可得 $x_j^{\\top} y = n s_j s_y r_j$。\n将此代入 $(Z^{\\top} y)_j$ 的表达式中：\n$$\n(Z^{\\top} y)_j = \\frac{1}{s_j} (n s_j s_y r_j) = n s_y r_j\n$$\n由于这对每个分量 $j$ 都成立，向量 $Z^{\\top} y$ 等于 $n s_y r$。\n现在我们可以计算 $Z^{\\top} z_y$：\n$$\nZ^{\\top} z_y = \\frac{1}{s_y} (Z^{\\top} y) = \\frac{1}{s_y} (n s_y r) = n r\n$$\n最后，我们将 $Z^{\\top}Z$ 和 $Z^{\\top}z_y$ 的表达式代回到 $\\hat{\\gamma}_{std}$ 的解中：\n$$\n\\hat{\\gamma}_{std} = (n R)^{-1} (n r) = \\frac{1}{n} R^{-1} (n r) = R^{-1} r\n$$\n这就是将标准化响应对标准化预测变量进行回归时的系数向量的闭式表达式，它完全由预测变量相关矩阵 $R$ 和预测变量-响应相关性向量 $r$ 表示。", "answer": "$$\\boxed{R^{-1} r}$$", "id": "3146102"}, {"introduction": "当预测变量高度相关（即存在多重共线性）时，普通最小二乘（OLS）估计会因数值不稳定性而变得不可靠。岭回归通过对系数的大小施加惩罚来解决这一难题，但其工作原理常被视为一个“黑箱”。本练习将通过奇异值分解（SVD）这一强大的工具，为你揭开这个黑箱，让你从几何上理解岭回归是如何通过“收缩”系数来稳定模型的，并精确地看到它是如何选择性地抑制由共线性引起的不稳定数据维度的影响。[@problem_id:3146059]", "problem": "考虑多元线性回归的矩阵表示，并重点讨论奇异值分解（SVD）在解释正则化中的作用。在纯数学术语下进行推导，不涉及物理单位。从最小二乘估计量（作为平方误差的最小化器）和岭估计量（作为带有平方参数惩罚项的平方误差的最小化器）的定义出发。您不能先验地假设任何闭式解；相反，在需要时，应从这些定义和线性代数知识中推导出它们。目标是表达岭回归下的预测值，并通过SVD解释惩罚项如何收缩沿微小奇异方向的贡献，然后设计一个具体的数值矩阵和响应向量，在该情况下，微小奇异方向在普通最小二乘法中占主导地位，并观察岭回归如何稳定它们。\n\n任务：\n1) 按如下方式构造设计矩阵 $X \\in \\mathbb{R}^{6 \\times 3}$，其中每个条目都是实标量：\n$$\nX =\n\\begin{bmatrix}\n1  1 + 10^{-4}\\cdot 1  1 + 2\\cdot 10^{-4}\\cdot (-2) \\\\\n2  2 + 10^{-4}\\cdot (-2)  2 + 2\\cdot 10^{-4}\\cdot 1 \\\\\n3  3 + 10^{-4}\\cdot 3  3 + 2\\cdot 10^{-4}\\cdot (-3) \\\\\n4  4 + 10^{-4}\\cdot (-4)  4 + 2\\cdot 10^{-4}\\cdot 4 \\\\\n5  5 + 10^{-4}\\cdot 5  5 + 2\\cdot 10^{-4}\\cdot (-5) \\\\\n6  6 + 10^{-4}\\cdot (-6)  6 + 2\\cdot 10^{-4}\\cdot 6\n\\end{bmatrix}.\n$$\n该矩阵是满列秩的，但由于其列向量近似共线，因此是高度病态的。\n\n2) 计算奇异值分解 $X = U\\Sigma V^\\top$，其中 $U \\in \\mathbb{R}^{6 \\times 3}$ 具有标准正交列，$\\Sigma \\in \\mathbb{R}^{3 \\times 3}$ 是对角矩阵，其非负对角元按非增顺序排列，且 $V \\in \\mathbb{R}^{3 \\times 3}$ 是标准正交矩阵。定义响应向量 $y \\in \\mathbb{R}^{6}$ 为与最小奇异值相对应的左奇异向量，即 $U$ 的最后一列。\n\n3) 将普通最小二乘估计量 $\\hat{\\beta}_{\\text{OLS}} \\in \\mathbb{R}^{3}$ 定义为问题\n$$\n\\min_{\\beta \\in \\mathbb{R}^{3}} \\ \\frac{1}{2}\\|y - X\\beta\\|_2^2\n$$\n的任何最小化器。将岭估计量 $\\hat{\\beta}_{\\lambda} \\in \\mathbb{R}^{3}$（对于任何惩罚强度 $\\lambda \\ge 0$）定义为问题\n$$\n\\min_{\\beta \\in \\mathbb{R}^{3}} \\ \\frac{1}{2}\\|y - X\\beta\\|_2^2 + \\frac{\\lambda}{2}\\|\\beta\\|_2^2.\n$$\n的任何最小化器。计算拟合值 $\\hat{y}_{\\text{OLS}} = X\\hat{\\beta}_{\\text{OLS}}$ 和 $\\hat{y}_{\\lambda} = X\\hat{\\beta}_{\\lambda}$。\n\n4) 将岭回归解释为对奇异方向的滤波。使用 $X$ 的SVD，在左奇异向量基下表示预测值 $\\hat{y}_{\\lambda}$，并确定应用于每个奇异分量的收缩因子。然后，在步骤2中选择的 $y$ 的情况下，通过计算验证与最小奇异值相关的奇异方向是收缩最严重的方向。\n\n5) 测试套件。使用上面定义的单个矩阵 $X$ 和响应 $y$，并评估以下四个共同构成测试套件的案例：\n- 案例A（边界，零惩罚）：$\\lambda = 0$。验证岭回归预测是否与普通最小二乘预测一致（在 $10^{-8}$ 的数值容差内）。输出通过计算 $ \\|\\hat{y}_{\\lambda} - \\hat{y}_{\\text{OLS}}\\|_2 \\le 10^{-8}$ 得到的布尔值。\n- 案例B（小惩罚，温和收缩）：$\\lambda = 10^{-6}$。输出比率 $ \\|\\hat{\\beta}_{\\lambda}\\|_2 / \\|\\hat{\\beta}_{\\text{OLS}}\\|_2 $，作为一个浮点数。\n- 案例C（中等惩罚，奇异方向滤波检查）：$\\lambda = 10^{-4}$。令 $v_{\\min}$ 为与最小奇异值 $\\sigma_{\\min}$ 对应的右奇异向量。计算分量收缩比\n$$\nr_{\\text{emp}} = \\frac{|\\langle v_{\\min}, \\hat{\\beta}_{\\lambda} \\rangle|}{|\\langle v_{\\min}, \\hat{\\beta}_{\\text{OLS}} \\rangle|},\n$$\n和理论滤波因子\n$$\nr_{\\text{theory}} = \\frac{\\sigma_{\\min}^2}{\\sigma_{\\min}^2 + \\lambda}.\n$$\n输出绝对偏差 $|r_{\\text{emp}} - r_{\\text{theory}}|$，作为一个浮点数。\n- 案例D（大惩罚，强力收缩）：$\\lambda = 10^{2}$。输出欧几里得范数 $\\|\\hat{\\beta}_{\\lambda}\\|_2$，作为一个浮点数。\n\n6) 最终输出格式。您的程序应生成单行输出，其中包含从案例A到D的四个结果，按顺序排列，以逗号分隔，并用方括号括起来（例如，“[resultA,resultB,resultC,resultD]”）。这些结果必须分别是案例A的布爾值以及案例B到D的三个浮点数。", "solution": "该问题要求将岭回归作为一种收缩估计量进行分析，并通过设计矩阵 $X$ 的奇异值分解（SVD）来解释。我们必须首先从其优化定义推导出估计量，然后将它们应用于一个为突显正则化效应而专门构造的病态矩阵 $X$ 和响应向量 $y$。\n\n首先，我们从第一性原理出发，推导普通最小二乘（OLS）和岭回归估计量。\n\nOLS估计量 $\\hat{\\beta}_{\\text{OLS}}$ 定义为残差平方和的最小化器：\n$$\nL(\\beta) = \\frac{1}{2}\\|y - X\\beta\\|_2^2 = \\frac{1}{2}(y - X\\beta)^\\top(y - X\\beta)\n$$\n为求最小值，我们计算 $L(\\beta)$ 关于 $\\beta$ 的梯度并将其设为零。\n$$\n\\nabla_\\beta L(\\beta) = \\nabla_\\beta \\left( \\frac{1}{2}(y^\\top y - 2y^\\top X\\beta + \\beta^\\top X^\\top X\\beta) \\right) = -X^\\top y + X^\\top X\\beta\n$$\n将梯度设为零，得到正规方程组：\n$$\nX^\\top X \\hat{\\beta}_{\\text{OLS}} = X^\\top y\n$$\n由于问题指明 $X$ 是一个 $6 \\times 3$ 的满列秩矩阵，因此矩阵 $X^\\top X \\in \\mathbb{R}^{3 \\times 3}$ 是可逆的。因此，唯一的OLS解是：\n$$\n\\hat{\\beta}_{\\text{OLS}} = (X^\\top X)^{-1} X^\\top y\n$$\n\n岭估计量 $\\hat{\\beta}_{\\lambda}$ 定义为带惩罚项的残差平方和的最小化器：\n$$\nL_\\lambda(\\beta) = \\frac{1}{2}\\|y - X\\beta\\|_2^2 + \\frac{\\lambda}{2}\\|\\beta\\|_2^2\n$$\n其中 $\\lambda \\ge 0$ 是正则化参数。其梯度为：\n$$\n\\nabla_\\beta L_\\lambda(\\beta) = \\nabla_\\beta \\left( \\frac{1}{2}\\|y - X\\beta\\|_2^2 \\right) + \\nabla_\\beta \\left( \\frac{\\lambda}{2}\\beta^\\top\\beta \\right) = (-X^\\top y + X^\\top X\\beta) + \\lambda\\beta\n$$\n将梯度设为零可得：\n$$\n(X^\\top X + \\lambda I)\\hat{\\beta}_{\\lambda} = X^\\top y\n$$\n对于任何 $\\lambda > 0$，矩阵 $(X^\\top X + \\lambda I)$ 都是正定的，因此是可逆的。当 $\\lambda = 0$ 时，该问题简化为OLS情况。唯一的岭回归解是：\n$$\n\\hat{\\beta}_{\\lambda} = (X^\\top X + \\lambda I)^{-1} X^\\top y\n$$\n\n接下来，我们使用 $X$ 的奇异值分解（SVD）来解释这些估计量。设 $X = U\\Sigma V^\\top$，其中 $U \\in \\mathbb{R}^{6 \\times 3}$ 具有标准正交列，$\\Sigma = \\text{diag}(\\sigma_1, \\sigma_2, \\sigma_3)$ 是一个 $3 \\times 3$ 的对角矩阵，其奇异值为 $\\sigma_1 \\ge \\sigma_2 \\ge \\sigma_3 > 0$，且 $V \\in \\mathbb{R}^{3 \\times 3}$ 是一个标准正交矩阵。\n我们将SVD代入估计量的表达式中。首先，注意到 $X^\\top X = (U\\Sigma V^\\top)^\\top(U\\Sigma V^\\top) = V\\Sigma^\\top U^\\top U\\Sigma V^\\top = V\\Sigma^2 V^\\top$。\n\n对于OLS估计量：\n$$\n\\hat{\\beta}_{\\text{OLS}} = (V\\Sigma^2 V^\\top)^{-1} (U\\Sigma V^\\top)^\\top y = (V(\\Sigma^2)^{-1}V^\\top)(V\\Sigma U^\\top y) = V\\Sigma^{-1}U^\\top y\n$$\n如果我们将 $U$ 和 $V$ 的列分别表示为 $u_j$ 和 $v_j$，这可以写成一个和式：\n$$\n\\hat{\\beta}_{\\text{OLS}} = \\sum_{j=1}^{3} v_j \\frac{u_j^\\top y}{\\sigma_j}\n$$\nOLS解是右奇异向量 $v_j$ 的线性组合，其系数是 $y$ 在左奇异向量 $u_j$ 上的投影，并被相应奇异值的倒数 $1/\\sigma_j$放大。\n\n对于岭估计量：\n$$\nX^\\top X + \\lambda I = V\\Sigma^2 V^\\top + \\lambda V V^\\top = V(\\Sigma^2 + \\lambda I)V^\\top\n$$\n其逆为 $(X^\\top X + \\lambda I)^{-1} = V(\\Sigma^2 + \\lambda I)^{-1}V^\\top$。因此，\n$$\n\\hat{\\beta}_{\\lambda} = (V(\\Sigma^2 + \\lambda I)^{-1}V^\\top)(V\\Sigma U^\\top y) = V(\\Sigma^2 + \\lambda I)^{-1}\\Sigma U^\\top y\n$$\n用求和符号表示，即为：\n$$\n\\hat{\\beta}_{\\lambda} = \\sum_{j=1}^{3} v_j \\frac{\\sigma_j}{\\sigma_j^2 + \\lambda}(u_j^\\top y)\n$$\n比较 $\\hat{\\beta}_{\\text{OLS}}$ 和 $\\hat{\\beta}_{\\lambda}$ 展开式中 $v_j$ 的系数，我们看到一个乘法因子：\n$$\n\\frac{\\sigma_j/(\\sigma_j^2+\\lambda)}{1/\\sigma_j} = \\frac{\\sigma_j^2}{\\sigma_j^2 + \\lambda}\n$$\n这个因子总是在区间 $[0, 1]$ 内，被称为收缩因子。对于大的奇异值 $\\sigma_j \\gg \\sqrt{\\lambda}$，该因子接近1，收缩很小。对于小的奇异值 $\\sigma_j \\ll \\sqrt{\\lambda}$，该因子接近0，解的相应分量被强烈地向零收缩。\n\n预测值为 $\\hat{y} = X\\beta$。对于OLS，$\\hat{y}_{\\text{OLS}} = X\\hat{\\beta}_{\\text{OLS}} = (U\\Sigma V^\\top)(V\\Sigma^{-1}U^\\top y) = UU^\\top y$，这是 $y$ 在 $X$ 的列空间上的正交投影。对于岭回归，\n$$\n\\hat{y}_{\\lambda} = X\\hat{\\beta}_{\\lambda} = (U\\Sigma V^\\top)(V(\\Sigma^2 + \\lambda I)^{-1}\\Sigma U^\\top y) = U \\Sigma (\\Sigma^2 + \\lambda I)^{-1} \\Sigma U^\\top y = \\sum_{j=1}^{3} u_j \\left( \\frac{\\sigma_j^2}{\\sigma_j^2+\\lambda} \\right) (u_j^\\top y)\n$$\n这表明岭回归的预测值是通过将 $y$ 沿着左奇异向量 $u_j$ 的分量乘以相同的因子来形成的。\n\n问题定义 $y$ 为与最小奇异值相对应的左奇异向量，即 $y = u_3$（假设 $\\sigma_3 = \\sigma_{\\min}$）。根据 $U$ 列向量的标准正交性，我们有 $u_j^\\top y = u_j^\\top u_3 = \\delta_{j3}$。\n这将极大地简化估计量：\n$$\n\\hat{\\beta}_{\\text{OLS}} = v_3 \\frac{1}{\\sigma_3}\n$$\n$$\n\\hat{\\beta}_{\\lambda} = v_3 \\frac{\\sigma_3}{\\sigma_3^2 + \\lambda}\n$$\n当 $\\sigma_3 \\to 0$ 时，OLS解的模长会爆炸式增长，而岭回归解则保持有界。\n\n我们现在进行测试套件的具体计算。\n设 $X$ 为给定的 $6 \\times 3$ 矩阵。我们计算其SVD $X=U\\Sigma V^\\top$ 并定义 $y = u_3$，其中 $u_3$ 对应于 $\\sigma_3 = \\sigma_{\\min}$。设 $v_3 = v_{\\min}$。\n\n案例A ($\\lambda = 0$)：我们比较 $\\hat{y}_{0}$ 和 $\\hat{y}_{\\text{OLS}}$。根据岭回归预测公式，当 $\\lambda=0$ 时：\n$$\n\\hat{y}_{0} = \\sum_{j=1}^{3} u_j \\left( \\frac{\\sigma_j^2}{\\sigma_j^2+0} \\right) (u_j^\\top y) = \\sum_{j=1}^{3} u_j (u_j^\\top y) = UU^\\top y = \\hat{y}_{\\text{OLS}}\n$$\n预测值必须相同。在数值上，$\\|\\hat{y}_{0} - \\hat{y}_{\\text{OLS}}\\|_2$ 应该接近于0。\n\n案例B ($\\lambda = 10^{-6}$)：我们计算范数之比。\n$$\n\\frac{\\|\\hat{\\beta}_{\\lambda}\\|_2}{\\|\\hat{\\beta}_{\\text{OLS}}\\|_2} = \\frac{\\|v_3 \\frac{\\sigma_3}{\\sigma_3^2 + \\lambda}\\|_2}{\\|v_3 \\frac{1}{\\sigma_3}\\|_2} = \\frac{|\\frac{\\sigma_3}{\\sigma_3^2 + \\lambda}|}{|\\frac{1}{\\sigma_3}|} \\|v_3\\|_2 = \\frac{\\sigma_3^2}{\\sigma_3^2 + \\lambda}\n$$\n这就是 $\\sigma_{\\min}$ 的理论收缩因子。\n\n案例C ($\\lambda = 10^{-4}$)：我们测试分量收缩比。\n经验比率为 $r_{\\text{emp}} = \\frac{|\\langle v_{\\min}, \\hat{\\beta}_{\\lambda} \\rangle|}{|\\langle v_{\\min}, \\hat{\\beta}_{\\text{OLS}} \\rangle|}$。\n使用 $v_{\\min} = v_3$ 和推导出的估计量表达式：\n$$\n\\langle v_3, \\hat{\\beta}_{\\text{OLS}} \\rangle = \\langle v_3, v_3 \\frac{1}{\\sigma_3} \\rangle = \\frac{1}{\\sigma_3} \\|v_3\\|_2^2 = \\frac{1}{\\sigma_3}\n$$\n$$\n\\langle v_3, \\hat{\\beta}_{\\lambda} \\rangle = \\langle v_3, v_3 \\frac{\\sigma_3}{\\sigma_3^2+\\lambda} \\rangle = \\frac{\\sigma_3}{\\sigma_3^2+\\lambda}\n$$\n因此，$r_{\\text{emp}} = \\frac{|\\sigma_3/(\\sigma_3^2+\\lambda)|}{|1/\\sigma_3|} = \\frac{\\sigma_3^2}{\\sigma_3^2+\\lambda}$，这与理论滤波因子 $r_{\\text{theory}}$ 相同。它们的绝对差 $|r_{\\text{emp}} - r_{\\text{theory}}|$ 在数值上应为零。\n\n案例D ($\\lambda = 10^2$)：大的惩罚项应该产生一个范数很小的解向量。\n$$\n\\|\\hat{\\beta}_{\\lambda}\\|_2 = \\left\\| v_3 \\frac{\\sigma_3}{\\sigma_3^2 + \\lambda} \\right\\|_2 = \\frac{\\sigma_3}{\\sigma_3^2 + \\lambda} \\|v_3\\|_2 = \\frac{\\sigma_3}{\\sigma_3^2 + \\lambda}\n$$\n当 $\\lambda = 100$ 时，这个范数将会非常小。\n\n数值实现将遵循这些推导出的公式。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the ridge regression problem as specified.\n    1. Constructs the design matrix X.\n    2. Performs SVD on X and defines the response vector y.\n    3. Computes OLS and ridge solutions for different lambda values.\n    4. Calculates the required metrics for the four test cases.\n    \"\"\"\n\n    # 1. Construct the design matrix X\n    i_vals = np.arange(1, 7, dtype=float).reshape(-1, 1)\n    \n    # Perturbations for columns 2 and 3\n    p2_coeffs = np.array([1, -2, 3, -4, 5, -6]).reshape(-1, 1)\n    p3_coeffs = np.array([-2, 1, -3, 4, -5, 6]).reshape(-1, 1)\n    \n    col1 = i_vals\n    col2 = i_vals + 1e-4 * p2_coeffs\n    col3 = i_vals + 2e-4 * p3_coeffs\n    \n    X = np.hstack([col1, col2, col3])\n    n, p = X.shape\n\n    # 2. Compute SVD of X and define y\n    # U has shape (n, p), s has shape (p,), Vt has shape (p, p)\n    U, s, Vt = np.linalg.svd(X, full_matrices=False)\n    \n    # y is the left singular vector for the smallest singular value\n    # which is the last column of U\n    y = U[:, -1]\n\n    # Get smallest singular value and corresponding right singular vector\n    sigma_min = s[-1]\n    # Vt is V.T, so the last row of Vt is the last column of V (as a row)\n    # v_min should be a column vector\n    v_min = Vt[-1, :].reshape(p, 1)\n    \n    # Helper to compute ridge estimator\n    def get_beta_ridge(lam, X_mat, y_vec):\n        p_dim = X_mat.shape[1]\n        XTX = X_mat.T @ X_mat\n        XTy = X_mat.T @ y_vec\n        I = np.identity(p_dim)\n        # Using np.linalg.solve for better numerical stability than inv()\n        beta = np.linalg.solve(XTX + lam * I, XTy)\n        return beta\n\n    results = []\n\n    # Case A: lambda = 0. Verify ridge prediction equals OLS prediction.\n    lam_A = 0.0\n    beta_ols = np.linalg.lstsq(X, y, rcond=None)[0]\n    y_hat_ols = X @ beta_ols\n    beta_ridge_A = get_beta_ridge(lam_A, X, y)\n    y_hat_ridge_A = X @ beta_ridge_A\n    result_A = np.linalg.norm(y_hat_ridge_A - y_hat_ols) = 1e-8\n    results.append(result_A)\n\n    # Case B: lambda = 1e-6. Compute norm ratio.\n    lam_B = 1e-6\n    # beta_ols is already computed\n    beta_ridge_B = get_beta_ridge(lam_B, X, y)\n    result_B = np.linalg.norm(beta_ridge_B) / np.linalg.norm(beta_ols)\n    results.append(result_B)\n\n    # Case C: lambda = 1e-4. Compare empirical and theoretical shrinkage.\n    lam_C = 1e-4\n    # beta_ols is already computed\n    beta_ridge_C = get_beta_ridge(lam_C, X, y)\n    \n    # Empirical ratio\n    # Inner product of a (p,) vector and a (p,1) vector results in a (1,) array\n    # We extract the scalar value with [0]\n    r_emp_num = np.abs((v_min.T @ beta_ridge_C))[0]\n    r_emp_den = np.abs((v_min.T @ beta_ols))[0]\n    r_emp = r_emp_num / r_emp_den\n\n    # Theoretical ratio\n    r_theory = (sigma_min**2) / (sigma_min**2 + lam_C)\n    \n    result_C = np.abs(r_emp - r_theory)\n    results.append(result_C)\n\n    # Case D: lambda = 1e2. Compute norm of the heavily shrunk estimator.\n    lam_D = 1e2\n    beta_ridge_D = get_beta_ridge(lam_D, X, y)\n    result_D = np.linalg.norm(beta_ridge_D)\n    results.append(result_D)\n    \n    # Final output formatting\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3146059"}]}