{"hands_on_practices": [{"introduction": "理论知识需要通过实践来巩固。本练习将指导你从基本原理出发，亲手构建库克距离。你将通过直接实现留一法（leave-one-out）的重新拟合过程，具体理解“影响”是如何被定义为模型参数的变化，并探索如何衡量单个观测点对交互项的特定影响。这个练习旨在让你超越仅仅使用现有函数的层面，巩固影响诊断背后的基本原理。[@problem_id:3111539]", "problem": "给定一个包含交互项的线性回归设置。考虑一个响应向量 $y \\in \\mathbb{R}^{n}$ 和一个设计矩阵 $X \\in \\mathbb{R}^{n \\times p}$，该矩阵包括一个截距列、两个预测变量 $x_1$ 和 $x_2$，以及它们的交互项 $x_1 x_2$，因此 $p=4$。假设普通最小二乘 (OLS) 模型为 $y = X \\beta + \\varepsilon$，其中 $\\mathbb{E}[\\varepsilon] = 0$ 且 $\\operatorname{Var}(\\varepsilon) = \\sigma^2 I$。OLS 估计量是误差平方和的最小化器，可通过正规方程组求得。对于每个观测索引 $i \\in \\{0,1,\\dots,n-1\\}$，通过移除 $(X,y)$ 的第 $i$ 行后重新拟合 OLS 来定义留一法 (leave-one-out) 估计量。令全样本估计与留一法估计之差的向量表示为 $\\Delta_i \\in \\mathbb{R}^{p}$。\n\n您的任务是：\n- 仅使用普通最小二乘 (OLS) 估计、留一法重拟合以及由 $X^\\top X$ 导出的基于样本的二次型的基本定义，推导出一个有原则的、针对每个观测值的总体影响力的标量度量。该度量通过 $X^\\top X$ 中的信息和噪声方差的估计来缩放参数偏移 $\\Delta_i$，并由模型维度 $p$ 进行归一化。\n- 此外，定义一个局部模拟度量，其仅关注系数的特定子集 $S \\subset \\{0,1,\\dots,p-1\\}$。该度量通过将参数偏移限制在 $S$ 中的坐标，并使用 $X^\\top X$ 相应的主子矩阵来计算，再由 $|S|$ 和相同的噪声方差估计进行归一化。对于本问题，$S$ 将始终是仅包含交互项系数索引的单元素集。\n- 通过直接重拟合（全样本拟合和 $n$ 次留一法拟合）以及适当的二次型，算法化地实现这些定义。不要假设任何关于留一法或影响力的快捷恒等式；根据第一性原理计算定义所要求的内容。\n- 将索引解释为从 0 开始。当要求计算局部度量与总体度量之比且分母为零时，将该观测值的比率定义为 $0$。\n\n使用以下测试套件。在所有情况下，按顺序用列 $[1, x_1, x_2, x_1 x_2]$ 构建 $X$，并令 $S = \\{3\\}$（仅交互项系数）。使用相同的基础数据生成系数 $\\beta^\\star = (\\beta_0,\\beta_1,\\beta_2,\\beta_3) = (1.0, 2.0, -1.5, 0.8)$ 与明确提供的噪声向量一同构成响应值。以下所有数值均需按原样使用。\n\n测试用例 1 (一个具有极端交互项值和较大响应偏差的点)：\n- $n = 10$.\n- $x_1 = [-1.0, -0.5, 0.0, 0.5, 1.0, -0.8, 0.7, -0.2, 0.3, 5.0]$.\n- $x_2 = [0.9, -1.1, 0.2, -0.4, 0.6, -0.7, 0.8, 0.3, -0.5, 5.0]$.\n- 噪声向量 $\\eta = [0.05, -0.04, 0.01, -0.02, 0.03, 0.02, -0.01, 0.00, 0.01, 0.00]$.\n- 通过 $y_i = \\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + \\beta_3 (x_{1,i} x_{2,i}) + \\eta_i$（其中 $i = 0,\\dots,9$）构建 $y$，然后为最后一个响应值增加一个 $8.0$ 的额外偏差：$y_9 \\leftarrow y_9 + 8.0$。\n- 此用例的必需输出：\n  1. 使关于 $S = \\{3\\}$ 的局部度量最大化的索引 $i^\\star$（一个整数）。\n  2. 在 $i^\\star$ 处的总体度量（一个浮点数），四舍五入到 $6$ 位小数。\n  3. 在 $i^\\star$ 处的局部度量（一个浮点数），四舍五入到 $6$ 位小数。\n\n测试用例 2 (没有极端值；中等交互作用)：\n- $n = 12$.\n- $x_1 = [-1.2, -0.9, -0.6, -0.3, 0.0, 0.3, 0.6, 0.9, 1.2, -0.4, 0.4, 0.1]$.\n- $x_2 = [0.5, -0.4, 0.7, -0.8, 0.3, -0.2, 0.1, -0.1, 0.2, 0.9, -0.9, 0.0]$.\n- 噪声向量 $\\eta = [0.01, -0.02, 0.03, -0.01, 0.00, 0.02, -0.01, 0.00, 0.01, 0.02, -0.02, 0.00]$.\n- 通过 $y_i = \\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + \\beta_3 (x_{1,i} x_{2,i}) + \\eta_i$（其中 $i = 0,\\dots,11$）构建 $y$。\n- 设阈值为 $\\tau = 0.05$。\n- 此用例的必需输出：\n  1. 所有 $i$ 的最大总体度量（一个浮点数），四舍五入到 $6$ 位小数。\n  2. 局部度量严格大于 $\\tau$ 的点的数量（一个整数）。\n\n测试用例 3 (一个高杠杆率的交互点，但残差很小)：\n- $n = 6$.\n- $x_1 = [-2.0, -1.0, 0.0, 1.0, 2.0, 4.0]$.\n- $x_2 = [0.5, -0.5, 0.2, -0.2, 0.5, 4.0]$.\n- 噪声向量 $\\eta = [0.00, 0.01, -0.01, 0.00, 0.02, 0.00]$.\n- 通过 $y_i = \\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + \\beta_3 (x_{1,i} x_{2,i}) + \\eta_i$（其中 $i = 0,\\dots,5$）构建 $y$。\n- 此用例的必需输出：\n  1. 所有 $i$ 的比率 $\\rho_i$ 的平均值，其中当总体度量不为零时，$\\rho_i$ 是在 $i$ 处的局部度量与总体度量的比值；如果总体度量为 $0$，则 $\\rho_i = 0$。将此平均值报告为四舍五入到 $6$ 位小数的浮点数。\n\n实现说明和约束：\n- 通过无偏残差方差 $\\hat{\\sigma}^2 = \\text{RSS}/(n-p)$ 从全样本拟合中估计噪声方差，其中 $\\text{RSS}$ 是全样本拟合的残差平方和，且 $p = 4$。\n- 对于每个观测值 $i$，计算一次留一法重拟合以获得相应的参数偏移。\n- 子集 $S$ 是 $\\{3\\}$，即仅交互项系数。\n- 所有索引都是从 0 开始的。\n- 您的程序应生成单行输出，其中按顺序包含所有三个测试用例的结果，平铺为一个列表，并用方括号括起来，值之间用逗号分隔且无空格。具体来说，输出格式为\n  $[i^\\star, \\text{overall at } i^\\star, \\text{partial at } i^\\star, \\max \\text{ overall}, \\text{count partial}>\\tau, \\text{mean ratio}]$\n  其中浮点数按指定要求四舍五入到 $6$ 位小数。", "solution": "该问题要求推导并实现线性回归模型中观测值的两种影响力测度，然后将这些测度应用于三个不同的测试用例。验证证实了该问题具有科学依据、提法明确且客观。我们可以着手提供解决方案。\n\n### 1. 理论框架\n\n该问题设置在普通最小二乘 (OLS) 线性回归的背景下。\n\n#### 1.1. 模型与估计\n模型由 $y = X \\beta + \\varepsilon$ 给出，其中 $y \\in \\mathbb{R}^n$ 是响应向量，$X \\in \\mathbb{R}^{n \\times p}$ 是设计矩阵，$\\beta \\in \\mathbb{R}^p$ 是系数向量，$\\varepsilon \\in \\mathbb{R}^n$ 是一个不可观测的误差向量，满足 $\\mathbb{E}[\\varepsilon] = 0$ 和 $\\operatorname{Var}(\\varepsilon) = \\sigma^2 I$。参数向量的维度为 $p=4$，X 的列对应于一个截距、两个预测变量 $x_1$ 和 $x_2$，以及它们的交互项 $x_1 x_2$。\n\nOLS 估计量 $\\hat{\\beta}$ 最小化残差平方和 $\\|y - X\\beta\\|^2$。其解通过正规方程组找到：\n$$ (X^\\top X) \\hat{\\beta} = X^\\top y $$\n如果 $X^\\top X$ 可逆，则唯一解为 $\\hat{\\beta} = (X^\\top X)^{-1} X^\\top y$。\n\n此全数据拟合的残差为 $e = y - X\\hat{\\beta}$。残差平方和为 $\\text{RSS} = e^\\top e$。误差方差 $\\sigma^2$ 的一个无偏估计量是：\n$$ \\hat{\\sigma}^2 = \\frac{\\text{RSS}}{n-p} $$\n该估计量将用于缩放影响力测度。\n\n#### 1.2. 留一法 (LOO) 估计\n对于每个观测值 $i \\in \\{0, 1, \\dots, n-1\\}$，我们通过从 $X$ 和 $y$ 中移除第 $i$ 行来定义留一法 (LOO) 数据集。将这些表示为 $X_{(i)} \\in \\mathbb{R}^{(n-1) \\times p}$ 和 $y_{(i)} \\in \\mathbb{R}^{n-1}$。\n\nLOO OLS 估计 $\\hat{\\beta}_{(i)}$ 是通过将模型拟合到这个简化的数据集得到的：\n$$ \\hat{\\beta}_{(i)} = (X_{(i)}^\\top X_{(i)})^{-1} X_{(i)}^\\top y_{(i)} $$\n移除观测值 $i$ 后系数估计的变化即为参数偏移向量：\n$$ \\Delta_i = \\hat{\\beta} - \\hat{\\beta}_{(i)} $$\n\n### 2. 影响力测度的推导\n\n问题要求两种有原则的影响力测度，我们根据提供的规范进行推导。\n\n#### 2.1. 总体影响力测度\n该测度必须使用 $X^\\top X$ 中的信息和估计的噪声方差 $\\hat{\\sigma}^2$ 来缩放参数偏移 $\\Delta_i$，并由模型维度 $p$ 进行归一化。\n\n在设计矩阵 $X$ 的几何背景下，衡量偏移向量 $\\Delta_i$ “大小” 的一种自然方法是使用由矩阵 $X^\\top X$ 导出的二次型。这个量 $\\Delta_i^\\top (X^\\top X) \\Delta_i$ 表示由全模型和 LOO 模型作出的预测值之间的平方差之和：\n$$ \\|X\\hat{\\beta} - X\\hat{\\beta}_{(i)}\\|^2 = \\|X(\\hat{\\beta} - \\hat{\\beta}_{(i)})\\|^2 = \\|X\\Delta_i\\|^2 = (X\\Delta_i)^\\top(X\\Delta_i) = \\Delta_i^\\top X^\\top X \\Delta_i $$\n该项量化了移除观测值 $i$ 对拟合值 $\\hat{y}$ 的总体影响。\n\n为了创建一个标准化的无量纲度量，我们用估计的误差方差 $\\hat{\\sigma}^2$ 对其进行缩放。再通过参数数量 $p$ 进行归一化，得到每个参数的平均影响力。这将文字描述形式化为以下总体影响力测度 $D_i^{\\text{overall}}$：\n$$ D_i^{\\text{overall}} = \\frac{\\Delta_i^\\top (X^\\top X) \\Delta_i}{p \\hat{\\sigma}^2} $$\n该公式等同于 Cook's 距离的标准定义。\n\n#### 2.2. 局部影响力测度\n需要一个局部模拟度量，其专注于系数的子集 $S \\subset \\{0, 1, \\dots, p-1\\}$。该测度是通过将参数偏移向量限制在其在 $S$ 中的分量（表示为 $\\Delta_{i,S}$），并使用 $X^\\top X$ 的相应主子矩阵（表示为 $(X^\\top X)_S$）来构建的。结果由子集的大小 $|S|$ 和方差估计 $\\hat{\\sigma}^2$ 进行归一化。\n\n遵循同样的逻辑，局部偏移的二次型为 $\\Delta_{i,S}^\\top (X^\\top X)_S \\Delta_{i,S}$。应用指定的归一化定义了局部影响力测度 $D_{i,S}^{\\text{partial}}$：\n$$ D_{i,S}^{\\text{partial}} = \\frac{\\Delta_{i,S}^\\top (X^\\top X)_S \\Delta_{i,S}}{|S| \\hat{\\sigma}^2} $$\n对于本问题，子集是 $S=\\{3\\}$，对应于交互项 $x_1 x_2$ 的系数。因此， $|S|=1$。向量 $\\Delta_{i,S}$ 变为标量 $\\Delta_{i,3} = \\hat{\\beta}_3 - \\hat{\\beta}_{(i),3}$，矩阵 $(X^\\top X)_S$ 是标量元素 $(X^\\top X)_{3,3}$（使用从 0 开始的索引）。局部测度简化为：\n$$ D_{i,\\{3\\}}^{\\text{partial}} = \\frac{(\\Delta_{i,3})^2 (X^\\top X)_{3,3}}{\\hat{\\sigma}^2} $$\n\n### 3. 算法实现\n\n解决方案将通过直接遵循这些推导出的公式来实现，而不借助于任何用于 LOO 估计的代数捷径（例如涉及帽子矩阵的那些）。\n\n对于每个测试用例：\n1.  用给定的数据生成过程和噪声构建设计矩阵 $X \\in \\mathbb{R}^{n \\times 4}$（其列为 $[1, x_1, x_2, x_1 x_2]$）和响应向量 $y \\in \\mathbb{R}^n$。\n2.  通过求解正规方程 $(X^\\top X) \\hat{\\beta} = X^\\top y$ 来计算全数据 OLS 估计 $\\hat{\\beta}$。\n3.  计算无偏方差估计 $\\hat{\\sigma}^2 = \\frac{1}{n-p} \\|y - X\\hat{\\beta}\\|^2$。\n4.  遍历每个观测值 $i=0, \\dots, n-1$：\n    a. 通过删除第 $i$ 行构建 LOO 数据 $(X_{(i)}, y_{(i)})$。\n    b. 通过求解 $(X_{(i)}^\\top X_{(i)}) \\hat{\\beta}_{(i)} = X_{(i)}^\\top y_{(i)}$ 来计算 LOO 估计 $\\hat{\\beta}_{(i)}$。\n    c. 计算参数偏移 $\\Delta_i = \\hat{\\beta} - \\hat{\\beta}_{(i)}$。\n    d. 使用上面推导的公式计算 $D_i^{\\text{overall}}$ 和 $D_{i,\\{3\\}}^{\\text{partial}}$。\n5.  收集计算出的测度并计算每个测试用例所需的具体输出（例如，最大值、计数、比率的均值）。\n\n所有数值结果将按要求四舍五入到 $6$ 位小数。如果 $D_i^{\\text{overall}}=0$，则比率 $\\rho_i = D_{i,\\{3\\}}^{\\text{partial}} / D_i^{\\text{overall}}$ 定义为 $0$。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve all test cases and print the final result.\n    \"\"\"\n    beta_star = np.array([1.0, 2.0, -1.5, 0.8])\n    p = 4\n    S_idx = 3\n\n    # Test Case 1\n    # Givens\n    n1 = 10\n    x1_1 = np.array([-1.0, -0.5, 0.0, 0.5, 1.0, -0.8, 0.7, -0.2, 0.3, 5.0])\n    x2_1 = np.array([0.9, -1.1, 0.2, -0.4, 0.6, -0.7, 0.8, 0.3, -0.5, 5.0])\n    eta1 = np.array([0.05, -0.04, 0.01, -0.02, 0.03, 0.02, -0.01, 0.00, 0.01, 0.00])\n    y1_mod = (9, 8.0)\n    \n    # Process Case 1\n    overall1, partial1 = process_case(n1, x1_1, x2_1, beta_star, eta1, y1_mod, p, S_idx)\n    i_star = np.argmax(partial1)\n    result1 = int(i_star)\n    result2 = round(overall1[i_star], 6)\n    result3 = round(partial1[i_star], 6)\n\n    # Test Case 2\n    # Givens\n    n2 = 12\n    x1_2 = np.array([-1.2, -0.9, -0.6, -0.3, 0.0, 0.3, 0.6, 0.9, 1.2, -0.4, 0.4, 0.1])\n    x2_2 = np.array([0.5, -0.4, 0.7, -0.8, 0.3, -0.2, 0.1, -0.1, 0.2, 0.9, -0.9, 0.0])\n    eta2 = np.array([0.01, -0.02, 0.03, -0.01, 0.00, 0.02, -0.01, 0.00, 0.01, 0.02, -0.02, 0.00])\n    tau = 0.05\n    \n    # Process Case 2\n    overall2, partial2 = process_case(n2, x1_2, x2_2, beta_star, eta2, None, p, S_idx)\n    result4 = round(np.max(overall2), 6)\n    result5 = int(np.sum(partial2 > tau))\n\n    # Test Case 3\n    # Givens\n    n3 = 6\n    x1_3 = np.array([-2.0, -1.0, 0.0, 1.0, 2.0, 4.0])\n    x2_3 = np.array([0.5, -0.5, 0.2, -0.2, 0.5, 4.0])\n    eta3 = np.array([0.00, 0.01, -0.01, 0.00, 0.02, 0.00])\n\n    # Process Case 3\n    overall3, partial3 = process_case(n3, x1_3, x2_3, beta_star, eta3, None, p, S_idx)\n    ratios = np.zeros_like(overall3)\n    non_zero_mask = overall3 != 0\n    ratios[non_zero_mask] = partial3[non_zero_mask] / overall3[non_zero_mask]\n    result6 = round(np.mean(ratios), 6)\n\n    # Combine results and print\n    final_results = [result1, result2, result3, result4, result5, result6]\n    print(f\"[{','.join(map(str, final_results))}]\")\n\n\ndef process_case(n, x1, x2, beta_star, eta, y_mod, p, S_idx):\n    \"\"\"\n    Performs the full analysis for a single test case.\n    \"\"\"\n    # 1. Construct design matrix X and response vector y\n    X = np.column_stack([np.ones(n), x1, x2, x1*x2])\n    y = X @ beta_star + eta\n    if y_mod:\n        y[y_mod[0]] += y_mod[1]\n\n    # 2. Full data fit\n    XTX = X.T @ X\n    XTy = X.T @ y\n    beta_hat = np.linalg.solve(XTX, XTy)\n    \n    # 3. Estimate noise variance\n    residuals = y - X @ beta_hat\n    rss = residuals.T @ residuals\n    sigma2_hat = rss / (n - p)\n\n    overall_measures = np.zeros(n)\n    partial_measures = np.zeros(n)\n\n    # 4. Leave-one-out refitting loop\n    for i in range(n):\n        # Create LOO data\n        X_i = np.delete(X, i, axis=0)\n        y_i = np.delete(y, i, axis=0)\n        \n        # LOO fit\n        XTX_i = X_i.T @ X_i\n        XTy_i = X_i.T @ y_i\n        try:\n            beta_hat_i = np.linalg.solve(XTX_i, XTy_i)\n        except np.linalg.LinAlgError:\n            # Handle cases where LOO matrix is singular, not expected for these tests\n            beta_hat_i = np.full(p, np.nan)\n\n        # Parameter shift\n        delta_i = beta_hat - beta_hat_i\n        \n        # Avoid issues with division by zero or NaN propagation\n        if np.isnan(delta_i).any() or sigma2_hat == 0:\n            overall_measures[i] = 0.0\n            partial_measures[i] = 0.0\n            continue\n        \n        # Overall influence measure\n        numerator_overall = delta_i.T @ XTX @ delta_i\n        d_overall = numerator_overall / (p * sigma2_hat)\n        overall_measures[i] = d_overall\n\n        # Partial influence measure\n        delta_i_S = delta_i[S_idx] \n        XTX_S_val = XTX[S_idx, S_idx]\n        \n        numerator_partial = (delta_i_S**2) * XTX_S_val\n        # |S| = 1, so no division by |S|\n        d_partial = numerator_partial / sigma2_hat\n        partial_measures[i] = d_partial\n\n    return overall_measures, partial_measures\n\nsolve()\n\n```", "id": "3111539"}, {"introduction": "库克距离与其他诊断工具有何不同？本练习通过将衡量整体拟合影响的库克距离、预测残差平方和（PRESS）以及对特定新数据点预测的影响进行比较，挑战了“单一最强影响点”的概念。你将构建一个情景，证明这些不同的度量标准可能会将不同的数据点识别为最关键点，从而强调根据你的分析目标选择合适诊断工具的重要性。[@problem_id:3111497]", "problem": "考虑一个带截距的单变量普通最小二乘（OLS）线性回归，其设计矩阵为 $X \\in \\mathbb{R}^{n \\times p}$，其中 $p = 2$ 列对应于截距和单个预测变量。设预测变量值为 $x_i \\in \\mathbb{R}$（$i = 0, 1, \\dots, n-1$），响应为 $y_i \\in \\mathbb{R}$。模型为 $y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i$，OLS 估计量 $\\hat{\\beta}$ 由正规方程定义。将帽子矩阵 $H$ 定义为到 $X$ 列空间上的正交投影算子，残差 $r = y - X\\hat{\\beta}$，均方误差 $\\mathrm{MSE}$ 定义为残差平方和除以自由度 $n - p$。库克距离 $D_i$ 衡量删除观测点 $i$ 对估计回归的总体影响，而观测点 $i$ 的预测残差平方和（PRESS）残差，记为 $\\mathrm{PRESS}_i$，是观测点 $i$ 的留一法残差。\n\n您的任务是：\n- 从 OLS 正规方程和帽子矩阵 $H$ 的定义出发，推导表达式，使您能够为每个观测点 $i$ 计算库克距离 $D_i$ 和留一法残差的绝对值 $\\lvert \\mathrm{PRESS}_i \\rvert$，而无需重新拟合回归 $n$ 次。\n- 将删除观测点 $i$ 在特定目标点 $x^\\ast$ 处的预测损害定义为在 $x^\\ast$ 处的预测值在全数据拟合和删除观测点 $i$ 后的拟合之间的绝对变化，即 $h_i(x^\\ast) = \\lvert \\hat{y}(x^\\ast) - \\hat{y}^{(-i)}(x^\\ast) \\rvert$。\n- 使用这些定义，构建一个数据示例，并证明按库克距离 $D_i$ 对观测点进行的排序与在特定 $x^\\ast$ 处为预测损害评估时由 $\\lvert \\mathrm{PRESS}_i \\rvert$ 的大小所建议的排序不同。换句话说，证明被 $D_i$ 标记为最具损害性的观测点与删除后在 $x^\\ast$ 处预测变化最大的观测点不同，并且这还可能与具有最大 $\\lvert \\mathrm{PRESS}_i \\rvert$ 的观测点不同。\n\n使用以下科学上一致的、自洽的数据集（其构造使得 OLS 拟合等于一条简单的直线，且残差与 $X$ 的列空间正交）：\n- 观测点数量 $n = 5$。\n- 预测变量值 $x = [-2, -1, 0, 1, 2]$。\n- 选择一条目标直线 $y = 1 + x$ 和满足 $X^\\top r = 0$ 的残差 $r = [1, -2.5, 3, -2.5, 1]$；定义 $y_i = 1 + x_i + r_i$ (对于 $i = 0, 1, 2, 3, 4$）。\n\n测试套件（三个目标点）：\n- 情况 1：$x^\\ast = 2.5$。\n- 情况 2：$x^\\ast = 0.0$。\n- 情况 3：$x^\\ast = 10.0$。\n\n实现要求：\n- 从第一性原理出发：使用 OLS 正规方程、帽子矩阵的定义和线性代数恒等式来获得计算公式；不要重新拟合模型 $n$ 次。\n- 对于每个测试用例，计算：\n    1. 具有最大库克距离 $D_i$ 的观测点的索引（使用从零开始的索引）。\n    2. 具有最大 $\\lvert \\mathrm{PRESS}_i \\rvert$ 的观测点的索引（从零开始）。\n    3. 具有最大预测损害 $h_i(x^\\ast)$ 的观测点的索引（从零开始）。\n    4. 一个布尔值，指示最大的库克距离是否对应于 $x^\\ast$ 处的最大预测损害。\n    5. 一个布尔值，指示最大的 $\\lvert \\mathrm{PRESS}_i \\rvert$ 是否对应于 $x^\\ast$ 处的最大预测损害。\n- 如果出现平局，选择最小的索引。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含三个测试用例的结果，格式为一个用方括号括起来的逗号分隔列表。每个测试用例的结果必须是 $[\\text{argmaxD}, \\text{argmaxPRESS}, \\text{argmaxHarm}, \\text{CookEqualsHarm}, \\text{PRESSEqualsHarm}]$ 形式的列表，并在单行上聚合为 $[[\\dots],[\\dots],[\\dots]]$。所有索引都是从零开始的整数，布尔值必须是 $\\text{True}$ 或 $\\text{False}$。", "solution": "该问题要求推导普通最小二乘（OLS）回归中几种影响诊断的计算公式，并将其应用于一个特定数据集，以证明不同的诊断方法可以对观测点的影响进行不同的排序。\n\n### **1. 理论推导**\n\n设 OLS 模型为 $y = X\\beta + \\varepsilon$，其中 $y \\in \\mathbb{R}^n$，$X \\in \\mathbb{R}^{n \\times p}$，$\\beta \\in \\mathbb{R}^p$，$\\varepsilon$ 是误差向量。$\\beta$ 的 OLS 估计由正规方程 $X^\\top X \\hat{\\beta} = X^\\top y$ 给出，从而得到 $\\hat{\\beta} = (X^\\top X)^{-1} X^\\top y$。预测值为 $\\hat{y} = X\\hat{\\beta} = Hy$，其中 $H = X(X^\\top X)^{-1}X^\\top$ 是帽子矩阵。$H$ 的对角元素，记为 $h_{ii}$，是每个观测点的杠杆值。残差为 $r = y - \\hat{y} = (I-H)y$。\n\n设 $\\hat{\\beta}^{(-i)}$ 表示当从数据集中移除观测点 $i$（对应的数据行为 $x_i^\\top$，响应为 $y_i$）时 $\\beta$ 的 OLS 估计。我们的目标是计算与此留一法（LOO）模型相关的诊断指标，而无需显式地重新拟合回归 $n$ 次。\n\n核心结果是系数向量变化 $\\hat{\\beta} - \\hat{\\beta}^{(-i)}$ 的表达式。这可以使用 Sherman-Morrison 公式（用于秩为 1 更新矩阵的逆）高效地推导出来。矩阵 $(X^{(-i)})^\\top X^{(-i)}$ 可以写成 $X^\\top X$ 的一个秩为 1 的更新：\n$$ (X^{(-i)})^\\top X^{(-i)} = \\sum_{j \\neq i} x_j x_j^\\top = X^\\top X - x_i x_i^\\top $$\n应用 Sherman-Morrison 公式 $(A - uv^\\top)^{-1} = A^{-1} + \\frac{A^{-1}uv^\\top A^{-1}}{1 - v^\\top A^{-1}u}$，并令 $A=X^\\top X$ 和 $u=v=x_i$，可得：\n$$ ((X^{(-i)})^\\top X^{(-i)})^{-1} = (X^\\top X)^{-1} + \\frac{(X^\\top X)^{-1} x_i x_i^\\top (X^\\top X)^{-1}}{1 - x_i^\\top (X^\\top X)^{-1} x_i} = (X^\\top X)^{-1} + \\frac{(X^\\top X)^{-1} x_i x_i^\\top (X^\\top X)^{-1}}{1 - h_{ii}} $$\nLOO 估计为 $\\hat{\\beta}^{(-i)} = ((X^{(-i)})^\\top X^{(-i)})^{-1} (X^{(-i)})^\\top y^{(-i)}$。使用上述恒等式以及 $(X^{(-i)})^\\top y^{(-i)} = X^\\top y - x_i y_i$，经过代数简化，我们得到众所周知的结果：\n$$ \\hat{\\beta} - \\hat{\\beta}^{(-i)} = \\frac{(X^\\top X)^{-1} x_i (y_i - x_i^\\top \\hat{\\beta})}{1 - h_{ii}} = \\frac{(X^\\top X)^{-1} x_i r_i}{1 - h_{ii}} $$\n其中 $r_i = y_i - \\hat{y}_i$ 是第 $i$ 个普通残差。这个表达式是高效计算所有所需 LOO 诊断指标的核心。\n\n#### **PRESS 残差 ($\\mathrm{PRESS}_i$)**\n预测残差平方和（PRESS）残差是使用一个不包含观测点 $i$ 的模型来预测 $y_i$ 时的误差。\n$$ \\mathrm{PRESS}_i = y_i - \\hat{y}_i^{(-i)} = y_i - x_i^\\top \\hat{\\beta}^{(-i)} $$\n代入 $\\hat{\\beta}^{(-i)} = \\hat{\\beta} - (\\hat{\\beta} - \\hat{\\beta}^{(-i)})$：\n$$ \\mathrm{PRESS}_i = y_i - x_i^\\top \\left( \\hat{\\beta} - \\frac{(X^\\top X)^{-1} x_i r_i}{1 - h_{ii}} \\right) = (y_i - x_i^\\top \\hat{\\beta}) + \\frac{x_i^\\top (X^\\top X)^{-1} x_i r_i}{1 - h_{ii}} $$\n$$ \\mathrm{PRESS}_i = r_i + \\frac{h_{ii} r_i}{1 - h_{ii}} = r_i \\left( 1 + \\frac{h_{ii}}{1 - h_{ii}} \\right) = \\frac{r_i}{1 - h_{ii}} $$\n这提供了一个仅使用普通残差 $r_i$ 和杠杆值 $h_{ii}$ 来计算 PRESS 残差的简单公式。我们关心的是它的绝对值 $\\lvert \\mathrm{PRESS}_i \\rvert$。\n\n#### **库克距离 ($D_i$)**\n库克距离衡量了删除观测点 $i$ 对拟合值向量的影响。其定义为：\n$$ D_i = \\frac{(\\hat{y} - \\hat{y}^{(-i)})^\\top (\\hat{y} - \\hat{y}^{(-i)})}{p \\cdot \\mathrm{MSE}} $$\n其中 $\\hat{y}^{(-i)} = X \\hat{\\beta}^{(-i)}$ 且 $\\mathrm{MSE} = \\frac{1}{n-p} \\sum_{j=1}^n r_j^2$。分子是拟合值向量之间的欧几里得距离的平方。\n$$ \\hat{y} - \\hat{y}^{(-i)} = X(\\hat{\\beta} - \\hat{\\beta}^{(-i)}) = X \\frac{(X^\\top X)^{-1} x_i r_i}{1 - h_{ii}} $$\n分子变为：\n$$ \\left( X \\frac{(X^\\top X)^{-1} x_i r_i}{1 - h_{ii}} \\right)^\\top \\left( X \\frac{(X^\\top X)^{-1} x_i r_i}{1 - h_{ii}} \\right) = \\frac{r_i^2}{(1 - h_{ii})^2} x_i^\\top (X^\\top X)^{-1} X^\\top X (X^\\top X)^{-1} x_i = \\frac{r_i^2 h_{ii}}{(1 - h_{ii})^2} $$\n将此代回 $D_i$ 的定义中：\n$$ D_i = \\frac{1}{p \\cdot \\mathrm{MSE}} \\frac{r_i^2 h_{ii}}{(1 - h_{ii})^2} $$\n该公式允许从残差 $r_i$、杠杆值 $h_{ii}$、预测变量数量 $p$ 和均方误差 $\\mathrm{MSE}$ 来计算 $D_i$。\n\n#### **预测损害 ($h_i(x^\\ast)$)**\n在目标点 $x^\\ast$ 处的预测损害是当删除观测点 $i$ 时该点预测值的绝对变化。设 $x_{new}^\\top = [1, x^\\ast]$。\n$$ h_i(x^\\ast) = \\lvert \\hat{y}(x^\\ast) - \\hat{y}^{(-i)}(x^\\ast) \\rvert = \\lvert x_{new}^\\top \\hat{\\beta} - x_{new}^\\top \\hat{\\beta}^{(-i)} \\rvert = \\lvert x_{new}^\\top (\\hat{\\beta} - \\hat{\\beta}^{(-i)}) \\rvert $$\n使用我们关于 $\\hat{\\beta} - \\hat{\\beta}^{(-i)}$ 的核心结果：\n$$ h_i(x^\\ast) = \\left\\lvert x_{new}^\\top \\frac{(X^\\top X)^{-1} x_i r_i}{1 - h_{ii}} \\right\\rvert = \\frac{\\lvert r_i \\rvert}{1 - h_{ii}} \\lvert x_{new}^\\top (X^\\top X)^{-1} x_i \\rvert = \\lvert \\mathrm{PRESS}_i \\rvert \\cdot \\lvert x_{new}^\\top (X^\\top X)^{-1} x_i \\rvert $$\n这表明预测损害是 PRESS 残差的绝对值，通过一个依赖于目标点 $x^\\ast$ 和被删除点 $x_i$ 的因子进行重新加权。\n\n### **2. 在数据集上的应用**\n\n给定 $n=5$，$p=2$，预测变量 $x = [-2, -1, 0, 1, 2]^\\top$，残差 $r = [1, -2.5, 3, -2.5, 1]^\\top$。设计矩阵 $X$ 是：\n$$ X = \\begin{pmatrix} 1  -2 \\\\ 1  -1 \\\\ 1  0 \\\\ 1  1 \\\\ 1  2 \\end{pmatrix} $$\n问题的构造使得 $y_i = (1+x_i) + r_i$ 的 OLS 拟合恰好是 $\\hat{y}_i = 1+x_i$。预测变量值是中心化的，$\\sum_{i=0}^{4} x_i = 0$。这简化了 $X^\\top X$：\n$$ X^\\top X = \\begin{pmatrix} n  \\sum x_i \\\\ \\sum x_i  \\sum x_i^2 \\end{pmatrix} = \\begin{pmatrix} 5  0 \\\\ 0  10 \\end{pmatrix} \\implies (X^\\top X)^{-1} = \\begin{pmatrix} 1/5  0 \\\\ 0  1/10 \\end{pmatrix} $$\n观测点 $i$ 的杠杆值为 $h_{ii} = x_i^\\top (X^\\top X)^{-1} x_i = \\begin{pmatrix} 1  x_i \\end{pmatrix} \\begin{pmatrix} 1/5  0 \\\\ 0  1/10 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ x_i \\end{pmatrix} = \\frac{1}{5} + \\frac{x_i^2}{10}$。\n对于 $i=0, \\dots, 4$，杠杆值为 $[0.6, 0.3, 0.2, 0.3, 0.6]$。\n\n残差平方和为 $\\mathrm{RSS} = \\sum r_i^2 = 1^2 + (-2.5)^2 + 3^2 + (-2.5)^2 + 1^2 = 23.5$。\n均方误差为 $\\mathrm{MSE} = \\frac{\\mathrm{RSS}}{n-p} = \\frac{23.5}{5-2} = \\frac{23.5}{3}$。\n\n现在我们可以计算这些诊断指标。\n- 库克距离的因子为 $D_i \\propto r_i^2 \\frac{h_{ii}}{(1-h_{ii})^2}$。与 $D_i$ 成比例的值为 $[3.75, 3.8265, 2.8125, 3.8265, 3.75]$。最大值出现在索引 $1$ 和 $3$ 处。选择最小的索引：$\\text{argmax}(D_i) = 1$。\n- PRESS 残差的绝对值为 $|\\mathrm{PRESS}_i| = \\frac{|r_i|}{1-h_{ii}}$。值为 $[2.5, 3.5714, 3.75, 3.5714, 2.5]$。最大值出现在索引 $2$ 处：$\\text{argmax}(|\\mathrm{PRESS}_i|) = 2$。\n\n预测损害因子为 $\\lvert x_{new}^\\top (X^\\top X)^{-1} x_i \\rvert = \\lvert [1, x^\\ast] [1/5, x_i/10]^\\top \\rvert = \\lvert \\frac{1}{5} + \\frac{x^\\ast x_i}{10} \\rvert$。\n\n- **情况 1: $x^\\ast = 2.5$**\n$h_i(2.5) = |\\mathrm{PRESS}_i| \\cdot \\lvert 0.2 + 0.25 x_i \\rvert$。值为 $[0.75, 0.1786, 0.75, 1.6071, 1.75]$。\n$\\text{argmax}(h_i(2.5)) = 4$。\n结果：$[\\text{argmaxD}=1, \\text{argmaxPRESS}=2, \\text{argmaxHarm}=4, \\text{False}, \\text{False}]$。\n\n- **情况 2: $x^\\ast = 0.0$**\n$h_i(0.0) = |\\mathrm{PRESS}_i| \\cdot \\lvert 0.2 + 0 \\rvert = 0.2 |\\mathrm{PRESS}_i|$。排序与 $|\\mathrm{PRESS}_i|$ 的排序相同。\n$\\text{argmax}(h_i(0.0)) = 2$。\n结果：$[\\text{argmaxD}=1, \\text{argmaxPRESS}=2, \\text{argmaxHarm}=2, \\text{False}, \\text{True}]$。\n\n- **情况 3: $x^\\ast = 10.0$**\n$h_i(10.0) = |\\mathrm{PRESS}_i| \\cdot \\lvert 0.2 + x_i \\rvert$。值为 $[4.5, 2.8571, 0.75, 4.2857, 5.5]$。\n$\\text{argmax}(h_i(10.0)) = 4$。\n结果：$[\\text{argmaxD}=1, \\text{argmaxPRESS}=2, \\text{argmaxHarm}=4, \\text{False}, \\text{False}]$。\n\n该分析表明，“最具影响力的点”这一概念是依赖于上下文的。库克距离在整个数据空间上平均化影响，并因中等大小的残差和中等杠杆值的组合而将观测点 $1$（和 $3$）识别为最具影响力的点。PRESS 分数仅关注自我预测，因其最大的残差绝对值而将观测点 $2$ 识别为最具影响力的点。然而，预测损害取决于目标点 $x^\\ast$。对于远离数据中心（即 $|x^\\ast|$ 较大）的预测，高杠杆值点（如 $0$ 和 $4$）变得占主导地位，因为移除它们对回归斜率的影响最大。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the regression influence diagnostics problem.\n\n    The solution proceeds in three main steps:\n    1.  Setup: Define the dataset (x, r), the number of observations (n), and\n        the number of parameters (p). The response y and coefficients are\n        implicitly defined by the problem statement.\n    2.  Pre-computation: Calculate quantities that are constant across all test\n        cases. This includes the (X^T X)^-1 matrix, leverages (h_ii),\n        mean squared error (MSE), Cook's distances (D_i), and PRESS residuals.\n        The indices of the observations with the largest D_i and |PRESS_i|\n        are determined here.\n    3.  Per-case computation: Loop through each target point x_star provided\n        in the test suite. For each case, calculate the predictive harm h_i(x_star)\n        for all observations, find the index of the observation with the\n        largest harm, and compare this index with the pre-computed indices for\n        D_i and |PRESS_i|.\n    \"\"\"\n    # 1. Setup based on the problem statement\n    n = 5  # Number of observations\n    p = 2  # Number of parameters (intercept + 1 predictor)\n    x = np.array([-2.0, -1.0, 0.0, 1.0, 2.0])\n    r = np.array([1.0, -2.5, 3.0, -2.5, 1.0]) # Ordinary residuals\n\n    # 2. Pre-computation of constant quantities\n    \n    # Design matrix X would be np.c_[np.ones(n), x]\n    # Since sum(x) = 0, X^T*X is a diagonal matrix.\n    # X_T_X = [[n, sum(x)], [sum(x), sum(x^2)]] = [[5, 0], [0, 10]]\n    X_T_X_inv = np.array([[1/n, 0], [0, 1/np.sum(x**2)]])\n\n    # Calculate leverages (h_ii). h_ii = x_i^T * (X^T*X)^-1 * x_i\n    # where x_i is the i-th row of X, i.e., [1, x_i].\n    h_ii = np.zeros(n)\n    for i in range(n):\n        x_i_row = np.array([1, x[i]])\n        h_ii[i] = x_i_row @ X_T_X_inv @ x_i_row\n\n    # Calculate Mean Squared Error (MSE)\n    rss = np.sum(r**2)\n    mse = rss / (n - p)\n\n    # Calculate Cook's Distances (D_i)\n    # D_i = r_i^2 * h_ii / (p * MSE * (1 - h_ii)^2)\n    D_i = (r**2 / (p * mse)) * (h_ii / (1 - h_ii)**2)\n    argmax_D = np.argmax(D_i)\n\n    # Calculate magnitudes of PRESS residuals\n    # |PRESS_i| = |r_i / (1 - h_ii)|\n    press_i_mag = np.abs(r / (1 - h_ii))\n    argmax_press = np.argmax(press_i_mag)\n\n    # 3. Per-case computation for each target point x_star\n    test_cases = [2.5, 0.0, 10.0]\n    results = []\n\n    for x_star in test_cases:\n        # Calculate predictive harm h_i(x_star)\n        # h_i(x_star) = |PRESS_i| * |x_new^T * (X^T*X)^-1 * x_i|\n        # where x_new = [1, x_star] and x_i = [1, x[i]]\n        \n        harm_factor = np.zeros(n)\n        x_new_row = np.array([1, x_star])\n        for i in range(n):\n            x_i_row = np.array([1, x[i]])\n            harm_factor[i] = np.abs(x_new_row @ X_T_X_inv @ x_i_row)\n        \n        h_i = press_i_mag * harm_factor\n        argmax_harm = np.argmax(h_i)\n        \n        # Check if the argmax indices match\n        cook_equals_harm = (argmax_D == argmax_harm)\n        press_equals_harm = (argmax_press == argmax_harm)\n        \n        # Append the results for the current case\n        case_result = [\n            int(argmax_D),\n            int(argmax_press),\n            int(argmax_harm),\n            bool(cook_equals_harm),\n            bool(press_equals_harm)\n        ]\n        results.append(case_result)\n\n    # Final print statement in the exact required format.\n    # The string representation of a list of lists is the desired format.\n    print(str(results).replace(\" \", \"\"))\n\nsolve()\n```", "id": "3111497"}, {"introduction": "在实践中，数据很少是完美的，多重共线性是一个常见的挑战。本练习将探讨一个近似冗余的预测变量的存在，如何能极大地改变库克距离在观测点之间的分布，即便模型的整体预测几乎保持不变。这个动手模拟揭示了多重共线性会如何破坏影响度量的稳定性，这对于在真实世界建模场景中解读诊断统计量是至关重要的一课。[@problem_id:3111583]", "problem": "您的任务是实现一个数值实验，以说明在线性回归中，由于多重共线性而引入一个几乎冗余的预测变量，会如何显著地重新分配每个观测值的库克距离（Cook's distance），而拟合的预测值却几乎保持不变。您的实现必须是完全确定性的，并且必须为每个提供的测试用例计算一个布尔值。当且仅当以下两个条件同时成立时，该布尔值为真：拟合值的最大绝对变化很小，并且归一化的库克距离向量存在不可忽略的差异。\n\n从线性回归中的以下基本概念开始。考虑一个响应向量 $y \\in \\mathbb{R}^{n}$ 和一个满列秩的设计矩阵 $X \\in \\mathbb{R}^{n \\times p}$。普通最小二乘（OLS）估计量 $\\hat{\\beta} \\in \\mathbb{R}^{p}$ 最小化残差平方和，并满足正规方程。拟合值为 $\\hat{y} = X \\hat{\\beta}$，残差为 $e = y - \\hat{y}$。帽子矩阵为 $H = X (X^{\\top} X)^{-1} X^{\\top}$，其对角线元素为 $h_{ii}$。均方误差为 $\\operatorname{MSE} = \\|e\\|_{2}^{2} / (n - p)$。观测值 $i$ 的库克距离是衡量 $i$ 对所有拟合系数影响的度量，定义为移除第 $i$ 个观测值时 $\\hat{\\beta} - \\hat{\\beta}_{(i)}$ 的变化，等价地可以通过 $e$ 和 $h_{ii}$ 表示。\n\n您的程序必须基于这些基础，为两个模型实现拟合值和库克距离的计算：一个基础模型，其预测变量为 $\\{1, x_{1}, x_{2}\\}$；以及一个扩展模型，其预测变量为 $\\{1, x_{1}, x_{2}, x_{3}\\}$。在多重共线性场景中，$x_{3}$ 被构造成与 $x_{1}$ 几乎冗余。在非冗余场景中，$x_{3}$ 是独立的，并携带 $y$ 中的额外信号。\n\n对于每个测试用例，执行以下操作：\n- 使用固定的随机种子生成大小为 $n$ 的数据以确保可复现性。从标准正态分布中独立抽取 $x_{1}, x_{2}$。根据下面的案例描述构造 $x_{3}$。使用给定的系数和高斯噪声构造响应 $y$。始终在每个设计矩阵中包含一个全为1的截距列。\n- 使用普通最小二乘法拟合基础模型和扩展模型，以获得 $\\hat{y}_{\\mathrm{base}}$ 和 $\\hat{y}_{\\mathrm{ext}}$、残差 $e_{\\mathrm{base}}$ 和 $e_{\\mathrm{ext}}$，以及帽子矩阵对角线元素 $h_{ii}^{\\mathrm{base}}$ 和 $h_{ii}^{\\mathrm{ext}}$。\n- 计算每个模型中每个观测值的库克距离，然后通过除以各自的总和来归一化每个模型的库克距离，从而获得两个关于观测值的概率向量。如果总和为零，则该模型的归一化库克距离使用全零向量。\n- 将拟合值的最大绝对变化定义为 $\\Delta_{\\max} = \\max_{i} |\\hat{y}_{\\mathrm{ext}, i} - \\hat{y}_{\\mathrm{base}, i}|$。\n- 将重新分配幅度定义为归一化库克距离向量之间的 $\\ell_{1}$ 距离，即 $L_{1} = \\sum_{i=1}^{n} |d^{\\mathrm{ext}}_{i} - d^{\\mathrm{base}}_{i}|$。\n- 设小变化容差为 $\\varepsilon_{\\mathrm{pred}} = 0.05$（无单位，因为响应是抽象的）。设重新分配阈值为 $\\delta_{D} = 0.10$（无单位）。一个测试用例的布尔结果为真，当且仅当 $\\Delta_{\\max} \\le \\varepsilon_{\\mathrm{pred}}$ 且 $L_{1} \\ge \\delta_{D}$。\n\n实现三个测试用例，涵盖一个理想路径（happy path）、一个更强的多重共线性边界，以及一个对比的非冗余场景：\n- 测试用例 1（理想路径，几乎冗余的预测变量）：\n  - 种子 $= 42$，$n = 120$，系数 $(\\beta_{0}, \\beta_{1}, \\beta_{2}) = (0.5, 2.0, -1.5)$，噪声标准差 $\\sigma = 0.5$。\n  - 构造 $x_{3} = x_{1} + \\tau \\,\\eta$，其中 $\\eta \\sim \\mathcal{N}(0, 1)$ 独立于 $x_{1}$，且 $\\tau = 0.02$。\n  - 构造 $y = \\beta_{0} + \\beta_{1} x_{1} + \\beta_{2} x_{2} + \\varepsilon$，其中 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2})$ 且不直接依赖于 $x_{3}$。\n- 测试用例 2（边界，更强的近冗余性和更小的样本）：\n  - 种子 $= 7$，$n = 40$，系数 $(\\beta_{0}, \\beta_{1}, \\beta_{2}) = (1.0, 1.8, 0.7)$，噪声标准差 $\\sigma = 0.3$。\n  - 构造 $x_{3} = x_{1} + \\tau \\,\\eta$，其中 $\\tau = 0.005$。\n  - 构造 $y = \\beta_{0} + \\beta_{1} x_{1} + \\beta_{2} x_{2} + \\varepsilon$。\n- 测试用例 3（对比，带有额外信号的非冗余预测变量）：\n  - 种子 $= 99$，$n = 120$，系数 $(\\beta_{0}, \\beta_{1}, \\beta_{2}, \\beta_{3}) = (0.5, 2.0, -1.5, 1.5)$，噪声标准差 $\\sigma = 0.5$。\n  - 构造 $x_{3}$ 为一个独立的标准正态变量，独立于 $x_{1}$ 和 $x_{2}$。\n  - 构造 $y = \\beta_{0} + \\beta_{1} x_{1} + \\beta_{2} x_{2} + \\beta_{3} x_{3} + \\varepsilon$。\n\n您的程序必须按上述定义计算每个测试用例的布尔结果，并生成单行输出，其中包含用方括号括起来的逗号分隔的结果列表，无空格，例如 $[r_{1},r_{2},r_{3}]$，其中每个 $r_{k}$ 为 true 或 false。\n\n您的程序的最终输出必须是所描述格式的单一行。不允许用户输入，此问题不涉及物理单位；所有报告的量均为无单位量。角度单位和百分比在此不适用。您的代码必须是自包含的，并能使用提供的种子进行复现。预期输出是布尔值，实现必须严格遵守上述步骤。", "solution": "对用户提供的问题陈述进行验证。\n\n### 步骤 1：提取给定信息\n- **任务**：实现一个数值实验，以展示在线性回归中，一个几乎冗余的预测变量如何在最小化拟合值变化的同时影响库克距离。\n- **模型**：\n    - 基础模型，预测变量为 $\\{1, x_{1}, x_{2}\\}$。\n    - 扩展模型，预测变量为 $\\{1, x_{1}, x_{2}, x_{3}\\}$。\n- **核心公式**：\n    - 普通最小二乘（OLS）估计量：$\\hat{\\beta}$ 来自正规方程。\n    - 拟合值：$\\hat{y} = X \\hat{\\beta}$。\n    - 残差：$e = y - \\hat{y}$。\n    - 帽子矩阵：$H = X (X^{\\top} X)^{-1} X^{\\top}$，对角线元素为 $h_{ii}$。\n    - 均方误差（MSE）：$\\operatorname{MSE} = \\|e\\|_{2}^{2} / (n - p)$。\n    - 库克距离 $D_i$：给定为可通过 $e$ 和 $h_{ii}$ 表示。标准公式为 $D_i = \\frac{e_i^2}{p \\cdot \\operatorname{MSE}} \\left[ \\frac{h_{ii}}{(1 - h_{ii})^2} \\right]$。\n- **流程**：\n    1. 使用固定种子为样本大小 $n$ 生成数据（$x_1, x_2, x_3, y$）。始终包含截距列。\n    2. 拟合基础模型和扩展模型，以获取 $\\hat{y}_{\\mathrm{base}}$、$\\hat{y}_{\\mathrm{ext}}$ 以及用于计算库克距离的统计量。\n    3. 计算两个模型中每个观测值的库克距离。\n    4. 通过除以各自的总和，将每组库克距离归一化为概率向量 $d^{\\mathrm{base}}$ 和 $d^{\\mathrm{ext}}$。如果总和为零，则归一化向量为全零。\n    5. 计算 $\\Delta_{\\max} = \\max_{i} |\\hat{y}_{\\mathrm{ext}, i} - \\hat{y}_{\\mathrm{base}, i}|$。\n    6. 计算 $L_{1} = \\sum_{i=1}^{n} |d^{\\mathrm{ext}}_{i} - d^{\\mathrm{base}}_{i}|$。\n- **决策规则**：\n    - 当且仅当 $\\Delta_{\\max} \\le \\varepsilon_{\\mathrm{pred}}$ 且 $L_{1} \\ge \\delta_{D}$ 时，结果为 `true`。\n    - 阈值：$\\varepsilon_{\\mathrm{pred}} = 0.05$ 和 $\\delta_{D} = 0.10$。\n- **测试用例**：\n    - **案例 1**：种子=$42$，$n=120$，$(\\beta_0, \\beta_1, \\beta_2) = (0.5, 2.0, -1.5)$，$\\sigma=0.5$。$x_3 = x_1 + \\tau\\eta$，其中 $\\tau=0.02$。$y$ 由基础模型的预测变量生成。\n    - **案例 2**：种子=$7$，$n=40$，$(\\beta_0, \\beta_1, \\beta_2) = (1.0, 1.8, 0.7)$，$\\sigma=0.3$。$x_3 = x_1 + \\tau\\eta$，其中 $\\tau=0.005$。$y$ 由基础模型的预测变量生成。\n    - **案例 3**：种子=$99$，$n=120$，$(\\beta_0, \\beta_1, \\beta_2, \\beta_3) = (0.5, 2.0, -1.5, 1.5)$，$\\sigma=0.5$。$x_3 \\sim \\mathcal{N}(0,1)$ 是独立的。$y$ 由扩展模型的预测变量生成。\n- **输出格式**：单行 `[r1,r2,r3]`，其中 $r_k$ 为 `true` 或 `false`。\n\n### 步骤 2：使用提取的给定信息进行验证\n1.  **科学/事实合理性**：该问题在理论上根植于线性回归、多重共线性和影响诊断（库克距离）。所有定义和概念在统计学中都是标准的。\n2.  **可形式化性/相关性**：该问题是一个定义明确的计算任务，与理解统计学习中的影响诊断直接相关。\n3.  **完整性/一致性**：问题说明完整。它提供了所有必要的参数、数据生成过程、度量公式和决策阈值。没有矛盾之处。\n4.  **现实性/可行性**：该任务是一个数值模拟，在计算上是可行的。参数是合理的。在共线预测变量 $x_3$ 中引入噪声可确保设计矩阵保持满秩，从而防止奇异性问题。\n5.  **适定性**：问题是适定的。步骤是确定性的（给定种子），从而导向唯一的解。\n6.  **平凡性**：问题不平凡；它需要正确实现 OLS 回归及相关诊断，这涉及仔细的数值计算。它说明了回归分析中一个概念上很重要的点。\n7.  **可验证性**：通过使用给定的种子重新运行指定的算法，结果是可在计算上验证的。\n\n### 步骤 3：结论与行动\n问题是**有效的**。将提供一个解决方案。\n\n### 解决方案推导\n\n解决方案涉及按描述实现一个数值实验。实现的核心将是一个执行普通最小二乘（OLS）回归并计算必要统计数据的函数。为了数值稳定性，尤其是在存在多重共线性的情况下，将使用 QR 分解来获得 OLS 解。\n\n**1. OLS 拟合与诊断计算**\n给定设计矩阵 $X \\in \\mathbb{R}^{n \\times p}$ 和响应向量 $y \\in \\mathbb{R}^{n}$，OLS 估计量 $\\hat{\\beta}$ 最小化 $\\|y - X\\beta\\|_2^2$。\n设计矩阵 $X$ 被分解为 $X = QR$，其中 $Q \\in \\mathbb{R}^{n \\times p}$ 具有标准正交列（$Q^{\\top}Q = I_p$），而 $R \\in \\mathbb{R}^{p \\times p}$ 是一个上三角矩阵。\nOLS 估计量 $\\hat{\\beta}$ 通过求解稳定的三角系统 $R\\hat{\\beta} = Q^{\\top}y$ 来找到。\n然后按如下方式计算关键量：\n- **拟合值**：$\\hat{y} = X\\hat{\\beta}$。\n- **残差**：$e = y - \\hat{y}$。\n- **杠杆值**：帽子矩阵 $H = QQ^{\\top}$ 的对角元素 $h_{ii}$。对于每个观测值 $i$，杠杆值为 $h_{ii} = \\sum_{j=1}^{p} Q_{ij}^2$。\n- **均方误差（MSE）**：$\\operatorname{MSE} = \\frac{\\|e\\|_2^2}{n-p} = \\frac{\\sum_{i=1}^n e_i^2}{n-p}$。\n- **库克距离**：对于每个观测值 $i=1, \\dots, n$，库克距离使用以下公式计算：\n$$ D_i = \\frac{e_i^2}{p \\cdot \\operatorname{MSE}} \\cdot \\frac{h_{ii}}{(1 - h_{ii})^2} $$\n此处，$p$ 是预测变量的数量（包括截距），因此基础模型中 $p = 3$，扩展模型中 $p = 4$。\n\n**2. 数据生成与模型比较**\n对于每个测试用例，我们首先根据指定参数生成数据。\n- 预测变量 $x_1, x_2$ 和误差项 $\\varepsilon$ 从高斯分布中抽取。\n- 预测变量 $x_3$ 根据测试用例场景构造：或者与 $x_1$ 几乎共线（$x_3 = x_1 + \\tau\\eta$），或者作为一个独立的预测变量。\n- 响应向量 $y$ 被构造为预测变量的线性组合加上噪声。\n\n然后拟合两个模型：\n- **基础模型**：将 $y$ 对截距、$x_1$ 和 $x_2$ 进行回归。这会产生拟合值 $\\hat{y}_{\\mathrm{base}}$ 和库克距离 $D^{\\mathrm{base}}$。\n- **扩展模型**：将 $y$ 对截距、$x_1$、$x_2$ 和 $x_3$ 进行回归。这会产生 $\\hat{y}_{\\mathrm{ext}}$ 和 $D^{\\mathrm{ext}}$。\n\n**3. 度量计算与评估**\n计算以下两个度量以比较模型：\n- **拟合值的最大绝对变化**：\n$$ \\Delta_{\\max} = \\max_{i} |\\hat{y}_{\\mathrm{ext}, i} - \\hat{y}_{\\mathrm{base}, i}| $$\n这衡量了添加 $x_3$ 对模型预测的影响。\n- **影响力的重新分配**：首先，将每个模型的库克距离归一化，使其总和为 1：\n$$d_i = \\frac{D_i}{\\sum_{j=1}^n D_j}$$\n重新分配的程度通过两个归一化向量之间的 $\\ell_1$ 距离来量化：\n$$ L_{1} = \\sum_{i=1}^{n} |d^{\\mathrm{ext}}_{i} - d^{\\mathrm{base}}_{i}| $$\n$L_1$ 的可能范围是 $[0, 2]$。一个接近 0 的值意味着影响分布在模型间是相似的，而一个较大的值表示显著的重新分配。\n\n最后，对每个测试用例，评估布尔条件 $(\\Delta_{\\max} \\le 0.05) \\land (L_{1} \\ge 0.10)$。当引入多重共线性时（案例 1 和 2），该条件预期为真，因为几乎冗余的预测变量 $x_3$ 不会显著改变拟合值，但会使影响度量不稳定。当添加一个有意义的独立预测变量时（案例 3），该条件预期为假，因为这应该会显著改变（并改善）拟合值，从而使 $\\Delta_{\\max}$ 变大。\n\n实现将为每个测试用例精确遵循这些步骤，以生成最终的布尔结果列表。", "answer": "```python\nimport numpy as np\n\ndef fit_ols_and_get_stats(X, y):\n    \"\"\"\n    Fits an OLS model using QR decomposition for numerical stability and computes\n    fitted values and Cook's distances.\n\n    Args:\n        X (np.ndarray): Design matrix of shape (n, p).\n        y (np.ndarray): Response vector of shape (n,).\n\n    Returns:\n        tuple: A tuple containing:\n            - np.ndarray: Fitted values (y_hat).\n            - np.ndarray: Cook's distances for each observation.\n    \"\"\"\n    n, p = X.shape\n\n    # For robustness against collinearity, use QR decomposition to solve the least squares problem.\n    try:\n        Q, R = np.linalg.qr(X)\n        # Solve R @ beta_hat = Q.T @ y\n        qTy = Q.T @ y\n        beta_hat = np.linalg.solve(R, qTy)\n    except np.linalg.LinAlgError:\n        # Fallback for singular matrix, although problem setup should avoid this.\n        beta_hat = np.linalg.pinv(X) @ y\n        # Recalculate Q for leverage if pinv was used.\n        Q, R = np.linalg.qr(X)\n\n    y_hat = X @ beta_hat\n    residuals = y - y_hat\n    \n    # Leverages (diagonal of hat matrix H = Q @ Q.T) are sums of squares of Q's rows.\n    H_diag = np.sum(Q**2, axis=1)\n\n    # Mean Squared Error\n    if n > p:\n        rss = np.sum(residuals**2)\n        mse = rss / (n - p)\n    else:\n        mse = 0.0\n\n    # Cook's Distances\n    cooks_d = np.zeros(n)\n    if mse > 1e-15:  # Avoid division by zero if it's a perfect fit\n        # Formula: D_i = (e_i^2 / (p * MSE)) * (h_ii / (1 - h_ii)^2)\n        term1 = (residuals**2) / (p * mse)\n        denom = (1 - H_diag)**2\n        # Use np.divide to handle cases where denom is zero (i.e., h_ii=1)\n        # to prevent inf/nan values, which would disrupt normalization.\n        term2 = np.divide(H_diag, denom, out=np.zeros_like(H_diag), where=denom > 1e-15)\n        cooks_d = term1 * term2\n            \n    return y_hat, cooks_d\n\ndef process_case(seed, n, y_coeffs, sigma, tau, case_type):\n    \"\"\"\n    Runs one full test case: generates data, fits models, computes metrics, and returns the boolean result.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    \n    # 1. Generate data\n    x1 = rng.standard_normal(n)\n    x2 = rng.standard_normal(n)\n    \n    if case_type == 'collinear':\n        eta = rng.standard_normal(n)\n        x3 = x1 + tau * eta\n        y = y_coeffs[0] + y_coeffs[1] * x1 + y_coeffs[2] * x2 + rng.normal(0, sigma, n)\n    elif case_type == 'non-redundant':\n        x3 = rng.standard_normal(n)\n        y = y_coeffs[0] + y_coeffs[1] * x1 + y_coeffs[2] * x2 + y_coeffs[3] * x3 + rng.normal(0, sigma, n)\n    \n    intercept = np.ones(n)\n    X_base = np.column_stack([intercept, x1, x2])\n    X_ext = np.column_stack([intercept, x1, x2, x3])\n    \n    # 2. Fit base and extended models\n    y_hat_base, cooks_d_base = fit_ols_and_get_stats(X_base, y)\n    y_hat_ext, cooks_d_ext = fit_ols_and_get_stats(X_ext, y)\n    \n    # 3. Calculate metrics\n    # Max absolute change in fitted values\n    delta_max = np.max(np.abs(y_hat_ext - y_hat_base))\n    \n    # L1 distance between normalized Cook's distances\n    sum_cooks_base = np.sum(cooks_d_base)\n    sum_cooks_ext = np.sum(cooks_d_ext)\n    \n    norm_cooks_base = cooks_d_base / sum_cooks_base if sum_cooks_base > 0 else np.zeros_like(cooks_d_base)\n    norm_cooks_ext = cooks_d_ext / sum_cooks_ext if sum_cooks_ext > 0 else np.zeros_like(cooks_d_ext)\n    \n    l1_dist = np.sum(np.abs(norm_cooks_ext - norm_cooks_base))\n    \n    # 4. Apply decision rule\n    epsilon_pred = 0.05\n    delta_D = 0.10\n    \n    condition_fit = delta_max = epsilon_pred\n    condition_dist = l1_dist >= delta_D\n    \n    return condition_fit and condition_dist\n\ndef solve():\n    \"\"\"\n    Main function to define test cases, run them, and print results.\n    \"\"\"\n    test_cases = [\n        # Case 1: Happy path, nearly redundant predictor\n        {'seed': 42, 'n': 120, 'y_coeffs': (0.5, 2.0, -1.5), 'sigma': 0.5, 'tau': 0.02, 'case_type': 'collinear'},\n        # Case 2: Boundary, stronger near-redundancy and smaller sample\n        {'seed': 7, 'n': 40, 'y_coeffs': (1.0, 1.8, 0.7), 'sigma': 0.3, 'tau': 0.005, 'case_type': 'collinear'},\n        # Case 3: Contrast, non-redundant predictor with additional signal\n        {'seed': 99, 'n': 120, 'y_coeffs': (0.5, 2.0, -1.5, 1.5), 'sigma': 0.5, 'tau': None, 'case_type': 'non-redundant'}\n    ]\n    \n    results = []\n    for case_params in test_cases:\n        result = process_case(**case_params)\n        results.append(str(result).lower())\n        \n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3111583"}]}