{"hands_on_practices": [{"introduction": "在我们能够有意义地估计一条回归线之前，必须满足一个基本前提。这个练习探讨了这个基础：预测变量的变异性。通过分析当预测变量 $X$ 没有变化时的后果，你将理解为何从理论上无法唯一确定斜率，以及在实践中为何估计会变得极不稳定 [@problem_id:3159699]。", "problem": "考虑一个数据生成过程，其中标量预测变量 $X$ 和标量响应变量 $Y$ 满足 $Y=\\beta_{0}+\\beta_{1}X+\\varepsilon$，其中 $\\mathbb{E}[\\varepsilon\\mid X]=0$ 且 $\\operatorname{Var}(\\varepsilon\\mid X)=\\sigma^{2}\\in(0,\\infty)$，并且 $X=a+\\delta Z$，其中 $a\\in\\mathbb{R}$，$\\delta\\ge 0$，$Z$ 是一个均值为零、单位方差且与 $\\varepsilon$ 独立的随机变量。因此 $\\operatorname{Var}(X)=\\delta^{2}$，这个值可以非常小，甚至等于 $0$。您收集了 $n$ 个独立同分布的观测值 $\\{(X_{i},Y_{i})\\}_{i=1}^{n}$，并在可能的情况下对样本拟合普通最小二乘（OLS）直线。\n\n选择所有关于总体回归线与样本最小二乘线之间的关系、可识别性以及当 $\\operatorname{Var}(X)$ 变小时抽样变异性的正确陈述。\n\nA. 对于任何 $\\delta>0$，总体回归函数都存在，并且是直线 $\\mathbb{E}[Y\\mid X]=\\beta_{0}+\\beta_{1}X$。\n\nB. 当 $\\delta=0$ 时（因此 $\\operatorname{Var}(X)=0$ 且 $X\\equiv a$ 几乎必然成立），斜率 $\\beta_{1}$ 无法从 $(X,Y)$ 的联合分布中识别；只有值 $\\beta_{0}+\\beta_{1}a=\\mathbb{E}[Y\\mid X=a]$ 是可识别的。\n\nC. 对于任何 $\\delta>0$，OLS斜率估计量是 $\\beta_{1}$ 的无偏估计，其抽样方差与 $1/\\operatorname{Var}(X)$ 成比例，因此在 $n$ 固定的情况下，随着 $\\operatorname{Var}(X)\\to 0$，它会发散。\n\nD. 将 $X$ 越来越紧密地集中在 $a$ 附近（减小 $\\operatorname{Var}(X)$）会减小OLS斜率的抽样方差，因为预测变量更相似。\n\nE. 如果所有的 $X_{i}$ 都完全等于同一个常数 $a$（因此观测样本的 $\\sum_{i=1}^{n}(X_{i}-\\bar{X})^{2}=0$），那么OLS斜率估计量不是由最小二乘准则唯一确定的；在这种情况下，斜率无法从数据中恢复。", "solution": "用户希望我验证问题陈述，如果有效，则提供详细的解决方案。\n\n### 问题验证\n\n**步骤1：提取已知条件**\n\n问题陈述提供了以下信息：\n-   一个数据生成过程由模型 $Y = \\beta_{0} + \\beta_{1}X + \\varepsilon$ 定义，其中 $X$ 是一个标量预测变量，$Y$ 是一个标量响应变量。\n-   误差项 $\\varepsilon$ 满足条件均值独立性假设：$\\mathbb{E}[\\varepsilon \\mid X] = 0$。\n-   误差项具有恒定的条件方差（同方差性）：$\\operatorname{Var}(\\varepsilon \\mid X) = \\sigma^{2}$，其中 $\\sigma^{2} \\in (0, \\infty)$。\n-   预测变量 $X$ 由模型 $X = a + \\delta Z$ 生成，其中 $a \\in \\mathbb{R}$，$\\delta \\ge 0$，$Z$ 是一个随机变量，满足 $\\mathbb{E}[Z]=0$ 和 $\\operatorname{Var}(Z)=1$。\n-   随机变量 $Z$ 与误差项 $\\varepsilon$ 独立。\n-   从 $X$ 的模型可知，其方差为 $\\operatorname{Var}(X) = \\operatorname{Var}(a + \\delta Z) = \\delta^2 \\operatorname{Var}(Z) = \\delta^2$。\n-   从这个过程中收集了一个包含 $n$ 个独立同分布（i.i.d.）观测值的样本，$\\{(X_{i}, Y_{i})\\}_{i=1}^{n}$。\n-   对该样本拟合了一条普通最小二乘（OLS）直线。\n-   任务是评估关于总体回归线、样本最小二乘线、参数可识别性以及当 $\\operatorname{Var}(X)$ 趋近于 $0$ 时的抽样变异性的陈述。\n\n**步骤2：使用提取的已知条件进行验证**\n\n-   **科学依据**：该问题描述了一个标准的简单线性回归模型。所有假设（$\\mathbb{E}[\\varepsilon \\mid X]=0$、恒定方差、独立同分布抽样）都是统计学入门和高等理论的基础内容。该设置在科学和数学上是合理的。\n-   **提法得当**：问题提法得当。它提出了一个清晰的理论模型，并要求基于此模型评估几个命题。这些问题涉及基本概念，如总体回归函数的定义、参数可识别性以及OLS估计量的性质，所有这些在统计理论中都有明确的答案。\n-   **客观性**：语言客观、正式、精确，没有模糊性和主观性。\n\n**缺陷清单：**\n1.  **科学或事实上的不合理**：无。该模型是统计分析的基石。\n2.  **不可形式化或不相关**：无。该问题是一个正式的统计理论练习。\n3.  **不完整或矛盾的设置**：无。对于所问的问题，模型是完全指定的。包含 $\\delta=0$ 的情况（以及 $\\delta \\to 0$ 的极限）是故意的，并且对于正在测试的概念点至关重要。\n4.  **不现实或不可行**：无。这是一个旨在探究对回归原理理解的理论模型。预测变量方差为零的极限是理解多重共线性和估计量稳定性的一个关键概念。\n5.  **病态的或结构不良的**：无。待评估的陈述允许根据所提供的模型得出明确、唯一的判断（正确或不正确）。\n6.  **伪深刻、琐碎或同义反复**：无。该问题探讨了回归分析中非琐碎且重要的概念，特别是预测变量方差的作用。\n7.  **超出科学可验证性**：无。所有主张都可以通过数学和统计推导进行验证。\n\n**步骤3：结论与行动**\n\n问题陈述是有效的。这是一个关于简单线性回归理论性质的、表述良好的问题。我将继续进行解答。\n\n### 解答推导\n\n该问题要求分析关于一个简单线性回归模型的五个陈述，重点关注减小预测变量X方差的影响。\n\n**A. 对于任何 $\\delta>0$，总体回归函数都存在，并且是直线 $\\mathbb{E}[Y\\mid X]=\\beta_{0}+\\beta_{1}X$。**\n\n总体回归函数定义为响应变量 $Y$ 对预测变量 $X$ 的条件期望，即 $\\mathbb{E}[Y \\mid X]$。\n使用给定的模型 $Y = \\beta_0 + \\beta_1 X + \\varepsilon$，我们可以计算这个期望：\n$$ \\mathbb{E}[Y \\mid X] = \\mathbb{E}[\\beta_{0} + \\beta_{1}X + \\varepsilon \\mid X] $$\n根据期望的线性性质：\n$$ \\mathbb{E}[Y \\mid X] = \\mathbb{E}[\\beta_{0} \\mid X] + \\mathbb{E}[\\beta_{1}X \\mid X] + \\mathbb{E}[\\varepsilon \\mid X] $$\n由于 $\\beta_{0}$ 和 $\\beta_{1}$ 是常数，且在条件期望中 $X$ 是已知的，所以 $\\mathbb{E}[\\beta_{0} \\mid X] = \\beta_{0}$ 且 $\\mathbb{E}[\\beta_{1}X \\mid X] = \\beta_{1}X$。\n问题陈述中说明 $\\mathbb{E}[\\varepsilon \\mid X] = 0$。代入这些结果得到：\n$$ \\mathbb{E}[Y \\mid X] = \\beta_{0} + \\beta_{1}X + 0 = \\beta_{0} + \\beta_{1}X $$\n这个方程定义了 $(X, Y)$ 平面中的一条直线。只要模型 $Y = \\beta_0 + \\beta_1 X + \\varepsilon$ 且 $\\mathbb{E}[\\varepsilon|X]=0$ 成立，这个推导就有效，无论 $X$ 的分布如何，只要 $X$ 是一个随机变量。条件 $\\delta > 0$ 确保了 $\\operatorname{Var}(X) > 0$，所以 $X$ 不是一个常数。因此，总体回归函数存在并且就是这条直线。\n\n**结论：正确。**\n\n**B. 当 $\\delta=0$ 时（因此 $\\operatorname{Var}(X)=0$ 且 $X\\equiv a$ 几乎必然成立），斜率 $\\beta_{1}$ 无法从 $(X,Y)$ 的联合分布中识别；只有值 $\\beta_{0}+\\beta_{1}a=\\mathbb{E}[Y\\mid X=a]$ 是可识别的。**\n\n如果 $\\delta=0$，那么 $X$ 的模型变为 $X = a + 0 \\cdot Z = a$。这意味着 $X$ 是一个常数，固定为值 $a$。$(X,Y)$ 的整个联合分布都集中在垂直线 $x=a$ 上。\n从这个分布中，关于 $X$ 和 $Y$ 之间关系的唯一可观测信息是当 $X=a$ 时 $Y$ 的分布。具体来说，我们可以识别给定 $X=a$ 时 $Y$ 的均值，即 $\\mathbb{E}[Y \\mid X=a]$。\n从 A 中的一般结果，我们有：\n$$ \\mathbb{E}[Y \\mid X=a] = \\beta_{0} + \\beta_{1}a $$\n令 $c = \\mathbb{E}[Y \\mid X=a]$。来自联合分布的信息确定了线性组合 $\\beta_{0} + \\beta_{1}a = c$ 的值。但是，它没有提供进一步的约束来单独求解 $\\beta_{0}$ 和 $\\beta_{1}$。对于任何任意选择的斜率，比如 $\\beta_{1}^*$，我们都可以找到一个对应的截距 $\\beta_{0}^* = c - \\beta_{1}^*a$，使得这对 $(\\beta_{0}^*, \\beta_{1}^*)$ 与观测数据一致。例如，如果我们假设 $\\beta_{1}=0$，那么 $\\beta_{0}=c$。如果我们假设 $\\beta_{1}=1$，那么 $\\beta_{0}=c-a$。有无数对 $(\\beta_0, \\beta_1)$ 满足这一个方程。因此，$\\beta_1$ 是不可识别的。这是预测变量和截距列（一列全为1）之间完全多重共线性的经典问题。\n\n**结论：正确。**\n\n**C. 对于任何 $\\delta>0$，OLS斜率估计量是 $\\beta_{1}$ 的无偏估计，其抽样方差与 $1/\\operatorname{Var}(X)$ 成比例，因此在 $n$ 固定的情况下，随着 $\\operatorname{Var}(X)\\to 0$，它会发散。**\n\n对于一个样本 $\\{(X_{i}, Y_{i})\\}_{i=1}^{n}$，OLS斜率估计量由 $\\hat{\\beta_1} = \\frac{\\sum_{i=1}^{n}(X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sum_{i=1}^{n}(X_i - \\bar{X})^2}$ 给出。\n代入 $Y_i = \\beta_0 + \\beta_1 X_i + \\varepsilon_i$：\n$$ \\hat{\\beta_1} = \\beta_1 + \\frac{\\sum_{i=1}^{n}(X_i - \\bar{X})\\varepsilon_i}{\\sum_{i=1}^{n}(X_i - \\bar{X})^2} $$\n为了检验无偏性，我们计算 $\\hat{\\beta_1}$ 的期望。以 $X_1, \\dots, X_n$ 的值为条件：\n$$ \\mathbb{E}[\\hat{\\beta_1} \\mid X_1, \\dots, X_n] = \\beta_1 + \\frac{\\sum_{i=1}^{n}(X_i - \\bar{X})\\mathbb{E}[\\varepsilon_i \\mid X_1, \\dots, X_n]}{\\sum_{i=1}^{n}(X_i - \\bar{X})^2} $$\n因为 $\\varepsilon$ 与 $Z$ 独立（因此与所有 $X_i$ 独立），所以 $\\mathbb{E}[\\varepsilon_i \\mid X_1, \\dots, X_n] = \\mathbb{E}[\\varepsilon_i] = \\mathbb{E}[\\mathbb{E}[\\varepsilon_i \\mid X_i]] = \\mathbb{E}[0] = 0$。\n因此，$\\mathbb{E}[\\hat{\\beta_1} \\mid X_1, \\dots, X_n] = \\beta_1$。根据全期望定律，$\\mathbb{E}[\\hat{\\beta_1}] = \\mathbb{E}[\\mathbb{E}[\\hat{\\beta_1} \\mid X_1, \\dots, X_n]] = \\beta_1$。如果估计量有良好定义（在 $\\delta > 0$ 且 $n \\ge 2$ 的情况下，这以概率1成立），那么它是无偏的。\n\n对于方差，我们首先求条件方差：\n$$ \\operatorname{Var}(\\hat{\\beta_1} \\mid X_1, \\dots, X_n) = \\operatorname{Var}\\left(\\frac{\\sum_{i=1}^{n}(X_i - \\bar{X})\\varepsilon_i}{\\sum_{i=1}^{n}(X_i - \\bar{X})^2} \\mid X_1, \\dots, X_n\\right) = \\frac{\\sum_{i=1}^{n}(X_i - \\bar{X})^2 \\operatorname{Var}(\\varepsilon_i \\mid X_1, \\dots, X_n)}{(\\sum_{i=1}^{n}(X_i - \\bar{X})^2)^2} $$\n在 $\\varepsilon_i$ 独立和同方差性 $\\operatorname{Var}(\\varepsilon_i \\mid X_1, \\dots, X_n) = \\sigma^2$ 的条件下：\n$$ \\operatorname{Var}(\\hat{\\beta_1} \\mid X_1, \\dots, X_n) = \\frac{\\sigma^2 \\sum_{i=1}^{n}(X_i - \\bar{X})^2}{(\\sum_{i=1}^{n}(X_i - \\bar{X})^2)^2} = \\frac{\\sigma^2}{\\sum_{i=1}^{n}(X_i - \\bar{X})^2} $$\n无条件方差是 $\\operatorname{Var}(\\hat{\\beta_1})=\\mathbb{E}[\\frac{\\sigma^2}{\\sum (X_i-\\bar{X})^2}]$。我们来分析分母。$X_i = a + \\delta Z_i$，所以 $X_i - \\bar{X} = \\delta(Z_i - \\bar{Z})$。\n$$ \\sum_{i=1}^{n}(X_i - \\bar{X})^2 = \\sum_{i=1}^{n} \\delta^2(Z_i - \\bar{Z})^2 = \\delta^2 \\sum_{i=1}^{n}(Z_i - \\bar{Z})^2 = \\operatorname{Var}(X) \\sum_{i=1}^{n}(Z_i - \\bar{Z})^2 $$\n令 $W = \\sum_{i=1}^{n}(Z_i - \\bar{Z})^2$。这是一个随机变量，它依赖于 $n$ 和 $Z$ 的分布，但不依赖于 $\\delta$。对于 $n \\ge 2$，$W>0$ 几乎必然成立。\n无条件方差是：\n$$ \\operatorname{Var}(\\hat{\\beta_1}) = \\sigma^2 \\mathbb{E}\\left[\\frac{1}{\\delta^2 W}\\right] = \\frac{\\sigma^2}{\\delta^2} \\mathbb{E}\\left[\\frac{1}{W}\\right] = \\frac{1}{\\operatorname{Var}(X)} \\left( \\sigma^2 \\mathbb{E}\\left[\\frac{1}{W}\\right] \\right) $$\n对于固定的 $n$，括号中的项是一个正常数。因此，$\\hat{\\beta_1}$ 的抽样方差与 $\\operatorname{Var}(X)$ 成反比。随着 $\\operatorname{Var}(X) \\to 0$，$\\hat{\\beta_1}$ 的方差发散到无穷大。\n\n**结论：正确。**\n\n**D. 将 $X$ 越来越紧密地集中在 $a$ 附近（减小 $\\operatorname{Var}(X)$）会减小OLS斜率的抽样方差，因为预测变量更相似。**\n\n这个陈述声称减小 $\\operatorname{Var}(X)$ 会导致 $\\hat{\\beta}_1$ 抽样方差的*减小*。如在对陈述C的分析中所证明的，抽样方差 $\\operatorname{Var}(\\hat{\\beta_1})$ 与 $\\operatorname{Var}(X)$ 成反比。\n$$ \\operatorname{Var}(\\hat{\\beta_1}) \\propto \\frac{1}{\\operatorname{Var}(X)} $$\n因此，减小 $\\operatorname{Var}(X)$ 会*增加*OLS斜率估计量的抽样方差。直观地说，预测变量值的较小散布为估计斜率提供了一个不太稳定的“杠杆臂”，使得估计对随机噪声 $\\varepsilon$ 更敏感。“因为预测变量更相似”这一前提是正确的，但它导出了相反的结论：更相似的预测变量使得估计斜率更加困难，因此方差增加。\n\n**结论：不正确。**\n\n**E. 如果所有的 $X_{i}$ 都完全等于同一个常数 $a$（因此观测样本的 $\\sum_{i=1}^{n}(X_{i}-\\bar{X})^{2}=0$），那么OLS斜率估计量不是由最小二乘准则唯一确定的；在这种情况下，斜率无法从数据中恢复。**\n\n这个陈述描述了陈述B中情况在样本层面的后果。如果所有观测到的 $X_i$ 都等于一个常数 $a$，那么样本均值为 $\\bar{X}=a$。OLS斜率估计量的分母是：\n$$ \\sum_{i=1}^{n}(X_i - \\bar{X})^2 = \\sum_{i=1}^{n}(a-a)^2 = 0 $$\n$\\hat{\\beta}_1$ 的公式涉及除以零，使其无定义。为了理解原因，我们考察最小二乘准则，即最小化残差平方和：\n$$ S(\\beta_0, \\beta_1) = \\sum_{i=1}^{n} (Y_i - \\beta_0 - \\beta_1 X_i)^2 $$\n将所有 $i$ 的 $X_i=a$ 代入：\n$$ S(\\beta_0, \\beta_1) = \\sum_{i=1}^{n} (Y_i - (\\beta_0 + \\beta_1 a))^2 $$\n$S$ 的值仅依赖于线性组合 $\\gamma = \\beta_0 + \\beta_1 a$。当 $\\gamma$ 是 $Y_i$ 值的样本均值时，即 $\\gamma = \\bar{Y}$，平方和被最小化。\n这意味着任何满足方程 $\\beta_0 + \\beta_1 a = \\bar{Y}$ 的 $(\\beta_0, \\beta_1)$ 对都是一个有效的最小二乘解。不存在唯一的 $\\beta_1$ 值可以最小化误差平方和。我们可以选择任意斜率 $\\beta_1$，并找到一个满足该准则的对应截距 $\\beta_0 = \\bar{Y} - \\beta_1 a$。从几何上看，任何穿过点 $(a, \\bar{Y})$ 的直线对于所有位于垂直线 $x=a$ 上的数据点都是同样好的拟合。因此，斜率无法从数据中恢复。\n\n**结论：正确。**", "answer": "$$\\boxed{ABCE}$$", "id": "3159699"}, {"introduction": "当我们的模型假设与数据的真实生成过程不符时会发生什么？这个练习通过一个拟合线性模型于二次曲线数据的思想实验，揭示了模型设定不当所导致的系统性偏差。你将推导出“最佳线性近似”的斜率，并理解它为何会偏离真实的线性参数，以及这种偏差如何依赖于预测变量的分布 [@problem_id:3159615]。", "problem": "考虑一个实值预测变量 $X$ 和响应变量 $Y$ 的联合分布，其满足以下数据生成过程\n$$\nY \\;=\\; \\beta_{0} \\;+\\; \\beta_{1} X \\;+\\; \\beta_{2} X^{2} \\;+\\; \\varepsilon,\n$$\n其中 $\\mathbb{E}[\\varepsilon \\mid X]=0$ 且 $\\mathbb{E}[\\varepsilon^{2}]  \\infty$。因此，总体回归函数（总体二次目标）为 $m(x)=\\mathbb{E}[Y \\mid X=x]=\\beta_{0}+\\beta_{1} x+\\beta_{2} x^{2}$。假设我们将注意力限制在线性预测器 $f(x)=a+bx$ 上，并将总体线性目标定义为期望平方损失 $\\mathbb{E}\\big[(Y-a-bX)^{2}\\big]$ 的最小化者 $(a^{\\star},b^{\\star})$。令 $\\mu_{k}=\\mathbb{E}[X^{k}]$（对于 $k\\in\\{1,2,3\\}$），并假设 $\\operatorname{Var}(X)=\\mu_{2}-\\mu_{1}^{2}0$。\n\n任务：\n- 仅使用总体线性目标作为 $\\mathbb{E}\\big[(Y-a-bX)^{2}\\big]$ 最小化者的定义，推导出最优斜率 $b^{\\star}$ 的一个闭式表达式，用 $\\beta_{1}$、$\\beta_{2}$、$\\mu_{1}$、$\\mu_{2}$ 和 $\\mu_{3}$ 表示。由此，求出斜率偏差 $b^{\\star}-\\beta_{1}$，并通过 $\\mu_{1}$、$\\mu_{2}$ 和 $\\mu_{3}$ 将其用 $X$ 的分布表示。\n- 将您的表达式应用于 $X$ 的分布关于 $0$ 对称的特殊情况，此时 $\\mu_{1}=0$ 且 $\\mu_{3}=0$，并陈述此情况下得到的 $b^{\\star}$（最终答案无需计算过程）。\n- 最后，对于 $X \\sim \\mathrm{Uniform}[0,1]$ 且参数为 $\\beta_{0}=1$、$\\beta_{1}=2$ 和 $\\beta_{2}=3$ 的具体情况，计算 $b^{\\star}$ 的值。请提供 $b^{\\star}$ 的精确值作为您的最终答案（不要四舍五入）。", "solution": "本题要求推导一个线性预测器对二次生成响应的最优斜率 $b^{\\star}$，并在一个具体案例中计算其值。\n\n首先，我们验证问题陈述的有效性。\n给定条件如下：\n- 数据生成过程：$Y = \\beta_{0} + \\beta_{1} X + \\beta_{2} X^{2} + \\varepsilon$。\n- 误差条件：$\\mathbb{E}[\\varepsilon \\mid X]=0$ 且 $\\mathbb{E}[\\varepsilon^{2}]  \\infty$。\n- 总体二次目标：$m(x)=\\mathbb{E}[Y \\mid X=x]=\\beta_{0}+\\beta_{1} x+\\beta_{2} x^{2}$。\n- 线性预测器：$f(x)=a+bx$。\n- 总体线性目标：$(a^{\\star},b^{\\star}) = \\arg\\min_{a,b} \\mathbb{E}\\big[(Y-a-bX)^{2}\\big]$。\n- $X$ 的矩：$\\mu_{k}=\\mathbb{E}[X^{k}]$（对于 $k\\in\\{1,2,3\\}$）。\n- 方差条件：$\\operatorname{Var}(X)=\\mu_{2}-\\mu_{1}^{2}0$。\n\n该问题在科学上植根于统计回归理论，特别是分析由模型设定错误引起的偏差。这是一个适定问题，因为在 $\\operatorname{Var}(X)0$ 的条件下，对一个凸函数进行最小化可以保证解的唯一性。其语言客观，设定自洽。因此，该问题被视为有效。\n\n我们现在开始求解。总体线性目标参数 $(a^{\\star}, b^{\\star})$ 是使期望平方误差损失函数 $L(a,b) = \\mathbb{E}[(Y-a-bX)^2]$ 最小化的 $a$ 和 $b$ 的值。为了找到最小值，我们对 $L(a,b)$ 分别求关于 $a$ 和 $b$ 的偏导数，并将其设为零。\n\n关于 $a$ 的偏导数为：\n$$\n\\frac{\\partial L}{\\partial a} = \\frac{\\partial}{\\partial a} \\mathbb{E}[(Y-a-bX)^2] = \\mathbb{E}\\left[\\frac{\\partial}{\\partial a}(Y-a-bX)^2\\right] = \\mathbb{E}[-2(Y-a-bX)]\n$$\n将其设为零：\n$$\n\\mathbb{E}[Y - a^{\\star} - b^{\\star}X] = 0 \\implies \\mathbb{E}[Y] - a^{\\star} - b^{\\star}\\mathbb{E}[X] = 0\n$$\n这给出了第一个正规方程，它关联了 $a^{\\star}$ 和 $b^{\\star}$：\n$$\na^{\\star} = \\mathbb{E}[Y] - b^{\\star}\\mathbb{E}[X]\n$$\n\n关于 $b$ 的偏导数为：\n$$\n\\frac{\\partial L}{\\partial b} = \\frac{\\partial}{\\partial b} \\mathbb{E}[(Y-a-bX)^2] = \\mathbb{E}\\left[\\frac{\\partial}{\\partial b}(Y-a-bX)^2\\right] = \\mathbb{E}[-2X(Y-a-bX)]\n$$\n将其设为零可得到第二个正规方程：\n$$\n\\mathbb{E}[X(Y - a^{\\star} - b^{\\star}X)] = 0 \\implies \\mathbb{E}[XY] - a^{\\star}\\mathbb{E}[X] - b^{\\star}\\mathbb{E}[X^2] = 0\n$$\n将第一个方程中 $a^{\\star}$ 的表达式代入第二个方程：\n$$\n\\mathbb{E}[XY] - (\\mathbb{E}[Y] - b^{\\star}\\mathbb{E}[X])\\mathbb{E}[X] - b^{\\star}\\mathbb{E}[X^2] = 0\n$$\n$$\n\\mathbb{E}[XY] - \\mathbb{E}[Y]\\mathbb{E}[X] + b^{\\star}(\\mathbb{E}[X])^2 - b^{\\star}\\mathbb{E}[X^2] = 0\n$$\n项 $\\mathbb{E}[XY] - \\mathbb{E}[X]\\mathbb{E}[Y]$ 是协方差 $\\operatorname{Cov}(X,Y)$。项 $\\mathbb{E}[X^2] - (\\mathbb{E}[X])^2$ 是方差 $\\operatorname{Var}(X)$。方程变为：\n$$\n\\operatorname{Cov}(X,Y) - b^{\\star}\\operatorname{Var}(X) = 0\n$$\n解出 $b^{\\star}$，我们得到众所周知的结果：\n$$\nb^{\\star} = \\frac{\\operatorname{Cov}(X,Y)}{\\operatorname{Var}(X)}\n$$\n题目要求将此结果用 $\\beta_1$、$\\beta_2$ 和 $X$ 的矩来表示。我们必须计算 $\\operatorname{Cov}(X,Y)$。首先，我们求出 $\\mathbb{E}[Y]$ 和 $\\mathbb{E}[XY]$。根据全期望定律和给定条件 $\\mathbb{E}[\\varepsilon \\mid X]=0$，我们有 $\\mathbb{E}[\\varepsilon] = \\mathbb{E}[\\mathbb{E}[\\varepsilon \\mid X]] = \\mathbb{E}[0] = 0$。\n$$\n\\mathbb{E}[Y] = \\mathbb{E}[\\beta_0 + \\beta_1 X + \\beta_2 X^2 + \\varepsilon] = \\beta_0 + \\beta_1 \\mathbb{E}[X] + \\beta_2 \\mathbb{E}[X^2] + \\mathbb{E}[\\varepsilon] = \\beta_0 + \\beta_1 \\mu_1 + \\beta_2 \\mu_2\n$$\n类似地，我们计算 $\\mathbb{E}[XY]$。首先，注意到 $\\mathbb{E}[X\\varepsilon] = \\mathbb{E}[\\mathbb{E}[X\\varepsilon \\mid X]] = \\mathbb{E}[X\\mathbb{E}[\\varepsilon \\mid X]] = \\mathbb{E}[X \\cdot 0] = 0$。\n$$\n\\mathbb{E}[XY] = \\mathbb{E}[X(\\beta_0 + \\beta_1 X + \\beta_2 X^2 + \\varepsilon)] = \\mathbb{E}[\\beta_0 X + \\beta_1 X^2 + \\beta_2 X^3 + X\\varepsilon]\n$$\n$$\n\\mathbb{E}[XY] = \\beta_0 \\mathbb{E}[X] + \\beta_1 \\mathbb{E}[X^2] + \\beta_2 \\mathbb{E}[X^3] + \\mathbb{E}[X\\varepsilon] = \\beta_0 \\mu_1 + \\beta_1 \\mu_2 + \\beta_2 \\mu_3\n$$\n现在我们计算协方差：\n$$\n\\operatorname{Cov}(X,Y) = \\mathbb{E}[XY] - \\mathbb{E}[X]\\mathbb{E}[Y] = (\\beta_0 \\mu_1 + \\beta_1 \\mu_2 + \\beta_2 \\mu_3) - \\mu_1(\\beta_0 + \\beta_1 \\mu_1 + \\beta_2 \\mu_2)\n$$\n$$\n\\operatorname{Cov}(X,Y) = \\beta_0 \\mu_1 + \\beta_1 \\mu_2 + \\beta_2 \\mu_3 - \\beta_0 \\mu_1 - \\beta_1 \\mu_1^2 - \\beta_2 \\mu_1 \\mu_2\n$$\n$$\n\\operatorname{Cov}(X,Y) = \\beta_1 (\\mu_2 - \\mu_1^2) + \\beta_2 (\\mu_3 - \\mu_1 \\mu_2)\n$$\n$X$ 的方差是 $\\operatorname{Var}(X) = \\mu_2 - \\mu_1^2$。将这些代入 $b^{\\star}$ 的表达式：\n$$\nb^{\\star} = \\frac{\\beta_1 (\\mu_2 - \\mu_1^2) + \\beta_2 (\\mu_3 - \\mu_1 \\mu_2)}{\\mu_2 - \\mu_1^2} = \\beta_1 + \\beta_2 \\frac{\\mu_3 - \\mu_1 \\mu_2}{\\mu_2 - \\mu_1^2}\n$$\n这就是所求的 $b^{\\star}$ 的闭式表达式。斜率偏差是这个最优线性斜率与真实线性系数 $\\beta_1$ 之间的差值：\n$$\n\\text{斜率偏差} = b^{\\star} - \\beta_1 = \\beta_2 \\frac{\\mu_3 - \\mu_1 \\mu_2}{\\mu_2 - \\mu_1^2}\n$$\n这种偏差是模型设定错误的结果，即用线性模型去拟合由二次过程生成的数据。如果 $\\beta_2=0$（真实模型是线性的），或者项 $\\mu_3 - \\mu_1 \\mu_2 = \\mathbb{E}[X^3] - \\mathbb{E}[X]\\mathbb{E}[X^2] = \\operatorname{Cov}(X, X^2)$ 为零，则偏差为零。\n\n对于第二个任务，如果 $X$ 的分布关于 $0$ 对称，那么它的奇数阶矩为零。因此，$\\mu_1 = \\mathbb{E}[X] = 0$ 且 $\\mu_3 = \\mathbb{E}[X^3] = 0$。将这些代入 $b^{\\star}$ 的表达式：\n$$\nb^{\\star} = \\beta_1 + \\beta_2 \\frac{0 - 0 \\cdot \\mu_2}{\\mu_2 - 0^2} = \\beta_1 + \\beta_2 \\frac{0}{\\mu_2}\n$$\n由于 $\\operatorname{Var}(X) = \\mu_2 - \\mu_1^2 = \\mu_2  0$，分母不为零。表达式简化为 $b^{\\star} = \\beta_1$。在这种情况下，斜率偏差为 $0$。\n\n对于最后一个任务，我们计算当 $X \\sim \\mathrm{Uniform}[0,1]$ 且 $\\beta_1=2$、$\\beta_2=3$ 时 $b^{\\star}$ 的值。我们需要矩 $\\mu_1$、$\\mu_2$ 和 $\\mu_3$。对于一个在 $[0,1]$ 上服从均匀分布的随机变量 $X$，其 $k$ 阶矩为：\n$$\n\\mu_k = \\mathbb{E}[X^k] = \\int_0^1 x^k \\cdot 1 \\,dx = \\left[ \\frac{x^{k+1}}{k+1} \\right]_0^1 = \\frac{1}{k+1}\n$$\n因此：\n$$\n\\mu_1 = \\frac{1}{2}, \\quad \\mu_2 = \\frac{1}{3}, \\quad \\mu_3 = \\frac{1}{4}\n$$\n现在我们计算 $b^{\\star}$ 公式中所需的各项：\n分数项的分子是 $\\mu_3 - \\mu_1 \\mu_2$：\n$$\n\\mu_3 - \\mu_1 \\mu_2 = \\frac{1}{4} - \\left(\\frac{1}{2}\\right)\\left(\\frac{1}{3}\\right) = \\frac{1}{4} - \\frac{1}{6} = \\frac{3-2}{12} = \\frac{1}{12}\n$$\n分母是 $\\operatorname{Var}(X) = \\mu_2 - \\mu_1^2$：\n$$\n\\mu_2 - \\mu_1^2 = \\frac{1}{3} - \\left(\\frac{1}{2}\\right)^2 = \\frac{1}{3} - \\frac{1}{4} = \\frac{4-3}{12} = \\frac{1}{12}\n$$\n将这些代入 $b^{\\star}$ 的表达式：\n$$\nb^{\\star} = \\beta_1 + \\beta_2 \\frac{\\frac{1}{12}}{\\frac{1}{12}} = \\beta_1 + \\beta_2(1) = \\beta_1 + \\beta_2\n$$\n给定值 $\\beta_1 = 2$ 和 $\\beta_2 = 3$，我们求出 $b^{\\star}$ 的具体值：\n$$\nb^{\\star} = 2 + 3 = 5\n$$", "answer": "$$\\boxed{5}$$", "id": "3159615"}, {"introduction": "理论与实践的差距往往体现在我们处理的有限且不完美的数据上。这个练习提供了一个具体的样本，其中一个“异常”数据点显著地将样本回归线从已知的总体回归线上拉开。你将亲手计算并运用如库克距离 (Cook's distance) 这样的影响力诊断工具，来识别并量化单个数据点对模型拟合的巨大影响 [@problem_id:3159624]。", "problem": "在统计学习的设定下，对一个有限样本拟合一个简单线性回归模型，其中总体回归线假设为 $y = 2 + 1.5 x$。考虑根据 $y_i = 2 + 1.5 x_i + \\varepsilon_i$ 生成的 $n = 6$ 个观测值 $(x_i, y_i)$，其中 $\\varepsilon_i$ 是小的扰动项：\n$(x_1, y_1) = (0, 2)$，$(x_2, y_2) = (1, 3.5)$，$(x_3, y_3) = (2, 5)$，$(x_4, y_4) = (3, 6.5)$，$(x_5, y_5) = (8, 16)$，$(x_6, y_6) = (2, 5)$。\n将这些视为一个已实现的有限样本，并使用普通最小二乘法（OLS）拟合最小二乘线。仅使用最小二乘估计、杠杆值和影响力的基本定义，计算每个观测值的库克距离，并用它来评估在存在一个罕见 $x$ 值的情况下，有限样本的最小二乘线如何偏离总体回归线。然后选择正确的陈述。\n\n选项：\nA. $x = 8$ 的观测值具有最大的库克距离，移除它会使最小二乘线更接近总体回归线 $y = 2 + 1.5 x$。\nB. $x = 3$ 的观测值具有最大的库克距离，因为它在非罕见 $x$ 值中具有最大的残差绝对值。\nC. 库克距离仅取决于残差平方 $r_i^2$；具有相等残差的两个点将具有相等的库克距离，无论它们的 $x_i$ 值如何。\nD. 当 $x_i$ 非常接近样本均值 $\\bar{x}$ 时，会产生高杠杆值；因此，罕见的 $x = 8$ 点具有低杠杆值。\nE. 根据计算出的库克距离，所有点都低于经验阈值 $4/n$；因此没有观测值是有影响力的。", "solution": "用户希望我验证问题陈述，如果有效，则通过推导最小二乘线、计算库克距离并评估每个给定陈述来解决问题。\n\n### 步骤1：提取给定条件\n- 模型是一个简单线性回归：$y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i$。\n- 总体回归线给出为 $y = 2 + 1.5 x$，所以 $\\beta_0 = 2$ 且 $\\beta_1 = 1.5$。\n- 样本量为 $n=6$。\n- 观测数据点为：\n  - $(x_1, y_1) = (0, 2)$\n  - $(x_2, y_2) = (1, 3.5)$\n  - $(x_3, y_3) = (2, 5)$\n  - $(x_4, y_4) = (3, 6.5)$\n  - $(x_5, y_5) = (8, 16)$\n  - $(x_6, y_6) = (2, 5)$\n- 拟合方法是普通最小二乘法 (OLS)。\n- 任务是计算每个观测值的库克距离并评估其影响力。\n\n### 步骤2：使用提取的给定条件进行验证\n- **科学基础：** 该问题基于统计学中的基本和标准概念：简单线性回归、普通最小二乘法、杠杆值和库克距离。这些是统计学习中公认的核心主题。\n- **适定性：** 该问题提供了一个完整的数据集，并要求基于明确定义的统计公式进行计算。对于此数据集，存在唯一的 OLS 解和相应的诊断统计量。\n- **客观性：** 该问题提供数值数据，并要求基于标准统计指标进行客观计算和解释。描述“小的扰动项”和“罕见的 x 值”是对所提供数据结构的恰当非正式描述。我们可以验证扰动项 $\\varepsilon_i = y_i - (2 + 1.5 x_i)$:\n  - $\\varepsilon_1 = 2 - (2 + 1.5 \\cdot 0) = 0$\n  - $\\varepsilon_2 = 3.5 - (2 + 1.5 \\cdot 1) = 0$\n  - $\\varepsilon_3 = 5 - (2 + 1.5 \\cdot 2) = 0$\n  - $\\varepsilon_4 = 6.5 - (2 + 1.5 \\cdot 3) = 0$\n  - $\\varepsilon_5 = 16 - (2 + 1.5 \\cdot 8) = 16 - 14 = 2$\n  - $\\varepsilon_6 = 5 - (2 + 1.5 \\cdot 2) = 0$\n  扰动项确实很小，只有一个非零值。术语“罕见的 x 值”客观地描述了 $x_5=8$，它远离其他 $x$ 值 $(0, 1, 2, 3)$。\n- **完整性和一致性：** 问题是自洽的，提供了继续进行所需的所有必要数据和定义。没有内部矛盾。重复点 $(2, 5)$ 的存在是抽样中的有效情景。\n\n### 步骤3：结论和行动\n问题是 **有效的**。现在将继续解决过程。\n\n### 推导\n目标是拟合一条最小二乘线 $\\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x$ 到给定的 $n=6$ 个数据点，并使用库克距离分析每个点的影响力。\n\n**1. 计算 OLS 系数 ($\\hat{\\beta}_0, \\hat{\\beta}_1$)**\n\n首先，我们计算样本均值 $\\bar{x}$ 和 $\\bar{y}$。\n$x$ 值为 $(0, 1, 2, 3, 8, 2)$。$y$ 值为 $(2, 3.5, 5, 6.5, 16, 5)$。\n$$ \\bar{x} = \\frac{0+1+2+3+8+2}{6} = \\frac{16}{6} = \\frac{8}{3} $$\n$$ \\bar{y} = \\frac{2+3.5+5+6.5+16+5}{6} = \\frac{38}{6} = \\frac{19}{3} $$\nOLS 斜率估计量是 $\\hat{\\beta}_1 = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^n (x_i - \\bar{x})^2}$。\n分母是：\n$$ \\sum_{i=1}^n (x_i - \\bar{x})^2 = (0-\\frac{8}{3})^2 + (1-\\frac{8}{3})^2 + (2-\\frac{8}{3})^2 + (3-\\frac{8}{3})^2 + (8-\\frac{8}{3})^2 + (2-\\frac{8}{3})^2 $$\n$$ = (-\\frac{8}{3})^2 + (-\\frac{5}{3})^2 + (-\\frac{2}{3})^2 + (\\frac{1}{3})^2 + (\\frac{16}{3})^2 + (-\\frac{2}{3})^2 = \\frac{64+25+4+1+256+4}{9} = \\frac{354}{9} = \\frac{118}{3} $$\n分子是：\n$$ \\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y}) = (0-\\frac{8}{3})(2-\\frac{19}{3}) + (1-\\frac{8}{3})(3.5-\\frac{19}{3}) + \\dots $$\n$$ = (-\\frac{8}{3})(-\\frac{13}{3}) + (-\\frac{5}{3})(-\\frac{8.5}{3}) + (-\\frac{2}{3})(-\\frac{4}{3}) + (\\frac{1}{3})(\\frac{0.5}{3}) + (\\frac{16}{3})(\\frac{29}{3}) + (-\\frac{2}{3})(-\\frac{4}{3}) $$\n$$ = \\frac{104 + 42.5 + 8 + 0.5 + 464 + 8}{9} = \\frac{627}{9} = \\frac{209}{3} $$\n所以，斜率是：\n$$ \\hat{\\beta}_1 = \\frac{209/3}{118/3} = \\frac{209}{118} \\approx 1.771 $$\n截距估计量是 $\\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1 \\bar{x}$：\n$$ \\hat{\\beta}_0 = \\frac{19}{3} - \\frac{209}{118} \\cdot \\frac{8}{3} = \\frac{1}{3} \\left( 19 - \\frac{1672}{118} \\right) = \\frac{1}{3} \\left( \\frac{2242 - 1672}{118} \\right) = \\frac{1}{3} \\frac{570}{118} = \\frac{190}{118} = \\frac{95}{59} \\approx 1.610 $$\n最小二乘线是 $\\hat{y} = \\frac{95}{59} + \\frac{209}{118} x$，或大约是 $\\hat{y} \\approx 1.610 + 1.771 x$。\n这偏离了总体线 $y = 2 + 1.5 x$。\n\n**2. 计算杠杆值和库克距离**\n\n观测值 $i$ 的库克距离由下式给出：\n$$ D_i = \\frac{r_i^2}{p \\cdot \\text{MSE}} \\left[ \\frac{h_{ii}}{(1-h_{ii})^2} \\right] $$\n其中 $p$ 是估计参数的数量（对于 $\\hat{\\beta}_0, \\hat{\\beta}_1$，$p=2$），$r_i=y_i-\\hat{y}_i$ 是残差，MSE 是均方误差，而 $h_{ii}$ 是观测值 $i$ 的杠杆值。\n\n杠杆值 $h_{ii}$ 由下式给出：\n$$ h_{ii} = \\frac{1}{n} + \\frac{(x_i - \\bar{x})^2}{\\sum_{j=1}^n (x_j - \\bar{x})^2} $$\n$x_5 = 8$ 的观测值距离 $\\bar{x} = 8/3 \\approx 2.67$ 最远。其杠杆值将是最高的。\n$$ h_{55} = \\frac{1}{6} + \\frac{(8 - 8/3)^2}{118/3} = \\frac{1}{6} + \\frac{(16/3)^2}{118/3} = \\frac{1}{6} + \\frac{256/9}{118/3} = \\frac{1}{6} + \\frac{256 \\cdot 3}{9 \\cdot 118} = \\frac{1}{6} + \\frac{256}{3 \\cdot 118} = \\frac{1}{6} + \\frac{256}{354} $$\n$$ h_{55} = \\frac{59}{354} + \\frac{256}{354} = \\frac{315}{354} = \\frac{105}{118} \\approx 0.890 $$\n这是一个极高的杠杆值（所有杠杆值之和为 $p=2$）。\n\n### 逐项分析\n\n**A. $x = 8$ 的观测值具有最大的库克距离，移除它会使最小二乘线更接近总体回归线 $y = 2 + 1.5 x$。**\n\n库克距离 $D_i$ 的公式分母中有 $(1-h_{ii})^2$ 项。对于观测值5，$h_{55} \\approx 0.890$，所以 $(1-h_{55})^2 \\approx (0.110)^2$ 非常小，这使得 $\\frac{h_{ii}}{(1-h_{ii})^2}$ 项非常大。这强烈表明 $D_5$ 将是最大的。虽然它的残差可能不是最大的，但其极端的杠杆值使其成为最具影响力的点。\n点5的残差是 $r_5 = y_5 - \\hat{y}_5 = 16 - (\\frac{95}{59} + \\frac{209}{118} \\cdot 8) = 16 - \\frac{190+1672}{118} = 16 - \\frac{1862}{118} = 16 - \\frac{931}{59} = \\frac{944-931}{59} = \\frac{13}{59}$。\n\n为了检验陈述的第二部分，我们移除观测值5 ($(8, 16)$)并重新拟合回归线。剩下的5个点是：$(0, 2)$, $(1, 3.5)$, $(2, 5)$, $(3, 6.5)$, 和 $(2, 5)$。\n我们来检查这些点是否落在总体线 $y = 2 + 1.5x$ 上：\n- 对于 $(0, 2)$: $2 = 2 + 1.5(0)$。正确。\n- 对于 $(1, 3.5)$: $3.5 = 2 + 1.5(1)$。正确。\n- 对于 $(2, 5)$: $5 = 2 + 1.5(2)$。正确。\n- 对于 $(3, 6.5)$: $6.5 = 2 + 1.5(3)$。正确。\n由于所有剩余的点都完美地落在直线 $y = 2 + 1.5x$ 上，因此对该数据子集的 OLS 拟合将精确地得出 $\\hat{\\beta}_{0,(5)} = 2$ 和 $\\hat{\\beta}_{1,(5)} = 1.5$。\n原始的线是 $\\hat{y} \\approx 1.610 + 1.771 x$。移除点5后，线变为 $\\hat{y}_{(5)} = 2 + 1.5 x$，这正是总体回归线。因此，移除该点使最小二乘线更接近总体线。陈述的两部分都为真。\n\n结论：**正确**。\n\n**B. $x = 3$ 的观测值具有最大的库克距离，因为它在非罕见 $x$ 值中具有最大的残差绝对值。**\n\n我们已经确定观测值5 ($x=8$) 具有极大的影响力，并将具有最大的库克距离。让我们计算所有点的残差来验证第二个论断。\n$\\hat{y}_i = \\frac{95}{59} + \\frac{209}{118} x_i$。\n- $r_1 (x=0): 2 - \\frac{95}{59} = \\frac{23}{59} \\approx 0.390$\n- $r_2 (x=1): 3.5 - (\\frac{95}{59} + \\frac{209}{118}) = 3.5 - \\frac{399}{118} = \\frac{413-399}{118} = \\frac{14}{118} = \\frac{7}{59} \\approx 0.119$\n- $r_3, r_6 (x=2): 5 - (\\frac{95}{59} + \\frac{209}{118} \\cdot 2) = 5 - \\frac{608}{118} = \\frac{590-608}{118} = -\\frac{18}{118} = -\\frac{9}{59} \\approx -0.153$\n- $r_4 (x=3): 6.5 - (\\frac{95}{59} + \\frac{209}{118} \\cdot 3) = 6.5 - \\frac{817}{118} = \\frac{767-817}{118} = -\\frac{50}{118} = -\\frac{25}{59} \\approx -0.424$\n非罕见 $x$ 值的残差绝对值为 $|r_1| \\approx 0.390$，$|r_2| \\approx 0.119$，$|r_{3,6}| \\approx 0.153$，以及 $|r_4| \\approx 0.424$。点4 ($x=3$) 在这些值中确实具有最大的残差绝对值。然而，它的库克距离远小于点5的库克距离。 “因为...”这个前提是正确的，但主要结论是错误的。整个陈述是错误的。\n\n结论：**错误**。\n\n**C. 库克距离仅取决于残差平方 $r_i^2$；具有相等残差的两个点将具有相等的库克距离，无论它们的 $x_i$ 值如何。**\n\n库克距离的公式是 $D_i = \\frac{r_i^2}{p \\cdot \\text{MSE}} \\left[ \\frac{h_{ii}}{(1-h_{ii})^2} \\right]$。它清楚地表明 $D_i$ 是残差平方 $r_i^2$ 和杠杆值 $h_{ii}$ 的函数。杠杆值 $h_{ii}$ 是 $x_i$ 与均值 $\\bar{x}$ 距离的函数。因此，具有相同残差但不同 $x_i$ 值的两个点将具有不同的杠杆值，从而具有不同的库克距离。该陈述根本上是错误的。\n\n结论：**错误**。\n\n**D. 当 $x_i$ 非常接近样本均值 $\\bar{x}$ 时，会产生高杠杆值；因此，罕见的 $x = 8$ 点具有低杠杆值。**\n\n杠杆值的公式是 $h_{ii} = \\frac{1}{n} + \\frac{(x_i - \\bar{x})^2}{\\sum_{j=1}^n (x_j - \\bar{x})^2}$。这个公式表明，随着 $(x_i - \\bar{x})^2$（即 $x_i$ 与样本均值 $\\bar{x}$ 的距离平方）的增加，杠杆值也增加。该陈述声称的恰恰相反。$x_i$ 远离 $\\bar{x}$ 的点具有高杠杆值。点 $x_5=8$ 距离 $\\bar{x} = 8/3$ 最远，因此它具有最高的杠杆值，而不是低杠杆值。我们的计算证实了这一点：$h_{55} \\approx 0.890$。\n\n结论：**错误**。\n\n**E. 根据计算出的库克距离，所有点都低于经验阈值 $4/n$；因此没有观测值是有影响力的。**\n\n识别影响点的一个常用经验法则是检查是否 $D_i  4/n$。\n在这个问题中，$n=6$，所以阈值是 $4/6 = 2/3 \\approx 0.667$。\n让我们计算点5的库克距离。\n我们需要均方误差（MSE）：$\\text{MSE} = \\frac{\\sum r_i^2}{n-p} = \\frac{1}{4} \\sum r_i^2$。\n$\\sum r_i^2 = \\frac{1}{59^2} (23^2 + 7^2 + (-9)^2 + (-25)^2 + 13^2 + (-9)^2) = \\frac{529+49+81+625+169+81}{3481} = \\frac{1534}{3481}$。\n$\\text{MSE} = \\frac{1}{4} \\frac{1534}{3481} = \\frac{767}{2 \\cdot 3481} = \\frac{767}{6962} \\approx 0.110$。\n$p \\cdot \\text{MSE} = 2 \\cdot \\frac{767}{6962} = \\frac{767}{3481}$。\n$$ D_5 = \\frac{r_5^2}{p \\cdot \\text{MSE}} \\left[ \\frac{h_{55}}{(1-h_{55})^2} \\right] = \\frac{(13/59)^2}{767/3481} \\left[ \\frac{315/354}{(1 - 315/354)^2} \\right] $$\n$$ D_5 = \\frac{169/3481}{767/3481} \\left[ \\frac{315/354}{(39/354)^2} \\right] = \\frac{169}{767} \\frac{315 \\cdot 354}{39^2} = \\frac{12390}{767} \\approx 16.15 $$\n由于 $D_5 \\approx 16.15$ 远大于阈值 $4/6 \\approx 0.667$，所以“所有点都低于阈值”的陈述是错误的。观测值5具有高度影响力。\n\n结论：**错误**。", "answer": "$$\\boxed{A}$$", "id": "3159624"}]}