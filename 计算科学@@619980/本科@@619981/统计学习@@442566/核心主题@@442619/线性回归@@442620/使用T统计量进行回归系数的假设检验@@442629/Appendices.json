{"hands_on_practices": [{"introduction": "掌握任何统计工具的第一步都是理解其基本计算方法。本练习提供了一个具体的科学情景，旨在帮助你练习计算回归斜率的$t$统计量。这个统计量衡量的是估计出的系数偏离零假设值（通常为零）多少个标准误，是检验变量显著性的核心。", "problem": "一位环境科学家正在研究一种特定污染物对湖中某种藻类种群密度的影响。他们从不同地点采集水样，并对每个样本测量了污染物的浓度（单位：微克/升）和藻类的密度（单位：细胞/毫升）。\n\n该科学家对数据拟合了一个简单线性回归模型，$Y = \\beta_0 + \\beta_1 X + \\epsilon$，其中 $Y$ 表示藻类密度，$X$ 表示污染物浓度。对收集的数据进行分析，得到估计的斜率系数为 $\\hat{\\beta}_1 = -18.4$。与此斜率估计值相关的标准误差为 $SE(\\hat{\\beta}_1) = 5.25$。\n\n为了评估污染物浓度和藻类密度之间是否存在统计上显著的线性关系，研究者进行了一项假设检验。请计算用于检验真实斜率系数 $\\beta_1$ 为零这一原假设的t统计量的值。请将您的答案四舍五入到三位有效数字。", "solution": "我们检验原假设 $H_{0}:\\beta_{1}=0$ 和备择假设 $\\beta_{1}\\neq 0$。在简单线性回归中，用于检验斜率的检验统计量为\n$$\nt=\\frac{\\hat{\\beta}_{1}-\\beta_{1,0}}{SE(\\hat{\\beta}_{1})},\n$$\n其中 $\\beta_{1,0}$ 是在 $H_{0}$ 下的假设值。当 $\\beta_{1,0}=0$，$\\hat{\\beta}_{1}=-18.4$ 且 $SE(\\hat{\\beta}_{1})=5.25$ 时，我们有\n$$\nt=\\frac{-18.4-0}{5.25}=-\\frac{18.4}{5.25}\\approx -3.5047619\\ldots\n$$\n四舍五入到三位有效数字，得到 $t \\approx -3.50$。", "answer": "$$\\boxed{-3.50}$$", "id": "1955459"}, {"introduction": "在掌握了基本计算之后，我们来探索一个更深层次的概念特性。这个思想实验将揭示$t$统计量的一个关键性质：它对于预测变量的线性变换（例如，改变度量单位）具有不变性。理解这一点有助于我们认识到为何$t$统计量是衡量变量关系强度的一个稳健指标，其结论不会因为数据的单位变化而改变。", "problem": "考虑带截距项的简单线性回归，其中一个大小为 $n \\ge 3$ 的数据集由数据对 $\\{(x_i, y_i)\\}_{i=1}^n$ 组成，模型为 $y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i$。假设误差项 $\\varepsilon_i$ 满足通常的高斯-马尔可夫条件（独立，均值为 $0$，共同方差为 $\\sigma^2$），并且，为保证学生t检验的有效性，$\\varepsilon_i$ 服从正态分布。令 $\\bar x = \\frac{1}{n}\\sum_{i=1}^n x_i$ 并定义样本标准差 $s_x = \\sqrt{\\frac{1}{n-1}\\sum_{i=1}^n (x_i - \\bar x)^2}$。考虑变换后的预测变量 $z_i = \\frac{x_i - \\bar x}{s_x}$，并用普通最小二乘法 (OLS) 拟合模型 $y_i = \\tilde\\beta_0 + \\tilde\\beta_1 z_i + \\varepsilon_i$。\n\n令 $\\hat\\beta_1$ 表示以 $x$ 为预测变量的原始模型中 $\\beta_1$ 的 OLS 估计量，令 $\\hat\\beta_1^{(z)}$ 表示以 $z$ 为预测变量的标准化模型中斜率的 OLS 估计量。令 $t_1$ 为基于原始模型检验 $H_0:\\beta_1=0$ 的 OLS $t$ 统计量，令 $t_1^{(z)}$ 为标准化模型中对应的 $t$ 统计量。\n\n下列哪个陈述最好地描述了 $\\hat\\beta_1$、$\\hat\\beta_1^{(z)}$、$t_1$ 和 $t_1^{(z)}$ 之间的关系？\n\nA. $\\hat\\beta_1^{(z)} = s_x \\hat\\beta_1$ 且 $t_1^{(z)} = t_1$。\n\nB. $\\hat\\beta_1^{(z)} = \\hat\\beta_1/s_x$ 且 $t_1^{(z)} = t_1$。\n\nC. $\\hat\\beta_1^{(z)} = s_x \\hat\\beta_1$ 且 $t_1^{(z)} = s_x \\, t_1$。\n\nD. $\\hat\\beta_1^{(z)} = \\hat\\beta_1$ 且 $t_1^{(z)} = t_1$。", "solution": "在进行求解之前，将对问题陈述进行验证。\n\n### 第1步：提取已知条件\n- **模型1（原始）**：$y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i$，对于 $i=1, \\dots, n$。\n- **数据集**：$\\{(x_i, y_i)\\}_{i=1}^n$，样本量 $n \\ge 3$。\n- **误差假设**：$\\varepsilon_i$ 是独立同分布 (i.i.d.) 的正态随机变量，均值 $E[\\varepsilon_i]=0$，共同方差 $Var(\\varepsilon_i)=\\sigma^2$。这包含了高斯-马尔可夫条件和正态性假设。\n- **定义**：\n    - $x$ 的样本均值：$\\bar x = \\frac{1}{n}\\sum_{i=1}^n x_i$。\n    - $x$ 的样本标准差：$s_x = \\sqrt{\\frac{1}{n-1}\\sum_{i=1}^n (x_i - \\bar x)^2}$。这里隐含假设并非所有 $x_i$ 都相同，因此 $s_x > 0$。\n- **变换**：$z_i = \\frac{x_i - \\bar x}{s_x}$。\n- **模型2（标准化）**：$y_i = \\tilde\\beta_0 + \\tilde\\beta_1 z_i + \\varepsilon_i$。\n- **关注的量**：\n    - $\\hat\\beta_1$：模型1中 $\\beta_1$ 的普通最小二乘 (OLS) 估计量。\n    - $\\hat\\beta_1^{(z)}$：模型2中 $\\tilde\\beta_1$ 的 OLS 估计量。\n    - $t_1$：模型1中用于检验原假设 $H_0: \\beta_1 = 0$ 的 OLS $t$ 统计量。\n    - $t_1^{(z)}$：模型2中用于检验原假设 $H_0: \\tilde\\beta_1 = 0$ 的 OLS $t$ 统计量。\n- **问题**：描述 $\\hat\\beta_1$、$\\hat\\beta_1^{(z)}$、$t_1$ 和 $t_1^{(z)}$ 之间的关系。\n\n### 第2步：使用提取的已知条件进行验证\n- **科学依据**：该问题在线性回归理论中有充分的依据，这是统计学和统计学习中的一个核心课题。所有概念，包括 OLS 估计、变量标准化和使用 $t$ 统计量的假设检验，都是标准的且有严格定义。\n- **适定性**：这个问题是适定的。从 $x$ 到 $z$ 的变换是明确的。估计量和检验统计量之间的关系可以从 OLS 回归的基本原理中唯一地推导出来。条件 $n \\ge 3$ 确保了模型误差方差估计的自由度 $n-2$ 至少为1，这是 $t$ 统计量有良好定义的必要条件。\n- **客观性**：问题以客观的数学语言陈述，没有歧义或主观性。\n\n### 第3步：结论与行动\n问题陈述是有效的。这是一个统计理论中的标准、适定的问题。将推导解答。\n\n### 推导\n求解需要推导两个模型中斜率估计量和 $t$ 统计量之间的关系。\n\n**第1部分：$\\hat\\beta_1$ 和 $\\hat\\beta_1^{(z)}$ 之间的关系**\n\n在响应变量 $Y$ 对预测变量 $X$ 的简单线性回归中，斜率系数的 OLS 估计量由下式给出：\n$$ \\hat\\beta_{slope} = \\frac{\\sum_{i=1}^n (X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sum_{i=1}^n (X_i - \\bar{X})^2} $$\n对于原始模型（模型1），预测变量是 $x$，$\\beta_1$ 的 OLS 估计量是：\n$$ \\hat\\beta_1 = \\frac{\\sum_{i=1}^n (x_i - \\bar x)(y_i - \\bar y)}{\\sum_{i=1}^n (x_i - \\bar x)^2} $$\n对于标准化模型（模型2），预测变量是 $z$。我们首先求出 $z$ 的样本均值和离差平方和。\n$z$ 的样本均值是：\n$$ \\bar z = \\frac{1}{n}\\sum_{i=1}^n z_i = \\frac{1}{n}\\sum_{i=1}^n \\frac{x_i - \\bar x}{s_x} = \\frac{1}{n s_x} \\left( \\sum_{i=1}^n x_i - n\\bar x \\right) = \\frac{1}{n s_x} (n\\bar x - n\\bar x) = 0 $$\n$z$ 的离差平方和是：\n$$ \\sum_{i=1}^n (z_i - \\bar z)^2 = \\sum_{i=1}^n (z_i - 0)^2 = \\sum_{i=1}^n z_i^2 = \\sum_{i=1}^n \\left( \\frac{x_i - \\bar x}{s_x} \\right)^2 $$\n使用定义 $s_x^2 = \\frac{1}{n-1}\\sum_{i=1}^n (x_i - \\bar x)^2$，我们有 $\\sum_{i=1}^n (x_i - \\bar x)^2 = (n-1)s_x^2$。\n代入这个，我们得到：\n$$ \\sum_{i=1}^n (z_i - \\bar z)^2 = \\frac{1}{s_x^2} \\sum_{i=1}^n (x_i - \\bar x)^2 = \\frac{(n-1)s_x^2}{s_x^2} = n-1 $$\n现在我们可以写出模型2中斜率 $\\tilde\\beta_1$ 的 OLS 估计量，记作 $\\hat\\beta_1^{(z)}$：\n$$ \\hat\\beta_1^{(z)} = \\frac{\\sum_{i=1}^n (z_i - \\bar z)(y_i - \\bar y)}{\\sum_{i=1}^n (z_i - \\bar z)^2} = \\frac{\\sum_{i=1}^n z_i (y_i - \\bar y)}{n-1} $$\n代入 $z_i = \\frac{x_i - \\bar x}{s_x}$：\n$$ \\hat\\beta_1^{(z)} = \\frac{\\sum_{i=1}^n \\frac{x_i - \\bar x}{s_x} (y_i - \\bar y)}{n-1} = \\frac{1}{(n-1)s_x} \\sum_{i=1}^n (x_i - \\bar x)(y_i - \\bar y) $$\n从 $\\hat\\beta_1$ 的公式中，我们可以将分子中的交叉乘积和表示为 $\\sum_{i=1}^n (x_i - \\bar x)(y_i - \\bar y) = \\hat\\beta_1 \\sum_{i=1}^n (x_i - \\bar x)^2 = \\hat\\beta_1 (n-1)s_x^2$。\n将此代入 $\\hat\\beta_1^{(z)}$ 的表达式中：\n$$ \\hat\\beta_1^{(z)} = \\frac{1}{(n-1)s_x} \\left( \\hat\\beta_1 (n-1)s_x^2 \\right) = s_x \\hat\\beta_1 $$\n这就建立了第一个关系：$\\hat\\beta_1^{(z)} = s_x \\hat\\beta_1$。\n\n**第2部分：$t_1$ 和 $t_1^{(z)}$ 之间的关系**\n\n斜率系数的 $t$ 统计量是估计系数与其标准误的比值。\n$$ t_1 = \\frac{\\hat\\beta_1}{\\text{SE}(\\hat\\beta_1)} \\quad \\text{和} \\quad t_1^{(z)} = \\frac{\\hat\\beta_1^{(z)}}{\\text{SE}(\\hat\\beta_1^{(z)})} $$\n斜率估计的标准误由 $\\text{SE}(\\hat\\beta_{slope}) = \\sqrt{\\frac{\\hat\\sigma^2}{\\sum (X_i - \\bar{X})^2}}$ 给出，其中 $\\hat\\sigma^2$ 是误差方差 $\\sigma^2$ 的无偏估计量。\n\n我们首先证明两个模型的估计误差方差是相同的。估计量 $\\hat\\sigma^2$ 由残差平方和 (RSS) 计算得出：$\\hat\\sigma^2 = \\frac{\\text{RSS}}{n-2}$。\n对于模型1，拟合值为 $\\hat y_i = \\hat\\beta_0 + \\hat\\beta_1 x_i$。OLS 截距项为 $\\hat\\beta_0 = \\bar y - \\hat\\beta_1 \\bar x$。因此，\n$$ \\hat y_i = (\\bar y - \\hat\\beta_1 \\bar x) + \\hat\\beta_1 x_i = \\bar y + \\hat\\beta_1(x_i - \\bar x) $$\n对于模型2，拟合值为 $\\hat y_i^{(z)} = \\hat{\\tilde\\beta}_0 + \\hat\\beta_1^{(z)} z_i$。OLS 截距项为 $\\hat{\\tilde\\beta}_0 = \\bar y - \\hat\\beta_1^{(z)} \\bar z$。因为 $\\bar z = 0$，我们有 $\\hat{\\tilde\\beta}_0 = \\bar y$。因此，\n$$ \\hat y_i^{(z)} = \\bar y + \\hat\\beta_1^{(z)} z_i $$\n现在，代入关系式 $\\hat\\beta_1^{(z)} = s_x \\hat\\beta_1$ 和 $z_i = \\frac{x_i - \\bar x}{s_x}$：\n$$ \\hat y_i^{(z)} = \\bar y + (s_x \\hat\\beta_1) \\left( \\frac{x_i - \\bar x}{s_x} \\right) = \\bar y + \\hat\\beta_1(x_i - \\bar x) $$\n我们看到 $\\hat y_i^{(z)} = \\hat y_i$。两个模型的拟合值是相同的。因此，残差 $e_i = y_i - \\hat y_i$ 也相同，RSS 相同，估计的误差方差也相同。我们将其对两个模型都记为 $\\hat\\sigma^2$。\n\n现在，我们可以计算标准误。\n对于模型1：\n$$ \\text{SE}(\\hat\\beta_1) = \\sqrt{\\frac{\\hat\\sigma^2}{\\sum_{i=1}^n (x_i - \\bar x)^2}} = \\frac{\\hat\\sigma}{\\sqrt{(n-1)s_x^2}} = \\frac{\\hat\\sigma}{s_x \\sqrt{n-1}} $$\n$t$ 统计量是：\n$$ t_1 = \\frac{\\hat\\beta_1}{\\text{SE}(\\hat\\beta_1)} = \\frac{\\hat\\beta_1 s_x \\sqrt{n-1}}{\\hat\\sigma} $$\n对于模型2：\n$$ \\text{SE}(\\hat\\beta_1^{(z)}) = \\sqrt{\\frac{\\hat\\sigma^2}{\\sum_{i=1}^n (z_i - \\bar z)^2}} = \\sqrt{\\frac{\\hat\\sigma^2}{n-1}} = \\frac{\\hat\\sigma}{\\sqrt{n-1}} $$\n$t$ 统计量是：\n$$ t_1^{(z)} = \\frac{\\hat\\beta_1^{(z)}}{\\text{SE}(\\hat\\beta_1^{(z)})} = \\frac{s_x \\hat\\beta_1}{\\hat\\sigma / \\sqrt{n-1}} = \\frac{\\hat\\beta_1 s_x \\sqrt{n-1}}{\\hat\\sigma} $$\n比较最终的表达式，我们看到 $t_1^{(z)} = t_1$。\n\n*t统计量的另一种推导方法：*\n用于检验简单线性回归中斜率显著性的 $t$ 统计量也可以用预测变量和响应变量之间的皮尔逊相关系数 $r$ 来表示：\n$$ t = \\frac{r\\sqrt{n-2}}{\\sqrt{1-r^2}} $$\n皮尔逊相关系数对于两个变量各自的线性变换是不变的。也就是说，如果 $X' = aX+b$ 且 $Y' = cY+d$，那么 $|\\text{Corr}(X', Y')| = |\\text{Corr}(X, Y)|$。如果 $a$ 和 $c$ 的符号相同，则 $\\text{Corr}(X', Y') = \\text{Corr}(X, Y)$。\n在本例中，响应变量 $y$ 没有改变。预测变量通过 $z_i = \\frac{1}{s_x} x_i - \\frac{\\bar x}{s_x}$ 从 $x$ 变换到 $z$。这是一个具有正常数缩放因子 $\\frac{1}{s_x}$（因为 $s_x > 0$）的线性变换。因此，$z$ 和 $y$ 之间的相关性与 $x$ 和 $y$ 之间的相关性相同：\n$$ r_{zy} = \\text{Corr}(z, y) = \\text{Corr}(x, y) = r_{xy} $$\n由于 $t$ 统计量仅取决于样本大小 $n$ 和相关系数 $r$，而这两者对于两个模型都是相同的，因此 $t$ 统计量本身也必须相同。\n$$ t_1 = \\frac{r_{xy}\\sqrt{n-2}}{\\sqrt{1-r_{xy}^2}} = \\frac{r_{zy}\\sqrt{n-2}}{\\sqrt{1-r_{zy}^2}} = t_1^{(z)} $$\n两种推导都证实了 $t_1^{(z)} = t_1$。\n\n总结如下：\n- $\\hat\\beta_1^{(z)} = s_x \\hat\\beta_1$\n- $t_1^{(z)} = t_1$\n\n### 逐项分析\n\n**A. $\\hat\\beta_1^{(z)} = s_x \\hat\\beta_1$ 且 $t_1^{(z)} = t_1$。**\n- 第一部分，$\\hat\\beta_1^{(z)} = s_x \\hat\\beta_1$，与我们的推导一致。\n- 第二部分，$t_1^{(z)} = t_1$，也与我们的推导一致。\n- 这个陈述是**正确的**。\n\n**B. $\\hat\\beta_1^{(z)} = \\hat\\beta_1/s_x$ 且 $t_1^{(z)} = t_1$。**\n- 第一部分，$\\hat\\beta_1^{(z)} = \\hat\\beta_1/s_x$，与我们的发现 $\\hat\\beta_1^{(z)} = s_x \\hat\\beta_1$ 相矛盾。这仅在 $s_x^2=1$ 的情况下才成立，而通常情况并非如此。\n- 因此，该陈述是**不正确的**。\n\n**C. $\\hat\\beta_1^{(z)} = s_x \\hat\\beta_1$ 且 $t_1^{(z)} = s_x \\, t_1$。**\n- 第二部分，$t_1^{(z)} = s_x \\, t_1$，与我们的发现 $t_1^{(z)} = t_1$ 相矛盾。这仅在 $s_x=1$ 的情况下才成立。\n- 因此，该陈述是**不正确的**。\n\n**D. $\\hat\\beta_1^{(z)} = \\hat\\beta_1$ 且 $t_1^{(z)} = t_1$。**\n- 第一部分，$\\hat\\beta_1^{(z)} = \\hat\\beta_1$，与我们的发现 $\\hat\\beta_1^{(z)} = s_x \\hat\\beta_1$ 相矛盾。这仅在 $s_x=1$ 的情况下才成立。\n- 因此，该陈述是**不正确的**。", "answer": "$$\\boxed{A}$$", "id": "3131042"}, {"introduction": "现在，我们将$t$统计量的知识应用于一个在多元回归中常见且具有挑战性的问题：多重共线性。本练习阐释了当预测变量之间高度相关时，它们系数估计的标准误会如何被“放大”，从而导致$t$统计量变小且不显著。即便整个模型具有很高的解释力（即很高的$R^2$），这种情况也可能发生，因此理解这一现象对于正确解读回归结果至关重要。", "problem": "考虑多元线性回归模型 $Y_i = \\beta_0 + \\beta_1 x_{1i} + \\beta_2 x_{2i} + \\varepsilon_i$，其中 $\\varepsilon_i \\sim \\text{i.i.d. } \\mathcal{N}(0,\\sigma^2)$，且预测变量是中心化的，因此 $\\sum_{i=1}^{n} x_{1i} = 0$ 且 $\\sum_{i=1}^{n} x_{2i} = 0$。假设设计表现出近似共线性，即 $x_2 = x_1 + \\delta$，其中 $\\delta$ 是一个均值为0的扰动，独立于 $x_1$，其方差 $\\operatorname{Var}(\\delta) = \\tau^2$ 相对于 $\\operatorname{Var}(x_1) = v_1 > 0$ 很小。从普通最小二乘法和高斯线性模型下的抽样分布的基本原理出发（特别是普通最小二乘估计量 $\\hat{\\beta} = (X^\\top X)^{-1} X^\\top Y$ 的定义、其协方差 $\\operatorname{Var}(\\hat{\\beta}) = \\sigma^2 (X^\\top X)^{-1}$ 以及正交投影的作用），分析近似共线性如何影响 $\\hat{\\beta}_2$ 的标准误以及对 $H_0:\\beta_2=0$ 的 $t$ 检验。\n\n在此设置下，以下哪些陈述是正确的？选择所有适用的选项。\n\nA. 在这种 $x_2 = x_1 + \\delta$ 和中心化预测变量的构造中，$\\hat{\\beta}_2$ 的抽样方差等于 $\\sigma^2 \\big/ \\big(n\\, \\operatorname{Var}(\\delta)\\big)$，因此当 $\\tau^2 \\to 0$ 时，它会发散。\n\nB. 将 $\\operatorname{Var}(\\hat{\\beta}_2)$ 与一元回归方差联系起来的方差膨胀因子是 $(1 - R_2^2)^{-1}$，其中 $R_2^2$ 是将 $x_2$ 对 $x_1$ 进行回归得到的。在此设置中，$R_2^2 = v_1/(v_1+\\tau^2)$，因此膨胀因子等于 $(v_1+\\tau^2)/\\tau^2$，并随着 $\\tau^2 \\to 0$ 而无界增长。\n\nC. 由于 $x_1$ 的存在，一个模型的 $R^2$ 可以任意接近于1，而对 $H_0:\\beta_2=0$ 的检验却产生一个很小的 $|t_2|$，因为只有 $x_2$ 与 $x_1$ 正交的分量（即 $\\delta$）为 $\\beta_2$ 提供信息，而其方差很小。\n\nD. 因为当添加更多预测变量时，残差标准误 $s$ 会减小，所以当将 $x_2$ 添加到已包含 $x_1$ 的模型中时， $|t_2|$ 必须增加。\n\nE. 近似共线性会在 $\\hat{\\beta}_2$ 中引入趋向于0的偏差，这是即使在 $R^2$ 很大时 $|t_2|$ 也很小的主要原因。", "solution": "用户希望对近似共线性条件下的多元线性回归模型进行详细分析。\n\n### 问题验证\n\n**步骤1：提取已知条件**\n-   **模型:** $Y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\varepsilon$\n-   **误差分布:** $\\varepsilon$ 是独立同分布 (i.i.d.) 的 $\\mathcal{N}(0,\\sigma^2)$。\n-   **预测变量中心化:** $\\sum_{i=1}^{n} x_{1i} = 0$ 和 $\\sum_{i=1}^{n} x_{2i} = 0$。\n-   **共线性结构:** $x_2 = x_1 + \\delta$。\n-   **扰动属性:** $\\delta$ 是一个均值为0的扰动，独立于 $x_1$，其方差为 $\\operatorname{Var}(\\delta) = \\tau^2$。$\\tau^2$ 相对于 $\\operatorname{Var}(x_1) = v_1 > 0$ 很小。\n-   **OLS估计量:** $\\hat{\\beta} = (X^\\top X)^{-1} X^\\top Y$。\n-   **OLS估计量的协方差:** $\\operatorname{Var}(\\hat{\\beta}) = \\sigma^2 (X^\\top X)^{-1}$。\n-   **任务:** 分析近似共线性对 $\\hat{\\beta}_2$ 标准误和 $H_0: \\beta_2 = 0$ 的 $t$ 检验的影响。\n\n**步骤2：使用提取的已知条件进行验证**\n问题陈述描述了统计学和计量经济学中用于说明多重共线性后果的典型情景。\n-   **科学依据:** 该设置基于标准高斯线性模型和普通最小二乘估计理论。所有概念都是统计学习的基础。\n-   **问题适定:** 问题定义明确。它要求在指定模型内分析估计量的性质。所提供的条件足以进行此分析。\n-   **客观性:** 语言是形式化和数学化的，没有主观内容。\n-   **一致性检查:** 中心化条件 $\\sum x_{1i} = 0$ 和 $\\sum x_{2i} = 0$，与 $x_{2i} = x_{1i} + \\delta_i$ 结合，意味着 $\\sum (x_{1i} + \\delta_i) = 0$，从而得到 $\\sum \\delta_i = 0$。这与 $\\delta$ 是一个均值为0的扰动是一致的。该设置内部一致。\n\n**步骤3：结论与行动**\n问题是有效的。我将继续推导解决方案。\n\n### 从基本原理推导\n\n问题的核心在于理解OLS估计量 $\\hat{\\beta}_2$ 的方差。题目要求我们从基本原理出发，包括正交投影的思想。Frisch-Waugh-Lovell (FWL)定理是这里的相关原理。该定理指出，多元回归中的系数 $\\hat{\\beta}_2$ 与将 $Y_{\\perp 1}$ 对 $x_{2, \\perp 1}$ 进行简单回归得到的系数相同，其中 $Y_{\\perp 1}$ 是 $Y$ 对 $x_1$ 回归的残差向量，$x_{2, \\perp 1}$ 是 $x_2$ 对 $x_1$ 回归的残差向量。\n\n$\\hat{\\beta}_2$ 的方差由下式给出：\n$$ \\operatorname{Var}(\\hat{\\beta}_2) = \\frac{\\sigma^2}{\\|\\mathbf{x}_{2, \\perp 1}\\|^2} $$\n其中 $\\|\\mathbf{x}_{2, \\perp 1}\\|^2 = \\sum_{i=1}^n (x_{2i, \\perp 1})^2$ 是将 $\\mathbf{x}_2$ 对 $\\mathbf{x}_1$ 回归的残差平方和。\n\n我们来计算这些残差。模型为 $\\mathbf{x}_2 = \\gamma_1 \\mathbf{x}_1 + \\text{误差}$。由于预测变量是中心化的，截距为0。$\\gamma_1$ 的OLS估计为 $\\hat{\\gamma}_1 = (\\mathbf{x}_1^\\top \\mathbf{x}_1)^{-1} \\mathbf{x}_1^\\top \\mathbf{x}_2$。残差向量为 $\\mathbf{x}_{2, \\perp 1} = \\mathbf{x}_2 - \\hat{\\gamma}_1 \\mathbf{x}_1$。\n使用问题中的定义 $\\mathbf{x}_2 = \\mathbf{x}_1 + \\mathbf{\\delta}$：\n$$ \\mathbf{x}_{2, \\perp 1} = (\\mathbf{x}_1 + \\mathbf{\\delta}) - \\hat{\\gamma}_1 \\mathbf{x}_1 $$\n等等，使用投影矩阵的表述更简单。设 $P_1$ 是到由 $\\mathbf{x}_1$ 张成的空间的投影矩阵。则 $\\mathbf{x}_{2, \\perp 1} = (I - P_1)\\mathbf{x}_2$。\n代入 $\\mathbf{x}_2 = \\mathbf{x}_1 + \\mathbf{\\delta}$：\n$$ \\mathbf{x}_{2, \\perp 1} = (I - P_1)(\\mathbf{x}_1 + \\mathbf{\\delta}) = (I - P_1)\\mathbf{x}_1 + (I - P_1)\\mathbf{\\delta} = \\mathbf{0} + (I - P_1)\\mathbf{\\delta} $$\n项 $(I - P_1)\\mathbf{x}_1$ 为零，因为 $\\mathbf{x}_1$ 在其自身的列空间中。因此，$\\mathbf{x}_2$ 对 $\\mathbf{x}_1$ 回归的残差恰好是 $\\mathbf{\\delta}$ 对 $\\mathbf{x}_1$ 回归的残差。\n$\\hat{\\beta}_2$ 方差的分母是这个残差向量的平方范数：\n$$ \\|\\mathbf{x}_{2, \\perp 1}\\|^2 = \\|(I - P_1)\\mathbf{\\delta}\\|^2 = \\text{RSS}_{\\delta \\sim x_1} $$\n这是将 $\\mathbf{\\delta}$ 对 $\\mathbf{x}_1$ 回归的残差平方和。这就是“与 $\\mathbf{x}_1$ 正交的 $\\mathbf{\\delta}$ 的分量”。\n\n问题陈述 $x_1$ 和 $\\delta$ 是独立的。在有限样本中，这并不能保证样本相关性为零（即 $\\sum x_{1i}\\delta_i = 0$）。然而，对于这类理论分析，通常会假设样本属性反映总体属性，这意味着样本正交性。我们假设 $\\sum_{i=1}^n x_{1i}\\delta_i = 0$。这意味着在我们的样本中，$\\mathbf{x}_1$ 和 $\\mathbf{\\delta}$ 是正交的。\n在这种标准的理想化情况下，将 $\\mathbf{\\delta}$ 对 $\\mathbf{x}_1$ 进行回归会得到一个零系数，而残差就是 $\\mathbf{\\delta}$ 本身。\n$$ \\text{如果 } \\mathbf{x}_1^\\top \\mathbf{\\delta} = 0, \\text{ 那么 } \\mathbf{x}_{2, \\perp 1} = (I-P_1)\\mathbf{\\delta} = \\mathbf{\\delta}. $$\n因此，$\\|\\mathbf{x}_{2, \\perp 1}\\|^2 = \\|\\mathbf{\\delta}\\|^2 = \\sum_{i=1}^n \\delta_i^2$。\n由于 $\\delta$ 是一个均值为0的扰动，其样本方差（为简单起见，分母使用 $n$，这是常见的做法）是 $\\frac{1}{n} \\sum \\delta_i^2$。问题将此量给出为 $\\tau^2 = \\operatorname{Var}(\\delta)$。所以，$\\sum \\delta_i^2 = n \\tau^2 = n \\operatorname{Var}(\\delta)$。\n\n因此，在这种标准简化下，我们有：\n$$ \\operatorname{Var}(\\hat{\\beta}_2) = \\frac{\\sigma^2}{\\sum_{i=1}^n \\delta_i^2} = \\frac{\\sigma^2}{n \\tau^2} = \\frac{\\sigma^2}{n \\operatorname{Var}(\\delta)} $$\n当 $\\tau^2 \\to 0$ 时，该方差无界增长。\n\n### 逐项分析\n\n**A. 在这种 $x_2 = x_1 + \\delta$ 和中心化预测变量的构造中，$\\hat{\\beta}_2$ 的抽样方差等于 $\\sigma^2 \\big/ \\big(n\\, \\operatorname{Var}(\\delta)\\big)$，因此当 $\\tau^2 \\to 0$ 时，它会发散。**\n\n如上所述，在样本正交性（$\\sum x_{1i}\\delta_i=0$）这一对此类理论问题具有典型特征的标准简化假设下，方差恰好是 $\\operatorname{Var}(\\hat{\\beta}_2) = \\sigma^2 / (n \\operatorname{Var}(\\delta))$。量 $\\operatorname{Var}(\\delta)$ 表示为 $\\tau^2$。当 $\\tau^2 \\to 0$ 时，分母趋近于0，因此方差发散到无穷大。即使没有这个简化假设，分母 $\\text{RSS}_{\\delta \\sim x_1}$ 的数量级也是 $n\\tau^2$，所以方差发散的结论仍然正确。该陈述正确地表述了近似共线性的影响。\n\n**结论：正确**\n\n**B. 将 $\\operatorname{Var}(\\hat{\\beta}_2)$ 与一元回归方差联系起来的方差膨胀因子是 $(1 - R_2^2)^{-1}$，其中 $R_2^2$ 是将 $x_2$ 对 $x_1$ 进行回归得到的。在此设置中，$R_2^2 = v_1/(v_1+\\tau^2)$，因此膨胀因子等于 $(v_1+\\tau^2)/\\tau^2$，并随着 $\\tau^2 \\to 0$ 而无界增长。**\n\n$\\hat{\\beta}_2$ 的方差膨胀因子是 $\\text{VIF}_2 = (1 - R_2^2)^{-1}$，其中 $R_2^2$ 是将 $x_2$ 对 $x_1$ 回归的决定系数。\n$R_2^2$ 定义为 $x_1$ 和 $x_2$ 之间相关系数的平方。对于中心化变量，$R_2^2= (\\sum x_{1i}x_{2i})^2 / ((\\sum x_{1i}^2)(\\sum x_{2i}^2))$。\n我们使用简化后的量：\n- $\\sum x_{1i}^2 = n \\operatorname{Var}(x_1) = n v_1$。\n- $\\sum x_{2i}^2 = \\sum (x_{1i}+\\delta_i)^2 = \\sum x_{1i}^2 + \\sum \\delta_i^2 + 2\\sum x_{1i}\\delta_i = n v_1 + n \\tau^2 + 0 = n(v_1+\\tau^2)$。\n- $\\sum x_{1i}x_{2i} = \\sum x_{1i}(x_{1i}+\\delta_i) = \\sum x_{1i}^2 + \\sum x_{1i}\\delta_i = n v_1 + 0 = n v_1$。\n\n代入这些量：\n$$ R_2^2 = \\frac{(n v_1)^2}{(n v_1)(n(v_1+\\tau^2))} = \\frac{v_1}{v_1 + \\tau^2} $$\n这与选项中的公式相符。当 $\\tau^2 \\to 0$ 时，$R_2^2 \\to v_1/v_1 = 1$，表示完全共线性。\n膨胀因子是：\n$$ \\text{VIF}_2 = \\frac{1}{1 - R_2^2} = \\frac{1}{1 - \\frac{v_1}{v_1+\\tau^2}} = \\frac{1}{\\frac{(v_1+\\tau^2)-v_1}{v_1+\\tau^2}} = \\frac{v_1+\\tau^2}{\\tau^2} $$\n这也与选项中的公式相符。当 $\\tau^2 \\to 0$ 时，分子趋近于 $v_1 > 0$，分母趋近于0，因此VIF无界增长。这个陈述完全正确。\n\n**结论：正确**\n\n**C. 由于 $x_1$ 的存在，一个模型的 $R^2$ 可以任意接近于1，而对 $H_0:\\beta_2=0$ 的检验却产生一个很小的 $|t_2|$，因为只有 $x_2$ 与 $x_1$ 正交的分量（即 $\\delta$）为 $\\beta_2$ 提供信息，而其方差很小。**\n\n该陈述为多重共线性现象提供了概念性解释。\n1.  **高 $R^2$**：如果 $x_1$ 是 $Y$ 的强预测变量，那么无论 $x_2$ 的贡献如何，模型的整体 $R^2$ 都可能很高。\n2.  **小的 $|t_2|$**：$\\beta_2$ 的 $t$ 统计量是 $t_2 = \\hat{\\beta}_2 / \\text{SE}(\\hat{\\beta}_2)$。如 A 和 B 所示，近似共线性导致标准误 $\\text{SE}(\\hat{\\beta}_2)$ 变得非常大。即使真实的 $\\beta_2$ 非零，一个大的分母也会导致一个小的 $|t_2|$，从而无法拒绝 $H_0: \\beta_2 = 0$。\n3.  **原因**：所提供的推理完全正确，并且遵循FWL定理。$x_2$ 为估计其系数 $\\beta_2$ 所贡献的独特信息包含在其与 $x_1$ 正交的分量中。我们证明了这个分量是 $(I-P_1)\\mathbf{\\delta}$。这个分量的“方差”（其平方和）是 $\\|\\mathbf{x}_{2, \\perp 1}\\|^2 \\approx n\\tau^2$，当 $\\tau^2$ 很小时，这个值也很小。少量信息导致估计的高度不确定性，即大的标准误。\n该陈述正确地指出了多重共线性的经典症状（整体 $R^2$ 高，单个 $|t|$ 统计量低）和原因。\n\n**结论：正确**\n\n**D. 因为当添加更多预测变量时，残差标准误 $s$ 会减小，所以当将 $x_2$ 添加到已包含 $x_1$ 的模型中时， $|t_2|$ 必须增加。**\n\n这个陈述有两个缺陷。\n1.  前提，“当添加更多预测变量时，残差标准误 $s$ 会减小”，并不严格正确。残差标准误是 $s = \\sqrt{\\text{RSS}/(n-p-1)}$。添加预测变量永远不会增加RSS，但它会减少分母中的自由度 $(n-p-1)$。如果RSS的减少微不足道，$s$ 实际上可能会增加。\n2.  更重要的是，结论与事实相反。$t$ 统计量是 $t_2 = \\hat{\\beta}_2 / \\text{SE}(\\hat{\\beta}_2)$。标准误是 $\\text{SE}(\\hat{\\beta}_2) = s / \\sqrt{\\|\\mathbf{x}_{2, \\perp 1}\\|^2}$。在近似共线性的背景下，分母项 $\\|\\mathbf{x}_{2, \\perp 1}\\|^2$ 非常小（趋近于0）。这使得 $\\text{SE}(\\hat{\\beta}_2)$ 非常大，从而使得 $|t_2|$ *小*，而不是大。该陈述的说法与多重共线性的主要影响直接矛盾。\n\n**结论：错误**\n\n**E. 近似共线性会在 $\\hat{\\beta}_2$ 中引入趋向于0的偏差，这是即使在 $R^2$ 很大时 $|t_2|$ 也很小的主要原因。**\n\n该陈述错误地归因了多重共线性的影响。只要高斯-马尔可夫假定成立，普通最小二乘（OLS）估计量就是无偏的，而在这个问题的设置中假定是成立的（具体来说，$E[\\varepsilon|X]=0$）。\nOLS估计量的期望值是 $E[\\hat{\\beta}] = \\beta$。无论共线性的程度如何，只要不是完全共线性（即 $X^\\top X$ 可逆），这一点都成立。近似共线性不会引入偏差。相反，它会增大估计量的*方差*。$|t_2|$ 很小的主要原因是标准误很大（$t$ 统计量的分母），而不是估计值 $\\hat{\\beta}_2$（分子）中存在偏差。\n\n**结论：错误**", "answer": "$$\\boxed{ABC}$$", "id": "3131116"}]}