{"hands_on_practices": [{"introduction": "杠杆点并不总是能通过检查单个预测变量的极端值来识别。一个数据点的杠杆值取决于我们所拟合的具体模型，当模型中包含交互项时，一个在各维度上看似正常的点，可能在其特征组合上表现出极端性，从而成为一个高杠杆点。本练习将通过比较包含与不包含交互项的模型的杠杆值，帮助你深入理解杠杆值的模型依赖性，并学会如何识别这类“隐藏”的高杠杆点。[@problem_id:3154847]", "problem": "考虑一个线性回归情景，其中响应被建模为特征（可能包括交互项）的线性组合。普通最小二乘（OLS）估计量通过最小化残差平方和，得到一个对响应呈线性的拟合值算子。您的任务是实现一个完整的程序，该程序构建包含和不包含刻意构造的极端交互模式的数据集，然后通过包含交互项的增广设计矩阵来检测高杠杆点。\n\n请将推导和算法建立在以下基本定义和事实上：\n- 在一个设计矩阵为 $X \\in \\mathbb{R}^{n \\times p}$ 的线性模型中，OLS 拟合值是通过将一个线性算子应用于响应向量得到的。该拟合值算子可以用一个矩阵（投影矩阵或“帽子矩阵”）表示，它将任何响应向量映射到其在 $X$ 列空间中的预测值。\n- 帽子矩阵的对角线元素量化了杠杆值。观测点 $i$ 的杠杆值是帽子矩阵的第 $i$ 个对角元素，它衡量了在由 $X$ 导出的几何结构中，该观测点的预测变量向量与预测变量云主体的距离。\n- 帽子矩阵的迹等于设计矩阵的列数 $p$，这意味着平均杠杆值为 $p/n$。\n\n除了这些核心事实，您不能假设任何快捷公式。您应从这些原理中推导出您需要的任何工作表达式和算法。\n\n程序要求：\n1. 构建三个合成数据集，每个数据集的响应都由一个包含交互项的模型生成。对于每个数据集，拟合两个模型：一个不含交互项的基础模型和一个包含交互项的增广模型。对于这两个模型，计算所有观测点的杠杆值，并使用准则 $h_{ii} > 2 \\cdot \\bar{h}$ 标记高杠杆点，其中 $\\bar{h}$ 是相应模型的平均杠杆值。\n2. 响应应从模型\n   $$ y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 (x_1 x_2) + \\varepsilon, $$\n   生成，其中参数固定为 $ \\beta_0 = 0.5$，$ \\beta_1 = 1.0$，$ \\beta_2 = -0.7$，$ \\beta_3 = 0.8$，独立噪声为 $ \\varepsilon \\sim \\mathcal{N}(0, \\sigma^2)$，且 $ \\sigma = 0.3$。每个数据集中的所有随机生成必须通过固定的种子来确定，以确保结果是可复现的。\n\n3. 数据集和测试套件。完全按照规定构建以下三个数据集：\n   - 测试用例1（理想路径，相对于低方差主体，具有中等边缘效应但存在极端交互作用）：\n     - 样本大小 $n = 60$。\n     - 随机种子 $s = 12345$。\n     - 独立生成 $x_1$ 和 $x_2$，其中 $x_j \\sim \\mathcal{N}(0, 0.6^2)$，$j \\in \\{1,2\\}$。\n     - 强制将索引为 $i^\\star = 12$ 的单个观测点设置为 $x_1 = 1.8$ 和 $x_2 = 2.2$。这会产生交互作用 $x_1 x_2 \\approx 3.96$，鉴于数据主体方差较低，该交互项相对于典型的 $x_1 x_2$ 尺度而言是极端的。\n   - 测试用例2（边界条件，具有边缘异常值但无极端交互作用）：\n     - 样本大小 $n = 60$。\n     - 随机种子 $s = 24680$。\n     - 独立生成 $x_1$ 和 $x_2$，其中 $x_j \\sim \\mathcal{N}(0, 0.6^2)$。\n     - 强制将索引为 $i^\\star = 7$ 的单个观测点设置为 $x_1 = 4.0$ 和 $x_2 = 0.2$。这会产生交互作用 $x_1 x_2 = 0.8$，相对于数据主体而言并不极端，但 $x_1$ 是一个强边缘异常值。\n   - 测试用例3（边缘情况，近共线性放大了交互作用的杠杆效应）：\n     - 样本大小 $n = 25$。\n     - 随机种子 $s = 31415$。\n     - 独立生成 $x_1 \\sim \\mathcal{N}(0, 0.8^2)$ 并设置 $x_2 = x_1 + \\delta$，其中 $\\delta \\sim \\mathcal{N}(0, 0.1^2)$，从而产生近共线性。\n     - 强制将索引为 $i^\\star = 5$ 的单个观测点设置为 $x_1 = 2.0$ 和 $x_2 = 2.0$，这在小样本、近共线性的背景下导致了较大的交互作用 $x_1 x_2 = 4.0$。\n\n4. 对每个数据集：\n   - 构建基础设计矩阵，其列为 $[1, x_1, x_2]$（截距加主效应）。\n   - 构建增广设计矩阵，其列为 $[1, x_1, x_2, x_1 x_2]$（截距加主效应加交互作用）。\n   - 对每个设计矩阵，计算帽子矩阵及其对角线以获得杠杆值。标记满足 $h_{ii} > 2 \\cdot (p/n)$ 的索引，其中 $p$ 等于所用设计矩阵的列数。\n   - 对每个数据集，返回两个列表：由基础模型标记的已排序索引和由增广模型标记的已排序索引。\n\n5. 最终输出格式：\n   - 您的程序应生成单行输出，其中包含一个方括号内的逗号分隔列表，列表中的每个元素本身是包含两个列表的列表：第一个列表是基础模型标记的索引，第二个列表是增广模型标记的索引。\n   - 具体来说，输出应如下所示\n     $$ [ [ [i_{1,1}, i_{1,2}, \\dots], [j_{1,1}, j_{1,2}, \\dots] ], [ [i_{2,1}, \\dots], [j_{2,1}, \\dots] ], [ [i_{3,1}, \\dots], [j_{3,1}, \\dots] ] ] $$\n     其中索引是按升序排列的整数。\n\n此任务不涉及任何物理单位、角度单位，也不需要在任何地方使用百分比。", "solution": "我们从线性回归的第一性原理开始。在一个设计矩阵为 $X \\in \\mathbb{R}^{n \\times p}$、响应向量为 $y \\in \\mathbb{R}^{n}$ 的线性模型中，普通最小二乘（OLS）估计量旨在寻找参数 $\\hat{\\beta} \\in \\mathbb{R}^{p}$ 以最小化残差平方和\n$$ S(\\beta) = \\| y - X \\beta \\|_2^2. $$\n将 $S(\\beta)$ 对 $\\beta$ 的梯度设为零，得到正规方程组\n$$ X^\\top X \\hat{\\beta} = X^\\top y. $$\n当 $X^\\top X$ 可逆时，唯一解为\n$$ \\hat{\\beta} = (X^\\top X)^{-1} X^\\top y. $$\n若 $X^\\top X$ 不可逆，则可使用 Moore–Penrose 伪逆来获得最小范数解\n$$ \\hat{\\beta} = (X^\\top X)^{+} X^\\top y, $$\n其中 $(\\cdot)^{+}$ 表示伪逆。\n\nOLS 拟合值为\n$$ \\hat{y} = X \\hat{\\beta} = X (X^\\top X)^{+} X^\\top y = H y, $$\n其中\n$$ H = X (X^\\top X)^{+} X^\\top $$\n是投影矩阵（“帽子矩阵”）。矩阵 $H$ 是对称且幂等的，满足 $H^\\top = H$ 和 $H^2 = H$。一个直接且基本的性质是\n$$ \\mathrm{trace}(H) = p, $$\n即 $X$ 的列数。因此，平均杠杆值为\n$$ \\bar{h} = \\frac{1}{n} \\sum_{i=1}^{n} h_{ii} = \\frac{p}{n}, $$\n其中 $h_{ii}$ 表示 $H$ 的第 $i$ 个对角元素，即观测点 $i$ 的杠杆值。\n\n杠杆值量化了在由 $X$ 导出的度量下，一个观测点的预测变量向量与预测变量空间中心的距离。特别是，向 $X$ 添加列（例如，一个交互特征）会扩展特征空间，创造出新的方向，观测点可能沿着这些方向远离数据主体，从而可能增加其杠杆值。一个观测点可能单独表现出中等的主效应 $x_1$ 和 $x_2$，但却有一个极端的交互作用 $x_1 x_2$；在增广空间中，该观测点可能沿着交互维度远离主云团，这会增加其 $h_{ii}$，即使仅凭主效应不会产生高杠杆值。\n\n根据这些原理推导出的算法步骤：\n1. 对每个数据集，构建两个设计矩阵：\n   - 基础设计矩阵 $X_{\\mathrm{base}}$，其列为 $[1, x_1, x_2]$（截距和主效应）。\n   - 增广设计矩阵 $X_{\\mathrm{aug}}$，其列为 $[1, x_1, x_2, x_1 x_2]$（截距、主效应和交互作用）。\n2. 对每个 $X$，计算帽子矩阵 $H = X (X^\\top X)^{+} X^\\top$，使用 $X^\\top X$ 的伪逆以确保数值稳定性。\n3. 提取 $H$ 的对角线作为杠杆值 $h_{ii}$。\n4. 使用基本性质 $\\bar{h} = p/n$，标记满足 $h_{ii} > 2 \\cdot \\bar{h} = 2p/n$ 的索引。乘法常数 $2$ 设定了一个保守的准则，用于突显那些杠杆值远超平均值的观测点，并且该准则完全由从 $X$ 导出的 $p$ 和 $n$ 表示。\n\n数据集的构建：\n- 测试用例1（理想路径）：\n  - 使用种子 $s = 12345$，为 $n = 60$ 的样本独立生成 $x_1, x_2 \\sim \\mathcal{N}(0, 0.6^2)$。强制索引 $i^\\star = 12$ 处的观测点为 $x_1 = 1.8$，$x_2 = 2.2$，产生交互项 $x_1 x_2 \\approx 3.96$，相对于低方差的主体尺度而言较大。\n  - 生成 $y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_1 x_2 + \\varepsilon$，其中 $\\beta_0 = 0.5$，$\\beta_1 = 1.0$，$\\beta_2 = -0.7$，$\\beta_3 = 0.8$，且 $\\varepsilon \\sim \\mathcal{N}(0, 0.3^2)$（独立）。\n- 测试用例2（边界条件，边缘异常值，非极端交互）：\n  - 使用种子 $s = 24680$，为 $n = 60$ 的样本独立生成 $x_1, x_2 \\sim \\mathcal{N}(0, 0.6^2)$。强制索引 $i^\\star = 7$ 处的观测点为 $x_1 = 4.0$，$x_2 = 0.2$，产生交互项 $x_1 x_2 = 0.8$，相对于主体尺度而言不极端，但 $x_1$ 是一个大的边缘异常值。\n  - 按上文方式生成 $y$。\n- 测试用例3（边缘情况，近共线性）：\n  - 使用种子 $s = 31415$，为 $n = 25$ 的样本独立生成 $x_1 \\sim \\mathcal{N}(0, 0.8^2)$ 和 $\\delta \\sim \\mathcal{N}(0, 0.1^2)$。设置 $x_2 = x_1 + \\delta$，以产生近共线性。强制索引 $i^\\star = 5$ 处的观测点为 $x_1 = 2.0$，$x_2 = 2.0$，因此交互项为 $x_1 x_2 = 4.0$，这在小样本、近共线性的背景下是显著的。\n  - 按上文方式生成 $y$。\n\n检测原理：\n- 对于基础设计矩阵 $X_{\\mathrm{base}}$（$p=3$），平均杠杆值为 $\\bar{h} = 3/n$。一个边缘异常值（例如，大的 $x_1$）倾向于在基础模型下增加其杠杆值。然而，一个具有中等边缘值但交互作用较大的观测点在基础模型中并不显得极端，因为交互作用并未在该模型中体现；因此其杠杆值可能保持在平均水平附近。\n- 对于增广设计矩阵 $X_{\\mathrm{aug}}$（$p=4$），平均杠杆值为 $\\bar{h} = 4/n$。交互特征 $x_1 x_2$ 提供了一个新的方向。一个相对于典型尺度具有较大 $x_1 x_2$ 值的观测点将沿着这个方向远离数据主体，从而增加其在 $X_{\\mathrm{aug}}$ 中的杠杆值 $h_{ii}$。因此，即使其边缘分量仅为中等水平，这样的观测点也可能被增广模型标记为高杠杆点。\n\n实现与输出：\n- 对每个数据集，如上所述计算基础模型和增广模型标记的索引。按升序对标记的索引进行排序。\n- 程序必须打印单行输出，其中包含三个测试用例结果的列表。每个结果是一对列表：第一个用于基础模型，第二个用于增广模型。格式为\n  $$ [ [ \\text{base}_1, \\text{aug}_1 ], [ \\text{base}_2, \\text{aug}_2 ], [ \\text{base}_3, \\text{aug}_3 ] ], $$\n  其中每个 $\\text{base}_k$ 和 $\\text{aug}_k$ 是表示测试用例 $k$ 中被标记观测点索引的整数列表。\n\n此过程完全基于 OLS 正规方程组的推导、投影（帽子）矩阵 $H$ 及其基本性质，没有使用任何超出这些核心定义的快捷公式。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef hat_leverages(X: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute leverage values (diagonal of hat matrix) for design matrix X.\n    Uses the pseudoinverse of X^T X for numerical stability.\n    \"\"\"\n    XTX = X.T @ X\n    XTX_pinv = np.linalg.pinv(XTX)\n    H = X @ XTX_pinv @ X.T\n    # Numerical safety: clip tiny negative due to rounding to zero\n    h = np.clip(np.diag(H), 0.0, 1.0)\n    return h\n\ndef flag_high_leverage(h: np.ndarray, p: int) -> np.ndarray:\n    \"\"\"\n    Flag indices where leverage exceeds 2 * (p/n).\n    Returns sorted indices as a numpy array of ints.\n    \"\"\"\n    n = h.shape[0]\n    avg_h = p / n\n    threshold = 2.0 * avg_h\n    idx = np.where(h > threshold)[0]\n    return np.sort(idx)\n\ndef build_case_1():\n    # Test case 1: extreme interaction with moderate marginals relative to low-variance bulk\n    n = 60\n    rng = np.random.default_rng(12345)\n    x1 = rng.normal(0.0, 0.6, size=n)\n    x2 = rng.normal(0.0, 0.6, size=n)\n    # Force special observation\n    i_star = 12\n    x1[i_star] = 1.8\n    x2[i_star] = 2.2\n\n    # True parameters\n    beta0, beta1, beta2, beta3 = 0.5, 1.0, -0.7, 0.8\n    eps = rng.normal(0.0, 0.3, size=n)\n    y = beta0 + beta1 * x1 + beta2 * x2 + beta3 * (x1 * x2) + eps\n\n    X_base = np.column_stack([np.ones(n), x1, x2])\n    X_aug = np.column_stack([np.ones(n), x1, x2, x1 * x2])\n    return X_base, X_aug, y\n\ndef build_case_2():\n    # Test case 2: marginal outlier without extreme interaction\n    n = 60\n    rng = np.random.default_rng(24680)\n    x1 = rng.normal(0.0, 0.6, size=n)\n    x2 = rng.normal(0.0, 0.6, size=n)\n    i_star = 7\n    x1[i_star] = 4.0\n    x2[i_star] = 0.2\n\n    beta0, beta1, beta2, beta3 = 0.5, 1.0, -0.7, 0.8\n    eps = rng.normal(0.0, 0.3, size=n)\n    y = beta0 + beta1 * x1 + beta2 * x2 + beta3 * (x1 * x2) + eps\n\n    X_base = np.column_stack([np.ones(n), x1, x2])\n    X_aug = np.column_stack([np.ones(n), x1, x2, x1 * x2])\n    return X_base, X_aug, y\n\ndef build_case_3():\n    # Test case 3: near collinearity amplifies interaction leverage\n    n = 25\n    rng = np.random.default_rng(31415)\n    x1 = rng.normal(0.0, 0.8, size=n)\n    delta = rng.normal(0.0, 0.1, size=n)\n    x2 = x1 + delta\n    i_star = 5\n    x1[i_star] = 2.0\n    x2[i_star] = 2.0\n\n    beta0, beta1, beta2, beta3 = 0.5, 1.0, -0.7, 0.8\n    eps = rng.normal(0.0, 0.3, size=n)\n    y = beta0 + beta1 * x1 + beta2 * x2 + beta3 * (x1 * x2) + eps\n\n    X_base = np.column_stack([np.ones(n), x1, x2])\n    X_aug = np.column_stack([np.ones(n), x1, x2, x1 * x2])\n    return X_base, X_aug, y\n\ndef solve():\n    # Define the test cases from the problem statement.\n    cases = [build_case_1, build_case_2, build_case_3]\n\n    results = []\n    for build in cases:\n        X_base, X_aug, y = build()\n\n        # Compute leverages\n        h_base = hat_leverages(X_base)\n        h_aug = hat_leverages(X_aug)\n\n        # Flag high leverage indices using threshold 2 * (p/n)\n        base_flags = flag_high_leverage(h_base, p=X_base.shape[1]).tolist()\n        aug_flags = flag_high_leverage(h_aug, p=X_aug.shape[1]).tolist()\n\n        results.append([base_flags, aug_flags])\n\n    # Final print statement in the exact required format.\n    print(str(results))\n\nsolve()\n```", "id": "3154847"}, {"introduction": "在某些情况下，例如当数据存在严重共线性时，经典的普通最小二乘法（OLS）杠杆值可能会产生误导。它可能对偏离共线性主方向的微小扰动异常敏感，却忽略了在主方向上真正极端的点。本练习将构建一个这样的场景，并展示岭回归（Ridge Regression）如何通过其正则化机制，提供一个更稳健的杠杆度量，从而帮助我们更准确地识别数据中的极端观测。[@problem_id:3154882]", "problem": "您被要求构建并分析一个合成的线性回归设定，以揭示在存在强共线性时基于杠杆值的选择方法的行为，并评估岭正则化如何改变这种行为。请纯粹在统计学习领域的数学和算法层面进行研究。此任务的核心在于离群点、杠杆值和影响点之间的关系。\n\n从以下基本基础开始：\n- 普通最小二乘法（OLS）通过最小化线性模型 $y = X \\beta + \\varepsilon$ 的残差平方和来定义，其中 $X \\in \\mathbb{R}^{n \\times p}$，$y \\in \\mathbb{R}^n$，以及 $\\varepsilon \\in \\mathbb{R}^n$。OLS 预测算子是到 $X$ 的列空间上的正交投影，由帽子矩阵 $H = X (X^{\\top} X)^{-1} X^{\\top}$ 给出。观测点 $i$ 的 OLS 杠杆值为 $h_{ii} = H_{ii}$。\n- 岭回归引入了一个惩罚项 $\\lambda \\lVert \\beta \\rVert_2^2$，以在存在共线性的情况下稳定矩阵的逆。对于正则化参数 $\\lambda > 0$，岭回归的帽子矩阵为 $H_{\\lambda} = X \\left( X^{\\top} X + \\lambda I_p \\right)^{-1} X^{\\top}$，其中 $I_p$ 是 $p \\times p$ 的单位矩阵。观测点 $i$ 的岭回归杠杆值为 $h^{(\\lambda)}_{ii} = (H_{\\lambda})_{ii}$。\n- 影响可以通过结合残差和杠杆值的量（例如，库克距离）来评估，但对于本问题中的选择机制，请使用仅基于杠杆值的子抽样。\n\n构建一个玩具数据集 $X \\in \\mathbb{R}^{n \\times p}$，其中包含 $p=2$ 个具有强共线性的预测变量，以及一个响应 $y \\in \\mathbb{R}^n$：\n1. 生成 $n_{\\text{normal}} = 50$ 个“正常”点，其预测变量 $x_1$ 独立地从均值为 $0$、标准差为 $5$ 的正态分布中抽取，并定义 $x_2 = x_1 + \\varepsilon$，其中 $\\varepsilon$ 独立地从均值为 $0$、标准差为 $0.001$ 的正态分布中抽取。这将在直线 $x_2 \\approx x_1$ 周围产生近乎完美的共线性。\n2. 生成 $n_{\\text{jitter}} = 6$ 个“抖动”点，其中 $x_1$ 独立地从与正常点相同的正态分布中抽取，但定义 $x_2 = x_1 + \\delta$，其中固定偏移量 $\\delta = 0.005$，以引入与共线性方向正交的不可忽略的偏差。\n3. 添加一个“坏杠杆”候选点，其坐标为 $x_1^{\\star} = 12$ 和 $x_2^{\\star} = 12$；该点位于主共线性方向的远端，但没有正交偏差。\n\n按上述 1-3 步的顺序堆叠各行，构成 $X$。在计算任何杠杆值之前，将每个预测变量列进行中心化，使其均值为 $0$。通过 $y = \\beta_1 x_1 + \\beta_2 x_2 + \\eta$ 定义响应，其中 $\\beta_1 = 1$，$\\beta_2 = 1$，并且对于除最后一个点 $(x_1^{\\star}, x_2^{\\star})$ 外的所有点，$\\eta$ 均独立地从均值为 $0$、标准差为 $0.1$ 的正态分布中抽取。对于最后一个候选“坏杠杆”点，设置 $y^{\\star} = \\beta_1 x_1^{\\star} + \\beta_2 x_2^{\\star} + 100$，以在该点上产生一个大的残差。\n\n按如下方式实现基于杠杆值的子抽样：\n- 给定一个正则化参数 $\\lambda \\ge 0$，从 $H_{\\lambda}$ 计算杠杆值分数 $h^{(\\lambda)}_{ii}$，约定 $\\lambda = 0$ 对应于 OLS。然后选择杠杆值分数最大的 $m$ 行作为子样本。如果 $X$ 的最后一行（即构建的候选点）在被选中的索引之中，则声明子样本“包含坏点”。\n\n您的程序必须完全按照描述构建数据集，然后评估以下参数对 $(\\lambda, m)$ 的测试套件：\n- 测试用例 1：$(\\lambda, m) = (0, 3)$。\n- 测试用例 2：$(\\lambda, m) = (0.1, 3)$。\n- 测试用例 3：$(\\lambda, m) = (1.0, 2)$。\n- 测试用例 4：$(\\lambda, m) = (5.0, 1)$。\n\n对于每个测试用例，输出一个布尔值，指示子样本是否包含坏点。将所有四个布尔值按顺序聚合为单行，形式为逗号分隔的列表并用方括号括起，例如 $[b_1,b_2,b_3,b_4]$，其中每个 $b_k$ 为 $\\text{True}$ 或 $\\text{False}$。\n\n不涉及物理单位、角度单位或百分比。所有计算必须使用实数进行。数据集的生成必须通过在内部将随机种子固定为一个常数值来确保确定性，以便多次运行的输出是可复现的。最终输出格式必须是与所描述的四个布尔值的列表表示完全匹配的单行文本。", "solution": "用户的请求是一个有效且定义明确的统计学习问题。它要求构建一个特定的合成数据集，以探究共线性、杠杆值和岭正则化之间的相互作用。解决方案涉及根据提供的规则生成此数据集，为普通最小二乘法（OLS）和岭回归计算杠杆值分数，然后基于这些分数为一组给定参数执行选择任务。\n\n### 原理与方法论\n\n这个问题的核心在于理解杠杆值的几何解释以及它如何受到正则化的影响。\n\n**1. 数据集构建与中心化**\n\n首先，我们构建设计矩阵 $X \\in \\mathbb{R}^{n \\times p}$（其中 $n=57$，$p=2$）和响应向量 $y \\in \\mathbb{R}^n$。矩阵 $X$ 由三组不同的点组成：\n-   $n_{\\text{normal}} = 50$ 个表现出强共线性的点，其中 $x_2 \\approx x_1$。\n-   $n_{\\text{jitter}} = 6$ 个与主共线性趋势略有偏离的点，相对于主要数据结构，这些点构成了正交方向上的离群点。\n-   $1$ 个“坏杠杆”候选点 $(x_1^{\\star}, x_2^{\\star}) = (12, 12)$，该点远离数据中心，但完全位于主共线性方向（$x_1 = x_2$）上。\n\n将这些点堆叠成一个矩阵后，一个关键的预处理步骤是对数据进行中心化。设未中心化的数据矩阵为 $X_{\\text{uncentered}}$。中心化后的矩阵 $X$ 按如下方式计算：\n$$\nX_{ij} = (X_{\\text{uncentered}})_{ij} - \\bar{x}_j\n$$\n其中 $\\bar{x}_j$ 是 $X_{\\text{uncentered}}$ 第 $j$ 列的均值。中心化至关重要，因为杠杆值是相对于数据云的中心定义的。所有后续的杠杆值计算都将使用这个中心化后的矩阵 $X$。响应向量 $y$ 是基于未中心化的预测变量生成的，如题目所述，但它不影响杠杆值的计算。\n\n**2. OLS 杠杆值 ($\\lambda=0$)**\n\n在 OLS 模型中，第 $i$ 个观测值的杠杆值由 $h_{ii}$ 给出，即帽子矩阵 $H$ 的第 $i$ 个对角元素：\n$$\nh_{ii} = [H]_{ii} = [X (X^{\\top} X)^{-1} X^{\\top}]_{ii} = x_i^{\\top} (X^{\\top} X)^{-1} x_i\n$$\n这里，$x_i^{\\top}$ 是（中心化后的）设计矩阵 $X$ 的第 $i$ 行。在几何上，$h_{ii}$ 衡量了观测值 $x_i$ 相对于数据中心的偏远程度，并根据数据的协方差结构进行了缩放。在所构建的强共线性情况下，矩阵 $X^{\\top}X$ 近乎奇异，其逆矩阵会给予与数据变化主轴正交的方向很大的权重。“抖动”点被构造成在这个特定的正交方向上成为离群点。因此，对于 OLS（$\\lambda=0$），预计这些抖动点将具有非常高的杠杆值，可能高于“坏杠杆”点，后者尽管数值很大，但符合检测到的共线性结构。\n\n**3. 岭回归杠杆值 ($\\lambda > 0$)**\n\n岭回归将帽子矩阵修改为 $H_{\\lambda}$：\n$$\nH_{\\lambda} = X \\left( X^{\\top} X + \\lambda I_p \\right)^{-1} X^{\\top}\n$$\n观测值 $i$ 的岭回归杠杆值是相应的对角元素 $h_{ii}^{(\\lambda)} = [H_{\\lambda}]_{ii}$。引入正则化项 $\\lambda I_p$ 会产生深远的影响。它在 $X^{\\top}X$ 的对角线上增加了一个常数，使矩阵可逆且条件更好。在几何上，这相当于向数据的协方差结构中添加了球形信息。随着 $\\lambda$ 的增加，$\\lambda I_p$ 项开始主导 $X^{\\top}X$。在 $\\lambda$ 很大的极限情况下，我们有：\n$$\n(X^{\\top}X + \\lambda I_p)^{-1} \\approx (\\lambda I_p)^{-1} = \\frac{1}{\\lambda} I_p\n$$\n因此，岭回归杠杆值 $h_{ii}^{(\\lambda)}$ 近似为：\n$$\nh_{ii}^{(\\lambda)} \\approx x_i^{\\top} \\left( \\frac{1}{\\lambda} I_p \\right) x_i = \\frac{1}{\\lambda} x_i^{\\top} x_i = \\frac{1}{\\lambda} \\|x_i\\|^2_2\n$$\n这表明，对于大的 $\\lambda$，岭回归杠杆值变得与中心化观测向量 $x_i$ 的欧几里得范数的平方成正比。“偏远程度”的度量从由数据的特定协方差结构定义转变为由与中心的简单距离定义。“坏杠杆”点 $(x_1^{\\star}, x_2^{\\star}) = (12, 12)$ 被有意地放置在远离原点的位置，以确保其在中心化后具有非常大的欧几里得范数。\n\n**4. 子抽样与评估**\n\n该算法对每个测试用例 $(\\lambda, m)$ 执行以下步骤：\n1.  使用中心化矩阵 $X$ 和给定的 $\\lambda$ 计算所有 $n=57$ 个数据点的杠杆值 $h_{ii}^{(\\lambda)}$。\n2.  找出杠杆值分数最高的 $m$ 个点的索引。\n3.  检查“坏杠杆”点的索引（即最后一个点，索引为 $56$）是否在这前 $m$ 个索引之中。\n\n基于上述原理：\n-   对于 $\\lambda=0$，抖动点的高结构杠杆值应使它们排在最前面，很可能将“坏”点排除在小的子样本之外。\n-   随着 $\\lambda$ 的增加，“坏”点的杠杆值相对于“抖动”点将会增加，因为它的大范数变得更具影响力。最终，它将进入并主导杠杆值最高的排名。\n\n所提供的测试用例旨在探究这一转变过程。\n-   测试 1 $(\\lambda=0, m=3)$：OLS 杠杆值，预计抖动点会占优。\n-   测试 2 $(\\lambda=0.1, m=3)$：小正则化，可能是一个转变点。\n-   测试 3 $(\\lambda=1.0, m=2)$：中等正则化，欧几里得范数变得更加重要。\n-   测试 4 $(\\lambda=5.0, m=1)$：强正则化，预计范数最大的点将具有最高的杠杆值。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Constructs a synthetic dataset to analyze leverage-based selection\n    under collinearity and ridge regularization, then runs a suite of tests.\n    \"\"\"\n    # Fix the random seed for reproducibility as required.\n    np.random.seed(0)\n\n    # 1. Construct the dataset as per the problem description.\n    # Parameters\n    p = 2\n    n_normal = 50\n    n_jitter = 6\n    n_bad = 1\n    n = n_normal + n_jitter + n_bad\n    \n    # Generate \"normal\" points with near-perfect collinearity\n    x1_normal = np.random.normal(loc=0, scale=5, size=n_normal)\n    eps_normal = np.random.normal(loc=0, scale=0.001, size=n_normal)\n    x2_normal = x1_normal + eps_normal\n    X_normal = np.column_stack((x1_normal, x2_normal))\n\n    # Generate \"jitter\" points with orthogonal deviation\n    x1_jitter = np.random.normal(loc=0, scale=5, size=n_jitter)\n    delta_jitter = 0.005\n    x2_jitter = x1_jitter + delta_jitter\n    X_jitter = np.column_stack((x1_jitter, x2_jitter))\n\n    # Define the \"bad leverage\" candidate point\n    X_bad = np.array([[12.0, 12.0]])\n\n    # Stack the components to form the uncentered data matrix\n    X_uncentered = np.vstack((X_normal, X_jitter, X_bad))\n\n    # Center the predictor columns (essential for leverage calculations)\n    X_mean = X_uncentered.mean(axis=0)\n    X = X_uncentered - X_mean\n\n    # NB: The response vector y is not used for leverage calculation but is\n    # constructed for completeness as specified in the problem.\n    beta = np.array([1.0, 1.0])\n    eta = np.random.normal(loc=0, scale=0.1, size=n)\n    y = X_uncentered @ beta + eta\n    # Adjust the last point to have a large residual\n    y[-1] = X_uncentered[-1] @ beta + 100.0\n\n    # 2. Define a function to compute leverage scores\n    def get_leverages(X_centered, lambda_val):\n        \"\"\"\n        Computes leverage scores for OLS (lambda=0) or Ridge (lambda>0).\n        Leverage is the diagonal of the hat matrix.\n        \"\"\"\n        _n, _p = X_centered.shape\n        XTX = X_centered.T @ X_centered\n        \n        if lambda_val == 0:\n            # OLS case: H = X (X^T X)^-1 X^T\n            # np.linalg.inv is fine for p=2\n            hat_matrix = X_centered @ np.linalg.inv(XTX) @ X_centered.T\n        else:\n            # Ridge case: H_lambda = X (X^T X + lambda*I)^-1 X^T\n            identity = np.identity(_p)\n            hat_matrix = X_centered @ np.linalg.inv(XTX + lambda_val * identity) @ X_centered.T\n            \n        # The leverage scores are the diagonal elements of the hat matrix\n        return np.diag(hat_matrix)\n\n    # 3. Evaluate the test suite\n    test_cases = [\n        (0, 3),    # Case 1: lambda=0, m=3\n        (0.1, 3),  # Case 2: lambda=0.1, m=3\n        (1.0, 2),  # Case 3: lambda=1.0, m=2\n        (5.0, 1),  # Case 4: lambda=5.0, m=1\n    ]\n\n    results = []\n    bad_point_index = n - 1 # The last point is the \"bad leverage\" one\n\n    for lam, m in test_cases:\n        # Compute leverage scores for the current lambda\n        leverages = get_leverages(X, lam)\n        \n        # Get the indices of the top m leverage scores\n        # np.argsort returns indices from smallest to largest\n        top_m_indices = np.argsort(leverages)[-m:]\n        \n        # Check if the bad point's index is in the top m set\n        is_bad_point_included = bad_point_index in top_m_indices\n        results.append(is_bad_point_included)\n\n    # 4. Format and print the final output\n    # Convert booleans to strings (\"True\" or \"False\") and join\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3154882"}, {"introduction": "在处理多响应变量的回归问题时，逐个分析每个响应变量的影响点是不够的。一个观测可能对每个单独的响应变量影响不大，但其对整个模型系数矩阵的联合影响却可能非常显著。本练习将引导你从第一性原理出发，实现多变量版本的库克距离（Cook's Distance），并将其与单变量诊断进行比较，从而让你体会到在多维世界中整体大于部分之和的道理。[@problem_id:3154904]", "problem": "您面临一个多响应线性回归的场景，其中响应矩阵包含多个列，并且对于某些测试用例，有一个观测值在所有响应上都表现出极端性。您的任务是实现一个程序，为每个测试用例构建一个合成但科学上合理的数据集，使用普通最小二乘法（OLS）拟合模型，计算每个观测值的多变量库克距离，计算每个响应维度的单变量库克距离，并对这些诊断结果进行比较。\n\n推导的基础必须从标准的多响应线性模型、OLS正规方程、投影（帽子）矩阵、残差和残差协方差估计量出发，而不依赖于简化的影响公式。设 $n$ 表示观测值的数量，$p$ 表示包括截距在内的预测变量数量，$m$ 表示响应变量的数量。模型为 $Y \\in \\mathbb{R}^{n \\times m}$，其行向量为 $y_i^\\top$；预测变量矩阵为 $X \\in \\mathbb{R}^{n \\times p}$，其行向量为 $x_i^\\top$；系数矩阵为 $B \\in \\mathbb{R}^{p \\times m}$；以及加性噪声为 $E \\in \\mathbb{R}^{n \\times m}$。您必须使用从正规方程推导出的OLS估计量来拟合模型，并使用残差协方差估计量来缩放多变量影响度量。除了线性模型和最小二乘框架外，不要假设任何特殊的分布捷径。\n\n对于每个测试用例，按如下方式生成数据：\n- 创建矩阵 $X$，其中包含一列全为1的截距项和 $p - 1$ 个标准正态特征。\n- 从标准正态分布中抽取 $B$ 的元素。\n- 从均值为 $0$ 和指定标准差的正态分布中独立抽取噪声 $E$ 的元素。\n- 构建 $Y = X B + E$。\n- 对指定的观测值索引 $i^\\*$ 应用指定的极端配置：\n    - 高杠杆修改：将 $X$ 矩阵中第 $i^\\*$ 行的非截距特征乘以给定因子。\n    - 极端响应修改：为 $Y$ 矩阵中第 $i^\\*$ 行的所有 $m$ 个响应添加一个恒定的偏移量。\n- 在修改了第 $i^\\*$ 行的 $X$ 之后，在应用任何响应偏移之前，确保将 $Y_{i^\\* \\cdot}$ 重新计算为 $x_{i^\\*}^\\top B + e_{i^\\*}$，从而在进行极端响应偏移之前，$Y$ 与“线性模型+噪声”保持内部一致性。\n\n您必须计算：\n1. 每个观测值的多变量库克距离，其构造应基于案例删除下OLS解的变化并由残差协方差进行缩放；并通过一个布尔值总结每个测试用例，该布尔值指示指定的极端观测值 $i^\\*$ 是否获得了最大的多变量库克距离。\n2. 每个响应维度上每个观测值的单变量库克距离，使用标准OLS量计算；并通过一个整数总结每个测试用例，该整数计算了 $m$ 个单变量响应中有多少个将 $i^\\*$ 识别为库克距离最大的观测值。\n\n您的程序必须从第一性原理实现计算，不得依赖任何内置的统计学习例程。最终输出必须将所有测试用例的这些摘要聚合为单行，格式为方括号内以逗号分隔的列表。\n\n测试套件：\n- 案例 $1$（理想情况：极端残差但无高杠杆）：\n    - $n = 40$，$p = 3$（一个截距加 $2$ 个特征），$m = 3$，噪声标准差 $1.0$，随机种子 $12345$，极端值索引 $i^\\* = 5$，高杠杆乘数 $1.0$（无杠杆变化），极端响应偏移量 $12.0$。\n- 案例 $2$（边界情况：高杠杆但无极端残差）：\n    - $n = 40$，$p = 3$，$m = 3$，噪声标准差 $1.0$，随机种子 $12346$，极端值索引 $i^\\* = 10$，高杠杆乘数 $12.0$，极端响应偏移量 $0.0$。\n- 案例 $3$（边缘情况：既有高杠杆又有极端残差）：\n    - $n = 40$，$p = 3$，$m = 3$，噪声标准差 $1.0$，随机种子 $12347$，极端值索引 $i^\\* = 20$，高杠杆乘数 $12.0$，极端响应偏移量 $12.0$。\n\n答案格式：\n- 对于每个案例，产生两个值：\n    - 一个布尔值，表示指定的 $i^\\*$ 是否为多变量库克距离的 argmax。\n    - 一个整数，等于 $i^\\*$ 在其中是单变量库克距离 argmax 的响应维度计数。\n- 您的程序应生成单行输出，包含这六个结果，按测试用例的顺序，格式为以逗号分隔的列表并用方括号括起来：$[\\text{case1\\_multivariate\\_bool},\\text{case1\\_univariate\\_count},\\text{case2\\_multivariate\\_bool},\\text{case2\\_univariate\\_count},\\text{case3\\_multivariate\\_bool},\\text{case3\\_univariate\\_count}]$。\n\n此问题不要求任何物理单位、角度单位或百分比，所有输出必须是布尔或整数类型。通过严格遵守线性模型假设和使用指定种子进行可复现的随机生成，确保科学真实性。", "solution": "我们从多响应线性模型开始，该模型有 $n$ 个观测值，$p$ 个包括截距的预测变量，以及 $m$ 个响应变量。设 $Y \\in \\mathbb{R}^{n \\times m}$ 为响应矩阵，$X \\in \\mathbb{R}^{n \\times p}$ 为预测变量矩阵，并假设模型为 $Y = X B + E$，其中 $B \\in \\mathbb{R}^{p \\times m}$ 是待估计的系数矩阵，$E$ 是噪声。\n\n估计的基本方法是普通最小二乘法（OLS）。OLS估计量是通过最小化残差的弗罗贝尼乌斯范数得到的，这会导出正规方程。经过充分检验的OLS解的公式是\n$$\n\\hat{B} = (X^\\top X)^{-1} X^\\top Y,\n$$\n假设 $X^\\top X$ 可逆。在实践中，必要时可以使用Moore–Penrose伪逆，但由于特征是随机生成的且 $n \\gg p$，通常是可逆的。拟合值矩阵是 $\\hat{Y} = X \\hat{B}$，残差矩阵是 $\\hat{E} = Y - \\hat{Y}$。残差协方差估计量由下式给出\n$$\n\\hat{S} = \\frac{\\hat{E}^\\top \\hat{E}}{n - p},\n$$\n这是一个经过充分检验的噪声协方差估计量，它假设在多变量线性模型内具有同方差性。\n\n杠杆率是一个基本概念：投影或帽子矩阵 $H$ 是\n$$\nH = X (X^\\top X)^{-1} X^\\top,\n$$\n杠杆值为 $h_{ii} = H_{ii}$。高杠杆率表示一个观测值的预测变量值不寻常，这可能对拟合的模型产生强烈影响。\n\n为了进行影响比较，我们考虑删除观测值 $i$ 的效果。定义留一法估计量 $\\hat{B}_{(i)}$，它是从移除了第 $i$ 行的 $X$ 和 $Y$ 计算得出的。系数估计的变化量为 $\\Delta_i = \\hat{B} - \\hat{B}_{(i)}$。一个有原则的多变量影响度量使用 $X^\\top X$ 几何结构和逆残差协方差，聚合了这种变化在所有响应上的缩放二次型，\n$$\nM_i = \\Delta_i^\\top (X^\\top X) \\Delta_i \\in \\mathbb{R}^{m \\times m}, \\quad \\text{以及} \\quad D_i^{\\text{multi}} = \\frac{1}{m p} \\operatorname{tr}\\!\\left(M_i \\hat{S}^{-1}\\right).\n$$\n这种构造推广了单变量情况下的 $X^\\top X$ 加权和均方误差归一化，通过适当地使用跨越 $m$ 个响应维度的逆残差协方差进行缩放，并除以 $m p$，使得在 $m=1$ 的特殊情况下，它简化为我们所熟悉的单变量情况下的缩放方式，即除以 $p$ 倍均方误差。\n\n对于单变量诊断，当考虑单个响应 $y \\in \\mathbb{R}^n$ 时，OLS残差向量是 $\\hat{e} = y - X \\hat{\\beta}$，均方误差（MSE）是\n$$\n\\text{MSE} = \\frac{\\hat{e}^\\top \\hat{e}}{n - p},\n$$\n一个公认的影响度量是库克距离，\n$$\nD_i^{\\text{uni}} = \\frac{\\hat{e}_i^2}{p \\cdot \\text{MSE}} \\cdot \\frac{h_{ii}}{(1 - h_{ii})^2}.\n$$\n该表达式源于留一法拟合值变化与残差 $ \\hat{e}_i $ 及杠杆值 $ h_{ii} $ 之间的精确关系，该关系可通过分块矩阵求逆或Sherman–Morrison恒等式推导得出。\n\n算法设计：\n- 对于每个测试用例，构造 $X$，抽取 $B$，抽取噪声 $E$，形成 $Y = X B + E$，对 $X$ 和/或 $Y$ 应用指定的极端配置，并通过在应用极端响应偏移之前将 $Y_{i^\\* \\cdot}$ 重新计算为 $x_{i^\\*}^\\top B + e_{i^\\*}$ 来确保内部一致性。\n- 拟合多变量OLS以获得 $\\hat{B}$、残差 $\\hat{E}$ 和 $\\hat{S}$，以及 $X^\\top X$。\n- 对于每个观测值 $i$，通过移除第 $i$ 行并重新拟合来计算留一法估计量 $\\hat{B}_{(i)}$，计算 $\\Delta_i = \\hat{B} - \\hat{B}_{(i)}$，形成 $M_i = \\Delta_i^\\top (X^\\top X) \\Delta_i$，并计算多变量库克距离 $D_i^{\\text{multi}} = \\frac{1}{m p} \\operatorname{tr}(M_i \\hat{S}^{-1})$。\n- 为了计算单变量库克距离，对于每个响应列 $y^{(j)}$，计算 $\\hat{\\beta}^{(j)}$、残差 $\\hat{e}^{(j)}$、该响应的MSE以及来自公共帽子矩阵 $H$ 的杠杆值 $h_{ii}$，然后应用单变量公式以获得所有 $i$ 的 $D_i^{\\text{uni}, j}$。\n- 对于每个案例，确定指定的极端索引 $i^\\*$ 是否为多变量库克距离的argmax，并统计在多少个响应维度中 $i^\\*$ 是单变量库克距离的argmax。\n\n测试套件中的解释预期：\n- 案例 $1$ 呈现一个在所有响应中都有极端残差但没有高杠杆的观测值，这应该会被多变量和单变量库克距离强烈标记。\n- 案例 $2$ 呈现一个有高杠杆但没有极端残差的观测值；仅有杠杆而没有大残差通常不会产生高库克距离，因此影响可能不大，也不会是最大的。\n- 案例 $3$ 呈现了高杠杆和极端残差，这两者共同作用应该在多变量和单变量诊断中都产生巨大影响。\n\n程序按确切指定的格式输出单行，聚合三个案例中 $i^\\*$ 的多变量识别布尔值和 $i^\\*$ 的单变量识别整数计数。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef fit_multivariate_ols(X, Y):\n    \"\"\"\n    Fit multiresponse OLS: returns B_hat, residual covariance S_hat, and XTX.\n    \"\"\"\n    # Use explicit normal equations with pinv for numerical stability\n    X_pinv = np.linalg.pinv(X)\n    B_hat = X_pinv @ Y\n    Y_hat = X @ B_hat\n    E_hat = Y - Y_hat\n    n, p = X.shape\n    # Residual covariance estimator\n    S_hat = (E_hat.T @ E_hat) / (n - p)\n    XTX = X.T @ X\n    return B_hat, S_hat, XTX\n\ndef multivariate_cooks_distance(X, Y):\n    \"\"\"\n    Compute multivariate Cook's distance per observation using case deletion.\n    D_i = (1/(m*p)) * tr( (Delta_i^T X^T X Delta_i) S_hat^{-1} ).\n    \"\"\"\n    n, p = X.shape\n    m = Y.shape[1]\n    B_hat, S_hat, XTX = fit_multivariate_ols(X, Y)\n    S_inv = np.linalg.inv(S_hat)\n    D = np.zeros(n)\n    for i in range(n):\n        # Delete observation i\n        X_minus_i = np.delete(X, i, axis=0)\n        Y_minus_i = np.delete(Y, i, axis=0)\n        # Refit\n        B_minus_i = np.linalg.pinv(X_minus_i) @ Y_minus_i\n        Delta = B_hat - B_minus_i  # p x m\n        M = Delta.T @ XTX @ Delta  # m x m\n        D[i] = (np.trace(M @ S_inv)) / (m * p)\n    return D\n\ndef univariate_cooks_distance(X, y):\n    \"\"\"\n    Compute univariate Cook's distance for a single response vector y.\n    Uses D_i = (e_i^2/(p*MSE)) * (h_ii/(1 - h_ii)^2).\n    \"\"\"\n    n, p = X.shape\n    # OLS fit\n    beta_hat = np.linalg.pinv(X) @ y\n    y_hat = X @ beta_hat\n    e = y - y_hat\n    # Hat matrix and leverages\n    H = X @ np.linalg.pinv(X)\n    h = np.clip(np.diag(H), 0.0, 1.0)\n    # Mean Squared Error\n    MSE = float(e.T @ e) / (n - p)\n    # Cook's distance\n    denom = (1.0 - h) ** 2\n    # Avoid division by zero with safe handling\n    safe = denom > 1e-12\n    D = np.zeros(n)\n    D[safe] = ( (e[safe] ** 2) / (p * MSE) ) * ( h[safe] / denom[safe] )\n    # If denom is extremely small, set influence to a large number to reflect instability\n    D[~safe] = np.inf\n    return D\n\ndef generate_case(n, p, m, seed, extreme_index, leverage_multiplier, response_offset, noise_std):\n    \"\"\"\n    Generate a synthetic multiresponse regression case.\n    X: intercept + (p-1) standard normal features.\n    B: standard normal coefficients.\n    E: normal noise with std noise_std.\n    Apply leverage and response modifications at extreme_index.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    d_features = p - 1\n    X_features = rng.normal(0.0, 1.0, size=(n, d_features))\n    X = np.hstack([np.ones((n, 1)), X_features])\n    B = rng.normal(0.0, 1.0, size=(p, m))\n    # Noise\n    E = rng.normal(0.0, noise_std, size=(n, m))\n    # Apply leverage modification on X at extreme_index\n    if leverage_multiplier != 1.0:\n        X[extreme_index, 1:] *= leverage_multiplier\n    # Build Y consistent with X and noise\n    Y = X @ B + E\n    # Now apply response offset (extreme in all responses) at extreme_index\n    if response_offset != 0.0:\n        Y[extreme_index, :] += response_offset\n    return X, Y, B\n\ndef analyze_case(n, p, m, seed, extreme_index, leverage_multiplier, response_offset, noise_std):\n    \"\"\"\n    For a given case, compute multivariate Cook's distances and univariate counts.\n    Returns:\n      bool indicating if extreme_index is argmax of multivariate Cook's distance,\n      int count of univariate responses where extreme_index is argmax of Cook's distance.\n    \"\"\"\n    X, Y, _ = generate_case(n, p, m, seed, extreme_index, leverage_multiplier, response_offset, noise_std)\n    # Multivariate Cook's distances\n    D_multi = multivariate_cooks_distance(X, Y)\n    extreme_is_max_multi = int(np.argmax(D_multi)) == extreme_index\n    # Univariate Cook's distances per response\n    count_univariate_max = 0\n    for j in range(m):\n        D_uni = univariate_cooks_distance(X, Y[:, j])\n        if int(np.argmax(D_uni)) == extreme_index:\n            count_univariate_max += 1\n    return bool(extreme_is_max_multi), int(count_univariate_max)\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each tuple: (n, p, m, seed, extreme_index, leverage_multiplier, response_offset, noise_std)\n    test_cases = [\n        (40, 3, 3, 12345, 5, 1.0, 12.0, 1.0),   # Case 1: extreme residual, no leverage\n        (40, 3, 3, 12346, 10, 12.0, 0.0, 1.0),  # Case 2: high leverage, no residual extreme\n        (40, 3, 3, 12347, 20, 12.0, 12.0, 1.0), # Case 3: high leverage and extreme residual\n    ]\n\n    results = []\n    for case in test_cases:\n        n, p, m, seed, extreme_index, lev_mult, resp_off, noise_std = case\n        is_max_multi, uni_count = analyze_case(n, p, m, seed, extreme_index, lev_mult, resp_off, noise_std)\n        results.append(is_max_multi)\n        results.append(uni_count)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3154904"}]}