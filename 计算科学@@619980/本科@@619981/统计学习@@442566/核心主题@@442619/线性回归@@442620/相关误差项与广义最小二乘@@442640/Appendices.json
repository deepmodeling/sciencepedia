{"hands_on_practices": [{"introduction": "理论告诉我们，当线性模型的误差项存在相关性时，广义最小二乘法 (GLS) 是比普通最小二乘法 (OLS) 更有效的估计方法。然而，在某些特定条件下，OLS 估计量可以与 GLS 估计量完全相同。这个实践练习将引导我们通过数值计算，探索这些等价条件，并直观地感受当误差结构偏离这些条件时，两种估计量之间的差异是如何产生的。通过编写代码来处理具体案例，我们将加深对 OLS 何时依然有效以及 GLS 何时成为必需的理解。[@problem_id:3112106]", "problem": "给定一个可能带有相关误差项的线性模型：$y = X \\beta + \\varepsilon$，其中 $X \\in \\mathbb{R}^{n \\times p}$ 是满列秩矩阵，$y \\in \\mathbb{R}^{n}$，误差向量 $\\varepsilon$ 的均值为 $0$，协方差矩阵 $\\Sigma \\in \\mathbb{R}^{n \\times n}$ 是对称正定矩阵。普通最小二乘 (OLS) 估计量定义为未加权平方损失的最小化子，而广义最小二乘 (GLS) 估计量定义为由逆协方差加权的平方损失的最小化子。您的任务是实现一个程序，对于一个固定的数据集和一组协方差设定，计算 OLS 和 GLS 估计量之间差值的欧几里得范数，以评估它们在何种情况下完全重合，以及 GLS 对精确相等条件的微小偏离有多敏感。\n\n推导与实现的基本依据：\n- OLS 估计量是函数 $\\beta \\mapsto \\|y - X \\beta\\|_{2}^{2}$ 的唯一最小化子。\n- GLS 估计量是函数 $\\beta \\mapsto (y - X \\beta)^{\\top} \\Sigma^{-1} (y - X \\beta)$ 的唯一最小化子。\n\n数据集（所有测试用例中固定）：\n- 样本量和特征数：$n = 6$，$p = 2$。\n- 设计矩阵：\n$$\nX = \\begin{bmatrix}\n1  0 \\\\\n1  1 \\\\\n1  2 \\\\\n1  3 \\\\\n1  4 \\\\\n1  5\n\\end{bmatrix}.\n$$\n- 响应向量：\n$$\ny = \\begin{bmatrix}\n1 \\\\\n2 \\\\\n2 \\\\\n4 \\\\\n3 \\\\\n5\n\\end{bmatrix}.\n$$\n\n协方差矩阵 $\\Sigma$ 的测试集：\n1. 精确相等，球形误差：$\\Sigma_{1} = \\sigma^{2} I$，其中 $\\sigma^{2} = 2.0$，$I$ 是 $n \\times n$ 的单位矩阵。\n2. 精确相等，一般条件：$\\Sigma_{2} = \\sigma^{2} I + X \\Lambda X^{\\top}$，其中 $\\sigma^{2} = 1.5$ 且\n$$\n\\Lambda = \\begin{bmatrix}\n0.7  0.2 \\\\\n0.2  0.5\n\\end{bmatrix}.\n$$\n3. 近似相等，与 $\\operatorname{col}(X)$ 正交的微小扰动：$\\Sigma_{3} = \\Sigma_{2} + \\epsilon \\, v v^{\\top}$，其中 $\\epsilon = 10^{-6}$ 且\n$$\nv = \\begin{bmatrix}\n1 \\\\\n-2 \\\\\n1 \\\\\n0 \\\\\n0 \\\\\n0\n\\end{bmatrix},\n$$\n其中 $v$ 与 $X$ 的每一列都正交。\n4. 弱自相关：$\\Sigma_{4}$ 是参数为 $\\rho = 0.1$、方差为 $\\sigma^{2} = 1.0$ 的 1 阶自回归 (AR(1)) 协方差矩阵，即对于所有索引 $i,j$，其元素为 $(\\Sigma_{4})_{ij} = \\sigma^{2} \\rho^{|i - j|}$。\n5. 强自相关：$\\Sigma_{5}$ 是参数为 $\\rho = 0.8$、方差为 $\\sigma^{2} = 1.0$ 的 AR(1) 协方差矩阵，即对于所有索引 $i,j$，其元素为 $(\\Sigma_{5})_{ij} = \\sigma^{2} \\rho^{|i - j|}$。\n\n您的程序必须：\n- 计算作为未加权平方损失的唯一最小化子的 OLS 估计量 $\\hat{\\beta}_{\\mathrm{OLS}}$。\n- 对于测试集中的每个 $\\Sigma$，计算作为 $\\Sigma^{-1}$ 加权平方损失的唯一最小化子的 GLS 估计量 $\\hat{\\beta}_{\\mathrm{GLS}}(\\Sigma)$。\n- 对每个测试用例，计算差值的欧几里得范数 $\\|\\hat{\\beta}_{\\mathrm{OLS}} - \\hat{\\beta}_{\\mathrm{GLS}}(\\Sigma)\\|_{2}$。\n- 生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，列表中的每个值按 $\\Sigma_{1}, \\Sigma_{2}, \\Sigma_{3}, \\Sigma_{4}, \\Sigma_{5}$ 的顺序排列并四舍五入到 $8$ 位小数。\n\n覆盖设计：\n- 用例 $\\Sigma_{1}$ 测试 OLS 和 GLS 完全重合的基线条件。\n- 用例 $\\Sigma_{2}$ 测试一个非平凡的精确相等条件。\n- 用例 $\\Sigma_{3}$ 测试在与 $X$ 列正交的方向上，对一个违反精确相等条件的微小扰动的敏感性。\n- 用例 $\\Sigma_{4}$ 和 $\\Sigma_{5}$ 测试在结构化相关（弱和强 AR(1)）下，与相等条件的偏离程度逐渐增加的情况。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含一个由方括号括起来的五个结果的逗号分隔列表（例如 $[r_{1},r_{2},r_{3},r_{4},r_{5}]$），其中每个 $r_{k}$ 是一个四舍五入到 $8$ 位小数的浮点数。不得打印任何其他文本。", "solution": "用户希望通过计算在线性模型中普通最小二乘 (OLS) 和广义最小二乘 (GLS) 估计量在不同误差协方差矩阵设定下差值的欧几里得范数，来对这两种估计量进行比较。\n\n### 第 1 步：理论公式\n\n线性模型由 $y = X \\beta + \\varepsilon$ 给出，其中 $y \\in \\mathbb{R}^{n}$ 是响应向量，$X \\in \\mathbb{R}^{n \\times p}$ 是满列秩的设计矩阵，$\\beta \\in \\mathbb{R}^{p}$ 是系数向量，$\\varepsilon \\in \\mathbb{R}^{n}$ 是误差向量，其均值 $\\mathbb{E}[\\varepsilon] = 0$，协方差矩阵 $\\operatorname{Cov}(\\varepsilon) = \\Sigma$ 是一个对称正定矩阵。\n\n**普通最小二乘 (OLS) 估计量**\n\nOLS 估计量 $\\hat{\\beta}_{\\mathrm{OLS}}$ 通过最小化残差平方和 (RSS) 求得：\n$$\n\\text{RSS}(\\beta) = (y - X\\beta)^{\\top}(y - X\\beta) = \\| y - X\\beta \\|_{2}^{2}\n$$\n为求最小值，我们对 $\\beta$ 求导并将梯度设为零：\n$$\n\\frac{\\partial \\text{RSS}}{\\partial \\beta} = -2X^{\\top}(y - X\\beta) = 0\n$$\n$$\nX^{\\top}y - X^{\\top}X\\beta = 0 \\implies X^{\\top}X\\beta = X^{\\top}y\n$$\n由于假设 $X$ 是满列秩的，矩阵 $X^{\\top}X$ 是可逆的。因此，唯一的 OLS 估计量是：\n$$\n\\hat{\\beta}_{\\mathrm{OLS}} = (X^{\\top}X)^{-1}X^{\\top}y\n$$\n\n**广义最小二乘 (GLS) 估计量**\n\nGLS 估计量 $\\hat{\\beta}_{\\mathrm{GLS}}$ 考虑了误差协方差结构，通过最小化加权残差平方和来求得：\n$$\n\\text{RSS}_{\\Sigma}(\\beta) = (y - X\\beta)^{\\top}\\Sigma^{-1}(y - X\\beta)\n$$\n这可以看作是对一个转换后的模型执行 OLS。由于 $\\Sigma$ 是对称正定矩阵，其逆矩阵 $\\Sigma^{-1}$ 也是。我们可以找到一个矩阵 $L$ 使得 $\\Sigma^{-1} = L^{\\top}L$（例如，通过 Cholesky 分解）。目标函数变为：\n$$\n\\text{RSS}_{\\Sigma}(\\beta) = (y - X\\beta)^{\\top}L^{\\top}L(y - X\\beta) = \\|L(y - X\\beta)\\|_{2}^{2} = \\|y' - X'\\beta\\|_{2}^{2}\n$$\n其中 $y' = Ly$ 且 $X' = LX$。这个转换后问题的 OLS 解是：\n$$\n\\hat{\\beta}_{\\mathrm{GLS}} = ((X')^{\\top}X')^{-1}(X')^{\\top}y' = (X^{\\top}L^{\\top}LX)^{-1}X^{\\top}L^{\\top}Ly\n$$\n将 $\\Sigma^{-1} = L^{\\top}L$ 代回，我们得到 GLS 估计量：\n$$\n\\hat{\\beta}_{\\mathrm{GLS}}(\\Sigma) = (X^{\\top}\\Sigma^{-1}X)^{-1}X^{\\top}\\Sigma^{-1}y\n$$\n\n### 第 2 步：OLS 与 GLS 等价的条件\n\nOLS 和 GLS 估计量是相同的，即 $\\hat{\\beta}_{\\mathrm{OLS}} = \\hat{\\beta}_{\\mathrm{GLS}}$，当且仅当设计矩阵的列空间 $\\operatorname{col}(X)$ 是协方差矩阵 $\\Sigma$ 的一个不变子空间。这意味着对于任何向量 $z \\in \\operatorname{col}(X)$，我们必须有 $\\Sigma z \\in \\operatorname{col}(X)$。陈述此条件的一个实用方法是，必须存在一个 $p \\times p$ 矩阵 $M$ 使得 $\\Sigma X = X M$。\n\n让我们根据这个条件来检验前三个测试用例。\n1.  $\\Sigma_1 = \\sigma^2 I$：此处，$\\Sigma_1 X = (\\sigma^2 I) X = X(\\sigma^2 I_p)$。当 $M = \\sigma^2 I_p$ 时，条件成立，因此 $\\hat{\\beta}_{\\mathrm{OLS}} = \\hat{\\beta}_{\\mathrm{GLS}}(\\Sigma_1)$。\n2.  $\\Sigma_2 = \\sigma^2 I + X \\Lambda X^\\top$：此处，$\\Sigma_2 X = (\\sigma^2 I + X \\Lambda X^\\top)X = \\sigma^2 X + X\\Lambda(X^\\top X) = X(\\sigma^2 I_p + \\Lambda(X^\\top X))$。当 $M = \\sigma^2 I_p + \\Lambda(X^\\top X)$ 时，条件成立，因此 $\\hat{\\beta}_{\\mathrm{OLS}} = \\hat{\\beta}_{\\mathrm{GLS}}(\\Sigma_2)$。\n3.  $\\Sigma_3 = \\Sigma_2 + \\epsilon v v^\\top$：给定向量 $v$ 与 $X$ 的每一列正交，这意味着 $X^\\top v = 0$，因此 $v^\\top X = (X^\\top v)^\\top = 0$。于是，$\\Sigma_3 X = (\\Sigma_2 + \\epsilon v v^\\top)X = \\Sigma_2 X + \\epsilon v(v^\\top X) = \\Sigma_2 X + 0 = \\Sigma_2 X$。由于不变性条件对 $\\Sigma_2$ 成立，它也必然对 $\\Sigma_3$ 成立。因此，我们预期 $\\hat{\\beta}_{\\mathrm{OLS}} = \\hat{\\beta}_{\\mathrm{GLS}}(\\Sigma_3)$。\n\n对于用例 4 和 5，自回归协方差结构通常不满足此不变性，所以我们预期估计量之间存在非零差异。\n\n### 第 3 步：计算步骤\n\n该解决方案通过以下步骤实现：\n1.  将给定的数据集、设计矩阵 $X$ 和响应向量 $y$ 定义为数值数组。样本量为 $n=6$，特征数量为 $p=2$。\n2.  使用其闭式解公式一次性计算 OLS 估计量 $\\hat{\\beta}_{\\mathrm{OLS}}$。为保证数值稳定性，直接求解线性系统 $X^{\\top}X \\beta = X^{\\top}y$，而不是显式地计算 $(X^{\\top}X)^{-1}$。\n3.  遍历协方差矩阵 $\\Sigma$ 的五个指定测试用例：\n    a. 对每个用例，构造相应的 $6 \\times 6$ 矩阵 $\\Sigma_k$。\n    b. 对于具有 AR(1) 结构的用例 $\\Sigma_4$ 和 $\\Sigma_5$，其协方差矩阵是一个对称托普利茨矩阵，其中 $(\\Sigma)_{ij} = \\sigma^2 \\rho^{|i-j|}$。\n    c. 使用其公式计算 GLS 估计量 $\\hat{\\beta}_{\\mathrm{GLS}}(\\Sigma_k)$。同样，为保证稳定性，直接求解线性系统 $(X^{\\top}\\Sigma_k^{-1}X)\\beta = X^{\\top}\\Sigma_k^{-1}y$。\n4.  对每个用例，计算差向量的欧几里得范数 $\\|\\hat{\\beta}_{\\mathrm{OLS}} - \\hat{\\beta}_{\\mathrm{GLS}}(\\Sigma_k)\\|_{2}$。\n5.  收集五个范数值，并按规定格式化为单个字符串。\n\n理论分析预测前三个范数将为零（或在浮点精度限制下接近于零），而后两个范数将为非零，且 $\\Sigma_5$（$\\rho=0.8$）的范数预期将大于 $\\Sigma_4$（$\\rho=0.1$）的范数。", "answer": "```python\nimport numpy as np\nfrom scipy.linalg import toeplitz\n\ndef solve():\n    \"\"\"\n    Computes the Euclidean norm of the difference between OLS and GLS estimators\n    for a given dataset and five different error covariance structures.\n    \"\"\"\n    # Fixed dataset\n    n, p = 6, 2\n    X = np.array([\n        [1.0, 0.0],\n        [1.0, 1.0],\n        [1.0, 2.0],\n        [1.0, 3.0],\n        [1.0, 4.0],\n        [1.0, 5.0]\n    ])\n    y = np.array([1.0, 2.0, 2.0, 4.0, 3.0, 5.0])\n\n    # Compute OLS estimator once\n    # Solve (X^T X) beta = X^T y\n    beta_ols = np.linalg.solve(X.T @ X, X.T @ y)\n\n    # --- Define the five covariance matrices ---\n\n    # Case 1: Spherical errors\n    sigma2_1 = 2.0\n    Sigma1 = sigma2_1 * np.identity(n)\n\n    # Case 2: General exact-equality condition\n    sigma2_2 = 1.5\n    Lambda = np.array([[0.7, 0.2], [0.2, 0.5]])\n    Sigma2 = sigma2_2 * np.identity(n) + X @ Lambda @ X.T\n\n    # Case 3: Near-equality, perturbation orthogonal to col(X)\n    epsilon = 1e-6\n    v = np.array([1.0, -2.0, 1.0, 0.0, 0.0, 0.0])\n    Sigma3 = Sigma2 + epsilon * np.outer(v, v)\n\n    # Case 4: Weak autoregressive correlation (AR(1))\n    rho4 = 0.1\n    sigma2_4 = 1.0\n    c4 = sigma2_4 * (rho4 ** np.arange(n))\n    Sigma4 = toeplitz(c4)\n\n    # Case 5: Strong autoregressive correlation (AR(1))\n    rho5 = 0.8\n    sigma2_5 = 1.0\n    c5 = sigma2_5 * (rho5 ** np.arange(n))\n    Sigma5 = toeplitz(c5)\n\n    # List of covariance matrices for iteration\n    sigmas = [Sigma1, Sigma2, Sigma3, Sigma4, Sigma5]\n    \n    results = []\n\n    # Iterate through each covariance matrix to compute GLS and the difference norm\n    for Sigma in sigmas:\n        # Compute GLS estimator\n        # Solve (X^T Sigma^-1 X) beta = X^T Sigma^-1 y\n        Sigma_inv = np.linalg.inv(Sigma)\n        XT_S_inv = X.T @ Sigma_inv\n        \n        A_gls = XT_S_inv @ X\n        b_gls = XT_S_inv @ y\n        \n        beta_gls = np.linalg.solve(A_gls, b_gls)\n        \n        # Compute the Euclidean norm of the difference\n        diff_norm = np.linalg.norm(beta_ols - beta_gls)\n        results.append(diff_norm)\n\n    # Format the final output string as per requirements\n    output_str = f\"[{','.join([f'{r:.8f}' for r in results])}]\"\n    print(output_str)\n\nsolve()\n\n```", "id": "3112106"}, {"introduction": "我们已经知道，使用 GLS 的主要动机是提高估计效率。然而，效率的提升程度并非一成不变，它深刻地依赖于设计矩阵 $X$ 的结构与误差协方差矩阵 $\\Sigma$ 之间的相互作用。本练习旨在揭示这一关键关系，我们将通过精心设计不同的 predictor（即 $X$ 的列），来检验在固定的误差相关性下，GLS 相对于 OLS 的效率增益如何变化。这个练习将向我们展示，通过改变数据的内在几何结构（如 predictor 之间的正交性或共線性），我们可以显著放大或缩小 GLS 的优势。[@problem_id:3112084]", "problem": "考虑带有相关误差的线性回归模型，定义为 $y = X\\beta + \\varepsilon$，其中 $y \\in \\mathbb{R}^n$ 是响应向量，$X \\in \\mathbb{R}^{n \\times p}$ 是固定回归量的设计矩阵，$\\beta \\in \\mathbb{R}^p$ 是系数向量，$\\varepsilon \\in \\mathbb{R}^n$ 是一个均值为零的误差向量，其协方差矩阵为 $\\Sigma \\in \\mathbb{R}^{n \\times n}$。仅从这些定义出发，并利用随机向量的线性变换遵循 $\\operatorname{Cov}(A z) = A\\,\\operatorname{Cov}(z)\\,A^\\top$（对于任何相容矩阵 $A$）以及矩阵的逆和转置遵循标准线性代数性质这两个事实，推导普通最小二乘 (Ordinary Least Squares, OLS) 估计量和广义最小二乘 (Generalized Least Squares, GLS) 估计量的协方差矩阵表达式，并使用它们在固定的相关误差协方差 $\\Sigma$ 下，为几个固定的设计 $X$ 量化 GLS 相对于 OLS 的效率。广义最小二乘 (GLS) 是指在已知误差协方差 $\\Sigma$ 的情况下进行估计，而普通最小二乘 (OLS) 是指忽略 $\\Sigma$ 的估计。\n\n对于给定的设计 $X$ 和误差协方差 $\\Sigma$，将 GLS 相对于 OLS 的标量效率定义为两个估计量协方差矩阵的迹之比，即 $\\operatorname{Eff}(X,\\Sigma) = \\dfrac{\\operatorname{tr}\\big(\\operatorname{Cov}(\\hat{\\beta}_{\\text{OLS}})\\big)}{\\operatorname{tr}\\big(\\operatorname{Cov}(\\hat{\\beta}_{\\text{GLS}})\\big)}$，其中 $\\operatorname{tr}(\\cdot)$ 表示方阵的迹。这个量捕捉了每个参数的平均方差：接近 1 的值表示增益最小，而较大的值表示 GLS 相对于 OLS 有更大的效率增益。\n\n你的任务是实现一个程序，对于一个固定的相关误差协方差 $\\Sigma$，构建几个能够例示 GLS 最小和最大增益的设计矩阵 $X$，为每个设计矩阵计算 $\\operatorname{Eff}(X,\\Sigma)$，并报告结果。你必须通过使用正交列来设计 $X$ 以使 GLS 增益最小，并通过使用近似共线性来使其最大，所有这些都在相同的 $\\Sigma$ 下进行。此外，包括一个误差不相关的边界情况，以验证你的实现。\n\n使用以下测试套件：\n\n- 令 $n = 8$ 且 $p = 2$。对于除边界情况外的所有情况，将 $\\Sigma$ 固定为一阶自回归协方差，其参数 $\\rho = 0.8$，新息方差为 1，即 $\\Sigma \\in \\mathbb{R}^{8 \\times 8}$，其元素为 $\\Sigma_{ij} = \\rho^{|i-j|}$，对于所有 $i,j \\in \\{1,\\dots,8\\}$ 且 $\\rho = 0.8$。\n- 对于每种情况，令 $X$ 的第一列为截距向量 $\\mathbf{1}_8 = [1,1,1,1,1,1,1,1]^\\top$（八个 1），第二列为 $x_1$，具体规定如下：\n  1. 情况 A（正交列，目标为最小增益）：$x_1 = [-3,-2,-1,0,1,2,3,0]^\\top$，它满足 $\\mathbf{1}_8^\\top x_1 = 0$，因此在欧几里得内积下与截距正交。\n  2. 情况 B（近似共线性，目标为最大增益）：$x_1 = \\mathbf{1}_8 + 10^{-3} \\cdot [-3,-2,-1,0,1,2,3,0]^\\top$，这使得第二列与截距近似成比例。\n  3. 情况 C（中度共线性）：$x_1 = \\mathbf{1}_8 + 10^{-1} \\cdot [-3,-2,-1,0,1,2,3,0]^\\top$，这提供了与截距的中度相关性。\n  4. 情况 D（使用不相关误差的边界检查）：使用与情况 B 相同的 $X$，但设置 $\\Sigma = I_8$，即 $8 \\times 8$ 的单位矩阵，对应于 $\\rho = 0$。\n\n对于每种情况，计算如上定义的标量效率 $\\operatorname{Eff}(X,\\Sigma)$。将每个结果四舍五入到 6 位小数。\n\n你的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，按上述情况的顺序排列，即 $[\\operatorname{Eff}_A,\\operatorname{Eff}_B,\\operatorname{Eff}_C,\\operatorname{Eff}_D]$，其中每个条目都是一个四舍五入到 6 位小数的浮点数。不应打印任何额外的文本或空格。", "solution": "该问题要求推导普通最小二乘 (OLS) 和广义最小二乘 (GLS) 估计量的协方差矩阵，并使用它们为几个指定的场景计算相对效率度量。\n\n首先，我们通过从提供的定义中推导必要的协方差矩阵来建立理论基础。线性回归模型为 $y = X\\beta + \\varepsilon$，其中 $y \\in \\mathbb{R}^n$，$X \\in \\mathbb{R}^{n \\times p}$，$\\beta \\in \\mathbb{R}^p$，且 $\\varepsilon \\in \\mathbb{R}^n$ 是误差项，满足 $E[\\varepsilon] = 0$ 和 $\\operatorname{Cov}(\\varepsilon) = \\Sigma$。\n\n**OLS 估计量协方差矩阵的推导**\n\nOLS 估计量定义为 $\\hat{\\beta}_{\\text{OLS}} = (X^\\top X)^{-1} X^\\top y$。它是在 $\\operatorname{Cov}(\\varepsilon) = \\sigma^2 I$ 的假设下推导出来的，但在这里我们评估它在真实的、更一般的协方差结构 $\\Sigma$ 下的性能。将模型方程代入 $y$：\n$$\n\\hat{\\beta}_{\\text{OLS}} = (X^\\top X)^{-1} X^\\top (X\\beta + \\varepsilon) = (X^\\top X)^{-1} X^\\top X\\beta + (X^\\top X)^{-1} X^\\top \\varepsilon = \\beta + (X^\\top X)^{-1} X^\\top \\varepsilon\n$$\n该估计量是无偏的，因为 $E[\\hat{\\beta}_{\\text{OLS}}] = E[\\beta + (X^\\top X)^{-1} X^\\top \\varepsilon] = \\beta + (X^\\top X)^{-1} X^\\top E[\\varepsilon] = \\beta$。$\\hat{\\beta}_{\\text{OLS}}$ 的协方差矩阵是：\n$$\n\\operatorname{Cov}(\\hat{\\beta}_{\\text{OLS}}) = \\operatorname{Cov}(\\beta + (X^\\top X)^{-1} X^\\top \\varepsilon) = \\operatorname{Cov}((X^\\top X)^{-1} X^\\top \\varepsilon)\n$$\n使用给定的随机向量线性变换法则 $\\operatorname{Cov}(Az) = A\\operatorname{Cov}(z)A^\\top$，我们令 $A = (X^\\top X)^{-1} X^\\top$ 和 $z = \\varepsilon$。这得到：\n$$\n\\operatorname{Cov}(\\hat{\\beta}_{\\text{OLS}}) = \\left( (X^\\top X)^{-1} X^\\top \\right) \\operatorname{Cov}(\\varepsilon) \\left( (X^\\top X)^{-1} X^\\top \\right)^\\top\n$$\n代入 $\\operatorname{Cov}(\\varepsilon) = \\Sigma$ 并化简转置项：\n$$\n\\left( (X^\\top X)^{-1} X^\\top \\right)^\\top = (X^\\top)^\\top \\left((X^\\top X)^{-1}\\right)^\\top = X \\left((X^\\top X)^\\top\\right)^{-1} = X(X^\\top X)^{-1}\n$$\n最后一步利用了 $X^\\top X$ 是对称矩阵这一事实。因此，OLS 估计量的协方差矩阵为：\n$$\n\\operatorname{Cov}(\\hat{\\beta}_{\\text{OLS}}) = (X^\\top X)^{-1} X^\\top \\Sigma X (X^\\top X)^{-1}\n$$\n\n**GLS 估计量协方差矩阵的推导**\n\nGLS 估计量考虑了已知的协方差 $\\Sigma$，其定义为 $\\hat{\\beta}_{\\text{GLS}} = (X^\\top \\Sigma^{-1} X)^{-1} X^\\top \\Sigma^{-1} y$。遵循类似的步骤，我们将模型代入 $y$：\n$$\n\\hat{\\beta}_{\\text{GLS}} = (X^\\top \\Sigma^{-1} X)^{-1} X^\\top \\Sigma^{-1} (X\\beta + \\varepsilon) = \\beta + (X^\\top \\Sigma^{-1} X)^{-1} X^\\top \\Sigma^{-1} \\varepsilon\n$$\nGLS 估计量也是无偏的，因为 $E[\\hat{\\beta}_{\\text{GLS}}] = \\beta$。其协方差为：\n$$\n\\operatorname{Cov}(\\hat{\\beta}_{\\text{GLS}}) = \\operatorname{Cov}\\left( (X^\\top \\Sigma^{-1} X)^{-1} X^\\top \\Sigma^{-1} \\varepsilon \\right)\n$$\n令 $B = (X^\\top \\Sigma^{-1} X)^{-1} X^\\top \\Sigma^{-1}$ 并使用 $\\operatorname{Cov}(Bz) = B\\operatorname{Cov}(z)B^\\top$：\n$$\n\\operatorname{Cov}(\\hat{\\beta}_{\\text{GLS}}) = B \\Sigma B^\\top = \\left((X^\\top \\Sigma^{-1} X)^{-1} X^\\top \\Sigma^{-1}\\right) \\Sigma \\left((X^\\top \\Sigma^{-1} X)^{-1} X^\\top \\Sigma^{-1}\\right)^\\top\n$$\n转置项为 $\\left(\\Sigma^{-1}\\right)^\\top (X^\\top)^\\top \\left((X^\\top \\Sigma^{-1} X)^{-1}\\right)^\\top = \\Sigma^{-1} X (X^\\top \\Sigma^{-1} X)^{-1}$，因为 $\\Sigma$ 和 $X^\\top\\Sigma^{-1}X$ 是对称的。代回原式：\n$$\n\\operatorname{Cov}(\\hat{\\beta}_{\\text{GLS}}) = (X^\\top \\Sigma^{-1} X)^{-1} X^\\top \\Sigma^{-1} \\Sigma \\Sigma^{-1} X (X^\\top \\Sigma^{-1} X)^{-1}\n$$\n$$\n\\operatorname{Cov}(\\hat{\\beta}_{\\text{GLS}}) = (X^\\top \\Sigma^{-1} X)^{-1} (X^\\top \\Sigma^{-1} X) (X^\\top \\Sigma^{-1} X)^{-1} = (X^\\top \\Sigma^{-1} X)^{-1}\n$$\n这是 GLS 估计量协方差的标准结果。\n\n**相对效率和实施计划**\n\n标量相对效率由这两个协方差矩阵的迹之比给出：\n$$\n\\operatorname{Eff}(X,\\Sigma) = \\dfrac{\\operatorname{tr}\\big(\\operatorname{Cov}(\\hat{\\beta}_{\\text{OLS}})\\big)}{\\operatorname{tr}\\big(\\operatorname{Cov}(\\hat{\\beta}_{\\text{GLS}})\\big)} = \\dfrac{\\operatorname{tr}\\left( (X^\\top X)^{-1} X^\\top \\Sigma X (X^\\top X)^{-1} \\right)}{\\operatorname{tr}\\left( (X^\\top \\Sigma^{-1} X)^{-1} \\right)}\n$$\n实现将首先定义一个 Python 函数，用于为给定的设计矩阵 $X$ 和协方差矩阵 $\\Sigma$ 计算这个量。该函数将使用 `numpy` 库进行所有线性代数运算，包括矩阵乘法、转置、求逆和迹的计算。\n\n对于四个指定的情况中的每一种，我们将为相应的 $X$ 和 $\\Sigma$ 构建 `numpy` 数组。将为 $\\rho=0.8$ 和 $n=8$ 构建自回归协方差矩阵 $\\Sigma_{ij} = \\rho^{|i-j|}$。设计矩阵 $X$ 将根据每种情况的规定（正交、近似共线、中度共线）构建，包含一个截距列和第二列。边界情况对 $\\Sigma$ 使用单位矩阵。\n\n将为四种情况中的每一种计算效率，其中边界情况（情况 D）预期效率为 1.0，因为当误差不相关时（$\\Sigma=\\sigma^2I$），OLS 是最佳线性无偏估计量 (Best Linear Unbiased Estimator, BLUE)，并且与 GLS 相同。这可以作为对推导出的公式和实现的关键验证。然后将最终结果按照规定进行四舍五入和格式化。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef compute_efficiency(X, Sigma):\n    \"\"\"\n    Computes the relative efficiency of GLS vs. OLS.\n\n    Args:\n        X (np.ndarray): The design matrix of shape (n, p).\n        Sigma (np.ndarray): The error covariance matrix of shape (n, n).\n\n    Returns:\n        float: The scalar efficiency Eff(X, Sigma).\n    \"\"\"\n    # Numerator: Trace of the OLS estimator's covariance matrix\n    # Cov(b_OLS) = (X'X)^-1 X' Sigma X (X'X)^-1\n    try:\n        XT = X.T\n        XTX_inv = np.linalg.inv(XT @ X)\n        cov_ols = XTX_inv @ XT @ Sigma @ X @ XTX_inv\n        tr_cov_ols = np.trace(cov_ols)\n    except np.linalg.LinAlgError:\n        # This can happen if X is not full rank, though not expected\n        # for the given test cases.\n        return np.nan\n\n    # Denominator: Trace of the GLS estimator's covariance matrix\n    # Cov(b_GLS) = (X' Sigma^-1 X)^-1\n    try:\n        Sigma_inv = np.linalg.inv(Sigma)\n        cov_gls = np.linalg.inv(XT @ Sigma_inv @ X)\n        tr_cov_gls = np.trace(cov_gls)\n    except np.linalg.LinAlgError:\n        # This can happen if Sigma is singular or X' Sigma^-1 X is singular.\n        # Not expected for the given test cases.\n        return np.nan\n\n    # The efficiency ratio\n    if tr_cov_gls == 0:\n        return np.inf if tr_cov_ols > 0 else np.nan\n    \n    return tr_cov_ols / tr_cov_gls\n\ndef solve():\n    \"\"\"\n    Solves the problem by calculating the GLS vs. OLS efficiency for four cases.\n    \"\"\"\n    # Define problem parameters\n    n = 8\n    rho = 0.8\n    \n    # Construct AR(1) covariance matrix for Cases A, B, C\n    # Sigma_ij = rho^|i-j|\n    indices = np.arange(n)\n    Sigma_AR1 = rho**np.abs(indices[:, np.newaxis] - indices)\n\n    # Construct identity covariance matrix for Case D\n    Sigma_I = np.identity(n)\n\n    # Common vectors for constructing design matrices\n    intercept = np.ones(n)\n    v = np.array([-3.0, -2.0, -1.0, 0.0, 1.0, 2.0, 3.0, 0.0])\n\n    # --- Case A (orthogonal columns) ---\n    x1_A = v\n    X_A = np.column_stack((intercept, x1_A))\n    eff_A = compute_efficiency(X_A, Sigma_AR1)\n\n    # --- Case B (near collinearity) ---\n    x1_B = intercept + 1e-3 * v\n    X_B = np.column_stack((intercept, x1_B))\n    eff_B = compute_efficiency(X_B, Sigma_AR1)\n    \n    # --- Case C (moderate collinearity) ---\n    x1_C = intercept + 1e-1 * v\n    X_C = np.column_stack((intercept, x1_C))\n    eff_C = compute_efficiency(X_C, Sigma_AR1)\n\n    # --- Case D (boundary check with uncorrelated errors) ---\n    # Use the same X as in Case B but with Sigma = I\n    X_D = X_B\n    eff_D = compute_efficiency(X_D, Sigma_I)\n\n    results = [eff_A, eff_B, eff_C, eff_D]\n    \n    # Format the output as specified\n    # The requirement is rounding to 6 decimal places.\n    # The f-string format specifier '{:.6f}' correctly handles this rounding.\n    print(f\"[{','.join([f'{r:.6f}' for r in results])}]\")\n\nsolve()\n\n```", "id": "3112084"}, {"introduction": "在处理大规模数据集时，直接计算和求逆一个稠密的 $n \\times n$ 协方差矩阵 $\\Sigma$ 在计算上是不可行的。幸运的是，许多现实世界中的相关性结构，例如时间序列或网络数据中的相关性，可以用一个稀疏的精度矩阵 $\\mathbf{Q} = \\Sigma^{-1}$ 来描述。本练习将引导我们从理论走向实践，学习如何利用精度矩阵的稀疏性（在此例中为三对角结构）来实现高效的 GLS 估计，这对于将 GLS 应用于现代大规模问题至关重要。[@problem_id:3112081]", "problem": "给定一个在线性路径网络上定义的具有相关误差的线性模型。结果向量表示为 $\\mathbf{y} \\in \\mathbb{R}^n$，设计矩阵表示为 $\\mathbf{X} \\in \\mathbb{R}^{n \\times p}$，误差向量表示为 $\\boldsymbol{\\varepsilon} \\in \\mathbb{R}^n$。该模型为 $\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}$，其中 $E[\\boldsymbol{\\varepsilon}] = \\mathbf{0}$ 且 $\\operatorname{Cov}(\\boldsymbol{\\varepsilon}) = \\boldsymbol{\\Sigma}$。由于路径图上的共享链接，误差是相关的；这种相关性通过一个稀疏精度（逆协方差）矩阵 $\\mathbf{Q} = \\boldsymbol{\\Sigma}^{-1}$ 进行编码。\n\n从以下基本基础出发：\n- 线性模型定义 $\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}$，误差均值为零。\n- 假设 $\\boldsymbol{\\varepsilon}$ 服从协方差为 $\\boldsymbol{\\Sigma}$ 的多元正态分布，因此对数似然（忽略一个加法常数）与二次型 $(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})^\\top \\boldsymbol{\\Sigma}^{-1}(\\mathbf{y}-\\mathbf{X}\\boldsymbol{\\beta})$ 的负值成正比。\n- 带状矩阵的稀疏线性代数原理以及对称正定矩阵的 Cholesky 分解。\n\n网络引起的误差相关模型。考虑一个包含 $n$ 个节点的路径图，其中精度矩阵 $\\mathbf{Q}$ 定义为\n$$\n\\mathbf{Q} = \\alpha \\mathbf{I}_n + w \\mathbf{L}_{\\text{path}},\n$$\n其中 $\\alpha > 0$, $w \\ge 0$，$\\mathbf{I}_n$ 是 $n \\times n$ 的单位矩阵，而 $\\mathbf{L}_{\\text{path}}$ 是路径图的图拉普拉斯矩阵：$\\mathbf{L}_{\\text{path}}$ 在两个端点的对角线元素为 $1$，内部节点的对角线元素为 $2$，相邻节点的次对角线和超对角线元素为 $-1$，所有其他元素均为 $0$。这使得 $\\mathbf{Q}$ 成为对称正定的三对角矩阵。您的任务是利用这种结构，通过一个为三对角矩阵量身定制的稀疏 Cholesky 程序来利用稀疏性，从而高效地实现广义最小二乘法（GLS）。\n\n要求：\n- 仅从所述基础出发（不要使用提供给您的任何快捷估计量表达式），推导出获得 $\\boldsymbol{\\beta}$ 估计量的计算步骤，该估计量最小化由精度矩阵 $\\mathbf{Q}$ 导出的适当二次型。\n- 为 $\\mathbf{Q}$ 实现一个带状（三对角）稀疏 Cholesky 分解。即，计算一个下双对角矩阵 $\\mathbf{L}$ 使得 $\\mathbf{Q} = \\mathbf{L}\\mathbf{L}^\\top$，仅使用对对称正定三对角矩阵有效的 $O(n)$ 递推关系。\n- 使用该分解将问题转换为一个关于适当白化变量的普通最小二乘问题，并求解 $\\boldsymbol{\\beta}$。\n- 您不得依赖于稠密 Cholesky 程序。您的实现必须明确利用 $\\mathbf{Q}$ 的三对角稀疏性。\n\n测试套件。对于下面的每个测试用例，通过给定的 $\\alpha$ 和 $w$ 构建 $\\mathbf{Q}$，执行基于稀疏 Cholesky 的 GLS，并返回估计的系数向量 $\\widehat{\\boldsymbol{\\beta}}$，四舍五入到 $6$ 位小数。\n\n- 测试用例 A (正常路径)：\n  - $n = 5$, $\\alpha = 1.0$, $w = 0.4$。\n  - $\\mathbf{X} \\in \\mathbb{R}^{5 \\times 2}$，行向量为 $\\big([1,0],[1,1],[1,2],[1,3],[1,4]\\big)$。\n  - $\\mathbf{y} = [1.0, 2.2, 2.0, 3.6, 5.1]^\\top$。\n- 测试用例 B (边界，不相关误差)：\n  - $n = 5$, $\\alpha = 1.0$, $w = 0.0$。\n  - $\\mathbf{X}$ 和 $\\mathbf{y}$ 与测试用例 A 中相同。\n- 测试用例 C (更强相关性和更高维度)：\n  - $n = 6$, $\\alpha = 0.5$, $w = 1.0$。\n  - $\\mathbf{X} \\in \\mathbb{R}^{6 \\times 3}$，行向量为 $\\big([1,0,0],[1,1,1],[1,2,0],[1,3,1],[1,4,0],[1,5,1]\\big)$。\n  - $\\mathbf{y} = [0.5, 1.6, 2.1, 3.2, 3.7, 4.8]^\\top$。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果，其中每个元素是按 A、B、C 顺序排列的一个测试用例的估计系数列表。每个系数必须四舍五入到 $6$ 位小数。例如，一个有效的输出形状类似于 $[[b_{A,1},b_{A,2}], [b_{B,1},b_{B,2}], [b_{C,1},b_{C,2},b_{C,3}]]$，并用实际数值替换。\n- 此问题中没有物理单位或角度。\n- 所有答案必须是数字（浮点数），并且该单行必须与所描述的聚合列表完全一致，不得有多余文本。", "solution": "该问题要求计算一个线性模型的广义最小二乘法（GLS）估计量，其中误差表现出由一个特定的三对角精度矩阵 $\\mathbf{Q}$ 定义的相关性。解决方案必须从基本原理推导得出，并且必须通过自定义的稀疏 Cholesky 分解来利用 $\\mathbf{Q}$ 的稀疏结构。\n\n线性模型由 $\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}$ 给出，其中 $\\mathbf{y} \\in \\mathbb{R}^n$ 是结果向量，$\\mathbf{X} \\in \\mathbb{R}^{n \\times p}$ 是设计矩阵，$\\boldsymbol{\\beta} \\in \\mathbb{R}^p$ 是待估计的系数向量，$\\boldsymbol{\\varepsilon} \\in \\mathbb{R}^n$ 是误差向量。假设误差服从均值为 $\\mathbf{0}$、协方差矩阵为 $\\boldsymbol{\\Sigma}$ 的多元正态分布 $\\boldsymbol{\\varepsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\boldsymbol{\\Sigma})$。精度矩阵定义为 $\\mathbf{Q} = \\boldsymbol{\\Sigma}^{-1}$。\n\n$\\boldsymbol{\\varepsilon}$ 的概率密度函数与 $\\exp\\left(-\\frac{1}{2}\\boldsymbol{\\varepsilon}^\\top \\mathbf{Q} \\boldsymbol{\\varepsilon}\\right)$ 成正比。代入 $\\boldsymbol{\\varepsilon} = \\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta}$，数据的对数似然（忽略一个加法常数）与 $-\\frac{1}{2}(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})^\\top \\mathbf{Q} (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})$ 成正比。最大似然估计量 $\\widehat{\\boldsymbol{\\beta}}$ 是最小化该二次型的向量，该二次型即为 GLS 目标函数：\n$$\nS(\\boldsymbol{\\beta}) = (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})^\\top \\mathbf{Q} (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})\n$$\n问题指明精度矩阵 $\\mathbf{Q}$ 是对称正定的，这允许唯一的 Cholesky 分解 $\\mathbf{Q} = \\mathbf{L}\\mathbf{L}^\\top$，其中 $\\mathbf{L}$ 是一个实数下三角矩阵。将此代入目标函数：\n$$\nS(\\boldsymbol{\\beta}) = (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})^\\top (\\mathbf{L}\\mathbf{L}^\\top) (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta}) = \\left(\\mathbf{L}^\\top(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})\\right)^\\top \\left(\\mathbf{L}^\\top(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})\\right)\n$$\n这可以重写为标准的平方和形式。让我们通过左乘 $\\mathbf{L}^\\top$ 来定义“白化”变量：\n$$\n\\mathbf{y}' = \\mathbf{L}^\\top\\mathbf{y}\n$$\n$$\n\\mathbf{X}' = \\mathbf{L}^\\top\\mathbf{X}\n$$\n目标函数随后变为转换后变量的普通最小二乘（OLS）目标：\n$$\nS(\\boldsymbol{\\beta}) = (\\mathbf{y}' - \\mathbf{X}'\\boldsymbol{\\beta})^\\top (\\mathbf{y}' - \\mathbf{X}'\\boldsymbol{\\beta}) = ||\\mathbf{y}' - \\mathbf{X}'\\boldsymbol{\\beta}||_2^2\n$$\n此 OLS 问题的解 $\\widehat{\\boldsymbol{\\beta}}$ 通过求解正规方程组得到：\n$$\n(\\mathbf{X}')^\\top \\mathbf{X}' \\widehat{\\boldsymbol{\\beta}} = (\\mathbf{X}')^\\top \\mathbf{y}'\n$$\n因此，计算策略是：\n$1$. 构建精度矩阵 $\\mathbf{Q}$。\n$2$. 计算其 Cholesky 因子 $\\mathbf{L}$。\n$3$. 使用 $\\mathbf{L}$ 来白化数据 $\\mathbf{y}$ 和 $\\mathbf{X}$。\n$4$. 求解得到的 OLS 问题。\n\n高效实现的关键在于利用 $\\mathbf{Q}$ 的特定结构。给定 $\\mathbf{Q} = \\alpha \\mathbf{I}_n + w \\mathbf{L}_{\\text{path}}$，其中 $\\alpha > 0$, $w \\ge 0$，$\\mathbf{L}_{\\text{path}}$ 是路径图的拉普拉斯矩阵。这使得 $\\mathbf{Q}$ 成为一个对称三对角矩阵。\n设 $\\mathbf{Q}$ 的对角线为 $d_0, d_1, \\dots, d_{n-1}$，其次对角线为 $e_0, e_1, \\dots, e_{n-2}$。根据 $\\mathbf{L}_{\\text{path}}$ 的定义：\n- 对角线：对于 $i \\in \\{1, \\dots, n-2\\}$，$d_i = \\alpha + 2w$，且 $d_0 = d_{n-1} = \\alpha + w$。\n- 次对角线：对于 $i \\in \\{0, \\dots, n-2\\}$，$e_i = -w$。\n\n对称三对角矩阵的 Cholesky 因子 $\\mathbf{L}$ 是一个下双对角矩阵。设其对角线为 $l_0, \\dots, l_{n-1}$，其次对角线为 $m_0, \\dots, m_{n-2}$。将 $\\mathbf{Q}$ 的元素与 $\\mathbf{L}\\mathbf{L}^\\top$ 的元素相等：\n$$\n\\mathbf{Q}_{i,i} = d_i = l_i^2 + m_{i-1}^2 \\quad (\\text{with } m_{-1}=0)\n$$\n$$\n\\mathbf{Q}_{i,i-1} = e_{i-1} = l_{i-1}m_{i-1}\n$$\n这导出了以下用于計算 $\\mathbf{L}$ 元素的 $O(n)$ 递推关系：\n$1$. $l_0 = \\sqrt{d_0}$\n$2$. 对于 $i = 1, \\dots, n-1$：\n   a. $m_{i-1} = e_{i-1} / l_{i-1}$\n   b. $l_i = \\sqrt{d_i - m_{i-1}^2}$\n这就构成了一个稀疏 Cholesky 分解。\n\n接下来，白化步骤 $\\mathbf{v}' = \\mathbf{L}^\\top \\mathbf{v}$ 必须高效执行。矩阵 $\\mathbf{L}^\\top$ 是上双对角矩阵，其对角线元素为 $l_i$，超对角线元素为 $(\\mathbf{L}^\\top)_{i, i+1} = m_i$。该乘法可以在 $O(n)$ 时间内完成，而无需构建稠密矩阵：\n$$\nv'_i = l_i v_i + m_i v_{i+1} \\quad \\text{for } i = 0, \\dots, n-2\n$$\n$$\nv'_{n-1} = l_{n-1} v_{n-1}\n$$\n应用这种稀疏乘法来计算 $\\mathbf{y}' = \\mathbf{L}^\\top \\mathbf{y}$ 和 $\\mathbf{X}' = \\mathbf{L}^\\top \\mathbf{X}$ 的每一列。白化的总成本为 $O(np)$。\n\n最后，构建 $p \\times p$ 的正规方程组 $(\\mathbf{X}')^\\top \\mathbf{X}' \\widehat{\\boldsymbol{\\beta}} = (\\mathbf{X}')^\\top \\mathbf{y}'$ 并求解 $\\widehat{\\boldsymbol{\\beta}}$，通常使用标准的线性系统求解器。整个过程避免了构建稠密的 $n \\times n$ 矩阵，并且计算效率高。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for the given test cases.\n    \"\"\"\n\n    def perform_gls(n, alpha, w, X, y):\n        \"\"\"\n        Performs Generalized Least Squares using sparse Cholesky factorization.\n\n        Args:\n            n (int): Number of observations.\n            alpha (float): Parameter for the precision matrix.\n            w (float): Parameter for the precision matrix.\n            X (np.ndarray): Design matrix of shape (n, p).\n            y (np.ndarray): Outcome vector of shape (n,).\n\n        Returns:\n            np.ndarray: The estimated coefficient vector beta_hat.\n        \"\"\"\n        # Step 1: Construct the diagonal and sub-diagonal of the tridiagonal precision matrix Q.\n        # Q = alpha * I + w * L_path\n        q_diag = np.full(n, alpha + 2 * w)\n        if n > 0:\n            q_diag[0] = alpha + w\n            q_diag[-1] = alpha + w\n        q_subdiag = np.full(n - 1, -w)\n\n        # Step 2: Perform sparse Cholesky factorization Q = LL^T.\n        # L is a lower bidiagonal matrix with diagonal l_diag and sub-diagonal m_subdiag.\n        l_diag = np.zeros(n)\n        m_subdiag = np.zeros(n - 1)\n\n        if n > 0:\n            l_diag[0] = np.sqrt(q_diag[0])\n            for i in range(1, n):\n                m_subdiag[i - 1] = q_subdiag[i - 1] / l_diag[i - 1]\n                # The argument to sqrt is guaranteed to be positive because Q is positive definite.\n                l_diag[i] = np.sqrt(q_diag[i] - m_subdiag[i - 1]**2)\n\n        # Step 3: Whiten the variables using L^T.\n        # L^T is an upper bidiagonal matrix.\n        # y_prime = L^T * y and X_prime = L^T * X.\n        y_prime = np.zeros(n)\n        X_prime = np.zeros_like(X, dtype=float)\n        p = X.shape[1]\n\n        # Whiten y\n        if n > 0:\n            for i in range(n - 1):\n                y_prime[i] = l_diag[i] * y[i] + m_subdiag[i] * y[i + 1]\n            y_prime[n - 1] = l_diag[n - 1] * y[n - 1]\n\n        # Whiten X column by column\n        if n > 0:\n            for j in range(p):\n                x_col = X[:, j]\n                x_prime_col = np.zeros(n)\n                for i in range(n - 1):\n                    x_prime_col[i] = l_diag[i] * x_col[i] + m_subdiag[i] * x_col[i + 1]\n                x_prime_col[n - 1] = l_diag[n - 1] * x_col[n - 1]\n                X_prime[:, j] = x_prime_col\n        \n        # Step 4: Solve the OLS problem for the whitened variables.\n        # (X_prime^T * X_prime) * beta_hat = X_prime^T * y_prime\n        A_ols = X_prime.T @ X_prime\n        b_ols = X_prime.T @ y_prime\n        beta_hat = np.linalg.solve(A_ols, b_ols)\n\n        return beta_hat\n\n    test_cases = [\n        # Test Case A\n        {\n            \"n\": 5, \"alpha\": 1.0, \"w\": 0.4,\n            \"X\": np.array([[1, 0], [1, 1], [1, 2], [1, 3], [1, 4]]),\n            \"y\": np.array([1.0, 2.2, 2.0, 3.6, 5.1]),\n        },\n        # Test Case B\n        {\n            \"n\": 5, \"alpha\": 1.0, \"w\": 0.0,\n            \"X\": np.array([[1, 0], [1, 1], [1, 2], [1, 3], [1, 4]]),\n            \"y\": np.array([1.0, 2.2, 2.0, 3.6, 5.1]),\n        },\n        # Test Case C\n        {\n            \"n\": 6, \"alpha\": 0.5, \"w\": 1.0,\n            \"X\": np.array([[1, 0, 0], [1, 1, 1], [1, 2, 0], [1, 3, 1], [1, 4, 0], [1, 5, 1]]),\n            \"y\": np.array([0.5, 1.6, 2.1, 3.2, 3.7, 4.8]),\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        beta_hat = perform_gls(case[\"n\"], case[\"alpha\"], case[\"w\"], case[\"X\"], case[\"y\"])\n        # Round to 6 decimal places and convert to a list for formatting.\n        results.append(np.round(beta_hat, 6).tolist())\n\n    # Format the output as a list of lists.\n    # The f-string calls str() on each element, which correctly formats the inner lists.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3112081"}]}