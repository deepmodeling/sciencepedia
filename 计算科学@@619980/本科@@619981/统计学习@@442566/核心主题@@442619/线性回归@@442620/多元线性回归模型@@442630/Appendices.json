{"hands_on_practices": [{"introduction": "线性回归的核心在于找到最能拟合数据的系数。本次实践将引导你亲手完成普通最小二乘（OLS）估计的矩阵运算，从而揭开统计软件自动化过程背后的神秘面纱。这个练习使用一个经过精心设计的正交实验数据，可以简化计算，让你专注于方法本身。", "problem": "一位食品科学家正在研发一种新型烘焙零食，并希望了解烘焙条件如何影响其酥脆度。酥脆度是根据一个定量标度来衡量的。这位科学家进行了包含四个批次的小型实验，改变了两个因素：烘焙温度和湿度。这些因素由编码变量表示，其中 $-1$ 代表低水平设置，$+1$ 代表高水平设置。\n\n提出的统计模型是一个形式如下的多元线性回归模型：\n$$y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\epsilon$$\n其中：\n- $y$ 是酥脆度得分。\n- $x_1$ 是烘焙温度的编码变量。\n- $x_2$ 是湿度的编码变量。\n- $\\beta_0$、$\\beta_1$ 和 $\\beta_2$ 是未知的模型系数。\n- $\\epsilon$ 是随机误差项。\n\n四个实验批次的结果如下：\n- 批次1：温度编码 ($x_1$) = -1，湿度编码 ($x_2$) = -1，酥脆度 ($y$) = 2。\n- 批次2：温度编码 ($x_1$) = -1，湿度编码 ($x_2$) = 1，酥脆度 ($y$) = 4。\n- 批次3：温度编码 ($x_1$) = 1，湿度编码 ($x_2$) = -1，酥脆度 ($y$) = 6。\n- 批次4：温度编码 ($x_1$) = 1，湿度编码 ($x_2$) = 1，酥脆度 ($y$) = 8。\n\n使用最小二乘法，确定这些系数的估计值。请将答案以单行矩阵的形式呈现，其中包含按 $(\\hat{\\beta}_0, \\hat{\\beta}_1, \\hat{\\beta}_2)$ 顺序排列的三个估计系数。", "solution": "目标是求出多元线性回归模型中系数 $\\beta_0$、$\\beta_1$ 和 $\\beta_2$ 的最小二乘估计值。该模型可以写成矩阵形式 $\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}$，其中 $\\mathbf{y}$ 是响应向量，$\\mathbf{X}$ 是设计矩阵，$\\boldsymbol{\\beta}$ 是系数向量，而 $\\boldsymbol{\\epsilon}$ 是误差向量。\n\n首先，我们根据给定的实验数据构建响应向量 $\\mathbf{y}$ 和设计矩阵 $\\mathbf{X}$。设计矩阵中包含一列全为1的列，用于表示截距项 $\\beta_0$。\n\n响应向量 $\\mathbf{y}$ 包含酥脆度得分：\n$$\n\\mathbf{y} = \\begin{pmatrix} 2 \\\\ 4 \\\\ 6 \\\\ 8 \\end{pmatrix}\n$$\n\n设计矩阵 $\\mathbf{X}$ 的构造方式是，第一列为截距项（全为1），后面跟着编码变量 $x_1$ 和 $x_2$ 的列：\n$$\n\\mathbf{X} = \\begin{pmatrix} 1  x_{11}  x_{21} \\\\ 1  x_{12}  x_{22} \\\\ 1  x_{13}  x_{23} \\\\ 1  x_{14}  x_{24} \\end{pmatrix} = \\begin{pmatrix} 1  -1  -1 \\\\ 1  -1  1 \\\\ 1  1  -1 \\\\ 1  1  1 \\end{pmatrix}\n$$\n\n待估计的系数向量是 $\\boldsymbol{\\beta} = (\\beta_0, \\beta_1, \\beta_2)^T$。最小二乘估计值（记为 $\\hat{\\boldsymbol{\\beta}}$）由正规方程组 $\\mathbf{X}^T\\mathbf{X}\\hat{\\boldsymbol{\\beta}} = \\mathbf{X}^T\\mathbf{y}$ 的解给出。假设 $\\mathbf{X}^T\\mathbf{X}$ 可逆，则解为：\n$$\n\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}\n$$\n\n我们将分几步进行计算。首先，求 $\\mathbf{X}$ 的转置：\n$$\n\\mathbf{X}^T = \\begin{pmatrix} 1  1  1  1 \\\\ -1  -1  1  1 \\\\ -1  1  -1  1 \\end{pmatrix}\n$$\n\n接下来，我们计算乘积 $\\mathbf{X}^T\\mathbf{X}$：\n$$\n\\mathbf{X}^T\\mathbf{X} = \\begin{pmatrix} 1  1  1  1 \\\\ -1  -1  1  1 \\\\ -1  1  -1  1 \\end{pmatrix} \\begin{pmatrix} 1  -1  -1 \\\\ 1  -1  1 \\\\ 1  1  -1 \\\\ 1  1  1 \\end{pmatrix}\n$$\n结果矩阵的元素为：\n- $(1,1): 1 \\cdot 1 + 1 \\cdot 1 + 1 \\cdot 1 + 1 \\cdot 1 = 4$\n- $(1,2): 1 \\cdot (-1) + 1 \\cdot (-1) + 1 \\cdot 1 + 1 \\cdot 1 = 0$\n- $(1,3): 1 \\cdot (-1) + 1 \\cdot 1 + 1 \\cdot (-1) + 1 \\cdot 1 = 0$\n- $(2,1): (-1) \\cdot 1 + (-1) \\cdot 1 + 1 \\cdot 1 + 1 \\cdot 1 = 0$\n- $(2,2): (-1) \\cdot (-1) + (-1) \\cdot (-1) + 1 \\cdot 1 + 1 \\cdot 1 = 4$\n- $(2,3): (-1) \\cdot (-1) + (-1) \\cdot 1 + 1 \\cdot (-1) + 1 \\cdot 1 = 0$\n- $(3,1): (-1) \\cdot 1 + 1 \\cdot 1 + (-1) \\cdot 1 + 1 \\cdot 1 = 0$\n- $(3,2): (-1) \\cdot (-1) + 1 \\cdot (-1) + (-1) \\cdot 1 + 1 \\cdot 1 = 0$\n- $(3,3): (-1) \\cdot (-1) + 1 \\cdot 1 + (-1) \\cdot (-1) + 1 \\cdot 1 = 4$\n\n因此，该矩阵是对角矩阵：\n$$\n\\mathbf{X}^T\\mathbf{X} = \\begin{pmatrix} 4  0  0 \\\\ 0  4  0 \\\\ 0  0  4 \\end{pmatrix} = 4\\mathbf{I}_3\n$$\n其中 $\\mathbf{I}_3$ 是 $3 \\times 3$ 的单位矩阵。\n\n这个对角矩阵的逆矩阵很容易计算：\n$$\n(\\mathbf{X}^T\\mathbf{X})^{-1} = \\frac{1}{4}\\mathbf{I}_3 = \\begin{pmatrix} \\frac{1}{4}  0  0 \\\\ 0  \\frac{1}{4}  0 \\\\ 0  0  \\frac{1}{4} \\end{pmatrix}\n$$\n\n现在，我们计算乘积 $\\mathbf{X}^T\\mathbf{y}$：\n$$\n\\mathbf{X}^T\\mathbf{y} = \\begin{pmatrix} 1  1  1  1 \\\\ -1  -1  1  1 \\\\ -1  1  -1  1 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ 4 \\\\ 6 \\\\ 8 \\end{pmatrix} = \\begin{pmatrix} 1(2)+1(4)+1(6)+1(8) \\\\ -1(2)-1(4)+1(6)+1(8) \\\\ -1(2)+1(4)-1(6)+1(8) \\end{pmatrix} = \\begin{pmatrix} 20 \\\\ 8 \\\\ 4 \\end{pmatrix}\n$$\n\n最后，我们将 $(\\mathbf{X}^T\\mathbf{X})^{-1}$ 乘以 $\\mathbf{X}^T\\mathbf{y}$ 来求得 $\\hat{\\boldsymbol{\\beta}}$：\n$$\n\\hat{\\boldsymbol{\\beta}} = \\begin{pmatrix} \\hat{\\beta}_0 \\\\ \\hat{\\beta}_1 \\\\ \\hat{\\beta}_2 \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{4}  0  0 \\\\ 0  \\frac{1}{4}  0 \\\\ 0  0  \\frac{1}{4} \\end{pmatrix} \\begin{pmatrix} 20 \\\\ 8 \\\\ 4 \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{4}(20) \\\\ \\frac{1}{4}(8) \\\\ \\frac{1}{4}(4) \\end{pmatrix} = \\begin{pmatrix} 5 \\\\ 2 \\\\ 1 \\end{pmatrix}\n$$\n\n最小二乘估计值为 $\\hat{\\beta}_0 = 5$，$\\hat{\\beta}_1 = 2$ 和 $\\hat{\\beta}_2 = 1$。题目要求答案以行矩阵 $(\\hat{\\beta}_0, \\hat{\\beta}_1, \\hat{\\beta}_2)$ 的形式呈现。", "answer": "$$\n\\boxed{\\begin{pmatrix} 5  2  1 \\end{pmatrix}}\n$$", "id": "1938980"}, {"introduction": "拟合模型后，其主要用途之一就是进行预测。本次实践将超越点估计，构建一个预测区间，这对于量化新预测相关的不确定性至关重要。你将学习如何利用回归分析的输出结果来解决一个实际的商业问题，从而体会到模型在现实决策中的价值。", "problem": "一家零售分析公司正在开发一个模型，以预测新快闪店的周销售额。他们从 `n=30` 家现有店铺收集了数据，并拟合了一个多元线性回归模型，其形式如下：\n$$\n\\text{Sales} = \\beta_0 + \\beta_1 \\cdot \\text{Size} + \\beta_2 \\cdot \\text{PopDensity} + \\epsilon\n$$\n其中 `Sales` 以千美元为单位，`Size` 是以平方米为单位的建筑面积，`PopDensity` 是周边1公里半径内的人口密度，单位是千人/平方公里。\n\n该团队基于样本数据的分析得出了以下结果：\n- 估计的回归方程为：$\\hat{y} = 51.5 + 0.24 x_1 + 14.8 x_2$，其中 $x_1$ 代表 Size，$x_2$ 代表 PopDensity。\n- 残差标准误：$s_e = 12.5$（单位：千美元）。\n- 模型中预测变量的数量为 $k=2$。\n\n该公司计划在一个 `PopDensity` 为4.5千人/平方公里的地区开设一家新店，其 `Size` 为520平方米。对于这个由向量 $x_0$ 代表的新观测值，统计软件报告的杠杆相关项的值为 $x_0^T (X^T X)^{-1} x_0 = 0.082$。此处，$X$ 是原始30个观测值的设计矩阵。\n\n使用给定的t分布临界值 $t_{0.05, 27} = 1.703$，为这家新店的周销售额构建一个90%的预测区间。报告该区间的下限和上限。两个值都应以千美元表示，并四舍五入到三位有效数字。", "solution": "题目要求我们为一个多元线性回归中的新观测值计算一个90%的预测区间，其中 $n=30$，$k=2$ 个预测变量，残差标准误 $s_{e}=12.5$，杠杆项 $h_{0}=x_{0}^{T}(X^{T}X)^{-1}x_{0}=0.082$。自由度为 $n-k-1=27$，给定的临界值为 $t_{0.05,27}=1.703$。\n\n在 $x_{1}=520$，$x_{2}=4.5$ 处的点预测值由拟合方程 $\\hat{y}=51.5+0.24x_{1}+14.8x_{2}$ 得到：\n$$\n\\hat{y}_{0}=51.5+0.24\\cdot 520+14.8\\cdot 4.5=51.5+124.8+66.6=242.9.\n$$\n\n预测的标准误是 $s_{e}\\sqrt{1+h_{0}}=12.5\\sqrt{1+0.082}=12.5\\sqrt{1.082}$。$90\\%$的预测区间是\n$$\n\\hat{y}_{0}\\pm t_{0.05,27}\\,s_{e}\\sqrt{1+h_{0}}\n=242.9\\pm 1.703\\cdot 12.5\\cdot \\sqrt{1.082}.\n$$\n计算边际误差：\n$$\n\\sqrt{1.082}\\approx 1.040192,\\quad 12.5\\cdot \\sqrt{1.082}\\approx 13.0024,\\quad\n1.703\\cdot 13.0024\\approx 22.143.\n$$\n因此，区间界限为\n$$\n\\text{Lower}=242.9-22.143\\approx 220.757,\\qquad\n\\text{Upper}=242.9+22.143\\approx 265.043.\n$$\n将每个界限四舍五入到三位有效数字（单位：千美元），得到 $221$ 和 $265$。", "answer": "$$\\boxed{\\begin{pmatrix}221  265\\end{pmatrix}}$$", "id": "1938959"}, {"introduction": "OLS模型依赖于几个关键假设，包括误差的方差恒定（同方差性）。这个高级实践将探讨当此假设被违反（异方差性）时会发生什么，以及如何使用加权最小二乘法（WLS）来修正它。通过基于代码的模拟，你将直接比较OLS和WLS的性能，从而对模型诊断和稳健估计技术有更深刻、更实际的理解。", "problem": "考虑一个多元线性回归模型，其单个观测索引为 $i \\in \\{1,\\dots,n\\}$，定义为 $y_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\varepsilon_i$，其中协变量 $x_{i1}$ 和 $x_{i2}$ 是分析师已知的非随机设计值，扰动项 $\\varepsilon_i$ 满足 $\\mathbb{E}[\\varepsilon_i \\mid x_{i1}, x_{i2}] = 0$ 和 $\\operatorname{Var}(\\varepsilon_i \\mid x_{i1}, x_{i2}) = \\sigma^2 x_{i1}^2$。该模型表现出异方差性，其方差与第一个协变量的平方成比例。您必须研究残差模式并比较两种估计程序：普通最小二乘法 (OLS) 和加权最小二乘法 (WLS)，其中 WLS 使用权重 $w_i = 1/x_{i1}^2$。\n\n仅从以下原则出发：(i) OLS 选择的系数能最小化未加权的残差平方和，(ii) WLS 选择的系数能最小化使用预先指定的非负权重的加权残差平方和，实现一个程序，对下述每个测试用例执行以下所有步骤：\n\n1. 数据生成。根据 $y_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\varepsilon_i$ 构建 $n$ 个观测值，其中 $\\varepsilon_i = \\sigma x_{i1} z_i$，$z_i$ 是独立同分布的标准正态变量。确保对所有 $i$ 都有 $x_{i1} > 0$，并将 $x_{i2}$ 构建为独立的协变量。使用指定的随机种子使所有抽样都具有确定性。\n2. 估计。计算 OLS 系数向量，作为 $\\sum_{i=1}^n (y_i - \\beta_0 - \\beta_1 x_{i1} - \\beta_2 x_{i2})^2$ 的最小化者；计算 WLS 系数向量，作为 $\\sum_{i=1}^n w_i (y_i - \\beta_0 - \\beta_1 x_{i1} - \\beta_2 x_{i2})^2$ 的最小化者，其中 $w_i = 1/x_{i1}^2$。\n3. 残差模式诊断。计算 OLS 残差 $e_i = y_i - \\hat{\\beta}_0^{\\text{OLS}} - \\hat{\\beta}_1^{\\text{OLS}} x_{i1} - \\hat{\\beta}_2^{\\text{OLS}} x_{i2}$。评估序列 $(e_i^2)_{i=1}^n$ 和 $(x_{i1}^2)_{i=1}^n$ 之间的经验皮尔逊相关性，以量化较大的 $x_{i1}$ 是否与较大的平方残差相关。\n4. 效率比较。令 $\\boldsymbol{\\beta}^\\star = (\\beta_0,\\beta_1,\\beta_2)$ 表示真实系数向量。为每个估计量计算系数平方误差，定义为 $\\mathrm{CSE}_{\\text{OLS}} = \\sum_{j=0}^2 (\\hat{\\beta}_j^{\\text{OLS}} - \\beta_j)^2$ 和 $\\mathrm{CSE}_{\\text{WLS}} = \\sum_{j=0}^2 (\\hat{\\beta}_j^{\\text{WLS}} - \\beta_j)^2$。报告比率 $R = \\mathrm{CSE}_{\\text{WLS}} / \\mathrm{CSE}_{\\text{OLS}}$。\n\n您的程序必须处理以下三个测试用例，每个用例由 $(n, \\boldsymbol{\\beta}^\\star, \\sigma, \\text{设计参数}, \\text{种子})$ 指定，其中设计参数定义了如何生成 $x_{i1}$ 和 $x_{i2}$：\n\n- 测试用例 A (通用正常路径)：$n = 200$, $\\boldsymbol{\\beta}^\\star = (1.5, 2.0, -1.0)$, $\\sigma = 1.0$, $x_{i1} \\sim \\text{Uniform}[0.2, 3.0]$, $x_{i2} \\sim \\text{Normal}(0, 1.5^2)$, 种子 $= 42$。\n- 测试用例 B (轻度异方差边界)：$n = 200$, $\\boldsymbol{\\beta}^\\star = (0.5, 1.0, 0.5)$, $\\sigma = 0.2$, $x_{i1} \\sim \\text{Uniform}[0.5, 1.5]$, $x_{i2} \\sim \\text{Normal}(0, 1.0^2)$, 种子 $= 123$。\n- 测试用例 C (近乎常数 $x_{i1}$ 的边缘情况)：$n = 200$, $\\boldsymbol{\\beta}^\\star = (2.0, -0.5, 1.0)$, $\\sigma = 0.8$, $x_{i1} = 1.0 + \\delta_i$，其中 $\\delta_i \\sim \\text{Normal}(0, 0.01^2)$ 被截断以保持 $x_{i1} \\geq 0.1$, $x_{i2} \\sim \\text{Normal}(0, 2.0^2)$, 种子 $= 987$。\n\n对于每个测试用例，计算：\n- $(e_i^2)$ 和 $(x_{i1}^2)$ 之间的皮尔逊相关性，以浮点数形式报告，四舍五入到六位小数。\n- 比率 $R = \\mathrm{CSE}_{\\text{WLS}} / \\mathrm{CSE}_{\\text{OLS}}$，以浮点数形式报告，四舍五入到六位小数。\n\n最终输出格式：您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，顺序为 $[\\text{corr}_A, R_A, \\text{corr}_B, R_B, \\text{corr}_C, R_C]$，其中每个条目都是一个四舍五入到六位小数的浮点数。此任务不涉及单位。", "solution": "该问题陈述已经过验证，被认为是合理的。它以线性回归的统计理论为科学基础，问题是适定的，提供了所有必要的参数和条件，并使用客观、正式的语言进行了阐述。不存在矛盾、模糊之处或不切实际的假设。该问题是计量经济学和统计学中一个标准的、非平凡的模拟练习，旨在说明当误差方差结构已知时，异方差性的后果以及加权最小二乘法 (WLS) 相对于普通最小二乘法 (OLS) 的更优效率。\n\n该问题研究了一个多元线性回归模型，对于一个包含 $n$ 个观测值的样本，该模型可以表示为矩阵形式：\n$$\n\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}\n$$\n其中 $\\mathbf{y}$ 是因变量观测值的 $n \\times 1$ 向量，$\\mathbf{X}$ 是协变量的 $n \\times p$ 设计矩阵（此处 $p=3$），$\\boldsymbol{\\beta}$ 是系数的 $p \\times 1$ 向量，$\\boldsymbol{\\varepsilon}$ 是未观测到的扰动项的 $n \\times 1$ 向量。对于此问题，$\\mathbf{X}$ 的第 $i$ 行（表示为 $\\mathbf{x}_i^T$）是 $[1, x_{i1}, x_{i2}]$，系数向量是 $\\boldsymbol{\\beta} = (\\beta_0, \\beta_1, \\beta_2)^T$。\n\n假设扰动项的条件均值为零，即 $\\mathbb{E}[\\boldsymbol{\\varepsilon} \\mid \\mathbf{X}] = \\mathbf{0}$。问题为方差指定了一种特定形式的异方差性：\n$$\n\\operatorname{Var}(\\varepsilon_i \\mid x_{i1}, x_{i2}) = \\sigma^2 x_{i1}^2\n$$\n这意味着误差向量的协方差矩阵 $\\boldsymbol{\\Omega} = \\operatorname{Cov}(\\boldsymbol{\\varepsilon} \\mid \\mathbf{X})$ 不再是经典模型中常见的 $\\sigma^2 \\mathbf{I}$，而是一个对角元素非恒定的对角矩阵：\n$$\n\\boldsymbol{\\Omega} = \\sigma^2 \\operatorname{diag}(x_{11}^2, x_{21}^2, \\dots, x_{n1}^2)\n$$\n条件 $x_{i1} > 0$ 确保所有方差均为正。\n\n**普通最小二乘法 (OLS) 估计**\nOLS 估计量 $\\hat{\\boldsymbol{\\beta}}_{\\text{OLS}}$ 是通过最小化残差平方和 (SSR) 求得的：\n$$\nS(\\boldsymbol{\\beta}) = \\sum_{i=1}^n (y_i - \\mathbf{x}_i^T \\boldsymbol{\\beta})^2 = (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})^T (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})\n$$\n将关于 $\\boldsymbol{\\beta}$ 的梯度设为零，即 $\\frac{\\partial S}{\\partial \\boldsymbol{\\beta}} = -2\\mathbf{X}^T(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta}) = \\mathbf{0}$，得出正规方程 $\\mathbf{X}^T\\mathbf{X}\\boldsymbol{\\beta} = \\mathbf{X}^T\\mathbf{y}$。因此，OLS 估计量为：\n$$\n\\hat{\\boldsymbol{\\beta}}_{\\text{OLS}} = (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{y}\n$$\n在给定的异方差性下，OLS 估计量仍然是无偏的，但它不再是最佳线性无偏估计量 (BLUE)；在所有线性无偏估计量中，它的方差不是最小的。\n\n**加权最小二乘法 (WLS) 估计**\nWLS 是 OLS 的一个扩展，它通过为每个观测值分配权重来解决异方差性问题，权重与误差方差成反比。WLS 估计量 $\\hat{\\boldsymbol{\\beta}}_{\\text{WLS}}$ 最小化加权残差平方和：\n$$\nS_W(\\boldsymbol{\\beta}) = \\sum_{i=1}^n w_i (y_i - \\mathbf{x}_i^T \\boldsymbol{\\beta})^2 = (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})^T \\mathbf{W} (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})\n$$\n其中 $\\mathbf{W}$ 是一个对角权重矩阵。最优权重为 $w_i \\propto 1/\\operatorname{Var}(\\varepsilon_i)$。对于此问题，我们选择 $w_i = 1/x_{i1}^2$，这对应于 $\\mathbf{W} = \\operatorname{diag}(1/x_{11}^2, \\dots, 1/x_{n1}^2)$。\n将 $S_W(\\boldsymbol{\\beta})$ 对 $\\boldsymbol{\\beta}$ 求导并令结果为零，得出 WLS 正规方程 $\\mathbf{X}^T\\mathbf{W}\\mathbf{X}\\boldsymbol{\\beta} = \\mathbf{X}^T\\mathbf{W}\\mathbf{y}$。WLS 估计量为：\n$$\n\\hat{\\boldsymbol{\\beta}}_{\\text{WLS}} = (\\mathbf{X}^T \\mathbf{W} \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{W} \\mathbf{y}\n$$\nWLS 可以看作是对一个转换后的模型应用 OLS。让我们定义转换后的观测值 $y_i^* = y_i \\sqrt{w_i} = y_i / x_{i1}$ 和转换后的协变量 $\\mathbf{x}_i^* = \\mathbf{x}_i \\sqrt{w_i} = \\mathbf{x}_i / x_{i1}$。转换后的模型是 $y_i^* = \\mathbf{x}_i^{*T} \\boldsymbol{\\beta} + \\varepsilon_i^*$，其中转换后的误差是 $\\varepsilon_i^* = \\varepsilon_i / x_{i1}$。转换后误差的方差是恒定的：\n$$\n\\operatorname{Var}(\\varepsilon_i^*) = \\operatorname{Var}\\left(\\frac{\\varepsilon_i}{x_{i1}}\\right) = \\frac{1}{x_{i1}^2} \\operatorname{Var}(\\varepsilon_i) = \\frac{1}{x_{i1}^2} (\\sigma^2 x_{i1}^2) = \\sigma^2\n$$\n由于转换后的模型是同方差的，高斯-马尔可夫定理适用，因此转换后模型的 OLS 估计量（也即 WLS 估计量）是 BLUE。因此，WLS 比 OLS 更有效。\n\n**计算步骤**\n对于每个测试用例，执行以下步骤：\n1.  **数据生成**：使用指定的种子，从各自的分布中抽取 $n$ 个 $x_{i1}$ 和 $x_{i2}$ 的值。构建设计矩阵 $\\mathbf{X}$。生成误差 $\\varepsilon_i = \\sigma x_{i1} z_i$，其中 $z_i \\sim \\mathcal{N}(0,1)$ 是独立同分布的抽样。计算响应向量 $\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta}^\\star + \\boldsymbol{\\varepsilon}$。\n2.  **估计**：使用矩阵公式计算向量 $\\hat{\\boldsymbol{\\beta}}_{\\text{OLS}}$ 和 $\\hat{\\boldsymbol{\\beta}}_{\\text{WLS}}$，为保证数值稳定性，通过 `np.linalg.solve` 实现。\n3.  **残差模式诊断**：计算 OLS 残差 $\\mathbf{e} = \\mathbf{y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}}_{\\text{OLS}}$。计算平方残差序列 $(e_i^2)_{i=1}^n$ 与平方协变量序列 $(x_{i1}^2)_{i=1}^n$ 之间的皮尔逊相关系数。预期会出现正相关，这证实了较大的残差方差与较大的 $x_{i1}$ 值相关联。\n4.  **效率比较**：为两个估计量计算系数平方误差 (CSE)：$\\mathrm{CSE}_{\\text{OLS}} = \\|\\hat{\\boldsymbol{\\beta}}_{\\text{OLS}} - \\boldsymbol{\\beta}^\\star\\|_2^2$ 和 $\\mathrm{CSE}_{\\text{WLS}} = \\|\\hat{\\boldsymbol{\\beta}}_{\\text{WLS}} - \\boldsymbol{\\beta}^\\star\\|_2^2$。计算比率 $R = \\mathrm{CSE}_{\\text{WLS}} / \\mathrm{CSE}_{\\text{OLS}}$。由于 WLS 预期会更有效，因此预计比率 $R < 1$。$R$ 的值量化了对于这个特定的数据实现，WLS 相对于 OLS 所实现的效率增益。较小的 $R$ 表示增益较大。这种增益的大小取决于异方差性的严重程度，而异方差性的严重程度由 $\\sigma$ 和 $x_{i1}$ 的变异性共同决定。\n\n将此程序应用于三个测试用例中的每一个，并汇编得出的相关性和比率值。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the regression problem for all test cases.\n    \"\"\"\n    test_cases = [\n        {\n            \"name\": \"A\",\n            \"n\": 200,\n            \"beta_star\": np.array([1.5, 2.0, -1.0]),\n            \"sigma\": 1.0,\n            \"x1_params\": {\"type\": \"uniform\", \"low\": 0.2, \"high\": 3.0},\n            \"x2_params\": {\"type\": \"normal\", \"mean\": 0, \"std\": 1.5},\n            \"seed\": 42\n        },\n        {\n            \"name\": \"B\",\n            \"n\": 200,\n            \"beta_star\": np.array([0.5, 1.0, 0.5]),\n            \"sigma\": 0.2,\n            \"x1_params\": {\"type\": \"uniform\", \"low\": 0.5, \"high\": 1.5},\n            \"x2_params\": {\"type\": \"normal\", \"mean\": 0, \"std\": 1.0},\n            \"seed\": 123\n        },\n        {\n            \"name\": \"C\",\n            \"n\": 200,\n            \"beta_star\": np.array([2.0, -0.5, 1.0]),\n            \"sigma\": 0.8,\n            \"x1_params\": {\"type\": \"truncated_normal\", \"base\": 1.0, \"mean\": 0, \"std\": 0.01, \"min_val\": 0.1},\n            \"x2_params\": {\"type\": \"normal\", \"mean\": 0, \"std\": 2.0},\n            \"seed\": 987\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        n = case[\"n\"]\n        beta_star = case[\"beta_star\"]\n        sigma = case[\"sigma\"]\n        seed = case[\"seed\"]\n        rng = np.random.default_rng(seed)\n\n        # 1. Data generation\n        # Generate x1\n        x1_params = case[\"x1_params\"]\n        if x1_params[\"type\"] == \"uniform\":\n            x1 = rng.uniform(low=x1_params[\"low\"], high=x1_params[\"high\"], size=n)\n        elif x1_params[\"type\"] == \"truncated_normal\":\n            delta = rng.normal(loc=x1_params[\"mean\"], scale=x1_params[\"std\"], size=n)\n            x1 = x1_params[\"base\"] + delta\n            # The problem states truncation is to keep x1 >= 0.1.\n            # With mean 1.0 and std 0.01, the probability of x1  0.1 is negligible.\n            # Using np.maximum is a robust way to enforce this constraint.\n            x1 = np.maximum(x1, x1_params[\"min_val\"])\n\n        # Generate x2\n        x2_params = case[\"x2_params\"]\n        x2 = rng.normal(loc=x2_params[\"mean\"], scale=x2_params[\"std\"], size=n)\n\n        # Construct Design Matrix X\n        X = np.column_stack((np.ones(n), x1, x2))\n\n        # Generate errors and response variable y\n        z = rng.normal(size=n)\n        epsilon = sigma * x1 * z\n        y = X @ beta_star + epsilon\n\n        # 2. Estimation\n        # OLS estimation\n        # beta_hat_ols = (X'X)^-1 * X'y\n        try:\n            beta_hat_ols = np.linalg.solve(X.T @ X, X.T @ y)\n        except np.linalg.LinAlgError:\n            # Fallback to pseudoinverse if matrix is singular, though unlikely with this design\n            beta_hat_ols = np.linalg.pinv(X.T @ X) @ X.T @ y\n\n        # WLS estimation\n        # beta_hat_wls = (X'WX)^-1 * X'Wy\n        # W is diagonal with w_i = 1/x1_i^2\n        w = 1.0 / (x1**2)\n        # Efficiently compute X'WX and X'Wy without forming the full W matrix\n        X_T_W_X = X.T @ (w[:, np.newaxis] * X)\n        X_T_W_y = X.T @ (w * y)\n        try:\n            beta_hat_wls = np.linalg.solve(X_T_W_X, X_T_W_y)\n        except np.linalg.LinAlgError:\n            beta_hat_wls = np.linalg.pinv(X_T_W_X) @ X_T_W_y\n\n        # 3. Residual pattern diagnostic\n        # Compute OLS residuals\n        e_ols = y - X @ beta_hat_ols\n        # Pearson correlation between e_ols^2 and x1^2\n        corr = np.corrcoef(e_ols**2, x1**2)[0, 1]\n\n        # 4. Efficiency comparison\n        # Coefficient Squared Error (CSE)\n        cse_ols = np.sum((beta_hat_ols - beta_star)**2)\n        cse_wls = np.sum((beta_hat_wls - beta_star)**2)\n        \n        # Ratio R\n        # Handle division by zero, although cse_ols should not be zero in this context.\n        if cse_ols == 0:\n            ratio_R = np.inf if cse_wls > 0 else 1.0\n        else:\n            ratio_R = cse_wls / cse_ols\n\n        results.extend([corr, ratio_R])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{val:.6f}' for val in results)}]\")\n\nsolve()\n```", "id": "3152038"}]}