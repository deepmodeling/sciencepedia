{"hands_on_practices": [{"introduction": "在构建包含交互作用的模型时，正确解释其系数至关重要。这个练习是一个思想实验，探讨了像标准化这样的特征变换如何影响模型的系数和假设检验。通过分析，您将深入理解为何在有交互项存在时，主效应的系数并不会简单地按比例缩放，这对于避免模型解释中的常见误区至关重要 [@problem_id:3132263]。", "problem": "一位数据分析师正在使用普通最小二乘法 (OLS) 研究一个响应变量 $y$ 和两个预测变量 $x_1$ 和 $x_2$。他们考虑了两个带有交互项和截距项的线性模型。第一个模型使用原始预测变量：\n$$\n\\text{Model-U:}\\quad y_i \\;=\\; \\beta_0 \\;+\\; \\beta_1 x_{1i} \\;+\\; \\beta_2 x_{2i} \\;+\\; \\beta_3 x_{1i} x_{2i} \\;+\\; \\varepsilon_i,\\quad i=1,\\dots,n,\n$$\n其中 $\\varepsilon_i$ 是均值为零的误差， $n$ 是样本大小。第二个模型使用标准化后的预测变量 $z_1$ 和 $z_2$，其定义为\n$$\nz_{1i} \\;=\\; \\frac{x_{1i} - \\bar{x}_1}{s_1},\\qquad z_{2i} \\;=\\; \\frac{x_{2i} - \\bar{x}_2}{s_2},\n$$\n其中 $\\bar{x}_j$ 是 $x_j$ 的样本均值， $s_j$ 是 $x_j$ 的样本标准差（对于 $j\\in\\{1,2\\}$），交互项是 $z_{1i} z_{2i}$ 的乘积：\n$$\n\\text{Model-S:}\\quad y_i \\;=\\; \\alpha_0 \\;+\\; \\alpha_1 z_{1i} \\;+\\; \\alpha_2 z_{2i} \\;+\\; \\alpha_3 z_{1i} z_{2i} \\;+\\; \\varepsilon_i.\n$$\n两个模型都包含截距项，并通过最小化 OLS 残差平方和进行拟合。假设 $s_1>0$ 且 $s_2>0$。\n\n下列哪个陈述是正确的？\n\nA. 对于所有的 $i$，来自模型 U 和模型 S 的 OLS 拟合值 $\\hat{y}_i$ 是相同的。\n\nB. 交互项系数按比例因子转换：$\\alpha_3 = \\beta_3 \\, s_1 s_2$。\n\nC. 在存在交互项的情况下，标准化确保了 $\\alpha_1 = \\beta_1 / s_1$ 和 $\\alpha_2 = \\beta_2 / s_2$。\n\nD. 在使用 OLS 时，关于无交互作用的原假设 $H_0:\\beta_3=0$ 与 $H_0:\\alpha_3=0$ 会得出相同的检验决策和 $p$ 值。\n\nE. 标准化预测变量会将主效应的方差膨胀因子 (VIF) 降低到 $1$。", "solution": "用户提供了一个关于比较两个普通最小二乘法 (OLS) 线性回归模型的问题。第一个模型使用原始预测变量，第二个模型使用标准化后的预测变量。我将首先验证问题陈述，然后进行详细解答。\n\n### 问题验证\n\n**第一步：提取已知信息**\n\n*   响应变量：$y$\n*   预测变量：$x_1$, $x_2$\n*   建模技术：普通最小二乘法 (OLS)\n*   模型-U (未标准化)：$y_i \\;=\\; \\beta_0 \\;+\\; \\beta_1 x_{1i} \\;+\\; \\beta_2 x_{2i} \\;+\\; \\beta_3 x_{1i} x_{2i} \\;+\\; \\varepsilon_i$, for $i=1,\\dots,n$。\n*   模型-S (标准化)：$y_i \\;=\\; \\alpha_0 \\;+\\; \\alpha_1 z_{1i} \\;+\\; \\alpha_2 z_{2i} \\;+\\; \\alpha_3 z_{1i} z_{2i} \\;+\\; \\varepsilon_i$。\n*   误差项 $\\varepsilon_i$ 的均值为零。\n*   $n$ 是样本大小。\n*   标准化预测变量定义如下：\n    $$z_{1i} \\;=\\; \\frac{x_{1i} - \\bar{x}_1}{s_1},\\qquad z_{2i} \\;=\\; \\frac{x_{2i} - \\bar{x}_2}{s_2}$$\n    其中 $\\bar{x}_j$ 是 $x_j$ 的样本均值， $s_j$ 是 $x_j$ 的样本标准差。\n*   假设：$s_1 > 0$ 且 $s_2 > 0$。\n\n**第二步：使用提取的信息进行验证**\n\n1.  **科学依据**：问题在统计线性模型理论中有坚实的基础。它探讨了预测变量变换（特别是标准化）对模型参数、拟合值和假设检验的影响。这是统计学和机器学习中的一个基础和标准课题。\n2.  **良态问题**：问题是良态的。先验假设是模型的设计矩阵具有满列秩，从而允许唯一的 OLS 解。变换被明确定义。所提出的问题是关于两个模型之间的数学关系，这些关系可以明确地确定。\n3.  **客观性**：问题使用精确的数学和统计术语陈述。它没有歧义、主观性或观点性。\n4.  **缺陷检查表**：该问题没有违反任何指定的无效标准。它是科学合理的、可形式化的、完整的、现实的且结构良好的。\n\n**第三步：结论和行动**\n\n问题陈述是**有效的**。我将进行全面的推导和分析。\n\n### 解答推导\n\n这个问题的核心在于理解模型 U 中的预测变量集合 $\\{1, x_1, x_2, x_1 x_2\\}$ 与模型 S 中的预测变量集合 $\\{1, z_1, z_2, z_1 z_2\\}$ 之间的关系。\n\n从 $x_j$ 到 $z_j$ 的变换是一个可逆的仿射变换：\n$x_{1i} = s_1 z_{1i} + \\bar{x}_1$\n$x_{2i} = s_2 z_{2i} + \\bar{x}_2$\n\n我们来检验每个模型的设计矩阵的列空间。设 $C(X_U)$ 是模型 U 的列空间， $C(X_S)$ 是模型 S 的列空间。\n\n模型 S 的预测变量可以表示为模型 U 预测变量的线性组合：\n*   $1$ 是两个模型中的共同预测变量。\n*   $z_{1i} = \\frac{1}{s_1} x_{1i} - \\frac{\\bar{x}_1}{s_1} (1)$。这是 $x_1$ 和 $1$ 的线性组合。\n*   $z_{2i} = \\frac{1}{s_2} x_{2i} - \\frac{\\bar{x}_2}{s_2} (1)$。这是 $x_2$ 和 $1$ 的线性组合。\n*   $z_{1i}z_{2i} = \\left(\\frac{x_{1i} - \\bar{x}_1}{s_1}\\right) \\left(\\frac{x_{2i} - \\bar{x}_2}{s_2}\\right) = \\frac{1}{s_1 s_2} (x_{1i}x_{2i} - \\bar{x}_2 x_{1i} - \\bar{x}_1 x_{2i} + \\bar{x}_1 \\bar{x}_2)$。这是 $x_1 x_2$、$x_1$、$x_2$ 和 $1$ 的线性组合。\n\n由于模型 S 中的每个预测变量都是模型 U 预测变量的线性组合，因此模型 S 设计矩阵的列空间是模型 U 设计矩阵列空间的子空间，即 $C(X_S) \\subseteq C(X_U)$。\n\n反之，模型 U 的预测变量也可以表示为模型 S 预测变量的线性组合：\n*   $1$ 是两个模型中的共同预测变量。\n*   $x_{1i} = s_1 z_{1i} + \\bar{x}_1 (1)$。这是 $z_1$ 和 $1$ 的线性组合。\n*   $x_{2i} = s_2 z_{2i} + \\bar{x}_2 (1)$。这是 $z_2$ 和 $1$ 的线性组合。\n*   $x_{1i} x_{2i} = (s_1 z_{1i} + \\bar{x}_1)(s_2 z_{2i} + \\bar{x}_2) = s_1 s_2 z_{1i} z_{2i} + s_1 \\bar{x}_2 z_{1i} + s_2 \\bar{x}_1 z_{2i} + \\bar{x}_1 \\bar{x}_2$。这是 $z_1 z_2$、$z_1$、$z_2$ 和 $1$ 的线性组合。\n\n由于模型 U 中的每个预测变量都是模型 S 预测变量的线性组合，因此 $C(X_U) \\subseteq C(X_S)$。\n结合这两个结果，我们得到 $C(X_U) = C(X_S)$。这两个模型是彼此的重新参数化，它们张成了完全相同的预测变量向量空间。\n\n### 逐项分析\n\n**A. 对于所有的 $i$，来自模型 U 和模型 S 的 OLS 拟合值 $\\hat{y}_i$ 是相同的。**\n在 OLS 中，拟合值向量 $\\hat{\\mathbf{y}}$ 是响应向量 $\\mathbf{y}$ 在设计矩阵列空间上的正交投影。如上所述，两个模型具有相同的预测变量列空间，即 $C(X_U) = C(X_S)$。$\\mathbf{y}$ 在这个公共空间上的投影必须是唯一的。因此，两个模型的拟合值向量 $\\hat{\\mathbf{y}}$ 是相同的，这意味着对于每个观测值 $i=1,\\dots,n$，$\\hat{y}_i$ 都是相同的。\n**结论：正确。**\n\n**B. 交互项系数按比例因子转换：$\\alpha_3 = \\beta_3 \\, s_1 s_2$。**\n为了找到系数之间的关系，我们可以将 $z_1$ 和 $z_2$ 的定义代入模型 S，并将其重新整理成模型 U 的形式。\n$$ y_i = \\alpha_0 + \\alpha_1 \\left(\\frac{x_{1i} - \\bar{x}_1}{s_1}\\right) + \\alpha_2 \\left(\\frac{x_{2i} - \\bar{x}_2}{s_2}\\right) + \\alpha_3 \\left(\\frac{x_{1i} - \\bar{x}_1}{s_1}\\right) \\left(\\frac{x_{2i} - \\bar{x}_2}{s_2}\\right) + \\varepsilon_i $$\n展开各项：\n$$ y_i = \\dots + \\frac{\\alpha_3}{s_1 s_2} (x_{1i} x_{2i} - \\bar{x}_2 x_{1i} - \\bar{x}_1 x_{2i} + \\bar{x}_1 \\bar{x}_2) + \\dots + \\varepsilon_i $$\n在这个展开的模型中，$x_{1i}x_{2i}$ 项的系数是 $\\frac{\\alpha_3}{s_1 s_2}$。通过与模型 U 中 $x_{1i}x_{2i}$ 的系数 $\\beta_3$ 进行比较，我们必然得到：\n$$ \\beta_3 = \\frac{\\alpha_3}{s_1 s_2} $$\n重新整理以求解 $\\alpha_3$：\n$$ \\alpha_3 = \\beta_3 s_1 s_2 $$\n**结论：正确。**\n\n**C. 在存在交互项的情况下，标准化确保了 $\\alpha_1 = \\beta_1 / s_1$ 和 $\\alpha_2 = \\beta_2 / s_2$。**\n我们继续 (B) 中的展开，并收集 $x_{1i}$ 和 $x_{2i}$ 的系数。\n$x_{1i}$ 的系数：从 $\\alpha_1$ 项中，我们得到 $\\frac{\\alpha_1}{s_1}$。从 $\\alpha_3$ 项中，我们得到 $-\\frac{\\alpha_3 \\bar{x}_2}{s_1 s_2}$。因此，$x_{1i}$ 的总系数是 $\\frac{\\alpha_1}{s_1} - \\frac{\\alpha_3 \\bar{x}_2}{s_1 s_2}$。与模型 U 比较，我们得到：\n$$ \\beta_1 = \\frac{\\alpha_1}{s_1} - \\frac{\\alpha_3 \\bar{x}_2}{s_1 s_2} $$\n求解 $\\alpha_1$：\n$$ \\alpha_1 = s_1 \\beta_1 + \\frac{\\alpha_3 \\bar{x}_2}{s_2} $$\n使用 (B) 中的关系 $\\alpha_3 = \\beta_3 s_1 s_2$：\n$$ \\alpha_1 = s_1 \\beta_1 + \\frac{(\\beta_3 s_1 s_2) \\bar{x}_2}{s_2} = s_1 \\beta_1 + s_1 \\bar{x}_2 \\beta_3 = s_1(\\beta_1 + \\bar{x}_2 \\beta_3) $$\n该陈述声称 $\\alpha_1 = \\beta_1 / s_1$。这在一般情况下显然是不正确的。这种简单的比例关系仅在第二项为零时才成立，这要求 $\\beta_3=0$（无交互作用）或 $\\bar{x}_2=0$（预测变量 $x_2$ 已中心化）。由于问题包含了交互作用，并且没有假设数据是中心化的，因此该陈述是错误的。对于 $\\alpha_2$ 和 $\\beta_2$ 也可以进行类似的推导。\n**结论：不正确。**\n\n**D. 在使用 OLS 时，关于无交互作用的原假设 $H_0:\\beta_3=0$ 与 $H_0:\\alpha_3=0$ 会得出相同的检验决策和 $p$ 值。**\n单个系数的假设检验通常是 t-检验或等价的 F-检验。F-检验比较的是完整模型的残差平方和 (RSS) 与一个将该系数约束为零的简化模型的 RSS。\nF-统计量由 $F = \\frac{(\\text{RSS}_{\\text{reduced}} - \\text{RSS}_{\\text{full}})/df_{\\text{dropped}}}{\\text{RSS}_{\\text{full}}/df_{\\text{full}}}$ 给出。\n对于检验 $H_0: \\beta_3=0$，完整模型是模型 U，简化模型是 $y_i = \\beta_0' + \\beta_1' x_{1i} + \\beta_2' x_{2i} + \\varepsilon_i'$。\n对于检验 $H_0: \\alpha_3=0$，完整模型是模型 S，简化模型是 $y_i = \\alpha_0' + \\alpha_1' z_{1i} + \\alpha_2' z_{2i} + \\varepsilon_i'$。\n从 (A) 我们知道，完整模型 U 和完整模型 S 的 RSS 是相同的：$\\text{RSS}_{\\text{full,U}} = \\text{RSS}_{\\text{full,S}}$。\n现在考虑简化模型。简化 U 模型的预测变量空间是 $\\text{span}\\{1, x_1, x_2\\}$。简化 S 模型的预测变量空间是 $\\text{span}\\{1, z_1, z_2\\}$。由于 $z_1$ 和 $z_2$ 只是 $x_1$ 和 $x_2$ 的仿射变换，这两个空间是相同的。因此，在这两个空间上的投影是相同的，它们的 RSS 值也是相同的：$\\text{RSS}_{\\text{reduced,U}} = \\text{RSS}_{\\text{reduced,S}}$。\n由于两个假设检验的所有相应 RSS 值和自由度都相同，因此得出的 F-统计量必须相同。相同的 F-统计量意味着相同的 p 值，因此在任何给定的显著性水平下，检验决策都是相同的。\n另外，$\\alpha_3$ 的 t-统计量是 $t_{\\alpha_3} = \\hat{\\alpha}_3/SE(\\hat{\\alpha}_3)$。从 (B) 可知，$\\hat{\\alpha}_3 = \\hat{\\beta}_3 s_1 s_2$。标准误也成比例转换，$SE(\\hat{\\alpha}_3) = SE(\\hat{\\beta}_3) s_1 s_2$。因此，$t_{\\alpha_3} = \\frac{\\hat{\\beta}_3 s_1 s_2}{SE(\\hat{\\beta}_3) s_1 s_2} = t_{\\beta_3}$。t-统计量是相同的。\n**结论：正确。**\n\n**E. 标准化预测变量会将主效应的方差膨胀因子 (VIF) 降低到 $1$。**\n预测变量 $j$ 的 VIF 是 $VIF_j = 1/(1 - R_j^2)$，其中 $R_j^2$ 是将预测变量 $j$ 对模型中所有其他预测变量进行回归得到的决定系数。VIF 为 $1$ 意味着 $R_j^2=0$，这表示预测变量 $j$ 与所有其他预测变量完全不相关。\n我们来考虑模型 S 中的 $VIF_{z_1}$。预测变量是 $\\{1, z_1, z_2, z_1 z_2\\}$。为了计算 $VIF_{z_1}$，我们需要将 $z_1$ 对 $\\{1, z_2, z_1 z_2\\}$ 进行回归得到的 $R^2$。\n根据定义，标准化变量 $z_1$ 的均值为零，因此它与截距项（全为 1 的向量）正交。但是，$z_1$ 通常不与其他预测变量 $z_2$ 和 $z_1 z_2$ 正交。\n1. $z_1$ 和 $z_2$ 之间的相关性：$\\text{corr}(z_1, z_2) = \\text{corr}(x_1, x_2)$。这通常不为零。\n2. $z_1$ 和 $z_1 z_2$ 之间的相关性：样本协方差与 $\\sum_{i=1}^n z_{1i} (z_{1i} z_{2i}) = \\sum_{i=1}^n z_{1i}^2 z_{2i}$ 成正比。这个和不保证为零。例如，如果 $z_1$ 和 $z_2$ 呈正相关，那么 $z_{1i}^2$ 较大的点可能倾向于有正的 $z_{2i}$，从而导致一个正的和。\n由于 $z_1$ 通常与 $z_2$ 和/或 $z_1 z_2$ 相关，将 $z_1$ 对其他预测变量进行回归得到的 $R^2$ 不会为零。因此，$VIF_{z_1}$ 不会是 $1$。虽然标准化通常可以减少多重共线性（特别是在主效应和像 $x_1$ 与 $x_1x_2$ 这样的交互项之间），但它并不能消除多重共线性或保证 VIF 为 $1$。\n**结论：不正确。**", "answer": "$$\\boxed{ABD}$$", "id": "3132263"}, {"introduction": "理论是基础，但通过模拟亲眼见证统计现象会带来更深刻的理解。本练习将指导您通过编码来探究交互模型中的一个核心挑战：多重共线性 [@problem_id:3132247]。您将通过蒙特卡洛模拟，直观地观察到预测变量之间的相关性如何增加交互项系数估计的不确定性（即更宽的置信区间），从而为在实践中采用均值中心化等技术提供坚实的实践依据。", "problem": "本题要求您通过受控模拟，检验在正态线性模型中，预测变量之间的相关性如何增加交互项估计系数的不确定性。构建一个程序，针对一组指定的测试用例，在预测变量相关性变化的情况下，估计交互项系数的双侧置信区间的经验覆盖率和平均长度。该问题以纯数学术语进行描述，并要求使用正态线性模型、普通最小二乘法 (OLS) 以及使用学生t分布的小样本推断等基本原理。\n\n考虑正态线性模型\n$$\ny = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_{12} (x_1 x_2) + \\varepsilon,\n$$\n其中 $\\varepsilon \\sim \\mathcal{N}(0,\\sigma^2)$ 且独立于预测变量。对于每次模拟重复，将预测变量生成为相关的二元正态向量\n$$\n\\begin{pmatrix}\nx_1 \\\\\nx_2\n\\end{pmatrix}\n\\sim \\mathcal{N}\\!\\left(\n\\begin{pmatrix}\n\\mu_1 \\\\\n\\mu_2\n\\end{pmatrix},\n\\begin{pmatrix}\n1 & \\rho \\\\\n\\rho & 1\n\\end{pmatrix}\n\\right)\n$$\n其均值固定为 $\\mu_1 = \\mu_2 = 1$，相关系数为 $\\rho \\in (-1,1)$。使用固定参数 $\\beta_0 = 0$, $\\beta_1 = 1$, $\\beta_2 = 1$, $\\beta_{12} = 0.5$ 和 $\\sigma = 1$ 的模型生成响应变量。\n\n对于每个测试用例，您的程序必须执行包含 $R$ 次独立重复的蒙特卡洛模拟。在每次重复中：从指定的分布中为 $(x_1,x_2)$ 和 $\\varepsilon$ 抽取一个大小为 $n$ 的样本，拟合包含截距项、主效应 $x_1$、$x_2$ 和交互作用 $x_1 x_2$ 的 OLS 模型，并使用自由度为 $n-p$ 的学生t分布（其中 $p=4$ 是包括截距在内的回归系数数量）为 $\\beta_{12}$ 构建一个名义水平为 $1-\\alpha = 0.95$ 的双侧置信区间。记录真实的 $\\beta_{12}$ 是否包含在区间内（如果包含，则覆盖指示符为 $1$，否则为 $0$）以及区间长度（一个非负实数）。在 $R$ 次重复之后，为每个测试用例报告：\n- 经验覆盖概率，以小数形式表示（覆盖指示符的平均值），以及\n- 平均区间长度（重复区间长度的平均值）。\n\n对所有测试用例使用以下固定值：\n- $R = 500$，\n- $\\alpha = 0.05$，\n- $\\beta_0 = 0$, $\\beta_1 = 1$, $\\beta_2 = 1$, $\\beta_{12} = 0.5$，\n- $\\sigma = 1$，\n- $\\mu_1 = 1$, $\\mu_2 = 1$，\n- 随机种子固定为 $123456$ 以确保可复现性。\n\n测试套件：\n1. $(n,\\rho) = (200, 0.0)$,\n2. $(n,\\rho) = (200, 0.9)$,\n3. $(n,\\rho) = (200, -0.9)$,\n4. $(n,\\rho) = (60, 0.99)$.\n\n输出规范：\n- 对于列表中的每个测试用例，按顺序输出一个双元素列表 $[c,\\ell]$，其中 $c$ 是经验覆盖概率，$\\ell$ 是平均置信区间长度。$c$ 和 $\\ell$ 都必须四舍五入到小数点后 $3$ 位。\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，列表中的每个元素本身都是相同格式的双元素列表，例如：\n$[[c_1,\\ell_1],[c_2,\\ell_2],[c_3,\\ell_3],[c_4,\\ell_4]]$。\n- 本题不涉及物理单位或角度。覆盖率必须以小数表示，不能使用百分号。\n\n科学真实性和推导基础：\n- 仅使用正态线性模型、OLS估计量和基于t分布的小样本置信区间的标准属性作为基本依据。不要假设或使用超出这些基础的任何快捷公式。\n- 确保所有随机数生成都由固定的种子控制，以便输出是可复现的。\n\n您的程序必须是一个完整的、可运行的脚本，不需要用户输入，也不需要外部文件，并且必须严格遵守指定的输出格式。", "solution": "该问题是有效的。这是一个在计算统计学中定义明确且具有科学依据的练习，旨在标准正态线性模型框架内，研究多重共线性对交互项推断的影响。所有必需的参数、常数和程序都已明确指定。\n\n目标是进行一次蒙特卡洛模拟，以量化两个预测变量 $x_1$ 和 $x_2$ 之间的相关性 $\\rho$ 如何影响交互项系数 $\\beta_{12}$ 的置信区间的经验覆盖率和平均长度。\n\n其统计基础是正态线性模型，其矩阵形式表示为：\n$$\ny = X\\beta + \\varepsilon\n$$\n其中 $y$ 是一个 $n \\times 1$ 的观测向量，$X$ 是 $n \\times p$ 的设计矩阵，$\\beta$ 是 $p \\times 1$ 的系数向量，$\\varepsilon$ 是 $n \\times 1$ 的未观测误差向量。对于本问题，模型是 $y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_{12} (x_1 x_2) + \\varepsilon$，因此参数数量为 $p=4$。系数向量为 $\\beta = [\\beta_0, \\beta_1, \\beta_2, \\beta_{12}]^T$。设计矩阵 $X$ 的第 $i$ 行为 $[1, x_{i1}, x_{i2}, x_{i1}x_{i2}]$。误差 $\\varepsilon_i$ 被假定为独立同分布于 $\\mathcal{N}(0, \\sigma^2)$。\n\n系数向量 $\\beta$ 的普通最小二乘法 (OLS) 估计量由下式给出：\n$$\n\\hat{\\beta} = (X^T X)^{-1} X^T y\n$$\n在模型假设下，给定设计矩阵 $X$，OLS估计量的条件分布是正态的，其均值为 $\\beta$，方差-协方差矩阵为：\n$$\n\\text{Var}(\\hat{\\beta} | X) = \\sigma^2 (X^T X)^{-1}\n$$\n在实际应用中，误差方差 $\\sigma^2$ 是未知的，必须从数据中估计。$\\sigma^2$ 的无偏估计量是：\n$$\ns^2 = \\frac{e^T e}{n-p} = \\frac{\\sum_{i=1}^n (y_i - \\hat{y}_i)^2}{n-p}\n$$\n其中 $e = y - X\\hat{\\beta}$ 是残差向量，$n-p$ 是自由度。系数向量的估计方差则为 $\\hat{\\text{Var}}(\\hat{\\beta}) = s^2(X^T X)^{-1}$。单个系数估计 $\\hat{\\beta}_j$ 的标准误是该矩阵第 $j$ 个对角元素的平方根：\n$$\nse(\\hat{\\beta}_j) = \\sqrt{s^2 \\left((X^T X)^{-1}\\right)_{jj}}\n$$\n其中索引 $j$ 的范围从 $0$ 到 $p-1=3$。我们感兴趣的系数 $\\beta_{12}$ 对应于索引 $j=3$。\n\n对于小样本，推断基于学生t分布。交互项系数 $\\beta_{12}$ 的枢轴量为：\n$$\n\\frac{\\hat{\\beta}_{12} - \\beta_{12}}{se(\\hat{\\beta}_{12})} \\sim t_{n-p}\n$$\n其中 $t_{n-p}$ 是自由度为 $n-p$ 的学生t分布。在名义水平 $1-\\alpha$ 下，$\\beta_{12}$ 的双侧置信区间构造如下：\n$$\n\\hat{\\beta}_{12} \\pm t_{1-\\alpha/2, n-p} \\cdot se(\\hat{\\beta}_{12})\n$$\n其中 $t_{1-\\alpha/2, n-p}$ 是 $t_{n-p}$ 分布的上 $(1-\\alpha/2)$ 临界值。该区间的长度为 $2 \\cdot t_{1-\\alpha/2, n-p} \\cdot se(\\hat{\\beta}_{12})$。设计矩阵 $X$ 中预测变量之间的高度相关性（多重共线性）会使 $(X^T X)^{-1}$ 的对角元素膨胀，导致更大的标准误，从而使置信区间变宽。本模拟旨在定量地展示这一效应。\n\n对于每个测试用例 $(n, \\rho)$，模拟过程如下：\n1.  使用固定种子 $123456$ 初始化一个随机数生成器。\n2.  执行一个包含 $R=500$ 次重复的循环。在每次重复中：\n    a. 从一个均值为 $\\mu = [1, 1]^T$、协方差矩阵为 $\\Sigma = \\begin{pmatrix} 1 & \\rho \\\\ \\rho & 1 \\end{pmatrix}$ 的二元正态分布中，生成一个大小为 $n$ 的预测变量对 $(x_1, x_2)$ 样本。\n    b. 构建一个 $n \\times 4$ 的设计矩阵 $X$，其列分别为截距项、$x_1$、$x_2$ 和交互项 $x_1 x_2$。\n    c. 从 $\\mathcal{N}(0, \\sigma^2)$（其中 $\\sigma=1$）中生成一个大小为 $n$ 的误差样本 $\\varepsilon$。\n    d. 使用真实模型 $y = X\\beta_{true} + \\varepsilon$ 生成响应向量 $y$，其中 $\\beta_{true} = [0, 1, 1, 0.5]^T$。\n    e. 计算 OLS 估计值 $\\hat{\\beta} = (X^T X)^{-1} X^T y$。\n    f. 计算估计的误差方差 $s^2$。\n    g. 使用上述公式计算交互项系数估计的标准误 $se(\\hat{\\beta}_{12})$。\n    h. 确定 $\\alpha=0.05$ 时的临界值 $t_{crit} = t_{1-\\alpha/2, n-4}$。\n    i. 构建 $\\beta_{12}$ 的置信区间并计算其长度。\n    j. 记录一个二元指示符（如果区间包含真值 $\\beta_{12}=0.5$，则为 $1$，否则为 $0$）和区间长度。\n3.  在所有重复结束后，计算覆盖指示符的平均值以获得经验覆盖概率，并计算区间长度的平均值。\n4.  存储该测试用例的最终四舍五入结果。\n\n对所有四个测试用例重复此过程，并将结果汇编成最终指定的输出格式。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import t\n\ndef solve():\n    \"\"\"\n    Simulates the effect of predictor correlation on the confidence interval \n    of an interaction term in a linear model.\n    \"\"\"\n    # Define fixed parameters from the problem statement\n    R = 500\n    ALPHA = 0.05\n    BETA_TRUE = np.array([0.0, 1.0, 1.0, 0.5])  # [beta0, beta1, beta2, beta12]\n    SIGMA = 1.0\n    MU = np.array([1.0, 1.0])\n    RANDOM_SEED = 123456\n    \n    # Define test cases (n, rho)\n    test_cases = [\n        (200, 0.0),\n        (200, 0.9),\n        (200, -0.9),\n        (60, 0.99),\n    ]\n\n    all_results = []\n    \n    # Initialize a single random number generator for reproducibility across all cases\n    rng = np.random.default_rng(RANDOM_SEED)\n\n    for n, rho in test_cases:\n        coverage_indicators = []\n        interval_lengths = []\n\n        # Number of model parameters (intercept, x1, x2, x1*x2)\n        p = 4\n        # Degrees of freedom for the t-distribution\n        df = n - p\n        # Critical t-value for a two-sided (1-alpha)% confidence interval\n        t_crit = t.ppf(1 - ALPHA / 2, df)\n\n        for _ in range(R):\n            # Step 1: Generate the data for one replicate\n            \n            # Define the covariance matrix for the predictors (x1, x2)\n            cov_matrix = np.array([[1.0, rho], [rho, 1.0]])\n            \n            # Generate n samples of [x1, x2]\n            predictors = rng.multivariate_normal(MU, cov_matrix, size=n)\n            x1 = predictors[:, 0]\n            x2 = predictors[:, 1]\n            \n            # Construct the design matrix X of size n x p\n            X = np.ones((n, p))\n            X[:, 1] = x1\n            X[:, 2] = x2\n            X[:, 3] = x1 * x2\n            \n            # Generate n error terms\n            epsilon = rng.normal(loc=0, scale=SIGMA, size=n)\n            \n            # Generate the response variable y\n            y = X @ BETA_TRUE + epsilon\n\n            # Step 2: Fit the OLS model and get coefficient estimates\n            \n            # Calculate (X'X)^-1 for standard errors\n            try:\n                inv_xtx = np.linalg.inv(X.T @ X)\n            except np.linalg.LinAlgError:\n                # In extreme cases of multicollinearity, matrix might be singular.\n                # Skip this replicate if it happens.\n                continue\n\n            # Calculate OLS coefficient estimates: beta_hat = (X'X)^-1 * X'y\n            beta_hat = inv_xtx @ X.T @ y\n            \n            # Step 3: Construct the confidence interval for the interaction term (beta_12)\n            \n            # Calculate residuals\n            residuals = y - X @ beta_hat\n            \n            # Calculate residual sum of squares (RSS)\n            rss = residuals.T @ residuals\n            \n            # Estimate the error variance (s^2)\n            s2 = rss / df\n            \n            # Calculate the standard error of the interaction coefficient estimate\n            # This is the 4th coefficient (index 3)\n            se_beta12 = np.sqrt(s2 * inv_xtx[3, 3])\n            \n            # Calculate the margin of error\n            margin_of_error = t_crit * se_beta12\n            \n            # Calculate the length of the confidence interval\n            length = 2 * margin_of_error\n            interval_lengths.append(length)\n            \n            # Get the point estimate for the interaction coefficient\n            beta12_hat = beta_hat[3]\n            \n            # Define the confidence interval bounds\n            lower_bound = beta12_hat - margin_of_error\n            upper_bound = beta12_hat + margin_of_error\n            \n            # Check if the true parameter value is covered by the interval\n            true_beta12 = BETA_TRUE[3]\n            covered = 1 if (lower_bound = true_beta12 = upper_bound) else 0\n            coverage_indicators.append(covered)\n\n        # Calculate empirical coverage and average length for the current test case\n        empirical_coverage = np.mean(coverage_indicators)\n        avg_length = np.mean(interval_lengths)\n        \n        # Format results as specified (rounded to 3 decimal places)\n        result = [round(empirical_coverage, 3), round(avg_length, 3)]\n        all_results.append(result)\n\n    # Final print statement in the exact required format: [[c1,l1],[c2,l2],...]\n    formatted_results = [f\"[{c},{l}]\" for c, l in all_results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3132247"}, {"introduction": "现实世界的数据模式往往比简单的全局模型所假设的要复杂得多，交互作用可能只存在于特征空间的特定区域。本练习将挑战您去处理这样一个场景：一个交互效应是“局部”的 [@problem_id:3132277]。通过比较一个全局多项式模型和一个更具适应性的局部模型（决策树）的性能，您将体会到非线性模型在捕捉复杂数据结构时的优势，并培养出根据数据特性选择合适模型的关键能力。", "problem": "您被要求编写一个完整的、可运行的程序，用以比较全局多项式交互回归与局部逐段常数回归树。比较所用的数据中，预测变量之间的交互作用仅存在于输入空间的一个子集内。您的实现必须遵循经验风险最小化（使用平方损失）、普通最小二乘法和贪心平方和分裂的定义。\n\n考虑预测变量 $x_1$ 和 $x_2$，它们从 $[0,1]$ 上的均匀分布中独立采样。潜在回归函数为\n$$\nf(x_1,x_2) = a_1 x_1 + a_2 x_2 + b_1 x_1^2 + b_2 x_2^2 + \\mathbf{1}\\{x_1  \\tau_1, x_2  \\tau_2\\}\\, c\\, x_1 x_2,\n$$\n其中 $\\mathbf{1}\\{\\cdot\\}$ 是指示函数，$a_1 = 1.0$，$a_2 = -1.0$，$b_1 = 0.5$，$b_2 = -0.5$，而 $(\\tau_1,\\tau_2)$ 和 $c$ 是特定情景的参数。观测值的生成方式如下\n$$\ny = f(x_1,x_2) + \\varepsilon,\n$$\n其中 $\\varepsilon \\sim \\mathcal{N}(0,\\sigma^2)$ 是独立高斯噪声。\n\n您的程序必须实现并比较以下两种估计量：\n\n- 带交互项的全局多项式：使用普通最小二乘法（OLS, Ordinary Least Squares）对特征向量拟合一个线性模型\n$$\n\\phi(x_1,x_2) = \\left[1,\\, x_1,\\, x_2,\\, x_1^2,\\, x_2^2,\\, x_1 x_2\\right].\n$$\n该模型在整个输入空间上施加了一个单一的全局交互项 $x_1 x_2$。\n\n- 局部回归树：拟合一个二元、轴对齐的回归树，该树在每个叶节点中预测一个常数。通过在每次分裂时贪心地最小化经验平方误差和来构建树。在一个包含样本响应 $\\{y_i\\}_{i=1}^n$ 的节点上，节点不纯度为\n$$\n\\mathrm{SSE} = \\sum_{i=1}^n \\left(y_i - \\bar{y}\\right)^2,\\quad \\bar{y} = \\frac{1}{n}\\sum_{i=1}^n y_i.\n$$\n在特征 $j \\in \\{1,2\\}$ 和阈值 $t$ 处的候选分裂将节点划分为左右两个子节点；选择能够最小化子节点 SSE 之和的分裂，同时要满足最小叶节点大小的约束。当达到最大深度、没有有效分裂满足最小叶节点大小，或者没有分裂能减少 SSE 时，停止分裂。每个叶节点预测该叶节点内 $y$ 的样本均值。\n\n为了进行评估，对于下面的每个情景，生成一个大小为 $n_{\\text{train}}$ 的训练集和一个大小为 $n_{\\text{test}} = 20000$ 的独立测试集。在训练数据上训练这两个模型。在测试集上，计算相对于无噪声目标 $f(x_1,x_2)$ 的样本外均方误差（MSE）：\n$$\n\\mathrm{MSE} = \\frac{1}{n_{\\text{test}}}\\sum_{i=1}^{n_{\\text{test}}} \\left(\\hat{f}(x_{1,i},x_{2,i}) - f(x_{1,i},x_{2,i})\\right)^2.\n$$\n此 MSE 衡量的是函数逼近的质量，而非噪声预测。\n\n使用以下测试情景套件。对于每个情景 $k$，使用指定的随机种子进行训练和测试数据生成，以确保可复现性。树使用指定的最大深度和最小叶节点大小。\n\n- 情景 1：$n_{\\text{train}} = 400$，$c = 3.0$，$(\\tau_1,\\tau_2) = (0.5, 0.5)$，$\\sigma = 0.1$，树最大深度 $= 2$，树最小叶节点大小 $= 20$，训练种子 $= 7$，测试种子 $= 97$。\n- 情景 2：$n_{\\text{train}} = 400$，$c = 0.5$，$(\\tau_1,\\tau_2) = (0.6, 0.6)$，$\\sigma = 0.1$，树最大深度 $= 2$，树最小叶节点大小 $= 20$，训练种子 $= 8$，测试种子 $= 98$。\n- 情景 3：$n_{\\text{train}} = 400$，$c = 3.0$，$(\\tau_1,\\tau_2) = (0.5, 0.5)$，$\\sigma = 1.0$，树最大深度 $= 2$，树最小叶节点大小 $= 40$，训练种子 $= 9$，测试种子 $= 99$。\n- 情景 4：$n_{\\text{train}} = 800$，$c = 5.0$，$(\\tau_1,\\tau_2) = (0.85, 0.85)$，$\\sigma = 0.1$，树最大深度 $= 3$，树最小叶节点大小 $= 10$，训练种子 $= 10$，测试种子 $= 100$。\n\n您的程序必须按顺序为每个情景输出两个浮点数：首先是带交互项的全局多项式模型的 MSE，然后是局部回归树的 MSE。将所有情景的结果连接成一个列表。\n\n最终输出格式：您的程序应生成单行输出，其中包含一个用方括号括起来的、逗号分隔的十进制数列表，每个数字精确到 $6$ 位小数，顺序如下\n$$\n[\\mathrm{MSE}^{\\text{global}}_1,\\mathrm{MSE}^{\\text{tree}}_1,\\mathrm{MSE}^{\\text{global}}_2,\\mathrm{MSE}^{\\text{tree}}_2,\\mathrm{MSE}^{\\text{global}}_3,\\mathrm{MSE}^{\\text{tree}}_3,\\mathrm{MSE}^{\\text{global}}_4,\\mathrm{MSE}^{\\text{tree}}_4].\n$$\n不应打印任何其他文本。", "solution": "该问题要求在一个模拟数据集上比较两种回归模型——一个全局多项式模型和一个局部回归树。在该数据集中，交互项仅存在于特征空间的特定子区域。比较基于相对于真实的、无噪声的数据生成函数的样本外均方误差（MSE）。\n\n### 步骤 1：问题验证\n\n第一步是验证问题陈述。\n\n**提取的已知条件：**\n- **预测变量：** $x_1, x_2$ 从 $[0,1]$ 上的均匀分布中独立采样。\n- **潜在函数：** $f(x_1,x_2) = a_1 x_1 + a_2 x_2 + b_1 x_1^2 + b_2 x_2^2 + \\mathbf{1}\\{x_1  \\tau_1, x_2  \\tau_2\\}\\, c\\, x_1 x_2$。\n- **常数：** $a_1 = 1.0$，$a_2 = -1.0$，$b_1 = 0.5$，$b_2 = -0.5$。\n- **噪声模型：** $y = f(x_1,x_2) + \\varepsilon$，其中 $\\varepsilon \\sim \\mathcal{N}(0,\\sigma^2)$。\n- **估计量 1（全局多项式）：** 对特征向量 $\\phi(x_1,x_2) = [1, x_1, x_2, x_1^2, x_2^2, x_1 x_2]$ 进行普通最小二乘法（OLS）拟合。\n- **估计量 2（局部回归树）：** 一个二元的、轴对齐的树，通过在每次分裂时贪心地最小化平方误差和（SSE）来构建，受最大深度和最小叶节点大小的限制。叶节点预测响应的样本均值。\n- **树不纯度：** $\\mathrm{SSE} = \\sum_{i=1}^n (y_i - \\bar{y})^2$。\n- **评估指标：** 在大小为 $n_{\\text{test}} = 20000$ 的测试集上的样本外 MSE，计算公式为 $\\mathrm{MSE} = \\frac{1}{n_{\\text{test}}}\\sum_{i=1}^{n_{\\text{test}}} (\\hat{f}(x_{1,i},x_{2,i}) - f(x_{1,i},x_{2,i}))^2$。\n- **情景：** 定义了四个情景，包含 $n_{\\text{train}}$、$c$、$(\\tau_1,\\tau_2)$、$\\sigma$、树的最大深度、树的最小叶节点大小以及用于训练和测试数据生成的随机种子的具体参数。\n\n**对照标准进行验证：**\n- **科学基础：** 该问题牢固地植根于标准的统计学习理论，采用了诸如 OLS、回归树、使用平方误差损失的经验风险最小化以及基于仿真的模型比较等成熟概念。所有定义都是标准的且在数学上是合理的。\n- **适定性：** 问题提供了一套完整的指令。数据生成过程、模型结构、拟合程序和评估指标都得到了明确定义。使用随机种子确保了数值结果的可复现性。存在一个唯一且有意义的解。\n- **客观性：** 问题以精确、正式的语言陈述，没有歧义或主观论断。\n- **完整性与一致性：** 提供了四个情景中每一个所需的所有参数，并且没有内部矛盾。\n- **相关性：** 该问题直接探讨了交互作用和非线性建模这一统计学习的核心主题。它提出了一个有意义的问题，即一个可能被错误设定的全局模型与一个可能过拟合或更擅长捕捉局部现象的局部、更灵活的模型之间的权衡。\n\n**结论：** 该问题是有效的、科学合理的、适定的和完整的。我们可以继续进行求解。\n\n### 步骤 2：解法推导与算法设计\n\n任务的核心是为每个给定的情景实现数据生成过程、两种指定的模型以及评估程序。\n\n**数据生成**\n对于每个情景，我们生成一个大小为 $n_{\\text{train}}$ 的训练集 $(X_{\\text{train}}, y_{\\text{train}})$ 和一个大小为 $n_{\\text{test}} = 20000$ 的测试集 $(X_{\\text{test}}, y_{\\text{test}})$。预测变量 $x_1, x_2$ 从 $\\text{Uniform}(0,1)$ 中抽取。计算真实函数值 $f(x_1, x_2)$，并通过添加高斯噪声 $\\varepsilon \\sim \\mathcal{N}(0,\\sigma^2)$ 生成观测响应 $y$。为了可复现性，随机数生成器按规定进行播种。\n\n**模型 1：带交互项的全局多项式**\n该模型假设预测变量和响应之间的关系可以通过整个特征空间上的单个多项式函数来近似：\n$$\n\\hat{f}(x_1, x_2) = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_1 + \\hat{\\beta}_2 x_2 + \\hat{\\beta}_3 x_1^2 + \\hat{\\beta}_4 x_2^2 + \\hat{\\beta}_5 x_1 x_2\n$$\n系数 $\\hat{\\boldsymbol{\\beta}} = [\\hat{\\beta}_0, \\dots, \\hat{\\beta}_5]^T$ 使用 OLS 进行估计。给定训练数据 $\\{(\\mathbf{x}_i, y_i)\\}_{i=1}^{n_{\\text{train}}}$，我们构建设计矩阵 $\\mathbf{X}$ 和响应向量 $\\mathbf{y}$：\n$$\n\\mathbf{X} = \\begin{pmatrix}\n1  x_{1,1}  x_{2,1}  x_{1,1}^2  x_{2,1}^2  x_{1,1}x_{2,1} \\\\\n1  x_{1,2}  x_{2,2}  x_{1,2}^2  x_{2,2}^2  x_{1,2}x_{2,2} \\\\\n\\vdots  \\vdots  \\vdots  \\vdots  \\vdots  \\vdots \\\\\n1  x_{1,n_{\\text{train}}}  x_{2,n_{\\text{train}}}  x_{1,n_{\\text{train}}}^2  x_{2,n_{\\text{train}}}^2  x_{1,n_{\\text{train}}}x_{2,n_{\\text{train}}}\n\\end{pmatrix}, \\quad \\mathbf{y} = \\begin{pmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_{n_{\\text{train}}} \\end{pmatrix}\n$$\nOLS 解法找到最小化残差平方和 $\\|\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta}\\|^2$ 的 $\\hat{\\boldsymbol{\\beta}}$。该解由 $\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{y}$ 给出。为了数值稳定性，最好使用诸如 QR 分解或 SVD 之类的方法来求解这个线性系统，这由 `scipy.linalg.lstsq` 等库函数处理。一旦找到 $\\hat{\\boldsymbol{\\beta}}$，就可以通过 $\\hat{\\mathbf{y}}_{\\text{test}} = \\mathbf{X}_{\\text{test}}\\hat{\\boldsymbol{\\beta}}$ 对测试集中的新数据点进行预测。\n\n**模型 2：局部回归树**\n该模型将特征空间 $[0,1] \\times [0,1]$ 划分为不相交的矩形区域，并在每个区域中拟合一个简单的常数模型（响应的均值）。划分是贪心地、递归地完成的。\n\n- **节点表示：** 树由节点组成。内部节点指定一个分裂（一个特征索引 $j \\in \\{1,2\\}$ 和一个阈值 $t$），并有两个子节点（左和右）。叶节点没有子节点，并存储一个预测值（落入其区域的训练点的 $y$ 的样本均值）。\n\n- **分裂内部节点：** 在包含训练数据子集的给定节点处，我们搜索最佳分裂。分裂由一个特征 $j$ 和一个值 $t$ 定义。它将数据划分为一个左集合 $\\{ \\mathbf{x}_i \\mid x_{i,j} \\le t \\}$ 和一个右集合 $\\{ \\mathbf{x}_i \\mid x_{i,j}  t \\}$。分裂的质量通过总平方误差和的减少量来衡量：\n$$\n\\text{Gain}(j, t) = \\text{SSE}_{\\text{parent}} - (\\text{SSE}_{\\text{left}} + \\text{SSE}_{\\text{right}})\n$$\n其中 $\\text{SSE}_{\\text{region}} = \\sum_{i \\in \\text{region}} (y_i - \\bar{y}_{\\text{region}})^2$。我们搜索所有特征 $j \\in \\{1,2\\}$ 和所有有效的分割点 $t$，以找到使此增益最大化的组合。一个分裂只有在遵守最小叶节点大小约束时才有效：左子节点和右子节点都必须包含至少最小数量的样本。一个特征的潜在分割点 $t$ 可以有效地选择为该特征连续唯一排序值之间的中点。\n\n- **递归树构建：** 算法过程如下：\n  1. 从包含所有训练数据的根节点开始。\n  2. 对于一个节点，检查停止条件：\n     a. 当前深度等于允许的最大深度。\n     b. 节点中的样本数小于最小叶节点大小的两倍（因为任何分裂都会违反约束）。\n     c. 节点中所有的响应值 $y_i$ 都相同（SSE 为 0，无法进一步改进）。\n  3. 如果满足停止条件，则创建一个叶节点，并将其响应的均值存储为其预测值。\n  4. 否则，通过最大化 SSE 增益找到最佳分裂 $(j^*, t^*)$。\n  5. 如果没有分裂提供正增益，或不存在有效分裂，则创建一个叶节点。\n  6. 否则，使用分裂 $(j^*, t^*)$ 创建一个内部节点。划分数据并递归地构建左右子树。\n\n- **预测：** 为了对新点 $\\mathbf{x}_{\\text{new}}$ 进行预测，我们从根节点开始遍历树。在每个内部节点，我们将 $\\mathbf{x}_{\\text{new}}$ 的相关特征与节点的阈值进行比较，以决定向左还是向右走，直到到达一个叶节点。预测值是存储在该叶节点中的值。\n\n**评估**\n对于每个情景，在训练数据上训练两个模型后，我们为大型测试集中的特征生成预测 $\\hat{f}_{\\text{poly}}$ 和 $\\hat{f}_{\\text{tree}}$。性能通过相对于真实函数值 $f_{\\text{test}}$（无噪声）的 MSE 来衡量：\n$$\n\\text{MSE}_{\\text{poly}} = \\frac{1}{n_{\\text{test}}} \\sum_{i=1}^{n_{\\text{test}}} (\\hat{f}_{\\text{poly}}(x_{1,i}, x_{2,i}) - f(x_{1,i}, x_{2,i}))^2\n$$\n$$\n\\text{MSE}_{\\text{tree}} = \\frac{1}{n_{\\text{test}}} \\sum_{i=1}^{n_{\\text{test}}} (\\hat{f}_{\\text{tree}}(x_{1,i}, x_{2,i}) - f(x_{1,i}, x_{2,i}))^2\n$$\n为四个情景中的每一个计算这两个值并报告。\n\n这种系统性的方法确保了问题的所有要求都得到满足，提供了一个在全局参数模型和局部非参数模型之间的严格比较。实现将精确遵循这些原则。", "answer": "```python\nimport numpy as np\nfrom scipy import linalg\n\ndef solve():\n    \"\"\"\n    Main solver function that orchestrates the data generation, model training,\n    evaluation, and printing of results for all specified scenarios.\n    \"\"\"\n\n    # --- Helper Classes and Functions ---\n\n    class Node:\n        \"\"\"Represents a node in the regression tree.\"\"\"\n        def __init__(self, feature_index=None, threshold=None, left=None, right=None, value=None):\n            self.feature_index = feature_index  # Feature to split on\n            self.threshold = threshold          # Threshold for the split\n            self.left = left                    # Left subtree (for values = threshold)\n            self.right = right                  # Right subtree (for values > threshold)\n            self.value = value                  # Prediction value if it's a leaf node\n\n    class DecisionTree:\n        \"\"\"\n        A regression tree that uses greedy sum-of-squares splitting.\n        \"\"\"\n        def __init__(self, max_depth=2, min_samples_leaf=1):\n            self.max_depth = max_depth\n            self.min_samples_leaf = min_samples_leaf\n            self.root = None\n\n        def fit(self, X, y):\n            \"\"\"Build the regression tree from training data.\"\"\"\n            self.root = self._grow_tree(X, y)\n\n        def _grow_tree(self, X, y, depth=0):\n            \"\"\"Recursively grow the tree.\"\"\"\n            n_samples, n_features = X.shape\n            \n            # Check stopping criteria\n            is_pure = len(np.unique(y)) == 1\n            if (depth >= self.max_depth or\n                    n_samples  2 * self.min_samples_leaf or\n                    is_pure):\n                leaf_value = np.mean(y)\n                return Node(value=leaf_value)\n\n            best_split = self._find_best_split(X, y, n_samples, n_features)\n\n            if best_split['gain'] = 0:\n                leaf_value = np.mean(y)\n                return Node(value=leaf_value)\n            \n            left_indices = X[:, best_split['feature_index']] = best_split['threshold']\n            right_indices = ~left_indices\n            \n            left_subtree = self._grow_tree(X[left_indices, :], y[left_indices], depth + 1)\n            right_subtree = self._grow_tree(X[right_indices, :], y[right_indices], depth + 1)\n            \n            return Node(best_split['feature_index'], best_split['threshold'], left_subtree, right_subtree)\n\n        def _find_best_split(self, X, y, n_samples, n_features):\n            \"\"\"Find the best feature and threshold to split on.\"\"\"\n            best_split = {'gain': -1}\n            current_sse = self._calculate_sse(y)\n\n            for feat_idx in range(n_features):\n                thresholds = np.unique(X[:, feat_idx])\n                \n                # Use midpoints between unique sorted values as potential splits\n                if len(thresholds) > 1:\n                    test_thresholds = (thresholds[:-1] + thresholds[1:]) / 2\n                else: \n                    continue\n\n                for threshold in test_thresholds:\n                    left_indices = X[:, feat_idx] = threshold\n                    right_indices = ~left_indices\n                    \n                    if np.sum(left_indices)  self.min_samples_leaf or np.sum(right_indices)  self.min_samples_leaf:\n                        continue\n                        \n                    y_left, y_right = y[left_indices], y[right_indices]\n                    \n                    sse_left = self._calculate_sse(y_left)\n                    sse_right = self._calculate_sse(y_right)\n                    \n                    total_child_sse = sse_left + sse_right\n                    gain = current_sse - total_child_sse\n\n                    if gain > best_split['gain']:\n                        best_split = {\n                            'feature_index': feat_idx,\n                            'threshold': threshold,\n                            'gain': gain\n                        }\n            return best_split\n\n        @staticmethod\n        def _calculate_sse(y):\n            \"\"\"Calculate Sum of Squared Errors.\"\"\"\n            if len(y) == 0:\n                return 0\n            mean = np.mean(y)\n            return np.sum((y - mean) ** 2)\n\n        def predict(self, X):\n            \"\"\"Make predictions for a set of samples.\"\"\"\n            return np.array([self._predict_one(x, self.root) for x in X])\n\n        def _predict_one(self, x, node):\n            \"\"\"Traverse the tree to predict for a single sample.\"\"\"\n            if node.value is not None:\n                return node.value\n            if x[node.feature_index] = node.threshold:\n                return self._predict_one(x, node.left)\n            else:\n                return self._predict_one(x, node.right)\n\n    def true_f(x1, x2, c, tau1, tau2):\n        \"\"\"Computes the latent noise-free function value.\"\"\"\n        a1, a2, b1, b2 = 1.0, -1.0, 0.5, -0.5\n        interaction_term = c * x1 * x2 * ((x1 > tau1)  (x2 > tau2))\n        return a1 * x1 + a2 * x2 + b1 * x1**2 + b2 * x2**2 + interaction_term\n\n    def generate_data(n, seed, c, tau1, tau2, sigma):\n        \"\"\"Generates training or test data.\"\"\"\n        rng = np.random.default_rng(seed)\n        X = rng.uniform(0, 1, size=(n, 2))\n        x1, x2 = X[:, 0], X[:, 1]\n        \n        f_vals = true_f(x1, x2, c, tau1, tau2)\n        noise = rng.normal(0, sigma, size=n)\n        y = f_vals + noise\n        \n        return X, y, f_vals\n\n    def fit_predict_polynomial(X_train, y_train, X_test):\n        \"\"\"Fits OLS polynomial model and predicts on test data.\"\"\"\n        # Construct design matrix for training\n        X_design_train = np.c_[\n            np.ones(X_train.shape[0]),\n            X_train[:, 0],\n            X_train[:, 1],\n            X_train[:, 0]**2,\n            X_train[:, 1]**2,\n            X_train[:, 0] * X_train[:, 1]\n        ]\n        \n        # Solve for coefficients using least squares\n        beta, _, _, _ = linalg.lstsq(X_design_train, y_train, rcond=None)\n        \n        # Construct design matrix for testing\n        X_design_test = np.c_[\n            np.ones(X_test.shape[0]),\n            X_test[:, 0],\n            X_test[:, 1],\n            X_test[:, 0]**2,\n            X_test[:, 1]**2,\n            X_test[:, 0] * X_test[:, 1]\n        ]\n        \n        # Make predictions\n        y_pred = X_design_test @ beta\n        return y_pred\n    \n    # --- Scenarios and Main Execution Logic ---\n\n    test_cases = [\n        {'n_train': 400, 'c': 3.0, 'tau1': 0.5, 'tau2': 0.5, 'sigma': 0.1, 'depth': 2, 'min_leaf': 20, 'train_seed': 7, 'test_seed': 97},\n        {'n_train': 400, 'c': 0.5, 'tau1': 0.6, 'tau2': 0.6, 'sigma': 0.1, 'depth': 2, 'min_leaf': 20, 'train_seed': 8, 'test_seed': 98},\n        {'n_train': 400, 'c': 3.0, 'tau1': 0.5, 'tau2': 0.5, 'sigma': 1.0, 'depth': 2, 'min_leaf': 40, 'train_seed': 9, 'test_seed': 99},\n        {'n_train': 800, 'c': 5.0, 'tau1': 0.85, 'tau2': 0.85, 'sigma': 0.1, 'depth': 3, 'min_leaf': 10, 'train_seed': 10, 'test_seed': 100},\n    ]\n\n    all_results = []\n    n_test = 20000\n\n    for case in test_cases:\n        # Generate data\n        X_train, y_train, _ = generate_data(case['n_train'], case['train_seed'], case['c'], case['tau1'], case['tau2'], case['sigma'])\n        X_test, _, f_test = generate_data(n_test, case['test_seed'], case['c'], case['tau1'], case['tau2'], case['sigma'])\n\n        # Model 1: Global Polynomial\n        poly_preds = fit_predict_polynomial(X_train, y_train, X_test)\n        mse_poly = np.mean((poly_preds - f_test)**2)\n        \n        # Model 2: Local Regression Tree\n        tree = DecisionTree(max_depth=case['depth'], min_samples_leaf=case['min_leaf'])\n        tree.fit(X_train, y_train)\n        tree_preds = tree.predict(X_test)\n        mse_tree = np.mean((tree_preds - f_test)**2)\n\n        all_results.extend([mse_poly, mse_tree])\n    \n    # Format and print final results\n    print(f\"[{','.join([f'{r:.6f}' for r in all_results])}]\")\n\nsolve()\n\n```", "id": "3132277"}]}