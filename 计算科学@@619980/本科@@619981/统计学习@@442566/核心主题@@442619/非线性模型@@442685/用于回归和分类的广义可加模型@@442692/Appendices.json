{"hands_on_practices": [{"introduction": "广义可加模型（GAMs）以其灵活性著称，但其在训练数据范围之外（即外推）的行为可能难以预测，尤其是当使用基于多项式的样条时。此练习将通过一个分类任务，直观地展示在没有适当约束的情况下，模型预测的概率在远离数据区域时会如何趋于极端（0 或 1）。我们将探索如何通过对模型系数施加惩罚（这在贝叶斯视角下等同于设置先验）来约束模型的尾部行为，从而获得更稳定和合理的预测结果 [@problem_id:3123638]。", "problem": "要求您研究单个预测变量的逻辑斯谛广义相加模型（GAM）中的外推行为。该模型由伯努利似然和标准 logit 链接定义：对于标记数据 $\\{(x_i, y_i)\\}_{i=1}^n$（其中 $y_i \\in \\{0,1\\}$），条件均值 $\\mu_i = \\mathbb{E}[y_i \\mid x_i]$ 被建模为 $g(\\mu_i) = \\eta_i$，其中 $g(\\cdot)$ 是 logit 函数 $g(p) = \\log\\left(\\frac{p}{1-p}\\right)$，而 $\\eta_i = \\beta_0 + f(x_i)$ 是包含一个未知平滑函数 $f(\\cdot)$ 的相加预测变量。函数 $f(\\cdot)$ 将使用三次回归样条基来表示。目标是量化对于远在训练域之外的预测值 $x$，拟合的分类概率 $\\hat{p}(x)$ 的行为，并使用先验或惩罚项来控制这种行为，以避免出现接近 $0$ 或 $1$ 的极端概率。\n\n从以下基本原理开始：\n- 单个观测值的伯努利对数似然为 $\\ell(\\mu; y) = y \\log(\\mu) + (1-y) \\log(1-\\mu)$，其中 $\\mu = \\mathbb{E}[y]$。\n- 逻辑斯谛回归的标准链接是 logit，$g(\\mu) = \\log\\left(\\frac{\\mu}{1-\\mu}\\right)$，因此线性预测变量 $\\eta$ 满足 $\\mu = \\frac{1}{1 + e^{-\\eta}}$。\n- 用于广义线性模型的迭代重加权最小二乘法（IRLS）使用从线性预测变量中对数似然的二阶泰勒近似导出的加权最小二乘更新。\n\n使用带有内部节点的三次回归样条截断幂基来表示平滑函数 $f(x)$。具体来说，令基函数为\n- $b_0(x) = 1$，\n- $b_1(x) = x$，\n- $b_2(x) = x^2$，\n- $b_3(x) = x^3$，\n- 并且，对于内部节点 $c_j$，有 $b_{3+j}(x) = \\max(x - c_j, 0)^3$。\n那么 $f(x) = \\sum_{j} \\beta_j b_j(x)$，其中 $\\beta_j$ 是通过最大化惩罚对数似然来估计的系数。\n\n为避免在远离训练数据的 $x$ 处出现极端的 $\\hat{p}(x)$，对系数施加高斯先验，这等同于在 IRLS 正规方程中施加岭惩罚。该惩罚是一个对角矩阵，其非截距项系数对应的项为非负值。对高阶多项式项 $x^2$、$x^3$ 以及截断三次项施加更强的惩罚，会强制产生更保守的尾部，并减少对数几率在训练区域外大小无界增长的趋势。\n\n您的程序必须：\n- 生成大小为 $n = 200$ 的合成训练数据，其中预测值 $x_i$ 从区间 $[-2,2]$ 中均匀抽取，使用固定的随机种子 $s = 42$ 以确保可复现性。真实数据生成模型为 $y_i \\sim \\text{Bernoulli}(p_i)$，其中\n$$\np_i = \\frac{1}{1 + \\exp\\left(-\\left(\\alpha + f^*(x_i)\\right)\\right)}, \\quad \\alpha = 0.2,\\quad f^*(x) = 2 \\sin(x).\n$$\n- 构建内部节点位于 $c_1 = -1.5$、$c_2 = -0.5$、$c_3 = 0.5$ 和 $c_4 = 1.5$ 的三次回归样条基。\n- 使用 IRLS 拟合一个惩罚逻辑斯谛 GAM。在每个 IRLS 步骤中，求解带对角岭惩罚的加权正规方程，其项编码了对 $\\beta$ 的一个零均值和选定方差的高斯先验。截距项 $\\beta_0$ 不得被惩罚。\n- 对于给定的惩罚配置，计算四个外推点 $x = -10$、$x = -6$、$x = 6$ 和 $x = 10$ 处的拟合概率 $\\hat{p}(x)$，并以浮点数形式返回它们。\n\n测试套件：\n运行以下四个测试用例，每个用例对应一个不同的对角惩罚配置（列表项按顺序对应于系数 $[b_0,b_1,b_2,b_3,b_{4},b_{5},b_{6},b_{7}]$）：\n- 用例 A（除数值稳定外无惩罚）：$[0, 0, 0, 0, 0, 0, 0, 0]$。\n- 用例 B（对所有非截距项施加统一的温和惩罚）：$[0, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5]$。\n- 用例 C（对高阶项施加强惩罚，对线性项施加较轻惩罚）：$[0, 0.1, 100, 100, 10, 10, 10, 10]$。\n- 用例 D（对所有非截距项施加非常强的惩罚）：$[0, 1000, 1000, 1000, 1000, 1000, 1000, 1000]$。\n\n对于每个用例，拟合模型并计算包含四个浮点数的列表 $[\\hat{p}(-10), \\hat{p}(-6), \\hat{p}(6), \\hat{p}(10)]$。\n\n最终输出格式：\n您的程序应生成单行输出，其中包含所有四个用例的结果，格式为逗号分隔的浮点数列表的列表，不含空格，并用方括号括起来。例如：$[[p_{A,-10},p_{A,-6},p_{A,6},p_{A,10}],[p_{B,-10},p_{B,-6},p_{B,6},p_{B,10}],[p_{C,-10},p_{C,-6},p_{C,6},p_{C,10}],[p_{D,-10},p_{D,-6},p_{D,6},p_{D,10}]]$. 单位是 $[0,1]$ 范围内的纯数，解释为概率，并以小数形式表示。", "solution": "该问题要求实现并分析用于二元分类的惩罚逻辑斯谛广义相加模型（GAM）。主要目标是通过将模型拟合到合成数据，并在不同正则化方案下评估训练数据范围之外远点处的预测概率，来研究模型的外推行为。\n\n### 模型设定\n对于给定的单个预测变量 $x_i$，二元响应 $y_i \\in \\{0, 1\\}$ 的模型基于伯努利分布。条件均值 $\\mu_i = \\mathbb{E}[y_i | x_i]$（即概率 $p(y_i=1|x_i)$）通过 logit 链接函数与线性预测变量 $\\eta_i$ 相关联：\n$$\ng(\\mu_i) = \\log\\left(\\frac{\\mu_i}{1-\\mu_i}\\right) = \\eta_i\n$$\n对链接函数求逆，可得到作为线性预测变量函数的概率：\n$$\n\\mu_i = \\frac{1}{1 + e^{-\\eta_i}}\n$$\n线性预测变量 $\\eta_i$ 被建模为截距项 $\\beta_0$ 和平滑函数 $f(x_i)$ 的相加组合：\n$$\n\\eta_i = \\beta_0 + f(x_i)\n$$\n\n### 样条基表示\n平滑函数 $f(x)$ 使用带有截断幂基的三次回归样条表示。这使我们能够将非线性函数 $f(x)$ 表示为基函数的线性组合。完整的线性预测变量 $\\eta(x)$ 可以写成一个线性模型 $\\eta(x) = \\mathbf{B}(x)^T \\boldsymbol{\\beta}$，其中 $\\mathbf{B}(x)$ 是在 $x$ 处求值的基函数向量，$\\boldsymbol{\\beta}$ 是待估计的系数向量。\n\n对于此问题，预测值 $x$ 的基向量定义为：\n$$\n\\mathbf{B}(x) = [1, x, x^2, x^3, (x - c_1)_+^3, (x - c_2)_+^3, (x - c_3)_+^3, (x - c_4)_+^3]^T\n$$\n其中 $(u)_+ = \\max(u, 0)$ 是正部函数。该基包含用于截距项 $\\beta_0$ 的常数项、最高三次的多项式项（$x$, $x^2$, $x^3$），以及对应于 4 个内部节点 $c_j$ 中每一个的截断三次项。指定的节点为 $c_1=-1.5$、$c_2=-0.5$、$c_3=0.5$ 和 $c_4=1.5$。这产生一个 8 维系数向量 $\\boldsymbol{\\beta} = [\\beta_0, \\beta_1, \\dots, \\beta_7]^T$。对于一组 $n$ 个观测值 $\\{x_i\\}_{i=1}^n$，我们构建 $n \\times 8$ 的设计矩阵 $X$，其中第 $i$ 行是 $\\mathbf{B}(x_i)^T$。所有观测值的线性预测变量向量则为 $\\boldsymbol{\\eta} = X\\boldsymbol{\\beta}$。\n\n### 通过迭代重加权最小二乘法 (IRLS) 进行惩罚估计\n系数 $\\boldsymbol{\\beta}$ 是通过最大化惩罚对数似然来估计的。$n$ 个观测值的伯努利对数似然为：\n$$\n\\ell(\\boldsymbol{\\beta}) = \\sum_{i=1}^n \\left( y_i \\eta_i - \\log(1 + e^{\\eta_i}) \\right)\n$$\n为了控制模型复杂度和防止过拟合，特别是高阶多项式在训练范围外的不稳定行为，我们增加一个惩罚项。这个惩罚项等同于对系数施加一个零均值高斯先验。截距项 $\\beta_0$ 不受惩罚。惩罚对数似然为：\n$$\n\\ell_p(\\boldsymbol{\\beta}) = \\ell(\\boldsymbol{\\beta}) - \\frac{1}{2}\\sum_{j=1}^{7} \\lambda_j \\beta_j^2 = \\ell(\\boldsymbol{\\beta}) - \\frac{1}{2}\\boldsymbol{\\beta}^T S \\boldsymbol{\\beta}\n$$\n其中 $S$ 是一个对角惩罚矩阵，其对角线上的元素为 $[\\lambda_0, \\lambda_1, \\dots, \\lambda_7]$，并且我们设定 $\\lambda_0 = 0$。\n\n$\\ell_p(\\boldsymbol{\\beta})$ 的最大化是使用迭代重加权最小二乘（IRLS）算法完成的。IRLS 是 Newton 方法的一个应用。在每次迭代 $(t)$ 中，它通过求解一个惩罚加权最小二乘问题来更新系数向量。更新规则是：\n$$\n\\boldsymbol{\\beta}^{(t+1)} = (X^T W^{(t)} X + S)^{-1} X^T W^{(t)} \\mathbf{z}^{(t)}\n$$\n此处：\n- $\\boldsymbol{\\beta}^{(t)}$ 是上一次迭代的系数向量。\n- $W^{(t)}$ 是一个对角权重矩阵，其元素为 $W_{ii}^{(t)} = \\mu_i^{(t)}(1-\\mu_i^{(t)})$，其中 $\\mu_i^{(t)}$ 是使用 $\\boldsymbol{\\beta}^{(t)}$ 计算出的拟合概率。\n- $\\mathbf{z}^{(t)}$ 是工作响应向量，其元素为 $z_i^{(t)} = \\eta_i^{(t)} + \\frac{y_i - \\mu_i^{(t)}}{\\mu_i^{(t)}(1-\\mu_i^{(t)})}$。\n\n算法以 $\\boldsymbol{\\beta}^{(0)} = \\mathbf{0}$ 初始化，并运行固定的迭代次数。\n\n### 实现与分析\n该过程首先生成大小为 $n=200$ 的合成训练数据。预测值 $x_i$ 从 $[-2, 2]$ 上的均匀分布中抽取。二元结果 $y_i$ 从伯努利分布中抽样，其真实概率为 $p_i = (1 + \\exp(-(\\alpha + f^*(x_i))))^{-1}$，其中 $\\alpha=0.2$ 且 $f^*(x) = 2\\sin(x)$。使用固定的随机种子 $s=42$ 来确保可复现性。\n\n对于四个指定的惩罚配置（用例 A-D）中的每一个，执行 IRLS 算法以找到估计的系数向量 $\\hat{\\boldsymbol{\\beta}}$。这些配置代表：\n- 用例 A ($[0, 0, 0, 0, 0, 0, 0, 0]$)：无正则化。\n- 用例 B ($[0, 0.5, \\dots, 0.5]$)：对所有非截距项施加温和、统一的惩罚。\n- 用例 C ($[0, 0.1, 100, 100, 10, \\dots, 10]$)：对二次、三次和样条项施加强惩罚，对线性项施加较弱的惩罚。\n- 用例 D ($[0, 1000, \\dots, 1000]$)：对所有非截距项施加非常强的惩罚，迫使模型趋向于一个常数对数几率（仅含截距项）。\n\n拟合后，使用这些模型来预测外推点 $x \\in \\{-10, -6, 6, 10\\}$ 处的概率 $\\hat{p}(x)$。这通过首先为这些点构建设计矩阵 $X_{extrap}$，然后计算对数几率 $\\boldsymbol{\\eta}_{extrap} = X_{extrap} \\hat{\\boldsymbol{\\beta}}$，最后应用逆 logit 函数 $\\hat{\\mathbf{p}}_{extrap} = (1 + \\exp(-\\boldsymbol{\\eta}_{extrap}))^{-1}$ 来完成。比较四个用例的结果表明，增加对高阶项的惩罚如何抑制外推行为，使预测概率远离 $0$ 和 $1$ 的极端值。", "answer": "```python\nimport numpy as np\n\ndef build_design_matrix(x, knots):\n    \"\"\"\n    Constructs the design matrix for a cubic regression spline with a truncated power basis.\n\n    Args:\n        x (np.ndarray): 1D array of predictor values.\n        knots (np.ndarray): 1D array of interior knot locations.\n\n    Returns:\n        np.ndarray: The design matrix X.\n    \"\"\"\n    n_obs = len(x)\n    n_knots = len(knots)\n    # Total columns: 1 (intercept) + 3 (polynomial terms) + n_knots\n    X = np.zeros((n_obs, 4 + n_knots))\n    \n    X[:, 0] = 1.0\n    X[:, 1] = x\n    X[:, 2] = x**2\n    X[:, 3] = x**3\n    \n    for i, knot in enumerate(knots):\n        X[:, 4 + i] = np.maximum(x - knot, 0)**3\n        \n    return X\n\n\ndef fit_penalized_gam(X, y, penalty_diag, n_iter=25):\n    \"\"\"\n    Fits a penalized logistic GAM using Iteratively Reweighted Least Squares (IRLS).\n\n    Args:\n        X (np.ndarray): The design matrix.\n        y (np.ndarray): The binary response vector.\n        penalty_diag (np.ndarray): The diagonal entries of the penalty matrix.\n        n_iter (int): The number of IRLS iterations.\n\n    Returns:\n        np.ndarray: The estimated coefficient vector beta.\n    \"\"\"\n    n_features = X.shape[1]\n    beta = np.zeros(n_features)\n    S = np.diag(penalty_diag)\n    \n    # Epsilon to prevent mu=0 or mu=1, which cause infinite weights/responses\n    epsilon = 1e-8\n\n    for _ in range(n_iter):\n        eta = X @ beta\n        \n        # Clip eta to avoid numerical overflow in exp()\n        eta = np.clip(eta, -25, 25)\n\n        mu = 1.0 / (1.0 + np.exp(-eta))\n        \n        # Clip mu to prevent weights from becoming exactly zero\n        mu = np.clip(mu, epsilon, 1.0 - epsilon)\n        \n        weights = mu * (1.0 - mu)\n        z = eta + (y - mu) / weights\n        \n        # Efficiently compute the left-hand side and right-hand side of the normal equations\n        # This avoids forming the full n x n weight matrix W\n        X_w = X * weights[:, np.newaxis]\n        lhs = X.T @ X_w + S\n        rhs = X.T @ (weights * z)\n        \n        try:\n            beta = np.linalg.solve(lhs, rhs)\n        except np.linalg.LinAlgError:\n            # Add a small ridge for numerical stability if lhs is singular.\n            # This is relevant for the unpenalized case (Case A).\n            beta = np.linalg.solve(lhs + np.eye(n_features) * 1e-8, rhs)\n\n    return beta\n\n\ndef solve():\n    \"\"\"\n    Main function to run the simulation, fit models for each test case,\n    and generate the final output in the required format.\n    \"\"\"\n    # Define the four test cases for the penalty vector\n    test_cases = [\n        [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n        [0.0, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5],\n        [0.0, 0.1, 100.0, 100.0, 10.0, 10.0, 10.0, 10.0],\n        [0.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0]\n    ]\n\n    # --- Problem constants and data generation ---\n    n_samples = 200\n    seed = 42\n    alpha = 0.2\n    knots = np.array([-1.5, -0.5, 0.5, 1.5])\n    extrapolation_points = np.array([-10.0, -6.0, 6.0, 10.0])\n\n    # Generate synthetic training data for reproducibility\n    rng = np.random.default_rng(seed)\n    x_train = rng.uniform(-2.0, 2.0, n_samples)\n    true_log_odds = alpha + 2.0 * np.sin(x_train)\n    p_true = 1.0 / (1.0 + np.exp(-true_log_odds))\n    y_train = rng.binomial(1, p_true, n_samples)\n    \n    # --- Construct design matrices ---\n    X_train = build_design_matrix(x_train, knots)\n    X_extrap = build_design_matrix(extrapolation_points, knots)\n\n    all_results = []\n    \n    # --- Loop through test cases, fit model, and predict ---\n    for penalty_values in test_cases:\n        penalty_diag = np.array(penalty_values)\n        \n        # Fit the penalized GAM model using IRLS\n        beta_hat = fit_penalized_gam(X_train, y_train, penalty_diag)\n        \n        # Predict probabilities at extrapolation points\n        eta_extrap = X_extrap @ beta_hat\n        p_extrap = 1.0 / (1.0 + np.exp(-eta_extrap))\n        \n        all_results.append(list(p_extrap))\n        \n    # --- Format final output string ---\n    # The required format is [[...],[...],...] with no spaces and decimal float representation.\n    formatted_lists = []\n    for result_list in all_results:\n        # Format each inner list to ensure decimal float representation without scientific notation\n        inner_str = f\"[{','.join([f'{val:.15f}' for val in result_list])}]\"\n        formatted_lists.append(inner_str)\n    \n    final_output = f\"[{','.join(formatted_lists)}]\"\n    print(final_output)\n\nsolve()\n```", "id": "3123638"}, {"introduction": "在广义可加模型中，联结函数（link function）是连接线性预测器和响应均值的关键桥梁，例如在二分类问题中常用的 logit 联结。选择错误的联结函数会导致系统性偏差和错误的推断。本实践将通过一个模拟研究，向您展示如何利用残差分析来诊断联结函数的误设问题，即当数据由一个真实的联结函数（互补 log-log）生成，而我们却用另一个（logit）来拟合模型时，残差中会呈现出特定的模式 [@problem_id:3123718]。", "problem": "考虑用于回归和分类的广义可加模型，其中广义可加模型 (GAM) 将响应的条件均值表示为由连结函数转换的协变量的可加函数。对于二元结果，广义线性模型 (GLM) 框架内的 GAM 指定了一个连结函数 $g$，它将条件均值 $E[Y \\mid \\mathbf{x}] = \\mu(\\mathbf{x})$ 连接到一个可加预测变量 $\\eta(\\mathbf{x}) = \\beta_0 + \\sum_{j=1}^p f_j(x_j)$，其中每个 $f_j$ 是一个通过固定基展开表示的光滑函数，并带有一个惩罚项以控制光滑度。估计的基本基础是伯努利分布的最大似然法与 Fisher-scoring 算法（迭代重加权最小二乘法）相结合，从对数似然 $L(\\mu \\mid y) = \\sum_{i=1}^n \\left\\{ y_i \\log \\mu_i + (1 - y_i) \\log (1 - \\mu_i) \\right\\}$ 和 GLM 结构 $g(\\mu_i) = \\eta_i$（方差函数为 $V(\\mu_i) = \\mu_i (1 - \\mu_i)$）开始。\n\n您的任务是通过从互补 log-log 连结中模拟数据并拟合一个 logit GAM 来诊断连结误设；然后评估揭示误设的系统性残差模式。\n\n执行以下步骤：\n\n1. 数据生成：\n   - 令 $n$ 表示样本量，定义两个协变量 $x_1$ 和 $x_2$，它们独立地从 $[0,1]$ 上的均匀分布中抽取。\n   - 定义光滑函数 $f_1(x_1) = 1.2 \\sin(2 \\pi x_1) + 0.3(x_1 - 0.5)$ 和 $f_2(x_2) = -0.8 \\cos(2 \\pi x_2) + 0.5(x_2 - 0.5)^2$。\n   - 令一个尺度参数 $s$ 控制信号强度，并设置可加预测变量 $\\eta_{\\text{true}} = s \\cdot \\left( f_1(x_1) + f_2(x_2) \\right)$。\n   - 使用互补 log-log 逆连结函数获得概率 $p_i = 1 - \\exp\\left( -\\exp\\left( \\eta_{\\text{true}, i} \\right) \\right)$，然后独立地从 $\\text{Bernoulli}(p_i)$ 中抽样 $y_i$。\n\n2. 模型拟合：\n   - 使用基函数 $\\{x, x^2, x^3, (x - \\kappa_m)_+^3\\}_{m=1}^4$（其中 $(u)_+ = \\max\\{u, 0\\}$），通过在 $[0,1]$ 上位于固定位置 $0.2、0.4、0.6$ 和 $0.8$ 的节点，用截断三次样条基表示每个 $f_j$。\n   - 构建一个设计矩阵，包含截距项以及 $x_1$ 和 $x_2$ 的基展开（无交互项）。\n   - 使用 Fisher-scoring 算法（迭代重加权最小二乘法）拟合一个惩罚 GLM，其中在每次迭代中，您需要求解一个从伯努利对数似然和所选连结函数 $g$ 推导出的惩罚加权最小二乘系统：\n     - 对于一个通用连结 $g$，定义 $\\eta_i = g(\\mu_i)$、导数 $d\\mu_i/d\\eta_i$、方差 $V(\\mu_i) = \\mu_i (1 - \\mu_i)$、权重 $w_i = \\left( d\\mu_i/d\\eta_i \\right)^2 / V(\\mu_i)$ 以及工作响应 $z_i = \\eta_i + (y_i - \\mu_i) / (d\\mu_i/d\\eta_i)$。\n     - 求解惩罚正规方程 $\\left( X^\\top W X + \\lambda P \\right) \\boldsymbol{\\beta}^{\\text{new}} = X^\\top W \\mathbf{z}$，其中 $W$ 是对角线元素为 $w_i$ 的对角矩阵，$\\lambda > 0$ 是岭惩罚参数，而 $P$ 是截距位置为零的单位矩阵。\n   - 用两个连结函数拟合模型：\n     - logit 逆连结 $\\mu_i = \\sigma(\\eta_i) = 1 / \\left( 1 + \\exp(-\\eta_i) \\right)$。\n     - 互补 log-log 逆连结 $\\mu_i = 1 - \\exp\\left( -\\exp(\\eta_i) \\right)$。\n\n3. 残差诊断：\n   - 使用模型的拟合均值 $\\hat{\\mu}_i$ 和适当的边界值数值保护措施，计算伯努利偏差残差 $r_i = \\operatorname{sign}(y_i - \\hat{\\mu}_i) \\sqrt{2 \\left( y_i \\log\\left( \\frac{y_i}{\\hat{\\mu}_i} \\right) + (1 - y_i) \\log\\left( \\frac{1 - y_i}{1 - \\hat{\\mu}_i} \\right) \\right)}$。\n   - 使用以下方法量化系统性模式：\n     - 残差与真实可加预测变量之间的绝对皮尔逊相关系数 $|\\operatorname{corr}(r, \\eta_{\\text{true}})|$。\n     - 将 $r$ 对 $x_1$ 的样条基（包括截距）进行回归得到的决定系数 $R^2$。\n     - 将 $r$ 对 $x_2$ 的样条基（包括截距）进行回归得到的决定系数 $R^2$。\n   - 报告每个测试案例的这三个浮点数诊断结果。\n\n使用以下参数设置的测试套件来执行该过程：\n- 案例 1：$n = 800$，$s = 1.0$，随机种子 $42$，使用 logit 连结拟合从互补 log-log 连结生成的误设数据。\n- 案例 2：$n = 800$，$s = 1.0$，随机种子 $42$，使用互补 log-log 连结拟合从互补 log-log 连结生成的正确设定数据。\n- 案例 3：$n = 800$，$s = 2.5$（信号更强；概率更频繁地接近 $0$ 或 $1$），随机种子 $123$，使用 logit 连结拟合从互补 log-log 连结生成的误设数据。\n- 案例 4：$n = 800$，$s = 0.2$（信号更弱；概率聚集在 $0.5$ 附近），随机种子 $7$，使用 logit 连结拟合从互补 log-log 连结生成的误设数据。\n\n您的程序应生成单行输出，其中包含结果，格式为一个由方括号括起来的逗号分隔列表，其中每个内部列表是每个案例的三元组 $[|\\operatorname{corr}(r, \\eta_{\\text{true}})|, R^2_{x_1}, R^2_{x_2}]$，按指定的四个案例顺序排列（例如，格式为 $[[a_1,b_1,c_1],[a_2,b_2,c_2],[a_3,b_3,c_3],[a_4,b_4,c_4]]$）。此问题不涉及物理单位；所有输出均以浮点数表示，不带百分号。", "solution": "用户希望实现一个统计模拟，以诊断用于二元分类的广义可加模型 (GAM) 中的连结函数误设。\n\n### 步骤 1：问题验证\n\n**已知信息提取：**\n\n*   **模型框架**：用于二元结果的广义可加模型 (GAM)，使用连结函数 $g$ 将条件均值 $\\mu(\\mathbf{x})$ 连接到可加预测变量 $\\eta(\\mathbf{x}) = \\beta_0 + \\sum_{j=1}^p f_j(x_j)$。\n*   **估计方法**：通过 Fisher-scoring 算法（迭代重加权最小二乘法，IRLS）对伯努利分布进行最大似然估计。\n*   **对数似然**：$L(\\mu \\mid y) = \\sum_{i=1}^n \\left\\{ y_i \\log \\mu_i + (1 - y_i) \\log (1 - \\mu_i) \\right\\}$。\n*   **方差函数**：$V(\\mu_i) = \\mu_i (1 - \\mu_i)$。\n*   **数据生成**：\n    *   样本量 $n$。\n    *   协变量 $x_1, x_2 \\sim U[0,1]$，独立。\n    *   平滑函数：$f_1(x_1) = 1.2 \\sin(2 \\pi x_1) + 0.3(x_1 - 0.5)$ 和 $f_2(x_2) = -0.8 \\cos(2 \\pi x_2) + 0.5(x_2 - 0.5)^2$。\n    *   真实可加预测变量：$\\eta_{\\text{true}} = s \\cdot \\left( f_1(x_1) + f_2(x_2) \\right)$，其中 $s$ 是尺度参数。\n    *   真实连结函数（用于数据生成）：互补 log-log，即 $p_i = 1 - \\exp\\left( -\\exp\\left( \\eta_{\\text{true}, i} \\right) \\right)$。\n    *   响应变量：$y_i \\sim \\text{Bernoulli}(p_i)$。\n*   **模型拟合**：\n    *   $f_j$ 的基：截断三次样条基，节点位于 $[0,1]$ 上的 $\\{0.2, 0.4, 0.6, 0.8\\}$，基函数为 $\\{x, x^2, x^3, (x - \\kappa_m)_+^3\\}_{m=1}^4$。\n    *   设计矩阵 $X$：截距项，$x_1$ 的基，$x_2$ 的基。\n    *   IRLS 算法：求解惩罚正规方程 $\\left( X^\\top W X + \\lambda P \\right) \\boldsymbol{\\beta}^{\\text{new}} = X^\\top W \\mathbf{z}$，其中 $W_{ii} = \\left( d\\mu_i/d\\eta_i \\right)^2 / V(\\mu_i)$ 且 $z_i = \\eta_i + (y_i - \\mu_i) / (d\\mu_i/d\\eta_i)$。\n    *   惩罚项：岭惩罚，参数为 $\\lambda > 0$，P 为单位矩阵，但截距项对应的位置为零。\n    *   用于拟合的连结函数：logit ($\\mu_i = 1 / \\left( 1 + \\exp(-\\eta_i) \\right)$) 和互补 log-log。\n*   **残差诊断**：\n    *   偏差残差：$r_i = \\operatorname{sign}(y_i - \\hat{\\mu}_i) \\sqrt{2 \\left( y_i \\log\\left( \\frac{y_i}{\\hat{\\mu}_i} \\right) + (1 - y_i) \\log\\left( \\frac{1 - y_i}{1 - \\hat{\\mu}_i} \\right) \\right)}$。\n    *   度量指标：$|\\operatorname{corr}(r, \\eta_{\\text{true}})|$，$r$ 对 $x_1$ 样条基回归的 $R^2$，$r$ 对 $x_2$ 样条基回归的 $R^2$。\n*   **测试案例**：\n    1.  $n = 800, s = 1.0$，种子 $42$，用 logit 拟合。\n    2.  $n = 800, s = 1.0$，种子 $42$，用 cloglog 拟合。\n    3.  $n = 800, s = 2.5$，种子 $123$，用 logit 拟合。\n    4.  $n = 800, s = 0.2$，种子 $7$，用 logit 拟合。\n*   **输出格式**：单行字符串 `[[a1,b1,c1],[a2,b2,c2],[a3,b3,c3],[a4,b4,c4]]`。\n\n**有效性评估：**\n\n*   **科学依据**：该问题在广义线性模型和广义可加模型的理论中有坚实的基础，这些都是现代统计学和机器学习中的基本主题。所有概念（IRLS、连结函数、基展开、偏差残差）都是标准的。\n*   **良构性**：问题具体，并提供了构建解决方案所需的所有组件。惩罚参数 $\\lambda$ 的值存在轻微的模糊性。问题陈述其为“一个岭惩罚参数 $\\lambda > 0$”，但未指定具体值。在模拟研究的背景下，这是一个可以接受的省略，因为实现者应选择一个合理的固定值，以确保不同测试案例之间的公平比较。我将选择一个小的标准值，$\\lambda=0.01$，以轻微正则化拟合过程并确保数值稳定性。\n*   **客观性**：问题以精确的数学语言陈述，没有主观性或观点。\n*   **结论**：问题有效。它是一个在统计建模中定义明确的计算任务，科学上合理且可形式化。\n\n### 解决方案\n\n解决方案按照问题中描述的三个主要阶段进行：数据生成、模型拟合和残差诊断。\n\n**1. 数据生成**\n首先，我们定义平滑函数 $f_1(x_1)$ 和 $f_2(x_2)$。对于每个测试案例，我们从均匀分布 $U[0,1]$ 中生成 $n$ 个协变量 $x_1$ 和 $x_2$ 的样本。然后，我们计算真实的加性预测变量 $\\eta_{\\text{true}, i} = s \\cdot (f_1(x_{1i}) + f_2(x_{2i}))$。使用互补 log-log (cloglog) 逆连结函数生成概率 $p_i$：$p_i = 1 - \\exp(-\\exp(\\eta_{\\text{true},i}))$。最后，二元响应 $y_i$ 从具有这些概率的伯努利分布中抽样，$y_i \\sim \\text{Bernoulli}(p_i)$。\n\n**2. 模型拟合**\n任务的核心是拟合一个 GAM 到生成的数据。这涉及两个关键部分：构建设计矩阵和实现带惩罚的 IRLS 算法。\n\n**2.1. 设计矩阵构建**\n每个平滑函数 $f_j$ 由基函数的线性组合来近似。问题指定了一个截断三次样条基。对于一个协变量 $x$，基函数是 $\\{x, x^2, x^3, (x - \\kappa_1)_+^3, \\dots, (x - \\kappa_4)_+^3\\}$，其中 $(u)_+ = \\max(u,0)$，节点 $\\kappa_m$ 给定为 $\\{0.2, 0.4, 0.6, 0.8\\}$。这导致每个协变量有 $3+4=7$ 个基函数。完整的设计矩阵 $X$ 是通过拼接一列 1（用于截距 $\\beta_0$）、$x_1$ 的 7 个基列和 $x_2$ 的 7 个基列来构建的。这使得 $X$ 共有 $1+7+7=15$ 列。\n\n**2.2. 带惩罚的迭代重加权最小二乘法 (IRLS)**\nIRLS 算法是一个迭代过程，用于找到系数 $\\boldsymbol{\\beta}$ 的最大似然估计。从 $\\boldsymbol{\\beta}$ 的一个初始猜测开始，每次迭代包括以下步骤：\n1.  **计算预测变量和均值**：计算线性预测变量 $\\eta_i = \\mathbf{x}_i^\\top \\boldsymbol{\\beta}$ 和条件均值 $\\mu_i = g^{-1}(\\eta_i)$，其中 $g^{-1}$ 是逆连结函数（logit 或 cloglog）。\n2.  **计算工作响应和权重**：更新是通过解决一个加权最小二乘问题来找到的。权重 $w_i$ 和工作响应 $z_i$ 取决于所选的连结函数：\n    *   $d\\mu_i/d\\eta_i$：逆连结函数的导数。\n    *   $V(\\mu_i) = \\mu_i(1-\\mu_i)$：伯努利变量的方差。\n    *   $w_i = (d\\mu_i/d\\eta_i)^2 / V(\\mu_i)$。\n    *   $z_i = \\eta_i + (y_i - \\mu_i) / (d\\mu_i/d\\eta_i)$。\n3.  **求解带惩罚的系统**：通过求解惩罚正规方程找到新的系数向量 $\\boldsymbol{\\beta}^{\\text{new}}$：\n    $$ \\left( X^\\top W X + \\lambda P \\right) \\boldsymbol{\\beta}^{\\text{new}} = X^\\top W \\mathbf{z} $$\n    这里，$W$ 是权重 $w_i$ 的对角矩阵，$\\lambda$ 是惩罚参数（选择为 $\\lambda=0.01$），$P$ 是一个 $15 \\times 15$ 的单位矩阵，其中 $P_{00}=0$ 以避免惩罚截距项。\n这个过程重复进行，直到系数向量 $\\boldsymbol{\\beta}$ 收敛。\n\n对于指定的两个连结函数：\n*   **Logit**：$\\mu = \\sigma(\\eta) = (1+e^{-\\eta})^{-1}$。这里，$d\\mu/d\\eta = \\mu(1-\\mu)$，这简化了权重为 $w = \\mu(1-\\mu)$。\n*   **Cloglog**：$\\mu = 1 - e^{-e^{\\eta}}$。这里，$d\\mu/d\\eta = (1-\\mu)e^{\\eta}$。\n\n**3. 残差诊断**\n在拟合模型并获得估计的均值 $\\hat{\\mu}_i$ 后，我们计算伯努利偏差残差以评估模型拟合度。公式为：\n$$ r_i = \\operatorname{sign}(y_i - \\hat{\\mu}_i) \\sqrt{-2 \\left[ y_i \\log(\\hat{\\mu}_i) + (1 - y_i) \\log(1 - \\hat{\\mu}_i) \\right]} $$\n对 $\\hat{\\mu}_i$ 使用适当的数值截断以避免对零取对数。\n\n然后我们计算三个诊断指标来量化这些残差中的系统性模式：\n1.  **$|\\operatorname{corr}(r, \\eta_{\\text{true}})|$**：偏差残差与真实加性预测变量之间的绝对皮尔逊相关系数。高值表明模型的误差与基础信号系统性相关，这是误设的迹象。\n2.  **$R^2_{x_1}$**：将残差 $r$ 对 $x_1$ 的样条基（包括截距）进行普通最小二乘回归得到的决定系数。高 $R^2$ 表示残差具有可由 $x_1$ 解释的结构，这在一个设定良好的模型中不应发生。\n3.  **$R^2_{x_2}$**：类似地，将 $r$ 对 $x_2$ 的样条基进行回归得到的 $R^2$。\n\n对四个测试案例中的每一个都计算这些诊断指标。一个正确设定的模型（案例 2）预计所有三个指标的值都很低，表明残差接近随机噪声。误设的模型（案例 1、3、4）预计会显示更大的值，揭示了使用错误连结函数引入的系统性误差。该误差的大小将取决于信号强度 $s$。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    实现一个模拟，以诊断广义可加模型 (GAM) 中的连结函数误设，并报告诊断指标。\n    \"\"\"\n    \n    # 定义问题常数和超参数\n    KNOTS = np.array([0.2, 0.4, 0.6, 0.8])\n    LAMBDA_PEN = 0.01  # 一个固定的岭惩罚参数\n    MAX_ITER = 25      # IRLS 的最大迭代次数\n    TOL = 1e-7         # IRLS 的收敛容差\n\n    # --- 步骤 1：数据生成 ---\n\n    def f1(x1):\n        \"\"\"协变量 x1 的平滑函数。\"\"\"\n        return 1.2 * np.sin(2 * np.pi * x1) + 0.3 * (x1 - 0.5)\n\n    def f2(x2):\n        \"\"\"协变量 x2 的平滑函数。\"\"\"\n        return -0.8 * np.cos(2 * np.pi * x2) + 0.5 * (x2 - 0.5)**2\n\n    def generate_data(n, s, seed):\n        \"\"\"\n        从具有互补 log-log 连结的真实模型生成数据。\n        \"\"\"\n        rng = np.random.default_rng(seed)\n        x1 = rng.uniform(0, 1, n)\n        x2 = rng.uniform(0, 1, n)\n        \n        eta_true = s * (f1(x1) + f2(x2))\n        \n        # 真实连结是互补 log-log\n        p = 1 - np.exp(-np.exp(eta_true))\n        # 截断以保证数值稳定性\n        p = np.clip(p, 1e-15, 1 - 1e-15)\n        \n        y = rng.binomial(1, p)\n        \n        return x1, x2, y, eta_true\n\n    # --- 步骤 2：模型拟合 ---\n\n    def create_basis_matrix(x, knots):\n        \"\"\"为单个协变量创建截断三次样条基矩阵。\"\"\"\n        basis_cols = [x, x**2, x**3]\n        for k in knots:\n            basis_cols.append(np.maximum(x - k, 0)**3)\n        return np.stack(basis_cols, axis=1)\n\n    def create_design_matrix(x1, x2, knots):\n        \"\"\"创建包含截距项和基展开的完整设计矩阵 X。\"\"\"\n        n = len(x1)\n        intercept = np.ones((n, 1))\n        basis_x1 = create_basis_matrix(x1, knots)\n        basis_x2 = create_basis_matrix(x2, knots)\n        return np.hstack([intercept, basis_x1, basis_x2])\n\n    def fit_gam(X, y, link, lambda_pen):\n        \"\"\"\n        使用迭代重加权最小二乘法 (IRLS) 拟合一个带惩罚的 GAM。\n        \"\"\"\n        n, p = X.shape\n        beta = np.zeros(p)\n\n        # 基于经验 logit 对 eta 进行初始猜测\n        mu_init = np.clip((y + 0.5) / 2, 1e-15, 1 - 1e-15)\n        if link == 'logit':\n            eta = np.log(mu_init / (1 - mu_init))\n        else: # cloglog\n            eta = np.log(-np.log(1 - mu_init))\n\n        penalty_matrix = lambda_pen * np.diag([0] + [1] * (p - 1))\n\n        for _ in range(MAX_ITER):\n            # 截断 eta 以避免 exp() 溢出，尤其是在使用 cloglog 时\n            eta = np.clip(eta, -30, 30)\n\n            if link == 'logit':\n                mu = 1 / (1 + np.exp(-eta))\n                d_mu_d_eta = mu * (1 - mu)\n            else: # cloglog\n                exp_eta = np.exp(eta)\n                mu = 1 - np.exp(-exp_eta)\n                d_mu_d_eta = (1 - mu) * exp_eta\n            \n            # 截断值以保证数值稳定性\n            mu = np.clip(mu, 1e-15, 1 - 1e-15)\n            V = np.maximum(mu * (1 - mu), 1e-15)\n            d_mu_d_eta = np.maximum(d_mu_d_eta, 1e-15)\n\n            weights = (d_mu_d_eta**2) / V\n            working_response = eta + (y - mu) / d_mu_d_eta\n            \n            # 求解带惩罚的加权最小二乘系统\n            XtW = X.T * weights\n            lhs = XtW @ X + penalty_matrix\n            rhs = XtW @ working_response\n\n            try:\n                beta_new = np.linalg.solve(lhs, rhs)\n            except np.linalg.LinAlgError:\n                # 如果矩阵是奇异的，添加扰动以保证稳定性\n                lhs += np.eye(p) * 1e-8 \n                beta_new = np.linalg.solve(lhs, rhs)\n\n            # 基于系数的变化检查收敛性\n            if np.linalg.norm(beta_new - beta) / (np.linalg.norm(beta) + TOL)  TOL:\n                beta = beta_new\n                break\n            \n            beta = beta_new\n            eta = X @ beta\n            \n        # 计算最终的拟合均值\n        eta = np.clip(X @ beta, -30, 30)\n        if link == 'logit':\n            mu_hat = 1 / (1 + np.exp(-eta))\n        else: # cloglog\n            mu_hat = 1 - np.exp(-np.exp(eta))\n            \n        return np.clip(mu_hat, 1e-15, 1 - 1e-15)\n\n    # --- 步骤 3：残差诊断 ---\n\n    def calculate_diagnostics(y, mu_hat, eta_true, x1, x2, knots):\n        \"\"\"\n        计算偏差残差和三个诊断指标。\n        \"\"\"\n        # 伯努利偏差残差\n        term1 = np.where(y == 1, -np.log(mu_hat), 0)\n        term2 = np.where(y == 0, -np.log(1 - mu_hat), 0)\n        dev_res_unsigned = np.sqrt(2 * (term1 + term2))\n        residuals = np.sign(y - mu_hat) * dev_res_unsigned\n\n        # 指标 1：与真实线性预测变量的绝对相关性\n        corr_eta_true = np.abs(np.corrcoef(residuals, eta_true)[0, 1])\n\n        # 指标 2 和 3：将残差对基样条进行回归得到的 R^2\n        def get_r_squared(x, knots_local, r):\n            n_obs = len(x)\n            X_reg = np.hstack([np.ones((n_obs, 1)), create_basis_matrix(x, knots_local)])\n            \n            coeffs = np.linalg.lstsq(X_reg, r, rcond=None)[0]\n            r_hat = X_reg @ coeffs\n            ss_res = np.sum((r - r_hat)**2)\n            ss_tot = np.sum((r - np.mean(r))**2)\n            \n            if ss_tot  1e-12:\n                return 1.0 if ss_res  1e-12 else 0.0\n            return 1 - ss_res / ss_tot\n\n        r_squared_x1 = get_r_squared(x1, knots, residuals)\n        r_squared_x2 = get_r_squared(x2, knots, residuals)\n\n        return [corr_eta_true, r_squared_x1, r_squared_x2]\n\n    # --- 主执行循环 ---\n    \n    test_cases = [\n        # n, s, seed, link_to_fit\n        (800, 1.0, 42, 'logit'),   # 情况 1：误设（标准）\n        (800, 1.0, 42, 'cloglog'), # 情况 2：正确设定\n        (800, 2.5, 123, 'logit'),  # 情况 3：误设，强信号\n        (800, 0.2, 7, 'logit'),    # 情况 4：误设，弱信号\n    ]\n    \n    results = []\n    for n, s, seed, link_to_fit in test_cases:\n        x1, x2, y, eta_true = generate_data(n=n, s=s, seed=seed)\n        X = create_design_matrix(x1, x2, KNOTS)\n        mu_hat = fit_gam(X, y, link=link_to_fit, lambda_pen=LAMBDA_PEN)\n        diagnostics = calculate_diagnostics(y, mu_hat, eta_true, x1, x2, KNOTS)\n        results.append(diagnostics)\n\n    # 完全按照要求格式化最终输出字符串\n    case_strs = [f\"[{','.join(map(str, res))}]\" for res in results]\n    print(f\"[{','.join(case_strs)}]\")\n\nsolve()\n```", "id": "3123718"}, {"introduction": "在处理计数数据的模型中，如泊松（Poisson）GAM，偏移项（offset）或暴露度（exposure）项扮演着至关重要的角色，它用于校正因观测时间或空间区域不同而带来的影响。一个设定错误的偏移项会扭曲模型对协变量效应的估计。此练习将引导您实现一种实用的验证技术，通过比较包含和移除偏移项后模型中平滑函数估计形状的变化，来诊断偏移项是否被正确指定 [@problem_id:3123654]。", "problem": "要求您在一个独立的程序中实现泊松广义可加模型（GAM）的暴露验证。其目的是通过比较在去除偏移量（offset）并重新拟合模型时，估计的平滑分量的变化情况，来检测错误指定的暴露（偏移量）。\n\n基本基础和设置：\n- 考虑由 $i = 1, \\dots, n$ 索引的独立观测值，其中响应 $Y_i$ 被建模为均值为 $\\mu_i$ 的泊松随机变量，即 $Y_i \\sim \\mathrm{Poisson}(\\mu_i)$ 且 $\\mathrm{E}[Y_i] = \\mu_i$。\n- 广义可加模型（GAM）假设通过对数连接 $\\eta_i = \\log(\\mu_i)$ 对正则参数具有可加结构，其中\n$$\n\\eta_i = \\mathrm{offset}_i + \\beta_0 + f_1(x_{i1}) + f_2(x_{i2}).\n$$\n- 偏移量 $\\mathrm{offset}_i$ 是已知的对数暴露项，$f_j(\\cdot)$ 是我们从数据中估计的未知平滑函数。\n- 将每个平滑函数 $f_j$ 写成基展开式 $f_j(x_{ij}) = \\sum_{k=1}^{m_j} \\theta_{jk} B_{jk}(x_{ij})$，其中 $B_{jk}$ 是固定的样条基函数，$\\theta_{jk}$ 是待估计的系数。对每个系数向量的二阶离散差分引入二次粗糙度惩罚以控制平滑度。\n- 使用迭代重加权最小二乘法（IRLS）拟合泊松GAM，这是一种基于广义线性模型最大似然的经典方法：在迭代 $t$ 次时，构建工作响应 $z^{(t)}$ 和权重 $W^{(t)}$，并求解一个惩罚加权最小二乘系统来获得系数。偏移量在线性预测器中以加法形式进入。\n- 为了验证暴露设定，需要拟合模型两次：第一次使用提供的偏移量向量，第二次去除偏移量（即设为零）。对于每个平滑分量 $j$，计算在观测协变量处的拟合平滑函数，记为 $\\hat{f}_j^{\\mathrm{with}}$ 和 $\\hat{f}_j^{\\mathrm{drop}}$。为了比较形状而非水平，通过减去其均值并除以其标准差来对每个向量进行标准化。计算均方差\n$$\n\\Delta_j = \\frac{1}{n} \\sum_{i=1}^n \\left(\\tilde{f}_{j,i}^{\\mathrm{with}} - \\tilde{f}_{j,i}^{\\mathrm{drop}}\\right)^2,\n$$\n其中 $\\tilde{f}_{j,i}^{\\mathrm{with}}$ 表示观测值 $i$ 处 $\\hat{f}_j^{\\mathrm{with}}$ 的标准化值，$\\tilde{f}_{j,i}^{\\mathrm{drop}}$ 同理。如果 $\\max_j \\Delta_j$ 超过给定的阈值 $\\tau$，则声明暴露被错误指定。\n\n您的程序必须实现：\n- 在 $[0,1]$ 上构建具有均匀间隔内部节点的立方B样条基 $B_{jk}(x)$。\n- 对每个平滑函数的系数施加二阶差分惩罚，为所有平滑函数生成一个块对角惩罚矩阵，同时保持截距项不受惩罚。\n- 用于泊松模型（带对数连接和可选偏移量）的惩罚IRLS算法，在每次迭代中求解一个对称线性系统以获得系数。\n- 通过计算 $j=1,2$ 的 $\\Delta_j$ 并将 $\\max_j \\Delta_j$ 与阈值 $\\tau$ 进行比较来进行暴露验证。\n\n用于评估的数据生成（固定、可复现）：\n- 对于每个测试用例，在 $[0,1]$ 上独立均匀地生成协变量 $x_{i1}, x_{i2}$，并从一个基准真实模型中模拟计数 $Y_i$，该模型为\n$$\n\\log(\\mu_i) = \\beta_0^\\star + f_1^\\star(x_{i1}) + f_2^\\star(x_{i2}),\n$$\n其中 $\\beta_0^\\star = 1.0$, $f_1^\\star(x) = \\sin(2\\pi x)$, 且 $f_2^\\star(x) = 0.5\\,(x - 0.5)^2$。真实的暴露是常数1，因此真实的偏移量为零。在不同测试用例中，向“带偏移量”的拟合提供不同的偏移量向量，以模拟正确和错误指定的暴露。\n\n测试套件：\n提供以下测试用例；对于每个用例，使用指定的参数模拟数据并执行暴露验证，根据阈值 $\\tau$ 返回一个指示是否存在错误指定的布尔值：\n\n1. 用例A（理想情况，正确的暴露）：\n   - $n = 400$，随机种子 $= 2021$，\n   - “带偏移量”拟合中使用的偏移量：对于所有 $i$，$\\mathrm{offset}_i = 0$，\n   - 阈值 $\\tau = 0.2$，\n   - 预期行为：形状应保持稳定，因此布尔值应为false。\n\n2. 用例B（与 $x_{1}$ 相关的错误指定的暴露）：\n   - $n = 400$，随机种子 $= 2022$，\n   - “带偏移量”拟合中使用的偏移量：$\\mathrm{offset}_i = 1.5\\,(x_{i1} - 0.5)$，\n   - 阈值 $\\tau = 0.2$，\n   - 预期行为：去除偏移量会因补偿作用而揭示形状差异，因此布尔值应为true。\n\n3. 用例C（边缘情况，具有非线性 $x_{2}$ 模式和较小样本的错误指定的暴露）：\n   - $n = 120$，随机种子 $= 2023$，\n   - “带偏移量”拟合中使用的偏移量：$\\mathrm{offset}_i = 1.5 \\cos(2\\pi x_{i2})$，\n   - 阈值 $\\tau = 0.2$，\n   - 预期行为：非线性的错误指定会产生显著的形状变化，因此布尔值应为true。\n\n建模和算法细节必须严格按照上述规定实现。所有随机性必须由测试套件中给定的种子控制。\n\n最终输出格式：\n您的程序应生成单行输出，其中包含用例A、B和C的三个布尔值，以逗号分隔的列表形式并用方括号括起来，例如，“[False,True,True]”。", "solution": "用户要求实现泊松广义可加模型（GAM）的暴露验证程序。该问题是自包含的且科学上合理的，基于统计学习的既定原则，包括GAM、B样条、惩罚似然和迭代重加权最小二乘（IRLS）算法。验证逻辑定义清晰，测试用例指定了所有必要参数。我将继续进行解答。\n\n任务的核心是拟合泊松GAM两次：一次使用指定的偏移量（对数暴露），一次不使用（偏移量设为零）。从这两次拟合中得到的估计平滑函数形状之间的差异，用于诊断暴露项可能存在的错误指定。\n\n分步过程如下：\n\n1.  **数据模拟**：对于每个测试用例，我们首先根据提供的基准真实模型模拟一个数据集。协变量 $x_{i1}, x_{i2}$ 从 $[0, 1]$ 上的均匀分布中抽取。然后从泊松分布中抽样响应变量 $Y_i$，其均值 $\\mu_i$ 的对数由一个真实的可加模型给出：\n    $$\n    \\log(\\mu_i) = \\beta_0^\\star + f_1^\\star(x_{i1}) + f_2^\\star(x_{i2})\n    $$\n    其中 $\\beta_0^\\star = 1.0$，$f_1^\\star(x) = \\sin(2\\pi x)$，以及 $f_2^\\star(x) = 0.5(x - 0.5)^2$。\n\n2.  **模型设定**：待拟合的GAM由线性预测器定义：\n    $$\n    \\eta_i = \\mathrm{offset}_i + \\beta_0 + f_1(x_{i1}) + f_2(x_{i2})\n    $$\n    每个平滑函数 $f_j$ 都通过使用立方B样条的基展开来表示。我们将为每个平滑函数使用 $m_j=10$ 个基函数，这提供了足够的灵活性来捕捉底层的真实函数。模型矩阵 $X$ 是通过连接一个用于截距 $\\beta_0$ 的全1列以及 $f_1$ 和 $f_2$ 的基矩阵来构建的。\n\n3.  **惩罚似然**：为防止过拟合并确保数值稳定性，通过惩罚其相应系数的粗糙度来鼓励函数 $f_j$ 的平滑性。对每个B样条展开的系数应用二阶差分惩罚。完整的惩罚矩阵 $S$ 是块对角矩阵，其中未惩罚的截距 $\\beta_0$ 对应一个零块，每个平滑分量对应一个惩罚矩阵 $\\lambda_j S_j$。对两个平滑函数都使用固定的平滑参数 $\\lambda_j=1.0$。\n\n4.  **通过惩罚IRLS进行模型拟合**：通过最大化惩罚泊松对数似然来估计模型系数。这是通过使用迭代重加权最小二乘（IRLS）算法实现的。在每次迭代中，我们更新均值 $\\mu^{(t)}$ 的当前估计，并形成一个工作响应 $z^{(t)}$ 和一个对角权重矩阵 $W^{(t)}$。然后通过求解以下惩罚加权最小二乘系统来找到更新后的系数向量 $\\beta^{(t+1)}$：\n    $$\n    (X^T W^{(t)} X + S) \\beta^{(t+1)} = X^T W^{(t)} z'^{(t)}\n    $$\n    其中 $z'^{(t)}$ 是为固定偏移量调整过的工作响应。这个迭代过程持续进行，直到系数收敛。\n\n5.  **暴露验证**：对每个测试用例执行两次拟合程序：\n    a.  **“带偏移量”拟合**：使用测试用例中指定的偏移量向量。\n    b.  **“去除偏移量”拟合**：使用全零的偏移量向量。\n\n    这将产生两组估计系数，$\\hat{\\beta}^{\\mathrm{with}}$ 和 $\\hat{\\beta}^{\\mathrm{drop}}$。根据这些系数，我们计算每个分量的拟合平滑函数，例如 $\\hat{f}_{j}^{\\mathrm{with}} = X_j \\hat{\\theta}_j^{\\mathrm{with}}$。为了在不考虑水平偏移的情况下比较它们的形状，将四个生成的拟合平滑向量中的每一个都标准化，使其均值为0，标准差为1。\n\n6.  **差异度量与决策**：为每个分量 $j \\in \\{1, 2\\}$ 计算标准化的“带偏移量”和“去除偏移量”拟合平滑函数之间的均方差 $\\Delta_j$。\n    $$\n    \\Delta_j = \\frac{1}{n} \\sum_{i=1}^n \\left(\\tilde{f}_{j,i}^{\\mathrm{with}} - \\tilde{f}_{j,i}^{\\mathrm{drop}}\\right)^2\n    $$\n    如果这些差异的最大值 $\\max(\\Delta_1, \\Delta_2)$ 超过给定的阈值 $\\tau$，则将暴露标记为错误指定。\n\n该实现将这些步骤封装到一系列函数中：用于生成B样条基和惩罚矩阵的辅助函数、一个核心的P-IRLS求解器，以及一个主驱动函数，用于按问题陈述中指定的方式为每个测试用例执行验证。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.interpolate import BSpline\n\ndef get_bspline_basis(x, n_basis, degree):\n    \"\"\"\n    Constructs a B-spline basis matrix with uniformly spaced knots in [0, 1].\n\n    Args:\n        x (np.ndarray): 1D array of data points, assumed to be in [0, 1].\n        n_basis (int): The number of basis functions.\n        degree (int): The degree of the spline (e.g., 3 for cubic).\n\n    Returns:\n        np.ndarray: The basis matrix of shape (len(x), n_basis).\n    \"\"\"\n    n_interior_knots = n_basis - degree - 1\n    if n_interior_knots  0:\n        n_interior_knots = 0\n    \n    # Create uniformly spaced interior knots in (0, 1)\n    interior_knots = np.linspace(0.0, 1.0, n_interior_knots + 2)[1:-1]\n    \n    # Full knot vector with clamped boundaries\n    knots = np.concatenate([\n        np.zeros(degree + 1),\n        interior_knots,\n        np.ones(degree + 1)\n    ])\n    \n    basis_matrix = np.zeros((len(x), n_basis))\n    for i in range(n_basis):\n        # Create a spline basis function by setting the i-th coefficient to 1\n        c = np.zeros(n_basis)\n        c[i] = 1.0\n        spl = BSpline(knots, c, degree, extrapolate=False)\n        basis_matrix[:, i] = spl(x)\n\n    basis_matrix[np.isnan(basis_matrix)] = 0.0\n    \n    return basis_matrix\n\n\ndef get_penalty_matrix(n_basis, penalty_lambda):\n    \"\"\"\n    Constructs the penalty matrix for penalizing second differences of coefficients.\n\n    Args:\n        n_basis (int): Number of coefficients to penalize.\n        penalty_lambda (float): The smoothing parameter.\n\n    Returns:\n        np.ndarray: The (n_basis x n_basis) penalty matrix S.\n    \"\"\"\n    # Create the second-order difference matrix D\n    D = np.diff(np.eye(n_basis), n=2, axis=0)\n    # The penalty matrix is S = D^T D\n    S = D.T @ D\n    return penalty_lambda * S\n\n\ndef fit_gam_poisson(y, X, S_full, offset, n_iter=25, tol=1e-6):\n    \"\"\"\n    Fits a Poisson GAM using Penalized Iteratively Reweighted Least Squares (P-IRLS).\n\n    Args:\n        y (np.ndarray): The response vector (counts).\n        X (np.ndarray): The full model matrix (intercept and basis matrices).\n        S_full (np.ndarray): The block-diagonal penalty matrix.\n        offset (np.ndarray): The offset vector (log-exposure).\n        n_iter (int): Maximum number of IRLS iterations.\n        tol (float): Convergence tolerance for the coefficient vector.\n\n    Returns:\n        np.ndarray: The estimated coefficient vector beta.\n    \"\"\"\n    n_coeffs = X.shape[1]\n    \n    # Initialize coefficients\n    beta = np.zeros(n_coeffs)\n    \n    for _ in range(n_iter):\n        beta_old = beta.copy()\n        \n        eta = X @ beta + offset\n        eta = np.clip(eta, -20, 20)\n        mu = np.exp(eta)\n        mu[mu  1e-8] = 1e-8\n        \n        weights = mu\n        working_response = eta - offset + (y - mu) / mu\n        \n        LHS = X.T @ (weights[:, np.newaxis] * X) + S_full\n        RHS = X.T @ (weights * working_response)\n        \n        try:\n            beta = np.linalg.solve(LHS, RHS)\n        except np.linalg.LinAlgError:\n            beta = np.linalg.pinv(LHS) @ RHS\n            \n        if np.linalg.norm(beta - beta_old) / (np.linalg.norm(beta_old) + 1e-8)  tol:\n            break\n            \n    return beta\n\n\ndef standardize_vec(v):\n    \"\"\"\n    Standardizes a vector to have mean 0 and standard deviation 1.\n    Handles the case where the standard deviation is zero.\n    \"\"\"\n    s = np.std(v)\n    if s  1e-8:\n        return np.zeros_like(v, dtype=float)\n    return (v - np.mean(v)) / s\n\ndef execute_validation_case(n, seed, offset_func, tau):\n    \"\"\"\n    Sets up and runs a single exposure validation case.\n    \"\"\"\n    # Model hyperparameters\n    n_basis = 10\n    degree = 3\n    lambda_val = 1.0\n\n    # 1. Generate data based on the true model\n    rng = np.random.default_rng(seed)\n    x1 = rng.uniform(0, 1, n)\n    x2 = rng.uniform(0, 1, n)\n    \n    beta0_star = 1.0\n    f1_star = np.sin(2 * np.pi * x1)\n    f2_star = 0.5 * (x2 - 0.5)**2\n    \n    log_mu_true = beta0_star + f1_star + f2_star\n    mu_true = np.exp(log_mu_true)\n    y = rng.poisson(mu_true)\n    \n    # 2. Build model design matrix X\n    B1 = get_bspline_basis(x1, n_basis=n_basis, degree=degree)\n    B2 = get_bspline_basis(x2, n_basis=n_basis, degree=degree)\n    X = np.c_[np.ones(n), B1, B2]\n    \n    # 3. Build the full penalty matrix S_full\n    S1 = get_penalty_matrix(n_basis, lambda_val)\n    S2 = get_penalty_matrix(n_basis, lambda_val)\n    \n    n_coeffs = 1 + n_basis + n_basis\n    S_full = np.zeros((n_coeffs, n_coeffs))\n    S_full[1:1+n_basis, 1:1+n_basis] = S1\n    S_full[1+n_basis:, 1+n_basis:] = S2\n    \n    # 4. Fit the GAM with and without the specified offset\n    offset_with = offset_func(x1, x2)\n    offset_drop = np.zeros(n)\n    \n    beta_with = fit_gam_poisson(y, X, S_full, offset_with)\n    beta_drop = fit_gam_poisson(y, X, S_full, offset_drop)\n    \n    # 5. Extract, standardize, and compare the fitted smooth components\n    theta1_with = beta_with[1:1+n_basis]\n    theta2_with = beta_with[1+n_basis:]\n    theta1_drop = beta_drop[1:1+n_basis]\n    theta2_drop = beta_drop[1+n_basis:]\n    \n    f1_hat_with = B1 @ theta1_with\n    f2_hat_with = B2 @ theta2_with\n    f1_hat_drop = B1 @ theta1_drop\n    f2_hat_drop = B2 @ theta2_drop\n    \n    f1_tilde_with = standardize_vec(f1_hat_with)\n    f2_tilde_with = standardize_vec(f2_hat_with)\n    f1_tilde_drop = standardize_vec(f1_hat_drop)\n    f2_tilde_drop = standardize_vec(f2_hat_drop)\n\n    delta1 = np.mean((f1_tilde_with - f1_tilde_drop)**2)\n    delta2 = np.mean((f2_tilde_with - f2_tilde_drop)**2)\n    \n    # 6. Declare mis-specification if the max difference exceeds the threshold\n    return max(delta1, delta2) > tau\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the results.\n    \"\"\"\n    test_cases = [\n        # Case A: Correct exposure (offset=0, matches true model)\n        {'n': 400, 'seed': 2021, 'offset_func': lambda x1, x2: np.zeros(len(x1)), 'tau': 0.2},\n        # Case B: Mis-specified exposure correlated with x1\n        {'n': 400, 'seed': 2022, 'offset_func': lambda x1, x2: 1.5 * (x1 - 0.5), 'tau': 0.2},\n        # Case C: Mis-specified exposure non-linearly related to x2\n        {'n': 120, 'seed': 2023, 'offset_func': lambda x1, x2: 1.5 * np.cos(2 * np.pi * x2), 'tau': 0.2}\n    ]\n    \n    results = []\n    for case in test_cases:\n        is_misspecified = execute_validation_case(\n            n=case['n'],\n            seed=case['seed'],\n            offset_func=case['offset_func'],\n            tau=case['tau']\n        )\n        results.append(is_misspecified)\n    \n    # Print results in the specified format [boolean1,boolean2,...]\n    print(f\"[{','.join(map(lambda b: str(b).lower(), results))}]\")\n\nsolve()\n\n```", "id": "3123654"}]}