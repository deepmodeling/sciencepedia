{"hands_on_practices": [{"introduction": "在深入研究样条拟合之前，理解平滑惩罚项的性质至关重要。本练习 [@problem_id:3168910] 探讨了对输入变量进行重新缩放会如何影响粗糙度惩罚项，这是一个确保您能够在不同数据集之间正确解释和比较平滑参数 $\\lambda$ 的基本问题。通过完成这个推导，您将深刻理解为何选择合适的 $\\lambda$ 值依赖于数据的尺度。", "problem": "考虑一个数据集 $\\{(x_i,y_i)\\}_{i=1}^{n}$，其中 $x_i \\in [a,b]$，以及一个二阶连续可微函数 $f:[a,b]\\to \\mathbb{R}$。在使用三次回归样条的惩罚最小二乘平滑中，需要最小化以下准则\n$$\nJ(f) \\;=\\; \\sum_{i=1}^{n} \\big(y_i - f(x_i)\\big)^2 \\;+\\; \\lambda \\int_{a}^{b} \\big(f''(x)\\big)^2 \\, dx,\n$$\n其中 $\\lambda > 0$ 是一个平滑参数，用于控制对数据的保真度与粗糙度惩罚项 $\\int_{a}^{b} \\big(f''(x)\\big)^2 \\, dx$ 之间的权衡。\n\n定义重新缩放的变量 $u \\in [0,1]$ 为 $u = (x-a)/(b-a)$，以及重新缩放的函数 $g:[0,1]\\to \\mathbb{R}$ 为 $g(u) = f\\big(a + (b-a)u\\big)$。假设我们将相应的重新缩放的惩罚准则写成以下形式\n$$\n\\tilde{J}(g) \\;=\\; \\sum_{i=1}^{n} \\big(y_i - g(u_i)\\big)^2 \\;+\\; \\tilde{\\lambda} \\int_{0}^{1} \\big(g''(u)\\big)^2 \\, du,\n$$\n其中 $u_i = (x_i-a)/(b-a)$，以及某个变换后的平滑参数 $\\tilde{\\lambda} > 0$。\n\n仅使用微积分的基本原理，推导在此变量变换下粗糙度惩罚项的精确转换，并确定 $\\tilde{\\lambda}$ 关于 $\\lambda$ 和 $b-a$ 的表达式。你的最终答案必须是 $\\tilde{\\lambda}$ 关于 $\\lambda$ 和 $b-a$ 的一个单一封闭形式表达式。", "solution": "该问题是有效的，因为它科学地基于微积分和统计学习的原理，问题本身提法明确、客观，并包含一个完整且一致的设定。我们可以开始推导。\n\n问题陈述了两个惩罚最小二乘准则，一个是关于区间 $[a,b]$ 上的函数 $f(x)$，另一个是关于区间 $[0,1]$ 上的重新缩放函数 $g(u)$。为了使这两个准则表示相同的底层优化问题，对于任何函数 $f$ 及其对应的重新缩放版本 $g$，它们必须相等。原始准则是\n$$\nJ(f) = \\sum_{i=1}^{n} \\big(y_i - f(x_i)\\big)^2 + \\lambda \\int_{a}^{b} \\big(f''(x)\\big)^2 \\, dx\n$$\n重新缩放后的准则是\n$$\n\\tilde{J}(g) = \\sum_{i=1}^{n} \\big(y_i - g(u_i)\\big)^2 + \\tilde{\\lambda} \\int_{0}^{1} \\big(g''(u)\\big)^2 \\, du\n$$\n函数和变量之间的关系由 $u = (x-a)/(b-a)$ 和 $g(u) = f(a + (b-a)u)$ 给出。数据点之间的关系为 $u_i = (x_i-a)/(b-a)$。\n\n首先，我们来考察平方和项。重新缩放函数 $g$ 在重新缩放点 $u_i$ 处的定义是 $g(u_i) = f(a + (b-a)u_i)$。根据 $u_i$ 的定义，我们有 $x_i = a + (b-a)u_i$。因此，$g(u_i) = f(x_i)$。这意味着平方和项是相同的：\n$$\n\\sum_{i=1}^{n} \\big(y_i - f(x_i)\\big)^2 = \\sum_{i=1}^{n} \\big(y_i - g(u_i)\\big)^2\n$$\n为了使两个准则 $J(f)$ 和 $\\tilde{J}(g)$ 相等，惩罚项也必须相等：\n$$\n\\lambda \\int_{a}^{b} \\big(f''(x)\\big)^2 \\, dx = \\tilde{\\lambda} \\int_{0}^{1} \\big(g''(u)\\big)^2 \\, du\n$$\n我们的目标是通过将左侧的积分从变量 $x$ 变换到变量 $u$，来找到 $\\tilde{\\lambda}$ 和 $\\lambda$ 之间的关系。\n\n变量变换由 $x = a + (b-a)u$ 给出。首先，我们用 $du$ 来表示微分 $dx$：\n$$\n\\frac{dx}{du} = b-a \\quad \\implies \\quad dx = (b-a) \\, du\n$$\n接下来，我们确定新变量 $u$ 的积分限。当 $x=a$ 时，我们有 $u = (a-a)/(b-a) = 0$。当 $x=b$ 时，我们有 $u = (b-a)/(b-a) = 1$。\n\n现在，我们必须用 $g$ 关于 $u$ 的导数来表示二阶导数 $f''(x)$。关系是 $g(u) = f(x(u))$，其中 $x(u) = a + (b-a)u$。我们应用链式法则求导。$g(u)$ 的一阶导数是：\n$$\ng'(u) = \\frac{dg}{du} = \\frac{df}{dx} \\frac{dx}{du} = f'(x) \\cdot (b-a)\n$$\n为了求二阶导数，我们对 $g'(u)$ 关于 $u$ 求导，再次使用链式法则：\n$$\ng''(u) = \\frac{d^2g}{du^2} = \\frac{d}{du} \\Big( f'(x(u)) \\cdot (b-a) \\Big) = (b-a) \\cdot \\frac{d}{du} \\big(f'(x(u))\\big)\n$$\n$$\ng''(u) = (b-a) \\cdot \\left( \\frac{d(f'(x))}{dx} \\frac{dx}{du} \\right) = (b-a) \\cdot \\Big( f''(x) \\cdot (b-a) \\Big) = (b-a)^2 f''(x)\n$$\n从这个关系中，我们可以用 $g''(u)$ 来表示 $f''(x)$：\n$$\nf''(x) = \\frac{1}{(b-a)^2} g''(u)\n$$\n我们现在拥有了变换定义粗糙度惩罚项的积分所需的所有要素。我们将 $f''(x)$、$dx$ 和积分限的表达式代入原积分中：\n$$\n\\int_{a}^{b} \\big(f''(x)\\big)^2 \\, dx = \\int_{0}^{1} \\left( \\frac{1}{(b-a)^2} g''(u) \\right)^2 (b-a) \\, du\n$$\n$$\n\\int_{a}^{b} \\big(f''(x)\\big)^2 \\, dx = \\int_{0}^{1} \\frac{1}{(b-a)^4} \\big(g''(u)\\big)^2 (b-a) \\, du\n$$\n化简常数因子，我们得到：\n$$\n\\int_{a}^{b} \\big(f''(x)\\big)^2 \\, dx = \\frac{1}{(b-a)^3} \\int_{0}^{1} \\big(g''(u)\\big)^2 \\, du\n$$\n现在我们回到惩罚项相等的等式：\n$$\n\\lambda \\int_{a}^{b} \\big(f''(x)\\big)^2 \\, dx = \\tilde{\\lambda} \\int_{0}^{1} \\big(g''(u)\\big)^2 \\, du\n$$\n代入我们变换后的积分：\n$$\n\\lambda \\left( \\frac{1}{(b-a)^3} \\int_{0}^{1} \\big(g''(u)\\big)^2 \\, du \\right) = \\tilde{\\lambda} \\int_{0}^{1} \\big(g''(u)\\big)^2 \\, du\n$$\n假设在一个非平凡的平滑问题中，粗糙度积分不为零（即，$g''$ 不恒等于零），我们可以将等式两边同时除以 $\\int_{0}^{1} \\big(g''(u)\\big)^2 \\, du$ 来解出 $\\tilde{\\lambda}$：\n$$\n\\tilde{\\lambda} = \\frac{\\lambda}{(b-a)^3}\n$$\n这就是所求的原始平滑参数 $\\lambda$ 与重新缩放的平滑参数 $\\tilde{\\lambda}$ 之间的关系。", "answer": "$$\n\\boxed{\\frac{\\lambda}{(b-a)^3}}\n$$", "id": "3168910"}, {"introduction": "回归样条的灵活性由其节点（knots）决定，但我们应将节点置于何处？这项动手实践 [@problem_id:3168933] 通过比较两种常见的策略来探讨这个关键问题：均匀放置节点与根据数据分位数放置节点。您将通过模拟亲眼看到，当数据点分布不均或真实函数含有尖锐特征时，一种适应性的节点放置策略为何至关重要。", "problem": "考虑一个单变量回归问题，其中观测值为数据对 $(x_i, y_i)$，$i = 1, \\dots, n$，且 $x_i \\in [0,1]$。数据由一个在指定位置 $x_0$ 具有尖锐拐点并带有加性噪声的分段平滑函数生成。真实的回归函数定义为\n$$\nf(x) = g(x) + \\alpha \\,(x - x_0)_+,\n$$\n其中 $(u)_+ = \\max(0,u)$ 表示正部函数（铰链函数），$\\alpha$ 是一个控制拐点严重程度的正常量，$g(x)$ 是一个平滑基线。在本问题中，平滑基线为\n$$\ng(x) = \\tfrac{1}{2}\\sin(4\\pi x).\n$$\n观测值遵循以下模型\n$$\ny_i = f(x_i) + \\varepsilon_i,\n$$\n其中 $\\varepsilon_i$ 是独立同分布 (i.i.d.) 的噪声项，满足 $\\varepsilon_i \\sim \\mathcal{N}(0,\\sigma^2)$。\n\n我们将使用两种节点放置策略对数据拟合三次回归样条，并比较它们在拐点处的局部偏差。一个具有节点 $\\{\\xi_j\\}_{j=1}^K$ 的三次回归样条可以通过截断幂基表示为\n$$\n\\hat{f}(x) = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\beta_3 x^3 + \\sum_{j=1}^K \\theta_j (x - \\xi_j)_+^3,\n$$\n其参数 $\\beta_0, \\beta_1, \\beta_2, \\beta_3, \\theta_1, \\dots, \\theta_K$ 通过最小二乘法 (LS) 估计，即通过最小化\n$$\n\\sum_{i=1}^n \\left(y_i - \\hat{f}(x_i)\\right)^2.\n$$\n将比较两种节点放置策略：\n- 等距节点：在 $[0,1]$ 上选择 $K$ 个等距的内部节点，位置为 $\\xi_j = \\frac{j}{K+1}$，$j = 1, \\dots, K$。\n- 经验分位数节点：在观测到的 $x$ 值的经验分位数处选择 $K$ 个内部节点，概率为 $p_j = \\frac{j}{K+1}$，$j = 1, \\dots, K$。\n\n对于一个估计量 $\\hat{f}$，其在拐点位置 $x_0$ 的局部偏差定义为\n$$\nb(x_0) = \\hat{f}(x_0) - f(x_0).\n$$\n我们将测量并报告每种节点策略的绝对局部偏差 $|b(x_0)|$。\n\n推导和算法设计的基本基础：\n- 带有基函数的线性模型表示：$y = X\\beta + \\varepsilon$，其中 $X$ 是由所选基函数形成的设计矩阵，$\\beta$ 是系数，$\\varepsilon$ 是均值为零的随机噪声。\n- 最小二乘估计量：参数向量 $\\hat{\\beta}$ 最小化残差平方和，并在满列秩的条件下满足正规方程 $X^\\top X \\hat{\\beta} = X^\\top y$，或者在一般情况下通过稳定的数值求解器获得。\n\n您的任务是实现一个程序，该程序：\n1. 根据下面测试套件中指定的生成模型模拟数据集，使用固定的随机种子 $42$ 以保证可复现性。\n2. 使用两种节点策略拟合三次回归样条。\n3. 计算每种策略在拐点处的绝对局部偏差 $|b(x_0)|$。\n4. 生成单行输出，包含一个由方括号括起来的逗号分隔列表。每个测试用例贡献一个包含两个浮点数的子列表，顺序为 $[\\text{equal\\_spaced\\_bias}, \\text{quantile\\_bias}]$。最终输出格式必须为\n$$\n[\\,[b_{1,\\text{eq}}, b_{1,\\text{qt}}],\\,[b_{2,\\text{eq}}, b_{2,\\text{qt}}],\\,\\dots\\,]\n$$\n内部列表中没有空格，条目之间用逗号分隔。\n\n测试套件规范：\n对于每个测试用例，参数为 $(n, \\text{distribution}, x_0, \\sigma, K, \\alpha)$，如下所示。当分布为 uniform 时，抽取 $x_i \\sim \\text{Uniform}(0,1)$。当分布为 Beta 时，抽取 $x_i \\sim \\text{Beta}(a,b)$，然后线性映射到 $[0,1]$（请注意，标准 Beta 分布已经位于 $[0,1]$ 区间内）。\n\n- 用例 $1$：$(n = 200,\\, \\text{Uniform}(0,1),\\, x_0 = 0.5,\\, \\sigma = 0.05,\\, K = 6,\\, \\alpha = 1.5)$。\n- 用例 $2$：$(n = 200,\\, \\text{Beta}(a=2,b=5),\\, x_0 = 0.8,\\, \\sigma = 0.05,\\, K = 6,\\, \\alpha = 1.5)$。\n- 用例 $3$：$(n = 80,\\, \\text{Uniform}(0,1),\\, x_0 = 0.2,\\, \\sigma = 0.10,\\, K = 4,\\, \\alpha = 2.0)$。\n- 用例 $4$：$(n = 100,\\, \\text{Beta}(a=0.5,b=0.5),\\, x_0 = 0.5,\\, \\sigma = 0.05,\\, K = 6,\\, \\alpha = 2.0)$。\n- 用例 $5$：$(n = 50,\\, \\text{Uniform}(0,1),\\, x_0 = 0.9,\\, \\sigma = 0.20,\\, K = 3,\\, \\alpha = 1.0)$。\n\n算法细节和约束：\n- 构建设计矩阵 $X$，其列为 $1, x, x^2, x^3$ 和 $(x - \\xi_j)_+^3$（对于每个节点 $\\xi_j$）。\n- 使用数值稳定的求解器（例如，通过奇异值分解）求解 LS 问题来估计系数。\n- 如果经验分位数因数值相同而产生非严格递增的节点，通过添加微小扰动（例如，将下一个节点增加一个小的正增量使其稍大一些）来强制实现严格递增，以避免共线性。\n- 在任何出现的地方都使用铰链函数 $(u)_+ = \\max(0,u)$。\n\n最终输出格式：\n- 您的程序应生成单行输出，包含一个逗号分隔的子列表构成的列表，每个子列表包含两个浮点数，顺序为 $[\\text{equal\\_spaced\\_bias}, \\text{quantile\\_bias}]$，不带任何附加文本。例如，一个有效的输出格式是\n$$\n[[0.012345,0.023456],[0.034567,0.045678],\\dots]\n$$\n确保浮点数以十进制形式打印。\n\n本问题不涉及物理单位。三角函数中出现的任何角度（如果存在）均以弧度为单位。", "solution": "用户提供了一个有效的问题陈述。任务是从一个带拐点的分段平滑函数模拟数据，使用两种不同的节点放置策略拟合三次回归样条，并比较拟合结果在拐点位置的局部偏差。\n\n解决方案通过一系列原则性步骤展开：数据生成、模型设定、参数估计和偏差评估。\n\n问题的核心在于理解一旦基函数被定义，回归样条就是一个线性模型。决定基函数的节点选择对模型的性能至关重要，特别是其适应真实函数中局部特征（如拐点或急剧变化）的能力。\n\n数据生成过程遵循模型 $y_i = f(x_i) + \\varepsilon_i$，其中 $i=1, \\dots, n$。自变量 $x_i$ 从区间 $[0,1]$ 上的一个指定分布（Uniform 或 Beta）中抽取。真实的回归函数是 $f(x) = g(x) + \\alpha \\,(x - x_0)_+$，它由一个平滑基线部分 $g(x) = \\frac{1}{2}\\sin(4\\pi x)$ 和一个分段线性部分 $\\alpha \\,(x - x_0)_+$ 组成，后者在 $x=x_0$ 处引入一个拐点（一阶导数的不连续点）。术语 $(u)_+ = \\max(0,u)$ 是正部函数或铰链函数。噪声项 $\\varepsilon_i$ 是从均值为 $0$、方差为 $\\sigma^2$ 的正态分布中抽取的独立同分布样本，即 $\\varepsilon_i \\sim \\mathcal{N}(0,\\sigma^2)$。\n\n要拟合的模型是具有 $K$ 个内部节点 $\\{\\xi_j\\}_{j=1}^K$ 的三次回归样条。该模型使用截断幂基表示：\n$$\n\\hat{f}(x) = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\beta_3 x^3 + \\sum_{j=1}^K \\theta_j (x - \\xi_j)_+^3\n$$\n该函数是一个分段三次多项式，具有连续的二阶导数。该模型对其 $4+K$ 个参数 $\\mathbf{b} = (\\beta_0, \\beta_1, \\beta_2, \\beta_3, \\theta_1, \\dots, \\theta_K)^\\top$ 是线性的。对于一组观测值 $\\{x_i\\}_{i=1}^n$，我们可以构建一个 $n \\times (4+K)$ 的设计矩阵 $X$。$X$ 的每一行对应一个观测值 $x_i$，并包含在该点处求值的基函数的值：\n$$\nX_{i, \\cdot} = \\begin{bmatrix} 1 & x_i & x_i^2 & x_i^3 & (x_i - \\xi_1)_+^3 & \\dots & (x_i - \\xi_K)_+^3 \\end{bmatrix}\n$$\n模型参数向量 $\\mathbf{b}$ 通过最小化残差平方和 $\\sum_{i=1}^n (y_i - \\hat{f}(x_i))^2$ 来估计。这是标准的普通最小二乘 (LS) 问题。解 $\\hat{\\mathbf{b}}$ 可以通过求解线性系统 $X^\\top X \\hat{\\mathbf{b}} = X^\\top Y$ 得到，其中 $Y = (y_1, \\dots, y_n)^\\top$。为了数值稳定性，采用基于奇异值分解 (SVD) 的求解器，例如 `scipy.linalg.lstsq`。\n\n比较了两种节点放置策略：\n1.  **等距节点**：$K$ 个节点放置在区间 $[0,1]$ 内的固定、等距的位置：$\\xi_j = \\frac{j}{K+1}$，$j=1, \\dots, K$。此策略忽略了 $x_i$ 数据点的分布。\n2.  **经验分位数节点**：$K$ 个节点放置在观测到的 $x_i$ 值的样本分位数处，对应概率为 $p_j = \\frac{j}{K+1}$，$j=1, \\dots, K$。此策略是自适应的，在数据更密集的区域放置更多节点。为确保设计矩阵是满列秩的，该方法产生的任何非唯一节点都必须经过扰动以使其严格递增。\n\n性能度量是拐点处的绝对局部偏差，$|b(x_0)| = |\\hat{f}(x_0) - f(x_0)|$。为了计算这个值，通过取 $x_0$ 处的基向量与估计的参数向量 $\\hat{\\mathbf{b}}$ 的点积来评估拟合模型 $\\hat{f}(x_0)$。真实的函数值 $f(x_0)$ 直接根据其定义计算。\n\n对于每个指定的测试用例，整体算法流程如下：\n1.  用固定的种子 $42$ 初始化一个随机数生成器，以保证可复现性。\n2.  根据指定的参数模拟一个大小为 $n$ 的数据集 $(x_i, y_i)$。\n3.  确定两组节点（等距和基于分位数）。\n4.  对于每个节点集：\n    a. 构建设计矩阵 $X$。\n    b. 计算 LS 参数估计值 $\\hat{\\mathbf{b}}$。\n    c. 计算预测值 $\\hat{f}(x_0)$ 和真实值 $f(x_0)$。\n    d. 计算并存储绝对局部偏差 $|b(x_0)|$。\n5.  将所有测试用例的两种策略的结果收集起来，并格式化为所要求的单行输出。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import linalg\n\ndef solve():\n    \"\"\"\n    Solves the regression splines problem for the specified test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (200, ('uniform',), 0.5, 0.05, 6, 1.5),\n        (200, ('beta', 2, 5), 0.8, 0.05, 6, 1.5),\n        (80,  ('uniform',), 0.2, 0.1, 4, 2.0),\n        (100, ('beta', 0.5, 0.5), 0.5, 0.05, 6, 2.0),\n        (50,  ('uniform',), 0.9, 0.2, 3, 1.0),\n    ]\n\n    all_results = []\n    rng = np.random.default_rng(42)\n\n    for case in test_cases:\n        n, dist_info, x0, sigma, K, alpha = case\n        \n        # 1. Simulate data\n        dist_name = dist_info[0]\n        if dist_name == 'uniform':\n            x = rng.uniform(0, 1, size=n)\n        elif dist_name == 'beta':\n            a, b = dist_info[1], dist_info[2]\n            x = rng.beta(a, b, size=n)\n        \n        def hinge(u):\n            return np.maximum(0, u)\n\n        def true_f(x_vals, x0_kink, alpha_kink):\n            return 0.5 * np.sin(4 * np.pi * x_vals) + alpha_kink * hinge(x_vals - x0_kink)\n\n        noise = rng.normal(loc=0, scale=sigma, size=n)\n        y = true_f(x, x0, alpha) + noise\n        \n        f_x0_true = true_f(np.array([x0]), x0, alpha)[0]\n\n        case_biases = []\n\n        # 2. Define knot strategies\n        # Strategy 1: Equally spaced knots\n        knots_eq = np.linspace(0, 1, K + 2)[1:-1]\n        \n        # Strategy 2: Empirical quantile knots\n        probs = np.linspace(0, 1, K + 2)[1:-1]\n        knots_qt = np.quantile(x, probs)\n        \n        # Enforce strictly increasing knots for quantile strategy\n        knots_qt_sorted = np.sort(knots_qt)\n        for i in range(len(knots_qt_sorted) - 1):\n            if knots_qt_sorted[i+1] == knots_qt_sorted[i]:\n                knots_qt_sorted[i+1] = knots_qt_sorted[i] + 1e-8\n        \n        knot_strategies = [knots_eq, knots_qt_sorted]\n\n        # 3. Loop through strategies, fit model, and compute bias\n        for knots in knot_strategies:\n            # Construct design matrix\n            num_params = 4 + K\n            X = np.zeros((n, num_params))\n            X[:, 0] = 1\n            X[:, 1] = x\n            X[:, 2] = x**2\n            X[:, 3] = x**3\n            for j in range(K):\n                X[:, j + 4] = hinge(x - knots[j])**3\n            \n            # Estimate coefficients using LS\n            beta_hat, _, _, _ = linalg.lstsq(X, y, cond=None)\n            \n            # Predict at x0\n            x0_basis = np.zeros(num_params)\n            x0_basis[0] = 1\n            x0_basis[1] = x0\n            x0_basis[2] = x0**2\n            x0_basis[3] = x0**3\n            for j in range(K):\n                x0_basis[j + 4] = hinge(np.array([x0 - knots[j]]))**3\n            \n            f_x0_hat = x0_basis @ beta_hat\n            \n            # Compute absolute local bias\n            bias = f_x0_hat - f_x0_true\n            case_biases.append(np.abs(bias))\n            \n        all_results.append(case_biases)\n\n    # Final print statement in the exact required format.\n    result_str = ','.join([f\"[{res[0]},{res[1]}]\" for res in all_results])\n    print(f\"[{result_str}]\")\n\nsolve()\n```", "id": "3168933"}, {"introduction": "如前一个练习所示，手动放置节点可能颇具挑战性。一种更高级的方法是让数据自行决定哪些节点是重要的。本练习 [@problem_id:3168944] 介绍了一种强大的技术，它利用 $\\ell_1$ 惩罚（类似于 LASSO）从一个庞大的候选列表中自动选择一个稀疏的活动节点集。从零开始实现这一过程，将使您对样条与现代稀疏建模方法之间的联系有更深刻的认识。", "problem": "考虑通过截断幂基构建的三次回归样条。对于单变量预测变量 $x \\in [0,1]$，通过拼接多项式部分和截断部分来定义设计矩阵 $\\mathbf{X} \\in \\mathbb{R}^{n \\times p}$，具体如下：前四列是多项式基 $1$、$x$、$x^2$、$x^3$，对于一组候选节点 $\\{\\xi_k\\}_{k=1}^K$，其余 $K$ 列是 $(x - \\xi_k)_+^3$，其中 $(t)_+ = \\max(t,0)$。给定响应向量 $\\mathbf{y} \\in \\mathbb{R}^n$ 和系数 $\\boldsymbol{\\beta} \\in \\mathbb{R}^p$，考虑一个只惩罚截断基系数的优化问题：\n$$\n\\min_{\\boldsymbol{\\beta} \\in \\mathbb{R}^p} \\ \\left\\| \\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta} \\right\\|_2^2 \\ + \\ \\lambda \\left\\| \\boldsymbol{\\beta}_{\\text{trunc}} \\right\\|_1,\n$$\n其中 $\\boldsymbol{\\beta}_{\\text{trunc}}$ 表示与截断基列相关联的系数向量，$\\lambda \\ge 0$ 是一个调整参数。多项式部分的系数不被惩罚。\n\n您的任务是实现一个程序，该程序能够：\n- 使用具有已知活动节点的立方样条构建合成数据集，添加高斯噪声，并建立截断幂基设计矩阵。\n- 从第一性原理出发，使用基于次梯度微积分和最小二乘法正规方程的坐标级优化方法，解决上述优化问题，而不依赖于预封装的求解器。\n- 对于惩罚参数 $\\lambda$ 的每个指定值，确定哪些候选节点在优化后仍然是活动的，即在数值阈值 $\\tau$ 下，相应的截断基系数为非零。\n\n数据生成和基函数规范：\n- 使用固定的伪随机种子 $42$，从 $[0,1]$ 上独立且均匀地生成 $n = 200$ 个点 $x_i$。\n- 定义真实的回归函数\n$$\nf(x) \\ = \\ 0.5 \\ + \\ 2x \\ - \\ x^2 \\ + \\ 1.5 \\, (x - 0.3)_+^3 \\ - \\ 1.0 \\, (x - 0.7)_+^3.\n$$\n- 生成响应 $y_i = f(x_i) + \\varepsilon_i$，其中噪声 $\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma^2)$ 是独立的高斯噪声，且 $\\sigma = 0.05$。\n- 使用候选节点集 $\\{\\xi_k\\}_{k=1}^K$，其中 $K = 6$ 且 $\\xi_k \\in \\{0.2, 0.3, 0.4, 0.5, 0.6, 0.7\\}$。\n\n稀疏性与剪枝标准：\n- 解出 $\\boldsymbol{\\beta}$ 后，如果 $|\\beta_j| \\ge \\tau$（其中 $\\tau = 10^{-6}$），则声明一个截断基系数 $\\beta_j$ 为活动的。\n- 报告每个测试用例中活动的截断基函数在候选节点列表中的索引（从零开始）。\n\n测试套件：\n- 对以下惩罚值 $\\lambda \\in \\{\\ 0, \\ 0.1, \\ 1.0, \\ 100.0 \\ \\}$ 求解优化问题，这些值包括一个边界情况（$\\lambda = 0$）、一个小惩罚、一个中等惩罚以及一个强烈鼓励稀疏性的大惩罚。\n\n输出规范：\n- 您的程序应生成一行输出，其中包含一个用方括号括起来的逗号分隔列表。每个条目按顺序对应于测试套件中的一个 $\\lambda$ 值，其本身是一个整数列表，表示候选集内活动节点的从零开始的索引。例如，一个包含四个测试用例的输出应如下所示：$[\\,[i_{1,1},i_{1,2},\\dots],\\,[i_{2,1},\\dots],\\,[\\dots],\\,[\\dots]\\,]$。", "solution": "我们从标准的线性回归设置开始，其中有一个固定的设计矩阵 $\\mathbf{X} \\in \\mathbb{R}^{n \\times p}$ 和响应向量 $\\mathbf{y} \\in \\mathbb{R}^n$。目标是最小化关于 $\\boldsymbol{\\beta} \\in \\mathbb{R}^p$ 的一个带惩罚的目标函数：\n$$\nJ(\\boldsymbol{\\beta}) \\ = \\ \\left\\| \\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta} \\right\\|_2^2 \\ + \\ \\lambda \\left\\| \\boldsymbol{\\beta}_{\\text{trunc}} \\right\\|_1,\n$$\n其中只有截断基的系数被惩罚。多项式部分（对应于 $1$、$x$、$x^2$、$x^3$ 的前四列）保持不被惩罚。三次回归样条的截断幂基使用函数 $(x - \\xi_k)_+^3$（对于候选节点 $\\xi_k$）；众所周知，该基可以张成在指定位置有节点的三次样条空间。\n\n为从第一性原理推导算法，我们使用两个基本依据：\n- 针对固定系数子集的最小二乘回归正规方程：在保持其他系数固定的情况下，关于单个坐标 $\\beta_j$ 最小化 $\\left\\| \\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta} \\right\\|_2^2$ 的问题可以简化为一个一维二次最小化问题。\n- 针对绝对值惩罚的次梯度微积分。对于标量参数 $\\beta_j$，当 $\\beta_j \\ne 0$ 时， $|\\beta_j|$ 的次梯度为 $\\partial |\\beta_j| = \\{\\ \\text{sgn}(\\beta_j)\\ \\}$，而在 $\\beta_j = 0$ 时为区间 $[-1,1]$。\n\n考虑坐标级优化。固定除 $\\beta_j$ 之外的所有系数，并定义偏残差\n$$\n\\mathbf{r}_j \\ = \\ \\mathbf{y} \\ - \\sum_{k \\ne j} \\mathbf{X}_{\\cdot k} \\beta_k \\ = \\ \\mathbf{y} \\ - \\mathbf{X}\\boldsymbol{\\beta} \\ + \\ \\mathbf{X}_{\\cdot j} \\beta_j,\n$$\n其中 $\\mathbf{X}_{\\cdot j}$是 $\\mathbf{X}$ 的第 $j$ 列。仅限于 $\\beta_j$ 的目标函数变为\n$$\n\\phi_j(\\beta_j) \\ = \\ \\left\\| \\mathbf{r}_j \\ - \\ \\mathbf{X}_{\\cdot j} \\beta_j \\right\\|_2^2 \\ + \\ \\lambda \\, p_j \\, |\\beta_j|,\n$$\n其中 $p_j \\in \\{0,1\\}$ 表示坐标 $j$ 是否被惩罚（对于截断部分，$p_j=1$；对于多项式部分，$p_j=0$）。展开平方项，\n$$\n\\left\\| \\mathbf{r}_j - \\mathbf{X}_{\\cdot j} \\beta_j \\right\\|_2^2 \\ = \\ \\mathbf{r}_j^\\top \\mathbf{r}_j \\ - \\ 2 \\beta_j \\, \\mathbf{X}_{\\cdot j}^\\top \\mathbf{r}_j \\ + \\ \\beta_j^2 \\, \\mathbf{X}_{\\cdot j}^\\top \\mathbf{X}_{\\cdot j}.\n$$\n对于 $\\beta_j \\ne 0$ 的次梯度最优性条件是\n$$\n\\frac{\\partial}{\\partial \\beta_j} \\left\\| \\mathbf{r}_j - \\mathbf{X}_{\\cdot j} \\beta_j \\right\\|_2^2 \\ + \\ \\lambda \\, p_j \\, \\text{sgn}(\\beta_j) \\ = \\ 0,\n$$\n这得到\n$$\n- 2 \\mathbf{X}_{\\cdot j}^\\top \\mathbf{r}_j \\ + \\ 2 \\left( \\mathbf{X}_{\\cdot j}^\\top \\mathbf{X}_{\\cdot j} \\right) \\beta_j \\ + \\ \\lambda \\, p_j \\, \\text{sgn}(\\beta_j) \\ = \\ 0.\n$$\n重新整理后，\n$$\n2 \\left( \\mathbf{X}_{\\cdot j}^\\top \\mathbf{X}_{\\cdot j} \\right) \\beta_j \\ = \\ 2 \\mathbf{X}_{\\cdot j}^\\top \\mathbf{r}_j \\ - \\ \\lambda \\, p_j \\, \\text{sgn}(\\beta_j).\n$$\n这导出了软阈值解\n$$\n\\beta_j^\\star \\ = \\ \\frac{S\\left( \\mathbf{X}_{\\cdot j}^\\top \\mathbf{r}_j, \\ \\frac{\\lambda \\, p_j}{2} \\right)}{\\mathbf{X}_{\\cdot j}^\\top \\mathbf{X}_{\\cdot j}},\n$$\n其中 $S(a,t)$ 是软阈值算子 $S(a,t) = \\text{sgn}(a)\\,\\max\\left(|a| - t, \\ 0\\right)$。当 $p_j = 0$（未惩罚的坐标）时，阈值 $t$ 为 $0$，我们恢复到最小二乘更新\n$$\n\\beta_j^\\star \\ = \\ \\frac{\\mathbf{X}_{\\cdot j}^\\top \\mathbf{r}_j}{\\mathbf{X}_{\\cdot j}^\\top \\mathbf{X}_{\\cdot j}}.\n$$\n当 $\\beta_j^\\star = 0$ 时，只要 $|\\mathbf{X}_{\\cdot j}^\\top \\mathbf{r}_j| \\le \\frac{\\lambda p_j}{2}$，在零点的次梯度条件就得到满足，这解释了增加 $\\lambda$ 如何促进被惩罚坐标的稀疏性。\n\n算法设计：\n- 将 $\\boldsymbol{\\beta}$ 初始化为零向量。\n- 迭代坐标 $j = 1, \\dots, p$。对每个坐标，计算 $\\mathbf{r}_j = \\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta} + \\mathbf{X}_{\\cdot j}\\beta_j$，评估 $\\mathbf{X}_{\\cdot j}^\\top \\mathbf{r}_j$ 和 $\\mathbf{X}_{\\cdot j}^\\top \\mathbf{X}_{\\cdot j}$，并使用上述软阈值表达式更新 $\\beta_j$。\n- 重复遍历，直到收敛，例如，当 $\\boldsymbol{\\beta}$ 的最大绝对变化小于一个容忍度 $\\varepsilon$。\n\n这种坐标下降法直接从正规方程和次梯度微积分推导而来，体现了最小绝对收缩和选择算子（LASSO）的原理，但仅应用于截断部分。收敛后，我们通过检查截断系数的量级与数值阈值 $\\tau$ 的大小来识别活动的节点。\n\n合成数据构建：\n- 从 $\\text{Uniform}([0,1])$ 中抽取 $x_i$，其中 $n = 200$，种子为 $42$。\n- 定义真实函数 $f(x) = 0.5 + 2x - x^2 + 1.5(x - 0.3)_+^3 - 1.0(x - 0.7)_+^3$。\n- 生成 $y_i = f(x_i) + \\varepsilon_i$，其中 $\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma^2)$ 且 $\\sigma = 0.05$。\n- 从多项式列和对应于节点 $\\xi_k \\in \\{0.2, 0.3, 0.4, 0.5, 0.6, 0.7\\}$ 的截断列构建 $\\mathbf{X}$。\n\n测试套件与预期行为：\n- 当 $\\lambda = 0$ 时，解简化为普通最小二乘法，通常会导致许多非零的截断系数，因为没有诱导稀疏性的惩罚；一些系数可能由于多重共线性而很小，但没有惩罚意味着没有故意的剪枝。\n- 当 $\\lambda = 0.1$ 和 $\\lambda = 1.0$ 时，软阈值操作会逐渐将较小的截断系数归零，从而鼓励稀疏性并剪除数据中支持较弱的节点。\n- 当 $\\lambda = 100.0$ 时，阈值 $\\lambda/2 = 50.0$ 如此之大，以至于截断系数被强烈地推向零，最终只留下一个纯多项式拟合（所有节点都被剪除）。\n\n最后，对于测试套件中的每个 $\\lambda$，我们输出候选节点列表中相应截断系数为活动（$|\\beta_j| \\ge \\tau$）的节点的从零开始的索引列表。单行输出简洁地总结了不同惩罚水平下的稀疏模式和节点剪枝情况。", "answer": "```python\nimport numpy as np\n\ndef soft_threshold(a, t):\n    if a > t:\n        return a - t\n    elif a  -t:\n        return a + t\n    else:\n        return 0.0\n\ndef coordinate_descent_partial_l1(X, y, penalty_mask, lam, max_iter=1000, tol=1e-8):\n    \"\"\"\n    Solve min ||y - X beta||_2^2 + lam * ||beta_trunc||_1\n    where penalty_mask is 1 for penalized coordinates (truncated basis) and 0 for unpenalized (polynomial part).\n    Uses coordinate descent with soft-thresholding derived from subgradient conditions.\n    \"\"\"\n    n, p = X.shape\n    beta = np.zeros(p)\n    # Precompute column norms (X^T X diagonal)\n    col_sq_norms = (X ** 2).sum(axis=0)\n    for _ in range(max_iter):\n        beta_old = beta.copy()\n        for j in range(p):\n            # Partial residual r_j = y - X beta + X_j * beta_j\n            r_j = y - X @ beta + X[:, j] * beta[j]\n            z = X[:, j].dot(r_j)\n            Hjj = col_sq_norms[j]\n            t = (lam / 2.0) * penalty_mask[j]\n            # Update coordinate with soft-thresholding\n            beta[j] = soft_threshold(z, t) / (Hjj if Hjj > 0 else 1.0)\n        if np.max(np.abs(beta - beta_old))  tol:\n            break\n    return beta\n\ndef build_design_matrix(x, knots):\n    \"\"\"\n    Build cubic truncated power basis: [1, x, x^2, x^3, (x - xi)_+^3 for xi in knots]\n    \"\"\"\n    n = x.shape[0]\n    poly = np.stack([np.ones(n), x, x**2, x**3], axis=1)\n    trunc = np.stack([np.maximum(x - xi, 0.0)**3 for xi in knots], axis=1) if len(knots) > 0 else np.empty((n, 0))\n    X = np.concatenate([poly, trunc], axis=1)\n    return X\n\ndef generate_data(n=200, seed=42, sigma=0.05):\n    rng = np.random.RandomState(seed)\n    x = rng.uniform(0.0, 1.0, size=n)\n    f = 0.5 + 2.0*x - 1.0*(x**2) + 1.5*np.maximum(x - 0.3, 0.0)**3 - 1.0*np.maximum(x - 0.7, 0.0)**3\n    y = f + rng.normal(0.0, sigma, size=n)\n    return x, y\n\ndef active_knots_indices(beta, num_poly, tau=1e-6):\n    \"\"\"\n    Return zero-based indices of active truncated coefficients relative to the knot list.\n    \"\"\"\n    trunc_beta = beta[num_poly:]\n    return [i for i, b in enumerate(trunc_beta) if abs(b) >= tau]\n\ndef solve():\n    # Parameters\n    n = 200\n    seed = 42\n    sigma = 0.05\n    knots = [0.2, 0.3, 0.4, 0.5, 0.6, 0.7]\n    num_poly = 4\n    lambdas = [0.0, 0.1, 1.0, 100.0]\n    tau = 1e-6\n\n    # Generate data and design matrix\n    x, y = generate_data(n=n, seed=seed, sigma=sigma)\n    X = build_design_matrix(x, knots)\n\n    # Penalty mask: 0 for polynomial part, 1 for truncated part\n    penalty_mask = np.array([0]*num_poly + [1]*len(knots), dtype=float)\n\n    results = []\n    for lam in lambdas:\n        beta = coordinate_descent_partial_l1(X, y, penalty_mask, lam, max_iter=2000, tol=1e-9)\n        active = active_knots_indices(beta, num_poly=num_poly, tau=tau)\n        results.append(active)\n\n    # Print in the required single-line format\n    print(f\"[{','.join(f'[{\",\".join(map(str, res))}]' for res in results)}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3168944"}]}