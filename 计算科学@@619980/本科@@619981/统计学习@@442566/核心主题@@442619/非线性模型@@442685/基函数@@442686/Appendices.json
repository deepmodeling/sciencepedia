{"hands_on_practices": [{"introduction": "我们通常会从一组直观的基函数开始，比如单项式基 $\\{1, x, x^2, \\dots\\}$。然而，这些基函数并非相互正交，这可能导致数值计算和模型解释上的困难。本练习 [@problem_id:2161554] 将引导你亲手应用Gram-Schmidt过程，这是一个从非正交基集合构造正交基集合的基本算法，为后续更稳健的建模技术奠定基础。", "problem": "在数值分析和逼近理论中，从一个更简单的非正交基集合构造一组正交基函数通常很有用。考虑在区间 $[0, 1]$ 上连续的实值函数空间。在此空间中，两个函数 $f(x)$ 和 $g(x)$ 的内积定义为：\n$$ \\langle f, g \\rangle = \\int_{0}^{1} f(x)g(x) \\, dx $$\n如果两个函数的内积为零，则认为它们是正交的。\n\n从初始的非正交基函数集 $\\{v_1(x), v_2(x)\\}$（其中 $v_1(x) = 1$ 和 $v_2(x) = x$）开始，通过应用以下过程构造一个新的正交基函数集 $\\{u_1(x), u_2(x)\\}$：\n1.  将第一个正交函数设置为与第一个初始函数相同：$u_1(x) = v_1(x)$。\n2.  通过从 $v_2(x)$ 中减去其在 $u_1(x)$ 上的投影来构造第二个正交函数：\n    $$ u_2(x) = v_2(x) - \\frac{\\langle v_2, u_1 \\rangle}{\\langle u_1, u_1 \\rangle} u_1(x) $$\n\n求所得的正交函数 $u_1(x)$ 和 $u_2(x)$。将你的答案表示为一对函数。", "solution": "我们在 $[0,1]$ 上的连续实值函数空间中进行计算，其内积为 $\\langle f,g\\rangle=\\int_{0}^{1}f(x)g(x)\\,dx$。初始集合为 $v_{1}(x)=1$ 和 $v_{2}(x)=x$。\n\n步骤1：根据给定过程，设置 $u_{1}(x)=v_{1}(x)=1$。\n\n步骤2：计算 $v_{2}$ 在 $u_{1}$ 上的投影系数：\n$$\n\\langle v_{2},u_{1}\\rangle=\\int_{0}^{1}x\\cdot 1\\,dx=\\int_{0}^{1}x\\,dx=\\frac{1}{2},\n\\qquad\n\\langle u_{1},u_{1}\\rangle=\\int_{0}^{1}1\\cdot 1\\,dx=\\int_{0}^{1}1\\,dx=1.\n$$\n因此，\n$$\nu_{2}(x)=v_{2}(x)-\\frac{\\langle v_{2},u_{1}\\rangle}{\\langle u_{1},u_{1}\\rangle}u_{1}(x)\n=x-\\frac{\\frac{1}{2}}{1}\\cdot 1=x-\\frac{1}{2}.\n$$\n\n验证正交性：\n$$\n\\langle u_{1},u_{2}\\rangle=\\int_{0}^{1}1\\cdot\\left(x-\\frac{1}{2}\\right)\\,dx=\\int_{0}^{1}x\\,dx-\\frac{1}{2}\\int_{0}^{1}1\\,dx=\\frac{1}{2}-\\frac{1}{2}=0,\n$$\n所以 $u_{1}$ 和 $u_{2}$ 是正交的。因此，正交函数对为 $u_{1}(x)=1$ 和 $u_{2}(x)=x-\\frac{1}{2}$。", "answer": "$$\\boxed{\\begin{pmatrix}1 & x-\\frac{1}{2}\\end{pmatrix}}$$", "id": "2161554"}, {"introduction": "在正交性概念的基础上，本练习旨在解决一个关键的实际问题：数值稳定性。在回归任务中，我们为什么应该优先选择正交多项式而不是简单的单项式基？本练习 [@problem_id:3102236] 邀请你编写代码来直接比较这两种基函数，通过实践展示即使在输入数据被缩放或平移（这在数据分析中很常见）的情况下，正交基如何能够提供稳定且可预测的结果。", "problem": "在统计学习的背景下，考虑使用线性基展开的监督回归。设一个数据集由输入 $x_i$ 和目标 $t_i$ 组成（$i=1,\\dots,n$），并考虑一个线性模型 $m(x) = \\sum_{j=0}^{d} w_j \\,\\phi_j(x)$，其中基函数 $\\phi_j$ 是固定的。最小二乘估计量最小化残差平方和，并且可以使用经过良好测试的数值线性代数方法，从一个设计矩阵 $X$（其条目为 $X_{ij} = \\phi_j(x_i)$）计算得出。矩阵的2-范数条件数，记作 $\\kappa_2(X)$，量化了求解以 $X$ 为系数矩阵的线性系统的数值稳定性。\n\n本问题研究在仿射变换 $x \\mapsto y = a x + b$ 下，输入重缩放对两族基函数的影响：单项式（范德蒙）基和正交多项式基。单项式基由 $\\phi_j(x) = x^j$ 定义（$j=0,\\dots,d$），其设计矩阵 $V$ 是范德蒙矩阵。此处考虑的正交多项式基是勒让德多项式族 $\\{P_j(z)\\}_{j=0}^{d}$，它在标准化输入 $z$ 上求值。标准化通过以下方式将观测到的输入范围映射到区间 $[-1,1]$ 来执行：\n$$\nz = \\frac{2(x - m_x)}{r_x}, \\quad m_x = \\frac{x_{\\min} + x_{\\max}}{2}, \\quad r_x = x_{\\max} - x_{\\min},\n$$\n对重缩放后的输入 $y$ 也进行类似操作：\n$$\nz' = \\frac{2(y - m_y)}{r_y}, \\quad m_y = \\frac{y_{\\min} + y_{\\max}}{2}, \\quad r_y = y_{\\max} - y_{\\min}.\n$$\n勒让德多项式满足奇偶性 $P_j(-z) = (-1)^j P_j(z)$。\n\n从上述核心定义出发，您将推导用于检验以下论断的算法测试：\n- 正交多项式基的不变性论断：当设计矩阵由标准化输入构建时，通过最小二乘法计算出的系数在 $x \\mapsto y = a x + b$ 变换下是不变的，除了当 $a < 0$ 时有一个可预测的奇偶性调整。\n- 范德蒙基的敏感性论断：单项式基的系数和设计矩阵的条件数对缩放因子 $a$ 和平移量 $b$ 很敏感，通常会导致巨大的变化。\n\n您必须实现一个程序，对下面指定的每个测试用例，按纯数学术语执行以下步骤：\n1. 在指定的区间 $[x_{\\min}, x_{\\max}]$ 上生成均匀间隔的输入 $x_i$。\n2. 计算目标 $t_i = f(x_i)$，其中 $f(x) = \\sin(x)$，角度单位为弧度。\n3. 使用正交多项式基 $\\{P_j(z)\\}_{j=0}^{d}$，在由 $x$ 构建的标准化输入 $z$ 上拟合一个 $d$ 次模型，得到系数 $w^{\\mathrm{leg}}$。然后应用重缩放 $y_i = a x_i + b$，从 $y$ 构建新的标准化输入 $z'$，并重新拟合以获得系数 $w'^{\\mathrm{leg}}$。\n4. 通过检查 $w'^{\\mathrm{leg}}$ 是否等于 $w^{\\mathrm{leg}}$（除了第 $j$ 个系数上有一个奇偶性调整 $(-\\operatorname{sign}(a))^j$），即 $\\max_j \\left| w'^{\\mathrm{leg}}_j - \\left(\\operatorname{sign}(a)\\right)^j w^{\\mathrm{leg}}_j \\right|$ 是否低于容差 $\\epsilon$，来检验不变性论断。\n5. 使用单项式基，在原始输入 $x$ 上拟合一个 $d$ 次模型，得到系数 $w^{\\mathrm{van}}$，然后在重缩放后的输入 $y$ 上拟合，得到 $w'^{\\mathrm{van}}$。计算相对系数变化\n$$\n\\Delta = \\frac{\\left\\| w'^{\\mathrm{van}} - w^{\\mathrm{van}} \\right\\|_2}{\\left\\| w^{\\mathrm{van}} \\right\\|_2 + 10^{-12}},\n$$\n和条件数比率\n$$\n\\rho = \\frac{\\kappa_2(V_y)}{\\kappa_2(V_x)},\n$$\n其中 $V_x$ 和 $V_y$ 分别是基于 $x$ 和 $y$ 的范德蒙设计矩阵。\n6. 为每个测试用例返回一个列表 $[B, \\Delta, \\rho]$，其中 $B$ 是步骤4中不变性测试的布尔结果。\n\n使用标准的2-范数目标函数进行最小二乘计算，并使用2-范数计算条件数。在不变性测试中使用容差 $\\epsilon = 10^{-9}$。正弦函数的角度必须以弧度为单位。本问题中除弧度外，不出现其他物理单位。\n\n测试套件：\n- 情况 1：$n = 50$，次数 $d = 10$，$x_{\\min} = -3$，$x_{\\max} = 3$，$a = 2$，$b = 1$。\n- 情况 2：$n = 6$，次数 $d = 5$，$x_{\\min} = -1$，$x_{\\max} = 1$，$a = -1$，$b = 0$。\n- 情况 3：$n = 50$，次数 $d = 10$，$x_{\\min} = -1$，$x_{\\max} = 2$，$a = 100$，$b = -5$。\n- 情况 4：$n = 50$，次数 $d = 10$，$x_{\\min} = -3$，$x_{\\max} = 3$，$a = 0.01$，$b = 100$。\n- 情况 5：$n = 50$，次数 $d = 10$，$x_{\\min} = -2$，$x_{\\max} = 4$，$a = 1$，$b = 10$。\n\n您的程序应生成一行输出，其中包含一个由方括号括起来的逗号分隔列表，每个测试用例一个元素。每个元素本身必须是一个形式为 $[B,\\Delta,\\rho]$ 的列表。例如，一个有效的输出格式是 $[[\\text{True},0.1,2.0],[\\text{False},3.5,100.0]]$。", "solution": "该问题是有效的，因为它在科学上基于数值线性代数和统计回归的原理，问题提出得很好，提供了所有必要的信息，并且其表述是客观的。它提出了对两种常见基函数选择的标准、可验证的比较。\n\n问题的核心在于分析线性模型在输入变量经过仿射变换 $x \\mapsto y = a x + b$ 后的数值稳定性和系数不变性。我们研究两种类型的基展开：一种使用标准化域上的正交多项式（勒让德多项式），另一种使用朴素的单项式基。\n\n**1. 正交多项式基：勒让德多项式**\n\n勒让德多项式基的稳定性关键在于输入变量的标准化。对于任何在区间 $[v_{\\min}, v_{\\max}]$上定义的变量 $v$，其标准化映射为：\n$$\nz(v) = \\frac{2(v - m_v)}{r_v}, \\quad \\text{其中 } m_v = \\frac{v_{\\min} + v_{\\max}}{2} \\text{ and } r_v = v_{\\max} - v_{\\min}.\n$$\n此变换将区间 $[v_{\\min}, v_{\\max}]$ 映射到 $[-1, 1]$，这是勒让德多项式的标准正交域。然后，模型使用基函数 $\\phi_j(x) = P_j(z(x))$ 构建。\n\n让我们分析仿射变换 $y = a x + b$ 对标准化变量的影响。原始输入 $x_i$ 位于 $[x_{\\min}, x_{\\max}]$。重缩放后的输入 $y_i$ 将位于一个新的区间 $[y_{\\min}, y_{\\max}]$。\n\n情况 1：$a > 0$。变换是保序的。\n$y_{\\min} = a x_{\\min} + b$ 且 $y_{\\max} = a x_{\\max} + b$。\n新的中点是 $m_y = \\frac{(a x_{\\min} + b) + (a x_{\\max} + b)}{2} = a \\frac{x_{\\min} + x_{\\max}}{2} + b = a m_x + b$。\n新的范围是 $r_y = (a x_{\\max} + b) - (a x_{\\min} + b) = a (x_{\\max} - x_{\\min}) = a r_x$。\n新的标准化变量 $z'$ 是：\n$$\nz'(y) = \\frac{2(y - m_y)}{r_y} = \\frac{2((ax+b) - (am_x+b))}{a r_x} = \\frac{2a(x - m_x)}{a r_x} = z(x).\n$$\n由于 $z'(y_i) = z(x_i)$，在新点上求值的基函数与旧的基函数完全相同：$\\phi_j(y_i) = P_j(z'(y_i)) = P_j(z(x_i)) = \\phi_j(x_i)$。因此，两次拟合的设计矩阵是相同的。由于目标向量 $\\mathbf{t}$ 保持不变，系数的最小二乘解也必须相同：$w'^{\\mathrm{leg}} = w^{\\mathrm{leg}}$。\n\n情况 2：$a < 0$。变换是逆序的。\n$y_{\\min} = a x_{\\max} + b$ 且 $y_{\\max} = a x_{\\min} + b$。\n中点 $m_y = a m_x + b$ 的推导类似。\n新的范围是 $r_y = (a x_{\\min} + b) - (a x_{\\max} + b) = a (x_{\\min} - x_{\\max}) = -a (x_{\\max} - x_{\\min}) = |a| r_x$。\n新的标准化变量 $z'$ 是：\n$$\nz'(y) = \\frac{2(y - m_y)}{r_y} = \\frac{2((ax+b) - (am_x+b))}{|a| r_x} = \\frac{a}{|a|} \\frac{2(x - m_x)}{r_x} = -z(x), \\text{ 因为 } a < 0 \\implies a/|a| = -1.\n$$\n新拟合的基函数是 $\\phi_j(y_i) = P_j(z'(y_i)) = P_j(-z(x_i))$。利用勒让德多项式的奇偶性 $P_j(-z) = (-1)^j P_j(z)$，我们得到 $\\phi_j(y_i) = (-1)^j P_j(z(x_i))$。\n令 $X^{\\mathrm{leg}}$ 为基于 $x$ 拟合的设计矩阵，其元素为 $(X^{\\mathrm{leg}})_{ij} = P_j(z(x_i))$。令 $X'^{\\mathrm{leg}}$ 为基于 $y$ 拟合的设计矩阵，其元素为 $(X'^{\\mathrm{leg}})_{ij} = P_j(z'(y_i))$。它们之间的关系是 $(X'^{\\mathrm{leg}})_{ij} = (-1)^j (X^{\\mathrm{leg}})_{ij}$。这可以写成矩阵形式 $X'^{\\mathrm{leg}} = X^{\\mathrm{leg}} S$，其中 $S$ 是一个对角矩阵，且 $S_{jj} = (-1)^j$。\n\n两个最小二乘问题分别是找到 $w^{\\mathrm{leg}}$ 和 $w'^{\\mathrm{leg}}$ 以最小化 $\\| X^{\\mathrm{leg}} w^{\\mathrm{leg}} - \\mathbf{t} \\|_2^2$ 和 $\\| X'^{\\mathrm{leg}} w'^{\\mathrm{leg}} - \\mathbf{t} \\|_2^2$。将矩阵间的关系代入第二个问题，得到 $\\| (X^{\\mathrm{leg}} S) w'^{\\mathrm{leg}} - \\mathbf{t} \\|_2^2$。令 $w^* = S w'^{\\mathrm{leg}}$。问题变为找到 $w^*$ 以最小化 $\\| X^{\\mathrm{leg}} w^* - \\mathbf{t} \\|_2^2$。根据最小二乘解的唯一性，$w^* = w^{\\mathrm{leg}}$。因此，$S w'^{\\mathrm{leg}} = w^{\\mathrm{leg}}$。由于 $S$ 是其自身的逆矩阵（$S^2=I$），我们可以解出 $w'^{\\mathrm{leg}}$: $w'^{\\mathrm{leg}} = S^{-1} w^{\\mathrm{leg}} = S w^{\\mathrm{leg}}$，这意味着 $w'^{\\mathrm{leg}}_j = (-1)^j w^{\\mathrm{leg}}_j$。\n\n使用 $\\operatorname{sign}(a)$ 结合两种情况，我们发现 $w'^{\\mathrm{leg}}_j = (\\operatorname{sign}(a))^j w^{\\mathrm{leg}}_j$。不变性测试在数值容差 $\\epsilon=10^{-9}$ 内检验此关系。布尔值 $B$ 是该测试的结果。\n\n**2. 单项式基：范德蒙矩阵**\n\n单项式基由 $\\phi_j(x) = x^j$ 定义。设计矩阵 $V_x$ 的元素为 $(V_x)_{ij} = x_i^j$，它是一个范德蒙矩阵。与正交基不同，该基没有自我修正的标准化机制。\n\n当输入变换为 $y = ax+b$ 时，新的基函数为 $\\phi_j(y) = (ax+b)^j$。使用二项式展开：\n$$\n\\phi_j(y) = (ax+b)^j = \\sum_{k=0}^{j} \\binom{j}{k} (ax)^k b^{j-k} = \\sum_{k=0}^{j} \\left[ \\binom{j}{k} a^k b^{j-k} \\right] x^k = \\sum_{k=0}^{j} C_{jk} \\phi_k(x)\n$$\n每个新的基函数都是旧基函数直至相同次数的线性组合。这在系数向量 $w^{\\mathrm{van}}$ 和 $w'^{\\mathrm{van}}$ 之间产生了一种复杂的关系，排除了任何简单的不变性。相对变化 $\\Delta = \\frac{\\| w'^{\\mathrm{van}} - w^{\\mathrm{van}} \\|_2}{\\| w^{\\mathrm{van}} \\|_2 + 10^{-12}}$ 预计会很大。\n\n范德蒙矩阵是出了名的病态，特别是当点的区间远离原点或具有非常大/小的尺度时。矩阵的列，作为 $x_i$ 的幂，可能变得几乎共线。例如，如果所有 $|x_i| \\gg 1$，向量 $[x_i^d]$ 和 $[x_i^{d-1}]$ 将指向非常相似的方向，导致高的条件数 $\\kappa_2(V_x)$。变换 $y=ax+b$ 可以极大地改变输入域的尺度和位置，常常加剧此问题。条件数的比率 $\\rho = \\frac{\\kappa_2(V_y)}{\\kappa_2(V_x)}$ 量化了这种不稳定性。大的 $|a|$ 或 $|b|$ 值预计会产生大的 $\\rho$ 值。\n\n**3. 算法实现**\n\n上述分析按如下方式实现：\n- 对每个测试用例，我们生成 $n$ 个输入 $x_i$ 和目标 $t_i = \\sin(x_i)$。\n- 对于勒让德基，我们首先分别对 $x_i$ 和重缩放后的 $y_i$ 输入进行标准化，然后计算勒让德多项式 $P_j$（$j=0, \\dots, d$）的值，从而构建设计矩阵 $X^{\\mathrm{leg}}$ 和 $X'^{\\mathrm{leg}}$。系数 $w^{\\mathrm{leg}}$ 和 $w'^{\\mathrm{leg}}$ 使用 `numpy.linalg.lstsq` 求得。检验不变性论断，得到布尔值 $B$。\n- 对于单项式基，我们使用 `numpy.vander` 构建范德蒙矩阵 $V_x$ 和 $V_y$。系数 $w^{\\mathrm{van}}$ 和 $w'^{\\mathrm{van}}$ 再次通过最小二乘法求得。计算相对系数变化 $\\Delta$ 和条件数比率 $\\rho$（使用 `numpy.linalg.cond`）。\n- 对所有测试用例收集结果 $[B, \\Delta, \\rho]$，并按指定格式输出。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import legendre\n\ndef run_case(n, d, x_min, x_max, a, b):\n    \"\"\"\n    Performs the calculations for a single test case.\n    \"\"\"\n    # Step 1: Generate evenly spaced inputs and compute targets\n    x = np.linspace(x_min, x_max, n, dtype=np.float64)\n    t = np.sin(x)\n\n    # --- Part 1: Orthogonal Polynomial Basis (Legendre) ---\n\n    # Step 3 (part 1): Fit the model using the original inputs x\n    m_x = (x_min + x_max) / 2.0\n    r_x = x_max - x_min\n    # Handle the case where the interval has zero width\n    z = 2.0 * (x - m_x) / r_x if r_x != 0 else np.zeros_like(x)\n    \n    X_leg_x = np.zeros((n, d + 1), dtype=np.float64)\n    for j in range(d + 1):\n        p_j = legendre(j)\n        X_leg_x[:, j] = p_j(z)\n        \n    w_leg, _, _, _ = np.linalg.lstsq(X_leg_x, t, rcond=None)\n\n    # Step 3 (part 2): Apply rescaling y = ax + b and refit the model\n    y = a * x + b\n    y_min, y_max = np.min(y), np.max(y)\n    \n    m_y = (y_min + y_max) / 2.0\n    r_y = y_max - y_min\n    z_prime = 2.0 * (y - m_y) / r_y if r_y != 0 else np.zeros_like(y)\n\n    X_leg_y = np.zeros((n, d + 1), dtype=np.float64)\n    for j in range(d + 1):\n        p_j = legendre(j)\n        X_leg_y[:, j] = p_j(z_prime)\n\n    w_prime_leg, _, _, _ = np.linalg.lstsq(X_leg_y, t, rcond=None)\n\n    # Step 4: Test the invariance claim for the Legendre basis coefficients\n    epsilon = 1e-9\n    s_a = np.sign(a)\n    # The parity factor is (sign(a))^j for the j-th coefficient\n    parity_factor = np.array([s_a**j for j in range(d + 1)])\n    \n    expected_w_prime_leg = parity_factor * w_leg\n    max_abs_diff = np.max(np.abs(w_prime_leg - expected_w_prime_leg))\n    B = bool(max_abs_diff  epsilon)\n\n    # --- Part 2: Monomial Basis (Vandermonde) ---\n\n    # Step 5 (part 1): Fit models using the monomial basis\n    V_x = np.vander(x, d + 1, increasing=True)\n    w_van, _, _, _ = np.linalg.lstsq(V_x, t, rcond=None)\n\n    V_y = np.vander(y, d + 1, increasing=True)\n    w_prime_van, _, _, _ = np.linalg.lstsq(V_y, t, rcond=None)\n    \n    # Step 5 (part 2): Compute the relative coefficient change Delta\n    delta_num = np.linalg.norm(w_prime_van - w_van)\n    delta_den = np.linalg.norm(w_van) + 1e-12\n    Delta = delta_num / delta_den\n\n    # Step 5 (part 3): Compute the condition number ratio rho\n    kappa_x = np.linalg.cond(V_x, 2)\n    kappa_y = np.linalg.cond(V_y, 2)\n    \n    rho = kappa_y / kappa_x if kappa_x != 0 else np.inf\n\n    # Step 6: Return the list of results for this case\n    return [B, Delta, rho]\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # n, d, xmin, xmax, a, b\n        (50, 10, -3.0, 3.0, 2.0, 1.0),\n        (6, 5, -1.0, 1.0, -1.0, 0.0),\n        (50, 10, -1.0, 2.0, 100.0, -5.0),\n        (50, 10, -3.0, 3.0, 0.01, 100.0),\n        (50, 10, -2.0, 4.0, 1.0, 10.0),\n    ]\n\n    results = []\n    for case_params in test_cases:\n        result = run_case(*case_params)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    # The format [B, Delta, rho] is achieved by Python's default str(list).\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3102236"}, {"introduction": "最后的这个练习旨在连接近似理论与机器学习实践之间的桥梁。理论上，“最佳”的函数近似是在连续的内积（$L^2$ 范数）下定义的。然而在现实世界中，我们的模型是基于离散数据点，使用经验内积（即最小二乘法）来构建的。通过这个编码练习 [@problem_id:3102308]，你将探索训练数据的分布如何影响你的数据驱动模型与理论理想模型之间的一致性，从而深入理解有限数据的力量与局限。", "problem": "给定一个在有限维子空间中使用基函数、基于两种不同内积的函数逼近任务。设置如下。令输入域为闭区间 $[0,1]$。考虑候选基函数 $\\{v_k(x)\\}_{k=0}^{m-1}$，其中 $v_k(x) = x^k$ 且 $m = 4$，因此模型子空间是由 $\\{1, x, x^2, x^3\\}$ 张成的空间。在 $[0,1]$ 上的平方可积函数空间上定义两种内积：\n- 连续 $L^2$ 内积：对于函数 $g$ 和 $h$，$\\langle g, h \\rangle_{L^2} = \\int_{0}^{1} g(x) h(x) \\, dx$。\n- 与有限训练集 $\\{x_i\\}_{i=1}^n$ 相关的经验内积：$\\langle g, h \\rangle_{\\text{emp}} = \\frac{1}{n} \\sum_{i=1}^{n} g(x_i) h(x_i)$。\n\n目标函数为 $f(x) = \\sin(6 \\pi x) + x$，其中 $\\sin$ 函数的角度以弧度为单位。\n\n你的任务是：\n1. 仅使用内积空间的核心定义和 Gram–Schmidt 正交化过程，针对每种内积构造模型子空间的一个标准正交基：\n   - 一个关于 $\\langle \\cdot, \\cdot \\rangle_{L^2}$ 的 $L^2$-标准正交基 $\\{\\psi_k\\}_{k=0}^{m-1}$。\n   - 一个关于 $\\langle \\cdot, \\cdot \\rangle_{\\text{emp}}$ 的经验标准正交基 $\\{\\phi_k\\}_{k=0}^{m-1}$，该基由特定训练集 $\\{x_i\\}_{i=1}^n$ 给出。\n   每个标准正交基都必须使用相应的内积，通过 Gram–Schmidt 过程从候选基集 $\\{v_k\\}_{k=0}^{m-1}$ 获得。\n\n2. 构建 $f$ 在模型子空间上的两个投影：\n   - $L^2$ 投影 $P_{L^2} f = \\sum_{k=0}^{m-1} b_k \\, \\psi_k$，其系数为 $b_k = \\langle f, \\psi_k \\rangle_{L^2}$。\n   - 经验投影 $P_{\\text{emp}} f = \\sum_{k=0}^{m-1} a_k \\, \\phi_k$，其系数为 $a_k = \\langle f, \\phi_k \\rangle_{\\text{emp}}$。\n   请注意，$P_{L^2} f$ 仅依赖于连续内积，而 $P_{\\text{emp}} f$ 通过 $\\langle \\cdot, \\cdot \\rangle_{\\text{emp}}$ 依赖于训练集。\n\n3. 对于下面测试套件中指定的每个训练集，在一个包含 $[0,1]$ 区间内 $201$ 个均匀间隔点的测试网格上评估这两种逼近，即 $T = \\{t_j\\}_{j=0}^{200}$，其中 $t_j = \\frac{j}{200}$。计算在 $T$ 上这两个预测之间的最大逐点绝对差：\n   $$\\Delta = \\max_{t \\in T} \\left| P_{\\text{emp}} f(t) - P_{L^2} f(t) \\right|.$$\n   正弦函数使用的角度单位必须是弧度。不涉及物理单位。\n\n测试套件（每一项定义一个训练集 $\\{x_i\\}_{i=1}^n$）：\n- 情况 1（均匀中点）：$n = 101$ 且 $x_i = \\frac{i + 0.5}{101}$，其中整数 $i = 0, 1, \\dots, 100$。\n- 情况 2（靠近 $0$ 的二次聚类）：$n = 101$ 且 $x_i = \\left(\\frac{i + 0.5}{101}\\right)^2$，其中整数 $i = 0, 1, \\dots, 100$。\n- 情况 3（小的固定集）：$n = 4$ 且 $x = [0.05, 0.2, 0.5, 0.95]$。\n\n你的程序必须：\n- 仅使用上述定义，实现针对每种内积的 Gram–Schmidt 正交化。\n- 按照描述构建 $P_{L^2} f$ 和 $P_{\\text{emp}} f$。\n- 对于每种情况，在指定的测试网格上计算 $\\Delta$。\n\n最终输出格式：\n- 你的程序应生成单行输出，其中包含三个情况的结果，格式为方括号内以逗号分隔的列表，例如，“[r1,r2,r3]”。\n- 每个 $r_k$ 都必须是精确到 $6$ 位小数的浮点数。", "solution": "该问题要求在一个有限维多项式子空间内，对目标函数 $f(x)$ 的两种不同逼近进行比较分析。问题的核心在于理解内积的选择如何影响“最佳”逼近，内积定义了函数空间的几何结构。我们给定一个模型子空间 $V = \\text{span}\\{1, x, x^2, x^3\\}$，这是次数至多为 $3$ 的多项式空间。这些逼近是 $f(x)$ 在 $V$ 上关于两种不同内积的正交投影：连续 $L^2$ 内积和从有限数据点集导出的离散经验内积。\n\n构建这些投影的基本工具是 Gram-Schmidt 正交化过程。给定一个具有内积 $\\langle \\cdot, \\cdot \\rangle$ 的向量空间和一组线性无关的向量 $\\{v_k\\}_{k=0}^{m-1}$，该过程会为 $\\{v_k\\}$ 的张成空间生成一个标准正交基 $\\{u_k\\}_{k=0}^{m-1}$。这个过程是迭代的：\n令 $w_0 = v_0$ 且 $u_0 = \\frac{w_0}{\\|w_0\\|}$。\n对于 $k = 1, 2, \\dots, m-1$，我们计算\n$$w_k = v_k - \\sum_{j=0}^{k-1} \\langle v_k, u_j \\rangle u_j$$\n$$u_k = \\frac{w_k}{\\|w_k\\|}$$\n其中 $\\|g\\| = \\sqrt{\\langle g, g \\rangle}$ 是由内积导出的范数。向量 $\\sum_{j=0}^{k-1} \\langle v_k, u_j \\rangle u_j$ 是 $v_k$ 在由 $\\{u_0, \\dots, u_{k-1}\\}$ 张成的子空间上的正交投影，因此 $w_k$ 是 $v_k$ 正交于该子空间的分量。\n\n一旦找到了子空间 $V$ 的一个标准正交基 $\\{u_k\\}_{k=0}^{m-1}$，任何函数 $f$ 在 $V$ 上的正交投影由下式给出：\n$$P_V f = \\sum_{k=0}^{m-1} \\langle f, u_k \\rangle u_k$$\n对于 $p \\in V$，这个投影是 $V$ 中唯一能最小化距离 $\\|f - p\\|$ 的元素。\n\n现在，我们将此框架应用于两种指定的内积。候选基为 $\\{v_k(x) = x^k\\}_{k=0}^{3}$。\n\n**1. $L^2$ 投影 ($P_{L^2} f$)**\n\n$[0,1]$ 区间上的连续 $L^2$ 内积定义为：\n$$\\langle g, h \\rangle_{L^2} = \\int_{0}^{1} g(x) h(x) \\, dx$$\n我们从 $\\{v_k(x) = x^k\\}_{k=0}^{3}$ 构建一个 $L^2$-标准正交基 $\\{\\psi_k(x)\\}_{k=0}^{3}$。\n\n对于 $k=0$：\n$w_0(x) = v_0(x) = 1$。\n$\\|w_0\\|^2_{L^2} = \\int_0^1 1^2 \\,dx = 1$。\n$\\psi_0(x) = \\frac{w_0(x)}{\\|w_0\\|_{L^2}} = 1$。\n\n对于 $k=1$：\n$v_1$ 在 $\\{\\psi_0\\}$ 的张成空间上的投影是 $\\langle v_1, \\psi_0 \\rangle_{L^2} \\psi_0(x)$。\n$\\langle v_1, \\psi_0 \\rangle_{L^2} = \\int_0^1 x \\cdot 1 \\,dx = \\frac{1}{2}$。\n$w_1(x) = v_1(x) - \\frac{1}{2} \\psi_0(x) = x - \\frac{1}{2}$。\n$\\|w_1\\|^2_{L^2} = \\int_0^1 (x - \\frac{1}{2})^2 \\,dx = \\frac{1}{12}$。\n$\\psi_1(x) = \\frac{w_1(x)}{\\|w_1\\|_{L^2}} = \\sqrt{12}(x - \\frac{1}{2})$。\n\n对 $k=2$ 和 $k=3$ 继续此过程以获得 $\\psi_2(x)$ 和 $\\psi_3(x)$。所得到的多项式是 $[0,1]$ 上移位的勒让德多项式（Legendre polynomials）的缩放版本。\n\n目标函数是 $f(x) = \\sin(6 \\pi x) + x$。$f$ 在模型子空间上的 $L^2$ 投影是：\n$$P_{L^2} f(x) = \\sum_{k=0}^{3} b_k \\psi_k(x), \\quad \\text{where } b_k = \\langle f, \\psi_k \\rangle_{L^2} = \\int_0^1 f(x) \\psi_k(x) \\,dx$$\n这些系数 $b_k$ 是常数，因为它们仅依赖于固定的函数 $f$ 和 $L^2$ 内积，而不依赖于任何训练数据。这些积分将通过数值计算得出。\n\n**2. 经验投影 ($P_{\\text{emp}} f$)**\n\n经验内积是相对于一个训练集 $\\{x_i\\}_{i=1}^n$ 定义的：\n$$\\langle g, h \\rangle_{\\text{emp}} = \\frac{1}{n} \\sum_{i=1}^{n} g(x_i) h(x_i)$$\n与 $L^2$ 的情况不同，这个内积以及由此产生的标准正交基和投影，都依赖于点集 $\\{x_i\\}$ 的具体选择。对于每个测试用例，我们都给定了一组不同的点。\n\n对于一个特定的训练集，我们使用相同的 Gram-Schmidt 过程从 $\\{v_k(x) = x^k\\}_{k=0}^{3}$ 构建一个经验标准正交基 $\\{\\phi_k(x)\\}_{k=0}^{3}$，但使用的是 $\\langle \\cdot, \\cdot \\rangle_{\\text{emp}}$ 而不是 $\\langle \\cdot, \\cdot \\rangle_{L^2}$。\n\n$f$ 的投影则为：\n$$P_{\\text{emp}} f(x) = \\sum_{k=0}^{3} a_k \\phi_k(x), \\quad \\text{where } a_k = \\langle f, \\phi_k \\rangle_{\\text{emp}} = \\frac{1}{n} \\sum_{i=1}^{n} f(x_i) \\phi_k(x_i)$$\n多项式 $P_{\\text{emp}} f(x)$ 是将一个 $3$ 次多项式拟合到数据点 $(x_i, f(x_i))$ 的普通最小二乘解。在情况 3 中，点的数量 $n=4$ 等于基函数的数量 $m=4$，此时该投影成为在给定的四个点上对 $f(x)$ 进行插值的唯一一个次数至多为 $3$ 的多项式。\n\n**3. 投影的比较**\n\n问题要求在一个精细的测试网格 $T = \\{t_j = \\frac{j}{200}\\}_{j=0}^{200}$ 上，计算两个投影之间的最大逐点绝对差：\n$$\\Delta = \\max_{t \\in T} \\left| P_{\\text{emp}} f(t) - P_{L^2} f(t) \\right|$$\n该度量量化了数据驱动的逼近 ($P_{\\text{emp}} f$) 与“真实”函数空间逼近 ($P_{L^2} f$) 之间的偏差程度。当点 $\\{x_i\\}$ 的经验分布与 $[0,1]$ 上的均匀分布非常接近时（如情况 1），偏差预计会很小；而当分布倾斜（情况 2）或稀疏（情况 3）时，偏差会较大。下面的代码为每个指定的训练集实现了这整个过程。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.integrate import quad\n\ndef solve():\n    \"\"\"\n    Computes the maximum difference between L2 and empirical projections\n    for three different training sets.\n    \"\"\"\n\n    # Define the target function and basis functions\n    m = 4\n    target_func = lambda x: np.sin(6 * np.pi * x) + x\n    basis_v = [np.poly1d([1] + [0] * k) for k in range(m - 1, -1, -1)]\n\n    # Define the test grid\n    test_grid = np.linspace(0, 1, 201)\n\n    # =========================================================================\n    # Part 1: L2 Projection (independent of test cases)\n    # =========================================================================\n\n    def l2_inner_product(p1, p2):\n        integrand = p1 * p2\n        return quad(integrand, 0, 1)[0]\n\n    def gram_schmidt(basis_functions, inner_prod_func):\n        \"\"\"\n        Applies classical Gram-Schmidt to a list of polynomial functions.\n        \"\"\"\n        orthonormal_basis = []\n        for v in basis_functions:\n            w = v\n            for u in orthonormal_basis:\n                proj_coeff = inner_prod_func(v, u)\n                w = w - proj_coeff * u\n            \n            norm_w_sq = inner_prod_func(w, w)\n            # Add a small epsilon for numerical stability, though problem setup\n            # guarantees linear independence.\n            if norm_w_sq  1e-20:\n                # This should not be reached with the given problem sets.\n                # Handle gracefully by returning a zero polynomial if needed.\n                u_new = np.poly1d([0])\n            else:\n                u_new = w / np.sqrt(norm_w_sq)\n            \n            orthonormal_basis.append(u_new)\n        return orthonormal_basis\n\n    # Construct the L2-orthonormal basis {psi_k}\n    psi_basis = gram_schmidt(basis_v, l2_inner_product)\n\n    # Compute the coefficients b_k for the L2 projection\n    b_coeffs = []\n    for psi_k in psi_basis:\n        integrand = lambda x: target_func(x) * psi_k(x)\n        b_k = quad(integrand, 0, 1)[0]\n        b_coeffs.append(b_k)\n\n    # Construct the L2 projection polynomial P_L2 f\n    p_l2_f = np.poly1d([0])\n    for b_k, psi_k in zip(b_coeffs, psi_basis):\n        p_l2_f += b_k * psi_k\n\n    # Evaluate the L2 projection on the test grid\n    p_l2_f_values = p_l2_f(test_grid)\n\n    # =========================================================================\n    # Part 2: Empirical Projections (for each test case)\n    # =========================================================================\n\n    # Define test cases\n    test_cases = [\n        # Case 1: n = 101, uniform midpoints\n        (lambda: (101, (np.arange(101) + 0.5) / 101)),\n        # Case 2: n = 101, quadratic cluster near 0\n        (lambda: (101, ((np.arange(101) + 0.5) / 101)**2)),\n        # Case 3: n = 4, small, fixed set\n        (lambda: (4, np.array([0.05, 0.2, 0.5, 0.95])))\n    ]\n\n    results = []\n    for case_generator in test_cases:\n        n, x_nodes = case_generator()\n\n        # Define the empirical inner product for the current training set\n        def empirical_inner_product(p1, p2):\n            return np.mean(p1(x_nodes) * p2(x_nodes))\n\n        # Construct the empirical-orthonormal basis {phi_k}\n        phi_basis = gram_schmidt(basis_v, empirical_inner_product)\n\n        # Compute the coefficients a_k for the empirical projection\n        f_at_nodes = target_func(x_nodes)\n        a_coeffs = []\n        for phi_k in phi_basis:\n            a_k = np.mean(f_at_nodes * phi_k(x_nodes))\n            a_coeffs.append(a_k)\n\n        # Construct the empirical projection polynomial P_emp f\n        p_emp_f = np.poly1d([0])\n        for a_k, phi_k in zip(a_coeffs, phi_basis):\n            p_emp_f += a_k * phi_k\n\n        # Evaluate the empirical projection on the test grid\n        p_emp_f_values = p_emp_f(test_grid)\n\n        # Compute the maximum absolute pointwise difference Delta\n        delta = np.max(np.abs(p_emp_f_values - p_l2_f_values))\n        results.append(delta)\n\n    # Final print statement in the exact required format\n    formatted_results = [f\"{r:.6f}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3102308"}]}