{"hands_on_practices": [{"introduction": "理论知识最好通过实践来巩固。理解感知机算法的一个关键方面是认识到数据本身的特性会如何影响其性能。本练习将引导你探究特征之间的相关性（即数据的“几何形状”）如何影响感知机的收敛速度，通过编程实验，你将量化这一效应，并利用格拉姆-施密特（Gram-Schmidt）正交化方法来预处理数据，从而直观地看到特征解耦如何能显著加速学习过程。[@problem_id:3099389]", "problem": "要求您编写一个完整、可运行的程序，以经验性地分析特征相关性如何影响感知机学习算法的收敛速度，以及通过 Gram–Schmidt 过程对特征进行去相关处理如何改变此行为。您的程序必须从头开始实现感知机，并执行基于 Gram–Schmidt 正交归一化的特征空间变换，以获得去相关的表示形式进行比较。\n\n使用的基本原理：\n- 具有感知机更新规则的线性分类器定义：对于数据点 $\\{(x_i,y_i)\\}_{i=1}^n$（其中 $x_i \\in \\mathbb{R}^d$，标签 $y_i \\in \\{-1,+1\\}$），感知机维护一个权重向量 $w \\in \\mathbb{R}^d$，并在对 $(x_i,y_i)$ 分类错误时，使用规则 $w \\leftarrow w + y_i x_i$ 对其进行更新。\n- Gram–Schmidt 正交归一化：对于矩阵 $X \\in \\mathbb{R}^{n \\times d}$，Gram–Schmidt 过程产生一个分解 $X = QR$，其中 $Q \\in \\mathbb{R}^{n \\times d}$ 具有正交归一的列，当 $X$ 具有满列秩时，$R \\in \\mathbb{R}^{d \\times d}$ 是一个上三角可逆矩阵。相应的线性特征变换 $A = R^{-1}$ 将每个 $x_i$ 映射到 $z_i = A x_i$，生成 $Z = X A = Q$，其列在整个数据集上是正交归一的。\n\n您的程序必须：\n- 生成具有受控特征相关性的合成线性可分数据集。对于每个测试用例，从一个零均值多元正态分布中抽取 $n$ 个 $d$ 维样本，其协方差矩阵为等相关结构 $\\Sigma = (1-\\rho) I_d + \\rho \\mathbf{1}\\mathbf{1}^\\top$，其中 $\\rho \\in (-1,1)$ 是相关性参数，$\\mathbf{1}$ 是 $\\mathbb{R}^d$ 中的全一向量。从标准正态分布中抽取一个真实权重 $w_\\star \\in \\mathbb{R}^d$，并分配标签 $y_i = \\operatorname{sign}(w_\\star^\\top x_i) \\in \\{-1,+1\\}$。为确保正间隔，通过 $x_i \\leftarrow x_i + m\\, y_i\\, u$ 修改特征，其中 $u = \\frac{w_\\star}{\\|w_\\star\\|_2}$ 且 $m  0$ 是一个对所有测试用例通用的固定间隔注入常数。\n- 实现一个确定性的感知机过程，该过程：\n  - 初始化 $w = 0 \\in \\mathbb{R}^d$。\n  - 按固定的索引顺序 $i = 1,2,\\dots,n$ 扫描 $n$ 个样本。\n  - 对每个满足 $y_i (w^\\top x_i) \\le 0$ 的错误分类，应用更新规则 $w \\leftarrow w + y_i x_i$。\n  - 持续对数据集进行完整遍历，直到一次完整的遍历不产生任何错误为止，并返回所做的总更新次数。\n- 对原始特征矩阵 $X \\in \\mathbb{R}^{n \\times d}$ 通过 $X = Q R$ 计算 Gram–Schmidt 变换，并构造去相关特征 $Z = X R^{-1} = Q$。使用相同的标签 $y$ 在 $Z$ 上运行完全相同的感知机过程，并记录收敛所需的总更新次数。\n- 为保证可复现性，每个测试用例使用固定的随机种子。所有随机抽样都必须使用该测试用例指定的种子。\n- 对于所有线性代数运算和范数，均使用标准欧几里得内积。\n\n要实现的测试套件：\n- 使用间隔注入 $m = 1.0$。\n- 对于每个测试用例，使用相应的元组 $(d,n,\\rho,\\text{seed})$ 按上述规定生成数据：\n  - 用例 1：$(d,n,\\rho,\\text{seed}) = (\\,2,\\,100,\\,0.0,\\,42\\,)$。\n  - 用例 2：$(d,n,\\rho,\\text{seed}) = (\\,2,\\,100,\\,0.95,\\,43\\,)$。\n  - 用例 3：$(d,n,\\rho,\\text{seed}) = (\\,2,\\,100,\\,{-0.95},\\,44\\,)$。\n  - 用例 4：$(d,n,\\rho,\\text{seed}) = (\\,3,\\,120,\\,0.8,\\,45\\,)$。\n\n每个测试用例的所需输出：\n- 生成一对整数 $[u_{\\text{orig}}, u_{\\text{ortho}}]$，其中 $u_{\\text{orig}}$ 是在原始相关特征 $X$ 上感知机收敛所需的总更新次数，而 $u_{\\text{ortho}}$ 是在经 Gram–Schmidt 去相关处理的特征 $Z$ 上感知机收敛所需的总更新次数。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含四个整数对的结果，以逗号分隔的列表形式呈现，并用方括号括起来，不含空格。例如：$[[u_1,v_1],[u_2,v_2],[u_3,v_3],[u_4,v_4]]$，其中每个 $u_k$ 和 $v_k$ 是对应于上述顺序中第 $k$ 个测试用例的整数。", "solution": "用户要求对感知机算法的收敛速度作为特征相关性的函数进行经验性分析。为实现此目的，需要生成具有受控特征间相关性的合成数据集，在该原始数据上运行确定性感知的算法，然后将其性能与在数据的去相关版本上运行相同算法的性能进行比较。去相关操作通过源自 Gram-Schmidt 正交归一化过程的特征空间变换来执行。\n\n解决方案的结构如下：\n1.  **数据生成**：一种创建具有指定相关性结构的线性可分数据集的精确方法。\n2.  **感知机算法**：感知机学习规则的实现。\n3.  **特征去相关**：应用 Gram-Schmidt 正交归一化来变换特征空间。\n4.  **实验流程**：单个测试用例的完整流程，结合上述组件以生成所需输出。\n\n**1. 数据生成**\n\n一个合成数据集包含 $n$ 个实例，每个实例是一个数据对 $(x_i, y_i)$，其中 $x_i \\in \\mathbb{R}^d$ 是一个特征向量，$y_i \\in \\{-1, +1\\}$ 是一个类别标签。\n\n- **初始特征生成**：特征向量最初从一个零均值的 $d$ 维多元正态分布中抽取，该分布具有指定的协方差矩阵 $\\Sigma$：\n$$\nx_i \\sim \\mathcal{N}(0, \\Sigma)\n$$\n协方差矩阵 $\\Sigma$ 被构造成具有等相关结构，由参数 $\\rho \\in (-1/(d-1), 1)$ 控制：\n$$\n\\Sigma = (1-\\rho) I_d + \\rho \\mathbf{1}\\mathbf{1}^\\top\n$$\n此处，$I_d$ 是 $d \\times d$ 的单位矩阵，$\\mathbf{1}$ 是一个 $d \\times 1$ 的全一向量。这种结构确保了每个特征的方差为 $1$，并且任意两个不同特征 $j$ 和 $k$ 之间的协方差为 $\\text{Cov}(X_j, X_k) = \\rho$。\n\n- **标签分配**：一个真实的分割超平面由一个权重向量 $w_\\star \\in \\mathbb{R}^d$ 定义，其分量从标准正态分布中抽取，$w_{\\star,j} \\sim \\mathcal{N}(0,1)$。然后根据每个点 $x_i$ 落在该超平面的哪一侧来分配标签：\n$$\ny_i = \\operatorname{sgn}(w_\\star^\\top x_i)\n$$\n为确保 $y_i \\in \\{-1, +1\\}$，任何 $w_\\star^\\top x_i = 0$ 的情况（在使用连续分布时是罕见事件）都通过分配 $y_i = +1$ 来解决。\n\n- **间隔注入**：为确保数据是线性可分的且具有非零间隔（这是感知机算法保证收敛的一个条件），特征向量需要被调整。这个过程将每个点 $x_i$ 沿正确的方向进一步推离分割超平面。修改后的特征向量 $x'_i$ 由下式给出：\n$$\nx'_i = x_i + m y_i u\n$$\n其中 $m  0$ 是一个固定的间隔常数（在本问题中为 $m=1.0$），$u$ 是垂直于真实超平面的单位向量，$u = \\frac{w_\\star}{\\|w_\\star\\|_2}$。此操作保证了每个点相对于由 $w_\\star$ 定义的超平面至少有 $m$ 的几何间隔。这些修改后的向量 $\\{x'_i\\}$ 的集合构成了最终的特征矩阵 $X$。\n\n**2. 感知机学习算法**\n\n感知机算法是一种用于为线性可分数据集寻找分割超平面的迭代方法。此处实现的算法是确定性的。\n\n- **初始化**：权重向量初始化为零向量，$w = 0 \\in \\mathbb{R}^d$。总更新次数的计数器初始化为 $0$。\n\n- **迭代**：该算法分多轮进行。在每一轮中，它按固定顺序遍历所有数据点 $(x_i, y_i)$（$i=1, \\dots, n$）。对于每个点，它检查分类条件：\n$$\ny_i (w^\\top x_i) \\le 0\n$$\n如果满足此条件，则该点被错误分类（或位于边界上），权重向量根据感知机规则进行更新：\n$$\nw \\leftarrow w + y_i x_i\n$$\n总更新计数器加一。\n\n- **终止**：当算法完成对数据集的一次完整遍历而没有任何更新时，算法终止。更新计数器的最终值是关注的结果，记为 $u$。\n\n**3. 通过 Gram-Schmidt 进行特征去相关**\n\n问题要求与一个去相关的特征集进行比较。这是通过对原始特征矩阵 $X \\in \\mathbb{R}^{n \\times d}$ 应用线性变换来实现的。\n\n- **QR 分解**：Gram-Schmidt 过程应用于特征矩阵 $X$ 的*列*。$X$ 的列 $\\{c_1, \\dots, c_d\\}$ 可被视为 $\\mathbb{R}^n$ 中的向量，其中每个向量代表单个特征的所有观测值。该过程通过对 $X$ 进行简化的 QR 分解在计算上实现：\n$$\nX = QR\n$$\n其中 $Q \\in \\mathbb{R}^{n \\times d}$ 是一个具有正交归一列的矩阵（即 $Q^\\top Q = I_d$），$R \\in \\mathbb{R}^{d \\times d}$ 是一个可逆的上三角矩阵（假设 $X$ 具有满列秩，对于给定的数据生成过程，这是极有可能的）。\n\n- **特征变换**：问题通过矩阵 $A = R^{-1}$ 定义变换。新的特征矩阵 $Z \\in \\mathbb{R}^{n \\times d}$ 是通过将此变换应用于 $X$ 获得的：\n$$\nZ = XA = XR^{-1}\n$$\n通过代入 $X=QR$，我们发现新的特征矩阵就是 $Q$：\n$$\nZ = (QR)R^{-1} = Q(RR^{-1}) = QI_d = Q\n$$\n新的特征向量，即 $Z=Q$ 的行，然后与原始标签 $y$ 一起用于训练第二个感知机。由此产生的特征空间具有这样的属性：其定义特征的列向量是正交归一的，这意味着它们在数据集的样本中是不相关的。\n\n**4. 实验流程**\n\n对于由元组 $(d, n, \\rho, \\text{seed})$ 指定的每个测试用例，执行以下步骤：\n1.  为保证可复现性，设置随机数生成器的种子。\n2.  使用第 1 节中描述的过程生成原始特征矩阵 $X$ 和标签 $y$。\n3.  在数据集 $(X,y)$ 上运行感知机算法（第 2 节），并记录收敛所需的总更新次数 $u_{\\text{orig}}$。\n4.  计算 $X$ 的简化 QR 分解以获得 $Q$ 和 $R$。\n5.  设置变换后的特征矩阵 $Z=Q$。\n6.  在变换后的数据集 $(Z,y)$ 上运行感知机算法，并记录总更新次数 $u_{\\text{ortho}}$。\n7.  该测试用例的最终输出是整数对 $[u_{\\text{orig}}, u_{\\text{ortho}}]$。\n\n这种比较分析预计将表明，对于高相关性值 $|\\rho|$，更新次数 $u_{\\text{orig}}$ 会显著大于 $u_{\\text{ortho}}$。正交归一化过程规范化了数据的几何结构，通常会导致更快、更稳定的收敛。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef run_perceptron(features, labels):\n    \"\"\"\n    Runs the deterministic perceptron algorithm.\n\n    Args:\n        features (np.ndarray): The feature matrix (n_samples, n_features).\n        labels (np.ndarray): The label vector (n_samples,).\n\n    Returns:\n        int: The total number of updates until convergence.\n    \"\"\"\n    n_samples, n_features = features.shape\n    w = np.zeros(n_features)\n    total_updates = 0\n    \n    while True:\n        updates_in_pass = 0\n        for i in range(n_samples):\n            # Perceptron mistake condition\n            if labels[i] * np.dot(w, features[i]) = 0:\n                # Perceptron update rule\n                w += labels[i] * features[i]\n                total_updates += 1\n                updates_in_pass += 1\n        \n        # Termination condition\n        if updates_in_pass == 0:\n            break\n            \n    return total_updates\n\ndef run_single_case(d, n, rho, seed, m):\n    \"\"\"\n    Performs the full analysis for a single test case.\n\n    Args:\n        d (int): Number of features (dimensions).\n        n (int): Number of samples.\n        rho (float): Correlation coefficient.\n        seed (int): Random seed for reproducibility.\n        m (float): Margin injection constant.\n\n    Returns:\n        list[int, int]: A pair of integers [u_orig, u_ortho].\n    \"\"\"\n    # 1. Set seed for reproducibility\n    rng = np.random.default_rng(seed)\n    \n    # 2. Generate synthetic data\n    # Create the equicorrelation covariance matrix\n    cov_matrix = (1 - rho) * np.eye(d) + rho * np.ones((d, d))\n    \n    # Draw samples from a multivariate normal distribution\n    X_initial = rng.multivariate_normal(mean=np.zeros(d), cov=cov_matrix, size=n)\n    \n    # Draw a ground-truth weight vector\n    w_star = rng.standard_normal(size=d)\n    \n    # Assign labels\n    y = np.sign(X_initial @ w_star)\n    # Ensure labels are in {-1, +1}\n    y[y == 0] = 1\n    \n    # 3. Perform margin injection\n    u = w_star / np.linalg.norm(w_star)\n    # Use broadcasting to add the margin term to each row of X\n    X_orig = X_initial + m * y[:, np.newaxis] * u\n    \n    # 4. Run perceptron on original features\n    u_orig = run_perceptron(X_orig, y)\n    \n    # 5. Decorrelate features using Gram-Schmidt (QR decomposition)\n    # 'reduced' mode is essential for non-square matrices\n    Q, R = np.linalg.qr(X_orig, mode='reduced')\n    Z_ortho = Q\n    \n    # 6. Run perceptron on decorrelated features\n    u_ortho = run_perceptron(Z_ortho, y)\n    \n    return [u_orig, u_ortho]\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the final result.\n    \"\"\"\n    # Margin injection constant common to all tests\n    m = 1.0\n\n    # Test suite: (d, n, rho, seed)\n    test_cases = [\n        (2, 100, 0.0, 42),\n        (2, 100, 0.95, 43),\n        (2, 100, -0.95, 44),\n        (3, 120, 0.8, 45),\n    ]\n\n    results = []\n    for d, n, rho, seed in test_cases:\n        result = run_single_case(d, n, rho, seed, m)\n        results.append(result)\n\n    # Format the output string as specified, e.g., [[u1,v1],[u2,v2],...]\n    result_str_parts = [f\"[{u},{v}]\" for u, v in results]\n    final_output = f\"[{','.join(result_str_parts)}]\"\n    \n    print(final_output)\n\nsolve()\n```", "id": "3099389"}, {"introduction": "除了数据本身的特性，我们还可以通过改进学习算法本身来提升其性能。动量法（Momentum）是一种经典的优化加速技术，它通过积累历史更新的“速度”来帮助算法冲过平坦区域并抑制振荡。在此练习中，你将为感知机算法加入动量项，并在一系列精心设计的、处于线性可分边缘的数据集上进行测试，以探索动量如何影响收敛速度，并观察它是否会引起“过冲”现象，即在某些更新后反而增加了错误分类的数量。[@problem_id:3099401]", "problem": "考虑欧氏空间中的二元分类问题，其中输入为向量 $x \\in \\mathbb{R}^d$，标签为 $y \\in \\{-1,+1\\}$。带偏置的线性分类器使用增广权重向量 $w \\in \\mathbb{R}^{d+1}$ 和增广输入 $\\tilde{x} \\in \\mathbb{R}^{d+1}$（定义为 $\\tilde{x} = [x; 1]$），并预测 $f(\\tilde{x}) = \\mathrm{sign}(w^\\top \\tilde{x})$。感知机更新是一种由错误驱动的规则，每当数据点被错误分类时，它会修改 $w$。在本任务中，您将在临界可分数据集上实现一个带动量的感知机（一种重球式更新），以探究收敛速度和过冲现象。\n\n本任务的基础：\n- 线性可分性：如果存在一个 $w^\\star$ 使得对所有 $i$ 都有 $y_i (w^{\\star\\top} \\tilde{x}_i)  0$，那么数据集 $\\{(x_i,y_i)\\}_{i=1}^n$ 是线性可分的。感知机学习准则在 $y_i (w^\\top \\tilde{x}_i) \\le 0$ 时更新 $w$。\n- 错误驱动的更新：对于一个被错误分类的样本 $(x_t,y_t)$，经典感知机执行更新 $w_{t+1} = w_t + \\eta y_t \\tilde{x}_t$，其中学习率 $\\eta  0$。\n- 动量（重球法）：维持一个速度向量 $v_t$，当出现错分时，其更新规则为 $v_{t+1} = \\beta v_t + \\eta y_t \\tilde{x}_t$（动量系数 $\\beta \\in [0,1)$）；否则为 $v_{t+1} = \\beta v_t$。权重更新为 $w_{t+1} = w_t + v_{t+1}$。\n\n您的程序必须：\n1. 实现一个单遍、循环、错误驱动的带动量感知机。在第 $t$ 次迭代中，令 $i = t \\bmod n$ 作为下一个数据点 $(x_i,y_i)$ 的索引。计算间隔 $m_t = y_i (w_t^\\top \\tilde{x}_i)$。如果 $m_t \\le 0$，执行动量更新 $v_{t+1} = \\beta v_t + \\eta y_i \\tilde{x}_i$，将错误计数器加 1，并设置 $w_{t+1} = w_t + v_{t+1}$。如果 $m_t  0$，执行 $v_{t+1} = \\beta v_t$ 和 $w_{t+1} = w_t + v_{t+1}$。初始化 $w_0 = 0$ 和 $v_0 = 0$。如果未达到收敛，使用迭代上限 $T_{\\max}$ 停止。\n2. 收敛准则：一旦对所有的 $j \\in \\{1,\\dots,n\\}$ 都满足 $y_j (w^\\top \\tilde{x}_j)  0$ 时，就宣布收敛。\n3. 过冲量化：令 $M(w)$ 为在权重 $w$ 下被错误分类的点的总数，即 $M(w) = \\sum_{i=1}^n \\mathbf{1}\\{y_i (w^\\top \\tilde{x}_i) \\le 0\\}$。对于每次错误驱动的更新（即 $m_t \\le 0$ 的每次迭代），在更新前立即计算 $M(w_t)$，在更新后立即计算 $M(w_{t+1})$。如果 $M(w_{t+1})  M(w_t)$，则计为一个过冲事件。将过冲率定义为总过冲事件数除以总错误驱动更新次数。如果没有错误驱动的更新，则将过冲率定义为 $0$。\n4. 收敛速度度量：报告直到收敛（如果未收敛，则直到达到迭代上限）所执行的错误驱动更新的总次数。\n\n测试套件。使用以下固定的数据集和超参数：\n- 数据集 $\\mathcal{D}_1$（在 $\\mathbb{R}^2$ 中临界可分）：正样本位于 $(1.0, 1.05)$、$(2.0, 2.05)$、$(3.0, 3.05)$，负样本位于 $(1.0, 0.95)$、$(2.0, 1.95)$、$(3.0, 2.95)$。正样本标签为 $+1$，负样本标签为 $-1$。\n- 数据集 $\\mathcal{D}_2$（极小间隔）：正样本位于 $(0.0, 0.001)$、$(1.0, 1.001)$、$(2.0, 2.001)$，负样本位于 $(0.0, -0.001)$、$(1.0, 0.999)$、$(2.0, 1.999)$，标签分别为 $+1$ 和 $-1$。\n- 数据集 $\\mathcal{D}_3$（不可分边界情况）：正样本位于 $(0.0, 0.0)$、$(1.0, 1.0)$，负样本位于 $(0.0, 0.0)$、$(1.0, 1.0)$，标签分别为 $+1$ 和 $-1$。\n\n构建以下四个测试用例：\n- 测试用例 1：数据集 $\\mathcal{D}_1$，学习率 $\\eta = 0.1$，动量 $\\beta = 0.0$，迭代上限 $T_{\\max} = 5000$。\n- 测试用例 2：数据集 $\\mathcal{D}_1$，学习率 $\\eta = 0.1$，动量 $\\beta = 0.9$，迭代上限 $T_{\\max} = 5000$。\n- 测试用例 3：数据集 $\\mathcal{D}_2$，学习率 $\\eta = 0.05$，动量 $\\beta = 0.95$，迭代上限 $T_{\\max} = 8000$。\n- 测试用例 4：数据集 $\\mathcal{D}_3$，学习率 $\\eta = 0.1$，动量 $\\beta = 0.9$，迭代上限 $T_{\\max} = 2000$。\n\n每个测试用例按顺序所需的输出：\n- 直到收敛（如果未收敛，则直到迭代上限）的错误驱动更新次数的整数值。\n- 过冲率，为浮点数，四舍五入到 $6$ 位小数。\n- 一个布尔值，指示是否达到收敛。\n\n最终输出格式：\n您的程序应生成单行输出，其中包含四个测试用例的聚合结果，格式为一个逗号分隔的列表，并用方括号括起来，其中每个元素是针对一个测试用例的列表 $[\\text{steps}, \\text{overshoot\\_ratio}, \\text{converged}]$。例如，形式为 $[[s_1, r_1, c_1],[s_2, r_2, c_2],[s_3, r_3, c_3],[s_4, r_4, c_4]]$ 的输出，其中过冲率四舍五入到 $6$ 位小数。", "solution": "我们从增广形式的感知机分类模型开始。对于一个输入 $x \\in \\mathbb{R}^d$，我们附加一个常数偏置坐标以获得 $\\tilde{x} = [x; 1] \\in \\mathbb{R}^{d+1}$。一个由 $w \\in \\mathbb{R}^{d+1}$ 参数化的线性分类器进行预测 $f(\\tilde{x}) = \\mathrm{sign}(w^\\top \\tilde{x})$。$(\\tilde{x},y)$ 在 $w$ 下的间隔为 $m = y (w^\\top \\tilde{x})$。正确分类对应于 $m  0$。\n\n感知机学习准则是错误驱动的：每当一个点 $(\\tilde{x}_i, y_i)$ 被错误分类或位于决策边界上，即 $y_i (w^\\top \\tilde{x}_i) \\le 0$ 时，参数会朝着 $y_i \\tilde{x}_i$ 的方向更新。经典更新是 $w_{t+1} = w_t + \\eta y_t \\tilde{x}_t$，其中学习率 $\\eta  0$。为了引入动量，我们维持一个速度向量 $v_t \\in \\mathbb{R}^{d+1}$，它对过去的更新进行指数平均。重球式更新定义为\n$$\nv_{t+1} =\n\\begin{cases}\n\\beta v_t + \\eta y_t \\tilde{x}_t  \\text{若 } y_t (w_t^\\top \\tilde{x}_t) \\le 0,\\\\\n\\beta v_t  \\text{若 } y_t (w_t^\\top \\tilde{x}_t)  0,\n\\end{cases}\n\\qquad\nw_{t+1} = w_t + v_{t+1},\n$$\n其中 $\\beta \\in [0,1)$ 是动量系数。我们初始化 $w_0 = 0$ 和 $v_0 = 0$。迭代确定性地循环遍历数据集：在第 $t$ 次迭代中，索引为 $i = t \\bmod n$（总共 $n$ 个点）。这是一个与错误驱动逻辑一致的确定性循环调度，用于测试收敛行为。\n\n当当前参数 $w$ 正确分类所有训练点时，即对所有 $j \\in \\{1,\\dots,n\\}$ 都有 $y_j (w^\\top \\tilde{x}_j)  0$ 时，检测到收敛。我们量化两个方面：\n1. 收敛速度：直到收敛所进行的错误驱动更新的总次数。形式上，计算 $y_t (w_t^\\top \\tilde{x}_t) \\le 0$ 且更新使用了 $\\eta y_t \\tilde{x}_t$ 的迭代次数；用整数 $S$ 表示此计数。\n2. 过冲：动量可能导致更新恶化即时的分类性能，尤其是在具有小间隔的临界可分数据集上。我们将错分计数定义为 $M(w) = \\sum_{i=1}^n \\mathbf{1}\\{y_i (w^\\top \\tilde{x}_i) \\le 0\\}$。在每次错误驱动的更新中，更新前计算 $M(w_t)$，更新后计算 $M(w_{t+1})$。如果 $M(w_{t+1})  M(w_t)$，则计为一个过冲事件。过冲率是过冲事件数除以 $S$，如果 $S = 0$，则定义为 $0$。\n\n算法流程如下：\n- 初始化 $w_0 = 0$，$v_0 = 0$，错误计数 $S = 0$，过冲计数 $O = 0$。\n- 对于 $t = 0,1,2,\\dots$ 直到达到迭代上限 $T_{\\max}$：\n  - 设置 $i = t \\bmod n$ 并计算 $m_t = y_i (w_t^\\top \\tilde{x}_i)$。\n  - 计算当前的错分计数 $M(w_t)$。\n  - 如果 $m_t \\le 0$，更新 $v_{t+1} = \\beta v_t + \\eta y_i \\tilde{x}_i$，然后 $w_{t+1} = w_t + v_{t+1}$，增加 $S$。计算 $M(w_{t+1})$；如果 $M(w_{t+1})  M(w_t)$，增加 $O$。\n  - 如果 $m_t  0$，更新 $v_{t+1} = \\beta v_t$ 和 $w_{t+1} = w_t + v_{t+1}$。\n  - 更新后，检查收敛性：如果 $M(w_{t+1}) = 0$，则停止。\n- 输出为 $S$，过冲率 $O/S$（四舍五入到 $6$ 位小数，若 $S=0$ 则为 $0$），以及一个指示是否收敛的布尔值。\n\n为何这些定义是有根据的：\n- 间隔 $y (w^\\top \\tilde{x})$ 直接编码了正确的分类，是感知机准则下线性分类的基础量。\n- 错误驱动规则遵循了经过充分检验的感知机算法，这是统计学习中处理线性可分数据的基石。\n- 带有系数 $\\beta$ 的动量会对过去的更新进行平均，以加速沿一致方向的移动，这可能减少在临界可分数据上达到分离超平面所需的更新次数，但由于惯性，它可能会过冲，从而暂时增加 $M(w)$。\n\n该测试套件探究了：\n- 一个典型的临界可分数据集 $\\mathcal{D}_1$，其类别间有小偏移，用于比较无动量（$\\beta = 0$）和强动量（$\\beta = 0.9$）。\n- 一个具有极小间隔的数据集 $\\mathcal{D}_2$，使用强动量（$\\beta = 0.95$）来凸显过冲趋势。\n- 一个不可分数据集 $\\mathcal{D}_3$，作为边界条件，以验证在不收敛的情况下能通过迭代上限终止。\n\n最终的程序精确地实现了这个算法，为每个测试用例计算所需的指标，将过冲率四舍五入到 $6$ 位小数，并以指定的单行格式打印聚合结果。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef misclassification_count(w, Xb, y):\n    # y * (Xb @ w) = 0 counts as misclassification\n    margins = y * (Xb @ w)\n    return int(np.sum(margins = 0.0))\n\ndef perceptron_with_momentum(X, y, eta, beta, max_steps):\n    \"\"\"\n    Implements cyclic mistake-driven perceptron with momentum.\n\n    Parameters:\n        X: np.ndarray of shape (n_samples, n_features)\n        y: np.ndarray of shape (n_samples,), labels in {-1, +1}\n        eta: float, learning rate\n        beta: float, momentum coefficient in [0,1)\n        max_steps: int, iteration cap (each iteration visits one example)\n\n    Returns:\n        steps: int, number of mistake-driven updates performed\n        overshoot_ratio: float, overshoot events / steps (rounded to 6 decimals)\n        converged: bool, True if converged before cap\n    \"\"\"\n    n, d = X.shape\n    # Augment with bias term\n    Xb = np.hstack([X, np.ones((n, 1))])\n    w = np.zeros(d + 1, dtype=float)\n    v = np.zeros(d + 1, dtype=float)\n\n    steps = 0  # mistake-driven updates\n    overshoots = 0  # overshoot events only counted on mistake-driven updates\n\n    converged = False\n    for t in range(max_steps):\n        i = t % n\n        margin = y[i] * (np.dot(w, Xb[i]))\n        before_mis = misclassification_count(w, Xb, y)\n\n        if margin = 0.0:\n            # mistake-driven update\n            v = beta * v + eta * y[i] * Xb[i]\n            w_new = w + v\n            after_mis = misclassification_count(w_new, Xb, y)\n            steps += 1\n            if after_mis > before_mis:\n                overshoots += 1\n            w = w_new\n        else:\n            # decay-only update\n            v = beta * v\n            w = w + v\n\n        # check convergence\n        if misclassification_count(w, Xb, y) == 0:\n            converged = True\n            break\n\n    if steps == 0:\n        overshoot_ratio = 0.0\n    else:\n        overshoot_ratio = round(overshoots / steps, 6)\n\n    return steps, overshoot_ratio, converged\n\ndef solve():\n    # Define the datasets as per the problem statement.\n    # Dataset D1: borderline-separable\n    X1 = np.array([\n        [1.0, 1.05], [2.0, 2.05], [3.0, 3.05],  # positives\n        [1.0, 0.95], [2.0, 1.95], [3.0, 2.95]   # negatives\n    ], dtype=float)\n    y1 = np.array([+1, +1, +1, -1, -1, -1], dtype=int)\n\n    # Dataset D2: extremely small margin\n    X2 = np.array([\n        [0.0, 0.001], [1.0, 1.001], [2.0, 2.001],   # positives\n        [0.0, -0.001], [1.0, 0.999], [2.0, 1.999]   # negatives\n    ], dtype=float)\n    y2 = np.array([+1, +1, +1, -1, -1, -1], dtype=int)\n\n    # Dataset D3: non-separable edge case\n    X3 = np.array([\n        [0.0, 0.0], [1.0, 1.0],   # positives\n        [0.0, 0.0], [1.0, 1.0]    # negatives (duplicate locations)\n    ], dtype=float)\n    y3 = np.array([+1, +1, -1, -1], dtype=int)\n\n    # Test cases: (X, y, eta, beta, max_steps)\n    test_cases = [\n        (X1, y1, 0.1, 0.0, 5000),   # Case 1: baseline no momentum\n        (X1, y1, 0.1, 0.9, 5000),   # Case 2: strong momentum on D1\n        (X2, y2, 0.05, 0.95, 8000), # Case 3: tiny margin with strong momentum\n        (X3, y3, 0.1, 0.9, 2000)    # Case 4: non-separable edge case\n    ]\n\n    results = []\n    for X, y, eta, beta, max_steps in test_cases:\n        steps, overshoot_ratio, converged = perceptron_with_momentum(X, y, eta, beta, max_steps)\n        # Ensure overshoot ratio is a float with up to 6 decimals already\n        results.append([steps, overshoot_ratio, converged])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3099401"}, {"introduction": "真实世界的数据往往是不完美的，离群值（outliers）是常见的一种数据噪声。标准的感知机更新规则对具有极大范数的离群点尤其敏感，单个异常点就可能导致决策边界发生剧烈偏移。这个实践练习旨在让你亲手量化这种敏感性，并通过编程实现两种实用的缓解策略：更新剪裁（clipping updates）和稳健归一化（robust normalization），从而加深对算法鲁棒性重要性的理解。[@problem_id:3099471]", "problem": "要求您实现并评估一个由感知机算法训练的线性分类器，该分类器在存在少量大范数离群值的情况下进行训练。目的是量化分类器对这些离群值的敏感性，并测试两种缓解策略：裁剪更新幅度以及基于分布尺度估计的稳健归一化。您的实现必须是一个完整的、可运行的程序，并能产生与指定格式完全一致的输出。\n\n请从以下基本和核心定义开始。线性分类器由一个权重向量 $w \\in \\mathbb{R}^d$ 和一个偏置 $b \\in \\mathbb{R}$ 定义，它通过决策规则 $\\hat{y} = \\mathrm{sign}(w^\\top x + b)$ 对输入 $x \\in \\mathbb{R}^d$ 预测一个标签 $\\hat{y} \\in \\{-1,+1\\}$。感知机学习规则在样本 $(x,y)$ 被误分类时更新参数，即当 $y(w^\\top x + b) \\le 0$ 时，执行 $w \\leftarrow w + y x$ 和 $b \\leftarrow b + y$。已知感知机在线性可分数据上所犯错误的次数有一个上界，该上界取决于数据半径和间隔；但是，您不应假设任何特定的简化公式，而必须通过实现来经验性地测量其行为。\n\n您将在 $d=2$ 维空间中生成具有两个线性可分类别的合成数据。在所有测试用例中，请使用以下构造方法和参数：\n- 设基本尺度为 $R = 1.0$。\n- 将正类中心置于 $\\mu_+ = (3R, 0)$，负类中心置于 $\\mu_- = (-3R, 0)$。\n- 对每个类别，生成 $n_+ = n_- = 50$ 个基点，其坐标为 $x = (\\mu_{\\mathrm{class},1}, u)$，其中对每个点而言 $u$ 是从区间 $[-R, R]$ 中独立均匀采样的，而 $\\mu_{\\mathrm{class},1}$ 是相应类别中心的第一个坐标。这确保了数据可被由 $x_1 = 0$ 定义的超平面线性分离。\n- 离群值：当测试用例指定时，为每个类别添加 $k_+ = k_- = 2$ 个离群值，确定性地放置在正类的 $x = (N\\cdot R, 0)$ 和负类的 $x = (-N\\cdot R, 0)$ 位置，其中 $N$ 是该测试用例特有的离群值范数因子。将离群值按类别交替顺序（正、负、正、负）插入序列的开头，然后附加所有基点（先是所有正类基点，然后是所有负类基点）。这种排序因在训练早期交替出现大范数误分类，从而增加了产生有害更新的可能性。\n- 正类点的标签为 $y=+1$，负类点的标签为 $y=-1$。\n\n需要实现的训练协议：\n- 通过附加一个常数特征 $1$ 来增广输入以加入偏置项，这样更新就可以在增广向量上以向量形式写出。\n- 将权重向量初始化为零向量，偏置初始化为 $0$，这等同于将增广权重向量初始化为零向量。\n- 按照上述固定顺序对数据集进行 $T = 5$ 轮（epochs）完整遍历，轮次之间不进行数据重排。\n- 对每个样本 $(x, y)$，如果 $y \\cdot (w^\\top x + b) \\le 0$，则进行一次更新。更新的具体形式取决于测试用例指定的缓解方法：\n  1. 标准感知机：对原始的 $x$ 执行未经修改的更新 $w \\leftarrow w + y x$ 和 $b \\leftarrow b + y$。\n  2. 裁剪更新：计算欧几里得范数 $\\|x\\|_2$。设 $c$ 为一个正阈值；定义一个缩放因子 $s = \\min\\{1, c / \\|x\\|\\}$。使用 $s x$ 代替 $x$ 应用更新，即 $w \\leftarrow w + y (s x)$ 和 $b \\leftarrow b + y$。这限制了任何单个样本贡献的最大步长幅度。\n  3. 稳健归一化：通过计算整个训练集上的稳健尺度参数 $s_{\\mathrm{rob}} = \\mathrm{median}(\\{\\|x_i\\|_2\\})$ 来预处理所有输入 $x_i$。将每个 $x_i$ 替换为 $\\tilde{x}_i = x_i \\cdot \\frac{s_{\\mathrm{rob}}}{\\max\\{s_{\\mathrm{rob}}, \\|x_i\\|_2\\}}$，然后在归一化后的输入上使用标准感知机更新进行训练。这使得范数 $\\|x_i\\|_2 \\le s_{\\mathrm{rob}}$ 的点保持不变，并将范数较大的点按比例缩小，使其范数最大为 $s_{\\mathrm{rob}}$。\n\n每个测试用例需要报告的指标：\n- 统计在所有 $T$ 轮中进行的总更新次数（等同于总错误次数）。这是一个整数。\n\n您必须在单个程序中实现以上内容，并在以下测试套件上运行它。该套件旨在覆盖一般情况、中度离群值和极端离群值，以及各种缓解策略：\n- 测试用例1：无离群值 ($N = 0$)，标准感知机。\n- 测试用例2：中度离群值，$N = 50$，标准感知机。\n- 测试用例3：中度离群值，$N = 50$，裁剪更新，阈值 $c = R$。\n- 测试用例4：中度离群值，$N = 50$，稳健归一化。\n- 测试用例5：极端离群值，$N = 1000$，标准感知机。\n- 测试用例6：极端离群值，$N = 1000$，裁剪更新，阈值 $c = R/2$。\n\n确定性要求：\n- 在数据生成中对任何随机性使用固定种子 $s_0 = 2025$ 以确保确定性输出。\n- 不要在轮次之间重排数据集。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果，例如 $[r_1, r_2, r_3, r_4, r_5, r_6]$，其中 $r_i$ 是按上述顺序列出的第 $i$ 个测试用例的整数总更新次数。打印的行必须只包含此列表，不得有任何附加文本。", "solution": "该问题要求实现并评估感知机线性分类器，特别关注其对大范数离群值的敏感性以及两种缓解策略的功效。解决方案涉及生成合成数据，实现带有特定修改的感知机算法，并报告在多个测试用例中分类错误（即更新）的总次数。\n\n线性分类器由一个权重向量 $w \\in \\mathbb{R}^d$ 和一个标量偏置 $b \\in \\mathbb{R}$ 定义。对于一个输入向量 $x \\in \\mathbb{R}^d$，它根据规则 $\\hat{y} = \\mathrm{sign}(w^\\top x + b)$ 预测一个标签 $\\hat{y} \\in \\{-1, +1\\}$。问题设置在 $d=2$ 维空间中。感知机学习算法迭代地调整参数 $w$ 和 $b$ 以正确分类训练样本。当一个样本 $(x, y)$ 被误分类时，即满足条件 $y(w^\\top x + b) \\le 0$ 时，参数按如下方式更新：\n$$\nw \\leftarrow w + y x\n$$\n$$\nb \\leftarrow b + y\n$$\n为了记法和计算上的便利，我们可以增广输入向量和权重向量。设增广权重向量为 $\\tilde{w} = [w_1, \\dots, w_d, b]^\\top \\in \\mathbb{R}^{d+1}$，增广输入向量为 $\\tilde{x} = [x_1, \\dots, x_d, 1]^\\top \\in \\mathbb{R}^{d+1}$。决策规则变为 $\\hat{y} = \\mathrm{sign}(\\tilde{w}^\\top \\tilde{x})$，而对于误分类 $y(\\tilde{w}^\\top \\tilde{x}) \\le 0$ 的更新规则简化为单个向量加法：\n$$\n\\tilde{w} \\leftarrow \\tilde{w} + y\\tilde{x}\n$$\n参数被初始化为零，即 $\\tilde{w}_{\\text{init}} = \\mathbf{0}$。\n\n标准感知机更新的一个关键特性是权重向量的变化幅度为 $\\|\\Delta \\tilde{w}\\|_2 = \\|y\\tilde{x}\\|_2 = \\|\\tilde{x}\\|_2 = \\sqrt{\\|x\\|_2^2 + 1}$。这种对输入向量范数的直接依赖性是该算法对离群值敏感的根源。一个具有异常大范数的误分类点将导致不成比例的大更新，可能将决策边界推离最优位置，并导致后续在其他表现良好的数据点上产生许多错误。\n\n本实验旨在展示这一现象。一个基础数据集由两个类别生成，其中心分别位于 $\\mu_+ = (3R, 0)$ 和 $\\mu_- = (-3R, 0)$，基本尺度为 $R=1.0$。每个类别包含 $n=50$ 个形式为 $(\\mu_{\\mathrm{class},1}, u)$ 的点，其中 $u$ 从 $[-R, R]$ 上的均匀分布中抽取。这种构造创建了一个可被垂直轴 $x_1=0$ 线性分离的数据集。在这个干净的数据集上，我们添加了 $k=2$ 对离群值，确定性地位于 $(\\pm N \\cdot R, 0)$。离群值范数因子 $N$ 很大（$N \\in \\{50, 1000\\}$），确保这些点具有非常大的范数。这些离群值被放置在训练序列的开头，以最大化它们对初始为零的权重向量的破坏性效应。\n\n测试了两种缓解策略以与标准感知机进行对比：\n\n1.  **裁剪更新**：该策略直接限制参数更新的幅度。对于一个误分类点 $(x, y)$，根据一个预定义的裁剪阈值 $c0$ 计算一个缩放因子 $s = \\min\\{1, c/\\|x\\|_2\\}$。权重向量 $w$ 通过 $w \\leftarrow w + y(sx)$ 更新，而偏置正常更新，$b \\leftarrow b+y$。这意味着对空间权重的更新被缩放了，但对偏置项的更新没有。这种方法确保了由任何单个样本引起的 $w$ 的变化是有界的。\n\n2.  **稳健归一化**：这是一种在训练开始前应用一次的数据预处理技术。首先，在整个训练集上计算数据尺度的稳健估计 $s_{\\mathrm{rob}} = \\mathrm{median}(\\{\\|x_i\\|_2\\})$。然后，每个输入向量 $x_i$ 被替换为一个归一化版本 $\\tilde{x}_i$：\n    $$\n    \\tilde{x}_i = x_i \\cdot \\frac{s_{\\mathrm{rob}}}{\\max\\{s_{\\mathrm{rob}}, \\|x_i\\|_2\\}}\n    $$\n    这种变换使得范数小于或等于中位范数的点保持不变，同时将所有范数大于中位数的点缩小，使其范数恰好为 $s_{\\mathrm{rob}}$。这有效地“拉回”了大范数的离群值，在学习过程开始前减少了它们的潜在影响。\n\n实现过程将首先为每个测试用例生成指定的数据集。然后，对于“稳健归一化”的情况，数据将被预处理。训练循环在固定顺序的数据集上迭代 $T=5$ 次（轮次）。在每个轮次中，对每个样本进行评估。如果发生错误，总错误计数器会增加，并根据该测试用例指定的方法（“标准”、“裁剪”或“稳健”）更新权重。每个用例的最终输出是累积的总错误次数。整个过程通过在数据生成中使用固定的随机种子 $s_0=2025$ 并且不在轮次之间重排数据来确保其确定性。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef run_single_test_case(params, seed):\n    \"\"\"\n    Runs a single test case for the perceptron algorithm.\n\n    Args:\n        params (dict): A dictionary containing the parameters for the test case,\n                       including 'N', 'method', and 'c'.\n        seed (int): The random seed for reproducibility.\n\n    Returns:\n        int: The total number of updates (mistakes) made during training.\n    \"\"\"\n    np.random.seed(seed)\n    \n    # Define problem constants\n    R = 1.0\n    n = 50\n    k = 2\n    d = 2\n    T = 5\n    \n    # Extract test case parameters\n    N = params['N']\n    method = params['method']\n    c = params['c']\n\n    # --- Data Generation ---\n    # Base points\n    x_pos_base = np.zeros((n, d))\n    x_pos_base[:, 0] = 3 * R\n    x_pos_base[:, 1] = np.random.uniform(-R, R, size=n)\n    y_pos_base = np.ones(n)\n\n    x_neg_base = np.zeros((n, d))\n    x_neg_base[:, 0] = -3 * R\n    x_neg_base[:, 1] = np.random.uniform(-R, R, size=n)\n    y_neg_base = -np.ones(n)\n\n    # Assemble dataset with outliers first, in specified alternating order\n    X_list = []\n    y_list = []\n\n    if N > 0:\n        x_pos_outlier = np.array([N * R, 0.0])\n        x_neg_outlier = np.array([-N * R, 0.0])\n        for _ in range(k):\n            X_list.append(x_pos_outlier)\n            y_list.append(1.0)\n            X_list.append(x_neg_outlier)\n            y_list.append(-1.0)\n    \n    X_list.extend(list(x_pos_base))\n    y_list.extend(list(y_pos_base))\n    X_list.extend(list(x_neg_base))\n    y_list.extend(list(y_neg_base))\n\n    X = np.array(X_list)\n    y = np.array(y_list)\n\n    # --- Pre-processing for Robust Normalization ---\n    if method == 'robust':\n        norms = np.linalg.norm(X, axis=1)\n        # Handle the case where all norms are zero to avoid division by zero\n        s_rob = np.median(norms)\n        if s_rob > 0:\n            scaling_factors = s_rob / np.maximum(s_rob, norms)\n            X = X * scaling_factors[:, np.newaxis]\n\n    # --- Training ---\n    # Augment inputs with a bias term\n    X_aug = np.hstack([X, np.ones((X.shape[0], 1))])\n    \n    # Initialize augmented weight vector\n    w_aug = np.zeros(d + 1)\n    \n    update_count = 0\n\n    for _ in range(T):\n        for i in range(X.shape[0]):\n            x_i_aug = X_aug[i]\n            y_i = y[i]\n            \n            # Check for misclassification\n            if y_i * (w_aug @ x_i_aug) = 0:\n                update_count += 1\n                \n                # Apply update based on the specified method\n                if method == 'standard' or method == 'robust':\n                    w_aug += y_i * x_i_aug\n                elif method == 'clipped':\n                    x_i = X[i]  # Original un-augmented vector for norm calculation\n                    norm_x = np.linalg.norm(x_i)\n                    s = 1.0\n                    if norm_x > 0 and c is not None:\n                        s = min(1.0, c / norm_x)\n                    \n                    # Update weights and bias separately as specified\n                    w_update = y_i * s * x_i\n                    b_update = y_i * 1.0  # Bias update is not scaled\n                    \n                    w_aug[:d] += w_update\n                    w_aug[d] += b_update\n\n    return update_count\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the results.\n    \"\"\"\n    s0 = 2025\n    test_cases = [\n        {'N': 0, 'method': 'standard', 'c': None},\n        {'N': 50, 'method': 'standard', 'c': None},\n        {'N': 50, 'method': 'clipped', 'c': 1.0},\n        {'N': 50, 'method': 'robust', 'c': None},\n        {'N': 1000, 'method': 'standard', 'c': None},\n        {'N': 1000, 'method': 'clipped', 'c': 0.5},\n    ]\n\n    all_results = []\n    for params in test_cases:\n        result = run_single_test_case(params, s0)\n        all_results.append(result)\n\n    print(f\"[{','.join(map(str, all_results))}]\")\n\nif __name__ == '__main__':\n    solve()\n```", "id": "3099471"}]}