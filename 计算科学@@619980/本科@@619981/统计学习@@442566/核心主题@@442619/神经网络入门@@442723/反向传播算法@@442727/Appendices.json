{"hands_on_practices": [{"introduction": "要真正理解反向传播，我们必须深入其内部机制。这个练习将指导您从零开始，为一个简单的标量表达式语言实现反向模式自动微分（Reverse-Mode Automatic Differentiation）——这正是反向传播算法的别称。通过亲手构建一个“计算带”（computational tape）来记录前向传播中的操作，然后逆向回放它以累积梯度，您将直接应用链式法则的核心思想，将反向传播从一个“黑箱”转变为一个具体、可理解的计算过程。[@problem_id:3100018]", "problem": "您的任务是为一个小型标量表达式语言实现反向模式自动微分（AD），并演示如何通过反向回放一个操作记录带（tape）来计算伴随变量（adjoint variables），记作 $\\bar{x} = \\partial L / \\partial x$。在计算图中，反向模式 AD 与反向传播算法是同义的。您的实现必须从第一性原理出发，特别是复合函数的链式法则，以及将计算图定义为一个有向无环图，其中包含基本操作，并以标量损失 $L$ 为根节点。您必须设计一个记录带结构，用于记录基本操作的正向执行过程，然后反向回放此记录带，以利用链式法则累积伴随变量。\n\n您的小型表达式语言必须支持标量变量和常量，以及以下基本操作：二元加法 $+$、二元减法 $-$、二元乘法 $\\cdot$、二元除法 $\\div$、一元正弦 $\\sin(\\cdot)$、一元指数 $\\exp(\\cdot)$ 和一元自然对数 $\\log(\\cdot)$。三角函数中的所有角度都必须以弧度为单位。必须遵守定义域约束，例如 $\\log(\\cdot)$ 的输入必须为正数。您必须设计计算记录带，以记录每个非叶节点操作及其操作数和前向计算值，以确保反向回放的正确性。\n\n您的程序必须：\n- 在评估标量损失 $L$ 时，构建一个内部计算图和记录带。\n- 通过从 $\\bar{L} = \\partial L / \\partial L = 1$ 开始，对记录带进行单次反向回放，为每个输入变量 $x_i$ 计算其伴随变量，即 $\\partial L / \\partial x_i$。\n- 为每个测试用例生成一个列表，其第一个元素是标量损失值 $L$，随后的元素是按引入顺序排列的变量的伴随变量。\n\n仅从基本原理出发：复合函数的链式法则、中间值 $v$ 的伴随变量 $\\bar{v} = \\partial L / \\partial v$ 的定义，以及计算图的语义。不要依赖跳过推导路径的预打包微分公式；相反，应为每个基本操作使用基础微积分推导并实现反向回放所需的局部偏导数。\n\n实现并运行以下测试套件。在每个案例中，按指定顺序定义变量，使用基本操作构建表达式，并计算输出。所有角度均以弧度为单位，本问题不涉及物理单位。\n\n- 测试用例 1 (通用复合)：变量 $x, y$，损失 $L = \\sin(x \\cdot y) + \\exp(y)$，其中 $x = 0.5, y = -1.0$。此用例的输出格式：$[L, \\partial L / \\partial x, \\partial L / \\partial y]$。\n- 测试用例 2 (零和常数的边界情况)：变量 $x$，损失 $L = x \\cdot 0 + \\sin(0) + \\log(1)$，其中 $x = 2.0$。输出格式：$[L, \\partial L / \\partial x]$。\n- 测试用例 3 (重复使用变量)：变量 $x$，损失 $L = (x \\cdot x) \\cdot x$，其中 $x = 2.0$。输出格式：$[L, \\partial L / \\partial x]$。\n- 测试用例 4 (除法和对数)：变量 $x, y$，损失 $L = x \\div y + \\log(y)$，其中 $x = 1.0, y = 1.5$。输出格式：$[L, \\partial L / \\partial x, \\partial L / \\partial y]$。\n- 测试用例 5 (嵌套一元复合)：变量 $x$，损失 $L = \\exp(\\sin(x))$，其中 $x = 0.0$。输出格式：$[L, \\partial L / \\partial x]$。\n\n您的程序应生成单行输出，其中包含所有测试用例的结果，结果为一个逗号分隔的列表，并用方括号括起来，每个测试用例的结果本身也是一个用逗号分隔的列表，并用方括号括起来。例如，两个测试用例的输出应如下所示：$[[L_1,\\partial L_1/\\partial x_1,\\dots],[L_2,\\partial L_2/\\partial x_1,\\dots]]$。您的最终输出必须严格遵循此格式，使用标准的浮点数。", "solution": "该问题要求从第一性原理出发，实现反向模式自动微分（AD），通常也称为反向传播。此方法通过首先对表达式 $L$ 进行前向评估以计算中间值并记录计算图，然后反向遍历该图以根据链式法则传播梯度，从而计算标量损失函数 $L$ 相对于一组输入变量 $x_i$ 的梯度。\n\n**基本原理：链式法则和伴随变量**\n\n反向模式 AD 的基础是微积分的链式法则。如果一个标量损失 $L$ 是中间变量 $v_j$ 的函数，而 $v_j$ 本身是其他变量 $v_i$ 的函数，那么 $L$ 相对于 $v_i$ 的梯度由从 $v_i$ 到 $L$ 的所有路径的贡献之和给出。对于单一路径 $L \\to v_j \\to v_i$，链式法则表述为：\n$$\n\\frac{\\partial L}{\\partial v_i} = \\frac{\\partial L}{\\partial v_j} \\frac{\\partial v_j}{\\partial v_i}\n$$\n在 AD 的术语中，我们将变量 $v$ 的“伴随变量”（adjoint）定义为 $\\bar{v} \\equiv \\frac{\\partial L}{\\partial v}$。使用此表示法，链式法则变为：\n$$\n\\bar{v}_i = \\bar{v}_j \\frac{\\partial v_j}{\\partial v_i}\n$$\n反向模式 AD 算法利用了这一关系，首先计算 $L$ 的值，然后将伴随变量从 $L$ 反向传播到输入变量。该过程从设定损失函数自身的伴随变量种子开始，即 $\\bar{L} = \\frac{\\partial L}{\\partial L} = 1$。\n\n**计算图与记录带**\n\n任何标量表达式都可以分解为一系列基本操作（例如，加法、乘法、正弦）。这种分解自然形成一个有向无环图（DAG），其中节点表示数值（输入变量、常量和中间结果），边表示基本操作。\n\n从输入到最终损失 $L$ 的表达式前向评估过程用于构建此图。在我们的实现中，我们使用一种“记录带”（tape）数据结构，它是图的线性化表示。记录带是在前向传播期间记录的操作的有序列表。记录带上的每个条目都存储了操作类型、对其输入节点的引用以及对其输出节点的引用。这种记录方式确保我们拥有完整的结构和所有必要的中间值以进行反向传播。\n\n**前向传播：求值与记录**\n\n前向传播过程如下：\n1. 输入变量和常量被初始化为我们图中的起始节点。\n2. 表达式被顺序求值。每当应用一个基本操作时，会发生两件事：\n    a. 计算该操作的数值结果并将其作为图中的一个新节点存储。\n    b. 向记录带中添加一个条目，记录操作类型、其输入节点以及新创建的输出节点。\n\n例如，对于表达式 $z = x \\cdot y$，我们会使用 $x$ 和 $y$ 的当前值计算出 $z$ 的值，为 $z$ 创建一个新节点，并在记录带上记录 `('mul', [x_node, y_node], z_node)`。\n\n**反向传播：伴随变量累积**\n\n一旦前向传播完成并计算出最终的损失值 $L$，反向传播就开始了。它按照记录带创建顺序的逆序进行遍历。\n1. 创建一个伴随变量数组，对应图中的每个节点，并初始化为零。\n2. 最终损失节点的伴随变量被设置为 $1$，即 $\\bar{L} = 1$。\n3. 对于记录带上的每个操作 $z = f(x_1, \\dots, x_n)$（按相反顺序处理）：\n    a. 我们检索已经计算出的输出的伴随变量 $\\bar{z}$。\n    b. 我们使用链式法则计算 $\\bar{z}$ 对输入伴随变量的贡献。每个输入 $x_i$ 的伴随变量通过累积此贡献进行更新：\n    $$\n    \\bar{x}_i \\mathrel{+}= \\bar{z} \\cdot \\frac{\\partial z}{\\partial x_i}\n    $$\n    使用累加（$\\mathrel{+}=$）至关重要，因为单个变量可能在多个操作中使用（即，在图中它可以是多个子节点的父节点）。其总伴随变量是从其所有子节点回传的梯度信号的总和。反向回放记录带可以保证一个节点的伴随变量（$\\bar{z}$）在其被传播到其自身输入（$x_i$）之前已完全计算好。\n\n**基本操作的伴随变量更新规则**\n\n每个基本操作的局部偏导数 $\\frac{\\partial z}{\\partial x_i}$ 是已知的。计算这些导数所需的输入值（例如，对于 $z = x \\cdot y$，$\\frac{\\partial z}{\\partial x} = y$）可从前向传播中获得。\n\n- **加法：** $z = x + y$\n  - $\\frac{\\partial z}{\\partial x} = 1$, $\\frac{\\partial z}{\\partial y} = 1$.\n  - 更新规则：$\\bar{x} \\mathrel{+}= \\bar{z}$, $\\bar{y} \\mathrel{+}= \\bar{z}$。\n\n- **减法：** $z = x - y$\n  - $\\frac{\\partial z}{\\partial x} = 1$, $\\frac{\\partial z}{\\partial y} = -1$.\n  - 更新规则：$\\bar{x} \\mathrel{+}= \\bar{z}$, $\\bar{y} \\mathrel{+}= -\\bar{z}$。\n\n- **乘法：** $z = x \\cdot y$\n  - $\\frac{\\partial z}{\\partial x} = y$, $\\frac{\\partial z}{\\partial y} = x$.\n  - 更新规则：$\\bar{x} \\mathrel{+}= \\bar{z} \\cdot y$, $\\bar{y} \\mathrel{+}= \\bar{z} \\cdot x$。\n\n- **除法：** $z = x \\div y$\n  - $\\frac{\\partial z}{\\partial x} = \\frac{1}{y}$, $\\frac{\\partial z}{\\partial y} = -\\frac{x}{y^2}$.\n  - 更新规则：$\\bar{x} \\mathrel{+}= \\bar{z} \\cdot \\frac{1}{y}$, $\\bar{y} \\mathrel{+}= \\bar{z} \\cdot \\left(-\\frac{x}{y^2}\\right)$。\n\n- **正弦：** $z = \\sin(x)$\n  - $\\frac{dz}{dx} = \\cos(x)$.\n  - 更新规则：$\\bar{x} \\mathrel{+}= \\bar{z} \\cdot \\cos(x)$。\n\n- **指数：** $z = \\exp(x)$\n  - $\\frac{dz}{dx} = \\exp(x) = z$.\n  - 更新规则：$\\bar{x} \\mathrel{+}= \\bar{z} \\cdot z$。\n\n- **自然对数：** $z = \\log(x)$\n  - $\\frac{dz}{dx} = \\frac{1}{x}$.\n  - 更新规则：$\\bar{x} \\mathrel{+}= \\bar{z} \\cdot \\frac{1}{x}$。\n\n**实现设计**\n\n该实现使用两个主要类：`Graph` 和 `Node`。`Graph` 类管理计算的状态：它存储所有节点的 `values`（值）、操作 `tape`（记录带）以及计算出的 `adjoints`（伴随变量）。`Node` 类作为节点 ID 的一个包装器，通过重载 Python 的算术运算符（`+`, `*` 等）提供了一个直观的接口。当对 `Node` 对象执行像 `c = a + b` 这样的操作时，它会透明地调用关联 `Graph` 对象上的一个方法，该方法执行前向计算，将操作记录在记录带上，并为结果 `c` 返回一个新的 `Node`。这种面向对象的设计允许以自然的方式构建表达式，同时在后台正确地构建计算图。在计算出最终的损失 `Node` 后，调用 `Graph.compute_gradients()` 会执行如上所述的反向传播过程。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\n# A consistent execution environment requires no user input and all dependencies declared.\n\n# Define unary functions that can operate on Node objects or raw numbers\ndef sin(node):\n    \"\"\"Computes sine, handling both Node objects and numeric types.\"\"\"\n    if isinstance(node, Node):\n        return node.graph.sin(node)\n    return np.sin(node)\n\ndef exp(node):\n    \"\"\"Computes exponential, handling both Node objects and numeric types.\"\"\"\n    if isinstance(node, Node):\n        return node.graph.exp(node)\n    return np.exp(node)\n\ndef log(node):\n    \"\"\"Computes natural logarithm, handling both Node objects and numeric types.\"\"\"\n    if isinstance(node, Node):\n        return node.graph.log(node)\n    return np.log(node)\n\nclass Graph:\n    \"\"\"Manages the computational graph, tape, and differentiation process.\"\"\"\n    def __init__(self):\n        # A linearized list of operations representing the computational graph.\n        # Each entry is a tuple: (op_type, [input_node_ids], output_node_id)\n        self.tape = []\n        # Stores the numerical value of each node computed during the forward pass.\n        self.values = []\n        # Stores the adjoint (dL/dv) for each node, computed during the reverse pass.\n        self.adjoints = None\n\n    def _add_node(self, value):\n        \"\"\"Adds a new node (value) to the graph and returns its ID.\"\"\"\n        node_id = len(self.values)\n        self.values.append(value)\n        return node_id\n\n    def variable(self, value):\n        \"\"\"Creates a variable node, which is a leaf in the graph.\"\"\"\n        node_id = self._add_node(value)\n        return Node(self, node_id)\n\n    def _promote_to_node(self, other):\n        \"\"\"Promotes a numeric constant to a Node to allow operations like `x + 5`.\"\"\"\n        if not isinstance(other, Node):\n            # Treat numeric constants as new variable nodes in the graph.\n            return self.variable(other)\n        return other\n\n    # Methods for primitive operations (Forward Pass)\n    def add(self, n1, n2):\n        res_val = self.values[n1.node_id] + self.values[n2.node_id]\n        res_id = self._add_node(res_val)\n        self.tape.append(('add', [n1.node_id, n2.node_id], res_id))\n        return Node(self, res_id)\n\n    def sub(self, n1, n2):\n        res_val = self.values[n1.node_id] - self.values[n2.node_id]\n        res_id = self._add_node(res_val)\n        self.tape.append(('sub', [n1.node_id, n2.node_id], res_id))\n        return Node(self, res_id)\n\n    def mul(self, n1, n2):\n        res_val = self.values[n1.node_id] * self.values[n2.node_id]\n        res_id = self._add_node(res_val)\n        self.tape.append(('mul', [n1.node_id, n2.node_id], res_id))\n        return Node(self, res_id)\n\n    def truediv(self, n1, n2):\n        res_val = self.values[n1.node_id] / self.values[n2.node_id]\n        res_id = self._add_node(res_val)\n        self.tape.append(('div', [n1.node_id, n2.node_id], res_id))\n        return Node(self, res_id)\n\n    def sin(self, n1):\n        res_val = np.sin(self.values[n1.node_id])\n        res_id = self._add_node(res_val)\n        self.tape.append(('sin', [n1.node_id], res_id))\n        return Node(self, res_id)\n\n    def exp(self, n1):\n        res_val = np.exp(self.values[n1.node_id])\n        res_id = self._add_node(res_val)\n        self.tape.append(('exp', [n1.node_id], res_id))\n        return Node(self, res_id)\n\n    def log(self, n1):\n        value = self.values[n1.node_id]\n        if value = 0:\n            raise ValueError(\"Domain error: input to log must be positive.\")\n        res_val = np.log(value)\n        res_id = self._add_node(res_val)\n        self.tape.append(('log', [n1.node_id], res_id))\n        return Node(self, res_id)\n\n    def compute_gradients(self, loss_node):\n        \"\"\"Performs the reverse pass to compute gradients for all nodes.\"\"\"\n        num_nodes = len(self.values)\n        self.adjoints = np.zeros(num_nodes)\n        self.adjoints[loss_node.node_id] = 1.0  # Seed the reverse pass\n\n        # Replay the tape in reverse to propagate adjoints\n        for op_type, input_ids, output_id in reversed(self.tape):\n            adjoint_out = self.adjoints[output_id]\n            \n            if adjoint_out == 0.0:  # Optimization: no gradient to propagate\n                continue\n\n            # Apply the chain rule based on the operation\n            if op_type == 'add':\n                self.adjoints[input_ids[0]] += adjoint_out\n                self.adjoints[input_ids[1]] += adjoint_out\n            elif op_type == 'sub':\n                self.adjoints[input_ids[0]] += adjoint_out\n                self.adjoints[input_ids[1]] -= adjoint_out\n            elif op_type == 'mul':\n                val0 = self.values[input_ids[0]]\n                val1 = self.values[input_ids[1]]\n                self.adjoints[input_ids[0]] += adjoint_out * val1\n                self.adjoints[input_ids[1]] += adjoint_out * val0\n            elif op_type == 'div':\n                val0 = self.values[input_ids[0]]\n                val1 = self.values[input_ids[1]]\n                self.adjoints[input_ids[0]] += adjoint_out / val1\n                self.adjoints[input_ids[1]] -= adjoint_out * val0 / (val1**2)\n            elif op_type == 'sin':\n                val0 = self.values[input_ids[0]]\n                self.adjoints[input_ids[0]] += adjoint_out * np.cos(val0)\n            elif op_type == 'exp':\n                out_val = self.values[output_id] # d/dx(exp(x)) = exp(x)\n                self.adjoints[input_ids[0]] += adjoint_out * out_val\n            elif op_type == 'log':\n                val0 = self.values[input_ids[0]]\n                self.adjoints[input_ids[0]] += adjoint_out / val0\n\nclass Node:\n    \"\"\"A node in the computational graph, with overloaded operators.\"\"\"\n    def __init__(self, graph, node_id):\n        self.graph = graph\n        self.node_id = node_id\n\n    @property\n    def value(self):\n        \"\"\"Get the node's numerical value from its graph.\"\"\"\n        return self.graph.values[self.node_id]\n        \n    @property\n    def adjoint(self):\n        \"\"\"Get the node's adjoint after the reverse pass.\"\"\"\n        if self.graph.adjoints is None:\n            raise RuntimeError(\"Gradients not computed yet. Call `compute_gradients` on the loss node first.\")\n        return self.graph.adjoints[self.node_id]\n    \n    # Left-side binary operators\n    def __add__(self, other):\n        other = self.graph._promote_to_node(other)\n        return self.graph.add(self, other)\n\n    def __sub__(self, other):\n        other = self.graph._promote_to_node(other)\n        return self.graph.sub(self, other)\n\n    def __mul__(self, other):\n        other = self.graph._promote_to_node(other)\n        return self.graph.mul(self, other)\n\n    def __truediv__(self, other):\n        other = self.graph._promote_to_node(other)\n        return self.graph.truediv(self, other)\n\n    # Right-side binary operators (for expressions like `5 + x`)\n    def __radd__(self, other):\n        other = self.graph._promote_to_node(other)\n        return self.graph.add(other, self)\n\n    def __rsub__(self, other):\n        other = self.graph._promote_to_node(other)\n        return self.graph.sub(other, self)\n\n    def __rmul__(self, other):\n        other = self.graph._promote_to_node(other)\n        return self.graph.mul(other, self)\n\n    def __rtruediv__(self, other):\n        other = self.graph._promote_to_node(other)\n        return self.graph.truediv(other, self)\n\n    def __repr__(self):\n        return f\"Node(id={self.node_id}, value={self.value:.4f})\"\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1: L = sin(x*y) + exp(y), x=0.5, y=-1.0\n        {'vars': {'x': 0.5, 'y': -1.0}, 'expr': lambda x, y: sin(x * y) + exp(y)},\n        # Case 2: L = x*0 + sin(0) + log(1), x=2.0\n        {'vars': {'x': 2.0}, 'expr': lambda x: x * 0.0 + sin(0.0) + log(1.0)},\n        # Case 3: L = (x*x)*x, x=2.0\n        {'vars': {'x': 2.0}, 'expr': lambda x: (x * x) * x},\n        # Case 4: L = x/y + log(y), x=1.0, y=1.5\n        {'vars': {'x': 1.0, 'y': 1.5}, 'expr': lambda x, y: x / y + log(y)},\n        # Case 5: L = exp(sin(x)), x=0.0\n        {'vars': {'x': 0.0}, 'expr': lambda x: exp(sin(x))},\n    ]\n\n    all_results = []\n    for case in test_cases:\n        g = Graph()\n        # Create variable nodes in the specified order (determined by dict insertion order in Python 3.7+)\n        var_nodes = {name: g.variable(val) for name, val in case['vars'].items()}\n        \n        # Build the graph by executing the expression\n        loss_node = case['expr'](**var_nodes)\n        \n        # Compute gradients via reverse-mode AD\n        g.compute_gradients(loss_node)\n        \n        # Collect results: [L, dL/dx1, dL/dx2, ...]\n        case_result = [loss_node.value]\n        for node in var_nodes.values():\n            case_result.append(node.adjoint)\n        \n        all_results.append(f\"[{','.join(map(str, case_result))}]\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(all_results)}]\")\n\nsolve()\n\n```", "id": "3100018"}, {"introduction": "在掌握了反向传播的基本机制后，让我们将其应用于神经网络的一个基本构件：单个线性神经元。这个问题将算法与模型拟合及优化的目标联系起来。您将运用反向传播（即链式法则）来计算损失函数相对于模型参数的梯度，并通过分析海森矩阵（Hessian matrix）来探索损失曲面的几何形状，从而加深对临界点、局部最小值等优化概念的理解。[@problem_id:3099996]", "problem": "给定一个具有线性激活的单神经元模型，由参数化函数 $f(x; \\theta) = W x + b$ 定义，其中 $\\theta = (W, b)$，$W \\in \\mathbb{R}$ 且 $b \\in \\mathbb{R}$。训练集包含三个输入输出对 $(x_i, y_i)$，$i = 1, 2, 3$，具体为 $(x_1, y_1) = (0, 1)$、$(x_2, y_2) = (1, 3)$ 和 $(x_3, y_3) = (2, 5)$。经验风险是平方误差和的一半，定义为\n$$\nJ(\\theta) = \\frac{1}{2} \\sum_{i=1}^{3} \\left(f(x_i; \\theta) - y_i\\right)^{2}.\n$$\n从微积分的链式法则的基本定义以及梯度和海森矩阵（二阶偏导数矩阵）的定义出发，完成以下任务：\n1. 选择参数 $W$ 和 $b$ 以精确拟合这三个数据点，即对于所有 $i \\in \\{1, 2, 3\\}$，都有 $f(x_i; \\theta) = y_i$。\n2. 使用反向传播（即应用于模型计算图的链式法则），推导梯度 $\\nabla_{\\theta} J(\\theta)$，并在第1部分中选择的精确拟合参数处求值。\n3. 推导 $J(\\theta)$ 关于 $\\theta$ 的海森矩阵 $H(\\theta)$，并在精确拟合参数处求值。计算最小特征值 $\\lambda_{\\min}(H)$。\n4. 根据 $\\lambda_{\\min}(H)$ 的符号，简要说明该精确拟合点是 $J(\\theta)$ 的局部最小值还是鞍点。\n\n给出在解处 $\\lambda_{\\min}(H)$ 的精确值作为最终答案。无需四舍五入。", "solution": "任务是分析单个线性神经元模型 $f(x; \\theta) = W x + b$（参数为 $\\theta = (W, b)$）的经验风险函数 $J(\\theta)$。风险定义为三个数据点 $(x_1, y_1) = (0, 1)$、$(x_2, y_2) = (1, 3)$ 和 $(x_3, y_3) = (2, 5)$ 上的平方误差和的一半。风险函数为：\n$$\nJ(W, b) = \\frac{1}{2} \\sum_{i=1}^{3} \\left( (Wx_i + b) - y_i \\right)^{2}\n$$\n代入给定的数据点：\n$$\nJ(W, b) = \\frac{1}{2} \\left[ (W(0) + b - 1)^{2} + (W(1) + b - 3)^{2} + (W(2) + b - 5)^{2} \\right]\n$$\n$$\nJ(W, b) = \\frac{1}{2} \\left[ (b - 1)^{2} + (W + b - 3)^{2} + (2W + b - 5)^{2} \\right]\n$$\n\n**1. 寻找精确拟合参数 $\\theta^* = (W, b)$**\n\n为了精确拟合，模型必须对所有 $i \\in \\{1, 2, 3\\}$ 满足 $f(x_i; \\theta) = y_i$。这为 $W$ 和 $b$ 产生了一个线性方程组：\n\\begin{enumerate}\n    \\item 对于 $(x_1, y_1) = (0, 1)$：$W(0) + b = 1 \\implies b = 1$。\n    \\item 对于 $(x_2, y_2) = (1, 3)$：$W(1) + b = 3 \\implies W + b = 3$。\n    \\item 对于 $(x_3, y_3) = (2, 5)$：$W(2) + b = 5 \\implies 2W + b = 5$。\n\\end{enumerate}\n将第一个方程的 $b=1$ 代入第二个方程，得到 $W+1=3$，这意味着 $W=2$。\n我们必须验证这些值是否满足第三个方程：$2W + b = 2(2) + 1 = 4 + 1 = 5$，这与 $y_3=5$ 一致。\n因此，精确拟合的参数是 $W=2$ 和 $b=1$。我们将此点表示为 $\\theta^* = (2, 1)$。\n\n**2. 推导梯度 $\\nabla_{\\theta} J(\\theta)$ 并在 $\\theta^*$ 处求值**\n\n$J(\\theta)$ 关于 $\\theta = (W, b)$ 的梯度是 $\\nabla_{\\theta} J = \\begin{pmatrix} \\frac{\\partial J}{\\partial W} \\\\ \\frac{\\partial J}{\\partial b} \\end{pmatrix}$。\n根据反向传播方法的要求，使用链式法则，我们将每个点的误差定义为 $e_i(\\theta) = f(x_i; \\theta) - y_i = Wx_i + b - y_i$。损失为 $J = \\frac{1}{2} \\sum_{i=1}^3 e_i^2$。\n偏导数是：\n$$\n\\frac{\\partial J}{\\partial W} = \\sum_{i=1}^{3} \\frac{\\partial J}{\\partial e_i} \\frac{\\partial e_i}{\\partial W} = \\sum_{i=1}^{3} e_i \\cdot x_i = \\sum_{i=1}^{3} (Wx_i + b - y_i) x_i\n$$\n$$\n\\frac{\\partial J}{\\partial b} = \\sum_{i=1}^{3} \\frac{\\partial J}{\\partial e_i} \\frac{\\partial e_i}{\\partial b} = \\sum_{i=1}^{3} e_i \\cdot 1 = \\sum_{i=1}^{3} (Wx_i + b - y_i)\n$$\n在精确拟合点 $\\theta^* = (2, 1)$ 处，根据定义，误差项为零：对于所有 $i$，都有 $e_i(\\theta^*) = Wx_i + b - y_i = 0$。\n因此，在 $\\theta^*$ 处计算梯度：\n$$\n\\frac{\\partial J}{\\partial W}\\bigg|_{\\theta^*} = \\sum_{i=1}^{3} (0) x_i = 0\n$$\n$$\n\\frac{\\partial J}{\\partial b}\\bigg|_{\\theta^*} = \\sum_{i=1}^{3} (0) = 0\n$$\n在精确拟合点的梯度是零向量：$\\nabla_{\\theta} J(\\theta^*) = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$。这证实了 $\\theta^*$ 是损失函数 $J(\\theta)$ 的一个临界点。\n\n**3. 推导海森矩阵 $H(\\theta)$ 并计算其最小特征值**\n\n海森矩阵 $H(\\theta)$ 包含 $J(\\theta)$ 的二阶偏导数：\n$$\nH(\\theta) = \\begin{pmatrix} \\frac{\\partial^2 J}{\\partial W^2}  \\frac{\\partial^2 J}{\\partial W \\partial b} \\\\ \\frac{\\partial^2 J}{\\partial b \\partial W}  \\frac{\\partial^2 J}{\\partial b^2} \\end{pmatrix}\n$$\n我们通过对一阶偏导数求导来计算它们：\n$$\n\\frac{\\partial^2 J}{\\partial W^2} = \\frac{\\partial}{\\partial W} \\left[ \\sum_{i=1}^{3} (Wx_i + b - y_i) x_i \\right] = \\sum_{i=1}^{3} x_i^2\n$$\n$$\n\\frac{\\partial^2 J}{\\partial b \\partial W} = \\frac{\\partial}{\\partial b} \\left[ \\sum_{i=1}^{3} (Wx_i + b - y_i) x_i \\right] = \\sum_{i=1}^{3} x_i\n$$\n$$\n\\frac{\\partial^2 J}{\\partial b^2} = \\frac{\\partial}{\\partial b} \\left[ \\sum_{i=1}^{3} (Wx_i + b - y_i) \\right] = \\sum_{i=1}^{3} 1 = 3\n$$\n注意，正如预期的，$\\frac{\\partial^2 J}{\\partial W \\partial b} = \\frac{\\partial^2 J}{\\partial b \\partial W}$。海森矩阵是常数，不依赖于 $W$ 或 $b$。我们使用给定的输入 $x_1=0$、$x_2=1$、$x_3=2$ 来计算这些和：\n$$\n\\sum_{i=1}^{3} x_i^2 = 0^2 + 1^2 + 2^2 = 0 + 1 + 4 = 5\n$$\n$$\n\\sum_{i=1}^{3} x_i = 0 + 1 + 2 = 3\n$$\n海森矩阵为：\n$$\nH = \\begin{pmatrix} 5  3 \\\\ 3  3 \\end{pmatrix}\n$$\n$H$ 的特征值 $\\lambda$ 是特征方程 $\\det(H - \\lambda I) = 0$ 的根：\n$$\n\\det \\begin{pmatrix} 5-\\lambda  3 \\\\ 3  3-\\lambda \\end{pmatrix} = (5-\\lambda)(3-\\lambda) - (3)(3) = 0\n$$\n$$\n15 - 8\\lambda + \\lambda^2 - 9 = 0\n$$\n$$\n\\lambda^2 - 8\\lambda + 6 = 0\n$$\n使用二次公式 $\\lambda = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}$：\n$$\n\\lambda = \\frac{8 \\pm \\sqrt{(-8)^2 - 4(1)(6)}}{2} = \\frac{8 \\pm \\sqrt{64 - 24}}{2} = \\frac{8 \\pm \\sqrt{40}}{2}\n$$\n化简 $\\sqrt{40} = \\sqrt{4 \\cdot 10} = 2\\sqrt{10}$：\n$$\n\\lambda = \\frac{8 \\pm 2\\sqrt{10}}{2} = 4 \\pm \\sqrt{10}\n$$\n两个特征值是 $\\lambda_1 = 4 + \\sqrt{10}$ 和 $\\lambda_2 = 4 - \\sqrt{10}$。最小特征值是 $\\lambda_{\\min}(H) = 4 - \\sqrt{10}$。\n\n**4. 对临界点 $\\theta^*$ 进行分类**\n\n为了对临界点 $\\theta^*$ 进行分类，我们检查在该点计算的海森矩阵的特征值的符号。由于 $H$ 是常数，我们使用刚才计算出的特征值。\n我们知道 $3 = \\sqrt{9}  \\sqrt{10}  \\sqrt{16} = 4$。\n因此，最小特征值 $\\lambda_{\\min}(H) = 4 - \\sqrt{10}$ 是正的，因为 $4  \\sqrt{10}$。\n最大特征值 $\\lambda_{\\max}(H) = 4 + \\sqrt{10}$ 也显然是正的。\n由于海森矩阵的两个特征值都是正的，所以海森矩阵是正定的。根据二阶偏导数检验，海森矩阵为正定的临界点是局部最小值。对于这个二次损失函数，它也是唯一的全局最小值。该精确拟合点是一个局部最小值。\n最终答案是最小特征值的值。", "answer": "$$\n\\boxed{4 - \\sqrt{10}}\n$$", "id": "3099996"}, {"introduction": "现代神经网络依赖于专门的层和损失函数，而softmax激活函数与交叉熵损失的组合是分类任务中最常见的配对之一。您将推导交叉熵损失关于激活前“logits”的梯度，并发现一个出乎意料的简洁形式：$p - y$，其中 $p$ 是预测概率，$y$ 是真实标签。这个推导对于理解分类模型至关重要，并能让您洞察现代深度学习框架在数值稳定性和计算效率方面的设计考量。[@problem_id:3101047]", "problem": "一个多类分类器产生一个 logits 向量 $\\mathbf{z} \\in \\mathbb{R}^{K}$，其中预测的类概率由 softmax 函数给出，$p_{k}(\\mathbf{z}) = \\frac{\\exp(z_{k})}{\\sum_{j=1}^{K} \\exp(z_{j})}$，对于 $k \\in \\{1,\\dots,K\\}$。对于一个表示有效类分布（例如，一个独热向量）的目标标签向量 $\\mathbf{y} \\in \\mathbb{R}^{K}$，满足 $\\sum_{k=1}^{K} y_{k} = 1$ 和 $y_{k} \\ge 0$，交叉熵损失定义为 $L(\\mathbf{z};\\mathbf{y}) = -\\sum_{k=1}^{K} y_{k} \\ln p_{k}(\\mathbf{z})$。仅使用这些定义和标准微积分（如链式法则），推导梯度 $\\nabla_{\\mathbf{z}} L(\\mathbf{z};\\mathbf{y})$ 关于 $\\mathbf{z}$ 和 $\\mathbf{y}$ 的简化闭式表达式。然后，基于 log-sum-exp (LSE) 技巧，使用恒等式 $\\ln\\!\\left(\\sum_{j=1}^{K} \\exp(z_{j})\\right)$ 以数值稳定的方式重写该损失函数，并解释为什么这种稳定形式不会改变其关于 $\\mathbf{z}$ 的梯度。最后，对特定情况 $K = 3$，logits $\\mathbf{z} = (2,-1,0.5)$，以及独热目标 $\\mathbf{y} = (1,0,0)$ 计算梯度值。将你最终的梯度向量四舍五入到 $4$ 位有效数字。", "solution": "### 梯度推导\n\n交叉熵损失函数由下式给出：\n$$L(\\mathbf{z};\\mathbf{y}) = -\\sum_{k=1}^{K} y_{k} \\ln p_{k}(\\mathbf{z})$$\n我们旨在计算梯度 $\\nabla_{\\mathbf{z}} L$，其分量为偏导数 $\\frac{\\partial L}{\\partial z_i}$，其中 $i \\in \\{1, \\dots, K\\}$。\n\n使用链式法则，$L$ 对于任意 logit $z_i$ 的偏导数为：\n$$\\frac{\\partial L}{\\partial z_i} = -\\sum_{k=1}^{K} y_k \\frac{\\partial}{\\partial z_i} (\\ln p_k) = -\\sum_{k=1}^{K} \\frac{y_k}{p_k} \\frac{\\partial p_k}{\\partial z_i}$$\n\n接下来，我们必须求 softmax 函数 $p_k$ 关于 $z_i$ 的偏导数。Softmax 函数为 $p_k(\\mathbf{z}) = \\frac{\\exp(z_k)}{\\sum_{j=1}^{K} \\exp(z_j)}$。令 $N_k = \\exp(z_k)$ 且 $D = \\sum_{j=1}^{K} \\exp(z_j)$，因此 $p_k = N_k/D$。\n\n我们使用商法则，分两种情况讨论导数 $\\frac{\\partial p_k}{\\partial z_i}$。\n\n情况 1：$i = k$。\n$$\\frac{\\partial p_k}{\\partial z_k} = \\frac{\\frac{\\partial N_k}{\\partial z_k} D - N_k \\frac{\\partial D}{\\partial z_k}}{D^2} = \\frac{\\exp(z_k) \\left(\\sum_j \\exp(z_j)\\right) - \\exp(z_k) \\exp(z_k)}{\\left(\\sum_j \\exp(z_j)\\right)^2}$$\n$$\\frac{\\partial p_k}{\\partial z_k} = \\frac{\\exp(z_k)}{\\sum_j \\exp(z_j)} - \\left(\\frac{\\exp(z_k)}{\\sum_j \\exp(z_j)}\\right)^2 = p_k - p_k^2 = p_k(1-p_k)$$\n\n情况 2：$i \\neq k$。\n$$\\frac{\\partial p_k}{\\partial z_i} = \\frac{\\frac{\\partial N_k}{\\partial z_i} D - N_k \\frac{\\partial D}{\\partial z_i}}{D^2} = \\frac{0 \\cdot D - \\exp(z_k) \\exp(z_i)}{\\left(\\sum_j \\exp(z_j)\\right)^2}$$\n$$\\frac{\\partial p_k}{\\partial z_i} = -\\frac{\\exp(z_k)}{\\sum_j \\exp(z_j)} \\frac{\\exp(z_i)}{\\sum_j \\exp(z_j)} = -p_k p_i$$\n\n这两种情况可以使用克罗内克 δ 符号 $\\delta_{ik}$ 紧凑地写出，当 $i=k$ 时为 $1$，否则为 $0$：\n$$\\frac{\\partial p_k}{\\partial z_i} = p_k \\delta_{ik} - p_k p_i = p_k(\\delta_{ik} - p_i)$$\n该表达式代表 softmax 函数的雅可比矩阵。\n\n将此代回损失函数的导数中：\n$$\\frac{\\partial L}{\\partial z_i} = -\\sum_{k=1}^{K} \\frac{y_k}{p_k} \\left( p_k(\\delta_{ik} - p_i) \\right) = -\\sum_{k=1}^{K} y_k (\\delta_{ik} - p_i)$$\n分配求和符号：\n$$\\frac{\\partial L}{\\partial z_i} = -\\left( \\sum_{k=1}^{K} y_k \\delta_{ik} - \\sum_{k=1}^{K} y_k p_i \\right)$$\n第一项可以简化，因为只有当 $k=i$ 时 $\\delta_{ik}$ 才非零：$\\sum_{k=1}^{K} y_k \\delta_{ik} = y_i$。\n对于第二项，$p_i$ 相对于求和索引 $k$ 是常数，因此可以提取出来：$\\sum_{k=1}^{K} y_k p_i = p_i \\sum_{k=1}^{K} y_k$。\n根据约束条件 $\\sum_{k=1}^{K} y_k = 1$，第二项变为 $p_i \\cdot 1 = p_i$。\n\n将这些结果代回：\n$$\\frac{\\partial L}{\\partial z_i} = -(y_i - p_i) = p_i - y_i$$\n这个优雅简洁的结果对所有 $i \\in \\{1, \\dots, K\\}$ 都成立。因此，梯度向量是预测概率向量与目标向量之差：\n$$\\nabla_{\\mathbf{z}} L(\\mathbf{z};\\mathbf{y}) = \\mathbf{p}(\\mathbf{z}) - \\mathbf{y}$$\n\n### 损失的数值稳定形式\n\n损失函数 $L = -\\sum_{k=1}^{K} y_{k} \\ln p_{k}$ 可以通过代入 $p_k$ 的定义来重写：\n$$L = -\\sum_{k=1}^{K} y_{k} \\ln \\left( \\frac{\\exp(z_{k})}{\\sum_{j=1}^{K} \\exp(z_{j})} \\right) = -\\sum_{k=1}^{K} y_{k} \\left( \\ln(\\exp(z_k)) - \\ln\\left(\\sum_{j=1}^{K} \\exp(z_{j})\\right) \\right)$$\n$$L = -\\sum_{k=1}^{K} y_{k} \\left( z_k - \\ln\\left(\\sum_{j=1}^{K} \\exp(z_{j})\\right) \\right)$$\n$$L = -\\sum_{k=1}^{K} y_{k} z_k + \\left(\\sum_{k=1}^{K} y_k\\right) \\ln\\left(\\sum_{j=1}^{K} \\exp(z_{j})\\right)$$\n使用 $\\sum y_k = 1$，我们得到：\n$$L = -\\sum_{k=1}^{K} y_{k} z_k + \\ln\\left(\\sum_{j=1}^{K} \\exp(z_{j})\\right)$$\n项 $\\ln\\left(\\sum_{j=1}^{K} \\exp(z_{j})\\right)$ 被称为 log-sum-exp (LSE) 函数，记为 $\\text{LSE}(\\mathbf{z})$。\n\n对于较大的 $z_j$ 值，$\\exp(z_j)$ 可能会超出标准浮点表示的范围。LSE 技巧为其计算提供了一种数值稳定的方法。令 $z_{\\max} = \\max_j z_j$。我们可以将 LSE 项重写为：\n$$\\text{LSE}(\\mathbf{z}) = \\ln\\left(\\sum_{j=1}^{K} \\exp(z_j)\\right) = \\ln\\left(\\exp(z_{\\max}) \\sum_{j=1}^{K} \\exp(z_j-z_{\\max})\\right)$$\n$$= \\ln(\\exp(z_{\\max})) + \\ln\\left(\\sum_{j=1}^{K} \\exp(z_j-z_{\\max})\\right) = z_{\\max} + \\ln\\left(\\sum_{j=1}^{K} \\exp(z_j-z_{\\max})\\right)$$\n这个变换是一个代数恒等式。以“稳定”形式表示的函数在数学上与原始函数完全相同。由于函数本身未变，其关于 $\\mathbf{z}$ 的梯度也保持不变。LSE 技巧纯粹是一种防止数值上溢和下溢的计算手段；它不会改变函数的数学性质。\n\n为了验证这一点，我们可以对 LSE 形式的损失函数求导：\n$$\\frac{\\partial L}{\\partial z_i} = \\frac{\\partial}{\\partial z_i} \\left( -\\sum_{k=1}^{K} y_{k} z_k + \\ln\\left(\\sum_{j=1}^{K} \\exp(z_{j})\\right) \\right)$$\n$$\\frac{\\partial L}{\\partial z_i} = -y_i + \\frac{1}{\\sum_{j=1}^{K} \\exp(z_{j})} \\cdot \\frac{\\partial}{\\partial z_i} \\left(\\sum_{j=1}^{K} \\exp(z_j)\\right)$$\n$$\\frac{\\partial L}{\\partial z_i} = -y_i + \\frac{\\exp(z_i)}{\\sum_{j=1}^{K} \\exp(z_{j})} = -y_i + p_i$$\n这证实了梯度确实是 $p_i - y_i$。\n\n### 特定情况的求值\n\n我们已知 $K=3$，logits $\\mathbf{z} = (2, -1, 0.5)$，以及一个独热目标 $\\mathbf{y} = (1, 0, 0)$。\n梯度是 $\\nabla_{\\mathbf{z}}L = \\mathbf{p} - \\mathbf{y}$。首先，我们计算概率向量 $\\mathbf{p}$。\n\nsoftmax 函数的分母是：\n$$D = \\sum_{j=1}^{3} \\exp(z_j) = \\exp(2) + \\exp(-1) + \\exp(0.5)$$\n$$D \\approx 7.389056 + 0.367879 + 1.648721 \\approx 9.405656$$\n\n概率是：\n$$p_1 = \\frac{\\exp(2)}{D} \\approx \\frac{7.389056}{9.405656} \\approx 0.785601$$\n$$p_2 = \\frac{\\exp(-1)}{D} \\approx \\frac{0.367879}{9.405656} \\approx 0.039112$$\n$$p_3 = \\frac{\\exp(0.5)}{D} \\approx \\frac{1.648721}{9.405656} \\approx 0.175287$$\n\n现在，我们计算梯度 $\\mathbf{g} = \\mathbf{p} - \\mathbf{y}$ 的分量：\n$$g_1 = p_1 - y_1 \\approx 0.785601 - 1 = -0.214399$$\n$$g_2 = p_2 - y_2 \\approx 0.039112 - 0 = 0.039112$$\n$$g_3 = p_3 - y_3 \\approx 0.175287 - 0 = 0.175287$$\n\n将这些值四舍五入到 $4$ 位有效数字：\n$$g_1 \\approx -0.2144$$\n$$g_2 \\approx 0.03911$$\n$$g_3 \\approx 0.1753$$\n\n梯度向量约为 $(-0.2144, 0.03911, 0.1753)$。", "answer": "$$\\boxed{\\begin{pmatrix} -0.2144  0.03911  0.1753 \\end{pmatrix}}$$", "id": "3101047"}]}