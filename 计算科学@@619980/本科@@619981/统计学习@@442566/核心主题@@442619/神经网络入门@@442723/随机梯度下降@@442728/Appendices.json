{"hands_on_practices": [{"introduction": "要真正理解随机梯度下降（SGD），最好的方法就是亲手执行一次更新步骤。这个练习将整个过程分解为最基本的组成部分：选择一个数据点，计算其局部梯度，并根据这个梯度更新参数。通过这个具体的计算，你将清晰地看到在算法的每一次迭代中究竟发生了什么，为后续更复杂的概念打下坚实的基础 [@problem_id:2206637]。", "problem": "一个迭代优化算法用于寻找使成本函数最小化的参数 $x$。总成本函数是若干分量函数的平均值：$F(x) = \\frac{1}{N}\\sum_{i=1}^{N} f_i(x)$。在这个具体案例中，分量函数是二次函数，由 $f_i(x) = (x - c_i)^2$ 给出，其中常数 $c_i = i$，$i = 1, 2, \\dots, 10$，因此 $N=10$。\n\n优化过程从参数的初始猜测值 $x_0$ 开始。在每一步中，通过仅使用一个随机选择的分量函数 $f_j(x)$，从当前估计值 $x_k$ 计算出一个新的估计值 $x_{k+1}$。更新规则定义为：\n$$x_{k+1} = x_k - \\eta \\left( \\frac{d f_j(x)}{dx} \\bigg|_{x=x_k} \\right)$$\n其中 $\\eta$ 是一个常数，称为学习率。\n\n给定初始参数值 $x_0 = 10.0$ 和学习率 $\\eta = 0.1$，计算经过一次更新步骤后参数 $x_1$ 的值。对于这第一步，使用的分量函数是索引为 $j=5$ 的 $f_j(x)$。", "solution": "我们已知分量函数的形式为 $f_{i}(x) = (x - c_{i})^{2}$，其中 $c_{i} = i$。对于第一次更新，选择的索引是 $j=5$，所以 $f_{5}(x) = (x - 5)^{2}$。\n\n更新规则是\n$$\nx_{k+1} = x_{k} - \\eta \\left.\\frac{d f_{j}(x)}{dx}\\right|_{x=x_{k}}.\n$$\n使用幂法则和链式法则，所选分量函数的导数是\n$$\n\\frac{d f_{5}(x)}{dx} = \\frac{d}{dx}\\left[(x - 5)^{2}\\right] = 2(x - 5).\n$$\n在当前迭代点 $x_{0} = 10$ 处求值，得到\n$$\n\\left.\\frac{d f_{5}(x)}{dx}\\right|_{x=10} = 2(10 - 5) = 10.\n$$\n当学习率为 $\\eta = 0.1$ 时，更新变为\n$$\nx_{1} = x_{0} - \\eta \\cdot 10 = 10 - 0.1 \\times 10 = 10 - 1 = 9.\n$$\n因此，在使用 $f_{5}$ 进行一次更新步骤后，参数值为 $x_{1} = 9$。", "answer": "$$\\boxed{9}$$", "id": "2206637"}, {"introduction": "在掌握了基本的更新机制后，下一个关键概念是学习率 $\\eta$。这个超参数控制着每一步更新的幅度，对算法的成败至关重要。本练习作为一个警示案例，通过一个简单的计算，生动地展示了过大的学习率如何导致参数更新越过最小值并最终发散，从而帮助你建立起调试和选择学习率的实践直觉 [@problem_id:2206673]。", "problem": "在机器学习的背景下，我们通常通过最小化损失函数来优化模型的参数。考虑一个具有单一标量参数 $w$ 的简化模型。与单个数据点相关的损失函数由 $L(w) = \\frac{1}{2} c w^2$ 给出，其中最小损失出现在 $w=0$ 处。该参数使用随机梯度下降 (Stochastic Gradient Descent, SGD) 算法进行更新。在第 $k$ 步，参数的更新规则由 $w_{k+1} = w_k - \\eta \\nabla L(w_k)$ 给出，其中 $\\nabla L(w_k)$ 是在 $w_k$ 处评估的损失函数的梯度，而 $\\eta$ 是学习率。\n\n假设参数的初始值为 $w_0 = 4.0$。模型参数设置为 $c = 0.75$，学习率为 $\\eta = 3.2$。计算经过 3 次更新步骤后参数 $w$ 的值（即，求 $w_3$）。\n\n将您的最终答案四舍五入到三位有效数字。", "solution": "损失为 $L(w)=\\frac{1}{2} c w^{2}$。其梯度通过求导得到：\n$$\n\\nabla L(w)=\\frac{\\mathrm{d}}{\\mathrm{d}w}\\left(\\frac{1}{2} c w^{2}\\right)=c w.\n$$\nSGD 更新规则是\n$$\nw_{k+1}=w_{k}-\\eta \\nabla L(w_{k})=w_{k}-\\eta c w_{k}=(1-\\eta c)\\,w_{k}.\n$$\n这个线性递推关系解为\n$$\nw_{k}=(1-\\eta c)^{k} w_{0}.\n$$\n代入 $c=0.75$、$\\eta=3.2$ 和 $w_{0}=4.0$，\n$$\n1-\\eta c=1-(3.2)(0.75)=1-2.4=-1.4,\n$$\n所以\n$$\nw_{3}=(-1.4)^{3}\\cdot 4.0=-10.976.\n$$\n四舍五入到三位有效数字得到 $-11.0$。", "answer": "$$\\boxed{-11.0}$$", "id": "2206673"}, {"introduction": "像SGD这样的梯度下降方法虽然强大，但并非万能。它们的有效性在很大程度上取决于损失函数的性质。这个思想实验探讨了一个SGD完全失效的场景，其损失函数在大部分区域是“平坦”的。这个问题将促使你深入思考梯度下降方法的一个核心前提：必须存在一个有用的、非零的梯度来引导搜索方向 [@problem_id:2206644]。", "problem": "一位工程师正在为工厂中的一个关键工序开发一个实时监控系统。该系统使用单个传感器读数（表示为 $x$）来预测一个关键绩效指标 $y$。对于此任务，工程师选择了一个不带偏置项的简单线性模型：$y_{pred} = w \\cdot x$，其中 $w$ 是要学习的单个模型参数。\n\n在这个特定应用中，只有当误差的大小 $|y_{true} - y_{pred}|$ 超过一个预定义的容差阈值 $\\epsilon > 0$ 时，该误差才被认为是显著的。在此容差范围内的任何预测误差都被认为是可接受的。为了将此形式化，工程师为单个数据点 $(x, y_{true})$ 设计了一个自定义的损失函数 $L(w)$，如下所示：\n$$\nL(w) = \\begin{cases} 0  \\text{if } |y_{true} - w \\cdot x| \\le \\epsilon \\\\ 1  \\text{if } |y_{true} - w \\cdot x| > \\epsilon \\end{cases}\n$$\n工程师尝试通过在大型 $(x, y_{true})$ 对数据集上使用随机梯度下降 (SGD) 算法来训练模型，以找到 $w$ 的最优值。在运行训练过程后，他们观察到，无论使用何种学习率，参数 $w$ 都几乎不从其初始随机值改变。\n\n以下哪个陈述为训练过程的这种失败提供了最准确和最根本的解释？\n\nA. 阶跃函数损失是非凸的，这意味着随机梯度下降 (SGD) 很容易陷入非全局最小值的局部最小值。\nB. 对于参数 $w$ 的几乎任何给定值，损失函数关于 $w$ 的梯度都为零，这意味着 SGD 更新步骤不会改变参数的值。\nC. 损失函数是不连续的，不连续点处的无穷大梯度会导致 SGD 算法中的数值溢出和不稳定更新。\nD. 在使用二元 (0/1) 损失时，SGD 的随机性引入了过多的噪声，从而阻止参数 $w$ 收敛到稳定值。", "solution": "我们将每个样本的损失视为单个参数 $w$ 的函数：\n$$\nL(w)=\\begin{cases}\n0  \\text{if } |y_{\\text{true}}-wx|\\le \\epsilon,\\\\\n1  \\text{if } |y_{\\text{true}}-wx|>\\epsilon.\n\\end{cases}\n$$\n定义误差 $e(w)=y_{\\text{true}}-wx$。损失函数是 $w$ 的一个阶跃函数，它在 $|e(w)|=\\epsilon$ 的点处改变其值。对于 $x\\neq 0$，条件 $|y_{\\text{true}}-wx|\\le \\epsilon$ 定义了 $w$ 的一个闭区间：\n$$\n|y_{\\text{true}}-wx|\\le \\epsilon \\iff -\\epsilon \\le y_{\\text{true}}-wx \\le \\epsilon.\n$$\n如果 $x>0$，这给出\n$$\n\\frac{y_{\\text{true}}-\\epsilon}{x} \\le w \\le \\frac{y_{\\text{true}}+\\epsilon}{x},\n$$\n而如果 $x0$，不等式会反向，但可行集仍然是以 $\\frac{y_{\\text{true}}-\\epsilon}{x}$ 和 $\\frac{y_{\\text{true}}+\\epsilon}{x}$ 为端点的闭区间。在此区间外，$L(w)=1$；在区间内，$L(w)=0$。因此，$L(w)$ 是分段常数函数，仅在两个边界点处有跳跃。\n$$\nw=\\frac{y_{\\text{true}}-\\epsilon}{x} \\quad \\text{和} \\quad w=\\frac{y_{\\text{true}}+\\epsilon}{x}.\n$$\n对于 $x=0$，我们有 $e(w)=y_{\\text{true}}$，它与 $w$ 无关，因此 $L(w)$ 对所有 $w$ 都是常数，因而在任何地方都是平坦的。\n\n可微性：在任何不包含边界点的开区间上，$L(w)$ 是常数，因此其导数为\n$$\n\\frac{dL}{dw}=0 \\quad \\text{for all } w \\notin S,\n$$\n其中 $S=\\{w:\\,|y_{\\text{true}}-wx|=\\epsilon\\}$。在 $w\\in S$ 处，$L(w)$ 是不连续且不可微的；对于这种 0/1 阶跃损失，不存在有限的导数，也没有明确定义的次梯度。由于对于单个样本，$S$ 是一个有限集（并且在 $\\mathbb{R}$ 中是一个测度为零的集合），因此关于 $w$ 的梯度几乎处处为零。\n\n在使用任何小批量或单个样本的随机梯度下降中，更新公式为\n$$\nw_{t+1}=w_{t}-\\eta \\,\\frac{\\partial L}{\\partial w}(w_{t}),\n$$\n但是对于所有 $w_{t}\\notin S$，都有 $\\frac{\\partial L}{\\partial w}(w_{t})=0$。因为在连续更新和浮点数运算下，精确地命中 $w_{t}\\in S$ 的概率为零，所以算法几乎永远不会遇到不可微的点，因此几乎总是计算出零梯度。因此，在基本上所有的步骤中都有 $w_{t+1}=w_{t}$，所以无论学习率如何，参数几乎不发生改变。\n\n这就证明了训练失败的根本原因是梯度几乎处处为零。选项 A 提到了非凸性，这是正确的，但仅非凸性本身并不会导致零更新；这里的决定性问题是梯度消失。选项 C 错误地将问题归因于不连续点处的无穷大梯度；导数在跳跃点是未定义的，而在实践中这些点很少（如果会的话）被遇到。选项 D 将问题归咎于随机噪声，但参数不发生变化是由于梯度为零，而不是噪声。因此，最准确和最根本的解释是，对于几乎所有的 $w$，梯度都为零，这阻止了 SGD 更新参数，这与选项 B 相对应。", "answer": "$$\\boxed{B}$$", "id": "2206644"}]}