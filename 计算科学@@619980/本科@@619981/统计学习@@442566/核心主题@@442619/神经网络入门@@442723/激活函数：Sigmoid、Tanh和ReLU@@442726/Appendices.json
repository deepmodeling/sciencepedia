{"hands_on_practices": [{"introduction": "在深入研究复杂网络之前，关键是要理解基于梯度的学习能否在简单的理想化情境下奏效。这个练习将你置于一个“教师-学生”框架中，通过分析损失函数的地貌，来验证对于 Sigmoid、Tanh 和 ReLU 这三种激活函数，学生的参数确实能够正确地收敛到教师的参数，从而建立对学习过程的基本信心。[@problem_id:3094674]", "problem": "考虑一个统计学习中的标量输入回归场景，其中输入 $x \\in \\{-1, +1\\}$ 以相等概率抽取，并采用师生范式。教师模型输出 $y = \\phi(\\theta x)$，其中 $\\theta > 0$ 是一个固定参数；学生模型为 $f_{w}(x) = \\phi(w x)$，其中 $w \\in \\mathbb{R}$ 是可训练参数。激活函数 $\\phi$ 是以下之一：\n- 逻辑S型函数 (Logistic sigmoid) $\\sigma(z) = \\frac{1}{1 + \\exp(-z)}$，\n- 双曲正切 (Hyperbolic tangent) $\\tanh(z) = \\frac{\\exp(z) - \\exp(-z)}{\\exp(z) + \\exp(-z)}$，\n- 修正线性单元 (Rectified Linear Unit) $\\mathrm{ReLU}(z) = \\max(0, z)$。\n\n学习目标是期望平方误差损失，其标准选择为 $\\ell(y, f) = \\frac{1}{2}(y - f)^{2}$，因此期望风险为\n$$\nL(w) = \\mathbb{E}\\!\\left[\\ell\\big(y, f_{w}(x)\\big)\\right] = \\mathbb{E}\\!\\left[\\frac{1}{2}\\big(\\phi(\\theta x) - \\phi(w x)\\big)^{2}\\right],\n$$\n其中期望是关于 $x$ 的分布计算的。\n\n训练遵循由常微分方程（ODE）给出的梯度流动力学\n$$\n\\dot{w}(t) = - \\frac{\\mathrm{d}}{\\mathrm{d}w} L(w(t)).\n$$\n\n从上述基本定义出发，不使用任何快捷公式，推导对于每种激活函数 $\\phi \\in \\{\\sigma, \\tanh, \\mathrm{ReLU}\\}$ 的梯度流ODE的临界点（平衡点），并使用对平衡点局部行为的原则性分析（例如，在适用的情况下通过 $L$ 的二阶导数，或在不可微的情况下使用适当的次梯度论证）来分类它们的稳定性。最后，按 $(\\sigma, \\tanh, \\mathrm{ReLU})$ 的顺序，以 $\\theta$ 的闭式解析表达式形式，给出稳定平衡值 $w^{\\star}$ 的有序三元组。请使用 $\\mathrm{pmatrix}$ 环境将最终答案表示为单行矩阵。无需四舍五入。", "solution": "用户提供的问题经验证是适定的、有科学依据且内部一致的。它提出了一个统计学习中的标准理论练习，要求将微积分和稳定性分析应用于一个明确定义的动力系统。所有必要的定义和条件均已提供。\n\n分析过程如下：首先推导期望风险函数，然后为三种指定的激活函数分别找出其临界点，最后对这些点的稳定性进行分类以确定稳定平衡点。\n\n期望风险由下式给出\n$$\nL(w) = \\mathbb{E}\\!\\left[\\frac{1}{2}\\big(\\phi(\\theta x) - \\phi(w x)\\big)^{2}\\right]\n$$\n期望是关于输入分布 $P(x=+1) = P(x=-1) = \\frac{1}{2}$ 计算的。展开期望可得：\n$$\nL(w) = \\frac{1}{2} P(x=+1) \\left(\\phi(\\theta \\cdot 1) - \\phi(w \\cdot 1)\\right)^2 + \\frac{1}{2} P(x=-1) \\left(\\phi(\\theta \\cdot (-1)) - \\phi(w \\cdot (-1))\\right)^2\n$$\n代入概率，我们得到\n$$\nL(w) = \\frac{1}{4} \\left[ \\left(\\phi(\\theta) - \\phi(w)\\right)^2 + \\left(\\phi(-\\theta) - \\phi(-w)\\right)^2 \\right]\n$$\n梯度流 $\\dot{w}(t) = - \\frac{\\mathrm{d}}{\\mathrm{d}w} L(w(t))$ 的平衡点是势函数 $L(w)$ 的临界点，即满足 $\\frac{\\mathrm{d}L}{\\mathrm{d}w} = 0$ 的 $w$ 值。一个稳定平衡点对应于 $L(w)$ 的一个局部最小值，可以通过分析二阶导数 $\\frac{\\mathrm{d}^2L}{\\mathrm{d}w^2}$ 或通过考察 $L(w)$ 的局部行为来确定。\n\n我们分别分析每种激活函数 $\\phi$。\n\n**1. 逻辑S型函数 (Logistic Sigmoid): $\\phi(z) = \\sigma(z) = \\frac{1}{1 + \\exp(-z)}$**\n\nS型函数具有性质 $\\sigma(-z) = 1 - \\sigma(z)$。将此代入 $L(w)$ 的表达式中：\n$$\nL(w) = \\frac{1}{4} \\left[ (\\sigma(\\theta) - \\sigma(w))^2 + (1 - \\sigma(\\theta) - (1 - \\sigma(w)))^2 \\right]\n$$\n$$\nL(w) = \\frac{1}{4} \\left[ (\\sigma(\\theta) - \\sigma(w))^2 + (\\sigma(w) - \\sigma(\\theta))^2 \\right] = \\frac{1}{2} (\\sigma(\\theta) - \\sigma(w))^2\n$$\n为了找到临界点，我们计算 $L(w)$ 关于 $w$ 的导数。S型函数的导数是 $\\sigma'(z) = \\sigma(z)(1 - \\sigma(z))$。\n$$\n\\frac{\\mathrm{d}L}{\\mathrm{d}w} = \\frac{1}{2} \\cdot 2 (\\sigma(\\theta) - \\sigma(w)) \\cdot (-\\sigma'(w)) = - (\\sigma(\\theta) - \\sigma(w)) \\sigma'(w)\n$$\n临界点出现在 $\\frac{\\mathrm{d}L}{\\mathrm{d}w} = 0$ 处。这意味着 $\\sigma(\\theta) - \\sigma(w) = 0$ 或 $\\sigma'(w) = 0$。\n项 $\\sigma'(w) = \\sigma(w)(1-\\sigma(w))$ 对于任何有限的 $w \\in \\mathbb{R}$ 都是严格为正的。因此，我们必须有 $\\sigma(\\theta) - \\sigma(w) = 0$。\n由于 $\\sigma(z)$ 是一个严格单调递增的函数，$\\sigma(w) = \\sigma(\\theta)$ 意味着 $w = \\theta$。唯一的临界点是 $w = \\theta$。\n\n为了分类其稳定性，我们考察二阶导数：\n$$\n\\frac{\\mathrm{d}^2L}{\\mathrm{d}w^2} = \\frac{\\mathrm{d}}{\\mathrm{d}w} \\left[ - (\\sigma(\\theta) - \\sigma(w)) \\sigma'(w) \\right] = \\sigma'(w)\\sigma'(w) - (\\sigma(\\theta) - \\sigma(w))\\sigma''(w) = (\\sigma'(w))^2 - (\\sigma(\\theta) - \\sigma(w))\\sigma''(w)\n$$\n在临界点 $w = \\theta$ 处：\n$$\n\\left.\\frac{\\mathrm{d}^2L}{\\mathrm{d}w^2}\\right|_{w=\\theta} = (\\sigma'(\\theta))^2 - (\\sigma(\\theta) - \\sigma(\\theta))\\sigma''(\\theta) = (\\sigma'(\\theta))^2\n$$\n由于 $\\theta > 0$，$\\sigma'(\\theta) > 0$，因此 $(\\sigma'(\\theta))^2 > 0$。正的二阶导数表明 $L(w)$ 在 $w=\\theta$ 处有一个局部最小值。因此，$w^\\star = \\theta$ 是一个稳定平衡点。\n\n**2. 双曲正切 (Hyperbolic Tangent): $\\phi(z) = \\tanh(z)$**\n\n双曲正切是一个奇函数，即 $\\tanh(-z) = -\\tanh(z)$。将此代入 $L(w)$ 的表达式中：\n$$\nL(w) = \\frac{1}{4} \\left[ (\\tanh(\\theta) - \\tanh(w))^2 + (-\\tanh(\\theta) - (-\\tanh(w)))^2 \\right]\n$$\n$$\nL(w) = \\frac{1}{4} \\left[ (\\tanh(\\theta) - \\tanh(w))^2 + (\\tanh(w) - \\tanh(\\theta))^2 \\right] = \\frac{1}{2} (\\tanh(\\theta) - \\tanh(w))^2\n$$\n$L(w)$ 的函数形式与S型函数的情况相同。我们计算导数，注意到 $\\tanh'(z) = 1 - \\tanh^2(z) = \\mathrm{sech}^2(z)$。\n$$\n\\frac{\\mathrm{d}L}{\\mathrm{d}w} = \\frac{1}{2} \\cdot 2 (\\tanh(\\theta) - \\tanh(w)) \\cdot (-\\tanh'(w)) = - (\\tanh(\\theta) - \\tanh(w)) \\tanh'(w)\n$$\n临界点出现在 $\\frac{\\mathrm{d}L}{\\mathrm{d}w} = 0$ 处。导数 $\\tanh'(w) = \\mathrm{sech}^2(w)$ 对于任何有限的 $w \\in \\mathbb{R}$ 都是严格为正的。因此，我们必须有 $\\tanh(\\theta) - \\tanh(w) = 0$。\n由于 $\\tanh(z)$ 是一个严格单调递增的函数，$\\tanh(w) = \\tanh(\\theta)$ 意味着 $w = \\theta$。唯一的临界点是 $w = \\theta$。\n\n为了确定稳定性，我们评估在 $w=\\theta$ 处的二阶导数：\n$$\n\\frac{\\mathrm{d}^2L}{\\mathrm{d}w^2} = (\\tanh'(w))^2 - (\\tanh(\\theta) - \\tanh(w))\\tanh''(w)\n$$\n$$\n\\left.\\frac{\\mathrm{d}^2L}{\\mathrm{d}w^2}\\right|_{w=\\theta} = (\\tanh'(\\theta))^2 - (\\tanh(\\theta) - \\tanh(\\theta))\\tanh''(\\theta) = (\\tanh'(\\theta))^2\n$$\n由于 $\\theta > 0$，$\\tanh'(\\theta) > 0$，因此 $(\\tanh'(\\theta))^2 > 0$。这表明在 $w=\\theta$ 处有一个局部最小值。因此，$w^\\star = \\theta$ 是一个稳定平衡点。\n\n**3. 修正线性单元 (Rectified Linear Unit): $\\phi(z) = \\mathrm{ReLU}(z) = \\max(0, z)$**\n\n给定 $\\theta > 0$，我们有 $\\mathrm{ReLU}(\\theta) = \\theta$ 和 $\\mathrm{ReLU}(-\\theta) = 0$。风险函数变为：\n$$\nL(w) = \\frac{1}{4} \\left[ (\\theta - \\mathrm{ReLU}(w))^2 + (0 - \\mathrm{ReLU}(-w))^2 \\right] = \\frac{1}{4} \\left[ (\\theta - \\mathrm{ReLU}(w))^2 + (\\mathrm{ReLU}(-w))^2 \\right]\n$$\nReLU函数是分段线性的，所以我们对不同的 $w$ 区域分析 $L(w)$。\n\n情况 (i)：$w > 0$。此时，$\\mathrm{ReLU}(w) = w$ 且 $\\mathrm{ReLU}(-w) = 0$。\n$$\nL(w) = \\frac{1}{4} \\left[ (\\theta - w)^2 + 0^2 \\right] = \\frac{1}{4}(\\theta - w)^2\n$$\n导数为 $\\frac{\\mathrm{d}L}{\\mathrm{d}w} = \\frac{1}{4} \\cdot 2(\\theta - w) \\cdot (-1) = -\\frac{1}{2}(\\theta - w)$。\n令 $\\frac{\\mathrm{d}L}{\\mathrm{d}w} = 0$ 得到 $w = \\theta$。由于 $\\theta > 0$，此临界点位于区域 $w>0$ 内。二阶导数为 $\\frac{\\mathrm{d}^2L}{\\mathrm{d}w^2} = \\frac{1}{2} > 0$，所以 $w = \\theta$ 是一个局部最小值，因此是一个稳定平衡点。\n\n情况 (ii)：$w  0$。此时，$\\mathrm{ReLU}(w) = 0$ 且 $\\mathrm{ReLU}(-w) = -w$。\n$$\nL(w) = \\frac{1}{4} \\left[ (\\theta - 0)^2 + (-w)^2 \\right] = \\frac{1}{4}(\\theta^2 + w^2)\n$$\n导数为 $\\frac{\\mathrm{d}L}{\\mathrm{d}w} = \\frac{1}{4}(2w) = \\frac{w}{2}$。对于 $w  0$，$\\frac{\\mathrm{d}L}{\\mathrm{d}w}  0$，所以该区域内没有临界点。梯度流为 $\\dot{w} = -L'(w) = -w/2 > 0$，意味着状态 $w$ 向 0 移动。\n\n情况 (iii)：$w=0$。函数 $L(w)$ 在 $w=0$ 处不可微。我们分析其次梯度。一个点 $w^\\star$ 是临界点，如果 $0 \\in \\partial L(w^\\star)$，其中 $\\partial L$ 是次微分。\n在不可微点的次微分是左右导数之间的区间。\n右导数：$\\lim_{w\\to 0^+} \\frac{\\mathrm{d}L}{\\mathrm{d}w} = \\lim_{w\\to 0^+} \\left[-\\frac{1}{2}(\\theta - w)\\right] = -\\frac{\\theta}{2}$。\n左导数：$\\lim_{w\\to 0^-} \\frac{\\mathrm{d}L}{\\mathrm{d}w} = \\lim_{w\\to 0^-} \\frac{w}{2} = 0$。\n在 $w=0$ 处的次微分是 $\\partial L(0) = [-\\frac{\\theta}{2}, 0]$。由于 $0 \\in [-\\frac{\\theta}{2}, 0]$（因为 $\\theta0$），$w=0$ 是一个临界点。\n\n为了评估 $w=0$ 的稳定性，我们考察其邻域内的势函数 $L(w)$。\n$L(0) = \\frac{1}{4}(\\theta^2)$。\n对于一个小的 $w > 0$，$L(w) = \\frac{1}{4}(\\theta-w)^2 = \\frac{1}{4}(\\theta^2 - 2\\theta w + w^2)$。\n差值为 $L(w) - L(0) = \\frac{1}{4}(-2\\theta w + w^2) = \\frac{w}{4}(w-2\\theta)$。\n对于 $w \\in (0, 2\\theta)$，这个差是负的，意味着 $L(w)  L(0)$。如果存在势能更低的邻近点，系统可以向其演化，那么该点就不是一个稳定平衡点。由于对于任何小的扰动 $\\epsilon0$，都有 $L(\\epsilon)  L(0)$，系统将远离 $w=0$（具体来说，是朝向 $w=\\theta$）。因此，$w=0$ 是一个不稳定平衡点。\n\n对于ReLU激活函数，唯一的稳定平衡点是 $w^\\star = \\theta$。\n\n**结果总结**\n- 对于 $\\phi = \\sigma(z)$，稳定平衡点是 $w^\\star = \\theta$。\n- 对于 $\\phi = \\tanh(z)$，稳定平衡点是 $w^\\star = \\theta$。\n- 对于 $\\phi = \\mathrm{ReLU}(z)$，稳定平衡点是 $w^\\star = \\theta$。\n\n按 $(\\sigma, \\tanh, \\mathrm{ReLU})$ 顺序排列的稳定平衡值 $w^{\\star}$ 的有序三元组是 $(\\theta, \\theta, \\theta)$。", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\theta  \\theta  \\theta\n\\end{pmatrix}\n}\n$$", "id": "3094674"}, {"introduction": "训练深度网络历史上最大的挑战之一是“梯度消失”问题，它会导致学习停滞。这个基于编码的练习为你提供了一个关于此问题的生动、亲手的演示。你将看到 Sigmoid 和 Tanh 的饱和特性如何使学习陷入停顿，而非饱和的 ReLU 如何继续有效地学习。[@problem_id:3094585]", "problem": "考虑一个在合成数据集上通过梯度下降训练的单神经元模型。该神经元根据输入向量 $x \\in \\mathbb{R}^d$、权重向量 $w \\in \\mathbb{R}^d$ 和偏置 $b \\in \\mathbb{R}$ 计算标量预激活值 $z = w^\\top x + b$，然后输出 $y_{\\text{hat}} = g(z)$，其中 $g$ 是一种标准激活函数。训练目标是预测值与固定目标值 $y_i$ 之间的均方误差（MSE）$\\frac{1}{N}\\sum_{i=1}^N \\left(y_{\\text{hat},i} - y_i\\right)^2$。数据集只生成一次，并在所有测试用例中保持不变。\n\n基础知识：\n- 使用微积分中的链式法则推导损失函数关于参数 $w$ 和 $b$ 的梯度，以及标准梯度下降更新规则 $w \\leftarrow w - \\eta \\nabla_w L$，$b \\leftarrow b - \\eta \\nabla_b L$，其中学习率为 $\\eta$。\n- 使用激活函数的定义：逻辑（sigmoid）函数 $\\sigma(z)$、双曲正切函数 $\\tanh(z)$ 和修正线性单元（ReLU）。\n\n课程设置：\n- 课程学习（curriculum）在不同轮次（epoch）中缩放输入的大小。对于轮次索引 $e \\in \\{0,1,\\dots,E-1\\}$，定义一个标量 $s_e  0$，并在保持数据集和目标不变的情况下，使用缩放后的输入 $x^{(e)} = s_e x$ 进行训练。这使得在第 $e$ 轮中，有效的预激活值从 $z = w^\\top x + b$ 变为 $z^{(e)} = w^\\top (s_e x) + b$。\n- 对于主要测试，课程安排 $s_e$ 随 $e$ 递增；为了覆盖边界情况，也包含了一个 $s_e$ 为常数的用例。\n\n数据集规格：\n- 将随机种子固定为 $123$。\n- 使用输入维度 $d = 10$ 和样本数量 $N = 256$。\n- 从均值为零、协方差为单位矩阵的标准多元正态分布中独立抽取输入 $x_i \\in \\mathbb{R}^d$。\n- 独立地，从同一分布中抽取一个教师权重向量 $v \\in \\mathbb{R}^d$（使用相同的种子以确保可复现性）。\n- 定义目标 $y_i = \\sigma(v^\\top x_i)$，因此 $y_i \\in (0,1)$。\n\n模型初始化：\n- 初始化 $w = 0_d$（$\\mathbb{R}^d$ 中的零向量）。\n- 初始化 $b = 5.0$。\n- 使用学习率 $\\eta = 10^{-6}$。\n- 训练 $E$ 轮。每一轮使用当前的课程缩放因子 $s_e$ 并执行一次全批量梯度下降更新。\n\n科学目标与预测：\n- 从链式法则出发，推断当 $|z|$ 变大时，每种激活函数的梯度大小行为。预测在输入大小递增的课程下，$\\tanh$ 和 $\\sigma$ 的饱和现象（当 $|z|$ 很大时，导数趋近于 $0$）如何影响学习，并与 ReLU 无界正区间的特性（当 $z  0$ 时导数约为 $1$，当 $z  0$ 时为 $0$）进行对比。解释在以下安排下，参数更新的预期相对困难程度。\n- 通过在每个测试用例下进行训练并报告一个量化性能指标，来凭经验进行验证。\n\n评估指标：\n- 对于每个测试用例，在任何训练开始前，计算缩放因子 $s = 1$ 时的初始损失 $L_{\\text{init}}$，以及在所有轮次结束后，缩放因子 $s = 1$ 时的最终损失 $L_{\\text{final}}$。以每个测试用例一个浮点数的形式，报告损失减少率 $r = \\frac{L_{\\text{init}} - L_{\\text{final}}}{L_{\\text{init}}}$。\n\n激活函数：\n- $\\sigma(z) = \\frac{1}{1 + e^{-z}}$。\n- $\\tanh(z)$ 是双曲正切函数。\n- $\\text{ReLU}(z) = \\max(0, z)$。\n\n测试套件：\n所有用例均使用 $E = 10$ 轮。\n1. 激活函数 $g = \\sigma$，指数课程 $s_e = \\alpha^e$，其中 $\\alpha = 3.0$。\n2. 激活函数 $g = \\tanh$，指数课程 $s_e = \\alpha^e$，其中 $\\alpha = 3.0$。\n3. 激活函数 $g = \\text{ReLU}$，指数课程 $s_e = \\alpha^e$，其中 $\\alpha = 3.0$。\n4. 边界覆盖：激活函数 $g = \\sigma$，常数课程 $s_e = 1$ 对所有 $e$ 成立（无增长）。\n5. 激活函数 $g = \\text{ReLU}$，指数课程 $s_e = \\alpha^e$，其中 $\\alpha = 1.5$（中度增长）。\n\n你的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表（例如，“[r1,r2,r3,r4,r5]”），其中每个 $r_k$ 是测试用例 $k$ 的损失减少率。不涉及物理单位或角度。输出中的所有数值均为浮点数。", "solution": "用户提供的问题是一个有效且定义明确的科学探究，旨在研究不同神经网络激活函数在特定训练课程下的行为。我现在将提供完整的解决方案。\n\n### 1. 理论框架\n\n该问题研究了一个单神经元模型，其定义为预激活函数 $z = w^\\top x + b$ 和激活函数 $y_{\\text{hat}} = g(z)$，其中 $x \\in \\mathbb{R}^d$ 是输入，$w \\in \\mathbb{R}^d$ 是权重，$b \\in \\mathbb{R}$ 是偏置。该模型使用全批量梯度下降进行训练，以最小化均方误差（MSE）损失 $L = \\frac{1}{N}\\sum_{i=1}^N (y_{\\text{hat},i} - y_i)^2$。\n\n引入了一个课程学习（curriculum），其中在每个轮次 $e$，输入 $x_i$ 被一个因子 $s_e$ 缩放。因此，在轮次 $e$ 中样本 $i$ 的预激活值为 $z_i^{(e)} = w^\\top(s_e x_i) + b$。参数根据以下规则进行更新：\n$$ w \\leftarrow w - \\eta \\nabla_w L $$\n$$ b \\leftarrow b - \\eta \\nabla_b L $$\n其中 $\\eta$ 是学习率。\n\n### 2. 梯度推导\n\n为了理解课程学习的效果，我们推导损失函数 $L$ 关于参数 $w$ 和 $b$ 的梯度。对轮次 $e$ 中的单个样本 $i$ 使用链式法则：\n$$ \\frac{\\partial L_i}{\\partial w} = \\frac{\\partial L_i}{\\partial y_{\\text{hat},i}} \\frac{\\partial y_{\\text{hat},i}}{\\partial z_i^{(e)}} \\frac{\\partial z_i^{(e)}}{\\partial w} $$\n各组成部分为：\n- $\\frac{\\partial L_i}{\\partial y_{\\text{hat},i}} = 2(y_{\\text{hat},i} - y_i) = 2(g(z_i^{(e)}) - y_i)$\n- $\\frac{\\partial y_{\\text{hat},i}}{\\partial z_i^{(e)}} = g'(z_i^{(e)})$\n- $\\frac{\\partial z_i^{(e)}}{\\partial w} = \\frac{\\partial}{\\partial w} (w^\\top (s_e x_i) + b) = s_e x_i$\n\n对 $N$ 个样本的批次进行平均，权重向量 $w$ 的梯度为：\n$$ \\nabla_w L = \\frac{2s_e}{N} \\sum_{i=1}^N (g(z_i^{(e)}) - y_i) g'(z_i^{(e)}) x_i $$\n\n类似地，对于偏置 $b$，预激活值的导数为 $\\frac{\\partial z_i^{(e)}}{\\partial b} = 1$。偏置 $b$ 的梯度为：\n$$ \\nabla_b L = \\frac{2}{N} \\sum_{i=1}^N (g(z_i^{(e)}) - y_i) g'(z_i^{(e)}) $$\n\n关键的观察是，课程缩放因子 $s_e$ 直接作为权重梯度 $\\nabla_w L$ 的乘数，但并不作用于偏置梯度 $\\nabla_b L$。此外，$s_e$ 会影响预激活值 $z_i^{(e)}$ 的大小，这反过来又影响了关键的导数项 $g'(z_i^{(e)})$。\n\n### 3. 激活函数导数分析\n\n项 $g'(z)$ 决定了误差信号有多少被反向传播到参数。它的行为，尤其是在 $|z|$ 很大时，至关重要。\n\n- **Sigmoid（逻辑）函数, $\\sigma(z) = \\frac{1}{1 + e^{-z}}$**：\n  其导数为 $\\sigma'(z) = \\sigma(z)(1 - \\sigma(z))$。当 $z$ 为大的正数或负数时，$\\sigma(z)$ 分别趋近于 $1$ 或 $0$。在这两种情况下，乘积 $\\sigma(z)(1 - \\sigma(z))$ 都趋近于 $0$。这种在函数“饱和”区域梯度变得极小的现象，被称为梯度消失问题。\n\n- **双曲正切函数, $\\tanh(z)$**：\n  其导数为 $\\tanh'(z) = 1 - \\tanh^2(z)$。与 sigmoid 类似，当 $|z| \\to \\infty$ 时，$\\tanh(z)$ 趋近于 $\\pm 1$，其导数 $\\tanh'(z)$ 趋近于 $0$。它同样会因为饱和而遭受梯度消失的影响。\n\n- **修正线性单元（ReLU）, $\\text{ReLU}(z) = \\max(0, z)$**：\n  其导数在 $z  0$ 时为 $\\text{ReLU}'(z) = 1$，在 $z  0$ 时为 $0$。对于任何正输入，梯度是恒定的，不会消失。这个特性防止了在正区间的饱和，使得梯度能够无障碍地流动。\n\n### 4. 训练动态预测\n\n问题指定了初始状态为 $w=0_d$ 和 $b=5.0$。在第一个轮次（$e=0$），任何样本的预激活值为 $z_i^{(0)} = w_0^\\top(s_0 x_i) + b_0 = 0 + 5.0 = 5.0$。这个大的正偏置立即将神经元置于一个激活函数特性表现出显著差异的区域。\n\n- **测试用例 1（$\\sigma$, $\\alpha=3.0$）和 2（$\\tanh$, $\\alpha=3.0$）**：\n  初始预激活值 $z=5.0$ 对于 sigmoid 和 tanh 都在饱和区域。$\\sigma'(5.0) \\approx 0.0066$ 且 $\\tanh'(5.0) \\approx 0.00018$。这些梯度已经非常小。课程安排 $s_e = 3.0^e$ 会导致 $z_i^{(e)} = s_e(w^\\top x_i) + b$ 变得更大（假设 $w$ 不会变为强负值），从而将神经元推向更深的饱和区。这将使 $g'(z)$ 更接近于零。尽管 $\\nabla_w L$ 中的项 $s_e$ 呈指数增长，但随着 $z$ 的增加，项 $g'(z)$ 呈指数衰减。后者的影响占主导地位，实际上会中止学习。我们预测 sigmoid 的损失减少会非常小，而 tanh 的损失减少会更小，因为它饱和得更快。\n\n- **测试用例 3 (ReLU, $\\alpha=3.0$)**：\n  初始预激活值 $z=5.0$ 位于 ReLU 的激活区域，其中 $\\text{ReLU}'(z) = 1$。梯度不会消失。权重梯度 $\\nabla_w L$ 与巨大且不断增长的缩放因子 $s_e$ 成正比。这相当于对权重施加了一个巨大且递增的学习率，尽管可能不稳定，但这应该会促进快速学习，特别是考虑到基础学习率 $\\eta=10^{-6}$ 非常小。我们预测会有显著的损失减少。\n\n- **测试用例 4（$\\sigma$, $s_e=1$）**：\n  这是一个对照用例。缩放因子是常数。神经元仍然在饱和状态下初始化（$z=5.0$），因此由于梯度小，学习会很慢。然而，与用例 1 不同，预激活值不会被增长的 $s_e$ 主动推向更深的饱和区。我们预测损失减少量会很低但不可忽略，并且应该大于用例 1 中的减少量。\n\n- **测试用例 5 (ReLU, $\\alpha=1.5$)**：\n  与用例 3 类似，ReLU 避免了饱和问题。课程缩放因子 $s_e = 1.5^e$ 更为温和。这仍应提供一个强大的学习信号，同时没有像 $\\alpha=3.0$ 用例那样有同样的不稳定风险。我们预测会有显著的损失减少，如果更激进的缩放导致优化问题，其效果可能与用例 3 相当甚至更好。\n\n总而言之，预期的性能顺序（损失减少率 $r$）是 $r_2  r_1  r_4  r_3 \\approx r_5$。这个实验旨在凭经验证明激活函数饱和的后果。\n\n### 5. 算法实现\n\n该解决方案使用 `numpy` 库在 Python 中实现。\n1.  **数据生成**：一个函数根据指定的分布和随机种子生成固定的数据集 $(X, y)$ 和教师向量 $v$。\n2.  **模型组件**：为 sigmoid、tanh 和 ReLU 激活函数及其各自的导数定义了辅助函数。也定义了一个用于计算 MSE 损失的函数。\n3.  **训练过程**：一个 `train` 函数封装了核心逻辑。它将数据集、激活函数、课程安排和超参数作为输入。在函数内部，它初始化权重和偏置。然后，它迭代指定的轮次数。在每一轮中，它会： a. 计算当前的缩放因子 $s_e$。 b. 使用缩放后的输入计算预激活值 $z^{(e)}$。 c. 计算预测值 $y_{\\text{hat}}$ 和激活函数的导数值 $g'(z^{(e)})$。 d. 使用推导出的矩阵形式表达式计算梯度 $\\nabla_w L$ 和 $\\nabla_b L$。 e. 更新参数 $w$ 和 $b$。\n4.  **评估**：主函数 `solve` 协调五个测试用例。对于每个用例，它计算缩放因子 $s=1$ 时的初始损失 $L_{\\text{init}}$ 和训练后同样在缩放因子 $s=1$ 时的最终损失 $L_{\\text{final}}$。计算并存储损失减少率 $r = (L_{\\text{init}} - L_{\\text{final}}) / L_{\\text{init}}$。\n5.  **输出**：将收集到的五个测试用例的比率格式化为所需的字符串格式。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Validates and solves the problem of training a single neuron with different \n    activation functions under a scaling curriculum.\n    \"\"\"\n\n    # --- Dataset Specification ---\n    D = 10  # Input dimension\n    N = 256  # Number of samples\n    SEED = 123\n\n    # --- Model and Training Parameters ---\n    W_INIT = np.zeros((D, 1))\n    B_INIT = 5.0\n    ETA = 1e-6  # Learning rate\n    E = 10  # Number of epochs\n\n    def generate_data(d, n, seed):\n        \"\"\"Generates the synthetic dataset.\"\"\"\n        rng = np.random.default_rng(seed)\n        X = rng.standard_normal(size=(n, d))\n        v = rng.standard_normal(size=(d, 1))\n        # Targets are generated by a teacher neuron with sigmoid activation\n        y = 1 / (1 + np.exp(-(X @ v)))\n        return X, y\n\n    # Generate the dataset once, fixed for all test cases.\n    X_data, y_data = generate_data(D, N, SEED)\n    \n    # --- Activation Functions and Derivatives ---\n    def sigmoid(z):\n        return 1 / (1 + np.exp(-z))\n\n    def sigmoid_prime(z):\n        s_z = sigmoid(z)\n        return s_z * (1 - s_z)\n\n    def tanh(z):\n        return np.tanh(z)\n\n    def tanh_prime(z):\n        return 1 - np.tanh(z)**2\n\n    def relu(z):\n        return np.maximum(0, z)\n\n    def relu_prime(z):\n        return (z  0).astype(float)\n\n    # --- Loss Function ---\n    def mse_loss(y_hat, y):\n        return np.mean((y_hat - y)**2)\n\n    def train(X, y, g, g_prime, schedule, epochs, eta):\n        \"\"\"\n        Trains a single neuron model using full-batch gradient descent.\n        \"\"\"\n        d = X.shape[1]\n        n = X.shape[0]\n        w = np.copy(W_INIT)\n        b = B_INIT\n\n        for e in range(epochs):\n            s_e = schedule[e]\n            \n            # Forward pass with current epoch's scale\n            X_scaled = s_e * X\n            z = X_scaled @ w + b\n            y_hat = g(z)\n            \n            # Gradient computation\n            error = y_hat - y\n            g_prime_z = g_prime(z)\n            \n            # Note: For grad_w, the s_e factor is implicitly included in X_scaled.\n            # grad_w = (2/n) * X_scaled.T @ (error * g_prime_z)\n            # The problem formulation shows s_e explicitly. Both are equivalent:\n            # (s_e * X).T = s_e * X.T\n            grad_w = (2 * s_e / n) * X.T @ (error * g_prime_z)\n            grad_b = (2 / n) * np.sum(error * g_prime_z)\n            \n            # Parameter update\n            w -= eta * grad_w\n            b -= eta * grad_b\n            \n        return w, b\n\n    # --- Test Suite Definition ---\n    test_cases = [\n        # Case 1: Sigmoid, aggressive exponential curriculum\n        {'name': 'Sigmoid_alpha3.0', 'g': sigmoid, 'g_prime': sigmoid_prime, \n         'schedule': [3.0**e for e in range(E)]},\n        # Case 2: Tanh, aggressive exponential curriculum\n        {'name': 'Tanh_alpha3.0', 'g': tanh, 'g_prime': tanh_prime, \n         'schedule': [3.0**e for e in range(E)]},\n        # Case 3: ReLU, aggressive exponential curriculum\n        {'name': 'ReLU_alpha3.0', 'g': relu, 'g_prime': relu_prime, \n         'schedule': [3.0**e for e in range(E)]},\n        # Case 4: Sigmoid, constant curriculum (control)\n        {'name': 'Sigmoid_alpha1.0', 'g': sigmoid, 'g_prime': sigmoid_prime, \n         'schedule': [1.0 for _ in range(E)]},\n        # Case 5: ReLU, moderate exponential curriculum\n        {'name': 'ReLU_alpha1.5', 'g': relu, 'g_prime': relu_prime, \n         'schedule': [1.5**e for e in range(E)]},\n    ]\n\n    results = []\n    for case in test_cases:\n        g, schedule = case['g'], case['schedule']\n        \n        # Calculate initial loss at scale s=1\n        z_init = X_data @ W_INIT + B_INIT\n        y_hat_init = g(z_init)\n        L_init = mse_loss(y_hat_init, y_data)\n        \n        # Train the model\n        w_final, b_final = train(X_data, y_data, case['g'], case['g_prime'], schedule, E, ETA)\n        \n        # Calculate final loss at scale s=1\n        z_final = X_data @ w_final + b_final\n        y_hat_final = g(z_final)\n        L_final = mse_loss(y_hat_final, y_data)\n        \n        # Compute the loss reduction ratio\n        # Handle division by zero for L_init, although unlikely here.\n        if L_init == 0:\n            # If initial loss is 0, any reduction is also 0.\n            # If L_final is also 0, ratio is ill-defined, 0 is a safe choice.\n            # If L_final  0, something is very wrong, but ratio is negative.\n            # Let's define the ratio as 0 if L_init is 0.\n            r = 0.0 if L_final == 0 else -np.inf\n        else:\n            r = (L_init - L_final) / L_init\n        \n        results.append(r)\n\n    # Format the final output as specified\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3094585"}, {"introduction": "如果激活函数可能导致学习过程中的梯度消失或爆炸，我们该如何构建从一开始就稳定的深度网络呢？这项高级练习将带你进入“混沌边缘”，使用平均场理论来推导临界权重初始化方案（如 Xavier/Glorot 和 He 初始化）。这些方案是现代深度学习的基石，能确保信号在深度网络中稳定传播。[@problem_id:3094645]", "problem": "考虑一个深度为 $L$ 的全连接网络，每层宽度为 $n$，偏置为零，且权重 $W_{ij}^{\\ell} \\sim \\mathcal{N}(0, \\sigma_w^2/n)$ 对于层索引 $\\ell \\in \\{1, \\dots, L\\}$ 是独立同分布的。设输入 $h^0$ 的坐标是独立同分布的，其均值为 $0$ 且方差有限。定义第 $\\ell$ 层的激活前的值为 $z^{\\ell} = W^{\\ell} h^{\\ell-1}$，激活值为 $h^{\\ell} = \\phi(z^{\\ell})$，其中 $\\phi$ 是 logistic sigmoid 函数 $\\sigma(x) = \\frac{1}{1 + \\exp(-x)}$、双曲正切函数 $\\tanh(x)$ 或整流线性单元 (ReLU) $\\mathrm{ReLU}(x) = \\max\\{0, x\\}$。根据中心极限定理 (CLT)，在无限宽度极限 $n \\to \\infty$ 下，$z^{\\ell}$ 的每个坐标近似服从均值为 $0$、方差为 $q_{\\ell}$ 的高斯分布。\n\n关注输入的一个无穷小扰动 $\\delta h^0$ 的传播。将平均场敏感度 $\\chi$ 定义为在无限宽度极限下，扰动的期望平方范数从一层传播到下一层时变化的乘法因子。从上述定义出发，并使用基本原理，用权重方差 $\\sigma_w^2$ 和在一个典型的激活前的值处求值的激活函数导数的性质来表示 $\\chi$。然后，在小方差情况 $q_{\\ell} \\ll 1$（因此典型的激活前的值接近 $0$）下，计算当 $\\phi = \\sigma$、$\\phi = \\tanh$ 和 $\\phi = \\mathrm{ReLU}$ 时的 $\\chi$，并确定使信号传播处于临界稳定状态（即 $\\chi = 1$）的 $\\sigma_w^2$ 的临界初始化值。按 $\\phi = \\sigma, \\tanh, \\mathrm{ReLU}$ 的顺序提供这三个临界值。将你的最终答案表示为单行矩阵。无需四舍五入。", "solution": "用户希望找到三种不同激活函数的临界权重方差 $\\sigma_w^2$，使得深度神经网络中的信号传播处于临界稳定状态。这种以平均场敏感度 $\\chi=1$ 为特征的状态，可以防止信号扰动在网络层间传播时发生指数增长（爆炸）或衰减（消失）。\n\n### 步骤 1：平均场敏感度 $\\chi$ 的推导\n\n我们首先分析一个无穷小扰动从第 $\\ell-1$ 层到第 $\\ell$ 层的传播。第 $\\ell$ 层的激活前的值和激活值由下式给出：\n$$z^{\\ell} = W^{\\ell} h^{\\ell-1}$$\n$$h^{\\ell} = \\phi(z^{\\ell})$$\n其中 $h^{\\ell-1}$ 是来自前一层的 $n$ 维激活向量，$W^{\\ell}$ 是第 $\\ell$ 层的 $n \\times n$ 权重矩阵。权重 $W_{ij}^{\\ell}$ 是从高斯分布 $\\mathcal{N}(0, \\sigma_w^2/n)$ 中抽取的独立同分布（i.i.d.）随机变量。\n\n第 $\\ell-1$ 层激活值中的一个无穷小扰动 $\\delta h^{\\ell-1}$ 会在第 $\\ell$ 层的激活前的值中引起一个扰动 $\\delta z^{\\ell}$：\n$$\\delta z^{\\ell} = W^{\\ell} \\delta h^{\\ell-1}$$\n这接着会在第 $\\ell$ 层的激活值中引起一个扰动 $\\delta h^{\\ell}$。对于无穷小扰动，我们可以使用激活函数 $\\phi$ 的一阶泰勒展开：\n$$\\delta h^{\\ell} = \\phi(z^{\\ell} + \\delta z^{\\ell}) - \\phi(z^{\\ell}) \\approx \\phi'(z^{\\ell}) \\odot \\delta z^{\\ell}$$\n其中 $\\odot$ 表示逐元素（哈达玛）积。用坐标形式表示为 $(\\delta h^{\\ell})_i = \\phi'((z^{\\ell})_i) (\\delta z^{\\ell})_i$。\n\n平均场敏感度 $\\chi$ 被定义为在无限宽度极限（$n \\to \\infty$）下，连续层之间扰动的期望平方范数的比率：\n$$\\chi = \\frac{\\mathbb{E}[\\|\\delta h^{\\ell}\\|^2]}{\\mathbb{E}[\\|\\delta h^{\\ell-1}\\|^2]}$$\n期望 $\\mathbb{E}[\\cdot]$ 是对初始输入 $h^0$ 的分布和所有网络权重的分布计算的。\n\n让我们计算分子 $\\mathbb{E}[\\|\\delta h^{\\ell}\\|^2]$：\n$$\\mathbb{E}[\\|\\delta h^{\\ell}\\|^2] = \\mathbb{E}\\left[\\sum_{i=1}^{n} ((\\delta h^{\\ell})_i)^2\\right] = \\mathbb{E}\\left[\\sum_{i=1}^{n} (\\phi'((z^{\\ell})_i))^2 ((\\delta z^{\\ell})_i)^2\\right]$$\n根据期望的线性性，并注意到所有神经元索引 $i$ 在统计上是相同的：\n$$\\mathbb{E}[\\|\\delta h^{\\ell}\\|^2] = n \\, \\mathbb{E}\\left[(\\phi'((z^{\\ell})_i))^2 ((\\delta z^{\\ell})_i)^2\\right]$$\n在平均场（无限宽度）极限下，出现一个被称为“混沌”的关键现象。激活前的值 $(z^{\\ell})_i = \\sum_j W_{ij}^{\\ell} h_{j}^{\\ell-1}$ 作为许多弱相关项的和，根据中心极限定理成为一个高斯随机变量。关键的是，它的分布变得与构成它的权重的具体实现无关。这使我们能够将关于激活前的值项 $\\phi'((z^{\\ell})i)$ 的期望与扰动项 $(\\delta z^{\\ell})_i$ 的期望解耦。设 $Z$ 是一个代表典型激活前的值的随机变量，其中 $Z \\sim \\mathcal{N}(0, q_{\\ell})$。期望解耦为：\n$$\\mathbb{E}[\\|\\delta h^{\\ell}\\|^2] = n \\, \\mathbb{E}_{Z \\sim \\mathcal{N}(0, q_{\\ell})}[(\\phi'(Z))^2] \\, \\mathbb{E}[((\\delta z^{\\ell})_i)^2]$$\n现在我们计算激活前扰动平方的期望，即 $\\mathbb{E}[((\\delta z^{\\ell})_i)^2]$：\n$$\\mathbb{E}[((\\delta z^{\\ell})_i)^2] = \\mathbb{E}\\left[\\left(\\sum_{j=1}^{n} W_{ij}^{\\ell} (\\delta h^{\\ell-1})_j\\right)^2\\right] = \\mathbb{E}\\left[\\sum_{j,k=1}^{n} W_{ij}^{\\ell} W_{ik}^{\\ell} (\\delta h^{\\ell-1})_j (\\delta h^{\\ell-1})_k\\right]$$\n权重 $W^{\\ell}$ 与扰动 $\\delta h^{\\ell-1}$ 无关，所以我们可以分离期望。由于权重是独立同分布的，且 $\\mathbb{E}[W_{ij}^{\\ell} W_{ik}^{\\ell}] = \\delta_{jk} \\mathrm{Var}(W_{ij}^{\\ell}) = \\delta_{jk} (\\sigma_w^2/n)$：\n$$\\mathbb{E}[((\\delta z^{\\ell})_i)^2] = \\sum_{j,k=1}^{n} \\mathbb{E}[W_{ij}^{\\ell} W_{ik}^{\\ell}] \\, \\mathbb{E}[(\\delta h^{\\ell-1})_j (\\delta h^{\\ell-1})_k] = \\sum_{j=1}^{n} \\frac{\\sigma_w^2}{n} \\mathbb{E}[((\\delta h^{\\ell-1})_j)^2]$$\n假设神经元之间的扰动是不相关的（这是独立同分布随机结构的一个结果），并且由于所有神经元 $j$ 的统计相似性：\n$$\\mathbb{E}[((\\delta z^{\\ell})_i)^2] = \\frac{\\sigma_w^2}{n} \\sum_{j=1}^{n} \\mathbb{E}[((\\delta h^{\\ell-1})_j)^2] = \\frac{\\sigma_w^2}{n} \\mathbb{E}[\\|\\delta h^{\\ell-1}\\|^2]$$\n将此代入 $\\mathbb{E}[\\|\\delta h^{\\ell}\\|^2]$ 的表达式中：\n$$\\mathbb{E}[\\|\\delta h^{\\ell}\\|^2] = n \\, \\mathbb{E}_{Z \\sim \\mathcal{N}(0, q_{\\ell})}[(\\phi'(Z))^2] \\left( \\frac{\\sigma_w^2}{n} \\mathbb{E}[\\|\\delta h^{\\ell-1}\\|^2] \\right)$$\n$$\\mathbb{E}[\\|\\delta h^{\\ell}\\|^2] = \\sigma_w^2 \\, \\mathbb{E}_{Z \\sim \\mathcal{N}(0, q_{\\ell})}[(\\phi'(Z))^2] \\, \\mathbb{E}[\\|\\delta h^{\\ell-1}\\|^2]$$\n由此，我们确定敏感度 $\\chi$ 为：\n$$\\chi = \\sigma_w^2 \\, \\mathbb{E}_{Z \\sim \\mathcal{N}(0, q_{\\ell})}[(\\phi'(Z))^2]$$\n\n### 步骤 2：计算每种激活函数的临界 $\\sigma_w^2$\n\n问题指定了小方差情况，$q_{\\ell} \\ll 1$。这意味着激活前的值 $z^{\\ell}$ 集中在 $0$ 附近。我们必须找到导致 $\\chi=1$ 的 $\\sigma_w^2$ 的临界值。\n\n**1. Logistic Sigmoid 函数：** $\\phi(x) = \\sigma(x) = \\frac{1}{1 + \\exp(-x)}$\n对于像 sigmoid 这样的平滑函数，在极限 $q_{\\ell} \\to 0$ 时，高斯分布 $\\mathcal{N}(0, q_{\\ell})$ 趋近于 $0$ 处的狄拉克δ函数。因此，期望变成了在 $z=0$ 处的求值：\n$$\\mathbb{E}_{Z \\sim \\mathcal{N}(0, q_{\\ell})}[(\\phi'(Z))^2] \\approx (\\phi'(0))^2$$\n首先，我们求导数 $\\phi'(x)$：\n$$\\phi'(x) = \\sigma'(x) = \\sigma(x)(1-\\sigma(x)) = \\frac{\\exp(-x)}{(1+\\exp(-x))^2}$$\n在 $x=0$ 处：\n$$\\phi'(0) = \\sigma(0)(1-\\sigma(0)) = \\frac{1}{1+1}\\left(1-\\frac{1}{1+1}\\right) = \\frac{1}{2} \\cdot \\frac{1}{2} = \\frac{1}{4}$$\n敏感度为 $\\chi \\approx \\sigma_w^2 (\\phi'(0))^2 = \\sigma_w^2 (1/4)^2 = \\sigma_w^2/16$。\n对于临界稳定性，$\\chi=1$：\n$$\\frac{\\sigma_w^2}{16} = 1 \\implies \\sigma_w^2 = 16$$\n\n**2. 双曲正切函数：** $\\phi(x) = \\tanh(x)$\n这也是一个平滑函数，所以我们使用与 sigmoid 相同的近似方法。\n导数为 $\\phi'(x) = 1 - \\tanh^2(x)$。\n在 $x=0$ 处：\n$$\\phi'(0) = 1 - \\tanh^2(0) = 1 - 0^2 = 1$$\n敏感度为 $\\chi \\approx \\sigma_w^2 (\\phi'(0))^2 = \\sigma_w^2 (1)^2 = \\sigma_w^2$。\n对于临界稳定性，$\\chi=1$：\n$$\\sigma_w^2 = 1$$\n这就是著名的 Glorot/Xavier 初始化条件。\n\n**3. 整流线性单元 (ReLU)：** $\\phi(x) = \\mathrm{ReLU}(x) = \\max\\{0, x\\}$\nReLU 函数在 $x=0$ 处不可微。它的导数是亥维赛阶跃函数 $\\phi'(x) = H(x)$，当 $x0$ 时为 $1$，当 $x0$ 时为 $0$。我们不能使用在 $x=0$ 附近的泰勒展开。相反，我们必须直接计算期望 $\\mathbb{E}[(\\phi'(Z))^2]$。\n设 $Z \\sim \\mathcal{N}(0, q_{\\ell})$。导数的平方是 $(\\phi'(Z))^2 = (H(Z))^2$。由于 $H(Z)$ 的取值在 $\\{0, 1\\}$ 中，我们有 $(H(Z))^2 = H(Z)$。\n$$\\mathbb{E}[(\\phi'(Z))^2] = \\mathbb{E}[H(Z)] = P(Z  0)$$\n由于高斯分布 $\\mathcal{N}(0, q_{\\ell})$ 关于其均值 $0$ 对称，所以抽取的值为正的概率恰好是 $1/2$。\n$$P(Z  0) = \\int_{0}^{\\infty} \\frac{1}{\\sqrt{2\\pi q_{\\ell}}} \\exp\\left(-\\frac{z^2}{2q_{\\ell}}\\right) dz = \\frac{1}{2}$$\n这个结果与方差 $q_{\\ell}$ 无关（只要 $q_{\\ell}0$）。\n敏感度为 $\\chi = \\sigma_w^2 \\cdot \\frac{1}{2}$。\n对于临界稳定性，$\\chi=1$：\n$$\\frac{\\sigma_w^2}{2} = 1 \\implies \\sigma_w^2 = 2$$\n这就是著名的 He 初始化条件。\n\n对于 $\\phi = \\sigma$、$\\tanh$ 和 $\\mathrm{ReLU}$，$\\sigma_w^2$ 的三个临界值分别为 $16$、$1$ 和 $2$。", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n16  1  2\n\\end{pmatrix}\n}\n$$", "id": "3094645"}]}