{"hands_on_practices": [{"introduction": "要想熟练运用任何统计工具，第一步是掌握其基本公式。本练习提供了一个清晰的场景，旨在帮助你实践贝叶斯信息准则 (BIC) 的计算。通过这个练习，你将学会如何准确识别模型中的参数数量 $k$，并将其与样本量 $n$ 和最大化对数似然 $\\ell$ 一起代入 BIC 公式 [@problem_id:806248]，为后续更复杂的模型选择问题打下坚实的计算基础。", "problem": "**背景**\n\n一个随机变量 $Y$ 服从均值为 $\\mu > 0$ 且离散参数为 $\\theta > 0$ 的负二项分布，记作 $Y \\sim \\text{NB}(\\mu, \\theta)$，如果其概率质量函数由下式给出：\n$$ P(Y=y) = \\frac{\\Gamma(y+\\theta)}{\\Gamma(y+1)\\Gamma(\\theta)} \\left(\\frac{\\theta}{\\theta+\\mu}\\right)^\\theta \\left(\\frac{\\mu}{\\theta+\\mu}\\right)^y, \\quad \\text{对于 } y \\in \\{0, 1, 2, \\dots\\}. $$\n其均值和方差分别为 $E[Y] = \\mu$ 和 $\\text{Var}(Y) = \\mu + \\mu^2/\\theta$。\n\n在负二项回归模型中，响应变量 $Y_i$ 的条件均值 $\\mu_i$ 被建模为预测变量向量 $\\mathbf{x}_i$ 的函数。一个常见的设定是使用对数链接函数：\n$$ \\log(\\mu_i) = \\mathbf{x}_i^T \\boldsymbol{\\beta}, $$\n其中 $\\boldsymbol{\\beta}$ 是回归系数向量。该模型中需要从数据中估计的参数是回归系数 $\\boldsymbol{\\beta}$ 和共同的离散参数 $\\theta$。\n\n贝叶斯信息准则 (BIC) 是一种广泛使用的模型选择准则，定义为：\n$$ \\text{BIC} = k \\ln(n) - 2 \\ell, $$\n其中 $n$ 是样本量，$k$ 是模型中估计参数的总数，$\\ell$ 是模型对数似然函数的最大化值。\n\n**问题描述**\n\n一位生物统计学家正在使用负二项回归模型对植物叶片上的病斑数量进行建模。该分析基于 $n$ 片叶子的样本。回归模型包含 $p$ 个不同的预测变量（例如植物年龄、土壤pH值和日照时长）以及一个截距项。\n\n在进行最大似然估计后，确定该模型的最大化对数似然为：\n$$ \\ell = Cnp - D n \\log(n) $$\n其中 $C$ 和 $D$ 是给定的正常数。\n\n你的任务是推导该负二项回归模型的贝叶斯信息准则 (BIC) 的表达式，用 $n$、$p$、$C$ 和 $D$ 表示。", "solution": "1. 估计参数的数量：\n$$\nk = p + 1\\ (\\text{系数，包括截距项}) + 1\\ (\\theta)\n= p + 2.\n$$\n2. BIC 定义：\n$$\n\\mathrm{BIC} = k\\ln(n) \\;-\\; 2\\,\\ell.\n$$\n3. 代入 $k$ 和 $\\ell$：\n$$\n\\mathrm{BIC} \n= (p+2)\\ln(n) \\;-\\; 2\\bigl(Cnp - Dn\\ln(n)\\bigr).\n$$\n4. 展开并简化：\n$$\n\\mathrm{BIC}\n= (p+2)\\ln(n) - 2Cnp + 2D\\,n\\ln(n).\n$$", "answer": "$$\\boxed{(p + 2)\\ln(n) + 2D\\,n\\ln(n) - 2C\\,n\\,p}$$", "id": "806248"}, {"introduction": "在模型选择的领域，BIC 并非唯一的准则，与它齐名的还有赤池信息准则 (AIC)。本练习通过直接对比这两种方法，揭示了 BIC 的一个核心特性。通过推导 BIC 与 AIC 对模型偏好发生分歧的临界样本量 [@problem_id:2734851]，你将深刻理解 BIC 是如何随着数据量的增加而更严厉地惩罚模型复杂度的，从而在实践中倾向于选择更简洁的模型。", "problem": "考虑两个用于固定系统发育树上核苷酸演化的嵌套连续时间马尔可夫链替换模型：一个较简单的模型 $\\mathcal{M}_{s}$（有 $k_{s}$ 个自由参数）和一个较复杂的模型 $\\mathcal{M}_{c}$（有 $k_{c}$ 个自由参数），其中 $\\Delta k = k_{c} - k_{s} > 0$。设比对长度为 $n$ 个独立同分布的位点。设$\\ell_c$和$\\ell_s$分别为复杂模型和简单模型的最大化对数似然。假设对于在 $\\mathcal{M}_{s}$ 模型下生成的数据，$\\mathcal{M}_{c}$ 相对于 $\\mathcal{M}_{s}$ 的最大对数似然增益是一个固定常数 $D = \\ell_{c} - \\ell_{s} > 0$，该常数不随 $n$ 的增加而增加（这反映了不随独立位点增加而变化的过拟合增益）。使用赤池信息准则 (AIC) 和贝叶斯信息准则 (BIC) 的标准定义，并将 $n$ 视为样本量，完成以下任务：\n\n- 从第一性原理出发，解释 BIC 中的惩罚项如何依赖于 $\\ln n$，而 AIC 中的惩罚项不依赖于 $n$。\n- 推导一个精确的、封闭形式的表达式，用于计算最小比对长度 $n_{\\star}$。在此长度下，BIC 会选择 $\\mathcal{M}_{s}$ 而 AIC 仍然选择 $\\mathcal{M}_{c}$。表达式应以 $D$ 和 $\\Delta k$ 表示。\n\n假设 $D > \\Delta k$，因此对于较小的 $n$，赤池信息准则最初会偏好 $\\mathcal{M}_{c}$。你的最终答案必须是 $n_{\\star}$ 的单个解析表达式。不需要进行数值计算，也不需要单位。", "solution": "我们从模型选择中使用的两个信息准则的标准定义开始。对于一个最大化对数似然为 $\\ell$、参数数量为 $k$、样本量为 $n$ 的模型，赤池信息准则 (AIC) 和贝叶斯信息准则 (BIC) 分别是\n$$\n\\mathrm{AIC} = -2 \\ell + 2 k, \\quad \\mathrm{BIC} = -2 \\ell + k \\ln n.\n$$\n这些是经过充分检验的公式，分别源于对Kullback–Leibler风险最小化的渐近估计，以及在特定先验正则性条件下对边际似然的大样本近似。\n\n首先，我们展示惩罚项如何随 $n$ 变化。AIC 的惩罚项是 $2k$，它相对于 $n$ 是一个常数。BIC 的惩罚项是 $k \\ln n$，它随着 $n$ 对数增长，因为当 $n > 1$ 时，$\\ln n$ 是 $n$ 的单调递增函数。因此，随着 $n$ 的增长，BIC 的惩罚项与 $\\ln n$ 成比例地无界增加，而 AIC 的惩罚项保持不变。\n\n接下来，比较两个嵌套模型 $\\mathcal{M}_{c}$（复杂）和 $\\mathcal{M}_{s}$（简单）。定义 $\\Delta k = k_{c} - k_{s} > 0$ 和 $D = \\ell_{c} - \\ell_{s} > 0$。考虑这两个准则的差值：\n$$\n\\Delta \\mathrm{AIC} \\equiv \\mathrm{AIC}_{c} - \\mathrm{AIC}_{s} = \\left[-2 \\ell_{c} + 2 k_{c}\\right] - \\left[-2 \\ell_{s} + 2 k_{s}\\right] = -2(\\ell_{c} - \\ell_{s}) + 2 (k_{c} - k_{s}) = -2D + 2 \\Delta k.\n$$\n类似地，\n$$\n\\Delta \\mathrm{BIC} \\equiv \\mathrm{BIC}_{c} - \\mathrm{BIC}_{s} = \\left[-2 \\ell_{c} + k_{c} \\ln n\\right] - \\left[-2 \\ell_{s} + k_{s} \\ln n\\right] = -2(\\ell_{c} - \\ell_{s}) + (k_{c} - k_{s}) \\ln n = -2D + \\Delta k \\, \\ln n.\n$$\n当一个模型产生较小的准则值时，它被该准则所偏好。因此，当 $\\Delta \\mathrm{AIC}  0$ 时，AIC 偏好 $\\mathcal{M}_{c}$；当 $\\Delta \\mathrm{AIC}  0$ 时，AIC 偏好 $\\mathcal{M}_{s}$。对于 BIC 和 $\\Delta \\mathrm{BIC}$ 也是如此。根据假设，$D  \\Delta k$，这意味着\n$$\n\\Delta \\mathrm{AIC} = -2D + 2 \\Delta k  0,\n$$\n因此，赤池信息准则偏好 $\\mathcal{M}_{c}$，这与 $n$ 无关，因为 $\\Delta \\mathrm{AIC}$ 不依赖于 $n$。\n\n为了让贝叶斯信息准则选择更简单的模型，我们需要 $\\Delta \\mathrm{BIC}  0$，即\n$$\n-2D + \\Delta k \\, \\ln n  0 \\quad \\Longleftrightarrow \\quad \\ln n  \\frac{2D}{\\Delta k} \\quad \\Longleftrightarrow \\quad n  \\exp\\!\\left(\\frac{2D}{\\Delta k}\\right).\n$$\n因此，当贝叶斯信息准则开始偏好更简单的模型（而 AIC 仍然偏好复杂模型）时，最小的比对长度阈值为\n$$\nn_{\\star} = \\exp\\!\\left(\\frac{2D}{\\Delta k}\\right).\n$$\n在所述假设 $D  \\Delta k$ 下，这个阈值是良定义的。该假设确保了 AIC 持续偏好 $\\mathcal{M}_{c}$ 而不受 $n$ 影响，而一旦 $\\ln n$ 惩罚项超过固定的过拟合增益 $D$，BIC 最终会偏好 $\\mathcal{M}_{s}$。", "answer": "$$\\boxed{\\exp\\!\\left(\\frac{2D}{\\Delta k}\\right)}$$", "id": "2734851"}, {"introduction": "理论通过模拟得以鲜活。这个练习将带你从纸笔推导走向代码实践，模拟回归分析中一个常见挑战：多重共线性。你将亲眼见证 BIC 的惩罚机制如何有效地识别并剔除那些由于高度相关而几乎不提供新信息的冗余预测变量 [@problem_id:3102696]，这清晰地展示了 BIC 在构建更简单、更易于解释的模型中的关键作用。", "problem": "考虑一个高斯线性回归模型，其中响应向量 $y \\in \\mathbb{R}^n$ 是由两个预测变量 $x_1 \\in \\mathbb{R}^n$ 和 $x_2 \\in \\mathbb{R}^n$ 以及一个截距项生成的。您将研究 $x_1$ 和 $x_2$ 之间的高度共线性对通过贝叶斯信息准则（BIC）进行模型选择的影响，并将其与方差膨胀因子（VIF）以及最大化对数似然的变化联系起来。任务是从高斯线性模型的似然函数出发，从基本原理开始实现所有内容。\n\n基本基础：\n- 假设数据由高斯线性模型 $y \\mid X, \\beta, \\sigma^2 \\sim \\mathcal{N}(X \\beta, \\sigma^2 I_n)$ 生成，其中 $X$ 是包含截距和预测变量的设计矩阵，$\\beta$ 是回归系数向量，$\\sigma^2$ 是噪声方差。对数似然必须从此模型导出。\n- 通过使用独立的标准正态噪声构造与 $x_1$ 具有目标相关性的 $x_2$ 来生成共线性预测变量。\n\n实现所需的定义：\n- 贝叶斯信息准则（BIC）必须使用其基于高斯线性模型的最大化对数似然和与自由参数数量及样本大小成比例的复杂度项的定义来实现。将所有回归系数（包括截距）加上方差参数计为自由参数。\n- 预测变量 $x_j$ 的方差膨胀因子（VIF）是利用将 $x_j$ 对其余预测变量进行回归时的决定系数来定义的。\n\n每次模拟需要比较的候选模型：\n- 全模型：截距、 $x_1$ 和 $x_2$。\n- 简化模型：仅截距和 $x_1$。\n\n对于下面的测试套件中的每个参数集，执行以下步骤：\n1. 首先从标准正态分布生成 $x_1$，然后设置 $x_2 = \\rho \\, x_1 + \\sqrt{1 - \\rho^2} \\, \\eta$（其中 $\\eta$ 是一个独立的标准正态向量），从而构造长度为 $n$ 的相关预测变量 $x_1$ 和 $x_2$。使用提供的随机种子以确保可复现性。\n2. 根据 $y = \\alpha + \\beta_1 x_1 + \\beta_2 x_2 + \\varepsilon$（其中 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I_n)$）使用指定的参数生成 $y$。设置截距 $\\alpha = 0$。\n3. 对于两个候选模型中的每一个，计算回归系数和噪声方差的最大似然估计，然后计算最大化对数似然。使用这些值计算贝叶斯信息准则（BIC），其中自由参数的数量计为所有回归系数（包括截距）加上方差参数。\n4. 在全模型中，通过将 $x_2$ 对截距和 $x_1$ 进行回归，并使用该辅助回归中的决定系数，计算 $x_2$ 相对于 $x_1$ 的方差膨胀因子（VIF）。\n5. 通过检查简化模型的BIC是否严格小于全模型的BIC，来确定BIC是否舍弃了冗余变量。将此结果报告为一个整数：如果简化模型更优（舍弃 $x_2$），则为 $1$，否则为 $0$。\n6. 计算全模型和简化模型之间最大化对数似然的绝对差。\n7. 对于每个测试用例，输出一个包含三项的列表：整数选择指示符、四舍五入到四位小数的 $x_2$ 的VIF，以及四舍五入到六位小数的最大化对数似然的绝对差。\n\n测试套件参数：\n- 案例 1：$n = 200$, $\\rho = 0.95$, $\\beta_1 = 1.0$, $\\beta_2 = 0.0$, $\\sigma = 1.0$, 种子 $= 42$。\n- 案例 2：$n = 200$, $\\rho = 0.95$, $\\beta_1 = 1.0$, $\\beta_2 = 0.1$, $\\sigma = 1.0$, 种子 $= 123$。\n- 案例 3：$n = 200$, $\\rho = 0.0$, $\\beta_1 = 1.0$, $\\beta_2 = 0.5$, $\\sigma = 1.0$, 种子 $= 7$。\n- 案例 4：$n = 40$, $\\rho = 0.9$, $\\beta_1 = 1.0$, $\\beta_2 = 0.0$, $\\sigma = 1.0$, 种子 $= 777$。\n- 案例 5：$n = 200$, $\\rho = 0.99$, $\\beta_1 = 1.0$, $\\beta_2 = 0.5$, $\\sigma = 1.0$, 种子 $= 2024$。\n\n最终输出格式：\n您的程序应生成一行输出，其中包含一个用方括号括起来的逗号分隔列表，列表内无空格。每个元素对应于给定顺序的一个测试用例，并且其本身是如步骤 7 中所述格式化的列表。例如，输出应类似于 $[[s_1,v_1,d_1],[s_2,v_2,d_2],\\dots]$，其中 $s_i$ 是选择指示符，$v_i$ 是舍入后的VIF，$d_i$ 是案例 $i$ 的舍入后绝对对数似然差。", "solution": "该问题要求在具有共线性预测变量的高斯线性回归模型背景下，对贝叶斯信息准则（BIC）进行透彻分析。该分析涉及从基本原理开始实现必要的统计工具——最大似然估计（MLE）、BIC 和方差膨胀因子（VIF）。\n\n### 理论框架\n\n#### 1. 高斯线性模型和最大似然估计\n\n此分析的基础是高斯线性模型。我们假设响应向量 $y \\in \\mathbb{R}^n$ 的生成方式如下：\n$$\ny = X\\beta + \\varepsilon\n$$\n其中 $X$ 是 $n \\times (p+1)$ 的设计矩阵（包含一个截距列），$\\beta \\in \\mathbb{R}^{p+1}$ 是回归系数向量，$\\varepsilon$ 是一个独立同分布（i.i.d.）的高斯噪声向量，即 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I_n)$。\n\n单个观测值 $y_i$ 的概率密度函数是：\n$$\nf(y_i \\mid x_i, \\beta, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y_i - x_i^T\\beta)^2}{2\\sigma^2}\\right)\n$$\n鉴于观测值是独立的，整个数据集的似然函数是各个密度的乘积：\n$$\nL(\\beta, \\sigma^2 \\mid y, X) = \\prod_{i=1}^n \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y_i - x_i^T\\beta)^2}{2\\sigma^2}\\right) = (2\\pi\\sigma^2)^{-n/2} \\exp\\left(-\\frac{1}{2\\sigma^2}\\|y - X\\beta\\|^2\\right)\n$$\n对数似然函数 $\\ell(\\beta, \\sigma^2) = \\log L(\\beta, \\sigma^2)$ 是：\n$$\n\\ell(\\beta, \\sigma^2) = -\\frac{n}{2}\\log(2\\pi) - \\frac{n}{2}\\log(\\sigma^2) - \\frac{1}{2\\sigma^2}\\|y - X\\beta\\|^2\n$$\n为了找到最大似然估计（MLEs），我们相对于 $\\beta$ 和 $\\sigma^2$ 最大化 $\\ell$。\n相对于 $\\beta$ 最大化 $\\ell$ 等价于最小化残差平方和（RSS），即 $\\|y - X\\beta\\|^2$。这得出了标准的普通最小二乘法（OLS）估计量：\n$$\n\\hat{\\beta}_{MLE} = (X^TX)^{-1}X^Ty\n$$\n将 $\\hat{\\beta}_{MLE}$ 代入对数似然函数并对 $\\sigma^2$ 求导，得到方差的MLE：\n$$\n\\hat{\\sigma}^2_{MLE} = \\frac{1}{n}\\|y - X\\hat{\\beta}_{MLE}\\|^2 = \\frac{\\text{RSS}}{n}\n$$\n最大化的对数似然，记为 $\\ell(\\hat{\\theta})$，其中 $\\hat{\\theta} = (\\hat{\\beta}_{MLE}, \\hat{\\sigma}^2_{MLE})$，是通过将这些MLE代回对数似然函数得到的：\n$$\n\\ell(\\hat{\\theta}) = -\\frac{n}{2}\\left( \\log(2\\pi) + \\log(\\hat{\\sigma}^2_{MLE}) + 1 \\right)\n$$\n这个公式是计算BIC的核心。\n\n#### 2. 贝叶斯信息准则 (BIC)\n\nBIC是一种模型选择准则，它在模型拟合度（由最大化对数似然衡量）和模型复杂度之间进行权衡。其公式为：\n$$\n\\text{BIC} = k \\log(n) - 2\\ell(\\hat{\\theta})\n$$\n其中 $n$ 是样本数量，$k$ 是模型中的自由参数数量。较低的BIC值表示更优的模型。\n\n按照规定，$k$ 是所有估计参数的数量。对于一个有 $p$ 个预测变量的线性模型，我们有 $p+1$ 个回归系数（包括截距）和一个方差参数 $\\sigma^2$。因此，$k = p+2$。\n对于我们考虑的两个模型：\n- **全模型 ($M_f$)**：$y$ 对一个截距、$x_1$ 和 $x_2$ 进行回归。这里 $p=2$，所以自由参数的数量是 $k_f = 2 + 2 = 4$。\n- **简化模型 ($M_r$)**：$y$ 对一个截距和 $x_1$ 进行回归。这里 $p=1$，所以自由参数的数量是 $k_r = 1 + 2 = 3$。\n\n每个模型的BIC是：\n$$\n\\text{BIC}_f = 4 \\log(n) - 2\\ell(\\hat{\\theta}_f)\n$$\n$$\n\\text{BIC}_r = 3 \\log(n) - 2\\ell(\\hat{\\theta}_r)\n$$\n如果 $\\text{BIC}_r  \\text{BIC}_f$，则简化模型更优。\n\n#### 3. 方差膨胀因子 (VIF)\n\nVIF量化了OLS回归中多重共线性的严重程度。预测变量 $x_j$ 的VIF由下式给出：\n$$\n\\text{VIF}_j = \\frac{1}{1 - R_j^2}\n$$\n其中 $R_j^2$ 是将 $x_j$ 对模型中其他预测变量（包括截距）进行辅助回归得到的决定系数。高的VIF（通常 $5$ 或 $10$）表明该预测变量与其他预测变量高度相关，这会夸大其系数估计的方差。\n\n对于这个问题，我们计算全模型中 $x_2$ 的VIF。这涉及到将 $x_2$ 对一个截距和 $x_1$ 进行回归。这个回归的 $R^2$，即 $R^2_{x_2|x_1}$，计算如下：\n$$\nR^2_{x_2|x_1} = 1 - \\frac{\\text{RSS}_{aux}}{\\text{TSS}_{aux}}\n$$\n其中 $\\text{RSS}_{aux}$ 是辅助回归的残差平方和，$\\text{TSS}_{aux}$ 是 $x_2$ 的总平方和。\n\n### 计算过程\n\n对于每个测试用例，执行以下计算序列：\n\n1.  **数据生成**：\n    - 使用指定的种子初始化随机数生成器以保证可复现性。\n    - 从标准正态分布 $\\mathcal{N}(0, 1)$ 中抽取一个 $n$ 维向量 $x_1$。\n    - 同样从 $\\mathcal{N}(0, 1)$ 中抽取一个独立的 $n$ 维噪声向量 $\\eta$。\n    - 相关的预测变量 $x_2$ 构造为 $x_2 = \\rho x_1 + \\sqrt{1 - \\rho^2} \\eta$，其中 $\\rho$ 是目标相关性。\n    - 从 $\\mathcal{N}(0, \\sigma^2 I_n)$ 中抽取一个噪声向量 $\\varepsilon$。\n    - 通过真实模型生成响应向量 $y$：$y = \\beta_1 x_1 + \\beta_2 x_2 + \\varepsilon$（截距 $\\alpha = 0$）。\n\n2.  **模型拟合与BIC计算**：\n    - 设计一个统一的函数，用于为任何给定的设计矩阵 $X$ 和响应 $y$ 计算最大化对数似然和参数计数。\n    - **全模型 ($M_f$)**：通过连接一个全为1的列、$x_1$ 和 $x_2$ 来形成设计矩阵 $X_f$。使用 $X_f$ 和 $y$，计算最大化对数似然 $\\ell(\\hat{\\theta}_f)$ 和参数计数 $k_f=4$，然后用它们来计算 $\\text{BIC}_f$。\n    - **简化模型 ($M_r$)**：用一个全为1的列和 $x_1$ 形成设计矩阵 $X_r$。类似地，计算 $\\ell(\\hat{\\theta}_r)$、$k_r=3$ 和 $\\text{BIC}_r$。\n\n3.  **VIF计算**：\n    - 计算 $x_2$ 的VIF。辅助设计矩阵 $X_{aux}$ 由一个全为1的列和 $x_1$ 构成。\n    - 对 $x_2$ 在 $X_{aux}$ 上进行OLS回归，以找到残差平方和 $\\text{RSS}_{aux}$。\n    - 计算 $x_2$ 的总平方和 $\\text{TSS}_{aux}$。\n    - 使用它们的定义计算决定系数 $R^2$ 和随后的VIF。\n\n4.  **结果汇总**：\n    - 确定选择指示符：如果 $\\text{BIC}_r  \\text{BIC}_f$，则为 $1$，否则为 $0$。\n    - 计算最大化对数似然的绝对差 $|\\ell(\\hat{\\theta}_f) - \\ell(\\hat{\\theta}_r)|$。\n    - 该案例的最终输出是一个列表，包含选择指示符、$x_2$ 的VIF（四舍五入到4位小数）和对数似然差（四舍五入到6位小数）。对所有测试用例重复此过程。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Performs the full analysis for each test case as specified in the problem statement.\n    This involves data generation, model fitting (full and reduced), BIC comparison,\n    VIF calculation, and log-likelihood difference computation.\n    \"\"\"\n    test_cases = [\n        # (n, rho, beta1, beta2, sigma, seed)\n        (200, 0.95, 1.0, 0.0, 1.0, 42),\n        (200, 0.95, 1.0, 0.1, 1.0, 123),\n        (200, 0.0, 1.0, 0.5, 1.0, 7),\n        (40, 0.9, 1.0, 0.0, 1.0, 777),\n        (200, 0.99, 1.0, 0.5, 1.0, 2024),\n    ]\n\n    results = []\n\n    def compute_mle_metrics(X, y):\n        \"\"\"\n        Computes the maximized log-likelihood and parameter count for a linear model.\n\n        Args:\n            X (np.ndarray): The design matrix (n_samples, n_features_with_intercept).\n            y (np.ndarray): The response vector (n_samples,).\n\n        Returns:\n            tuple: A tuple containing:\n                - float: The maximized log-likelihood.\n                - int: The number of free parameters (coefficients + variance).\n        \"\"\"\n        n_samples = X.shape[0]\n        p_coeffs = X.shape[1]\n\n        # Solve for coefficients and RSS using numerically stable least squares\n        try:\n            _, rss_array, _, _ = np.linalg.lstsq(X, y, rcond=None)\n            # lstsq returns rss as an array, even if it's a single value\n            rss = rss_array[0]\n        except np.linalg.LinAlgError:\n            return -np.inf, p_coeffs + 1\n        \n        # MLE for variance sigma^2\n        mle_var = rss / n_samples\n\n        if mle_var  1e-9:  # Avoid log(0) for perfect fits\n            log_likelihood = np.inf if np.allclose(y, X @ np.linalg.lstsq(X, y, rcond=None)[0]) else -np.inf\n        else:\n            # Maximized log-likelihood for Gaussian model\n            log_likelihood = -n_samples / 2.0 * (np.log(2.0 * np.pi) + np.log(mle_var) + 1.0)\n            \n        # Number of free parameters: p coefficients + 1 variance parameter\n        k = p_coeffs + 1\n        return log_likelihood, k\n\n    for n, rho, beta1, beta2, sigma, seed in test_cases:\n        rng = np.random.default_rng(seed)\n\n        # Step 1: Construct correlated predictors\n        x1 = rng.normal(size=n)\n        eta = rng.normal(size=n)\n        x2 = rho * x1 + np.sqrt(1 - rho**2) * eta\n\n        # Step 2: Generate response variable y\n        epsilon = rng.normal(loc=0, scale=sigma, size=n)\n        y = beta1 * x1 + beta2 * x2 + epsilon  # Intercept alpha is 0\n\n        # Step 3: Analyze full and reduced models\n        # Full model: intercept, x1, x2\n        X_f = np.c_[np.ones(n), x1, x2]\n        logL_f, k_f = compute_mle_metrics(X_f, y)\n        bic_f = k_f * np.log(n) - 2 * logL_f\n\n        # Reduced model: intercept, x1\n        X_r = np.c_[np.ones(n), x1]\n        logL_r, k_r = compute_mle_metrics(X_r, y)\n        bic_r = k_r * np.log(n) - 2 * logL_r\n\n        # Step 4: Compute VIF for x2\n        # Auxiliary regression: x2 on intercept and x1\n        X_aux = np.c_[np.ones(n), x1]\n        try:\n            _, rss_aux_array, _, _ = np.linalg.lstsq(X_aux, x2, rcond=None)\n            rss_aux = rss_aux_array.item()\n            # Total sum of squares for x2\n            tss_aux = np.sum((x2 - np.mean(x2))**2)\n            if tss_aux  1e-9: # x2 has no variance, R^2 is undefined\n                vif_x2 = 1.0\n            else:\n                r_squared_aux = 1 - rss_aux / tss_aux\n                if r_squared_aux >= 1.0: # Perfect collinearity\n                    vif_x2 = np.inf\n                else:    \n                    vif_x2 = 1 / (1 - r_squared_aux)\n        except np.linalg.LinAlgError:\n            vif_x2 = np.inf\n\n        # Step 5: Determine selection based on BIC\n        selection_indicator = 1 if bic_r  bic_f else 0\n\n        # Step 6: Compute absolute difference in maximized log-likelihood\n        log_likelihood_diff = np.abs(logL_f - logL_r)\n\n        # Step 7: Store results for this case\n        results.append([\n            selection_indicator,\n            round(vif_x2, 4),\n            round(log_likelihood_diff, 6)\n        ])\n\n    # Final print statement in the exact required format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3102696"}]}