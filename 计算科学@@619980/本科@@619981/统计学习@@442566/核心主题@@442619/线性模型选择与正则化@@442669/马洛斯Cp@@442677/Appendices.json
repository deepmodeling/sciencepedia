{"hands_on_practices": [{"introduction": "在建模过程中，一个核心挑战是决定包含哪些特征。Mallows' $C_p$ 统计量提供了一种在模型拟合优度（较低的残差平方和）和模型复杂度（参数数量）之间进行权衡的原则性方法。本练习将通过一个常见的场景指导您，即评估添加非线性特征是否能真正改善模型，从而让您亲手实践如何应用 $C_p$ 进行模型选择[@problem_id:3143724]。", "problem": "考虑一个带有线性回归模型的监督学习场景。设响应向量为 $y \\in \\mathbb{R}^n$，预测变量矩阵为 $X \\in \\mathbb{R}^{n \\times p}$，模型中包含一个作为独立参数的截距项。假设采用经典的同方差独立噪声模型，其中误差是独立同分布的 (i.i.d.)，均值为 $0$，方差为 $\\sigma^2$。给定一个外部的误差方差估计值，记为 $\\hat{\\sigma}^2$，你将用它来评估向模型中添加非线性项的效果。具体来说，你将探究当用预测变量的平方项来增强线性模型时，被称为 Mallows' $C_p$ 统计量的选择准则的敏感性，并且你将计算从线性模型转换到二次增广模型（即为每个原始预测变量 $x_j$ 添加 $x_j^2$）时 $C_p$ 的变化。\n\n你的任务是编写一个程序，该程序：\n- 使用普通最小二乘法 (OLS) 拟合线性模型，以计算训练残差平方和，称为残差平方和 (RSS)。\n- 使用普通最小二乘法 (OLS) 拟合二次增广模型，其中每个原始预测变量 $x_j$ 都伴随其平方项 $x_j^2$，并再次计算训练 RSS。\n- 将模型参数计数为包括截距在内的估计系数总数。对于一个有 $p$ 个预测变量的线性模型，参数数量为 $1 + p$。对于为每个预测变量添加平方项（无交互项）的二次增广模型，参数数量为 $1 + p + p = 1 + 2p$。\n- 根据线性回归和误差方差的基本定义和性质，推导出每个模型的 Mallows' $C_p$，然后计算 $C_p$ 的变化量，\n$$\\Delta C_p = C_p^{\\text{quad}} - C_p^{\\text{lin}}.$$\n- 根据计算出的 $\\Delta C_p$ 和给定的 $\\hat{\\sigma}^2$，使用一个从第一性原理推导出的基于原则的决策规则，来决定是否应该添加二次项。\n\n你必须根据以下明确定义的测试套件生成合成数据。在每种情况下，生成具有独立标准正态分布条目（即每个 $x_{ij} \\sim \\mathcal{N}(0,1)$）的矩阵 $X$，并从一个可能包含线性和二次贡献以及独立同分布高斯噪声的模型中生成 $y$。使用指定的随机种子以保证可复现性。在以下所有情况中，数据生成过程均包含截距。\n\n测试套件（每种情况提供 $(\\text{seed}, n, p, \\text{intercept}, \\beta, \\gamma, \\hat{\\sigma}^2)$）：\n- 情况 1：seed $= 123$, $n = 50$, $p = 2$, intercept $= 0.5$, 线性系数 $\\beta = [1.0, -2.0]$, 二次系数 $\\gamma = [0.0, 0.0]$, 以及 $\\hat{\\sigma}^2 = 1.0$。数据生成过程为\n$$y = 0.5 + 1.0 \\cdot x_1 - 2.0 \\cdot x_2 + \\varepsilon,$$\n其中 $\\varepsilon \\sim \\mathcal{N}(0, 1.0)$。\n- 情况 2：seed $= 456$, $n = 50$, $p = 2$, intercept $= -0.2$, 线性系数 $\\beta = [1.0, -2.0]$, 二次系数 $\\gamma = [0.8, 0.0]$, 以及 $\\hat{\\sigma}^2 = 1.0$。数据生成过程为\n$$y = -0.2 + 1.0 \\cdot x_1 - 2.0 \\cdot x_2 + 0.8 \\cdot x_1^2 + \\varepsilon,$$\n其中 $\\varepsilon \\sim \\mathcal{N}(0, 1.0)$。\n- 情况 3：seed $= 789$, $n = 200$, $p = 3$, intercept $= 0.0$, 线性系数 $\\beta = [0.5, -1.0, 0.0]$, 二次系数 $\\gamma = [0.2, 0.2, 0.0]$, 以及 $\\hat{\\sigma}^2 = 1.0$。数据生成过程为\n$$y = 0.0 + 0.5 \\cdot x_1 - 1.0 \\cdot x_2 + 0.2 \\cdot x_1^2 + 0.2 \\cdot x_2^2 + \\varepsilon,$$\n其中 $\\varepsilon \\sim \\mathcal{N}(0, 1.0)$。\n- 情况 4：seed $= 42$, $n = 80$, $p = 1$, intercept $= 0.0$, 线性系数 $\\beta = [0.0]$, 二次系数 $\\gamma = [3.0]$, 以及 $\\hat{\\sigma}^2 = 1.0$。数据生成过程为\n$$y = 0.0 + 3.0 \\cdot x_1^2 + \\varepsilon,$$\n其中 $\\varepsilon \\sim \\mathcal{N}(0, 1.0)$。\n\n实现细节：\n- 使用普通最小二乘法 (OLS)，其定义为最小化训练残差平方和 (RSS)，并为每个拟合模型计算 $RSS = \\sum_{i=1}^n (y_i - \\hat{y}_i)^2$。\n- 在二次增广模型中仅使用平方项（无交互项）。\n- 精确使用每个测试用例中给定的 $\\hat{\\sigma}^2$。\n- 当且仅当 $\\Delta C_p  0$ 时，判定二次项是合理的。\n\n最终输出格式：\n- 你的程序应生成单行输出，其中包含四个测试用例的决策，形式为方括号内以逗号分隔的布尔值列表（例如，$[\\text{True},\\text{False},\\text{True},\\text{False}]$），其中 $\\text{True}$ 表示二次项是合理的，而 $\\text{False}$ 表示不是。\n\n你的程序必须完全自包含，且不得读取任何输入。它必须按上述规定生成合成数据，并遵循概述的规则来产生最终输出。唯一允许使用的库是 Numerical Python (NumPy) 和 Python 标准库。", "solution": "当前问题要求对一个简单的线性回归模型和一个包含二次项的增广模型进行定量比较。目标是为一个给定的数据集决定是否应该包含这些高阶项。该决策将基于 Mallows' $C_p$ 统计量，这是一个在拟合优度与模型复杂度之间进行权衡的、成熟的模型选择准则。\n\n首先，我们对数学设定进行形式化。给定一个包含 $n$ 个观测值的数据集。对于每个观测值 $i \\in \\{1, 2, \\dots, n\\}$，我们有一个响应 $y_i$ 和 $p$ 个预测变量 $x_{i1}, x_{i2}, \\dots, x_{ip}$。我们可以使用响应向量 $y \\in \\mathbb{R}^n$ 和预测变量矩阵 $X \\in \\mathbb{R}^{n \\times p}$ 来表示数据。\n\n我们考虑两个用于描述预测变量与响应之间关系的嵌套模型。\n\n1.  线性模型 ($M_{\\text{lin}}$): 该模型假设一个线性关系，并包含一个截距项 $\\beta_0$。模型方程为：\n    $$ y_i = \\beta_0 + \\sum_{j=1}^{p} \\beta_j x_{ij} + \\varepsilon_i $$\n    需要估计的参数是系数 $\\beta_0, \\beta_1, \\dots, \\beta_p$。该模型中的参数总数为 $d_{\\text{lin}} = p + 1$。\n\n2.  二次增广模型 ($M_{\\text{quad}}$): 该模型通过将每个预测变量的平方作为新特征来扩展线性模型。形如 $x_{ij}x_{ik}$（其中 $j \\neq k$）的交互项被明确排除。模型方程为：\n    $$ y_i = \\beta_0 + \\sum_{j=1}^{p} \\beta_j x_{ij} + \\sum_{j=1}^{p} \\gamma_j x_{ij}^2 + \\varepsilon_i $$\n    需要估计的参数是 $\\beta_0$、线性系数 $\\beta_1, \\dots, \\beta_p$ 和二次系数 $\\gamma_1, \\dots, \\gamma_p$。参数总数为 $d_{\\text{quad}} = 1 + p + p = 2p + 1$。\n\n对于这两个模型，项 $\\varepsilon_i$ 代表误差，它被假设为一个独立同分布 (i.i.d.) 的随机变量，其均值为 $E[\\varepsilon_i] = 0$，方差为 $Var(\\varepsilon_i) = \\sigma^2$。\n\n这些模型的参数使用普通最小二乘法 (OLS) 进行估计。OLS 寻找能够最小化残差平方和 (RSS) 的参数值，RSS 是观测响应 $y_i$ 和预测响应 $\\hat{y}_i$ 之间差的平方和。\n$$ \\text{RSS} = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 $$\n在矩阵表示法中，设 $Z$ 为给定模型的设计矩阵（它包含一个用于截距的前导全 1 列）。对于 $M_{\\text{lin}}$，有 $Z_{\\text{lin}} \\in \\mathbb{R}^{n \\times (p+1)}$。对于 $M_{\\text{quad}}$，有 $Z_{\\text{quad}} \\in \\mathbb{R}^{n \\times (2p+1)}$。OLS 系数向量 $\\hat{\\theta}$ 是正规方程组的解：\n$$ (Z^T Z) \\hat{\\theta} = Z^T y $$\n预测值则为 $\\hat{y} = Z \\hat{\\theta}$，RSS 由 $\\|y - Z\\hat{\\theta}\\|_2^2$ 给出。\n\n由于 $M_{\\text{quad}}$ 拥有更多参数并包含了 $M_{\\text{lin}}$ 的所有项，它在训练数据上总能实现至少与 $M_{\\text{lin}}$ 一样好的拟合。这意味着 $\\text{RSS}_{\\text{quad}} \\le \\text{RSS}_{\\text{lin}}$。仅仅一个更小的 RSS 并不足以让我们偏好一个更复杂的模型，因为这可能导致过拟合。\n\nMallows' $C_p$ 通过惩罚模型复杂度来解决这个问题。它是对新数据上缩放后的期望预测误差的估计。对于一个有 $d$ 个参数的模型，$C_p$ 统计量定义为：\n$$ C_p = \\frac{\\text{RSS}}{\\hat{\\sigma}^2} + 2d - n $$\n在这里，$\\hat{\\sigma}^2$ 是真实误差方差 $\\sigma^2$ 的一个估计。一个好的模型其 $C_p$ 值会接近其参数数量 $d$。在比较模型时，我们偏好 $C_p$ 值较低的模型。\n\n该问题要求我们计算从线性模型转换到二次模型时 $C_p$ 的变化：\n$$ \\Delta C_p = C_p^{\\text{quad}} - C_p^{\\text{lin}} $$\n代入每个模型的 $C_p$ 定义：\n$$ \\Delta C_p = \\left( \\frac{\\text{RSS}_{\\text{quad}}}{\\hat{\\sigma}^2} + 2d_{\\text{quad}} - n \\right) - \\left( \\frac{\\text{RSS}_{\\text{lin}}}{\\hat{\\sigma}^2} + 2d_{\\text{lin}} - n \\right) $$\n$$ \\Delta C_p = \\frac{\\text{RSS}_{\\text{quad}} - \\text{RSS}_{\\text{lin}}}{\\hat{\\sigma}^2} + 2(d_{\\text{quad}} - d_{\\text{lin}}) $$\n我们有 $d_{\\text{lin}} = p+1$ 和 $d_{\\text{quad}} = 2p+1$，因此参数数量的变化是 $d_{\\text{quad}} - d_{\\text{lin}} = (2p+1) - (p+1) = p$。\n$C_p$ 变化的最终表达式是：\n$$ \\Delta C_p = \\frac{\\text{RSS}_{\\text{quad}} - \\text{RSS}_{\\text{lin}}}{\\hat{\\sigma}^2} + 2p $$\n决策规则是，如果二次增广模型产生更低的 $C_p$ 值，则选择该模型，这对应于条件 $\\Delta C_p  0$。该条件可以改写为：\n$$ \\frac{\\text{RSS}_{\\text{lin}} - \\text{RSS}_{\\text{quad}}}{\\hat{\\sigma}^2} > 2p $$\n这个不等式提供了一个清晰的原则：残差平方和的减少量，经过噪声方差估计值的缩放后，必须大于所增加参数数量的两倍。这反映了偏差与方差之间的权衡。RSS 的大幅减少表明增加的二次项正在捕捉数据中的真实结构（减少偏差），如果这个减少量足够大以克服复杂度惩罚，则偏好新模型。\n\n每个测试用例的计算流程如下：\n1.  设置随机种子以保证可复现性。\n2.  生成一个 $n \\times p$ 的预测变量矩阵 $X$，其条目从 $\\mathcal{N}(0,1)$ 中抽取。\n3.  使用指定的数据生成过程生成响应向量 $y$，该过程包括截距、线性项、二次项（如果有）和高斯噪声 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2)$，其中所有测试用例的 $\\sigma^2=1.0$。\n4.  通过在 $X$ 前面附加一列全 1 来构造线性模型的设计矩阵 $Z_{\\text{lin}}$。\n5.  对 $y$ 和 $Z_{\\text{lin}}$ 执行 OLS 回归，以获得 $\\text{RSS}_{\\text{lin}}$。\n6.  通过在由 $X$ 和 $X^2$（逐元素平方）连接而成的矩阵前附加一列全 1 来构造二次模型的设计矩阵 $Z_{\\text{quad}}$。\n7.  对 $y$ 和 $Z_{\\text{quad}}$ 执行 OLS 回归，以获得 $\\text{RSS}_{\\text{quad}}$。\n8.  使用给定的 $p$ 和 $\\hat{\\sigma}^2$ 值，计算 $\\Delta C_p = (\\text{RSS}_{\\text{quad}} - \\text{RSS}_{\\text{lin}}) / \\hat{\\sigma}^2 + 2p$。\n9.  通过检查是否 $\\Delta C_p  0$ 来判断二次项是否合理。记录布尔结果。\n对所有四个测试用例重复此过程以生成最终输出。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem of deciding whether to augment a linear model with quadratic terms\n    based on the change in Mallows' Cp statistic for four specified test cases.\n    \"\"\"\n    \n    # Each tuple contains: (seed, n, p, intercept, beta, gamma, sigma_hat_sq)\n    test_cases = [\n        (123, 50, 2, 0.5, np.array([1.0, -2.0]), np.array([0.0, 0.0]), 1.0),\n        (456, 50, 2, -0.2, np.array([1.0, -2.0]), np.array([0.8, 0.0]), 1.0),\n        (789, 200, 3, 0.0, np.array([0.5, -1.0, 0.0]), np.array([0.2, 0.2, 0.0]), 1.0),\n        (42, 80, 1, 0.0, np.array([0.0]), np.array([3.0]), 1.0)\n    ]\n\n    results = []\n\n    for case in test_cases:\n        seed, n, p, intercept, beta, gamma, sigma_hat_sq = case\n\n        # Set the random seed for reproducibility\n        np.random.seed(seed)\n\n        # 1. Generate synthetic data\n        # Generate predictor matrix X with standard normal entries\n        X = np.random.randn(n, p)\n        \n        # Generate response vector y based on the true model\n        # The true error variance is 1.0 for all cases\n        true_sigma = 1.0\n        epsilon = np.random.randn(n) * true_sigma\n        \n        # Calculate the true mean response\n        mu = intercept + X @ beta + (X**2) @ gamma\n        \n        y = mu + epsilon\n\n        # 2. Fit the linear model\n        # Construct the design matrix with an intercept term\n        Z_lin = np.c_[np.ones(n), X]\n        \n        # Perform OLS using np.linalg.lstsq\n        # The function returns coefficients, RSS, rank, and singular values.\n        # The second return value is an array containing the RSS.\n        _, rss_lin_array, _, _ = np.linalg.lstsq(Z_lin, y, rcond=None)\n        rss_lin = rss_lin_array[0]\n\n        # 3. Fit the quadratic-augmented model\n        # Construct the design matrix with intercept, linear, and squared terms\n        X_quad_features = np.c_[X, X**2]\n        Z_quad = np.c_[np.ones(n), X_quad_features]\n        \n        # Perform OLS for the quadratic model\n        _, rss_quad_array, _, _ = np.linalg.lstsq(Z_quad, y, rcond=None)\n        rss_quad = rss_quad_array[0]\n\n        # 4. Compute Delta Cp and make a decision\n        # The change in the number of parameters is p\n        # delta_d = (2*p + 1) - (p + 1) = p\n        \n        # Mallows' Cp formula leads to: Delta_Cp = (RSS_quad - RSS_lin)/sigma_hat^2 + 2*p\n        delta_cp = (rss_quad - rss_lin) / sigma_hat_sq + 2 * p\n        \n        # Decision: quadratic terms are warranted if Delta_Cp  0\n        is_warranted = delta_cp  0\n        results.append(is_warranted)\n\n    # Format and print the final output\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == '__main__':\n    solve()\n```", "id": "3143724"}, {"introduction": "虽然 Mallows' $C_p$ 的公式 $C_p = \\frac{\\mathrm{RSS}}{\\hat{\\sigma}^2} - n + 2d$ 看起来很简单，但其应用需要仔细考量，尤其是在处理常含有相关或冗余预测变量的真实数据时。本练习探讨了当预测变量间存在完全共线性时会发生什么，以及 $C_p$ 的惩罚项如何正确地反映模型的真实复杂度，即有效自由度 $d$ [@problem_id:3143701]。理解这一点对于建立稳健的模型至关重要，因为它揭示了模型复杂度的本质。", "problem": "考虑一个具有同方差高斯误差的线性回归模型，由 $y = X\\beta + \\varepsilon$ 给出，其中 $y \\in \\mathbb{R}^{n}$，$X \\in \\mathbb{R}^{n \\times p}$ 包含一个截距项和 $p-1$ 个预测变量，$\\beta \\in \\mathbb{R}^{p}$ 是一个未知参数向量，以及 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2} I_{n})$。假设您通过普通最小二乘法 (OLS) 拟合该模型，将估计量解释为通过 Moore-Penrose 伪逆在 $X$ 的列空间上的正交投影，因此拟合值为 $\\hat{y} = H y$，其中 $H = X X^{+}$，$X^{+}$ 表示 Moore-Penrose 伪逆。矩阵 $H$ 通常被称为帽子矩阵。Mallows' $C_p$ 是一种模型选择准则，旨在在所述假设下，为观测设计中的样本内预测误差提供一个无偏估计。\n\n给定一个设计，其中有 $n = 25$ 个观测值和 $X$ 中的 $p = 5$ 列：一个截距项和四个预测变量列 $(x_{1}, x_{2}, x_{3}, x_{4})$。预测变量满足一个完全线性冗余关系 $x_{4} = 2 x_{1} - x_{3}$，并且您可以假设截距列与其他预测变量列线性无关。您使用伪逆通过 OLS 拟合这个冗余模型，并得到残差平方和 $\\mathrm{RSS} = 180$。已知噪声方差为 $\\sigma^{2} = 9$。\n\n任务：\n1. 从线性最小二乘投影的基本性质和线性平滑器的无偏风险估计出发，解释为什么即使在完全冗余的情况下，量 $\\mathrm{tr}(H)$ 也等于 $X$ 的秩，而不是原始列数。特别地，论证进入 Mallows $C_{p}$ 的惩罚项取决于 $\\mathrm{tr}(H)$，因此反映了秩亏。\n2. 确定给定 $X$ 的 $\\mathrm{tr}(H)$ 的值。\n3. 使用您的结果，计算此拟合模型的 Mallows $C_{p}$ 值。\n\n请将 $C_{p}$ 的最终数值以单个数字的形式提供。无需四舍五入；请给出精确值。", "solution": "该问题要求解释在多重共线性下，帽子矩阵的迹在 Mallows' $C_p$ 中的作用，然后为一个给定的秩亏线性模型计算 $C_p$ 的值。\n\n### 第一部分：对 $\\mathrm{tr}(H)$ 及其在 Mallows' $C_p$ 中作用的解释\n\n在普通最小二乘法 (OLS) 回归中，拟合值由 $\\hat{y} = Hy$ 给出，其中 $H$ 是帽子矩阵。矩阵 $H$ 是到设计矩阵 $X$ 的列空间（表示为 $\\mathrm{col}(X)$）上的正交投影矩阵。当 $X$ 是满列秩时，$H = X(X^T X)^{-1}X^T$。在一般情况下，包括秩亏的 $X$，帽子矩阵使用 Moore-Penrose 伪逆 $X^{+}$ 定义为 $H = XX^{+}$。在这两种情况下，$H$ 都是一个对称且幂等的 ($H^2 = H$) 投影矩阵。\n\n任何投影矩阵 $P$ 的一个基本性质是其迹等于其秩：$\\mathrm{tr}(P) = \\mathrm{rank}(P)$。由于 $H$ 是到 $\\mathrm{col}(X)$ 上的投影矩阵，其秩是这个子空间的维度。根据定义，一个矩阵的列空间的维度就是该矩阵的秩。因此，我们有以下等式链：\n$$\n\\mathrm{tr}(H) = \\mathrm{rank}(H) = \\mathrm{dim}(\\mathrm{col}(X)) = \\mathrm{rank}(X)\n$$\n这个关系成立，无论 $X$ 是满秩还是秩亏。如果 $X \\in \\mathbb{R}^{n \\times p}$ 且 $\\mathrm{rank}(X) = r \\le p$，那么 $\\mathrm{tr}(H) = r$。量 $d = \\mathrm{tr}(H)$ 通常被称为线性拟合的有效自由度。\n\nMallows' $C_p$ 旨在成为按噪声方差 $\\sigma^2$ 缩放的样本内均方预测误差的无偏估计。真实的样本内均方误差是拟合值 $\\hat{y}$ 与真实均值向量 $\\mu = E[y] = X\\beta$ 之间期望平方距离。我们将其表示为 $J$：\n$$\nJ = \\frac{1}{\\sigma^2} E\\left[ \\| \\hat{y} - \\mu \\|_2^2 \\right]\n$$\n我们可以将 $\\hat{y} - \\mu$ 表示为 $Hy - \\mu = H(X\\beta + \\varepsilon) - X\\beta = H(X\\beta) + H\\varepsilon - X\\beta$。由于 $\\mu = X\\beta$ 在 $X$ 的列空间中，投影 $H$ 使其保持不变，即 $H(X\\beta) = X\\beta$。因此，$\\hat{y} - \\mu = H\\varepsilon$。\n期望变为：\n$$\nE\\left[ \\| H\\varepsilon \\|_2^2 \\right] = E\\left[ \\varepsilon^T H^T H \\varepsilon \\right] = E\\left[ \\varepsilon^T H \\varepsilon \\right]\n$$\n因为 $H$ 是幂等的 ($H^2 = H$) 和对称的 ($H^T=H$)。使用随机向量二次型的迹恒等式 $E[\\varepsilon^T A \\varepsilon] = \\mathrm{tr}(A \\cdot \\mathrm{Cov}(\\varepsilon)) + E[\\varepsilon]^T A E[\\varepsilon]$，并且给定 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I_n)$，我们有 $E[\\varepsilon]=0$ 和 $\\mathrm{Cov}(\\varepsilon) = \\sigma^2 I_n$。\n这得到：\n$$\nE\\left[ \\| H\\varepsilon \\|_2^2 \\right] = \\mathrm{tr}(H \\cdot \\sigma^2 I_n) = \\sigma^2 \\mathrm{tr}(H)\n$$\n所以，真实的、经缩放的样本内预测误差是 $J = \\frac{1}{\\sigma^2} (\\sigma^2 \\mathrm{tr}(H)) = \\mathrm{tr}(H) = d$。\n\nMallows' $C_p$ 统计量的规范定义是：\n$$\nC_p = \\frac{\\mathrm{RSS}}{\\sigma^2} - n + 2d\n$$\n其中 $\\mathrm{RSS} = \\|y - \\hat{y}\\|_2^2$ 是残差平方和，$d = \\mathrm{tr}(H)$。这个统计量是 $J=d$ 的一个无偏估计量。为了验证这一点，我们计算它的期望。RSS可以写成 $\\mathrm{RSS} = \\|(I-H)y\\|^2 = \\|(I-H)(X\\beta+\\varepsilon)\\|^2 = \\|(I-H)\\varepsilon\\|^2$，因为 $(I-H)X\\beta = 0$。期望是：\n$$\nE[\\mathrm{RSS}] = E[\\|(I-H)\\varepsilon\\|^2] = \\sigma^2 \\mathrm{tr}(I-H) = \\sigma^2(n - \\mathrm{tr}(H)) = \\sigma^2(n-d)\n$$\n现在，求 $C_p$ 统计量的期望：\n$$\nE[C_p] = E\\left[\\frac{\\mathrm{RSS}}{\\sigma^2} - n + 2d\\right] = \\frac{E[\\mathrm{RSS}]}{\\sigma^2} - n + 2d = \\frac{\\sigma^2(n-d)}{\\sigma^2} - n + 2d = (n-d) - n + 2d = d\n$$\n由于 $E[C_p] = d = J$，所以 $C_p$ 统计量确实是经缩放的样本内预测误差的无偏估计量。此表达式中的惩罚项 $2d$ 是有效自由度的两倍，即 $2 \\cdot \\mathrm{tr}(H)$。这明确地表明，惩罚项通过依赖于 $\\mathrm{rank}(X)$ 而不是总列数 $p$ 来正确地对秩亏进行校正。\n\n### 第二部分：确定 $\\mathrm{tr}(H)$ 的值\n\n我们需要找到设计矩阵 $X$ 的秩。矩阵 $X$ 有 $n=25$ 行和 $p=5$ 列。这些列对应于一个截距项和四个预测变量 $(x_1, x_2, x_3, x_4)$。\n设 $X$ 的列为 $c_0, c_1, c_2, c_3, c_4$。\n我们被告知预测变量列之间存在一个完全线性相关性：\n$$\nx_4 = 2x_1 - x_3\n$$\n这意味着列向量 $c_4$ 是列 $c_1$ 和 $c_3$ 的线性组合：$c_4 = 2c_1 - c_3$。这个关系意味着这五个列的集合是线性相关的。因此，$X$ 的秩必须小于 $5$。这种相关性使秩至少减少一。\n\n题目暗示这是各列之间唯一的线性相关性。具体来说，张成 $X$ 列空间的列集合 $\\{c_0, c_1, c_2, c_3\\}$ 被假设为线性无关的。\n$X$ 的列空间是 $\\mathrm{col}(X) = \\mathrm{span}\\{c_0, c_1, c_2, c_3, c_4\\}$。由于 $c_4$ 在 $\\{c_1, c_3\\}$ 的张成空间中，我们可以写出 $\\mathrm{col}(X) = \\mathrm{span}\\{c_0, c_1, c_2, c_3\\}$。\n这个空间的维度是基中向量的数量。如果 $\\{c_0, c_1, c_2, c_3\\}$ 是线性无关的，它们就构成了 $\\mathrm{col}(X)$ 的一个基。在这种情况下，列空间的维度是 $4$。\n因此，矩阵 $X$ 的秩是 $4$。\n\n如第一部分所述，$\\mathrm{tr}(H) = \\mathrm{rank}(X)$。\n因此，$\\mathrm{tr}(H)$ 的值是 $4$。\n\n### 第三部分：计算 Mallows' $C_p$ 的值\n\n当噪声方差 $\\sigma^2$ 已知时，Mallows' $C_p$ 的公式是：\n$$\nC_p = \\frac{\\mathrm{RSS}}{\\sigma^2} - n + 2d\n$$\n其中 $d = \\mathrm{tr}(H)$。\n\n根据题目陈述和我们的分析，我们有以下数值：\n-   残差平方和, $\\mathrm{RSS} = 180$。\n-   噪声方差, $\\sigma^2 = 9$。\n-   观测数量, $n = 25$。\n-   有效自由度, $d = \\mathrm{tr}(H) = 4$。\n\n将这些值代入公式：\n$$\nC_p = \\frac{180}{9} - 25 + 2(4)\n$$\n$$\nC_p = 20 - 25 + 8\n$$\n$$\nC_p = -5 + 8\n$$\n$$\nC_p = 3\n$$\n该拟合模型的 Mallows' $C_p$ 值为 3。", "answer": "$$\n\\boxed{3}\n$$", "id": "3143701"}, {"introduction": "这项进阶练习揭示了两种看似不同的统计方法之间一个令人惊讶而优雅的联系。通过在正交设计的特殊情况下分析 $C_p$，我们可以证明最小化 $C_p$ 等价于一个简单的规则：只保留那些估计系数的绝对值超过某个阈值的预测变量[@problem_id:3143696]。这一发现为 $C_p$ 为何有效提供了更深刻的直觉，并将其与现代高维统计中的核心思想——硬阈值法——联系起来。", "problem": "考虑固定设计线性模型 $y = X \\beta + \\varepsilon$，其中 $y \\in \\mathbb{R}^{n}$，$X \\in \\mathbb{R}^{n \\times p}$，且 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2} I_{n})$。为消除截距带来的复杂性并简化几何结构，我们假设以下设定：\n- 响应变量 $y$ 和 $X$ 的每一列 $x_{j}$ 都经过中心化处理，因此模型中不包含截距。\n- $X$ 的列是正交的，并经过缩放以满足 $X^{\\top} X = n I_{p}$，即对于 $j \\neq k$，有 $\\langle x_{j}, x_{k} \\rangle = 0$，且对于所有 $j \\in \\{1, \\dots, p\\}$，有 $\\|x_{j}\\|^{2} = n$。\n\n对于任意基数为 $|S| = k$ 的子集 $S \\subseteq \\{1, \\dots, p\\}$，记 $X_{S}$ 为由 $S$ 索引的列组成的 $X$ 的子矩阵，并令 $\\hat{\\beta}_{S}$ 为将 $y$ 对 $X_{S}$ 进行回归得到的普通最小二乘 (OLS) 估计。令 $\\text{RSS}(S)$ 表示该子集拟合的残差平方和。令 $\\hat{\\sigma}^{2}$ 为从包含全部 $p$ 个变量的模型中获得的 $\\sigma^{2}$ 的一个固定估计值，并假定它对于所有子集都是常数。\n\n模型 $S$ 的 Mallows $C_{p}$ 定义为\n$$\nC_{p}(S) = \\frac{\\text{RSS}(S)}{\\hat{\\sigma}^{2}} - \\left(n - 2k\\right).\n$$\n\n请从基本原理出发，仅使用上述定义以及正交投影的线性代数和 OLS 正规方程，推导在指定的正交 $X$ 条件下，通过最小化所有子集 $S$ 的 $C_p(S)$ 所得到的精确决策规则。特别地，证明最小化 $C_p$ 的子集是通过对单个 OLS 系数 $\\{ \\hat{\\beta}_{j} \\}_{j=1}^{p}$ 进行硬阈值处理得到的，具体方法是将 $|\\hat{\\beta}_{j}|$ 与一个不依赖于 $j$ 的、基于方差的单一阈值进行比较。\n\n该通用阈值 $T$ 的精确闭式表达式（用 $n$ 和 $\\hat{\\sigma}^{2}$ 表示）是什么？使得最小化 $C_{p}$ 的规则当且仅当 $|\\hat{\\beta}_{j}| > T$ 时包含预测变量 $j$。请以单一解析表达式的形式给出最终答案。不需要进行数值取整。", "solution": "本题的目标是在正交设计矩阵 $X$ 的特定条件下，推导最小化 Mallows' $C_p$ 统计量的子集选择决策规则。我们必须证明该规则等价于对普通最小二乘 (OLS) 系数进行硬阈值处理，并找出该阈值的显式表达式。\n\n对于一个包含预测变量子集 $S \\subseteq \\{1, \\dots, p\\}$（大小为 $|S|=k$）的模型，Mallows' $C_p$ 统计量由下式给出：\n$$\nC_{p}(S) = \\frac{\\text{RSS}(S)}{\\hat{\\sigma}^{2}} - (n - 2k)\n$$\n其中 $\\text{RSS}(S)$ 是使用 $S$ 中预测变量的模型的残差平方和，$n$ 是观测数量，$\\hat{\\sigma}^{2}$ 是误差方差 $\\sigma^2$ 的一个固定估计，通常来自全模型。为了最小化 $C_p(S)$，我们必须首先找到一个更方便的 $\\text{RSS}(S)$ 表达式。\n\n题目指出，设计矩阵 $X \\in \\mathbb{R}^{n \\times p}$ 的列是正交的，并经过缩放以满足 $X^{\\top} X = n I_{p}$。这一性质极大地简化了 OLS 计算。\n\n首先，考虑包含所有 $p$ 个预测变量的全模型的 OLS 估计。系数估计 $\\hat{\\beta} \\in \\mathbb{R}^p$ 由正规方程给出：\n$$\n\\hat{\\beta} = (X^{\\top}X)^{-1} X^{\\top}y\n$$\n代入正交性条件 $X^{\\top}X = n I_{p}$，我们得到：\n$$\n\\hat{\\beta} = (n I_{p})^{-1} X^{\\top}y = \\frac{1}{n} X^{\\top}y\n$$\n该向量的第 $j$ 个分量是 $\\hat{\\beta}_j = \\frac{1}{n} x_j^{\\top}y$，其中 $x_j$ 是 $X$ 的第 $j$ 列。\n\n接下来，考虑一个使用由集合 $S$ 索引的预测变量的子集模型，其中 $|S|=k$。该模型的设计矩阵是 $X_S$。该子集模型的 OLS 估计 $\\hat{\\beta}_S$ 由下式给出：\n$$\n\\hat{\\beta}_S = (X_S^{\\top}X_S)^{-1} X_S^{\\top}y\n$$\n由于 $X$ 的列具有正交性，子矩阵 $X_S$ 的列也是正交的。因此，$X_S^{\\top}X_S = n I_{k}$。这导致：\n$$\n\\hat{\\beta}_S = (n I_{k})^{-1} X_S^{\\top}y = \\frac{1}{n} X_S^{\\top}y\n$$\n一个关键的推论是，对于任何预测变量 $j \\in S$，其在子集模型中的系数估计为 $\\frac{1}{n} x_j^{\\top}y$，这与它在全模型中的系数估计 $\\hat{\\beta}_j$ 完全相同。这意味着在正交条件下，子集选择不会改变所选变量的系数。\n\n现在，我们推导 $\\text{RSS}(S)$ 的表达式。子集模型的拟合值为 $\\hat{y}_S = X_S \\hat{\\beta}_S$。残差平方和为 $\\text{RSS}(S) = \\|y - \\hat{y}_S\\|^2$。由于 $\\hat{y}_S$ 是 $y$ 在 $X_S$ 列空间上的正交投影，根据勾股定理，我们有 $\\|y\\|^2 = \\|\\hat{y}_S\\|^2 + \\|y - \\hat{y}_S\\|^2$。因此，$\\text{RSS}(S) = \\|y\\|^2 - \\|\\hat{y}_S\\|^2$。\n我们来计算 $\\|\\hat{y}_S\\|^2$：\n$$\n\\|\\hat{y}_S\\|^2 = \\hat{y}_S^{\\top}\\hat{y}_S = (X_S \\hat{\\beta}_S)^{\\top}(X_S \\hat{\\beta}_S) = \\hat{\\beta}_S^{\\top} X_S^{\\top}X_S \\hat{\\beta}_S\n$$\n代入 $X_S^{\\top}X_S = n I_{k}$：\n$$\n\\|\\hat{y}_S\\|^2 = \\hat{\\beta}_S^{\\top} (n I_{k}) \\hat{\\beta}_S = n \\|\\hat{\\beta}_S\\|^2 = n \\sum_{j \\in S} (\\hat{\\beta}_j)^2\n$$\n注意，我们使用 $\\hat{\\beta}_j$ 表示来自全模型的系数，我们已经证明对于 $j \\in S$ 而言，该系数是相同的。\n所以，子集模型的残差平方和为：\n$$\n\\text{RSS}(S) = \\|y\\|^2 - n \\sum_{j \\in S} \\hat{\\beta}_j^2\n$$\n现在我们将其代入 $C_p(S)$ 的表达式中：\n$$\nC_{p}(S) = \\frac{\\|y\\|^2 - n \\sum_{j \\in S} \\hat{\\beta}_j^2}{\\hat{\\sigma}^{2}} - (n - 2|S|)\n$$\n为了最小化 $C_p(S)$，我们需要选择集合 $S$。我们可以通过为每个预测变量 $j \\in \\{1, \\dots, p\\}$ 做出独立的决策来重新表述这个最小化问题。让我们为每个预测变量引入一个指示变量 $\\delta_j \\in \\{0, 1\\}$，如果预测变量 $j$ 被包含在模型中（即 $j \\in S$），则 $\\delta_j=1$，否则 $\\delta_j=0$。\n使用这个记号，我们有 $|S| = k = \\sum_{j=1}^{p} \\delta_j$ 和 $\\sum_{j \\in S} \\hat{\\beta}_j^2 = \\sum_{j=1}^{p} \\delta_j \\hat{\\beta}_j^2$。\n$C_p$ 表达式变为：\n$$\nC_{p}(S) = \\frac{\\|y\\|^2 - n \\sum_{j=1}^{p} \\delta_j \\hat{\\beta}_j^2}{\\hat{\\sigma}^{2}} - n + 2 \\sum_{j=1}^{p} \\delta_j\n$$\n我们重新排列各项，以分离出依赖于 $\\delta_j$ 选择的部分：\n$$\nC_{p}(S) = \\left( \\frac{\\|y\\|^2}{\\hat{\\sigma}^2} - n \\right) + \\sum_{j=1}^{p} \\left( 2 \\delta_j - \\frac{n \\hat{\\beta}_j^2}{\\hat{\\sigma}^2} \\delta_j \\right)\n$$\n$$\nC_{p}(S) = \\left( \\frac{\\|y\\|^2}{\\hat{\\sigma}^2} - n \\right) + \\sum_{j=1}^{p} \\delta_j \\left( 2 - \\frac{n \\hat{\\beta}_j^2}{\\hat{\\sigma}^2} \\right)\n$$\n项 $\\left( \\frac{\\|y\\|^2}{\\hat{\\sigma}^2} - n \\right)$ 相对于子集 $S$ 的选择是常数。因此，要最小化 $C_p(S)$，我们必须最小化求和项。由于对每个 $\\delta_j$ 的决策与其他决策是独立的，我们可以通过单独最小化每一项来最小化总和。\n\n对于每个预测变量 $j$，我们对 $\\delta_j$ 有两种选择：\n1.  排除预测变量 $j$：设置 $\\delta_j = 0$。此预测变量对总和的贡献为 $0 \\times \\left( 2 - \\frac{n \\hat{\\beta}_j^2}{\\hat{\\sigma}^2} \\right) = 0$。\n2.  包含预测变量 $j$：设置 $\\delta_j = 1$。此预测变量对总和的贡献为 $1 \\times \\left( 2 - \\frac{n \\hat{\\beta}_j^2}{\\hat{\\sigma}^2} \\right) = 2 - \\frac{n \\hat{\\beta}_j^2}{\\hat{\\sigma}^2}$。\n\n当且仅当包含预测变量 $j$（即设置 $\\delta_j=1$）能够减少总和时，我们才选择包含它。这发生在包含它的贡献小于排除它的贡献时：\n$$\n2 - \\frac{n \\hat{\\beta}_j^2}{\\hat{\\sigma}^2}  0\n$$\n重新整理这个不等式，我们得到包含预测变量 $j$ 的条件：\n$$\n2  \\frac{n \\hat{\\beta}_j^2}{\\hat{\\sigma}^2}\n$$\n$$\n\\hat{\\beta}_j^2 > \\frac{2 \\hat{\\sigma}^2}{n}\n$$\n对两边取平方根（两边都非负），我们得到最终的决策规则：\n$$\n|\\hat{\\beta}_j| > \\sqrt{\\frac{2 \\hat{\\sigma}^2}{n}}\n$$\n这是一个硬阈值规则。最小化 $C_p$ 的子集 $S$ 由所有满足其全模型 OLS 系数估计 $\\hat{\\beta}_j$ 的绝对值大于一个通用阈值 $T = \\sqrt{\\frac{2 \\hat{\\sigma}^2}{n}}$ 的预测变量 $j$ 组成。这个阈值仅取决于样本大小 $n$ 和方差估计 $\\hat{\\sigma}^2$，而不取决于具体的预测变量索引 $j$。\n\n因此，该阈值的精确闭式表达式为 $T = \\sqrt{\\frac{2 \\hat{\\sigma}^2}{n}}$。", "answer": "$$\\boxed{\\sqrt{\\frac{2 \\hat{\\sigma}^{2}}{n}}}$$", "id": "3143696"}]}