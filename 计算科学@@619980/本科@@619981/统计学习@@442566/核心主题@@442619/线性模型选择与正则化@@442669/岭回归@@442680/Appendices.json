{"hands_on_practices": [{"introduction": "理论学习之后，最好的巩固方式就是动手实践。我们从一个最简单的情形入手：一个不含截距项的单预测变量线性模型。通过这个练习，你将从岭回归最根本的目标函数出发，亲手推导出系数的估计量，并将其应用于一个微型数据集。这个基础练习旨在帮助你牢固掌握惩罚项如何影响最小二乘估计的核心思想，为理解更复杂的多元回归情景打下坚实的基础 [@problem_id:1951876]。", "problem": "在一个机器学习的背景下，我们的任务是为一个包含 $n$ 个数据点 $(x_i, y_i)$ 的集合拟合一个不带截距的简单线性模型 $y = \\beta x$。为防止在小数据集上发生过拟合，我们采用岭回归。系数 $\\beta$ 的岭估计值是使惩罚平方和误差（也称为目标函数 $L(\\beta)$）最小化的值：\n$$L(\\beta) = \\sum_{i=1}^{n} (y_i - \\beta x_i)^2 + \\lambda \\beta^2$$\n其中 $\\lambda > 0$ 是控制收缩量的正则化参数。\n\n你的任务有两部分。首先，通过最小化目标函数 $L(\\beta)$，推导出岭估计 $\\hat{\\beta}_{\\text{ridge}}$ 关于数据点 $(x_i, y_i)$ 和参数 $\\lambda$ 的通用闭式表达式。\n\n其次，将这个推导出的表达式应用于一个包含两个点的特定数据集：$(x_1, y_1) = (1, 3)$ 和 $(x_2, y_2) = (2, 5)$。使用正则化参数 $\\lambda = 1$ 计算岭估计 $\\hat{\\beta}_{\\text{ridge}}$ 的数值。\n\n以精确分数形式给出最终的数值。", "solution": "我们最小化无截距线性模型 $y=\\beta x$ 的惩罚平方和误差，其目标函数为\n$$\nL(\\beta)=\\sum_{i=1}^{n}(y_{i}-\\beta x_{i})^{2}+\\lambda \\beta^{2}, \\quad \\lambda>0.\n$$\n展开平方项并合并 $\\beta$ 的同次幂：\n$$\nL(\\beta)=\\sum_{i=1}^{n}\\left(y_{i}^{2}-2\\beta x_{i}y_{i}+\\beta^{2}x_{i}^{2}\\right)+\\lambda \\beta^{2}\n= \\sum_{i=1}^{n}y_{i}^{2}-2\\beta \\sum_{i=1}^{n}x_{i}y_{i}+\\beta^{2}\\sum_{i=1}^{n}x_{i}^{2}+\\lambda \\beta^{2}.\n$$\n对 $\\beta$ 求导并将导数设为零（一阶最优性条件）：\n$$\n\\frac{dL}{d\\beta}=-2\\sum_{i=1}^{n}x_{i}y_{i}+2\\beta \\sum_{i=1}^{n}x_{i}^{2}+2\\lambda \\beta=0.\n$$\n求解 $\\beta$：\n$$\n2\\beta\\left(\\sum_{i=1}^{n}x_{i}^{2}+\\lambda\\right)=2\\sum_{i=1}^{n}x_{i}y_{i}\n\\quad \\Longrightarrow \\quad\n\\hat{\\beta}_{\\text{ridge}}=\\frac{\\sum_{i=1}^{n}x_{i}y_{i}}{\\sum_{i=1}^{n}x_{i}^{2}+\\lambda}.\n$$\n二阶导数为\n$$\n\\frac{d^{2}L}{d\\beta^{2}}=2\\sum_{i=1}^{n}x_{i}^{2}+2\\lambda>0,\n$$\n因此该解是唯一的最小化子。\n\n将此应用于 $(x_{1},y_{1})=(1,3)$，$(x_{2},y_{2})=(2,5)$ 且 $\\lambda=1$ 的情况：\n$$\n\\sum_{i=1}^{2}x_{i}y_{i}=1\\cdot 3+2\\cdot 5=13,\\qquad\n\\sum_{i=1}^{2}x_{i}^{2}=1^{2}+2^{2}=5.\n$$\n因此，\n$$\n\\hat{\\beta}_{\\text{ridge}}=\\frac{13}{5+1}=\\frac{13}{6}.\n$$", "answer": "$$\\boxed{\\frac{13}{6}}$$", "id": "1951876"}, {"introduction": "在掌握了单一变量的基本原理后，我们自然要过渡到更普遍、更强大的多预测变量场景。在实际应用中，处理多个特征的回归问题通常借助线性代数工具来完成。这个练习将引导你使用岭回归估计量的矩阵形式进行计算 [@problem_id:1951893]。熟练掌握矩阵运算不仅是应用岭回归到真实世界数据集的关键技能，也是理解许多其他机器学习算法的基础。", "problem": "在机器学习领域，岭回归是一种常用于正则化线性回归模型的技术。这对于防止过拟合和处理预测变量之间的多重共线性特别有用。岭回归系数向量的估计量 $\\hat{\\beta}_{\\lambda}$ 由以下公式给出：\n$$ \\hat{\\beta}_{\\lambda} = (X^\\top X + \\lambda I)^{-1} X^\\top y $$\n其中，$X$ 是设计矩阵，$y$ 是观测结果向量，$I$ 是适当维度的单位矩阵，而 $\\lambda$ 是一个非负的正则化参数。\n\n假设对于一个具有两个预测变量的特定数据集，以下量已被预先计算：\n$$ X^\\top X = \\begin{pmatrix} 10  5 \\\\ 5  10 \\end{pmatrix} \\quad \\text{and} \\quad X^\\top y = \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix} $$\n使用正则化参数 $\\lambda = 5$，确定岭回归系数向量 $\\hat{\\beta}_5$。", "solution": "岭回归估计量定义为\n$$\n\\hat{\\beta}_{\\lambda} = (X^{\\top}X + \\lambda I)^{-1} X^{\\top} y.\n$$\n根据给定的数据，\n$$\nX^{\\top}X = \\begin{pmatrix} 10  5 \\\\ 5  10 \\end{pmatrix}, \\quad X^{\\top} y = \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix}, \\quad \\lambda = 5.\n$$\n计算正则化矩阵：\n$$\nX^{\\top}X + \\lambda I = \\begin{pmatrix} 10  5 \\\\ 5  10 \\end{pmatrix} + 5 \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix} = \\begin{pmatrix} 15  5 \\\\ 5  15 \\end{pmatrix}.\n$$\n对于一个 $2 \\times 2$ 矩阵 $\\begin{pmatrix} a  b \\\\ c  d \\end{pmatrix}$，其逆矩阵由以下公式给出\n$$\n\\left(\\begin{pmatrix} a  b \\\\ c  d \\end{pmatrix}\\right)^{-1} = \\frac{1}{ad - bc} \\begin{pmatrix} d  -b \\\\ -c  a \\end{pmatrix}.\n$$\n应用此公式，\n$$\n\\det(X^{\\top}X + \\lambda I) = 15 \\cdot 15 - 5 \\cdot 5 = 225 - 25 = 200,\n$$\n所以\n$$\n(X^{\\top}X + \\lambda I)^{-1} = \\frac{1}{200} \\begin{pmatrix} 15  -5 \\\\ -5  15 \\end{pmatrix}.\n$$\n然后\n$$\n\\hat{\\beta}_{5} = \\frac{1}{200} \\begin{pmatrix} 15  -5 \\\\ -5  15 \\end{pmatrix} \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix} = \\frac{1}{200} \\begin{pmatrix} 15 \\cdot 3 - 5 \\cdot 1 \\\\ -5 \\cdot 3 + 15 \\cdot 1 \\end{pmatrix} = \\frac{1}{200} \\begin{pmatrix} 40 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{5} \\\\ 0 \\end{pmatrix}.\n$$", "answer": "$$\\boxed{\\begin{pmatrix}\\frac{1}{5}\\\\0\\end{pmatrix}}$$", "id": "1951893"}, {"introduction": "计算岭回归系数是模型构建的一步，但如何确保模型具有最佳的预测性能呢？这引出了岭回归实践中至关重要的一环：选择正则化参数 $\\lambda$。$\\lambda$ 的取值直接决定了模型的复杂度和最终效果。这个练习通过一个排序任务，让你梳理并掌握 K 折交叉验证的完整流程 [@problem_id:1951879]。这是数据科学家用来寻找最优 $\\lambda$ 值、构建稳健预测模型的标准技术，是连接理论与实践的重要桥梁。", "problem": "一位数据科学家负责使用岭回归构建一个预测模型。岭回归是一种线性回归，它包含一个惩罚项来收缩系数估计，这对于减轻多重共线性和防止过拟合特别有用。这个惩罚的强度由一个非负的调整参数 $\\lambda$ 控制。建模过程中的一个关键步骤是从一组候选值中选择 $\\lambda$ 的最优值。一个常用的方法是K折交叉验证。\n\n该数据科学家确定了以下关键操作，以执行K折交叉验证来找到最优的 $\\lambda$ 并构建最终模型。预测误差使用均方误差 (MSE) 来衡量。\n\n(i) 从候选集中选择能在各折中产生最小平均MSE的 $\\lambda$ 值。\n(ii) 对于每个候选的 $\\lambda$ 值，通过遍历K个折来计算平均MSE，每次在K-1个折上训练模型，并在留出的那个折上进行验证。\n(iii) 将整个数据集随机划分为K个大小近似相等的子集，即“折”。\n(iv) 使用上一步选择的最优 $\\lambda$ 值，在*整个*数据集上训练最终的岭回归模型。\n\n这些操作的正确逻辑顺序是什么？\n\nA. (iii) -> (i) -> (ii) -> (iv)\n\nB. (ii) -> (iii) -> (i) -> (iv)\n\nC. (iii) -> (ii) -> (i) -> (iv)\n\nD. (iii) -> (ii) -> (iv) -> (i)\n\nE. (ii) -> (i) -> (iv) -> (iii)", "solution": "岭回归通过最小化惩罚最小二乘目标函数来拟合系数 $\\beta$\n$$\n\\frac{1}{n}\\|y - X\\beta\\|_{2}^{2} + \\lambda \\|\\beta\\|_{2}^{2},\n$$\n其中 $\\lambda \\geq 0$ 是一个通过交叉验证选择的调整参数。为了选择 $\\lambda$，定义一个候选集 $\\Lambda$ 并按如下方式执行K折交叉验证。\n\n首先，将数据集索引 $\\{1,\\dots,n\\}$ 随机划分为 $K$ 个大小近似相等的不相交的折 $I_{1},\\dots,I_{K}$，这对应于操作 (iii)。对于每个 $\\lambda \\in \\Lambda$，遍历折 $k=1,\\dots,K$：使用 $\\lambda$ 在由 $I_{-k} = \\{1,\\dots,n\\} \\setminus I_{k}$ 索引的训练集上拟合岭模型以获得 $\\hat{\\beta}^{(-k,\\lambda)}$，为 $i \\in I_{k}$ 计算验证预测值 $\\hat{y}_{i}^{(-k,\\lambda)}$，并计算该折的MSE\n$$\n\\mathrm{MSE}_{k}(\\lambda) = \\frac{1}{|I_{k}|} \\sum_{i \\in I_{k}} \\left(y_{i} - \\hat{y}_{i}^{(-k,\\lambda)}\\right)^{2}.\n$$\n然后计算 $\\lambda$ 的平均交叉验证误差，\n$$\n\\overline{\\mathrm{MSE}}(\\lambda) = \\frac{1}{K} \\sum_{k=1}^{K} \\mathrm{MSE}_{k}(\\lambda),\n$$\n这对应于操作 (ii)。通过以下方式选择最优调整参数\n$$\n\\lambda^{*} = \\arg\\min_{\\lambda \\in \\Lambda} \\overline{\\mathrm{MSE}}(\\lambda),\n$$\n这对应于操作 (i)。最后，使用 $\\lambda^{*}$ 在整个数据集上重新拟合岭回归模型，这对应于操作 (iv)。\n\n因此，正确的逻辑顺序是 (iii) → (ii) → (i) → (iv)，即选项 C。", "answer": "$$\\boxed{C}$$", "id": "1951879"}]}