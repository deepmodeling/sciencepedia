{"hands_on_practices": [{"introduction": "在高维数据分析中，一个首要的挑战是如何从成千上万的潜在预测变量中筛选出真正重要的特征。一个直观的方法是“确保独立筛选”（Sure Independence Screening, SIS），它通过计算每个特征与响应变量之间的边际相关性，并保留相关性最高的特征。这个动手实践将引导你通过代码来探索 SIS 的有效性，并揭示在预测变量相互关联的特定情况下，这种简单方法为何会失效。通过构建并分析这些场景，你将深入理解特征筛选的基本原理及其在实践中的局限性，为学习更复杂的稀疏建模方法（如 LASSO）奠定基础。[@problem_id:3174653]", "problem": "您将通过确定独立性筛选（Sure Independence Screening, SIS）来分析特征筛选，并评估在预测变量相关的情况下，按边际相关性对特征进行排序何时会失败。在以下高维稀疏线性模型中进行分析：响应变量 $y \\in \\mathbb{R}$ 和一个 $p$ 维特征向量 $x \\in \\mathbb{R}^{p}$ 服从模型 $y = x^{\\top}\\beta + \\varepsilon$，其中 $\\beta \\in \\mathbb{R}^{p}$ 是稀疏的（只有一小部分条目非零），$x$ 的均值为零，协方差矩阵为 $\\Sigma \\in \\mathbb{R}^{p \\times p}$，而 $\\varepsilon$ 是均值为零且独立于 $x$ 的噪声。确定独立性筛选（SIS）通过特征与 $y$ 的边际相关性大小对特征进行排序，并保留排名前 $d$ 的特征。\n\n从基本原理出发：使用协方差的定义以及 $\\varepsilon$ 和 $x$ 的独立性，用 $\\Sigma$ 和 $\\beta$ 来表示第 $j$ 个特征与响应变量之间的总体边际协方差。假设特征是标准化的，因此对所有 $j$ 都有 $\\operatorname{Var}(x_j) = 1$，并注意按相关性的绝对值排序等价于按协方差的绝对值排序（两者相差一个正常数因子）。对于相关参数为 $\\rho \\in [0,1)$ 的一阶自回归（AR(1)）高斯设计，其协方差矩阵是托普利茨（Toeplitz）矩阵，其元素为 $\\Sigma_{jk} = \\rho^{|j-k|}$。\n\n您的任务是实现一个程序，对于每个指定的测试用例，仅使用 $\\Sigma$ 和 $\\beta$ 计算所有 $p$ 个特征的总体边际相关性得分（不要模拟有限样本），按绝对值得分选出前 $d$ 个特征（若得分相同，则优先选择较小的特征索引），并返回一个布尔值，指示所选集合是否包含真实支撑集（即，在该案例中 SIS 是否具有确定筛选性质）。\n\n请使用以下测试套件。在每个案例中，都有 $p = 1000$ 个特征，索引从 $0$ 到 $999$。对于参数为 $\\rho$ 的 AR(1) 设计，特征 $j$ 的总体得分为\n$$\nr_j \\propto \\sum_{k \\in S} \\beta_k \\, \\rho^{|j-k|},\n$$\n其中 $S$ 是非零系数的索引集合。比例常数对所有 $j$ 都相同，因此与排序无关。\n\n测试用例：\n\n- 案例 A（独立，成功）：$p = 1000$，$\\rho = 0$，支撑集 $S = \\{0, 200, 400, 600, 800\\}$，非零系数 $\\beta_k = 3$（对于 $k \\in S$），保留水平 $d = 5$。\n- 案例 B（中等相关，成功）：$p = 1000$，$\\rho = 0.3$，支撑集 $S = \\{0, 200, 400, 600, 800\\}$，非零系数 $\\beta_k = 3$（对于 $k \\in S$），保留水平 $d = 5$。\n- 案例 C（最近邻抑制，失败）：$p = 1000$，$\\rho = 0.9$，支撑集 $S = \\{10, 11\\}$，非零系数为 $\\beta_{10} = 1$ 和 $\\beta_{11} = -1/\\rho$，保留水平 $d = 5$。\n- 案例 D（次近邻抑制，失败）：$p = 1000$，$\\rho = 0.95$，支撑集 $S = \\{100, 102\\}$，非零系数为 $\\beta_{100} = 1$ 和 $\\beta_{102} = -1/\\rho^{2}$，保留水平 $d = 5$。\n\n程序要求：\n\n- 对于每个案例，计算向量 $r \\in \\mathbb{R}^{p}$，其条目为 $r_j = \\sum_{k \\in S} \\beta_k \\, \\rho^{|j-k|}$，并按 $|r_j|$ 对特征进行排序。\n- 按 $|r_j|$ 降序选择前 $d$ 个索引，若得分相同则优先选择较小的索引。\n- 每个案例输出一个布尔值，表示 $S \\subseteq \\widehat{S}_d$ 是否成立，其中 $\\widehat{S}_d$ 是所选的 $d$ 个索引的集合。\n\n最终输出格式：\n\n- 您的程序应生成单行输出，其中包含用方括号括起来的逗号分隔列表形式的结果；例如，`[True,False,True,False]`。\n\n不涉及物理单位或角度。所有结果均为布尔值。程序必须完全自包含，无需用户输入或外部数据。", "solution": "该问题要求通过计算在 AR(1) 协方差结构下几个测试用例的总体水平边际相关性得分，来分析确定独立性筛选（SIS）。我们必须确定在每种情况下，真实的稀疏支撑集是否被恢复。\n\n首先，我们从基本原理推导第 $j$ 个特征 $x_j$ 与响应变量 $y$ 之间的总体边际协方差。模型由 $y = x^{\\top}\\beta + \\varepsilon$ 给出。特征向量 $x$ 的均值为零，$E[x] = 0$，其协方差矩阵为 $\\Sigma = E[xx^{\\top}]$。噪声项 $\\varepsilon$ 的均值为零，$E[\\varepsilon]=0$，且独立于 $x$。\n\n$x_j$ 和 $y$ 之间的协方差定义为：\n$$\n\\operatorname{Cov}(x_j, y) = E[x_j y] - E[x_j]E[y]\n$$\n由于 $E[x_j] = 0$（因为它是均值为零的向量 $x$ 的一个分量），第二项消失。我们通过代入 $y$ 的模型来计算第一项：\n$$\n\\operatorname{Cov}(x_j, y) = E[x_j y] = E[x_j (x^{\\top}\\beta + \\varepsilon)] = E[x_j x^{\\top}\\beta] + E[x_j \\varepsilon]\n$$\n由于 $x$ 和 $\\varepsilon$ 的独立性，我们有 $E[x_j \\varepsilon] = E[x_j]E[\\varepsilon] = 0 \\cdot 0 = 0$。剩余项可以展开为：\n$$\nE[x_j x^{\\top}\\beta] = E\\left[x_j \\sum_{k=1}^{p} x_k \\beta_k\\right]\n$$\n根据期望的线性性质，我们可以将期望移到求和符号内部：\n$$\n\\sum_{k=1}^{p} E[x_j x_k] \\beta_k\n$$\n$E[x_j x_k]$ 项是协方差矩阵 $\\Sigma$ 的第 $(j,k)$ 个元素的定义，记为 $\\Sigma_{jk}$。因此，边际协方差为：\n$$\n\\operatorname{Cov}(x_j, y) = \\sum_{k=1}^{p} \\Sigma_{jk} \\beta_k\n$$\n该表达式对应于向量积 $\\Sigma\\beta$ 的第 $j$ 个元素。\n\n接下来，我们建立按协方差排序和按相关性排序的等价性。相关性定义为：\n$$\n\\operatorname{Corr}(x_j, y) = \\frac{\\operatorname{Cov}(x_j, y)}{\\sqrt{\\operatorname{Var}(x_j)\\operatorname{Var}(y)}}\n$$\n问题陈述特征是标准化的，因此对所有 $j$ 都有 $\\operatorname{Var}(x_j) = 1$。响应变量的方差 $\\operatorname{Var}(y)$ 对于所有特征 $j$ 都是常数。具体来说，$\\operatorname{Var}(y) = \\operatorname{Var}(x^{\\top}\\beta + \\varepsilon) = \\operatorname{Var}(x^{\\top}\\beta) + \\operatorname{Var}(\\varepsilon) = \\beta^{\\top}\\Sigma\\beta + \\sigma^2_{\\varepsilon}$，它是一个正常数。因此，\n$$\n\\operatorname{Corr}(x_j, y) = \\frac{\\operatorname{Cov}(x_j, y)}{\\sqrt{\\operatorname{Var}(y)}}\n$$\n由于 $\\sqrt{\\operatorname{Var}(y)}$ 是一个不依赖于 $j$ 的正常数，因此按特征的相关性大小 $|\\operatorname{Corr}(x_j, y)|$ 排序，等价于按其协方差大小 $|\\operatorname{Cov}(x_j, y)|$ 排序。\n\n我们现在将其特化到 AR(1) 协方差结构，其中 $\\Sigma_{jk} = \\rho^{|j-k|}$（对于 $\\rho \\in [0, 1)$）。边际协方差变为：\n$$\n\\operatorname{Cov}(x_j, y) = \\sum_{k=1}^{p} \\rho^{|j-k|} \\beta_k\n$$\n由于系数向量 $\\beta$ 是稀疏的，其支撑集为 $S = \\{k \\mid \\beta_k \\neq 0\\}$，求和可以限制在 $k \\in S$ 的范围内：\n$$\n\\operatorname{Cov}(x_j, y) = \\sum_{k \\in S} \\rho^{|j-k|} \\beta_k\n$$\n这正是问题陈述中给出的得分 $r_j$。我们将使用此公式计算所有 $p$ 个特征的筛选得分。\n\n如果真实支撑集 $S$ 是筛选过程所选索引集 $\\widehat{S}_d$ 的子集，则确定独立性筛选性质在该案例中成立。也就是说，我们必须检查 $S \\subseteq \\widehat{S}_d$ 是否成立。\n\n每个测试用例的算法如下：\n1.  定义参数 $p$、$\\rho$、$d$、支撑集 $S$ 以及非零系数 $\\beta_k$。\n2.  构建得分向量 $r \\in \\mathbb{R}^p$，其条目为 $r_j = \\sum_{k \\in S} \\beta_k \\rho^{|j-k|}$。\n3.  计算绝对值得分 $|r_j|$。\n4.  按绝对值得分降序对特征索引 $j \\in \\{0, \\dots, p-1\\}$ 进行排序。若得分相同，则优先选择较小的索引。\n5.  从此排序列表中选择前 $d$ 个索引，形成集合 $\\widehat{S}_d$。\n6.  验证 $S \\subseteq \\widehat{S}_d$ 是否成立。结果是一个布尔值。\n\n让我们简要分析这些测试用例：\n-   **案例 A 和 B**：真实支撑集 $S$ 中的特征分隔良好。对于 $\\rho=0$（案例 A），只有 $S$ 中的特征得分非零，从而保证了恢复。对于 $\\rho=0.3$（案例 B），相关性中等，S 中特征的得分预计将占主导地位，从而成功恢复。\n-   **案例 C**：在高相关性（$\\rho=0.9$）且 $S$ 中相邻特征具有符号相反的系数（$\\beta_{10}=1$, $\\beta_{11}=-1/\\rho$）的情况下，预计会出现信号抵消效应。特征 $j=10$ 的得分是 $r_{10} = \\rho^{|10-10|}\\beta_{10} + \\rho^{|10-11|}\\beta_{11} = 1 \\cdot \\beta_{10} + \\rho \\cdot \\beta_{11} = 1 + \\rho(-1/\\rho) = 1-1=0$。由于其他特征将具有非零得分，特征 10 不会进入前 $d=5$ 的行列，导致 SIS 失败。\n-   **案例 D**：此案例产生了类似的抵消效应，但作用于次近邻。对于特征 $j=100$，其得分为 $r_{100} = \\rho^{|100-100|}\\beta_{100} + \\rho^{|100-102|}\\beta_{102} = 1 \\cdot \\beta_{100} + \\rho^2 \\cdot \\beta_{102} = 1 + \\rho^2(-1/\\rho^2) = 1-1=0$。同样，SIS 将无法选出特征 100。\n\n实现将通过数值计算执行这些操作，以确认结果。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the Sure Independence Screening (SIS) problem for the given test cases.\n    \"\"\"\n    \n    p = 1000\n\n    test_cases = [\n        {\n            \"name\": \"Case A\",\n            \"p\": p,\n            \"rho\": 0.0,\n            \"S\": {0, 200, 400, 600, 800},\n            \"betas\": {0: 3, 200: 3, 400: 3, 600: 3, 800: 3},\n            \"d\": 5\n        },\n        {\n            \"name\": \"Case B\",\n            \"p\": p,\n            \"rho\": 0.3,\n            \"S\": {0, 200, 400, 600, 800},\n            \"betas\": {0: 3, 200: 3, 400: 3, 600: 3, 800: 3},\n            \"d\": 5\n        },\n        {\n            \"name\": \"Case C\",\n            \"p\": p,\n            \"rho\": 0.9,\n            \"S\": {10, 11},\n            \"betas\": {10: 1, 11: -1/0.9},\n            \"d\": 5\n        },\n        {\n            \"name\": \"Case D\",\n            \"p\": p,\n            \"rho\": 0.95,\n            \"S\": {100, 102},\n            \"betas\": {100: 1, 102: -1/(0.95**2)},\n            \"d\": 5\n        }\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        p_val = case[\"p\"]\n        rho = case[\"rho\"]\n        S_set = case[\"S\"]\n        betas_dict = case[\"betas\"]\n        d = case[\"d\"]\n\n        # Ensure S_indices are sorted for consistent beta ordering\n        S_indices = np.array(sorted(list(S_set)))\n        betas_S = np.array([betas_dict[k] for k in S_indices])\n        \n        j_indices = np.arange(p_val)\n        scores = np.zeros(p_val, dtype=np.float64)\n\n        if rho == 0.0:\n            # For rho=0, Sigma is identity, so scores are just beta_j\n            scores[S_indices] = betas_S\n        else:\n            # Use broadcasting for efficiency:\n            # j_indices_col is (p, 1)\n            # S_indices is (len(S),)\n            # abs_diffs will be (p, len(S))\n            j_indices_col = j_indices[:, np.newaxis]\n            abs_diffs = np.abs(j_indices_col - S_indices)\n            \n            # rho_powers will be (p, len(S))\n            rho_powers = rho ** abs_diffs\n            \n            # Matrix-vector product to get scores (p, len(S)) x (len(S),) -> (p,)\n            scores = rho_powers @ betas_S\n\n        abs_scores = np.abs(scores)\n        \n        # Sort indices: primary key -abs_scores (descending), secondary key j_indices (ascending for ties)\n        sorted_indices = np.lexsort((j_indices, -abs_scores))\n        \n        # Select top d features\n        selected_indices_set = set(sorted_indices[:d])\n        \n        # Check if true support is a subset of the selected set\n        is_recovered = S_set.issubset(selected_indices_set)\n        results.append(is_recovered)\n        \n    # Format and print the final output\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3174653"}, {"introduction": "在了解了简单筛选方法的局限性之后，我们转向一个更强大和广泛使用的工具：LASSO（Least Absolute Shrinkage and Selection Operator）。LASSO 不仅能进行系数收缩以防止过拟合，还能同时将某些系数精确地设置为零，从而实现变量选择。这个练习要求你从头开始实现坐标下降算法来求解 LASSO 问题，并探索所谓的“LASSO 路径”——即随着惩罚参数 $\\lambda$ 的变化，模型系数和非零特征集是如何演变的。通过这个实践，你将观察到一个关键且非直观的现象：在特征相关的设计中，一个变量可能在路径的早期被选入模型，但随着 $\\lambda$ 的减小，又被移出模型，这揭示了 LASSO 路径的非单调性。[@problem_id:3174656]", "problem": "您必须编写一个完整、可运行的程序，该程序构建三个确定性的线性回归实例，并分析最小绝对收缩和选择算子 (lasso) 路径相对于惩罚参数的变化。核心目标是测试 lasso 路径上支持集的单调性，构建一个变量进入然后又离开活动集的案例，并通过 Karush–Kuhn–Tucker (KKT) 条件来量化变化。所有计算都必须使用带有标准经验风险缩放的 lasso 目标函数来进行。\n\nlasso 估计量被定义为凸函数的最小化器\n$$\n\\min_{\\beta \\in \\mathbb{R}^p} \\left\\{ \\frac{1}{2n} \\lVert y - X \\beta \\rVert_2^2 + \\lambda \\lVert \\beta \\rVert_1 \\right\\},\n$$\n其中 $X \\in \\mathbb{R}^{n \\times p}$，$y \\in \\mathbb{R}^n$，$\\beta \\in \\mathbb{R}^p$，并且 $\\lambda \\ge 0$ 是正则化强度。此问题中的所有矩阵和向量都是无量纲的；因此，不涉及物理单位。\n\n您必须实现一个坐标下降求解器，对于任何固定的 $\\lambda$，该求解器能找到一个近似的最小化器 $\\hat{\\beta}(\\lambda)$，然后您必须验证 KKT 最优性条件。对于此目标函数，并且在 $X$ 的列被缩放以满足 $\\lVert X_{\\cdot j} \\rVert_2^2 = n$ (对所有 $j$) 的情况下，KKT 条件表明，对于残差 $r = y - X \\hat{\\beta}$：\n- 如果 $\\hat{\\beta}_j \\ne 0$，则 $X_{\\cdot j}^{\\top} r / n = \\lambda \\,\\mathrm{sign}(\\hat{\\beta}_j)$，\n- 如果 $\\hat{\\beta}_j = 0$，则 $\\lvert X_{\\cdot j}^{\\top} r / n \\rvert \\le \\lambda$。\n\n您的程序必须：\n1. 为每个测试用例生成一个 $\\lambda$ 值的网格，作为一个从 $\\lambda_{\\max}$ 到 $\\lambda_{\\min}$ 的递减序列，其中\n$$\n\\lambda_{\\max} \\equiv \\max_{1 \\le j \\le p} \\frac{\\lvert X_{\\cdot j}^{\\top} y \\rvert}{n},\n\\quad\n\\lambda_{\\min} \\equiv 0.01 \\,\\lambda_{\\max},\n$$\n总共 40 个对数间隔的值，包括端点。\n2. 对于网格上的每个 $\\lambda$，使用热启动的循环坐标下降法计算近似的 lasso 解 $\\hat{\\beta}(\\lambda)$，其中每个特征列都经过预缩放以满足 $\\lVert X_{\\cdot j} \\rVert_2^2 = n$（需要这种缩放，以便闭式坐标更新是在水平 $\\lambda$ 上的软阈值操作）。\n3. 沿着路径 $\\{\\lambda_\\ell\\}_{\\ell=1}^{40}$（其中 $\\lambda_1 = \\lambda_{\\max} \\ge \\cdots \\ge \\lambda_{40} = \\lambda_{\\min}$），为每个 $\\ell$ 跟踪活动集 $A(\\lambda_\\ell) \\equiv \\{ j : \\hat{\\beta}_j(\\lambda_\\ell) \\ne 0 \\}$，使用数值阈值 $\\lvert \\hat{\\beta}_j \\rvert > 10^{-8}$ 来判断非零。\n4. 为每个测试用例计算：\n   - 一个布尔标志，指示活动集是否是支持集单调的，即对于每个特征索引 $j$，一旦 $j$ 在某个 $\\lambda_\\ell$ 处变为活动状态，它在所有后续更小的惩罚参数下都保持活动状态（也就是说，没有在 $\\lambda_\\ell$ 处非零的 $j$ 会在之后的某个 $m > \\ell$ 的 $\\lambda_m$ 处变为零）。\n   - 一个布尔标志，指示是否存在至少一个特征沿着路径进入然后又离开模型，即存在索引 $\\ell  m$ 使得 $\\hat{\\beta}_j(\\lambda_\\ell) \\ne 0$ 且 $\\hat{\\beta}_j(\\lambda_m) = 0$。\n   - 整个路径上的最大 KKT 违反度，定义为\n     $$\n     \\max_{\\ell \\in \\{1,\\dots,40\\}} \\max_{j \\in \\{1,\\dots,p\\}} v_{j}(\\lambda_\\ell),\n     $$\n     其中\n     $$\n     v_j(\\lambda) \\equiv\n     \\begin{cases}\n     \\left| \\dfrac{X_{\\cdot j}^{\\top} r(\\lambda)}{n} - \\lambda \\,\\mathrm{sign}(\\hat{\\beta}_j(\\lambda)) \\right|,  \\text{if } \\hat{\\beta}_j(\\lambda) \\ne 0, \\\\\n     \\max\\!\\left( 0, \\left| \\dfrac{X_{\\cdot j}^{\\top} r(\\lambda)}{n} \\right| - \\lambda \\right),  \\text{if } \\hat{\\beta}_j(\\lambda) = 0,\n     \\end{cases}\n     $$\n     并且 $r(\\lambda) = y - X \\hat{\\beta}(\\lambda)$。\n\n测试套件规范。用所述的构造方法实现以下三个确定性案例。所有随机性必须以 $0$ 为种子以确保确定性。\n\n- 案例 1 (正交设计；预期支持集单调):\n  1. 令 $n = 12$ 且 $p = 4$。\n  2. 使用种子 0 生成一个具有独立标准正态分布元素的随机矩阵 $G \\in \\mathbb{R}^{n \\times p}$，并通过对 $G$ 进行瘦 QR 分解来为其列获得一个标准正交基 $Q$。\n  3. 令 $X = \\sqrt{n} \\, Q$ 以使得 $X^{\\top} X / n = I_p$。\n  4. 令 $y = X \\beta^{\\star}$，其中 $\\beta^{\\star} = (0.8, 0.4, 0, 0)^\\top$。\n\n- 案例 2 (高度共线性；创建一个进入后又离开的变量):\n  1. 令 $n = 60$ 且 $p = 3$。使用种子 0 生成三个独立的高斯向量 $a, b, c \\in \\mathbb{R}^n$。\n  2. 通过 Gram–Schmidt 方法对 $a, b, c$ 进行标准正交化，得到标准正交向量 $u, v, w$。\n  3. 定义原始特征 $x_2^{\\mathrm{raw}} = u$，$x_3^{\\mathrm{raw}} = v$ 以及 $x_1^{\\mathrm{raw}} = u + v + 0.05\\, w$。\n  4. 缩放每一列以使 $\\lVert x_j \\rVert_2^2 = n$，即对于 $j \\in \\{1,2,3\\}$，$x_j = \\sqrt{n} \\, x_j^{\\mathrm{raw}} / \\lVert x_j^{\\mathrm{raw}} \\rVert_2$，并设置 $X = [x_1, x_2, x_3]$。\n  5. 令 $y = u + v$。通过这种构造，$x_1$ 初始时与 $y$ 具有最大的边际相关性，因此它倾向于最先进入，但对于足够小的 $\\lambda$，使用两个坐标的精确拟合 $y \\approx x_2 + x_3$ 可能占主导地位，导致 $x_1$ 离开。\n\n- 案例 3 (欠定情况 $p > n$；稀疏信号):\n  1. 令 $n = 30$ 且 $p = 50$。\n  2. 使用种子 0 生成具有独立标准正态分布元素的 $X$，然后缩放每一列以满足 $\\lVert X_{\\cdot j} \\rVert_2^2 = n$。\n  3. 定义一个稀疏向量 $\\beta^{\\star} \\in \\mathbb{R}^p$，其在索引 $1, 5, 10, 20, 30$ 处有且仅有 5 个非零元素，值分别为 $(1.5, -1.0, 0.8, 1.2, -0.7)$，其余元素均为零。\n  4. 令 $y = X \\beta^{\\star}$。\n\n算法要求：\n- 沿着 $\\lambda$ 路径使用带热启动的循环坐标下降法。对于满足 $\\lVert X_{\\cdot j} \\rVert_2^2 = n$ 的列归一化 $X$，索引 $j$ 的坐标更新由软阈值算子给出\n$$\n\\beta_j \\leftarrow \\mathcal{S}\\!\\left( \\frac{X_{\\cdot j}^{\\top} r}{n} + \\beta_j, \\; \\lambda \\right),\n\\quad\n\\mathcal{S}(z,\\lambda) \\equiv \\mathrm{sign}(z)\\,\\max\\{ \\lvert z \\rvert - \\lambda, \\, 0 \\},\n$$\n其中 $r = y - X \\beta$ 是当前残差，更新之后接着进行 $r \\leftarrow r - X_{\\cdot j} \\, (\\beta_j^{\\mathrm{new}} - \\beta_j^{\\mathrm{old}})$。\n- 迭代扫描，直到两次连续完整扫描之间 $\\beta$ 的无穷范数差异低于 $10^{-9}$，或直到达到 $10{,}000$ 次迭代，以先发生者为准。使用最终的 $\\beta$ 进行 KKT 检查。\n\n输出内容：\n- 对于每个案例 $i \\in \\{1,2,3\\}$，返回一个列表 $[b_i, \\ell_i, \\kappa_i]$，其中 $b_i$ 是支持集单调性的布尔标志，$\\ell_i$ 是进入后离开事件的布尔标志，$\\kappa_i$ 是整个路径上的最大 KKT 违反度，四舍五入到六位小数。\n- 您的程序应生成单行输出，其中包含三个测试用例的结果，格式为这些单个案例列表的逗号分隔列表，并用方括号括起来。例如，所需格式的输出是\n\"[ [True,False,0.0],[False,True,0.000001],[True,False,0.0] ]\"\n但要使用您为这三个案例计算出的值。\n\n不提供用户输入；您的程序必须按原样运行，并以指定格式精确打印一行。", "solution": "我们从 lasso 和 Karush–Kuhn–Tucker (KKT) 条件的定义出发，推导算法及其理由，然后设计测试用例来探究活动集在递减惩罚路径上的单调性。\n\n理论基础。lasso 估计量最小化\n$$\nF(\\beta;\\lambda) \\equiv \\frac{1}{2n} \\lVert y - X \\beta \\rVert_2^2 + \\lambda \\lVert \\beta \\rVert_1,\n$$\n该函数在 $\\beta$ 上是凸的。对于任何具有可分离非可微项的凸函数，每次对一个坐标进行精确一维最小化的坐标下降法会收敛到全局最优解。对于具有满足 $\\lVert X_{\\cdot j} \\rVert_2^2 = n$ 的标准化列的最小二乘项，坐标 $j$ 上的一维子问题简化为软阈值操作。具体来说，令 $r = y - X \\beta$ 表示残差，并认识到\n$$\n\\frac{1}{2n} \\lVert y - X \\beta \\rVert_2^2\n=\n\\frac{1}{2n} \\lVert r + X_{\\cdot j} \\beta_j \\rVert_2^2\n=\n\\frac{1}{2n} \\lVert r \\rVert_2^2 + \\frac{1}{n} \\beta_j X_{\\cdot j}^{\\top} r + \\frac{1}{2n} \\lVert X_{\\cdot j} \\rVert_2^2 \\beta_j^2,\n$$\n因此，在固定 $\\beta_{-j}$ 的情况下，逐坐标的目标函数是\n$$\n\\phi(\\beta_j) = \\frac{1}{2} \\beta_j^2 - \\left(\\frac{X_{\\cdot j}^{\\top} r}{n} \\right) \\beta_j + \\lambda \\lvert \\beta_j \\rvert + \\text{constant},\n$$\n使用了 $\\lVert X_{\\cdot j} \\rVert_2^2 / n = 1$。这可以通过软阈值更新来最小化\n$$\n\\beta_j \\leftarrow \\mathcal{S}\\!\\left( \\frac{X_{\\cdot j}^{\\top} r}{n} + \\beta_j, \\; \\lambda \\right),\n\\quad\n\\mathcal{S}(z,\\lambda) = \\mathrm{sign}(z)\\,\\max\\{ \\lvert z \\rvert - \\lambda, \\, 0 \\}.\n$$\n更新 $\\beta_j$ 后，我们通过以下方式精确地更新残差\n$$\nr \\leftarrow r - X_{\\cdot j} \\, (\\beta_j^{\\mathrm{new}} - \\beta_j^{\\mathrm{old}}).\n$$\n\nKKT 最优性条件。对于带有绝对值惩罚的复合目标函数，一阶最优性条件表明 $0$ 必须属于次微分：\n$$\n0 \\in - \\frac{1}{n} X^{\\top} (y - X \\hat{\\beta}) + \\lambda \\, \\partial \\lVert \\hat{\\beta} \\rVert_1.\n$$\n记 $r = y - X \\hat{\\beta}$ 并表示绝对值的次梯度，\n$$\n\\partial \\lvert t \\rvert =\n\\begin{cases}\n\\{\\mathrm{sign}(t)\\},  t \\ne 0,\\\\\n[-1,1],  t = 0,\n\\end{cases}\n$$\n我们得到逐坐标的条件：\n- 如果 $\\hat{\\beta}_j \\ne 0$，则 $X_{\\cdot j}^{\\top} r / n = \\lambda \\,\\mathrm{sign}(\\hat{\\beta}_j)$，\n- 如果 $\\hat{\\beta}_j = 0$，则 $\\lvert X_{\\cdot j}^{\\top} r / n \\rvert \\le \\lambda$。\n\n这些条件既可以作为我们数值求解器正确性的检验，也可以作为解释路径上支持集变化的透镜。\n\n路径与热启动。在 $\\lambda_{\\max} \\equiv \\max_j \\lvert X_{\\cdot j}^{\\top} y \\rvert / n$ 时，零向量 $\\beta = 0$ 满足 KKT 条件：根据构造，$r = y$ 且对于每个 $j$ 都有 $\\lvert X_{\\cdot j}^{\\top} r / n \\rvert \\le \\lambda_{\\max}$。随着 $\\lambda$ 减小，解路径是分段线性的，当变量的 KKT 不等式变为紧约束时，新变量可能进入活动集。在每个后续的 $\\lambda$ 处，使用前一个 $\\hat{\\beta}$ 对坐标下降进行热启动，有助于快速收敛并跟踪路径。\n\n支持集单调性与离开事件。在 $X^{\\top} X / n = I$ 的正交设计中，问题在坐标上解耦，解为\n$$\n\\hat{\\beta}_j(\\lambda) = \\mathcal{S}\\!\\left( \\frac{X_{\\cdot j}^{\\top} y}{n}, \\lambda \\right),\n$$\n因此一旦随着 $\\lambda$ 减小 $\\hat{\\beta}_j$ 变为非零，它对于所有更小的 $\\lambda$ 都保持非零。在一般的相关设计中，lasso 路径不一定是单调的：一个变量可能因为高的边际相关性而早期进入，但随后在较小的 $\\lambda$ 值下被其他相关的预测变量所取代，因为后者集体能更有效地减小目标函数。在这种情况下，存在索引 $\\ell  m$ 使得 $\\hat{\\beta}_j(\\lambda_\\ell) \\ne 0$ 且 $\\hat{\\beta}_j(\\lambda_m) = 0$；这是一个离开事件，构成了对单调性的违反。\n\n测试用例设计。\n\n- 案例 1 构建具有标准正交列的 $X$ 以使得 $X^{\\top} X / n = I_p$，并将 $y$ 设置为一个稀疏线性组合。由于解耦，支持集单调性应该成立，并且不会发生离开事件。\n\n- 案例 2 构建了三个近似共线的特征：$x_2$ 和 $x_3$ 是标准正交的，而 $x_1$ 大约是它们的和加上一个微小的扰动。响应 $y = u + v$ 等于与 $x_2$ 和 $x_3$ 对齐的和。对于大的 $\\lambda$，所有系数都为零。对于中等大的 $\\lambda$，$x_1$ 与 $y$ 的边际相关性最大，因此它倾向于首先进入。然而，对于足够小的 $\\lambda$，同时使用 $x_2$ 和 $x_3$ 能以零残差精确拟合 $y$，这使得为了减少惩罚项而放弃 $x_1$ 同时保持良好拟合变得有利；这就为 $x_1$ 实现了一次进入后离开的事件。在离开点上的 KKT 转换反映了，在 $\\hat{\\beta}_1$ 恰好为零的时刻，其平稳性条件从等式 $X_{\\cdot 1}^{\\top} r / n = \\lambda \\,\\mathrm{sign}(\\hat{\\beta}_1)$ 变为不等式 $\\lvert X_{\\cdot 1}^{\\top} r / n \\rvert \\le \\lambda$。\n\n- 案例 3 是一个 $p > n$ 的欠定问题，带有一个稀疏的真实信号。这用于检验求解器在高维设置下的鲁棒性和 KKT 符合性，在这种设置下，许多非活动坐标必须满足 KKT 不等式条件。支持集单调性是否成立取决于列之间的相关性。\n\n算法细节与正确性检查。\n\n- 列缩放。我们缩放每一列 $X_{\\cdot j}$ 以使得 $\\lVert X_{\\cdot j} \\rVert_2^2 = n$，这使得坐标方向上的二次项具有单位曲率，并验证了在水平 $\\lambda$ 上的软阈值坐标更新的有效性。\n\n- 收敛性。我们执行循环扫描，直到两次完整扫描之间所有坐标上 $\\beta$ 的最大绝对变化小于 $10^{-9}$ 或直到达到 $10{,}000$ 次扫描。热启动和残差维护确保了效率。\n\n- KKT 验证。对于每个 $\\lambda$ 及其收敛的 $\\hat{\\beta}$，我们计算\n$$\ng_j(\\lambda) \\equiv \\frac{X_{\\cdot j}^{\\top} r(\\lambda)}{n}, \\quad r(\\lambda) = y - X \\hat{\\beta}(\\lambda),\n$$\n并定义逐坐标的违反度\n$$\nv_j(\\lambda) =\n\\begin{cases}\n\\left| g_j(\\lambda) - \\lambda \\, \\mathrm{sign}(\\hat{\\beta}_j(\\lambda)) \\right|,  \\hat{\\beta}_j(\\lambda) \\ne 0,\\\\\n\\max\\left( 0, \\lvert g_j(\\lambda) \\rvert - \\lambda \\right),  \\hat{\\beta}_j(\\lambda) = 0.\n\\end{cases}\n$$\n在所有 $\\lambda$ 和 $j$ 上的最大违反度在数值上应该很小（与收敛容差在同一数量级），从而证明其接近最优。\n\n- 单调性与离开事件检测。对于一个 $\\lambda$ 序列 $\\lambda_1 > \\cdots > \\lambda_{40}$ 及其对应的解，我们记录一个布尔活动矩阵，指示是否 $\\lvert \\hat{\\beta}_j(\\lambda_\\ell) \\rvert > 10^{-8}$。如果对于每个 $j$，一旦在某个 $\\ell$ 处变为活动状态，在所有后续的索引 $m > \\ell$ 处都保持活动状态，则支持集单调性成立。如果存在 $j$ 和索引 $\\ell  m$ 使得 $\\hat{\\beta}_j(\\lambda_\\ell) \\ne 0$ 且 $\\hat{\\beta}_j(\\lambda_m) = 0$，则检测到离开事件。\n\n输出。对于三个案例中的每一个，我们输出 $[b_i, \\ell_i, \\kappa_i]$，其中 $b_i$ 是支持集单调性标志，$\\ell_i$ 是离开事件标志，$\\kappa_i$ 是四舍五入到六位小数的最大 KKT 违反度。程序打印包含这三个列表的一个列表的单行输出。\n\n这个设计直接测试了正交设计产生单调支持集，而相关设计可以产生变量进入后又离开的非单调路径这一理论预期。基于 KKT 的违反度度量从数量上验证了整个路径上计算出的解。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef soft_threshold(z, lam):\n    if z > lam:\n        return z - lam\n    elif z  -lam:\n        return z + lam\n    else:\n        return 0.0\n\ndef coordinate_descent_lasso(X, y, lam, beta_init=None, tol=1e-9, max_sweeps=10000):\n    \"\"\"\n    Solve min (1/(2n))||y - X b||^2 + lam * ||b||_1\n    Requires columns of X scaled so that ||X_j||^2 = n for all j.\n    Uses cyclic coordinate descent with residual updates and warm start.\n\n    Returns beta, residual, and a dict with convergence info.\n    \"\"\"\n    n, p = X.shape\n    if beta_init is None:\n        beta = np.zeros(p, dtype=float)\n    else:\n        beta = beta_init.copy()\n\n    r = y - X @ beta  # residual\n    prev_beta = beta.copy()\n    for sweep in range(max_sweeps):\n        max_change = 0.0\n        for j in range(p):\n            # Compute z_j = X_j^T r / n + beta_j\n            zj = (X[:, j].dot(r)) / n + beta[j]\n            new_bj = soft_threshold(zj, lam)\n            delta = new_bj - beta[j]\n            if delta != 0.0:\n                r -= X[:, j] * delta\n                beta[j] = new_bj\n                chg = abs(delta)\n                if chg > max_change:\n                    max_change = chg\n        if max_change  tol:\n            break\n        prev_beta[:] = beta\n    info = {\"sweeps\": sweep + 1, \"max_change\": max_change}\n    return beta, r, info\n\ndef kkt_max_violation(X, y, beta, lam):\n    \"\"\"\n    Compute maximum KKT violation for lasso with scaling 1/(2n)||y - Xb||^2 + lam ||b||_1\n    Columns of X must satisfy ||X_j||^2 = n.\n    \"\"\"\n    n, p = X.shape\n    r = y - X @ beta\n    g = (X.T @ r) / n  # X^T r / n\n    violations = np.empty(p, dtype=float)\n    active = np.abs(beta) > 1e-12\n    # Active: |g_j - lam sign(bj)|; Inactive: max(0, |g_j| - lam)\n    if np.any(active):\n        violations[active] = np.abs(g[active] - lam * np.sign(beta[active]))\n    if np.any(~active):\n        violations[~active] = np.maximum(0.0, np.abs(g[~active]) - lam)\n    return float(np.max(violations))\n\ndef lambda_path(X, y, L=40, min_ratio=0.01):\n    n, p = X.shape\n    # lambda_max = max_j |X_j^T y|/n (so beta=0 satisfies KKT)\n    lam_max = float(np.max(np.abs(X.T @ y)) / n)\n    lam_min = lam_max * min_ratio\n    # Log-spaced decreasing path including endpoints\n    path = np.exp(np.linspace(np.log(lam_max), np.log(lam_min), L))\n    return path\n\ndef standardize_columns_to_n(X):\n    # Scale columns so that ||X_j||^2 = n\n    n, p = X.shape\n    Xs = X.copy().astype(float)\n    for j in range(p):\n        cj = np.linalg.norm(Xs[:, j])\n        if cj == 0:\n            continue\n        # want cj_new^2 = n -> scale by sqrt(n)/cj\n        Xs[:, j] *= (np.sqrt(n) / cj)\n    return Xs\n\ndef orthonormal_columns_from_gaussian(n, p, rng):\n    G = rng.standard_normal((n, p))\n    # Thin QR\n    Q, _ = np.linalg.qr(G)\n    # Ensure exactly p columns\n    Q = Q[:, :p]\n    return Q\n\ndef gram_schmidt_columns(A):\n    # Orthonormalize columns of A using classical Gram-Schmidt with re-orthogonalization\n    n, p = A.shape\n    Q = np.zeros_like(A, dtype=float)\n    for j in range(p):\n        v = A[:, j].astype(float).copy()\n        for _ in range(2):  # re-orthogonalize twice for stability\n            for k in range(j):\n                proj = np.dot(Q[:, k], v) * Q[:, k]\n                v = v - proj\n        normv = np.linalg.norm(v)\n        if normv  1e-14:\n            # If degenerate, fill with a random orthogonal direction deterministically\n            # Here we fallback to a standard basis vector not in the span if possible\n            e = np.zeros(n)\n            idx = j % n\n            e[idx] = 1.0\n            v = e.copy()\n            # re-orthogonalize\n            for k in range(j):\n                v -= np.dot(Q[:, k], v) * Q[:, k]\n            normv = np.linalg.norm(v)\n        Q[:, j] = v / normv\n    return Q\n\ndef analyze_case(X, y, L=40):\n    n, p = X.shape\n    # Standardize columns so ||X_j||^2 = n\n    Xs = standardize_columns_to_n(X)\n    # Build lambda path\n    lam_seq = lambda_path(Xs, y, L=L, min_ratio=0.01)\n    # Warm-start coordinate descent\n    beta = np.zeros(p, dtype=float)\n    active_matrix = np.zeros((L, p), dtype=bool)\n    max_kkt = 0.0\n    for t, lam in enumerate(lam_seq):\n        beta, r, info = coordinate_descent_lasso(Xs, y, lam, beta_init=beta, tol=1e-9, max_sweeps=10000)\n        # Record activity\n        active = np.abs(beta) > 1e-8\n        active_matrix[t, :] = active\n        # KKT violation\n        kkt_v = kkt_max_violation(Xs, y, beta, lam)\n        if kkt_v > max_kkt:\n            max_kkt = kkt_v\n    # Monotonicity: once active, stays active\n    monotone = True\n    leaving = False\n    for j in range(p):\n        act = active_matrix[:, j]\n        # If there's a pattern True then later False -> leaving and non-monotone\n        first_active = np.argmax(act) if np.any(act) else None\n        if first_active is not None and act[first_active]:\n            # If any later inactive exists\n            if np.any(~act[first_active:]):\n                monotone = False\n                leaving = True\n                break\n    return monotone, leaving, round(max_kkt, 6)\n\ndef build_case_1():\n    # Case 1: Orthogonal design via QR\n    rng = np.random.default_rng(0)\n    n, p = 12, 4\n    Q = orthonormal_columns_from_gaussian(n, p, rng)\n    X = np.sqrt(n) * Q  # ensures X^T X / n = I\n    beta_star = np.array([0.8, 0.4, 0.0, 0.0])\n    y = X @ beta_star\n    return X, y\n\ndef build_case_2():\n    # Case 2: Highly collinear construction with enter-then-leave behavior\n    rng = np.random.default_rng(0)\n    n, p = 60, 3\n    a = rng.standard_normal(n)\n    b = rng.standard_normal(n)\n    c = rng.standard_normal(n)\n    U = gram_schmidt_columns(np.column_stack([a, b, c]))\n    u = U[:, 0]; v = U[:, 1]; w = U[:, 2]\n    x2_raw = u.copy()\n    x3_raw = v.copy()\n    x1_raw = u + v + 0.05 * w\n    # Stack and scale to ||x_j||^2 = n\n    Xraw = np.column_stack([x1_raw, x2_raw, x3_raw])\n    # Scale each raw column to norm sqrt(n)\n    X = standardize_columns_to_n(Xraw)\n    # Response\n    y = u + v\n    return X, y\n\ndef build_case_3():\n    # Case 3: Underdetermined, sparse signal\n    rng = np.random.default_rng(0)\n    n, p = 30, 50\n    X = rng.standard_normal((n, p))\n    X = standardize_columns_to_n(X)\n    beta_star = np.zeros(p)\n    nonzero_idx = [0, 4, 9, 19, 29] # 1,5,10,20,30 in problem desc are 1-based\n    beta_vals = [1.5, -1.0, 0.8, 1.2, -0.7]\n    for idx, val in zip(nonzero_idx, beta_vals):\n        beta_star[idx] = val\n    y = X @ beta_star\n    return X, y\n\ndef solve():\n    # Define the test cases from the problem statement.\n    cases = [\n        build_case_1(),\n        build_case_2(),\n        build_case_3(),\n    ]\n    results = []\n    for (X, y) in cases:\n        monotone, leaving, max_kkt = analyze_case(X, y, L=40)\n        results.append([monotone, leaving, max_kkt])\n    # Final print statement in the exact required format.\n    # Print as a single list of per-case lists; booleans and floats.\n    def format_case(case):\n        # case is [bool, bool, float]\n        b1, b2, f = case\n        return f\"[{str(b1)},{str(b2)},{f:.6f}]\" # ensure 6 decimal places\n    \n    # Correcting case 3 indices to be 0-based for python as in problem desc\n    rng = np.random.default_rng(0)\n    n, p = 30, 50\n    X = rng.standard_normal((n, p))\n    X = standardize_columns_to_n(X)\n    beta_star = np.zeros(p)\n    nonzero_idx = [1-1, 5-1, 10-1, 20-1, 30-1] # 0, 4, 9, 19, 29\n    beta_vals = [1.5, -1.0, 0.8, 1.2, -0.7]\n    for idx, val in zip(nonzero_idx, beta_vals):\n        beta_star[idx] = val\n    y = X @ beta_star\n    cases[2] = (X, y) # replace case 3\n    \n    # Re-run analysis\n    results = []\n    for (X, y) in cases:\n        monotone, leaving, max_kkt = analyze_case(X, y, L=40)\n        results.append([monotone, leaving, max_kkt])\n\n    out = \"[\" + \",\".join(format_case(c) for c in results) + \"]\"\n    print(out.replace(\"True\", \"True\").replace(\"False\", \"False\"))\n\nsolve()\n```", "id": "3174656"}, {"introduction": "前一个练习展示了 LASSO 路径的复杂行为，现在我们将深入探讨其背后的理论。LASSO 能否成功恢复真实的稀疏模式，并非总是理所当然，其成功依赖于一个关键的理论条件——“不可表征条件”（Irrepresentable Condition）。当设计矩阵中的相关结构导致此条件被违反时，即使在样本量很大且没有噪声的理想情况下，LASSO 也可能无法正确识别所有重要特征及其符号。这个动手实践将指导你构建一个精确违反不可表征条件的回归问题，并通过计算来验证 LASSO 在这种结构下确实会系统性地失败。这让你能将抽象的理论与具体的计算结果联系起来，深刻理解 LASSO 成功的边界。[@problem_id:3174691]", "problem": "要求您构建一个高维线性回归实例族，并通过算法测试最小绝对收缩和选择算子 (LASSO) 何时能够实现符号一致性，以及何时即使在样本量很大时也会失效。此项任务的重点是不可表示条件在稀疏恢复中的作用。请完全在纯数学环境下进行操作（无物理单位）。您的程序必须实现所有计算，并按以下规定生成最终结果。\n\n考虑一个线性模型，其响应向量为 $y \\in \\mathbb{R}^n$，设计矩阵为 $X \\in \\mathbb{R}^{n \\times p}$，其中各列 $x_j \\in \\mathbb{R}^n$ 已被标准化至经验方差为 $1$，以及一个未知的稀疏向量 $\\beta^\\star \\in \\mathbb{R}^p$。对于惩罚项 $\\lambda \\ge 0$，LASSO 估计量 $\\hat{\\beta}(\\lambda)$ 定义为以下表达式的最小化器：\n$$\n\\frac{1}{2n}\\lVert y - X \\beta \\rVert_2^2 + \\lambda \\lVert \\beta \\rVert_1.\n$$\n定义活跃集（支撑集）$S = \\{ j : \\beta^\\star_j \\ne 0 \\}$ 和非活跃集 $S^c$。如果对于给定的数据集，存在某个 $\\lambda$ 使得 $\\mathrm{sign}(\\hat{\\beta}_S(\\lambda)) = \\mathrm{sign}(\\beta^\\star_S)$ 且 $\\hat{\\beta}_{S^c}(\\lambda) = 0$（精确为零），则称 LASSO 具有符号一致性。\n\n需要使用的基本原理和定义：\n- LASSO 的 Karush–Kuhn–Tucker (KKT) 条件指出，在最优解 $\\hat{\\beta}$ 处，次梯度平稳性条件为：\n$$\n\\frac{1}{n} X^\\top (y - X \\hat{\\beta}) = \\lambda z,\n$$\n其中 $z \\in \\partial \\lVert \\hat{\\beta} \\rVert_1$ 满足：如果 $\\hat{\\beta}_j \\ne 0$，则 $z_j = \\mathrm{sign}(\\hat{\\beta}_j)$；如果 $\\hat{\\beta}_j = 0$，则 $z_j \\in [-1,1]$。\n- 总体格拉姆（协方差）矩阵为 $\\Sigma = \\mathbb{E}[X^\\top X / n]$，对于索引集 $A,B$，块 $\\Sigma_{A,B}$ 是行在 $A$ 中、列在 $B$ 中的子矩阵。\n- 不可表示条件为：\n$$\n\\left\\lVert \\Sigma_{S^c,S} \\, \\Sigma_{S,S}^{-1} \\, \\mathrm{sign}(\\beta^\\star_S) \\right\\rVert_\\infty  1.\n$$\n当左侧的值至少为 $1$ 时，该条件被违反。\n\n数据生成模型。对于每个测试用例，您必须按如下方式生成 $X$ 和 $y$。\n- 固定 $p = 6$ 和 $S = \\{1,2\\}$，其中 $\\beta^\\star_1 = \\beta^\\star_2 = b$ 且对于 $j \\in \\{3,4,5,6\\}$ 有 $\\beta^\\star_j = 0$。使用 $b = 1$。\n- 给定 $\\rho \\in (0,1)$ 和 $\\alpha \\in (0,1)$。通过对具有独立同分布条目的潜在标准正态向量 $u_1, u_2, u_3 \\in \\mathbb{R}^n$ 进行采样来构建 $X$ 的前三列，并设置：\n  - $x_1 = u_1$，\n  - $x_2 = \\rho \\, u_1 + \\sqrt{1-\\rho^2} \\, u_2$，\n  - $c = \\alpha / (1+\\rho)$ 且 $v_3 = 1 - 2 \\alpha^2/(1+\\rho)$；则 $x_3 = c (x_1 + x_2) + \\sqrt{\\max(v_3, 10^{-12})}\\, u_3$。\n  这确保了总体相关性满足 $\\mathbb{E}[x_1^\\top x_1 / n] = \\mathbb{E}[x_2^\\top x_2 / n] = \\mathbb{E}[x_3^\\top x_3 / n] = 1$，$\\mathbb{E}[x_1^\\top x_2 / n] = \\rho$ 和 $\\mathbb{E}[x_3^\\top x_1 / n] = \\mathbb{E}[x_3^\\top x_2 / n] = \\alpha$。\n- 对于第 $4$ 到第 $6$ 列，对独立的标准正态量 $x_j$ 进行采样，其条目独立同分布，且独立于 $u_1,u_2,u_3$。\n- 对 $X$ 的每一列进行中心化，并将其标准化至经验方差为 $1$；也就是说，在中心化之后，将每列除以 $\\sqrt{(1/n)\\sum_{i=1}^n x_{ij}^2}$。\n- 生成一个无噪声响应 $y = X \\beta^\\star$。使用零噪声以隔离结构性效应。\n\n此模型中的不可表示条件。当 $S = \\{1,2\\}$ 且符号为 $\\mathrm{sign}(\\beta^\\star_S) = (1,1)$ 时，总体格拉姆子矩阵满足：\n$$\n\\Sigma_{S,S} = \\begin{pmatrix} 1  \\rho \\\\ \\rho  1 \\end{pmatrix}, \\qquad\n\\Sigma_{\\{3\\},S} = \\begin{pmatrix} \\alpha  \\alpha \\end{pmatrix}.\n$$\n直接计算可得标量：\n$$\n\\left\\lVert \\Sigma_{\\{3\\},S} \\, \\Sigma_{S,S}^{-1} \\, \\mathrm{sign}(\\beta^\\star_S) \\right\\rVert_\\infty\n= \\frac{2 \\alpha}{1+\\rho}.\n$$\n因此，不可表示条件被违反的充要条件是 $\\frac{2\\alpha}{1+\\rho} \\ge 1$。\n\n算法要求。为 LASSO 目标函数实现一个坐标下降求解器：\n$$\n\\min_{\\beta \\in \\mathbb{R}^p} \\; \\frac{1}{2n}\\lVert y - X \\beta \\rVert_2^2 + \\lambda \\lVert \\beta \\rVert_1,\n$$\n使用格拉姆形式，其中 $G = X^\\top X/n$ 和 $c = X^\\top y/n$，并采用软阈值更新。对于在 $\\lambda_{\\max}$ 和 $\\lambda_{\\min}$ 之间对数间隔的一系列 $\\lambda$ 值，其中\n$$\n\\lambda_{\\max} = \\lVert c \\rVert_\\infty, \\qquad \\lambda_{\\min} = 10^{-3} \\lambda_{\\max},\n$$\n使用热启动为每个 $\\lambda$ 计算 $\\hat{\\beta}(\\lambda)$，并确定是否存在任何 $\\lambda$ 使其解与 $\\beta^\\star$ 符号一致。\n\n测试套件。使用以下四个测试用例，每个用例由 $(n, p, \\rho, \\alpha, b, \\text{seed})$ 指定，其中 $p=6$ 且 $b=1$：\n- 用例 A（违反，裕度较大）：$(n = 3000, p = 6, \\rho = 0.5, \\alpha = 0.8, b = 1, \\text{seed} = 123)$。此处 $\\frac{2\\alpha}{1+\\rho} = \\frac{1.6}{1.5} \\approx 1.066\\ge 1$。\n- 用例 B（未违反）：$(n = 3000, p = 6, \\rho = 0.5, \\alpha = 0.3, b = 1, \\text{seed} = 456)$。此处 $\\frac{2\\alpha}{1+\\rho} = \\frac{0.6}{1.5} = 0.4  1$。\n- 用例 C（接近边界，略低于）：$(n = 3000, p = 6, \\rho = 0.5, \\alpha = 0.74, b = 1, \\text{seed} = 789)$。此处 $\\frac{2\\alpha}{1+\\rho} \\approx 0.9867  1$。\n- 用例 D（接近边界，略高于）：$(n = 3000, p = 6, \\rho = 0.5, \\alpha = 0.755, b = 1, \\text{seed} = 321)$。此处 $\\frac{2\\alpha}{1+\\rho} \\approx 1.0067 \\ge 1$。\n\n所需输出。对于每个用例，返回一个布尔值，指示在网格上是否存在至少一个 $\\lambda$ 使其 LASSO 解与 $\\beta^\\star$ 符号一致（如果存在这样的 $\\lambda$，则为 True，否则为 False）。您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，按 A、B、C、D 的顺序显示结果，例如：\"[False,True,True,False]\"。\n\n所有随机性必须由提供的种子控制，以确保可复现性。角度单位和物理单位不适用。百分比不适用。答案为布尔值。", "solution": "该问题定义明确、科学合理，并为一项计算实验提供了完整的规范，以测试 LASSO 估计量的稀疏恢复理论的预测。其理论背景，包括不可表示条件及其在给定数据模型下的具体形式，都陈述正确。数据生成过程明确且可通过算法实现。任务是数值验证在不可表示条件成立与被违反的情况下，LASSO 是否能实现符号一致性。\n\n解决方案通过为四个测试用例中的每一个实施指定的程序来进行。针对一系列正则化参数 $\\lambda$ 计算完整的 LASSO 路径，并检查每个得到的估计 $\\hat{\\beta}(\\lambda)$ 是否具有符号一致性。\n\n**1. 数据生成与预处理**\n\n对于每个测试用例，我们都给定参数 $(n, p, \\rho, \\alpha, b, \\text{seed})$。真实系数向量 $\\beta^\\star \\in \\mathbb{R}^p$ 是稀疏的，其中 $p=6$，非零项仅存在于活跃集 $S=\\{1, 2\\}$ 中，且 $\\beta^\\star_1 = \\beta^\\star_2 = b = 1$。其余分量为零。\n\n$n \\times p$ 的设计矩阵 $X$ 构建如下：\n- 首先，我们生成三个独立同分布的潜在标准正态向量 $u_1, u_2, u_3 \\in \\mathbb{R}^n$。\n- $X$ 的前三列，记为 $x_1, x_2, x_3$，被构造成具有特定的总体相关结构。\n  - $x_1 = u_1$\n  - $x_2 = \\rho \\, u_1 + \\sqrt{1-\\rho^2} \\, u_2$\n  - $x_3 = c (x_1 + x_2) + \\sqrt{\\max(v_3, 10^{-12})}\\, u_3$，其中 $c = \\frac{\\alpha}{1+\\rho}$ 且 $v_3 = 1 - \\frac{2\\alpha^2}{1+\\rho}$。这种构造是专门设计的，旨在设定总体相关性 $\\mathbb{E}[x_1^\\top x_2 / n] = \\rho$ 和 $\\mathbb{E}[x_1^\\top x_3 / n] = \\mathbb{E}[x_2^\\top x_3 / n] = \\alpha$，同时保持所有列的单位方差。\n- 剩余的列 $x_4, \\dots, x_6$ 作为独立的标准正态向量生成，这使得它们在期望上与所有其他列不相关。\n- 随后，对生成的矩阵 $X$ 的每一列进行中心化（减去其均值）并标准化，使其经验方差为 $1$。对于列 $x_j$，这意味着将其替换为 $(x_j - \\bar{x}_j) / \\sigma_j$，其中 $\\sigma_j = \\sqrt{\\frac{1}{n} \\sum_{i=1}^n (x_{ij} - \\bar{x}_j)^2}$。\n- 最后，生成无噪声响应向量 $y = X \\beta^\\star$。\n\n**2. 通过坐标下降求解 LASSO**\n\nLASSO 估计 $\\hat{\\beta}(\\lambda)$ 最小化目标函数：\n$$ L(\\beta; \\lambda) = \\frac{1}{2n}\\lVert y - X \\beta \\rVert_2^2 + \\lambda \\lVert \\beta \\rVert_1 $$\n我们采用坐标下降法，这是一种迭代优化算法，它在固定所有其他系数的同时，一次只对一个系数最小化目标函数。此过程循环重复直至收敛。\n\n目标函数可以表示为样本格拉姆矩阵 $G = X^\\top X/n$ 和向量 $c = X^\\top y/n$ 的形式：\n$$ L(\\beta; \\lambda) = \\frac{1}{2} \\beta^\\top G \\beta - c^\\top \\beta + \\lambda \\lVert \\beta \\rVert_1 + \\text{常数} $$\n$L$ 的光滑部分关于 $\\beta_j$ 的偏导数为 $(G\\beta)_j - c_j$。将次梯度设为零，得到坐标 $j$ 的平稳性条件：\n$$ 0 \\in (G\\beta)_j - c_j + \\lambda \\partial |\\beta_j|$$\n其中 $\\partial|\\beta_j|$ 是绝对值函数的次梯度。由于列标准化确保了 $G$ 的对角元素为 $G_{jj}=1$，我们可以写出 $(G\\beta)_j = \\beta_j + \\sum_{k \\ne j} G_{jk}\\beta_k$。$\\beta_j$ 的更新规则变为：\n$$ \\beta_j \\leftarrow \\mathcal{S}_\\lambda \\left( c_j - \\sum_{k \\ne j} G_{jk}\\beta_k \\right) $$\n其中 $\\mathcal{S}_\\lambda(\\cdot)$ 是软阈值算子，定义为 $\\mathcal{S}_\\lambda(z) = \\mathrm{sign}(z) \\max(|z| - \\lambda, 0)$。\n\n为测试符号一致性，我们必须计算在一系列 $\\lambda$ 值上的解路径 $\\{\\hat{\\beta}(\\lambda)\\}$。我们定义一个包含 100 个对数间隔值的网格，从 $\\lambda_{\\max} = \\lVert c \\rVert_\\infty$ 到 $\\lambda_{\\min} = 10^{-3} \\lambda_{\\max}$。我们从 $\\hat{\\beta}(\\lambda_{\\max})=\\mathbf{0}$ 开始，并使用每个 $\\lambda$ 的解作为下一个更小 $\\lambda$ 的“热启动”，这显著加快了收敛速度。\n\n**3. 符号一致性验证**\n\n对于计算出的路径上的每个解 $\\hat{\\beta}(\\lambda)$，我们检查其与真实向量 $\\beta^\\star$ 的符号一致性。一个解是符号一致的，需要同时满足两个条件：\n1.  **正确的支撑集恢复**：对应于非活跃集 $S^c = \\{3, 4, 5, 6\\}$ 的系数必须全部精确为零。即，对于所有 $j \\in S^c$，$\\hat{\\beta}_j(\\lambda) = 0$。\n2.  **正确的符号恢复**：活跃集 $S=\\{1, 2\\}$ 中系数的符号必须与真实系数的符号匹配。由于 $\\beta^\\star_1 = \\beta^\\star_2 = 1$，这要求 $\\hat{\\beta}_1(\\lambda)  0$ 和 $\\hat{\\beta}_2(\\lambda)  0$。\n\n对于每个测试用例，我们遍历 $\\lambda$ 路径上的所有解。如果有任何一个 $\\hat{\\beta}(\\lambda)$ 满足这两个条件，则认为该算法在该用例下实现了符号一致性，结果为 `True`。如果遍历整个路径都未找到这样的解，则结果为 `False`。这个过程直接测试了是否存在一个合适的 $\\lambda$，正如符号一致性的定义所要求的那样。预计结果将与基于不可表示条件的理论预测一致，该条件在用例 B 和 C 中成立，但在用例 A 和 D 中被违反。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the LASSO sign consistency experiment.\n    \"\"\"\n\n    def soft_threshold(rho, lam):\n        \"\"\"Soft-thresholding operator.\"\"\"\n        return np.sign(rho) * np.maximum(np.abs(rho) - lam, 0.)\n\n    def coordinate_descent_lasso(beta_init, G, c, lam, max_iter, tol):\n        \"\"\"\n        Coordinate descent for LASSO using the Gram matrix formulation.\n        \n        Args:\n            beta_init (np.ndarray): Initial guess for beta.\n            G (np.ndarray): Gram matrix (X'X)/n.\n            c (np.ndarray): Correlation vector (X'y)/n.\n            lam (float): Regularization parameter.\n            max_iter (int): Maximum number of cycles.\n            tol (float): Convergence tolerance.\n            \n        Returns:\n            np.ndarray: The estimated LASSO coefficients.\n        \"\"\"\n        p = len(beta_init)\n        beta = beta_init.copy()\n        \n        for _ in range(max_iter):\n            beta_old = beta.copy()\n            for j in range(p):\n                # Calculate rho_j = c_j - sum_{k != j} G_jk * beta_k\n                # G[j,j] is 1 due to standardization.\n                rho_j = c[j] - (np.dot(G[j, :], beta) - beta[j])\n                beta[j] = soft_threshold(rho_j, lam)\n            \n            if np.max(np.abs(beta - beta_old))  tol:\n                break\n                \n        return beta\n\n    def check_sign_consistency_for_case(n, p, rho, alpha, b, seed):\n        \"\"\"\n        Generates data for one case, runs the LASSO path, and checks for sign consistency.\n        \n        Args:\n            n, p, rho, alpha, b: Model parameters.\n            seed (int): Random seed for reproducibility.\n            \n        Returns:\n            bool: True if a sign-consistent solution is found, False otherwise.\n        \"\"\"\n        # 1. Set seed for reproducibility\n        np.random.seed(seed)\n        \n        # 2. Define true beta vector\n        beta_star = np.zeros(p)\n        beta_star[0] = b  # Corresponds to index 0 (1-based index 1)\n        beta_star[1] = b  # Corresponds to index 1 (1-based index 2)\n        \n        # 3. Generate data according to the specified model\n        u1 = np.random.randn(n)\n        u2 = np.random.randn(n)\n        u3 = np.random.randn(n)\n        \n        X = np.zeros((n, p))\n        X[:, 0] = u1\n        X[:, 1] = rho * u1 + np.sqrt(1 - rho**2) * u2\n        \n        const_c = alpha / (1 + rho)\n        v3 = 1 - 2 * alpha**2 / (1 + rho)\n        X[:, 2] = const_c * (X[:, 0] + X[:, 1]) + np.sqrt(max(v3, 1e-12)) * u3\n        \n        for j in range(3, p):\n            X[:, j] = np.random.randn(n)\n            \n        # 4. Center and standardize X to have empirical variance 1\n        X_mean = np.mean(X, axis=0)\n        X = X - X_mean\n        X_std = np.sqrt(np.mean(X**2, axis=0))\n        X_std[X_std  1e-9] = 1.0 # Avoid division by zero\n        X = X / X_std\n        \n        # 5. Generate noiseless response vector\n        y = X @ beta_star\n        \n        # 6. Set up for LASSO path computation\n        G = (X.T @ X) / n\n        c = (X.T @ y) / n\n        \n        lambda_max = np.max(np.abs(c))\n        if lambda_max  1e-9: # Handle edge case of zero correlation\n            return False # No coefficients will be selected.\n\n        lambda_min = 1e-3 * lambda_max\n        n_lambdas = 100\n        lambda_grid = np.logspace(np.log10(lambda_max), np.log10(lambda_min), num=n_lambdas)\n        \n        # 7. Run LASSO path and check for sign consistency\n        beta_hat = np.zeros(p)\n        sign_beta_star_S = np.sign(beta_star[:2])\n        found_consistent = False\n        \n        for lam in lambda_grid:\n            beta_hat = coordinate_descent_lasso(beta_hat, G, c, lam, max_iter=2000, tol=1e-9)\n            \n            # Check for exact zeros in the inactive set\n            inactive_is_zero = np.all(np.abs(beta_hat[2:])  1e-9)\n            \n            if inactive_is_zero:\n                # Check for correct signs in the active set\n                signs_hat_S = np.sign(beta_hat[:2])\n                active_signs_correct = np.all(signs_hat_S == sign_beta_star_S)\n                \n                if active_signs_correct:\n                    found_consistent = True\n                    break \n                    \n        return found_consistent\n\n    test_cases = [\n        # Case A: (n, p, rho, alpha, b, seed)\n        (3000, 6, 0.5, 0.8, 1, 123),\n        # Case B:\n        (3000, 6, 0.5, 0.3, 1, 456),\n        # Case C:\n        (3000, 6, 0.5, 0.74, 1, 789),\n        # Case D:\n        (3000, 6, 0.5, 0.755, 1, 321),\n    ]\n    \n    results = []\n    for case in test_cases:\n        result = check_sign_consistency_for_case(*case)\n        results.append(result)\n\n    # Format output as specified: [res_A,res_B,res_C,res_D]\n    print(f\"[{','.join(map(str, results))}]\".replace(\"True\", \"True\").replace(\"False\", \"False\"))\n\nsolve()\n```", "id": "3174691"}]}