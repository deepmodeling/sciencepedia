{"hands_on_practices": [{"introduction": "在特征数量 $p$ 远大于样本数量 $n$ 的“小 $n$ 大 $p$”场景下，一个直观的首要步骤是快速筛选掉明显不相关的特征。独立性筛选（Sure Independence Screening, SIS）正是通过计算并排序各个特征与响应变量之间的边际相关性来实现这一目标。这个练习的价值不仅在于实现 SIS 算法，更在于通过精心构建的反例来深刻理解其内在局限性，揭示为何在特征高度相关时，这种看似简单的方法可能会失效，从而为后续更复杂的模型奠定基础 `[@problem_id:3186687]`。", "problem": "您需要编写一个完整的程序，以评估确定独立筛选（Sure Independence Screening, SIS）在小样本大特征情景下的筛选性能，即特征数量远超样本数量（$p \\gg n$）的情况。工作将在经典线性模型下进行，其中响应变量为 $y \\in \\mathbb{R}^{n}$，设计矩阵为 $X \\in \\mathbb{R}^{n \\times p}$，模型为 $y = X \\beta + \\varepsilon$。SIS 规则根据特征与响应变量的样本皮尔逊相关系数的绝对值对特征进行排序，并保留前 $k$ 个特征。对于一组给定的构造测试用例，您的程序必须为每个用例确定 SIS 是否在选出的前 $k$ 个特征中成功保留了所有真正活跃的特征。\n\n使用的基本原理：\n- 线性回归设置 $y = X \\beta + \\varepsilon$。\n- 列 $X_{j}$ 和 $y$ 之间的样本皮尔逊相关系数定义为\n$$\n\\mathrm{corr}(X_{j}, y) \\;=\\; \\frac{\\sum_{i=1}^{n} (X_{ij} - \\bar{X}_{j})(y_{i} - \\bar{y})}{\\sqrt{\\sum_{i=1}^{n} (X_{ij} - \\bar{X}_{j})^{2}} \\; \\sqrt{\\sum_{i=1}^{n} (y_{i} - \\bar{y})^{2}} }.\n$$\n特征 $j$ 的 SIS 分数是 $|\\mathrm{corr}(X_{j}, y)|$。\n\n您的程序必须为指定的测试套件实现以下评估协议。对于每个测试用例：\n1. 完全按照下文所述构造 $X$ 和 $y$，确保在计算样本相关系数时，所有列和响应变量都经过均值中心化。\n2. 计算所有 $j \\in \\{0,1,\\dots,p-1\\}$ 的绝对样本相关系数 $|\\mathrm{corr}(X_{j}, y)|$。\n3. 按得分降序选择前 $k$ 个特征的索引，若得分相同，则按列索引升序打破平局（即稳定排序）。\n4. 报告一个布尔值，指示是否所有真正活跃的索引都出现在这前 $k$ 个索引中。\n\n设计构造和测试套件：\n以下所有构造都是确定性的线性代数过程。在每种情况下，当一个步骤需要抽取一个 $\\mathbb{R}^{n}$中的随机向量时，生成一个具有独立标准正态分布条目的向量，然后对其进行均值中心化。当需要正交规范化时，使用Gram–Schmidt方法对先前构造的向量进行处理，以获得单位范数、相互正交的向量。对于“投影到 $y$ 的正交补空间上”，将向量 $v$ 替换为 $v - \\frac{\\langle v, y \\rangle}{\\langle y, y \\rangle} y$。\n\n- 测试用例 1（独立，理想路径）：\n  - 参数：$n = 60$，$p = 200$，活跃特征数 $s = 3$，顶部选择大小 $k = 3$。\n  - 构造 $s$ 个正交规范列 $u_{1}, u_{2}, u_{3} \\in \\mathbb{R}^{n}$ 并设置活跃集 $S = \\{10, 50, 150\\}$。定义系数 $\\beta_{\\text{active}} = [1.0, -1.5, 0.5]$。设置 $y = \\sum_{\\ell=1}^{3} \\beta_{\\text{active},\\ell} u_{\\ell}$，无噪声。将 $u_{1}, u_{2}, u_{3}$ 分别放置为 $X$ 中索引在 $S$ 内的列。对于每个其他列索引 $j \\notin S$，构造一个随机的均值中心化向量 $v_{j}$ 并将其投影到 $y$ 的正交补空间上，使得 $\\mathrm{corr}(v_{j}, y) = 0$，然后将其作为列 $X_{j}$ 放置。\n  - 预期行为：只有活跃列与 $y$ 具有非零相关性，因此 $k=3$ 时的 SIS 应该包含所有活跃索引。\n\n- 测试用例 2（相关性抵消与影子变量；失败模式）：\n  - 参数：$n = 40$，$p = 300$，$k = 2$，相关性调整 $\\epsilon = 0.1$。\n  - 构造两个正交规范向量 $a, d \\in \\mathbb{R}^{n}$。定义活跃列 $X_{5} = a$ 和 $X_{25} = a + \\epsilon d$。设置 $y = X_{5} - X_{25} = -\\epsilon d$（无噪声）。引入一个影子变量 $X_{0} = y$。对于每个其他列索引 $j \\notin \\{0,5,25\\}$，构造一个随机的均值中心化向量并将其投影到 $y$ 的正交补空间上，然后将其作为 $X_{j}$ 放置。\n  - 预期行为：$|\\mathrm{corr}(X_{0}, y)| = 1$，$|\\mathrm{corr}(X_{25}, y)| = \\frac{\\epsilon}{\\sqrt{1 + \\epsilon^{2}}}$，以及 $|\\mathrm{corr}(X_{5}, y)| = 0$，而所有其他列的相关性为零。当 $k = 2$ 时，SIS 选择 $\\{0, 25\\}$ 并错过了索引 5，因此 SIS 未能保留所有活跃特征。\n\n- 测试用例 3（相关、同符号信号与影子变量；成功模式）：\n  - 参数：$n = 50$，$p = 300$，$k = 3$，相关性调整 $\\epsilon = 0.5$。\n  - 构造两个正交规范向量 $a, d \\in \\mathbb{R}^{n}$。定义活跃列 $X_{10} = a$ 和 $X_{20} = a + \\epsilon d$。设置 $y = X_{10} + X_{20}$（无噪声）。引入一个影子变量 $X_{0} = y$。对于每个其他列索引 $j \\notin \\{0,10,20\\}$，构造一个随机的均值中心化向量并将其投影到 $y$ 的正交补空间上，然后将其作为 $X_{j}$ 放置。\n  - 预期行为：$|\\mathrm{corr}(X_{0}, y)| = 1$，并且两个活跃列都与 $y$ 具有大的正相关性。当 $k = 3$ 时，SIS 应该在所选集合中包含两个活跃索引。\n\n- 测试用例 4（重复边缘案例；在最小k值时失败）：\n  - 参数：$n = 30$，$p = 1000$，$k = 1$。\n  - 构造一个均值中心化、单位范数的向量 $a \\in \\mathbb{R}^{n}$。在索引 50 处定义真正的活跃列 $X_{50} = a$，并设置 $y = X_{50}$（无噪声）。定义一个重复列 $X_{0} = X_{50}$。对于每个其他列索引 $j \\notin \\{0,50\\}$，构造一个随机的均值中心化向量并将其投影到 $y$ 的正交补空间上，然后将其作为 $X_{j}$ 放置。\n  - 预期行为：$X_{0}$ 和 $X_{50}$ 与 $y$ 的相关性大小都为 1。当 $k = 1$ 并通过按索引升序打破平局时，SIS 选择了索引 0 并排除了真正的活跃索引 50，因此 SIS 失败。\n\n将“筛选成功”定义为一个布尔值，其为真的条件是当且仅当活跃索引集 $S$ 是所选前 $k$ 个索引集的子集。您的程序必须输出一行，其中包含上述四个案例的布尔结果，按顺序排列，格式为方括号括起来的逗号分隔列表，例如 `[ True, False, True, False ]`。不需要用户输入，也不得使用外部数据。所有量都是无单位的，必须完全按照规定进行计算。", "solution": "该问题要求在一个高维线性回归设定（$p \\gg n$）下，对四个指定的测试用例评估确定独立筛选（Sure Independence Screening, SIS）方法的性能。任务的核心是根据确定性的线性代数过程为每个案例构造数据矩阵 $X \\in \\mathbb{R}^{n \\times p}$ 和响应向量 $y \\in \\mathbb{R}^{n}$，然后确定 SIS 是否成功识别了所有真正活跃的特征。\n\n首先，我们将问题形式化。线性模型为 $y = X\\beta + \\varepsilon$。SIS 通过特征与响应变量 $y$ 的样本皮尔逊相关系数的绝对值对特征进行排序。特征 $j$ 的样本相关系数为：\n$$\n\\mathrm{corr}(X_j, y) = \\frac{\\sum_{i=1}^{n} (X_{ij} - \\bar{X}_j)(y_i - \\bar{y})}{\\sqrt{\\sum_{i=1}^{n} (X_{ij} - \\bar{X}_j)^2} \\sqrt{\\sum_{i=1}^{n} (y_i - \\bar{y})^2}}\n$$\n其中 $X_j$ 是 $X$ 的第 $j$ 列。由于构造中的所有向量都指定为均值中心化的（即 $\\bar{X}_j = 0$ 和 $\\bar{y} = 0$），该公式简化为余弦相似度：\n$$\n\\mathrm{corr}(X_j, y) = \\frac{\\langle X_j, y \\rangle}{\\|X_j\\|_2 \\|y\\|_2}\n$$\n特征 $j$ 的 SIS 分数是 $|\\mathrm{corr}(X_j, y)|$。计算完所有 $p$ 个特征的分数后，选择前 $k$ 个特征。分数上的平局通过选择列索引较小的特征来打破。筛选成功定义为真实活跃特征索引集 $S$ 是所选前 $k$ 个索引集的子集这一事件。\n\n评估通过分析每个测试用例来进行。\n\n**测试用例 1: 独立，理想路径**\n- 参数：$n = 60$，$p = 200$，活跃特征数 $s = 3$，顶部选择大小 $k = 3$。\n- 活跃集 $S = \\{10, 50, 150\\}$，系数 $\\beta_{\\text{active}} = [1.0, -1.5, 0.5]$。\n- 构造：我们构造三个正交规范、均值中心化的向量 $u_1, u_2, u_3 \\in \\mathbb{R}^{n}$。响应变量为 $y = 1.0 u_1 - 1.5 u_2 + 0.5 u_3$。$X$ 的活跃列设置为 $X_{10} = u_1$，$X_{50} = u_2$ 和 $X_{150} = u_3$。所有其他列 $X_j$（$j \\notin S$）被构造成与 $y$ 正交，这意味着 $\\langle X_j, y \\rangle = 0$，因此 $\\mathrm{corr}(X_j, y) = 0$。\n- 相关性分析：\n    - $y$ 的范数是 $\\|y\\|_2 = \\sqrt{1.0^2 + (-1.5)^2 + 0.5^2} = \\sqrt{1 + 2.25 + 0.25} = \\sqrt{3.5}$。\n    - 对于活跃特征 10：$\\mathrm{corr}(X_{10}, y) = \\frac{\\langle u_1, y \\rangle}{\\|u_1\\|_2 \\|y\\|_2} = \\frac{1.0}{\\sqrt{3.5}}$。\n    - 对于活跃特征 50：$\\mathrm{corr}(X_{50}, y) = \\frac{\\langle u_2, y \\rangle}{\\|u_2\\|_2 \\|y\\|_2} = \\frac{-1.5}{\\sqrt{3.5}}$。\n    - 对于活跃特征 150：$\\mathrm{corr}(X_{150}, y) = \\frac{\\langle u_3, y \\rangle}{\\|u_3\\|_2 \\|y\\|_2} = \\frac{0.5}{\\sqrt{3.5}}$。\n- 结果：活跃特征的绝对相关系数分别为 $\\frac{1.5}{\\sqrt{3.5}}$、$\\frac{1.0}{\\sqrt{3.5}}$ 和 $\\frac{0.5}{\\sqrt{3.5}}$。它们都非零。所有其他特征的相关性为 0。因此，这三个活跃特征将被排在前3名。当 k=3 时，SIS 精确地选择了活跃特征集 S={10, 50, 150}。条件 $S \\subseteq \\{10, 50, 150\\}$ 得到满足。\n- 结果：**True**\n\n**测试用例 2: 相关性抵消与影子变量**\n- 参数：$n = 40$，$p = 300$，$k = 2$，$\\epsilon = 0.1$。\n- 活跃集 $S = \\{5, 25\\}$。\n- 构造：我们构造两个正交规范、均值中心化的向量 $a, d \\in \\mathbb{R}^{n}$。活跃列为 $X_5 = a$ 和 $X_{25} = a + \\epsilon d$。响应变量为 $y = X_5 - X_{25} = -\\epsilon d$。在索引 0 处引入一个“影子”变量 $X_0 = y$。所有其他列与 y 的相关性为零。\n- 相关性分析：\n    - 对于影子特征 0：$X_0=y$，所以 $\\mathrm{corr}(X_0,y)=1$。\n    - 对于活跃特征 5：$\\mathrm{corr}(X_5, y) = \\frac{\\langle a, -\\epsilon d \\rangle}{\\|a\\|_2 \\|-\\epsilon d\\|_2} = \\frac{-\\epsilon \\langle a, d \\rangle}{\\epsilon \\|d\\|_2} = 0$，因为 $\\langle a, d \\rangle = 0$。\n    - 对于活跃特征 25：$\\mathrm{corr}(X_{25}, y) = \\frac{\\langle a+\\epsilon d, -\\epsilon d \\rangle}{\\|a+\\epsilon d\\|_2 \\|-\\epsilon d\\|_2} = \\frac{-\\epsilon^2 \\langle d,d \\rangle}{\\sqrt{\\|a\\|^2+\\epsilon^2\\|d\\|^2} (\\epsilon\\|d\\|_2)} = \\frac{-\\epsilon^2}{\\sqrt{1+\\epsilon^2} \\cdot \\epsilon} = \\frac{-\\epsilon}{\\sqrt{1+\\epsilon^2}}$。\n- 结果：分数值为 $|\\mathrm{corr}(X_0,y)| = 1$，$|\\mathrm{corr}(X_5,y)| = 0$，以及 $|\\mathrm{corr}(X_{25},y)| = \\frac{\\epsilon}{\\sqrt{1+\\epsilon^2}}$。对于 $\\epsilon = 0.1$，索引 25 的分数约为 $\\frac{0.1}{\\sqrt{1.01}} \\approx 0.0995$。所有其他特征的相关性为零。最高的分数属于索引 0 和 25。当 k=2 时，SIS 选择了集合 {0, 25}。真正的活跃集是 S={5, 25}。条件 $S \\subseteq \\{0, 25\\}$ 未被满足，因为 $5 \\notin \\{0, 25\\}$。\n- 结果：**False**\n\n**测试用例 3: 相关、同符号信号与影子变量**\n- 参数：$n = 50$，$p = 300$，$k = 3$，$\\epsilon = 0.5$。\n- 活跃集 $S = \\{10, 20\\}$。\n- 构造：我们使用两个正交规范、均值中心化的向量 $a, d \\in \\mathbb{R}^{n}$。活跃列为 $X_{10} = a$ 和 $X_{20} = a + \\epsilon d$。响应变量为 $y = X_{10} + X_{20} = 2a + \\epsilon d$。影子变量是 $X_0 = y$。其他列的相关性为零。\n- 相关性分析：\n    - 对于影子特征 0：$X_0=y$，所以 $\\mathrm{corr}(X_0,y)=1$。\n    - 对于活跃特征 10：$\\mathrm{corr}(X_{10}, y) = \\frac{\\langle a, 2a + \\epsilon d \\rangle}{\\|a\\|_2 \\|2a + \\epsilon d\\|_2} = \\frac{2}{\\sqrt{4+\\epsilon^2}}$。\n    - 对于活跃特征 20：$\\mathrm{corr}(X_{20}, y) = \\frac{\\langle a+\\epsilon d, 2a + \\epsilon d \\rangle}{\\|a+\\epsilon d\\|_2 \\|2a + \\epsilon d\\|_2} = \\frac{2+\\epsilon^2}{\\sqrt{1+\\epsilon^2}\\sqrt{4+\\epsilon^2}}$。\n- 结果：当 $\\epsilon=0.5$时：\n    - $|\\mathrm{corr}(X_0,y)| = 1$。\n    - $|\\mathrm{corr}(X_{10},y)| = \\frac{2}{\\sqrt{4+0.25}} = \\frac{2}{\\sqrt{4.25}} \\approx 0.970$。\n    - $|\\mathrm{corr}(X_{20},y)| = \\frac{2+0.25}{\\sqrt{1+0.25}\\sqrt{4+0.25}} = \\frac{2.25}{\\sqrt{1.25}\\sqrt{4.25}} \\approx 0.976$。\n所有其他特征的相关性为零。得分最高的三个特征是 0, 10, 和 20。当 k=3 时，SIS 选择了 {0, 10, 20}。真正的活跃集是 S={10, 20}。条件 $S \\subseteq \\{0, 10, 20\\}$ 得到满足。\n- 结果：**True**\n\n**测试用例 4: 重复边缘案例**\n- 参数：$n = 30$，$p = 1000$，$k = 1$。\n- 活跃集 $S = \\{50\\}$。\n- 构造：我们使用一个均值中心化、单位范数的向量 $a \\in \\mathbb{R}^{n}$。活跃列是 $X_{50} = a$，响应变量是 $y = X_{50} = a$。在索引 0 处引入一个重复列：$X_0 = X_{50} = a$。其他列的相关性为零。\n- 相关性分析：\n    - 对于活跃特征 50：$\\mathrm{corr}(X_{50}, y) = \\frac{\\langle a, a \\rangle}{\\|a\\|_2 \\|a\\|_2} = 1$。\n    - 对于重复特征 0：$\\mathrm{corr}(X_0, y) = \\frac{\\langle a, a \\rangle}{\\|a\\|_2 \\|a\\|_2} = 1$。\n- 结果：特征 0 和 50 的最高分均为 1。所有其他特征的分数为 0。这造成了平局。问题规定平局通过增加列索引来打破。因此，特征 0 的排名高于特征 50。当 k=1 时，SIS 只选择了集合 {0}。真正的活跃集是 S={50}。条件 $S \\subseteq \\{0\\}$ 未被满足。\n- 结果：**False**", "answer": "```python\nimport numpy as np\n\ndef _mean_center(v):\n    \"\"\"Mean-centers a vector.\"\"\"\n    return v - np.mean(v)\n\ndef _gram_schmidt_process(vectors):\n    \"\"\"\n    Performs modified Gram-Schmidt orthonormalization on a list of vectors.\n    Assumes vectors are columns of a matrix. Returns a list of orthonormal vectors.\n    \"\"\"\n    basis = []\n    for v in vectors:\n        v_orth = v.copy()\n        # Subtract projection on previous basis vectors\n        for u in basis:\n            v_orth -= np.dot(v_orth, u) * u\n        \n        norm = np.linalg.norm(v_orth)\n        if norm > 1e-12:\n            basis.append(v_orth / norm)\n    return basis\n\ndef _run_sis_test(X, y, k, active_indices):\n    \"\"\"\n    Performs Sure Independence Screening and checks for success.\n    \n    Args:\n        X (np.ndarray): Design matrix of shape (n, p).\n        y (np.ndarray): Response vector of shape (n,).\n        k (int): Number of features to select.\n        active_indices (set): Set of true active feature indices.\n        \n    Returns:\n        bool: True if all active indices are selected, False otherwise.\n    \"\"\"\n    n, p = X.shape\n    \n    # Per the problem, X and y are constructed from mean-centered vectors.\n    # To be robust, a full Pearson correlation calculation is implemented.\n    y_centered = y - np.mean(y)\n    y_norm = np.linalg.norm(y_centered)\n    \n    scores = np.zeros(p)\n    for j in range(p):\n        col = X[:, j]\n        col_centered = col - np.mean(col)\n        col_norm = np.linalg.norm(col_centered)\n        \n        if col_norm > 1e-12 and y_norm > 1e-12:\n            corr = np.dot(col_centered, y_centered) / (col_norm * y_norm)\n            scores[j] = abs(corr)\n        else:\n            scores[j] = 0.0\n\n    # Tie-breaking: sort by decreasing score, then increasing index.\n    indices = np.arange(p)\n    # np.lexsort sorts by the last key first.\n    # So, (indices, -scores) sorts by -scores, then by indices for ties.\n    sorted_indices = np.lexsort((indices, -scores))\n    top_k_indices = sorted_indices[:k]\n    \n    # Check if all active indices are in the top k\n    success = set(active_indices).issubset(set(top_k_indices))\n    return success\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the results.\n    \"\"\"\n    # A fixed seed ensures reproducibility for the \"random\" vector constructions.\n    rng = np.random.default_rng(0)\n    results = []\n\n    # Test Case 1: independent, happy path\n    n1, p1, s1, k1 = 60, 200, 3, 3\n    active_indices1 = {10, 50, 150}\n    beta_active1 = np.array([1.0, -1.5, 0.5])\n    \n    initial_vectors1 = [ _mean_center(rng.standard_normal(n1)) for _ in range(s1)]\n    basis1 = _gram_schmidt_process(initial_vectors1)\n    u1, u2, u3 = basis1[0], basis1[1], basis1[2]\n    \n    y1 = beta_active1[0] * u1 + beta_active1[1] * u2 + beta_active1[2] * u3\n    \n    X1 = np.zeros((n1, p1))\n    X1[:, 10] = u1\n    X1[:, 50] = u2\n    X1[:, 150] = u3\n    \n    y1_norm_sq = np.dot(y1, y1)\n    for j in range(p1):\n        if j not in active_indices1:\n            vj = _mean_center(rng.standard_normal(n1))\n            X1[:, j] = vj - (np.dot(vj, y1) / y1_norm_sq) * y1\n    results.append(_run_sis_test(X1, y1, k1, active_indices1))\n\n    # Test Case 2: correlated cancellation\n    n2, p2, k2, epsilon2 = 40, 300, 2, 0.1\n    active_indices2 = {5, 25}\n    \n    initial_vectors2 = [_mean_center(rng.standard_normal(n2)) for _ in range(2)]\n    basis2 = _gram_schmidt_process(initial_vectors2)\n    a2, d2 = basis2[0], basis2[1]\n    \n    X2 = np.zeros((n2, p2))\n    X5 = a2\n    X25 = a2 + epsilon2 * d2\n    y2 = X5 - X25\n    \n    X2[:, 5] = X5\n    X2[:, 25] = X25\n    X2[:, 0] = y2\n    \n    y2_norm_sq = np.dot(y2, y2)\n    for j in range(p2):\n        if j not in {0, 5, 25}:\n            vj = _mean_center(rng.standard_normal(n2))\n            X2[:, j] = vj - (np.dot(vj, y2) / y2_norm_sq) * y2\n    results.append(_run_sis_test(X2, y2, k2, active_indices2))\n\n    # Test Case 3: correlated, same-sign signals\n    n3, p3, k3, epsilon3 = 50, 300, 3, 0.5\n    active_indices3 = {10, 20}\n    \n    initial_vectors3 = [_mean_center(rng.standard_normal(n3)) for _ in range(2)]\n    basis3 = _gram_schmidt_process(initial_vectors3)\n    a3, d3 = basis3[0], basis3[1]\n\n    X3 = np.zeros((n3, p3))\n    X10 = a3\n    X20 = a3 + epsilon3 * d3\n    y3 = X10 + X20\n\n    X3[:, 10] = X10\n    X3[:, 20] = X20\n    X3[:, 0] = y3\n\n    y3_norm_sq = np.dot(y3, y3)\n    for j in range(p3):\n        if j not in {0, 10, 20}:\n            vj = _mean_center(rng.standard_normal(n3))\n            X3[:, j] = vj - (np.dot(vj, y3) / y3_norm_sq) * y3\n    results.append(_run_sis_test(X3, y3, k3, active_indices3))\n\n    # Test Case 4: duplication edge case\n    n4, p4, k4 = 30, 1000, 1\n    active_indices4 = {50}\n    \n    initial_vector4 = _mean_center(rng.standard_normal(n4))\n    a4 = initial_vector4 / np.linalg.norm(initial_vector4)\n    \n    X4 = np.zeros((n4, p4))\n    X50 = a4\n    y4 = X50\n    \n    X4[:, 50] = X50\n    X4[:, 0] = X50\n    \n    y4_norm_sq = np.dot(y4, y4)\n    if y4_norm_sq > 1e-12:\n        for j in range(p4):\n            if j not in {0, 50}:\n                vj = _mean_center(rng.standard_normal(n4))\n                X4[:, j] = vj - (np.dot(vj, y4) / y4_norm_sq) * y4\n    results.append(_run_sis_test(X4, y4, k4, active_indices4))\n    \n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3186687"}, {"introduction": "LASSO 是高维统计的基石，能够同时进行回归和变量选择。然而，它的成功并非毫无条件，尤其是在预测变量相互关联时。本练习将带您深入 LASSO 的“引擎盖”之下，探索其成功的理论保障——“不可表示条件”（Irrepresentable Condition）。通过推导该条件并在模拟数据上进行验证，您将深刻理解这个强大工具在何种情况下能够精确地识别出真正的稀疏信号，以及特征间的相关性如何影响其性能 `[@problem_id:3186678]`。", "problem": "考虑在线性模型 $y = X \\beta + \\varepsilon$ 的小样本大维度情境下，其中 $p \\gg n$，且 $X \\in \\mathbb{R}^{n \\times p}$，$\\beta \\in \\mathbb{R}^{p}$，$\\varepsilon \\in \\mathbb{R}^{n}$。最小绝对收缩和选择算子 (Least Absolute Shrinkage and Selection Operator, LASSO) 估计量定义为以下凸目标函数的最小化子：\n$$\n\\widehat{\\beta}_{\\lambda} \\in \\arg\\min_{\\beta \\in \\mathbb{R}^{p}} \\left\\{\\frac{1}{2n}\\|y - X\\beta\\|_2^2 + \\lambda \\|\\beta\\|_1\\right\\},\n$$\n其中 $\\lambda > 0$ 是一个正则化参数，$\\|\\cdot\\|_1$ 表示 $\\ell_1$范数。精确支持集恢复问题旨在探究是否 $\\operatorname{supp}(\\widehat{\\beta}_{\\lambda}) = \\operatorname{supp}(\\beta)$。\n\n仅从凸优化的 Karush-Kuhn-Tucker (KKT) 最优性条件、$\\ell_1$范数的次梯度以及样本Gram矩阵 $G = X^{\\top} X / n$ 的定义出发，推导出一个LASSO实现精确支持集恢复所必须满足的必要不等式，即不可表示条件。您的推导必须从线性模型和KKT条件开始，并且必须指明由 $G$ 的分区量化的特征之间的相关性如何控制恢复真实支持集的能力。请精确解释在 $p \\gg n$ 的情境下，特征相关性如何可能违反此条件，从而阻碍精确支持集恢复。\n\n然后，实现一个程序，为提供的一个包含三个综合场景的测试套件执行以下步骤，每个场景都旨在探究不可表示条件的不同方面：\n\n- 对于每个场景，生成一个具有 $n$ 行和 $p$ 列的设计矩阵 $X$，一个具有已知支持集 $S \\subset \\{1,\\dots,p\\}$ 的真实系数向量 $\\beta$，以及一个响应向量 $y = X\\beta + \\varepsilon$，其中 $\\varepsilon$ 从零均值高斯分布中抽取。$X$ 的所有列都必须标准化为均值为 $0$ 和欧几里得范数为 $\\sqrt{n}$，$y$ 必须中心化为均值为 $0$（无截距项）。支持集 $S$ 固定为前 $k$ 个索引 $\\{0,1,\\dots,k-1\\}$，其系数为严格正且大小相等。每个场景使用固定的随机种子以确保可复现性。\n\n- 仅使用样本Gram矩阵 $G = X^{\\top} X / n$，计算不可表示指数\n$$\n\\mu = \\left\\|G_{S^c,S} \\, G_{S,S}^{-1} \\, \\operatorname{sgn}(\\beta_S)\\right\\|_{\\infty},\n$$\n其中 $S^c$ 是 $S$ 的补集，$G_{S,S}$ 是由 $S$ 索引的 $G$ 的主子矩阵，$G_{S^c,S}$ 是行在 $S^c$ 中、列在 $S$ 中的 $G$ 的子矩阵，$\\operatorname{sgn}(\\cdot)$ 是按元素取符号的函数。如果 $G_{S,S}$ 不可逆，请使用数值稳定的替代方法（例如Moore-Penrose伪逆）来评估表达式。当且仅当 $\\mu  1$ 时，声明不可表示条件成立。\n\n- 通过坐标下降法拟合LASSO以获得 $\\widehat{\\beta}_{\\lambda}$，对每个场景使用上述目标函数和给定的 $\\lambda$。通过检查 $\\widehat{\\beta}_{\\lambda}$ 的非零条目的索引集是否等于 $S$ 来判断是否实现了精确支持集恢复，其中如果估计系数的绝对值严格大于 $10^{-6}$，则认为该索引非零。\n\n测试套件和参数：\n1. 理想情况（弱相关性）：\n   - $n = 30$，$p = 80$，$k = 5$，系数大小 $b = 1.0$，噪声标准差 $\\sigma = 0.01$，正则化参数 $\\lambda = 0.2$，随机种子 $0$。\n   - 构造方法：$X$ 具有独立的标准正态分布条目；除了随机抽样外，不强制施加特殊的关​​联结构。\n2. 近边界相关性（单个强相关项）：\n   - $n = 30$，$p = 80$，$k = 5$，$b = 1.0$，$\\sigma = 0.05$，$\\lambda = 0.05$，随机种子 $1$。\n   - 构造方法：从独立的标准正态分布 $X$ 开始，然后对于索引为 $50$、$51$、$52$ 的三个非支持集列，设置 $X_{\\cdot, j} \\leftarrow 0.95 \\cdot X_{\\cdot, 0} + \\sqrt{1 - 0.95^2} \\cdot u_j$，其中 $u_j$ 是一个独立的标准正态向量。这为真实支持集中的一个特征创建了一个高度相关的代理。\n3. 反例（多个强相关项）：\n   - $n = 30$，$p = 80$，$k = 5$，$b = 1.0$，$\\sigma = 0.01$，$\\lambda = 0.2$，随机种子 $2$。\n   - 构造方法：从独立的标准正态分布 $X$ 开始，然后对于索引为 $60$、$61$、$62$、$63$、$64$ 的五个非支持集列，设置：\n     - $X_{\\cdot, 60} \\leftarrow 0.8 \\cdot X_{\\cdot, 0} + 0.8 \\cdot X_{\\cdot, 1} + 0.1 \\cdot u_{60}$，\n     - $X_{\\cdot, 61} \\leftarrow 0.8 \\cdot X_{\\cdot, 1} + 0.8 \\cdot X_{\\cdot, 2} + 0.1 \\cdot u_{61}$，\n     - $X_{\\cdot, 62} \\leftarrow 0.8 \\cdot X_{\\cdot, 2} + 0.8 \\cdot X_{\\cdot, 3} + 0.1 \\cdot u_{62}$，\n     - $X_{\\cdot, 63} \\leftarrow 0.8 \\cdot X_{\\cdot, 3} + 0.8 \\cdot X_{\\cdot, 4} + 0.1 \\cdot u_{63}$，\n     - $X_{\\cdot, 64} \\leftarrow 0.8 \\cdot X_{\\cdot, 0} + 0.8 \\cdot X_{\\cdot, 4} + 0.1 \\cdot u_{64}$，\n     其中每个 $u_j$ 是一个独立的标准正态向量。这产生了多个强相关项，其综合效应挑战了不可表示条件。\n\n对于每个场景，如上构造 $X$ 后，将每列标准化为均值为 $0$ 和欧几里得范数为 $\\sqrt{n}$，将 $y$ 中心化为均值为 $0$，并设置真实支持集 $S = \\{0,1,2,3,4\\}$，其中对于 $j \\in S$，$\\beta_j = b$，否则 $\\beta_j = 0$。使用给定的 $\\sigma$ 抽取 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I_n)$。\n\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果，每个场景贡献一个形如 $[\\mu, c, r]$ 的列表，包含以下组件：\n- $\\mu$：不可表示指数，为一个四舍五入到六位小数的浮点数，\n- $c$：一个布尔值，指示不可表示条件是否成立（如果 $\\mu  1$ 则为真，否则为假），\n- $r$：一个布尔值，指示LASSO是否恢复了精确支持集（如果 $\\operatorname{supp}(\\widehat{\\beta}_{\\lambda}) = S$ 则为真，否则为假）。\n\n例如，最终输出应类似于 `[[\\mu_1,c_1,r_1],[\\mu_2,c_2,r_2],[\\mu_3,c_3,r_3]]`，其中符号由数值替换。此问题不适用任何物理单位或角度单位。所有随机抽样必须在指定的随机种子下执行以保证可复现性。", "solution": "该问题要求推导LASSO估计量实现精确支持集恢复的不可表示条件，并实现一个程序来测试此条件。\n\n### 不可表示条件的推导\n\n我们从线性模型 $y = X \\beta + \\varepsilon$ 开始，其中 $X \\in \\mathbb{R}^{n \\times p}$，$ \\beta \\in \\mathbb{R}^{p}$，$y \\in \\mathbb{R}^{n}$，$\\varepsilon \\in \\mathbb{R}^{n}$ 是一个噪声向量。LASSO估计量 $\\widehat{\\beta}_{\\lambda}$ 最小化以下目标函数：\n$$L(\\beta) = \\frac{1}{2n}\\|y - X\\beta\\|_2^2 + \\lambda \\|\\beta\\|_1$$\n这是一个凸优化问题。一个向量 $\\widehat{\\beta}$ 是解，当且仅当零向量是 $L(\\beta)$ 在 $\\widehat{\\beta}$ 处的次微分的一个元素。$L(\\beta)$ 的次微分由 $\\partial L(\\beta) = \\nabla \\left(\\frac{1}{2n}\\|y - X\\beta\\|_2^2\\right) + \\lambda \\partial \\|\\beta\\|_1$ 给出。\n\n最小二乘项的梯度是 $\\frac{1}{n}X^{\\top}(X\\beta - y)$。$\\ell_1$范数 $\\|\\beta\\|_1 = \\sum_{j=1}^p |\\beta_j|$ 的次微分是向量 $z \\in \\mathbb{R}^p$ 的集合，其中对每个分量 $j$：\n$$\nz_j = \\begin{cases}\n\\operatorname{sgn}(\\beta_j)  \\text{如果 } \\beta_j \\neq 0 \\\\\nv_j \\in [-1, 1]  \\text{如果 } \\beta_j = 0\n\\end{cases}\n$$\n其中 $\\operatorname{sgn}(\\cdot)$ 是符号函数。\n\n因此，$\\widehat{\\beta}_{\\lambda}$ 的Karush-Kuhn-Tucker (KKT) 最优性条件是，存在一个次梯度向量 $z$，使得当 $\\widehat{\\beta}_{\\lambda,j} \\neq 0$ 时 $z_j = \\operatorname{sgn}(\\widehat{\\beta}_{\\lambda,j})$，当 $\\widehat{\\beta}_{\\lambda,j} = 0$ 时 $|z_j| \\le 1$，且满足：\n$$ \\frac{1}{n}X^{\\top}(X\\widehat{\\beta}_{\\lambda} - y) + \\lambda z = 0 \\implies \\frac{1}{n}X^{\\top}(y - X\\widehat{\\beta}_{\\lambda}) = \\lambda z $$\n\n现在，我们假设LASSO实现了精确支持集恢复。令 $S = \\operatorname{supp}(\\beta)$ 为真实支持集，即 $\\beta_j \\neq 0$ 的索引集合。精确支持集恢复的假设意味着 $\\operatorname{supp}(\\widehat{\\beta}_{\\lambda}) = S$。这表明对于 $j \\in S$，$\\widehat{\\beta}_{\\lambda,j} \\neq 0$，而对于 $j \\in S^c$，$\\widehat{\\beta}_{\\lambda,j} = 0$，其中 $S^c$ 是 $S$ 的补集。\n\n我们将设计矩阵 $X$ 划分为 $X_S$ 和 $X_{S^c}$，分别对应于 $S$ 和 $S^c$ 中的列。同样，我们将像 $\\beta$ 这样的向量划分为 $\\beta_S$ 和 $\\beta_{S^c}$。在支持集恢复的假设下，$X\\widehat{\\beta}_{\\lambda} = X_S \\widehat{\\beta}_{\\lambda,S}$，因为 $\\widehat{\\beta}_{\\lambda,S^c} = 0$。\n\nKKT条件现在可以根据划分 $(S, S^c)$ 分为两部分：\n1. 对于索引 $j \\in S$（活动集）：\n    $$ \\frac{1}{n}X_S^{\\top}(y - X_S\\widehat{\\beta}_{\\lambda,S}) = \\lambda \\operatorname{sgn}(\\widehat{\\beta}_{\\lambda,S}) $$\n2. 对于索引 $j \\in S^c$（非活动集）：\n    $$ \\left|\\frac{1}{n}X_{S^c}^{\\top}(y - X_S\\widehat{\\beta}_{\\lambda,S})\\right| \\le \\lambda \\quad (\\text{按元素不等式}) $$\n\n为了使精确恢复有意义，特别是在 $\\lambda \\to 0$ 的渐近意义下，我们需要符号一致性：$\\operatorname{sgn}(\\widehat{\\beta}_{\\lambda,S}) = \\operatorname{sgn}(\\beta_S)$。我们假设这一点成立。第一个KKT条件变为：\n$$ \\frac{1}{n}X_S^{\\top}(y - X_S\\widehat{\\beta}_{\\lambda,S}) = \\lambda \\operatorname{sgn}(\\beta_S) $$\n代入线性模型 $y = X_S\\beta_S + \\varepsilon$（因为 $\\beta_{S^c} = 0$）：\n$$ \\frac{1}{n}X_S^{\\top}(X_S\\beta_S + \\varepsilon - X_S\\widehat{\\beta}_{\\lambda,S}) = \\lambda \\operatorname{sgn}(\\beta_S) $$\n使用样本Gram矩阵定义 $G = X^{\\top}X/n$，其子矩阵为 $G_{S,S} = X_S^{\\top}X_S/n$。方程简化为：\n$$ G_{S,S}(\\beta_S - \\widehat{\\beta}_{\\lambda,S}) + \\frac{1}{n}X_S^{\\top}\\varepsilon = \\lambda \\operatorname{sgn}(\\beta_S) $$\n假设 $G_{S,S}$ 是可逆的，我们可以表示出差值 $(\\beta_S - \\widehat{\\beta}_{\\lambda,S})$：\n$$ \\beta_S - \\widehat{\\beta}_{\\lambda,S} = G_{S,S}^{-1} \\left( \\lambda \\operatorname{sgn}(\\beta_S) - \\frac{1}{n}X_S^{\\top}\\varepsilon \\right) $$\n\n现在我们分析非活动集 $S^c$ 的第二个KKT条件。我们代入残差 $y - X_S\\widehat{\\beta}_{\\lambda,S}$ 的表达式：\n$$ y - X_S\\widehat{\\beta}_{\\lambda,S} = (X_S\\beta_S + \\varepsilon) - X_S\\widehat{\\beta}_{\\lambda,S} = X_S(\\beta_S - \\widehat{\\beta}_{\\lambda,S}) + \\varepsilon $$\n代入 $(\\beta_S - \\widehat{\\beta}_{\\lambda,S})$ 的表达式：\n$$ y - X_S\\widehat{\\beta}_{\\lambda,S} = X_S G_{S,S}^{-1} \\left( \\lambda \\operatorname{sgn}(\\beta_S) - \\frac{1}{n}X_S^{\\top}\\varepsilon \\right) + \\varepsilon $$\n现在，我们将其代入非活动集的KKT条件中：\n$$ \\left| \\frac{1}{n}X_{S^c}^{\\top} \\left( X_S G_{S,S}^{-1} \\lambda \\operatorname{sgn}(\\beta_S) - X_S G_{S,S}^{-1} \\frac{1}{n}X_S^{\\top}\\varepsilon + \\varepsilon \\right) \\right| \\le \\lambda $$\n使用 $G_{S^c,S} = X_{S^c}^{\\top}X_S/n$ 并整理各项：\n$$ \\left| \\lambda G_{S^c,S} G_{S,S}^{-1} \\operatorname{sgn}(\\beta_S) + \\frac{1}{n}(X_{S^c}^{\\top} - X_{S^c}^{\\top}X_S G_{S,S}^{-1} \\frac{1}{n}X_S^{\\top})\\varepsilon \\right| \\le \\lambda $$\n两边除以 $\\lambda  0$：\n$$ \\left| G_{S^c,S} G_{S,S}^{-1} \\operatorname{sgn}(\\beta_S) + \\frac{1}{\\lambda} \\cdot (\\text{噪声项}) \\right| \\le 1 $$\n为了使这个不等式成立，特别是在一个小的 $\\lambda$ 范围内，此时噪声项可能不可忽略，表达式的确定性部分必须严格地小于 $1$。如果向量 $G_{S^c,S} G_{S,S}^{-1} \\operatorname{sgn}(\\beta_S)$ 的任何元素的幅度等于或大于 $1$，那么少量的噪声 $\\varepsilon$ 很容易导致不等式被违反。因此，稳健支持集恢复的一个必要条件是**不可表示条件**：\n$$ \\left\\| G_{S^c,S} G_{S,S}^{-1} \\operatorname{sgn}(\\beta_S) \\right\\|_{\\infty}   1 $$\n量 $\\mu = \\left\\| G_{S^c,S} G_{S,S}^{-1} \\operatorname{sgn}(\\beta_S) \\right\\|_{\\infty}$ 是不可表示指数。\n\n### 特征相关性和 $p \\gg n$ 情境的作用\n\n不可表示条件突出了特征相关性的关键作用。\n- $G_{S,S} = X_S^{\\top}X_S/n$ 代表真实预测变量*之间*的相关性。如果这些预测变量高度相关（多重共线性），$G_{S,S}$ 会是病态的，其逆矩阵 $G_{S,S}^{-1}$ 将有很大的元素。\n- $G_{S^c,S} = X_{S^c}^{\\top}X_S/n$ 代表真实预测变量（在 $S$ 中）与不相关的“噪声”预测变量（在 $S^c$ 中）*之间*的相关性。\n\n乘积 $G_{S^c,S} G_{S,S}^{-1}$ 可以解释为将每个噪声预测变量 $X_j$（$j \\in S^c$）对真实预测变量集合 $X_S$ 进行回归时的回归系数矩阵。如果一个噪声预测变量 $X_j$ 可以被真实预测变量的线性组合很好地近似，则 $G_{S^c,S} G_{S,S}^{-1}$ 中相应的行将有很大的元素。当这个线性组合与真实系数 $\\beta_S$ 的符号一致时，不可表示指数 $\\mu$ 就可能超过 $1$。\n\n在这种情况下，LASSO无法区分真实的稀疏模型和一个包含高度相关的噪声预测变量的模型。“不可表示”这个术语意味着真实预测变量不能被噪声预测变量“表示”。\n\n在小样本、大维度（$p \\gg n$）的情境下，有几个因素会加剧这个问题：\n1.  **高维度**：由于有大量的噪声预测变量（$p-k \\gg n$），在 $S^c$ 中找到一个或多个与 $S$ 中变量存在伪高度相关的预测变量的概率急剧增加。\n2.  **病态条件**：当真实预测变量的数量 $k$ 接近样本量 $n$ 时，矩阵 $G_{S,S}$ 变得病态或奇异，导致 $\\|G_{S,S}^{-1}\\|$ 爆炸。这会放大活动集和非活动集之间任何已有的相关性。\n3.  **伪相关性**：当 $n$ 很小时，样本Gram矩阵 $G$ 是总体协方差的一个有噪声的估计。随机性可能会产生在底层数据生成过程中不存在的强样本相关性，从而导致违反该条件。\n\n这些因素的结合使得在 $p \\gg n$ 的情境下使用LASSO进行精确支持集恢复变得非常具有挑战性。不可表示条件为这一挑战提供了精确的数学表述，表明成功与否关键取决于设计矩阵的相关结构。", "answer": "```python\nimport numpy as np\nfrom scipy import linalg\n\ndef _generate_data(n, p, k, b, sigma, seed, construction):\n    \"\"\"Generates synthetic data for a single LASSO scenario.\"\"\"\n    rng = np.random.default_rng(seed)\n    # Start with an independent standard normal design matrix\n    X = rng.standard_normal((n, p))\n    \n    # Apply specific correlation structures based on the scenario\n    if construction == 'single_correlate':\n        # Create non-support features highly correlated with a single support feature\n        for j in [50, 51, 52]:\n            u_j = rng.standard_normal(n)\n            X[:, j] = 0.95 * X[:, 0] + np.sqrt(1.0 - 0.95**2) * u_j\n    elif construction == 'multiple_correlates':\n        # Create non-support features that are linear combinations of multiple support features\n        correlates_def = {\n            60: [0, 1], 61: [1, 2], 62: [2, 3], 63: [3, 4], 64: [0, 4]\n        }\n        for j, support_indices in correlates_def.items():\n            u_j = rng.standard_normal(n)\n            # Coefficients are chosen to likely violate the irrepresentable condition\n            linear_combo = 0.8 * X[:, support_indices[0]] + 0.8 * X[:, support_indices[1]]\n            X[:, j] = linear_combo + 0.1 * u_j\n            \n    # Standardize columns of X: mean 0, Euclidean norm sqrt(n)\n    X -= X.mean(axis=0)\n    col_norms = np.linalg.norm(X, axis=0)\n    # Avoid division by zero for columns that might be all zero by adding a small epsilon.\n    X = X / (col_norms + 1e-9) * np.sqrt(n)\n    \n    # Define true coefficient vector beta and support S\n    beta_true = np.zeros(p)\n    beta_true[:k] = b\n    S = set(range(k))\n    \n    # Generate response vector y\n    epsilon = rng.standard_normal(n) * sigma\n    y = X @ beta_true + epsilon\n    \n    # Center y\n    y -= y.mean()\n    \n    return X, y, beta_true, S\n\ndef _calculate_mu(X, S, beta_true, n, p):\n    \"\"\"Calculates the irrepresentable index mu.\"\"\"\n    # Compute the sample Gram matrix\n    G = (X.T @ X) / n\n    \n    # Partition the Gram matrix according to the support S and its complement Sc\n    S_list = sorted(list(S))\n    Sc_list = sorted(list(set(range(p)) - S))\n    \n    G_Sc_S = G[np.ix_(Sc_list, S_list)]\n    G_S_S = G[np.ix_(S_list, S_list)]\n    \n    # Compute the inverse of G_S_S using the Moore-Penrose pseudoinverse for stability\n    # The check_finite=False argument is needed for some edge cases with older scipy versions.\n    G_S_S_inv = linalg.pinv(G_S_S, check_finite=False)\n\n    # Get the sign vector of the true coefficients on the support\n    sgn_beta_S = np.sign(beta_true[S_list])\n    \n    # Compute the vector whose max absolute value is the irrepresentable index\n    v = G_Sc_S @ G_S_S_inv @ sgn_beta_S\n    \n    # The irrepresentable index mu is the infinity norm of this vector\n    mu = np.max(np.abs(v))\n    \n    return mu\n\ndef _lasso_cd(X, y, lambda_, max_iter=1000, tol=1e-7):\n    \"\"\"Fits the LASSO estimator using coordinate descent.\"\"\"\n    n, p = X.shape\n    beta = np.zeros(p)\n    \n    # Coordinate descent with efficient residual updates.\n    # Because X columns are normalized to have squared L2-norm of n,\n    # the updates simplify nicely.\n    residual = y.copy()\n\n    for _ in range(max_iter):\n        max_change = 0.0\n        for j in range(p):\n            beta_j_old = beta[j]\n            \n            # The argument for the soft-thresholding operator is a_j.\n            # a_j = (Xj.T @ (y - X@beta_except_j)) / n\n            # This can be efficiently computed as:\n            # a_j = (Xj.T @ current_residual)/n + (Xj.T @ Xj / n) * beta_j_old\n            # Since Xj.T @ Xj / n = 1 in our setup\n            a_j = (X[:, j] @ residual) / n + beta_j_old\n            \n            # Apply the soft-thresholding operator S_lambda(a_j)\n            beta[j] = np.sign(a_j) * max(abs(a_j) - lambda_, 0.0)\n            \n            delta_beta_j = beta[j] - beta_j_old\n            if delta_beta_j != 0.0:\n                residual -= X[:, j] * delta_beta_j\n            \n            if abs(delta_beta_j)  max_change:\n                max_change = abs(delta_beta_j)\n        \n        if max_change   tol:\n            break\n            \n    return beta\n\ndef _check_recovery(beta_hat, S, tol=1e-6):\n    \"\"\"Checks if the estimated support equals the true support.\"\"\"\n    S_hat = {i for i, b in enumerate(beta_hat) if abs(b)  tol}\n    return S_hat == S\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Scenario 1: Weak correlations, recovery expected.\n        {'n': 30, 'p': 80, 'k': 5, 'b': 1.0, 'sigma': 0.01, 'lambda_': 0.2, 'seed': 0, 'construction': 'independent'},\n        # Scenario 2: Strong correlation, recovery challenging.\n        {'n': 30, 'p': 80, 'k': 5, 'b': 1.0, 'sigma': 0.05, 'lambda_': 0.05, 'seed': 1, 'construction': 'single_correlate'},\n        # Scenario 3: Multiple strong correlates, recovery expected to fail.\n        {'n': 30, 'p': 80, 'k': 5, 'b': 1.0, 'sigma': 0.01, 'lambda_': 0.2, 'seed': 2, 'construction': 'multiple_correlates'},\n    ]\n\n    results = []\n    for params in test_cases:\n        # Unpack parameters\n        n, p, k, b = params['n'], params['p'], params['k'], params['b']\n        sigma, lambda_ = params['sigma'], params['lambda_']\n        seed, construction = params['seed'], params['construction']\n        \n        # 1. Generate data\n        X, y, beta_true, S = _generate_data(n, p, k, b, sigma, seed, construction)\n        \n        # 2. Compute irrepresentable index and condition\n        mu = _calculate_mu(X, S, beta_true, n, p)\n        irrep_cond_holds = mu   1.0\n        \n        # 3. Fit LASSO model using coordinate descent\n        beta_hat = _lasso_cd(X, y, lambda_)\n\n        # 4. Check for exact support recovery\n        recovery_succeeded = _check_recovery(beta_hat, S)\n        \n        results.append([mu, irrep_cond_holds, recovery_succeeded])\n\n    # Format the final output string as specified\n    formatted_results = [f\"[{res[0]:.6f},{str(res[1])},{str(res[2])}]\" for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3186678"}, {"introduction": "虽然像 LASSO 这样的方法在线性模型中表现出色，但许多现实世界的关系本质上是非线性的。高斯过程（Gaussian Processes, GPs）为此类问题提供了一个灵活的非参数框架。在这个练习中，您将实现一个带有自动相关性判定（Automatic Relevance Determination, ARD）核函数的高斯过程模型。通过最大化边际似然，该模型能够自动“忽略”不相关的维度，为高维非线性场景下的特征选择提供了一种强大的贝叶斯方法 `[@problem_id:3186634]`。", "problem": "考虑统计学习中的小样本高维场景，其中数据点数量 $n$ 远小于特征数量 $p$。在这种情况下，一种在执行特征相关性分析的同时对非线性关系进行建模的常用方法是使用带有自动相关性确定（Automatic Relevance Determination, ARD）核的高斯过程（Gaussian Process, GP）回归模型，该核通常实例化为径向基函数（Radial Basis Function, RBF）核，并为每个特征维度设置一个独立的长度尺度（lengthscale）。\n\n您需要实现一个完整、可运行的程序，该程序能够：\n- 构建合成数据集，其中 $p \\gg n$，并且只有一个已知的特征维度子集与响应变量相关，其余维度则不相关。\n- 通过最大化关于超参数的边缘似然，来拟合一个带有 ARD RBF 核的高斯过程（GP）。\n- 使用学习到的 ARD 长度尺度对特征相关性进行排序，并评估正确识别出的真实相关特征的数量。\n\n理论基础：\n- 高斯过程（GP）回归假设一个先验 $f \\sim \\mathcal{GP}(0, k(\\cdot, \\cdot))$，其中 $k(\\cdot, \\cdot)$ 是一个正定核函数。\n- 对于训练输入 $\\mathbf{X} \\in \\mathbb{R}^{n \\times p}$ 和目标 $\\mathbf{y} \\in \\mathbb{R}^{n}$，在高斯过程下，给定协方差矩阵 $\\mathbf{K}$ 的边缘对数似然为\n$$\n\\log p(\\mathbf{y} \\mid \\theta) = -\\frac{1}{2} \\mathbf{y}^\\top \\mathbf{K}^{-1} \\mathbf{y} - \\frac{1}{2} \\log\\det(\\mathbf{K}) - \\frac{n}{2}\\log(2\\pi),\n$$\n其中 $\\theta$ 表示超参数。这个公式是高斯过程回归中一个经过充分检验的事实。\n- 具有每个维度长度尺度 $l_1, \\ldots, l_p$ 和信号标准差 $\\sigma_f$ 的 ARD RBF 核定义为\n$$\nk_{\\text{ARD}}(\\mathbf{x}, \\mathbf{x}') = \\sigma_f^2 \\exp\\left(-\\frac{1}{2}\\sum_{j=1}^p \\frac{(x_j - x_j')^2}{l_j^2}\\right).\n$$\n- 观测噪声被建模为独立同分布（i.i.d.）的高斯噪声，其标准差为 $\\sigma_n$，这会在协方差矩阵上增加一个 $\\sigma_n^2 \\mathbf{I}$ 项。\n\n程序要求：\n1.  基于上述理论基础，推导出一个有原则的算法，通过最大化 GP 边缘似然来学习对数超参数 $\\{\\log l_j\\}_{j=1}^p$、$\\log \\sigma_f$ 和 $\\log \\sigma_n$。在您的解决方案中，推导过程必须从边缘对数似然出发，并使用矩阵微积分来获得可用于优化的梯度。不要使用未从该基础推导出的简化公式。\n2.  为小样本 $n$ 大维度 $p$ 的场景高效地实现该算法。您的实现必须：\n    - 构建每个维度的 ARD 配对平方距离贡献，并组装核矩阵。\n    - 使用数值稳定的线性代数，包括使用 Cholesky 分解来求解线性系统和计算 $\\log\\det(\\mathbf{K})$。\n    - 在 $\\mathbf{K}$ 的对角线上添加一个小的抖动项（jitter term）以确保其正定性，这是一种科学上符合实际的做法。\n    - 使用基于梯度的优化方法，并设置适当的边界，以确保对数超参数在取指数后为正值。\n3.  拟合后，计算学习到的长度尺度 $\\{l_j\\}$，通过逆长度尺度 $s_j = 1/l_j$ 对特征进行排序（越大的 $s_j$ 表示相关性越强），并选择前 $k$ 个特征，其中 $k$ 等于测试用例中真实相关特征的数量。计算真阳性（即在前 $k$ 个特征中正确识别出的相关维度的数量）。\n\n测试套件：\n实现四个测试用例以评估解决方案的不同方面。对于每个测试用例，输入和数据生成必须如下：\n\n- 所有用例的通用数据生成方法：\n  - 生成 $\\mathbf{X} \\in \\mathbb{R}^{n \\times p}$，其中元素 $x_{ij} \\sim \\mathcal{N}(0, 1)$ 独立同分布。\n  - 令 $\\mathcal{R} \\subset \\{0,1,\\ldots,p-1\\}$ 为相关特征的索引集合，具体由每个测试用例指定。\n  - 使用以下非线性函数生成目标 $\\mathbf{y}$：\n  $$\n  y_i = \\sum_{j \\in \\mathcal{R}} \\left[\\sin\\left(x_{ij}\\right) + 0.5\\, x_{ij}\\right] + \\epsilon_i,\n  $$\n  其中 $\\epsilon_i \\sim \\mathcal{N}(0, \\sigma_{\\text{noise}}^2)$ 独立同分布。通过减去其经验均值来中心化目标。\n  - 定义 $k = |\\mathcal{R}|$。\n\n- 每个测试的具体参数值：\n  1.  理想情况：$n = 25$，$p = 40$，$\\mathcal{R} = \\{2,5,11\\}$，$\\sigma_{\\text{noise}} = 0.05$，随机种子为 $0$。\n  2.  单一相关维度：$n = 12$，$p = 50$，$\\mathcal{R} = \\{0\\}$，$\\sigma_{\\text{noise}} = 0.10$，随机种子为 $1$。\n  3.  极端小样本高维：$n = 8$，$p = 60$，$\\mathcal{R} = \\{3,7\\}$，$\\sigma_{\\text{noise}} = 0.10$，随机种子为 $2$。\n  4.  所有维度均不相关（边界情况）：$n = 20$，$p = 30$，$\\mathcal{R} = \\varnothing$（空集），$\\sigma_{\\text{noise}} = 0.15$，随机种子为 $3$。\n\n最终输出规范：\n- 对于每个测试用例，计算按逆长度尺度 $s_j = 1/l_j$ 排名的前 $k$ 个特征中，正确识别出的相关特征的整数数量。\n- 您的程序应生成单行输出，包含四个测试用例的结果，格式为逗号分隔的列表并用方括号括起，例如 `[r_1,r_2,r_3,r_4]`，其中每个 $r_i$ 是对应于第 $i$ 个测试用例的整数。", "solution": "该问题要求在一个小样本 $n$ 大维度 $p$ 的场景下，实现一个带有自动相关性确定（ARD）径向基函数（RBF）核的高斯过程（GP）回归模型，以执行特征相关性排序。任务的核心是推导并实现一个通过最大化边缘对数似然来学习核超参数的算法。\n\n### 问题验证\n对问题陈述进行解析和验证。\n- **给定条件**：问题提供了 GP 的数学定义、边缘对数似然函数、ARD RBF 核、观测噪声模型、一个精确的数据生成过程，以及包含所有必要参数的一组四个具体测试用例。\n- **验证**：该问题在统计学习中成熟的高斯过程理论上具有科学依据。它是一个定义明确的问题，具有清晰的目标和一套完整、无矛盾的指令。术语精确且客观。该任务并非微不足道，需要使用矩阵微积分进行理论推导和实现一个数值稳定的程序。\n- **结论**：问题有效。\n\n### 算法推导\n\n目标是找到最大化观测数据边缘对数似然的超参数。我们处理负边缘对数似然（NLL），并旨在最小化它。NLL 由下式给出：\n$$\n\\text{NLL}(\\theta) = \\frac{1}{2} \\mathbf{y}^\\top \\mathbf{K}_y^{-1} \\mathbf{y} + \\frac{1}{2} \\log\\det(\\mathbf{K}_y) + \\frac{n}{2}\\log(2\\pi)\n$$\n其中 $\\mathbf{y} \\in \\mathbb{R}^n$ 是中心化后的训练目标，$n$ 是数据点的数量，$\\mathbf{K}_y$ 是带噪声观测的 $n \\times n$ 协方差矩阵。它定义为：\n$$\n\\mathbf{K}_y = \\mathbf{K}_{\\text{ARD}} + \\sigma_n^2 \\mathbf{I}\n$$\n这里，$\\mathbf{K}_{\\text{ARD}}$ 是由 ARD RBF 核生成的核矩阵，$\\sigma_n$ 是观测噪声的标准差，$\\mathbf{I}$ 是单位矩阵。$\\mathbf{K}_{\\text{ARD}}$ 的元素由下式给出：\n$$\n[\\mathbf{K}_{\\text{ARD}}]_{ik} = k_{\\text{ARD}}(\\mathbf{x}_i, \\mathbf{x}_k) = \\sigma_f^2 \\exp\\left(-\\frac{1}{2}\\sum_{j=1}^p \\frac{(x_{ij} - x_{kj})^2}{l_j^2}\\right)\n$$\n需要优化的超参数集合是 $\\theta' = (\\sigma_f, l_1, \\ldots, l_p, \\sigma_n)$。为确保这些参数保持正值，我们优化它们的对数：$\\theta = (\\zeta_f, \\lambda_1, \\ldots, \\lambda_p, \\zeta_n)$，其中 $\\zeta_f = \\log \\sigma_f$，$\\lambda_j = \\log l_j$，$\\zeta_n = \\log \\sigma_n$。\n\n我们使用基于梯度的优化方法，这需要 NLL 关于每个对数超参数的偏导数。令 $\\phi$ 为 $\\theta$ 中的任意一个对数超参数。NLL 的导数为：\n$$\n\\frac{\\partial \\text{NLL}}{\\partial \\phi} = \\frac{1}{2} \\mathbf{y}^\\top \\frac{\\partial \\mathbf{K}_y^{-1}}{\\partial \\phi} \\mathbf{y} + \\frac{1}{2} \\frac{\\partial \\log\\det(\\mathbf{K}_y)}{\\partial \\phi}\n$$\n使用矩阵微积分恒等式 $\\frac{\\partial \\mathbf{A}^{-1}}{\\partial \\phi} = -\\mathbf{A}^{-1}\\frac{\\partial \\mathbf{A}}{\\partial \\phi}\\mathbf{A}^{-1}$ 和 $\\frac{\\partial \\log\\det \\mathbf{A}}{\\partial \\phi} = \\text{tr}\\left(\\mathbf{A}^{-1}\\frac{\\partial \\mathbf{A}}{\\partial \\phi}\\right)$，我们得到：\n$$\n\\frac{\\partial \\text{NLL}}{\\partial \\phi} = -\\frac{1}{2} \\mathbf{y}^\\top \\mathbf{K}_y^{-1}\\frac{\\partial \\mathbf{K}_y}{\\partial \\phi}\\mathbf{K}_y^{-1} \\mathbf{y} + \\frac{1}{2} \\text{tr}\\left(\\mathbf{K}_y^{-1}\\frac{\\partial \\mathbf{K}_y}{\\partial \\phi}\\right)\n$$\n让我们定义 $\\boldsymbol{\\alpha} = \\mathbf{K}_y^{-1}\\mathbf{y}$。表达式简化为：\n$$\n\\frac{\\partial \\text{NLL}}{\\partial \\phi} = -\\frac{1}{2} \\boldsymbol{\\alpha}^\\top \\frac{\\partial \\mathbf{K}_y}{\\partial \\phi} \\boldsymbol{\\alpha} + \\frac{1}{2} \\text{tr}\\left(\\mathbf{K}_y^{-1}\\frac{\\partial \\mathbf{K}_y}{\\partial \\phi}\\right)\n$$\n使用迹恒等式 $\\mathbf{u}^\\top \\mathbf{M} \\mathbf{u} = \\text{tr}(\\mathbf{M}\\mathbf{u}\\mathbf{u}^\\top)$，这可以紧凑地写为：\n$$\n\\frac{\\partial \\text{NLL}}{\\partial \\phi} = -\\frac{1}{2} \\text{tr}\\left(\\left(\\boldsymbol{\\alpha}\\boldsymbol{\\alpha}^\\top - \\mathbf{K}_y^{-1}\\right)\\frac{\\partial \\mathbf{K}_y}{\\partial \\phi}\\right)\n$$\n接下来，我们推导 $\\mathbf{K}_y$ 关于每个对数超参数的偏导数。\n\n1.  **关于对数信号标准差 $\\zeta_f = \\log \\sigma_f$ 的导数**：\n    我们有 $\\sigma_f^2 = e^{2\\zeta_f}$。核矩阵为 $\\mathbf{K}_{\\text{ARD}} = e^{2\\zeta_f} \\mathbf{K}_{\\text{base}}$，其中 $[\\mathbf{K}_{\\text{base}}]_{ik} = \\exp(-\\frac{1}{2}\\sum_j (x_{ij}-x_{kj})^2/l_j^2)$。\n    $$\n    \\frac{\\partial \\mathbf{K}_y}{\\partial \\zeta_f} = \\frac{\\partial \\mathbf{K}_{\\text{ARD}}}{\\partial \\zeta_f} = \\frac{\\partial (e^{2\\zeta_f})}{\\partial \\zeta_f} \\mathbf{K}_{\\text{base}} = 2e^{2\\zeta_f} \\mathbf{K}_{\\text{base}} = 2\\mathbf{K}_{\\text{ARD}}\n    $$\n    梯度为：$\\frac{\\partial \\text{NLL}}{\\partial \\zeta_f} = -\\text{tr}\\left(\\left(\\boldsymbol{\\alpha}\\boldsymbol{\\alpha}^\\top - \\mathbf{K}_y^{-1}\\right)\\mathbf{K}_{\\text{ARD}}\\right)$。\n\n2.  **关于对数噪声标准差 $\\zeta_n = \\log \\sigma_n$ 的导数**：\n    噪声项为 $\\sigma_n^2 \\mathbf{I} = e^{2\\zeta_n} \\mathbf{I}$。\n    $$\n    \\frac{\\partial \\mathbf{K}_y}{\\partial \\zeta_n} = \\frac{\\partial (e^{2\\zeta_n}\\mathbf{I})}{\\partial \\zeta_n} = 2 e^{2\\zeta_n}\\mathbf{I} = 2\\sigma_n^2\\mathbf{I}\n    $$\n    梯度为：$\\frac{\\partial \\text{NLL}}{\\partial \\zeta_n} = -\\text{tr}\\left(\\left(\\boldsymbol{\\alpha}\\boldsymbol{\\alpha}^\\top - \\mathbf{K}_y^{-1}\\right)\\sigma_n^2\\mathbf{I}\\right) = -\\sigma_n^2 \\text{tr}\\left(\\boldsymbol{\\alpha}\\boldsymbol{\\alpha}^\\top - \\mathbf{K}_y^{-1}\\right)$。\n\n3.  **关于对数长度尺度 $\\lambda_j = \\log l_j$ 的导数**：\n    核函数依赖于 $l_j^{-2} = e^{-2\\lambda_j}$。令 $\\mathbf{D}_j$ 为第 $j$ 个特征的配对平方距离矩阵，即 $[\\mathbf{D}_j]_{ik} = (x_{ij} - x_{kj})^2$。\n    $$\n    \\frac{\\partial \\mathbf{K}_{\\text{ARD}}}{\\partial \\lambda_j} = \\mathbf{K}_{\\text{ARD}} \\circ \\frac{\\partial}{\\partial \\lambda_j} \\left(-\\frac{1}{2} \\sum_{m=1}^p \\frac{\\mathbf{D}_m}{l_m^2}\\right) = \\mathbf{K}_{\\text{ARD}} \\circ \\left(-\\frac{1}{2} \\mathbf{D}_j(-2e^{-2\\lambda_j})\\right) = \\mathbf{K}_{\\text{ARD}} \\circ \\frac{\\mathbf{D}_j}{l_j^2}\n    $$\n    其中 $\\circ$ 表示逐元素的哈达玛积（Hadamard product）。梯度为：\n    $$\n    \\frac{\\partial \\text{NLL}}{\\partial \\lambda_j} = -\\frac{1}{2} \\text{tr}\\left(\\left(\\boldsymbol{\\alpha}\\boldsymbol{\\alpha}^\\top - \\mathbf{K}_y^{-1}\\right) \\left(\\mathbf{K}_{\\text{ARD}} \\circ \\frac{\\mathbf{D}_j}{l_j^2}\\right)\\right)\n    $$\n\n### 实现策略\n\n-   **数值稳定性**：为了计算 NLL 及其梯度，我们必须评估涉及 $\\mathbf{K}_y^{-1}$ 和 $\\log\\det(\\mathbf{K}_y)$ 的项。我们通过使用 $\\mathbf{K}_y$ 的 Cholesky 分解 $\\mathbf{K}_y = \\mathbf{L}\\mathbf{L}^\\top$ 来避免显式矩阵求逆。在 $\\mathbf{K}_y$ 的对角线上添加一个小的抖动项（$10^{-8}$）以确保其正定性。\n    -   $\\log\\det(\\mathbf{K}_y) = 2 \\sum_{i} \\log([\\mathbf{L}]_{ii})$。\n    -   $\\boldsymbol{\\alpha} = \\mathbf{K}_y^{-1}\\mathbf{y}$ 通过 `cho_solve` 高效求解。\n    -   $\\mathbf{K}_y^{-1}$ 也通过求解 $\\mathbf{K}_y \\mathbf{X} = \\mathbf{I}$ 的 `cho_solve` 来计算。\n-   **效率**：计算一个梯度向量涉及 $p+2$ 个偏导数。矩阵 $\\boldsymbol{\\alpha}\\boldsymbol{\\alpha}^\\top - \\mathbf{K}_y^{-1}$ 只计算一次。主要成本是遍历 $p$ 个特征维度来计算每个 $\\lambda_j$ 的梯度。对于对称矩阵，迹 $\\text{tr}(\\mathbf{A}\\mathbf{B})$ 可以计算为 `np.sum(A * B.T)`，即 `np.sum(A * B)`。因此，每个梯度分量的计算复杂度为 $O(n^2)$，导致总梯度计算成本为 $O(pn^2)$，这在 $n \\ll p$ 的场景下是高效的。\n-   **优化**：使用 `scipy.optimize.minimize` 函数的 `L-BFGS-B` 方法来最小化 NLL。该方法可以利用计算出的梯度，并允许对超参数设置边界以防止数值问题。\n-   **特征排序**：优化后，提取学习到的长度尺度 $l_j$。每个特征 $j$ 的相关性得分定义为 $s_j = 1/l_j$。一个小的 $l_j$ 意味着核函数对特征 $j$ 在短距离上的变化很敏感，表明相关性高。因此，大的得分 $s_j$ 对应于高的相关性。特征按其得分降序排列。", "answer": "```python\nimport numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.linalg import cholesky, cho_solve\nfrom scipy.spatial.distance import pdist, squareform\n\ndef solve():\n    \"\"\"\n    Executes the full pipeline for all test cases as specified in the problem.\n    \"\"\"\n\n    test_cases = [\n        # (n, p, relevant_features, noise_std, seed)\n        (25, 40, {2, 5, 11}, 0.05, 0),\n        (12, 50, {0}, 0.10, 1),\n        (8, 60, {3, 7}, 0.10, 2),\n        (20, 30, set(), 0.15, 3),\n    ]\n\n    results = []\n    for n, p, R_indices, sigma_noise, seed in test_cases:\n        # 1. Generate synthetic data\n        X, y = generate_data(n, p, R_indices, sigma_noise, seed)\n        k = len(R_indices)\n\n        # 2. Fit GP model\n        # The state vector for optimization: [log(sf), log(sn), log(l_1), ..., log(l_p)]\n        # Initial guess for hyperparameters\n        initial_log_hyperparams = np.concatenate([\n            [np.log(1.0)],          # log(sigma_f)\n            [np.log(0.1)],          # log(sigma_n)\n            np.zeros(p)             # log(l_j) for j=1..p\n        ])\n\n        # Bounds for hyperparameters (on log scale) to prevent numerical issues\n        # log(1e-5) approx -11.5, log(1e5) approx 11.5\n        bounds = [(-10, 10)] * 2 + [(-10, 10)] * p\n\n        # Optimization\n        res = minimize(\n            objective_and_grad,\n            initial_log_hyperparams,\n            args=(X, y),\n            method='L-BFGS-B',\n            jac=True,\n            bounds=bounds\n        )\n\n        # 3. Evaluate feature relevance\n        learned_log_hyperparams = res.x\n        log_l_vec = learned_log_hyperparams[2:]\n        l_vec = np.exp(log_l_vec)\n\n        # Relevance scores are inverse lengthscales\n        relevance_scores = 1.0 / l_vec\n        \n        # Get indices of top k features\n        if k > 0:\n            ranked_indices = np.argsort(relevance_scores)[::-1]\n            top_k_indices = set(ranked_indices[:k])\n            # Count true positives\n            true_positives = len(top_k_indices.intersection(R_indices))\n        else: # Case where there are no relevant features\n            true_positives = 0\n            \n        results.append(true_positives)\n        \n    print(f\"[{','.join(map(str, results))}]\")\n\ndef generate_data(n, p, R_indices, sigma_noise, seed):\n    \"\"\"\n    Generates synthetic dataset for a given configuration.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    X = rng.normal(0, 1, size=(n, p))\n    \n    y = np.zeros(n)\n    if R_indices:\n        for j in R_indices:\n            y += np.sin(X[:, j]) + 0.5 * X[:, j]\n\n    noise = rng.normal(0, sigma_noise, size=n)\n    y += noise\n    \n    # Center the target variable\n    y -= np.mean(y)\n    \n    return X, y\n\ndef objective_and_grad(log_hyperparams, X, y):\n    \"\"\"\n    Computes the negative log-likelihood and its gradient for GP regression.\n    \"\"\"\n    n, p = X.shape\n    jitter = 1e-8\n\n    # Unpack hyperparameters\n    log_sf = log_hyperparams[0]\n    log_sn = log_hyperparams[1]\n    log_l_vec = log_hyperparams[2:]\n\n    sf2 = np.exp(2 * log_sf)\n    sn2 = np.exp(2 * log_sn)\n    l_vec = np.exp(log_l_vec)\n\n    # --- Construct Kernel Matrix K_y ---\n    # Using scipy.spatial.distance for efficiency\n    # scaled_sq_dists is the matrix with elements D_ik = sum_j (x_ij - x_kj)^2 / l_j^2\n    X_scaled = X / l_vec\n    scaled_sq_dists = squareform(pdist(X_scaled, 'sqeuclidean'))\n    \n    K = sf2 * np.exp(-0.5 * scaled_sq_dists)\n    Ky = K + (sn2 + jitter) * np.eye(n)\n\n    # --- Compute Negative Log-Likelihood (NLL) ---\n    try:\n        L = cholesky(Ky, lower=True)\n    except np.linalg.LinAlgError:\n        # If matrix is not positive definite, return a large value\n        return 1e9, np.zeros_like(log_hyperparams)\n\n    # Solve K_y * alpha = y using cholesky factor\n    alpha = cho_solve((L, True), y)\n    \n    log_det_Ky = 2 * np.sum(np.log(np.diag(L)))\n    \n    nll = 0.5 * y.dot(alpha) + 0.5 * log_det_Ky + 0.5 * n * np.log(2 * np.pi)\n\n    # --- Compute Gradients of NLL ---\n    # Common term in gradients: (alpha * alpha^T - K_y^-1)\n    Ky_inv = cho_solve((L, True), np.eye(n))\n    A = np.outer(alpha, alpha) - Ky_inv\n\n    # Gradient w.r.t. log(sigma_f)\n    dK_d_log_sf = 2 * K\n    grad_log_sf = -0.5 * np.sum(A * dK_d_log_sf)\n\n    # Gradient w.r.t. log(sigma_n)\n    dK_d_log_sn = 2 * sn2 * np.eye(n)\n    grad_log_sn = -0.5 * np.sum(A * dK_d_log_sn)\n    \n    # Gradients w.r.t. log(l_j) for j=1...p\n    grad_log_l = np.zeros(p)\n    term_A_K = A * K\n    for j in range(p):\n        # Pairwise squared distances for dimension j\n        sq_dist_j = squareform(pdist(X[:, j:j+1], 'sqeuclidean'))\n        \n        # Derivative of K w.r.t log(l_j) is K o (D_j / l_j^2)\n        dK_d_log_lj = K * (sq_dist_j / (l_vec[j]**2))\n        grad_log_l[j] = -0.5 * np.sum(A * dK_d_log_lj)\n\n    gradients = np.concatenate([[grad_log_sf], [grad_log_sn], grad_log_l])\n    \n    return nll, gradients\n\nsolve()\n```", "id": "3186634"}]}