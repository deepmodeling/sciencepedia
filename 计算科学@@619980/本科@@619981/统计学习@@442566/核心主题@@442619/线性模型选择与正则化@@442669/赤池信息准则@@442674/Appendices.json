{"hands_on_practices": [{"introduction": "在可靠性工程中，我们常常需要对产品或系统的失效时间进行建模。一个常见的抉择是：我们应该使用简单的指数分布模型，还是选择一个更灵活但更复杂的威布尔分布模型？赤池信息准则 (AIC) 为我们提供了做出此选择的原则性方法，它通过惩罚额外的模型参数来防止过拟合，除非该参数能显著提升模型对数据的拟合优度。这个练习 [@problem_id:3097983] 将让你亲身体验如何在这种基础的模型权衡中应用 AIC。", "problem": "在可靠性工程中，您会获得独立的失效时间观测数据。对于每个数据集，您需要判断，通过威布尔模型引入一个形状参数是否比指数模型具有更强的解释力。您的决策必须基于一个有原则的、基于推导的模型选择准则，该准则以信息论风险和最大似然为基础，而不是基于临时的拟合优度度量。\n\n假设非负失效时间服从以下参数族：\n\n- 指数模型，其率参数为 $\\beta > 0$，概率密度函数为 $f_{\\text{exp}}(t \\mid \\beta)$，定义于 $t > 0$。\n- 威布尔模型，其形状参数为 $k > 0$，尺度参数为 $\\lambda > 0$，概率密度函数为 $f_{\\text{weib}}(t \\mid k,\\lambda)$，定义于 $t > 0$。\n\n假设每个数据集中的所有观测值在模型参数给定的条件下是独立同分布的。您的模型选择应基于连续分布、最大似然估计（MLE）和库尔贝克-莱布勒散度的框架，并使用一个渐近地倾向于最小化期望库尔贝克-莱布勒风险的模型的准则。该准则必须依赖于最大化对数似然，并包含一个与模型中自由参数数量成正比的惩罚项。不要使用任何启发式或图形化方法。\n\n对于每个数据集，您的程序必须：\n- 在指数模型和威布尔模型下，通过最大化对数似然来估计参数。\n- 计算从最大化对数似然的渐近偏差推导出的信息论模型选择准则，该准则带有一个关于自由参数数量的惩罚项。\n- 当且仅当威布尔模型的准则值比指数模型的准则值严格小至少一个小的数值公差 $\\epsilon$（其中 $\\epsilon = 10^{-8}$）时，判定威布尔形状参数提高了模型的解释力。\n\n所有失效时间均以小时为单位。无需进行单位转换；只需将时间视为正实数。最终输出为布尔值；因此，输出中无需报告物理单位。\n\n测试套件：\n使用以下五个观测到的失效时间（单位：小时）数据集：\n- 测试用例 $1$：$[0.7, 1.1, 1.6, 2.2, 3.0, 3.8, 4.7, 5.7, 6.8, 8.0, 9.3, 10.7, 12.2, 13.8, 15.5]$。\n- 测试用例 $2$：$[0.05, 0.16, 0.20, 0.35, 0.42, 0.59, 0.60, 0.83, 0.90, 1.31, 1.39, 1.60, 2.50]$。\n- 测试用例 $3$：$[0.2, 0.3, 0.5, 0.9, 1.5, 2.5, 4.0, 6.8, 11.0, 18.0]$。\n- 测试用例 $4$：$[1.5, 0.7, 2.1, 1.0, 0.4, 3.3, 2.8, 1.2, 0.9, 2.0, 1.8, 2.4, 0.6, 3.0]$。\n- 测试用例 $5$：$[1.0, 2.0, 3.0]$。\n\n最终输出格式：\n您的程序应生成单行输出，其中包含一个用方括号括起来的、以逗号分隔的结果列表。每个元素都应为一个布尔值，用于指示在相应测试用例中，根据该准则，添加威布尔形状参数是否提高了模型的解释力。例如，输出应类似于 $[b_1,b_2,b_3,b_4,b_5]$，其中每个 $b_i$ 为 `True` 或 `False`。", "solution": "用户提供了一个统计模型选择方面的适定问题，该问题基于可靠性工程和信息论的原理。任务是比较两种用于失效时间数据的嵌套参数模型——指数分布和威布尔分布——并确定哪种模型对几个数据集的拟合效果更好。决策准则被指定为基于最大化似然函数，并根据模型参数的数量进行惩罚，这是对赤池信息量准则（Akaike Information Criterion, AIC）的直接描述。\n\n验证过程确认了该问题在科学上是合理的、适定的且客观的。它包含了所有必要信息，没有矛盾或含糊之处。我现在将进行解法的形式化推导。\n\n问题的核心在于应用赤池信息量准则（$AIC$）。对于一个给定的模型，$AIC$ 的定义为：\n$$AIC = 2p - 2\\mathcal{L}_{\\text{max}}$$\n其中 $p$ 是模型中估计参数的数量，$\\mathcal{L}_{\\text{max}}$ 是模型对数似然函数的最大化值。$AIC$ 值较低的模型被认为在拟合优度和模型复杂性之间有更好的权衡。\n\n设失效时间数据集表示为 $T = \\{t_1, t_2, \\ldots, t_n\\}$，其中 $n$ 是观测值的数量。我们假设观测值是独立同分布的。\n\n**模型 1：指数模型**\n指数分布有一个单一的率参数 $\\beta > 0$。其概率密度函数（$PDF$）为：\n$$f_{\\text{exp}}(t \\mid \\beta) = \\beta e^{-\\beta t} \\quad \\text{for } t > 0$$\n参数数量为 $p_{\\text{exp}} = 1$。对于数据集 $T$ 的似然函数是：\n$$L(\\beta \\mid T) = \\prod_{i=1}^{n} f_{\\text{exp}}(t_i \\mid \\beta) = \\prod_{i=1}^{n} \\beta e^{-\\beta t_i} = \\beta^n e^{-\\beta \\sum_{i=1}^{n} t_i}$$\n对数似然函数为：\n$$\\ell_{\\text{exp}}(\\beta \\mid T) = \\ln(L(\\beta \\mid T)) = n \\ln(\\beta) - \\beta \\sum_{i=1}^{n} t_i$$\n为了找到 $\\beta$ 的最大似然估计（MLE），我们对 $\\beta$ 求导并将结果设为零：\n$$\\frac{\\partial \\ell_{\\text{exp}}}{\\partial \\beta} = \\frac{n}{\\beta} - \\sum_{i=1}^{n} t_i = 0$$\n解出 $\\beta$ 得到最大似然估计 $\\hat{\\beta}$：\n$$\\hat{\\beta} = \\frac{n}{\\sum_{i=1}^{n} t_i} = \\frac{1}{\\bar{t}}$$\n其中 $\\bar{t}$ 是失效时间的样本均值。\n将 $\\hat{\\beta}$ 代回对数似然函数，得到最大化的对数似然 $\\mathcal{L}_{\\text{exp}}$：\n$$\\mathcal{L}_{\\text{exp}} = \\ell_{\\text{exp}}(\\hat{\\beta} \\mid T) = n \\ln(\\hat{\\beta}) - \\hat{\\beta} \\sum_{i=1}^{n} t_i = n \\ln\\left(\\frac{1}{\\bar{t}}\\right) - \\frac{1}{\\bar{t}}(n\\bar{t}) = -n \\ln(\\bar{t}) - n$$\n因此，指数模型的 $AIC$ 为：\n$$AIC_{\\text{exp}} = 2p_{\\text{exp}} - 2\\mathcal{L}_{\\text{exp}} = 2(1) - 2(-n \\ln(\\bar{t}) - n) = 2 + 2n \\ln(\\bar{t}) + 2n$$\n\n**模型 2：威布尔模型**\n威布尔分布有一个形状参数 $k > 0$ 和一个尺度参数 $\\lambda > 0$。其 $PDF$ 为：\n$$f_{\\text{weib}}(t \\mid k, \\lambda) = \\frac{k}{\\lambda} \\left(\\frac{t}{\\lambda}\\right)^{k-1} e^{-(t/\\lambda)^k} \\quad \\text{for } t > 0$$\n参数数量为 $p_{\\text{weib}} = 2$。对于数据集 $T$ 的对数似然函数是：\n$$\\ell_{\\text{weib}}(k, \\lambda \\mid T) = \\sum_{i=1}^{n} \\ln\\left[\\frac{k}{\\lambda} \\left(\\frac{t_i}{\\lambda}\\right)^{k-1} e^{-(t_i/\\lambda)^k}\\right]$$\n$$ = \\sum_{i=1}^{n} \\left[ \\ln(k) - k\\ln(\\lambda) + (k-1)\\ln(t_i) - \\left(\\frac{t_i}{\\lambda}\\right)^k \\right]$$\n$$ = n\\ln(k) - nk\\ln(\\lambda) + (k-1)\\sum_{i=1}^{n}\\ln(t_i) - \\lambda^{-k}\\sum_{i=1}^{n}t_i^k$$\n为了找到最大似然估计 $(\\hat{k}, \\hat{\\lambda})$，我们分别对 $k$ 和 $\\lambda$ 求偏导数并将其设为零。对 $\\lambda$ 的导数产生了一个方便的关系式：\n$$\\frac{\\partial \\ell_{\\text{weib}}}{\\partial \\lambda} = -\\frac{nk}{\\lambda} + \\frac{k}{\\lambda^{k+1}}\\sum_{i=1}^{n}t_i^k = 0 \\implies \\lambda^k = \\frac{1}{n}\\sum_{i=1}^{n}t_i^k$$\n这个方程提供了 $\\lambda$ 作为 $k$ 的函数的最大似然估计：$\\hat{\\lambda}(k) = \\left(\\frac{1}{n}\\sum_{i=1}^{n}t_i^k\\right)^{1/k}$。\n将此关系代回对数似然函数，得到一个仅依赖于 $k$ 的剖面对数似然。对此剖面对数似然关于 $k$ 求导并设为零，得到以下关于最大似然估计 $\\hat{k}$ 的隐式方程：\n$$\\frac{1}{k} - \\frac{\\sum_{i=1}^{n} t_i^k \\ln(t_i)}{\\sum_{i=1}^{n} t_i^k} + \\frac{1}{n}\\sum_{i=1}^{n}\\ln(t_i) = 0$$\n这个方程无法求得 $k$ 的闭式解。因此，必须使用求根算法数值求解 $\\hat{k}$。一旦确定了 $\\hat{k}$，就可以使用推导出的公式计算 $\\hat{\\lambda}$：\n$$\\hat{\\lambda} = \\left(\\frac{1}{n}\\sum_{i=1}^{n}t_i^{\\hat{k}}\\right)^{1/\\hat{k}}$$\n然后，将 $(\\hat{k}, \\hat{\\lambda})$ 代入对数似然函数，计算出最大化的对数似然 $\\mathcal{L}_{\\text{weib}}$：\n$$\\mathcal{L}_{\\text{weib}} = n\\ln(\\hat{k}) - n\\hat{k}\\ln(\\hat{\\lambda}) + (\\hat{k}-1)\\sum_{i=1}^{n}\\ln(t_i) - \\hat{\\lambda}^{-\\hat{k}}\\sum_{i=1}^{n}t_i^{\\hat{k}}$$\n使用 $\\hat{\\lambda}$ 的表达式，最后一项简化为 $\\hat{\\lambda}^{-\\hat{k}}\\sum_{i=1}^{n}t_i^{\\hat{k}} = (\\frac{1}{n}\\sum t_i^{\\hat{k}})^{-1} \\sum t_i^{\\hat{k}} = n$。所以，\n$$\\mathcal{L}_{\\text{weib}} = n\\ln(\\hat{k}) - n\\hat{k}\\ln(\\hat{\\lambda}) + (\\hat{k}-1)\\sum_{i=1}^{n}\\ln(t_i) - n$$\n威布尔模型的 $AIC$ 为：\n$$AIC_{\\text{weib}} = 2p_{\\text{weib}} - 2\\mathcal{L}_{\\text{weib}} = 2(2) - 2\\mathcal{L}_{\\text{weib}} = 4 - 2\\mathcal{L}_{\\text{weib}}$$\n\n**决策准则**\n问题陈述，当且仅当威布尔模型的准则值比指数模型的准则值严格小至少 $\\epsilon = 10^{-8}$ 时，引入威布尔形状参数才算提高了模型的解释力。这可转化为以下条件：\n$$AIC_{\\text{weib}}  AIC_{\\text{exp}} - \\epsilon$$\n对于每个测试用例，我们将计算 $AIC_{\\text{exp}}$ 和 $AIC_{\\text{weib}}$，并应用此规则来确定是否应首选更复杂的威布尔模型。\n\n**实现算法**\n对于每个数据集：\n1.  将输入的失效时间列表转换为 `numpy` 数组。\n2.  使用解析公式计算 $AIC_{\\text{exp}}$。\n3.  定义威布尔形状参数 $\\hat{k}$ 的隐式函数。\n4.  使用数值求根器（例如 `scipy.optimize.root_scalar`）来求解 $\\hat{k}$。\n5.  计算相应的尺度参数 $\\hat{\\lambda}$。\n6.  计算最大化的对数似然 $\\mathcal{L}_{\\text{weib}}$。\n7.  计算 $AIC_{\\text{weib}}$。\n8.  使用指定的决策规则比较 $AIC_{\\text{weib}}$ 和 $AIC_{\\text{exp}}$，并将得到的布尔值附加到结果列表中。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import root_scalar\n\ndef solve():\n    \"\"\"\n    Solves the model selection problem for the provided test cases.\n    It compares the an exponential model and a Weibull model\n    for failure time data using the Akaike Information Criterion (AIC).\n    \"\"\"\n\n    test_cases = [\n        # Test Case 1\n        [0.7, 1.1, 1.6, 2.2, 3.0, 3.8, 4.7, 5.7, 6.8, 8.0, 9.3, 10.7, 12.2, 13.8, 15.5],\n        # Test Case 2\n        [0.05, 0.16, 0.20, 0.35, 0.42, 0.59, 0.60, 0.83, 0.90, 1.31, 1.39, 1.60, 2.50],\n        # Test Case 3\n        [0.2, 0.3, 0.5, 0.9, 1.5, 2.5, 4.0, 6.8, 11.0, 18.0],\n        # Test Case 4\n        [1.5, 0.7, 2.1, 1.0, 0.4, 3.3, 2.8, 1.2, 0.9, 2.0, 1.8, 2.4, 0.6, 3.0],\n        # Test Case 5\n        [1.0, 2.0, 3.0]\n    ]\n\n    epsilon = 1e-8\n    results = []\n\n    for data_list in test_cases:\n        data = np.array(data_list)\n        n = len(data)\n\n        # 1. Exponential Model AIC Calculation\n        t_bar = np.mean(data)\n        # Maximized log-likelihood for Exponential model\n        max_log_lik_exp = -n * np.log(t_bar) - n\n        # Number of parameters for Exponential model\n        p_exp = 1\n        aic_exp = 2 * p_exp - 2 * max_log_lik_exp\n\n        # 2. Weibull Model AIC Calculation\n        log_data = np.log(data)\n        sum_log_data = np.sum(log_data)\n\n        # We need to solve for k_hat from the implicit equation derived from MLE.\n        # This function represents the equation set to zero.\n        def k_equation(k, data, n, sum_log_data):\n            if k == 0:\n                return -np.inf  # Ensure k is positive\n            try:\n                t_k = np.power(data, k)\n                sum_t_k = np.sum(t_k)\n                sum_t_k_log_t = np.sum(t_k * log_data)\n                \n                # Check for numerical instability\n                if sum_t_k == 0 or not np.isfinite(sum_t_k):\n                    return np.nan\n\n                # The equation to be solved for k:\n                # 1/k - (sum(t_i^k * log(t_i)) / sum(t_i^k)) + (sum(log(t_i)) / n) = 0\n                return 1.0 / k - sum_t_k_log_t / sum_t_k + sum_log_data / n\n            except (OverflowError, ValueError):\n                return np.nan\n\n        try:\n            # Search for the root k_hat in a reasonable interval.\n            # brentq is a robust root-finding method for a bracketed root.\n            sol = root_scalar(k_equation, args=(data, n, sum_log_data),\n                              bracket=[0.1, 25.0], method='brentq')\n            k_hat = sol.root\n            \n            # MLE for lambda (scale parameter)\n            lambda_hat = (np.sum(np.power(data, k_hat)) / n)**(1.0 / k_hat)\n\n            # Maximized log-likelihood for Weibull model\n            # loglik = n*log(k) - n*k*log(lambda) + (k-1)*sum(log(t_i)) - n\n            max_log_lik_weib = (n * np.log(k_hat) - n * k_hat * np.log(lambda_hat) +\n                                (k_hat - 1) * sum_log_data - n)\n\n            # Number of parameters for Weibull model\n            p_weib = 2\n            aic_weib = 2 * p_weib - 2 * max_log_lik_weib\n\n        except ValueError:\n            # If root finding fails (e.g., no root in bracket, equation is ill-behaved),\n            # the model fit is problematic. We treat this as not an improvement.\n            # A huge AIC indicates a very poor fit.\n            aic_weib = np.inf\n        \n        # 3. Decision\n        # Weibull is preferred if its AIC is smaller by at least epsilon\n        is_weibull_better = aic_weib  aic_exp - epsilon\n        results.append(is_weibull_better)\n        \n    # Format and print the final output as a single list of booleans\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3097983"}, {"introduction": "现在，我们将 AIC 的应用场景转向机器学习中的分类问题。高斯判别分析 (Gaussian Discriminant Analysis, GDA) 是一种经典的分类方法，它包括假设类别协方差相等的线性判别分析 (Linear Discriminant Analysis, LDA) 和假设类别协方差不等的二次判别分析 (Quadratic Discriminant Analysis, QDA)。AIC 可以帮助我们判断，现有数据是否支持为每个类别使用独立协方差矩阵的更复杂模型，还是一个更简单、更稳健的共享协方差模型就已经足够。这项练习 [@problem_id:3097943] 将挑战你在多变量机器学习的背景下应用 AIC。", "problem": "一个双类别高斯判别分析（GDA）模型假设，每个带标签的观测值 $(x_i, y_i)$ 的生成过程如下：首先从一个类别概率为 $(\\pi_0, \\pi_1)$ 的分类分布中抽取一个类别标签 $y_i \\in \\{0,1\\}$，然后根据 $y_i$ 从一个多变量正态分布中抽取特征向量 $x_i \\in \\mathbb{R}^p$。有两种常见的模型族：一种是跨类别协方差相等的模型（通常称为线性判别分析（LDA）），另一种是具有特定于类别的协方差的模型（通常称为二次判别分析（QDA））。给定带标签的数据和这两个候选模型族，选择使赤池信息准则（Akaike Information Criterion, AIC）最小化的那一个。其中，赤池信息准则（AIC）是根据似然函数的最大值和自由参数数量构建的经典信息论模型选择分数。\n\n您的程序必须在每个候选族中对所有参数使用最大似然估计，并实现以下功能：\n\n- 对于协方差相等族：估计一个跨类别的共享协方差矩阵，以及特定于类别的均值向量和类别先验概率。\n- 对于协方差不等族：为每个类别估计一个单独的协方差矩阵，以及特定于类别的均值向量和类别先验概率。\n- 对于每个族，在估计出的参数下，计算带标签数据集的联合数据对数似然，然后计算该族的 AIC。选择 AIC 值较小的族。\n\n所有计算都必须根据上述生成式描述，以纯数学术语进行。数据集以合成高斯数据的形式提供，为保证可复现性而使用固定种子生成。对于每个测试用例，数据是通过连接来自两个具有指定均值和协方差以及已知类别标签的多变量正态分布的样本来构建的。协方差矩阵通过其类 Cholesky 因子给出，以保证正定性：对于每个类别 $k \\in \\{0,1\\}$，协方差定义为 $\\Sigma_k = L_k L_k^{\\top}$。\n\n您的程序应生成单行输出，其中包含一个布尔值列表，每个测试用例对应一个布尔值。如果选择了协方差不等族，则布尔值为 `True`，否则为 `False`。该行的格式必须与逗号分隔的 Python 列表完全一致，例如 `[True,False,True]`。\n\n实现并评估以下测试套件。在所有情况下，使用指定的随机种子生成数据，并为类别 0 生成 $n_0$ 个样本，为类别 1 生成 $n_1$ 个样本。\n\n- 测试用例 1（理想情况，类别不平衡，协方差差异显著）：\n  - 维度 $p = 2$。\n  - 样本数量：$n_0 = 140$, $n_1 = 35$。\n  - 随机种子：$202311$。\n  - 均值：$\\mu_0 = [0, 0]$, $\\mu_1 = [2, -2]$。\n  - 协方差因子：\n    - $L_0 = \\begin{bmatrix} 1.0  0.0 \\\\ 0.3  0.7 \\end{bmatrix}$，因此 $\\Sigma_0 = L_0 L_0^{\\top}$。\n    - $L_1 = \\begin{bmatrix} 1.6  0.0 \\\\ -0.4  1.0 \\end{bmatrix}$，因此 $\\Sigma_1 = L_1 L_1^{\\top}$。\n\n- 测试用例 2（协方差几乎相等的边界情况）：\n  - 维度 $p = 3$。\n  - 样本数量：$n_0 = 100$, $n_1 = 60$。\n  - 随机种子：$202312$。\n  - 均值：$\\mu_0 = [0, 0, 0]$, $\\mu_1 = [0.2, -0.1, 0.1]$。\n  - 协方差因子：\n    - $L_0 = \\begin{bmatrix} 1.0  0.0  0.0 \\\\ 0.05  1.0  0.0 \\\\ 0.02  0.03  1.0 \\end{bmatrix}$，因此 $\\Sigma_0 = L_0 L_0^{\\top}$。\n    - $L_1 = \\begin{bmatrix} 1.02  0.0  0.0 \\\\ 0.05  1.05  0.0 \\\\ 0.02  0.03  0.95 \\end{bmatrix}$，因此 $\\Sigma_1 = L_1 L_1^{\\top}$。\n\n- 测试用例 3（高维度下的边缘情况，协方差差异大，类别不平衡）：\n  - 维度 $p = 4$。\n  - 样本数量：$n_0 = 120$, $n_1 = 30$。\n  - 随机种子：$202313$。\n  - 均值：$\\mu_0 = [0, 0, 0, 0]$, $\\mu_1 = [1.5, -1.5, 1.0, 0.5]$。\n  - 协方差因子：\n    - $L_0 = \\begin{bmatrix} 1.0  0.0  0.0  0.0 \\\\ 0.1  1.0  0.0  0.0 \\\\ 0.0  0.05  1.0  0.0 \\\\ 0.0  0.0  0.02  1.0 \\end{bmatrix}$，因此 $\\Sigma_0 = L_0 L_0^{\\top}$。\n    - $L_1 = \\begin{bmatrix} 1.7  0.0  0.0  0.0 \\\\ 0.5  1.3  0.0  0.0 \\\\ 0.1  0.4  1.1  0.0 \\\\ 0.0  0.1  -0.2  1.0 \\end{bmatrix}$，因此 $\\Sigma_1 = L_1 L_1^{\\top}$。\n\n要求与说明：\n\n- 使用带标签的数据，在每个模型族下通过最大似然估计来估计参数。\n- 将类别先验概率 $(\\pi_0,\\pi_1)$ 视为要从标签中估计的参数。\n- 在构建模型选择分数时，请根据模型族正确计算参数数量：包括类别先验、类别均值和协方差矩阵的条目。\n- 不涉及角度；不涉及物理单位。\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的、以逗号分隔的列表（例如，`[True,False,True]`），每个布尔值表示相应测试用例是否选择了协方差不等族。", "solution": "用户希望在两种高斯判别分析（GDA）模型族之间进行选择——一种是所有类别共享一个协方差矩阵（类似于线性判别分析，LDA），另一种是每个类别都有特定的协方差矩阵（类似于二次判别分析，QDA）。选择标准是赤池信息准则（AIC），必须将其最小化。解决方案涉及推导和实现每个模型族参数的最大似然估计（MLEs），计算最大化后的对数似然，然后计算 AIC 进行比较。\n\n设数据集为 $\\mathcal{D} = \\{(x_i, y_i)\\}_{i=1}^N$，其中 $x_i \\in \\mathbb{R}^p$ 是特征向量，$y_i \\in \\{0, 1\\}$ 是类别标签。设 $n_k$ 为类别 k 中的样本数，因此 $N = n_0 + n_1$。生成模型假设 $y \\sim \\text{Categorical}(\\pi_0, \\pi_1)$ 且 $x|y=k \\sim \\mathcal{N}(\\mu_k, \\Sigma_k)$。\n\n数据的总对数似然由下式给出：\n$$ \\ln \\mathcal{L}(\\theta | \\mathcal{D}) = \\sum_{i=1}^N \\ln p(x_i, y_i | \\theta) = \\sum_{i=1}^N \\left[ \\ln p(y_i | \\theta) + \\ln p(x_i | y_i, \\theta) \\right] $$\n该表达式可分为类别标签项和特征项。\n\n_1. 公共参数的最大似然估计_\n类别先验概率 $\\pi_k$ 和类别均值 $\\mu_k$ 的最大似然估计（MLEs）对于两个模型族是相同的。\n标签的对数似然为 $\\sum_{i=1}^N \\ln p(y_i) = n_0 \\ln \\pi_0 + n_1 \\ln \\pi_1$。在约束条件 $\\pi_0 + \\pi_1 = 1$ 下最大化该式，可得：\n$$ \\hat{\\pi}_k = \\frac{n_k}{N} $$\n特征的对数似然按类别分开。对于每个类别 k，均值向量 $\\mu_k$ 的 MLE 是该类别中数据的样本均值：\n$$ \\hat{\\mu}_k = \\frac{1}{n_k} \\sum_{i: y_i=k} x_i $$\n\n_2. 协方差不等模型（QDA 族）_\n在此模型中，每个类别都有自己的协方差矩阵 $\\Sigma_0$ 和 $\\Sigma_1$。\n\n_参数估计_：每个协方差矩阵 $\\Sigma_k$ 的 MLE 是对应类别数据的样本协方差矩阵：\n$$ \\hat{\\Sigma}_k = \\frac{1}{n_k} \\sum_{i: y_i=k} (x_i - \\hat{\\mu}_k)(x_i - \\hat{\\mu}_k)^\\top $$\n\n_参数数量 ($k_{QDA}$)_：\n- 类别先验：1 个参数（因为 $\\pi_0+\\pi_1=1$）。\n- 类别均值：2 个大小为 $p$ 的向量，因此是 $2p$ 个参数。\n- 协方差矩阵：2 个对称的 $p \\times p$ 矩阵。每个有 $p(p+1)/2$ 个独立参数。总共是 $2 \\times \\frac{p(p+1)}{2} = p(p+1)$ 个参数。\n总参数数量：$k_{QDA} = 1 + 2p + p(p+1) = p^2 + 3p + 1$。\n\n_最大化对数似然_：当将 MLEs 代入多变量正态分布的对数似然函数时，类别 k 的表达式简化为：\n$$ \\ln \\hat{\\mathcal{L}}_k = -\\frac{n_k}{2} \\left( p \\ln(2\\pi) + \\ln|\\hat{\\Sigma}_k| + p \\right) $$\nQDA 模型的总最大化对数似然是标签对数似然与每个类别特征对数似然的总和：\n$$ \\ln \\hat{\\mathcal{L}}_{QDA} = \\left( \\sum_{k=0}^1 n_k \\ln \\hat{\\pi}_k \\right) + \\left( \\sum_{k=0}^1 -\\frac{n_k}{2} (p \\ln(2\\pi) + \\ln|\\hat{\\Sigma}_k| + p) \\right) $$\n\n_3. 协方差相等模型（LDA 族）_\n在此模型中，所有类别共享一个协方差矩阵 $\\Sigma$。\n\n_参数估计_：共享协方差矩阵 $\\Sigma$ 的 MLE 是由类别大小加权的池化协方差矩阵：\n$$ \\hat{\\Sigma}_{pool} = \\frac{1}{N} \\sum_{k=0}^1 \\sum_{i: y_i=k} (x_i - \\hat{\\mu}_k)(x_i - \\hat{\\mu}_k)^\\top = \\frac{n_0 \\hat{\\Sigma}_0 + n_1 \\hat{\\Sigma}_1}{N} $$\n\n_参数数量 ($k_{LDA}$)_：\n- 类别先验：1 个参数。\n- 类别均值：$2p$ 个参数。\n- 协方差矩阵：1 个对称的 $p \\times p$ 矩阵，有 $p(p+1)/2$ 个独立参数。\n总参数数量：$k_{LDA} = 1 + 2p + \\frac{p(p+1)}{2}$。\n\n_最大化对数似然_：逻辑与 QDA 情况类似，但对所有 $N$ 个数据点使用单一的池化协方差矩阵 $\\hat{\\Sigma}_{pool}$：\n$$ \\ln \\hat{\\mathcal{L}}_{LDA} = \\left( \\sum_{k=0}^1 n_k \\ln \\hat{\\pi}_k \\right) - \\frac{N}{2} \\left( p \\ln(2\\pi) + \\ln|\\hat{\\Sigma}_{pool}| + p \\right) $$\n\n_4. 使用 AIC 进行模型选择_\n赤池信息准则定义为 $AIC = 2k - 2 \\ln \\hat{\\mathcal{L}}$，其中 $k$ 是估计参数的数量，$\\hat{\\mathcal{L}}$ 是似然函数的最大值。我们为每个模型计算 AIC：\n$$ AIC_{QDA} = 2k_{QDA} - 2 \\ln \\hat{\\mathcal{L}}_{QDA} $$\n$$ AIC_{LDA} = 2k_{LDA} - 2 \\ln \\hat{\\mathcal{L}}_{LDA} $$\n选择 AIC 较低的模型。如果 $AIC_{QDA}  AIC_{LDA}$，程序将返回 `True`，否则返回 `False`。此过程将应用于每个测试用例，以生成最终的布尔结果列表。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef _calculate_aic_selection(p, n0, n1, seed, mu0, mu1, L0, L1):\n    \"\"\"\n    Performs GDA model selection for a single test case.\n\n    Selects between an equal-covariance (LDA-like) and an unequal-covariance\n    (QDA-like) model using the Akaike Information Criterion (AIC).\n    \"\"\"\n\n    # 1. Generate synthetic data based on the problem specification\n    rng = np.random.default_rng(seed)\n    N = n0 + n1\n    \n    mu0_true = np.array(mu0)\n    mu1_true = np.array(mu1)\n    L0_true = np.array(L0)\n    L1_true = np.array(L1)\n    \n    Sigma0_true = L0_true @ L0_true.T\n    Sigma1_true = L1_true @ L1_true.T\n    \n    # Generate data for each class\n    X0 = rng.multivariate_normal(mu0_true, Sigma0_true, size=n0)\n    X1 = rng.multivariate_normal(mu1_true, Sigma1_true, size=n1)\n\n    # 2. Compute parameters and AIC for each model\n    # The term for the log-likelihood of labels is common to both models,\n    # so it's calculated once.\n    pi0_hat = n0 / N\n    pi1_hat = n1 / N\n    logL_labels = n0 * np.log(pi0_hat) + n1 * np.log(pi1_hat)\n\n    # --- Unequal-Covariance Model (QDA) ---\n    \n    # Number of parameters: 1 (priors) + 2*p (means) + p*(p+1) (two cov matrices)\n    k_qda = 1 + 2 * p + p * (p + 1)\n    \n    # MLE for class-specific covariance matrices\n    Sigma0_hat = np.cov(X0, rowvar=False, bias=True)\n    Sigma1_hat = np.cov(X1, rowvar=False, bias=True)\n    \n    # Log-determinants of the covariance matrices\n    _, log_det_S0 = np.linalg.slogdet(Sigma0_hat)\n    _, log_det_S1 = np.linalg.slogdet(Sigma1_hat)\n    \n    # Maximized log-likelihood for features under the QDA model\n    logL_feat_qda = -0.5 * n0 * (p * np.log(2 * np.pi) + log_det_S0 + p)\n    logL_feat_qda += -0.5 * n1 * (p * np.log(2 * np.pi) + log_det_S1 + p)\n    \n    # Total maximized log-likelihood and AIC for QDA\n    logL_qda = logL_labels + logL_feat_qda\n    aic_qda = 2 * k_qda - 2 * logL_qda\n\n    # --- Equal-Covariance Model (LDA) ---\n    \n    # Number of parameters: 1 (priors) + 2*p (means) + p*(p+1)/2 (one cov matrix)\n    k_lda = 1 + 2 * p + p * (p + 1) / 2\n    \n    # MLE for the pooled covariance matrix\n    Sigma_pool_hat = (n0 * Sigma0_hat + n1 * Sigma1_hat) / N\n    \n    # Log-determinant of the pooled covariance matrix\n    _, log_det_Spool = np.linalg.slogdet(Sigma_pool_hat)\n    \n    # Maximized log-likelihood for features under the LDA model\n    logL_feat_lda = -0.5 * N * (p * np.log(2 * np.pi) + log_det_Spool + p)\n\n    # Total maximized log-likelihood and AIC for LDA\n    logL_lda = logL_labels + logL_feat_lda\n    aic_lda = 2 * k_lda - 2 * logL_lda\n\n    # 3. Model Selection\n    # Return True if the unequal-covariance model (QDA) has a lower AIC\n    return aic_qda  aic_lda\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print results.\n    \"\"\"\n    test_cases = [\n        {\n            \"p\": 2, \"n0\": 140, \"n1\": 35, \"seed\": 202311,\n            \"mu0\": [0, 0], \"mu1\": [2, -2],\n            \"L0\": [[1.0, 0.0], [0.3, 0.7]],\n            \"L1\": [[1.6, 0.0], [-0.4, 1.0]],\n        },\n        {\n            \"p\": 3, \"n0\": 100, \"n1\": 60, \"seed\": 202312,\n            \"mu0\": [0, 0, 0], \"mu1\": [0.2, -0.1, 0.1],\n            \"L0\": [[1.0, 0.0, 0.0], [0.05, 1.0, 0.0], [0.02, 0.03, 1.0]],\n            \"L1\": [[1.02, 0.0, 0.0], [0.05, 1.05, 0.0], [0.02, 0.03, 0.95]],\n        },\n        {\n            \"p\": 4, \"n0\": 120, \"n1\": 30, \"seed\": 202313,\n            \"mu0\": [0, 0, 0, 0], \"mu1\": [1.5, -1.5, 1.0, 0.5],\n            \"L0\": [[1.0, 0.0, 0.0, 0.0], [0.1, 1.0, 0.0, 0.0], [0.0, 0.05, 1.0, 0.0], [0.0, 0.0, 0.02, 1.0]],\n            \"L1\": [[1.7, 0.0, 0.0, 0.0], [0.5, 1.3, 0.0, 0.0], [0.1, 0.4, 1.1, 0.0], [0.0, 0.1, -0.2, 1.0]],\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        decision = _calculate_aic_selection(\n            p=case[\"p\"],\n            n0=case[\"n0\"],\n            n1=case[\"n1\"],\n            seed=case[\"seed\"],\n            mu0=case[\"mu0\"],\n            mu1=case[\"mu1\"],\n            L0=case[\"L0\"],\n            L1=case[\"L1\"]\n        )\n        results.append(decision)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```", "id": "3097943"}, {"introduction": "AIC 的用途并不仅限于参数数量明确的参数模型。在如局部多项式回归等非参数方法中，模型的复杂度由平滑参数（如带宽 $h$）控制，而非固定的参数个数。这项练习 [@problem_id:3098018] 将展示如何使用“有效自由度” ($\\mathrm{df}(h)$) 这一概念作为模型复杂度的代理，从而让 AIC 成为调整这些模型、防止欠拟合或过拟合的强大工具。这揭示了 AIC 框架在简单参数模型之外的深刻性和灵活性。", "problem": "考虑在独立高斯噪声假设下，局部多项式回归中的带宽选择任务。[@problem_id:483] 你需要从第一性原理出发，推导并实现一个基于赤池信息准则（Akaike information criterion, AIC）的准则，以选择合适的带宽。最终的程序必须计算多个候选带宽的AIC值，并使用通过平滑矩阵的迹定义的有效自由度，选择使该准则最小化的带宽。\n\n你将处理一维输入上次数为 $1$ 的局部多项式回归（局部估计散点平滑，LOESS）。在每个评估点 $x_i$ 处，使用高斯核权重 $K(u) = \\exp\\!\\left(-\\frac{u^2}{2}\\right)$ 和带宽参数 $h  0$ 拟合一个加权最小二乘多项式模型。在 $x_i$ 处进行拟合时，数据点 $x_j$ 的权重为 $w_{ij} = \\exp\\!\\left(-\\frac{(x_j - x_i)^2}{2 h^2}\\right)$。在 $x_i$ 处的局部设计矩阵是 $X_i \\in \\mathbb{R}^{n \\times 2}$，其列为 $[1, (x_j - x_i)]$，对角权重矩阵是 $W_i \\in \\mathbb{R}^{n \\times n}$，其对角线元素为 $w_{ij}$。在 $x_i$ 处的局部拟合值是局部加权最小二乘解的截距。对所有 $x_i$ 的最终估计器可以写成一个线性平滑器 $\\hat{\\mathbf{y}} = S(h)\\,\\mathbf{y}$，其中 $S(h) \\in \\mathbb{R}^{n \\times n}$ 是依赖于 $h$ 的平滑矩阵。\n\n假设数据由 $y_i = f(x_i) + \\varepsilon_i$ 生成，其中 $\\varepsilon_i$ 独立同分布于 $\\mathcal{N}(0, \\sigma^2)$，且 $\\sigma^2  0$ 未知。令残差平方和为 $\\mathrm{RSS}(h) = \\sum_{i=1}^n (y_i - \\hat{y}_i(h))^2$。线性平滑器的有效自由度定义为 $\\mathrm{df}(h) = \\mathrm{tr}\\!\\left(S(h)\\right)$。\n\n从模型下数据的高斯对数似然函数出发，并使用噪声方差的最大似然估计，推导一个形式为 $AIC(h)$ 的基于AIC的选择准则。该准则应包含一个基于 $\\mathrm{RSS}(h)$ 的拟合优度项和一个基于 $\\mathrm{df}(h)$ 的复杂度惩罚项。在AIC中，将参数数量视为平滑器的有效自由度 $\\mathrm{df}(h)$。你的推导必须从高斯似然函数开始，并避免使用任何预先给出的AIC简化公式。\n\n然后，实现所得到的 $AIC(h)$，并为以下每个测试用例选择使其最小化的带宽。在所有情况下，均使用次数为 $1$ 的局部多项式回归、上述指定的高斯核，以及候选带宽列表 $[0.03, 0.06, 0.10, 0.20, 0.35, 0.50]$。\n\n测试套件：\n- 案例 $1$ (振荡信号): $n = 80$, 随机种子 $= 123$，输入 $x_i$ 从 $\\mathrm{Uniform}(0,1)$ 独立采样然后升序排序。均值函数为 $f(x) = \\sin(6\\pi x)$，噪声标准差为 $0.2$，因此 $y_i = f(x_i) + 0.2\\,\\eta_i$，其中 $\\eta_i \\sim \\mathcal{N}(0,1)$。\n- 案例 $2$ (近似线性): $n = 70$, 随机种子 $= 456$，输入 $x_i$ 从 $\\mathrm{Uniform}(0,1)$ 独立采样然后升序排序。均值函数为 $f(x) = 0.5 + 2 x$，噪声标准差为 $0.15$，因此 $y_i = f(x_i) + 0.15\\,\\eta_i$，其中 $\\eta_i \\sim \\mathcal{N}(0,1)$。\n- 案例 $3$ (近似常数): $n = 60$, 随机种子 $= 789$，输入 $x_i$ 从 $\\mathrm{Uniform}(0,1)$ 独立采样然后升序排序。均值函数为 $f(x) = 1.0$，噪声标准差为 $0.25$，因此 $y_i = f(x_i) + 0.25\\,\\eta_i$，其中 $\\eta_i \\sim \\mathcal{N}(0,1)$。\n\n要求：\n- 通过为每个 $x_i$ 显式地构建局部设计矩阵 $X_i$ 和权重 $W_i$，并使用加权最小二乘法得到从 $\\mathbf{y}$ 到 $\\hat{y}_i$ 的线性映射，来构建平滑矩阵 $S(h)$。具体来说，如果 $A_i = X_i^\\top W_i X_i$ 且 $B_i = X_i^\\top W_i$，那么 $S(h)$ 的第 $i$ 行必须由 $A_i^{-1} B_i$ 的第一行给出。在计算中使用数值稳定的伪逆来处理边界附近潜在的病态条件。\n- 对于每个候选带宽 $h$，计算 $\\hat{\\mathbf{y}}(h)$、$\\mathrm{RSS}(h)$、$\\mathrm{df}(h)$，然后计算推导出的 $AIC(h)$，并选择使 $AIC(h)$ 最小的 $h$。\n- 最终输出格式：你的程序应生成单行输出，其中包含三个案例所选的带宽，格式为方括号内以逗号分隔的列表。每个带宽必须打印到三位小数（例如，$[0.200,0.100,0.500]$）。不应打印任何其他文本。\n\n所有答案都必须是无单位的数字。最终输出为浮点数。确保你的实现和推导与给定假设一致且科学合理。", "solution": "该问题被评估为有效。它在科学上基于统计学习理论，问题设定良好，目标明确，数据充分，并且没有任何列出的无效性缺陷。\n\n### 基于AIC准则的推导\n\n我们首先建立统计框架。数据模型由 $y_i = f(x_i) + \\varepsilon_i$ 给出，其中 $i=1, \\dots, n$，误差项 $\\varepsilon_i$ 独立同分布于高斯分布，即 $\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma^2)$。用向量形式表示，即 $\\mathbf{y} \\sim \\mathcal{N}(\\boldsymbol{\\mu}, \\sigma^2 I_n)$，其中 $\\boldsymbol{\\mu}$ 是真实均值 $f(x_i)$ 的向量。\n\n局部多项式回归产生一个拟合向量 $\\hat{\\mathbf{y}}(h) = S(h)\\mathbf{y}$，它作为我们对均值向量 $\\boldsymbol{\\mu}$ 的估计。在给定拟合值 $\\hat{\\mathbf{y}}(h)$ 和噪声方差 $\\sigma^2$ 的条件下，观测到数据 $\\mathbf{y}$ 的似然函数是各个高斯概率密度的乘积：\n$$ L(\\hat{\\mathbf{y}}(h), \\sigma^2 | \\mathbf{y}) = \\prod_{i=1}^n \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y_i - \\hat{y}_i(h))^2}{2\\sigma^2}\\right) $$\n因此，对数似然函数（记为 $\\ell$）为：\n$$ \\ell(\\hat{\\mathbf{y}}(h), \\sigma^2 | \\mathbf{y}) = \\sum_{i=1}^n \\left( -\\frac{1}{2}\\log(2\\pi) - \\frac{1}{2}\\log(\\sigma^2) - \\frac{(y_i - \\hat{y}_i(h))^2}{2\\sigma^2} \\right) $$\n使用残差平方和 $\\mathrm{RSS}(h) = \\sum_{i=1}^n (y_i - \\hat{y}_i(h))^2$ 可以更紧凑地写成：\n$$ \\ell(\\hat{\\mathbf{y}}(h), \\sigma^2 | \\mathbf{y}) = -\\frac{n}{2}\\log(2\\pi) - \\frac{n}{2}\\log(\\sigma^2) - \\frac{\\mathrm{RSS}(h)}{2\\sigma^2} $$\n为了找到固定带宽 $h$ 下的最大化对数似然，我们首先求出未知方差 $\\sigma^2$ 的最大似然估计（MLE）。我们将 $\\ell$ 对 $\\sigma^2$ 求导并令其结果为零：\n$$ \\frac{\\partial \\ell}{\\partial \\sigma^2} = -\\frac{n}{2\\sigma^2} + \\frac{\\mathrm{RSS}(h)}{2(\\sigma^2)^2} = 0 $$\n解出 $\\sigma^2$ 得到其MLE：\n$$ \\hat{\\sigma}^2_{\\mathrm{ML}} = \\frac{\\mathrm{RSS}(h)}{n} $$\n将此估计值代回对数似然函数，得到给定 $h$ 下的最大化对数似然：\n$$ \\ell_{\\max}(h) = \\ell(\\hat{\\mathbf{y}}(h), \\hat{\\sigma}^2_{\\mathrm{ML}} | \\mathbf{y}) = -\\frac{n}{2}\\log(2\\pi) - \\frac{n}{2}\\log\\left(\\frac{\\mathrm{RSS}(h)}{n}\\right) - \\frac{\\mathrm{RSS}(h)}{2\\left(\\frac{\\mathrm{RSS}(h)}{n}\\right)} $$\n$$ \\ell_{\\max}(h) = -\\frac{n}{2}\\log(2\\pi) - \\frac{n}{2}\\log\\left(\\frac{\\mathrm{RSS}(h)}{n}\\right) - \\frac{n}{2} $$\n$$ \\ell_{\\max}(h) = -\\frac{n}{2}\\left(\\log(2\\pi) + 1\\right) - \\frac{n}{2}\\log(\\mathrm{RSS}(h)) + \\frac{n}{2}\\log(n) $$\n\n赤池信息准则（AIC）定义为：\n$$ \\mathrm{AIC} = -2 \\times (\\text{最大化对数似然}) + 2 \\times (\\text{参数数量}) $$\n问题指定平滑器的有效参数数量是其有效自由度，即 $k = \\mathrm{df}(h) = \\mathrm{tr}(S(h))$。应用该定义，我们得到：\n$$ \\mathrm{AIC}(h) = -2 \\left( -\\frac{n}{2}\\left(\\log(2\\pi) + 1\\right) - \\frac{n}{2}\\log(\\mathrm{RSS}(h)) + \\frac{n}{2}\\log(n) \\right) + 2 \\, \\mathrm{df}(h) $$\n$$ \\mathrm{AIC}(h) = n\\left(\\log(2\\pi) + 1\\right) + n\\log(\\mathrm{RSS}(h)) - n\\log(n) + 2 \\, \\mathrm{df}(h) $$\n为了从一组候选者中选择最佳带宽 $h$，我们只需考虑依赖于 $h$ 的项。项 $n(\\log(2\\pi) + 1)$ 和 $-n\\log(n)$ 在所有候选模型中都是常数。舍去这些常数项，我们得到一个与完整AIC成正比并且能产生相同模型排序的准则：\n$$ \\mathrm{AIC_{sel}}(h) = n\\log(\\mathrm{RSS}(h)) + 2 \\, \\mathrm{df}(h) $$\n这就是我们将要实现用来选择最优带宽 $h$ 的准则。我们将选择使该值最小的带宽。\n\n### 实现策略\n\n对于每个测试用例和每个候选带宽 $h$，我们将执行以下步骤：\n\n1.  **数据生成**：根据指定的数据生成过程生成大小为 $n$ 的输入向量 $\\mathbf{x}$ 和响应向量 $\\mathbf{y}$，包括用于可复现性的随机种子。\n\n2.  **平滑矩阵构建**：构建 $n \\times n$ 的平滑矩阵 $S(h)$。这是逐行完成的。对于每个目标点 $x_i$（其中 $i \\in \\{1, \\dots, n\\}$）：\n    a.  计算权重 $w_{ij} = \\exp\\left(-\\frac{(x_j - x_i)^2}{2 h^2}\\right)$，其中 $j \\in \\{1, \\dots, n\\}$。\n    b.  构建局部设计矩阵 $X_i \\in \\mathbb{R}^{n \\times 2}$，其行为 $[1, (x_j - x_i)]$。\n    c.  根据问题要求，计算 $A_i = X_i^\\top W_i X_i$ 和 $B_i = X_i^\\top W_i$，其中 $W_i$ 是由权重 $w_{ij}$ 构成的对角矩阵。在数值上，这可以通过广播权重向量来高效完成。\n    d.  $S(h)$ 的第 $i$ 行（记为 $s_i^\\top$）是 $A_i^{-1} B_i$ 的第一行。使用伪逆 `np.linalg.pinv` 来计算 $A_i^{-1}$ 以确保数值稳定性。\n\n3.  **准则计算**：\n    a.  计算有效自由度 $\\mathrm{df}(h) = \\mathrm{tr}(S(h))$。\n    b.  计算拟合值 $\\hat{\\mathbf{y}}(h) = S(h) \\mathbf{y}$。\n    c.  计算残差平方和 $\\mathrm{RSS}(h) = \\sum_{j=1}^n (y_j - \\hat{y}_j(h))^2$。\n    d.  最后，计算选择准则值 $\\mathrm{AIC_{sel}}(h) = n\\log(\\mathrm{RSS}(h)) + 2 \\, \\mathrm{df}(h)$。\n\n4.  **带宽选择**：在为所有候选带宽计算完 $\\mathrm{AIC_{sel}}(h)$ 后，选择产生最小 $\\mathrm{AIC_{sel}}$ 值的带宽 $h$ 作为该测试用例的最优带宽。\n\n对所有三个测试用例重复此过程，并将所选的带宽收集起来用于最终输出。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Derives and implements an AIC-based criterion for bandwidth selection\n    in local polynomial regression.\n    \"\"\"\n\n    test_cases = [\n        # Case 1 (oscillatory signal)\n        {\"n\": 80, \"seed\": 123, \"f\": lambda x: np.sin(6 * np.pi * x), \"sigma\": 0.2},\n        # Case 2 (approximately linear)\n        {\"n\": 70, \"seed\": 456, \"f\": lambda x: 0.5 + 2 * x, \"sigma\": 0.15},\n        # Case 3 (approximately constant)\n        {\"n\": 60, \"seed\": 789, \"f\": lambda x: np.ones_like(x), \"sigma\": 0.25},\n    ]\n\n    h_candidates = [0.03, 0.06, 0.10, 0.20, 0.35, 0.50]\n    \n    selected_bandwidths = []\n\n    for case in test_cases:\n        n = case[\"n\"]\n        seed = case[\"seed\"]\n        f = case[\"f\"]\n        sigma = case[\"sigma\"]\n\n        # Generate data\n        rng = np.random.default_rng(seed)\n        x = rng.uniform(0, 1, n)\n        x = np.sort(x)\n        eta = rng.normal(0, 1, n)\n        y = f(x) + sigma * eta\n\n        aic_scores = []\n        \n        for h in h_candidates:\n            # Construct the smoother matrix S\n            S = np.zeros((n, n))\n            for i in range(n):\n                # Target point for the local fit\n                x_i = x[i]\n                \n                # Distances from the target point\n                d = x - x_i\n                \n                # Gaussian kernel weights\n                weights = np.exp(-d**2 / (2 * h**2))\n                \n                # Local design matrix X_i\n                X_i = np.vstack([np.ones(n), d]).T\n                \n                # Calculate A_i = X_i^T W_i X_i and B_i = X_i^T W_i efficiently\n                # (X_i.T * weights) performs X_i.T @ diag(weights)\n                A_i = (X_i.T * weights) @ X_i\n                B_i = X_i.T * weights\n                \n                # Use pseudoinverse for numerical stability\n                A_i_inv = np.linalg.pinv(A_i)\n                \n                # The i-th row of S is the first row of (A_i^-1 B_i)\n                s_i_row = (A_i_inv @ B_i)[0, :]\n                S[i, :] = s_i_row\n                \n            # Calculate effective degrees of freedom\n            df_h = np.trace(S)\n            \n            # Calculate fitted values and RSS\n            y_hat = S @ y\n            rss_h = np.sum((y - y_hat)**2)\n            \n            # Calculate AIC-based criterion\n            # AIC_sel(h) = n*log(RSS(h)) + 2*df(h)\n            if rss_h = 0: # Avoid log(0) or log(-)\n                aic_h = np.inf\n            else:\n                aic_h = n * np.log(rss_h) + 2 * df_h\n            \n            aic_scores.append(aic_h)\n            \n        # Select the bandwidth that minimizes AIC\n        best_h_index = np.argmin(aic_scores)\n        best_h = h_candidates[best_h_index]\n        selected_bandwidths.append(best_h)\n\n    # Format the final output\n    formatted_results = [f\"{bw:.3f}\" for bw in selected_bandwidths]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3098018"}]}