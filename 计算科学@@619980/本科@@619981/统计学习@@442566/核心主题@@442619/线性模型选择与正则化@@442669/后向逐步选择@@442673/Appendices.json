{"hands_on_practices": [{"introduction": "要真正理解逐步选择方法，最好的方式莫过于亲手实现它。这项练习 [@problem_id:3105032] 将指导你使用贝叶斯信息准则 ($BIC$) 作为评判标准，编写后向和前向逐步选择的完整代码。通过在精心构建的数据集上比较它们的搜索路径和最终模型，你将深入体会到这些贪心算法的本质，并理解为何它们可能无法保证找到全局最优解。", "problem": "给定一个具有固定设计矩阵和响应向量的线性回归设定。基本基础包括高斯误差线性模型的定义、普通最小二乘法 (OLS) 和贝叶斯信息准则 (BIC)。设数据包含排列成矩阵 $X \\in \\mathbb{R}^{n \\times p}$ 的预测变量和响应 $y \\in \\mathbb{R}^{n}$。线性模型假设为 $y = \\beta_{0}\\mathbf{1} + X\\beta + \\varepsilon$，其中 $\\varepsilon \\sim \\mathcal{N}(0,\\sigma^{2}I)$，$\\beta_{0} \\in \\mathbb{R}$ 是截距，$\\beta \\in \\mathbb{R}^{p}$ 是系数。普通最小二乘法 (OLS) 通过最小化残差平方和 (RSS) 来估计 $(\\beta_{0},\\beta)$。在高斯误差的假设下，最大化对数似然产生一个信息准则，定义为贝叶斯信息准则 (BIC)。对于一个预测变量子集 $S \\subseteq \\{1,2,\\dots,p\\}$，并将截距计入参数，其形式为\n$$\\mathrm{BIC}(S) = n \\cdot \\log\\left(\\frac{\\mathrm{RSS}(S)}{n}\\right) + k \\cdot \\log(n),$$\n其中 $k = |S| + 1$ 计算了截距和 $|S|$ 个选定的预测变量。残差平方和 $\\mathrm{RSS}(S)$ 是通过 OLS 拟合计算的，使用的设计矩阵由一列全为1的向量和由 $S$ 索引的列增广而成。\n\n您的任务是实现两种由贝叶斯信息准则 (BIC) 驱动的逐步选择程序：\n- 从零模型开始的前向选择：从 $S=\\varnothing$ 开始，在每一步中考虑添加一个变量 $j \\notin S$ 以最小化 $\\mathrm{BIC}(S \\cup \\{j\\})$，并且仅当这严格减小当前 BIC 时才添加它；当没有添加操作能严格减小当前 BIC 时停止。\n- 从全模型开始的后向消除：从 $S=\\{1,2,\\dots,p\\}$ 开始，在每一步中考虑移除一个变量 $j \\in S$ 以最小化 $\\mathrm{BIC}(S \\setminus \\{j\\})$，并且仅当这严格减小当前 BIC 时才移除它；当没有移除操作能严格减小当前 BIC 时停止。\n\n在计算 $\\mathrm{BIC}(S)$ 时，始终使用包含截距的 OLS 拟合。平局决胜必须是确定性的：如果多个候选变量产生的 BIC 值在很小的容差范围内相等，则选择具有最小预测变量索引的候选者。所有索引都应被视为并报告为基于 1 的索引。\n\n在相同的数据集上实现这两种程序，以比较 BIC 选择的路径和最终子集。\n\n测试套件：\n在以下三个确定性测试用例上运行您的程序。对于每个用例，使用独立的标准正态变量和给定的种子，精确地生成指定的 $X$ 和 $y$。\n\n- 用例 A（代理混淆导致不同的最终子集）：\n  - 参数：$n=80$，$p=3$，种子 $=1$。\n  - 生成：\n    - 使用给定的种子，抽取长度为 $n$ 的向量 $x_{1}, x_{2}, z, e \\stackrel{\\text{iid}}{\\sim} \\mathcal{N}(0,1)$。\n    - 定义 $x_{3} = x_{1} + x_{2} + 0.1 z$。\n    - 定义 $y = x_{1} + x_{2} + 0.05 e$。\n    - 设置 $X = [x_{1}, x_{2}, x_{3}]$。\n  - 预期的定性行为：前向选择倾向于首先选择代理变量 $x_{3}$ 然后停止，而后向消除倾向于丢弃代理变量并保留 $\\{x_{1},x_{2}\\}$。\n\n- 用例 B（单一强信号，预期结果一致）：\n  - 参数：$n=60$，$p=5$，种子 $=2$。\n  - 生成：\n    - 使用给定的种子，为 $j \\in \\{1,2,3,4,5\\}$ 抽取独立的 $x_{j} \\stackrel{\\text{iid}}{\\sim} \\mathcal{N}(0,1)$ 以及 $e \\sim \\mathcal{N}(0,1)$。\n    - 定义 $y = 3.0 \\cdot x_{4} + 0.1 e$。\n    - 设置 $X = [x_{1}, x_{2}, x_{3}, x_{4}, x_{5}]$。\n  - 预期的定性行为：两种方法都应选择 $\\{4\\}$。\n\n- 用例 C（中度相关的真实对，预期结果一致但路径不同）：\n  - 参数：$n=100$，$p=6$，种子 $=3$。\n  - 生成：\n    - 使用给定的种子，抽取长度为 $n$ 的向量 $x_{1}, z_{1}, x_{3}, x_{4}, x_{5}, x_{6}, e \\stackrel{\\text{iid}}{\\sim} \\mathcal{N}(0,1)$。\n    - 定义 $x_{2} = x_{1} + 0.5 z_{1}$。\n    - 定义 $y = 1.5 x_{1} + 1.5 x_{2} + 0.2 e$。\n    - 设置 $X = [x_{1}, x_{2}, x_{3}, x_{4}, x_{5}, x_{6}]$。\n  - 预期的定性行为：两种方法都应选择 $\\{1,2\\}$，但由于起点和贪心决策的不同，路径（访问的子集序列）可能会有所不同。\n\n输出规范：\n对于每个测试用例，您的程序必须按顺序输出一个列表，其中包含以下确切元素：\n- 前向路径：一个子集列表，其中每个子集是升序排列的基于 1 的索引列表；包括初始空子集和每次接受添加后直到终止的每个子集。\n- 后向路径：一个子集列表，其中每个子集是升序排列的基于 1 的索引列表；包括初始全子集和每次接受移除后直到终止的每个子集。\n- 最终前向子集：一个升序排列的基于 1 的索引列表。\n- 最终后向子集：一个升序排列的基于 1 的索引列表。\n- 一个布尔值，指示最终前向子集是否等于最终后向子集。\n\n您的程序应生成单行输出，其中包含三个测试用例的结果，格式为一个用方括号括起来的逗号分隔列表（例如，$[r_{A}, r_{B}, r_{C}]$），其中每个 $r_{\\cdot}$ 是该用例上述的列表。此问题不涉及任何物理单位或角度单位，所有答案均为指定的纯数值或布尔值。", "solution": "该问题要求实现和比较两种应用于线性回归设定的逐步模型选择算法：前向选择和后向消除。模型选择的指导原则是贝叶斯信息准则 (BIC)，它在模型拟合度和复杂性之间进行权衡。\n\n### 1. 基本原理\n\n线性模型定义为 $y = \\beta_0\\mathbf{1} + X\\beta + \\varepsilon$，其中误差 $\\varepsilon$ 假定为独立且服从正态分布，即 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2I)$。目标是从 $X$ 的列中选择一个最优的预测变量子集。\n\n**贝叶斯信息准则 (BIC)**：\n对于由预测变量子集 $S$ 定义的模型，BIC 由下式给出：\n$$ \\mathrm{BIC}(S) = n \\cdot \\log\\left(\\frac{\\mathrm{RSS}(S)}{n}\\right) + k \\cdot \\log(n) $$\n其中：\n- $n$ 是观测值的数量。\n- $\\mathrm{RSS}(S)$ 是包含预测变量 $S$ 的模型的残差平方和。\n- $k = |S| + 1$ 是模型中的参数数量，包括截距。\n\n较低的 BIC 值表示更好的模型。第一项 $n \\cdot \\log(\\mathrm{RSS}(S)/n)$ 衡量模型的拟合不足，而第二项 $k \\cdot \\log(n)$ 是对模型复杂度的惩罚，该惩罚随着参数数量的增加而增加。\n\n**普通最小二乘法 (OLS)**：\n$\\mathrm{RSS}(S)$ 是通过使用 OLS 拟合指定模型来计算的。对于设计矩阵 $X_S$（包括一个截距和与 $S$ 对应的 $X$ 的列），OLS 系数估计值 $\\hat{\\beta}_S$ 最小化了观测响应和预测响应之间的平方差之和。$\\hat{\\beta}_S$ 由正规方程 $(X_S^T X_S)\\hat{\\beta}_S = X_S^T y$ 的解给出。\n\n### 2. 算法设计\n\n解决方案的核心组件是一个函数，用于计算任意预测变量子集的 BIC。\n\n**BIC 计算函数**：\n该函数接受一组预测变量索引 $S$、完整的设计矩阵 $X$ 和响应向量 $y$。\n1. 它通过将一列全为1的向量（用于截距）与由 $S$ 索引的 $X$ 的列进行增广，来构造模型的设计矩阵 $X_S$。\n2. 它使用 `numpy.linalg.lstsq(X_S, y)` 来解决 OLS 问题。这个函数很稳健，并提供 OLS 系数以及至关重要的 RSS。\n3. 如果矩阵 $X_S$ 是秩亏的（由于多重共线性），则会出现一种特殊情况。在这种情况下，`numpy.linalg.lstsq` 会为残差返回一个空数组。此时必须使用计算出的系数手动计算 RSS：$\\mathrm{RSS}(S) = \\sum_{i=1}^{n} (y_i - (X_S\\hat{\\beta}_S)_i)^2$。\n4. 在已知 RSS、$n$ 和 $k = |S| + 1$ 的情况下，使用其定义计算 BIC。\n\n**前向选择算法**：\n这是一种贪心的、自下而上的搜索算法。\n1. **初始化**：从零模型（仅含截距）开始，$S_0 = \\varnothing$。计算其 BIC，即 $\\mathrm{BIC}(S_0)$。在解路径中记录初始的空集。\n2. **迭代**：在每一步中，考虑添加当前模型中尚未包含的每个预测变量。对于当前模型 $S_{curr}$，找出能使模型 $S_{cand} = S_{curr} \\cup \\{j\\}$ 获得最小可能 BIC 值的预测变量 $j \\notin S_{curr}$。\n3. **决策**：如果 $\\mathrm{BIC}(S_{cand})$ 严格小于 $\\mathrm{BIC}(S_{curr})$，则接受该变更。更新当前模型 $S_{curr} \\leftarrow S_{cand}$，更新当前 BIC，并将新子集附加到路径中。\n4. **终止**：如果没有任何单个预测变量的添加能够严格减小 BIC，则算法终止。\n平局决胜规则（选择最小的预测变量索引）通过按索引升序检查候选预测变量来确定性地处理。\n\n**后向消除算法**：\n这是一种贪心的、自上而下的搜索算法。\n1. **初始化**：从包含所有 $p$ 个预测变量的全模型开始，$S_0 = \\{1, 2, \\dots, p\\}$。计算其 BIC，即 $\\mathrm{BIC}(S_0)$。在解路径中记录这个全集。\n2. **迭代**：在每一步中，考虑移除当前模型中的每个预测变量。对于当前模型 $S_{curr}$，找出移除后能使模型 $S_{cand} = S_{curr} \\setminus \\{j\\}$ 获得最小可能 BIC 值的预测变量 $j \\in S_{curr}$。\n3. **决策**：如果 $\\mathrm{BIC}(S_{cand})$ 严格小于 $\\mathrm{BIC}(S_{curr})$，则接受该变更。更新 $S_{curr} \\leftarrow S_{cand}$，更新 BIC，并将新的、更小的子集附加到路径中。\n4. **终止**：如果没有任何单个预测变量的移除能够严格减小 BIC，则算法终止。\n平局决胜的处理方式与前向选择中相同。\n\n### 3. 执行与输出\n\n解决方案首先实现用于 BIC 计算和两种搜索算法的辅助函数。对于每个指定的测试用例：\n1. 使用规定的参数和随机种子，通过 `numpy.random.default_rng` 生成数据（$X$ 和 $y$），以确保可复现性。\n2. 在数据集上执行前向选择和后向选择两种程序，得出它们各自的路径和最终选择的子集。\n3. 将结果——前向路径、后向路径、最终前向子集、最终后向子集，以及一个指示最终子集是否相同的布尔值——编译成该测试用例的列表。\n4. 最后，根据输出规范，将所有测试用例的结果格式化为单个紧凑的字符串并打印。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and compares forward and backward stepwise selection using BIC.\n    \"\"\"\n\n    def calculate_bic(S_indices, X, y):\n        \"\"\"\n        Calculates BIC for a model with predictors indexed by S_indices.\n        S_indices: a set of 0-based predictor indices.\n        \"\"\"\n        n = X.shape[0]\n        k = len(S_indices) + 1\n        \n        intercept = np.ones((n, 1))\n        if not S_indices:\n            X_S = intercept\n        else:\n            sorted_indices = sorted(list(S_indices))\n            X_S = np.hstack([intercept, X[:, sorted_indices]])\n\n        coeffs, residuals, rank, s = np.linalg.lstsq(X_S, y, rcond=None)\n        \n        # In case of rank deficiency, `residuals` is empty. We must calculate RSS manually.\n        if residuals.size == 0:\n            y_pred = X_S @ coeffs\n            rss = np.sum((y - y_pred)**2)\n        else:\n            rss = residuals[0]\n\n        # Handle perfect fit case where rss might be effectively zero\n        if rss = 1e-12:\n            return -np.inf\n            \n        bic = n * np.log(rss / n) + k * np.log(n)\n        return bic\n\n    def forward_selection(X, y):\n        \"\"\"\n        Performs forward stepwise selection using BIC.\n        \"\"\"\n        n, p = X.shape\n        current_S = set()\n        path = [[]]\n        current_bic = calculate_bic(current_S, X, y)\n        \n        while True:\n            best_bic_in_step = current_bic\n            best_candidate_to_add = -1\n            \n            # Candidates are predictors not currently in the model, sorted for tie-breaking\n            candidates = sorted(list(set(range(p)) - current_S))\n            \n            for j in candidates:\n                test_S = current_S | {j}\n                test_bic = calculate_bic(test_S, X, y)\n                \n                if test_bic  best_bic_in_step:\n                    best_bic_in_step = test_bic\n                    best_candidate_to_add = j\n            \n            if best_candidate_to_add != -1:\n                current_S.add(best_candidate_to_add)\n                current_bic = best_bic_in_step\n                path.append(sorted([idx + 1 for idx in current_S]))\n            else:\n                break\n                \n        final_subset = sorted([idx + 1 for idx in current_S])\n        return path, final_subset\n\n    def backward_elimination(X, y):\n        \"\"\"\n        Performs backward stepwise elimination using BIC.\n        \"\"\"\n        n, p = X.shape\n        current_S = set(range(p))\n        path = [list(range(1, p + 1))] # Handles p=0 case correctly\n\n        if p == 0:\n            return path, []\n\n        current_bic = calculate_bic(current_S, X, y)\n        \n        while True:\n            if not current_S:\n                break\n                \n            best_bic_in_step = current_bic\n            best_candidate_to_remove = -1\n            \n            # Candidates for removal, sorted for tie-breaking\n            candidates = sorted(list(current_S))\n            \n            for j in candidates:\n                test_S = current_S - {j}\n                test_bic = calculate_bic(test_S, X, y)\n                \n                if test_bic  best_bic_in_step:\n                    best_bic_in_step = test_bic\n                    best_candidate_to_remove = j\n                    \n            if best_candidate_to_remove != -1:\n                current_S.remove(best_candidate_to_remove)\n                current_bic = best_bic_in_step\n                path.append(sorted([idx + 1 for idx in current_S]))\n            else:\n                break\n                \n        final_subset = sorted([idx + 1 for idx in current_S])\n        return path, final_subset\n\n    def generate_data(case_id):\n        \"\"\"\n        Generates data for a given test case ID.\n        \"\"\"\n        if case_id == 'A':\n            n, p, seed = 80, 3, 1\n            rng = np.random.default_rng(seed)\n            x1, x2, z, e = [rng.standard_normal(n) for _ in range(4)]\n            x3 = x1 + x2 + 0.1 * z\n            y = x1 + x2 + 0.05 * e\n            X = np.vstack([x1, x2, x3]).T\n        elif case_id == 'B':\n            n, p, seed = 60, 5, 2\n            rng = np.random.default_rng(seed)\n            X = rng.standard_normal((n, p))\n            e = rng.standard_normal(n)\n            y = 3.0 * X[:, 3] + 0.1 * e # x4 is at 0-based index 3\n        else: # Case 'C'\n            n, p, seed = 100, 6, 3\n            rng = np.random.default_rng(seed)\n            x1, z1, x3, x4, x5, x6, e = [rng.standard_normal(n) for _ in range(7)]\n            x2 = x1 + 0.5 * z1\n            y = 1.5 * x1 + 1.5 * x2 + 0.2 * e\n            X = np.vstack([x1, x2, x3, x4, x5, x6]).T\n        return X, y\n\n    def to_compact_str(obj):\n        \"\"\"\n        Recursively converts a Python object to a compact string representation without spaces.\n        \"\"\"\n        if isinstance(obj, list):\n            return f\"[{','.join(to_compact_str(item) for item in obj)}]\"\n        elif isinstance(obj, bool):\n            return 'True' if obj else 'False'\n        else:\n            return repr(obj)\n\n    test_cases_ids = ['A', 'B', 'C']\n    all_results = []\n    \n    for case_id in test_cases_ids:\n        X, y = generate_data(case_id)\n        \n        fwd_path, fwd_final = forward_selection(X, y)\n        bwd_path, bwd_final = backward_elimination(X, y)\n        \n        are_equal = (fwd_final == bwd_final)\n        \n        case_result = [fwd_path, bwd_path, fwd_final, bwd_final, are_equal]\n        all_results.append(case_result)\n        \n    final_output_str = f\"[{','.join(to_compact_str(res) for res in all_results)}]\"\n    print(final_output_str)\n\nsolve()\n```", "id": "3105032"}, {"introduction": "在自动化模型构建中，数据泄漏是一个必须警惕的重大陷阱。如果盲目地应用后向选择，算法可能会选出一个看似表现优异，实则依赖于虚假相关性的模型，导致其在真实世界中彻底失效。这项练习 [@problem_id:3101321] 旨在让你学会防患于未然：你将实现一个数据泄漏的预检测程序，并比较“天真”选择与经过泄漏检测的稳健选择所产生的截然不同的结果。", "problem": "您必须编写一个完整的程序，该程序能够构建合成的回归数据集，并演示一个朴素的后向步进选择方法如何会保留一个仅因数据泄露而具有预测能力的变量，而一个能够感知泄漏的预处理步骤则可以在选择前检测并移除此类变量。该程序必须实现泄漏检测和后向步进选择，并且必须为指定的测试套件输出结果。所有计算必须以纯粹的数学和逻辑术语表示，无需外部输入。\n\n请从以下基础开始：\n- 普通最小二乘（OLS）线性回归假设响应向量 $y \\in \\mathbb{R}^n$ 是由预测变量 $X \\in \\mathbb{R}^{n \\times p}$ 和一个截距项通过 $y = \\beta_0 \\mathbf{1} + X \\beta + \\varepsilon$ 生成的，其中 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I_n)$。\n- OLS 估计器最小化残差平方和（RSS），其中 $\\mathrm{RSS} = \\lVert y - \\hat{y} \\rVert_2^2$。\n- 在高斯噪声模型下，最大化对数似然可以用 $\\mathrm{RSS}$ 来表示，对于一个有 $k$ 个参数（包括截距）和 $n$ 个观测值的模型，其贝叶斯信息准则（BIC）为 $ \\mathrm{BIC} = n \\log\\left(\\frac{\\mathrm{RSS}}{n}\\right) + k \\log(n)$。较低的 $\\mathrm{BIC}$ 表示模型更优。\n- 后向步进选择从一个包含所有候选特征的完整集合开始，每次迭代移除一个特征，选择能最大程度降低所选信息准则的移除操作，并在没有任何单一特征的移除能改善该准则时停止。\n- 两个向量 $a, b \\in \\mathbb{R}^n$ 之间的样本相关性是皮尔逊相关性，即协方差除以标准差的乘积。\n\n您的程序必须：\n1) 实现由 BIC 驱动的后向步进选择：\n   - 给定 $X \\in \\mathbb{R}^{n \\times p}$ 和 $y \\in \\mathbb{R}^n$，截距项始终包含在内（不计入 p 个可移除的候选特征中），从所有 $p$ 个特征开始，在每一步评估移除每一个剩余特征。当且仅当移除某个特征后得到的 BIC 严格小于当前 BIC 时，才移除该特征，并选择移除能使 BIC 最小化的那个特征。重复此过程，直到没有任何单一特征的移除可以降低 BIC。如果所有特征都被移除，模型将简化为仅含截距的模型。\n\n2) 实现一个泄漏检测程序，该程序仅使用 $X$、$y$ 和观测索引向量 $t = [0, 1, \\dots, n-1]^\\top$ 在选择前标记可疑变量：\n   - 规则 A（直接标签复制检测）：如果单个特征 $x_j$ 与 $y$ 的绝对相关性大于一个高阈值 $\\tau_{\\text{direct}} = 0.995$，则将 $x_j$ 标记为泄漏。\n   - 规则 B（时间代理检测）：如果单个特征 $x_j$ 与时间索引向量 $t$ 的绝对相关性大于 $\\tau_{\\text{time}} = 0.98$ 并且与 $y$ 的绝对相关性大于 $\\tau_{\\text{time},y} = 0.7$，则将 $x_j$ 标记为可能基于时间的泄漏代理。\n   - 所有阈值 $\\tau_{\\text{direct}}$、$\\tau_{\\text{time}}$ 和 $\\tau_{\\text{time},y}$ 都是固定的，必须严格按照规定使用。\n\n3) 对每个数据集，运行两次后向选择：\n   - 朴素选择：对所有特征进行，不移除泄漏。\n   - 泄漏感知选择：移除泄漏检测程序标记的所有特征后，再对其余特征应用后向选择。\n\n4) 对每个数据集，返回一个包含以下内容的三元组：\n   - 朴素选择后选定的特征索引列表。\n   - 泄漏感知选择后选定的特征索引列表。\n   - 一个布尔值，指示指定的候选泄漏特征（始终是 $X$ 的最后一列）是否被泄漏检测程序标记。\n\n两个索引列表都必须引用 $X$ 的原始列索引（从 $0$ 到 $p-1$），并且必须按升序排序。\n\n数据集生成细节（测试套件）：\n- 对所有数据集，通过水平拼接三个块来构建 $X$：$X = [X_{\\text{true}} \\;|\\; X_{\\text{noise}} \\;|\\; x_{\\text{leak}}]$，其中 $X_{\\text{true}} \\in \\mathbb{R}^{n \\times q}$ 是具有非零系数的真实预测变量，$X_{\\text{noise}} \\in \\mathbb{R}^{n \\times r}$ 是纯噪声预测变量，而 $x_{\\text{leak}} \\in \\mathbb{R}^n$ 是指定的候选泄漏特征。真实响应生成为 $y = X_{\\text{true}} \\beta + \\varepsilon$，或在指定的时间趋势情况下，生成为 $y = X_{\\text{true}} \\beta + \\gamma \\tilde{t} + \\varepsilon$，其中 $\\tilde{t}$ 是均值为零、单位方差的标准化时间索引向量。噪声 $\\varepsilon$ 是标准差为 $\\sigma$ 的独立高斯噪声。在所有回归拟合中都使用截距项。\n\n提供以下四个具有固定参数和随机种子以确保可复现性的数据集：\n- 案例 1（直接泄漏的理想情况）：\n  - $n = 120$，$q = 2$，$r = 3$，$\\beta = [3.0, -2.0]$，$\\sigma = 1.5$。\n  - 随机种子 $42$。\n  - 泄漏类型：直接标签复制；用 $\\eta \\sim \\mathcal{N}(0, \\sigma_{\\text{leak}}^2)$ 和 $\\sigma_{\\text{leak}} = 0.02$ 构建 $x_{\\text{leak}} = y + \\eta$。\n- 案例 2（无泄漏，独立的候选特征）：\n  - $n = 100$，$q = 3$，$r = 2$，$\\beta = [1.2, -0.8, 1.5]$，$\\sigma = 2.0$。\n  - 随机种子 $7$。\n  - 泄漏类型：随机；构建 $x_{\\text{leak}} \\sim \\mathcal{N}(0, 1)$，独立于其他所有变量。\n- 案例 3（小样本，多噪声特征）：\n  - $n = 25$，$q = 2$，$r = 6$，$\\beta = [1.0, 0.5]$，$\\sigma = 1.0$。\n  - 随机种子 $123$。\n  - 泄漏类型：随机；构建 $x_{\\text{leak}} \\sim \\mathcal{N}(0, 1)$，独立于其他所有变量。\n- 案例 4（通过趋势实现的时间代理泄漏）：\n  - $n = 150$，$q = 1$，$r = 2$，$\\beta = [1.0]$，$\\sigma = 0.5$。\n  - 随机种子 $99$。\n  - 令 $\\tilde{t}$ 为标准化时间索引向量。响应为 $y = X_{\\text{true}} \\beta + \\gamma \\tilde{t} + \\varepsilon$，其中 $\\gamma = 2.0$。\n  - 泄漏类型：时间代理；设置 $x_{\\text{leak}} = \\tilde{t}$。\n\n对于所有案例，$X_{\\text{true}}$ 和 $X_{\\text{noise}}$ 的条目是独立同分布于标准正态随机变量，指定的泄漏变量被放置为 $X$ 的最后一列，因此其索引为 $p-1$，其中 $p = q + r + 1$。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，列表内无空格。每个数据集贡献一个形式为 $[\\text{list\\_before},\\text{list\\_after},\\text{flagged}]$ 的三元组，其中列表包含整数，flagged 值是一个布尔值。因此，总输出是这四个数据集的四个三元组的列表，例如 $[[\\cdot,\\cdot,\\cdot],[\\cdot,\\cdot,\\cdot],[\\cdot,\\cdot,\\cdot],[\\cdot,\\cdot,\\cdot]]$。\n\n此问题不涉及任何物理单位、角度或百分比。所有数值阈值和参数必须严格按照规定使用。程序必须是自包含的，使用指定的随机种子，并且不需要任何输入。", "solution": "所提出的问题是有效的。这是一个定义明确、自成体系且具有科学依据的统计学习练习，要求在不同数据条件下实现和比较特征选择过程。所有必要的参数、算法和条件都已明确提供，从而允许一个唯一且可验证的解决方案。\n\n任务是构建一个程序，以展示朴素后向步进特征选择算法在存在数据泄漏时的不可靠性，以及一个旨在检测此类泄漏的预处理步骤如何纠正此问题。这将通过实现必要的统计组件并将其应用于四个不同的、合成生成的数据集来实现。\n\n解决方案的结构如下：首先，我们定义用于模型选择的普通最小二乘（OLS）回归模型和贝叶斯信息准则（BIC）。其次，我们形式化后向步进选择算法。第三，我们指定用于检测数据泄漏的启发式规则。最后，我们概述生成数据集和运行比较分析的程序。\n\n**1. 线性回归和模型选择**\n\n我们假设标准线性模型，其中响应向量 $y \\in \\mathbb{R}^n$ 是从预测变量的设计矩阵 $X \\in \\mathbb{R}^{n \\times p}$ 和一个随机误差项 $\\varepsilon \\in \\mathbb{R}^n$ 生成的。包含截距项的模型由下式给出：\n$$ y = \\beta_0 \\mathbf{1} + X \\beta + \\varepsilon $$\n其中 $\\beta_0 \\in \\mathbb{R}$ 是截距，$\\mathbf{1}$ 是一个 $n$ 维的全一向量，$\\beta \\in \\mathbb{R}^p$ 是特征系数向量，误差项假定为独立同分布，即 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I_n)$。\n\n普通最小二乘（OLS）方法寻找使残差平方和（RSS）最小化的估计值 $\\hat{\\beta}_0$ 和 $\\hat{\\beta}$：\n$$ \\mathrm{RSS} = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 = \\lVert y - \\hat{y} \\rVert_2^2 $$\n其中 $\\hat{y} = \\hat{\\beta}_0 \\mathbf{1} + X \\hat{\\beta}$ 是拟合值。\n\n为了在一组具有不同预测变量子集的候选模型中进行选择，我们采用贝叶斯信息准则（BIC）。对于一个基于 $n$ 个观测值、具有 $k$ 个估计参数（包括截距）的模型，BIC 定义为：\n$$ \\mathrm{BIC} = n \\log\\left(\\frac{\\mathrm{RSS}}{n}\\right) + k \\log(n) $$\n$n \\log(\\mathrm{RSS}/n)$ 项与模型的拟合优度（最大化对数似然）有关，而 $k \\log(n)$ 是对模型复杂度的惩罚项。较低的 BIC 值表示一个在拟合度和复杂度之间取得更好平衡的更简约的模型。\n\n**2. 后向步进选择算法**\n\n后向步进选择是一种用于特征选择的贪婪启发式算法。它从一个包含所有候选预测变量的模型开始，迭代地移除最不有用的预测变量，直到没有进一步的移除能够改善模型质量。由 BIC 驱动的算法如下：\n\n令 $\\mathcal{F}$ 为所有 $p$ 个候选特征索引的集合，$\\mathcal{F} = \\{0, 1, \\dots, p-1\\}$。\n1.  **初始化**：从完整模型开始，其中活动特征集为 $\\mathcal{S}_{\\text{current}} = \\mathcal{F}$。计算此模型的 BIC，记为 $\\mathrm{BIC}_{\\text{current}}$。\n2.  **迭代**：\n    a. 对于 $\\mathcal{S}_{\\text{current}}$ 中的每个特征 $j$，考虑一个特征集为 $\\mathcal{S}_{\\text{trial}, j} = \\mathcal{S}_{\\text{current}} \\setminus \\{j\\}$ 的试验模型。计算其对应的 BIC，记为 $\\mathrm{BIC}_{\\text{trial}, j}$。\n    b. 找到移除后导致 BIC 下降最大（即 BIC 值最小）的特征 $j^*$：$j^* = \\arg\\min_{j \\in \\mathcal{S}_{\\text{current}}} \\mathrm{BIC}_{\\text{trial}, j}$。令最佳结果的 BIC 为 $\\mathrm{BIC}_{\\text{best}} = \\mathrm{BIC}_{\\text{trial}, j^*}$。\n    c. **决策**：如果 $\\mathrm{BIC}_{\\text{best}}  \\mathrm{BIC}_{\\text{current}}$，则移除是有益的。更新活动集 $\\mathcal{S}_{\\text{current}} \\leftarrow \\mathcal{S}_{\\text{current}} \\setminus \\{j^*\\}$ 并更新 $\\mathrm{BIC}_{\\text{current}} \\leftarrow \\mathrm{BIC}_{\\text{best}}$。重复迭代。\n    d. 如果 $\\mathrm{BIC}_{\\text{best}} \\ge \\mathrm{BIC}_{\\text{current}}$，则没有单一特征的移除能改善模型。算法终止。\n3.  **输出**：最终选定的特征集为 $\\mathcal{S}_{\\text{current}}$。如果该集合为空，则最终模型为仅含截距的模型。\n\n**3. 数据泄漏检测**\n\n当来自建模过程之外的信息被不当地包含在特征集中时，就会发生数据泄漏，这通常会导致不切实际的高预测性能。我们实现一个预处理程序，基于两条简单而强大的启发式规则来标记和移除可疑特征。令 $x_j$ 为第 $j$ 个特征的向量，$y$ 为响应向量，$t = [0, 1, \\dots, n-1]^\\top$ 为观测时间索引向量。相关性指皮尔逊相关系数。\n\n-   **规则 A（直接标签复制检测）**：一个几乎是目标变量完美副本的特征是泄漏的一种经典形式。如果其与响应的绝对相关性高于一个高阈值 $\\tau_{\\text{direct}}$，它将被标记。\n    $$ |\\text{corr}(x_j, y)| > \\tau_{\\text{direct}} = 0.995 $$\n-   **规则 B（时间代理检测）**：在时间序列或有序数据中，一个特征可能不直接复制标签，但可能成为时间的代理，而时间本身可能具有也驱动响应的强劲趋势。如果一个特征与时间索引和响应都高度相关，它将被标记。\n    $$ |\\text{corr}(x_j, t)| > \\tau_{\\text{time}} = 0.98 \\quad \\text{and} \\quad |\\text{corr}(x_j, y)| > \\tau_{\\text{time},y} = 0.7 $$\n\n**4. 实验过程**\n\n对于四个指定的数据集中的每一个，我们执行以下步骤：\n1.  **数据生成**：根据其规格合成数据集 $(X, y)$，包括真实预测变量的数量（$q$）、噪声预测变量的数量（$r$）、回归系数（$\\beta$）以及指定泄漏特征 $x_{p-1}$ 的泄漏类型。为保证可复现性，随机数生成器已设置种子。\n2.  **朴素选择**：将后向步进选择算法应用于完整的特征矩阵 $X \\in \\mathbb{R}^{n \\times p}$ 和响应向量 $y$。记录最终选定的特征索引集。\n3.  **泄漏感知选择**：\n    a. 将泄漏检测程序应用于 $(X, y)$ 以识别一组被标记的特征索引 $\\mathcal{F}_{\\text{flagged}}$。我们记录指定的泄漏特征（索引 $p-1$）是否在该集合中。\n    b. 通过从 $X$ 中移除与 $\\mathcal{F}_{\\text{flagged}}$ 对应的列来创建一个清洗后的特征矩阵 $X_{\\text{clean}}$。\n    c. 将后向步进选择应用于 $(X_{\\text{clean}}, y)$。\n    d. 从 $X_{\\text{clean}}$ 中选出的索引被映射回其在 $X$ 中的原始索引。记录这个最终集合。\n4.  **输出**：为每个数据集组合一个三元组，包含来自朴素选择的排序索引列表、来自泄漏感知选择的排序索引列表，以及一个指示指定泄漏特征是否被标记的布尔值。\n\n这种比较分析旨在突显朴素方法因其强大（但虚假）的预测能力而错误地保留泄漏变量的场景，而泄漏感知的预处理步骤则正确地识别并移除了它，从而产生一个更稳健、更具泛化能力的模型。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the entire simulation as specified.\n    \"\"\"\n\n    def calculate_bic(X_subset, y):\n        \"\"\"\n        Calculates the Bayesian Information Criterion (BIC) for a linear model.\n        \n        Args:\n            X_subset: Design matrix for the model. Can be None for intercept-only.\n            y: Response vector.\n            \n        Returns:\n            BIC value.\n        \"\"\"\n        n = len(y)\n        if X_subset is None or X_subset.shape[1] == 0:\n            k = 1  # Intercept only\n            rss = np.sum((y - np.mean(y))**2)\n        else:\n            num_features = X_subset.shape[1]\n            k = num_features + 1  # a coefficient for each feature + intercept\n            X_aug = np.c_[np.ones(n), X_subset]\n            try:\n                coeffs, _, _, _ = np.linalg.lstsq(X_aug, y, rcond=None)\n                rss = np.sum((y - (X_aug @ coeffs))**2)\n            except np.linalg.LinAlgError:\n                return np.inf  # Should not happen with this problem's data generation\n\n        # To prevent log(0) for perfect fits\n        if rss  1e-9:\n            rss = 1e-9\n            \n        bic = n * np.log(rss / n) + k * np.log(n)\n        return bic\n\n    def backward_selection(X, y):\n        \"\"\"\n        Performs backward stepwise selection using BIC.\n        \n        Args:\n            X: Full design matrix of candidate predictors.\n            y: Response vector.\n            \n        Returns:\n            A sorted list of indices of the selected features.\n        \"\"\"\n        p = X.shape[1]\n        current_indices = list(range(p))\n        if not current_indices:\n            return []\n        \n        current_bic = calculate_bic(X, y)\n\n        while len(current_indices) > 0:\n            bics_on_removal = []\n            for idx_to_remove in current_indices:\n                trial_indices = [i for i in current_indices if i != idx_to_remove]\n                if not trial_indices:\n                    X_trial = None\n                else:\n                    X_trial = X[:, trial_indices]\n                \n                bic = calculate_bic(X_trial, y)\n                bics_on_removal.append((bic, idx_to_remove))\n            \n            if not bics_on_removal:\n                break\n\n            best_bic, removed_idx = min(bics_on_removal)\n\n            if best_bic  current_bic:\n                current_bic = best_bic\n                current_indices.remove(removed_idx)\n            else:\n                break # No further improvement\n        \n        return sorted(current_indices)\n\n    def detect_leakage(X, y):\n        \"\"\"\n        Detects suspicious features based on correlation rules.\n        \n        Args:\n            X: Design matrix.\n            y: Response vector.\n            \n        Returns:\n            A set of indices of flagged features.\n        \"\"\"\n        n, p = X.shape\n        flagged_indices = set()\n        \n        t = np.arange(n)\n        \n        tau_direct = 0.995\n        tau_time = 0.98\n        tau_time_y = 0.7\n\n        for j in range(p):\n            x_j = X[:, j]\n            if np.std(x_j) == 0: continue\n            \n            # Rule A: Direct label-copy detection\n            corr_xy = np.corrcoef(x_j, y)[0, 1]\n            if abs(corr_xy) > tau_direct:\n                flagged_indices.add(j)\n\n            # Rule B: Time-proxy detection\n            if np.std(t) == 0: continue\n            corr_xt = np.corrcoef(x_j, t)[0, 1]\n            if abs(corr_xt) > tau_time and abs(corr_xy) > tau_time_y:\n                 flagged_indices.add(j)\n        \n        return flagged_indices\n\n    def generate_dataset(n, q, r, beta, sigma, seed, leak_type, gamma=None, sigma_leak=None):\n        \"\"\"\n        Generates a synthetic dataset based on specified parameters.\n        \"\"\"\n        rng = np.random.default_rng(seed)\n        \n        X_true = rng.standard_normal((n, q))\n        X_noise = rng.standard_normal((n, r))\n        \n        epsilon = rng.normal(0, sigma, n)\n        \n        if leak_type == 'time_proxy':\n            t = np.arange(n)\n            t_tilde = (t - np.mean(t)) / np.std(t)\n            y = X_true @ beta + gamma * t_tilde + epsilon\n            x_leak = t_tilde.reshape(-1, 1)\n        else:\n            y = X_true @ beta + epsilon\n            if leak_type == 'direct_leakage':\n                eta = rng.normal(0, sigma_leak, n)\n                x_leak = (y + eta).reshape(-1, 1)\n            else: # 'random'\n                x_leak = rng.standard_normal((n, 1))\n        \n        X_parts = []\n        if q > 0: X_parts.append(X_true)\n        if r > 0: X_parts.append(X_noise)\n        X_parts.append(x_leak)\n\n        X = np.hstack(X_parts)\n        return X, y\n\n    test_cases = [\n        {'n': 120, 'q': 2, 'r': 3, 'beta': [3.0, -2.0], 'sigma': 1.5, 'seed': 42, 'leak_type': 'direct_leakage', 'sigma_leak': 0.02},\n        {'n': 100, 'q': 3, 'r': 2, 'beta': [1.2, -0.8, 1.5], 'sigma': 2.0, 'seed': 7, 'leak_type': 'random'},\n        {'n': 25, 'q': 2, 'r': 6, 'beta': [1.0, 0.5], 'sigma': 1.0, 'seed': 123, 'leak_type': 'random'},\n        {'n': 150, 'q': 1, 'r': 2, 'beta': [1.0], 'sigma': 0.5, 'seed': 99, 'leak_type': 'time_proxy', 'gamma': 2.0},\n    ]\n\n    results = []\n    for params in test_cases:\n        X, y = generate_dataset(**params)\n        p = X.shape[1]\n        leakage_feature_idx = p - 1\n\n        # 1. Naive selection\n        naive_selected = backward_selection(X, y)\n        \n        # 2. Leakage-aware selection\n        flagged_indices = detect_leakage(X, y)\n        leak_feature_is_flagged = leakage_feature_idx in flagged_indices\n\n        clean_indices_map = [i for i in range(p) if i not in flagged_indices]\n        if not clean_indices_map:\n            aware_selected = []\n        else:\n            X_clean = X[:, clean_indices_map]\n            selected_clean_indices = backward_selection(X_clean, y)\n            aware_selected = [clean_indices_map[i] for i in selected_clean_indices]\n\n        results.append((naive_selected, aware_selected, leak_feature_is_flagged))\n\n    # Format the final output string without spaces\n    result_strings = []\n    for naive_list, aware_list, flag in results:\n        naive_str = f\"[{','.join(map(str, naive_list))}]\"\n        aware_str = f\"[{','.join(map(str, aware_list))}]\"\n        flag_str = str(flag)\n        result_strings.append(f\"[{naive_str},{aware_str},{flag_str}]\")\n    \n    print(f\"[{','.join(result_strings)}]\")\n\nsolve()\n```", "id": "3101321"}, {"introduction": "当模型包含交互项时，特征选择变得更加复杂，需要遵循一定的统计原则。本练习 [@problem_id:3101387] 将引导你处理一个高级建模问题：在后向选择过程中强制执行“强层次性原则”。这意味着，你将修改算法，以确保在移除一个主效应之前，必须先移除所有与之相关的交互项。这能帮助你理解如何将结构化规则融入自动化选择过程，从而构建出更具解释性和统计学意义的模型。", "problem": "您的任务是为一个带有交互项的线性回归模型实现一个基于统计学原理的反向逐步选择过程，同时强制执行强层次原则。该模型包含一个截距项、三个主效应和两个交互项。该过程必须使用一个从普通最小二乘法 (OLS) 假设推导出的有效统计检验，以在每一步决定移除哪个项。\n\n基本假设与定义：\n- 数据遵循线性模型 $y = X \\beta + \\varepsilon$，其中 $\\varepsilon \\sim \\mathcal{N}(0,\\sigma^2 I_n)$，$X \\in \\mathbb{R}^{n \\times p}$ 是设计矩阵，$\\beta \\in \\mathbb{R}^p$ 是系数向量，$I_n$ 是 $n \\times n$ 的单位矩阵。\n- 普通最小二乘法 (OLS) 估计量是残差平方和的最小化器，并且在高斯误差模型下，它能产生具有已知分布的检验统计量。\n- 强层次原则：如果模型中包含一个交互项，那么其对应的主效应也必须被包含。等价地，如果任何包含在模型中的交互项涉及到某个主效应，则该主效应不能被移除；而一个交互项只有在其两个对应的主效应都当前包含在模型中时，才能被考虑移除。\n\n模型结构：\n- 预测变量为 $X_1, X_2, X_3$，交互项为 $X_1 X_2$ 和 $X_2 X_3$。\n- 设计矩阵的列按以下顺序排列：索引 $0$ 是截距项，索引 $1$ 是 $X_1$，索引 $2$ 是 $X_2$，索引 $3$ 是 $X_3$，索引 $4$ 是 $X_1 X_2$，索引 $5$ 是 $X_2 X_3$。\n- 截距项（索引 $0$）必须始终保留在模型中，永远不是被移除的候选者。\n\n需要实现的反向逐步规则：\n- 从包含截距项、所有三个主效应以及两个交互项的全模型开始。\n- 在每次迭代中，根据强层次原则，形成有资格被移除的候选集：\n  - 交互项（索引 $4$ 或 $5$）只有在其两个对应的主效应都包含在模型中时才有资格被移除。\n  - 主效应（索引 $1$、$2$、$3$）只有在当前模型中不包含任何涉及它的交互项时才有资格被移除。\n- 对于每一个符合移除条件的单项，通过一个从第一性原理（使用残差平方和）推导出的有效嵌套模型 F 检验，将当前模型（全模型）与仅移除该单项的简化模型进行比较。\n- 通过 F 统计量及其 p 值计算决策。在每次迭代中，移除具有最大 p 值的单个合格项，前提是该 p 值严格大于阈值 $\\alpha = 0.01$。如果没有合格项的 p 值超过 $\\alpha$，则停止。\n\n检验统计量要求：\n- 您的推导、实现和决策规则必须仅从线性模型和高斯误差假设出发。您必须推导并使用正确的嵌套模型 F 检验，该检验将残差平方和的变化与估计的误差方差进行比较，并考虑正确的自由度。\n\n模拟与测试套件：\n- 对于每个测试用例，独立生成预测变量 $X_1, X_2, X_3 \\overset{\\text{i.i.d.}}{\\sim} \\mathcal{N}(0,1)$，并根据模拟的主效应确定性地构建交互项 $X_1 X_2$ 和 $X_2 X_3$。\n- 生成响应变量 $y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 X_3 + \\beta_{12} (X_1 X_2) + \\beta_{23} (X_2 X_3) + \\varepsilon$，其中 $\\varepsilon \\sim \\mathcal{N}(0,\\sigma^2)$，$\\sigma$ 在下面指定。\n- 为每个测试用例使用独立的随机种子以确保确定性结果。\n- 所有测试用例均使用 $n = 400$ 和 $\\sigma = 0.5$。\n\n提供并使用以下包含种子和真实系数的测试套件：\n- 测试用例 T1 (理想路径): 种子 $= 123$, $(\\beta_0,\\beta_1,\\beta_2,\\beta_3,\\beta_{12},\\beta_{23}) = (0, 1.0, 1.0, 0.0, 1.2, 0.0)$。\n- 测试用例 T2 (交互项移除的边界情况): 种子 $= 456$, $(\\beta_0,\\beta_1,\\beta_2,\\beta_3,\\beta_{12},\\beta_{23}) = (0, 1.0, 0.8, 0.5, 0.0, 0.0)$。\n- 测试用例 T3 (含一个交互项的边缘情况): 种子 $= 789$, $(\\beta_0,\\beta_1,\\beta_2,\\beta_3,\\beta_{12},\\beta_{23}) = (0, 0.0, 0.8, 0.8, 0.0, 1.2)$。\n\n要求的程序行为：\n- 实现上述具有强层次约束的反向逐步剔除法，并使用显著性水平为 $\\alpha = 0.01$ 的嵌套模型 F 检验。\n- 对于每个测试用例，以升序列表的形式输出最终模型中包含的预测变量的整数列索引（来自 $\\{1,2,3,4,5\\}$），不包括截距项。例如，如果最终模型包含 $X_1$、$X_2$ 和 $X_1 X_2$，则为该测试用例打印 $[1,2,4]$。\n- 最终的程序输出必须将三个测试结果聚合成单行，该行为一个包含三个列表的列表，顺序与 T1, T2, T3 相同。例如，一个有效的整体输出字符串应如下所示：[[1,2,4],[1,2,3],[2,3,5]]。\n\n角度单位和物理单位不适用于本问题。所有数值输出均无单位。确保您的程序只产生一行包含指定聚合列表字符串的内容，不含任何其他内容。", "solution": "该问题要求为一个线性回归模型实现一个反向逐步选择算法，该算法遵循强层次原则，并使用嵌套模型 F 检验来移除项。\n\n**1. 理论框架：嵌套模型 F 检验**\n\n该选择过程的基础是用于比较两个嵌套线性模型的 F 检验。如果简化模型（模型 $0$）的预测变量是全模型（模型 $1$）预测变量的子集，则称模型 $0$ 嵌套于模型 $1$ 中。\n\n一般线性模型由下式给出：\n$$y = X\\beta + \\varepsilon$$\n其中 $y \\in \\mathbb{R}^n$ 是响应向量，$X \\in \\mathbb{R}^{n \\times p}$ 是包含 $p$ 个预测变量（包括一个截距项）的设计矩阵，$\\beta \\in \\mathbb{R}^p$ 是系数向量，$\\varepsilon \\in \\mathbb{R}^n$ 是误差向量。我们假设误差是独立同分布的，遵循正态分布 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I_n)$。\n\n$\\beta$ 的普通最小二乘法 (OLS) 估计值通过最小化残差平方和 (RSS) 找到：\n$$\\hat{\\beta} = \\arg\\min_{\\beta} ||y - X\\beta||^2 = (X^T X)^{-1} X^T y$$\n对于给定的设计矩阵为 $X$ 的模型，其最小化的 RSS 为 $RSS = ||y - X\\hat{\\beta}||^2$。\n\n为了比较具有 $p_0$ 个参数的简化模型（模型 $0$）和具有 $p_1$ 个参数的全模型（模型 $1$）（$p_1  p_0$），我们构建一个原假设 $H_0$，即全模型中额外的 $p_1 - p_0$ 个系数全部为零。检验统计量为：\n$$F = \\frac{(RSS_0 - RSS_1) / (p_1 - p_0)}{RSS_1 / (n - p_1)}$$\n其中 $RSS_0$ 和 $RSS_1$ 分别是简化模型和全模型的残差平方和。项 $MSE_1 = RSS_1 / (n - p_1)$ 是全模型的均方误差，如果全模型是正确的，它就是误差方差 $\\sigma^2$ 的一个无偏估计量。\n\n在原假设 $H_0$ 下，此 F 统计量遵循自由度为 $(p_1 - p_0)$ 和 $(n - p_1)$ 的 F 分布。较大的 F 值表明全模型中额外的预测变量显著减少了 RSS，从而导致拒绝 $H_0$。通过将检验的 p 值与预先定义的显著性水平 $\\alpha$ 进行比较来做出决策。\n\n在我们特定的反向选择背景下，我们在每一步检验单个项的移除。因此，$p_1 - p_0 = 1$。“全模型”是选择过程中的当前模型，“简化模型”是移除了一个候选项后的模型。用于检验移除项 $j$ 的 F 统计量是：\n$$F_j = \\frac{RSS_{\\text{reduced}, j} - RSS_{\\text{current}}}{MSE_{\\text{current}}}$$\n其中 $p_{\\text{current}}$ 是当前模型中的参数数量。相应的 p 值从 $F_{1, n - p_{\\text{current}}}$ 分布计算得出。\n\n**2. 算法：具有强层次性的反向逐步选择**\n\n该算法从全模型开始，迭代地移除有资格被移除的最不显著的预测变量，直到没有预测变量可以被合理地移除为止。\n\n**模型结构：**\n全模型包含一个截距项、三个主效应 ($X_1, X_2, X_3$) 和两个交互项 ($X_1 X_2, X_2 X_3$)。设计矩阵的列索引如下：$0$: 截距项, $1$: $X_1$, $2$: $X_2$, $3$: $X_3$, $4$: $X_1 X_2$, $5$: $X_2 X_3$。截距项（索引 $0$）永远不会被移除。\n\n**强层次原则：**\n该原则对哪些项有资格被移除施加了约束：\n1.  一个主效应只有在当前模型中没有任何交互项涉及它时才能被移除。\n    -   $X_1$ (索引 $1$) 只有在 $X_1 X_2$ (索引 $4$) 不在模型中时才有资格被移除。\n    -   $X_2$ (索引 $2$) 只有在 $X_1 X_2$ (索引 $4$) 和 $X_2 X_3$ (索引 $5$) 都不在模型中时才有资格被移除。\n    -   $X_3$ (索引 $3$) 只有在 $X_2 X_3$ (索引 $5$) 不在模型中时才有资格被移除。\n2.  一个交互项总是有资格被移除。（所述的其主效应必须被包含在内的前提条件，在从一个层次模型开始的反向选择过程中总是被满足的）。\n\n**分步过程：**\n1.  **初始化**：从包含所有预测变量索引的全集开始，$S = \\{1, 2, 3, 4, 5\\}$。\n2.  **迭代**：重复以下步骤，直到满足停止条件：\n    a.  **识别候选者**：根据强层次原则，确定有资格被移除的预测变量子集 $C \\subseteq S$。\n    b.  **检验候选者**：对于每个预测变量索引 $j \\in C$：\n        i.   定义当前模型，其预测变量索引为 $S$（外加一个截距项）。设其参数数量为 $p_{\\text{current}} = |S| + 1$。\n        ii.  使用 OLS 拟合此模型，并计算其残差平方和 $RSS_{\\text{current}}$ 以及均方误差 $MSE_{\\text{current}} = RSS_{\\text{current}} / (n - p_{\\text{current}})$。\n        iii. 定义简化模型，其预测变量索引为 $S \\setminus \\{j\\}$。\n        iv.  拟合简化模型，并计算其 RSS，即 $RSS_{\\text{reduced}, j}$。\n        v.   计算 F 统计量：$F_j = (RSS_{\\text{reduced}, j} - RSS_{\\text{current}}) / MSE_{\\text{current}}$。\n        vi.  从 $F_{1, n-p_{\\text{current}}}$ 分布中计算相应的 p 值 $p_j$。\n    c.  **选择移除项**：找到具有最大 p 值的预测变量 $j^*$：$j^* = \\arg\\max_{j \\in C} p_j$。\n    d.  **决策**：\n        i.   如果 $p_{j^*} > \\alpha$（其中 $\\alpha=0.01$），则从模型中移除该预测变量：$S \\leftarrow S \\setminus \\{j^*\\}$。继续下一次迭代。\n        ii.  如果 $p_{j^*} \\le \\alpha$，则没有预测变量可以被移除。算法终止。\n3.  **最终模型**：终止时的预测变量索引集 $S$ 即为最终模型。使用指定的模拟参数（$n=400, \\sigma=0.5$）、种子和真实系数，将此过程应用于每个测试用例。.", "answer": "```python\nimport numpy as np\nfrom scipy.stats import f\n\ndef fit_ols_get_rss(X_data, y_data, indices):\n    \"\"\"\n    Fits an OLS model for the given data using the specified predictor indices.\n    \n    Args:\n        X_data (np.ndarray): The full design matrix (n, 6).\n        y_data (np.ndarray): The response vector (n,).\n        indices (list): List of predictor indices (from {1, ..., 5}) to include.\n                        The intercept (index 0) is always included.\n    \n    Returns:\n        tuple: A tuple containing:\n            - rss (float): The residual sum of squares.\n            - p (int): The number of parameters in the model (including intercept).\n    \"\"\"\n    # Always include the intercept (column 0)\n    cols_to_use = [0] + indices\n    X_model = X_data[:, cols_to_use]\n    \n    # number of parameters in the model\n    p = X_model.shape[1]\n    \n    # Use np.linalg.lstsq for robust OLS fitting. It returns RSS as the second element.\n    _, rss, _, _ = np.linalg.lstsq(X_model, y_data, rcond=None)\n    \n    # np.linalg.lstsq returns a 0-dim array for rss if solution is unique.\n    # It might return an empty array if underdetermined, handle this.\n    if isinstance(rss, np.ndarray) and rss.size == 0:\n        # This case implies an underdetermined system where residuals are zero.\n        # Should not happen with n > p.\n        rss = 0.0\n    else:\n        # Ensure rss is a float\n        rss = float(rss)\n\n    return rss, p\n\n\ndef perform_backward_selection(seed, beta_true, n, sigma, alpha):\n    \"\"\"\n    Performs backward stepwise selection with strong hierarchy constraint.\n\n    Args:\n        seed (int): Random seed for data generation.\n        beta_true (tuple): True coefficients (b0, b1, b2, b3, b12, b23).\n        n (int): Number of samples.\n        sigma (float): Standard deviation of the noise.\n        alpha (float): Significance level for removal.\n\n    Returns:\n        list: Sorted list of indices of predictors in the final model.\n    \"\"\"\n    np.random.seed(seed)\n    \n    # 1. Generate Data\n    X1 = np.random.normal(0, 1, n)\n    X2 = np.random.normal(0, 1, n)\n    X3 = np.random.normal(0, 1, n)\n    \n    # Create interactions\n    X12 = X1 * X2\n    X23 = X2 * X3\n    \n    # Full design matrix X with intercept\n    X_full = np.column_stack([np.ones(n), X1, X2, X3, X12, X23])\n    \n    # Generate response y\n    epsilon = np.random.normal(0, sigma, n)\n    y = X_full @ np.array(beta_true) + epsilon\n\n    # 2. Backward Selection Algorithm\n    # Predictor indices {1:X1, 2:X2, 3:X3, 4:X1X2, 5:X2X3}\n    active_indices = list(range(1, 6))\n\n    while True:\n        # Fit current model to get RSS_current and MSE_current\n        rss_current, p_current = fit_ols_get_rss(X_full, y, active_indices)\n        df_error = n - p_current\n        if df_error = 0: # Avoid division by zero\n            break\n        mse_current = rss_current / df_error\n\n        # Determine eligible predictors for removal based on strong hierarchy\n        eligible_to_remove = []\n        for j in active_indices:\n            if j == 1:  # X1\n                if 4 not in active_indices:\n                    eligible_to_remove.append(j)\n            elif j == 2:  # X2\n                if 4 not in active_indices and 5 not in active_indices:\n                    eligible_to_remove.append(j)\n            elif j == 3:  # X3\n                if 5 not in active_indices:\n                    eligible_to_remove.append(j)\n            elif j in [4, 5]: # Interactions X1X2, X2X3\n                eligible_to_remove.append(j)\n\n        if not eligible_to_remove:\n            break\n\n        # Calculate p-values for all eligible predictors\n        p_values = {}\n        for j in eligible_to_remove:\n            reduced_indices = [idx for idx in active_indices if idx != j]\n            rss_reduced, _ = fit_ols_get_rss(X_full, y, reduced_indices)\n            \n            F_stat = (rss_reduced - rss_current) / mse_current\n            # p_1 - p_0 = 1 degree of freedom for numerator\n            p_val = f.sf(F_stat, 1, df_error)\n            p_values[j] = p_val\n        \n        # Find term with largest p-value\n        if not p_values:\n            break\n            \n        term_to_remove = max(p_values, key=p_values.get)\n        max_p_value = p_values[term_to_remove]\n        \n        # Decide whether to remove the term\n        if max_p_value > alpha:\n            active_indices.remove(term_to_remove)\n        else:\n            # Stop if no term can be removed\n            break\n            \n    return sorted(active_indices)\n\n\ndef solve():\n    \"\"\"\n    Main function to run the test cases and print the final output.\n    \"\"\"\n    # Parameters for all test cases\n    n = 400\n    sigma = 0.5\n    alpha = 0.01\n\n    # Test cases: (seed, beta_true_vector)\n    test_cases = [\n        (123, (0.0, 1.0, 1.0, 0.0, 1.2, 0.0)),   # T1\n        (456, (0.0, 1.0, 0.8, 0.5, 0.0, 0.0)),   # T2\n        (789, (0.0, 0.0, 0.8, 0.8, 0.0, 1.2)),   # T3\n    ]\n\n    results = []\n    for seed, beta_true in test_cases:\n        final_indices = perform_backward_selection(seed, beta_true, n, sigma, alpha)\n        results.append(final_indices)\n\n    # Format the output as a string representing a list of lists.\n    # e.g., \"[[1, 2, 4],[1, 2, 3],[2, 3, 5]]\"\n    output_str = \"[\" + \",\".join(str(res).replace(\" \", \"\") for res in results) + \"]\"\n    print(output_str)\n\nsolve()\n```", "id": "3101387"}]}