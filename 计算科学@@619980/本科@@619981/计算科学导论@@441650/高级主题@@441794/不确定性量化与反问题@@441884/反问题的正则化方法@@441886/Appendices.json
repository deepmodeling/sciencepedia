{"hands_on_practices": [{"introduction": "正则化方法的核心在于将先验知识融入逆问题的求解中。一个强大的先验是稀疏性，即假设未知信号中只有少数非零元素。此练习将探索两种不同的稀疏性惩罚项：标准的$\\ell_1$范数（促进元素级稀疏性）和组稀疏性惩罚项（促进系数的整个分组一起被选中或剔除）。通过在一个特意简化的设定（前向算子为单位矩阵）中解决这些问题，我们可以清晰地隔离并理解不同正则化器在没有复杂模型影响下的核心作用[@problem_id:3185666]。这个实践将帮助你理解何时以及为何选择组稀疏性模型，而不是更常见的$\\ell_1$正则化。", "problem": "考虑一个线性反问题，其正向算子已知，未知系数被划分为预先定义的、不相交的组。目标是比较两种用于估计未知系数的正则化选择：普通 $\\ell_1$ 范数（它促进逐元素的稀疏性）和组稀疏惩罚（它促进组级别的选择）。您必须编写一个完整的程序，对于给定的测试套件，解决这两个正则化反问题，并报告哪种方法能正确选择整个活跃组。\n\n基本问题设置：\n- 正向模型：给定一个矩阵 $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$ 和数据 $\\mathbf{y} \\in \\mathbb{R}^{m}$，通过最小化以下形式的目标函数来恢复 $\\mathbf{x} \\in \\mathbb{R}^{n}$\n$$\n\\min_{\\mathbf{x} \\in \\mathbb{R}^{n}} \\ \\frac{1}{2}\\|\\mathbf{A}\\mathbf{x}-\\mathbf{y}\\|_{2}^{2} + \\lambda \\, \\mathcal{R}(\\mathbf{x}).\n$$\n- 两种正则化项 $\\mathcal{R}$ 的选择：\n  1. 普通 $\\ell_1$：$\\mathcal{R}(\\mathbf{x}) = \\|\\mathbf{x}\\|_{1} = \\sum_{i=1}^{n} |x_{i}|$。\n  2. 组稀疏（组套索）：假设索引被划分为 $G$ 个不相交的组 $\\{g_{1},\\dots,g_{G}\\}$；定义 $\\mathcal{R}(\\mathbf{x}) = \\sum_{g} \\|\\mathbf{x}_{g}\\|_{2}$，其中 $\\mathbf{x}_{g}$ 是 $\\mathbf{x}$ 限制在组 g 上的子向量。\n\n您必须使用的基本基础和假设：\n- 使用最小二乘数据保真度 $\\frac{1}{2}\\|\\mathbf{A}\\mathbf{x}-\\mathbf{y}\\|_{2}^{2}$ 和上面定义的两个凸正则化项。\n- 在下述所有测试用例中，取 $\\mathbf{A} = \\mathbf{I}_{n}$，即大小为 n x n 的单位矩阵，因此数据为 $\\mathbf{y} = \\mathbf{x}_{\\text{true}}$ 且无噪声。这是一个标准且经过充分测试的设计，它分离了正则化项对选择的影响，而不会与正向模型的条件问题相混淆。\n- 组是不相交的且被明确指定。成功标准涉及对整个活跃组的恢复。\n\n评估定义：\n- 令真实向量为 $\\mathbf{x}_{\\text{true}} \\in \\mathbb{R}^{n}$，划分为组 $\\{g_{1},\\dots,g_{G}\\}$。如果 $\\|\\mathbf{x}_{\\text{true},g}\\|_{2} > 0$，则称组 g 为活跃组，否则为非活跃组。\n- 给定一个估计值 $\\widehat{\\mathbf{x}}$，当且仅当满足以下条件时，我们称其正确选择了整个组：\n  - 对于每个活跃组 g，$\\widehat{\\mathbf{x}}_{g}$ 的所有元素都非零。\n  - 对于每个非活跃组 g，$\\widehat{\\mathbf{x}}_{g}$ 的所有元素都恰好为零。\n- 在数值计算中，将绝对值小于容差 $\\varepsilon = 10^{-8}$ 的任何元素视为零，将绝对值大于或等于 $\\varepsilon$ 的任何元素视为非零。\n\n您的程序必须：\n- 对于每个测试用例，解决这两个问题\n  - $\\min_{\\mathbf{x}} \\ \\frac{1}{2}\\|\\mathbf{x}-\\mathbf{y}\\|_{2}^{2} + \\lambda \\|\\mathbf{x}\\|_{1}$，\n  - $\\min_{\\mathbf{x}} \\ \\frac{1}{2}\\|\\mathbf{x}-\\mathbf{y}\\|_{2}^{2} + \\lambda \\sum_{g} \\|\\mathbf{x}_{g}\\|_{2}$，\n  使用您选择的任何正确的数值方法，达到一个能确保根据上述规则进行稳定识别的数值容差。\n- 对于每种方法和每个测试用例，判断是否正确选择了整个活跃组。\n- 将每个测试用例的结果映射到一个整数代码 $c \\in \\{0,1,2,3\\}$，如下所示：\n  - $c = 0$：两种方法都未能正确选择整个活跃组，\n  - $c = 1$：只有普通 $\\ell_1$ 方法成功，\n  - $c = 2$：只有组稀疏方法成功，\n  - $c = 3$：两种方法都成功。\n\n测试套件：\n- 所有用例均使用 $n = 6$，组 $g_{1} = [0,1,2]$，$g_{2} = [3,4,5]$，$\\mathbf{A} = \\mathbf{I}_{6}$，以及 $\\mathbf{y} = \\mathbf{x}_{\\text{true}}$。\n- 用例 1（$\\ell_1$ 失败但组稀疏成功的理想情况）：$\\mathbf{x}_{\\text{true}} = [0.6, 0.6, 0.6, 0, 0, 0]$，$\\lambda = 0.8$。\n- 用例 2（具有非常强正则化的边界情况）：$\\mathbf{x}_{\\text{true}} = [1, 1, 1, 0, 0, 0]$，$\\lambda = 5$。\n- 用例 3（组内大小相关，$\\ell_1$ 只保留一个子集的情况）：$\\mathbf{x}_{\\text{true}} = [1.2, 0.3, 0.2, 0, 0, 0]$，$\\lambda = 0.5$。\n- 用例 4（两种方法都成功的弱正则化情况）：$\\mathbf{x}_{\\text{true}} = [2, 2, 2, 0, 0, 0]$，$\\lambda = 0.1$。\n\n数值和实现要求：\n- 不涉及角度；不涉及物理单位。\n- 使用数值容差 $\\varepsilon = 10^{-8}$ 进行零/非零判断。\n- 您的程序应生成单行输出，其中包含方括号内以逗号分隔的整数列表形式的结果，按四个测试用例的 $[c_{1}, c_{2}, c_{3}, c_{4}]$ 顺序排列。\n\n您的任务：\n- 实现求解器，在四个指定的用例上运行它，应用选择规则，并以指定的确切格式打印单行结果。不需要用户输入，也不允许使用外部数据文件。", "solution": "该问题要求比较两种正则化方法：普通 $\\ell_1$ 范数和组稀疏范数，用于一个线性反问题。目标是确定哪种方法能正确识别预定义的系数活跃组。通过将正向算子 $\\mathbf{A}$ 设置为单位矩阵 $\\mathbf{I}$ 并提供无噪声数据 $\\mathbf{y} = \\mathbf{x}_{\\text{true}}$，问题得到了简化。这种设置分离了正则化项相关近端算子的行为。\n\n每个测试用例需要解决的两个优化问题是：\n1.  **普通 $\\ell_1$ 正则化 (LASSO):**\n    $$\n    \\min_{\\mathbf{x} \\in \\mathbb{R}^{n}} \\ \\frac{1}{2}\\|\\mathbf{x}-\\mathbf{y}\\|_{2}^{2} + \\lambda \\|\\mathbf{x}\\|_{1}\n    $$\n2.  **组稀疏正则化 (Group LASSO):**\n    $$\n    \\min_{\\mathbf{x} \\in \\mathbb{R}^{n}} \\ \\frac{1}{2}\\|\\mathbf{x}-\\mathbf{y}\\|_{2}^{2} + \\lambda \\sum_{g \\in G} \\|\\mathbf{x}_{g}\\|_{2}\n    $$\n其中 $\\mathbf{y} = \\mathbf{x}_{\\text{true}}$，$\\lambda$ 是正则化参数，$G$ 是预定义的、不相交的索引组集合。在所有测试用例中，向量维度为 $n=6$，组为 $g_1$（索引 $\\{0, 1, 2\\}$）和 $g_2$（索引 $\\{3, 4, 5\\}$）。\n\n这些优化问题的解可以通过将相应正则化项的近端算子应用于数据 $\\mathbf{y}$ 来找到。\n\n**普通 $\\ell_1$ 正则化的解**\n\n目标函数相对于 $\\mathbf{x}$ 的各个分量是可分的。问题可以写成：\n$$\n\\sum_{i=1}^{n} \\min_{x_i \\in \\mathbb{R}} \\left( \\frac{1}{2}(x_i - y_i)^2 + \\lambda |x_i| \\right)\n$$\n每个分量 $\\widehat{x}_i$ 的解由软阈值算子给出：\n$$\n\\widehat{x}_i = \\text{prox}_{\\lambda|\\cdot|}(y_i) = \\text{sign}(y_i) \\max(|y_i| - \\lambda, 0)\n$$\n该算子独立地对每个分量进行阈值处理。幅度小于 $\\lambda$ 的分量 $y_i$ 在解 $\\widehat{x}_i$ 中被设为零。这促进了逐元素的稀疏性。\n\n**组稀疏正则化的解**\n\n目标函数相对于系数组是可分的。问题可以写成：\n$$\n\\sum_{g \\in G} \\min_{\\mathbf{x}_g \\in \\mathbb{R}^{|g|}} \\left( \\frac{1}{2}\\|\\mathbf{x}_g - \\mathbf{y}_g\\|_2^2 + \\lambda \\|\\mathbf{x}_g\\|_2 \\right)\n$$\n每个子向量 $\\widehat{\\mathbf{x}}_g$ 的解由组（或块）软阈值算子给出：\n$$\n\\widehat{\\mathbf{x}}_g = \\text{prox}_{\\lambda\\|\\cdot\\|_2}(\\mathbf{y}_g) = \\left( 1 - \\frac{\\lambda}{\\|\\mathbf{y}_g\\|_2} \\right)_+ \\mathbf{y}_g = \\frac{\\mathbf{y}_g}{\\|\\mathbf{y}_g\\|_2} \\max(\\|\\mathbf{y}_g\\|_2 - \\lambda, 0)\n$$\n其中 $(z)_+ = \\max(z, 0)$。该算子作用于整个组。如果数据子向量的欧几里得范数 $\\|\\mathbf{y}_g\\|_2$ 小于 $\\lambda$，则整个对应的解子向量 $\\widehat{\\mathbf{x}}_g$ 被设为零向量。否则，该子向量被缩放，但其方向保持不变。这促进了组稀疏性，意味着一个组中的所有系数要么都为零，要么都可以非零。\n\n**评估与实现**\n\n对于每个测试用例，我们计算估计值 $\\widehat{\\mathbf{x}}_{\\ell_1}$ 和 $\\widehat{\\mathbf{x}}_{\\text{group}}$。然后我们根据提供的标准评估它们的成功与否。对于一个估计值 $\\widehat{\\mathbf{x}}$，一种方法是成功的，如果：\n1.  对于每个活跃组 g（其中 $\\|\\mathbf{x}_{\\text{true},g}\\|_2 > 0$），$\\widehat{\\mathbf{x}}_g$ 的所有元素都非零。\n2.  对于每个非活跃组 g（其中 $\\|\\mathbf{x}_{\\text{true},g}\\|_2 = 0$），$\\widehat{\\mathbf{x}}_g$ 的所有元素都为零。\n\n在数值上，“零”被定义为绝对值小于 $\\varepsilon = 10^{-8}$。\n\n在所有测试用例中，$\\mathbf{x}_{\\text{true}, g_1}$ 至少有一个非零分量，使 $g_1$ 成为活跃组。$\\mathbf{x}_{\\text{true}, g_2}$ 是零向量，使 $g_2$ 成为非活跃组。\n- 如果 $\\lambda$ 足够大，以至于将活跃组的某些（但非全部）分量置零（例如，如果 $\\mathbf{y}_{g_1}$ 的分量具有不同的大小），则 $\\ell_1$ 方法将不满足成功标准。\n- 组稀疏方法旨在避免这种情况：它将每个组视为一个单元，要么将其完全置零，要么保留它（并进行缩放）。因此，它要么将 $\\widehat{\\mathbf{x}}_{g_1}$ 全部设为零，要么保持其所有分量非零（假设 $\\mathbf{y}_{g_1}$ 没有零项，这在相关测试用例中是成立的）。\n\n程序将实现这两个近端算子和评估逻辑，计算四个测试用例的结果，并将它们映射到指定的整数代码。最终输出将是这些代码的列表。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the regularization comparison problem for a given test suite.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    # Each case is a tuple: (x_true_list, lambda_val)\n    test_cases = [\n        # Case 1\n        (np.array([0.6, 0.6, 0.6, 0.0, 0.0, 0.0]), 0.8),\n        # Case 2\n        (np.array([1.0, 1.0, 1.0, 0.0, 0.0, 0.0]), 5.0),\n        # Case 3\n        (np.array([1.2, 0.3, 0.2, 0.0, 0.0, 0.0]), 0.5),\n        # Case 4\n        (np.array([2.0, 2.0, 2.0, 0.0, 0.0, 0.0]), 0.1),\n    ]\n\n    # Global problem parameters\n    groups = [[0, 1, 2], [3, 4, 5]]\n    epsilon = 1e-8\n\n    def solve_l1(y, lam):\n        \"\"\"\n        Solves the l1-regularized problem using soft-thresholding.\n        min_x 0.5 * ||x - y||_2^2 + lam * ||x||_1\n        \"\"\"\n        return np.sign(y) * np.maximum(np.abs(y) - lam, 0)\n\n    def solve_group_sparsity(y, lam, groups_list):\n        \"\"\"\n        Solves the group sparsity regularized problem using block soft-thresholding.\n        min_x 0.5 * ||x - y||_2^2 + lam * sum_g ||x_g||_2\n        \"\"\"\n        x_hat = np.zeros_like(y)\n        for g_indices in groups_list:\n            y_g = y[g_indices]\n            norm_y_g = np.linalg.norm(y_g)\n            \n            if norm_y_g > lam:\n                scaler = (1 - lam / norm_y_g)\n                x_hat[g_indices] = scaler * y_g\n            else:\n                x_hat[g_indices] = 0.0\n        return x_hat\n\n    def check_success(x_hat, x_true, groups_list, tol):\n        \"\"\"\n        Checks if an estimate x_hat correctly selects entire active groups.\n        Success criterion:\n        1. For active groups, all entries of x_hat_g must be nonzero.\n        2. For inactive groups, all entries of x_hat_g must be zero.\n        \"\"\"\n        for g_indices in groups_list:\n            x_true_g = x_true[g_indices]\n            x_hat_g = x_hat[g_indices]\n            \n            # Determine if the ground-truth group is active\n            is_active = np.linalg.norm(x_true_g) > 0\n            \n            if is_active:\n                # All entries must be nonzero\n                if not np.all(np.abs(x_hat_g) >= tol):\n                    return False\n            else: # inactive\n                # All entries must be zero\n                if not np.all(np.abs(x_hat_g)  tol):\n                    return False\n        return True\n\n    results = []\n    for x_true, lam in test_cases:\n        y = x_true  # Data is noise-free\n\n        # Solve and evaluate for plain l1\n        x_hat_l1 = solve_l1(y, lam)\n        l1_succeeds = check_success(x_hat_l1, x_true, groups, epsilon)\n\n        # Solve and evaluate for group sparsity\n        x_hat_group = solve_group_sparsity(y, lam, groups)\n        group_succeeds = check_success(x_hat_group, x_true, groups, epsilon)\n        \n        # Determine the integer code based on success\n        code = 0\n        if l1_succeeds and group_succeeds:\n            code = 3\n        elif group_succeeds:\n            code = 2\n        elif l1_succeeds:\n            code = 1\n        \n        results.append(code)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```", "id": "3185666"}, {"introduction": "稀疏性的概念不仅限于信号本身，还可以应用于信号的梯度。总变差（Total Variation, TV）正则化就是基于这一思想，它惩罚梯度的$\\ell_1$范数，从而倾向于产生分段常数解。这个特性在保留图像边缘方面非常有效。在此练习中，你将把TV正则化应用于一个实际问题——从量化信号中恢复原始信号，这在去除图像“色带”效应等场景中非常关键。通过将其与传统的二次方平滑（即梯度的$\\ell_2$范数）方法进行比较，你将亲身体验TV正则化在保持图像清晰边缘方面的卓越能力[@problem_id:3185702]。", "problem": "考虑一个长度为 $N$ 的一维离散信号，表示为向量 $x \\in \\mathbb{R}^{N}$。观测信号 $y \\in \\mathbb{R}^{N}$ 是通过步长为 $q  0$ 的均匀中步长量化（uniform mid-tread quantization）产生的，该过程由算子 $Q_{q} : \\mathbb{R}^{N} \\to \\mathbb{R}^{N}$ 建模，其分量形式定义为 $y_{i} = Q_{q}(x_{i}) = q \\cdot \\mathrm{round}\\!\\left(\\frac{x_{i}}{q}\\right)$，其中 $i = 1, \\dots, N$。逆问题是通过施加源于量化的具有物理意义的约束，并应用正则化来偏好结构简单的解，从而从 $y$ 估计 $x$。\n\n量化过程意味着一致性约束：对于每个索引 $i$，未知量 $x_{i}$ 必须位于量化区间 $[y_{i} - \\frac{q}{2}, \\, y_{i} + \\frac{q}{2}]$ 内。令可行集为 $C = \\{x \\in \\mathbb{R}^{N} \\mid y_{i} - \\frac{q}{2} \\le x_{i} \\le y_{i} + \\frac{q}{2} \\ \\text{for all} \\ i\\}$。定义离散前向差分算子 $D : \\mathbb{R}^{N} \\to \\mathbb{R}^{N-1}$ 为 $(Dx)_{i} = x_{i+1} - x_{i}$，其中 $i = 1, \\dots, N-1$。信号 $x$ 的全变分（Total Variation, TV）是其离散梯度的 $\\ell_{1}$ 范数，即 $\\mathrm{TV}(x) = \\|Dx\\|_{1} = \\sum_{i=1}^{N-1} |x_{i+1} - x_{i}|$。\n\n你需要实现两种重建方法：\n\n1. 一种 TV 正则化的逆舍入方法：找到一个估计值 $x^{\\mathrm{TV}}$，它能解决以下约束下的 TV 最小化问题\n$$\n\\min_{x \\in C} \\ \\mathrm{TV}(x).\n$$\n该问题强制解与量化区间保持保真度，并惩罚剧烈变化以消除条带伪影（banding artifacts）。\n\n2. 一种使用梯度欧几里得二范数（$\\ell_2$）的二次平滑方法：找到一个估计值 $x^{\\ell_2}$，它能解决\n$$\n\\min_{x \\in \\mathbb{R}^{N}} \\ \\frac{1}{2}\\|x - y\\|_{2}^{2} + \\alpha \\|Dx\\|_{2}^{2},\n$$\n其中 $\\alpha  0$ 是一个控制平滑强度的正则化参数。此方法不强制执行量化区间一致性，因此可能会模糊边缘并引入平滑伪影。\n\n为了进行评估，使用均方误差（Mean Squared Error, MSE），其定义为\n$$\n\\mathrm{MSE}(x^{\\star}, x) = \\frac{1}{N} \\sum_{i=1}^{N} (x^{\\star}_{i} - x_{i})^{2},\n$$\n其中 $x^{\\star}$ 是未量化的真实信号（ground truth signal）。\n\n实现以下测试套件。在所有情况下，均使用 $N = 64$。对于每个测试，合成真实信号 $x^{\\star}$，通过四舍五入到 $q$ 的最接近倍数来计算 $y = Q_{q}(x^{\\star})$，执行两种重建，计算它们相对于 $x^{\\star}$ 的 MSE，并生成一个布尔值，指示 TV 正则化重建是否至少与 $\\ell_2$ 平滑重建一样精确，即 $\\mathrm{MSE}(x^{\\star}, x^{\\mathrm{TV}}) \\le \\mathrm{MSE}(x^{\\star}, x^{\\ell_{2}})$ 是否成立。\n\n- 测试用例 1（分段常数信号，粗量化）：\n  - 真实信号 $x^{\\star}$：长度为 $16$ 的分段，电平为 $[0.0, \\, 0.7, \\, -0.1, \\, 0.5]$。\n  - 量化步长 $q = 0.25$。\n  - $\\ell_2$ 正则化参数 $\\alpha = 0.5$。\n\n- 测试用例 2（信号值为常数且恰好在量化电平上）：\n  - 真实信号 $x^{\\star}_{i} = 0.5$ (对所有 $i$)\n  - 量化步长 $q = 0.5$。\n  - $\\ell_2$ 正则化参数 $\\alpha = 0.5$。\n\n- 测试用例 3（高频正弦波）：\n  - 真实信号 $x^{\\star}_{i} = 0.7 \\sin\\!\\left(2\\pi \\cdot 8 \\cdot \\frac{i-1}{N}\\right)$ for $i = 1, \\dots, N$。\n  - 量化步长 $q = 0.3$。\n  - $\\ell_2$ 正则化参数 $\\alpha = 0.3$。\n\n- 测试用例 4（分段常数信号，精细量化）：\n  - 真实信号 $x^{\\star}$：与测试用例 1 相同。\n  - 量化步长 $q = 0.05$。\n  - $\\ell_2$ 正则化参数 $\\alpha = 0.2$。\n\n你的实现细节必须遵循以下原则：\n- 带约束的 TV 最小化问题应作为一个凸优化问题来解决，使用一种有原则的一阶方法，该方法能处理量化区间约束 $C$ 和 TV 正则化项。对 $D$ 使用前向差分，对 $D^{\\top}$ 使用相应的伴随算子。为离散算子施加反射（Neumann 型）边界行为。\n- $\\ell_2$ 平滑解应通过求解由一阶最优性条件产生的正规方程组来计算，这将得到一个对称正定三对角线性系统。\n\n你的程序应产生单行输出，其中包含一个用方括号括起来的逗号分隔列表（例如，$[result1,result2,result3,result4]$），其中每个条目是对应测试用例的布尔值，指示 $\\mathrm{MSE}(x^{\\star}, x^{\\mathrm{TV}}) \\le \\mathrm{MSE}(x^{\\star}, x^{\\ell_{2}})$ 是否成立。此问题不涉及任何物理单位、角度或百分比；所有量均为无量纲的实标量和向量。", "solution": "该问题要求实现并比较两种不同的正则化方法，用于从一维信号 $x \\in \\mathbb{R}^{N}$ 的量化测量值 $y \\in \\mathbb{R}^{N}$ 重建原始信号。量化是一个步长为 $q  0$ 的均匀中步长过程，定义为 $y_{i} = q \\cdot \\mathrm{round}(x_i / q)$。该关系意味着真实信号值 $x_i$ 必须位于区间 $[y_i - q/2, y_i + q/2]$ 内，这定义了一个凸可行集 $C$。我们将比较一种强制执行此约束的基于全变分（TV）最小化的方法，以及一种不强制此约束的二次平滑方法。\n\n第一种方法，二次或 $\\ell_2$ 平滑，通过解决以下无约束优化问题来寻求估计值 $x^{\\ell_2}$：\n$$\n\\min_{x \\in \\mathbb{R}^{N}} \\ J(x) = \\frac{1}{2}\\|x - y\\|_{2}^{2} + \\alpha \\|Dx\\|_{2}^{2}\n$$\n此处，$y$ 是观测到的量化信号，$\\alpha  0$ 是一个正则化参数，$D$ 是前向差分算子，$(Dx)_i = x_{i+1} - x_i$。目标函数 $J(x)$ 是二次且严格凸的。通过将梯度 $\\nabla J(x)$ 设置为零，可以找到唯一的最小值点。\n梯度由下式给出：\n$$\n\\nabla J(x) = (x - y) + 2\\alpha D^{\\top}Dx = 0\n$$\n这产生了一个关于 $x$ 的线性方程组，即正规方程组：\n$$\n(I + 2\\alpha D^{\\top}D)x = y\n$$\n其中 $I$ 是单位矩阵，$D^{\\top}$ 是算子 $D$ 的伴随（转置）。矩阵 $L = D^{\\top}D$ 代表具有 Neumann 边界条件的一维离散拉普拉斯算子，如问题所述。对于定义的前向差分算子 $D \\in \\mathbb{R}^{(N-1) \\times N}$，矩阵 $L \\in \\mathbb{R}^{N \\times N}$ 是对称三对角矩阵：\n$$\nL = D^{\\top}D =\n\\begin{pmatrix}\n1  -1    \\\\\n-1  2  -1   \\\\\n \\ddots  \\ddots  \\ddots  \\\\\n  -1  2  -1 \\\\\n   -1  1\n\\end{pmatrix}\n$$\n因此，矩阵 $A = I + 2\\alpha L$ 也是对称、三对角且在 $\\alpha  0$ 时为严格正定的。这样的线性系统可以使用带状矩阵求解器高效且稳健地求解。\n\n第二种方法是全变分（TV）正则化，它通过解决以下约束凸优化问题来找到估计值 $x^{\\mathrm{TV}}$：\n$$\n\\min_{x \\in C} \\ \\mathrm{TV}(x) \\quad \\text{where} \\quad \\mathrm{TV}(x) = \\|Dx\\|_1\n$$\n可行集是超矩形 $C = \\{x \\in \\mathbb{R}^{N} \\mid y_i - q/2 \\le x_i \\le y_i + q/2, \\forall i \\}$。该问题在与量化区间一致的约束下，最小化信号梯度的 $\\ell_1$ 范数。由于目标函数 $\\mathrm{TV}(x)$ 是凸的但非光滑，且约束定义了一个凸集，该问题非常适合使用一阶原始-对偶算法求解。我们采用 Chambolle-Pock 算法。该问题可以写成 $\\min_x F(Dx) + G(x)$ 的形式，其中 $F(u) = \\|u\\|_1$，$G(x) = I_C(x)$ 是集合 $C$ 的指示函数，如果 $x \\in C$ 则其值为 $0$，否则为 $+\\infty$。\n\nChambolle-Pock 迭代（外推参数 $\\theta=1$）如下：\n1. $u^{k+1} = \\mathrm{prox}_{\\sigma F^*} (u^k + \\sigma D \\bar{x}^k)$\n2. $x^{k+1} = \\mathrm{prox}_{\\tau G} (x^k - \\tau D^{\\top} u^{k+1})$\n3. $\\bar{x}^{k+1} = 2x^{k+1} - x^k$\n\n邻近算子是标准的。$G$ 的邻近算子是到集合 $C$ 上的欧几里得投影：\n$$\n(\\mathrm{prox}_{\\tau G}(z))_i = \\Pi_C(z)_i = \\mathrm{clip}(z_i, y_i - q/2, y_i + q/2)\n$$\n$\\ell_1$ 范数的凸共轭 $F^*(u)$ 是 $\\ell_{\\infty}$ 单位球的指示函数。其邻近算子是到该球上的投影：\n$$\n(\\mathrm{prox}_{\\sigma F^*}(v))_i = \\Pi_{\\|\\cdot\\|_{\\infty}\\le 1}(v)_i = \\frac{v_i}{\\max(1, |v_i|)}\n$$\n为了保证收敛，步长 $\\tau$ 和 $\\sigma$ 必须满足 $\\tau \\sigma \\|D\\|^2  1$。算子范数的平方 $\\|D\\|^2 = \\lambda_{\\max}(D^{\\top}D)$ 的上界为 $4$。我们选择保守的步长 $\\sigma$ 和 $\\tau$ 以确保稳定性。\n\n对于每个测试用例，生成真实信号 $x^{\\star}$，然后进行量化得到 $y$。将两种重建方法应用于 $y$ 以获得 $x^{\\ell_2}$ 和 $x^{\\mathrm{TV}}$。每种重建的准确性通过相对于原始信号 $x^{\\star}$ 的均方误差（MSE）来衡量：$\\mathrm{MSE}(x^{\\star}, x) = \\frac{1}{N} \\|x^{\\star} - x\\|_2^2$。每个用例的最终输出是一个布尔值，指示基于 TV 的重建是否至少与基于 $\\ell_2$ 的重建一样精确，即 $\\mathrm{MSE}(x^{\\star}, x^{\\mathrm{TV}}) \\le \\mathrm{MSE}(x^{\\star}, x^{\\ell_2}})$ 是否成立。", "answer": "```python\nimport numpy as np\nfrom scipy.linalg import solve_banded\n\ndef solve():\n    \"\"\"\n    Solves the signal dequantization problem using two regularization methods\n    and compares their performance on a suite of test cases.\n    \"\"\"\n\n    def generate_x_star(case_num, N):\n        \"\"\"Generates the ground truth signal x_star for a given test case.\"\"\"\n        if case_num == 1 or case_num == 4:\n            x_star = np.zeros(N)\n            levels = [0.0, 0.7, -0.1, 0.5]\n            segment_len = N // 4\n            for i in range(4):\n                start, end = i * segment_len, (i + 1) * segment_len\n                x_star[start:end] = levels[i]\n            return x_star\n        elif case_num == 2:\n            return np.full(N, 0.5)\n        elif case_num == 3:\n            indices = np.arange(N)\n            return 0.7 * np.sin(2 * np.pi * 8 * indices / N)\n        return None\n\n    def quantize(x, q):\n        \"\"\"Performs uniform mid-tread quantization.\"\"\"\n        return q * np.round(x / q)\n\n    def mse(x_star, x):\n        \"\"\"Computes the Mean Squared Error.\"\"\"\n        return np.mean((x_star - x)**2)\n\n    def solve_l2(y, alpha, N):\n        \"\"\"Solves the l2-smoothing problem via normal equations.\"\"\"\n        # A = I + 2*alpha*D^T*D\n        # This matrix is symmetric and tridiagonal.\n        # Main diagonal: 1+2a, 1+4a, ..., 1+4a, 1+2a\n        # Off-diagonals: -2a\n        ab = np.zeros((3, N))\n        \n        # Upper diagonal (u=1)\n        ab[0, 1:] = -2 * alpha\n        \n        # Main diagonal (l=1, u=1)\n        ab[1, :] = 1 + 4 * alpha\n        ab[1, 0] = 1 + 2 * alpha\n        ab[1, -1] = 1 + 2 * alpha\n        \n        # Lower diagonal (l=1)\n        ab[2, :-1] = -2 * alpha\n        \n        x_l2 = solve_banded((1, 1), ab, y)\n        return x_l2\n\n    def solve_tv(y, q, N, iters=2000):\n        \"\"\"Solves the constrained TV minimization problem using Chambolle-Pock.\"\"\"\n        # Step sizes for primal-dual algorithm\n        # We need tau * sigma * ||D||^2  1. ||D||^2 = 4 for 1D Neumann.\n        # We choose tau*sigma = 0.24, which is  1/4.\n        sigma = 0.5\n        tau = 0.48\n\n        # Initialize variables\n        x = y.copy()\n        x_bar = x.copy()\n        u = np.zeros(N - 1)\n        \n        # Pre-calculate bin limits\n        x_min = y - q / 2.0\n        x_max = y + q / 2.0\n\n        # Operator D and D_T\n        def D(vec):\n            return vec[1:] - vec[:-1]\n\n        def D_T(vec):\n            res = np.zeros(N)\n            res[0] = -vec[0]\n            res[1:-1] = vec[:-1] - vec[1:]\n            res[-1] = vec[-1]\n            return res\n\n        for _ in range(iters):\n            # Dual update (prox of F*)\n            u_next_arg = u + sigma * D(x_bar)\n            u = u_next_arg / np.maximum(1.0, np.abs(u_next_arg))\n            \n            # Primal update (prox of G)\n            x_old = x\n            x_next_arg = x - tau * D_T(u)\n            x = np.clip(x_next_arg, x_min, x_max)\n            \n            # Extrapolation\n            x_bar = 2 * x - x_old\n            \n        return x\n\n    test_cases = [\n        # (case_num, q, alpha)\n        (1, 0.25, 0.5),\n        (2, 0.5, 0.5),\n        (3, 0.3, 0.3),\n        (4, 0.05, 0.2),\n    ]\n\n    results = []\n    N = 64\n\n    for case_num, q, alpha in test_cases:\n        x_star = generate_x_star(case_num, N)\n        y = quantize(x_star, q)\n\n        # L2-smoothing reconstruction\n        x_l2 = solve_l2(y, alpha, N)\n        mse_l2 = mse(x_star, x_l2)\n\n        # TV reconstruction\n        x_tv = solve_tv(y, q, N)\n        mse_tv = mse(x_star, x_tv)\n        \n        results.append(mse_tv = mse_l2)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```", "id": "3185702"}, {"introduction": "尽管总变差（TV）正则化在保护边缘方面表现出色，但它也有一个众所周知的缺点，即“阶梯效应”，它倾向于将平滑的斜坡区域转换为一系列小台阶。本练习旨在深入探讨这一问题，并介绍更先进的边缘保持正则化器，如Huber惩罚项和Perona-Malik型惩罚项，它们旨在缓解阶梯效应。你将通过在一个精心设计的包含平滑斜坡和锐利边缘的合成信号上实现并比较这几种方法，从而量化分析Huber和Perona-Malik等方法如何在不牺牲边缘清晰度的前提下，更准确地重构缓变区域[@problem_id:3185682]。", "problem": "您的任务是设计并实现一个数值实验，以比较用于一维去噪逆问题的不同正则化惩罚项。其基本基础是逆问题的变分表示法：给定未知干净信号 $x^{\\star} \\in \\mathbb{R}^{N}$ 的含噪观测值 $y \\in \\mathbb{R}^{N}$，一种常见的方法是通过最小化以下形式的能量来计算估计值 $x \\in \\mathbb{R}^{N}$\n$$\n\\min_{x \\in \\mathbb{R}^{N}} \\; \\frac{1}{2}\\lVert x - y \\rVert_{2}^{2} + \\lambda \\, R(x),\n$$\n其中 $\\lambda  0$ 控制正则化的强度，$R(x)$ 是一个编码了关于 $x$ 的先验知识的惩罚项。在本问题中，您将比较在逆问题正则化方法中广泛使用的三种惩罚项：\n\n- 全变分 (Total Variation, TV): $R_{\\mathrm{TV}}(x) = \\sum_{i=1}^{N-1} \\lvert x_{i+1} - x_{i} \\rvert$。\n- Huber化全变分: $R_{\\mathrm{Huber},\\delta}(x) = \\sum_{i=1}^{N-1} \\phi_{\\delta}(x_{i+1} - x_{i})$，其中 Huber 函数为\n$$\n\\phi_{\\delta}(d) =\n\\begin{cases}\n\\frac{1}{2\\delta} d^{2},  \\text{if } \\lvert d \\rvert \\le \\delta, \\\\\n\\lvert d \\rvert - \\frac{\\delta}{2},  \\text{if } \\lvert d \\rvert  \\delta,\n\\end{cases}\n$$\n参数 $\\delta  0$。\n- Perona–Malik 型惩罚项: $R_{\\mathrm{PM},k}(x) = \\sum_{i=1}^{N-1} \\psi_{k}(x_{i+1} - x_{i})$，其中\n$$\n\\psi_{k}(d) = \\frac{k^{2}}{2} \\log\\!\\big(1 + (d/k)^{2}\\big),\n$$\n参数 $k  0$。\n\n目标是凭经验证明，诸如 Huber化全变分和 Perona–Malik 型惩罚项这类保边惩罚项，可以在避免对小梯度过度平滑的同时，减少标准全变分中常见的“阶梯效应”。\n\n构建一个合成的基准真相信号 $x^{\\star} \\in \\mathbb{R}^{N}$，该信号同时包含一个小的非零梯度区域和一个锐利的边缘：\n- 令 $N = 256$。\n- 定义一个从索引 $i_{0}$ 开始到索引 $i_{1}$ 结束、斜率恒为 $m  0$ 的斜坡，以及在索引 $j$ 处的一个振幅为 $A  0$ 的锐利阶跃。\n- 精确地，对于索引 $i \\in \\{0,1,\\dots,N-1\\}$，\n  - $x^{\\star}_{i} = 0$ 对于 $i  i_{0}$，\n  - $x^{\\star}_{i} = m\\,(i - i_{0})$ 对于 $i_{0} \\le i  i_{1}$，\n  - $x^{\\star}_{i} = m\\,(i_{1} - i_{0})$ 对于 $i_{1} \\le i  j$，\n  - $x^{\\star}_{i} = m\\,(i_{1} - i_{0}) + A$ 对于 $i \\ge j$。\n\n生成含噪数据 $y = x^{\\star} + \\eta$，其中 $\\eta$ 是均值为零、方差为 $\\sigma^{2}$ 的独立同分布高斯噪声。\n\n对于每个惩罚项，使用相同的数据 $y$ 和正则化参数 $\\lambda$ 计算相应能量的数值最小化器。为每个惩罚项使用合适的一阶迭代方法：\n- 对于不可微的全变分，使用一种基于凸对偶和对偶可行集投影的可证明收敛的一阶方法。\n- 对于可微的惩罚项 $R_{\\mathrm{Huber},\\delta}$ 和 $R_{\\mathrm{PM},k}$，使用一种带有保证能量下降的单调线搜索的梯度下降法。\n\n设计定量度量来比较在斜坡区域 $\\{i_{0}, i_{0}+1, \\dots, i_{1}-1\\}$ 的重建结果：\n- 定义离散梯度 $g_{i} = x_{i+1} - x_{i}$ 对于 $i \\in \\{0,1,\\dots,N-2\\}$。\n- 在斜坡区域，计算“平台比率” $P(x)$，即索引 $i \\in \\{i_{0},\\dots,i_{1}-2\\}$ 中满足 $\\lvert g_{i} \\rvert  \\tau$ 的比例，其中 $\\tau$ 是一个固定阈值，取为真实斜率 $m$ 的一小部分。\n- 将斜坡区域上 $g_{i}$ 的均值作为斜率的估计值，并报告绝对斜率误差 $\\lvert \\widehat{m}(x) - m \\rvert$。\n\n您的程序必须构建信号、添加噪声、求解三个最小化问题，并根据以下逻辑标准，评估与标准全变分相比，每种保边惩罚项是否在不导致小梯度过度平滑的情况下减少了阶梯效应：\n- 令 $x^{\\mathrm{TV}}$ 表示全变分法的解，$x^{\\mathrm{E}}$ 表示一个保边惩罚项的解（Huber化全变分或 Perona–Malik 型）。\n- 如果 $P(x^{\\mathrm{E}})  P(x^{\\mathrm{TV}}) - \\varepsilon_{P}$（对于一个固定的边际值 $\\varepsilon_{P}  0$），则声明“减少了阶梯效应”。\n- 如果 $\\lvert \\widehat{m}(x^{\\mathrm{E}}) - m \\rvert  \\lvert \\widehat{m}(x^{\\mathrm{TV}}) - m \\rvert + \\varepsilon_{m}$（对于一个固定的边际值 $\\varepsilon_{m}  0$），则声明“未过度平滑小梯度”。\n\n您的程序应运行以下测试套件，其中每个测试都定义了所有参数和一个用于可复现性的固定随机种子：\n- 测试 1: $N=256$, $i_{0}=30$, $i_{1}=130$, $j=170$, $m=0.015$, $A=1.0$, $\\sigma=0.08$, $\\lambda=0.35$, $\\delta=0.02$, $k=0.06$, $\\tau=0.5\\,m$, $\\varepsilon_{P}=0.08$, $\\varepsilon_{m}=0.0015$, seed $=2026$。\n- 测试 2: $N=256$, $i_{0}=30$, $i_{1}=130$, $j=170$, $m=0.015$, $A=1.0$, $\\sigma=0.08$, $\\lambda=0.60$, $\\delta=0.02$, $k=0.06$, $\\tau=0.5\\,m$, $\\varepsilon_{P}=0.08$, $\\varepsilon_{m}=0.0015$, seed $=2027$。\n- 测试 3: $N=256$, $i_{0}=30$, $i_{1}=130$, $j=170$, $m=0.015$, $A=1.0$, $\\sigma=0.12$, $\\lambda=0.35$, $\\delta=0.02$, $k=0.06$, $\\tau=0.5\\,m$, $\\varepsilon_{P}=0.08$, $\\varepsilon_{m}=0.0015$, seed $=2028$。\n\n对于每个测试，产生两个布尔值结果：\n- 一个布尔值用于 Huber化全变分，指示相对于全变分是否同时满足两个条件。\n- 一个布尔值用于 Perona–Malik 型惩罚项，指示相对于全变分是否同时满足两个条件。\n\n最终输出格式：您的程序应生成单行输出，其中包含一个列表，按测试和方法的顺序列出六个布尔值结果，形式为方括号括起来的逗号分隔列表，即 $[\\text{Huber}_{1}, \\text{PM}_{1}, \\text{Huber}_{2}, \\text{PM}_{2}, \\text{Huber}_{3}, \\text{PM}_{3}]$，其中每个条目为 $\\text{True}$ 或 $\\text{False}$。", "solution": "该问题提出了一个适定的数值实验，旨在比较不同正则化惩罚项在一维信号去噪中的性能。该问题在科学上基于变分逆问题的原理，自成体系，包含了所有必要的参数和定义，其目标可通过指定的计算程序进行形式化验证。因此，该问题被认为是有效的。\n\n核心任务是通过最小化以下形式的泛函，从含噪观测值 $y \\in \\mathbb{R}^{N}$ 中找到真实信号 $x^{\\star} \\in \\mathbb{R}^{N}$ 的一个估计 $x \\in \\mathbb{R}^{N}$：\n$$\n\\min_{x \\in \\mathbb{R}^{N}} E(x) = \\frac{1}{2}\\lVert x - y \\rVert_{2}^{2} + \\lambda \\, R(x)\n$$\n此处，$\\frac{1}{2}\\lVert x - y \\rVert_{2}^{2}$ 是数据保真项，它促使解 $x$ 接近测量值 $y$。$R(x)$ 项是一个正则化惩罚项，由参数 $\\lambda  0$ 加权，它编码了关于信号 $x^{\\star}$ 结构的先验知识。我们将比较三种特定的正则化项。\n\n令 $D: \\mathbb{R}^{N} \\to \\mathbb{R}^{N-1}$ 为离散前向差分算子，使得 $(Dx)_i = x_{i+1} - x_i$ 对于 $i \\in \\{0, \\dots, N-2\\}$。这三种正则化项用离散梯度 $Dx$ 表示如下：\n\n1.  **全变分 (Total Variation, TV)**: $R_{\\mathrm{TV}}(x) = \\sum_{i=0}^{N-2} \\lvert (Dx)_i \\rvert = \\lVert Dx \\rVert_{1}$。该惩罚项是凸的，并能促进分段常数解，这可能在平滑梯度区域导致一种称为“阶梯效应”的不良伪影。\n\n2.  **Huber化全变分 (Huber-TV)**: $R_{\\mathrm{Huber},\\delta}(x) = \\sum_{i=0}^{N-2} \\phi_{\\delta}((Dx)_i)$，其中 Huber 函数 $\\phi_{\\delta}$ 定义为：\n    $$\n    \\phi_{\\delta}(d) =\n    \\begin{cases}\n    \\frac{1}{2\\delta} d^{2},  \\text{if } \\lvert d \\rvert \\le \\delta \\\\\n    \\lvert d \\rvert - \\frac{\\delta}{2},  \\text{if } \\lvert d \\rvert  \\delta\n    \\end{cases}\n    $$\n    该惩罚项也是凸的。它对小梯度（$\\lvert d \\rvert \\le \\delta$）呈二次方特性，对大梯度呈线性特性，从而提供了对 $\\ell_1$ 范数的光滑近似，旨在减少阶梯效应。\n\n3.  **Perona–Malik 型 (PM)**: $R_{\\mathrm{PM},k}(x) = \\sum_{i=0}^{N-2} \\psi_{k}((Dx)_i)$，其势函数为：\n    $$\n    \\psi_{k}(d) = \\frac{k^{2}}{2} \\log\\!\\left(1 + \\left(\\frac{d}{k}\\right)^{2}\\right)\n    $$\n    该惩罚项是非凸的，被称为保边势函数。它对大梯度的惩罚比小梯度轻，这可以在平滑噪声的同时保留锐利边缘。\n\n**数值最小化算法**\n\n为了找到每个正则化项的最小化器 $x$，我们采用合适的一阶迭代方法。\n\n**全变分最小化**：带有 TV 正则化项的能量泛函 $E(x)$ 是凸的，但由于 $\\ell_1$ 范数而不可微。按照建议，我们使用一种基于凸对偶的方法。原始问题是 $\\min_{x} \\frac{1}{2}\\lVert x - y \\rVert_{2}^{2} + \\lambda \\lVert Dx \\rVert_{1}$。其 Fenchel-Rockafellar 对偶问题是关于对偶变量 $p \\in \\mathbb{R}^{N-1}$ 最小化以下函数：\n$$\n\\min_{p: \\lVert p \\rVert_{\\infty} \\le 1} \\frac{1}{2} \\left\\lVert y - \\lambda D^{*}p \\right\\rVert_{2}^{2}\n$$\n其中 $D^{*}: \\mathbb{R}^{N-1} \\to \\mathbb{R}^{N}$ 是前向差分算子（负散度）的伴随算子。这是一个在简单凸集（无穷范数球）上的可微二次优化问题。我们使用投影梯度下降法来求解。对 $p$ 的迭代更新为：\n$$\np^{k+1} = \\text{proj}_{\\lVert \\cdot \\rVert_{\\infty} \\le 1} \\left( p^{k} - \\alpha_{k} \\nabla_{p} f(p^k) \\right)\n$$\n其中 $f(p) = \\frac{1}{2} \\lVert y - \\lambda D^{*}p \\rVert_{2}^{2}$，其梯度为 $\\nabla_{p}f(p) = -\\lambda D(y - \\lambda D^{*}p)$。投影 $\\text{proj}_{\\lVert \\cdot \\rVert_{\\infty} \\le 1}(q)$ 是将向量 $q$ 逐元素裁剪到区间 $[-1, 1]$ 内的简单操作。步长 $\\alpha_k$ 的选择必须保证收敛性；一个常数步长 $\\alpha  2/L$ 即可，其中 $L$ 是 $\\nabla_p f(p)$ 的 Lipschitz 常数，即 $L = \\lambda^2 \\lVert DD^{*} \\rVert_2$。对于一维情况，$\\lVert DD^{*} \\rVert_2 \\le 4$。一旦找到最优对偶变量 $p^{\\star}$，就可以通过关系式 $x^{\\star} = y - \\lambda D^{*}p^{\\star}$ 恢复原始解。\n\n**Huber-TV 和 PM 最小化**：Huber-TV 和 PM 惩罚项的能量泛函是可微的。因此，我们可以使用梯度下降法。能量泛函 $E(x)$ 的梯度是：\n$$\n\\nabla E(x) = (x - y) + \\lambda \\nabla R(x)\n$$\n正则化项的梯度 $\\nabla R(x)$ 可以紧凑地表示为 $\\nabla R(x) = D^{*}\\left( \\rho'(Dx) \\right)$，其中 $\\rho'$ 是势函数（$\\phi'_{\\delta}$ 或 $\\psi'_k$）的导数，逐元素应用于差分向量 $Dx$。导数分别为：\n- 对于 Huber-TV：$\\phi'_{\\delta}(d) = \\text{clip}(d/\\delta, -1, 1)$。\n- 对于 PM：$\\psi'_{k}(d) = \\frac{d}{1 + (d/k)^2}$。\n梯度下降的更新公式为 $x^{k+1} = x^{k} - \\alpha_{k} \\nabla E(x^{k})$。为满足单调线搜索的要求，步长 $\\alpha_k$ 在每次迭代中使用回溯线搜索来确定。我们从一个试验步长 $\\alpha$ 开始，并以一个因子 $\\beta \\in (0,1)$ 将其减小，直到满足 Armijo 条件：\n$$\nE(x^k - \\alpha \\nabla E(x^k)) \\le E(x^k) - c \\alpha \\lVert \\nabla E(x^k) \\rVert_2^2\n$$\n对于一个常数 $c \\in (0,1)$，通常取 $c=10^{-4}$。这保证了能量在每一步都会减少，从而收敛到一个（局部）最小值。\n\n**评估与比较**\n\n构建一个带有斜坡（小的恒定梯度）和锐利阶跃的合成信号 $x^{\\star}$。添加高斯噪声以形成观测值 $y$。在计算出重建结果 $x^{\\mathrm{TV}}$、$x^{\\mathrm{Huber}}$ 和 $x^{\\mathrm{PM}}$ 后，我们根据在斜坡区域计算的两个定量指标对它们进行评估：\n1.  **平台比率 $P(x)$**：斜坡段中梯度幅值接近于零（具体来说，小于阈值 $\\tau$）的梯度所占的比例。高值表示存在显著的阶梯效应。\n    $P(x) = \\frac{|\\{ i \\in \\{i_{0}, \\dots, i_{1}-2\\} \\,:\\, |x_{i+1}-x_i|  \\tau \\}|}{i_1 - i_0 - 1}$\n2.  **绝对斜率误差**：真实斜坡斜率 $m$ 与从重建结果在斜坡区域估计的平均斜率 $\\widehat{m}(x)$ 之间的绝对差值。\n    $\\widehat{m}(x) = \\frac{1}{i_1 - i_0 - 1}\\sum_{i=i_0}^{i_1-2} (x_{i+1}-x_i)$\n\n如果一个保边惩罚项（Huber 或 PM）相对于 TV 同时满足以下条件，则认为其表现更优：\n- **减少阶梯效应**：$P(x^{\\mathrm{E}})  P(x^{\\mathrm{TV}}) - \\varepsilon_{P}$\n- **未过度平滑小梯度**：$\\lvert \\widehat{m}(x^{\\mathrm{E}}) - m \\rvert  \\lvert \\widehat{m}(x^{\\mathrm{TV}}) - m \\rvert + \\varepsilon_{m}$\n其中 $x^{\\mathrm{E}}$ 是来自保边惩罚项的重建结果，$\\varepsilon_P, \\varepsilon_m$ 是小的正边际值。下面的实现为指定的测试套件执行了此比较。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\n# from scipy import ... # No scipy used\n\ndef solve():\n    \"\"\"\n    Main function to run the numerical experiment comparing regularization penalties.\n    \"\"\"\n\n    test_cases = [\n        {'N': 256, 'i0': 30, 'i1': 130, 'j': 170, 'm': 0.015, 'A': 1.0, 'sigma': 0.08, 'lambda_reg': 0.35, 'delta': 0.02, 'k': 0.06, 'tau_frac': 0.5, 'eps_p': 0.08, 'eps_m': 0.0015, 'seed': 2026},\n        {'N': 256, 'i0': 30, 'i1': 130, 'j': 170, 'm': 0.015, 'A': 1.0, 'sigma': 0.08, 'lambda_reg': 0.60, 'delta': 0.02, 'k': 0.06, 'tau_frac': 0.5, 'eps_p': 0.08, 'eps_m': 0.0015, 'seed': 2027},\n        {'N': 256, 'i0': 30, 'i1': 130, 'j': 170, 'm': 0.015, 'A': 1.0, 'sigma': 0.12, 'lambda_reg': 0.35, 'delta': 0.02, 'k': 0.06, 'tau_frac': 0.5, 'eps_p': 0.08, 'eps_m': 0.0015, 'seed': 2028},\n    ]\n\n    # --- Finite Difference Operators ---\n    def D_op(x):\n        \"\"\"Forward difference operator D.\"\"\"\n        return x[1:] - x[:-1]\n\n    def DT_op(p):\n        \"\"\"Adjoint of D (negative divergence).\"\"\"\n        N = p.shape[0] + 1\n        res = np.zeros(N, dtype=p.dtype)\n        res[1:] += p\n        res[:-1] -= p\n        return res\n\n    # --- Signal Generation ---\n    def generate_signal(N, i0, i1, j, m, A):\n        x_star = np.zeros(N)\n        indices = np.arange(N)\n        ramp_val = m * (i1 - i0)\n        \n        # Ramp region\n        mask_ramp = (indices >= i0)  (indices  i1)\n        x_star[mask_ramp] = m * (indices[mask_ramp] - i0)\n        \n        # Plateau after ramp\n        mask_plateau = (indices >= i1)  (indices  j)\n        x_star[mask_plateau] = ramp_val\n        \n        # Step region\n        mask_step = (indices >= j)\n        x_star[mask_step] = ramp_val + A\n        \n        return x_star\n\n    # --- Solvers ---\n    MAX_ITER = 1000\n\n    def solve_tv(y, lambda_reg):\n        \"\"\"Solves the TV denoising problem using dual projected gradient ascent.\"\"\"\n        N = len(y)\n        p = np.zeros(N - 1)\n        # Spectral norm of D is ~2, so ||DD*|| is ~4.\n        # Step size  2 / (lambda^2 * ||DD*||) ~= 1/(2*lambda^2)\n        step_size = 0.4 / (lambda_reg**2) \n\n        for _ in range(MAX_ITER):\n            grad = -lambda_reg * D_op(y - lambda_reg * DT_op(p))\n            p_new = p - step_size * grad\n            p = np.clip(p_new, -1.0, 1.0)\n            \n        x_rec = y - lambda_reg * DT_op(p)\n        return x_rec\n\n    def solve_differentiable(y, lambda_reg, reg_type, param):\n        \"\"\"Solves denoising for differentiable regularizers (Huber, PM) using gradient descent.\"\"\"\n        x = y.copy()\n        N = len(y)\n\n        # Backtracking line search parameters\n        alpha_init = 1.0\n        beta = 0.5\n        c_armijo = 1e-4\n\n        for _ in range(MAX_ITER):\n            d = D_op(x)\n            \n            # Compute gradient of regularizer\n            if reg_type == 'huber':\n                grad_R_term = np.clip(d / param, -1.0, 1.0)\n            elif reg_type == 'pm':\n                grad_R_term = d / (1 + (d / param)**2)\n            else:\n                raise ValueError(\"Unknown regularization type\")\n\n            grad_E = (x - y) + lambda_reg * DT_op(grad_R_term)\n\n            # Compute current energy\n            if reg_type == 'huber':\n                mask = np.abs(d) = param\n                R_val = np.sum(0.5 / param * d[mask]**2) + np.sum(np.abs(d[~mask]) - 0.5 * param)\n            elif reg_type == 'pm':\n                R_val = np.sum(0.5 * param**2 * np.log(1 + (d / param)**2))\n            \n            E_current = 0.5 * np.sum((x - y)**2) + lambda_reg * R_val\n            \n            # Backtracking line search\n            alpha = alpha_init\n            grad_E_norm_sq = np.sum(grad_E**2)\n            \n            while True:\n                x_new = x - alpha * grad_E\n                d_new = D_op(x_new)\n\n                if reg_type == 'huber':\n                    mask_new = np.abs(d_new) = param\n                    R_val_new = np.sum(0.5 / param * d_new[mask_new]**2) + np.sum(np.abs(d_new[~mask_new]) - 0.5 * param)\n                elif reg_type == 'pm':\n                    R_val_new = np.sum(0.5 * param**2 * np.log(1 + (d_new / param)**2))\n                \n                E_new = 0.5 * np.sum((x_new - y)**2) + lambda_reg * R_val_new\n                \n                if E_new = E_current - c_armijo * alpha * grad_E_norm_sq:\n                    break\n                alpha *= beta\n                if alpha  1e-9: # Failsafe\n                    break\n\n            x = x - alpha * grad_E\n\n        return x\n\n    # --- Evaluation Metrics ---\n    def evaluate_reconstruction(x, i0, i1, m, tau):\n        \"\"\"Calculates plateau ratio and slope error.\"\"\"\n        ramp_grads = D_op(x[i0:i1])\n        num_grads = len(ramp_grads)\n        \n        # Plateau ratio\n        plateau_count = np.sum(np.abs(ramp_grads)  tau)\n        plateau_ratio = plateau_count / num_grads if num_grads > 0 else 0.0\n        \n        # Slope error\n        m_hat = np.mean(ramp_grads) if num_grads > 0 else 0.0\n        slope_error = np.abs(m_hat - m)\n        \n        return plateau_ratio, slope_error\n\n    # --- Main Loop ---\n    final_results = []\n    for params in test_cases:\n        # Set parameters for the current test\n        N, i0, i1, j = params['N'], params['i0'], params['i1'], params['j']\n        m, A, sigma = params['m'], params['A'], params['sigma']\n        lambda_reg, delta, k = params['lambda_reg'], params['delta'], params['k']\n        tau = params['tau_frac'] * m\n        eps_p, eps_m = params['eps_p'], params['eps_m']\n        seed = params['seed']\n        \n        # Generate signal and noise\n        np.random.seed(seed)\n        x_star = generate_signal(N, i0, i1, j, m, A)\n        noise = np.random.normal(0, sigma, N)\n        y = x_star + noise\n        \n        # Solve for all three regularizers\n        x_tv = solve_tv(y, lambda_reg)\n        x_hub = solve_differentiable(y, lambda_reg, 'huber', delta)\n        x_pm = solve_differentiable(y, lambda_reg, 'pm', k)\n        \n        # Evaluate metrics\n        p_tv, m_err_tv = evaluate_reconstruction(x_tv, i0, i1, m, tau)\n        p_hub, m_err_hub = evaluate_reconstruction(x_hub, i0, i1, m, tau)\n        p_pm, m_err_pm = evaluate_reconstruction(x_pm, i0, i1, m, tau)\n        \n        # Apply comparison criteria\n        hub_better = (p_hub  p_tv - eps_p) and (m_err_hub  m_err_tv + eps_m)\n        pm_better = (p_pm  p_tv - eps_p) and (m_err_pm  m_err_tv + eps_m)\n        \n        final_results.extend([hub_better, pm_better])\n        \n    print(f\"[{','.join(map(str, final_results))}]\")\n\nsolve()\n```", "id": "3185682"}]}