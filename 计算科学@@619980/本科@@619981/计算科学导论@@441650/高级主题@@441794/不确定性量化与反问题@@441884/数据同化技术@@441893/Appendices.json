{"hands_on_practices": [{"introduction": "变分数据同化方法通过最小化一个衡量模型状态与观测之间差异的代价函数来工作。为了高效地找到这个最小值，我们需要计算代价函数的梯度，这依赖于观测算子的“伴随”。本练习提供了一种动手验证基本伴随性质的方法，通常称为“点积检验”，它确保我们在将数学工具用于更庞大的优化系统之前，已经正确地推导和实现了它们 [@problem_id:3116123]。", "problem": "在变分资料同化中，观测算子将状态向量映射到观测空间。考虑状态 $x = (T, q)^{\\top}$ 的非线性观测算子 $H: \\mathbb{R}^{2} \\to \\mathbb{R}^{3}$ 定义为\n$$\nH(x) = \\begin{pmatrix}\nT^{2} \\\\\n\\exp(q) \\\\\nT\\,q\n\\end{pmatrix}.\n$$\n假设给定背景状态 $x_{b} = (T_{b}, q_{b})^{\\top}$，其中 $T_{b} = 2$ 且 $q_{b} = 0$。使用关于 $x_{b}$ 的一阶泰勒展开，在 $x_{b}$ 处的线性化观测算子由在 $x_{b}$ 处求值的雅可比矩阵表示，记为 $H'(x_{b})$。在状态空间和观测空间中均使用标准欧几里得内积。\n\n给定状态扰动 $\\delta x = \\begin{pmatrix} 1 \\\\ -3 \\end{pmatrix}$ 和观测空间方向 $\\delta y = \\begin{pmatrix} 2 \\\\ 5 \\\\ -1 \\end{pmatrix}$。请执行以下步骤：\n\n1. 通过计算 $H$ 的雅可比矩阵并在 $x_{b}$ 处求值，显式地导出 $H'(x_{b})$ 作为一个 $3 \\times 2$ 矩阵。\n2. 在 $\\mathbb{R}^{2}$ 和 $\\mathbb{R}^{3}$ 上的欧几里得内积下，确定 $H'(x_{b})$ 的伴随算子。\n3. 计算 $H'(x_{b})\\,\\delta x$ 与 $\\delta y$ 的欧几里得内积。\n4. 计算 $\\delta x$ 与作用于 $\\delta y$ 的 $H'(x_{b})$ 的伴随算子的欧几里得内积。\n\n将步骤3和4中得到的共同标量值报告为一个无单位的精确整数。不要四舍五入；请提供精确值。", "solution": "该问题经验证具有科学依据、是适定的、客观且完整的。所有必要的数据和定义都已提供，任务在数学上是合理的且没有歧义。因此，我们可以开始求解。\n\n该问题要求我们进行一系列计算，涉及非线性观测算子 $H$、其线性化 $H'$ 以及相应的伴随算子。状态向量为 $x = \\begin{pmatrix} T \\\\ q \\end{pmatrix} \\in \\mathbb{R}^{2}$，观测算子为 $H: \\mathbb{R}^{2} \\to \\mathbb{R}^{3}$，定义如下：\n$$\nH(x) = H(T, q) = \\begin{pmatrix}\nT^{2} \\\\\n\\exp(q) \\\\\nTq\n\\end{pmatrix}\n$$\n背景状态给定为 $x_{b} = \\begin{pmatrix} T_{b} \\\\ q_{b} \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 0 \\end{pmatrix}$。\n\n**步骤1：导出线性化算子 $H'(x_{b})$**\n\n线性化观测算子 $H'(x)$ 是 $H$ 的雅可比矩阵。$H$ 的分量为 $H_1(T, q) = T^2$，$H_2(T, q) = \\exp(q)$ 和 $H_3(T, q) = Tq$。我们计算关于 $T$ 和 $q$ 的偏导数：\n$$\n\\frac{\\partial H_1}{\\partial T} = 2T, \\quad \\frac{\\partial H_1}{\\partial q} = 0\n$$\n$$\n\\frac{\\partial H_2}{\\partial T} = 0, \\quad \\frac{\\partial H_2}{\\partial q} = \\exp(q)\n$$\n$$\n\\frac{\\partial H_3}{\\partial T} = q, \\quad \\frac{\\partial H_3}{\\partial q} = T\n$$\n因此，雅可比矩阵 $H'(x)$ 为：\n$$\nH'(x) = \\begin{pmatrix}\n\\frac{\\partial H_1}{\\partial T}  \\frac{\\partial H_1}{\\partial q} \\\\\n\\frac{\\partial H_2}{\\partial T}  \\frac{\\partial H_2}{\\partial q} \\\\\n\\frac{\\partial H_3}{\\partial T}  \\frac{\\partial H_3}{\\partial q}\n\\end{pmatrix} = \\begin{pmatrix}\n2T  0 \\\\\n0  \\exp(q) \\\\\nq  T\n\\end{pmatrix}\n$$\n现在，我们在背景状态 $x_{b} = (T_{b}, q_{b})^{\\top} = (2, 0)^{\\top}$ 处计算该雅可比矩阵：\n$$\nH'(x_{b}) = H'(2, 0) = \\begin{pmatrix}\n2(2)  0 \\\\\n0  \\exp(0) \\\\\n0  2\n\\end{pmatrix} = \\begin{pmatrix}\n4  0 \\\\\n0  1 \\\\\n0  2\n\\end{pmatrix}\n$$\n这就是在 $x_{b}$ 处线性化算子所要求的 $3 \\times 2$ 矩阵表示。\n\n**步骤2：确定 $H'(x_{b})$ 的伴随算子**\n\n问题指明在状态空间 $\\mathbb{R}^{2}$ 和观测空间 $\\mathbb{R}^{3}$ 中都使用标准欧几里得内积。对于一个在有限维欧几里得空间之间由实矩阵 $M$ 表示的线性算子，其伴随算子 $M^*$ 由该矩阵的转置 $M^{\\top}$ 表示。\n令 $M = H'(x_{b})$。$M$ 的伴随算子，记为 $M^*$，对于任意 $\\delta x \\in \\mathbb{R}^{2}$ 和 $\\delta y \\in \\mathbb{R}^{3}$，满足以下关系：\n$$\n\\langle M \\delta x, \\delta y \\rangle = \\langle \\delta x, M^* \\delta y \\rangle\n$$\n在标准欧几里得内积（点积）下，这等价于 $(M \\delta x)^{\\top} \\delta y = (\\delta x)^{\\top} (M^* \\delta y)$。由于 $(M \\delta x)^{\\top} = (\\delta x)^{\\top} M^{\\top}$，可得 $M^* = M^{\\top}$。\n因此，$H'(x_{b})$ 的伴随算子是其转置 $(H'(x_{b}))^{\\top}$：\n$$\n(H'(x_{b}))^{\\top} = \\begin{pmatrix}\n4  0 \\\\\n0  1 \\\\\n0  2\n\\end{pmatrix}^{\\top} = \\begin{pmatrix}\n4  0  0 \\\\\n0  1  2\n\\end{pmatrix}\n$$\n\n**步骤3：计算 $H'(x_{b})\\,\\delta x$ 与 $\\delta y$ 的内积**\n\n我们已知状态扰动 $\\delta x = \\begin{pmatrix} 1 \\\\ -3 \\end{pmatrix}$ 和观测空间方向 $\\delta y = \\begin{pmatrix} 2 \\\\ 5 \\\\ -1 \\end{pmatrix}$。\n首先，我们将线性化算子 $H'(x_{b})$ 作用于 $\\delta x$：\n$$\nH'(x_{b})\\,\\delta x = \\begin{pmatrix}\n4  0 \\\\\n0  1 \\\\\n0  2\n\\end{pmatrix}\n\\begin{pmatrix}\n1 \\\\\n-3\n\\end{pmatrix} = \\begin{pmatrix}\n4(1) + 0(-3) \\\\\n0(1) + 1(-3) \\\\\n0(1) + 2(-3)\n\\end{pmatrix} = \\begin{pmatrix}\n4 \\\\\n-3 \\\\\n-6\n\\end{pmatrix}\n$$\n接下来，我们计算这个得到的向量与 $\\delta y$ 的欧几里得内积（点积）：\n$$\n\\langle H'(x_{b})\\,\\delta x, \\delta y \\rangle = \\begin{pmatrix}\n4 \\\\\n-3 \\\\\n-6\n\\end{pmatrix} \\cdot \\begin{pmatrix}\n2 \\\\\n5 \\\\\n-1\n\\end{pmatrix} = 4(2) + (-3)(5) + (-6)(-1) = 8 - 15 + 6 = -1\n$$\n\n**步骤4：计算 $\\delta x$ 与作用于 $\\delta y$ 的 $H'(x_{b})$ 的伴随算子的内积**\n\n首先，我们将伴随算子 $(H'(x_{b}))^{\\top}$ 作用于 $\\delta y$：\n$$\n(H'(x_{b}))^{\\top} \\delta y = \\begin{pmatrix}\n4  0  0 \\\\\n0  1  2\n\\end{pmatrix}\n\\begin{pmatrix}\n2 \\\\\n5 \\\\\n-1\n\\end{pmatrix} = \\begin{pmatrix}\n4(2) + 0(5) + 0(-1) \\\\\n0(2) + 1(5) + 2(-1)\n\\end{pmatrix} = \\begin{pmatrix}\n8 \\\\\n5 - 2\n\\end{pmatrix} = \\begin{pmatrix}\n8 \\\\\n3\n\\end{pmatrix}\n$$\n接下来，我们计算 $\\delta x$ 与这个得到的向量的欧几里得内积：\n$$\n\\langle \\delta x, (H'(x_{b}))^{\\top} \\delta y \\rangle = \\begin{pmatrix}\n1 \\\\\n-3\n\\end{pmatrix} \\cdot \\begin{pmatrix}\n8 \\\\\n3\n\\end{pmatrix} = 1(8) + (-3)(3) = 8 - 9 = -1\n$$\n正如伴随算子的定义所预期的，步骤3和步骤4的结果是相同的。共同的标量值为 $-1$。", "answer": "$$\n\\boxed{-1}\n$$", "id": "3116123"}, {"introduction": "在掌握了线性化的概念后，这个实践将我们从理论推向应用，通过为一个变分数据同化问题实现一个迭代求解器。你将使用高斯-牛顿法，该方法通过反复线性化观测算子来寻找最小化代价函数的最优状态。这个练习将让你对数据同化中的优化过程有一个实践性的理解，并探索其成功如何依赖于初始猜测和指定的误差方差等因素 [@problem_id:3116124]。", "problem": "本题要求您为一个标量非线性数据同化问题推导并实现高斯-牛顿迭代，并通过计算来展示，其收敛性如何根据线性化点和观测误差方差而成功或失败。此推导的基本基础是加权最小二乘原理和一阶泰勒线性化。\n\n考虑一个标量状态变量 $x \\in \\mathbb{R}$，其估计值通过最小化一个三维变分 (3D-Var) 代价函数得到\n$$\nJ(x) = (x - x_b)^2 B^{-1} + \\left(y - h(x)\\right)^2 R^{-1},\n$$\n其中 $x_b \\in \\mathbb{R}$ 是背景场状态，$B \\in \\mathbb{R}_{0}$ 是背景场误差方差，$y \\in \\mathbb{R}$ 是观测值，$R \\in \\mathbb{R}_{0}$ 是观测误差方差，$h(x)$ 是一个非线性观测算子。高斯-牛顿法通过应用以下基本步骤得到：使用一阶泰勒近似在当前迭代点附近对非线性模型进行线性化，并使用加权最小二乘法方程最小化得到的二次模型。不要假设任何特定的“捷径”更新公式；相反，应从这些原理出发推导出更新公式。\n\n在本练习中，定义观测算子为 $h(x) = x^2$。任务是实现高斯-牛顿迭代，该迭代在每一步中，围绕当前迭代点构造一个线性化的观测模型，并计算能减小 $J(x)$ 的增量。使用一个停止准则，如果归一化观测失配\n$$\nm(x) = \\sqrt{\\frac{\\left(y - h(x)\\right)^2}{R}},\n$$\n小于容差 $m_{\\mathrm{tol}}$，或者如果绝对更新幅度小于增量容差 $\\varepsilon_{\\mathrm{inc}}$，则宣布成功。如果迭代停止时 $m(x)$ 仍然高于 $m_{\\mathrm{tol}}$，或者达到最大迭代次数仍未满足失配容差，则宣布失败。此问题不涉及角度，因此不需要角度单位。此问题中没有物理单位。\n\n实现您的程序以处理以下测试套件，每个测试用例由元组 $(y, x_b, B, R, x_0, \\varepsilon_{\\mathrm{inc}}, m_{\\mathrm{tol}}, N_{\\max})$ 指定：\n\n- 理想路径，收敛到正根：$(y, x_b, B, R, x_0, \\varepsilon_{\\mathrm{inc}}, m_{\\mathrm{tol}}, N_{\\max}) = (\\,4,\\,1.5,\\,1,\\,0.25,\\,1,\\,10^{-8},\\,10^{-6},\\,50\\,)$。\n- 在背景场引导下收敛到负根：$(\\,4,\\,-2.5,\\,0.5,\\,0.25,\\,-1,\\,10^{-8},\\,10^{-6},\\,50\\,)$。\n- 因线性化点导数为零且与背景场匹配而失败：$(\\,1,\\,0,\\,1,\\,0.25,\\,0,\\,10^{-8},\\,10^{-6},\\,50\\,)$。\n- 因观测误差方差非常大（观测可信度弱）而失败：$(\\,9,\\,-1,\\,0.1,\\,1000,\\,-1,\\,10^{-8},\\,10^{-6},\\,50\\,)$。\n- 在观测可信度强而背景场可信度弱的情况下成功收敛：$(\\,9,\\,0,\\,10,\\,0.01,\\,0.5,\\,10^{-8},\\,10^{-6},\\,50\\,)$。\n\n您的程序必须对每个测试用例，在当前迭代点使用一阶线性化来执行高斯-牛顿迭代。对于每个测试用例，返回一个列表，其中包含：一个指示成功（$\\mathrm{True}$ 或 $\\mathrm{False}$）的布尔值，最终估计值 $x$（浮点数），执行的迭代次数（整数），以及最终的归一化观测失配 $m(x)$（浮点数）。将所有测试用例的结果汇总到单行输出中，该行包含一个用方括号括起来的逗号分隔列表，其中不含空格，每个测试用例的结果本身也是一个同样用方括号括起来的逗号分隔格式的列表（例如：$[\\,[\\mathrm{True},2.0,12,0.0],\\ldots\\,]$）。", "solution": "该问题要求推导并实现高斯-牛顿法，以解决一个标量非线性数据同化问题。目标是找到最小化 3D-Var 代价函数的状态变量 $x \\in \\mathbb{R}$：\n$$\nJ(x) = (x - x_b)^2 B^{-1} + \\left(y - h(x)\\right)^2 R^{-1}\n$$\n其中 $x_b$ 是背景场状态，$B  0$ 是背景场误差方差，$y$ 是观测值，$R  0$ 是观测误差方差，$h(x)=x^2$ 是非线性观测算子。\n\n解是通过应用高斯-牛顿算法推导出来的，该算法包括迭代地对非线性模型进行线性化，并求解由此产生的线性最小二乘问题。\n\n**第一步：代价函数的线性化**\n\n高斯-牛顿法在每次迭代 $k$ 时，用围绕当前估计值 $x_k$ 的一阶泰勒展开来近似非线性函数 $h(x)$。我们寻求一个增量 $\\delta x$，使得下一个估计值 $x_{k+1} = x_k + \\delta x$ 更接近 $J(x)$ 的最小值。\n\n$h(x)$ 在 $x_k$ 周围的线性化为：\n$$\nh(x_{k+1}) = h(x_k + \\delta x) \\approx h(x_k) + H_k \\delta x\n$$\n其中 $H_k$ 是 $h(x)$ 关于 $x$ 的导数（雅可比），在 $x_k$ 处求值。对于给定的观测算子 $h(x) = x^2$，其导数为 $h'(x) = 2x$。因此，在 $x_k$ 处的雅可比为：\n$$\nH_k = 2x_k\n$$\n将线性化的观测算子代入代价函数 $J(x)$，我们得到关于增量 $\\delta x$ 的代价函数的二次近似，记为 $J_k(\\delta x)$：\n$$\nJ_k(\\delta x) = (x_k + \\delta x - x_b)^2 B^{-1} + \\left(y - (h(x_k) + H_k \\delta x)\\right)^2 R^{-1}\n$$\n这可以重写以强调包含 $\\delta x$ 的项：\n$$\nJ_k(\\delta x) = ((x_k - x_b) + \\delta x)^2 B^{-1} + ((y - h(x_k)) - H_k \\delta x)^2 R^{-1}\n$$\n\n**第二步：最小化与增量推导**\n\n为了找到最小化此二次函数 $J_k(\\delta x)$ 的增量 $\\delta x$，我们计算其关于 $\\delta x$ 的导数并令其为零。\n$$\n\\frac{dJ_k}{d(\\delta x)} = \\frac{d}{d(\\delta x)} \\left[ ((x_k - x_b) + \\delta x)^2 B^{-1} + ((y - h(x_k)) - H_k \\delta x)^2 R^{-1} \\right] = 0\n$$\n应用链式法则，我们得到：\n$$\n2((x_k - x_b) + \\delta x) \\cdot B^{-1} + 2((y - h(x_k)) - H_k \\delta x) \\cdot (-H_k) \\cdot R^{-1} = 0\n$$\n除以 $2$ 并展开各项：\n$$\nB^{-1}(x_k - x_b) + B^{-1}\\delta x - R^{-1}H_k(y - h(x_k)) + R^{-1}H_k^2 \\delta x = 0\n$$\n现在，我们将含有 $\\delta x$ 的项分组到方程的一边，其余项放在另一边：\n$$\n(B^{-1} + R^{-1}H_k^2) \\delta x = R^{-1}H_k(y - h(x_k)) - B^{-1}(x_k - x_b)\n$$\n为了求解 $\\delta x$，我们可以将整个方程乘以 $BR$ 来消去分母，这是允许的，因为 $B  0$ 且 $R  0$：\n$$\n(BR)(B^{-1} + R^{-1}H_k^2) \\delta x = (BR)(R^{-1}H_k(y - h(x_k)) - B^{-1}(x_k - x_b))\n$$\n$$\n(R + B H_k^2) \\delta x = B H_k(y - h(x_k)) - R(x_k - x_b)\n$$\n最后，分离出增量 $\\delta x$ 得到更新公式：\n$$\n\\delta x = \\frac{B H_k(y - h(x_k)) - R(x_k - x_b)}{R + B H_k^2}\n$$\n\n**第三步：迭代算法**\n\n完整的高斯-牛顿迭代算法如下：\n1.  **初始化**：在迭代 $k=0$ 时，从一个初始猜测值 $x_0$ 开始。\n2.  **迭代**：对于 $k = 0, 1, 2, \\dots, N_{\\max}-1$：\n    a.  计算观测算子值 $h(x_k) = x_k^2$ 和雅可比 $H_k = 2x_k$。\n    b.  使用推导出的公式计算增量 $\\delta x$：\n        $$\n        \\delta x = \\frac{B (2x_k)(y - x_k^2) - R(x_k - x_b)}{R + B (2x_k)^2}\n        $$\n    c.  更新状态估计：$x_{k+1} = x_k + \\delta x$。\n    d.  检查停止准则：\n        i.  **成功收敛（失配）**：如果归一化观测失配 $m(x_{k+1}) = \\sqrt{(y - h(x_{k+1}))^2 / R}$ 小于容差 $m_{\\mathrm{tol}}$，则迭代成功并终止。\n        ii.  **终止（增量）**：如果更新的绝对幅度 $|\\delta x|$ 小于容差 $\\varepsilon_{\\mathrm{inc}}$，则迭代终止。如果 $m(x_{k+1})  m_{\\mathrm{tol}}$，则结果为成功，否则为失败。\n3.  **终止（最大迭代次数）**：如果在 $N_{\\max}$ 次迭代后循环完成仍未满足成功准则，则过程终止并被视为失败。\n\n每个测试用例的最终输出将是一个列表，其中包含一个表示成功/失败的布尔值、最终估计值 $x$、执行的总迭代次数以及最终的归一化失配 $m(x)$。", "answer": "```python\nimport numpy as np\n\ndef gauss_newton_scalar(params):\n    \"\"\"\n    Performs Gauss-Newton iteration for a scalar 3D-Var problem.\n    \n    Args:\n        params (tuple): A tuple containing (y, xb, B, R, x0, eps_inc, m_tol, N_max).\n    \n    Returns:\n        list: [is_success, final_x, num_iterations, final_misfit]\n    \"\"\"\n    y, xb, B, R, x0, eps_inc, m_tol, N_max = params\n    \n    x_k = float(x0) # Ensure initial guess is a float\n    \n    # Check for convergence on the initial state (0 iterations)\n    h_x0 = x_k**2\n    # The problem statement guarantees R > 0, so no division by zero here.\n    m_x0 = np.sqrt((y - h_x0)**2 / R)\n    if m_x0  m_tol:\n        return [True, x_k, 0, m_x0]\n\n    for k in range(N_max):\n        # Current number of iterations performed to reach the next state is k+1\n        num_iters = k + 1\n\n        # Calculate values at the current iterate x_k\n        h_xk = x_k**2\n        H_k = 2.0 * x_k\n        \n        # Calculate the increment delta_x from the derived formula\n        numerator = B * H_k * (y - h_xk) - R * (x_k - xb)\n        denominator = R + B * H_k**2\n        \n        # Since R > 0 and B > 0, the denominator is always positive.\n        delta_x = numerator / denominator\n        \n        # Update the state\n        x_k_plus_1 = x_k + delta_x\n\n        # Calculate misfit at the new state\n        m_xk_plus_1 = np.sqrt((y - x_k_plus_1**2)**2 / R)\n\n        # Check stopping condition 1: small increment\n        if np.abs(delta_x)  eps_inc:\n            is_success = m_xk_plus_1  m_tol\n            return [is_success, x_k_plus_1, num_iters, m_xk_plus_1]\n            \n        # Check stopping condition 2: misfit tolerance met\n        if m_xk_plus_1  m_tol:\n            return [True, x_k_plus_1, num_iters, m_xk_plus_1]\n        \n        # Prepare for the next iteration\n        x_k = x_k_plus_1\n\n    # If the loop completes, it's a failure due to max iterations\n    # The final state is the last computed x_k, and num_iters is N_max\n    m_final = np.sqrt((y - x_k**2)**2 / R)\n    # The success condition will be false if execution reaches here, as the misfit check inside the loop would have already caught a success.\n    is_success = m_final  m_tol\n    return [is_success, x_k, N_max, m_final]\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    test_cases = [\n        # Case 1: Happy-path convergence toward a positive root\n        (4.0, 1.5, 1.0, 0.25, 1.0, 1e-8, 1e-6, 50),\n        # Case 2: Convergence toward a negative root guided by the background\n        (4.0, -2.5, 0.5, 0.25, -1.0, 1e-8, 1e-6, 50),\n        # Case 3: Failure due to a zero-derivative linearization point\n        (1.0, 0.0, 1.0, 0.25, 0.0, 1e-8, 1e-6, 50),\n        # Case 4: Failure caused by a very large observation error variance\n        (9.0, -1.0, 0.1, 1000.0, -1.0, 1e-8, 1e-6, 50),\n        # Case 5: Successful convergence with strongly trusted observations\n        (9.0, 0.0, 10.0, 0.01, 0.5, 1e-8, 1e-6, 50),\n    ]\n\n    results = []\n    for case in test_cases:\n        result = gauss_newton_scalar(case)\n        # Format the result for the inner list: [bool,float,int,float]\n        results.append(f\"[{str(result[0])},{float(result[1])},{int(result[2])},{float(result[3])}]\")\n\n    # Final print statement in the exact required format: [[...],[...],...]\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3116124"}, {"introduction": "与一次性处理所有观测的变分方法不同，序列方法逐个同化观测数据。由此产生了一个关键问题：同化的顺序会影响最终结果吗？本练习通过比较不同同化顺序的结果来探索这个问题。你将发现一个关键的理论区别：对于线性系统，最终状态是与顺序无关的，但当使用扩展卡尔曼滤波器（Extended Kalman Filter, EKF）等近似方法处理非线性系统时，这一性质便不再成立，这是在实际应用中必须考虑的重要因素 [@problem_id:3116062]。", "problem": "给定一个有限维状态向量 $\\,\\mathbf{x}\\in\\mathbb{R}^n\\,$，其高斯先验分布由均值 $\\,\\boldsymbol{\\mu}_0\\,$ 和协方差 $\\,\\mathbf{P}_0\\,$ 表征。一组观测数据由形式为 $\\,y_i = h_i(\\mathbf{x}) + \\varepsilon_i\\,$ 的标量观测 $\\,y_i\\,$ 组成，其中 $\\,h_i\\,$ 是观测算子，$\\,\\varepsilon_i\\,$ 是观测噪声。假设 $\\,\\varepsilon_i\\,$ 相互独立，各自服从零均值、方差为 $\\,r_i\\,$ 的高斯分布，因此在多观测意义上，观测误差协方差矩阵 $\\,\\mathbf{R}\\,$ 是对角的。在序列数据同化中，观测被逐一并入以更新状态。本问题要求您在两种场景下实现序列同化：使用线性观测算子和使用非线性观测算子。您必须比较两种不同的序列同化顺序，并测试最终的后验（均值和协方差）是否因同化顺序而改变。\n\n从贝叶斯法则的基本原理 $\\,p(\\mathbf{x}\\mid \\mathbf{y}) \\propto p(\\mathbf{y}\\mid \\mathbf{x})\\,p(\\mathbf{x})\\,$ 出发，结合高斯先验和高斯观测误差的假设。对于线性观测算子，利用独立高斯似然的乘积是可交换的，以及在线性-高斯假设下后验分布是高斯的这些基本事实。对于非线性观测算子，使用 $\\,h(\\mathbf{x})\\,$ 在当前均值附近的一阶泰勒近似来获得局部线性更新（首次出现时，Extended Kalman Filter 应完整写为 Extended Kalman Filter (EKF)）。您的程序必须实现从这些原则推导出的序列同化更新，而不能假设或使用问题陈述中明确提供的任何快捷公式。\n\n您的程序必须：\n- 将状态和协方差表示为 $\\,\\boldsymbol{\\mu}\\,$ 和 $\\,\\mathbf{P}\\,$。\n- 对每个带有算子 $\\,h(\\cdot)\\,$ 和噪声方差 $\\,r\\,$ 的观测 $\\,y\\,$，执行单次同化更新步骤。\n- 对于线性算子 $\\,h(\\mathbf{x})=\\mathbf{H}\\mathbf{x}\\,$, 将 $\\,\\mathbf{H}\\,$ 视为行向量并相应地更新 $\\,(\\boldsymbol{\\mu},\\mathbf{P})\\,$。\n- 对于非线性算子，使用 EKF 风格的局部线性化：$\\,h(\\mathbf{x}) \\approx h(\\boldsymbol{\\mu}) + \\mathbf{H}(\\boldsymbol{\\mu}) (\\mathbf{x}-\\boldsymbol{\\mu})\\,$，其中 $\\,\\mathbf{H}(\\boldsymbol{\\mu})\\,$ 是在当前均值处计算的雅可比矩阵，然后执行线性-高斯更新。\n- 对每个测试用例执行两种序列顺序，计算每种顺序得到的最终后验均值和协方差，并在严格的数值容差 $\\,\\epsilon\\,$ 内确定结果是否具有顺序不变性。\n\n使用以下测试套件和参数。在所有情况下，测量误差都是独立的，因此观测误差协方差 $\\,\\mathbf{R}\\,$ 是对角的，其各自的方差 $\\,r_i\\,$ 在下面指定。对于每个测试，比较两种序列顺序 $\\,(\\text{顺序 } A)\\,$ 和 $\\,(\\text{顺序 } B)\\,$。如果两种顺序的后验均值之间的最大绝对差和后验协方差之间的弗罗贝尼乌斯范数差均小于或等于 $\\,\\epsilon\\,$，则将顺序不变性报告为布尔值 $\\,\\text{True}\\,$，否则报告为 $\\,\\text{False}\\,$。使用数值容差 $\\,\\epsilon = 10^{-10}\\,$。\n\n测试用例 $\\,1\\,$ (线性，理想情况):\n- 状态维度 $\\,n = 2\\,$。\n- 先验 $\\,\\boldsymbol{\\mu}_0 = [\\,1.0,\\,-2.0\\,]\\,$, $\\,\\mathbf{P}_0 = \\begin{bmatrix} 2.0  0.5 \\\\ 0.5  1.0 \\end{bmatrix}\\,$。\n- 观测 $\\,1\\,$: $\\,y_1 = 0.9\\,$, $\\,h_1(\\mathbf{x}) = [\\,1,\\,0\\,]\\mathbf{x}\\,$, $\\,r_1 = 0.4\\,$。\n- 观测 $\\,2\\,$: $\\,y_2 = -1.8\\,$, $\\,h_2(\\mathbf{x}) = [\\,2,\\,-1\\,]\\mathbf{x}\\,$, $\\,r_2 = 0.3\\,$。\n- 顺序: $\\,(\\text{顺序 } A) = (1,2)\\,$, $\\,(\\text{顺序 } B) = (2,1)\\,$。\n预期行为：对于线性的 $\\,\\mathbf{H}\\,$ 和对角的 $\\,\\mathbf{R}\\,$，顺序不变性应该成立。\n\n测试用例 $\\,2\\,$ (线性，带有一个弱观测的边界条件):\n- 状态维度 $\\,n = 1\\,$。\n- 先验 $\\,\\mu_0 = 0.0\\,$, $\\,P_0 = 1.0\\,$。\n- 观测 $\\,1\\,$: $\\,y_1 = 1.0\\,$, $\\,h_1(x) = x\\,$, $\\,r_1 = 0.5\\,$。\n- 观测 $\\,2\\,$: $\\,y_2 = -2.0\\,$, $\\,h_2(x) = x\\,$, $\\,r_2 = 1000.0\\,$。\n- 顺序: $\\,(\\text{顺序 } A) = (1,2)\\,$, $\\,(\\text{顺序 } B) = (2,1)\\,$。\n预期行为：对于一个噪声非常大的第二次观测和对角的 $\\,\\mathbf{R}\\,$，顺序不变性应该成立。\n\n测试用例 $\\,3\\,$ (非线性，轻度非线性):\n- 状态维度 $\\,n = 1\\,$。\n- 先验 $\\,\\mu_0 = 0.5\\,$, $\\,P_0 = 0.5\\,$。\n- 观测 $\\,1\\,$: $\\,y_1 = 0.0\\,$, $\\,h_1(x) = \\sin(x)\\,$, $\\,r_1 = 0.05\\,$。\n- 观测 $\\,2\\,$: $\\,y_2 = 1.0\\,$, $\\,h_2(x) = x^2\\,$, $\\,r_2 = 0.05\\,$。\n- 顺序: $\\,(\\text{顺序 } A) = (1,2)\\,$, $\\,(\\text{顺序 } B) = (2,1)\\,$。\n预期行为：使用在当前均值处的 EKF 风格线性化，由于线性化点不同，顺序不变性通常不成立。\n\n测试用例 $\\,4\\,$ (非线性，较强非线性):\n- 状态维度 $\\,n = 1\\,$。\n- 先验 $\\,\\mu_0 = -1.2\\,$, $\\,P_0 = 0.2\\,$。\n- 观测 $\\,1\\,$: $\\,y_1 = 1.0\\,$, $\\,h_1(x) = \\exp(x)\\,$, $\\,r_1 = 0.1\\,$。\n- 观测 $\\,2\\,$: $\\,y_2 = -0.5\\,$, $\\,h_2(x) = \\tanh(x)\\,$, $\\,r_2 = 0.1\\,$。\n- 顺序: $\\,(\\text{顺序 } A) = (1,2)\\,$, $\\,(\\text{顺序 } B) = (2,1)\\,$。\n预期行为：使用 EKF 风格线性化，由于不同的线性化点和显著的非线性，顺序不变性不成立。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表。对于上面列出的四个测试用例，按顺序输出 $\\,\\text{[result}_1\\text{,result}_2\\text{,result}_3\\text{,result}_4\\text{]}\\,$, 其中每个 $\\,\\text{result}_i\\,$ 是一个布尔值 $\\,\\text{True}\\,$ 或 $\\,\\text{False}\\,$，通过使用容差 $\\,\\epsilon = 10^{-10}\\,$ 按指定方式比较两种顺序计算得出。", "solution": "我们从贝叶斯法则 $\\,p(\\mathbf{x}\\mid \\mathbf{y}) \\propto p(\\mathbf{y}\\mid \\mathbf{x})\\,p(\\mathbf{x})\\,$ 出发，并基于以下假设：先验是高斯的，即 $\\,\\mathbf{x}\\sim\\mathcal{N}(\\boldsymbol{\\mu}_0,\\mathbf{P}_0)\\,$，且每个标量观测都带有独立的高斯噪声，即 $\\,y_i = h_i(\\mathbf{x}) + \\varepsilon_i\\,$，其中 $\\,\\varepsilon_i\\sim\\mathcal{N}(0,r_i)\\,$。对于序列同化，我们每次并入一个观测，并在每次观测后更新均值和协方差。\n\n在线性观测算子 $\\,h(\\mathbf{x}) = \\mathbf{H}\\mathbf{x}\\,$（其中 $\\,\\mathbf{H}\\,$ 是行向量，$\\,y\\,$ 是标量）下，似然为 $\\,p(y\\mid \\mathbf{x}) \\propto \\exp\\!\\left(-\\frac{1}{2r}(y-\\mathbf{H}\\mathbf{x})^2\\right)\\,$。一次观测后的后验分布仍然是高斯的。卡尔曼风格的更新是通过对乘积 $\\,p(y\\mid \\mathbf{x})\\,p(\\mathbf{x})\\,$ 的指数项进行配方得到的。具体来说，后验均值 $\\,\\boldsymbol{\\mu}^+\\,$ 和协方差 $\\,\\mathbf{P}^+\\,$ 是通过在精度域中组合 $\\,\\mathbf{P}^{-1}\\,$ 和 $\\,\\mathbf{H}^T r^{-1} \\mathbf{H}\\,$ 以及在信息向量中组合线性项 $\\,\\mathbf{H}^T r^{-1} y\\,$ 得到的。等价地，可以推导出熟悉的卡尔曼增益表达式：\n$$\n\\mathbf{K} = \\mathbf{P}\\mathbf{H}^T\\left(\\mathbf{H}\\mathbf{P}\\mathbf{H}^T + r\\right)^{-1},\n$$\n更新公式为\n$$\n\\boldsymbol{\\mu}^+ = \\boldsymbol{\\mu} + \\mathbf{K}\\left(y - \\mathbf{H}\\boldsymbol{\\mu}\\right),\\quad\n\\mathbf{P}^+ = \\left(\\mathbf{I} - \\mathbf{K}\\mathbf{H}\\right)\\mathbf{P}\\left(\\mathbf{I} - \\mathbf{K}\\mathbf{H}\\right)^T + \\mathbf{K} r \\mathbf{K}^T,\n$$\n其中后者是 Joseph 形式，它能在数值上保持对称性和正半定性。\n\n对于多个独立观测 $\\,\\{y_i\\}\\,$、线性算子 $\\,\\{\\mathbf{H}_i\\}\\,$ 和方差 $\\,\\{r_i\\}\\,$，联合似然可分解为乘积 $\\,\\prod_i p(y_i\\mid \\mathbf{x})\\,$。由于乘法是可交换的，$\\,\\prod_i p(y_i\\mid \\mathbf{x})\\,$ 不因观测顺序而改变。在线性-高斯情况下，序列同化会重现一次性乘以所有似然得到的批量结果，因此最终的后验 $\\,(\\boldsymbol{\\mu}^+,\\mathbf{P}^+)\\,$ 不依赖于顺序。因此，对于对角的 $\\,\\mathbf{R}\\,$ 和线性的 $\\,\\mathbf{H}\\,$，顺序不变性成立。\n\n对于非线性观测算子 $\\,h(\\mathbf{x})\\,$，我们采用扩展卡尔曼滤波器 (EKF) 方法：在当前均值 $\\,\\boldsymbol{\\mu}\\,$ 处，用其一阶泰勒展开来近似 $\\,h(\\mathbf{x})\\,$，\n$$\nh(\\mathbf{x}) \\approx h(\\boldsymbol{\\mu}) + \\mathbf{H}(\\boldsymbol{\\mu}) (\\mathbf{x}-\\boldsymbol{\\mu}),\n$$\n其中 $\\,\\mathbf{H}(\\boldsymbol{\\mu})\\,$ 是 $\\,h\\,$ 在 $\\,\\boldsymbol{\\mu}\\,$ 处计算的雅可比矩阵。我们使用这个局部线性化来代替 $\\,h\\,$，并以有效的观测模型 $\\,y \\approx h(\\boldsymbol{\\mu}) + \\mathbf{H}(\\boldsymbol{\\mu})(\\mathbf{x}-\\boldsymbol{\\mu})\\,$ 执行一次线性-高斯更新。EKF 更新公式与线性情况类似，但分别用 $\\,\\mathbf{H}(\\boldsymbol{\\mu})\\,$ 和 $\\,h(\\boldsymbol{\\mu})\\,$ 替代 $\\,\\mathbf{H}\\,$ 和 $\\,\\mathbf{H}\\boldsymbol{\\mu}\\,$。\n\n关键在于，序列 EKF 同化在每次更新前都会在当前的后验均值处对 $\\,h\\,$ 进行线性化。如果以不同顺序同化观测，后验均值的序列会改变，因此线性化点 $\\,\\boldsymbol{\\mu}\\,$ 也会改变。由于 $\\,\\mathbf{H}(\\boldsymbol{\\mu})\\,$ 和 $\\,h(\\boldsymbol{\\mu})\\,$ 以非线性方式依赖于 $\\,\\boldsymbol{\\mu}\\,$，两次序列更新的累积效应通常依赖于顺序。因此，顺序不变性无法得到保证，且通常不成立。\n\n算法设计：\n- 将状态均值表示为 $\\,\\boldsymbol{\\mu}\\,$，协方差表示为 $\\,\\mathbf{P}\\,$。\n- 对于一个带有行向量 $\\,\\mathbf{H}\\,$、标量 $\\,y\\,$、方差 $\\,r\\,$ 的线性观测：\n  - 计算 $\\,\\mathbf{K} = \\mathbf{P}\\mathbf{H}^T(\\mathbf{H}\\mathbf{P}\\mathbf{H}^T + r)^{-1}\\,$。\n  - 更新 $\\,\\boldsymbol{\\mu} \\leftarrow \\boldsymbol{\\mu} + \\mathbf{K}(y - \\mathbf{H}\\boldsymbol{\\mu})\\,$。\n  - 更新 $\\,\\mathbf{P} \\leftarrow (\\mathbf{I}-\\mathbf{K}\\mathbf{H})\\mathbf{P}(\\mathbf{I}-\\mathbf{K}\\mathbf{H})^T + \\mathbf{K} r \\mathbf{K}^T\\,$。\n- 对于一个带有 $\\,h\\,$ 和雅可比矩阵 $\\,\\mathbf{H}(\\cdot)\\,$ 的非线性观测：\n  - 计算 $\\,h(\\boldsymbol{\\mu})\\,$ 和 $\\,\\mathbf{H}(\\boldsymbol{\\mu})\\,$。\n  - 在新息中使用 $\\,h(\\boldsymbol{\\mu})\\,$，并使用 $\\,\\mathbf{H}(\\boldsymbol{\\mu})\\,$ 作为线性算子，应用相同的线性更新。\n\n对每个测试用例，运行两种序列顺序并计算：\n- 最终均值的最大绝对差：$\\,\\Delta_\\mu = \\max_j |\\,\\mu_j^{(A)} - \\mu_j^{(B)}\\,|\\,$。\n- 最终协方差的弗罗贝尼乌斯范数差：$\\,\\Delta_P = \\lVert \\mathbf{P}^{(A)} - \\mathbf{P}^{(B)} \\rVert_F\\,$。\n如果 $\\,\\Delta_\\mu \\le \\epsilon\\,$ 且 $\\,\\Delta_P \\le \\epsilon\\,$，则声明顺序不变性为 $\\,\\text{True}\\,$，否则为 $\\,\\text{False}\\,$，使用 $\\,\\epsilon = 10^{-10}\\,$。\n\n对所提供测试的解释：\n- 测试 $\\,1\\,$ 和测试 $\\,2\\,$ 使用线性的 $\\,\\mathbf{H}\\,$ 和对角的 $\\,\\mathbf{R}\\,$，因此顺序 $\\,A\\,$ 和顺序 $\\,B\\,$ 的最终后验必须在数值容差内匹配，结果为 $\\,\\text{True}\\,$。\n- 测试 $\\,3\\,$ 和测试 $\\,4\\,$ 使用非线性的 $\\,h\\,$ 和 EKF 风格的线性化；先同化 $\\,y_1\\,$ 再同化 $\\,y_2\\,$ 产生的中间均值与先同化 $\\,y_2\\,$ 再同化 $\\,y_1\\,$ 的不同，因此线性化点不同，最终后验也通常不同，结果为 $\\,\\text{False}\\,$。\n\n程序实现了这些更新，并打印单行 $\\,\\text{[result}_1\\text{,result}_2\\text{,result}_3\\text{,result}_4\\text{]}\\,$, 其中每个 $\\,\\text{result}_i\\,$ 是相应测试用例的布尔结果。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef kalman_update_linear(mean, cov, H, y, r):\n    \"\"\"\n    Perform a single linear-Gaussian update with scalar observation.\n    mean: (n,) state mean\n    cov: (n,n) state covariance\n    H: (n,) observation row vector\n    y: scalar observation\n    r: scalar observation variance\n    Returns updated (mean, cov)\n    \"\"\"\n    H = H.reshape(-1)  # ensure 1D\n    n = mean.shape[0]\n    S = float(H @ cov @ H.T + r)  # innovation variance (scalar)\n    K = (cov @ H.T) / S           # gain shape (n,)\n    innovation = y - float(H @ mean)\n    mean_upd = mean + K * innovation\n    I = np.eye(n)\n    KH = np.outer(K, H)  # (n,n)\n    # Joseph form for numerical stability\n    cov_upd = (I - KH) @ cov @ (I - KH).T + np.outer(K, K) * r\n    return mean_upd, cov_upd\n\ndef ekf_update(mean, cov, h_func, jacobian_func, y, r):\n    \"\"\"\n    Perform a single EKF-style update with scalar observation.\n    mean: (n,) state mean\n    cov: (n,n) state covariance\n    h_func: function h(x) -> scalar\n    jacobian_func: function J(x) -> (n,) row vector (Jacobian of h at x)\n    y: scalar observation\n    r: scalar observation variance\n    Returns updated (mean, cov)\n    \"\"\"\n    H = np.array(jacobian_func(mean))  # (n,)\n    y_pred = float(h_func(mean))       # scalar\n    innovation = y - y_pred # The innovation in EKF uses the nonlinear model prediction\n    \n    # The rest of the update is identical to the linear case, but using the linearized H\n    n = mean.shape[0]\n    S = float(H @ cov @ H.T + r)\n    K = (cov @ H.T) / S\n    mean_upd = mean + K * innovation\n    I = np.eye(n)\n    KH = np.outer(K, H)\n    cov_upd = (I - KH) @ cov @ (I - KH).T + np.outer(K, K) * r\n    return mean_upd, cov_upd\n\n\ndef assimilate_serial(mu0, P0, observations, order):\n    \"\"\"\n    Assimilate observations serially in the given order.\n    observations: list of dicts describing each observation\n    order: list of indices indicating assimilation order\n    Returns (mean, cov)\n    \"\"\"\n    mean = mu0.copy()\n    cov = P0.copy()\n    for idx in order:\n        obs = observations[idx]\n        if obs['type'] == 'linear':\n            mean, cov = kalman_update_linear(mean, cov, obs['H'], obs['y'], obs['r'])\n        elif obs['type'] == 'nonlinear':\n            # The logic was slightly off in the initial `ekf_update` plan.\n            # The innovation term y - H*mu is replaced by y - h(mu). Let's adjust EKF update logic.\n            # The corrected logic should be inside the EKF update function.\n            # Re-checking the problem: \"h(x) approx h(mu) + H(mu)(x-mu)\"\n            # \"effective observational model y approx h(mu) + H(mu)(x-mu)\".\n            # This is y - h(mu) approx H(mu)(x-mu). This is a linear model for (x-mu) with observation y-h(mu).\n            # So the innovation is indeed y-h(mu). My EKF update function is correct.\n            mean, cov = ekf_update(mean, cov, obs['h'], obs['jacobian'], obs['y'], obs['r'])\n        else:\n            raise ValueError(\"Unknown observation type\")\n    return mean, cov\n\ndef solve():\n    # Numerical tolerance for invariance\n    EPS = 1e-10\n\n    # Define the test cases from the problem statement.\n\n    # Test case 1: linear, n=2\n    mu0_1 = np.array([1.0, -2.0])\n    P0_1 = np.array([[2.0, 0.5],\n                     [0.5, 1.0]])\n    obs1_case1 = {\n        'type': 'linear',\n        'H': np.array([1.0, 0.0]),\n        'y': 0.9,\n        'r': 0.4\n    }\n    obs2_case1 = {\n        'type': 'linear',\n        'H': np.array([2.0, -1.0]),\n        'y': -1.8,\n        'r': 0.3\n    }\n    observations_case1 = [obs1_case1, obs2_case1]\n    orderA_case1 = [0, 1]\n    orderB_case1 = [1, 0]\n\n    # Test case 2: linear, n=1\n    mu0_2 = np.array([0.0])\n    P0_2 = np.array([[1.0]])\n    obs1_case2 = {\n        'type': 'linear',\n        'H': np.array([1.0]),\n        'y': 1.0,\n        'r': 0.5\n    }\n    obs2_case2 = {\n        'type': 'linear',\n        'H': np.array([1.0]),\n        'y': -2.0,\n        'r': 1000.0\n    }\n    observations_case2 = [obs1_case2, obs2_case2]\n    orderA_case2 = [0, 1]\n    orderB_case2 = [1, 0]\n\n    # Test case 3: nonlinear, n=1\n    mu0_3 = np.array([0.5])\n    P0_3 = np.array([[0.5]])\n\n    def h1_case3(x):\n        return np.sin(x[0])\n\n    def J1_case3(x):\n        return np.array([np.cos(x[0])])\n\n    def h2_case3(x):\n        return x[0] ** 2\n\n    def J2_case3(x):\n        return np.array([2.0 * x[0]])\n\n    obs1_case3 = {\n        'type': 'nonlinear',\n        'h': h1_case3,\n        'jacobian': J1_case3,\n        'y': 0.0,\n        'r': 0.05\n    }\n    obs2_case3 = {\n        'type': 'nonlinear',\n        'h': h2_case3,\n        'jacobian': J2_case3,\n        'y': 1.0,\n        'r': 0.05\n    }\n    observations_case3 = [obs1_case3, obs2_case3]\n    orderA_case3 = [0, 1]\n    orderB_case3 = [1, 0]\n\n    # Test case 4: nonlinear, n=1\n    mu0_4 = np.array([-1.2])\n    P0_4 = np.array([[0.2]])\n\n    def h1_case4(x):\n        return np.exp(x[0])\n\n    def J1_case4(x):\n        return np.array([np.exp(x[0])])\n\n    def h2_case4(x):\n        return np.tanh(x[0])\n\n    def J2_case4(x):\n        t = np.tanh(x[0])\n        return np.array([1.0 - t * t])  # sech^2(x) = 1 - tanh^2(x)\n\n    obs1_case4 = {\n        'type': 'nonlinear',\n        'h': h1_case4,\n        'jacobian': J1_case4,\n        'y': 1.0,\n        'r': 0.1\n    }\n    obs2_case4 = {\n        'type': 'nonlinear',\n        'h': h2_case4,\n        'jacobian': J2_case4,\n        'y': -0.5,\n        'r': 0.1\n    }\n    observations_case4 = [obs1_case4, obs2_case4]\n    orderA_case4 = [0, 1]\n    orderB_case4 = [1, 0]\n\n    test_cases = [\n        (mu0_1, P0_1, observations_case1, orderA_case1, orderB_case1),\n        (mu0_2, P0_2, observations_case2, orderA_case2, orderB_case2),\n        (mu0_3, P0_3, observations_case3, orderA_case3, orderB_case3),\n        (mu0_4, P0_4, observations_case4, orderA_case4, orderB_case4),\n    ]\n\n    results = []\n    for mu0, P0, observations, orderA, orderB in test_cases:\n        muA, PA = assimilate_serial(mu0, P0, observations, orderA)\n        muB, PB = assimilate_serial(mu0, P0, observations, orderB)\n        # Compare means and covariances\n        mean_diff = np.max(np.abs(muA - muB))\n        cov_diff = np.linalg.norm(PA - PB, ord='fro')\n        invariant = (mean_diff = EPS) and (cov_diff = EPS)\n        results.append(str(invariant))\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3116062"}]}