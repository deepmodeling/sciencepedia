{"hands_on_practices": [{"introduction": "理论是基础，但动手实践能让我们更深刻地理解概念。这个练习将引导你手动计算一个简单的 $2 \\times 2$ 病态系统的吉洪诺夫正则化解。通过这个过程，你将熟悉正则化解的公式 $x_\\lambda = (A^T A + \\lambda I)^{-1} A^T b$，并直观地感受正则化参数 $\\lambda$ 如何影响解的轨迹，从而为理解更复杂的问题奠定坚实的基础。[@problem_id:2223157]", "problem": "在许多科学和工程应用中，人们需要求解线性方程组 $Ax=b$，其中矩阵 $A$ 是病态的。一种稳定解的常用方法是吉洪诺夫正则化（Tikhonov regularization），该方法旨在寻找一个向量 $x$ 以最小化复合目标函数 $\\|Ax-b\\|^2_2 + \\lambda \\|x\\|^2_2$，其中 $\\| \\cdot \\|_2$ 表示欧几里得范数，$\\lambda > 0$ 是一个正则化参数。该最小化问题的唯一解（表示为 $x_\\lambda$）由公式 $x_\\lambda = (A^T A + \\lambda I)^{-1} A^T b$ 给出，其中 $I$ 是单位矩阵。\n\n考虑一个病态线性系统，其矩阵 $A$ 和向量 $b$ 定义如下：\n$$\nA = \\begin{pmatrix} 1 & 1.01 \\\\ 1 & 1 \\end{pmatrix}, \\quad b = \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix}\n$$\n解向量 $x_\\lambda = \\begin{pmatrix} x_1(\\lambda) \\\\ x_2(\\lambda) \\end{pmatrix}$ 随着参数 $\\lambda$ 的变化在平面上描绘出一条路径。存在一个唯一的正值 $\\lambda$（我们称之为 $\\lambda^*$），使得解路径与 $x_2$ 轴相交。这对应于解向量的第一个分量为零，即 $x_1(\\lambda^*) = 0$。\n\n求出在该特定交点处第二个分量 $x_2(\\lambda^*)$ 的值。将你的最终答案四舍五入到四位有效数字。", "solution": "系统 $Ax=b$ 的吉洪诺夫正则化解由 $x_\\lambda = (A^T A + \\lambda I)^{-1} A^T b$ 给出。我们给定的矩阵 $A$ 和向量 $b$ 是：\n$$\nA = \\begin{pmatrix} 1 & 1.01 \\\\ 1 & 1 \\end{pmatrix}, \\quad b = \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix}\n$$\n\n首先，我们计算所需的矩阵和向量乘积。\n$A$ 的转置是：\n$$\nA^T = \\begin{pmatrix} 1 & 1 \\\\ 1.01 & 1 \\end{pmatrix}\n$$\n接下来，我们计算 $A^T A$：\n$$\nA^T A = \\begin{pmatrix} 1 & 1 \\\\ 1.01 & 1 \\end{pmatrix} \\begin{pmatrix} 1 & 1.01 \\\\ 1 & 1 \\end{pmatrix} = \\begin{pmatrix} 1 \\cdot 1 + 1 \\cdot 1 & 1 \\cdot 1.01 + 1 \\cdot 1 \\\\ 1.01 \\cdot 1 + 1 \\cdot 1 & 1.01 \\cdot 1.01 + 1 \\cdot 1 \\end{pmatrix} = \\begin{pmatrix} 2 & 2.01 \\\\ 2.01 & 1.0201 + 1 \\end{pmatrix} = \\begin{pmatrix} 2 & 2.01 \\\\ 2.01 & 2.0201 \\end{pmatrix}\n$$\n现在，我们计算 $A^T b$：\n$$\nA^T b = \\begin{pmatrix} 1 & 1 \\\\ 1.01 & 1 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 1 \\cdot 2 + 1 \\cdot 1 \\\\ 1.01 \\cdot 2 + 1 \\cdot 1 \\end{pmatrix} = \\begin{pmatrix} 3 \\\\ 2.02 + 1 \\end{pmatrix} = \\begin{pmatrix} 3 \\\\ 3.02 \\end{pmatrix}\n$$\n$x_\\lambda$ 的表达式涉及到矩阵 $(A^T A + \\lambda I)$：\n$$\nA^T A + \\lambda I = \\begin{pmatrix} 2 & 2.01 \\\\ 2.01 & 2.0201 \\end{pmatrix} + \\lambda \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 2+\\lambda & 2.01 \\\\ 2.01 & 2.0201+\\lambda \\end{pmatrix}\n$$\n为了求该矩阵的逆，我们首先计算其行列式：\n$$\n\\det(A^T A + \\lambda I) = (2+\\lambda)(2.0201+\\lambda) - (2.01)^2\n$$\n$$\n= (4.0402 + 2\\lambda + 2.0201\\lambda + \\lambda^2) - 4.0401\n$$\n$$\n= \\lambda^2 + 4.0201\\lambda + 0.0001\n$$\n逆矩阵是：\n$$\n(A^T A + \\lambda I)^{-1} = \\frac{1}{\\lambda^2 + 4.0201\\lambda + 0.0001} \\begin{pmatrix} 2.0201+\\lambda & -2.01 \\\\ -2.01 & 2+\\lambda \\end{pmatrix}\n$$\n现在我们可以写出 $x_\\lambda$ 的完整表达式：\n$$\nx_\\lambda = \\begin{pmatrix} x_1(\\lambda) \\\\ x_2(\\lambda) \\end{pmatrix} = \\frac{1}{\\lambda^2 + 4.0201\\lambda + 0.0001} \\begin{pmatrix} 2.0201+\\lambda & -2.01 \\\\ -2.01 & 2+\\lambda \\end{pmatrix} \\begin{pmatrix} 3 \\\\ 3.02 \\end{pmatrix}\n$$\n我们来计算分子中的矩阵-向量乘积：\n$$\n\\begin{pmatrix} (2.0201+\\lambda) \\cdot 3 - 2.01 \\cdot 3.02 \\\\ -2.01 \\cdot 3 + (2+\\lambda) \\cdot 3.02 \\end{pmatrix} = \\begin{pmatrix} 6.0603 + 3\\lambda - 6.0702 \\\\ -6.03 + 6.04 + 3.02\\lambda \\end{pmatrix} = \\begin{pmatrix} 3\\lambda - 0.0099 \\\\ 3.02\\lambda + 0.01 \\end{pmatrix}\n$$\n所以，解向量是：\n$$\nx_\\lambda = \\begin{pmatrix} x_1(\\lambda) \\\\ x_2(\\lambda) \\end{pmatrix} = \\frac{1}{\\lambda^2 + 4.0201\\lambda + 0.0001} \\begin{pmatrix} 3\\lambda - 0.0099 \\\\ 3.02\\lambda + 0.01 \\end{pmatrix}\n$$\n问题要求的是 $x_2(\\lambda^*)$ 的值，其中 $\\lambda^*$ 是使 $x_1(\\lambda) = 0$ 的正值 $\\lambda$。\n我们将第一个分量的分子设为零来求解 $\\lambda^*$：\n$$\n3\\lambda^* - 0.0099 = 0\n$$\n$$\n3\\lambda^* = 0.0099 \\implies \\lambda^* = 0.0033\n$$\n该值为正，符合要求。现在我们将 $\\lambda^*=0.0033$ 代入 $x_2(\\lambda)$ 的表达式中：\n$$\nx_2(\\lambda^*) = \\frac{3.02(0.0033) + 0.01}{(0.0033)^2 + 4.0201(0.0033) + 0.0001}\n$$\n计算分子：\n$$\n3.02 \\times 0.0033 + 0.01 = 0.009966 + 0.01 = 0.019966\n$$\n计算分母：\n$$\n(0.0033)^2 = 0.00001089\n$$\n$$\n4.0201 \\times 0.0033 = 0.01326633\n$$\n$$\n\\text{分母} = 0.00001089 + 0.01326633 + 0.0001 = 0.01337722\n$$\n最后，我们计算 $x_2(\\lambda^*)$ 的值：\n$$\nx_2(\\lambda^*) = \\frac{0.019966}{0.01337722} \\approx 1.4925343\n$$\n将结果四舍五入到四位有效数字，我们得到 $1.493$。", "answer": "$$\\boxed{1.493}$$", "id": "2223157"}, {"introduction": "在实际的计算科学问题中，我们常常遇到具有特定结构的矩阵，直接使用通用求解器可能效率低下且不稳定。这个练习 [@problem_id:3283854] 挑战你利用 Sherman-Morrison-Woodbury (SMW) 恒等式，为一个具有低秩结构的矩阵推导出一个高效的正则化解算法。通过将理论（矩阵恒等式）与实践（编程实现）相结合，你将学会如何利用问题结构来设计更优越的数值方法，这是计算科学家的一项核心技能。", "problem": "给定一个形式为 $A = I + u v^{\\top}$ 的方形系统矩阵，其中向量 $u, v \\in \\mathbb{R}^{n}$。当内积 $u^{\\top} v$ 接近 $-1$ 时，矩阵 $A$ 会变得接近奇异，这在对 $A$ 求逆或求解线性系统时可能导致数值不稳定性。为处理此问题，考虑 Tikhonov 正则化最小二乘问题：对于正则化参数 $\\alpha \\ge 0$，找到使 $\\lVert A x - b \\rVert_{2}^{2} + \\alpha \\lVert x \\rVert_{2}^{2}$ 最小化的 $x_{\\alpha} \\in \\mathbb{R}^{n}$。该解满足正规方程 $(A^{\\top} A + \\alpha I) x_{\\alpha} = A^{\\top} b$。\n\n仅从核心定义和已验证的事实出发，推导一个计算上稳定的 $x_{\\alpha}$ 公式，该公式利用 $A^{\\top} A + \\alpha I$ 的低秩结构，并避免显式地对大矩阵求逆：\n\n- 使用正规方程 $(A^{\\top} A + \\alpha I) x_{\\alpha} = A^{\\top} b$ 作为 Tikhonov 正则化解的定义关系。\n- 使用矩阵求逆引理（也称为 Sherman–Morrison–Woodbury 恒等式）作为基础工具：对于可逆矩阵 $C \\in \\mathbb{R}^{n \\times n}$ 和矩阵 $U, V \\in \\mathbb{R}^{n \\times k}$，有 $(C + U V^{\\top})^{-1} = C^{-1} - C^{-1} U (I_{k} + V^{\\top} C^{-1} U)^{-1} V^{\\top} C^{-1}$。不要假设任何其他专用公式。\n- 你的最终算法应仅用标量内积、小规模定长矩阵的逆以及与 $u$ 和 $v$ 的矩阵-向量乘积来表示。当低维计算足够时，避免构建稠密的 $n \\times n$ 矩阵。\n\n然后实现一个程序，对下面套件中的每个测试用例，用两种方法计算 $x_{\\alpha}$：\n- 方法 1（直接法）：使用稠密线性求解器求解 $(A^{\\top} A + \\alpha I) x = A^{\\top} b$。\n- 方法 2（利用结构法）：应用你推导出的基于矩阵求逆引理的低秩公式来计算 $(A^{\\top} A + \\alpha I)^{-1} A^{\\top} b$，而无需构建大的逆矩阵。\n\n对于每个测试用例，报告两个计算出的向量 $x_{\\alpha}$ 之间的最大绝对差，即 $\\max_{i} |(x_{\\alpha}^{(1)})_{i} - (x_{\\alpha}^{(2)})_{i}|$，作为一个浮点数。\n\n不涉及物理单位或角度单位。所有输出必须是实数。\n\n测试套件（请精确使用这些值）：\n- 维度：$n = 5$。\n- 固定向量 $u = [1, -2, 3, -4, 5]^{\\top}$。\n- 基础右端项 $b_{0} = [2, -1, 0.5, 1.5, -3]^{\\top}$。\n- 对于每个测试用例，通过 $v = c u$ 从指定的 $\\delta$ 构建 $v$，其中 $c = \\dfrac{-1 + \\delta}{\\lVert u \\rVert_{2}^{2}}$。这确保了 $u^{\\top} v = -1 + \\delta$。\n- 将五个案例定义为元组 $(\\delta, \\alpha, b)$：\n  1. $(0.2, 0, b_{0})$，\n  2. $(10^{-8}, 10^{-8}, b_{0})$，\n  3. $(10^{-12}, 10^{-4}, b_{0})$，\n  4. $(0, 1, b_{0})$，\n  5. $(10^{-14}, 10^{-6}, 0 \\cdot b_{0})$。\n\n你的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，列表内容为按上述测试套件顺序排列的结果，例如 $[r_{1}, r_{2}, r_{3}, r_{4}, r_{5}]$，其中每个 $r_{i}$ 是按所述方式为测试用例 $i$ 计算出的最大绝对差。", "solution": "目标是为 Tikhonov 正则化解 $x_{\\alpha}$ 推导一个计算上高效的公式，该解由正规方程定义：\n$$ (A^{\\top} A + \\alpha I) x_{\\alpha} = A^{\\top} b $$\n这可以写成 $x_{\\alpha} = (A^{\\top} A + \\alpha I)^{-1} A^{\\top} b$。令待求逆的矩阵为 $M = A^{\\top} A + \\alpha I$。我们将利用 $A$ 的低秩结构，使用 Sherman-Morrison-Woodbury (SMW) 恒等式来求 $M^{-1}$。\n\n矩阵 $A$ 给定为 $A = I + u v^{\\top}$。其转置为 $A^{\\top} = I + v u^{\\top}$。我们首先展开 $A^{\\top} A$ 项：\n$$ A^{\\top} A = (I + v u^{\\top})(I + u v^{\\top}) = I \\cdot I + I u v^{\\top} + v u^{\\top} I + v u^{\\top} u v^{\\top} $$\n$$ A^{\\top} A = I + u v^{\\top} + v u^{\\top} + v (u^{\\top} u) v^{\\top} $$\n令 $s_{uu} = u^{\\top} u$ 为内积的标量结果。则：\n$$ A^{\\top} A = I + u v^{\\top} + v u^{\\top} + s_{uu} v v^{\\top} $$\n现在，我们将其代入 $M$ 的表达式中：\n$$ M = A^{\\top} A + \\alpha I = (1 + \\alpha)I + u v^{\\top} + v u^{\\top} + s_{uu} v v^{\\top} $$\n为应用 SMW 公式，我们需要将更新部分 $u v^{\\top} + v u^{\\top} + s_{uu} v v^{\\top}$ 表示为 $UV^{\\top}$ 的形式。我们可以按如下方式对各项进行分组：\n$$ u v^{\\top} + v u^{\\top} + s_{uu} v v^{\\top} = u v^{\\top} + v(u^{\\top} + s_{uu}v^{\\top}) = u v^{\\top} + v(u + s_{uu}v)^{\\top} $$\n这个表达式是两个外积的和。我们定义两个列向量 $u_1=u$ 和 $u_2=v$，以及另外两个列向量 $v_1=v$ 和 $v_2=u+s_{uu}v$。那么表达式为 $u_1 v_1^{\\top} + u_2 v_2^{\\top}$。这可以写成矩阵形式 $[u_1, u_2] [v_1, v_2]^{\\top}$。\n因此，我们可以将 $M$ 写为：\n$$ M = C + U_{smw} V_{smw}^{\\top} $$\n其中：\n- $C = (1 + \\alpha)I$，对于 $\\alpha \\ge 0$，这是一个可逆的 $n \\times n$ 矩阵。\n- $U_{smw} = [u, v]$，一个 $n \\times 2$ 矩阵。\n- $V_{smw} = [v, u + s_{uu}v]$，一个 $n \\times 2$ 矩阵。\n\n我们应用 SMW 公式 $(C + U_{smw}V_{smw}^{\\top})^{-1} = C^{-1} - C^{-1}U_{smw}(I_2 + V_{smw}^{\\top}C^{-1}U_{smw})^{-1}V_{smw}^{\\top}C^{-1}$，其中 $k=2$。\n代入 $C^{-1} = \\frac{1}{1+\\alpha}I$：\n$$ M^{-1} = \\frac{1}{1+\\alpha}I - \\left(\\frac{1}{1+\\alpha}I\\right) U_{smw} \\left(I_2 + V_{smw}^{\\top}\\left(\\frac{1}{1+\\alpha}I\\right)U_{smw}\\right)^{-1} V_{smw}^{\\top} \\left(\\frac{1}{1+\\alpha}I\\right) $$\n$$ M^{-1} = \\frac{1}{1+\\alpha}I - \\frac{1}{(1+\\alpha)^2} U_{smw} \\left(I_2 + \\frac{1}{1+\\alpha}V_{smw}^{\\top}U_{smw}\\right)^{-1} V_{smw}^{\\top} $$\n令 $K = V_{smw}^{\\top}U_{smw}$。这是一个 $2 \\times 2$ 矩阵，其元素为内积：\n$$ K = \\begin{pmatrix} v^{\\top}u & v^{\\top}v \\\\ (u+s_{uu}v)^{\\top}u & (u+s_{uu}v)^{\\top}v \\end{pmatrix} = \\begin{pmatrix} u^{\\top}v & v^{\\top}v \\\\ u^{\\top}u+s_{uu}(v^{\\top}u) & u^{\\top}v+s_{uu}(v^{\\top}v) \\end{pmatrix} $$\n解 $x_{\\alpha}$ 由 $x_{\\alpha} = M^{-1} (A^{\\top} b)$ 给出：\n$$ x_{\\alpha} = \\left( \\frac{1}{1+\\alpha}I - \\frac{1}{(1+\\alpha)^2} U_{smw} \\left(I_2 + \\frac{1}{1+\\alpha}K\\right)^{-1} V_{smw}^{\\top} \\right) (A^{\\top}b) $$\n我们可以分配 $A^{\\top}b$ 以获得最终的算法公式：\n$$ x_{\\alpha} = \\frac{1}{1+\\alpha}A^{\\top}b - \\frac{1}{(1+\\alpha)^2} U_{smw} \\left(I_2 + \\frac{1}{1+\\alpha}K\\right)^{-1} (V_{smw}^{\\top}A^{\\top}b) $$\n该公式除了单位矩阵外，避免了构建任何大的 $n \\times n$ 矩阵。主要的计算任务是：\n1.  计算向量 $A^{\\top}b = (I + v u^{\\top})b = b + (u^{\\top}b)v$。这需要一次内积 ($u^{\\top}b$)、一次标量-向量乘法和一次向量加法。\n2.  计算构建 $2 \\times 2$ 矩阵 $K$ 所需的标量内积。\n3.  构建并求逆 $2 \\times 2$ 矩阵 $M_{2\\times2} = I_2 + \\frac{1}{1+\\alpha}K$。\n4.  计算 $2 \\times 1$ 向量 $z=V_{smw}^{\\top}(A^{\\top}b)$。这涉及与已计算向量 $A^{\\top}b$ 的两次内积。\n5.  使用向量缩放和加法组装最终解 $x_{\\alpha}$。令 $\\gamma = M_{2\\times2}^{-1}z$。项 $U_{smw}\\gamma$ 是向量 $u$ 和 $v$ 的线性组合：$U_{smw}\\gamma = \\gamma_1 u + \\gamma_2 v$。\n\n这个过程在计算上是高效的，其复杂度主要由向量运算决定（时间复杂度为 $O(n)$），与直接求解 $n \\times n$ 系统（时间复杂度为 $O(n^3)$）形成对比。它在数值上也更稳定，因为它避免了显式构建 $A^{\\top}A$，后者的条件数可能比 $A$ 大得多。", "answer": "```python\nimport numpy as np\n\ndef solve_direct(A, b, alpha):\n    \"\"\"\n    Method 1: Solve (A^T A + alpha*I)x = A^T b using a dense linear solver.\n    \"\"\"\n    n = A.shape[0]\n    I = np.identity(n)\n    \n    A_T_A = A.T @ A\n    \n    # System matrix and right-hand side\n    system_matrix = A_T_A + alpha * I\n    rhs_vector = A.T @ b\n    \n    # Solve the linear system\n    try:\n        x1 = np.linalg.solve(system_matrix, rhs_vector)\n    except np.linalg.LinAlgError:\n        # Fallback to pseudo-inverse if solve fails (e.g., for alpha=0 and singular matrix)\n        x1 = np.linalg.pinv(system_matrix) @ rhs_vector\n        \n    return x1\n\ndef solve_smw(u, v, b, alpha):\n    \"\"\"\n    Method 2: Apply the derived low-rank, SMW-based formula.\n    \"\"\"\n    n = u.shape[0]\n    I = np.identity(n)\n    I2 = np.identity(2)\n\n    # 1. Compute scalar products and related values\n    s_uu = u.T @ u\n    s_uv = u.T @ v\n    s_vv = v.T @ v\n    u_b = u.T @ b\n\n    # 2. Compute vectors needed for the formula\n    A_T_b = b + v * u_b\n    w = u + s_uu * v\n\n    # 3. Form the 2x2 matrix K\n    # K = V_smw.T @ U_smw, where U_smw=[u,v], V_smw=[v,w]\n    K = np.array([\n        [s_uv, s_vv],\n        [w.T @ u, w.T @ v]\n    ])\n\n    # 4. Invert the 2x2 matrix M_2x2 = I2 + (1/(1+alpha)) * K\n    inv_1_plus_alpha = 1.0 / (1.0 + alpha)\n    M_2x2 = I2 + inv_1_plus_alpha * K\n    M_2x2_inv = np.linalg.inv(M_2x2)\n\n    # 5. Compute the 2x1 vector z = V_smw.T @ A_T_b\n    z = np.array([v.T @ A_T_b, w.T @ A_T_b])\n\n    # 6. Compute gamma = M_2x2_inv @ z\n    gamma = M_2x2_inv @ z\n\n    # 7. Assemble the final solution x_alpha\n    # U_smw @ gamma = gamma[0]*u + gamma[1]*v\n    U_smw_gamma = gamma[0] * u + gamma[1] * v\n    \n    term1 = inv_1_plus_alpha * A_T_b\n    term2 = (inv_1_plus_alpha**2) * U_smw_gamma\n    \n    x2 = term1 - term2\n    \n    return x2\n\n\ndef solve():\n    \"\"\"\n    Main function to run test cases and compute differences.\n    \"\"\"\n    # Fixed parameters from the problem statement\n    n = 5\n    u = np.array([1.0, -2.0, 3.0, -4.0, 5.0])\n    b0 = np.array([2.0, -1.0, 0.5, 1.5, -3.0])\n    \n    test_cases = [\n        (0.2, 0.0, b0),\n        (1e-8, 1e-8, b0),\n        (1e-12, 1e-4, b0),\n        (0.0, 1.0, b0),\n        (1e-14, 1e-6, 0.0 * b0),\n    ]\n\n    results = []\n\n    s_uu = u.T @ u\n\n    for delta, alpha, b in test_cases:\n        # Construct v based on delta\n        c = (-1.0 + delta) / s_uu\n        v = c * u\n        \n        # Form matrix A\n        A = np.identity(n) + np.outer(u, v)\n\n        # Method 1: Direct solver\n        x1 = solve_direct(A, b, alpha)\n        \n        # Method 2: SMW-based formula\n        x2 = solve_smw(u, v, b, alpha)\n\n        # Calculate the maximum absolute difference\n        max_abs_diff = np.max(np.abs(x1 - x2))\n        results.append(max_abs_diff)\n    \n    print(f\"[{','.join(f'{r:.15e}' for r in results)}]\")\n\nsolve()\n```", "id": "3283854"}, {"introduction": "为了在实践中稳健地解决各种正则化问题，奇异值分解 (SVD) 是一种不可或缺的黄金标准方法。这个综合性练习 [@problem_id:3200591] 将指导你使用 SVD 推导并实现一个数值稳定的算法，用于计算完整的正则化路径，涵盖从良态到病态、从超定到欠定的多种情况。通过编程验证解路径的关键理论特性，你将全面掌握吉洪诺夫正则化在真实场景中的应用与分析。", "problem": "给定带有噪声观测的线性系统，其中观测向量 $b \\in \\mathbb{R}^m$ 通过矩阵 $A \\in \\mathbb{R}^{m \\times n}$ 和一个未知的系数向量 $x^\\star \\in \\mathbb{R}^n$ 近似建模为 $b \\approx A x^\\star$。当 $A$ 是病态或秩亏时，为了稳定估计，考虑 Tikhonov 正则化目标\n$$\nJ_\\lambda(x) = \\lVert A x - b \\rVert_2^2 + \\lambda^2 \\lVert x \\rVert_2^2,\n$$\n其中 $\\lambda \\ge 0$ 是正则化参数，$\\lVert \\cdot \\rVert_2$ 表示欧几里得范数。统计学上的岭回归路径是通过改变 $\\lambda$ 并观察系数如何收缩来获得的。\n\n从上述基本定义和关于线性代数的公认事实（包括奇异值分解 (SVD) 和 Moore–Penrose 伪逆的存在性及性质）出发，推导一个数值稳定的算法，用于在对数等间距的正则化强度 $\\lambda$ 网格上，计算 $J_\\lambda(x)$ 对应的最小化子 $x_\\lambda$，并沿着这条路径跟踪系数的收缩情况。不要依赖本问题陈述中给出的任何快捷公式。\n\n您的程序必须实现该算法，并将其应用于以下测试套件。为了可复现性，所有随机抽取必须使用指定的种子和所指示的标准正态分布。\n\n- 测试用例 1（良态，超定）：\n  - 维度：$m = 80$，$n = 20$。\n  - 随机种子：$42$。\n  - 构造方法：\n    - 抽取 $A \\in \\mathbb{R}^{80 \\times 20}$，其元素为独立的标准正态分布。\n    - 抽取 $x^\\text{true} \\in \\mathbb{R}^{20}$，其元素为独立的标准正态分布。\n    - 抽取 $\\epsilon \\in \\mathbb{R}^{80}$，其元素为独立的标准正态分布，然后按 $\\sigma = 0.01$ 进行缩放。\n    - 设置 $b = A x^\\text{true} + \\epsilon$。\n- 测试用例 2（欠定，宽设计）：\n  - 维度：$m = 50$，$n = 100$。\n  - 随机种子：$123$。\n  - 构造方法：\n    - 抽取 $A \\in \\mathbb{R}^{50 \\times 100}$，$x^\\text{true} \\in \\mathbb{R}^{100}$ 和 $\\epsilon \\in \\mathbb{R}^{50}$，均为独立的标准正态分布，其中 $\\epsilon$ 按 $\\sigma = 0.01$ 进行缩放。\n    - 设置 $b = A x^\\text{true} + \\epsilon$。\n- 测试用例 3（秩亏设计）：\n  - 维度：$m = 40$，$n = 40$。\n  - 随机种子：$7$。\n  - 构造方法：\n    - 抽取 $A \\in \\mathbb{R}^{40 \\times 40}$，其元素为独立的标准正态分布。\n    - 通过精确的列复制来强制秩亏：将索引为 $10$ 的列设置为与索引为 $5$ 的列相等，并将索引为 $15$ 的列设置为与索引为 $5$ 的列相等。\n    - 抽取 $x^\\text{true} \\in \\mathbb{R}^{40}$ 和 $\\epsilon \\in \\mathbb{R}^{40}$，均为独立的标准正态分布，其中 $\\epsilon$ 按 $\\sigma = 0.01$ 进行缩放。\n    - 设置 $b = A x^\\text{true} + \\epsilon$。\n- 测试用例 4（具有相关列的高度病态设计）：\n  - 维度：$m = 60$，$n = 20$。\n  - 随机种子：$314$。\n  - 构造方法：\n    - 抽取 $G \\in \\mathbb{R}^{60 \\times 20}$，其元素为独立的标准正态分布。\n    - 通过缩放创建相关且病态的列：对于列索引 $j \\in \\{0,1,\\dots,19\\}$，将 $A$ 的第 $j$ 列设置为 $G$ 的第 $j$ 列乘以 $10^{-\\frac{j}{3}}$。\n    - 抽取 $x^\\text{true} \\in \\mathbb{R}^{20}$ 和 $\\epsilon \\in \\mathbb{R}^{60}$，均为独立的标准正态分布，其中 $\\epsilon$ 按 $\\sigma = 0.01$ 进行缩放。\n    - 设置 $b = A x^\\text{true} + \\epsilon$。\n\n对于每个测试用例，使用一个包含 $60$ 个值的对数等间距网格\n$$\n\\lambda \\in \\{\\lambda_k \\mid \\lambda_k = 10^{\\ell_k},\\ \\ell_k \\text{ equispaced in } [-10,6]\\},\n$$\n即从 $10^{-10}$ 到 $10^{6}$。\n\n对于每个测试用例，计算正则化路径 $\\{x_{\\lambda_k}\\}$ 并评估以下三个布尔属性：\n\n1.  系数欧几里得范数的单调收缩：序列 $\\{\\lVert x_{\\lambda_k} \\rVert_2\\}_{k=1}^{60}$ 在一个小的数值容差范围内是单调非增的，即对所有 $k$，有 $\\lVert x_{\\lambda_{k}} \\rVert_2 \\le \\lVert x_{\\lambda_{k-1}} \\rVert_2 + \\tau$，其中 $\\tau = 10^{-12} \\cdot \\max(1, \\lVert x_{\\lambda_{k-1}} \\rVert_2)$。\n2.  在小正则化下与最小范数最小二乘解的近似一致性：令 $x_{\\mathrm{lsq}}$ 为最小化 $\\lVert A x - b \\rVert_2$ 且具有最小 $\\lVert x \\rVert_2$ 的 Moore–Penrose 伪逆解。检查 $x_{\\lambda_{\\min}}$ 和 $x_{\\mathrm{lsq}}$ 之间的相对差异是否满足\n$$\n\\frac{\\lVert x_{\\lambda_{\\min}} - x_{\\mathrm{lsq}} \\rVert_2}{\\max(1,\\lVert x_{\\mathrm{lsq}} \\rVert_2)}  10^{-6}\n$$\n3.  在非常大的正则化下系数趋近于零：检查\n$$\n\\frac{\\lVert x_{\\lambda_{\\max}} \\rVert_2}{\\max(1,\\lVert x_{\\mathrm{lsq}} \\rVert_2)}  10^{-6}\n$$\n\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果，每个测试用例的结果按上述顺序分组，并展平到所有测试用例中。具体来说，输出必须是一个包含 $12$ 个布尔值的列表\n$$\n[\\text{m1\\_t1},\\text{m2\\_t1},\\text{m3\\_t1},\\ \\text{m1\\_t2},\\text{m2\\_t2},\\text{m3\\_t2},\\ \\text{m1\\_t3},\\text{m2\\_t3},\\text{m3\\_t3},\\ \\text{m1\\_t4},\\text{m2\\_t4},\\text{m3\\_t4}],\n$$\n其中，对于测试用例 $i$，$\\text{m1\\_t}i$ 是单调收缩检查，$\\text{m2\\_t}i$ 是小 $\\lambda$ 一致性检查，$\\text{m3\\_t}i$ 是大 $\\lambda$ 近零检查。", "solution": "Tikhonov 正则化目标函数由下式给出：\n$$\nJ_\\lambda(x) = \\lVert A x - b \\rVert_2^2 + \\lambda^2 \\lVert x \\rVert_2^2\n$$\n其中 $A \\in \\mathbb{R}^{m \\times n}$，$b \\in \\mathbb{R}^m$，$x \\in \\mathbb{R}^n$，$\\lambda \\ge 0$ 是正则化参数。函数 $J_\\lambda(x)$ 对 $x$ 是凸的。对于 $\\lambda > 0$，它是严格凸的，保证了唯一的最小化子。通过将 $J_\\lambda(x)$ 对 $x$ 的梯度设为零，可以找到最小化子 $x_\\lambda$。\n\n首先，我们展开目标函数：\n$$\nJ_\\lambda(x) = (A x - b)^T (A x - b) + \\lambda^2 x^T x = x^T A^T A x - 2 b^T A x + b^T b + \\lambda^2 x^T I x\n$$\n关于 $x$ 的梯度是：\n$$\n\\nabla_x J_\\lambda(x) = 2 A^T A x - 2 A^T b + 2 \\lambda^2 I x\n$$\n将梯度设为零，$\\nabla_x J_\\lambda(x) = 0$，得到 Tikhonov 正规方程：\n$$\n(A^T A + \\lambda^2 I) x = A^T b\n$$\n形式上，解为 $x_\\lambda = (A^T A + \\lambda^2 I)^{-1} A^T b$。然而，直接构建矩阵 $A^T A$ 在数值上是不稳定的，特别是当 $A$ 是病态矩阵时。$A^T A$ 的条件数是 $A$ 条件数的平方，这可能导致精度的严重损失。\n\n使用 $A$ 的奇异值分解 (SVD) 可以推导出一种数值稳定的算法。设 $A$ 的 SVD 为：\n$$\nA = U \\Sigma V^T\n$$\n这里，$U \\in \\mathbb{R}^{m \\times m}$ 和 $V \\in \\mathbb{R}^{n \\times n}$ 是正交矩阵 ($U^T U = I_m$, $V^T V = I_n$)，$\\Sigma \\in \\mathbb{R}^{m \\times n}$ 是一个矩形对角矩阵，包含按降序排列的非负奇异值 $\\sigma_i$。设 $k = \\min(m, n)$。奇异值为 $(\\Sigma)_{ii} = \\sigma_i$ for $i=1, \\dots, k$。\n\n我们将 SVD 代入目标函数的首项。利用欧几里得范数在正交变换下不变的性质（即 $\\lVert U z \\rVert_2 = \\lVert z \\rVert_2$），我们有：\n$$\n\\lVert A x - b \\rVert_2^2 = \\lVert U \\Sigma V^T x - b \\rVert_2^2 = \\lVert U^T (U \\Sigma V^T x - b) \\rVert_2^2 = \\lVert \\Sigma V^T x - U^T b \\rVert_2^2\n$$\n我们引入变量替换。定义 $y = V^T x$ 和 $c = U^T b$。由于 $V$ 是正交的，$x = V y$，并且范数得以保持：$\\lVert x \\rVert_2^2 = \\lVert V y \\rVert_2^2 = \\lVert y \\rVert_2^2$。目标函数在 $y$ 的表示下变换为一个更简单的形式：\n$$\nJ_\\lambda(y) = \\lVert \\Sigma y - c \\rVert_2^2 + \\lambda^2 \\lVert y \\rVert_2^2\n$$\n这种形式是可分的。我们可以将其写成关于 $y$ 和 $c$ 各分量的和：\n$$\nJ_\\lambda(y) = \\sum_{i=1}^m \\left( (\\Sigma y)_i - c_i \\right)^2 + \\lambda^2 \\sum_{j=1}^n y_j^2\n$$\n考虑到 $\\Sigma$ 的结构：\n- 对于 $i \\le k = \\min(m, n)$，$(\\Sigma y)_i = \\sigma_i y_i$。\n- 对于 $i > k$，$(\\Sigma y)_i = 0$。\n目标函数变为：\n$$\nJ_\\lambda(y) = \\sum_{i=1}^k (\\sigma_i y_i - c_i)^2 + \\sum_{i=k+1}^m c_i^2 + \\lambda^2 \\left( \\sum_{i=1}^k y_i^2 + \\sum_{i=k+1}^n y_i^2 \\right)\n$$\n项 $\\sum_{i=k+1}^m c_i^2$ 是关于 $y$ 的常数。为了最小化 $J_\\lambda(y)$，我们可以对每个分量 $y_i$ 独立地最小化其余部分。\n对于 $i \\in \\{1, \\dots, k\\}$：我们最小化 $(\\sigma_i y_i - c_i)^2 + \\lambda^2 y_i^2$。将其对 $y_i$ 的导数设为零，得到：\n$$\n2(\\sigma_i y_i - c_i)\\sigma_i + 2\\lambda^2 y_i = 0 \\implies (\\sigma_i^2 + \\lambda^2) y_i = \\sigma_i c_i\n$$\n$y_i$ 的解为：\n$$\ny_i = \\frac{\\sigma_i c_i}{\\sigma_i^2 + \\lambda^2}\n$$\n即使 $\\sigma_i = 0$，此公式也是良定义的，此时 $y_i=0$（对于 $\\lambda > 0$）。\n对于 $i \\in \\{k+1, \\dots, n\\}$（这种情况仅在 $n>m$ 时发生，此时 $k=m$）：我们最小化 $\\lambda^2 y_i^2$。对于 $\\lambda > 0$，最小值在 $y_i = 0$ 处取得。\n\n因此，解向量 $y_\\lambda \\in \\mathbb{R}^n$ 的分量为：\n$$\n(y_\\lambda)_i =\n\\begin{cases}\n\\frac{\\sigma_i (U^T b)_i}{\\sigma_i^2 + \\lambda^2}  \\text{for } 1 \\le i \\le k \\\\\n0  \\text{for } k  i \\le n\n\\end{cases}\n$$\n最后，我们使用 $x = V y$ 将解 $y_\\lambda$ 变换回原始变量 $x_\\lambda$：\n$$\nx_\\lambda = V y_\\lambda = \\sum_{i=1}^k (y_\\lambda)_i v_i\n$$\n其中 $v_i$ 是 $V$ 的列（或 $V^T$ 的行）。\n\n这引出了以下数值稳定的算法：\n1.  计算 $A$ 的完全 SVD 分解 $A = U \\Sigma V^T$。\n2.  计算变换后的向量 $c = U^T b$。\n3.  对于网格中每个期望的 $\\lambda$：\n    a. 初始化一个 $n$ 维零向量 $y_\\lambda$。\n    b. 对于 $i=1, \\dots, k=\\min(m,n)$，计算 $(y_\\lambda)_i = \\frac{\\sigma_i c_i}{\\sigma_i^2 + \\lambda^2}$。\n    c. 计算解 $x_\\lambda = V y_\\lambda$。\n该算法避免了 $A^T A$ 的形成，并依赖于稳健的 SVD 计算，使其在数值上更优越。它能正确处理秩亏和病态矩阵。\n\n我们必须验证 $\\lVert x_\\lambda \\rVert_2$ 是 $\\lambda$ 的一个非增函数。从解析上看，$\\lVert x_\\lambda \\rVert_2^2 = \\lVert y_\\lambda \\rVert_2^2 = \\sum_{i=1}^k \\left(\\frac{\\sigma_i c_i}{\\sigma_i^2 + \\lambda^2}\\right)^2$。和中的每一项都是关于 $\\lambda \\ge 0$ 的非增函数，因此它们的和也是非增的。因此，该性质应在数值精度范围内成立。当 $\\lambda \\to 0$ 时，$x_\\lambda$ 应趋近于最小范数最小二乘解 $x_{\\mathrm{lsq}} = A^+ b$，其中 $A^+$ 是 $A$ 的 Moore-Penrose 伪逆。根据我们的推导，当 $\\lambda \\to 0$ 时：\n$$\n(y_\\lambda)_i \\to\n\\begin{cases}\nc_i / \\sigma_i  \\text{if } \\sigma_i > 0 \\\\\n0  \\text{if } \\sigma_i = 0\n\\end{cases}\n$$\n这个极限向量恰好是 $\\Sigma^+ c$。因此，$x_0 = V \\Sigma^+ c = V \\Sigma^+ U^T b = A^+ b$。该性质应成立。当 $\\lambda \\to \\infty$ 时，分母 $\\sigma_i^2 + \\lambda^2$ 增大，导致每个 $(y_\\lambda)_i \\to 0$。因此，$y_\\lambda \\to 0$ 且 $x_\\lambda = V y_\\lambda \\to 0$。系数应收缩至零。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import svd\n\ndef solve():\n    \"\"\"\n    Solves the Tikhonov regularization problem for four test cases\n    and verifies theoretical properties of the regularization path.\n    \"\"\"\n    test_cases_params = [\n        # (m, n, seed)\n        (80, 20, 42),\n        (50, 100, 123),\n        (40, 40, 7),\n        (60, 20, 314),\n    ]\n\n    all_results = []\n\n    for i, params in enumerate(test_cases_params):\n        m, n, seed = params\n        rng = np.random.default_rng(seed)\n\n        if i == 0:  # Case 1: well-conditioned, overdetermined\n            A = rng.standard_normal((m, n))\n            x_true = rng.standard_normal(n)\n            epsilon = rng.standard_normal(m) * 0.01\n            b = A @ x_true + epsilon\n        elif i == 1:  # Case 2: underdetermined\n            A = rng.standard_normal((m, n))\n            x_true = rng.standard_normal(n)\n            epsilon = rng.standard_normal(m) * 0.01\n            b = A @ x_true + epsilon\n        elif i == 2:  # Case 3: rank-deficient\n            A = rng.standard_normal((m, n))\n            A[:, 10] = A[:, 5]\n            A[:, 15] = A[:, 5]\n            x_true = rng.standard_normal(n)\n            epsilon = rng.standard_normal(m) * 0.01\n            b = A @ x_true + epsilon\n        elif i == 3:  # Case 4: ill-conditioned\n            G = rng.standard_normal((m, n))\n            A = np.zeros_like(G)\n            for j in range(n):\n                A[:, j] = G[:, j] * (10**(-j / 3.0))\n            x_true = rng.standard_normal(n)\n            epsilon = rng.standard_normal(m) * 0.01\n            b = A @ x_true + epsilon\n\n        # Regularization path calculation\n        lambda_grid = np.logspace(-10, 6, 60)\n        \n        # SVD-based solution\n        U, s, Vt = svd(A, full_matrices=True)\n        k = len(s)\n        V = Vt.T\n        c = U.T @ b\n\n        x_path = []\n        for lam in lambda_grid:\n            y = np.zeros(n)\n            y[:k] = (s * c[:k]) / (s**2 + lam**2)\n            x_lam = V @ y\n            x_path.append(x_lam)\n\n        x_path_norms = [np.linalg.norm(x) for x in x_path]\n\n        # Verification checks\n        \n        # 1. Monotone shrinkage of the coefficient Euclidean norm\n        is_monotone = True\n        for k_idx in range(1, len(x_path_norms)):\n            norm_prev = x_path_norms[k_idx - 1]\n            norm_curr = x_path_norms[k_idx]\n            # Since lambda grid is increasing, norm should be non-increasing\n            tolerance = 1e-12 * max(1, norm_prev)\n            if norm_curr > norm_prev + tolerance:\n                is_monotone = False\n                break\n        \n        # 2. Near-agreement with least squares at small lambda\n        x_lsq = np.linalg.pinv(A) @ b\n        norm_x_lsq = np.linalg.norm(x_lsq)\n        x_lambda_min = x_path[0]\n        \n        rel_diff_small_lambda = np.linalg.norm(x_lambda_min - x_lsq) / max(1, norm_x_lsq)\n        agrees_at_small_lambda = rel_diff_small_lambda  1e-6\n        \n        # 3. Near-zero coefficients at very large lambda\n        x_lambda_max = x_path[-1]\n        norm_x_lambda_max = np.linalg.norm(x_lambda_max)\n        \n        ratio_large_lambda = norm_x_lambda_max / max(1, norm_x_lsq)\n        zero_at_large_lambda = ratio_large_lambda  1e-6\n\n        all_results.extend([is_monotone, agrees_at_small_lambda, zero_at_large_lambda])\n\n    # Final print statement in the exact required format.\n    # The boolean values are converted to lowercase strings as required by the format.\n    print(f\"[{','.join(str(r).lower() for r in all_results)}]\")\n\nsolve()\n```", "id": "3200591"}]}