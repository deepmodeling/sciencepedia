{"hands_on_practices": [{"introduction": "在共享内存系统中，并行性能的瓶颈并不仅仅是任务划分，还深刻地根植于底层硬件的数据管理方式。这个练习将引导你探索一个常见但隐蔽的性能陷阱——伪共享 (false sharing)，它源于缓存一致性协议以缓存行 (cache line) 为单位运作的机制。通过分析数据布局如何影响缓存行争用，你将学会从根本上消除这种性能瓶颈，这是编写高效多线程代码的一项关键技能。[@problem_id:3145329]", "problem": "一个共享内存多核系统使用一种基于失效（invalidation-based）的缓存一致性协议，该协议与 MESI（Modified, Exclusive, Shared, Invalid）协议一致。一致性单元是一个大小为 $L = 64$ 字节的缓存行。考虑在一个处理器上，有 $T$ 个线程，每个线程固定（pinned）在一个核心上，该处理器的私有缓存行大小也为 $L = 64$ 字节。每个线程 $i$ 在一个紧凑循环（tight loop）内，对共享数组 $c[\\,]$ 中索引为 $i$ 的每个线程各自的计数器进行递增操作。$c[\\,]$ 的类型是 $64$ 位无符号整数，因此每个元素的大小为 $s = 8$ 字节。数组的基地址与 $L = 64$ 字节对齐。线程间的通信模式在循环期间不需要共享计数器的逻辑值；每个线程只更新自己的计数器，并在循环结束后执行一次归约（reduction）操作来合并结果。\n\n根据经验，递增循环的吞吐量低于预期，并且当线程数减少到 $T = 1$ 时，性能有所提高。你需要从缓存一致性的第一性原理出发，解释观察到的性能下降是否与伪共享（false sharing）一致，如果是，则确定哪些数据布局重新设计可以在不改变循环语义的情况下，保证消除由伪共享引起的缓存行“乒乓”（ping-pong）现象。\n\n选择所有在这些假设下，能保证对于任何 $T \\ge 1$，不同线程在递增循环期间更新各自的计数器时不会导致伪共享的选项。\n\nA. 对每个计数器进行填充（pad），使连续的计数器之间至少相隔 $L = 64$ 字节，并将数组基地址与 $L = 64$ 字节对齐，从而使每个线程的计数器位于不同的缓存行上。\n\nB. 保留原始的由 $s = 8$ 字节计数器组成的连续数组，但仅将数组基地址与 $L = 64$ 字节对齐；依赖基地址对齐来防止伪共享。\n\nC. 将数组结构（struct-of-arrays）布局替换为结构体数组（array-of-structs），其中每个线程的结构体包含计数器并被填充，使得结构体的大小恰好为 $L = 64$ 字节；并将数组基地址与 $L = 64$ 字节对齐。\n\nD. 在原始的由 $s = 8$ 字节计数器组成的连续数组上使用原子递增操作（例如，原子 fetch-and-add），以防止缓存行乒乓现象。\n\nE. 使用数组结构（struct-of-arrays）布局，将所有计数器连续分组以提高计数器字段的空间局部性；依赖于改进的局部性来减少一致性流量。\n\n提供你的选择并进行论证，通过推导原始布局下发生伪共享的条件以及避免伪共享所需的条件，并以缓存行和基于失效的一致性的核心定义为基础。", "solution": "问题描述阐述了并行计算中一个经典的性能问题，称为伪共享（false sharing）。该问题具有科学依据，提法明确，并提供了充分且一致的信息，可以从计算机体系结构的第一性原理推导出解决方案。\n\n**1. 问题陈述的验证**\n\n已知条件如下：\n- 一个共享内存多核系统，有 $T$ 个线程，每个核心一个。\n- 一种基于失效的缓存一致性协议，与 MESI 一致。\n- 一致性单元（以及私有缓存行大小）为 $L = 64$ 字节。\n- 每个线程 $i$ 在一个紧凑循环中递增自己的计数器 $c[i]$。\n- 计数器是 $64$ 位无符号整数，因此它们的大小为 $s = 8$ 字节。\n- 数组 $c$ 在内存中是连续的，其基地址与一个 $64$ 字节的边界对齐。\n- 在循环期间，线程之间没有共享计数器值的逻辑需求。\n\n当 $T > 1$ 时观察到的性能下降与伪共享现象是一致的。让我们从第一性原理来分析原因。\n\n**2. 从第一性原理推导**\n\n缓存一致性协议在缓存行的粒度上操作。在像 MESI 这样基于失效的协议中，当一个核心要写入一个内存位置时，它必须首先获得包含该位置的缓存行的独占所有权。这通常通过互连总线向可能拥有该缓存行副本的所有其他核心发送一个失效信号来实现。这些核心必须将其副本标记为无效（I 状态）。之后，这些核心任何对该缓存行的读取或写入尝试都将导致缓存未命中，从而强制从内存或直接从修改核心的缓存中获取更新后的缓存行。\n\n当多个由不同线程更新的独立数据项恰好位于同一个物理缓存行上时，问题就出现了。这被称为**伪共享（false sharing）**。这些数据项在逻辑上是独立的，但它们在内存中的物理邻近性通过缓存一致性机制将它们的性能耦合在了一起。\n\n在给定场景中：\n- 单个计数器元素的大小为 $s = 8$ 字节。\n- 缓存行的大小为 $L = 64$ 字节。\n- 因此，单个缓存行内可以容纳的计数器数量为 $N = L/s = 64/8 = 8$。\n\n数组 $c$ 是连续的，其基地址与 $64$ 字节边界对齐。这意味着前 $8$ 个计数器 $c[0], c[1], \\dots, c[7]$ 的内存地址将占用同一个缓存行。具体来说，如果 $c$ 的基地址是 $A_{base}$，那么 $c[i]$ 的地址是 $A_{base} + i \\times s$。对于 $i \\in \\{0, 1, \\dots, 7\\}$，这些地址都落在第一个缓存行内，该缓存行覆盖的内存范围是 $[A_{base}, A_{base} + L - 1]$。\n\n让我们考虑两个线程，线程 0 和线程 1，它们分别在不同的核心上运行，并分别更新 $c[0]$ 和 $c[1]$。\n1. 线程 0 执行 `c[0]++`。这是一个读-改-写操作。为了执行写操作，核心 0 必须获取该缓存行的独占所有权。假设它将该行以“已修改”（M）状态载入其私有缓存。\n2. 线程 1 执行 `c[1]++`。由于 $c[1]$ 位于同一个缓存行上，核心 1 也必须获取独占所有权。它发送一个对该缓存行的请求。\n3. 这个请求导致核心 0 失去独占所有权。它可能会将该缓存行写回内存和/或将其转发给核心 1。核心 0 的缓存行副本被置为无效（I 状态）。\n4. 核心 1 现在以“已修改”状态拥有该缓存行，并递增 $c[1]$。\n5. 当线程 0 再次循环并尝试递增 $c[0]$ 时，它发现其副本是无效的，导致缓存未命中。它必须重新获取该缓存行，这又会使核心 1 缓存中的副本失效。\n\n这种在核心之间来回传输缓存行的过程被称为“缓存行乒乓”（cache line ping-pong）。这是一个代价高昂的过程，涉及停顿以及内存总线或互连上的流量，这解释了观察到的低吞吐量。当 $T=1$ 时，这个问题会消失，因为没有其他核心来争夺该缓存行。\n\n为了**保证**消除伪共享，我们必须确保由不同线程修改的任意两个计数器不会位于同一个缓存行上。由于每个线程 $i$ 只修改 $c[i]$，条件就是对于任意两个不同的线程 $i$ 和 $j$，$c[i]$ 和 $c[j]$ 必须位于不同的缓存行上。这可以通过构建数据结构来实现，使得每个 `c[i]` 都位于一个至少为 $L=64$ 字节且不与任何其他 $c[j]$ 共享的内存块中。\n\n**3. 选项评估**\n\n**A. 对每个计数器进行填充，使连续的计数器之间至少相隔 $L = 64$ 字节，并将数组基地址与 $L = 64$ 字节对齐，从而使每个线程的计数器位于不同的缓存行上。**\n这种方法明确地强制执行了必要条件。如果数组的基地址与 $64$ 字节对齐，并且每个元素都放置在一个是 $64$ 字节倍数的偏移量上，那么每个元素都将从一个新的缓存行的开始处算起。例如，$c[i]$ 的地址将是 $A_{base} + i \\times P$，其中 $P \\ge L = 64$。由于一个缓存行不能跨越两个元素，并且每个元素的起始位置与下一个元素至少相距 $L$ 字节，因此每个 $c[i]$ 将位于一个唯一的缓存行上（相对于其他计数器）。这种布局直接防止了伪共享。\n结论：**正确**。\n\n**B. 保留原始的由 $s = 8$-字节计数器组成的连续数组，但仅将数组基地址与 $L = 64$ 字节对齐；依赖基地址对齐来防止伪共享。**\n这描述了原始存在问题的设置。如推导所示，对一个由小元素组成的连续数组的基地址进行对齐，并不能防止这些元素共享一个缓存行。前 $8$ 个计数器，$c[0]$ 到 $c[7]$，仍然会占用第一个缓存行，导致前 $8$ 个线程之间出现伪共享。\n结论：**错误**。\n\n**C. 将数组结构（struct-of-arrays）布局替换为结构体数组（array-of-structs），其中每个线程的结构体包含计数器并被填充，使得结构体的大小恰好为 $L = 64$ 字节；并将数组基地址与 $L = 64$ 字节对齐。**\n这是选项 A 中策略的另一种常见实现方式。一个类似 `struct { uint64_t counter; char padding[56]; } data[T];` 的结构体数组，其 `sizeof` 将等于 $64$ 字节。由于该数组是连续的，并且其基地址与 $64$ 字节对齐，第 $i$ 个结构体 `data[i]` 的地址将是 $A_{base} + i \\times 64$。这意味着每个结构体，以及其中的每个计数器，都将从一个新的缓存行边界开始。线程 $i$ 的计数器与线程 $j$ ($i \\neq j$) 的计数器位于不同的缓存行上。这保证了伪共享的消除。\n结论：**正确**。\n\n**D. 在原始的由 $s = 8$-字节计数器组成的连续数组上使用原子递增操作（例如，原子 fetch-and-add），以防止缓存行乒乓现象。**\n原子操作确保读-改-写序列从 CPU 指令流的角度看是作为一个不可分割的单元执行的。这对于在*真共享*（多个线程更新同一个变量）的情况下防止数据竞争至关重要。然而，它们不改变底层的内存布局或缓存一致性协议的行为。对 $c[i]$ 的原子写操作仍然是对缓存行内某个内存位置的写操作。如果其他线程正在写入同一缓存行上的其他变量（如 $c[j]$），硬件仍然必须使其副本失效，以便让写入的核心获得独占访问权。原子操作不会也无法防止由伪共享引起的缓存行乒乓现象。\n结论：**错误**。\n\n**E. 使用数组结构（struct-of-arrays）布局，将所有计数器连续分组以提高计数器字段的空间局部性；依赖于改进的局部性来减少一致性流量。**\n初始的问题设置*就是*一个数组结构布局（或者对于一个简单数组来说是等效的），其中所有的计数器被连续地分组在一起。这种设计为*顺序*访问模式（一个线程遍历所有计数器）提供了极好的空间局部性。然而，对于给定的*并行*访问模式，即每个线程重复访问自己单个、独立的计数器，正是这种空间局部性导致了伪共享。将计数器连续放置可确保它们会共享缓存行，这反过来会*最大化*而不是减少一致性流量。\n结论：**错误**。", "answer": "$$\\boxed{AC}$$", "id": "3145329"}, {"introduction": "掌握了底层数据布局的优化后，我们把目光转向更高层次的并行策略：任务分发与负载均衡。当循环中的迭代计算成本差异巨大时，如何将工作公平地分配给多个核心，成为决定程序整体执行时间（即完成时间 makespan）的关键。这个练习通过对比 OpenMP 中的静态与动态调度策略，揭示了在负载均衡收益和调度开销之间的经典权衡，帮助你为不同特性的并行循环选择最优的调度方案。[@problem_id:3145384]", "problem": "考虑一个包含 $N$ 次独立迭代的循环，该循环将使用 Open Multi-Processing (OpenMP) 在 $p$ 个相同的中央处理器（CPU）核心上运行。每次迭代 $i$ 的运行时间 $T_i$ 独立地从一个重尾混合分布中抽取：有 $q$ 的概率是一次“重”迭代，耗时 $t_H$ 个时间单位；有 $1 - q$ 的概率是一次“轻”迭代，耗时 $t_L$ 个时间单位。假设 $t_H \\gg t_L$，并且该混合模型是重尾分布的一个合理替代。该循环可以被静态调度（每个核心获得一个固定的连续迭代块）或动态调度（每个核心从共享队列中重复提取大小为 $c$ 的下一个可用块，直到所有迭代完成）。每次动态块的获取会给获取核心带来 $t_s$ 的调度开销；静态调度的开销可以忽略不计。\n\n在此特定场景下，设 $N = 400$, $p = 4$, $q = 0.02$, $t_L = 1\\,\\mathrm{ms}$, $t_H = 100\\,\\mathrm{ms}$，以及 $t_s = 0.2\\,\\mathrm{ms}$。对于动态调度，考虑 $c = 1$ 和 $c = 10$ 的情况。假设运行时系统在其他方面是理想的：除了每个块的开销 $t_s$ 之外没有其他竞争，并且核心以指定的时长执行迭代。\n\n使用控制并行循环执行的基本定义：\n- 完成时间 $T_{\\mathrm{par}}$ 是指最后一个核心完成其分配的工作所需的时间。\n- 在完美负载均衡和可忽略的开销下，$T_{\\mathrm{par}}$ 约等于总工作量除以 $p$，即 $T_{\\mathrm{par}} \\approx \\left(\\sum_{i=1}^{N} T_i\\right)/p$。\n- 在静态调度下，每个核心接收恰好 $N/p$ 次迭代，因此其运行时间等于其分配到的迭代块中所有 $T_i$ 的总和；完成时间由具有最大总和的核心决定。\n- 在动态调度下，运行时间包括执行的迭代时间和获取块的累积调度开销；分块（chunking）同时影响负载均衡的粒度和开销。\n\n基于这些定义和给定参数，关于预期负载均衡和调度开销权衡的陈述中，哪些最为准确？\n\nA. 静态调度实现了接近完美的负载均衡，因为每个核心都恰好获得 $N/p$ 次迭代，所以其预期完成时间接近 $\\left(\\mathbb{E}[T]\\,N\\right)/p$，并且其性能优于 $c=1$ 的动态调度。\n\nB. 即使在考虑了每次迭代的调度开销 $t_s$ 之后，$c=1$ 的动态调度也比静态调度实现了显著更好的预期负载均衡，因此其预期完成时间更接近 $\\left(\\sum_{i=1}^{N} T_i\\right)/p$。\n\nC. 与 $c=1$ 相比，$c=10$ 的动态调度将调度开销减少了10倍，同时保留了动态调度的大部分负载均衡优势，因此其预期完成时间低于 $c=1$ 的情况。\n\nD. 在静态分配前随机排列迭代顺序，使得静态调度实际上等效于 $c=1$ 的动态调度，从而在没有开销的情况下，产生一个接近 $\\left(\\sum_{i=1}^{N} T_i\\right)/p$ 的预期完成时间。\n\n选择所有适用项。", "solution": "该问题陈述已经过验证，被认为是具有科学依据、定义明确且客观的。它展示了并行计算性能分析中的一个经典场景。\n\n我们首先根据所提供的数据，为我们的分析建立基准参数。\n- 总迭代次数：$N = 400$\n- 处理器核心数：$p = 4$\n- 重迭代的概率：$q = 0.02$\n- 轻迭代的运行时间：$t_L = 1\\,\\mathrm{ms}$\n- 重迭代的运行时间：$t_H = 100\\,\\mathrm{ms}$\n- 动态块的调度开销：$t_s = 0.2\\,\\mathrm{ms}$\n\n首先，我们计算预期的总计算工作量 $W_{total}$。预期的重迭代次数为 $N \\times q = 400 \\times 0.02 = 8$。预期的轻迭代次数为 $N \\times (1-q) = 400 \\times (1 - 0.02) = 392$。\n预期的总工作量是所有迭代时间的总和：\n$$W_{total} = (Nq) t_H + (N(1-q)) t_L = (8 \\times 100\\,\\mathrm{ms}) + (392 \\times 1\\,\\mathrm{ms}) = 800\\,\\mathrm{ms} + 392\\,\\mathrm{ms} = 1192\\,\\mathrm{ms}$$\n为了比较不同调度器，我们将此预期工作量视为要执行的实际工作量，即 $\\sum_{i=1}^{N} T_i = 1192\\,\\mathrm{ms}$。\n\n理想完成时间 $T_{ideal}$ 假设了完美的负载均衡和零开销。它可作为预期完成时间的理论下界。\n$$T_{ideal} = \\frac{W_{total}}{p} = \\frac{1192\\,\\mathrm{ms}}{4} = 298\\,\\mathrm{ms}$$\n\n现在，我们分析每种调度策略。\n\n**静态调度**\n在静态调度下，$N=400$ 次迭代被分成 $p=4$ 个连续块，每个核心接收 $N/p = 100$ 次迭代。开销可以忽略不计。完成时间由接收到工作量最大块的核心决定。\n由于 $t_H \\gg t_L$，工作负载是“重尾”的。这意味着完成时间主要由这 $8$ 次重迭代在 $4$ 个核心间的分布情况决定。\n- **最佳情况（完美均衡）：** 如果 $8$ 次重迭代被均匀分配，每个核心接收到 $8/4=2$ 次重迭代，那么每个核心的工作量将完全相同：$W_{core} = (2 \\times t_H) + ((100-2) \\times t_L) = (2 \\times 100) + (98 \\times 1) = 298\\,\\mathrm{ms}$。完成时间将是 $T_{static} = 298\\,\\mathrm{ms}$，即理想完成时间。\n- **可能/最差情况（不均衡）：** 迭代的运行时间是独立的，因此不能保证均匀分布。分布很可能是不均匀的。例如，考虑一个可能的情景，重迭代的分布是 $(3, 2, 2, 1)$。完成时间将由拥有 $3$ 次重迭代的核心决定：$W_{max} = (3 \\times 100) + (97 \\times 1) = 397\\,\\mathrm{ms}$。在一个更不均衡的情况下，如 $(4, 2, 1, 1)$，完成时间将是 $W_{max} = (4 \\times 100) + (96 \\times 1) = 496\\,\\mathrm{ms}$。\n由于这种不均衡情况的概率很高，静态调度的*预期*完成时间将显著高于理想的 $T_{ideal} = 298\\,\\mathrm{ms}$。\n\n**块大小 $c=1$ 的动态调度**\n每个处理器一次从一个中央队列中请求一次迭代。\n- **负载均衡：** 这种细粒度的方法提供了近乎完美的负载均衡。每当一个核心完成一次迭代（无论是轻是重），它会立即开始下一次。工作几乎完全均匀地分布在各个核心之间。\n- **开销：** 每个获取的块都有一个 $t_s$ 的开销。由于 $c=1$，总共有 $N=400$ 个块。总调度开销为 $O_{c=1} = N \\times t_s = 400 \\times 0.2\\,\\mathrm{ms} = 80\\,\\mathrm{ms}$。\n- **完成时间：** 总工作量是计算工作量加上开销：$W_{total} + O_{c=1} = 1192\\,\\mathrm{ms} + 80\\,\\mathrm{ms} = 1272\\,\\mathrm{ms}$。由于其出色的负载均衡，预期完成时间可以通过将此总工作量除以核心数来很好地近似。\n$$E[T_{dyn, c=1}] \\approx \\frac{W_{total} + O_{c=1}}{p} = \\frac{1272\\,\\mathrm{ms}}{4} = 318\\,\\mathrm{ms}$$\n这个值由于开销而高于理想的 $298\\,\\mathrm{ms}$，但它比不均衡的静态情况（例如 $397\\,\\mathrm{ms}$ 或 $496\\,\\mathrm{ms}$）要好得多。\n\n**块大小 $c=10$ 的动态调度**\n每个处理器一次请求一个包含 $10$ 次迭代的块。\n- **负载均衡：** 粒度比 $c=1$ 时更粗。这可能会导致一些负载不均衡，但由于有 $N/c = 400/10 = 40$ 个块可以动态分配，它仍然远优于静态调度。\n- **开销：** 块的数量减少了10倍。总调度开销为 $O_{c=10} = (N/c) \\times t_s = 40 \\times 0.2\\,\\mathrm{ms} = 8\\,\\mathrm{ms}$。与 $c=1$ 时的 $80\\,\\mathrm{ms}$ 开销相比，这是一个显著的减少。\n- **完成时间：** 总工作量为 $W_{total} + O_{c=10} = 1192\\,\\mathrm{ms} + 8\\,\\mathrm{ms} = 1200\\,\\mathrm{ms}$。这种情况下的理想化完成时间为：\n$$E[T_{dyn, c=10}]_{ideal} = \\frac{W_{total} + O_{c=10}}{p} = \\frac{1200\\,\\mathrm{ms}}{4} = 300\\,\\mathrm{ms}$$\n由于粒度较粗（一个核心可能在末尾处于空闲状态，而另一个核心正在完成一个长时间运行的块），实际完成时间可能会略高一些，但开销的减少是巨大的（$80\\,\\mathrm{ms}$ 对比 $8\\,\\mathrm{ms}$）。总开销减少了 $72\\,\\mathrm{ms}$，这使得每个核心的平均工作量减少了 $72/4 = 18\\,\\mathrm{ms}$。要使 $c=10$ 的情况比 $c=1$ 的情况慢，负载不均衡造成的损失需要超过这 $18\\,\\mathrm{ms}$ 的收益。在 $4$ 个核心间分配 $40$ 个块的情况下，这不太可能发生。因此，我们预期 $E[T_{dyn, c=10}]  E[T_{dyn, c=1}]$。\n\n现在让我们基于此分析评估给出的选项。\n\n**A. 静态调度实现了接近完美的负载均衡，因为每个核心都恰好获得 $N/p$ 次迭代，所以其预期完成时间接近 $\\left(\\mathbb{E}[T]\\,N\\right)/p$，并且其性能优于 $c=1$ 的动态调度。**\n这个陈述在多个方面都有缺陷。首先，当迭代时间差异很大时（如此处所示），给予每个核心相同*数量*的迭代并不能保证均衡的*工作负载*。这会导致糟糕的负载均衡，而非“接近完美”。其次，由于这种糟糕的负载均衡，预期完成时间会显著*大于*理想时间 $(\\mathbb{E}[T]\\,N)/p = 298\\,\\mathrm{ms}$。第三，我们的分析表明，静态调度的预期速度（例如 $397\\,\\mathrm{ms}$ 或更高）会比 $c=1$ 的动态调度（约 $318\\,\\mathrm{ms}$）慢。\n**结论：错误。**\n\n**B. 即使在考虑了每次迭代的调度开销 $t_s$ 之后，$c=1$ 的动态调度也比静态调度实现了显著更好的预期负载均衡，因此其预期完成时间更接近 $\\left(\\sum_{i=1}^{N} T_i\\right)/p$。**\n这个陈述是准确的。$c=1$ 的动态调度旨在实现出色的负载均衡，对于这种类型的工作负载，这比静态调度是一个显著的改进。$(\\sum_{i=1}^{N} T_i)/p$ 代表了每个核心的理想平均工作量，$T_{ideal}=298\\,\\mathrm{ms}$。虽然动态调度的完成时间（$E[T_{dyn, c=1}] \\approx 318\\,\\mathrm{ms}$）包含了开销，但这个开销带来的损失（$318 - 298 = 20\\,\\mathrm{ms}$）远小于静态调度下预期的严重负载不均衡所带来的损失（例如 $397 - 298 = 99\\,\\mathrm{ms}$）。因此，动态调度下的完成时间确实比静态调度下的完成时间更接近理想的并行时间。\n**结论：正确。**\n\n**C. 与 $c=1$ 相比，$c=10$ 的动态调度将调度开销减少了10倍，同时保留了动态调度的大部分负载均衡优势，因此其预期完成时间低于 $c=1$ 的情况。**\n这个陈述准确地描述了这种权衡。$c=10$ 时的开销是 $8\\,\\mathrm{ms}$，而 $c=1$ 时是 $80\\,\\mathrm{ms}$，所以确实减少了10倍。虽然负载均衡不如 $c=1$ 时完美，但与静态调度相比仍然非常有效，因此“保留了动态调度的大部分负载均衡优势”是一个公允的描述。开销的显著减少（总共 $72\\,\\mathrm{ms}$）导致了更低的总工作负载。如计算所示，$E[T_{dyn, c=10}]$ 预期在 $300\\,\\mathrm{ms}$ 左右，低于 $E[T_{dyn, c=1}]$ 的约 $318\\,\\mathrm{ms}$。从减少开销中获得的收益超过了负载均衡效率的微小损失。\n**结论：正确。**\n\n**D. 在静态分配前随机排列迭代顺序，使得静态调度实际上等效于 $c=1$ 的动态调度，从而在没有开销的情况下，产生一个接近 $\\left(\\sum_{i=1}^{N} T_i\\right)/p$ 的预期完成时间。**\n在静态分配前随机排列迭代有助于避免重迭代聚集在一起的最坏情况，但它并不能消除不均衡的统计可能性。一个核心仍然可能偶然被分配到比其他核心更多的重迭代。这与动态调度根本不同，后者在*运行时*适应工作负载。因此，它并非“实际上等效于”动态调度。最终，完成时间仍然会有显著的方差，并且由于这种不均衡，预期完成时间将显著大于理想的 $(\\sum_{i=1}^{N} T_i)/p$。声称两者等效是错误的。\n**结论：错误。**", "answer": "$$\\boxed{BC}$$", "id": "3145384"}, {"introduction": "现在，我们将视野从单台多核处理器扩展到通过网络互连的计算集群，此时通信效率成为性能的关键。在分布式内存编程中，网络传输的成本通常可以用启动延迟 $\\alpha$ 和每字节带宽成本 $\\beta$ 来建模，这使得发送大量小消息的开销极高。本练习要求你通过一个消息捆绑优化的场景，量化分析并找到最佳策略，以平衡等待延迟与通信开销，这是 MPI 等并行编程实践中一项基础而重要的优化技术。[@problem_id:3145373]", "problem": "一位开发人员正在一个同构集群互连上优化消息传递接口（MPI）的通信路径。该应用程序生成发往单个远程 rank 的小负载。每个负载是大小为 $s$ 字节的固定大小消息。消息以每秒 $\\lambda$ 条的速率，根据泊松过程到达。开发人员考虑将达到整数阈值 $k$ 的到达消息捆绑到单个缓冲区中，进行一次聚合发送操作，然后将聚合后的负载作为一条消息传输。用于在此目的地进行捆绑的可用缓冲区内存为 $B$ 字节。在该互连上传输大小为 $m$ 字节的消息的通信成本，可以通过标准的启动-带宽关系精确建模，其中发送时间是启动延迟 $\\alpha$ 与一个与大小相关的项（由反带宽因子 $\\beta$ 乘以 $m$ 得出）之和。任何消息的端到端延迟定义为从消息到达发送方那一刻起，到包含该消息的整个聚合发送操作完成并送达接收方那一刻止的时间。假设在一个捆绑包中第 $k$ 条消息到达后立即开始传输，并且消息在到达后除了聚合和传输外会立即被处理。\n\n为了控制启动（延迟）成本，系统对单位墙上时钟时间的启动时间设定了预算：在持续的数据流中，每秒消耗的总启动时间不得超过每秒 $C_{\\mathrm{max}}$ 微秒。开发人员必须选择捆绑阈值 $k$ 以最小化每条消息的预期端到端延迟，同时满足启动预算和缓冲区内存约束。\n\n使用以下参数值：$s = 128$ 字节, $\\lambda = 2400$ 条消息/秒, $\\alpha = 3.5$ 微秒, $\\beta = 0.04$ 微秒/字节, $B = 16384$ 字节, 以及 $C_{\\mathrm{max}} = 3000$ 微秒/秒。确定在给定约束条件下，能使每条消息的预期端到端延迟最小化的整数捆绑阈值 $k$。将您的最终答案表示为不带单位的整数 $k$。", "solution": "用户希望找到整数捆绑阈值 $k$，使得在满足缓冲区内存和启动成本约束的前提下，每条消息的预期端到端延迟最小。\n\n首先，我们必须形式化对 $k$ 的约束。变量 $k$ 必须是正整数。\n\n第一个约束是缓冲区内存大小。一个捆绑包包含 $k$ 条消息，每条大小为 $s$。聚合负载的总大小为 $m = k \\cdot s$。这个聚合消息必须能放入大小为 $B$ 的可用缓冲区中。\n$$k \\cdot s \\le B$$\n我们可以解出 $k$ 的上限：\n$$k \\le \\frac{B}{s}$$\n使用给定的参数值，$s = 128$ 字节，$B = 16384$ 字节：\n$$k \\le \\frac{16384}{128} = \\frac{2^{14}}{2^7} = 2^7 = 128$$\n因此，捆绑阈值 $k$ 必须小于或等于 $128$。\n\n第二个约束是单位墙上时钟时间的启动时间预算。消息以每秒 $\\lambda$ 条的速率到达。由于我们将 $k$ 条消息捆绑成一次发送操作，因此这些发送操作的速率为 $\\frac{\\lambda}{k}$。每次发送操作会产生 $\\alpha$ 的启动延迟成本。每秒消耗的总启动时间是发送速率与每次发送成本的乘积：\n$$\\text{每秒启动时间} = \\frac{\\lambda}{k} \\cdot \\alpha$$\n该值不得超过最大允许预算 $C_{\\mathrm{max}}$：\n$$\\frac{\\lambda \\alpha}{k} \\le C_{\\mathrm{max}}$$\n我们可以解出 $k$ 的下限：\n$$k \\ge \\frac{\\lambda \\alpha}{C_{\\mathrm{max}}}$$\n使用给定的参数值，$\\lambda = 2400$ 条消息/秒，$\\alpha = 3.5$ 微秒，以及 $C_{\\mathrm{max}} = 3000$ 微秒/秒：\n$$k \\ge \\frac{2400 \\times 3.5}{3000} = \\frac{8400}{3000} = 2.8$$\n由于 $k$ 必须是整数，满足此约束的最小可能值是 $k=3$。\n\n结合这两个约束，整数捆绑阈值 $k$ 的有效范围是 $3 \\le k \\le 128$。\n\n接下来，我们构造目标函数，即每条消息的预期端到端延迟，记为 $L(k)$。任何消息的延迟都是其在缓冲区中的等待时间 $T_{\\text{wait}}$ 和聚合捆绑包的传输时间 $T_{\\text{transmit}}$ 之和。\n$$L(k) = E[T_{\\text{wait}}] + T_{\\text{transmit}}$$\n传输时间 $T_{\\text{transmit}}$ 是针对大小为 $m = sk$ 的聚合消息的。根据启动-带宽模型，该时间为：\n$$T_{\\text{transmit}}(k) = \\alpha + \\beta m = \\alpha + \\beta s k$$\n等待时间取决于到达过程。消息以速率 $\\lambda$ 根据泊松过程到达。这意味着连续消息到达之间的时间间隔是独立同分布的指数随机变量，其均值为 $\\frac{1}{\\lambda}$。\n当第 $k$ 条消息到达时，一个捆绑包完成并开始传输。考虑一个捆绑包中的消息，按到达顺序从 $i=1$ 到 $k$ 索引。第 $i$ 条消息必须等待随后的 $k-i$ 条消息到达。因此，第 $i$ 条消息的等待时间是 $k-i$ 个到达间隔时间之和。第 $i$ 条消息的预期等待时间为：\n$$E[T_{\\text{wait},i}] = (k-i) \\times \\frac{1}{\\lambda}$$\n为了求出捆绑包中每条消息的平均预期等待时间，我们将所有 $k$ 条消息的预期等待时间相加，然后除以 $k$：\n$$E[T_{\\text{wait}}] = \\frac{1}{k} \\sum_{i=1}^{k} E[T_{\\text{wait},i}] = \\frac{1}{k} \\sum_{i=1}^{k} \\frac{k-i}{\\lambda} = \\frac{1}{k\\lambda} \\sum_{i=1}^{k} (k-i)$$\n求和项是从 $0$ 到 $k-1$ 的整数之和：\n$$\\sum_{i=1}^{k} (k-i) = (k-1) + (k-2) + \\dots + 0 = \\frac{(k-1)k}{2}$$\n将此代回平均预期等待时间的表达式中：\n$$E[T_{\\text{wait}}] = \\frac{1}{k\\lambda} \\cdot \\frac{(k-1)k}{2} = \\frac{k-1}{2\\lambda}$$\n现在，我们可以写出每条消息的预期端到端延迟 $L(k)$ 的完整表达式：\n$$L(k) = \\frac{k-1}{2\\lambda} + \\alpha + \\beta s k$$\n为了找到使 $L(k)$ 最小化的 $k$ 值，我们可以重新排列各项来分析其对 $k$ 的依赖关系：\n$$L(k) = \\frac{k}{2\\lambda} - \\frac{1}{2\\lambda} + \\alpha + \\beta s k = k \\left(\\beta s + \\frac{1}{2\\lambda}\\right) + \\left(\\alpha - \\frac{1}{2\\lambda}\\right)$$\n该表达式表明 $L(k)$ 是一个形如 $f(k) = M k + C$ 的关于 $k$ 的线性函数，其中斜率是 $M = \\beta s + \\frac{1}{2\\lambda}$，截距是 $C = \\alpha - \\frac{1}{2\\lambda}$。\n给定参数为 $\\beta = 0.04  0$，$s = 128  0$，以及 $\\lambda = 2400  0$。斜率 $M$ 是两项正数之和：\n$$M = (0.04 \\times 128) + \\frac{1}{2 \\times 2400} = 5.12 + \\frac{1}{4800}  0$$\n由于斜率 $M$ 为正，所以 $L(k)$ 是关于 $k$ 的严格递增函数。\n要在一个闭区间上最小化一个严格递增函数，我们必须选择该区间中的最小值。$k$ 的有效定义域是区间 $[3, 128]$ 内的整数集合。\n因此，$L(k)$ 的最小值将在 $k$ 的最小有效整数值处取得，即 $k=3$。", "answer": "$$\\boxed{3}$$", "id": "3145373"}]}