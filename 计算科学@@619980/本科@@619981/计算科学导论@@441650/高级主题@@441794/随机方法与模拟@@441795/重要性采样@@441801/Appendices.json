{"hands_on_practices": [{"introduction": "重要性采样是一项强大的技术，但它建立在一个关键假设之上：在目标分布不为零的任何地方，提案分布也必须不为零。本练习是一个思想实验，旨在探讨当这个“支撑集条件”被违反时会发生什么。通过分析这个精心构建的场景，您将深刻理解估计量产生偏差的潜在风险，并明白为何确保适当的支撑集覆盖是设计一个有效的重要性采样估计器的首要且最关键的一步。[@problem_id:3143026]", "problem": "你的任务是估计量 $\\mu = \\mathbb{E}_{p}[g(X)]$，其中 $g$ 是一个有界可测函数，$X$ 是一个实值随机变量，其概率密度函数为 $p$。考虑以下科学上合理的情景：目标密度 $p$ 在区间 $[0, 2]$ 上均匀分布，而用于重要性采样的提议密度 $q$ 在区间 $[0, 1.5]$ 上均匀分布。具体来说，\n$$\np(x) = \\frac{1}{2}\\,\\mathbf{1}_{[0, 2]}(x), \\qquad q(x) = \\frac{2}{3}\\,\\mathbf{1}_{[0, 1.5]}(x),\n$$\n其中 $\\mathbf{1}_{A}(x)$ 是集合 $A$ 的指示函数。标准重要性采样方法使用样本 $X_{1}, \\dots, X_{n} \\overset{\\text{i.i.d.}}{\\sim} q$ 和权重 $w(x) = \\frac{p(x)}{q(x)}$ 来构造 $\\mu$ 的估计量。在这种情景下，区间 $[1.5, 2]$ 在 $p$ 下有正测度，但在 $q$ 下测度为零，这在提议支撑集中造成了一个间隙。\n\n为了弥补支撑集间隙，一种常见的策略是用一个保证支撑集覆盖的混合提议来替换 $q$。令 $r$ 为 $[0, 2]$ 上的均匀密度，即\n$$\nr(x) = \\frac{1}{2}\\,\\mathbf{1}_{[0, 2]}(x),\n$$\n并考虑混合提议\n$$\nq_{\\varepsilon}(x) = (1 - \\varepsilon)\\,q(x) + \\varepsilon\\,r(x),\n$$\n其中 $\\varepsilon \\in (0, 1)$。\n\n选择在此情景下所有正确的陈述。\n\nA. 对于 $g(x) = 1$，使用 $q$ 的标准重要性采样估计量是 $\\mu$ 的无偏估计量。\n\nB. $p$ 关于 $q$ 绝对连续的条件（即 $p \\ll q$）在此不成立。因此，使用 $q$ 的标准重要性采样估计量是有偏的，除非在 $[1.5, 2]$ 上 $g(x) = 0$。\n\nC. 使用混合提议 $q_{\\varepsilon}$ 对任何有界函数 $g$ 恢复了无偏性，并产生一个有限方差的重要性采样估计量；然而，当 $\\varepsilon \\to 0$ 时，方差可能变得任意大。\n\nD. 将权重 $\\frac{p(x)}{q(x)}$ 乘以一个固定常数来限制其大小，可以防止偏差并确保有限方差，即使 $q$ 在 $p$ 的部分支撑集上密度为零。\n\nE. 改用自归一化重要性采样 (SNIS) 估计量可以消除对支撑集覆盖的需求；即使 $q$ 在 $p(x) > 0$ 的地方密度为零，无偏性也成立。", "solution": "用户希望我验证问题陈述，如果有效，则提供一个详细的解决方案来评估每个选项。\n\n### 问题验证\n\n**第 1 步：提取已知条件**\n- 待估计量：$\\mu = \\mathbb{E}_{p}[g(X)]$。\n- $g$：一个有界可测函数。\n- $X$：一个实值随机变量。\n- 目标概率密度函数 (PDF)：$p(x) = \\frac{1}{2}\\,\\mathbf{1}_{[0, 2]}(x)$。\n- 标准重要性采样的提议 PDF：$q(x) = \\frac{2}{3}\\,\\mathbf{1}_{[0, 1.5]}(x)$。\n- 标准重要性采样使用独立同分布样本 $X_{1}, \\dots, X_{n} \\sim q$。\n- 重要性权重：$w(x) = \\frac{p(x)}{q(x)}$。\n- 用于混合建模的次级密度：$r(x) = \\frac{1}{2}\\,\\mathbf{1}_{[0, 2]}(x)$。\n- 混合提议 PDF：$q_{\\varepsilon}(x) = (1 - \\varepsilon)\\,q(x) + \\varepsilon\\,r(x)$，其中 $\\varepsilon \\in (0, 1)$。\n\n**第 2 步：使用提取的已知条件进行验证**\n- **科学基础：** 该问题是计算统计学中的一个标准练习，特别是蒙特卡罗方法。重要性采样、概率密度、期望、偏差和方差等概念都是公认的数学和统计学原理。该设置是合理的。\n- **适定性：** 问题为密度 $p$、$q$ 和 $r$ 提供了具体的函数形式，并定义了估计量和感兴趣的量。它要求评估几个精确的陈述，这是一个明确定义的任务。\n- **客观性：** 问题以精确、客观的数学语言陈述。\n- **不完整或矛盾的设置：** 提供了所有必要的信息。密度函数定义明确且积分为 $1$。没有矛盾之处。\n- **不切实际或不可行：** 该设置是一个简化的理论案例，旨在说明重要性采样中的一个关键概念。它在物理上并非不切实际。\n- **不适定或结构不良：** 所用术语在计算科学和统计学领域是标准的且无歧义的。\n\n**第 3 步：结论与行动**\n问题陈述是有效的。它提出了一个清晰、一致且可解的问题，植根于既定的统计理论。我将继续进行解答。\n\n### 解答推导\n\n待估计的量是在分布 $p$ 下 $g(X)$ 的期望：\n$$\n\\mu = \\mathbb{E}_{p}[g(X)] = \\int_{-\\infty}^{\\infty} g(x)p(x)dx\n$$\n重要性采样旨在通过从另一个分布 $q$ 中抽取样本并重新加权来估计此值。基本恒等式为：\n$$\n\\mu = \\int_{-\\infty}^{\\infty} g(x) \\frac{p(x)}{q(x)} q(x) dx = \\mathbb{E}_{q}\\left[g(X)\\frac{p(X)}{q(X)}\\right]\n$$\n该恒等式成立的充要条件是 $p$ 的支撑集是 $q$ 的支撑集的子集，即 $p(x) > 0 \\implies q(x) > 0$。此条件也写作 $p$ 关于 $q$ 绝对连续（$p \\ll q$）。\n\n对于大小为 $n$ 的样本，标准（或“普通”）重要性采样估计量为：\n$$\n\\hat{\\mu}_{n} = \\frac{1}{n} \\sum_{i=1}^n g(X_i) w(X_i), \\quad \\text{其中 } X_i \\overset{\\text{i.i.d.}}{\\sim} q \\text{ 且 } w(x) = \\frac{p(x)}{q(x)}。\n$$\n该估计量的期望为：\n$$\n\\mathbb{E}[\\hat{\\mu}_{n}] = \\mathbb{E}_{q}\\left[g(X)w(X)\\right] = \\int_{\\{x: q(x) > 0\\}} g(x)\\frac{p(x)}{q(x)} q(x) dx = \\int_{\\{x: q(x) > 0\\}} g(x)p(x)dx。\n$$\n如果 $\\mathbb{E}[\\hat{\\mu}_{n}] = \\mu$，则估计量是无偏的。这要求 $\\int_{\\{x: q(x) > 0\\}} g(x)p(x)dx = \\int_{\\{x: p(x) > 0\\}} g(x)p(x)dx$，这当且仅当 $\\int_{\\{x: p(x)>0, q(x)=0\\}} g(x)p(x)dx = 0$ 时成立。\n\n在这个问题中：\n- $p$ 的支撑集是 $\\text{supp}(p) = [0, 2]$。\n- $q$ 的支撑集是 $\\text{supp}(q) = [0, 1.5]$。\n$p(x) > 0$ 且 $q(x) = 0$ 的集合是区间 $(1.5, 2]$。由于在此区间上 $p(x) = 1/2 > 0$，所以估计量是有偏的，除非 $\\int_{1.5}^{2} g(x) dx = 0$。\n\n### 逐项分析\n\n**A. 对于 $g(x) = 1$，使用 $q$ 的标准重要性采样估计量是 $\\mu$ 的无偏估计量。**\n\n首先，我们求出真值 $\\mu$。对于 $g(x) = 1$：\n$$\n\\mu = \\mathbb{E}_{p}[1] = \\int_{0}^{2} 1 \\cdot p(x) dx = \\int_{0}^{2} \\frac{1}{2} dx = \\frac{1}{2} [x]_{0}^{2} = 1。\n$$\n接下来，我们求出估计量 $\\hat{\\mu}_{n}$ 的期望。样本 $X_i$ 是从 $q$ 中抽取的，其支撑集为 $[0, 1.5]$。\n$$\n\\mathbb{E}[\\hat{\\mu}_{n}] = \\int_{\\text{supp}(q)} g(x) \\frac{p(x)}{q(x)} q(x) dx = \\int_{0}^{1.5} 1 \\cdot p(x) dx = \\int_{0}^{1.5} \\frac{1}{2} dx = \\frac{1}{2} [x]_{0}^{1.5} = \\frac{1.5}{2} = 0.75。\n$$\n由于 $\\mathbb{E}[\\hat{\\mu}_{n}] = 0.75$ 而 $\\mu = 1$，该估计量是有偏的。\n\n结论：**错误**。\n\n**B. $p$ 关于 $q$ 绝对连续的条件（即 $p \\ll q$）在此不成立。因此，使用 $q$ 的标准重要性采样估计量是有偏的，除非在 $[1.5, 2]$ 上 $g(x) = 0$。**\n\n绝对连续性条件 $p \\ll q$ 要求 $\\text{supp}(p) \\subseteq \\text{supp}(q)$。这里，$\\text{supp}(p) = [0, 2]$ 且 $\\text{supp}(q) = [0, 1.5]$。由于 $[0, 2] \\not\\subseteq [0, 1.5]$，该条件不成立。这部分陈述是正确的。\n\n估计量的偏差由下式给出：\n$$\n\\text{Bias} = \\mathbb{E}[\\hat{\\mu}_{n}] - \\mu = \\int_{\\text{supp}(q)} g(x)p(x)dx - \\int_{\\text{supp}(p)} g(x)p(x)dx = -\\int_{\\text{supp}(p) \\setminus \\text{supp}(q)} g(x)p(x)dx。\n$$\n在本例中，这变为：\n$$\n\\text{Bias} = -\\int_{1.5}^{2} g(x)p(x)dx = -\\frac{1}{2} \\int_{1.5}^{2} g(x)dx。\n$$\n该估计量是无偏的，当且仅当此偏差项为零。由于在 $(1.5, 2]$ 上 $p(x) > 0$，这要求 $\\int_{1.5}^{2} g(x)dx = 0$。“在 $[1.5, 2]$ 上 $g(x) = 0$”是使该积分为零的一个充分条件。对于一个一般的非负函数 $g$，它（几乎处处）也是一个必要条件。因此，该陈述准确地指出了支撑集条件的失效及其对偏差的直接后果，以及消除这种偏差所需的关于 $g$ 的条件。\n\n结论：**正确**。\n\n**C. 使用混合提议 $q_{\\varepsilon}$ 对任何有界函数 $g$ 恢复了无偏性，并产生一个有限方差的重要性采样估计量；然而，当 $\\varepsilon \\to 0$ 时，方差可能变得任意大。**\n\n混合提议是 $q_{\\varepsilon}(x) = (1 - \\varepsilon)\\,q(x) + \\varepsilon\\,r(x)$。由于 $\\text{supp}(q) = [0, 1.5]$ 和 $\\text{supp}(r) = [0, 2]$，混合提议的支撑集是 $\\text{supp}(q_{\\varepsilon}) = [0, 2]$。这与 $p(x)$ 的支撑集相匹配，因此 $p \\ll q_{\\varepsilon}$。这保证了使用 $q_{\\varepsilon}$ 的重要性采样估计量对于任何有界函数 $g$ 都是无偏的。\n\n估计量的方差与加权样本的二阶矩有关，这涉及到积分 $\\int g(x)^2 \\frac{p(x)^2}{q_{\\varepsilon}(x)} dx$。为了使方差有限，该积分必须收敛。\n让我们分析在有问题的区间 $(1.5, 2]$ 上的被积函数。在此区间上，$q(x) = 0$ 且 $p(x)=r(x)=1/2$。所以，$q_{\\varepsilon}(x) = \\varepsilon r(x) = \\varepsilon/2$。\n来自此区间的方差积分的贡献是：\n$$\n\\int_{1.5}^{2} g(x)^2 \\frac{p(x)^2}{q_{\\varepsilon}(x)} dx = \\int_{1.5}^{2} g(x)^2 \\frac{(1/2)^2}{\\varepsilon/2} dx = \\frac{1}{2\\varepsilon} \\int_{1.5}^{2} g(x)^2 dx。\n$$\n因为 $g$ 是有界的，对于任何固定的 $\\varepsilon > 0$，这个积分是有限的。在 $[0, 1.5]$ 上的积分也是有限的，因为 $q_{\\varepsilon}(x)$ 由一个正常数从下方界定。因此，对于任何 $\\varepsilon \\in (0, 1)$，方差是有限的。\n\n然而，当 $\\varepsilon \\to 0$ 时，项 $\\frac{1}{2\\varepsilon} \\int_{1.5}^{2} g(x)^2 dx$ 将发散到无穷大，除非在 $(1.5, 2]$ 上 $g(x)=0$。因此，当 $\\varepsilon \\to 0$ 时，方差可能变得任意大。该陈述在所有三点上都是正确的。\n\n结论：**正确**。\n\n**D. 将权重 $\\frac{p(x)}{q(x)}$ 乘以一个固定常数来限制其大小，可以防止偏差并确保有限方差，即使 $q$ 在 $p$ 的部分支撑集上密度为零。**\n\n这种技术被称为权重截断。定义一个新权重 $w'(x) = \\min(w(x), C)$，其中 $C$ 为某个上限。得到的估计量是 $\\hat{\\mu}'_n = \\frac{1}{n} \\sum g(X_i) w'(X_i)$。其期望是 $\\mathbb{E}[\\hat{\\mu}'_n] = \\mathbb{E}_q[g(X) \\min(w(X), C)]$。这不等于 $\\mathbb{E}_q[g(X)w(X)]$，所以这个过程为了控制方差而*引入了*偏差。它“防止偏差”的说法是错误的。\n此外，在当前问题中，问题不在于权重过大，而是在区域 $(1.5, 2]$ 中完全没有样本。在采样区域 $[0, 1.5]$ 上的权重是恒定的：$w(x) = \\frac{p(x)}{q(x)} = \\frac{1/2}{2/3} = 3/4$。除非 $C  3/4$，否则对其进行限制没有效果。更重要的是，这种技术没有做任何事情来从区域 $(1.5, 2]$ 收集信息，因此由支撑集不匹配引起的偏差仍然存在。该陈述有根本性缺陷。\n\n结论：**错误**。\n\n**E. 改用自归一化重要性采样 (SNIS) 估计量可以消除对支撑集覆盖的需求；即使 $q$ 在 $p(x)  0$ 的地方密度为零，无偏性也成立。**\n\n自归一化重要性采样 (SNIS) 估计量是 $\\hat{\\mu}_{SNIS} = \\frac{\\sum_{i=1}^n g(X_i) w(X_i)}{\\sum_{i=1}^n w(X_i)}$。该估计量是随机变量的比率，对于有限样本量 $n$ 通常是有偏的，即使在支撑集条件成立时也是如此。其主要优点是在 $p$ 仅在相差一个归一化常数的情况下已知时也能工作。“无偏性成立”的说法对于有限 $n$ 从一开始就是错误的。\n\n我们来检查一致性（即当 $n \\to \\infty$ 时是否收敛到 $\\mu$）。根据大数定律：\n- 分子平均值：$\\frac{1}{n}\\sum g(X_i)w(X_i) \\to \\mathbb{E}_q[g(X)w(X)] = \\int_{0}^{1.5} g(x)p(x)dx$。\n- 分母平均值：$\\frac{1}{n}\\sum w(X_i) \\to \\mathbb{E}_q[w(X)] = \\int_{0}^{1.5} p(x)dx$。\n\n所以，当 $n \\to \\infty$ 时，$\\hat{\\mu}_{SNIS} \\to \\frac{\\int_{0}^{1.5} g(x)p(x)dx}{\\int_{0}^{1.5} p(x)dx}$。这是 $g(X)$ 关于一个被截断到 $[0, 1.5]$ 并重新归一化的 $p(x)$ 版本的期望值。这通常不等于 $\\mu = \\int_{0}^{2} g(x)p(x)dx$。该估计量是不一致的，更不用说无偏了。支撑集条件 $p \\ll q$ 对于 SNIS 的一致性也至关重要。\n\n结论：**错误**。", "answer": "$$\\boxed{BC}$$", "id": "3143026"}, {"introduction": "一旦我们确保了提案分布覆盖了整个目标区域，下一个问题就是如何从无限的可能性中选择“最佳”的提案分布。一个理想的提案分布应能最小化我们估计的方差，从而在给定的计算量下得到最精确的结果。本练习要求您在一个受控的环境中，通过解析方法找到最优的提案分布，这为指导高效蒙特卡洛模拟设计的方差缩减原则提供了一个具体的范例。[@problem_id:767870]", "problem": "重要性采样是一种方差缩减技术，用于蒙特卡罗方法中，在从一个不同的分布中抽取样本的同时，估计某个特定分布的性质。假设我们希望估计期望 $I = E_p[f(X)] = \\int f(x) p(x) dx$，其中 $p(x)$ 是目标概率密度函数。我们可以不从 $p(x)$ 中采样，而是从一个提议分布 $q(x)$ 中抽取样本 $\\{X_i\\}_{i=1}^N$，并构建如下估计量：\n$$ \\hat{I}_N = \\frac{1}{N} \\sum_{i=1}^N f(X_i) w(X_i) $$\n其中 $w(x) = \\frac{p(x)}{q(x)}$ 是重要性权重。该估计量是无偏的，即 $E_q[\\hat{I}_N] = I$。此方法的效率取决于估计量的方差，其方差由 $\\text{Var}_q(\\hat{I}_N) = \\frac{1}{N} \\text{Var}_q(f(X)w(X))$ 给出。为了在固定样本量 $N$ 的情况下最小化估计量的方差，必须最小化 $\\text{Var}_q(f(X)w(X))$。由于 $\\text{Var}_q(Y) = E_q[Y^2] - (E_q[Y])^2$ 且 $E_q[f(X)w(X)] = I$ 是一个相对于 $q(x)$ 的选择而言的常数，因此最小化方差等价于最小化二阶矩 $E_q[(f(X)w(X))^2]$。\n\n考虑一个场景，其中目标分布 $p(x)$ 是尺度参数为 $\\sigma_p$ 的瑞利分布。尺度参数为 $\\sigma$ 的瑞利分布的概率密度函数由下式给出：\n$$ g(x; \\sigma) = \\frac{x}{\\sigma^2} e^{-x^2 / (2\\sigma^2)} \\quad \\text{for } x \\ge 0 $$\n我们希望估计这个目标分布的均值，因此 $f(x)=x$。我们选择另一个具有可调尺度参数 $\\sigma_q$ 的瑞利分布作为我们的提议分布 $q(x)$。\n\n推导使重要性采样估计量方差最小化的提议分布尺度参数 $\\sigma_q$ 的最优值。用 $\\sigma_p$ 表示你的答案。", "solution": "我们希望最小化二阶矩\n$$M_2(\\sigma_q)=E_q[(f(X)w(X))^2]\n=\\int_0^\\infty x^2\\Bigl(\\frac{p(x)}{q(x)}\\Bigr)^2q(x)\\,dx\n=\\int_0^\\infty\\frac{x^2p(x)^2}{q(x)}\\,dx$$\n其中\n$$p(x)=\\frac{x}{\\sigma_p^2}e^{-x^2/(2\\sigma_p^2)},\\quad\nq(x)=\\frac{x}{\\sigma_q^2}e^{-x^2/(2\\sigma_q^2)}.$$\n代入可得\n$$M_2\n=\\int_0^\\infty\\frac{x^2\\,(x/\\sigma_p^2)^2e^{-x^2/\\sigma_p^2}}\n{(x/\\sigma_q^2)e^{-x^2/(2\\sigma_q^2)}}\\,dx\n=\\frac{\\sigma_q^2}{\\sigma_p^4}\\int_0^\\infty x^3e^{-ax^2}\\,dx,$$\n其中\n$$a=\\frac1{\\sigma_p^2}-\\frac1{2\\sigma_q^2}\n=\\frac{2\\sigma_q^2-\\sigma_p^2}{2\\sigma_p^2\\sigma_q^2}.$$\n使用\n$$\\int_0^\\infty x^3e^{-ax^2}dx=\\frac1{2a^2},$$\n我们得到\n$$M_2\n=\\frac{\\sigma_q^2}{\\sigma_p^4}\\,\\frac1{2a^2}\n=2\\,\\frac{\\sigma_q^6}{(2\\sigma_q^2-\\sigma_p^2)^2}.$$\n令 $x=\\sigma_q^2$ 并最小化\n$$F(x)=\\frac{x^3}{(2x-\\sigma_p^2)^2}$$\n通过求解 $dF/dx=0$。可以求得可行的临界点\n$$\\sigma_q^2=\\tfrac32\\,\\sigma_p^2,\n\\quad\\sigma_q=\\sigma_p\\sqrt{\\tfrac32}.$$", "answer": "$$\\boxed{\\sigma_p\\sqrt{\\frac{3}{2}}}$$", "id": "767870"}, {"introduction": "现在，让我们将理论应用于一个实际问题：在一个巨大的组合搜索空间中估计稀有事件的概率，从而将理论付诸实践。这个编程练习要求您从零开始实现一个重要性采样估计器，并有策略地选择提案分布，以便有效地采样我们感兴趣的稀有结果。您将面对诸如数值稳定性等真实世界中的挑战，并实现一个解决方案，以展示重要性采样在解决简单随机采样难以处理的问题时所表现出的真正威力。[@problem_id:3241899]", "problem": "考虑一个组合搜索空间，该空间定义为所有序列 $x = (x_1,x_2,\\dots,x_n)$ 的集合 $\\mathcal{X} = \\{1,2,\\dots,m\\}^n$，其中每个坐标 $x_i$ 是一个介于 $1$ 和 $m$ 之间的整数。一个从 $\\mathcal{X}$ 中均匀采样的随机搜索算法在 $\\mathcal{X}$ 上引入了一个目标分布 $p(x)$，其中对每个 $x \\in \\mathcal{X}$ 都有 $p(x) = m^{-n}$。在 $\\mathcal{X}$ 上定义一个实值成本函数 $C(x)$，其形式为\n$$\nC(x) = \\sum_{i=1}^{n} a_i x_i,\n$$\n其中 $a_i  0$ 是固定系数。对于给定的阈值 $T  0$，如果 $C(x) \\le T$，则称解 $x$ 是满足条件的。我们感兴趣的量是概率\n$$\n\\pi = \\mathbb{P}_p\\big(C(X) \\le T\\big),\n$$\n其中 $X \\sim p$。\n\n请根据期望、概率的定义以及测度变换原理，推导出一个可计算的 $\\pi$ 的重要性采样估计器。该估计器使用一个在 $\\mathcal{X}$ 上易于处理的提议分布 $q(x)$，此分布构造为独立的、逐坐标分布的乘积。具体来说，设 $q(x)$ 由独立坐标定义，其中\n$$\nq(x) = \\prod_{i=1}^{n} q_i(x_i),\n$$\n并且对于给定的参数 $\\beta \\ge 0$，\n$$\nq_i(k) \\propto \\exp\\big(-\\beta\\, a_i\\, k\\big), \\quad k \\in \\{1,2,\\dots,m\\}。\n$$\n为了使 $q_i$ 成为 $\\{1,2,\\dots,m\\}$ 上的有效概率质量函数，必须包含每个坐标的归一化常数。使用此提议分布，实现一个程序，该程序：\n- 从 $q$ 中抽取 $N$ 个独立点 $X^{(1)}, X^{(2)}, \\dots, X^{(N)}$。\n- 为每个采样点计算一个理论上合理的重要性权重 $w(x)$。\n- 基于采样点及其权重，生成 $\\pi$ 的一个估计器 $\\hat{\\pi}$。\n\n您必须在适当之处使用对数空间进行计算，以确保在大 $n$ 和 $m$ 情况下的数值稳定性。请使用固定的伪随机种子，以使结果可复现。所有采样和估计都应是纯数学的，无单位。\n\n您的程序必须解决以下参数设置的测试套件。对于每个测试用例，返回单个重要性采样估计值 $\\hat{\\pi}$，形式为浮点数：\n\n- 测试用例 1（一般情况）：$n = 12$, $m = 10$, $a_i = i$ 对于 $i = 1,2,\\dots,12$, $T = 300$, $\\beta = 0.2$, $N = 30000$, 种子 $= 42$。\n- 测试用例 2（通过 $\\beta = 0$ 实现的均匀基线）：$n = 12$, $m = 10$, $a_i = i$ 对于 $i = 1,2,\\dots,12$, $T = 300$, $\\beta = 0.0$, $N = 30000$, 种子 $= 43$。\n- 测试用例 3（稀有事件强调）：$n = 20$, $m = 20$, $a_i = ((i-1) \\bmod 5) + 1$ 对于 $i = 1,2,\\dots,20$, $T = 200$, $\\beta = 0.3$, $N = 40000$, 种子 $= 44$。\n- 测试用例 4（每个样本都满足条件的边界情况）：$n = 8$, $m = 15$, $a_i = 2$ 对于所有 $i$, $T = 240$, $\\beta = 0.5$, $N = 10000$, 种子 $= 45$。\n\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表的结果，每个浮点数四舍五入到六位小数，例如 $[r_1,r_2,r_3,r_4]$，其中每个 $r_j$ 对应于按上述顺序给出的测试用例 $j$ 的估计值。", "solution": "用户希望找到一个用于概率的重要性采样估计器并加以实现。\n\n### 第1步：问题验证\n\n问题是估计概率 $\\pi = \\mathbb{P}_p(C(X) \\le T)$，其中搜索空间为 $\\mathcal{X} = \\{1,2,\\dots,m\\}^n$，目标分布为均匀分布 $p(x) = m^{-n}$，成本函数为 $C(x) = \\sum_{i=1}^{n} a_i x_i$。估计必须使用重要性采样方法，并采用一个特定的提议分布 $q(x)$。\n\n**提取的已知条件：**\n- **搜索空间**：$\\mathcal{X} = \\{1,2,\\dots,m\\}^n$。\n- **目标分布**：对所有 $x \\in \\mathcal{X}$，$p(x) = m^{-n}$。\n- **成本函数**：$C(x) = \\sum_{i=1}^{n} a_i x_i$，其中 $a_i  0$。\n- **感兴趣的事件**：$C(x) \\le T$，对于阈值 $T  0$。\n- **感兴趣的量**：$\\pi = \\mathbb{P}_p\\big(C(X) \\le T\\big)$。\n- **提议分布**：$q(x) = \\prod_{i=1}^{n} q_i(x_i)$，其中对于 $k \\in \\{1,2,\\dots,m\\}$ 和 $\\beta \\ge 0$，$q_i(k) \\propto \\exp(-\\beta\\, a_i\\, k)$。\n- **实现细节**：使用 $N$ 个样本，使用对数空间确保数值稳定性，使用固定的种子，并解决四个指定的测试用例。\n\n**使用提取的已知条件进行验证：**\n1.  **科学依据**：该问题是重要性采样的一个标准应用，这是蒙特卡洛方法中的一种基本技术，广泛应用于物理学、统计学和科学计算。其数学表述基于成熟的概率论。问题有效。\n2.  **适定性**：每个测试用例的所有参数（$n, m, a_i, T, \\beta, N$, 种子）都已指定。分布和待估计的量都得到了明确的定义。可以推导出估计器的唯一形式。问题有效。\n3.  **客观性**：问题使用形式化的数学语言陈述，所有术语都已明确定义。它不含主观性。问题有效。\n4.  **完整性与一致性**：问题是自洽的，并提供了所有必要信息。要求对提议分布进行归一化，这与概率质量函数的定义是一致的。问题有效。\n5.  **可行性**：指定的参数在计算可行的范围内。提示正确地预见到了数值稳定性问题，并建议使用对数空间，这是一种标准做法。问题有效。\n\n**结论**：问题有效且定义明确。我将继续推导和实现解决方案。\n\n### 第2步：重要性采样估计器的推导\n\n概率 $\\pi$ 可以表示为关于目标分布 $p(x)$ 的指示函数 $\\mathbb{I}(\\cdot)$ 的期望：\n$$\n\\pi = \\mathbb{E}_p[\\mathbb{I}(C(X) \\le T)] = \\sum_{x \\in \\mathcal{X}} p(x) \\mathbb{I}(C(x) \\le T)\n$$\n使用测度变换原理，我们引入提议分布 $q(x)$，将期望重写为关于 $q$ 的期望：\n$$\n\\pi = \\sum_{x \\in \\mathcal{X}} \\frac{p(x)}{q(x)} q(x) \\mathbb{I}(C(x) \\le T) = \\mathbb{E}_q\\left[\\frac{p(X)}{q(X)} \\mathbb{I}(C(X) \\le T)\\right]\n$$\n项 $w(x) = \\frac{p(x)}{q(x)}$ 是重要性权重。重要性采样估计器 $\\hat{\\pi}$ 是这个新量的样本均值，基于从 $q(x)$ 中抽取的 $N$ 个样本 $X^{(j)}$：\n$$\n\\hat{\\pi} = \\frac{1}{N} \\sum_{j=1}^{N} w(X^{(j)}) \\mathbb{I}(C(X^{(j)}) \\le T)\n$$\n\n为了实现这一点，我们需要 $p(x)$、$q(x)$ 和 $w(x)$ 的显式形式。\n目标分布是 $p(x) = m^{-n}$。\n提议分布是 $q(x) = \\prod_{i=1}^{n} q_i(x_i)$。每个逐坐标分布 $q_i$ 必须进行归一化：\n$$\nq_i(k) = \\frac{\\exp(-\\beta a_i k)}{Z_i}, \\quad \\text{对于 } k \\in \\{1, 2, \\dots, m\\}\n$$\n其中 $Z_i$ 是归一化常数（配分函数）：\n$$\nZ_i = \\sum_{k=1}^{m} \\exp(-\\beta a_i k)\n$$\n完整的提议分布是 $q(x) = \\prod_{i=1}^{n} \\frac{\\exp(-\\beta a_i x_i)}{Z_i} = \\frac{\\exp(-\\beta \\sum_{i=1}^n a_i x_i)}{\\prod_{i=1}^n Z_i} = \\frac{\\exp(-\\beta C(x))}{\\prod_{i=1}^n Z_i}$。\n\n重要性权重 $w(x)$ 是：\n$$\nw(x) = \\frac{p(x)}{q(x)} = \\frac{m^{-n}}{\\frac{\\exp(-\\beta C(x))}{\\prod_{i=1}^n Z_i}} = m^{-n} \\exp(\\beta C(x)) \\prod_{i=1}^n Z_i\n$$\n\n### 第3步：数值实现与对数空间计算\n\n直接计算 $m^{-n}$ 和许多项的乘积在数值上是不稳定的。我们必须在对数空间中工作。\n\n重要性权重的对数是：\n$$\n\\log w(x) = \\log(m^{-n}) + \\log(\\exp(\\beta C(x))) + \\log\\left(\\prod_{i=1}^n Z_i\\right)\n$$\n$$\n\\log w(x) = -n \\log m + \\beta C(x) + \\sum_{i=1}^n \\log Z_i\n$$\n为了稳定性，归一化常数 $\\log Z_i$ 使用 log-sum-exp 技巧计算：\n$$\n\\log Z_i = \\log\\left(\\sum_{k=1}^{m} \\exp(-\\beta a_i k)\\right)\n$$\n设 $v_k = -\\beta a_i k$。设 $v_{\\max} = \\max_{k} \\{v_k\\} = -\\beta a_i$（因为 $a_i  0, \\beta \\ge 0, k \\ge 1$）。\n$$\n\\log Z_i = v_{\\max} + \\log\\left(\\sum_{k=1}^{m} \\exp(v_k - v_{\\max})\\right)\n$$\n在 $\\beta=0$ 的特殊情况下，$q_i(k)$ 变为 $\\{1,\\dots,m\\}$ 上的均匀分布，所以 $q_i(k) = 1/m$。在这种情况下，$q(x) = p(x)$，所有权重 $w(x)=1$。我们的公式必须能正确处理这种情况：如果 $\\beta=0$，则 $\\exp(-\\beta a_i k)=1$，所以 $Z_i = \\sum_{k=1}^m 1 = m$，且 $\\log Z_i = \\log m$。对数权重变为 $\\log w(x) = -n \\log m + 0 \\cdot C(x) + \\sum_{i=1}^n \\log m = -n \\log m + n \\log m = 0$，所以 $w(x)=1$，符合预期。\n\n然后，估计器可以计算为：\n$$\n\\hat{\\pi} = \\frac{1}{N} \\sum_{j=1}^{N} \\mathbb{I}(C(X^{(j)}) \\le T) \\exp(\\log w(X^{(j)}))\n$$\n\n### 算法\n对于每个测试用例：\n1.  **设置**：初始化参数 $n, m, a, T, \\beta, N$，并使用给定的种子初始化随机数生成器。\n2.  **预计算**：\n    a. 对于每个坐标 $i \\in \\{1, \\dots, n\\}$，计算 $\\log Z_i$。单独处理 $\\beta=0$ 的情况。\n    b. 对于每个坐标 $i$，计算概率质量函数（PMF）向量 $P_i = [q_i(1), \\dots, q_i(m)]$。概率为 $q_i(k) = \\exp(-\\beta a_i k - \\log Z_i)$。\n3.  **采样**：\n    a. 创建一个 $N \\times n$ 的矩阵来存储样本。\n    b. 对于每个坐标 $i$，从由PMF $P_i$ 定义的离散分布中抽取 $N$ 个样本，并将它们存储在矩阵的第 $i$ 列。这可以通过逆变换采样或使用像 `numpy.random.choice` 这样的库函数来完成。\n4.  **估计**：\n    a. 对于 $N$ 个样本向量中的每一个 $X^{(j)}$，计算成本 $C(X^{(j)}) = \\sum_i a_i X_i^{(j)}$。这可以向量化为矩阵向量乘积。\n    b. 识别出 $C(X^{(j)}) \\le T$ 的样本。\n    c. 仅对这些满足条件的样本，计算对数权重 $\\log w(X^{(j)})$。\n    d. 通过指数运算将对数权重转换为权重，并将它们相加。\n    e. 将总和除以 $N$ 以获得最终估计值 $\\hat{\\pi}$。\n\n这个过程提供了一种数值稳定且理论上可靠的方法来计算所需的估计值。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves a set of importance sampling problems.\n    \"\"\"\n    test_cases = [\n        # (n, m, a_rule, T, beta, N, seed)\n        (12, 10, 'i',      300, 0.2, 30000, 42),\n        (12, 10, 'i',      300, 0.0, 30000, 43),\n        (20, 20, 'mod',    200, 0.3, 40000, 44),\n        ( 8, 15, 'const',  240, 0.5, 10000, 45),\n    ]\n\n    results = []\n    \n    for n, m, a_rule, T, beta, N, seed in test_cases:\n        rng = np.random.default_rng(seed)\n\n        # 1. Generate 'a' vector based on rule\n        if a_rule == 'i':\n            a = np.arange(1, n + 1, dtype=np.float64)\n        elif a_rule == 'mod':\n            a = ((np.arange(n)) % 5) + 1.0\n        elif a_rule == 'const':\n            a = np.full(n, 2.0)\n\n        # 2. Pre-compute log_Z and sampling distributions (PMFs)\n        log_Z = np.zeros(n)\n        all_pmfs = []\n        k_vals = np.arange(1, m + 1, dtype=np.float64)\n\n        for i in range(n):\n            if beta == 0.0:\n                log_Z[i] = np.log(m)\n                pmf = np.full(m, 1.0 / m)\n            else:\n                # Use log-sum-exp trick for numerical stability\n                v = -beta * a[i] * k_vals\n                # v_max is the first element since k_vals is increasing and arg is negative\n                v_max = v[0] \n                log_Z[i] = v_max + np.log(np.sum(np.exp(v - v_max)))\n                \n                # Compute Probability Mass Function (PMF)\n                log_pmf = v - log_Z[i]\n                pmf = np.exp(log_pmf)\n            \n            # Normalize to correct for minute floating-point inaccuracies\n            pmf /= np.sum(pmf)\n            all_pmfs.append(pmf)\n\n        # 3. Generate N samples from the proposal distribution q(x)\n        samples = np.zeros((N, n), dtype=int)\n        for i in range(n):\n            samples[:, i] = rng.choice(k_vals, size=N, p=all_pmfs[i])\n            \n        # 4. Compute costs for all N samples (vectorized)\n        costs = samples @ a\n        \n        # 5. Filter for satisfactory samples\n        satisfactory_mask = costs = T\n        satisfactory_costs = costs[satisfactory_mask]\n        \n        pi_hat = 0.0\n        if satisfactory_costs.size > 0:\n            # 6. Compute importance weights for satisfactory samples\n            log_w_const = -n * np.log(m) + np.sum(log_Z)\n            log_weights = beta * satisfactory_costs + log_w_const\n            \n            # 7. Sum weights and compute the final estimator\n            # The sum is only over the satisfactory samples as per the estimator formula\n            total_sum_of_weights = np.sum(np.exp(log_weights))\n            pi_hat = total_sum_of_weights / N\n            \n        results.append(f\"{pi_hat:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3241899"}]}