{"hands_on_practices": [{"introduction": "我们以一个经典的蒙特卡洛问题——估算 $\\pi$——来开启我们的动手实践。这个练习将引导你从第一性原理出发，通过解析推导来得到最优控制变量系数 $\\beta^{\\star}$。通过这个具体的计算，你将加深对控制变量法如何以及为何能有效减少方差的理论理解 [@problem_id:3218869]。", "problem": "一种估计常数 $\\pi$ 的常用蒙特卡洛方法是，从正方形 $[-1,1] \\times [-1,1]$ 中均匀抽取 $N$ 个独立同分布 (i.i.d.) 的点，并利用落在以原点为中心的单位圆内的点的比例。设 $Y_i$ 为指示随机变量 $Y_i = \\mathbf{1}\\{(U_i,V_i) \\in \\{(x,y): x^2 + y^2 \\leq 1\\}\\}$，其中 $(U_i,V_i)$ 是第 $i$ 个样本点，并定义估计量 $\\hat{\\pi} = 4 \\,\\bar{Y}_N$，其中 $\\bar{Y}_N = \\frac{1}{N} \\sum_{i=1}^{N} Y_i$。\n\n为使用控制变量法减小方差，引入指示随机变量 $X_i = \\mathbf{1}\\{(U_i,V_i) \\in P_{12}\\}$，其中 $P_{12}$ 是内接于同一单位圆的正十二边形。期望 $\\mathbb{E}[X_i]$ 是 $P_{12}$ 的面积除以正方形 $[-1,1] \\times [-1,1]$ 的面积。考虑调整后的估计量\n$$\n\\hat{\\pi}_{\\beta} = 4 \\left( \\bar{Y}_N - \\beta \\left( \\bar{X}_N - \\mathbb{E}[X_i] \\right) \\right),\n$$\n其中 $\\bar{X}_N = \\frac{1}{N} \\sum_{i=1}^{N} X_i$ 且 $\\beta \\in \\mathbb{R}$ 是一个待选系数。\n\n仅从期望、方差、协方差以及圆内接正多边形面积的基本定义出发，推导使 $\\hat{\\pi}_{\\beta}$ 方差最小化的最优控制变量系数 $\\beta^{\\star}$ 的表达式，然后计算其对于正十二边形 $P_{12}$ 的精确值。将最终答案表示为单个闭式解析表达式。无需进行数值取整。", "solution": "目标是找到使估计量 $\\hat{\\pi}_{\\beta}$ 方差最小化的最优控制变量系数 $\\beta^{\\star}$。该估计量由下式给出\n$$ \\hat{\\pi}_{\\beta} = 4 \\left( \\bar{Y}_N - \\beta \\left( \\bar{X}_N - \\mathbb{E}[X_i] \\right) \\right) $$\n其中 $\\bar{Y}_N = \\frac{1}{N} \\sum_{i=1}^{N} Y_i$ 且 $\\bar{X}_N = \\frac{1}{N} \\sum_{i=1}^{N} X_i$。随机变量 $(Y_i, X_i)$ 对于 $i=1, \\dots, N$ 是独立同分布的。为简便起见，我们将一个泛型对记为 $(Y, X)$。\n\n首先，我们推导最优系数 $\\beta^{\\star}$ 的通用表达式。我们从计算 $\\hat{\\pi}_{\\beta}$ 的方差开始。\n$$ \\text{Var}(\\hat{\\pi}_{\\beta}) = \\text{Var}\\left( 4 \\left( \\bar{Y}_N - \\beta \\bar{X}_N + \\beta \\mathbb{E}[X] \\right) \\right) $$\n项 $\\beta \\mathbb{E}[X]$ 是一个常数，不影响方差。因此，\n$$ \\text{Var}(\\hat{\\pi}_{\\beta}) = \\text{Var}(4(\\bar{Y}_N - \\beta \\bar{X}_N)) = 16 \\, \\text{Var}(\\bar{Y}_N - \\beta \\bar{X}_N) $$\n利用方差的性质，\n$$ \\text{Var}(\\bar{Y}_N - \\beta \\bar{X}_N) = \\text{Var}(\\bar{Y}_N) + \\beta^2 \\text{Var}(\\bar{X}_N) - 2\\beta \\text{Cov}(\\bar{Y}_N, \\bar{X}_N) $$\n由于样本是独立同分布的，样本均值的方差是单个观测值方差的 $\\frac{1}{N}$ 倍，样本均值的协方差也类似：\n$$ \\text{Var}(\\bar{Y}_N) = \\frac{1}{N} \\text{Var}(Y), \\quad \\text{Var}(\\bar{X}_N) = \\frac{1}{N} \\text{Var}(X), \\quad \\text{Cov}(\\bar{Y}_N, \\bar{X}_N) = \\frac{1}{N} \\text{Cov}(Y, X) $$\n将这些代入 $\\hat{\\pi}_{\\beta}$ 的方差表达式中：\n$$ \\text{Var}(\\hat{\\pi}_{\\beta}) = \\frac{16}{N} \\left( \\text{Var}(Y) + \\beta^2 \\text{Var}(X) - 2\\beta \\text{Cov}(Y, X) \\right) $$\n为了找到使该方差最小的 $\\beta$ 值，我们对 $\\beta$ 求导并将结果设为零。\n$$ \\frac{d}{d\\beta} \\text{Var}(\\hat{\\pi}_{\\beta}) = \\frac{16}{N} \\left( 2\\beta \\text{Var}(X) - 2 \\text{Cov}(Y, X) \\right) $$\n将其设为零可得：\n$$ 2\\beta^{\\star} \\text{Var}(X) - 2 \\text{Cov}(Y, X) = 0 $$\n解出最优系数 $\\beta^{\\star}$，我们得到标准结果：\n$$ \\beta^{\\star} = \\frac{\\text{Cov}(Y, X)}{\\text{Var}(X)} $$\n现在，我们必须为该特定问题计算 $\\text{Cov}(Y, X)$ 和 $\\text{Var}(X)$。抽样域是正方形 $S = [-1,1] \\times [-1,1]$，其面积为 $A_S = 2 \\times 2 = 4$。\n随机变量定义为指示变量：\n$Y = \\mathbf{1}\\{C_1\\}$，其中 $C_1$ 是单位圆 $\\{(x,y) : x^2 + y^2 \\leq 1\\}$。\n$X = \\mathbf{1}\\{P_{12}\\}$，其中 $P_{12}$ 是内接于单位圆的正十二边形。\n\n对于一个对应于区域 $R$ 的指示变量 $Z = \\mathbf{1}\\{R\\}$，其期望是一个均匀抽样的点落在 $R$ 内的概率，即 $\\mathbb{E}[Z] = \\frac{\\text{Area}(R)}{\\text{Area}(S)}$。\n\n单位圆的面积是 $A_C = \\pi (1)^2 = \\pi$。因此，\n$$ \\mathbb{E}[Y] = \\frac{A_C}{A_S} = \\frac{\\pi}{4} $$\n内接于半径为 $R$ 的圆的正 $n$ 边形的面积由公式 $A_n = \\frac{1}{2} n R^2 \\sin(\\frac{2\\pi}{n})$ 给出。对于正十二边形 $P_{12}$，我们有 $n=12$ 和 $R=1$：\n$$ A_{12} = \\frac{1}{2} (12) (1)^2 \\sin\\left(\\frac{2\\pi}{12}\\right) = 6 \\sin\\left(\\frac{\\pi}{6}\\right) = 6 \\cdot \\frac{1}{2} = 3 $$\n因此 $X$ 的期望是：\n$$ \\mathbb{E}[X] = \\frac{A_{12}}{A_S} = \\frac{3}{4} $$\n接下来，我们计算 $X$ 的方差。由于 $X$ 是一个指示变量（一个伯努利随机变量），其方差由 $\\text{Var}(X) = \\mathbb{E}[X](1-\\mathbb{E}[X])$ 给出。\n$$ \\text{Var}(X) = \\frac{3}{4} \\left(1 - \\frac{3}{4}\\right) = \\frac{3}{4} \\cdot \\frac{1}{4} = \\frac{3}{16} $$\n现在我们计算协方差，$\\text{Cov}(Y, X) = \\mathbb{E}[YX] - \\mathbb{E}[Y]\\mathbb{E}[X]$。\n指示变量的乘积是 $YX = \\mathbf{1}\\{C_1\\} \\cdot \\mathbf{1}\\{P_{12}\\} = \\mathbf{1}\\{C_1 \\cap P_{12}\\}$。\n由于正十二边形 $P_{12}$ 内接于单位圆 $C_1$，任何在 $P_{12}$ 内的点也都在 $C_1$ 内。这意味着区域 $P_{12}$ 是区域 $C_1$ 的一个子集，即 $P_{12} \\subseteq C_1$。\n因此，它们的交集就是正十二边形本身：$C_1 \\cap P_{12} = P_{12}$。\n这意味着 $YX = \\mathbf{1}\\{P_{12}\\} = X$。\n所以，乘积的期望是：\n$$ \\mathbb{E}[YX] = \\mathbb{E}[X] = \\frac{3}{4} $$\n现在我们可以计算协方差：\n$$ \\text{Cov}(Y, X) = \\mathbb{E}[YX] - \\mathbb{E}[Y]\\mathbb{E}[X] = \\frac{3}{4} - \\left(\\frac{\\pi}{4}\\right)\\left(\\frac{3}{4}\\right) = \\frac{3}{4} - \\frac{3\\pi}{16} = \\frac{12 - 3\\pi}{16} = \\frac{3(4-\\pi)}{16} $$\n最后，我们将协方差和方差代入 $\\beta^{\\star}$ 的公式中：\n$$ \\beta^{\\star} = \\frac{\\text{Cov}(Y, X)}{\\text{Var}(X)} = \\frac{\\frac{3(4-\\pi)}{16}}{\\frac{3}{16}} $$\n分子和分母中的公因数 $\\frac{3}{16}$ 被消去，剩下：\n$$ \\beta^{\\star} = 4 - \\pi $$\n这就是最优控制变量系数。", "answer": "$$\n\\boxed{4-\\pi}\n$$", "id": "3218869"}, {"introduction": "第二个练习将探讨控制变量法中一个更为精妙的概念。我们将构建一个场景，其中两个随机变量之间线性不相关（即皮尔逊相关系数为零），但其中一个变量仍然可以作为另一个的强效控制变量。这个思想实验旨在挑战一个普遍的误解，即认为只有线性相关的变量才能用于方差缩减，并强调了理解变量间深层函数关系的重要性 [@problem_id:3218904]。", "problem": "考虑一个蒙特卡洛估计问题，目标是估计均值 $\\mu = \\mathbb{E}[X]$，其中您可以获取两个随机变量 $X$ 和 $Y$ 的样本。您需要构造一个 $(X,Y)$ 的联合分布，使得皮尔逊相关系数 $\\rho(X,Y)$ 为零，但一个非线性控制变量 $Z = g(Y)$ 却能带来非常大的方差缩减。仅使用期望、方差、协方差的基本定义以及正态分布的性质。\n\n设 $Y \\sim \\mathcal{N}(0,1)$ 且 $\\varepsilon \\sim \\mathcal{N}(0,\\sigma^{2})$，与 $Y$ 独立，其中 $\\sigma^{2} = 0.01$。定义\n$$\nX = (Y^{2} - 1) + \\varepsilon\n$$\n并考虑非线性控制变量\n$$\nZ = g(Y) = Y^{2} - 1,\n$$\n其均值 $\\mathbb{E}[Z]$ 可以从 $Y$ 的分布中得知。\n\n任务：\n- 使用协方差的核心定义和标准正态分布的矩的性质，验证协方差 $\\operatorname{Cov}(X,Y)$ 等于 $0$，从而相关系数 $\\rho(X,Y)$ 也等于 $0$。\n- 从方差和协方差的定义出发，不假设任何特定公式，推导使调整后估计量 $X - \\beta(Z - \\mathbb{E}[Z])$（用于估计 $\\mu$）的方差最小化的系数 $\\beta$ 的值。\n- 计算在最优 $\\beta$ 选择下，调整后估计量的单样本方差，以及方差缩减因子\n$$\nR = \\frac{\\operatorname{Var}(X)}{\\operatorname{Var}\\big(X - \\beta^{\\star}(Z - \\mathbb{E}[Z])\\big)}.\n$$\n\n以有序对 $(\\beta^{\\star}, R)$ 的精确形式给出您的最终答案。无需四舍五入。最终答案必须是按规定进行的计算。", "solution": "问题陈述经验证具有科学依据、定义明确且客观。它构成了一个概率论和数值方法中的规范练习。所有必要信息均已提供，任务定义清晰。我们可以开始求解。\n\n该问题要求完成与蒙特卡洛估计场景相关的三个任务。我们已知随机变量 $Y \\sim \\mathcal{N}(0,1)$ 和 $\\varepsilon \\sim \\mathcal{N}(0,\\sigma^{2})$，其中 $\\sigma^{2} = 0.01$。随机变量 $Y$ 和 $\\varepsilon$ 是独立的。我们关心的变量是 $X = (Y^{2} - 1) + \\varepsilon$，控制变量是 $Z = g(Y) = Y^{2} - 1$。\n\n首先，我们确定标准正态变量 $Y$ 的必要矩。\n$Y$ 的概率密度函数关于 $0$ 对称。因此，$Y$ 的所有奇数阶矩均为零。\n$\\mathbb{E}[Y] = 0$\n$\\mathbb{E}[Y^3] = 0$\n偶数阶矩是众所周知的。因为均值为零，所以二阶矩即为方差：\n$\\mathbb{E}[Y^2] = \\operatorname{Var}(Y) = 1$\n标准正态分布的四阶矩是：\n$\\mathbb{E}[Y^4] = 3$\n\n有了这些，我们就可以确定 $X$ 和 $Z$ 的性质。\n$Z$ 的均值是：\n$\\mathbb{E}[Z] = \\mathbb{E}[Y^2 - 1] = \\mathbb{E}[Y^2] - 1 = 1 - 1 = 0$。\n$X$ 的均值，即待估计的量 $\\mu$，是：\n$\\mu = \\mathbb{E}[X] = \\mathbb{E}[(Y^2 - 1) + \\varepsilon] = \\mathbb{E}[Y^2 - 1] + \\mathbb{E}[\\varepsilon] = \\mathbb{E}[Z] + 0 = 0$。\n\n**任务1：验证 $\\operatorname{Cov}(X,Y) = 0$**\n\n协方差定义为 $\\operatorname{Cov}(X,Y) = \\mathbb{E}[XY] - \\mathbb{E}[X]\\mathbb{E}[Y]$。\n我们有 $\\mathbb{E}[Y] = 0$，所以第二项消失：\n$\\operatorname{Cov}(X,Y) = \\mathbb{E}[XY] - \\mathbb{E}[X] \\cdot 0 = \\mathbb{E}[XY]$。\n我们通过代入 $X$ 的定义来计算 $\\mathbb{E}[XY]$：\n$\\mathbb{E}[XY] = \\mathbb{E}[((Y^2 - 1) + \\varepsilon)Y] = \\mathbb{E}[Y^3 - Y + \\varepsilon Y]$。\n根据期望的线性性质：\n$\\mathbb{E}[XY] = \\mathbb{E}[Y^3] - \\mathbb{E}[Y] + \\mathbb{E}[\\varepsilon Y]$。\n如前所述，$\\mathbb{E}[Y^3] = 0$ 且 $\\mathbb{E}[Y] = 0$。对于最后一项，由于 $\\varepsilon$ 和 $Y$ 是独立的，所以 $\\mathbb{E}[\\varepsilon Y] = \\mathbb{E}[\\varepsilon]\\mathbb{E}[Y]$。我们知道 $\\mathbb{E}[\\varepsilon]=0$ 且 $\\mathbb{E}[Y]=0$，所以 $\\mathbb{E}[\\varepsilon Y] = 0 \\cdot 0 = 0$。\n因此，$\\mathbb{E}[XY] = 0 - 0 + 0 = 0$。\n这证实了 $\\operatorname{Cov}(X,Y) = 0$。\n皮尔逊相关系数为 $\\rho(X,Y) = \\frac{\\operatorname{Cov}(X,Y)}{\\sqrt{\\operatorname{Var}(X)\\operatorname{Var}(Y)}}$。由于分子为 $0$ 且方差非零（稍后将证明），所以 $\\rho(X,Y) = 0$。\n\n**任务2：推导最优系数 $\\beta^{\\star}$**\n\n我们的任务是找到 $\\beta$ 的值，记为 $\\beta^{\\star}$，该值使得调整后估计量 $X_{\\beta} = X - \\beta(Z - \\mathbb{E}[Z])$ 的方差最小。令 $V(\\beta) = \\operatorname{Var}(X_{\\beta})$。\n由于 $\\mathbb{E}[Z] = 0$，该估计量为 $X_{\\beta} = X - \\beta Z$。\n需要最小化的方差是：\n$V(\\beta) = \\operatorname{Var}(X - \\beta Z)$。\n利用方差的性质，其中 $\\beta$ 是一个常数：\n$V(\\beta) = \\operatorname{Var}(X) + \\operatorname{Var}(-\\beta Z) + 2\\operatorname{Cov}(X, -\\beta Z)$。\n$V(\\beta) = \\operatorname{Var}(X) + \\beta^{2}\\operatorname{Var}(Z) - 2\\beta\\operatorname{Cov}(X, Z)$。\n这是一个关于 $\\beta$ 的二次函数。为求最小值，我们计算其关于 $\\beta$ 的导数并令其为零：\n$\\frac{d V}{d\\beta} = \\frac{d}{d\\beta} \\left( \\operatorname{Var}(X) + \\beta^{2}\\operatorname{Var}(Z) - 2\\beta\\operatorname{Cov}(X, Z) \\right) = 2\\beta\\operatorname{Var}(Z) - 2\\operatorname{Cov}(X, Z)$。\n令导数为零：\n$2\\beta\\operatorname{Var}(Z) - 2\\operatorname{Cov}(X, Z) = 0$。\n解出最优系数 $\\beta^{\\star}$：\n$\\beta^{\\star} = \\frac{\\operatorname{Cov}(X, Z)}{\\operatorname{Var}(Z)}$。\n二阶导数 $\\frac{d^2 V}{d\\beta^2} = 2\\operatorname{Var}(Z)$ 为正，因为方差是非负的（在本例中非零），这证实了我们求得的是最小值。\n\n**任务3：计算 $\\beta^{\\star}$ 和方差缩减因子 $R$**\n\n为了计算 $\\beta^{\\star}$，我们需要计算 $\\operatorname{Var}(Z)$ 和 $\\operatorname{Cov}(X,Z)$。\n首先，我们求控制变量 $Z$ 的方差：\n$\\operatorname{Var}(Z) = \\mathbb{E}[Z^2] - (\\mathbb{E}[Z])^2$。由于 $\\mathbb{E}[Z]=0$，$\\operatorname{Var}(Z) = \\mathbb{E}[Z^2]$。\n$\\operatorname{Var}(Z) = \\mathbb{E}[(Y^2-1)^2] = \\mathbb{E}[Y^4 - 2Y^2 + 1]$。\n利用期望的线性性质和 $Y$ 的矩：\n$\\operatorname{Var(Z)} = \\mathbb{E}[Y^4] - 2\\mathbb{E}[Y^2] + 1 = 3 - 2(1) + 1 = 2$。\n\n接下来，我们计算 $X$ 和 $Z$ 之间的协方差：\n$\\operatorname{Cov}(X,Z) = \\mathbb{E}[XZ] - \\mathbb{E}[X]\\mathbb{E}[Z]$。\n由于 $\\mathbb{E}[X] = 0$ 且 $\\mathbb{E}[Z] = 0$，我们有 $\\operatorname{Cov}(X,Z) = \\mathbb{E}[XZ]$。\n代入 $X = Z + \\varepsilon$：\n$\\operatorname{Cov}(X,Z) = \\mathbb{E}[(Z+\\varepsilon)Z] = \\mathbb{E}[Z^2 + \\varepsilon Z] = \\mathbb{E}[Z^2] + \\mathbb{E}[\\varepsilon Z]$。\n我们知道 $\\mathbb{E}[Z^2] = \\operatorname{Var}(Z) = 2$。\n对于 $\\mathbb{E}[\\varepsilon Z]$ 项，我们利用 $\\varepsilon$ 和 $Y$ 的独立性。由于 $Z = Y^2-1$ 是 $Y$ 的函数，因此 $Z$ 和 $\\varepsilon$ 也是独立的。\n因此，$\\mathbb{E}[\\varepsilon Z] = \\mathbb{E}[\\varepsilon]\\mathbb{E}[Z] = 0 \\cdot 0 = 0$。\n所以，$\\operatorname{Cov}(X,Z) = 2 + 0 = 2$。\n\n现在我们可以计算最优系数 $\\beta^{\\star}$：\n$\\beta^{\\star} = \\frac{\\operatorname{Cov}(X, Z)}{\\operatorname{Var}(Z)} = \\frac{2}{2} = 1$。\n\n最后，我们计算方差缩减因子 $R = \\frac{\\operatorname{Var}(X)}{\\operatorname{Var}\\big(X - \\beta^{\\star}(Z - \\mathbb{E}[Z])\\big)}$。\n首先，是分子 $\\operatorname{Var}(X)$：\n$X = Z + \\varepsilon$。由于 $Z$ 和 $\\varepsilon$ 是独立的，它们的和的方差等于它们方差的和：\n$\\operatorname{Var}(X) = \\operatorname{Var}(Z) + \\operatorname{Var}(\\varepsilon) = 2 + \\sigma^2 = 2 + 0.01 = 2.01$。\n\n接下来，是分母，即在最优 $\\beta^{\\star}=1$ 和 $\\mathbb{E}[Z]=0$ 下调整后估计量的方差：\n$\\operatorname{Var}\\big(X - \\beta^{\\star}(Z - \\mathbb{E}[Z])\\big) = \\operatorname{Var}(X - 1 \\cdot (Z - 0)) = \\operatorname{Var}(X-Z)$。\n代入 $X = Z + \\varepsilon$：\n$\\operatorname{Var}(X-Z) = \\operatorname{Var}((Z+\\varepsilon) - Z) = \\operatorname{Var}(\\varepsilon)$。\n我们已知 $\\operatorname{Var}(\\varepsilon) = \\sigma^2 = 0.01$。\n\n现在，我们可以计算方差缩减因子 $R$：\n$R = \\frac{\\operatorname{Var}(X)}{\\operatorname{Var}(X-Z)} = \\frac{2.01}{0.01} = 201$。\n\n最终答案是有序对 $(\\beta^{\\star}, R)$。\n$\\beta^{\\star} = 1$\n$R = 201$\n最终的有序对是 $(1, 201)$。", "answer": "$$\n\\boxed{\\begin{pmatrix} 1 & 201 \\end{pmatrix}}\n$$", "id": "3218904"}, {"introduction": "最后的练习旨在连接理论与实际应用。你将编写代码，使用控制变量法来估算一个定积分，其中最优系数 $\\beta$ 不再是解析推导得出，而是直接从数据中凭经验估计。这个编程实践不仅会带你走过一个完整的计算流程，还将通过比较不同方法的误差，让你直观地感受到控制变量法在实际问题中的强大威力 [@problem_id:3218755]。", "problem": "您需要实现并分析带有控制变量的蒙特卡洛（MC）积分方法，用于计算积分 $$I=\\int_{0}^{1} e^{-x^2}\\,dx,$$。您将使用两种不同的控制变量，并与应用于残差的高斯-勒让德（GL）正交积分进行混合比较。目标是通过从采样数据中凭经验选择控制变量系数，来展示有原则的方差缩减，并将结果与应用于残差函数的确定性正交积分进行比较。\n\n使用以下基本原理：\n- 一个函数在某个概率密度下的期望，等于该函数在其定义域上的积分：如果 $X \\sim \\mathrm{Uniform}(0,1)$，那么 $$\\mathbb{E}[f(X)] = \\int_{0}^{1} f(x)\\,dx.$$\n- 对于任何已知均值 $\\mu_g = \\int_{0}^{1} g(x)\\,dx$ 的可积函数 $g$，经控制变量调整后的随机变量 $$Z_\\beta = f(X) - \\beta \\big(g(X) - \\mu_g\\big)$$ 满足 $$\\mathbb{E}[Z_\\beta] = \\mathbb{E}[f(X)],$$，适用于任何标量 $\\beta \\in \\mathbb{R}$，因为 $\\mathbb{E}[g(X)] = \\mu_g$。\n- 通过以数据驱动的方式最小化 $Z_\\beta$ 的方差来获得合适的 $\\beta$ 选择，从而使方差最小化。\n\n您的任务是：\n1. 使用独立同分布的 $X_i \\sim \\mathrm{Uniform}(0,1)$（其中 $i=1,\\dots,N$，$N$ 为样本数量），为积分 $I$ 实现 MC 估计器。函数为 $f(x)=e^{-x^2}$。\n2. 使用控制变量 $g_1(x)=1-x$ 和 $g_2(x)=x(1-x)$，它们在 $[0,1]$ 上的精确均值分别为 $\\mu_{g_1}=\\int_{0}^{1} (1-x)\\,dx = \\tfrac{1}{2}$ 和 $\\mu_{g_2}=\\int_{0}^{1} x(1-x)\\,dx = \\tfrac{1}{6}$。\n3. 通过在 $\\beta \\in \\mathbb{R}$ 上最小化调整后数值 $Z_\\beta$ 的样本方差，从随机样本中经验性地选择 $\\beta$。您必须严格地从采样数据中执行此操作，而不能使用问题陈述中提供的任何封闭形式公式。如果控制变量的经验方差在数值上为零，则将 $\\beta$ 设置为 0 以避免不稳定性。\n4. 构建以下三个估计器，并计算它们相对于 $I$ 的精确值的绝对误差：\n   - 普通 MC：$$\\hat{I}_{\\mathrm{MC}} = \\frac{1}{N} \\sum_{i=1}^{N} f(X_i).$$\n   - 带控制变量的 MC（经验性 $\\beta$）：$$\\hat{I}_{\\mathrm{CV}} = \\frac{1}{N} \\sum_{i=1}^{N} \\left[f(X_i) - \\beta \\big(g(X_i) - \\mu_g\\big)\\right].$$\n   - 对残差使用 GL 正交积分（使用经验选择的 $\\beta$）：定义残差函数 $$r_\\beta(x) = f(x) - \\beta \\big(g(x) - \\mu_g\\big),$$ 然后使用在 $[0,1]$ 上的 $m$ 点高斯-勒让德正交积分来近似 $$\\hat{I}_{\\mathrm{GL,res}} \\approx \\int_{0}^{1} r_\\beta(x)\\,dx$$。使用标准 $m$ 点高斯-勒让德正交积分在 $[-1,1]$ 上的节点和权重，通过 $t=\\tfrac{1}{2}(u+1)$ 映射到 $[0,1]$，权重按 $\\tfrac{1}{2}$ 缩放。请注意，如果 $g$ 是一个低阶多项式，且 $m$ 足够大，GL 正交积分可以精确地对 $g$ 进行积分，因此将 GL 应用于 $r_\\beta$ 可能与直接将 GL 应用于 $f$ 的结果相同；尽管如此，仍请按规定执行残差正交积分。\n\n$I$ 的精确值可以用高斯误差函数表示为 $$I = \\frac{\\sqrt{\\pi}}{2}\\,\\mathrm{erf}(1).$$ 使用此值计算绝对误差。\n\n测试套件：\n提供一个程序，为以下每一组参数评估三个绝对误差：\n- 情况 1（理想情况，二次控制变量）：$N=100000$，种子 $=2024$，控制变量 $g_2$，$m=16$。\n- 情况 2（理想情况，线性控制变量）：$N=100000$，种子 $=99$，控制变量 $g_1$，$m=16$。\n- 情况 3（小 $N$ 边界，粗略正交对二次函数不精确）：$N=10$，种子 $=7$，控制变量 $g_2$，$m=1$。\n- 情况 4（极端 $N=1$ 边缘情况，线性控制变量，粗略正交）：$N=1$，种子 $=42$，控制变量 $g_1$，$m=1$。\n\n最终输出格式：\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表。该列表必须按情况 1 到 4 的顺序包含绝对误差，并且在每种情况下，按普通 MC、带控制变量的 MC、GL 残差的顺序排列。即，输出格式为 $$[\\mathrm{err}_{1,\\mathrm{MC}},\\mathrm{err}_{1,\\mathrm{CV}},\\mathrm{err}_{1,\\mathrm{GLres}},\\mathrm{err}_{2,\\mathrm{MC}},\\mathrm{err}_{2,\\mathrm{CV}},\\mathrm{err}_{2,\\mathrm{GLres}},\\mathrm{err}_{3,\\mathrm{MC}},\\mathrm{err}_{3,\\mathrm{CV}},\\mathrm{err}_{3,\\mathrm{GLres}},\\mathrm{err}_{4,\\mathrm{MC}},\\mathrm{err}_{4,\\mathrm{CV}},\\mathrm{err}_{4,\\mathrm{GLres}}].$$", "solution": "该问题要求实现并分析三种数值方法，用于近似定积分 $I=\\int_{0}^{1} e^{-x^2}\\,dx$。这些方法是：标准蒙特卡洛（MC）积分、带控制变量（CV）的 MC 积分，以及应用于从控制变量法派生的残差函数的高斯-勒让德（GL）正交积分。该分析将针对一组指定的测试用例，比较这三个估计器的绝对误差与积分的已知精确值。\n\n要积分的函数是在定义域 $[0, 1]$ 上的 $f(x) = e^{-x^2}$。随机方法的基础是生成 $N$ 个独立同分布的随机样本 $X_i \\sim \\mathrm{Uniform}(0,1)$。积分 $I$ 随后等价于期望 $\\mathbb{E}[f(X)]$。\n\n计算误差需要积分的精确值，由 $I = \\frac{\\sqrt{\\pi}}{2}\\,\\mathrm{erf}(1)$ 给出，其中 $\\mathrm{erf}$ 是高斯误差函数。\n\n问题指定了两种可能的控制变量：\n1.  线性函数 $g_1(x) = 1-x$，其在 $[0,1]$ 上的已知均值为：\n    $$\\mu_{g_1} = \\int_{0}^{1} (1-x)\\,dx = \\left[x - \\frac{x^2}{2}\\right]_{0}^{1} = 1 - \\frac{1}{2} = \\frac{1}{2}.$$\n2.  二次函数 $g_2(x) = x(1-x)$，其在 $[0,1]$ 上的已知均值为：\n    $$\\mu_{g_2} = \\int_{0}^{1} (x-x^2)\\,dx = \\left[\\frac{x^2}{2} - \\frac{x^3}{3}\\right]_{0}^{1} = \\frac{1}{2} - \\frac{1}{3} = \\frac{1}{6}.$$\n\n确定这三个估计器中每一个的过程如下：\n\n1.  **普通蒙特卡洛（MC）估计器**\n    标准 MC 估计器 $\\hat{I}_{\\mathrm{MC}}$ 是函数 $f(x)$ 在随机点 $X_i$ 处求值的样本均值：\n    $$\\hat{I}_{\\mathrm{MC}} = \\frac{1}{N} \\sum_{i=1}^{N} f(X_i).$$\n    根据大数定律，当 $N \\to \\infty$ 时，$\\hat{I}_{\\mathrm{MC}} \\to I$。\n\n2.  **控制变量（CV）估计器**\n    此方法旨在减少 MC 估计器的方差。它引入一个调整后的随机变量 $Z_\\beta = f(X) - \\beta(g(X) - \\mu_g)$，其中 $g(x)$ 是一个已知均值为 $\\mu_g$ 的控制变量。$Z_\\beta$ 的期望是 $\\mathbb{E}[f(X)] = I$，适用于任何标量系数 $\\beta \\in \\mathbb{R}$。该估计器是这个新变量样本的均值：\n    $$\\hat{I}_{\\mathrm{CV}} = \\frac{1}{N} \\sum_{i=1}^{N} \\left[f(X_i) - \\beta \\big(g(X_i) - \\mu_g\\big)\\right].$$\n    最优系数 $\\beta^*$ 使方差 $\\mathrm{Var}(Z_\\beta)$ 最小化。该方差是 $\\beta$ 的二次函数：\n    $$\\mathrm{Var}(Z_\\beta) = \\mathrm{Var}(f(X) - \\beta g(X)) = \\mathrm{Var}(f(X)) - 2\\beta \\mathrm{Cov}(f(X), g(X)) + \\beta^2 \\mathrm{Var}(g(X)).$$\n    对 $\\beta$ 进行最小化可得出最优值：\n    $$\\beta^* = \\frac{\\mathrm{Cov}(f(X), g(X))}{\\mathrm{Var}(g(X))}.$$\n    问题要求从生成的样本中经验性地估计 $\\beta$。设 $f_i = f(X_i)$ 和 $g_i = g(X_i)$。基于样本的估计 $\\hat{\\beta}$ 计算如下：\n    $$\\hat{\\beta} = \\frac{\\widehat{\\mathrm{Cov}}(f, g)}{\\widehat{\\mathrm{Var}}(g)} = \\frac{\\sum_{i=1}^{N} (f_i - \\bar{f})(g_i - \\bar{g})}{\\sum_{i=1}^{N} (g_i - \\bar{g})^2},$$\n    其中 $\\bar{f}$ 和 $\\bar{g}$ 分别是样本均值。如果分母（与 $g$ 的样本方差成正比）在数值上为零（如果 $N \\le 1$ 或所有 $g_i$ 恰好相等，则会发生这种情况），则根据问题说明将 $\\hat{\\beta}$ 设置为 0，以防止数值不稳定性。然后可以高效地计算 CV 估计器：\n    $$\\hat{I}_{\\mathrm{CV}} = \\bar{f} - \\hat{\\beta}(\\bar{g} - \\mu_g).$$\n\n3.  **对残差的高斯-勒让德正交积分（GL,res）估计器**\n    该方法将控制变量概念与确定性正交积分相结合。首先，如上所述，从 $N$ 个 MC 样本中确定经验系数 $\\hat{\\beta}$。然后，定义一个残差函数 $r_{\\hat{\\beta}}(x)$：\n    $$r_{\\hat{\\beta}}(x) = f(x) - \\hat{\\beta}(g(x) - \\mu_g).$$\n    接着使用 $m$ 点高斯-勒让德正交积分来近似该残差函数的积分。标准的 GL 正交积分规则定义在区间 $[-1, 1]$ 上：\n    $$\\int_{-1}^{1} h(u)\\,du \\approx \\sum_{j=1}^{m} w_j h(u_j),$$\n    其中 $u_j$ 是节点（$m$ 阶勒让德多项式的根），$w_j$ 是相应的权重。为了将其应用于区间 $[0, 1]$，使用线性变量变换 $x = \\frac{1}{2}(u+1)$，其中 $dx = \\frac{1}{2}du$。正交积分规则变为：\n    $$\\int_{0}^{1} r_{\\hat{\\beta}}(x)\\,dx \\approx \\sum_{j=1}^{m} \\frac{w_j}{2} r_{\\hat{\\beta}}\\left(\\frac{u_j+1}{2}\\right).$$\n    这个和提供了估计器 $\\hat{I}_{\\mathrm{GL,res}}$。此处的原理是，如果 $g(x)$ 是 $f(x)$ 的一个良好近似，残差 $r_{\\hat{\\beta}}(x)$ 将比 $f(x)$ 本身更“平滑”或幅值更小，这可能使得固定阶数的正交积分规则产生更精确的结果。\n\n对于四个测试用例中的每一个，都计算这三个估计器。最后，计算它们相对于精确值 $I = \\frac{\\sqrt{\\pi}}{2}\\,\\mathrm{erf}(1)$ 的绝对误差。", "answer": "```python\nimport numpy as np\nimport math\n\ndef solve():\n    \"\"\"\n    Implements and analyzes Monte Carlo integration with control variates\n    and a hybrid Gauss-Legendre quadrature method for the integral of exp(-x^2) from 0 to 1.\n    \"\"\"\n\n    # Define the primary function and the control variates\n    def f(x):\n        return np.exp(-x**2)\n\n    def g1(x):\n        return 1.0 - x\n\n    def g2(x):\n        return x * (1.0 - x)\n\n    # Dictionary to map control variate names to functions and their exact means\n    g_map = {\n        'g1': {'func': g1, 'mean': 0.5},\n        'g2': {'func': g2, 'mean': 1.0 / 6.0}\n    }\n\n    # Test cases as specified in the problem statement\n    test_cases = [\n        {'N': 100000, 'seed': 2024, 'g_name': 'g2', 'm': 16},\n        {'N': 100000, 'seed': 99, 'g_name': 'g1', 'm': 16},\n        {'N': 10, 'seed': 7, 'g_name': 'g2', 'm': 1},\n        {'N': 1, 'seed': 42, 'g_name': 'g1', 'm': 1}\n    ]\n\n    # Exact value of the integral for error calculation\n    i_exact = (math.sqrt(math.pi) / 2.0) * math.erf(1.0)\n\n    results = []\n\n    for case in test_cases:\n        N = case['N']\n        seed = case['seed']\n        g_name = case['g_name']\n        m = case['m']\n\n        g_func = g_map[g_name]['func']\n        mu_g = g_map[g_name]['mean']\n\n        # --- Monte Carlo Sampling ---\n        rng = np.random.default_rng(seed)\n        x_samples = rng.uniform(0.0, 1.0, N)\n\n        f_samples = f(x_samples)\n        g_samples = g_func(x_samples)\n\n        # --- Estimator 1: Plain Monte Carlo ---\n        i_mc = np.mean(f_samples)\n\n        # --- Estimator 2: Monte Carlo with Control Variates ---\n        # Empirically determine beta\n        if N == 1:\n            beta = 0.0\n        else:\n            # Using the direct formula for beta to avoid numerical issues with np.var at N=1\n            # The scaling factors (like 1/(N-1)) cancel in the ratio.\n            g_samples_mean = np.mean(g_samples)\n            # Sum of squared deviations for variance term\n            var_g_numerator = np.sum((g_samples - g_samples_mean)**2)\n\n            if var_g_numerator  1e-15:  # Check for numerically zero variance\n                beta = 0.0\n            else:\n                f_samples_mean = np.mean(f_samples)\n                # Sum of products of deviations for covariance term\n                cov_fg_numerator = np.sum((f_samples - f_samples_mean) * (g_samples - g_samples_mean))\n                beta = cov_fg_numerator / var_g_numerator\n\n        f_mean = np.mean(f_samples)\n        g_mean = np.mean(g_samples)\n        i_cv = f_mean - beta * (g_mean - mu_g)\n\n        # --- Estimator 3: Gauss-Legendre Quadrature on the Residual ---\n        # Get standard GL nodes and weights for [-1, 1]\n        gl_nodes_std, gl_weights_std = np.polynomial.legendre.leggauss(m)\n\n        # Transform nodes and weights for [0, 1]\n        gl_nodes = 0.5 * (gl_nodes_std + 1.0)\n        gl_weights = 0.5 * gl_weights_std\n        \n        # Define the residual function using the empirically found beta\n        def r_beta(x, beta_val, g_function, g_mean_val):\n            return f(x) - beta_val * (g_function(x) - g_mean_val)\n\n        # Evaluate residual at the transformed GL nodes\n        r_vals_at_nodes = r_beta(gl_nodes, beta, g_func, mu_g)\n        \n        # Compute the integral estimator\n        i_gl_res = np.sum(gl_weights * r_vals_at_nodes)\n\n        # --- Calculate Absolute Errors ---\n        err_mc = abs(i_mc - i_exact)\n        err_cv = abs(i_cv - i_exact)\n        err_gl_res = abs(i_gl_res - i_exact)\n        \n        results.extend([err_mc, err_cv, err_gl_res])\n\n    # Print the final result in the specified format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3218755"}]}