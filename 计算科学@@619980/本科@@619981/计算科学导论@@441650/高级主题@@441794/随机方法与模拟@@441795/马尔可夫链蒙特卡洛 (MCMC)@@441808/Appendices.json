{"hands_on_practices": [{"introduction": "Metropolis-Hastings 算法的核心在于其接受概率，它决定了马尔可夫链是否移动到一个新的状态。这个简单的计算练习旨在巩固你对这一核心机制的理解，通过一个具体的例子，让你亲手计算从当前状态移动到候选状态的接受概率，这是构建任何MCMC模拟的第一步。[@problem_id:1371728]", "problem": "一位数据科学家正在实现一个马尔可夫链蒙特卡洛 (MCMC) 模拟，以从参数 $x$ 的后验概率分布中抽取样本。目标分布 $\\pi(x)$ 与参数负绝对值的指数成正比，即 $\\pi(x) \\propto \\exp(-|x|)$。\n\n该科学家使用 Metropolis 算法和一个对称提议分布 $q(x'|x)$，其中在给定当前状态 $x$ 的情况下提议一个新状态 $x'$ 的概率等于在给定 $x'$ 的情况下提议 $x$ 的概率（即 $q(x'|x) = q(x|x')$）。\n\n假设在模拟的某一步，链的当前状态是 $x = 1.5$。然后算法提议移动到一个新的候选状态 $x' = 2.0$。\n\n计算这次特定移动的接受概率。你的答案应该是一个无量纲的实数。将你的最终答案四舍五入到四位有效数字。", "solution": "对于一个从 $x$ 到 $x'$ 的移动，在对称提议分布 $q(x'|x)=q(x|x')$ 下的 Metropolis 接受概率是\n$$\n\\alpha(x \\to x')=\\min\\left(1,\\frac{\\pi(x')q(x|x')}{\\pi(x)q(x'|x)}\\right)=\\min\\left(1,\\frac{\\pi(x')}{\\pi(x)}\\right).\n$$\n给定目标分布 $\\pi(x)\\propto \\exp(-|x|)$，该比率简化为\n$$\n\\frac{\\pi(x')}{\\pi(x)}=\\frac{\\exp(-|x'|)}{\\exp(-|x|)}=\\exp\\!\\left(-\\left(|x'|-|x|\\right)\\right).\n$$\n当 $x=1.5$ 且 $x'=2.0$ 时，我们有 $|x|=1.5$ 和 $|x'|=2.0$，所以\n$$\n\\frac{\\pi(x')}{\\pi(x)}=\\exp\\!\\left(-\\left(2.0-1.5\\right)\\right)=\\exp(-0.5).\n$$\n因此，\n$$\n\\alpha(x \\to x')=\\min\\left(1,\\exp(-0.5)\\right)=\\exp(-0.5).\n$$\n数值上，$\\exp(-0.5)$ 四舍五入到四位有效数字约为 $0.6065$。", "answer": "$$\\boxed{0.6065}$$", "id": "1371728"}, {"introduction": "理论知识需要通过实践来巩固，这个练习将指导你将MCMC方法应用于一个经典的组合优化问题——最大割问题。你将学习如何为离散状态空间设计一个MCMC采样器，并通过实现一个完整的算法来体会MCMC在解决复杂计算问题中的威力与灵活性。[@problem_id:3250464]", "problem": "要求您为一个离散状态空间实现 Metropolis–Hastings 算法，以近似求解小型无向图上的最大割问题。状态空间是一组代表二分划分的二元赋值。从一个基本概念出发：马尔可夫链是一个在可数状态空间上具有转移核的随机过程，通过强制满足细致平衡条件可以达到期望的平稳分布。Metropolis–Hastings 构建方法通过一个接受-拒绝步骤来确保相对于目标分布的细致平衡。\n\n考虑一个无向简单图，其顶点集为 $V=\\{0,1,\\dots,n-1\\}$，边集为 $E\\subseteq V\\times V$，图中没有自环或重复边。一个割由一个二元赋值 $s\\in\\{-1,+1\\}^n$ 导出，其中 $s_i=+1$ 表示顶点 $i$ 在划分的一侧，$s_i=-1$ 表示它在另一侧。割的大小 $C(s)$ 是跨越割的边的数量，由下式给出：\n$$\nC(s)=\\sum_{(i,j)\\in E}\\frac{1-s_i s_j}{2}.\n$$\n定义一个在状态 $s$ 上的目标分布，该分布与割大小的指数成正比，\n$$\n\\pi(s)\\propto \\exp\\left(\\lambda\\, C(s)\\right),\n$$\n其中 $\\lambda0$ 是一个逆温度参数，它将概率质量集中在较大的割上。\n\n在每一步中，通过均匀随机地选择一个顶点 $i$ 并翻转其赋值（即 $s'_i=-s_i$ 且对所有 $k\\neq i$ 有 $s'_k=s_k$）来从状态 $s$ 提议一个新状态 $s'$。令 $q(s\\to s')$ 表示此提议概率。因为均匀选择单个顶点并翻转它是对称的，所以我们有 $q(s\\to s')=q(s'\\to s)$。使用 Metropolis–Hastings 接受概率：\n$$\n\\alpha(s\\to s')=\\min\\left(1,\\frac{\\pi(s')\\,q(s'\\to s)}{\\pi(s)\\,q(s\\to s')}\\right),\n$$\n在这种对称情况下，该概率简化为：\n$$\n\\alpha(s\\to s')=\\min\\left(1,\\exp\\left(\\lambda\\,[C(s')-C(s)]\\right)\\right).\n$$\n请注意，单顶点翻转下割大小的变化量可以进行局部计算。如果 $N(i)$ 表示顶点 $i$ 的邻居集合，那么变化量为：\n$$\n\\Delta C=C(s')-C(s)=\\sum_{j\\in N(i)} s_i s_j.\n$$\n基于此，实现一个马尔可夫链蒙特卡洛 (MCMC) 方法，该方法从全为 $+1$ 的状态开始，运行指定的步数，并追踪遇到的最佳割大小（即轨迹上 $C(s)$ 的最大值）。必须为每个测试用例设置随机数生成器的种子以确保可复现性。使用以下测试套件，并保持所有索引都是从 0 开始的：\n\n- 测试用例 1 (三角形图):\n  - $n=3$\n  - $E=\\{(0,1),(1,2),(0,2)\\}$\n  - 步数 $T=20000$\n  - 逆温度 $\\lambda=2.0$\n  - 种子 $s=7$\n- 测试用例 2 (长度为 4 的环图):\n  - $n=4$\n  - $E=\\{(0,1),(1,2),(2,3),(3,0)\\}$\n  - 步数 $T=30000$\n  - 逆温度 $\\lambda=2.0$\n  - 种子 $s=11$\n- 测试用例 3 (空图):\n  - $n=4$\n  - $E=\\emptyset$\n  - 步数 $T=1000$\n  - 逆温度 $\\lambda=2.0$\n  - 种子 $s=5$\n- 测试用例 4 (两条不相交的边):\n  - $n=4$\n  - $E=\\{(0,1),(2,3)\\}$\n  - 步数 $T=20000$\n  - 逆温度 $\\lambda=2.0$\n  - 种子 $s=13$\n\n您的程序必须在此离散状态空间上实现所述的 Metropolis–Hastings 算法，为每个测试用例计算马尔可夫链过程中遇到的最佳割大小，并以整数形式返回最佳割大小的列表。最终输出格式必须是单行，包含一个用方括号括起来的逗号分隔列表，例如 $[x_1,x_2,x_3,x_4]$，其中每个 $x_k$ 是为测试用例 $k$ 找到的最佳割大小。", "solution": "问题陈述已经过验证，并被认为是合理的。它描述了 Metropolis-Hastings 算法（一种标准的马尔可夫链蒙特卡洛 (MCMC) 方法）在寻找图中最大割这一组合优化问题上的一个良构应用。所有组成部分，包括状态空间、目标分布、提议机制和接受概率，都以科学和数学上的正确性进行了定义。\n\n目标是实现这个 MCMC 模拟，为几个给定的图找到近似的最大割。模拟从一个确定性状态开始，并根据旨在偏好对应于较大割的状态的随机规则进行演化。\n\n实现的核心是一个函数，它对给定的图和一组参数执行 Metropolis-Hastings 算法。该过程可以分解为以下步骤：\n\n1.  **初始化**：\n    -   对于一个有 `n` 个顶点的图，系统的状态由一个向量 $s \\in \\{-1, +1\\}^n$ 表示。\n    -   模拟从初始状态 $s = (+1, +1, \\dots, +1)$ 开始，其中所有顶点都在同一个划分中。\n    -   计算初始割的大小 $C(s) = \\sum_{(i,j)\\in E}\\frac{1-s_i s_j}{2}$。对于全为 $+1$ 的状态，对所有 $i, j$ 都有 $s_i s_j = 1$，因此初始割的大小为 $C(s) = 0$。\n    -   一个变量 `max_cut_size` 被初始化为这个起始值（`0`），用于追踪模拟过程中遇到的最大割的大小。\n    -   图结构存储在邻接表中，以便高效地检索任何顶点 `i` 的邻居，记为 $N(i)$。\n    -   为每个测试用例设置伪随机数生成器的种子，以确保可复现性。\n\n2.  **MCMC 模拟循环**：模拟进行指定的步数 `T`。在每一步中，都会提议一个新的候选状态，然后接受或拒绝它。\n    -   **提议**：从当前状态 `s` 提议一个新状态 `s'`。从 $V = \\{0, 1, \\dots, n-1\\}$ 中均匀随机地选择一个顶点 `i`。通过翻转这个顶点的赋值生成新状态 `s'`：$s'_i = -s_i$ 且对所有 $k \\neq i$ 有 $s'_k = s_k$。这种提议机制是对称的，意味着从 `s` 提议 `s'` 的概率等于从 `s'` 提议 `s` 的概率：$q(s \\to s') = q(s' \\to s) = 1/n$。\n\n    -   **评估**：我们不重新计算整个割的大小 $C(s')$（这样做计算成本很高），而是计算割大小的变化量 $\\Delta C = C(s') - C(s)$。正如问题陈述中推导的那样，这个变化量可以局部且高效地计算：\n        $$\n        \\Delta C = \\sum_{j \\in N(i)} s_i s_j\n        $$\n        这个和是关于被翻转顶点 `i` 的邻居 `j` 的。如果 `i` 是一个孤立顶点，则 $N(i)$ 为空，且 $\\Delta C = 0$。\n\n    -   **接受**：提议的状态 `s'` 以 Metropolis-Hastings 概率 $\\alpha(s \\to s')$ 被接受。鉴于提议的对称性，这可以简化为：\n        $$\n        \\alpha(s \\to s') = \\min\\left(1, \\frac{\\pi(s')}{\\pi(s)}\\right) = \\min\\left(1, \\frac{\\exp(\\lambda C(s'))}{\\exp(\\lambda C(s))}\\right) = \\min\\left(1, \\exp(\\lambda [C(s') - C(s)])\\right) = \\min(1, \\exp(\\lambda \\Delta C))\n        $$\n        为了实现这一点，从 $[0, 1)$ 上的均匀分布中抽取一个随机数 `u`。如果 `u`  $\\alpha(s \\to s')$，则接受该移动。\n        -   如果 $\\Delta C > 0$，割的大小增加。那么 $\\exp(\\lambda \\Delta C) > 1$，所以 $\\alpha = 1$，该移动总是被接受。\n        -   如果 $\\Delta C \\le 0$，割的大小不增加。那么 $\\exp(\\lambda \\Delta C) \\le 1$，所以 $\\alpha = \\exp(\\lambda \\Delta C)$，并且该移动以这个概率被接受。这使得模拟能够逃离局部最优解。\n\n    -   **状态更新**：\n        -   如果移动被接受，状态更新为 $s \\leftarrow s'$（通过翻转 $s_i$），当前割的大小更新为 `current_cut_size` $\\leftarrow$ `current_cut_size` + $\\Delta C$。\n        -   如果移动被拒绝，状态 `s` 和 `current_cut_size` 在下一次迭代中保持不变。\n\n    -   **追踪最大值**：在每一步之后，将 `current_cut_size` 与 `max_cut_size` 进行比较，如果找到了一个更大的割，则更新 `max_cut_size`：`max_cut_size` $\\leftarrow$ `max(max_cut_size, current_cut_size)`。\n\n3.  **结果**：在 `T` 步之后，`max_cut_size` 的最终值作为该测试用例的结果返回。对问题中定义的每个测试用例，都使用其特定的图、参数和随机种子重复整个过程。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef run_mcmc_max_cut(n, edges, T, lambda_val, seed):\n    \"\"\"\n    Performs Metropolis-Hastings MCMC to find the approximate maximum cut of a graph.\n\n    Args:\n        n (int): The number of vertices in the graph.\n        edges (list of tuples): The edge set of the graph.\n        T (int): The number of MCMC steps to perform.\n        lambda_val (float): The inverse-temperature parameter.\n        seed (int): The seed for the random number generator.\n\n    Returns:\n        int: The maximum cut size found during the simulation.\n    \"\"\"\n    # 1. Setup the random number generator with the specified seed for reproducibility.\n    rng = np.random.default_rng(seed)\n\n    # 2. Build an adjacency list for efficient neighbor lookup.\n    adj = [[] for _ in range(n)]\n    for u, v in edges:\n        adj[u].append(v)\n        adj[v].append(u)\n\n    # 3. Initialize the state vector s to all +1.\n    s = np.ones(n, dtype=np.int8)\n\n    # The initial cut size for an all-+1 state is always 0.\n    # C(s) = sum((1 - s_i*s_j)/2) = sum((1 - 1*1)/2) = 0.\n    current_cut_size = 0\n    max_cut_size = 0\n\n    # 4. Run the MCMC simulation for T steps.\n    for _ in range(T):\n        # a. Propose a new state by flipping a single vertex's assignment.\n        # Select a vertex 'i' uniformly at random.\n        i = rng.integers(n)\n\n        # b. Calculate the change in cut size (Delta C).\n        # This is a local computation: Delta_C = s_i * sum_{j in N(i)} s_j\n        # Using numpy's advanced indexing for efficiency.\n        if adj[i]:\n            delta_C = s[i] * np.sum(s[adj[i]])\n        else: # Handle isolated vertices\n            delta_C = 0\n\n        # c. Calculate the acceptance probability alpha.\n        # alpha = min(1, exp(lambda * Delta_C))\n        # If delta_C > 0, the move improves the cut, so we always accept.\n        # The acceptance probability is 1.\n        if delta_C > 0:\n            accept = True\n        else:\n            # If delta_C = 0, the move might be accepted to escape local optima.\n            acceptance_prob = np.exp(lambda_val * delta_C)\n            if rng.random()  acceptance_prob:\n                accept = True\n            else:\n                accept = False\n        \n        # d. Update state if the move is accepted.\n        if accept:\n            s[i] *= -1  # Flip the spin\n            current_cut_size += delta_C\n\n        # e. Track the maximum cut size encountered so far.\n        if current_cut_size > max_cut_size:\n            max_cut_size = current_cut_size\n            \n    return int(max_cut_size)\n\ndef solve():\n    \"\"\"\n    Defines and runs the test cases, then prints the results in the required format.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test case 1 (triangle graph)\n        {'n': 3, 'E': [(0, 1), (1, 2), (0, 2)], 'T': 20000, 'lambda': 2.0, 'seed': 7},\n        # Test case 2 (cycle of length 4)\n        {'n': 4, 'E': [(0, 1), (1, 2), (2, 3), (3, 0)], 'T': 30000, 'lambda': 2.0, 'seed': 11},\n        # Test case 3 (empty graph)\n        {'n': 4, 'E': [], 'T': 1000, 'lambda': 2.0, 'seed': 5},\n        # Test case 4 (two disjoint edges)\n        {'n': 4, 'E': [(0, 1), (2, 3)], 'T': 20000, 'lambda': 2.0, 'seed': 13},\n    ]\n\n    results = []\n    for case in test_cases:\n        best_cut = run_mcmc_max_cut(\n            n=case['n'],\n            edges=case['E'],\n            T=case['T'],\n            lambda_val=case['lambda'],\n            seed=case['seed']\n        )\n        results.append(best_cut)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3250464"}, {"introduction": "一个成功运行的MCMC采样器不一定是一个正确的采样器；理解其潜在的失效模式至关重要。此练习通过一个精心设计的例子，展示了当马尔可夫链不满足“不可约性”这一关键条件时，MCMC如何无法收敛到正确的目标分布。通过动手实现并诊断这一失效过程，你将深刻理解MCMC理论假设在实践中的重要性。[@problem_id:3157911]", "problem": "你将实现并分析一个 Metropolis–Hastings 马尔可夫链蒙特卡洛 (MCMC) 采样器，以展示当提议核在具有不连通支撑集的目标分布上不是不可约时，采样器会出现收敛失败的情况。整个工作将在一个维度上进行。目标密度 $\\pi(x)$ 在不连通集合 $S = [-3,-1] \\cup [1,3]$ 上与均匀密度成正比，在其他地方为零。也就是说，当 $x \\in S$ 时，$\\pi(x) \\propto 1$；当 $x \\notin S$ 时，$\\pi(x) = 0$。Metropolis–Hastings 链使用形式为 $x' \\sim q(\\cdot \\mid x)$ 的提议，其接受概率为 $\\alpha(x,x') = \\min\\left(1, \\dfrac{\\pi(x')\\,q(x \\mid x')}{\\pi(x)\\,q(x' \\mid x)}\\right)$。你必须将每条链的起始点设为 $x_0 = -2$，该点位于左侧分量 $[-3,-1]$ 中。\n\n基本原理：使用马尔可夫链、不可约性、提议核、Metropolis–Hastings 接受概率的标准定义，以及以下概念：一个马尔可夫链蒙特卡洛采样器若要以 $\\pi(x)$ 为目标分布，其必须是不可约、非周期的，并满足关于 $\\pi(x)$ 的细致平衡条件，而一个正确实现的 Metropolis–Hastings 算法通过其构造即可满足这些条件。\n\n你将考虑两种提议分布族：\n- 步长为 $\\Delta  0$ 的均匀有界随机游走：$q_{\\Delta}(x' \\mid x)$ 是在 $[x-\\Delta, x+\\Delta]$ 上的均匀分布。\n- 标准差为 $\\sigma  0$ 的高斯随机游走：$q_{\\sigma}(x' \\mid x)$ 是 $\\mathcal{N}(x, \\sigma^2)$。\n\n观察：由于 $\\pi(x)$ 在 $S$ 上是常数，在 $S$ 外为零，对于对称提议（均匀有界随机游走和高斯随机游走都是对称的），Metropolis–Hastings 接受准则简化为：接受所有落在 $S$ 内的提议，拒绝所有落在 $S$ 外的提议。\n\n定义分量标签函数为：如果 $x \\in [-3,-1]$，则 $c(x) = 0$；如果 $x \\in [1,3]$，则 $c(x) = 1$。由于链从 $S$ 内开始，并且拒绝任何提议状态 $x' \\notin S$，因此所有状态在初始化后都保持在 $S$ 内。为了进行分析，使用 $B = 1000$ 步的预烧期（burn-in），然后对其余样本计算诊断指标。\n\n你必须为每条链在预烧期后实现以下诊断指标：\n1. 访问过的不同分量的数量，即集合 $\\{c(x_t) : t \\ge B\\}$ 的基数；以 $\\{1,2\\}$ 中的整数表示。\n2. 预烧期后样本中位于右侧分量的比例，计算公式为 $\\frac{1}{T-B}\\sum_{t=B}^{T-1} \\mathbf{1}\\{c(x_t)=1\\}$，其中 $T$ 是总步数；以小数点后四位四舍五入的小数表示。\n3. 各分量间的经验双态转移矩阵，根据连续的预烧期后标签估计。令 $N_{ij}$ 为在时间 $t$ 从分量 $i$ 转移到时间 $t+1$ 的分量 $j$ 的计数，其中 $t=B,\\dots,T-2$。定义 $\\widehat{P}_{ij} = \\frac{N_{ij}}{\\sum_{k \\in \\{0,1\\}} N_{ik}}$（若分母为正），否则 $\\widehat{P}_{ij} = 0$。报告非对角线元素 $\\widehat{P}_{0,1}$ 和 $\\widehat{P}_{1,0}$（每个都四舍五入到小数点后四位）。\n4. 双分量聚合链的估计不可约性标志，定义为布尔值 $(\\widehat{P}_{0,1}  0)$ 和 $(\\widehat{P}_{1,0}  0)$。\n\n你必须实现采样器并为下面的测试套件计算这些诊断指标。按照规定使用独立的随机数生成器种子以保证可复现性。\n\n测试套件：\n- 情况 1（非不可约）：均匀有界提议，$\\Delta = 1.5$，总步数 $T = 5000$，随机种子 $12345$。\n- 情况 2（边界非不可约）：均匀有界提议，$\\Delta = 2.0$，总步数 $T = 5000$，随机种子 $12345$。\n- 情况 3（因范围而不可约）：均匀有界提议，$\\Delta = 2.5$，总步数 $T = 5000$，随机种子 $12345$。\n- 情况 4（因无界支撑集而不可约）：高斯提议，$\\sigma = 1.0$，总步数 $T = 6000$，随机种子 $67890$。\n\n基于第一性原理的预期行为指导：两个分量之间的间隙宽度为 2。对于均匀有界提议，当 $\\Delta  2$ 时，链无法在一步内穿越间隙，因此在 $S$ 上不是不可约的；当 $\\Delta = 2$ 时，唯一的穿越方式需要提议一个恰好为 2 的跳跃，这在连续提议分布下的概率为 0，因此链仍然是非不可约的；当 $\\Delta  2$ 时，穿越成为可能。对于高斯提议，其支撑集是无界的，因此对于任何 $\\sigma  0$，穿越的概率都为正。\n\n最终输出格式：你的程序应生成单行输出，包含一个由方括号括起来的逗号分隔列表。每个测试用例贡献一个形如 [visited_components,positive_fraction,p01,p10,is_irreducible_estimated] 的子列表，其中 visited_components 是一个整数，positive_fraction、p01 和 p10 是四舍五入到四位小数的小数，is_irreducible_estimated 是一个布尔值。因此，总输出必须是一个包含这四个子列表的单一列表，且没有空格，例如：[[1,0.0000,0.0000,0.0000,False],[...],[...],[...]]。不允许使用外部文件或输入；所有代码必须独立运行。\n\n角度单位不适用。物理单位不适用。百分比必须以小数形式表示。", "solution": "本问题的目标是展示 Metropolis-Hastings 马尔可夫链蒙特卡洛 (MCMC) 算法的一个关键失效模式：当底层马尔可夫链不是不可约时，算法无法收敛到真实的目标分布。如果一条马尔可夫链可以从任何状态在有限步数内到达任何其他状态，则称其为不可约的。为了使一个 MCMC 采样器能够从目标分布 $\\pi(x)$ 中正确采样，其对应的马尔可夫链必须在 $\\pi(x)$ 的整个支撑集上是不可约的。如果提议核使得支撑集的某些区域无法从其他区域到达，采样器将无法探索整个分布，从而导致有偏和不正确的估计。\n\n我们将使用一个一维目标密度 $\\pi(x)$ 来研究这一现象，该密度在一个不连通的支撑集 $S = [-3, -1] \\cup [1, 3]$ 上是均匀的，在其他地方为零。因此，概率密度为：\n$$\n\\pi(x) = \\begin{cases}\n    1/4  \\text{ if } x \\in [-3, -1] \\cup [1, 3] \\\\\n    0  \\text{ otherwise}\n\\end{cases}\n$$\n支撑集 $S$ 的总长度为 $( -1 - (-3)) + (3 - 1) = 2 + 2 = 4$。\n\nMetropolis-Hastings 算法通过从一个提议分布 $q(x' \\mid x_t)$ 中提议一个新状态 $x'$，并以如下概率接受它，来生成一个状态序列 $\\{x_t\\}$：\n$$\n\\alpha(x_t, x') = \\min\\left(1, \\frac{\\pi(x') q(x_t \\mid x')}{\\pi(x') q(x' \\mid x_t)}\\right)\n$$\n我们将使用两种对称的提议核，对于这类核，$q(x \\mid x') = q(x' \\mid x)$。接受概率简化为 $\\alpha(x_t, x') = \\min(1, \\pi(x') / \\pi(x_t))$。考虑到我们特定的 $\\pi(x)$，如果当前状态 $x_t$ 在支撑集 $S$ 内，则 $\\pi(x_t) = 1/4$。\n- 如果提议的状态 $x'$ 也在 $S$ 内，则 $\\pi(x') = 1/4$，比率为 1，接受概率为 $\\alpha(x_t, x') = 1$。提议总是被接受。\n- 如果提议的状态 $x'$ 不在 $S$ 内，则 $\\pi(x') = 0$，比率为 0，接受概率为 $\\alpha(x_t, x') = 0$。提议总是被拒绝，链保持在 $x_{t+1} = x_t$。\n\n链在 $x_0 = -2$ 处初始化，该点位于支撑集的左分量 $[-3,-1]$ 中。\n\n两种提议核是：\n1.  均匀有界随机游走，其中 $x' \\sim U(x - \\Delta, x + \\Delta)$。为了让链能够从左分量 $[-3, -1]$ 转移到右分量 $[1, 3]$，提议区间必须能够跨越它们之间宽度为 2 的间隙。左分量的最右点是 $x = -1$。从此点出发的提议位于 $[-1-\\Delta, -1+\\Delta]$ 内。为了使该区间与 $[1, 3]$ 重叠，我们必须有 $-1+\\Delta  1$，这意味着 $\\Delta  2$。如果 $\\Delta \\le 2$，链就不可能从左分量跳到右分量，从而使得链在 $S$ 上非不可约。\n2.  高斯随机游走，其中 $x' \\sim \\mathcal{N}(x, \\sigma^2)$。该分布具有无界支撑集 $(-\\infty, \\infty)$。因此，对于任何当前状态 $x \\in S$ 和任何 $\\sigma  0$，在支撑集的任何其他部分提议一个新状态 $x'$ 的概率密度都非零。这确保了链是不可约的。\n\n为了分析其行为，我们将模拟链 $T$ 步，并丢弃前 $B = 1000$ 步作为预烧期（burn-in）。我们定义一个分量标签函数 $c(x)$，其中对于 $x \\in [-3,-1]$，$c(x) = 0$；对于 $x \\in [1,3]$，$c(x) = 1$。我们在预烧期后的样本上计算以下诊断指标：\n1.  访问过的不同分量的数量。对于一条已经混合的不可约链，这个数量必须是 2。\n2.  位于右侧分量 ($c(x)=1$) 的样本比例。对于一个从对称分布中采样的不可约链，这个比例应接近 0.5。\n3.  分量间经验转移矩阵的非对角线元素 $\\widehat{P}_{0,1}$ 和 $\\widehat{P}_{1,0}$。它们分别代表从分量 0 跳到 1 和从分量 1 跳到 0 的估计概率。\n4.  不可约性的经验标志，$(\\widehat{P}_{0,1}  0) \\land (\\widehat{P}_{1,0}  0)$。当且仅当观察到两个分量之间的转移时，该标志应为真。\n\n根据此理论分析，我们实现算法并按规定对四个测试用例进行评估，以展示预测的行为。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the MCMC simulations and print the final results.\n    \"\"\"\n\n    def run_mcmc(proposal_type, param, T, x0, seed):\n        \"\"\"\n        Generates a Markov chain using the Metropolis-Hastings algorithm.\n\n        Args:\n            proposal_type (str): 'uniform' or 'gaussian'.\n            param (float): Delta for uniform or sigma for Gaussian.\n            T (int): Total number of steps.\n            x0 (float): Initial state.\n            seed (int): Random number generator seed.\n\n        Returns:\n            np.ndarray: The generated Markov chain of length T.\n        \"\"\"\n        rng = np.random.default_rng(seed)\n        chain = np.zeros(T)\n        chain[0] = x0\n        current_x = x0\n\n        for t in range(T - 1):\n            if proposal_type == 'uniform':\n                proposal = rng.uniform(current_x - param, current_x + param)\n            elif proposal_type == 'gaussian':\n                proposal = rng.normal(current_x, param)\n            \n            # Check if the proposal is in the support S = [-3,-1] U [1,3]\n            in_S = (-3.0 = proposal = -1.0) or (1.0 = proposal = 3.0)\n            \n            # Simplified Metropolis-Hastings acceptance rule\n            if in_S:\n                current_x = proposal\n            \n            chain[t + 1] = current_x\n        \n        return chain\n\n    def analyze_chain(chain, B):\n        \"\"\"\n        Computes diagnostics for a given Markov chain after burn-in.\n\n        Args:\n            chain (np.ndarray): The Markov chain.\n            B (int): The number of burn-in steps.\n\n        Returns:\n            list: A list containing the computed diagnostics.\n        \"\"\"\n        post_burn_in = chain[B:]\n        \n        def component_label(x):\n            return 0 if -3.0 = x = -1.0 else 1\n\n        labels = [component_label(x) for x in post_burn_in]\n        \n        # 1. Number of distinct components visited\n        visited_components = len(set(labels))\n        \n        # 2. Fraction of post–burn-in samples in the right component\n        if not labels:\n            positive_fraction = 0.0\n        else:\n            num_in_right = sum(1 for label in labels if label == 1)\n            positive_fraction = num_in_right / len(labels)\n        \n        # 3. Empirical two-state transition matrix\n        N00, N01, N10, N11 = 0, 0, 0, 0\n        for i in range(len(labels) - 1):\n            if labels[i] == 0:\n                if labels[i+1] == 0: N00 += 1\n                else: N01 += 1\n            else: # labels[i] == 1\n                if labels[i+1] == 0: N10 += 1\n                else: N11 += 1\n                \n        N0_total = N00 + N01\n        N1_total = N10 + N11\n        \n        p01 = float(N01) / N0_total if N0_total > 0 else 0.0\n        p10 = float(N10) / N1_total if N1_total > 0 else 0.0\n        \n        # 4. Estimated irreducibility flag\n        is_irreducible_estimated = (p01 > 0) and (p10 > 0)\n        \n        return [visited_components, positive_fraction, p01, p10, is_irreducible_estimated]\n\n    test_cases = [\n        {'type': 'uniform', 'param': 1.5, 'T': 5000, 'seed': 12345},\n        {'type': 'uniform', 'param': 2.0, 'T': 5000, 'seed': 12345},\n        {'type': 'uniform', 'param': 2.5, 'T': 5000, 'seed': 12345},\n        {'type': 'gaussian', 'param': 1.0, 'T': 6000, 'seed': 67890}\n    ]\n    \n    B = 1000\n    x0 = -2.0\n    \n    all_results = []\n    for case in test_cases:\n        chain = run_mcmc(case['type'], case['param'], case['T'], x0, case['seed'])\n        diagnostics = analyze_chain(chain, B)\n        all_results.append(diagnostics)\n\n    result_strings = []\n    for diag in all_results:\n        # Format: [visited_components, positive_fraction, p01, p10, is_irreducible_estimated]\n        # Decimals rounded to 4 places.\n        s = (f\"[{diag[0]},{diag[1]:.4f},{diag[2]:.4f},\"\n             f\"{diag[3]:.4f},{diag[4]}]\")\n        result_strings.append(s)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(result_strings)}]\")\n\nsolve()\n```", "id": "3157911"}]}