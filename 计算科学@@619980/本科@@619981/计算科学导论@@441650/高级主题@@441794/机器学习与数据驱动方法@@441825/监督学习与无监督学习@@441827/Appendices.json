{"hands_on_practices": [{"introduction": "在生物信息学中，我们经常需要分析大规模数据集以揭示生物学规律。本练习将以转录因子（TF）的全基因组结合数据为例，带领你探索两种截然不同的分析范式。你将实践无监督聚类方法，在没有任何先验知识的情况下根据结合模式的相似性发现TF的自然分组；同时，你也会实现一个有监督分类器，利用已知的TF类别来训练模型并对新数据进行预测。[@problem_id:2432815] 这个实践将帮助你理解如何根据不同的科学问题选择合适的学习方法。", "problem": "给定在源于染色质免疫沉淀测序 (ChIP-seq) 的全基因组结合谱上的聚类和分类任务的形式化定义。每个谱对应一个转录因子 (TF)。一个数据集表示为一个矩阵 $X \\in \\mathbb{R}_{\\ge 0}^{n \\times m}$，其中有 $n$ 个转录因子 (行) 和 $m$ 个基因组区间 (列)，$X_{ij}$ 是一个非负整数计数。对于所有任务，请从基本原理出发定义以下操作和量。\n\n- 行归一化：对于每个转录因子谱 $\\mathbf{x}_i \\in \\mathbb{R}_{\\ge 0}^m$，定义其 $\\ell_2$归一化向量为 $\\mathbf{y}_i = \\mathbf{x}_i / \\lVert \\mathbf{x}_i \\rVert_2$，其中 $\\lVert \\cdot \\rVert_2$ 表示欧几里得范数。如果 $\\lVert \\mathbf{x}_i \\rVert_2 = 0$，则将 $\\mathbf{y}_i$ 视为零向量。\n- 余弦距离：对于任意两个归一化向量 $\\mathbf{u}, \\mathbf{v} \\in \\mathbb{R}^m$，定义 $d(\\mathbf{u}, \\mathbf{v}) = 1 - \\mathbf{u}^\\top \\mathbf{v}$。\n- 归一化空间中的簇内平方和 (WCSS)：对于将 $\\{1,\\dots,n\\}$ 划分为 $k$ 个非空簇的一个划分 $\\mathcal{P} = \\{S_1, \\dots, S_k\\}$，以及对应的质心 $\\boldsymbol{\\mu}_c = \\frac{1}{|S_c|} \\sum_{i \\in S_c} \\mathbf{y}_i$，定义\n$$\n\\mathrm{WCSS}(\\mathcal{P}) = \\sum_{c=1}^k \\sum_{i \\in S_c} \\lVert \\mathbf{y}_i - \\boldsymbol{\\mu}_c \\rVert_2^2.\n$$\n- 在归一化空间中使用余弦距离的轮廓系数：对于每个 $i \\in \\{1,\\dots,n\\}$ 及其所属簇 $c(i)$，定义\n$$\na(i) = \\frac{1}{|S_{c(i)}|-1} \\sum_{j \\in S_{c(i)},\\, j \\ne i} d(\\mathbf{y}_i, \\mathbf{y}_j)\n$$\n如果 $|S_{c(i)}| \\ge 2$，若 $|S_{c(i)}| = 1$ 则设 $a(i) = 0$。对于任何其他簇 $c' \\ne c(i)$，定义\n$$\n\\bar{d}(i, c') = \\frac{1}{|S_{c'}|} \\sum_{j \\in S_{c'}} d(\\mathbf{y}_i, \\mathbf{y}_j).\n$$\n令 $b(i) = \\min_{c' \\ne c(i)} \\bar{d}(i,c')$。$i$ 的轮廓系数是\n$$\ns(i) = \n\\begin{cases}\n0,  \\text{若 } a(i) = 0 \\text{ 且 } b(i) = 0, \\\\\n\\frac{b(i) - a(i)}{\\max\\{a(i), b(i)\\}},  \\text{其他情况}.\n\\end{cases}\n$$\n平均轮廓系数为 $\\bar{s} = \\frac{1}{n} \\sum_{i=1}^n s(i)$。\n- 归一化空间中的最近质心分类：对于带标签的类别 $\\mathcal{C}$ 和一个训练集 $T \\subset \\{1,\\dots,n\\}$（其中 $i \\in T$ 的标签为 $\\ell(i) \\in \\mathcal{C}$），定义类别均值为 $\\boldsymbol{\\nu}_c = \\frac{1}{|\\{i \\in T : \\ell(i) = c\\}|} \\sum_{i \\in T, \\ell(i)=c} \\mathbf{y}_i$。对于每个测试样本 $j \\notin T$，预测其类别为 $\\hat{\\ell}(j) = \\arg\\min_{c \\in \\mathcal{C}} \\lVert \\mathbf{y}_j - \\boldsymbol{\\nu}_c \\rVert_2$。准确率是测试样本中正确预测的比例。如果某个类别均值为零向量，则直接使用。\n\n聚类任务：对于一个给定的候选集 $\\mathcal{K} \\subset \\mathbb{N}$（其中所有 $k \\in \\mathcal{K}$ 满足 $k \\ge 2$）和一个无标签的数据集，对每个 $k \\in \\mathcal{K}$，生成一个划分 $\\mathcal{P}_k$ 以最小化归一化空间中的 $\\mathrm{WCSS}(\\mathcal{P}_k)$。令 $\\bar{s}_k$ 为 $\\mathcal{P}_k$ 的平均轮廓系数。选择 $k^\\star = \\arg\\max_{k \\in \\mathcal{K}} \\bar{s}_k$。如果 $\\bar{s}_k$ 出现相同值，则选择最小的 $k$。如果所有两两之间的距离均为零（即所有归一化向量都相同），则对于所有的 $k$，将所有轮廓系数定义为 $0$。\n\n监督分类任务：对于一个给定的带标签数据集，在指定的测试集上计算如上定义的最近质心准确率。\n\n数据生成模型：除非另有说明，无标签数据集由每个区间的独立泊松分布混合生成。对于簇 $c$，指定均值向量 $\\boldsymbol{\\lambda}^{(c)} \\in \\mathbb{R}_{>0}^m$；簇 $c$ 中的每个转录因子 $\\mathbf{x}$ 的分量 $x_j$ 均独立抽取于 $\\mathrm{Poisson}(\\lambda^{(c)}_j)$ 分布，其中 $j \\in \\{1,\\dots,m\\}$。带标签的数据集遵循相同的生成过程，其标签由生成它的簇决定。所有随机抽取均使用指定的随机种子进行，且各簇按指定的样本数量实现。在所有谱都是确定性且相同的边界情况下，对所有行使用所提供的固定向量。\n\n测试套件。请精确实现以下四个测试用例；它们构成了您程序的全部输入。\n\n- 测试用例 1 (无标签聚类；理想情况):\n  - $n = 90$, $m = 60$, 随机种子 $= 123$。\n  - 三个簇 $C_1, C_2, C_3$，每个簇有 $30$ 个转录因子。\n  - 背景均值 $\\mu_{\\mathrm{low}} = 2$ 和提升均值 $\\mu_{\\mathrm{high}} = 30$。\n  - 均值:\n    - 对于 $C_1$：区间 $0$ 到 $19$ (含) 的均值为 $\\mu_{\\mathrm{high}}$，其余为 $\\mu_{\\mathrm{low}}$。\n    - 对于 $C_2$：区间 $20$ 到 $39$ (含) 的均值为 $\\mu_{\\mathrm{high}}$，其余为 $\\mu_{\\mathrm{low}}$。\n    - 对于 $C_3$：区间 $40$ 到 $59$ (含) 的均值为 $\\mu_{\\mathrm{high}}$，其余为 $\\mu_{\\mathrm{low}}$。\n  - 候选集 $\\mathcal{K} = \\{2,3,4,5\\}$。\n  - 此用例的输出：一个列表 $[k^\\star, \\bar{s}_{k^\\star}]$。\n\n- 测试用例 2 (无标签聚类；边界条件):\n  - $n = 20$, $m = 10$。\n  - 所有转录因子均相同且为确定性：$X$ 的每一行都等于同一个固定的非零向量，其所有分量均为 $5$。\n  - 候选集 $\\mathcal{K} = \\{2,3\\}$。\n  - 此用例的输出：一个列表 $[k^\\star, \\bar{s}_{k^\\star}]$，其结果必须反映针对零距离的指定平局处理规则和轮廓系数定义。\n\n- 测试用例 3 (监督分类；带标签评估):\n  - $n = 100$, $m = 40$, 随机种子 $= 321$。\n  - 两个类别 $A$ 和 $B$，每个类别有 $50$ 个转录因子，由泊松模型生成。\n  - 背景均值 $\\mu_{\\mathrm{low}} = 2$ 和提升均值 $\\mu_{\\mathrm{high}} = 25$。\n  - 均值:\n    - 对于类别 $A$：区间 $0$ 到 $19$ (含) 的均值为 $\\mu_{\\mathrm{high}}$，其余为 $\\mu_{\\mathrm{low}}$。\n    - 对于类别 $B$：区间 $20$ 到 $39$ (含) 的均值为 $\\mu_{\\mathrm{high}}$，其余为 $\\mu_{\\mathrm{low}}$。\n  - 训练/测试集划分：从每个类别中精确使用 $30$ 个转录因子进行训练，剩余的 $20$ 个转录因子用于测试。数据按所有A样本在前、所有B样本在后的顺序排列；选择每个类别的前30个样本（按此顺序）用于训练，其余用于测试。\n  - 此用例的输出：一个浮点数，等于测试集上的最近质心准确率，表示为 $[0,1]$ 区间内的小数。\n\n- 测试用例 4 (无标签聚类；高维、部分重叠信号):\n  - $n = 60$, $m = 200$, 随机种子 $= 999$。\n  - 两个簇 $C_1, C_2$，每个簇有 $30$ 个转录因子。\n  - 背景均值 $\\mu_{\\mathrm{low}} = 3$ 和提升均值 $\\mu_{\\mathrm{high}} = 15$。\n  - 均值:\n    - 对于 $C_1$：区间 $0$ 到 $99$ (含) 的均值为 $\\mu_{\\mathrm{high}}$，其余为 $\\mu_{\\mathrm{low}}$。\n    - 对于 $C_2$：区间 $20$ 到 $119$ (含) 的均值为 $\\mu_{\\mathrm{high}}$，其余为 $\\mu_{\\mathrm{low}}$。\n  - 候选集 $\\mathcal{K} = \\{2,3\\}$。\n  - 此用例的输出：一个列表 $[k^\\star, \\bar{s}_{k^\\star}]$。\n\n最终输出格式。您的程序应生成单行输出，其中包含四个测试用例的结果，结果为方括号括起来的逗号分隔列表，顺序如下：测试用例1，测试用例2，测试用例3，测试用例4。每个测试用例的结果必须是布尔值、整数、浮点数或由这些类型组成的列表，与上面的规定完全一致。例如，您的输出应类似于 $[ [k_1^\\star, \\bar{s}_{k_1^\\star}], [k_2^\\star, \\bar{s}_{k_2^\\star}], \\mathrm{acc}_3, [k_4^\\star, \\bar{s}_{k_4^\\star}] ]$，其中包含根据定义计算出的具体数值。不应打印任何额外文本。", "solution": "所提供的问题陈述是有效的。它在生物信息学和机器学习领域具有科学依据，提法明确，具有清晰的数学定义和目标，并且没有任何矛盾、歧义或不成立的前提。该问题要求实现用于ChIP-seq数据谱的聚类和分类算法，并在指定的测试套件上进行评估。\n\n我的方法包括直接实现指定的数学公式和程序。整个过程分为两个主要部分：一个无监督聚类任务和一个监督分类任务。\n\n首先，我为常用操作建立辅助函数。原始数据，一个整数计数的矩阵 $X \\in \\mathbb{R}_{\\ge 0}^{n \\times m}$，是根据指定的泊松模型 $x_j \\sim \\mathrm{Poisson}(\\lambda_j)$ 生成的。然后，每个行向量 $\\mathbf{x}_i$ 被归一化为单位向量 $\\mathbf{y}_i = \\mathbf{x}_i / \\lVert \\mathbf{x}_i \\rVert_2$，并注意处理 $\\lVert \\mathbf{x}_i \\rVert_2 = 0$ 的情况，此时 $\\mathbf{y}_i$ 成为一个零向量。\n\n对于聚类任务，目标是找到一个划分 $\\mathcal{P}_k$，以最小化簇内平方和 (WCSS)，其定义为 $\\mathrm{WCSS}(\\mathcal{P}) = \\sum_{c=1}^k \\sum_{i \\in S_c} \\lVert \\mathbf{y}_i - \\boldsymbol{\\mu}_c \\rVert_2^2$，其中 $\\boldsymbol{\\mu}_c$ 是簇的质心。这是K-均值聚类的标准目标。由于找到全局最小值是一个NP难问题，我采用广泛使用的K-均值算法（Lloyd算法）作为一种启发式方法。我使用了 `scipy.cluster.vq.kmeans` 的实现，它会执行多次随机初始化运行并返回找到的最佳划分，从而为WCSS最小值提供一个鲁棒的近似。\n\n一旦为每个候选簇数 $k \\in \\mathcal{K}$ 获得了划分 $\\mathcal{P}_k$，就使用平均轮廓系数 $\\bar{s}_k$ 来评估其质量。单个数据点 $i$ 的轮廓系数 $s(i)$ 是衡量其在所属簇内与相邻簇相比的拟合程度。其定义为 $s(i) = (b(i) - a(i)) / \\max\\{a(i), b(i)\\}$，其中 $a(i)$ 是到其自身簇中其他点的平均距离，而 $b(i)$ 是到任何其他簇中点的最小平均距离。所使用的距离度量是指定的余弦距离 $d(\\mathbf{u}, \\mathbf{v}) = 1 - \\mathbf{u}^\\top \\mathbf{v}$。我的实现会计算所有点的 $s(i)$ 并将它们平均以得到 $\\bar{s}_k$。我处理了所有两两之间距离均为零的特殊情况，此时轮廓系数定义为零。最佳簇数 $k^\\star$ 选择为最大化 $\\bar{s}_k$ 的那个，若有平局则选择最小的 $k$。\n\n对于监督分类任务，模型是一个最近质心分类器。给定一个带有类别标签 $\\{\\ell(i)\\}$ 的归一化向量训练集 $\\{\\mathbf{y}_i\\}$，我首先为每个类别 $c$ 计算均值向量或称质心 $\\boldsymbol{\\nu}_c$。对于任何新的测试向量 $\\mathbf{y}_j$，通过找到欧几里得距离上离它最近的质心来预测其类别：$\\hat{\\ell}(j) = \\arg\\min_c \\lVert \\mathbf{y}_j - \\boldsymbol{\\nu}_c \\rVert_2$。分类器的性能由其准确率来衡量，即测试集中正确预测标签的比例。\n\n然后将这些已实现的程序应用于四个指定的测试用例，每个用例都有其自己的数据生成和评估任务参数。严格遵循用于随机数生成的种子以确保可复现性。最终结果被收集并按要求格式化为单个列表。", "answer": "```python\nimport numpy as np\nfrom scipy.cluster.vq import kmeans, vq\n\ndef solve():\n    \"\"\"\n    Main function to solve the four test cases and print the final result.\n    \"\"\"\n    \n    def generate_data(cluster_means, samples_per_cluster, seed):\n        \"\"\"Generates data from a mixture of Poisson distributions.\"\"\"\n        rng = np.random.default_rng(seed)\n        X_parts = []\n        for i, lambda_vec in enumerate(cluster_means):\n            n_samples = samples_per_cluster[i]\n            m = lambda_vec.shape[0]\n            cluster_data = rng.poisson(lambda_vec, size=(n_samples, m))\n            X_parts.append(cluster_data)\n        return np.vstack(X_parts)\n\n    def normalize_data(X):\n        \"\"\"L2-normalizes the rows of matrix X.\"\"\"\n        norms = np.linalg.norm(X, axis=1, keepdims=True)\n        # Handle zero-norm vectors as per problem statement\n        Y = np.divide(X, norms, out=np.zeros_like(X, dtype=float), where=(norms != 0))\n        return Y\n\n    def calculate_mean_silhouette(Y, labels):\n        \"\"\"Calculates the mean silhouette coefficient for a given partition.\"\"\"\n        n = Y.shape[0]\n        unique_labels, counts = np.unique(labels, return_counts=True)\n        k_found = len(unique_labels)\n        \n        # Pre-compute pairwise cosine distances\n        dot_products = Y @ Y.T\n        np.clip(dot_products, -1.0, 1.0, out=dot_products) # For floating point precision\n        D = 1.0 - dot_products\n\n        # Per problem statement for case of identical vectors\n        if np.allclose(D, 0):\n            return 0.0\n\n        s_coeffs = np.zeros(n)\n        for i in range(n):\n            c_i_label = labels[i]\n            \n            is_in_cluster_i = (labels == c_i_label)\n            count_in_cluster_i = counts[unique_labels == c_i_label][0]\n            \n            if count_in_cluster_i > 1:\n                a_i = np.sum(D[i, is_in_cluster_i]) / (count_in_cluster_i - 1)\n            else: # Singleton cluster\n                a_i = 0.0\n            \n            b_i = np.inf\n            if k_found > 1:\n                for c_prime_label in unique_labels:\n                    if c_prime_label == c_i_label:\n                        continue\n                    is_in_cluster_prime = (labels == c_prime_label)\n                    count_in_cluster_prime = counts[unique_labels == c_prime_label][0]\n                    d_bar = np.sum(D[i, is_in_cluster_prime]) / count_in_cluster_prime\n                    if d_bar < b_i:\n                        b_i = d_bar\n            \n            if np.isinf(b_i): # Only happens if k_found = 1\n                b_i = 0.0\n\n            if a_i == 0.0 and b_i == 0.0:\n                s_coeffs[i] = 0.0\n            else:\n                denom = max(a_i, b_i)\n                s_coeffs[i] = (b_i - a_i) / denom\n        \n        return np.mean(s_coeffs)\n\n    def test_case_1():\n        n, m, seed = 90, 60, 123\n        n_per_cluster = 30\n        mu_low, mu_high = 2, 30\n        K_set = [2, 3, 4, 5]\n        \n        np.random.seed(seed)\n        \n        lambda1 = np.full(m, mu_low); lambda1[0:20] = mu_high\n        lambda2 = np.full(m, mu_low); lambda2[20:40] = mu_high\n        lambda3 = np.full(m, mu_low); lambda3[40:60] = mu_high\n        \n        X = generate_data([lambda1, lambda2, lambda3], [n_per_cluster] * 3, seed)\n        Y = normalize_data(X)\n        \n        s_scores = []\n        for k in K_set:\n            centroids, _ = kmeans(Y, k, iter=100)\n            labels, _ = vq(Y, centroids)\n            s_k = calculate_mean_silhouette(Y, labels)\n            s_scores.append(s_k)\n\n        best_s_idx = np.argmax(s_scores)\n        k_star = K_set[best_s_idx]\n        best_s = s_scores[best_s_idx]\n\n        return [k_star, best_s]\n\n    def test_case_2():\n        n, m = 20, 10\n        K_set = [2, 3]\n        \n        X = np.full((n, m), 5.0)\n        Y = normalize_data(X)\n        \n        # All pairwise distances are zero. By definition, silhouette is 0 for all k.\n        s_scores = {k: 0.0 for k in K_set}\n\n        # Tie-breaking rule: select smallest k\n        k_star = min(K_set)\n        s_k_star = s_scores[k_star]\n        \n        return [k_star, s_k_star]\n\n    def test_case_3():\n        n, m, seed = 100, 40, 321\n        n_per_class = 50\n        mu_low, mu_high = 2, 25\n        \n        np.random.seed(seed)\n        \n        lambdaA = np.full(m, mu_low); lambdaA[0:20] = mu_high\n        lambdaB = np.full(m, mu_low); lambdaB[20:40] = mu_high\n        \n        X = generate_data([lambdaA, lambdaB], [n_per_class] * 2, seed)\n        labels = np.concatenate([np.zeros(n_per_class), np.ones(n_per_class)])\n        \n        train_indices = np.concatenate([np.arange(0, 30), np.arange(50, 80)])\n        test_indices = np.concatenate([np.arange(30, 50), np.arange(80, 100)])\n        \n        Y = normalize_data(X)\n        Y_train, labels_train = Y[train_indices], labels[train_indices]\n        Y_test, labels_test = Y[test_indices], labels[test_indices]\n        \n        mean_A = Y_train[labels_train == 0].mean(axis=0)\n        mean_B = Y_train[labels_train == 1].mean(axis=0)\n        \n        predictions = []\n        for y_j in Y_test:\n            dist_A = np.linalg.norm(y_j - mean_A)\n            dist_B = np.linalg.norm(y_j - mean_B)\n            predictions.append(0 if dist_A < dist_B else 1)\n        \n        return np.mean(np.array(predictions) == labels_test)\n        \n    def test_case_4():\n        n, m, seed = 60, 200, 999\n        n_per_cluster = 30\n        mu_low, mu_high = 3, 15\n        K_set = [2, 3]\n\n        np.random.seed(seed)\n        \n        lambda1 = np.full(m, mu_low); lambda1[0:100] = mu_high\n        lambda2 = np.full(m, mu_low); lambda2[20:120] = mu_high\n        \n        X = generate_data([lambda1, lambda2], [n_per_cluster] * 2, seed)\n        Y = normalize_data(X)\n        \n        s_scores = []\n        for k in K_set:\n            centroids, _ = kmeans(Y, k, iter=100)\n            labels, _ = vq(Y, centroids)\n            s_k = calculate_mean_silhouette(Y, labels)\n            s_scores.append(s_k)\n\n        best_s_idx = np.argmax(s_scores)\n        k_star = K_set[best_s_idx]\n        best_s = s_scores[best_s_idx]\n\n        return [k_star, best_s]\n\n    results = [\n        test_case_1(),\n        test_case_2(),\n        test_case_3(),\n        test_case_4()\n    ]\n\n    # Format the final output string as specified\n    # The default str() for lists includes spaces, which we remove.\n    print(f\"[{','.join(map(lambda x: str(x).replace(' ', ''), results))}]\")\n\nsolve()\n```", "id": "2432815"}, {"introduction": "评估模型性能的指标直接反映了学习任务的目标。有监督学习旨在最大化预测准确率，而无监督聚类则追求发现数据的内在结构，例如通过轮廓系数（silhouette score）来衡量。本练习通过一系列精心设计的思想实验，挑战你分析一些特殊的数据集，在这些数据集上，有监督分类的准确率很高，但无监督聚类的质量却很差。[@problem_id:3199424] 这种目标上的冲突，深刻揭示了两种学习范式之间的根本差异。", "problem": "在监督学习中，训练分类器将特征向量映射到给定的标签，以最小化经验误差；而在非监督学习中，仅从数据本身推断结构，无需标签。考虑二维欧几里得空间中的点，其特征表示为 $x = (x_1, x_2) \\in \\mathbb{R}^2$，类别标签为二元值 $y \\in \\{0, 1\\}$。监督学习的目标是为每个$x$选择预测值$\\hat{y}$，以最小化训练集上的经验分类误差，这是经验风险最小化 (ERM) 的一个直接实例。聚类中的非监督目标是将点划分为 $k$ 个组，以优化一个内聚和分离准则；一个广泛使用的选项是基于成对欧几里得距离的轮廓系数。本问题将此比较操作化：您将使用一个在标签上训练的简单阈值分类器计算监督学习的准确率，并使用在原始特征上运行的、采用确定性初始化的K-均值聚类 (K-Means) 计算非监督学习的轮廓系数，然后评估三个旨在突显这两种目标何时出现分歧的数据集。\n\n定义和所需计算：\n- 监督分类器。限制在第一个坐标$x_1$上进行一维决策。对于一个阈值$t \\in \\mathbb{R}$和一个左侧类别$c_{\\text{left}} \\in \\{0, 1\\}$，分类器预测为\n$$\n\\hat{y}(x) = \\begin{cases}\nc_{\\text{left}},  \\text{if } x_1 \\le t,\\\\\n1 - c_{\\text{left}},  \\text{if } x_1 > t.\n\\end{cases}\n$$\n在所有由连续排序的 $x_1$ 值（包括刚好低于最小值和刚好高于最大值的值）之间的中点构成的阈值中，选择能使训练准确率最大化的$(t, c_{\\text{left}})$\n$$\n\\text{Acc} = \\frac{1}{N}\\sum_{i=1}^N \\mathbf{1}\\left(\\hat{y}(x^{(i)}) = y^{(i)}\\right),\n$$\n其中 $N$ 是样本数量，$\\mathbf{1}(\\cdot)$ 是指示函数。若出现平局，选择 $c_{\\text{left}} = 0$ 且能达到最大值的最小 $t$。将$\\text{Acc}$报告为一个在 $[0,1]$ 区间内的小数。\n- 非监督聚类。在特征向量$x^{(i)}$上使用欧几里得距离运行$k=2$个簇的K-均值算法。确定性地将两个质心初始化为$x_1$值最小和最大的点。迭代地将点重新分配到最近的质心，并将质心更新为所分配点的均值，直到分配稳定或达到 $100$ 次迭代。如果某个簇变为空，则将其质心重新初始化为距离另一个质心最远（欧几里得距离）的点。收敛后，计算轮廓系数。对于每个点$i$，令$a(i)$为$x^{(i)}$与其所在簇中所有其他点的平均欧几里得距离（如果该簇大小为 $1$，则令 $a(i) = 0$）。令$b(i)$为$x^{(i)}$到所有其他簇中点的平均欧几里得距离的最小值。轮廓值为\n$$\ns(i) = \\begin{cases}\n\\frac{b(i) - a(i)}{\\max\\{a(i), b(i)\\}},  \\text{if } \\max\\{a(i), b(i)\\} > 0,\\\\\n0,  \\text{otherwise},\n\\end{cases}\n$$\n而轮廓系数是所有点$s(i)$的平均值。\n\n测试套件数据集（每个样本以 $(x_1, x_2)$ 及其关联标签 $y$ 的形式给出；顺序任意，所有数字均为实数）：\n1. 数据集 A（沿 $x_1$ 中度分离，有轻微重叠）：\n   - 类别 0：$(-0.4, 0.0)$, $(-0.2, 0.1)$, $(0.2, -0.1)$, $(0.3, 0.05)$\n   - 类别 1：$(0.2, 0.0)$, $(0.6, -0.1)$, $(0.8, 0.1)$, $(1.0, -0.05)$\n2. 数据集 B（沿 $x_1$ 分离度较大，但沿 $x_2$ 噪声较大）：\n   - 类别 0：$(-0.4, 5.0)$, $(-0.2, -5.0)$, $(0.2, 7.0)$, $(0.3, -7.0)$\n   - 类别 1：$(1.2, 6.0)$, $(1.6, -6.0)$, $(1.8, 8.0)$, $(2.0, -8.0)$\n3. 数据集 C（几乎完全重叠：按构造具有相同的分布）：\n   - 类别 0：$(-0.1, 0.05)$, $(0.0, -0.02)$, $(0.1, 0.03)$, $(-0.05, -0.04)$\n   - 类别 1：$(-0.08, 0.01)$, $(0.02, -0.03)$, $(0.09, 0.04)$, $(-0.02, -0.05)$\n\n您的程序必须实现上述过程，并为每个数据集输出一个对$[\\text{Acc}, \\text{Sil}]$，其中$\\text{Acc}$是监督学习的训练准确率，$\\text{Sil}$是最终K-均值聚类的轮廓系数。将每个值表示为保留六位小数的小数。最终输出格式：单行包含一个顶级列表，其中有三对值，按数据集 A、B、C 的顺序排列，条目间用逗号分隔，例如 $[[a_1,s_1],[a_2,s_2],[a_3,s_3]]$，数值保留六位小数。", "solution": "用户提供的问题经评估有效。它在科学上基于机器学习的原理，问题定义良好，具有确定性过程以确保唯一解，并以客观、正式的语言表达。解决问题所需的所有数据和定义都已提供且一致。我现在将着手提供一个完整的解决方案。\n\n该问题要求对三个数据集进行监督学习和非监督学习的比较分析。对于每个数据集，我们必须计算两个指标：一个简单监督分类器的准确率和一个非监督聚类算法的轮廓系数。\n\n一个数据集由 $N$ 个二维空间中的点组成，其中每个点 $x^{(i)} = (x_1^{(i)}, x_2^{(i)}) \\in \\mathbb{R}^2$ 关联一个二元标签 $y^{(i)} \\in \\{0, 1\\}$。\n\n### 监督分类：一维阈值分类器\n\n监督学习任务是找到最优的一维阈值分类器，以最小化训练集上的分类误差。这是经验风险最小化 (ERM) 的一个实例。\n\n对于一个点 $x = (x_1, x_2)$，分类器的预测 $\\hat{y}(x)$ 基于第一个特征 $x_1$ 上的一个阈值 $t$ 和决策边界“左侧”的类别分配 $c_{\\text{left}} \\in \\{0, 1\\}$ 来定义：\n$$\n\\hat{y}(x) = \\begin{cases}\nc_{\\text{left}},  \\text{if } x_1 \\le t, \\\\\n1 - c_{\\text{left}},  \\text{if } x_1 > t.\n\\end{cases}\n$$\n目标是选择能使训练准确率最大化的对 $(t, c_{\\text{left}})$，准确率定义为：\n$$\n\\text{Acc} = \\frac{1}{N}\\sum_{i=1}^N \\mathbf{1}\\left(\\hat{y}(x^{(i)}) = y^{(i)}\\right)\n$$\n其中 $\\mathbf{1}(\\cdot)$ 是指示函数，当其参数为真时为 $1$，否则为 $0$。\n\n为了找到最优分类器，我们必须搜索一组候选阈值 $t$ 和 $c_{\\text{left}}$ 的两种可能值。分类边界只需要在改变数据划分的位置进行检查。设第一个特征的唯一排序值为 $u_1  u_2  \\dots  u_m$。有意义的阈值集合包括这些连续值之间的中点，$t_j = (u_j + u_{j+1})/2$ for $j=1, \\dots, m-1$。此外，为了考虑所有点被分到同一组的情况，我们测试低于最小 $x_1$ 值（例如 $u_1 - 1$）和高于最大 $x_1$ 值（例如 $u_m + 1$）的阈值。\n\n对于每个候选阈值 $t$，我们评估 $c_{\\text{left}} = 0$ 和 $c_{\\text{left}} = 1$ 两种情况下的准确率。我们选择产生最高准确率的对 $(t, c_{\\text{left}})$。问题指定了一个平局打破规则：在准确率相同的情况下，我们必须选择 $c_{\\text{left}} = 0$ 的分类器。如果仍然平局，则选择阈值 $t$ 最小的那个。\n\n### 非监督聚类：K-均值与轮廓系数\n\n非监督任务是使用 K-均值算法将数据点 $x^{(i)}$ 划分为 $k=2$ 个簇，然后使用轮廓系数评估该划分的质量。\n\n**K-均值算法**\n算法过程如下：\n1.  **初始化**：两个质心 $\\mu_1$ 和 $\\mu_2$ 被确定性地初始化为第一个特征 $x_1$ 值分别为最小和最大的数据点。\n2.  **迭代**：算法在两个步骤之间迭代，直到簇分配稳定或达到最多 $100$ 次迭代。\n    a.  **分配步骤**：根据欧几里得距离，将每个数据点 $x^{(i)}$ 分配给最近的质心所对应的簇：$C_j \\leftarrow \\{x^{(i)} \\mid \\|x^{(i)} - \\mu_j\\|_2 \\le \\|x^{(i)} - \\mu_l\\|_2 \\text{ for all } l=1, \\dots, k\\}$。\n    b.  **更新步骤**：每个质心 $\\mu_j$ 被重新计算为其簇中所有点的均值：$\\mu_j \\leftarrow \\frac{1}{|C_j|} \\sum_{x \\in C_j} x$。\n3.  **空簇处理**：如果在更新步骤中某个簇 $C_j$ 变为空，则重新初始化其质心。新的质心被设置为整个数据集中距离另一个（非空）簇的质心最远（欧几里得距离）的数据点。\n\n**轮廓系数**\nK-均值算法收敛后，使用轮廓系数评估所得到的聚类结果。对于每个点 $x^{(i)}$，我们计算：\n-   $a(i)$：$x^{(i)}$ 与其自身簇内所有其他点的平均欧几里得距离。如果簇中只有一个点，则 $a(i) = 0$。\n-   $b(i)$：$x^{(i)}$ 与最近邻簇中所有点的平均欧几里得距离。由于 $k=2$，这即是与另一个簇中所有点的平均距离。\n\n点 $i$ 的轮廓值为：\n$$\ns(i) = \\begin{cases}\n\\frac{b(i) - a(i)}{\\max\\{a(i), b(i)\\}},  \\text{if } \\max\\{a(i), b(i)\\} > 0, \\\\\n0,  \\text{otherwise}.\n\\end{cases}\n$$\n整个聚类的轮廓系数是所有数据点轮廓值 $s(i)$ 的均值。接近 $1$ 的分数表示分离良好的簇，接近 $0$ 的分数表示重叠的簇，负分则表明点可能被分配到了错误的簇。\n\n### 应用于数据集\n\n将所描述的过程应用于提供的三个数据集中的每一个。对于每个数据集，我们首先构建特征矩阵 $X$ 和标签向量 $y$。然后，我们执行监督学习的准确率计算和非监督学习的 K-均值/轮廓系数计算。为每个数据集计算得到的值对 $[\\text{Acc}, \\text{Sil}]$，每个值四舍五入到六位小数。\n\n- **数据集 A** 呈现了一个沿 $x_1$ 轴有中度分离但存在一些重叠的场景，这使得监督任务并非易事。非监督聚将取决于点云的几何结构。\n- **数据集 B** 的设计使得标签沿 $x_1$ 轴是完美可分的，但沿 $x_2$ 轴的巨大方差起到了干扰作用。这测试了非监督算法是否会被在欧几里得距离度量中占主导地位的噪声特征 $x_2$ 所误导。\n- **数据集 C** 的两个类别的分布严重重叠。预计监督和非监督方法都将表现不佳，导致低准确率（接近随机猜测的 $0.5$）和低轮廓系数（接近 $0$）。\n\n最终输出是一个列表，按顺序包含数据集 A、B 和 C 的 $[\\text{Acc}, \\text{Sil}]$ 对。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef _calculate_accuracy(X, y):\n    \"\"\"\n    Calculates the maximum accuracy of a 1D threshold classifier on x1.\n    \"\"\"\n    x1 = X[:, 0]\n    \n    unique_x1 = np.unique(x1)\n    \n    thresholds = []\n    if len(unique_x1) > 1:\n        # Midpoints between unique consecutive sorted x1 values\n        thresholds.extend((unique_x1[:-1] + unique_x1[1:]) / 2)\n    # Add thresholds to classify all points into one group\n    thresholds.append(unique_x1[0] - 1.0)\n    thresholds.append(unique_x1[-1] + 1.0)\n    \n    # Use a tuple (acc, c_left, t) for lexicographical comparison\n    # Goal: max(acc), min(c_left), min(t)\n    # This is equivalent to finding max of (acc, 1-c_left, -t)\n    best_params = (-1.0, 2, float('inf')) # (acc, c_left, t)\n\n    for t in sorted(list(set(thresholds))):\n        for c_left in [0, 1]:\n            c_right = 1 - c_left\n            y_pred = np.where(x1 <= t, c_left, c_right)\n            acc = np.mean(y_pred == y)\n            \n            # Tie-breaking logic\n            current_params = (acc, c_left, t)\n            if current_params[0] > best_params[0]:\n                best_params = current_params\n            elif current_params[0] == best_params[0]:\n                if current_params[1] < best_params[1]:\n                    best_params = current_params\n                elif current_params[1] == best_params[1] and current_params[2] < best_params[2]:\n                    best_params = current_params\n                    \n    return best_params[0]\n\ndef _calculate_silhouette(X):\n    \"\"\"\n    Performs K-Means clustering (k=2) and calculates the silhouette score.\n    \"\"\"\n    N, D = X.shape\n    k = 2\n\n    if N < k:\n        return 0.0\n\n    # 1. Deterministic Initialization\n    idx_min_x1 = np.argmin(X[:, 0])\n    idx_max_x1 = np.argmax(X[:, 0])\n    \n    if idx_min_x1 == idx_max_x1:\n        # Pathological case not present in test data but good practice to handle.\n        # If all x1 are same, initialization would give one centroid.\n        # For this problem's scope, we assume this does not happen.\n        if N > 1:\n             idx_max_x1 = (idx_min_x1 + 1) % N # Pick another point\n        else:\n             return 0.0 # Single point has 0 silhouette score.\n    \n    centroids = np.array([X[idx_min_x1], X[idx_max_x1]], dtype=np.float64)\n    \n    assignments = np.zeros(N, dtype=int)\n    \n    for _ in range(100):\n        # 2. Assignment step (vectorized)\n        distances = np.linalg.norm(X[:, np.newaxis, :] - centroids[np.newaxis, :, :], axis=2)\n        new_assignments = np.argmin(distances, axis=1)\n            \n        # 3. Check for convergence\n        if np.array_equal(assignments, new_assignments):\n            break\n        assignments = new_assignments\n        \n        # 4. Update step with empty cluster handling\n        new_centroids = np.zeros_like(centroids)\n        cluster_sizes = np.bincount(assignments, minlength=k)\n\n        # First, compute centroids for non-empty clusters\n        non_empty_updated = [False, False]\n        for j in range(k):\n            if cluster_sizes[j] > 0:\n                new_centroids[j] = np.mean(X[assignments == j], axis=0)\n                non_empty_updated[j] = True\n\n        # Then, handle any empty clusters\n        for j in range(k):\n            if cluster_sizes[j] == 0:\n                other_j = 1 - j\n                # Use the newly computed centroid of the other cluster\n                other_centroid = new_centroids[other_j]\n                \n                # Reinitialize as the point farthest from the other centroid\n                dists_from_other_centroid = np.linalg.norm(X - other_centroid, axis=1)\n                farthest_point_idx = np.argmax(dists_from_other_centroid)\n                new_centroids[j] = X[farthest_point_idx]\n        \n        centroids = new_centroids\n    \n    # After convergence, use the final assignments\n    assignments = new_assignments\n    \n    # 5. Silhouette score calculation\n    if len(np.unique(assignments)) < 2:\n        return 0.0\n\n    silhouette_values = np.zeros(N)\n    for i in range(N):\n        point_i = X[i]\n        cluster_idx = assignments[i]\n        \n        # Intra-cluster distance a(i)\n        mask_a = (assignments == cluster_idx) & (np.arange(N) != i)\n        if not np.any(mask_a):\n            a_i = 0.0\n        else:\n            a_i = np.mean(np.linalg.norm(X[mask_a] - point_i, axis=1))\n\n        # Inter-cluster distance b(i)\n        other_cluster_idx = 1 - cluster_idx\n        mask_b = assignments == other_cluster_idx\n        \n        if not np.any(mask_b): # Should not happen if len(unique(assignments)) > 1\n             b_i = 0.0\n        else:\n             b_i = np.mean(np.linalg.norm(X[mask_b] - point_i, axis=1))\n\n        if max(a_i, b_i) == 0:\n            silhouette_values[i] = 0.0\n        else:\n            silhouette_values[i] = (b_i - a_i) / max(a_i, b_i)\n            \n    return np.mean(silhouette_values)\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Dataset A\n        {\n            \"class_0\": [(-0.4, 0.0), (-0.2, 0.1), (0.2, -0.1), (0.3, 0.05)],\n            \"class_1\": [(0.2, 0.0), (0.6, -0.1), (0.8, 0.1), (1.0, -0.05)]\n        },\n        # Dataset B\n        {\n            \"class_0\": [(-0.4, 5.0), (-0.2, -5.0), (0.2, 7.0), (0.3, -7.0)],\n            \"class_1\": [(1.2, 6.0), (1.6, -6.0), (1.8, 8.0), (2.0, -8.0)]\n        },\n        # Dataset C\n        {\n            \"class_0\": [(-0.1, 0.05), (0.0, -0.02), (0.1, 0.03), (-0.05, -0.04)],\n            \"class_1\": [(-0.08, 0.01), (0.02, -0.03), (0.09, 0.04), (-0.02, -0.05)]\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        points_0 = np.array(case[\"class_0\"])\n        points_1 = np.array(case[\"class_1\"])\n        \n        X = np.concatenate((points_0, points_1), axis=0)\n        y = np.concatenate((np.zeros(len(points_0), dtype=int), np.ones(len(points_1), dtype=int)), axis=0)\n\n        accuracy = _calculate_accuracy(X, y)\n        silhouette = _calculate_silhouette(X)\n        \n        results.append([accuracy, silhouette])\n\n    # Final print statement in the exact required format.\n    results_as_strings = [f\"[{acc:.6f},{sil:.6f}]\" for acc, sil in results]\n    print(f\"[{','.join(results_as_strings)}]\")\n\nsolve()\n```", "id": "3199424"}, {"introduction": "预测蛋白质的二级结构是计算生物学中的一个经典问题。这个练习将让你对比一个复杂的有监督模型——循环神经网络（RNN），和一个简单的无监督模型——K-均值聚类。本问题的巧妙之处在于，它展示了当无监督方法使用一个精心设计的特征时，其性能可以与需要从零开始学习复杂模式的监督模型相媲美。[@problem_id:2432793] 这突显了特征工程的重要性，并引发我们思考模型复杂性与任务目标之间的权衡。", "problem": "给定一个代表蛋白质二级结构分配的二元、逐位预测任务。设氨基酸字母表为包含 $20$ 种单字母代码的标准集合。对于任意氨基酸 $a$，定义一个标量倾向函数 $\\phi(a)$ 如下：\n- 如果 $a \\in \\{\\text{A}, \\text{L}, \\text{M}, \\text{Q}, \\text{E}, \\text{K}, \\text{R}, \\text{H}\\}$（通常倾向于形成螺旋），则 $\\phi(a) = +1$。\n- 如果 $a \\in \\{\\text{V}, \\text{I}, \\text{Y}, \\text{F}, \\text{W}, \\text{T}\\}$（通常倾向于形成折叠片），则 $\\phi(a) = -1$。\n- 否则 $\\phi(a) = 0$（中性集合 $\\{\\text{C}, \\text{D}, \\text{N}, \\text{P}, \\text{G}, \\text{S}\\}$）。\n\n对于一个蛋白质序列 $s = (a_1, a_2, \\dots, a_T)$，通过局部上下文规则，将每个位置 $i \\in \\{1,\\dots,T\\}$ 的真实二级结构标签定义为 $y_i \\in \\{0,1\\}$，其中 $1$ 表示 α-螺旋，$0$ 表示 β-折叠片：\n$$\n\\tilde{z}_i \\;=\\; \\beta_0 + \\beta_1 \\,\\phi(a_i) + \\beta_2 \\,\\phi(a_{i-1}) + \\beta_3 \\,\\phi(a_{i+1}),\n$$\n边界约定为 $\\phi(a_0)=\\phi(a_{T+1})=0$。然后\n$$\ny_i \\;=\\; \\begin{cases}\n1  \\text{if } \\tilde{z}_i \\ge 0,\\\\\n0  \\text{otherwise}.\n\\end{cases}\n$$\n使用固定系数 $\\beta_0 = 0$，$\\beta_1 = 1$，$\\beta_2 = 0.5$，$\\beta_3 = 0.5$。\n\n考虑一个监督模型，该模型通过一个循环神经网络（RNN）将 $a_i$ 的独热编码 $x_i \\in \\{0,1\\}^{20}$ 映射到一个隐藏状态 $h_i \\in \\mathbb{R}^d$ 和一个输出 $\\hat{y}_i \\in (0,1)$。该 RNN 定义如下。设 $h_0 = 0 \\in \\mathbb{R}^d$，则对于 $i=1,\\dots,T$，\n$$\nh_i \\;=\\; \\tanh\\!\\left(W_x x_i + W_h h_{i-1} + b\\right), \\quad s_i \\;=\\; U h_i + c, \\quad \\hat{y}_i \\;=\\; \\sigma(s_i),\n$$\n其中 $\\sigma(u) = \\frac{1}{1+e^{-u}}$，$W_x \\in \\mathbb{R}^{d \\times 20}$，$W_h \\in \\mathbb{R}^{d \\times d}$，$b \\in \\mathbb{R}^d$，$U \\in \\mathbb{R}^{1 \\times d}$，以及 $c \\in \\mathbb{R}$。在一组带标签的序列上，监督学习的目标是带有 $\\ell_2$-正则化的平均二元交叉熵，\n$$\n\\mathcal{L} \\;=\\; \\frac{1}{N}\\sum_{n=1}^N \\frac{1}{T_n} \\sum_{i=1}^{T_n} \\left[-y_i^{(n)} \\log \\hat{y}_i^{(n)} - \\left(1-y_i^{(n)}\\right)\\log\\left(1-\\hat{y}_i^{(n)}\\right)\\right] \\;+\\; \\lambda \\left(\\lVert W_x\\rVert_F^2 + \\lVert W_h\\rVert_F^2 + \\lVert U\\rVert_F^2\\right),\n$$\n正则化系数为 $\\lambda = 10^{-4}$，隐藏维度为 $d=6$。\n\n同时考虑一个无监督基线模型，该模型忽略标签，并使用 $k=2$ 的 $k$-means 算法在一维特征空间中进行聚类。对于任意位置$i$，定义标量特征\n$f_i \\;=\\; \\phi(a_i) + 0.5\\,\\phi(a_{i-1}) + 0.5\\,\\phi(a_{i+1})$,\n使用相同的边界约定。聚类产生两个质心$m_1, m_2 \\in \\mathbb{R}$；将质心较大的簇分配标签$\\hat{y}^{\\text{unsup}}_i = 1$，另一个簇分配标签$\\hat{y}^{\\text{unsup}}_i = 0$。在评估时，位置$i$被分配给欧几里得距离最近的质心。\n\n训练和评估数据的规定如下。监督模型必须仅在训练集序列上进行训练，使用上面定义的标签$y_i$。无监督模型必须仅在从训练序列构建的训练数据特征$\\{f_i\\}$上进行拟合。然后，两个模型都必须在下面的测试集序列上进行评估，通过计算相对于上面定义的真实标签$y_i$的正确预测标签的比例（准确率）。\n\n训练集序列：\n- S-train-1: \"ALMEKRALMEKRAALMQEKRHALMEKRA\"\n- S-train-2: \"VIVTWIYVIVTWYVIVTWIYVIVTWI\"\n- S-train-3: \"ALMEKRGPGSNNDCALMEKRGPGS\"\n- S-train-4: \"GPGSNDNCGPGSGPGSALVIALVIALVIA\"\n- S-train-5: \"ALMEKRALVIALMEKRVIYTWALMEKRV\"\n\n测试集序列：\n- S1 (general mixed case): \"ALMEKRVIVTWTALMEQKRVIV\"\n- S2 (boundary length-$1$ case): \"A\"\n- S3 (all helix-favoring bias): \"ALMEKRAALMEKR\"\n- S4 (all sheet-favoring bias): \"VIVTWIYVIVTWI\"\n\n您的程序必须实现以下功能并产生指定的输出：\n- 在训练集上训练上述监督循环模型，然后对每个测试序列计算准确率\n$$\nA_{\\text{sup}}(\\text{S}j) \\;=\\; \\frac{1}{T_j} \\sum_{i=1}^{T_j} \\mathbb{I}\\left[\\mathbb{I}\\left(\\hat{y}_i^{(\\text{S}j)} \\ge 0.5\\right) = y_i^{(\\text{S}j)}\\right],\n$$\n对于 $j \\in \\{1,2,3,4\\}$，其中 $T_j$ 是序列 $\\text{S}j$ 的长度。\n- 在训练特征$\\{f_i\\}$上拟合无监督基线模型，然后对每个测试序列计算准确率\n$$\nA_{\\text{unsup}}(\\text{S}j) \\;=\\; \\frac{1}{T_j} \\sum_{i=1}^{T_j} \\mathbb{I}\\left[\\hat{y}^{\\text{unsup}}_i(\\text{S}j) = y_i^{(\\text{S}j)}\\right].\n$$\n\n最终输出格式：您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表。该列表必须按以下顺序包含 $8$ 个浮点数：\n$$\n\\big[ A_{\\text{sup}}(\\text{S}1),\\, A_{\\text{unsup}}(\\text{S}1),\\, A_{\\text{sup}}(\\text{S}2),\\, A_{\\text{unsup}}(\\text{S}2),\\, A_{\\text{sup}}(\\text{S}3),\\, A_{\\text{unsup}}(\\text{S}3),\\, A_{\\text{sup}}(\\text{S}4),\\, A_{\\text{unsup}}(\\text{S}4) \\big].\n$$\n所有准确率必须表示为 $[0,1]$ 范围内的十进制小数，不带百分号。程序不得读取任何输入，并且必须仅使用上面指定的训练和测试序列。", "solution": "所呈现的问题陈述是生物信息学中一个明确定义的计算任务，它在一个简化的蛋白质二级结构预测问题上，比较了一个监督学习模型和一个无监督基线模型。所有组成部分——数据、模型、参数和评估指标——都已足够清晰地指定。该问题具有科学依据、逻辑一致且结构严谨。因此，它被认为是有效的。\n\n解决方案将分三个主要阶段进行开发：\n1.  实现真实标签和特征生成逻辑。\n2.  实现和评估无监督 $k$-means 聚类模型。\n3.  实现、训练和评估监督循环神经网络（RNN）模型。\n\n**1. 真实数据生成**\n\n这个问题的基础是生成真实标签 $y_i \\in \\{0, 1\\}$ 的确定性规则。此规则依赖于每个氨基酸 $a$ 的标量倾向函数 $\\phi(a)$。\n-   对于倾向于形成螺旋的残基 $\\{\\text{A}, \\text{L}, \\text{M}, \\text{Q}, \\text{E}, \\text{K}, \\text{R}, \\text{H}\\}$，$\\phi(a) = +1$。\n-   对于倾向于形成折叠片的残基 $\\{\\text{V}, \\text{I}, \\text{Y}, \\text{F}, \\text{W}, \\text{T}\\}$，$\\phi(a) = -1$。\n-   对于中性残基 $\\{\\text{C}, \\text{D}, \\text{N}, \\text{P}, \\text{G}, \\text{S}\\}$，$\\phi(a) = 0$。\n\n对于长度为 $T$ 的蛋白质序列，每个位置 $i$ 的实值分数 $\\tilde{z}_i$ 是一个大小为 $3$ 的局部窗口内倾向性的线性组合：\n$$\n\\tilde{z}_i = \\beta_0 + \\beta_1 \\phi(a_i) + \\beta_2 \\phi(a_{i-1}) + \\beta_3 \\phi(a_{i+1})\n$$\n使用给定的系数 $\\beta_0 = 0$, $\\beta_1 = 1$, $\\beta_2 = 0.5$, $\\beta_3 = 0.5$ 和边界条件 $\\phi(a_0) = \\phi(a_{T+1}) = 0$，这可以简化为：\n$$\n\\tilde{z}_i = \\phi(a_i) + 0.5 \\phi(a_{i-1}) + 0.5 \\phi(a_{i+1})\n$$\n然后，二元真实标签 $y_i$ 由 $\\tilde{z}_i$ 的符号确定：\n$$\ny_i = \\begin{cases} 1  \\text{if } \\tilde{z}_i \\ge 0 \\\\ 0  \\text{if } \\tilde{z}_i  0 \\end{cases}\n$$\n此逻辑将被封装在一个函数中，用于为任何给定的氨基酸序列生成标签。\n\n**2. 无监督基线模型**\n\n无监督模型基于 $k=2$ 的 $k$-means 聚类。关键的是，用于在位置 $i$ 进行聚类的特征定义为：\n$$\nf_i = \\phi(a_i) + 0.5 \\phi(a_{i-1}) + 0.5 \\phi(a_{i+1})\n$$\n这与用于生成真实分数的 $\\tilde{z}_i$ 相同。该模型按以下方式运行：\n-   **拟合：** 从所有训练序列中计算出所有特征的集合 $\\{f_i\\}$。将一维 $k$-means 算法应用于此数据集，以找到两个簇质心 $m_1$ 和 $m_2$。\n-   **标签约定：** 具有较大质心的簇（例如 $\\max(m_1, m_2)$）被分配标签 $1$（螺旋），另一个被分配标签 $0$（折叠片）。\n-   **预测：** 对于来自测试序列的特征 $f_j$，预测 $\\hat{y}^{\\text{unsup}}_j$ 是通过将 $f_j$ 分配给具有最近质心的簇来确定的。如果我们假设 $m_1  m_2$，则决策边界位于 $\\frac{m_1 + m_2}{2}$。如果 $f_j > \\frac{m_1 + m_2}{2}$，则位置 $j$ 被预测为螺旋（标签 $1$），否则预测为折叠片（标签 $0$）。\n\n该模型的准确性取决于数据驱动的决策边界 $\\frac{m_1 + m_2}{2}$ 与固定在 $0$ 的真实边界的比较情况。由于特征 $f_i$ 与真实分数 $\\tilde{z}_i$ 相同，用于聚类的训练数据自然地分为一组非负值（真实标签 $1$）和一组负值（真实标签 $0$）。$k$-means 算法预期会找到一个正质心和一个负质心，从而导致决策边界接近 $0$，因此准确率很高。\n\n**3. 监督 RNN 模型**\n\n监督模型是一个标准的循环神经网络（RNN）。\n-   **架构：**\n    $$\n    h_i = \\tanh(W_x x_i + W_h h_{i-1} + b) \\\\\n    \\hat{y}_i = \\sigma(U h_i + c)\n    $$\n    输入 $x_i \\in \\{0, 1\\}^{20}$ 是氨基酸 $a_i$ 的独热编码。隐藏维度为 $d=6$。\n-   **局限性：** 这是一个单向 RNN，意味着状态 $h_i$ 和预测 $\\hat{y}_i$ 仅依赖于到位置 $i$ 为止的输入序列 $(a_1, \\dots, a_i)$。然而，真实标签 $y_i$ 依赖于未来的氨基酸 $a_{i+1}$。这种架构上的不匹配意味着该模型原则上无法在任意序列上完美复制真实函数。它只能通过从训练数据中学习统计模式来成功，这些模式使其能够基于历史 $(a_1, \\dots, a_i)$ 来“预测” $a_{i+1}$。\n-   **训练：** 模型参数 ($W_x, W_h, b, U, c$) 通过最小化训练集上的正则化二元交叉熵损失函数进行优化。\n    $$\n    \\mathcal{L} = \\text{BCE} + \\lambda \\cdot (\\lVert W_x\\rVert_F^2 + \\lVert W_h\\rVert_F^2 + \\lVert U\\rVert_F^2)\n    $$\n    优化是使用随机梯度下降法进行的，梯度通过时间反向传播（BPTT）算法计算。正则化系数为 $\\lambda = 10^{-4}$。我们将使用固定的学习率和设定的训练轮数，这些是此类过程的标准超参数。\n-   **预测：** 训练后，如果模型输出 $\\hat{y}_i \\ge 0.5$，则预测标签为 $1$，否则为 $0$。\n\n**4. 实现与评估**\n\n程序将实现这两种模型。对于无监督模型，一个简单的迭代 $k$-means 算法就足够了。对于监督模型，我们将实现前向传播和 BPTT 来训练网络。然后，两个训练好的模型将在四个测试序列上进行评估，以计算它们各自的准确率，准确率定义为正确预测标签的比例。最终输出将是按指定顺序排列的这八个准确率值的列表。RNN 的参数将随机初始化，训练过程具有固有的随机性。然而，考虑到真实标签的确定性以及问题的小规模，训练过程预计将收敛到一个稳定且具有代表性的解。", "answer": "```python\nimport numpy as np\nfrom scipy.special import expit as sigmoid\n\ndef solve():\n    \"\"\"\n    Implements and evaluates supervised and unsupervised models for a protein secondary structure task.\n    \"\"\"\n    \n    # ------------------- PROBLEM DEFINITION -------------------\n    \n    AMINO_ACIDS = \"ACDEFGHIKLMNPQRSTVWY\"\n    AA_TO_IX = {aa: i for i, aa in enumerate(AMINO_ACIDS)}\n    VOCAB_SIZE = len(AMINO_ACIDS)\n\n    HELIX_PREF = set(\"ALMQEKRH\")\n    SHEET_PREF = set(\"VIYFWT\")\n\n    BETA_0, BETA_1, BETA_2, BETA_3 = 0.0, 1.0, 0.5, 0.5\n    \n    D_HIDDEN = 6\n    LAMBDA_REG = 1e-4\n    LEARNING_RATE = 0.01\n    EPOCHS = 1000\n    \n    # Using a fixed seed for reproducibility of RNN initialization and training.\n    np.random.seed(42)\n\n    TRAIN_SEQS = [\n        \"ALMEKRALMEKRAALMQEKRHALMEKRA\",\n        \"VIVTWIYVIVTWYVIVTWIYVIVTWI\",\n        \"ALMEKRGPGSNNDCALMEKRGPGS\",\n        \"GPGSNDNCGPGSGPGSALVIALVIALVIA\",\n        \"ALMEKRALVIALMEKRVIYTWALMEKRV\",\n    ]\n    \n    TEST_SUITE = {\n        \"S1\": \"ALMEKRVIVTWTALMEQKRVIV\",\n        \"S2\": \"A\",\n        \"S3\": \"ALMEKRAALMEKR\",\n        \"S4\": \"VIVTWIYVIVTWI\",\n    }\n    \n    # ------------------- HELPER FUNCTIONS -------------------\n\n    def get_propensity(aa):\n        if aa in HELIX_PREF: return 1.0\n        if aa in SHEET_PREF: return -1.0\n        return 0.0\n\n    def generate_truth(sequence):\n        T = len(sequence)\n        propensities = [get_propensity(aa) for aa in sequence]\n        z_tilde = np.zeros(T)\n        for i in range(T):\n            phi_i = propensities[i]\n            phi_prev = propensities[i-1] if i > 0 else 0.0\n            phi_next = propensities[i+1] if i < T - 1 else 0.0\n            z_tilde[i] = BETA_0 + BETA_1 * phi_i + BETA_2 * phi_prev + BETA_3 * phi_next\n        \n        labels = (z_tilde >= 0).astype(int)\n        features = z_tilde\n        return features, labels\n\n    def one_hot_encode(sequence):\n        T = len(sequence)\n        x = np.zeros((T, VOCAB_SIZE))\n        for i, aa in enumerate(sequence):\n            x[i, AA_TO_IX[aa]] = 1\n        return x\n\n    # ------------------- UNSUPERVISED MODEL -------------------\n    \n    class KMeans1D:\n        def __init__(self, k=2):\n            self.k = k\n            self.centroids = None\n            self.cluster_labels = None\n\n        def fit(self, data):\n            # Initialize centroids\n            centroids = np.random.choice(np.unique(data), self.k, replace=False)\n            \n            for _ in range(100): # Max iterations\n                clusters = [[] for _ in range(self.k)]\n                for point in data:\n                    distances = [np.abs(point - c) for c in centroids]\n                    closest_idx = np.argmin(distances)\n                    clusters[closest_idx].append(point)\n                \n                new_centroids = np.array([np.mean(c) if c else centroids[i] for i, c in enumerate(clusters)])\n                \n                if np.all(new_centroids == centroids):\n                    break\n                centroids = new_centroids\n            \n            self.centroids = centroids\n            # Assign label 1 to larger centroid, 0 to smaller\n            self.cluster_labels = (self.centroids == np.max(self.centroids)).astype(int)\n\n        def predict(self, data):\n            preds = []\n            for point in data:\n                distances = [np.abs(point - c) for c in self.centroids]\n                closest_idx = np.argmin(distances)\n                preds.append(self.cluster_labels[closest_idx])\n            return np.array(preds)\n            \n    # ------------------- SUPERVISED MODEL (RNN) -------------------\n    \n    class SimpleRNN:\n        def __init__(self, input_dim, hidden_dim, output_dim, reg_lambda):\n            self.input_dim = input_dim\n            self.hidden_dim = hidden_dim\n            self.reg_lambda = reg_lambda\n\n            # Xavier/Glorot initialization\n            self.Wx = np.random.randn(hidden_dim, input_dim) * np.sqrt(1.0 / input_dim)\n            self.Wh = np.random.randn(hidden_dim, hidden_dim) * np.sqrt(1.0 / hidden_dim)\n            self.b = np.zeros(hidden_dim)\n            self.U = np.random.randn(output_dim, hidden_dim) * np.sqrt(1.0 / hidden_dim)\n            self.c = np.zeros(output_dim)\n\n        def forward(self, x_seq):\n            T = x_seq.shape[0]\n            h = np.zeros((T + 1, self.hidden_dim))\n            s = np.zeros(T)\n            y_hat = np.zeros(T)\n            \n            for t in range(T):\n                h[t+1] = np.tanh(self.Wx @ x_seq[t] + self.Wh @ h[t] + self.b)\n                s[t] = (self.U @ h[t+1] + self.c).item()\n                y_hat[t] = sigmoid(s[t])\n            return h, s, y_hat\n        \n        def train_step(self, x_seq, y_seq, learning_rate):\n            T = x_seq.shape[0]\n            \n            # Forward pass\n            h, s, y_hat = self.forward(x_seq)\n\n            # --- Backward pass (BPTT) ---\n            # Initialize gradients\n            d_Wx, d_Wh, d_b = np.zeros_like(self.Wx), np.zeros_like(self.Wh), np.zeros_like(self.b)\n            d_U, d_c = np.zeros_like(self.U), np.zeros_like(self.c)\n            \n            # Gradient of loss w.r.t. pre-sigmoid output\n            d_s = y_hat - y_seq\n            \n            d_h_next = np.zeros(self.hidden_dim)\n            \n            for t in reversed(range(T)):\n                # Output layer gradients\n                d_U += d_s[t] * h[t+1].reshape(1, -1)\n                d_c += d_s[t]\n                \n                # Propagate gradient back to hidden state\n                d_h = d_s[t] * self.U.flatten() + d_h_next\n                \n                # Propagate through tanh non-linearity\n                d_tanh = d_h * (1 - h[t+1]**2)\n                \n                # Recurrent layer gradients\n                d_b += d_tanh\n                d_Wh += np.outer(d_tanh, h[t])\n                d_Wx += np.outer(d_tanh, x_seq[t])\n                \n                # Pass gradient to previous time step\n                d_h_next = d_tanh @ self.Wh\n            \n            # Add L2 regularization gradients\n            d_Wx += 2 * self.reg_lambda * self.Wx\n            d_Wh += 2 * self.reg_lambda * self.Wh\n            d_U += 2 * self.reg_lambda * self.U\n\n            # Update parameters\n            self.Wx -= learning_rate * d_Wx\n            self.Wh -= learning_rate * d_Wh\n            self.b -= learning_rate * d_b\n            self.U -= learning_rate * d_U\n            self.c -= learning_rate * d_c\n    \n        def predict(self, x_seq):\n            _, _, y_hat = self.forward(x_seq)\n            return (y_hat >= 0.5).astype(int)\n\n    # ------------------- MAIN EXECUTION LOGIC -------------------\n\n    # 1. Prepare training data\n    train_features_list = []\n    train_labels_list = []\n    train_onehot_list = []\n    \n    for seq in TRAIN_SEQS:\n        features, labels = generate_truth(seq)\n        train_features_list.append(features)\n        train_labels_list.append(labels)\n        train_onehot_list.append(one_hot_encode(seq))\n\n    all_train_features = np.concatenate(train_features_list)\n\n    # 2. Fit and evaluate unsupervised model\n    unsup_model = KMeans1D()\n    unsup_model.fit(all_train_features)\n    \n    unsup_accuracies = {}\n    for name, seq in TEST_SUITE.items():\n        test_features, test_labels = generate_truth(seq)\n        preds = unsup_model.predict(test_features)\n        acc = np.mean(preds == test_labels) if len(test_labels) > 0 else 1.0\n        unsup_accuracies[name] = acc\n        \n    # 3. Train and evaluate supervised model\n    rnn = SimpleRNN(\n        input_dim=VOCAB_SIZE, \n        hidden_dim=D_HIDDEN, \n        output_dim=1,\n        reg_lambda=LAMBDA_REG\n    )\n\n    for epoch in range(EPOCHS):\n        indices = np.random.permutation(len(TRAIN_SEQS))\n        for i in indices:\n            x_seq = train_onehot_list[i]\n            y_seq = train_labels_list[i]\n            rnn.train_step(x_seq, y_seq, LEARNING_RATE)\n\n    sup_accuracies = {}\n    for name, seq in TEST_SUITE.items():\n        x_seq = one_hot_encode(seq)\n        _, test_labels = generate_truth(seq)\n        preds = rnn.predict(x_seq)\n        acc = np.mean(preds == test_labels) if len(test_labels) > 0 else 1.0\n        sup_accuracies[name] = acc\n\n    # 4. Assemble and print results\n    results = []\n    for name in sorted(TEST_SUITE.keys()):\n        results.append(sup_accuracies[name])\n        results.append(unsup_accuracies[name])\n    \n    print(f\"[{','.join(f'{r:.12f}' for r in results)}]\")\n\n\nsolve()\n```", "id": "2432793"}]}