{"hands_on_practices": [{"introduction": "为了有效地控制过拟合，我们不能仅仅停留在讨论“模型复杂度”的抽象层面，而需要对其进行量化。本练习将引导你运用“有效自由度”（$d_{\\text{eff}}$）这一强大工具，来量化核岭回归等非参数模型的实际复杂度。通过亲手实现这个概念，你将具体地理解正则化超参数——正则化系数 $\\lambda$ 和核函数尺度 $\\ell$——是如何直接控制模型的灵活性与学习能力的。[@problem_id:3168571]", "problem": "给定一个一维训练设计，其输入 $\\mathbf{x} \\in \\mathbb{R}^{n}$ 由闭区间 $\\left[0,1\\right]$ 上的 $n=20$ 个等距点定义，具体为 $x_i = \\dfrac{i-1}{n-1}$，其中 $i \\in \\{1,2,\\dots,n\\}$。考虑使用径向基函数 (Radial Basis Function, RBF) 核的核岭回归 (Kernel Ridge Regression, KRR)。长度尺度为 $\\ell>0$ 的 RBF 核定义为 $k_{\\ell}(x,x') = \\exp\\left(-\\dfrac{\\|x-x'\\|^2}{2\\ell^2}\\right)$，相关的格拉姆矩阵 (Gram matrix) $\\mathbf{K} \\in \\mathbb{R}^{n \\times n}$ 的元素为 $K_{ij} = k_{\\ell}(x_i,x_j)$。KRR 通过在再生核希尔伯特空间 (Reproducing Kernel Hilbert Space) 上最小化吉洪诺夫正则化 (Tikhonov-regularized) 的平方损失来估计响应，其正则化参数为 $\\lambda \\ge 0$。\n\n从基本定义出发，推导将训练输出 $\\mathbf{y} \\in \\mathbb{R}^n$ 映射到 KRR 拟合值 $\\hat{\\mathbf{y}} \\in \\mathbb{R}^n$ 的线性平滑算子 (linear smoother)，并用它将有效自由度 (effective degrees of freedom) $d_{\\mathrm{eff}}$ 定义为该平滑算子的迹。您的程序必须为给定的 $\\lambda$ 和 $\\ell$ 以及上述固定的设计 $\\mathbf{x}$，实现一种数值稳定的 $d_{\\mathrm{eff}}$ 计算方法，且不执行显式矩阵求逆。\n\n通过报告以下 $(\\lambda,\\ell)$ 对测试套件的 $d_{\\mathrm{eff}}$ 值来展示过拟合控制：\n$(10^{-6}, \\, 0.05)$、\n$(10^{-6}, \\, 5.0)$、\n$(10^{-1}, \\, 0.2)$、\n$(10^{-1}, \\, 1.0)$、\n$(10^{1}, \\, 0.05)$、\n$(10^{1}, \\, 5.0)$。\n\n您的程序应使用固定的 $\\mathbf{x}$ 和指定的 RBF 核计算每个测试用例的 $d_{\\mathrm{eff}}$，并生成单行输出，其中包含用方括号括起来的逗号分隔列表形式的结果，例如 $\\left[\\text{result}_1,\\text{result}_2,\\dots\\right]$。每个 $d_{\\mathrm{eff}}$ 必须打印为四舍五入到 $6$ 位小数的十进制数。不涉及物理单位，也不使用角度。", "solution": "用户希望我解决以下问题：\n给定一个一维训练设计，其输入 $\\mathbf{x} \\in \\mathbb{R}^{n}$ 由闭区间 $\\left[0,1\\right]$ 上的 $n=20$ 个等距点定义，具体为 $x_i = \\dfrac{i-1}{n-1}$，其中 $i \\in \\{1,2,\\dots,n\\}$。考虑使用径向基函数 (Radial Basis Function, RBF) 核的核岭回归 (Kernel Ridge Regression, KRR)。长度尺度为 $\\ell>0$ 的 RBF 核定义为 $k_{\\ell}(x,x') = \\exp\\left(-\\dfrac{\\|x-x'\\|^2}{2\\ell^2}\\right)$，相关的格拉姆矩阵 (Gram matrix) $\\mathbf{K} \\in \\mathbb{R}^{n \\times n}$ 的元素为 $K_{ij} = k_{\\ell}(x_i,x_j)$。KRR 通过在再生核希尔伯特空间 (Reproducing Kernel Hilbert Space) 上最小化吉洪诺夫正则化 (Tikhonov-regularized) 的平方损失来估计响应，其正则化参数为 $\\lambda \\ge 0$。\n\n从基本定义出发，推导将训练输出 $\\mathbf{y} \\in \\mathbb{R}^n$ 映射到 KRR 拟合值 $\\hat{\\mathbf{y}} \\in \\mathbb{R}^n$ 的线性平滑算子 (linear smoother)，并用它将有效自由度 (effective degrees of freedom) $d_{\\mathrm{eff}}$ 定义为该平滑算子的迹。您的程序必须为给定的 $\\lambda$ 和 $\\ell$ 以及上述固定的设计 $\\mathbf{x}$，实现一种数值稳定的 $d_{\\mathrm{eff}}$ 计算方法，且不执行显式矩阵求逆。\n\n通过报告以下 $(\\lambda,\\ell)$ 对测试套件的 $d_{\\mathrm{eff}}$ 值来展示过拟合控制：\n$(10^{-6}, \\, 0.05)$、\n$(10^{-6}, \\, 5.0)$、\n$(10^{-1}, \\, 0.2)$、\n$(10^{-1}, \\, 1.0)$、\n$(10^{1}, \\, 0.05)$、\n$(10^{1}, \\, 5.0)$。\n\n您的程序应使用固定的 $\\mathbf{x}$ 和指定的 RBF 核计算每个测试用例的 $d_{\\mathrm{eff}}$，并生成单行输出，其中包含用方括号括起来的逗号分隔列表形式的结果，例如 $\\left[\\text{result}_1,\\text{result}_2,\\dots\\right]$。每个 $d_{\\mathrm{eff}}$ 必须打印为四舍五入到 $6$ 位小数的十进制数。不涉及物理单位，也不使用角度。\n\n### 问题验证\n\n#### 步骤 1：提取已知条件\n- **领域：** 一维回归，核岭回归 (KRR)。\n- **输入设计：** $\\left[0,1\\right]$ 上的 $n=20$ 个等距点。具体来说，$x_i = \\dfrac{i-1}{n-1}$，其中 $i \\in \\{1,2,\\dots,n\\}$。\n- **核函数：** 径向基函数 (RBF) 核，$k_{\\ell}(x,x') = \\exp\\left(-\\dfrac{\\|x-x'\\|^2}{2\\ell^2}\\right)$，长度尺度为 $\\ell>0$。\n- **格拉姆矩阵：** $\\mathbf{K} \\in \\mathbb{R}^{n \\times n}$，其元素为 $K_{ij} = k_{\\ell}(x_i,x_j)$。\n- **模型：** KRR，吉洪诺夫正则化参数为 $\\lambda \\ge 0$。\n- **任务 1（推导）：** 推导线性平滑算子 $\\mathbf{S}$（其中 $\\hat{\\mathbf{y}} = \\mathbf{S}\\mathbf{y}$），以及有效自由度 $d_{\\mathrm{eff}} = \\mathrm{Tr}(\\mathbf{S})$。\n- **任务 2（计算）：** 为给定的设计 $\\mathbf{x}$ 实现一种数值稳定的方法来计算 $d_{\\mathrm{eff}}$，不进行显式矩阵求逆。\n- **测试用例：** 计算以下 $(\\lambda, \\ell)$ 对的 $d_{\\mathrm{eff}}$：\n  - $(10^{-6}, 0.05)$\n  - $(10^{-6}, 5.0)$\n  - $(10^{-1}, 0.2)$\n  - $(10^{-1}, 1.0)$\n  - $(10^{1}, 0.05)$\n  - $(10^{1}, 5.0)$\n- **输出格式：** 单行输出，包含一个逗号分隔的结果列表，例如 `[result1,result2,...]`，每个结果四舍五入到 6 位小数。\n\n#### 步骤 2：使用提取的已知条件进行验证\n1.  **科学上合理：** 该问题基于核岭回归，这是机器学习和统计学中的一个标准和基本技术。RBF 核、格拉姆矩阵、吉洪诺夫正则化和有效自由度的定义都是标准的且数学上正确的。该问题牢固地植根于已建立的统计学习理论。\n2.  **良构的 (Well-Posed)：** 所有必要的参数（$n$、$\\mathbf{x}$、核函数、$\\lambda$、$\\ell$）都已指定。任务是计算一个明确定义的量 $d_{\\mathrm{eff}}$。对于任何给定的有效参数集 $(\\lambda > 0, \\ell > 0)$，$d_{\\mathrm{eff}}$ 解的存在性和唯一性是有保证的。对于不同的点，RBF 格拉姆矩阵 $\\mathbf{K}$ 是正定的，因此对于 $\\lambda \\ge 0$，$\\mathbf{K} + \\lambda\\mathbf{I}$ 是可逆的。该问题是良构的。\n3.  **客观的：** 该问题使用精确、客观的数学语言陈述。没有主观或基于观点的断言。\n4.  **完整性：** 该问题是自包含的。它提供了完整解决方案所需的所有数据、定义和约束。\n5.  **可行性：** 对于现代硬件来说，$n=20$ 的数值计算在计算上是微不足道的。对数值稳定方法的要求指向了标准、成熟的算法，如特征值分解，这是合适且可行的。\n\n#### 步骤 3：结论与行动\n该问题是有效的，因为它科学上合理、良构、客观且完整。我将继续进行推导和求解。\n\n###\n核岭回归 (KRR) 问题旨在从再生核希尔伯特空间 (RKHS) $\\mathcal{H}_k$ 中找到一个函数 $f$，以最小化正则化的平方损失。对于训练集 $\\{ (x_i, y_i) \\}_{i=1}^n$，目标函数为：\n$$\n\\min_{f \\in \\mathcal{H}_k} \\sum_{i=1}^n (y_i - f(x_i))^2 + \\lambda \\|f\\|_{\\mathcal{H}_k}^2\n$$\n这里，$\\mathbf{y} = (y_1, \\dots, y_n)^T$ 是观测响应向量，$\\lambda \\ge 0$ 是正则化参数。\n\n根据表示定理 (Representer Theorem)，最小化器 $f$ 可以表示为在训练点上求值的核函数的线性组合：\n$$\nf(x) = \\sum_{j=1}^n \\alpha_j k(x, x_j)\n$$\n其中 $\\boldsymbol{\\alpha} = (\\alpha_1, \\dots, \\alpha_n)^T$ 是一个系数向量。平方 RKHS 范数由 $\\|f\\|_{\\mathcal{H}_k}^2 = \\boldsymbol{\\alpha}^T \\mathbf{K} \\boldsymbol{\\alpha}$ 给出，其中 $\\mathbf{K}$ 是格拉姆矩阵，其元素为 $K_{ij} = k(x_i, x_j)$。\n\n在训练点处的函数求值向量 $\\mathbf{f} = (f(x_1), \\dots, f(x_n))^T$ 可以写为 $\\mathbf{f} = \\mathbf{K}\\boldsymbol{\\alpha}$。将这些代入目标函数，我们将问题转化为寻找最优系数 $\\boldsymbol{\\alpha}$：\n$$\n\\min_{\\boldsymbol{\\alpha} \\in \\mathbb{R}^n} \\|\\mathbf{y} - \\mathbf{K}\\boldsymbol{\\alpha}\\|_2^2 + \\lambda \\boldsymbol{\\alpha}^T \\mathbf{K} \\boldsymbol{\\alpha}\n$$\n这是一个二次优化问题。为了找到最小值，我们对目标函数关于 $\\boldsymbol{\\alpha}$ 求导，并令梯度为零。目标函数是 $L(\\boldsymbol{\\alpha}) = (\\mathbf{y} - \\mathbf{K}\\boldsymbol{\\alpha})^T(\\mathbf{y} - \\mathbf{K}\\boldsymbol{\\alpha}) + \\lambda \\boldsymbol{\\alpha}^T \\mathbf{K} \\boldsymbol{\\alpha}$。\n$$\n\\nabla_{\\boldsymbol{\\alpha}} L = -2\\mathbf{K}^T(\\mathbf{y} - \\mathbf{K}\\boldsymbol{\\alpha}) + 2\\lambda \\mathbf{K}\\boldsymbol{\\alpha} = 0\n$$\n由于核是对称的，$\\mathbf{K}^T = \\mathbf{K}$。\n$$\n-\\mathbf{K}\\mathbf{y} + \\mathbf{K}\\mathbf{K}\\boldsymbol{\\alpha} + \\lambda \\mathbf{K}\\boldsymbol{\\alpha} = 0\n$$\n$$\n\\mathbf{K}(\\mathbf{K} + \\lambda\\mathbf{I})\\boldsymbol{\\alpha} = \\mathbf{K}\\mathbf{y}\n$$\n对于不同的点和有效的 RBF 核 ($\\ell>0$)，格拉姆矩阵 $\\mathbf{K}$ 是正定的，因此是可逆的。我们可以乘以 $\\mathbf{K}^{-1}$：\n$$\n(\\mathbf{K} + \\lambda\\mathbf{I})\\boldsymbol{\\alpha} = \\mathbf{y}\n$$\n解出 $\\boldsymbol{\\alpha}$，我们得到：\n$$\n\\boldsymbol{\\alpha} = (\\mathbf{K} + \\lambda\\mathbf{I})^{-1}\\mathbf{y}\n$$\n拟合值 $\\hat{\\mathbf{y}}$ 是在训练点处的预测值，由 $\\hat{\\mathbf{y}} = \\mathbf{f} = \\mathbf{K}\\boldsymbol{\\alpha}$ 给出。代入 $\\boldsymbol{\\alpha}$ 的表达式：\n$$\n\\hat{\\mathbf{y}} = \\mathbf{K}(\\mathbf{K} + \\lambda\\mathbf{I})^{-1}\\mathbf{y}\n$$\n这个方程显示了观测输出 $\\mathbf{y}$ 和拟合值 $\\hat{\\mathbf{y}}$ 之间的线性关系。执行此映射的矩阵称为平滑矩阵 (smoother matrix) $\\mathbf{S}$：\n$$\n\\mathbf{S} = \\mathbf{K}(\\mathbf{K} + \\lambda\\mathbf{I})^{-1}\n$$\n有效自由度 $d_{\\mathrm{eff}}$ 用于衡量拟合模型的复杂度，其定义为平滑矩阵的迹：\n$$\nd_{\\mathrm{eff}} = \\mathrm{Tr}(\\mathbf{S}) = \\mathrm{Tr}\\left(\\mathbf{K}(\\mathbf{K} + \\lambda\\mathbf{I})^{-1}\\right)\n$$\n为了在不进行显式矩阵求逆的情况下计算该量，我们对格拉姆矩阵 $\\mathbf{K}$ 进行谱分解。由于 $\\mathbf{K}$ 是实对称矩阵，它可以被对角化为 $\\mathbf{K} = \\mathbf{U\\Sigma U}^T$，其中 $\\mathbf{U}$ 是一个特征向量的正交矩阵（$\\mathbf{U}\\mathbf{U}^T = \\mathbf{U}^T\\mathbf{U} = \\mathbf{I}$），$\\mathbf{\\Sigma}$ 是相应非负特征值 $\\sigma_i$ 的对角矩阵。\n将此代入 $d_{\\mathrm{eff}}$ 的表达式中：\n$$\nd_{\\mathrm{eff}} = \\mathrm{Tr}\\left( (\\mathbf{U\\Sigma U}^T) (\\mathbf{U\\Sigma U}^T + \\lambda\\mathbf{I})^{-1} \\right) \\\\\n= \\mathrm{Tr}\\left( \\mathbf{U\\Sigma U}^T \\left(\\mathbf{U}(\\mathbf{\\Sigma} + \\lambda\\mathbf{I})\\mathbf{U}^T\\right)^{-1} \\right) \\\\\n= \\mathrm{Tr}\\left( \\mathbf{U\\Sigma U}^T (\\mathbf{U}^T)^{-1} (\\mathbf{\\Sigma} + \\lambda\\mathbf{I})^{-1} \\mathbf{U}^{-1} \\right)\n$$\n使用 $\\mathbf{U}^{-1} = \\mathbf{U}^T$ 和 $(\\mathbf{U}^T)^{-1} = \\mathbf{U}$：\n$$\nd_{\\mathrm{eff}} = \\mathrm{Tr}\\left( \\mathbf{U\\Sigma} (\\mathbf{U}^T \\mathbf{U}) (\\mathbf{\\Sigma} + \\lambda\\mathbf{I})^{-1} \\mathbf{U}^T \\right) = \\mathrm{Tr}\\left( \\mathbf{U}\\mathbf{\\Sigma} (\\mathbf{\\Sigma} + \\lambda\\mathbf{I})^{-1} \\mathbf{U}^T \\right)\n$$\n利用迹的循环特性 $\\mathrm{Tr}(\\mathbf{ABC}) = \\mathrm{Tr}(\\mathbf{CAB})$：\n$$\nd_{\\mathrm{eff}} = \\mathrm{Tr}\\left( \\mathbf{U}^T \\mathbf{U}\\mathbf{\\Sigma} (\\mathbf{\\Sigma} + \\lambda\\mathbf{I})^{-1} \\right) = \\mathrm{Tr}\\left( \\mathbf{\\Sigma} (\\mathbf{\\Sigma} + \\lambda\\mathbf{I})^{-1} \\right)\n$$\n矩阵 $\\mathbf{\\Sigma} (\\mathbf{\\Sigma} + \\lambda\\mathbf{I})^{-1}$ 是对角矩阵，其对角元素为 $\\frac{\\sigma_i}{\\sigma_i + \\lambda}$。迹是这些对角元素的总和：\n$$\nd_{\\mathrm{eff}} = \\sum_{i=1}^n \\frac{\\sigma_i}{\\sigma_i + \\lambda}\n$$\n该公式提供了一种数值稳定的方法来计算 $d_{\\mathrm{eff}}$，即首先求出 $\\mathbf{K}$ 的特征值。实现将为每个 $(\\lambda, \\ell)$ 对构建格拉姆矩阵 $\\mathbf{K}$，计算其特征值，然后应用此公式。\n\n$d_{\\mathrm{eff}}$ 的高值（接近 $n=20$）表示模型过于复杂，可能存在过拟合，因为它使用许多参数来拟合数据。这在 $\\lambda$ 很小时发生。$d_{\\mathrm{eff}}$ 的低值（接近 $0$）表示模型非常简单、高度正则化，可能存在欠拟合。这在 $\\lambda$ 很大时，或者当 $\\ell$ 非常大时（迫使函数接近常数，其复杂度为 $1$）发生。\n\n- 对于 $(\\lambda=10^{-6}, \\ell=0.05)$：小的 $\\lambda$ 和与点间距相当的长度尺度 $\\ell$ 意味着低正则化和复杂模型。我们预期 $d_{\\mathrm{eff}} \\approx 20$。\n- 对于 $(\\lambda=10^{-6}, \\ell=5.0)$：小的 $\\lambda$ 但大的 $\\ell$ 迫使函数非常平滑。格拉姆矩阵接近一个秩为 1 的矩阵，导致有效参数很少。我们预期 $d_{\\mathrm{eff}} \\approx 1$。\n- 对于 $(\\lambda=10^{1}, \\ell=0.05)$：大的 $\\lambda$ 施加强正则化，尽管 $\\ell$ 很小，但仍降低了复杂度。我们预期一个小的 $d_{\\mathrm{eff}}$。\n- 对于 $(\\lambda=10^{1}, \\ell=5.0)$：大的 $\\lambda$ 和大的 $\\ell$ 都促进了模型的简化。我们预期一个非常小的 $d_{\\mathrm{eff}}$。\n\n下面的代码实现了这一逻辑。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the effective degrees of freedom for Kernel Ridge Regression\n    with an RBF kernel for a set of (lambda, ell) parameters.\n    \"\"\"\n    \n    # Define fixed problem parameters\n    n = 20\n    x = np.linspace(0, 1, n)\n\n    # Define the test suite of (lambda, ell) pairs\n    test_cases = [\n        (1e-6, 0.05),\n        (1e-6, 5.0),\n        (1e-1, 0.2),\n        (1e-1, 1.0),\n        (1e+1, 0.05),\n        (1e+1, 5.0)\n    ]\n\n    results = []\n    \n    # Use broadcasting to create a matrix of squared distances\n    # x_col has shape (n, 1), x has shape (n,). Broadcasting creates an (n, n) matrix.\n    x_col = x.reshape(-1, 1)\n    sq_dist_matrix = (x_col - x)**2\n\n    for lmbda, ell in test_cases:\n        # Construct the RBF Gram matrix K\n        # K_ij = exp(-||x_i - x_j||^2 / (2 * ell^2))\n        K = np.exp(-sq_dist_matrix / (2 * ell**2))\n\n        # Compute the eigenvalues of the symmetric matrix K.\n        # np.linalg.eigvalsh is numerically stable and optimized for Hermitian matrices.\n        eigenvalues = np.linalg.eigvalsh(K)\n\n        # Compute the effective degrees of freedom using the derived formula.\n        # d_eff = sum(sigma_i / (sigma_i + lambda))\n        d_eff = np.sum(eigenvalues / (eigenvalues + lmbda))\n        \n        # Round the result to 6 decimal places and store as a string\n        results.append(f\"{d_eff:.6f}\")\n\n    # Print the final results in the specified format\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n\n```", "id": "3168571"}, {"introduction": "正则化的本质是在两个相互竞争的目标之间寻求平衡：一是最小化训练误差，二是保持模型简洁。本实践将标准的岭回归问题重构成一个多目标优化问题，让你能直观地看到这一权衡过程。通过绘制训练误差与正则化惩罚项的二维图像并识别出“帕累托最优”模型集合，你将对模型选择中固有的妥协与折中建立起更深刻的直觉。[@problem_id:3168619]", "problem": "您将通过对带二次正则化的线性模型进行双目标分析来研究偏差-方差权衡。请在以下数学框架内进行研究，该框架基于经验风险最小化（Empirical Risk Minimization）的核心定义：一个带有参数 $w \\in \\mathbb{R}^p$ 的线性预测器通过最小化经验平方误差和与二次正则化惩罚项之和进行训练。对于每种正则化强度，分别考虑两个目标：训练均方误差和正则化惩罚项本身的值。目标是构建一个在一组正则化强度上权衡曲线的离散近似，并识别出帕累托最优模型。\n\n您必须使用的定义：\n- 训练数据矩阵为 $X \\in \\mathbb{R}^{n \\times p}$，响应向量为 $y \\in \\mathbb{R}^n$。\n- 对于 $w \\in \\mathbb{R}^p$，线性模型预测 $\\hat{y} = X w$。\n- 训练均方误差（MSE）为 $E_{\\text{train}}(w) = \\frac{1}{n}\\lVert X w - y \\rVert_2^2$。\n- 对于给定的正则化强度 $\\lambda > 0$，二次（岭）正则化惩罚项的值为 $P_\\lambda(w) = \\lambda \\lVert w \\rVert_2^2$。\n- 对于一个固定的数据集和一组固定的 $\\lambda$ 值，为每个 $\\lambda$ 训练一个独立的模型会得到一组点对 $\\left(E_{\\text{train}}(w_\\lambda), P_\\lambda(w_\\lambda)\\right)$。将这些视为目标空间中的点。\n- 一个目标对为 $(E_i, P_i)$ 的模型 $i$ 被一个目标对为 $(E_j, P_j)$ 的模型 $j$ 帕累托支配，当且仅当 $E_j \\le E_i$ 且 $P_j \\le P_i$，并且两个不等式中至少有一个是严格的。如果一个模型没有被集合中任何其他模型所支配，则该模型是帕累托最优的。\n\n您的程序必须为下面定义的每个测试用例执行以下操作：\n1. 使用所述的生成模型生成一个合成数据集。\n2. 对于测试用例中给定的每个 $\\lambda$，训练正则化线性模型并计算点对 $\\left(E_{\\text{train}}(w_\\lambda), P_\\lambda(w_\\lambda)\\right)$。\n3. 确定帕累托最优模型的索引集合（从零开始，与所列出的 $\\lambda$ 值的顺序对齐）。\n4. 统计有多少模型被支配。\n\n数据生成协议（确定性且科学合理）：\n- 对于给定的种子 $s$、维度 $p$ 和样本量 $n$，从均值为 $0$、方差为 $1$ 的正态分布（表示为 $\\mathcal{N}(0,1)$）中独立抽取设计矩阵 $X \\in \\mathbb{R}^{n \\times p}$ 的元素。抽样必须是独立同分布（i.i.d.）的。\n- 使用相同的种子 $s$（但确定性地推进状态，以使 $X$ 和 $w^\\star$ 不是相同的抽样结果）从 $\\mathcal{N}(0,1)$ 中独立同分布地抽取真实参数向量 $w^\\star \\in \\mathbb{R}^p$ 的元素。\n- 对于指定的 $\\sigma$，从 $\\mathcal{N}(0,\\sigma^2)$ 中独立同分布地抽取噪声 $\\epsilon \\in \\mathbb{R}^n$ 的元素。\n- 构造响应为 $y = X w^\\star + \\epsilon$。\n\n重要的实现说明：\n- 所有正则化强度 $\\lambda$ 都将是严格正数。不要包含 $\\lambda = 0$。\n- 您识别帕累托最优点集的算法必须遵守上述定义。在浮点数运算中，比较实数时请使用一个合理的小容差，以减轻由数值舍入引起的虚假支配，但至少要在一个坐标上保持严格的改进。\n\n覆盖典型和边缘场景的测试套件：\n- 测试用例A（适定问题，低噪声）：\n  - 种子 $s = 7$\n  - 样本量 $n = 40$\n  - 特征数 $p = 6$\n  - 噪声标准差 $\\sigma = 10^{-1}$\n  - 正则化强度（按顺序）：$\\lambda \\in \\{10^{-6}, 10^{-4}, 10^{-2}, 10^{-1}, 1, 10\\}$\n- 测试用例B（欠定问题，极低噪声）：\n  - 种子 $s = 101$\n  - 样本量 $n = 20$\n  - 特征数 $p = 50$\n  - 噪声标准差 $\\sigma = 5 \\times 10^{-2}$\n  - 正则化强度（按顺序）：$\\lambda \\in \\{10^{-6}, 10^{-5}, 10^{-4}, 10^{-3}, 10^{-2}, 10^{-1}, 1\\}$\n- 测试用例C（适定问题，高噪声）：\n  - 种子 $s = 2023$\n  - 样本量 $n = 50$\n  - 特征数 $p = 10$\n  - 噪声标准差 $\\sigma = 1$\n  - 正则化强度（按顺序）：$\\lambda \\in \\{10^{-5}, 10^{-3}, 10^{-2}, 10^{-1}, 1, 10^2\\}$\n\n要求的最终输出格式：\n- 您的程序应生成单行输出，其中包含一个列表，每个测试用例一个元素，顺序为A、B、C。\n- 对于每个测试用例，输出一个包含两个条目的列表：\n  1. 帕累托最优模型的从零开始的索引（整数）列表，按升序排序。\n  2. 一个整数，等于被支配模型的数量。\n- 具体来说，输出必须是以下形式的单行\n  - $[ [\\text{pareto\\_A}, \\text{dominated\\_count\\_A}], [\\text{pareto\\_B}, \\text{dominated\\_count\\_B}], [\\text{pareto\\_C}, \\text{dominated\\_count\\_C}] ]$\n  其中每个 $\\text{pareto\\_X}$ 是一个如指定的整数列表。\n- 输出必须不包含任何单位和附加文本，元素必须用逗号分隔，并完全按照所示用方括号括起来。", "solution": "用户提供的问题已经过分析，被认为是有效的。它科学合理、定义明确、客观，并包含了获得唯一解所需的所有信息。该问题要求从双目标优化视角对岭回归进行分析，其中两个目标是训练均方误差和正则化惩罚项的值。目标是从一组使用不同正则化强度训练的模型中，识别出帕累托最优模型。\n\n解决方案主要分三步：\n1.  推导正则化线性模型参数的解析解。\n2.  阐明数据生成过程和两个目标函数。\n3.  描述在已训练模型中识别帕累托最优集的算法。\n\n**1. 正则化线性回归（岭回归）**\n\n该问题涉及一个线性模型，其中预测响应 $\\hat{y} \\in \\mathbb{R}^n$ 由 $\\hat{y} = Xw$ 给出，其中数据矩阵为 $X \\in \\mathbb{R}^{n \\times p}$，参数向量为 $w \\in \\mathbb{R}^p$。参数 $w$ 是通过最小化一个由残差平方和与二次正则化项（也称为 $\\ell_2$ 范数惩罚）组成的目标函数来确定的。这是岭回归的定义性特征。\n\n需要最小化的目标函数是：\n$$\nL(w) = \\sum_{i=1}^n (x_i^T w - y_i)^2 + \\lambda \\sum_{j=1}^p w_j^2\n$$\n用矩阵表示法，这表示为：\n$$\nL(w) = \\lVert Xw - y \\rVert_2^2 + \\lambda \\lVert w \\rVert_2^2\n$$\n其中 $y \\in \\mathbb{R}^n$ 是真实响应向量，$\\lambda > 0$ 是正则化强度参数。\n\n为了找到最小化 $L(w)$ 的最优参数向量 $w_\\lambda$，我们计算 $L(w)$ 关于 $w$ 的梯度并将其设为零。\n$$\n\\nabla_w L(w) = \\nabla_w \\left( (Xw - y)^T(Xw - y) + \\lambda w^T w \\right)\n$$\n$$\n\\nabla_w L(w) = \\nabla_w \\left( w^T X^T X w - 2y^T X w + y^T y + \\lambda w^T w \\right)\n$$\n使用标准矩阵微积分法则，梯度为：\n$$\n\\nabla_w L(w) = 2 X^T X w - 2 X^T y + 2 \\lambda I w\n$$\n其中 $I$ 是 $p \\times p$ 的单位矩阵。将梯度设为零以求最小值：\n$$\n2 X^T X w - 2 X^T y + 2 \\lambda I w = 0\n$$\n$$\n(X^T X + \\lambda I) w = X^T y\n$$\n由于 $X^T X$ 是一个半正定矩阵，且 $\\lambda$ 是严格正数（$\\lambda > 0$），矩阵 $(X^T X + \\lambda I)$ 是正定的，因此总是可逆的。这确保了对于任何 $\\lambda > 0$，$w$ 都有唯一解，即使在 $n  p$ 的情况下（即 $X^T X$ 是奇异的欠定系统）。\n\n对于给定的 $\\lambda$，参数向量的解是：\n$$\nw_\\lambda = (X^T X + \\lambda I)^{-1} X^T y\n$$\n\n**2. 双目标框架**\n\n该问题将这个单目标最小化问题重构为一个双目标分析。对于每个用特定 $\\lambda$ 训练的模型，我们评估两个不同的目标函数：\n1.  **训练均方误差 ($E_{\\text{train}}$)**：这衡量了模型对训练数据的拟合程度。\n    $$\n    E_{\\text{train}}(w_\\lambda) = \\frac{1}{n} \\lVert Xw_\\lambda - y \\rVert_2^2\n    $$\n2.  **正则化惩罚值 ($P_\\lambda$)**：这是惩罚项本身的值。\n    $$\n    P_\\lambda(w_\\lambda) = \\lambda \\lVert w_\\lambda \\rVert_2^2\n    $$\n对于给定的一组正则化强度中的每个 $\\lambda_k$，我们计算目标值对 $(E_k, P_k) = (E_{\\text{train}}(w_{\\lambda_k}), P_{\\lambda_k}(w_{\\lambda_k}))$。\n\n数据 $(X, y)$ 是基于一个真实模型合成生成的。在给定种子 $s$、样本量 $n$、特征维度 $p$ 和噪声水平 $\\sigma$ 的情况下，该过程是确定性的。\n- 使用种子 $s$ 初始化一个随机数生成器。\n- 数据矩阵 $X \\in \\mathbb{R}^{n \\times p}$ 的元素从 $\\mathcal{N}(0,1)$ 中独立同分布地抽取。\n- 真实参数向量 $w^\\star \\in \\mathbb{R}^p$ 的元素从 $\\mathcal{N}(0,1)$ 中独立同分布地抽取。\n- 噪声向量 $\\epsilon \\in \\mathbb{R}^n$ 的元素从 $\\mathcal{N}(0, \\sigma^2)$ 中独立同分布地抽取。\n- 响应向量构造为 $y = Xw^\\star + \\epsilon$。\n\n**3. 帕累托最优性分析**\n\n有了目标对集合 $\\{(E_k, P_k)\\}$，我们识别出帕累托最优模型。在这个双目标背景下（两个目标都希望最小化），如果一个模型在两个目标上都优于或等于另一个模型，并且在至少一个目标上严格更优，则认为它更优越。\n\n- **支配**：一个目标为 $(E_j, P_j)$ 的模型 $j$ *支配* 一个目标为 $(E_i, P_i)$ 的模型 $i$，当且仅当：\n  $E_j \\le E_i$ 且 $P_j \\le P_i$，并且至少一个不等式是严格的（即 $E_j  E_i$ 或 $P_j  P_i$）。\n\n- **帕累托最优**：如果模型 $i$ 在集合中没有被任何其他模型 $j$ 所支配，则称其为*帕累托最优*。所有帕累托最优点组成的集合称为帕累托前沿。\n\n识别帕累托最优模型并计算被支配模型数量的算法如下：\n1.  对于每个测试用例，按规定生成数据 $(X, y)$。\n2.  对于每个提供的正则化强度 $\\lambda_k$，使用解析解计算相应的权重向量 $w_{\\lambda_k}$。\n3.  为每个模型 $k$ 计算目标对 $(E_k, P_k)$。\n4.  初始化一个与模型数量相同大小的布尔数组 `is_dominated`，所有条目均设为 `False`。\n5.  遍历每个模型 $i$：\n    a. 遍历所有其他模型 $j$（其中 $j \\neq i$）。\n    b. 根据上述定义检查模型 $j$ 是否支配模型 $i$。为处理浮点运算，比较时使用一个小的容差 $\\epsilon_{tol}$。如果 $(E_j \\le E_i + \\epsilon_{tol} \\text{ and } P_j \\le P_i + \\epsilon_{tol})$ 并且 $(E_j  E_i - \\epsilon_{tol} \\text{ or } P_j  P_i - \\epsilon_{tol})$，则模型 $j$ 支配 $i$。\n    c. 如果为模型 $i$ 找到了一个支配模型 $j$，则将 `is_dominated[i]` 设为 `True`，并为了效率跳出内层循环（对 $j$ 的循环）。\n6.  循环结束后，`is_dominated[k]` 为 `False` 的索引 $k$ 对应于帕累托最优模型。收集这些索引。\n7.  被支配模型的总数是 `is_dominated` 数组中 `True` 值的总和。\n8.  每个测试用例的结果是帕累托最优索引的排序列表和被支配模型的数量。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef process_case(s, n, p, sigma, lambdas):\n    \"\"\"\n    Generates data, trains models, and performs Pareto analysis for one test case.\n\n    Args:\n        s (int): Seed for the random number generator.\n        n (int): Number of samples.\n        p (int): Number of features.\n        sigma (float): Standard deviation of the noise.\n        lambdas (list[float]): List of regularization strengths.\n\n    Returns:\n        list: A list containing two elements:\n              1. A list of zero-based indices of Pareto-optimal models.\n              2. The integer count of dominated models.\n    \"\"\"\n    # 1. Data Generation using a deterministic protocol\n    rng = np.random.default_rng(seed=s)\n    X = rng.normal(loc=0.0, scale=1.0, size=(n, p))\n    # The RNG state advances, so w_star is drawn from a different state than X\n    w_star = rng.normal(loc=0.0, scale=1.0, size=p)\n    epsilon = rng.normal(loc=0.0, scale=sigma, size=n)\n    y = X @ w_star + epsilon\n\n    # 2. Model Training and Objective Calculation for each lambda\n    objectives = []\n    XTX = X.T @ X\n    XTy = X.T @ y\n    I = np.identity(p)\n    \n    for lmbda in lambdas:\n        # Solve (X'X + lambda*I)w = X'y for w\n        w_lambda = np.linalg.solve(XTX + lmbda * I, XTy)\n\n        # Calculate the two objectives: training MSE and penalty value\n        pred_error = X @ w_lambda - y\n        E_train = (1.0 / n) * np.sum(pred_error**2)\n        P_lambda_val = lmbda * np.sum(w_lambda**2)\n        \n        objectives.append((E_train, P_lambda_val))\n\n    # 3. Pareto Optimality Analysis\n    num_models = len(lambdas)\n    is_dominated = [False] * num_models\n    tol = 1e-12  # Tolerance for floating-point comparisons\n\n    for i in range(num_models):\n        for j in range(num_models):\n            if i == j:\n                continue\n            \n            # Check if model j dominates model i\n            E_i, P_i = objectives[i]\n            E_j, P_j = objectives[j]\n            \n            # Dominance condition: E_j = E_i and P_j = P_i, with one being strict\n            is_le_in_both = (E_j = E_i + tol) and (P_j = P_i + tol)\n            is_lt_in_one = (E_j  E_i - tol) or (P_j  P_i - tol)\n            \n            if is_le_in_both and is_lt_in_one:\n                is_dominated[i] = True\n                break  # Model i is dominated, no need to check other j's\n\n    pareto_indices = [i for i, dominated in enumerate(is_dominated) if not dominated]\n    dominated_count = sum(is_dominated)\n\n    return [pareto_indices, dominated_count]\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the final result.\n    \"\"\"\n    test_cases = [\n        # Test case A (well-posed, low noise)\n        {'s': 7, 'n': 40, 'p': 6, 'sigma': 1e-1, 'lambdas': [1e-6, 1e-4, 1e-2, 1e-1, 1.0, 10.0]},\n        \n        # Test case B (underdetermined, very low noise)\n        {'s': 101, 'n': 20, 'p': 50, 'sigma': 5e-2, 'lambdas': [1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1.0]},\n        \n        # Test case C (well-posed, high noise)\n        {'s': 2023, 'n': 50, 'p': 10, 'sigma': 1.0, 'lambdas': [1e-5, 1e-3, 1e-2, 1e-1, 1.0, 1e2]}\n    ]\n\n    results = []\n    for case in test_cases:\n        result = process_case(case['s'], case['n'], case['p'], case['sigma'], case['lambdas'])\n        results.append(result)\n\n    # Format the output string exactly as required, without extra spaces.\n    outer_list_parts = []\n    for res_pair in results:\n        pareto_indices, dominated_count = res_pair\n        indices_str = '[' + ','.join(map(str, pareto_indices)) + ']'\n        pair_str = f\"[{indices_str},{dominated_count}]\"\n        outer_list_parts.append(pair_str)\n    \n    final_output = '[' + ','.join(outer_list_parts) + ']'\n    print(final_output)\n\nsolve()\n```", "id": "3168619"}, {"introduction": "正则化并非总是通过在损失函数中添加显式惩罚项来实现。本练习将带你探索一个引人入胜的概念——“隐式正则化”，即优化过程本身也能帮助防止过拟合。你将为 LASSO 问题实现迭代收缩阈值算法（ISTA），并发现提前停止训练这一简单的操作就能成为一种强大的正则化手段，其效果有时甚至能超越传统的 $\\ell_1$ 惩罚。[@problem_id:3168592]", "problem": "将最小绝对收缩和选择算子 (LASSO) 的目标函数看作一个复合函数，它由一个平滑的数据拟合项和一个非平滑的稀疏性促进惩罚项组成。设训练设计矩阵为 $A_{\\text{train}} \\in \\mathbb{R}^{n_{\\text{train}} \\times d}$，验证设计矩阵为 $A_{\\text{val}} \\in \\mathbb{R}^{n_{\\text{val}} \\times d}$，相应的响应向量为 $y_{\\text{train}} \\in \\mathbb{R}^{n_{\\text{train}}}$ 和 $y_{\\text{val}} \\in \\mathbb{R}^{n_{\\text{val}}}$。带有 $\\ell_1$ 惩罚的经验风险最小化问题是\n$$\n\\min_{x \\in \\mathbb{R}^d} \\; \\frac{1}{2 n_{\\text{train}}} \\left\\| A_{\\text{train}} x - y_{\\text{train}} \\right\\|_2^2 \\; + \\; \\lambda \\left\\| x \\right\\|_1,\n$$\n其中 $\\lambda \\ge 0$ 用于平衡数据拟合度和稀疏性。迭代收缩阈值算法 (ISTA) 是通过对该复合目标函数应用近端梯度下降得到的，该算法交替执行平滑项上的梯度步和应用 $\\ell_1$ 范数的近端算子。在此问题中，您将把固定次数 $T$ 的 ISTA 迭代展开为一个逐层计算图（将 $T$ 解释为早停），并将其泛化行为与具有显式 $\\lambda$ 正则化的长期运行 ISTA 进行比较。\n\n您的程序必须：\n- 使用以下基本且经过充分测试的规范，构建具有稀疏真实解的合成线性回归数据集：\n  1. 真实解参数 $x^\\star \\in \\mathbb{R}^d$ 是 $k$-稀疏的，有且仅有 $k$ 个非零项，这些项独立地从零均值单位方差分布中抽取。支撑集（非零项的位置）是均匀随机选择的。\n  2. 生成的训练和验证矩阵具有可控的列相关性。对于给定的相关性参数 $\\rho \\in [0,1)$，设 $Z_{\\text{train}} \\in \\mathbb{R}^{n_{\\text{train}} \\times d}$ 和 $Z_{\\text{val}} \\in \\mathbb{R}^{n_{\\text{val}} \\times d}$ 的元素是独立同分布 (i.i.d.) 的标准正态分布。对于每个数据集，抽取一个潜在向量 $u_{\\text{train}} \\in \\mathbb{R}^{n_{\\text{train}}}$ 和 $u_{\\text{val}} \\in \\mathbb{R}^{n_{\\text{val}}}$，其元素为独立同分布的标准正态分布。定义\n  $$\n  A_{\\text{train}}[:,j] \\;=\\; \\sqrt{1-\\rho}\\; Z_{\\text{train}}[:,j] \\;+\\; \\sqrt{\\rho}\\; u_{\\text{train}}, \\quad\n  A_{\\text{val}}[:,j] \\;=\\; \\sqrt{1-\\rho}\\; Z_{\\text{val}}[:,j] \\;+\\; \\sqrt{\\rho}\\; u_{\\text{val}},\n  $$\n  对于每个列索引 $j \\in \\{1,\\dots,d\\}$。这种构造方法产生的特征列其相关性由 $\\rho$ 控制，并且对于研究共线性的影响是科学合理的。\n  3. 训练和验证响应满足\n  $$\n  y_{\\text{train}} \\;=\\; A_{\\text{train}} x^\\star \\;+\\; \\varepsilon_{\\text{train}}, \\quad\n  y_{\\text{val}} \\;=\\; A_{\\text{val}} x^\\star \\;+\\; \\varepsilon_{\\text{val}},\n  $$\n  其中 $\\varepsilon_{\\text{train}}$ 和 $\\varepsilon_{\\text{val}}$ 的元素是独立同分布的高斯分布，均值为零，标准差为 $\\sigma$（噪声水平）。\n- 从平方损失和 $\\ell_1$ 惩罚的核心定义出发，通过以下步骤实现 ISTA：\n  1. 使用多元微积分的基本原理计算平滑项的梯度。\n  2. 选择一个常数步长 $s$，其值至多为平滑项梯度 Lipschitz 常数的倒数，其中 Lipschitz 常数等于 $\\frac{1}{n_{\\text{train}}} A_{\\text{train}}^\\top A_{\\text{train}}$ 的最大特征值。\n  3. 对梯度更新后的迭代值应用 $\\ell_1$ 范数的近端算子。\n- 从零向量 $x^{(0)} = 0$ 开始，精确展开 $T$ 次迭代以获得 $x^{(T)}$，并将有限的 $T$ 解释为早停，这是一种在优化收敛前停止而产生的隐式正则化形式。\n\n对于每个数据集，构建两个估计器：\n- 无显式 $\\ell_1$ 惩罚的早停估计器：设置 $\\lambda = 0$ 并运行 ISTA 恰好 $T_{\\text{small}}$ 次迭代以获得 $x^{(T_{\\text{small}})}$。\n- 显式正则化估计器：设置一个预定的 $\\lambda  0$ 并运行 ISTA $T_{\\text{large}}$ 次迭代以近似收敛并获得 $x^{(T_{\\text{large}})}$。\n\n通过计算两个估计器在验证集上的均方误差 (MSE) 来评估泛化能力：\n$$\n\\text{MSE}_{\\text{val}}(x) \\;=\\; \\frac{1}{n_{\\text{val}}} \\left\\| A_{\\text{val}} x - y_{\\text{val}} \\right\\|_2^2.\n$$\n对于每个数据集，报告早停是否比显式 $\\lambda$ 正则化取得了更低的验证误差。将此比较表示为一个整数：如果 $\\text{MSE}_{\\text{val}}(x^{(T_{\\text{small}})})  \\text{MSE}_{\\text{val}}(x^{(T_{\\text{large}})})$，则为 $1$，否则为 $0$。\n\n使用以下测试套件以确保覆盖不同的机制：\n- 案例 1（理想路径，中等噪声，弱相关性）：$d = 50$, $n_{\\text{train}} = 80$, $n_{\\text{val}} = 80$, $k = 5$, $\\sigma = 0.2$, $\\rho = 0.2$, $T_{\\text{small}} = 10$, $T_{\\text{large}} = 500$, $\\lambda = 0.05$。\n- 案例 2（$T=0$ 的边界条件和较高噪声）：$d = 50$, $n_{\\text{train}} = 80$, $n_{\\text{val}} = 80$, $k = 5$, $\\sigma = 1.0$, $\\rho = 0.0$, $T_{\\text{small}} = 0$, $T_{\\text{large}} = 500$, $\\lambda = 0.1$。\n- 案例 3（强特征相关性，低噪声）：$d = 50$, $n_{\\text{train}} = 80$, $n_{\\text{val}} = 80$, $k = 5$, $\\sigma = 0.05$, $\\rho = 0.9$, $T_{\\text{small}} = 3$, $T_{\\text{large}} = 500$, $\\lambda = 0.01$。\n\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表（例如，$[r_1,r_2,r_3]$）作为结果，其中每个 $r_i$ 是案例 $i$ 的整数比较结果。不涉及物理单位或角度单位。通过在程序内部为每个案例固定种子，确保所有随机选择都是可复现的。", "solution": "用户提供的问题是有效的。这是一个在计算科学领域中提法恰当、有科学依据且客观的问题，特别关注机器学习中的正则化技术。它提供了一套完整且一致的定义、参数和评估标准，从而能够得出一个唯一且有意义的解。\n\n该问题要求对 LASSO（最小绝对收缩和选择算子）模型的两种正则化形式进行比较：显式的 $\\ell_1$ 惩罚和通过早停优化算法实现的隐式正则化。所选算法是迭代收缩阈值算法 (ISTA)，这是解决此类问题的标准近端梯度方法。\n\n问题的核心在于求解以下优化目标：\n$$\n\\min_{x \\in \\mathbb{R}^d} \\; F(x) = \\min_{x \\in \\mathbb{R}^d} \\left( \\frac{1}{2 n_{\\text{train}}} \\left\\| A_{\\text{train}} x - y_{\\text{train}} \\right\\|_2^2 \\; + \\; \\lambda \\left\\| x \\right\\|_1 \\right)\n$$\n其中目标函数 $F(x)$ 是一个复合函数 $F(x) = f(x) + g(x)$。项 $f(x) = \\frac{1}{2 n_{\\text{train}}} \\| A_{\\text{train}} x - y_{\\text{train}} \\|_2^2$ 是平滑、可微的数据拟合项（来自平方损失的经验风险），而 $g(x) = \\lambda \\|x\\|_1$ 是非平滑、凸的正则化项。\n\n### 数据生成\n根据严格的规范构建合成数据集，以确保可控性和可复现性。\n1.  生成一个 $k$-稀疏的真实解参数向量 $x^\\star \\in \\mathbb{R}^d$。其支撑集（其 $k$ 个非零元素索引的集合）是均匀随机选择的。这些非零位置上的值是从标准正态分布 $N(0, 1)$ 中抽取的。\n2.  设计矩阵 $A_{\\text{train}} \\in \\mathbb{R}^{n_{\\text{train}} \\times d}$ 和 $A_{\\text{val}} \\in \\mathbb{R}^{n_{\\text{val}} \\times d}$ 的构建使其具有指定的特征相关性 $\\rho$。每一列 $A[:,j]$ 是一个独立随机向量 $Z[:,j]$ 和一个公共潜在向量 $u$ 的线性组合：\n    $$\n    A[:,j] = \\sqrt{1-\\rho} Z[:,j] + \\sqrt{\\rho} u\n    $$\n    其中 $Z$ 和 $u$ 的元素是独立同分布的标准正态分布。这种构造对于 $i \\neq j$ 导出了理论相关性 $\\text{Cov}(A[:,i], A[:,j]) = \\rho$。\n3.  响应向量 $y_{\\text{train}}$ 和 $y_{\\text{val}}$ 由线性模型 $y = Ax^\\star + \\varepsilon$ 生成，其中 $\\varepsilon$ 是均值为 $0$、标准差为 $\\sigma$ 的加性高斯噪声。\n\n### 迭代收缩阈值算法 (ISTA)\nISTA 是近端梯度方法的一个实例，适用于最小化形如 $f(x) + g(x)$ 的复合目标函数。每次迭代包括两个步骤：\n1.  对平滑项 $f(x)$ 进行梯度下降步。\n2.  应用非平滑项 $g(x)$ 的近端算子。\n\n平滑项 $f(x)$ 的梯度由下式给出：\n$$\n\\nabla f(x) = \\frac{1}{n_{\\text{train}}} A_{\\text{train}}^\\top (A_{\\text{train}} x - y_{\\text{train}})\n$$\n步长为 $s$ 的 $g(x) = \\lambda \\|x\\|_1$ 的近端算子是软阈值算子 $S_{s\\lambda}(\\cdot)$：\n$$\n\\text{prox}_{s g}(v)_i = S_{s\\lambda}(v_i) = \\text{sign}(v_i) \\max(|v_i| - s\\lambda, 0)\n$$\n结合这些，从初始猜测 $x^{(0)}$ 开始的 ISTA 更新规则是：\n$$\nx^{(t+1)} = S_{s\\lambda} \\left( x^{(t)} - s \\nabla f(x^{(t)}) \\right)\n$$\n如果步长 $s$ 满足 $0  s \\le 1/L$，则该算法的收敛性得到保证，其中 $L$ 是 $\\nabla f(x)$ 的 Lipschitz 常数。 $f(x)$ 的 Hessian 矩阵是 $\\nabla^2 f(x) = \\frac{1}{n_{\\text{train}}} A_{\\text{train}}^\\top A_{\\text{train}}$。Lipschitz 常数 $L$ 是该 Hessian 矩阵的最大特征值（即谱范数）。一个安全且标准的选择是 $s = 1/L$。\n\n### 正则化比较\n构建并比较了两个估计器：\n1.  **早停估计器**：该模型使用隐式正则化。我们设置显式正则化参数 $\\lambda = 0$，并运行 ISTA 算法一个小的、固定的迭代次数 $T_{\\text{small}}$。当 $\\lambda=0$ 时，软阈值算子成为恒等算子，$S_0(v) = v$，ISTA 简化为对最小二乘目标的标准梯度下降。从 $x^{(0)} = 0$ 开始，几次迭代会使参数向经验风险的最小值移动，但提早停止可以防止参数变得过大和过拟合训练数据。\n2.  **显式正则化估计器**：该模型使用标准的 LASSO 公式，并带有预设的惩罚项 $\\lambda  0$。ISTA 算法运行大量的迭代次数 $T_{\\text{large}}$，以近似收敛到显式正则化目标的唯一最小值点。\n\n### 评估\n两个估计器 $x^{(T_{\\text{small}})}$ 和 $x^{(T_{\\text{large}})}$ 的泛化性能在未见过的验证集上进行评估。评估指标是均方误差 (MSE)：\n$$\n\\text{MSE}_{\\text{val}}(x) = \\frac{1}{n_{\\text{val}}} \\left\\| A_{\\text{val}} x - y_{\\text{val}} \\right\\|_2^2\n$$\n每个测试用例的最终输出是一个整数，如果早停估计器比显式正则化估计器取得了更低的验证 MSE，则为 $1$，否则为 $0$。通过为每个测试用例使用固定的随机种子来确保可复现性。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef generate_data(d, n_train, n_val, k, sigma, rho, rng):\n    \"\"\"\n    Generates synthetic data for a sparse linear regression problem.\n    \n    Args:\n        d (int): Number of features.\n        n_train (int): Number of training samples.\n        n_val (int): Number of validation samples.\n        k (int): Sparsity level of the true parameter vector.\n        sigma (float): Standard deviation of the noise.\n        rho (float): Correlation parameter for the design matrix columns.\n        rng (numpy.random.Generator): NumPy random number generator instance.\n        \n    Returns:\n        tuple: A_train, y_train, A_val, y_val\n    \"\"\"\n    # 1. Generate k-sparse ground-truth parameter vector x_star\n    x_star = np.zeros(d)\n    support = rng.choice(d, k, replace=False)\n    x_star[support] = rng.standard_normal(k)\n\n    # 2. Generate training and validation design matrices A\n    # Training data\n    Z_train = rng.standard_normal((n_train, d))\n    u_train = rng.standard_normal(n_train)\n    A_train = np.sqrt(1 - rho) * Z_train + np.sqrt(rho) * u_train[:, np.newaxis]\n    \n    # Validation data\n    Z_val = rng.standard_normal((n_val, d))\n    u_val = rng.standard_normal(n_val)\n    A_val = np.sqrt(1 - rho) * Z_val + np.sqrt(rho) * u_val[:, np.newaxis]\n\n    # 3. Generate response vectors y\n    eps_train = sigma * rng.standard_normal(n_train)\n    y_train = A_train @ x_star + eps_train\n    \n    eps_val = sigma * rng.standard_normal(n_val)\n    y_val = A_val @ x_star + eps_val\n\n    return A_train, y_train, A_val, y_val\n\ndef ista(A, y, lambda_reg, T, step_size):\n    \"\"\"\n    Implements the Iterative Shrinkage-Thresholding Algorithm (ISTA).\n    \n    Args:\n        A (np.ndarray): Design matrix.\n        y (np.ndarray): Response vector.\n        lambda_reg (float): L1 regularization parameter.\n        T (int): Number of iterations.\n        step_size (float): Step size for the gradient descent step.\n        \n    Returns:\n        np.ndarray: The estimated parameter vector x after T iterations.\n    \"\"\"\n    n, d = A.shape\n    x = np.zeros(d)\n    \n    if T == 0:\n        return x\n\n    for _ in range(T):\n        # Gradient of the smooth term\n        grad = (1 / n) * A.T @ (A @ x - y)\n        \n        # Gradient update step\n        z = x - step_size * grad\n        \n        # Proximal operator (soft-thresholding)\n        x = np.sign(z) * np.maximum(np.abs(z) - step_size * lambda_reg, 0)\n        \n    return x\n\ndef calculate_mse(A, y, x):\n    \"\"\"Calculates the Mean Squared Error.\"\"\"\n    n = A.shape[0]\n    error = A @ x - y\n    return (1 / n) * np.sum(error**2)\n\ndef solve():\n    \"\"\"\n    Main function to run the test cases and produce the final output.\n    \"\"\"\n    test_cases = [\n        # d, n_train, n_val, k, sigma, rho, T_small, T_large, lambda\n        (50, 80, 80, 5, 0.2, 0.2, 10, 500, 0.05),\n        (50, 80, 80, 5, 1.0, 0.0, 0, 500, 0.1),\n        (50, 80, 80, 5, 0.05, 0.9, 3, 500, 0.01),\n    ]\n\n    results = []\n    \n    for i, params in enumerate(test_cases):\n        d, n_train, n_val, k, sigma, rho, T_small, T_large, lambda_val = params\n        \n        # Ensure reproducibility for each case by fixing the seed\n        rng = np.random.default_rng(seed=i)\n\n        # Generate data\n        A_train, y_train, A_val, y_val = generate_data(d, n_train, n_val, k, sigma, rho, rng)\n        \n        # Calculate step size\n        C = (1 / n_train) * (A_train.T @ A_train)\n        L = np.max(np.linalg.eigvalsh(C))\n        step_size = 1.0 / L\n\n        # Early-stopped estimator (lambda = 0)\n        x_early = ista(A_train, y_train, lambda_reg=0.0, T=T_small, step_size=step_size)\n        \n        # Explicitly regularized estimator (lambda > 0)\n        x_reg = ista(A_train, y_train, lambda_reg=lambda_val, T=T_large, step_size=step_size)\n        \n        # Evaluate validation MSE for both\n        mse_early = calculate_mse(A_val, y_val, x_early)\n        mse_reg = calculate_mse(A_val, y_val, x_reg)\n        \n        # Compare and store result\n        result = 1 if mse_early  mse_reg else 0\n        results.append(result)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```", "id": "3168592"}]}