{"hands_on_practices": [{"introduction": "主成分分析（PCA）的核心思想是找到数据方差最大的方向。这个练习将带你回归本源，从第一性原理出发，通过解决一个约束优化问题来推导主成分。你将亲手计算一个协方差矩阵的特征值和特征向量，并验证它们确实对应着数据的最大和最小方差方向。通过这次实践 [@problem_id:3177001]，你将深刻理解为什么寻找最大方差的方向等价于求解协方差矩阵的特征值问题，从而牢固掌握 PCA 的数学基础。", "problem": "考虑一个待使用主成分分析（PCA）进行分析的二维、零均值数据集。设所求的样本协方差矩阵为\n$$\nC=\\begin{pmatrix} 3  2 \\\\ 2  3 \\end{pmatrix}.\n$$\n你必须基于以下基本定义进行操作：对于中心化的样本 $\\boldsymbol{x}_{i}\\in\\mathbb{R}^{2}$，样本协方差定义为\n$$\nC=\\frac{1}{n-1}\\sum_{i=1}^{n}\\boldsymbol{x}_{i}\\boldsymbol{x}_{i}^{\\top},\n$$\n并且对于任意满足 $|\\boldsymbol{u}|=1$ 的单位方向 $\\boldsymbol{u}\\in\\mathbb{R}^{2}$，投影数据沿 $\\boldsymbol{u}$ 方向的方差为\n$$\n\\operatorname{Var}(\\boldsymbol{u}^{\\top}\\boldsymbol{x})=\\boldsymbol{u}^{\\top}C\\,\\boldsymbol{u}.\n$$\n任务：\n1. 构建一个具体的、至少包含 $n=4$ 个样本的零均值数据集，使其样本协方差恰好等于 $C$，并仅使用给定的定义验证其样本协方差为 $C$。\n2. 使用第一性原理和约束优化，确定在约束 $|\\boldsymbol{u}|=1$ 下使得投影方差 $\\boldsymbol{u}^{\\top}C\\,\\boldsymbol{u}$ 最大化和最小化的 $\\mathbb{R}^{2}$ 中的单位方向，并计算相应的极值方差。\n3. 根据你的结果，计算第一主成分（任务2中的最大化方向）所解释的总方差比例，该比例定义为最大投影方差与数据集总方差之比。\n\n仅报告此比例作为你的最终答案。你可以将答案表示为最简分数。无需四舍五入，最终答案中不包含任何单位。", "solution": "该问题是适定的、有科学依据的，并且包含了推导出最终所求量的唯一解所需的所有信息。我们将按顺序完成这三个任务。\n\n## 任务1：数据集构建与验证\n\n第一个任务是构建一个至少包含 $n=4$ 个样本的零均值数据集，其样本协方差矩阵恰好为 $C=\\begin{pmatrix} 3  2 \\\\ 2  3 \\end{pmatrix}$。我们已知零均值数据的样本协方差定义为 $C=\\frac{1}{n-1}\\sum_{i=1}^{n}\\boldsymbol{x}_{i}\\boldsymbol{x}_{i}^{\\top}$。\n\n我们选择指定的最小样本量 $n=4$。该条件变为 $C=\\frac{1}{3}\\sum_{i=1}^{4}\\boldsymbol{x}_{i}\\boldsymbol{x}_{i}^{\\top}$，这意味着我们需要找到四个向量 $\\boldsymbol{x}_i \\in \\mathbb{R}^2$ 使得：\n1. 数据是零均值的：$\\sum_{i=1}^{4}\\boldsymbol{x}_{i} = \\boldsymbol{0}$。\n2. 外积之和为：$\\sum_{i=1}^{4}\\boldsymbol{x}_{i}\\boldsymbol{x}_{i}^{\\top} = 3C = 3\\begin{pmatrix} 3  2 \\\\ 2  3 \\end{pmatrix} = \\begin{pmatrix} 9  6 \\\\ 6  9 \\end{pmatrix}$。\n\n构建这样一个数据集的系统方法是利用协方差矩阵 $C$ 的谱特性。$C$ 的特征向量代表了数据方差的主轴。我们来求 $C$ 的特征值和特征向量。特征方程为 $\\det(C-\\lambda I)=0$。\n$$\n\\det\\begin{pmatrix} 3-\\lambda  2 \\\\ 2  3-\\lambda \\end{pmatrix} = (3-\\lambda)^2 - 4 = 0\n$$\n这得到 $3-\\lambda = \\pm 2$，所以特征值为 $\\lambda_1 = 3+2=5$ 和 $\\lambda_2 = 3-2=1$。\n\n对于第一个特征值 $\\lambda_1 = 5$，通过求解 $(C-5I)\\boldsymbol{u}_1=\\boldsymbol{0}$ 来找到特征向量 $\\boldsymbol{u}_1$：\n$$\n\\begin{pmatrix} -2  2 \\\\ 2  -2 \\end{pmatrix} \\begin{pmatrix} u_{11} \\\\ u_{12} \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} \\implies -2u_{11} + 2u_{12} = 0 \\implies u_{11} = u_{12}\n$$\n对应的单位特征向量是 $\\boldsymbol{u}_1 = \\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$。\n\n对于第二个特征值 $\\lambda_2 = 1$，通过求解 $(C-1I)\\boldsymbol{u}_2=\\boldsymbol{0}$ 来找到特征向量 $\\boldsymbol{u}_2$：\n$$\n\\begin{pmatrix} 2  2 \\\\ 2  2 \\end{pmatrix} \\begin{pmatrix} u_{21} \\\\ u_{22} \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} \\implies 2u_{21} + 2u_{22} = 0 \\implies u_{21} = -u_{22}\n$$\n对应的单位特征向量是 $\\boldsymbol{u}_2 = \\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$。\n\n$C$ 的谱分解为 $C = \\lambda_1 \\boldsymbol{u}_1 \\boldsymbol{u}_1^\\top + \\lambda_2 \\boldsymbol{u}_2 \\boldsymbol{u}_2^\\top$。\n我们需要 $\\sum_{i=1}^{4}\\boldsymbol{x}_{i}\\boldsymbol{x}_{i}^{\\top} = 3C = 3\\lambda_1 \\boldsymbol{u}_1 \\boldsymbol{u}_1^\\top + 3\\lambda_2 \\boldsymbol{u}_2 \\boldsymbol{u}_2^\\top$。\n我们构造一个与这些特征向量对齐的、对称的、零均值的数据点集：$\\boldsymbol{x}_1 = a\\boldsymbol{u}_1$，$\\boldsymbol{x}_2 = -a\\boldsymbol{u}_1$，$\\boldsymbol{x}_3 = b\\boldsymbol{u}_2$，$\\boldsymbol{x}_4 = -b\\boldsymbol{u}_2$。\n均值为 $\\boldsymbol{x}_1+\\boldsymbol{x}_2+\\boldsymbol{x}_3+\\boldsymbol{x}_4 = \\boldsymbol{0}$。外积之和为：\n$\\sum_{i=1}^{4}\\boldsymbol{x}_{i}\\boldsymbol{x}_{i}^{\\top} = (a\\boldsymbol{u}_1)(a\\boldsymbol{u}_1)^\\top + (-a\\boldsymbol{u}_1)(-a\\boldsymbol{u}_1)^\\top + (b\\boldsymbol{u}_2)(b\\boldsymbol{u}_2)^\\top + (-b\\boldsymbol{u}_2)(-b\\boldsymbol{u}_2)^\\top = 2a^2 \\boldsymbol{u}_1 \\boldsymbol{u}_1^\\top + 2b^2 \\boldsymbol{u}_2 \\boldsymbol{u}_2^\\top$。\n将其与 $3\\lambda_1 \\boldsymbol{u}_1 \\boldsymbol{u}_1^\\top + 3\\lambda_2 \\boldsymbol{u}_2 \\boldsymbol{u}_2^\\top$ 进行比较可得：\n$2a^2 = 3\\lambda_1 = 3(5)=15 \\implies a^2 = \\frac{15}{2} \\implies a = \\sqrt{\\frac{15}{2}}$。\n$2b^2 = 3\\lambda_2 = 3(1)=3 \\implies b^2 = \\frac{3}{2} \\implies b = \\sqrt{\\frac{3}{2}}$。\n\n因此，我们的具体数据集是：\n$\\boldsymbol{x}_1 = \\sqrt{\\frac{15}{2}} \\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\frac{\\sqrt{15}}{2}\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} \\frac{\\sqrt{15}}{2} \\\\ \\frac{\\sqrt{15}}{2} \\end{pmatrix}$\n$\\boldsymbol{x}_2 = -\\boldsymbol{x}_1 = \\begin{pmatrix} -\\frac{\\sqrt{15}}{2} \\\\ -\\frac{\\sqrt{15}}{2} \\end{pmatrix}$\n$\\boldsymbol{x}_3 = \\sqrt{\\frac{3}{2}} \\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} = \\frac{\\sqrt{3}}{2}\\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} \\frac{\\sqrt{3}}{2} \\\\ -\\frac{\\sqrt{3}}{2} \\end{pmatrix}$\n$\\boldsymbol{x}_4 = -\\boldsymbol{x}_3 = \\begin{pmatrix} -\\frac{\\sqrt{3}}{2} \\\\ \\frac{\\sqrt{3}}{2} \\end{pmatrix}$\n\n验证：根据构建方法，该数据集是零均值的。我们来计算 $\\sum \\boldsymbol{x}_i \\boldsymbol{x}_i^\\top$。令 $\\boldsymbol{x}_i = (x_{i1}, x_{i2})^\\top$。\n$\\sum_{i=1}^{4} x_{i1}^2 = (\\frac{\\sqrt{15}}{2})^2 + (-\\frac{\\sqrt{15}}{2})^2 + (\\frac{\\sqrt{3}}{2})^2 + (-\\frac{\\sqrt{3}}{2})^2 = \\frac{15}{4} + \\frac{15}{4} + \\frac{3}{4} + \\frac{3}{4} = \\frac{30+6}{4} = \\frac{36}{4}=9$。\n$\\sum_{i=1}^{4} x_{i2}^2 = (\\frac{\\sqrt{15}}{2})^2 + (-\\frac{\\sqrt{15}}{2})^2 + (-\\frac{\\sqrt{3}}{2})^2 + (\\frac{\\sqrt{3}}{2})^2 = \\frac{15}{4} + \\frac{15}{4} + \\frac{3}{4} + \\frac{3}{4} = 9$。\n$\\sum_{i=1}^{4} x_{i1}x_{i2} = (\\frac{\\sqrt{15}}{2})(\\frac{\\sqrt{15}}{2}) + (-\\frac{\\sqrt{15}}{2}})(-\\frac{\\sqrt{15}}{2}}) + (\\frac{\\sqrt{3}}{2}})(-\\frac{\\sqrt{3}}{2}}) + (-\\frac{\\sqrt{3}}{2}})(\\frac{\\sqrt{3}}{2}}) = \\frac{15}{4} + \\frac{15}{4} - \\frac{3}{4} - \\frac{3}{4} = \\frac{30-6}{4} = \\frac{24}{4}=6$。\n所以，$\\sum_{i=1}^{4}\\boldsymbol{x}_{i}\\boldsymbol{x}_{i}^{\\top} = \\begin{pmatrix} 9  6 \\\\ 6  9 \\end{pmatrix} = 3C$。样本协方差为 $\\frac{1}{3}\\sum \\boldsymbol{x}_i \\boldsymbol{x}_i^\\top = C$，符合要求。\n\n## 任务2：通过约束优化寻找极值方差方向\n\n我们希望在约束 $g(\\boldsymbol{u}) = |\\boldsymbol{u}|^2 - 1 = \\boldsymbol{u}^{\\top}\\boldsymbol{u} - 1 = 0$ 下，找到投影方差 $f(\\boldsymbol{u}) = \\boldsymbol{u}^{\\top}C\\,\\boldsymbol{u}$ 的极值。我们使用拉格朗日乘数法。拉格朗日函数为：\n$$\n\\mathcal{L}(\\boldsymbol{u}, \\lambda) = \\boldsymbol{u}^{\\top}C\\,\\boldsymbol{u} - \\lambda(\\boldsymbol{u}^{\\top}\\boldsymbol{u} - 1)\n$$\n为了找到驻点，我们将关于 $\\boldsymbol{u}$ 的梯度设为零：\n$$\n\\nabla_{\\boldsymbol{u}} \\mathcal{L} = 2C\\boldsymbol{u} - 2\\lambda\\boldsymbol{u} = \\boldsymbol{0}\n$$\n这可以简化为特征值方程：\n$$\nC\\boldsymbol{u} = \\lambda\\boldsymbol{u}\n$$\n这表明，使投影方差取极值的单位向量 $\\boldsymbol{u}$ 是协方差矩阵 $C$ 的特征向量。拉格朗日乘数 $\\lambda$ 是对应的特征值。\n在特征向量 $\\boldsymbol{u}$ 处的投影方差值为 $\\boldsymbol{u}^{\\top}C\\,\\boldsymbol{u} = \\boldsymbol{u}^{\\top}(\\lambda\\boldsymbol{u}) = \\lambda(\\boldsymbol{u}^{\\top}\\boldsymbol{u})$。在约束 $\\boldsymbol{u}^{\\top}\\boldsymbol{u}=1$ 下，投影方差就是特征值 $\\lambda$。\n\n从任务1中，我们求得 $C$ 的特征值为 $\\lambda_1 = 5$ 和 $\\lambda_2 = 1$。\n最大投影方差是最大的特征值，$\\lambda_{\\max} = 5$。达到该最大值的方向是对应的特征向量，也就是第一主成分：$\\boldsymbol{u}_1 = \\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$（及其负方向）。\n最小投影方差是最小的特征值，$\\lambda_{\\min} = 1$。达到该最小值的方向是第二主成分：$\\boldsymbol{u}_2 = \\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$（及其负方向）。\n\n## 任务3：总方差占比\n\n最后一个任务是计算第一主成分所解释的总方差比例。第一主成分是使投影方差最大化的方向，我们已经求出该方向为 $\\boldsymbol{u}_1$。此分量所解释的方差即为最大投影方差，$\\lambda_1 = 5$。\n\n数据集的总方差是沿各个维度的方差之和，即协方差矩阵的迹 $\\operatorname{Tr}(C)$。\n$$\n\\text{Total Variance} = \\operatorname{Tr}(C) = C_{11} + C_{22} = 3+3 = 6\n$$\n注意，这也等于特征值之和：$\\lambda_1 + \\lambda_2 = 5+1=6$，与预期相符。\n\n第一主成分所解释的总方差比例是该主成分方向上的方差与总方差之比：\n$$\n\\text{Proportion} = \\frac{\\text{Variance of PC1}}{\\text{Total Variance}} = \\frac{\\lambda_1}{\\operatorname{Tr}(C)} = \\frac{5}{6}\n$$\n该分数为最简形式。", "answer": "$$\\boxed{\\frac{5}{6}}$$", "id": "3177001"}, {"introduction": "理解了 PCA 背后的数学原理后，下一步是将其转化为可执行的代码。这个练习将指导你完成一个完整的 PCA 计算流程：从数据中心化、计算协方差矩阵，到执行特征分解和最终将原始数据投影到主成分上，得到降维后的表示。通过完成这个任务 [@problem_id:2421751]，你不仅能掌握 PCA 的核心实现步骤，还能获得一个在未来可用于数据“匿名化”、特征提取或降维的实用工具。", "problem": "给定代表客户属性的有限数据矩阵。对于每个矩阵，通过将原始特征替换为前 $k$ 个主成分来构建一个匿名化表示，定义如下。设 $X \\in \\mathbb{R}^{n \\times d}$ 表示具有 $n$ 个观测值和 $d$ 个特征的原始数据矩阵。定义列均值 $\\mu \\in \\mathbb{R}^{d}$ 为 $\\mu_j = \\frac{1}{n} \\sum_{i=1}^{n} X_{ij}$，其中 $j \\in \\{1,\\dots,d\\}$。设中心化数据为 $X_c = X - \\mathbf{1}\\mu^{\\top}$，其中 $\\mathbf{1} \\in \\mathbb{R}^{n}$ 是全一向量。定义样本协方差矩阵 $S \\in \\mathbb{R}^{d \\times d}$ 为\n$$\nS = \\frac{1}{n-1} X_c^{\\top} X_c.\n$$\n设 $(\\lambda_1, v_1), \\dots, (\\lambda_d, v_d)$ 表示 $S$ 的特征值-特征向量对，其中特征值按非递增顺序排列 $\\lambda_1 \\ge \\lambda_2 \\ge \\cdots \\ge \\lambda_d \\ge 0$，而 $v_j \\in \\mathbb{R}^{d}$ 是相应的单位范数特征向量。为确保唯一的符号约定，对每个 $v_j$，令 $p(j)$ 为实现 $\\max_{\\ell \\in \\{1,\\dots,d\\}} |(v_j)_\\ell|$ 的最小索引，如果 $(v_j)_{p(j)}  0$，则将 $v_j$ 替换为 $-v_j$。定义 $V_k = [v_1 \\ \\cdots \\ v_k] \\in \\mathbb{R}^{d \\times k}$。匿名化表示（主成分分数）为\n$$\nZ = X_c V_k \\in \\mathbb{R}^{n \\times k}.\n$$\n如果 $k = 0$，定义 $Z$ 为 $n \\times 0$ 的空矩阵。对于每个测试用例，您必须以行主序（即，从第一行到最后一行连接各行）输出 $Z$ 的扁平化条目，每个实数精确到 $6$ 位小数。如果 $Z$ 没有任何条目，则输出一个空列表。\n\n您的程序必须处理以下测试套件。对于每一项，$X$ 被明确指定，$k$ 也被给出：\n\n- 测试用例 1：$X \\in \\mathbb{R}^{4 \\times 3}$ 且 $k = 2$，\n$$\nX =\n\\begin{bmatrix}\n1  2  3 \\\\\n2  3  4 \\\\\n3  2  1 \\\\\n4  0  -1\n\\end{bmatrix}.\n$$\n\n- 测试用例 2：使用与测试用例 1 相同的 $X$ 且 $k = 0$。\n\n- 测试用例 3：$X \\in \\mathbb{R}^{3 \\times 2}$ 且 $k = 1$，\n$$\nX =\n\\begin{bmatrix}\n1  2 \\\\\n2  4 \\\\\n3  6\n\\end{bmatrix}.\n$$\n\n- 测试用例 4：$X \\in \\mathbb{R}^{5 \\times 2}$ 且 $k = 2$，\n$$\nX =\n\\begin{bmatrix}\n10  0 \\\\\n12  2 \\\\\n9  -1 \\\\\n11  1 \\\\\n13  3\n\\end{bmatrix}.\n$$\n\n您的程序应生成单行输出，其中包含一个由方括号括起来的逗号分隔列表。每个测试用例的结果本身必须是一个由方括号括起来的逗号分隔列表，其中包含按行主序排列的 $Z$ 的扁平化值，每个值都精确到 $6$ 位小数。例如，总体格式为\n$$\n\\big[ [z_{1,1}, z_{1,2}, \\dots], [\\ ], [\\dots], [\\dots] \\big],\n$$\n打印输出中不含空格，每个 $z_{i}$ 精确到 $6$ 位小数。不应打印任何其他文本。", "solution": "问题陈述已经过严格验证，被认为是自洽、一致且科学合理的。它为主成分分析（PCA）的标准流程提供了一个清晰、正式的定义，并附带了确保唯一解的特征向量确定性符号约定。该问题是适定的，其组成部分基于线性代数和统计学的既定原则。因此，我们将着手解决此问题。\n\n将要实现的算法遵循问题说明中列出的精确步骤。给定一个具有 $n$ 个观测值和 $d$ 个特征的数据矩阵 $X \\in \\mathbb{R}^{n \\times d}$，以及一个非负整数 $k$，匿名化表示 $Z \\in \\mathbb{R}^{n \\times k}$ 的构建过程如下。\n\n1.  **数据均值中心化**：\n    首先，我们计算每个特征（列）的均值。均值向量 $\\mu \\in \\mathbb{R}^{d}$ 由其分量 $\\mu_j = \\frac{1}{n} \\sum_{i=1}^{n} X_{ij}$ 定义，其中 $j=1, \\dots, d$。然后通过从每个观测值（行）中减去该均值向量来对数据矩阵 $X$ 进行中心化处理。这就得到了中心化数据矩阵 $X_c = X - \\mathbf{1}\\mu^{\\top}$，其中 $\\mathbf{1} \\in \\mathbb{R}^{n}$ 是一个全一的列向量。$X_c$ 的每一列的均值都为 $0$。\n\n2.  **样本协方差矩阵**：\n    接下来，我们计算样本协方差矩阵 $S \\in \\mathbb{R}^{d \\times d}$。它由公式 $S = \\frac{1}{n-1} X_c^{\\top} X_c$ 给出。使用因子 $\\frac{1}{n-1}$ 对应于对总体协方差进行无偏估计的 Bessel's 校正。这是适用的，因为所有测试用例都满足 $n > 1$。\n\n3.  **协方差矩阵的特征分解**：\n    然后我们求解对称半正定矩阵 $S$ 的特征值和特征向量。这通过求解特征值问题 $S v = \\lambda v$ 来实现。这将产生 $d$ 对特征值-特征向量对 $(\\lambda_1, v_1), (\\lambda_2, v_2), \\dots, (\\lambda_d, v_d)$。特征值按非递增顺序排序，即 $\\lambda_1 \\ge \\lambda_2 \\ge \\cdots \\ge \\lambda_d \\ge 0$。相应的特征向量 $v_j$ 被选择为单位范数，即 $\\|v_j\\|_2 = 1$。\n\n4.  **特征向量的唯一符号约定**：\n    由于特征向量 $v_j$ 及其负向量 $-v_j$ 是等价的，因此我们强制采用一个确定性的符号约定以确保结果的唯一性。对每个特征向量 $v_j$，我们找到其绝对值最大元素所在的索引 $p(j)$。问题规定，如果存在多个这样的索引，则 $p(j)$ 应为其中最小的一个：$p(j) = \\min(\\arg\\max_{\\ell \\in \\{1,\\dots,d\\}} |(v_j)_\\ell|)$。如果该索引处的元素 $(v_j)_{p(j)}$ 为负，则翻转该特征向量的符号：$v_j \\leftarrow -v_j$。\n\n5.  **到主成分上的投影**：\n    前 $k$ 个经过归一化和符号约定处理的特征向量构成投影矩阵 $V_k = [v_1, v_2, \\dots, v_k] \\in \\mathbb{R}^{d \\times k}$ 的列。最终的匿名化数据，也称为主成分分数，是通过将中心化数据投影到这些主方向上得到的：$Z = X_c V_k$。结果矩阵 $Z$ 的维度为 $n \\times k$。如果 $k=0$，$V_0$ 是一个 $d \\times 0$ 的矩阵，导致 $Z$ 成为一个 $n \\times 0$ 的空矩阵。\n\n该实现将是使用 `numpy` 库进行数值计算，将此过程直接转换为 Python 代码。一个函数将封装这些步骤，处理提供的每个测试用例，并根据指定的舍入和布局规则格式化输出。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Processes all test cases for PCA and prints the final result.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (np.array([\n            [1, 2, 3],\n            [2, 3, 4],\n            [3, 2, 1],\n            [4, 0, -1]\n        ], dtype=float), 2),\n        (np.array([\n            [1, 2, 3],\n            [2, 3, 4],\n            [3, 2, 1],\n            [4, 0, -1]\n        ], dtype=float), 0),\n        (np.array([\n            [1, 2],\n            [2, 4],\n            [3, 6]\n        ], dtype=float), 1),\n        (np.array([\n            [10, 0],\n            [12, 2],\n            [9, -1],\n            [11, 1],\n            [13, 3]\n        ], dtype=float), 2)\n    ]\n\n    all_case_results_str = []\n    \n    for X, k in test_cases:\n        # Main logic to calculate the PCA scores for one case.\n        n, d = X.shape\n\n        if k == 0:\n            # For k=0, the resulting matrix is empty.\n            Z = np.empty((n, 0))\n        else:\n            # Step 1: Mean Centering of Data\n            mu = np.mean(X, axis=0)\n            Xc = X - mu\n\n            # Step 2: Sample Covariance Matrix\n            # The problem assumes n > 1 for all test cases.\n            S = (Xc.T @ Xc) / (n - 1)\n\n            # Step 3: Eigendecomposition of the Covariance Matrix\n            # np.linalg.eigh is for symmetric matrices and returns eigenvalues in ascending order.\n            eigenvalues, eigenvectors = np.linalg.eigh(S)\n\n            # Sort eigenvalues and corresponding eigenvectors in descending order.\n            idx = np.argsort(eigenvalues)[::-1]\n            eigenvectors = eigenvectors[:, idx]\n\n            # Step 4: Unique Sign Convention for Eigenvectors\n            for j in range(d):\n                # Find the smallest index of the element with the maximum absolute value.\n                # np.argmax returns the first occurrence, which satisfies the condition.\n                p_j = np.argmax(np.abs(eigenvectors[:, j]))\n                # If the element at this index is negative, flip the eigenvector's sign.\n                if eigenvectors[p_j, j]  0:\n                    eigenvectors[:, j] *= -1\n            \n            # Step 5: Projection onto Principal Components\n            # Form the matrix Vk from the first k eigenvectors.\n            Vk = eigenvectors[:, :k]\n            # Project the centered data to get the scores.\n            Z = Xc @ Vk\n\n        # Format the output string for the current test case.\n        if Z.size == 0:\n            all_case_results_str.append(\"[]\")\n        else:\n            # Flatten the Z matrix in row-major order and format each number.\n            z_flat_str = [\"{:.6f}\".format(x) for x in Z.flatten()]\n            case_str = \"[\" + \",\".join(z_flat_str) + \"]\"\n            all_case_results_str.append(case_str)\n\n    # Final print statement in the exact required format.\n    # The output string is built manually to avoid spaces introduced by list-to-string conversion.\n    final_output_str = \"[\" + \",\".join(all_case_results_str) + \"]\"\n    print(final_output_str)\n\nsolve()\n```", "id": "2421751"}, {"introduction": "在真实世界的数据中直接应用 PCA 可能会产生误导性结果，因为该算法对特征的尺度非常敏感。这个练习 [@problem_id:2430028] 旨在通过一个对比实验来揭示这一关键点。你将对具有不同数量级特征的数据分别在标准化前后进行 PCA，并量化比较主成分方向和解释方差比例的变化。这次实践将让你直观地看到为什么特征缩放（例如，标准化）通常是应用 PCA 之前一个必不可少的预处理步骤。", "problem": "给定一个数据矩阵族，其列具有差异巨大的数值尺度。对于每种情况，考虑一个实值数据矩阵 $X \\in \\mathbb{R}^{n \\times d}$，其中包含 $d=3$ 个特征和 $n$ 个样本。对于下文中的每个测试用例，通过显式公式确定性地定义 $X$。任务是比较在均值中心化数据和标准化数据上执行主成分分析 (PCA) 的结果，并量化主导主成分方向及其方差解释比例因标准化而发生的变化。此处，主成分分析 (PCA) 定义为转换后数据的样本协方差矩阵的特征分解。此处，特征的标准化定义为对每一列进行中心化（减去其样本均值）并除以其样本标准差；如果某一列的标准差为零，则在中心化后，将该标准化列恒置为零。\n\n使用的定义：\n- 给定 $X \\in \\mathbb{R}^{n \\times d}$，令 $\\bar{\\mathbf{x}} \\in \\mathbb{R}^{d}$ 为列样本均值，令 $X_c = X - \\mathbf{1}\\bar{\\mathbf{x}}^\\top$ 表示中心化数据，其中 $\\mathbf{1} \\in \\mathbb{R}^{n}$ 是全一向量。样本协方差矩阵为 $S = \\frac{1}{n-1} X_c^\\top X_c$。\n- $S$ 的 PCA 特征值和特征向量是其特征对 $\\{(\\lambda_k,\\mathbf{v}_k)\\}_{k=1}^d$，其中 $\\lambda_1 \\ge \\lambda_2 \\ge \\cdots \\ge \\lambda_d \\ge 0$，且特征向量 $\\{\\mathbf{v}_k\\}$ 是标准正交的。第一个主成分的方差解释比为 $r_1 = \\frac{\\lambda_1}{\\sum_{j=1}^d \\lambda_j}$。\n- 对于标准化数据，计算 $X_c$ 的列样本标准差 $\\sigma_j$。通过对所有 $\\sigma_j \\neq 0$ 的 $j$ 令 $Z_{:,j} = X_{c,:,j}/\\sigma_j$ 来构成 $Z$，对于任何 $\\sigma_j = 0$ 的 $j$ 令 $Z_{:,j} = \\mathbf{0}$。然后定义 $S^{(z)} = \\frac{1}{n-1} Z^\\top Z$ 及其特征对 $\\{(\\mu_k,\\mathbf{u}_k)\\}_{k=1}^d$，排序为 $\\mu_1 \\ge \\mu_2 \\ge \\mu_3 \\ge 0$，其方差解释比为 $r_1^{(z)} = \\frac{\\mu_1}{\\sum_{j=1}^d \\mu_j}$。\n- 两个单位主方向 $\\mathbf{v}_1$ 和 $\\mathbf{u}_1$ 之间的一致性通过 $a = |\\mathbf{v}_1^\\top \\mathbf{u}_1| \\in [0,1]$ 来衡量。特征向量的符号不确定性通过绝对值来处理。\n- 为量化原始坐标轴对分量的主导性，定义 $i_{\\mathrm{before}} = \\arg\\max_{j \\in \\{0,1,2\\}} |(\\mathbf{v}_1)_j|$ 和 $i_{\\mathrm{after}} = \\arg\\max_{j \\in \\{0,1,2\\}} |(\\mathbf{u}_1)_j|$，特征索引使用从零开始的索引。\n\n对于每个测试用例，您必须计算以下有序的量列表：\n- 从 $S$ 计算的 $r_1$，\n- 从 $S^{(z)}$ 计算的 $r_1^{(z)}$，\n- $a = |\\mathbf{v}_1^\\top \\mathbf{u}_1|$，\n- $i_{\\mathrm{before}}$，\n- $i_{\\mathrm{after}}$。\n\n测试套件（每个用例定义 $n$，然后是 $i \\in \\{0,\\dots,n-1\\}$ 的 $t_i$，以及作为 $t_i$ 函数的三个特征）：\n- 情况 1：$n=200$。对于每个 $i \\in \\{0,\\dots,199\\}$，令 $t_i = \\frac{i}{199}$，并定义\n  - $x_{i1} = 1000 \\cos(2\\pi t_i)$，\n  - $x_{i2} = \\sin(2\\pi t_i) + 0.1 \\cos(4\\pi t_i)$，\n  - $x_{i3} = 0.001\\, t_i$。\n  通过将这三个特征堆叠为列来组合成 $X$。\n- 情况 2：$n=100$。对于每个 $i \\in \\{0,\\dots,99\\}$，令 $t_i = \\frac{i}{99}$，并定义\n  - $x_{i1} = 10^6\\, t_i$，\n  - $x_{i2} = 0$，\n  - $x_{i3} = 10\\,(t_i - 0.5)$。\n  通过将这三个特征堆叠为列来组合成 $X$。\n- 情况 3：$n=150$。对于每个 $i \\in \\{0,\\dots,149\\}$，令 $t_i = \\frac{i}{149}$，并定义\n  - $x_{i1} = 1000\\,(2 t_i - 1)$，\n  - $x_{i2} = (2 t_i - 1) + 0.1 \\sin(3\\pi t_i)$，\n  - $x_{i3} = 0.01 \\cos(5\\pi t_i)$。\n  通过将这三个特征堆叠为列来组合成 $X$。\n\n要求的最终输出格式：\n- 您的程序应生成一行输出，其中包含用方括号括起来的逗号分隔列表形式的结果。对于每个用例，输出有序列表 $[r_1, r_1^{(z)}, a, i_{\\mathrm{before}}, i_{\\mathrm{after}}]$。按情况 $1, 2, 3$ 的顺序，将三个用例的结果汇总成一个包含三个列表的单一列表。例如，总的打印结构必须是 $[[\\cdot,\\cdot,\\cdot,\\cdot,\\cdot],[\\cdot,\\cdot,\\cdot,\\cdot,\\cdot],[\\cdot,\\cdot,\\cdot,\\cdot,\\cdot]]$ 的形式。\n\n所有答案均为指定的无量纲实数或整数。由于所有要求的量都是纯数，因此不需要物理单位或角度单位。", "solution": "用户提供了一个有效且需要解决方案的问题。问题陈述在科学上植根于线性代数和统计学领域，特别是主成分分析 (PCA)。它定义明确，为构建数据、定义所有必要的数学对象和程序以及请求一组特定的可计算量提供了确定性的指令。语言客观且无歧义。因此，可以构建一个合理的、分步的解决方案。\n\n该问题要求对三种不同情况下的均值中心化数据与标准化数据执行的 PCA 进行比较。问题的核心在于观察缩放如何影响 PCA 的结果。PCA 识别数据集中的最大方差方向。当特征（数据矩阵的列）具有差异巨大的尺度时，无论底层数据结构如何，方差最大的特征都将主导第一个主成分。标准化通过将每个特征重新缩放至均值为 $0$、标准差为 $1$，将所有特征置于平等地位，从而防止了这种情况。\n\n总体步骤如下：\n$1$. 对于每个测试用例，构建 $n \\times d$ 数据矩阵 $X$，其中 $d=3$。\n$2$. 对均值中心化数据 $X_c$ 执行 PCA。\n    a. 计算列样本均值向量 $\\bar{\\mathbf{x}}$。\n    b. 对数据进行中心化：$X_c = X - \\mathbf{1}\\bar{\\mathbf{x}}^\\top$。\n    c. 计算样本协方差矩阵 $S = \\frac{1}{n-1} X_c^\\top X_c$。\n    d. 通过求解特征问题 $S\\mathbf{v}_k = \\lambda_k\\mathbf{v}_k$ 来找到 $S$ 的特征值 $\\lambda_k$ 和特征向量 $\\mathbf{v}_k$。特征值已排序，$\\lambda_1 \\ge \\lambda_2 \\ge \\lambda_3$，并且特征向量 $\\{\\mathbf{v}_k\\}$ 是标准正交的。第一个主成分方向是 $\\mathbf{v}_1$。\n    e. 计算第一个分量的方差解释比：$r_1 = \\lambda_1 / (\\sum_{j=1}^d \\lambda_j)$。\n    f. 识别主导 $\\mathbf{v}_1$ 的原始特征：$i_{\\mathrm{before}} = \\arg\\max_{j \\in \\{0,1,2\\}} |(\\mathbf{v}_1)_j|$。\n\n$3$. 对标准化数据 $Z$ 执行 PCA。\n    a. 使用 $n-1$ 作为除数，计算 $X_c$ 的列样本标准差 $\\sigma_j$。\n    b. 构建标准化数据矩阵 $Z$。每个列 $Z_{:,j}$ 是通过将相应的中心化列 $X_{c,:,j}$ 乘以 $1/\\sigma_j$ 得到的。如果 $\\sigma_j=0$，则将列 $Z_{:,j}$ 设置为零向量。\n    c. 计算 $Z$ 的样本协方差矩阵：$S^{(z)} = \\frac{1}{n-1} Z^\\top Z$。该矩阵等价于 $X$ 的样本相关矩阵。对于任何非常数特征，其对角线元素为 $1$。\n    d. 找到 $S^{(z)}$ 的特征值 $\\mu_k$ 和特征向量 $\\mathbf{u}_k$，并排序以使 $\\mu_1 \\ge \\mu_2 \\ge \\mu_3$。标准化数据的第一个主成分方向是 $\\mathbf{u}_1$。\n    e. 计算相应的方差解释比：$r_1^{(z)} = \\mu_1 / (\\sum_{j=1}^d \\mu_j)$。分母中的和，即 $\\text{Tr}(S^{(z)})$，等于非常数特征的数量。\n    f. 识别主导 $\\mathbf{u}_1$ 的原始特征：$i_{\\mathrm{after}} = \\arg\\max_{j \\in \\{0,1,2\\}} |(\\mathbf{u}_1)_j|$。\n\n$4$. 通过计算一致性度量 $a = |\\mathbf{v}_1^\\top \\mathbf{u}_1|$ 来比较两种分析的结果，该度量衡量两个主方向之间夹角的余弦值。\n\n$5$. 对于每种情况，最终输出为有序列表 $[r_1, r_1^{(z)}, a, i_{\\mathrm{before}}, i_{\\mathrm{after}}]$。\n\n具体情况分析：\n- **情况 1**：数据包含三个特征，其尺度分别为 $O(10^3)$、$O(1)$ 和 $O(10^{-3})$。第一个特征 $x_1 = 1000 \\cos(2\\pi t_i)$ 的方差将远大于其他特征。因此，未标准化数据的第一个主成分 $\\mathbf{v}_1$ 预计几乎与第一个特征轴完全对齐。这将得到 $r_1 \\approx 1$ 和 $i_{\\mathrm{before}} = 0$。标准化后，所有特征的方差均为单位方差，其结构关系（在 $(x_1, x_2)$ 平面上的椭圆轨迹）将变得明显。方差将更均匀地分布，导致 $r_1^{(z)}$ 较小，而 $\\mathbf{u}_1$ 将是特征 1 和 2 的组合。\n- **情况 2**：第一个特征 $x_1 = 10^6 t_i$ 具有巨大的尺度。第二个特征 $x_2=0$ 是常数，方差为零。第三个特征 $x_3 = 10(t_i-0.5)$ 的尺度远小于第一个特征。对于未标准化的数据，PCA 将由特征 1 主导，得到 $r_1 \\approx 1$ 和 $i_{\\mathrm{before}}=0$。标准化后，常数特征 $x_2$ 仍然是零向量。特征 1 和 3 都是 $t_i$ 的线性函数，在中心化和缩放后将变得完全相关。它们的标准化版本将是相同的，$Z_{:,1} = Z_{:,3}$。数据将在 $(Z_1, Z_3)$ 平面上坍缩到一个单一方向。这将导致 $r_1^{(z)}=1$（因为在非常数特征中，有效秩为 1），并且特征向量 $\\mathbf{u}_1$ 的形式为 $[1/\\sqrt{2}, 0, 1/\\sqrt{2}]^\\top$。\n- **情况 3**：第一个特征 $x_1 = 1000(2t_i-1)$ 具有大尺度。第二个特征 $x_2 = (2t_i-1) + 0.1 \\sin(3\\pi t_i)$ 与第一个特征高度相关，但尺度小得多。第三个特征的尺度可以忽略不计。与其他情况一样，未标准化的 PCA 将由第一个特征的尺度决定，因此 $r_1 \\approx 1$ 且 $i_{\\mathrm{before}}=0$。标准化后，特征 1 和 2 之间的强线性关系将成为最显著的特征。第一个主成分 $\\mathbf{u}_1$ 将捕获这种共享方差，代表一个沿着相关数据云主轴的方向，大致在特征 1 和 2 的标准化轴之间呈 $45$ 度角。\n\n实现将使用 `numpy` 进行所有数值计算，特别是使用 `numpy.linalg.eigh` 对对称协方差矩阵进行特征分解。将注意处理特征值的降序排序以及标准差为零的情况。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the PCA comparison problem for three given test cases.\n    \"\"\"\n\n    def generate_case_1_data():\n        n = 200\n        t = np.linspace(0, 1, n)\n        x1 = 1000 * np.cos(2 * np.pi * t)\n        x2 = np.sin(2 * np.pi * t) + 0.1 * np.cos(4 * np.pi * t)\n        x3 = 0.001 * t\n        return np.stack([x1, x2, x3], axis=1)\n\n    def generate_case_2_data():\n        n = 100\n        t = np.linspace(0, 1, n)\n        x1 = 1e6 * t\n        x2 = np.zeros(n)\n        x3 = 10 * (t - 0.5)\n        return np.stack([x1, x2, x3], axis=1)\n\n    def generate_case_3_data():\n        n = 150\n        t = np.linspace(0, 1, n)\n        x1 = 1000 * (2 * t - 1)\n        x2 = (2 * t - 1) + 0.1 * np.sin(3 * np.pi * t)\n        x3 = 0.01 * np.cos(5 * np.pi * t)\n        return np.stack([x1, x2, x3], axis=1)\n\n    def perform_pca_analysis(X):\n        \"\"\"\n        Performs PCA on both mean-centered and standardized data, returning the required metrics.\n        \"\"\"\n        n, d = X.shape\n        \n        # --- PCA on Mean-Centered Data ---\n        X_c = X - X.mean(axis=0)\n        S = (X_c.T @ X_c) / (n - 1)\n        \n        # Eigendecomposition. eigh returns sorted eigenvalues (ascending).\n        # We reverse them to get principal components in descending order of variance.\n        evals, evecs = np.linalg.eigh(S)\n        evals = evals[::-1]\n        evecs = evecs[:, ::-1]\n        \n        lambda_1 = evals[0]\n        v1 = evecs[:, 0]\n        \n        # Explained variance ratio\n        total_variance = np.sum(evals)\n        r1 = lambda_1 / total_variance if total_variance > 0 else 0\n        \n        # Dominant feature index\n        i_before = np.argmax(np.abs(v1))\n\n        # --- PCA on Standardized Data ---\n        stds = np.std(X_c, axis=0, ddof=1)\n        \n        # Handle features with zero standard deviation\n        Z = np.zeros_like(X_c)\n        non_zero_std_mask = stds > 0\n        if np.any(non_zero_std_mask):\n            Z[:, non_zero_std_mask] = X_c[:, non_zero_std_mask] / stds[non_zero_std_mask]\n\n        S_z = (Z.T @ Z) / (n - 1)\n        \n        # Eigendecomposition of the correlation matrix\n        mu, u = np.linalg.eigh(S_z)\n        mu = mu[::-1]\n        u = u[:, ::-1]\n        \n        mu_1 = mu[0]\n        u1 = u[:, 0]\n        \n        # Explained variance ratio for standardized data\n        total_variance_z = np.sum(mu)\n        r1_z = mu_1 / total_variance_z if total_variance_z > 0 else 0\n        \n        # Dominant feature index after standardization\n        i_after = np.argmax(np.abs(u1))\n        \n        # Alignment between the first principal components\n        alignment = np.abs(np.dot(v1, u1))\n        \n        return [r1, r1_z, alignment, int(i_before), int(i_after)]\n\n    test_cases = [\n        generate_case_1_data,\n        generate_case_2_data,\n        generate_case_3_data\n    ]\n    \n    all_results = []\n    for case_generator in test_cases:\n        X = case_generator()\n        result = perform_pca_analysis(X)\n        all_results.append(result)\n\n    # Format the output string without spaces as [[...],[...],[...]]\n    inner_strings = [f\"[{','.join(map(str, r))}]\" for r in all_results]\n    final_output = f\"[{','.join(inner_strings)}]\"\n    \n    print(final_output)\n\nsolve()\n```", "id": "2430028"}]}