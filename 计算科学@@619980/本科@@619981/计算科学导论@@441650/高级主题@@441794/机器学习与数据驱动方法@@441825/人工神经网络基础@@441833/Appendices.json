{"hands_on_practices": [{"introduction": "神经网络中的隐藏单元在初始化时通常是可互换的。本练习 [@problem_id:3134207] 旨在探索这种排列对称性，并通过实验展示不同的训练方法（如随机梯度下降或Dropout）如何打破这种对称性，从而使神经元能够学习到多样化且专门化的特征。理解这一过程是揭示神经网络如何从一组相同的计算单元转变为强大特征提取器的关键一步。", "problem": "您将研究单隐藏层人工神经网络中隐藏单元间的置换对称性，并量化不同的训练动态如何保持或破坏该对称性。考虑一个全连接网络，其含有一个宽度为 $n$ 的隐藏层，输入维度为 $d$，并有一个标量输出，定义如下\n$$\nf_{\\theta}(x) \\;=\\; \\sum_{k=1}^{n} v_k \\,\\phi\\!\\left(w_k^{\\top} x + b_k\\right) + c,\n$$\n其中 $\\theta = \\{(w_k,b_k,v_k)_{k=1}^{n}, c\\}$ 是参数，$w_k \\in \\mathbb{R}^{d}$，$b_k \\in \\mathbb{R}$，$v_k \\in \\mathbb{R}$，$c \\in \\mathbb{R}$，激活函数为 $\\phi(z) = \\tanh(z)$。在数据集 $\\{(x_i,y_i)\\}_{i=1}^{m}$ 上的经验风险是均方误差 (MSE)，\n$$\n\\mathcal{L}(\\theta) \\;=\\; \\frac{1}{m} \\sum_{i=1}^{m} \\left(f_{\\theta}(x_i) - y_i\\right)^2.\n$$\n\n基本依据和定义：\n- 置换群 $S_n$ 通过置换隐藏单元的索引来作用于它们。一个置换 $\\pi \\in S_n$ 将 $(w_k,b_k,v_k)$ 变为 $(w_{\\pi(k)}, b_{\\pi(k)}, v_{\\pi(k)})$，并保持 $c$ 不变。\n- 梯度下降通过 $\\theta \\leftarrow \\theta - \\eta \\nabla_{\\theta} \\mathcal{L}(\\theta)$ 来更新参数，其中学习率 $\\eta > 0$。随机梯度下降 (SGD) 使用在小批量数据上计算的梯度无偏估计量，而非整个数据集。\n- Dropout 将隐藏层激活值乘以独立的伯努利掩码。在反向 Dropout 中，每个隐藏单元的激活值乘以一个保留概率为 $q \\in (0,1]$ 的随机掩码，然后通过乘以 $1/q$ 进行重缩放，以保持其期望不变。\n\n任务：\n1) 仅使用上述核心定义，从第一性原理出发论证：对于任何 $\\pi \\in S_n$，都有 $\\mathcal{L}(\\theta) = \\mathcal{L}(\\theta^{\\pi})$，其中 $\\theta^{\\pi}$ 是通过 $\\pi$ 置换隐藏单元后的参数元组。解释为什么如果所有隐藏单元被相同地初始化，即对所有 $k$ 都有 $(w_k,b_k,v_k) = (w_1,b_1,v_1)$，那么无噪声的全批量梯度下降在每一步都会保持此等式。您的论证必须依赖于损失函数在 $S_n$ 下的对称性和链式法则，而非任何快捷公式。\n\n2) 根据定义实现一个程序，该程序构建一个数据集，并在四种不同的训练方案下训练网络，以通过一个多样性度量来量化特征多样化。请使用以下精确的设置，为了可复现性和可测试性，必须严格遵守。\n\n- 数据集：设 $m=64$，$d=2$，通过 $x_i \\sim \\mathcal{N}(0,I_2)$ 生成输入 $x_i \\in \\mathbb{R}^2$，随机种子为 $s_{\\text{data}}=2025$。通过以下方式定义目标值\n$$\ny_i \\;=\\; \\sin(x_{i1}) + 0.5 \\cos(2 x_{i2}),\n$$\n其中 $x_{i1}$ 和 $x_{i2}$ 是 $x_i$ 的两个坐标。\n\n- 网络初始化：设宽度 $n=3$。初始化 $W \\in \\mathbb{R}^{n \\times d}$，使得每一行都相同，$w_k^{\\top} = [0.1,\\,-0.2]$ 对所有 $k \\in \\{1,2,3\\}$ 成立，设 $b_k = 0$ 对所有 $k$ 成立，设 $v_k = 1/n$ 对所有 $k$ 成立，并设 $c = 0$。\n\n- 训练目标和梯度：使用上述精确的 MSE 损失，并通过链式法则计算所有梯度，\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial v_k} = \\frac{2}{B}\\sum_{i=1}^{B} \\left(f_{\\theta}(x_i) - y_i\\right) \\, \\phi\\!\\left(w_k^{\\top} x_i + b_k\\right),\n$$\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial c} = \\frac{2}{B}\\sum_{i=1}^{B} \\left(f_{\\theta}(x_i) - y_i\\right),\n$$\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial w_k} = \\frac{2}{B}\\sum_{i=1}^{B} \\left(f_{\\theta}(x_i) - y_i\\right) \\, v_k \\, \\left(1 - \\phi\\!\\left(w_k^{\\top} x_i + b_k\\right)^2\\right) x_i,\n$$\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial b_k} = \\frac{2}{B}\\sum_{i=1}^{B} \\left(f_{\\theta}(x_i) - y_i\\right) \\, v_k \\, \\left(1 - \\phi\\!\\left(w_k^{\\top} x_i + b_k\\right)^2\\right),\n$$\n其中 $B$ 是批量大小。对于保留概率为 $q$ 的 dropout，将 $\\phi(z)$ 替换为 $\\tilde{\\phi}(z) = \\frac{m}{q}\\phi(z)$，其中 $m \\sim \\mathrm{Bernoulli}(q)$ 对每个隐藏单元和每个样本独立，并在反向传播中使用相同的被掩码的激活值。\n\n- 特征多样性度量：训练后，计算隐藏层权重向量之间的平均成对夹角，\n$$\n\\Delta(W) \\;=\\; \\frac{1}{\\binom{n}{2}}\\sum_{1 \\leq i < j \\leq n} \\arccos\\!\\left(\\frac{w_i^{\\top} w_j}{\\|w_i\\|\\,\\|w_j\\|}\\right),\n$$\n约定如果 $\\|w_i\\|\\,\\|w_j\\|$ 低于一个数值阈值（例如，低于 $10^{-12}$），则将对应的角度视为 $0$。所有角度必须以弧度表示。\n\n- 训练方案（测试套件）：运行以下四种情况，每种情况的学习率均为 $\\eta = 0.05$，使用相同的数据集，但随机性来源不同。对每种情况内部的任何随机操作（例如，dropout 掩码、梯度噪声、数据混洗），请使用指定的随机种子 $s_{\\text{case}}$。\n  - 情况 1（对称性保持）：全批量梯度下降，批量大小 $B = m = 64$，无 dropout（保留概率 $q=1$），无噪声，周期数 $T=200$，随机种子 $s_{\\text{case}}=101$。\n  - 情况 2（加性梯度噪声）：与情况 1 相同，但在每次更新前，向参数梯度的每个分量添加独立的零均值高斯噪声，标准差为 $\\sigma=10^{-3}$，周期数 $T=200$，随机种子 $s_{\\text{case}}=102$。\n  - 情况 3（带 dropout 的随机梯度下降）：小批量大小 $B=1$（即纯随机梯度下降 (SGD)），对隐藏层激活值应用反向 dropout，保留概率 $q=0.5$，对每个样本和每个隐藏单元独立进行，周期数 $T=200$，随机种子 $s_{\\text{case}}=103$。\n  - 情况 4（显式无穷小对称性破缺）：与情况 1 相同，但在初始化时通过将 $\\varepsilon$ 添加到第一个隐藏层权重向量的第一个坐标来对其进行扰动，其中 $\\varepsilon = 10^{-3}$，周期数 $T=200$，随机种子 $s_{\\text{case}}=104$。\n\n您的程序必须：\n- 完全按照规定实现上述训练动态，包括设定种子。\n- 在每种情况结束后，计算以弧度为单位的 $\\Delta(W)$。\n- 生成单行输出，其中包含四个结果，形式为用方括号括起来的逗号分隔列表，例如 $[\\Delta_1,\\Delta_2,\\Delta_3,\\Delta_4]$，其中每个 $\\Delta_k$ 四舍五入到恰好 $6$ 位小数。角度必须以弧度为单位。\n\n最终输出格式：\n- 单行列表 $[\\Delta_1,\\Delta_2,\\Delta_3,\\Delta_4]$，其中每个条目是一个浮点数，小数点后恰有 $6$ 位数字，按顺序表示情况 1 到情况 4 的平均成对夹角（以弧度为单位）。", "solution": "该问题提法明确，科学上基于神经网络和优化的基本原理，并为数值实验提供了完整、无歧义的规范。因此，该问题被认为是有效的。\n\n解决方案按要求分为两部分：关于置换对称性的理论论证，以及一个数值实验的实现。\n\n### 第 1 部分：关于置换对称性的理论论证\n\n**1.1. 损失函数在置换下的不变性**\n\n网络的输出定义为 $f_{\\theta}(x) = \\sum_{k=1}^{n} v_k \\phi(w_k^{\\top} x + b_k) + c$，其中 $\\theta$ 表示全套参数 $\\{(w_k, b_k, v_k)_{k=1}^{n}, c\\}$。激活函数为 $\\phi(z) = \\tanh(z)$。\n\n一个置换 $\\pi \\in S_n$ 通过重新索引其参数来作用于隐藏单元。置换后的参数集为 $\\theta^{\\pi} = \\{(w_{\\pi(k)}, b_{\\pi(k)}, v_{\\pi(k)})_{k=1}^{n}, c\\}$。使用这些置换后参数的网络函数为：\n$$ f_{\\theta^{\\pi}}(x) = \\sum_{k=1}^{n} v_{\\pi(k)} \\phi\\left(w_{\\pi(k)}^{\\top} x + b_{\\pi(k)}\\right) + c $$\n令 $j = \\pi(k)$。当 $k$ 从 $1$ 遍历到 $n$ 时，$j$ 也会覆盖从 $1$ 到 $n$ 的所有索引，因为 $\\pi$ 是一个置换。因此，求和仅仅是其各项的重新排序，由于加法的交换律，这不会改变和的值：\n$$ f_{\\theta^{\\pi}}(x) = \\sum_{j \\in \\{\\pi(1), \\dots, \\pi(n)\\}} v_j \\phi\\left(w_j^{\\top} x + b_j\\right) + c = \\sum_{j=1}^{n} v_j \\phi\\left(w_j^{\\top} x + b_j\\right) + c = f_{\\theta}(x) $$\n由于对于任何 $\\theta$ 及其置换 $\\theta^{\\pi}$，函数的输出 $f_{\\theta}(x)$ 都是相同的，因此均方误差 (MSE) 损失函数 $\\mathcal{L}(\\theta)$ 在此置换下也是不变的：\n$$ \\mathcal{L}(\\theta^{\\pi}) = \\frac{1}{m} \\sum_{i=1}^{m} \\left(f_{\\theta^{\\pi}}(x_i) - y_i\\right)^2 = \\frac{1}{m} \\sum_{i=1}^{m} \\left(f_{\\theta}(x_i) - y_i\\right)^2 = \\mathcal{L}(\\theta) $$\n这证实了损失函数具有 $S_n$ 置换对称性。\n\n**1.2. 全批量梯度下降对对称性的保持**\n\n我们现在论证，如果所有隐藏单元被相同地初始化，那么在全批量梯度下降的每一步中，这种对称性都会被保持。我们使用归纳法进行论证。\n\n*基础情形（初始化）：* 参数被初始化为对所有 $k \\in \\{1, \\dots, n\\}$ 都有 $(w_k^{(0)}, b_k^{(0)}, v_k^{(0)}) = (\\mathbf{w}_0, \\mathbf{b}_0, \\mathbf{v}_0)$。在步骤 $t=0$ 时，对称性成立。\n\n*归纳步骤：* 假设在某个步骤 $t$，所有隐藏单元的参数都是相同的：对所有 $k$ 都有 $(w_k^{(t)}, b_k^{(t)}, v_k^{(t)}) = (\\mathbf{w}_t, \\mathbf{b}_t, \\mathbf{v}_t)$。我们必须证明在一个梯度下降步骤之后，参数仍然保持相同，即对于所有 $k$，$(w_k^{(t+1)}, b_k^{(t+1)}, v_k^{(t+1)})$ 都是相同的。\n\n参数 $\\vartheta$ 的梯度下降更新规则是 $\\vartheta^{(t+1)} = \\vartheta^{(t)} - \\eta \\nabla_{\\vartheta} \\mathcal{L}(\\theta^{(t)})$。我们需要证明对于每个隐藏单元的参数，其梯度是相同的。\n\n让我们检查任意一个隐藏单元 $k$ 的梯度分量。对于全批量梯度下降，$B=m$。\n关于输出权重 $v_k$ 的梯度是：\n$$ \\frac{\\partial \\mathcal{L}}{\\partial v_k} = \\frac{2}{m}\\sum_{i=1}^{m} \\left(f_{\\theta^{(t)}}(x_i) - y_i\\right) \\phi\\left((w_k^{(t)})^{\\top} x_i + b_k^{(t)}\\right) $$\n根据归纳假设，对所有 $k$ 都有 $w_k^{(t)} = \\mathbf{w}_t$ 和 $b_k^{(t)} = \\mathbf{b}_t$。这意味着 $\\phi(\\dots)$ 项对所有单元都是相同的。此外，函数值 $f_{\\theta^{(t)}}(x_i) = \\sum_{j=1}^n v_j^{(t)} \\phi((w_j^{(t)})^{\\top} x_i + b_j^{(t)}) + c^{(t)}$ 也与单元索引 $k$ 的选择无关，因为所有单元对总和的贡献是相同的。因此，$\\frac{\\partial \\mathcal{L}}{\\partial v_k}$ 的整个表达式对所有 $k$ 都是相同的。\n\n关于输入权重 $w_k$ 的梯度是：\n$$ \\frac{\\partial \\mathcal{L}}{\\partial w_k} = \\frac{2}{m}\\sum_{i=1}^{m} \\left(f_{\\theta^{(t)}}(x_i) - y_i\\right) v_k^{(t)} \\left(1 - \\phi\\left((w_k^{(t)})^{\\top} x_i + b_k^{(t)}\\right)^2\\right) x_i $$\n根据归纳假设，$v_k^{(t)} = \\mathbf{v}_t$，$w_k^{(t)} = \\mathbf{w}_t$，$b_k^{(t)} = \\mathbf{b}_t$。和之前一样，$f_{\\theta^{(t)}}(x_i)$ 是公共项。因此，求和内的所有项对于任何 $k$ 的选择都是相同的。因此，$\\frac{\\partial \\mathcal{L}}{\\partial w_k}$ 对所有 $k$ 都是相同的。\n\n类似的论证也适用于偏置项梯度：\n$$ \\frac{\\partial \\mathcal{L}}{\\partial b_k} = \\frac{2}{m}\\sum_{i=1}^{m} \\left(f_{\\theta^{(t)}}(x_i) - y_i\\right) v_k^{(t)} \\left(1 - \\phi\\left((w_k^{(t)})^{\\top} x_i + b_k^{(t)}\\right)^2\\right) $$\n这个梯度对所有 $k$ 也都是相同的。\n\n由于梯度 $(\\nabla_{w_k}\\mathcal{L}, \\nabla_{b_k}\\mathcal{L}, \\nabla_{v_k}\\mathcal{L})$ 对所有单元 $k$ 都是相同的，并且参数 $(w_k^{(t)}, b_k^{(t)}, v_k^{(t)})$ 也是相同的，更新步骤\n$$ (w_k^{(t+1)}, b_k^{(t+1)}, v_k^{(t+1)}) = (w_k^{(t)}, b_k^{(t)}, v_k^{(t)}) - \\eta (\\nabla_{w_k}\\mathcal{L}, \\nabla_{b_k}\\mathcal{L}, \\nabla_{v_k}\\mathcal{L}) $$\n会产生一组对所有单元同样相同的新参数。\n\n根据归纳法，从相同的参数开始，全批量梯度下降在所有训练步骤中都维持这种对称性。这导致所有隐藏单元学习完全相同的特征，并且权重向量 $w_k$ 保持共线（在这种情况下是相同的），从而导致多样性度量 $\\Delta(W) = 0$。对此对称设置的任何偏离（例如，随机梯度、梯度噪声、dropout 或非相同初始化）都将破坏对称性，并导致隐藏单元多样化。\n\n### 第 2 部分：实现\n\n以下程序实现了指定的四种训练方案，并为每种方案计算特征多样性度量 $\\Delta(W)$。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\n# Global constants from problem\nM = 64\nD = 2\nN = 3\nETA = 0.05\nEPOCHS = 200\nNUM_THRESHOLD = 1e-12\n\ndef phi(z):\n    \"\"\"Tanh activation function.\"\"\"\n    return np.tanh(z)\n\ndef phi_prime(z):\n    \"\"\"Derivative of the tanh activation function.\"\"\"\n    return 1 - np.tanh(z)**2\n\ndef calculate_diversity(w_matrix):\n    \"\"\"\n    Computes the mean pairwise angle between rows of the weight matrix W.\n    \"\"\"\n    total_angle = 0.0\n    num_pairs = 0\n    for i in range(N):\n        for j in range(i + 1, N):\n            w_i = w_matrix[i]\n            w_j = w_matrix[j]\n            \n            norm_i = np.linalg.norm(w_i)\n            norm_j = np.linalg.norm(w_j)\n            \n            denominator = norm_i * norm_j\n            if denominator  NUM_THRESHOLD:\n                angle = 0.0\n            else:\n                dot_product = np.dot(w_i, w_j)\n                # Clip for numerical stability of arccos\n                cosine_sim = np.clip(dot_product / denominator, -1.0, 1.0)\n                angle = np.arccos(cosine_sim)\n            \n            total_angle += angle\n            num_pairs += 1\n            \n    return total_angle / num_pairs if num_pairs > 0 else 0.0\n\ndef run_case(case_num, seed, batch_size, q, noise_std, initial_perturbation, X, Y):\n    \"\"\"\n    Runs a single training experiment as specified by the case parameters.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n\n    # Initialize parameters\n    W = np.array([[0.1, -0.2]] * N, dtype=np.float64)\n    b = np.zeros(N, dtype=np.float64)\n    v = np.ones(N, dtype=np.float64) / N\n    c = 0.0\n\n    # Apply perturbation for Case 4\n    if initial_perturbation > 0:\n        W[0, 0] += initial_perturbation\n    \n    indices = np.arange(M)\n\n    for epoch in range(EPOCHS):\n        if batch_size  M:  # For SGD, shuffle data each epoch\n            rng.shuffle(indices)\n\n        for i in range(0, M, batch_size):\n            batch_indices = indices[i:i + batch_size]\n            X_batch = X[batch_indices]\n            Y_batch = Y[batch_indices]\n            \n            B = X_batch.shape[0]\n\n            # Forward pass\n            Z = X_batch @ W.T + b\n            A_pre = phi(Z)\n            \n            dropout_mask = None\n            if q  1.0:\n                dropout_mask = rng.binomial(1, q, size=A_pre.shape)\n                A_post = A_pre * dropout_mask / q\n            else:\n                A_post = A_pre\n\n            Y_pred = A_post @ v + c\n            \n            # Backward pass (compute gradients)\n            error_term = (2 / B) * (Y_pred - Y_batch)  # shape (B,)\n            \n            grad_c = np.sum(error_term)\n            grad_v = A_post.T @ error_term  # shape (n,)\n            \n            delta_out = error_term[:, np.newaxis] * v  # shape (B, n)\n            \n            dZ = delta_out * (1 - A_pre**2) # phi_prime(Z) is 1-phi(Z)^2 = 1-A_pre^2\n            \n            if q  1.0:\n                dZ = dZ * dropout_mask / q\n            \n            grad_b = np.sum(dZ, axis=0)  # shape (n,)\n            grad_W = dZ.T @ X_batch      # shape (n, d)\n\n            # Add gradient noise for Case 2\n            if noise_std > 0:\n                grad_W += rng.normal(0, noise_std, size=grad_W.shape)\n                grad_b += rng.normal(0, noise_std, size=grad_b.shape)\n                grad_v += rng.normal(0, noise_std, size=grad_v.shape)\n                grad_c += rng.normal(0, noise_std)\n\n            # Update parameters\n            W -= ETA * grad_W\n            b -= ETA * grad_b\n            v -= ETA * grad_v\n            c -= ETA * grad_c\n\n    return calculate_diversity(W)\n\ndef solve():\n    # Dataset generation\n    data_rng = np.random.default_rng(2025)\n    X = data_rng.normal(0, 1, size=(M, D))\n    Y = np.sin(X[:, 0]) + 0.5 * np.cos(2 * X[:, 1])\n\n    # Test suite definition\n    test_cases = [\n        # (case_num, seed, batch_size, q, noise_std, initial_perturbation)\n        (1, 101, M, 1.0, 0.0, 0.0),            # Case 1: Full-batch GD\n        (2, 102, M, 1.0, 1e-3, 0.0),           # Case 2: Gradient noise\n        (3, 103, 1, 0.5, 0.0, 0.0),            # Case 3: SGD with dropout\n        (4, 104, M, 1.0, 0.0, 1e-3),           # Case 4: Initial perturbation\n    ]\n\n    results = []\n    for case_params in test_cases:\n        result = run_case(*case_params, X, Y)\n        results.append(result)\n\n    print(f\"[{','.join([f'{r:.6f}' for r in results])}]\")\n\nsolve()\n```", "id": "3134207"}, {"introduction": "在理解了神经元如何分化学习之后，我们必须深入探究它们是如何被“告知”要如何变化的。本练习 [@problem_id:3134219] 将聚焦于分类模型的核心引擎——Softmax交叉熵损失函数的梯度。你将亲手推导这个至关重要的公式，并通过计算来探索其行为特性，从而揭示梯度饱和等现象以及像标签平滑这类正则化技术的内在作用。", "problem": "你将分析人工神经网络 (ANN) 背景下的 Softmax 交叉熵 (SCE) 损失的梯度。你的任务包括：从第一性原理出发，推导梯度如何依赖于 logits；将梯度大小与 logit 差异（间隔）联系起来；并检验标签平滑对梯度饱和的影响。然后，你必须实现一个程序，为给定的测试套件计算量化的梯度指标。\n\n基础知识（仅定义）：\n- 给定一个 logits 向量 $\\mathbf{z} \\in \\mathbb{R}^K$，softmax 函数会生成类别概率 $\\mathbf{p} \\in \\mathbb{R}^K$，其中 $p_i \\ge 0$ 且 $\\sum_{i=1}^K p_i = 1$。\n- 对于一个目标分布 $\\mathbf{y} \\in \\mathbb{R}^K$，其中 $y_i \\ge 0$ 且 $\\sum_{i=1}^K y_i = 1$，交叉熵损失是 $\\mathbf{p}$ 的一个标量函数。\n- 标签平滑通过为正确类别索引 $c$ 分配 $y_c = 1 - \\varepsilon$ 以及为所有 $j \\ne c$ 分配 $y_j = \\varepsilon/(K-1)$ 来修改独热（one-hot）目标，其中 $\\varepsilon \\in [0,1)$。\n\n要求的推导与分析：\n1) 严格从上述定义和基础微积分（商法则、链式法则以及指数和对数的性质）出发，推导 Softmax 交叉熵损失关于 logits $\\mathbf{z}$ 的梯度。给出针对任意目标分布 $\\mathbf{y}$ 的通用结果，然后将其特化到独热目标和标签平滑目标。不要假设任何已知的梯度公式；必须自行推导。\n2) 使用你的梯度表达式以及 softmax 概率仅依赖于 logits 的差异（平移不变性）这一事实，分析梯度大小如何与 logit 差异相关联。形式化正确类别 logit 间隔 $\\Delta = z_c - \\max_{j \\ne c} z_j$ 的概念，并使用你的公式解释：为什么对于独热目标，大的正 $\\Delta$ 会导致小梯度（饱和）；为什么大的负 $\\Delta$ 会导致大梯度；以及标签平滑（$\\varepsilon  0$）即使在模型非常自信时，如何改变这些梯度的大小。\n3) 实现一个程序，为每个测试用例计算以下量值：\n   - 使用数值稳定方法计算的 softmax 概率 $\\mathbf{p}$。\n   - 在标签平滑参数 $\\varepsilon$ 下，关于 logits 的梯度向量 $\\nabla_{\\mathbf{z}} L$。\n   - 正确类别的间隔 $\\Delta = z_c - \\max_{j \\ne c} z_j$。\n   - 欧几里得范数（也称 $\\ell_2$ 范数）$\\lVert \\nabla_{\\mathbf{z}} L \\rVert_2$。\n   - 正确类别的绝对梯度 $\\lvert (\\nabla_{\\mathbf{z}} L)_c \\rvert$。\n   - 所有类别中的最大绝对梯度 $\\max_i \\lvert (\\nabla_{\\mathbf{z}} L)_i \\rvert$。\n\n实现要求：\n- 使用自然对数。\n- 使用数值稳定的 softmax 实现（你可以在求指数前从所有 logits 中减去 $\\max_i z_i$）。\n- 对于每个测试用例，返回一个包含四个浮点数的列表 $[\\Delta, \\lVert \\nabla_{\\mathbf{z}} L \\rVert_2, \\lvert (\\nabla_{\\mathbf{z}} L)_c \\rvert, \\max_i \\lvert (\\nabla_{\\mathbf{z}} L)_i \\rvert]$，每个浮点数四舍五入到 $6$ 位小数。\n- 你的程序应生成单行输出，其中包含所有测试用例的结果，格式为逗号分隔的列表的列表，不含空格，并用方括号括起来。例如，输出格式必须类似于 $[[a\\_1,b\\_1,c\\_1,d\\_1],[a\\_2,b\\_2,c\\_2,d\\_2],\\dots]$，其中每个 $a_i,b_i,c_i,d_i$ 都是小数点后恰好有 $6$ 位的小数。\n\n测试套件（每个测试用例是一个三元组 $(\\mathbf{z}, c, \\varepsilon)$）：\n- 用例 1：$\\mathbf{z} = [0.1, 0.2, 0.15, 0.05]$，$c = 1$，$\\varepsilon = 0.0$。\n- 用例 2：$\\mathbf{z} = [8.0, 1.0, 0.0, -2.0]$，$c = 0$，$\\varepsilon = 0.0$。\n- 用例 3：$\\mathbf{z} = [-2.0, 5.0, -1.0, 0.0]$，$c = 0$，$\\varepsilon = 0.0$。\n- 用例 4：$\\mathbf{z} = [1000.0, 1000.0, 1000.0, 1000.0]$，$c = 2$，$\\varepsilon = 0.0$。\n- 用例 5：$\\mathbf{z} = [8.0, 1.0, 0.0, -2.0]$，$c = 0$，$\\varepsilon = 0.1$。\n- 用例 6：$\\mathbf{z} = [-2.0, 5.0, -1.0, 0.0]$，$c = 0$，$\\varepsilon = 0.1$。\n\n最终输出规范：\n- 程序必须精确打印一行：一个顶层列表，其中包含 $6$ 个内部列表，每个测试用例一个，按上面列出的顺序排列。\n- 每个内部列表必须是 $[\\Delta, \\lVert \\nabla_{\\mathbf{z}} L \\rVert_2, \\lvert (\\nabla_{\\mathbf{z}} L)_c \\rvert, \\max_i \\lvert (\\nabla_{\\mathbf{z}} L)_i \\rvert]$，每个浮点数四舍五入到恰好 $6$ 位小数。\n- 输出行中任何位置都不得有空格。", "solution": "该问题是有效的，因为它具有科学依据、问题定义明确且客观。它包含一个标准的、非平凡的推导，一个植根于该推导的概念性分析，以及一个具体的实现任务，所有这些都是人工神经网络基础知识的核心。\n\n### 1. Softmax 交叉熵损失梯度的推导\n\n我们的任务是推导 Softmax 交叉熵 (SCE) 损失关于输入 logits $\\mathbf{z}$ 的梯度。\n\n**定义：**\n- Logits: $\\mathbf{z} = [z_1, z_2, \\dots, z_K]^T \\in \\mathbb{R}^K$。\n- 类别 $i$ 的 Softmax 概率：$p_i = \\frac{e^{z_i}}{\\sum_{j=1}^K e^{z_j}}$。\n- 目标概率分布：$\\mathbf{y} = [y_1, y_2, \\dots, y_K]^T$，其中 $y_i \\ge 0$ 且 $\\sum_{i=1}^K y_i = 1$。\n- 交叉熵损失：$L(\\mathbf{p}, \\mathbf{y}) = -\\sum_{i=1}^K y_i \\log p_i$。注意此处的对数是自然对数 $\\ln$。\n\n我们的目标是计算梯度向量 $\\nabla_{\\mathbf{z}} L$，其分量为 $\\frac{\\partial L}{\\partial z_k}$（$k \\in \\{1, \\dots, K\\}$）。我们应用链式法则：\n$$\n\\frac{\\partial L}{\\partial z_k} = \\sum_{i=1}^K \\frac{\\partial L}{\\partial p_i} \\frac{\\partial p_i}{\\partial z_k}\n$$\n\n**步骤 1.1：计算 $\\frac{\\partial L}{\\partial p_i}$**\n损失为 $L = -\\sum_{j=1}^K y_j \\log p_j$。关于特定概率 $p_i$ 的偏导数很简单：\n$$\n\\frac{\\partial L}{\\partial p_i} = \\frac{\\partial}{\\partial p_i} \\left( -y_i \\log p_i - \\sum_{j \\ne i} y_j \\log p_j \\right) = -\\frac{y_i}{p_i}\n$$\n\n**步骤 1.2：计算 $\\frac{\\partial p_i}{\\partial z_k}$ (Softmax 函数的雅可比矩阵)**\n对于 $p_i = \\frac{e^{z_i}}{\\sum_{j=1}^K e^{z_j}}$ 关于 $z_k$ 的导数，我们必须考虑两种情况。令 $D = \\sum_{j=1}^K e^{z_j}$。\n\n情况 A：$i = k$。我们使用商法则 $\\left(\\frac{f}{g}\\right)' = \\frac{f'g - fg'}{g^2}$。\n$$\n\\frac{\\partial p_k}{\\partial z_k} = \\frac{\\frac{\\partial(e^{z_k})}{\\partial z_k} D - e^{z_k} \\frac{\\partial D}{\\partial z_k}}{D^2} = \\frac{e^{z_k} D - e^{z_k} e^{z_k}}{D^2} = \\frac{e^{z_k}}{D} \\frac{D - e^{z_k}}{D} = p_k (1 - p_k)\n$$\n\n情况 B：$i \\ne k$。\n$$\n\\frac{\\partial p_i}{\\partial z_k} = \\frac{\\frac{\\partial(e^{z_i})}{\\partial z_k} D - e^{z_i} \\frac{\\partial D}{\\partial z_k}}{D^2} = \\frac{0 \\cdot D - e^{z_i} e^{z_k}}{D^2} = -\\frac{e^{z_i}}{D} \\frac{e^{z_k}}{D} = -p_i p_k\n$$\n\n**步骤 1.3：组合导数**\n现在我们将这些结果代回到 $\\frac{\\partial L}{\\partial z_k}$ 的链式法则表达式中：\n$$\n\\frac{\\partial L}{\\partial z_k} = \\sum_{i=1}^K \\frac{\\partial L}{\\partial p_i} \\frac{\\partial p_i}{\\partial z_k} = \\left(\\frac{\\partial L}{\\partial p_k} \\frac{\\partial p_k}{\\partial z_k}\\right) + \\sum_{i \\ne k} \\left(\\frac{\\partial L}{\\partial p_i} \\frac{\\partial p_i}{\\partial z_k}\\right)\n$$\n$$\n\\frac{\\partial L}{\\partial z_k} = \\left(-\\frac{y_k}{p_k}\\right) (p_k(1 - p_k)) + \\sum_{i \\ne k} \\left(-\\frac{y_i}{p_i}\\right) (-p_i p_k)\n$$\n$$\n\\frac{\\partial L}{\\partial z_k} = -y_k(1 - p_k) + \\sum_{i \\ne k} y_i p_k = -y_k + y_k p_k + p_k \\sum_{i \\ne k} y_i\n$$\n提出 $p_k$：\n$$\n\\frac{\\partial L}{\\partial z_k} = p_k \\left(y_k + \\sum_{i \\ne k} y_i \\right) - y_k\n$$\n因为 $\\mathbf{y}$ 是一个概率分布，所以 $\\sum_{i=1}^K y_i = y_k + \\sum_{i \\ne k} y_i = 1$。这可将表达式简化为：\n$$\n\\frac{\\partial L}{\\partial z_k} = p_k(1) - y_k = p_k - y_k\n$$\n这个简洁的结果表明，Softmax 交叉熵损失关于 logit $z_k$ 的梯度是预测概率 $p_k$ 与目标概率 $y_k$ 之间的差。写成向量形式，梯度为：\n$$\n\\nabla_{\\mathbf{z}} L = \\mathbf{p} - \\mathbf{y}\n$$\n\n**特例：**\n1.  **独热（One-Hot）目标：** 对于正确的类别索引 $c$，目标向量 $\\mathbf{y}$ 满足 $y_c = 1$ 且对于所有 $j \\ne c$ 都有 $y_j = 0$。梯度分量为：\n    - $(\\nabla_{\\mathbf{z}} L)_c = p_c - 1$\n    - $(\\nabla_{\\mathbf{z}} L)_j = p_j - 0 = p_j \\quad (\\text{对于 } j \\ne c)\n\n2.  **标签平滑目标：** 对于平滑参数 $\\varepsilon \\in [0, 1)$，目标向量 $\\mathbf{y}$ 定义为 $y_c = 1 - \\varepsilon$ 且对于所有 $j \\ne c$ 都有 $y_j = \\frac{\\varepsilon}{K-1}$。梯度分量为：\n    - $(\\nabla_{\\mathbf{z}} L)_c = p_c - (1 - \\varepsilon)$\n    - $(\\nabla_{\\mathbf{z}} L)_j = p_j - \\frac{\\varepsilon}{K-1} \\quad (\\text{对于 } j \\ne c)\n\n### 2. 梯度大小与 Logit 间隔的分析\n\nSoftmax 函数对于所有 logits 的常数平移具有不变性，即对于任意标量 $C$，$\\text{softmax}(\\mathbf{z} + C) = \\text{softmax}(\\mathbf{z})$。这意味着概率 $\\mathbf{p}$ 仅取决于 logits 之间的*差值*。\n\n我们将正确类别的 logit 间隔定义为 $\\Delta = z_c - \\max_{j \\ne c} z_j$。此间隔衡量模型对正确类别 $c$ 的“置信度”比对最可能的错误类别高多少。\n\n**独热目标（$\\varepsilon = 0$）的分析：**\n- **大的正间隔（$\\Delta \\to \\infty$）：** 如果 $z_c$ 远大于所有其他的 $z_j$，则模型对正确类别非常自信。\n    - $p_c = \\frac{e^{z_c}}{e^{z_c} + \\sum_{j \\ne c} e^{z_j}} = \\frac{1}{1 + \\sum_{j \\ne c} e^{z_j - z_c}} \\to 1$ 因为 $z_j - z_c \\to -\\infty$。\n    - 因此，对于 $j \\ne c$ 有 $p_j \\to 0$。\n    - 梯度分量变为：\n        - $(\\nabla_{\\mathbf{z}} L)_c = p_c - 1 \\to 1 - 1 = 0$。\n        - $(\\nabla_{\\mathbf{z}} L)_j = p_j \\to 0$。\n    - 整个梯度向量 $\\nabla_{\\mathbf{z}} L$ 趋近于零。这种现象被称为**梯度饱和**。随着模型变得更加自信，它接收到的学习信号会减弱，从而有效地停止对该样本的学习。\n\n- **大的负间隔（$\\Delta \\to -\\infty$）：** 如果 $z_c$ 远小于最大的不正确 logit（假设为 $z_m = \\max_{j \\ne c} z_j$），则模型对一个错误的类别非常自信。\n    - $p_c \\to 0$。\n    - 正确类别 logit 的梯度为 $(\\nabla_{\\mathbf{z}} L)_c = p_c - 1 \\to -1$。这是一个增加 $z_c$ 的强烈信号。\n    - “获胜”的错误类别 $m$ 的概率趋近于 $1$，即 $p_m \\to 1$。\n    - 这个不正确 logit 的梯度为 $(\\nabla_{\\mathbf{z}} L)_m = p_m \\to 1$。这是一个减小 $z_m$ 的强烈信号。\n    - 梯度值很大，为模型提供了强烈的修正信号。\n\n**标签平滑（$\\varepsilon  0$）的影响：**\n标签平滑改变了目标分布，这反过来又改变了梯度行为，尤其是在高置信度区域。\n- **大的正间隔（$\\Delta \\to \\infty$）：** 与之前一样，$p_c \\to 1$ 且对于 $j \\ne c$ 有 $p_j \\to 0$。梯度分量变为：\n    - $(\\nabla_{\\mathbf{z}} L)_c = p_c - (1 - \\varepsilon) \\to 1 - (1 - \\varepsilon) = \\varepsilon$。\n    - $(\\nabla_{\\mathbf{z}} L)_j = p_j - \\frac{\\varepsilon}{K-1} \\to 0 - \\frac{\\varepsilon}{K-1} = -\\frac{\\varepsilon}{K-1}$。\n- 与独热情况不同，梯度分量不会消失。\n    - 正确 logit 的梯度 $\\varepsilon$ 是一个小的正值。这通过鼓励 $z_c$ 不要无限大于其他 logits 来惩罚过度自信。$p_c$ 的目标现在是 $1-\\varepsilon$，而不是 $1$。\n    - 不正确 logits 的梯度是小的负值，鼓励它们的 logits $z_j$ 略微增加。\n- 这种持续存在的非零梯度可以防止模型变得过度确定，起到正则化器的作用，从而可以改善泛化能力和模型校准。\n\n### 3. 实现细节\n\n该实现为每个测试用例计算所需的指标。\n1.  **稳定的 Softmax：** 为防止 logits 过大导致数值上溢/下溢，我们利用平移不变性，在求指数前从所有 logits 中减去最大 logit 值：$p_i = \\frac{e^{z_i - \\max(\\mathbf{z})}}{\\sum_j e^{z_j - \\max(\\mathbf{z})}}$。\n2.  **目标向量构建：** 基于正确类别索引 $c$、类别数 $K$ 和平滑参数 $\\varepsilon$ 创建目标向量 $\\mathbf{y}$。\n3.  **梯度计算：** 梯度 $\\nabla_{\\mathbf{z}} L$ 简单地计算为 $\\mathbf{p} - \\mathbf{y}$。\n4.  **指标计算：**\n    - 通过在不正确的类别中找到最大的 logit，并从正确类别的 logit 中减去它，来计算间隔 $\\Delta$。\n    - 梯度向量的欧几里得范数（$\\ell_2$ 范数）使用 `np.linalg.norm` 计算。\n    - 正确类别索引处的绝对梯度和所有类别中的最大绝对梯度使用 `np.abs` 和 `np.max` 找到。\n5.  **输出格式化：** 结果被格式化为一个表示列表的列表的字符串，每个浮点数格式化为恰好六位小数，且不含空格。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Analyzes and computes gradient metrics for the Softmax Cross-Entropy loss\n    for a given suite of test cases.\n    \"\"\"\n    # Test suite: each case is a tuple (z, c, epsilon)\n    # z: logits vector (list of floats)\n    # c: correct class index (int)\n    # epsilon: label smoothing parameter (float)\n    test_cases = [\n        ([0.1, 0.2, 0.15, 0.05], 1, 0.0),\n        ([8.0, 1.0, 0.0, -2.0], 0, 0.0),\n        ([-2.0, 5.0, -1.0, 0.0], 0, 0.0),\n        ([1000.0, 1000.0, 1000.0, 1000.0], 2, 0.0),\n        ([8.0, 1.0, 0.0, -2.0], 0, 0.1),\n        ([-2.0, 5.0, -1.0, 0.0], 0, 0.1),\n    ]\n\n    all_results = []\n    \n    for z_list, c, epsilon in test_cases:\n        z = np.array(z_list, dtype=np.float64)\n        K = len(z)\n\n        # 1. Compute softmax probabilities (numerically stable)\n        z_max = np.max(z)\n        exp_z = np.exp(z - z_max)\n        p = exp_z / np.sum(exp_z)\n\n        # 2. Compute the gradient vector (grad_L = p - y)\n        if K > 1:\n            y = np.full(K, epsilon / (K - 1))\n        else: # Handle edge case of K=1, though not in tests\n            y = np.array([1.0])\n        y[c] = 1.0 - epsilon\n        \n        grad_L = p - y\n\n        # 3. Compute the correct-class margin Delta\n        if K > 1:\n            mask = np.ones(K, dtype=bool)\n            mask[c] = False\n            max_z_incorrect = np.max(z[mask])\n            delta = z[c] - max_z_incorrect\n        else: # Only one class, so margin is ill-defined, use 0\n            delta = 0.0\n\n        # 4. Compute the required gradient metrics\n        norm_grad_L = np.linalg.norm(grad_L)\n        abs_grad_c = np.abs(grad_L[c])\n        max_abs_grad = np.max(np.abs(grad_L))\n\n        # Store the four required floats\n        case_result = [delta, norm_grad_L, abs_grad_c, max_abs_grad]\n        all_results.append(case_result)\n\n    # Format the final output string exactly as required.\n    # e.g., [[val1,val2,...],[...],...] with no spaces and 6 decimal places.\n    inner_results_str = []\n    for result_vector in all_results:\n        formatted_values = [f'{v:.6f}' for v in result_vector]\n        inner_results_str.append(f\"[{','.join(formatted_values)}]\")\n\n    final_output = f\"[{','.join(inner_results_str)}]\"\n    print(final_output)\n\nsolve()\n```", "id": "3134219"}, {"introduction": "现在，我们将网络结构和学习动力学的知识结合起来，进行一个受控的模拟实验。通过一个“教师-学生”模型 [@problem_id:3134222]，你将从一个已知的“教师”网络生成数据，然后训练一个“学生”网络，观察它是否能恢复出教师网络的原始参数。这个实践将为你提供关于样本复杂度、过参数化以及网络从数据中“学习”真实函数意味着什么等核心概念的具体见解。", "problem": "给定一个双层人工神经网络（ANN）的教师-学生模型，其中教师网络是已知的，学生网络试图从教师网络生成的数据中恢复其参数。教师网络有一个隐藏层，使用双曲正切作为激活函数。学生网络具有相同的架构，但可能有不同的隐藏层宽度，以模拟过参数化的情况。你的任务是实现一个完整的程序，该程序从教师网络生成数据，通过最小化经验均方误差来训练学生网络，然后评估在不同样本量和宽度下，学生网络是否在置换不变性下成功恢复了教师网络的隐藏单元。\n\n从以下基本概念开始：\n- 一个参数向量为 $w \\in \\mathbb{R}^d$、输入为 $x \\in \\mathbb{R}^d$ 的人工神经元，对于一个平滑的非线性函数 $\\sigma$，其输出为 $\\sigma(w^\\top x)$。这里，使用双曲正切激活函数 $\\sigma(z) = \\tanh(z)$。\n- 一个隐藏层宽度为 $H$、输出为标量的双层网络计算 $f(x) = \\sum_{j=1}^{H} a_j \\sigma(w_j^\\top x)$，其中 $\\{w_j\\}_{j=1}^{H}$ 是隐藏层权重，$\\{a_j\\}_{j=1}^{H}$ 是输出层权重。\n- 使用均方误差的经验风险最小化定义了目标函数 $L(W,a) = \\frac{1}{m} \\sum_{k=1}^{m} \\left(f_S(x_k;W,a) - y_k\\right)^2$，其中 $\\{(x_k,y_k)\\}_{k=1}^{m}$ 是训练集， $W = [w_1,\\dots,w_{H_s}]^\\top$ 集合了学生网络中的隐藏层权重， $a = [a_1,\\dots,a_{H_s}]^\\top$ 集合了输出层权重。\n- 全批量梯度下降通过沿 $L(W,a)$ 相对于 $W$ 和 $a$ 的负梯度方向更新参数。\n\n教师网络和数据生成：\n- 固定输入维度 $d \\in \\mathbb{N}$ 和教师网络隐藏层宽度 $H_t \\in \\mathbb{N}$。\n- 教师函数为 $f_T(x) = \\sum_{j=1}^{H_t} v_j \\tanh(u_j^\\top x)$，其中教师网络隐藏层权重为 $\\{u_j \\in \\mathbb{R}^d\\}_{j=1}^{H_t}$，输出层权重为 $\\{v_j \\in \\mathbb{R}\\}_{j=1}^{H_t}$。训练输入 $x_k$ 对于 $k=1,\\dots,m$ 是独立地从 $\\mathcal{N}(0,I_d)$ 中抽取的。输出为 $y_k = f_T(x_k) + \\epsilon_k$，其中 $\\epsilon_k \\sim \\mathcal{N}(0,\\sigma^2)$ 是独立噪声。\n- 学生函数为 $f_S(x;W,a) = \\sum_{i=1}^{H_s} a_i \\tanh(w_i^\\top x)$，其中 $H_s \\in \\mathbb{N}$ 是学生网络隐藏层宽度。\n\n训练：\n- 将学生网络的参数 $(W,a)$ 随机初始化为较小的值。\n- 使用学习率 $\\eta$ 在训练数据 $\\{(x_k,y_k)\\}_{k=1}^{m}$ 上执行 $T$ 步全批量梯度下降，以最小化 $L(W,a)$。\n- 使用链式法则推导关于 $W$ 和 $a$ 的梯度。\n\n恢复准则：\n- 因为隐藏单元仅在置换意义下是可识别的，所以使用一个从 $\\mathcal{N}(0,I_d)$ 中独立同分布抽取的验证集 $\\{x^\\text{(val)}_\\ell\\}_{\\ell=1}^{m_\\text{val}}$，来定义教师和学生隐藏单元之间的置换不变匹配。\n- 对于每个教师单元 $j \\in \\{1,\\dots,H_t\\}$ 和学生单元 $i \\in \\{1,\\dots,H_s\\}$，定义在验证输入上的贡献向量：\n  $$c_{T,j} = \\left[v_j \\tanh\\left(u_j^\\top x^\\text{(val)}_\\ell\\right)\\right]_{\\ell=1}^{m_\\text{val}}, \\quad c_{S,i} = \\left[a_i \\tanh\\left(w_i^\\top x^\\text{(val)}_\\ell\\right)\\right]_{\\ell=1}^{m_\\text{val}}.$$\n- 定义配对成本\n  $$C_{j,i} = \\frac{1}{m_\\text{val}} \\sum_{\\ell=1}^{m_\\text{val}} \\left(c_{S,i}(\\ell) - c_{T,j}(\\ell)\\right)^2.$$\n- 使用一个线性指派程序找到教师单元到学生单元的最小成本分配，该程序最小化所选配对的 $C_{j,i}$ 之和。如果 $H_s  H_t$，一些教师单元将保持未匹配状态；对每个未匹配的教师单元施加一个固定的惩罚成本 $P  0$。\n- 令 $J^\\star$ 为已匹配的教师-学生配对集合。定义每个教师单元的平均匹配成本\n  $$\\overline{C} = \\frac{1}{H_t} \\left( \\sum_{(j,i)\\in J^\\star} C_{j,i} + P \\cdot \\left(H_t - |J^\\star|\\right) \\right).$$\n- 对于一个固定的阈值 $\\tau  0$，如果 $\\overline{C} \\leq \\tau$，则宣布恢复成功。\n\n研究样本复杂度的影响：\n- 改变样本数量 $m$ 和学生网络宽度 $H_s$（相对于教师网络宽度 $H_t$），并评估是否发生恢复。过参数化对应于 $H_s  H_t$。\n\n程序要求行为：\n- 实现完整的流程：教师网络生成、数据生成、通过梯度下降训练学生网络、验证集生成、通过指派进行置换不变匹配以及恢复决策。\n- 使用 $m_\\text{val} = 512$ 个验证输入。\n- 使用超参数：训练步数 $T = 2000$，学习率 $\\eta = 0.05$，未匹配惩罚 $P = 2.0$，恢复阈值 $\\tau = 0.12$。在训练期间，对 $W$ 包含一个强度为 $\\lambda_w = 10^{-4}$ 的少量 $\\ell_2$ 正则化，以稳定优化过程。\n- 按照下面的测试套件中的规定，将所有随机数生成器设置为固定的种子以保证可复现性。\n\n测试套件：\n在以下五个案例上评估程序。每个案例是一个元组 $(d,H_t,H_s,m,\\sigma,\\text{seed})$：\n- 案例 1：$(5,3,3,2000,0.0,12345)$，一个设定正确且样本量大的学生网络。\n- 案例 2：$(5,3,2,2000,0.0,22345)$，一个欠参数化且样本量大的学生网络。\n- 案例 3：$(5,3,6,800,0.0,32345)$，一个过参数化且样本量中等的学生网络。\n- 案例 4：$(5,3,3,50,0.1,42345)$，一个小样本、有噪声的场景。\n- 案例 5：$(5,3,3,200,0.02,52345)$，一个中等样本量、有轻微噪声的场景。\n\n答案规范：\n- 对于每个测试案例，根据准则 $\\overline{C} \\leq \\tau$ 输出一个布尔值，指示是否恢复成功。\n- 你的程序应生成单行输出，其中包含一个按上述测试套件顺序排列的、用方括号括起来的逗号分隔列表。例如，输出格式必须类似于 $[b_1,b_2,b_3,b_4,b_5]$，其中每个 $b_k$ 是 $True$ 或 $False$。", "solution": "该问题要求实现并评估一个用于双层人工神经网络的教师-学生框架。目标是确定一个通过梯度下降训练的学生网络，在模型大小、样本量和噪声等不同条件下，是否能够恢复一个已知教师网络的功能组件。\n\n问题的核心涉及以下步骤序列：\n1.  定义教师和学生网络架构。\n2.  从教师网络生成一个合成数据集。\n3.  通过最小化正则化的均方误差损失，在此数据集上训练学生网络。\n4.  使用置换不变的匹配准则，评估学生网络的隐藏单元对教师网络隐藏单元的恢复情况。\n\n让我们正式地详细说明每个组成部分。\n\n**1. 网络架构和数据生成**\n\n教师和学生网络是具有单个隐藏层和标量输出的双层神经网络。隐藏层神经元的激活函数是双曲正切函数 $\\sigma(z) = \\tanh(z)$。\n\n具有输入维度 $d$ 和隐藏层宽度 $H_t$ 的教师网络函数 $f_T(x)$ 由下式给出：\n$$ f_T(x) = \\sum_{j=1}^{H_t} v_j \\tanh(u_j^\\top x) $$\n其中 $\\{u_j \\in \\mathbb{R}^d\\}_{j=1}^{H_t}$ 是教师网络的隐藏层权重向量，$\\{v_j \\in \\mathbb{R}\\}_{j=1}^{H_t}$ 是教师网络的输出层权重。这些参数是固定的，并被视为真实值。\n\n训练数据集 $\\{(x_k, y_k)\\}_{k=1}^{m}$ 由 $m$ 个样本组成。输入 $x_k$ 独立地从标准正态分布中抽取，$x_k \\sim \\mathcal{N}(0, I_d)$。相应的输出 $y_k$ 由教师网络生成并加上高斯噪声：\n$$ y_k = f_T(x_k) + \\epsilon_k, \\quad \\text{其中} \\quad \\epsilon_k \\sim \\mathcal{N}(0, \\sigma^2) $$\n\n学生网络具有相同的架构，但可能有不同的隐藏层宽度 $H_s$。其函数 $f_S(x)$ 为：\n$$ f_S(x; W, a) = \\sum_{i=1}^{H_s} a_i \\tanh(w_i^\\top x) $$\n这里，$W = [w_1, \\dots, w_{H_s}]^\\top$ 是学生网络的隐藏层权重矩阵（$W \\in \\mathbb{R}^{H_s \\times d}$），$a = [a_1, \\dots, a_{H_s}]^\\top$ 是学生网络的输出层权重向量（$a \\in \\mathbb{R}^{H_s}$）。这些是在训练期间要学习的参数。\n\n**2. 通过梯度下降进行训练**\n\n学生网络通过最小化经验均方误差进行训练，并附加一个关于隐藏层权重的 $\\ell_2$ 正则化项以提高稳定性。损失函数 $L(W, a)$ 为：\n$$ L(W, a) = \\frac{1}{m} \\sum_{k=1}^{m} \\left(f_S(x_k; W, a) - y_k\\right)^2 + \\frac{\\lambda_w}{2} \\|W\\|_F^2 $$\n其中 $\\|W\\|_F^2 = \\sum_{i=1}^{H_s} \\|w_i\\|_2^2$ 是权重矩阵 $W$ 的弗罗贝尼乌斯范数的平方，$\\lambda_w$ 是正则化强度。\n\n训练使用全批量梯度下降进行 $T$ 次迭代。参数根据以下规则更新：\n$$ a^{(t+1)} = a^{(t)} - \\eta \\nabla_a L_t $$\n$$ W^{(t+1)} = W^{(t)} - \\eta \\nabla_W L_t $$\n其中 $\\eta$ 是学习率，梯度是在当前参数值 $(W^{(t)}, a^{(t)})$ 处计算的。\n\n梯度是使用链式法则推导出来的。激活函数的导数是 $\\sigma'(z) = \\frac{d}{dz}\\tanh(z) = 1 - \\tanh^2(z)$。\n令 $\\delta_k = f_S(x_k) - y_k$ 为样本 $k$ 的预测误差。\n\n关于输出权重 $a_i$ 的梯度是：\n$$ \\frac{\\partial L}{\\partial a_i} = \\frac{1}{m} \\sum_{k=1}^{m} 2 \\delta_k \\frac{\\partial f_S(x_k)}{\\partial a_i} = \\frac{2}{m} \\sum_{k=1}^{m} \\delta_k \\tanh(w_i^\\top x_k) $$\n\n关于隐藏层权重向量 $w_i$ 的梯度是：\n$$ \\frac{\\partial L}{\\partial w_i} = \\left( \\frac{1}{m} \\sum_{k=1}^{m} 2 \\delta_k \\frac{\\partial f_S(x_k)}{\\partial w_i} \\right) + \\frac{\\partial}{\\partial w_i}\\left(\\frac{\\lambda_w}{2} \\sum_{j=1}^{H_s} w_j^\\top w_j\\right) $$\n$$ \\frac{\\partial L}{\\partial w_i} = \\left( \\frac{2}{m} \\sum_{k=1}^{m} \\delta_k a_i \\left(1 - \\tanh^2(w_i^\\top x_k)\\right) x_k \\right) + \\lambda_w w_i $$\n\n这些表达式可以使用对整个数据集的向量化操作来高效计算。\n\n**3. 恢复评估准则**\n\n神经网络的隐藏单元仅在置换和符号翻转的意义下是可识别的。例如，将 $(a_i, w_i)$ 变为 $(-a_i, -w_i)$ 不会改变项 $a_i \\tanh(w_i^\\top x)$ 的值，因为 $\\tanh$ 是一个奇函数。此外，重新排序隐藏单元也不会改变网络的输出。为了解决这个问题，恢复的评估使用了一个置换不变的匹配程序。\n\n一个大小为 $m_\\text{val}$ 的验证集 $\\{x^{(\\text{val})}_\\ell\\}_{\\ell=1}^{m_\\text{val}}$ 从 $\\mathcal{N}(0, I_d)$ 中抽取。对于每个教师单元 $j$ 和学生单元 $i$，我们计算它们在此集合上的功能贡献：\n$$ c_{T,j} = \\left[v_j \\tanh\\left(u_j^\\top x^{(\\text{val})}_\\ell\\right)\\right]_{\\ell=1}^{m_\\text{val}} \\in \\mathbb{R}^{m_\\text{val}} $$\n$$ c_{S,i} = \\left[a_i \\tanh\\left(w_i^\\top x^{(\\text{val})}_\\ell\\right)\\right]_{\\ell=1}^{m_\\text{val}} \\in \\mathbb{R}^{m_\\text{val}} $$\n\n定义一个配对成本矩阵 $C \\in \\mathbb{R}^{H_t \\times H_s}$，其中每个条目 $C_{j,i}$ 是教师单元 $j$ 和学生单元 $i$ 贡献向量之间的均方误差：\n$$ C_{j,i} = \\frac{1}{m_\\text{val}} \\sum_{\\ell=1}^{m_\\text{val}} \\left(c_{S,i}(\\ell) - c_{T,j}(\\ell)\\right)^2 $$\n\n寻找教师和学生单元之间最佳匹配的问题对应于一个线性指派问题（或最小权二分匹配）。目标是找到一组配对 $(j,i)$，以最小化总成本 $\\sum C_{j,i}$。这个问题可以使用如匈牙利算法等高效算法解决，该算法在 `scipy.optimize.linear_sum_assignment` 中可用。\n\n令 $J^\\star$ 为找到的最优成本配对集合。匹配的配对数量为 $|J^\\star| = \\min(H_t, H_s)$。如果学生网络是欠参数化的（$H_s  H_t$），一些教师单元将未被匹配。对 $H_t - |J^\\star|$ 个未匹配的教师单元中的每一个都增加一个惩罚 $P$。\n\n然后，计算每个教师单元的平均匹配成本 $\\overline{C}$：\n$$ \\overline{C} = \\frac{1}{H_t} \\left( \\sum_{(j,i)\\in J^\\star} C_{j,i} + P \\cdot \\left(H_t - |J^\\star|\\right) \\right) $$\n\n最后，如果这个平均成本低于给定的阈值 $\\tau$，则宣布恢复成功：\n$$ \\text{恢复} = (\\overline{C} \\leq \\tau) $$\n\n完整的实现将对每个指定的测试案例执行这整个流程，使用固定的随机种子以保证可复现性，并报告二元恢复结果。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import linear_sum_assignment\n\ndef solve():\n    \"\"\"\n    Main function to run the teacher-student ANN simulation for all test cases.\n    \"\"\"\n    # Test cases: (d, H_t, H_s, m, sigma, seed)\n    test_cases = [\n        (5, 3, 3, 2000, 0.0, 12345),\n        (5, 3, 2, 2000, 0.0, 22345),\n        (5, 3, 6, 800, 0.0, 32345),\n        (5, 3, 3, 50, 0.1, 42345),\n        (5, 3, 3, 200, 0.02, 52345),\n    ]\n\n    # Hyperparameters\n    m_val = 512\n    T = 2000\n    eta = 0.05\n    P = 2.0\n    tau = 0.12\n    lambda_w = 1e-4\n\n    results = []\n\n    for d, H_t, H_s, m, sigma, seed in test_cases:\n        rng = np.random.default_rng(seed)\n\n        # 1. Teacher network and data generation\n        # Teacher parameters\n        U_teacher = rng.standard_normal(size=(H_t, d))\n        v_teacher = rng.standard_normal(size=H_t)\n\n        # Training data\n        X_train = rng.standard_normal(size=(m, d))\n        teacher_activations = np.tanh(X_train @ U_teacher.T)\n        y_true = teacher_activations @ v_teacher\n        noise = rng.normal(scale=sigma, size=m)\n        y_train = y_true + noise\n\n        # 2. Student network training\n        # Student parameters initialization\n        # Initialize with small random values to break symmetry but stay in a reasonable regime\n        W_student = rng.normal(scale=0.1, size=(H_s, d))\n        a_student = rng.normal(scale=0.1, size=H_s)\n\n        # Full-batch gradient descent loop\n        for _ in range(T):\n            # Forward pass\n            student_pre_activations = X_train @ W_student.T\n            student_activations = np.tanh(student_pre_activations)\n            y_pred = student_activations @ a_student\n\n            # Compute error\n            error = y_pred - y_train\n\n            # Compute gradients\n            grad_a = (2 / m) * (student_activations.T @ error)\n            \n            # The gradient for W requires careful vectorization\n            # term_for_w_grad has shape (m, H_s)\n            term_for_w_grad = (2 / m) * (error[:, np.newaxis] * a_student[np.newaxis, :]) * (1 - student_activations**2)\n            grad_W = term_for_w_grad.T @ X_train + lambda_w * W_student\n\n            # Update parameters\n            a_student -= eta * grad_a\n            W_student -= eta * grad_W\n\n        # 3. Recovery criterion evaluation\n        # Generate validation data\n        X_val = rng.standard_normal(size=(m_val, d))\n        \n        # Compute contribution vectors for teacher and student units\n        contrib_teacher = np.tanh(X_val @ U_teacher.T) * v_teacher[np.newaxis, :]\n        contrib_student = np.tanh(X_val @ W_student.T) * a_student[np.newaxis, :]\n        \n        # Compute pairwise cost matrix C\n        # Using broadcasting to avoid loops\n        # C_si shape: (m_val, 1, H_s)\n        # C_tj shape: (m_val, H_t, 1)\n        # diff shape: (m_val, H_t, H_s) after broadcasting\n        diff_sq = (contrib_student[:, np.newaxis, :] - contrib_teacher[:, :, np.newaxis])**2\n        cost_matrix = np.mean(diff_sq, axis=0) # shape (H_t, H_s)\n\n        # Find minimum-cost assignment\n        row_ind, col_ind = linear_sum_assignment(cost_matrix)\n        \n        # Calculate total and average cost\n        matched_cost = cost_matrix[row_ind, col_ind].sum()\n        num_matched = len(row_ind)\n        num_unmatched_teachers = H_t - num_matched\n        \n        penalty_cost = P * num_unmatched_teachers\n        total_cost = matched_cost + penalty_cost\n        avg_cost = total_cost / H_t\n        \n        # Check for recovery\n        recovery_success = avg_cost = tau\n        results.append(recovery_success)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3134222"}]}