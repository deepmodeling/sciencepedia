{"hands_on_practices": [{"introduction": "任何迭代算法都需要一个明确的停止规则，$k$-means也不例外。该算法通过交替执行分配和更新步骤来逐步最小化其目标函数，但我们何时才能认为它已经“收敛”了呢？本练习将引导你从第一性原理出发，探讨两种主流的停止准则——分配稳定性与目标函数的相对改进，并通过动手实现来比较它们在不同数据集上的表现，从而加深对算法收敛过程的理解。[@problem_id:3107749]", "problem": "您的任务是为 $k$-均值聚类算法推导有原则的停止准则，并实现一个程序，在一个小型测试集上比较它们的迭代次数。请从适合大学中级水平《计算科学导论》课程的第一性原理出发。\n\n从以下基本依据开始：\n- $\\mathbb{R}^d$ 上的欧几里得范数定义为 $\\lVert x \\rVert = \\sqrt{\\sum_{j=1}^d x_j^2}$，点 $x$ 和 $y$ 之间的欧几里得距离平方为 $\\lVert x - y \\rVert^2$。\n- 对于将点集 $\\{x_i\\}_{i=1}^n$ 划分（分配）到 $k$ 个簇的给定情况，簇内平方和距离目标函数为\n$$\nJ(\\{\\mu_j\\}_{j=1}^k, \\{a_i\\}_{i=1}^n) = \\sum_{i=1}^n \\left\\lVert x_i - \\mu_{a_i} \\right\\rVert^2,\n$$\n其中 $a_i \\in \\{1,\\dots,k\\}$ 是分配给 $x_i$ 的簇索引，$\\mu_j \\in \\mathbb{R}^d$ 是簇 $j$ 的中心。\n- 对于固定的分配，目标函数关于 $\\mu_j$ 的最小化器是每个簇中点的算术平均值，即，如果 $C_j = \\{ i : a_i = j \\}$，则 $\\mu_j^\\star = \\frac{1}{|C_j|} \\sum_{i \\in C_j} x_i$，前提是 $|C_j| > 0$。\n\n利用这些事实，首先解释为什么标准的 $k$-均值迭代过程会产生一个单调不增且以 0 为下界的目标值序列 $\\{J_t\\}_{t \\ge 0}$。该过程包括在以下两者之间交替进行：\n(1) 将每个点分配给其最近的当前中心（在欧几里得距离平方下），以及\n(2) 将每个中心更新为其分配点的均值。\n由此，推导并论证一个基于每次迭代相对改进量的停止准则：\n$$\n\\frac{\\Delta J_t}{J_{t-1}}  \\epsilon,\n$$\n其中 $\\Delta J_t = J_{t-1} - J_t$，$\\epsilon > 0$ 是用户指定的容差。为确保在 $J_{t-1}$ 为 0 时的数值鲁棒性，使用分母 $\\max(J_{t-1}, \\delta)$，其中 $\\delta$ 是一个小的正常数。另外，推导并论证一个基于分配稳定性的停止准则，定义为连续两次迭代之间任何 $a_i$ 都没有变化。\n\n实现这两个停止准则，并在以下测试集上比较它们的迭代次数。在所有情况下，使用基于欧几里得距离平方的 $k$-均值和带有固定随机种子的确定性 $k$-means++ 初始化，以确保可复现性。将迭代次数计为完整的分配-更新周期数。对于相对改进量的计数，使用运行至分配稳定性时的序列来评估该比率；如果在达到稳定性之前没有比率满足该不等式，则将分配稳定性的迭代次数作为基于相对改进量的计数。\n\n设 $\\delta = 10^{-12}$，$T_{\\max}$ 为最大迭代次数，超过该次数算法应停止以防止无限循环。\n\n测试集：\n- 测试用例1（分离良好的簇）：\n  - $\\mathbb{R}^2$ 中的数据点：$x_1 = (0,0)$, $x_2 = (0.4,-0.2)$, $x_3 = (-0.3,0.1)$, $x_4 = (5.0,5.0)$, $x_5 = (5.2,4.7)$, $x_6 = (4.8,5.3)$, $x_7 = (5.1,5.1)$, $x_8 = (-0.2,-0.1)$。\n  - 簇的数量：$k = 2$。\n  - 相对改进容差：$\\epsilon = 10^{-4}$。\n  - 最大迭代次数：$T_{\\max} = 100$。\n- 测试用例2（因簇重叠导致的缓慢改进）：\n  - $\\mathbb{R}^2$ 中的数据点：$x_1 = (0.0,0.0)$, $x_2 = (0.1,0.0)$, $x_3 = (0.0,0.1)$, $x_4 = (0.2,0.2)$, $x_5 = (0.3,0.3)$, $x_6 = (1.0,1.0)$, $x_7 = (1.1,1.2)$, $x_8 = (0.9,1.05)$, $x_9 = (1.2,0.9)$, $x_{10} = (1.05,1.1)$。\n  - 簇的数量：$k = 2$。\n  - 相对改进容差：$\\epsilon = 10^{-6}$。\n  - 最大迭代次数：$T_{\\max} = 200$。\n- 测试用例3（具有相同点的退化情况）：\n  - $\\mathbb{R}^2$ 中的数据点：$x_1 = (1.0,1.0)$, $x_2 = (1.0,1.0)$, $x_3 = (1.0,1.0)$, $x_4 = (1.0,1.0)$, $x_5 = (1.0,1.0)$, $x_6 = (1.0,1.0)$。\n  - 簇的数量：$k = 3$。\n  - 相对改进容差：$\\epsilon = 10^{-9}$。\n  - 最大迭代次数：$T_{\\max} = 50$。\n\n为确保科学真实性和可复现性的实现细节：\n- 使用带有固定种子的确定性 $k$-means++ 初始化，均匀随机地选择第一个中心，后续中心则以与到最近的已选中心的平方距离成正比的概率进行选择。\n- 在整个过程中均使用欧几里得距离平方。\n- 在每个分配步骤之后，如果任何簇为空，则将确定性选择的点重新分配给空簇，以确保在更新中心之前每个簇至少有一个点。需要一个确定性规则；例如，对于每个空簇，从当前分配给最大簇的点中，移动索引最大的那个点。这在不引入随机性的情况下保证了算法的进展。\n- 将 $J_t$ 定义为在迭代 $t$ 的更新步骤之后，使用当时的分配和中心计算出的目标值。\n- 将迭代次数定义为执行的分配-更新周期的数量。\n\n您的程序应为每个测试用例计算两个整数：直到相对改进准则首次成立时的迭代次数，以及直到达到分配稳定性时的迭代次数。最终输出必须是单行，包含一个用方括号括起来的逗号分隔列表，按测试集顺序将配对扁平化为\n$[i_{1,\\mathrm{rel}}, i_{1,\\mathrm{stab}}, i_{2,\\mathrm{rel}}, i_{2,\\mathrm{stab}}, i_{3,\\mathrm{rel}}, i_{3,\\mathrm{stab}}]$。", "solution": "该问题要求对 $k$-均值算法的两个停止准则进行有原则的推导，并实现一个程序在给定的测试集上比较它们的性能。该问题是适定的、科学上合理的，并为唯一的、可复现的解提供了足够的细节。\n\n首先，我们从所提供的第一性原理出发，推导并论证 $k$-均值算法的性质和停止准则。\n\n$k$-均值算法是一种迭代过程，旨在将 $\\mathbb{R}^d$ 中的 $n$ 个数据点 $\\{x_i\\}_{i=1}^n$ 的集合划分成 $k$ 个不相交的簇。其目标是最小化簇内平方和距离（WCSS），由以下目标函数给出：\n$$\nJ(\\{\\mu_j\\}_{j=1}^k, \\{a_i\\}_{i=1}^n) = \\sum_{i=1}^n \\left\\lVert x_i - \\mu_{a_i} \\right\\rVert^2\n$$\n其中 $a_i$ 是点 $x_i$ 所分配到的簇的索引，$\\mu_j$ 是簇 $j$ 的质心。标准算法，也称为 Lloyd 算法，在两个步骤之间交替进行：分配步骤和更新步骤。\n\n**目标函数的单调性和有界性**\n\n设算法在完成 $t-1$ 次完整迭代后的状态由质心集合 $\\{\\mu_j^{(t-1)}\\}_{j=1}^k$ 来表征。目标值为 $J_{t-1}$。第 $t$ 次迭代包括两个步骤：\n\n1.  **分配步骤**：每个数据点 $x_i$ 被分配到具有最近质心的簇，从而最小化其对 WCSS 的贡献。点 $x_i$ 的新分配 $a_i^{(t)}$ 通过以下方式找到：\n    $$\n    a_i^{(t)} = \\arg\\min_{j \\in \\{1,\\dots,k\\}} \\lVert x_i - \\mu_j^{(t-1)} \\rVert^2\n    $$\n    根据构造，对于给定的质心，这个新分配不会增加目标函数的值。设 $J'$ 为此步骤之后的目标值，使用新的分配 $\\{a_i^{(t)}\\}$ 但旧的质心 $\\{\\mu_j^{(t-1)}\\}$。我们有：\n    $$\n    J' = \\sum_{i=1}^n \\lVert x_i - \\mu_{a_i^{(t)}}^{(t-1)} \\rVert^2 \\le \\sum_{i=1}^n \\lVert x_i - \\mu_{a_i^{(t-1)}}^{(t-1)} \\rVert^2 = J_{t-1}\n    $$\n    这个不等式成立，因为由于 $\\arg\\min$ 操作的性质，左边和式中的每一项都小于或等于右边对应的项。\n\n2.  **更新步骤**：每个簇的质心被更新为分配给它的所有数据点的算术平均值。对于每个簇 $j$，新的质心 $\\mu_j^{(t)}$ 计算如下：\n    $$\n    \\mu_j^{(t)} = \\frac{1}{|C_j^{(t)}|} \\sum_{i \\in C_j^{(t)}} x_i, \\quad \\text{其中 } C_j^{(t)} = \\{i : a_i^{(t)} = j\\}\n    $$\n    如问题所述，$\\mu_j^{(t)}$ 的这一选择是最小化簇 $C_j^{(t)}$ 中点的平方距离之和的选择。因此，与分配步骤之后的值相比，此步骤也必须使目标函数值减小或保持不变：\n    $$\n    J_t = \\sum_{i=1}^n \\lVert x_i - \\mu_{a_i^{(t)}}^{(t)} \\rVert^2 \\le \\sum_{i=1}^n \\lVert x_i - \\mu_{a_i^{(t)}}^{(t-1)} \\rVert^2 = J'\n    $$\n\n结合这两个步骤，我们有 $J_t \\le J' \\le J_{t-1}$，这证明了目标函数值序列 $\\{J_t\\}_{t \\ge 0}$ 是单调不增的。此外，由于 $J$ 是范数平方和，它内在地是非负的，即对所有 $t$ 都有 $J_t \\ge 0$。一个单调不增且有下界的序列保证会收敛到一个极限。这种收敛性是停止准则的基础。\n\n**停止准则1：相对改进量**\n\n由于目标值序列 $\\{J_t\\}$ 收敛，每一步的改进量 $\\Delta J_t = J_{t-1} - J_t$ 将趋近于 0。一个简单的准则可以是在 $\\Delta J_t$ 低于某个绝对阈值时停止。然而，这不是尺度不变的；$J$ 的大小取决于数据坐标的尺度。一个更鲁棒的准则是相对改进量，它通过目标值本身来归一化减少量：\n$$\n\\frac{J_{t-1} - J_t}{J_{t-1}}  \\epsilon\n$$\n对于某个小的用户指定的容差 $\\epsilon > 0$。该准则衡量的是分数改进，并且是无量纲的。为避免在 $J_{t-1}$ 接近 0 时出现除以零或数值不稳定性，分母通过使用一个小的正常数 $\\delta$ 来稳定，如题目指定。最终的鲁棒准则是：\n$$\n\\frac{\\Delta J_t}{\\max(J_{t-1}, \\delta)}  \\epsilon\n$$\n该准则表明，算法已达到一个点，在该点上，进一步的迭代在优化目标函数方面的回报递减。\n\n**停止准则2：分配稳定性**\n\n当簇分配在迭代之间不再改变时，$k$-均值算法收敛到一个不动点，即目标函数的一个局部最小值。让我们假设在第 $t$ 次迭代中，分配步骤产生的分配集合 $\\{a_i^{(t)}\\}$ 与前一次迭代的分配集合 $\\{a_i^{(t-1)}\\}$ 完全相同。\n$$\na_i^{(t)} = a_i^{(t-1)} \\quad \\forall i \\in \\{1, \\dots, n\\}\n$$\n如果分配不变，每个簇 $j$ 的点集 $C_j$ 也不变。因此，更新步骤将产生与前一次迭代完全相同的质心：\n$$\n\\mu_j^{(t)} = \\frac{1}{|C_j^{(t)}|} \\sum_{i \\in C_j^{(t)}} x_i = \\frac{1}{|C_j^{(t-1)}|} \\sum_{i \\in C_j^{(t-1)}} x_i = \\mu_j^{(t-1)}\n$$\n由于分配和质心都与前一个状态相同，算法已达到一个不动点。所有后续迭代都将产生相同的结果。因此，检查任何点的簇分配是否没有变化，是检验算法是否收敛到局部最优的一个确定性测试。这被称为分配稳定性准则。\n\n**实现计划**\n\n实现将遵循问题的详细规范。对于每个测试用例，执行单次 $k$-均值算法运行，直到达到分配稳定性或超过最大迭代次数 $T_{\\max}$。\n1.  **初始化**：使用确定性版本的 $k$-means++ 初始化中心，其中固定的随机种子（我们使用 0）使概率选择过程可复现。\n2.  **迭代循环**：主循环执行分配-更新周期。\n3.  **分配**：使用 `scipy.spatial.distance.cdist` 函数高效地计算所有点和中心之间的欧几里得距离平方。\n4.  **空簇**：分配后，检查是否存在空簇。如果发现任何空簇，则应用确定性规则：对于每个空簇，将当前最大簇中索引最大的点重新分配给它。这确保了所有 $k$ 个质心都可以被更新。\n5.  **状态跟踪**：存储前一次迭代的分配以检查稳定性。在每次更新步骤后计算目标函数值 $J_t$。\n6.  **准则评估**：在运行期间，我们跟踪迭代次数。\n    -   `iter_stab`：在分配稳定之前完成的完整周期数。当在迭代 $t$ 开始时，发现分配与迭代 $t-1$ 的分配相同时记录。此时计数为 $t-1$。\n    -   `iter_rel`：相对改进量首次低于 $\\epsilon$ 的迭代次数 $t$。它在计算出 $J_t$ 之后计算。\n7.  **最终计数**：运行持续到达到稳定性或 $T_{\\max}$。`iter_rel` 的最终值取为满足条件的第一次迭代。如果条件从未满足，则根据问题的指示，将 `iter_rel` 设置为最终的 `iter_stab` 值。\n\n这种设计确保了两个停止准则都是基于完全相同的算法轨迹进行评估，从而提供了一个公平的比较。", "answer": "```python\nimport numpy as np\nfrom scipy.spatial.distance import cdist\n\ndef kmeans_plusplus_init(X, k, seed):\n    \"\"\"Deterministic k-means++ initialization.\"\"\"\n    rng = np.random.default_rng(seed)\n    n_samples, n_features = X.shape\n    centers = np.zeros((k, n_features))\n    \n    # 1. Choose the first center uniformly at random (from indices)\n    first_center_idx = rng.choice(n_samples)\n    centers[0] = X[first_center_idx]\n    \n    # 2. Choose remaining k-1 centers\n    for j in range(1, k):\n        # Calculate squared distances to the nearest existing center\n        dist_sq = cdist(X, centers[:j], 'sqeuclidean')\n        min_dist_sq = np.min(dist_sq, axis=1)\n        \n        # Calculate probabilities\n        prob_sum = np.sum(min_dist_sq)\n        if prob_sum == 0:\n            # Handle degenerate case (e.g., all points identical)\n            # Fallback to uniform probability\n            prob = np.full(n_samples, 1/n_samples)\n        else:\n            prob = min_dist_sq / prob_sum\n        \n        # Choose next center based on probabilities\n        next_center_idx = rng.choice(n_samples, p=prob)\n        centers[j] = X[next_center_idx]\n        \n    return centers\n\ndef handle_empty_clusters(assignments, k):\n    \"\"\"Deterministically handle empty clusters by reassigning points.\"\"\"\n    assignments = assignments.copy()\n    cluster_indices, counts = np.unique(assignments, return_counts=True)\n    \n    if len(cluster_indices)  k:\n        all_clusters = set(range(k))\n        assigned_clusters = set(cluster_indices)\n        empty_clusters = sorted(list(all_clusters - assigned_clusters))\n        \n        for j_empty in empty_clusters:\n            # Recalculate largest cluster each time\n            current_counts = np.bincount(assignments, minlength=k)\n            j_largest = np.argmax(current_counts)\n            \n            # Find point with the largest index in the largest cluster\n            indices_in_largest = np.where(assignments == j_largest)[0]\n            point_to_move_idx = np.max(indices_in_largest)\n            \n            # Reassign the point\n            assignments[point_to_move_idx] = j_empty\n            \n    return assignments\n\ndef run_kmeans_comparison(X, k, epsilon, T_max, delta, seed=0):\n    \"\"\"\n    Runs k-means until stability and reports iteration counts for two stopping criteria.\n    \"\"\"\n    n_samples, _ = X.shape\n    \n    # 1. Initialization\n    centers = kmeans_plusplus_init(X, k, seed)\n    \n    # Calculate an initial objective value to compare against J_1.\n    # This J_0 is based on initial centers and the assignments they induce.\n    initial_assignments = np.argmin(cdist(X, centers, 'sqeuclidean'), axis=1)\n    J_prev = np.sum((X - centers[initial_assignments])**2)\n    \n    assignments_prev = np.full(n_samples, -1, dtype=int)\n    \n    iter_rel = None\n    iter_stab = T_max # Default if not reached\n    \n    for t in range(1, T_max + 1):\n        # 2. Assignment step\n        assignments_curr_prime = np.argmin(cdist(X, centers, 'sqeuclidean'), axis=1)\n        \n        # 3. Handle empty clusters (after assignment, before stability check and update)\n        assignments_curr = handle_empty_clusters(assignments_curr_prime, k)\n\n        # 4. Stability check\n        if np.array_equal(assignments_curr, assignments_prev):\n            iter_stab = t - 1\n            break\n        \n        # 5. Update centers\n        new_centers = np.zeros_like(centers)\n        for j in range(k):\n            # This is guaranteed to be non-empty due to handle_empty_clusters\n            points_in_cluster = X[assignments_curr == j]\n            new_centers[j] = np.mean(points_in_cluster, axis=0)\n        centers = new_centers\n\n        # 6. Calculate objective J_t\n        J_curr = np.sum((X - centers[assignments_curr])**2)\n        \n        # 7. Relative improvement check\n        if iter_rel is None:\n            denominator = max(J_prev, delta)\n            relative_improvement = (J_prev - J_curr) / denominator\n            if relative_improvement  epsilon:\n                iter_rel = t\n        \n        # 8. Update state for next iteration\n        J_prev = J_curr\n        assignments_prev = assignments_curr\n    \n    # If the loop finished due to T_max, iter_stab is already T_max.\n    # If stability was reached, `break` was triggered.\n    else: # This `else` belongs to the `for` loop, runs if no `break`\n        iter_stab = T_max\n\n    # \"if no ratio satisfies the inequality before stability is reached, report \n    # the assignment stability iteration count\"\n    if iter_rel is None:\n        iter_rel = iter_stab\n        \n    return iter_rel, iter_stab\n\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    delta = 1e-12\n    # Using a fixed seed for reproducibility as requested.\n    fixed_seed = 0\n\n    test_cases = [\n        {\n            \"data\": np.array([\n                [0.0, 0.0], [0.4, -0.2], [-0.3, 0.1], [5.0, 5.0], \n                [5.2, 4.7], [4.8, 5.3], [5.1, 5.1], [-0.2, -0.1]\n            ]),\n            \"k\": 2, \"epsilon\": 1e-4, \"T_max\": 100\n        },\n        {\n            \"data\": np.array([\n                [0.0, 0.0], [0.1, 0.0], [0.0, 0.1], [0.2, 0.2], [0.3, 0.3], \n                [1.0, 1.0], [1.1, 1.2], [0.9, 1.05], [1.2, 0.9], [1.05, 1.1]\n            ]),\n            \"k\": 2, \"epsilon\": 1e-6, \"T_max\": 200\n        },\n        {\n            \"data\": np.array([\n                [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], \n                [1.0, 1.0], [1.0, 1.0], [1.0, 1.0]\n            ]),\n            \"k\": 3, \"epsilon\": 1e-9, \"T_max\": 50\n        }\n    ]\n\n    all_results = []\n    for case in test_cases:\n        iter_rel, iter_stab = run_kmeans_comparison(\n            case[\"data\"], case[\"k\"], case[\"epsilon\"], case[\"T_max\"], delta, seed=fixed_seed\n        )\n        all_results.extend([iter_rel, iter_stab])\n\n    print(f\"[{','.join(map(str, all_results))}]\")\n\nsolve()\n```", "id": "3107749"}, {"introduction": "掌握了算法的收敛机制后，我们必须关注一个在实际应用中至关重要的问题：数据预处理。$k$-means算法依赖于欧几里得距离来度量相似性，这意味着它对特征的尺度非常敏感。本练习将通过一个精心设计的思想实验，让你推导和实现特征缩放如何从根本上改变聚类决策边界，直观地揭示为什么在应用$k$-means之前进行特征归一化通常是必不可少的步骤。[@problem_id:3107771]", "problem": "您将研究在二维设置中，逐特征缩放如何改变 $k$-means 算法中的簇分配，此过程仅使用固定质心的分配步骤。您的推导必须基于平方欧氏距离的标准定义和 $k$-means 分配规则。请从这些基本定义出发，推导两个固定质心之间的决策边界在对角特征缩放下的变化方式。\n\n定义和设置：\n- 考虑一个位于 $\\mathbb{R}^2$ 中包含六个点 $x_i \\in \\mathbb{R}^2$ 的数据集：\n  - $x_1 = (0.0, 0.0)$\n  - $x_2 = (0.0, 3.0)$\n  - $x_3 = (3.0, 0.0)$\n  - $x_4 = (2.9, 3.0)$\n  - $x_5 = (1.0, 2.0)$\n  - $x_6 = (2.0, 1.0)$\n- 使用 $k = 2$ 个簇，其质心固定为 $c_1 = (0.0, 0.0)$ 和 $c_2 = (2.9, 3.0)$。\n- 对于任何具有严格正条目的缩放向量 $s \\in \\mathbb{R}^2$，定义对角缩放变换 $S = \\mathrm{diag}(s_1, s_2)$ 和缩放后的平方距离 $d_s^2(x, c) = \\lVert S(x - c) \\rVert_2^2$。\n- 在缩放 $s$ 下的 $k$-means 分配规则将一个点 $x$ 分配给使 $d_s^2(x, c_j)$ 最小化的质心 $c_j$。\n\n任务：\n1. 从平方欧氏距离的定义和 $k$-means 分配规则出发，推导 $c_1$ 和 $c_2$ 之间决策边界的解析方程，该方程是 $s$ 的函数。在原始坐标 $(x, y)$ 中表示该边界，并将其写成线性形式 $A(s)\\,x + B(s)\\,y + C(s) = 0$，明确地用 $s$、$c_1$ 和 $c_2$ 表示 $A(s)$、$B(s)$ 和 $C(s)$。\n2. 使用您推导出的边界，实现一个程序，该程序：\n   - 使用未缩放情况 $s = (1, 1)$ 计算所有六个点的基准分配。\n   - 对于每个指定的缩放向量 $s$，在缩放距离 $d_s^2(\\cdot,\\cdot)$ 下，仅使用固定质心 $c_1$ 和 $c_2$ 的分配步骤重新分配所有点，并计算与基准分配不同的点的数量。\n   - 计算边界法向量的方向 $\\theta(s) = \\mathrm{atan2}(B(s), A(s))$，以弧度为单位，并归一化到区间 $[0, \\pi)$。\n   - 计算边界到原点的有向距离，由 $d_0(s) = \\dfrac{C(s)}{\\sqrt{A(s)^2 + B(s)^2}}$ 给出。\n3. 针对一组测试缩放向量，报告分配变化的数量和边界特征。\n\n测试套件：\n- 使用以下五个缩放向量 $s = (s_1, s_2)$：\n  - $s = (1.0, 1.0)$\n  - $s = (3.0, 1.0)$\n  - $s = (1.0, 3.0)$\n  - $s = (0.5, 2.0)$\n  - $s = (2.0, 0.5)$\n\n最终输出格式：\n- 对于上面按序列出的每个缩放向量，输出一个形式为 $[\\text{flips}, \\theta, d_0]$ 的列表，其中 $\\text{flips}$ 是在 $s = (1.0, 1.0)$ 时与基准分配不同的点的整数数量，$\\theta$ 是边界法线角度（以弧度为单位），$d_0$ 是边界到原点的有向距离。\n- 以弧度表示 $\\theta$，并将 $\\theta$ 和 $d_0$ 都四舍五入到 $6$ 位小数。\n- 将五个测试用例的结果聚合为单行，形式为用方括号括起来的逗号分隔列表，不含空格，例如：\n  - $[[\\text{flips}_1,\\theta_1,d_{0,1}],[\\text{flips}_2,\\theta_2,d_{0,2}],\\dots]$\n\n注意：\n- 不涉及物理单位。\n- 角度必须以弧度表示。\n- 所有作为实数输出的数值必须四舍五入到 $6$ 位小数。", "solution": "该问题要求分析逐特征缩放如何影响 $k$-means 算法中的分配步骤。这涉及推导在缩放距离度量下两个固定质心之间决策边界的方程，然后将其应用于特定数据集。\n\n解决方案分为三个部分：\n1.  通用决策边界方程的推导。\n2.  边界几何特征的计算。\n3.  应用于具体问题数据并概述计算过程。\n\n**1. 决策边界方程的推导**\n\n一个点 $x \\in \\mathbb{R}^2$ 被分配到具有质心 $c_1$ 和 $c_2$ 的两个簇之一，取决于哪个质心更近。决策边界是与两个质心等距的点的轨迹。在缩放距离度量下，此条件表示为：\n$$\nd_s^2(x, c_1) = d_s^2(x, c_2)\n$$\n其中 $d_s^2(x, c)$ 是缩放后的平方欧氏距离，定义为 $d_s^2(x, c) = \\lVert S(x - c) \\rVert_2^2$。这里，$x = (x, y)$，$c_j = (c_{jx}, c_{jy})$，并且 $S = \\mathrm{diag}(s_1, s_2)$ 是对角缩放矩阵，其中 $s_1 > 0$ 且 $s_2 > 0$。\n\n变换 $S(x-c)$ 由下式给出：\n$$\nS(x-c) = \\begin{pmatrix} s_1  0 \\\\ 0  s_2 \\end{pmatrix} \\begin{pmatrix} x - c_x \\\\ y - c_y \\end{pmatrix} = \\begin{pmatrix} s_1(x - c_x) \\\\ s_2(y - c_y) \\end{pmatrix}\n$$\n其平方欧氏范数为：\n$$\nd_s^2(x, c) = \\lVert S(x - c) \\rVert_2^2 = (s_1(x - c_x))^2 + (s_2(y - c_y))^2 = s_1^2(x - c_x)^2 + s_2^2(y - c_y)^2\n$$\n将此代入决策边界条件：\n$$\ns_1^2(x - c_{1x})^2 + s_2^2(y - c_{1y})^2 = s_1^2(x - c_{2x})^2 + s_2^2(y - c_{2y})^2\n$$\n为求得直线方程，我们展开平方项：\n$$\ns_1^2(x^2 - 2xc_{1x} + c_{1x}^2) + s_2^2(y^2 - 2yc_{1y} + c_{1y}^2) = s_1^2(x^2 - 2xc_{2x} + c_{2x}^2) + s_2^2(y^2 - 2yc_{2y} + c_{2y}^2)\n$$\n二次项 $s_1^2x^2$ 和 $s_2^2y^2$ 在等式两边同时出现并相互抵消。我们得到一个关于 $x$ 和 $y$ 的线性方程：\n$$\n-2s_1^2xc_{1x} + s_1^2c_{1x}^2 - 2s_2^2yc_{1y} + s_2^2c_{1y}^2 = -2s_1^2xc_{2x} + s_1^2c_{2x}^2 - 2s_2^2yc_{2y} + s_2^2c_{2y}^2\n$$\n重新排列各项，将 $x$、$y$ 的系数和常数项分组，我们得到：\n$$\n(2s_1^2c_{2x} - 2s_1^2c_{1x})x + (2s_2^2c_{2y} - 2s_2^2c_{1y})y + (s_1^2c_{1x}^2 + s_2^2c_{1y}^2 - s_1^2c_{2x}^2 - s_2^2c_{2y}^2) = 0\n$$\n该方程符合所要求的线性形式 $A(s)x + B(s)y + C(s) = 0$。这些系数被明确地识别为缩放向量 $s = (s_1, s_2)$ 和质心坐标的函数：\n$$\nA(s) = 2s_1^2(c_{2x} - c_{1x})\n$$\n$$\nB(s) = 2s_2^2(c_{2y} - c_{1y})\n$$\n$$\nC(s) = s_1^2(c_{1x}^2 - c_{2x}^2) + s_2^2(c_{1y}^2 - c_{2y}^2) = \\lVert Sc_1 \\rVert_2^2 - \\lVert Sc_2 \\rVert_2^2\n$$\n\n**2. 边界特征的计算**\n\n使用系数 $A(s)$、$B(s)$ 和 $C(s)$，我们可以计算决策边界的几何性质。\n\n边界法向量的方向 $\\theta(s)$ 是向量 $(A(s), B(s))$ 相对于 x 轴正方向的角度。它由双参数反正切函数给出：\n$$\n\\theta(s) = \\mathrm{atan2}(B(s), A(s))\n$$\n问题要求此角度归一化到区间 $[0, \\pi)$。由于 `atan2` 函数返回一个在 $(-\\pi, \\pi]$ 内的值，一个简单的调整 `if theta  0: theta = theta + np.pi` 就可以将任何负结果正确地映射到所需范围内，这对应于选择指向（xy坐标系）上半平面的法向量。\n\n边界到原点的有向距离 $d_0(s)$ 由点 $(x_0, y_0)$ 到直线 $Ax + By + C = 0$ 的标准距离公式 $\\frac{Ax_0 + By_0 + C}{\\sqrt{A^2+B^2}}$ 给出。对于原点 $(x_0, y_0) = (0, 0)$，该公式简化为：\n$$\nd_0(s) = \\frac{C(s)}{\\sqrt{A(s)^2 + B(s)^2}}\n$$\n$d_0(s)$ 的符号表示原点位于直线的哪一侧。\n\n**3. 应用与计算过程**\n\n提供的具体数据如下：\n- 点 $x_1 = (0.0, 0.0)$、$x_2 = (0.0, 3.0)$、$x_3 = (3.0, 0.0)$、$x_4 = (2.9, 3.0)$、$x_5 = (1.0, 2.0)$、$x_6 = (2.0, 1.0)$。\n- 质心 $c_1 = (0.0, 0.0)$ 和 $c_2 = (2.9, 3.0)$。\n\n计算过程如下：\n首先，我们建立基准分配。这是通过使用未缩放的距离完成的，对应于缩放向量 $s = (1.0, 1.0)$。对于每个点 $x_i$，我们计算 $d_s^2(x_i, c_1)$ 和 $d_s^2(x_i, c_2)$，并将该点分配给产生最小距离的质心。这些分配作为参考。\n\n其次，对于所提供的测试套件中的每个缩放向量 $s$，我们重复分配过程：\n- 对于每个点 $x_i$，计算缩放后的平方距离 $d_s^2(x_i, c_1)$ 和 $d_s^2(x_i, c_2)$。\n- 将 $x_i$ 分配给更近的质心。\n- 将每个点的新分配与其基准分配进行比较。分配发生变化的点的数量记为 `flips`。\n\n第三，对于每个缩放向量 $s$，我们计算边界特征：\n- 我们将 $c_1=(0,0)$ 和 $c_2=(2.9, 3.0)$ 的具体坐标以及 $s=(s_1,s_2)$ 的分量代入推导出的 $A(s)$、$B(s)$ 和 $C(s)$ 公式中：\n  $A(s) = 2s_1^2(2.9 - 0) = 5.8s_1^2$\n  $B(s) = 2s_2^2(3.0 - 0) = 6.0s_2^2$\n  $C(s) = s_1^2(0^2 - 2.9^2) + s_2^2(0^2 - 3.0^2) = -8.41s_1^2 - 9.0s_2^2$\n- 然后我们使用这些系数计算 $\\theta(s)$ 和 $d_0(s)$。\n\n最后，收集每个缩放向量的结果——分配变化的数量、归一化角度 $\\theta(s)$ 和有向距离 $d_0(s)$——并按指定格式进行格式化。所有浮点值都四舍五入到 $6$ 位小数。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem of analyzing k-means assignment changes under feature scaling.\n    \"\"\"\n    # Define the dataset, centroids, and scaling vectors from the problem statement.\n    points = np.array([\n        [0.0, 0.0],\n        [0.0, 3.0],\n        [3.0, 0.0],\n        [2.9, 3.0],\n        [1.0, 2.0],\n        [2.0, 1.0]\n    ])\n\n    centroids = np.array([\n        [0.0, 0.0],\n        [2.9, 3.0]\n    ])\n\n    test_cases = [\n        (1.0, 1.0),\n        (3.0, 1.0),\n        (1.0, 3.0),\n        (0.5, 2.0),\n        (2.0, 0.5)\n    ]\n\n    def get_assignments(points, centroids, s_vector):\n        \"\"\"\n        Calculates cluster assignments for a given scaling vector.\n        \"\"\"\n        s = np.array(s_vector)\n        s_sq = s**2\n        \n        # Broadcasting to compute distances for all points to all centroids\n        # diff shape: (num_points, num_centroids, num_dims)\n        diff = points[:, np.newaxis, :] - centroids[np.newaxis, :, :]\n        \n        # scaled_sq_diffs shape: (num_points, num_centroids, num_dims)\n        # Broadcasting s_sq over the first two dimensions\n        scaled_sq_diffs = diff**2 * s_sq\n        \n        # dist_sq shape: (num_points, num_centroids)\n        # Sum over the dimensions axis\n        dist_sq = np.sum(scaled_sq_diffs, axis=2)\n        \n        # assignments shape: (num_points,)\n        # Find the index of the minimum distance for each point\n        assignments = np.argmin(dist_sq, axis=1)\n        return assignments\n\n    # Calculate baseline assignments for s = (1.0, 1.0)\n    baseline_assignments = get_assignments(points, centroids, test_cases[0])\n    \n    results = []\n    \n    # Process each test case\n    for s_vector in test_cases:\n        s1, s2 = s_vector\n        \n        # 1. Reassign points and count flips\n        current_assignments = get_assignments(points, centroids, s_vector)\n        flips = np.sum(current_assignments != baseline_assignments)\n        \n        # 2. Compute boundary characteristics\n        c1x, c1y = centroids[0]\n        c2x, c2y = centroids[1]\n\n        # Using derived formulas for A(s), B(s), C(s)\n        A = 2 * s1**2 * (c2x - c1x)\n        B = 2 * s2**2 * (c2y - c1y)\n        C = s1**2 * (c1x**2 - c2x**2) + s2**2 * (c1y**2 - c2y**2)\n        \n        # Compute orientation theta\n        theta = np.arctan2(B, A)\n        \n        # Normalize theta to the interval [0, pi)\n        # atan2 returns in (-pi, pi], so adding pi to negative values is sufficient\n        if theta  0:\n            theta += np.pi\n\n        # Compute signed distance d0\n        norm = np.sqrt(A**2 + B**2)\n        d0 = C / norm\n        \n        # Format the result entry for the current test case\n        formatted_entry = f\"[{flips},{theta:.6f},{d0:.6f}]\"\n        results.append(formatted_entry)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3107771"}, {"introduction": "除了对数据尺度的敏感性，$k$-means的目标函数还存在一种更微妙的内在偏见。由于其目标是最小化簇内平方和，该算法天然地倾向于发现大小相近的球状簇，这在处理尺寸差异悬殊的簇时会导致系统性的错误。本练习将指导你量化这种偏见，并实现一种加权$k$-means算法作为修正方案，这不仅能帮助你认识标准$k$-means的局限性，也展示了如何通过修改算法本身来更好地适应数据的内在结构。[@problem_id:3107780]", "problem": "您需要从基本原理出发，实现 $k$-均值聚类算法及其加权变体，并利用它们分析在簇大小存在数量级差异的合成数据中，标准 $k$-均值目标函数如何使解偏向于大簇。\n\n从以下核心定义开始。给定数据点 $\\{x_i \\in \\mathbb{R}^d\\}_{i=1}^n$、预设的簇数 $k$、簇分配 $z_i \\in \\{1,\\dots,k\\}$ 以及质心 $\\{c_j \\in \\mathbb{R}^d\\}_{j=1}^k$，标准 $k$-均值目标函数旨在最小化簇内平方和 (Within-Cluster Sum of Squares, WCSS)，其定义如下：\n$$\nJ(\\{c_j\\}, \\{z_i\\}) = \\sum_{i=1}^n \\left\\| x_i - c_{z_i} \\right\\|_2^2,\n$$\n其中 $\\left\\| \\cdot \\right\\|_2$ 表示欧几里得范数。加权 $k$-均值目标函数引入了非负的数据点权重 $\\{w_i\\}_{i=1}^n$，并最小化以下表达式：\n$$\nJ_w(\\{c_j\\}, \\{z_i\\}) = \\sum_{i=1}^n w_i \\left\\| x_i - c_{z_i} \\right\\|_2^2.\n$$\n使用迭代的分配-更新范式实现 $k$-均值算法：通过最近质心进行分配，并通过最小化关于 $\\{c_j\\}$ 的目标函数来更新质心。加权变体必须将质心更新为已分配点的加权平均值。使用 $k$-means++ 初始化方法来选择初始质心。\n\n通过从具有指定中心和标准差的各向同性高斯分布 $\\mathcal{N}(\\mu, \\sigma^2 I)$ 中独立采样，构建二维合成数据集。为保证可复现性，必须使用固定的随机种子生成数据集。在评估时，使用已知的数据生成标签来计算以下指标：\n\n- 将小簇错分率定义为：源于小簇但被分配给映射到大簇真实中心的质心的点的比例。类似地定义大簇错分率。为了将每个学习到的质心映射到一个真实中心，选择 $\\ell_2$ 距离最近的真实中心。\n- 将偏差分数 $B$ 定义为：\n$$\nB = \\frac{\\text{misassigned small-cluster points}}{n_s} - \\frac{\\text{misassigned large-cluster points}}{n_\\ell},\n$$\n其中 $n_s$ 和 $n_\\ell$ 分别是小簇和大簇的大小。\n- 对于 $k > 2$ 的情况，将大簇的重复计数 $D$ 定义为：其最近的真实中心是大簇真实中心的学习质心的数量。\n\n您的程序必须实现这些算法并计算以下测试套件。在所有情况下，数据都是二维的，角度不适用。没有物理单位。\n\n测试 1 (理想情况):\n- 数据：两个簇，大小为 $n_s = 60$, $n_\\ell = 600$，均值为 $\\mu_s = (-5, 0)$, $\\mu_\\ell = (5, 0)$，标准差为 $\\sigma_s = 1$, $\\sigma_\\ell = 1$。\n- 聚类：标准 $k$-均值，其中 $k = 2$ 并使用 $k$-means++ 初始化。\n- 随机种子：数据生成和初始化使用 $42$。\n- 输出：偏差分数 $B$，四舍五入到三位小数。\n\n测试 2 (重叠引起的偏差):\n- 数据：两个簇，大小为 $n_s = 60$, $n_\\ell = 600$，均值为 $\\mu_s = (0.1, 0)$, $\\mu_\\ell = (0, 0)$，标准差为 $\\sigma_s = 1$, $\\sigma_\\ell = 1$。\n- 聚类：标准 $k$-均值，其中 $k = 2$ 并使用 $k$-means++ 初始化。\n- 随机种子：数据生成和初始化使用 $43$。\n- 输出：偏差分数 $B$，四舍五入到三位小数。\n\n测试 3 (极端大小差异，额外质心分配):\n- 数据：两个簇，大小为 $n_s = 50$, $n_\\ell = 5000$，均值为 $\\mu_s = (3, 0)$, $\\mu_\\ell = (0, 0)$，标准差为 $\\sigma_s = 0.3$, $\\sigma_\\ell = 1.5$。\n- 聚类：标准 $k$-均值，其中 $k = 3$ 并使用 $k$-means++ 初始化。\n- 随机种子：数据生成和初始化使用 $44$。\n- 输出：大簇的重复计数 $D$，为整数。\n\n测试 4 (通过反转大小加权进行修正):\n- 数据：与测试 3 相同。\n- 聚类：加权 $k$-均值，其中 $k = 3$ 并使用 $k$-means++ 初始化，源于小簇的点的权重为 $w_i = 1/n_s$，源于大簇的点的权重为 $w_i = 1/n_\\ell$。\n- 随机种子：数据生成和初始化使用 $44$。\n- 输出：大簇的重复计数 $D$，为整数。\n\n最终输出格式：您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，顺序为 $[B_{\\text{Test 1}}, B_{\\text{Test 2}}, D_{\\text{Test 3}}, D_{\\text{Test 4}}]$。偏差分数必须四舍五入到三位小数，重复计数必须为整数，生成形如 $[b_1,b_2,d_3,d_4]$ 的列表。", "solution": "该问题要求实现标准和加权的 $k$-均值聚类算法，以分析标准方法对大簇的固有偏差。实现将从基本原理开始，包括 $k$-means++ 初始化方案。\n\n### 算法公式化\n\n给定 $n$ 个数据点 $\\{x_i \\in \\mathbb{R}^d\\}_{i=1}^n$ 和指定的簇数 $k$，$k$-均值的目标是找到簇分配 $z_i \\in \\{1, \\dots, k\\}$ 和质心 $\\{c_j \\in \\mathbb{R}^d\\}_{j=1}^k$，通过最小化一个目标函数来对数据进行划分。\n\n标准 $k$-均值算法最小化簇内平方和 (WCSS)，定义如下：\n$$\nJ(\\{c_j\\}, \\{z_i\\}) = \\sum_{i=1}^n \\left\\| x_i - c_{z_i} \\right\\|_2^2\n$$\n其中 $c_{z_i}$ 是点 $x_i$ 所属簇的质心。\n\n加权 $k$-均值变体通过为每个数据点引入非负权重 $\\{w_i\\}_{i=1}^n$ 来修改此目标函数：\n$$\nJ_w(\\{c_j\\}, \\{z_i\\}) = \\sum_{i=1}^n w_i \\left\\| x_i - c_{z_i} \\right\\|_2^2\n$$\n\n这两个目标函数通常使用一种称为 Lloyd 算法的迭代过程来最小化，该过程在两个步骤之间交替进行，直到收敛：\n\n1.  **分配步骤**：固定质心 $\\{c_j\\}$，通过将每个数据点 $x_i$ 分配给最近的质心来最小化目标函数。对于标准和加权 $k$-均值，此步骤是等效的，因为权重 $w_i > 0$ 不影响为给定点 $x_i$ 选择最近的质心。分配规则是：\n    $$\n    z_i \\leftarrow \\underset{j \\in \\{1, \\dots, k\\}}{\\mathrm{argmin}} \\left\\| x_i - c_j \\right\\|_2^2\n    $$\n\n2.  **更新步骤**：固定分配 $\\{z_i\\}$，通过将每个质心 $c_j$ 更新为其所分配点的新中心来最小化目标函数。设 $C_j = \\{i \\mid z_i = j\\}$ 为分配给簇 $j$ 的点的索引集。\n    *   对于**标准 $k$-均值**，我们最小化关于 $c_j$ 的 $J_j = \\sum_{i \\in C_j} \\left\\| x_i - c_j \\right\\|_2^2$。将梯度设为零，$\\nabla_{c_j} J_j = \\sum_{i \\in C_j} -2(x_i - c_j) = 0$，得到更新规则：\n        $$\n        c_j \\leftarrow \\frac{1}{|C_j|} \\sum_{i \\in C_j} x_i\n        $$\n        新的质心是簇中各点的几何平均值。\n    *   对于**加权 $k$-均值**，我们最小化 $J_{w,j} = \\sum_{i \\in C_j} w_i \\left\\| x_i - c_j \\right\\|_2^2$。将梯度设为零，$\\nabla_{c_j} J_{w,j} = \\sum_{i \\in C_j} -2 w_i (x_i - c_j) = 0$，得到更新规则：\n        $$\n        c_j \\leftarrow \\frac{\\sum_{i \\in C_j} w_i x_i}{\\sum_{i \\in C_j} w_i}\n        $$\n        新的质心是簇中各点的加权几何平均值。\n\n为了减轻对初始质心位置的敏感性，使用 **$k$-means++** 算法进行初始化。它会顺序选择初始质心，后续每个质心都从数据点中选择，其被选中的概率与其到最近的现有质心的平方距离成正比。这种初始化偏向于将质心放置在彼此远离的位置，这通常会带来更好、更一致的结果。\n\n### 合成数据与评估\n\n分析是在由两个各向同性高斯分布 $\\mathcal{N}(\\mu, \\sigma^2 I)$ 的混合生成的合成二维数据上进行的。我们在数据生成和 $k$-means++ 初始化中都使用固定的随机种子以保证可复现性。\n\n为了量化算法的行为，我们使用数据生成过程中的真实标签。\n- 首先，通过找到欧几里得距离中最近的真实中心，将学习到的聚类质心（带有任意标签 $1, \\dots, k$）映射到真实的簇中心（$\\mu_s, \\mu_\\ell$）。\n- **偏差分数** $B$ 定义为小簇错分率与大簇错分率之差：\n  $$\n  B = \\frac{\\text{misassigned small-cluster points}}{n_s} - \\frac{\\text{misassigned large-cluster points}}{n_\\ell}\n  $$\n  正分表示存在对小簇不利的偏差。\n- 当 $k > 2$ 时，大簇的**重复计数** $D$ 是指被映射到大簇真实中心的学习质心的数量。该指标量化了 $k$-均值将额外质心分配给更大、更分散的簇的趋势。\n\n具体的测试用例将展示簇重叠、极端的大小差异和反转大小加权如何影响这些指标，从而对 $k$-均值算法的行为提供定量的观察。实现将使用 Python 进行，利用 `numpy` 进行数值计算，并使用 `scipy` 进行高效的距离计算。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.spatial.distance import cdist\n\ndef generate_data(ns, nl, mu_s, mu_l, sigma_s, sigma_l, seed):\n    \"\"\"Generates a 2D dataset from two isotropic Gaussian distributions.\"\"\"\n    rng = np.random.default_rng(seed)\n    dim = len(mu_s)\n    \n    small_cluster = rng.multivariate_normal(mu_s, np.eye(dim) * sigma_s**2, size=ns)\n    large_cluster = rng.multivariate_normal(mu_l, np.eye(dim) * sigma_l**2, size=nl)\n    \n    X = np.vstack([small_cluster, large_cluster])\n    # True labels: 0 for small cluster, 1 for large cluster\n    y_true = np.array([0] * ns + [1] * nl) \n    true_centers = np.array([mu_s, mu_l])\n    \n    return X, y_true, true_centers\n\ndef kmeans_plusplus_init(X, k, seed):\n    \"\"\"Initializes k centroids using the k-means++ algorithm.\"\"\"\n    rng = np.random.default_rng(seed)\n    n_samples, n_features = X.shape\n    centroids = np.empty((k, n_features))\n    \n    # 1. Choose first centroid uniformly at random from data points.\n    centroids[0] = X[rng.choice(n_samples)]\n    \n    # For subsequent centroids\n    for i in range(1, k):\n        # 2. Calculate squared distances to the nearest existing centroid.\n        sq_dists = cdist(X, centroids[:i, :], 'sqeuclidean')\n        min_sq_dists = np.min(sq_dists, axis=1)\n        \n        # 3. Choose the next centroid with probability proportional to D(x)^2.\n        if np.sum(min_sq_dists) > 0:\n            probs = min_sq_dists / np.sum(min_sq_dists)\n            next_idx = rng.choice(n_samples, p=probs)\n        else:\n            # Handle cases where all points are duplicates or already chosen.\n            next_idx = rng.choice(n_samples)\n            \n        centroids[i] = X[next_idx]\n        \n    return centroids\n\ndef perform_kmeans(X, k, seed, weights=None, max_iter=100, tol=1e-6):\n    \"\"\"Performs standard or weighted k-means clustering.\"\"\"\n    centroids = kmeans_plusplus_init(X, k, seed)\n    \n    for i in range(max_iter):\n        # Assignment step: find the closest centroid for each point.\n        sq_dists = cdist(X, centroids, 'sqeuclidean')\n        assignments = np.argmin(sq_dists, axis=1)\n        \n        new_centroids = np.copy(centroids)\n        # Update step: recompute centroids based on new assignments.\n        for j in range(k):\n            assigned_points_mask = (assignments == j)\n            if np.any(assigned_points_mask):\n                if weights is None:\n                    # Standard k-means: centroid is the mean.\n                    new_centroids[j] = np.mean(X[assigned_points_mask], axis=0)\n                else:\n                    # Weighted k-means: centroid is the weighted mean.\n                    assigned_weights = weights[assigned_points_mask]\n                    new_centroids[j] = np.average(X[assigned_points_mask], axis=0, weights=assigned_weights)\n        \n        # Check for convergence: if centroids stop moving.\n        if np.sum((new_centroids - centroids)**2)  tol:\n            break\n            \n        centroids = new_centroids\n        \n    return centroids, assignments\n\ndef map_centroids_to_true_centers(centroids, true_centers):\n    \"\"\"Maps each learned centroid to the index of the nearest true center.\"\"\"\n    dists = cdist(centroids, true_centers)\n    mapping = np.argmin(dists, axis=1)\n    return mapping\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {'ns': 60, 'nl': 600, 'mu_s': np.array([-5.0, 0.0]), 'mu_l': np.array([5.0, 0.0]), 'sigma_s': 1.0, 'sigma_l': 1.0, 'k': 2, 'seed': 42, 'type': 'bias'},\n        {'ns': 60, 'nl': 600, 'mu_s': np.array([0.1, 0.0]), 'mu_l': np.array([0.0, 0.0]), 'sigma_s': 1.0, 'sigma_l': 1.0, 'k': 2, 'seed': 43, 'type': 'bias'},\n        {'ns': 50, 'nl': 5000, 'mu_s': np.array([3.0, 0.0]), 'mu_l': np.array([0.0, 0.0]), 'sigma_s': 0.3, 'sigma_l': 1.5, 'k': 3, 'seed': 44, 'type': 'duplication_std'},\n        {'ns': 50, 'nl': 5000, 'mu_s': np.array([3.0, 0.0]), 'mu_l': np.array([0.0, 0.0]), 'sigma_s': 0.3, 'sigma_l': 1.5, 'k': 3, 'seed': 44, 'type': 'duplication_weighted'}\n    ]\n\n    results = []\n\n    for params in test_cases:\n        X, y_true, true_centers = generate_data(\n            params['ns'], params['nl'], \n            params['mu_s'], params['mu_l'],\n            params['sigma_s'], params['sigma_l'],\n            params['seed']\n        )\n        \n        weights = None\n        if params['type'] == 'duplication_weighted':\n            weights = np.zeros_like(y_true, dtype=float)\n            # small cluster points have label 0, large cluster points have label 1\n            weights[y_true == 0] = 1.0 / params['ns']\n            weights[y_true == 1] = 1.0 / params['nl']\n\n        centroids, assignments = perform_kmeans(X, params['k'], params['seed'], weights=weights)\n\n        centroid_to_true_map = map_centroids_to_true_centers(centroids, true_centers)\n\n        if params['type'] == 'bias':\n            # Map each point's assignment to its effective true cluster label\n            mapped_assignments = centroid_to_true_map[assignments]\n            \n            # Small cluster has label 0, large cluster has label 1\n            # Misassigned small point: true label is 0, assigned to centroid mapped to 1\n            misassigned_small_mask = (y_true == 0)  (mapped_assignments == 1)\n            # Misassigned large point: true label is 1, assigned to centroid mapped to 0\n            misassigned_large_mask = (y_true == 1)  (mapped_assignments == 0)\n            \n            num_misassigned_small = np.sum(misassigned_small_mask)\n            num_misassigned_large = np.sum(misassigned_large_mask)\n            \n            ns, nl = params['ns'], params['nl']\n            \n            bias_score = (num_misassigned_small / ns) - (num_misassigned_large / nl)\n            results.append(f\"{bias_score:.3f}\")\n\n        elif 'duplication' in params['type']:\n            # Large cluster has true index 1\n            large_cluster_true_index = 1\n            duplication_count = np.sum(centroid_to_true_map == large_cluster_true_index)\n            results.append(str(duplication_count))\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3107780"}]}