{"hands_on_practices": [{"introduction": "在多项式回归中，一个至关重要的步骤是选择合适的多项式阶数。过于简单的模型会导致欠拟合，而过于复杂的模型又容易过拟合。本练习 [@problem_id:3158769] 介绍了一种形式化的统计方法——嵌套模型F检验，以判断增加一个更高阶的项（如 $x^3$）是否能显著改善模型拟合优度。通过这个实践，你将学会如何利用严谨的统计原理来权衡模型的复杂性与性能。", "problem": "考虑在 $n$ 个样本上观测到的标量响应 $y$ 和标量预测变量 $x$。假设在高斯-马尔可夫假设和正态分布误差下的经典线性模型设定：数据由 $y = X \\beta + \\varepsilon$ 生成，其中 $X$ 是一个固定的设计矩阵，$\\beta$ 是一个固定但未知的参数向量，误差向量 $\\varepsilon$ 的分量独立且服从分布 $\\mathcal{N}(0,\\sigma^2)$，其中 $\\sigma^2$ 是某个未知的方差。你将比较两个关于 $y$ 作为 $x$ 的函数的嵌套多项式回归模型：一个 2 次的简化模型（包括截距项、$x$ 和 $x^2$）和一个 3 次的完整模型（额外包括 $x^3$）。任务是通过在正态线性模型下进行嵌套模型检验，来判断添加 $x^3$ 项是否在拟合优度上提供了统计上显著的改进。\n\n编写一个程序，对下面指定的每个测试用例，纯粹用数学术语执行以下步骤：\n- 构建简化模型（设计矩阵为 $X_{\\text{red}} = [\\mathbf{1}, x, x^2]$）和完整模型（设计矩阵为 $X_{\\text{full}} = [\\mathbf{1}, x, x^2, x^3]$），其中 $\\mathbf{1}$ 表示长度为 $n$ 的全一列向量。\n- 在普通最小二乘法 (OLS) 的意义下，通过最小化残差平方和来拟合每个模型，从而得到残差平方和 $RSS_{\\text{red}}$ 和 $RSS_{\\text{full}}$。\n- 使用适用于正态分布误差的线性模型的标准嵌套模型检验框架，计算一个比较 $RSS_{\\text{red}}$ 和 $RSS_{\\text{full}}$ 的检验统计量，评估其零分布，并为“在简化模型各项存在条件下，三次项没有效果”这一假设计算 p 值。\n- 判断在相应测试用例指定的显著性水平 $\\alpha$ 下，这种改进是否具有统计显著性，并返回一个布尔值，指示三次项是否显著。\n\n你的程序必须处理以下测试套件。每个测试用例都以 $x$ 和 $y$ 值的数组（按顺序列出）以及一个显著性水平 $\\alpha$ 的形式给出：\n1. 情况 A（清晰的三次信号，无噪声）：\n   - $x = [-2, -1, 0, 1, 2, 3]$\n   - $y = 0.5 + 0.2 x - 0.1 x^2 + 1.0 x^3$ 在给定的 $x$ 值上逐元素计算。\n   - $\\alpha = 0.05$。\n2. 情况 B（带微小噪声的二次信号）：\n   - $x = [-2, -1, 0, 1, 2, 3]$\n   - $y = 1.0 + 0.5 x - 0.3 x^2 + \\epsilon$，其中 $\\epsilon$ 逐元素给出为 $\\epsilon = [0.005, -0.010, 0.0125, -0.0075, 0.0000, 0.0100]$。\n   - $\\alpha = 0.05$。\n3. 情况 C（小样本，带微小噪声的二次信号）：\n   - $x = [-2, -1, 0, 1, 2]$\n   - $y = 0.4 + 0.1 x - 0.2 x^2 + \\epsilon$，其中 $\\epsilon$ 逐元素给出为 $\\epsilon = [0.005, -0.003, 0.002, -0.004, 0.001]$。\n   - $\\alpha = 0.05$。\n\n所有数组必须被视为精确值，不进行任何随机化处理。没有物理单位；将所有量视为无量纲数。不涉及角度，也不需要百分比格式的输出。\n\n你的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表。每个条目按上述顺序对应一个测试用例，并且是一个布尔值，指示在指定的 $\\alpha$ 水平下，$x^3$ 的包含是否具有统计显著性。例如，如果三次项在情况 A 中显著，而在情况 B 和 C 中不显著，则输出可能看起来像 $[{\\rm True},{\\rm False},{\\rm False}]$。", "solution": "该问题要求使用统计假设检验来比较两个嵌套的多项式回归模型。目标是确定在二次模型中添加一个三次项 $x^3$ 是否能显著改进对响应变量 $y$ 变异的解释。这是一个标准的模型选择问题，可以使用嵌套线性模型的 F 检验来解决。解决方案的步骤是：定义模型，计算它们各自的拟合优度，构造检验统计量，并将其产生的 p 值与给定的显著性水平 $\\alpha$ 进行比较。\n\n首先，我们定义这两个模型。简化模型 $\\mathcal{M}_{\\text{red}}$ 是一个 2 次多项式。其对应的线性模型方程是：\n$$ y_i = \\beta_0 + \\beta_1 x_i + \\beta_2 x_i^2 + \\varepsilon_i $$\n对于 $n$ 个观测值，这可以写成矩阵形式 $y = X_{\\text{red}}\\beta_{\\text{red}} + \\varepsilon$。设计矩阵 $X_{\\text{red}}$ 是一个 $n \\times 3$ 的矩阵，其列对应于截距项、$x$ 和 $x^2$：\n$$ X_{\\text{red}} = [\\mathbf{1}, x, x^2] $$\n简化模型中的参数数量为 $p_{\\text{red}} = 3$。\n\n完整模型 $\\mathcal{M}_{\\text{full}}$ 是一个 3 次多项式。其方程是：\n$$ y_i = \\beta_0 + \\beta_1 x_i + \\beta_2 x_i^2 + \\beta_3 x_i^3 + \\varepsilon_i $$\n在矩阵形式下为 $y = X_{\\text{full}}\\beta_{\\text{full}} + \\varepsilon$。设计矩阵 $X_{\\text{full}}$ 是一个 $n \\times 4$ 的矩阵：\n$$ X_{\\text{full}} = [\\mathbf{1}, x, x^2, x^3] $$\n完整模型中的参数数量为 $p_{\\text{full}} = 4$。\n\n两个模型都使用普通最小二乘法 (OLS) 进行拟合，该方法找到使残差平方和 (RSS) 最小化的参数估计值 $\\hat{\\beta}$。对于一个设计矩阵为 $X$ 的通用模型，其 RSS 由下式给出：\n$$ RSS = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 = \\| y - X\\hat{\\beta} \\|_2^2 $$\nOLS 解为 $\\hat{\\beta} = (X^T X)^{-1}X^T y$。设 $RSS_{\\text{red}}$ 和 $RSS_{\\text{full}}$ 分别为简化模型和完整模型的最小化残差平方和。由于完整模型更复杂（它包含简化模型的所有项外加一项），它对数据的拟合总是至少与简化模型一样好，这意味着 $RSS_{\\text{full}} \\le RSS_{\\text{red}}$。\n\n问题的核心是检验由差值 $RSS_{\\text{red}} - RSS_{\\text{full}}$衡量的拟合改进是否大到可以认为具有统计显著性，或者它是否小到可以合理解释为随机偶然性所致。这被表述为一个假设检验：\n- 原假设 ($H_0$)：三次项的系数为零 ($\\beta_3 = 0$)。简化模型是充分的。\n- 备择假设 ($H_1$)：三次项的系数不为零 ($\\beta_3 \\ne 0$)。完整模型提供了显著的改进。\n\n为了检验这些假设，我们使用嵌套模型的 F 统计量。该统计量将每个额外参数带来的 RSS 减少量与来自完整模型的估计误差方差进行比较。其定义如下：\n$$ F = \\frac{(RSS_{\\text{red}} - RSS_{\\text{full}}) / (p_{\\text{full}} - p_{\\text{red}})}{RSS_{\\text{full}} / (n - p_{\\text{full}})} $$\n分子代表每增加一个参数所带来的 RSS 的平均减少量。增加的参数数量是分子自由度，$df_1 = p_{\\text{full}} - p_{\\text{red}} = 4 - 3 = 1$。分母 $MSE_{\\text{full}} = RSS_{\\text{full}} / (n - p_{\\text{full}})$ 是完整模型的均方误差，假设完整模型被正确指定，它就是误差方差 $\\sigma^2$ 的一个无偏估计。分母自由度为 $df_2 = n - p_{\\text{full}}$。\n\n在原假设 $H_0$ 下，此 F 统计量服从自由度为 $df_1$ 和 $df_2$ 的 F 分布，记作 $F \\sim F_{df_1, df_2}$。\n\n为了做出决策，我们计算 p 值，它是在假设 $H_0$ 为真的情况下，观测到等于或比计算出的 F 统计量 ($F_{\\text{obs}}$) 更极端的 F 统计量的概率。对于 F 检验，这是一个单尾概率：\n$$ p\\text{-value} = P(F_{df_1, df_2} \\ge F_{\\text{obs}}) $$\n该概率是使用 F 分布的生存函数（或互补累积分布函数）计算的。\n\n最终决策是通过将 p 值与预先指定的显著性水平 $\\alpha$ 进行比较来做出的：\n- 如果 $p\\text{-value}  \\alpha$，我们拒绝 $H_0$。证据表明三次项具有统计显著性。\n- 如果 $p\\text{-value} \\ge \\alpha$，我们不拒绝 $H_0$。没有足够的证据断定三次项是显著的。\n\n如果完整模型完美拟合数据，就会出现一个特殊情况，导致 $RSS_{\\text{full}} = 0$。这种情况发生在情况 A 中，其数据是由一个三次多项式无噪声生成的。在这种情况下，F 统计量的分母变为 $0$，导致数学上无穷大的 F 统计量。无穷大的检验统计量对应的 p 值为 $0$。因此，对于任何正的 $\\alpha$，都将拒绝 $H_0$。\n\n该实现将为每个测试用例计算这些量。设计矩阵 $X_{\\text{red}}$ 和 $X_{\\text{full}}$ 由给定的 $x$ 值构建。残差平方和 $RSS_{\\text{red}}$ 和 $RSS_{\\text{full}}$ 是通过使用数值线性代数库中的最小二乘法程序（例如 `numpy.linalg.lstsq`）高效获得的。然后计算 F 统计量及其对应的 p 值（使用 `scipy.stats.f.sf`），以得出每个情况的最终布尔结论。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import f\n\ndef solve():\n    \"\"\"\n    Performs nested model F-tests for three polynomial regression cases.\n    \"\"\"\n\n    # Helper function to perform the nested F-test for a single case.\n    def perform_nested_model_test(x_vals, y_vals, alpha):\n        \"\"\"\n        Compares a degree 2 vs. degree 3 polynomial model via an F-test.\n\n        Args:\n            x_vals (np.ndarray): The predictor variable values.\n            y_vals (np.ndarray): The response variable values.\n            alpha (float): The significance level for the test.\n\n        Returns:\n            bool: True if the cubic term is significant, False otherwise.\n        \"\"\"\n        n = len(x_vals)\n        p_red = 3  # degree 2 model: intercept, x, x^2\n        p_full = 4 # degree 3 model: intercept, x, x^2, x^3\n\n        # Construct the design matrices for the reduced and full models.\n        # np.vander with increasing=True produces columns [x^0, x^1, x^2, ...]\n        X_red = np.vander(x_vals, N=p_red, increasing=True)\n        X_full = np.vander(x_vals, N=p_full, increasing=True)\n\n        # Calculate the Residual Sum of Squares (RSS) for both models.\n        # np.linalg.lstsq returns RSS as the second element of its output tuple.\n        # This is numerically stabler than inverting the matrix X.T @ X.\n        # The returned RSS is an array, so we extract the scalar value.\n        rss_red = np.linalg.lstsq(X_red, y_vals, rcond=None)[1][0]\n        \n        # When the full model provides a perfect fit, lstsq might return an empty\n        # array for the residuals. In this case, RSS is exactly 0.\n        lstsq_full_result = np.linalg.lstsq(X_full, y_vals, rcond=None)\n        rss_full = lstsq_full_result[1][0] if len(lstsq_full_result[1]) > 0 else 0.0\n\n        # Define degrees of freedom for the F-test\n        df1 = p_full - p_red\n        df2 = n - p_full\n        \n        # Handle the case where the denominator of the F-statistic is zero\n        if rss_full  np.finfo(float).eps:\n            # If RSS_full is effectively zero, the full model is a perfect fit.\n            # This implies an infinite F-statistic and a p-value of 0.\n            p_value = 0.0\n        elif df2 = 0:\n            # If there are no degrees of freedom for the error term, the test\n            # is not well-defined. By convention, we can consider it not significant.\n            # This case does not occur in the given problem set.\n            return False\n        else:\n            # Calculate the F-statistic\n            f_statistic = ((rss_red - rss_full) / df1) / (rss_full / df2)\n            \n            # Calculate the p-value using the survival function (1 - CDF) of the F-distribution\n            p_value = f.sf(f_statistic, df1, df2)\n\n        # The cubic term is significant if the p-value is less than the significance level alpha\n        return p_value  alpha\n\n    # Define the test cases from the problem statement.\n    case_a_x = np.array([-2.0, -1.0, 0.0, 1.0, 2.0, 3.0])\n    case_a_y = 0.5 + 0.2*case_a_x - 0.1*case_a_x**2 + 1.0*case_a_x**3\n    case_a_alpha = 0.05\n    \n    case_b_x = np.array([-2.0, -1.0, 0.0, 1.0, 2.0, 3.0])\n    case_b_noise = np.array([0.005, -0.010, 0.0125, -0.0075, 0.0000, 0.0100])\n    case_b_y = 1.0 + 0.5*case_b_x - 0.3*case_b_x**2 + case_b_noise\n    case_b_alpha = 0.05\n    \n    case_c_x = np.array([-2.0, -1.0, 0.0, 1.0, 2.0])\n    case_c_noise = np.array([0.005, -0.003, 0.002, -0.004, 0.001])\n    case_c_y = 0.4 + 0.1*case_c_x - 0.2*case_c_x**2 + case_c_noise\n    case_c_alpha = 0.05\n\n    test_cases = [\n        (case_a_x, case_a_y, case_a_alpha),\n        (case_b_x, case_b_y, case_b_alpha),\n        (case_c_x, case_c_y, case_c_alpha),\n    ]\n\n    results = []\n    for x_vals, y_vals, alpha in test_cases:\n        is_significant = perform_nested_model_test(x_vals, y_vals, alpha)\n        results.append(is_significant)\n\n    # Final print statement in the exact required format.\n    # The expected format is `[True,False,False]`.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3158769"}, {"introduction": "延续模型复杂性的主题，本练习将探讨高阶多项式的一个主要陷阱：在训练数据范围之外的泛化能力极差。虽然高阶多项式可以完美地拟合训练数据点（即插值），但它在数据点之间和范围之外常常表现出剧烈的振荡。本练习 [@problem_id:3175168] 将通过动手实践，让你亲身体验著名的“龙格现象”，并量化地观察随着多项式阶数的增加，外推误差是如何急剧增大的。", "problem": "您将实现并分析多项式回归，作为一个计算实验来研究外推误差。核心场景是：在 $x \\in [-0.5, 0.5]$ 上进行训练，并在 $x \\in [0.5, 1.0]$ 上进行测试，针对不同的多项式阶数 $d$，以经验性地证明多项式预测在训练范围外的不稳定性。从最小二乘回归的基本定义出发，推导出离散优化公式。然后设计一个算法，该算法构建多项式特征图，并使用数值稳定的方法求解回归问题。\n\n从以下基本原理开始。多项式回归使用多项式基对未知函数进行建模。对于一个 $d$ 阶多项式模型，其预测器形式为 $f_{\\boldsymbol{w}}(x) = \\sum_{k=0}^{d} w_k x^k$。给定训练样本 $\\{(x_i, y_i)\\}_{i=1}^{n}$，最小二乘法的目标是最小化经验残差平方和，即 $\\min_{\\boldsymbol{w} \\in \\mathbb{R}^{d+1}} \\sum_{i=1}^{n} \\left( y_i - \\sum_{k=0}^{d} w_k x_i^k \\right)^2$。设计矩阵将特征 $x_i^k$ 编码为范德蒙（Vandermonde）结构，当设计矩阵为满列秩时，最小化器满足正规方程。\n\n您必须在单个程序中实现以下步骤：\n- 定义提供目标值的基准真相函数 $g(x)$。对所有实数 $x$ 使用 $g(x) = \\dfrac{1}{1 + 25 x^2}$，这是一个经过充分研究的光滑函数，但不是多项式。这一选择确保了多项式模型是一个近似，并允许进行有意义的外推评估。\n- 在 $[-0.5, 0.5]$ 区间内（包括端点）生成 $n$ 个等距点作为训练输入 $x_i$。对于每个 $x_i$，设置 $y_i = g(x_i)$。不添加噪声。\n- 对于给定的多项式阶数 $d$，构建范德蒙设计矩阵 $\\boldsymbol{A} \\in \\mathbb{R}^{n \\times (d+1)}$，其元素为 $A_{i,k} = x_i^k$，其中 $i \\in \\{1,\\dots,n\\}$ 且 $k \\in \\{0,\\dots,d\\}$，并使用一种不显式构造 $\\left(\\boldsymbol{A}^\\top \\boldsymbol{A}\\right)^{-1}$ 的数值稳定方法求解最小二乘问题以获得 $\\boldsymbol{w}$。\n- 在一个包含 $m$ 个位于 $[0.5, 1.0]$ 区间内（包括端点）的等距点的测试网格上评估外推性能。计算预测值 $\\hat{y}_j = f_{\\boldsymbol{w}}(x_j^{\\text{test}})$ 并与 $g(x_j^{\\text{test}})$ 进行比较。\n- 将外推误差量化为测试网格上的均方误差 (MSE)，即 $\\text{MSE} = \\dfrac{1}{m} \\sum_{j=1}^{m} \\left( \\hat{y}_j - g(x_j^{\\text{test}}) \\right)^2$。为每个测试的阶数 $d$ 报告这个单一的标量值。\n\n您的程序必须遵守以下测试套件和输出规范：\n- 在 $[-0.5, 0.5]$ 中使用 $n = 11$ 个训练点，在 $[0.5, 1.0]$ 中使用 $m = 101$ 个测试点，两者均为等距分布且包含端点。\n- 评估多项式阶数测试套件 $d \\in \\{0, 1, 3, 5, 9, 10\\}$。\n- 对于每个阶数 $d$，以浮点数形式输出均方外推误差。\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，结果的顺序与上面列出的阶数相同，即 $[ \\text{MSE}(d{=}0), \\text{MSE}(d{=}1), \\text{MSE}(d{=}3), \\text{MSE}(d{=}5), \\text{MSE}(d{=}9), \\text{MSE}(d{=}10) ]$。\n\n科学真实性和推导要求：\n- 从最小二乘原理和范德蒙设计矩阵的构建推导出算法。不要使用快捷公式；设计矩阵和最小二乘最小化必须是解题路径的一部分。\n- 通过使用基于奇异值分解 (SVD) 或等效的数值稳定求解器来确保数值稳定性，避免对 $\\boldsymbol{A}^\\top \\boldsymbol{A}$ 进行显式求逆。\n- 此问题不涉及物理单位。不使用角度。所有输出均为浮点标量。最终输出格式为上述单行描述的格式。\n\n目标是从第一性原理出发，推理多项式回归为何在外推时会变得不稳定，并通过指定的测试套件经验性地证明这一效应。", "solution": "此问题是有效的，因为它在数值分析和机器学习领域提出了一个适定（well-posed）且具有科学依据的计算任务。所有参数和目标都已明确定义，而任务本身——经验性地证明多项式外推的不稳定性——是基于既定原则的一项经典且有意义的练习。\n\n问题的核心是解决一系列多项式回归任务，并在一个外推域上评估其性能。我们从第一性原理开始，对问题进行形式化。\n\n一个 $d$ 阶多项式回归模型试图使用以下形式的预测器来近似一个未知函数：\n$$\nf_{\\boldsymbol{w}}(x) = w_0 + w_1x + w_2x^2 + \\dots + w_dx^d = \\sum_{k=0}^{d} w_k x^k\n$$\n在此，$\\boldsymbol{w} = [w_0, w_1, \\dots, w_d]^\\top$ 是必须确定的系数或权重向量。\n\n给定一组 $n$ 个训练样本 $\\{(x_i, y_i)\\}_{i=1}^{n}$，最小二乘原理要求我们找到权重 $\\boldsymbol{w}$，以最小化模型预测值 $f_{\\boldsymbol{w}}(x_i)$ 与真实目标值 $y_i$ 之间的残差平方和 (SSR)。需要最小化的目标函数是：\n$$\nE(\\boldsymbol{w}) = \\sum_{i=1}^{n} (y_i - f_{\\boldsymbol{w}}(x_i))^2 = \\sum_{i=1}^{n} \\left( y_i - \\sum_{k=0}^{d} w_k x_i^k \\right)^2\n$$\n为了便于求解，我们将此系统表示为矩阵-向量形式。设 $\\boldsymbol{y} \\in \\mathbb{R}^n$ 为目标值向量，$\\boldsymbol{y} = [y_1, y_2, \\dots, y_n]^\\top$。设 $\\boldsymbol{A}$ 为一个 $n \\times (d+1)$ 的设计矩阵，其中每一行对应一个数据点 $x_i$，每一列对应一个多项式基函数 $x^k$。$\\boldsymbol{A}$ 的元素由 $A_{i,k} = x_i^k$ 给出，其中 $i \\in \\{1,\\dots,n\\}$ 且 $k \\in \\{0,\\dots,d\\}$。这种特定结构将 $\\boldsymbol{A}$ 定义为范德蒙（Vandermonde）矩阵。\n$$\n\\boldsymbol{A} = \\begin{pmatrix}\nx_1^0  x_1^1  \\dots  x_1^d \\\\\nx_2^0  x_2^1  \\dots  x_2^d \\\\\n\\vdots  \\vdots  \\ddots  \\vdots \\\\\nx_n^0  x_n^1  \\dots  x_n^d\n\\end{pmatrix}\n$$\n根据这些定义，所有训练点的预测向量为 $\\hat{\\boldsymbol{y}} = \\boldsymbol{A}\\boldsymbol{w}$。最小二乘目标函数变为残差向量的平方欧几里得范数：\n$$\nE(\\boldsymbol{w}) = \\|\\boldsymbol{y} - \\boldsymbol{A}\\boldsymbol{w}\\|_2^2\n$$\n最小化问题为 $\\min_{\\boldsymbol{w} \\in \\mathbb{R}^{d+1}} \\|\\boldsymbol{y} - \\boldsymbol{A}\\boldsymbol{w}\\|_2^2$。这是一个标准的线性最小二乘问题。一个常见的解析解源于正规方程 $\\boldsymbol{A}^\\top \\boldsymbol{A} \\boldsymbol{w} = \\boldsymbol{A}^\\top \\boldsymbol{y}$。如果 $\\boldsymbol{A}^\\top \\boldsymbol{A}$ 可逆，则解为 $\\boldsymbol{w} = (\\boldsymbol{A}^\\top \\boldsymbol{A})^{-1} \\boldsymbol{A}^\\top \\boldsymbol{y}$。\n\n然而，问题正确地要求避免使用此方法。范德蒙矩阵是出了名的病态（ill-conditioned），尤其是在高阶 $d$ 的情况下。$\\boldsymbol{A}^\\top\\boldsymbol{A}$ 的条件数是 $\\boldsymbol{A}$ 条件数的平方，这在有限精度算术中构造和求逆 $\\boldsymbol{A}^\\top\\boldsymbol{A}$ 时可能导致严重的数值不稳定和不准确的结果。\n\n解决最小二乘问题的一种数值稳定方法涉及矩阵分解，例如 QR 分解或奇异值分解 (SVD)。由于 SVD 通常是最稳健的，我们的方法将基于它。任何矩阵 $\\boldsymbol{A} \\in \\mathbb{R}^{n \\times (d+1)}$ 都有一个形式为 $\\boldsymbol{A} = \\boldsymbol{U}\\boldsymbol{\\Sigma}\\boldsymbol{V}^\\top$ 的 SVD，其中 $\\boldsymbol{U}$ 和 $\\boldsymbol{V}$ 是正交矩阵，$\\boldsymbol{\\Sigma}$ 是一个包含奇异值的矩形对角矩阵。最小二乘解由 $\\boldsymbol{w} = \\boldsymbol{A}^+ \\boldsymbol{y}$ 给出，其中 $\\boldsymbol{A}^+ = \\boldsymbol{V}\\boldsymbol{\\Sigma}^+\\boldsymbol{U}^\\top$ 是 Moore-Penrose 伪逆。数值库提供了高效且稳定的 `lstsq` 求解器，其内部使用了此类分解。\n\n完整的算法如下：\n1.  定义基准真相函数 $g(x) = \\frac{1}{1 + 25x^2}$。\n2.  生成训练数据：在区间 $[-0.5, 0.5]$ 内创建一个包含 $n=11$ 个等距点的向量 $x_{\\text{train}}$，并计算相应的目标值 $y_{\\text{train},i} = g(x_{\\text{train},i})$。\n3.  生成测试数据：在外推区间 $[0.5, 1.0]$ 内创建一个包含 $m=101$ 个等距点的向量 $x_{\\text{test}}$，并计算基准真相值 $y_{\\text{test},j} = g(x_{\\text{test},j})$。\n4.  初始化一个空列表来存储结果。\n5.  对于测试套件 $\\{0, 1, 3, 5, 9, 10\\}$ 中的每个多项式阶数 $d$：\n    a. 构建 $n \\times (d+1)$ 的范德蒙训练矩阵 $\\boldsymbol{A}_{\\text{train}}$，其中第 $i$ 行、第 $k$ 列（0-索引）的元素为 $(x_{\\text{train},i})^k$。\n    b. 使用数值稳定的求解器（如 `numpy.linalg.lstsq`）求解线性最小二乘系统 $\\boldsymbol{A}_{\\text{train}}\\boldsymbol{w} \\approx \\boldsymbol{y}_{\\text{train}}$ 以得到权重向量 $\\boldsymbol{w}$。\n    c. 构建 $m \\times (d+1)$ 的范德蒙测试矩阵 $\\boldsymbol{A}_{\\text{test}}$，其中第 $j$ 行、第 $k$ 列的元素为 $(x_{\\text{test},j})^k$。\n    d. 计算模型在测试集上的预测：$\\hat{\\boldsymbol{y}}_{\\text{test}} = \\boldsymbol{A}_{\\text{test}}\\boldsymbol{w}$。\n    e. 计算外推域上的均方误差 (MSE)：$\\text{MSE} = \\frac{1}{m} \\sum_{j=1}^{m} (\\hat{y}_{\\text{test},j} - y_{\\text{test},j})^2$。\n    f. 将计算出的 MSE 附加到结果列表中。\n6.  输出 MSE 值的列表。\n\n这个过程将经验性地展示龙格现象（Runge's phenomenon）。虽然高阶多项式可以在训练区间内实现更好的拟合，但它们在区间外往往会剧烈振荡。预计外推误差对于低阶数会相对较小，但对于高阶数（$d=9, 10$）会急剧增加，这显示了高阶多项式外推的不可靠性。请注意，当 $d=10$ 时，系数数量（$d+1=11$）等于数据点数量（$n=11$），因此模型成为一个插值多项式，这种多项式特别容易在数据点之间发生剧烈振荡，包括在外推区域。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and analyzes polynomial regression to study extrapolation error.\n    \"\"\"\n    # Define the test suite of polynomial degrees.\n    test_cases_degrees = [0, 1, 3, 5, 9, 10]\n\n    # Define parameters for data generation.\n    n_train_points = 11\n    n_test_points = 101\n    \n    # Define the ground-truth function g(x).\n    g = lambda x: 1.0 / (1.0 + 25.0 * x**2)\n\n    # 1. Generate training data\n    # n points equally spaced in [-0.5, 0.5] including endpoints.\n    x_train = np.linspace(-0.5, 0.5, n_train_points)\n    y_train = g(x_train)\n\n    # 2. Generate test data for extrapolation evaluation\n    # m points equally spaced in [0.5, 1.0] including endpoints.\n    x_test = np.linspace(0.5, 1.0, n_test_points)\n    y_test_ground_truth = g(x_test)\n    \n    results = []\n    \n    # 3. Loop through each specified polynomial degree.\n    for d in test_cases_degrees:\n        # a. Construct the Vandermonde design matrix for training.\n        # The number of columns is d + 1 for a degree-d polynomial (powers 0 to d).\n        # np.vander with increasing=True gives columns [x^0, x^1, ..., x^d].\n        A_train = np.vander(x_train, d + 1, increasing=True)\n        \n        # b. Solve the least squares problem to find the weights w.\n        # np.linalg.lstsq is a numerically stable solver.\n        # It returns the solution, residuals, rank, and singular values.\n        # We only need the solution vector w.\n        w, _, _, _ = np.linalg.lstsq(A_train, y_train, rcond=None)\n        \n        # c. Construct the Vandermonde matrix for the test set.\n        A_test = np.vander(x_test, d + 1, increasing=True)\n        \n        # d. Compute predictions on the test set.\n        y_pred_test = A_test @ w\n        \n        # e. Quantify extrapolation error using Mean Squared Error (MSE).\n        mse = np.mean((y_pred_test - y_test_ground_truth)**2)\n        \n        results.append(mse)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3175168"}, {"introduction": "即使有理由使用高阶多项式，其实际应用也面临着一个严峻的数值计算挑战：标准的单项式基 ($\\{1, x, x^2, \\dots\\}$) 会导致设计矩阵（范德蒙矩阵）呈病态，使得求解过程在数值上非常不稳定。本练习 [@problem_id:2425191] 旨在解决这一根本性的实现问题，通过对比标准基与正交多项式基（勒让德多项式）的效果，你将定量地看到使用正交多项式如何显著改善问题的数值稳定性。这是进行可靠科学计算的一项关键技术。", "problem": "您的任务是定量比较由标准单项式基与正交多项式基构建的多项式回归设计矩阵的数值条件。在多项式回归中，通过选择一组基函数并求解线性最小二乘问题来对数据进行建模。解的数值稳定性在很大程度上取决于设计矩阵的条件数。矩阵的谱条件数根据奇异值分解计算出的奇异值来定义，并被广泛用于评估数值稳定性。与朴素的单项式相比，正交多项式以减少多重共线性和改善条件数而闻名，尤其是在输入被缩放到标准区间时。\n\n从最小二乘回归、正交多项式和谱条件数的基本定义出发，实现一个程序，该程序为每个测试用例使用相同的输入构建两个设计矩阵：\n- 一个由标准基 $\\{1, x, x^{2}, \\dots \\}$ 构建的单项式范德蒙型设计矩阵。\n- 一个由在相同点上经过仿射归一化后求值的第一类 Legendre 多项式构建的设计矩阵。\n\n对于每个测试用例，请遵循以下要求：\n1. 给定任意实数区间内的一组输入点 $\\{x_{i}\\}_{i=0}^{n-1}$，首先使用仿射映射将它们归一化到区间 $[-1,1]$ 上的 $\\{t_{i}\\}$：\n$$\nt_{i} = \\frac{2\\,(x_{i} - x_{\\min})}{x_{\\max} - x_{\\min}} - 1,\n$$\n其中 $x_{\\min} = \\min_{i} x_{i}$ 且 $x_{\\max} = \\max_{i} x_{i}$。\n2. 对于指定的非负整数阶数 $d$，构建两个 $n \\times (d+1)$ 的设计矩阵：\n   - 单项式设计矩阵 $A$，其元素为 $A_{i,k} = t_{i}^{k}$，其中 $k = 0, 1, \\dots, d$。\n   - Legendre 设计矩阵 $B$，其元素为 $B_{i,k} = P_{k}(t_{i})$，其中 $k = 0, 1, \\dots, d$，$P_{k}$ 表示在 $[-1,1]$ 上的第 $k$ 阶 Legendre 多项式。\n3. 使用奇异值分解计算谱条件数 $\\kappa_{2}(A)$ 和 $\\kappa_{2}(B)$。使用以下定义：\n$$\n\\kappa_{2}(M) = \\frac{\\sigma_{\\max}(M)}{\\sigma_{\\min}(M)},\n$$\n其中 $\\sigma_{\\max}(M)$ 和 $\\sigma_{\\min}(M)$ 分别是 $M$ 的最大和最小奇异值。\n4. 对于每个测试用例，计算改善比：\n$$\nr = \\frac{\\kappa_{2}(A)}{\\kappa_{2}(B)}.\n$$\n报告 $r$ 的值，四舍五入到恰好六位小数。\n\n角度单位：在下文中，每当使用余弦函数定义点时，其参数均以弧度为单位。\n\n您的程序必须实现以下测试套件，并以指定的最终输出格式生成结果。\n\n测试套件：\n- 用例 $1$（等距分布，中等阶数）：$n = 50$，$d = 12$，输入为 $x_{i} = -1 + \\dfrac{2\\,i}{n-1}$，其中 $i = 0, 1, \\dots, n-1$。\n- 用例 $2$（聚集在某一端点附近）：$n = 50$，$d = 12$，输入为 $x_{i} = 1 - \\exp\\!\\left(-\\dfrac{5\\,i}{n-1}\\right)$，其中 $i = 0, 1, \\dots, n-1$。\n- 用例 $3$（类似 Chebyshev 的内部集中分布）：$n = 30$，$d = 14$，输入为 $x_{i} = \\cos\\!\\left(\\dfrac{\\pi\\,(i+0.5)}{n}\\right)$，其中 $i = 0, 1, \\dots, n-1$。\n- 用例 $4$（原始尺度范围宽，然后归一化）：$n = 40$，$d = 18$，输入为 $x_{i} = 0 + \\dfrac{1000\\,i}{n-1}$，其中 $i = 0, 1, \\dots, n-1$。\n\n所有计算必须以双精度进行。对于每个用例，确保 $d+1 \\le n$，以便对于不同的 $t_{i}$，设计矩阵是满列秩的。\n\n最终输出格式：\n您的程序应生成单行输出，其中包含四个测试用例的改善比，以逗号分隔的列表形式用方括号括起来，按用例 $1$ 到 $4$ 的顺序排列，每个值都四舍五入到恰好六位小数，例如：“[1.234567,8.901234,5.678901,2.345678]”。不应打印任何其他文本。所有值都是无单位的实数，表示为小数点后有六位数字的十进制浮点数。", "solution": "问题陈述经过验证，被认为是有效的。它具有科学依据，是适定且客观的。它提出了计算工程领域中的一个明确任务，特别是在回归方法的数值分析方面。所有必要的数据和定义都已提供，没有矛盾或歧义。因此，我将着手提供一个完整的解决方案。\n\n任务是定量评估在多项式回归问题中，使用正交多项式基（Legendre 多项式）代替标准单项式基时数值条件的改善情况。此分析的核心在于设计矩阵的属性，该矩阵将回归系数映射到预测值。\n\n在多项式回归问题中，我们试图用一个 $d$ 次多项式来建模一组数据点 $\\{ (x_i, y_i) \\}_{i=0}^{n-1}$。该多项式可以表示为基函数 $\\{ \\phi_k(x) \\}_{k=0}^{d}$ 的线性组合：\n$$\nf(x) = \\sum_{k=0}^{d} c_k \\phi_k(x)\n$$\n目标是找到系数 $c_k$，以最小化模型预测值 $f(x_i)$ 与观测数据 $y_i$ 之间的平方误差之和。这是一个线性最小二乘问题。系数向量 $C = [c_0, c_1, \\dots, c_d]^T$ 与预测值向量 $Y_{pred} = [f(x_0), f(x_1), \\dots, f(x_{n-1})]^T$ 之间的关系由以下矩阵方程给出：\n$$\nY_{pred} = M C\n$$\n其中 $M$ 是一个 $n \\times (d+1)$ 的设计矩阵，其元素为 $M_{ik} = \\phi_k(x_i)$。求解系数向量 $C$ 的数值稳定性在很大程度上取决于该设计矩阵 $M$ 的条件数。一个常用的度量是谱条件数 $\\kappa_2(M)$，定义为 $M$ 的最大奇异值与最小奇异值之比：\n$$\n\\kappa_2(M) = \\frac{\\sigma_{\\max}(M)}{\\sigma_{\\min}(M)}\n$$\n大的条件数表示这是一个病态问题，输入数据中的小误差可能导致计算出的系数产生大误差，从而使解不可靠。\n\n该问题要求比较两种基函数的选择：\n1. 标准单项式基 $\\phi_k(x) = x^k$。对于高阶数 $d$ 或聚集在远离原点的输入点 $x_i$，所得到的设计矩阵（一个范德蒙型矩阵）的列会变得近似线性相关。这会导致非常大的条件数。\n2. Legendre 多项式基 $\\phi_k(x) = P_k(x)$。Legendre 多项式 $\\{ P_k(t) \\}_{k=0}^{\\infty}$ 在区间 $[-1, 1]$ 上关于 $L^2$ 内积是正交的，即当 $j \\neq k$ 时，$\\int_{-1}^{1} P_j(t) P_k(t) dt = 0$。这种正交性有助于确保相应设计矩阵的列远非线性相关，从而得到显著更小的条件数和更稳定的数值问题。\n\n为确保实现正交性的好处，必须将输入数据点映射到 Legendre 多项式定义的区间 $[-1, 1]$ 上。问题为一组点 $\\{x_i\\}$ 指定了以下仿射变换：\n$$\nt_i = \\frac{2(x_i - x_{\\min})}{x_{\\max} - x_{\\min}} - 1\n$$\n其中 $x_{\\min}$ 和 $x_{\\max}$ 是集合 $\\{x_i\\}$ 中的最小值和最大值。在构建设计矩阵之前，此变换会应用于所有输入点。请注意，如果 $x_{\\max} = x_{\\min}$，则所有点都相同，该公式没有明确定义。然而，问题确保使用不同的点，因此 $x_{\\max}  x_{\\min}$。\n\n解决此问题的算法步骤如下：\n对于每个由点数 $n$、多项式阶数 $d$ 以及生成点 $\\{x_i\\}$ 的公式指定的测试用例：\n1. 生成 $n$ 个输入点 $\\{x_i\\}_{i=0}^{n-1}$。\n2. 计算 $x_{\\min} = \\min_{i} x_i$ 和 $x_{\\max} = \\max_{i} x_i$。\n3. 使用仿射映射将点归一化，以获得在区间 $[-1, 1]$ 内的 $\\{t_i\\}_{i=0}^{n-1}$。\n4. 构建 $n \\times (d+1)$ 的单项式设计矩阵 $A$，其中第 $i$ 行第 $k$ 列的元素为 $A_{ik} = t_i^k$，其中 $i \\in \\{0, \\dots, n-1\\}$ 且 $k \\in \\{0, \\dots, d\\}$。\n5. 构建 $n \\times (d+1)$ 的 Legendre 设计矩阵 $B$，其中 $B_{ik} = P_k(t_i)$。Legendre 多项式 $P_k(t)$ 使用标准的三项递推关系计算：\n   $$ P_0(t) = 1 $$\n   $$ P_1(t) = t $$\n   $$ (k+1)P_{k+1}(t) = (2k+1)tP_k(t) - kP_{k-1}(t) \\quad \\text{for } k \\geq 1 $$\n   我们将使用一个预先存在的库函数进行此评估，这是计算科学中的标准做法。\n6. 对于每个矩阵 $A$ 和 $B$，使用奇异值分解（SVD）计算其奇异值。设奇异值为 $\\{\\sigma_j\\}$。\n7. 计算每个矩阵的条件数：$\\kappa_2(A) = \\sigma_{\\max}(A) / \\sigma_{\\min}(A)$ 和 $\\kappa_2(B) = \\sigma_{\\max}(B) / \\sigma_{\\min}(B)$。\n8. 计算改善比 $r = \\kappa_2(A) / \\kappa_2(B)$。\n9. 按要求报告 $r$ 的值，四舍五入到六位小数。\n\n对四个指定的测试用例中的每一个都实施此过程，并将结果汇总为指定的输出格式。使用双精度浮点运算是标准的，并且对于此分析是足够的。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import eval_legendre\n\ndef solve():\n    \"\"\"\n    Computes the improvement in design matrix conditioning by using Legendre\n    polynomials over monomials for several test cases.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"n\": 50, \"d\": 12,\n            \"x_generator\": lambda n: -1.0 + 2.0 * np.arange(n) / (n - 1)\n        },\n        {\n            \"n\": 50, \"d\": 12,\n            \"x_generator\": lambda n: 1.0 - np.exp(-5.0 * np.arange(n) / (n - 1))\n        },\n        {\n            \"n\": 30, \"d\": 14,\n            \"x_generator\": lambda n: np.cos(np.pi * (np.arange(n) + 0.5) / n)\n        },\n        {\n            \"n\": 40, \"d\": 18,\n            \"x_generator\": lambda n: 1000.0 * np.arange(n) / (n - 1)\n        }\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        n = case[\"n\"]\n        d = case[\"d\"]\n        \n        # Step 1: Generate input points\n        x = case[\"x_generator\"](n).astype(np.float64)\n\n        # Step 2: Normalize points to [-1, 1]\n        x_min, x_max = np.min(x), np.max(x)\n        \n        # Handle the case where all points are the same to avoid division by zero.\n        if np.isclose(x_max, x_min):\n            t = np.zeros_like(x)\n        else:\n            t = 2.0 * (x - x_min) / (x_max - x_min) - 1.0\n\n        # Step 3: Construct the monomial design matrix A (Vandermonde matrix)\n        # A_{ik} = t_i^k for k = 0, ..., d\n        # numpy.vander with increasing=True creates columns t^0, t^1, ..., t^d\n        A = np.vander(t, N=d + 1, increasing=True)\n\n        # Step 4: Construct the Legendre design matrix B\n        # B_{ik} = P_k(t_i) for k = 0, ..., d\n        B = np.zeros((n, d + 1), dtype=np.float64)\n        for k in range(d + 1):\n            B[:, k] = eval_legendre(k, t)\n\n        # Step 5: Compute the singular values for both matrices\n        # SVD returns singular values sorted in descending order.\n        # compute_uv=False is an optimization as we only need the singular values.\n        s_A = np.linalg.svd(A, compute_uv=False)\n        s_B = np.linalg.svd(B, compute_uv=False)\n\n        # Step 6: Compute the spectral condition numbers\n        # kappa(M) = sigma_max / sigma_min\n        kappa_A = s_A[0] / s_A[-1]\n        kappa_B = s_B[0] / s_B[-1]\n\n        # Step 7: Compute the improvement ratio\n        improvement_ratio = kappa_A / kappa_B\n        \n        # Step 8: Format the result and add to the list\n        results.append(f\"{improvement_ratio:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "2425191"}]}