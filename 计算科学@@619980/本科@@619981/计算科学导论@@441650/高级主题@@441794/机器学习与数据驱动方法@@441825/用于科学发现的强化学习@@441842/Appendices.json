{"hands_on_practices": [{"introduction": "在计算科学中，一个核心挑战是在计算精度和成本之间找到平衡。本练习将指导您构建一个强化学习代理，它能根据具体情境（科学假设的特征）智能地选择使用快速但有偏差的均场近似，还是精确但昂贵的蒙特卡洛方法。通过从头开始推导并实现策略梯度算法，您将掌握在资源受限的情况下优化科学工作流程的基本技能 [@problem_id:3186206]。", "problem": "要求您为一个有科学依据的决策任务，形式化并实现一个最小化的强化学习（RL; Reinforcement Learning）设置：选择使用哪种计算近似方法来评估一组科学假设，同时平衡预期准确性与计算成本。该场景是一个上下文赌博机（contextual bandit），其中每个上下文对应一个具有给定潜藏参数的假设，每个动作对应两种计算近似方法之一。\n\n您必须仅从基本定义出发，构建并解决该问题。智能体必须通过最大化预期回报来学习一个策略，该策略将数值上下文映射到一个动作。回报必须编码准确性与成本之间的权衡。您不能硬编码任何直接输出最优动作的预言机；而应通过在预期回报上使用梯度上升，从第一性原理学习一个参数化策略来解决该优化问题。\n\n基本设置：\n\n- 存在一个有限的假设集，由 $h \\in \\mathcal{H}$ 索引。每个假设都有一个真实的标量潜藏值 $ \\theta_h \\in \\mathbb{R} $。\n- 在每次决策时，智能体观察一个由 $ \\theta_h $ 构建的固定特征向量 $ x_h \\in \\mathbb{R}^d $，然后选择动作 $ a \\in \\{0,1\\} $。动作 $ a=0 $ 表示平均场近似（确定性，有偏），动作 $ a=1 $ 表示蒙特卡洛近似（随机性，无偏但有方差）。\n- 智能体收到一个标量回报 $ r $，其定义为所选近似的估计值与真实值之间的负平方误差，并受到线性计算成本的惩罚。形式上，对于瞬时估计 $ \\widehat{\\theta}_{h,a} $，回报为\n$$\nr(h,a) \\;=\\; -\\bigl(\\widehat{\\theta}_{h,a} - \\theta_h\\bigr)^2 \\;-\\; \\lambda \\, c_a,\n$$\n其中 $ \\lambda \\ge 0 $ 是一个权衡系数，$ c_a \\ge 0 $ 是依赖于动作的成本。不涉及物理单位。\n\n- 对于平均场近似（$ a=0 $），估计是一个确定性的有偏收缩\n$$\n\\widehat{\\theta}_{h,0} \\;=\\; \\alpha \\, \\theta_h,\n$$\n其中 $ \\alpha \\in (0,1) $。\n- 对于蒙特卡洛近似（$ a=1 $），估计是一个无偏的噪声观测\n$$\n\\widehat{\\theta}_{h,1} \\;=\\; \\theta_h \\;+\\; \\varepsilon, \\qquad \\varepsilon \\sim \\mathcal{N}(0,\\sigma^2),\n$$\n其中方差参数为 $ \\sigma^2 \\ge 0 $。因此，当 $ a=1 $ 时，由于 $ \\varepsilon $ 的存在，回报是一个随机变量。\n\n策略与目标：\n\n- 使用一个基于两个动作的 softmax 策略 $ \\pi_\\mathbf{W}(a \\mid x_h) $，其 logit 为线性的。令 $ d=2 $ 且 $ x_h = \\begin{bmatrix}1 \\\\ \\theta_h^2\\end{bmatrix} \\in \\mathbb{R}^2 $。令 $ \\mathbf{W} \\in \\mathbb{R}^{2 \\times 2} $ 在其行中收集特定动作的参数向量。logits 为 $ z_a = \\mathbf{w}_a^{\\top} x_h $，动作概率为\n$$\n\\pi_\\mathbf{W}(a \\mid x_h) \\;=\\; \\frac{\\exp(z_a)}{\\exp(z_0) + \\exp(z_1)}.\n$$\n- 目标是有限假设集上的平均预期回报，\n$$\nJ(\\mathbf{W}) \\;=\\; \\frac{1}{|\\mathcal{H}|} \\sum_{h \\in \\mathcal{H}} \\,\\mathbb{E}_{a \\sim \\pi_\\mathbf{W}(\\cdot \\mid x_h)} \\bigl[\\, \\mathbb{E}[\\, r(h,a) \\mid h, a \\,] \\,\\bigr],\n$$\n这源于上下文赌博机的预期回报定义和全期望定律。您必须通过梯度上升最大化 $ J(\\mathbf{W}) $，且仅使用基本定义。您不能假设梯度公式；必须从 softmax 和期望的定义中推导它。\n\n评估指标：\n\n- 训练后，对于每个 $ h \\in \\mathcal{H} $，通过选择具有最大概率的动作 $ \\arg\\max_a \\pi_\\mathbf{W}(a \\mid x_h) $，将随机策略转换为确定性选择。\n- 令 $ Q(h,a) = \\mathbb{E}[\\,r(h,a) \\mid h,a\\,] $ 表示在上下文 $ h $ 下采取动作 $ a $ 的预期回报。评估：\n    - 学习到的策略值 $ V_{\\text{learn}} = \\frac{1}{|\\mathcal{H}|} \\sum_h Q(h, \\arg\\max_a \\pi_\\mathbf{W}(a \\mid x_h)) $。\n    - 最优策略值 $ V_{\\text{opt}} = \\frac{1}{|\\mathcal{H}|} \\sum_h \\max_a Q(h,a) $。\n    - 遗憾值 $ \\Delta = V_{\\text{opt}} - V_{\\text{learn}} $。\n- 将每个测试用例的遗憾值 $ \\Delta $ 报告为浮点数。不适用物理单位。将每个遗憾值四舍五入到恰好六位小数。\n\n可从第一性原理获得的 $ Q(h,a) $ 的显式表达式：\n\n- 因为 $ \\widehat{\\theta}_{h,0} = \\alpha \\theta_h $ 是确定性的，\n$$\nQ(h,0) \\;=\\; -(\\alpha\\theta_h - \\theta_h)^2 \\;-\\; \\lambda c_0 \\;=\\; -(\\alpha - 1)^2\\theta_h^2 \\;-\\; \\lambda c_0.\n$$\n- 因为 $ \\widehat{\\theta}_{h,1} = \\theta_h + \\varepsilon $，其中 $ \\varepsilon \\sim \\mathcal{N}(0,\\sigma^2) $ 且 $ \\mathbb{E}[\\varepsilon^2] = \\sigma^2 $，\n$$\nQ(h,1) \\;=\\; -\\mathbb{E}[(\\varepsilon)^2] \\;-\\; \\lambda c_1 \\;=\\; -\\sigma^2 \\;-\\; \\lambda c_1.\n$$\n\n测试套件：\n\n使用固定的假设集\n$$\n\\mathcal{H} \\;=\\; \\{\\,-2.0,\\,-1.0,\\,-0.5,\\,0.0,\\,0.5,\\,1.0,\\,1.5,\\,2.0\\,\\},\n$$\n因此 $ |\\mathcal{H}| = 8 $，特征为 $ x_h = \\begin{bmatrix}1 \\\\ \\theta_h^2\\end{bmatrix} $。对于每个测试用例，将参数 $ (\\alpha,\\sigma,\\lambda,c_0,c_1) $ 定义为：\n\n- 情况1（典型的混合机制）：$ \\alpha=0.5, \\ \\sigma=0.2, \\ \\lambda=0.5, \\ c_0=1.0, \\ c_1=1.0 $。\n- 情况2（成本主导，蒙特卡洛方法昂贵）：$ \\alpha=0.7, \\ \\sigma=0.1, \\ \\lambda=2.0, \\ c_0=1.0, \\ c_1=5.0 $。\n- 情况3（仅考虑准确性，$ \\lambda=0.0 $）：$ \\alpha=0.6, \\ \\sigma=0.05, \\ \\lambda=0.0, \\ c_0=0.0, \\ c_1=0.0 $。\n- 情况4（中等成本下的阈值行为）：$ \\alpha=0.8, \\ \\sigma=0.1, \\ \\lambda=0.3, \\ c_0=1.0, \\ c_1=1.0 $。\n- 情况5（在 $ |\\theta|=1 $ 附近的边缘平局情况）：$ \\alpha=0.75, \\ \\sigma=0.25, \\ \\lambda=0.4, \\ c_0=1.0, \\ c_1=1.0 $。\n\n实现要求：\n\n- 使用 softmax 策略和上面定义的预期回报 $ Q(h,a) $，对 $ J(\\mathbf{W}) $ 实现梯度上升。该回报从期望的基本定律和 softmax 定义推导得出。使用基于小梯度范数或最大迭代次数的停止准则来确保收敛。将 $ \\mathbf{W} $ 初始化为零。\n- 为了可复现性，训练中不需要随机性，因为 $ Q(h,a) $ 具有封闭形式的解。在训练或评估期间，您不得对 $ \\varepsilon $ 进行采样；仅使用 $ Q(h,a) $。\n- 对于每个测试用例，输出四舍五入到六位小数的遗憾值 $ \\Delta $。\n\n最终输出格式：\n\n您的程序应产生单行输出，其中包含案例1到5的遗憾值，格式为逗号分隔的列表，并用方括号括起来（例如，\"[0.000000,0.123456,0.000100,0.000000,0.000000]\"）。不应打印任何额外文本。", "solution": "所提出的问题是一个上下文赌博机（contextual bandit）场景，旨在模拟计算科学中的一个常见决策：计算近似的准确性与其相关成本之间的权衡。智能体必须学习一个由权重矩阵 $ \\mathbf{W} $ 参数化的策略，以根据上下文（科学假设的特征）选择一个动作（一种近似方法），从而最大化预期回报。回报函数同时编码了准确性（通过负平方误差）和计算成本。该问题是有效的、适定的（well-posed），并要求从第一性原理推导出解决方案。\n\n我们首先形式化目标函数，然后推导通过梯度上升进行优化所需的梯度。\n\n假设集为 $ \\mathcal{H} $，其中每个假设 $ h \\in \\mathcal{H} $ 都与一个真实的潜藏值 $ \\theta_h $ 相关联。对于每个假设，智能体观察一个特征向量 $ x_h \\in \\mathbb{R}^2 $，其中 $ x_h = [1, \\theta_h^2]^\\top $。可用动作为 $ a \\in \\{0,1\\} $。在假设 $ h $ 上采取动作 $ a $ 的预期回报，由质量函数 $ Q(h,a) $ 表示，已给出。\n对于动作 $ a=0 $（平均场）：\n$$\nQ(h,0) = -(\\alpha - 1)^2\\theta_h^2 - \\lambda c_0\n$$\n对于动作 $ a=1 $（蒙特卡洛）：\n$$\nQ(h,1) = -\\sigma^2 - \\lambda c_1\n$$\n\n智能体的策略 $ \\pi_\\mathbf{W}(a \\mid x_h) $ 是一个基于线性 logits 的 softmax 函数。logits 为 $ z_a = \\mathbf{w}_a^\\top x_h $，其中 $ \\mathbf{w}_a^\\top $ 是权重矩阵 $ \\mathbf{W} \\in \\mathbb{R}^{2 \\times 2} $ 的第 $ a $ 行。策略为：\n$$\n\\pi_\\mathbf{W}(a \\mid x_h) = \\frac{\\exp(\\mathbf{w}_a^\\top x_h)}{\\sum_{k \\in \\{0,1\\}} \\exp(\\mathbf{w}_k^\\top x_h)}\n$$\n目标是找到参数 $ \\mathbf{W} $，以最大化所有假设上的平均预期回报：\n$$\nJ(\\mathbf{W}) = \\frac{1}{|\\mathcal{H}|} \\sum_{h \\in \\mathcal{H}} \\mathbb{E}_{a \\sim \\pi_\\mathbf{W}(\\cdot \\mid x_h)} [Q(h,a)] = \\frac{1}{|\\mathcal{H}|} \\sum_{h \\in \\mathcal{H}} \\sum_{a \\in \\{0,1\\}} \\pi_\\mathbf{W}(a \\mid x_h) Q(h,a)\n$$\n为了最大化 $ J(\\mathbf{W}) $，我们采用梯度上升法。这需要计算 $ J(\\mathbf{W}) $ 相对于参数 $ \\mathbf{W} $（即向量 $ \\mathbf{w}_0 $ 和 $ \\mathbf{w}_1 $）的梯度。由于梯度算子和求和的线性性质，我们可以专注于单个假设 $ h $ 的预期回报的梯度，我们将其表示为 $ J_h(\\mathbf{W}) = \\sum_{a \\in \\{0,1\\}} \\pi_\\mathbf{W}(a \\mid x_h) Q(h,a) $。\n\n$ J_h(\\mathbf{W}) $ 相对于任意权重向量 $ \\mathbf{w}_k $（$ k \\in \\{0,1\\} $）的梯度是：\n$$\n\\nabla_{\\mathbf{w}_k} J_h(\\mathbf{W}) = \\sum_{a \\in \\{0,1\\}} Q(h,a) \\nabla_{\\mathbf{w}_k} \\pi_\\mathbf{W}(a \\mid x_h)\n$$\n为了计算 softmax 策略的梯度，我们使用对数-导数恒等式（log-derivative identity），$ \\nabla \\pi = \\pi \\nabla \\log \\pi $。对数策略概率的梯度是：\n$$\n\\nabla_{\\mathbf{w}_k} \\log \\pi_\\mathbf{W}(a \\mid x_h) = \\nabla_{\\mathbf{w}_k} \\left( \\mathbf{w}_a^\\top x_h - \\log\\sum_{j \\in \\{0,1\\}} e^{\\mathbf{w}_j^\\top x_h} \\right)\n$$\n$$\n= (\\nabla_{\\mathbf{w}_k} \\mathbf{w}_a^\\top x_h) - \\frac{1}{\\sum_j e^{\\mathbf{w}_j^\\top x_h}} (\\nabla_{\\mathbf{w}_k} \\sum_j e^{\\mathbf{w}_j^\\top x_h})\n$$\n使用克罗内克 delta（Kronecker delta）$ \\delta_{ak} $，我们有 $ \\nabla_{\\mathbf{w}_k} \\mathbf{w}_a^\\top x_h = \\delta_{ak} x_h $。第二项变为：\n$$\n\\frac{1}{\\sum_j e^{\\mathbf{w}_j^\\top x_h}} (e^{\\mathbf{w}_k^\\top x_h} \\nabla_{\\mathbf{w}_k} \\mathbf{w}_k^\\top x_h) = \\frac{e^{\\mathbf{w}_k^\\top x_h}}{\\sum_j e^{\\mathbf{w}_j^\\top x_h}} x_h = \\pi_\\mathbf{W}(k \\mid x_h) x_h\n$$\n结合这些结果，我们得到：\n$$\n\\nabla_{\\mathbf{w}_k} \\log \\pi_\\mathbf{W}(a \\mid x_h) = (\\delta_{ak} - \\pi_\\mathbf{W}(k \\mid x_h)) x_h\n$$\n将此代入 $ J_h(\\mathbf{W}) $ 的梯度中：\n$$\n\\nabla_{\\mathbf{w}_k} J_h(\\mathbf{W}) = \\sum_{a \\in \\{0,1\\}} Q(h,a) \\pi_\\mathbf{W}(a \\mid x_h) (\\delta_{ak} - \\pi_\\mathbf{W}(k \\mid x_h)) x_h\n$$\n我们可以重新整理这个和：\n$$\n= \\left( Q(h,k)\\pi_\\mathbf{W}(k \\mid x_h) - \\pi_\\mathbf{W}(k \\mid x_h) \\sum_{a \\in \\{0,1\\}} Q(h,a) \\pi_\\mathbf{W}(a \\mid x_h) \\right) x_h\n$$\n令 $ V_h(\\mathbf{W}) = \\sum_{a \\in \\{0,1\\}} Q(h,a) \\pi_\\mathbf{W}(a \\mid x_h) $ 为在策略 $ \\pi_\\mathbf{W} $ 下假设 $ h $ 的期望值。表达式简化为带有基线的标准策略梯度形式：\n$$\n\\nabla_{\\mathbf{w}_k} J_h(\\mathbf{W}) = \\pi_\\mathbf{W}(k \\mid x_h) (Q(h,k) - V_h(\\mathbf{W})) x_h\n$$\n这是一个通用且基本的结果。对于我们的双动作情况，我们可以进一步简化基线项 $ Q(h,k) - V_h(\\mathbf{W}) $：\n对于 $ k=0 $：$ Q(h,0) - (\\pi_0 Q_0 + \\pi_1 Q_1) = (1-\\pi_0)Q_0 - \\pi_1 Q_1 = \\pi_1 Q_0 - \\pi_1 Q_1 = \\pi_1(Q_0 - Q_1) $。\n对于 $ k=1 $：$ Q(h,1) - (\\pi_0 Q_0 + \\pi_1 Q_1) = Q_1 - \\pi_0 Q_0 - (1-\\pi_0)Q_1 = \\pi_0 Q_1 - \\pi_0 Q_0 = \\pi_0(Q_1 - Q_0) $。\n对于单个假设 $ h $，相对于 $ \\mathbf{w}_0 $ 和 $ \\mathbf{w}_1 $ 的梯度是：\n$$\n\\nabla_{\\mathbf{w}_0} J_h(\\mathbf{W}) = \\pi_0 (\\pi_1(Q_0 - Q_1)) x_h = \\pi_0\\pi_1(Q(h,0) - Q(h,1))x_h\n$$\n$$\n\\nabla_{\\mathbf{w}_1} J_h(\\mathbf{W}) = \\pi_1 (\\pi_0(Q_1 - Q_0)) x_h = \\pi_0\\pi_1(Q(h,1) - Q(h,0))x_h\n$$\n注意 $ \\nabla_{\\mathbf{w}_0} J_h(\\mathbf{W}) = -\\nabla_{\\mathbf{w}_1} J_h(\\mathbf{W}) $，这反映了策略是由 logits 之差 $ z_1 - z_0 = (\\mathbf{w}_1 - \\mathbf{w}_0)^\\top x_h $ 决定的事实。\n\n目标函数 $ J(\\mathbf{W}) $ 的总梯度是这些单个假设梯度在集合 $ \\mathcal{H} $ 上的平均值：\n$$\n\\nabla_\\mathbf{W} J(\\mathbf{W}) = \\frac{1}{|\\mathcal{H}|} \\sum_{h \\in \\mathcal{H}} \\nabla_\\mathbf{W} J_h(\\mathbf{W})\n$$\n学习算法是一个简单的梯度上升过程。从 $ \\mathbf{W} $ 为零矩阵开始，我们使用学习率 $ \\eta $ 迭代更新权重：\n$$\n\\mathbf{W}_{t+1} = \\mathbf{W}_t + \\eta \\nabla_\\mathbf{W} J(\\mathbf{W}_t)\n$$\n此过程持续进行固定的迭代次数，或直到梯度的范数低于某个容差，以确保收敛到目标函数的局部最大值。\n\n训练后，计算遗憾值 $ \\Delta $。最优策略的值 $ V_{\\text{opt}} $ 是通过为每个假设采取最佳可能动作来确定的：$ V_{\\text{opt}} = \\frac{1}{|\\mathcal{H}|} \\sum_h \\max_{a} Q(h,a) $。学习到的策略的值 $ V_{\\text{learn}} $ 是通过首先将随机 softmax 策略转换为确定性策略（选择概率最高的动作 $ a^*_h = \\arg\\max_a \\pi_{\\mathbf{W}}(a \\mid x_h) $），然后计算平均回报：$ V_{\\text{learn}} = \\frac{1}{|\\mathcal{H}|} \\sum_h Q(h, a^*_h) $。遗憾值是差值 $ \\Delta = V_{\\text{opt}} - V_{\\text{learn}} $。非零的遗憾值表明，对于至少一个假设，学习到的确定性策略没有选择产生最大预期回报的动作。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the contextual bandit problem for a series of test cases using\n    gradient ascent on the expected reward.\n    \"\"\"\n    \n    # Fixed hypothesis set\n    thetas = np.array([-2.0, -1.0, -0.5, 0.0, 0.5, 1.0, 1.5, 2.0])\n    H_size = len(thetas)\n    # Feature vectors x_h = [1, theta_h^2]^T\n    features = np.vstack((np.ones(H_size), thetas**2)).T\n\n    # Test cases: (alpha, sigma, lambda, c0, c1)\n    test_cases = [\n        (0.5, 0.2, 0.5, 1.0, 1.0),\n        (0.7, 0.1, 2.0, 1.0, 5.0),\n        (0.6, 0.05, 0.0, 0.0, 0.0),\n        (0.8, 0.1, 0.3, 1.0, 1.0),\n        (0.75, 0.25, 0.4, 1.0, 1.0)\n    ]\n\n    results = []\n\n    for case in test_cases:\n        alpha, sigma, lam, c0, c1 = case\n        \n        # --- Pre-calculate Q-values for all hypotheses ---\n        # Q_values matrix of shape (H_size, 2)\n        Q_values = np.zeros((H_size, 2))\n        \n        # Q(h, 0) for all h\n        Q_values[:, 0] = -((alpha - 1)**2) * (thetas**2) - lam * c0\n        # Q(h, 1) for all h\n        Q_values[:, 1] = -(sigma**2) - lam * c1\n        \n        # --- Gradient Ascent Training ---\n        W = np.zeros((2, 2))  # Policy weights: W[0] for w0, W[1] for w1\n        learning_rate = 0.1\n        num_iterations = 20000\n\n        for _ in range(num_iterations):\n            grad_W = np.zeros((2, 2))\n            \n            # Compute logits for all hypotheses at once\n            # logits shape: (H_size, 2)\n            logits = features @ W.T\n            \n            # Stable softmax\n            logits_stable = logits - np.max(logits, axis=1, keepdims=True)\n            exps = np.exp(logits_stable)\n            probs = exps / np.sum(exps, axis=1, keepdims=True) # shape (H_size, 2)\n            \n            # Calculate gradient contributions\n            # pi_0 * pi_1 * (Q0 - Q1) for each hypothesis\n            factor = probs[:, 0] * probs[:, 1] * (Q_values[:, 0] - Q_values[:, 1]) # shape (H_size,)\n\n            # Reshape factor to (H_size, 1) to broadcast with features (H_size, 2)\n            factor = factor[:, np.newaxis]\n            \n            # Gradient for w0 for all h\n            grad_w0 = np.sum(factor * features, axis=0)\n            # Gradient for w1 is the negative of grad_w0\n            grad_w1 = -grad_w0\n            \n            # Average gradient over all hypotheses\n            grad_W[0, :] = grad_w0 / H_size\n            grad_W[1, :] = grad_w1 / H_size\n\n            # Update weights\n            W += learning_rate * grad_W\n\n        # --- Evaluation ---\n        # Optimal policy value\n        V_opt_sum = np.sum(np.max(Q_values, axis=1))\n        V_opt = V_opt_sum / H_size\n        \n        # Learned policy value\n        # Recalculate final probabilities with trained W\n        final_logits = features @ W.T\n        final_logits_stable = final_logits - np.max(final_logits, axis=1, keepdims=True)\n        final_exps = np.exp(final_logits_stable)\n        final_probs = final_exps / np.sum(final_exps, axis=1, keepdims=True)\n        \n        # Deterministic learned actions\n        learned_actions = np.argmax(final_probs, axis=1)\n        \n        # Calculate value of learned policy\n        V_learn_sum = np.sum(Q_values[np.arange(H_size), learned_actions])\n        V_learn = V_learn_sum / H_size\n        \n        # Regret\n        regret = V_opt - V_learn\n        \n        # Format and append result\n        results.append(f\"{regret:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3186206"}, {"introduction": "科学发现不仅依赖于强大的分析工具，更依赖于高质量的数据。本练习模拟了一个公民科学场景，您的任务是设计一个智能代理，通过序列化地向参与者提问来优化数据收集过程，从而最大化分类模型的精度增益。您将应用前沿的上下文赌博机算法（如 LinUCB），学习如何在探索（尝试新问题类型）和利用（选择已知效果最好的问题）之间做出权衡，以加速科学发现的进程 [@problem_id:3186203]。", "problem": "您的任务是设计并评估一种强化学习（RL）策略，该策略用于在一个公民科学平台上按顺序选择下一个要向参与者提出的问题。每个问题都属于一个离散的问题类型集合，目标是最大化有助于科学发现的累积分类改进。将此交互建模为一个上下文赌博机（contextual bandit），该模型源于单步马尔可夫决策过程（MDP）的近似，其中即时奖励捕获了因参与者响应而产生的分类增益。必须使用以下定义和约束来构建一个科学上合理且自洽的仿真测试平台：\n\n- 强化学习（RL）在离散时间步 $t \\in \\{1,2,\\dots,T\\}$ 上运行，其动作集为 $\\mathcal{A} = \\{1,2,\\dots,K\\}$，代表问题类型。环境在每一步独立采样提供一个上下文向量 $c_t \\in \\mathbb{R}^d$，并维护一个标量分类准确率 $a_t \\in [0,1]$，该准确率根据参与者的贡献而演变。\n- 在每一步，智能体观察 $c_t$，选择一个动作 $i \\in \\mathcal{A}$，并获得一个奖励 $r_t \\in \\mathbb{R}_{\\ge 0}$，该奖励代表分类准确率的增量收益。定义松弛量 $s_t = 1 - a_t$ 以捕捉收益递减效应，并通过拼接一个偏置项、上下文和松弛量来定义增强特征向量 $x_t \\in \\mathbb{R}^{d+2}$：\n$$\nx_t = \\begin{bmatrix} 1 \\\\ c_t \\\\ s_t \\end{bmatrix}.\n$$\n- 动作 $i$ 的真实期望奖励是特征的线性函数，其参数 $w^{(i)} \\in \\mathbb{R}^{d+2}$ 未知，并受到噪声干扰，同时被截断以反映非负增益和收益递减。具体来说，观察到的奖励是：\n$$\nr_t = \\min\\left\\{ s_t,\\ \\max\\left\\{ 0,\\ x_t^\\top w^{(i)} + \\epsilon_t \\right\\}\\right\\},\n$$\n其中 $\\epsilon_t \\sim \\mathcal{N}(0, \\sigma^2)$ 是独立高斯噪声。分类准确率随之更新为：\n$$\na_{t+1} = \\min\\{1,\\ a_t + r_t\\}.\n$$\n- 您的强化学习策略必须仅根据过去观察到的上下文和奖励来选择动作，并以一种有原则的方式权衡探索与利用。该问题期望一种上下文策略，利用观察到的 $x_t$ 来推断哪个动作将最大化预期累积奖励。\n\n基本基础和假设：\n- 将此交互视为一个上下文赌博机，该模型通过单步 MDP 简化推导而来，其原理是即时奖励概括了参与者贡献的价值。\n- 使用经过充分检验的统计建模假设：独立高斯噪声、特征中的线性期望奖励，以及通过松弛量 $s_t$ 实现的收益递减。\n- 在选择动作时，不要假设对真实参数 $w^{(i)}$ 有任何先验知识；智能体必须从数据中学习。\n\n您的程序必须实现上述环境和一个能学习选择动作的强化学习策略。它必须使用指定的参数运行以下测试套件，并生成所需的输出格式。\n\n测试套件规范：\n- 所有测试的通用上下文分布：$c_t \\sim \\mathcal{N}(\\mu, \\Sigma)$，其中 $\\mu = [0, 0]$ 且 $\\Sigma = I_2$，$I_2$ 是 $2 \\times 2$ 单位矩阵。因此 $d = 2$ 且 $x_t \\in \\mathbb{R}^{4}$。\n- 对于所有测试，特征排序为 $x_t = [1,\\ c_{t,1},\\ c_{t,2},\\ s_t]^\\top$。\n\n测试用例 1（正常路径）：\n- 动作数量 $K = 3$。\n- 时间范围 $T = 200$。\n- 初始准确率 $a_0 = 0.5$。\n- 噪声标准差 $\\sigma = 0.05$。\n- 动作的真实参数：\n$$\nw^{(1)} = [0.05,\\ 0.8,\\ 0.0,\\ 0.5],\\quad\nw^{(2)} = [0.05,\\ -0.2,\\ 0.9,\\ 0.3],\\quad\nw^{(3)} = [0.05,\\ 0.4,\\ 0.4,\\ 0.7].\n$$\n\n测试用例 2（边界条件：接近饱和的准确率）：\n- 动作数量 $K = 2$。\n- 时间范围 $T = 100$。\n- 初始准确率 $a_0 = 0.95$。\n- 噪声标准差 $\\sigma = 0.05$。\n- 动作的真实参数：\n$$\nw^{(1)} = [0.02,\\ 0.5,\\ 0.5,\\ 0.2],\\quad\nw^{(2)} = [0.02,\\ 0.6,\\ -0.1,\\ 0.2].\n$$\n\n测试用例 3（边缘情况：高噪声环境）：\n- 动作数量 $K = 3$。\n- 时间范围 $T = 300$。\n- 初始准确率 $a_0 = 0.4$。\n- 噪声标准差 $\\sigma = 0.3$。\n- 动作的真实参数：\n$$\nw^{(1)} = [0.10,\\ 0.3,\\ 0.3,\\ 0.4],\\quad\nw^{(2)} = [0.10,\\ 0.35,\\ 0.25,\\ 0.35],\\quad\nw^{(3)} = [0.10,\\ 0.2,\\ 0.5,\\ 0.3].\n$$\n\n输出要求：\n- 对于每个测试用例，计算两个量：最终准确率 $a_T$ 和累积奖励 $R = \\sum_{t=1}^T r_t$。\n- 您的程序必须生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果，每个测试用例表示为一个双元素列表。确切格式为：\n$$\n\\text{[}[a_T^{(1)}, R^{(1)}],[a_T^{(2)}, R^{(2)}],[a_T^{(3)}, R^{(3)}]\\text{]},\n$$\n其中所有数字必须打印为保留六位小数的小数，并且行内任何位置都不允许有空格。\n- 在内部设置固定的随机种子以确保可复现性。不要读取任何输入，也不要写入任何文件。\n\n科学真实性和普适性：\n- 该设置期望智能体通过选择问题来提高准确率，其收益应反映对参与者上下文的统计依赖性以及模型饱和时的收益递减。\n- 所有随机变量和参数都得到了良好定义，并与计算科学中的标准统计建模保持一致。", "solution": "问题陈述已经过验证，被认为是合理的。它具有科学依据，定义明确且客观。为实现可复现的仿真，所有必要的参数和定义均已提供。该问题要求设计一种强化学习（RL）策略，以解决一个为模拟公民科学场景而构建的上下文赌博机问题。以下解决方案提供了一种有原则的算法方法及其实现。\n\n该问题是线性上下文赌博机的一个实例。在每个离散时间步 $t \\in \\{1, 2, \\dots, T\\}$，智能体必须从包含 $K$ 个可用动作的集合 $\\mathcal{A} = \\{1, 2, \\dots, K\\}$ 中选择一个动作 $i$。决策由上下文向量 $c_t \\in \\mathbb{R}^d$ 提供信息。问题将增强特征向量 $x_t \\in \\mathbb{R}^{d+2}$ 定义为一个偏置项、上下文和一个依赖于状态的松弛项 $s_t = 1 - a_t$ 的拼接，其中 $a_t \\in [0, 1]$ 是当前的分类准确率。\n$$\nx_t = \\begin{bmatrix} 1 \\\\ c_t \\\\ s_t \\end{bmatrix}\n$$\n采取动作 $i$ 的期望奖励是该特征向量的线性函数，由一个未知的权重向量 $w^{(i)} \\in \\mathbb{R}^{d+2}$ 参数化。观察到的奖励 $r_t$ 是此线性函数的含噪实现，并被截断以确保非负性，且准确率增益不超过可用的松弛量 $s_t$。\n$$\nr_t = \\min\\left\\{ s_t,\\ \\max\\left\\{ 0,\\ x_t^\\top w^{(i)} + \\epsilon_t \\right\\}\\right\\}, \\quad \\text{其中 } \\epsilon_t \\sim \\mathcal{N}(0, \\sigma^2)\n$$\n系统状态（由准确率 $a_t$ 表示）根据获得的奖励进行演变：\n$$\na_{t+1} = \\min\\{1,\\ a_t + r_t\\}\n$$\n智能体不知道真实参数 $w^{(i)}$，必须通过观察上下文、动作和奖励来学习这些参数，以最大化累积奖励 $R = \\sum_{t=1}^T r_t$。这需要一种策略来平衡探索（尝试不同动作以学习其参数）和利用（选择被认为是最佳的动作）。\n\n对于此类问题，一个高效且有理论基础的算法是线性上置信界（LinUCB）算法。LinUCB 将每个动作 $i$ 的期望奖励建模为上下文的线性函数，即 $\\mathbb{E}[r_t | x_t, i] \\approx x_t^\\top w^{(i)}$，并使用在线岭回归来估计未知的权重向量 $\\hat{w}^{(i)}$。\n\n对于每个动作 $i \\in \\mathcal{A}$，该算法维护一个设计矩阵 $A_i \\in \\mathbb{R}^{(d+2) \\times (d+2)}$ 和一个奖励向量 $b_i \\in \\mathbb{R}^{d+2}$。它们被初始化为 $A_i = I_{d+2}$（单位矩阵，对应于正则化先验）和 $b_i = \\mathbf{0}$。在观察到所选动作 $i_t$ 的一个特征-奖励对 $(x_t, r_t)$ 后，统计数据更新如下：\n$$\nA_{i_t} \\leftarrow A_{i_t} + x_t x_t^\\top\n$$\n$$\nb_{i_t} \\leftarrow b_{i_t} + r_t x_t\n$$\n在每个时间步 $t$，智能体为每个动作 $i$ 计算权重向量的估计值：\n$$\n\\hat{w}^{(i)} = A_i^{-1} b_i\n$$\nLinUCB 的核心在于其动作选择策略。它不是贪婪地选择具有最高预测奖励 $x_t^\\top \\hat{w}^{(i)}$ 的动作，而是增加一个探索奖励。该奖励与当前上下文 $x_t$ 的估计不确定性成正比。动作 $i_t$ 的选择依据如下：\n$$\ni_t = \\arg\\max_{i \\in \\mathcal{A}} \\left( x_t^\\top \\hat{w}^{(i)} + \\alpha \\sqrt{x_t^\\top A_i^{-1} x_t} \\right)\n$$\n项 $x_t^\\top \\hat{w}^{(i)}$ 是预测奖励（利用），而项 $\\alpha \\sqrt{x_t^\\top A_i^{-1} x_t}$ 是该估计的上置信界（探索）。超参数 $\\alpha \\ge 0$ 控制着权衡；对于本实现，选择标准值 $\\alpha = 1.0$。\n\n每个测试用例的仿真过程如下：\n1. 为保证可复现性，设置固定的随机种子。\n2. 初始化仿真参数：时间范围 $T$、动作数量 $K$、初始准确率 $a_0$、噪声 $\\sigma$ 和真实权重 $w^{(i)}$。\n3. 初始化 LinUCB 智能体，其有 $K$ 个臂，特征维度为 $d+2=4$。\n4. 初始化当前准确率 $a_t = a_0$ 和累积奖励 $R = 0$。\n5. 对于从 $1$ 到 $T$ 的每个时间步 $t$：\n    a. 计算当前松弛量 $s_t = 1 - a_t$。\n    b. 采样一个上下文向量 $c_t \\sim \\mathcal{N}(\\mu, \\Sigma)$，其中 $\\mu=[0,0]$ 且 $\\Sigma=I_2$。\n    c. 构建增强特征向量 $x_t = [1, c_{t,1}, c_{t,2}, s_t]^\\top$。\n    d. LinUCB 智能体使用 UCB 准则选择一个动作 $i_t$。\n    e. 环境根据真实权重 $w^{(i_t)}$、特征向量 $x_t$、一个随机噪声样本 $\\epsilon_t \\sim \\mathcal{N}(0, \\sigma^2)$ 生成奖励 $r_t$，并应用指定的截断。\n    f. 更新分类准确率 $a_{t+1} = \\min\\{1, a_t + r_t\\}$ 和累积奖励 $R = R + r_t$。\n    g. 智能体的内部模型使用观察值 $(x_t, i_t, r_t)$ 进行更新。\n6. 在 $T$ 步之后，记录最终准确率 $a_T$ 和总累积奖励 $R$。对问题陈述中指定的所有三个测试用例重复此过程。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the simulation for all test cases and print the results.\n    \"\"\"\n    # Set a fixed random seed for reproducibility of the simulation.\n    np.random.seed(42)\n\n    test_cases = [\n        {\n            \"K\": 3, \"T\": 200, \"a0\": 0.5, \"sigma\": 0.05,\n            \"w\": np.array([\n                [0.05, 0.8, 0.0, 0.5],\n                [0.05, -0.2, 0.9, 0.3],\n                [0.05, 0.4, 0.4, 0.7]\n            ]),\n            \"mu\": np.array([0.0, 0.0]), \"d\": 2\n        },\n        {\n            \"K\": 2, \"T\": 100, \"a0\": 0.95, \"sigma\": 0.05,\n            \"w\": np.array([\n                [0.02, 0.5, 0.5, 0.2],\n                [0.02, 0.6, -0.1, 0.2]\n            ]),\n            \"mu\": np.array([0.0, 0.0]), \"d\": 2\n        },\n        {\n            \"K\": 3, \"T\": 300, \"a0\": 0.4, \"sigma\": 0.3,\n            \"w\": np.array([\n                [0.10, 0.3, 0.3, 0.4],\n                [0.10, 0.35, 0.25, 0.35],\n                [0.10, 0.2, 0.5, 0.3]\n            ]),\n            \"mu\": np.array([0.0, 0.0]), \"d\": 2\n        }\n    ]\n\n    all_results = []\n    for params in test_cases:\n        final_a, total_r = run_simulation(params)\n        all_results.append(f\"[{final_a:.6f},{total_r:.6f}]\")\n\n    print(f\"[{','.join(all_results)}]\")\n\nclass LinUCBAgent:\n    \"\"\"\n    Implements the Linear Upper Confidence Bound (LinUCB) algorithm.\n    \"\"\"\n    def __init__(self, K, d, alpha=1.0):\n        self.K = K\n        self.dim = d + 2  # bias, context_dims, slack\n        self.alpha = alpha\n        # Initialize A as identity matrix and b as zero vector for each arm\n        self.A = [np.identity(self.dim) for _ in range(K)]\n        self.b = [np.zeros(self.dim) for _ in range(K)]\n\n    def select_action(self, x_t):\n        \"\"\"\n        Selects an action based on the UCB criterion for the given context.\n        \"\"\"\n        scores = []\n        for i in range(self.K):\n            A_inv = np.linalg.inv(self.A[i])\n            w_hat = A_inv @ self.b[i]\n            p_t_i = x_t.T @ w_hat\n            ucb = self.alpha * np.sqrt(x_t.T @ A_inv @ x_t)\n            scores.append(p_t_i + ucb)\n        \n        return np.argmax(scores)\n\n    def update(self, action_idx, x_t, r_t):\n        \"\"\"\n        Updates the model for the chosen action with the observed reward.\n        \"\"\"\n        self.A[action_idx] += np.outer(x_t, x_t)\n        self.b[action_idx] += r_t * x_t\n\ndef run_simulation(params):\n    \"\"\"\n    Runs a single simulation episode for a given set of parameters.\n    \"\"\"\n    K = params[\"K\"]\n    T = params[\"T\"]\n    a_t = params[\"a0\"]\n    sigma = params[\"sigma\"]\n    true_w = params[\"w\"]\n    mu = params[\"mu\"]\n    d = params[\"d\"]\n    \n    agent = LinUCBAgent(K, d, alpha=1.0)\n    \n    cumulative_reward = 0.0\n    context_cov = np.identity(d)\n\n    for _ in range(T):\n        s_t = 1.0 - a_t\n        \n        # Sample context and form feature vector\n        c_t = np.random.multivariate_normal(mu, context_cov)\n        x_t = np.concatenate(([1.0], c_t, [s_t]))\n\n        # Agent selects an action\n        action_idx = agent.select_action(x_t)\n\n        # Environment generates reward\n        noise = np.random.normal(0, sigma)\n        raw_reward = x_t.T @ true_w[action_idx] + noise\n        r_t = np.min([s_t, np.max([0, raw_reward])])\n\n        # Update system state\n        a_t = np.min([1.0, a_t + r_t])\n        cumulative_reward += r_t\n\n        # Agent updates its model\n        agent.update(action_idx, x_t, r_t)\n\n    return a_t, cumulative_reward\n\nsolve()\n```", "id": "3186203"}, {"introduction": "最高层次的科学决策涉及对整个研究组合的战略管理。这个高级练习将科学探究过程本身构建为一个马尔可夫决策过程（MDP），您的强化学习代理将扮演“科研经理”的角色，决定淘汰哪些假设以将有限资源重新分配给更有前景的研究方向。通过实现 Q-learning 算法并利用信息熵设计奖励函数，您将体验到如何运用强化学习来解决复杂的、涉及长远规划的科学策略问题 [@problem_id:3186163]。", "problem": "给定一个强化学习（RL）的正式决策场景，其背景是选择放弃哪些科学假说，以最大化剩余假说的信息期望价值。该环境被建模为一个马尔可夫决策过程（MDP），其中每个状态编码了仍在考虑中的假说子集。动作是放弃一个当前活跃的假说。每一步的奖励基于采取动作后剩余假说的信息期望价值（EVI），并根据一个简单的资源共享模型进行缩放。\n\n基本基础:\n- 马尔可夫决策过程 (MDP): 一个元组 $(\\mathcal{S}, \\mathcal{A}, P, r, \\gamma)$，包含状态 $\\mathcal{S}$、动作 $\\mathcal{A}$、转移概率 $P$、奖励函数 $r$ 和折扣因子 $\\gamma$。\n- 期望价值与信息: 对于先验概率为 $p \\in (0,1)$ 的二元假说，其信息内容通过 Shannon 熵（以自然单位，奈特(nats)计量）来衡量，即 $H(p) = -p \\ln(p) - (1-p) \\ln(1-p)$。\n- 资源共享: 一个固定的预算 $B$ 在所有当前活跃的假说中平均分配。如果剩下 $n$ 个假说，每个假说获得资源份额 $B/n$。\n\n模型详述:\n- 设有 $N$ 个假说。每个假说 $i \\in \\{0,1,\\dots,N-1\\}$ 具有先验概率 $p_i \\in (0,1)$ 和信息权重 $w_i > 0$，代表了学习关于假说 $i$ 的决策理论价值。\n- 一个状态 $s$ 是当前活跃假说的索引的任意子集。一个动作 $a \\in s$ 会放弃假说 $a$，从而产生新状态 $s' = s \\setminus \\{a\\}$。\n- 在状态 $s$ 中采取动作 $a$ 的即时奖励定义为\n$$\nr(s,a) = \\frac{B}{|s'|} \\sum_{j \\in s'} w_j H(p_j),\n$$\n其中 $H(p_j) = -p_j \\ln(p_j) - (1-p_j)\\ln(1-p_j)$ 且 $|s'|$ 是采取动作后剩余假说的数量。对数为自然对数；信息单位是奈特(nats)。\n- 当执行完 $R_{\\max}$ 次放弃操作后，回合即终止。我们要求 $R_{\\max} \\le N-1$，以确保终止时至少还剩一个假说。如果 $R_{\\max} = 0$，则不采取任何动作，基线奖励定义为\n$$\nr_{\\mathrm{baseline}}(s) = \\frac{B}{|s|} \\sum_{j \\in s} w_j H(p_j).\n$$\n\n任务:\n- 实现 Q-learning 算法来学习一个用于放弃决策的策略。使用 $\\varepsilon$-贪心探索策略和学习率 $\\alpha$。状态可以用一个覆盖 $N$ 个假说的位掩码来表示。在给定状态下，只允许采取对应于当前活跃假说的动作。Q-learning 更新规则为\n$$\nQ(s,a) \\leftarrow (1-\\alpha) Q(s,a) + \\alpha \\left( r(s,a) + \\gamma \\max_{a' \\in \\mathcal{A}(s')} Q(s',a') \\right),\n$$\n其中 $s'$ 是在状态 $s$ 中采取动作 $a$ 后的下一个状态。\n- 训练结束后，从完整的假说集合开始，评估贪心策略（在允许的动作中选择具有最大 $Q$ 值的动作），直到执行了 $R_{\\max}$ 次放弃操作。对于每个测试用例，报告被放弃假说的索引列表（使用从零开始的索引）和评估期间获得的累积无折扣奖励总和。对于 $R_{\\max} = 0$ 的情况，报告一个空的放弃列表和上述定义的基线奖励。\n- 对数的底必须是自然对数。\n\n测试套件:\n为以下参数集提供结果。在每个案例中，使用给定的参数 $(N, p, w, B, R_{\\max}, \\gamma, \\text{episodes}, \\varepsilon_{\\text{init}}, \\varepsilon_{\\text{final}}, \\alpha)$，其中所有向量均按照假说索引 $i \\in \\{0,1,\\dots,N-1\\}$ 的顺序给出。\n\n- 案例 1 (一般情况):\n  - $N = 4$\n  - $p = [0.10, 0.70, 0.50, 0.05]$\n  - $w = [1.00, 0.90, 0.60, 0.20]$\n  - $B = 1.00$\n  - $R_{\\max} = 2$\n  - $\\gamma = 0.95$\n  - $\\text{episodes} = 12000$\n  - $\\varepsilon_{\\text{init}} = 0.30$\n  - $\\varepsilon_{\\text{final}} = 0.02$\n  - $\\alpha = 0.30$\n\n- 案例 2 (边缘概率，接近 $0$ 或 $1$ 但严格在 $(0,1)$ 内部):\n  - $N = 5$\n  - $p = [0.01, 0.85, 0.40, 0.95, 0.20]$\n  - $w = [0.50, 1.20, 0.80, 0.10, 0.70]$\n  - $B = 1.00$\n  - $R_{\\max} = 2$\n  - $\\gamma = 0.90$\n  - $\\text{episodes} = 15000$\n  - $\\varepsilon_{\\text{init}} = 0.30$\n  - $\\varepsilon_{\\text{final}} = 0.02$\n  - $\\alpha = 0.30$\n\n- 案例 3 (平局：相同的先验概率和权重):\n  - $N = 4$\n  - $p = [0.50, 0.50, 0.50, 0.50]$\n  - $w = [1.00, 1.00, 1.00, 1.00]$\n  - $B = 1.00$\n  - $R_{\\max} = 3$\n  - $\\gamma = 0.95$\n  - $\\text{episodes} = 10000$\n  - $\\varepsilon_{\\text{init}} = 0.30$\n  - $\\varepsilon_{\\text{final}} = 0.02$\n  - $\\alpha = 0.30$\n\n- 案例 4 (无放弃预算):\n  - $N = 3$\n  - $p = [0.40, 0.60, 0.55]$\n  - $w = [0.90, 0.80, 0.70]$\n  - $B = 1.00$\n  - $R_{\\max} = 0$\n  - $\\gamma = 0.95$\n  - $\\text{episodes} = 1$\n  - $\\varepsilon_{\\text{init}} = 0.00$\n  - $\\varepsilon_{\\text{final}} = 0.00$\n  - $\\alpha = 0.30$\n\n最终输出格式:\n你的程序应生成单行输出，其中包含一个用方括号括起来的、以逗号分隔的结果列表，不含空格。每个案例的结果本身是一个包含两个元素的列表：放弃序列（一个从零开始的索引列表）和累积无折扣奖励（一个浮点数）。例如，两个案例的输出可能如下所示：\n$[[[0,2],1.234],[[],0.567]]$.", "solution": "我们将该决策问题建模为马尔可夫决策过程（MDP），并使用强化学习（RL）通过 Q-learning 方法来解决。目标是选择要放弃的假说，以便在一个简单的资源共享模型下，在固定的放弃期限内最大化剩余假说的信息期望价值（EVI）。\n\n原理与定义:\n- 强化学习（RL）在一个由 MDP $(\\mathcal{S}, \\mathcal{A}, P, r, \\gamma)$ 建模的环境中优化决策，其中 $\\gamma \\in [0,1)$ 是折扣因子。智能体选择动作以最大化期望的折扣累积奖励。\n- 信息期望价值，基于决策论和信息论：对于一个先验概率为 $p \\in (0,1)$ 的二元假说，Shannon 熵 $H(p) = -p \\ln(p) - (1-p) \\ln(1-p)$（使用自然对数）衡量其不确定性。如果我们用 $w > 0$ 代表学习的效用，并假设资源在活跃假说中平均分配，那么即时奖励与剩余假说的 $w H(p)$ 总和成正比，并按资源份额进行缩放。\n\n环境构建:\n- 设有 $N$ 个假说，其先验概率为 $p_i \\in (0,1)$，权重为 $w_i > 0$。一个状态 $s \\subseteq \\{0,1,\\dots,N-1\\}$ 代表活跃的假说。一个动作 $a \\in s$ 会放弃假说 $a$，产生新状态 $s' = s \\setminus \\{a\\}$。\n- 如果 $n' = |s'|$，那么每个剩余假说的资源份额为 $B/n'$。即时奖励为\n$$\nr(s,a) = \\frac{B}{n'} \\sum_{j \\in s'} w_j H(p_j), \\quad H(p_j) = -p_j \\ln(p_j) - (1-p_j)\\ln(1-p_j).\n$$\n这是期望价值定义和 Shannon 熵作为一种经过充分检验的信息度量的直接应用。\n- 一旦完成 $R_{\\max}$ 次放弃操作，回合即终止。这构成了一个具有离散且确定性转移动态的有限期决策问题。\n\n算法推导:\n- Q-learning 近似满足 Bellman 最优性方程的最优动作价值函数 $Q^*(s,a)$：\n$$\nQ^*(s,a) = r(s,a) + \\gamma \\max_{a' \\in \\mathcal{A}(s')} Q^*(s',a'),\n$$\n其中 $s'$ 是在状态 $s$ 中采取动作 $a$ 后的状态。\n- 学习更新规则为：\n$$\nQ(s,a) \\leftarrow (1-\\alpha) Q(s,a) + \\alpha \\left( r(s,a) + \\gamma \\max_{a' \\in \\mathcal{A}(s')} Q(s',a') \\right),\n$$\n其中学习率为 $\\alpha \\in (0,1]$。探索通过 $\\varepsilon$-贪心策略实现：以概率 $\\varepsilon$ 选择一个随机的允许动作；否则选择最大化 $Q(s,a)$ 的动作。\n- 状态由长度为 $N$ 的位掩码表示。在状态 $s$ 中，允许的动作是使得位掩码在位置 $i$ 处为 1 的索引 $i$。一个回合从完整掩码（所有假说都活跃）开始，并在 $R_{\\max}$ 次放弃后结束。对于 $R_{\\max} = 0$ 的情况，不采取任何动作，基线奖励为：\n$$\nr_{\\mathrm{baseline}}(s) = \\frac{B}{|s|} \\sum_{j \\in s} w_j H(p_j).\n$$\n\n科学依据:\n- 奖励设计遵循以下原则：期望价值是所有结果与其价值乘积的总和；使用 $H(p)$ 作为不确定性下的期望信息，使用权重 $w$ 作为学习某个假说的效用缩放。这结合了基本的决策理论构造和经过充分检验的 Shannon 熵。\n- 放弃一个假说会释放其资源份额，以便在剩余假说中重新平均分配，从而提高每个假说的期望信息捕获率，同时减少对总和有贡献的假说数量。智能体必须在移除低信息假说和保持足够广度之间进行权衡，这是科研组合管理中一个现实的考量。\n\n实现细节:\n- 对于每个测试用例，Q-learning 智能体将训练指定的 episodes 数量，探索参数 $\\varepsilon$ 从 $\\varepsilon_{\\text{init}}$ 线性退火到 $\\varepsilon_{\\text{final}}$。训练后，执行贪心策略以生成放弃序列和累积无折扣奖励。\n- 假说使用从零开始的索引进行报告。输出是一个单行字符串，包含每个案例结果的列表，其中每个结果本身是一个列表 $[\\text{sequence}, \\text{reward}]$。\n\n边缘情况覆盖:\n- 案例 2 使用非常接近 0 或 1（但严格在内部）的 $p$ 值，测试 $H(p)$ 的数值稳定性；实现将 $p$ 限制在远离 0 和 1 的机器安全边界内，以避免 $\\ln(0)$，这与定义域要求 $p \\in (0,1)$ 一致。\n- 案例 3 通过相同的 $p$ 和 $w$ 制造了平局情况，表明学习过程中的任何对称性破缺仍然能产生一致的序列。\n- 案例 4 设置 $R_{\\max} = 0$，确保实现能正确返回无放弃操作和所定义的基线奖励。\n\n最终输出:\n- 程序准确打印一行：一个用方括号括起来的、无空格的逗号分隔列表。每个元素是一个表示为列表的对：放弃序列（整数列表）和累积无折扣奖励（浮点数），遵循指定的格式。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef shannon_entropy_nats(p: float) -> float:\n    \"\"\"Compute Shannon entropy H(p) in nats for a binary variable.\"\"\"\n    # Clamp to avoid log(0); the problem states p in (0,1), but clamp for numerical safety.\n    eps = 1e-12\n    p = float(np.clip(p, eps, 1.0 - eps))\n    return -(p * np.log(p) + (1.0 - p) * np.log(1.0 - p))\n\nclass HypothesisEnv:\n    \"\"\"Finite deterministic environment for retiring hypotheses.\"\"\"\n    def __init__(self, p_vec, w_vec, B, Rmax):\n        self.p = np.array(p_vec, dtype=float)\n        self.w = np.array(w_vec, dtype=float)\n        self.B = float(B)\n        self.N = len(self.p)\n        self.Rmax = int(Rmax)\n        # Precompute entropies\n        self.H = np.array([shannon_entropy_nats(pi) for pi in self.p], dtype=float)\n        # Full state bitmask: all ones for N hypotheses\n        self.full_state = (1 << self.N) - 1\n\n    def remaining_indices(self, state_mask: int):\n        return [i for i in range(self.N) if (state_mask >> i) & 1]\n\n    def count_remaining(self, state_mask: int) -> int:\n        return int(np.sum([(state_mask >> i) & 1 for i in range(self.N)]))\n\n    def retired_count(self, state_mask: int) -> int:\n        return self.N - self.count_remaining(state_mask)\n\n    def is_terminal(self, state_mask: int) -> bool:\n        return self.retired_count(state_mask) >= self.Rmax\n\n    def allowed_actions(self, state_mask: int):\n        if self.is_terminal(state_mask):\n            return []\n        return self.remaining_indices(state_mask)\n\n    def step(self, state_mask: int, action: int):\n        \"\"\"Take action (retire hypothesis 'action'), return (next_state, reward).\"\"\"\n        if ((state_mask >> action) & 1) == 0:\n            raise ValueError(\"Action must retire an active hypothesis.\")\n        next_state = state_mask & ~(1 << action)\n        rem = self.remaining_indices(next_state)\n        n_rem = len(rem)\n        if n_rem == 0:\n            # Should not occur because Rmax = N-1 per problem spec\n            reward = 0.0\n        else:\n            share = self.B / n_rem\n            reward = share * float(np.sum(self.w[rem] * self.H[rem]))\n        return next_state, reward\n\n    def baseline_reward(self, state_mask: int) -> float:\n        \"\"\"Reward with no retirement: resource equally split among current hypotheses.\"\"\"\n        rem = self.remaining_indices(state_mask)\n        n_rem = len(rem)\n        if n_rem == 0:\n            return 0.0\n        share = self.B / n_rem\n        return share * float(np.sum(self.w[rem] * self.H[rem]))\n\ndef q_learning(env: HypothesisEnv, gamma: float, episodes: int,\n               eps_init: float, eps_final: float, alpha: float, rng: np.random.Generator):\n    \"\"\"Train Q-learning on the environment.\"\"\"\n    num_states = 1 << env.N\n    Q = np.zeros((num_states, env.N), dtype=float)\n\n    def epsilon_for_episode(ep):\n        # Linear annealing from eps_init to eps_final\n        if episodes == 1:\n            return eps_final\n        return eps_init + (eps_final - eps_init) * (ep / (episodes - 1))\n\n    for ep in range(episodes):\n        state = env.full_state\n        while not env.is_terminal(state):\n            actions = env.allowed_actions(state)\n            eps = epsilon_for_episode(ep)\n            if actions:\n                if rng.random() < eps:\n                    action = int(rng.choice(actions))\n                else:\n                    # Greedy: argmax Q among allowed actions\n                    q_vals = np.array([Q[state, a] for a in actions], dtype=float)\n                    # Break ties consistently by choosing smallest index among best\n                    best_idx = int(np.argmax(q_vals))\n                    action = int(actions[best_idx])\n            else:\n                break\n            next_state, reward = env.step(state, action)\n            next_actions = env.allowed_actions(next_state)\n            if next_actions:\n                max_next_q = np.max([Q[next_state, a] for a in next_actions])\n            else:\n                max_next_q = 0.0\n            # Q-learning update\n            Q[state, action] = (1.0 - alpha) * Q[state, action] + alpha * (reward + gamma * max_next_q)\n            state = next_state\n    return Q\n\ndef evaluate_greedy(env: HypothesisEnv, Q: np.ndarray):\n    \"\"\"Evaluate greedy policy derived from Q from full state.\"\"\"\n    state = env.full_state\n    seq = []\n    total_reward = 0.0\n    if env.Rmax == 0:\n        return seq, env.baseline_reward(state)\n    steps = 0\n    while not env.is_terminal(state):\n        actions = env.allowed_actions(state)\n        if not actions:\n            break\n        # Greedy among allowed\n        q_vals = np.array([Q[state, a] for a in actions], dtype=float)\n        best_idx = int(np.argmax(q_vals))\n        action = int(actions[best_idx])\n        next_state, reward = env.step(state, action)\n        seq.append(action)\n        total_reward += reward\n        state = next_state\n        steps += 1\n        if steps > env.Rmax:\n            # Safety check; should not happen\n            break\n    return seq, total_reward\n\ndef serialize(obj):\n    \"\"\"Serialize lists and primitive numbers without spaces.\"\"\"\n    if isinstance(obj, (int, np.integer)):\n        return str(int(obj))\n    if isinstance(obj, (float, np.floating)):\n        # Use repr to keep reasonable precision\n        return repr(float(obj))\n    if isinstance(obj, list) or isinstance(obj, tuple):\n        return \"[\" + \",\".join(serialize(x) for x in obj) + \"]\"\n    raise TypeError(f\"Unsupported type for serialization: {type(obj)}\")\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1\n        {\n            \"N\": 4,\n            \"p\": [0.10, 0.70, 0.50, 0.05],\n            \"w\": [1.00, 0.90, 0.60, 0.20],\n            \"B\": 1.00,\n            \"Rmax\": 2,\n            \"gamma\": 0.95,\n            \"episodes\": 12000,\n            \"eps_init\": 0.30,\n            \"eps_final\": 0.02,\n            \"alpha\": 0.30,\n        },\n        # Case 2\n        {\n            \"N\": 5,\n            \"p\": [0.01, 0.85, 0.40, 0.95, 0.20],\n            \"w\": [0.50, 1.20, 0.80, 0.10, 0.70],\n            \"B\": 1.00,\n            \"Rmax\": 2,\n            \"gamma\": 0.90,\n            \"episodes\": 15000,\n            \"eps_init\": 0.30,\n            \"eps_final\": 0.02,\n            \"alpha\": 0.30,\n        },\n        # Case 3\n        {\n            \"N\": 4,\n            \"p\": [0.50, 0.50, 0.50, 0.50],\n            \"w\": [1.00, 1.00, 1.00, 1.00],\n            \"B\": 1.00,\n            \"Rmax\": 3,\n            \"gamma\": 0.95,\n            \"episodes\": 10000,\n            \"eps_init\": 0.30,\n            \"eps_final\": 0.02,\n            \"alpha\": 0.30,\n        },\n        # Case 4\n        {\n            \"N\": 3,\n            \"p\": [0.40, 0.60, 0.55],\n            \"w\": [0.90, 0.80, 0.70],\n            \"B\": 1.00,\n            \"Rmax\": 0,\n            \"gamma\": 0.95,\n            \"episodes\": 1,\n            \"eps_init\": 0.00,\n            \"eps_final\": 0.00,\n            \"alpha\": 0.30,\n        },\n    ]\n\n    rng = np.random.default_rng(42)\n    results = []\n    for case in test_cases:\n        env = HypothesisEnv(case[\"p\"], case[\"w\"], case[\"B\"], case[\"Rmax\"])\n        Q = q_learning(\n            env=env,\n            gamma=case[\"gamma\"],\n            episodes=case[\"episodes\"],\n            eps_init=case[\"eps_init\"],\n            eps_final=case[\"eps_final\"],\n            alpha=case[\"alpha\"],\n            rng=rng,\n        )\n        seq, total_reward = evaluate_greedy(env, Q)\n        results.append([seq, total_reward])\n\n    # Final print statement in the exact required format (no spaces).\n    print(serialize(results))\n\nsolve()\n```", "id": "3186163"}]}