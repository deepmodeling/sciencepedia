{"hands_on_practices": [{"introduction": "为了真正掌握反向传播的精髓，没有什么比从零开始构建它更有效的方法了。这个练习将指导你为一个简单的标量表达式语言实现反向模式自动微分（即反向传播背后的通用算法）。通过亲手设计计算“磁带”（tape）并实现梯度的反向传播，你将深刻理解链式法则在计算图上是如何运作的，从而揭开反向传播的神秘面纱。[@problem_id:3100018]", "problem": "你的任务是为一个小型标量表达式语言实现反向模式自动微分（AD），并演示如何通过反向回放操作磁带来计算伴随变量（表示为 $\\bar{x} = \\partial L / \\partial x$）。在计算图中，反向模式AD与反向传播算法是同义的。你的实现必须从基本原理出发，具体来说，是复合函数的链式求导法则，以及将计算图定义为一个以标量损失 $L$ 为根节点、由基本操作构成的有向无环图。你必须设计一个磁带结构，用于记录基本操作的正向执行过程，然后反向回放此磁带，以使用链式法则累积伴随变量。\n\n你所设计的小型表达式语言必须支持标量变量和常量，以及以下基本操作：二元加法 $+$、二元减法 $-$、二元乘法 $\\cdot$、二元除法 $\\div$、一元正弦 $\\sin(\\cdot)$、一元指数 $\\exp(\\cdot)$ 和一元自然对数 $\\log(\\cdot)$。三角函数中的所有角度都必须以弧度为单位。必须遵守定义域约束，例如，$\\log(\\cdot)$ 的输入必须严格为正。你必须设计计算磁带，以记录每个非叶节点操作及其操作数和正向计算值，从而确保正确的反向回放。\n\n你的程序必须：\n- 在计算标量损失 $L$ 时，构建一个内部计算图和磁带。\n- 从 $\\bar{L} = \\partial L / \\partial L = 1$ 开始，通过单次反向回放磁带，计算每个输入变量 $x_i$ 的伴随变量，即 $\\partial L / \\partial x_i$。\n- 为每个测试用例生成一个列表，其第一个元素是标量损失值 $L$，随后的元素是各变量的伴随变量，顺序与变量引入的顺序一致。\n\n仅从基本原理出发：复合函数的链式法则、中间值 $v$ 的伴随变量定义 $\\bar{v} = \\partial L / \\partial v$ 以及计算图的语义。不要依赖跳过推导路径的预封装微分公式；相反，应使用基础微积分为每个基本操作推导并实现反向回放所需的局部偏导数。\n\n实现并运行以下测试套件。在每个案例中，按指定顺序定义变量，使用基本操作构建表达式，并计算输出。所有角度均以弧度为单位，此问题不涉及任何物理单位。\n\n- 测试用例 $1$ (通用复合)：变量 $x, y$，损失 $L = \\sin(x \\cdot y) + \\exp(y)$，其中 $x = 0.5$, $y = -1.0$。此案例的输出格式：$[L, \\partial L / \\partial x, \\partial L / \\partial y]$。\n- 测试用例 $2$ (零和常数的边界情况)：变量 $x$，损失 $L = x \\cdot 0 + \\sin(0) + \\log(1)$，其中 $x = 2.0$。输出格式：$[L, \\partial L / \\partial x]$。\n- 测试用例 $3$ (变量重复使用)：变量 $x$，损失 $L = (x \\cdot x) \\cdot x$，其中 $x = 2.0$。输出格式：$[L, \\partial L / \\partial x]$。\n- 测试用例 $4$ (除法和对数)：变量 $x, y$，损失 $L = x \\div y + \\log(y)$，其中 $x = 1.0$, $y = 1.5$。输出格式：$[L, \\partial L / \\partial x, \\partial L / \\partial y]$。\n- 测试用例 $5$ (嵌套一元复合)：变量 $x$，损失 $L = \\exp(\\sin(x))$，其中 $x = 0.0$。输出格式：$[L, \\partial L / \\partial x]$。\n\n你的程序应生成单行输出，其中包含所有测试用例的结果，形式为一个用方括号括起来的逗号分隔列表，每个测试用例的结果本身也是一个用方括号括起来的逗号分隔列表。例如，两个测试用例的输出应如下所示：$[[L_1,\\partial L_1/\\partial x_1,\\dots],[L_2,\\partial L_2/\\partial x_1,\\dots]]$。你的最终输出必须严格遵循此格式，使用标准浮点数。", "solution": "该问题要求从基本原理出发，实现反向模式自动微分（AD），也就是俗称的反向传播。该方法通过首先对损失函数 $L$ 的表达式进行正向求值以计算中间值并记录计算图，然后反向遍历该图以基于链式法则传播梯度，从而计算标量损失函数 $L$ 相对于一组输入变量 $x_i$ 的梯度。\n\n**基本原理：链式法则和伴随变量**\n\n反向模式AD的基础是微积分中的链式法则。如果一个标量损失 $L$ 是中间变量 $v_j$ 的函数，而 $v_j$ 本身又是其他变量 $v_i$ 的函数，那么 $L$ 相对于 $v_i$ 的梯度由从 $v_i$ 到 $L$ 的所有路径的贡献之和给出。对于单条路径 $L \\to v_j \\to v_i$，链式法则表述为：\n$$\n\\frac{\\partial L}{\\partial v_i} = \\frac{\\partial L}{\\partial v_j} \\frac{\\partial v_j}{\\partial v_i}\n$$\n在AD的术语中，我们将变量 $v$ 的“伴随变量”定义为 $\\bar{v} \\equiv \\frac{\\partial L}{\\partial v}$。使用此表示法，链式法则变为：\n$$\n\\bar{v}_i = \\bar{v}_j \\frac{\\partial v_j}{\\partial v_i}\n$$\n反向模式AD算法利用了这种关系，首先计算 $L$ 的值，然后将伴随变量从 $L$ 反向传播到输入变量。该过程从为损失函数自身的伴随变量设定种子值开始，即 $\\bar{L} = \\frac{\\partial L}{\\partial L} = 1$。\n\n**计算图和磁带**\n\n任何标量表达式都可以分解为一系列基本操作（例如，加法、乘法、正弦函数）。这种分解自然形成一个有向无环图（DAG），其中节点代表数值（输入变量、常量和中间结果），边代表基本操作。\n\n从输入到最终损失 $L$ 的表达式正向求值过程被用来构建这个图。在我们的实现中，我们使用一个“磁带”数据结构，它是图的线性化表示。磁带是在正向传播过程中记录的操作的有序列表。磁带上的每个条目存储操作类型、对其输入节点的引用以及对其输出节点的引用。这种记录方式确保我们拥有完整的结构和反向传播所需的所有必要中间值。\n\n**正向传播：求值与磁带记录**\n\n正向传播按以下步骤进行：\n$1$. 输入变量和常量被初始化为我们图中的起始节点。\n$2$. 表达式被顺序求值。每次应用一个基本操作时，会发生两件事：\n    a. 计算操作的数值结果，并作为图中的一个新节点存储。\n    b. 向磁带添加一个条目，记录操作类型、其输入节点以及新创建的输出节点。\n\n例如，对于表达式 $z = x \\cdot y$，我们将使用 $x$ 和 $y$ 的当前值计算 $z$ 的值，为 $z$ 创建一个新节点，并在磁带上记录 `('mul', [x_node, y_node], z_node)`。\n\n**反向传播：伴随变量累积**\n\n一旦正向传播完成并计算出最终损失值 $L$，反向传播就开始了。它以磁带创建的相反顺序遍历磁带。\n$1$. 初始化一个与图中每个节点相对应的伴随变量数组，其所有元素均为零。\n$2$. 最终损失节点的伴随变量被设置为 $1$，即 $\\bar{L} = 1$。\n$3$. 对于磁带上的每个操作 $z = f(x_1, \\dots, x_n)$（按相反顺序处理）：\n    a. 我们检索已经计算出的输出的伴随变量 $\\bar{z}$。\n    b. 我们使用链式法则来计算 $\\bar{z}$ 对输入伴随变量的贡献。每个输入 $x_i$ 的伴随变量通过累积这个贡献来更新：\n    $$\n    \\bar{x}_i \\mathrel{+}= \\bar{z} \\cdot \\frac{\\partial z}{\\partial x_i}\n    $$\n    使用累积（$\\mathrel{+}=$）至关重要，因为单个变量可能在多个操作中使用（即，它在图中可以是多个子节点的父节点）。其总伴随变量是从其所有子节点反向流回的梯度信号的总和。反向回放磁带保证了一个节点的伴随变量（$\\bar{z}$）在传播到其自身输入（$x_i$）之前已被完全计算。\n\n**基本操作的伴随变量更新规则**\n\n每个基本操作的局部偏导数 $\\frac{\\partial z}{\\partial x_i}$ 都是已知的。计算这些导数所需的输入值（例如，对于 $z = x \\cdot y$，$\\frac{\\partial z}{\\partial x} = y$）可以从正向传播中获得。\n\n- **加法:** $z = x + y$\n  - $\\frac{\\partial z}{\\partial x} = 1$, $\\frac{\\partial z}{\\partial y} = 1$。\n  - 更新规则: $\\bar{x} \\mathrel{+}= \\bar{z}$, $\\bar{y} \\mathrel{+}= \\bar{z}$。\n\n- **减法:** $z = x - y$\n  - $\\frac{\\partial z}{\\partial x} = 1$, $\\frac{\\partial z}{\\partial y} = -1$。\n  - 更新规则: $\\bar{x} \\mathrel{+}= \\bar{z}$, $\\bar{y} \\mathrel{+}= -\\bar{z}$。\n\n- **乘法:** $z = x \\cdot y$\n  - $\\frac{\\partial z}{\\partial x} = y$, $\\frac{\\partial z}{\\partial y} = x$。\n  - 更新规则: $\\bar{x} \\mathrel{+}= \\bar{z} \\cdot y$, $\\bar{y} \\mathrel{+}= \\bar{z} \\cdot x$。\n\n- **除法:** $z = x \\div y$\n  - $\\frac{\\partial z}{\\partial x} = \\frac{1}{y}$, $\\frac{\\partial z}{\\partial y} = -\\frac{x}{y^2}$。\n  - 更新规则: $\\bar{x} \\mathrel{+}= \\bar{z} \\cdot \\frac{1}{y}$, $\\bar{y} \\mathrel{+}= \\bar{z} \\cdot \\left(-\\frac{x}{y^2}\\right)$。\n\n- **正弦:** $z = \\sin(x)$\n  - $\\frac{dz}{dx} = \\cos(x)$。\n  - 更新规则: $\\bar{x} \\mathrel{+}= \\bar{z} \\cdot \\cos(x)$。\n\n- **指数:** $z = \\exp(x)$\n  - $\\frac{dz}{dx} = \\exp(x) = z$。\n  - 更新规则: $\\bar{x} \\mathrel{+}= \\bar{z} \\cdot z$。\n\n- **自然对数:** $z = \\log(x)$\n  - $\\frac{dz}{dx} = \\frac{1}{x}$。\n  - 更新规则: $\\bar{x} \\mathrel{+}= \\bar{z} \\cdot \\frac{1}{x}$。\n\n**实现设计**\n\n该实现使用两个主要类：`Graph` 和 `Node`。`Graph` 类管理计算的状态：它存储所有节点的 `values`（值）、操作 `tape`（磁带）和计算出的 `adjoints`（伴随变量）。`Node` 类作为节点ID的封装，通过重载Python的算术运算符（`+`、`*` 等）提供直观的接口。当对 `Node` 对象执行像 `c = a + b` 这样的操作时，它会透明地调用关联 `Graph` 对象上的一个方法，该方法执行正向计算，将操作记录在磁带上，并为结果 `c` 返回一个新的 `Node`。这种面向对象的设计允许以自然的方式构建表达式，同时在后台正确地构建计算图。在计算出最终的损失 `Node` 后，调用 `Graph.compute_gradients()` 会执行如上所述的反向传播过程。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\n# A consistent execution environment requires no user input and all dependencies declared.\n\n# Define unary functions that can operate on Node objects or raw numbers\ndef sin(node):\n    \"\"\"Computes sine, handling both Node objects and numeric types.\"\"\"\n    if isinstance(node, Node):\n        return node.graph.sin(node)\n    return np.sin(node)\n\ndef exp(node):\n    \"\"\"Computes exponential, handling both Node objects and numeric types.\"\"\"\n    if isinstance(node, Node):\n        return node.graph.exp(node)\n    return np.exp(node)\n\ndef log(node):\n    \"\"\"Computes natural logarithm, handling both Node objects and numeric types.\"\"\"\n    if isinstance(node, Node):\n        return node.graph.log(node)\n    return np.log(node)\n\nclass Graph:\n    \"\"\"Manages the computational graph, tape, and differentiation process.\"\"\"\n    def __init__(self):\n        # A linearized list of operations representing the computational graph.\n        # Each entry is a tuple: (op_type, [input_node_ids], output_node_id)\n        self.tape = []\n        # Stores the numerical value of each node computed during the forward pass.\n        self.values = []\n        # Stores the adjoint (dL/dv) for each node, computed during the reverse pass.\n        self.adjoints = None\n\n    def _add_node(self, value):\n        \"\"\"Adds a new node (value) to the graph and returns its ID.\"\"\"\n        node_id = len(self.values)\n        self.values.append(value)\n        return node_id\n\n    def variable(self, value):\n        \"\"\"Creates a variable node, which is a leaf in the graph.\"\"\"\n        node_id = self._add_node(value)\n        return Node(self, node_id)\n\n    def _promote_to_node(self, other):\n        \"\"\"Promotes a numeric constant to a Node to allow operations like `x + 5`.\"\"\"\n        if not isinstance(other, Node):\n            # Treat numeric constants as new variable nodes in the graph.\n            return self.variable(other)\n        return other\n\n    # Methods for primitive operations (Forward Pass)\n    def add(self, n1, n2):\n        res_val = self.values[n1.node_id] + self.values[n2.node_id]\n        res_id = self._add_node(res_val)\n        self.tape.append(('add', [n1.node_id, n2.node_id], res_id))\n        return Node(self, res_id)\n\n    def sub(self, n1, n2):\n        res_val = self.values[n1.node_id] - self.values[n2.node_id]\n        res_id = self._add_node(res_val)\n        self.tape.append(('sub', [n1.node_id, n2.node_id], res_id))\n        return Node(self, res_id)\n\n    def mul(self, n1, n2):\n        res_val = self.values[n1.node_id] * self.values[n2.node_id]\n        res_id = self._add_node(res_val)\n        self.tape.append(('mul', [n1.node_id, n2.node_id], res_id))\n        return Node(self, res_id)\n\n    def truediv(self, n1, n2):\n        res_val = self.values[n1.node_id] / self.values[n2.node_id]\n        res_id = self._add_node(res_val)\n        self.tape.append(('div', [n1.node_id, n2.node_id], res_id))\n        return Node(self, res_id)\n\n    def sin(self, n1):\n        res_val = np.sin(self.values[n1.node_id])\n        res_id = self._add_node(res_val)\n        self.tape.append(('sin', [n1.node_id], res_id))\n        return Node(self, res_id)\n\n    def exp(self, n1):\n        res_val = np.exp(self.values[n1.node_id])\n        res_id = self._add_node(res_val)\n        self.tape.append(('exp', [n1.node_id], res_id))\n        return Node(self, res_id)\n\n    def log(self, n1):\n        value = self.values[n1.node_id]\n        if value = 0:\n            raise ValueError(\"Domain error: input to log must be positive.\")\n        res_val = np.log(value)\n        res_id = self._add_node(res_val)\n        self.tape.append(('log', [n1.node_id], res_id))\n        return Node(self, res_id)\n\n    def compute_gradients(self, loss_node):\n        \"\"\"Performs the reverse pass to compute gradients for all nodes.\"\"\"\n        num_nodes = len(self.values)\n        self.adjoints = np.zeros(num_nodes)\n        self.adjoints[loss_node.node_id] = 1.0  # Seed the reverse pass\n\n        # Replay the tape in reverse to propagate adjoints\n        for op_type, input_ids, output_id in reversed(self.tape):\n            adjoint_out = self.adjoints[output_id]\n            \n            if adjoint_out == 0.0:  # Optimization: no gradient to propagate\n                continue\n\n            # Apply the chain rule based on the operation\n            if op_type == 'add':\n                self.adjoints[input_ids[0]] += adjoint_out\n                self.adjoints[input_ids[1]] += adjoint_out\n            elif op_type == 'sub':\n                self.adjoints[input_ids[0]] += adjoint_out\n                self.adjoints[input_ids[1]] -= adjoint_out\n            elif op_type == 'mul':\n                val0 = self.values[input_ids[0]]\n                val1 = self.values[input_ids[1]]\n                self.adjoints[input_ids[0]] += adjoint_out * val1\n                self.adjoints[input_ids[1]] += adjoint_out * val0\n            elif op_type == 'div':\n                val0 = self.values[input_ids[0]]\n                val1 = self.values[input_ids[1]]\n                self.adjoints[input_ids[0]] += adjoint_out / val1\n                self.adjoints[input_ids[1]] -= adjoint_out * val0 / (val1**2)\n            elif op_type == 'sin':\n                val0 = self.values[input_ids[0]]\n                self.adjoints[input_ids[0]] += adjoint_out * np.cos(val0)\n            elif op_type == 'exp':\n                out_val = self.values[output_id] # d/dx(exp(x)) = exp(x)\n                self.adjoints[input_ids[0]] += adjoint_out * out_val\n            elif op_type == 'log':\n                val0 = self.values[input_ids[0]]\n                self.adjoints[input_ids[0]] += adjoint_out / val0\n\nclass Node:\n    \"\"\"A node in the computational graph, with overloaded operators.\"\"\"\n    def __init__(self, graph, node_id):\n        self.graph = graph\n        self.node_id = node_id\n\n    @property\n    def value(self):\n        \"\"\"Get the node's numerical value from its graph.\"\"\"\n        return self.graph.values[self.node_id]\n        \n    @property\n    def adjoint(self):\n        \"\"\"Get the node's adjoint after the reverse pass.\"\"\"\n        if self.graph.adjoints is None:\n            raise RuntimeError(\"Gradients not computed yet. Call `compute_gradients` on the loss node first.\")\n        return self.graph.adjoints[self.node_id]\n    \n    # Left-side binary operators\n    def __add__(self, other):\n        other = self.graph._promote_to_node(other)\n        return self.graph.add(self, other)\n\n    def __sub__(self, other):\n        other = self.graph._promote_to_node(other)\n        return self.graph.sub(self, other)\n\n    def __mul__(self, other):\n        other = self.graph._promote_to_node(other)\n        return self.graph.mul(self, other)\n\n    def __truediv__(self, other):\n        other = self.graph._promote_to_node(other)\n        return self.graph.truediv(self, other)\n\n    # Right-side binary operators (for expressions like `5 + x`)\n    def __radd__(self, other):\n        other = self.graph._promote_to_node(other)\n        return self.graph.add(other, self)\n\n    def __rsub__(self, other):\n        other = self.graph._promote_to_node(other)\n        return self.graph.sub(other, self)\n\n    def __rmul__(self, other):\n        other = self.graph._promote_to_node(other)\n        return self.graph.mul(other, self)\n\n    def __rtruediv__(self, other):\n        other = self.graph._promote_to_node(other)\n        return self.graph.truediv(other, self)\n\n    def __repr__(self):\n        return f\"Node(id={self.node_id}, value={self.value:.4f})\"\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1: L = sin(x*y) + exp(y), x=0.5, y=-1.0\n        {'vars': {'x': 0.5, 'y': -1.0}, 'expr': lambda x, y: sin(x * y) + exp(y)},\n        # Case 2: L = x*0 + sin(0) + log(1), x=2.0\n        {'vars': {'x': 2.0}, 'expr': lambda x: x * 0.0 + sin(0.0) + log(1.0)},\n        # Case 3: L = (x*x)*x, x=2.0\n        {'vars': {'x': 2.0}, 'expr': lambda x: (x * x) * x},\n        # Case 4: L = x/y + log(y), x=1.0, y=1.5\n        {'vars': {'x': 1.0, 'y': 1.5}, 'expr': lambda x, y: x / y + log(y)},\n        # Case 5: L = exp(sin(x)), x=0.0\n        {'vars': {'x': 0.0}, 'expr': lambda x: exp(sin(x))},\n    ]\n\n    all_results = []\n    for case in test_cases:\n        g = Graph()\n        # Create variable nodes in the specified order (determined by dict insertion order in Python 3.7+)\n        var_nodes = {name: g.variable(val) for name, val in case['vars'].items()}\n        \n        # Build the graph by executing the expression\n        loss_node = case['expr'](**var_nodes)\n        \n        # Compute gradients via reverse-mode AD\n        g.compute_gradients(loss_node)\n        \n        # Collect results: [L, dL/dx1, dL/dx2, ...]\n        case_result = [loss_node.value]\n        for node in var_nodes.values():\n            case_result.append(node.adjoint)\n        \n        all_results.append(f\"[{','.join(map(str, case_result))}]\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(all_results)}]\")\n\nsolve()\n\n```", "id": "3100018"}, {"introduction": "在理解了反向传播的通用机制之后，将其应用于一个具体的神经网络案例是加深理解的关键一步。本练习采用最简单的单个线性神经元模型，要求你手动计算损失函数的梯度和Hessian矩阵。这种手算练习不仅能巩固理论知识，还能将抽象的梯度概念与损失曲面的几何形态联系起来，帮助你直观地理解优化算法在“寻找”什么。[@problem_id:3099996]", "problem": "给定一个具有线性激活的单神经元模型，由参数函数 $f(x; \\theta) = W x + b$ 定义，其中 $\\theta = (W, b)$，$W \\in \\mathbb{R}$ 且 $b \\in \\mathbb{R}$。训练集包含三个输入-输出对 $(x_i, y_i)$（$i = 1, 2, 3$），具体为 $(x_1, y_1) = (0, 1)$，$(x_2, y_2) = (1, 3)$ 和 $(x_3, y_3) = (2, 5)$。经验风险是平方误差半和，定义为\n$$\nJ(\\theta) = \\frac{1}{2} \\sum_{i=1}^{3} \\left(f(x_i; \\theta) - y_i\\right)^{2}.\n$$\n从微积分链式法则的基本定义以及梯度和海森矩阵（二阶偏导数矩阵）的定义出发，完成以下操作：\n1. 选择参数 $W$ 和 $b$ 以精确拟合这三个数据点，即对于每个 $i \\in \\{1, 2, 3\\}$，都有 $f(x_i; \\theta) = y_i$。\n2. 使用反向传播（即应用于模型计算图的链式法则），推导梯度 $\\nabla_{\\theta} J(\\theta)$，并在第1部分中选择的精确拟合参数处求值。\n3. 推导 $J(\\theta)$ 关于 $\\theta$ 的海森矩阵 $H(\\theta)$，并在精确拟合参数处求值。计算最小特征值 $\\lambda_{\\min}(H)$。\n4. 根据 $\\lambda_{\\min}(H)$ 的符号，简要说明该精确拟合点是 $J(\\theta)$ 的局部最小值还是鞍点。\n\n请给出解处 $\\lambda_{\\min}(H)$ 的精确值作为最终答案。无需四舍五入。", "solution": "任务是分析单个线性神经元模型 $f(x; \\theta) = W x + b$（参数为 $\\theta = (W, b)$）的经验风险函数 $J(\\theta)$。风险定义为三个数据点上的平方误差半和：$(x_1, y_1) = (0, 1)$，$(x_2, y_2) = (1, 3)$ 和 $(x_3, y_3) = (2, 5)$。风险函数为：\n$$\nJ(W, b) = \\frac{1}{2} \\sum_{i=1}^{3} \\left( (Wx_i + b) - y_i \\right)^{2}\n$$\n代入给定的数据点：\n$$\nJ(W, b) = \\frac{1}{2} \\left[ (W(0) + b - 1)^{2} + (W(1) + b - 3)^{2} + (W(2) + b - 5)^{2} \\right]\n$$\n$$\nJ(W, b) = \\frac{1}{2} \\left[ (b - 1)^{2} + (W + b - 3)^{2} + (2W + b - 5)^{2} \\right]\n$$\n\n**1. 找到精确拟合参数 $\\theta^* = (W, b)$**\n\n为了实现精确拟合，模型必须对所有 $i \\in \\{1, 2, 3\\}$ 满足 $f(x_i; \\theta) = y_i$。这为 $W$ 和 $b$ 导出一个线性方程组：\n\\begin{enumerate}\n    \\item 对于 $(x_1, y_1) = (0, 1)$：$W(0) + b = 1 \\implies b = 1$。\n    \\item 对于 $(x_2, y_2) = (1, 3)$：$W(1) + b = 3 \\implies W + b = 3$。\n    \\item 对于 $(x_3, y_3) = (2, 5)$：$W(2) + b = 5 \\implies 2W + b = 5$。\n\\end{enumerate}\n将第一个方程中的 $b = 1$ 代入第二个方程，得到 $W + 1 = 3$，这意味着 $W = 2$。\n我们必须验证这些值是否满足第三个方程：$2W + b = 2(2) + 1 = 4 + 1 = 5$，这与 $y_3 = 5$ 一致。\n因此，精确拟合的参数是 $W = 2$ 和 $b = 1$。我们将此点表示为 $\\theta^* = (2, 1)$。\n\n**2. 在 $\\theta^*$ 处推导并计算梯度 $\\nabla_{\\theta} J(\\theta)$**\n\n$J(\\theta)$ 关于 $\\theta = (W, b)$ 的梯度是 $\\nabla_{\\theta} J = \\begin{pmatrix} \\frac{\\partial J}{\\partial W} \\\\ \\frac{\\partial J}{\\partial b} \\end{pmatrix}$。\n根据反向传播方法的要求，使用链式法则，我们将每个点的误差定义为 $e_i(\\theta) = f(x_i; \\theta) - y_i = Wx_i + b - y_i$。损失函数是 $J = \\frac{1}{2} \\sum_{i=1}^3 e_i^2$。\n偏导数是：\n$$\n\\frac{\\partial J}{\\partial W} = \\sum_{i=1}^{3} \\frac{\\partial J}{\\partial e_i} \\frac{\\partial e_i}{\\partial W} = \\sum_{i=1}^{3} e_i \\cdot x_i = \\sum_{i=1}^{3} (Wx_i + b - y_i) x_i\n$$\n$$\n\\frac{\\partial J}{\\partial b} = \\sum_{i=1}^{3} \\frac{\\partial J}{\\partial e_i} \\frac{\\partial e_i}{\\partial b} = \\sum_{i=1}^{3} e_i \\cdot 1 = \\sum_{i=1}^{3} (Wx_i + b - y_i)\n$$\n在精确拟合点 $\\theta^* = (2, 1)$ 处，根据定义，误差项为零：对于所有 $i$，$e_i(\\theta^*) = Wx_i + b - y_i = 0$。\n因此，在 $\\theta^*$ 处计算梯度：\n$$\n\\frac{\\partial J}{\\partial W}\\bigg|_{\\theta^*} = \\sum_{i=1}^{3} (0) x_i = 0\n$$\n$$\n\\frac{\\partial J}{\\partial b}\\bigg|_{\\theta^*} = \\sum_{i=1}^{3} (0) = 0\n$$\n在精确拟合点处的梯度是零向量：$\\nabla_{\\theta} J(\\theta^*) = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$。这证实了 $\\theta^*$ 是损失函数 $J(\\theta)$ 的一个临界点。\n\n**3. 推导海森矩阵 $H(\\theta)$ 并计算其最小特征值**\n\n海森矩阵 $H(\\theta)$ 包含 $J(\\theta)$ 的二阶偏导数：\n$$\nH(\\theta) = \\begin{pmatrix} \\frac{\\partial^2 J}{\\partial W^2}  \\frac{\\partial^2 J}{\\partial W \\partial b} \\\\ \\frac{\\partial^2 J}{\\partial b \\partial W}  \\frac{\\partial^2 J}{\\partial b^2} \\end{pmatrix}\n$$\n我们通过对一阶偏导数求导来计算这些值：\n$$\n\\frac{\\partial^2 J}{\\partial W^2} = \\frac{\\partial}{\\partial W} \\left[ \\sum_{i=1}^{3} (Wx_i + b - y_i) x_i \\right] = \\sum_{i=1}^{3} x_i^2\n$$\n$$\n\\frac{\\partial^2 J}{\\partial b \\partial W} = \\frac{\\partial}{\\partial b} \\left[ \\sum_{i=1}^{3} (Wx_i + b - y_i) x_i \\right] = \\sum_{i=1}^{3} x_i\n$$\n$$\n\\frac{\\partial^2 J}{\\partial b^2} = \\frac{\\partial}{\\partial b} \\left[ \\sum_{i=1}^{3} (Wx_i + b - y_i) \\right] = \\sum_{i=1}^{3} 1 = 3\n$$\n注意，正如预期的那样，$\\frac{\\partial^2 J}{\\partial W \\partial b} = \\frac{\\partial^2 J}{\\partial b \\partial W}$。海森矩阵是常数，不依赖于 $W$ 或 $b$。我们使用给定的输入 $x_1=0$, $x_2=1$, $x_3=2$ 来计算这些和：\n$$\n\\sum_{i=1}^{3} x_i^2 = 0^2 + 1^2 + 2^2 = 0 + 1 + 4 = 5\n$$\n$$\n\\sum_{i=1}^{3} x_i = 0 + 1 + 2 = 3\n$$\n海森矩阵是：\n$$\nH = \\begin{pmatrix} 5  3 \\\\ 3  3 \\end{pmatrix}\n$$\n$H$ 的特征值 $\\lambda$ 是特征方程 $\\det(H - \\lambda I) = 0$ 的根：\n$$\n\\det \\begin{pmatrix} 5-\\lambda  3 \\\\ 3  3-\\lambda \\end{pmatrix} = (5-\\lambda)(3-\\lambda) - (3)(3) = 0\n$$\n$$\n15 - 8\\lambda + \\lambda^2 - 9 = 0\n$$\n$$\n\\lambda^2 - 8\\lambda + 6 = 0\n$$\n使用二次方程求根公式 $\\lambda = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}$：\n$$\n\\lambda = \\frac{8 \\pm \\sqrt{(-8)^2 - 4(1)(6)}}{2} = \\frac{8 \\pm \\sqrt{64 - 24}}{2} = \\frac{8 \\pm \\sqrt{40}}{2}\n$$\n化简 $\\sqrt{40} = \\sqrt{4 \\cdot 10} = 2\\sqrt{10}$：\n$$\n\\lambda = \\frac{8 \\pm 2\\sqrt{10}}{2} = 4 \\pm \\sqrt{10}\n$$\n两个特征值是 $\\lambda_1 = 4 + \\sqrt{10}$ 和 $\\lambda_2 = 4 - \\sqrt{10}$。最小特征值是 $\\lambda_{\\min}(H) = 4 - \\sqrt{10}$。\n\n**4. 对临界点 $\\theta^*$ 进行分类**\n\n为了对临界点 $\\theta^*$ 进行分类，我们检查在该点计算的海森矩阵的特征值的符号。由于 $H$ 是常数，我们使用刚刚计算出的特征值。\n我们知道 $3 = \\sqrt{9}  \\sqrt{10}  \\sqrt{16} = 4$。\n因此，最小特征值 $\\lambda_{\\min}(H) = 4 - \\sqrt{10}$ 是正的，因为 $4  \\sqrt{10}$。\n最大特征值 $\\lambda_{\\max}(H) = 4 + \\sqrt{10}$ 显然也是正的。\n由于海森矩阵的两个特征值都是正的，所以该海森矩阵是正定的。根据二阶偏导数检验，海森矩阵为正定的临界点是局部最小值。对于这个二次损失函数，它是唯一的全局最小值。该精确拟合点是一个局部最小值。\n最终答案是最小特征值的值。", "answer": "$$\n\\boxed{4 - \\sqrt{10}}\n$$", "id": "3099996"}]}