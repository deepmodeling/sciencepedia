{"hands_on_practices": [{"introduction": "科学模型，特别是那些基于偏微分方程 (PDE) 的模型，通常包含大量输入参数。理解哪些参数对模型输出影响最大，是一项被称为敏感性分析的关键任务。主动子空间方法运用机器学习原理，在高维参数空间中寻找一个能够捕捉模型输出大部分变化的低维子空间。本练习 ([@problem_id:3157324]) 将指导你从零开始实现这一强大的降维技术，通过计算梯度和分析敏感度矩阵的光谱，最终识别出最具影响力的参数组合。", "problem": "考虑一个在计算科学中用于近似偏微分方程（PDE）产生的感兴趣量（QOI）的参数到输出映射 $f:\\mathbb{R}^d \\to \\mathbb{R}$。在活性子空间方法中，我们寻求参数空间中的一些方向，在这些方向上，$f$ 相对于输入分布的平均变化最大。从一阶泰勒展开出发，局部变化由梯度 $\\nabla_\\theta f(\\theta)$ 控制。沿单位向量 $u$ 的全局平均方向敏感度由二次型 $u^\\top C u$ 量化，其中平均梯度外积矩阵定义为\n$$\nC \\equiv \\mathbb{E}\\left[\\nabla_\\theta f(\\theta)\\,\\nabla_\\theta f(\\theta)^\\top\\right],\n$$\n其中期望是关于 $\\theta$ 的分布计算的。活性子空间由 $C$ 的主特征向量构成；较大的特征值表示具有较高均方方向导数的方向。\n\n您的任务是实现一个完整的程序，对于给定的标量函数 $f$、维度 $d$、样本量 $N$、阈值 $\\tau$ 和随机种子 $s$，该程序将：\n(1) 对 $i=1,\\dots,N$，独立地从 $\\mathcal{N}(0, I_d)$ 中采样输入参数 $\\theta_i$，\n(2) 仅使用多元微积分的基本法则（特别是链式法则和线性性），计算精确的解析梯度 $\\nabla_\\theta f(\\theta_i)$，而不使用任何有限差分近似，\n(3) 通过蒙特卡洛平均估计矩阵 $C$\n$$\n\\widehat{C} = \\frac{1}{N}\\sum_{i=1}^N \\left(\\nabla_\\theta f(\\theta_i)\\right)\\left(\\nabla_\\theta f(\\theta_i)\\right)^\\top,\n$$\n(4) 计算 $\\widehat{C}$ 的特征值，将其按降序排序，并确定满足以下条件的最小非负整数 $m \\in \\{0,1,\\dots,d\\}$\n$$\n\\frac{\\sum_{j=1}^m \\lambda_j}{\\sum_{j=1}^d \\lambda_j} \\ge \\tau,\n$$\n其中 $\\lambda_1 \\ge \\lambda_2 \\ge \\dots \\ge \\lambda_d \\ge 0$ 是 $\\widehat{C}$ 的特征值。如果分母为零，则按惯例将 $m$ 设为 $0$。返回每个测试用例的结果 $m$。\n\n您必须将您的推导和设计建立在梯度的基本定义及其在一阶敏感度中的作用之上。程序不得使用任何外部数据或用户输入。\n\n使用以下三个科学上合理的函数 $f$，它们能分离出不同的变化机制，并且适合进行解析梯度计算：\n(1) 由一个线性泛函与一个三次非线性函数复合生成的秩-1 函数：\n$$\nf_{\\text{rank1}}(\\theta) = \\left(w^\\top \\theta\\right)^3,\n$$\n其中 $w\\in\\mathbb{R}^d$ 是一个从标准正态分布中抽取然后归一化为单位长度的单位向量。\n(2) 一个各向同性的二次函数：\n$$\nf_{\\text{iso}}(\\theta) = \\frac{1}{2}\\left\\|\\theta\\right\\|_2^2,\n$$\n在标准正态输入下，该函数在所有方向上产生相同的敏感度。\n(3) 由两个正交的三次岭函数之和构成的秩-2 函数：\n$$\nf_{\\text{rank2}}(\\theta) = \\left(v_1^\\top \\theta\\right)^3 + \\left(v_2^\\top \\theta\\right)^3,\n$$\n其中 $v_1, v_2 \\in \\mathbb{R}^d$ 是正交单位向量，通过抽取两个独立的标准正态随机向量然后将它们正交化（例如，通过 Gram–Schmidt 方法）来构建。\n\n所有随机向量（$w$、$v_1$、$v_2$）必须使用标准的伪随机数生成器，从指定的种子 $s$ 确定性地生成，然后归一化为单位长度（对于向量对则进行正交归一化）。为了可复现性，输入参数必须使用相同的种子 $s$ 从 $\\theta_i \\sim \\mathcal{N}(0, I_d)$ 中采样。\n\n设计一个测试套件，覆盖正常路径情况、各向同性边界行为和多方向活性：\n测试用例 1：函数 $f_{\\text{rank1}}$，$d = 7$，$N = 4096$，$\\tau = 0.9$，种子 $s = 11$。\n测试用例 2：函数 $f_{\\text{iso}}$，$d = 5$，$N = 2048$，$\\tau = 0.68$，种子 $s = 13$。\n测试用例 3：函数 $f_{\\text{rank2}}$，$d = 10$，$N = 4096$，$\\tau = 0.9$，种子 $s = 17$。\n测试用例 4：函数 $f_{\\text{iso}}$，$d = 4$，$N = 1024$，$\\tau = 0.99$，种子 $s = 19$。\n\n所有量都是无量纲的；不涉及物理单位或角度。您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表（例如，$[m_1,m_2,m_3,m_4]$），其中每个 $m_k$ 是根据上述阈值定义计算的第 $k$ 个测试用例的整数活性子空间维度。", "solution": "所提出的问题是有效的。它在科学上植根于敏感性分析和不确定性量化的原理，特别是活性子空间方法。该问题是适定的，所有必要的函数、参数和过程都已明确定义，以确保一个唯一的、确定性的解。它是客观的，不含任何非科学或模糊的语言。因此，我们可以着手给出一个正式的解。\n\n核心目标是确定给定函数 $f(\\theta)$ 的活性子空间的维度 $m$。这个维度代表参数空间 $\\mathbb{R}^d$ 中主导方向的数量，函数输出沿着这些方向平均表现出最显著的变化。该方法基于矩阵 $C = \\mathbb{E}\\left[\\nabla_\\theta f(\\theta)\\,\\nabla_\\theta f(\\theta)^\\top\\right]$ 的谱分析，该矩阵聚合了整个输入参数分布上的局部、梯度驱动的敏感度信息。我们将使用蒙特卡洛方法估计 $C$，然后通过分析其特征值的累积能量来找到 $m$。\n\n解决方案的结构如下：首先，我们为指定的函数推导解析梯度。其次，我们形式化确定性地生成所需随机向量和输入样本的过程。第三，我们详细说明用于估计矩阵 $C$ 和计算其特征值的算法。最后，我们描述基于指定的能量阈值 $\\tau$ 计算活性子空间维度 $m$ 的方法。\n\n**1. 解析梯度推导**\n\n问题要求使用精确的解析梯度。我们利用多元微积分的基本法则为三个函数族中的每一个推导这些梯度。设 $\\theta = [\\theta_1, \\dots, \\theta_d]^\\top \\in \\mathbb{R}^d$。\n\n*   **函数 1：$f_{\\text{rank1}}(\\theta) = (w^\\top \\theta)^3$**\n    该函数是线性泛函 $u(\\theta) = w^\\top \\theta = \\sum_{k=1}^d w_k \\theta_k$ 和三次函数 $g(u) = u^3$ 的复合。应用链式法则，梯度为 $\\nabla_\\theta f = \\frac{dg}{du} \\nabla_\\theta u$。\n    $g(u)$ 的导数是 $\\frac{dg}{du} = 3u^2 = 3(w^\\top \\theta)^2$。\n    $u(\\theta)$ 关于 $\\theta$ 的梯度是 $\\nabla_\\theta (w^\\top \\theta) = w$。\n    因此，$f_{\\text{rank1}}$ 的梯度是：\n    $$\n    \\nabla_\\theta f_{\\text{rank1}}(\\theta) = 3(w^\\top \\theta)^2 w\n    $$\n\n*   **函数 2：$f_{\\text{iso}}(\\theta) = \\frac{1}{2}\\left\\|\\theta\\right\\|_2^2$**\n    这个各向同性的二次函数可以写成 $f_{\\text{iso}}(\\theta) = \\frac{1}{2} \\sum_{k=1}^d \\theta_k^2$。关于任意分量 $\\theta_j$ 的偏导数是：\n    $$\n    \\frac{\\partial f_{\\text{iso}}}{\\partial \\theta_j} = \\frac{\\partial}{\\partial \\theta_j} \\left( \\frac{1}{2} \\sum_{k=1}^d \\theta_k^2 \\right) = \\frac{1}{2} (2\\theta_j) = \\theta_j\n    $$\n    由于这对所有 $j=1, \\dots, d$ 都成立，梯度向量就是输入向量本身：\n    $$\n    \\nabla_\\theta f_{\\text{iso}}(\\theta) = \\theta\n    $$\n\n*   **函数 3：$f_{\\text{rank2}}(\\theta) = (v_1^\\top \\theta)^3 + (v_2^\\top \\theta)^3$**\n    梯度算子是线性的。因此，我们可以分别计算每一项的梯度，然后将结果相加。每一项的形式都与 $f_{\\text{rank1}}$ 相同。应用我们之前的结果，我们得到：\n    $$\n    \\nabla_\\theta \\left( (v_1^\\top \\theta)^3 \\right) = 3(v_1^\\top \\theta)^2 v_1\n    $$\n    $$\n    \\nabla_\\theta \\left( (v_2^\\top \\theta)^3 \\right) = 3(v_2^\\top \\theta)^2 v_2\n    $$\n    $f_{\\text{rank2}}$ 的梯度是这两个向量的和：\n    $$\n    \\nabla_\\theta f_{\\text{rank2}}(\\theta) = 3(v_1^\\top \\theta)^2 v_1 + 3(v_2^\\top \\theta)^2 v_2\n    $$\n\n**2. 随机向量和样本的确定性生成**\n\n为确保可复现性，所有随机数必须从给定的种子 $s$ 确定性地生成。使用 $s$ 初始化一个伪随机数生成器（PRNG）。所有需要的随机向量都使用这同一个 PRNG 实例生成。\n\n*   **参数向量（$w, v_1, v_2$）：**\n    对于 $f_{\\text{rank1}}$，从标准正态分布 $\\mathcal{N}(0, I_d)$ 中抽取一个向量，然后将其欧几里得范数归一化为 $1$，以产生单位向量 $w$。\n    对于 $f_{\\text{rank2}}$，从 $\\mathcal{N}(0, I_d)$ 中抽取两个独立的向量，比如说 $u_1$ 和 $u_2$。然后使用 Gram-Schmidt 过程将它们正交归一化。\n    首先，通过归一化 $u_1$ 得到 $v_1$：$v_1 = u_1 / \\|u_1\\|_2$。其次，移除 $u_2$ 平行于 $v_1$ 的分量：$u_2' = u_2 - (v_1^\\top u_2) v_1$。最后，将得到的向量 $u_2'$ 归一化以产生 $v_2 = u_2' / \\|u_2'\\|_2$。对于连续分布，$u_1$ 和 $u_2$ 线性相关的事件（使得 $\\|u_2'\\|_2 = 0$）的概率为零，但健壮的代码应处理这种不大可能发生的情况。\n\n*   **输入样本（$\\theta_i$）：**\n    对于每个测试用例，从多元标准正态分布 $\\mathcal{N}(0, I_d)$ 中抽取 $N$ 个独立样本 $\\theta_i$（$i=1, \\dots, N$），其中 $I_d$ 是 $d \\times d$ 的单位矩阵。\n\n**3. 蒙特卡洛估计和特征分解**\n\n矩阵 $C$ 通过梯度外积的样本均值来估计，记为 $\\widehat{C}$：\n$$\n\\widehat{C} = \\frac{1}{N}\\sum_{i=1}^N g_i g_i^\\top\n$$\n其中 $g_i = \\nabla_\\theta f(\\theta_i)$。算法流程如下：\n1. 初始化一个 $d \\times d$ 矩阵 $\\widehat{C}$ 为全零。\n2. 对每个样本 $i=1, \\dots, N$：\n   a. 抽取一个样本 $\\theta_i \\sim \\mathcal{N}(0, I_d)$。\n   b. 使用相应的推导公式计算解析梯度向量 $g_i = \\nabla_\\theta f(\\theta_i)$。\n   c. 计算外积 $g_i g_i^\\top$，这是一个 $d \\times d$ 矩阵。\n   d. 将该矩阵加到累加器 $\\widehat{C}$ 中。\n3. 循环结束后，将累加的矩阵除以样本量 $N$。\n\n根据构造，矩阵 $\\widehat{C}$ 是对称半正定的。然后我们计算它的特征值。由于 $\\widehat{C}$ 是实对称的，其所有特征值都是实数且非负。为此，我们使用一种高效的数值算法，例如在 `numpy.linalg.eigh` 中实现的算法。得到的特征值 $\\{\\lambda_j\\}_{j=1}^d$ 按降序排序：$\\lambda_1 \\ge \\lambda_2 \\ge \\dots \\ge \\lambda_d \\ge 0$。\n\n**4. 活性子空间维度计算**\n\n最后一步是找到最小的非负整数 $m$，使得前 $m$ 个特征值所捕获的方差比例达到或超过阈值 $\\tau$。\n总方差是所有特征值的和，$S = \\sum_{j=1}^d \\lambda_j$。前 $k$ 个特征值捕获的累积方差是 $\\sum_{j=1}^k \\lambda_j$。我们寻求满足以下条件的最小 $m \\in \\{0, 1, \\dots, d\\}$：\n$$\n\\frac{\\sum_{j=1}^m \\lambda_j}{S} \\ge \\tau\n$$\n步骤如下：\n1. 计算总和 $S$。如果 $S$ 实际上为零（例如，小于机器精度容差），这表示函数为常数，梯度处处为零。在这种情况下，按照规定，我们设置 $m=0$。\n2. 我们将累加和初始化为零。我们从 $j=1$ 到 $d$ 遍历排序后的特征值，将每个 $\\lambda_j$ 加入累加和。\n3. 在迭代的每一步中，我们检查当前累加和与总和 $S$ 的比率是否大于或等于 $\\tau$。\n4. 第一个满足此条件的索引 $j$ 即为活性子空间维度 $m=j$。然后终止当前测试用例的流程。\n5. 如果循环完成时仍未满足条件（例如，由于浮点不精确性，当 $\\tau=1.0$ 时可能发生这种情况），则维度取为 $d$。\n\n这就完成了解决该问题的理论和算法框架。实现将直接遵循这些步骤来处理所提供的每个测试用例。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite for active subspace dimension calculation.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        {'f_type': 'rank1', 'd': 7, 'N': 4096, 'tau': 0.9, 's': 11},\n        {'f_type': 'iso', 'd': 5, 'N': 2048, 'tau': 0.68, 's': 13},\n        {'f_type': 'rank2', 'd': 10, 'N': 4096, 'tau': 0.9, 's': 17},\n        {'f_type': 'iso', 'd': 4, 'N': 1024, 'tau': 0.99, 's': 19},\n    ]\n\n    results = []\n    for case in test_cases:\n        m = compute_active_subspace_dimension(**case)\n        results.append(m)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef compute_active_subspace_dimension(f_type, d, N, tau, s):\n    \"\"\"\n    Computes the active subspace dimension for a single test case.\n\n    Args:\n        f_type (str): The type of function ('rank1', 'iso', 'rank2').\n        d (int): The dimension of the parameter space.\n        N (int): The number of Monte Carlo samples.\n        tau (float): The energy threshold for the active subspace.\n        s (int): The random seed for reproducibility.\n\n    Returns:\n        int: The computed active subspace dimension 'm'.\n    \"\"\"\n    \n    # 1. Initialize deterministic random number generator\n    rng = np.random.default_rng(s)\n\n    # 2. Define gradient function based on f_type\n    grad_func = None\n    if f_type == 'rank1':\n        # Generate the fixed random vector w\n        w_raw = rng.normal(size=d)\n        w = w_raw / np.linalg.norm(w_raw)\n        \n        def grad_rank1(theta):\n            # Gradient of f(theta) = (w^T * theta)^3 is 3 * (w^T * theta)^2 * w\n            return 3 * (w.T @ theta)**2 * w\n        grad_func = grad_rank1\n\n    elif f_type == 'iso':\n        # Gradient of f(theta) = 0.5 * ||theta||^2 is theta\n        grad_func = lambda theta: theta\n\n    elif f_type == 'rank2':\n        # Generate two orthonormal vectors v1, v2 using Gram-Schmidt\n        u1_raw = rng.normal(size=d)\n        v1 = u1_raw / np.linalg.norm(u1_raw)\n        \n        u2_raw = rng.normal(size=d)\n        proj_u2_on_v1 = (v1.T @ u2_raw) * v1\n        u2_prime = u2_raw - proj_u2_on_v1\n        norm_u2_prime = np.linalg.norm(u2_prime)\n        \n        # This case is highly improbable with continuous distributions\n        if norm_u2_prime < 1e-15:\n            # Regenerate until we get a linearly independent vector\n            while norm_u2_prime < 1e-15:\n                u2_raw = rng.normal(size=d)\n                proj_u2_on_v1 = (v1.T @ u2_raw) * v1\n                u2_prime = u2_raw - proj_u2_on_v1\n                norm_u2_prime = np.linalg.norm(u2_prime)\n\n        v2 = u2_prime / norm_u2_prime\n        \n        def grad_rank2(theta):\n            # Gradient of f(theta) = (v1^T*theta)^3 + (v2^T*theta)^3\n            # is 3*(v1^T*theta)^2*v1 + 3*(v2^T*theta)^2*v2\n            term1 = 3 * (v1.T @ theta)**2 * v1\n            term2 = 3 * (v2.T @ theta)**2 * v2\n            return term1 + term2\n        grad_func = grad_rank2\n\n    # 3. Estimate the matrix C using Monte Carlo\n    C_hat = np.zeros((d, d))\n    \n    # Generate all samples at once for efficiency\n    thetas = rng.normal(size=(N, d))\n    \n    for i in range(N):\n        theta_i = thetas[i, :]\n        grad = grad_func(theta_i)\n        C_hat += np.outer(grad, grad)\n        \n    C_hat /= N\n    \n    # 4. Compute eigenvalues and determine the active subspace dimension m\n    \n    # C_hat is real and symmetric, so use eigh for efficiency and stability.\n    # It returns eigenvalues in ascending order.\n    eigenvalues = np.linalg.eigh(C_hat)[0]\n    \n    # Sort eigenvalues in descending order and ensure non-negativity\n    eigenvalues = np.sort(eigenvalues)[::-1]\n    eigenvalues = np.maximum(0, eigenvalues)\n\n    total_eig_sum = np.sum(eigenvalues)\n\n    # Handle the case where the function is constant (zero gradient)\n    if total_eig_sum < 1e-15:\n        return 0\n    \n    # Find the smallest m such that the cumulative energy exceeds tau\n    cumulative_eig_sum = 0\n    for i in range(d):\n        cumulative_eig_sum += eigenvalues[i]\n        if cumulative_eig_sum / total_eig_sum >= tau:\n            return i + 1\n\n    # If threshold is not met even with all eigenvalues (e.g., tau=1.0 and rounding)\n    return d\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3157324"}, {"introduction": "在分析给定模型的基础上，科学探索的一个核心目标是从数据中发现系统的底层结构。区分因果关系与简单的相关性，对于获得真正的科学理解和进行有效干预至关重要。在本练习 ([@problem_id:3157243]) 中，你将扮演一位计算生物学家的角色，任务是揭示一个基因调控网络。你将实现一个经典的基于约束的因果发现算法，利用观测数据和干预数据来推断网络中的方向性连接，从而掌握数据驱动科学研究的一项基本技能。", "problem": "考虑一个建模为有向无环图 (DAG) 的小型基因调控网络，其中变量 $x_1, x_2, x_3, x_4$ 服从一个线性高斯结构方程模型。数据生成遵循因果马尔可夫条件和忠实性假设（在线性高斯噪声下）。具体来说，每个变量 $x_i$ 由一个结构方程给出，形式为 $x_i = \\sum_{j \\in \\mathrm{Pa}(i)} w_{ji} x_j + \\varepsilon_i$，其中 $\\mathrm{Pa}(i)$ 表示 DAG 中 $x_i$ 的父节点集，$w_{ji} \\in \\mathbb{R}$ 是固定系数，而 $\\varepsilon_i \\sim \\mathcal{N}(0,\\sigma_i^2)$ 是相互独立的噪声项。\n\n您将获得一个代表最小基因调控模体的 $4$ 变量网络的真实机制：\n- $x_1 = \\varepsilon_1$,\n- $x_2 = \\varepsilon_2$,\n- $x_3 = w_{13} x_1 + w_{23} x_2 + \\varepsilon_3$,\n- $x_4 = w_{34} x_3 + \\varepsilon_4$,\n系数为 $w_{13} = 0.9$、$w_{23} = 0.8$、$w_{34} = 0.7$，噪声标准差为 $\\sigma_1 = 1.0$、$\\sigma_2 = 1.0$、$\\sigma_3 = 0.5$、$\\sigma_4 = 0.5$。根据构造，所有变量都以 $0$ 为中心。这定义了一个真实的 DAG，其边为 $x_1 \\rightarrow x_3$、$x_2 \\rightarrow x_3$ 和 $x_3 \\rightarrow x_4$。\n\n您的任务是实现一个基于约束的因果发现算法，从样本中推断 DAG，并测试推断结构在人工干预 $do(x_i = \\tilde{x})$ 下的稳定性。人工干预会切断所有进入 $x_i$ 的边，并在所有样本中将 $x_i$ 设为常数 $\\tilde{x}$。为避免在 $do(x_i=\\tilde{x})$ 下因零方差引入的退化问题，在干预测试用例中，仅在未干预的变量中推断 DAG。\n\n算法要求：\n- 使用一种基于约束的方法，该方法根植于 Peter-Clark (PC) 算法，通过条件独立性检验来学习骨架，然后通过对撞结构识别和一条 Meek 定向规则来定向边。\n- 条件独立性检验必须基于一个经过充分检验的事实：对于联合高斯变量，条件独立性 $x_i \\perp x_j \\mid S$ 等价于零偏相关 $\\rho_{ij\\cdot S} = 0$。通过 Fisher $z$ 变换实现检验：如果残差的样本相关性为 $r$，则检验统计量为 $z = \\frac{1}{2} \\ln \\left( \\frac{1+r}{1-r} \\right) \\sqrt{n - |S| - 3}$，其中 $n$ 是样本量， $|S|$ 是条件集的大小。在显著性水平 $\\alpha$ 下使用双边检验。\n- 通过在条件集大小最大为 $2$ 的情况下检测到条件独立性时移除边来学习无向骨架。\n- 定向v型结构（对撞结构）：对于三元组 $(i,k,j)$，若存在边 $i - k$ 和 $j - k$，且 $i$ 和 $j$ 不相邻，则当 $k$ 不在骨架发现过程中为 $(i,j)$ 找到的任何分离集中时，定向为 $i \\rightarrow k \\leftarrow j$。\n- 应用一条 Meek 定向规则来传播方向：如果存在 $i \\rightarrow j$ 和 $j - k$，且 $i$ 和 $k$ 不相邻，则将 $j-k$ 定向为 $j \\rightarrow k$。\n\n评估指标：\n- 计算推断出的有向图与真实的（基准）有向图之间的结构汉明距离 (SHD)，对于干预情况，该计算仅限于未干预变量的集合。对于限制集合中的每个无序对 $\\{i,j\\}$，如果推断的邻接关系与真实的邻接关系不匹配，则计数 $1$：真实图中存在边而推断图中缺失，真实图中不存在边而推断图中多出，或者方向错误。如果真实图中有向边在推断图中仍为无向边，则计数 $1$。\n\n人工数据生成：\n- 观测样本从上述结构方程中抽取。\n- 干预样本 $do(x_i = \\tilde{x})$ 是通过将 $x_i$ 的结构方程替换为 $x_i := \\tilde{x}$ 来生成的，此替换对所有样本均相同；所有出边保持不变，而为了数据生成的目的，所有进入 $x_i$ 的入边被有效切断。推断应仅在由未干预变量诱导的子图上执行。\n\n测试套件：\n提供以下 $4$ 个测试用例的 SHD 结果。在所有情况下，除特别注明外，均使用上述指定的真实权重和噪声参数。独立性检验的显著性水平为每个案例指定的 $\\alpha$，且条件集的大小最多为 $2$。\n\n- 测试用例 $1$（理想情况）：观测数据，$n=1000$，$\\alpha = 0.01$，在变量 $\\{x_1, x_2, x_3, x_4\\}$ 上进行推断。\n- 测试用例 $2$（对父节点进行干预）：干预数据 $do(x_1 = \\tilde{x})$，其中 $\\tilde{x} = 0$，$n=1000$，$\\alpha = 0.01$，在变量 $\\{x_2, x_3, x_4\\}$ 上进行推断。\n- 测试用例 $3$（边界样本量）：观测数据，$n=80$，$\\alpha = 0.05$，在变量 $\\{x_1, x_2, x_3, x_4\\}$ 上进行推断。\n- 测试用例 $4$（对对撞结构进行干预）：干预数据 $do(x_3 = \\tilde{x})$，其中 $\\tilde{x} = 0$，$n=1000$，$\\alpha = 0.01$，在变量 $\\{x_1, x_2, x_4\\}$ 上进行推断。\n\n最终输出格式：\n您的程序应产生单行输出，包含四个测试用例的 SHD 结果，形式为一个用方括号括起来的逗号分隔列表（例如，$[h_1,h_2,h_3,h_4]$），其中每个 $h_i$ 是一个整数。", "solution": "该问题要求实现一个基于约束的因果发现算法，以推断一个建模为有向无环图 (DAG) 的小型基因调控网络的结构。底层系统是一个线性高斯结构方程模型 (SEM)。解决方案涉及数据生成、因果结构推断，以及针对包括观测数据和干预数据在内的多个测试用例，与一个真实模型进行评估。\n\n首先，我们处理问题的验证。所有给定条件，包括结构方程、参数（$w_{ij}, \\sigma_i$）、算法要求（类PC的骨架化、v型结构定向、一条Meek规则）和评估指标（结构汉明距离，SHD），都已明确陈述。该问题在完善的因果推断理论中有其科学依据，特别是因果马尔可夫条件和忠实性假设，它们为数据中的统计独立性与因果DAG中d-分离的图属性之间提供了理论联系。其表述是适定的、客观的，并且不包含科学或逻辑上的矛盾。因此，该问题被认为是有效的。\n\n解决方案按以下一系列原则性步骤进行：\n\n**1. 从结构方程模型生成数据**\n\n真实网络由一组带高斯噪声的线性方程定义，代表了因果机制：\n$$x_1 = \\varepsilon_1$$\n$$x_2 = \\varepsilon_2$$\n$$x_3 = w_{13} x_1 + w_{23} x_2 + \\varepsilon_3$$\n$$x_4 = w_{34} x_3 + \\varepsilon_4$$\n其中 $\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma_i^2)$ 是独立的噪声项。样本是通过首先从噪声分布中抽样，然后按拓扑顺序计算 $x_i$ 的值来生成的。对于干预数据 $do(x_i = \\tilde{x})$，在计算其后代的值之前，将 $x_i$ 的结构方程替换为赋值 $x_i := \\tilde{x}$。这正确地模拟了理想干预的效果，即打破所有指向被干预变量的因果影响。\n\n**2. 条件独立性 (CI) 检验**\n\n基于约束的因果发现的基石是能够检验条件独立性。对于联合高斯变量，条件 $x_i \\perp x_j \\mid S$ 等价于零偏相关 $\\rho_{ij \\cdot S} = 0$。我们使用样本数据来检验这个假设。样本偏相关 $r$ 是通过计算 $x_i$ 和 $x_j$ 在都对条件集 $S$ 中的变量进行回归后的残差之间的相关性来计算的。\n\n样本偏相关的统计显著性通过 Fisher $z$ 变换来评估。检验统计量由下式给出：\n$$z = \\frac{1}{2} \\ln \\left( \\frac{1+r}{1-r} \\right) \\sqrt{n - |S| - 3}$$\n其中 $n$ 是样本量， $|S|$ 是条件集的大小。在零偏相关的原假设下，$z$ 服从标准正态分布 $\\mathcal{N}(0, 1)$。我们根据此 $z$ 统计量计算双边p值。如果p值大于预定的显著性水平 $\\alpha$，我们未能拒绝原假设，并得出变量是条件独立的结论。\n\n**3. 因果图推断算法**\n\n我们实现一个简化版的 Peter-Clark (PC) 算法，它包含三个主要阶段：\n\n*   **骨架发现**：我们从一个覆盖所有变量的全连接无向图开始。然后，我们对每个相邻变量对 $(i,j)$，在条件集 $S$ 的大小递增（此处为 $|S|=0, 1, 2$）的情况下，系统地检验条件独立性。如果发现一个独立性 $x_i \\perp x_j \\mid S$，则移除 $i$ 和 $j$ 之间的边，并将集合 $S$ 记录为对 $(i, j)$ 的分离集。此过程将图简化为一个无向骨架，该骨架代表了数据中存在的条件独立性关系。\n\n*   **V型结构定向**：在建立骨架之后，我们对一种称为v型结构或对撞结构的特定子结构进行定向。对于任何形成非耦合链 $i - k - j$（即 $i$ 和 $j$ 不相邻）的变量三元组 $(i, k, j)$，当且仅当节点 $k$ *不*在被发现使 $i$ 和 $j$ 独立的那个分离集中时，我们将边定向为 $i \\rightarrow k \\leftarrow j$。此规则源于d-分离的原则：位于 $i$ 和 $j$ 之间路径上的对撞结构 $k$ 不会阻断该路径，但以其为条件会打开该路径。因此，如果 $i$ 和 $j$ 被一个*不*包含 $k$ 的集合 $S$ 变得独立，那么 $k$ 必定是它们之间路径上的一个对撞结构。\n\n*   **边定向传播 (Meek规则)**：最后，我们应用进一步的定向规则，从已识别的v型结构开始在整个图中传播方向，同时避免产生新的v型结构或有向环。按照规定，我们使用第一条Meek规则：如果存在一条边 $i \\rightarrow j$ 和一条无向边 $j - k$，且节点 $i$ 和 $k$ 不相邻，则将边 $j - k$ 定向为 $j \\rightarrow k$。此过程被迭代应用，直到没有更多的边可以被定向为止。\n\n**4. 通过结构汉明距离 (SHD) 进行评估**\n\n推断图的准确性通过与真实DAG的结构汉明距离 (SHD) 来量化。SHD 计算两个图之间的边差异数量：\n1.  **多余边**：推断图中有边，但真实图中没有。\n2.  **缺失边**：真实图中有边，但推断图中没有。\n3.  **错误方向**：两个图中都存在边，但方向不同（例如，真实图中为 $i \\rightarrow j$，而推断图中为 $j \\rightarrow i$ 或 $i - j$）。\n\n该算法被应用于四个不同的测试用例，以评估其在不同样本量下以及在存在干预的情况下的性能。计算并报告每个用例的最终SHD。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import norm\nfrom itertools import combinations, permutations\n\ndef solve():\n    \"\"\"\n    Main function to run the four test cases and print the results.\n    \"\"\"\n    # Use a fixed random seed for reproducibility of results.\n    np.random.seed(42)\n\n    # Ground-truth parameters\n    W_13 = 0.9\n    W_23 = 0.8\n    W_34 = 0.7\n    SIGMAS = [1.0, 1.0, 0.5, 0.5]\n\n    test_cases = [\n        # Case 1: Observational, n=1000, alpha=0.01\n        {'n': 1000, 'alpha': 0.01, 'intervention': None, 'vars': [0, 1, 2, 3],\n         'true_adj': np.array([[0, 0, 1, 0], [0, 0, 1, 0], [0, 0, 0, 1], [0, 0, 0, 0]])},\n        # Case 2: Intervention do(x1=0), n=1000, alpha=0.01, on vars {x2, x3, x4}\n        {'n': 1000, 'alpha': 0.01, 'intervention': (0, 0.0), 'vars': [1, 2, 3],\n         'true_adj': np.array([[0, 1, 0], [0, 0, 1], [0, 0, 0]])},\n        # Case 3: Observational, n=80, alpha=0.05\n        {'n': 80, 'alpha': 0.05, 'intervention': None, 'vars': [0, 1, 2, 3],\n         'true_adj': np.array([[0, 0, 1, 0], [0, 0, 1, 0], [0, 0, 0, 1], [0, 0, 0, 0]])},\n        # Case 4: Intervention do(x3=0), n=1000, alpha=0.01, on vars {x1, x2, x4}\n        {'n': 1000, 'alpha': 0.01, 'intervention': (2, 0.0), 'vars': [0, 1, 3],\n         'true_adj': np.array([[0, 0, 0], [0, 0, 0], [0, 0, 0]])}\n    ]\n\n    results = []\n    for case in test_cases:\n        # 1. Generate Data\n        data_full = _generate_data(case['n'], SIGMAS, W_13, W_23, W_34, case['intervention'])\n        data_subset = data_full[:, case['vars']]\n\n        # 2. Discover DAG\n        inferred_adj = _discover_dag(data_subset, case['alpha'])\n\n        # 3. Calculate SHD\n        shd = _calculate_shd(inferred_adj, case['true_adj'])\n        results.append(shd)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef _generate_data(n, sigmas, w13, w23, w34, intervention=None):\n    \"\"\"Generates synthetic data from the linear Gaussian SEM.\"\"\"\n    eps = [np.random.normal(0, s, n) for s in sigmas]\n    x = [np.zeros(n) for _ in range(4)]\n\n    # Equations are solved in topological order, applying interventions\n    x[0] = eps[0]\n    if intervention and intervention[0] == 0:\n        x[0] = np.full(n, intervention[1])\n    \n    x[1] = eps[1]\n    if intervention and intervention[0] == 1:\n        x[1] = np.full(n, intervention[1])\n    \n    x[2] = w13 * x[0] + w23 * x[1] + eps[2]\n    if intervention and intervention[0] == 2:\n        x[2] = np.full(n, intervention[1])\n        \n    x[3] = w34 * x[2] + eps[3]\n    if intervention and intervention[0] == 3:\n        x[3] = np.full(n, intervention[1])\n        \n    return np.stack(x, axis=1)\n\ndef _ci_test(data, i, j, S, alpha):\n    \"\"\"Performs a conditional independence test using Fisher's z-transform.\"\"\"\n    n = data.shape[0]\n    k = len(S)\n    \n    if k == 0:\n        r = np.corrcoef(data[:, i], data[:, j])[0, 1]\n    else:\n        y_i = data[:, i]\n        y_j = data[:, j]\n        X_S = data[:, S]\n        \n        X_S_intercept = np.hstack([np.ones((n, 1)), X_S.reshape(n, k)])\n        \n        try:\n            beta_i = np.linalg.lstsq(X_S_intercept, y_i, rcond=None)[0]\n            res_i = y_i - X_S_intercept @ beta_i\n            beta_j = np.linalg.lstsq(X_S_intercept, y_j, rcond=None)[0]\n            res_j = y_j - X_S_intercept @ beta_j\n        except np.linalg.LinAlgError:\n            return False\n\n        with np.errstate(invalid='ignore'):\n            r_matrix = np.corrcoef(res_i, res_j)\n        \n        if np.any(np.isnan(r_matrix)) or np.any(np.isinf(r_matrix)):\n             return False # Treat as dependent if correlation is undefined\n        r = r_matrix[0, 1]\n\n    # Clip for numerical stability with arctanh\n    r = np.clip(r, -1.0 + 1e-12, 1.0 - 1e-12)\n    # Fisher's z-transform\n    z = 0.5 * np.log((1 + r) / (1 - r)) * np.sqrt(n - k - 3)\n    p_val = 2 * (1 - norm.cdf(np.abs(z)))\n    \n    return p_val > alpha\n\ndef _discover_dag(data, alpha):\n    \"\"\"Infers the DAG structure using a simplified PC algorithm.\"\"\"\n    num_vars = data.shape[1]\n    \n    # 1. Skeleton Discovery\n    adj = np.ones((num_vars, num_vars), dtype=int) - np.eye(num_vars)\n    separating_sets = {}\n    \n    for d in range(3): # Conditioning sets up to size 2\n        for i in range(num_vars):\n            for j in range(i + 1, num_vars):\n                if adj[i, j] == 1:\n                    neighbors_i = [k for k in range(num_vars) if adj[i, k] == 1 and k != j]\n                    if len(neighbors_i) >= d:\n                        for S in combinations(neighbors_i, d):\n                            if _ci_test(data, i, j, list(S), alpha):\n                                adj[i, j] = adj[j, i] = 0\n                                separating_sets[(i, j)] = list(S)\n                                separating_sets[(j, i)] = list(S)\n                                break\n\n    # 2. V-structure orientation\n    pdag = adj.copy() # Partially Directed Acyclic Graph\n    for i, j, k in permutations(range(num_vars), 3):\n        # Find uncoupled colliders i-k-j where i and j are not adjacent\n        if pdag[i, k] == 1 and pdag[k, j] == 1 and pdag[i, j] == 0 and pdag[j, i] == 0:\n            sep_set = separating_sets.get((i, j))\n            if sep_set is not None and k not in sep_set:\n                pdag[k, i] = 0\n                pdag[k, j] = 0\n\n    # 3. Meek Rule R1 Application\n    changed = True\n    while changed:\n        changed = False\n        for i, j, k in permutations(range(num_vars), 3):\n            is_i_to_j = (pdag[i, j] == 1 and pdag[j, i] == 0)\n            is_j_undir_k = (pdag[j, k] == 1 and pdag[k, j] == 1)\n            is_i_k_nonadj = (pdag[i, k] == 0 and pdag[k, i] == 0)\n            \n            if is_i_to_j and is_j_undir_k and is_i_k_nonadj:\n                pdag[k, j] = 0\n                changed = True\n                \n    return pdag\n\ndef _calculate_shd(inferred_adj, true_adj):\n    \"\"\"Calculates the Structural Hamming Distance.\"\"\"\n    shd = 0\n    num_vars = inferred_adj.shape[0]\n    for i in range(num_vars):\n        for j in range(i + 1, num_vars):\n            true_edge_exists = (true_adj[i, j] != 0 or true_adj[j, i] != 0)\n            inferred_edge_exists = (inferred_adj[i, j] != 0 or inferred_adj[j, i] != 0)\n            \n            if true_edge_exists != inferred_edge_exists:\n                shd += 1 # Extra or missing edge\n            elif true_edge_exists: # Both have an edge, check orientation\n                if (inferred_adj[i, j] != true_adj[i, j] or inferred_adj[j, i] != true_adj[j, i]):\n                    shd += 1 # Wrong orientation or undirected vs directed\n    return shd\n\nsolve()\n```", "id": "3157243"}, {"introduction": "许多数据驱动的科学方法的终极目标是自动发现支配一个系统的数学方程。我们可以通过构建一个包含各种可能数学项的庞大“候选库”，然后利用稀疏回归技术来筛选出能够最佳描述观测数据的少数几个关键项，从而实现这一目标。在本练习 ([@problem_id:3157268]) 中，你将体验以偏微分方程形式发现物理定律的过程。你将实现一个近端梯度算法来求解一个稀疏组套索 (sparse group Lasso) 问题，从而从候选库中筛选并识别出底层方程的真实组成部分。", "problem": "你将实现一种稀疏回归方法，从一个为偏微分方程结构发现而设计的多项式库中选择相关项，该方法使用一个基于稀疏组正则化的凸复合优化模型。从以下基本基础出发：监督学习的线性经验风险最小化原则（最小化经验最小二乘风险）、平方损失的凸性，以及结构化惩罚的近端算子定义。具体来说，设特征矩阵为 $X \\in \\mathbb{R}^{n \\times p}$，目标向量为 $y \\in \\mathbb{R}^{n}$。考虑一个结合了最小二乘法与稀疏组惩罚的凸目标函数，其形式如下\n$$\n\\min_{w \\in \\mathbb{R}^{p}} \\; \\frac{1}{2n}\\lVert X w - y \\rVert_2^2 \\;+\\; \\lambda \\left( (1 - \\alpha) \\sum_{g \\in \\mathcal{G}} \\lVert w_g \\rVert_2 \\;+\\; \\alpha \\lVert w \\rVert_1 \\right),\n$$\n其中 $n$ 是样本数量，$p$ 是特征数量，$w$ 是系数向量，$\\lambda \\in \\mathbb{R}_{>0}$ 是正则化强度，$\\alpha \\in (0,1)$ 在组稀疏和元素级稀疏之间进行插值，$\\mathcal{G}$ 是特征索引到非重叠组的划分，其中 $w_g$ 表示由组 $g$ 索引的 $w$ 的子向量。你将使用的分组 $\\mathcal{G}$ 对应于在一个一维标量场的多项式库中按空间导数阶数分组的项。你必须使用一种基于原则的一阶近端梯度方法来解决这个优化问题，其步长需从梯度的 Lipschitz 常数导出，并具有可证明的有效性。不要使用任何预打包的机器学习求解器。\n\n按如下方式构建一个合成数据集，使用解析表达式以避免数值微分误差。设 $x$ 和 $t$ 是在矩形网格上采样的自变量，该网格在 $[0,1]$ 上有 $N_x$ 个等距点，在 $[0,1]$ 上有 $N_t$ 个等距点；令 $n = N_x \\cdot N_t$。定义标量场\n$$\nu(x,t) = \\sin(2\\pi x)\\, e^{-t} + \\frac{1}{2} \\cos(4\\pi x)\\, e^{-0.2\\, t},\n$$\n及其一阶和二阶空间导数\n$$\nu_x(x,t) = 2\\pi \\cos(2\\pi x)\\, e^{-t} - 2\\pi \\sin(4\\pi x)\\, e^{-0.2\\, t},\n$$\n$$\nu_{xx}(x,t) = - (2\\pi)^2 \\sin(2\\pi x)\\, e^{-t} - 8\\pi^2 \\cos(4\\pi x)\\, e^{-0.2\\, t}.\n$$\n构建一个多项式库 $X \\in \\mathbb{R}^{n \\times p}$，包含 $p = 9$ 列，顺序如下：\n$[\\, 1,\\; u,\\; u^2,\\; u_x,\\; u\\,u_x,\\; u^2 u_x,\\; u_{xx},\\; u\\,u_{xx},\\; (u_x)^2 \\,]$。\n将响应的真实生成机制定义为恰好两个项的线性组合：\n$$\ny = \\beta_{u u_x} \\cdot (u\\,u_x) + \\beta_{u_{xx}} \\cdot u_{xx} + \\varepsilon,\n$$\n其中 $\\beta_{u u_x} = 2.0$，$\\beta_{u_{xx}} = 0.1$，并加入标准差为 $\\sigma$ 的加性独立高斯噪声 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2)$。因此，系数的真实非零支撑集恰好由指定排序中对应于 $u\\,u_x$ 和 $u_{xx}$ 的索引组成。\n\n按空间导数阶数将 $p = 9$ 个特征划分为 3 个非重叠组：\n- 组 0（0 阶）：索引 $[0,1,2]$ 对应于 $[1,\\; u,\\; u^2]$。\n- 组 1（1 阶）：索引 $[3,4,5]$ 对应于 $[u_x,\\; u\\,u_x,\\; u^2 u_x]$。\n- 组 2（2 阶）：索引 $[6,7,8]$ 对应于 $[u_{xx},\\; u\\,u_{xx},\\; (u_x)^2]$。\n\n实现一个最小化给定目标函数的近端梯度算法。从梯度 Lipschitz 常数的界推导出一个有效的恒定步长，并为稀疏组惩罚实现正确的近端算子。在优化前，将 $X$ 的每一列标准化为零均值和单位方差，并通过减去其均值来中心化 $y$。优化后，通过使用用户指定的阈值 $\\tau > 0$ 对系数的绝对值进行阈值处理来确定预测的支撑集，如果 $|w_j| \\ge \\tau$，则认为该索引被选中。按如下方式计算精确率和召回率：精确率是真阳性数量除以预测阳性总数，召回率是真阳性数量除以真实非零项总数。将它们表示为小数（而非百分比）。\n\n你的程序必须实现以下测试套件，运行所有案例，并汇总结果。使用 $N_x = 64$ 和 $N_t = 16$，因此 $n = 1024$。对于每个案例，使用指定的标准差 $\\sigma$ 构建一个噪声向量。正则化参数应设置为 $\\lambda = \\kappa \\cdot \\Lambda(\\alpha)$，其中 $\\kappa$ 是一个给定的正标量，$\\Lambda(\\alpha)$ 是一个依赖于数据的尺度，由标准化的 $(X,y)$ 计算得出\n$$\n\\Lambda(\\alpha) = \\max\\left( \\frac{1}{n}\\lVert X^\\top y \\rVert_{\\infty} \\cdot \\frac{1}{\\alpha}, \\; \\max_{g \\in \\mathcal{G}} \\frac{1}{n}\\lVert X_g^\\top y \\rVert_2 \\cdot \\frac{1}{1 - \\alpha} \\right),\n$$\n约定当 $\\alpha$ 等于 $0$ 或 $1$ 时，通过省略相应项来忽略除以零的情况，这在测试套件中不会发生。这种缩放为 $\\lambda$ 相对于观测数据提供了一个稳健的参考量级。\n\n测试套件参数，每个都指定为一个元组 $(\\alpha, \\kappa, \\sigma, \\tau)$：\n- 案例 1：$(0.5, 0.10, 0.0, 10^{-3})$。\n- 案例 2：$(0.5, 0.20, 0.01, 10^{-3})$。\n- 案例 3：$(0.8, 0.30, 0.02, 10^{-3})$。\n- 案例 4：$(0.2, 0.60, 0.05, 10^{-3})$。\n\n对于每个案例，使用带有原则性步长的近端梯度下降法拟合模型，然后计算恢复的支撑集相对于真实情况的精确率和召回率。你的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表的结果，顺序如下\n$[\\, \\text{precision}_1, \\text{recall}_1, \\text{precision}_2, \\text{recall}_2, \\text{precision}_3, \\text{recall}_3, \\text{precision}_4, \\text{recall}_4 \\,]$，\n每个值都四舍五入到恰好三位小数。不应打印任何其他输出。未使用角度，因此不需要角度单位。此问题中没有物理单位。所有计算都将使用实数进行。最终输出必须严格遵守指定格式。", "solution": "用户提供了一个有效的问题陈述。\n\n### 步骤 1：提取已知条件\n\n- **目标函数**：最小化 $w \\in \\mathbb{R}^{p}$ 的凸目标函数：\n$$\n\\frac{1}{2n}\\lVert X w - y \\rVert_2^2 \\;+\\; \\lambda \\left( (1 - \\alpha) \\sum_{g \\in \\mathcal{G}} \\lVert w_g \\rVert_2 \\;+\\; \\alpha \\lVert w \\rVert_1 \\right)\n$$\n- **数据**：特征矩阵 $X \\in \\mathbb{R}^{n \\times p}$ 和目标向量 $y \\in \\mathbb{R}^{n}$，有 $n$ 个样本和 $p$ 个特征。\n- **参数**：正则化强度 $\\lambda \\in \\mathbb{R}_{>0}$，插值参数 $\\alpha \\in (0,1)$。\n- **组**：$\\mathcal{G}$ 是特征索引 $\\{0, 1, \\dots, p-1\\}$ 到非重叠组的一个划分。$w_g$ 是 $w$ 对应于组 $g$ 的子向量。\n- **合成数据集**：\n    - 网格：$N_x = 64$ 个点在 $[0,1]$ 上，以及 $N_t = 16$ 个点在 $[0,1]$ 上，总样本点数 $n = N_x \\cdot N_t = 1024$。\n    - 标量场：$u(x,t) = \\sin(2\\pi x)\\, e^{-t} + \\frac{1}{2} \\cos(4\\pi x)\\, e^{-0.2\\, t}$。\n    - 空间导数：$u_x(x,t) = 2\\pi \\cos(2\\pi x)\\, e^{-t} - 2\\pi \\sin(4\\pi x)\\, e^{-0.2\\, t}$ 和 $u_{xx}(x,t) = - (2\\pi)^2 \\sin(2\\pi x)\\, e^{-t} - 8\\pi^2 \\cos(4\\pi x)\\, e^{-0.2\\, t}$。\n    - 特征库 ($p=9$)：$X$ 的列按顺序为 $[\\, 1,\\; u,\\; u^2,\\; u_x,\\; u\\,u_x,\\; u^2 u_x,\\; u_{xx},\\; u\\,u_{xx},\\; (u_x)^2 \\,]$。\n    - 目标向量：$y = \\beta_{u u_x} \\cdot (u\\,u_x) + \\beta_{u_{xx}} \\cdot u_{xx} + \\varepsilon$，系数为 $\\beta_{u u_x} = 2.0$，$\\beta_{u_{xx}} = 0.1$，噪声为 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2)$。\n- **真实情况**：真实的非零系数对应于项 $u\\,u_x$（索引 4）和 $u_{xx}$（索引 6）。\n- **分组结构**：$p=9$ 个特征按导数阶数划分为 3 个组：\n    - 组 0：索引 $[0, 1, 2]$ 对应于 $[1, u, u^2]$。\n    - 组 1：索引 $[3, 4, 5]$ 对应于 $[u_x, u\\,u_x, u^2 u_x]$。\n    - 组 2：索引 $[6, 7, 8]$ 对应于 $[u_{xx}, u\\,u_{xx}, (u_x)^2]$。\n- **方法**：带有原则性恒定步长的近端梯度下降法。\n- **预处理**：将 $X$ 的列标准化为零均值和单位方差。通过减去其均值来中心化 $y$。\n- **正则化缩放**：$\\lambda = \\kappa \\cdot \\Lambda(\\alpha)$，其中 $\\Lambda(\\alpha) = \\max\\left( \\frac{1}{n}\\lVert X^\\top y \\rVert_{\\infty} \\cdot \\frac{1}{\\alpha}, \\; \\max_{g \\in \\mathcal{G}} \\frac{1}{n}\\lVert X_g^\\top y \\rVert_2 \\cdot \\frac{1}{1 - \\alpha} \\right)$，在标准化/中心化数据上计算。\n- **评估**：\n    - 预测的支撑集：索引 $j$ 其中 $|w_j| \\ge \\tau$。\n    - 指标：精确率和召回率，定义为 $\\text{precision} = \\frac{\\text{TP}}{\\text{predicted positives}}$ 和 $\\text{recall} = \\frac{\\text{TP}}{\\text{true nonzeros}}$。\n- **测试套件**：4 个案例，参数为 $(\\alpha, \\kappa, \\sigma, \\tau)$:\n    1. $(0.5, 0.10, 0.0, 10^{-3})$\n    2. $(0.5, 0.20, 0.01, 10^{-3})$\n    3. $(0.8, 0.30, 0.02, 10^{-3})$\n    4. $(0.2, 0.60, 0.05, 10^{-3})$\n- **输出格式**：单行 `[precision_1, recall_1, ..., precision_4, recall_4]`，值四舍五入到 3 位小数。\n\n### 步骤 2：使用提取的已知条件进行验证\n\n该问题具有科学依据，问题陈述清晰且目标明确。它基于凸优化和稀疏回归的既定原则，在机器学习和计算科学中是标准方法。问题是自包含的，提供了构建数据集和实现算法所需的所有公式、参数和步骤。设置是一致的，任务在计算上是可行的。根据验证清单，未发现任何缺陷。\n\n### 步骤 3：结论与行动\n\n问题是**有效的**。将提供一个解决方案。\n\n### 基于原则的解决方案设计\n\n该问题需要使用近端梯度方法解决一个凸复合优化问题。目标函数的形式为 $\\min_w F(w) = f(w) + g(w)$，其中 $f(w)$ 是一个光滑、凸函数，而 $g(w)$ 是一个凸、可能非光滑的正则化项。\n\n1.  **目标函数分解**：\n    目标函数分解为：\n    - 光滑部分（最小二乘损失）：$f(w) = \\frac{1}{2n}\\lVert X w - y \\rVert_2^2$。\n    - 非光滑部分（稀疏组惩罚）：$g(w) = \\lambda \\left( (1 - \\alpha) \\sum_{g \\in \\mathcal{G}} \\lVert w_g \\rVert_2 + \\alpha \\lVert w \\rVert_1 \\right)$。\n    对于优化过程，我们使用标准化的特征矩阵 $\\tilde{X}$ 和中心化的目标向量 $\\tilde{y}$。因此，我们最小化 $f(w) = \\frac{1}{2n}\\lVert \\tilde{X} w - \\tilde{y} \\rVert_2^2$ 加上相同的正则化项 $g(w)$。\n\n2.  **近端梯度方法**：\n    近端梯度算法通过以下更新规则生成一系列迭代：\n    $$\n    w^{(k+1)} = \\text{prox}_{\\gamma g}\\left(w^{(k)} - \\gamma \\nabla f(w^{(k)})\\right)\n    $$\n    其中 $\\gamma > 0$ 是步长，$\\text{prox}_{\\gamma g}$ 是函数 $\\gamma g$ 的近端算子。\n\n3.  **光滑部分的梯度**：\n    最小二乘项 $f(w)$ 的梯度是：\n    $$\n    \\nabla f(w) = \\frac{1}{n} \\tilde{X}^\\top (\\tilde{X} w - \\tilde{y})\n    $$\n\n4.  **步长选择**：\n    如果步长 $\\gamma$ 满足 $0 < \\gamma \\le 1/L$，其中 $L$ 是 $\\nabla f(w)$ 的 Lipschitz 常数，则近端梯度方法的收敛性得到保证。梯度是 Lipschitz 连续的，常数 $L = \\frac{1}{n} \\lVert \\tilde{X}^\\top \\tilde{X} \\rVert_2 = \\frac{1}{n} \\sigma_{\\max}(\\tilde{X}^\\top \\tilde{X})$，其中 $\\sigma_{\\max}$ 表示最大特征值（因为 $\\tilde{X}^\\top \\tilde{X}$ 是半正定的）。一个有原则的选择是将步长设置为其上界 $\\gamma = 1/L$，我们可以从数据中计算出来。\n\n5.  **稀疏组惩罚的近端算子**：\n    近端算子定义为 $\\text{prox}_{\\eta h}(v) = \\arg\\min_z \\left\\{ \\frac{1}{2}\\lVert z - v \\rVert_2^2 + \\eta h(z) \\right\\}$。惩罚项 $g(w)$ 在预定义的组 $\\mathcal{G}$ 之间是可分的。这意味着我们可以独立地为每个组子向量 $w_g$ 计算近端算子。对于单个组 $g$，我们必须解决：\n    $$\n    \\text{prox}_{\\gamma g_g}(v_g) = \\arg\\min_{w_g} \\left\\{ \\frac{1}{2}\\lVert w_g - v_g \\rVert_2^2 + \\gamma \\lambda(1-\\alpha)\\lVert w_g \\rVert_2 + \\gamma \\lambda\\alpha\\lVert w_g \\rVert_1 \\right\\}\n    $$\n    这个子问题的解，即稀疏组套索近端算子，是一个两阶段过程：\n    a. 将元素级软阈值算子 $S_{\\eta}(\\cdot)$ 应用于输入向量 $v_g$，阈值为 $\\eta_1 = \\gamma \\lambda \\alpha$：\n    $$\n    u_g = S_{\\eta_1}(v_g) \\quad \\text{其中 } (S_{\\eta_1}(v_g))_j = \\text{sign}((v_g)_j) \\max(|\\,(v_g)_j| - \\eta_1, 0)\n    $$\n    b. 将组级别软阈值（或块软阈值）应用于结果 $u_g$，阈值为 $\\eta_2 = \\gamma \\lambda (1-\\alpha)$：\n    $$\n    w_g^* = u_g \\cdot \\max\\left(0, 1 - \\frac{\\eta_2}{\\lVert u_g \\rVert_2}\\right)\n    $$\n    对所有组 $g \\in \\mathcal{G}$ 执行此操作，以获得完整的更新向量 $w^{(k+1)}$。\n\n6.  **算法总结**：\n    对于每个测试案例，所实现的算法将按以下步骤进行：\n    a. 根据指定的解析函数和噪声水平 $\\sigma$ 生成数据矩阵 $X$ 和目标向量 $y$。\n    b. 标准化 $X$ 得到 $\\tilde{X}$，中心化 $y$ 得到 $\\tilde{y}$。\n    c. 计算数据依赖的正则化尺度 $\\Lambda(\\alpha)$ 和最终的正则化参数 $\\lambda = \\kappa \\Lambda(\\alpha)$。\n    d. 计算 Lipschitz 常数 $L$ 和步长 $\\gamma = 1/L$。\n    e. 初始化权重向量 $w = \\mathbf{0}$。\n    f. 迭代近端梯度更新规则，直到 $w$ 的变化小于容差或达到最大迭代次数。\n    g. 在最终的系数向量 $w$ 上，通过阈值 $\\tau$ 识别非零支撑集。\n    h. 计算恢复的支撑集相对于已知的真实支撑集（索引 $\\{4, 6\\}$）的精确率和召回率。\n    i. 按要求存储和格式化结果。\n\n这种基于原则的方法确保了解决给定科学机器学习问题的正确和稳健的实现。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite for sparse group regression.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (alpha, kappa, sigma, tau)\n        (0.5, 0.10, 0.0, 1e-3),\n        (0.5, 0.20, 0.01, 1e-3),\n        (0.8, 0.30, 0.02, 1e-3),\n        (0.2, 0.60, 0.05, 1e-3),\n    ]\n\n    results = []\n    \n    # Fixed parameters for data generation\n    Nx = 64\n    Nt = 16\n    n = Nx * Nt\n    p = 9\n    \n    # Grid generation\n    x_grid = np.linspace(0, 1, Nx)\n    t_grid = np.linspace(0, 1, Nt)\n    xx, tt = np.meshgrid(x_grid, t_grid)\n    x = xx.ravel()\n    t = tt.ravel()\n\n    # Analytic field and derivatives\n    u = np.sin(2 * np.pi * x) * np.exp(-t) + 0.5 * np.cos(4 * np.pi * x) * np.exp(-0.2 * t)\n    u_x = 2 * np.pi * np.cos(2 * np.pi * x) * np.exp(-t) - 2 * np.pi * np.sin(4 * np.pi * x) * np.exp(-0.2 * t)\n    u_xx = -(2 * np.pi)**2 * np.sin(2 * np.pi * x) * np.exp(-t) - 8 * np.pi**2 * np.cos(4 * np.pi * x) * np.exp(-0.2 * t)\n\n    # Polynomial library\n    X = np.zeros((n, p))\n    X[:, 0] = 1.0\n    X[:, 1] = u\n    X[:, 2] = u**2\n    X[:, 3] = u_x\n    X[:, 4] = u * u_x\n    X[:, 5] = u**2 * u_x\n    X[:, 6] = u_xx\n    X[:, 7] = u * u_xx\n    X[:, 8] = u_x**2\n\n    # Group definitions\n    groups = [[0, 1, 2], [3, 4, 5], [6, 7, 8]]\n    true_support = {4, 6}\n\n    for i, case in enumerate(test_cases):\n        alpha, kappa, sigma, tau = case\n        \n        # Set seed for reproducible noise generation for each case\n        np.random.seed(i)\n        \n        # Generate target vector y\n        y_true = 2.0 * X[:, 4] + 0.1 * X[:, 6]\n        noise = np.random.normal(0, sigma, size=n)\n        y = y_true + noise\n\n        # Preprocessing: Standardize X and center y\n        y_mean = y.mean()\n        y_c = y - y_mean\n        \n        X_mean = X.mean(axis=0)\n        X_std = X.std(axis=0)\n        # Avoid division by zero for columns with zero variance (e.g., constant column)\n        X_std[X_std == 0] = 1.0\n        X_s = (X - X_mean) / X_std\n\n        # Calculate data-dependent regularization scale Lambda(alpha)\n        X_s_T_y_c = X_s.T @ y_c\n        \n        term1 = np.linalg.norm(X_s_T_y_c, ord=np.inf) / (n * alpha)\n        \n        term2_vals = []\n        for g_indices in groups:\n            norm_Xg_T_y_c = np.linalg.norm(X_s_T_y_c[g_indices], ord=2)\n            term2_vals.append(norm_Xg_T_y_c / (n * (1 - alpha)))\n        term2 = np.max(term2_vals)\n        \n        lambda_scale = max(term1, term2)\n        lambda_val = kappa * lambda_scale\n\n        # Proximal Gradient Descent\n        # Compute Lipschitz constant and stepsize\n        L = np.max(np.linalg.eigvalsh(X_s.T @ X_s / n))\n        if L == 0:\n            gamma = 1.0\n        else:\n            gamma = 1.0 / L\n\n        # Algorithm parameters\n        w = np.zeros(p)\n        max_iter = 20000\n        tol = 1e-7\n        lambda1 = lambda_val * (1 - alpha)\n        lambda2 = lambda_val * alpha\n\n        for _ in range(max_iter):\n            w_old = w.copy()\n            grad = X_s.T @ (X_s @ w - y_c) / n\n            v = w - gamma * grad\n            \n            w_new = np.zeros(p)\n            for g_indices in groups:\n                v_g = v[g_indices]\n                # Soft-thresholding for L1 penalty\n                u_g = np.sign(v_g) * np.maximum(np.abs(v_g) - gamma * lambda2, 0)\n                norm_u_g = np.linalg.norm(u_g)\n                # Group soft-thresholding for group L2 penalty\n                if norm_u_g > 0:\n                    scale_factor = np.maximum(0, 1 - (gamma * lambda1) / norm_u_g)\n                    w_new[g_indices] = scale_factor * u_g\n            \n            w = w_new\n            if np.linalg.norm(w - w_old) < tol:\n                break\n        \n        # Evaluation\n        predicted_support = {j for j, val in enumerate(w) if np.abs(val) >= tau}\n        \n        tp = len(true_support.intersection(predicted_support))\n        num_predicted_positives = len(predicted_support)\n        num_true_nonzeros = len(true_support)\n        \n        precision = tp / num_predicted_positives if num_predicted_positives > 0 else 0.0\n        recall = tp / num_true_nonzeros\n        \n        results.append(precision)\n        results.append(recall)\n\n    # Format the final output string\n    output_str = \",\".join([f\"{r:.3f}\" for r in results])\n    print(f\"[{output_str}]\")\n\nsolve()\n```", "id": "3157268"}]}