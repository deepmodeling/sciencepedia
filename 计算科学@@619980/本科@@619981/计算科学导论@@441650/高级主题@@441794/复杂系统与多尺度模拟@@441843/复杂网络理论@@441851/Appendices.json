{"hands_on_practices": [{"introduction": "社群结构是现实世界网络的一个基本特征，指的是网络中节点可以被划分为若干个群组，群组内部的连接比群组之间的连接更为密集。这项实践将从两个角度探索社群发现。首先，我们将基于节点邻域重叠度（Jaccard相似度）这一局部指标进行聚类；其次，我们将使用一种经典的动态算法——标签传播算法（LPA）。通过实现这些方法并使用归一化互信息（NMI）来比较它们的结果，你将对社群的定义、识别和评估方法获得深刻的实践理解 [@problem_id:3108222]。", "problem": "给定节点标签集为 $\\{0,1,2,\\dots,n-1\\}$ 的简单、无向、无权图。对于一个节点 $i$，将其开邻域 $N(i)$ 定义为与 $i$ 相邻的节点集合（即 $i \\notin N(i)$）。基于集合论和信息论的第一性原理，对每个测试图执行以下步骤：\n\n1.  仅使用集合交集、集合并集和集合基数的定义，计算每对无序节点 $\\{i,j\\}$ 之间的邻域重叠相似度。该相似度基于它们的邻域相对于两个邻域中所有不同邻居总数的重叠程度。如果邻域的并集基数为 $0$，则按惯例定义 $i \\neq j$ 时的两两相似度为 $0$，并定义所有节点 $i$ 的自相似度为 $1$。\n2.  通过构建一个相似度图，将这些两两相似度转换为簇。该图与原图使用相同的节点集，当且仅当节点 $i$ 和 $j$ 之间的相似度大于或等于给定阈值 $\\tau$ 时，它们之间存在一条无向边。将簇定义为该相似度图的连通分量。\n3.  使用标签传播算法（Label Propagation Algorithm, LPA）在原图上独立计算社群分配。为确保确定性，LPA 定义如下：将每个节点 $i$ 的标签初始化为其自身的索引 $i$。按节点索引递增的顺序重复对节点进行完整的异步扫描；当访问邻域 $N(i)$ 非空的节点 $i$ 时，将其标签更新为 $\\{ \\text{label}(u) : u \\in N(i) \\}$ 中最频繁的标签。通过选择最小的标签值来打破平局。如果 $N(i)$ 为空，则保持其当前标签。当一次完整的扫描没有导致任何标签改变，或扫描次数达到 $100$ 次时，算法终止。\n4.  使用信息论中的归一化互信息来量化这两个划分（阈值相似度簇和标签传播算法社群）之间的一致性。根据节点集上的计数，构建两个划分的簇标签的经验联合分布。然后，根据此联合分布计算互信息，并使用两个划分各自熵的几何平均数对其进行归一化。如果两个划分的熵均为零，则定义归一化互信息为 $1$。如果恰好一个划分的熵为零而另一个不为零，则定义归一化互信息为 $0$。将每个测试图的结果表示为一个在闭区间 $[0,1]$ 内的实数，四舍五入到 $6$ 位小数。\n\n使用以下测试图和阈值套件。每个图都是简单、无向、无权的，且无自环。\n\n- 测试图 $1$，$n=8$，阈值 $\\tau=0.5$。边集\n  $E_1 = \\{(0,1),(0,2),(0,3),(1,2),(1,3),(2,3),(4,5),(4,6),(4,7),(5,6),(5,7),(6,7)\\}$。\n- 测试图 $2$，$n=8$，阈值 $\\tau=0.5$。边集\n  $E_2 = E_1 \\cup \\{(3,4)\\}$。\n- 测试图 $3$，$n=5$，阈值 $\\tau=0.3$。边集\n  $E_3 = \\{(0,1),(1,2),(2,0)\\}$，节点 $3$ 和 $4$ 是孤立的。\n\n您的程序必须为每个测试图实现上述所有步骤，并生成单行输出，其中包含三个测试的归一化互信息值。该输出为一个以逗号分隔的浮点数列表，四舍五入到 $6$ 位小数，并用方括号括起来，例如 $[x_1,x_2,x_3]$，其中每个 $x_k$ 是测试 $k$ 的结果，四舍五入到 $6$ 位小数。", "solution": "问题陈述已经过仔细验证，并被确定为有效。它在科学上基于复杂网络理论，通过确定性的算法定义得到了适定的问题设定，并且表述客观。所有必要的数据和约定都已提供，确保了唯一且可验证的解的存在。\n\n该问题要求对三个测试图中的每一个执行四个步骤的分析：1. 计算两两邻域重叠相似度矩阵。2. 基于此相似度矩阵的阈值化进行聚类。3. 使用确定性的标签传播算法（LPA）进行社群检测。4. 使用归一化互信息（NMI）量化所得两个划分之间的一致性。\n\n解决方案按照规定，从第一性原理出发实现每个步骤。\n\n**步骤 1：邻域重叠相似度**\n\n对于一个给定的、有 $n$ 个节点（标签为 $\\{0, 1, \\dots, n-1\\}$）的简单无向图，令节点 $i$ 的开邻域为其相邻节点的集合 $N(i)$。两个不同节点 $i$ 和 $j$ 之间的相似度 $S(i,j)$ 定义为它们邻域的重叠部分相对于它们共同拥有的所有不同邻居总数的比例。这在形式上是其邻域集合的杰卡德指数（Jaccard index）：\n$$\nS(i,j) = \\frac{|N(i) \\cap N(j)|}{|N(i) \\cup N(j)|}\n$$\n问题提供了两个约定：\n1.  如果 $|N(i) \\cup N(j)| = 0$（即 $i$ 和 $j$ 都是孤立节点）且 $i \\neq j$，则相似度为 $S(i,j) = 0$。\n2.  任何节点 $i$ 的自相似度为 $S(i,i) = 1$。\n\n计算一个 $n \\times n$ 的对称相似度矩阵 $S$，其中每个条目 $S_{ij}$ 存储值 $S(i,j)$。\n\n**步骤 2：阈值相似度聚类**\n\n从相似度矩阵 $S$ 中导出节点的划分，我们称之为划分 A。在相同的 $n$ 个节点上构建一个新的“相似度图”。当且仅当节点 $i$ 和 $j$ 之间的相似度 $S(i,j)$ 达到或超过给定的阈值 $\\tau$ 时，就在它们之间放置一条无向边：\n$$\n(i,j) \\in E_{\\text{sim}} \\iff S(i,j) \\ge \\tau\n$$\n然后将簇定义为该相似度图的连通分量。这些分量可以使用标准的图遍历算法（如广度优先搜索 (BFS) 或深度优先搜索 (DFS)）来识别。每个节点被分配一个对应于其所属分量的簇标签。\n\n**步骤 3：标签传播算法 (LPA)**\n\n使用标签传播算法 (LPA) 获得节点的第二个独立划分，即划分 B。该算法采用确定性规则指定，以确保结果唯一。\n- **初始化**：每个节点 $i$ 被分配其自身的索引作为初始标签，$L(i) \\leftarrow i$。\n- **迭代**：算法以扫描的方式进行。在每次扫描中，按节点索引的递增顺序（从 $0$ 到 $n-1$）访问节点。访问节点 $i$ 时，其标签根据其在原图中邻居的标签进行更新。\n- **更新规则**：对于邻域 $N(i)$ 非空的节点 $i$，其新标签变为其邻居 $\\{L(u) : u \\in N(i)\\}$ 中最频繁的标签。此更新是异步的，意味着在当前扫描中更新的节点 $j  i$ 的标签会立即用于节点 $i$ 的更新。\n- **平局打破规则**：如果多个标签在邻域中具有相同的最高频率，则选择数值最小的那个。\n- **孤立节点**：如果 $N(i)$ 为空，$L(i)$ 保持不变。\n- **终止**：当对所有节点的一次完整扫描没有导致任何标签变化时，或在最多 $100$ 次扫描后，过程停止。\n\n所有节点的最终标签 $L(i)$ 定义了划分 B 的社群。\n\n**步骤 4：归一化互信息 (NMI)**\n\n划分 A（来自相似度聚类）和划分 B（来自 LPA）之间的一致性通过归一化互信息 (NMI) 进行量化。这需要用到信息论中的概念。\n\n给定 $n$ 个节点的两个划分 A 和 B，我们首先构建它们的联合概率分布。设 A 中的簇为 $\\{a_k\\}$，B 中的社群为 $\\{b_l\\}$。一个节点属于簇 $a_k$ 的概率是 $P(a_k) = |a_k|/n$，属于社群 $b_l$ 的概率是 $P(b_l) = |b_l|/n$。联合概率为 $P(a_k, b_l) = |a_k \\cap b_l|/n$。\n\n一个划分（例如 A）的香农熵（Shannon entropy）衡量其信息内容或不确定性：\n$$\nH(A) = - \\sum_{k} P(a_k) \\log P(a_k)\n$$\n互信息 $I(A,B)$ 衡量两个划分之间共享的信息：\n$$\nI(A,B) = \\sum_{k} \\sum_{l} P(a_k, b_l) \\log \\frac{P(a_k, b_l)}{P(a_k)P(b_l)}\n$$\nNMI 是互信息通过各个熵的几何平均数进行归一化，将结果缩放到 $[0,1]$ 区间：\n$$\n\\text{NMI}(A,B) = \\frac{I(A,B)}{\\sqrt{H(A)H(B)}}\n$$\n问题为分母为零的情况指定了约定：\n1.  如果 $H(A)=0$ 且 $H(B)=0$（两个划分都是平凡的，即只包含一个组），则 $\\text{NMI} = 1$。\n2.  如果 $H(A)$ 或 $H(B)$ 中恰好有一个为零（一个划分是平凡的，另一个不是），则 $\\text{NMI} = 0$。\n\n每个测试用例的最终结果是此 NMI 值，四舍五入到 $6$ 位小数。", "answer": "```python\nimport numpy as np\n\ndef _calculate_similarity_matrix(adj, n):\n    \"\"\"Computes the Jaccard similarity matrix for node neighborhoods.\"\"\"\n    S = np.zeros((n, n), dtype=float)\n    for i in range(n):\n        S[i, i] = 1.0\n        for j in range(i + 1, n):\n            N_i = adj[i]\n            N_j = adj[j]\n            \n            intersection_size = len(N_i.intersection(N_j))\n            union_size = len(N_i.union(N_j))\n            \n            if union_size == 0:\n                similarity = 0.0\n            else:\n                similarity = intersection_size / union_size\n            \n            S[i, j] = S[j, i] = similarity\n    return S\n\ndef _get_similarity_clusters(S, n, tau):\n    \"\"\"Finds clusters as connected components of the thresholded similarity graph.\"\"\"\n    sim_adj = [set() for _ in range(n)]\n    for i in range(n):\n        for j in range(i + 1, n):\n            if S[i, j] >= tau:\n                sim_adj[i].add(j)\n                sim_adj[j].add(i)\n\n    visited = np.full(n, False, dtype=bool)\n    clusters = np.full(n, -1, dtype=int)\n    cluster_id = 0\n    for i in range(n):\n        if not visited[i]:\n            queue = [i]\n            visited[i] = True\n            while queue:\n                u = queue.pop(0)\n                clusters[u] = cluster_id\n                for v in sim_adj[u]:\n                    if not visited[v]:\n                        visited[v] = True\n                        queue.append(v)\n            cluster_id += 1\n    return clusters\n\ndef _get_most_frequent_label(neighbor_labels):\n    \"\"\"Finds the most frequent label, with tie-breaking by smallest value.\"\"\"\n    if not neighbor_labels:\n        return None\n    \n    counts = {}\n    for label in neighbor_labels:\n        counts[label] = counts.get(label, 0) + 1\n    \n    max_freq = 0\n    # In Python 3.7+ dicts are insertion ordered. No assumptions made here.\n    for count in counts.values():\n        if count > max_freq:\n            max_freq = count\n\n    tied_labels = []\n    for label, count in counts.items():\n        if count == max_freq:\n            tied_labels.append(label)\n    \n    return min(tied_labels)\n\ndef _run_lpa(adj, n):\n    \"\"\"Runs the deterministic Label Propagation Algorithm.\"\"\"\n    labels = np.arange(n)\n    max_sweeps = 100\n    \n    for _ in range(max_sweeps):\n        changed = False\n        for i in range(n):\n            neighbors = adj[i]\n            if not neighbors:\n                continue\n            \n            neighbor_labels = [labels[neighbor] for neighbor in neighbors]\n            new_label = _get_most_frequent_label(neighbor_labels)\n            \n            if labels[i] != new_label:\n                labels[i] = new_label\n                changed = True\n        \n        if not changed:\n            break\n            \n    return labels\n\ndef _calculate_nmi(partition_A, partition_B, n):\n    \"\"\"Computes the Normalized Mutual Information between two partitions.\"\"\"\n    unique_labels_A = np.unique(partition_A)\n    unique_labels_B = np.unique(partition_B)\n    num_clusters_A = len(unique_labels_A)\n    num_clusters_B = len(unique_labels_B)\n    \n    map_A = {label: i for i, label in enumerate(unique_labels_A)}\n    map_B = {label: i for i, label in enumerate(unique_labels_B)}\n\n    contingency = np.zeros((num_clusters_A, num_clusters_B), dtype=float)\n    for i in range(n):\n        a_idx = map_A[partition_A[i]]\n        b_idx = map_B[partition_B[i]]\n        contingency[a_idx, b_idx] += 1\n    \n    p_ab = contingency / n\n    p_a = np.sum(p_ab, axis=1)\n    p_b = np.sum(p_ab, axis=0)\n\n    # Calculate entropies\n    h_a = -np.sum(p_a[p_a > 0] * np.log(p_a[p_a > 0]))\n    h_b = -np.sum(p_b[p_b > 0] * np.log(p_b[p_b > 0]))\n\n    # Handle zero entropy cases as per problem convention\n    is_ha_zero = np.isclose(h_a, 0)\n    is_hb_zero = np.isclose(h_b, 0)\n    \n    if is_ha_zero and is_hb_zero:\n        return 1.0\n    if is_ha_zero or is_hb_zero:\n        return 0.0\n\n    # Calculate mutual information\n    i_ab = 0.0\n    for r in range(num_clusters_A):\n        for c in range(num_clusters_B):\n            if p_ab[r, c] > 0:\n                i_ab += p_ab[r, c] * np.log(p_ab[r, c] / (p_a[r] * p_b[c]))\n    \n    # Calculate NMI\n    nmi = i_ab / np.sqrt(h_a * h_b)\n    return nmi\n\ndef solve_one_case(n, edges, tau):\n    \"\"\"Processes a single test graph through all steps.\"\"\"\n    # Build adjacency list (list of sets for efficient operations)\n    adj = [set() for _ in range(n)]\n    for u, v in edges:\n        adj[u].add(v)\n        adj[v].add(u)\n    \n    # 1. Compute neighborhood-overlap similarity\n    similarity_matrix = _calculate_similarity_matrix(adj, n)\n    \n    # 2. Get clusters from similarity graph\n    partition_A = _get_similarity_clusters(similarity_matrix, n, tau)\n    \n    # 3. Get communities from LPA\n    partition_B = _run_lpa(adj, n)\n    \n    # 4. Quantify consistency with NMI\n    nmi = _calculate_nmi(partition_A, partition_B, n)\n    \n    return nmi\n\ndef solve():\n    \"\"\"Main function to run all test cases and print results.\"\"\"\n    E1 = {(0,1),(0,2),(0,3),(1,2),(1,3),(2,3),(4,5),(4,6),(4,7),(5,6),(5,7),(6,7)}\n    E2 = E1.union({(3,4)})\n    E3 = {(0,1),(1,2),(2,0)}\n\n    test_cases = [\n        (8, E1, 0.5),\n        (8, E2, 0.5),\n        (5, E3, 0.3)\n    ]\n\n    results = []\n    for n, edges, tau in test_cases:\n        result = solve_one_case(n, edges, tau)\n        results.append(f\"{result:.6f}\")\n\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3108222"}, {"introduction": "网络并非静止不变，它们会随时间演化。一个核心问题是：我们能否预测未来哪些新的连接最有可能形成？这就是链接预测任务的核心。本练习将介绍几种经典的链接预测方法，从直观的“朋友的朋友也可能是我的朋友”（共同邻居）到更精细的局部指标（Adamic-Adar），以及基于谱嵌入的全局方法 [@problem_id:3108231]。在时序网络数据上实现并评估这些不同策略，将使你掌握分析和预测网络动态的基本技能，这在社交媒体分析、系统生物学等领域至关重要。", "problem": "给定一个在简单、无向、无权网络上的时序链接预测任务。目标是实现三种链接预测评分函数，并在多个时间快照上，以未来的真实连边为基准，对它们进行性能评测。你的程序必须计算第一个快照中所有可能存在的非连边的排名，并评估每种方法在识别第二个快照中实际出现的连边方面的表现。\n\n推理和实现的基本依据：\n- 使用简单无向图、节点邻居集合和节点度的定义，这些都源自第一个快照的邻接关系。\n- 使用基于拓扑（共同邻居和度感知加权）以及图的谱嵌入得到的节点对之间的相似性分数概念。\n\n仅使用时间 $t=0$ 的第一个快照需要实现的方法：\n- 共同邻居：仅根据候选对在第一个快照中邻居集合交集的大小为其分配一个分数。\n- Adamic–Adar：根据第一个快照中的共同邻居为每个候选对分配一个分数，其中度较大的共同邻居的贡献小于度较小的共同邻居。\n- 基于嵌入的方法：通过取邻接结构对称归一化后的前 $d$ 个特征向量，为第一个快照的节点构建一个 $d$ 维谱嵌入，然后通过其嵌入向量的余弦相似度为节点对评分。\n\n评估协议：\n- 候选对是在时间 $t=0$ 的第一个快照中所有不构成连边的无序节点对 $(u,v)$，其中 $u  v$。\n- 真实正例集 $P$ 是在时间 $t=1$ 实际出现的新连边集合，其大小为 $L=|P|$。\n- 对于每种方法，候选对根据其分数降序排名。平局通过对节点对 $(u,v)$ 进行字典序排序来确定性地打破。\n- 性能通过 top-$L$ 精度来衡量，即排名前 $L$ 的预测中属于真实正例集的比例。\n- 对每个测试案例，将三个精度值四舍五入到 $3$ 位小数。\n\n测试套件：\n- 案例 1：节点集 $\\{0, 1, 2, 3, 4, 5, 6\\}$；时间 $t=0$ 时的边集 $\\{(0, 1), (0, 2), (1, 2), (2, 3), (3, 4), (4, 5)\\}$；时间 $t=1$ 时的新边 $\\{(0, 3), (1, 3), (5, 6)\\}$；嵌入维度 $d=2$。\n- 案例 2：节点集 $\\{0, 1, 2, 3, 4, 5\\}$；时间 $t=0$ 时的边集 $\\{(0, 1), (0, 2), (0, 3), (3, 4), (4, 5)\\}$；时间 $t=1$ 时的新边 $\\{(1, 2), (1, 3), (2, 3)\\}$；嵌入维度 $d=2$。\n- 案例 3：节点集 $\\{0, 1, 2, 3, 4\\}$；时间 $t=0$ 时的边集 $\\{(0, 1)\\}$；时间 $t=1$ 时的新边 $\\{(2, 3)\\}$；嵌入维度 $d=1$。\n\n输出格式：\n您的程序应生成单行输出，其中包含一个方括号括起来的逗号分隔列表，按顺序列出所有测试案例的所有九个精度值。例如 `[cn1, aa1, eb1, cn2, aa2, eb2, cn3, aa3, eb3]`。", "solution": "所提出的问题是计算网络科学中一个明确定义的任务，特别是时序链接预测。它要求在一组网络快照上实现和评估三种标准的链接预测评分函数。该问题具有科学依据、客观性，并为得出唯一解提供了所有必要信息。\n\n问题的核心是基于网络当前的拓扑结构来预测其未来的连接。给定一个在初始时间 $t=0$ 的简单、无向、无权图，由其节点集 $V$ 和连边集 $E_0$ 表示。同时，也给出了在未来时间 $t=1$ 出现的新连边集合 $P$。目标是根据在 $t=1$ 时出现的可能性，对所有在 $t=0$ 时不存在的连边（称为候选对）进行排序。\n\n设时间 $t=0$ 时的图为 $G_0=(V, E_0)$。一个候选对是任意满足 $u  v$ 且 $(u,v) \\notin E_0$ 的无序节点对 $(u,v)$。真实正例集（用 $P$ 表示）包含所有在 $t=1$ 时形成连边的候选对。该集合的大小为 $L = |P|$。我们必须实现三种评分方法，按分数的降序对候选对进行排名，并使用“top-$L$ 精度”来评估每种方法的性能。该指标定义为前 $L$ 个预测链接中属于真实正例集 $P$ 的比例。\n\n需要实现的三种评分方法是：\n\n1.  **共同邻居 (Common Neighbors, CN)**：这是一种基于三元闭包原理的局部相似性指标，该原理假定如果两个节点共享一个共同邻居，它们就更有可能连接。一对节点 $(u,v)$ 的分数是它们在 $G_0$ 中共享的邻居数量：\n    $$S_{CN}(u,v) = |\\Gamma_0(u) \\cap \\Gamma_0(v)|$$\n    其中 $\\Gamma_0(x)$ 是图 $G_0$ 中节点 $x$ 的邻居集合。\n\n2.  **Adamic–Adar (AA)**：该方法通过为自身连接较少的共同邻居分配更多权重，从而改进了共同邻居分数。其直觉是，共享一个低度邻居比共享一个高度中心节点更能表明相似性。分数通过对共同邻居的度的对数的倒数求和来计算：\n    $$S_{AA}(u,v) = \\sum_{z \\in \\Gamma_0(u) \\cap \\Gamma_0(v)} \\frac{1}{\\log k_z}$$\n    其中 $k_z$ 是共同邻居 $z$ 在 $G_0$ 中的度。由于 $u$ 和 $v$ 的任何共同邻居 $z$ 都必须与两者都有连边，因此其度 $k_z$ 至少为 $2$，这确保了 $\\log k_z > 0$。\n\n3.  **基于嵌入的方法 (Embedding-based, EB)**：这是一种全局方法，通过将节点嵌入到低维向量空间中来捕捉网络的结构。其步骤如下：\n    a. 从图 $G_0$ 构建邻接矩阵 $A$ 和对角度矩阵 $D$。\n    b. 计算对称归一化的邻接矩阵，定义为 $A_{sym} = D^{-1/2} A D^{-1/2}$。对于度 $k_i=0$ 的任何节点 $i$， $D^{-1/2}$ 中对应的对角线元素取为 $0$，这导致 $A_{sym}$ 的第 $i$ 行和第 $i$ 列为零。\n    c. 对对称矩阵 $A_{sym}$ 进行特征分解。节点嵌入由与 $d$ 个最大特征值相对应的 $d$ 个特征向量构成。这些特征向量构成一个 $N \\times d$ 的嵌入矩阵的列，其中 $N = |V|$。节点 $u$ 的嵌入是该矩阵的第 $u$ 行，表示为 $\\vec{x}_u$。\n    d. 一对节点 $(u,v)$ 的分数是其嵌入向量的余弦相似度：\n    $$S_{EB}(u,v) = \\frac{\\vec{x}_u \\cdot \\vec{x}_v}{\\|\\vec{x}_u\\| \\|\\vec{x}_v\\|}$$\n    根据问题规范，如果 $\\|\\vec{x}_u\\|$ 或 $\\|\\vec{x}_v\\|$ 为零，则分数定义为 $0$。\n\n对于每种方法，都会生成一个候选对及其分数的列表。然后该列表按分数降序排序。分数上的任何平局都通过按字典序（先按 $u$ 升序，再按 $v$ 升序）对节点对 $(u,v)$ 排序来确定性地打破。此排序列表中的前 $L$ 个对构成了预测结果。然后精度计算如下：\n$$\\text{Precision@L} = \\frac{|\\{\\text{Top } L \\text{ predicted pairs}\\} \\cap P|}{L}$$\n\n将这整个过程应用于提供的三个测试案例中的每一个。最终输出是所有案例中所有方法的精度分数的汇总。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import eigh\n\ndef solve():\n    \"\"\"\n    Computes link prediction precision for three methods on three test cases.\n    \"\"\"\n\n    def solve_case(nodes, edges_t0, new_edges_t1, d):\n        \"\"\"\n        Processes a single test case for link prediction evaluation.\n\n        Args:\n            nodes (list): A list of node identifiers.\n            edges_t0 (list of tuples): Edges in the graph at time t=0.\n            new_edges_t1 (list of tuples): Edges added at time t=1.\n            d (int): The dimension for the spectral embedding.\n\n        Returns:\n            list: A list of three float values representing the precision-at-top-L\n                  for Common Neighbors, Adamic-Adar, and Embedding-based methods.\n        \"\"\"\n        num_nodes = len(nodes)\n\n        # 1. Build graph representations at t=0\n        adj_matrix = np.zeros((num_nodes, num_nodes), dtype=int)\n        adj_list = {i: set() for i in range(num_nodes)}\n        for u, v in edges_t0:\n            adj_matrix[u, v] = 1\n            adj_matrix[v, u] = 1\n            adj_list[u].add(v)\n            adj_list[v].add(u)\n        \n        degrees = np.sum(adj_matrix, axis=1)\n\n        # 2. Identify candidate and ground truth pairs\n        candidate_pairs = []\n        for i in range(num_nodes):\n            for j in range(i + 1, num_nodes):\n                if adj_matrix[i, j] == 0:\n                    candidate_pairs.append((i, j))\n        \n        ground_truth_positives = {tuple(sorted(edge)) for edge in new_edges_t1}\n        L = len(ground_truth_positives)\n        \n        # If L is 0, precision is typically considered 1.0 or undefined.\n        # The problem cases ensure L > 0.\n        if L == 0:\n            return [1.0, 1.0, 1.0]\n\n        precisions = []\n\n        # -- Method 1: Common Neighbors --\n        cn_scores = []\n        for u, v in candidate_pairs:\n            score = float(len(adj_list[u].intersection(adj_list[v])))\n            cn_scores.append((score, u, v))\n\n        cn_scores.sort(key=lambda x: (-x[0], x[1], x[2]))\n        top_L_cn = { (u,v) for score, u, v in cn_scores[:L] }\n        hits_cn = len(top_L_cn.intersection(ground_truth_positives))\n        precisions.append(hits_cn / L)\n\n        # -- Method 2: Adamic-Adar --\n        aa_scores = []\n        for u, v in candidate_pairs:\n            score = 0.0\n            common_neighbors = adj_list[u].intersection(adj_list[v])\n            for z in common_neighbors:\n                # Degree of a common neighbor is at least 2, so log(deg) > 0.\n                score += 1 / np.log(degrees[z])\n            aa_scores.append((score, u, v))\n        \n        aa_scores.sort(key=lambda x: (-x[0], x[1], x[2]))\n        top_L_aa = { (u,v) for score, u, v in aa_scores[:L] }\n        hits_aa = len(top_L_aa.intersection(ground_truth_positives))\n        precisions.append(hits_aa / L)\n\n        # -- Method 3: Embedding-based --\n        eb_scores = []\n        # Calculate embedding only if dimension d is positive\n        if d > 0:\n            D_inv_sqrt_diag = [1/np.sqrt(deg) if deg > 0 else 0 for deg in degrees]\n            D_inv_sqrt = np.diag(D_inv_sqrt_diag)\n            A_sym = D_inv_sqrt @ adj_matrix @ D_inv_sqrt\n            \n            # eigh returns eigenvalues in ascending order.\n            _, eigenvectors = eigh(A_sym)\n            \n            # Take eigenvectors corresponding to the d largest eigenvalues.\n            embedding = eigenvectors[:, -d:]\n\n            for u, v in candidate_pairs:\n                vec_u = embedding[u]\n                vec_v = embedding[v]\n                norm_u = np.linalg.norm(vec_u)\n                norm_v = np.linalg.norm(vec_v)\n                \n                if norm_u == 0 or norm_v == 0:\n                    score = 0.0\n                else:\n                    score = np.dot(vec_u, vec_v) / (norm_u * norm_v)\n                eb_scores.append((score, u, v))\n        else: # If d=0, embedding is trivial, all scores are 0.\n            for u, v in candidate_pairs:\n                eb_scores.append((0.0, u, v))\n\n        eb_scores.sort(key=lambda x: (-x[0], x[1], x[2]))\n        top_L_eb = { (u,v) for score, u, v in eb_scores[:L] }\n        hits_eb = len(top_L_eb.intersection(ground_truth_positives))\n        precisions.append(hits_eb / L)\n        \n        return precisions\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"nodes\": list(range(7)),\n            \"edges_t0\": [(0, 1), (0, 2), (1, 2), (2, 3), (3, 4), (4, 5)],\n            \"new_edges_t1\": [(0, 3), (1, 3), (5, 6)],\n            \"d\": 2\n        },\n        {\n            \"nodes\": list(range(6)),\n            \"edges_t0\": [(0, 1), (0, 2), (0, 3), (3, 4), (4, 5)],\n            \"new_edges_t1\": [(1, 2), (1, 3), (2, 3)],\n            \"d\": 2\n        },\n        {\n            \"nodes\": list(range(5)),\n            \"edges_t0\": [(0, 1)],\n            \"new_edges_t1\": [(2, 3)],\n            \"d\": 1\n        }\n    ]\n\n    all_results = []\n    for case in test_cases:\n        results = solve_case(case[\"nodes\"], case[\"edges_t0\"], case[\"new_edges_t1\"], case[\"d\"])\n        all_results.extend(results)\n\n    # Final print statement in the exact required format.\n    formatted_results = [f\"{x:.3f}\" for x in all_results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3108231"}, {"introduction": "网络是各种传播过程的载体，理解信息、行为甚至疾病如何在网络中传播是复杂网络理论的核心议题。在这个基于仿真的实践中，你将模拟两种不同传播机制之间的竞争：一种是通过单次接触就能轻易传播的“简单”传染病，另一种是需要多个邻居社会强化的“复杂”传染病 [@problem_id:3108279]。通过在不同拓扑结构（如随机网络、小世界网络和无标度网络）上运行此模拟，你将亲身体验网络结构如何调节动态过程，并最终决定哪种传染机制在竞争中胜出。", "problem": "考虑一个社交网络的有限、简单、无向图模型，由邻接矩阵 $A \\in \\{0,1\\}^{n \\times n}$ 表示，其中当且仅当节点 $i$ 和 $j$ 由一条边连接时，$A_{ij} = 1$；并且对于所有 $i$，$A_{ii} = 0$。两种传染过程在同一网络上同时传播：一种是简单传染，另一种是复杂传染。在任何时刻，每个节点都恰好处于以下三种状态之一：易感 ($0$)、已采纳简单传染 ($1$) 或已采纳复杂传染 ($2$)。两种传染都从不相交的随机种子集合开始，并以离散时间步同步演化。\n\n基本定义：\n- 设 $G = (V,E)$ 是一个有 $|V| = n$ 个节点的图。节点 $i$ 的度为 $d_i = \\sum_{j=1}^{n} A_{ij}$。\n- 简单传染沿边进行独立暴露：对于一个易感节点 $i$，如果它有 $k_i^{(S)}$ 个邻居当前已采纳简单传染，那么在下一个时间步中，节点 $i$ 采纳简单传染的概率是 $p_i^{(S)} = 1 - (1 - \\beta)^{k_i^{(S)}}$，其中 $\\beta \\in (0,1)$ 是每条边的传播概率，并假设在当前状态条件下，各条边的影响是独立的。\n- 复杂传染采用社会强化机制：对于一个易感节点 $i$，如果其已采纳复杂传染的邻居比例 $\\phi_i = \\frac{k_i^{(C)}}{d_i}$ 达到或超过一个阈值 $\\tau \\in [0,1]$，那么在下一个时间步中，节点 $i$ 会确定性地采纳复杂传染。对于度为 $d_i=0$ 的节点，除非 $\\tau=0$，否则复杂传染无法被其采纳；在 $\\tau=0$ 的情况下，阈值条件被空洞地满足，该节点确定性地采纳复杂传染。\n- 采纳是持久性的：一旦一个节点采纳了任一种传染，它将在所有后续时间步中保持该状态（不会恢复）。\n- 竞争规则：在任何时间步，一个易感节点可能同时收到两种传染的采纳尝试。如果来自简单传染的尝试（根据 $p_i^{(S)}$）发生，并且复杂传染的阈值条件也同时满足，则采用公平的平局打破机制：节点在两者之间均匀随机选择一种传染进行采纳。\n\n初始化与动力学：\n- 在时间 $t=0$ 时的初始状态通过选择总种子比例为 $s \\in (0,1)$ 的节点来构建：从所有节点中无放回地均匀随机抽取 $\\lceil s n \\rceil$ 个节点，然后将它们尽可能均匀地分割成两个不相交的集合，分别用于简单传染和复杂传染。简单传染的种子被设置为状态 $1$，复杂传染的种子被设置为状态 $2$，所有其他节点均为易感状态 ($0$)。\n- 系统演化 $T$ 个同步时间步。在每个时间步 $t=1,2,\\dots,T$，所有采纳决策都基于 $t-1$ 时刻的状态进行评估，并且状态更新是同时应用的。\n\n网络生成模型：\n- Erdős–Rényi (ER) 随机图 $G(n,p)$：选择 $n$ 和目标平均度 $k$，设置 $p = \\frac{k}{n-1}$，并以概率 $p$ 独立地包含每个潜在的边，同时确保对称性和 $A_{ii} = 0$。\n- Watts–Strogatz (WS) 小世界图：选择 $n$、一个满足 $0  k  n$ 的偶数 $k$ 和重连概率 $r \\in [0,1]$。从一个环状格点开始，其中每个节点连接到其两侧各 $k/2$ 个最近邻。然后，对于每个节点及其 $k/2$ 条顺时针方向的边，以概率 $r$ 将该边重连到一个均匀选择的目标节点，该选择过程避免产生自环和重复边。\n- Barabási–Albert (BA) 无标度图：选择 $n$、一个初始团大小 $m_0$ (满足 $m_0 \\ge 2$) 和一个连接参数 $m$ (满足 $1 \\le m \\le m_0$) 。从一个包含 $m_0$ 个节点的团开始，然后一次添加一个节点，直到达到 $n$ 个节点；每个新节点会与 $m$ 个现有节点建立连接，这些现有节点是根据与其当前度成正比的概率进行无放回选择的。\n\n优势度量：\n- 经过 $T$ 步后，令 $f_S$ 为处于状态 $1$ 的节点最终比例， $f_C$ 为处于状态 $2$ 的节点最终比例。定义一个布尔型优势结果 $D$，当且仅当 $f_C > f_S$ 时 $D = \\text{True}$，否则 $D = \\text{False}$。\n\n任务：\n编写一个完整、可运行的程序，该程序能够构建指定的网络、初始化种子、根据上述规则模拟简单和复杂传染的竞争性传播，并为每个提供的测试用例计算优势结果 $D$。为了保证可复现性，请使用固定的随机种子。您的程序必须生成一行输出，该输出将所有测试用例的结果聚合为一个包含在方括号中的逗号分隔列表。\n\n测试套件：\n为以下参数集提供结果，每个参数集都描述为一个元组，包含网络类型及其参数，后跟传染参数 $(\\beta,\\tau)$、种子比例 $s$ 和时间范围 $T$：\n- 案例 1：ER 图，参数为 $(n=150,k=6)$，传染参数为 $(\\beta=0.2,\\tau=0.5)$，$s=0.06$，$T=30$。\n- 案例 2：WS 图，参数为 $(n=150,k=8,r=0.05)$，传染参数为 $(\\beta=0.15,\\tau=0.4)$，$s=0.06$，$T=30$。\n- 案例 3：BA 图，参数为 $(n=150,m_0=5,m=3)$，传染参数为 $(\\beta=0.1,\\tau=0.6)$，$s=0.04$，$T=30$。\n- 案例 4：WS 环状格点（无重连），参数为 $(n=150,k=10,r=0)$，传染参数为 $(\\beta=0.2,\\tau=0.7)$，$s=0.04$，$T=30$。\n- 案例 5：ER 图，参数为 $(n=150,k=4)$，传染参数为 $(\\beta=0.05,\\tau=0.3)$，$s=0.06$，$T=30$。\n\n最终输出格式：\n您的程序应生成单行输出，其中包含一个方括号括起来的逗号分隔列表（例如 $[result_1,result_2,\\dots,result_5]$），其中每个 $result_i$ 是相应测试用例的布尔值 $D$。", "solution": "问题陈述已经过严格验证，并被确定为有效。它在科学上基于网络科学和计算社会科学的既有模型，定义清晰一致，问题良构，且表述客观。该任务是一个标准的计算模拟问题。因此，我们可以着手提供解决方案。\n\n该解决方案对每个测试用例都涉及一个多步骤的计算过程：网络生成、状态初始化、传染动力学的时间模拟以及最终结果评估。将采用固定的随机种子以确保可复现性。\n\n**1. 算法与方法论框架**\n\n解决方案的核心是离散时间模拟。对于五个测试用例中的每一个，都将执行以下操作序列：\n1.  使用固定种子初始化一个随机数生成器，以确保所有随机过程的结果都是确定性和可复现的。\n2.  根据指定的网络模型（Erdős–Rényi、Watts–Strogatz 或 Barabási–Albert）及其相关参数，构建一个具有 $n$ 个节点的图 $G=(V,E)$。该图由邻接矩阵 $A$ 表示。\n3.  分配节点在时间 $t=0$ 的初始状态。这包括选择一组种子节点，并将其划分给简单传染和复杂传染。\n4.  系统的状态在 $T$ 个同步时间步内演化。在每一步 $t \\in \\{1, \\dots, T\\}$ 中，所有节点的状态都基于 $t-1$ 时刻的状态同时更新。\n5.  经过 $T$ 步后，计算采纳了简单传染的节点最终比例 $f_S$ 和采纳了复杂传染的节点最终比例 $f_C$。\n6.  确定布尔型优势结果 $D = (f_C > f_S)$。\n\n**2. 网络生成算法**\n\n需要三种不同的网络生成模型。设 $n$ 为节点数。\n\n- **Erdős–Rényi (ER) 模型 $G(n,p)$**：给定 $n$ 和目标平均度 $k$，边的概率为 $p = k / (n-1)$。一个 $n \\times n$ 的邻接矩阵 $A$ 初始化为零。对于每一对不同的节点 $(i, j)$ 且 $i  j$，以概率 $p$ 创建一条边。如果创建了边，我们设置 $A_{ij} = A_{ji} = 1$。这确保了图是简单和无向的。\n\n- **Watts–Strogatz (WS) 模型**：给定 $n$、一个偶数平均度 $k$ 和重连概率 $r \\in [0,1]$。\n    1.  **格点生成**：首先构建一个规则的环状格点。每个节点 $i$ 连接到其两侧各 $k/2$ 个最近邻，即节点 $(i \\pm j) \\pmod{n}$（对于 $j=1, \\dots, k/2$）。\n    2.  **边重连**：对于每个节点 $i \\in \\{0, \\dots, n-1\\}$，我们考虑其连接到节点 $j = (i+l) \\pmod{n}$（对于 $l=1, \\dots, k/2$）的 $k/2$ 条边（顺时针方向的边）。对于每条这样的边 $(i, j)$，抽取一个随机数。以概率 $r$ 对该边进行重连。重连包括移除边 $(i, j)$ 并创建一条新边 $(i, m)$，其中 $m$ 是从所有不是 $i$ 且尚未与 $i$ 连接的可能节点中均匀随机选择的。这可以防止自环和多重边。\n\n- **Barabási–Albert (BA) 模型**：给定 $n$、一个初始团大小 $m_0 \\ge 2$ 和一个连接参数 $m$ (满足 $1 \\le m \\le m_0$) 。\n    1.  **初始化**：网络从一个 $m_0$ 个节点的完全图（一个团）开始。\n    2.  **增长与优先连接**：剩下的 $n - m_0$ 个节点被逐一添加。每个新节点 $i$ 连接到 $m$ 个现有节点。新节点连接到现有节点 $j$ 的概率 $\\Pi(j)$ 与节点 $j$ 的度 $d_j$ 成正比：$\\Pi(j) = d_j / \\sum_{l} d_l$，其中求和遍历所有现有节点 $l$。新节点的 $m$ 个邻居是根据此概率分布从现有节点集合中无放回地选择的。\n\n**3. 传染状态的初始化**\n\n对于一个大小为 $n$ 的网络和给定的种子比例 $s \\in (0,1)$：\n1.  种子节点总数计算为 $N_{seed} = \\lceil s \\cdot n \\rceil$。\n2.  从 $n$ 个可用节点中无放回地均匀随机选择一个包含 $N_{seed}$ 个不同节点的集合。\n3.  该集合被尽可能均匀地划分为两个不相交的子集：一个大小为 $N_S = \\lfloor N_{seed} / 2 \\rfloor$ 的简单传染种子集和一个大小为 $N_C = N_{seed} - N_S$ 的复杂传染种子集。\n4.  简单种子集中的节点被分配为状态 $1$，复杂种子集中的节点被分配为状态 $2$，所有其他 $n-N_{seed}$ 个节点被分配为易感状态 $0$。\n\n**4. 动力学模拟**\n\n模拟从 $t=1$ 到 $T$ 以离散、同步的时间步进行。系统在时间 $t$ 的状态完全基于其在时间 $t-1$ 的状态进行计算。\n\n对于每个时间步 $t$：\n1.  识别所有易感节点的集合，$V_{susceptible} = \\{i \\in V \\mid \\text{state}_i(t-1) = 0 \\}$。\n2.  对于每个易感节点 $i \\in V_{susceptible}$：\n    a. 确定其邻居集合 $N(i)$。令其度为 $d_i = |N(i)|$。\n    b. 基于 $t-1$ 时刻的状态，计算其邻居中处于状态 $1$ (简单) 的数量 $k_i^{(S)}$ 和处于状态 $2$ (复杂) 的数量 $k_i^{(C)}$。\n    c. 评估简单传染的采纳潜力。采纳尝试以概率 $p_i^{(S)} = 1 - (1-\\beta)^{k_i^{(S)}}$ 发生。这是一个随机事件。\n    d. 评估复杂传染的采纳条件。这是一个确定性事件。如果 i) $d_i > 0$ 且已采纳复杂传染的邻居比例 $\\phi_i = k_i^{(C)} / d_i$ 满足 $\\phi_i \\ge \\tau$，或者 ii) $d_i = 0$ 且 $\\tau = 0$，则该条件满足。\n    e. 应用竞争规则来确定节点 $i$ 的状态更新：\n        - 如果只有简单传染的尝试成功，节点 $i$ 将被安排采纳状态 $1$。\n        - 如果只有复杂传染的条件满足，节点 $i$ 将被安排采纳状态 $2$。\n        - 如果两个事件都发生，通过一次公平的抛硬币（均匀随机选择）来决定节点 $i$ 将采纳状态 $1$ 还是状态 $2$。\n        - 如果两个事件都未发生，节点 $i$ 保持状态 $0$。\n3.  所有预定的状态变更都会被临时存储。在评估完所有易感节点后，整个网络的状态将同时更新。已经采纳了传染（状态 $1$ 或 $2$）的节点将永久保持在该状态。\n\n**5. 优势度量计算**\n\n在最后一个时间步 $T$ 之后，模拟终止。令 $N_S^{(final)}$ 为状态 $1$ 的节点总数，$N_C^{(final)}$ 为状态 $2$ 的节点总数。\n最终比例为 $f_S = N_S^{(final)} / n$ 和 $f_C = N_C^{(final)} / n$。\n计算布尔型优势结果 $D$ 为 $D = (f_C > f_S)$。对每个测试用例重复此过程，并聚合得到的布尔值。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef generate_er(n, k, rng):\n    \"\"\"Generates an Erdős-Rényi G(n,p) graph.\"\"\"\n    if n = 1:\n        return np.zeros((n, n), dtype=int)\n    p = k / (n - 1)\n    adj = rng.random((n, n))\n    adj = (adj  p).astype(int)\n    adj = np.triu(adj, 1)\n    adj += adj.T\n    return adj\n\ndef generate_ws(n, k, r, rng):\n    \"\"\"Generates a Watts-Strogatz small-world graph.\"\"\"\n    adj = np.zeros((n, n), dtype=int)\n    for i in range(n):\n        for j in range(1, k // 2 + 1):\n            neighbor = (i + j) % n\n            adj[i, neighbor] = 1\n            adj[neighbor, i] = 1\n    \n    if r > 0:\n        for i in range(n):\n            # Consider clockwise edges for rewiring\n            for j in range(1, k // 2 + 1):\n                if rng.random()  r:\n                    neighbor = (i + j) % n\n                    \n                    # Find potential new neighbors: not i and not already connected to i\n                    possible_targets = np.where(adj[i] == 0)[0]\n                    possible_targets = possible_targets[possible_targets != i]\n                    \n                    if len(possible_targets) > 0:\n                        new_neighbor = rng.choice(possible_targets)\n                        \n                        # Rewire: remove old edge, add new one\n                        adj[i, neighbor] = 0\n                        adj[neighbor, i] = 0\n                        adj[i, new_neighbor] = 1\n                        adj[new_neighbor, i] = 1\n    return adj\n\ndef generate_ba(n, m0, m, rng):\n    \"\"\"Generates a Barabási-Albert scale-free graph.\"\"\"\n    if n  m0:\n        raise ValueError(\"n must be greater than or equal to m0\")\n    if m  1 or m > m0:\n        raise ValueError(\"m must be between 1 and m0\")\n\n    # Start with a clique of m0 nodes\n    adj = np.zeros((n, n), dtype=int)\n    adj[:m0, :m0] = 1\n    np.fill_diagonal(adj, 0)\n    \n    degrees = np.sum(adj, axis=1)\n    \n    for i in range(m0, n):\n        # Preferential attachment\n        existing_nodes = np.arange(i)\n        \n        # Calculate probabilities from degrees of existing nodes\n        existing_degrees = degrees[:i]\n        \n        # If all degrees are zero, connect randomly\n        if np.sum(existing_degrees) == 0:\n            probs = None\n        else:\n            probs = existing_degrees / np.sum(existing_degrees)\n        \n        # Select m distinct targets\n        targets = rng.choice(existing_nodes, size=m, replace=False, p=probs)\n        \n        # Add edges\n        for target in targets:\n            adj[i, target] = 1\n            adj[target, i] = 1\n            degrees[i] += 1\n            degrees[target] += 1\n            \n    return adj\n\ndef run_simulation(adj, beta, tau, s, T, rng):\n    \"\"\"Simulates the competitive contagion process.\"\"\"\n    n = adj.shape[0]\n    \n    # 1. Initialization\n    states = np.zeros(n, dtype=int)\n    num_seeds = int(np.ceil(s * n))\n    \n    if num_seeds > 0:\n        seed_nodes = rng.choice(n, size=num_seeds, replace=False)\n        num_simple_seeds = num_seeds // 2\n        \n        simple_seeds = seed_nodes[:num_simple_seeds]\n        complex_seeds = seed_nodes[num_simple_seeds:]\n        \n        states[simple_seeds] = 1\n        states[complex_seeds] = 2\n\n    degrees = np.sum(adj, axis=1)\n\n    # 2. Dynamics\n    for _ in range(T):\n        next_states = states.copy()\n        susceptible_nodes = np.where(states == 0)[0]\n        \n        if len(susceptible_nodes) == 0:\n            break\n        \n        updates = {}\n        \n        for i in susceptible_nodes:\n            neighbors = np.where(adj[i] == 1)[0]\n            if len(neighbors) == 0:\n                k_S, k_C, d_i = 0, 0, 0\n            else:\n                neighbor_states = states[neighbors]\n                k_S = np.sum(neighbor_states == 1)\n                k_C = np.sum(neighbor_states == 2)\n                d_i = degrees[i]\n\n            # Simple contagion evaluation\n            p_S = 1 - (1 - beta)**k_S\n            simple_attempt = rng.random()  p_S\n            \n            # Complex contagion evaluation\n            complex_condition = (d_i > 0 and (k_C / d_i) >= tau) or (d_i == 0 and tau == 0)\n            \n            # Competition rule\n            if simple_attempt and complex_condition:\n                updates[i] = rng.choice([1, 2])\n            elif simple_attempt:\n                updates[i] = 1\n            elif complex_condition:\n                updates[i] = 2\n        \n        for i, new_state in updates.items():\n            next_states[i] = new_state\n        \n        states = next_states\n\n    # 3. Dominance metric\n    f_S = np.sum(states == 1) / n\n    f_C = np.sum(states == 2) / n\n    \n    return f_C > f_S\n    \ndef solve():\n    \"\"\"Main function to run all test cases and print results.\"\"\"\n    # Using a fixed seed for reproducibility of the entire test suite.\n    rng = np.random.default_rng(0)\n    \n    test_cases = [\n        # Case 1: ER graph\n        {'type': 'ER', 'params': {'n': 150, 'k': 6}, \n         'contagion': {'beta': 0.2, 'tau': 0.5}, 's': 0.06, 'T': 30},\n        # Case 2: WS graph\n        {'type': 'WS', 'params': {'n': 150, 'k': 8, 'r': 0.05}, \n         'contagion': {'beta': 0.15, 'tau': 0.4}, 's': 0.06, 'T': 30},\n        # Case 3: BA graph\n        {'type': 'BA', 'params': {'n': 150, 'm0': 5, 'm': 3}, \n         'contagion': {'beta': 0.1, 'tau': 0.6}, 's': 0.04, 'T': 30},\n        # Case 4: WS ring lattice\n        {'type': 'WS', 'params': {'n': 150, 'k': 10, 'r': 0}, \n         'contagion': {'beta': 0.2, 'tau': 0.7}, 's': 0.04, 'T': 30},\n        # Case 5: ER graph\n        {'type': 'ER', 'params': {'n': 150, 'k': 4}, \n         'contagion': {'beta': 0.05, 'tau': 0.3}, 's': 0.06, 'T': 30},\n    ]\n\n    results = []\n    for case in test_cases:\n        if case['type'] == 'ER':\n            adj = generate_er(**case['params'], rng=rng)\n        elif case['type'] == 'WS':\n            adj = generate_ws(**case['params'], rng=rng)\n        elif case['type'] == 'BA':\n            adj = generate_ba(**case['params'], rng=rng)\n        else:\n            raise ValueError(f\"Unknown network type: {case['type']}\")\n\n        dominance = run_simulation(adj, \n                                   beta=case['contagion']['beta'], \n                                   tau=case['contagion']['tau'], \n                                   s=case['s'], T=case['T'], rng=rng)\n        results.append(dominance)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3108279"}]}