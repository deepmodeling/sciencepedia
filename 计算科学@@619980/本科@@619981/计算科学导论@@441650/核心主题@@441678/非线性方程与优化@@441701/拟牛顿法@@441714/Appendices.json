{"hands_on_practices": [{"introduction": "要真正理解一个复杂的算法，最好的方法往往是从最简单的情形入手。本练习将揭示多维拟牛顿法与你可能已经熟悉的一维割线法之间的巧妙联系。通过解决这个问题，你将看到作为拟牛顿法核心的“割线条件”，是如何自然地从寻找导数根的割线近似思想推广而来的。[@problem_id:2195876]", "problem": "考虑最小化一个二次连续可微函数 $f(x)$ 的一维无约束优化问题，其中 $x$ 是一个标量变量。我们采用拟牛顿法来寻找最小值。一般多维情况下的迭代更新规则由下式给出：\n$$x_{k+1} = x_k - (B_k)^{-1} \\nabla f(x_k)$$\n其中 $x_k$ 是最小值的当前估计，$\\nabla f(x_k)$ 是函数在 $x_k$ 处的梯度，而 $B_k$ 是 Hessian 矩阵在 $x_k$ 处的近似。\n\n在我们的一维情况下，梯度 $\\nabla f(x)$ 变为一阶导数 $f'(x)$，Hessian 矩阵变为二阶导数 $f''(x)$。因此，近似值 $B_k$ 是一个标量，我们将其记为 $b_k$。这个标量 $b_k$ 在每一步都会被更新。\n\n假设我们处于第 $k$ 次迭代，已知前一个迭代点 $x_{k-1}$ 和当前迭代点 $x_k$。$b_k$ 的值通过一维割线条件，利用这两点的信息来确定：\n$$b_k (x_k - x_{k-1}) = f'(x_k) - f'(x_{k-1})$$\n\n推导出下一个迭代点 $x_{k+1}$ 的表达式。你的最终答案应该用 $x_k$、$x_{k-1}$ 以及函数在这些点上的一阶导数 $f'$ 来表示。", "solution": "我们考虑一维拟牛顿更新，在标量情况下可写为\n$$\nx_{k+1} = x_k - b_k^{-1} f'(x_k),\n$$\n其中 $b_k$ 是第 $k$ 次迭代时二阶导数的标量近似。一维情况下的割线条件是\n$$\nb_k (x_k - x_{k-1}) = f'(x_k) - f'(x_{k-1}).\n$$\n对该式求解 $b_k$ 可得\n$$\nb_k = \\frac{f'(x_k) - f'(x_{k-1})}{x_k - x_{k-1}},\n$$\n条件是 $x_k \\neq x_{k-1}$ 且 $f'(x_k) \\neq f'(x_{k-1})$。因此，\n$$\nb_k^{-1} = \\frac{x_k - x_{k-1}}{f'(x_k) - f'(x_{k-1})}.\n$$\n将此代入更新公式可得\n$$\nx_{k+1} = x_k - \\frac{x_k - x_{k-1}}{f'(x_k) - f'(x_{k-1})} \\, f'(x_k)\n= x_k - \\frac{f'(x_k)\\,(x_k - x_{k-1})}{f'(x_k) - f'(x_{k-1})}.\n$$\n这就是将割线法应用于求根问题 $f'(x)=0$ 的更新公式。", "answer": "$$\\boxed{x_{k+1} = x_k - \\frac{f'(x_k)\\,(x_k - x_{k-1})}{f'(x_k) - f'(x_{k-1})}}$$", "id": "2195876"}, {"introduction": "任何迭代优化算法都必须有一个起点。本练习聚焦于关键的第一步：确定初始搜索方向。你将探索当我们对Hessian矩阵使用最简单的近似——单位矩阵——时会发生什么，并发现这个选择如何与另一种基础优化技术联系起来。[@problem_id:2195903]", "problem": "采用一种迭代优化算法来寻找双变量函数 $f(x_1, x_2) = \\sin(x_1) + \\cosh(x_2)$ 的一个局部最小值。该算法在点 $x_0 = \\begin{pmatrix} 0 \\\\ \\ln(2) \\end{pmatrix}$ 处进行初始化。在第一步（迭代 $k=0$）中，通过求解线性方程组 $B_0 p_0 = -\\nabla f(x_0)$ 来计算搜索方向 $p_0$，其中 $\\nabla f(x_0)$ 是函数 $f$ 在点 $x_0$ 处的梯度，而 $B_0$ 是 Hessian 矩阵的一个近似。对于此过程，初始近似矩阵选为 $2 \\times 2$ 的单位矩阵 $I$。确定初始搜索方向向量 $p_0$ 的分量。", "solution": "问题要求解初始搜索方向向量 $p_0$，它是线性方程组 $B_0 p_0 = -\\nabla f(x_0)$ 的解。我们可以通过以下三个主要步骤解决此问题：首先，计算函数 $f(x_1, x_2)$ 的梯度；其次，在初始点 $x_0$ 处计算此梯度；最后，求解给定的线性方程组得到 $p_0$。\n\n步骤1：计算函数的梯度。\n函数由 $f(x_1, x_2) = \\sin(x_1) + \\cosh(x_2)$ 给出。\n$f$ 的梯度（记为 $\\nabla f$）是其偏导数组成的向量：\n$$\n\\nabla f(x_1, x_2) = \\begin{pmatrix} \\frac{\\partial f}{\\partial x_1} \\\\ \\frac{\\partial f}{\\partial x_2} \\end{pmatrix}\n$$\n关于 $x_1$ 的偏导数是：\n$$\n\\frac{\\partial f}{\\partial x_1} = \\frac{\\partial}{\\partial x_1} (\\sin(x_1) + \\cosh(x_2)) = \\cos(x_1)\n$$\n关于 $x_2$ 的偏导数是：\n$$\n\\frac{\\partial f}{\\partial x_2} = \\frac{\\partial}{\\partial x_2} (\\sin(x_1) + \\cosh(x_2)) = \\sinh(x_2)\n$$\n因此，梯度向量是：\n$$\n\\nabla f(x_1, x_2) = \\begin{pmatrix} \\cos(x_1) \\\\ \\sinh(x_2) \\end{pmatrix}\n$$\n\n步骤2：在初始点 $x_0$ 处计算梯度。\n初始点为 $x_0 = \\begin{pmatrix} 0 \\\\ \\ln(2) \\end{pmatrix}$。我们将这些值代入梯度表达式：\n$$\n\\nabla f(x_0) = \\nabla f(0, \\ln(2)) = \\begin{pmatrix} \\cos(0) \\\\ \\sinh(\\ln(2)) \\end{pmatrix}\n$$\n我们计算每个分量。0的余弦值为：\n$$\n\\cos(0) = 1\n$$\n双曲正弦函数定义为 $\\sinh(y) = \\frac{\\exp(y) - \\exp(-y)}{2}$。对于 $y = \\ln(2)$：\n$$\n\\sinh(\\ln(2)) = \\frac{\\exp(\\ln(2)) - \\exp(-\\ln(2))}{2} = \\frac{2 - \\exp(\\ln(2^{-1}))}{2} = \\frac{2 - \\frac{1}{2}}{2} = \\frac{\\frac{3}{2}}{2} = \\frac{3}{4}\n$$\n所以，在初始点的梯度向量是：\n$$\n\\nabla f(x_0) = \\begin{pmatrix} 1 \\\\ \\frac{3}{4} \\end{pmatrix}\n$$\n\n步骤3：求解线性方程组以得到 $p_0$。\n需要求解的系统是 $B_0 p_0 = -\\nabla f(x_0)$。我们已知 $B_0$ 是 $2 \\times 2$ 的单位矩阵 $I = \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix}$。\n该系统变为：\n$$\nI p_0 = -\\nabla f(x_0)\n$$\n任何向量乘以单位矩阵都保持不变，所以 $p_0 = -\\nabla f(x_0)$。\n代入我们找到的梯度值：\n$$\np_0 = - \\begin{pmatrix} 1 \\\\ \\frac{3}{4} \\end{pmatrix} = \\begin{pmatrix} -1 \\\\ -\\frac{3}{4} \\end{pmatrix}\n$$\n因此，初始搜索方向向量 $p_0$ 的分量是 $-1$ 和 $-\\frac{3}{4}$。", "answer": "$$\\boxed{\\begin{pmatrix} -1 \\\\ -\\frac{3}{4} \\end{pmatrix}}$$", "id": "2195903"}, {"introduction": "BFGS算法的威力源于其在每一步智能地更新Hessian矩阵近似的公式。虽然完整的公式可能看起来令人生畏，但它是由更简单的秩一矩阵校正项构建而成的。本练习通过让你计算其中一个关键组成部分来剖析它，让你对更新背后的力学原理有一个具体的感受，并体会曲率信息是如何被逐步整合的。[@problem_id:2195881]", "problem": "在数值优化领域，Broyden–Fletcher–Goldfarb–Shanno (BFGS) 算法是一种流行的拟牛顿法，用于解决无约束非线性优化问题。在每次迭代中，该算法会近似目标函数的海森矩阵。从一个初始近似 $B_k$ 开始，下一个近似 $B_{k+1}$ 通过以下更新公式计算：\n$$B_{k+1} = B_k + \\frac{y_k y_k^T}{y_k^T s_k} - \\frac{B_k s_k s_k^T B_k}{s_k^T B_k s_k}$$\n其中 $s_k$ 是所取的步长向量，$y_k$ 是梯度的变化量。此更新包含将两个秩一矩阵加到当前的近似 $B_k$ 上。\n\n考虑 BFGS 算法的一次迭代，其中当前的海森矩阵近似是 $2 \\times 2$ 的单位矩阵，$B_k = I$。步长向量为 $s_k = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$，梯度变化向量为 $y_k = \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix}$。\n\n计算 $B_{k+1}$ 的负秩一更新项，该项由表达式 $-\\frac{B_k s_k s_k^T B_k}{s_k^T B_k s_k}$ 给出。您的最终答案应该是一个 $2 \\times 2$ 的矩阵。", "solution": "我们需要计算负秩一更新项\n$$-\\frac{B_{k}s_{k}s_{k}^{T}B_{k}}{s_{k}^{T}B_{k}s_{k}}$$\n其中 $B_{k}=I$，$s_{k}=\\begin{pmatrix}1 \\\\ 0\\end{pmatrix}$，而 $y_{k}$ 已给出，但此项的计算中不需要。\n\n首先，利用 $B_{k}=I$ 这一事实来简化分子：\n$$B_{k}s_{k} = Is_{k} = s_{k}, \\quad \\text{以及} \\quad B_{k}s_{k}s_{k}^{T}B_{k} = s_{k}s_{k}^{T}。$$\n\n接下来，计算分母：\n$$s_{k}^{T}B_{k}s_{k} = s_{k}^{T}Is_{k} = s_{k}^{T}s_{k}。$$\n当 $s_{k}=\\begin{pmatrix}1 \\\\ 0\\end{pmatrix}$ 时，我们有\n$$s_{k}^{T}s_{k} = 1^{2} + 0^{2} = 1。$$\n\n因此，负秩一更新项变为\n$$-\\frac{B_{k}s_{k}s_{k}^{T}B_{k}}{s_{k}^{T}B_{k}s_{k}} = -\\frac{s_{k}s_{k}^{T}}{s_{k}^{T}s_{k}} = -s_{k}s_{k}^{T}。$$\n\n显式地计算 $s_{k}s_{k}^{T}$：\n$$s_{k}s_{k}^{T} = \\begin{pmatrix}1 \\\\ 0\\end{pmatrix}\\begin{pmatrix}1  0\\end{pmatrix} = \\begin{pmatrix}1  0 \\\\ 0  0\\end{pmatrix}。$$\n\n因此，负秩一更新项是\n$$-\\begin{pmatrix}1  0 \\\\ 0  0\\end{pmatrix} = \\begin{pmatrix}-1  0 \\\\ 0  0\\end{pmatrix}。$$", "answer": "$$\\boxed{\\begin{pmatrix}-1 & 0 \\\\ 0 & 0\\end{pmatrix}}$$", "id": "2195881"}]}