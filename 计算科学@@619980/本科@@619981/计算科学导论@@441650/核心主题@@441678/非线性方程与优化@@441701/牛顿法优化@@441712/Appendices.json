{"hands_on_practices": [{"introduction": "为了将理论付诸实践，我们首先通过一个基础练习来手动计算牛顿法的一次迭代。这个练习旨在阐明该方法的核心机制：如何利用函数的梯度（最速上升方向）和Hessian矩阵（曲率信息）来构建一个局部二次模型，并找到该模型的最小值。通过完成这个计算 [@problem_id:2190727]，您将对牛顿步 (Newton step) 的几何和代数意义有更深刻的理解。", "problem": "一个工程师团队正在致力于最小化一个新型机械臂的运营成本。该成本由一个关于两个无量纲设计参数 $x$ 和 $y$ 的函数 $f(x, y)$ 建模。该函数为：\n$$f(x, y) = (y - x^2)^2 + (1-x)^2$$\n为找到最小化此成本的最优参数，该团队决定使用牛顿法进行无约束优化。他们从初始设计猜测 $(x_0, y_0) = (2, 3)$ 开始。\n\n计算应用一步牛顿法后得到的下一次迭代的坐标 $(x_1, y_1)$。将你的答案表示为一个由精确分数组成的有序对。", "solution": "对于两个变量的无约束优化问题，牛顿法的一步迭代通过以下公式更新迭代点 $\\mathbf{z} = (x, y)$：\n$$\n\\mathbf{z}_{k+1} = \\mathbf{z}_{k} - H(\\mathbf{z}_{k})^{-1} \\nabla f(\\mathbf{z}_{k}),\n$$\n其中 $\\nabla f$ 是 $f$ 的梯度，而 $H$ 是 $f$ 的海森矩阵。\n\n给定 $f(x, y) = (y - x^{2})^{2} + (1 - x)^{2}$，计算其梯度：\n$$\n\\frac{\\partial f}{\\partial x} = 2(y - x^{2})(-2x) + 2(1 - x)(-1) = -4xy + 4x^{3} - 2 + 2x,\n$$\n$$\n\\frac{\\partial f}{\\partial y} = 2(y - x^{2}) = 2y - 2x^{2}.\n$$\n因此，\n$$\n\\nabla f(x, y) = \\begin{pmatrix} -4xy + 4x^{3} - 2 + 2x \\\\ 2y - 2x^{2} \\end{pmatrix}.\n$$\n\n计算海森矩阵：\n$$\n\\frac{\\partial^{2} f}{\\partial x^{2}} = \\frac{\\partial}{\\partial x}\\left(-4xy + 4x^{3} - 2 + 2x\\right) = -4y + 12x^{2} + 2,\n$$\n$$\n\\frac{\\partial^{2} f}{\\partial x \\partial y} = \\frac{\\partial}{\\partial y}\\left(-4xy + 4x^{3} - 2 + 2x\\right) = -4x,\n$$\n$$\n\\frac{\\partial^{2} f}{\\partial y^{2}} = \\frac{\\partial}{\\partial y}\\left(2y - 2x^{2}\\right) = 2.\n$$\n所以，\n$$\nH(x, y) = \\begin{pmatrix} 12x^{2} - 4y + 2 & -4x \\\\ -4x & 2 \\end{pmatrix}.\n$$\n\n在点 $(x_{0}, y_{0}) = (2, 3)$ 处求值：\n$$\n\\nabla f(2, 3) = \\begin{pmatrix} -4\\cdot 2 \\cdot 3 + 4\\cdot 2^{3} - 2 + 2\\cdot 2 \\\\ 2\\cdot 3 - 2\\cdot 2^{2} \\end{pmatrix} = \\begin{pmatrix} 10 \\\\ -2 \\end{pmatrix},\n$$\n$$\nH(2, 3) = \\begin{pmatrix} 12\\cdot 2^{2} - 4\\cdot 3 + 2 & -4\\cdot 2 \\\\ -4\\cdot 2 & 2 \\end{pmatrix} = \\begin{pmatrix} 38 & -8 \\\\ -8 & 2 \\end{pmatrix}.\n$$\n\n牛顿步长 $\\mathbf{s}$ 求解方程 $H(2, 3)\\,\\mathbf{s} = -\\nabla f(2, 3)$：\n$$\n\\begin{pmatrix} 38 & -8 \\\\ -8 & 2 \\end{pmatrix} \\begin{pmatrix} s_{1} \\\\ s_{2} \\end{pmatrix} = \\begin{pmatrix} -10 \\\\ 2 \\end{pmatrix}.\n$$\n从该方程组\n$$\n38 s_{1} - 8 s_{2} = -10, \\quad -8 s_{1} + 2 s_{2} = 2,\n$$\n将第二个方程乘以 $4$ 得到 $-32 s_{1} + 8 s_{2} = 8$，然后加到第一个方程上得到 $6 s_{1} = -2$，因此 $s_{1} = -\\frac{1}{3}$。将其代入 $-8 s_{1} + 2 s_{2} = 2$ 得到 $8/3 + 2 s_{2} = 2$，所以 $2 s_{2} = -2/3$ 且 $s_{2} = -\\frac{1}{3}$。\n\n因此，\n$$\n(x_{1}, y_{1}) = (x_{0}, y_{0}) + \\mathbf{s} = \\left(2 - \\frac{1}{3},\\, 3 - \\frac{1}{3}\\right) = \\left(\\frac{5}{3},\\, \\frac{8}{3}\\right).\n$$", "answer": "$$\\boxed{\\left(\\frac{5}{3}, \\frac{8}{3}\\right)}$$", "id": "2190727"}, {"introduction": "虽然牛顿法在理想条件下收敛速度很快，但它有一个关键的弱点：当Hessian矩阵是奇异或接近奇异时，该方法可能会失败。这个动手实践 [@problem_id:3164396] 通过一个具体的编码任务，让您探索如何处理这个问题。您将比较“纯”牛顿法、正则化（或阻尼）牛顿法以及混合策略，从而理解在实践中如何构建一个在更广泛问题上都表现稳健的优化算法。", "problem": "要求您在 Hessian 矩阵在最小值点处奇异的情景下，实现并分析用于无约束优化的牛顿法。分析和实现必须基于多变量标量函数的二阶泰勒展开，以及牛顿步作为该二次模型最小值点的定义，并在 Hessian 矩阵不可逆时进行正则化处理。\n\n从以下基本原理出发：\n- 函数 $f : \\mathbb{R}^n \\to \\mathbb{R}$ 在点 $x \\in \\mathbb{R}^n$ 附近的二阶泰勒展开为\n$$\nf(x + s) \\approx f(x) + \\nabla f(x)^\\top s + \\frac{1}{2} s^\\top \\nabla^2 f(x) s,\n$$\n其中 $\\nabla f(x)$ 是梯度，$\\nabla^2 f(x)$ 是 Hessian 矩阵。\n- 对二次模型关于 $s$ 求最小值得到了以下形式的线性系统\n$$\n\\nabla^2 f(x) \\, s = - \\nabla f(x).\n$$\n当 Hessian 矩阵是奇异或不定时，该系统可能无解，或者求出的步长可能不是一个下降方向。\n\n任务：\n- 构建并使用函数\n$$\nf(x,y) = x^4 + y^2,\n$$\n其 Hessian 矩阵在最小值点 $(x^\\star,y^\\star) = (0,0)$ 处是奇异的。实现以下三种算法：\n    $1.$ 朴素牛顿法 (Vanilla Newton)：尝试通过求解 $\\nabla^2 f(x_k) s_k = -\\nabla f(x_k)$ 来计算牛顿步，并更新 $x_{k+1} = x_k + s_k$。如果在任何迭代中线性系统是奇异的，则声明失败并返回 $+\\infty$ 作为最终目标值。\n    $2.$ 阻尼牛顿法 (Damped Newton, Tikhonov 正则化)：使用一个固定的阻尼参数 $\\lambda > 0$，由 $(\\nabla^2 f(x_k) + \\lambda I) s_k = -\\nabla f(x_k)$ 计算 $s_k$，并更新 $x_{k+1} = x_k + s_k$。\n    $3.$ 混合回退法 (Hybrid fallback)：首先尝试朴素牛顿步。如果线性系统是奇异的，或者计算出的步长 $s_k$ 不是下降方向 (即 $\\nabla f(x_k)^\\top s_k \\ge 0$)，则改为采用最速下降步 $p_k = -\\nabla f(x_k)$，并使用满足 Armijo 下降条件的回溯线搜索。\n\n必须实现的具体要求：\n- 使用函数 $f(x,y) = x^4 + y^2$，其梯度和 Hessian 矩阵为\n$$\n\\nabla f(x,y) = \\begin{bmatrix} 4 x^3 \\\\ 2 y \\end{bmatrix}, \\qquad\n\\nabla^2 f(x,y) = \\begin{bmatrix} 12 x^2 & 0 \\\\ 0 & 2 \\end{bmatrix}.\n$$\n- 每种算法最多运行 $N$ 次迭代，其中 $N = 10$，如果 $\\|\\nabla f(x_k,y_k)\\|_2 \\le \\varepsilon$ 则提前停止，其中 $\\varepsilon = 10^{-12}$。\n- 对于阻尼牛顿法，使用固定的 $\\lambda = 10^{-3}$。\n- 对于混合回退法，使用回溯参数 $c = 10^{-4}$ 和 $\\rho = 1/2$，初始步长为 $\\alpha = 1$，然后不断缩减 $\\alpha \\leftarrow \\rho \\alpha$，直到满足 Armijo 条件\n$$\nf(x_k + \\alpha p_k) \\le f(x_k) + c \\alpha \\nabla f(x_k)^\\top p_k\n$$\n，或者直到 $\\alpha$ 在数值上变得可以忽略不计。\n- 如果朴素牛顿法在任何迭代中遇到奇异线性系统，它必须为该测试用例返回 $+\\infty$ 作为最终目标值。其他两种方法必须保持稳健并返回一个有限的目标值。\n\n测试套件：\n将所有三种方法应用于上述函数 $f(x,y)$ 的以下初始条件：\n- 情况 1：$(x_0,y_0) = (1, 1)$。\n- 情况 2：$(x_0,y_0) = (0, 1)$。\n- 情况 3：$(x_0,y_0) = (10^{-8}, 10^{-8})$。\n\n答案规格：\n- 对于每种情况，在使用上述规则运行三种方法后，报告最终的目标值 $f(x_{\\text{final}},y_{\\text{final}})$，并按以下固定顺序排列：\n    [ 朴素牛顿法最终值, 阻尼牛顿法最终值, 混合回退法最终值 ]。\n- 将所有三种情况的结果按情况 1、情况 2、情况 3 的顺序汇总到一个长度为 9 的扁平列表中。\n- 如果某个方法根据指定规则失败，则在该位置输出 $+\\infty$。\n- 最终输出格式：您的程序应生成单行输出，其中包含用方括号括起来的逗号分隔列表形式的结果（例如，$[r_1,r_2,\\dots,r_9]$）。不涉及单位。所有数字必须以标准实数形式打印；如果遇到，则 $+\\infty$ 可作为浮点无穷大值。", "solution": "用户提供的问题是有效的。这是一个在数值优化（计算科学的一个子领域）中适定且具有科学依据的问题。所有必需的参数、函数和初始条件都已指定，任务是客观且可验证的。\n\n目标是使用牛顿法的三种变体来寻找函数 $f(x,y) = x^4 + y^2$ 的最小值。全局最小值点位于 $(x^\\star, y^\\star) = (0,0)$，其中 $f(0,0)=0$。梯度 $\\nabla f(x,y)$ 和 Hessian 矩阵 $\\nabla^2 f(x,y)$ 由下式给出：\n$$\n\\nabla f(x,y) = \\begin{bmatrix} 4 x^3 \\\\ 2 y \\end{bmatrix}, \\qquad\n\\nabla^2 f(x,y) = \\begin{bmatrix} 12 x^2 & 0 \\\\ 0 & 2 \\end{bmatrix}.\n$$\n在最小值点 $(0,0)$ 处，Hessian 矩阵为 $\\nabla^2 f(0,0) = \\begin{bmatrix} 0 & 0 \\\\ 0 & 2 \\end{bmatrix}$，该矩阵是奇异的。这种奇异性是该问题旨在测试的核心挑战。\n\n用于优化的牛顿法是一种迭代算法，它在当前迭代点 $x_k$ 处用一个二次模型来近似函数 $f$，然后移动到该模型的最小值点。围绕 $x_k$ 的二阶泰勒展开为步长 $s$ 提供了这个模型：\n$$\nf(x_k + s) \\approx m_k(s) = f(x_k) + \\nabla f(x_k)^\\top s + \\frac{1}{2} s^\\top \\nabla^2 f(x_k) s.\n$$\n为了找到 $m_k(s)$ 的最小值，我们将其关于 $s$ 的梯度设为零：\n$$\n\\nabla_s m_k(s) = \\nabla f(x_k) + \\nabla^2 f(x_k) s = 0.\n$$\n这就得到了牛顿系统，这是一个关于步长 $s_k$ 的线性方程组：\n$$\n\\nabla^2 f(x_k) s_k = - \\nabla f(x_k).\n$$\n下一个迭代点则为 $x_{k+1} = x_k + s_k$。三种指定的算法以不同的方式处理 Hessian 矩阵 $\\nabla^2 f(x_k)$ 的潜在奇异性。\n\n### 方法 1：朴素牛顿法\n这是该方法的纯粹形式。更新步长 $s_k = [s_x, s_y]^\\top$ 是通过直接求解牛顿系统来计算的：\n$$\n\\begin{bmatrix} 12 x_k^2 & 0 \\\\ 0 & 2 \\end{bmatrix} \\begin{bmatrix} s_x \\\\ s_y \\end{bmatrix} = - \\begin{bmatrix} 4 x_k^3 \\\\ 2 y_k \\end{bmatrix}.\n$$\nHessian 矩阵是奇异的，当且仅当其行列式为零，对于此对角矩阵，这意味着 $24x_k^2 = 0$，即 $x_k=0$。如果 $x_k \\neq 0$，该系统有唯一解：\n$s_x = - \\frac{4x_k^3}{12x_k^2} = -\\frac{1}{3}x_k$\n$s_y = - \\frac{2y_k}{2} = -y_k$\n更新后的值为 $x_{k+1} = x_k - \\frac{1}{3}x_k = \\frac{2}{3}x_k$ 和 $y_{k+1} = y_k - y_k = 0$。\n该方法在每一步将 $x$ 分量减少 $2/3$ 的因子，并在第一步将 $y$ 分量设为零。如果任何迭代点有 $x_k=0$，系统就是奇异的，算法按规定失败，返回 $+\\infty$。\n\n### 方法 2：阻尼牛顿法 (Tikhonov 正则化)\n该方法修改 Hessian 矩阵以确保其始终可逆且正定。它求解一个正则化系统：\n$$\n(\\nabla^2 f(x_k) + \\lambda I) s_k = - \\nabla f(x_k),\n$$\n其中 $\\lambda > 0$ 是一个阻尼参数，$I$ 是单位矩阵。新矩阵 $\\nabla^2 f(x_k) + \\lambda I$ 的特征值为 $\\mu_i + \\lambda$，其中 $\\mu_i$ 是 $\\nabla^2 f(x_k)$ 的特征值。对于我们的问题，正则化 Hessian 矩阵的特征值为 $12x_k^2 + \\lambda$ 和 $2+\\lambda$。由于 $\\lambda=10^{-3}>0$，两者都严格为正，保证了 $s_k$ 有唯一解。\n$$\n\\begin{bmatrix} 12 x_k^2 + \\lambda & 0 \\\\ 0 & 2 + \\lambda \\end{bmatrix} \\begin{bmatrix} s_x \\\\ s_y \\end{bmatrix} = - \\begin{bmatrix} 4 x_k^3 \\\\ 2 y_k \\end{bmatrix}.\n$$\n解为：\n$s_x = - \\frac{4x_k^3}{12x_k^2 + \\lambda}$ 且 $s_y = - \\frac{2y_k}{2 + \\lambda}$。\n更新后的值为 $x_{k+1} = x_k + s_x$ 且 $y_{k+1} = y_k + s_y = y_k \\left(1 - \\frac{2}{2+\\lambda}\\right) = y_k \\frac{\\lambda}{2+\\lambda}$。\n该方法对奇异性是稳健的。当 $x_k$ 很大时，其步长几乎与朴素牛顿步相同。当 $x_k$ 很小时，对 $x$ 的更新变为 $x_{k+1} \\approx x_k - (4/\\lambda)x_k^3$，这表明在解附近收敛速度较慢（线性收敛）。\n\n### 方法 3：混合回退法\n该方法尝试使用快速收敛的朴素牛顿步，但在需要时回退到更稳健的方法。\n1.  **尝试牛顿步**：通过求解 $\\nabla^2 f(x_k) s_k = -\\nabla f(x_k)$ 来计算 $s_k$。这仅在 $x_k \\neq 0$ 时才可能。如果找到了解 $s_k$，则会进行检查以确保它是一个下降方向，即 $\\nabla f(x_k)^\\top s_k < 0$。对于我们的函数，如果 $x_k \\neq 0$，牛顿步 $s_k = [-x_k/3, -y_k]^\\top$ 得到 $\\nabla f(x_k)^\\top s_k = [4x_k^3, 2y_k] \\cdot [-x_k/3, -y_k] = -\\frac{4}{3}x_k^4 - 2y_k^2$。对于任何 $(x_k, y_k) \\neq (0,0)$，此值都是严格为负的。因此，回退仅由 Hessian 矩阵的奇异性触发，这发生在 $x_k=0$ 时。\n2.  **回退**：如果 $x_k=0$，算法采用最速下降步，$p_k = -\\nabla f(x_k)$。步长 $\\alpha$ 由回溯线搜索确定，以满足 Armijo 条件：\n    $$\n    f(x_k + \\alpha p_k) \\le f(x_k) + c \\alpha \\nabla f(x_k)^\\top p_k.\n    $$\n    如果一个迭代点是 $(0, y_k)$ 且 $y_k \\neq 0$，搜索方向为 $p_k = -[0, 2y_k]^\\top = [0, -2y_k]^\\top$。Armijo 条件变为 $f(0, y_k - 2\\alpha y_k) \\le f(0, y_k) + c \\alpha (-4y_k^2)$，可简化为 $\\alpha \\le 1-c$。当 $c = 10^{-4}$ 时，初始步长 $\\alpha=1$ 会失败，但下一次尝试 $\\alpha = \\rho \\alpha = 1/2$ 会成功。更新结果为 $(0, y_k) + \\frac{1}{2}(0, -2y_k) = (0,0)$，这使得从 $y$ 轴上任何一点（原点除外）出发，都能在一步内收敛到精确的最小值点。\n\n### 测试用例分析摘要\n- **情况 1：$(1, 1)$** - 由于 $x_0 \\neq 0$，所有三种方法在最初几步中的行为将类似。朴素牛顿法和混合法将完全相同。阻尼牛顿法会略有不同，但会遵循相似的轨迹。所有方法都将收敛。\n- **情况 2：$(0, 1)$** - 朴素牛顿法将立即失败，因为 $x_0=0$ 导致 Hessian 矩阵奇异。阻尼牛顿法将继续进行，保持 $x_k=0$ 并使 $y_k$ 收敛到 $0$。混合法将检测到奇异的 Hessian 矩阵，切换到最速下降法，并在一步内收敛到 $(0,0)$。\n- **情况 3：$(10^{-8}, 10^{-8})$** - 由于 $x_0 \\neq 0$，朴素牛顿法和混合法将执行一次牛顿步。梯度将在一步之后变得小于容差 $\\varepsilon=10^{-12}$，因此它们会迅速终止。阻尼牛顿法也会执行一步，但由于 $x_0$ 相对于 $\\sqrt{\\lambda}$ 很小，其 $x$ 分量的收敛速度会减慢，而 $y$ 分量会快速收敛。它可能需要比其他两种方法更多的迭代次数才能满足梯度容差。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and compares three variants of Newton's method for unconstrained optimization on\n    the function f(x,y) = x^4 + y^2, which has a singular Hessian at its minimum.\n    \"\"\"\n\n    # --- Problem Specifications ---\n    N = 10\n    eps = 1e-12\n    lam = 1e-3  # Damping parameter for Method 2\n    c = 1e-4    # Armijo condition parameter for Method 3\n    rho = 0.5   # Backtracking reduction factor for Method 3\n\n    # --- Test Cases ---\n    test_cases = [\n        np.array([1.0, 1.0]),\n        np.array([0.0, 1.0]),\n        np.array([1e-8, 1e-8])\n    ]\n\n    # --- Objective Function and its Derivatives ---\n    def f(x_vec):\n        \"\"\"The objective function f(x, y) = x^4 + y^2.\"\"\"\n        x, y = x_vec\n        return x**4 + y**2\n\n    def grad_f(x_vec):\n        \"\"\"The gradient of f(x, y).\"\"\"\n        x, y = x_vec\n        return np.array([4 * x**3, 2 * y])\n\n    def hess_f(x_vec):\n        \"\"\"The Hessian of f(x, y).\"\"\"\n        x, y = x_vec\n        return np.array([[12 * x**2, 0.0], [0.0, 2.0]])\n\n    # --- Algorithm Implementations ---\n\n    def vanilla_newton(x0, max_iter, tol):\n        \"\"\"Algorithm 1: Vanilla Newton's method.\"\"\"\n        x_k = np.copy(x0).astype(float)\n        for _ in range(max_iter):\n            g = grad_f(x_k)\n            if np.linalg.norm(g) = tol:\n                break\n            \n            H = hess_f(x_k)\n            # Check for singularity using a tolerance. It's singular if x=0.\n            if abs(H[0, 0])  1e-30:\n                return np.inf\n\n            try:\n                s_k = np.linalg.solve(H, -g)\n                x_k += s_k\n            except np.linalg.LinAlgError:\n                return np.inf\n                \n        return f(x_k)\n\n    def damped_newton(x0, max_iter, tol, lambda_val):\n        \"\"\"Algorithm 2: Damped Newton's method (Tikhonov regularization).\"\"\"\n        x_k = np.copy(x0).astype(float)\n        I = np.identity(2)\n        for _ in range(max_iter):\n            g = grad_f(x_k)\n            if np.linalg.norm(g) = tol:\n                break\n            \n            H = hess_f(x_k)\n            H_reg = H + lambda_val * I\n            \n            s_k = np.linalg.solve(H_reg, -g)\n            x_k += s_k\n            \n        return f(x_k)\n\n    def hybrid_fallback(x0, max_iter, tol, c_bt, rho_bt):\n        \"\"\"Algorithm 3: Hybrid Newton with steepest descent fallback.\"\"\"\n        x_k = np.copy(x0).astype(float)\n        for _ in range(max_iter):\n            g = grad_f(x_k)\n            if np.linalg.norm(g) = tol:\n                break\n            \n            H = hess_f(x_k)\n            \n            use_fallback = False\n            # Check for singularity\n            if abs(H[0, 0])  1e-30:\n                use_fallback = True\n            else:\n                try:\n                    s_k = np.linalg.solve(H, -g)\n                    # Check for descent direction\n                    if g.T @ s_k >= 0:\n                        use_fallback = True\n                except np.linalg.LinAlgError:\n                    use_fallback = True\n\n            if not use_fallback:\n                # Use Newton step\n                x_k += s_k\n            else:\n                # Fallback to steepest descent with backtracking line search\n                p_k = -g\n                alpha = 1.0\n                fk = f(x_k)\n                gTp = g.T @ p_k  # This is -norm(g)**2\n                \n                # Armijo condition backtracking loop\n                while f(x_k + alpha * p_k) > fk + c_bt * alpha * gTp:\n                    alpha *= rho_bt\n                    if alpha  1e-16: # Safeguard for numerical precision\n                        break\n                \n                x_k += alpha * p_k\n                \n        return f(x_k)\n\n    # --- Main Execution Logic ---\n    results = []\n    for x_initial in test_cases:\n        # Vanilla Newton\n        res1 = vanilla_newton(x_initial, N, eps)\n        results.append(res1)\n        \n        # Damped Newton\n        res2 = damped_newton(x_initial, N, eps, lam)\n        results.append(res2)\n        \n        # Hybrid Fallback\n        res3 = hybrid_fallback(x_initial, N, eps, c, rho)\n        results.append(res3)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3164396"}, {"introduction": "即使Hessian矩阵非奇异，一个完整的牛顿步（即步长 $\\alpha=1$）也并非总是最佳选择，尤其是在远离最小值或非凸区域时。这个高级实践 [@problem_id:2414720] 介绍了一种“全局化”策略：使用线搜索和Armijo条件来确定一个保证函数值充分下降的步长。这个练习将牛顿法的快速局部收敛性与一种稳健的全局策略相结合，展示了现代优化算法在解决科学和经济学中复杂问题时的强大威力。", "problem": "考虑在每次迭代都强制执行充分下降条件的更新规则下，对光滑实值函数进行无约束最小化。设 $f:\\mathbb{R}^n \\to \\mathbb{R}$ 是二次连续可微的函数。在迭代点 $x_k \\in \\mathbb{R}^n$ 处，定义梯度 $\\nabla f(x_k)$ 和海森矩阵 $\\nabla^2 f(x_k)$。一个候选更新 $x_{k+1} = x_k + \\alpha_k p_k$ 必须满足带有常数 $c_1 \\in (0,1)$ 和 $\\rho \\in (0,1)$ 的 Armijo 充分下降条件，即\n$$\nf(x_k + \\alpha_k p_k) \\le f(x_k) + c_1 \\alpha_k \\nabla f(x_k)^\\top p_k,\n$$\n其中 $\\alpha_k$ 通过回溯法从 $\\alpha_k = 1$ 开始，在集合 $\\{ \\rho^j : j \\in \\{0,1,2,\\ldots\\} \\}$ 中选取。对于给定的 $x_k$，首先尝试选择 $p_k$ 作为以下线性系统的解：\n$$\n\\nabla^2 f(x_k)\\, p_k = - \\nabla f(x_k).\n$$\n如果这个候选方向未能在回溯序列中产生一个满足充分下降条件的步长 $\\alpha_k$，则拒绝该方向，转而选择 $p_k = -\\nabla f(x_k)$，并通过相同的回溯充分下降条件来确定 $\\alpha_k$。当 $\\|\\nabla f(x_k)\\|_2 \\le \\varepsilon$ 或达到最大迭代次数 $K$ 时，迭代终止。\n\n使用以下固定常数实现此迭代方案：$c_1 = 10^{-4}$，$\\rho = 0.5$，容差 $\\varepsilon = 10^{-8}$，每次迭代的回溯限制为 $M = 50$ 次缩减，以及最大迭代次数 $K = 50$。如果在某个 $x_k$ 处线性系统是奇异或病态的，则将由 $\\nabla^2 f(x_k)\\, p_k = - \\nabla f(x_k)$ 定义的方向视为失败，并按上述方式继续使用 $p_k = -\\nabla f(x_k)$。所有计算都应在实数算术中进行，无需任何外部数据输入。\n\n将此方案应用于以下测试集。在所有情况下，使用欧几里得范数 $\\|\\cdot\\|_2$ 作为终止准则，并在每次迭代时从步长 $\\alpha_k = 1$ 开始回溯。报告每种情况下由终止条件返回的最小化点估计值。将报告的每个数字表示为精确到小数点后六位的十进制数。\n\n测试用例 A（两个参数的二元选择负对数似然）。设参数为 $x = (b_0, b_1)^\\top \\in \\mathbb{R}^2$。设数据为 $\\{(x_i,y_i)\\}_{i=1}^5$，其中\n$$\n(x_1,y_1) = (-2,0),\\quad (x_2,y_2) = (-1,0),\\quad (x_3,y_3) = (0,1),\\quad (x_4,y_4) = (1,1),\\quad (x_5,y_5) = (2,0).\n$$\n定义负对数似然函数\n$$\nf(x) = \\sum_{i=1}^5 \\left[ \\log\\!\\left(1 + e^{b_0 + b_1 x_i}\\right) - y_i\\,(b_0 + b_1 x_i) \\right].\n$$\n使用初始点 $x_0 = (0,0)^\\top$。\n\n测试用例 B（病态二次型）。设 $x \\in \\mathbb{R}^2$。定义\n$$\nf(x) = \\tfrac{1}{2} x^\\top Q x - b^\\top x,\\quad Q = \\begin{bmatrix} 10^{-6}  0 \\\\ 0  1 \\end{bmatrix},\\quad b = \\begin{bmatrix} 10^{-6} \\\\ 1 \\end{bmatrix}.\n$$\n使用初始点 $x_0 = (10,-10)^\\top$。\n\n测试用例 C（非凸一维四次函数）。设 $x \\in \\mathbb{R}$。定义\n$$\nf(x) = x^4 - 3 x^2 + x.\n$$\n使用初始点 $x_0 = 0.2$。\n\n输出规格。你的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果，每个测试用例的结果按顺序显示如下：\n- 对于测试用例 A：包含两个数的列表 $[b_0^\\star,b_1^\\star]$，\n- 对于测试用例 B：包含两个数的列表 $[x_1^\\star,x_2^\\star]$，\n- 对于测试用例 C：单个数字 $x^\\star$，\n每个数字都四舍五入到小数点后六位。例如，格式必须是\n$$\n\\big[ [b_0^\\star,b_1^\\star], [x_1^\\star,x_2^\\star], x^\\star \\big].\n$$\n打印的行中不允许有空格。所有数字都应表示为普通十进制数。", "solution": "所呈现的问题是数值优化中一个明确定义的任务。它要求实现一种结合了牛顿法和最速下降法的混合迭代算法，并辅以回溯线搜索以确保每一步都有充分的下降。该问题在科学上是合理的，基于非线性优化的既定原则，并为三个不同的测试用例提供了所有必要的参数、初始条件和目标函数。因此，该问题被认为是有效的。\n\n要实现的算法是一个用于二次连续可微函数 $f: \\mathbb{R}^n \\to \\mathbb{R}$ 的无约束最小化过程。给定一个迭代点 $x_k$，下一个迭代点 $x_{k+1}$ 通过 $x_{k+1} = x_k + \\alpha_k p_k$ 找到，其中 $p_k$ 是搜索方向，$\\alpha_k$ 是步长。\n\n算法的核心是搜索方向 $p_k$ 的选择。主要选择是牛顿方向，通过求解以下线性系统获得：\n$$\n\\nabla^2 f(x_k) p_k = - \\nabla f(x_k)\n$$\n其中 $\\nabla f(x_k)$ 是梯度，$\\nabla^2 f(x_k)$ 是 $f$ 在 $x_k$ 处的海森矩阵。对于二次函数，此方向是最优的，并且在海森矩阵是正定的最小值附近提供快速的局部收敛（二次收敛）。然而，如果海森矩阵是奇异的，牛顿方向可能未定义；或者如果海森矩阵不是正定的，它可能不是一个下降方向。如果 $\\nabla f(x_k)^\\top p_k  0$，则方向 $p_k$ 是一个下降方向。对于牛顿方向，$\\nabla f(x_k)^\\top p_k = -\\nabla f(x_k)^\\top (\\nabla^2 f(x_k))^{-1} \\nabla f(x_k)$，这个值仅在 $\\nabla^2 f(x_k)$ 是正定的时候才为负。如果牛顿方向不是下降方向或未能产生合适的步长，算法必须切换到一个稳健的替代方案。\n\n备用方向是最速下降方向，$p_k = -\\nabla f(x_k)$。只要梯度非零，这个方向就保证是一个下降方向，因为 $\\nabla f(x_k)^\\top p_k = -\\nabla f(x_k)^\\top \\nabla f(x_k) = -\\|\\nabla f(x_k)\\|_2^2  0$。虽然它的收敛通常较慢（线性收敛），但它能确保从任何 $\\nabla f(x_k) \\neq 0$ 的点取得进展。\n\n步长 $\\alpha_k$ 由回溯线搜索确定。从试探步长 $\\alpha = 1$ 开始，步长被连续乘以一个因子 $\\rho \\in (0,1)$，直到满足 Armijo 充分下降条件：\n$$\nf(x_k + \\alpha p_k) \\le f(x_k) + c_1 \\alpha \\nabla f(x_k)^\\top p_k\n$$\n其中 $c_1 \\in (0,1)$ 是一个常数。问题设定 $c_1 = 10^{-4}$ 和 $\\rho = 0.5$。回溯步数限制为 $M=50$ 次。\n\n整个迭代过程如下：\n对于 $k = 0, 1, 2, \\ldots, K-1$:\n1. 计算梯度 $g_k = \\nabla f(x_k)$。如果 $\\|g_k\\|_2 \\le \\varepsilon$，则终止。\n2. 尝试计算牛顿方向 $p_{\\text{Newton}}$。如果 $\\nabla^2f(x_k)$ 是奇异的，或者得到的方向不是下降方向（$\\nabla f(x_k)^\\top p_{\\text{Newton}} \\ge 0$），则此次尝试失败。\n3. 如果牛顿方向有效，则执行回溯线搜索。如果在 $M$ 次缩减内找到步长 $\\alpha_k$，则使用 $p_k = p_{\\text{Newton}}$ 和 $\\alpha_k$ 执行更新。\n4. 如果牛顿方向步骤因任何原因失败（奇异性、非下降方向或回溯失败），算法将退回到最速下降方向 $p_k = -\\nabla f(x_k)$，并执行另一次回溯线搜索以找到 $\\alpha_k$。\n5. 更新迭代点：$x_{k+1} = x_k + \\alpha_k p_k$。\n如果梯度范数低于容差 $\\varepsilon=10^{-8}$ 或达到最大迭代次数 $K=50$，则过程终止。\n\n该实现将此逻辑应用于三个测试用例。对于每个用例，我们必须提供目标函数 $f(x)$、其梯度 $\\nabla f(x)$ 和其海森矩阵 $\\nabla^2 f(x)$ 的解析形式。\n\n**测试用例 A：二元选择负对数似然**\n目标函数是逻辑回归模型的负对数似然。\n$f(b_0, b_1) = \\sum_{i=1}^5 \\left[ \\log(1 + e^{b_0 + b_1 x_i}) - y_i(b_0 + b_1 x_i) \\right]$。\n令 $u_i(b) = b_0 + b_1 x_i$。sigmoid 函数为 $\\sigma(u) = 1/(1+e^{-u})$。\n梯度分量为：\n$$\n\\frac{\\partial f}{\\partial b_0} = \\sum_{i=1}^5 (\\sigma(u_i) - y_i)\n$$\n$$\n\\frac{\\partial f}{\\partial b_1} = \\sum_{i=1}^5 x_i (\\sigma(u_i) - y_i)\n$$\n海森矩阵分量为（使用 $\\sigma'(u) = \\sigma(u)(1-\\sigma(u))$）：\n$$\n\\frac{\\partial^2 f}{\\partial b_0^2} = \\sum_{i=1}^5 \\sigma(u_i)(1-\\sigma(u_i))\n$$\n$$\n\\frac{\\partial^2 f}{\\partial b_1 \\partial b_0} = \\sum_{i=1}^5 x_i \\sigma(u_i)(1-\\sigma(u_i))\n$$\n$$\n\\frac{\\partial^2 f}{\\partial b_1^2} = \\sum_{i=1}^5 x_i^2 \\sigma(u_i)(1-\\sigma(u_i))\n$$\n这个海森矩阵总是半正定的，确保牛顿方向（如果已定义）是一个下降方向。\n\n**测试用例 B：病态二次型**\n目标函数是 $f(x) = \\tfrac{1}{2} x^\\top Q x - b^\\top x$。\n梯度是 $\\nabla f(x) = Qx - b$。\n海森矩阵是常数：$\\nabla^2 f(x) = Q$。\n由于 $Q$ 是一个常数、正定矩阵，使用完整步长（$\\alpha=1$）的牛顿法将在一次迭代中找到精确的最小值 $x^*=Q^{-1}b$。\n\n**测试用例 C：非凸一维四次函数**\n目标函数是 $f(x) = x^4 - 3x^2 + x$。\n梯度（导数）是 $f'(x) = 4x^3 - 6x + 1$。\n海森矩阵（二阶导数）是 $f''(x) = 12x^2 - 6$。\n在初始点 $x_0 = 0.2$ 处，海森矩阵为 $f''(0.2) = 12(0.04) - 6 = -5.52$，为负数。因此，$x_0$ 处的牛顿方向将是一个上升方向，算法必须在第一次迭代时回退到最速下降法。\n\n以下 Python 代码实现了所述算法并将其应用于三个测试用例，并按规定格式化输出。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the optimization for all test cases and print results.\n    \"\"\"\n    # Fixed constants for the algorithm\n    C1 = 1e-4\n    RHO = 0.5\n    EPS = 1e-8\n    MAX_ITER = 50\n    BT_LIMIT = 50\n\n    def optimizer(f, grad_f, hess_f, x0):\n        \"\"\"\n        Implements the hybrid Newton/Gradient-Descent optimization algorithm.\n        \"\"\"\n        x = np.array(x0, dtype=float)\n\n        for _ in range(MAX_ITER):\n            g = grad_f(x)\n            if np.linalg.norm(g) = EPS:\n                return x\n\n            p = None\n            alpha = None\n            newton_step_succeeded = False\n\n            # --- Attempt Newton's Method ---\n            try:\n                H = hess_f(x)\n                p_newton = np.linalg.solve(H, -g)\n\n                # Newton direction must be a descent direction\n                if np.dot(g, p_newton)  0:\n                    # Backtracking line search for Newton's direction\n                    alpha_bt = 1.0\n                    fk = f(x)\n                    g_dot_p = np.dot(g, p_newton)\n                    for _ in range(BT_LIMIT):\n                        if f(x + alpha_bt * p_newton) = fk + C1 * alpha_bt * g_dot_p:\n                            p = p_newton\n                            alpha = alpha_bt\n                            newton_step_succeeded = True\n                            break\n                        alpha_bt *= RHO\n            except np.linalg.LinAlgError:\n                # Hessian is singular or ill-posed\n                pass\n\n            # --- Fallback to Steepest Descent ---\n            if not newton_step_succeeded:\n                p_sd = -g\n                \n                # Backtracking line search for steepest descent\n                alpha_bt = 1.0\n                fk = f(x)\n                g_dot_p = np.dot(g, p_sd)\n                for _ in range(BT_LIMIT):\n                    if f(x + alpha_bt * p_sd) = fk + C1 * alpha_bt * g_dot_p:\n                        p = p_sd\n                        alpha = alpha_bt\n                        break\n                    alpha_bt *= RHO\n            \n            # If no step could be found (highly unlikely for steepest descent)\n            if p is None or alpha is None:\n                return x\n\n            # Update x\n            x = x + alpha * p\n        \n        return x\n\n    # --- Test Case A: Logistic Regression ---\n    x_data_A = np.array([-2.0, -1.0, 0.0, 1.0, 2.0])\n    y_data_A = np.array([0.0, 0.0, 1.0, 1.0, 0.0])\n    x0_A = np.array([0.0, 0.0])\n\n    def f_A(b):\n        u = b[0] + b[1] * x_data_A\n        # Use np.logaddexp for numerical stability: log(1+exp(x)) = logaddexp(0,x)\n        return np.sum(np.logaddexp(0, u) - y_data_A * u)\n\n    def grad_f_A(b):\n        u = b[0] + b[1] * x_data_A\n        # Sigmoid function: 1 / (1 + exp(-u))\n        sigma = 1 / (1 + np.exp(-u))\n        diff = sigma - y_data_A\n        return np.array([np.sum(diff), np.sum(diff * x_data_A)])\n\n    def hess_f_A(b):\n        u = b[0] + b[1] * x_data_A\n        sigma = 1 / (1 + np.exp(-u))\n        weights = sigma * (1 - sigma)\n        h00 = np.sum(weights)\n        h01 = np.sum(weights * x_data_A)\n        h11 = np.sum(weights * x_data_A**2)\n        return np.array([[h00, h01], [h01, h11]])\n\n    result_A = optimizer(f_A, grad_f_A, hess_f_A, x0_A)\n\n    # --- Test Case B: Ill-Conditioned Quadratic ---\n    Q_B = np.array([[1e-6, 0.0], [0.0, 1.0]])\n    b_vec_B = np.array([1e-6, 1.0])\n    x0_B = np.array([10.0, -10.0])\n\n    def f_B(x):\n        return 0.5 * x.T @ Q_B @ x - b_vec_B.T @ x\n    \n    def grad_f_B(x):\n        return Q_B @ x - b_vec_B\n    \n    def hess_f_B(x):\n        return Q_B\n\n    result_B = optimizer(f_B, grad_f_B, hess_f_B, x0_B)\n\n    # --- Test Case C: Nonconvex Quartic ---\n    x0_C = np.array([0.2])\n\n    def f_C(x):\n        xv = x[0]\n        return xv**4 - 3 * xv**2 + xv\n    \n    def grad_f_C(x):\n        xv = x[0]\n        return np.array([4 * xv**3 - 6 * xv + 1])\n    \n    def hess_f_C(x):\n        xv = x[0]\n        return np.array([[12 * xv**2 - 6]])\n    \n    result_C = optimizer(f_C, grad_f_C, hess_f_C, x0_C)\n\n    # --- Format Output ---\n    results_list = [result_A, result_B, result_C]\n    formatted_parts = []\n    for res in results_list:\n        if res.size > 1:\n            num_strs = [f\"{val:.6f}\" for val in res]\n            formatted_parts.append(f\"[{','.join(num_strs)}]\")\n        else:\n            formatted_parts.append(f\"{res.item():.6f}\")\n\n    final_output = f\"[{','.join(formatted_parts)}]\"\n    print(final_output)\n\nsolve()\n```", "id": "2414720"}]}