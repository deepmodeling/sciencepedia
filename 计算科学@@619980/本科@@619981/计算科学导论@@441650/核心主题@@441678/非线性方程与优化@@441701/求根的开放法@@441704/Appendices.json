{"hands_on_practices": [{"introduction": "要掌握开放求根法，最好的起点是亲手实现一个核心算法。本练习将指导您应用割线法——一种无需计算导数的强大开放方法——来求解一个典型的超越方程。通过这个实践 [@problem_id:2422713]，您将直接体验迭代过程的逻辑、收敛条件的设置以及处理非线性问题的基本流程。", "problem": "考虑一个以弧度表示的单实数变量非线性方程 $f(x) = \\sin(x) - \\dfrac{x}{2}$。该方程的解是任何满足 $f(x) = 0$ 的实数 $x$。其平凡解为 $x = 0$。您的任务是根据指定的初始种子点，在不使用任何外部数据的情况下，计算 $f(x) = 0$ 的解的数值近似值。角度必须以弧度处理。\n\n请使用以下测试套件。每个测试用例提供一个有序的实数值种子点对 $(x_0, x_1)$。对于每个测试用例，仅使用问题陈述和种子点所隐含的信息，计算 $f(x)$ 的一个根的近似值 $\\hat{x}$，并同时施加以下停止准则：当当前近似值处的函数绝对值满足 $\\lvert f(\\hat{x}) \\rvert \\le \\tau$ 或连续近似值之间的变化量满足 $\\lvert \\Delta \\hat{x} \\rvert \\le \\tau$ 时，终止计算，其中 $\\tau = 10^{-12}$。为每个测试用例设置 $N_{\\max} = 100$ 次迭代的硬性上限。如果在 $N_{\\max}$ 次迭代内未达到收敛，则返回最后一次迭代的结果作为近似值。\n\n测试套件（所有种子点均以弧度为单位）：\n- 用例 1: $(x_0, x_1) = (1.0, 2.0)$\n- 用例 2: $(x_0, x_1) = (-1.0, -2.0)$\n- 用例 3: $(x_0, x_1) = (0.1, -0.1)$\n- 用例 4: $(x_0, x_1) = (1.0, 1.5)$\n\n数值计算细节要求：\n- 角度以弧度处理。\n- 完全按照所述使用函数 $f(x) = \\sin(x) - \\dfrac{x}{2}$。\n- 使用如上定义的容差 $\\tau = 10^{-12}$ 和迭代上限 $N_{\\max} = 100$。\n\n您的程序应生成单行输出，其中包含用方括号括起来的、以逗号分隔的结果列表。对于每个测试用例，输出近似根 $\\hat{x}$，并四舍五入到10位小数。因此，最后一行必须是 $[\\hat{x}_1,\\hat{x}_2,\\hat{x}_3,\\hat{x}_4]$ 的形式，其中每个 $\\hat{x}_k$ 是一个四舍五入到10位小数的十进制格式实数。不允许有其他任何输出。", "solution": "所给问题要求对非线性超越方程 $f(x) = \\sin(x) - \\dfrac{x}{2} = 0$ 的根进行数值近似。除了平凡解 $x=0$ 外，这些根是函数 $y = \\sin(x)$ 和 $y = \\dfrac{x}{2}$ 图像的交点。对这两个函数的图像进行简单分析可以发现，除了在原点的平凡解外，还有一个正根和一个负根。\n\n问题指定必须使用一种开区间求根法，并且为每个测试用例提供了两个初始种子点 $(x_0, x_1)$。这一设定唯一地确定了所选方法：割线法。这是一种两点迭代法，它用一条穿过最近两个近似点 $(x_{k-1}, f(x_{k-1}))$ 和 $(x_k, f(x_k))$ 的割线来近似函数 $f(x)$。下一个近似值 $x_{k+1}$ 是这条割线的根。\n\n割线法的递推关系式为：\n$$ x_{k+1} = x_k - f(x_k) \\frac{x_k - x_{k-1}}{f(x_k) - f(x_{k-1})} $$\n该算法从初始种子点 $x_0$ 和 $x_1$ 开始，迭代生成一个近似序列 $\\{x_k\\}$。\n\n对于每个测试用例 $(x_0, x_1)$，实现将按以下步骤进行：\n$1$. 初始化序列中的前两个点，$x_{k-1} = x_0$ 和 $x_k = x_1$。\n$2$. 进入一个最多迭代 $N_{\\max} = 100$ 次的循环。\n$3$. 在循环内部，计算函数值 $f(x_k)$ 和 $f(x_{k-1})$。\n$4$. 必须对分母 $f(x_k) - f(x_{k-1})$ 进行关键检查。如果其绝对值接近于零，则割线近乎水平，方法会因除以零而变得不稳定或失败。在这种情况下，必须终止迭代。\n$5$. 使用割线公式计算下一个近似值 $\\hat{x} \\equiv x_{k+1}$。\n$6$. 如果满足以下两个指定停止准则中的任意一个，则迭代终止：\n    a) 残差足够小：$|f(\\hat{x})| \\le \\tau$。\n    b) 连续近似值之间的变化足够小：$|\\Delta \\hat{x}| = |x_{k+1} - x_k| \\le \\tau$。\n指定的容差为 $\\tau = 10^{-12}$。\n$7$. 如果未达到收敛，则更新迭代值：将 $x_{k-1}$ 设置为 $x_k$，将 $x_k$ 设置为 $x_{k+1}$。然后继续循环。\n$8$. 如果循环在 $N_{\\max}$ 次迭代后仍未收敛，则返回最后计算的近似值 $x_k$ 作为结果。\n\n测试用例简要分析：\n- 用例1和2的初始种子点分别为 $(1.0, 2.0)$ 和 $(-1.0, -2.0)$，分别位于非平凡正根和负根的两侧。函数 $f(x)$ 是奇函数，$f(-x) = -f(x)$，因此预计负根是正根的加法逆元。割线法预计会快速收敛。\n- 用例3的种子点为 $(0.1, -0.1)$，是关于原点对称的初始点。由于函数 $f(x)$ 的奇对称性，割线法的第一步迭代将产生 $x_2 = 0$，即精确的平凡解。\n- 用例4的种子点为 $(1.0, 1.5)$，这是一个更具挑战性的情况。区间 $[1.0, 1.5]$ 包含 $f(x)$ 的一个局部最大值，该点满足 $f'(x) = \\cos(x) - \\frac{1}{2} = 0$，发生在 $x = \\arccos(0.5) = \\frac{\\pi}{3} \\approx 1.047$ 弧度处。初始割线的斜率会很小，可能将下一个迭代点投影到远离根的位置，但该算法足够鲁棒，最终能够收敛到正确的正根。\n\n最终的程序将为每个测试用例实现此算法，并按规定格式化所得的近似结果。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes numerical approximations for roots of f(x) = sin(x) - x/2\n    using the Secant method for a given set of test cases.\n    \"\"\"\n\n    def f(x: float) -> float:\n        \"\"\"The nonlinear function f(x) = sin(x) - x/2.\"\"\"\n        return np.sin(x) - x / 2.0\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (1.0, 2.0),\n        (-1.0, -2.0),\n        (0.1, -0.1),\n        (1.0, 1.5),\n    ]\n\n    # Parameters from the problem statement.\n    TAU = 1e-12\n    N_MAX = 100\n\n    results = []\n    for case in test_cases:\n        x_prev, x_curr = case\n        \n        for _ in range(N_MAX):\n            f_curr = f(x_curr)\n            f_prev = f(x_prev)\n\n            # Denominator for the Secant method formula\n            denominator = f_curr - f_prev\n\n            # If the denominator is very close to zero, the method may fail or stagnate.\n            # This can happen if f(x_curr) is very close to f(x_prev).\n            # The loop will naturally terminate if convergence is not achieved,\n            # so we only protect against division by zero.\n            if abs(denominator)  1e-15:  # Use a small epsilon to avoid division by zero\n                break\n\n            # Calculate the next approximation using the Secant method formula\n            x_next = x_curr - f_curr * (x_curr - x_prev) / denominator\n\n            # Check the stopping criteria:\n            # 1. The absolute value of the function at the new approximation.\n            # 2. The absolute change in successive approximations.\n            if abs(f(x_next)) = TAU or abs(x_next - x_curr) = TAU:\n                x_curr = x_next  # Update to the final converged value\n                break\n\n            # Update points for the next iteration\n            x_prev = x_curr\n            x_curr = x_next\n        \n        # Append the final approximation for this case to the results list.\n        # The problem requires rounding to 10 decimal places.\n        results.append(x_curr)\n\n    # Final print statement in the exact required format.\n    # The format specifier '.10f' ensures rounding to 10 decimal places.\n    print(f\"[{','.join(f'{r:.10f}' for r in results)}]\")\n\nsolve()\n```", "id": "2422713"}, {"introduction": "标准的牛顿法在遇到多重根时，其二次收敛的优势会退化为线性收敛，效率大打折扣。这个高级练习 [@problem_id:3260046] 旨在揭示这一关键限制，并探索解决方案。您不仅需要从第一性原理出发分析收敛阶数，还需设计并实现两种不同的修正方法，以恢复算法的快速收敛特性，这是构建稳健数值求解器的重要一步。", "problem": "考虑将称为牛顿法的开放求根方法应用于具有重根的多项式。设函数定义为 $f(x) = (x - 2)^2(x + 1)$，该函数在 $x = 2$ 处有一个重数为 $m = 2$ 的根，在 $x = -1$ 处有一个重数为 $m = 1$ 的根。基于基本原理进行严格分析，研究根的重数对牛顿法局部收敛行为的影响，然后设计并实现两种重数修正方法：一种使用已知的重数 $m$，另一种使用从导数估计的 $m$。\n\n您的推导必须从以下基本依据开始：\n- 重数为 $m$ 的根的定义：如果 $r$ 是 $f$ 的一个根，则存在一个函数 $g$ 满足 $g(r) \\neq 0$，使得 $f(x) = (x - r)^m g(x)$。\n- 足够光滑的函数 $f$ 在点 $x_k$ 附近的一阶泰勒展开：$f(x_{k+1}) \\approx f(x_k) + f'(x_k)(x_{k+1} - x_k)$。\n- 牛顿法的定义，即通过在根附近将一阶泰勒近似设为零得到的解的更新，以及局部误差的概念 $e_k = x_k - r$。\n\n基于这些依据：\n- 推导牛顿法在重数为 $m$ 的根附近的局部误差递推关系，并确定其收敛阶和渐进误差比。\n- 在重数 $m$ 已知的情况下，推导重数修正的牛顿更新公式。\n- 推导一个仅用 $f$、$f'$ 和 $f''$ 表示的重数 $m$ 的实用估计量，并用它来构造当 $m$ 未知时的重数修正的牛顿更新公式。\n\n实现要求：\n- 实现三种方法：标准牛顿法、已知重数 $m$ 的重数修正牛顿法，以及从导数估计 $m$ 的重数修正的牛顿法。\n- 对于数值实验，使用 $f(x) = (x - 2)^2(x + 1)$ 的显式导数，并根据迭代值计算经验收敛阶。将相对于所选目标根 $r$ 的局部误差定义为 $e_k = x_k - r$，并使用三个连续误差通过以下公式计算经验阶\n$$\np \\approx \\frac{\\ln\\left(\\lvert e_{k+1} \\rvert / \\lvert e_k \\rvert\\right)}{\\ln\\left(\\lvert e_k \\rvert / \\lvert e_{k-1} \\rvert\\right)}.\n$$\n同时，在终止时使用最后两个非零误差报告最后可用的渐进误差比 $\\rho \\approx \\lvert e_{k+1} \\rvert / \\lvert e_k \\rvert$。如果可用于阶估计的非零误差少于三个，则报告 $p = 0.0$；如果可用于比率估计的非零误差少于两个，则报告 $\\rho = 0.0$。\n\n迭代的停止准则：\n- 当 $\\lvert f(x_k) \\rvert \\leq 10^{-16}$ 或 $\\lvert e_k \\rvert \\leq 10^{-16}$ 时，或在 $50$ 次迭代后终止，以先到者为准。\n\n测试套件：\n- 使用以下 $6$ 个测试用例，每个用例由方法标签、初始猜测值 $x_0$ 和目标根 $r$ 定义：\n    1. 标准牛顿法，$x_0 = 3.5$，$r = 2$。\n    2. 已知重数 $m = 2$ 的重数修正牛顿法，$x_0 = 3.5$，$r = 2$。\n    3. 从导数估计 $m$ 的重数修正牛顿法，$x_0 = 3.5$，$r = 2$。\n    4. 标准牛顿法，$x_0 = -0.8$，$r = -1$。\n    5. 从导数估计 $m$ 的重数修正牛顿法，$x_0 = -0.8$，$r = -1$。\n    6. 标准牛顿法，$x_0 = 2.1$，$r = 2$。\n\n对于每个测试用例，报告两个浮点数，四舍五入到六位小数：\n- 从最后三个非零误差计算出的经验收敛阶 $p$。\n- 从最后两个非零误差计算出的最后渐进误差比 $\\rho$。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含结果，格式为方括号括起来的逗号分隔的对，每对格式为 $[p,\\rho]$。整体输出应如下所示\n$[[p_1,\\rho_1],[p_2,\\rho_2],[p_3,\\rho_3],[p_4,\\rho_4],[p_5,\\rho_5],[p_6,\\rho_6]]$。", "solution": "该问题陈述是数值分析领域一个定义明确且有科学依据的练习，具体涉及牛顿法对不同重数根的收敛性质。所有提供的信息都是自洽的、数学上一致的，并且与开放求根方法的主题直接相关。因此，该问题被认为是有效的，下面将给出完整解答。\n\n问题的核心是分析并修正当牛顿法应用于重数 $m  1$ 的根时其收敛速度下降的问题。我们将首先推导标准方法的收敛行为，然后推导两种修正方法，最后通过实现它们来验证理论结果。\n\n所考虑的函数是 $f(x) = (x - 2)^2(x + 1)$。该多项式有一个重数为 $m=2$ 的根 $r=2$ 和一个重数为 $m=1$ 的单根 $r=-1$。为了实现算法，需要其导数：\n$f(x) = x^3 - 3x^2 + 4$\n$f'(x) = 3x^2 - 6x$\n$f''(x) = 6x - 6$\n\n### 1. 标准牛顿法的局部收敛性\n\n设 $r$ 是一个重数为 $m \\ge 1$ 的根。根据定义，$f(x)$ 可以写成 $f(x) = (x-r)^m g(x)$，其中 $g(r) \\neq 0$。\n其一阶导数为 $f'(x) = m(x-r)^{m-1}g(x) + (x-r)^m g'(x)$。\n\n标准牛顿迭代法由下式给出：\n$$x_{k+1} = x_k - \\frac{f(x_k)}{f'(x_k)}$$\n代入 $f(x_k)$ 和 $f'(x_k)$ 的表达式：\n$$x_{k+1} = x_k - \\frac{(x_k-r)^m g(x_k)}{m(x_k-r)^{m-1}g(x_k) + (x_k-r)^m g'(x_k)}$$\n设第 $k$ 步的误差为 $e_k = x_k - r$。那么下一步的误差为 $e_{k+1} = x_{k+1} - r$。\n$$e_{k+1} = e_k - \\frac{e_k^m g(x_k)}{m e_k^{m-1}g(x_k) + e_k^m g'(x_k)}$$\n从分母中提取因子 $e_k^{m-1}$：\n$$e_{k+1} = e_k - \\frac{e_k g(x_k)}{m g(x_k) + e_k g'(x_k)} = e_k \\left( 1 - \\frac{g(x_k)}{m g(x_k) + e_k g'(x_k)} \\right)$$\n$$e_{k+1} = e_k \\left( \\frac{m g(x_k) + e_k g'(x_k) - g(x_k)}{m g(x_k) + e_k g'(x_k)} \\right) = e_k \\left( \\frac{(m-1) g(x_k) + e_k g'(x_k)}{m g(x_k) + e_k g'(x_k)} \\right)$$\n当 $x_k \\to r$ 时，我们有 $e_k \\to 0$ 且 $g(x_k) \\to g(r)$。误差的表达式变为：\n$$e_{k+1} \\approx e_k \\left( \\frac{(m-1) g(r)}{m g(r)} \\right) = \\left( \\frac{m-1}{m} \\right) e_k$$\n这表明对于 $m  1$，每一步的误差都按一个常数因子减小。这是线性收敛（阶数 $p=1$）的定义。渐进误差比为 $\\rho = \\lim_{k\\to\\infty} \\frac{\\lvert e_{k+1} \\rvert}{\\lvert e_k \\rvert} = \\frac{m-1}{m}$。对于本问题中的 $m=2$ 的情况，我们预期收敛是线性的，且 $\\rho = 1/2$。\n\n对于单根（$m=1$），该比率为 $(1-1)/1 = 0$，意味着超线性收敛。更详细的泰勒级数分析表明，对于 $m=1$，收敛是二次的（$p=2$），且有 $e_{k+1} \\approx \\frac{f''(r)}{2f'(r)}e_k^2$。\n\n### 2. 重数修正的牛顿法（已知 $m$）\n\n为了恢复二次收敛，我们可以修改迭代公式。考虑一个新函数 $u(x) = f(x)^{1/m}$。如果 $f(x) = (x-r)^m g(x)$，则 $u(x) = (x-r)g(x)^{1/m}$。由于 $g(r) \\neq 0$，函数 $u(x)$ 在 $x=r$ 处有一个单根。对 $u(x)$ 应用标准牛顿法可以得到对其根 $r$ 的二次收敛。\n\n$u(x)$ 的迭代公式为 $x_{k+1} = x_k - \\frac{u(x_k)}{u'(x_k)}$。我们可以用 $f(x_k)$ 和 $f'(x_k)$ 来表示它：\n$$u'(x) = \\frac{d}{dx} \\left(f(x)^{1/m}\\right) = \\frac{1}{m}f(x)^{\\frac{1}{m}-1}f'(x)$$\n步长修正是：\n$$\\frac{u(x)}{u'(x)} = \\frac{f(x)^{1/m}}{\\frac{1}{m}f(x)^{\\frac{1}{m}-1}f'(x)} = \\frac{f(x)^{1/m}}{\\frac{f(x)^{1/m}}{m f(x)}f'(x)} = m \\frac{f(x)}{f'(x)}$$\n这导出了重数修正的牛顿法：\n$$x_{k+1} = x_k - m \\frac{f(x_k)}{f'(x_k)}$$\n由于这等价于对一个具有单根的函数应用标准牛顿法，因此收敛是二次的（$p=2$）。\n\n### 3. 重数修正的牛顿法（估计 $m$）\n\n当 $m$ 未知时，可以从函数及其导数中估计它。我们可以找到一个 $m$ 的极限表达式。在根 $r$ 附近，使用渐近形式 $f(x) \\propto (x-r)^m$、$f'(x) \\propto m(x-r)^{m-1}$ 和 $f''(x) \\propto m(m-1)(x-r)^{m-2}$，我们可以构造一个比率，使得未知项被抵消。\n$$\\lim_{x\\to r} \\frac{f(x)f''(x)}{[f'(x)]^2} = \\frac{(x-r)^m g(r) \\cdot m(m-1)(x-r)^{m-2} g(r)}{[m(x-r)^{m-1} g(r)]^2} = \\frac{m(m-1)(x-r)^{2m-2}}{m^2(x-r)^{2m-2}} = \\frac{m-1}{m}$$\n我们可以从中解出 $m$：\n$$m = \\frac{1}{1 - \\frac{f(x)f''(x)}{[f'(x)]^2}} = \\frac{[f'(x)]^2}{[f'(x)]^2 - f(x)f''(x)}$$\n这提出了一个自适应方案，在每次迭代 $k$ 中，我们估计 $m$ 为：\n$$m_k = \\frac{[f'(x_k)]^2}{[f'(x_k)]^2 - f(x_k)f''(x_k)}$$\n并将此估计值用于修正的牛顿更新中：\n$$x_{k+1} = x_k - m_k \\frac{f(x_k)}{f'(x_k)} = x_k - \\frac{f(x_k)f'(x_k)}{[f'(x_k)]^2 - f(x_k)f''(x_k)}$$\n这被认为是 Halley 方法。已知该方法对于重根能恢复二次收敛（$p=2$），对于单根能达到三次收敛（$p=3$）。\n\n### 实现方法的总结\n实现将分析以下三种迭代方案：\n1.  **标准牛顿法：** $x_{k+1} = x_k - \\frac{f(x_k)}{f'(x_k)}$\n2.  **修正法（已知 $m$）：** $x_{k+1} = x_k - m \\frac{f(x_k)}{f'(x_k)}$\n3.  **修正法（估计 $m$）：** $x_{k+1} = x_k - \\frac{f(x_k)f'(x_k)}{[f'(x_k)]^2 - f(x_k)f''(x_k)}$\n\n这些方法将被应用于测试用例，以经验性地计算收敛阶 $p$ 和渐进误差比 $\\rho$，预期这些结果将与理论推导一致。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the numerical analysis problem concerning Newton's method and root multiplicity.\n    \"\"\"\n\n    def f(x_val):\n        \"\"\"The function f(x) = (x-2)^2(x+1).\"\"\"\n        # x**3 - 3*x**2 + 4\n        return x_val**3 - 3 * x_val**2 + 4\n\n    def df(x_val):\n        \"\"\"The first derivative f'(x).\"\"\"\n        # 3*x**2 - 6*x\n        return 3 * x_val**2 - 6 * x_val\n\n    def d2f(x_val):\n        \"\"\"The second derivative f''(x).\"\"\"\n        # 6*x - 6\n        return 6 * x_val - 6\n\n    def run_iteration(method, x_0, r, m_known=None):\n        \"\"\"\n        Runs a specified root-finding iteration and computes empirical convergence metrics.\n\n        Args:\n            method (str): The method to use ('standard', 'corrected_known_m', 'corrected_estimated_m').\n            x_0 (float): The initial guess.\n            r (float): The target root.\n            m_known (int, optional): The known multiplicity for the corrected method.\n\n        Returns:\n            A tuple (p, rho) containing the empirical order and asymptotic ratio.\n        \"\"\"\n        x = float(x_0)\n        errors = [x - r]\n        max_iter = 50\n        tol = 1e-16\n\n        for _ in range(max_iter):\n            fx = f(x)\n\n            if abs(fx) = tol or abs(x - r) = tol:\n                break\n\n            dfx = df(x)\n            if abs(dfx)  np.finfo(float).eps:\n                break\n\n            if method == 'standard':\n                x = x - fx / dfx\n            elif method == 'corrected_known_m':\n                x = x - m_known * fx / dfx\n            elif method == 'corrected_estimated_m':\n                d2fx = d2f(x)\n                denominator = dfx**2 - fx * d2fx\n                if abs(denominator)  np.finfo(float).eps:\n                    m_est = 1.0  # Fallback to standard Newton step\n                else:\n                    m_est = dfx**2 / denominator\n                x = x - m_est * fx / dfx\n            else:\n                raise ValueError(f\"Unknown method: {method}\")\n\n            errors.append(x - r)\n\n        # Using a small threshold to filter out errors that are effectively zero\n        nonzero_errors = [e for e in errors if abs(e) > 1e-18]\n\n        p = 0.0\n        if len(nonzero_errors) >= 3:\n            try:\n                e_k_plus_1 = abs(nonzero_errors[-1])\n                e_k = abs(nonzero_errors[-2])\n                e_k_minus_1 = abs(nonzero_errors[-3])\n\n                log_num = np.log(e_k_plus_1 / e_k)\n                log_den = np.log(e_k / e_k_minus_1)\n\n                if abs(log_den) > np.finfo(float).eps:\n                    p = log_num / log_den\n            except (ValueError, ZeroDivisionError):\n                p = 0.0 # Calculation failed\n\n        rho = 0.0\n        if len(nonzero_errors) >= 2:\n            try:\n                e_k_plus_1 = abs(nonzero_errors[-1])\n                e_k = abs(nonzero_errors[-2])\n                rho = e_k_plus_1 / e_k\n            except ZeroDivisionError:\n                rho = 0.0\n\n        return p, rho\n\n    test_cases = [\n        # (method, initial_guess_x0, target_root_r, known_multiplicity_m)\n        ('standard', 3.5, 2, None),                # Case 1\n        ('corrected_known_m', 3.5, 2, 2),         # Case 2\n        ('corrected_estimated_m', 3.5, 2, None),  # Case 3\n        ('standard', -0.8, -1, None),               # Case 4\n        ('corrected_estimated_m', -0.8, -1, None), # Case 5\n        ('standard', 2.1, 2, None)                 # Case 6\n    ]\n\n    results = []\n    for case in test_cases:\n        method, x0, r, m_known = case\n        p, rho = run_iteration(method, x0, r, m_known)\n        results.append([round(p, 6), round(rho, 6)])\n\n    formatted_results = [f\"[{p_val:.6f},{rho_val:.6f}]\" for p_val, rho_val in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3260046"}, {"introduction": "在科学计算中，将不同算法的优点结合起来，往往能创造出更高效、更稳健的工具。本练习 [@problem_id:3167328] 是一个实用的设计挑战，它引导您构建一个混合求解器。该算法首先利用割线法在全局范围内稳定地逼近解，然后在进入收敛盆地后切换到收敛更快的牛顿法，这体现了根据问题特性定制算法的核心思想。", "problem": "考虑标量非线性方程 $f(x)=x-e^{-x}$。目标是设计并实现一种用于求根的混合开放式方法，该方法以类割线法开始以进入收敛盆，一旦局部导数幅值足够大（由用户指定的阈值确定），则切换到类牛顿法更新。从一个基本原理出发：可微函数 $f(x)$ 的近似根可以通过局部线性化和有限差分斜率近似进行迭代改进。该方法必须满足以下原则：\n\n- 使用 $f(x)$ 的一阶局部线性模型来推导旨在求解 $f(x)=0$ 的更新规则。\n- 当有两个不同的迭代点可用时，使用有限差分近似来计算斜率。\n- 一旦导数幅值 $|f'(x)|$ 超过指定阈值，则使用精确导数以加速收敛。\n\n实现规范：\n\n1. 实现一个函数，该函数使用一种混合方法尝试求解 $f(x)=x-e^{-x}$ 的 $f(x)=0$ 问题：\n   - 基于两个初始猜测值 $x_0$ 和 $x_1$，以类割线法更新开始。\n   - 在每次迭代中，生成下一个迭代点 $x_{k+1}$ 后，计算导数的幅值 $|f'(x_{k+1})|$。如果 $|f'(x_{k+1})|$ 超过阈值 $\\tau$，则将所有后续迭代切换到使用 $f(x)$ 解析导数的类牛顿法更新。\n   - 当函数绝对值 $|f(x_k)|$ 小于容差 $\\varepsilon$ 或绝对变化量 $|x_{k}-x_{k-1}|$ 小于 $\\varepsilon$ 时，或达到最大迭代次数 $N_{\\max}$ 时终止。如果在达到 $N_{\\max}$ 时仍未满足容差，则返回最后一个迭代点。\n\n2. 用于切换决策和类牛顿法更新的导数必须通过对给定的 $f(x)$ 进行解析计算得到。\n\n3. 数值参数：\n   - 使用容差 $\\varepsilon=10^{-12}$。\n   - 每个测试用例使用指定的迭代限制 $N_{\\max}$。\n\n4. 输出格式：\n   - 对于每个测试用例，将根的最终近似值作为浮点数返回，并四舍五入到 $10$ 位小数。\n   - 您的程序应生成单行输出，其中包含用方括号括起来的逗号分隔的结果列表（例如，$[r_1,r_2,r_3,r_4]$）。\n\n测试套件和覆盖范围：\n\n- 用例 $1$（正常路径，中等阈值，在收敛盆附近早期切换）：$x_0=0.0$, $x_1=1.0$, $\\tau=1.4$, $\\varepsilon=10^{-12}$, $N_{\\max}=50$。\n- 用例 $2$（边界情况：高阈值导致从不使用类牛顿法更新，仅用割线法收敛）：$x_0=10.0$, $x_1=12.0$, $\\tau=1.6$, $\\varepsilon=10^{-12}$, $N_{\\max}=100$。\n- 用例 $3$（边缘情况：由于早期迭代点的导数幅值较大而立即切换）：$x_0=-3.0$, $x_1=-2.0$, $\\tau=1.1$, $\\varepsilon=10^{-12}$, $N_{\\max}=50$。\n- 用例 $4$（边缘情况：极小的 $N_{\\max}$ 导致过早终止，得到一个粗糙的近似值）：$x_0=0.0$, $x_1=2.0$, $\\tau=1.5$, $\\varepsilon=10^{-12}$, $N_{\\max}=2$。\n\n所有答案都是纯数字，没有物理单位或角度。函数 $f(x)$ 是无量纲的。通过仅在满足切换准则时才转而使用类牛顿法更新，否则继续使用最后两个迭代点进行类割线法更新，来确保在存在小的有限差分分母时的数值稳定性。", "solution": "该问题要求设计并实现一种混合数值方法，以寻找标量非线性方程 $f(x) = x - e^{-x}$ 的根。该方法从割线法开始，并根据特定准则转换到牛顿法。\n\n所考虑的函数是 $f(x) = x - e^{-x}$。为了实现牛顿法和切换准则，需要 $f(x)$ 的解析导数。其导数 $f'(x)$ 计算如下：\n$$\nf'(x) = \\frac{d}{dx}(x - e^{-x}) = 1 - (-e^{-x}) = 1 + e^{-x}\n$$\n函数 $f(x)$ 对于所有 $x \\in \\mathbb{R}$ 都是连续且可微的。由于对于所有实数 $x$，都有 $e^{-x}  0$，因此导数 $f'(x) = 1 + e^{-x}  1$。这意味着 $f(x)$ 是一个严格单调递增的函数。一个严格单调的函数最多与x轴相交一次，这保证了方程 $f(x)=0$ 有一个唯一的实根。\n\n该混合算法由两种基本的开放式求根方法构成：割线法和牛顿法。两者都基于寻找函数局部线性近似的根。\n\n割线法使用基于前两个迭代点 $x_k$ 和 $x_{k-1}$ 的后向有限差分来近似导数 $f'(x_k)$：\n$$\nf'(x_k) \\approx \\frac{f(x_k) - f(x_{k-1})}{x_k - x_{k-1}}\n$$\n将此近似值代入牛顿法公式，得到割线法的更新规则：\n$$\nx_{k+1} = x_k - f(x_k) \\frac{x_k - x_{k-1}}{f(x_k) - f(x_{k-1})}\n$$\n该方法表现出超线性收敛（阶数约为 $1.618$），但不需要计算解析导数。\n\n牛顿法（也称为牛顿-拉夫逊方法）使用函数在当前迭代点 $x_k$ 处的切线来近似下一个迭代点。这是从 $f(x)$ 在 $x_k$ 附近的一阶泰勒级数展开推导出来的：\n$$\nf(x) \\approx f(x_k) + f'(x_k)(x - x_k)\n$$\n通过设置 $f(x) = 0$ 并将根的新近似值指定为 $x_{k+1}$，我们求解 $x_{k+1}$：\n$$\n0 = f(x_k) + f'(x_k)(x_{k+1} - x_k) \\implies x_{k+1} = x_k - \\frac{f(x_k)}{f'(x_k)}\n$$\n只要导数非零且表现良好，该方法在根附近通常表现出二次收敛。\n\n指定的混合算法综合了这两种方法。它从割线法开始，使用两个初始猜测值 $x_0$ 和 $x_1$。在每次迭代 $k$ 中，计算一个新的迭代点 $x_{k+1}$。然后，算法评估这个新点的解析导数幅值 $|f'(x_{k+1})|$。如果该值超过给定阈值 $\\tau$，算法将永久切换到牛顿法用于所有后续迭代。该策略利用计算成本较低的割线法来接近收敛盆，然后在函数的局部几何形状（由导数幅值表示）被认为合适时，切换到收敛更快的牛顿法。\n\n该算法的实现维护一个状态变量，例如一个布尔标志 `use_newton`，初始化为 false。迭代过程如下：\n1. 用 $x_{k-1} = x_0$ 和 $x_k = x_1$ 进行初始化。\n2. 在每次迭代中，根据步长检查终止条件：$|x_k - x_{k-1}|  \\varepsilon$。\n3. 如果 `use_newton` 为 false，则使用割线法法则计算下一个迭代点 $x_{k+1}$。否则，使用牛顿法法则计算 $x_{k+1}$。\n4. 更新存储的迭代点：$x_{k-1} \\leftarrow x_k$ 和 $x_k \\leftarrow x_{k+1}$。\n5. 根据函数值检查终止条件：$|f(x_k)|  \\varepsilon$。\n6. 如果 `use_newton` 当前为 false，则评估切换准则：如果 $|f'(x_k)|  \\tau$，则将 `use_newton` 设置为 true 用于所有未来的迭代。\n7. 这个过程持续进行，直到满足其中一个终止条件或达到最大迭代次数 $N_{\\max}$。如果达到了 $N_{\\max}$，则返回最后计算的迭代点。\n\n这种设计为求根提供了一个鲁棒的程序，其测试用例旨在检验算法的各种逻辑路径：标准切换、仅使用割线法的路径、立即切换以及过早终止。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for all test cases and print the results.\n    \"\"\"\n    \n    def f(x: float) -> float:\n        \"\"\"The scalar nonlinear function f(x) = x - e^(-x).\"\"\"\n        return x - np.exp(-x)\n\n    def f_prime(x: float) -> float:\n        \"\"\"The analytical derivative f'(x) = 1 + e^(-x).\"\"\"\n        return 1.0 + np.exp(-x)\n\n    def hybrid_solver(x0: float, x1: float, tau: float, epsilon: float, n_max: int) -> float:\n        \"\"\"\n        Implements the hybrid secant-Newton root-finding method.\n\n        Args:\n            x0 (float): The first initial guess.\n            x1 (float): The second initial guess.\n            tau (float): The derivative magnitude threshold for switching to Newton's method.\n            epsilon (float): The tolerance for termination.\n            n_max (int): The maximum number of iterations.\n\n        Returns:\n            float: The final approximation of the root.\n        \"\"\"\n        xk_minus_1 = float(x0)\n        xk = float(x1)\n        use_newton = False\n\n        # Pre-check for convergence on the second initial guess\n        if abs(f(xk))  epsilon:\n            return xk\n\n        for _ in range(n_max):\n            # Termination condition 1: absolute change in iterates\n            if abs(xk - xk_minus_1)  epsilon:\n                return xk\n\n            # Select method and compute next iterate\n            if use_newton:\n                # Newton-Raphson update\n                fxk = f(xk)\n                fpxk = f_prime(xk)\n                # The derivative f'(x) for this function is always >= 1, so no division by zero\n                xk_plus_1 = xk - fxk / fpxk\n            else:\n                # Secant method update\n                fxk = f(xk)\n                fxk_minus_1 = f(xk_minus_1)\n                denominator = fxk - fxk_minus_1\n                # Check for stagnation, which would cause division by zero.\n                if denominator == 0:\n                    return xk\n                xk_plus_1 = xk - fxk * (xk - xk_minus_1) / denominator\n            \n            # Update iterates for the next step\n            xk_minus_1 = xk\n            xk = xk_plus_1\n            \n            # Termination condition 2: absolute function value\n            if abs(f(xk))  epsilon:\n                return xk\n                \n            # Switching condition: check if derivative magnitude exceeds threshold\n            if not use_newton:\n                if abs(f_prime(xk)) > tau:\n                    use_newton = True\n                    \n        # Termination condition 3: maximum iterations reached\n        return xk\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (0.0, 1.0, 1.4, 1e-12, 50),     # Case 1\n        (10.0, 12.0, 1.6, 1e-12, 100),   # Case 2\n        (-3.0, -2.0, 1.1, 1e-12, 50),   # Case 3\n        (0.0, 2.0, 1.5, 1e-12, 2)       # Case 4\n    ]\n\n    results = []\n    for case in test_cases:\n        x0, x1, tau, epsilon, n_max = case\n        root = hybrid_solver(x0, x1, tau, epsilon, n_max)\n        # Round the result to 10 decimal places as specified\n        results.append(round(root, 10))\n\n    # Final print statement in the exact required format.\n    # Using f-string formatting to avoid trailing zeros for integers.\n    print(f\"[{','.join(f'{r:.10f}' for r in results)}]\")\n\nsolve()\n```", "id": "3167328"}]}