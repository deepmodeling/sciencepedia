{"hands_on_practices": [{"introduction": "优化景观的理论知识只有在您亲手实现算法并观察其行为时才会变得具体。第一个练习将挑战您，在一个具有狭窄陡峭山谷和叠加波纹的函数上使用基本的梯度下降法，这些特征在现实世界的问题中很常见。通过比较固定步长和自适应步长，您将亲身体验到为何智能的步长控制不仅是理论上的改进，更是在复杂能量景观中导航、避免陷入局部最小值或发散的实际必需品 [@problem_id:3156569]。", "problem": "考虑由 $f(x,y)=x^2+100\\,y^2+\\sin(10\\,x)\\,\\sin(10\\,y)$ 定义的函数 $f:\\mathbb{R}^2\\rightarrow\\mathbb{R}$，其中所有角度均以弧度为单位。该函数结合了一个强各向异性的二次碗型结构和高频振荡，这导致了许多局部最小值的产生。在计算科学导论中，区分局部最小值与全局最小值，并设计一个能够穿过狭窄山谷而不被困住的步长方案是至关重要的。\n\n核心定义如下：\n- 如果存在点 $(x^\\star,y^\\star)$ 的一个邻域 $\\mathcal{N}$，使得对于所有 $(x,y)\\in\\mathcal{N}$，都有 $f(x^\\star,y^\\star)\\le f(x,y)$，则点 $(x^\\star,y^\\star)$ 是一个局部最小值。\n- 一个全局最小值满足对于所有 $(x,y)\\in\\mathbb{R}^2$，都有 $f(x^\\star,y^\\star)\\le f(x,y)$。\n- 基于梯度的方法通过以选定的步长沿最速下降方向 $-\\nabla f(x,y)$ 移动来更新 $(x,y)$。\n\n你的任务是在有界域 $[-1,1]\\times[-1,1]$ 上对函数 $f$ 实现两种优化方案，每次更新后投影回该域，以避免在合理的搜索区域之外出现无界增长：\n1. 固定步长梯度下降：使用固定的 $\\alpha$ 执行 $(x_{k+1},y_{k+1})=(x_k,y_k)-\\alpha\\,\\nabla f(x_k,y_k)$，然后将 $(x_{k+1},y_{k+1})$ 投影到 $[-1,1]\\times[-1,1]$ 上。\n2. 使用带有充分下降 (Armijo) 条件的回溯线搜索的自适应步长梯度下降：从一个初始步长 $\\alpha_0$ 开始，通过一个因子 $\\rho$ 不断缩减步长，直到满足 $f((x,y)-\\alpha\\,\\nabla f(x,y))\\le f(x,y)-c\\,\\alpha\\,\\|\\nabla f(x,y)\\|_2^2$，然后执行该步并将结果投影到 $[-1,1]\\times[-1,1]$ 上。\n\n你必须解析地计算梯度 $\\nabla f(x,y)$，并严格按照这些定义实现算法。$\\sin(\\cdot)$ 和 $\\cos(\\cdot)$ 函数中的角度必须按弧度处理。\n\n实现这两种方法，并在选定的一组初始条件和步长参数上比较它们的行为，以揭示狭窄山谷效应以及局部与全局最小值之间的区别。使用以下测试套件，其中指定了方法类型、步长参数、初始点 $(x_0,y_0)$ 和最大迭代次数 $K$：\n- 测试 1：固定步长下降，$\\alpha=0.02$，$(x_0,y_0)=(0.6,0.15)$，$K=1000$。\n- 测试 2：固定步长下降，$\\alpha=0.05$，$(x_0,y_0)=(0.6,0.15)$，$K=1000$。\n- 测试 3：固定步长下降，$\\alpha=0.0005$，$(x_0,y_0)=(0.6,0.15)$，$K=1000$。\n- 测试 4：自适应步长下降，$\\alpha_0=0.1$，$\\rho=0.5$，$c=10^{-4}$，$(x_0,y_0)=(0.6,0.15)$，$K=1000$。\n- 测试 5：自适应步长下降，$\\alpha_0=0.1$，$\\rho=0.5$，$c=10^{-4}$，$(x_0,y_0)=(0.0,0.9)$，$K=1000$。\n\n对于每个测试，运行算法直到达到最大迭代次数 $K$ 或梯度范数 $\\|\\nabla f(x,y)\\|_2$ 小于 $10^{-8}$ 为止。运行结束后，报告最终的目标函数值 $f(x_K,y_K)$，四舍五入保留六位小数。不涉及物理单位；角度以弧度为单位。\n\n你的程序应产生单行输出，其中包含一个用方括号括起来的逗号分隔列表的结果（例如，$[r_1,r_2,r_3,r_4,r_5]$），其中每个 $r_i$ 是对应上述顺序的测试 $i$ 的最终目标函数值，四舍五入到六位小数。", "solution": "用户提供了一个计算科学领域中有效且适定的问题。任务是在指定函数上实现并比较两种基于梯度下降的优化算法，并受限于域约束。\n\n### 问题阐述\n\n目标是找到定义如下的函数 $f: \\mathbb{R}^2 \\rightarrow \\mathbb{R}$ 的局部最小值：\n$$f(x,y) = x^2 + 100\\,y^2 + \\sin(10\\,x)\\,\\sin(10\\,y)$$\n优化在有界域 $\\mathcal{D} = [-1, 1] \\times [-1, 1]$ 上进行。函数 $f(x,y)$ 由两部分组成：一个二次碗型项 $x^2 + 100y^2$ 和一个振荡项 $\\sin(10x)\\sin(10y)$。该二次项创建了一个与 $x$ 轴对齐的狭长山谷，这一点可由该项的海森矩阵 $\\begin{pmatrix} 2  0 \\\\ 0  200 \\end{pmatrix}$ 证明，该矩阵具有 100 的大条件数。振荡项在这种底层结构上叠加了许多局部最小值，这使得简单的优化算法难以找到全局最小值。\n\n### 梯度计算\n\n基于梯度的方法需要目标函数的解析梯度 $\\nabla f$。关于 $x$ 和 $y$ 的偏导数计算如下：\n$$ \\frac{\\partial f}{\\partial x} = \\frac{\\partial}{\\partial x} (x^2 + 100\\,y^2 + \\sin(10\\,x)\\,\\sin(10\\,y)) = 2x + 10\\cos(10\\,x)\\sin(10\\,y) $$\n$$ \\frac{\\partial f}{\\partial y} = \\frac{\\partial}{\\partial y} (x^2 + 100\\,y^2 + \\sin(10\\,x)\\,\\sin(10\\,y)) = 200y + 10\\sin(10\\,x)\\cos(10\\,y) $$\n因此，梯度向量为：\n$$ \\nabla f(x,y) = \\begin{pmatrix} 2x + 10\\cos(10\\,x)\\sin(10\\,y) \\\\ 200y + 10\\sin(10\\,x)\\cos(10\\,y) \\end{pmatrix} $$\n所有三角函数都对以弧度为单位的角度进行运算。\n\n### 优化算法\n\n核心算法是投影梯度下降法。在每次迭代 $k$ 中，通过从当前点 $(x_k, y_k)$ 沿负梯度（最速下降）方向移动来找到一个候选点。然后将这个新点投影回可行域 $\\mathcal{D}$。迭代更新公式为：\n$$ (\\tilde{x}_{k+1}, \\tilde{y}_{k+1}) = (x_k, y_k) - \\alpha_k \\nabla f(x_k, y_k) $$\n$$ (x_{k+1}, y_{k+1}) = \\text{Proj}_{\\mathcal{D}}(\\tilde{x}_{k+1}, \\tilde{y}_{k+1}) $$\n其中 $\\alpha_k$ 是第 $k$ 次迭代的步长，$\\text{Proj}_{\\mathcal{D}}$ 是到盒子 $[-1, 1] \\times [-1, 1]$ 上的投影算子，定义为 $(\\text{clip}(x, -1, 1), \\text{clip}(y, -1, 1))$。\n\n实现了两种选择步长 $\\alpha_k$ 的策略：\n\n1.  **固定步长梯度下降**：所有迭代都使用固定的步长 $\\alpha_k = \\alpha$。$\\alpha$ 的选择至关重要：如果太大，算法可能会变得不稳定并发生发散；如果太小，收敛速度会慢得不切实际。该方法对函数的局部曲率很敏感，而在此问题中局部曲率变化显著。\n\n2.  **带回溯线搜索的自适应步长梯度下降**：步长 $\\alpha_k$ 在每次迭代中动态确定。从一个初始猜测值 $\\alpha_0$ 开始，步长通过一个因子 $\\rho \\in (0,1)$ 不断减小，直到满足 Armijo (或充分下降) 条件：\n    $$ f((x_k,y_k) - \\alpha \\nabla f(x_k,y_k)) \\le f(x_k,y_k) - c \\alpha \\|\\nabla f(x_k,y_k)\\|_2^2 $$\n    其中 $c$ 为一个小常数（例如，$10^{-4}$）。这确保了每一步在减小目标函数值方面都取得足够的进展，使得该算法比固定步长法更鲁棒，通常也更高效。根据规定，Armijo 条件在应用投影之前的无约束步上进行检查。\n\n### 实现与执行\n\n算法使用 `numpy` 库在 Python 中实现。对于五个指定的测试用例中的每一个，相应的算法都使用给定的起始点 $(x_0, y_0)$ 和参数进行初始化。迭代过程持续进行，直到达到最大迭代次数 $K$，或者梯度的 L2-范数 $\\|\\nabla f(x,y)\\|_2$ 低于 $10^{-8}$ 的容差。然后计算并报告在终止点的最终目标函数值 $f(x,y)$。对每个测试用例重复此过程，以比较不同参数化和方法的有效性。\n\n预期行为是，步长选择不当的固定步长方法（测试 1 和 2）将表现不佳，要么振荡，要么在域边界上卡住并得到一个很高的目标函数值。非常小的步长（测试 3）将导致进展缓慢。自适应步长方法（测试 4 和 5）预计将是鲁棒的，能够自动找到合适的步长并收敛到一个值较低的局部最小值，从而展示其在复杂优化景观中的优越性。测试 4 和 5 中不同的起始点可能会导致收敛到不同的局部最小值，这凸显了局部优化的路径依赖性。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and compares constant-step and adaptive-step projected gradient\n    descent algorithms for a given function and test suite.\n    \"\"\"\n\n    # Define the objective function f(p), its gradient grad_f(p), and the\n    # projection operator. p is a numpy array [x, y].\n    def f(p):\n        x, y = p\n        return x**2 + 100*y**2 + np.sin(10*x) * np.sin(10*y)\n\n    def grad_f(p):\n        x, y = p\n        df_dx = 2*x + 10*np.cos(10*x)*np.sin(10*y)\n        df_dy = 200*y + 10*np.sin(10*x)*np.cos(10*y)\n        return np.array([df_dx, df_dy])\n\n    def project(p):\n        return np.clip(p, -1, 1)\n\n    # Algorithm 1: Constant-step projected gradient descent\n    def constant_step_descent(alpha, p0, K, tol):\n        p = np.array(p0, dtype=float)\n        for _ in range(K):\n            grad = grad_f(p)\n            if np.linalg.norm(grad)  tol:\n                break\n            p_next_unprojected = p - alpha * grad\n            p = project(p_next_unprojected)\n        return f(p)\n\n    # Algorithm 2: Adaptive-step projected gradient descent with backtracking\n    def adaptive_step_descent(alpha0, rho, c, p0, K, tol):\n        p = np.array(p0, dtype=float)\n        for _ in range(K):\n            grad = grad_f(p)\n            grad_norm = np.linalg.norm(grad)\n            if grad_norm  tol:\n                break\n            \n            # Backtracking line search to satisfy the Armijo condition\n            alpha = alpha0\n            current_f_val = f(p)\n            grad_norm_sq = grad_norm**2\n            \n            while True:\n                p_candidate = p - alpha * grad\n                \n                # Check Armijo condition on the unprojected candidate point\n                if f(p_candidate) = current_f_val - c * alpha * grad_norm_sq:\n                    break\n                \n                alpha *= rho\n                \n                # Failsafe for pathologically small step sizes\n                if alpha  1e-16:\n                    break\n\n            p_next_unprojected = p - alpha * grad\n            p = project(p_next_unprojected)\n        return f(p)\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test 1: Constant-step, alpha=0.02\n        {'method': 'constant', 'params': {'alpha': 0.02, 'p0': (0.6, 0.15), 'K': 1000}},\n        # Test 2: Constant-step, alpha=0.05\n        {'method': 'constant', 'params': {'alpha': 0.05, 'p0': (0.6, 0.15), 'K': 1000}},\n        # Test 3: Constant-step, alpha=0.0005\n        {'method': 'constant', 'params': {'alpha': 0.0005, 'p0': (0.6, 0.15), 'K': 1000}},\n        # Test 4: Adaptive-step\n        {'method': 'adaptive', 'params': {'alpha0': 0.1, 'rho': 0.5, 'c': 1e-4, 'p0': (0.6, 0.15), 'K': 1000}},\n        # Test 5: Adaptive-step, different initial point\n        {'method': 'adaptive', 'params': {'alpha0': 0.1, 'rho': 0.5, 'c': 1e-4, 'p0': (0.0, 0.9), 'K': 1000}},\n    ]\n\n    results = []\n    TOLERANCE = 1e-8\n    \n    for case in test_cases:\n        params = case['params']\n        if case['method'] == 'constant':\n            res = constant_step_descent(\n                alpha=params['alpha'],\n                p0=params['p0'],\n                K=params['K'],\n                tol=TOLERANCE\n            )\n        else: # 'adaptive'\n            res = adaptive_step_descent(\n                alpha0=params['alpha0'],\n                rho=params['rho'],\n                c=params['c'],\n                p0=params['p0'],\n                K=params['K'],\n                tol=TOLERANCE\n            )\n        results.append(res)\n    \n    # Final print statement in the exact required format.\n    formatted_results = [f\"{res:.6f}\" for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3156569"}, {"introduction": "虽然自适应方法可以改进局部搜索，但它们无法从根本上解决陷入非全局最优的局部最小值陷阱的问题。本实践将向您介绍盆地跳跃（basin hopping），这是一种为解决此问题而设计的强大元启发式算法。您将实现一种策略，该策略将高效的局部优化程序与随机跳跃相结合，使您的搜索能够摆脱浅层盆地的引力，从而更广泛地探索整个景观，极大地增加定位真正全局最小值的机会 [@problem_id:3156501]。", "problem": "您需要编写一个完整且可运行的程序，实现一种盆地跳跃（basin hopping）元启发式算法，以研究二维景观上的局部最小值与全局最小值。目标函数为 $f(x,y) = \\sin(x) + \\sin(y) + 0.2(x^2 + y^2)$，其中正弦函数的所有角度参数必须以弧度为单位处理。计算任务是研究随机跳跃半径如何影响发现全局最小值的能力，实现方式为从随机点开始重复进行局部最小化，并与随机化跳跃交错进行。\n\n本问题的基础是以下经过充分检验的定义和事实：\n- 如果存在点 $(x^\\star,y^\\star)$ 的一个邻域 $U$，使得对于所有 $(x,y) \\in U$ 都有 $f(x^\\star,y^\\star) \\le f(x,y)$，则称点 $(x^\\star,y^\\star)$ 是可微函数 $f$ 的一个局部最小值。如果对于定义域中所有的 $(x,y)$ 都有 $f(x^\\star,y^\\star) \\le f(x,y)$，则称点 $(x^\\star,y^\\star)$ 是一个全局最小值。\n- 对于可微函数，局部最小化点必须满足一阶平稳性条件 $\\nabla f(x^\\star,y^\\star) = \\mathbf{0}$，并且二阶导数（Hessian 矩阵）提供影响局部最优性的局部曲率信息。\n- 盆地跳跃是一种元启发式算法，它将确定性的局部最小化与搜索空间中的随机跳跃交替进行，以逃离吸引盆（basins of attraction），从而提高找到全局最小值的机会。\n\n您的程序必须纯粹地用数学和逻辑术语实现以下内容：\n1. 在有界域 $[-6,6] \\times [-6,6]$ 上定义函数 $f(x,y) = \\sin(x) + \\sin(y) + 0.2(x^2 + y^2)$。\n2. 使用一个遵循给定边界的、基于梯度的例程来实现 $f$ 的局部最小化。正弦函数的导数必须在角度以弧度为单位的情况下计算。\n3. 实现采用贪婪接受准则的盆地跳跃：从一个随机初始点 $(x_0,y_0)$ 开始，执行局部最小化以获得一个局部最小值 $(x_\\mathrm{loc},y_\\mathrm{loc})$，然后从以 $(x_\\mathrm{loc},y_\\mathrm{loc})$ 为中心、半径为 $r$ 的圆盘上均匀分布地提出一个随机跳跃，将跳跃后的点裁剪到域 $[-6,6] \\times [-6,6]$ 内，然后从该裁剪点开始进行局部最小化。仅当新的局部最小值严格改进了相对于当前局部最小值的目标值时，才接受它；否则，保留当前的局部最小值。将此过程重复固定次数的步骤。\n4. 使用一维多起点局部最小化来计算 $g(t) = \\sin(t) + 0.2 t^2$ 在 $[-6,6]$ 上的全局最小值的精确近似，并利用可分离性推断 $f$ 的全局最小值。具体来说，计算使 $g$ 最小化的 $t^\\star \\in [-6,6]$，那么 $f$ 的全局最小值出现在 $(t^\\star,t^\\star)$ 处，其值为 $f^\\star = 2 g(t^\\star)$。\n5. 使用具有确定性种子的伪随机数生成器（PRNG）以确保可复现性。每个测试用例必须使用从其参数确定性派生的种子。您的程序中使用的所有随机角度和均匀变量必须分别以弧度和无量纲单位表示。\n6. 对于每个测试用例，统计有多少次独立的盆地跳跃运行最终达到了全局最小值（在规定的容差范围内），并将此计数报告为整数。\n\n角度单位规范：\n- 所有三角函数求值和角度采样都必须以弧度为单位。\n\n测试套件包含五个测试用例，它们改变跳跃半径 $r$ 以测试算法的不同方面：\n- 情况 1（边界情况）：$r = 0.0$（等同于没有跳跃的纯局部最小化）。\n- 情况 2（小跳跃）：$r = 0.5$。\n- 情况 3（中等跳跃）：$r = 1.5$。\n- 情况 4（大跳跃）：$r = 3.0$。\n- 情况 5（非常大的跳跃）：$r = 5.0$。\n\n所有测试用例的通用参数：\n- 域边界：$[-6,6] \\times [-6,6]$。\n- 每次运行的盆地跳跃步数：$N = 60$。\n- 每个测试用例的独立运行次数：$R = 30$。\n- 全局最优性容差：如果最终目标值与计算出的全局最小值 $f^\\star$ 之差在 $\\varepsilon = 10^{-6}$ 以内，则声明成功。\n\n最终输出规范：\n- 您的程序应生成单行输出，其中包含一个方括号括起来的逗号分隔列表形式的结果。每个测试用例的结果是 $R$ 次运行中成功次数的整数计数，按上述测试用例的顺序排列。例如，输出格式为 $[s_1,s_2,s_3,s_4,s_5]$，其中每个 $s_i$ 是一个整数。\n\n科学真实性和约束：\n- 盆地跳跃必须在半径为 $r$ 的圆盘上进行面积均匀采样。为实现均匀性，将半径采样为 $r \\sqrt{u}$，其中 $u$ 在 $[0,1]$ 中均匀分布，角度在 $[0,2\\pi]$ 弧度范围内均匀分布。\n- 局部最小化必须使用梯度信息，并通过有界方法强制执行域边界。\n- 给定每个测试用例的固定种子，所有计算必须是确定性的。\n\n您的程序必须是自包含的，不需要外部输入，并严格遵守指定的最终输出格式。", "solution": "该问题要求实现一种盆地跳跃元启发式算法，以探索一个二维函数的能量景观，并评估算法寻找全局最小值的能力如何受到其随机跳跃大小的影响。解决方案是系统地构建的，首先分析目标函数以确定真实的全局最小值，然后实现指定的盆地跳跃算法。\n\n目标函数为 $f(x,y) = \\sin(x) + \\sin(y) + 0.2(x^2 + y^2)$，定义域为 $(x,y) \\in [-6,6] \\times [-6,6]$。该函数的一个关键特性是其可分离性，即它可以表示为两个独立的一维函数之和：$f(x,y) = g(x) + g(y)$，其中 $g(t) = \\sin(t) + 0.2t^2$。此特性意味着 $f(x,y)$ 的全局最小值出现在点 $(t^\\star, t^\\star)$ 处，其中 $t^\\star$ 是使 $g(t)$ 在区间 $[-6,6]$ 上达到全局最小值的 $t$ 值。\n\n首先，我们必须精确确定这个全局最小值，它将作为成功的基准。$g(t)$ 的最小值点要么是导数 $g'(t) = \\cos(t) + 0.4t$ 为零的驻点，要么是边界点 $t = -6$ 和 $t = 6$。数值研究表明，$g(t)$ 在区间 $[-6,6]$ 内有两个局部最小值。按照规定，我们使用多起点局部最小化方法来寻找 $g(t)$ 的全局最小值。我们在包含这些局部最小值的子区间（例如 $[-6,0]$ 和 $[0,6]$）上执行有界一维最小化，并将结果值与定义域边界处的函数值 $g(-6)$ 和 $g(6)$进行比较。此过程能稳健地确定 $g(t)$ 在 $t^\\star \\approx -1.30644$ 处的全局最小值，其值为 $g(t^\\star) \\approx -0.62638$。因此，二维函数 $f(x,y)$ 的全局最小值为 $f^\\star = f(t^\\star, t^\\star) = 2g(t^\\star) \\approx -1.252767$。这个值将是我们优化运行的目标。\n\n问题的核心是盆地跳跃算法。这是一种随机全局优化技术，旨在克服局部搜索方法的主要弱点：陷入局部最小值。该算法通过迭代应用两个阶段来运作：一个确定性的局部最小化和一个到新起点的随机“跳跃”或扰动。\n\n对于每个由特定跳跃半径 $r$ 定义的测试用例，实现过程如下：\n1.  进行一组 $R=30$ 次的独立运行，以确保结果具有统计意义。每次运行都使用从测试用例和运行索引派生的确定性种子进行初始化，以保证可复现性。\n2.  每次运行开始时，从定义域 $[-6,6] \\times [-6,6]$ 中均匀选择一个随机起始点 $(x_0, y_0)$。从该点开始进行局部最小化，找到第一个局部最小值，它成为初始找到的“最优”解 $(x_{\\text{best}}, y_{\\text{best}})$。\n3.  局部最小化使用基于梯度的拟牛顿法，特别是带箱型约束的有限内存 Broyden–Fletcher–Goldfarb–Shanno 算法（`L-BFGS-B`）。该方法效率高且遵守定义域边界，符合要求。它使用 $f(x,y)$ 的解析梯度（雅可比矩阵），即 $\\nabla f(x,y) = (\\cos(x)+0.4x, \\cos(y)+0.4y)$，以实现更快的收敛速度。\n4.  然后，主盆地跳跃循环执行 $N=60$ 步。在每一步中：\n    a. 通过从当前最优最小值 $x_{\\text{best}}$ 进行随机跳跃来生成一个新的候选点。跳跃从半径为 $r$ 的圆盘中均匀采样。为实现这一点，跳跃幅度 $\\rho$ 采样为 $\\rho = r \\sqrt{u}$，其中 $u \\sim U(0,1)$，跳跃角度 $\\theta$ 从 $\\theta \\sim U(0, 2\\pi)$ 中采样。新点为 $x_{\\text{prop}} = x_{\\text{best}} + (\\rho\\cos\\theta, \\rho\\sin\\theta)$。\n    b. 对提出的点 $x_{\\text{prop}}$ 进行裁剪，以确保其保持在定义域 $[-6,6] \\times [-6,6]$ 内。\n    c. 从这个裁剪后的点开始进行新的局部最小化，得到一个新的局部最小值 $x_{\\text{new}}$，其函数值为 $f_{\\text{new}}$。\n    d. 应用贪婪接受准则：如果 $f_{\\text{new}}  f_{\\text{best}}$，则接受新的最小值作为当前最优解。否则，算法停留在之前的最优最小值处。这确保了在局部最小值构成的景观上的搜索轨迹在能量上是单调递减的。\n5.  $N=60$ 步之后，将最终函数值 $f_{\\text{best}}$ 与预先计算的全局最小值 $f^\\star$进行比较。如果 $|f_{\\text{best}} - f^\\star|  \\varepsilon$，其中容差 $\\varepsilon=10^{-6}$，则该次运行被计为一次成功。\n6.  对于给定的跳跃半径 $r$，记录 $R=30$ 次运行中成功的总次数。\n\n对五个指定的半径（$r=0.0, 0.5, 1.5, 3.0, 5.0$）中的每一个重复整个过程，并将得到的成功计数汇总到一个最终列表中。$r=0.0$ 的情况作为对照组，对应于没有跳跃的多起点局部搜索。不同的半径旨在探究在寻找全局最小值过程中，探索（大跳跃）和利用（小跳跃）之间的权衡。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import minimize, minimize_scalar\n\ndef solve():\n    \"\"\"\n    Implements a basin hopping metaheuristic to find the global minimum of a 2D function.\n    The program evaluates the success rate for different jump radii.\n    \"\"\"\n    # Common parameters for all test cases\n    N_STEPS = 60\n    N_RUNS = 30\n    DOMAIN_BOUNDS_1D = (-6, 6)\n    DOMAIN_BOUNDS_2D = [(-6, 6), (-6, 6)]\n    GLOBAL_TOLERANCE = 1e-6\n    TEST_CASES_R = [0.0, 0.5, 1.5, 3.0, 5.0]\n\n    # --- Step 1: Define objective function and its gradient ---\n    def f(p):\n        \"\"\"The 2D objective function f(x, y).\"\"\"\n        x, y = p\n        return np.sin(x) + np.sin(y) + 0.2 * (x**2 + y**2)\n\n    def jac_f(p):\n        \"\"\"The Jacobian (gradient) of the objective function.\"\"\"\n        x, y = p\n        return np.array([np.cos(x) + 0.4 * x, np.cos(y) + 0.4 * y])\n\n    # --- Step 2: Compute the true global minimum value ---\n    def g(t):\n        \"\"\"The 1D component of the separable function f(x, y).\"\"\"\n        return np.sin(t) + 0.2 * t**2\n\n    # Use multi-start local minimization for the 1D function g(t)\n    # Search for minima in sub-intervals based on analysis of g'(t).\n    res1 = minimize_scalar(g, bounds=(-3, 0), method='bounded')\n    res2 = minimize_scalar(g, bounds=(2, 5), method='bounded')\n    \n    # Consider the domain endpoints as potential minima\n    g_at_endpoints = [g(DOMAIN_BOUNDS_1D[0]), g(DOMAIN_BOUNDS_1D[1])]\n\n    # The global minimum of g(t) is the minimum of all found local minima and endpoint values.\n    global_min_g = min(res1.fun, res2.fun, *g_at_endpoints)\n    \n    # The global minimum of f(x,y) is twice the global minimum of g(t).\n    f_star = 2 * global_min_g\n\n    # --- Step 3: Run basin hopping for each test case ---\n    results = []\n    for i, r in enumerate(TEST_CASES_R):\n        success_count = 0\n        for j in range(N_RUNS):\n            # Use a deterministic seed for reproducibility, unique for each run.\n            seed = i * N_RUNS + j\n            rng = np.random.default_rng(seed)\n\n            # Start from a random initial point within the domain.\n            x0 = rng.uniform(DOMAIN_BOUNDS_1D[0], DOMAIN_BOUNDS_1D[1], size=2)\n\n            # Perform an initial local minimization.\n            res = minimize(f, x0, jac=jac_f, method='L-BFGS-B', bounds=DOMAIN_BOUNDS_2D)\n            x_best = res.x\n            f_best = res.fun\n\n            # Perform N basin hopping steps.\n            for _ in range(N_STEPS):\n                # Propose a random jump, sampled uniformly from a disk of radius r.\n                jump_magnitude = r * np.sqrt(rng.uniform(0, 1))\n                jump_angle = rng.uniform(0, 2 * np.pi)\n                \n                jump_vector = np.array([jump_magnitude * np.cos(jump_angle), \n                                        jump_magnitude * np.sin(jump_angle)])\n                \n                x_proposed = x_best + jump_vector\n                \n                # Clip the new point to stay within the domain.\n                x_proposed_clipped = np.clip(x_proposed, DOMAIN_BOUNDS_1D[0], DOMAIN_BOUNDS_1D[1])\n\n                # Perform a local minimization from the new point.\n                res_new = minimize(f, x_proposed_clipped, jac=jac_f, method='L-BFGS-B', bounds=DOMAIN_BOUNDS_2D)\n                \n                # Greedy acceptance criterion: accept only if strictly better.\n                if res_new.fun  f_best:\n                    x_best = res_new.x\n                    f_best = res_new.fun\n\n            # After all steps, check if the found minimum is the global minimum.\n            if abs(f_best - f_star)  GLOBAL_TOLERANCE:\n                success_count += 1\n                \n        results.append(success_count)\n\n    # --- Step 4: Final Output ---\n    # Print the success counts for each test case in the specified format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3156501"}, {"introduction": "处理复杂优化景观的另一种完全不同且优雅的策略是连续介质法（continuation method），您将在最后的这个实践中进行探索。您将不会试图一次性征服一个困难的景观，而是从一个问题的极简版本（傅里叶级数中包含较少的模式）开始，并找到其简单的解。然后，您将逐步增加模型的复杂度，将每个阶段的解作为下一个更复杂阶段的起点，从而在景观变形和新局部最小值出现时有效地“追踪”全局最小值 [@problem_id:3156481]。", "problem": "编写一个完整且可运行的程序，来演示局部最小值（local minima）的出现如何依赖于截断级数（truncated series）中傅里叶模式（Fourier modes）的数量，以及模型复杂度（model complexity）中的连续性策略（continuation strategy）如何帮助在模式增加时跟踪最小值。该程序必须构建一个在周期域（periodic domain）上定义的单参数函数族，并使用有原则的定义（principled definitions）对其进行分析。\n\n使用以下基于核心定义的设置：\n- 令 $x$ 表示定义域 $[0,2\\pi)$ 上的一个角度，单位为弧度。\n- 对于一个正整数 $M$，定义截断傅里叶级数\n$$\nf_M(x) \\equiv \\sum_{k=1}^{M} \\frac{\\cos(k x)}{k}.\n$$\n- 如果存在 $\\delta0$ 使得对于所有满足 $|x-x^\\star|\\delta$ 的 $x$ 都有 $f_M(x^\\star) \\le f_M(x)$，则点 $x^\\star$ 是 $f_M$ 的一个局部最小值。全局最小值是点 $x^\\dagger$，使得对于整个定义域中的所有 $x$ 都有 $f_M(x^\\dagger) \\le f_M(x)$。\n\n基于基本原理要实现的核心任务：\n1) 使用一个包含 $N$ 个点的均匀网格（使用 $N=8192$）将定义域 $[0,2\\pi)$ 离散化，并通过在周期性边界条件下与直接相邻点进行离散比较，来数值化地检测 $f_M$ 的不同局部最小值的数量。具体来说，对网格点 $x_i$ 使用离散准则：如果在索引 $i$ 处满足 $f_M(x_i)  f_M(x_{i-1})$ 和 $f_M(x_i)  f_M(x_{i+1})$，则检测到一个离散局部最小值，其中索引对 $N$ 取模以强制周期性。当 $N$ 足够大时，这为连续局部最小值的数量提供了一个稳健的近似。所有角度都必须以弧度处理。\n2) 实现带有回溯线搜索（backtracking line search）的梯度下降法（gradient descent），从给定的初始条件开始寻找 $f_M$ 的一个局部最小化子（local minimizer）。使用解析梯度\n$$\n\\frac{d}{dx} f_M(x) \\;=\\; -\\sum_{k=1}^{M} \\sin(kx)\n$$\n对 $x$ 执行有原则的最速下降步骤（steepest-descent step）（步进方向是负梯度）。确保步骤能够减小目标函数值（Armijo型充分下降），并在每次更新后将 $x$ 对 $2\\pi$ 取模以保持在定义域内。当梯度幅值足够小或达到最大迭代预算时停止。\n3) 在复杂度参数 $M$ 上执行连续性策略（continuation）：从最粗糙的模型 $M_1=1$ 开始，并使用在离散化网格上的穷举搜索（exhaustive grid search）找到其全局最小化子。然后，对于测试套件中每个后续的 $M_j$（严格递增），将为 $M_{j-1}$ 获得的最小化子用作对 $f_{M_j}$ 进行梯度下降的初始条件。这种连续性策略旨在应对随着 $M$ 增加而出现的新局部最小值。\n4) 对于测试套件中的每个 $M_j$，确定：\n   - 在网格上检测到的 $f_{M_j}$ 的局部最小值数量。\n   - 一个成功指示符，定义如下：计算 $M_j$ 在网格上的全局最小值，并将其与从前一阶段初始化的梯度下降返回的点所达到的值进行比较。如果获得的值在网格全局最小值的一个小的非负容差 $\\varepsilon$ （使用 $\\varepsilon = 10^{-9}$）之内，则记录成功为 $1$，否则记录为 $0$。对于 $M_1=1$，按照约定将成功指示符定义为 $1$，因为该过程在最粗糙模型的全局最小化子处初始化。\n\n角度单位要求：\n- 所有角度必须以弧度为单位。\n\n测试套件以涵盖不同方面：\n- 所有计算均使用 $N=8192$ 个网格点。\n- 使用模式计数序列 $M \\in \\{\\,1,2,5,10,20\\,\\}$。这涵盖了：\n  - 一个具有单个最小值的基本情况（$M=1$）。\n  - 出现额外局部最小值的中间情况（$M=2,5$）。\n  - 具有许多局部最小值的更复杂情况（$M=10,20$）。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果。\n- 对于指定顺序中的每个测试值 $M$，输出一个双元素列表 $[n_M,s_M]$，其中 $n_M$ 是检测到的局部最小值的数量（一个整数），$s_M$ 是成功指示符（$0$ 或 $1$）。\n- 因此，最终打印的行必须看起来像\n$[[n_{1},s_{1}],[n_{2},s_{2}],[n_{5},s_{5}],[n_{10},s_{10}],[n_{20},s_{20}]]$\n，不带任何附加文本。所有值都是无量纲的，角度以弧度为单位。", "solution": "该问题要求对由截断傅里叶级数定义的单参数函数族 $f_M(x)$ 进行分析。目标是编程研究局部最小值的数量如何随着复杂度参数 $M$（模式数）的增加而变化，并评估一种用于在这种变化的地形中跟踪全局最小值的连续性策略。\n\n所考虑的函数定义在周期域 $x \\in [0, 2\\pi)$ 上，形式为：\n$$\nf_M(x) \\equiv \\sum_{k=1}^{M} \\frac{\\cos(k x)}{k}\n$$\n其中 $M$ 是一个正整数。我们将在该域的离散化版本上进行分析，该版本由一个包含 $N=8192$ 个点的均匀网格表示，记作 $\\{x_i\\}_{i=0}^{N-1}$，其中 $x_i = \\frac{2\\pi i}{N}$。\n\n核心任务基于以下原则实现：\n\n1. **局部最小值的数值检测：** 如果在 $x^\\star$ 的一个邻域内有 $f_M(x^\\star) \\le f_M(x)$，则点 $x^\\star$ 是一个局部最小值。在我们的离散网格上，我们通过识别一个点 $x_i$ 来近似这一点，如果该点的函数值严格小于其直接相邻点的函数值，则该点为离散局部最小值。为了尊重域的周期性，相邻点的索引对 $N$ 取模。也就是说，如果点 $x_i$ 满足条件：\n$$\nf_M(x_i)  f_M(x_{i-1 \\pmod N}) \\quad \\text{and} \\quad f_M(x_i)  f_M(x_{i+1 \\pmod N})\n$$\n则它是一个被检测到的局部最小值。此类点的总数 $n_M$ 为给定的 $M$ 提供了局部最小值数量的数值估计。对于足够大的 $N$，这个离散计数是连续函数最小值数量的可靠近似。\n\n2. **梯度下降优化：** 为了从给定的起始点找到一个最小化子，我们采用梯度下降算法。这种迭代方法通过沿负梯度方向（即最速下降方向）采取步骤来寻求最小化 $f_M(x)$。$f_M(x)$ 的解析梯度为：\n$$\n\\frac{d}{dx} f_M(x) = -\\sum_{k=1}^{M} \\sin(kx)\n$$\n一次梯度下降迭代根据规则将当前点 $x_{j}$ 更新为 $x_{j+1}$：\n$$\nx_{j+1} = \\left( x_j - t_j \\left( \\sum_{k=1}^{M} \\sin(kx_j) \\right) \\right) \\pmod{2\\pi}\n$$\n其中 $t_j  0$ 是步长。新位置对 $2\\pi$ 取模以保持在定义域内。\n\n步长 $t_j$ 使用回溯线搜索来确定，以确保取得足够的进展。这包括从一个试验步长开始，并迭代地减小它，直到满足 Armijo 条件：\n$$\nf_M(x_{j+1}) \\le f_M(x_j) + \\alpha t_j \\nabla f_M(x_j)^T p_j\n$$\n其中 $p_j = -\\nabla f_M(x_j)$ 是下降方向，$\\alpha \\in (0, 1)$ 是一个控制参数（例如 $\\alpha=0.3$）。这个条件保证了相对于步长和方向导数，该步骤在函数值上提供了足够的减小。当梯度的幅值低于一个小容差或达到最大迭代次数时，算法终止。\n\n3. **模型复杂度中的连续性策略：** 随着 $M$ 的增加，函数 $f_M(x)$ 会产生更多的局部最小值，这使得像梯度下降这样的局部搜索方法从任意起点找到全局最小值变得具有挑战性。连续性策略试图通过利用来自更简单模型的解来缓解这个问题。该过程如下：\n- 对于基本情况 $M_1=1$，函数 $f_1(x) = \\cos(x)$很简单，在 $x=\\pi$ 处有唯一的全局最小值。这个最小化子是通过在离散网格上进行穷举搜索找到的。\n- 对于每个后续更复杂的模型 $M_j$（$j1$），将为前一个模型 $M_{j-1}$ 找到的最小化子用作应用于 $f_{M_j}$ 的梯度下降算法的初始猜测。其原理是，对于模型复杂度的微小增量，全局最小值的位置预计只会略有移动，因此前一个解提供了一个很好的起点。\n\n4. **连续性策略的评估：** 为了评估这种连续性策略的有效性，我们定义一个成功指示符 $s_M$。对于每个 $M$，我们将我们基于连续性策略的梯度下降所找到的点处的函数值 $f_M(x_{\\text{GD}}^\\star)$ 与网格上的真实全局最小值 $f_M^\\dagger = \\min_{i} f_M(x_i)$进行比较。如果找到的最小值足够接近全局最小值，则该策略被认为是成功的：\n$$\ns_M = \\begin{cases} 1  \\text{if } f_M(x_{\\text{GD}}^\\star) \\le f_M^\\dagger + \\varepsilon \\\\ 0  \\text{otherwise} \\end{cases}\n$$\n其中 $\\varepsilon$ 是一个小的非负容差，设置为 $10^{-9}$。对于基本情况 $M=1$，按照约定定义成功，因此 $s_1=1$。这个度量标准揭示了连续性方法是成功地跟踪了全局最小值，还是陷入了局部最小值。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Analyzes a family of functions f_M(x) to demonstrate the emergence of\n    local minima and the use of a continuation strategy.\n    \"\"\"\n    # Define problem parameters from the statement.\n    N = 8192\n    M_VALUES = [1, 2, 5, 10, 20]\n    EPSILON = 1e-9\n\n    # Pre-compute the grid for x.\n    # The domain is [0, 2*pi), so endpoint is False.\n    x_grid = np.linspace(0, 2 * np.pi, N, endpoint=False)\n    \n    # Store the results for each M.\n    results = []\n    \n    # This variable will store the minimizer from the previous stage (M_{j-1})\n    # to be used as the starting point for the current stage (M_j).\n    x_continuation_min = 0.0\n\n    def f_M(x, M):\n        \"\"\"\n        Computes the function f_M(x) = sum_{k=1 to M} cos(k*x)/k.\n        This function is vectorized to handle an array of x values.\n        \"\"\"\n        k = np.arange(1, M + 1)\n        # Use broadcasting for vectorized computation.\n        # x must be a column vector for this to work with a row vector k.\n        if x.ndim == 1:\n            x_col = x[:, np.newaxis]\n        else: # Handle scalar input for gradient descent\n            x_col = np.array([[x]])\n        \n        cos_terms = np.cos(k * x_col) / k\n        # Sum over the k-axis (axis=1)\n        return np.sum(cos_terms, axis=1).squeeze()\n\n    def grad_f_M(x_scalar, M):\n        \"\"\"\n        Computes the gradient of f_M(x) at a single point x.\n        The gradient is -sum_{k=1 to M} sin(k*x).\n        \"\"\"\n        k = np.arange(1, M + 1)\n        return np.sum(-np.sin(k * x_scalar))\n\n    def gradient_descent(x_start, M):\n        \"\"\"\n        Performs gradient descent with backtracking line search to find a local\n        minimum of f_M(x).\n        \"\"\"\n        # Hyperparameters for the optimization algorithm.\n        max_iter = 1000\n        grad_tol = 1e-8\n        alpha = 0.3  # Armijo condition control parameter\n        beta = 0.8   # Backtracking step size reduction factor\n\n        x_current = x_start\n\n        for _ in range(max_iter):\n            # Calculate gradient at the current point.\n            g = grad_f_M(x_current, M)\n\n            # Stop if the gradient is sufficiently small.\n            if np.abs(g)  grad_tol:\n                break\n\n            # Set descent direction (negative gradient).\n            p_k = -g\n            \n            # --- Backtracking Line Search ---\n            t = 1.0  # Initial step size\n            f_current = f_M(np.array([x_current]), M)\n            \n            while True:\n                x_next = x_current + t * p_k\n                # Function f_M is periodic, so modulo is not strictly necessary\n                # for evaluation but good practice.\n                f_next = f_M(np.array([x_next]), M)\n                # Armijo condition: f(x+tp) = f(x) + alpha*t*grad(f)^T*p\n                # Here p = -grad(f), so grad(f)^T*p = -|grad(f)|^2\n                if f_next = f_current - alpha * t * g * g:\n                    break\n                t = beta * t\n\n            # --- Update Step ---\n            # Update position and wrap around the 2*pi domain.\n            x_current = (x_current + t * p_k) % (2 * np.pi)\n            \n        return x_current\n\n    # Main loop to iterate through the specified values of M.\n    for M in M_VALUES:\n        # 1. Evaluate the function f_M on the entire grid.\n        y_grid = f_M(x_grid, M)\n\n        # 2. Detect and count the number of local minima on the grid.\n        # A point is a local minimum if it's smaller than its left and right neighbors.\n        # np.roll handles the periodic boundary conditions.\n        is_local_min = (y_grid  np.roll(y_grid, 1))  (y_grid  np.roll(y_grid, -1))\n        n_M = int(np.sum(is_local_min))\n\n        s_M = 0  # Default to failure (0).\n\n        if M == 1:\n            # For the base case M=1, find the global minimizer by exhaustive grid search.\n            min_idx = np.argmin(y_grid)\n            x_continuation_min = x_grid[min_idx]\n            # Success is defined as 1 for the base case.\n            s_M = 1\n        else:\n            # Continuation: use the minimizer from the previous M as the starting point.\n            x_start = x_continuation_min\n            x_gd_min = gradient_descent(x_start, M)\n            \n            # The result of this optimization becomes the starting point for the next M.\n            x_continuation_min = x_gd_min\n\n            # 4. Evaluate success by comparing the found value with the grid's global minimum.\n            grid_global_min_val = np.min(y_grid)\n            val_at_gd_min = f_M(np.array([x_gd_min]), M)\n\n            if val_at_gd_min = grid_global_min_val + EPSILON:\n                s_M = 1\n        \n        results.append([n_M, s_M])\n\n    # Print the final result in the specified format: [[n1,s1],[n2,s2],...].\n    # Using str() on a list of lists and removing spaces to match the format precisely.\n    print(str(results).replace(\" \", \"\"))\n\nsolve()\n```", "id": "3156481"}]}