{"hands_on_practices": [{"introduction": "首先，我们需要一种方法来量化迭代方法的收敛速度。Q-线性收敛是衡量这种速度的一个基本标准。本练习将通过一个简单仿射映射的例子，帮助你巩固这一概念。你将推导其收敛率，并将这个理论值与两种实用的数值估计方法联系起来：一种是计算连续误差的比率，另一种是通过分析误差对数图的斜率。这个练习为你搭建了从理论到实践的桥梁 [@problem_id:3113939]。", "problem": "考虑实数线上的一个不动点迭代，定义为 $x_{k+1} = g(x_k)$，其中 $g$ 是一个从 $\\mathbb{R}$ 到 $\\mathbb{R}$ 的函数。假设 $g$ 是关于标准绝对值度量的压缩映射，即存在一个常数 $L$ 满足 $0  L  1$，使得对于所有 $x,y \\in \\mathbb{R}$，不等式 $|g(x) - g(y)| \\le L |x - y|$ 成立。令 $x_\\star \\in \\mathbb{R}$ 表示 $g$ 的不动点，它满足 $g(x_\\star) = x_\\star$。定义第 $k$ 次迭代的误差为 $e_k = x_k - x_\\star$，其大小为 $|e_k|$。\n\n任务：\n- 仅从压缩性质和不动点迭代的定义出发，推导一个关联 $|e_{k+1}|$ 和 $|e_k|$ 的不等式。利用此不等式论证该迭代表现出商线性（Q-线性）收敛，其中商线性（Q-线性）收敛意味着当 $k \\to \\infty$ 时，比率序列 $|e_{k+1}| / |e_k|$ 收敛到一个常数。然后，对于特定的仿射映射 $g(x) = a x + b$（其中 $|a|  1$），用 $a$ 和 $b$ 来确定商线性因子，并说明单步误差不等式中等号成立的条件。\n- 将压缩因子与 $\\log(|e_k|)$ 对 $k$ 的（自然对数）图像的斜率联系起来。利用对数和序列的基本性质，解释该斜率如何与商线性因子相关。\n- 实现一个程序，对于一组仿射压缩映射 $g(x) = a x + b$，生成误差序列，并通过两种方式数值估计压缩因子：(i) 使用序列尾部的比率 $|e_{k+1}| / |e_k|$，以及 (ii) 对 $\\log(|e_k|)$ 与 $k$ 进行最小二乘拟合，并将拟合的斜率取指数以恢复因子。将两种数值估计与理论 Lipschitz 常数 $L$进行比较。\n\n使用以下参数集 $(a,b,x_0,N)$ 的测试套件，其中 $x_0$ 是初始迭代值， $N$ 是迭代次数：\n- 测试 1：$(a,b,x_0,N) = (0.5, 1.0, 10.0, 30)$\n- 测试 2：$(a,b,x_0,N) = (0.9, -2.0, 0.0, 60)$\n- 测试 3：$(a,b,x_0,N) = (-0.75, 3.0, -5.0, 50)$\n- 测试 4：$(a,b,x_0,N) = (0.2, 0.0, 100.0, 40)$\n\n对于每个测试，解析地计算不动点 $x_\\star$，通过 $x_{k+1} = a x_k + b$ 生成序列 $(x_k)_{k=0}^{N}$，计算误差大小 $|e_k| = |x_k - x_\\star|$，从最后一对有效的连续非零误差中估计基于比率的因子，通过对所有严格正误差的迭代（以避免计算 $\\log(0)$）进行 $\\log(|e_k|)$ 相对于迭代索引 $k$ 的最小二乘线性拟合来估计基于斜率的因子，然后对拟合的斜率取指数。仿射映射的理论 Lipschitz 常数是 $L = |a|$。\n\n最终输出格式：\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果。每个测试用例的结果必须是按 $[L,\\widehat{L}_{\\mathrm{ratio}},\\widehat{L}_{\\mathrm{slope}}]$ 顺序排列的三个浮点数列表，其中 $L$ 是理论 Lipschitz 常数，$\\widehat{L}_{\\mathrm{ratio}}$ 是经验尾部比率估计，$\\widehat{L}_{\\mathrm{slope}}$ 是拟合斜率的指数。例如，输出应类似于 $[[L_1,\\widehat{L}_{\\mathrm{ratio},1},\\widehat{L}_{\\mathrm{slope},1}],[L_2,\\widehat{L}_{\\mathrm{ratio},2},\\widehat{L}_{\\mathrm{slope},2}],\\ldots]$。不涉及任何物理单位或角度；所有报告的值都是无量纲实数。", "solution": "该问题要求分析一维不动点迭代 $x_{k+1} = g(x_k)$ 的收敛性质，重点关注仿射映射 $g(x) = ax+b$ 的特殊情况。我们将首先推导基本误差不等式，然后将收敛速率与误差的对数图联系起来，最后概述一个数值验证过程。\n\n### 第一部分：单步误差不等式与Q-线性收敛\n\n不动点迭代由序列 $x_{k+1} = g(x_k)$ 定义，其中 $k = 0, 1, 2, \\ldots$。不动点用 $x_\\star$ 表示，满足方程 $g(x_\\star) = x_\\star$。第 $k$ 次迭代的误差定义为 $e_k = x_k - x_\\star$。我们关心的是误差大小 $|e_k|$ 的行为。\n\n第 $k+1$ 步的误差大小由 $|e_{k+1}| = |x_{k+1} - x_\\star|$ 给出。通过代入 $x_{k+1}$ 的迭代定义和不动点的等价定义 $x_\\star = g(x_\\star)$，我们得到：\n$$|e_{k+1}| = |g(x_k) - g(x_\\star)|$$\n问题陈述 $g$ 是关于 $\\mathbb{R}$ 上标准绝对值度量的压缩映射。这意味着存在一个 Lipschitz 常数 $L$，使得 $0  L  1$ 且对于所有 $x, y \\in \\mathbb{R}$：\n$$|g(x) - g(y)| \\le L |x - y|$$\n将此不等式应用于 $x = x_k$ 和 $y = x_\\star$，我们得到：\n$$|g(x_k) - g(x_\\star)| \\le L |x_k - x_\\star|$$\n注意到 $|x_k - x_\\star| = |e_k|$，我们便得到了单步误差不等式：\n$$|e_{k+1}| \\le L |e_k|$$\n这个不等式表明，每次迭代中误差大小保证至少减小一个因子 $L$。由于 $0  L  1$，这确保了 $\\lim_{k \\to \\infty} |e_k| = 0$，因此迭代序列 $(x_k)$ 收敛于不动点 $x_\\star$。\n\n如果误差比率序列收敛到一个常数 $C \\in [0, 1)$，则确立了商线性（Q-线性）收敛：\n$$\\lim_{k \\to \\infty} \\frac{|e_{k+1}|}{|e_k|} = C$$\n常数 $C$ 被称为商线性因子或收敛速率。\n\n对于仿射映射 $g(x) = ax + b$（其中 $|a|  1$）的特殊情况，我们可以建立一个关于误差传播的精确关系。不动点 $x_\\star$ 解出 $x_\\star = ax_\\star + b$，得到 $x_\\star = b/(1-a)$。\n让我们重新审视 $|e_{k+1}|$ 的表达式：\n$$|e_{k+1}| = |g(x_k) - g(x_\\star)| = |(ax_k + b) - (ax_\\star + b)| = |ax_k - ax_\\star| = |a(x_k - x_\\star)|$$\n这可以简化为：\n$$|e_{k+1}| = |a| |x_k - x_\\star| = |a| |e_k|$$\n对于这个仿射函数，一般不等式变成了一个精确的等式。如果初始误差 $e_0$ 非零，那么所有后续误差 $e_k$ 也将非零（因为在相关测试用例中 $|a| > 0$）。因此，我们可以除以 $|e_k|$ 来求比率：\n$$\\frac{|e_{k+1}|}{|e_k|} = |a|$$\n由于这个比率对所有 $k$ 都是常数，其当 $k \\to \\infty$ 时的极限自然是 $|a|$。因此，迭代表现出Q-线性收敛，并且商线性因子恰好是 $|a|$。该因子取决于 $a$ 而不取决于 $b$。如上所示，单步误差不等式中等号成立的条件是函数 $g$ 是仿射的。对于这样的函数，最紧的 Lipschitz 常数是 $L = |a|$。\n\n### 第二部分：与对数-误差图斜率的联系\n\n对于仿射映射，精确关系 $|e_{k+1}| = |a| |e_k|$ 允许我们通过递归代入找到 $|e_k|$ 的闭式表达式：\n$$|e_k| = |a| |e_{k-1}| = |a|^2 |e_{k-2}| = \\dots = |a|^k |e_0|$$\n其中 $|e_0| = |x_0 - x_\\star|$ 是初始误差大小。这个关系对所有 $k \\ge 0$ 都有效。\n\n为了揭示一种线性结构，我们对两边取自然对数（假设 $|e_0| > 0$）：\n$$\\ln(|e_k|) = \\ln(|a|^k |e_0|)$$\n利用对数的性质，特别是 $\\ln(xy) = \\ln(x) + \\ln(y)$ 和 $\\ln(x^p) = p \\ln(x)$，我们可以展开表达式：\n$$\\ln(|e_k|) = \\ln(|a|^k) + \\ln(|e_0|) = k \\ln(|a|) + \\ln(|e_0|)$$\n这个方程是直线方程 $y = mx + c$ 的形式，其中：\n- 因变量是 $y = \\ln(|e_k|)$。\n- 自变量是迭代索引 $x = k$。\n- 直线的斜率是 $m = \\ln(|a|)$。\n- y轴截距是 $c = \\ln(|e_0|)$。\n\n这表明，误差大小的自然对数对迭代索引的图像将产生一条直线。这条线的斜率 $m$ 是Q-线性因子 $|a|$ 的自然对数。因此，可以通过对数值估计的斜率取指数来恢复该因子：\n$$|a| = e^m$$\n这提供了第二种数值估计收敛因子的方法，即对数据点 $(k, \\ln(|e_k|))$ 进行线性最小二乘拟合。\n\n### 第三部分：数值实现计划\n\n程序将为每个给定的测试用例 $(a, b, x_0, N)$ 执行以下步骤：\n$1$。理论 Lipschitz 常数 $L$ 被确定为 $|a|$。\n$2$。不动点 $x_\\star$ 通过解析计算得出 $x_\\star = b / (1-a)$。\n$3$。从 $x_0$ 开始，使用关系 $x_{k+1} = a x_k + b$ 生成迭代序列 $(x_k)_{k=0}^{N}$。同时，计算并存储误差大小序列 $|e_k| = |x_k - x_\\star|$。\n$4$。基于比率的估计 $\\widehat{L}_{\\mathrm{ratio}}$ 是使用序列中最后两个误差大小计算的：$\\widehat{L}_{\\mathrm{ratio}} = |e_N| / |e_{N-1}|$。这是对“尾部比率”的有效解释，并且对于仿射映射，该比率理论上是恒定的。\n$5$。基于斜率的估计 $\\widehat{L}_{\\mathrm{slope}}$ 通过线性最小二乘回归获得。\n    a. 为所有 $|e_k| > 0$ 的 $k \\in \\{0, 1, \\dots, N\\}$ 构建数据点集 $(k, \\ln(|e_k|))$。\n    b. 对这些点进行线性拟合，提供斜率 $m$ 的估计。\n    c. 然后将估计值计算为 $\\widehat{L}_{\\mathrm{slope}} = e^m$。\n每个测试用例的最终输出将是三元组 $[L, \\widehat{L}_{\\mathrm{ratio}}, \\widehat{L}_{\\mathrm{slope}}]$。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem of estimating convergence factors for affine fixed-point iterations.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (a, b, x_0, N)\n        (0.5, 1.0, 10.0, 30),\n        (0.9, -2.0, 0.0, 60),\n        (-0.75, 3.0, -5.0, 50),\n        (0.2, 0.0, 100.0, 40),\n    ]\n\n    results = []\n    for a, b, x_0, N in test_cases:\n        # Theoretical Lipschitz constant\n        L_theoretical = np.abs(a)\n\n        # Analytical fixed point\n        # x_star = a * x_star + b  =  x_star * (1 - a) = b  =  x_star = b / (1 - a)\n        # Since |a|  1 is given, 1 - a is never zero.\n        if 1 - a == 0:\n            # This case should not be reached with the given |a|  1.\n            # However, for robustness, handle it.\n            # If a=1 and b=0, any x is a fixed point. If b!=0, no fixed point.\n            # Problem constraints make this moot.\n            x_star = np.nan\n        else:\n            x_star = b / (1.0 - a)\n\n        # Generate sequence and error magnitudes\n        x_k = x_0\n        e_magnitudes = np.zeros(N + 1)\n        for k in range(N + 1):\n            e_k = x_k - x_star\n            e_magnitudes[k] = np.abs(e_k)\n            # Update for next iteration\n            x_k = a * x_k + b\n\n        # (i) Estimate factor using the ratio from the tail of the sequence\n        # The problem states \"last valid pair of successive nonzero errors\".\n        # For the given affine maps with |a|>0 and e_0 != 0, errors are never zero.\n        # So we can safely use the last two elements.\n        L_ratio_estimate = e_magnitudes[N] / e_magnitudes[N-1]\n\n        # (ii) Estimate factor using a least-squares fit of log(|e_k|) vs k\n        # Filter for strictly positive errors to avoid log(0)\n        k_indices = np.arange(N + 1)\n        positive_error_mask = e_magnitudes > 0\n        \n        k_filtered = k_indices[positive_error_mask]\n        log_e_filtered = np.log(e_magnitudes[positive_error_mask])\n        \n        # Perform linear least-squares fit: log(|e_k|) = m*k + c\n        # np.polyfit returns [slope, intercept] for a degree 1 polynomial\n        if len(k_filtered)  2:\n            # Not enough data points for a fit\n            L_slope_estimate = np.nan\n        else:\n            slope, _ = np.polyfit(k_filtered, log_e_filtered, 1)\n            # The factor is the exponential of the slope\n            L_slope_estimate = np.exp(slope)\n\n        results.append([L_theoretical, L_ratio_estimate, L_slope_estimate])\n\n    # Final print statement in the exact required format.\n    # e.g., [[L1,L_ratio1,L_slope1],[L2,L_ratio2,L_slope2],...]\n    output_str = f\"[{','.join([f'[{l1},{l2},{l3}]' for l1, l2, l3 in results])}]\"\n    print(output_str)\n\nsolve()\n```", "id": "3113939"}, {"introduction": "在掌握了一维情况的基础上，我们现在将探索更高维度下的收敛问题。坐标下降法是一种直观的算法，它将单变量优化的思想扩展到了多变量问题。本练习将引导你推导坐标下降法的迭代矩阵，并使用其谱半径——这是线性代数中的一个关键概念——来分析收敛速度。通过比较循环坐标选择和随机坐标选择这两种策略，你将深入理解算法设计在高维空间中如何影响性能 [@problem_id:3113892]。", "problem": "考虑最小化严格凸二次函数 $f(\\mathbf{x}) = \\tfrac{1}{2} \\mathbf{x}^{\\top} A \\mathbf{x} - \\mathbf{b}^{\\top} \\mathbf{x}$，其中 $A \\in \\mathbb{R}^{n \\times n}$ 是一个具有正对角线元素的实对称严格对角占优矩阵，且 $\\mathbf{b} \\in \\mathbb{R}^{n}$。这样的矩阵 $A$ 是对称正定 (SPD) 的，这保证了存在一个满足 $A \\mathbf{x}^{\\star} = \\mathbf{b}$ 的唯一最小化子 $\\mathbf{x}^{\\star}$。坐标下降法通过每次对 $f$ 沿单个坐标轴进行精确的一维最小化，同时保持其他坐标固定，来迭代地更新 $\\mathbf{x}$ 的一个坐标。我们考虑两种坐标采样方案：循环（确定性顺序）和随机（根据坐标上的分布进行随机选择）。\n\n仅从二次函数 $f$ 梯度的基本定义和精确一维最小化的概念出发，推导单个坐标更新的线性误差更新表示，其形式为一个作用于当前误差 $\\mathbf{e} = \\mathbf{x} - \\mathbf{x}^{\\star}$ 的迭代算子。利用此表示来定义代表以下过程的迭代矩阵：\n- 一个以固定顺序访问所有坐标一次的完整循环周期。\n- 在给定坐标采样分布下的一个随机步骤，以期望算子的形式表示。\n然后，通过作用于期望误差的相应迭代矩阵的谱半径（定义为 $\\rho(M) = \\max \\{ |\\lambda| : \\lambda \\text{ is an eigenvalue of } M \\}$）来解释它们的收敛速度。解释为什么在 $k$ 个独立的随机步骤之后，期望误差在期望意义上遵循线性迭代，并构建对应于 $n$ 个独立步骤的一个随机周期的算子。\n\n你的程序必须针对所提供的测试套件实现以下任务，且无需任何外部输入：\n1. 对于每个测试用例，根据推导出的公式构建每个坐标的迭代算子，并构成：\n   - 循环周期迭代矩阵（按固定顺序遍历一次的乘积）。\n   - 针对给定采样分布的期望单步随机迭代矩阵。\n2. 计算循环周期矩阵和期望单步随机矩阵的谱半径。同时计算对应于 $n$ 个独立随机步骤的随机周期谱半径。\n3. 对于每个测试用例，输出一个包含四个值的序列：\n   - 循环周期的谱半径（一个浮点数）。\n   - 随机单步的谱半径（一个浮点数）。\n   - 随机周期的谱半径（一个浮点数）。\n   - 一个布尔值，指示随机周期的谱半径是否严格小于循环周期的谱半径（即，在这种谱半径的解释下，是否预测随机周期会更快）。\n\n测试套件：\n- 案例 1（边界情况，纯对角矩阵）：$n = 3$, \n  $$A = \\begin{bmatrix}\n  4  0  0 \\\\\n  0  5  0 \\\\\n  0  0  6\n  \\end{bmatrix}, \\quad \\text{均匀采样 } \\mathbf{p} = \\left[\\tfrac{1}{3}, \\tfrac{1}{3}, \\tfrac{1}{3}\\right].$$\n- 案例 2（弱耦合，严格对角占优）：$n = 3$, \n  $$A = \\begin{bmatrix}\n  4  -0.9  0.2 \\\\\n  -0.9  3.5  -0.4 \\\\\n  0.2  -0.4  2.8\n  \\end{bmatrix}, \\quad \\text{均匀采样 } \\mathbf{p} = \\left[\\tfrac{1}{3}, \\tfrac{1}{3}, \\tfrac{1}{3}\\right].$$\n- 案例 3（更大规模，与对角元素成比例的非均匀采样）：$n = 5$, \n  $$A = \\begin{bmatrix}\n  8  -0.5  0.2  0  0.1 \\\\\n  -0.5  7  0.3  -0.4  0 \\\\\n  0.2  0.3  9  -0.6  0.5 \\\\\n  0  -0.4  -0.6  6  -0.3 \\\\\n  0.1  0  0.5  -0.3  5\n  \\end{bmatrix}, \\quad \\mathbf{p} \\text{ 由 } p_j = \\frac{A_{jj}}{\\sum_{i=1}^{n} A_{ii}} \\text{ for } j = 1,\\dots,n. \\text{ 给出。}$$\n\n最终输出格式：\n你的程序应生成单行输出，其中包含用方括号括起来的、以逗号分隔的结果列表。将案例 1 的四个值、案例 2 的四个值、案例 3 的四个值依次连接起来。例如，格式为 $[\\text{c1\\_cyc},\\text{c1\\_rand\\_step},\\text{c1\\_rand\\_epoch},\\text{c1\\_better},\\text{c2\\_cyc},\\dots,\\text{c3\\_better}]$，其中每个 $\\text{c\\*}$ 值是浮点数，每个 $\\text{better}$ 值是布尔值。不涉及任何物理单位或角度单位；所有数值答案必须在指定的单行输出中以普通小数或布尔值的形式提供。", "solution": "该问题要求推导应用于二次函数的循环和随机坐标下降的迭代矩阵，并通过谱半径分析它们的收敛速度。随后是一个计算任务，要求为特定案例计算这些速度。\n\n该问题具有科学依据、提法明确且客观。所有提供的信息都是自洽且一致的。矩阵 $A$ 被定义为具有正对角线元素的实对称严格对角占优矩阵。根据 Gershgorin 定理和对角占优的性质，可以保证这类矩阵的所有特征值均为正，从而证实其为对称正定 (SPD)。这确保了目标函数 $f(\\mathbf{x})$ 是严格凸的，并且拥有一个梯度为零的唯一最小化子 $\\mathbf{x}^{\\star}$。因此，该问题是有效的。\n\n我们开始进行推导。\n\n需要最小化的目标函数是 $f(\\mathbf{x}) = \\tfrac{1}{2} \\mathbf{x}^{\\top} A \\mathbf{x} - \\mathbf{b}^{\\top} \\mathbf{x}$。由于矩阵 $A$ 是对称的，$f(\\mathbf{x})$ 的梯度由下式给出：\n$$\n\\nabla f(\\mathbf{x}) = A \\mathbf{x} - \\mathbf{b}\n$$\n唯一的最小化子 $\\mathbf{x}^{\\star}$ 是线性系统 $\\nabla f(\\mathbf{x}^{\\star}) = \\mathbf{0}$ 的解，这意味着 $A \\mathbf{x}^{\\star} = \\mathbf{b}$。\n\n**1. 单坐标更新推导**\n\n坐标下降法一次更新一个坐标。设 $\\mathbf{x}^{(k)}$ 为当前迭代点。为了通过更新第 $i$ 个坐标来找到下一个迭代点 $\\mathbf{x}^{(k+1)}$，我们保持所有其他坐标固定（$j \\neq i$ 时 $x_j^{(k+1)} = x_j^{(k)}$）并沿第 $i$ 个坐标轴对 $f$ 进行精确的一维最小化。这等价于求解 $x_i^{(k+1)}$ 使得新点处梯度的第 $i$ 个分量为零：\n$$\n(\\nabla f(\\mathbf{x}^{(k+1)}))_i = (A \\mathbf{x}^{(k+1)} - \\mathbf{b})_i = 0\n$$\n展开该矩阵方程的第 $i$ 行可得：\n$$\n\\sum_{j=1}^{n} A_{ij} x_j^{(k+1)} - b_i = 0\n$$\n分离含 $x_i^{(k+1)}$ 的项，并代入 $j \\neq i$ 时的 $x_j^{(k+1)} = x_j^{(k)}$：\n$$\nA_{ii} x_i^{(k+1)} + \\sum_{j \\neq i} A_{ij} x_j^{(k)} - b_i = 0\n$$\n求解 $x_i^{(k+1)}$ 得到更新规则：\n$$\nx_i^{(k+1)} = \\frac{1}{A_{ii}} \\left( b_i - \\sum_{j \\neq i} A_{ij} x_j^{(k)} \\right)\n$$\n注意，由于 $A$ 是具有正对角线元素的严格对角占优矩阵，因此 $A_{ii} \\neq 0$。\n\n**2. 误差传播与单坐标算子 ($M_i$)**\n\n我们现在用误差向量 $\\mathbf{e}^{(k)} = \\mathbf{x}^{(k)} - \\mathbf{x}^{\\star}$ 来表示此更新。将 $\\mathbf{x}^{(k)} = \\mathbf{e}^{(k)} + \\mathbf{x}^{\\star}$ 代入更新规则：\n$$\ne_i^{(k+1)} + x_i^{\\star} = \\frac{1}{A_{ii}} \\left( b_i - \\sum_{j \\neq i} A_{ij} (e_j^{(k)} + x_j^{\\star}) \\right)\n$$\n根据最优性条件 $A \\mathbf{x}^{\\star} = \\mathbf{b}$，我们有 $b_i = \\sum_{j=1}^{n} A_{ij} x_j^{\\star} = A_{ii} x_i^{\\star} + \\sum_{j \\neq i} A_{ij} x_j^{\\star}$。将此式代入 $b_i$：\n$$\ne_i^{(k+1)} + x_i^{\\star} = \\frac{1}{A_{ii}} \\left( \\left( A_{ii} x_i^{\\star} + \\sum_{j \\neq i} A_{ij} x_j^{\\star} \\right) - \\sum_{j \\neq i} A_{ij} e_j^{(k)} - \\sum_{j \\neq i} A_{ij} x_j^{\\star} \\right)\n$$\n简化表达式：\n$$\ne_i^{(k+1)} + x_i^{\\star} = \\frac{1}{A_{ii}} \\left( A_{ii} x_i^{\\star} - \\sum_{j \\neq i} A_{ij} e_j^{(k)} \\right) = x_i^{\\star} - \\frac{1}{A_{ii}} \\sum_{j \\neq i} A_{ij} e_j^{(k)}\n$$\n这给出了误差向量第 $i$ 个分量的更新：\n$$\ne_i^{(k+1)} = - \\sum_{j \\neq i} \\frac{A_{ij}}{A_{ii}} e_j^{(k)}\n$$\n在此步骤中，误差的其他分量保持不变：对于 $j \\neq i$，$e_j^{(k+1)} = e_j^{(k)}$。对误差向量 $\\mathbf{e}^{(k)}$ 的这种线性变换可以用矩阵-向量乘积 $\\mathbf{e}^{(k+1)} = M_i \\mathbf{e}^{(k)}$ 来表示。矩阵 $M_i$ 除了第 $i$ 行外，与单位矩阵 $I$ 相同。$M_i$ 的元素为：\n$$\n(M_i)_{jk} =\n\\begin{cases}\n\\delta_{jk}  \\text{ if } j \\neq i \\\\\n-A_{ik}/A_{ii}  \\text{ if } j = i, k \\neq i \\\\\n0  \\text{ if } j = i, k = i\n\\end{cases}\n$$\n其中 $\\delta_{jk}$ 是克罗内克 delta 符号。\n\n**3. 循环周期迭代算子 ($M_{\\text{cyc}}$)**\n\n一个完整的循环周期涉及按固定顺序（例如 $i=1, 2, \\dots, n$）更新坐标。如果 $\\mathbf{e}^{(k)}$ 是一个周期开始时的误差，则更新坐标 1 后的误差为 $M_1 \\mathbf{e}^{(k)}$。随后更新坐标 2 后的误差为 $M_2 (M_1 \\mathbf{e}^{(k)})$，以此类推。经过一个完整周期后，最终误差 $\\mathbf{e}^{(k+1)}$ 为：\n$$\n\\mathbf{e}^{(k+1)} = M_n M_{n-1} \\cdots M_2 M_1 \\mathbf{e}^{(k)}\n$$\n一个完整循环周期的迭代矩阵是各个算子的乘积：\n$$\nM_{\\text{cyc}} = M_n M_{n-1} \\cdots M_1\n$$\n渐进收敛速度由谱半径 $\\rho(M_{\\text{cyc}})$ 决定。\n\n**4. 随机步和随机周期算子 ($M_{\\text{rand\\_step}}$, $M_{\\text{rand\\_epoch}}$)**\n\n在随机坐标下降中，在每一步 $k$，从 $\\{1, \\dots, n\\}$ 中以概率 $p_{i_k}$ 选择一个坐标 $i_k$，其中 $\\mathbf{p} = [p_1, \\dots, p_n]^{\\top}$ 是一个概率分布。误差更新则为 $\\mathbf{e}^{(k+1)} = M_{i_k} \\mathbf{e}^{(k)}$。\n\n在给定当前误差 $\\mathbf{e}^{(k)}$ 的条件下，一步之后的期望误差为：\n$$\n\\mathbb{E}[\\mathbf{e}^{(k+1)} | \\mathbf{e}^{(k)}] = \\sum_{j=1}^{n} p_j (M_j \\mathbf{e}^{(k)}) = \\left( \\sum_{j=1}^{n} p_j M_j \\right) \\mathbf{e}^{(k)}\n$$\n这表明期望误差遵循线性迭代。相应的算子是期望单步随机迭代矩阵：\n$$\nM_{\\text{rand\\_step}} = \\mathbb{E}[M_{i_k}] = \\sum_{j=1}^{n} p_j M_j\n$$\n期望误差的收敛性由 $\\rho(M_{\\text{rand\\_step}})$ 控制。\n\n一个随机周期定义为 $n$ 个独立的随机步骤。设所选索引的序列为 $i_1, i_2, \\dots, i_n$。$n$ 步后的误差为 $\\mathbf{e}^{(k+n)} = M_{i_n} \\cdots M_{i_1} \\mathbf{e}^{(k)}$。此周期后的期望误差为：\n$$\n\\mathbb{E}[\\mathbf{e}^{(k+n)} | \\mathbf{e}^{(k)}] = \\mathbb{E}[M_{i_n} \\cdots M_{i_1}] \\mathbf{e}^{(k)}\n$$\n由于坐标选择的独立性，乘积的期望等于期望的乘积：\n$$\n\\mathbb{E}[\\mathbf{e}^{(k+n)} | \\mathbf{e}^{(k)}] = \\mathbb{E}[M_{i_n}] \\cdots \\mathbb{E}[M_{i_1}] \\mathbf{e}^{(k)} = (M_{\\text{rand\\_step}})^n \\mathbf{e}^{(k)}\n$$\n因此，一个随机周期的算子是 $M_{\\text{rand\\_epoch}} = (M_{\\text{rand\\_step}})^n$。\n\n这个周期算子的谱半径可以通过使用任何方阵 $M$ 和整数 $k \\ge 1$ 的性质 $\\rho(M^k) = (\\rho(M))^k$ 与单步算子的谱半径相关联。因此：\n$$\n\\rho(M_{\\text{rand\\_epoch}}) = \\rho((M_{\\text{rand\\_step}})^n) = (\\rho(M_{\\text{rand\\_step}}))^n\n$$\n这使我们能够通过比较 $\\rho(M_{\\text{cyc}})$ 和 $\\rho(M_{\\text{rand\\_epoch}})$ 来比较循环和随机方案的每周期收敛速度。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef compute_metrics(A, p):\n    \"\"\"\n    Computes the spectral radii for cyclic and randomized coordinate descent.\n\n    Args:\n        A (np.ndarray): The symmetric positive definite matrix of the quadratic form.\n        p (np.ndarray): The probability distribution for randomized coordinate selection.\n\n    Returns:\n        tuple: A tuple containing:\n            - rho_cyc (float): Spectral radius of the cyclic epoch operator.\n            - rho_rand_step (float): Spectral radius of the one-step randomized operator.\n            - rho_rand_epoch (float): Spectral radius of the n-step randomized epoch operator.\n            - is_faster (bool): True if randomized epoch has a strictly smaller spectral radius.\n    \"\"\"\n    n = A.shape[0]\n\n    # 1. Construct the per-coordinate iteration operators M_i\n    m_coords = []\n    for i in range(n):\n        Mi = np.identity(n)\n        # The i-th row is updated based on the derivation\n        for j in range(n):\n            if i == j:\n                Mi[i, j] = 0.0\n            else:\n                Mi[i, j] = -A[i, j] / A[i, i]\n        m_coords.append(Mi)\n\n    # 2. Construct the cyclic-epoch iteration matrix M_cyc = M_{n-1} ... M_1 M_0\n    # The order of updates is 0, 1, ..., n-1\n    M_cyc = np.identity(n)\n    for i in range(n):\n        M_cyc = m_coords[i] @ M_cyc\n\n    # 3. Construct the expected one-step randomized iteration matrix M_rand_step\n    M_rand_step = np.zeros((n, n))\n    for i in range(n):\n        M_rand_step += p[i] * m_coords[i]\n\n    # 4. Compute spectral radii\n    # For a complex number z = x + iy, np.abs(z) computes sqrt(x^2 + y^2)\n    rho_cyc = np.max(np.abs(np.linalg.eigvals(M_cyc)))\n    rho_rand_step = np.max(np.abs(np.linalg.eigvals(M_rand_step)))\n    \n    # The spectral radius of the randomized epoch operator is (rho_rand_step)^n\n    rho_rand_epoch = rho_rand_step ** n\n\n    # 5. Compare the epoch spectral radii\n    is_faster = rho_rand_epoch  rho_cyc\n\n    return rho_cyc, rho_rand_step, rho_rand_epoch, is_faster\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"n\": 3,\n            \"A\": np.array([\n                [4.0, 0.0, 0.0],\n                [0.0, 5.0, 0.0],\n                [0.0, 0.0, 6.0]\n            ]),\n            \"p_type\": \"uniform\"\n        },\n        {\n            \"n\": 3,\n            \"A\": np.array([\n                [4.0, -0.9, 0.2],\n                [-0.9, 3.5, -0.4],\n                [0.2, -0.4, 2.8]\n            ]),\n            \"p_type\": \"uniform\"\n        },\n        {\n            \"n\": 5,\n            \"A\": np.array([\n                [8.0, -0.5, 0.2, 0.0, 0.1],\n                [-0.5, 7.0, 0.3, -0.4, 0.0],\n                [0.2, 0.3, 9.0, -0.6, 0.5],\n                [0.0, -0.4, -0.6, 6.0, -0.3],\n                [0.1, 0.0, 0.5, -0.3, 5.0]\n            ]),\n            \"p_type\": \"proportional\"\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        A = case[\"A\"]\n        n = case[\"n\"]\n        \n        if case[\"p_type\"] == \"uniform\":\n            p = np.full(n, 1.0 / n)\n        elif case[\"p_type\"] == \"proportional\":\n            diagonals = np.diag(A)\n            p = diagonals / np.sum(diagonals)\n        \n        metrics = compute_metrics(A, p)\n        results.extend(metrics)\n\n    # Format the final output string\n    # Booleans will be converted to \"True\" or \"False\" by str()\n    # Floats will be in standard decimal representation\n    output_str = f\"[{','.join(map(str, results))}]\"\n    print(output_str)\n\nsolve()\n```", "id": "3113892"}, {"introduction": "线性收敛虽然有效，但许多先进的优化算法能够实现更快的收敛速度。本练习将通过拟牛顿方法中的BFGS算法——现代优化领域的“主力军”——向你介绍超线性收敛。你将研究沃尔夫（Wolfe）线搜索条件，特别是曲率条件，在实现这种快速收敛中所扮演的关键角色。通过构建一个违反此条件导致性能下降的实例，你将对这些理论要求为何对高级算法的效率至关重要，获得一个实践性的理解 [@problem_id:3113882]。", "problem": "你的任务是研究强制执行线搜索曲率条件如何影响拟牛顿法观察到的收敛速度，并构建一个违反曲率条件会破坏超线性收敛的具体案例。你必须编写一个完整的程序，实现一个优化器，并为给定的测试套件生成所要求的输出。\n\n使用的基本定义：\n- 如果 $\\lim_{k \\to \\infty} \\|x_k - x^\\star\\|_2 = 0$，则称迭代序列 $\\{x_k\\}$ 收敛于点 $x^\\star$。如果 $\\lim_{k \\to \\infty} \\frac{\\|x_{k+1} - x^\\star\\|_2}{\\|x_k - x^\\star\\|_2} = 0$，则收敛是超线性的；如果该比值趋向于区间 $(0,1)$ 内的一个常数，则收敛是线性的。\n- Broyden–Fletcher–Goldfarb–Shanno (BFGS) 逆Hessian更新使用位移 $s_k = x_{k+1} - x_k$ 和梯度差 $y_k = \\nabla f(x_{k+1}) - \\nabla f(x_k)$ 来更新逆Hessian近似 $H_k$，具体如下\n$$\nH_{k+1} \\;=\\; \\left(I - \\rho_k s_k y_k^\\top\\right) \\, H_k \\, \\left(I - \\rho_k y_k s_k^\\top\\right) \\;+\\; \\rho_k \\, s_k s_k^\\top,\n\\quad \\text{其中 } \\rho_k = \\frac{1}{y_k^\\top s_k},\n$$\n前提是曲率量 $y_k^\\top s_k$ 严格为正。如果 $y_k^\\top s_k \\le 0$，则不执行更新。\n- 如果一个线搜索同时强制执行充分下降（Armijo）条件和曲率条件，则称其满足强Wolfe条件。给定搜索方向 $p_k$、步长 $\\alpha_k$ 以及常数 $c_1 \\in (0,1)$ 和 $c_2 \\in (c_1,1)$，充分下降条件要求\n$$\nf(x_k + \\alpha_k p_k) \\le f(x_k) + c_1 \\alpha_k \\nabla f(x_k)^\\top p_k,\n$$\n而强Wolfe曲率条件要求\n$$\n\\left| \\nabla f(x_k + \\alpha_k p_k)^\\top p_k \\right| \\le c_2 \\left| \\nabla f(x_k)^\\top p_k \\right|.\n$$\n\n你的任务：\n- 为 $\\mathbb{R}^n$ 中的无约束最小化问题实现一个BFGS方法，该方法具有两种线搜索变体：\n  1. 一个强Wolfe线搜索，使用固定参数 $c_1 = 10^{-4}$ 和 $c_2 = 0.9$ 同时强制执行充分下降和曲率条件。\n  2. 一个仅Armijo的回溯线搜索，它强制执行充分下降条件，但通过使用一个过于保守的初始步长 $\\alpha_0 = 10^{-6}$ 和乘法缩减因子 $\\tau = 0.5$ 直到Armijo条件成立，来故意违反曲率条件。此变体旨在生成非常小的步长，使得 $\\nabla f(x_{k+1})^\\top p_k$ 在数值上接近 $\\nabla f(x_k)^\\top p_k$，从而通常无法满足曲率条件。\n- 在BFGS更新中，如果测得的曲率 $y_k^\\top s_k \\le 10^{-12}$，则跳过对 $H_{k+1}$ 的更新。\n- 对所有向量范数使用欧几里得范数。\n- 基于以下经验模型实现一个超线性收敛检测器：渐近地假设 $\\|e_{k+1}\\|_2 \\approx C \\|e_k\\|_2^{q}$，其中 $e_k = x_k - x^\\star$，$C > 0$，$q > 0$ 为常数。通过对最后可用的数据对 $\\left(\\log \\|e_k\\|_2, \\log \\|e_{k+1}\\|_2\\right)$ 进行最小二乘拟合来估计 $q$，如果可用，则至少使用最后4个连续的数据对。在以下任一情况下，将运行归类为超线性收敛：\n  - 它在最多 $n$ 次迭代内终止，且 $\\|\\nabla f(x)\\|_2$ 低于 $10^{-9}$（在维度 $n$ 内有限步终止），或者\n  - 对最后数据对拟合的斜率 $\\hat{q}$ 超过 $1.1$。\n  否则，将其归类为非超线性收敛。\n- 对每个测试用例，运行BFGS方法，最大迭代次数为 $200$ 次，梯度范数终止容差为 $10^{-9}$。\n\n测试套件：\n- 测试用例1（理想情况，在严格凸二次函数上使用强Wolfe条件）：\n  - 维度 $n = 2$，\n  - 目标函数 $f_1(x) = \\tfrac{1}{2} x^\\top A x$，其中 $A = \\begin{bmatrix} 4  1 \\\\ 1  3 \\end{bmatrix}$，\n  - 梯度 $\\nabla f_1(x) = A x$，\n  - 极小点 $x^\\star = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$，\n  - 初始点 $x_0 = \\begin{bmatrix} 2.0 \\\\ -1.0 \\end{bmatrix}$，\n  - 线搜索模式：强Wolfe，参数为 $c_1 = 10^{-4}$ 和 $c_2 = 0.9$。\n  - 预期行为：超线性收敛或在 $n$ 步内有限步终止。\n- 测试用例2（边界变体，在相同的二次函数上使用仅Armijo条件和故意设置的微小步长）：\n  - 与测试用例1相同的 $f_1$、$\\nabla f_1$、$x^\\star$ 和 $x_0$，\n  - 线搜索模式：仅Armijo，初始 $\\alpha_0 = 10^{-6}$，缩减因子 $\\tau = 0.5$，Armijo常数 $c_1 = 10^{-4}$，\n  - 预期行为：由于步长极小且频繁违反曲率条件，导致非超线性收敛，尽管对于二次型函数，解析上 $y_k^\\top s_k > 0$ 成立。\n- 测试用例3（非线性，在二维Rosenbrock函数接近其极小点处使用强Wolfe条件）：\n  - 维度 $n = 2$，\n  - 目标函数 $f_2(x) = 100 (x_2 - x_1^2)^2 + (1 - x_1)^2$，\n  - 梯度 $\\nabla f_2(x) = \\begin{bmatrix} -400 x_1 (x_2 - x_1^2) - 2 (1 - x_1) \\\\ 200 (x_2 - x_1^2) \\end{bmatrix}$，\n  - 极小点 $x^\\star = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}$，\n  - 初始点 $x_0 = \\begin{bmatrix} -1.0 \\\\ 1.2 \\end{bmatrix}$，\n  - 线搜索模式：强Wolfe，参数为 $c_1 = 10^{-4}$ 和 $c_2 = 0.9$，\n  - 预期行为：在渐近区域内超线性收敛。\n- 测试用例4（构造的失败案例，在Rosenbrock函数上使用仅Armijo条件和故意设置的微小步长）：\n  - 与测试用例3相同的 $f_2$、$\\nabla f_2$、$x^\\star$ 和 $x_0$，\n  - 线搜索模式：仅Armijo，初始 $\\alpha_0 = 10^{-6}$，缩减因子 $\\tau = 0.5$，以及 $c_1 = 10^{-4}$，\n  - 预期行为：由于持续违反曲率条件和不良的Hessian近似更新，导致非超线性收敛。\n\n最终输出规范：\n- 你的程序必须按顺序运行所有四个测试用例，并生成一行输出，其中包含一个由四个布尔值组成的列表，每个布尔值指示在该测试用例中是否观察到如上定义的超线性收敛。格式必须是严格的一行，内容为一个用方括号括起来的、逗号分隔的列表，例如：“[True,False,True,False]”。\n- 本问题不涉及任何物理单位或角度。\n\n你的程序必须是一个完整的、可运行的程序，不接受任何输入，并按指定格式向标准输出写入一行内容。", "solution": "我们从核心定义开始。Broyden–Fletcher–Goldfarb–Shanno (BFGS) 方法在每次迭代时构造一个近似的逆Hessian矩阵 $H_k$，并计算一个下降方向 $p_k = - H_k \\nabla f(x_k)$。下一个迭代点是 $x_{k+1} = x_k + \\alpha_k p_k$，其中 $\\alpha_k$ 是由线搜索确定的步长。更新 $x_{k+1}$ 后，该方法定义 $s_k = x_{k+1} - x_k$ 和 $y_k = \\nabla f(x_{k+1}) - \\nabla f(x_k)$，并且如果曲率量 $y_k^\\top s_k$ 为正，则执行BFGS更新\n$$\nH_{k+1} \\;=\\; \\left(I - \\rho_k s_k y_k^\\top\\right) \\, H_k \\, \\left(I - \\rho_k y_k s_k^\\top\\right) \\;+\\; \\rho_k \\, s_k s_k^\\top,\n\\quad \\text{其中 } \\rho_k = \\frac{1}{y_k^\\top s_k}.\n$$\n当 $H_k$ 是正定的且 $y_k^\\top s_k > 0$ 时，此更新保持 $H_k$ 的正定性。线搜索中曲率条件的作用是确保新点的方向导数在数值上不会保持过大，这在实践中通常是保证 $y_k^\\top s_k > 0$ 的充分条件，并支持 $H_{k+1}$ 的正定性。强Wolfe条件提供了两个不等式：充分下降条件，\n$$\nf(x_k + \\alpha_k p_k) \\le f(x_k) + c_1 \\alpha_k \\nabla f(x_k)^\\top p_k,\n$$\n和曲率条件，\n$$\n\\left| \\nabla f(x_k + \\alpha_k p_k)^\\top p_k \\right| \\le c_2 \\left| \\nabla f(x_k)^\\top p_k \\right|,\n$$\n其中 $0  c_1  c_2  1$，这确保了充分的进展并控制了新迭代点处的斜率。\n\n收敛速度使用误差序列 $e_k = x_k - x^\\star$ 来描述。超线性收敛的特征是比率 $\\|e_{k+1}\\|_2 / \\|e_k\\|_2$ 趋近于零，而线性收敛对应于该比率趋向于一个在 $(0,1)$ 区间内的固定常数。\n\n实验和算法的原理性设计：\n1. 对于一个严格凸二次函数 $f_1(x) = \\tfrac{1}{2} x^\\top A x$ 且 $A \\succ 0$，沿任何搜索方向的精确线搜索都会导致 $y_k = A s_k$，因此 $y_k^\\top s_k = s_k^\\top A s_k > 0$。此外，对于采用精确线搜索的BFGS方法，该方法通过重构精确的逆Hessian矩阵，在最多 $n$ 步内终止。具有足够精确的区间限定和插值的强Wolfe线搜索通常能找到一个足够接近精确线搜索解的步长，以展示有限步终止或超线性行为。\n2. 如果我们使用仅Armijo的线搜索，该搜索从一个非常小的初始步长 $\\alpha_0 = 10^{-6}$ 开始，并根据需要进一步减小，步长将保持微小。这导致 $x_{k+1} \\approx x_k$，$y_k \\approx 0$，因此 $y_k^\\top s_k$ 可能非常小。在这种情况下，当 $y_k^\\top s_k \\le 10^{-12}$ 时，BFGS更新将被跳过，或者更新效果可以忽略不计。因此，逆Hessian近似 $H_k$ 不会迅速改善，所以我们不期望超线性收敛。这故意违反了线搜索的曲率条件，因为 $|\\nabla f(x_{k+1})^\\top p_k|$ 的值仍然接近 $|\\nabla f(x_k)^\\top p_k|$，而不是显著减小。\n3. 对于一个在其极小点附近具有Lipschitz连续Hessian矩阵的光滑非二次函数，例如二维Rosenbrock函数 $f_2(x) = 100 (x_2 - x_1^2)^2 + (1 - x_1)^2$，已知在标准假设下，采用强Wolfe线搜索的BFGS方法在渐近区域表现出超线性收敛。曲率条件有助于确保 $H_k$ 保持为逆Hessian矩阵的一个良好的正定近似，从而随着 $k$ 的增加，局部行为能够从类线性收敛过渡到超线性收敛。\n4. 数值上衡量超线性行为依赖于使用最后几次迭代拟合经验模型 $\\|e_{k+1}\\|_2 \\approx C \\|e_k\\|_2^q$。取对数后，我们得到 $\\log \\|e_{k+1}\\|_2 \\approx \\log C + q \\log \\|e_k\\|_2$，我们通过对 $(\\log \\|e_k\\|_2, \\log \\|e_{k+1}\\|_2)$ 对进行最小二乘拟合来估计 $q$。如果 $\\hat{q} > 1.1$，我们宣布为超线性收敛；如果求解器在最多 $n$ 步内以很小的梯度范数终止，我们也宣布为超线性收敛（在 $\\mathbb{R}^n$ 中有限步终止）。否则，运行被视为非超线性。\n\n算法细节：\n- 方向计算：$p_k = - H_k \\nabla f(x_k)$，并带有一个保障措施，即如果 $\\nabla f(x_k)^\\top p_k \\ge 0$，我们重置 $H_k = I$ 并设置 $p_k = - \\nabla f(x_k)$ 以恢复下降方向。\n- 强Wolfe线搜索：使用标准的强Wolfe搜索，参数为 $c_1 = 10^{-4}$ 和 $c_2 = 0.9$。如果它未能返回一个步长，则回退到初始 $\\alpha = 1$ 的常规Armijo回溯。\n- 仅Armijo线搜索（通过构造来违反曲率）：从 $\\alpha_0 = 10^{-6}$ 开始，并以 $\\tau = 0.5$ 的比例缩减，直到满足 $c_1 = 10^{-4}$ 的Armijo条件。这会产生非常小的步长，并且在很大程度上违反了曲率条件。\n- BFGS更新：如果 $y_k^\\top s_k > 10^{-12}$，则使用标准的BFGS逆Hessian更新来更新 $H_{k+1}$，并将 $H_{k+1}$ 对称化为 $\\tfrac{1}{2}(H_{k+1} + H_{k+1}^\\top)$。否则，跳过更新。\n\n测试用例：\n- 测试用例1（二次函数与强Wolfe）：我们期望超线性行为或在 $n = 2$ 次迭代内有限步终止。这验证了强制执行曲率（通过强Wolfe）可以得到经典的快速收敛。\n- 测试用例2（二次函数与仅Armijo的微小步长）：我们期望非超线性收敛，原因是微小步长和曲率条件违规导致的无效更新。\n- 测试用例3（Rosenbrock函数与强Wolfe）：我们期望在渐近区域出现超线性行为，验证了拟牛顿法在光滑非二次问题上使用曲率条件的理论。\n- 测试用例4（Rosenbrock函数与仅Armijo的微小步长）：我们期望非超线性行为。\n\n程序实现了这些组件，对每个测试用例运行最多200次迭代和梯度容差为$10^{-9}$，使用最后几次误差（或有限步终止准则）来估计收敛阶，并按要求的顺序输出一个单行的布尔值列表。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import line_search\n\n# --------------------------\n# Objective and gradient definitions\n# --------------------------\n\ndef quad_A():\n    A = np.array([[4.0, 1.0],\n                  [1.0, 3.0]])\n    return A\n\ndef f_quad(x):\n    A = quad_A()\n    return 0.5 * float(x.T @ A @ x)\n\ndef g_quad(x):\n    A = quad_A()\n    return A @ x\n\ndef f_rosen(x):\n    # 2D Rosenbrock\n    x1, x2 = x[0], x[1]\n    return 100.0 * (x2 - x1**2)**2 + (1.0 - x1)**2\n\ndef g_rosen(x):\n    x1, x2 = x[0], x[1]\n    df_dx1 = -400.0 * x1 * (x2 - x1**2) - 2.0 * (1.0 - x1)\n    df_dx2 = 200.0 * (x2 - x1**2)\n    return np.array([df_dx1, df_dx2])\n\n# --------------------------\n# Line search implementations\n# --------------------------\n\ndef strong_wolfe_search(f, g, xk, pk, c1=1e-4, c2=0.9):\n    gk = g(xk)\n    old_fval = f(xk)\n    # SciPy's line_search sometimes expects f/g with signature f(x), g(x)\n    # It returns alpha, fc, gc, new_fval, old_fval, new_slope\n    ls_res = line_search(f, g, xk, pk, gfk=gk, old_fval=old_fval, c1=c1, c2=c2)\n    alpha = ls_res[0]\n    if alpha is None or alpha = 0:\n        # Fallback to a simple backtracking with initial alpha 1.0\n        alpha = backtracking_armijo(f, g, xk, pk, alpha0=1.0, c1=c1, tau=0.5, min_alpha=1e-16)\n    return alpha\n\ndef backtracking_armijo(f, g, xk, pk, alpha0=1e-6, c1=1e-4, tau=0.5, min_alpha=1e-16):\n    alpha = alpha0\n    fxk = f(xk)\n    gk = g(xk)\n    gkpk = np.dot(gk, pk)\n    # Ensure pk is descent; caller should ensure descent, but safeguard:\n    if gkpk >= 0:\n        # fallback to steepest descent direction\n        pk = -gk\n        gkpk = np.dot(gk, pk)\n    while True:\n        xtrial = xk + alpha * pk\n        if f(xtrial) = fxk + c1 * alpha * gkpk:\n            break\n        alpha *= tau\n        if alpha  min_alpha:\n            break\n    return alpha\n\n# --------------------------\n# BFGS solver\n# --------------------------\n\ndef bfgs_solver(f, g, x0, x_star, mode=\"strong_wolfe\",\n                max_iter=200, tol_grad=1e-9, c1=1e-4, c2=0.9):\n    n = len(x0)\n    xk = x0.copy()\n    Hk = np.eye(n)\n    err_hist = []\n    # Store error at initial iterate to build pairs\n    err_hist.append(np.linalg.norm(xk - x_star))\n    for k in range(max_iter):\n        gk = g(xk)\n        ng = np.linalg.norm(gk)\n        if ng = tol_grad:\n            break\n        pk = -Hk @ gk\n        # Ensure descent; if not, reset H and use steepest descent\n        if np.dot(gk, pk) >= 0:\n            Hk = np.eye(n)\n            pk = -gk\n        if mode == \"strong_wolfe\":\n            alpha = strong_wolfe_search(f, g, xk, pk, c1=c1, c2=c2)\n        elif mode == \"armijo_bad\":\n            alpha = backtracking_armijo(f, g, xk, pk, alpha0=1e-6, c1=c1, tau=0.5, min_alpha=1e-16)\n        else:\n            # default safe\n            alpha = backtracking_armijo(f, g, xk, pk, alpha0=1.0, c1=c1, tau=0.5, min_alpha=1e-16)\n        # Take step\n        s = alpha * pk\n        x_next = xk + s\n        g_next = g(x_next)\n        y = g_next - gk\n        yTs = float(np.dot(y, s))\n        # BFGS update if curvature sufficiently positive\n        if yTs > 1e-12:\n            rho = 1.0 / yTs\n            I = np.eye(n)\n            V = I - rho * np.outer(s, y)\n            Hk = V @ Hk @ V.T + rho * np.outer(s, s)\n            # Symmetrize to control numerical drift\n            Hk = 0.5 * (Hk + Hk.T)\n        # Update iterate and error history\n        xk = x_next\n        err_hist.append(np.linalg.norm(xk - x_star))\n    # Determine superlinear behavior\n    superlinear = classify_superlinear(err_hist, n, tol_grad)\n    return superlinear, err_hist\n\ndef classify_superlinear(err_hist, n, tol_grad):\n    # If the method finished in at most n iterations with tiny final error progression,\n    # treat as superlinear (finite termination).\n    # Use length of err_hist: it records initial + per step, so = n+1 => = n steps\n    if len(err_hist) = n + 1 and (len(err_hist) >= 2 and err_hist[-1]  1e-12):\n        return True\n    # Build log-log pairs from last several errors\n    # Filter out zeros and nonpositive errors\n    e = np.array([v for v in err_hist if v > 0])\n    if len(e)  3:\n        # Not enough data to assess; treat as not superlinear unless trivial success\n        return False\n    # Compute pairs (log e_k, log e_{k+1})\n    logs = np.log(e)\n    # Use last m pairs, with m at least 4 if available\n    m = min(8, len(logs) - 1)\n    if m  2:\n        return False\n    x_vals = logs[-(m+1):-1]\n    y_vals = logs[-m:]\n    # If any infs or nans, return False\n    if not np.all(np.isfinite(x_vals)) or not np.all(np.isfinite(y_vals)):\n        return False\n    # Least squares slope\n    x_mean = np.mean(x_vals)\n    y_mean = np.mean(y_vals)\n    denom = np.sum((x_vals - x_mean) ** 2)\n    if denom == 0:\n        return False\n    slope = np.sum((x_vals - x_mean) * (y_vals - y_mean)) / denom\n    # Superlinear if slope > 1.1\n    return bool(slope > 1.1)\n\n# --------------------------\n# Test suite execution\n# --------------------------\n\ndef solve():\n    # Define test cases:\n    # 1) Quadratic, strong Wolfe\n    # 2) Quadratic, Armijo-only tiny steps\n    # 3) Rosenbrock, strong Wolfe\n    # 4) Rosenbrock, Armijo-only tiny steps\n    test_cases = [\n        (\"quad\", \"strong_wolfe\"),\n        (\"quad\", \"armijo_bad\"),\n        (\"rosen\", \"strong_wolfe\"),\n        (\"rosen\", \"armijo_bad\"),\n    ]\n\n    results = []\n    for func_name, mode in test_cases:\n        if func_name == \"quad\":\n            f = f_quad\n            g = g_quad\n            x0 = np.array([2.0, -1.0])\n            x_star = np.array([0.0, 0.0])\n        elif func_name == \"rosen\":\n            f = f_rosen\n            g = g_rosen\n            x0 = np.array([-1.0, 1.2])\n            x_star = np.array([1.0, 1.0])\n        else:\n            # Fallback (should not occur)\n            f = f_quad\n            g = g_quad\n            x0 = np.array([1.0, 1.0])\n            x_star = np.array([0.0, 0.0])\n\n        superlinear, _ = bfgs_solver(\n            f, g, x0, x_star,\n            mode=mode,\n            max_iter=200,\n            tol_grad=1e-9,\n            c1=1e-4,\n            c2=0.9\n        )\n        results.append(superlinear)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3113882"}]}