{"hands_on_practices": [{"introduction": "我们从零开始，推导一个嵌入式配对的结构。这个练习将指导你求解阶条件，以创建一个 $2(1)$ 阶配对，揭示其设计中的自由度。更重要的是，通过一个精心设计的思想实验，你将发现主方法和嵌入式估计器必须具有不同阶精度的根本原因——这是可靠误差估计的基石。[@problem_id:3123519]", "problem": "考虑一个带有一个嵌入式估计器的一般两阶显式龙格-库塔 (RK) 方法。其阶段定义为\n$$\nk_{1} \\;=\\; f(y_{n}), \\qquad k_{2} \\;=\\; f\\!\\big(y_{n} + h\\,a_{21}\\,k_{1}\\big),\n$$\n主更新和嵌入式更新分别为\n$$\ny_{n+1}^{(p)} \\;=\\; y_{n} + h\\big(b_{1}\\,k_{1} + b_{2}\\,k_{2}\\big), \\qquad y_{n+1}^{(\\hat{p})} \\;=\\; y_{n} + h\\big(\\hat{b}_{1}\\,k_{1} + \\hat{b}_{2}\\,k_{2}\\big),\n$$\n其中 $h$ 是时间步长，$f$ 是一个足够光滑的右端项，$a_{21}$、$b_{1}$、$b_{2}$、$\\hat{b}_{1}$、$\\hat{b}_{2}$ 是方法系数。目标是构造一个主阶 $p=2$、嵌入阶 $\\hat{p}=1$ 的嵌入对，记为 $p=2(1)$。\n\n从初值问题 $y'(t) = f(y(t))$（其中 $y(t_{n}) = y_{n}$）的精确解的泰勒展开式以及上述方法的定义出发，通过匹配主方法和嵌入式估计器的级数系数到适当的阶，推导阶条件。对于两阶显式情况，以自由阶段参数 $a_{21} = \\alpha \\neq 0$ 的形式符号化地求解阶条件，并生成 $p=2(1)$ 嵌入对的单参数族 $\\{b_{1}(\\alpha), b_{2}(\\alpha); \\hat{b}_{1}, \\hat{b}_{2}\\}$。\n\n然后，构造一个病态系统，以证明为何对于可靠的步长控制，嵌入误差的阶必须与主方法的阶不同。考虑线性自治测试方程 $y'(t) = \\lambda\\,y(t)$（其中常数 $\\lambda \\in \\mathbb{R}$），并通过直接计算单步更新来证明，如果选择的嵌入式估计器与主方法具有相同的阶 $p=2$，同时共享相同的阶段参数 $a_{21}=\\alpha$，那么嵌入误差估计\n$$\nE \\;=\\; y_{n+1}^{(p)} - y_{n+1}^{(\\hat{p})}\n$$\n对于所有的 $h$、$\\lambda$ 和 $y_{n}$ 都会消失，即使对于该测试方程，两种方法的真实局部截断误差都不为零。最后给出此病态情况下 $E$ 的显式表达式。\n\n您的最终答案必须是一个单行矩阵，包含您为推导出的 $p=2(1)$ 对的五个系数 $\\big(b_{1}(\\alpha), b_{2}(\\alpha), a_{21}(\\alpha), \\hat{b}_{1}, \\hat{b}_{2}\\big)$，以及上述病态情况下 $E$ 的表达式。无需数值舍入。请使用精确的符号形式表示最终答案。", "solution": "用户提供了计算科学领域的一个有效问题陈述，具体涉及嵌入式龙格-库塔方法的推导和分析。该问题具有科学依据、提法明确且客观。我将给出完整解答。\n\n问题分为两部分。首先，我们推导一个两阶显式龙格-库塔 (RK) 方法构成 $p=2(1)$ 嵌入对的阶条件。其次，我们分析一个主方法和嵌入式估计器均为 $p=2$ 阶的病态情况。\n\n**第一部分：$p=2(1)$ RK 对的推导**\n\n我们首先考虑初值问题 $y'(t) = f(y(t))$（其中 $y(t_n) = y_n$）精确解的泰勒级数展开。在时间 $t_{n+1} = t_n + h$ 处的解由以下公式给出：\n$$\ny(t_n + h) = y(t_n) + h y'(t_n) + \\frac{h^2}{2} y''(t_n) + O(h^3)\n$$\n利用链式法则和常微分方程 (ODE) 的定义，我们可以用 $f$ 及其关于 $y$ 的偏导数（我们记为 $f_y$，$f_{yy}$ 等）来表示 $y$ 的导数。\n$y'(t_n) = f(y(t_n)) = f(y_n)$\n$y''(t_n) = \\frac{d}{dt} f(y(t)) \\Big|_{t=t_n} = f_y(y(t_n)) \\cdot y'(t_n) = f_y f$\n将这些代入泰勒展开式，我们得到：\n$$\ny(t_n + h) = y_n + h f + \\frac{h^2}{2} f_y f + O(h^3)\n$$\n接下来，我们展开由 RK 方法提供的数值解。该方法定义如下：\n$$\nk_{1} = f(y_{n}) \\\\\nk_{2} = f(y_{n} + h a_{21} k_{1}) \\\\\ny_{n+1} = y_{n} + h (b_{1} k_{1} + b_{2} k_{2})\n$$\n我们将阶段 $k_2$ 在 $y_n$ 附近作泰勒级数展开：\n$k_1 = f(y_n) = f$\n$$\nk_2 = f(y_n + h a_{21} f) = f(y_n) + (h a_{21} f) f_y(y_n) + O(h^2) = f + h a_{21} f_y f + O(h^2)\n$$\n现在，我们将 $k_1$ 和 $k_2$ 的展开式代入主方法 $y_{n+1}^{(p)}$ 的更新公式中：\n$$\ny_{n+1}^{(p)} = y_n + h \\big( b_1 f + b_2 (f + h a_{21} f_y f + O(h^2)) \\big)\n$$\n$$\ny_{n+1}^{(p)} = y_n + h (b_1 + b_2) f + h^2 (b_2 a_{21}) f_y f + O(h^3)\n$$\n要使主方法具有 $p=2$ 阶，其展开式必须与精确解的展开式匹配到 $O(h^2)$ 项。通过比较 $f$ 和 $f_y f$ 的系数，我们得到阶条件：\n\\begin{align*}\n\\text{$p=2$ 阶条件:} \\\\\nO(h):  \\quad b_1 + b_2 = 1 \\\\\nO(h^2):  \\quad b_2 a_{21} = \\frac{1}{2}\n\\end{align*}\n类似地，对于嵌入式方法 $y_{n+1}^{(\\hat{p})} = y_{n} + h(\\hat{b}_{1}k_{1} + \\hat{b}_{2}k_{2})$，我们得到其展开式：\n$$\ny_{n+1}^{(\\hat{p})} = y_n + h (\\hat{b}_1 + \\hat{b}_2) f + h^2 (\\hat{b}_2 a_{21}) f_y f + O(h^3)\n$$\n要使嵌入式方法具有 $\\hat{p}=1$ 阶，其展开式必须与精确解的展开式匹配到 $O(h)$ 项。这给出了一个单一的阶条件：\n\\begin{align*}\n\\text{$\\hat{p}=1$ 阶条件:} \\\\\nO(h):  \\quad \\hat{b}_1 + \\hat{b}_2 = 1\n\\end{align*}\n现在，我们求解这些方程组。问题指定了一个自由参数 $a_{21} = \\alpha \\neq 0$。\n从 $p=2$ 阶条件得出：\n$b_2 \\alpha = \\frac{1}{2} \\implies b_2 = \\frac{1}{2\\alpha}$\n$b_1 = 1 - b_2 = 1 - \\frac{1}{2\\alpha}$\n对于 $\\hat{p}=1$ 阶条件，存在无穷多解。一个常见且简单的选择是设 $\\hat{b}_2=0$，这意味着 $\\hat{b}_1=1$。这个选择使嵌入式方法等同于前向欧拉法，$y_{n+1}^{(\\hat{p})} = y_n + h k_1 = y_n + h f(y_n)$，这确实是一个一阶方法。\n\n得到的 $p=2(1)$ 对的单参数系数族是：\n$b_1(\\alpha) = 1 - \\frac{1}{2\\alpha}$\n$b_2(\\alpha) = \\frac{1}{2\\alpha}$\n$a_{21}(\\alpha) = \\alpha$\n$\\hat{b}_1 = 1$\n$\\hat{b}_2 = 0$\n\n**第二部分：病态情况分析**\n\n我们现在研究嵌入式方法与主方法具有相同阶数（即 $p=\\hat{p}=2$）且共享相同阶段值（相同的 $a_{21}$）的情形。测试方程是线性自治系统 $y'(t) = \\lambda y(t)$，此时 $f(y) = \\lambda y$。\n\n此时，两种方法的阶条件是相同的：\n对于主方法 ($p=2$): $b_1 + b_2 = 1$ 且 $b_2 a_{21} = \\frac{1}{2}$。\n对于嵌入式方法 ($\\hat{p}=2$): $\\hat{b}_1 + \\hat{b}_2 = 1$ 且 $\\hat{b}_2 a_{21} = \\frac{1}{2}$。\n\n让我们计算此测试问题的阶段值：\n$k_1 = f(y_n) = \\lambda y_n$\n$k_2 = f(y_n + h a_{21} k_1) = \\lambda(y_n + h a_{21} (\\lambda y_n)) = \\lambda y_n (1 + h a_{21} \\lambda)$\n\n主方法的更新公式为：\n$$\ny_{n+1}^{(p)} = y_n + h \\left( b_1(\\lambda y_n) + b_2(\\lambda y_n (1 + h a_{21} \\lambda)) \\right)\n$$\n$$\ny_{n+1}^{(p)} = y_n \\left( 1 + h\\lambda(b_1 + b_2) + (h\\lambda)^2 (b_2 a_{21}) \\right)\n$$\n代入 $p=2$ 阶条件，$b_1+b_2=1$ 和 $b_2 a_{21}=1/2$：\n$$\ny_{n+1}^{(p)} = y_n \\left( 1 + h\\lambda + \\frac{1}{2}(h\\lambda)^2 \\right)\n$$\n嵌入式方法的更新公式为：\n$$\ny_{n+1}^{(\\hat{p})} = y_n + h \\left( \\hat{b}_1(\\lambda y_n) + \\hat{b}_2(\\lambda y_n (1 + h a_{21} \\lambda)) \\right)\n$$\n$$\ny_{n+1}^{(\\hat{p})} = y_n \\left( 1 + h\\lambda(\\hat{b}_1 + \\hat{b}_2) + (h\\lambda)^2 (\\hat{b}_2 a_{21}) \\right)\n$$\n代入（假设的）$\\hat{p}=2$ 阶条件，$\\hat{b}_1+\\hat{b}_2=1$ 和 $\\hat{b}_2 a_{21}=1/2$：\n$$\ny_{n+1}^{(\\hat{p})} = y_n \\left( 1 + h\\lambda + \\frac{1}{2}(h\\lambda)^2 \\right)\n$$\n嵌入误差估计定义为 $E = y_{n+1}^{(p)} - y_{n+1}^{(\\hat{p})}$。计算此差值：\n$$\nE = y_n \\left( 1 + h\\lambda + \\frac{1}{2}(h\\lambda)^2 \\right) - y_n \\left( 1 + h\\lambda + \\frac{1}{2}(h\\lambda)^2 \\right) = 0\n$$\n对于任何 $h$、$\\lambda$ 和 $y_n$ 的选择，该误差估计恒为零。然而，两种方法的真实局部截断误差 (LTE) 都不为零。对于精确解 $y(t_{n+1}) = y_n \\exp(h\\lambda)$，LTE为：\n$$\n\\text{LTE} = y_n \\exp(h\\lambda) - y_n \\left( 1 + h\\lambda + \\frac{1}{2}(h\\lambda)^2 \\right) = y_n \\left( \\frac{(h\\lambda)^3}{6} + O(h^4) \\right) \\neq 0\n$$\n这表明，如果在两阶格式中，嵌入式方法与主方法具有相同的阶数并共享其阶段计算，则误差估计器对于线性问题会失效，因为它会错误地报告零误差。这就是为什么嵌入对要用不同阶的方法构建，例如 $p(\\hat{p})$ 其中 $\\hat{p}=p-1$。\n\n在这种病态情况下，$E$ 的表达式是 $E=0$。$p=2(1)$ 对的五个系数是 $b_{1}(\\alpha) = 1 - \\frac{1}{2\\alpha}$、$b_{2}(\\alpha) = \\frac{1}{2\\alpha}$、$a_{21}(\\alpha) = \\alpha$、$\\hat{b}_{1}=1$ 和 $\\hat{b}_{2}=0$。", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n1 - \\frac{1}{2\\alpha}  \\frac{1}{2\\alpha}  \\alpha  1  0  0\n\\end{pmatrix}\n}\n$$", "id": "3123519"}, {"introduction": "既然我们理解了为什么嵌入式配对要使用不同阶的方法，那么就来分析误差估计本身的性质。本练习要求你进行理论推导，确定一个通用 $p(p-1)$ 阶配对的误差估计器的渐近阶。随后，你将通过数值实验来验证这一理论结果，从而在抽象分析与具体计算证据之间建立联系。[@problem_id:3123523]", "problem": "您的任务是为显式嵌入式龙格-库塔对推导和验证其嵌入式误差估计器的渐近阶。请从局部截断误差和单步法一致性的基本定义出发，不假设任何超出标准定义的特殊公式。\n\n任务A部分（推导）：考虑一个显式嵌入式龙格-库塔对，它在一个步长内计算两个近似值：一个阶为 $p$ 的高阶近似 $y_{n+1}$ 和一个阶为 $p-1$ 的低阶近似 $\\hat{y}_{n+1}$，两者都由同一组阶段求值得到。使用单步法局部截断误差的定义及其在固定点 $t_n$ 处关于步长 $h$ 的泰勒展开，推导对于一个通用的 $p(p-1)$ 对，其嵌入式误差估计器 $\\lVert \\hat{y}_{n+1} - y_{n+1} \\rVert$ 的渐近阶（当 $h \\to 0$ 时）。您的推导必须从“一个 $r$ 阶方法的局部截断误差在达到 $h^r$ 阶项之前均为零，且其主误差项表现为 $C h^{r+1}$（其中 $C$ 是一个与问题相关的常数）”这一定义出发，并且不能假设任何已知的龙格-库塔误差系数。\n\n任务B部分（数值测试）：用一种现代编程语言实现一个程序，该程序：\n- 使用固定步长，在一个有限区间上对标量初值问题 $y'=\\sin t$（初值为 $y(0)=0$）进行积分。角度必须以弧度处理。\n- 使用两种显式嵌入式龙格-库塔对：一个 $p=3, p-1=2$ 的 Bogacki–Shampine $3(2)$ 对，以及一个 $p=2, p-1=1$ 的 Heun–Euler $2(1)$ 对。\n- 对于每个固定的步长 $h$，计算所有步长中嵌入式误差估计器的最大值 $\\max_{n} \\lvert \\hat{y}_{n+1} - y_{n+1} \\rvert$。\n- 通过对测量到的最大值的对数与步长的对数进行最小二乘线性拟合来估计观测阶，使得拟合直线的斜率近似于关于 $h$ 的渐近阶。\n\n测试套件：\n- 测试用例1：Bogacki–Shampine $3(2)$ 对，在区间 $[0, 4.0]$ 上，步长 $h \\in \\{0.5, 0.25, 0.125, 0.0625\\}$。\n- 测试用例2：Heun–Euler $2(1)$ 对，在区间 $[0, 4.0]$ 上，步长 $h \\in \\{0.5, 0.25, 0.125, 0.0625\\}$。\n- 测试用例3：Bogacki–Shampine $3(2)$ 对，在区间 $[0, 0.5]$ 上，步长 $h \\in \\{0.1, 0.05, 0.025, 0.0125\\}$。\n- 测试用例4：Heun–Euler $2(1)$ 对，在区间 $[0, 0.5]$ 上，步长 $h \\in \\{0.1, 0.05, 0.025, 0.0125\\}$。\n\n最终输出格式：\n您的程序应生成单行输出，其中包含按测试用例1到4顺序排列的四个观测阶（拟合斜率），每个数值四舍五入到三位小数，并以逗号分隔的列表形式用方括号括起来。例如，输出必须类似于 $[s_1,s_2,s_3,s_4]$，其中每个 $s_i$ 是一个打印到三位小数的实数。不应打印任何其他文本。", "solution": "这个问题包含两部分。A部分要求推导一个通用的阶为 $p$ 和 $p-1$ 的显式嵌入式龙格-库塔对的嵌入式误差估计器的渐近阶。B部分要求对 Bogacki–Shampine $3(2)$ 对和 Heun–Euler $2(1)$ 对的推导阶数进行数值验证。\n\n### A部分：渐近阶的推导\n\n设初值问题（IVP）由 $y'(t) = f(t, y(t))$ 和初始条件 $y(t_0) = y_0$ 给出。我们假设精确解（记为 $y(t)$）是足够光滑的。\n\n单步数值方法使用在 $t_n$ 处的信息来近似 $t_{n+1} = t_n + h$ 处的解。设 $y_n$ 是在 $t_n$ 处的数值近似。为了分析*局部误差*，我们假设在该步开始时数值解是精确的，即 $y_n = y(t_n)$。\n\n问题描述中定义，如果一个方法从精确解出发的单步误差 $y(t_{n+1}) - y_{n+1}$ 的主导项表现为 $C h^{r+1}$（其中 $C$ 是某个非零的、与问题相关的常数），则该方法为 $r$ 阶。这就是局部截断误差。\n\n我们有一个由两种方法组成的嵌入式龙格-库塔对：\n1.  一个阶为 $p$ 的高阶方法，它产生近似值 $y_{n+1}$。\n2.  一个阶为 $p-1$ 的低阶方法，它产生近似值 $\\hat{y}_{n+1}$。\n\n根据阶的定义，这两种方法的局部误差为：\n$$y(t_{n+1}) - y_{n+1} = \\mathcal{E}_{p,n} h^{p+1} + O(h^{p+2})$$\n$$y(t_{n+1}) - \\hat{y}_{n+1} = \\mathcal{E}_{p-1,n} h^{p} + O(h^{p+1})$$\n\n此处，$\\mathcal{E}_{p,n}$ 和 $\\mathcal{E}_{p-1,n}$ 是在第 $n$ 步的主局部截断误差系数。为了使方法的阶分别为 $p$ 和 $p-1$（且不更高），这些系数通常必须为非零。项 $O(h^k)$ 代表当 $h \\to 0$ 时比 $h^{k-1}$ 更快消失的高阶项。\n\n我们记为 $\\Delta_{n+1}$ 的嵌入式误差估计器是两个数值近似之间的差：\n$$\\Delta_{n+1} = \\hat{y}_{n+1} - y_{n+1}$$\n\n为了找到 $\\Delta_{n+1}$ 的渐近行为，我们可以用精确解 $y(t_{n+1})$ 及其各自的局部误差来表示 $y_{n+1}$ 和 $\\hat{y}_{n+1}$：\n$$y_{n+1} = y(t_{n+1}) - \\left( \\mathcal{E}_{p,n} h^{p+1} + O(h^{p+2}) \\right)$$\n$$\\hat{y}_{n+1} = y(t_{n+1}) - \\left( \\mathcal{E}_{p-1,n} h^{p} + O(h^{p+1}) \\right)$$\n\n现在，我们通过代入这些表达式来计算差值 $\\Delta_{n+1} = \\hat{y}_{n+1} - y_{n+1}$：\n$$\\Delta_{n+1} = \\left( y(t_{n+1}) - \\mathcal{E}_{p-1,n} h^{p} - O(h^{p+1}) \\right) - \\left( y(t_{n+1}) - \\mathcal{E}_{p,n} h^{p+1} - O(h^{p+2}) \\right)$$\n\n精确解项 $y(t_{n+1})$ 被消掉了：\n$$\\Delta_{n+1} = - \\mathcal{E}_{p-1,n} h^{p} - O(h^{p+1}) + \\mathcal{E}_{p,n} h^{p+1} + O(h^{p+2})$$\n\n我们可以合并高阶项。项 $\\mathcal{E}_{p,n} h^{p+1}$ 的阶为 $h^{p+1}$，因此它被吸收到 $O(h^{p+1})$ 项中。这得到：\n$$\\Delta_{n+1} = - \\mathcal{E}_{p-1,n} h^{p} + O(h^{p+1})$$\n\n为使低阶方法的阶数恰好为 $p-1$，对于一个一般问题，系数 $\\mathcal{E}_{p-1,n}$ 必须为非零。因此，误差估计器 $\\Delta_{n+1}$ 展开式中的主导项是 $-\\mathcal{E}_{p-1,n} h^{p}$。\n\n那么，误差估计器的范数为：\n$$\\lVert \\Delta_{n+1} \\rVert = \\lVert \\hat{y}_{n+1} - y_{n+1} \\rVert = \\lVert - \\mathcal{E}_{p-1,n} h^{p} + O(h^{p+1}) \\rVert$$\n当 $h \\to 0$ 时，该范数的行为由主导项决定：\n$$\\lVert \\hat{y}_{n+1} - y_{n+1} \\rVert \\approx \\lVert \\mathcal{E}_{p-1,n} \\rVert h^{p}$$\n\n这证明了嵌入式误差估计器的渐近阶是 $p$。该估计器是低阶方法局部误差 $y(t_{n+1}) - \\hat{y}_{n+1}$ 的一个代理，该误差的阶为 $p$。一个重要的推论是，误差估计器的精度阶高于它所估计的误差。\n\n### B部分：数值验证\n\n为了验证这个结果，我们将实现指定的龙格-库塔对，并分析嵌入式误差估计器的收敛性。根据推导，我们预期：\n- 对于 Bogacki–Shampine $3(2)$ 对 ($p=3$)，误差估计器 $\\lvert \\hat{y}_{n+1} - y_{n+1} \\rvert$ 应以 $p=3$ 阶收敛。\n- 对于 Heun–Euler $2(1)$ 对 ($p=2$)，误差估计器 $\\lvert \\hat{y}_{n+1} - y_{n+1} \\rvert$ 应以 $p=2$ 阶收敛。\n\n数值流程包括使用一组递减的步长 $h_i$ 在给定区间上对初值问题 $y'=\\sin t, y(0)=0$ 进行积分。对于每个 $h_i$，我们计算区间上的最大误差估计 $E_i = \\max_{n} \\lvert \\hat{y}_{n+1} - y_{n+1} \\rvert$。由于我们预期 $E_i \\approx C h_i^p$，取对数可得 $\\log(E_i) \\approx \\log(C) + p \\log(h_i)$。对 $\\log(E_i)$ 与 $\\log(h_i)$ 进行线性最小二乘拟合，将得出一个近似于阶数 $p$ 的斜率。以下代码为四个指定的测试用例执行此验证。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Derives and verifies the asymptotic order of embedded Runge-Kutta error estimators.\n    \"\"\"\n\n    # Define the differential equation for the IVP: y' = sin(t)\n    def f(t, y):\n        return np.sin(t)\n\n    # Implementation of the Heun-Euler 2(1) embedded pair step\n    def heun_euler_step(f_func, t, y, h):\n        \"\"\"\n        Performs one step of the Heun-Euler 2(1) method.\n        p=2 (Heun's method), p-1=1 (Forward Euler method).\n        \"\"\"\n        k1 = f_func(t, y)\n        k2 = f_func(t + h, y + h * k1)\n        \n        # Higher-order solution (p=2, Heun's method)\n        y_next = y + (h / 2.0) * (k1 + k2)\n        \n        # Lower-order solution (p-1=1, Forward Euler)\n        y_hat_next = y + h * k1\n        \n        return y_next, y_hat_next\n\n    # Implementation of the Bogacki-Shampine 3(2) embedded pair step\n    def bs32_step(f_func, t, y, h):\n        \"\"\"\n        Performs one step of the Bogacki-Shampine 3(2) method.\n        Uses the standard 4-stage explicit formulation.\n        p=3, p-1=2.\n        \"\"\"\n        k1 = f_func(t, y)\n        k2 = f_func(t + 0.5 * h, y + 0.5 * h * k1)\n        k3 = f_func(t + 0.75 * h, y + 0.75 * h * k2)\n        \n        # Higher-order solution (p=3)\n        y_next = y + h * (2.0/9.0 * k1 + 1.0/3.0 * k2 + 4.0/9.0 * k3)\n        \n        k4 = f_func(t + h, y_next)\n        \n        # Lower-order solution (p-1=2)\n        y_hat_next = y + h * (7.0/24.0 * k1 + 1.0/4.0 * k2 + 1.0/3.0 * k3 + 1.0/8.0 * k4)\n        \n        return y_next, y_hat_next\n\n    def run_integration(step_func, t_span, y0, h):\n        \"\"\"\n        Integrates the ODE using a given step function and fixed step size h.\n        Returns the maximum embedded error estimate over the interval.\n        \"\"\"\n        t = t_span[0]\n        y = y0\n        error_estimates = []\n\n        while t  t_span[1]:\n            # Ensure the last step does not overshoot the interval end\n            current_h = min(h, t_span[1] - t)\n            \n            y_next, y_hat_next = step_func(f, t, y, current_h)\n            \n            error_estimates.append(np.abs(y_hat_next - y_next))\n            \n            y = y_next\n            t += current_h\n        \n        if not error_estimates:\n            return 0.0\n            \n        return np.max(error_estimates)\n\n    def estimate_order(h_values, max_errors):\n        \"\"\"\n        Estimates the convergence order by a linear fit on log-log data.\n        \"\"\"\n        log_h = np.log(h_values)\n        log_e = np.log(max_errors)\n        \n        # np.polyfit with degree 1 performs a linear least-squares fit.\n        # It returns [slope, intercept]. The slope is the estimated order.\n        slope, _ = np.polyfit(log_h, log_e, 1)\n        return slope\n\n    # Define the four test cases from the problem statement\n    test_cases = [\n        {'method': bs32_step, 't_span': [0.0, 4.0], 'h_values': np.array([0.5, 0.25, 0.125, 0.0625])},\n        {'method': heun_euler_step, 't_span': [0.0, 4.0], 'h_values': np.array([0.5, 0.25, 0.125, 0.0625])},\n        {'method': bs32_step, 't_span': [0.0, 0.5], 'h_values': np.array([0.1, 0.05, 0.025, 0.0125])},\n        {'method': heun_euler_step, 't_span': [0.0, 0.5], 'h_values': np.array([0.1, 0.05, 0.025, 0.0125])},\n    ]\n\n    results = []\n    y0 = 0.0  # Initial condition y(0) = 0\n\n    for case in test_cases:\n        max_errors = []\n        for h in case['h_values']:\n            max_err = run_integration(case['method'], case['t_span'], y0, h)\n            max_errors.append(max_err)\n        \n        observed_order = estimate_order(case['h_values'], np.array(max_errors))\n        results.append(observed_order)\n\n    # Print the final results in the specified format\n    print(f\"[{','.join([f'{r:.3f}' for r in results])}]\")\n\nsolve()\n```", "id": "3123523"}, {"introduction": "在坚实的理论基础上，我们转向一个更实际的应用：求解微分方程组。本练习使用经典的 Lotka-Volterra 捕食者-被捕食者模型，探索在多变量系统中进行误差控制所面临的挑战。你将实现一个完整的自适应求解器，并比较标量与矢量容差策略，从而深入理解当系统不同分量的尺度差异巨大时，应如何有效控制误差。[@problem_id:3123484]", "problem": "您的任务是设计并实现一个完整的、可运行的程序，该程序在一个基于嵌入式龙格-库塔（RK）对的自适应常微分方程（ODE）求解器中比较标量和矢量绝对容差，并分析单步接受图。该问题使用经典的捕食者-被捕食者（Lotka-Volterra）模型，您的程序必须生成单行输出，该输出汇总了来自一个小的、指定的测试套件的定量结果。\n\n起点是由自治系统给出的基本模型\n$$\n\\frac{d\\mathbf{y}}{dt}=\\mathbf{f}(t,\\mathbf{y}), \\quad \\mathbf{y}(t)=\\begin{bmatrix}y_1(t)\\\\y_2(t)\\end{bmatrix},\n$$\n其中，对于捕食者-被捕食者模型，\n$$\n\\mathbf{f}(t,\\mathbf{y})=\\begin{bmatrix}\n\\alpha y_1 - \\beta y_1 y_2\\\\\n\\delta y_1 y_2 - \\gamma y_2\n\\end{bmatrix},\n$$\n参数为 $\\alpha>0$、$\\beta>0$、$\\gamma>0$ 和 $\\delta>0$。稳态 $\\mathbf{s}=\\begin{bmatrix}s_1\\\\s_2\\end{bmatrix}$ 满足 $\\mathbf{f}(t,\\mathbf{s})=\\mathbf{0}$，对于该系统，可得\n$$\ns_1=\\frac{\\gamma}{\\delta},\\quad s_2=\\frac{\\alpha}{\\beta}.\n$$\n\n一个阶为 $(p,p-1)$ 的嵌入式龙格-库塔（ERK）对在步长为 $h$ 的一步结束时产生两个近似值 $\\mathbf{y}^{[p]}$ 和 $\\mathbf{y}^{[p-1]}$，它们的差值可估计局部截断误差。您必须使用一个 $p=3$ 和 $p-1=2$ 的 ERK 对来驱动自适应步长控制。步长接受/拒绝的决定必须通过将分量级缩放误差的均方根（RMS）范数与阈值 $1$ 进行比较来做出，具体如下。如果 $\\mathbf{e}$ 是一步的误差估计向量，则定义每个分量的缩放因子\n$$\n\\tau_i = a_i + r\\cdot \\max\\!\\big(|y_i|,|y_i^{\\text{new}}|\\big),\n$$\n其中 $a_i$ 是绝对容差，$r$ 是相对容差，如果满足以下条件，则接受该步长\n$$\n\\left(\\frac{1}{n}\\sum_{i=1}^n \\left(\\frac{e_i}{\\tau_i}\\right)^2\\right)^{1/2}\\le 1,\n$$\n对于此系统，$n=2$。\n\n您必须实现并比较两种绝对容差方案：\n- 标量绝对容差：对于所有分量，$a_i=a_{\\text{sc}}$，其中\n$$\na_{\\text{sc}}=\\eta\\cdot \\frac{s_1+s_2}{2},\n$$\n其中 $\\eta>0$ 是用户指定的比例因子。\n- 矢量绝对容差：对于每个分量 $i\\in\\{1,2\\}$，$a_i=\\eta\\cdot s_i$。\n\n使用形式如下的标准自适应步长更新规则\n$$\nh_{\\text{new}}=h\\cdot \\theta\\cdot \\left(\\max\\!\\big(\\varepsilon, \\min(M,\\rho)\\big)\\right),\n$$\n其中 $\\rho$ 是误差度量的函数，当误差过大时减小 $h$，当误差较小时增大 $h$；$\\theta\\in(0,1)$ 是一个安全系数；$\\varepsilon$ 和 $M$ 是最小和最大增长限制因子。您必须选择参数，以使 $h_{\\text{new}}$ 能根据所选的 $p=3$ 的 ERK 对测得的误差做出适当响应。\n\n在给定状态 $(t,\\mathbf{y})$ 下，将“嵌入式接受图”定义为映射 $h\\mapsto \\{\\text{接受},\\text{拒绝}\\}$，该映射是在不实际推进状态的情况下，使用上述准则测试大小为 $h$ 的单个 ERK 步长时产生的结果。该图可以通过将被接受的最大测试步长 $h$ 来总结。\n\n实现要求：\n- 使用上面定义的捕食者-被捕食者矢量场 $\\mathbf{f}$。\n- 使用阶为 $(3,2)$ 的嵌入式龙格-库塔对进行局部误差估计和自适应步长控制。\n- 使用相同的相对容差 $r$ 实现上述两种绝对容差方案。\n- 对于每次模拟，将初始步长初始化为 $h_0=(t_1-t_0)/2$。\n- 对于初始状态 $(t_0,\\mathbf{y}_0)$ 的接受图，在 $[h_{\\min},h_{\\max}]$ 范围内评估一个对数间隔的候选步长 $h$ 网格，其中 $h_{\\min}=10^{-4}(t_1-t_0)$ 且 $h_{\\max}=0.9(t_1-t_0)$，并以被接受的最大 $h$ 进行总结。\n\n测试套件：\n对于以下三个测试用例中的每一个，执行两项任务：使用两种容差方案在 $[t_0,t_1]$ 上进行自适应积分，以及使用两种方案在 $(t_0,\\mathbf{y}_0)$ 处进行接受图分析。对于积分，记录被拒绝的步数。对于接受图，报告比率 $h_{\\max}^{\\text{vec}}/h_{\\max}^{\\text{sc}}$，其中 $h_{\\max}^{\\text{vec}}$ 是矢量绝对容差下的最大接受步长，$h_{\\max}^{\\text{sc}}$ 是标量绝对容差下的最大接受步长。使用以下参数集：\n- 案例 A（基准）：$\\alpha=1.0$, $\\beta=0.1$, $\\gamma=1.5$, $\\delta=0.075$, $\\mathbf{y}_0=\\begin{bmatrix}10.0\\\\5.0\\end{bmatrix}$, $[t_0,t_1]=[0.0,20.0]$, $r=10^{-3}$, $\\eta=10^{-6}$。\n- 案例 B（不平衡的初始条件）：$\\alpha=1.0$, $\\beta=0.1$, $\\gamma=1.5$, $\\delta=0.075$, $\\mathbf{y}_0=\\begin{bmatrix}1000.0\\\\0.1\\end{bmatrix}$, $[t_0,t_1]=[0.0,5.0]$, $r=10^{-4}$, $\\eta=10^{-8}$。\n- 案例 C（接近稳态）：$\\alpha=1.0$, $\\beta=0.1$, $\\gamma=1.5$, $\\delta=0.075$, $\\mathbf{y}_0=\\begin{bmatrix}20.0(1+10^{-4})\\\\10.0(1-10^{-4})\\end{bmatrix}$, $[t_0,t_1]=[0.0,10.0]$, $r=10^{-6}$, $\\eta=10^{-10}$。\n\n对于每个案例，按顺序输出一个三元组值：\n- 使用标量绝对容差时被拒绝的步数的整数值。\n- 使用矢量绝对容差时被拒绝的步数的整数值。\n- 浮点数比率 $h_{\\max}^{\\text{vec}}/h_{\\max}^{\\text{sc}}$，四舍五入到六位小数。\n\n最终输出格式：\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果，按案例 A、B、C 的顺序排列，每个案例贡献其三元组，并展平成一个单一列表。例如，输出必须是以下形式\n$$\n[\\text{A\\_rej\\_sc},\\text{A\\_rej\\_vec},\\text{A\\_ratio},\\text{B\\_rej\\_sc},\\text{B\\_rej\\_vec},\\text{B\\_ratio},\\text{C\\_rej\\_sc},\\text{C\\_rej\\_vec},\\text{C\\_ratio}],\n$$\n其中每个三元组的前两个条目是整数，第三个是四舍五入到六位小数的小数。不得打印任何额外文本。", "solution": "用户希望解决一个涉及常微分方程（ODE）的数值分析问题。该问题要求设计并实现一个自适应 ODE 求解器，以分析 Lotka-Volterra 捕食者-被捕食者模型的行为。分析的重点是在嵌入式龙格-库塔方法的框架内，比较两种不同的绝对容差方案——标量与矢量。\n\n**1. 捕食者-被捕食者模型**\n\n该系统由一阶常微分方程的自治系统描述：\n$$\n\\frac{d\\mathbf{y}}{dt} = \\mathbf{f}(t, \\mathbf{y}), \\quad \\text{with } \\mathbf{y}(t) = \\begin{bmatrix} y_1(t) \\\\ y_2(t) \\end{bmatrix}\n$$\n这里，$y_1(t)$ 代表被捕食者种群，$y_2(t)$ 代表捕食者种群。Lotka-Volterra 模型的矢量场 $\\mathbf{f}$ 由以下公式给出：\n$$\n\\mathbf{f}(t, \\mathbf{y}) = \\begin{bmatrix}\n\\alpha y_1 - \\beta y_1 y_2 \\\\\n\\delta y_1 y_2 - \\gamma y_2\n\\end{bmatrix}\n$$\n其中 $\\alpha, \\beta, \\gamma, \\delta$ 是控制种群动态的正参数。非平凡稳态（或平衡点）$\\mathbf{s} = \\begin{bmatrix} s_1 \\\\ s_2 \\end{bmatrix}$ 是通过设置 $\\mathbf{f}(t, \\mathbf{s}) = \\mathbf{0}$ 求得的，结果为：\n$$\ns_1 = \\frac{\\gamma}{\\delta}, \\quad s_2 = \\frac{\\alpha}{\\beta}\n$$\n该稳态是定义绝对容差方案的关键参考点。\n\n**2. 嵌入式龙格-库塔（3,2）方法**\n\n为了数值求解该 ODE 系统，我们采用一种由阶数为 $p=3$ 和 $p-1=2$ 的嵌入式龙格-库塔（ERK）对驱动的自适应步长策略。这种方法对在每一步提供两个近似值，一个高阶近似 $\\mathbf{y}^{[3]}$ 和一个低阶近似 $\\mathbf{y}^{[2]}$。它们的差值可作为局部截断误差的估计。\n\n我们将使用 Bogacki-Shampine 3(2) 对，这是一个因其效率（包括“初值即终值”（First Same As Last, FSAL）特性）而广受欢迎的选择。对于从 $t_n$ 到 $t_{n+1} = t_n + h$ 的一个步长，其各阶段计算如下：\n$$\n\\begin{aligned}\n\\mathbf{k}_1 = \\mathbf{f}(t_n, \\mathbf{y}_n) \\\\\n\\mathbf{k}_2 = \\mathbf{f}(t_n + \\frac{1}{2}h, \\mathbf{y}_n + \\frac{1}{2}h\\mathbf{k}_1) \\\\\n\\mathbf{k}_3 = \\mathbf{f}(t_n + \\frac{3}{4}h, \\mathbf{y}_n + \\frac{3}{4}h\\mathbf{k}_2)\n\\end{aligned}\n$$\n用于推进解的三阶近似值为：\n$$\n\\mathbf{y}_{n+1}^{[3]} = \\mathbf{y}_n + h \\left( \\frac{2}{9}\\mathbf{k}_1 + \\frac{1}{3}\\mathbf{k}_2 + \\frac{4}{9}\\mathbf{k}_3 \\right)\n$$\n为了估计误差，需要计算第四个阶段。由于 FSAL 特性，这个阶段是在新点 $(t_{n+1}, \\mathbf{y}_{n+1}^{[3]})$ 处的求值，这意味着如果当前步被接受，它可以作为后续步骤的第一个阶段（$\\mathbf{k}_1$）重复使用。\n$$\n\\mathbf{k}_4 = \\mathbf{f}(t_{n+1}, \\mathbf{y}_{n+1}^{[3]})\n$$\n局部误差估计向量 $\\mathbf{e}_{n+1}$ 是三阶解与嵌入的二阶解之间的差值。它可以使用这些阶段直接计算：\n$$\n\\mathbf{e}_{n+1} = \\mathbf{y}_{n+1}^{[3]} - \\mathbf{y}_{n+1}^{[2]} = h \\left( -\\frac{5}{72}\\mathbf{k}_1 + \\frac{1}{12}\\mathbf{k}_2 + \\frac{1}{9}\\mathbf{k}_3 - \\frac{1}{8}\\mathbf{k}_4 \\right)\n$$\n\n**3. 自适应步长控制**\n\n接受或拒绝一个步长的决定是基于误差估计的缩放均方根（RMS）范数。首先，定义每个分量的容差尺度 $\\tau_i$：\n$$\n\\tau_i = a_i + r \\cdot \\max(|y_{n,i}|, |y_{n+1,i}^{[3]}|)\n$$\n其中 $a_i$ 是分量 $i$ 的绝对容差，$r$ 是相对容差，$y_{n,i}$ 是步长开始时解的第 $i$ 个分量，$y_{n+1,i}^{[3]}$ 是步长结束时建议的值。\n\n然后误差范数 $E$ 计算如下：\n$$\nE = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} \\left(\\frac{e_i}{\\tau_i}\\right)^2}\n$$\n对于我们的二维系统，$n=2$。如果 $E \\le 1$，则接受该步长。\n\n如果步长被接受，则计算新的步长。如果被拒绝，则使用更小的步长重试当前步。新步长 $h_{\\text{new}}$ 的公式是：\n$$\nh_{\\text{new}} = h \\cdot \\theta \\cdot \\max(\\varepsilon, \\min(M, E^{-1/p}))\n$$\n- $h$ 是当前步长。\n- $p=3$ 是方法的阶数。因子 $E^{-1/p}$ 是基于局部误差理论的理想缩放比例。\n- $\\theta \\in (0,1)$ 是一个安全系数，以防止过于乐观的步长增加。我们选择 $\\theta=0.9$。\n- $\\varepsilon$ 和 $M$ 是限制步长最小和最大变化的因子。我们选择 $\\varepsilon=0.2$ 和 $M=5.0$。\n\n**4. 绝对容差方案**\n\n该问题要求比较两种设置绝对容差 $a_i$ 的方案：\n- **标量容差：** 所有分量共享相同的绝对容差 $a_i = a_{\\text{sc}}$，该容差基于稳态值的平均值：\n  $$\n  a_{\\text{sc}} = \\eta \\cdot \\frac{s_1 + s_2}{2}\n  $$\n- **矢量容差：** 每个分量都有自己的绝对容差，按其相应的稳态值进行缩放：\n  $$\n  a_i = \\eta \\cdot s_i \\quad \\text{for } i \\in \\{1, 2\\}\n  $$\n在这两种情况下，$\\eta$ 都是一个用户指定的小参数。这种比较突出了根据每个变量的特征尺度定制容差如何影响求解器的性能。\n\n**5. 分析任务与实现**\n\n对于每个测试用例，执行两种分析：\n1.  **自适应积分：** 在指定区间 $[t_0, t_1]$ 上对 ODE 进行积分。对标量和矢量容差方案分别计算被拒绝的总步数。该指标可以洞察每种方案的效率。\n2.  **接受图分析：** 在初始状态 $(t_0, \\mathbf{y}_0)$，我们确定误差控制准则将接受的最大步长 $h$。这是通过测试从 $h_{\\min}=10^{-4}(t_1-t_0)$ 到 $h_{\\max}=0.9(t_1-t_0)$ 的对数间隔候选步长 $h$ 网格来完成的。计算矢量容差下的最大接受步长（$h_{\\max}^{\\text{vec}}$）与标量容差下的最大接受步长（$h_{\\max}^{\\text{sc}}$）之比。该比率量化了容差选择对允许步长的直接影响。\n\n实现将包括执行单个 ERK 步、运行完整的自适应积分以及分析接受图的函数。将为问题陈述中指定的每个测试用例调用这些函数，以生成所需的输出值。", "answer": "```python\nimport numpy as np\nfrom functools import partial\n\ndef solve():\n    \"\"\"\n    Main solver function to orchestrate the test suite and generate the final output.\n    \"\"\"\n    \n    # ERK 3(2) Bogacki-Shampine coefficients (FSAL variant)\n    # The method advances with a 3rd-order solution. The error is estimated\n    # by taking the difference with an embedded 2nd-order solution.\n    # c: nodes, A: matrix, b: 3rd-order weights, e: error weights (b_3 - b_2)\n    C = np.array([0, 1/2, 3/4, 1])\n    A = np.array([\n        [0, 0, 0, 0],\n        [1/2, 0, 0, 0],\n        [0, 3/4, 0, 0],\n        [2/9, 1/3, 4/9, 0]\n    ])\n    B3 = np.array([2/9, 1/3, 4/9, 0])\n    B2 = np.array([7/24, 1/4, 1/3, 1/8])\n    E = B3 - B2  # [-5/72, 1/12, 1/9, -1/8]\n\n    # Adaptive step-size controller parameters\n    SAFETY_FACTOR = 0.9\n    MIN_GROWTH = 0.2\n    MAX_GROWTH = 5.0\n    METHOD_ORDER = 3\n\n    def lotka_volterra_rhs(t, y, alpha, beta, gamma, delta):\n        \"\"\"Lotka-Volterra predator-prey model RHS function.\"\"\"\n        y1, y2 = y\n        dy1_dt = alpha * y1 - beta * y1 * y2\n        dy2_dt = delta * y1 * y2 - gamma * y2\n        return np.array([dy1_dt, dy2_dt])\n\n    def perform_single_step(rhs, t, y, h, k1_in=None):\n        \"\"\"\n        Performs a single step of the Bogacki-Shampine 3(2) method.\n        Implements the FSAL (First Same As Last) property by accepting k1.\n        \"\"\"\n        k = np.zeros((4, len(y)))\n        \n        if k1_in is None:\n            k[0] = rhs(t, y)\n        else:\n            k[0] = k1_in\n        \n        k[1] = rhs(t + C[1] * h, y + h * A[1,0] * k[0])\n        k[2] = rhs(t + C[2] * h, y + h * (A[2,0] * k[0] + A[2,1] * k[1]))\n\n        y_new = y + h * (B3[0]*k[0] + B3[1]*k[1] + B3[2]*k[2])\n        \n        # This k4 evaluation can be used as k1 of the next step (FSAL).\n        k[3] = rhs(t + h, y_new)\n\n        error_estimate = h * (E[0]*k[0] + E[1]*k[1] + E[2]*k[2] + E[3]*k[3])\n        \n        return y_new, error_estimate, k[3]\n\n    def run_integration(rhs, y0, t_span, r, a):\n        \"\"\"Runs the adaptive integration and returns the number of rejected steps.\"\"\"\n        t0, t1 = t_span\n        t = t0\n        y = np.copy(y0)\n        h = (t1 - t0) / 2.0  # Initial step size per problem spec\n        rejected_steps = 0\n        k1_next = None\n\n        while t  t1:\n            if t + h > t1:\n                h = t1 - t\n\n            y_new, err_est, k4 = perform_single_step(rhs, t, y, h, k1_in=k1_next)\n\n            # Calculate error norm E\n            y_max_abs = np.maximum(np.abs(y), np.abs(y_new))\n            tol_scale = a + r * y_max_abs\n            \n            # Guard against division by zero if tolerance is zero\n            err_ratio = np.divide(err_est, tol_scale, out=np.zeros_like(err_est), where=tol_scale!=0)\n            \n            error_norm = np.sqrt(np.mean(err_ratio**2))\n\n            if error_norm = 1.0: # Step accepted\n                t += h\n                y = y_new\n                k1_next = k4 # FSAL property\n            else: # Step rejected\n                rejected_steps += 1\n                k1_next = None # Must recompute k1 for the new, smaller step\n\n            # Update step size\n            if error_norm == 0:\n                scale_factor = MAX_GROWTH\n            else:\n                scale_factor = (error_norm)**(-1.0 / METHOD_ORDER)\n\n            h_new_unlimited = h * SAFETY_FACTOR * scale_factor\n            h = np.clip(h_new_unlimited, h * MIN_GROWTH, h * MAX_GROWTH)\n            \n        return rejected_steps\n\n    def analyze_acceptance_map(rhs, y0, t_span, r, a):\n        \"\"\"Finds the maximum accepted step size h at the initial point.\"\"\"\n        t0, t1 = t_span\n        h_min = 1e-4 * (t1 - t0)\n        h_max = 0.9 * (t1 - t0)\n        h_candidates = np.logspace(np.log10(h_min), np.log10(h_max), 500)\n        \n        max_h_accepted = 0.0\n\n        for h in h_candidates:\n            y_new, err_est, _ = perform_single_step(rhs, t0, y0, h)\n            \n            y_max_abs = np.maximum(np.abs(y0), np.abs(y_new))\n            tol_scale = a + r * y_max_abs\n            \n            err_ratio = np.divide(err_est, tol_scale, out=np.full_like(err_est, np.inf), where=tol_scale!=0)\n            \n            error_norm = np.sqrt(np.mean(err_ratio**2))\n            \n            if error_norm = 1.0:\n                max_h_accepted = h\n        \n        return max_h_accepted\n\n    test_cases = [\n        # Case A (baseline)\n        {'alpha': 1.0, 'beta': 0.1, 'gamma': 1.5, 'delta': 0.075, 'y0': np.array([10.0, 5.0]), 't_span': [0.0, 20.0], 'r': 1e-3, 'eta': 1e-6},\n        # Case B (unbalanced initial condition)\n        {'alpha': 1.0, 'beta': 0.1, 'gamma': 1.5, 'delta': 0.075, 'y0': np.array([1000.0, 0.1]), 't_span': [0.0, 5.0], 'r': 1e-4, 'eta': 1e-8},\n        # Case C (near steady state)\n        {'alpha': 1.0, 'beta': 0.1, 'gamma': 1.5, 'delta': 0.075, 'y0': np.array([20.0*(1+1e-4), 10.0*(1-1e-4)]), 't_span': [0.0, 10.0], 'r': 1e-6, 'eta': 1e-10},\n    ]\n    \n    results = []\n\n    for case in test_cases:\n        p = {k: case[k] for k in ('alpha', 'beta', 'gamma', 'delta')}\n        y0, t_span, r, eta = case['y0'], case['t_span'], case['r'], case['eta']\n        \n        rhs = partial(lotka_volterra_rhs, **p)\n\n        # Steady state\n        s1 = p['gamma'] / p['delta']\n        s2 = p['alpha'] / p['beta']\n\n        # Scalar absolute tolerance\n        a_sc_val = eta * (s1 + s2) / 2\n        a_sc = np.array([a_sc_val, a_sc_val])\n        \n        # Vector absolute tolerance\n        a_vec = eta * np.array([s1, s2])\n\n        # Task 1: Adaptive integration\n        rej_sc = run_integration(rhs, y0, t_span, r, a_sc)\n        rej_vec = run_integration(rhs, y0, t_span, r, a_vec)\n\n        # Task 2: Acceptance map analysis\n        h_max_sc = analyze_acceptance_map(rhs, y0, t_span, r, a_sc)\n        h_max_vec = analyze_acceptance_map(rhs, y0, t_span, r, a_vec)\n        \n        ratio = h_max_vec / h_max_sc if h_max_sc > 0 else 0.0\n\n        results.extend([rej_sc, rej_vec, ratio])\n\n    # Format the results for the final output string\n    formatted_results = []\n    for i, res in enumerate(results):\n        if (i + 1) % 3 == 0: # It's a ratio\n            formatted_results.append(f\"{res:.6f}\")\n        else: # It's a rejection count\n            formatted_results.append(str(res))\n    \n    print(f\"[{','.join(formatted_results)}]\")\n\nif __name__ == '__main__':\n    solve()\n```", "id": "3123484"}]}