{"hands_on_practices": [{"introduction": "我们对最小二乘近似的探索始于其最经典的应用：将一条直线拟合到一组数据点。练习 [@problem_id:1031896] 要求您从理论走向实践，为一个小型数据集手动推导最佳拟合线。通过建立并求解正规方程，您将直接接触到最小化误差平方和的数学核心，为后续所有关于数据拟合的学习奠定坚实的基础。", "problem": "考虑数据点：$(t_1, y_1) = (1, 3)$，$(t_2, y_2) = (2, 1)$，$(t_3, y_3) = (3, 4)$，$(t_4, y_4) = (4, 6)$。使用最小二乘法，对这些数据拟合一个线性模型 $y = \\alpha + \\beta t$。计算斜率参数 $\\beta$ 的最小二乘估计值。", "solution": "为了求出 $\\beta$ 的最小二乘估计值，我们求解通过最小化残差平方和 $S = \\sum_{i=1}^4 (y_i - \\alpha - \\beta t_i)^2$ 导出的正规方程组：\n$$\\frac{\\partial S}{\\partial \\alpha} = -2 \\sum_{i=1}^4 (y_i - \\alpha - \\beta t_i) = 0$$  \n$$\\frac{\\partial S}{\\partial \\beta} = -2 \\sum_{i=1}^4 t_i (y_i - \\alpha - \\beta t_i) = 0$$  \n这可以简化为以下方程组：\n$$\\sum_{i=1}^4 y_i = n\\alpha + \\beta \\sum_{i=1}^4 t_i$$  \n$$\\sum_{i=1}^4 t_i y_i = \\alpha \\sum_{i=1}^4 t_i + \\beta \\sum_{i=1}^4 t_i^2$$  \n其中 $n = 4$。计算所需的和：\n$$\\sum_{i=1}^4 t_i = 1 + 2 + 3 + 4 = 10$$  \n$$\\sum_{i=1}^4 t_i^2 = 1^2 + 2^2 + 3^2 + 4^2 = 1 + 4 + 9 + 16 = 30$$  \n$$\\sum_{i=1}^4 y_i = 3 + 1 + 4 + 6 = 14$$  \n$$\\sum_{i=1}^4 t_i y_i = (1 \\cdot 3) + (2 \\cdot 1) + (3 \\cdot 4) + (4 \\cdot 6) = 3 + 2 + 12 + 24 = 41$$  \n代入正规方程组：\n$$14 = 4\\alpha + 10\\beta \\quad (1)$$  \n$$41 = 10\\alpha + 30\\beta \\quad (2)$$  \n求解该方程组。将方程(1)乘以5：\n$$70 = 20\\alpha + 50\\beta \\quad (1a)$$  \n将方程(2)乘以2：\n$$82 = 20\\alpha + 60\\beta \\quad (2a)$$  \n将方程(2a)减去方程(1a)：\n$$82 - 70 = (20\\alpha + 60\\beta) - (20\\alpha + 50\\beta)$$  \n$$12 = 10\\beta$$  \n$$\\beta = \\frac{12}{10} = \\frac{6}{5}$$", "answer": "$$\\boxed{\\dfrac{6}{5}}$$", "id": "1031896"}, {"introduction": "虽然直线很有用，但许多现象需要更灵活的模型，例如多项式。练习 [@problem_id:2408214] 探索了使用多项式进行近似的方法，并揭示了一个关键陷阱：过拟合。通过将不同阶数的多项式拟合到著名的龙格函数上，您将通过计算直观地看到模型如何能够完美匹配数据点，却无法捕捉其内在趋势，从而导致糟糕的预测能力。这个练习生动地展示了模型复杂性与泛化能力之间的权衡，这是数值分析和机器学习中的一个核心概念。", "problem": "给定一个定义在 $[-1,1]$ 上的实值函数 $f:\\,[-1,1]\\to\\mathbb{R}$，其表达式为 $f(x)=\\dfrac{1}{1+25x^{2}}$。对于给定的正整数 $m\\geq 2$，定义等距采样点 $x_i=-1+\\dfrac{2i}{m-1}$，其中 $i=0,1,\\dots,m-1$。令 $y_i=f(x_i)$ 对所有 $i$ 成立。对于给定的非负整数 $n$（满足 $n\\leq m-1$），考虑一个次数至多为 $n$ 的多项式空间 $\\mathcal{P}_n=\\{p:\\,p(x)=\\sum_{j=0}^{n}c_j x^{j}\\}$。定义函数 $g,h:[-1,1]\\to\\mathbb{R}$ 上的离散内积为 $\\langle g,h\\rangle_m=\\sum_{i=0}^{m-1} g(x_i)\\,h(x_i)$。设 $p_n\\in\\mathcal{P}_n$ 是在所有 $p\\in\\mathcal{P}_n$ 中使离散平方和 $\\sum_{i=0}^{m-1}\\big(p(x_i)-y_i\\big)^2$ 最小的任意多项式。这个 $p_n$ 是 $f$ 在 $\\mathcal{P}_n$ 上关于内积 $\\langle\\cdot,\\cdot\\rangle_m$ 的正交投影。\n\n定义一个验证网格 $G$，它由 $[-1,1]$ 上的 $N_v=1001$ 个等距点组成。$p_n$ 相对于 $f$ 的验证均方根（RMS）误差为\n$$\nE_{\\text{val}}=\\sqrt{\\frac{1}{N_v}\\sum_{x\\in G}\\big(p_n(x)-f(x)\\big)^2}\\,.\n$$\n您必须编写一个完整的、可运行的程序，该程序能对每个指定的测试用例计算如上定义的 $E_{\\text{val}}$。不涉及物理单位。如果出现任何角度，都必须以弧度为单位进行解释。使用标准四舍五入将每个 $E_{\\text{val}}$ 精确到小数点后 $10$ 位。\n\n要使用的参数对 $(m,n)$ 的测试套件：\n- 用例 1：$(m,n)=(21,5)$。\n- 用例 2：$(m,n)=(21,20)$。\n- 用例 3：$(m,n)=(2,1)$。\n- 用例 4：$(m,n)=(41,20)$。\n- 用例 5：$(m,n)=(9,8)$。\n\n您的程序应生成单行输出，其中包含用方括号括起来的、按上述用例顺序排列的、以逗号分隔的结果列表。例如，一个可接受的格式是 $[e_1,e_2,e_3,e_4,e_5]$，其中每个 $e_k$ 是用例 $k$ 的四舍五入后的验证 RMS 误差，表示为十进制数。", "solution": "该问题要求通过离散最小二乘法确定给定函数 $f(x)$ 的一个多项式逼近 $p_n(x)$，然后评估该逼近的精度。\n\n需要逼近的函数是龙格函数 (Runge function)，定义为 $f(x) = \\dfrac{1}{1+25x^{2}}$，定义域为 $x \\in [-1, 1]$。对于每个测试用例，我们给定一个整数 $m \\geq 2$ 指定采样点数量，以及一个整数 $n \\geq 0$ 指定最大多项式次数，并满足约束 $n \\leq m-1$。\n\n$m$ 个采样点，记为 $x_i$（其中 $i=0, 1, \\dots, m-1$），规定在区间 $[-1, 1]$ 内等距分布，使得 $x_i = -1 + \\dfrac{2i}{m-1}$。在这些点上对应的函数值为 $y_i = f(x_i)$。\n\n我们的任务是在所有次数至多为 $n$ 的多项式空间 $\\mathcal{P}_n$ 中找到一个多项式 $p_n(x)$。这个多项式在单项式基中定义为 $p_n(x) = \\sum_{j=0}^{n} c_j x^j$。其系数由向量 $\\mathbf{c} = [c_0, c_1, \\dots, c_n]^T$ 表示，必须选择这些系数以最小化多项式与函数在采样点上差值的平方和：\n$$\nS(\\mathbf{c}) = \\sum_{i=0}^{m-1} \\left( p_n(x_i) - y_i \\right)^2 = \\sum_{i=0}^{m-1} \\left( \\left( \\sum_{j=0}^{n} c_j x_i^j \\right) - y_i \\right)^2\n$$\n这个最小化问题是一个经典的线性最小二乘问题。它可以通过定义一个 $m \\times (n+1)$ 的范德蒙矩阵 (Vandermonde matrix) $\\mathbf{A}$，其元素为 $A_{ij} = x_i^j$（其中 $i=0, \\dots, m-1$ 且 $j=0, \\dots, n$），用矩阵代数来表述。如果我们令 $\\mathbf{y}$ 为采样值的 $m \\times 1$ 列向量，$\\mathbf{y} = [y_0, y_1, \\dots, y_{m-1}]^T$，那么目标就是最小化残差向量的欧几里得范数的平方：\n$$\nS(\\mathbf{c}) = \\| \\mathbf{A}\\mathbf{c} - \\mathbf{y} \\|_2^2\n$$\n最小化此表达式的唯一系数向量 $\\mathbf{c}$ 是正规方程组的解：\n$$\n\\mathbf{A}^T \\mathbf{A} \\mathbf{c} = \\mathbf{A}^T \\mathbf{y}\n$$\n约束 $n \\leq m-1$ 意味着至少有 $n+1$ 个不同的采样点，这确保了矩阵 $\\mathbf{A}$ 的列是线性无关的。这反过来保证了格拉姆矩阵 (Gram matrix) $\\mathbf{A}^T\\mathbf{A}$ 是对称正定的，因此是可逆的，从而确保了 $\\mathbf{c}$ 有唯一解。虽然正规方程组提供了理论解，但由于 $\\mathbf{A}^T\\mathbf{A}$ 可能的病态性，直接计算可能会遭受数值不稳定性的影响。在计算上，采用基于矩阵分解的方法（如对 $\\mathbf{A}$ 进行 QR 分解或奇异值分解 (SVD)）更为优越，这些方法已在高质量的数值库中实现。\n\n在 $n = m-1$ 的特定情况下，待定系数的数量 $n+1$ 与采样点的数量 $m$ 相同。矩阵 $\\mathbf{A}$ 变成一个可逆的方阵。此时，最小二乘解对应于线性系统 $\\mathbf{A}\\mathbf{c} = \\mathbf{y}$ 的精确解。这意味着多项式 $p_n(x)$ 精确地插值了数据点，满足对所有 $i$ 都有 $p_n(x_i) = y_i$。\n\n在为给定的 $(m,n)$ 对计算出最优系数向量 $\\mathbf{c}$ 后，多项式 $p_n(x)$ 就确定了。然后在一个精细的验证网格 $G$ 上评估其精度，该网格由 $[-1, 1]$ 上的 $N_v = 1001$ 个等距点组成。验证均方根（RMS）误差定义和计算如下：\n$$\nE_{\\text{val}} = \\sqrt{\\frac{1}{N_v} \\sum_{x \\in G} \\left( p_n(x) - f(x) \\right)^2}\n$$\n因此，为每个测试用例 $(m, n)$ 执行的算法如下：\n1.  生成 $m$ 个采样点 $x_i$ 并计算相应的函数值 $y_i=f(x_i)$。\n2.  求解线性最小二乘问题 $\\min_{\\mathbf{c}} \\| \\mathbf{A}\\mathbf{c} - \\mathbf{y} \\|_2^2$ 以获得多项式系数 $\\mathbf{c}$，其中 $A_{ij} = x_i^j$。为此目的，使用一个数值稳定的库函数。\n3.  建立包含 $N_v=1001$ 个点的验证网格 $G$。\n4.  在 $G$ 中的所有点 $x$ 上，计算已确定的多项式 $p_n(x)$ 和原始函数 $f(x)$ 的值。\n5.  根据这些计算值计算 RMS 误差 $E_{\\text{val}}$。\n6.  将结果四舍五入到小数点后 $10$ 位。\n\n将此系统化步骤应用于每个指定的测试用例，以产生最终结果。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the validation RMS error for polynomial least-squares approximations\n    of the Runge function for several test cases.\n    \"\"\"\n\n    def f(x: np.ndarray) -> np.ndarray:\n        \"\"\"\n        The Runge function to be approximated.\n        f(x) = 1 / (1 + 25*x^2)\n        \"\"\"\n        return 1.0 / (1.0 + 25.0 * x**2)\n\n    # Test suite of parameter pairs (m, n)\n    # m: number of sample points\n    # n: degree of the polynomial\n    test_cases = [\n        (21, 5),\n        (21, 20),\n        (2, 1),\n        (41, 20),\n        (9, 8),\n    ]\n\n    results = []\n\n    # Define the validation grid G\n    N_v = 1001\n    x_val = np.linspace(-1.0, 1.0, N_v)\n    f_val = f(x_val)\n\n    for m, n in test_cases:\n        # Step 1: Generate m equispaced sample points and their function values.\n        x_samples = np.linspace(-1.0, 1.0, m)\n        y_samples = f(x_samples)\n\n        # Step 2: Find the polynomial p_n of degree n that best fits the\n        # (x_samples, y_samples) data in a least-squares sense.\n        # The numpy.polynomial.polynomial.polyfit function solves this by\n        # finding the coefficients c that minimize the squared error.\n        # The coefficients are returned for the basis 1, x, x^2, ..., x^n.\n        coeffs = np.polynomial.polynomial.polyfit(x_samples, y_samples, n)\n\n        # Step 3: Evaluate the obtained polynomial p_n on the validation grid.\n        p_n_val = np.polynomial.polynomial.polyval(x_val, coeffs)\n\n        # Step 4: Compute the validation root mean square (RMS) error.\n        squared_errors = (p_n_val - f_val)**2\n        mean_squared_error = np.mean(squared_errors)\n        rms_error = np.sqrt(mean_squared_error)\n\n        # Step 5: Round the result to 10 decimal places as specified.\n        rounded_error = round(rms_error, 10)\n        results.append(rounded_error)\n\n    # Format the final output as a comma-separated list in brackets.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2408214"}, {"introduction": "现实世界通常是非线性的，描述它的模型也是如此。最后一个练习 [@problem_id:3152233] 将带您超越线性模型，进入非线性最小二乘优化的领域。您将实现高斯-牛顿算法，这是一种强大的迭代方法，它通过一系列线性问题来逼近非线性问题。通过这个练习，您将亲手实践数值优化的核心机制，包括理解雅可比矩阵的作用和使用线搜索确保收敛的重要性，从而掌握解决更广泛拟合问题的关键技能。", "problem": "你需要实现一个完整的高斯-牛顿求解器，用于拟合一个非线性最小二乘模型，并从多个初始猜测凭经验检验其收敛性。请完全使用纯数学术语，并从最小二乘法和一阶泰勒近似的基本原理推导出该算法。\n\n给定一个输入-输出对的数据集 $\\{(x_i, y_i)\\}_{i=1}^n$ 和一个参数模型 $y \\approx f(x,\\theta)$（参数向量为 $\\theta \\in \\mathbb{R}^p$），目标是最小化非线性最小二乘目标 $S(\\theta) = \\tfrac{1}{2}\\sum_{i=1}^n r_i(\\theta)^2$，其中 $r_i(\\theta) = y_i - f(x_i,\\theta)$ 是残差。从最小二乘目标的定义和 $f(x,\\theta)$ 在当前估计值附近的一阶泰勒近似出发，推导出高斯-牛顿更新规则，并将其实现为一个程序，在每次迭代中使用线性最小二乘法求解参数增量。所得算法必须在必要时通过回溯步长来确保目标值非递增。\n\n模型和数据：\n- 使用双参数指数模型 $f(x,\\theta) = \\theta_1 \\exp(\\theta_2 x)$，其中 $\\theta = (\\theta_1,\\theta_2)^\\top \\in \\mathbb{R}^2$。\n- 在输入 $x = [\\,0,\\ 0.25,\\ 0.5,\\ 0.75,\\ 1.0\\,]$ 处使用 $n = 5$ 个数据点。\n- 设真实参数为 $\\theta^\\star = (2.0,-1.0)^\\top$，并通过 $y_i = \\theta_1^\\star \\exp(\\theta_2^\\star x_i) + \\varepsilon_i$ 定义测量输出，其中确定性噪声向量为 $\\varepsilon = [\\,0.01,\\ -0.02,\\ 0.015,\\ -0.005,\\ 0.0\\,]$。所有数值均无单位。\n\n算法要求：\n- 在每次迭代中，计算残差向量 $r(\\theta) \\in \\mathbb{R}^n$ 和模型的雅可比矩阵 $J_f(\\theta) \\in \\mathbb{R}^{n \\times p}$，其中 $(i,j)$ 项为 $\\partial f(x_i,\\theta)/\\partial \\theta_j$。使用一阶泰勒近似来推导并求解用于参数增量的线性最小二乘子问题。\n- 对高斯-牛顿方向使用回溯线搜索，以确保目标 $S(\\theta)$ 非递增。如果一个完整步长增加了 $S(\\theta)$ 或产生非有限值，则重复地将步长乘以一个常数因子进行缩减，直到观察到目标下降或达到最小步长阈值。\n- 当参数增量范数低于容差、梯度范数低于容差或达到最大迭代次数时，终止算法。\n\n收敛性评估：\n- 对于给定的初始猜测，算法终止后，如果 $\\|\\hat{\\theta} - \\theta^\\star\\|_2 \\le 0.2$（其中 $\\hat{\\theta}$ 是最终估计值），则宣布收敛成功。否则，宣布失败。\n\n测试套件：\n对以下 $7$ 个 $\\theta_0$ 的初始猜测运行你的求解器：\n- 情况 1：$\\theta_0 = (1.0,\\ 0.0)^\\top$。\n- 情况 2：$\\theta_0 = (0.1,\\ -3.0)^\\top$。\n- 情况 3：$\\theta_0 = (5.0,\\ -0.5)^\\top$。\n- 情况 4：$\\theta_0 = (-1.0,\\ -1.0)^\\top$。\n- 情况 5：$\\theta_0 = (2.0,\\ 2.0)^\\top$。\n- 情况 6：$\\theta_0 = (0.0,\\ 0.0)^\\top$。\n- 情况 7：$\\theta_0 = (1.0,\\ 50.0)^\\top$。\n\n超参数：\n- 使用最大迭代次数 $50$。\n- 使用步长范数容差 $10^{-8}$ 和梯度范数容差 $10^{-8}$。\n- 对于回溯，步长 $\\alpha$ 从 $1$ 开始，根据需要乘以因子 $0.5$ 进行缩减，并在 $\\alpha  10^{-8}$ 时中止回溯。\n\n数值安全性：\n- 如果在某次迭代中（包括初始猜测），对 $f(x,\\theta)$ 或 $J_f(\\theta)$ 的任何求值产生了非有限数，立即停止并宣布该情况为失败。\n\n最终输出格式：\n你的程序必须生成单行输出，其中包含 $7$ 个测试用例的结果，格式为方括号内由逗号分隔的布尔值列表，顺序与上述用例一致。例如，一个有效的输出形如 $[\\text{True},\\text{False},\\text{True},\\text{True},\\text{False},\\text{True},\\text{False}]$。布尔值表示对于每个初始猜测，求解器是否成功收敛（根据上述标准）。", "solution": "任务是推导并实现高斯-牛顿算法，以解决一个特定指数模型的非线性最小二乘问题。该算法的收敛性将通过多个初始参数猜测进行经验性测试。\n\n### 1. 非线性最小二乘问题\n\n给定一组 $n$ 个数据点 $\\{(x_i, y_i)\\}_{i=1}^n$ 和一个依赖于参数向量 $\\theta \\in \\mathbb{R}^p$ 的非线性模型函数 $f(x, \\theta)$，我们的目标是找到最能拟合数据的参数 $\\theta$。拟合质量由残差平方和来衡量。第 $i$ 个数据点的残差定义为观测输出 $y_i$ 与模型预测值 $f(x_i, \\theta)$ 之间的差：\n$$\nr_i(\\theta) = y_i - f(x_i, \\theta)\n$$\n需要最小化的非线性最小二乘目标函数是这些残差的平方和，为方便起见乘以 $\\frac{1}{2}$：\n$$\nS(\\theta) = \\frac{1}{2} \\sum_{i=1}^n r_i(\\theta)^2\n$$\n在向量表示法中，如果我们定义残差向量 $r(\\theta) = [r_1(\\theta), \\dots, r_n(\\theta)]^\\top$，则目标函数为：\n$$\nS(\\theta) = \\frac{1}{2} r(\\theta)^\\top r(\\theta) = \\frac{1}{2} \\|r(\\theta)\\|_2^2\n$$\n\n### 2. 高斯-牛顿算法的推导\n\n高斯-牛顿法是一种用于最小化 $S(\\theta)$ 的迭代算法。从一个初始猜测 $\\theta_k$ 开始，我们寻求一个更新步长 $\\Delta\\theta$，使得 $\\theta_{k+1} = \\theta_k + \\Delta\\theta$ 能够得到一个更小的 $S(\\theta)$ 值。\n\n核心思想是在当前参数估计 $\\theta_k$ 附近对模型函数 $f(x_i, \\theta)$ 进行线性化。对残差向量 $r(\\theta)$ 的每个分量在 $\\theta_k$ 附近使用一阶泰勒展开，我们得到：\n$$\nr_i(\\theta_k + \\Delta\\theta) \\approx r_i(\\theta_k) + \\nabla_\\theta r_i(\\theta_k)^\\top \\Delta\\theta\n$$\n残差关于 $\\theta$ 的梯度是 $\\nabla_\\theta r_i(\\theta_k) = -\\nabla_\\theta f(x_i, \\theta_k)$。令 $J_f(\\theta)$ 为模型函数 $f$ 的雅可比矩阵，这是一个 $n \\times p$ 矩阵，其 $(i, j)$ 项为 $\\frac{\\partial f(x_i, \\theta)}{\\partial \\theta_j}$。因此，$J_f(\\theta_k)$ 的第 $i$ 行为 $\\nabla_\\theta f(x_i, \\theta_k)^\\top$。\n\n以向量形式表示，整个残差向量的近似为：\n$$\nr(\\theta_k + \\Delta\\theta) \\approx r(\\theta_k) - J_f(\\theta_k) \\Delta\\theta\n$$\n将此线性近似代入目标函数 $S(\\theta)$，得到关于 $\\Delta\\theta$ 的二次近似目标：\n$$\nS(\\theta_k + \\Delta\\theta) \\approx \\frac{1}{2} \\|r(\\theta_k) - J_f(\\theta_k) \\Delta\\theta\\|_2^2\n$$\n高斯-牛顿法寻找使该二次近似最小化的步长 $\\Delta\\theta$。这是一个线性最小二乘问题：\n$$\n\\min_{\\Delta\\theta} \\|J_f(\\theta_k) \\Delta\\theta - r(\\theta_k)\\|_2^2\n$$\n这个标准线性最小二乘问题的解由正规方程给出：\n$$\n(J_f(\\theta_k)^\\top J_f(\\theta_k)) \\Delta\\theta = J_f(\\theta_k)^\\top r(\\theta_k)\n$$\n求解这个关于 $\\Delta\\theta$ 的 $p \\times p$ 线性系统，得到高斯-牛顿搜索方向。参数更新则为 $\\theta_{k+1} = \\theta_k + \\alpha_k \\Delta\\theta$，其中 $\\alpha_k$ 是为确保目标函数 $S(\\theta)$ 下降而选择的步长。\n\n对于其中一个终止准则，我们需要目标函数的梯度 $\\nabla S(\\theta)$。它的第 $j$ 个分量是：\n$$\n\\frac{\\partial S}{\\partial \\theta_j} = \\sum_{i=1}^n r_i(\\theta) \\frac{\\partial r_i}{\\partial \\theta_j} = \\sum_{i=1}^n r_i(\\theta) \\left( -\\frac{\\partial f(x_i, \\theta)}{\\partial \\theta_j} \\right) = - \\sum_{i=1}^n (J_f(\\theta))_{ij} r_i(\\theta)\n$$\n以向量形式表示，梯度由下式给出：\n$$\n\\nabla S(\\theta) = -J_f(\\theta)^\\top r(\\theta)\n$$\n\n### 3. 对特定模型的应用\n\n问题指定了双参数指数模型：\n$$\nf(x, \\theta) = \\theta_1 e^{\\theta_2 x}, \\quad \\text{with } \\theta = [\\theta_1, \\theta_2]^\\top\n$$\n为构建雅可比矩阵 $J_f(\\theta) \\in \\mathbb{R}^{n \\times 2}$，我们计算 $f$ 相对于 $\\theta_1$ 和 $\\theta_2$ 的偏导数：\n$$\n\\frac{\\partial f}{\\partial \\theta_1} = e^{\\theta_2 x}\n$$\n$$\n\\frac{\\partial f}{\\partial \\theta_2} = \\theta_1 x e^{\\theta_2 x}\n$$\n对于一组 $n$ 个输入 $x_1, \\dots, x_n$，雅可比矩阵为：\n$$\nJ_f(\\theta) = \\begin{pmatrix}\n\\frac{\\partial f(x_1, \\theta)}{\\partial \\theta_1}  \\frac{\\partial f(x_1, \\theta)}{\\partial \\theta_2} \\\\\n\\vdots  \\vdots \\\\\n\\frac{\\partial f(x_n, \\theta)}{\\partial \\theta_1}  \\frac{\\partial f(x_n, \\theta)}{\\partial \\theta_2}\n\\end{pmatrix} =\n\\begin{pmatrix}\ne^{\\theta_2 x_1}  \\theta_1 x_1 e^{\\theta_2 x_1} \\\\\n\\vdots  \\vdots \\\\\ne^{\\theta_2 x_n}  \\theta_1 x_n e^{\\theta_2 x_n}\n\\end{pmatrix}\n$$\n\n### 4. 算法实现\n\n求解器实现如下：\n\n1.  **数据生成**：一次性生成数据点 $(x_i, y_i)$。输入为 $x = [\\,0, 0.25, 0.5, 0.75, 1.0\\,]^\\top$。输出为 $y_i = \\theta_1^\\star \\exp(\\theta_2^\\star x_i) + \\varepsilon_i$，其中真实参数为 $\\theta^\\star = [2.0, -1.0]^\\top$，噪声向量为 $\\varepsilon = [\\,0.01, -0.02, 0.015, -0.005, 0.0\\,]^\\top$。\n\n2.  **迭代循环**：对每个初始猜测 $\\theta_0$，算法按以下方式迭代：\n    a. 在第 $k$ 次迭代时，计算残差向量 $r(\\theta_k)$ 和雅可比矩阵 $J_f(\\theta_k)$。\n    b. 检查模型或雅可比矩阵求值中是否存在非有限值（如 `inf` 或 `nan`）。如果发现，则终止对此初始猜测的优化，并声明为失败。\n    c. 计算梯度范数 $\\|\\nabla S(\\theta_k)\\|_2 = \\|J_f(\\theta_k)^\\top r(\\theta_k)\\|_2$。如果低于容差 ($10^{-8}$)，则终止并判定为收敛。\n    d. 求解线性最小二乘子问题 $J_f(\\theta_k) \\Delta\\theta \\approx r(\\theta_k)$ 以找到高斯-牛顿步长 $\\Delta\\theta$。在数值上，这是通过一个稳定的方法（如QR分解）来完成的，例如 `numpy.linalg.lstsq` 提供的功能。\n    e. 检查步长范数 $\\|\\Delta\\theta\\|_2$。如果低于容差 ($10^{-8}$)，则终止并判定为收敛。\n    f. **回溯线搜索**：为确保稳定性，我们寻找一个步长 $\\alpha \\in (0, 1]$，使得 $S(\\theta_k + \\alpha\\Delta\\theta)  S(\\theta_k)$。从 $\\alpha=1$ 开始。如果条件不满足或新参数导致非有限的函数求值，则将 $\\alpha$ 乘以一个因子 $0.5$ 并重复。如果 $\\alpha$ 低于阈值 ($10^{-8}$)，则线搜索失败，主循环终止。\n    g. 更新参数：$\\theta_{k+1} = \\theta_k + \\alpha \\Delta\\theta$。\n    h. 如果满足收敛准则、达到最大迭代次数 ($50$) 或线搜索失败，则循环终止。\n\n3.  **收敛性评估**：无论因何原因终止算法，最终的参数估计值 $\\hat{\\theta}$ 将与真实值 $\\theta^\\star$ 进行比较。如果 $\\|\\hat{\\theta} - \\theta^\\star\\|_2 \\le 0.2$，则该次运行被声明为成功。否则，为失败。此最终评估决定了每个测试用例的布尔值输出。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements a Gauss-Newton solver with backtracking line search to fit a\n    nonlinear model and assesses its convergence from multiple initial guesses.\n    \"\"\"\n\n    # --- Problem Definition ---\n    # Model: f(x, theta) = theta_1 * exp(theta_2 * x)\n    # Ground-truth parameters and data generation\n    theta_star = np.array([2.0, -1.0])\n    x_data = np.array([0.0, 0.25, 0.5, 0.75, 1.0])\n    noise = np.array([0.01, -0.02, 0.015, -0.005, 0.0])\n    y_data = theta_star[0] * np.exp(theta_star[1] * x_data) + noise\n\n    # --- Test Suite ---\n    test_cases = [\n        np.array([1.0, 0.0]),\n        np.array([0.1, -3.0]),\n        np.array([5.0, -0.5]),\n        np.array([-1.0, -1.0]),\n        np.array([2.0, 2.0]),\n        np.array([0.0, 0.0]),\n        np.array([1.0, 50.0]),\n    ]\n\n    # --- Hyperparameters ---\n    max_iterations = 50\n    tol_step = 1e-8\n    tol_grad = 1e-8\n    convergence_dist_tol = 0.2\n    \n    # Backtracking line search parameters\n    alpha_initial = 1.0\n    alpha_shrink = 0.5\n    alpha_min = 1e-8\n\n    results = []\n\n    # --- Main Loop over Test Cases ---\n    for theta_initial in test_cases:\n        theta = theta_initial.astype(np.float64).copy()\n        \n        # Suppress overflow warnings that are handled by checks\n        with np.errstate(over='ignore', invalid='ignore'):\n            \n            # --- Gauss-Newton Iteration ---\n            for k in range(max_iterations):\n                # Evaluate model, residuals, and Jacobian at current theta\n                try:\n                    exp_term = np.exp(theta[1] * x_data)\n                    f_vals = theta[0] * exp_term\n                    \n                    if not np.all(np.isfinite(f_vals)):\n                        break # Terminate due to numerical instability\n\n                    residuals = y_data - f_vals\n                    \n                    # Jacobian columns\n                    J_col1 = exp_term\n                    J_col2 = theta[0] * x_data * exp_term\n                    J = np.stack([J_col1, J_col2], axis=1)\n\n                    if not np.all(np.isfinite(J)):\n                        break # Terminate due to numerical instability\n\n                except FloatingPointError:\n                    break\n\n                # Calculate objective function value and gradient\n                S_current = 0.5 * np.sum(residuals**2)\n                grad_S = -J.T @ residuals\n                \n                # Check for convergence based on gradient norm\n                if np.linalg.norm(grad_S)  tol_grad:\n                    break\n\n                # Solve the linear least-squares subproblem: J * delta = r\n                try:\n                    delta_theta = np.linalg.lstsq(J, residuals, rcond=None)[0]\n                except np.linalg.LinAlgError:\n                    break # Failed to solve subproblem\n                \n                # Check for convergence based on parameter step norm\n                if np.linalg.norm(delta_theta)  tol_step:\n                    break\n                    \n                # --- Backtracking Line Search ---\n                alpha = alpha_initial\n                step_accepted = False\n                while alpha >= alpha_min:\n                    theta_new = theta + alpha * delta_theta\n                    \n                    try:\n                        f_vals_new = theta_new[0] * np.exp(theta_new[1] * x_data)\n                        if not np.all(np.isfinite(f_vals_new)):\n                            alpha *= alpha_shrink\n                            continue\n                        \n                        S_new = 0.5 * np.sum((y_data - f_vals_new)**2)\n                        \n                        if S_new  S_current:\n                            theta = theta_new\n                            step_accepted = True\n                            break\n                        else:\n                            alpha *= alpha_shrink\n                    except FloatingPointError:\n                        alpha *= alpha_shrink\n\n                if not step_accepted:\n                    # Line search failed to find a decreasing step\n                    break\n\n        # --- Convergence Assessment ---\n        # The final estimate is the last valid `theta`\n        hat_theta = theta\n        error_norm = np.linalg.norm(hat_theta - theta_star)\n        is_success = error_norm = convergence_dist_tol\n        results.append(is_success)\n\n    # Format the final output as a comma-separated list of booleans\n    # The map(str,...) is used to get 'True'/'False' strings\n    print(f\"[{','.join(map(str, [r.item() for r in results]))}]\")\n\nsolve()\n\n```", "id": "3152233"}]}