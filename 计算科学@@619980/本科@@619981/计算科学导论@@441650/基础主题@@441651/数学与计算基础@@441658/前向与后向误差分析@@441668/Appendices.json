{"hands_on_practices": [{"introduction": "理论是灰色的，而实践之树常青。理解前向误差和后向误差最有效的方法莫过于亲手实践。本节的第一个练习将带你直面数值计算中最常见的陷阱之一：灾难性相消。你将为函数 $f(x)=\\sqrt{1+x}-1$ 实现两种算法——一种是直接计算的“天真”算法，另一种是经过代数变换的稳定算法。通过用代码计算并比较它们的真实误差，你将直观地看到数值稳定性在实践中意味着什么。[@problem_id:3232048]", "problem": "设计并实现一个程序，对量级很小的 $x$ 计算函数 $f(x)=\\sqrt{1+x}-1$ 时，进行一次基于基本原理的前向误差和后向误差研究。从浮点计算中前向误差和后向误差的核心定义出发，并基于标准且经过充分检验的浮点模型：对于任何基本算术运算或初等函数求值，计算结果满足 $\\mathrm{fl}(a\\ \\mathrm{op}\\ b) = (a\\ \\mathrm{op}\\ b)(1+\\varepsilon)$ 且 $|\\varepsilon|\\le u$，其中 $u$ 是该格式的单位舍入误差；对于一个良态的初等函数 $g$，$\\mathrm{fl}(g(a)) = g(a)(1+\\varepsilon)$ 且 $|\\varepsilon|\\le u$。你的推导和算法设计必须依赖于这些原则。\n\n任务：\n- 构建两种在二进制64位浮点算术（即 $u \\approx 2^{-53}$）中计算 $f(x)$ 的算法：\n  - 算法 A：直接使用公式，该公式在 $x$ 很小时会出现相消误差。\n  - 算法 B：一个通过代数变换推导出的、在数学上等价且数值稳定的形式，它避免了灾难性相消。\n- 对每种算法，将输入 $x$ 的前向误差定义为 $|\\widehat{f}(x) - f(x)|$，其中 $\\widehat{f}(x)$ 是算法的浮点计算结果，$f(x)$ 是数学上的精确值。使用高精度算术来近似 $f(x)$，作为真实值的替代。\n- 对每种算法，通过精确求解方程 $\\widehat{f}(x) = f(x+\\delta)$ 得到用 $\\widehat{f}(x)$ 和 $x$ 表示的 $\\delta$，从而定义后向误差 $\\delta$。报告其量级 $|\\delta|$ 作为后向误差。你的程序必须使用高精度算术来数值计算这个 $\\delta$，以避免在诊断计算本身中产生虚假的舍入误差。\n- 使用以下输入测试集：\n  - $x = 10^{-8}$，\n  - $x = 10^{-16}$，\n  - $x = -10^{-8}$，\n  - $x = 0$，\n  - $x = -1 + 10^{-15}$。\n- 对于测试集中的每个 $x$，按以下顺序输出一个包含四个浮点数的列表：\n  - 算法 A 的绝对前向误差，\n  - 算法 B 的绝对前向误差，\n  - 算法 A 的绝对后向误差 $|\\delta|$，\n  - 算法 B 的绝对后向误差 $|\\delta|$。\n- 你的程序应生成单行输出，其中包含所有测试用例的结果，格式为一个用方括号括起来的逗号分隔列表，其中每个元素是对应测试输入的四元数列表，顺序与上文所述一致。\n\n注释与约束：\n- 所有的推导都只使用上面给出的核心定义和浮点模型。不要使用或引用任何针对此特定表达式的专门的、预先推导出的误差界。\n- 内部使用高精度算术来近似精确值以计算前向和后向误差；确保替代精确值的精度远高于两种算法所使用的二进制64位算术的浮点噪声。\n- 不涉及任何物理单位；所有量均为无量纲实数。\n- 本任务不涉及角度。\n- 所有报告的数字必须是标准的十进制浮点值。", "solution": "所述问题是有效的：它在科学上基于数值分析的原理，问题适定，有明确的目标和充足的数据，并且没有主观或含糊不清的语言。因此，我们可以着手进行推导和实现。\n\n任务是为计算函数 $f(x) = \\sqrt{1+x} - 1$ 进行前向和后向误差分析，分析的情形包括 $x$ 的量级很小以及 $x$ 接近 $-1$。我们将设计两种算法，基于标准的浮点算术模型分析它们的行为，并实现一个程序来为一组给定的测试输入计算和报告误差。\n\n设浮点模型定义如下：对于任何二元运算 $\\mathrm{op}$ 和任何初等函数 $g$，计算结果满足 $\\mathrm{fl}(a\\ \\mathrm{op}\\ b) = (a\\ \\mathrm{op}\\ b)(1+\\varepsilon_1)$ 和 $\\mathrm{fl}(g(a)) = g(a)(1+\\varepsilon_2)$，其中对于单位舍入误差 $u$ 有 $|\\varepsilon_i| \\le u$。对于指定的二进制64位算术，$u = 2^{-53}$。\n\n**算法设计**\n\n**算法 A (直接法):**\n该算法使用给定的公式计算 $f(x)$：\n$f_A(x) = \\sqrt{1+x} - 1$。\n对于一个 $|x|$ 很小的输入 $x$，$1+x$ 是一个非常接近 $1$ 的值。因此，其平方根 $\\sqrt{1+x}$ 也非常接近 $1$。最后一步涉及两个几乎相等的数的减法，这一现象被称为灾难性相消。此操作会导致有效数字的损失。计算结果 $\\widehat{f_A}(x)$ 将具有很大的相对误差，且无论真值 $f(x) \\approx x/2$ 的量级如何，其绝对误差都将在单位舍入误差 $u$ 的量级。这表明了数值不稳定性。\n\n**算法 B (稳定法):**\n为了避免灾难性相消，我们可以通过将分子乘以并除以其共轭表达式 $\\sqrt{1+x}+1$ 来重构 $f(x)$ 的表达式：\n$$ f(x) = (\\sqrt{1+x} - 1) \\times \\frac{\\sqrt{1+x} + 1}{\\sqrt{1+x} + 1} = \\frac{(1+x) - 1^2}{\\sqrt{1+x} + 1} = \\frac{x}{\\sqrt{1+x} + 1} $$\n这就得到了算法 B：\n$f_B(x) = \\frac{x}{\\sqrt{1+x} + 1}$。\n当 $|x|$ 很小时，分母 $\\sqrt{1+x}+1$ 接近 $2$。所涉及的运算有与 $1$ 相加、开平方、与 $1$ 相加，以及除以一个远离零的数。在当前情境下，这些运算都不是数值敏感的。该计算避免了对几乎相等的数进行减法，因此预期是数值稳定的。计算结果 $\\widehat{f_B}(x)$ 的相对误差应该是单位舍入误差 $u$ 的一个小倍数。\n\n**误差分析框架**\n\n**前向误差：**\n绝对前向误差定义为计算值 $\\widehat{f}(x)$ 与真实数学值 $f(x)$ 之差的量级：\n$$ E_f = |\\widehat{f}(x) - f(x)| $$\n为了在我们的分析中计算这个量，$f(x)$ 必须用比所研究的二进制64位算术高得多的精度来计算。这可以作为“真实值”的替代。\n\n**后向误差：**\n后向误差是衡量对输入 $x$ 的一个最小扰动 $\\delta$ 的度量，该扰动能使计算结果 $\\widehat{f}(x)$ 成为扰动后输入 $x+\\delta$ 的精确结果。我们必须找到满足以下条件的 $\\delta$：\n$$ \\widehat{f}(x) = f(x+\\delta) = \\sqrt{1 + (x+\\delta)} - 1 $$\n我们可以解这个方程得到 $\\delta$。设 $y = \\widehat{f}(x)$：\n$$ y + 1 = \\sqrt{1 + x + \\delta} $$\n$$ (y + 1)^2 = 1 + x + \\delta $$\n$$ \\delta = (y + 1)^2 - 1 - x $$\n绝对后向误差是 $|\\delta|$。此公式将用于数值计算 $\\delta$，同样使用高精度算术以确保诊断计算本身不成为误差的来源。\n\n对于一个良态问题，一个数值稳定的算法会同时具有小的前向误差和小的后向误差。对于一个病态问题，稳定算法的特点是后向误差小，但仍可能表现出大的前向误差，因为问题本身会内在地放大输入扰动。\n\n**测试用例分析**\n\n1.  对于 $x = 10^{-8}$ 和 $x = -10^{-8}$：\n    此时， $|x|$ 很小。算法 A 会遭受灾难性相消的影响，导致大的前向误差（绝对误差 $\\approx u \\approx 10^{-16}$）和相对于 $x$ 而言大的后向误差。算法 B 是稳定的，将产生小的前向和后向误差。\n\n2.  对于 $x = 10^{-16}$：\n    $x$ 的这个值小于二进制64位算术的单位舍入误差（$u = 2^{-53} \\approx 1.11 \\times 10^{-16}$）。因此，在浮点算术中，运算 $1.0 + x$ 会舍入为 $1.0$。\n    对于算法 A，$\\widehat{f_A}(x) = \\mathrm{fl}(\\sqrt{1.0} - 1.0) = 0$。真值为 $f(10^{-16}) \\approx 0.5 \\times 10^{-16}$。前向误差为 $|0 - f(10^{-16})| \\approx 0.5 \\times 10^{-16}$。后向误差 $\\delta$ 通过求解 $f(10^{-16}+\\delta)=0$ 得出，这意味着 $\\delta = -10^{-16}$。\n    对于算法 B，$\\widehat{f_B}(x) = \\mathrm{fl}(x / (\\sqrt{1.0} + 1.0)) = \\mathrm{fl}(x/2)$。结果将非常接近真值 $f(x) \\approx x/2$，因此前向和后向误差都将极其微小。\n\n3.  对于 $x = 0$：\n    两种算法都将精确计算出 $0$，而真值是 $f(0)=0$。所有的误差度量都将是 $0$。\n\n4.  对于 $x = -1 + 10^{-15}$：\n    此处 $x$ 接近 $-1$，而非 $0$。函数 $f(x)$ 的条件数是 $C_f(x) = \\left| \\frac{x f'(x)}{f(x)} \\right| = \\left| \\frac{x(\\frac{1}{2\\sqrt{1+x}})}{\\sqrt{1+x}-1} \\right| = \\left| \\frac{x}{2(1+x-\\sqrt{1+x})} \\right|$。一个更简单的形式是 $C_f(x) = \\left| \\frac{x}{2(1+x)} \\frac{\\sqrt{1+x}+1}{\\sqrt{1+x}} \\right| \\approx \\left| \\frac{x}{2\\sqrt{1+x}} \\right|$。当 $x \\to -1^+$ 时，$1+x \\to 0^+$ 且 $C_f(x) \\to \\infty$。该问题是高度病态的。\n    算法 A 的相消问题在此处不会发生，因为 $\\sqrt{1+x} = \\sqrt{10^{-15}} \\approx 3.16 \\times 10^{-8}$，这并不接近 $1$。对于此输入，算法 A 和算法 B 都是稳定的。\n    因此，两种算法都应表现出小的后向误差，量级在 $u \\approx 10^{-16}$ 左右。然而，由于问题的极端病态性，这个小的后向误差（以及初始输入的表示误差）将被极大地放大，从而导致两种算法都产生大的前向误差。前向误差的量级将大约为 $C_f(x) \\times |x| \\times u$。\n\n实现将遵循这些原则，使用高精度算术进行基准真相和误差计算。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nimport decimal\n\ndef solve():\n    \"\"\"\n    Performs a forward and backward error study for two algorithms computing\n    f(x) = sqrt(1+x) - 1.\n    \"\"\"\n    # Set a high precision for the decimal module to serve as \"exact\" arithmetic.\n    decimal.setcontext(decimal.Context(prec=100))\n\n    # Define the test cases. Using strings preserves precision for Decimal.\n    # The case '-1+1e-15' requires special construction.\n    test_case_defs = ['1e-8', '1e-16', '-1e-8', '0', ('-1', '1e-15')]\n\n    def get_decimal_from_def(case_def):\n        if isinstance(case_def, str):\n            return decimal.Decimal(case_def)\n        elif isinstance(case_def, tuple):\n            return decimal.Decimal(case_def[0]) + decimal.Decimal(case_def[1])\n\n    test_cases = [get_decimal_from_def(d) for d in test_case_defs]\n\n    all_results = []\n\n    one_dec = decimal.Decimal('1')\n\n    for x_dec in test_cases:\n        # Step 1: Compute the \"ground truth\" value using high-precision arithmetic.\n        # The function is defined for x >= -1.\n        if x_dec  -one_dec:\n            # This path is not taken by the problem's test suite.\n            continue\n        \n        # Exact mathematical value f(x)\n        f_exact = (one_dec + x_dec).sqrt() - one_dec\n\n        # Step 2: Convert input to standard float64 for algorithmic computation.\n        x_float = np.float64(x_dec)\n\n        # Step 3: Execute Algorithm A (direct, unstable for x approx 0).\n        # f_hat_A = sqrt(1+x) - 1\n        f_hat_A = np.sqrt(np.float64(1.0) + x_float) - np.float64(1.0)\n\n        # Step 4: Execute Algorithm B (re-rationalized, stable for x approx 0).\n        # f_hat_B = x / (sqrt(1+x) + 1)\n        if x_float == 0.0:\n            f_hat_B = np.float64(0.0)\n        else:\n            denominator = np.sqrt(np.float64(1.0) + x_float) + np.float64(1.0)\n            f_hat_B = x_float / denominator\n        \n        # Step 5: Compute forward errors. Convert float results to Decimal for\n        # high-precision subtraction from the exact value.\n        f_hat_A_dec = decimal.Decimal(f_hat_A)\n        f_hat_B_dec = decimal.Decimal(f_hat_B)\n        \n        forward_error_A = abs(f_hat_A_dec - f_exact)\n        forward_error_B = abs(f_hat_B_dec - f_exact)\n\n        # Step 6: Compute backward errors using the derived formula:\n        # delta = (f_hat(x) + 1)^2 - 1 - x\n        # All calculations are done in high precision.\n        delta_A = (f_hat_A_dec + one_dec)**2 - one_dec - x_dec\n        backward_error_A = abs(delta_A)\n\n        delta_B = (f_hat_B_dec + one_dec)**2 - one_dec - x_dec\n        backward_error_B = abs(delta_B)\n\n        # Append the four required error metrics for this test case.\n        # Convert final Decimal error values to float for output.\n        all_results.append([\n            float(forward_error_A),\n            float(forward_error_B),\n            float(backward_error_A),\n            float(backward_error_B)\n        ])\n\n    # Step 7: Format the final output string exactly as required.\n    # The output should be a string representing a list of lists,\n    # with no spaces inside the inner lists.\n    inner_lists_str = [f\"[{r[0]},{r[1]},{r[2]},{r[3]}]\" for r in all_results]\n    final_output = f\"[{','.join(inner_lists_str)}]\"\n    \n    print(final_output)\n\nsolve()\n```", "id": "3232048"}, {"introduction": "在第一个练习中，我们看到了数值不稳定性的后果。现在，我们将更深入地探究其根源，并学习如何量化它。这个练习聚焦于灾难性相消的核心——两个相近数值的减法 $f(\\mathbf{x}) = x_1 - x_2$。你不仅要从第一性原理推导减法运算的条件数，还要通过编程验证前向误差、后向误差与条件数之间的关键关系：$\\text{前向误差} \\approx \\text{条件数} \\times \\text{后向误差}$。这个练习将清晰地揭示，即使一个算法是后向稳定的（即输入误差很小），一个病态问题（即条件数很大）本身也会导致巨大的输出误差。[@problem_id:3132110]", "problem": "给定标量函数 $f:\\mathbb{R}^2\\to\\mathbb{R}$，定义为 $f(\\mathbf{x})=x_1-x_2$，其中 $\\mathbf{x}=(x_1,x_2)$。考虑一个简单的浮点模型，该模型首先将每个输入四舍五入到固定数量的十进制有效数字，然后精确执行减法。具体来说，对于给定的正整数 $t$，计算值为\n$$\\widehat{f}=\\operatorname{fl}_t(x_1)-\\operatorname{fl}_t(x_2),$$\n其中 $\\operatorname{fl}_t(\\cdot)$ 表示使用“最近偶数舍入”（ties-to-even）规则四舍五入到 $t$ 位十进制有效数字。\n\n您的任务是：\n1. 仅从前向误差、后向误差和条件数的核心定义出发，推断当 $x_1\\approx x_2$ 时，减法 $f(\\mathbf{x})=x_1-x_2$ 的行为。您必须：\n   - 使用输出 $y=f(\\mathbf{x})$ 和计算输出 $\\widehat{y}$ 的相对前向误差定义：\n     $$\\text{relative forward error}=\\frac{|\\widehat{y}-y|}{|y|},$$\n     假定 $y\\neq 0$。\n   - 在先舍入模型下，从后向误差的角度出发：将 $\\widehat{f}$ 解释为在某个扰动输入 $\\tilde{\\mathbf{x}}=(\\tilde{x}_1,\\tilde{x}_2)$ 下的精确结果 $f(\\tilde{\\mathbf{x}})$，并将 $\\mathbf{x}$ 的相对后向误差定义为以下量：\n     $$\\eta=\\max\\!\\left(\\frac{|\\tilde{x}_1-x_1|}{|x_1|},\\frac{|\\tilde{x}_2-x_2|}{|x_2|}\\right),$$\n     在此模型中，一个自然的选择是 $\\tilde{x}_i=\\operatorname{fl}_t(x_i)$。\n   - 从 $\\mathbf{x}$ 处相对条件数的基本定义出发，\n     $$\\kappa(\\mathbf{x})=\\lim_{\\rho\\to 0^+}\\ \\sup_{\\substack{\\Delta \\mathbf{x}\\neq \\mathbf{0}\\\\ \\max(|\\Delta x_1|/|x_1|,\\ |\\Delta x_2|/|x_2|)\\le \\rho}}\\ \\frac{\\left|\\ f(\\mathbf{x}+\\Delta \\mathbf{x})-f(\\mathbf{x})\\ \\right|/|f(\\mathbf{x})|}{\\max(|\\Delta x_1|/|x_1|,\\ |\\Delta x_2|/|x_2|)},$$\n     推导出专用于 $f(\\mathbf{x})=x_1-x_2$ 的 $\\kappa(\\mathbf{x})$ 的显式公式，并解释当 $x_1\\approx x_2$ 时其值为何会变大。\n\n2. 实现一个程序，对每个指定的测试用例计算以下内容：\n   - 精确值 $f(\\mathbf{x})=x_1-x_2$。\n   - 计算值 $\\widehat{f}=\\operatorname{fl}_t(x_1)-\\operatorname{fl}_t(x_2)$，其中 $\\operatorname{fl}_t(\\cdot)$ 的定义如上。\n   - 相对前向误差 $\\varepsilon=\\frac{|\\widehat{f}-f(\\mathbf{x})|}{|f(\\mathbf{x})|}$（所有答案必须以小数而非百分比形式报告）。\n   - 在先舍入模型下 $\\mathbf{x}$ 的相对后向误差：\n     $$\\eta=\\max\\!\\left(\\frac{|\\operatorname{fl}_t(x_1)-x_1|}{|x_1|},\\frac{|\\operatorname{fl}_t(x_2)-x_2|}{|x_2|}\\right).$$\n   - 在任务1中从第一性原理得到的相对条件数 $\\kappa(\\mathbf{x})$（实现您推导的表达式）。\n   - 放大因子 $A=\\varepsilon/\\eta$，它量化了微小的相对输入误差（后向误差）在输出（前向误差）中被放大的程度。\n\n3. 使用以下参数值的测试套件。每个测试用例是一个三元组 $(x_1,x_2,t)$，其中 $t$ 是舍入时使用的十进制有效数字位数。这些用例都不会产生 $f(\\mathbf{x})=0$ 的结果，因此相对前向误差是良定义的。\n   - 用例1（良态）：$(x_1,x_2,t)=\\left(12345.6789,\\ 2345.678901,\\ 7\\right)$。\n   - 用例2（小尺度上的强相消）：$(x_1,x_2,t)=\\left(1.0000005,\\ 1.0000004,\\ 7\\right)$。\n   - 用例3（大尺度上的强相消）：$(x_1,x_2,t)=\\left(100000003.0,\\ 100000000.0,\\ 7\\right)$。\n   - 用例4（中度相消）：$(x_1,x_2,t)=\\left(0.123456789,\\ 0.123446789,\\ 7\\right)$。\n\n4. 最终输出规范：\n   - 对于每个测试用例，按此确切顺序输出列表 $[A,\\ \\varepsilon,\\ \\eta,\\ \\kappa(\\mathbf{x})]$。\n   - 将所有用例聚合到单行中，形式为这些列表的逗号分隔列表，并用方括号括起来，例如：\n     $$\\left[\\ [A_1,\\varepsilon_1,\\eta_1,\\kappa_1],\\ [A_2,\\varepsilon_2,\\eta_2,\\kappa_2],\\ \\ldots\\ \\right].$$\n   - 以科学记数法打印所有浮点数，精确到10位有效数字（例如，$1.234567890\\times 10^{-3}$ 必须打印为 $1.2345678900\\mathrm{e}{-03}$）。\n\n科学真实性和约束条件：\n- 将所有量视为无量纲实数。\n- 不涉及角度。\n- 所有误差量必须是小数或分数，而不是百分比。\n- 对以十为底的 $\\operatorname{fl}_t(\\cdot)$ 使用“最近偶数舍入”规则。\n- 您的程序必须生成单行输出，其中包含按上述规定格式化的结果，即用方括号括起来的逗号分隔列表。", "solution": "所提出的问题是计算科学导论中一个有效且经典的练习，具体涉及一项基本算术运算的前向和后向误差分析。该问题在科学上是合理的、良态的、客观且完整的。\n\n任务是分析减法 $f(\\mathbf{x})=x_1-x_2$ 的数值稳定性，尤其是在 $x_1 \\approx x_2$ 发生相减抵消的情况下。\n\n我们将首先根据所提供的定义推导出必要的理论结果，然后实现一个程序来为给定的测试用例计算指定的量。\n\n### 第1部分：从第一性原理进行的理论分析\n\n**1. 相对条件数 $\\kappa(\\mathbf{x})$ 的推导**\n\n问题给出了函数 $f:\\mathbb{R}^2\\to\\mathbb{R}$ 在点 $\\mathbf{x}=(x_1, x_2)$ 处关于输入相对扰动的相对条件数的形式化定义：\n$$\n\\kappa(\\mathbf{x})=\\lim_{\\rho\\to 0^+}\\ \\sup_{\\substack{\\Delta \\mathbf{x}\\neq \\mathbf{0}\\\\ \\max(|\\Delta x_1|/|x_1|,\\ |\\Delta x_2|/|x_2|)\\le \\rho}}\\ \\frac{\\left|\\ f(\\mathbf{x}+\\Delta \\mathbf{x})-f(\\mathbf{x})\\ \\right|/|f(\\mathbf{x})|}{\\max(|\\Delta x_1|/|x_1|,\\ |\\Delta x_2|/|x_2|)}\n$$\n让我们针对函数 $f(\\mathbf{x}) = x_1 - x_2$ 来具体化这个定义。我们假设 $f(\\mathbf{x}) = x_1 - x_2 \\neq 0$ 且 $x_1, x_2 \\neq 0$。\n\n由扰动 $\\Delta\\mathbf{x}=(\\Delta x_1, \\Delta x_2)$ 引起的函数输出变化为：\n$$\nf(\\mathbf{x}+\\Delta \\mathbf{x}) - f(\\mathbf{x}) = ((x_1+\\Delta x_1) - (x_2+\\Delta x_2)) - (x_1 - x_2) = \\Delta x_1 - \\Delta x_2\n$$\n因此，输出的相对变化为：\n$$\n\\frac{|f(\\mathbf{x}+\\Delta \\mathbf{x}) - f(\\mathbf{x})|}{|f(\\mathbf{x})|} = \\frac{|\\Delta x_1 - \\Delta x_2|}{|x_1 - x_2|}\n$$\n设最大相对输入扰动表示为 $E_{\\text{rel}} = \\max\\left(\\frac{|\\Delta x_1|}{|x_1|}, \\frac{|\\Delta x_2|}{|x_2|}\\right)$。这意味着 $|\\Delta x_1| \\le E_{\\text{rel}} |x_1|$ 且 $|\\Delta x_2| \\le E_{\\text{rel}} |x_2|$。\n\n我们现在可以表达上确界内的比率：\n$$\n\\frac{\\left|\\ f(\\mathbf{x}+\\Delta \\mathbf{x})-f(\\mathbf{x})\\ \\right|/|f(\\mathbf{x})|}{E_{\\text{rel}}} = \\frac{|\\Delta x_1 - \\Delta x_2|/|x_1 - x_2|}{E_{\\text{rel}}}\n$$\n为了求得上确界，我们必须在给定 $E_{\\text{rel}}=\\rho$ 的情况下，最大化 $|\\Delta x_1 - \\Delta x_2|$ 这一项。使用三角不等式：\n$$\n|\\Delta x_1 - \\Delta x_2| \\le |\\Delta x_1| + |\\Delta x_2| \\le \\rho |x_1| + \\rho |x_2| = \\rho (|x_1| + |x_2|)\n$$\n这个上界可以通过选择幅度最大且作用相反的扰动来达到。例如，如果我们选择 $\\Delta x_1 = \\rho x_1$ 和 $\\Delta x_2 = -\\rho x_2$，那么 $E_{\\text{rel}} = \\max(|\\rho x_1|/|x_1|, |-\\rho x_2|/|x_2|) = \\rho$。等等，这个选择产生 $|\\Delta x_1-\\Delta x_2| = |\\rho(x_1+x_2)| = \\rho|x_1+x_2|$。这不完全是 $\\rho(|x_1|+|x_2|)$ 这个界。\n上确界通过选择 $\\Delta x_1 = \\rho \\cdot \\text{sgn}(\\Delta x_1) \\cdot |x_1|$ 和 $\\Delta x_2 = \\rho \\cdot \\text{sgn}(\\Delta x_2) \\cdot |x_2|$ 来达到，并设置符号以最大化差值。例如，选择 $\\Delta x_1 = \\rho|x_1|$ 和 $\\Delta x_2 = -\\rho|x_2|$ 不是一个有效的扰动选择，因为那将意味着 $\\Delta x_1/x_1$ 的幅度可能不为 $\\rho$。\n正确的方法是选择 $\\Delta x_1$ 和 $\\Delta x_2$ 使得 $|\\Delta x_1|/|x_1| \\le \\rho$ 且 $|\\Delta x_2|/|x_2| \\le \\rho$。为了最大化 $|\\Delta x_1 - \\Delta x_2|$，我们选取 $\\Delta x_1 = \\rho \\cdot \\text{sgn}(x_1) \\cdot |x_1|' $，例如我们想要 $\\Delta x_1$ 为正，$\\Delta x_2$ 为负。上确界是通过选取 $|\\Delta x_1| = \\rho|x_1|$、 $|\\Delta x_2| = \\rho|x_2|$ 且它们符号相反来实现的。例如，设 $\\Delta x_1 = \\rho|x_1|$ 和 $\\Delta x_2 = -\\rho|x_2|$。不，这不对。\n让我们选择 $\\Delta x_1$ 使得 $\\Delta x_1/x_1 = \\rho$（因此 $\\Delta x_1 = \\rho x_1$）以及 $\\Delta x_2$ 使得 $\\Delta x_2/x_2 = -\\rho$（因此 $\\Delta x_2 = -\\rho x_2$）。通过这些选择，$E_{rel}=\\rho$。那么 $|\\Delta x_1 - \\Delta x_2| = |\\rho x_1 - (-\\rho x_2)| = \\rho|x_1+x_2|$。\n然而，正确找到上确界的标准推导过程确定了 $\\sup |\\Delta x_1 - \\Delta x_2| = \\rho(|x_1| + |x_2|)$。这可以通过设置 $\\Delta x_1 = \\rho \\cdot \\text{sgn}(x_1)|x_1| \\implies \\Delta x_1 = \\rho x_1$ 和 $\\Delta x_2 = -\\rho \\cdot \\text{sgn}(x_2)|x_2| \\implies \\Delta x_2 = -\\rho x_2$ 来观察到，它满足约束条件（因为 $|x_1|/|x_1|=1$ 和 $|x_2|/|x_2|=1$）。结果项是 $\\rho|x_1+x_2|$。一个不同的选择是 $\\Delta x_1 = \\rho x_1$ 和 $\\Delta x_2 = -\\rho x_2$。\n正确的界确实是 $\\rho(|x_1|+|x_2|)$。当我们选择 $\\Delta x_1$ 和 $\\Delta x_2$ 符号相反且具有最大允许幅度时，达到这个最大值，例如 $\\Delta x_1 = \\rho x_1$ 和 $\\Delta x_2 = -\\rho x_2$ 不够通用。我们应该选择 $\\Delta x_1 = \\rho |x_1|$ 和 $\\Delta x_2 = -\\rho |x_2|$ 或反之，但这可能违反相对误差的约束。\n让我们考虑扰动 $\\Delta x_1 = \\rho \\cdot \\alpha_1$ 和 $\\Delta x_2 = \\rho \\cdot \\alpha_2$，其中 $|\\alpha_1/x_1| \\le 1$ 且 $|\\alpha_2/x_2| \\le 1$。我们最大化 $|\\rho \\alpha_1 - \\rho \\alpha_2|$。\n在约束条件下， $|\\Delta x_1 - \\Delta x_2|$ 的正确上确界是 $\\rho(|x_1|+|x_2|)$。\n\n将此上确界代入条件数的表达式中：\n$$\n\\kappa(\\mathbf{x}) = \\lim_{\\rho\\to 0^+} \\frac{\\sup (|\\Delta x_1 - \\Delta x_2|)/|x_1 - x_2|}{\\rho} = \\lim_{\\rho\\to 0^+} \\frac{\\rho(|x_1| + |x_2|)/|x_1 - x_2|}{\\rho}\n$$\n对 $\\rho$ 的线性依赖性被消除了，极限是平凡的。因此，推导出的公式是：\n$$\n\\kappa(\\mathbf{x}) = \\frac{|x_1| + |x_2|}{|x_1 - x_2|}\n$$\n\n**2. 当 $x_1 \\approx x_2$ 时 $\\kappa(\\mathbf{x})$ 的行为**\n\n当 $x_1$ 和 $x_2$ 的值接近时（$x_1 \\approx x_2$），分母 $|x_1 - x_2|$ 趋近于0。假设 $x_1$ 和 $x_2$ 不接近于0且符号相同（这是灾难性抵消的典型情景），分子 $|x_1| + |x_2|$ 约等于 $2|x_1|$。因此，比率 $\\kappa(\\mathbf{x})$ 会变得非常大。大的条件数意味着问题是“病态的”，即输入中的微小相对误差可能会被放大为输出中的巨大相对误差。\n\n**3. 前向误差、后向误差和条件数之间的关系**\n\n问题定义了一个“先舍入”计算模型，其中 $\\widehat{f} = \\operatorname{fl}_t(x_1) - \\operatorname{fl}_t(x_2)$。这可以从后向误差的角度来理解，即定义一个扰动输入 $\\tilde{\\mathbf{x}} = (\\tilde{x}_1, \\tilde{x}_2) = (\\operatorname{fl}_t(x_1), \\operatorname{fl}_t(x_2))$。计算出的结果 $\\widehat{f}$ 于是就是这个扰动输入下的精确结果：$\\widehat{f} = f(\\tilde{\\mathbf{x}})$。\n\n相对后向误差 $\\eta$ 是对输入的相对扰动的大小：\n$$\n\\eta = \\max\\left(\\frac{|\\tilde{x}_1-x_1|}{|x_1|},\\frac{|\\tilde{x}_2-x_2|}{|x_2|}\\right)\n$$\n相对前向误差 $\\varepsilon$ 是在输出中产生的相对扰动：\n$$\n\\varepsilon = \\frac{|\\widehat{f}-f(\\mathbf{x})|}{|f(\\mathbf{x})|} = \\frac{|f(\\tilde{\\mathbf{x}})-f(\\mathbf{x})|}{|f(\\mathbf{x})|}\n$$\n条件数 $\\kappa(\\mathbf{x})$ 将这两个量联系起来。对于小扰动，它们之间的关系近似为：\n$$\n\\varepsilon \\approx \\kappa(\\mathbf{x}) \\cdot \\eta \\quad \\text{或更正式地} \\quad \\varepsilon \\le \\kappa(\\mathbf{x}) \\cdot \\eta + O(\\eta^2)\n$$\n放大因子 $A = \\varepsilon / \\eta$ 是对由舍入引入的特定扰动下此关系的经验量化。由于条件数定义为所有可能的小扰动的上确界，我们有不等式 $A \\le \\kappa(\\mathbf{x})$。\n\n当 $x_1 \\approx x_2$ 时，$\\kappa(\\mathbf{x})$ 很大。舍入过程会引入一个小的后向误差 $\\eta$，通常在机器精度的数量级。然而，这个小的输入误差被大的条件数放大，从而导致一个潜在的巨大前向误差 $\\varepsilon$。这种现象被称为**灾难性抵消**（catastrophic cancellation）：减去几乎相等的数值会导致结果中有效数字的损失，这并非因为减法本身不精确，而是因为它放大了输入中原已存在的舍入误差。\n\n**关于输出格式的说明**\n问题陈述要求“精确到10位有效数字”，但提供的数字 $1.234567890 \\times 10^{-3}$ 的格式化示例为 `1.2345678900e-03`。该示例输出有11位有效数字（小数点前1位，小数点后10位）。这存在轻微的歧义。我们将遵循明确的示例，因为它比一般性描述更具体。这对应于Python的格式说明符 `\"{:.10e}\"`。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef fl_t(x: float, t: int) - float:\n    \"\"\"\n    Rounds a floating-point number x to t significant decimal digits.\n    Uses round-to-nearest, ties-to-even rule.\n    \"\"\"\n    if x == 0.0:\n        return 0.0\n\n    # Using np.float64 for calculations to maintain precision.\n    x = np.float64(x)\n\n    # Calculate the base-10 exponent of the number to find its magnitude.\n    exponent = np.floor(np.log10(np.abs(x)))\n    \n    # Calculate the power of 10 to scale the number.\n    # to round to t sig-figs, we round x / 10^(e-t+1) to integer.\n    scale_power = exponent - t + 1\n    \n    # Scale the number.\n    scaled_val = x / (10.0**scale_power)\n    \n    # Python 3's round() and np.round() both implement round-half-to-even.\n    rounded_scaled_val = np.round(scaled_val)\n    \n    # Scale back to the original magnitude.\n    result = rounded_scaled_val * (10.0**scale_power)\n    \n    return float(result)\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for the given test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1 (well-conditioned)\n        (12345.6789, 2345.678901, 7),\n        # Case 2 (strong cancellation at small scale)\n        (1.0000005, 1.0000004, 7),\n        # Case 3 (strong cancellation at large scale)\n        (100000003.0, 100000000.0, 7),\n        # Case 4 (moderate cancellation)\n        (0.123456789, 0.123446789, 7)\n    ]\n\n    all_results = []\n\n    for x1_in, x2_in, t in test_cases:\n        # Use high-precision numpy floats for internal calculations\n        x1 = np.float64(x1_in)\n        x2 = np.float64(x2_in)\n\n        # 1. Exact value\n        f_exact = x1 - x2\n\n        # 2. Rounded inputs and computed value\n        x1_rounded = fl_t(x1, t)\n        x2_rounded = fl_t(x2, t)\n        f_computed = x1_rounded - x2_rounded\n\n        # 3. Relative forward error\n        # Problem statement guarantees f_exact is non-zero\n        rel_forward_error = np.abs(f_computed - f_exact) / np.abs(f_exact)\n\n        # 4. Relative backward error\n        # Problem statement inputs are non-zero.\n        rel_err_x1 = np.abs(x1_rounded - x1) / np.abs(x1)\n        rel_err_x2 = np.abs(x2_rounded - x2) / np.abs(x2)\n        rel_backward_error = np.max([rel_err_x1, rel_err_x2])\n\n        # 5. Relative condition number\n        condition_number = (np.abs(x1) + np.abs(x2)) / np.abs(f_exact)\n\n        # 6. Amplification factor\n        # If backward error is zero, forward error must also be zero.\n        # For the given test cases, rel_backward_error will not be zero.\n        if rel_backward_error == 0.0:\n            # This case means the inputs didn't need rounding, so perturbation is 0.\n            # Forward error is also 0. The ratio 0/0 is indeterminate.\n            amplification_factor = 1.0\n        else:\n            amplification_factor = rel_forward_error / rel_backward_error\n\n        # Store results for this case\n        all_results.append([amplification_factor, rel_forward_error, rel_backward_error, condition_number])\n\n    # Format the output as specified in the problem statement.\n    # The example \"1.2345678900e-03\" has 11 significant digits (1 before decimal, 10 after).\n    # This corresponds to the format specifier \"{:.10e}\".\n    def format_list(data_list):\n        return [f\"{val:.10e}\" for val in data_list]\n\n    formatted_cases = []\n    for result_case in all_results:\n        formatted_list_str =\",\".join(format_list(result_case))\n        formatted_cases.append(f\"[{formatted_list_str}]\")\n        \n    final_output_str = f\"[{','.join(formatted_cases)}]\"\n\n    print(final_output_str)\n\nsolve()\n```", "id": "3132110"}, {"introduction": "掌握了标量函数中的误差分析后，是时候将这些概念应用于更复杂的实际问题了。本练习将带你进入数值线性代数的世界，比较两种经典的 QR 分解算法：经典的 Gram-Schmidt（CGS）和改进的 Gram-Schmidt（MGS）算法。除了比较它们的稳定性差异，本练习还引入了一种更深刻的后向误差视角：将算法的后向误差视为其输出（$\\widehat{Q}$ 矩阵）在多大程度上偏离了其应有的数学性质（即正交性）。通过量化 $\\lVert \\widehat{Q}^\\top \\widehat{Q} - I \\rVert_2$，你将体会到误差分析在设计和选择稳健数值算法中的核心作用。[@problem_id:3232097]", "problem": "你的任务是比较两种广泛使用的标准正交化过程——经典Gram–Schmidt和修正Gram–Schmidt——在标准浮点运算下实现时的前向误差和后向误差。请使用基于浮点运算模型的前向误差和后向误差定义：对于应用于实数的任何基本运算，其计算结果可以表示为 $\\mathrm{fl}(x \\,\\mathrm{op}\\, y) = (x \\,\\mathrm{op}\\, y)(1+\\delta)$，其中 $\\mathrm{op} \\in \\{+,-,\\times,\\div\\}$ 且 $|\\delta| \\le u$，$u$ 是所用算法的单位舍入。在精确算术中，Gram–Schmidt方法产生一个分解 $A = Q R$，其中 $Q^\\top Q = I$ 且 $R$ 是一个具有非负对角元的上三角矩阵。在浮点运算中，计算出的因子 $\\widehat{Q}$ 和 $\\widehat{R}$ 满足 $A \\approx \\widehat{Q}\\widehat{R}$ 和 $\\widehat{Q}^\\top \\widehat{Q} \\approx I$。\n\n你的程序必须：\n- 实现两个函数，给定一个满秩或秩亏的实矩阵 $A \\in \\mathbb{R}^{m \\times n}$（$m \\ge n$），分别使用以下方法返回 $(Q,R)$：\n  1. 经典Gram–Schmidt（单次，无再正交化）。\n  2. 修正Gram–Schmidt（单次，无再正交化）。\n- 对于每对 $(Q,R)$ 和每个输入矩阵 $A$，计算：\n  1. 分解的前向误差，即相对谱范数残差\n     $$\\eta_{\\mathrm{fwd}} = \\frac{\\lVert A - Q R \\rVert_2}{\\lVert A \\rVert_2}.$$\n  2. 后向误差，以正交性损失表示\n     $$F = Q^\\top Q - I,\\quad \\eta_{\\mathrm{orth}} = \\lVert F \\rVert_2.$$\n- 对所有矩阵范数均使用谱范数 $\\lVert \\cdot \\rVert_2$。\n- 将报告的每个标量值四舍五入到 $12$ 位小数。\n\n测试集：\n- 情况1（良态高矩阵）：\n  $$A_1 = \\begin{bmatrix}\n  1   2   3   4 \\\\\n  2   1   0   1 \\\\\n  0   1   2   3 \\\\\n  1   0   1   0 \\\\\n  2   2   2   2 \\\\\n  3   1   4   1\n  \\end{bmatrix} \\in \\mathbb{R}^{6 \\times 4}.$$\n- 情况2（近似线性相关的列）：令 $\\varepsilon = 10^{-8}$ 且\n  $$c_1 = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\\\ 4 \\\\ 5 \\end{bmatrix},\\quad\n  w = \\begin{bmatrix} 1 \\\\ -1 \\\\ 1 \\\\ -1 \\\\ 1 \\end{bmatrix},\\quad\n  c_2 = c_1 + \\varepsilon w,\\quad\n  c_3 = \\begin{bmatrix} 2 \\\\ 0 \\\\ 1 \\\\ 0 \\\\ 2 \\end{bmatrix},\\quad\n  A_2 = [\\, c_1\\ \\ c_2\\ \\ c_3 \\,] \\in \\mathbb{R}^{5 \\times 3}.$$\n- 情况3（希尔伯特型高矩阵，病态列）：对于 $m = 8, n = 5$，定义\n  $$\\left(A_3\\right)_{i,j} = \\frac{1}{i + j + 1},\\quad 0 \\le i \\le 7,\\ 0 \\le j \\le 4,$$\n  因此 $A_3 \\in \\mathbb{R}^{8 \\times 5}$。\n- 情况4（已是标准正交的列）：令 $m = 6, n = 3$ 且\n  $$A_4 = \\begin{bmatrix}\n  1   0   0 \\\\\n  0   0   0 \\\\\n  0   1   0 \\\\\n  0   0   0 \\\\\n  0   0   0 \\\\\n  0   0   1\n  \\end{bmatrix} \\in \\mathbb{R}^{6 \\times 3}.$$\n\n对于每个测试用例 $A_k$，计算四个标量值：\n- $\\eta_{\\mathrm{fwd}}^{\\mathrm{CGS}}$,\n- $\\eta_{\\mathrm{fwd}}^{\\mathrm{MGS}}$,\n- $\\eta_{\\mathrm{orth}}^{\\mathrm{CGS}}$,\n- $\\eta_{\\mathrm{orth}}^{\\mathrm{MGS}}$,\n每个值都四舍五入到 $12$ 位小数。\n\n最终输出格式：\n- 你的程序应生成单行输出，包含所有四个测试用例的结果，格式为逗号分隔的列表的列表，每个内部列表按\n  $$[\\, \\eta_{\\mathrm{fwd}}^{\\mathrm{CGS}},\\ \\eta_{\\mathrm{fwd}}^{\\mathrm{MGS}},\\ \\eta_{\\mathrm{orth}}^{\\mathrm{CGS}},\\ \\eta_{\\mathrm{orth}}^{\\mathrm{MGS}} \\,],$$\n  顺序排列，并用方括号括起来。例如：\n  $$[\\ [a_1,b_1,c_1,d_1],\\ [a_2,b_2,c_2,d_2],\\ [a_3,b_3,c_3,d_3],\\ [a_4,b_4,c_4,d_4]\\ ].$$\n\n所有计算必须在标准双精度浮点运算中执行。不允许用户输入；程序必须是自包含的，并且必须严格按照指定格式打印一行。所有报告的值都是无单位的实数，并四舍五入到 $12$ 位小数。", "solution": "该问题陈述是数值线性代数领域一个有效、适定且客观的练习。它要求比较用于计算矩阵QR分解的两种标准算法：经典Gram–Schmidt（CGS）和修正Gram–Schmidt（MGS）。该问题具有科学依据，为所涉及的算法和待计算的误差度量提供了精确的数学定义。所有必要的数据和测试条件均已提供，不存在任何歧义或矛盾。任务是实现指定的算法和度量标准，并将它们应用于一系列旨在突显CGS和MGS数值特性的测试用例。\n\n问题的核心是计算给定矩阵 $A \\in \\mathbb{R}^{m \\times n}$（其中 $m \\ge n$）的QR分解，即 $A=QR$，其中 $Q \\in \\mathbb{R}^{m \\times n}$ 具有标准正交列（$Q^\\top Q = I_n$），$R \\in \\mathbb{R}^{n \\times n}$ 是一个上三角矩阵。在浮点运算中，计算出的因子（记为 $\\widehat{Q}$ 和 $\\widehat{R}$）只能近似地满足这些性质。\n\nGram-Schmidt过程是构造这种分解的一种基本方法。它生成一系列标准正交向量 $q_0, q_1, \\dots, q_{n-1}$，这些向量构成了 $A$ 的列空间的一个标准正交基。需要实现的两个变体是：\n\n1. **经典Gram–Schmidt (CGS)**：该算法通过从 $A$ 的相应列 $a_j$ 中显式地减去其在所有先前计算出的标准正交向量 $q_0, \\dots, q_{j-1}$ 上的投影来计算每个向量 $q_j$。对于第 $j$ 列的所有投影计算，都使用向量 $a_j$。第 $j$ 列的步骤是：\n$$v_j = a_j - \\sum_{i=0}^{j-1} (q_i^\\top a_j) q_i$$\n$$r_{jj} = \\lVert v_j \\rVert_2, \\quad q_j = v_j / r_{jj}$$\n系数 $r_{ij} = q_i^\\top a_j$（$i  j$）也会被计算。\n虽然CGS在数学上是正确的，但它在数值上是不稳定的。计算 $q_i^\\top a_j$ 时微小的舍入误差可能导致计算出的 $\\widehat{Q}$ 的列严重丧失正交性。\n\n2. **修正Gram–Schmidt (MGS)**：这是CGS在代数上等价但数值上更稳定的一个重排版本。MGS不是将 $a_j$ 投影到每个 $q_i$ 上，而是在每一步更新 $A$ 的剩余列。在计算出 $q_i$ 后，将其分量从所有后续列 $a_{i+1}, \\dots, a_{n-1}$ 中移除。这可以表示为：\n令对所有 $j$ 都有 $v^{(0)}_j = a_j$。对于 $i = 0, \\dots, n-1$：\n$$r_{ii} = \\lVert v^{(i)}_i \\rVert_2, \\quad q_i = v^{(i)}_i / r_{ii}$$\n对于 $j = i+1, \\dots, n-1$：\n$$r_{ij} = q_i^\\top v^{(i)}_j, \\quad v^{(i+1)}_j = v^{(i)}_j - r_{ij} q_i$$\n这个过程确保了每个新向量都是与已经正交化了的向量进行正交化，从而减少了误差的累积，并得到一个其列向量更接近于完全正交的矩阵 $\\widehat{Q}$。\n\n在秩亏的情况下，向量 $v_j$（在CGS中）或 $v_i^{(i)}$（在MGS中）可能变为零或数值上接近于零。实现中将通过检查其范数是否低于一个小的容差（$10^{-12}$）来处理这种情况。如果是，则将相应的列 $q_j$ 设置为零向量，并将其在 $R$ 中的对角元 $r_{jj}$ 设置为 $0$。\n\n这些算法的稳定性和准确性通过两个度量来评估：\n\n1. **前向误差**：$\\eta_{\\mathrm{fwd}} = \\frac{\\lVert A - \\widehat{Q} \\widehat{R} \\rVert_2}{\\lVert A \\rVert_2}$。这衡量了分解的相对残差。较小的值表示计算出的因子 $\\widehat{Q}$ 和 $\\widehat{R}$ 更精确地重构了原始矩阵 $A$。这可以解释为分解问题本身的后向稳定性度量；计算出的因子是某个邻近矩阵 $A+E$ 的精确因子，而此误差衡量了 $E$ 的大小。\n\n2. **正交性损失**：$\\eta_{\\mathrm{orth}} = \\lVert \\widehat{Q}^\\top \\widehat{Q} - I \\rVert_2$。这是关于算法生成标准正交 $Q$ 这一性质的后向误差的直接度量。接近 $0$ 的值表示 $\\widehat{Q}$ 的列几乎是标准正交的，而接近 $1$ 的值表示正交性有显著损失。这个度量预计能够清晰地区分MGS的稳定性与CGS的不稳定性，尤其是在处理病态矩阵时。\n\n程序将实现CGS和MGS，将它们应用于四个提供的测试矩阵，并为每个分解计算两种误差度量。所有矩阵范数计算均使用谱范数 $\\lVert \\cdot \\rVert_2$。四个测试用例中每个用例的最终结果，包括两种算法的两个误差，都将四舍五入到 $12$ 位小数，并以指定的列表的列表格式呈现。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef classical_gram_schmidt(A):\n    \"\"\"\n    Computes the QR factorization of a matrix A using Classical Gram-Schmidt.\n    \"\"\"\n    m, n = A.shape\n    Q = np.zeros((m, n), dtype=float)\n    R = np.zeros((n, n), dtype=float)\n    \n    for j in range(n):\n        v = A[:, j].copy()\n        for i in range(j):\n            R[i, j] = Q[:, i].T @ A[:, j] # Project a_j onto q_i\n            v -= R[i, j] * Q[:, i]\n            \n        norm_v = np.linalg.norm(v)\n        \n        if norm_v > 1e-12:\n            R[j, j] = norm_v\n            Q[:, j] = v / R[j, j]\n        else:\n            R[j, j] = 0.0\n            # Q[:, j] remains a zero vector\n            \n    return Q, R\n\ndef modified_gram_schmidt(A):\n    \"\"\"\n    Computes the QR factorization of a matrix A using Modified Gram-Schmidt.\n    \"\"\"\n    m, n = A.shape\n    V = A.copy()\n    Q = np.zeros((m, n), dtype=float)\n    R = np.zeros((n, n), dtype=float)\n    \n    for i in range(n):\n        norm_v_i = np.linalg.norm(V[:, i])\n        R[i, i] = norm_v_i\n        \n        if R[i, i] > 1e-12:\n            Q[:, i] = V[:, i] / R[i, i]\n            for j in range(i + 1, n):\n                R[i, j] = Q[:, i].T @ V[:, j]\n                V[:, j] -= R[i, j] * Q[:, i]\n        else:\n            # R[i, i] is already set to the small norm (or 0)\n            # Q[:, i] remains a zero vector.\n            # Projections of V[:, j] onto a zero Q[:, i] are zero, so R[i, j] for j>i are zero\n            # and subsequent V columns are not modified.\n            pass\n            \n    return Q, R\n\ndef compute_errors(A, Q, R):\n    \"\"\"\n    Computes the forward error and orthogonality loss for a given QR factorization.\n    \"\"\"\n    m, n = A.shape\n    \n    # Forward error: ||A - QR||_2 / ||A||_2\n    norm_A = np.linalg.norm(A, 2)\n    if norm_A == 0:\n        fwd_error = 0.0\n    else:\n        fwd_error = np.linalg.norm(A - Q @ R, 2) / norm_A\n\n    # Orthogonality loss: ||Q^T Q - I||_2\n    if n > 0:\n        orth_error = np.linalg.norm(Q.T @ Q - np.eye(n), 2)\n    else:\n        orth_error = 0.0\n        \n    return fwd_error, orth_error\n\ndef solve():\n    \"\"\"\n    Main function to run the analysis on the defined test cases.\n    \"\"\"\n    # Case 1: Well-conditioned tall matrix\n    A1 = np.array([\n        [1, 2, 3, 4],\n        [2, 1, 0, 1],\n        [0, 1, 2, 3],\n        [1, 0, 1, 0],\n        [2, 2, 2, 2],\n        [3, 1, 4, 1]\n    ], dtype=float)\n\n    # Case 2: Nearly dependent columns\n    epsilon = 1e-8\n    c1 = np.array([1, 2, 3, 4, 5], dtype=float).reshape(-1, 1)\n    w = np.array([1, -1, 1, -1, 1], dtype=float).reshape(-1, 1)\n    c2 = c1 + epsilon * w\n    c3 = np.array([2, 0, 1, 0, 2], dtype=float).reshape(-1, 1)\n    A2 = np.hstack([c1, c2, c3])\n\n    # Case 3: Hilbert-type tall matrix\n    m3, n3 = 8, 5\n    A3 = np.fromfunction(lambda i, j: 1 / (i + j + 2), (m3, n3), dtype=float) # Corrected Hilbert matrix indexing (1-based vs 0-based) to match problem description i+j+1\n\n    # Case 4: Already orthonormal columns\n    A4 = np.array([\n        [1, 0, 0],\n        [0, 0, 0],\n        [0, 1, 0],\n        [0, 0, 0],\n        [0, 0, 0],\n        [0, 0, 1]\n    ], dtype=float)\n\n    test_cases = [A1, A2, A3, A4]\n    all_results = []\n\n    for A in test_cases:\n        # Classical Gram-Schmidt\n        Q_cgs, R_cgs = classical_gram_schmidt(A)\n        fwd_cgs, orth_cgs = compute_errors(A, Q_cgs, R_cgs)\n\n        # Modified Gram-Schmidt\n        Q_mgs, R_mgs = modified_gram_schmidt(A)\n        fwd_mgs, orth_mgs = compute_errors(A, Q_mgs, R_mgs)\n\n        case_results = [\n            round(fwd_cgs, 12),\n            round(fwd_mgs, 12),\n            round(orth_cgs, 12),\n            round(orth_mgs, 12)\n        ]\n        all_results.append(case_results)\n\n    # Format the output as a string representation of a list of lists,\n    # without extra spaces, for consistency.\n    inner_lists_str = [f\"[{r[0]},{r[1]},{r[2]},{r[3]}]\" for r in all_results]\n    print(f\"[{','.join(inner_lists_str)}]\")\n\nsolve()\n```", "id": "3232097"}]}