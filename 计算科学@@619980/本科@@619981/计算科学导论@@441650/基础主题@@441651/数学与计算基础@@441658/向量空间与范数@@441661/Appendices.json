{"hands_on_practices": [{"introduction": "在向量空间的研究中，我们通常首先接触的是 $\\mathbb{R}^n$ 空间。然而，向量的概念远不止于此，函数本身也可以构成向量空间。这个练习将带你进入函数空间的世界，让你亲手计算两个函数之间的 $L_1$ “距离”，即它们图像之间围成的面积，从而体会范数在更广阔领域中的应用。[@problem_id:2308541]", "problem": "在泛函分析领域，我们可以用多种方式来衡量函数之间的“距离”。考虑向量空间 $C[0,1]$，它由定义在闭区间 $[0, 1]$ 上的所有连续实值函数组成。\n\n定义距离的一种常用方法是使用 $L_1$ 范数。对于 $C[0,1]$ 中的任何函数 $f(x)$，其 $L_1$ 范数（记作 $\\|f\\|_1$）定义为：\n$$\n\\|f\\|_1 = \\int_{0}^{1} |f(x)| dx\n$$\n在此空间中，两个函数 $p(x)$ 和 $q(x)$ 之间的距离由它们差的范数给出，即 $d(p, q) = \\|p - q\\|_1$。\n\n计算在 $C[0,1]$ 空间中，关于 $L_1$ 范数，函数 $p(x) = x$ 与常数函数 $q(x) = \\frac{1}{2}$ 之间的精确距离。将您的答案表示为最简分数。", "solution": "给定在 $[0,1]$ 上的函数 $p(x)=x$ 和 $q(x)=\\frac{1}{2}$。$L_{1}$ 距离为\n$$\nd(p,q)=\\|p-q\\|_{1}=\\int_{0}^{1}|p(x)-q(x)|\\,dx=\\int_{0}^{1}\\left|x-\\frac{1}{2}\\right|\\,dx.\n$$\n绝对值在 $x=\\frac{1}{2}$ 处分开，对于 $x\\in[0,\\frac{1}{2}]$ 有 $\\left|x-\\frac{1}{2}\\right|=\\frac{1}{2}-x$，对于 $x\\in[\\frac{1}{2},1]$ 有 $\\left|x-\\frac{1}{2}\\right|=x-\\frac{1}{2}$。因此，\n$$\n\\int_{0}^{1}\\left|x-\\frac{1}{2}\\right|\\,dx=\\int_{0}^{\\frac{1}{2}}\\left(\\frac{1}{2}-x\\right)\\,dx+\\int_{\\frac{1}{2}}^{1}\\left(x-\\frac{1}{2}\\right)\\,dx.\n$$\n分别计算每个积分。对于第一个积分，\n$$\n\\int_{0}^{\\frac{1}{2}}\\left(\\frac{1}{2}-x\\right)\\,dx=\\left[\\frac{1}{2}x-\\frac{x^{2}}{2}\\right]_{0}^{\\frac{1}{2}}=\\left(\\frac{1}{2}\\cdot\\frac{1}{2}-\\frac{\\left(\\frac{1}{2}\\right)^{2}}{2}\\right)-0=\\frac{1}{4}-\\frac{1}{8}=\\frac{1}{8}.\n$$\n对于第二个积分，\n$$\n\\int_{\\frac{1}{2}}^{1}\\left(x-\\frac{1}{2}\\right)\\,dx=\\left[\\frac{x^{2}}{2}-\\frac{1}{2}x\\right]_{\\frac{1}{2}}^{1}=\\left(\\frac{1}{2}-\\frac{1}{2}\\right)-\\left(\\frac{\\left(\\frac{1}{2}\\right)^{2}}{2}-\\frac{1}{2}\\cdot\\frac{1}{2}\\right)=0-\\left(\\frac{1}{8}-\\frac{1}{4}\\right)=\\frac{1}{8}.\n$$\n将它们相加得到\n$$\n\\int_{0}^{1}\\left|x-\\frac{1}{2}\\right|\\,dx=\\frac{1}{8}+\\frac{1}{8}=\\frac{1}{4}.\n$$\n因此，精确的 $L_{1}$ 距离是 $\\frac{1}{4}$。", "answer": "$$\\boxed{\\frac{1}{4}}$$", "id": "2308541"}, {"introduction": "从向量范数到矩阵范数，是理解线性变换“尺度”的关键一步。矩阵范数衡量了一个矩阵在对向量进行变换时，最大可能产生的“拉伸”程度。这个练习将指导你计算一个具体矩阵的 $L_1$ 诱导范数，让你掌握从抽象定义到简明计算方法的转换，为后续的算法稳定性分析打下基础。[@problem_id:2308606]", "problem": "考虑向量空间 $\\mathbb{R}^2$。$L_1$-范数，也称为出租车范数，对于该空间中的一个向量 $x = (x_1, x_2)$ 定义为 $\\|x\\|_1 = |x_1| + |x_2|$。\n一个从 $\\mathbb{R}^2$ 到自身的线性变换可以由一个 $2 \\times 2$ 矩阵表示。由向量范数 $\\|\\cdot\\|_1$ 诱导的矩阵 $A$ 的算子范数定义为\n$$ \\|A\\|_1 = \\sup_{x \\in \\mathbb{R}^2, x \\neq 0} \\frac{\\|Ax\\|_1}{\\|x\\|_1} $$\n这等价于在所有满足 $\\|x\\|_1=1$ 的向量 $x$ 上求 $\\|Ax\\|_1$ 的最大值。\n\n给定矩阵 $A = \\begin{pmatrix} 1  -4 \\\\ 2  -1 \\end{pmatrix}$，计算其诱导范数 $\\|A\\|_1$ 的值。", "solution": "我们使用由向量 1-范数诱导的算子范数。对于一个矩阵 $A = (a_{ij}) \\in \\mathbb{R}^{2 \\times 2}$ 和任意 $x \\in \\mathbb{R}^{2}$，我们有\n$$\n\\|Ax\\|_{1} = \\sum_{i=1}^{2} \\left| \\sum_{j=1}^{2} a_{ij} x_{j} \\right| \\leq \\sum_{i=1}^{2} \\sum_{j=1}^{2} |a_{ij}|\\,|x_{j}| = \\sum_{j=1}^{2} \\left( \\sum_{i=1}^{2} |a_{ij}| \\right) |x_{j}|.\n$$\n令 $c_{j} := \\sum_{i=1}^{2} |a_{ij}|$ 表示绝对列和。则\n$$\n\\|Ax\\|_{1} \\leq \\left( \\max_{j} c_{j} \\right) \\sum_{j=1}^{2} |x_{j}| = \\left( \\max_{j} c_{j} \\right) \\|x\\|_{1}.\n$$\n对所有 $x \\neq 0$ 取上确界，并使用诱导范数的定义，可得\n$$\n\\|A\\|_{1} \\leq \\max_{j} c_{j}.\n$$\n为证明等式成立，选择 $x = e_{k}$，其中 $k$ 是使 $\\max_{j} c_{j}$ 取得最大值的下标，$e_{k}$ 是第 $k$ 个标准基向量。那么 $Ax$ 等于 $A$ 的第 $k$ 列，所以\n$$\n\\|Ax\\|_{1} = c_{k} = \\max_{j} c_{j}, \\quad \\|x\\|_{1} = 1,\n$$\n这意味着\n$$\n\\|A\\|_{1} = \\max_{j} c_{j}.\n$$\n对于给定的矩阵 $A = \\begin{pmatrix} 1  -4 \\\\ 2  -1 \\end{pmatrix}$，其绝对列和为\n$$\nc_{1} = |1| + |2| = 3, \\qquad c_{2} = |-4| + |-1| = 5.\n$$\n因此，\n$$\n\\|A\\|_{1} = \\max\\{3, 5\\} = 5.\n$$", "answer": "$$\\boxed{5}$$", "id": "2308606"}, {"introduction": "在理论学习中，我们了解了多种不同的范数，如 $L_1$ 范数和 $L_2$ 范数。但在实际应用中，选择不同的范数会带来什么影响？本练习通过一个具体的 k-最近邻（KNN）分类任务，让你通过编程直观地看到，范数的选择如何改变“距离”的度量，进而影响最近邻居的集合，甚至最终改变算法的预测结果。[@problem_id:3201753]", "problem": "您将在有限维实数向量空间 $\\mathbb{R}^d$ 中工作，并比较由两种范数（$L^1$ 范数和 $L^2$ 范数）诱导的度量的行为。$L^1$ 范数定义为 $\\|\\mathbf{x}\\|_1 = \\sum_{i=1}^d |x_i|$，$L^2$ 范数定义为 $\\|\\mathbf{x}\\|_2 = \\left(\\sum_{i=1}^d x_i^2\\right)^{1/2}$。对于任何范数 $\\|\\cdot\\|$，其在 $\\mathbb{R}^d$ 上诱导的度量为 $d(\\mathbf{x},\\mathbf{y}) = \\|\\mathbf{x}-\\mathbf{y}\\|$。\n\n按如下方式构造一个由整数和实数 $d \\in \\mathbb{N}$、$a \\in \\mathbb{R}$、$b \\in \\mathbb{R}$ 和 $k \\in \\mathbb{N}$ 参数化的数据集：\n- 查询点是原点 $\\mathbf{0} \\in \\mathbb{R}^d$。\n- 定义坐标轴对齐点 $\\mathcal{A} = \\{a \\mathbf{e}_i : i \\in \\{1,2,\\dots,d\\}\\}$，其中 $\\mathbf{e}_i$ 是 $\\mathbb{R}^d$ 中的第 $i$ 个标准基向量。将 $\\mathcal{A}$ 中的每个点分配类别标签 $0$。\n- 定义一个对角点 $\\mathbf{u} = b(\\mathbf{e}_1+\\mathbf{e}_2)$，并为其分配类别标签 $1$。\n- 按顺序对点进行索引：首先是坐标轴点，索引为 $1,2,\\dots,d$，然后是对角点，索引为 $d+1$。\n\n设比较的度量为 $d_1(\\mathbf{x},\\mathbf{y}) = \\|\\mathbf{x}-\\mathbf{y}\\|_1$ 和 $d_2(\\mathbf{x},\\mathbf{y}) = \\|\\mathbf{x}-\\mathbf{y}\\|_2$。在距离相等的情况下，通过选择索引最小的点来打破平局。对于下游算法，使用应用于查询点 $\\mathbf{0}$ 的 $k$-最近邻 (KNN) 多数投票分类器，其中类别标签是 $k$ 个最近邻标签中的多数；如果多数投票出现平局，则选择较小的整数标签。\n\n您的程序必须：\n- 计算 $\\mathbf{0}$ 在 $d_1$ 和 $d_2$ 下的最近邻，并输出它们是否不同，结果为一个整数（如果不同则为 $1$，如果相同则为 $0$）。\n- 计算 $\\mathbf{0}$ 在 $d_1$ 和 $d_2$ 下的 $k$ 个最近邻的索引集合，并输出它们对称差的大小，结果为一个整数。\n- 计算查询点 $\\mathbf{0}$ 在 $d_1$ 和 $d_2$ 下的 KNN 多数投票类别标签，并输出分类结果是否改变，结果为一个整数（如果不同则为 $1$，如果相同则为 $0$）。\n\n使用以下参数值测试套件来检验不同的行为：\n- 测试用例 $1$：$d=2$，$a=\\frac{3}{2}$，$b=1$，$k=3$。\n- 测试用例 $2$：$d=2$，$a=\\sqrt{2}$，$b=1$，$k=2$。\n- 测试用例 $3$：$d=3$，$a=1$，$b=\\frac{1}{5}$，$k=2$。\n- 测试用例 $4$：$d=3$，$a=3$，$b=2$，$k=1$。\n\n您的程序应生成单行输出，其中包含每个测试用例结果的逗号分隔列表，其中每个测试用例结果本身是一个格式为 $[x_1,x_2,x_3]$ 的三元组，不含空格，顺序与上述测试套件相同。因此，最终输出必须是单个列表的列表，例如 $[[x_{1,1},x_{1,2},x_{1,3}],[x_{2,1},x_{2,2},x_{2,3}],\\dots]$。本问题不涉及物理单位或角度单位，并且所有返回值必须是整数。", "solution": "用户提供的问题已经过严格验证，是自洽、科学合理且定义明确的。所有必需的数据、定义和条件都已明确说明，没有矛盾、歧义或违反科学或数学原则之处。该问题是计算科学中的一个标准练习，旨在探索不同范数对向量空间几何形状的影响及其对基于距离的算法（如 $k$-最近邻 (KNN)）的意义。我现在将提供一个完整的解决方案。\n\n问题的核心在于使用两种不同的度量——由 $L^1$ 范数诱导的 $d_1$ 和由 $L^2$ 范数诱导的 $d_2$——来比较从一个查询点到一组数据点的距离排名。查询点是原点 $\\mathbf{q} = \\mathbf{0} \\in \\mathbb{R}^d$。从原点到任意点 $\\mathbf{p}$ 的距离简化为其范数，即 $d(\\mathbf{0}, \\mathbf{p}) = \\|\\mathbf{p}\\|$。\n\n数据集由两种类型的点组成：\n1.  一组 $d$ 个坐标轴对齐的点，$\\mathcal{A} = \\{ \\mathbf{p}_i = a \\mathbf{e}_i : i \\in \\{1, \\dots, d\\} \\}$。\n2.  一个对角点，$\\mathbf{u} = \\mathbf{p}_{d+1} = b(\\mathbf{e}_1 + \\mathbf{e}_2)$。\n\n让我们计算这些点在两种范数下到原点 $\\mathbf{0}$ 的距离。\n\n对于任何坐标轴对齐的点 $\\mathbf{p}_i = a \\mathbf{e}_i$：\n$L^1$ 距离是 $d_1(\\mathbf{0}, \\mathbf{p}_i) = \\|a \\mathbf{e}_i\\|_1 = \\sum_{j=1}^d |(a \\mathbf{e}_i)_j| = |a|$。\n$L^2$ 距离是 $d_2(\\mathbf{0}, \\mathbf{p}_i) = \\|a \\mathbf{e}_i\\|_2 = \\left(\\sum_{j=1}^d (a \\mathbf{e}_i)_j^2\\right)^{1/2} = \\sqrt{a^2} = |a|$。\n对于所有坐标轴对齐的点，到原点的 $L^1$ 和 $L^2$ 距离是相同的。\n\n对于对角点 $\\mathbf{u} = b(\\mathbf{e}_1 + \\mathbf{e}_2) = (b, b, 0, \\dots, 0)$：\n$L^1$ 距离是 $d_1(\\mathbf{0}, \\mathbf{u}) = \\|\\mathbf{u}\\|_1 = |b| + |b| = 2|b|$。\n$L^2$ 距离是 $d_2(\\mathbf{0}, \\mathbf{u}) = \\|\\mathbf{u}\\|_2 = \\sqrt{b^2 + b^2} = \\sqrt{2b^2} = \\sqrt{2}|b|$。\n\n邻居的相对顺序取决于 $L^1$ 度量下 $|a|$ 和 $2|b|$ 之间的比较，以及 $L^2$ 度量下 $|a|$ 和 $\\sqrt{2}|b|$ 之间的比较。这种差异是问题中观察到的所有变化来源。我们现在分析每个测试用例。\n\n**测试用例 1：$d=2$，$a=3/2$，$b=1$，$k=3$**\n点是 $\\mathbf{p}_1=(3/2, 0)$（索引 $1$，标签 $0$），$\\mathbf{p}_2=(0, 3/2)$（索引 $2$，标签 $0$），以及 $\\mathbf{p}_3=\\mathbf{u}=(1, 1)$（索引 $3$，标签 $1$）。\n到原点的距离是：\n- 对于 $\\mathbf{p}_1, \\mathbf{p}_2$：$d_1 = d_2 = |a| = |3/2| = 1.5$。\n- 对于 $\\mathbf{p}_3=\\mathbf{u}$：$d_1 = 2|b| = 2|1| = 2$ 且 $d_2 = \\sqrt{2}|b| = \\sqrt{2}|1| \\approx 1.414$。\n\n邻居的排序（从近到远，平局时使用最小索引）：\n- 在 $d_1$ 下：距离为 $1.5$（对于索引 $1,2$）和 $2$（对于索引 $3$）。索引的排序列表：$[1, 2, 3]$。\n- 在 $d_2$ 下：距离为 $1.5$（对于索引 $1,2$）和 $\\sqrt{2}$（对于索引 $3$）。由于 $\\sqrt{2}  1.5$，顺序不同。索引的排序列表：$[3, 1, 2]$。\n\n1.  **最近邻比较 ($x_1$)**：在 $d_1$ 下的最近邻是 $\\mathbf{p}_1$（索引 $1$）。在 $d_2$ 下的最近邻是 $\\mathbf{p}_3$（索引 $3$）。它们不同。因此，$x_1=1$。\n2.  **对称差 ($x_2$)**：对于 $k=3$，在 $d_1$ 下的最近邻集合是 $N_1=\\{1, 2, 3\\}$。在 $d_2$ 下，它是 $N_2=\\{3, 1, 2\\}$。这些集合是相同的 ($N_1=N_2$)。它们的对称差的大小为 $0$。因此，$x_2=0$。\n3.  **KNN 分类 ($x_3$)**：对于 $k=3$，我们使用邻居 $\\{1, 2, 3\\}$。标签是 $\\{0, 0, 1\\}$。多数投票结果是标签 $0$。这对两种度量都成立。分类没有改变。因此，$x_3=0$。\n测试用例 1 的结果：$[1, 0, 0]$。\n\n**测试用例 2：$d=2$，$a=\\sqrt{2}$，$b=1$，$k=2$**\n点是 $\\mathbf{p}_1=(\\sqrt{2}, 0)$（索引 $1$，标签 $0$），$\\mathbf{p}_2=(0, \\sqrt{2})$（索引 $2$，标签 $0$），以及 $\\mathbf{p}_3=\\mathbf{u}=(1, 1)$（索引 $3$，标签 $1$）。\n到原点的距离是：\n- 对于 $\\mathbf{p}_1, \\mathbf{p}_2$：$d_1 = d_2 = |a| = |\\sqrt{2}| = \\sqrt{2}$。\n- 对于 $\\mathbf{p}_3=\\mathbf{u}$：$d_1 = 2|b| = 2|1| = 2$ 且 $d_2 = \\sqrt{2}|b| = \\sqrt{2}|1| = \\sqrt{2}$。\n\n邻居的排序：\n- 在 $d_1$ 下：距离为 $\\sqrt{2}$（对于索引 $1,2$）和 $2$（对于索引 $3$）。索引的排序列表：$[1, 2, 3]$。\n- 在 $d_2$ 下：所有三个点到原点的距离相等，均为 $\\sqrt{2}$。我们通过索引来打破平局。索引的排序列表：$[1, 2, 3]$。\n\n1.  **最近邻比较 ($x_1$)**：两种度量下的最近邻都是 $\\mathbf{p}_1$（索引 $1$）。它们是相同的。因此，$x_1=0$。\n2.  **对称差 ($x_2$)**：对于 $k=2$，在 $d_1$ 下的最近邻集合是 $N_1=\\{1, 2\\}$。在 $d_2$ 下，它是 $N_2=\\{1, 2\\}$。这些集合是相同的。它们的对称差的大小为 $0$。因此，$x_2=0$。\n3.  **KNN 分类 ($x_3$)**：对于 $k=2$，我们使用邻居 $\\{1, 2\\}$。标签是 $\\{0, 0\\}$。两种度量下的多数投票结果都是标签 $0$。分类没有改变。因此，$x_3=0$。\n测试用例 2 的结果：$[0, 0, 0]$。\n\n**测试用例 3：$d=3$，$a=1$，$b=1/5$，$k=2$**\n点是 $\\mathbf{p}_1=(1,0,0)$、$\\mathbf{p}_2=(0,1,0)$、$\\mathbf{p}_3=(0,0,1)$（索引 $1,2,3$，标签 $0$）以及 $\\mathbf{p}_4=\\mathbf{u}=(1/5, 1/5, 0)$（索引 $4$，标签 $1$）。\n到原点的距离是：\n- 对于 $\\mathbf{p}_1, \\mathbf{p}_2, \\mathbf{p}_3$：$d_1 = d_2 = |a| = |1| = 1$。\n- 对于 $\\mathbf{p}_4=\\mathbf{u}$：$d_1 = 2|b| = 2|1/5| = 0.4$ 且 $d_2 = \\sqrt{2}|b| = \\sqrt{2}|1/5| \\approx 0.283$。\n\n邻居的排序：\n- 在 $d_1$ 下：距离为 $1$（对于索引 $1,2,3$）和 $0.4$（对于索引 $4$）。索引的排序列表：$[4, 1, 2, 3]$。\n- 在 $d_2$ 下：距离为 $1$（对于索引 $1,2,3$）和 $\\sqrt{2}/5$（对于索引 $4$）。由于 $\\sqrt{2}/5  1$，顺序相同。索引的排序列表：$[4, 1, 2, 3]$。\n\n1.  **最近邻比较 ($x_1$)**：两种度量下的最近邻都是 $\\mathbf{p}_4$（索引 $4$）。它们是相同的。因此，$x_1=0$。\n2.  **对称差 ($x_2$)**：对于 $k=2$，两种度量下的最近邻集合都是 $N_1=N_2=\\{4, 1\\}$。对称差为空。因此，$x_2=0$。\n3.  **KNN 分类 ($x_3$)**：对于 $k=2$，我们使用邻居 $\\{4, 1\\}$。标签是 $\\{1, 0\\}$。在多数投票中出现平局。平局打破规则指定选择较小的整数标签，所以类别是 $0$。这对两种度量都成立。分类没有改变。因此，$x_3=0$。\n测试用例 3 的结果：$[0, 0, 0]$。\n\n**测试用例 4：$d=3$，$a=3$，$b=2$，$k=1$**\n点是 $\\mathbf{p}_1=(3,0,0)$、$\\mathbf{p}_2=(0,3,0)$、$\\mathbf{p}_3=(0,0,3)$（索引 $1,2,3$，标签 $0$）以及 $\\mathbf{p}_4=\\mathbf{u}=(2, 2, 0)$（索引 $4$，标签 $1$）。\n到原点的距离是：\n- 对于 $\\mathbf{p}_1, \\mathbf{p}_2, \\mathbf{p}_3$：$d_1 = d_2 = |a| = |3| = 3$。\n- 对于 $\\mathbf{p}_4=\\mathbf{u}$：$d_1 = 2|b| = 2|2| = 4$ 且 $d_2 = \\sqrt{2}|b| = \\sqrt{2}|2| = 2\\sqrt{2} \\approx 2.828$。\n\n邻居的排序：\n- 在 $d_1$ 下：距离为 $3$（对于索引 $1,2,3$）和 $4$（对于索引 $4$）。索引的排序列表：$[1, 2, 3, 4]$。\n- 在 $d_2$ 下：距离为 $3$（对于索引 $1,2,3$）和 $2\\sqrt{2}$（对于索引 $4$）。由于 $2\\sqrt{2}  3$，顺序改变。索引的排序列表：$[4, 1, 2, 3]$。\n\n1.  **最近邻比较 ($x_1$)**：在 $d_1$ 下的最近邻是 $\\mathbf{p}_1$（索引 $1$）。在 $d_2$ 下，它是 $\\mathbf{p}_4$（索引 $4$）。它们不同。因此，$x_1=1$。\n2.  **对称差 ($x_2$)**：对于 $k=1$，在 $d_1$ 下的最近邻集合是 $N_1=\\{1\\}$。在 $d_2$ 下，它是 $N_2=\\{4\\}$。对称差是 $N_1 \\Delta N_2 = \\{1, 4\\}$，其大小为 $2$。因此，$x_2=2$。\n3.  **KNN 分类 ($x_3$)**：对于 $k=1$，分类由最近邻的标签决定。对于 $d_1$，邻居是 $\\mathbf{p}_1$（标签 $0$）。对于 $d_2$，邻居是 $\\mathbf{p}_4$（标签 $1$）。分类改变了。因此，$x_3=1$。\n测试用例 4 的结果：$[1, 2, 1]$。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n# from scipy import ...\n\ndef solve():\n    \"\"\"\n    Solves the KNN comparison problem for a given test suite.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (d, a, b, k)\n        (2, 3/2, 1, 3),\n        (2, np.sqrt(2), 1, 2),\n        (3, 1, 1/5, 2),\n        (3, 3, 2, 1),\n    ]\n\n    all_results = []\n\n    for d, a, b, k in test_cases:\n        # 1. Construct the dataset for the current test case.\n        points = []\n        point_labels = {}\n\n        # Axis-aligned points\n        for i in range(1, d + 1):\n            vector = np.zeros(d)\n            vector[i-1] = a\n            index = i\n            label = 0\n            points.append({'vector': vector, 'index': index, 'label': label})\n            point_labels[index] = label\n\n        # Diagonal point\n        vector_u = np.zeros(d)\n        vector_u[0] = b\n        vector_u[1] = b\n        index_u = d + 1\n        label_u = 1\n        points.append({'vector': vector_u, 'index': index_u, 'label': label_u})\n        point_labels[index_u] = label_u\n        \n        # 2. Calculate distances from the origin (query point)\n        query_point = np.zeros(d)\n        for p in points:\n            p['dist1'] = np.linalg.norm(p['vector'] - query_point, ord=1)\n            p['dist2'] = np.linalg.norm(p['vector'] - query_point, ord=2)\n\n        # 3. Sort points based on each metric, breaking ties by index\n        sorted_points_1 = sorted(points, key=lambda p: (p['dist1'], p['index']))\n        sorted_points_2 = sorted(points, key=lambda p: (p['dist2'], p['index']))\n\n        sorted_indices_1 = [p['index'] for p in sorted_points_1]\n        sorted_indices_2 = [p['index'] for p in sorted_points_2]\n\n        # 4. Compute the three required outputs\n        \n        # x1: Nearest neighbor comparison\n        nn_1 = sorted_indices_1[0]\n        nn_2 = sorted_indices_2[0]\n        x1 = 1 if nn_1 != nn_2 else 0\n\n        # x2: Symmetric difference of k-NN sets\n        k_neighbors_1 = set(sorted_indices_1[:k])\n        k_neighbors_2 = set(sorted_indices_2[:k])\n        x2 = len(k_neighbors_1.symmetric_difference(k_neighbors_2))\n\n        # x3: KNN classification comparison\n        def get_knn_label(neighbor_indices, labels_map):\n            \"\"\"\n            Performs majority vote classification with tie-breaking.\n            \"\"\"\n            neighbor_labels = [labels_map[i] for i in neighbor_indices]\n            count_0 = neighbor_labels.count(0)\n            count_1 = neighbor_labels.count(1)\n            \n            if count_1 > count_0:\n                return 1\n            # If count_0 > count_1 or if count_0 == count_1, return 0.\n            # The second case handles the tie-breaking rule (choose smaller label).\n            else:\n                return 0\n\n        knn_label_1 = get_knn_label(k_neighbors_1, point_labels)\n        knn_label_2 = get_knn_label(k_neighbors_2, point_labels)\n        x3 = 1 if knn_label_1 != knn_label_2 else 0\n\n        all_results.append([x1, x2, x3])\n\n    # Final print statement in the exact required format.\n    # e.g., [[x_1,x_2,x_3],[y_1,y_2,y_3]]\n    result_str = \",\".join([f\"[{','.join(map(str, res))}]\" for res in all_results])\n    print(f\"[{result_str}]\")\n\nsolve()\n```", "id": "3201753"}]}