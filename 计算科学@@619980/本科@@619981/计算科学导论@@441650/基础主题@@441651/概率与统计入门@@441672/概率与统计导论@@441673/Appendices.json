{"hands_on_practices": [{"introduction": "许多科学计算问题，从金融建模到分子动力学，都涉及高维空间中的积分。这个练习旨在通过一个具体的例子，揭示一个被称为“维度灾难”的核心挑战[@problem_id:3145824]。通过亲手比较确定性的网格积分方法（如梯形法则）与概率性的蒙特卡洛方法，你将深刻理解为何蒙特卡洛积分的收敛速度不依赖于维度，从而使其成为解决高维问题的首选工具。", "problem": "考虑使用两种方法估算一个可分函数在单位超立方体上的积分：蒙特卡罗 (MC) 采样和张量积网格复合梯形求积。设函数在 $[0,1]^d$ 上定义为 $f(\\mathbf{x}) = \\prod_{i=1}^{d} g(x_i)$，其中 $g(t) = \\sin(\\pi t)$，角度以弧度为单位。我们关注的积分是 $I_d = \\int_{[0,1]^d} f(\\mathbf{x}) \\, d\\mathbf{x}$。你的任务是实现一个程序，该程序针对一组给定的维度 $d$ 和容差 $\\varepsilon$，计算并比较蒙特卡罗方法和张量积网格求积法为保证均方根误差至多为 $\\varepsilon$ 所需的最小样本数。\n\n从以下基础出发：\n- 在 $[0,1]^d$ 上均匀分布的随机变量的期望和方差的定义。\n- 一维复合梯形法则的经典误差界：对于在 $[a,b]$ 上二次连续可微的函数，若将区间划分为 $m$ 个等长部分（因此步长为 $h = (b-a)/m$），则绝对误差满足 $\\left|\\int_{a}^{b} f(x)\\,dx - T_m\\right| \\le \\dfrac{(b-a)}{12} h^2 \\sup_{x \\in [a,b]} |f''(x)|$。对于 $[0,1]$ 区间，该界简化为 $\\left|\\int_{0}^{1} f(x)\\,dx - T_m\\right| \\le \\dfrac{1}{12 m^2} \\sup_{x \\in [0,1]} |f''(x)|$。\n\n利用这些基础和函数 $f$ 的可分结构：\n- 推导在 $[0,1]^d$ 上的均匀分布下 $f(\\mathbf{X})$ 的方差，其中 $\\mathbf{X}$ 在 $[0,1]^d$ 上均匀分布。利用此方差确定最小整数独立样本数 $n_{\\mathrm{MC}}$，使得蒙特卡罗估计量的标准误差至多为 $\\varepsilon$；也就是说，找到 $n_{\\mathrm{MC}}$ 使得样本均值方差的平方根至多为 $\\varepsilon$。\n- 对于张量积网格求积，在每个坐标上使用一维复合梯形法则，每个维度包含相同数量的 $m$ 个子区间，然后取张量积得到 $d$ 维求积。利用 $f$ 的可分性，根据 $d$、$m$、一维积分 $I_1 = \\int_{0}^{1} g(t)\\,dt$ 以及 $g$ 的一维梯形误差界，为 $d$ 维求积误差构建一个可计算的上界。根据此界，确定使得绝对误差至多为 $\\varepsilon$ 的最小整数 $m$，然后计算张量积网格法则使用的网格点总数 $n_{\\mathrm{TG}} = (m+1)^d$。\n\n你的程序应针对每个测试用例 $(d,\\varepsilon)$ 计算：\n- $n_{\\mathrm{MC}}$，蒙特卡罗方法达到标准误差至多为 $\\varepsilon$ 所需的最小整数样本数。\n- $n_{\\mathrm{TG}}$，张量积网格法达到绝对误差上界至多为 $\\varepsilon$ 所需的最小整数网格点数。\n- 一个布尔值，表示蒙特卡罗方法在样本效率上是否占优，定义为 $n_{\\mathrm{MC}}  n_{\\mathrm{TG}}$。\n\n测试套件：\n使用以下 $(d,\\varepsilon)$ 对集合来测试不同情况：\n- 情况 1：$d = 1$, $\\varepsilon = 10^{-3}$ (一维基准)。\n- 情况 2：$d = 5$, $\\varepsilon = 10^{-3}$ (中等维度，紧凑容差)。\n- 情况 3：$d = 10$, $\\varepsilon = 10^{-3}$ (更高维度，紧凑容差)。\n- 情况 4：$d = 20$, $\\varepsilon = 10^{-3}$ (高维度，紧凑容差)。\n- 情况 5：$d = 1$, $\\varepsilon = 10^{-2}$ (一维，宽松容差)。\n\n最终输出格式：\n你的程序应生成单行输出，其中包含一个由方括号括起来的逗号分隔列表。每个测试用例对应一个形式为 $[n_{\\mathrm{MC}},n_{\\mathrm{TG}},\\mathrm{MC\\_dominates}]$ 的内部列表。布尔值应打印为 $\\mathrm{True}$ 或 $\\mathrm{False}$。输出中不允许有任何空格。例如，输出应类似于 $[[\\text{case1}],[\\text{case2}],\\dots]$，且不含任何空格。", "solution": "该问题要求确定两种数值积分方法——蒙特卡罗 (MC) 采样和张量积复合梯形求积——为将积分 $I_d = \\int_{[0,1]^d} f(\\mathbf{x}) \\, d\\mathbf{x}$ 估算至指定容差 $\\varepsilon$ 所需的最小函数求值次数。被积函数是一个可分函数 $f(\\mathbf{x}) = \\prod_{i=1}^{d} g(x_i)$，其中 $g(t) = \\sin(\\pi t)$，定义在单位超立方体 $[0,1]^d$ 上。\n\n首先，我们验证问题陈述。\n**第 1 步：提取已知条件**\n- **函数**：$f(\\mathbf{x}) = \\prod_{i=1}^{d} g(x_i)$，定义域为 $[0,1]^d$。\n- **构成函数**：$g(t) = \\sin(\\pi t)$，角度以弧度为单位。\n- **积分**：$I_d = \\int_{[0,1]^d} f(\\mathbf{x}) \\, d\\mathbf{x}$。\n- **方法**：蒙特卡罗 (MC) 采样和张量积网格复合梯形求积。\n- **容差**：$\\varepsilon$。\n- **MC 条件**：MC 估计量的标准误差必须至多为 $\\varepsilon$。\n- **张量积网格 (TG) 条件**：TG 求积的绝对误差必须至多为 $\\varepsilon$。\n- **一维梯形误差界**：对于 $[0,1]$ 上的 $m$ 个子区间，$|\\int_{0}^{1} \\phi(x)\\,dx - T_m[\\phi]| \\le \\frac{1}{12 m^2} \\sup_{x \\in [0,1]} |\\phi''(x)|$。\n- **每个测试用例 $(d, \\varepsilon)$ 的输出**：\n    1.  $n_{\\mathrm{MC}}$：最小整数 MC 样本数。\n    2.  $n_{\\mathrm{TG}}$：最小整数 TG 网格点数。\n    3.  一个表示 $n_{\\mathrm{MC}}  n_{\\mathrm{TG}}$ 的布尔值。\n- **测试套件**：$(d,\\varepsilon)$ 对为 $(1, 10^{-3}), (5, 10^{-3}), (10, 10^{-3}), (20, 10^{-3}), (1, 10^{-2})$。\n\n**第 2 步：使用提取的已知条件进行验证**\n该问题在科学上基于数值分析和概率论的标准原理。函数 $g(t) = \\sin(\\pi t)$ 是良态且无限可微的。问题是适定的，因为它要求基于推导出的误差界计算最小整数样本数。定义和约束是自洽且数学上精确的，从而允许唯一解的存在。该设置不是矛盾的、不切实际的或不适定的。\n\n**第 3 步：结论与行动**\n问题被认为是**有效的**。我们继续推导必要的公式。\n\n### 蒙特卡罗方法分析\n\n基于从 $[0,1]^d$ 中均匀抽取的 $n$ 个独立样本 $\\mathbf{X}_j$ 的 $I_d$ 的蒙特卡罗估计量是样本均值 $\\hat{I}_{d,n} = \\frac{1}{n} \\sum_{j=1}^{n} f(\\mathbf{X}_j)$。该估计量是无偏的，即 $E[\\hat{I}_{d,n}] = I_d$。\n\n估计量的标准误差是其方差的平方根：\n$$\n\\mathrm{SE}(\\hat{I}_{d,n}) = \\sqrt{\\mathrm{Var}(\\hat{I}_{d,n})} = \\sqrt{\\mathrm{Var}\\left(\\frac{1}{n} \\sum_{j=1}^{n} f(\\mathbf{X}_j)\\right)} = \\frac{1}{\\sqrt{n}} \\sqrt{\\mathrm{Var}(f(\\mathbf{X}))}\n$$\n条件是 $\\mathrm{SE}(\\hat{I}_{d,n}) \\le \\varepsilon$，这意味着 $\\frac{\\sqrt{\\mathrm{Var}(f(\\mathbf{X}))}}{\\sqrt{n}} \\le \\varepsilon$，或者 $n \\ge \\frac{\\mathrm{Var}(f(\\mathbf{X}))}{\\varepsilon^2}$。\n最小整数样本数为 $n_{\\mathrm{MC}} = \\left\\lceil \\frac{\\mathrm{Var}(f(\\mathbf{X}))}{\\varepsilon^2} \\right\\rceil$。\n\n我们必须计算 $\\mathrm{Var}(f(\\mathbf{X})) = E[f(\\mathbf{X})^2] - (E[f(\\mathbf{X})])^2$。\n由于 $f$ 的可分性以及 $\\mathbf{X}$ 各分量的独立性，期望值是一维期望的乘积。设 $X \\sim U[0,1]$。\n\n1.  $g(X)$ 的期望：\n    $$\n    E[g(X)] = \\int_0^1 g(t)\\,dt = \\int_0^1 \\sin(\\pi t)\\,dt = \\left[-\\frac{1}{\\pi}\\cos(\\pi t)\\right]_0^1 = -\\frac{1}{\\pi}(\\cos(\\pi) - \\cos(0)) = \\frac{2}{\\pi}\n    $$\n    设此值为 $I_1 = \\frac{2}{\\pi}$。则 $I_d = E[f(\\mathbf{X})] = (I_1)^d = \\left(\\frac{2}{\\pi}\\right)^d$。\n\n2.  $g(X)^2$ 的期望：\n    $$\n    E[g(X)^2] = \\int_0^1 g(t)^2\\,dt = \\int_0^1 \\sin^2(\\pi t)\\,dt = \\int_0^1 \\frac{1 - \\cos(2\\pi t)}{2}\\,dt = \\frac{1}{2}\\left[t - \\frac{\\sin(2\\pi t)}{2\\pi}\\right]_0^1 = \\frac{1}{2}\n    $$\n    那么 $E[f(\\mathbf{X})^2] = \\prod_{i=1}^d E[g(X_i)^2] = \\left(\\frac{1}{2}\\right)^d$。\n\n$f(\\mathbf{X})$ 的方差为：\n$$\n\\mathrm{Var}(f(\\mathbf{X})) = \\left(\\frac{1}{2}\\right)^d - \\left(\\left(\\frac{2}{\\pi}\\right)^d\\right)^2 = \\left(\\frac{1}{2}\\right)^d - \\left(\\frac{2}{\\pi}\\right)^{2d}\n$$\n所以，蒙特卡罗方法的最小样本数为：\n$$\nn_{\\mathrm{MC}} = \\left\\lceil \\frac{(1/2)^d - (2/\\pi)^{2d}}{\\varepsilon^2} \\right\\rceil\n$$\n\n### 张量积求积分析\n\n每个维度有 $m$ 个子区间的 $d$ 维张量积梯形法则 $T_{m,d}$ 是 $d$ 个一维梯形法则 $T_m$ 的乘积。对于可分函数 $f(\\mathbf{x}) = \\prod_{i=1}^d g(x_i)$，其积分为 $T_{m,d}[f] = \\prod_{i=1}^d T_m[g]$。\n\n误差为 $I_d - T_{m,d}[f] = (I_1)^d - (T_m[g])^d$。我们可以使用恒等式 $A^d - B^d = (A-B)\\sum_{k=0}^{d-1} A^{d-1-k} B^k$ 来表示这个差。设 $E_m = I_1 - T_m[g]$。\n总误差为 $(I_1 - T_m[g]) \\sum_{k=0}^{d-1} (I_1)^{d-1-k} (T_m[g])^k = E_m \\sum_{k=0}^{d-1} (I_1)^{d-1-k} (T_m[g])^k$。为了界定这个误差，我们需要界定 $|T_m[g]|$。函数 $g(t)=\\sin(\\pi t)$ 在 $[0,1]$ 上是凹函数，因为对于 $t \\in [0,1]$，$g''(t) = -\\pi^2 \\sin(\\pi t) \\le 0$。对于凹函数，梯形法则会低估积分值，所以 $T_m[g] \\le I_1$。由于 $g(t) \\ge 0$，我们有 $0 \\le T_m[g] \\le I_1$。因此， $|T_m[g]| \\le I_1 = |I_1|$。\n\n绝对误差的界为：\n$$\n|I_d - T_{m,d}[f]| \\le |E_m| \\sum_{k=0}^{d-1} |I_1|^{d-1-k} |I_1|^k = |E_m| \\sum_{k=0}^{d-1} |I_1|^{d-1} = d |I_1|^{d-1} |E_m|\n$$\n现在我们使用给定的一维误差界 $|E_m| = |I_1 - T_m[g]| \\le \\frac{1}{12 m^2} \\sup_{t \\in [0,1]} |g''(t)|$。对于 $g(t) = \\sin(\\pi t)$，$g''(t) = -\\pi^2 \\sin(\\pi t)$。其上确界为 $\\sup_{t \\in [0,1]} |-\\pi^2 \\sin(\\pi t)| = \\pi^2$。所以， $|E_m| \\le \\frac{\\pi^2}{12m^2}$。\n\n总误差界变为：\n$$\n|I_d - T_{m,d}[f]| \\le d \\left(\\frac{2}{\\pi}\\right)^{d-1} \\frac{\\pi^2}{12m^2}\n$$\n我们要求此界至多为 $\\varepsilon$：\n$$\nd \\left(\\frac{2}{\\pi}\\right)^{d-1} \\frac{\\pi^2}{12m^2} \\le \\varepsilon \\implies m^2 \\ge \\frac{d \\pi^2}{12 \\varepsilon} \\left(\\frac{2}{\\pi}\\right)^{d-1}\n$$\n每个维度的最小整数子区间数 $m$ 是：\n$$\nm = \\left\\lceil \\sqrt{\\frac{d \\pi^2}{12 \\varepsilon} \\left(\\frac{2}{\\pi}\\right)^{d-1}} \\right\\rceil\n$$\n具有 $m$ 个子区间的复合梯形法则在每个维度上使用 $m+1$ 个点。对于 $d$ 维张量积网格，网格点总数为 $n_{\\mathrm{TG}} = (m+1)^d$。\n\n### 总结与计算\n对于每个测试用例 $(d, \\varepsilon)$，我们计算：\n1.  $n_{\\mathrm{MC}} = \\left\\lceil \\frac{(1/2)^d - (2/\\pi)^{2d}}{\\varepsilon^2} \\right\\rceil$\n2.  $m = \\left\\lceil \\sqrt{\\frac{d \\pi^2}{12 \\varepsilon} (2/\\pi)^{d-1}} \\right\\rceil$ 和 $n_{\\mathrm{TG}} = (m+1)^d$。\n3.  $n_{\\mathrm{MC}}  n_{\\mathrm{TG}}$ 的布尔值。\n\n这些计算将在最终的程序中实现。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes and compares the sample counts for Monte Carlo and tensor-grid\n    quadrature for a given set of test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (1, 10**-3),  # Case 1\n        (5, 10**-3),  # Case 2\n        (10, 10**-3), # Case 3\n        (20, 10**-3), # Case 4\n        (1, 10**-2),  # Case 5\n    ]\n\n    results = []\n    for d, epsilon in test_cases:\n        # --- Monte Carlo Calculation ---\n        # Variance of f(X), where X is uniform on [0,1]^d\n        # Var(f) = E[f^2] - (E[f])^2\n        # E[f] = (integral_0^1 sin(pi*t) dt)^d = (2/pi)^d\n        # E[f^2] = (integral_0^1 sin^2(pi*t) dt)^d = (1/2)^d\n        var_f = (0.5)**d - (2 / np.pi)**(2 * d)\n        \n        # n_MC = Var(f) / epsilon^2\n        # The number of samples must be an integer, so we take the ceiling.\n        n_mc = np.ceil(var_f / epsilon**2)\n        \n        # --- Tensor-Grid Quadrature Calculation ---\n        # Error_d = d * |I_1|^(d-1) * Error_1\n        # Error_1 = sup|g''| / (12 * m^2) = pi^2 / (12 * m^2)\n        # We need Error_d = epsilon\n        # d * (2/pi)^(d-1) * pi^2 / (12 * m^2) = epsilon\n        # m^2 = (d * pi^2 / (12 * epsilon)) * (2/pi)^(d-1)\n        \n        m_squared = (d * np.pi**2 / (12 * epsilon)) * (2 / np.pi)**(d - 1)\n        \n        # Minimal integer number of panels m\n        m = np.ceil(np.sqrt(m_squared))\n        \n        # Total number of grid points is (m+1)^d.\n        # Python's int handles arbitrary-precision integers, so overflow is not an issue.\n        n_tg = (int(m) + 1)**d\n        \n        # --- Comparison ---\n        mc_dominates = bool(n_mc  n_tg)\n        \n        # Append results as integers and a boolean.\n        # Use int() to convert from numpy float types.\n        results.append([int(n_mc), n_tg, mc_dominates])\n\n    # Final print statement in the exact required format.\n    # The format is [[case1_val1,case1_val2,...],[case2_val1,...],...] with no spaces.\n    # Using str() and then replace() is a robust way to achieve this.\n    print(str(results).replace(\" \", \"\"))\n\nsolve()\n```", "id": "3145824"}, {"introduction": "在认识到随机性在应对高维问题时的威力后，我们自然会问：它能否帮助我们解决复杂的优化问题？这个练习将引导你探索一种强大的算法技巧——随机化取整（randomized rounding）[@problem_id:3145804]。你将学习如何应用概率论中的一个核心工具——切诺夫界（Chernoff bound）——来分析算法的性能，从而确定需要多少次独立的随机化尝试，才能以极高的概率找到满足特定约束的优质解。这项实践将理论与应用相结合，展示了如何为随机化算法的可靠性提供严格的数学保证。", "problem": "您将设计并实现一个确定性程序，该程序计算一个随机舍入过程需要多少次独立重复，才能使一个线性约束系统以高概率在指定的乘法容差范围内成立。场景如下。给定一个二元矩阵 $A \\in \\{0,1\\}^{m \\times n}$ 和一个分数向量 $x \\in [0,1]^n$，该向量代表一个线性规划松弛的可行解。考虑随机舍入，其中对于 $i \\in \\{1,\\dots,n\\}$，独立地设置 $X_i \\in \\{0,1\\}$ 且 $\\mathbb{P}(X_i = 1) = x_i$。对于每个约束索引 $j \\in \\{1,\\dots,m\\}$，定义随机和 $S_j = \\sum_{i=1}^n A_{j,i} X_i$ 及其期望 $\\mu_j = \\mathbb{E}[S_j] = \\sum_{i=1}^n A_{j,i} x_i$。如果对于所有的 $j \\in \\{1,\\dots,m\\}$ 都有 $S_j \\le (1+\\varepsilon)\\,\\mu_j$，我们就称单次舍入试验是成功的。您不需要模拟 $X_i$；相反，您将通过解析方法来界定失败概率。\n\n从概率论的基本原理出发，特别是将马尔可夫不等式应用于独立伯努利随机变量之和的矩生成函数 (MGF)，推导出一个关于 $\\mathbb{P}(S_j  (1+\\varepsilon)\\,\\mu_j)$ 的有效的乘法切尔诺夫式上尾界。该界是关于 $\\mu_j$ 和 $\\varepsilon$ 的函数，适用于任何固定的 $j$ 和任何固定的 $\\varepsilon  0$。然后，对 $m$ 个约束使用联合界，获得一个关于每轮失败概率的上限 $q \\in [0,1]$，其中失败定义为至少有一个约束违反了 $(1+\\varepsilon)$ 的乘法容差。最后，利用重复、全新的随机舍入试验之间的独立性，确定最小整数 $t \\ge 1$，使得 $t$ 次试验中至少有一次成功的概率不小于 $1 - \\alpha$，其中 $\\alpha \\in (0,1)$ 是给定的目标失败概率。如果您的界太弱，无法为任何有限次数的试验保证成功（即，您得到的每轮失败界 $q$ 大于或等于 $1$），则对该测试用例报告特殊值 $-1$。\n\n您的程序必须确定性地实现这一流程，仅使用推导出的解析界和以下针对边界情况的规则：\n- 如果对于某行 $j$，$\\mu_j = 0$，那么在该模型下 $S_j = 0$ 几乎必然成立，因此将其单约束失败界设为 $0$。\n- 使用联合界合并单约束界，以获得每轮失败界 $q = \\min\\left(1, \\sum_{j=1}^m \\text{bound}_j\\right)$。\n- 如果 $q = 0$，返回 $t = 1$。\n- 如果 $q \\ge 1$，返回 $t = -1$。\n- 否则，返回能够保证目标可靠性水平 $1 - \\alpha$ 的最小整数 $t \\ge 1$，该计算仅使用各轮之间的独立性和您的每轮失败界。\n\n对于任何所需的对数计算，请使用自然对数。不要使用随机性。所有输出均为无单位的数字。\n\n需要实现和评估的测试套件：\n- 情况 A: $A = \\begin{bmatrix} 1  1  0  1  0 \\\\ 0  1  1  0  1 \\\\ 1  0  1  0  1 \\end{bmatrix}$, $x = [\\,0.4,\\,0.5,\\,0.3,\\,0.6,\\,0.2\\,]$, $\\varepsilon = 2.0$, $\\alpha = 0.0001$。\n- 情况 B: $A$ 是一个所有元素均为 $1$ 的 $2 \\times 10$ 矩阵，$x = [\\,0.5,\\,0.5,\\,0.5,\\,0.5,\\,0.5,\\,0.5,\\,0.5,\\,0.5,\\,0.5,\\,0.5\\,]$, $\\varepsilon = 2.5$, $\\alpha = 0.0003$。\n- 情况 C: $A = \\begin{bmatrix} 1  0  0 \\\\ 0  1  0 \\end{bmatrix}$, $x = [\\,0.3,\\,0.3,\\,0.0\\,]$, $\\varepsilon = 0.1$, $\\alpha = 0.01$。\n- 情况 D: $A = \\begin{bmatrix} 0  0  0  0 \\\\ 1  1  1  1 \\end{bmatrix}$, $x = [\\,0.0,\\,0.0,\\,0.0,\\,0.5\\,]$, $\\varepsilon = 1.0$, $\\alpha = 0.1$。\n- 情况 E: $A$ 是一个所有元素均为 $1$ 的 $3 \\times 8$ 矩阵，$x = [\\,0.6,\\,0.6,\\,0.6,\\,0.6,\\,0.6,\\,0.6,\\,0.6,\\,0.6\\,]$, $\\varepsilon = 1.5$, $\\alpha = 0.01$。\n\n最终输出格式：\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表（例如，$[r_1,r_2,r_3,r_4,r_5]$），其中 $r_k$ 是第 $k$ 个测试用例（按 A, B, C, D, E 的顺序）的整数结果，其值要么是最小试验次数 $t$，要么是在所述界下无法保证成功时的 $-1$。", "solution": "该问题要求确定一个随机舍入过程所需的最小独立试验次数 $t$，以确保一个线性约束系统以指定的高概率 $1-\\alpha$ 成立。这将通过以下步骤完成：首先为单个约束的违反推导一个切尔诺夫式概率界，然后使用联合界将所有约束的这些界组合起来，最后计算满足总体可靠性目标所需的试验次数。\n\n基于概率论的基本原理，该问题被验证为科学上合理、定义明确、客观且自洽。\n\n### 步骤 1：单约束失败概率界的推导\n\n令 $S_j = \\sum_{i=1}^n A_{j,i} X_i$ 为第 $j$ 个约束的随机和，其中 $X_i$ 是独立的伯努利随机变量，且 $\\mathbb{P}(X_i = 1) = x_i$。其期望为 $\\mu_j = \\mathbb{E}[S_j] = \\sum_{i=1}^n A_{j,i} x_i$。我们想要为给定的 $\\varepsilon  0$ 找到失败概率 $\\mathbb{P}(S_j  (1+\\varepsilon)\\mu_j)$ 的一个上界。\n\n我们从马尔可夫不等式开始，该不等式指出，对于任何非负随机变量 $Y$ 和任何常数 $a  0$，有 $\\mathbb{P}(Y \\ge a) \\le \\frac{\\mathbb{E}[Y]}{a}$。\n为了获得更紧的界，我们将此不等式应用于我们随机变量的指数。对于任何 $\\lambda  0$，事件 $S_j  (1+\\varepsilon)\\mu_j$ 等价于 $e^{\\lambda S_j}  e^{\\lambda(1+\\varepsilon)\\mu_j}$。应用马尔可夫不等式，其中 $Y = e^{\\lambda S_j}$ 且 $a = e^{\\lambda(1+\\varepsilon)\\mu_j}$：\n$$\n\\mathbb{P}(S_j  (1+\\varepsilon)\\mu_j) = \\mathbb{P}(e^{\\lambda S_j}  e^{\\lambda(1+\\varepsilon)\\mu_j}) \\le \\frac{\\mathbb{E}[e^{\\lambda S_j}]}{e^{\\lambda(1+\\varepsilon)\\mu_j}}\n$$\n项 $\\mathbb{E}[e^{\\lambda S_j}]$ 是 $S_j$ 的矩生成函数 (MGF)。由于 $S_j = \\sum_{i=1}^n A_{j,i} X_i$ 且 $X_i$ 是独立的，和的 MGF 是 MGF 的乘积：\n$$\n\\mathbb{E}[e^{\\lambda S_j}] = \\mathbb{E}\\left[\\exp\\left(\\lambda \\sum_{i=1}^n A_{j,i} X_i\\right)\\right] = \\mathbb{E}\\left[\\prod_{i=1}^n e^{\\lambda A_{j,i} X_i}\\right] = \\prod_{i=1}^n \\mathbb{E}[e^{\\lambda A_{j,i} X_i}]\n$$\n由于 $A_{j,i} \\in \\{0,1\\}$，如果 $A_{j,i}=0$，该项为 $\\mathbb{E}[e^0] = 1$。如果 $A_{j,i}=1$，该项为 $X_i$ 的 MGF：\n$$\n\\mathbb{E}[e^{\\lambda X_i}] = (1-x_i)e^{\\lambda \\cdot 0} + x_i e^{\\lambda \\cdot 1} = 1 - x_i + x_i e^\\lambda = 1 + x_i(e^\\lambda - 1)\n$$\n我们使用不等式 $1+z \\le e^z$（对于任何实数 $z$）。设 $z = x_i(e^\\lambda - 1)$，我们得到 $1 + x_i(e^\\lambda - 1) \\le e^{x_i(e^\\lambda-1)}$。\n将此代入 MGF 的乘积形式：\n$$\n\\mathbb{E}[e^{\\lambda S_j}] \\le \\prod_{i: A_{j,i}=1} e^{x_i(e^\\lambda - 1)} = \\exp\\left(\\sum_{i: A_{j,i}=1} x_i(e^\\lambda - 1)\\right) = \\exp\\left((e^\\lambda - 1)\\sum_{i: A_{j,i}=1} x_i\\right)\n$$\n根据定义，$\\mu_j = \\sum_{i=1}^n A_{j,i} x_i = \\sum_{i: A_{j,i}=1} x_i$。因此，我们有界：\n$$\n\\mathbb{E}[e^{\\lambda S_j}] \\le e^{\\mu_j(e^\\lambda - 1)}\n$$\n将此代回马尔可夫不等式表达式中：\n$$\n\\mathbb{P}(S_j  (1+\\varepsilon)\\mu_j) \\le \\frac{e^{\\mu_j(e^\\lambda - 1)}}{e^{\\lambda(1+\\varepsilon)\\mu_j}} = \\exp\\left(\\mu_j(e^\\lambda - 1) - \\lambda(1+\\varepsilon)\\mu_j\\right)\n$$\n这个界对任何 $\\lambda  0$ 都成立。为了获得最紧的界，我们对指数关于 $\\lambda$ 求最小值。令 $f(\\lambda) = \\mu_j(e^\\lambda - 1) - \\lambda(1+\\varepsilon)\\mu_j$。其导数为 $f'(\\lambda) = \\mu_j e^\\lambda - (1+\\varepsilon)\\mu_j$。令 $f'(\\lambda) = 0$ 得到 $e^\\lambda = 1+\\varepsilon$，即 $\\lambda = \\ln(1+\\varepsilon)$。由于 $\\varepsilon  0$，我们有 $\\lambda  0$，符合要求。\n\n将这个最优的 $\\lambda$ 代回指数中：\n$$\n\\mu_j(e^{\\ln(1+\\varepsilon)} - 1) - \\ln(1+\\varepsilon)(1+\\varepsilon)\\mu_j = \\mu_j((1+\\varepsilon) - 1) - (1+\\varepsilon)\\mu_j\\ln(1+\\varepsilon) = \\mu_j(\\varepsilon - (1+\\varepsilon)\\ln(1+\\varepsilon))\n$$\n因此，单个约束 $j$ 失败的切尔诺夫界是：\n$$\n\\mathbb{P}(S_j  (1+\\varepsilon)\\mu_j) \\le \\exp\\left(\\mu_j(\\varepsilon - (1+\\varepsilon)\\ln(1+\\varepsilon))\\right) = \\left[\\frac{e^\\varepsilon}{(1+\\varepsilon)^{1+\\varepsilon}}\\right]^{\\mu_j}\n$$\n我们用 $\\text{bound}_j$ 表示这个界。根据问题陈述，如果 $\\mu_j=0$，则 $S_j=0$ 几乎必然成立，因此其失败概率为 $0$。在这种情况下，我们设 $\\text{bound}_j = 0$。\n\n### 步骤 2：每轮失败概率\n\n如果至少有一个约束被违反，则单次舍入试验失败。令 $F_j$ 为事件 $S_j  (1+\\varepsilon)\\mu_j$。一次试验的总失败概率为 $\\mathbb{P}(\\cup_{j=1}^m F_j)$。使用联合界（布尔不等式）：\n$$\n\\mathbb{P}\\left(\\bigcup_{j=1}^m F_j\\right) \\le \\sum_{j=1}^m \\mathbb{P}(F_j) \\le \\sum_{j=1}^m \\text{bound}_j\n$$\n问题将每轮失败概率的上界 $q$ 定义为：\n$$\nq = \\min\\left(1, \\sum_{j=1}^m \\text{bound}_j\\right)\n$$\n\n### 步骤 3：最小试验次数\n\n我们需要找到最小的独立试验次数 $t \\ge 1$，使得至少有一次成功试验的概率至少为 $1-\\alpha$。设 $q$ 是我们对单次试验失败概率的上界。\n在 $t$ 次试验中至少有一次成功的概率是 $1 - \\mathbb{P}(\\text{所有 } t \\text{ 次试验都失败})$。\n各次试验是独立的，所以 $\\mathbb{P}(\\text{所有 } t \\text{ 次试验都失败}) = (\\mathbb{P}(\\text{一次试验失败}))^t$。由于 $\\mathbb{P}(\\text{一次试验失败}) \\le q$，我们有 $\\mathbb{P}(\\text{所有 } t \\text{ 次试验都失败}) \\le q^t$。\n我们要求 $1 - \\mathbb{P}(\\text{所有 } t \\text{ 次试验都失败}) \\ge 1-\\alpha$，这等价于 $\\mathbb{P}(\\text{所有 } t \\text{ 次试验都失败}) \\le \\alpha$。\n为满足此条件，我们必须找到最小的整数 $t \\ge 1$，使得 $q^t \\le \\alpha$。\n\n我们分析 $q$ 的各种情况：\n1.  如果 $q = 0$：根据我们的界，失败是不可能的。单次试验保证成功。因此，$t=1$。\n2.  如果 $q \\ge 1$：界太弱。对于任何 $t \\ge 1$，$q^t \\ge 1$。由于 $\\alpha \\in (0,1)$，条件 $q^t \\le \\alpha$ 永远无法满足。在这种情况下，无法保证成功，我们报告 $t=-1$。\n3.  如果 $0  q  1$：我们对 $q^t \\le \\alpha$ 的两边取自然对数：\n    $$\n    t \\ln(q) \\le \\ln(\\alpha)\n    $$\n    由于 $0  q  1$，$\\ln(q)$ 是负数。除以 $\\ln(q)$ 会反转不等号：\n    $$\n    t \\ge \\frac{\\ln(\\alpha)}{\\ln(q)}\n    $$\n    由于 $t$ 必须是整数，我们对右侧取上取整。满足此条件的最小整数 $t \\ge 1$ 是：\n    $$\n    t = \\left\\lceil \\frac{\\ln(\\alpha)}{\\ln(q)} \\right\\rceil\n    $$\n\n总体算法如下：\n1. 对于给定的测试用例（$A, x, \\varepsilon, \\alpha$），计算期望向量 $\\mu = Ax$。\n2. 对于每个约束 $j=1,\\dots,m$，计算切尔诺夫界：\n   - 如果 $\\mu_j=0$，则 $\\text{bound}_j = 0$。\n   - 否则，$\\text{bound}_j = \\exp(\\mu_j(\\varepsilon - (1+\\varepsilon)\\ln(1+\\varepsilon)))$。\n3. 计算每轮失败概率界 $q = \\min(1, \\sum_{j=1}^m \\text{bound}_j)$。\n4. 根据 $q$ 和 $\\alpha$ 使用上述推导的规则确定试验次数 $t$。\n\n这个过程是确定性的，仅依赖于提供的输入和推导出的解析界。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the minimal number of randomized rounding trials required\n    to satisfy a system of linear constraints with high probability.\n    \"\"\"\n    \n    test_cases = [\n        # Case A\n        (\n            np.array([\n                [1, 1, 0, 1, 0],\n                [0, 1, 1, 0, 1],\n                [1, 0, 1, 0, 1]\n            ]),\n            np.array([0.4, 0.5, 0.3, 0.6, 0.2]),\n            2.0,\n            0.0001\n        ),\n        # Case B\n        (\n            np.ones((2, 10)),\n            np.full(10, 0.5),\n            2.5,\n            0.0003\n        ),\n        # Case C\n        (\n            np.array([\n                [1, 0, 0],\n                [0, 1, 0]\n            ]),\n            np.array([0.3, 0.3, 0.0]),\n            0.1,\n            0.01\n        ),\n        # Case D\n        (\n            np.array([\n                [0, 0, 0, 0],\n                [1, 1, 1, 1]\n            ]),\n            np.array([0.0, 0.0, 0.0, 0.5]),\n            1.0,\n            0.1\n        ),\n        # Case E\n        (\n            np.ones((3, 8)),\n            np.full(8, 0.6),\n            1.5,\n            0.01\n        )\n    ]\n\n    results = []\n\n    for A, x, epsilon, alpha in test_cases:\n        # Step 1: Calculate expectations mu_j for each constraint\n        mu = A @ x\n        \n        # Step 2: Calculate the Chernoff bound for each constraint's failure probability\n        single_constraint_bounds = []\n        if epsilon  0:\n            # Pre-calculate the log of the base of the Chernoff bound for numerical stability\n            log_chernoff_base = epsilon - (1 + epsilon) * np.log(1 + epsilon)\n        \n        for mu_j in mu:\n            if mu_j == 0:\n                # If mu_j is 0, S_j is 0, so the constraint cannot be violated\n                bound_j = 0.0\n            else:\n                # The bound is (base)^mu_j, or exp(mu_j * log(base))\n                bound_j = np.exp(mu_j * log_chernoff_base)\n            single_constraint_bounds.append(bound_j)\n            \n        # Step 3: Use the union bound to get the per-round failure probability q\n        q = min(1.0, np.sum(single_constraint_bounds))\n        \n        # Step 4: Determine the minimal number of trials t\n        t = 0\n        if q == 0.0:\n            # If failure is impossible, 1 trial is sufficient\n            t = 1\n        elif q = 1.0:\n            # If the failure bound is = 1, the bound is too weak to certify success\n            t = -1\n        else: # 0  q  1\n            # We need q^t = alpha, which means t = log(alpha) / log(q)\n            # Since t must be an integer, we take the ceiling.\n            t = int(np.ceil(np.log(alpha) / np.log(q)))\n            \n        results.append(t)\n\n    # Format and print the final output\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3145804"}, {"introduction": "标准的蒙特卡洛方法在估计极端罕见事件的概率时效率极低，例如估算一个高可靠性系统的失效概率。这个实践将向你介绍一种专为此类问题设计的高级算法——交叉熵方法（Cross-Entropy method）[@problem_id:3145853]。你将亲手实现一个自适应的重要性采样过程，其中采样分布在每次迭代中都会根据信息论中的Kullback-Leibler (KL)散度最小化原则进行优化，从而将计算资源智能地集中到我们最感兴趣的罕见事件区域。通过这个练习，你将掌握一种能够显著提升模拟效率的强大计算统计技术。", "problem": "实现交叉熵方法，以调整单变量高斯建议分布来估计稀有事件概率，并完全基于概率论的第一性原理，追踪其 Kullback–Leibler 散度向最优重要性采样分布的演进过程。基础分布是标准正态分布，其密度为 $f(x) = (2\\pi)^{-1/2} \\exp(-x^2/2)$，参数化建议分布是正态族 $q_\\mu(x) = (2\\pi)^{-1/2} \\exp(-(x-\\mu)^2/2)$，其均值参数 $\\mu \\in \\mathbb{R}$ 未知，方差为单位方差。对于给定的阈值 $a  0$，稀有事件为 $A = \\{x \\in \\mathbb{R} : x \\ge a\\}$。稀有事件概率为 $p = \\mathbb{P}_f(X \\ge a)$，其中 $X \\sim \\mathcal{N}(0,1)$。用于估计 $p$ 的最优重要性分布是零方差密度 $p^\\star(x) = f(x)\\mathbf{1}\\{x \\in A\\}/p$。交叉熵方法通过最小化 Kullback–Leibler 散度 $\\mathrm{D}_{\\mathrm{KL}}(p^\\star \\Vert q_\\mu)$ 来选择参数，这等价于最大化交叉熵 $\\mathbb{E}_{p^\\star}[\\log q_\\mu(X)]$。从重要性采样、Kullback–Leibler 散度和正态族中的最大似然估计的核心定义出发，推导出 $\\mu$ 的迭代更新公式。该公式源于最大化 $\\mathbb{E}_{p^\\star}[\\log q_\\mu(X)]$，其中 $p^\\star$ 下的期望是通过使用当前建议分布 $q_{\\mu_k}$ 的自归一化重要性采样来近似的。证明对于 $q_{\\mu_k}$，样本 $x$ 的重要性权重等于 $w_k(x) = f(x) / q_{\\mu_k}(x) = \\exp(-\\mu_k x + \\mu_k^2/2)$，并且 $\\mu$ 的加权最大似然更新简化为限制在稀有事件上的加权一阶矩与零阶矩之比：\n$$\n\\tilde{\\mu}_{k+1} \\;=\\; \\frac{\\mathbb{E}_{q_{\\mu_k}}\\!\\left[w_k(X)\\,\\mathbf{1}\\{X \\in A\\}\\,X\\right]}{\\mathbb{E}_{q_{\\mu_k}}\\!\\left[w_k(X)\\,\\mathbf{1}\\{X \\in A\\}\\right]}.\n$$\n为增强稳定性，使用一个指数平滑步骤\n$$\n\\mu_{k+1} \\;=\\; (1-\\alpha)\\,\\mu_k \\;+\\; \\alpha\\,\\tilde{\\mu}_{k+1},\n$$\n其中平滑参数为 $\\alpha \\in (0,1]$。对于经过 $K$ 次迭代后的最终调整建议分布 $q_{\\mu_K}$，估计：\n- 通过重要性采样估计稀有事件概率：\n$$\n\\hat{p} \\;=\\; \\frac{1}{N}\\sum_{i=1}^N w_K(X_i)\\,\\mathbf{1}\\{X_i \\in A\\}, \\quad X_i \\overset{\\text{i.i.d.}}{\\sim} q_{\\mu_K},\n$$\n- 相对于初始建议分布 $q_{\\mu_0}$ 的相对 Kullback–Leibler 散度：\n$$\n\\Delta_K \\;=\\; \\mathrm{D}_{\\mathrm{KL}}(p^\\star \\Vert q_{\\mu_K}) \\;-\\; \\mathrm{D}_{\\mathrm{KL}}(p^\\star \\Vert q_{\\mu_0}) \\;=\\; -\\,\\mathbb{E}_{p^\\star}\\!\\left[\\log q_{\\mu_K}(X) - \\log q_{\\mu_0}(X)\\right],\n$$\n该值必须使用在 $q_{\\mu_K}$ 分布下的自归一化重要性采样来估计。对于单位方差正态族，证明 $\\log q_{\\mu}(x) = -\\tfrac{1}{2}\\log(2\\pi) - \\tfrac{1}{2}(x-\\mu)^2$，因此，假定 $\\mu_0=0$：\n$$\n\\log q_{\\mu_K}(x) - \\log q_{\\mu_0}(x) \\;=\\; \\mu_K x - \\tfrac{1}{2}\\mu_K^2,\n$$\n所以\n$$\n\\widehat{\\Delta}_K \\;=\\; -\\,\\frac{\\sum_{i=1}^N w_K(X_i)\\,\\mathbf{1}\\{X_i \\in A\\}\\,\\big(\\mu_K X_i - \\tfrac{1}{2}\\mu_K^2\\big)}{\\sum_{i=1}^N w_K(X_i)\\,\\mathbf{1}\\{X_i \\in A\\}}.\n$$\n算法要求：\n- 初始化 $\\mu_0 = 0$ 并固定一个随机数生成器种子以确保结果的可复现性。\n- 在每次迭代 $k \\in \\{0,\\dots,K-1\\}$ 中，从 $q_{\\mu_k}$ 中抽取 $N$ 个独立样本，计算权重 $w_k(x)$ 和在 $A$ 上的受限矩，通过使用参数 $\\alpha$ 的平滑方法更新 $\\mu_{k+1}$。如果加权更新中的分母在数值上为零，则在该次迭代中保持 $\\mu_{k+1} = \\mu_k$ 不变。\n- 经过 $K$ 次迭代后，从 $q_{\\mu_K}$ 中抽取 $N$ 个新的样本，以计算如上定义的 $\\hat{p}$ 和 $\\widehat{\\Delta}_K$。如果自归一化分母在数值上为零，则设置 $\\hat{p} = 0$ 和 $\\widehat{\\Delta}_K = 0$。\n测试套件：\n为以下参数集 $(a,N,K,\\alpha)$ 运行你的程序：\n- 情况1：$(a,N,K,\\alpha) = (\\,2.5,\\,20000,\\,5,\\,0.7\\,)$。\n- 情况2：$(a,N,K,\\alpha) = (\\,3.0,\\,30000,\\,7,\\,0.7\\,)$。\n- 情况3：$(a,N,K,\\alpha) = (\\,3.5,\\,60000,\\,9,\\,0.7\\,)$。\n使用单一固定种子 $s = 123456$ 来初始化你的随机数生成器，并且在不同情况之间不要改变该种子。\n最终输出格式：\n你的程序应产生单行输出，其中包含按顺序排列的三个情况的结果，形式为一个逗号分隔的列表。每个情况的结果是内部列表 $[\\hat{p},\\widehat{\\Delta}_K,\\mu_K]$，其中每个值都四舍五入到小数点后恰好六位数字，并且任何地方都不能有空格。例如：[[0.012345,-0.678901,1.234567],[...],[...]]。不应打印任何额外的文本。", "solution": "用户提供的问题是计算统计学中一个有效且良构的练习。它在科学上基于重要性采样和信息论的理论，所有必要的参数和算法步骤都得到了清晰的定义。问题描述中提供的推导在数学上是合理的，并且基于第一性原理。该问题是形式化、客观且自洽的，要求针对特定应用实现交叉熵方法。\n\n该问题要求实现交叉熵（CE）方法来估计一个稀有事件概率 $p = \\mathbb{P}_f(X \\ge a)$，其中 $X$ 是一个标准正态随机变量，其概率密度函数（PDF）为 $f(x) = \\mathcal{N}(x; 0, 1)$。CE方法是一种自适应重要性采样技术，它通过迭代方式寻找更好的建议分布，以使稀有事件更可能发生，从而降低蒙特卡洛估计量的方差。\n\n所选的建议分布族是单位方差的单变量正态族 $q_\\mu(x) = \\mathcal{N}(x; \\mu, 1)$，由其均值 $\\mu \\in \\mathbb{R}$ 参数化。CE方法的目标是找到参数 $\\mu$，使得建议密度 $q_\\mu(x)$ 与最优的零方差重要性采样密度 $p^\\star(x)$ “最接近”。最优密度定义为 $p^\\star(x) = f(x)\\mathbf{1}\\{x \\ge a\\} / p$，其中 $\\mathbf{1}\\{x \\ge a\\}$ 是稀有事件 $A=\\{x \\in \\mathbb{R} : x \\ge a\\}$ 的指示函数。然而，这个最优密度是未知的，因为它依赖于我们旨在估计的概率 $p$ 本身。\n\n两个分布之间的“接近度”由 Kullback–Leibler（KL）散度来衡量。CE 方法旨在找到最小化 $\\mathrm{D}_{\\mathrm{KL}}(p^\\star \\Vert q_\\mu)$ 的参数 $\\mu$：\n$$\n\\min_{\\mu} \\mathrm{D}_{\\mathrm{KL}}(p^\\star \\Vert q_\\mu) = \\min_{\\mu} \\int_{-\\infty}^{\\infty} p^\\star(x) \\log\\left(\\frac{p^\\star(x)}{q_\\mu(x)}\\right) dx\n$$\n这个最小化问题等价于最大化交叉熵项 $\\mathbb{E}_{p^\\star}[\\log q_\\mu(X)]$：\n$$\n\\max_{\\mu} \\int_{-\\infty}^{\\infty} p^\\star(x) \\log(q_\\mu(x)) dx = \\max_{\\mu} \\mathbb{E}_{p^\\star}[\\log q_\\mu(X)]\n$$\n建议分布的对数为 $\\log q_\\mu(x) = \\log\\left((2\\pi)^{-1/2} e^{-(x-\\mu)^2/2}\\right) = -\\frac{1}{2}\\log(2\\pi) - \\frac{1}{2}(x-\\mu)^2$。最大化该量关于 $\\mu$ 的期望等价于最大化 $\\mathbb{E}_{p^\\star}[-(X-\\mu)^2]$。对 $\\mu$ 求导并令其为零，可得：\n$$\n\\frac{\\partial}{\\partial \\mu} \\mathbb{E}_{p^\\star}[-X^2 + 2\\mu X - \\mu^2] = \\mathbb{E}_{p^\\star}[2X - 2\\mu] = 2\\mathbb{E}_{p^\\star}[X] - 2\\mu = 0\n$$\n这意味着最优参数为 $\\mu^\\star = \\mathbb{E}_{p^\\star}[X]$。由于 $p^\\star(x)$ 仅在稀有事件集 $A$ 上非零，因此这是在给定 $X \\in A$ 的条件下 $X$ 的条件期望。\n\n由于 $\\mu^\\star$ 未知，CE 方法采用迭代过程。在第 $k$ 次迭代中，期望 $\\mathbb{E}_{p^\\star}[X]$ 使用当前建议分布 $q_{\\mu_k}(x)$ 通过重要性采样来近似。样本 $x$ 的重要性采样权重是目标密度与建议密度之比，$w_k(x) = f(x)/q_{\\mu_k}(x)$。对于给定的正态分布，这可简化为：\n$$\nw_k(x) = \\frac{(2\\pi)^{-1/2} e^{-x^2/2}}{(2\\pi)^{-1/2} e^{-(x-\\mu_k)^2/2}} = e^{-x^2/2 + (x^2 - 2\\mu_k x + \\mu_k^2)/2} = e^{-\\mu_k x + \\mu_k^2/2}\n$$\n用于 $\\mu^\\star = \\mathbb{E}_{p^\\star}[X]$ 的自归一化重要性采样估计量被用来定义下一个参数估计值 $\\tilde{\\mu}_{k+1}$：\n$$\n\\tilde{\\mu}_{k+1} = \\frac{\\sum_{i=1}^N w_k(X_i)\\,\\mathbf{1}\\{X_i \\in A\\}\\,X_i}{\\sum_{i=1}^N w_k(X_i)\\,\\mathbf{1}\\{X_i \\in A\\}} \\approx \\frac{\\mathbb{E}_{q_{\\mu_k}}\\!\\left[w_k(X)\\,\\mathbf{1}\\{X \\in A\\}\\,X\\right]}{\\mathbb{E}_{q_{\\mu_k}}\\!\\left[w_k(X)\\,\\mathbf{1}\\{X \\in A\\}\\right]}\n$$\n其中样本 $X_i$ 从 $q_{\\mu_k}$ 中独立同分布地抽取。为防止参数估计值出现大的、可能不稳定的跳跃，应用了指数平滑步骤：\n$$\n\\mu_{k+1} = (1-\\alpha)\\mu_k + \\alpha\\tilde{\\mu}_{k+1}\n$$\n其中平滑参数为 $\\alpha \\in (0,1]$。\n\n经过 $K$ 次迭代后，最终的自适应分布 $q_{\\mu_K}$ 用于估计稀有事件概率 $p$ 和建议分布的改进程度。$p = \\mathbb{E}_f[\\mathbf{1}\\{X \\in A\\}] = \\mathbb{E}_{q_{\\mu_K}}[w_K(X)\\mathbf{1}\\{X \\in A\\}]$ 的重要性采样估计量为：\n$$\n\\hat{p} = \\frac{1}{N}\\sum_{i=1}^N w_K(X_i)\\,\\mathbf{1}\\{X_i \\in A\\}, \\quad X_i \\sim q_{\\mu_K}\n$$\n改进程度由相对 KL 散度 $\\Delta_K = \\mathrm{D}_{\\mathrm{KL}}(p^\\star \\Vert q_{\\mu_K}) - \\mathrm{D}_{\\mathrm{KL}}(p^\\star \\Vert q_{\\mu_0})$ 来量化。这可以简化为 $\\Delta_K = -\\mathbb{E}_{p^\\star}[\\log q_{\\mu_K}(X) - \\log q_{\\mu_0}(X)]$。使用初始参数 $\\mu_0 = 0$，我们有：\n$$\n\\log q_{\\mu_K}(x) - \\log q_{\\mu_0}(x) = \\left(-\\frac{1}{2}\\log(2\\pi) - \\frac{(x-\\mu_K)^2}{2}\\right) - \\left(-\\frac{1}{2}\\log(2\\pi) - \\frac{x^2}{2}\\right) = \\frac{1}{2}\\left(-x^2+2\\mu_K x - \\mu_K^2 + x^2\\right) = \\mu_K x - \\frac{1}{2}\\mu_K^2\n$$\n估计值 $\\widehat{\\Delta}_K$ 是通过将自归一化重要性采样器应用于期望 $\\mathbb{E}_{p^\\star}[\\mu_K X - \\frac{1}{2}\\mu_K^2]$（使用从 $q_{\\mu_K}$ 抽取的样本）而获得的：\n$$\n\\widehat{\\Delta}_K = -\\frac{\\sum_{i=1}^N w_K(X_i)\\,\\mathbf{1}\\{X_i \\in A\\}\\,\\left(\\mu_K X_i - \\frac{1}{2}\\mu_K^2\\right)}{\\sum_{i=1}^N w_K(X_i)\\,\\mathbf{1}\\{X_i \\in A\\}}\n$$\n算法的执行过程是：初始化 $\\mu_0=0$，迭代 $K$ 次以找到 $\\mu_K$，然后使用一组从 $q_{\\mu_K}$ 中抽取的新样本来计算最终估计值 $\\hat{p}$ 和 $\\widehat{\\Delta}_K$。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements the Cross-Entropy method to estimate a rare-event probability\n    and track the Kullback-Leibler divergence progression.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (2.5, 20000, 5, 0.7),\n        (3.0, 30000, 7, 0.7),\n        (3.5, 60000, 9, 0.7),\n    ]\n\n    # Initialize a single random number generator for reproducibility across all cases.\n    seed = 123456\n    rng = np.random.default_rng(seed)\n\n    results = []\n    for a, N, K, alpha in test_cases:\n        # --- Iterative adaptation phase ---\n        mu = 0.0  # Initialize mu_0 = 0\n\n        for _ in range(K):\n            # 1. Draw N samples from the current proposal distribution q_mu_k\n            samples = rng.normal(loc=mu, scale=1.0, size=N)\n\n            # 2. Identify samples in the rare event set A = {x = a}\n            indicator = samples = a\n            \n            # If no samples fall in the rare event set, the update cannot be computed.\n            if not np.any(indicator):\n                # Per problem, keep mu_k+1 = mu_k\n                continue\n\n            successful_samples = samples[indicator]\n\n            # 3. Compute importance weights for successful samples\n            # w_k(x) = exp(-mu_k * x + mu_k^2 / 2)\n            weights = np.exp(-mu * successful_samples + mu**2 / 2.0)\n\n            # 4. Compute the un-smoothed parameter update mu_tilde\n            numerator = np.sum(weights * successful_samples)\n            denominator = np.sum(weights)\n\n            # Handle case where denominator is numerically zero\n            if denominator  1e-100:\n                 # Per problem, keep mu_k+1 = mu_k\n                continue\n\n            mu_tilde = numerator / denominator\n\n            # 5. Apply exponential smoothing\n            mu = (1.0 - alpha) * mu + alpha * mu_tilde\n        \n        mu_K = mu\n\n        # --- Final estimation phase ---\n        # 1. Draw N fresh samples from the final proposal distribution q_mu_K\n        final_samples = rng.normal(loc=mu_K, scale=1.0, size=N)\n\n        # 2. Identify samples in the rare event set A\n        final_indicator = final_samples = a\n\n        # If no samples fall in the rare event set\n        if not np.any(final_indicator):\n            # Per problem, set p_hat and delta_K_hat to 0\n            p_hat = 0.0\n            delta_K_hat = 0.0\n        else:\n            final_successful_samples = final_samples[final_indicator]\n\n            # 3. Compute final weights\n            # w_K(x) = exp(-mu_K * x + mu_K^2 / 2)\n            final_weights = np.exp(-mu_K * final_successful_samples + mu_K**2 / 2.0)\n\n            # 4. Estimate the rare-event probability p_hat\n            # p_hat = (1/N) * sum(w_K(X_i) * I{X_i = a})\n            sum_of_weights = np.sum(final_weights)\n            p_hat = sum_of_weights / N\n\n            # 5. Estimate the relative KL divergence Delta_K\n            # This is the denominator for the self-normalized estimator of Delta_K\n            norm_denominator = sum_of_weights\n            \n            # Handle case where denominator is numerically zero\n            if norm_denominator  1e-100:\n                # Per problem, p_hat is also zero in this case\n                p_hat = 0.0\n                delta_K_hat = 0.0\n            else:\n                log_q_diff_term = mu_K * final_successful_samples - 0.5 * mu_K**2\n                norm_numerator = np.sum(final_weights * log_q_diff_term)\n                delta_K_hat = -norm_numerator / norm_denominator\n\n        # Store results for this case as a pre-formatted string\n        case_result_str = f\"[{p_hat:.6f},{delta_K_hat:.6f},{mu_K:.6f}]\"\n        results.append(case_result_str)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3145853"}]}