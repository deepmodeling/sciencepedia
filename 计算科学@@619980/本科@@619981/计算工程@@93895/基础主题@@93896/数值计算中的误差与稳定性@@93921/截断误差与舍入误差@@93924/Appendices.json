{"hands_on_practices": [{"introduction": "在数值计算中，截断误差和舍入误差常常是一对相互制约的因素。本练习将通过一个经典的数值微分问题，让你亲手揭示并量化这种制约关系[@problem_id:2447368]。我们将为函数 $f(x) = \\exp(x)$ 在 $x=1$ 处的一阶导数寻找一个理论上的最优步长 $h^\\star$，并通过编程实验来验证我们的理论预测，从而直观地理解如何在减小截断误差和抑制舍入误差之间找到最佳平衡点。", "problem": "实现一个程序，研究指数函数向前差分数值微分中的截断误差和舍入误差。考虑函数 $f(x) = \\exp(x)$ 在点 $x = 1$ 处的导数。使用向前差分近似 $f'(x) \\approx \\dfrac{f(x+h) - f(x)}{h}$。您的任务是：从第一性原理推导出一个误差模型，利用该模型获得理论上的最佳步长 $h$，然后通过在浮点运算中扫描一系列步长 $h$ 来凭经验验证这一预测。\n\n您的推导应仅基于以下广为接受的原理：\n- 一个足够光滑的函数在点 $x$ 附近的泰勒展开：当 $h \\to 0$ 时，$f(x+h) = f(x) + f'(x)h + \\dfrac{1}{2} f''(x) h^2 + \\mathcal{O}(h^3)$。\n- IEEE 754（电气和电子工程师协会）算术中浮点就近舍入的标准模型：对于任何基本运算和数 $y$，$\\operatorname{fl}(y) = y(1+\\delta)$，其中 $|\\delta| \\le u$，$u$ 是单位舍入误差。对于一个给定的机器epsilon $\\varepsilon$（定义为1与下一个可表示数之间的差值），$u = \\varepsilon/2$。\n\n需执行的任务：\n1. 根据上述原理，推导在 $x=1$ 处向前差分近似的主阶绝对误差模型，该模型结合了截断误差和舍入误差，并用 $h$、$f(1)$、$f''(1)$ 和机器epsilon $\\varepsilon$ 表示。然后，从此模型中得出使主阶误差最小化的理论最佳步长 $h^\\star$。明确地用 $\\varepsilon$ 表达当 $f(x) = \\exp(x)$ 且 $x=1$ 时的 $h^\\star$。所有量均为无量纲。\n2. 实现一个程序，该程序：\n   - 使用与指定浮点数据类型相关联的机器epsilon $\\varepsilon$ 来计算理论上的 $h^\\star$。\n   - 通过在指定区间内扫描一组对数间隔的 $h$ 值，并选择使绝对误差 $|D(h) - f'(1)|$ 最小的 $h$，来凭经验估算最佳 $h$。其中 $D(h) = \\dfrac{f(1+h) - f(1)}{h}$ 是用目标浮点数据类型计算的。参考值 $f'(1)$ 必须用足够高的精度计算，以确保它不会对经验评估产生偏差。所有量均为无量纲。\n   - 针对每个测试用例，报告一个包含三个浮点数的列表：理论最佳步长 $h^\\star$、经验最佳步长 $h_{\\mathrm{emp}}$ 以及在扫描中达到的最小绝对误差。\n3. 使用以下测试套件，该套件通过改变浮点精度和扫描区间来探究不同的误差状况。对于每个用例，在指定区间上扫描200个对数间隔的 $h$ 值：\n   - 测试用例A（理想情况，双精度）：数据类型 $\\texttt{float64}$，$h \\in [10^{-16}, 10^{-1}]$。\n   - 测试用例B（单精度）：数据类型 $\\texttt{float32}$，$h \\in [10^{-10}, 10^{0}]$。\n   - 测试用例C（边界情况，双精度下的舍入误差主导窗口）：数据类型 $\\texttt{float64}$，$h \\in [10^{-20}, 10^{-14}]$。\n4. 最终输出格式：您的程序应生成单行输出，其中包含用方括号括起来的逗号分隔列表形式的结果。每个元素本身是一个三元素列表，顺序为 $[h^\\star, h_{\\mathrm{emp}}, \\min\\_h |D(h) - f'(1)|]$，对应一个测试用例。例如，结构必须严格符合 $[[a_1,b_1,c_1],[a_2,b_2,c_2],[a_3,b_3,c_3]]$ 的形式。所有量均为无量纲实数。\n\n不需要外部输入，所有量均为无量纲。不涉及角度，输出中也没有百分比。", "solution": "在尝试任何解决方案之前，需对问题陈述进行验证。\n\n步骤1：提取已知条件\n- 待微分函数：$f(x) = \\exp(x)$。\n- 微分点：$x = 1$。\n- 数值微分公式：向前差分近似，$f'(x) \\approx \\dfrac{f(x+h) - f(x)}{h}$。\n- 原理1（泰勒展开）：当 $h \\to 0$ 时，$f(x+h) = f(x) + f'(x)h + \\dfrac{1}{2} f''(x) h^2 + \\mathcal{O}(h^3)$。\n- 原理2（浮点误差模型）：对于一个数 $y$，其浮点表示为 $\\operatorname{fl}(y) = y(1+\\delta)$，其中 $|\\delta| \\le u$，$u = \\varepsilon/2$ 是单位舍入误差，$\\varepsilon$ 是机器epsilon。\n- 任务1：针对 $x=1$ 处的近似，推导一个结合了截断误差和舍入误差的主阶绝对误差模型。从此模型中，推导当 $f(x)=\\exp(x)$ 且 $x=1$ 时的理论最佳步长 $h^\\star$，用 $\\varepsilon$ 表示。\n- 任务2：实现一个程序，用于计算理论上的 $h^\\star$ 并通过在扫描的 $h$ 值范围内最小化绝对误差来找到经验最佳步长 $h_{\\mathrm{emp}}$。\n- 任务3：程序必须针对三个测试用例运行：\n    - 用例 A：`float64` 精度，$h \\in [10^{-16}, 10^{-1}]$。\n    - 用例 B：`float32` 精度，$h \\in [10^{-10}, 10^{0}]$。\n    - 用例 C：`float64` 精度，$h \\in [10^{-20}, 10^{-14}]$。\n    - 所有用例中，扫描都必须使用200个对数间隔点。\n- 任务4：最终输出必须是表示列表的列表的单个字符串，每个内部列表为 $[h^\\star, h_{\\mathrm{emp}}, \\text{min_error}]$。所有量均为无量纲。\n\n步骤2：使用提取的已知条件进行验证\n- **科学依据：** 该问题稳固地处在数值分析和计算工程的标准体系之内。它探讨了有限差分法中截断误差和舍入误差之间的基本权衡。所用的函数、原理和误差模型都是标准的并且在事实上是正确的。\n- **定义明确：** 问题定义明确。它指定了函数、近似方法、求值点、推导原理以及计算和报告的精确任务。可以推导出 $h^\\star$ 的唯一解析结果，并且明确指定了经验搜索方法。\n- **客观性：** 问题以精确、客观的语言陈述，没有歧义或主观论断。\n\n步骤3：结论与行动\n问题陈述是有效的。这是一个计算科学中的标准、定义明确的问题。我将继续进行推导和实现。\n\n**误差模型与最佳步长的推导**\n\n目标是分析 $f(x) = \\exp(x)$ 在 $x=1$ 处导数的计算向前差分近似 $\\hat{D}(h)$ 的总误差。总误差是两个分量的和：截断误差，源于用有限差分近似导数；以及舍入误差，源于浮点运算的有限精度。\n\n1.  **截断误差分析**\n    精确的向前差商为 $D(h) = \\dfrac{f(x+h) - f(x)}{h}$。我们使用 $f(x+h)$ 在 $x$ 点的泰勒展开：\n    $$f(x+h) = f(x) + h f'(x) + \\frac{h^2}{2} f''(x) + \\mathcal{O}(h^3)$$\n    将此代入 $D(h)$ 的表达式中：\n    $$D(h) = \\frac{\\left( f(x) + h f'(x) + \\frac{h^2}{2} f''(x) + \\mathcal{O}(h^3) \\right) - f(x)}{h} = f'(x) + \\frac{h}{2}f''(x) + \\mathcal{O}(h^2)$$\n    截断误差 $E_{\\text{trunc}}(h)$ 是近似值与真实导数之间的差：\n    $$E_{\\text{trunc}}(h) = D(h) - f'(x) = \\frac{h}{2}f''(x) + \\mathcal{O}(h^2)$$\n    因此，主阶绝对截断误差为 $|\\frac{h}{2}f''(x)|$。\n\n2.  **舍入误差分析**\n    在浮点运算中，我们计算的是 $\\hat{D}(h) = \\operatorname{fl}\\left(\\dfrac{\\operatorname{fl}(f(x+h)) - \\operatorname{fl}(f(x))}{h}\\right)$。我们首先分析分子中的误差。设 $y_1 = f(x)$ 和 $y_2 = f(x+h)$。它们的计算值为：\n    $$\\hat{y}_1 = \\operatorname{fl}(y_1) = y_1(1+\\delta_1)$$\n    $$\\hat{y}_2 = \\operatorname{fl}(y_2) = y_2(1+\\delta_2)$$\n    其中 $|\\delta_1|, |\\delta_2| \\le u$，而 $u = \\varepsilon/2$ 是单位舍入误差。\n    减法运算同样会产生舍入：\n    $$\\operatorname{fl}(\\hat{y}_2 - \\hat{y}_1) = (\\hat{y}_2 - \\hat{y}_1)(1+\\delta_3) = (y_2(1+\\delta_2) - y_1(1+\\delta_1))(1+\\delta_3)$$\n    展开并仅保留 $\\delta_i$ 的一阶项：\n    $$\\operatorname{fl}(\\hat{y}_2 - \\hat{y}_1) \\approx (y_2 - y_1) + y_2\\delta_2 - y_1\\delta_1$$\n    分子中的舍入误差约为 $y_2\\delta_2 - y_1\\delta_1$。当 $h \\to 0$ 时，$y_2 = f(x+h) \\approx f(x) = y_1$。因此，计算出的分子误差的界为：\n    $$|y_2\\delta_2 - y_1\\delta_1| \\le |y_2||\\delta_2| + |y_1||\\delta_1| \\approx |f(x)|u + |f(x)|u = 2u|f(x)| = \\varepsilon|f(x)|$$\n    此误差随后被 $h$ 除。最终结果中的舍入误差 $E_{\\text{round}}(h)$ 主要由分子中的误差决定。因此，主阶绝对舍入误差为：\n    $$|E_{\\text{round}}(h)| \\approx \\frac{\\varepsilon |f(x)|}{h}$$\n\n3.  **总误差和最佳步长**\n    总绝对误差 $\\mathcal{E}(h)$ 是主阶截断误差和舍入误差的量值之和：\n    $$\\mathcal{E}(h) \\approx |E_{\\text{trunc}}(h)| + |E_{\\text{round}}(h)| = \\frac{h}{2}|f''(x)| + \\frac{\\varepsilon |f(x)|}{h}$$\n    为找到使该总误差最小化的步长 $h^\\star$，我们将 $\\mathcal{E}(h)$ 对 $h$ 求导并令结果为零：\n    $$\\frac{d\\mathcal{E}}{dh} = \\frac{1}{2}|f''(x)| - \\frac{\\varepsilon |f(x)|}{h^2} = 0$$\n    求解 $h^2$：\n    $$h^2 = \\frac{2\\varepsilon |f(x)|}{|f''(x)|}$$\n    这给出了最佳步长：\n    $$h^\\star = \\sqrt{\\frac{2\\varepsilon |f(x)|}{|f''(x)|}}$$\n\n4.  **应用于 $f(x) = \\exp(x)$ 在 $x=1$ 的情况**\n    对于给定函数 $f(x) = e^x$，我们有 $f'(x) = e^x$ 和 $f''(x) = e^x$。在点 $x=1$ 处：\n    $$f(1) = e^1 = e$$\n    $$f''(1) = e^1 = e$$\n    由于 $e > 0$，绝对值符号可以去掉。将这些代入 $h^\\star$ 的公式中：\n    $$h^\\star = \\sqrt{\\frac{2\\varepsilon \\cdot e}{e}} = \\sqrt{2\\varepsilon}$$\n    这就是对 $e^x$ 在 $x=1$ 处的导数进行向前差分近似的理论最佳步长。它仅取决于所用浮点运算的机器epsilon $\\varepsilon$。推导到此完成。接下来的实现将着手找到这个 $h^\\star$，并将其与经验确定的值进行比较。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Investigates truncation and round-off error in the forward-difference\n    numerical differentiation of f(x) = exp(x) at x = 1.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    # Each case is (data_type_string, h_min, h_max).\n    test_cases = [\n        ('float64', 1e-16, 1e-1),\n        ('float32', 1e-10, 1e0),\n        ('float64', 1e-20, 1e-14),\n    ]\n\n    # Use a high-precision value for the true derivative f'(1) = e.\n    # np.longdouble provides higher precision than float64, preventing a bias\n    # in the error calculation.\n    true_derivative = np.exp(np.longdouble(1))\n\n    results = []\n    \n    for dtype_str, h_min, h_max in test_cases:\n        # 1. Set up the environment for the current test case.\n        if dtype_str == 'float64':\n            dtype = np.float64\n        elif dtype_str == 'float32':\n            dtype = np.float32\n        else:\n            raise ValueError(f\"Unsupported data type: {dtype_str}\")\n\n        # Get machine epsilon for the current data type.\n        eps = np.finfo(dtype).eps\n\n        # 2. Calculate the theoretical optimal step size h_star.\n        # As derived, h_star = sqrt(2 * epsilon).\n        h_star = np.sqrt(2 * eps)\n\n        # 3. Perform the empirical scan to find the optimal h.\n        \n        # Define the point of differentiation and the function f(x) = e^x,\n        # ensuring calculations use the specified data type.\n        x_val = dtype(1.0)\n        f = lambda val: np.exp(val, dtype=dtype)\n        \n        # Generate 200 logarithmically spaced values for h in the given interval.\n        # These values are cast to the target data type.\n        h_values = np.logspace(np.log10(h_min), np.log10(h_max), 200, dtype=dtype)\n        \n        min_abs_error = np.inf\n        h_emp = np.nan\n        \n        for h in h_values:\n            # Ensure h is not zero, which can happen with very small logspace ends.\n            if h == 0:\n                continue\n\n            # Calculate the forward-difference approximation D(h).\n            # The calculation is performed entirely in the target precision.\n            D_h = (f(x_val + h) - f(x_val)) / h\n            \n            # Calculate the absolute error. The subtraction is done with D_h promoted\n            # to the higher precision of true_derivative.\n            abs_error = np.abs(D_h - true_derivative)\n            \n            # Update the minimum error and corresponding h.\n            if abs_error < min_abs_error:\n                min_abs_error = abs_error\n                h_emp = h\n        \n        # Cast the minimum error back to a standard Python float for consistent output.\n        min_abs_error_float = float(min_abs_error)\n        \n        # 4. Store the results for this test case.\n        results.append([h_star, h_emp, min_abs_error_float])\n\n    # Final print statement in the exact required format.\n    # The output should look like [[a1,b1,c1],[a2,b2,c2],[a3,b3,c3]].\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2447368"}, {"introduction": "在数学上完全等价的公式，在有限精度的计算机上可能表现出截然不同的数值稳定性。这个练习将通过计算一个数据集的方差来深刻揭示这一问题[@problem_id:2447454]。你将比较“单遍”原始矩公式 $\\mathbb{E}[X^2] - (\\mathbb{E}[X])^2$ 和“双遍”中心矩公式 $\\mathbb{E}[(X-\\mathbb{E}[X])^2]$ 的计算结果，并亲眼见证当数据均值远大于其标准差时，前者如何因“灾难性抵消”而产生巨大误差，而后者则能保持稳健。", "problem": "您需要实现一个完整、可运行的程序，该程序通过比较两种计算公式，来演示在计算均值大、偏差小的数据集方差时出现的截断误差和舍入误差。其理论基础是方差作为实值随机变量的二阶中心矩的定义以及标准的浮点舍入模型。请使用以下事实作为起点：\n- 对于具有有限二阶矩的实值随机变量 $X$，其方差由二阶中心矩定义：$\\operatorname{Var}(X) = \\mathbb{E}\\big[(X - \\mu)^2\\big]$，其中 $\\mu = \\mathbb{E}[X]$。\n- 对于实数，二阶原始矩满足 $\\mathbb{E}[X^2] = \\operatorname{Var}(X) + \\mu^2$。\n- 在电子电气工程师协会（IEEE）binary64 格式（常称为双精度）中的浮点运算近似遵循舍入模型 $ \\operatorname{fl}(a \\,\\circ\\, b) = (a \\,\\circ\\, b)(1 + \\delta)$，其中 $|\\delta| \\le \\epsilon_{\\text{mach}}$，$\\epsilon_{\\text{mach}}$ 是机器 epsilon，$\\circ$ 是一种算术运算。减去两个几乎相等的数会导致灾难性抵消，从而丢失有效数字。\n\n您的程序必须：\n- 构建指定的、元素具有大均值和小偏差的数据集。\n- 使用 IEEE binary64 算法通过两种方法计算方差：\n  1. 原始矩单遍形式：计算 $\\mathbb{E}[X]$ 和 $\\mathbb{E}[X^2]$，然后构成 $\\mathbb{E}[X^2] - (\\mathbb{E}[X])^2$。\n  2. 中心化两遍形式：首先计算 $\\mu = \\mathbb{E}[X]$，然后在第二遍中计算 $\\mathbb{E}[(X-\\mu)^2]$。\n- 使用十进制算术计算一个高精度参考方差，其精度要足够高，以至于运算中的舍入误差与 IEEE binary64 相比可以忽略不计。在高精度计算中使用中心矩定义 $\\mathbb{E}[(X-\\mu)^2]$。\n- 量化每种浮点方法相对于高精度参考值的绝对误差。\n- 为测试套件中的每个数据集，按顺序生成单行输出，其中包含一个包含五个值的列表：单遍法方差、两遍法方差、高精度参考方差、单遍法结果的绝对误差以及两遍法结果的绝对误差。\n\n所有数据集都是纯数字的；本问题不涉及物理单位，也不涉及角度。\n\n为覆盖正常路径、边界重点以及灾难性抵消的边缘情况而设计的测试套件：\n- 测试 $1$（大均值周围的对称小偏差，使用整数以避免输入量化）：设 $M = 10^{8}$ 且 $D = \\{-3,-1,0,1,3\\}$。数据集为 $X = \\{ M + d \\mid d \\in D\\}$。对于精确实数，真实方差等于偏差平方的平均值，即 $4$。\n- 测试 $2$（偏差均值非零的非对称小偏差）：设 $M = 10^{8}$ 且 $D = \\{0,1,2,3,4\\}$。数据集为 $X = \\{ M + d \\mid d \\in D\\}$。对于精确实数，真实方差等于 $2$。\n- 测试 $3$（样本更大，偏差微小且平滑变化）：设 $M = 10^{8}$ 且 $D = \\left\\{ \\frac{k-500}{1000} \\;\\middle|\\; k=0,1,\\dots,999 \\right\\}$。数据集为 $X = \\{ M + d \\mid d \\in D\\}$。对于精确实数，真实方差是此算术网格的总体方差；它接近于 $1/12$ 减去微小偏差均值的平方，并且必须由您的高精度例程精确计算。\n- 测试 $4$（相对于均值，在分辨率极限附近的极端小偏差）：设 $M = 10^{8}$ 且 $D = \\{10^{-8},-10^{-8}\\}$。数据集为 $X = \\{ M + d \\mid d \\in D\\}$。对于精确实数，真实方差等于 $10^{-16}$。\n\n高精度参考要求：\n- 使用基数为十的十进制算术构建参考值，精度至少为 $p = 100$ 位。如上所述，使用 $M$ 和 $D$ 的精确十进制值构建数据集（例如，精确使用 $M = 100000000$ 以及诸如 $(k-500)/1000$ 的有理偏差作为精确小数）。使用两遍中心矩定义 $\\mathbb{E}[(X-\\mu)^2]$ 计算总体方差，其中 $\\mathbb{E}[\\cdot]$ 是有限集上的算术平均值。\n\n浮点计算要求：\n- 通过数值库中的标准数组，使用 IEEE binary64（双精度）计算单遍原始矩和两遍中心矩的总体方差。不要应用任何补偿求和或数值稳定技巧；以显而易见的方式使用直接的均值和总和，以便截断和舍入误差可见。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含一个 Python 风格的列表，该列表包含四个内部列表，按测试 $1$ 到 $4$ 的顺序对应每个测试用例。每个内部列表必须包含五个浮点数：$[\\text{var\\_one\\_pass}, \\text{var\\_two\\_pass}, \\text{var\\_ref}, \\text{abs\\_err\\_one}, \\text{abs\\_err\\_two}]$。例如，一个语法上有效的输出行看起来像 $[[v_{11},v_{12},v_{13},e_{11},e_{12}],[v_{21},v_{22},v_{23},e_{21},e_{22}],\\dots]$，其中的数值由您的程序填充。\n\n程序不得有用户输入，也不得使用外部文件。程序必须完全确定测试数据，执行计算，并打印所需的单行输出。输出必须是浮点数。", "solution": "用户提出了一个计算工程领域的问题，要求分析方差计算中的数值稳定性。该问题是有效的、适定的，并且有科学依据。它解决了数值方法中的一个基本问题：由于浮点运算中的灾难性抵消导致的精度损失。\n\n核心任务是比较计算数据集 $X = \\{x_1, x_2, \\dots, x_N\\}$ 总体方差的两种公式：\n\n1.  **单遍（或原始矩）公式：** 此方法源于代数恒等式 $\\operatorname{Var}(X) = \\mathbb{E}[X^2] - (\\mathbb{E}[X])^2$。在计算上，它涉及单遍遍历数据以计算值的总和与平方和，然后从中计算出均值。该公式为：\n    $$ \\sigma^2 = \\frac{1}{N}\\sum_{i=1}^{N} x_i^2 - \\left(\\frac{1}{N}\\sum_{i=1}^{N} x_i\\right)^2 $$\n    虽然该公式对于实数是数学上精确的，但当标准差 $\\sigma$ 相对于均值 $\\mu = \\mathbb{E}[X]$ 很小时，它在数值上是不稳定的。$\\mathbb{E}[X^2]$ 和 $(\\mathbb{E}[X])^2$ 这两项变得非常接近。具体来说，$\\mathbb{E}[X^2] = \\sigma^2 + \\mu^2$，因此该公式相当于计算 $\\sigma^2 = (\\sigma^2 + \\mu^2) - \\mu^2$。当使用有限精度浮点运算（例如 IEEE binary64）进行求值时，这涉及到两个非常大且几乎相等的数的减法。此操作是灾难性抵消的典型例子。两个数的首位数字相互抵消，导致微小差值的大部分或全部有效数字丢失。计算 $\\mathbb{E}[X^2]$ 和 $(\\mathbb{E}[X])^2$ 时的舍入误差量级约为 $\\mu^2 \\epsilon_{\\text{mach}}$，成为最终结果的主导部分，可能产生一个极不准确甚至为负的方差。\n\n2.  **两遍（或中心矩）公式：** 此方法更贴近方差的定义，即离均差平方的均值。它需要对数据进行两遍遍历。\n    $$ \\mu = \\frac{1}{N}\\sum_{i=1}^{N} x_i $$\n    $$ \\sigma^2 = \\frac{1}{N}\\sum_{i=1}^{N} (x_i - \\mu)^2 $$\n    在第一遍中，计算均值 $\\mu$。在第二遍中，使用这个计算出的均值来找到平方偏差 $(x_i - \\mu)^2$，然后对其求平均。此方法在数值上要稳健得多。减法 $x_i - \\mu$ 仍然被执行，但结果是一组小数（偏差）。对这些小数进行平方和求和不涉及大数的减法。误差的主要来源是 $\\mu$ 的初始计算。计算出的均值 $\\hat{\\mu}$ 的误差会传播到偏差中。然而，这个误差通常很小。对于具有大均值 $M$ 和小偏差的数据，计算出的均值误差量级约为 $M\\epsilon_{\\text{mach}}$。只要这个误差相对于真实偏差的量级很小，两遍算法就能得出准确的结果。\n\n对于本问题，还需要进行高精度参考计算。这将使用 Python 的 `decimal` 模块，以 $p=100$ 位的精度执行。在此精度水平上，与标准 binary64 浮点运算相比，舍入误差可以忽略不计，从而提供了一个“基准值”，用来与其他两种方法进行比较。\n\n该程序将被构造成处理四个特定的测试用例。每个用例都使用一个具有大均值（$M=10^8$）和小偏差的数据集，旨在暴露单遍公式的数值缺陷。\n\n-   **测试 1 & 2：** 偏差是小整数。预计两遍法将非常准确。预计单遍法会因灾难性抵消而失败。\n-   **测试 3：** 一个具有平滑变化的微小有理偏差的较大数据集。预计行为类似。\n-   **测试 4：** 偏差极小（$d = \\pm 10^{-8}$），接近 binary64 相对于均值的解析度极限。对于 $x = M+d$， $d$ 的值小于量级为 $M$ 的数可能有的最小增量（即 $M \\cdot \\epsilon_{\\text{mach}} \\approx 10^8 \\cdot 2.22 \\times 10^{-16} = 2.22 \\times 10^{-8}$）。结果是，$fl(M+d)$ 将被舍入为 $M$ 本身。这展示了另一种误差来源：初始数据表示中的信息丢失，这将导致两种浮点方法计算出的方差均为 $0$。\n\n实现将通过定义一个函数来进行，该函数接收一个数据集，使用三种方法（单遍浮点法、两遍浮点法和高精度参考法）计算方差，计算浮点方法的绝对误差，并返回结果。该函数将对每个测试用例调用，收集到的结果将被格式化为指定的输出字符串。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom decimal import Decimal, getcontext\n\ndef solve():\n    \"\"\"\n    Computes and compares variance using three different methods to demonstrate\n    truncation and round-off errors.\n    \"\"\"\n    # Set the precision for decimal arithmetic to 100 digits, as required.\n    getcontext().prec = 100\n\n    def analyze_dataset(M_str: str, D_list: list):\n        \"\"\"\n        Performs the full analysis for a given dataset definition.\n\n        Args:\n            M_str: The large mean component as a string for exact Decimal conversion.\n            D_list: A list of deviation values (as Decimal objects).\n\n        Returns:\n            A list containing the five required values:\n            [var_one_pass, var_two_pass, var_ref, abs_err_one, abs_err_two]\n        \"\"\"\n        # 1. High-precision reference calculation using the decimal module.\n        # This serves as the ground truth.\n        M_dec = Decimal(M_str)\n        X_dec = [M_dec + d for d in D_list]\n        N_dec = Decimal(len(X_dec))\n        \n        # Use two-pass formula for the reference calculation.\n        mu_dec = sum(X_dec) / N_dec\n        var_ref = sum([(x - mu_dec)**2 for x in X_dec]) / N_dec\n\n        # 2. Floating-point calculations using numpy (IEEE binary64).\n        # Construct the dataset using standard float64.\n        # Note: float() conversion from Decimal can introduce small errors,\n        # but the dominant error source is the variance algorithm itself.\n        X_fp = np.array([float(x) for x in X_dec], dtype=np.float64)\n        \n        # Method 1: One-pass (raw-moment) formula. Prone to catastrophic cancellation.\n        # sigma^2 = E[X^2] - (E[X])^2\n        mean_of_squares = np.mean(X_fp**2)\n        square_of_mean = np.mean(X_fp)**2\n        var_one_pass = mean_of_squares - square_of_mean\n        \n        # Method 2: Two-pass (centered-moment) formula. More numerically stable.\n        # sigma^2 = E[(X - E[X])^2]\n        mu_fp = np.mean(X_fp)\n        var_two_pass = np.mean((X_fp - mu_fp)**2)\n        \n        # 3. Quantify absolute errors.\n        abs_err_one = abs(var_one_pass - float(var_ref))\n        abs_err_two = abs(var_two_pass - float(var_ref))\n        \n        return [var_one_pass, var_two_pass, float(var_ref), abs_err_one, abs_err_two]\n\n    # --- Define and run all test cases ---\n    test_cases_defs = [\n        # Test 1: Symmetric small integer deviations. True Var = 4.\n        {'M': '1e8', 'D': [Decimal(s) for s in ['-3', '-1', '0', '1', '3']]},\n        \n        # Test 2: Non-symmetric small integer deviations. True Var = 2.\n        {'M': '1e8', 'D': [Decimal(s) for s in ['0', '1', '2', '3', '4']]},\n        \n        # Test 3: Larger sample with small rational deviations.\n        {'M': '1e8', 'D': [(Decimal(k) - Decimal(500)) / Decimal(1000) for k in range(1000)]},\n        \n        # Test 4: Extremely small deviations at the limit of float64 resolution. True Var = 1e-16.\n        {'M': '1e8', 'D': [Decimal('1e-8'), Decimal('-1e-8')]}\n    ]\n\n    all_results = []\n    for case in test_cases_defs:\n        result = analyze_dataset(case['M'], case['D'])\n        all_results.append(result)\n\n    # Final print statement in the exact required format.\n    # The format is a list of lists, e.g., [[v1,v2,...],[v1,v2,...]]\n    formatted_inner_lists = []\n    for res_list in all_results:\n        # Format each inner list as \"[v1,v2,v3,e1,e2]\"\n        formatted_list_str = f\"[{','.join(map(str, res_list))}]\"\n        formatted_inner_lists.append(formatted_list_str)\n    \n    # Join the inner lists into the final output format \"[ [...], [...], ... ]\"\n    final_output = f\"[{','.join(formatted_inner_lists)}]\"\n    print(final_output)\n\nsolve()\n```", "id": "2447454"}, {"introduction": "当对大量浮点数进行求和时，尤其是当这些数字的量级差异巨大时，简单的累加会因舍入误差的不断累积而导致最终结果严重偏离真实值。本练习旨在介绍并实现 Kahan 求和算法，这是一种经典的补偿求和技术[@problem_id:2447409]。通过在专门设计的测试用例上将其与朴素求和进行比较，你将体验到该算法如何通过巧妙地追踪和补偿每一步的舍入误差，从而显著提高求和的精度。", "problem": "您必须编写一个可完整运行的程序，用以评估对实数序列求和时的舍入误差，并展示如何使用 Kahan 求和算法来减小该误差。所有计算都必须使用标准双精度二进制浮点运算进行。对于每个测试用例，请对同一序列计算两个和：一个是朴素的从左到右浮点数求和，另一个是使用 Kahan 求和算法的补偿求和。对于每个和，都需计算其相对于一个高精度参考和的绝对误差。您的程序随后必须输出单行文本，其中包含所有测试用例的所有绝对误差，并遵循指定的顺序和格式。\n\n将计算和 $\\hat{S}$ 相对于参考值 $S^{\\star}$ 的绝对误差定义为 $E = \\lvert \\hat{S} - S^{\\star} \\rvert$。\n\n测试套件由以下四个序列组成：\n\n- 测试用例 $1$（在一个较大的基准值上加上许多微小增量）：\n  - 序列 $S_1$ 的长度为 $N_1 + 1$，其中 $N_1 = 10^{6}$。首项为 $s^{(1)}_0 = 1$，其余 $N_1$ 项为 $s^{(1)}_k = 10^{-16}$（$1 \\le k \\le N_1$）。\n\n- 测试用例 $2$（重复的灾难性抵消三元组）：\n  - 令 $M = 2 \\cdot 10^{5}$。序列 $S_2$ 由 $M$ 个三项块 $(1, 10^{-16}, -1)$ 串联而成。\n\n- 测试用例 $3$（带有轻微偏差的确定性伪随机小数值）：\n  - 令模数 $m = 2^{64}$，乘数 $a = 6364136223846793005$，增量 $c = 1442695040888963407$，种子 $x_0 = 123456789123456789$。通过 $x_{k+1} \\equiv a x_k + c \\pmod{m}$（$k \\ge 0$）定义一个线性同余生成器。令 $N_3 = 5 \\cdot 10^{4}$。对于 $k = 1, 2, \\dots, N_3$，定义 $u_k = \\frac{x_k}{m} - \\frac{1}{2}$ 和序列项 $s^{(3)}_k = 10^{-12} u_k + 10^{-16}$。序列 $S_3$ 由这 $N_3$ 个项组成。\n\n- 测试用例 $4$（短序列中的动态范围和抵消）：\n  - 序列 $S_4$ 包含五项：$(10^{16}, 1, -10^{16}, 3, 4 \\cdot 10^{-16})$。\n\n对于每个测试用例 $i \\in \\{1,2,3,4\\}$，计算：\n- 朴素和 $\\hat{S}^{\\text{naive}}_i$，通过双精度浮点运算进行从左到右的累加。\n- Kahan 补偿和 $\\hat{S}^{\\text{Kahan}}_i$，使用 Kahan 求和算法及双精度浮点运算得出。\n- 一个高精度参考值 $S^{\\star}_i$，根据序列的数学定义计算得出，尽可能使用精确算术，或使用至少有 50 位正确小数位的十进制任意精度算术，以确保双精度舍入不会污染 $S^{\\star}_i$。\n\n对于每个测试用例 $i$，计算绝对误差 $E^{\\text{naive}}_i = \\lvert \\hat{S}^{\\text{naive}}_i - S^{\\star}_i \\rvert$ 和 $E^{\\text{Kahan}}_i = \\lvert \\hat{S}^{\\text{Kahan}}_i - S^{\\star}_i \\rvert$。\n\n最终输出格式：\n- 生成单行输出，包含一个用方括号括起来的逗号分隔列表。该列表必须按以下顺序包含 8 个数字：\n  - $E^{\\text{naive}}_1, E^{\\text{Kahan}}_1, E^{\\text{naive}}_2, E^{\\text{Kahan}}_2, E^{\\text{naive}}_3, E^{\\text{Kahan}}_3, E^{\\text{naive}}_4, E^{\\text{Kahan}}_4$。\n- 每个数字必须四舍五入到 12 位有效数字，并以十进制形式表示（可接受科学记数法）。\n- 所需单行格式示例（仅为说明）：$[e_1,e_2,e_3,e_4,e_5,e_6,e_7,e_8]$。\n\n本问题不涉及任何物理单位或角度单位。程序必须是自包含的，且不得要求任何用户输入或外部文件。只要现代编程语言遵循标准双精度浮点语义，根据上述定义，结果就必须是可精确复现的。", "solution": "问题陈述已经过分析，并被确定为有效。它在科学上基于数值分析的原理，特别是关于浮点运算和舍入误差。这是一个适定问题，提供了计算唯一、可验证解所需的所有数据和定义。它是客观的，没有歧义。\n\n这个问题的核心是展示并量化在对量级差异巨大的浮点数求和过程中发生的精度损失，以及如何使用补偿求和算法来减小这种误差。\n\n所有计算均使用标准双精度浮点运算，这对应于 IEEE 754 $64$ 位格式。该格式具有大约 $15$ 到 $17$ 位十进制数字的精度。对于此格式，机器 epsilon $\\epsilon$（即满足 $1.0 + \\epsilon > 1.0$ 的最小数）约为 $2.22 \\times 10^{-16}$。当两个量级差异巨大的数相加时，较小的数可能会部分或完全丢失。这种现象被称为“吞没”(swamping)。\n\n第一种求和方法是朴素的从左到右累加。对于序列 $s_0, s_1, \\dots, s_N$，其和 $\\hat{S}^{\\text{naive}}$ 计算为 $(\\dots((s_0 + s_1) + s_2) + \\dots + s_N)$。此方法极易受到舍入误差的影响。\n\n第二种方法是 Kahan 求和算法，这是一种补偿求和方法。它能显著减少对有限精度浮点数序列求和时产生的数值误差。该算法维护一个运行中的补偿变量 $c$，用于累积那些本会丢失的误差。对于序列中的每一项 $s_k$，更新规则如下：\n$$y_k = s_k - c_{k-1}$$\n$$t_k = \\text{sum}_{k-1} + y_k$$\n$$c_k = (t_k - \\text{sum}_{k-1}) - y_k$$\n$$\\text{sum}_k = t_k$$\n此处，$\\text{sum}_0 = 0$ 且 $c_0 = 0$。项 $(t_k - \\text{sum}_{k-1})$ 恢复了 $y_k$ 的高位部分，从中减去 $y_k$ 则分离出低位部分（即舍入误差），该部分存储在 $c_k$ 中，并会从下一项 $s_{k+1}$ 中减去。\n\n绝对误差定义为 $E = \\lvert \\hat{S} - S^{\\star} \\rvert$，其中 $\\hat{S}$ 是计算所得的和，$S^{\\star}$ 是高精度参考和。\n\n测试用例分析：\n\n测试用例 1：\n序列为 $s^{(1)}_0 = 1$，后跟 $N_1 = 10^6$ 个项，这些项为 $s^{(1)}_k = 10^{-16}$（$k \\ge 1$）。\n精确和为 $S^{\\star}_1 = 1 + 10^6 \\times 10^{-16} = 1 + 10^{-10}$。\n在朴素求和中，我们计算 $1 + 10^{-16} + 10^{-16} + \\dots$。相对于 $1.0$，$10^{-16}$ 这一项非常接近机器 epsilon。在双精度运算中，操作 $1.0 + 10^{-16}$ 会遭受“吞没”；结果很可能会被舍入回 $1.0$。因此，大部分小数项将会丢失，预计 $\\hat{S}^{\\text{naive}}_1$ 会非常接近 $1.0$，导致误差接近 $10^{-10}$。\nKahan 算法会在每一步中将丢失的部分 $10^{-16}$ 捕获到补偿变量 $c$ 中并重新引入，从而得到一个与 $S^{\\star}_1$ 极其接近的结果 $\\hat{S}^{\\text{Kahan}}_1$。误差 $E^{\\text{Kahan}}_1$ 应接近机器精度。\n\n测试用例 2：\n序列由 $M = 2 \\cdot 10^5$ 个块 $(1, 10^{-16}, -1)$ 组成。\n单个块的精确和为 $1 + 10^{-16} - 1 = 10^{-16}$。总精确和为 $S^{\\star}_2 = 2 \\cdot 10^5 \\times 10^{-16} = 2 \\cdot 10^{-11}$。\n朴素求和会计算 $(1 + 10^{-16}) - 1$。与第一个用例一样，$1 + 10^{-16}$ 很可能被舍入为 $1.0$，因此 $(1 + 10^{-16}) - 1$ 的计算结果为 $0$。对所有块重复此过程，$\\hat{S}^{\\text{naive}}_2$ 预计为 $0.0$，导致误差 $E^{\\text{naive}}_2$ 恰好为 $2 \\cdot 10^{-11}$。\nKahan 算法将防止这种抵消误差，生成一个非常接近 $S^{\\star}_2$ 的和 $\\hat{S}^{\\text{Kahan}}_2$，以及一个更小的误差 $E^{\\text{Kahan}}_2$。\n\n测试用例 3：\n序列由 $N_3 = 5 \\cdot 10^4$ 个项 $s^{(3)}_k = 10^{-12} u_k + 10^{-16}$ 组成，其中 $u_k = \\frac{x_k}{m} - \\frac{1}{2}$，而 $x_k$ 来自一个线性同余生成器（LCG）。$u_k$ 的值在 $[-0.5, 0.5)$ 范围内伪随机分布。$s^{(3)}_k$ 项是小数值，带有一个 $10^{-16}$ 的微小正偏差。\n精确和为 $S^{\\star}_3 = \\sum_{k=1}^{N_3} (10^{-12} u_k + 10^{-16}) = 10^{-12} \\sum_{k=1}^{N_3} u_k + N_3 \\cdot 10^{-16}$。\n必须使用高精度算术计算此和，以作为参考值 $S^{\\star}_3$。LCG 状态 $x_{k+1} \\equiv a x_k + c \\pmod{m}$ 使用 $64$ 位整数运算计算。和 $\\sum x_k$ 使用任意精度整数计算，而 $S^{\\star}_3$ 的最终表达式使用高精度十进制算术求值。\n在 $5 \\cdot 10^4$ 次加法中，朴素求和会累积微小的舍入误差。预计 Kahan 算法将最小化这种累积，导致 $E^{\\text{Kahan}}_3 \\ll E^{\\text{naive}}_3$。\n\n测试用例 4：\n序列为 $(10^{16}, 1, -10^{16}, 3, 4 \\cdot 10^{-16})$。\n精确和为 $S^{\\star}_4 = (10^{16} - 10^{16}) + (1 + 3) + 4 \\cdot 10^{-16} = 4 + 4 \\cdot 10^{-16}$。\n朴素的从左到右求和按步计算如下：\n1. $10^{16} + 1 = 10^{16}$ (发生吞没，因为 $1$ 小于 $10^{16}$ 的末位单位值)。\n2. $10^{16} - 10^{16} = 0$。\n3. $0 + 3 = 3$。\n4. $3 + 4 \\cdot 10^{-16} = 3$ (发生吞没，因为相对于 $3$，$4 \\cdot 10^{-16}$ 小于机器 epsilon)。\n因此，$\\hat{S}^{\\text{naive}}_4 = 3$。误差为 $E^{\\text{naive}}_4 = \\lvert 3 - (4 + 4 \\cdot 10^{-16}) \\rvert \\approx 1$。\nKahan 求和算法正是为处理这种情况而设计的。第一步中损失的 $1$ 将被补偿变量捕获。最终的和 $\\hat{S}^{\\text{Kahan}}_4$ 应非常接近真实和 $S^{\\star}_4$，从而产生一个非常小的误差 $E^{\\text{Kahan}}_4$。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nimport decimal\n\ndef solve():\n    \"\"\"\n    Computes and prints round-off errors for naive and Kahan summations\n    for four specific test cases, adhering to the problem specification.\n    \"\"\"\n\n    def naive_sum(sequence):\n        \"\"\"Computes the naive left-to-right sum of a sequence.\"\"\"\n        s = 0.0\n        for x in sequence:\n            s += x\n        return s\n\n    def kahan_sum(sequence):\n        \"\"\"Computes the sum of a sequence using Kahan's algorithm.\"\"\"\n        s = 0.0\n        c = 0.0\n        for x in sequence:\n            y = x - c\n            t = s + y\n            c = (t - s) - y\n            s = t\n        return s\n\n    def generate_test_cases():\n        \"\"\"Generates the sequences for all four test cases.\"\"\"\n        # Test Case 1: Many tiny increments\n        N1 = 10**6\n        seq1 = np.full(N1, 1e-16, dtype=np.float64)\n        seq1 = np.insert(seq1, 0, 1.0)\n        \n        # Test Case 2: Repeated catastrophic cancellation\n        M = 2 * 10**5\n        block = np.array([1.0, 1e-16, -1.0], dtype=np.float64)\n        seq2 = np.tile(block, M)\n        \n        # Test Case 3: LCG-based sequence\n        m = 2**64\n        a = 6364136223846793005\n        c = 1442695040888963407\n        x0 = 123456789123456789\n        N3 = 5 * 10**4\n        \n        seq3 = np.zeros(N3, dtype=np.float64)\n        x_current = x0\n        for k in range(N3):\n            x_current = (a * x_current + c) % m\n            u_k = x_current / m - 0.5\n            seq3[k] = 1e-12 * u_k + 1e-16\n\n        # Test Case 4: Dynamic range and cancellation\n        seq4 = np.array([1e16, 1.0, -1e16, 3.0, 4e-16], dtype=np.float64)\n        \n        return [seq1, seq2, seq3, seq4]\n\n    def get_reference_sums():\n        \"\"\"Computes high-accuracy reference sums for all test cases.\"\"\"\n        # Set precision for Decimal calculations\n        decimal.getcontext().prec = 100\n\n        # Reference Sum 1\n        N1 = 10**6\n        s_star_1 = decimal.Decimal(1) + decimal.Decimal(N1) * decimal.Decimal('1e-16')\n\n        # Reference Sum 2\n        M = 2 * 10**5\n        s_star_2 = decimal.Decimal(M) * decimal.Decimal('1e-16')\n\n        # Reference Sum 3\n        m = 2**64\n        a = 6364136223846793005\n        c = 1442695040888963407\n        x0 = 123456789123456789\n        N3 = 5 * 10**4\n        \n        sum_x = 0\n        x_current = x0\n        for _ in range(N3):\n            x_current = (a * x_current + c) % m\n            sum_x += x_current\n        \n        D_sum_x = decimal.Decimal(sum_x)\n        D_m = decimal.Decimal(m)\n        D_N3 = decimal.Decimal(N3)\n        D_1e_12 = decimal.Decimal('1e-12')\n        D_1e_16 = decimal.Decimal('1e-16')\n        D_half = decimal.Decimal('0.5')\n        \n        sum_u = D_sum_x / D_m - D_N3 * D_half\n        s_star_3 = D_1e_12 * sum_u + D_N3 * D_1e_16\n\n        # Reference Sum 4\n        s_star_4 = decimal.Decimal('4') + decimal.Decimal('4e-16')\n        \n        return [float(s_star_1), float(s_star_2), float(s_star_3), float(s_star_4)]\n\n    sequences = generate_test_cases()\n    reference_sums = get_reference_sums()\n    \n    results = []\n    \n    for i in range(4):\n        seq = sequences[i]\n        s_star = reference_sums[i]\n        \n        # Naive sum and its error\n        s_naive = naive_sum(seq)\n        e_naive = abs(s_naive - s_star)\n        \n        # Kahan sum and its error\n        s_kahan = kahan_sum(seq)\n        e_kahan = abs(s_kahan - s_star)\n        \n        results.extend([e_naive, e_kahan])\n\n    # Format output to 12 significant digits and print\n    formatted_results = [f\"{res:.12g}\" for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "2447409"}]}