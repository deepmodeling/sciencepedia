{"hands_on_practices": [{"introduction": "QR 分解不仅仅是一种抽象的矩阵分解；它的 $Q$ 因子为几何运算提供了直接的工具。这个练习将巩固您的理解，即 $Q$ 的列构成了原矩阵 $A$ 列空间的一组标准正交基，从而使投影计算变得简单直接。通过这个计算，您将亲身体会到 QR 分解与正交投影之间的内在联系 [@problem_id:1385303]。", "problem": "设矩阵 $A$ 和向量 $\\mathbf{b}$ 定义如下：\n$$A = \\begin{pmatrix} 1 & 1 \\\\ 0 & 1 \\\\ 1 & 0 \\end{pmatrix}, \\quad \\mathbf{b} = \\begin{pmatrix} 1 \\\\ 2 \\\\ 1 \\end{pmatrix}$$\n$A$ 的列向量张成 $\\mathbb{R}^3$ 的一个子空间，我们将其记为 $W = \\operatorname{col}(A)$。$A$ 的QR分解为 $A=QR$，其中 $Q$ 是一个列向量标准正交的矩阵，$R$ 是一个上三角矩阵。矩阵 $Q$ 的列向量构成了 $W$ 的一个标准正交基，其表达式为：\n$$Q = \\begin{pmatrix} \\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{6}} \\\\ 0 & \\frac{2}{\\sqrt{6}} \\\\ \\frac{1}{\\sqrt{2}} & -\\frac{1}{\\sqrt{6}} \\end{pmatrix}$$\n求向量 $\\mathbf{p}$，它是 $\\mathbf{b}$ 在子空间 $W$ 上的正交投影。将您的最终答案表示为一个包含 $\\mathbf{p}$ 的三个分量的行矩阵，并使用精确分数。", "solution": "我们已知一个矩阵 $Q$，其标准正交的列向量张成了 $W=\\operatorname{col}(A)$。$\\mathbf{b}$ 在 $W$ 上的正交投影 $\\mathbf{p}$ 由以下公式给出\n$$\n\\mathbf{p} = Q Q^{T} \\mathbf{b} = \\sum_{i=1}^{2} (q_{i}^{T}\\mathbf{b})\\, q_{i},\n$$\n其中 $q_{1}$ 和 $q_{2}$ 是 $Q$ 的列向量。\n\n根据给定的 $Q$，\n$$\nq_{1} = \\begin{pmatrix} \\frac{1}{\\sqrt{2}} \\\\ 0 \\\\ \\frac{1}{\\sqrt{2}} \\end{pmatrix}, \\quad\nq_{2} = \\begin{pmatrix} \\frac{1}{\\sqrt{6}} \\\\ \\frac{2}{\\sqrt{6}} \\\\ -\\frac{1}{\\sqrt{6}} \\end{pmatrix}, \\quad\n\\mathbf{b} = \\begin{pmatrix} 1 \\\\ 2 \\\\ 1 \\end{pmatrix}.\n$$\n计算系数：\n$$\nq_{1}^{T}\\mathbf{b} = \\frac{1}{\\sqrt{2}}\\cdot 1 + 0\\cdot 2 + \\frac{1}{\\sqrt{2}}\\cdot 1 = \\frac{2}{\\sqrt{2}} = \\sqrt{2},\n$$\n$$\nq_{2}^{T}\\mathbf{b} = \\frac{1}{\\sqrt{6}}\\cdot 1 + \\frac{2}{\\sqrt{6}}\\cdot 2 - \\frac{1}{\\sqrt{6}}\\cdot 1 = \\frac{1+4-1}{\\sqrt{6}} = \\frac{4}{\\sqrt{6}}.\n$$\n因此，\n$$\n\\mathbf{p} = (q_{1}^{T}\\mathbf{b})\\,q_{1} + (q_{2}^{T}\\mathbf{b})\\,q_{2}\n= \\sqrt{2}\\,q_{1} + \\frac{4}{\\sqrt{6}}\\,q_{2}.\n$$\n计算每一项：\n$$\n\\sqrt{2}\\,q_{1} = \\sqrt{2}\\begin{pmatrix} \\frac{1}{\\sqrt{2}} \\\\ 0 \\\\ \\frac{1}{\\sqrt{2}} \\end{pmatrix}\n= \\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\end{pmatrix},\n\\quad\n\\frac{4}{\\sqrt{6}}\\,q_{2} = \\frac{4}{\\sqrt{6}}\\begin{pmatrix} \\frac{1}{\\sqrt{6}} \\\\ \\frac{2}{\\sqrt{6}} \\\\ -\\frac{1}{\\sqrt{6}} \\end{pmatrix}\n= \\begin{pmatrix} \\frac{4}{6} \\\\ \\frac{8}{6} \\\\ -\\frac{4}{6} \\end{pmatrix}\n= \\begin{pmatrix} \\frac{2}{3} \\\\ \\frac{4}{3} \\\\ -\\frac{2}{3} \\end{pmatrix}.\n$$\n将它们相加得到\n$$\n\\mathbf{p} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\end{pmatrix} + \\begin{pmatrix} \\frac{2}{3} \\\\ \\frac{4}{3} \\\\ -\\frac{2}{3} \\end{pmatrix}\n= \\begin{pmatrix} \\frac{5}{3} \\\\ \\frac{4}{3} \\\\ \\frac{1}{3} \\end{pmatrix}.\n$$\n按照要求，这已用精确分数表示。根据题目要求写成行矩阵，我们将分量写在单行中。", "answer": "$$\\boxed{\\begin{pmatrix} \\frac{5}{3} & \\frac{4}{3} & \\frac{1}{3} \\end{pmatrix}}$$", "id": "1385303"}, {"introduction": "从使用给定的 $Q$ 矩阵，我们现在转向亲手构造它。本实践将指导您通过算法化实现 Gram-Schmidt 过程来构建“薄”QR 分解，这是许多计算任务的核心 [@problem_id:2430018]。您还将接触到关键的实际计算概念，如数值秩检测、通过重正交化提高数值稳定性，以及“薄”QR 相对于“完整”QR 在存储和计算上的效率优势。", "problem": "要求您在一个纯粹的数学线性代数背景下，实现并验证一个用于“瘦”QR分解的计算过程。请从以下基本基础出发：内积的定义、欧几里得范数以及单位正交向量集的概念。具体来说，利用内积可以导出正交投影以及单位正交向量可以简化投影运算这两个事实。除了这些核心定义外，不要假设任何预先推导的算法公式或恒等式。通过将向量反复投影到已构建的单位正交方向上并减去这些投影来构建算法步骤。\n\n给定一个实矩阵 $A \\in \\mathbb{R}^{m \\times n}$ 且 $m \\ge n$，“瘦”QR 分解是一种分解 $A \\approx Q R$，其中 $Q \\in \\mathbb{R}^{m \\times r}$ 具有单位正交列，其列向量张成的子空间与 $A$ 的列向量张成的子空间相同（直到数值秩 $r \\le n$），而 $R \\in \\mathbb{R}^{r \\times n}$ 是一个上梯形矩阵（当 $r = n$ 时为上三角矩阵）。“完全”QR分解使用一个方形正交因子 $Q_{\\mathrm{full}} \\in \\mathbb{R}^{m \\times m}$ 和一个 $R_{\\mathrm{full}} \\in \\mathbb{R}^{m \\times n}$，其中后 $(m-n)$ 行为零。\n\n您的任务：\n\n1) 实现一个函数，在给定 $A$ 的情况下，仅使用内积和正交投影的定义来产生“瘦”QR 分解。具体而言，通过迭代地减去当前列在先前构建的 $q_i$ 上的投影并进行归一化来构造列 $q_j$，并可选择进行第二遍正交化以提高数值稳定性。使用一个数值容差 $\\tau$ 来检测数值秩 $r$，使得投影后范数低于 $\\tau$ 的列被视为数值上线性相关而从 $Q$ 中排除。您的函数必须返回 $(Q, R, r)$，其中 $Q \\in \\mathbb{R}^{m \\times r}$ 具有单位正交列，$R \\in \\mathbb{R}^{r \\times n}$ 是上梯形矩阵，而 $r$ 是检测到的秩。\n\n2) 使用构建的 $Q$ 形成到 $A$ 的列空间（由 $Q$ 的列向量捕获）的正交投影算子 $P = Q Q^{\\mathsf{T}} \\in \\mathbb{R}^{m \\times m}$。数值上验证该投影算子的性质：对称性和幂等性。您的验证指标必须是定量的，并基于范数。\n\n3) 仅使用维度计数和投影性质（而非预先引用的算法成本公式），解释“瘦”QR 分解在何时以及为何在存储和应用方面比“完全”QR 分解更高效。根据因子中存储的标量元素的数量来量化存储。\n\n为了进行评估，您的程序必须实现上述要求，并为每个测试用例生成一个包含四个浮点数的列表：\n- 相对重构误差 $e_{\\mathrm{rec}} = \\lVert A - Q R \\rVert_{\\mathrm{F}} / \\lVert A \\rVert_{\\mathrm{F}}$。\n- 单位正交性误差 $e_{\\mathrm{orth}} = \\lVert Q^{\\mathsf{T}} Q - I_r \\rVert_{\\mathrm{F}}$，其中 $I_r$ 是 $r \\times r$ 的单位矩阵。\n- 投影算子幂等性误差 $e_{\\mathrm{proj}} = \\lVert P^2 - P \\rVert_{\\mathrm{F}}$，其中 $P = Q Q^{\\mathsf{T}}$。\n- 存储比率 $\\rho = \\dfrac{\\text{瘦因子中存储的标量数}}{\\text{完全因子中存储的标量数}}$，其中分子是 $Q \\in \\mathbb{R}^{m \\times r}$ 和 $R \\in \\mathbb{R}^{r \\times n}$ 中标量元素的总数，分母是 $Q_{\\mathrm{full}} \\in \\mathbb{R}^{m \\times m}$ 和 $R_{\\mathrm{full}} \\in \\mathbb{R}^{m \\times n}$ 中标量元素的总数。\n\n测试套件（三个矩阵），需确定性地生成：\n- 情况 1 (高且良态的): $m = 8$, $n = 3$。令 $A \\in \\mathbb{R}^{8 \\times 3}$ 的元素由种子为 $7$ 的伪随机数生成器生成，为独立同分布的标准正态分布。\n- 情况 2 (高且近乎秩亏的): $m = 10$, $n = 5$。首先使用种子 $13$ 生成一个 $B \\in \\mathbb{R}^{10 \\times 4}$，其元素为独立同分布的标准正态分布，然后设置第五列为 $A_{:,5} = B_{:,1} + \\epsilon \\, \\eta$，其中 $\\eta \\in \\mathbb{R}^{10}$ 的元素由同一生成器生成，为独立同分布的标准正态分布，且 $\\epsilon = 10^{-16}$。$A$ 的前四列按顺序设置为 $B$ 的列。\n- 情况 3 (方形范德蒙矩阵): $m = n = 5$。令 $x_k = \\dfrac{k}{m-1}$，其中 $k = 0, 1, 2, 3, 4$。定义 $A \\in \\mathbb{R}^{5 \\times 5}$ 为 $A_{i,j} = x_i^{j}$，其中 $i = 0, 1, 2, 3, 4$ 且 $j = 0, 1, 2, 3, 4$。\n\n使用数值容差 $\\tau = \\varepsilon \\cdot \\max(m,n) \\cdot \\lVert A \\rVert_{\\mathrm{F}}$，其中 $\\varepsilon$ 是双精度浮点运算的机器精度。所有范数均为 Frobenius 范数。不涉及角度；不涉及物理单位。\n\n您的程序应产生单行输出，其中包含三个测试用例的结果，格式为逗号分隔的列表的列表，每个内部列表按 $[e_{\\mathrm{rec}}, e_{\\mathrm{orth}}, e_{\\mathrm{proj}}, \\rho]$ 的顺序列出，并用方括号括起来（例如，$[[a,b,c,d],[a',b',c',d'],[a'',b'',c'',d'']]$），其中所有 $a,b,c,d$ 符号表示您的程序计算出的浮点数。", "solution": "该问题要求实现并验证一种“瘦”QR 分解算法，该算法从线性代数的基本原理推导而来，即内积和正交投影。该算法必须对数值秩亏具有鲁棒性。随后，需要构建一个正交投影算子并验证其性质。最后，要求解释瘦分解相对于完全分解在计算和存储效率方面的优势。该问题是适定的、科学上合理的，并包含了得出确定性解所需的所有信息。\n\n### 第 1 部分：QR 分解算法的推导\n\n目标是将一个具有列向量 $\\{a_1, a_2, \\dots, a_n\\}$ 的矩阵 $A \\in \\mathbb{R}^{m \\times n}$ 分解为 $A \\approx QR$，其中 $Q \\in \\mathbb{R}^{m \\times r}$ 具有单位正交列 $\\{q_1, q_2, \\dots, q_r\\}$，$R \\in \\mathbb{R}^{r \\times n}$ 是一个上梯形矩阵。$Q$ 的列必须构成 $A$ 的列空间（记为 $\\operatorname{span}(A)$）的一个单位正交基。整数 $r \\le n$ 代表 $A$ 的数值秩。\n\n此过程的基础是 Gram-Schmidt 正交化过程，它从一组线性无关的向量构造出一个单位正交集。我们从两个向量 $u, v \\in \\mathbb{R}^{m}$ 的标准欧几里得内积的定义开始，其定义为 $\\langle u, v \\rangle = u^{\\mathsf{T}}v$。该内积导出欧几里得范数 $\\lVert u \\rVert_2 = \\sqrt{\\langle u, u \\rangle}$。\n\n一个向量 $a_j$ 到一个单位向量 $q_i$ 方向上的正交投影由 $\\operatorname{proj}_{q_i} a_j = \\langle a_j, q_i \\rangle q_i$ 给出。为了使向量 $a_j$ 与 $q_i$ 正交，我们减去这个投影：$a_j - \\operatorname{proj}_{q_i} a_j$。为了使 $a_j$ 与整个单位正交集 $\\{q_1, \\dots, q_{k}\\}$ 正交，我们减去在每个基向量上的投影：\n$$ v_j = a_j - \\sum_{i=1}^{k} \\operatorname{proj}_{q_i} a_j = a_j - \\sum_{i=1}^{k} \\langle a_j, q_i \\rangle q_i $$\n得到的向量 $v_j$ 与 $\\{q_1, \\dots, q_{k}\\}$ 中的每个向量都正交。\n\n算法逐列进行，对于 $j=1, \\dots, n$：\n1.  对于当前列 $a_j$，我们计算一个向量 $v_j$，它与所有先前构建的单位正交向量 $\\{q_1, \\dots, q_{r}\\}$ 正交，其中 $r$ 是当前已找到的基向量的数量。\n2.  如果 $v_j$ 不是零向量（即其范数高于数值容差），它代表一个独立于先前方向的新方向。我们将其归一化以获得下一个单位正交向量，$q_{r+1} = v_j / \\lVert v_j \\rVert_2$。\n3.  投影的系数和归一化因子构成了 $R$ 矩阵的第 $j$ 列。\n\n这个过程构建了分解 $A=QR$。对于一个满秩矩阵（$r=n$），重排投影公式，我们对每个列 $a_j$ 有：\n$$ a_j = \\sum_{i=1}^{j-1} \\langle a_j, q_i \\rangle q_i + \\lVert v_j \\rVert_2 q_j = \\sum_{i=1}^{j} r_{ij} q_i $$\n其中对于 $i<j$，有 $r_{ij} = \\langle a_j, q_i \\rangle$，而 $r_{jj} = \\lVert v_j \\rVert_2$。用矩阵形式表示，即为 $A = QR$。\n\n为了数值稳定性，上述的经典 Gram-Schmidt 过程由于浮点数相消误差容易导致正交性的丧失。问题陈述建议进行“第二遍正交化”，我们将其实现为迭代 Gram-Schmidt 过程。在通过减去投影计算出 $v_j$ 后，我们对 $v_j$ 本身重复该过程，以消除因有限精度运算而持续存在的、在 $\\{q_1, \\dots, q_r\\}$ 方向上的任何剩余分量。这等同于执行两次正交化步骤。\n\n为了处理潜在的秩亏，我们引入一个容差 $\\tau$。问题指定 $\\tau = \\varepsilon \\cdot \\max(m,n) \\cdot \\lVert A \\rVert_{\\mathrm{F}}$，其中 $\\varepsilon$ 是机器精度。在将列 $a_j$ 与现有基正交化后，如果所得向量 $v_j$ 的范数小于或等于 $\\tau$，即 $\\lVert v_j \\rVert_2 \\le \\tau$，我们认为 $a_j$ 在数值上与前面的列线性相关。在这种情况下，我们不为 $Q$ 生成新的列，秩 $r$ 也不增加。$R$ 的相应列将包含表示 $a_j$ 作为 $Q$ 的列的线性组合的系数，其对角线及以下元素为零。\n\n由此产生的算法（带秩检测的迭代修正 Gram-Schmidt）如下：\n1.  初始化一个空列表用于存放 $Q$ 的列，一个 $n \\times n$ 的零矩阵用于 $R$，并设置秩 $r=0$。\n2.  对于 $A$ 的每一列 $a_j$（从 $j=0$到 $n-1$）：\n    a. 设置 $v = a_j$。\n    b. 进行两遍（再次正交化）：\n        i. 对于每个已有的单位正交向量 $q_i$ （从 $i=0$到 $r-1$）：\n           - 计算投影系数 $c = \\langle v, q_i \\rangle$。\n           - 减去投影：$v \\leftarrow v - c \\cdot q_i$。\n           - 将系数加到 $R$ 的相应条目上：$R_{i,j} \\leftarrow R_{i,j} + c$。\n    c. 计算正交化后向量的范数，$\\lVert v \\rVert_2$。\n    d. 如果 $\\lVert v \\rVert_2 > \\tau$：\n        i. 增加秩：$r \\leftarrow r+1$。\n        ii. 设置 $R$ 的对角元素：$R_{r-1,j} = \\lVert v \\rVert_2$。\n        iii. 归一化以找到新的基向量：$q_r = v / \\lVert v \\rVert_2$。将 $q_r$ 添加到 $Q$ 的列集合中。\n3.  在遍历 $A$ 的所有列之后，组装最终的矩阵：$Q$ 是由收集到的 $r$ 个基向量堆叠而成，得到一个 $m \\times r$ 矩阵。$R$ 是取临时 $n \\times n$ 矩阵的前 $r$ 行，得到一个 $r \\times n$ 的上梯形矩阵。\n\n### 第 2 部分：正交投影算子和验证\n\n给定瘦 QR 分解 $A \\approx QR$，其中 $Q \\in \\mathbb{R}^{m \\times r}$ 具有单位正交列，矩阵 $P = QQ^{\\mathsf{T}}$ 是到 $Q$ 的列空间 $\\operatorname{span}(Q)$ 上的正交投影算子，该空间在计算上等价于 $\\operatorname{span}(A)$。矩阵 $P$ 属于 $\\mathbb{R}^{m \\times m}$。\n\n一个正交投影算子必须满足两个关键性质：\n1.  **对称性**：$P = P^{\\mathsf{T}}$。这由构造保证：$P^{\\mathsf{T}} = (QQ^{\\mathsf{T}})^{\\mathsf{T}} = (Q^{\\mathsf{T}})^{\\mathsf{T}}Q^{\\mathsf{T}} = QQ^{\\mathsf{T}} = P$。\n2.  **幂等性**：$P^2 = P$。此性质源于 $Q$ 列的单位正交性。由于 $Q^{\\mathsf{T}}Q = I_r$（$r \\times r$ 单位矩阵），我们有 $P^2 = (QQ^{\\mathsf{T}})(QQ^{\\mathsf{T}}) = Q(Q^{\\mathsf{T}}Q)Q^{\\mathsf{T}} = Q(I_r)Q^{\\mathsf{T}} = QQ^{\\mathsf{T}} = P$。\n\n这些性质的数值验证涉及计算与理想情况偏差的范数。问题要求计算两个这样的指标：\n-   **单位正交性误差**：$e_{\\mathrm{orth}} = \\lVert Q^{\\mathsf{T}}Q - I_r \\rVert_{\\mathrm{F}}$。这衡量了计算出的 $Q$ 的列与完全单位正交的接近程度。任何偏离零的数值都表示正交性的损失，这将影响 $P$ 的幂等性。\n-   **投影算子幂等性误差**：$e_{\\mathrm{proj}} = \\lVert P^2 - P \\rVert_{\\mathrm{F}}$。这直接衡量了幂等性。从上面的推导可知，此误差与单位正交性误差直接相关。\n\n### 第 3 部分：瘦 QR 分解与完全 QR 分解的效率\n\n“瘦”QR 分解相对于“完全”版本的效率可以通过考虑存储需求和应用成本来分析，这些都基于因子的维度。\n\n-   **完全 QR 分解**：对于一个矩阵 $A \\in \\mathbb{R}^{m \\times n}$，完全分解为 $A = Q_{\\mathrm{full}} R_{\\mathrm{full}}$，其中 $Q_{\\mathrm{full}} \\in \\mathbb{R}^{m \\times m}$ 是一个正交矩阵，$R_{\\mathrm{full}} \\in \\mathbb{R}^{m \\times n}$ 是一个上梯形矩阵（其最后 $m-n$ 行为零）。\n-   **瘦 QR 分解**：瘦分解为 $A \\approx QR$，其中 $Q \\in \\mathbb{R}^{m \\times r}$ 具有单位正交列，$R \\in \\mathbb{R}^{r \\times n}$ 是一个上梯形矩阵。这里，$r \\le n \\le m$ 是数值秩。\n\n**存储效率：**\n\n我们通过因子中需要存储的标量元素总数来量化存储。\n-   完全 QR 的存储：矩阵 $Q_{\\mathrm{full}}$ 需要存储 $m \\times m = m^2$ 个标量。矩阵 $R_{\\mathrm{full}}$ 需要存储 $m \\times n$ 个标量。\n    总存储量 $S_{\\mathrm{full}} = m^2 + mn = m(m+n)$。\n-   瘦 QR 的存储：矩阵 $Q$ 需要 $m \\times r$ 个标量。矩阵 $R$ 需要 $r \\times n$ 个标量。\n    总存储量 $S_{\\text{thin}} = mr + rn = r(m+n)$。\n\n按要求，存储比率为：\n$$ \\rho = \\frac{S_{\\text{thin}}}{S_{\\text{full}}} = \\frac{r(m+n)}{m(m+n)} = \\frac{r}{m} $$\n由于 $r \\le n \\le m$，比率 $\\rho$ 总是小于或等于 $1$。当 $r$ 远小于 $m$ ($r \\ll m$) 时，可以实现显著的存储节省。对于 $n \\ll m$（意味着 $r \\ll m$）的“高瘦”矩阵，或对于任何 $r < n$ 的秩亏矩阵，这一点尤其正确。完全分解计算并存储了 $\\mathbb{R}^m$ 的一个完整单位正交基，而瘦分解仅计算和存储了与 $A$ 相关的子空间（即其列空间）的基。\n\n**应用效率（计算成本）：**\n\n效率增益也延伸到应用这些因子时，这在求解线性系统或最小二乘问题中很常见。例如，求解最小二乘问题 $\\min_x \\lVert Ax-b \\rVert_2$ 涉及计算 $Q^{\\mathsf{T}}b$。\n-   使用完全因子：乘积 $Q_{\\mathrm{full}}^{\\mathsf{T}}b$（其中 $b \\in \\mathbb{R}^m$）需要的运算量与 $m^2$ 成正比。\n-   使用瘦因子：乘积 $Q^{\\mathsf{T}}b$ 需要的运算量与 $mr$ 成正比。\n同样，如果 $r \\ll m$，应用瘦 $Q$ 因子的计算成本远低于应用完全 $Q_{\\mathrm{full}}$ 因子。瘦分解避免了涉及 $A$ 的列空间的正交补的计算，而这些计算通常是不必要的。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and evaluates a thin QR factorization algorithm from first principles.\n    \"\"\"\n\n    def thin_qr_from_scratch(A: np.ndarray):\n        \"\"\"\n        Computes the thin QR factorization of matrix A using an iterated\n        Gram-Schmidt process with rank detection.\n\n        Args:\n            A: A real matrix of size (m, n).\n\n        Returns:\n            A tuple (Q, R, r) where:\n            - Q is an (m, r) matrix with orthonormal columns.\n            - R is an (r, n) upper trapezoidal matrix.\n            - r is the detected numerical rank.\n        \"\"\"\n        m, n = A.shape\n        \n        # Define the numerical tolerance for rank detection\n        frob_norm_A = np.linalg.norm(A, 'fro')\n        if frob_norm_A == 0:\n            return np.zeros((m, 0)), np.zeros((0, n)), 0\n            \n        eps = np.finfo(A.dtype).eps\n        tolerance = eps * max(m, n) * frob_norm_A\n\n        # Pre-allocate matrices; they will be trimmed at the end\n        Q = np.zeros((m, n), dtype=A.dtype)\n        R = np.zeros((n, n), dtype=A.dtype)\n        rank = 0\n\n        for j in range(n):\n            v = A[:, j].copy()\n            \n            # Use two passes of orthogonalization for numerical stability\n            # This is an implementation of Iterated Gram-Schmidt\n            for _ in range(2):\n                for i in range(rank):\n                    q_i = Q[:, i]\n                    # Inner product: <v, q_i>\n                    coeff = q_i.T @ v\n                    v -= coeff * q_i\n                    R[i, j] += coeff\n\n            norm_v = np.linalg.norm(v)\n\n            if norm_v > tolerance:\n                # We found a new linearly independent direction\n                R[rank, j] = norm_v\n                Q[:, rank] = v / norm_v\n                rank += 1\n        \n        # Trim matrices to their final dimensions\n        Q_thin = Q[:, :rank]\n        R_thin = R[:rank, :]\n        \n        return Q_thin, R_thin, rank\n\n    def generate_test_cases():\n        \"\"\"\n        Generates the three deterministic test cases specified in the problem.\n        \"\"\"\n        cases = []\n\n        # Case 1: Tall, well-conditioned\n        m1, n1 = 8, 3\n        rng1 = np.random.default_rng(seed=7)\n        A1 = rng1.standard_normal((m1, n1))\n        cases.append({'A': A1, 'm': m1, 'n': n1})\n\n        # Case 2: Tall, nearly rank-deficient\n        m2, n2 = 10, 5\n        rng2 = np.random.default_rng(seed=13)\n        B2 = rng2.standard_normal((m2, n2 - 1))\n        eta = rng2.standard_normal(m2)\n        epsilon = 1e-16\n        A2 = np.zeros((m2, n2))\n        A2[:, :n2 - 1] = B2\n        A2[:, n2 - 1] = B2[:, 0] + epsilon * eta\n        cases.append({'A': A2, 'm': m2, 'n': n2})\n        \n        # Case 3: Square Vandermonde\n        m3, n3 = 5, 5\n        x3 = np.linspace(0, 1, m3)\n        # The problem defines A_ij = x_i^j.\n        # np.vander(x, increasing=False) (default) gives columns with powers N-1, N-2, ..., 0.\n        # So we flip the columns to get powers 0, 1, ..., N-1.\n        A3 = np.fliplr(np.vander(x3, N=n3))\n        cases.append({'A': A3, 'm': m3, 'n': n3})\n        \n        return cases\n\n    test_cases = generate_test_cases()\n    results = []\n\n    for case in test_cases:\n        A, m, n = case['A'], case['m'], case['n']\n\n        Q, R, r = thin_qr_from_scratch(A)\n        \n        norm_A = np.linalg.norm(A, 'fro')\n        \n        # 1. Relative reconstruction error\n        if norm_A > 0:\n            e_rec = np.linalg.norm(A - Q @ R, 'fro') / norm_A\n        else:\n            e_rec = 0.0\n\n        # 2. Orthonormality error\n        if r > 0:\n            e_orth = np.linalg.norm(Q.T @ Q - np.eye(r), 'fro')\n        else:\n            e_orth = 0.0\n\n        # 3. Projector idempotence error\n        if r > 0:\n            P = Q @ Q.T\n            e_proj = np.linalg.norm(P @ P - P, 'fro')\n        else:\n            e_proj = 0.0\n            \n        # 4. Storage ratio\n        storage_thin = m * r + r * n\n        storage_full = m * m + m * n\n        if storage_full > 0:\n            rho = storage_thin / storage_full\n        else:\n            rho = 0.0\n        \n        results.append([e_rec, e_orth, e_proj, rho])\n    \n    # Format the final output string\n    output_str = \"[\" + \",\".join([f\"[{e[0]},{e[1]},{e[2]},{e[3]}]\" for e in results]) + \"]\"\n    print(output_str)\n\nsolve()\n```", "id": "2430018"}, {"introduction": "在您实现了 QR 算法之后，现在是时候测试它的极限了。本练习探讨了数值算法的一个关键方面：稳定性。通过将经典的 Gram-Schmidt 过程应用于一个著名的病态矩阵——希尔伯特矩阵 (Hilbert matrix)，您将观察到浮点误差如何破坏 $Q$ 矩阵的理论正交性，这突显了为何稳健的算法实现在科学计算中至关重要 [@problem_id:2429964]。", "problem": "设 $n$ 为一个正整数，并设 $A \\in \\mathbb{R}^{n \\times n}$ 为希尔伯特矩阵，其元素为 $A_{i j} = \\dfrac{1}{i + j - 1}$，其中 $i \\in \\{1, \\ldots, n\\}$ 且 $j \\in \\{1, \\ldots, n\\}$。考虑正交三角 (QR) 分解 $A = Q R$，该分解通过如下定义一个向量序列来获得。对于每个列索引 $j \\in \\{1, \\ldots, n\\}$，令 $a_{j} \\in \\mathbb{R}^{n}$ 表示 $A$ 的第 $j$ 列。通过以下方式定义向量 $q_{1}, \\ldots, q_{n} \\in \\mathbb{R}^{n}$ 和标量 $r_{k j} \\in \\mathbb{R}$：\n$$\nv_{j} = a_{j} - \\sum_{k=1}^{j-1} \\left(q_{k}^{\\top} a_{j}\\right) q_{k}, \\quad\nr_{j j} = \\|v_{j}\\|_{2}, \\quad\nq_{j} = \\frac{v_{j}}{r_{j j}}, \\quad\nr_{k j} = q_{k}^{\\top} a_{j} \\text{ for } k < j,\n$$\n这得到矩阵 $Q = [\\,q_{1}\\ \\cdots\\ q_{n}\\,] \\in \\mathbb{R}^{n \\times n}$，其列向量为单位长度；以及上三角矩阵 $R \\in \\mathbb{R}^{n \\times n}$，其元素在 $k \\le j$ 时为 $r_{k j}$，其余为零。将 $Q$ 的正交性损失定义为\n$$\n\\varepsilon(Q) = \\| Q^{\\top} Q - I \\|_{F},\n$$\n其中 $I \\in \\mathbb{R}^{n \\times n}$ 是单位矩阵，$\\|\\cdot\\|_{F}$ 表示弗罗贝尼乌斯范数 (Frobenius norm)。\n\n您的任务是，对每个指定的测试用例，将 $A$ 构建为给定大小的希尔伯特矩阵，通过上述关系式构成 $Q$ 和 $R$，并计算标量 $\\varepsilon(Q)$。\n\n测试套件：\n- 用例 1：$n = 1$。\n- 用例 2：$n = 5$。\n- 用例 3：$n = 8$。\n- 用例 4：$n = 12$。\n\n所有计算都是纯数值的，无单位。您的程序应产生单行输出，包含一个由方括号括起来的逗号分隔列表（例如，“[x1,x2,x3,x4]”），其中 $xj$ 等于用例 $j$ 的 $\\varepsilon(Q)$ 值，表示为十进制浮点数。", "solution": "所呈现的问题是数值线性代数中的一个适定 (well-posed) 练习。它具有科学依据、自成体系且算法明确。因此，我们可以开始求解。\n\n任务是使用一组指定的关系式计算希尔伯特矩阵 $A \\in \\mathbb{R}^{n \\times n}$ 的 QR 分解，然后量化所得因子 $Q$ 的正交性中的数值误差。希尔伯特矩阵的定义为其元素 $A_{i j} = \\frac{1}{i + j - 1}$，其中 $i,j \\in \\{1, \\ldots, n\\}$。\n\n为计算矩阵 $Q$ 和 $R$ 所提供的关系式构成了**经典格拉姆-施密特 (Classical Gram-Schmidt, CGS)** 算法。该算法为由矩阵 $A$ 的列向量 $\\{a_{1}, a_{2}, \\ldots, a_{n}\\}$ 张成的向量空间构造一个标准正交基 $\\{q_{1}, q_{2}, \\ldots, q_{n}\\}$。这个过程是迭代的。对于 $A$ 的每一列 $a_{j}$，我们首先将其相对于先前已构造的标准正交向量 $\\{q_{1}, \\ldots, q_{j-1}\\}$ 进行正交化。这是通过减去 $a_{j}$ 在这些向量张成的子空间上的投影来实现的。得到的向量 $v_{j}$ 随后被归一化为欧几里得范数为 $1$，从而产生标准正交集中的下一个向量 $q_{j}$。\n\n具体指定的流程如下：\n对于从 $1$ 到 $n$ 的每个列索引 $j$：\n1. 向量 $a_{j}$ 相对于先前的标准正交向量 $q_{1}, \\ldots, q_{j-1}$ 进行正交化：\n$$v_{j} = a_{j} - \\sum_{k=1}^{j-1} \\operatorname{proj}_{q_k}(a_j) = a_{j} - \\sum_{k=1}^{j-1} (q_{k}^{\\top} a_{j}) q_{k}$$\n对于 $k < j$，系数 $r_{k j} = q_{k}^{\\top} a_{j}$ 是上三角矩阵 $R$ 对角线上方的元素。\n\n2. 对得到的向量 $v_{j}$ 进行归一化，以产生单位向量 $q_{j}$：\n$$r_{j j} = \\|v_{j}\\|_{2}$$\n$$q_{j} = \\frac{v_{j}}{r_{j j}}$$\n值 $r_{j j}$ 是矩阵 $R$ 的第 $j$ 个对角元素。\n\n3. 生成的矩阵是 $Q = [\\,q_{1}\\ \\cdots\\ q_{n}\\,]$ 和一个上三角矩阵 $R$，其元素在 $k \\le j$ 时为 $r_{k j}$。根据构造，有 $A = QR$。\n\n最后一步是评估计算出的矩阵 $Q$ 的正交性。在精确算术中，矩阵 $Q$ 将是完全正交的，即 $Q^{\\top} Q = I$，其中 $I$ 是单位矩阵。然而，在浮点运算中，舍入误差会累积。CGS 算法是众所周知的数值不稳定算法，这意味着这些误差可能导致 $Q$ 的列向量丧失显著的正交性。当原始矩阵 $A$ 的列向量近似线性相关时，这种现象尤为显著，而这正是希尔伯特矩阵的一个关键特性，因为其条件数随 $n$ 极快增长。\n\n正交性损失使用与单位矩阵偏差的弗罗贝尼乌斯范数来衡量：\n$$\\varepsilon(Q) = \\| Q^{\\top} Q - I \\|_{F}$$\n矩阵 $X \\in \\mathbb{R}^{n \\times n}$ 的弗罗贝尼乌斯范数定义为 $\\|X\\|_{F} = \\sqrt{\\sum_{i=1}^{n}\\sum_{j=1}^{n} X_{ij}^2}$。\n\n实现过程如下：\n1. 对于测试套件中给定的每个整数 $n$，我们首先构造 $n \\times n$ 的希尔伯特矩阵 $A$。注意，对于一个 0-索引的计算数组，第 $i$ 行第 $j$ 列的元素将是 $1/(i+j+1)$。\n2. 我们将矩阵 $Q \\in \\mathbb{R}^{n \\times n}$ 和 $R \\in \\mathbb{R}^{n \\times n}$ 初始化为零。\n3. 我们从 $j = 0$ 到 $n-1$ 进行迭代（使用 0-基索引）：\n    a. 从 $A$ 中提取列 $a_j$。\n    b. 初始化 $v_j = a_j$。\n    c. 对于 $k = 0$ 到 $j-1$，计算系数 $r_{kj} = q_k^{\\top}a_j$，将其存储在 $R$ 中，并更新 $v_j \\leftarrow v_j - r_{kj}q_k$。\n    d. 计算范数 $r_{jj} = \\|v_j\\|_2$ 并将其存储在 $R$ 中。\n    e. 计算新的标准正交向量 $q_j = v_j/r_{jj}$ 并将其作为一列存储在 $Q$ 中。\n4. 迭代完成后，我们计算矩阵乘积 $C = Q^{\\top}Q$。\n5. 我们计算误差矩阵 $E = C - I$。\n6. 最后，我们计算 $E$ 的弗罗贝尼乌斯范数以获得 $\\varepsilon(Q)$。\n\n对测试套件中提供的每个 $n$ 值重复此过程。$\\varepsilon(Q)$ 值随 $n$ 的增加将展示 CGS 算法应用于病态问题时特有的渐进的正交性损失。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the orthogonality loss of the Q factor from a QR factorization\n    of the Hilbert matrix using the Classical Gram-Schmidt (CGS) algorithm.\n    \"\"\"\n    test_cases = [1, 5, 8, 12]\n    results = []\n\n    for n in test_cases:\n        # Step 1: Construct the Hilbert matrix A.\n        # The problem uses 1-based indexing for matrix entries: A_ij = 1/(i+j-1).\n        # In 0-based numpy, this corresponds to A[i,j] = 1/((i+1)+(j+1)-1) = 1/(i+j+1).\n        A = np.fromfunction(lambda i, j: 1.0 / (i + j + 1.0), (n, n), dtype=float)\n\n        # Initialize Q and R matrices.\n        Q = np.zeros((n, n), dtype=float)\n        R = np.zeros((n, n), dtype=float)\n\n        # Step 2: Apply the Classical Gram-Schmidt algorithm as specified.\n        # This loop iterates through the columns of A to build Q and R.\n        for j in range(n):\n            a_j = A[:, j]\n            v_j = a_j.copy()\n\n            # Orthogonalize a_j against the previously computed orthonormal vectors q_k.\n            # This implements v_j = a_j - sum_{k=0}^{j-1} (q_k^T a_j) q_k.\n            # The coefficients r_kj = q_k^T a_j are also stored in R.\n            for k in range(j):\n                q_k = Q[:, k]\n                r_kj = np.dot(q_k, a_j)\n                R[k, j] = r_kj\n                v_j -= r_kj * q_k\n\n            # Normalize the orthogonal vector v_j to get the orthonormal vector q_j.\n            # The norm r_jj = ||v_j||_2 is the diagonal element of R.\n            r_jj = np.linalg.norm(v_j)\n            R[j, j] = r_jj\n            \n            # The Hilbert matrix is non-singular, so r_jj will not be zero\n            # in exact arithmetic. Numerically, it should not be close to zero for\n            # the given values of n.\n            q_j = v_j / r_jj\n            Q[:, j] = q_j\n            \n        # Step 3: Compute the orthogonality loss epsilon(Q).\n        # epsilon(Q) = ||Q^T Q - I||_F\n        identity_matrix = np.identity(n, dtype=float)\n        loss_matrix = Q.T @ Q - identity_matrix\n        epsilon_Q = np.linalg.norm(loss_matrix, 'fro')\n        \n        results.append(epsilon_Q)\n\n    # Final print statement must produce a single line in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```", "id": "2429964"}]}