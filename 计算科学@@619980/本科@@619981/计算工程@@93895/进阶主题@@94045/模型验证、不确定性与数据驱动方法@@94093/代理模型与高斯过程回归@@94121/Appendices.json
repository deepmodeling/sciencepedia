{"hands_on_practices": [{"introduction": "本节的第一个练习是建立高斯过程（GP）实践理解的基石。通过为一个假设场景中跑步者的配速和心率之间的关系建模，你将亲手实现后验预测的核心方程，计算均值估计及其不确定性，并观察模型在内插和外插等不同场景下的行为。这项实践 [@problem_id:2441367] 将帮助你建立对 GP 模型基本力学的信心。", "problem": "一个标量值潜函数 $f$ 将跑步者的配速 $p$（单位：分钟/公里）映射到心率 $f(p)$（单位：次/分钟）。给定带有噪声的观测值 $\\{(p_i, y_i)\\}_{i=1}^n$，其中 $y_i$ 是测量的心率（单位：次/分钟）。假设一个加性噪声模型 $y_i = f(p_i) + \\varepsilon_i$，其中噪声 $\\varepsilon_i$ 是独立的高斯噪声，$\\varepsilon_i \\sim \\mathcal{N}(0,\\sigma_n^2)$。\n\n假设 $f$ 的先验是一个零均值高斯过程，其协方差函数为\n$$\nk(p,p') = \\sigma_f^2 \\exp\\!\\left(-\\frac{(p - p')^2}{2\\ell^2}\\right),\n$$\n其中 $\\sigma_f$ 是信号尺度（单位：次/分钟），$\\ell$ 是长度尺度（单位：分钟/公里）。\n\n训练数据为以下 $n = 7$ 个配速-心率对：\n- 配速（分钟/公里）：$[\\,3.5,\\,4.0,\\,4.5,\\,5.0,\\,5.5,\\,6.0,\\,6.5\\,]$。\n- 测量心率（次/分钟）：$[\\,176,\\,168,\\,160,\\,150,\\,140,\\,130,\\,122\\,]$。\n\n除非下文另有说明，否则假设 $\\sigma_n = 2.0$（单位：次/分钟）。对于下面列出的每个测试用例，在指定的超参数下，计算单个测试配速 $p_\\star$ 的后验预测均值 $\\mu_\\star$（单位：次/分钟）和后验预测标准差 $s_\\star$（单位：次/分钟）。报告 $\\mu_\\star$ 和 $s_\\star$ 的值，四舍五入到三位小数。\n\n测试套件（每个用例相互独立）：\n- 用例 1：$p_\\star = 5.2$，$\\sigma_f = 60.0$，$\\ell = 0.6$，$\\sigma_n = 2.0$。\n- 用例 2：$p_\\star = 3.2$，$\\sigma_f = 60.0$，$\\ell = 0.6$，$\\sigma_n = 2.0$。\n- 用例 3：$p_\\star = 7.0$，$\\sigma_f = 60.0$，$\\ell = 0.6$，$\\sigma_n = 2.0$。\n- 用例 4：$p_\\star = 5.2$，$\\sigma_f = 60.0$，$\\ell = 0.2$，$\\sigma_n = 2.0$。\n\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，顺序为 $[\\,\\mu_\\star^{(1)}, s_\\star^{(1)}, \\mu_\\star^{(2)}, s_\\star^{(2)}, \\mu_\\star^{(3)}, s_\\star^{(3)}, \\mu_\\star^{(4)}, s_\\star^{(4)}\\,]$。所有八个数字都必须以“次/分钟”为单位表示，并四舍五入到三位小数，不含空格。例如：$[\\,150.123,2.345, \\dots\\,]$（注意：实际值会有所不同）。", "solution": "首先对问题陈述进行严格验证。所有给定条件都经过科学合理性、自洽性和客观性检查。\n\n该问题指定了一个回归任务，旨在对一个潜函数 $f(p)$ 进行建模，该函数将跑步者的配速 $p$（单位：分钟/公里）映射到心率 $f(p)$（单位：次/分钟）。该模型是一个高斯过程，具有零均值先验，$f \\sim \\mathcal{GP}(0, k(p, p'))$。其协方差函数是平方指数核函数：\n$$k(p,p') = \\sigma_f^2 \\exp\\left(-\\frac{(p - p')^2}{2\\ell^2}\\right)$$\n观测值 $y_i$ 是潜函数的带噪测量，由模型 $y_i = f(p_i) + \\varepsilon_i$ 描述，其中噪声 $\\varepsilon_i$ 是独立同分布的，服从 $\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma_n^2)$。\n\n训练数据包含 $n=7$ 对配速 $p_i$ 和观测心率 $y_i$：\n-   配速 $P = [3.5, 4.0, 4.5, 5.0, 5.5, 6.0, 6.5]$ 分钟/公里。\n-   测量心率 $Y = [176, 168, 160, 150, 140, 130, 122]$ 次/分钟。\n\n任务是为四个不同的测试用例计算单个测试配速 $p_\\star$ 的后验预测均值 $\\mu_\\star$ 和后验预测标准差 $s_\\star$。在所有用例中，噪声标准差给定为 $\\sigma_n = 2.0$ 次/分钟。\n\n验证结论：问题有效。这是一个在计算工程学的一个分支学科——代理建模领域中，定义明确且具有科学依据的问题。问题提供了所有必要的数据和模型规范，没有内部矛盾，并且以客观、明确的语言表述。我们可以继续进行解析解。\n\n解决方案的理论基础是高斯过程模型的后验预测分布的推导。设训练输入表示为向量 $X = [p_1, \\dots, p_n]^T$，对应的训练输出表示为 $\\mathbf{y} = [y_1, \\dots, y_n]^T$。潜函数在一个新测试点 $p_\\star$ 的值表示为 $f_\\star = f(p_\\star)$。\n\n根据高斯过程的定义，潜函数在训练点的值 $\\mathbf{f} = [f(p_1), \\dots, f(p_n)]^T$ 和在测试点的值 $f_\\star$ 服从联合高斯分布。由于先验均值为零，该分布为：\n$$ \\begin{pmatrix} \\mathbf{f} \\\\ f_\\star \\end{pmatrix} \\sim \\mathcal{N} \\left( \\mathbf{0}, \\begin{pmatrix} K(X, X) & K(X, p_\\star) \\\\ K(p_\\star, X) & k(p_\\star, p_\\star) \\end{pmatrix} \\right) $$\n其中 $K(X, X)$ 是 $n \\times n$ 的协方差矩阵，其元素为 $K_{ij} = k(p_i, p_j)$；$K(X, p_\\star)$ 是训练点和测试点之间的协方差构成的 $n \\times 1$ 向量；$k(p_\\star, p_\\star)$ 是测试点的先验方差。\n\n观测模型 $\\mathbf{y} = \\mathbf{f} + \\boldsymbol{\\varepsilon}$（其中 $\\boldsymbol{\\varepsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\sigma_n^2 I)$）意味着观测到的训练输出 $\\mathbf{y}$ 和潜测试值 $f_\\star$ 的联合分布也是高斯的：\n$$ \\begin{pmatrix} \\mathbf{y} \\\\ f_\\star \\end{pmatrix} \\sim \\mathcal{N} \\left( \\mathbf{0}, \\begin{pmatrix} K(X, X) + \\sigma_n^2 I & K(X, p_\\star) \\\\ K(p_\\star, X) & k(p_\\star, p_\\star) \\end{pmatrix} \\right) $$\n为方便表示，令 $K = K(X, X)$，$\\mathbf{k}_\\star = K(X, p_\\star)$，$k_{\\star\\star} = k(p_\\star, p_\\star)$。矩阵 $K_y = K + \\sigma_n^2 I$ 表示带噪观测的协方差。\n\n后验预测分布 $p(f_\\star | X, \\mathbf{y}, p_\\star)$ 是通过应用多元高斯分布的条件化规则推导出来的。这会得到一个高斯后验分布，其均值和方差如下：\n\n后验预测均值：\n$$ \\mu_\\star = \\mathbb{E}[f_\\star | X, \\mathbf{y}, p_\\star] = \\mathbf{k}_\\star^T (K + \\sigma_n^2 I)^{-1} \\mathbf{y} $$\n后验预测方差：\n$$ \\sigma_\\star^2 = \\text{Var}[f_\\star | X, \\mathbf{y}, p_\\star] = k_{\\star\\star} - \\mathbf{k}_\\star^T (K + \\sigma_n^2 I)^{-1} \\mathbf{k}_\\star $$\n问题要求的是后验预测标准差，即 $s_\\star = \\sqrt{\\sigma_\\star^2}$。这个量衡量的是对真实函数值 $f(p_\\star)$ 估计的不确定性，而不是对带噪观测值 $y_\\star$ 的不确定性。\n\n每个测试用例的逐步计算过程如下：\n$1$. 定义大小为 $n=7$ 的训练数据向量 $X$ 和 $\\mathbf{y}$。\n$2$. 对于给定的测试用例，其超参数为 $\\sigma_f, \\ell$，噪声为 $\\sigma_n$，测试点为 $p_\\star$：\n$3$. 构建 $7 \\times 7$ 的训练协方差矩阵 $K$，其元素为 $K_{ij} = \\sigma_f^2 \\exp(-\\frac{(p_i - p_j)^2}{2\\ell^2})$。\n$4$. 构建 $7 \\times 1$ 的测试协方差向量 $\\mathbf{k}_\\star$，其元素为 $k_{\\star,i} = k(p_i, p_\\star) = \\sigma_f^2 \\exp(-\\frac{(p_i - p_\\star)^2}{2\\ell^2})$。\n$5$. 测试点的先验方差为 $k_{\\star\\star} = k(p_\\star, p_\\star) = \\sigma_f^2$。\n$6$. 构造带噪协方差矩阵 $K_y = K + \\sigma_n^2 I$，其中 $I$ 是 $7 \\times 7$ 的单位矩阵，$\\sigma_n^2 = 2.0^2 = 4.0$。\n$7$. 为确保数值稳定性，避免直接进行矩阵求逆。而是求解两个线性方程组。首先，求解 $(K_y) \\boldsymbol{\\alpha} = \\mathbf{y}$ 得到向量 $\\boldsymbol{\\alpha}$。然后后验均值由点积 $\\mu_\\star = \\mathbf{k}_\\star^T \\boldsymbol{\\alpha}$ 给出。\n$8$. 其次，求解 $(K_y) \\mathbf{v} = \\mathbf{k}_\\star$ 得到向量 $\\mathbf{v}$。随后后验方差计算为 $\\sigma_\\star^2 = k_{\\star\\star} - \\mathbf{k}_\\star^T \\mathbf{v}$。求解这些方程组最稳健的方法是使用 $K_y$ 的 Cholesky 分解，因为 $K_y$ 保证是对称正定矩阵。\n$9$. 后验标准差为 $s_\\star = \\sqrt{\\sigma_\\star^2}$。必须注意处理可能由浮点运算误差引起的 $\\sigma_\\star^2$ 的微小负值，方法是确保平方根的参数为非负数。\n$10$. 将得到的 $\\mu_\\star$ 和 $s_\\star$ 值四舍五入到三位小数。\n\n对四个指定的测试用例分别以编程方式实现此完整过程。结果按要求组合成最终的输出格式。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import cholesky, cho_solve\n\ndef solve():\n    \"\"\"\n    Computes the posterior predictive mean and standard deviation for a Gaussian Process\n    regression model based on the provided problem statement.\n    \"\"\"\n    \n    # Define the training data from the problem statement.\n    p_train = np.array([3.5, 4.0, 4.5, 5.0, 5.5, 6.0, 6.5])\n    y_train = np.array([176, 168, 160, 150, 140, 130, 122])\n\n    # Define the test cases from the problem statement.\n    # Each tuple is (p_star, sigma_f, l, sigma_n).\n    test_cases = [\n        (5.2, 60.0, 0.6, 2.0),\n        (3.2, 60.0, 0.6, 2.0),\n        (7.0, 60.0, 0.6, 2.0),\n        (5.2, 60.0, 0.2, 2.0),\n    ]\n\n    results = []\n    \n    # Reshape training inputs to be a column vector for broadcasting\n    p_train_col = p_train[:, np.newaxis]\n\n    for case in test_cases:\n        p_star, sigma_f, l, sigma_n = case\n\n        # Main logic to calculate the result for one case goes here.\n        \n        # 1. Define the squared exponential covariance function (kernel).\n        def kernel(a, b, sf, ls):\n            \"\"\"Squared exponential kernel.\"\"\"\n            # Using scipy.spatial.distance.cdist would be cleaner but not allowed.\n            # We use broadcasting to compute the squared Euclidean distances.\n            sqdist = (a - b.T)**2\n            return (sf**2) * np.exp(-sqdist / (2 * ls**2))\n\n        # 2. Compute the covariance matrices and vectors.\n        # K(X, X): Covariance of training inputs\n        K = kernel(p_train_col, p_train_col, sigma_f, l)\n        \n        # K_y = K(X, X) + sigma_n^2 * I: Covariance of noisy observations\n        K_y = K + (sigma_n**2) * np.eye(len(p_train))\n        \n        # k_star = K(X, x_star): Covariance between training and test inputs\n        k_star = kernel(p_train_col, np.array([[p_star]]), sigma_f, l).flatten()\n        \n        # k_star_star = k(x_star, x_star): Prior variance at the test point\n        k_star_star = sigma_f**2\n\n        # 3. Compute posterior predictive mean and variance.\n        # Use Cholesky decomposition for stable and efficient linear system solving.\n        # K_y is symmetric and positive definite. L is its lower-triangular Cholesky factor.\n        try:\n            L = cholesky(K_y, lower=True)\n        except np.linalg.LinAlgError:\n            # This should not happen with a valid kernel and non-zero noise.\n            # Handle as an error if it occurs.\n            results.extend([np.nan, np.nan])\n            continue\n            \n        # Solve (K + sigma_n^2*I) * alpha = y for alpha.\n        # This is equivalent to alpha = inv(K + sigma_n^2*I) @ y.\n        alpha = cho_solve((L, True), y_train)\n        \n        # Calculate posterior mean: mu_star = k_star.T @ alpha\n        mu_star = k_star.T @ alpha\n        \n        # Solve (K + sigma_n^2*I) * v = k_star for v.\n        # This is equivalent to v = inv(K + sigma_n^2*I) @ k_star.\n        v = cho_solve((L, True), k_star)\n        \n        # Calculate posterior variance: sigma_star^2 = k_star_star - k_star.T @ v\n        var_star = k_star_star - k_star.T @ v\n        \n        # Ensure variance is non-negative due to potential floating point errors.\n        s_star = np.sqrt(max(0, var_star))\n        \n        # 4. Append rounded results to the list.\n        results.append(round(mu_star, 3))\n        results.append(round(s_star, 3))\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2441367"}, {"introduction": "虽然高斯过程等代理模型功能强大，但在高维空间中它们也面临挑战。这个练习 [@problem_id:2441366] 是一个计算实验，旨在让你对“维度灾难”有一个直观的感受。你将系统地测量，随着输入维度的增加，GP 模型为保持特定精度所需的数据量如何增长，从而揭示为什么高维问题在难度上是指数级增长的。", "problem": "要求您设计并实现一个完整的可运行程序，用以根据经验估算高斯过程（GP）代理模型所需的最小训练样本数量如何随输入维度增长。您的任务是，对于定义在单位超立方体上的一个指定平滑函数族，测量在固定的、维度匹配的测试集上达到目标均方根误差（RMSE）所需的最小训练集大小。程序必须遵循严格的规范，并生成单行输出，汇总所提供测试套件的结果。\n\n从以下基本概念开始：\n- 高斯过程（GP）是随机变量的集合，其任何有限子集都服从联合高斯分布。一个GP完全由其均值函数和协方差函数指定。\n- 平方指数协方差函数（也称为径向基函数核）是一种广泛使用的、编码平滑性信息的选择。对于任何维度，它都是平稳且正定的。\n- 多元正态分布的条件属性给出了高斯过程回归的闭式后验预测均值，该均值通过求解一个由核矩阵加上一个用于保证数值稳定性的噪声项所定义的线性系统得到。\n- 均方根误差（RMSE）是预测误差平方的算术平均值的平方根。\n\n问题设置：\n- 对于维度 $D \\in \\mathbb{N}$，目标函数 $f_D : [0,1]^D \\to \\mathbb{R}$ 定义为\n$$\nf_D(\\mathbf{x}) \\;=\\; \\frac{1}{\\sqrt{D}} \\sum_{i=1}^{D} \\sin\\!\\big(2\\pi x_i\\big),\n$$\n其中 $\\mathbf{x} = (x_1,\\dots,x_D)^\\top \\in [0,1]^D$。这种归一化可以产生大致不随维度变化的振幅，并保持函数在每个坐标上都是平滑和周期性的。\n- 使用零均值和平方指数协方差的高斯过程回归\n$$\nk(\\mathbf{x},\\mathbf{x}') \\;=\\; \\sigma_f^2 \\exp\\!\\left( -\\frac{1}{2} \\sum_{i=1}^{D} \\frac{(x_i - x_i')^2}{\\ell^2} \\right),\n$$\n使用固定的超参数 $\\sigma_f^2 = 1$ 和 $\\ell = 0.2$。在核矩阵的对角线上加入一个独立同方差的高斯噪声方差 $\\sigma_n^2 = 10^{-10}$，纯粹作为数值扰动项；将训练观测值视为 $f_D$ 的无噪声目标。\n- 使用低差异 Halton 序列在 $[0,1]^D$ 中生成点集以避免随机性。对于每个维度 $D$，训练输入必须是加扰种子为 $12345$ 的加扰 Halton 序列的前 $n$ 个点，测试输入必须是加扰种子为 $98765$ 的独立加扰 Halton 序列的前 $M$ 个点。两个序列都必须为每个 $D$ 重新生成，并且根据构造，训练序列和测试序列之间必须是独立的。设 $M = 512$。\n\n测量协议：\n- 对于每个测试用例 $(D, \\tau, n_{\\max})$，您必须找到最小整数 $n$（训练样本数），使得在 M 点固定测试集上，用前 $n$ 个 Halton 点训练的 GP 所达到的 RMSE 小于或等于阈值 $\\tau$：\n$$\n\\mathrm{RMSE}(n) \\;=\\; \\sqrt{\\frac{1}{M} \\sum_{j=1}^{M} \\big(\\hat{f}_D(\\mathbf{x}^{(\\mathrm{test})}_j) - f_D(\\mathbf{x}^{(\\mathrm{test})}_j)\\big)^2} \\;\\le\\; \\tau,\n$$\n其中 $\\hat{f}_D$ 是 GP 后验预测均值。\n- 搜索策略要求：为确保可复现性和覆盖性，您的程序必须使用以下两阶段搜索来寻找最小的 $n$。\n  1. 粗略几何增长以界定可行范围：对序列 $\\{4, 8, 16, 32, 64, 128, 256\\}$ 中截断至 $\\le n_{\\max}$ 的 $n$ 值，评估 $\\mathrm{RMSE}(n)$。如果没有值满足阈值，则报告该测试用例在预算内不可行，为此用例返回 $-1$。\n  2. 如果在某个 $n^\\star$ 处首次满足阈值，并且在之前的某个 $n^{-} < n^\\star$ 处未满足，则通过依次检查 $n = n^{-} + 1, n^{-} + 2, \\dots, n^\\star - 1$ 进行线性细化，以确定满足阈值的最小 $n$。如果首次在第一个粗略值处就满足阈值（之前没有失败），则根据定义，该值就是最小的 $n$。\n- 对于同一维度，在所有 $n$ 值上使用大小为 $M$ 的相同固定测试集。\n\n测试套件：\n- 您的程序必须按给定顺序评估以下六个测试用例，并为每个用例返回一个整数结果：\n  1. $(D=\\;1,\\; \\tau=\\;0.05,\\; n_{\\max}=\\;256)$\n  2. $(D=\\;2,\\; \\tau=\\;0.05,\\; n_{\\max}=\\;256)$\n  3. $(D=\\;4,\\; \\tau=\\;0.05,\\; n_{\\max}=\\;256)$\n  4. $(D=\\;8,\\; \\tau=\\;0.05,\\; n_{\\max}=\\;256)$\n  5. $(D=\\;8,\\; \\tau=\\;0.02,\\; n_{\\max}=\\;64)$\n  6. $(D=\\;1,\\; \\tau=\\;0.20,\\; n_{\\max}=\\;8)$\n- 这些用例包括一个渐进的维度扫描，以根据经验揭示维度灾难；一个带有小预算的有意设置的严格阈值，以测试不可行情况的处理；以及一个带有小预算的宽松阈值，以测试近边界可行情况。\n\n最终输出规范：\n- 您的程序必须生成一行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果，顺序与上述测试用例相同。例如，一个有效的输出格式是\n$[\\text{result}_1,\\text{result}_2,\\text{result}_3,\\text{result}_4,\\text{result}_5,\\text{result}_6]$\n其中每个 $\\text{result}_i$ 是一个整数，约定如果对于 $n \\le n_{\\max}$ 没有值满足 $\\mathrm{RMSE}(n) \\le \\tau$，则 $\\text{result}_i=-1$。\n\n不涉及物理单位。根据构造，正弦函数中隐含的所有角度都以弧度为单位。所有误差阈值都必须视为实数，而不是百分比。最终输出必须是严格按照指定格式的一行，不含任何额外文本。", "solution": "所提出的问题是代理模型领域中一个定义明确的计算实验，具体涉及高斯过程（GP）回归的应用。其目标是根据经验确定一个 GP 模型在测试函数上达到指定预测精度所需的最小训练样本数（表示为 $n$）。该研究将在不同的输入维度 $D$ 和精度阈值 $\\tau$ 下进行，从而在此背景下探索所谓的“维度灾难”。这个问题在科学上是合理的，在数学上是明确的，并且在计算上是可验证的。因此，我将着手提供一个完整的解决方案。\n\n该解决方案的方法论基于应用于函数近似的贝叶斯推断原理，这也是高斯过程回归的理论基础。GP 定义了函数上的一个先验分布。给定一组训练数据点，该先验会更新为一个拟合数据的函数后验分布。该后验分布的均值可作为对新的、未见过的点的函数值的预测。\n\n一个高斯过程完全由一个均值函数 $m(\\mathbf{x})$ 和一个协方差函数（或称核函数）$k(\\mathbf{x}, \\mathbf{x}')$ 指定。在本问题中，我们被要求使用零均值函数 $m(\\mathbf{x}) = 0$ 和平方指数协方差函数：\n$$\nk(\\mathbf{x},\\mathbf{x}') = \\sigma_f^2 \\exp\\!\\left( -\\frac{1}{2\\ell^2} \\sum_{i=1}^{D} (x_i - x_i')^2 \\right) = \\sigma_f^2 \\exp\\!\\left( -\\frac{\\|\\mathbf{x} - \\mathbf{x}'\\|^2_2}{2\\ell^2} \\right)\n$$\n超参数给定为信号方差 $\\sigma_f^2 = 1$ 和长度尺度 $\\ell = 0.2$。该核函数假设要建模的函数是无限可微且平稳的。\n\n设训练数据为一组 $n$ 个输入点 $\\mathbf{X} = \\{\\mathbf{x}_1, \\dots, \\mathbf{x}_n\\}$ 及其对应的函数值 $\\mathbf{y} = \\{f_D(\\mathbf{x}_1), \\dots, f_D(\\mathbf{x}_n)\\}^\\top$。目标函数指定为：\n$$\nf_D(\\mathbf{x}) = \\frac{1}{\\sqrt{D}} \\sum_{i=1}^{D} \\sin(2\\pi x_i)\n$$\n对于一组 $M$ 个测试点 $\\mathbf{X}_* = \\{\\mathbf{x}_1^*, \\dots, \\mathbf{x}_M^*\\}$，我们希望预测相应的函数值 $\\mathbf{f}_*$。根据 GP 理论，训练输出 $\\mathbf{y}$ 和测试输出 $\\mathbf{f}_*$ 的联合分布是一个多元高斯分布：\n$$\n\\begin{pmatrix} \\mathbf{y} \\\\ \\mathbf{f}_* \\end{pmatrix} \\sim \\mathcal{N} \\left( \\mathbf{0}, \\begin{pmatrix} \\mathbf{K}(\\mathbf{X}, \\mathbf{X}) + \\sigma_n^2 \\mathbf{I} & \\mathbf{K}(\\mathbf{X}, \\mathbf{X}_*) \\\\ \\mathbf{K}(\\mathbf{X}_*, \\mathbf{X}) & \\mathbf{K}(\\mathbf{X}_*, \\mathbf{X}_*) \\end{pmatrix} \\right)\n$$\n其中 $[\\mathbf{K}(\\mathbf{A}, \\mathbf{B})]_{ij} = k(\\mathbf{a}_i, \\mathbf{b}_j)$。项 $\\sigma_n^2 \\mathbf{I}$ 表示观测中的噪声。在这里，它以一个小的数值“扰动项”（nugget）$\\sigma_n^2 = 10^{-10}$ 的形式给出，以确保协方差矩阵的正定性和数值稳定性，尽管训练数据被认为是无噪声的。\n\n通过在观测到的训练数据上对联合分布进行条件化，我们得到 $\\mathbf{f}_*$ 的后验分布，它也是高斯分布。其均值即为我们的预测值 $\\hat{\\mathbf{f}}_*$，由下式给出：\n$$\n\\hat{\\mathbf{f}}_* = \\mathbf{K}(\\mathbf{X}_*, \\mathbf{X}) [\\mathbf{K}(\\mathbf{X}, \\mathbf{X}) + \\sigma_n^2 \\mathbf{I}]^{-1} \\mathbf{y}\n$$\n实现中将避免直接进行矩阵求逆，因为这种方法数值不稳定且计算效率低下。一种更稳健的方法是首先求解线性系统 $(\\mathbf{K}(\\mathbf{X}, \\mathbf{X}) + \\sigma_n^2 \\mathbf{I}) \\boldsymbol{\\alpha} = \\mathbf{y}$ 以得到向量 $\\boldsymbol{\\alpha}$。然后，预测值可计算为 $\\hat{\\mathbf{f}}_* = \\mathbf{K}(\\mathbf{X}_*, \\mathbf{X}) \\boldsymbol{\\alpha}$。由于矩阵 $\\mathbf{K}_{reg} = \\mathbf{K}(\\mathbf{X}, \\mathbf{X}) + \\sigma_n^2 \\mathbf{I}$ 是对称正定的，该系统可以使用 Cholesky 分解高效求解。我们计算 Cholesky 因子 $\\mathbf{L}$ 使得 $\\mathbf{L}\\mathbf{L}^\\top = \\mathbf{K}_{reg}$，然后通过前向和后向替换求解 $\\boldsymbol{\\alpha}$。\n\n问题要求使用特定的测量协议来找到最小的 $n$。对于每个测试用例 $(D, \\tau, n_{\\max})$，执行以下步骤：\n1.  **数据生成**：对于给定的维度 $D$，使用加扰 Halton 序列生成两个独立的、确定性的点集。\n    -   一个包含 $n_{\\max}$ 个点的训练池，位于 $[0,1]^D$ 中，使用加扰种子 $12345$。\n    -   一个包含 $M=512$ 个点的固定测试集，位于 $[0,1]^D$ 中，使用加扰种子 $98765$。\n    -   使用 $f_D(\\mathbf{x})$ 计算所有点的相应真实函数值。\n\n2.  **搜索最小 $n$**：执行两阶段搜索。\n    -   **粗略搜索**：对于一个几何增长的样本大小序列 $n \\in \\{4, 8, 16, 32, 64, 128, 256\\}$（限制为 $n \\le n_{\\max}$），训练并评估模型。对于每个 $n$，使用训练池中的前 $n$ 个点。计算均方根误差（RMSE）：\n    $$\n    \\mathrm{RMSE}(n) = \\sqrt{\\frac{1}{M} \\sum_{j=1}^{M} (\\hat{f}_D(\\mathbf{x}_j^*) - f_D(\\mathbf{x}_j^*))^2}\n    $$\n    如果 $\\mathrm{RMSE}(n) \\le \\tau$，我们便找到了最小 $n$ 的一个上界。此粗略搜索将持续进行，直到找到第一个满足条件的 $n$，记为 $n^\\star$。\n    -   **精细搜索**：如果粗略搜索在 $n^\\star$ 处成功，并且前一个粗略点 $n^- < n^\\star$ 未通过测试，则在 $[n^- + 1, n^\\star]$ 范围内对 $n$ 进行线性搜索。该范围内第一个满足 $\\mathrm{RMSE}(n) \\le \\tau$ 的 $n$ 值即为所需的最小样本量。如果测试的第一个粗略点已满足标准，则该点被宣布为最小值。\n    -   **不可行性**：如果在粗略搜索中，直到 $n_{\\max}$ 都没有 $n$ 满足 RMSE 阈值，则认为该情况不可行，结果报告为 $-1$。\n\n整个算法通过遍历每个提供的测试用例元组 $(D, \\tau, n_{\\max})$ 来进行，执行上述过程，并收集整数结果。最终输出是将这些结果汇总成一个格式化的字符串。实现将依赖于 `numpy`进行高效的数值计算，`scipy.stats.qmc.Halton`用于生成 Halton 序列，以及 `scipy.linalg.cho_solve`用于稳定的 GP 预测。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import qmc\nfrom scipy.linalg import cholesky, cho_solve\nfrom scipy.spatial.distance import cdist\n\ndef solve():\n    \"\"\"\n    Main function to execute the problem specification for all test cases.\n    \"\"\"\n\n    def target_function(x, D):\n        \"\"\"\n        Computes the target function f_D(x).\n        x is an (N, D) array.\n        Returns an (N,) array.\n        \"\"\"\n        if D == 0:\n            return np.zeros(x.shape[0])\n        return (1.0 / np.sqrt(D)) * np.sum(np.sin(2.0 * np.pi * x), axis=1)\n\n    def squared_exponential_kernel(X1, X2, D, sigma_f_sq=1.0, length_scale=0.2):\n        \"\"\"\n        Computes the squared exponential kernel matrix between two sets of points.\n        X1 is (N1, D), X2 is (N2, D).\n        Returns an (N1, N2) kernel matrix.\n        \"\"\"\n        # Using cdist is more stable and often faster than manual broadcasting for squared Euclidean distances\n        sqdist = cdist(X1 / length_scale, X2 / length_scale, 'sqeuclidean')\n        return sigma_f_sq * np.exp(-0.5 * sqdist)\n\n    def predict_gp(X_train, y_train, X_test, D, sigma_n_sq=1e-10):\n        \"\"\"\n        Computes the GP posterior predictive mean.\n        \"\"\"\n        n_train = X_train.shape[0]\n        if n_train == 0:\n            return np.zeros(X_test.shape[0])\n\n        K = squared_exponential_kernel(X_train, X_train, D)\n        K += np.eye(n_train) * sigma_n_sq\n\n        try:\n            # Use Cholesky decomposition for stability and efficiency\n            L = cholesky(K, lower=True)\n            alpha = cho_solve((L, True), y_train)\n        except np.linalg.LinAlgError:\n            # Fallback to direct solve if Cholesky fails, though unlikely with nugget\n            alpha = np.linalg.solve(K, y_train)\n\n        K_star = squared_exponential_kernel(X_test, X_train, D)\n        y_pred = K_star @ alpha\n        return y_pred\n\n    def calculate_rmse(y_pred, y_true):\n        \"\"\"\n        Calculates the Root Mean Squared Error.\n        \"\"\"\n        return np.sqrt(np.mean((y_pred - y_true)**2))\n\n    def find_minimal_n(D, tau, n_max):\n        \"\"\"\n        Finds the minimal number of training samples n to achieve RMSE <= tau.\n        \"\"\"\n        # Constants for the experiment\n        M = 512\n        training_seed = 12345\n        test_seed = 98765\n\n        # Generate fixed training pool and test set for the given dimension D\n        halton_train_gen = qmc.Halton(d=D, scramble=True, seed=training_seed)\n        X_train_pool = halton_train_gen.random(n=n_max)\n        y_train_pool = target_function(X_train_pool, D)\n\n        halton_test_gen = qmc.Halton(d=D, scramble=True, seed=test_seed)\n        X_test = halton_test_gen.random(n=M)\n        y_test = target_function(X_test, D)\n\n        coarse_n_values = [4, 8, 16, 32, 64, 128, 256]\n        \n        # Helper function to evaluate RMSE for a given n\n        def evaluate_rmse_at_n(n):\n            if n == 0:\n                return np.inf\n            X_train = X_train_pool[:n, :]\n            y_train = y_train_pool[:n]\n            y_pred = predict_gp(X_train, y_train, X_test, D)\n            return calculate_rmse(y_pred, y_test)\n\n        # 1. Coarse geometric search\n        n_minus = 0\n        n_star = -1\n\n        for n_coarse in coarse_n_values:\n            if n_coarse > n_max:\n                break\n            \n            rmse = evaluate_rmse_at_n(n_coarse)\n\n            if rmse <= tau:\n                n_star = n_coarse\n                break\n            else:\n                n_minus = n_coarse\n        \n        # If no coarse point satisfies the threshold\n        if n_star == -1:\n            return -1\n\n        # If the first coarse point is a success, it's the minimum\n        if n_minus == 0:\n            return n_star\n\n        # 2. Fine linear search\n        for n_fine in range(n_minus + 1, n_star + 1):\n            if n_fine > n_max:  # Should not happen with n_star <= n_max\n                break\n            rmse = evaluate_rmse_at_n(n_fine)\n            if rmse <= tau:\n                return n_fine\n        \n        # This part should be unreachable if logic is correct, but as a safeguard\n        return n_star\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (1, 0.05, 256),\n        (2, 0.05, 256),\n        (4, 0.05, 256),\n        (8, 0.05, 256),\n        (8, 0.02, 64),\n        (1, 0.20, 8),\n    ]\n\n    results = []\n    for D, tau, n_max in test_cases:\n        result = find_minimal_n(D, tau, n_max)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2441366"}, {"introduction": "高斯过程不仅仅是函数逼近器，它们还是强大的概率计算工具。这最后一个实践 [@problem_id:2441410] 将介绍贝叶斯积分（Bayesian Quadrature）的概念。你将学习如何使用一个 GP 模型来估计一个函数的定积分 $I(a,b) = \\int_a^b f(x)dx$，并且——至关重要的是——量化该估计中的不确定性。通过将 GP 的性质应用于积分这样的线性算子，你将打开通往更高级应用的大门。", "problem": "一个一维黑箱函数 $f:\\mathbb{R}\\to\\mathbb{R}$ 使用零均值高斯过程 (GP) 先验 $f \\sim \\mathcal{GP}\\!\\left(0,k\\right)$ 进行建模，其协方差核函数为平方指数核函数\n$$\nk(x,x') \\;=\\; \\sigma_f^2 \\exp\\!\\left(-\\dfrac{(x-x')^2}{2\\ell^2}\\right),\n$$\n其中信号标准差为 $\\sigma_f = 1.0$，长度尺度为 $\\ell = 0.8$。假设观测值受到独立高斯噪声的干扰，其方差为 $\\sigma_n^2$，其中 $\\sigma_n = 0.05$。在输入位置 $x_{\\text{train}} = \\big[ -2.0,\\,-1.2,\\,-0.5,\\,0.0,\\,0.6,\\,1.5,\\,2.3 \\big]$ 处提供了一个训练数据集，其对应的输出为 $y_i = f(x_i)$，其中黑箱函数定义为\n$$\nf(x) \\;=\\; \\exp\\!\\left(-\\dfrac{x^2}{2}\\right) \\;+\\; \\dfrac{1}{2}\\sin(3x).\n$$\n假设高斯过程回归模型由给定的核超参数 $\\sigma_f$、$\\ell$ 和噪声标准差 $\\sigma_n$完全指定；不要估计超参数。\n\n对于一个闭区间 $[a,b]$，令我们感兴趣的积分为\n$$\nI(a,b) \\;=\\; \\int_a^b f(x)\\,dx.\n$$\n仅使用由所提供的训练数据和超参数所隐含的已训练高斯过程后验，为每个测试区间计算 $I(a,b)$ 的后验均值和 $I(a,b)$ 的后验标准差。同时计算一个布尔指标，如果 $I(a,b)$ 的真实值位于对称的 $0.95$ 可信区间 $[\\mu_I - z\\sigma_I, \\mu_I + z\\sigma_I]$ (其中 $z = 1.96$) 之内，则该指标为 $\\text{True}$，否则为 $\\text{False}$。此处 $\\mu_I$ 和 $\\sigma_I$ 分别表示 $I(a,b)$ 的后验均值和后验标准差。\n\n测试套件：\n- 情况 1：$a=-1.5$, $b=1.5$。\n- 情况 2：$a=-0.2$, $b=0.8$。\n- 情况 3：$a=2.0$, $b=3.0$。\n- 情况 4：$a=-2.0$, $b=-1.5$。\n- 情况 5：$a=0.7$, $b=0.7$。\n\n你的程序必须使用上面定义的单个已训练模型，为所有测试情况生成数值答案。对于每种情况，按顺序输出三个量：$I(a,b)$ 的后验均值（四舍五入到 6 位小数），$I(a,b)$ 的后验标准差（四舍五入到 6 位小数），以及如上定义的布尔覆盖率指标。将 5 个测试情况的结果按情况顺序汇总成一个单一的扁平列表，得到一个长度为 15 的列表。最终输出必须只有一行，包含这个列表，形式为用逗号分隔的序列并用方括号括起来，例如 $[r_1,r_2,r_3,\\ldots,r_{15}]$，其中 $r_j$ 是按指定顺序排列的所需结果。不涉及物理单位，也没有出现角度。所有布尔输出必须是 $\\text{True}$ 或 $\\text{False}$（不带引号）。所有浮点输出必须精确四舍五入到 6 位小数。", "solution": "该问题具有科学依据、良定、客观且自洽。它描述了高斯过程 (GP) 回归在贝叶斯积分任务中的一个标准应用，即对积分进行概率估计。所有必要的数据、函数和参数都已提供。因此，该问题是有效的，并且可以构建一个解决方案。\n\n目标是计算积分 $I(a,b) = \\int_a^b f(x)\\,dx$ 的后验分布，其中函数 $f(x)$ 由高斯过程建模。鉴于积分是一个线性算子，且 $f(x)$ 服从高斯过程，积分 $I(a,b)$ 是一个高斯随机变量。我们的目标是找到它的后验均值 $\\mu_I$ 和后验标准差 $\\sigma_I$。\n\n高斯过程由一个均值函数（此处假设为零）和一个协方差（或核）函数 $k(x, x')$ 指定。在输入 $x$ 处的一个观测值 $y$ 的模型是 $y = f(x) + \\epsilon$，其中 $\\epsilon \\sim \\mathcal{N}(0, \\sigma_n^2)$ 是独立同分布的高斯噪声。\n\n给定一组大小为 $N$ 的训练数据点 $(\\mathbf{x}_{\\text{train}}, \\mathbf{y}_{\\text{train}})$，在测试点 $x_*$ 处的函数值 $f(x_*)$ 的后验分布是一个均值为 $\\mu_*(x_*)$、方差为 $\\sigma_*^2(x_*)$ 的高斯分布：\n$$\n\\mu_*(x_*) = \\mathbf{k}(x_*, \\mathbf{x}_{\\text{train}})^T (K + \\sigma_n^2 I_N)^{-1} \\mathbf{y}_{\\text{train}}\n$$\n$$\n\\sigma_*^2(x_*) = k(x_*, x_*) - \\mathbf{k}(x_*, \\mathbf{x}_{\\text{train}})^T (K + \\sigma_n^2 I_N)^{-1} \\mathbf{k}(x_*, \\mathbf{x}_{\\text{train}})\n$$\n其中 $K$ 是 $N \\times N$ 的格拉姆矩阵，其元素为 $K_{ij} = k(x_{\\text{train},i}, x_{\\text{train},j})$，$I_N$ 是单位矩阵，且 $\\mathbf{k}(x_*, \\mathbf{x}_{\\text{train}})$ 是测试点 $x_*$ 与所有训练点之间的协方差向量。为了数值稳定性，我们通过求解线性系统 $(K + \\sigma_n^2 I_N)\\boldsymbol{\\alpha} = \\mathbf{y}_{\\text{train}}$ 来预先计算向量 $\\boldsymbol{\\alpha} = (K + \\sigma_n^2 I_N)^{-1} \\mathbf{y}_{\\text{train}}$。\n\n1.  **积分的后验均值**\n\n积分的后验均值 $\\mu_I$ 是后验均值函数 $\\mu_*(x)$ 在区间 $[a,b]$ 上的积分。\n$$\n\\mu_I = E\\left[\\int_a^b f(x) dx \\bigg| \\mathbf{x}_{\\text{train}}, \\mathbf{y}_{\\text{train}}\\right] = \\int_a^b E[f(x) | \\ldots] dx = \\int_a^b \\mu_*(x) dx\n$$\n代入 $\\mu_*(x)$ 的表达式并交换积分和矩阵乘法的顺序：\n$$\n\\mu_I = \\int_a^b \\mathbf{k}(x, \\mathbf{x}_{\\text{train}})^T \\boldsymbol{\\alpha} \\, dx = \\left(\\int_a^b \\mathbf{k}(x, \\mathbf{x}_{\\text{train}}) dx\\right)^T \\boldsymbol{\\alpha}\n$$\n令 $\\mathbf{k}_I(a,b) = \\int_a^b \\mathbf{k}(x, \\mathbf{x}_{\\text{train}}) dx$。该向量的第 $i$ 个元素需要对平方指数核进行积分：\n$$\n(\\mathbf{k}_I)_i = \\int_a^b \\sigma_f^2 \\exp\\left(-\\frac{(x-x_{\\text{train},i})^2}{2\\ell^2}\\right) dx\n$$\n这个积分有解析解，可用误差函数 $\\text{erf}(z) = \\frac{2}{\\sqrt{\\pi}}\\int_0^z e^{-t^2} dt$ 表示：\n$$\n(\\mathbf{k}_I)_i = \\sigma_f^2 \\ell \\sqrt{\\frac{\\pi}{2}} \\left[ \\text{erf}\\left(\\frac{b-x_{\\text{train},i}}{\\sqrt{2}\\ell}\\right) - \\text{erf}\\left(\\frac{a-x_{\\text{train},i}}{\\sqrt{2}\\ell}\\right) \\right]\n$$\n在计算出 $\\mathbf{k}_I(a,b)$ 后，后验均值为 $\\mu_I = \\mathbf{k}_I(a,b)^T \\boldsymbol{\\alpha}$。\n\n2.  **积分的后验方差**\n\n积分的后验方差 $\\sigma_I^2$ 是后验协方差函数 $\\text{cov}(f(x), f(x') | \\ldots)$ 在区间 $[a,b]$ 上的二重积分。\n$$\n\\sigma_I^2 = \\text{Var}\\left[\\int_a^b f(x) dx \\bigg| \\ldots\\right] = \\int_a^b \\int_a^b \\text{cov}(f(x), f(x') | \\ldots) dx' dx\n$$\n后验协方差由 $\\Sigma_*(x, x') = k(x, x') - \\mathbf{k}(x, \\mathbf{x}_{\\text{train}})^T (K + \\sigma_n^2 I_N)^{-1} \\mathbf{k}(x', \\mathbf{x}_{\\text{train}})$ 给出。对该表达式进行积分可得：\n$$\n\\sigma_I^2 = \\int_a^b \\int_a^b k(x, x') dx' dx - \\left(\\int_a^b \\mathbf{k}(x, \\mathbf{x}_{\\text{train}}) dx\\right)^T (K + \\sigma_n^2 I_N)^{-1} \\left(\\int_a^b \\mathbf{k}(x', \\mathbf{x}_{\\text{train}}) dx'\\right)\n$$\n这可以简化为：\n$$\n\\sigma_I^2 = K_{II} - \\mathbf{k}_I(a,b)^T (K + \\sigma_n^2 I_N)^{-1} \\mathbf{k}_I(a,b)\n$$\n其中 $K_{II} = \\int_a^b \\int_a^b k(x, x') dx' dx$。这个平方指数核的二重积分也有一个闭式解。对于 $\\delta = b-a$：\n$$\nK_{II} = 2 \\sigma_f^2 \\ell^2 \\left( \\exp\\left(-\\frac{\\delta^2}{2\\ell^2}\\right) - 1 \\right) + \\sigma_f^2 \\sqrt{2\\pi}\\ell \\delta \\cdot \\text{erf}\\left(\\frac{\\delta}{\\sqrt{2}\\ell}\\right)\n$$\n那么后验标准差为 $\\sigma_I = \\sqrt{\\sigma_I^2}$。\n\n3.  **真实积分与可信区间**\n\n为了验证模型，我们计算给定函数 $f(x) = \\exp(-x^2/2) + \\frac{1}{2}\\sin(3x)$ 的积分真实值 $I_{\\text{true}}(a,b)$。该积分为：\n$$\nI_{\\text{true}}(a,b) = \\int_a^b \\left( e^{-x^2/2} + \\frac{1}{2}\\sin(3x) \\right) dx = \\left[\\sqrt{2\\pi} \\Phi(x) - \\frac{1}{6}\\cos(3x)\\right]_a^b\n$$\n其中 $\\Phi(x)$ 是标准正态分布的累积分布函数 (CDF)。其计算公式为：\n$$\nI_{\\text{true}}(a,b) = \\sqrt{2\\pi}(\\Phi(b) - \\Phi(a)) - \\frac{1}{6}(\\cos(3b) - \\cos(3a))\n$$\n问题要求检查该真实值是否位于对称的 $95\\%$ 可信区间内，该区间定义为 $[\\mu_I - z\\,\\sigma_I, \\mu_I + z\\,\\sigma_I]$，其中 $z=1.96$。为此条件计算一个布尔指标。\n\n求解过程首先从真实函数 $f(x)$ 计算训练输出 $\\mathbf{y}_{\\text{train}}$。然后，构造矩阵 $(K + \\sigma_n^2 I_N)$ 并使用其 Cholesky 分解来高效地求解 $\\boldsymbol{\\alpha}$ 和计算二次型。对于每个测试区间，计算 $\\mathbf{k}_I$、$K_{II}$、$\\mu_I$ 和 $\\sigma_I^2$，接着计算 $I_{\\text{true}}$ 并进行最终的覆盖率检查。", "answer": "```python\nimport numpy as np\nfrom scipy.linalg import cho_factor, cho_solve\nfrom scipy.special import erf\nfrom scipy.stats import norm\n\ndef solve():\n    \"\"\"\n    Computes the posterior mean and standard deviation of the integral of a function\n    modeled by a Gaussian Process, and verifies if the true integral value lies\n    within the 95% credible interval.\n    \"\"\"\n    # 1. Define hyperparameters, training data, and model parameters\n    sigma_f = 1.0\n    ell = 0.8\n    sigma_n = 0.05\n    x_train = np.array([-2.0, -1.2, -0.5, 0.0, 0.6, 1.5, 2.3])\n    N = x_train.shape[0]\n\n    # 2. Define true function and generate training outputs\n    def f_true(x):\n        return np.exp(-x**2 / 2.0) + 0.5 * np.sin(3 * x)\n    y_train = f_true(x_train)\n\n    # 3. Define the squared-exponential kernel function\n    def kernel(x1, x2, sigma_f_sq, ell_sq_2):\n        # Using broadcasting to compute squared Euclidean distances efficiently\n        sqdist = np.sum(x1**2, 1).reshape(-1, 1) + np.sum(x2**2, 1) - 2 * np.dot(x1, x2.T)\n        return sigma_f_sq * np.exp(-sqdist / ell_sq_2)\n\n    # 4. Perform GP pre-computation (once for the trained model)\n    sigma_f_sq = sigma_f**2\n    ell_sq = ell**2\n    \n    # Reshape x_train into a column vector for the kernel function\n    x_train_col = x_train.reshape(-1, 1)\n    \n    K = kernel(x_train_col, x_train_col, sigma_f_sq, 2 * ell_sq)\n    Ky = K + (sigma_n**2) * np.eye(N)\n    \n    # Use Cholesky decomposition for stable and efficient linear solves\n    L, lower = cho_factor(Ky)\n    alpha = cho_solve((L, lower), y_train)\n\n    # 5. Define test cases\n    test_cases = [\n        (-1.5, 1.5),\n        (-0.2, 0.8),\n        (2.0, 3.0),\n        (-2.0, -1.5),\n        (0.7, 0.7)\n    ]\n    \n    results = []\n    \n    # 6. Process each test case\n    for a, b in test_cases:\n        # a) Compute the vector of integrated kernels, k_I\n        arg_b = (b - x_train) / (np.sqrt(2) * ell)\n        arg_a = (a - x_train) / (np.sqrt(2) * ell)\n        k_I = sigma_f_sq * ell * np.sqrt(np.pi / 2.0) * (erf(arg_b) - erf(arg_a))\n        \n        # b) Compute posterior mean of the integral, mu_I\n        mu_I = np.dot(k_I, alpha)\n        \n        # c) Compute posterior variance of the integral, sigma_I^2\n        if a == b:\n            K_II = 0.0\n        else:\n            delta = b - a\n            K_II = (sigma_f_sq * 2 * ell_sq * (np.exp(-delta**2 / (2 * ell_sq)) - 1.0) +\n                    sigma_f_sq * np.sqrt(2 * np.pi) * ell * delta * erf(delta / (np.sqrt(2) * ell)))\n            \n        var_reduction = np.dot(k_I, cho_solve((L, lower), k_I))\n        sigma_I_sq = K_II - var_reduction\n        \n        # Clamp to zero to handle potential small negative values from numerical error\n        sigma_I = np.sqrt(max(0, sigma_I_sq))\n        \n        # d) Compute the true value of the integral, I_true\n        if a == b:\n            I_true = 0.0\n        else:\n            term1 = np.sqrt(2 * np.pi) * (norm.cdf(b) - norm.cdf(a))\n            term2 = (-1/6.0) * (np.cos(3 * b) - np.cos(3 * a))\n            I_true = term1 + term2\n            \n        # e) Perform the coverage check\n        z_score = 1.96\n        lower_bound = mu_I - z_score * sigma_I\n        upper_bound = mu_I + z_score * sigma_I\n        is_covered = lower_bound <= I_true <= upper_bound\n        \n        # f) Store results, rounded as required\n        results.extend([round(mu_I, 6), round(sigma_I, 6), is_covered])\n\n    # 7. Format and print the final output\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```", "id": "2441410"}]}