{"hands_on_practices": [{"introduction": "序列优化方法的核心思想是将一个复杂的约束问题转化为一系列较简单的无约束子问题。这个练习将指导你实现一个经典的二次惩罚方法，并探索一个能够显著提升求解效率的关键实用技巧。通过对比“冷启动”与“热启动”策略，你将亲手量化将前一个子问题的解 $x_k^*$ 作为下一个子问题初始猜测点（即“热启动”）所带来的巨大计算优势。[@problem_id:2423453]", "problem": "要求您实现一个用于约束优化的序列二次罚函数法，并通过实验量化热启动（通过复用前一个子问题的解作为下一个罚参数的初始猜测值）带来的益处。实现一个带有回溯 Armijo 线搜索的基于梯度的求解器，以最小化一系列无约束的罚子问题。对于一个固定的罚参数序列 $\\{\\rho_k\\}_{k=1}^K$（其中 $\\rho_1 < \\rho_2 < \\dots < \\rho_K$），请对每个测试用例比较两种策略：(i) 从相同的初始点冷启动每个子问题，以及 (ii) 从已计算出的子问题 $k$ 的最小化器开始热启动子问题 $k+1$。报告加速比，其定义为在整个罚参数序列上，冷启动所用的梯度下降总迭代次数与热启动所用的总迭代次数之比。\n\n使用的基本原理和定义：\n- 一个约束最小化问题具有目标函数 $f:\\mathbb{R}^n\\to\\mathbb{R}$，不等式约束 $g_i(x)\\le 0$（其中 $i\\in\\{1,\\dots,m\\}$），以及等式约束 $h_j(x)=0$（其中 $j\\in\\{1,\\dots,p\\}$）。\n- 经典二次罚函数对不等式约束的惩罚应用于其违反量，形式为 $\\max\\{0, g_i(x)\\}^2$；对等式约束的惩罚形式为 $h_j(x)^2$。\n- 对于给定的 $\\rho>0$，罚子问题是最小化\n$$\n\\Phi_\\rho(x)=f(x)+\\rho\\left(\\sum_{i=1}^m \\max\\{0,g_i(x)\\}^2+\\sum_{j=1}^p h_j(x)^2\\right).\n$$\n- 使用带有回溯 Armijo 法则的梯度下降法：给定当前点 $x$、梯度 $\\nabla\\Phi_\\rho(x)$、初始步长 $t_0$、缩减因子 $\\beta\\in(0,1)$ 和 Armijo 参数 $c\\in(0,1)$，从序列 $\\{t_0, \\beta t_0, \\beta^2 t_0,\\dots\\}$ 中选择满足以下条件的最大 $t$：\n$$\n\\Phi_\\rho(x - t \\nabla \\Phi_\\rho(x)) \\le \\Phi_\\rho(x) - c\\,t\\,\\|\\nabla \\Phi_\\rho(x)\\|_2^2.\n$$\n- 当 $\\|\\nabla \\Phi_\\rho(x)\\|_2\\le \\varepsilon$ 时，停止内部求解器。\n\n实现要求：\n- 完全按照上述定义实现二次罚函数法和带有回溯 Armijo 线搜索的梯度下降法。\n- 对于不等式约束，仅通过在罚函数值及其梯度中使用 $\\max\\{0,\\cdot\\}$ 结构来处理正向违反量。对于等式约束，惩罚其残差的平方。\n- 使用罚参数序列 $\\rho\\in\\{10,10^2,10^3\\}$，即 $\\rho \\in \\{10,100,1000\\}$。\n- 对于所有子问题，使用梯度容差 $\\varepsilon=10^{-6}$、Armijo 参数 $c=10^{-4}$、缩减因子 $\\beta=\\tfrac{1}{2}$ 和初始步长 $t_0=1$。将每个子问题的最大梯度迭代次数限制在 $N_{\\max}=10^4$。\n- 计算使一个子问题收敛所需的外部梯度下降迭代次数（每次线搜索后接受的步数）；不要单独计算线搜索的回溯步数。\n\n测试套件：\n实现并求解以下三个二维测试用例。在每个用例中，返回加速比\n$$\nS=\\frac{N_{\\mathrm{cold}}}{N_{\\mathrm{warm}}},\n$$\n其中 $N_{\\mathrm{cold}}$ 是从指定的初始点冷启动每个子问题时，在所有罚参数上累加的梯度下降总迭代次数；$N_{\\mathrm{warm}}$ 是从前一个子问题的解热启动每个子问题时的总迭代次数。\n\n- 用例 $\\mathbf{A}$（带有一个紧致线性不等式的凸二次问题）：\n  - 目标函数：$f(x,y)=(x-1)^2+2\\,(y+2)^2$。\n  - 不等式约束：$g_1(x,y)=1-x-y\\le 0$。\n  - 无等式约束。\n  - 初始点：$x_0=(0,0)$。\n\n- 用例 $\\mathbf{B}$（带有一个等式约束的凸二次问题）：\n  - 目标函数：$f(x,y)=(x-3)^2+(y-1)^2$。\n  - 等式约束：$h_1(x,y)=x-y=0$。\n  - 无不等式约束。\n  - 初始点：$x_0=(0,0)$。\n\n- 用例 $\\mathbf{C}$（带有一个曲线不等式约束的凸二次问题）：\n  - 目标函数：$f(x,y)=(x+2)^2+y^2$。\n  - 不等式约束：$g_1(x,y)=x^2+y^2-1\\le 0$。\n  - 无等式约束。\n  - 初始点：$x_0=(0,0)$。\n\n输出规范：\n- 对于每个用例，计算如上定义的加速比 $S$。\n- 您的程序应生成单行输出，其中包含三个加速比，格式为用方括号括起来的逗号分隔列表，顺序为 $\\left[S_A,S_B,S_C\\right]$，其中 $S_A$ 对应于用例 $\\mathbf{A}$，$S_B$ 对应于用例 $\\mathbf{B}$，$S_C$ 对应于用例 $\\mathbf{C}$。例如，输出格式为 $\\left[\\text{结果}_1,\\text{结果}_2,\\text{结果}_3\\right]$，其中包含数值。\n- 将每个加速比表示为浮点数。您可以在内部进行四舍五入，但打印的值必须是标准的十进制浮点数。\n\n不涉及物理单位。不使用角度。不使用百分比。\n\n最终的程序必须是自包含的，不需要任何输入，并遵守指定的运行时环境。正确性将通过验证实现是否遵循定义，以及热启动产生的迭代次数是否严格更少或至少不多于冷启动，从而为指定用例产生有意义的加速比来评估。输出必须严格为指定格式的一行。", "solution": "该问题要求实现一个序列二次罚函数法来求解约束优化问题。任务的核心是比较用于一系列无约束子问题的两种初始化策略的计算效率：冷启动策略与热启动策略。效率将通过加速比来量化，该加速比定义为梯度下降总迭代次数的比率。\n\n约束优化问题的一般形式是最小化目标函数 $f(x)$，其受限于一组不等式约束 $g_i(x) \\le 0$（其中 $i \\in \\{1, \\dots, m\\}$）和等式约束 $h_j(x) = 0$（其中 $j \\in \\{1, \\dots, p\\}$），其中 $x \\in \\mathbb{R}^n$。\n\n二次罚函数法通过求解一系列无约束最小化问题来逼近该问题的解。对于给定的罚参数 $\\rho > 0$，通过向原始目标函数中添加惩罚违反约束的项来构造罚目标函数 $\\Phi_\\rho(x)$。罚函数的具体形式是：\n$$\n\\Phi_\\rho(x) = f(x) + \\rho \\left( \\sum_{i=1}^m \\left(\\max\\{0, g_i(x)\\}\\right)^2 + \\sum_{j=1}^p \\left(h_j(x)\\right)^2 \\right)\n$$\n然后，相对于 $x$ 最小化该函数 $\\Phi_\\rho(x)$。通过对一个递增的罚参数序列 $\\rho_1 < \\rho_2 < \\dots < \\rho_K$ 求解此无约束问题，最小化器序列 $x^*(\\rho_k)$ 会收敛到原始约束问题的解。\n\n为了最小化每个无约束子问题 $\\min_x \\Phi_\\rho(x)$，需要一个基于梯度的方法。罚目标函数的梯度 $\\nabla \\Phi_\\rho(x)$ 是使用链式法则推导出来的。对于不等式约束项 $P_i(x) = \\rho (\\max\\{0, g_i(x)\\})^2$，其梯度为 $\\nabla P_i(x) = 2 \\rho \\max\\{0, g_i(x)\\} \\nabla g_i(x)$。对于等式约束项 $Q_j(x) = \\rho (h_j(x))^2$，其梯度为 $\\nabla Q_j(x) = 2 \\rho h_j(x) \\nabla h_j(x)$。将这些与目标函数的梯度相结合，完整的梯度是：\n$$\n\\nabla \\Phi_\\rho(x) = \\nabla f(x) + 2\\rho \\left( \\sum_{i=1}^m \\max\\{0, g_i(x)\\} \\nabla g_i(x) + \\sum_{j=1}^p h_j(x) \\nabla h_j(x) \\right)\n$$\n无约束最小化使用梯度下降法执行。从一个点 $x_k$ 开始，通过沿负梯度方向移动来找到下一个点 $x_{k+1}$：\n$$\nx_{k+1} = x_k - t \\nabla \\Phi_\\rho(x_k)\n$$\n步长 $t > 0$ 是通过采用 Armijo 条件的回溯线搜索确定的。对于给定的下降方向 $d_k = -\\nabla \\Phi_\\rho(x_k)$，我们从序列 $\\{t_0, \\beta t_0, \\beta^2 t_0, \\dots\\}$ 中寻找满足以下条件的最大 $t$：\n$$\n\\Phi_\\rho(x_k + t d_k) \\le \\Phi_\\rho(x_k) + c \\, t \\, \\nabla \\Phi_\\rho(x_k)^T d_k\n$$\n使用 $d_k = -\\nabla \\Phi_\\rho(x_k)$，这可以简化为问题陈述中给出的形式：\n$$\n\\Phi_\\rho(x_k - t \\nabla \\Phi_\\rho(x_k)) \\le \\Phi_\\rho(x_k) - c \\, t \\, \\|\\nabla \\Phi_\\rho(x_k)\\|_2^2\n$$\n算法迭代进行，直到梯度的范数低于指定的容差 $\\varepsilon$，即 $\\|\\nabla \\Phi_\\rho(x)\\|_2 \\le \\varepsilon$。此求解器的参数是固定的：初始步长 $t_0=1$，Armijo 参数 $c=10^{-4}$，缩减因子 $\\beta=0.5$，以及梯度范数容差 $\\varepsilon=10^{-6}$。每个子问题的最大迭代次数上限为 $N_{\\max}=10^4$。\n\n该实验在罚参数序列 $\\rho \\in \\{10, 100, 1000\\}$ 上比较两种策略：\n1.  **冷启动 (Cold-Start):** 每个针对 $\\rho_k$ 的子问题都从相同的起始点 $x_0$ 初始化。总迭代次数 $N_{\\mathrm{cold}}$ 是独立求解每个子问题所需迭代次数的总和。\n2.  **热启动 (Warm-Start):** 第一个子问题（针对 $\\rho_1=10$）从 $x_0$ 初始化。后续每个针对 $\\rho_{k+1}$ 的子问题都使用从前一个针对 $\\rho_k$ 的子问题获得的解进行初始化。总迭代次数 $N_{\\mathrm{warm}}$ 是此序列中所有迭代次数的总和。\n\n热启动的基本原理是，解 $x^*(\\rho_k)$ 有望成为 $\\Phi_{\\rho_{k+1}}(x)$ 的最小化器的一个良好初始猜测，特别是当 $\\rho_{k+1}$ 不比 $\\rho_k$ 大很多时。这应该会带来更快的收敛速度。性能增益通过加速比 $S = N_{\\mathrm{cold}} / N_{\\mathrm{warm}}$ 来衡量。\n\n实现过程将首先为每个测试用例的目标函数、约束函数及其各自的梯度定义 Python 函数。一个通用的求解器函数将执行带有 Armijo 线搜索的梯度下降。一个顶层函数将管理罚参数序列，应用冷启动和热启动策略，计算每种策略的总迭代次数，并计算加速比。对所提供的所有三个测试用例都将重复此过程。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the specified test cases and print the results.\n    \"\"\"\n    \n    # --- Solver Parameters ---\n    SOLVER_PARAMS = {\n        'epsilon': 1e-6,\n        'c_armijo': 1e-4,\n        'beta': 0.5,\n        't0': 1.0,\n        'n_max': 10000\n    }\n    PENALTY_PARAMS = [10.0, 100.0, 1000.0]\n\n    # --- Test Case Definitions ---\n    \n    # Case A: (x-1)^2 + 2(y+2)^2, s.t. 1-x-y <= 0\n    case_A = {\n        'f': lambda x: (x[0] - 1.0)**2 + 2.0 * (x[1] + 2.0)**2,\n        'grad_f': lambda x: np.array([2.0 * (x[0] - 1.0), 4.0 * (x[1] + 2.0)]),\n        'g': [lambda x: 1.0 - x[0] - x[1]],\n        'grad_g': [lambda x: np.array([-1.0, -1.0])],\n        'h': [],\n        'grad_h': [],\n        'x0': np.array([0.0, 0.0])\n    }\n\n    # Case B: (x-3)^2 + (y-1)^2, s.t. x-y = 0\n    case_B = {\n        'f': lambda x: (x[0] - 3.0)**2 + (x[1] - 1.0)**2,\n        'grad_f': lambda x: np.array([2.0 * (x[0] - 3.0), 2.0 * (x[1] - 1.0)]),\n        'g': [],\n        'grad_g': [],\n        'h': [lambda x: x[0] - x[1]],\n        'grad_h': [lambda x: np.array([1.0, -1.0])],\n        'x0': np.array([0.0, 0.0])\n    }\n    \n    # Case C: (x+2)^2 + y^2, s.t. x^2+y^2-1 <= 0\n    case_C = {\n        'f': lambda x: (x[0] + 2.0)**2 + x[1]**2,\n        'grad_f': lambda x: np.array([2.0 * (x[0] + 2.0), 2.0 * x[1]]),\n        'g': [lambda x: x[0]**2 + x[1]**2 - 1.0],\n        'grad_g': [lambda x: np.array([2.0 * x[0], 2.0 * x[1]])],\n        'h': [],\n        'grad_h': [],\n        'x0': np.array([0.0, 0.0])\n    }\n\n    test_cases = [case_A, case_B, case_C]\n    \n    def get_penalized_funcs(case, rho):\n        \"\"\"Creates the penalized function and its gradient for a given case and rho.\"\"\"\n        \n        def phi(x):\n            f_val = case['f'](x)\n            g_sum = sum(max(0, g_func(x))**2 for g_func in case['g'])\n            h_sum = sum(h_func(x)**2 for h_func in case['h'])\n            return f_val + rho * (g_sum + h_sum)\n\n        def grad_phi(x):\n            grad_f_val = case['grad_f'](x)\n            \n            grad_g_sum = np.zeros_like(x)\n            for g_func, grad_g_func in zip(case['g'], case['grad_g']):\n                g_val = g_func(x)\n                if g_val > 0:\n                    grad_g_sum += 2.0 * g_val * grad_g_func(x)\n\n            grad_h_sum = np.zeros_like(x)\n            for h_func, grad_h_func in zip(case['h'], case['grad_h']):\n                h_val = h_func(x)\n                grad_h_sum += 2.0 * h_val * grad_h_func(x)\n                \n            return grad_f_val + rho * (grad_g_sum + grad_h_sum)\n        \n        return phi, grad_phi\n\n    def gradient_descent(phi, grad_phi, x_init, params):\n        \"\"\"\n        Performs gradient descent with backtracking Armijo line search.\n        \"\"\"\n        x = np.copy(x_init)\n        n_iters = 0\n        \n        for k in range(params['n_max']):\n            grad = grad_phi(x)\n            grad_norm_sq = np.dot(grad, grad)\n\n            if np.sqrt(grad_norm_sq) <= params['epsilon']:\n                break\n            \n            # Backtracking line search\n            t = params['t0']\n            phi_x = phi(x)\n            \n            while True:\n                x_new = x - t * grad\n                phi_new = phi(x_new)\n                armijo_check = phi_x - params['c_armijo'] * t * grad_norm_sq\n                \n                if phi_new <= armijo_check:\n                    break\n                t *= params['beta']\n            \n            x = x_new\n            n_iters += 1\n        \n        return x, n_iters\n\n    def run_penalty_method(case, solver_params, penalty_params):\n        \"\"\"\n        Runs the full sequential penalty method for a case,\n        calculating iterations for both cold and warm starts.\n        \"\"\"\n        # Cold start\n        total_iters_cold = 0\n        for rho in penalty_params:\n            phi, grad_phi = get_penalized_funcs(case, rho)\n            _, n_iters = gradient_descent(phi, grad_phi, case['x0'], solver_params)\n            total_iters_cold += n_iters\n            \n        # Warm start\n        total_iters_warm = 0\n        x_warm = np.copy(case['x0'])\n        for rho in penalty_params:\n            phi, grad_phi = get_penalized_funcs(case, rho)\n            x_sol, n_iters = gradient_descent(phi, grad_phi, x_warm, solver_params)\n            total_iters_warm += n_iters\n            x_warm = x_sol\n            \n        if total_iters_warm == 0:\n             # This case should not happen in this problem, but is a safeguard.\n             # If cold is also 0, speedup is 1. If cold > 0, speedup is \"infinite\".\n            return 1.0 if total_iters_cold == 0 else float('inf')\n            \n        return float(total_iters_cold) / float(total_iters_warm)\n\n    results = []\n    for case in test_cases:\n        speedup = run_penalty_method(case, SOLVER_PARAMS, PENALTY_PARAMS)\n        results.append(speedup)\n        \n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2423453"}, {"introduction": "在掌握了惩罚法的基本实现后，我们转向障碍法，并从纯粹的数值计算深入到分析层面。这个练习旨在揭示障碍函数法的内在机理，即解的路径如何随着参数 $\\rho$ 的变化而演进。通过推导一个简约问题的闭式解，你将能精确地观察到，当障碍参数变化时，最优解如何被“推”向可行域的边界，从而使原本不活跃的约束逐渐被激活。[@problem_id:2423438]", "problem": "给你一个标量约束优化问题，旨在说明在一个障碍函数法框架内，随着一个可调参数的顺序增加，约束如何从非激活（严格满足）过渡到激活（近似约束）。考虑一个凸优化问题，即在一个单一不等式约束下最小化一个平滑的目标函数。设目标函数为 $f(x) = (x - 2)^2$，不等式约束为 $c(x) = x - 1 \\le 0$（等价于 $x \\le 1$）。为了通过对数障碍函数来强制执行该约束，考虑增广了障碍函数的目标函数\n$$\n\\psi_{\\rho}(x) = \\rho\\, f(x) - \\log(1 - x),\n$$\n该函数定义在开域 $x < 1$ 上，其中 $\\rho > 0$ 是一个可调的权重参数。障碍项 $-\\log(1 - x)$ 编码了约束 $x \\le 1$，并确保迭代点保持严格可行。随着 $\\rho$ 的顺序增加，障碍项相对于 $f(x)$ 的影响减小，$\\psi_{\\rho}$ 的最小化子会逼近约束在近似相等意义上变为激活的边界。\n\n从基本原理（开域 $x < 1$ 上无约束最小化的一阶最优性条件和凸性性质）出发，推导出 $\\rho > 0$ 时唯一最小化子 $x(\\rho)$ 的闭式解，并用它来检测约束何时在近似意义上是“激活”的。定义松弛量 $s(\\rho) = 1 - x(\\rho)$。对于给定的容差 $\\tau > 0$，如果 $s(\\rho) \\le \\tau$，则称约束是激活的；如果 $s(\\rho) > \\tau$，则称约束是非激活的。\n\n你的任务是编写一个完整的、可运行的程序，该程序：\n- 使用推导出的 $x(\\rho)$ 的闭式解，为测试套件中 $\\rho$ 的每个测试值计算 $s(\\rho)$。\n- 将 $s(\\rho)$ 与固定的容差 $\\tau$ 进行比较，以产生一个激活指示符：如果约束是激活的，则输出 $1$，否则输出 $0$。\n\n你可以使用的基本原理：凸平滑最小化的一阶必要和充分条件（在开域上，梯度为零表征了最小化子）、不等式约束 $x \\le 1$ 的对数障碍函数的定义，以及求解所得最优性方程的基本代数。\n\n测试套件：\n- 障碍权重 $\\rho \\in \\{0.1, 1, 10, 50, 100\\}$。\n- 容差 $\\tau = 0.01$。\n\n最终输出格式：\n- 你的程序应生成单行输出，其中包含测试套件的激活结果，形式为方括号内以逗号分隔的整数列表，顺序与给定的 $\\rho$ 值相同。例如，输出可能看起来像 $[0,1,1,1,1]$，但应为本问题的正确结果。\n\n此问题不涉及物理单位。不使用角度。不使用百分比；与容差 $\\tau$ 比较时，所有值都是纯实数。每个测试用例的答案必须是 $\\{0,1\\}$ 中的整数，并且程序必须按规定将这些整数聚合到单个列表中。", "solution": "所提出的问题是约束优化中一个适定的练习，具体说明了对数障碍函数法的机理。其组成部分在数学上是精确的，在科学上以凸优化原理为基础，并构成了一个完整且一致的设定。因此，该问题被认为是有效的。我们将继续进行其形式化求解。\n\n任务是为给定的参数 $\\rho > 0$ 找到增广了障碍函数的目标函数 $\\psi_{\\rho}(x)$ 的唯一最小化子，其中\n$$\n\\psi_{\\rho}(x) = \\rho f(x) - \\log(1 - x)\n$$\n目标函数为 $f(x) = (x - 2)^2$。定义域是开区间 $(-\\infty, 1)$，这强制严格满足约束 $x - 1 < 0$。\n\n函数 $\\psi_{\\rho}(x)$ 是两个函数之和：$\\rho(x-2)^2$ 和 $-\\log(1-x)$。第一项，由于 $\\rho > 0$，是一个具有正主系数的缩放二次函数，因此是一个凸函数。第二项，即对数障碍函数，在其定义域 $x < 1$ 上也是凸的。凸函数之和是凸的。为了证明严格凸性，我们考察二阶导数。\n\n$\\psi_{\\rho}(x)$ 关于 $x$ 的一阶导数是：\n$$\n\\frac{d\\psi_{\\rho}}{dx} = \\frac{d}{dx} \\left[ \\rho (x - 2)^2 - \\log(1 - x) \\right] = 2\\rho(x - 2) - \\frac{-1}{1 - x} = 2\\rho(x - 2) + \\frac{1}{1 - x}.\n$$\n二阶导数是：\n$$\n\\frac{d^2\\psi_{\\rho}}{dx^2} = \\frac{d}{dx} \\left[ 2\\rho(x - 2) + (1 - x)^{-1} \\right] = 2\\rho + (-1)(1 - x)^{-2}(-1) = 2\\rho + \\frac{1}{(1 - x)^2}.\n$$\n对于任何 $\\rho > 0$ 和定义域 $x < 1$ 内的任何 $x$，两项 $2\\rho$ 和 $\\frac{1}{(1 - x)^2}$ 都是严格为正的。因此，对于所有 $x < 1$，都有 $\\frac{d^2\\psi_{\\rho}}{dx^2} > 0$，这证实了 $\\psi_{\\rho}(x)$ 在其定义域上是一个严格凸函数。\n\n对于开集上的一个严格凸且可微的函数，最小值的一阶必要条件，即梯度（在此标量情况下为导数）为零，同时也是一个充分条件。因此，唯一的最小化子 $x(\\rho)$ 可以通过求解方程 $\\frac{d\\psi_{\\rho}}{dx} = 0$ 得到：\n$$\n2\\rho(x - 2) + \\frac{1}{1 - x} = 0.\n$$\n为了解出 $x$，我们首先通过乘以 $(1 - x)$ 来消去分母，在定义域内它不为零：\n$$\n2\\rho(x - 2)(1 - x) + 1 = 0\n$$\n$$\n2\\rho(x - x^2 - 2 + 2x) + 1 = 0\n$$\n$$\n2\\rho(-x^2 + 3x - 2) + 1 = 0\n$$\n$$\n-2\\rho x^2 + 6\\rho x - 4\\rho + 1 = 0.\n$$\n这是一个形如 $ax^2 + bx + c = 0$ 的二次方程，其系数为 $a = 2\\rho$，$b = -6\\rho$，$c = 4\\rho - 1$。我们应用二次公式来求根：\n$$\nx = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a} = \\frac{6\\rho \\pm \\sqrt{(-6\\rho)^2 - 4(2\\rho)(4\\rho - 1)}}{2(2\\rho)}\n$$\n$$\nx = \\frac{6\\rho \\pm \\sqrt{36\\rho^2 - 8\\rho(4\\rho - 1)}}{4\\rho} = \\frac{6\\rho \\pm \\sqrt{36\\rho^2 - 32\\rho^2 + 8\\rho}}{4\\rho}\n$$\n$$\nx = \\frac{6\\rho \\pm \\sqrt{4\\rho^2 + 8\\rho}}{4\\rho} = \\frac{6\\rho \\pm 2\\sqrt{\\rho^2 + 2\\rho}}{4\\rho} = \\frac{3\\rho \\pm \\sqrt{\\rho^2 + 2\\rho}}{2\\rho}.\n$$\n这给出了两个可能的解。我们必须选择满足定义域约束 $x < 1$ 的那个解。\n考虑带加号的根：\n$x_1 = \\frac{3\\rho + \\sqrt{\\rho^2 + 2\\rho}}{2\\rho}$。由于 $\\rho > 0$，我们有 $\\sqrt{\\rho^2 + 2\\rho} > \\sqrt{\\rho^2} = \\rho$。因此，分子大于 $3\\rho + \\rho = 4\\rho$，并且 $x_1 > \\frac{4\\rho}{2\\rho} = 2$。这个根是无效的，因为它位于定义域 $x < 1$ 之外。\n\n考虑带减号的根：\n$x_2 = \\frac{3\\rho - \\sqrt{\\rho^2 + 2\\rho}}{2\\rho}$。由于 $\\rho < \\sqrt{\\rho^2 + 2\\rho}$，分子 $3\\rho - \\sqrt{\\rho^2 + 2\\rho} < 3\\rho - \\rho = 2\\rho$。因此，$x_2 < \\frac{2\\rho}{2\\rho} = 1$。这个根总是在可行域内。\n因此，唯一的最小化子由以下闭式表达式给出：\n$$\nx(\\rho) = \\frac{3\\rho - \\sqrt{\\rho^2 + 2\\rho}}{2\\rho}.\n$$\n问题将松弛变量定义为 $s(\\rho) = 1 - x(\\rho)$。代入 $x(\\rho)$ 的表达式：\n$$\ns(\\rho) = 1 - \\left( \\frac{3\\rho - \\sqrt{\\rho^2 + 2\\rho}}{2\\rho} \\right) = \\frac{2\\rho - (3\\rho - \\sqrt{\\rho^2 + 2\\rho})}{2\\rho} = \\frac{\\sqrt{\\rho^2 + 2\\rho} - \\rho}{2\\rho}.\n$$\n约束被定义为，如果 $s(\\rho) \\le \\tau$ 则为“激活”，否则为“非激活”。当 $\\tau = 0.01$ 时，我们必须为每个给定的 $\\rho$ 值评估此条件。程序将实现这一逻辑。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the constrained optimization problem using a logarithmic barrier method\n    and determines constraint activity for a suite of test cases.\n    \"\"\"\n    # The test suite consists of a set of barrier weights `rho`\n    # and a fixed tolerance `tau`.\n    test_cases = {\n        \"rho_values\": [0.1, 1, 10, 50, 100],\n        \"tau\": 0.01\n    }\n\n    rho_values = test_cases[\"rho_values\"]\n    tau = test_cases[\"tau\"]\n\n    results = []\n\n    # For each barrier weight `rho`, calculate the slack and determine if the\n    # constraint is active.\n    for rho in rho_values:\n        # The minimizer x(rho) of the barrier-augmented objective is derived from\n        # the first-order optimality condition d/dx [rho*(x-2)^2 - log(1-x)] = 0.\n        # This leads to the quadratic equation 2*rho*x^2 - 6*rho*x + (4*rho - 1) = 0.\n        # The valid root (satisfying x < 1) is x(rho) = (3*rho - sqrt(rho^2 + 2*rho)) / (2*rho).\n\n        # The slack is s(rho) = 1 - x(rho).\n        # A simpler expression for the slack is derived as:\n        # s(rho) = (sqrt(rho^2 + 2*rho) - rho) / (2*rho)\n        \n        # We must use floating-point numbers for the calculations.\n        rho_f = float(rho)\n        \n        # Calculate the slack s(rho).\n        numerator = np.sqrt(rho_f**2 + 2 * rho_f) - rho_f\n        denominator = 2 * rho_f\n        slack = numerator / denominator\n\n        # The constraint is \"active\" if the slack is less than or equal to the tolerance.\n        # Activity indicator is 1 for active, 0 for inactive.\n        if slack <= tau:\n            activity_indicator = 1\n        else:\n            activity_indicator = 0\n        \n        results.append(activity_indicator)\n\n    # The final output must be a single line containing the activity results\n    # as a comma-separated list of integers enclosed in square brackets.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2423438"}, {"introduction": "上一个练习展示了解如何逐渐逼近可行域边界，而本练习将探讨这一过程中出现的关键数值挑战。障碍法的一个固有难题是在解靠近边界时会产生数值病态（ill-conditioning）。通过分析障碍目标函数在边界附近的局部二次模型，你将直接探究问题的曲率（Hessian矩阵）如何急剧增大，并理解这为何会给求解器算法带来严峻的挑战，这对于设计稳健的优化算法至关重要。[@problem_id:2423460]", "problem": "考虑在一维空间中与单个不等式约束相关的障碍增广目标的无约束最小化问题。设原始目标为二次函数 $f(x) = (x - a)^2$，不等式约束为 $g(x) = c - x \\le 0$，因此可行域为 $\\{x \\in \\mathbb{R} \\, | \\, x \\ge c\\}$，内点区域为 $\\{x \\in \\mathbb{R} \\, | \\, x > c\\}$。对于障碍参数 $\\mu > 0$，定义对数障碍增广目标\n$$\n\\phi_{\\mu}(x) \\;=\\; f(x) \\;-\\; \\mu \\,\\log\\!\\big(-g(x)\\big) \\;=\\; (x - a)^2 \\;-\\; \\mu \\,\\log(x - c),\n$$\n其在 $x > c$ 时有良好定义。对于内点起始点 $x_0 = c + \\delta$（其中 $\\delta > 0$ 且 $g(x_0) = -\\delta$），分析 $\\phi_{\\mu}$ 在 $x_0$ 处的局部二次模型。\n\n仅使用梯度和海森的定义，令 $\\nabla \\phi_{\\mu}(x_0)$ 表示 $\\phi_{\\mu}$ 在 $x_0$ 处的一阶导数，令 $\\nabla^2 \\phi_{\\mu}(x_0)$ 表示 $\\phi_{\\mu}$ 在 $x_0$ 处的二阶导数。定义标量 $s$ 为以下线性方程的唯一解\n$$\n\\big(\\nabla^2 \\phi_{\\mu}(x_0)\\big)\\, s \\;=\\; -\\,\\nabla \\phi_{\\mu}(x_0).\n$$\n定义在 $x_0$ 处沿方向 $s$ 的二次模型预测变化量 $\\Delta$ 为\n$$\n\\Delta \\;=\\; \\nabla \\phi_{\\mu}(x_0)\\, s \\;+\\; \\tfrac{1}{2}\\, s^2 \\, \\nabla^2 \\phi_{\\mu}(x_0).\n$$\n最后，定义比率\n$$\nr \\;=\\; \\frac{s}{x_0 - c} \\;=\\; \\frac{s}{\\delta}.\n$$\n\n对于下方的每个测试用例，您必须在 $x_0 = c + \\delta$ 处求值并返回包含四个量的列表：\n- $|s|$，\n- $\\Delta$，\n- 一个正曲率指示符，如果 $\\nabla^2 \\phi_{\\mu}(x_0) > 0$ 则为 $1$，否则为 $0$，\n- 比率 $r$。\n\n所有计算必须使用标准双精度浮点运算完成。不要对结果进行四舍五入。\n\n测试套件：\n- 用例 1：$a = 1$, $c = 0$, $\\mu = 10^{-1}$, $\\delta = 10^{-3}$。\n- 用例 2：$a = 1$, $c = 0$, $\\mu = 1$, $\\delta = 10^{-9}$。\n- 用例 3：$a = 1$, $c = 0$, $\\mu = 1$, $\\delta = 10^{-12}$。\n- 用例 4：$a = 10^{-8}$, $c = 0$, $\\mu = 1$, $\\delta = 10^{-12}$。\n- 用例 5：$a = 1$, $c = 0$, $\\mu = 10^{-6}$, $\\delta = 10^{-12}$。\n\n最终输出格式：\n您的程序应生成单行输出，其中包含所有五个测试用例的结果，结果为逗号分隔的列表，并用方括号括起来，不含空格。每个测试用例的结果本身也必须是相同格式的列表，即，总体输出格式应如下所示\n$[[q_{11},q_{12},q_{13},q_{14}],[q_{21},q_{22},q_{23},q_{24}],[q_{31},q_{32},q_{33},q_{34}],[q_{41},q_{42},q_{43},q_{44}],[q_{51},q_{52},q_{53},q_{54}]]$,\n其中 $q_{ij}$ 按上面列出的顺序表示用例 $i$ 所需的量。", "solution": "所呈现的问题陈述是在约束优化的内点法分析中一个提法恰当且科学严谨的练习。所有定义、参数和目标都已足够精确地指定。该问题是有效的，并存在一个唯一、可验证的解。我们将继续进行推导和计算。\n\n任务是分析障碍增广目标函数在指定内点处的局部二次模型。原始目标是 $f(x) = (x - a)^2$，不等式约束是 $g(x) = c - x \\le 0$。对数障碍增广目标函数由下式给出：\n$$\n\\phi_{\\mu}(x) = f(x) - \\mu \\log(-g(x)) = (x - a)^2 - \\mu \\log(x - c)\n$$\n该函数在 $x > c$ 时有定义，障碍参数 $\\mu > 0$。分析将在点 $x_0 = c + \\delta$ 处执行，其中 $\\delta > 0$。\n\n首先，我们必须计算 $\\phi_{\\mu}(x)$ 关于 $x$ 的一阶和二阶导数。在一维情况下，这些分别被称为梯度 $\\nabla \\phi_{\\mu}(x)$ 和海森 $\\nabla^2 \\phi_{\\mu}(x)$。\n\n一阶导数为：\n$$\n\\nabla \\phi_{\\mu}(x) = \\frac{d}{dx} \\phi_{\\mu}(x) = \\frac{d}{dx} \\left( (x - a)^2 - \\mu \\log(x - c) \\right) = 2(x - a) - \\frac{\\mu}{x - c}\n$$\n二阶导数为：\n$$\n\\nabla^2 \\phi_{\\mu}(x) = \\frac{d^2}{dx^2} \\phi_{\\mu}(x) = \\frac{d}{dx} \\left( 2(x - a) - \\frac{\\mu}{x - c} \\right) = 2 + \\frac{\\mu}{(x - c)^2}\n$$\n接下来，我们在指定点 $x_0 = c + \\delta$ 处计算这些导数的值。在该点，$x_0-c$ 项简化为 $\\delta$。\n$$\n\\nabla \\phi_{\\mu}(x_0) = 2(x_0 - a) - \\frac{\\mu}{x_0 - c} = 2(c + \\delta - a) - \\frac{\\mu}{\\delta}\n$$\n$$\n\\nabla^2 \\phi_{\\mu}(x_0) = 2 + \\frac{\\mu}{(x_0 - c)^2} = 2 + \\frac{\\mu}{\\delta^2}\n$$\n量 $s$ 定义为代表最小化 $\\phi_{\\mu}(x)$ 的牛顿法的线性系统的解：\n$$\n\\big(\\nabla^2 \\phi_{\\mu}(x_0)\\big) s = -\\nabla \\phi_{\\mu}(x_0)\n$$\n由于 $\\nabla^2 \\phi_{\\mu}(x_0)$ 是一个标量，我们可以通过除法求解 $s$：\n$$\ns = -\\frac{\\nabla \\phi_{\\mu}(x_0)}{\\nabla^2 \\phi_{\\mu}(x_0)} = -\\frac{2(c + \\delta - a) - \\frac{\\mu}{\\delta}}{2 + \\frac{\\mu}{\\delta^2}}\n$$\n要返回的第一个量是 $s$ 的绝对值，记为 $|s|$。\n\n第二个量是来自二次模型的预测变化量 $\\Delta$：\n$$\n\\Delta = \\nabla \\phi_{\\mu}(x_0) s + \\frac{1}{2} s^2 \\nabla^2 \\phi_{\\mu}(x_0)\n$$\n我们可以将关系式 $\\nabla \\phi_{\\mu}(x_0) = -s \\nabla^2 \\phi_{\\mu}(x_0)$ 代入此表达式以简化它：\n$$\n\\Delta = (-s \\nabla^2 \\phi_{\\mu}(x_0)) s + \\frac{1}{2} s^2 \\nabla^2 \\phi_{\\mu}(x_0) = -s^2 \\nabla^2 \\phi_{\\mu}(x_0) + \\frac{1}{2} s^2 \\nabla^2 \\phi_{\\mu}(x_0) = -\\frac{1}{2} s^2 \\nabla^2 \\phi_{\\mu}(x_0)\n$$\n这种简化形式对于数值计算是稳健的。\n\n第三个量是正曲率的指示符。我们必须检查海森 $\\nabla^2 \\phi_{\\mu}(x_0)$ 的符号。\n$$\n\\nabla^2 \\phi_{\\mu}(x_0) = 2 + \\frac{\\mu}{\\delta^2}\n$$\n鉴于问题约束 $\\mu > 0$ 和 $\\delta > 0$，显然 $\\mu/\\delta^2 > 0$。因此，$\\nabla^2 \\phi_{\\mu}(x_0)$ 严格大于 $2$。海森矩阵总是正的，这意味着障碍增广函数 $\\phi_{\\mu}(x)$ 是严格凸的。因此，正曲率指示符（如果 $\\nabla^2 \\phi_{\\mu}(x_0) > 0$ 则为 $1$，否则为 $0$）始终为 $1$。\n\n第四个量是比率 $r$：\n$$\nr = \\frac{s}{x_0 - c} = \\frac{s}{\\delta}\n$$\n这可以在求出 $s$ 后直接计算。\n\n每个测试用例 $(a, c, \\mu, \\delta)$ 的计算步骤如下：\n1. 计算 $\\nabla \\phi_{\\mu}(x_0) = 2(c + \\delta - a) - \\mu / \\delta$。\n2. 计算 $\\nabla^2 \\phi_{\\mu}(x_0) = 2 + \\mu / \\delta^2$。\n3. 计算 $s = -\\nabla \\phi_{\\mu}(x_0) / \\nabla^2 \\phi_{\\mu}(x_0)$。\n4. 计算所需的四个值：\n    - $|s|$\n    - $\\Delta = -0.5 \\cdot s^2 \\cdot \\nabla^2 \\phi_{\\mu}(x_0)$\n    - 曲率指示符：$1.0$\n    - $r = s / \\delta$\n\n这些步骤在提供的 Python 代码中实现，以使用标准双精度浮点运算计算所有指定测试用例的结果。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem for the given test suite by analyzing the local\n    quadratic model of a barrier-augmented objective function.\n    \"\"\"\n\n    test_cases = [\n        # Case 1: a = 1, c = 0, mu = 1e-1, delta = 1e-3\n        (1.0, 0.0, 1e-1, 1e-3),\n        # Case 2: a = 1, c = 0, mu = 1, delta = 1e-9\n        (1.0, 0.0, 1.0, 1e-9),\n        # Case 3: a = 1, c = 0, mu = 1, delta = 1e-12\n        (1.0, 0.0, 1.0, 1e-12),\n        # Case 4: a = 10e-9, c = 0, mu = 1, delta = 1e-12\n        (10e-9, 0.0, 1.0, 1e-12),\n        # Case 5: a = 1, c = 0, mu = 1e-6, delta = 1e-12\n        (1.0, 0.0, 1e-6, 1e-12),\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        a, c, mu, delta = case\n        \n        # Calculate the gradient (first derivative) at x_0 = c + delta\n        # grad_phi = 2 * (x_0 - a) - mu / (x_0 - c)\n        # x_0 - c = delta\n        # x_0 - a = c + delta - a\n        grad_phi = 2.0 * (c + delta - a) - mu / delta\n        \n        # Calculate the Hessian (second derivative) at x_0 = c + delta\n        # hess_phi = 2 + mu / (x_0 - c)^2\n        hess_phi = 2.0 + mu / (delta**2)\n        \n        # Calculate the Newton step s\n        # hess_phi * s = -grad_phi\n        s = -grad_phi / hess_phi\n        \n        # 1. |s|\n        abs_s = abs(s)\n        \n        # 2. Predicted change Delta\n        # Delta = -0.5 * s^2 * hess_phi\n        delta_change = -0.5 * s**2 * hess_phi\n        \n        # 3. Indicator of positive curvature\n        # hess_phi = 2 + mu/delta^2. Since mu > 0, delta > 0, hess_phi is always > 0.\n        pos_curv_indicator = 1.0 if hess_phi > 0 else 0.0\n        \n        # 4. Ratio r\n        # r = s / delta\n        r = s / delta\n        \n        results.append([abs_s, delta_change, pos_curv_indicator, r])\n\n    # Format the final output string as a list of lists with no spaces\n    # e.g., [[q11,q12,q13,q14],[q21,q22,q23,q24],...]\n    inner_results_str = [f\"[{','.join(map(str, res))}]\" for res in results]\n    final_output = f\"[{','.join(inner_results_str)}]\"\n    \n    print(final_output)\n\nsolve()\n```", "id": "2423460"}]}