{"hands_on_practices": [{"introduction": "在我们开始使用强大的 BFGS 算法进行优化时，一个基本问题出现了：我们应该如何选择初始的逆 Hessian 矩阵近似 $H_0$？这个练习探讨了在没有任何关于函数曲率先验信息的情况下，最标准和最稳健的选择。通过分析不同的选项，你将理解为什么默认将第一次迭代的方向设为最速下降方向是一种明智且经过深思熟虑的策略 [@problem_id:2208648]。", "problem": "一个数据科学家团队的任务是通过最小化其损失函数 $f(x)$ 来训练一个复杂的机器学习模型，其中 $x$ 是模型在 $\\mathbb{R}^n$ 空间中的参数向量。他们决定使用 Broyden-Fletcher-Goldfarb-Shanno (BFGS) 算法，这是一种强大的拟牛顿优化方法。BFGS 算法的核心是迭代更新：\n$$x_{k+1} = x_k + \\alpha_k p_k$$\n其中 $\\alpha_k > 0$ 是通过线搜索找到的步长，而 $p_k$ 是由下式给出的搜索方向：\n$$p_k = -H_k g_k$$\n在这里，$g_k = \\nabla f(x_k)$ 是损失函数在当前迭代点 $x_k$ 处的梯度，而 $H_k$ 是 Hessian 矩阵逆的近似，它在每一步都会更新。\n\n对于第一步（$k=0$），团队需要选择一个初始矩阵 $H_0$。他们对损失函数曲面的形状或曲率没有具体的先验知识。他们正在考虑几种关于初始逆 Hessian 矩阵近似 $H_0$ 的选项。在这种常见情况下，以下哪个选项是标准且最合理的出发点？\n\nA. $H_0 = O$ (零矩阵)。理由：这代表了最大的不确定性状态，没有对函数的曲率施加任何初始信念。\n\nB. $H_0 = I$ (单位矩阵)。理由：这个选择将初始搜索方向简化为与最速下降法相同的方向，提供了一个稳健且易于理解的起点。\n\nC. $H_0 = \\epsilon I$，其中 $\\epsilon$ 是一个非常小的正数（例如，$10^{-6}$）。理由：这确保了第一步非常小，防止算法因初始猜测远离最小值点而发散。\n\nD. $H_0$ 是一个随机生成的对称正定矩阵。理由：这避免了单位矩阵的人为结构，并且如果随机矩阵恰好与真实的逆 Hessian 矩阵很好地对齐，可能会导致更快的收敛。\n\nE. $H_0 = \\frac{1}{g_0^T g_0} I$。理由：这将初始步长进行缩放，使其具有归一化的量级，从而防止因初始梯度的尺度而产生过大或过小的步长。", "solution": "我们要求 $H_{0}$ 是对称正定的，以确保在第一次迭代时得到一个下降方向。具体来说，BFGS 的搜索方向是 $p_{k}=-H_{k}g_{k}$，为了在 $g_{k}\\neq 0$ 时保证 $p_k$ 是一个下降方向，我们需要满足\n$$\np_{k}^{T}g_{k}=-g_{k}^{T}H_{k}g_{k}<0,\n$$\n该条件对所有非零的 $g_k$ 均成立，当且仅当 $H_k$ 是对称正定矩阵。\n\n评估每个提议的 $H_0$：\n\n- 选项 A: $H_{0}=O$。那么 $p_{0}=-H_{0}g_{0}=0$，因此不会发生移动。此外，$H_0$ 是奇异的且非正定，违反了上述要求，并可能导致第一次更新产生秩亏的矩阵\n$$\nH_{1}=(I-\\rho s y^{T})H_{0}(I-\\rho y s^{T})+\\rho s s^{T}=\\rho s s^{T},\n$$\n对于 $n>1$ 的情况，该矩阵是半正定的但不是正定的（秩为一），从而破坏了算法。\n\n- 选项 B: $H_{0}=I$。那么 $H_0$ 是对称正定的，并且\n$$\np_{0}=-H_{0}g_{0}=-g_{0},\n$$\n所以第一步与最速下降法一致，这是一个稳健的默认选择。通过线搜索强制 $y_{k}^{T}s_{k}>0$，BFGS 更新会保持正定性：\n$$\nH_{k+1}=(I-\\rho_{k}s_{k}y_{k}^{T})H_{k}(I-\\rho_{k}y_{k}s_{k}^{T})+\\rho_{k}s_{k}s_{k}^{T},\\quad \\rho_{k}=\\frac{1}{y_{k}^{T}s_{k}},\n$$\n从 $H_{0}=I$ 开始是文献中推荐的标准做法。\n\n- 选项 C: $H_{0}=\\epsilon I$，其中 $\\epsilon>0$ 非常小。这是对称正定的，但它人为地缩短了初始方向：\n$$\np_{0}=-\\epsilon g_{0}.\n$$\n尽管在精确线搜索下，$p_0$ 的缩放不影响沿射线方向的最小值点，但实际的非精确线搜索（例如，初始试验步长为 1 的 Wolfe 条件）会使这个选择导致不必要的过小有效步长和不良的数值尺度。它不是标准的默认选项。\n\n- 选项 D: $H_{0}$ 是一个随机的对称正定矩阵。虽然可以强制其对称正定，但这在没有先验信息的情况下引入了任意的各向异性和可变性，可能产生不良的搜索方向，并且不是标准做法。\n\n- 选项 E: $H_{0}=\\frac{1}{g_{0}^{T}g_{0}}I$。这是对称正定的，并产生\n$$\np_{0}=-\\frac{1}{g_{0}^{T}g_{0}}g_{0},\n$$\n其范数 $\\|p_{0}\\|=\\frac{1}{\\|g_{0}\\|}$，以一种非标准的方式将步长缩放与梯度大小耦合起来。这不是常规的初始化方法，并且可能适得其反。\n\n因此，在没有关于曲率的先验信息的情况下，标准且最合理的选择是 $H_{0}=I$，这使得第一步成为最速下降步，并在标准线搜索下保持了 BFGS 的理想属性。", "answer": "$$\\boxed{B}$$", "id": "2208648"}, {"introduction": "虽然使用单位矩阵作为初始逆 Hessian 矩阵近似（在某些文献中记为 $B_0=I$ 或 $H_0=I$）是一种安全的选择，但它不一定是最快的。本练习将引导你通过一个编码实践，探索一种更精妙的策略：根据沿梯度方向探测到的局部曲率信息来缩放初始矩阵。通过在一个著名的非凸函数上比较有缩放和无缩放的初始步骤，你将亲眼看到一个简单的启发式方法如何能显著加速算法在病态问题上的初始收敛速度 [@problem_id:2431054]。", "problem": "考虑使用拟牛顿法和 Broyden-Fletcher-Goldfarb-Shanno (BFGS) 更新来对光滑函数 $f:\\mathbb{R}^n \\to \\mathbb{R}$ 进行无约束最小化。该方法维护一个对逆 Hessian 矩阵（二阶导数矩阵的逆）的近似 $B_k$，并通过 $p_k = - B_k \\nabla f(x_k)$ 在第 k 次迭代时计算下降方向 $p_k$。然后，线搜索沿 $p_k$ 方向选择步长 $\\alpha_k$，以满足强 Wolfe 条件，其中常数 $c_1 = 10^{-4}$ 且 $c_2 = 0.9$。初始逆 Hessian 近似 $B_0$ 强烈影响第一步 $p_0$ 的大小，从而影响初始进展。\n\n您的任务是构建和评估一个可复现的场景，在该场景中，与使用缩放单位矩阵 $B_0 = \\gamma I$ 相比，选择 $B_0 = I$（单位矩阵）会导致初始进展非常缓慢。其中\n$$\n\\gamma = \\frac{y_0^\\top s_0}{y_0^\\top y_0}, \\quad s_0 = -\\eta \\, \\nabla f(x_0), \\quad y_0 = \\nabla f(x_0 + s_0) - \\nabla f(x_0),\n$$\n这里 $\\eta > 0$ 是一个小的探测步长。这种缩放选择旨在根据沿梯度方向测量的局部曲率来调整初始逆 Hessian 矩阵。\n\n使用偶数维度 $n$ 下的广义双参数 Rosenbrock 函数作为目标函数：\n$$\nf(x) = \\sum_{i=1}^{n/2} \\left[ \\beta\\left(x_{2i} - x_{2i-1}^2\\right)^2 + \\left(1 - x_{2i-1}\\right)^2 \\right],\n$$\n其梯度分量为\n$$\n\\frac{\\partial f}{\\partial x_{2i-1}} = -4\\beta\\,x_{2i-1}\\left(x_{2i} - x_{2i-1}^2\\right) + 2\\left(x_{2i-1} - 1\\right), \\quad\n\\frac{\\partial f}{\\partial x_{2i}} = 2\\beta\\left(x_{2i} - x_{2i-1}^2\\right),\n$$\n对于 $i=1,\\dots,n/2$。\n\n实现具有两种初始化方式的单次 BFGS 迭代：\n- 情况 A（未缩放）：$B_0 = I$。\n- 情况 B（缩放）：$B_0 = \\gamma I$，其中 $\\gamma$ 如上所给，使用探测步长 $s_0 = -\\eta \\nabla f(x_0)$。\n\n对于每种情况，计算 $p_0 = -B_0 \\nabla f(x_0)$，并从 $\\alpha = 1$ 开始通过强 Wolfe 线搜索（$c_1 = 10^{-4}$，$c_2 = 0.9$）选择 $\\alpha_0$。如果强 Wolfe 线搜索未能返回步长，则回退到 Armijo 回溯法（仅充分下降条件），缩减因子为 0.5，使用相同的 $c_1$ 和相同的初始 $\\alpha = 1$。然后对每种情况计算 $x_1 = x_0 + \\alpha_0 p_0$。将初始进展定义为目标函数的减少量 $f(x_0) - f(x_1)$。\n\n您的程序必须为每个测试用例计算比率\n$$\nR = \\frac{f(x_0) - f(x_1^{\\text{scaled}})}{\\max\\left(f(x_0) - f(x_1^{\\text{identity}}),\\,10^{-30}\\right)},\n$$\n其中 $x_1^{\\text{scaled}}$ 是通过缩放的 $B_0 = \\gamma I$ 得到的，而 $x_1^{\\text{identity}}$ 是通过 $B_0 = I$ 得到的。$R > 1$ 的值表示在第一次迭代中，缩放初始化比未缩放的初始化取得了更大的进展。报告 $R$ 的值，四舍五入到六位小数。\n\n测试套件：\n- 测试 1（高曲率，二维）：$n=2$, $\\beta = 10^4$, $x_0 = (-1.2,\\,1.0)$, $\\eta = 10^{-3}$。\n- 测试 2（中等曲率，四维）：$n=4$, $\\beta = 100$, $x_0 = (-1.2,\\,1.0,\\,-1.2,\\,1.0)$, $\\eta = 10^{-3}$。\n- 测试 3（接近最小值的边缘情况）：$n=2$, $\\beta = 100$, $x_0 = (1.0,\\,1.0001)$, $\\eta = 10^{-3}$。\n\n您的程序应产生单行输出，其中包含用方括号括起来的逗号分隔列表形式的结果，例如，“[r1,r2,r3]”。每个 $r_i$ 必须是相应测试的 $R$ 的浮点值，四舍五入到六位小数。不允许有任何其他输出。不涉及物理单位和角度；所有量均为无量纲实数。", "solution": "所提出的问题已经过严格验证。所有给定信息，包括目标函数、其梯度、算法参数和测试用例，均已提取。该问题被认为在数值优化这一成熟领域具有科学依据。问题提法恰当，为获得唯一的确定性解提供了所有必要信息。语言客观而精确。因此，该问题被视为有效，并将提供解决方案。\n\n该问题要求分析拟牛顿优化方法，特别是 Broyden-Fletcher-Goldfarb-Shanno (BFGS) 算法的第一次迭代。我们需要比较初始逆 Hessian 近似 $B_0$ 的两种不同选择所带来的初始进展。\n\n拟牛顿法的通用迭代格式由下式给出：\n$$ x_{k+1} = x_k + \\alpha_k p_k $$\n其中 $p_k$ 是搜索方向，$\\alpha_k$ 是步长。搜索方向是使用逆 Hessian 矩阵的当前近似 $B_k$ 和目标函数的梯度 $\\nabla f(x_k)$ 计算得出的：\n$$ p_k = -B_k \\nabla f(x_k) $$\n步长 $\\alpha_k$ 通过线搜索过程确定，以确保函数值充分下降并满足曲率条件。问题指定了强 Wolfe 条件：\n1. Armijo（充分下降）条件：$f(x_k + \\alpha_k p_k) \\le f(x_k) + c_1 \\alpha_k \\nabla f(x_k)^\\top p_k$\n2. 曲率条件：$|\\nabla f(x_k + \\alpha_k p_k)^\\top p_k| \\le c_2 |\\nabla f(x_k)^\\top p_k|$\n其中指定常数为 $c_1 = 10^{-4}$ 和 $c_2 = 0.9$。\n\n这个问题的核心在于初始近似 $B_0$ 的选择，它决定了优化过程的第一步 $p_0 = -B_0 \\nabla f(x_0)$。我们研究两种情况。\n\n情况 A：未缩放的单位矩阵初始化\n选择 $B_0 = I$（其中 $I$ 是单位矩阵）是最简单的可能选择。这导致初始搜索方向为 $p_0 = -\\nabla f(x_0)$，即最速下降方向。虽然这种方法很直观，但对于函数等值线高度偏心的病态问题，该方向可能效率低下，导致缓慢的锯齿状收敛。\n\n情况 B：缩放的单位矩阵初始化\n一种更复杂的方法是缩放初始单位矩阵，$B_0 = \\gamma I$。选择缩放因子 $\\gamma$ 以近似函数的曲率。问题规定了一种基于探测步长来寻找 $\\gamma$ 的具体方法。我们首先计算一个试验步长 $s_0 = -\\eta \\nabla f(x_0)$，其中 $\\eta > 0$ 是一个小数。然后我们测量梯度的变化，$y_0 = \\nabla f(x_0 + s_0) - \\nabla f(x_0)$。缩放因子由下式给出：\n$$ \\gamma = \\frac{y_0^\\top s_0}{y_0^\\top y_0} $$\n这个 $\\gamma$ 的公式是通过在最小二乘意义上找到一个最能满足割线方程 $s_0 = B_0 y_0$ 的标量而导出的，即通过最小化关于 $\\gamma$ 的 $\\|\\ s_0 - \\gamma y_0 \\|_2^2$。这旨在将函数曲率的一些信息赋予 $B_0$，从而可能产生一个尺度更好的初始步长 $p_0 = -\\gamma \\nabla f(x_0)$。为了使这是一个下降方向，我们需要 $\\gamma > 0$，这在函数沿着探测方向 $s_0$ 足够凸（即 $y_0^\\top s_0 > 0$）时成立。\n\n测试函数是偶数维度 $n$ 下的广义 Rosenbrock 函数：\n$$ f(x) = \\sum_{i=1}^{n/2} \\left[ \\beta\\left(x_{2i} - x_{2i-1}^2\\right)^2 + \\left(1 - x_{2i-1}\\right)^2 \\right] $$\n这个函数是优化算法的经典基准测试，因为它具有非凸性和一个狭窄的抛物线形山谷。对于较大的 $\\beta$，问题变得非常病态，使其成为一个绝佳的候选者，用以证明尺度良好的初始步长优于朴素的最速下降方向。\n\n计算步骤如下：\n1.  对于每个测试用例 ($n, \\beta, x_0, \\eta$)，我们将实现 Rosenbrock 函数及其梯度。\n2.  我们将为情况 A ($B_0 = I$) 和情况 B ($B_0 = \\gamma I$) 执行一次迭代。\n3.  对于情况 B，首先计算因子 $\\gamma$，这需要计算在 $x_0$ 和探测点 $x_0 + s_0$ 处的梯度。当分母 $y_0^\\top y_0$ 接近于零时，必须有安全措施。\n4.  对于两种情况，计算搜索方向 $p_0$。我们必须验证它是一个下降方向（即 $\\nabla f(x_0)^\\top p_0 < 0$）。\n5.  从 $\\alpha = 1$ 开始，沿 $p_0$ 执行线搜索，以找到满足强 Wolfe 条件的步长 $\\alpha_0$。我们将使用 `scipy.optimize` 库中的 `line_search` 函数。\n6.  根据问题要求，如果强 Wolfe 线搜索失败，我们必须回退到手动实现的 Armijo 回溯法，在此方法中，我们迭代地将 $\\alpha$ 乘以因子 0.5，直到满足充分下降条件。\n7.  在确定 $\\alpha_0$ 后，我们计算下一个迭代点 $x_1 = x_0 + \\alpha_0 p_0$ 和相应的函数值 $f(x_1)$。\n8.  每种情况的进展定义为目标函数的减少量 $f(x_0) - f(x_1)$。\n9.  最后，我们计算缩放方法取得的进展与未缩放方法取得的进展之比 $R$：\n$$ R = \\frac{f(x_0) - f(x_1^{\\text{scaled}})}{\\max\\left(f(x_0) - f(x_1^{\\text{identity}}),\\,10^{-30}\\right)} $$\n分母经过正则化处理，以防止除以零或数值不稳定。每个测试用例的 $R$ 值将四舍五入到六位小数。\n\n这个过程将被封装在一个 Python 程序中，该程序构成最终答案。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import line_search\n\ndef rosenbrock(x, beta):\n    \"\"\"\n    Computes the value of the generalized Rosenbrock function.\n    \"\"\"\n    n = len(x)\n    if n % 2 != 0:\n        raise ValueError(\"Dimension n must be even for the generalized Rosenbrock function.\")\n    \n    val = 0.0\n    for i in range(n // 2):\n        # x-indices are 2*i and 2*i+1, corresponding to problem's x_2i-1 and x_2i for i=1..n/2\n        idx1 = 2 * i\n        idx2 = 2 * i + 1\n        term1 = beta * (x[idx2] - x[idx1]**2)**2\n        term2 = (1 - x[idx1])**2\n        val += term1 + term2\n    return val\n\ndef rosenbrock_grad(x, beta):\n    \"\"\"\n    Computes the gradient of the generalized Rosenbrock function.\n    \"\"\"\n    n = len(x)\n    if n % 2 != 0:\n        raise ValueError(\"Dimension n must be even for the generalized Rosenbrock function.\")\n        \n    grad = np.zeros(n)\n    for i in range(n // 2):\n        idx1 = 2 * i\n        idx2 = 2 * i + 1\n        common_term = 2 * beta * (x[idx2] - x[idx1]**2)\n        grad[idx1] = -2 * x[idx1] * common_term + 2 * (x[idx1] - 1)\n        grad[idx2] = common_term\n    return grad\n\ndef compute_progress(x0, beta, eta, initialization, c1, c2):\n    \"\"\"\n    Computes the progress f(x0) - f(x1) for a single Quasi-Newton iteration.\n    Handles both identity and scaled initializations, and includes line search logic.\n    \"\"\"\n    # Create lambda functions to pass beta parameter\n    f = lambda x: rosenbrock(x, beta)\n    grad = lambda x: rosenbrock_grad(x, beta)\n\n    f0 = f(x0)\n    g0 = grad(x0)\n\n    # If gradient is virtually zero, no progress can be made.\n    if np.linalg.norm(g0) < 1e-12:\n        return 0.0\n\n    if initialization == 'identity':\n        p0 = -g0\n    elif initialization == 'scaled':\n        s0 = -eta * g0\n        \n        # Probing step to compute the scaling factor gamma\n        x_probe = x0 + s0\n        g_probe = grad(x_probe)\n        y0 = g_probe - g0\n\n        y0_dot_y0 = np.dot(y0, y0)\n        \n        # Guard against division by zero for ill-defined gamma\n        if y0_dot_y0 < 1e-20:\n            gamma = 1.0 # Fallback to identity scaling\n        else:\n            y0_dot_s0 = np.dot(y0, s0)\n            gamma = y0_dot_s0 / y0_dot_y0\n        \n        p0 = -gamma * g0\n    else:\n        raise ValueError(f\"Unknown initialization type: {initialization}\")\n\n    # The search direction must be a descent direction.\n    # If gamma <= 0, this will not hold, and progress should be zero.\n    pk_dot_g0 = np.dot(g0, p0)\n    if pk_dot_g0 >= 0:\n        return 0.0\n\n    # Perform strong Wolfe line search using SciPy\n    alpha, _, _, f_new, _, _ = line_search(\n        f=f,\n        myfprime=grad,\n        xk=x0,\n        pk=p0,\n        gfk=g0,\n        old_fval=f0,\n        c1=c1,\n        c2=c2\n    )\n\n    # Fallback to Armijo backtracking if strong Wolfe search fails\n    if alpha is None:\n        alpha = 1.0\n        rho = 0.5\n        \n        # Limit backtracking steps to prevent infinite loops\n        for _ in range(100):\n            x_new_check = x0 + alpha * p0\n            f_new_check = f(x_new_check)\n            if f_new_check <= f0 + c1 * alpha * pk_dot_g0:\n                f_new = f_new_check\n                break\n            alpha *= rho\n        else:\n            # If Armijo loop completes without break, step is negligible.\n            return 0.0\n\n    return f0 - f_new\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (n, beta, x0, eta)\n        (2, 1e4, np.array([-1.2, 1.0]), 1e-3),\n        (4, 100.0, np.array([-1.2, 1.0, -1.2, 1.0]), 1e-3),\n        (2, 100.0, np.array([1.0, 1.0001]), 1e-3)\n    ]\n    \n    # Line search parameters\n    c1 = 1e-4\n    c2 = 0.9\n\n    results = []\n    for n, beta, x0, eta in test_cases:\n        # Calculate progress for the unscaled (identity) case\n        progress_identity = compute_progress(x0, beta, eta, 'identity', c1, c2)\n        \n        # Calculate progress for the scaled case\n        progress_scaled = compute_progress(x0, beta, eta, 'scaled', c1, c2)\n\n        # Compute the ratio R, with a safeguard for the denominator\n        denominator = max(progress_identity, 1e-30)\n        R = progress_scaled / denominator\n        \n        results.append(round(R, 6))\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2431054"}, {"introduction": "BFGS 算法的一个主要限制是需要存储一个稠密的 $n \\times n$ 矩阵，这对于大规模问题是不可行的。有限内存 BFGS (L-BFGS) 算法巧妙地解决了这个问题，它也是当今许多优化软件库的核心。这个顶峰练习将带你深入 L-BFGS 的心脏，要求你从头开始实现著名的“双循环递归”，这是一种无需显式存储逆 Hessian 矩阵就能高效计算搜索方向的优雅技术 [@problem_id:2431082]。", "problem": "给定有限的曲率对序列 $\\{(s_i,y_i)\\}$、一个定义了 $H_0=\\gamma I$ 的初始标量 $\\gamma$ 以及一个光滑目标函数的当前梯度 $g$。考虑如下定义的对称正定线性算子 $H$：在所有对最近的 $m$ 个满足 $s_i^\\top y_i > 0$ 的曲率对 $\\{(s_i,y_i)\\}_{i=k-m+1}^k$ 满足割线条件 $H y_i = s_i$ 的对称正定矩阵中，算子 $H$ 是从 $H_0=\\gamma I$ 开始，以 Broyden-Fletcher-Goldfarb-Shanno (BFGS) 更新的方式，并按索引递增的顺序施加这些割线条件而得到的唯一算子。对于下述每个测试用例，请计算搜索方向 $p=-H g$。\n\n所有向量和矩阵均在实数域上。所有分量都应被视为量纲一致的纯数（无物理单位）。请使用以下测试套件；在每个案例中，仅需施加最后的 $m$ 个曲率对。\n\n测试用例 $1$ (边界情况，无曲率对):\n- 维度 $n=3$。\n- 内存大小 $m=0$ (无曲率对)。\n- 初始缩放因子 $\\gamma = 0.5$。\n- 梯度 $g = [1,-2,3]^\\top$。\n\n测试用例 $2$ (单个曲率对):\n- 维度 $n=2$。\n- 内存大小 $m=1$。\n- 初始缩放因子 $\\gamma = 1$。\n- 曲率对 $\\left(s_1,y_1\\right) = \\left([1,2]^\\top,[3,1]^\\top\\right)$，其中 $s_1^\\top y_1 = 5$。\n- 梯度 $g = [4,-1]^\\top$。\n\n测试用例 $3$ (多个曲率对张成空间，对二次函数的精确恢复):\n- 维度 $n=3$。\n- 内存大小 $m=3$。\n- 初始缩放因子 $\\gamma = 1$。\n- 定义 $A=\\mathrm{diag}\\!\\left(2,3,4\\right)$。令 $s_1=[1,0,0]^\\top$，$s_2=[0,1,0]^\\top$，$s_3=[0,0,1]^\\top$，且 $y_i = A s_i$，即 $y_1=[2,0,0]^\\top$，$y_2=[0,3,0]^\\top$，$y_3=[0,0,4]^\\top$。\n- 梯度 $g = [5,-6,7]^\\top$。\n\n测试用例 $4$ (有限内存，仅使用最近的曲率对):\n- 维度 $n=4$。\n- 内存大小 $m=2$ (仅使用下面列出的最后两个曲率对)。\n- 初始缩放因子 $\\gamma = 1$。\n- 定义对称正定矩阵\n$$\nA=\\begin{bmatrix}\n4 & 1 & 0 & 0\\\\\n1 & 3 & 0 & 0\\\\\n0 & 0 & 2 & 0\\\\\n0 & 0 & 0 & 1.5\n\\end{bmatrix}.\n$$\n- 对于 $s_1=[1,0,0,0]^\\top$（对应 $y_1=[4,1,0,0]^\\top$）、$s_2=[0,1,0,0]^\\top$（对应 $y_2=[1,3,0,0]^\\top$）、$s_3=[0,0,1,0]^\\top$（对应 $y_3=[0,0,2,0]^\\top$）和 $s_4=[0,0,0,1]^\\top$（对应 $y_4=[0,0,0,1.5]^\\top$），通过 $y_i = A s_i$ 定义曲率对。\n- 梯度 $g = [1,2,3,4]^\\top$。\n- 仅需施加最后的 $m=2$ 个曲率对，即 $\\left(s_3,y_3\\right)$ 和 $\\left(s_4,y_4\\right)$。\n\n你的程序必须为每个测试用例计算相应的搜索方向 $p=-H g$。输出格式要求：你的程序应生成单行输出，其中包含按顺序排列的四个搜索方向向量的列表，不含任何空白字符，并使用十进制表示法。具体来说，输出必须采用以下格式：`[[p^{(1)}_1,...,p^{(1)}_{n_1}],[p^{(2)}_1,...,p^{(2)}_{n_2}],[p^{(3)}_1,...,p^{(3)}_{n_3}],[p^{(4)}_1,...,p^{(4)}_{n_4}]]`,\n其中 $p^{(j)}$ 是测试用例 $j$ 的向量。每个数字都必须以十进制形式打印（例如，$-2$、$-1.5$ 或 $-2.6666666667$ 都是可接受的）。最终输出必须是严格符合这种方括号、逗号分隔格式的单行文本。", "solution": "经过严格审查，问题陈述被确认为有效。该问题具备科学依据、提法恰当、目标明确且内部一致。它提出了一个数值优化领域的标准计算任务：使用有限内存 Broyden-Fletcher-Goldfarb-Shanno (L-BFGS) 算法计算搜索方向。\n\n该问题通过一个构造性过程定义了一个对称正定矩阵 $H$。从初始矩阵 $H_0 = \\gamma I$（其中 $\\gamma$ 是一个正标量，$I$ 是单位矩阵）开始，使用给定的曲率对 $\\{(s_i, y_i)\\}$ 应用一系列共 $m$ 次的 BFGS 更新。问题指明，更新是按索引的递增顺序应用的。用于从逆 Hessian 矩阵近似 $H_k$ 获得 $H_{k+1}$ 的 BFGS 更新公式为：\n$$ H_{k+1} = (I - \\rho_k s_k y_k^\\top) H_k (I - \\rho_k y_k s_k^\\top) + \\rho_k s_k s_k^\\top $$\n其中 $\\rho_k = (y_k^\\top s_k)^{-1}$。条件 $s_k^\\top y_k > 0$ 确保 $\\rho_k$ 为正，并且在整个更新过程中保持正定性。\n\n任务是计算搜索方向 $p = -Hg$，其中 $g$ 是一个给定的梯度向量。对于 L-BFGS 所设计用于解决的大规模问题，通过先构造 $n \\times n$ 矩阵 $H$ 再执行矩阵-向量乘法的直接计算方法，其计算成本过高。标准且高效的方法是 L-BFGS 双循环递归法，它可以在不显式构造 $H$ 的情况下计算乘积 $Hg$。此过程仅利用存储的 $m$ 个曲率对和初始缩放因子 $\\gamma$。这是指导解决方案设计的原则。\n\n计算 $r=Hg$ 的算法如下。设最近的 $m$ 个曲率对为 $\\{(s_j, y_j)\\}_{j=1}^m$，在内存窗口中按从最旧到最新的顺序排列。对每个曲率对，令 $\\rho_j = (y_j^\\top s_j)^{-1}$。\n\n1.  初始化向量 $q \\leftarrow g$。\n2.  从最新的曲率对（$j=m$）到最旧的（$j=1$）执行一次向后传递（第一个循环）：\n    对于 $j = m, m-1, \\dots, 1$:\n    -   计算并存储 $\\alpha_j \\leftarrow \\rho_j s_j^\\top q$。\n    -   更新 $q \\leftarrow q - \\alpha_j y_j$。\n3.  用初始逆 Hessian 近似来缩放中间向量：$r \\leftarrow H_0 q = \\gamma q$。\n4.  从最旧的曲率对（$j=1$）到最新的（$j=m$）执行一次向前传递（第二个循环）：\n    对于 $j = 1, 2, \\dots, m$:\n    -   计算 $\\beta_j \\leftarrow \\rho_j y_j^\\top r$。\n    -   更新 $r \\leftarrow r + s_j(\\alpha_j - \\beta_j)$。\n\n得到的向量 $r$ 即是乘积 $Hg$。最终的搜索方向为 $p = -r$。此过程被实现用于解决每个指定的测试用例。\n\n-   **测试用例 1**：内存 $m=0$，不使用任何曲率对。循环不执行。计算简化为 $p = -H_0 g = -\\gamma g$。对于 $\\gamma = 0.5$ 和 $g = [1, -2, 3]^\\top$，我们有 $p = -0.5 \\times [1, -2, 3]^\\top = [-0.5, 1, -1.5]^\\top$。\n\n-   **测试用例 2**：使用 $m=1$ 个曲率对 $(s_1, y_1)$，应用双循环递归。我们计算 $\\rho_1 = (y_1^\\top s_1)^{-1} = 1/5 = 0.2$。算法得出 $p = [-1.8, 3.4]^\\top$。\n\n-   **测试用例 3**：当 $m=n=3$，曲率对 $(s_i, y_i)$ 中的 $s_i$ 是标准基向量，且对于一个对角矩阵 $A$ 有 $y_i=As_i$ 时，已知 L-BFGS 过程在 $n$ 次更新后能恢复精确的逆 Hessian 矩阵 $H=A^{-1}$。因此，搜索方向为 $p = -A^{-1}g$。对于 $A=\\mathrm{diag}(2,3,4)$ 和 $g = [5, -6, 7]^\\top$，这导致 $p = -[\\mathrm{diag}(0.5, 1/3, 0.25)] [5, -6, 7]^\\top = [-2.5, 2, -1.75]^\\top$。双循环递归验证了此结果。\n\n-   **测试用例 4**：内存 $m=2$，仅使用最后两个曲率对 $(s_3, y_3)$ 和 $(s_4, y_4)$。这些曲率对是正交的。初始矩阵为 $H_0 = I$ ($\\gamma=1$)。L-BFGS 更新实际上只修改了逆 Hessian 近似中对应于由 $\\{s_3, s_4\\}$ 张成的子空间的分量。计算得到的搜索方向为 $p = [-1, -2, -1.5, -8/3]^\\top$。", "answer": "```python\nimport numpy as np\n\ndef compute_lbfgs_direction(m, gamma, s_pairs, y_pairs, g):\n    \"\"\"\n    Computes the L-BFGS search direction p = -Hg using the two-loop recursion.\n\n    Args:\n        m (int): The memory size.\n        gamma (float): The initial scaling factor for H_0.\n        s_pairs (list of np.ndarray): List of 's' vectors {s_k}.\n        y_pairs (list of np.ndarray): List of 'y' vectors {y_k}.\n        g (np.ndarray): The current gradient vector.\n\n    Returns:\n        np.ndarray: The search direction vector p.\n    \"\"\"\n    if m == 0:\n        return -gamma * g\n\n    rhos = [1.0 / (y.T @ s) for s, y in zip(s_pairs, y_pairs)]\n    alphas = np.zeros(m)\n    \n    q = g.copy()\n\n    # First loop: from newest to oldest pair\n    for i in range(m - 1, -1, -1):\n        alphas[i] = rhos[i] * (s_pairs[i].T @ q)\n        q = q - alphas[i] * y_pairs[i]\n\n    r = gamma * q\n\n    # Second loop: from oldest to newest pair\n    for i in range(m):\n        beta = rhos[i] * (y_pairs[i].T @ r)\n        r = r + s_pairs[i] * (alphas[i] - beta)\n        \n    p = -r\n    return p\n\ndef solve():\n    \"\"\"\n    Defines the test cases from the problem statement and computes the results.\n    \"\"\"\n    # Test case 1\n    case1 = {\n        \"m\": 0, \"gamma\": 0.5, \"s_pairs\": [], \"y_pairs\": [],\n        \"g\": np.array([1., -2., 3.])\n    }\n\n    # Test case 2\n    case2 = {\n        \"m\": 1, \"gamma\": 1.0,\n        \"s_pairs\": [np.array([1., 2.])],\n        \"y_pairs\": [np.array([3., 1.])],\n        \"g\": np.array([4., -1.])\n    }\n\n    # Test case 3\n    s1_c3 = np.array([1., 0., 0.])\n    s2_c3 = np.array([0., 1., 0.])\n    s3_c3 = np.array([0., 0., 1.])\n    A_c3 = np.diag([2., 3., 4.])\n    y1_c3 = A_c3 @ s1_c3\n    y2_c3 = A_c3 @ s2_c3\n    y3_c3 = A_c3 @ s3_c3\n    case3 = {\n        \"m\": 3, \"gamma\": 1.0,\n        \"s_pairs\": [s1_c3, s2_c3, s3_c3],\n        \"y_pairs\": [y1_c3, y2_c3, y3_c3],\n        \"g\": np.array([5., -6., 7.])\n    }\n\n    # Test case 4\n    A_c4 = np.array([[4., 1., 0., 0.],\n                     [1., 3., 0., 0.],\n                     [0., 0., 2., 0.],\n                     [0., 0., 0., 1.5]])\n    s1_c4 = np.array([1., 0., 0., 0.])\n    s2_c4 = np.array([0., 1., 0., 0.])\n    s3_c4 = np.array([0., 0., 1., 0.])\n    s4_c4 = np.array([0., 0., 0., 1.])\n    y1_c4 = A_c4 @ s1_c4\n    y2_c4 = A_c4 @ s2_c4\n    y3_c4 = A_c4 @ s3_c4\n    y4_c4 = A_c4 @ s4_c4\n    m_c4 = 2\n    case4 = {\n        \"m\": m_c4, \"gamma\": 1.0,\n        \"s_pairs\": [s3_c4, s4_c4], # Only last m=2 pairs\n        \"y_pairs\": [y3_c4, y4_c4],\n        \"g\": np.array([1., 2., 3., 4.])\n    }\n\n    test_cases = [case1, case2, case3, case4]\n    \n    results = []\n    for case in test_cases:\n        p = compute_lbfgs_direction(case[\"m\"], case[\"gamma\"], case[\"s_pairs\"], case[\"y_pairs\"], case[\"g\"])\n        results.append(p)\n\n    str_results = [str(p.tolist()) for p in results]\n    final_output = f\"[{','.join(str_results)}]\"\n    \n    # Remove all whitespace to match the required output format.\n    print(final_output.replace(\" \", \"\"))\n\nsolve()\n```", "id": "2431082"}]}