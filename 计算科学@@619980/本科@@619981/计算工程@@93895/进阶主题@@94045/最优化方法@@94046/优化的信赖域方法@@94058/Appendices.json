{"hands_on_practices": [{"introduction": "在我们深入研究信赖域方法的细节之前，理解它们试图解决的问题至关重要。这个练习旨在揭示经典线搜索方法在某些情况下的局限性。通过分析一个具有极端曲率（即病态条件）的二次目标函数，你将亲手计算为何一个标准的沿最速下降方向的回溯线搜索策略可能需要非常多次迭代才能找到一个可接受的步长，甚至可能失败 [@problem_id:2447675]。这个例子突显了将步长方向和大小的选择分离的潜在缺陷，从而为信赖域方法这种更稳健的集成策略提供了强有力的动机。", "problem": "一个代表了病态工程设计子问题的双变量严格凸二次函数，由目标函数 $f:\\mathbb{R}^{2}\\to\\mathbb{R}$ 定义，其形式为\n$$\nf(\\mathbf{x}) \\;=\\; \\tfrac{1}{2}\\,\\mathbf{x}^{\\top}\\mathbf{Q}\\,\\mathbf{x}, \\quad \\text{其中} \\quad \\mathbf{Q} \\;=\\; \\begin{pmatrix} 1 & 0 \\\\ 0 & 10^{8} \\end{pmatrix}.\n$$\n考虑起始点 $\\mathbf{x}_{0} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$。对最速下降方向 $-\\nabla f(\\mathbf{x}_{0})$ 应用带有 Armijo 充分下降条件的回溯线搜索，使用的参数为 $\\alpha_{0} = 1$，$\\beta = \\tfrac{1}{2}$ 和 $c_{1} = 10^{-4}$，并允许最多进行 $R_{\\max} = 25$ 次回溯缩减。另外，在点 $\\mathbf{x}_{0}$ 处，考虑一个使用精确二次模型 $m(\\mathbf{p}) = f(\\mathbf{x}_{0}) + \\nabla f(\\mathbf{x}_{0})^{\\top}\\mathbf{p} + \\tfrac{1}{2}\\,\\mathbf{p}^{\\top}\\mathbf{Q}\\,\\mathbf{p}$ 和信赖域半径 $\\Delta = 10^{-2}$ 的信赖域方法。\n\n计算最小整数 $k$，使得在点 $\\mathbf{x}_{0}$ 沿最速下降方向的步长 $\\alpha_{0}\\beta^{k}$ 满足 Armijo 充分下降不等式。请仅提供 $k$ 的值作为最终答案。", "solution": "该问题已经过验证并被认为是有效的。这是一个适定的、有科学依据的数值优化问题，没有矛盾或含糊之处。\n\n目标是找到满足给定二次函数和特定起始点的 Armijo 充分下降条件的最小非负整数 $k$。\n\n目标函数为 $f(\\mathbf{x}) = \\frac{1}{2}\\mathbf{x}^{\\top}\\mathbf{Q}\\mathbf{x}$，其中 Hessian 矩阵为 $\\mathbf{Q} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 10^{8} \\end{pmatrix}$。起始点为 $\\mathbf{x}_{0} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$。\n\n首先，我们计算函数 $f(\\mathbf{x})$ 的梯度，即 $\\nabla f(\\mathbf{x}) = \\mathbf{Q}\\mathbf{x}$。\n在起始点 $\\mathbf{x}_{0}$，梯度为：\n$$\n\\nabla f(\\mathbf{x}_{0}) = \\mathbf{Q}\\mathbf{x}_{0} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 10^{8} \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 10^{8} \\end{pmatrix}\n$$\n搜索方向是最速下降方向，$\\mathbf{p}_{0} = -\\nabla f(\\mathbf{x}_{0})$。\n$$\n\\mathbf{p}_{0} = -\\begin{pmatrix} 1 \\\\ 10^{8} \\end{pmatrix}\n$$\n对于步长 $\\alpha > 0$，Armijo 充分下降条件的表达式为：\n$$\nf(\\mathbf{x}_{0} + \\alpha \\mathbf{p}_{0}) \\le f(\\mathbf{x}_{0}) + c_{1}\\alpha\\nabla f(\\mathbf{x}_{0})^{\\top}\\mathbf{p}_{0}\n$$\n其中 $c_{1} = 10^{-4}$ 是充分下降参数。\n\n由于 $f(\\mathbf{x})$ 是一个二次函数，其在 $\\mathbf{x}_{0}$ 附近的泰勒展开式在二阶项之前是精确的：\n$$\nf(\\mathbf{x}_{0} + \\alpha \\mathbf{p}_{0}) = f(\\mathbf{x}_{0}) + \\alpha \\nabla f(\\mathbf{x}_{0})^{\\top}\\mathbf{p}_{0} + \\frac{1}{2}\\alpha^2 \\mathbf{p}_{0}^{\\top}\\mathbf{Q}\\mathbf{p}_{0}\n$$\n将此式代入 Armijo 不等式，我们得到：\n$$\nf(\\mathbf{x}_{0}) + \\alpha \\nabla f(\\mathbf{x}_{0})^{\\top}\\mathbf{p}_{0} + \\frac{1}{2}\\alpha^2 \\mathbf{p}_{0}^{\\top}\\mathbf{Q}\\mathbf{p}_{0} \\le f(\\mathbf{x}_{0}) + c_{1}\\alpha\\nabla f(\\mathbf{x}_{0})^{\\top}\\mathbf{p}_{0}\n$$\n两边减去 $f(\\mathbf{x}_{0})$ 并除以 $\\alpha$（因为 $\\alpha > 0$）：\n$$\n\\nabla f(\\mathbf{x}_{0})^{\\top}\\mathbf{p}_{0} + \\frac{1}{2}\\alpha \\mathbf{p}_{0}^{\\top}\\mathbf{Q}\\mathbf{p}_{0} \\le c_{1}\\nabla f(\\mathbf{x}_{0})^{\\top}\\mathbf{p}_{0}\n$$\n整理各项以求解 $\\alpha$：\n$$\n\\frac{1}{2}\\alpha \\mathbf{p}_{0}^{\\top}\\mathbf{Q}\\mathbf{p}_{0} \\le (c_{1} - 1)\\nabla f(\\mathbf{x}_{0})^{\\top}\\mathbf{p}_{0}\n$$\n我们现在计算标量 $\\nabla f(\\mathbf{x}_{0})^{\\top}\\mathbf{p}_{0}$ 和 $\\mathbf{p}_{0}^{\\top}\\mathbf{Q}\\mathbf{p}_{0}$。\n项 $\\nabla f(\\mathbf{x}_{0})^{\\top}\\mathbf{p}_{0}$ 是沿最速下降方向的方向导数：\n$$\n\\nabla f(\\mathbf{x}_{0})^{\\top}\\mathbf{p}_{0} = \\nabla f(\\mathbf{x}_{0})^{\\top}(-\\nabla f(\\mathbf{x}_{0})) = -\\|\\nabla f(\\mathbf{x}_{0})\\|_{2}^{2} = -(1^{2} + (10^{8})^{2}) = -(1 + 10^{16})\n$$\n项 $\\mathbf{p}_{0}^{\\top}\\mathbf{Q}\\mathbf{p}_{0}$ 是在方向 $\\mathbf{p}_{0}$ 上的曲率。我们有 $\\mathbf{p}_{0} = \\begin{pmatrix} -1 \\\\ -10^8 \\end{pmatrix}$，因此：\n$$\n\\mathbf{p}_{0}^{\\top}\\mathbf{Q}\\mathbf{p}_{0} = \\begin{pmatrix} -1 & -10^8 \\end{pmatrix} \\begin{pmatrix} 1 & 0 \\\\ 0 & 10^8 \\end{pmatrix} \\begin{pmatrix} -1 \\\\ -10^8 \\end{pmatrix} = \\begin{pmatrix} -1 & -10^8 \\end{pmatrix} \\begin{pmatrix} -1 \\\\ -10^{16} \\end{pmatrix} = (-1)(-1) + (-10^8)(-10^{16}) = 1 + 10^{24}\n$$\n将这些表达式代回关于 $\\alpha$ 的不等式：\n$$\n\\frac{1}{2}\\alpha (1 + 10^{24}) \\le (c_{1} - 1)(-(1 + 10^{16}))\n$$\n由于 $\\nabla f(\\mathbf{x}_{0})^{\\top}\\mathbf{p}_{0}$ 是负的，而 $c_1-1$ 也是负的，所以它们的乘积是正的。由于 $\\mathbf{p}_{0}^{\\top}\\mathbf{Q}\\mathbf{p}_{0}$ 是正的，我们可以分离出 $\\alpha$：\n$$\n\\alpha \\le \\frac{2(1 - c_{1})(1 + 10^{16})}{1 + 10^{24}}\n$$\n步长由回溯过程 $\\alpha = \\alpha_{k} = \\alpha_{0}\\beta^{k}$ 确定，其中 $\\alpha_{0}=1$ 且 $\\beta = \\frac{1}{2}$。因此，$\\alpha_{k} = (\\frac{1}{2})^{k} = 2^{-k}$。我们寻求最小的非负整数 $k$ 使得：\n$$\n2^{-k} \\le \\frac{2(1 - c_{1})(1 + 10^{16})}{1 + 10^{24}}\n$$\n这等价于找到满足以下条件的最小 $k$：\n$$\n2^{k} \\ge \\frac{1 + 10^{24}}{2(1 - c_{1})(1 + 10^{16})}\n$$\n代入 $c_{1} = 10^{-4}$：\n$$\n2^{k} \\ge \\frac{1 + 10^{24}}{2(1 - 10^{-4})(1 + 10^{16})} = \\frac{1 + 10^{24}}{1.9998(1 + 10^{16})}\n$$\n右侧可以被精确近似。由于 $10^{16}$ 和 $10^{24}$ 远大于 $1$：\n$$\n\\frac{1 + 10^{24}}{1.9998(1 + 10^{16})} \\approx \\frac{10^{24}}{1.9998 \\times 10^{16}} \\approx \\frac{1}{1.9998} \\times 10^{8} \\approx 50,005,000.5\n$$\n我们需要找到满足 $2^{k} \\ge 50,005,000.5$ 的最小整数 $k$。我们测试 $2$ 的幂次：\n$$\n2^{25} = 33,554,432\n$$\n$$\n2^{26} = 2 \\times 2^{25} = 67,108,864\n$$\n因为 $2^{25} < 50,005,000.5$ 且 $2^{26} > 50,005,000.5$，所以满足该条件的最小整数 $k$ 是 $k=26$。\n\n值得注意的是，问题指定了最多 $R_{\\max} = 25$ 次回溯缩减，这对应于测试 $k$ 从 $0$ 到 $25$。所需的值 $k=26$ 超出了这个范围。因此，指定的线搜索算法将以失败告终。然而，所提出的问题是纯数学的：找到使不等式成立的最小整数 $k$。这个值是 $26$。", "answer": "$$\n\\boxed{26}\n$$", "id": "2447675"}, {"introduction": "在建立了对信赖域方法的需求后，我们来着手构建其核心部件。信赖域方法的心脏是在一个“信赖域”内最小化一个简单的函数二次模型，这个“子问题”的解决定了下一步的走向 [@problem_id:2224504]。这个练习将引导你完成一个基本的一维信赖域子问题的求解过程。通过这个具体的计算，你将清楚地看到算法是如何在模型的无约束最优点和信赖域边界上的保守步长之间做出抉择的，这是理解信赖域方法工作原理的基石。", "problem": "在无约束优化的背景下，信赖域方法在当前点附近使用一个更简单的模型函数 $m(p)$ 来迭代地逼近一个复杂函数。下一步的步长 $p$ 是通过求解信赖域子问题来确定的，该子问题涉及在一个半径为 $\\Delta > 0$ 的“信赖域”内最小化此模型，在该区域内模型被认为是原始函数的可靠近似。该子问题可以正式表述为：\n$$\n\\min_{p} m(p) \\quad \\text{subject to} \\quad \\|p\\| \\le \\Delta\n$$\n考虑一个一维优化场景，其中步长 $p \\in \\mathbb{R}$ 的模型函数是一个二次函数，由下式给出：\n$$\nm(p) = g p + \\frac{1}{2} H p^2 + c\n$$\n其中梯度项 $g = 2$，Hessian项 $H = 6$，以及一个任意常数 $c$。步长受限于一个半径为 $\\Delta = 0.1$ 的信赖域。\n\n确定求解此一维信赖域子问题的最优步长 $p$。请以精确小数形式提供您的答案。", "solution": "我们必须在信赖域约束 $|p| \\le \\Delta$ 下最小化二次模型 $m(p) = g p + \\frac{1}{2} H p^{2} + c$，其中给定值为 $g=2$，$H=6$ 和 $\\Delta=0.1$。常数 $c$ 不影响最小化问题的解，可以忽略。\n\n考虑该信赖域子问题的拉格朗日函数：\n$$\n\\mathcal{L}(p,\\lambda) = g p + \\frac{1}{2} H p^{2} + \\lambda \\left(p^{2} - \\Delta^{2}\\right),\n$$\n其中 $\\lambda \\ge 0$。Karush-Kuhn-Tucker (KKT) 条件如下：\n1. 平稳性 (Stationarity)：\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial p} = g + H p + 2 \\lambda p = 0.\n$$\n2. 原始可行性 (Primal feasibility)：$|p| \\le \\Delta$。\n3. 对偶可行性 (Dual feasibility)：$\\lambda \\ge 0$。\n4. 互补松弛性 (Complementary slackness)：$\\lambda \\left(p^{2} - \\Delta^{2}\\right) = 0$。\n\n情况1（内部解）：如果 $|p| < \\Delta$，则根据互补松弛性，$\\lambda = 0$，平稳性条件给出\n$$\ng + H p = 0 \\quad \\Rightarrow \\quad p_{u} = -\\frac{g}{H}.\n$$\n对于 $g=2$ 和 $H=6$，这得到 $p_{u} = -\\frac{2}{6} = -\\frac{1}{3}$。检查可行性：$|p_{u}| = \\frac{1}{3} > 0.1 = \\Delta$，因此内部解是不可行的。\n\n情况2（边界解）：此时 $p^{2} = \\Delta^{2}$，所以 $p = \\pm \\Delta$。平稳性条件变为\n$$\ng + H p + 2 \\lambda p = 0 \\quad \\Rightarrow \\quad (H + 2 \\lambda) p = -g.\n$$\n由于 $H > 0$ 且 $\\lambda \\ge 0$，因此 $H + 2 \\lambda > 0$。$p$ 的符号必须与 $-g$ 的符号相同。当 $g = 2 > 0$ 时，我们必须取 $p = -\\Delta = -0.1$。为了验证对偶可行性，求解 $\\lambda$：\n$$\n(H + 2 \\lambda)(-\\Delta) = -g \\quad \\Rightarrow \\quad (H + 2 \\lambda)\\Delta = g \\quad \\Rightarrow \\quad 2 \\lambda = \\frac{g}{\\Delta} - H.\n$$\n代入 $g=2$，$\\Delta=0.1$ 和 $H=6$ 可得\n$$\n2 \\lambda = \\frac{2}{0.1} - 6 = 20 - 6 = 14 \\quad \\Rightarrow \\quad \\lambda = 7 \\ge 0,\n$$\n这满足了对偶可行性和互补松弛性。\n\n因此，最优信赖域步长是负梯度方向上的边界步长：\n$$\np^{\\star} = -\\Delta = -0.1.\n$$", "answer": "$$\\boxed{-0.1}$$", "id": "2224504"}, {"introduction": "这个终极练习将之前的所有概念——子问题求解、步长评估和信赖域半径更新——整合到一个完整的算法中 [@problem_id:2461247]。你将通过编写代码，亲手实现一个信赖域优化器，并用它来求解一个具有非凸区域的典型双阱势函数。通过实践，你将直观地感受到信赖域方法如何稳健地在复杂的能量面上导航，并能有效避免纯牛顿法等简单方法可能遇到的“过射”或陷入局部最大值等问题，从而成功收敛到函数的真正最小值。", "problem": "考虑以无量纲单位给出的单变量目标函数 $f(x)=x^4-x^2$，其一阶导数为 $f'(x)=4x^3-2x$，二阶导数为 $f''(x)=12x^2-2$。驻点为位于 $x=0$ 的局部极大值点和位于 $x_{\\pm}=\\pm 1/\\sqrt{2}$ 的局部极小值点。对于一个迭代点 $x_k\\in\\mathbb{R}$，定义二次模型 $m_k(s)=f(x_k)+f'(x_k)s+\\tfrac{1}{2}f''(x_k)s^2$ 和一个信赖域半径 $\\Delta_k>0$。信赖域子问题是找到一个步长 $s_k\\in\\mathbb{R}$，在约束 $|s|\\le \\Delta_k$ 下最小化 $m_k(s)$。令预测下降量为 $\\operatorname{pred}_k=-(m_k(s_k)-m_k(0))=-(f'(x_k)s_k+\\tfrac{1}{2}f''(x_k)s_k^2)$，实际下降量为 $\\operatorname{ared}_k=f(x_k)-f(x_k+s_k)$。定义接受率 $\\rho_k=\\operatorname{ared}_k/\\operatorname{pred}_k$（当 $\\operatorname{pred}_k>0$ 时）；如果 $\\operatorname{pred}_k\\le 0$，则设置 $\\rho_k=-\\infty$。如果 $\\rho_k\\ge \\eta_1$，则接受该步长，否则拒绝。信赖域半径根据以下规则更新，其中参数 $\\eta_1\\in(0,1)$、$\\eta_2\\in(\\eta_1,1)$、$\\gamma_1\\in(0,1)$ 和 $\\gamma_2>1$ 为固定值：\n- 如果 $\\rho_k<\\eta_1$，则设置 $\\Delta_{k+1}=\\gamma_1\\Delta_k$。\n- 如果 $\\rho_k\\ge \\eta_2$ 且 $|s_k|\\ge 0.8\\,\\Delta_k$，则设置 $\\Delta_{k+1}=\\min\\{\\gamma_2\\Delta_k,\\Delta_{\\max}\\}$。\n- 否则，设置 $\\Delta_{k+1}=\\Delta_k$。\n如果一个步长被拒绝，则保持 $x_{k+1}=x_k$；如果被接受，则设置 $x_{k+1}=x_k+s_k$。当满足 $|f'(x_k)|\\le \\varepsilon_g$、对于一个被接受的步长 $|s_k|\\le \\varepsilon_s$、$\\Delta_k\\le \\varepsilon_s$ 中任意一个条件，或达到固定的迭代次数上限时，迭代终止。\n\n在一维空间中，受约束 $|s|\\le \\Delta$ 的二次模型的唯一全局极小值点可按如下方式表征。对于给定的 $g\\in\\mathbb{R}$ 和 $h\\in\\mathbb{R}$，其中 $g=f'(x)$ 且 $h=f''(x)$，\n- 如果 $h>0$ 且无约束极小值点 $s_N=-g/h$ 满足 $|s_N|\\le \\Delta$，则 $s^\\star=s_N$；否则 $s^\\star=-\\operatorname{sign}(g)\\,\\Delta$。\n- 如果 $h\\le 0$，则当 $g<0$ 时 $s^\\star=\\Delta$，当 $g>0$ 时 $s^\\star=-\\Delta$，当 $g=0$ 时 $s^\\star=\\Delta$。\n\n固定数值参数 $\\eta_1=0.1$、$\\eta_2=0.9$、$\\gamma_1=0.25$、$\\gamma_2=2$、$\\Delta_{\\max}=10$、梯度容差 $\\varepsilon_g=10^{-8}$、步长容差 $\\varepsilon_s=10^{-10}$ 以及最大迭代次数为 $100$ 次。对于下方的每个测试用例，从给定的初始点 $x_0$ 和初始半径 $\\Delta_0$ 开始，并在每一步中使用一维模型极小值点 $s_k$ 应用上述迭代过程。\n\n为每个测试用例定义三个标量输出：\n- $b_1$：一个逻辑值，当且仅当没有任何被接受的步长曾增加目标函数值时为真，即对于所有被接受的步长 $k$，$f(x_{k+1})\\le f(x_k)$。\n- $b_2$：一个逻辑值，当且仅当在第一个迭代点处的完整无约束 Newton 步长 $s_N=-f'(x_0)/f''(x_0)$（当 $f''(x_0)\\ne 0$ 时；如果 $f''(x_0)=0$ 则定义 $b_2$ 为假）产生了一个更高的目标函数值时为真，即 $f(x_0+s_N)>f(x_0)$。\n- $d$：从最终接受的迭代点 $x_{\\mathrm{final}}$ 到最近的局部极小值点的绝对距离，$d=\\min\\{|x_{\\mathrm{final}}-1/\\sqrt{2}|,|x_{\\mathrm{final}}+1/\\sqrt{2}|\\}$。\n\n待评估的测试套件参数：\n1. $(x_0,\\Delta_0)=(0.1,0.05)$\n2. $(x_0,\\Delta_0)=(0.1,2.0)$\n3. $(x_0,\\Delta_0)=(0.0,0.5)$\n4. $(x_0,\\Delta_0)=(0.1,0.001)$\n\n您的程序应生成单行输出，其中包含用方括号括起来的逗号分隔列表形式的结果。对于每个测试用例，输出一个列表 $[b_1,b_2,d]$，其中 $b_1$ 和 $b_2$ 是小写字符串“true”或“false”，$d$ 是一个精确到小数点后六位的小数（浮点数）。因此，最终输出必须是形如\n$[[b_{1,1},b_{1,2},d_1],[b_{2,1},b_{2,2},d_2],[b_{3,1},b_{3,2},d_3],[b_{4,1},b_{4,2},d_4]]$ 的单行文本，行内任何地方都没有空格。", "solution": "该问题是有效的。它提出了一个在数值优化领域的适定、自洽且科学上可靠的练习，具体而言是关于将信赖域算法应用于一维势能函数。所有必要的参数、算法规则、函数和终止准则都以数学精度给出。我们将继续进行完整的求解。\n\n这个问题的核心是实现并分析一个信赖域优化算法。这类方法在计算科学中是基础性的，尤其是在计算化学中用于定位稳定的分子几何构型，这些构型对应于势能面上的极小值点。目标函数 $f(x) = x^4 - x^2$ 是一个典型的一维双阱势，代表一个在 $x_{\\pm} = \\pm 1/\\sqrt{2}$ 处有两个稳定态（极小值）并在 $x=0$ 处有一个不稳定过渡态（极大值）的系统。\n\n信赖域算法通过构建目标函数的一个更简单的模型来迭代地寻找极小值，该模型仅在当前迭代点 $x_k$ 的一个邻域内是可信的。这个邻域是一个半径为 $\\Delta_k$ 的“信赖域”。该模型是一个二次函数 $m_k(s)$，它源自 $f(x)$ 在 $x_k$ 附近的二阶泰勒展开：\n$$m_k(s) = f(x_k) + f'(x_k)s + \\frac{1}{2}f''(x_k)s^2$$\n其中 $s$ 是从 $x_k$ 出发的步长。该模型在约束 $|s| \\le \\Delta_k$ 下对 $s$ 进行最小化。这个约束最小化问题被称为信赖域子问题。\n\n如前所述，一维子问题的解取决于模型的曲率，该曲率由二阶导数 $h = f''(x_k)$ 给出。\n1.  如果 $h > 0$，模型是凸的（一个开口向上的抛物线）。无约束极小值点是 Newton 步长 $s_N = -g/h$，其中 $g = f'(x_k)$。如果此步长位于信赖域内，即 $|s_N| \\le \\Delta_k$，它就是最优步长 $s_k$。否则，模型在信赖域的边界处取得最小值，即 $s_k = -\\operatorname{sign}(g)\\Delta_k$，这表示在最速下降方向上尽可能地移动。\n2.  如果 $h \\le 0$，模型是局部凹的或线性的。它在区间 $[-\\Delta_k, \\Delta_k]$ 上的最小值必定位于某个边界上。在 $s = \\Delta_k$ 和 $s = -\\Delta_k$ 之间的选择由梯度 $g$ 的符号决定，梯度符号指示了下降方向。\n\n一旦计算出试探步长 $s_k$，就需要通过比较目标函数的*实际下降量* $\\operatorname{ared}_k = f(x_k) - f(x_k + s_k)$ 和模型给出的*预测下降量* $\\operatorname{pred}_k = m_k(0) - m_k(s_k)$ 来评估其质量。它们的比率 $\\rho_k = \\operatorname{ared}_k / \\operatorname{pred}_k$ 衡量了模型的保真度。\n\n-   如果 $\\rho_k$ 接近 1，说明模型是一个出色的预测器。步长被接受，并且我们可以扩大信赖域（$\\Delta_{k+1} = \\gamma_2 \\Delta_k$）以允许更激进的步长，前提是当前步长已经接近信赖域边界。\n-   如果 $\\rho_k$ 为正但不大，说明模型是足够的。步长被接受，但信赖域大小保持不变（$\\Delta_{k+1} = \\Delta_k$）。\n-   如果 $\\rho_k$ 很小或为负，说明模型很差。步长被拒绝（$x_{k+1} = x_k$），并且缩小信赖域（$\\Delta_{k+1} = \\gamma_1 \\Delta_k$）以在后续迭代中提高模型精度。\n\n步长接受规则是 $\\rho_k \\ge \\eta_1$。由于 $\\eta_1 = 0.1 > 0$ 且子问题的解确保了 $\\operatorname{pred}_k \\ge 0$，任何被接受的步长都必须满足 $\\operatorname{ared}_k \\ge \\eta_1 \\operatorname{pred}_k \\ge 0$。如果 $\\operatorname{pred}_k > 0$，则 $\\operatorname{ared}_k > 0$，从而保证了 $f(x_{k+1}) < f(x_k)$。一个步长只有在模型预测会下降（$\\operatorname{pred}_k > 0$）时才可能被接受。因此，对于 $b_1$ 的条件——即没有任何被接受的步长会增加目标函数——根据该算法的构造本身就保证为真。任何偏差都将表明实现存在缺陷。\n\n输出 $b_2$ 探究了纯 Newton-Raphson 方法的局限性。无约束 Newton 步长 $s_N = -f'(x_0)/f''(x_0)$ 找到了二次模型的极值点。在极大值点附近，例如在 $x_0=0.0$ 或 $x_0=0.1$ 处，Hessian 矩阵 $f''(x_0)$ 是负的。因此，Newton 步长寻求的是局部二次模型的*极大值*，这对于最小化全局函数 $f(x)$ 来说是一个糟糕的策略，并且很可能导致一个上升步，即 $f(x_0+s_N) > f(x_0)$。信赖域框架通过约束步长大小来纠正这个缺陷。\n\n最终输出 $d$ 衡量了算法收敛到其中一个真实极小值点 $x_{\\pm} = \\pm 1/\\sqrt{2}$ 的精度。鉴于所有初始点都是非负的，该算法预计将收敛到正的极小值点 $x_+ = 1/\\sqrt{2}$。\n\n实现过程将首先定义目标函数及其导数。然后，对每个测试用例计算 $b_2$ 的值。接着执行主迭代循环，在每一步 $k$ 中包括：检查终止条件，求解信赖域子问题以获得 $s_k$，通过 $\\rho_k$ 评估步长质量，并根据指定规则更新状态变量 $x_k$ 和 $\\Delta_k$。在整个迭代过程中跟踪 $b_1$ 的值。终止时，计算最终距离 $d$。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the trust-region optimization problem for the given test cases.\n    \"\"\"\n\n    # --- Problem Definition ---\n    def f(x):\n        return x**4 - x**2\n\n    def f_prime(x):\n        return 4 * x**3 - 2 * x\n\n    def f_double_prime(x):\n        return 12 * x**2 - 2\n\n    # --- Algorithm Parameters ---\n    eta1 = 0.1\n    eta2 = 0.9\n    gamma1 = 0.25\n    gamma2 = 2.0\n    delta_max = 10.0\n    eps_g = 1e-8\n    eps_s = 1e-10\n    max_iter = 100\n    \n    minimizers = [-1/np.sqrt(2), 1/np.sqrt(2)]\n\n    # --- Test Cases ---\n    test_cases = [\n        (0.1, 0.05),\n        (0.1, 2.0),\n        (0.0, 0.5),\n        (0.1, 0.001)\n    ]\n\n    results = []\n\n    for x0, delta0 in test_cases:\n        # --- Output variable initialization ---\n        b1_flag = True\n        \n        # --- Calculate b2 before starting iterations ---\n        g0 = f_prime(x0)\n        h0 = f_double_prime(x0)\n        \n        if h0 == 0:\n            b2 = False\n        else:\n            s_N = -g0 / h0\n            b2 = f(x0 + s_N) > f(x0)\n            \n        # --- Main Trust-Region Loop ---\n        x_k = x0\n        delta_k = delta0\n        final_x = x0 # Will hold the last accepted iterate value\n        \n        for _ in range(max_iter):\n            g_k = f_prime(x_k)\n            \n            # --- Termination check 1: Gradient ---\n            if abs(g_k) <= eps_g:\n                final_x = x_k\n                break\n\n            # --- Solve the trust-region subproblem ---\n            h_k = f_double_prime(x_k)\n            s_k = 0.0\n            \n            if h_k > 0:\n                s_N = -g_k / h_k\n                if abs(s_N) <= delta_k:\n                    s_k = s_N\n                else:\n                    s_k = -np.sign(g_k) * delta_k\n            else: # h_k <= 0\n                if g_k < 0:\n                    s_k = delta_k\n                elif g_k > 0:\n                    s_k = -delta_k\n                else: # g_k == 0\n                    s_k = delta_k\n\n            # --- Evaluate step quality ---\n            pred_k = -(g_k * s_k + 0.5 * h_k * s_k**2)\n            ared_k = f(x_k) - f(x_k + s_k)\n            \n            rho_k = 0.0\n            # Use small tolerance for pred_k to avoid division by zero instability\n            if pred_k > 1e-12: # Check for meaningful predicted reduction\n                rho_k = ared_k / pred_k\n            else:\n                rho_k = -np.inf # Model predicts no improvement or a trivial step\n\n            delta_kp1 = delta_k\n            \n            # --- Step acceptance/rejection and state update ---\n            if rho_k >= eta1: # Accept step\n                if f(x_k + s_k) > f(x_k):\n                    b1_flag = False\n                \n                x_k += s_k\n                final_x = x_k\n                \n                # --- Termination check 2: Step size for an accepted step ---\n                if abs(s_k) <= eps_s:\n                    break\n                    \n                # --- Update trust radius (for accepted step) ---\n                if rho_k >= eta2 and abs(s_k) >= 0.8 * delta_k:\n                    delta_kp1 = min(gamma2 * delta_k, delta_max)\n                # else: delta_kp1 remains delta_k\n                \n            else: # Reject step\n                # x_k remains the same\n                delta_kp1 = gamma1 * delta_k\n            \n            delta_k = delta_kp1\n            \n            # --- Termination check 3: Trust radius size ---\n            if delta_k <= eps_s:\n                break\n        \n        # --- Calculate final distance d ---\n        d = min(abs(final_x - m) for m in minimizers)\n        \n        # Format results for the current test case\n        b1_str = \"true\" if b1_flag else \"false\"\n        b2_str = \"true\" if b2 else \"false\"\n        d_str = \"{:.6f}\".format(d)\n        results.append(f'[{b1_str},{b2_str},{d_str}]')\n        \n    # --- Final Print ---\n    # The final output is a single line, formatted as a list of lists.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "2461247"}]}