{"hands_on_practices": [{"introduction": "本节的第一个练习是奠基性的实践。它将指导您从头开始构建一个完整、可用的非线性共轭梯度求解器。通过将您编写的求解器应用于包括挑战性的 Rosenbrock 函数和病态二次函数在内的一系列基准测试问题，您将亲身体验该算法的强大功能，并观察它如何在不同的优化地形中寻找最优解。[@problem_id:2418452]", "problem": "给定欧几里得空间上的若干个可微目标函数及初始点。您的任务是编写一个完整的程序，对每种情况，仅使用函数值和精确的一阶导数来计算一个近似的最小化子（minimizer）。您的程序所使用的梯度对于给定的目标函数必须在机器精度内是精确的；请勿使用任何有限差分近似。请勿使用任何超过一阶导数的信息。使用基于梯度欧几里得范数的终止准则。\n\n数学设定：\n\n- 设 $f:\\mathbb{R}^n \\to \\mathbb{R}$ 是一个连续可微函数，并用 $\\nabla f(\\mathbf{x})$ 表示其梯度。从一个给定的初始点 $\\mathbf{x}_0 \\in \\mathbb{R}^n$ 开始，仅使用 $f$ 和 $\\nabla f$ 的求值来计算一个序列 $\\{\\mathbf{x}_k\\}$，以尝试最小化 $f$。\n- 当 $\\|\\nabla f(\\mathbf{x}_k)\\|_2 \\le \\varepsilon$ 或迭代次数达到指定的最大值时终止。对 $\\|\\cdot\\|_2$ 使用欧几里得范数。\n- 算法不得使用任何二阶信息（无海森矩阵或海森向量积），也不得使用有限差分导数近似。程序必须在机器精度内计算指定函数的精确梯度。\n\n测试套件：\n\n对于下文的每种情况，都给定了函数 $f(\\mathbf{x})$、维度 $n$ 和初始点 $\\mathbf{x}_0$。\n\n- 情况 A（非凸、窄谷、双变量）：\n  - 维度：$n=2$。\n  - 目标：\n    $$f(\\mathbf{x}) = 100\\,(x_2 - x_1^2)^2 + (1 - x_1)^2.$$\n  - 初始点：$\\mathbf{x}_0 = (-1.2,\\; 1.0)$。\n\n- 情况 B（病态可分二次函数，五变量）：\n  - 维度：$n=5$。\n  - 目标：\n    $$f(\\mathbf{x}) = \\tfrac{1}{2}\\sum_{i=1}^{5} \\lambda_i x_i^2,\\quad \\lambda_i = 10^{\\,i-1}.$$\n  - 初始点：$\\mathbf{x}_0 = (1,\\; -1,\\; 1,\\; -1,\\; 1)$。\n\n- 情况 C（光滑、耦合、非凸，三变量）：\n  - 维度：$n=3$。\n  - 目标：\n    $$f(\\mathbf{x}) = 0.1\\,(x_1^2 + x_2^2 + x_3^2) + \\sin(x_1)\\cos(x_2) + e^{x_3} - x_3.$$\n  - 初始点：$\\mathbf{x}_0 = (0.5,\\; -0.5,\\; 0.0)$。\n\n- 情况 D（已处于最小化子，四变量）：\n  - 维度：$n=4$。\n  - 目标：\n    $$f(\\mathbf{x}) = \\sum_{i=1}^{4} (x_i - 1)^2.$$\n  - 初始点：$\\mathbf{x}_0 = (1,\\; 1,\\; 1,\\; 1).$\n\n要求：\n\n- 停止容差：对梯度范数使用 $\\varepsilon = 10^{-6}$。\n- 最大迭代次数：每种情况使用 $N_{\\max} = 10000$。\n- 您的程序所使用的梯度对于给定的目标函数必须在机器精度内是精确的。\n- 程序不得读取任何输入，除了下文描述的最后一行外，不得写入任何输出。\n\n输出规范：\n\n- 对于每种情况，报告终止时的最终目标值 $f(\\mathbf{x}_\\star)$，四舍五入到小数点后六位。\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表。该列表必须按顺序包含情况 A、情况 B、情况 C 和情况 D 的四个四舍五入后的目标值。例如，一个有效的输出格式是：\n  - \"[0.000123,0.000000,1.234567,0.000000]\"\n\n不涉及物理单位。根据数学惯例，三角函数中使用的角度以弧度为单位。输出必须是实数，并且必须遵循上述确切格式。", "solution": "所提出的问题是一个标准的数值优化任务，要求最小化若干个定义明确的可微函数。该方法被限制为一阶方法，这意味着它只能使用函数值 $f(\\mathbf{x})$ 和梯度值 $\\nabla f(\\mathbf{x})$。禁止使用二阶信息，如海森矩阵。在这些约束条件下，非线性共轭梯度（NCG）法是一种非常合适且高效的算法。虽然更简单的最速下降法也满足这些约束，但对于具有高曲率或病态条件的问题，例如 Rosenbrock 函数（情况 A）和具有悬殊特征值的给定二次函数（情况 B），其收敛速度是出了名的慢。NCG 方法通过构建作为梯度共轭式扩展的搜索方向来加速收敛，从而有效地融合了先前步骤的信息。\n\n非线性 NCG 算法的迭代过程，从初始点 $\\mathbf{x}_0$ 开始，对 $k=0, 1, 2, \\dots$ 定义如下：\n1. 计算当前迭代点的梯度：$\\mathbf{g}_k = \\nabla f(\\mathbf{x}_k)$。\n2. 检查收敛性：如果梯度的欧几里得范数 $\\|\\mathbf{g}_k\\|_2$ 低于指定的容差 $\\varepsilon$，则算法终止。\n3. 计算搜索方向 $\\mathbf{p}_k$。对于第一次迭代（$k=0$），方向是最速下降方向，$\\mathbf{p}_0 = -\\mathbf{g}_0$。对于后续迭代（$k > 0$），方向是当前负梯度和前一个搜索方向的线性组合：\n   $$\n   \\mathbf{p}_k = -\\mathbf{g}_k + \\beta_k \\mathbf{p}_{k-1}\n   $$\n   标量 $\\beta_k$ 决定了 NCG 方法的具体变体。\n4. 执行线搜索，以确定沿方向 $\\mathbf{p}_k$ 的合适步长 $\\alpha_k > 0$。\n5. 更新迭代点：$\\mathbf{x}_{k+1} = \\mathbf{x}_k + \\alpha_k \\mathbf{p}_k$。\n\n对于此实现，选择 Polak–Ribière–Polyak (PRP) 公式来计算 $\\beta_k$，因为与其他公式（如 Fletcher-Reeves）相比，它通常具有更优越的经验性能。PRP 公式为：\n$$\n\\beta_k^{\\text{PRP}} = \\frac{\\mathbf{g}_k^T (\\mathbf{g}_k - \\mathbf{g}_{k-1})}{\\mathbf{g}_{k-1}^T \\mathbf{g}_{k-1}}\n$$\n为了提高鲁棒性并保证全局收敛性，这被增强为 PRP+ 方法，其中 $\\beta_k = \\max(0, \\beta_k^{\\text{PRP}})$。如果 $\\beta_k^{\\text{PRP}}$ 变为负值（这在远离局部最小值时可能发生），此修改可防止算法采取不良的步长。此外，作为一种保障措施，如果搜索方向 $\\mathbf{p}_k$ 不再是下降方向（即，如果 $\\mathbf{p}_k^T \\mathbf{g}_k \\ge 0$），它将被强制重置为最速下降方向 $-\\mathbf{g}_k$。\n\n步长 $\\alpha_k$ 是通过满足强 Wolfe 条件的线搜索找到的：\n1. 充分下降（Armijo）条件：$f(\\mathbf{x}_k + \\alpha_k \\mathbf{p}_k) \\le f(\\mathbf{x}_k) + c_1 \\alpha_k \\mathbf{g}_k^T \\mathbf{p}_k$\n2. 曲率条件：$|\\nabla f(\\mathbf{x}_k + \\alpha_k \\mathbf{p}_k)^T \\mathbf{p}_k| \\le c_2 |\\mathbf{g}_k^T \\mathbf{p}_k|$\n常数选择为标准值 $c_1 = 10^{-4}$ 和 $c_2 = 0.9$。这些条件确保每一步都能在目标函数值上实现有意义的减少。使用 `scipy.optimize.line_search` 函数来实现这一步。\n\n要求解析梯度是精确的。四个测试用例的梯度推导如下：\n\n情况 A：Rosenbrock 函数，$f(\\mathbf{x}) = 100(x_2 - x_1^2)^2 + (1 - x_1)^2$ 对于 $\\mathbf{x} \\in \\mathbb{R}^2$。\n$$\n\\nabla f(\\mathbf{x}) = \\begin{pmatrix} \\frac{\\partial f}{\\partial x_1} \\\\ \\frac{\\partial f}{\\partial x_2} \\end{pmatrix} = \\begin{pmatrix} -400x_1(x_2 - x_1^2) - 2(1 - x_1) \\\\ 200(x_2 - x_1^2) \\end{pmatrix}\n$$\n\n情况 B：病态可分二次函数，$f(\\mathbf{x}) = \\frac{1}{2}\\sum_{i=1}^{5} \\lambda_i x_i^2$ 且 $\\lambda_i = 10^{i-1}$ 对于 $\\mathbf{x} \\in \\mathbb{R}^5$。\n每个 $x_j$ 的梯度分量是 $\\frac{\\partial f}{\\partial x_j} = \\lambda_j x_j$。\n$$\n\\nabla f(\\mathbf{x})_j = 10^{j-1} x_j, \\quad \\text{for } j=1, \\dots, 5\n$$\n\n情况 C：光滑、耦合、非凸函数，$f(\\mathbf{x}) = 0.1(x_1^2 + x_2^2 + x_3^2) + \\sin(x_1)\\cos(x_2) + e^{x_3} - x_3$ 对于 $\\mathbf{x} \\in \\mathbb{R}^3$。\n$$\n\\nabla f(\\mathbf{x}) = \\begin{pmatrix} 0.2x_1 + \\cos(x_1)\\cos(x_2) \\\\ 0.2x_2 - \\sin(x_1)\\sin(x_2) \\\\ 0.2x_3 + e^{x_3} - 1 \\end{pmatrix}\n$$\n\n情况 D：简单二次函数，$f(\\mathbf{x}) = \\sum_{i=1}^{4} (x_i - 1)^2$ 对于 $\\mathbf{x} \\in \\mathbb{R}^4$。\n每个 $x_j$ 的梯度分量是 $\\frac{\\partial f}{\\partial x_j} = 2(x_j-1)$。\n$$\n\\nabla f(\\mathbf{x})_j = 2(x_j - 1), \\quad \\text{for } j=1, \\dots, 4\n$$\n对于这种情况，初始点 $\\mathbf{x}_0 = (1, 1, 1, 1)$ 是该函数的唯一全局最小值。因此，$\\nabla f(\\mathbf{x}_0) = \\mathbf{0}$，算法在第 $k=0$ 次迭代时立即终止，目标值为 $f(\\mathbf{x}_0)=0$。\n\n该实现将这些元素组合成一个单一的程序。一个通用的求解器函数封装了 NCG 逻辑，并针对每个测试用例，使用各自的目标函数、梯度和初始点来调用它。当 $\\|\\nabla f(\\mathbf{x}_k)\\|_2 \\le 10^{-6}$ 或在 10000 次迭代后，程序终止。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import optimize\n\ndef conjugate_gradient_solver(f, grad_f, x0, tol=1e-6, max_iter=10000):\n    \"\"\"\n    Minimizes a function using the Nonlinear Conjugate Gradient method (Polak-Ribière-Polyak+).\n    \"\"\"\n    x_k = np.array(x0, dtype=float)\n    f_k = f(x_k)\n    g_k = grad_f(x_k)\n    grad_norm = np.linalg.norm(g_k)\n\n    if grad_norm <= tol:\n        return f(x_k)\n\n    p_k = -g_k\n    \n    k = 0\n    while k < max_iter and grad_norm > tol:\n        # Perform line search to find alpha_k satisfying strong Wolfe conditions.\n        # c1=1e-4 and c2=0.9 are standard for CG.\n        try:\n            line_search_result = optimize.line_search(f, grad_f, x_k, p_k, gfk=g_k, old_fval=f_k, c1=1e-4, c2=0.9)\n            alpha_k = line_search_result[0]\n        except Exception:\n            # line_search can sometimes raise errors for extreme values\n            alpha_k = None\n\n        # If line search fails, restart with steepest descent.\n        if alpha_k is None:\n            p_k = -g_k\n            try:\n                line_search_result = optimize.line_search(f, grad_f, x_k, p_k, gfk=g_k, old_fval=f_k, c1=1e-4, c2=0.9)\n                alpha_k = line_search_result[0]\n            except Exception:\n                alpha_k = None\n            \n            if alpha_k is None:\n                # If it still fails, terminate. Could be due to precision limits.\n                break\n\n        x_k_plus_1 = x_k + alpha_k * p_k\n        g_k_plus_1 = grad_f(x_k_plus_1)\n\n        # Polak-Ribière-Polyak+ update for beta\n        g_k_dot_g_k = np.dot(g_k, g_k)\n        if g_k_dot_g_k == 0:\n            beta_k_plus_1 = 0.0\n        else:\n            beta_numerator = np.dot(g_k_plus_1, g_k_plus_1 - g_k)\n            beta_k_plus_1 = max(0, beta_numerator / g_k_dot_g_k)\n        \n        # New search direction\n        p_k_plus_1 = -g_k_plus_1 + beta_k_plus_1 * p_k\n\n        # Check for descent direction. If not, reset to steepest descent.\n        if np.dot(p_k_plus_1, g_k_plus_1) >= 0:\n            p_k_plus_1 = -g_k_plus_1\n\n        # Update variables for the next iteration\n        x_k = x_k_plus_1\n        g_k = g_k_plus_1\n        p_k = p_k_plus_1\n        f_k = f(x_k) # Can be taken from line_search output, but re-evaluating is simple.\n        \n        grad_norm = np.linalg.norm(g_k)\n        k += 1\n\n    return f(x_k)\n\ndef solve():\n    # Final print statement in the exact required format.\n    \n    # Case A: Rosenbrock function\n    def f_A(x):\n        return 100.0 * (x[1] - x[0]**2)**2 + (1.0 - x[0])**2\n\n    def grad_f_A(x):\n        df_dx1 = -400.0 * x[0] * (x[1] - x[0]**2) - 2.0 * (1.0 - x[0])\n        df_dx2 = 200.0 * (x[1] - x[0]**2)\n        return np.array([df_dx1, df_dx2])\n\n    # Case B: Ill-conditioned separable quadratic\n    def f_B(x):\n        lambdas = 10.**np.arange(len(x))\n        return 0.5 * np.sum(lambdas * x**2)\n\n    def grad_f_B(x):\n        lambdas = 10.**np.arange(len(x))\n        return lambdas * x\n\n    # Case C: Smooth, coupled, nonconvex\n    def f_C(x):\n        term1 = 0.1 * np.sum(x**2)\n        term2 = np.sin(x[0]) * np.cos(x[1])\n        term3 = np.exp(x[2]) - x[2]\n        return term1 + term2 + term3\n\n    def grad_f_C(x):\n        df_dx1 = 0.2 * x[0] + np.cos(x[0]) * np.cos(x[1])\n        df_dx2 = 0.2 * x[1] - np.sin(x[0]) * np.sin(x[1])\n        df_dx3 = 0.2 * x[2] + np.exp(x[2]) - 1.0\n        return np.array([df_dx1, df_dx2, df_dx3])\n    \n    # Case D: Simple quadratic\n    def f_D(x):\n        return np.sum((x - 1.0)**2)\n\n    def grad_f_D(x):\n        return 2.0 * (x - 1.0)\n    \n    test_cases = [\n        {'f': f_A, 'grad_f': grad_f_A, 'x0': [-1.2, 1.0]},\n        {'f': f_B, 'grad_f': grad_f_B, 'x0': [1.0, -1.0, 1.0, -1.0, 1.0]},\n        {'f': f_C, 'grad_f': grad_f_C, 'x0': [0.5, -0.5, 0.0]},\n        {'f': f_D, 'grad_f': grad_f_D, 'x0': [1.0, 1.0, 1.0, 1.0]}\n    ]\n\n    results = []\n    for case in test_cases:\n        final_f_val = conjugate_gradient_solver(\n            f=case['f'],\n            grad_f=case['grad_f'],\n            x0=case['x0'],\n            tol=1e-6,\n            max_iter=10000\n        )\n        results.append(final_f_val)\n\n    # Format output as specified\n    formatted_results = [\"{:.6f}\".format(r) for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "2418452"}, {"introduction": "一个优化算法不仅仅在于其搜索方向，步长的选择也同样至关重要。本练习将通过让您比较一个使用恰当线搜索的 NCG 求解器和一个使用固定步长的“有缺陷”的求解器，来突显 Armijo 充分下降条件的的关键作用。通过精心设计的测试案例，您将发现为什么系统性的线搜索不是可有可无的附加项，而是保证算法收敛的基本要求。[@problem_id:2418455]", "problem": "您的任务是实现并比较两种非线性共轭梯度法的变体，用于无约束最小化一个连续可微函数。第一种变体是标准的非线性共轭梯度算法，通过回溯线搜索强制满足第一 Wolfe 条件（Armijo 充分下降条件）。第二种变体是故意设置的错误版本：它不强制满足 Armijo 条件，而是在每次迭代中使用固定的单位步长。您的任务是通过精心选择的测试函数来证明，即使在标准方法能够收敛的情况下，省略 Armijo 条件也可能导致不收敛或发散。\n\n从以下基础理论出发：\n- 对于一个连续可微的目标函数 $f:\\mathbb{R}^n \\to \\mathbb{R}$，下降方法生成迭代点 $x_{k+1} = x_k + \\alpha_k d_k$，其中搜索方向 $d_k$ 满足 $g_k^\\top d_k < 0$，$g_k = \\nabla f(x_k)$，且 $\\alpha_k > 0$ 是步长。\n- 第一 Wolfe (Armijo) 充分下降条件要求，对于常数 $c_1 \\in (0,1)$，步长满足\n$$\nf(x_k + \\alpha_k d_k) \\le f(x_k) + c_1 \\alpha_k g_k^\\top d_k.\n$$\n- 在非线性共轭梯度法中，搜索方向按如下方式构造\n$$\nd_0 = -g_0,\\quad d_k = -g_k + \\beta_k d_{k-1}\\ \\text{for}\\ k \\ge 1,\n$$\n其中 $\\beta_k$ 的经典选择之一是 Polak–Ribiere–Plus，并带有一项保护措施，即如果 $g_k^\\top d_k \\ge 0$，则重置 $d_k=-g_k$ 以保持下降特性。\n- 对于形如 $f(x) = \\tfrac{1}{2} x^\\top Q x$ 的光滑凸二次目标函数，其中 $Q$ 是对称正定矩阵，使用固定步长 $\\alpha$ 的梯度下降法会产生线性迭代 $x_{k+1} = (I - \\alpha Q) x_k$。当且仅当谱半径满足 $\\rho(I - \\alpha Q) < 1$ 时，迭代会收敛到最小值点 $x^\\star=0$，这等价于 $0 < \\alpha < 2/\\lambda_{\\max}(Q)$。\n\n您的程序必须实现：\n- 一个标准的非线性共轭梯度求解器，它使用回溯线搜索来强制满足 Armijo 条件，其中常数 $c_1 \\in (0,1)$ 和回溯率 $\\tau \\in (0,1)$ 由用户选择。\n- 一个错误的非线性共轭梯度求解器，它对所有 $k$ 均使用 $\\alpha_k \\equiv 1$（无充分下降检查）。\n\n您必须遵循的设计细节：\n- 使用 Polak–Ribiere–Plus 选择共轭参数 $\\beta_k$，并在计算出的方向不是下降方向时使用重置保护措施。\n- 当 $\\|\\nabla f(x_k)\\|_2 \\le \\varepsilon$（给定容差 $\\varepsilon > 0$）或达到固定的迭代次数上限时终止。\n- 如果在耗尽迭代次数上限后仍未满足梯度容差，则声明为不收敛。如果目标函数值变为非数值（not-a-number）、超过一个很大的阈值，或者迭代点的范数超过一个很大的阈值，则声明为发散。\n\n构建一个测试套件以展示不同的行为：\n- 测试 A（发散见证）：一个凸二次函数 $f(x) = \\tfrac{1}{2} x^\\top Q x$，其中 $Q = \\mathrm{diag}(10.0, 0.1)$，初始点为 $x_0 = [1.0, 1.0]$。根据谱半径准则，固定步长 $\\alpha = 1$ 违反了 $0 < \\alpha < 2/\\lambda_{\\max}(Q)$，因为 $\\lambda_{\\max}(Q) = 10.0$，因此错误方法预计会发散，而强制执行 Armijo 条件的方法应该会收敛。\n- 测试 B（理想情况）：一个良态的凸二次函数，其中 $Q = \\mathrm{diag}(0.5, 0.25)$，初始点为 $x_0 = [2.0, -3.0]$。此时，$\\lambda_{\\max}(Q) = 0.5$，因此固定步长 $\\alpha = 1$ 满足 $0 < \\alpha < 2/\\lambda_{\\max}(Q)$，两种方法都应该收敛。\n- 测试 C（边界条件）：同样的凸二次函数 $Q = \\mathrm{diag}(1.0, 1.0)$，初始点为 $x_0 = [0.0, 0.0]$，这已是最小值点。两种方法都应立即检测到收敛。\n- 测试 D（非凸压力测试）：一个缩放的 Rosenbrock 函数 $f(x_1,x_2) = 10\\,(x_2 - x_1^2)^2 + (1 - x_1)^2$，初始点为 $x_0 = [-1.2, 1.0]$。错误方法的单位步长可能导致在此弯曲的谷底发生数值爆炸，而采用 Armijo 回溯的标准方法应能收敛到 $[1,1]$ 附近的最小值点。\n\n您的程序中要使用的数值参数：\n- 梯度范数容差 $\\varepsilon = 10^{-6}$，最大迭代次数 $N_{\\max} = 5000$，Armijo 常数 $c_1 = 10^{-4}$，回溯率 $\\tau = 0.5$。\n- 发散阈值：如果 $\\|x_k\\|_2 > 10^{12}$ 或 $f(x_k) > 10^{50}$ 或 $f(x_k)$ 为非数值，则声明为发散。\n\n您的程序必须：\n- 实现两个求解器，在所有四个测试上运行它们，并根据观察到的行为为每个测试确定一个整数代码：\n    - 如果标准方法收敛而错误方法不收敛（不收敛或发散），则输出 $1$。\n    - 如果两者都收敛，则输出 $0$。\n    - 如果两者都不收敛，则输出 $-1$。\n    - 如果错误方法收敛而标准方法不收敛，则输出 $2$。\n- 生成单行输出，包含一个逗号分隔的列表，并用方括号括起来。例如，输出格式必须类似于单个 Python 风格的列表字面量，如 [r1,r2,r3,r4]，其中每个条目是指定的整数代码之一。\n\n不涉及角度单位。此问题中没有物理单位。所有量都是无量纲的。输出必须严格遵循指定的单行格式。", "solution": "所提出的问题是实现并比较用于无约束优化的非线性共轭梯度（NCG）法的两种变体。一种变体被正确实现，它遵循线搜索方法的基本原则，强制执行 Armijo 充分下降条件。第二种变体是故意设置的有缺陷的版本，它采用固定的单位步长，从而省略了线搜索这一关键保障。目标是通过计算证明，在正确实现的算法能够成功的问​​题上，省略 Armijo 条件可能导致失败，具体表现为不收敛或发散。该问题定义明确，科学上合理，并为算法实现和验证提供了清晰的基础。\n\n考虑一个一般的无约束最小化问题，旨在寻找一个连续可微目标函数 $f: \\mathbb{R}^n \\to \\mathbb{R}$ 的局部最小值点。NCG 方法是一种迭代算法，它使用以下更新规则生成一系列点 $\\{x_k\\}_{k \\ge 0}$：\n$$\nx_{k+1} = x_k + \\alpha_k d_k\n$$\n此处，$x_k \\in \\mathbb{R}^n$ 是当前迭代点，$d_k \\in \\mathbb{R}^n$ 是搜索方向，$\\alpha_k > 0$ 是步长。目标函数在 $x_k$ 处的梯度表示为 $g_k = \\nabla f(x_k)$。\n\n搜索方向 $d_k$ 被构造成一个下降方向，即 $g_k^\\top d_k < 0$。NCG 方向是递归定义的。初始方向是最速下降方向 $d_0 = -g_0$。对于后续的迭代 $k \\ge 1$，方向是当前负梯度和前一个方向的线性组合：\n$$\nd_k = -g_k + \\beta_k d_{k-1}\n$$\n标量 $\\beta_k$ 是共轭参数。问题指定了 Polak–Ribière–Plus 变体，该变体以其强大的数值性能而闻名。其定义如下：\n$$\n\\beta_k = \\max \\left\\{ 0, \\frac{g_k^\\top(g_k - g_{k-1})}{\\|g_{k-1}\\|_2^2} \\right\\}\n$$\n这种选择包含了一个非负约束，有助于在特定条件下确保全局收敛。一个至关重要的保障是重置条件：如果计算出的方向 $d_k$ 未能成为下降方向（即，如果 $g_k^\\top d_k \\ge 0$），则通过将搜索方向设为最速下降方向 $d_k = -g_k$ 来重置该方法。\n\n本研究的核心在于步长 $\\alpha_k$ 的确定。\n\n**标准的 NCG 方法**采用回溯线搜索来找到一个满足 Armijo 充分下降条件的步长 $\\alpha_k$。对于给定的常数 $c_1 \\in (0, 1)$，该条件为：\n$$\nf(x_k + \\alpha_k d_k) \\le f(x_k) + c_1 \\alpha_k g_k^\\top d_k\n$$\n这个不等式确保了目标函数的减少量至少是在 $x_k$ 处 $f$ 的线性近似所预测的减少量的一小部分。回溯过程从一个初始试探步长（通常为 $\\alpha = 1$）开始，并以一个因子 $\\tau \\in (0, 1)$（例如 $\\alpha \\leftarrow \\tau \\alpha$）迭代地减小它，直到满足该条件。指定的参数是 $c_1 = 10^{-4}$ 和 $\\tau = 0.5$。\n\n**错误的 NCG 方法**绕过了这个关键检查，并简单地对所有迭代 $k \\ge 0$ 设置 $\\alpha_k = 1$。虽然对于某些表现良好的函数或者初始迭代点接近解时这可能是可以接受的，但通常这是一种不可靠的策略，可能导致失败。\n\n算法的终止由梯度的范数决定。如果 $\\|\\nabla f(x_k)\\|_2 \\le \\varepsilon$ 对于容差 $\\varepsilon = 10^{-6}$，则认为迭代收敛。如果迭代次数超过预算 $N_{\\max} = 5000$，则中止过程，并归类为不收敛。如果迭代点范数 $\\|x_k\\|_2$ 超过 $10^{12}$，函数值 $f(x_k)$ 超过 $10^{50}$，或者 $f(x_k)$ 变为非数值（NaN），则声明为发散。\n\n分析是在一套旨在揭示两种方法不同行为的四个测试案例上进行的。\n\n**测试 A**：一个凸二次函数 $f(x) = \\frac{1}{2} x^\\top Q x$，其中 Hessian 矩阵 $Q = \\mathrm{diag}(10.0, 0.1)$ 条件不佳。对于二次函数，使用固定步长 $\\alpha$ 的 NCG 方法等价于线性迭代系统 $x_{k+1} = (I - \\alpha Q) x_k$。该系统收敛当且仅当迭代矩阵的谱半径 $\\rho(I - \\alpha Q)$ 小于 1。当 $\\alpha=1$ 时，$I-Q$ 的特征值是 $1-10.0 = -9.0$ 和 $1-0.1 = 0.9$。谱半径为 $\\rho(I - Q) = \\max\\{|-9.0|, |0.9|\\} = 9.0$，大于 1。因此，错误的方法保证会发散。具有来自 Armijo 条件的自适应步长的标准方法预计会收敛。\n\n**测试 B**：一个良态的凸二次函数，其中 $Q = \\mathrm{diag}(0.5, 0.25)$。在这里，对于步长 $\\alpha=1$ 的错误方法，$I-Q$ 的特征值是 $1-0.5 = 0.5$ 和 $1-0.25=0.75$。谱半径是 $\\rho(I - Q) = 0.75 < 1$，满足收敛条件。因此，错误方法和标准方法都预计会收敛。\n\n**测试 C**：一个凸二次函数，其初始点 $x_0 = [0.0, 0.0]$ 是全局最小值点。初始梯度 $\\nabla f(x_0) = 0$。两个算法都必须在第一次迭代前检查终止条件并立即声明收敛。\n\n**测试 D**：非凸 Rosenbrock 函数，$f(x_1,x_2) = 10(x_2 - x_1^2)^2 + (1-x_1)^2$，从起始点 $x_0 = [-1.2, 1.0]$ 开始。这个函数是一个经典的基准测试，其特点是具有一个狭窄、弯曲的谷底。错误方法的固定单位步长很可能导致迭代点“跳过”谷底，从而导致函数值增加和不稳定的行为，可能引起发散或不收敛。相比之下，标准方法的回溯线搜索将系统地减小步长以确保充分下降，从而使迭代点能够沿着谷底走向位于 $[1,1]$ 的最小值点。\n\n每个测试的结果是一个整数代码：如果标准方法收敛而错误方法不收敛，则为 $1$；如果两者都收敛，则为 $0$；如果两者都不收敛，则为 $-1$；如果错误方法收敛但标准方法不收敛，则为 $2$。这种系统性的比较为线搜索机制在确保基于下降的优化算法的鲁棒性方面所起的不可或缺的作用提供了明确的证据。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and compares a proper and a faulty nonlinear conjugate gradient method.\n    \"\"\"\n\n    # --- Numerical Parameters ---\n    EPSILON = 1e-6\n    MAX_ITER = 5000\n    C1 = 1e-4\n    TAU = 0.5\n    DIV_NORM_THRESHOLD = 1e12\n    DIV_F_THRESHOLD = 1e50\n\n    def nonlinear_cg(f, grad_f, x0, use_armijo):\n        \"\"\"\n        Nonlinear Conjugate Gradient (NCG) solver.\n\n        Args:\n            f: Objective function.\n            grad_f: Gradient of the objective function.\n            x0: Initial point.\n            use_armijo: Boolean flag to use Armijo line search.\n\n        Returns:\n            A string indicating the outcome: \"converged\", \"nonconverged\", \"diverged\".\n        \"\"\"\n        x_k = np.copy(x0).astype(np.float64)\n        g_k = grad_f(x_k)\n        \n        # Initial check for convergence at x0\n        norm_g_k = np.linalg.norm(g_k)\n        if norm_g_k <= EPSILON:\n            return \"converged\"\n\n        d_k = -g_k\n        k = 0\n\n        while k < MAX_ITER:\n            # Line Search\n            if use_armijo:\n                alpha_k = 1.0\n                descent_condition_val = C1 * np.dot(g_k, d_k)\n                # The dot product g_k.T @ d_k should be < 0 due to safeguard\n                try:\n                    f_k = f(x_k)\n                    while f(x_k + alpha_k * d_k) > f_k + alpha_k * descent_condition_val:\n                        alpha_k *= TAU\n                        if alpha_k < 1e-15: # Prevent infinite loop if step size becomes too small\n                           return \"nonconverged\"\n                except (OverflowError, ValueError):\n                    return \"diverged\" # f() evaluation might fail\n            else:\n                alpha_k = 1.0\n\n            # Update position\n            x_k_plus_1 = x_k + alpha_k * d_k\n\n            # Check for divergence\n            try:\n                f_next = f(x_k_plus_1)\n                if np.linalg.norm(x_k_plus_1) > DIV_NORM_THRESHOLD or f_next > DIV_F_THRESHOLD or np.isnan(f_next):\n                    return \"diverged\"\n            except (OverflowError, ValueError):\n                return \"diverged\"\n\n            g_k_plus_1 = grad_f(x_k_plus_1)\n            norm_g_k_plus_1 = np.linalg.norm(g_k_plus_1)\n\n            # Check for convergence\n            if norm_g_k_plus_1 <= EPSILON:\n                return \"converged\"\n\n            # Polak-Ribiere-Plus (PR+) for beta\n            norm_g_k_sq = norm_g_k**2\n            if norm_g_k_sq > 1e-14: # Safeguard against division by zero\n                beta_k_plus_1 = max(0, np.dot(g_k_plus_1, g_k_plus_1 - g_k) / norm_g_k_sq)\n            else:\n                beta_k_plus_1 = 0.0\n\n            # Update search direction\n            d_k_plus_1 = -g_k_plus_1 + beta_k_plus_1 * d_k\n\n            # Restart if not a descent direction\n            if np.dot(g_k_plus_1, d_k_plus_1) >= 0:\n                d_k_plus_1 = -g_k_plus_1\n\n            # Prepare for next iteration\n            x_k = x_k_plus_1\n            g_k = g_k_plus_1\n            norm_g_k = norm_g_k_plus_1\n            d_k = d_k_plus_1\n            k += 1\n\n        return \"nonconverged\"\n    \n    # --- Test Case Definitions ---\n\n    # Test A: Divergence Witness\n    Q_A = np.diag([10.0, 0.1])\n    def f_A(x): return 0.5 * x.T @ Q_A @ x\n    def grad_f_A(x): return Q_A @ x\n    x0_A = np.array([1.0, 1.0])\n\n    # Test B: Happy Path\n    Q_B = np.diag([0.5, 0.25])\n    def f_B(x): return 0.5 * x.T @ Q_B @ x\n    def grad_f_B(x): return Q_B @ x\n    x0_B = np.array([2.0, -3.0])\n\n    # Test C: Boundary Condition\n    Q_C = np.diag([1.0, 1.0])\n    def f_C(x): return 0.5 * x.T @ Q_C @ x\n    def grad_f_C(x): return Q_C @ x\n    x0_C = np.array([0.0, 0.0])\n\n    # Test D: Nonconvex Stress Test (Scaled Rosenbrock)\n    def f_D(x): return 10.0 * (x[1] - x[0]**2)**2 + (1.0 - x[0])**2\n    def grad_f_D(x):\n        df_dx1 = -40.0 * x[0] * (x[1] - x[0]**2) - 2.0 * (1.0 - x[0])\n        df_dx2 = 20.0 * (x[1] - x[0]**2)\n        return np.array([df_dx1, df_dx2])\n    x0_D = np.array([-1.2, 1.0])\n\n    test_cases = [\n        (f_A, grad_f_A, x0_A),\n        (f_B, grad_f_B, x0_B),\n        (f_C, grad_f_C, x0_C),\n        (f_D, grad_f_D, x0_D),\n    ]\n\n    results = []\n    for f, grad_f, x0 in test_cases:\n        proper_status = nonlinear_cg(f, grad_f, x0, use_armijo=True)\n        faulty_status = nonlinear_cg(f, grad_f, x0, use_armijo=False)\n\n        proper_converged = (proper_status == \"converged\")\n        faulty_converged = (faulty_status == \"converged\")\n\n        if proper_converged and not faulty_converged:\n            results.append(1)\n        elif proper_converged and faulty_converged:\n            results.append(0)\n        elif not proper_converged and not faulty_converged:\n            results.append(-1)\n        elif not proper_converged and faulty_converged:\n            results.append(2)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2418455"}, {"introduction": "虽然我们期望目标函数值 $f(x)$ 在每一步都下降，但其他衡量进展的指标是否也同样表现得如此可预测？本练习挑战了一个常见的直觉，即梯度范数 $\\|\\nabla f(x_k)\\|_2$ 也必须单调递减，优化才能成功。通过在非凸问题上实施并观察 NCG 算法，您将看到通往最小值的路径可能会涉及梯度大小的暂时增加，从而对收敛过程的动态有更深刻的理解。[@problem_id:2418485]", "problem": "您必须编写一个完整且可运行的程序，该程序针对一组固定的目标函数和起始点，判断在基于非线性共轭梯度（NCG）的优化过程中，梯度（用 $\\lVert \\nabla f(x_k) \\rVert_2$ 表示）的欧几里得范数是否为非单调的。该任务的形式化描述如下。\n\n设 $f : \\mathbb{R}^n \\to \\mathbb{R}$ 是一个连续可微的函数。考虑一个生成序列 $\\{x_k\\}_{k \\ge 0}$ 的迭代方案，其搜索方向为 $\\{d_k\\}_{k \\ge 0}$，步长为 $\\{\\alpha_k\\}_{k \\ge 0}$，梯度为 $g_k \\equiv \\nabla f(x_k)$。更新规则为 $x_{k+1} = x_k + \\alpha_k d_k$。搜索方向 $d_k$ 由下式递归定义：\n$$\nd_k = -g_k + \\beta_k d_{k-1} \\quad (\\text{对于 } k \\ge 1),\n$$\n且初始搜索方向为 $d_0 = -g_0$。其中 $\\beta_k$ 根据经典的 NCG 公式之一进行选择：\n- Fletcher–Reeves (FR)：$\\beta_k^{\\mathrm{FR}} = \\dfrac{\\langle g_k, g_k \\rangle}{\\langle g_{k-1}, g_{k-1} \\rangle}$，\n- Polak–Ribière–Polyak 非负修正 (PR+)：$\\beta_k^{\\mathrm{PR}+} = \\max\\!\\left\\{0,\\; \\dfrac{\\langle g_k, g_k - g_{k-1} \\rangle}{\\langle g_{k-1}, g_{k-1} \\rangle}\\right\\}$，\n- Hestenes–Stiefel 非负修正 (HS+)：$\\beta_k^{\\mathrm{HS}+} = \\max\\!\\left\\{0,\\; \\dfrac{\\langle g_k, g_k - g_{k-1} \\rangle}{\\langle d_{k-1}, g_k - g_{k-1} \\rangle}\\right\\}$，\n其中 $\\langle \\cdot,\\cdot \\rangle$ 表示 $\\mathbb{R}^n$ 中的标准内积。\n\n步长 $\\alpha_k$ 必须满足强 Wolfe 条件，该条件沿着线搜索函数 $\\phi_k(\\alpha) = f(x_k + \\alpha d_k)$ 定义，常数为 $c_1 \\in (0,1)$ 和 $c_2 \\in (c_1,1)$：\n$$\n\\phi_k(\\alpha_k) \\le \\phi_k(0) + c_1 \\alpha_k \\phi_k'(0), \n\\quad\n\\lvert \\phi_k'(\\alpha_k) \\rvert \\le c_2 \\lvert \\phi_k'(0) \\rvert,\n$$\n其中 $\\phi_k'(\\alpha) = \\nabla f(x_k + \\alpha d_k)^\\top d_k$。如果计算出的搜索方向 $d_k$ 不是下降方向，即，如果 $\\langle d_k, g_k \\rangle \\ge 0$，则通过设置 $d_k \\leftarrow -g_k$ 来强制重启。\n\n您的程序必须实现此方法，并对下面列出的每个测试用例，计算算法生成的有限序列 $\\{\\lVert g_k \\rVert_2\\}_{k=0}^{K}$，直到满足 $\\lVert g_k \\rVert_2 \\le \\varepsilon$ 或 $k$ 达到给定的迭代次数上限为止。然后，为每个用例输出一个布尔值，指示梯度范数序列是否为非单调，意即，存在一个索引 $k$ 使得 $\\lVert g_{k+1} \\rVert_2 > \\lVert g_k \\rVert_2$。\n\n对所有测试用例使用以下固定常量：$c_1 = 10^{-4}$、$c_2 = 0.9$、初始步长 $\\alpha_0 = 1$、梯度容差 $\\varepsilon = 10^{-8}$、以及最大迭代次数 $K_{\\max} = 500$。所有内积和范数均为欧几里得范数，所有数字均为实数。\n\n您必须使用以下测试套件，包括目标函数、维度、起始点和 $\\beta_k$ 变体：\n\n- 测试用例 1（非凸，二维）：\n  - 目标函数：二维 Rosenbrock 函数\n    $$\n    f_1(x) = (1 - x_1)^2 + 100\\,(x_2 - x_1^2)^2,\n    $$\n    其中 $x = (x_1, x_2)^\\top \\in \\mathbb{R}^2$。\n  - 起始点：$x_0 = (-1.2,\\, 1.0)^\\top$。\n  - 变体：$\\beta_k = \\beta_k^{\\mathrm{PR}+}$。\n\n- 测试用例 2（严格凸二次函数，三维）：\n  - 目标函数：球面二次函数\n    $$\n    f_2(x) = \\tfrac{1}{2}\\,\\lVert x \\rVert_2^2,\n    $$\n    其中 $x \\in \\mathbb{R}^3$。\n  - 起始点：$x_0 = (3,\\, -4,\\, 1)^\\top$。\n  - 变体：$\\beta_k = \\beta_k^{\\mathrm{FR}}$。\n\n- 测试用例 3（非凸，五维）：\n  - 目标函数：5 维 Rosenbrock 函数\n    $$\n    f_3(x) = \\sum_{i=1}^{4} \\left[(1 - x_i)^2 + 100\\,\\left(x_{i+1} - x_i^2\\right)^2\\right],\n    $$\n    其中 $x = (x_1,\\dots,x_5)^\\top \\in \\mathbb{R}^5$。\n  - 起始点：$x_0 = (-1.2,\\, 1.0,\\, -1.2,\\, 1.0,\\, -1.2)^\\top$。\n  - 变体：$\\beta_k = \\beta_k^{\\mathrm{HS}+}$。\n\n对于每个测试用例，您必须返回一个布尔值，其定义如下：\n- 如果存在至少一个索引 $k$ 使得 $\\lVert g_{k+1} \\rVert_2 > \\lVert g_k \\rVert_2$，则返回 true。\n- 否则返回 false。\n\n最终输出格式：您的程序应生成单行输出，包含用方括号括起来的、以逗号分隔的结果列表（例如，“[result1,result2,result3]”）。结果必须按照上述测试用例的顺序出现。此问题不涉及物理单位。不使用角度。不使用百分比。输出元素为布尔值。", "solution": "所提出的问题是数值优化领域中一个明确定义的计算任务。它要求实现非线性共轭梯度（NCG）方法，以在一组指定的目标函数和起始条件下，确定梯度范数序列是否表现出非单调行为。该问题具有科学依据，形式化规范，并且所有必要的参数均已提供。因此，该问题被认为是有效的。\n\n问题的核心是求解一个形式如下的无约束优化问题\n$$\n\\min_{x \\in \\mathbb{R}^n} f(x)\n$$\n其中 $f$ 是一个连续可微的函数。NCG 方法从初始点 $x_0$ 开始生成一个迭代序列 $\\{x_k\\}$。更新规则由下式给出\n$$\nx_{k+1} = x_k + \\alpha_k d_k\n$$\n其中 $d_k$ 是搜索方向，$\\alpha_k > 0$ 是步长。\n\n初始搜索方向是最速下降方向，即 $d_0 = -g_0$，其中 $g_k \\equiv \\nabla f(x_k)$ 是目标函数在 $x_k$ 处的梯度。对于后续的迭代（$k \\ge 1$），搜索方向计算为当前负梯度和前一个搜索方向的线性组合：\n$$\nd_k = -g_k + \\beta_k d_{k-1}\n$$\n系数 $\\beta_k$ 至关重要，它定义了 NCG 方法的具体变体。问题指定了三种经典的 $\\beta_k$ 公式：\n\n1. Fletcher–Reeves (FR)：\n$$\n\\beta_k^{\\mathrm{FR}} = \\frac{\\langle g_k, g_k \\rangle}{\\langle g_{k-1}, g_{k-1} \\rangle}\n$$\n其中 $\\langle \\cdot, \\cdot \\rangle$ 表示标准欧几里得内积。\n\n2. Polak–Ribière–Polyak 非负约束 (PR+)：\n$$\n\\beta_k^{\\mathrm{PR}+} = \\max\\left\\{0, \\frac{\\langle g_k, g_k - g_{k-1} \\rangle}{\\langle g_{k-1}, g_{k-1} \\rangle}\\right\\}\n$$\n当方法远离解时，此修正可以防止 $\\beta_k$ 值过大，从而可以提高性能。\n\n3. Hestenes–Stiefel 非负约束 (HS+)：\n$$\n\\beta_k^{\\mathrm{HS}+} = \\max\\left\\{0, \\frac{\\langle g_k, g_k - g_{k-1} \\rangle}{\\langle d_{k-1}, g_k - g_{k-1} \\rangle}\\right\\}\n$$\n这个变体通常被认为是有效的，但需要谨慎实现，因为分母理论上可能变为零。然而，当与满足强 Wolfe 条件的线搜索结合使用时，分母保证为正。\n\n步长 $\\alpha_k$ 通过线搜索过程确定，以确保函数值充分下降和算法的收敛性。问题强制要求使用强 Wolfe 条件：\n1. Armijo（充分下降）条件：$f(x_k + \\alpha_k d_k) \\le f(x_k) + c_1 \\alpha_k \\langle g_k, d_k \\rangle$。\n2. 强曲率条件：$|\\langle \\nabla f(x_k + \\alpha_k d_k), d_k \\rangle| \\le c_2 |\\langle g_k, d_k \\rangle|$。\n常数被指定为 $c_1 = 10^{-4}$ 和 $c_2 = 0.9$，这是标准的选择。这些条件确保了步长既有效又不会过小。为此，使用了 `scipy.optimize.line_search` 函数，因为它为寻找这样的 $\\alpha_k$ 提供了一个稳健的实现。\n\n现代 NCG 方法的一个关键方面是重启策略。如果计算出的搜索方向 $d_k$ 不是下降方向，即 $\\langle g_k, d_k \\rangle \\ge 0$，则通过丢弃之前的方向信息并将 $d_k$ 设置为 $-g_k$ 来重置该方法。这确保了每一步都有助于最小化目标函数。\n\n整个算法流程如下：\n1. 初始化 $k=0$，$x_0$，并计算 $g_0 = \\nabla f(x_0)$。设置 $d_0 = -g_0$。维护一个梯度范数的记录。\n2. 对于 $k = 0, 1, 2, \\dots, K_{\\max}-1$：\n    a. 检查收敛性：如果 $\\lVert g_k \\rVert_2 \\le \\varepsilon = 10^{-8}$，则终止。\n    b. 确保 $d_k$ 是一个下降方向。如果 $\\langle g_k, d_k \\rangle \\ge 0$，则设置 $d_k = -g_k$。\n    c. 使用初始猜测值 $\\alpha=1$ 执行线搜索，以找到满足强 Wolfe 条件（常数为 $c_1=10^{-4}$ 和 $c_2=0.9$）的 $\\alpha_k > 0$。\n    d. 更新位置：$x_{k+1} = x_k + \\alpha_k d_k$。\n    e. 计算新梯度 $g_{k+1} = \\nabla f(x_{k+1})$。\n    f. 检查非单调性：比较 $\\lVert g_{k+1} \\rVert_2$ 和 $\\lVert g_k \\rVert_2$。如果 $\\lVert g_{k+1} \\rVert_2 > \\lVert g_k \\rVert_2$，则发生了非单调事件。\n    g. 使用为测试用例指定的公式（FR、PR+ 或 HS+）计算 $\\beta_{k+1}$。\n    h. 更新搜索方向：$d_{k+1} = -g_{k+1} + \\beta_{k+1} d_k$。\n3. 如果循环完成但未收敛，则在 $k = K_{\\max} = 500$ 处终止。\n\n此过程应用于三个测试用例：\n- 用例 1：二维 Rosenbrock 函数，一个经典的非凸基准问题，使用 PR+ 方法。其梯度为 $\\nabla f_1(x) = [400x_1^3 - 400x_1x_2 + 2x_1 - 2, 200(x_2 - x_1^2)]^\\top$。\n- 用例 2：一个简单的三维严格凸二次函数，使用 FR 方法。其梯度为 $\\nabla f_2(x) = x$。\n- 用例 3：五维 Rosenbrock 函数，扩展了用例 1 的挑战，使用 HS+ 方法。其梯度表达式更复杂，其首、末和中间分量有不同的公式。\n\n对于每个用例，跟踪一个布尔标志，以确定是否有任何步骤 $k$ 导致 $\\lVert g_{k+1} \\rVert_2 > \\lVert g_k \\rVert_2$。最终输出是这些布尔值的列表。该实现将封装在一个 Python 脚本中，使用 `numpy` 进行线性代数运算，使用 `scipy.optimize.line_search` 进行线搜索，并严格遵守问题的规范。", "answer": "```python\nimport numpy as np\nfrom scipy.optimize import line_search\n\ndef solve():\n    \"\"\"\n    Solves the main problem by running NCG for three test cases.\n    \"\"\"\n\n    # --- Global Constants ---\n    C1 = 1e-4\n    C2 = 0.9\n    EPSILON = 1e-8\n    K_MAX = 500\n\n    # --- Objective Functions and Gradients ---\n\n    # Test Case 1 & 3: Rosenbrock function and its gradient\n    def rosenbrock_f(x):\n        return np.sum(100.0 * (x[1:] - x[:-1]**2.0)**2.0 + (1.0 - x[:-1])**2.0)\n\n    def rosenbrock_grad(x):\n        n = len(x)\n        grad = np.zeros(n)\n        # Gradient for x_i, 1 < i < n (0-indexed: 0 < i < n-1)\n        grad[1:-1] = (200 * (x[1:-1] - x[:-2]**2)\n                      - 400 * (x[2:] - x[1:-1]**2) * x[1:-1]\n                      - 2 * (1 - x[1:-1]))\n        # Gradient for x_1 (0-indexed: x_0)\n        grad[0] = -400 * x[0] * (x[1] - x[0]**2) - 2 * (1 - x[0])\n        # Gradient for x_n (0-indexed: x_{n-1})\n        grad[-1] = 200 * (x[-1] - x[-2]**2)\n        return grad\n\n    # Test Case 2: Spherical quadratic function and its gradient\n    def quadratic_f(x):\n        return 0.5 * np.dot(x, x)\n\n    def quadratic_grad(x):\n        return x\n\n    # --- NCG Algorithm Implementation ---\n\n    def run_ncg(f, grad_f, x0, beta_method):\n        \"\"\"\n        Runs the Nonlinear Conjugate Gradient algorithm for a given function.\n\n        Args:\n            f: The objective function.\n            grad_f: The gradient of the objective function.\n            x0: The starting point.\n            beta_method: The formula for beta ('FR', 'PR+', 'HS+').\n\n        Returns:\n            A boolean indicating if the gradient norm was non-monotonic.\n        \"\"\"\n        k = 0\n        x = np.copy(x0)\n        \n        g = grad_f(x)\n        grad_norm = np.linalg.norm(g)\n        f_val = f(x)\n        \n        d = -g\n        \n        is_non_monotonic = False\n\n        while k < K_MAX and grad_norm > EPSILON:\n            # Restart if not a descent direction\n            if np.dot(g, d) >= 0:\n                d = -g\n\n            # Perform line search satisfying strong Wolfe conditions\n            # old_old_fval=None ensures the initial step guess is 1.0\n            alpha, _, _, f_val_new, _, g_new = line_search(\n                f=f, myfprime=grad_f, xk=x, pk=d, gfk=g, old_fval=f_val,\n                c1=C1, c2=C2, old_old_fval=None\n            )\n            \n            if alpha is None:\n                # Line search failed, terminate optimization\n                break\n\n            # Update position\n            x_new = x + alpha * d\n            \n            # Check for non-monotonicity in gradient norm\n            grad_norm_new = np.linalg.norm(g_new)\n            if grad_norm_new > grad_norm:\n                is_non_monotonic = True\n\n            # Calculate beta for the next iteration\n            y = g_new - g\n            g_dot_g = np.dot(g, g)\n            \n            if g_dot_g == 0:\n                beta = 0.0 # Should not happen due to termination condition\n            elif beta_method == 'FR':\n                beta = np.dot(g_new, g_new) / g_dot_g\n            elif beta_method == 'PR+':\n                beta = max(0.0, np.dot(g_new, y) / g_dot_g)\n            elif beta_method == 'HS+':\n                denom = np.dot(d, y)\n                if denom == 0:\n                    beta = 0.0 # Restart if denominator is zero\n                else:\n                    beta = max(0.0, np.dot(g_new, y) / denom)\n            else:\n                raise ValueError(\"Unknown beta method\")\n\n            # Update search direction\n            d_new = -g_new + beta * d\n            \n            # Prepare for next iteration\n            k += 1\n            x, g, f_val, d = x_new, g_new, f_val_new, d_new\n            grad_norm = grad_norm_new\n            \n        return is_non_monotonic\n\n    # --- Test Case Definitions ---\n    test_cases = [\n        {\n            'f': rosenbrock_f,\n            'grad_f': rosenbrock_grad,\n            'x0': np.array([-1.2, 1.0]),\n            'beta_method': 'PR+'\n        },\n        {\n            'f': quadratic_f,\n            'grad_f': quadratic_grad,\n            'x0': np.array([3.0, -4.0, 1.0]),\n            'beta_method': 'FR'\n        },\n        {\n            'f': rosenbrock_f,\n            'grad_f': rosenbrock_grad,\n            'x0': np.array([-1.2, 1.0, -1.2, 1.0, -1.2]),\n            'beta_method': 'HS+'\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        result = run_ncg(case['f'], case['grad_f'], case['x0'], case['beta_method'])\n        results.append(result)\n\n    # --- Final Output ---\n    # Convert booleans to lowercase strings as per implied format\n    output_str = ','.join(map(lambda b: str(b).lower(), results))\n    print(f\"[{output_str}]\")\n\nsolve()\n```", "id": "2418485"}]}