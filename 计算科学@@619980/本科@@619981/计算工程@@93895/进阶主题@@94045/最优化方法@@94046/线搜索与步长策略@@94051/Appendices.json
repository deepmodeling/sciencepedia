{"hands_on_practices": [{"introduction": "理论的掌握离不开实践的检验。许多现实世界中的优化问题并非简单的凸函数，而是充满了复杂的“地形”。本练习将引导你为经典的 Rosenbrock 函数实现回溯线搜索，这是一个以其狭窄弯曲的“山谷”而闻名的非凸函数，对许多优化算法都构成了挑战。通过这个实践，你将巩固对 Armijo 充分下降条件的理解，并学会在复杂的非凸环境中有效寻找步长。[@problem_id:2409367]", "problem": "给定一个带有可调曲率参数的双变量Rosenbrock函数，对于任意 $(x,y) \\in \\mathbb{R}^2$ 其定义如下\n$$\nf(x,y) = (1 - x)^2 + \\kappa \\,(y - x^2)^2,\n$$\n其中 $\\kappa > 0$ 控制着弯曲谷地的狭窄程度。当 $\\kappa$ 很大时，谷地在曲线 $y = x^2$ 周围会变得非常狭窄。考虑在当前点 $x = (x,y)$ 沿最速下降方向 $p = -\\nabla f(x,y)$ 选择一个步长 $\\alpha$，使得以下充分下降条件成立：\n$$\nf(x + \\alpha p) \\le f(x) + c_1 \\,\\alpha \\,\\nabla f(x)^{\\mathsf{T}} p,\n$$\n其中 $c_1 \\in (0,1)$ 是一个固定值。如果 $\\nabla f(x)^{\\mathsf{T}} p \\ge 0$，则定义 $\\alpha = 0$。否则，在固定 $\\alpha_0 > 0$ 和 $\\beta \\in (0,1)$ 的情况下，从 $\\alpha \\in \\{\\alpha_0, \\beta \\alpha_0, \\beta^2 \\alpha_0, \\dots\\}$ 这些值中，选择第一个满足充分下降条件的 $\\alpha$。如果在最多 $N_{\\max}$ 次缩减内没有找到这样的 $\\alpha$，则定义 $\\alpha = 0$。\n\n请实现一个程序，对每个测试用例，使用 $p = -\\nabla f(x,y)$ 并根据上述规则计算所选的 $\\alpha$。您必须从 $f(x,y)$ 的定义计算所有必需的导数。\n\n在所有情况下，请使用常数 $c_1 = 10^{-4}$，$\\beta = \\tfrac{1}{2}$ 和 $N_{\\max} = 50$。所有计算均为无量纲的实数。\n\n测试套件：\n- A用例（一般窄谷情况）：$\\kappa = 100$，$(x,y) = (-1.2, 1.0)$，$\\alpha_0 = 1$。\n- B用例（极窄谷情况）：$\\kappa = 1000$，$(x,y) = (-1.2, 1.0)$，$\\alpha_0 = 1$。\n- C用例（边界，驻点）：$\\kappa = 100$，$(x,y) = (1.0, 1.0)$，$\\alpha_0 = 1$。\n- D用例（初始步长已足够小）：$\\kappa = 100$，$(x,y) = (-1.2, 1.0)$，$\\alpha_0 = 10^{-4}$。\n\n最终输出格式：\n您的程序必须产生单行输出，其中包含按测试用例顺序排列的四个选定步长的列表，并四舍五入到小数点后六位。该列表必须以逗号分隔的序列形式打印，并用方括号括起来，例如 `[a,b,c,d]`。每个条目都必须是四舍五入到六位小数的实数。", "solution": "问题陈述已经过严格验证，并被认定为有效。它在科学上基于数值优化的原理，是适定的，提供了所有必要信息，并以客观、无歧义的术语表述。不存在逻辑矛盾、事实不准确或其他会妨碍正式求解的缺陷。因此，我们可以继续进行分析和实施。\n\n目标是使用回溯线搜索方法为一种优化算法计算步长 $\\alpha$，特别是为了满足Armijo充分下降条件。所考虑的函数是双变量Rosenbrock函数，它是无约束优化算法的一个标准基准测试函数。\n\nRosenbrock函数由下式给出：\n$$\nf(x,y) = (1 - x)^2 + \\kappa (y - x^2)^2\n$$\n其中 $\\mathbf{x} = (x,y)^{\\mathsf{T}} \\in \\mathbb{R}^2$，$\\kappa > 0$ 是一个控制函数谷地曲率的正常数。\n\n任何基于梯度的方法的第一步都是计算目标函数的梯度 $\\nabla f(\\mathbf{x})$。关于 $x$ 和 $y$ 的偏导数如下：\n$$\n\\frac{\\partial f}{\\partial x} = \\frac{\\partial}{\\partial x} \\left[ (1 - x)^2 + \\kappa (y - x^2)^2 \\right] = 2(1 - x)(-1) + \\kappa \\cdot 2(y - x^2)(-2x) = -2(1 - x) - 4\\kappa x(y - x^2)\n$$\n$$\n\\frac{\\partial f}{\\partial y} = \\frac{\\partial}{\\partial y} \\left[ (1 - x)^2 + \\kappa (y - x^2)^2 \\right] = \\kappa \\cdot 2(y - x^2)(1) = 2\\kappa(y - x^2)\n$$\n因此，梯度向量为：\n$$\n\\nabla f(x,y) = \\begin{pmatrix} -2(1 - x) - 4\\kappa x(y - x^2) \\\\ 2\\kappa(y - x^2) \\end{pmatrix}\n$$\n\n问题指定使用最速下降方向，其定义为梯度的负方向：\n$$\n\\mathbf{p} = -\\nabla f(x,y)\n$$\n只要梯度不为零，该方向就能保证函数值的局部下降。\n\n问题的核心是Armijo充分下降条件，该条件确保步长 $\\alpha$ 能使函数值得到有意义的减小。该条件是：\n$$\nf(\\mathbf{x} + \\alpha \\mathbf{p}) \\le f(\\mathbf{x}) + c_1 \\alpha \\nabla f(\\mathbf{x})^{\\mathsf{T}} \\mathbf{p}\n$$\n其中 $c_1 \\in (0,1)$ 是一个常数，此处给定为 $c_1 = 10^{-4}$。项 $\\nabla f(\\mathbf{x})^{\\mathsf{T}} \\mathbf{p}$ 是 $f$ 沿方向 $\\mathbf{p}$ 的方向导数。将 $\\mathbf{p} = -\\nabla f(\\mathbf{x})$ 代入可得：\n$$\n\\nabla f(\\mathbf{x})^{\\mathsf{T}} \\mathbf{p} = \\nabla f(\\mathbf{x})^{\\mathsf{T}}(-\\nabla f(\\mathbf{x})) = -\\|\\nabla f(\\mathbf{x})\\|_2^2\n$$\n该量始终为非正数。仅当 $\\nabla f(\\mathbf{x}) = \\mathbf{0}$ 时，它才等于零，这对应于一个驻点。问题正确地指出，如果 $\\nabla f(\\mathbf{x})^{\\mathsf{T}} \\mathbf{p} \\ge 0$，则必须将 $\\alpha$ 设为 $0$。这处理了驻点的情况，此时无法在最速下降方向上取得任何进展。\n\n寻找 $\\alpha$ 的算法过程是一个回溯搜索：\n1.  对于给定的点 $\\mathbf{x}=(x,y)$ 和参数 $\\kappa, \\alpha_0, c_1, \\beta, N_{\\max}$：\n2.  计算梯度 $\\mathbf{g} = \\nabla f(\\mathbf{x})$。\n3.  计算搜索方向 $\\mathbf{p} = -\\mathbf{g}$。\n4.  计算方向导数项 $d = \\mathbf{g}^{\\mathsf{T}}\\mathbf{p} = -\\|\\mathbf{g}\\|_2^2$。\n5.  如果 $d \\ge 0$，将最终步长设为 $0$ 并终止。\n6.  初始化试探步长 $\\alpha = \\alpha_0$。\n7.  最多进行 $N_{\\max}$ 次缩减迭代。对于从 $0$ 到 $N_{\\max}$ 的 $j$：\n    a. 计算候选点 $\\mathbf{x}_{\\text{new}} = \\mathbf{x} + \\alpha \\mathbf{p}$。\n    b. 在当前点 $f(\\mathbf{x})$ 和候选点 $f(\\mathbf{x}_{\\text{new}})$ 处评估函数值。\n    c. 检查Armijo条件是否满足：$f(\\mathbf{x}_{\\text{new}}) \\le f(\\mathbf{x}) + c_1 \\alpha d$。\n    d. 如果条件满足，则此 $\\alpha$ 为所需步长。终止搜索并返回该值。\n    e. 如果条件不满足，则缩减步长：$\\alpha \\leftarrow \\beta \\alpha$。\n8.  如果在测试了 $\\alpha_0, \\beta\\alpha_0, \\dots, \\beta^{N_{\\max}}\\alpha_0$ 之后循环完成仍未满足条件，则搜索失败。根据问题陈述，步长定义为 $0$。\n\n实现过程将把这一精确逻辑应用于每个指定的测试用例，并使用所提供的常数 $c_1 = 10^{-4}$，$\\beta = \\frac{1}{2}$ 和 $N_{\\max} = 50$。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem of finding the step length using backtracking line search\n    for the Rosenbrock function, according to the specified problem statement.\n    \"\"\"\n\n    # Global constants as per the problem\n    C1 = 1e-4\n    BETA = 0.5\n    N_MAX = 50\n\n    def rosenbrock_f(x_vec, kappa):\n        \"\"\"\n        Computes the Rosenbrock function value.\n        f(x,y) = (1 - x)^2 + kappa * (y - x^2)^2\n        \"\"\"\n        x, y = x_vec[0], x_vec[1]\n        return (1 - x)**2 + kappa * (y - x**2)**2\n\n    def rosenbrock_grad(x_vec, kappa):\n        \"\"\"\n        Computes the gradient of the Rosenbrock function.\n        \"\"\"\n        x, y = x_vec[0], x_vec[1]\n        df_dx = -2 * (1 - x) - 4 * kappa * x * (y - x**2)\n        df_dy = 2 * kappa * (y - x**2)\n        return np.array([df_dx, df_dy])\n\n    def find_step_length(kappa, x_start, alpha_0):\n        \"\"\"\n        Implements the backtracking line search to find a suitable step length alpha.\n        \"\"\"\n        x_vec = np.array(x_start, dtype=float)\n        \n        grad_f = rosenbrock_grad(x_vec, kappa)\n        p = -grad_f\n        \n        # Directional derivative term\n        grad_f_dot_p = np.dot(grad_f, p)\n\n        # If direction is not a descent direction (or at a stationary point)\n        if grad_f_dot_p >= 0:\n            return 0.0\n\n        alpha = float(alpha_0)\n        f_x = rosenbrock_f(x_vec, kappa)\n\n        # Backtracking loop: test alpha_0, beta*alpha_0, ..., beta^N_max * alpha_0\n        for _ in range(N_MAX + 1):\n            x_new = x_vec + alpha * p\n            f_x_new = rosenbrock_f(x_new, kappa)\n            \n            # Armijo sufficient decrease condition\n            if f_x_new = f_x + C1 * alpha * grad_f_dot_p:\n                return alpha\n            \n            # Reduce alpha for the next iteration\n            alpha *= BETA\n        \n        # If no suitable alpha is found within N_MAX reductions\n        return 0.0\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (kappa, (x, y), alpha_0)\n        (100, (-1.2, 1.0), 1.0),       # Case A\n        (1000, (-1.2, 1.0), 1.0),      # Case B\n        (100, (1.0, 1.0), 1.0),        # Case C\n        (100, (-1.2, 1.0), 1e-4),      # Case D\n    ]\n\n    results = []\n    for case in test_cases:\n        kappa, x_start, alpha_0 = case\n        result = find_step_length(kappa, x_start, alpha_0)\n        results.append(f\"{result:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "2409367"}, {"introduction": "从理想化的算法到计算机的实际应用，我们必须面对有限精度浮点运算带来的挑战。一个理论上完美的算法在实际执行中可能因为极小的步长被计算精度“吞噬”而失效。本练习将带你直面这些数值计算的现实问题，通过几个精巧设计的案例，你将学会识别并处理“算术停滞”等常见的数值陷阱，从而构建出更加稳健的优化代码。[@problem_id:2409357]", "problem": "给定两个可微标量目标函数 $f:\\mathbb{R}\\to\\mathbb{R}$ 及其精确梯度、一个起始点 $x\\_0\\in\\mathbb{R}$、一个固定搜索方向 $d\\in\\mathbb{R}$，以及一组基于充分下降条件定义步长选择规则的参数。算术运算必须以电气与电子工程师协会 (IEEE) $754$ 双精度进行。目标是返回一个满足充分下降不等式的步长 $\\alpha$，或者在由于算术或逻辑约束而无法实现此步长时，确定性地返回一个失败值。\n\n令 $f\\_1(x) = (x-3)^2$，其梯度为 $\\nabla f\\_1(x) = 2(x-3)$；令 $f\\_2(x) = x^8$，其梯度为 $\\nabla f\\_2(x) = 8x^7$。对于给定的函数对 $(f,\\nabla f)$、点 $x\\_0$、方向 $d$、初始试验步长 $\\alpha\\_00$、缩减因子 $\\rho\\in(0,1)$、充分下降常数 $c\\_1\\in(0,1)$、最小步长 $\\alpha\\_{\\min}0$ 以及最大缩减次数 $k\\_{\\max}\\in\\mathbb{N}$，定义候选步长为 $\\alpha\\_k=\\alpha\\_0\\rho^k$，其中 $k\\in\\{0,1,2,\\dots,k\\_{\\max}\\}$。如果一个步长 $\\alpha\\_k$ 满足不等式\n$$\nf(x\\_0+\\alpha\\_k d)\\ \\le\\ f(x\\_0)\\ +\\ c\\_1\\,\\alpha\\_k\\,\\nabla f(x\\_0)\\,d,\n$$\n并且浮点更新产生了一个不同的点，即计算值 $\\mathrm{fl}(x\\_0+\\alpha\\_k d)$ 在 IEEE $754$ 双精度下不完全等于 $x\\_0$，则该步长是可接受的。如果 $\\nabla f(x\\_0)\\,d\\ge 0$，您必须不加进一步检查直接返回 $0.0$。否则，您必须在序列 $\\{\\alpha\\_k\\}$ 中返回第一个可接受的 $\\alpha\\_k$。如果对于所有 $k\\in\\{0,1,\\dots,k\\_{\\max}\\}$，不等式不成立，或计算出的更新满足 $\\mathrm{fl}(x\\_0+\\alpha\\_k d)=x\\_0$，或者如果 $\\alpha\\_k\\alpha\\_{\\min}$，则返回 $0.0$。\n\n对所有测试用例使用以下参数值：$\\alpha\\_0=1.0$, $\\rho=0.5$, $c\\_1=10^{-4}$, $\\alpha\\_{\\min}=10^{-16}$, $k\\_{\\max}=1000$。\n\n测试套件：\n- 用例 $1$ (一般良态下降)：使用 $f=f\\_1$，$x\\_0=0.0$ 和 $d=-\\nabla f\\_1(x\\_0)$。\n- 用例 $2$ (导致微小步长的极平坦平台区)：使用 $f=f\\_2$，$x\\_0=10^{-3}$ 和 $d=-\\nabla f\\_2(x\\_0)$。\n- 用例 $3$ (由大数量级状态引起的算术停滞)：使用 $f=f\\_1$，$x\\_0=10^{16}$ 和 $d=-1.0$。\n- 用例 $4$ (非下降方向)：使用 $f=f\\_1$，$x\\_0=0.0$ 和 $d=+\\nabla f\\_1(x\\_0)$。\n\n您的程序必须实现上述选择规则，并为每个用例生成返回的步长 $\\alpha$（浮点数）。最终输出必须将所有用例的结果按顺序聚合为单行：一个无空格的逗号分隔的 Python 风格列表，例如 $[a\\_1,a\\_2,a\\_3,a\\_4]$，其中每个 $a\\_i$ 是用例 $i$ 返回的步长。\n\n本问题中没有物理单位或角度。所有实数都必须视为无单位的标量。程序所需的最终输出是包含列表 $[a\\_1,a\\_2,a\\_3,a\\_4]$ 的单行。", "solution": "该问题要求实现一个回溯线搜索算法，以找到一个满足充分下降条件的可接受步长 $\\alpha$，同时该算法对有限精度浮点运算的限制具有鲁棒性。分析和实现必须遵循所提供的参数和测试用例。\n\n该算法的核心是充分下降条件，也称为 Armijo 条件，它确保步长 $\\alpha$ 能导致目标函数 $f$ 的有意义的减小。对于一个起始点 $x_0$ 和一个搜索方向 $d$，一个可接受的步长 $\\alpha_k$ 必须满足：\n$$\nf(x_0 + \\alpha_k d) \\le f(x_0) + c_1 \\alpha_k \\nabla f(x_0) d\n$$\n其中 $c_1 \\in (0, 1)$ 是一个常数。仅当 $d$ 是一个下降方向时，即方向导数 $\\nabla f(x_0) d  0$ 时，这个条件才有意义。如果 $\\nabla f(x_0) d \\ge 0$，则该方向不是下降方向，搜索必须终止。\n\n该算法通过从一个初始猜测值 $\\alpha_0$ 开始，并以因子 $\\rho \\in (0,1)$ 迭代地减小它来搜索可接受的 $\\alpha$，从而生成一系列试验步长 $\\alpha_k = \\alpha_0 \\rho^k$（其中 $k = 0, 1, 2, \\dots$）。该序列中第一个满足条件的 $\\alpha_k$ 将被选中。\n\n引入了两个源于数值计算的实际约束：\n$1$. 最小步长 $\\alpha_{\\min}$：如果 $\\alpha_k$ 小于此阈值，搜索将终止，表明进一步的进展可以忽略不计。\n$2$. 算术停滞：由于 IEEE $754$ 双精度浮点运算的有限精度，更新 $x_{new} = \\mathrm{fl}(x_0 + \\alpha_k d)$ 可能导致 $x_{new}$ 精确地等于 $x_0$。当变化量 $|\\alpha_k d|$ 相对于 $x_0$ 的量级太小而无法表示时，会发生这种情况。这样的步长是无效的，必须被拒绝。\n\n完整的指定流程如下：\n首先，通过检查 $\\nabla f(x_0) d \\ge 0$ 来验证搜索方向 $d$ 是否为下降方向。如果该条件成立，算法必须返回 $0.0$。否则，从 $k$ 等于 $0$ 迭代到 $k_{\\max}$。在每次迭代中，计算 $\\alpha_k = \\alpha_0 \\rho^k$。如果 $\\alpha_k  \\alpha_{\\min}$，则终止并返回 $0.0$。接下来，通过计算 $x_{new} = \\mathrm{fl}(x_0 + \\alpha_k d)$ 并测试 $x_{new} == x_0$ 来检查是否停滞。如果它们相等，则步长太小；拒绝 $\\alpha_k$ 并继续下一次迭代。如果没有停滞，则检查充分下降条件。如果满足该条件，$\\alpha_k$ 就是所需的步长，算法返回它。如果循环完成仍未找到可接受的步长，则返回 $0.0$。\n\n我们使用参数 $\\alpha_0=1.0$、$\\rho=0.5$、$c_1=10^{-4}$、$\\alpha_{\\min}=10^{-16}$ 和 $k_{\\max}=1000$ 将此算法应用于四个测试用例。\n\n用例 $1$：$f(x)=f_1(x)=(x-3)^2$，$x_0=0.0$，$d = -\\nabla f_1(x_0)$。\n首先，我们计算梯度和方向：$\\nabla f_1(x_0) = 2(0.0 - 3) = -6.0$。方向是 $d = -(-6.0) = 6.0$。\n方向导数为 $\\nabla f_1(x_0) d = (-6.0)(6.0) = -36.0$。由于其为负，我们继续。我们有 $f(x_0) = (0.0-3)^2 = 9.0$。\n对于 $k=0$，$\\alpha_0=1.0$。新的点是 $x_{new} = 0.0 + 1.0 \\cdot 6.0 = 6.0$。未发生停滞。我们检查条件：\n$f(6.0) \\le f(0.0) + c_1 \\alpha_0 \\nabla f_1(0.0) d \\implies (6.0-3)^2 \\le 9.0 + 10^{-4}(1.0)(-36.0) \\implies 9.0 \\le 8.9964$。这是假的。\n对于 $k=1$，$\\alpha_1=0.5$。新的点是 $x_{new} = 0.0 + 0.5 \\cdot 6.0 = 3.0$。未发生停滞。我们检查条件：\n$f(3.0) \\le f(0.0) + c_1 \\alpha_1 \\nabla f_1(0.0) d \\implies (3.0-3)^2 \\le 9.0 + 10^{-4}(0.5)(-36.0) \\implies 0.0 \\le 8.9982$。这是真的。\n第一个可接受的步长是 $\\alpha_1=0.5$。\n\n用例 $2$：$f(x)=f_2(x)=x^8$，$x_0=10^{-3}$，$d = -\\nabla f_2(x_0)$。\n梯度是 $\\nabla f_2(x_0) = 8(10^{-3})^7 = 8 \\cdot 10^{-21}$。方向是 $d = -8 \\cdot 10^{-21}$。\n方向导数为 $\\nabla f_2(x_0) d = (8 \\cdot 10^{-21})(-8 \\cdot 10^{-21}) = -64 \\cdot 10^{-42}  0$。\n点的更新为 $x_{new} = \\mathrm{fl}(10^{-3} - \\alpha_k \\cdot 8 \\cdot 10^{-21})$。变化的量级是 $|\\alpha_k d| = \\alpha_k \\cdot 8 \\cdot 10^{-21}$。对于 $\\alpha_k \\le 1.0$，这个变化最多是 $8 \\cdot 10^{-21}$。在双精度下，$x_0=10^{-3}$ 的末位单位 (ULP) 大约是 $1.36 \\cdot 10^{-19}$。由于变化的量级远小于 $x_0$ 的 ULP，加法将被吸收，导致对所有试验步长都有 $\\mathrm{fl}(x_0 + \\alpha_k d) = x_0$。因此，对于每个 $k \\in \\{0, \\dots, k_{\\max}\\}$，停滞条件 $x_{new} == x_0$ 都将为真。循环将一直持续到所有迭代耗尽，函数将返回 $0.0$。\n\n用例 $3$：$f(x)=f_1(x)=(x-3)^2$，$x_0=10^{16}$，$d=-1.0$。\n梯度是 $\\nabla f_1(x_0) = 2(10^{16}-3)$，约等于 $2 \\cdot 10^{16}$。\n方向导数为 $\\nabla f_1(x_0) d \\approx (2 \\cdot 10^{16})(-1.0) = -2 \\cdot 10^{16}  0$。\n点的更新为 $x_{new} = \\mathrm{fl}(10^{16} - \\alpha_k)$。$x_0=10^{16}$ 的 ULP 是 $2.0$。试验步长是 $\\alpha_k = 0.5^k$，所有这些步长都 $\\le 1.0$。由于每个 $\\alpha_k$ 都小于 $x_0$ 的 ULP，减法将因吸收而丢失，$\\mathrm{fl}(10^{16} - \\alpha_k)$ 将计算为 $10^{16}$。所有试验步长都会发生停滞。循环完成时没有找到有效的步长，因此函数返回 $0.0$。\n\n用例 $4$：$f(x)=f_1(x)=(x-3)^2$，$x_0=0.0$，$d=+\\nabla f_1(x_0)$。\n梯度是 $\\nabla f_1(x_0) = -6.0$，所以方向是 $d=-6.0$。\n方向导数为 $\\nabla f_1(x_0) d = (-6.0)(-6.0) = 36.0$。\n由于方向导数 $36.0 \\ge 0$，方向 $d$ 不是一个下降方向。根据指定的流程，算法必须立即终止并返回 $0.0$。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n\n    # --- Fixed Parameters ---\n    ALPHA0 = 1.0\n    RHO = 0.5\n    C1 = 1e-4\n    ALPHA_MIN = 1e-16\n    K_MAX = 1000\n\n    # --- Objective Functions and Gradients ---\n    def f1(x: float) -> float:\n        return (x - 3.0)**2\n\n    def grad_f1(x: float) -> float:\n        return 2.0 * (x - 3.0)\n\n    def f2(x: float) -> float:\n        return x**8\n\n    def grad_f2(x: float) -> float:\n        return 8.0 * x**7\n\n    def line_search(f, grad_f, x0: float, d: float) -> float:\n        \"\"\"\n        Implements the backtracking line search algorithm.\n\n        Args:\n            f: The objective function.\n            grad_f: The gradient of the objective function.\n            x0: The starting point.\n            d: The search direction.\n\n        Returns:\n            The acceptable step size alpha, or 0.0 on failure.\n        \"\"\"\n        # All arithmetic is performed in IEEE 754 double precision,\n        # which is the standard for Python's float type.\n        \n        directional_derivative = grad_f(x0) * d\n\n        # Condition: Must be a descent direction\n        if directional_derivative >= 0:\n            return 0.0\n\n        f_x0 = f(x0)\n\n        for k in range(K_MAX + 1):\n            alpha_k = ALPHA0 * (RHO**k)\n\n            # Condition: Step size must not be smaller than the minimum\n            if alpha_k  ALPHA_MIN:\n                return 0.0\n\n            # Compute the new point and check for arithmetic stagnation\n            x_new = x0 + alpha_k * d\n            if x_new == x0:\n                continue  # Step is too small to change x0, try a smaller alpha\n\n            # Condition: Sufficient decrease (Armijo condition)\n            if f(x_new) = f_x0 + C1 * alpha_k * directional_derivative:\n                return alpha_k  # Acceptable step size found\n\n        # Failure: No acceptable step size found within k_max iterations\n        return 0.0\n\n    # --- Test Suite Definition ---\n    # Each case is a tuple: (function, gradient, start_point, direction_lambda)\n    # The direction_lambda calculates d based on the start point x0.\n    test_cases = [\n        # Case 1: General well-conditioned decrease\n        (f1, grad_f1, 0.0, lambda x: -grad_f1(x)),\n        # Case 2: Very flat plateau causing microscopic steps\n        (f2, grad_f2, 1e-3, lambda x: -grad_f2(x)),\n        # Case 3: Arithmetic stagnation from large magnitude state\n        (f1, grad_f1, 1e16, lambda x: -1.0),\n        # Case 4: Non-descent direction\n        (f1, grad_f1, 0.0, lambda x: grad_f1(x)),\n    ]\n\n    results = []\n    for f_handle, grad_f_handle, x0_val, d_lambda in test_cases:\n        direction = d_lambda(x0_val)\n        alpha_result = line_search(f_handle, grad_f_handle, x0_val, direction)\n        results.append(alpha_result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2409357"}, {"introduction": "在许多工程设计问题中，变量往往受到物理或实际条件的限制，例如尺寸、温度或压力不能超过某个范围，这些限制在数学上表现为约束。本练习将带领你将线搜索方法从无约束问题扩展到带约束问题，具体来说，是处理常见的箱式约束。你将学习并实现两种核心技术——投影法和截断法——来确保迭代过程始终保持在可行域内，这是将优化理论应用于实际场景的关键一步。[@problem_id:2409334]", "problem": "给定一个连续可微的目标函数 $f : \\mathbb{R}^n \\to \\mathbb{R}$，其梯度为 $\\nabla f$，一个满足分量级箱型约束 $l \\le x \\le u$ 的当前可行点 $x \\in \\mathbb{R}^n$，以及一个搜索方向 $p \\in \\mathbb{R}^n$。考虑一个离散的步长候选集\n$$\n\\mathcal{S} = \\{\\alpha_0 \\rho^k \\mid k \\in \\{0,1,\\dots,K\\}\\},\n$$\n其中 $\\alpha_0 \\in \\mathbb{R}_{0}$，$\\rho \\in \\mathbb{R}$ 满足 $0  \\rho  1$，且 $K \\in \\mathbb{N}$ 是有限的。定义到箱型区域 $[l,u]$ 的分量级投影为\n$$\n\\Pi_{[l,u]}(z)_i = \\min\\{\\max\\{z_i, l_i\\}, u_i\\}, \\quad i = 1,\\dots,n.\n$$\n\n考虑两种可行性强制模式，用于从候选步长 $\\alpha \\in \\mathcal{S}$ 构建试验点：\n\n- 投影模式 (Projection mode)：$s(\\alpha) = \\Pi_{[l,u]}(x + \\alpha p) - x$ 且 $x_{\\text{trial}}(\\alpha) = x + s(\\alpha)$。\n- 截断模式 (Truncation mode)：首先定义沿 $p$ 的最大可行步长为\n$$\n\\alpha_{\\max}(x,p) = \\min_{i=1,\\dots,n} \\begin{cases}\n\\dfrac{u_i - x_i}{p_i},  \\text{if } p_i > 0, \\\\\n\\dfrac{l_i - x_i}{p_i},  \\text{if } p_i  0, \\\\\n+\\infty,  \\text{if } p_i = 0,\n\\end{cases}\n$$\n然后设置 $s(\\alpha) = \\min\\{\\alpha,\\alpha_{\\max}(x,p)\\} \\, p$ 且 $x_{\\text{trial}}(\\alpha) = x + s(\\alpha)$。\n\n对于任一模式，如果带有实际步长向量 $s(\\alpha)$ 的 Armijo 充分下降条件成立，则接受候选步长 $\\alpha \\in \\mathcal{S}$：\n$$\nf\\big(x_{\\text{trial}}(\\alpha)\\big) \\le f(x) + c_1 \\, \\nabla f(x)^\\top s(\\alpha),\n$$\n其中 $c_1 \\in \\mathbb{R}$ 满足 $0  c_1  1$。在 $\\mathcal{S}$ 中所有被接受的候选项中，将值最大的定义为 $\\alpha^\\star$。如果没有 $\\alpha \\in \\mathcal{S}$ 被接受，则将 $\\alpha^\\star$ 定义为 $\\mathcal{S}$ 中最小的元素，即 $\\alpha^\\star = \\alpha_0 \\rho^K$。\n\n您的任务是编写一个完整的程序，该程序针对给定的测试套件，为每个测试用例计算 $\\alpha^\\star$，并将所有结果在单行中输出。所有量都是无量纲的；不涉及物理单位。\n\n使用以下目标函数及其梯度：\n- 两个变量的二次函数 ($n = 2$)：\n$$\nf_{\\text{quad}}(x) = \\tfrac{1}{2} x^\\top Q x + b^\\top x, \\quad \\nabla f_{\\text{quad}}(x) = Q x + b,\n$$\n其中\n$$\nQ = \\begin{bmatrix} 3  1 \\\\ 1  2 \\end{bmatrix}, \\quad b = \\begin{bmatrix} -1 \\\\ 2 \\end{bmatrix}.\n$$\n- 两个变量的 Rosenbrock 函数 ($n = 2$)：\n$$\nf_{\\text{ros}}(x_1,x_2) = 100\\,(x_2 - x_1^2)^2 + (1 - x_1)^2,\n$$\n其梯度为\n$$\n\\nabla f_{\\text{ros}}(x_1,x_2) = \\begin{bmatrix}\n-400 x_1 (x_2 - x_1^2) - 2(1 - x_1) \\\\\n200 (x_2 - x_1^2)\n\\end{bmatrix}.\n$$\n\n为以下每个测试用例计算 $\\alpha^\\star$。每个测试用例指定了 $(f,\\nabla f)$、模式、点 $x$、方向 $p$、边界 $l,u$ 以及参数 $\\alpha_0$、$\\rho$、$c_1$ 和 $K$：\n\n- 测试用例 1 (理想情况，内部点，投影模式)：$f = f_{\\text{quad}}$，$\\nabla f = \\nabla f_{\\text{quad}}$，模式 = 投影， $x = [0, 0]^\\top$，$p = -\\nabla f(x)$，$l = [-5,-5]^\\top$，$u = [5,5]^\\top$，$\\alpha_0 = 1.0$，$\\rho = 0.5$，$c_1 = 1 \\times 10^{-4}$，$K = 20$。\n- 测试用例 2 (边界截断，方向受箱型约束限制)：$f = f_{\\text{quad}}$，$\\nabla f = \\nabla f_{\\text{quad}}$，模式 = 截断，$x = [1.9, -1.9]^\\top$，$p = [1.0, -50.0]^\\top$，$l = [-2,-2]^\\top$，$u = [2,2]^\\top$，$\\alpha_0 = 1.0$，$\\rho = 0.5$，$c_1 = 1 \\times 10^{-4}$，$K = 20$。\n- 测试用例 3 (非凸情形，需要缩减步长，投影模式)：$f = f_{\\text{ros}}$，$\\nabla f = \\nabla f_{\\text{ros}}$，模式 = 投影，$x = [-1.2, 1.0]^\\top$，$p = -\\nabla f(x)$，$l = [-2,-1]^\\top$，$u = [2,3]^\\top$，$\\alpha_0 = 1.0$，$\\rho = 0.5$，$c_1 = 1 \\times 10^{-4}$，$K = 30$。\n- 测试用例 4 (零方向边缘情况)：$f = f_{\\text{quad}}$，$\\nabla f = \\nabla f_{\\text{quad}}$，模式 = 投影，$x = [0.5, -0.5]^\\top$，$p = [0.0, 0.0]^\\top$，$l = [-1,-1]^\\top$，$u = [1,1]^\\top$，$\\alpha_0 = 1.0$，$\\rho = 0.5$，$c_1 = 1 \\times 10^{-4}$，$K = 10$。\n- 测试用例 5 (投影将一个坐标裁剪到边界)：$f = f_{\\text{quad}}$，$\\nabla f = \\nabla f_{\\text{quad}}$，模式 = 投影，$x = [2.0, 0.0]^\\top$，$p = [1.0, -1.0]^\\top$，$l = [-2,-2]^\\top$，$u = [2,2]^\\top$，$\\alpha_0 = 1.0$，$\\rho = 0.5$，$c_1 = 1 \\times 10^{-4}$，$K = 20$。\n\n您的程序必须为每个测试用例计算 $\\alpha^\\star$，并生成一行输出，其中包含用方括号括起来的、以逗号分隔的十进制数列表形式的结果。例如，输出格式必须是\n$$\n[\\alpha^\\star_1,\\alpha^\\star_2,\\alpha^\\star_3,\\alpha^\\star_4,\\alpha^\\star_5]\n$$\n其中每个 $\\alpha^\\star_i$ 都打印到小数点后六位。", "solution": "目标是为每个测试用例，从一个离散集合中确定满足 Armijo 充分下降条件的最大步长，同时遵守箱型约束。这些约束通过试验点投影或步长截断到最大可行值来强制执行。\n\n我们从基本原理开始。给定 $x \\in \\mathbb{R}^n$、一个分量级满足 $l \\le u$ 的箱型区域 $[l,u]$ 以及一个方向 $p \\in \\mathbb{R}^n$，一步的可行性通过投影或截断来定义。投影算子 $\\Pi_{[l,u]}$ 按分量定义为\n$$\n\\Pi_{[l,u]}(z)_i = \\min\\{\\max\\{z_i,l_i\\},u_i\\}.\n$$\n这是欧几里得空间中到闭凸集 $[l,u]$ 的正交投影。它确保了对于任何 $\\alpha \\ge 0$，$x_{\\text{trial}}(\\alpha) = \\Pi_{[l,u]}(x+\\alpha p)$ 都是可行的。\n\n或者，沿射线 $x + \\alpha p$ 的最大可行步长可以从每个分量 $i$ 的不等式 $l_i \\le x_i + \\alpha p_i \\le u_i$ 推导出来。对这些不等式求解 $\\alpha$ 可得边界\n$$\n\\alpha \\le \\frac{u_i - x_i}{p_i} \\text{ if } p_i  0, \\quad\n\\alpha \\le \\frac{l_i - x_i}{p_i} \\text{ if } p_i  0, \\quad\n\\alpha \\in \\mathbb{R}_{\\ge 0} \\text{ arbitrary if } p_i = 0.\n$$\n能同时对所有分量保持可行性的最大 $\\alpha$ 则是\n$$\n\\alpha_{\\max}(x,p) = \\min_{i=1,\\dots,n} \\begin{cases}\n\\dfrac{u_i - x_i}{p_i},  \\text{if } p_i > 0, \\\\\n\\dfrac{l_i - x_i}{p_i},  \\text{if } p_i  0, \\\\\n+\\infty,  \\text{if } p_i = 0.\n\\end{cases}\n$$\n在截断模式下，使用实际步长向量 $s(\\alpha) = \\min\\{\\alpha,\\alpha_{\\max}(x,p)\\} p$，这保证了 $x + s(\\alpha)$ 位于箱型区域内。\n\nArmijo 充分下降条件是一个一阶准则，确保试验点处目标函数的下降量与当前点的方向导数成比例，缩放因子为 $c_1 \\in (0,1)$。对于实际步长向量 $s(\\alpha)$ 和试验点 $x_{\\text{trial}}(\\alpha) = x + s(\\alpha)$，该条件为\n$$\nf\\big(x_{\\text{trial}}(\\alpha)\\big) \\le f(x) + c_1 \\nabla f(x)^\\top s(\\alpha).\n$$\n请注意，右侧使用的是 $f$ 在 $x$ 点沿特定位移 $s(\\alpha)$ 的线性模型。在投影模式下，由于裁剪，$s(\\alpha)$ 可能与 $p$ 不共线。上述形式是两种模式的自然推广，因为它表达了相对于所采取的实际步长的充分下降。\n\n为确保选择是有限且适定的，我们将候选步长限制在一个有限的几何集合 $\\mathcal{S} = \\{\\alpha_0 \\rho^k \\mid k = 0,1,\\dots,K\\}$，其中 $\\alpha_0  0$ 且 $0  \\rho  1$。任务是选择\n$$\n\\alpha^\\star = \\max\\{\\alpha \\in \\mathcal{S} \\mid f(x_{\\text{trial}}(\\alpha)) \\le f(x) + c_1 \\nabla f(x)^\\top s(\\alpha)\\},\n$$\n并约定如果不存在这样的 $\\alpha$，则 $\\alpha^\\star = \\alpha_0 \\rho^K$。\n\n我们通过按 $\\alpha$ 值递减的顺序评估候选步长的不等式来实现此选择，并接受第一个满足不等式的步长；如果没有步长满足条件，则返回最小的 $\\alpha$。\n\n具体的目标函数和梯度是：\n- 二次函数：\n$$\nf_{\\text{quad}}(x) = \\tfrac{1}{2} x^\\top Q x + b^\\top x, \\quad \\nabla f_{\\text{quad}}(x) = Q x + b, \\quad\nQ = \\begin{bmatrix} 3  1 \\\\ 1  2 \\end{bmatrix}, \\quad b = \\begin{bmatrix} -1 \\\\ 2 \\end{bmatrix}.\n$$\n- Rosenbrock 函数：\n$$\nf_{\\text{ros}}(x_1,x_2) = 100\\,(x_2 - x_1^2)^2 + (1 - x_1)^2,\n$$\n$$\n\\nabla f_{\\text{ros}}(x_1,x_2) = \\begin{bmatrix}\n-400 x_1 (x_2 - x_1^2) - 2(1 - x_1) \\\\\n200 (x_2 - x_1^2)\n\\end{bmatrix}.\n$$\n\n使用的关键属性：\n- 对于投影模式，根据 $\\Pi_{[l,u]}$ 的定义，对任何 $\\alpha \\ge 0$ 都保证了可行性。\n- 对于截断模式，通过构建 $\\alpha_{\\max}(x,p)$ 并使用 $s(\\alpha) = \\min\\{\\alpha,\\alpha_{\\max}(x,p)\\} p$ 来保证可行性。\n- 如果 $s(\\alpha) = 0$，则 Armijo 不等式以等式形式成立，因为 $f(x_{\\text{trial}}(\\alpha)) = f(x)$ 且 $\\nabla f(x)^\\top s(\\alpha) = 0$。\n\n测试套件中边缘情况的讨论：\n- 测试用例 1 在箱型区域内部良好运行；对于足够小的 $\\alpha$，投影不会改变步长。下降方向 $p = -\\nabla f(x)$ 确保对于某个 $\\alpha \\in \\mathcal{S}$，Armijo 不等式将得到满足。\n- 测试用例 2 展示了在边界处的截断。方向 $p$ 试图同时增加第一个坐标并大幅减小第二个坐标。$\\alpha_{\\max}(x,p)$ 的值由第二个坐标的下界主导，这在截断模式下决定了有效步长。如果对于仍然发生截断的最大候选步长，Armijo 条件不成立，则进一步减小 $\\alpha$ 最终会减小实际步长 $s(\\alpha)$ 并导致接受。\n- 测试用例 3 是非凸的；从 Rosenbrock 峡谷附近开始，完整的步长可能不满足 Armijo 条件。通过 $\\rho$ 进行的离散化缩减确保了足够小的步长将满足该不等式。\n- 测试用例 4 中 $p = 0$。那么对于所有 $\\alpha$，$s(\\alpha) = 0$，Armijo 条件平凡地成立。根据定义选择最大的候选步长，因此 $\\alpha^\\star = \\alpha_0$。\n- 测试用例 5 从第一个坐标的上界开始，并沿该坐标向外移动，因此投影将步长的第一个分量裁剪为零，同时允许在第二个分量上移动。Armijo 条件评估的是沿实际位移 $s(\\alpha)$ 的下降，在这种情况下，对于小的 $\\alpha$，$s(\\alpha)$ 与第二个分量的负梯度方向对齐。\n\n程序的数值计算细节：\n- 每个测试用例在当前点 $x$ 处计算一次 $f$ 和 $\\nabla f$。\n- 对于 $\\mathcal{S}$ 中每个按递减顺序排列的 $\\alpha$，根据指定的模式计算 $s(\\alpha)$ 并检查 Armijo 不等式。\n- 选择最大的可接受 $\\alpha$，如果没有满足条件的，则选择最小的候选步长。\n- 按照要求，将每个 $\\alpha^\\star$ 报告为四舍五入到小数点后恰好六位的数值。\n\n该程序直接实现了这些定义，确保了基于上述数学陈述的正确性。对于截断模式，$\\alpha_{\\max}(x,p)$ 是按分量计算的，在 $p_i = 0$ 的地方设置为 $+\\infty$。对于投影模式，投影算子是按分量应用的。在所有情况下，Armijo 条件都使用实际步长向量 $s(\\alpha)$ 进行评估，这与约束设置中的一阶充分下降原则相符。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef f_quad(x):\n    Q = np.array([[3.0, 1.0],\n                  [1.0, 2.0]])\n    b = np.array([-1.0, 2.0])\n    return 0.5 * x @ (Q @ x) + b @ x\n\ndef grad_quad(x):\n    Q = np.array([[3.0, 1.0],\n                  [1.0, 2.0]])\n    b = np.array([-1.0, 2.0])\n    return Q @ x + b\n\ndef f_rosen(x):\n    x1, x2 = x[0], x[1]\n    return 100.0 * (x2 - x1**2)**2 + (1.0 - x1)**2\n\ndef grad_rosen(x):\n    x1, x2 = x[0], x[1]\n    df_dx1 = -400.0 * x1 * (x2 - x1**2) - 2.0 * (1.0 - x1)\n    df_dx2 = 200.0 * (x2 - x1**2)\n    return np.array([df_dx1, df_dx2], dtype=float)\n\ndef project_box(z, l, u):\n    return np.minimum(np.maximum(z, l), u)\n\ndef alpha_max_truncation(x, p, l, u):\n    # Compute maximum feasible alpha along direction p from x within [l,u]\n    alpha_max = np.inf\n    for i in range(len(x)):\n        if p[i] > 0:\n            cand = (u[i] - x[i]) / p[i]\n            if cand  alpha_max:\n                alpha_max = cand\n        elif p[i]  0:\n            cand = (l[i] - x[i]) / p[i]\n            if cand  alpha_max:\n                alpha_max = cand\n        else:\n            # p[i] == 0: no restriction from this component\n            pass\n    if not np.isfinite(alpha_max):\n        alpha_max = np.inf\n    if alpha_max  0:\n        # If negative due to numerical issues (should not happen if x is feasible)\n        alpha_max = 0.0\n    return alpha_max\n\ndef line_search_alpha_star(f, grad, x, p, l, u, mode, alpha0, rho, c1, K):\n    # Precompute current function value and gradient\n    fx = f(x)\n    g = grad(x)\n    # Generate candidate alphas in decreasing order\n    alphas = [alpha0 * (rho ** k) for k in range(0, K + 1)]\n    accepted_alpha = None\n\n    # Tolerance to guard against tiny numerical violations\n    tol = 1e-12\n\n    if mode == 'truncate':\n        amax = alpha_max_truncation(x, p, l, u)\n\n    for alpha in alphas:\n        if mode == 'project':\n            x_trial = project_box(x + alpha * p, l, u)\n            s = x_trial - x\n        elif mode == 'truncate':\n            alpha_eff = min(alpha, amax)\n            s = alpha_eff * p\n            x_trial = x + s\n            # Make sure numerical drift stays within bounds\n            x_trial = project_box(x_trial, l, u)\n        else:\n            raise ValueError(\"Unknown mode\")\n\n        lhs = f(x_trial)\n        rhs = fx + c1 * (g @ s)\n        if lhs = rhs + tol:\n            accepted_alpha = alpha\n            break\n\n    if accepted_alpha is None:\n        accepted_alpha = alphas[-1]\n    return accepted_alpha\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (f, grad, mode, x, p, l, u, alpha0, rho, c1, K)\n        # Test case 1\n        (f_quad, grad_quad, 'project',\n         np.array([0.0, 0.0]), None,\n         np.array([-5.0, -5.0]), np.array([5.0, 5.0]),\n         1.0, 0.5, 1e-4, 20),\n        # Test case 2\n        (f_quad, grad_quad, 'truncate',\n         np.array([1.9, -1.9]), np.array([1.0, -50.0]),\n         np.array([-2.0, -2.0]), np.array([2.0, 2.0]),\n         1.0, 0.5, 1e-4, 20),\n        # Test case 3\n        (f_rosen, grad_rosen, 'project',\n         np.array([-1.2, 1.0]), None,\n         np.array([-2.0, -1.0]), np.array([2.0, 3.0]),\n         1.0, 0.5, 1e-4, 30),\n        # Test case 4\n        (f_quad, grad_quad, 'project',\n         np.array([0.5, -0.5]), np.array([0.0, 0.0]),\n         np.array([-1.0, -1.0]), np.array([1.0, 1.0]),\n         1.0, 0.5, 1e-4, 10),\n        # Test case 5\n        (f_quad, grad_quad, 'project',\n         np.array([2.0, 0.0]), np.array([1.0, -1.0]),\n         np.array([-2.0, -2.0]), np.array([2.0, 2.0]),\n         1.0, 0.5, 1e-4, 20),\n    ]\n\n    results = []\n    for f, grad, mode, x, p, l, u, alpha0, rho, c1, K in test_cases:\n        # If p is None in projection cases where p = -grad(x)\n        if p is None:\n            p = -grad(x)\n        alpha_star = line_search_alpha_star(f, grad, x, p, l, u, mode, alpha0, rho, c1, K)\n        results.append(alpha_star)\n\n    # Format results to exactly six decimal places\n    formatted = [f\"{r:.6f}\" for r in results]\n    print(f\"[{','.join(formatted)}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "2409334"}]}