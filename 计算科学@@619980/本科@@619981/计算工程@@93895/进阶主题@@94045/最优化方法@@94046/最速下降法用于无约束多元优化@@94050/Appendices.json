{"hands_on_practices": [{"introduction": "二次函数是优化领域的“模式生物”，因为对它们行为的理解为更复杂的问题提供了深刻的见解。本练习将通过数值方式验证最速下降法的两个关键几何特性：它对坐标旋转的不变性，以及它对变量缩放的极度敏感性，后者直接关系到问题的条件数。通过这个练习，你将直观地理解为何预处理或“特征缩放”在实践中如此重要。[@problem_id:2448741]", "problem": "编写一个完整、可运行的程序，该程序为二次函数的无约束多元优化实现带精确线搜索的最速下降法（在欧几里得范数下也称为梯度下降法），并用它来数值地证明以下两个事实：(i) 最速下降法在坐标系旋转下具有不变性，以及 (ii) 最速下降法的性能高度依赖于变量的缩放。您可以假设的唯一基本事实是：梯度的定义是在欧几里得范数下最陡峭的上升方向，最速下降法的定义是沿着负梯度方向移动，以及精确线搜索的定义是选择能够最小化目标函数在由当前点和下降方向定义的射线上的步长。您必须在解决方案中推导您需要的任何其他公式。\n\n为以下形式的二次目标函数实现最速下降法\n$$\nf(\\mathbf{x}) = \\tfrac{1}{2}\\,\\mathbf{x}^\\top A \\,\\mathbf{x} - \\mathbf{b}^\\top \\mathbf{x} + c,\n$$\n其中 $A$ 是对称正定（SPD）矩阵，$\\mathbf{b}$ 是一个向量，$c$ 是一个标量。对于精确线搜索，在第 $k$ 次迭代时，当前点为 $\\mathbf{x}_k$，梯度为 $\\nabla f(\\mathbf{x}_k)$，定义沿射线的单变量函数 $\\phi(\\alpha) = f(\\mathbf{x}_k - \\alpha \\,\\nabla f(\\mathbf{x}_k))$，并选择 $\\alpha_k$ 来最小化 $\\phi(\\alpha)$。\n\n您的程序必须运行以下三个测试用例，并将其结果汇总到单行输出中。所有角度必须以弧度为单位。\n\n测试用例 1（旋转不变性检查）：\n- 维度 $n=2$。使用 $A_1 = \\begin{bmatrix} 3 & 1 \\\\ 1 & 2 \\end{bmatrix}$ 和 $\\mathbf{b}_1=\\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$。设旋转角为 $\\theta = \\pi/6$（即 $30$ 度），旋转矩阵为 $R=\\begin{bmatrix} \\cos\\theta & -\\sin\\theta \\\\ \\sin\\theta & \\cos\\theta\\end{bmatrix}$。通过 $f_R(\\mathbf{y}) = f(R\\,\\mathbf{y})$ 定义旋转后的二次函数，等价于 $A_R = R^\\top A_1 R$，$\\mathbf{b}_R = R^\\top \\mathbf{b}_1$。\n- 在原始坐标系中从 $\\mathbf{x}_0=\\begin{bmatrix}2\\\\-1\\end{bmatrix}$ 开始，在旋转后坐标系中从 $\\mathbf{y}_0=R^\\top \\mathbf{x}_0$开始。\n- 在两个坐标系中均执行恰好 $K=20$ 次带精确线搜索的最速下降迭代（不要因容差而提前停止）。\n- 定义布尔结果 $T_1$，如果对于每个迭代索引 $k\\in\\{0,1,\\dots,K\\}$，相应的迭代点都满足 $\\|\\mathbf{y}_k - R^\\top \\mathbf{x}_k\\|_\\infty \\le 10^{-10}$，则 $T_1$ 为真，否则为假。此处 $\\|\\cdot\\|_\\infty$ 表示无穷范数。\n\n测试用例 2（性能对缩放的敏感性）：\n- 维度 $n=2$。使用 $A_2=\\operatorname{diag}(1,100)$ 和 $\\mathbf{b}_2=\\begin{bmatrix}0\\\\0\\end{bmatrix}$。\n- 考虑对角缩放矩阵 $S=\\operatorname{diag}(1,10)$ 和缩放后坐标 $\\mathbf{y} = S \\mathbf{x}$，这会导出缩放后的二次函数 $f_S(\\mathbf{y}) = f(S^{-1}\\mathbf{y})$。在二次型中，这对应于 $A_S=S^{-\\top} A_2 S^{-1}$ 和 $\\mathbf{b}_S=S^{-\\top}\\mathbf{b}_2$。\n- 使用在两个坐标系中表示的相同物理起点：$\\mathbf{x}_0=\\begin{bmatrix}1\\\\1\\end{bmatrix}$ 和 $\\mathbf{y}_0=S\\,\\mathbf{x}_0$。\n- 在每个坐标系中运行带精确线搜索的最速下降法，直到梯度的欧几里得范数至多为 $10^{-8}$，最大迭代次数为 $10^5$ 以防止无限循环。记录在 $(A_2,\\mathbf{b}_2)$ 上未缩放运行的迭代次数 $\\text{iters}_\\text{unscaled}$ 和在 $(A_S,\\mathbf{b}_S)$ 上缩放后运行的迭代次数 $\\text{iters}_\\text{scaled}$。\n- 定义测试输出为浮点数比率 $r=\\text{iters}_\\text{unscaled}/\\text{iters}_\\text{scaled}$。\n\n测试用例 3（各向同性边缘情况）：\n- 维度 $n=2$。使用 $A_3=5 I_2$ 和 $\\mathbf{b}_3=\\begin{bmatrix}0\\\\0\\end{bmatrix}$，从 $\\mathbf{x}_0=\\begin{bmatrix}3\\\\-4\\end{bmatrix}$ 开始。\n- 运行带精确线搜索的最速下降法，直到梯度的欧几里得范数至多为 $10^{-12}$，最大迭代次数为 $10^5$。记录达到容差要求所需的迭代次数 $N_3$。\n\n程序输出格式：\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，不含空格，顺序为 $[T_1, r, N_3]$。此处 $T_1$ 是一个布尔字面量，$r$ 是一个浮点数，$N_3$ 是一个整数。例如，一个输出可能看起来像 $[True,123.0,1]$。", "solution": "该问题要求为二次函数的无约束优化实现带精确线搜索的最速下降算法。此实现将用于数值地证明该算法对坐标系旋转的不变性以及其对变量缩放的敏感性。从基本原理到最终实现的整个过程详述如下。\n\n一个一般的二次目标函数由下式给出\n$$\nf(\\mathbf{x}) = \\tfrac{1}{2}\\,\\mathbf{x}^\\top A \\,\\mathbf{x} - \\mathbf{b}^\\top \\mathbf{x} + c\n$$\n其中 $A \\in \\mathbb{R}^{n \\times n}$ 是一个对称正定（SPD）矩阵，$\\mathbf{b} \\in \\mathbb{R}^n$ 是一个向量，$c \\in \\mathbb{R}$ 是一个标量常数。$A$ 的对称正定性质确保了 $f(\\mathbf{x})$ 是严格凸的，并拥有唯一的全局最小值点。\n\n最速下降法是一种迭代算法，从一个初始猜测 $\\mathbf{x}_0$ 开始，生成一个点序列 $\\{\\mathbf{x}_k\\}$，该序列收敛到 $f(\\mathbf{x})$ 的最小值点。在每次迭代 $k$ 中，该方法沿着负梯度方向进行，这是在欧几里得范数下的最速下降方向。更新规则是：\n$$\n\\mathbf{x}_{k+1} = \\mathbf{x}_k - \\alpha_k \\nabla f(\\mathbf{x}_k)\n$$\n其中 $\\alpha_k > 0$ 是步长。问题指定了精确线搜索，意味着选择 $\\alpha_k$ 来最小化目标函数在搜索方向上的值。\n\n为了实现这个算法，我们必须推导两个关键公式：$f(\\mathbf{x})$ 的梯度和最优步长 $\\alpha_k$。\n\n首先，我们推导二次函数 $f(\\mathbf{x})$ 的梯度 $\\nabla f(\\mathbf{x})$。使用基本矩阵微积分，或通过对每个分量 $x_i$ 求导，我们分析 $f(\\mathbf{x})$ 的各项：\n$f(\\mathbf{x}) = \\frac{1}{2} \\sum_{i,j} A_{ij} x_i x_j - \\sum_i b_i x_i + c$。\n关于 $x_k$ 的偏导数是：\n$$\n\\frac{\\partial f}{\\partial x_k} = \\frac{\\partial}{\\partial x_k} \\left( \\frac{1}{2} \\sum_{i,j} A_{ij} x_i x_j \\right) - \\frac{\\partial}{\\partial x_k} \\left( \\sum_i b_i x_i \\right) + \\frac{\\partial c}{\\partial x_k}\n$$\n常数 $c$ 的导数为 $0$。线性项的导数为 $\\frac{\\partial}{\\partial x_k} (\\sum_i b_i x_i) = b_k$。对于二次项，使用乘法法则和 $A$ 的对称性（$A_{ij}=A_{ji}$）：\n$$\n\\frac{\\partial}{\\partial x_k} \\left( \\frac{1}{2} \\sum_{i,j} A_{ij} x_i x_j \\right) = \\frac{1}{2} \\sum_{i,j} A_{ij} (\\delta_{ik}x_j + x_i\\delta_{jk}) = \\frac{1}{2} \\left( \\sum_j A_{kj} x_j + \\sum_i A_{ik} x_i \\right) = \\frac{1}{2} \\left( [A\\mathbf{x}]_k + [A^\\top\\mathbf{x}]_k \\right) = [A\\mathbf{x}]_k\n$$\n其中 $[ \\cdot ]_k$ 表示第 $k$ 个分量。将各分量组合成一个向量，得到梯度：\n$$\n\\nabla f(\\mathbf{x}) = A\\mathbf{x} - \\mathbf{b}\n$$\n\n接下来，我们推导用于精确线搜索的最优步长 $\\alpha_k$ 的公式。我们定义一个单变量函数 $\\phi(\\alpha) = f(\\mathbf{x}_k - \\alpha \\mathbf{g}_k)$，其中 $\\mathbf{g}_k = \\nabla f(\\mathbf{x}_k)$ 是在当前迭代点 $\\mathbf{x}_k$ 处的梯度。我们寻求使 $\\phi(\\alpha)$ 最小化的 $\\alpha > 0$。通过将其导数设为零来找到这个值：$\\frac{d\\phi}{d\\alpha} = 0$。\n将 $\\mathbf{x} = \\mathbf{x}_k - \\alpha \\mathbf{g}_k$ 代入 $f(\\mathbf{x})$ 的表达式中：\n$$\n\\phi(\\alpha) = \\tfrac{1}{2}(\\mathbf{x}_k - \\alpha \\mathbf{g}_k)^\\top A (\\mathbf{x}_k - \\alpha \\mathbf{g}_k) - \\mathbf{b}^\\top(\\mathbf{x}_k - \\alpha \\mathbf{g}_k) + c\n$$\n展开该表达式并按 $\\alpha$ 的幂次对项进行分组：\n$$\n\\phi(\\alpha) = \\left( \\tfrac{1}{2}\\mathbf{x}_k^\\top A \\mathbf{x}_k - \\mathbf{b}^\\top \\mathbf{x}_k + c \\right) - \\alpha(\\mathbf{g}_k^\\top A \\mathbf{x}_k - \\mathbf{g}_k^\\top \\mathbf{b}) + \\tfrac{1}{2}\\alpha^2(\\mathbf{g}_k^\\top A \\mathbf{g}_k)\n$$\n第一项就是 $f(\\mathbf{x}_k)$。$\\alpha$ 的线性项系数可利用 $\\mathbf{g}_k = A\\mathbf{x}_k - \\mathbf{b}$ 进行简化，因此 $\\mathbf{g}_k^\\top A \\mathbf{x}_k - \\mathbf{g}_k^\\top \\mathbf{b} = \\mathbf{g}_k^\\top (A\\mathbf{x}_k - \\mathbf{b}) = \\mathbf{g}_k^\\top \\mathbf{g}_k$。\n因此，$\\phi(\\alpha)$ 是一个关于 $\\alpha$ 的简单二次函数：\n$$\n\\phi(\\alpha) = f(\\mathbf{x}_k) - \\alpha(\\mathbf{g}_k^\\top \\mathbf{g}_k) + \\tfrac{1}{2}\\alpha^2(\\mathbf{g}_k^\\top A \\mathbf{g}_k)\n$$\n对 $\\alpha$ 求导并将结果设为零，得到：\n$$\n\\frac{d\\phi}{d\\alpha} = -(\\mathbf{g}_k^\\top \\mathbf{g}_k) + \\alpha(\\mathbf{g}_k^\\top A \\mathbf{g}_k) = 0\n$$\n解出 $\\alpha$ 即可得到最优步长：\n$$\n\\alpha_k = \\frac{\\mathbf{g}_k^\\top \\mathbf{g}_k}{\\mathbf{g}_k^\\top A \\mathbf{g}_k}\n$$\n二阶导数 $\\frac{d^2\\phi}{d\\alpha^2} = \\mathbf{g}_k^\\top A \\mathbf{g}_k$ 为正，因为 $A$ 是对称正定矩阵且 $\\mathbf{g}_k \\neq \\mathbf{0}$（如果不在最小值点），这证实了它是一个最小值点。\n\n完整的用于二次优化的最速下降算法如下：\n1. 初始化 $\\mathbf{x}_0$。对于 $k=0, 1, 2, \\dots$：\n2. 计算梯度：$\\mathbf{g}_k = A\\mathbf{x}_k - \\mathbf{b}$。\n3. 检查收敛性，例如，如果 $\\|\\mathbf{g}_k\\|_2 \\le \\epsilon$。\n4. 计算步长：$\\alpha_k = (\\mathbf{g}_k^\\top \\mathbf{g}_k) / (\\mathbf{g}_k^\\top A \\mathbf{g}_k)$。\n5. 更新位置：$\\mathbf{x}_{k+1} = \\mathbf{x}_k - \\alpha_k \\mathbf{g}_k$。\n\n问题的测试用例旨在展示该算法的关键属性。\n\n测试用例 1（旋转不变性）：当坐标系通过矩阵 $R$（其中 $R^\\top R = I$）旋转时，一个点 $\\mathbf{x}$ 变为 $\\mathbf{y} = R^\\top \\mathbf{x}$。目标函数从 $f(\\mathbf{x})$ 转换为 $f_R(\\mathbf{y}) = f(R\\mathbf{y})$。这会产生新的二次参数 $A_R=R^\\top A R$ 和 $\\mathbf{b}_R=R^\\top \\mathbf{b}$。梯度也协变地转换为：$\\mathbf{g}_k^y = R^\\top \\mathbf{g}_k^x$。步长公式的分子 $(\\mathbf{g}_k^y)^\\top \\mathbf{g}_k^y = (\\mathbf{g}_k^x)^\\top R R^\\top \\mathbf{g}_k^x = (\\mathbf{g}_k^x)^\\top \\mathbf{g}_k^x$ 和分母 $(\\mathbf{g}_k^y)^\\top A_R \\mathbf{g}_k^y = (\\mathbf{g}_k^x)^\\top R(R^\\top A R)R^\\top \\mathbf{g}_k^x = (\\mathbf{g}_k^x)^\\top A \\mathbf{g}_k^x$ 在旋转下都是不变的。因此，$\\alpha_k^y = \\alpha_k^x$。通过归纳法，如果 $\\mathbf{y}_k = R^\\top \\mathbf{x}_k$，那么 $\\mathbf{y}_{k+1} = \\mathbf{y}_k - \\alpha_k^y \\mathbf{g}_k^y = R^\\top \\mathbf{x}_k - \\alpha_k^x R^\\top \\mathbf{g}_k^x = R^\\top(\\mathbf{x}_k - \\alpha_k^x \\mathbf{g}_k^x) = R^\\top \\mathbf{x}_{k+1}$。旋转后系统中的迭代点序列就是原始迭代点序列的旋转。该测试用例数值地验证了这一恒等关系。\n\n测试用例 2（缩放敏感性）：最速下降法的收敛速率由条件数 $\\kappa(A) = \\lambda_{\\max}(A)/\\lambda_{\\min}(A)$ 控制。对于未缩放的问题，$A_2 = \\operatorname{diag}(1, 100)$，所以 $\\kappa(A_2) = 100/1 = 100$。这个高条件数意味着二次函数的等值线是高度拉长的椭圆，导致最速下降路径以“之”字形缓慢地走向最小值点。对于缩放后的问题，变量替换 $\\mathbf{y}=S\\mathbf{x}$ 产生一个新的矩阵 $A_S = S^{-1}A_2S^{-1} = \\operatorname{diag}(1,1)\\operatorname{diag}(1,100)\\operatorname{diag}(1,1) = \\operatorname{diag}(1,1) = I_2$。新系统是各向同性的，其条件数为 $\\kappa(A_S) = 1/1 = 1$。对于各向同性的二次函数，最速下降法在一步之内收敛，因为梯度直接指向最小值点。这个测试用例展示了由于预处理（缩放）带来的性能显著提升。\n\n测试用例 3（各向同性边缘情况）：这个用例使用 $A_3 = 5I_2$，它也是完美条件的，条件数为 $\\kappa(A_3)=1$。与测试用例2中缩放后的问题一样，在任何点 $\\mathbf{x}_0$ 处的梯度都直接指向位于 $\\mathbf{x}^* = \\mathbf{0}$ 的最小值点。精确线搜索将选择一个 $\\alpha_0$ 来走完这整段距离，从而在单次迭代中收敛。这将被数值地验证。\n\n实现将遵循这些原理，将算法封装在一个可重用函数中，并将其应用于三个指定的场景。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef steepest_descent(A, b, x0, tol=None, max_iter=None, fixed_iter=None):\n    \"\"\"\n    Implements steepest descent with exact line search for f(x) = 0.5*x.T*A*x - b.T*x.\n\n    The function can run for a fixed number of iterations or until a tolerance is met.\n\n    Args:\n        A (np.ndarray): The symmetric positive definite matrix A.\n        b (np.ndarray): The vector b.\n        x0 (np.ndarray): The initial point x0.\n        tol (float, optional): The tolerance for the Euclidean norm of the gradient.\n                               Used as a stopping criterion.\n        max_iter (int, optional): The maximum number of iterations. Used with `tol`.\n        fixed_iter (int, optional): If provided, run for exactly this many iterations\n                                    and return all iterates.\n\n    Returns:\n        If `fixed_iter` is provided:\n            list[np.ndarray]: A list containing all iterates from x0 to x_K.\n        If `tol` and `max_iter` are provided:\n            tuple[np.ndarray, int]: The final point and the number of iterations taken.\n    \"\"\"\n    x = x0.astype(np.float64)\n\n    if fixed_iter is not None:\n        iterates = [x.copy()]\n        for _ in range(fixed_iter):\n            # Compute gradient g = Ax - b\n            g = A @ x - b\n            \n            # Numerator of alpha: g.T * g\n            gg = g.T @ g\n            \n            # If gradient is effectively zero, an optimum is reached.\n            # Avoid division by zero and stay at the current point.\n            if gg < 1e-30:\n                x = x\n            else:\n                # Denominator of alpha: g.T * A * g\n                gAg = g.T @ A @ g\n                alpha = gg / gAg\n                \n                # Update x: x_{k+1} = x_k - alpha * g_k\n                x = x - alpha * g\n            \n            iterates.append(x.copy())\n        return iterates\n    else:\n        for k in range(max_iter):\n            g = A @ x - b\n            grad_norm = np.linalg.norm(g)\n            \n            if grad_norm <= tol:\n                return x, k\n            \n            gg = g.T @ g\n            \n            # This check is theoretically redundant if tol > 0, but good practice.\n            if gg < 1e-30:\n                return x, k\n                \n            gAg = g.T @ A @ g\n            alpha = gg / gAg\n            x = x - alpha * g\n        \n        # Return last state if max_iter is reached before tolerance\n        return x, max_iter\n\ndef solve():\n    \"\"\"\n    Runs the three test cases and prints the results in the required format.\n    \"\"\"\n    results = []\n    \n    # --- Test Case 1: Rotation Invariance ---\n    A1 = np.array([[3.0, 1.0], [1.0, 2.0]])\n    b1 = np.array([0.0, 0.0])\n    x0_1 = np.array([2.0, -1.0])\n    theta = np.pi / 6.0\n    c, s = np.cos(theta), np.sin(theta)\n    R = np.array([[c, -s], [s, c]])\n    \n    A_R = R.T @ A1 @ R\n    b_R = R.T @ b1\n    y0_1 = R.T @ x0_1\n    \n    K = 20\n    x_iterates = steepest_descent(A=A1, b=b1, x0=x0_1, fixed_iter=K)\n    y_iterates = steepest_descent(A=A_R, b=b_R, x0=y0_1, fixed_iter=K)\n    \n    t1_check = True\n    for k in range(K + 1):\n        # Check if ||y_k - R.T * x_k||_inf <= 1e-10\n        diff = y_iterates[k] - R.T @ x_iterates[k]\n        inf_norm = np.linalg.norm(diff, ord=np.inf)\n        if inf_norm > 1e-10:\n            t1_check = False\n            break\n    results.append(t1_check)\n    \n    # --- Test Case 2: Scaling Sensitivity ---\n    A2 = np.array([[1.0, 0.0], [0.0, 100.0]])\n    b2 = np.array([0.0, 0.0])\n    x0_2 = np.array([1.0, 1.0])\n    \n    S = np.array([[1.0, 0.0], [0.0, 10.0]])\n    S_inv = np.linalg.inv(S)\n    \n    A_S = S_inv.T @ A2 @ S_inv\n    b_S = S_inv.T @ b2\n    y0_2 = S @ x0_2\n    \n    tol_2 = 1e-8\n    max_iter_2 = 100000\n    \n    _, iters_unscaled = steepest_descent(A=A2, b=b2, x0=x0_2, tol=tol_2, max_iter=max_iter_2)\n    _, iters_scaled = steepest_descent(A=A_S, b=b_S, x0=y0_2, tol=tol_2, max_iter=max_iter_2)\n    \n    r = float(iters_unscaled) / float(iters_scaled) if iters_scaled > 0 else float('inf')\n    results.append(r)\n    \n    # --- Test Case 3: Isotropic Edge Case ---\n    A3 = np.array([[5.0, 0.0], [0.0, 5.0]]) # 5 * I_2\n    b3 = np.array([0.0, 0.0])\n    x0_3 = np.array([3.0, -4.0])\n    \n    tol_3 = 1e-12\n    max_iter_3 = 100000\n    \n    _, n3 = steepest_descent(A=A3, b=b3, x0=x0_3, tol=tol_3, max_iter=max_iter_3)\n    results.append(n3)\n    \n    # Final print statement in the exact required format.\n    print(f\"[{results[0]},{results[1]},{results[2]}]\")\n\nsolve()\n\n```", "id": "2448741"}, {"introduction": "虽然最速下降法保证了每一步都朝着函数值下降的方向前进，但这并不意味着它总能成功找到一个最小值。本练习将探讨一个经典的陷阱：鞍点。你将通过解析和计算的方式，探索当算法在 $f(x,y) = x^2 - y^2$ 这样的鞍点附近游走时，其路径如何可能停滞或被引入一个无界下降的区域，从而揭示该算法的一个基本局限性。[@problem_id:2448659]", "problem": "编写一个完整且可运行的程序，分析最速下降法迭代应用于无约束多元函数 $f:\\mathbb{R}^{2}\\to\\mathbb{R}$（定义为 $f(x,y)=x^{2}-y^{2}$）的行为。考虑一个迭代映射，它从一个初始点 $(x_{0},y_{0})\\in\\mathbb{R}^{2}$ 开始，并通过从 $(x_{k},y_{k})$ 沿负梯度 $-\\nabla f(x_{k},y_{k})$ 方向的射线移动，到达该射线上使 $f$ 在该射线上最小化的点（如果这样的点存在），从而生成一个序列 $(x_{k},y_{k})$。如果 $f$ 沿该射线的下确界无法在有限步长内达到，则声明在该迭代点，线搜索在该方向上是下方无界的，并终止。如果在任何迭代点，$\\|\\nabla f(x_{k},y_{k})\\|_{2}$ 恰好为零，则声明已达到一个驻点，并终止。\n\n您的程序必须为每个指定的初始点生成此序列，最多进行 $N=50$ 次迭代，在适用的情况下使用上述定义所蕴含的精确算术公式，并且必须使用以下整数代码对结果进行分类：\n- 代码 $0$：起始点已经是驻点（即 $\\|\\nabla f(x_{0},y_{0})\\|_{2}=0$），序列不发生移动。\n- 代码 $1$：在达到迭代上限之前，经过有限次数（$k\\ge 1$）的迭代到达了一个驻点（即 $\\|\\nabla f(x_{k},y_{k})\\|_{2}=0$），并且过程在此终止。\n- 代码 $2$：在到达驻点之前的某次迭代中，沿着最速下降方向的线搜索是下方无界的（即，当步长沿该方向增加时，$f$ 无界地减小），由于该线上不存在有限的最小值点，过程终止。\n\n对于每个测试用例，返回一个列表，其中包含分类代码、实际执行的迭代次数（一个在 $\\{0,1,\\dots,N\\}$ 中的整数）以及终止时的终点坐标 $(x_{\\text{final}},y_{\\text{final}})$（作为实数）。\n\n使用以下初始点测试套件：\n- 测试 $1$：$(x_{0},y_{0})=(1.0,0.0)$。\n- 测试 $2$：$(x_{0},y_{0})=(1.0,0.99)$。\n- 测试 $3$：$(x_{0},y_{0})=(0.0,0.0)$。\n\n您的程序应生成单行输出，包含一个用方括号括起来的逗号分隔列表形式的结果，其中每个结果本身都是一个按顺序格式化为 $[{\\rm code},{\\rm iterations},x_{\\text{final}},y_{\\text{final}}]$ 的列表。例如，输出格式必须与 $[[c_{1},k_{1},x_{1},y_{1}],[c_{2},k_{2},x_{2},y_{2}],[c_{3},k_{3},x_{3},y_{3}]]]$ 完全一样，除了标准打印外没有额外的空格要求。所有数值都是无单位的实数。", "solution": "所给问题是计算工程领域一个有效的练习，具体涉及最速下降算法的分析。该问题提法恰当、科学严谨且无歧义。我们将进行严格的解析求解。\n\n目标是分析最速下降法为无约束最小化函数 $f:\\mathbb{R}^{2}\\to\\mathbb{R}$ 所生成的迭代序列，该函数定义为：\n$$f(x, y) = x^2 - y^2$$\n该函数表示一个鞍形曲面，在原点 $(0, 0)$ 有一个驻点，该点既不是最小值点也不是最大值点。\n\n最速下降迭代由以下更新规则定义：\n$$\\mathbf{x}_{k+1} = \\mathbf{x}_k + \\alpha_k \\mathbf{d}_k$$\n其中 $\\mathbf{x}_k = (x_k, y_k)$ 是第 $k$ 步的迭代点，$\\mathbf{d}_k$ 是最速下降方向，$\\alpha_k > 0$ 是由精确线搜索确定的步长。\n\n首先，我们计算 $f(\\mathbf{x})$ 的梯度，这是确定下降方向所必需的。\n$$\\nabla f(x, y) = \\begin{pmatrix} \\frac{\\partial f}{\\partial x} \\\\ \\frac{\\partial f}{\\partial y} \\end{pmatrix} = \\begin{pmatrix} 2x \\\\ -2y \\end{pmatrix}$$\n最速下降方向是梯度的负方向：\n$$\\mathbf{d}_k = -\\nabla f(x_k, y_k) = \\begin{pmatrix} -2x_k \\\\ 2y_k \\end{pmatrix}$$\n一个点 $(x_k, y_k)$ 是驻点当且仅当 $\\nabla f(x_k, y_k) = \\mathbf{0}$，这只在 $(x,y) = (0,0)$ 处发生。\n\n下一步是执行精确线搜索。我们定义一个函数 $\\phi(\\alpha)$，表示从 $\\mathbf{x}_k$ 点出发沿方向 $\\mathbf{d}_k$ 的射线上 $f$ 的值：\n$$\\phi(\\alpha) = f(\\mathbf{x}_k + \\alpha \\mathbf{d}_k) \\quad \\text{for } \\alpha \\ge 0$$\n代入 $\\mathbf{x}_k$ 和 $\\mathbf{d}_k$ 的表达式：\n$$\\mathbf{x}_k + \\alpha \\mathbf{d}_k = \\begin{pmatrix} x_k \\\\ y_k \\end{pmatrix} + \\alpha \\begin{pmatrix} -2x_k \\\\ 2y_k \\end{pmatrix} = \\begin{pmatrix} x_k(1 - 2\\alpha) \\\\ y_k(1 + 2\\alpha) \\end{pmatrix}$$\n因此，$\\phi(\\alpha)$ 变为：\n$$\\phi(\\alpha) = (x_k(1 - 2\\alpha))^2 - (y_k(1 + 2\\alpha))^2$$\n展开此表达式揭示了对 $\\alpha$ 的二次依赖关系：\n$$\\phi(\\alpha) = x_k^2(1 - 4\\alpha + 4\\alpha^2) - y_k^2(1 + 4\\alpha + 4\\alpha^2)$$\n$$\\phi(\\alpha) = (x_k^2 - y_k^2) - 4\\alpha(x_k^2 + y_k^2) + 4\\alpha^2(x_k^2 - y_k^2)$$\n这是一个形如 $\\phi(\\alpha) = A\\alpha^2 + B\\alpha + C$ 的二次函数，其系数为：\n- $A = 4(x_k^2 - y_k^2)$\n- $B = -4(x_k^2 + y_k^2)$\n- $C = x_k^2 - y_k^2 = f(x_k, y_k)$\n\n线搜索的行为关键取决于二次项系数 $A$ 的符号。\n1.  如果 $A > 0$，等价于 $x_k^2 > y_k^2$ 或 $|x_k| > |y_k|$，抛物线 $\\phi(\\alpha)$ 开口向上，并有唯一的最小值。通过令导数 $\\phi'(\\alpha) = 2A\\alpha + B$ 为零来找到最小值点 $\\alpha^*$：\n    $$\\alpha^* = -\\frac{B}{2A} = -\\frac{-4(x_k^2 + y_k^2)}{2 \\cdot 4(x_k^2 - y_k^2)} = \\frac{x_k^2 + y_k^2}{2(x_k^2 - y_k^2)}$$\n    由于 $x_k^2+y_k^2>0$（对于非驻点）且 $x_k^2-y_k^2>0$，我们有 $\\alpha^* > 0$。最优步长为 $\\alpha_k = \\alpha^*$。\n\n2.  如果 $A \\le 0$，等价于 $x_k^2 \\le y_k^2$ 或 $|x_k| \\le |y_k|$：\n    - 如果 $A < 0$，抛物线 $\\phi(\\alpha)$ 开口向下，并且当 $\\alpha \\to \\infty$ 时是下方无界的。\n    - 如果 $A = 0$（且该点不是原点），则 $\\phi(\\alpha) = B\\alpha + C$ 是一条斜率为 $B = -4(x_k^2+y_k^2) < 0$ 的直线，当 $\\alpha \\to \\infty$ 时也是下方无界的。\n    在这两种子情况下，对于 $\\alpha \\ge 0$ 都不存在有限的最小值点。问题陈述规定这是一种终止条件（代码 $2$）。\n\n当有限步长可行时（$|x_k| > |y_k|$），我们推导 $\\mathbf{x}_{k+1}$ 的更新公式：\n$$x_{k+1} = x_k(1 - 2\\alpha_k) = x_k\\left(1 - 2 \\frac{x_k^2 + y_k^2}{2(x_k^2 - y_k^2)}\\right) = x_k\\left(\\frac{x_k^2 - y_k^2 - x_k^2 - y_k^2}{x_k^2 - y_k^2}\\right) = x_k \\frac{-2y_k^2}{x_k^2 - y_k^2}$$\n$$y_{k+1} = y_k(1 + 2\\alpha_k) = y_k\\left(1 + 2 \\frac{x_k^2 + y_k^2}{2(x_k^2 - y_k^2)}\\right) = y_k\\left(\\frac{x_k^2 - y_k^2 + x_k^2 + y_k^2}{x_k^2 - y_k^2}\\right) = y_k \\frac{2x_k^2}{x_k^2 - y_k^2}$$\n我们来分析比率 $\\rho_k = y_k/x_k$（对于 $x_k \\ne 0$）。下一次迭代的比率为：\n$$\\rho_{k+1} = \\frac{y_{k+1}}{x_{k+1}} = \\frac{y_k \\frac{2x_k^2}{x_k^2 - y_k^2}}{x_k \\frac{-2y_k^2}{x_k^2 - y_k^2}} = \\frac{y_k}{x_k} \\frac{2x_k^2}{-2y_k^2} = \\rho_k \\left(-\\frac{1}{\\rho_k^2}\\right) = -\\frac{1}{\\rho_k}$$\n迭代继续的条件是 $|x_k| > |y_k|$，或者说 $|\\rho_k| < 1$。如果这个条件成立，那么 $|\\rho_{k+1}| = |-1/\\rho_k| = 1/|\\rho_k| > 1$。这意味着 $|y_{k+1}| > |x_{k+1}|$。因此，在下一次迭代中，有界线搜索的条件（$|x_{k+1}| > |y_{k+1}|$）将不满足，导致以代码 $2$ 终止。\n该分析证明，除非算法从原点开始或落在原点上，否则在终止前最多只会执行一次成功的步骤。\n\n现在我们将此框架应用于指定的测试用例。\n\n**测试用例 1：初始点 $(x_0, y_0) = (1.0, 0.0)$**\n- **迭代 $k=0$**：点为 $\\mathbf{x}_0 = (1.0, 0.0)$。\n- 梯度 $\\nabla f(1, 0) = (2, 0)$ 非零，因此该点不是驻点。\n- 我们检查线搜索条件：$|x_0| = 1.0 > |y_0| = 0.0$。存在有限步长 $\\alpha_0$。\n- 我们计算下一个迭代点 $\\mathbf{x}_1$：\n  $x_1 = 1.0 \\frac{-2(0.0)^2}{1.0^2 - 0.0^2} = 0.0$\n  $y_1 = 0.0 \\frac{2(1.0)^2}{1.0^2 - 0.0^2} = 0.0$\n- 新点是 $\\mathbf{x}_1 = (0.0, 0.0)$。\n- **迭代 $k=1$**：点为 $\\mathbf{x}_1 = (0.0, 0.0)$。\n- 梯度 $\\nabla f(0, 0) = (0, 0)$ 为零。已达到一个驻点。\n- 终止原因：代码 $1$。迭代次数：$1$。终点：$(0.0, 0.0)$。\n结果：`[1, 1, 0.0, 0.0]`\n\n**测试用例 2：初始点 $(x_0, y_0) = (1.0, 0.99)$**\n- **迭代 $k=0$**：点为 $\\mathbf{x}_0 = (1.0, 0.99)$。\n- 梯度非零。\n- 线搜索条件：$|x_0| = 1.0 > |y_0| = 0.99$。存在有限步长 $\\alpha_0$。\n- 我们计算 $\\mathbf{x}_1$：\n  $x_0^2 = 1.0$, $y_0^2 = 0.99^2 = 0.9801$。\n  $x_1 = 1.0 \\frac{-2(0.9801)}{1.0 - 0.9801} = \\frac{-1.9602}{0.0199} = -\\frac{19602}{199} \\approx -98.50251256$\n  $y_1 = 0.99 \\frac{2(1.0)}{1.0 - 0.9801} = \\frac{1.98}{0.0199} = \\frac{19800}{199} \\approx 99.49748744$\n- 新点是 $\\mathbf{x}_1 \\approx (-98.50, 99.50)$。\n- **迭代 $k=1$**：点为 $\\mathbf{x}_1$。\n- 在 $\\mathbf{x}_1$ 处检查线搜索条件：$|x_1| = \\frac{19602}{199}$， $|y_1| = \\frac{19800}{199}$。\n- 我们有 $|x_1| < |y_1|$。从 $\\mathbf{x}_1$ 出发的线搜索是下方无界的。\n- 算法终止。\n- 终止原因：代码 $2$。迭代次数：$1$（从 $\\mathbf{x}_0$ 到 $\\mathbf{x}_1$）。终点：$\\mathbf{x}_1$。\n结果：`[2, 1, -19602/199, 19800/199]`\n\n**测试用例 3：初始点 $(x_0, y_0) = (0.0, 0.0)$**\n- **迭代 $k=0$**：点为 $\\mathbf{x}_0 = (0.0, 0.0)$。\n- 梯度 $\\nabla f(0, 0) = (0, 0)$ 为零。起始点已经是驻点。\n- 算法在执行任何步骤之前终止。\n- 终止原因：代码 $0$。迭代次数：$0$。终点：$(0.0, 0.0)$。\n结果：`[0, 0, 0.0, 0.0]`", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\n# No libraries outside the standard library are needed for this problem.\n\ndef solve():\n    \"\"\"\n    Analyzes steepest descent for f(x,y) = x^2 - y^2 for a suite of test cases.\n    \"\"\"\n    \n    def analyze_single_case(x0: float, y0: float, N_max: int):\n        \"\"\"\n        Applies the steepest descent logic for a single initial point.\n        Based on the analytical derivation, the process terminates in at most 1 iteration.\n        \n        Args:\n            x0: Initial x-coordinate.\n            y0: Initial y-coordinate.\n            N_max: Maximum number of iterations (given as 50, but not reached).\n\n        Returns:\n            A list [code, iterations, x_final, y_final].\n        \"\"\"\n        x, y = float(x0), float(y0)\n        \n        # Iteration k=0\n        \n        # Check for Code 0: Initial point is stationary.\n        # This occurs only at the origin.\n        if x == 0.0 and y == 0.0:\n            return [0, 0, 0.0, 0.0]\n\n        # Check for Code 2 at k=0: Line search is unbounded from the start.\n        # This occurs if |x| <= |y|.\n        if abs(x) <= abs(y):\n            return [2, 0, x, y]\n\n        # If |x| > |y|, the line search is bounded, and we perform one iteration.\n        k = 1\n        x_sq = x * x\n        y_sq = y * y\n        denominator = x_sq - y_sq\n        \n        # Update formulas derived from exact line search\n        x_next = x * (-2.0 * y_sq) / denominator\n        y_next = y * (2.0 * x_sq) / denominator\n        \n        # After one step, the new point (x_next, y_next) is reached.\n        # The number of iterations performed is 1.\n\n        # Check for Code 1: Reached a stationary point.\n        # This happens if the initial point was on the x-axis (but not the origin),\n        # leading to the next iterate being (0,0).\n        if x_next == 0.0 and y_next == 0.0:\n            return [1, k, 0.0, 0.0]\n        \n        # Check for Code 2 after one iteration.\n        # Our analysis showed that if |x0/y0|<1, then |x1/y1|>1.\n        # This means the line search from (x_next, y_next) must be unbounded.\n        # The condition abs(x_next) <= abs(y_next) will be true.\n        return [2, k, x_next, y_next]\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (1.0, 0.0),\n        (1.0, 0.99),\n        (0.0, 0.0),\n    ]\n\n    # The maximum number of iterations as specified in the problem statement.\n    N = 50\n\n    all_results = []\n    for x_start, y_start in test_cases:\n        result = analyze_single_case(x_start, y_start, N)\n        all_results.append(result)\n\n    # Format the final output string as per the requirements.\n    # e.g., [[c1,k1,x1,y1],[c2,k2,x2,y2],[c3,k3,x3,y3]]\n    result_strings = [f\"[{c},{k},{x},{y}]\" for c, k, x, y in all_results]\n    print(f\"[{','.join(result_strings)}]\")\n\nsolve()\n```", "id": "2448659"}, {"introduction": "现实世界中的优化问题很少像单个二次碗那样简单，其目标函数的地形往往复杂且包含多个“山谷”，即局部最小值。本练习将挑战著名的 Himmelblau 函数，这是一个具有多个局部最小值的非凸函数。通过从不同的初始点运行最速下降法，你将亲身体验到该算法的局部收敛性，并理解在实践中选择合适的起始点对于找到满意解是多么关键。[@problem_id:2448739]", "problem": "设计并实现一个程序，针对由下式定义的二元 Himmelblau 函数\n$$\nf(x,y) = \\left(x^2 + y - 11\\right)^2 + \\left(x + y^2 - 7\\right)^2,\n$$\n从多个初始点执行最速下降法，并总结其终止行为。对于一个点 $\\mathbf{x}_k = [x_k, y_k]^\\top \\in \\mathbb{R}^2$，定义最速下降迭代如下\n$$\n\\mathbf{x}_{k+1} = \\mathbf{x}_k - \\alpha_k \\nabla f(\\mathbf{x}_k),\n$$\n其中 $\\alpha_k$ 被选择为一维函数\n$$\n\\phi_k(\\alpha) = f\\left(\\mathbf{x}_k - \\alpha \\nabla f(\\mathbf{x}_k)\\right)\n$$\n在闭区间 $\\alpha \\in [0,1]$ 上的最小化子。对向量使用欧几里得范数。当 $\\lVert \\nabla f(\\mathbf{x}_k) \\rVert_2 \\leq \\tau$ 或 $k \\geq k_{\\max}$ 时终止迭代，其中 $\\tau = 10^{-8}$ 且 $k_{\\max} = 10000$。梯度 $\\nabla f(x,y)$ 取自多元微积分中的标准定义。\n\n对于以下测试集中的每个初始点（按给定顺序列出），\n$$\n(3, 2),\\quad (0, 0),\\quad (-3, -3),\\quad (-4, 4),\\quad (5, 5),\\quad (3.5, -2.5),\n$$\n按规定运行最速下降法，并记录：\n- 终止时的最终迭代点 $\\left(x^\\star, y^\\star\\right)$，\n- 函数值 $f\\left(x^\\star, y^\\star\\right)$，\n- 执行的总迭代次数 $k$，\n- 以及一个整数标签，用于标识最终点在欧几里得距离上最接近四个已知局部最小化点中的哪一个：\n  - $m_0 = (3.000000, 2.000000)$，\n  - $m_1 = (-2.805118, 3.131312)$，\n  - $m_2 = (-3.779310, -3.283186)$，\n  - $m_3 = (3.584428, -1.848126)$。\n设 $d_j = \\lVert \\left(x^\\star, y^\\star\\right) - m_j \\rVert_2$，其中 $j \\in \\{0,1,2,3\\}$。如果 $\\min_j d_j \\leq 10^{-3}$，则输出达到最小值的索引 $j$；否则，输出 $-1$。\n\n要求的数值报告格式：\n- 将 $x^\\star$、$y^\\star$ 和 $f\\left(x^\\star, y^\\star\\right)$ 四舍五入到 $6$ 位小数。\n- 将 $k$ 和标签作为整数报告。\n\n您的程序应生成单行输出，其中包含一个以逗号分隔的列表的列表形式的结果，顺序与测试集相同，每个内部列表为\n$$\n[x^\\star, y^\\star, f(x^\\star, y^\\star), k, \\text{label}]。\n$$\n例如，总体格式必须为\n$$\n\\big[ [\\cdot,\\cdot,\\cdot,\\cdot,\\cdot], [\\cdot,\\cdot,\\cdot,\\cdot,\\cdot], \\ldots \\big]\n$$\n并打印在单行上。", "solution": "所提出的问题是计算工程领域中一个定义明确的练习，特别是在无约束多元优化领域。它要求实现带有精确线搜索的最速下降算法，以寻找 Himmelblau 函数的局部最小值。该问题具有科学依据，数学上一致，并包含其解决所需的所有必要信息。因此，该问题被认为是有效的。\n\n问题的核心是最小化二元 Himmelblau 函数，其定义如下：\n$$\nf(x,y) = \\left(x^2 + y - 11\\right)^2 + \\left(x + y^2 - 7\\right)^2\n$$\n设变量向量为 $\\mathbf{x} = [x, y]^\\top$。该函数可以写为 $f(\\mathbf{x})$。\n\n最速下降法是一种迭代优化算法，它从一个初始猜测 $\\mathbf{x}_0$ 开始，生成一个点序列 $\\mathbf{x}_1, \\mathbf{x}_2, \\ldots$，旨在收敛到一个局部最小值。每次迭代的更新规则由下式给出：\n$$\n\\mathbf{x}_{k+1} = \\mathbf{x}_k + \\alpha_k \\mathbf{d}_k\n$$\n其中 $\\mathbf{d}_k$ 是搜索方向，$\\alpha_k > 0$ 是步长。对于最速下降法，搜索方向选择为当前点处目标函数的负梯度，因为这是函数值下降最快的方向。因此，\n$$\n\\mathbf{d}_k = -\\nabla f(\\mathbf{x}_k)\n$$\n必须推导出 Himmelblau 函数的梯度 $\\nabla f(\\mathbf{x}) = \\left[ \\frac{\\partial f}{\\partial x}, \\frac{\\partial f}{\\partial y} \\right]^\\top$。其偏导数是：\n$$\n\\frac{\\partial f}{\\partial x} = 2(x^2 + y - 11)(2x) + 2(x + y^2 - 7)(1) = 4x(x^2 + y - 11) + 2(x + y^2 - 7)\n$$\n$$\n\\frac{\\partial f}{\\partial y} = 2(x^2 + y - 11)(1) + 2(x + y^2 - 7)(2y) = 2(x^2 + y - 11) + 4y(x + y^2 - 7)\n$$\n\n因此，迭代公式为：\n$$\n\\mathbf{x}_{k+1} = \\mathbf{x}_k - \\alpha_k \\nabla f(\\mathbf{x}_k)\n$$\n问题规定步长 $\\alpha_k$ 的选择必须使函数沿搜索方向最小化。这被称为精确线搜索。我们定义一个一维函数 $\\phi_k(\\alpha)$：\n$$\n\\phi_k(\\alpha) = f(\\mathbf{x}_k - \\alpha \\nabla f(\\mathbf{x}_k))\n$$\n最优步长 $\\alpha_k$ 是在指定的闭区间 $[0, 1]$ 上使 $\\phi_k(\\alpha)$ 最小化的 $\\alpha$ 值。\n$$\n\\alpha_k = \\arg\\min_{\\alpha \\in [0, 1]} \\phi_k(\\alpha)\n$$\n这种一维最小化是一个标准的子问题，可以使用既定方法进行数值求解，例如，使用像 `scipy.optimize.minimize_scalar` 这样的库例程，并采用 `bounded` 方法。\n\n迭代过程持续进行，直到满足两个终止条件之一：\n1.  梯度的模长降至指定容差 $\\tau = 10^{-8}$ 以下。这表明迭代点接近一个驻点（最小值点、最大值点或鞍点）。条件是 $\\lVert \\nabla f(\\mathbf{x}_k) \\rVert_2 \\leq \\tau$。\n2.  迭代次数 $k$ 达到最大限制 $k_{\\max} = 10000$。这可以防止在收敛缓慢或不收敛的情况下陷入无限循环。\n\n在第 $k$ 次迭代终止时，得到最终点 $\\mathbf{x}^\\star = \\mathbf{x}_k$，并记录以下数据：\n- 最终坐标 $(x^\\star, y^\\star)$。\n- 最终函数值 $f(x^\\star, y^\\star)$。\n- 总迭代次数 $k$。\n- 一个分类标签。\n\n分类标签通过将最终点 $\\mathbf{x}^\\star$ 与 Himmelblau 函数的四个已知局部最小化点进行比较来确定：\n- $m_0 = (3.000000, 2.000000)$\n- $m_1 = (-2.805118, 3.131312)$\n- $m_2 = (-3.779310, -3.283186)$\n- $m_3 = (3.584428, -1.848126)$\n\n对每个 $j \\in \\{0, 1, 2, 3\\}$ 计算欧几里得距离 $d_j = \\lVert \\mathbf{x}^\\star - m_j \\rVert_2$。如果这些距离的最小值 $\\min_j d_j$ 小于或等于 $10^{-3}$，则标签为对应此最小距离的索引 $j$。这表示成功收敛到已知最小化点的邻域。如果 $\\min_j d_j > 10^{-3}$，则标签为 $-1$，表示算法在离任何已知最小化点都不够近的点处终止。\n\n对每个给定的初始点，计算过程如下：\n1.  初始化 $\\mathbf{x}_0$ 并设置迭代计数器 $k = 0$。\n2.  开始主循环：\n    a. 计算梯度向量 $\\nabla f(\\mathbf{x}_k)$。\n    b. 计算梯度范数 $\\lVert \\nabla f(\\mathbf{x}_k) \\rVert_2$。\n    c. 如果 $\\lVert \\nabla f(\\mathbf{x}_k) \\rVert_2 \\leq 10^{-8}$ 或 $k \\geq 10000$，则终止循环。\n    d. 定义线搜索函数 $\\phi_k(\\alpha) = f(\\mathbf{x}_k - \\alpha \\nabla f(\\mathbf{x}_k))$。\n    e. 使用数值求解器找到 $\\alpha_k = \\arg\\min_{\\alpha \\in [0, 1]} \\phi_k(\\alpha)$。\n    f. 更新迭代点：$\\mathbf{x}_{k+1} = \\mathbf{x}_k - \\alpha_k \\nabla f(\\mathbf{x}_k)$。\n    g. 增加迭代计数器：$k \\leftarrow k+1$。\n3.  终止后，执行上述的分类和报告步骤，确保数值按要求四舍五入到 $6$ 位小数。\n对每个指定的初始点重复此整个过程。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import minimize_scalar\n\ndef solve():\n    \"\"\"\n    Implements the steepest descent algorithm for the Himmelblau function\n    from multiple starting points and formats the results.\n    \"\"\"\n    # Define problem constants and parameters\n    TOLERANCE = 1e-8\n    MAX_ITERATIONS = 10000\n    DISTANCE_THRESHOLD = 1e-3\n\n    # Define the Himmelblau function\n    def f(x, y):\n        return (x**2 + y - 11)**2 + (x + y**2 - 7)**2\n\n    # Define the gradient of the Himmelblau function\n    def grad_f(x, y):\n        df_dx = 4 * x * (x**2 + y - 11) + 2 * (x + y**2 - 7)\n        df_dy = 2 * (x**2 + y - 11) + 4 * y * (x + y**2 - 7)\n        return np.array([df_dx, df_dy])\n\n    # Known local minimizers of the Himmelblau function\n    minimizers = np.array([\n        [3.0, 2.0],\n        [-2.805118, 3.131312],\n        [-3.779310, -3.283186],\n        [3.584428, -1.848126]\n    ])\n\n    # Test suite of initial points\n    test_cases = [\n        (3.0, 2.0),\n        (0.0, 0.0),\n        (-3.0, -3.0),\n        (-4.0, 4.0),\n        (5.0, 5.0),\n        (3.5, -2.5)\n    ]\n\n    all_results = []\n\n    for initial_point in test_cases:\n        x_k = np.array(initial_point, dtype=float)\n        \n        # Iteration loop for steepest descent\n        k = 0\n        while k < MAX_ITERATIONS:\n            grad = grad_f(x_k[0], x_k[1])\n            grad_norm = np.linalg.norm(grad)\n\n            # Termination condition: gradient norm is below tolerance\n            if grad_norm <= TOLERANCE:\n                break\n\n            # Descent direction\n            d_k = -grad\n\n            # Line search for optimal step size alpha\n            # Define the 1D function to minimize\n            phi = lambda alpha: f(x_k[0] + alpha * d_k[0], x_k[1] + alpha * d_k[1])\n            \n            # Use a bounded scalar minimizer to find alpha in [0, 1]\n            res = minimize_scalar(phi, bounds=(0, 1), method='bounded')\n            alpha_k = res.x\n\n            # Update the iterate\n            x_k = x_k + alpha_k * d_k\n            \n            k += 1\n\n        # At this point, the loop has terminated. 'k' is the number of iterations.\n        x_star, y_star = x_k[0], x_k[1]\n        f_star = f(x_star, y_star)\n\n        # Classification: find the closest known minimizer\n        distances = np.linalg.norm(minimizers - x_k, axis=1)\n        min_dist_idx = np.argmin(distances)\n        min_dist = distances[min_dist_idx]\n        \n        label = -1\n        if min_dist <= DISTANCE_THRESHOLD:\n            label = int(min_dist_idx)\n        \n        # Store results for this case\n        all_results.append([x_star, y_star, f_star, k, label])\n\n    # Format the final output string according to the specification\n    result_parts = []\n    for res in all_results:\n        x_s, y_s, f_s, num_iter, lab = res\n        part = f\"[{x_s:.6f},{y_s:.6f},{f_s:.6f},{num_iter},{lab}]\"\n        result_parts.append(part)\n    \n    final_output = f\"[{','.join(result_parts)}]\"\n    print(final_output)\n\nsolve()\n```", "id": "2448739"}]}