{"hands_on_practices": [{"introduction": "牛顿法因其二次收敛速度而备受青睐，但这并非没有代价。此练习揭示了纯牛顿法的一个基本缺陷：当目标函数非凸时，其产生的步长不一定是下降方向，甚至可能导致函数值增加。通过一个简单的二次函数示例，你将亲手验证这一现象，从而深刻理解为何需要线搜索或信赖域等全局化策略来确保算法的稳定收敛。[@problem_id:2417369]", "problem": "设函数 $f:\\mathbb{R}^2 \\to \\mathbb{R}$ 定义为 $f(x_1,x_2)=\\tfrac{1}{2}x_1^2-\\tfrac{1}{2}x_2^2$。从 $x_0=(0,1)$ 开始，使用全步长更新 $x_{k+1}=x_k-\\left[\\nabla^2 f(x_k)\\right]^{-1}\\nabla f(x_k)$ 生成牛顿迭代点以寻找一个驻点。证明在所产生的迭代点序列中，梯度 $\\|\\nabla f(x_k)\\|_2$ 的欧几里得范数单调递减，而函数值 $f(x_k)$ 在第一次迭代时增加，即 $f(x_1)>f(x_0)$。计算 $\\Delta f=f(x_1)-f(x_0)$ 的精确值。请以精确值的形式给出你的答案，无需进行舍入。", "solution": "对问题陈述进行验证。\n\n步骤1：提取已知条件\n- 目标函数: $f:\\mathbb{R}^2 \\to \\mathbb{R}$ 定义为 $f(x_1,x_2)=\\tfrac{1}{2}x_1^2-\\tfrac{1}{2}x_2^2$。\n- 初始点: $x_0=(0,1)$。\n- 迭代规则: 全步长牛顿法, $x_{k+1}=x_k-\\left[\\nabla^2 f(x_k)\\right]^{-1}\\nabla f(x_k)$。\n- 任务1: 证明梯度的欧几里得范数 $\\|\\nabla f(x_k)\\|_2$ 单调递减。\n- 任务2: 证明 $f(x_1)>f(x_0)$。\n- 任务3: 计算 $\\Delta f=f(x_1)-f(x_0)$ 的精确值。\n\n步骤2：使用提取的已知条件进行验证\n该问题具有科学依据，是数值优化中的一个标准练习，具体涉及牛顿法。该问题是适定的，提供了所有必要信息：函数、起始点和迭代方法都已明确定义。该函数无限可微，其海森矩阵是常数且可逆的，这确保了牛顿步始终是良定义的。问题是客观的，不含任何歧义。任务是证明所得迭代点的特定、可验证的性质，并计算一个精确的量。该问题是将牛顿法应用于非凸二次函数的一个有效且具有指导意义的例子。\n\n步骤3：结论与操作\n问题有效。将提供完整解答。\n\n目标函数由 $f(x_1, x_2) = \\frac{1}{2}x_1^2 - \\frac{1}{2}x_2^2$ 给出。我们使用牛顿法寻找一个驻点。驻点 $x^*$ 满足 $\\nabla f(x^*) = 0$。\n\n首先，我们对于一个通用点 $x = \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix}$ 计算梯度向量 $\\nabla f(x)$ 和海森矩阵 $\\nabla^2 f(x)$。\n梯度为：\n$$\n\\nabla f(x) = \\begin{pmatrix} \\frac{\\partial f}{\\partial x_1} \\\\ \\frac{\\partial f}{\\partial x_2} \\end{pmatrix} = \\begin{pmatrix} x_1 \\\\ -x_2 \\end{pmatrix}\n$$\n海森矩阵为：\n$$\n\\nabla^2 f(x) = \\begin{pmatrix} \\frac{\\partial^2 f}{\\partial x_1^2} & \\frac{\\partial^2 f}{\\partial x_1 \\partial x_2} \\\\ \\frac{\\partial^2 f}{\\partial x_2 \\partial x_1} & \\frac{\\partial^2 f}{\\partial x_2^2} \\end{pmatrix} = \\begin{pmatrix} 1 & 0 \\\\ 0 & -1 \\end{pmatrix}\n$$\n对于所有 $x \\in \\mathbb{R}^2$，海森矩阵是常数。$\\nabla^2 f$ 的特征值为 $\\lambda_1 = 1$ 和 $\\lambda_2 = -1$。由于一个特征值为正，一个为负，因此海森矩阵是不定的。这意味着函数的驻点是一个鞍点。\n海森矩阵的逆矩阵为：\n$$\n\\left[\\nabla^2 f(x)\\right]^{-1} = \\begin{pmatrix} 1 & 0 \\\\ 0 & -1 \\end{pmatrix}^{-1} = \\begin{pmatrix} 1 & 0 \\\\ 0 & -1 \\end{pmatrix}\n$$\n海森矩阵是其自身的逆矩阵。\n\n牛顿更新规则是 $x_{k+1} = x_k - \\left[\\nabla^2 f(x_k)\\right]^{-1}\\nabla f(x_k)$。对于我们特定的函数，这变为：\n$$\nx_{k+1} = \\begin{pmatrix} x_{1,k} \\\\ x_{2,k} \\end{pmatrix} - \\begin{pmatrix} 1 & 0 \\\\ 0 & -1 \\end{pmatrix} \\begin{pmatrix} x_{1,k} \\\\ -x_{2,k} \\end{pmatrix} = \\begin{pmatrix} x_{1,k} \\\\ x_{2,k} \\end{pmatrix} - \\begin{pmatrix} x_{1,k} \\\\ x_{2,k} \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}\n$$\n这表明，对于任意不等于 $(0,0)$ 的起始点 $x_k$，牛顿法会在一次迭代中收敛到驻点 $(0,0)$。\n\n我们从 $x_0 = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$ 开始。\n第一个迭代点 $x_1$ 是：\n$$\nx_1 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}\n$$\n所有后续的迭代点都将停留在驻点上：对于所有 $k \\ge 1$，都有 $x_k = (0,0)$。\n\n现在，我们来解决问题中指定的任务。\n\n任务1：证明 $\\|\\nabla f(x_k)\\|_2$ 单调递减。\n我们计算每个迭代点的梯度。\n对于 $k=0$：\n$$\n\\nabla f(x_0) = \\nabla f(0,1) = \\begin{pmatrix} 0 \\\\ -1 \\end{pmatrix}\n$$\n其欧几里得范数为 $\\|\\nabla f(x_0)\\|_2 = \\sqrt{0^2 + (-1)^2} = 1$。\n对于 $k=1$：\n$$\n\\nabla f(x_1) = \\nabla f(0,0) = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}\n$$\n其欧几里得范数为 $\\|\\nabla f(x_1)\\|_2 = \\sqrt{0^2 + 0^2} = 0$。\n对于所有 $k > 1$，由于 $x_k = (0,0)$，我们有 $\\nabla f(x_k) = (0,0)$ 且 $\\|\\nabla f(x_k)\\|_2 = 0$。\n梯度范数的序列是 $1, 0, 0, 0, \\ldots$。这是一个单调不增（因此也是单调递减）的序列，因为 $1 > 0$ 且 $0 \\ge 0$。条件得到满足。\n\n任务2：证明 $f(x_1) > f(x_0)$。\n我们计算在 $x_0$ 和 $x_1$ 处的函数值。\n在起始点 $x_0 = (0,1)$ 处：\n$$\nf(x_0) = f(0,1) = \\frac{1}{2}(0)^2 - \\frac{1}{2}(1)^2 = -\\frac{1}{2}\n$$\n在第一个迭代点 $x_1 = (0,0)$ 处：\n$$\nf(x_1) = f(0,0) = \\frac{1}{2}(0)^2 - \\frac{1}{2}(0)^2 = 0\n$$\n比较这两个值，我们有 $f(x_1) = 0$ 和 $f(x_0) = -1/2$。确实，$0 > -1/2$，这证明了 $f(x_1) > f(x_0)$。函数值增加了。发生这种情况是因为海森矩阵不是正定的，因此牛顿方向不保证是下降方向。搜索方向为 $p_0 = x_1 - x_0 = (0,0) - (0,1) = (0,-1)$。从 $x_0$ 出发沿 $p_0$ 的方向导数为 $\\nabla f(x_0)^T p_0 = \\begin{pmatrix} 0 & -1 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ -1 \\end{pmatrix} = 1 > 0$，这证实了 $p_0$ 是一个上升方向。\n\n任务3：计算 $\\Delta f = f(x_1) - f(x_0)$ 的精确值。\n使用上一步计算出的值：\n$$\n\\Delta f = f(x_1) - f(x_0) = 0 - \\left(-\\frac{1}{2}\\right) = \\frac{1}{2}\n$$\n第一次迭代后函数值的变化量的精确值为 $\\frac{1}{2}$。", "answer": "$$\\boxed{\\frac{1}{2}}$$", "id": "2417369"}, {"introduction": "面对纯牛顿法在非凸问题上的局限性，信赖域方法提供了一个强大而稳健的解决方案。该方法并非寻找一个完整的牛顿步，而是在一个受信任的局部区域（“信赖域”）内求解一个二次模型的近似解。这个动手实践将指导你实现求解信赖域子问题的核心算法——Steihaug-Toint共轭梯度法，并亲身体验其如何巧妙地处理负曲率情况，从而保证算法的全局收敛性。[@problem_id:2417374]", "problem": "考虑$\\mathbb{R}^n$中的信赖域二次子问题：最小化二次模型 $m(\\mathbf{p}) = \\mathbf{g}^\\top \\mathbf{p} + \\tfrac{1}{2}\\mathbf{p}^\\top \\mathbf{B}\\mathbf{p}$，约束条件为欧几里得范数约束 $\\|\\mathbf{p}\\|_2 \\le \\Delta$，其中 $\\mathbf{B}\\in\\mathbb{R}^{n\\times n}$ 为对称矩阵，$\\mathbf{g}\\in\\mathbb{R}^n$ 是给定的，$\\Delta>0$ 为信赖域半径。矩阵 $\\mathbf{B}$ 可以是正定、半正定或不定的。\n\n编写一个完整的程序，针对下面列出的每个测试用例，返回一个由应用于二次模型 $m$ 的以下数学指定截断规则确定的步长向量 $\\mathbf{p}$：从 $\\mathbf{p}_0=\\mathbf{0}$、残差 $\\mathbf{r}_0=\\mathbf{g}$ 和搜索方向 $\\mathbf{d}_0=-\\mathbf{r}_0$ 开始。对于迭代索引 $k\\in\\{0,1,2,\\dots\\}$，定义曲率 $\\kappa_k=\\mathbf{d}_k^\\top \\mathbf{B}\\mathbf{d}_k$。如果 $\\kappa_k\\le 0$，则返回边界点 $\\mathbf{p}_\\tau=\\mathbf{p}_k+\\tau\\mathbf{d}_k$，其中选择 $\\tau>0$ 使得 $\\|\\mathbf{p}_k+\\tau\\mathbf{d}_k\\|_2=\\Delta$。否则，定义步长 $\\alpha_k=(\\mathbf{r}_k^\\top \\mathbf{r}_k)/\\kappa_k$ 和试探点 $\\mathbf{p}_{k+1}=\\mathbf{p}_k+\\alpha_k\\mathbf{d}_k$。如果 $\\|\\mathbf{p}_{k+1}\\|_2\\ge \\Delta$，则返回边界点 $\\mathbf{p}_\\tau=\\mathbf{p}_k+\\tau\\mathbf{d}_k$，其中 $\\tau\\in(0,\\alpha_k]$ 满足 $\\|\\mathbf{p}_k+\\tau\\mathbf{d}_k\\|_2=\\Delta$。否则，更新 $\\mathbf{r}_{k+1}=\\mathbf{r}_k+\\alpha_k\\mathbf{B}\\mathbf{d}_k$。如果 $\\|\\mathbf{r}_{k+1}\\|_2$ 相对于 $\\|\\mathbf{g}\\|_2$ 足够小，则返回 $\\mathbf{p}_{k+1}$。否则，设置 $\\beta_k=(\\mathbf{r}_{k+1}^\\top \\mathbf{r}_{k+1})/(\\mathbf{r}_k^\\top \\mathbf{r}_k)$ 和 $\\mathbf{d}_{k+1}=-\\mathbf{r}_{k+1}+\\beta_k\\mathbf{d}_k$ 并继续。该过程由所述方程和边界交点条件 $\\|\\mathbf{p}_k+\\tau\\mathbf{d}_k\\|_2=\\Delta$（即关于标量 $\\tau$ 的二次方程的正根）明确定义。\n\n每个测试用例中仅使用给定数据。不允许外部输入。不涉及角度。此问题中没有物理单位，因此答案应为纯实数。\n\n测试套件（每个测试用例是一个三元组 $(\\mathbf{B},\\mathbf{g},\\Delta)$）：\n- 情况1（正定曲率的内部解）：$\\mathbf{B}=\\begin{bmatrix}2 & 0\\\\ 0 & 4\\end{bmatrix}$, $\\mathbf{g}=\\begin{bmatrix}1\\\\ 2\\end{bmatrix}$, $\\Delta=5$.\n- 情况2（正定曲率的边界解）：$\\mathbf{B}=\\begin{bmatrix}2 & 0\\\\ 0 & 4\\end{bmatrix}$, $\\mathbf{g}=\\begin{bmatrix}1\\\\ 2\\end{bmatrix}$, $\\Delta=0.2$.\n- 情况3（不定矩阵并立即出现负曲率）：$\\mathbf{B}=\\begin{bmatrix}1 & 0\\\\ 0 & -1\\end{bmatrix}$, $\\mathbf{g}=\\begin{bmatrix}0\\\\ 1\\end{bmatrix}$, $\\Delta=0.5$.\n- 情况4（零梯度边界情况）：$\\mathbf{B}=\\begin{bmatrix}1 & 2\\\\ 2 & 1\\end{bmatrix}$, $\\mathbf{g}=\\begin{bmatrix}0\\\\ 0\\end{bmatrix}$, $\\Delta=1$.\n\n对于每种情况，计算由所述截断规则确定的步长向量 $\\mathbf{p}\\in\\mathbb{R}^2$。\n\n您的程序应生成单行输出，其中包含用方括号括起来的逗号分隔的结果列表，每个元素是对应一个测试用例的步长向量，其本身也用方括号括起来并以逗号分隔。例如，四个二维结果的有效格式是 $[[p_{1,1},p_{1,2}],[p_{2,1},p_{2,2}],[p_{3,1},p_{3,2}],[p_{4,1},p_{4,2}]]$，其中每个 $p_{i,j}$ 都以十进制实数形式打印。", "solution": "所提供的问题是数值优化中一个标准的信赖域子问题。我们必须找到一个步长向量 $\\mathbf{p} \\in \\mathbb{R}^n$ 来最小化二次模型 $m(\\mathbf{p}) = \\mathbf{g}^\\top \\mathbf{p} + \\frac{1}{2}\\mathbf{p}^\\top \\mathbf{B}\\mathbf{p}$，且满足约束 $\\|\\mathbf{p}\\|_2 \\le \\Delta$，其中 $\\mathbf{g} \\in \\mathbb{R}^n$ 是梯度，$\\mathbf{B} \\in \\mathbb{R}^{n \\times n}$ 是一个近似Hessian矩阵的对称矩阵，$\\Delta > 0$ 是信赖域半径。\n\n问题陈述指定了一种特定的算法来寻找近似解。该算法是共轭梯度（CG）法的一种截断形式，通常称为Steihaug-Toint方法。它旨在处理 $\\mathbf{B}$ 非正定的情况，这是信赖域方法的一个关键要求。该算法的构造如下：\n\n初始状态（迭代 $k=0$）：\n起点是原点，$\\mathbf{p}_0 = \\mathbf{0}$。初始残差是梯度，$\\mathbf{r}_0 = \\mathbf{g}$，初始搜索方向是最速下降方向，$\\mathbf{d}_0 = -\\mathbf{r}_0 = -\\mathbf{g}$。\n\n迭代过程（$k = 0, 1, 2, \\dots$）：\n算法迭代进行，构建一个点序列 $\\mathbf{p}_k$ 来近似 $m(\\mathbf{p})$ 的极小化子，但有特定的检查以确保解保持在信赖域内，并处理非正曲率。\n\n1. 曲率检查：在每次迭代 $k$ 中，我们计算沿搜索方向 $\\mathbf{d}_k$ 的曲率：$\\kappa_k = \\mathbf{d}_k^\\top \\mathbf{B} \\mathbf{d}_k$。如果 $\\kappa_k \\le 0$，则二次模型 $m(\\mathbf{p})$ 沿 $\\mathbf{d}_k$ 是非凸的。随着 $\\tau > 0$ 增加，沿着路径 $\\mathbf{p}_k + \\tau\\mathbf{d}_k$ 将导致 $m(\\mathbf{p})$ 无界下降。算法的规则是停止并沿此方向尽可能远地移动，直到达到信赖域边界。这需要找到 $\\tau > 0$ 使得 $\\|\\mathbf{p}_k + \\tau \\mathbf{d}_k\\|_2 = \\Delta$。解即为 $\\mathbf{p} = \\mathbf{p}_k + \\tau \\mathbf{d}_k$。\n\n2. 边界检查：如果曲率 $\\kappa_k$ 为正，则计算标准CG步长：$\\alpha_k = (\\mathbf{r}_k^\\top \\mathbf{r}_k) / \\kappa_k$。这个 $\\alpha_k$ 沿直线 $\\mathbf{p}_k + \\alpha\\mathbf{d}_k$ 最小化二次模型。计算一个试探点：$\\mathbf{p}_{k+1} = \\mathbf{p}_k + \\alpha_k \\mathbf{d}_k$。然后我们必须检查该点是否已离开信赖域。如果 $\\|\\mathbf{p}_{k+1}\\|_2 \\ge \\Delta$，则从 $\\mathbf{p}_k$ 出发的路径已越过边界。算法终止并返回交点 $\\mathbf{p} = \\mathbf{p}_k + \\tau \\mathbf{d}_k$，其中选择 $\\tau \\in (0, \\alpha_k]$ 使得 $\\|\\mathbf{p}_k + \\tau \\mathbf{d}_k\\|_2 = \\Delta$。\n\n3. 收敛与更新：如果试探点 $\\mathbf{p}_{k+1}$ 在信赖域内部，我们使用恒等式 $\\mathbf{r}_{k+1} = \\mathbf{r}_k + \\alpha_k \\mathbf{B} \\mathbf{d}_k$ 更新残差。如果新残差的范数 $\\|\\mathbf{r}_{k+1}\\|_2$ 足够小（例如，相对于初始残差范数 $\\|\\mathbf{g}\\|_2$），则算法已收敛，解为 $\\mathbf{p} = \\mathbf{p}_{k+1}$。如果未收敛，则使用 Fletcher-Reeves 公式构造新的搜索方向：$\\beta_k = (\\mathbf{r}_{k+1}^\\top \\mathbf{r}_{k+1}) / (\\mathbf{r}_k^\\top \\mathbf{r}_k)$，以及 $\\mathbf{d}_{k+1} = -\\mathbf{r}_{k+1} + \\beta_k \\mathbf{d}_k$。然后对下一次迭代重复此过程。\n\n求解边界交点 $\\tau$：\n条件 $\\|\\mathbf{p}_k + \\tau \\mathbf{d}_k\\|_2 = \\Delta$ 可转换为一个关于 $\\tau$ 的二次方程：\n$$(\\mathbf{d}_k^\\top \\mathbf{d}_k) \\tau^2 + (2 \\mathbf{p}_k^\\top \\mathbf{d}_k) \\tau + (\\mathbf{p}_k^\\top \\mathbf{p}_k - \\Delta^2) = 0$$\n令 $a = \\mathbf{d}_k^\\top \\mathbf{d}_k$，$b = 2 \\mathbf{p}_k^\\top \\mathbf{d}_k$，$c = \\mathbf{p}_k^\\top \\mathbf{p}_k - \\Delta^2$。由于在执行此计算时，当前点 $\\mathbf{p}_k$ 始终严格位于信赖域内部，我们有 $\\|\\mathbf{p}_k\\|_2 < \\Delta$，这意味着 $c < 0$。由于 $a = \\|\\mathbf{d}_k\\|_2^2 > 0$（除非 $\\mathbf{d}_k = \\mathbf{0}$，这是一个平凡情况），乘积 $ac$ 为负。这保证了判别式 $b^2 - 4ac$ 为正，从而得到两个不同的实根 $\\tau$。一个根为正，一个为负。问题要求 $\\tau > 0$，所以我们必须选择正根：\n$$ \\tau = \\frac{-b + \\sqrt{b^2 - 4ac}}{2a} $$\n\n提供的测试用例涵盖了主要场景：\n-   情况1：找到了一个内部解，因为 $m(\\mathbf{p})$ 的无约束极小化子位于信赖域内。算法在 $n=2$ 次迭代后终止，残差接近于零。\n-   情况2：第一个CG步尝试向无约束极小化子移动，但由于信赖域半径 $\\Delta$ 很小，该步离开了信赖域。算法通过找到与边界的交点而终止。\n-   情况3：矩阵 $\\mathbf{B}$ 是不定的，初始搜索方向被证明是一个负曲率方向（$\\kappa_0 < 0$）。算法立即终止，通过找到沿该方向与边界的交点。\n-   情况4：梯度为零，$\\mathbf{g} = \\mathbf{0}$。因此，初始残差 $\\mathbf{r}_0$ 和搜索方向 $\\mathbf{d}_0$ 也为 $\\mathbf{0}$。曲率 $\\kappa_0$ 为 $0$。算法在第一步终止，返回起始点 $\\mathbf{p} = \\mathbf{0}$。\n\n程序精确地按照规定实现了这一逻辑。对于收敛准则，使用了相对容差：$\\|\\mathbf{r}_{k+1}\\|_2 < \\epsilon \\|\\mathbf{g}\\|_2$，其中 $\\epsilon = 10^{-12}$ 是一个很小的容差。对于给定的二维问题，CG方法最多在两次迭代内精确收敛，因此容差的选择不是关键。最大迭代次数被限制为 $2n=4$ 作为保障，尽管预计不会达到这个次数。", "answer": "```python\nimport numpy as np\n\ndef find_tau(p, d, Delta):\n    \"\"\"\n    Solves for tau > 0 such that ||p + tau*d||_2 = Delta.\n    This is a quadratic equation in tau: a*tau^2 + b*tau + c = 0.\n    \"\"\"\n    a = np.dot(d, d)\n    b = 2 * np.dot(p, d)\n    c = np.dot(p, p) - Delta**2\n    \n    # Since p is inside the trust region, ||p|| < Delta, so c < 0.\n    # Also a = ||d||^2 > 0.\n    # Therefore, the discriminant b^2 - 4ac is always positive.\n    discriminant = b**2 - 4 * a * c\n    \n    # The quadratic equation has one positive and one negative root. We need the positive one.\n    # The formula (-b + sqrt(discriminant))/(2a) gives the positive root.\n    tau = (-b + np.sqrt(discriminant)) / (2 * a)\n    return tau\n\ndef solve_truncated_cg(B, g, Delta, tol=1e-12, max_iter=None):\n    \"\"\"\n    Implements the truncated conjugate gradient method for the trust-region subproblem\n    as specified in the problem description.\n    \"\"\"\n    n = len(g)\n    if max_iter is None:\n        max_iter = 2 * n  # A safe upper bound on iterations\n\n    p = np.zeros(n)\n    r = g.copy()\n    d = -r\n\n    g_norm = np.linalg.norm(g)\n    if g_norm == 0:\n        # If g=0, then r0=0, d0=0, kappa0=0. Algorithm terminates with p=0.\n        return p\n\n    r_sq_norm = np.dot(r, r)\n\n    for k in range(max_iter):\n        Bd = B @ d\n        kappa = np.dot(d, Bd)\n\n        # Termination Condition 1: Non-positive curvature\n        if kappa <= 0:\n            tau = find_tau(p, d, Delta)\n            return p + tau * d\n\n        alpha = r_sq_norm / kappa\n        p_next = p + alpha * d\n\n        # Termination Condition 2: Exceeds trust region\n        if np.linalg.norm(p_next) >= Delta:\n            tau = find_tau(p, d, Delta)\n            return p + tau * d\n\n        # Update step\n        p = p_next\n        r_next = r + alpha * Bd\n\n        # Termination Condition 3: Convergence\n        if np.linalg.norm(r_next) < tol * g_norm:\n            return p\n\n        # Prepare for next iteration\n        r_sq_norm_next = np.dot(r_next, r_next)\n        beta = r_sq_norm_next / r_sq_norm\n        \n        r = r_next\n        r_sq_norm = r_sq_norm_next\n        \n        d = -r + beta * d\n        \n    return p # Should not be reached for small n, but good practice\n\ndef solve():\n    \"\"\"\n    Main function to define test cases, solve them, and print the output\n    in the specified format.\n    \"\"\"\n    test_cases = [\n        (np.array([[2, 0], [0, 4]]), np.array([1, 2]), 5.0),\n        (np.array([[2, 0], [0, 4]]), np.array([1, 2]), 0.2),\n        (np.array([[1, 0], [0, -1]]), np.array([0, 1]), 0.5),\n        (np.array([[1, 2], [2, 1]]), np.array([0, 0]), 1.0),\n    ]\n\n    results = []\n    for case in test_cases:\n        B, g, Delta = case\n        p_sol = solve_truncated_cg(B, g, Delta)\n        results.append(p_sol.tolist())\n\n    # Format the final output string precisely as required:\n    # [[p1_1,p1_2],[p2_1,p2_2],...] with no extra spaces.\n    formatted_results = [f\"[{res[0]},{res[1]}]\" for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "2417374"}, {"introduction": "对于维度极高的大规模问题，计算和存储海森矩阵（或其逆）变得不切实际，这正是拟牛顿法大显身手的领域。这个综合性练习将理论与重要的机器学习应用相结合，要求你从零开始实现限制内存的BFGS（L-BFGS）算法。你将推导并实现其核心的双循环递归，用以训练一个多分类逻辑回归模型，切身感受在没有二阶信息的情况下，如何高效地解决复杂的现实世界优化问题。[@problem_id:2417391]", "problem": "你必须编写一个完整、可运行的程序，从第一性原理出发实现限制内存 Broyden–Fletcher–Goldfarb–Shanno (L-BFGS) 算法，通过最小化负对数似然来训练一个多分类逻辑回归模型。目标是从核心定义开始推导并实现该算法，然后将其应用于一个小型测试套件。最终输出必须是包含所有测试结果的单行，并遵循指定的格式。\n\n模型定义如下。给定数据矩阵 $\\mathbf{X} \\in \\mathbb{R}^{n \\times d}$、标签 $\\mathbf{y} \\in \\{0, 1, \\dots, K-1\\}^{n}$，以及通过在 $\\mathbf{X}$ 的每一行附加一个偏置特征 $1$ 形成的增广特征 $\\tilde{\\mathbf{X}}$（因此 $\\tilde{\\mathbf{X}} \\in \\mathbb{R}^{n \\times (d+1)}$），模型参数汇集在权重矩阵 $\\mathbf{W} \\in \\mathbb{R}^{(d+1) \\times K}$ 中。对于每个样本索引 $i \\in \\{1,\\dots,n\\}$，类别得分为 $\\mathbf{z}_i = \\tilde{\\mathbf{x}}_i^{\\top} \\mathbf{W} \\in \\mathbb{R}^{K}$，类别概率由 softmax 映射给出\n$$\np_{i,k} = \\frac{\\exp(z_{i,k})}{\\sum_{j=1}^{K} \\exp(z_{i,j})}, \\quad \\text{for } k \\in \\{1,\\dots,K\\}.\n$$\n带有 $\\ell_{2}$ 正则化（包括偏置权重）的负对数似然为\n$$\nf(\\mathbf{W}) = - \\sum_{i=1}^{n} \\log p_{i, y_i} + \\frac{\\lambda}{2} \\lVert \\mathbf{W} \\rVert_{F}^{2},\n$$\n其中 $\\lambda \\ge 0$ 是给定的正则化系数，$\\lVert \\cdot \\rVert_{F}$ 表示 Frobenius 范数。\n\n任务：\n- 从以上 softmax 函数和负对数似然的核心定义出发，推导梯度 $\\nabla f(\\mathbf{W})$ 的表达式，除了链式法则、乘积法则和指数函数的标准性质外，不得引用任何已有的结论。你的推导必须依赖于概率 $p_{i,k}$ 的定义、标签的独热编码（one-hot encoding）以及对数函数的导数，并且必须将梯度表示为可以高效实现的矩阵形式。\n- 推导限制内存 Broyden–Fletcher–Goldfarb–Shanno (L-BFGS) 搜索方向公式，该公式基于割线条件和正定逆 Hessian 矩阵更新。你的推导必须从 Broyden–Fletcher–Goldfarb–Shanno (BFGS) 的逆 Hessian 矩阵更新公式开始，解释限制内存的思想，并最终得出一个双循环递归，该递归仅使用最近一小部分曲率对来计算逆 Hessian 矩阵近似与当前梯度的乘积。解释曲率条件 $s_k^{\\top} y_k > 0$ 的必要性，以及如何处理在有限精度算术中违反该条件的情况。\n- 从头开始实现一个用于最小化 $f(\\mathbf{W})$ 的 L-BFGS 求解器，不得使用任何库中的优化器。你的实现必须包括：\n  - 使用最后 $m$ 个曲率对 $(s_k, y_k)$（其中 $s_k = w_{k+1} - w_k$，$y_k = \\nabla f(w_{k+1}) - \\nabla f(w_k)$，并且 $w$ 是 $\\mathbf{W}$ 的向量化形式）来计算搜索方向的双循环递归。\n  - 初始逆 Hessian 矩阵缩放 $H_0 = \\gamma_k \\mathbf{I}$，其中当 $s_{k-1}$ 和 $y_{k-1}$ 可用时 $\\gamma_k = \\frac{s_{k-1}^{\\top} y_{k-1}}{y_{k-1}^{\\top} y_{k-1}}$，否则 $H_0 = \\mathbf{I}$。\n  - 强制执行 Armijo 条件 $f(w_k + \\alpha p_k) \\le f(w_k) + c_1 \\alpha \\nabla f(w_k)^{\\top} p_k$ 的回溯线搜索，参数为 $c_1 \\in (0, 1)$，初始步长 $\\alpha_0 = 1$，收缩因子 $\\tau \\in (0, 1)$。\n  - 保障措施：如果 $p_k^{\\top} \\nabla f(w_k) \\ge 0$，则退回到最速下降方向 $p_k = -\\nabla f(w_k)$；如果 $s_k^{\\top} y_k \\le \\epsilon$（其中 $\\epsilon > 0$ 是一个小数），则跳过存储新的曲率对，以保持正定性。\n  - 基于梯度的无穷范数 $\\lVert \\nabla f(w_k) \\rVert_{\\infty} \\le \\text{tol}$ 或最大迭代次数的停止准则。\n- 将你的实现应用于以下测试套件。所有特征都是二维的，并附加了一个偏置项, 类别数量为 $K = 3$，标记为 $0$、$1$ 和 $2$。数据集是确定性的点列表，因此不需要伪随机数生成器。\n\n测试套件：\n- 测试用例 A（类别区分良好，中等正则化）：\n  - 数据：$\\mathbf{X}_A$ 包含 $n_A = 15$ 个样本，按类别组织如下（每行为 $(x_1, x_2)$）：\n    - 类别 $0$：$(0.0, 0.0)$, $(0.2, -0.1)$, $(-0.1, 0.1)$, $(0.1, 0.2)$, $(-0.2, 0.0)$。\n    - 类别 $1$：$(3.0, 3.0)$, $(2.9, 3.1)$, $(3.1, 2.8)$, $(3.2, 3.2)$, $(2.8, 2.9)$。\n    - 类别 $2$：$(-3.0, 3.0)$, $(-3.1, 3.2)$, $(-2.9, 2.8)$, $(-3.2, 2.9)$, $(-2.8, 3.1)$。\n  - 标签 $\\mathbf{y}_A$：五个 0，接着是五个 1，然后是五个 2。\n  - 正则化：$\\lambda_A = 0.1$。\n  - L-BFGS 超参数：内存 $m = 7$，梯度容差 $\\text{tol} = 10^{-8}$，最大迭代次数 $200$，线搜索常数 $c_1 = 10^{-4}$ 和收缩因子 $\\tau = 0.5$。\n  - 此案例的必需输出：最小化的目标函数值 $f(\\mathbf{W}^{\\star}_A)$，作为浮点数并四舍五入到六位小数。\n- 测试用例 B（数据相同，强正则化）：\n  - 数据和标签与测试用例 A 相同。\n  - 正则化：$\\lambda_B = 100$。\n  - L-BFGS 超参数与测试用例 A 相同。\n  - 此案例的必需输出：优化后权重矩阵的 Frobenius 范数 $\\lVert \\mathbf{W}^{\\star}_B \\rVert_F$，作为浮点数并四舍五入到六位小数。\n- 测试用例 C（共线性特征，弱正则化）：\n  - 数据：$\\mathbf{X}_C$ 包含 $n_C = 9$ 个样本，按类别排列：\n    - 类别 $0$：$(-2.0, -6.0)$, $(-1.0, -3.0)$, $(-1.5, -4.5)$。\n    - 类别 $1$：$(-0.2, -0.6)$, $(0.0, 0.0)$, $(0.5, 1.5)$。\n    - 类别 $2$：$(1.5, 4.5)$, $(2.0, 6.0)$, $(2.2, 6.6)$。\n  - 标签 $\\mathbf{y}_C$：三个 0，然后三个 1，然后三个 2。\n  - 正则化：$\\lambda_C = 0.01$。\n  - L-BFGS 超参数与测试用例 A 相同。\n  - 此案例的必需输出：一个布尔值，指示返回解的梯度无穷范数是否满足 $\\lVert \\nabla f(\\mathbf{W}^{\\star}_C) \\rVert_{\\infty} \\le 10^{-6}$。\n\n角度单位不适用。此问题中没有物理单位。\n\n最终输出格式：\n- 你的程序应生成单行输出，其中包含三个测试用例的结果，形式为方括号括起来的逗号分隔列表。条目必须按此顺序出现：$[ \\text{A-result}, \\text{B-result}, \\text{C-result} ]$，其中 A-result 和 B-result 是四舍五入到六位小数的浮点数，C-result 是一个布尔值。例如：$[1.234000,0.567890,True]$。", "solution": "所述问题定义明确，数学上连贯，并且在数值优化和机器学习领域有坚实的科学基础。它提出了计算工程中的一个标准但并非无足轻重的任务：为一个基础统计模型推导并实现一个核心优化算法。所有定义、参数和测试用例都以足够的精度给出，以确保一个唯一且可验证的解。因此，我们直接进行形式化推导及后续的实现。\n\n任务是最小化目标函数 $f(\\mathbf{W})$，该函数用于一个多分类逻辑回归模型：\n$$\nf(\\mathbf{W}) = - \\sum_{i=1}^{n} \\log p_{i, y_i} + \\frac{\\lambda}{2} \\lVert \\mathbf{W} \\rVert_{F}^{2}\n$$\n其中 $\\mathbf{W} \\in \\mathbb{R}^{(d+1) \\times K}$ 是模型权重，$\\lambda \\geq 0$ 是正则化参数，$p_{i, y_i}$ 是模型为样本 $i$ 的真实类别 $y_i$ 预测的概率。优化将使用限制内存 Broyden–Fletcher–Goldfarb–Shanno (L-BFGS) 算法执行，该算法需要目标函数的梯度 $\\nabla f(\\mathbf{W})$。\n\n**1. 梯度 $\\nabla f(\\mathbf{W})$ 的推导**\n\n目标函数是两个部分的总和：负对数似然 $L(\\mathbf{W}) = - \\sum_{i=1}^{n} \\log p_{i, y_i}$ 和 $\\ell_2$ 正则化项 $R(\\mathbf{W}) = \\frac{\\lambda}{2} \\lVert \\mathbf{W} \\rVert_{F}^{2}$。根据微分的线性性质，梯度为 $\\nabla f(\\mathbf{W}) = \\nabla L(\\mathbf{W}) + \\nabla R(\\mathbf{W})$。\n\n首先，正则化项的梯度是基础的。Frobenius 范数的平方是 $\\lVert \\mathbf{W} \\rVert_{F}^{2} = \\sum_{j=1}^{d+1}\\sum_{k=1}^{K} W_{jk}^2$。关于任意权重 $W_{ab}$ 的偏导数是：\n$$\n\\frac{\\partial R(\\mathbf{W})}{\\partial W_{ab}} = \\frac{\\partial}{\\partial W_{ab}} \\left( \\frac{\\lambda}{2} \\sum_{j,k} W_{jk}^2 \\right) = \\frac{\\lambda}{2} (2W_{ab}) = \\lambda W_{ab}\n$$\n写成矩阵形式，这给出 $\\nabla R(\\mathbf{W}) = \\lambda \\mathbf{W}$。\n\n接下来，我们推导负对数似然项的梯度 $\\nabla L(\\mathbf{W})$。我们使用链式法则。来自单个样本 $i$ 的损失贡献是 $L_i = -\\log p_{i, y_i}$。类别概率 $p_{i,k}$ 依赖于得分 $z_{i,k}$，而得分又依赖于权重 $\\mathbf{W}$。得分由 $z_{i,k} = \\tilde{\\mathbf{x}}_i^{\\top} \\mathbf{w}_k$ 给出，其中 $\\tilde{\\mathbf{x}}_i \\in \\mathbb{R}^{d+1}$ 是样本 $i$ 的增广特征向量，$\\mathbf{w}_k$ 是 $\\mathbf{W}$ 的第 $k$ 列。用矩阵表示法为 $\\mathbf{Z} = \\tilde{\\mathbf{X}}\\mathbf{W}$。样本 $i$ 属于类别 $k$ 的概率由 softmax 函数给出：\n$$\np_{i,k} = \\frac{\\exp(z_{i,k})}{\\sum_{j=1}^{K} \\exp(z_{i,j})}\n$$\n样本 $i$ 的损失可以重写为：\n$$\nL_i = -\\log p_{i, y_i} = -z_{i, y_i} + \\log\\left(\\sum_{j=1}^{K} \\exp(z_{i,j})\\right)\n$$\n我们首先求 $L_i$ 关于得分 $z_{i,k}$ 的偏导数：\n$$\n\\frac{\\partial L_i}{\\partial z_{i,k}} = -\\delta_{k, y_i} + \\frac{\\exp(z_{i,k})}{\\sum_{j=1}^{K} \\exp(z_{i,j})} = p_{i,k} - \\delta_{k, y_i}\n$$\n其中 $\\delta_{k, y_i}$ 是 Kronecker delta，当 $k=y_i$ 时为 $1$，否则为 $0$。这一项可以由一个独热编码的标签矩阵 $\\mathbf{Y} \\in \\mathbb{R}^{n \\times K}$ 表示，其中 $Y_{ik} = \\delta_{k, y_i}$。\n\n接下来，我们求得分 $z_{i,k}$ 关于权重 $W_{ab}$（$\\mathbf{W}$ 的第 $a$ 行，第 $b$ 列）的偏导数：\n$$\nz_{i,k} = \\sum_{j=1}^{d+1} \\tilde{x}_{ij} W_{jk} \\implies \\frac{\\partial z_{i,k}}{\\partial W_{ab}} = \\tilde{x}_{ia} \\delta_{kb}\n$$\n使用链式法则，总似然 $L = \\sum_i L_i$ 关于 $W_{ab}$ 的偏导数是：\n$$\n\\frac{\\partial L}{\\partial W_{ab}} = \\sum_{i=1}^{n} \\frac{\\partial L_i}{\\partial W_{ab}} = \\sum_{i=1}^{n} \\sum_{k=1}^{K} \\frac{\\partial L_i}{\\partial z_{i,k}} \\frac{\\partial z_{i,k}}{\\partial W_{ab}} = \\sum_{i=1}^{n} \\sum_{k=1}^{K} (p_{i,k} - Y_{ik}) (\\tilde{x}_{ia} \\delta_{kb})\n$$\n对 $k$ 的求和中只有一个非零项，即 $k=b$ 时。因此：\n$$\n\\frac{\\partial L}{\\partial W_{ab}} = \\sum_{i=1}^{n} \\tilde{x}_{ia} (p_{ib} - Y_{ib})\n$$\n这个表达式正是矩阵乘积 $\\tilde{\\mathbf{X}}^{\\top}(\\mathbf{P} - \\mathbf{Y})$ 的 $(a,b)$ 元素，其中 $\\mathbf{P}$ 是由概率 $p_{ik}$ 构成的 $n \\times K$ 矩阵。\n结合两项的梯度，我们得到 $f(\\mathbf{W})$ 梯度的最终表达式：\n$$\n\\nabla f(\\mathbf{W}) = \\tilde{\\mathbf{X}}^{\\top}(\\mathbf{P} - \\mathbf{Y}) + \\lambda \\mathbf{W}\n$$\n这种矩阵形式对于高效实现至关重要。\n\n**2. L-BFGS 算法**\n\nL-BFGS 算法是一种拟牛顿法，它通过近似目标函数的逆 Hessian 矩阵来确定搜索方向。它专为大规模问题设计，在这些问题中，构建、存储或求逆真实或近似的 Hessian 矩阵是不可行的。令 $w$ 为权重矩阵 $\\mathbf{W}$ 的向量化形式，$g_k = \\nabla f(w_k)$ 为在第 $k$ 次迭代时对应的扁平化梯度。\n\nBFGS 的基础是割线方程，$B_{k+1}s_k = y_k$，其中 $s_k = w_{k+1} - w_k$ 是参数的步长，$y_k = g_{k+1} - g_k$ 是梯度的变化。逆 Hessian 矩阵近似 $H_k \\approx (\\nabla^2 f(w_k))^{-1}$ 的 BFGS 更新公式为：\n$$\nH_{k+1} = (I - \\rho_k s_k y_k^{\\top}) H_k (I - \\rho_k y_k s_k^{\\top}) + \\rho_k s_k s_k^{\\top}, \\quad \\text{where } \\rho_k = \\frac{1}{y_k^{\\top} s_k}\n$$\n如果 $H_k$ 是正定的并且曲率条件 $s_k^{\\top} y_k > 0$ 成立，此更新将保持正定性。对于一个严格凸函数和精确线搜索，该条件是保证成立的，但在实践中，必须明确检查。如果 $s_k^{\\top} y_k \\le \\epsilon$（其中 $\\epsilon > 0$ 是某个小容差），则丢弃曲率对 $(s_k, y_k)$ 以保持稳定性。\n\nL-BFGS 避免存储稠密矩阵 $H_k$。相反，它存储最近的 $m$ 个曲率对 $\\{(s_i, y_i)\\}_{i=k-m}^{k-1}$。搜索方向 $p_k = -H_k g_k$ 是通过一个称为双循环递归的过程计算的。该过程将最近的 $m$ 次 BFGS 更新隐式地应用于一个初始 Hessian 矩阵近似 $H_k^0$。\n\n计算乘积 $r = H_k g_k$ 的算法如下：\n1.  初始化 $q \\leftarrow g_k$。\n2.  **第一循环（向后传递）：** 从 $i = k-1$ 向下迭代到 $k-m$。\n    -   计算 $\\rho_i = 1 / (y_i^{\\top} s_i)$。\n    -   计算 $\\alpha_i = \\rho_i s_i^{\\top} q$。\n    -   更新 $q \\leftarrow q - \\alpha_i y_i$。\n3.  **初始 Hessian 矩阵缩放：** 初始逆 Hessian 矩阵 $H_k^0$ 被选为一个简单的缩放单位矩阵，$H_k^0 = \\gamma_k I$。一个常见的缩放因子是 $\\gamma_k = \\frac{s_{k-1}^{\\top} y_{k-1}}{y_{k-1}^{\\top} y_{k-1}}$，它有助于近似真实逆 Hessian 矩阵的尺度。如果 $k=0$ 或没有可用的上一步，则使用 $\\gamma_k = 1$。应用此缩放：$r \\leftarrow \\gamma_k q$。\n4.  **第二循环（向前传递）：** 从 $i = k-m$ 向上迭代到 $k-1$。\n    -   取回 $\\rho_i = 1 / (y_i^{\\top} s_i)$。\n    -   计算 $\\beta = \\rho_i y_i^{\\top} r$。\n    -   从第一循环中取回 $\\alpha_i$。\n    -   更新 $r \\leftarrow r + s_i (\\alpha_i - \\beta)$。\n得到的向量 $r$ 是乘积 $H_k g_k$，搜索方向是 $p_k = -r$。\n\n必须有一个保障措施来确保 $p_k$ 是一个下降方向，即 $g_k^{\\top} p_k < 0$。如果 $g_k^{\\top} p_k \\ge 0$（这可能是由于数值不精确或限制内存近似的性质所致），算法必须退回到一个保证的下降方向，通常是最速下降方向 $p_k = -g_k$。\n\n一旦找到一个有效的下降方向 $p_k$，就执行线搜索以找到一个步长 $\\alpha_k > 0$，使得目标函数有足够的下降。满足 Armijo 条件的回溯线搜索是标准做法：\n$$\nf(w_k + \\alpha_k p_k) \\le f(w_k) + c_1 \\alpha_k g_k^{\\top} p_k\n$$\n对于一个常数 $c_1 \\in (0, 1)$。搜索从一个初始步长（例如，$\\alpha=1$）开始，并以一个因子 $\\tau \\in (0, 1)$ 对其进行收缩，直到满足条件为止。\n\n**3. 实现与应用**\n\n实现遵循了推导的原理。权重矩阵 $\\mathbf{W}$ 及其梯度必须被向量化，以供在一位向量上操作的 L-BFGS 求解器使用。一个用于逻辑回归模型的辅助类将为给定的向量化权重计算目标值和梯度。L-BFGS 求解器将封装双循环递归、历史记录管理、线搜索和收敛性检查。为保证数值稳定性，在进行指数运算前，通过从每个样本的得分行中减去最大得分来稳定 softmax 计算。\n\n然后，将开发的求解器应用于三个指定的测试用例，并使用提供的超参数（$m=7$, $\\text{tol}=10^{-8}$, max iterations=$200$, $c_1=10^{-4}$, $\\tau=0.5$）。初始权重被设置为零。每个案例所需的输出——最小化的目标值、最优权重的 Frobenius 范数以及对最终梯度范数的布尔检查——都按要求计算和格式化。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom collections import deque\n\nclass LogisticRegression:\n    \"\"\"\n    Encapsulates the multiclass logistic regression model, providing a function\n    to compute the objective value and its gradient.\n    \"\"\"\n    def __init__(self, X, y, K, lambda_reg):\n        \"\"\"\n        Initializes the model with data and hyperparameters.\n\n        Args:\n            X (np.ndarray): The n x d feature matrix.\n            y (np.ndarray): The n-dimensional label vector with integers 0 to K-1.\n            K (int): The number of classes.\n            lambda_reg (float): The L2 regularization coefficient.\n        \"\"\"\n        self.n, self.d = X.shape\n        self.K = K\n        self.lambda_reg = lambda_reg\n        \n        # Augment features with a bias term\n        self.X_tilde = np.c_[X, np.ones(self.n)]\n        \n        # Store original integer labels for indexing\n        self.y_indices = y\n        \n        # Create one-hot encoded label matrix\n        self.y_one_hot = np.zeros((self.n, self.K))\n        self.y_one_hot[np.arange(self.n), self.y_indices] = 1\n\n    def objective_and_grad(self, W_flat):\n        \"\"\"\n        Computes the objective function value and its gradient.\n\n        Args:\n            W_flat (np.ndarray): The flattened ((d+1)*K,) weight vector.\n\n        Returns:\n            tuple: A tuple containing:\n                - f_val (float): The objective function value.\n                - grad_flat (np.ndarray): The flattened gradient vector.\n        \"\"\"\n        W = W_flat.reshape((self.d + 1, self.K))\n\n        # (1) Compute scores\n        Z = self.X_tilde @ W\n\n        # (2) Compute probabilities using a numerically stable softmax\n        Z_max = np.max(Z, axis=1, keepdims=True)\n        exp_Z_shifted = np.exp(Z - Z_max)\n        P = exp_Z_shifted / np.sum(exp_Z_shifted, axis=1, keepdims=True)\n\n        # (3) Compute the objective function value (negative log-likelihood + L2 regularization)\n        log_probs = np.log(P[np.arange(self.n), self.y_indices])\n        log_likelihood = np.sum(log_probs)\n        frobenius_norm_sq = np.sum(W**2)\n        f_val = -log_likelihood + (self.lambda_reg / 2) * frobenius_norm_sq\n\n        # (4) Compute the gradient\n        grad_W = self.X_tilde.T @ (P - self.y_one_hot) + self.lambda_reg * W\n        grad_flat = grad_W.flatten()\n\n        return f_val, grad_flat\n\nclass LBFGSSolver:\n    \"\"\"\n    Implements the L-BFGS algorithm from first principles.\n    \"\"\"\n    def __init__(self, m, tol, max_iter, c1, tau, eps=1e-12):\n        \"\"\"\n        Initializes the L-BFGS solver with its hyperparameters.\n        \n        Args:\n            m (int): The memory size (number of curvature pairs to store).\n            tol (float): Gradient norm tolerance for convergence.\n            max_iter (int): Maximum number of iterations.\n            c1 (float): Armijo condition parameter for line search.\n            tau (float): Contraction factor for backtracking line search.\n            eps (float): Small tolerance for the curvature condition.\n        \"\"\"\n        self.m = m\n        self.tol = tol\n        self.max_iter = max_iter\n        self.c1 = c1\n        self.tau = tau\n        self.eps = eps\n\n    def solve(self, obj_grad_func, w0):\n        \"\"\"\n        Minimizes the objective function.\n\n        Args:\n            obj_grad_func (callable): A function that takes a vector w and returns (objective, gradient).\n            w0 (np.ndarray): The initial guess for the parameters.\n\n        Returns:\n            tuple: A tuple containing:\n                - w (np.ndarray): The optimized parameters.\n                - f_val (float): The objective value at the solution.\n                - g (np.ndarray): The gradient at the solution.\n        \"\"\"\n        w = w0.copy()\n        s_hist = deque(maxlen=self.m)\n        y_hist = deque(maxlen=self.m)\n        \n        f_val, g = obj_grad_func(w)\n\n        for k in range(self.max_iter):\n            if np.linalg.norm(g, ord=np.inf) < self.tol:\n                break\n\n            # 1. Compute search direction p_k using two-loop recursion\n            q = g.copy()\n            alphas = []\n            rhos = []\n\n            # First loop (backward)\n            for s, y in reversed(list(zip(s_hist, y_hist))):\n                rho = 1.0 / (y.T @ s)\n                rhos.insert(0, rho)\n                alpha = rho * (s.T @ q)\n                alphas.insert(0, alpha)\n                q -= alpha * y\n            \n            # Initial Hessian approximation scaling\n            gamma = 1.0\n            if s_hist:\n                s_last, y_last = s_hist[-1], y_hist[-1]\n                # Defensive check for y_last norm to avoid division by zero\n                y_norm_sq = y_last.T @ y_last\n                if y_norm_sq > self.eps:\n                    gamma = (s_last.T @ y_last) / y_norm_sq\n            \n            r = gamma * q\n\n            # Second loop (forward)\n            for i, (s, y) in enumerate(zip(s_hist, y_hist)):\n                beta = rhos[i] * (y.T @ r)\n                r += s * (alphas[i] - beta)\n\n            p = -r\n\n            # Safeguard: ensure p is a descent direction\n            g_dot_p = g.T @ p\n            if g_dot_p >= 0:\n                p = -g\n                g_dot_p = g.T @ p\n            \n            # 2. Backtracking line search for step size alpha_k\n            alpha = 1.0\n            w_new_temp = w + alpha * p\n            f_new_temp, _ = obj_grad_func(w_new_temp)\n            while f_new_temp > f_val + self.c1 * alpha * g_dot_p:\n                alpha *= self.tau\n                if alpha < 1e-15: # Safety break\n                    break\n                w_new_temp = w + alpha * p\n                f_new_temp, _ = obj_grad_func(w_new_temp)\n            \n            # 3. Update parameters and gradients\n            s_new = alpha * p\n            w_new = w + s_new\n            f_new, g_new = obj_grad_func(w_new)\n\n            y_new = g_new - g\n            \n            # 4. Update history if curvature condition is met\n            if s_new.T @ y_new > self.eps:\n                s_hist.append(s_new)\n                y_hist.append(y_new)\n            \n            w, f_val, g = w_new, f_new, g_new\n        \n        return w, f_val, g\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    K = 3\n    d = 2\n    lbfgs_params = {'m': 7, 'tol': 1e-8, 'max_iter': 200, 'c1': 1e-4, 'tau': 0.5}\n    solver = LBFGSSolver(**lbfgs_params)\n    w0 = np.zeros(((d + 1) * K,))\n    results = []\n\n    # --- Test Case A ---\n    XA = np.array([\n        [0.0, 0.0], [0.2, -0.1], [-0.1, 0.1], [0.1, 0.2], [-0.2, 0.0],\n        [3.0, 3.0], [2.9, 3.1], [3.1, 2.8], [3.2, 3.2], [2.8, 2.9],\n        [-3.0, 3.0], [-3.1, 3.2], [-2.9, 2.8], [-3.2, 2.9], [-2.8, 3.1]\n    ])\n    yA = np.array([0]*5 + [1]*5 + [2]*5)\n    lambda_A = 0.1\n    \n    model_A = LogisticRegression(XA, yA, K, lambda_A)\n    _, f_star_A, _ = solver.solve(model_A.objective_and_grad, w0)\n    results.append(f\"{f_star_A:.6f}\")\n\n    # --- Test Case B ---\n    yB = yA\n    lambda_B = 100.0\n    model_B = LogisticRegression(XA, yB, K, lambda_B)\n    w_star_B_flat, _, _ = solver.solve(model_B.objective_and_grad, w0)\n    W_star_B = w_star_B_flat.reshape((d + 1, K))\n    norm_W_star_B = np.linalg.norm(W_star_B, 'fro')\n    results.append(f\"{norm_W_star_B:.6f}\")\n\n    # --- Test Case C ---\n    XC = np.array([\n        [-2.0, -6.0], [-1.0, -3.0], [-1.5, -4.5],\n        [-0.2, -0.6], [0.0, 0.0], [0.5, 1.5],\n        [1.5, 4.5], [2.0, 6.0], [2.2, 6.6]\n    ])\n    yC = np.array([0]*3 + [1]*3 + [2]*3)\n    lambda_C = 0.01\n    \n    model_C = LogisticRegression(XC, yC, K, lambda_C)\n    _, _, g_star_C = solver.solve(model_C.objective_and_grad, w0)\n    grad_norm_inf = np.linalg.norm(g_star_C, ord=np.inf)\n    result_C = bool(grad_norm_inf <= 1e-6)\n    results.append(str(result_C))\n\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "2417391"}]}