{"hands_on_practices": [{"introduction": "在我们深入研究先进的缩减技术之前，首先必须理解标准方法为何以及在何种情况下会失效。这个练习将带您探讨一个关键的挑战：当一个特征值对应的特征向量不足以张成其整个不变子空间时（即矩阵是亏损的），会发生什么。通过分析一个具体的亏损矩阵，您将亲手揭示为何像 Hotelling 缩减这样的标准技术无法完全“移除”一个重根特征值，从而为我们后续学习更稳健的方法奠定基础。[@problem_id:2383519]", "problem": "考虑 $4 \\times 4$ 矩阵\n$$\nA \\;=\\;\n\\begin{pmatrix}\n1 & 1 & 0 & 0 \\\\\n0 & 1 & 1 & 0 \\\\\n0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 2\n\\end{pmatrix}.\n$$\n特征值 $\\lambda=1$ 是 $A$ 的一个重特征值。设 $v$ 是 $A$ 的与 $\\lambda=1$ 相关联的单位右特征向量。定义标准的 Hotelling 降阶（秩一降阶）为\n$$\nA_{\\mathrm{def}} \\;=\\; A \\;-\\; \\lambda\\, v v^{\\top}.\n$$\n仅使用特征值、特征向量、广义特征空间和行列式的定义，完成以下任务：\n\n- 验证 $\\lambda=1$ 是 $A$ 的一个代数重数大于 $1$ 的特征值，并确定一个对应于 $\\lambda=1$ 的单位右特征向量 $v$。\n- 构建 $A_{\\mathrm{def}}$，并根据 $A$ 的结构和广义特征向量的定义，解释为什么这种标准降阶方法无法揭示对应于 $\\lambda=1$ 的广义特征空间的完整基。\n- 计算 $\\det\\!\\big(A_{\\mathrm{def}} - \\lambda I\\big)$。\n\n答案规格：\n- 请以单个实数形式提供最终答案。\n- 无需四舍五入。", "solution": "首先根据要求验证题目陈述。\n\n**第 1 步：提取已知条件**\n- 矩阵 $A$：\n$$\nA = \\begin{pmatrix}\n1 & 1 & 0 & 0 \\\\\n0 & 1 & 1 & 0 \\\\\n0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 2\n\\end{pmatrix}\n$$\n- 一个特征值 $\\lambda = 1$。\n- Hotelling 降阶的定义：$A_{\\mathrm{def}} = A - \\lambda v v^{\\top}$，其中 $v$ 是 $A$ 的与 $\\lambda$ 相关联的单位右特征向量。\n- 任务是：\n    1. 验证 $\\lambda=1$ 是 $A$ 的一个重特征值，并找到一个对应的单位右特征向量 $v$。\n    2. 构建 $A_{\\mathrm{def}}$ 并解释为什么标准降阶法无法揭示对应于 $\\lambda=1$ 的广义特征空间的完整基。\n    3. 计算当 $\\lambda=1$ 时 $\\det(A_{\\mathrm{def}} - \\lambda I)$ 的值。\n\n**第 2 步：使用提取的已知条件进行验证**\n- **科学基础：** 该问题使用了线性代数和数值分析中标准的、公认的概念，即特征值、特征向量、广义特征空间、行列式和矩阵降阶技术。这些是计算工程中的基本主题。\n- **适定性：** 该问题提供了一个特定的矩阵和清晰的定义。任务明确，可以得出一个唯一的、可验证的解。\n- **客观性：** 该问题使用精确的数学语言陈述，不含主观或推测性内容。\n\n**第 3 步：结论和行动**\n该问题有效。它是数值线性代数中一个标准的、适定的问题。我现在将着手解决。\n\n**第 1 部分：特征值重数与特征向量**\n\n为确定 $A$ 的特征值，我们计算特征多项式 $p(\\lambda) = \\det(A - \\lambda I)$，其中 $I$ 是 $4 \\times 4$ 的单位矩阵。\n$$\nA - \\lambda I = \\begin{pmatrix}\n1-\\lambda & 1 & 0 & 0 \\\\\n0 & 1-\\lambda & 1 & 0 \\\\\n0 & 0 & 1-\\lambda & 0 \\\\\n0 & 0 & 0 & 2-\\lambda\n\\end{pmatrix}\n$$\n由于这是一个上三角矩阵，其行列式是对角元素的乘积：\n$$\n\\det(A - \\lambda I) = (1-\\lambda)(1-\\lambda)(1-\\lambda)(2-\\lambda) = (1-\\lambda)^{3}(2-\\lambda)\n$$\n特征方程 $\\det(A - \\lambda I) = 0$ 的根即为特征值。特征值为 $\\lambda_1 = 1$（代数重数 $m_a(\\lambda_1) = 3$）和 $\\lambda_2 = 2$（代数重数 $m_a(\\lambda_2) = 1$）。这验证了 $\\lambda=1$ 是一个重数大于 $1$ 的特征值。\n\n接下来，我们通过求解方程组 $(A - 1 \\cdot I)x = 0$ 来找到对应于 $\\lambda=1$ 的右特征向量 $x$。\n$$\n(A - I)x = \\begin{pmatrix}\n0 & 1 & 0 & 0 \\\\\n0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 1\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\\nx_2 \\\\\nx_3 \\\\\nx_4\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n0 \\\\\n0 \\\\\n0 \\\\\n0\n\\end{pmatrix}\n$$\n该矩阵向量方程可得到以下线性方程组：\n$x_2 = 0$\n$x_3 = 0$\n$x_4 = 0$\n变量 $x_1$ 是自由变量。因此，任何对应于 $\\lambda=1$ 的特征向量都具有 $c \\begin{pmatrix} 1 & 0 & 0 & 0 \\end{pmatrix}^{\\top}$ 的形式，其中 $c$ 为任意非零标量。特征空间是一维的，因此几何重数为 $m_g(1) = 1$。题目要求一个单位特征向量 $v$。我们选择特征空间的基向量并将其单位化。\n设 $x = \\begin{pmatrix} 1 & 0 & 0 & 0 \\end{pmatrix}^{\\top}$。其欧几里德范数为 $\\|x\\|_2 = \\sqrt{1^2 + 0^2 + 0^2 + 0^2} = 1$。\n因此，一个单位右特征向量是 $v = \\begin{pmatrix} 1 & 0 & 0 & 0 \\end{pmatrix}^{\\top}$。\n\n**第 2 部分：降阶矩阵与失败原因解释**\n\n我们使用 $\\lambda=1$ 和 $v = \\begin{pmatrix} 1 & 0 & 0 & 0 \\end{pmatrix}^{\\top}$ 构建降阶矩阵 $A_{\\mathrm{def}}$。\n首先，我们计算秩一矩阵 $v v^{\\top}$：\n$$\nv v^{\\top} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix} \\begin{pmatrix} 1 & 0 & 0 & 0 \\end{pmatrix} = \\begin{pmatrix}\n1 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 0\n\\end{pmatrix}\n$$\n现在，我们计算 $A_{\\mathrm{def}} = A - \\lambda v v^{\\top} = A - 1 \\cdot v v^{\\top}$：\n$$\nA_{\\mathrm{def}} = \\begin{pmatrix}\n1 & 1 & 0 & 0 \\\\\n0 & 1 & 1 & 0 \\\\\n0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 2\n\\end{pmatrix} - \\begin{pmatrix}\n1 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 0\n\\end{pmatrix} = \\begin{pmatrix}\n0 & 1 & 0 & 0 \\\\\n0 & 1 & 1 & 0 \\\\\n0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 2\n\\end{pmatrix}\n$$\n这种降阶方法失败的原因在于矩阵 $A$ 对于特征值 $\\lambda=1$ 是亏损的。如果一个矩阵某个特征值的代数重数大于其几何重数，则该矩阵对于此特征值是亏损的。在此例中，$m_a(1)=3$ 而 $m_g(1)=1$。\n\n这种亏损性意味着存在一个维度为 $3$ 的广义特征空间，它不仅由单个特征向量 $v$ 张成，还由两个广义特征向量张成。一个广义特征向量 $x_k$ 是一个由 $(A - \\lambda I)x_k = x_{k-1}$ 定义的链中的向量，其中 $x_1$ 是真特征向量。当 $k > 1$ 时，空间 $\\ker((A-\\lambda I)^k)$ 包含广义特征向量。\n\n标准的 Hotelling 降阶方法 $A_{\\mathrm{def}} = A - \\lambda v v^{\\top}$ 旨在将与特征向量 $v$ 对应的特征值 $\\lambda$ 移动到 $0$，使得 $A_{\\mathrm{def}}v = (A - \\lambda v v^{\\top})v = Av - \\lambda v(v^{\\top}v) = \\lambda v - \\lambda v(1) = 0$。然而，这种秩一修改仅利用了由 $v$ 张成的一维特征空间的信息。它没有考虑到广义特征空间（它是 $A$ 的一个不变子空间）的更高维结构。该降阶未能正确转换与 $\\lambda=1$ 相关联的整个 Jordan 块。因此，Jordan 链中其他向量的影响没有从矩阵中移除，降阶后的矩阵 $A_{\\mathrm{def}}$ 保留了与 $\\lambda=1$ 相关联的原始特征系统的分量。降阶的目标是找到其他特征值的特征向量，但在这种亏损情况下，对 $A_{\\mathrm{def}}$ 进行后续的幂迭代会收敛到 $\\lambda=1$ 的同一个广义特征空间内的另一个向量。\n\n为了明确地看到这一点，我们来求 $A_{\\mathrm{def}}$ 的特征值。其特征多项式为 $\\det(A_{\\mathrm{def}} - \\mu I)$：\n$$\n\\det\\begin{pmatrix}\n-\\mu & 1 & 0 & 0 \\\\\n0 & 1-\\mu & 1 & 0 \\\\\n0 & 0 & 1-\\mu & 0 \\\\\n0 & 0 & 0 & 2-\\mu\n\\end{pmatrix} = (-\\mu)(1-\\mu)^2(2-\\mu)\n$$\n$A_{\\mathrm{def}}$ 的特征值为 $\\{0, 1, 1, 2\\}$。$A$ 的原始特征值为 $\\{1, 1, 1, 2\\}$。降阶仅将特征值 $1$ 的一个实例移动到了 $0$，而另外两个实例仍然是 $1$。一次成功的降阶会简化特征问题，但在这里它未能消除 $\\lambda=1$ 的重数，而这正是首先要进行降阶的主要原因。\n\n**第 3 部分：最终计算**\n\n题目要求计算当 $\\lambda=1$ 时 $\\det(A_{\\mathrm{def}} - \\lambda I)$ 的值。这等价于求 $A_{\\mathrm{def}}$ 的特征多项式在 $\\mu=1$ 处的值。根据上面的分析，我们知道 $1$ 是 $A_{\\mathrm{def}}$ 的一个特征值。根据定义，如果 $\\mu$ 是矩阵 $M$ 的一个特征值，那么 $\\det(M - \\mu I) = 0$。\n\n为完整起见，我们直接计算它。\n$$\nA_{\\mathrm{def}} - 1 \\cdot I = \\begin{pmatrix}\n0 & 1 & 0 & 0 \\\\\n0 & 1 & 1 & 0 \\\\\n0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 2\n\\end{pmatrix} - \\begin{pmatrix}\n1 & 0 & 0 & 0 \\\\\n0 & 1 & 0 & 0 \\\\\n0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 1\n\\end{pmatrix} = \\begin{pmatrix}\n-1 & 1 & 0 & 0 \\\\\n0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 1\n\\end{pmatrix}\n$$\n所得矩阵有一行完全由零组成（第三行）。任何具有零行或零列的矩阵的行列式均为零。\n因此，$\\det(A_{\\mathrm{def}} - I) = 0$。", "answer": "$$\\boxed{0}$$", "id": "2383519"}, {"introduction": "处理重根特征值不仅仅是理论上的挑战，更是一个数值计算上的难题。即使矩阵是可对角化的，当特征值非常接近时（即“聚集”在一起），标准缩减方法中的浮点误差也会被急剧放大。本练习提供了一个理论分析的机会，旨在对比两种经典的缩减方法——Hotelling 缩减和 Wielandt 缩减——在处理由机器精度隔开的两个特征值时的数值稳定性。通过这个受控的思想实验，您将推导出哪种方法在数值上更为优越，并理解其背后的原因。[@problem_id:2383542]", "problem": "考虑一个实对称矩阵 $A \\in \\mathbb{R}^{2 \\times 2}$，其具有两个由机器精度分隔的簇集特征值。在全文中，我们假设标量单次运算（加法、减法或乘法）遵循标准的浮点（FP）舍入模型：如果 $z$ 表示对浮点输入进行运算的精确结果，则计算出的结果为 $\\operatorname{fl}(z) = z (1 + \\delta)$，其中 $|\\delta| \\leq \\varepsilon_{m}$，而 $\\varepsilon_{m}$ 是单位舍入（通常称为机器ε）。假设输入在浮点表示中可以精确表示，并且矩阵范数为谱范数。\n\n设 $A$ 在精确算术下由下式给出\n$$\nA \\;=\\; Q \\begin{pmatrix} \\lambda & 0 \\\\ 0 & \\lambda + \\delta \\end{pmatrix} Q^{\\top},\n$$\n其中 $Q$ 是正交矩阵，$\\lambda > 0$，且 $0 < \\delta \\ll \\lambda$。假设第一个特征对 $(\\lambda, v)$ 是精确已知的，其中 $v$ 是 $Q$ 的第一列。您将比较两种降阶策略，在特征值间距等于机器精度（即 $\\delta = \\varepsilon_{m}$）时，以有限精度估算第二个特征值 $\\lambda + \\delta$。\n\nHotelling 降阶法构建降阶矩阵\n$$\nA_{H} \\;=\\; A \\;-\\; \\lambda \\, v v^{\\top},\n$$\n然后将 $A_{H}$ 的非零特征值作为剩余特征值的估计。Wielandt 降阶法使用正交投影算子\n$$\nP \\;=\\; I \\;-\\; v v^{\\top},\n$$\n并且仅在 $v$ 的正交补上操作，将 $P A P$ 限制在 $\\mathrm{span}\\{v\\}^{\\perp}$ 上来估计剩余的特征值。在精确算术中，这两种方法都能恢复出相同的第二个特征值 $\\lambda + \\delta$。\n\n然而，在浮点算术中，这两种途径在消除大的 $O(\\lambda)$ 贡献时会产生不同的后向误差。按如下方式对每种途径建模。\n\n- Hotelling方法：显式形成的浮点降阶矩阵为\n$$\n\\widehat{A}_{H} \\;=\\; \\operatorname{fl}\\!\\left(A - \\lambda v v^{\\top}\\right) \\;=\\; \\left(A - \\lambda v v^{\\top}\\right) \\;+\\; E_{H},\n$$\n带有一个满足 $\\|E_{H}\\|_{2} \\approx c_{H}\\,\\varepsilon_{m}\\,\\lambda$ 的加性扰动 $E_{H}$，其中 $c_{H}$ 是一个与 $\\lambda$ 和 $\\delta$ 无关的不大的常数。\n\n- Wielandt方法：$P A P$ 在 $\\mathrm{span}\\{v\\}^{\\perp}$ 上的作用是通过计算算子实现，而不显式构造 $P A P$；也就是说，向量 $x \\in \\mathrm{span}\\{v\\}^{\\perp}$ 通过计算算子被映射\n$$\n\\widehat{B}\\,x \\;=\\; \\operatorname{fl}\\!\\big( P\\,\\operatorname{fl}(A\\,\\operatorname{fl}(P x)) \\big),\n$$\n该算子在精确算术中等于 $B x := (P A P)x$，并且在 $\\mathrm{span}\\{v\\}^{\\perp}$ 上简化为 $B x = (\\lambda + \\delta)x$。在浮点模型下，并使用 $x \\in \\mathrm{span}\\{v\\}^{\\perp}$（因此 $P x = x$ 精确成立），证明在构造 $A x$ 时，$\\mathrm{span}\\{v\\}^{\\perp}$ 上的导出算子扰动满足 $\\|\\widehat{B} - B\\|_{2} \\approx c_{W}\\,\\varepsilon_{m}\\,(\\lambda + \\delta)$，但随后到正交补的限制使得谱估计中的 $O(\\lambda)$ 部分被抵消，留下一个由 $\\delta$ 缩放部分控制的等效特征值扰动，其大小约为 $\\approx c_{W}\\,\\varepsilon_{m}\\,\\delta$，其中 $c_{W}$ 是一个不大的常数。\n\n进一步假设对称矩阵的一阶特征值扰动理论适用：如果 $\\mu$ 是对称矩阵 $M$ 的一个单特征值，对应单位特征向量为 $w$，那么在一个小的对称扰动 $E$ 下，一阶特征值误差为 $w^{\\top} E w$，因此其大小的界为 $\\|E\\|_{2}$。\n\n使用此模型，计算比率\n$$\nR(\\varepsilon_{m}) \\;=\\; \\frac{\\text{Hotelling 估计 } \\lambda + \\delta \\text{ 的绝对误差}}{\\text{Wielandt 估计 } \\lambda + \\delta \\text{ 的绝对误差}}\n$$\n在归一化情况 $\\|A\\|_{2} = \\lambda + \\delta = 1$ 且 $\\delta = \\varepsilon_{m}$ 下。您的最终答案必须是仅含 $\\varepsilon_{m}$ 的单个闭式表达式。不需要舍入，也不需要单位。", "solution": "所述问题是有效的。它在科学上基于数值线性代数的原理，特别是特征值算法中浮点误差的分析。该问题是适定的，提供了所有必要的模型和数据，并以客观、明确的语言表述。我将继续进行解答。\n\n目标是在指定的有限精度模型下，计算使用 Hotelling 降阶法和 Wielandt 降阶法估计特征值 $\\lambda+\\delta$ 时绝对误差的比率 $R(\\varepsilon_{m})$。\n\n首先，我们分析 Hotelling 降阶法的误差。该方法涉及显式构造矩阵 $\\widehat{A}_{H} = \\operatorname{fl}(A - \\lambda v v^{\\top})$。问题将其建模为 $\\widehat{A}_{H} = A_{H} + E_{H}$，其中 $A_{H} = A - \\lambda v v^{\\top}$ 是精确的降阶矩阵，而 $E_{H}$ 是由浮点运​​算引起的扰动。对称矩阵 $A_H$ 的特征值为 $0$ 和 $\\lambda + \\delta$。我们旨在估计 $\\lambda + \\delta$。\n\n根据对称矩阵的一阶特征值扰动理论，特征值的误差受扰动矩阵的谱范数限制。问题给出了此扰动的大小为 $\\|E_{H}\\|_{2} \\approx c_{H}\\,\\varepsilon_{m}\\,\\lambda$。因此，Hotelling 估计的绝对误差，我们记为 $\\Delta\\lambda_{H}$，具有以下数量级：\n$$\n\\Delta\\lambda_{H} \\approx c_{H}\\,\\varepsilon_{m}\\,\\lambda\n$$\n该误差与较大的特征值 $\\lambda$ 成正比，这是涉及大量值相减抵消的方法的一个典型结果。\n\n接下来，我们分析 Wielandt 降阶法的误差。该方法通过有效应用算子 $B = PAP$ 在子空间 $\\mathrm{span}\\{v\\}^{\\perp}$ 上操作，其中 $P = I - vv^{\\top}$。对于任何向量 $x \\in \\mathrm{span}\\{v\\}^{\\perp}$，我们有 $Px=x$，因此 $Bx = PAx = P((\\lambda+\\delta)x) = (\\lambda+\\delta)x$。该子空间上的精确算子是按期望的特征值 $\\lambda+\\delta$ 进行的简单缩放。\n\n问题为此过程中的误差提供了一个精细的模型。它指出，虽然计算的作用 $\\widehat{B}x$ 会产生一个与 $\\lambda+\\delta$ 成正比的算子扰动 $\\|\\widehat{B} - B\\|_{2}$，但“等效特征值扰动”要小得多。这是因为投影 $P$ 消除了与大特征值 $\\lambda$ 相关的主要误差部分。因此，Wielandt 估计中的绝对误差 $\\Delta\\lambda_{W}$ 被给出为具有以下数量级：\n$$\n\\Delta\\lambda_{W} \\approx c_{W}\\,\\varepsilon_{m}\\,\\delta\n$$\n此误差与小的特征值间距 $\\delta$ 成正比，表明其具有优越的数值稳定性。\n\n我们现在可以构建这两个绝对误差的比率 $R(\\varepsilon_{m})$：\n$$\nR(\\varepsilon_{m}) = \\frac{\\Delta\\lambda_{H}}{\\Delta\\lambda_{W}} \\approx \\frac{c_{H}\\,\\varepsilon_{m}\\,\\lambda}{c_{W}\\,\\varepsilon_{m}\\,\\delta} = \\frac{c_{H}}{c_{W}} \\frac{\\lambda}{\\delta}\n$$\n因子 $c_{H}$ 和 $c_{W}$ 被描述为依赖于浮点实现细节的“不大的常数”。在算法稳定性的高层比较中，通常假设这些常数具有相同的数量级，因此我们取近似值 $c_{H}/c_{W} \\approx 1$。这是分离两种方法在误差传播中结构性差异的合理步骤。该比率随后简化为：\n$$\nR(\\varepsilon_{m}) \\approx \\frac{\\lambda}{\\delta}\n$$\n问题要求在特定情况下计算此比率。条件是：\n1. $A$ 的谱范数被归一化：$\\|A\\|_{2} = \\lambda + \\delta = 1$。\n2. 特征值间距等于机器ε：$\\delta = \\varepsilon_{m}$。\n\n从第一个条件，我们用 $\\delta$ 表示 $\\lambda$：\n$$\n\\lambda = 1 - \\delta\n$$\n将第二个条件 $\\delta = \\varepsilon_{m}$ 代入此表达式，得到：\n$$\n\\lambda = 1 - \\varepsilon_{m}\n$$\n最后，我们将 $\\lambda$ 和 $\\delta$ 的这些表达式代入误差比率 $R(\\varepsilon_{m})$ 的公式中：\n$$\nR(\\varepsilon_{m}) \\approx \\frac{1 - \\varepsilon_{m}}{\\varepsilon_{m}}\n$$\n这一结果表明，在寻找与一个大特征值簇集的小特征值时，Wielandt 降阶法显著优于 Hotelling 方法。Hotelling 方法的误差大约大了一个因子 $1/\\varepsilon_{m}$。", "answer": "$$\n\\boxed{\\frac{1 - \\varepsilon_{m}}{\\varepsilon_{m}}}\n$$", "id": "2383542"}, {"introduction": "在分析了经典“硬”缩减方法的局限性之后，我们现在转向一种更现代且在实践中通常更稳健的策略：“软”缩减。这种方法不是直接从矩阵中移除一个已知的特征对，而是通过在瑞利商 (Rayleigh quotient) 中加入一个惩罚项，将寻找下一个特征向量的问题转化为一个等价的、经过修改的特征值问题。这个编程实践将指导您实现这种惩罚方法，通过一系列精心设计的测试，您将探索其在不同情况下的表现，包括处理近简并特征值和不精确初始特征向量的场景。[@problem_id:2383531]", "problem": "给定一个实对称矩阵 $A \\in \\mathbb{R}^{n \\times n}$ 和一个单位向量 $v_1 \\in \\mathbb{R}^n$，该向量是与 $A$ 的最小特征值 $\\lambda_1$ 相关联的特征向量的一个近似。考虑带罚项的瑞利商目标函数\n$$\nq(x) \\;=\\; \\frac{x^{\\mathsf{T}} A x}{x^{\\mathsf{T}} x} \\;+\\; \\gamma \\, (v_1^{\\mathsf{T}} x)^2,\n$$\n其中罚项参数 $\\gamma > 0$，以及约束最小化问题 $\\min_{\\|x\\|_2 = 1} q(x)$。请仅从对称矩阵的特征对和瑞利商的基本定义出发，推导如何通过将此问题转化为一个等价的特征值问题来计算求解该问题的向量 $x_\\star$，然后解释如何将 $x_\\star$ 转换为 $A$ 在 $\\lambda_1$ 之后的“下一个”特征值的数值估计 $\\widehat{\\lambda}_2$（也就是说，在与 $v_1$ 近似正交的向量中，$A$ 的最小特征值）。你的程序必须实现这个转换，并将 $\\widehat{\\lambda}_2$ 计算为 $A$ 在 $x_\\star$ 处的瑞利商。\n\n你的程序必须以数值可靠的方式实现以下算法任务：\n- 构造秩-1更新 $B \\;=\\; A \\;+\\; \\gamma \\, v_1 v_1^{\\mathsf{T}}$。\n- 计算与 $B$ 的最小特征值对应的单位范数特征向量 $x_\\star$。\n- 返回估计值 $\\widehat{\\lambda}_2 \\;=\\; x_\\star^{\\mathsf{T}} A x_\\star$。\n\n使用以下测试套件。对于每种情况，按规定计算并报告所需的输出。\n\n- 测试 1 (顺利情况，谱离散):\n  - $A_1 \\,=\\, \\mathrm{diag}(1,\\,2,\\,4) \\in \\mathbb{R}^{3 \\times 3}$,\n  - $v_{1,1} \\,=\\, [1,\\,0,\\,0]^{\\mathsf{T}}$,\n  - $\\gamma_1 \\,=\\, 3$.\n  - 输出：一个等于 $\\widehat{\\lambda}_2$ 的浮点数。\n\n- 测试 2 (前两个特征值近简并):\n  - $A_2 \\,=\\, \\mathrm{diag}(1.0,\\,1.001,\\,5.0) \\in \\mathbb{R}^{3 \\times 3}$,\n  - $v_{1,2} \\,=\\, [1,\\,0,\\,0]^{\\mathsf{T}}$,\n  - $\\gamma_2 \\,=\\, 0.01$.\n  - 输出：一个等于 $\\widehat{\\lambda}_2$ 的浮点数。\n\n- 测试 3 (最小特征值具有精确重数):\n  - $A_3 \\,=\\, \\mathrm{diag}(2.0,\\,2.0,\\,7.0) \\in \\mathbb{R}^{3 \\times 3}$,\n  - $v_{1,3} \\,=\\, [1,\\,0,\\,0]^{\\mathsf{T}}$,\n  - $\\gamma_3 \\,=\\, 5$.\n  - 输出：一个等于 $\\widehat{\\lambda}_2$ 的浮点数。\n\n- 测试 4 (不精确的降维方向):\n  - $A_4 \\,=\\, \\mathrm{diag}(1.0,\\,2.0,\\,3.0) \\in \\mathbb{R}^{3 \\times 3}$,\n  - 令角度为 $\\theta \\,=\\, 5$ 度（角度必须以度为单位进行解释），\n  - $v_{1,4} \\,=\\, [\\cos(\\theta),\\,\\sin(\\theta),\\,0]^{\\mathsf{T}}$ 归一化为单位长度，\n  - $\\gamma_4 \\,=\\, 10$.\n  - 输出：一个等于 $\\widehat{\\lambda}_2$ 的浮点数。\n\n- 测试 5 (罚项过小导致的失败情况):\n  - $A_5 \\,=\\, \\mathrm{diag}(1.0,\\,1.001,\\,3.0) \\in \\mathbb{R}^{3 \\times 3}$,\n  - $v_{1,5} \\,=\\, [1,\\,0,\\,0]^{\\mathsf{T}}$,\n  - $\\gamma_5 \\,=\\, 0.0001$.\n  - 输出：一个按如下方式定义的布尔值。设 $\\widehat{\\lambda}_2$ 按上述方法计算，并设 $\\lambda_1 \\,=\\, 1.0$ 和 $\\lambda_2 \\,=\\, 1.001$。输出表达式 $\\big|\\widehat{\\lambda}_2 - \\lambda_2\\big| < \\big|\\widehat{\\lambda}_2 - \\lambda_1\\big|$ 的布尔值。\n\n重要实现细节：\n- 所有矩阵都是实对称的。所有向量在需要时都必须归一化为单位长度。\n- 你不得依赖任何随机性；应使用确定性的线性代数例程。\n- 凡涉及角度，必须按规定以度为单位进行解释。\n- 本问题不涉及物理单位。\n\n最终输出格式：\n- 你的程序应生成一行输出，其中包含测试 1 到 5 的结果，形式为一个用方括号括起来的逗号分隔列表，例如 $[\\text{result}_1,\\text{result}_2,\\text{result}_3,\\text{result}_4,\\text{result}_5]$。对于测试 1-4，每个 $\\text{result}_k$ 是一个浮点数。对于测试 5，$\\text{result}_5$ 是一个布尔值。不得打印任何额外文本。", "solution": "所提出的问题是数值线性代数中的一个有效练习，具体涉及特征值问题的降维技术。我们将进行形式化推导。\n\n既定目标是解决约束最小化问题：\n$$\n\\min_{x \\in \\mathbb{R}^n, \\|x\\|_2 = 1} q(x)\n$$\n其中目标函数是带罚项的瑞利商：\n$$\nq(x) = \\frac{x^{\\mathsf{T}} A x}{x^{\\mathsf{T}} x} + \\gamma (v_1^{\\mathsf{T}} x)^2\n$$\n此处，$A \\in \\mathbb{R}^{n \\times n}$ 是一个实对称矩阵，$v_1 \\in \\mathbb{R}^n$ 是一个给定的单位向量，用以近似 $A$ 的最小特征值的特征向量，而 $\\gamma > 0$ 是一个标量罚项参数。\n\n首先，我们将问题形式化。约束 $\\|x\\|_2 = 1$ 意味着 $x^{\\mathsf{T}} x = 1$。将此代入目标函数，问题简化为在单位球面上最小化以下函数：\n$$\nf(x) = x^{\\mathsf{T}} A x + \\gamma (v_1^{\\mathsf{T}} x)^2 \\quad \\text{约束条件为} \\quad x^{\\mathsf{T}} x = 1\n$$\n罚项 $\\gamma (v_1^{\\mathsf{T}} x)^2$ 可以重写为二次型。由于 $v_1^{\\mathsf{T}} x$ 是一个标量，其平方为 $(v_1^{\\mathsf{T}} x)(v_1^{\\mathsf{T}} x) = (x^{\\mathsf{T}} v_1)(v_1^{\\mathsf{T}} x)$。根据矩阵乘法的结合律，该表达式等价于 $x^{\\mathsf{T}} (v_1 v_1^{\\mathsf{T}}) x$。项 $v_1 v_1^{\\mathsf{T}}$ 是向量 $v_1$ 与自身的外积，它是一个对称的秩一矩阵。\n\n将此代回 $f(x)$ 的表达式中，我们可以合并这两个二次型：\n$$\nf(x) = x^{\\mathsf{T}} A x + \\gamma x^{\\mathsf{T}} (v_1 v_1^{\\mathsf{T}}) x = x^{\\mathsf{T}} (A + \\gamma v_1 v_1^{\\mathsf{T}}) x\n$$\n让我们定义一个新矩阵 $B = A + \\gamma v_1 v_1^{\\mathsf{T}}$。由于 $A$ 是对称的且 $v_1 v_1^{\\mathsf{T}}$ 也是对称的，它们的和 $B$ 也是一个实对称矩阵。最小化问题现在被转换为：\n$$\n\\min_{x^{\\mathsf{T}} x = 1} x^{\\mathsf{T}} B x\n$$\n这是寻找矩阵 $B$ 的瑞利商最小值的标准问题。根据瑞利-里兹定理，对于任何对称矩阵 $B$，商 $\\frac{x^{\\mathsf{T}} B x}{x^{\\mathsf{T}} x}$ 的最小值是 $B$ 的最小特征值，记作 $\\mu_{\\min}(B)$。这个最小值当且仅当 $x$ 是一个对应的特征向量时达到。因此，解决我们原始最小化问题的向量 $x_\\star$ 是与 $B$ 的最小特征值相关联的单位范数特征向量。\n\n现在，我们必须理解这个过程如何与寻找 $A$ 的“下一个”特征值相关。设 $A$ 的特征对为 $(\\lambda_i, u_i)$，$i=1, \\dots, n$，并排序使得 $\\lambda_1 \\le \\lambda_2 \\le \\dots \\le \\lambda_n$。向量 $v_1$ 被作为 $u_1$ 的一个近似。\n\n考虑近似是精确的理想情况，即 $v_1 = u_1$。矩阵为 $B = A + \\gamma u_1 u_1^{\\mathsf{T}}$。让我们检验 $B$ 对 $A$ 的特征向量的作用：\n1. 对于 $A$ 的任何满足 $i > 1$ 的特征向量 $u_i$，我们假设它与 $u_1$ 正交（对于对称矩阵，即使有重复特征值，这也是总能做到的）。那么 $u_1^{\\mathsf{T}} u_i = 0$。将 $B$ 应用于 $u_i$ 得到：\n$$\nB u_i = A u_i + \\gamma u_1 (u_1^{\\mathsf{T}} u_i) = \\lambda_i u_i + \\gamma u_1 (0) = \\lambda_i u_i\n$$\n这表明对于 $i=2, \\dots, n$，$(\\lambda_i, u_i)$ 也是 $B$ 的特征对。\n\n2. 对于特征向量 $u_1$，因为它是单位向量，我们有 $u_1^{\\mathsf{T}} u_1 = 1$。将 $B$ 应用于 $u_1$ 得到：\n$$\nB u_1 = A u_1 + \\gamma u_1 (u_1^{\\mathsf{T}} u_1) = \\lambda_1 u_1 + \\gamma u_1(1) = (\\lambda_1 + \\gamma) u_1\n$$\n所以，$(\\lambda_1 + \\gamma, u_1)$ 是 $B$ 的一个特征对。\n\n因此，$B$ 的谱为 $\\{\\lambda_1 + \\gamma, \\lambda_2, \\lambda_3, \\dots, \\lambda_n\\}$。罚项的目的是将与已知特征向量方向 $u_1$ 对应的特征值“向上移动”，从而将下一个特征值 $\\lambda_2$ 暴露为最小值。为了实现这一点，$B$ 的最小特征值必须是 $\\lambda_2$。这要求 $\\lambda_2 < \\lambda_1 + \\gamma$。重新整理可得，罚项参数 $\\gamma$ 必须选择为满足 $\\gamma > \\lambda_2 - \\lambda_1$。如果这个条件成立，$B$ 的最小特征值就是 $\\lambda_2$，其对应的特征向量是 $u_2$。\n\n在一般情况下，$v_1$ 只是 $u_1$ 的一个近似，此时 $A$ 的（除 $u_1$ 外的）特征向量并不完全是 $B$ 的特征向量。然而，对于足够大的 $\\gamma$，$B$ 与方向 $v_1$ 相关联的特征值得到了显著增大。那么，与 $B$ 的最小特征值对应的特征向量 $x_\\star$ 将是 $u_2$ 的一个近似，而 $u_2$ 是 $A$ 的第二小特征值 $\\lambda_2$ 的特征向量。此近似的质量取决于 $v_1$ 的准确性以及特征值之间的间隙。\n\n最后，为了获得特征值 $\\lambda_2$ 的数值估计 $\\widehat{\\lambda}_2$，我们使用已找到的向量 $x_\\star$。给定一个近似特征向量 $x_\\star$ 时，对 $A$ 的一个特征值的最佳估计是其关于 $A$ 的瑞利商。由于 $x_\\star$ 是一个单位向量，这可以简化为：\n$$\n\\widehat{\\lambda}_2 = x_\\star^{\\mathsf{T}} A x_\\star\n$$\n推导至此完成。该算法与问题中所述完全一致：构造 $B = A + \\gamma v_1 v_1^{\\mathsf{T}}$，找到与 $B$ 的最小特征值对应的特征向量 $x_\\star$，并计算瑞利商 $\\widehat{\\lambda}_2 = x_\\star^{\\mathsf{T}} A x_\\star$ 作为所需的估计值。测试 5 展示了当 $\\gamma$ 过小以至于不满足条件 $\\gamma > \\lambda_2 - \\lambda_1$ 时该方法的失败情况，此时最小化向量 $x_\\star$ 仍与 $v_1$ 的方向对齐，而不是转去近似 $u_2$。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the series of eigenvalue deflation problems.\n    \"\"\"\n\n    test_cases = [\n        # Test 1: Happy path\n        {\n            'A': np.diag([1.0, 2.0, 4.0]),\n            'v1_unnormalized': np.array([1.0, 0.0, 0.0]),\n            'gamma': 3.0,\n            'task': 'compute_lambda_hat_2'\n        },\n        # Test 2: Near-degenerate eigenvalues\n        {\n            'A': np.diag([1.0, 1.001, 5.0]),\n            'v1_unnormalized': np.array([1.0, 0.0, 0.0]),\n            'gamma': 0.01,\n            'task': 'compute_lambda_hat_2'\n        },\n        # Test 3: Exact multiplicity\n        {\n            'A': np.diag([2.0, 2.0, 7.0]),\n            'v1_unnormalized': np.array([1.0, 0.0, 0.0]),\n            'gamma': 5.0,\n            'task': 'compute_lambda_hat_2'\n        },\n        # Test 4: Inexact deflation direction\n        {\n            'A': np.diag([1.0, 2.0, 3.0]),\n            'theta_deg': 5.0,\n            'gamma': 10.0,\n            'task': 'compute_lambda_hat_2'\n        },\n        # Test 5: Failure case with too-small penalty\n        {\n            'A': np.diag([1.0, 1.001, 3.0]),\n            'v1_unnormalized': np.array([1.0, 0.0, 0.0]),\n            'gamma': 0.0001,\n            'task': 'check_closeness',\n            'lambda1': 1.0,\n            'lambda2': 1.001\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        A = case['A']\n        gamma = case['gamma']\n\n        # Construct the deflation vector v1\n        if 'v1_unnormalized' in case:\n            v1_unnormalized = case['v1_unnormalized']\n        else:  # Test 4, construct from angle\n            theta_rad = np.deg2rad(case['theta_deg'])\n            v1_unnormalized = np.array([np.cos(theta_rad), np.sin(theta_rad), 0.0])\n        \n        # Normalize v1 to ensure it is a unit vector\n        norm_v1 = np.linalg.norm(v1_unnormalized)\n        if norm_v1 == 0:\n            # Handle potential zero vector, though not expected from problem spec\n            v1 = v1_unnormalized\n        else:\n            v1 = v1_unnormalized / norm_v1\n\n        # 1. Construct the rank-1 updated matrix B\n        # B = A + gamma * v1 * v1^T\n        B = A + gamma * np.outer(v1, v1)\n\n        # 2. Compute the eigenvector x_star for the smallest eigenvalue of B.\n        # np.linalg.eigh returns eigenvalues in ascending order and corresponding\n        # eigenvectors as columns.\n        eigenvalues_B, eigenvectors_B = np.linalg.eigh(B)\n        x_star = eigenvectors_B[:, 0]\n\n        # 3. Compute the estimate lambda_hat_2 as the Rayleigh quotient of A at x_star.\n        # Since x_star is a unit vector, this is x_star^T * A * x_star.\n        lambda_hat_2 = x_star.T @ A @ x_star\n        \n        if case['task'] == 'compute_lambda_hat_2':\n            results.append(lambda_hat_2)\n        elif case['task'] == 'check_closeness':\n            lambda1 = case['lambda1']\n            lambda2 = case['lambda2']\n            # Evaluate the boolean expression |lambda_hat_2 - lambda2| < |lambda_hat_2 - lambda1|\n            is_closer = np.abs(lambda_hat_2 - lambda2) < np.abs(lambda_hat_2 - lambda1)\n            results.append(bool(is_closer))\n\n    # Format the final output string as specified in the problem\n    # Example: [2.0,1.001,2.0,2.0076..,False]\n    # The map(str, ...) correctly handles float and bool types.\n    output_str = f\"[{','.join(map(str, results))}]\"\n    print(output_str)\n\nsolve()\n```", "id": "2383531"}]}