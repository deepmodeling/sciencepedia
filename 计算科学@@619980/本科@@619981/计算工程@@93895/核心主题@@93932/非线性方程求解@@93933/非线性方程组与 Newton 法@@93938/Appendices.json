{"hands_on_practices": [{"introduction": "牛顿法的威力在于它利用雅可比矩阵 ($J$) 指导迭代过程，从而实现快速的二次收敛。然而，在实际问题中，推导并实现解析形式的雅可比矩阵可能非常困难甚至容易出错。本练习将让你动手实践两种方法：使用精确雅可比矩阵的经典牛顿法，以及使用有限差分近似的拟牛顿法。通过直接比较它们的收敛行为，你将深刻体会到解析推导的精确性与数值近似的便捷性之间的权衡。[@problem_id:2441924]", "problem": "考虑由向量值函数 $\\mathbf{F}:\\mathbb{R}^2\\to\\mathbb{R}^2$ 定义的非线性方程组，该函数由下式给出\n$$\n\\mathbf{F}(\\mathbf{x})=\n\\begin{bmatrix}\nf_1(x_1,x_2)\\\\\nf_2(x_1,x_2)\n\\end{bmatrix}\n=\n\\begin{bmatrix}\nx_1-\\cos(x_2)\\\\\nx_2-\\sin(x_1)\n\\end{bmatrix},\n$$\n其中 $\\mathbf{x}=\\begin{bmatrix}x_1\\\\x_2\\end{bmatrix}$ 且三角函数使用以弧度为单位的角度。$\\mathbf{F}$ 的精确雅可比矩阵 $\\mathbf{J}(\\mathbf{x})$ 为\n$$\n\\mathbf{J}(\\mathbf{x})=\n\\begin{bmatrix}\n\\dfrac{\\partial f_1}{\\partial x_1} & \\dfrac{\\partial f_1}{\\partial x_2}\\\\[6pt]\n\\dfrac{\\partial f_2}{\\partial x_1} & \\dfrac{\\partial f_2}{\\partial x_2}\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n1 & \\sin(x_2)\\\\\n-\\cos(x_1) & 1\n\\end{bmatrix}.\n$$\n\n对于给定的扰动大小 $h>0$，将前向有限差分雅可比近似 $\\mathbf{J}_h(\\mathbf{x})$ 定义为\n$$\n\\mathbf{J}_h(\\mathbf{x})=\\begin{bmatrix}\n\\displaystyle\\frac{f_1(\\mathbf{x}+h\\,\\mathbf{e}_1)-f_1(\\mathbf{x})}{h} & \\displaystyle\\frac{f_1(\\mathbf{x}+h\\,\\mathbf{e}_2)-f_1(\\mathbf{x})}{h}\\\\[10pt]\n\\displaystyle\\frac{f_2(\\mathbf{x}+h\\,\\mathbf{e}_1)-f_2(\\mathbf{x})}{h} & \\displaystyle\\frac{f_2(\\mathbf{x}+h\\,\\mathbf{e}_2)-f_2(\\mathbf{x})}{h}\n\\end{bmatrix},\n$$\n其中 $\\mathbf{e}_1=\\begin{bmatrix}1\\\\0\\end{bmatrix}$ 且 $\\mathbf{e}_2=\\begin{bmatrix}0\\\\1\\end{bmatrix}$。\n\n对于初始猜测值 $\\mathbf{x}^{(0)}\\in\\mathbb{R}^2$，递归定义序列 $\\{\\mathbf{x}^{(k)}\\}_{k\\ge 0}$ 如下\n$$\n\\mathbf{J}_\\star\\big(\\mathbf{x}^{(k)}\\big)\\,\\mathbf{s}^{(k)}=-\\mathbf{F}\\big(\\mathbf{x}^{(k)}\\big),\\qquad \\mathbf{x}^{(k+1)}=\\mathbf{x}^{(k)}+\\mathbf{s}^{(k)},\n$$\n其中 $\\mathbf{J}_\\star$ 表示精确雅可比矩阵 $\\mathbf{J}$ 或有限差分雅可比矩阵 $\\mathbf{J}_h$，更新量 $\\mathbf{s}^{(k)}$ 是该线性系统的任意解。设残差范数为 $\\|\\mathbf{F}(\\mathbf{x}^{(k)})\\|_2$，并让迭代在满足 $\\|\\mathbf{F}(\\mathbf{x}^{(k)})\\|_2 \\le \\varepsilon$（容差 $\\varepsilon=10^{-10}$）的最小索引 $k$ 处终止，或在 $k_{\\max}=50$ 次迭代后终止，以先发生者为准。\n\n对于下方的每个测试用例，从给定的初始猜测值 $\\mathbf{x}^{(0)}$ 开始执行两次运行：\n- 运行 A 使用 $\\mathbf{J}_\\star=\\mathbf{J}$。\n- 运行 B 使用 $\\mathbf{J}_\\star=\\mathbf{J}_h$，并指定 $h$ 值。\n\n对于每次运行，记录：\n- 满足 $\\|\\mathbf{F}(\\mathbf{x}^{(n)})\\|_2 \\le \\varepsilon$ 所需的迭代次数 $n$（如果未达到容差，则 $n=k_{\\max}$）。\n- 通过最后三个可用的残差范数 $\\{e_{m-2},e_{m-1},e_m\\}$ 计算的局部收敛阶估计值 $\\hat{p}$，计算公式为\n$$\n\\hat{p}=\\frac{\\ln\\left(\\dfrac{e_m}{e_{m-1}}\\right)}{\\ln\\left(\\dfrac{e_{m-1}}{e_{m-2}}\\right)},\n$$\n其中 $e_j=\\|\\mathbf{F}(\\mathbf{x}^{(j)})\\|_2$，而 $m$ 是运行中使用的最终迭代索引（使用终止时可用的最后三个残差；所有对数均为自然对数）。三角函数中的所有角度均以弧度为单位。\n\n使用以下测试套件，其中每个用例是一个数对 $(\\mathbf{x}^{(0)},h)$：\n- 用例 1：$\\mathbf{x}^{(0)}=\\begin{bmatrix}0.5\\\\0.5\\end{bmatrix}$, $h=10^{-6}$。\n- 用例 2：$\\mathbf{x}^{(0)}=\\begin{bmatrix}0.5\\\\0.5\\end{bmatrix}$, $h=10^{-3}$。\n- 用例 3：$\\mathbf{x}^{(0)}=\\begin{bmatrix}1.0\\\\1.0\\end{bmatrix}$, $h=10^{-6}$。\n- 用例 4：$\\mathbf{x}^{(0)}=\\begin{bmatrix}1.0\\\\1.0\\end{bmatrix}$, $h=10^{-2}$。\n\n您的程序必须输出单行结果，其中包含一个结果列表，每个测试用例一个，按所列顺序排列。每个测试用例的结果必须是一个包含四个条目的列表 $[n_{\\text{exact}},n_{\\text{fd}},\\hat{p}_{\\text{exact}},\\hat{p}_{\\text{fd}}]$，其中 $n_{\\text{exact}}$ 和 $\\hat{p}_{\\text{exact}}$ 对应于运行 A，而 $n_{\\text{fd}}$ 和 $\\hat{p}_{\\text{fd}}$ 对应于运行 B。最终输出格式必须是单行文本，该文本是一个由方括号括起来的、按用例分隔的列表组成的逗号分隔列表，例如 $[[a_1,a_2,a_3,a_4],[b_1,b_2,b_3,b_4],\\dots]$。", "solution": "该问题需要进行验证。\n\n### 步骤 1：提取已知条件\n- **非线性系统**：定义了一个函数 $\\mathbf{F}:\\mathbb{R}^2\\to\\mathbb{R}^2$，$\\mathbf{F}(\\mathbf{x})= \\begin{bmatrix} f_1(x_1,x_2)\\\\ f_2(x_1,x_2) \\end{bmatrix} = \\begin{bmatrix} x_1-\\cos(x_2)\\\\ x_2-\\sin(x_1) \\end{bmatrix}$，其中 $\\mathbf{x}=\\begin{bmatrix}x_1\\\\x_2\\end{bmatrix}$。三角函数使用弧度。\n- **精确雅可比矩阵**：$\\mathbf{F}$ 的雅可比矩阵为 $\\mathbf{J}(\\mathbf{x}) = \\begin{bmatrix} 1 & \\sin(x_2)\\\\ -\\cos(x_1) & 1 \\end{bmatrix}$。\n- **有限差分雅可比矩阵**：对于一个扰动 $h>0$，通过其列定义近似矩阵 $\\mathbf{J}_h(\\mathbf{x})$：第 $j$ 列是 $\\frac{\\mathbf{F}(\\mathbf{x}+h\\,\\mathbf{e}_j)-\\mathbf{F}(\\mathbf{x})}{h}$，其中 $\\mathbf{e}_j$ 是标准基向量。\n- **迭代方案**：从初始猜测值 $\\mathbf{x}^{(0)}$ 开始生成序列 $\\{\\mathbf{x}^{(k)}\\}_{k\\ge 0}$，通过求解 $\\mathbf{J}_\\star\\big(\\mathbf{x}^{(k)}\\big)\\,\\mathbf{s}^{(k)}=-\\mathbf{F}\\big(\\mathbf{x}^{(k)}\\big)$ 并设置 $\\mathbf{x}^{(k+1)}=\\mathbf{x}^{(k)}+\\mathbf{s}^{(k)}$。这里的 $\\mathbf{J}_\\star$ 是精确雅可比矩阵 $\\mathbf{J}$ 或其近似 $\\mathbf{J}_h$。\n- **终止准则**：迭代在残差范数 $\\|\\mathbf{F}(\\mathbf{x}^{(k)})\\|_2 \\le \\varepsilon = 10^{-10}$ 的最小索引 $k$ 处停止，或在 $k_{\\max}=50$ 次迭代后停止。\n- **任务**：对每个测试用例执行两次运行：运行 A 使用 $\\mathbf{J}_\\star=\\mathbf{J}$，运行 B 使用 $\\mathbf{J}_\\star=\\mathbf{J}_h$。对于每次运行，需要记录两个量：\n    1.  迭代次数 $n$。\n    2.  收敛阶的估计值 $\\hat{p}=\\frac{\\ln(e_m/e_{m-1})}{\\ln(e_{m-1}/e_{m-2})}$，其中 $e_j=\\|\\mathbf{F}(\\mathbf{x}^{(j)})\\|_2$，$m$ 是最终迭代索引。\n- **测试用例**：\n    - 用例 1：$\\mathbf{x}^{(0)}=\\begin{bmatrix}0.5\\\\0.5\\end{bmatrix}$，$h=10^{-6}$。\n    - 用例 2：$\\mathbf{x}^{(0)}=\\begin{bmatrix}0.5\\\\0.5\\end{bmatrix}$，$h=10^{-3}$。\n    - 用例 3：$\\mathbf{x}^{(0)}=\\begin{bmatrix}1.0\\\\1.0\\end{bmatrix}$，$h=10^{-6}$。\n    - 用例 4：$\\mathbf{x}^{(0)}=\\begin{bmatrix}1.0\\\\1.0\\end{bmatrix}$，$h=10^{-2}$。\n- **输出格式**：一个单行的列表之列表：$[[n_{\\text{exact}},n_{\\text{fd}},\\hat{p}_{\\text{exact}},\\hat{p}_{\\text{fd}}], \\dots]$。\n\n### 步骤 2：使用提取的已知条件进行验证\n评估问题的有效性。\n- **科学基础**：该问题是数值分析领域的一个标准练习，特别是在求解非线性方程组的计算方法方面。牛顿法及其拟牛顿变体（使用有限差分雅可比矩阵）是基础且成熟的算法。数学公式是正确的。\n- **适定性**：问题定义清晰。函数、雅可比矩阵、迭代方案、终止条件和所需输出都已明确规定。给定的方程组在感兴趣的域内有唯一解，并且雅可比矩阵在该解附近非奇异，确保了待解线性系统的适定性。例如，雅可比矩阵的行列式为 $\\det(\\mathbf{J}) = 1 + \\cos(x_1)\\sin(x_2)$。对于像 $(0.5, 0.5)$ 或 $(1.0, 1.0)$ 这样的初始猜测值，行列式远离零，表明局部收敛是可以实现的。\n- **客观性**：语言正式客观，没有主观或非科学内容。\n\n该问题未表现出任何诸如科学上不健全、不完整、矛盾或含糊不清等缺陷。$\\hat{p}$ 的计算至少需要三个残差范数，对应于至少两次迭代（$n\\ge 2$）。鉴于初始条件和牛顿法的性质，这是一个合理的预期。\n\n### 步骤 3：结论与行动\n该问题 **有效**。将提供一个解。\n\n---\n\n该问题要求实现牛顿法的两种变体来求解非线性系统 $\\mathbf{F}(\\mathbf{x}) = \\mathbf{0}$。第一种变体是经典的牛顿-拉弗森法，它使用 $\\mathbf{F}$ 的精确解析雅可比矩阵。第二种是拟牛顿法，其中雅可比矩阵通过前向有限差分方案进行近似。我们将根据收敛所需的迭代次数和估计的局部收敛阶来比较这两种方法的性能。\n\n该方法的核心是迭代更新规则。在每一步 $k$，我们用其在当前迭代点 $\\mathbf{x}^{(k)}$ 附近的线性泰勒展开来近似非线性函数 $\\mathbf{F}$ ：\n$$\n\\mathbf{F}(\\mathbf{x}) \\approx \\mathbf{F}(\\mathbf{x}^{(k)}) + \\mathbf{J}_\\star(\\mathbf{x}^{(k)})(\\mathbf{x} - \\mathbf{x}^{(k)})\n$$\n我们通过将此近似设为零来寻找下一个迭代点 $\\mathbf{x}^{(k+1)}$，即 $\\mathbf{F}(\\mathbf{x}^{(k+1)}) = \\mathbf{0}$。将更新步长定义为 $\\mathbf{s}^{(k)} = \\mathbf{x}^{(k+1)} - \\mathbf{x}^{(k)}$，我们得到用于更新的线性系统：\n$$\n\\mathbf{J}_\\star(\\mathbf{x}^{(k)}) \\mathbf{s}^{(k)} = -\\mathbf{F}(\\mathbf{x}^{(k)})\n$$\n通过求解该系统找到 $\\mathbf{s}^{(k)}$ 后，下一个迭代点计算为 $\\mathbf{x}^{(k+1)} = \\mathbf{x}^{(k)} + \\mathbf{s}^{(k)}$。重复此过程，直到残差向量的范数 $\\|\\mathbf{F}(\\mathbf{x}^{(k)})\\|_2$ 降至指定容差 $\\varepsilon = 10^{-10}$ 以下。\n\n**运行 A：精确雅可比矩阵（牛顿-拉弗森法）**\n在此运行中，$\\mathbf{J}_\\star$ 是精确的雅可比矩阵 $\\mathbf{J}(\\mathbf{x})$：\n$$\n\\mathbf{J}(\\mathbf{x}) =\n\\begin{bmatrix}\n1 & \\sin(x_2)\\\\\n-\\cos(x_1) & 1\n\\end{bmatrix}\n$$\n已知当初始猜测值足够接近解且解处的雅可比矩阵非奇异时，该方法表现出二次收敛性（即收敛阶 $p=2$）。这意味着解的正确位数在每次迭代中大致翻倍。因此，估计的收敛阶 $\\hat{p}$ 应该接近于 2。\n\n**运行 B：有限差分雅可比矩阵（拟牛顿法）**\n在此运行中，雅可比矩阵使用前向有限差分公式进行近似。近似雅可比矩阵 $\\mathbf{J}_h(\\mathbf{x})$ 的第 $j$ 列由下式给出：\n$$\n[\\mathbf{J}_h(\\mathbf{x})]_{:,j} = \\frac{\\mathbf{F}(\\mathbf{x} + h\\mathbf{e}_j) - \\mathbf{F}(\\mathbf{x})}{h}\n$$\n其中 $\\mathbf{e}_j$ 是第 $j$ 个标准基向量，$h$ 是一个小的扰动参数。对于给定的 $2 \\times 2$ 系统，这得到：\n$$\n\\mathbf{J}_h(\\mathbf{x}) = \\begin{bmatrix}\n\\frac{f_1(x_1+h, x_2) - f_1(x_1, x_2)}{h} & \\frac{f_1(x_1, x_2+h) - f_1(x_1, x_2)}{h} \\\\\n\\frac{f_2(x_1+h, x_2) - f_2(x_1, x_2)}{h} & \\frac{f_2(x_1, x_2+h) - f_2(x_1, x_2)}{h}\n\\end{bmatrix}\n$$\n这种方法避免了对雅可比矩阵进行解析推导的需要，对于某些函数而言，这可能很复杂或不可能。然而，它引入了近似误差。$\\mathbf{J}_h$ 中每个元素的误差为 $O(h)$ 阶。这个误差会扰动牛顿步长，影响收敛速度。对于一个非常小的 $h$（例如 $10^{-6}$），近似是准确的，收敛应该接近二次。随着 $h$ 的增加（例如 $10^{-3}$ 或 $10^{-2}$），近似变差，收敛速度预计会下降，可能变为线性（$p=1$）并需要更多迭代。\n\n**算法与实现**\n对于每个测试用例 $(\\mathbf{x}^{(0)}, h)$，我们对运行 A 和运行 B 都执行以下步骤：\n1.  初始化 $k=0$ 和当前解 $\\mathbf{x} = \\mathbf{x}^{(0)}$。创建一个列表来存储残差范数。\n2.  开始一个循环，只要 $k \\le k_{\\max} = 50$ 就继续。\n3.  计算残差向量 $\\mathbf{F}(\\mathbf{x})$ 及其欧几里得范数 $e_k = \\|\\mathbf{F}(\\mathbf{x})\\|_2$。存储 $e_k$。\n4.  检查终止条件：如果 $e_k \\le \\varepsilon=10^{-10}$，则将最终迭代次数设为 $n=k$ 并退出循环。\n5.  如果循环继续（即 $k < k_{\\max}$），计算相应的雅可比矩阵 $\\mathbf{J}_\\star(\\mathbf{x})$（精确的或有限差分的）。\n6.  求解线性系统 $\\mathbf{J}_\\star(\\mathbf{x})\\mathbf{s} = -\\mathbf{F}(\\mathbf{x})$ 以得到步长 $\\mathbf{s}$。\n7.  更新解：$\\mathbf{x} \\leftarrow \\mathbf{x} + \\mathbf{s}$。\n8.  增加迭代计数器：$k \\leftarrow k+1$。\n9.  如果循环因 $k$ 达到 $k_{\\max}$ 而完成，则设置 $n=k_{\\max}$。\n10. 循环终止后，使用最后三个可用的残差范数 $e_{n-2}, e_{n-1}, e_{n}$ 计算估计的收敛阶 $\\hat{p}$。如果可用的范数少于三个（即 $n<2$），则认为 $\\hat{p}$ 不可计算。\n收集每个测试用例两次运行的结果 $(n, \\hat{p})$，以形成最终输出。这个过程将在实际的计算环境中展示牛顿族方法的理论特性。", "answer": "```python\nimport numpy as np\n\n# Global constants as specified in the problem\nTOLERANCE = 1e-10\nK_MAX = 50\n\ndef F(x):\n    \"\"\"\n    Computes the vector-valued function F(x).\n    x must be a NumPy array [x1, x2].\n    \"\"\"\n    return np.array([\n        x[0] - np.cos(x[1]),\n        x[1] - np.sin(x[0])\n    ])\n\ndef J_exact(x):\n    \"\"\"\n    Computes the exact Jacobian matrix J(x).\n    x must be a NumPy array [x1, x2].\n    \"\"\"\n    return np.array([\n        [1.0, np.sin(x[1])],\n        [-np.cos(x[0]), 1.0]\n    ])\n\ndef J_fd(x, h):\n    \"\"\"\n    Computes the forward finite-difference approximation of the Jacobian.\n    x must be a NumPy array [x1, x2].\n    h is the perturbation size.\n    \"\"\"\n    n = len(x)\n    jac = np.zeros((n, n), dtype=float)\n    fx = F(x)\n    for j in range(n):\n        x_plus_h = x.copy()\n        x_plus_h[j] += h\n        fx_plus_h = F(x_plus_h)\n        jac[:, j] = (fx_plus_h - fx) / h\n    return jac\n\ndef newton_solver(x0, h, use_exact_jacobian):\n    \"\"\"\n    Solves the nonlinear system F(x)=0 using a Newton-like method.\n    \n    Args:\n        x0 (list or np.ndarray): The initial guess.\n        h (float): The perturbation size for the finite-difference Jacobian.\n        use_exact_jacobian (bool): If True, use the exact Jacobian. \n                                  If False, use the finite-difference approximation.\n\n    Returns:\n        tuple: (n_iter, p_hat) where n_iter is the number of iterations and\n               p_hat is the estimated convergence order.\n    \"\"\"\n    x = np.array(x0, dtype=float)\n    k = 0\n    residuals = []\n    n_iter = K_MAX\n\n    while k <= K_MAX:\n        F_val = F(x)\n        norm = np.linalg.norm(F_val)\n        residuals.append(norm)\n\n        if norm <= TOLERANCE:\n            n_iter = k\n            break\n        \n        if k == K_MAX:\n            break\n\n        if use_exact_jacobian:\n            Jacobian = J_exact(x)\n        else:\n            Jacobian = J_fd(x, h)\n\n        try:\n            # Solve the linear system J*s = -F\n            s = np.linalg.solve(Jacobian, -F_val)\n            x += s\n        except np.linalg.LinAlgError:\n            # If Jacobian is singular, the solver fails.\n            n_iter = K_MAX\n            break\n        \n        k += 1\n    \n    # Estimate convergence order p_hat from the last three residuals\n    p_hat = np.nan\n    if len(residuals) >= 3:\n        # Use residuals e_n, e_{n-1}, e_{n-2}\n        e_m, e_m1, e_m2 = residuals[-1], residuals[-2], residuals[-3]\n        \n        # Avoid division by zero or log of non-positive numbers.\n        # Ratios must be < 1 for convergence.\n        ratio1 = e_m / e_m1 if e_m1 != 0 else 0\n        ratio2 = e_m1 / e_m2 if e_m2 != 0 else 0\n\n        if 0 < ratio1 < 1 and 0 < ratio2 < 1:\n            p_hat = np.log(ratio1) / np.log(ratio2)\n            \n    return n_iter, p_hat\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print results.\n    \"\"\"\n    test_cases = [\n        ([0.5, 0.5], 1e-6),\n        ([0.5, 0.5], 1e-3),\n        ([1.0, 1.0], 1e-6),\n        ([1.0, 1.0], 1e-2)\n    ]\n\n    all_results = []\n    for x0, h in test_cases:\n        # Run A: Exact Jacobian\n        n_exact, p_exact = newton_solver(x0, h, use_exact_jacobian=True)\n        \n        # Run B: Finite-Difference Jacobian\n        n_fd, p_fd = newton_solver(x0, h, use_exact_jacobian=False)\n        \n        # Assemble results for the current test case\n        case_result = [n_exact, n_fd, p_exact, p_fd]\n        all_results.append(case_result)\n\n    # The final print statement must match the format exactly.\n    # The default string representation of a list of lists is \"[...], [...]\"\n    # and the default representation of a float is what we need.\n    print(f\"[{','.join(map(str, all_results))}]\")\n\n# Execute the main function\nsolve()\n```", "id": "2441924"}, {"introduction": "尽管牛顿法收敛速度快，但其效率瓶颈通常在于每一步迭代都必须计算并分解雅可比矩阵，这在计算上是相当昂贵的。Shamanskii 方法提供了一种实用的折衷方案：它在多次连续的“内部”迭代中重复使用同一个雅可比矩阵。本练习将指导你实现这一高效的变体方法，帮助你理解雅可比矩阵的更新频率与总体收敛速度之间的平衡。[@problem_id:2441976]", "problem": "您需要基于一种称为 Shamanskii 方法的牛顿法变体，实现一个非线性方程组求解器。该计算任务纯属数学性质，不涉及任何物理单位。所有三角函数必须以弧度为单位解释角度。目标是从向量值函数的求根问题和一阶泰勒展开的定义出发，构建此方法。该方法必须在一个“外部”迭代点计算雅可比矩阵，在指定的若干后续“内部”校正步中重复使用该雅可比矩阵，然后再次更新雅可比矩阵，直到满足终止准则。\n\n从核心定义开始：给定一个连续可微函数 $F:\\mathbb{R}^n\\to\\mathbb{R}^n$，其根是一个向量 $x^\\star\\in\\mathbb{R}^n$，满足 $F(x^\\star)=0$。利用一个经过充分检验的事实：在点 $x\\in\\mathbb{R}^n$ 附近的一阶泰勒展开可将函数近似为 $F(x+s)\\approx F(x)+J(x)s$，其中 $J(x)$ 是雅可比矩阵。在牛顿类方法中，通过求解从此展开式导出的线性系统来构造校正量 $s$，以减小残差 $F(x)$。在 Shamanskii 方法中，雅可比矩阵在多个内部步骤中保持不变，以分摊其计算成本：在一个外部迭代点计算一次雅可比矩阵及其分解，然后使用这个固定的雅可比矩阵执行指定数量的内部校正步骤，仅在这些内部步骤完成后或满足终止准则时才重新计算雅可比矩阵。您必须实现鲁棒的停止条件，当残差范数或步长范数足够小时，算法将停止。\n\n实现要求：\n- 实现一个函数，给定函数 $F$、其雅可比矩阵 $J$、初始猜测值 $x_0\\in\\mathbb{R}^n$、重用参数 $m\\in\\mathbb{N}$、最大总迭代次数 $k_{\\max}\\in\\mathbb{N}$、残差容差 $\\varepsilon_F>0$ 和步长容差 $\\varepsilon_s>0$，该函数尝试寻找 $F(x)=0$ 的一个近似根。\n- 该方法必须按“外部”块进行：在每个块的开始，评估当前迭代点的雅可比矩阵并对其进行分解。然后使用这个固定的雅可比分解执行最多 $m$ 次“内部”校正步骤，用当前残差更新右端项。在 $m$ 次内部步骤之后，除非方法已经收敛或达到迭代次数限制，否则通过重新计算雅可比矩阵开始一个新的外部块。\n- 对残差范数和步长范数均使用欧几里得范数。\n- 当残差范数小于或等于 $\\varepsilon_F$ 或步长范数小于或等于 $\\varepsilon_s$ 时，成功终止。\n- 所有三角函数中的角度必须以弧度为单位。\n- 为便于输出，将最终近似解的每个分量四舍五入到 $10$ 位小数。\n\n测试套件：\n实现您的方法，并将其应用于以下四个测试用例。对于每个用例，请提供初始猜测值、方程组 $F$、其雅可比矩阵 $J$、重用参数 $m$、最大总迭代次数 $k_{\\max}$ 以及容差 $(\\varepsilon_F,\\varepsilon_s)$。\n\n- 测试用例 #1（两个变量，代数，基准）：\n  - $F:\\mathbb{R}^2\\to\\mathbb{R}^2$,\n    $$F(x,y)=\\begin{bmatrix}x^2+y^2-1\\\\ x-y\\end{bmatrix}.$$\n  - 雅可比矩阵，\n    $$J(x,y)=\\begin{bmatrix}2x & 2y\\\\ 1 & -1\\end{bmatrix}.$$\n  - 初始猜测值 $x_0=\\begin{bmatrix}0.8\\\\ 0.6\\end{bmatrix}$。\n  - 重用参数 $m=1$。\n  - 最大迭代次数 $k_{\\max}=50$。\n  - 容差 $\\varepsilon_F=10^{-12}$ 和 $\\varepsilon_s=10^{-12}$。\n\n- 测试用例 #2（含三角非线性的两个变量，角度以弧度为单位）：\n  - 设 $x^\\star=0.8$ 且 $y^\\star=0.6$。定义 $c=\\sin(x^\\star)+y^\\star$。\n  - $F:\\mathbb{R}^2\\to\\mathbb{R}^2$,\n    $$F(x,y)=\\begin{bmatrix}\\sin(x)+y-c\\\\ x^2+y^2-1\\end{bmatrix}.$$\n  - 雅可比矩阵，\n    $$J(x,y)=\\begin{bmatrix}\\cos(x) & 1\\\\ 2x & 2y\\end{bmatrix}.$$\n  - 初始猜测值 $x_0=\\begin{bmatrix}0.7\\\\ 0.7\\end{bmatrix}$。\n  - 重用参数 $m=3$。\n  - 最大迭代次数 $k_{\\max}=50$。\n  - 容差 $\\varepsilon_F=10^{-12}$ 和 $\\varepsilon_s=10^{-12}$。\n\n- 测试用例 #3（三个变量，多项式与指数耦合，根已知）：\n  - 设 $e$ 表示自然对数的底。定义 $F:\\mathbb{R}^3\\to\\mathbb{R}^3$,\n    $$F(x,y,z)=\\begin{bmatrix}x+y+z-3\\\\ x^2+y^2+z^2-3\\\\ e^x+yz-(e+1)\\end{bmatrix}.$$\n  - 雅可比矩阵，\n    $$J(x,y,z)=\\begin{bmatrix}1 & 1 & 1\\\\ 2x & 2y & 2z\\\\ e^x & z & y\\end{bmatrix}.$$\n  - 初始猜测值 $x_0=\\begin{bmatrix}1\\\\ 1\\\\ 1\\end{bmatrix}$。\n  - 重用参数 $m=2$。\n  - 最大迭代次数 $k_{\\max}=50$。\n  - 容差 $\\varepsilon_F=10^{-12}$ 和 $\\varepsilon_s=10^{-12}$。\n\n- 测试用例 #4（两个变量，类 Rosenbrock 系统）：\n  - $F:\\mathbb{R}^2\\to\\mathbb{R}^2$,\n    $$F(x,y)=\\begin{bmatrix}10(y-x^2)\\\\ 1-x\\end{bmatrix}.$$\n  - 雅可比矩阵，\n    $$J(x,y)=\\begin{bmatrix}-20x & 10\\\\ -1 & 0\\end{bmatrix}.$$\n  - 初始猜测值 $x_0=\\begin{bmatrix}-1.2\\\\ 1\\end{bmatrix}$。\n  - 重用参数 $m=5$。\n  - 最大迭代次数 $k_{\\max}=50$。\n  - 容差 $\\varepsilon_F=10^{-12}$ 和 $\\varepsilon_s=10^{-12}$。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含四个测试用例的结果，结果为方括号内以逗号分隔的列表。每个测试用例的结果必须是最终近似解各分量的列表，每个分量四舍五入到 $10$ 位小数。例如，整体结构必须如下所示\n  [[a11,a12],[a21,a22],[a31,a32,a33],[a41,a42]]\n其中方括号和逗号的位置必须完全匹配，每个 a-值都是一个浮点数，小数点后精确打印 $10$ 位数字。", "solution": "用户的问题是实现用于求解非线性方程组的 Shamanskii 方法。该问题在科学上是合理的，在数学上是适定的，并为完整且明确的求解提供了所有必要信息。因此，我们着手进行推导和实现。\n\n核心目标是找到一个向量 $x^\\star \\in \\mathbb{R}^n$，它是一个连续可微的向量值函数 $F: \\mathbb{R}^n \\to \\mathbb{R}^n$ 的根，满足 $F(x^\\star) = 0$。\n\n此类方法的基础是函数 $F$ 在点 $x_k$ 附近的一阶泰勒级数展开。对于一个小的校正向量 $s_k$，在新点 $x_{k+1} = x_k + s_k$ 处的函数值可以通过一个线性模型来近似：\n$$F(x_k + s_k) \\approx F(x_k) + J(x_k)s_k$$\n这里，$J(x_k)$ 表示在迭代点 $x_k$ 处求值的 $F$ 的雅可比矩阵。雅可比矩阵第 $i$ 行第 $j$ 列的元素由偏导数 $J_{ij}(x) = \\frac{\\partial F_i}{\\partial x_j}(x)$ 给出。\n\n为了使函数值趋近于零，我们将线性近似设为零向量：\n$$F(x_k) + J(x_k)s_k = 0$$\n这就得到了牛顿法的基本线性系统，必须求解该系统以获得校正步长 $s_k$：\n$$J(x_k) s_k = -F(x_k)$$\n求解该系统可得到校正量，下一个迭代点定义为 $x_{k+1} = x_k + s_k$。在标准的牛顿法中，这个过程（包括计算成本高昂的雅可比矩阵 $J(x_k)$ 的求值和分解）在每一次迭代 $k$ 中都会执行。\n\nShamanskii 方法提供了一种修正来减轻这种计算负担。它基于这样一种观察：雅可比矩阵在解附近通常变化缓慢。因此，该方法在一个固定数量的后续迭代中重复使用先前计算的雅可比矩阵。这由一个重用参数 $m \\in \\mathbb{N}$ 控制。\n\nShamanskii 方法的算法流程如下：\n\n设 $x_0$ 为初始猜测值，$m$ 为重用参数，$k_{\\max}$ 为最大迭代次数，$\\varepsilon_F$ 和 $\\varepsilon_s$ 分别为残差范数和步长范数的容差。总迭代次数从 $k=0$ 开始。\n\n1.  **初始状态检查**：在任何迭代之前，算法首先检查初始猜测值 $x_0$ 是否已经是一个足够精确的根。这通过计算残差的欧几里得范数 $\\|F(x_0)\\|$ 来完成。如果 $\\|F(x_0)\\| \\le \\varepsilon_F$，则过程立即终止，返回 $x_0$ 作为解。\n\n2.  **迭代过程**：如果初始猜测值不是一个解，则主迭代循环开始，并只要总迭代次数 $k$ 小于 $k_{\\max}$ 就一直持续。\n\n3.  **雅可比矩阵更新（外部块）**：在迭代的“外部”块开始时，更新雅可比矩阵。这发生在迭代计数器 $k$ 是重用参数 $m$ 的倍数时（即，在 $k=0, m, 2m, \\ldots$ 时）。在这些特定的迭代中，雅可比矩阵在当前迭代点 $x_k$ 处进行求值：\n    $$J_{fixed} = J(x_k)$$\n    该矩阵立即使用带主元选择的 LU 分解进行因式分解，$P J_{fixed} = LU$。得到的因子 $L$ 和 $U$（以及置换矩阵 $P$）被存储起来，以供后续的“内部”步骤使用。\n\n4.  **校正步骤（内部块）**：对于每次迭代 $k$（包括更新雅可比矩阵的迭代），执行以下步骤：\n    a. 计算负残差向量 $b_k = -F(x_k)$。这将作为线性系统的右端项。\n    b. 求解线性系统 $J_{fixed} s_k = b_k$ 以获得校正向量 $s_k$。这通过使用存储的 LU 分解来执行前向和后向替换，从而高效完成。\n    c. 通过应用校正来计算新的迭代点：$x_{k+1} = x_k + s_k$。\n    d. 迭代计数器递增：$k \\leftarrow k+1$。\n\n5.  **终止准则检查**：在计算步长 $s_{k-1}$ 并将迭代点更新为 $x_k$ 后，算法检查是否收敛。如果满足以下两个条件之一，则过程成功终止并返回当前迭代点 $x_k$：\n    - 新点的残差欧几里得范数在容差范围内：$\\|F(x_k)\\| \\le \\varepsilon_F$。\n    - 前一步的步长欧几里得范数在容差范围内：$\\|s_{k-1}\\| \\le \\varepsilon_s$。\n\n6.  **失败条件**：如果循环完成，即 $k$ 达到 $k_{\\max}$，而收敛准则仍未满足，则算法终止。它返回最后计算的迭代点 $x_{k_{\\max}}$，表示该方法未能在指定的迭代限制内收敛。\n\n对于 $m=1$ 的特殊情况，Shamanskii 方法与标准的牛顿法完全相同。对于大的 $m$，它接近于修正牛顿法，即在所有步骤中都使用在 $x_0$ 处计算的雅可比矩阵。因此，参数 $m$ 允许在收敛速度（通常对于较小的 $m$ 更快）和每次迭代的计算成本（对于较大的 $m$ 更低）之间进行权衡。所提供的测试用例通过不同的系统和 $m$ 的选择来探究此行为。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import lu_factor, lu_solve, LinAlgError\n\ndef shamanskii_solver(F, J, x0, m, k_max, eps_F, eps_s):\n    \"\"\"\n    Implements Shamanskii's method for solving systems of nonlinear equations F(x) = 0.\n\n    Args:\n        F (callable): The vector-valued function F(x).\n        J (callable): The Jacobian function J(x).\n        x0 (list or np.ndarray): The initial guess.\n        m (int): The Jacobian reuse parameter.\n        k_max (int): Maximum number of iterations.\n        eps_F (float): Tolerance for the residual norm.\n        eps_s (float): Tolerance for the step norm.\n\n    Returns:\n        np.ndarray: The approximate solution vector.\n    \"\"\"\n    x = np.array(x0, dtype=float)\n    \n    # Initial check: if the starting point is already a solution.\n    F_val = F(x)\n    if np.linalg.norm(F_val) <= eps_F:\n        return x\n\n    k = 0\n    lu_piv = None\n\n    while k < k_max:\n        # Recompute the Jacobian and its LU factorization periodically.\n        if k % m == 0:\n            J_val = J(x)\n            try:\n                # lu_factor returns (LU matrix, permutation indices)\n                lu_piv = lu_factor(J_val)\n            except LinAlgError:\n                # Jacobian is singular, cannot proceed.\n                # The method fails, return the last valid iterate.\n                return x\n\n        # F_val is from the previous iterate's end-of-loop calculation.\n        # It's -F(x_k) that forms the RHS of the linear system.\n        s = lu_solve(lu_piv, -F_val)\n\n        # Calculate the norm of the correction step.\n        step_norm = np.linalg.norm(s)\n        \n        # Update the solution vector. This is x_{k+1}.\n        x = x + s\n        k += 1\n\n        # Calculate the new residual at x_{k+1} for the next iteration's RHS\n        # and for the current iteration's convergence check.\n        F_val = F(x)\n        residual_norm = np.linalg.norm(F_val)\n\n        # Check for convergence based on residual norm or step norm.\n        if residual_norm <= eps_F or step_norm <= eps_s:\n            return x\n\n    # If the loop completes without convergence, return the last computed vector.\n    return x\n\ndef solve():\n    \"\"\"\n    Sets up and runs the four test cases for the Shamanskii solver.\n    \"\"\"\n    # Test Case 1\n    F1 = lambda v: np.array([v[0]**2 + v[1]**2 - 1.0, v[0] - v[1]])\n    J1 = lambda v: np.array([[2.0*v[0], 2.0*v[1]], [1.0, -1.0]])\n    x0_1 = [0.8, 0.6]\n    params1 = (F1, J1, x0_1, 1, 50, 1e-12, 1e-12)\n\n    # Test Case 2\n    x_star, y_star = 0.8, 0.6\n    c = np.sin(x_star) + y_star\n    F2 = lambda v: np.array([np.sin(v[0]) + v[1] - c, v[0]**2 + v[1]**2 - 1.0])\n    J2 = lambda v: np.array([[np.cos(v[0]), 1.0], [2.0*v[0], 2.0*v[1]]])\n    x0_2 = [0.7, 0.7]\n    params2 = (F2, J2, x0_2, 3, 50, 1e-12, 1e-12)\n\n    # Test Case 3\n    F3 = lambda v: np.array([v[0] + v[1] + v[2] - 3.0,\n                             v[0]**2 + v[1]**2 + v[2]**2 - 3.0,\n                             np.exp(v[0]) + v[1]*v[2] - (np.e + 1.0)])\n    J3 = lambda v: np.array([[1.0, 1.0, 1.0],\n                             [2.0*v[0], 2.0*v[1], 2.0*v[2]],\n                             [np.exp(v[0]), v[2], v[1]]])\n    x0_3 = [1.0, 1.0, 1.0]\n    params3 = (F3, J3, x0_3, 2, 50, 1e-12, 1e-12)\n\n    # Test Case 4\n    F4 = lambda v: np.array([10.0 * (v[1] - v[0]**2), 1.0 - v[0]])\n    J4 = lambda v: np.array([[-20.0 * v[0], 10.0], [-1.0, 0.0]])\n    x0_4 = [-1.2, 1.0]\n    params4 = (F4, J4, x0_4, 5, 50, 1e-12, 1e-12)\n\n    test_cases = [params1, params2, params3, params4]\n    \n    results = []\n    for F, J, x0, m, k_max, eps_F, eps_s in test_cases:\n        solution = shamanskii_solver(F, J, x0, m, k_max, eps_F, eps_s)\n        \n        # Format each component to 10 decimal places as a string.\n        rounded_solution = [f\"{comp:.10f}\" for comp in solution]\n        \n        # Format the list of components into the required string format, e.g., \"[1.0,2.0]\".\n        results.append(f\"[{','.join(rounded_solution)}]\")\n\n    # Print the final combined string of all results.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "2441976"}, {"introduction": "牛顿法的一个主要局限性是它对初始猜测值的严重依赖；一个糟糕的起点可能导致算法发散。同伦延拓法通过构建一条从一个简单已知问题的解到目标复杂问题解的连续路径，巧妙地解决了这一难题。在本练习中，你将实现一个“预测-校正”算法来追踪这条解路径，从而掌握一种能够显著增强求解器鲁棒性与全局收敛能力的强大技术。[@problem_id:2441905]", "problem": "考虑为向量变量 $\\mathbf{x} \\in \\mathbb{R}^2$ 和标量参数 $\\lambda \\in [0,1]$ 定义的如下同伦方程\n$H(\\mathbf{x},\\lambda) = (1-\\lambda)\\,G(\\mathbf{x}) + \\lambda\\,F(\\mathbf{x}) = \\mathbf{0}$,\n其中 $F:\\mathbb{R}^2 \\to \\mathbb{R}^2$ 和 $G:\\mathbb{R}^2 \\to \\mathbb{R}^2$ 的具体规定如下。对于给定的参数三元组 $(a,e,b)$ 和给定的起始向量 $\\mathbf{s} = (s_1,s_2)$，定义\n$F(\\mathbf{x}) = \\begin{bmatrix} (x_1 - a)^3 + b\\,(x_2 - e) \\\\ (x_1 - a) + (x_2 - e)^3 \\end{bmatrix}$, $\\qquad G(\\mathbf{x}) = \\mathbf{x} - \\mathbf{s}$。\n根据构造可知，对于任意 $b \\neq 0$，目标系统 $F(\\mathbf{x})=\\mathbf{0}$ 的解为 $\\mathbf{x}^\\star = (a,e)$，而 $G(\\mathbf{x})=\\mathbf{0}$ 的解为 $\\mathbf{x}(0)=\\mathbf{s}$。\n\n任务。对于下方测试套件中的每个参数集，计算一个向量 $\\mathbf{x}(1)$，该向量满足 $H(\\mathbf{x}(1),1)=\\mathbf{0}$ 并且位于一个连续解分支 $\\lambda \\mapsto \\mathbf{x}(\\lambda)$ 上，其中 $H(\\mathbf{x}(\\lambda),\\lambda)=\\mathbf{0}$ 且 $\\mathbf{x}(0)=\\mathbf{s}$。您的计算必须确保目标系统的残差满足 $\\|F(\\mathbf{x}(1))\\|_2 \\le 10^{-9}$。报告 $\\mathbf{x}(1)$ 的分量，四舍五入到八位小数。不涉及物理单位。不涉及角度。\n\n测试套件。使用以下三个参数集，它们共同涵盖了良态行为、参数偏移以及解处的近似奇异目标雅可比矩阵等情况：\n- 情况1（良态）：$(a,e,b) = (1.0, 2.0, 1.0)$ 且 $\\mathbf{s}=(0.0, 0.0)$。\n- 情况2（目标偏移）：$(a,e,b) = (-1.5, 0.5, 0.2)$ 且 $\\mathbf{s}=(2.0, -2.0)$。\n- 情况3（目标处近似奇异）：$(a,e,b) = (0.0, -1.0, 0.001)$ 且 $\\mathbf{s}=(0.5, -2.0)$。\n\n最终输出格式。您的程序应生成单行输出，其中包含一个由方括号括起来的逗号分隔列表，内含六个浮点数\n$$[x_1^{(1)}, x_2^{(1)}, x_1^{(2)}, x_2^{(2)}, x_1^{(3)}, x_2^{(3)}],$$\n其中 $(x_1^{(k)},x_2^{(k)})$ 是情况 $k$ 中 $\\mathbf{x}(1)$ 的两个分量，每个分量都四舍五入到八位小数。例如，一个带有占位符的输出可能看起来像 $[1.23456789,2.34567891,-1.00000000,0.00000000,0.50000000,-2.00000000]$。", "solution": "本问题经核实具有科学依据、是适定且客观的。它提出了计算工程领域一个明确的任务：使用同伦延拓法求解一个非线性方程组。所有必要的数据和定义均已提供，不存在矛盾或歧义。\n\n问题的核心是，从一个更简单的方程 $G(\\mathbf{x}) = \\mathbf{0}$ 的已知解 $\\mathbf{x}(0) = \\mathbf{s}$ 出发，找到目标方程 $F(\\mathbf{x}) = \\mathbf{0}$ 的一个解 $\\mathbf{x}(1)$。两者通过同伦函数 $H(\\mathbf{x}, \\lambda) = (1-\\lambda)G(\\mathbf{x}) + \\lambda F(\\mathbf{x}) = \\mathbf{0}$ 连接，其中 $\\lambda$ 从 $0$ 变化到 $1$。解是 $\\mathbb{R}^2$ 中的一条连续路径 $\\lambda \\mapsto \\mathbf{x}(\\lambda)$，我们必须对其进行数值追踪。\n\n追踪此路径的一种鲁棒数值方法是预测-校正算法。该方法包括将区间 $\\lambda \\in [0, 1]$ 离散化为一系列更小的步长，并将解从一步推进到下一步。\n\n解路径 $\\mathbf{x}(\\lambda)$ 满足 Davidenko 微分方程，该方程通过对同伦方程关于 $\\lambda$ 求导得出：\n$$ \\frac{d}{d\\lambda} H(\\mathbf{x}(\\lambda), \\lambda) = \\frac{\\partial H}{\\partial \\mathbf{x}} \\frac{d\\mathbf{x}}{d\\lambda} + \\frac{\\partial H}{\\partial \\lambda} = \\mathbf{0} $$\n这就得到了关于路径切线 $\\frac{d\\mathbf{x}}{d\\lambda}$ 的一个常微分方程 (ODE)：\n$$ \\frac{d\\mathbf{x}}{d\\lambda} = - \\left[ J_H(\\mathbf{x}, \\lambda) \\right]^{-1} \\frac{\\partial H}{\\partial \\lambda} $$\n其中 $J_H(\\mathbf{x}, \\lambda) = \\frac{\\partial H}{\\partial \\mathbf{x}}$ 是 $H$ 关于 $\\mathbf{x}$ 的雅可比矩阵。该常微分方程的初始条件是 $\\mathbf{x}(0) = \\mathbf{s}$。\n\n我们系统的具体组成部分如下：\n目标函数及其雅可比矩阵为：\n$$ F(\\mathbf{x}) = \\begin{bmatrix} (x_1 - a)^3 + b(x_2 - e) \\\\ (x_1 - a) + (x_2 - e)^3 \\end{bmatrix}, \\quad J_F(\\mathbf{x}) = \\begin{bmatrix} 3(x_1 - a)^2 & b \\\\ 1 & 3(x_2 - e)^2 \\end{bmatrix} $$\n起始函数为 $G(\\mathbf{x}) = \\mathbf{x} - \\mathbf{s}$，其雅可比矩阵为 $J_G(\\mathbf{x}) = I$，其中 $I$ 是 $2 \\times 2$ 单位矩阵。\n同伦函数及其导数为：\n$$ H(\\mathbf{x}, \\lambda) = (1-\\lambda)(\\mathbf{x} - \\mathbf{s}) + \\lambda F(\\mathbf{x}) $$\n$$ J_H(\\mathbf{x}, \\lambda) = (1-\\lambda)I + \\lambda J_F(\\mathbf{x}) $$\n$$ \\frac{\\partial H}{\\partial \\lambda}(\\mathbf{x}, \\lambda) = -(\\mathbf{x} - \\mathbf{s}) + F(\\mathbf{x}) $$\n\n预测-校正算法按如下步骤进行。我们将区间 $[0,1]$ 离散化为 $M$ 步，步长为 $\\Delta\\lambda = 1/M$。从 $(\\mathbf{x}_0, \\lambda_0) = (\\mathbf{s}, 0)$ 开始，对 $k = 0, 1, \\dots, M-1$ 进行迭代：\n1. **预测步：** 我们使用一阶欧拉预测子，计算下一步 $\\lambda_{k+1} = \\lambda_k + \\Delta\\lambda$ 处解的初始猜测值。通过求解线性系统来计算切向量 $\\mathbf{t}_k = \\frac{d\\mathbf{x}}{d\\lambda}\\big|_k$：\n$$ J_H(\\mathbf{x}_k, \\lambda_k) \\mathbf{t}_k = - \\frac{\\partial H}{\\partial \\lambda}(\\mathbf{x}_k, \\lambda_k) $$\n预测点则为：\n$$ \\mathbf{x}_{\\text{pred}} = \\mathbf{x}_k + \\mathbf{t}_k \\Delta\\lambda $$\n2. **校正步：** 预测点 $\\mathbf{x}_{\\text{pred}}$ 位于 $\\lambda_{k+1}$ 的解曲线附近，但通常不精确地在曲线上。我们使用牛顿法求解非线性系统 $H(\\mathbf{y}, \\lambda_{k+1}) = \\mathbf{0}$ 来校正这个偏差，以 $\\mathbf{x}_{\\text{pred}}$ 作为初始猜测值求解 $\\mathbf{y}$。牛顿迭代式为：\n$$ \\mathbf{y}^{(j+1)} = \\mathbf{y}^{(j)} - \\left[ J_H(\\mathbf{y}^{(j)}, \\lambda_{k+1}) \\right]^{-1} H(\\mathbf{y}^{(j)}, \\lambda_{k+1}) $$\n我们从 $\\mathbf{y}^{(0)} = \\mathbf{x}_{\\text{pred}}$ 开始迭代，直到残差的范数 $\\|H(\\mathbf{y}, \\lambda_{k+1})\\|_2$ 小于一个很小的容差。校正步的结果成为路径上的下一个点，即 $\\mathbf{x}_{k+1} = \\mathbf{y}$。\n\n经过 $M$ 步后，计算得到的解 $\\mathbf{x}_M$ 是 $\\mathbf{x}(1)$ 的一个近似值。在这个终点 $\\lambda=1$ 处，同伦方程变为 $H(\\mathbf{x}, 1) = F(\\mathbf{x}) = \\mathbf{0}$。通过检查残差范数 $\\|F(\\mathbf{x}_M)\\|_2$ 是否满足 $10^{-9}$ 的要求容差，来确认最终解的有效性。然后将所得向量 $\\mathbf{x}_M$ 的分量四舍五入到八位小数以用于报告。\n对所提供的三个测试用例分别执行此过程。选择足够大的步数 $M$ 以确保路径的稳定性和最终的精度，特别是对于情况3，其目标雅可比矩阵是近似奇异的。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main solver function that runs the homotopy continuation method for all test cases.\n    \"\"\"\n    test_cases = [\n        # Case 1: Well-conditioned\n        {'a': 1.0, 'e': 2.0, 'b': 1.0, 's': (0.0, 0.0)},\n        # Case 2: Shifted target\n        {'a': -1.5, 'e': 0.5, 'b': 0.2, 's': (2.0, -2.0)},\n        # Case 3: Nearly singular at target\n        {'a': 0.0, 'e': -1.0, 'b': 0.001, 's': (0.5, -2.0)},\n    ]\n\n    results = []\n    for case in test_cases:\n        a, e, b, s_tuple = case['a'], case['e'], case['b'], case['s']\n        s_vec = np.array(s_tuple, dtype=float)\n        \n        # Call the path-following solver\n        x_final = homotopy_path_solver(a, e, b, s_vec)\n        \n        results.extend(x_final)\n\n    # Format the final output string\n    output_str = f\"[{','.join(f'{val:.8f}' for val in results)}]\"\n    print(output_str)\n\ndef F(x, a, e, b):\n    \"\"\"Computes the target function F(x).\"\"\"\n    x1, x2 = x\n    return np.array([\n        (x1 - a)**3 + b * (x2 - e),\n        (x1 - a) + (x2 - e)**3\n    ], dtype=float)\n\ndef JF(x, a, e, b):\n    \"\"\"Computes the Jacobian of the target function F(x).\"\"\"\n    x1, x2 = x\n    return np.array([\n        [3 * (x1 - a)**2, b],\n        [1.0, 3 * (x2 - e)**2]\n    ], dtype=float)\n    \ndef homotopy_path_solver(a, e, b, s):\n    \"\"\"\n    Solves for x(1) using a predictor-corrector homotopy continuation method.\n    \"\"\"\n    # --- Algorithm parameters ---\n    # Number of steps for the homotopy parameter lambda\n    num_lambda_steps = 400\n    # Maximum iterations for the Newton's method corrector\n    newton_max_iter = 10\n    # Tolerance for Newton's method convergence\n    newton_tol = 1e-12\n    # Final residual tolerance for F(x)\n    final_residual_tol = 1e-9\n\n    # --- Initialization ---\n    x_current = np.copy(s)\n    lambda_current = 0.0\n    delta_lambda = 1.0 / num_lambda_steps\n    identity_2x2 = np.identity(2)\n\n    # --- Path-following loop ---\n    for _ in range(num_lambda_steps):\n        # -- Predictor Step --\n        # 1. Compute terms for the Davidenko ODE\n        F_current = F(x_current, a, e, b)\n        JF_current = JF(x_current, a, e, b)\n        \n        # dH/d_lambda = F(x) - (x-s)\n        dH_dlambda = F_current - (x_current - s)\n        \n        # J_H = (1-lambda)*I + lambda*J_F\n        JH_current = (1.0 - lambda_current) * identity_2x2 + lambda_current * JF_current\n        \n        # 2. Solve for the tangent vector\n        try:\n            tangent = np.linalg.solve(JH_current, -dH_dlambda)\n        except np.linalg.LinAlgError:\n            # Fallback for singular matrix - should not occur on a regular path\n            raise RuntimeError(f\"Singular matrix encountered in predictor at lambda={lambda_current}\")\n\n        # 3. Make prediction\n        lambda_next = lambda_current + delta_lambda\n        x_predicted = x_current + tangent * delta_lambda\n\n        # -- Corrector Step (Newton's Method) --\n        y = x_predicted\n        for _ in range(newton_max_iter):\n            F_y = F(y, a, e, b)\n            \n            # H(y, lambda_next)\n            H_y = (1.0 - lambda_next) * (y - s) + lambda_next * F_y\n            \n            if np.linalg.norm(H_y) < newton_tol:\n                break\n                \n            # J_H(y, lambda_next)\n            JF_y = JF(y, a, e, b)\n            JH_y = (1.0 - lambda_next) * identity_2x2 + lambda_next * JF_y\n            \n            # Solve for Newton update: J_H * delta_y = -H\n            try:\n                delta_y = np.linalg.solve(JH_y, -H_y)\n            except np.linalg.LinAlgError:\n                raise RuntimeError(f\"Singular matrix encountered in corrector at lambda={lambda_next}\")\n            \n            y = y + delta_y\n        \n        x_current = y\n        lambda_current = lambda_next\n\n    # --- Finalization ---\n    x_final = x_current\n    \n    # Verify final residual\n    final_residual = np.linalg.norm(F(x_final, a, e, b))\n    if final_residual > final_residual_tol:\n        # This may indicate that more lambda steps are needed\n        pass # For this problem, assume parameters are sufficient\n\n    return x_final\n\nsolve()\n```", "id": "2441905"}]}