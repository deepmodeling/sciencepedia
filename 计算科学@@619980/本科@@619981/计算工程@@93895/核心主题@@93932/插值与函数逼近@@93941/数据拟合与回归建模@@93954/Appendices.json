{"hands_on_practices": [{"introduction": "在科学与工程领域，许多描述物理现象的基本定律本质上是非线性的。一个强大而优雅的应对策略是通过数学变换将这些模型转化为线性形式，从而应用简单而高效的线性回归方法。本练习将通过一个化学动力学中的经典模型——阿伦尼乌斯方程，带你实践这一核心技能 [@problem_id:2383201]。你将学习如何通过对数变换，将指数关系转换为线性关系，并使用最小二乘法从实验数据中估计出关键的物理参数，如活化能 $E_a$。", "problem": "您的任务是构建一个完整、可运行的程序，通过拟合化学动力学中广泛使用的热活化速率模型，从与温度相关的反应速率数据中估算活化能。请从以下经过充分检验的温度依赖性模型开始：一个热活化过程的速率常数 $k(T)$ 遵循 Arrhenius 定律，该定律指出，存在一个指前因子 $A$ 和一个活化能 $E_a$，使得 $k(T)$ 通过指数形式依赖于绝对温度 $T$。您的工作是，在一个适当的噪声模型下，基于最大似然估计（MLE）的第一性原理，构建一个通过转换为线性回归模型的估算程序，并论证其合理性，然后实现该程序以从所提供的数据中估算 $E_a$。\n\n基本原理和任务：您必须从热活化反应速率模型以及线性回归和最小二乘估计的基本定义出发。使用通用气体常数 $R$，其值为 $R = 8.314462618$，单位为 $\\mathrm{J \\cdot mol^{-1} \\cdot K^{-1}}$。假设数据由数据对 $(T_i, k_i)$ 组成，其中 $T_i$ 的单位为 $\\mathrm{K}$，$k_i$ 的单位为 $\\mathrm{s^{-1}}$，并假设适当的变换可以产生一个适合普通最小二乘法（OLS）的线性模型。在此基础上，为活化能 $E_a$ 和指前因子 $A$ 推导出一个有原则的估计量，并解释该估计量为何是合适的。然后实现该估计量，以计算几个测试案例的 $E_a$。\n\n要求：\n- 从热活化速率模型和线性回归模型的定义出发，结合合理的噪声假设，根据第一性原理推导估算方法。\n- 将估算方法实现为一个程序，该程序接收下方提供的固定测试数据集（硬编码在程序中），拟合模型，并输出估算的活化能。\n\n数据和测试套件：\n使用以下四个数据集（每个数据集是一个温度列表和相应的速率常数列表）。所有温度单位均为 $\\mathrm{K}$，所有速率常数单位均为 $\\mathrm{s^{-1}}$。您的程序必须为每个数据集独立计算一个活化能估计值。这些数据集是：\n- 案例 1（通用多温度数据集）：\n  - 温度 $T$：$[290.0,310.0,330.0,350.0,370.0,390.0]$\n  - 速率 $k$：$[0.0201,0.0954,0.38,1.49,5.37,17.9]$\n- 案例 2（线性拟合所需最少点数的边界情况）：\n  - 温度 $T$：$[300.0,360.0]$\n  - 速率 $k$：$[0.10,3.20]$\n- 案例 3（温度范围窄，可能存在条件数问题）：\n  - 温度 $T$：$[345.0,350.0,355.0,360.0]$\n  - 速率 $k$：$[2.10,2.95,4.10,5.60]$\n- 案例 4（最高温度数据点存在异常值）：\n  - 温度 $T$：$[310.0,330.0,350.0,370.0,390.0]$\n  - 速率 $k$：$[0.25,0.90,4.10,15.0,12.0]$\n\n输出规格：\n- 对每个数据集，计算活化能估计值 $\\widehat{E}_a$，单位为 $\\mathrm{J \\cdot mol^{-1}}$。\n- 将每个 $\\widehat{E}_a$ 四舍五入到最近的整数（使用标准的四舍五入规则）。\n- 您的程序应生成单行输出，其中包含四个案例的结果，格式为方括号内用逗号分隔的列表，顺序为案例 1、案例 2、案例 3、案例 4。输出必须是 $[\\widehat{E}_{a,1},\\widehat{E}_{a,2},\\widehat{E}_{a,3},\\widehat{E}_{a,4}]$ 这种形式的单行内容，不能有任何其他内容。\n\n角度单位不适用。不涉及百分比。所有物理量都应以规定单位表示。确保您的实现明确使用 $R = 8.314462618$（单位为 $\\mathrm{J \\cdot mol^{-1} \\cdot K^{-1}}$）和以 $\\mathrm{K}$ 为单位的温度。\n\n您的程序必须是自包含的，不需要用户输入，并严格遵守上述输出格式。四个案例的最终答案必须是指定的数值（四舍五入后的整数）。程序应编写为可在任何现代编程环境中运行，并且对于给定的固定数据集必须是确定性的。单行输出必须将所有四个测试用例的结果汇总到一个列表中，例如 $[r_1,r_2,r_3,r_4]$。", "solution": "所提出的问题是计算工程学和化学动力学领域中一个标准且有效的练习。它要求从与温度相关的速率常数数据 $k(T)$ 中估算活化能 $E_a$。此项任务的基础是 Arrhenius 方程，它是物理化学的基石。该问题定义明确，具有科学依据，并为获得唯一解提供了所有必要的数据和常数。我们将首先严格推导估算方法，然后进行实现。\n\n对于一个热活化过程，速率常数 $k(T)$ 的温度依赖性由 Arrhenius 定律描述：\n$$k(T) = A e^{-E_a / (RT)}$$\n此处，$A$ 是指前因子，$E_a$ 是活化能，$R$ 是通用气体常数，$T$ 是绝对温度。该模型相对于其参数 $A$ 和 $E_a$ 是非线性的。为了应用线性回归技术，我们必须首先将该方程转换为线性形式。这通过对两边取自然对数来实现：\n$$\\ln(k(T)) = \\ln\\left(A e^{-E_a / (RT)}\\right)$$\n利用对数的性质，上式可简化为：\n$$\\ln(k(T)) = \\ln(A) - \\frac{E_a}{R} \\frac{1}{T}$$\n该方程现在是线性方程 $y = c + m x$ 的形式，我们可以做如下识别：\n- 因变量为 $y_i = \\ln(k_i)$。\n- 自变量为 $x_i = 1/T_i$。\n- 截距为 $c = \\ln(A)$。\n- 斜率为 $m = -E_a/R$。\n\n给定 $n$ 个数据对 $(T_i, k_i)$。在变换后的空间中，我们对每个数据点的模型是：\n$$y_i = c + m x_i + \\epsilon_i$$\n其中 $\\epsilon_i$ 表示与第 $i$ 次测量相关的噪声或误差。为了推导出一个有原则的估计量，我们必须对该误差的统计特性做出一个合理的假设。我们将使用最大似然估计（MLE）的原理。一个常见且在物理上合理的假设是，线性化模型中的误差 $\\epsilon_i$ 是独立同分布（i.i.d.）的正态随机变量，其均值为零，方差为常数 $\\sigma^2$，记为 $\\epsilon_i \\sim \\mathcal{N}(0, \\sigma^2)$。这个假设是合理的，因为速率常数 $k_i$ 测量的实验不确定性通常与其值成正比。对数变换可以稳定这种方差，使得在变换域中对 $\\epsilon_i$ 采用同方差性（恒定方差）的假设是可信的。\n\n在此高斯噪声假设下，观测到单个数据点 $y_i$ 的似然由正态概率密度函数给出：\n$$p(y_i | x_i, c, m, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y_i - (c + mx_i))^2}{2\\sigma^2}\\right)$$\n对于包含 $n$ 个独立点的整个数据集，总似然函数 $L$ 是各个概率的乘积：\n$$L(c, m, \\sigma^2 | \\mathbf{y}) = \\prod_{i=1}^n p(y_i | x_i, c, m, \\sigma^2) = \\left(2\\pi\\sigma^2\\right)^{-n/2} \\exp\\left(-\\frac{1}{2\\sigma^2} \\sum_{i=1}^n (y_i - c - mx_i)^2\\right)$$\n为了找到使该似然最大化的参数 $c$ 和 $m$，我们可以等价地最大化对数似然 $\\ln L$：\n$$\\ln L(c, m, \\sigma^2 | \\mathbf{y}) = -\\frac{n}{2}\\ln(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^n (y_i - c - mx_i)^2$$\n关于 $c$ 和 $m$ 最大化此表达式等效于最小化残差平方和（RSS）：\n$$\\text{RSS}(c, m) = \\sum_{i=1}^n (y_i - c - mx_i)^2$$\n这个最小化问题就是普通最小二乘法（OLS）的定义。因此，将 OLS 应用于线性化数据，即可得到参数 $c$ 和 $m$ 的最大似然估计。\n\nOLS 问题可以用矩阵形式优雅地表达。我们如下定义观测向量 $\\mathbf{y}$、设计矩阵 $\\mathbf{X}$和参数向量 $\\boldsymbol{\\beta}$：\n$$\\mathbf{y} = \\begin{pmatrix} \\ln(k_1) \\\\ \\ln(k_2) \\\\ \\vdots \\\\ \\ln(k_n) \\end{pmatrix}, \\quad \\mathbf{X} = \\begin{pmatrix} 1/T_1 & 1 \\\\ 1/T_2 & 1 \\\\ \\vdots & \\vdots \\\\ 1/T_n & 1 \\end{pmatrix}, \\quad \\boldsymbol{\\beta} = \\begin{pmatrix} m \\\\ c \\end{pmatrix} = \\begin{pmatrix} -E_a/R \\\\ \\ln(A) \\end{pmatrix}$$\n于是线性模型为 $\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}$。最小化残差向量的欧几里得范数平方 $\\|\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta}\\|_2^2$ 的 OLS 估计值 $\\hat{\\boldsymbol{\\beta}}$ 是正规方程组的解：\n$$(\\mathbf{X}^T \\mathbf{X}) \\hat{\\boldsymbol{\\beta}} = \\mathbf{X}^T \\mathbf{y}$$\n解由 $\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{y}$ 给出。这可以使用标准的线性最小二乘算法进行数值计算。从估计的参数向量 $\\hat{\\boldsymbol{\\beta}} = [\\hat{m}, \\hat{c}]^T$ 中，我们提取斜率 $\\hat{m}$。然后直接计算活化能的估计值 $\\widehat{E}_a$：\n$$\\widehat{E}_a = -\\hat{m} R$$\n我们将使用所提供的通用气体常数值，$R = 8.314462618 \\mathrm{J \\cdot mol^{-1} \\cdot K^{-1}}$。\n\n实现将通过以下步骤处理四个数据集中的每一个：\n1. 将原始数据 $(T_i, k_i)$ 转换为线性坐标 $(x_i, y_i) = (1/T_i, \\ln(k_i))$。\n2. 构建设计矩阵 $\\mathbf{X}$ 和观测向量 $\\mathbf{y}$。\n3. 求解线性最小二乘问题 $\\mathbf{y} \\approx \\mathbf{X}\\boldsymbol{\\beta}$ 以获得估计斜率 $\\hat{m}$。\n4. 计算 $\\widehat{E}_a = -\\hat{m} R$。\n5. 将 $\\widehat{E}_a$ 的结果四舍五入到最接近的整数，单位为 $\\mathrm{J \\cdot mol^{-1}}$。\n所有四个案例的结果将被汇总成指定的输出格式。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n# The problem statement allows for scipy, but numpy.linalg.lstsq provides a more\n# fundamental implementation of the derived least-squares solution, which aligns\n# with the first-principles derivation.\n\ndef solve():\n    \"\"\"\n    Estimates activation energy from temperature-dependent rate data\n    using a linearized Arrhenius model and Ordinary Least Squares.\n    \"\"\"\n    # Define the universal gas constant as specified.\n    # R has units of J * mol^{-1} * K^{-1}.\n    R = 8.314462618\n\n    # Define the test cases from the problem statement.\n    # Each case is a tuple of (Temperatures in K, Rate constants in s^{-1}).\n    test_cases = [\n        # Case 1 (general multitemperature dataset)\n        (np.array([290.0, 310.0, 330.0, 350.0, 370.0, 390.0]),\n         np.array([0.0201, 0.0954, 0.38, 1.49, 5.37, 17.9])),\n\n        # Case 2 (boundary case with the minimum number of points)\n        (np.array([300.0, 360.0]),\n         np.array([0.10, 3.20])),\n\n        # Case 3 (narrow temperature span)\n        (np.array([345.0, 350.0, 355.0, 360.0]),\n         np.array([2.10, 2.95, 4.10, 5.60])),\n\n        # Case 4 (presence of an outlier)\n        (np.array([310.0, 330.0, 350.0, 370.0, 390.0]),\n         np.array([0.25, 0.90, 4.10, 15.0, 12.0]))\n    ]\n\n    estimated_energies = []\n    \n    for T_data, k_data in test_cases:\n        # Step 1: Linearize the data according to the Arrhenius equation.\n        # y = ln(k)\n        # x = 1/T\n        y = np.log(k_data)\n        x = 1.0 / T_data\n\n        # Step 2: Set up the linear least-squares problem y = X * beta.\n        # The model is y = m*x + c, so beta = [m, c]^T.\n        # The design matrix X has columns for x and a constant for the intercept.\n        X = np.vstack([x, np.ones(len(x))]).T\n\n        # Step 3: Solve for the parameters [slope, intercept] using OLS.\n        # numpy.linalg.lstsq solves the equation y = X * beta for beta.\n        # The first element of the returned solution vector is the slope, m.\n        # rcond=None is specified to use the machine-precision default.\n        slope, intercept = np.linalg.lstsq(X, y, rcond=None)[0]\n\n        # Step 4: Calculate the activation energy from the slope.\n        # The slope m = -E_a / R.\n        # Therefore, E_a = -slope * R.\n        # The result E_a will be in J * mol^{-1}.\n        activation_energy = -slope * R\n\n        # Step 5: Round the result to the nearest integer as required.\n        rounded_E_a = int(round(activation_energy))\n        estimated_energies.append(rounded_E_a)\n\n    # Final print statement in the exact required format.\n    # The output is a comma-separated list of integers enclosed in brackets.\n    print(f\"[{','.join(map(str, estimated_energies))}]\")\n\nsolve()\n```", "id": "2383201"}, {"introduction": "尽管线性回归是一个强大的工具，但其最常见的形式——普通最小二乘法 (OLS) ——有一个显著的弱点：它对数据中的异常值（outliers）极为敏感。这是因为 OLS 旨在最小化残差的平方和，使得巨大的误差被不成比例地放大。本练习 [@problem_id:2383160] 引导你探索一种更稳健的替代方案——基于 Huber 损失的回归，它巧妙地结合了平方损失和绝对值损失的优点，有效降低了异常值对模型拟合的破坏性影响，这是处理真实世界测量数据的关键一步。", "problem": "你的任务是为一个包含单一预测变量和截距的简单线性模型实现并比较两种估计量：普通最小二乘（OLS）估计量和基于Huber损失的稳健估计量。该模型为\n$$\ny_i = a\\,x_i + b + \\varepsilon_i,\\quad i=1,\\dots,N,\n$$\n其中，$a$ 是斜率，$b$ 是截距，$\\varepsilon_i$ 是残差。对于OLS，参数估计 $(\\hat a_{\\mathrm{OLS}}, \\hat b_{\\mathrm{OLS}})$ 定义为残差平方和的最小化子\n$$\n(\\hat a_{\\mathrm{OLS}}, \\hat b_{\\mathrm{OLS}}) \\in \\arg\\min_{a,b} \\sum_{i=1}^{N} \\left(y_i - a\\,x_i - b\\right)^2.\n$$\n对于稳健估计量，定义参数为 $\\delta>0$ 的Huber损失为\n$$\n\\phi_\\delta(r) = \\begin{cases}\n\\dfrac{1}{2} r^2, & \\text{若 } |r|\\le \\delta, \\\\\n\\delta\\left(|r| - \\dfrac{1}{2}\\delta\\right), & \\text{若 } |r| > \\delta,\n\\end{cases}\n$$\n并设\n$$\n(\\hat a_{\\mathrm{Huber}}, \\hat b_{\\mathrm{Huber}}) \\in \\arg\\min_{a,b} \\sum_{i=1}^{N} \\phi_\\delta\\!\\left(y_i - a\\,x_i - b\\right).\n$$\n\n请实现一个程序，构建以下三个确定性测试用例，并为每个用例计算 $(\\hat a_{\\mathrm{OLS}}, \\hat b_{\\mathrm{OLS}})$ 和 $(\\hat a_{\\mathrm{Huber}}, \\hat b_{\\mathrm{Huber}})$。所有测试均使用相同的Huber参数 $\\delta$，即 $\\delta = 0.1$。\n\n测试套件（每个测试明确指定 $x$、$y$ 和 $N$）：\n\n- 测试 $1$（理想情况，无异常值）：\n  - $x_i = i$，$i \\in \\{-5,-4,\\dots,4,5\\}$，因此 $N=11$。\n  - 对每个 $i$，$y_i = 2\\,x_i + 1$。\n\n- 测试 $2$（单个高杠杆率异常值）：\n  - 从测试 $1$ 开始，并附加一个额外的点 $(x_o, y_o) = (50, -100)$，因此 $N=12$。\n  - 因此，$x$ 是测试 $1$ 中的序列附加一个 $50$，$y$ 是相应的序列附加一个 $-100$。\n\n- 测试 $3$（小样本边界情况）：\n  - $x = [0, 1]$，因此 $N=2$。\n  - $y = [1, 3]$。\n\n你的程序必须为每个测试用例计算 $(\\hat a_{\\mathrm{OLS}}, \\hat b_{\\mathrm{OLS}})$ 和 $(\\hat a_{\\mathrm{Huber}}, \\hat b_{\\mathrm{Huber}})$，并将其报告为实数。不涉及物理单位或角度。\n\n最终输出格式：你的程序应生成一行输出，包含一个外层列表，内含三个内层列表，每个内层列表对应一个测试用例，顺序固定如下\n$$\n[\\,[\\hat a_{\\mathrm{OLS}}, \\hat b_{\\mathrm{OLS}}, \\hat a_{\\mathrm{Huber}}, \\hat b_{\\mathrm{Huber}}]_{\\text{测试 }1},\\; [\\cdot]_{\\text{测试 }2},\\; [\\cdot]_{\\text{测试 }3}\\,],\n$$\n输出格式为一个用方括号括起来的逗号分隔列表，每个数值条目四舍五入到小数点后六位。例如，外层列表包含三个内层列表，每个内层列表按指定顺序包含四个实数。", "solution": "该问题陈述已经过验证，被认为是有效的。它具有科学依据、问题适定、客观，并包含了推导出唯一解所需的所有信息。该任务是计算统计学中的一个标准练习，旨在比较经典的普通最小二乘（OLS）估计量与一种基于Huber损失函数的现代稳健替代方法。\n\n我们需要为线性模型 $y_i = a x_i + b + \\varepsilon_i$ 找到参数估计 $(\\hat a, \\hat b)$。\n\n**普通最小二乘（OLS）估计量**\n\nOLS估计量最小化残差平方和，其损失函数定义为 $L_{\\mathrm{OLS}}(a, b) = \\sum_{i=1}^{N} (y_i - a\\,x_i - b)^2$。这是一个关于参数 $(a, b)$ 的二次、凸且平滑的函数。通过将其关于 $a$ 和 $b$ 的偏导数设为零，可以找到唯一的最小值。这会得到正规方程组：\n$$\n\\begin{pmatrix}\n\\sum_{i=1}^{N} x_i^2 & \\sum_{i=1}^{N} x_i \\\\\n\\sum_{i=1}^{N} x_i & N\n\\end{pmatrix}\n\\begin{pmatrix}\n\\hat a_{\\mathrm{OLS}} \\\\ \\hat b_{\\mathrm{OLS}}\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n\\sum_{i=1}^{N} x_i y_i \\\\\n\\sum_{i=1}^{N} y_i\n\\end{pmatrix}\n$$\n求解这个 $2 \\times 2$ 线性系统，可得到参数的闭式解析解：\n$$\n\\hat a_{\\mathrm{OLS}} = \\frac{N \\sum x_i y_i - (\\sum x_i)(\\sum y_i)}{N \\sum x_i^2 - (\\sum x_i)^2}\n$$\n$$\n\\hat b_{\\mathrm{OLS}} = \\bar{y} - \\hat a_{\\mathrm{OLS}} \\bar{x}\n$$\n其中 $\\bar{x} = \\frac{1}{N}\\sum x_i$ 且 $\\bar{y} = \\frac{1}{N}\\sum y_i$。这些公式将直接用于实现计算。\n\n**基于Huber损失的稳健估计量**\n\n稳健估计量最小化Huber损失之和，$L_{\\mathrm{Huber}}(a, b) = \\sum_{i=1}^{N} \\phi_\\delta\\!\\left(y_i - a\\,x_i - b\\right)$，其中Huber损失函数 $\\phi_\\delta(r)$ 定义为：\n$$\n\\phi_\\delta(r) = \\begin{cases}\n\\dfrac{1}{2} r^2, & \\text{若 } |r|\\le \\delta, \\\\\n\\delta\\left(|r| - \\dfrac{1}{2}\\delta\\right), & \\text{若 } |r| > \\delta.\n\\end{cases}\n$$\n该损失函数对于小残差（如OLS）表现为二次函数，对于大残差则表现为线性函数。这个特性使得估计量对异常值具有稳健性，因为大的误差不会以二次方的形式贡献到总损失中。函数 $L_{\\mathrm{Huber}}(a, b)$ 是凸函数，但不是连续可微的（在 $|y_i - a x_i - b| = \\delta$ 处有“扭结”）。因此，不存在像OLS那样的闭式解。\n\n这是一个凸优化问题，可以使用数值方法求解。我们将利用一个通用的数值优化程序，特别是 `scipy.optimize.minimize`，来找到最小化总Huber损失的参数 $(\\hat a_{\\mathrm{Huber}}, \\hat b_{\\mathrm{Huber}})$。OLS估计值将作为迭代求解器的一个合适的初始猜测值。给定的参数 $\\delta$ 为 $0.1$。\n\n**各测试用例的实现**\n\n对于每个测试用例，我们首先构建数据向量 $x$ 和 $y$。然后，我们将使用解析公式计算OLS估计值。随后，我们将定义Huber损失目标函数，并使用 `scipy.optimize.minimize` 找到Huber估计值。\n\n- **测试 1：** 数据点完美地落在直线 $y_i = 2x_i + 1$ 上。对于精确参数 $(a,b) = (2,1)$，所有残差均为零。OLS和Huber损失函数都在这一点上达到其全局最小值零。因此，我们预期 $(\\hat a_{\\mathrm{OLS}}, \\hat b_{\\mathrm{OLS}}) = (\\hat a_{\\mathrm{Huber}}, \\hat b_{\\mathrm{Huber}}) = (2, 1)$。\n\n- **测试 2：** 在测试1的数据中加入一个异常值 $(x_o, y_o) = (50, -100)$。这个异常值具有高杠杆率（其 $x$ 值远离其他 $x$ 值的均值）和一个大的残差。众所周知，OLS对这类点高度敏感；对大残差的二次惩罚将显著地将回归线“拉”向该异常值，从而极大地改变了来自 $(2, 1)$ 的估计值。相比之下，Huber估计量对大残差的线性惩罚将限制异常值的影响。我们预期 $(\\hat a_{\\mathrm{Huber}}, \\hat b_{\\mathrm{Huber}})$ 将保持更接近其他11个数据点真实的基础参数 $(2, 1)$，这展示了稳健回归的原理。\n\n- **测试 3：** 只有两个数据点 $(0, 1)$ 和 $(1, 3)$，存在一条唯一的直线穿过这两点。这条直线的方程是 $y = 2x + 1$。对于参数 $(a,b) = (2,1)$，残差为零。与测试1一样，OLS和Huber估计量都将识别出这些精确参数，得出 $(\\hat a_{\\mathrm{OLS}}, \\hat b_{\\mathrm{OLS}}) = (\\hat a_{\\mathrm{Huber}}, \\hat b_{\\mathrm{Huber}}) = (2, 1)$。", "answer": "```python\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef solve():\n    \"\"\"\n    Implements and compares OLS and Huber estimators for linear regression\n    on three specified test cases.\n    \"\"\"\n\n    # --- Estimator Implementations ---\n\n    def calculate_ols(x: np.ndarray, y: np.ndarray) -> tuple[float, float]:\n        \"\"\"\n        Calculates the Ordinary Least Squares (OLS) estimator for a and b.\n        y = a*x + b\n        \"\"\"\n        N = len(x)\n        if N < 2:\n            return (np.nan, np.nan)\n\n        sum_x = np.sum(x)\n        sum_y = np.sum(y)\n        sum_xy = np.sum(x * y)\n        sum_x2 = np.sum(x**2)\n\n        denominator = N * sum_x2 - sum_x**2\n        if np.abs(denominator) < 1e-12: # Check for collinearity (all x values are the same)\n            return (np.nan, np.nan)\n\n        a_ols = (N * sum_xy - sum_x * sum_y) / denominator\n        b_ols = (sum_y - a_ols * sum_x) / N\n        \n        return a_ols, b_ols\n\n    def calculate_huber(x: np.ndarray, y: np.ndarray, delta: float) -> tuple[float, float]:\n        \"\"\"\n        Calculates the Huber-loss-based robust estimator for a and b.\n        y = a*x + b\n        \"\"\"\n        \n        def huber_loss_objective(params: np.ndarray, x_data: np.ndarray, y_data: np.ndarray, d: float) -> float:\n            \"\"\"\n            Objective function: sum of Huber losses for a given set of parameters.\n            \"\"\"\n            a, b = params\n            residuals = y_data - (a * x_data + b)\n            abs_residuals = np.abs(residuals)\n            \n            # Quadratic part for small residuals\n            quadratic_loss = 0.5 * residuals[abs_residuals <= d]**2\n            \n            # Linear part for large residuals\n            linear_loss = d * (abs_residuals[abs_residuals > d] - 0.5 * d)\n            \n            return np.sum(quadratic_loss) + np.sum(linear_loss)\n\n        # Use OLS estimates as a good initial guess\n        a_ols, b_ols = calculate_ols(x, y)\n        initial_guess = np.array([a_ols, b_ols])\n\n        result = minimize(\n            huber_loss_objective,\n            x0=initial_guess,\n            args=(x, y, delta),\n            method='BFGS' # A standard quasi-Newton method suitable for this problem\n        )\n\n        a_huber, b_huber = result.x\n        return a_huber, b_huber\n\n    # --- Test Cases Definition ---\n    \n    # Test 1: Happy path, no outlier\n    x1 = np.arange(-5, 6, 1, dtype=float)\n    y1 = 2 * x1 + 1\n    \n    # Test 2: Single high-leverage outlier\n    x2 = np.append(x1, 50.0)\n    y2 = np.append(y1, -100.0)\n    \n    # Test 3: Small-sample boundary case\n    x3 = np.array([0.0, 1.0])\n    y3 = np.array([1.0, 3.0])\n    \n    test_cases = [\n        (x1, y1),\n        (x2, y2),\n        (x3, y3)\n    ]\n    \n    delta = 0.1\n    all_results = []\n    \n    for x_data, y_data in test_cases:\n        a_ols, b_ols = calculate_ols(x_data, y_data)\n        a_huber, b_huber = calculate_huber(x_data, y_data, delta)\n        \n        case_results = [a_ols, b_ols, a_huber, b_huber]\n        all_results.append(case_results)\n        \n    # --- Format and Print Output ---\n    \n    outer_list_str = []\n    for inner_list in all_results:\n        inner_list_str = [f\"{val:.6f}\" for val in inner_list]\n        outer_list_str.append(f\"[{','.join(inner_list_str)}]\")\n        \n    final_output = f\"[{','.join(outer_list_str)}]\"\n    print(final_output)\n\nsolve()\n```", "id": "2383160"}, {"introduction": "在处理现实世界的数据集时，我们经常会遇到另一个挑战：多重共线性（multicollinearity），即模型中的多个预测变量彼此高度相关。这种情况会导致普通最小二乘法 (OLS) 的系数估计变得非常不稳定，难以解释。本练习 [@problem_id:2383123] 介绍了一种有效的降维回归技术——主成分回归 (PCR)。你将通过实践学习 PCR 如何通过将原始特征转换为一组新的、不相关的“主成分”，并在此基础上进行回归，从而在存在严重共线性的情况下构建出更稳定、更可靠的预测模型。", "problem": "给定一个确定性的、高度共线的线性回归数据集，其定义如下。设 $n=60$，并用 $i \\in \\{1,2,\\dots,n\\}$ 为观测值索引。定义以下量\n$$\nz_i = i - \\frac{n+1}{2}, \\quad s_i = \\sin\\!\\left(\\frac{2\\pi i}{n}\\right), \\quad c_i = \\cos\\!\\left(\\frac{2\\pi i}{n}\\right),\n$$\n其中所有三角函数参数的单位均为弧度。设 $\\varepsilon = 10^{-6}$，$\\delta = 10^{-8}$，且 $\\eta = 10^{-4}$。构建三个特征\n$$\nx_{i1} = z_i + \\varepsilon s_i,\\quad\nx_{i2} = 2 z_i + \\varepsilon c_i,\\quad\nx_{i3} = 3 x_{i1} - 1.5 x_{i2} + \\delta \\sin\\!\\left(\\frac{4\\pi i}{n}\\right),\n$$\n并用系数 $\\theta_1 = 1$，$\\theta_2 = -\\frac{1}{2}$，$\\theta_3 = \\frac{1}{4}$ 定义响应\n$$\ny_i = \\theta_1 x_{i1} + \\theta_2 x_{i2} + \\theta_3 x_{i3} + \\eta \\sin\\!\\left(\\frac{6\\pi i}{n}\\right).\n$$\n设 $X \\in \\mathbb{R}^{n \\times 3}$ 是设计矩阵，其行为 $x_i^\\top = [x_{i1}, x_{i2}, x_{i3}]$，而 $y \\in \\mathbb{R}^{n}$ 是响应向量，其元素为 $y_i$。\n\n将普通最小二乘法 (OLS) 估计量定义为目标函数\n$$\nS(\\beta) = \\sum_{i=1}^{n} \\left(y_i - x_i^\\top \\beta\\right)^2,\n$$\n的任意最小化子 $\\hat{\\beta}_{\\mathrm{OLS}} \\in \\mathbb{R}^{3}$，以及相应的拟合值 $\\hat{y}_{\\mathrm{OLS}} = X \\hat{\\beta}_{\\mathrm{OLS}}$。将 OLS 的样本内均方误差 (MSE) 定义为\n$$\n\\mathrm{MSE}_{\\mathrm{OLS}} = \\frac{1}{n} \\left\\| y - \\hat{y}_{\\mathrm{OLS}} \\right\\|_2^2.\n$$\n\n使用 $X$ 的主子空间定义具有 $k$ 个分量的主成分回归 (PCR)。设奇异值分解为 $X = U \\Sigma V^\\top$，其中奇异值按非递增顺序排列。对于 $k \\in \\{0,1,2,3\\}$，用 $W_k \\in \\mathbb{R}^{3 \\times k}$ 表示由 $V$ 的前 $k$ 列组成的矩阵。将 $\\hat{\\alpha}_k \\in \\mathbb{R}^{k}$ 定义为\n$$\n\\sum_{i=1}^{n} \\left(y_i - x_i^\\top W_k \\alpha\\right)^2 \\quad \\text{在} \\quad \\alpha \\in \\mathbb{R}^{k} \\text{上}\n$$\n的任意最小化子，约定当 $k=0$ 时，最小化是在单元素集 $\\mathbb{R}^0$ 上进行，所得拟合为 $\\hat{y}_{\\mathrm{PCR},0} = 0$。令 $\\hat{\\beta}_{\\mathrm{PCR},k} = W_k \\hat{\\alpha}_k$ 且 $\\hat{y}_{\\mathrm{PCR},k} = X \\hat{\\beta}_{\\mathrm{PCR},k}$。将具有 $k$ 个分量的 PCR 的样本内均方误差定义为\n$$\n\\mathrm{MSE}_{\\mathrm{PCR}}(k) = \\frac{1}{n} \\left\\| y - \\hat{y}_{\\mathrm{PCR},k} \\right\\|_2^2.\n$$\n\n测试套件。对于参数值 $k \\in \\{0,1,2,3\\}$，计算比率\n$$\nr_k = \\frac{\\mathrm{MSE}_{\\mathrm{PCR}}(k)}{\\mathrm{MSE}_{\\mathrm{OLS}}}.\n$$\n\n您的任务是实现一个程序，按规定构建 $X$ 和 $y$，为测试套件中的 $k$ 值计算 $\\mathrm{MSE}_{\\mathrm{OLS}}$ 和 $\\mathrm{MSE}_{\\mathrm{PCR}}(k)$，并输出列表 $[r_0,r_1,r_2,r_3]$，其中每个 $r_k$ 都表示为四舍五入到六位小数的小数。\n\n最终输出格式。您的程序应生成单行输出，其中包含用方括号括起来的、以逗号分隔的结果列表，顺序为 $k=0,1,2,3$，例如 $[0.123456,0.234567,0.345678,0.456789]$。", "solution": "该问题陈述是有效的。它构成了一个计算统计学中适定的数值练习，旨在评估普通最小二乘法 (OLS) 回归相对于主成分回归 (PCR) 在一个表现出严重多重共线性数据集上的样本内性能。所有参数、变量和目标都经过了数学上的精确定义，并且在科学上是合理的。\n\n解决方案分四个阶段进行：\n1.  构建设计矩阵 $X \\in \\mathbb{R}^{60 \\times 3}$ 和响应向量 $y \\in \\mathbb{R}^{60}$。\n2.  计算 OLS 模型的样本内均方误差 $\\mathrm{MSE}_{\\mathrm{OLS}}$。\n3.  计算 PCR 模型的样本内均方误差 $\\mathrm{MSE}_{\\mathrm{PCR}}(k)$，其中 $k \\in \\{0, 1, 2, 3\\}$。\n4.  计算指定的比率 $r_k = \\mathrm{MSE}_{\\mathrm{PCR}}(k) / \\mathrm{MSE}_{\\mathrm{OLS}}$。\n\n**1. 数据构建**\n\n数据集根据提供的规范构建。观测数量为 $n=60$。索引 $i$ 的范围从 1 到 60。辅助量为：\n$$\nz_i = i - \\frac{n+1}{2} = i - 30.5\n$$\n$$\ns_i = \\sin\\left(\\frac{2\\pi i}{n}\\right) = \\sin\\left(\\frac{\\pi i}{30}\\right)\n$$\n$$\nc_i = \\cos\\left(\\frac{2\\pi i}{n}\\right) = \\cos\\left(\\frac{\\pi i}{30}\\right)\n$$\n常数为 $\\varepsilon = 10^{-6}$，$\\delta = 10^{-8}$，以及 $\\eta = 10^{-4}$。\n三个特征向量（$X$ 的列）构建如下：\n$$\nx_{i1} = z_i + \\varepsilon s_i\n$$\n$$\nx_{i2} = 2 z_i + \\varepsilon c_i\n$$\n$$\nx_{i3} = 3 x_{i1} - 1.5 x_{i2} + \\delta \\sin\\left(\\frac{4\\pi i}{n}\\right)\n$$\n第三个特征 $x_{i3}$ 被构造成几乎是 $x_{i1}$ 和 $x_{i2}$ 的完美线性组合，因为项 $3 x_{i1} - 1.5 x_{i2}$ 可简化为 $\\varepsilon (3 s_i - 1.5 c_i)$，这是一个非常小的量，而添加的扰动项由一个更小的 $\\delta$ 进行缩放。这在设计矩阵 $X$ 中引发了严重的多重共线性。\n响应向量 $y$ 定义如下：\n$$\ny_i = \\theta_1 x_{i1} + \\theta_2 x_{i2} + \\theta_3 x_{i3} + \\eta \\sin\\left(\\frac{6\\pi i}{n}\\right)\n$$\n其中系数为 $\\theta_1 = 1$，$\\theta_2 = -1/2$，以及 $\\theta_3 = 1/4$。项 $\\eta \\sin(6\\pi i/n)$ 代表一个确定性的噪声分量。\n\n**2. 普通最小二乘法 (OLS) 误差**\n\nOLS 估计量 $\\hat{\\beta}_{\\mathrm{OLS}}$ 最小化残差平方和 $S(\\beta) = \\|y - X\\beta\\|_2^2$。得到的拟合值 $\\hat{y}_{\\mathrm{OLS}} = X\\hat{\\beta}_{\\mathrm{OLS}}$ 代表响应向量 $y$ 在设计矩阵 $X$ 的列空间（记为 $\\mathrm{Col}(X)$）上的正交投影。\nOLS 的误差平方和 (SSE) 由下式给出：\n$$\n\\mathrm{SSE}_{\\mathrm{OLS}} = \\|y - \\hat{y}_{\\mathrm{OLS}}\\|_2^2\n$$\n尽管由于 $X$ 的多重共线性，$\\hat{\\beta}_{\\mathrm{OLS}}$ 在数值上可能不稳定，但投影 $\\hat{y}_{\\mathrm{OLS}}$ 是唯一且良定义的。使用数值稳健的方法，例如基于 QR 分解或奇异值分解 (SVD) 的方法，来计算最小二乘解。`numpy.linalg.lstsq` 函数提供了一种直接计算 SSE 的稳定方法。然后，均方误差为 $\\mathrm{MSE}_{\\mathrm{OLS}} = \\mathrm{SSE}_{\\mathrm{OLS}} / n$。\n\n**3. 主成分回归 (PCR) 误差**\n\nPCR 是一种回归技术，它将数据投影到由特征的主成分定义的低维子空间上。$X$ 的主成分由其 SVD（$X = U\\Sigma V^\\top$）确定，其中 $V$ 的列是主方向。\n\n具有 $k$ 个分量的 PCR 模型将回归系数限制在由前 $k$ 个主方向张成的子空间内。拟合值 $\\hat{y}_{\\mathrm{PCR},k}$ 是将 $y$ 投影到由 $X$ 的前 $k$ 个主成分张成的子空间上的结果。该子空间与由 $X$ 的前 $k$ 个左奇异向量张成的子空间相同，这些左奇异向量是矩阵 $U_k$（$U$ 的前 $k$ 列）的列。\n\n$y$ 在 $\\mathrm{Col}(U_k)$ 上的投影由下式给出：\n$$\n\\hat{y}_{\\mathrm{PCR},k} = U_k U_k^\\top y\n$$\n具有 $k$ 个分量的 PCR 的误差平方和为：\n$$\n\\mathrm{SSE}_{\\mathrm{PCR}}(k) = \\|y - \\hat{y}_{\\mathrm{PCR},k}\\|_2^2\n$$\n我们对 $k \\in \\{0, 1, 2, 3\\}$ 计算该值。\n- 当 $k=0$ 时，子空间是平凡的（$\\{\\mathbf{0}\\}$），因此拟合为 $\\hat{y}_{\\mathrm{PCR},0} = \\mathbf{0}$，且 $\\mathrm{SSE}_{\\mathrm{PCR}}(0) = \\|y\\|_2^2$。\n- 当 $k=3$ 时，PCR 使用所有主成分。由所有 3 个主成分张成的子空间是 $X$ 的整个列空间。因此，PCR 投影与 OLS 投影相同：$\\hat{y}_{\\mathrm{PCR},3} = U U^\\top y = \\hat{y}_{\\mathrm{OLS}}$。因此，$\\mathrm{SSE}_{\\mathrm{PCR}}(3) = \\mathrm{SSE}_{\\mathrm{OLS}}$。\n\nPCR 的均方误差为 $\\mathrm{MSE}_{\\mathrm{PCR}}(k) = \\mathrm{SSE}_{\\mathrm{PCR}}(k) / n$。\n\n**4. 比率计算**\n\n最后一步是计算 $k \\in \\{0, 1, 2, 3\\}$ 的比率 $r_k$：\n$$\nr_k = \\frac{\\mathrm{MSE}_{\\mathrm{PCR}}(k)}{\\mathrm{MSE}_{\\mathrm{OLS}}} = \\frac{\\mathrm{SSE}_{\\mathrm{PCR}}(k) / n}{\\mathrm{SSE}_{\\mathrm{OLS}} / n} = \\frac{\\mathrm{SSE}_{\\mathrm{PCR}}(k)}{\\mathrm{SSE}_{\\mathrm{OLS}}}\n$$\n由于 OLS 为任何基于 $X$ 列的线性模型提供了可能的最小误差平方和，所以对所有 $k$ 必定有 $\\mathrm{SSE}_{\\mathrm{PCR}}(k) \\ge \\mathrm{SSE}_{\\mathrm{OLS}}$。这意味着 $r_k \\ge 1$。如已确定，$r_3 = 1$。然后按要求将计算出的值四舍五入到六位小数。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Constructs the specified dataset, computes OLS and PCR models,\n    and calculates the ratios of their in-sample mean squared errors.\n    \"\"\"\n    \n    # 1. Data Construction\n    n = 60\n    epsilon = 1e-6\n    delta = 1e-8\n    eta = 1e-4\n\n    i = np.arange(1, n + 1)\n    \n    # Auxiliary quantities\n    z = i - (n + 1) / 2.0\n    s_term = np.sin(2 * np.pi * i / n)\n    c_term = np.cos(2 * np.pi * i / n)\n\n    # Feature construction\n    x1 = z + epsilon * s_term\n    x2 = 2 * z + epsilon * c_term\n    x3 = 3 * x1 - 1.5 * x2 + delta * np.sin(4 * np.pi * i / n)\n\n    X = np.stack([x1, x2, x3], axis=1)\n\n    # Response construction\n    theta1, theta2, theta3 = 1.0, -0.5, 0.25\n    y = theta1 * x1 + theta2 * x2 + theta3 * x3 + eta * np.sin(6 * np.pi * i / n)\n\n    # 2. OLS Calculation\n    # numpy.linalg.lstsq returns a tuple; the second element is the sum of squared residuals\n    # if y is a 1D array, this is an array of shape (1,).\n    sse_ols = np.linalg.lstsq(X, y, rcond=None)[1][0]\n    \n    # 3. PCR Calculation\n    # Perform SVD of the design matrix X\n    U, _, _ = np.linalg.svd(X, full_matrices=False)\n    \n    k_values = [0, 1, 2, 3]\n    ratios = []\n\n    for k in k_values:\n        if k == 0:\n            # For k=0, the prediction is the zero vector\n            y_pcr = np.zeros(n)\n        else:\n            # Project y onto the subspace spanned by the first k left singular vectors\n            Uk = U[:, :k]\n            y_pcr = Uk @ (Uk.T @ y)\n        \n        # Calculate Sum of Squared Errors for PCR\n        sse_pcr_k = np.sum((y - y_pcr)**2)\n        \n        # 4. Ratio Calculation\n        # The 1/n term for MSE cancels out in the ratio\n        ratio = sse_pcr_k / sse_ols\n        ratios.append(ratio)\n        \n    # Format the final output string\n    # Each ratio is formatted to 6 decimal places.\n    output_str = f\"[{','.join([f'{r:.6f}' for r in ratios])}]\"\n    print(output_str)\n\nsolve()\n```", "id": "2383123"}]}