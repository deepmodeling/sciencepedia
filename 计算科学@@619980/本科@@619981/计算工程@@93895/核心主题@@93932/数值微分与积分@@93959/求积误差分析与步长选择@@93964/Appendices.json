{"hands_on_practices": [{"introduction": "辛普森法则的理论收敛阶为 $\\mathcal{O}(h^4)$，但这依赖于被积函数足够光滑的假设。本练习将通过一个仅有一阶连续导数（$C^1$）的函数 $\\lvert x-c \\rvert^{3/2}$，让你亲手验证当光滑性条件不满足时，高阶求积法则的收敛阶会如何“降级”。通过这个实践[@problem_id:2430715]，你将深刻理解函数光滑性在数值积分误差分析中的核心作用，并学会如何通过数值实验来估计实际的收敛阶。", "problem": "考虑一个在闭区间上具有内部尖点的函数的积分。设区间为 $[a,b]=[0,1]$，对于给定的参数 $c \\in (0,1)$，定义函数 $f:[0,1]\\to\\mathbb{R}$ 为\n$$\nf(x) = \\lvert x-c \\rvert^{\\alpha}, \\quad \\text{其中 } \\alpha=\\tfrac{3}{2}。\n$$\n在 $[0,1]$ 上，函数 $f$ 属于 $C^1$ 类，但不属于 $C^2$ 类。该积分的精确值由下式给出\n$$\nI(c) = \\int_{0}^{1} \\lvert x-c \\rvert^{\\alpha}\\,dx = \\frac{(c-0)^{\\alpha+1} + (1-c)^{\\alpha+1}}{\\alpha+1}.\n$$\n定义在具有 $N$ 个子区间（其中 $N$ 为偶数）的均匀网格上的复合辛普森求积法则如下。令 $h = \\frac{b-a}{N}$ 且 $x_j = a + jh$，其中 $j=0,1,\\dots,N$。$\\int_a^b f(x)\\,dx$ 的复合辛普森近似值 $S_N(f)$ 为\n$$\nS_N(f) = \\frac{h}{3}\\left[f(x_0) + 4\\sum_{j=1,\\,j\\ \\text{odd}}^{N-1} f(x_j) + 2\\sum_{j=2,\\,j\\ \\text{even}}^{N-2} f(x_j) + f(x_N)\\right].\n$$\n对于每个参数值 $c$，令绝对误差为\n$$\nE_N(c) = \\left\\lvert I(c) - S_N(f) \\right\\rvert.\n$$\n您必须编写一个完整、可运行的程序，对下列关于 $c$ 的每个测试用例，使用所提供的 $N$ 值序列，估计当 $h \\to 0$ 时满足 $E_N(c) = \\Theta(h^{p})$ 的观测收敛阶 $p$：\n- 测试用例 1：$c = 0.3$，\n- 测试用例 2：$c = \\tfrac{3}{8}$，\n- 测试用例 3：$c = 10^{-3}$。\n\n对于每个测试用例，您需要对偶数子区间数序列 $N \\in \\{8,16,32,64,128,256,512\\}$ 计算 $E_N(c)$，并根据这些值确定该测试用例的观测收敛阶 $p$ 的单个代表性估计值。您的程序的最终输出必须是单行文本，包含一个由三个浮点数组成的列表，这些浮点数按上述顺序列出了三个测试用例的估计阶数。每个浮点数都应四舍五入到三位小数。要求的最终输出格式为单行文本，使用逗号分隔并用方括号括起的列表，例如：\n$$\n[\\text{result}_1,\\text{result}_2,\\text{result}_3].\n$$\n本问题不涉及物理单位，不出现角度，也不需要百分比。输出中的所有数值都必须是浮点数。", "solution": "该问题陈述是适定的、数学上一致且有科学依据的。它提出了一个数值分析中的标准任务：当求积法则应用于光滑性有限的函数时，通过实验确定其收敛阶。我们将着手进行求解。\n\n复合辛普森法则是四阶的数值积分方法。对于 $[a,b]$ 上的积分，其绝对误差 $E_N$ 通常表现为 $E_N = \\mathcal{O}(h^4)$，其中 $h = (b-a)/N$ 是步长。仅当被积函数 $f$ 四次连续可微，即 $f \\in C^4([a,b])$ 时，才能保证这个收敛阶。\n\n所提供的函数为 $f(x) = \\lvert x-c \\rvert^{\\alpha}$，其中 $\\alpha=\\frac{3}{2}$。其导数为：\n$$ f'(x) = \\frac{3}{2} \\operatorname{sgn}(x-c) \\lvert x-c \\rvert^{1/2} $$\n$$ f''(x) = \\frac{3}{4} \\lvert x-c \\rvert^{-1/2} $$\n一阶导数 $f'(x)$ 在 $[0,1]$上是连续的，因此 $f \\in C^1([0,1])$。然而，二阶导数 $f''(x)$ 在 $x=c$ 处有一个奇点，这意味着 $f \\notin C^2([0,1])$。光滑性条件 $f \\in C^4([0,1])$ 的不满足导致收敛阶的降低。\n\n对于具有 $|x-c|^{\\beta}$ 形式代数奇点的被积函数，像辛普森法则这样的 $m$ 点牛顿-柯特斯法则的收敛阶 $p$ 可由非光滑函数的求积理论给出。收敛阶取决于奇点 $c$ 是否与求积节点重合：\n1.  如果对于序列中的任何 $N$，奇点 $c$ 都不是求积节点，则收敛阶为 $p = \\alpha+1$。对于 $\\alpha = \\frac{3}{2}$，这得到 $p = \\frac{3}{2}+1 = 2.5$。\n2.  如果对于所有足够大的 $N$，奇点 $c$ 都是求积节点，则误差得到改善，收敛阶变为 $p = \\alpha+2$。对于 $\\alpha = \\frac{3}{2}$，这得到 $p = \\frac{3}{2}+2 = 3.5$。\n\n我们基于该理论基础分析这些测试用例：\n- **测试用例 1 ($c = 0.3$)**：值 $c=0.3 = \\frac{3}{10}$ 不是一个二进有理数。网格点为 $x_j = j/N$，其中 $N$ 是 8 乘以 2 的幂。因此，$c$ 永远不会成为网格点。我们预期收敛阶为 $p \\approx 2.5$。\n- **测试用例 2 ($c = \\frac{3}{8}$)**：值 $c=0.375=\\frac{3}{8}$ 是一个二进有理数。对于 $N=8, 16, 32, \\dots$，点 $c$ 始终是网格点（例如，当 $N=8$ 时，$x_3 = \\frac{3}{8}$）。我们预期收敛阶为 $p \\approx 3.5$。\n- **测试用例 3 ($c = 10^{-3}$)**：值 $c=0.001=\\frac{1}{1000}$ 不是一个二进有理数。它不会是网格点。我们预期收敛阶为 $p \\approx 2.5$。\n\n为了数值上估计收敛阶 $p$，我们假设误差遵循关系 $E_N \\approx K h^p$（其中 $K$ 为某个常数），这可以重写为 $E_N \\approx K' N^{-p}$。对该表达式取自然对数，得到：\n$$ \\ln(E_N) \\approx \\ln(K') - p \\ln(N) $$\n该方程显示了 $\\ln(E_N)$ 和 $\\ln(N)$ 之间的线性关系，斜率为 $-p$。为了得到对 $p$ 的一个稳健估计，我们将对给定的序列 $N \\in \\{8, 16, 32, 64, 128, 256, 512\\}$ 计算误差 $E_N(c)$。然后我们对点集 $\\{(\\ln(N_i), \\ln(E_{N_i}))\\}_{i=1}^7$ 进行线性最小二乘回归。最佳拟合线的斜率 $m$ 计算如下：\n$$ m = \\frac{\\sum_{i=1}^7 (\\ln(N_i) - \\overline{\\ln(N)}) (\\ln(E_{N_i}) - \\overline{\\ln(E)})}{\\sum_{i=1}^7 (\\ln(N_i) - \\overline{\\ln(N)})^2} $$\n那么估计的收敛阶为 $p = -m$。\n\n对每个测试用例执行以下步骤：\n1.  定义常数 $\\alpha = \\frac{3}{2}$ 和测试用例参数 $c$。\n2.  计算精确积分 $I(c) = \\frac{c^{\\alpha+1} + (1-c)^{\\alpha+1}}{\\alpha+1}$。\n3.  对于序列中的每个 $N$，计算复合辛普森法则近似值 $S_N(f)$。\n4.  计算绝对误差 $E_N(c) = |I(c) - S_N(f)|$。\n5.  使用计算出的误差集和相应的 $N$ 值，通过线性回归确定对数-对数图的斜率 $m$。\n6.  收敛阶为 $p = -m$。最终值四舍五入到三位小数。\n整个过程被封装在所提供的程序中。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef estimate_convergence_order(c, N_values, alpha):\n    \"\"\"\n    Estimates the convergence order for Simpson's rule for the function\n    f(x) = |x-c|^alpha.\n\n    Args:\n        c (float): The location of the cusp in the function.\n        N_values (list or np.ndarray): A sequence of even integers representing\n                                      the number of subintervals.\n        alpha (float): The exponent in the function definition.\n\n    Returns:\n        float: The estimated order of convergence p.\n    \"\"\"\n    # 1. Define the function and its exact integral\n    f = lambda x: np.abs(x - c)**alpha\n    alpha_p1 = alpha + 1\n    I_exact = (c**alpha_p1 + (1 - c)**alpha_p1) / alpha_p1\n\n    errors = []\n    # 2. Loop through N values to calculate quadrature error\n    for N in N_values:\n        a, b = 0.0, 1.0\n        h = (b - a) / N\n        x = np.linspace(a, b, N + 1)\n        y = f(x)\n\n        # Composite Simpson's rule formula\n        # S_N = (h/3) * [f(x_0) + 4*sum(f(x_odd)) + 2*sum(f(x_even)) + f(x_N)]\n        S_N = (h / 3) * (y[0] + 4 * np.sum(y[1:-1:2]) + 2 * np.sum(y[2:-1:2]) + y[-1])\n        \n        # Absolute error\n        error = np.abs(I_exact - S_N)\n        errors.append(error)\n\n    # 3. Estimate convergence order using linear regression on log-log data\n    # The error model is E_N ≈ K * N^(-p).\n    # Taking logs: log(E_N) ≈ log(K) - p * log(N).\n    # This is a linear relationship between log(E_N) and log(N).\n    # The slope of this line is -p.\n    \n    log_N = np.log(np.array(N_values, dtype=float))\n    log_E = np.log(np.array(errors, dtype=float))\n\n    # Calculate the slope 'm' of the best-fit line for y = mx + b,\n    # where y = log_E and x = log_N.\n    # m = Cov(x, y) / Var(x)\n    x = log_N\n    y = log_E\n    x_mean = np.mean(x)\n    y_mean = np.mean(y)\n    \n    slope = np.sum((x - x_mean) * (y - y_mean)) / np.sum((x - x_mean)**2)\n    \n    # The convergence order p is the negative of the slope.\n    p = -slope\n    \n    return p\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for all test cases.\n    \"\"\"\n    # Define the parameters from the problem statement.\n    test_cases = [\n        0.3,\n        3.0 / 8.0,\n        1e-3\n    ]\n    \n    # Sequence of subintervals for convergence analysis\n    N_values = [8, 16, 32, 64, 128, 256, 512]\n    \n    # Exponent alpha\n    alpha = 1.5\n\n    results = []\n    for c in test_cases:\n        # Calculate the order p for the current test case\n        p_estimated = estimate_convergence_order(c, N_values, alpha)\n        \n        # Round the result to three decimal places\n        results.append(p_estimated)\n\n    # Format the final output as a string with 3 decimal places for each number\n    formatted_results = [f\"{res:.3f}\" for res in results]\n    \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "2430715"}, {"introduction": "在计算资源有限的情况下，如何最有效地分配计算量是数值积分中的一个核心问题。本练习[@problem_id:2430732]将让你在一个带有尖锐局部特征（高斯峰）的函数上，直接比较固定步长的辛普森法则和自适应梯形法则的精度。通过在相同函数求值次数的“预算”下进行对比，你将直观地体验到自适应方法如何将计算点智能地集中在误差最大的区域，从而在处理非均匀行为的函数时展现出巨大优势。", "problem": "给定一个在闭区间 $\\left[0,1\\right]$ 上的积分族，其形式为\n$$\nI(c,w) \\;=\\; \\int_{0}^{1} f_{c,w}(x)\\,dx,\\quad \\text{其中}\\quad f_{c,w}(x) \\;=\\; \\sin(\\omega x) \\;+\\; A \\exp\\!\\left(-\\frac{(x-c)^2}{2w^2}\\right).\n$$\n正弦函数中的所有角度都必须以弧度为单位进行解释。对于本问题中的所有计算，取 $\\omega=20$ 和 $A=5$。\n\n对于每个测试用例，您必须使用至多 $N$ 次 $f_{c,w}$ 的求值来计算 $I(c,w)$ 的两种数值近似，然后报告它们与一个参考值之间的绝对误差。该参考值的绝对误差小于 $10^{-12}$。所要求的两种近似方法是：\n\n1. 在均匀网格上使用固定步长的复合辛普森法则。该法则使用 $M=\\left\\lfloor\\frac{N-1}{2}\\right\\rfloor$ 个复合子区间，因此在节点 $x_i = a + i h$（其中 $i=0,1,\\dots,2M$，$a=0$, $b=1$, $h=\\frac{b-a}{2M}$）处进行恰好 $2M+1\\le N$ 次函数求值。该近似为\n$$\nS_N \\;=\\; \\frac{h}{3}\\left[f(x_0) + f(x_{2M}) + 4\\sum_{j=1}^{M} f(x_{2j-1}) + 2\\sum_{j=1}^{M-1} f(x_{2j})\\right].\n$$\n\n2. 在严格的函数求值预算下的自适应梯形法则。自适应估计量 $T_N$ 的定义如下。从单一区间 $\\left[a,b\\right]=\\left[0,1\\right]$ 开始。对于任意一个在其中点 $x_M=\\frac{x_L+x_R}{2}$ 处 $f(x_L)$、$f(x_R)$ 和 $f(x_M)$ 均已知的区间 $\\left[x_L,x_R\\right]$，定义粗略梯形近似\n$$\nT_{\\text{coarse}} \\;=\\; \\frac{h}{2}\\left(f(x_L)+f(x_R)\\right), \\quad h = x_R - x_L,\n$$\n以及使用二分法 $\\left[x_L,x_M\\right]\\cup\\left[x_M,x_R\\right]$ 的精细梯形近似，\n$$\nT_{\\text{refined}} \\;=\\; \\frac{h}{4}\\left(f(x_L) + 2 f(x_M) + f(x_R)\\right).\n$$\n使用局部误差估计\n$$\nE \\;=\\; \\frac{\\left|T_{\\text{refined}} - T_{\\text{coarse}}\\right|}{3}.\n$$\n通过在 $x=0$、$x=1$ 和 $x=\\frac{1}{2}$（即 $x_L=0$、$x_R=1$ 和 $x_M=\\frac{1}{2}$）处求 $f$ 的值进行初始化，这会使用 3 次求值。维护当前不相交的叶子区间集合，每个叶子区间由其三元组 $\\left(x_L,x_M,x_R\\right)$（其中 $f(x_L)$、$f(x_M)$ 和 $f(x_R)$ 已知）及相关的 $T_{\\text{refined}}$ 和 $E$ 表示。在每一步中，如果增加两次新的函数求值不会超过预算 $N$，则选择具有最大 $E$ 值的叶子区间，将其二分为两个半区间 $\\left[x_L,x_M\\right]$ 和 $\\left[x_M,x_R\\right]$，在每个新的半区间中点 $\\frac{x_L+x_M}{2}$ 和 $\\frac{x_M+x_R}{2}$ 处求 $f$ 的值（这将使不同求值点的计数增加 2），计算它们的 $T_{\\text{refined}}$ 和 $E$，并在叶子集合中用这两个子区间替换父区间。当下次二分将需要超过 $N$ 次 $f$ 的总不同求值次数时停止。将最终的自适应估计 $T_N$ 定义为所有当前叶子区间的 $T_{\\text{refined}}$ 之和。在计算预算时，仅计算不同的求值点 $x$；共享的端点不重新求值。\n\n对于下面的每个测试用例，计算 $I(c,w)$ 的一个高精度参考值 $I^\\star(c,w)$，其绝对误差小于 $10^{-12}$，然后计算绝对误差\n$$\nE_S \\;=\\; \\left|S_N - I^\\star(c,w)\\right|, \\qquad E_T \\;=\\; \\left|T_N - I^\\star(c,w)\\right|.\n$$\n\n测试套件（四个用例）：\n- 用例 A: $N=33$, $c=0.5$, $w=0.02$.\n- 用例 B: $N=33$, $c=0.9$, $w=0.01$.\n- 用例 C: $N=65$, $c=0.3$, $w=0.005$.\n- 用例 D: $N=129$, $c=0.5$, $w=0.05$.\n\n您的程序必须输出单行结果，格式为逗号分隔的列表之列表，顺序为 $\\left[\\text{用例 A},\\text{用例 B},\\text{用例 C},\\text{用例 D}\\right]$，其中每个内部列表的形式为 $\\left[E_S,E_T\\right]$。每个浮点数必须以科学计数法格式化，小数点后恰好有 $10$ 位数字（例如，$1.2345678900e-03$）。也就是说，输出必须具有以下形式\n$$\n\\left[ [E_{S,A},E_{T,A}], [E_{S,B},E_{T,B}], [E_{S,C},E_{T,C}], [E_{S,D},E_{T,D}] \\right],\n$$\n打印为单行，逗号后无空格。", "solution": "我们对两种求积方法和受求值预算约束的自适应性进行形式化，然后从数值积分和误差分析的第一性原理推导出如何实现它们。\n\n被积函数为\n$$\nf_{c,w}(x) \\;=\\; \\sin(\\omega x) \\;+\\; A \\exp\\!\\left(-\\frac{(x-c)^2}{2w^2}\\right),\n$$\n其中 $\\omega=20$，$A=5$，定义域为 $x\\in[0,1]$。正弦函数使用弧度。积分为\n$$\nI(c,w) \\;=\\; \\int_{0}^{1} f_{c,w}(x)\\,dx.\n$$\n\n固定步长的复合辛普森法则。对于 $[a,b]=[0,1]$ 上的一个具有 $2M$ 个子区间（即 $2M+1$ 个节点）的均匀网格，根据 Newton–Cotes 公式的定义，复合辛普森法则是\n$$\nS_N \\;=\\; \\frac{h}{3}\\left[f(x_0) + f(x_{2M}) + 4\\sum_{j=1}^{M} f(x_{2j-1}) + 2\\sum_{j=1}^{M-1} f(x_{2j})\\right],\n$$\n其中 $h=\\frac{b-a}{2M}$ 且 $x_i=a+ih$, $i=0,1,\\dots,2M$。预算约束是我们可以使用至多 $N$ 次不同的函数求值。复合辛普森法则恰好需要 $2M+1$ 次求值。因此我们必须选择\n$$\nM \\;=\\; \\left\\lfloor \\frac{N-1}{2} \\right\\rfloor,\n$$\n这保证了求值次数 $2M+1\\le N$。对于足够光滑的函数 $f$，辛普森法则的经典截断误差为 $\\mathcal{O}(h^4)$，但这里我们不预先假设常数；我们计算近似值，并相对于一个高精度参考值来度量实际的绝对误差。\n\n严格预算下的自适应梯形法则。在区间 $\\left[x_L,x_R\\right]$ 上的梯形法则近似为\n$$\nT_{\\text{coarse}} \\;=\\; \\frac{h}{2}\\left(f(x_L)+f(x_R)\\right), \\quad h=x_R-x_L.\n$$\n如果我们在中点 $x_M=\\frac{x_L+x_R}{2}$ 处进行二分，并对两个半区间应用梯形法则，我们得到\n$$\nT_{\\text{refined}} \\;=\\; \\frac{h}{4}\\left(f(x_L)+2f(x_M)+f(x_R)\\right).\n$$\n对于足够光滑的函数 $f$，单个区间上的梯形法则误差的量级为 $\\mathcal{O}(h^2)$。假设存在一个渐进误差展开 $E(h)\\approx K h^2$（其中 $K$ 是某个局部常数），Richardson 外推法可以推导出\n$$\nE(h) \\;\\approx\\; \\frac{T_{\\text{refined}} - T_{\\text{coarse}}}{3},\n$$\n因此一个局部误差估计是\n$$\nE \\;=\\; \\frac{\\left|T_{\\text{refined}}-T_{\\text{coarse}}\\right|}{3}.\n$$\n自适应加密策略的目标是在该局部误差估计最大的地方分配更小的步长。我们从单一区间 $\\left[0,1\\right]$ 开始，在 $x=0$, $x=\\frac{1}{2}$ 和 $x=1$ 处求 $f$ 的值，消耗 3 次求值。对于当前不相交的叶子区间集合，每个叶子由其端点和中点 $\\left(x_L,x_M,x_R\\right)$（其中 $f(x_L)$、$f(x_M)$、$f(x_R)$ 已知）以及相关的 $T_{\\text{refined}}$ 和 $E$ 来表示。在每个加密步骤中，如果增加两次新的不同函数求值能够使总求值次数保持在 $N$ 以内，我们就选择具有最大 $E$ 值的叶子区间并将其二分为两个半区间。这将创建两个子区间：\n- 左子区间 $\\left[x_L,x_M\\right]$，新中点为 $x_{LM}=\\frac{x_L+x_M}{2}$，\n- 右子区间 $\\left[x_M,x_R\\right]$，新中点为 $x_{MR}=\\frac{x_M+x_R}{2}$。\n我们在这两个新中点处求 $f$ 的值（使不同采样点的计数增加 2），计算每个子区间的 $T_{\\text{refined}}$ 和 $E$，并更新叶子集合。当前的自适应积分估计是所有叶子区间的 $T_{\\text{refined}}$ 之和。这种贪婪加密过程会一直持续，直到下一次分裂将需要超过 $N$ 次总的不同求值。因为我们从 2 个端点和 1 个中点（共 3 个）开始，每次二分增加 2 个新中点，所以在 $k$ 次二分后，不同求值点的数量为 $3+2k$，通过构造，这个数量始终 $\\le N$。这个过程将子区间集中在局部误差最大的区域，对于本问题而言，这对应于具有大局部曲率的区域，例如 $x=c$ 附近的局部高斯特征或快速振荡的区域。\n\n参考积分和绝对误差。我们使用一个稳健的高阶求积程序来计算 $I(c,w)$ 的一个高精度参考值 $I^\\star(c,w)$，使其绝对误差小于 $10^{-12}$，这样所报告的绝对误差\n$$\nE_S \\;=\\; \\left|S_N - I^\\star(c,w)\\right|,\\qquad E_T \\;=\\; \\left|T_N - I^\\star(c,w)\\right|\n$$\n主要由被比较的两种方法的离散化效应决定，而不是由参考值决定。\n\n测试套件和预期结果。四个用例测试了不同的情况：\n- 用例 A: $N=33$, $c=0.5$, $w=0.02$（局部特征居中，中等狭窄）。\n- 用例 B: $N=33$, $c=0.9$, $w=0.01$（局部特征靠近边界）。\n- 用例 C: $N=65$, $c=0.3$, $w=0.005$（非常狭窄的特征，中等预算）。\n- 用例 D: $N=129$, $c=0.5$, $w=0.05$（较宽的特征，较大预算）。\n\n我们从第一性原理预期，固定步长的复合辛普森法则在均匀步长 $h$ 上是四阶的，当特征相对于网格间距不是太窄且函数在各个单元上保持足够光滑时，它将表现得非常好。然而，当特征是急剧局部的（小 $w$），特别是当预算 $N$ 不大时，均匀网格可能无法充分解析该特征，导致更大的误差。自适应梯形法则局部仅为二阶，但由于误差估计器将局部加密驱动到最具挑战性的区域，它可以通过将求值集中在局部高斯峰内部和周围（以及可能出现不对称性的边界附近），在相同的求值预算下胜过固定步长的辛普森法则。程序报告的结果将通过为每个测试用例计算的 $E_S$ 和 $E_T$ 来量化这些效应。最终输出格式为单行：一个由四个内部列表组成的逗号分隔列表，每个内部列表为 $\\left[E_S,E_T\\right]$，其中每个数字都以科学计数法格式化，小数点后恰好有 10 位数字。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom math import sin, exp\nfrom scipy.integrate import quad\n\ndef f_cw(x, c, w, omega=20.0, A=5.0):\n    # Sine uses radians; exponential is the localized feature.\n    return sin(omega * x) + A * exp(-((x - c) ** 2) / (2.0 * w ** 2))\n\ndef reference_integral(c, w):\n    # High-accuracy reference integral with stringent tolerances.\n    val, err = quad(lambda x: f_cw(x, c, w), 0.0, 1.0, epsabs=1e-13, epsrel=1e-13, limit=500)\n    return val\n\ndef composite_simpson_budget(c, w, N, a=0.0, b=1.0):\n    # Use M = floor((N-1)/2) composite Simpson subintervals (2M+1 nodes <= N).\n    M = int((N - 1) // 2)\n    if M < 1:\n        # Fallback: with too few points, use simple trapezoid on [a,b].\n        fa = f_cw(a, c, w)\n        fb = f_cw(b, c, w)\n        return 0.5 * (b - a) * (fa + fb)\n    h = (b - a) / (2 * M)\n    xs = a + h * np.arange(0, 2 * M + 1, dtype=float)\n    fs = np.array([f_cw(x, c, w) for x in xs])\n    s0 = fs[0] + fs[-1]\n    sodds = fs[1:-1:2].sum()  # odd indices: 1,3,...,2M-1\n    sevens = fs[2:-1:2].sum() if (2 * M - 1) >= 2 else 0.0  # even indices: 2,4,...,2M-2\n    return (h / 3.0) * (s0 + 4.0 * sodds + 2.0 * sevens)\n\nclass Interval:\n    __slots__ = (\"xL\",\"xM\",\"xR\",\"fL\",\"fM\",\"fR\",\"Tref\",\"E\",\"id\")\n    def __init__(self, xL, xM, xR, fL, fM, fR, Tref, E, idnum):\n        self.xL = xL; self.xM = xM; self.xR = xR\n        self.fL = fL; self.fM = fM; self.fR = fR\n        self.Tref = Tref; self.E = E; self.id = idnum\n\ndef adaptive_trapezoid_budget(c, w, N, a=0.0, b=1.0):\n    # Greedy adaptive trapezoidal rule under a strict evaluation budget of at most N distinct samples.\n    # Implementation matches the precise specification in the problem statement.\n    import heapq\n\n    # Map of distinct points to function values to avoid double counting.\n    values = {}\n\n    def eval_point(x):\n        # Evaluate f(x) if not already. Count distinct points only.\n        if x in values:\n            return values[x]\n        if len(values) >= N:\n            # Budget exhausted; should not happen if caller checks before adding new points.\n            return values.get(x, None)\n        fx = f_cw(x, c, w)\n        values[x] = fx\n        return fx\n\n    # Initialize with endpoints and midpoint\n    xL = a; xR = b; xM = 0.5 * (xL + xR)\n    fL = eval_point(xL); fR = eval_point(xR); fM = eval_point(xM)\n\n    # Compute initial refined trapezoid and error estimate\n    h = xR - xL\n    Tcoarse = 0.5 * h * (fL + fR)\n    Tref = 0.25 * h * (fL + 2.0 * fM + fR)\n    E = abs(Tref - Tcoarse) / 3.0\n\n    # Priority queue of intervals by negative error (max-heap behavior)\n    heap = []\n    counter = 0  # to break ties\n    first = Interval(xL, xM, xR, fL, fM, fR, Tref, E, counter)\n    heapq.heappush(heap, (-E, counter, first))\n    counter += 1\n\n    # Sum of refined trapezoid contributions across leaf intervals\n    integral_sum = Tref\n\n    # Current number of distinct evaluations is len(values)\n    # Each split requires adding exactly 2 new midpoints\n    while len(values) + 2 <= N and heap:\n        # Pop the interval with largest error\n        _, _, itv = heapq.heappop(heap)\n\n        # Prepare two children by bisecting\n        xL = itv.xL; xM = itv.xM; xR = itv.xR\n        fL = itv.fL; fM = itv.fM; fR = itv.fR\n\n        # Left child [xL, xM]\n        xLM = 0.5 * (xL + xM)\n        fLM = eval_point(xLM)\n        hL = xM - xL\n        Tcoarse_L = 0.5 * hL * (fL + fM)\n        Tref_L = 0.25 * hL * (fL + 2.0 * fLM + fM)\n        E_L = abs(Tref_L - Tcoarse_L) / 3.0\n\n        # Right child [xM, xR]\n        xMR = 0.5 * (xM + xR)\n        fMR = eval_point(xMR)\n        hR = xR - xM\n        Tcoarse_R = 0.5 * hR * (fM + fR)\n        Tref_R = 0.25 * hR * (fM + 2.0 * fMR + fR)\n        E_R = abs(Tref_R - Tcoarse_R) / 3.0\n\n        # Update integral sum: remove parent refined, add children refined\n        integral_sum += -itv.Tref + (Tref_L + Tref_R)\n\n        # Push children\n        left = Interval(xL, xLM, xM, fL, fLM, fM, Tref_L, E_L, counter); counter += 1\n        right = Interval(xM, xMR, xR, fM, fMR, fR, Tref_R, E_R, counter); counter += 1\n        heapq.heappush(heap, (-E_L, left.id, left))\n        heapq.heappush(heap, (-E_R, right.id, right))\n\n    return integral_sum\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each test case: (N, c, w)\n    test_cases = [\n        (33, 0.5, 0.02),   # Case A\n        (33, 0.9, 0.01),   # Case B\n        (65, 0.3, 0.005),  # Case C\n        (129, 0.5, 0.05),  # Case D\n    ]\n\n    results = []\n    for N, c, w in test_cases:\n        I_star = reference_integral(c, w)\n        S_N = composite_simpson_budget(c, w, N, a=0.0, b=1.0)\n        T_N = adaptive_trapezoid_budget(c, w, N, a=0.0, b=1.0)\n        err_S = abs(S_N - I_star)\n        err_T = abs(T_N - I_star)\n        results.append((err_S, err_T))\n\n    # Format: a single line with a comma-separated list of lists, each inner list [Es,Ea],\n    # with each float in scientific notation with exactly 10 digits after the decimal point.\n    formatted = \"[\" + \",\".join(f\"[{err_s:.10e},{err_t:.10e}]\" for (err_s, err_t) in results) + \"]\"\n    print(formatted)\n\nsolve()\n```", "id": "2430732"}, {"introduction": "自适应方法功能强大，但它是否总是最佳选择？对于像 $e^{-x^2}$ 这样无限光滑的解析函数，高斯求积等非自适应高阶方法的“指数收敛”特性可能更具效率。这个练习[@problem_id:2430710]要求你通过编程找出高斯-勒让德求积比自适应辛普森法则更高效（即函数求值次数更少）的精度“交叉点”。这个对比分析将帮助你建立一个更全面的认知：选择数值积分方法需要对问题本身的特性进行深入分析，不存在一招鲜的通用方案。", "problem": "开发一个完整的程序，该程序对于一个绝对公差族 $\\{\\varepsilon\\}$，确定族中满足以下条件的最小公差值：在该公差下，使用高斯-勒让德（Gauss–Legendre）高斯积分法在区间 $[0,1]$ 上近似积分 $f(x)=e^{-x^2}$ 时，所需的被积函数求值次数严格少于自适应辛普森（Simpson）法则，且绝对误差不超过 $\\varepsilon$。我们关注的积分是\n$$\nI=\\int_{0}^{1} e^{-x^2}\\,dx.\n$$\n为解决此问题，必须使用以下定义来量化效率和停止条件。\n\n1. 高斯积分的定义和成本模型：\n   - 对于每个 $n\\in\\mathbb{N}$，令 $Q_n$ 为 $[0,1]$ 上的 $n$ 点高斯-勒让德积分，它是通过从 $[-1,1]$ 到 $[0,1]$ 的标准仿射变量变换得到的。其绝对误差为 $E_n=\\lvert I-Q_n\\rvert$。\n   - 对于给定的 $\\varepsilon>0$，将 $n_{\\mathrm{G}}(\\varepsilon)$ 定义为满足 $E_n\\le \\varepsilon$ 的最小 $n$。在公差 $\\varepsilon$ 下，该积分所需的函数求值次数为 $N_{\\mathrm{G}}(\\varepsilon)=n_{\\mathrm{G}}(\\varepsilon)$。\n\n2. 自适应辛普森法则的定义和成本模型：\n   - 对于区间 $[a,b]$，定义辛普森近似\n     $$\n     S([a,b])=\\frac{b-a}{6}\\Big(f(a)+4f\\Big(\\frac{a+b}{2}\\Big)+f(b)\\Big).\n     $$\n   - 对于中点为 $m=(a+b)/2$ 的区间 $[a,b]$，类似地定义子区间上的辛普森近似 $S([a,m])$ 和 $S([m,b])$，以及经典的局部误差估计量\n     $$\n     \\eta([a,b])=\\frac{1}{15}\\,\\big\\lvert S([a,m])+S([m,b]) - S([a,b])\\big\\rvert.\n     $$\n   - 自适应辛普森策略的定义是：当 $\\eta([a,b])$ 超过其局部公差时，递归地对分区间 $[a,b]$；当划分中的所有子区间都满足其局部公差，从而使全局绝对误差最多为 $\\varepsilon$ 时，终止递归。必须分配局部公差，以使最终划分中所有局部公差之和以 $\\varepsilon$ 为界，例如，在每次对分时将公差均分。\n   - 成本 $N_{\\mathrm{S}}(\\varepsilon)$ 是此自适应辛普森法则为保证绝对误差不超过 $\\varepsilon$ 而在 $[0,1]$ 上对 $f$ 进行的独立求值总次数。如果一个点被重复访问，其求值不被重复计数。\n\n对于给定有限集合中的每个公差值 $\\varepsilon$，效率比较定义如下：如果 $N_{\\mathrm{G}}(\\varepsilon) < N_{\\mathrm{S}}(\\varepsilon)$，则认为高斯积分效率更高。对于每个给定的公差族，将临界值定义为该族中（相对于 $\\mathbb{R}$ 上的常规全序关系）满足高斯积分在此意义下效率更高的最小 $\\varepsilon$。如果族内不存在这样的 $\\varepsilon$，则临界值未定义，必须报告为特殊的浮点数值 $\\mathrm{NaN}$。\n\n您的程序必须为以下公差族的测试套件计算这些临界值：\n- 测试用例 1：$\\{\\varepsilon\\}=\\{10^{-2},\\,10^{-3},\\,10^{-4},\\,10^{-5}\\}$。\n- 测试用例 2：$\\{\\varepsilon\\}=\\{10^{-6},\\,10^{-7},\\,10^{-8},\\,10^{-9},\\,10^{-10},\\,10^{-11},\\,10^{-12}\\}$。\n- 测试用例 3：$\\{\\varepsilon\\}=\\{5\\times10^{-4},\\,2\\times10^{-6},\\,10^{-8},\\,5\\times10^{-10}\\}$。\n\n两种方法的绝对误差条件都必须与通过数值计算得到的 $I$ 的高精度参考值进行比较。成本的度量纯粹是 $f(x)$ 的求值次数，每次求值的单位成本为1，与 $x$ 无关。\n\n最终输出格式：您的程序应生成单行输出，其中包含一个方括号内的逗号分隔列表，结果按上述测试用例的顺序列出，例如 $[\\text{result}_1,\\text{result}_2,\\text{result}_3]$。每个 $\\text{result}_k$ 必须是一个浮点数，等于对应族中满足 $N_{\\mathrm{G}}(\\varepsilon) < N_{\\mathrm{S}}(\\varepsilon)$ 的最小 $\\varepsilon$，或者如果族中不存在这样的 $\\varepsilon$，则为 $\\mathrm{NaN}$。", "solution": "所述问题已经过验证，并被认定为有效。它在科学上基于数值分析原理，特别是数值积分。该问题是适定的、客观的，并为获得唯一解提供了完整且一致的定义和约束集。任务是比较两种标准数值积分方法——固定阶数的高斯-勒让德积分和自适应辛普森法则——在区间 $[0,1]$ 上近似积分 $f(x) = e^{-x^2}$ 时的计算效率。效率被严格定义为被积函数 $f(x)$ 的求值次数。\n\n待近似的积分为\n$$\nI = \\int_{0}^{1} e^{-x^2} \\, dx\n$$\n该积分的精确值可以用误差函数 $\\mathrm{erf}(x)$ 表示为\n$$\nI = \\frac{\\sqrt{\\pi}}{2} \\mathrm{erf}(1)\n$$\n使用标准库函数计算 $I$ 的一个高精度数值，并将其作为参考值，用以衡量积分法则的绝对误差。该参考值约为 $I \\approx 0.746824132812427$。\n\n求解方法如下。对于给定测试族中的每个指定公差 $\\varepsilon$，我们计算成本 $N_{\\mathrm{G}}(\\varepsilon)$ 和 $N_{\\mathrm{S}}(\\varepsilon)$。然后，我们在每个族中找出满足条件 $N_{\\mathrm{G}}(\\varepsilon) < N_{\\mathrm{S}}(\\varepsilon)$ 的最小 $\\varepsilon$。\n\n**1. 高斯-勒让德积分的成本：$N_{\\mathrm{G}}(\\varepsilon)$**\n\n$n$ 点高斯-勒让德积分法则 $Q_n$ 可用于近似该积分。该法则定义在标准区间 $[-1,1]$ 上。为了将其应用于区间 $[0,1]$，我们使用仿射变换 $x(t) = \\frac{1}{2}(t+1)$，其中 $t \\in [-1,1]$。积分为：\n$$\nI = \\int_{0}^{1} f(x) \\, dx = \\int_{-1}^{1} f\\left(\\frac{t+1}{2}\\right) \\frac{1}{2} \\, dt\n$$\n设 $\\{t_i^*, w_i^*\\}_{i=1}^n$ 为 $[-1,1]$ 上 $n$ 点高斯-勒让德积分的节点和权重。近似值 $Q_n$ 为：\n$$\nQ_n = \\sum_{i=1}^{n} \\frac{w_i^*}{2} f\\left(\\frac{t_i^*+1}{2}\\right)\n$$\n就函数求值而言，成本就是 $n$。根据问题定义，$N_{\\mathrm{G}}(\\varepsilon)$ 是使绝对误差 $E_n = |I - Q_n|$ 不大于 $\\varepsilon$ 的最小整数 $n$。\n为求给定 $\\varepsilon$ 下的 $N_{\\mathrm{G}}(\\varepsilon)$，我们计算 $n=1, 2, 3, \\dots$ 时的 $Q_n$，并在第一个满足误差界的 $n$ 处停止。对于像 $e^{-x^2}$ 这样的解析函数，高斯积分的收敛是超代数的（近似指数级的，误差 $E_n \\sim e^{-cn}$，其中 $c>0$ 为某个常数），因此随着 $\\varepsilon$ 的减小，$N_{\\mathrm{G}}(\\varepsilon)$ 增长非常缓慢，大约为 $N_{\\mathrm{G}}(\\varepsilon) \\propto \\ln(1/\\varepsilon)$。\n\n**2. 自适应辛普森法则的成本：$N_{\\mathrm{S}}(\\varepsilon)$**\n\n自适应辛普森方法构建了被积函数的分段二次近似。该方法的核心是通过局部误差估计来控制积分区间的递归对分。\n对于区间 $[a,b]$，辛普森法则的近似值为 $S([a,b])$。算法将其与通过对两个半区间求和得到的更精确的近似值 $S_2 = S([a,m]) + S([m,b])$ 进行比较，其中 $m=(a+b)/2$。局部误差估计为：\n$$\n\\eta([a,b]) = \\frac{1}{15} |S_2 - S([a,b])|\n$$\n在 $[0,1]$ 上针对全局公差 $\\varepsilon$ 的递归过程如下：\n- 从区间 $[0,1]$ 和公差 $\\varepsilon$ 开始。\n- 对于当前局部公差为 $\\tau$ 的区间 $[a,b]$，计算误差估计 $\\eta([a,b])$。此步骤需要在 5 个点上求值 $f(x)$：$a, b, m, (a+m)/2, (m+b)/2$。\n- 如果 $\\eta([a,b]) \\le \\tau$，则此区间的递归终止。$[a,b]$ 上的积分由 $S_2$ 近似。\n- 如果 $\\eta([a,b]) > \\tau$，则将区间 $[a,b]$ 对分为 $[a,m]$ 和 $[m,b]$，并对每个子区间递归调用该过程，局部公差均分为 $\\tau/2$。\n总积分是所有终点子区间结果的总和。\n成本 $N_{\\mathrm{S}}(\\varepsilon)$ 是在整个递归过程中对 $f(x)$ 进行求值的独立点的总数。这是通过使用记忆化技术来管理的，其中计算出的函数值被存储和重用，以避免冗余计算并正确统计唯一的求值点。对于一个光滑函数，所需区间的数量通常按 $\\varepsilon^{-1/4}$ 的比例缩放，导致成本为 $N_{\\mathrm{S}}(\\varepsilon) \\propto \\varepsilon^{-1/4}$。\n\n**3. 临界值的确定**\n\n对于每个给定的公差族 $\\{\\varepsilon_j\\}$，首先按升序对集合进行排序。然后，对于这个排序序列中的每个 $\\varepsilon_j$，我们计算 $N_{\\mathrm{G}}(\\varepsilon_j)$ 和 $N_{\\mathrm{S}}(\\varepsilon_j)$。第一个满足 $N_{\\mathrm{G}}(\\varepsilon_j) < N_{\\mathrm{S}}(\\varepsilon_j)$ 的值 $\\varepsilon_j$ 就是该族所需的临界值。如果族中没有任何公差满足此条件，则将临界值报告为非数字（$\\mathrm{NaN}$）。$N_{\\mathrm{G}}$ 的对数增长与 $N_{\\mathrm{S}}$ 的多项式增长的比较表明，随着所需精度的提高（即 $\\varepsilon$ 变小），高斯积分将变得越来越有优势。", "answer": "```python\nimport numpy as np\nfrom scipy.special import erf, roots_legendre\nimport sys\n\n# It is a good practice to increase recursion limit for deep adaptive quadratures,\n# although for this specific problem and tolerances, it is not strictly necessary.\nsys.setrecursionlimit(2000)\n\nclass AdaptiveSimpson:\n    \"\"\"\n    Implements the adaptive Simpson's rule as specified in the problem.\n    The cost is the number of distinct function evaluations.\n    \"\"\"\n    def __init__(self, f):\n        self.f = f\n        self.memo = {}\n\n    def _eval_f(self, x):\n        \"\"\"Evaluates f at x, using memoization to count distinct points.\"\"\"\n        if x not in self.memo:\n            self.memo[x] = self.f(x)\n        return self.memo[x]\n\n    def _recursive_step(self, a, b, tol):\n        \"\"\"A single recursive step of the adaptive Simpson's algorithm.\"\"\"\n        m = (a + b) / 2\n        \n        # S1 is the Simpson's rule over the whole interval [a,b]\n        fa = self._eval_f(a)\n        fb = self._eval_f(b)\n        fm = self._eval_f(m)\n        S1 = (b-a)/6 * (fa + 4*fm + fb)\n\n        # S2 is the sum of Simpson's rule on the two half-intervals\n        ml = (a + m) / 2\n        mr = (m + b) / 2\n        fml = self._eval_f(ml)\n        fmr = self._eval_f(mr)\n        S2 = (m-a)/6 * (fa + 4*fml + fm) + (b-m)/6 * (fm + 4*fmr + fb)\n\n        # Local error estimate as defined in the problem\n        error_estimate = abs(S2 - S1) / 15.0\n\n        if error_estimate <= tol:\n            return S2  # The more accurate sum is returned\n        else:\n            # Recurse on sub-intervals with halved tolerance\n            return self._recursive_step(a, m, tol / 2.0) + self._recursive_step(m, b, tol / 2.0)\n\n    def integrate_cost(self, a, b, epsilon):\n        \"\"\"\n        Calculates the integral to a given tolerance and returns the number of\n        function evaluations.\n        \"\"\"\n        self.memo.clear()\n        self._recursive_step(a, b, epsilon)\n        return len(self.memo)\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for the given test cases.\n    \"\"\"\n    integrand = lambda x: np.exp(-x**2)\n    # High-precision reference value for the integral\n    I_ref = np.sqrt(np.pi) / 2.0 * erf(1)\n\n    # Pre-compute Gauss-Legendre quadrature errors for various numbers of points 'n'\n    # This avoids re-computation and speeds up the search for n_G(epsilon).\n    # n up to 30 is sufficient for tolerances down to machine epsilon for this function.\n    max_n_gauss = 30\n    gauss_errors = {}\n    for n in range(1, max_n_gauss + 1):\n        nodes, weights = roots_legendre(n)\n        # Transform nodes and weights from [-1, 1] to [0, 1]\n        t = 0.5 * (nodes + 1.0)\n        w = 0.5 * weights\n        integral_approx = np.sum(w * integrand(t))\n        error = abs(I_ref - integral_approx)\n        gauss_errors[n] = error\n\n    def get_NG(epsilon):\n        \"\"\"\n        Finds the smallest n (cost) for Gauss-Legendre quadrature to achieve\n        the given tolerance, using the pre-computed error table.\n        \"\"\"\n        for n in sorted(gauss_errors.keys()):\n            if gauss_errors[n] <= epsilon:\n                return n\n        # Should not be reached if max_n_gauss is large enough\n        raise ValueError(\"max_n_gauss is too small for the given tolerance.\")\n\n    # Initialize the adaptive Simpson solver\n    simpson_solver = AdaptiveSimpson(integrand)\n\n    def get_NS(epsilon):\n        \"\"\"\n        Calculates the cost (number of function evaluations) for the adaptive\n        Simpson's rule to achieve the given tolerance.\n        \"\"\"\n        return simpson_solver.integrate_cost(0, 1, epsilon)\n\n    test_cases = [\n        [1e-2, 1e-3, 1e-4, 1e-5],\n        [1e-6, 1e-7, 1e-8, 1e-9, 1e-10, 1e-11, 1e-12],\n        [5e-4, 2e-6, 1e-8, 5e-10],\n    ]\n\n    final_results = []\n    for case in test_cases:\n        # Sort tolerances ascending to find the smallest epsilon that satisfies the condition\n        tolerances = sorted(case)\n        crossover_value = np.nan\n        \n        for eps in tolerances:\n            N_G = get_NG(eps)\n            N_S = get_NS(eps)\n            \n            if N_G < N_S:\n                crossover_value = eps\n                break  # Found the smallest epsilon in the family\n\n        final_results.append(crossover_value)\n    \n    # Format the final output string as per requirements. `str(np.nan)` gives 'nan'.\n    print(f\"[{','.join(map(str, final_results))}]\")\n\nsolve()\n```", "id": "2430710"}]}