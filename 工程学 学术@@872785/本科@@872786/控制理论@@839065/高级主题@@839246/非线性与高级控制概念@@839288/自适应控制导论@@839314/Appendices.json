{"hands_on_practices": [{"introduction": "自适应控制的核心思想之一是利用测量误差来在线调整控制器参数。本练习将通过一个简单的热力学模型，带你亲手实践最基本的参数估计算法——基于梯度的更新法则。通过计算智能恒温器如何逐步学习房间的热阻，你将直观地理解自适应机制是如何工作的 [@problem_id:1582140]。", "problem": "一个智能恒温器使用一个简化模型来关联室内与室外环境之间的稳态温差 $\\Delta T$（单位为开尔文，K）与加热器提供的功率 $P$（单位为瓦特，W）。该模型由线性关系 $\\Delta T = R_{th} P$ 给出，其中 $R_{th}$ 是房间的有效热阻，是恒温器需要学习的一个参数。\n\n为了估计这个参数，恒温器的控制算法在离散时间步 $k=0, 1, 2, \\dots$ 更新一个估计值 $\\hat{R}_{th,k}$。该更新基于梯度下降法，旨在最小化当前时间步的平方预测误差，该误差被形式化为成本函数 $J_k = \\frac{1}{2} e_k^2$，其中 $e_k = \\Delta T_k - \\hat{R}_{th,k} P_k$。估计值的更新规则由下式给出：\n$$ \\hat{R}_{th, k+1} = \\hat{R}_{th, k} - \\mu \\left. \\frac{\\partial J_k}{\\partial \\hat{R}_{th}} \\right|_{\\hat{R}_{th} = \\hat{R}_{th,k}} $$\n其中 $\\mu$ 是一个恒定的正常数学习率。\n\n给定以下实验数据和参数：\n- 热阻的初始估计值为 $\\hat{R}_{th,0} = 0.0500 \\text{ K/W}$。\n- 学习率为 $\\mu = 5.00 \\times 10^{-5} \\text{ W}^{-2}$。\n- 在时间步 $k=0$ 时，加热器功率为 $P_0 = 100.0 \\text{ W}$，测得的温差为 $\\Delta T_0 = 6.00 \\text{ K}$。\n- 在时间步 $k=1$ 时，加热器功率为 $P_1 = 150.0 \\text{ W}$，测得的温差为 $\\Delta T_1 = 9.00 \\text{ K}$。\n\n计算估计值 $\\hat{R}_{th,2}$ 的值。以 K/W 为单位表示答案，并四舍五入到三位有效数字。", "solution": "我们已知线性热模型 $\\Delta T = R_{th} P$ 和瞬时平方误差成本 $J_{k} = \\frac{1}{2} e_{k}^{2}$，其中 $e_{k} = \\Delta T_{k} - \\hat{R}_{th,k} P_{k}$。$J_{k}$ 相对于 $\\hat{R}_{th}$ 的梯度使用链式法则计算：\n$$\n\\frac{\\partial J_{k}}{\\partial \\hat{R}_{th}} = \\frac{\\partial}{\\partial \\hat{R}_{th}} \\left( \\frac{1}{2} e_{k}^{2} \\right) = e_{k} \\frac{\\partial e_{k}}{\\partial \\hat{R}_{th}}, \\quad \\text{以及} \\quad \\frac{\\partial e_{k}}{\\partial \\hat{R}_{th}} = -P_{k},\n$$\n因此\n$$\n\\frac{\\partial J_{k}}{\\partial \\hat{R}_{th}} = - e_{k} P_{k}.\n$$\n梯度下降更新为\n$$\n\\hat{R}_{th,k+1} = \\hat{R}_{th,k} - \\mu \\left. \\frac{\\partial J_{k}}{\\partial \\hat{R}_{th}} \\right|_{\\hat{R}_{th} = \\hat{R}_{th,k}} = \\hat{R}_{th,k} + \\mu\\, e_{k} P_{k}.\n$$\n等价地，\n$$\n\\hat{R}_{th,k+1} = \\hat{R}_{th,k} + \\mu\\, P_{k} \\left( \\Delta T_{k} - \\hat{R}_{th,k} P_{k} \\right).\n$$\n\n步骤 k = 0：\n给定 $\\hat{R}_{th,0} = 0.0500$，$P_{0} = 100.0$，$\\Delta T_{0} = 6.00$，计算\n$$\ne_{0} = \\Delta T_{0} - \\hat{R}_{th,0} P_{0} = 6.00 - (0.0500)(100.0) = 6.00 - 5.00 = 1.00.\n$$\n更新：\n$$\n\\hat{R}_{th,1} = \\hat{R}_{th,0} + \\mu e_{0} P_{0} = 0.0500 + (5.00 \\times 10^{-5})(1.00)(100.0) = 0.0500 + 0.00500 = 0.0550.\n$$\n\n步骤 k = 1：\n给定 $P_{1} = 150.0$，$\\Delta T_{1} = 9.00$，计算\n$$\ne_{1} = \\Delta T_{1} - \\hat{R}_{th,1} P_{1} = 9.00 - (0.0550)(150.0) = 9.00 - 8.25 = 0.75.\n$$\n更新：\n$$\n\\hat{R}_{th,2} = \\hat{R}_{th,1} + \\mu e_{1} P_{1} = 0.0550 + (5.00 \\times 10^{-5})(0.75)(150.0).\n$$\n计算增量：\n$$\n(5.00 \\times 10^{-5})(0.75)(150.0) = 5.00 \\times 10^{-5} \\times 112.5 = 0.005625.\n$$\n因此，\n$$\n\\hat{R}_{th,2} = 0.0550 + 0.005625 = 0.060625.\n$$\n四舍五入到三位有效数字，结果为 $0.0606$。", "answer": "$$\\boxed{0.0606}$$", "id": "1582140"}, {"introduction": "在成功地让系统输出跟踪上期望的轨迹后，我们很自然地会认为控制器中的参数估计也收敛到了它们的真实值。然而，事实并非总是如此。本练习 [@problem_id:1582114] 揭示了一个深刻的现象：即使控制性能完美，参数估计也可能出错，这引出了“信号丰富性”这一关键概念。", "problem": "考虑一个一阶动力学系统，其行为受两个未知的恒定参数 $\\theta_1$ 和 $\\theta_2$ 的影响。该系统由以下微分方程控制：\n$$ \\dot{y}(t) = u(t) + \\theta_1 w_1(t) + \\theta_2 w_2(t) $$\n其中 $y(t)$ 是系统输出，$u(t)$ 是控制输入，$w_1(t)$ 和 $w_2(t)$ 是已知的周期性信号，通常称为回归信号。\n\n为了将输出 $y(t)$ 调节至零，我们采用了一个自适应控制器。控制律定义为：\n$$ u(t) = -k y(t) - \\hat{\\theta}_1(t) w_1(t) - \\hat{\\theta}_2(t) w_2(t) $$\n其中 $\\hat{\\theta}_1(t)$ 和 $\\hat{\\theta}_2(t)$ 是未知参数 $\\theta_1$ 和 $\\theta_2$ 的在线估计值。参数估计值根据以下自适应律进行更新：\n$$ \\dot{\\hat{\\theta}}_1(t) = \\gamma_1 y(t) w_1(t) $$\n$$ \\dot{\\hat{\\theta}}_2(t) = \\gamma_2 y(t) w_2(t) $$\n其中 $\\gamma_1$ 和 $\\gamma_2$ 是正常数，称为自适应增益。\n\n给定以下具体值：\n- 真实（但控制器未知）的参数：$\\theta_1 = 3.0$, $\\theta_2 = 1.5$\n- 控制器和自适应增益：$k = 10.0$, $\\gamma_1 = 5.0$, $\\gamma_2 = 10.0$\n- 回归信号：$w_1(t) = \\cos(3t)$, $w_2(t) = 2\\cos(3t)$\n- 初始条件：$y(0) = 0$, $\\hat{\\theta}_1(0) = 1.0$, $\\hat{\\theta}_2(0) = 8.0$\n\n该自适应方案保证输出收敛到零，即 $\\lim_{t\\to\\infty} y(t) = 0$。您的任务是确定参数估计值收敛到的最终稳态值。令它们为 $\\hat{\\theta}_{1,\\infty} = \\lim_{t\\to\\infty} \\hat{\\theta}_1(t)$ 和 $\\hat{\\theta}_{2,\\infty} = \\lim_{t\\to\\infty} \\hat{\\theta}_2(t)$。\n\n将您的答案表示为一对精确分数 $(\\hat{\\theta}_{1,\\infty}, \\hat{\\theta}_{2,\\infty})$。", "solution": "定义参数估计误差为 $\\tilde{\\theta}_{1}(t) \\triangleq \\theta_{1} - \\hat{\\theta}_{1}(t)$ 和 $\\tilde{\\theta}_{2}(t) \\triangleq \\theta_{2} - \\hat{\\theta}_{2}(t)$。将控制律代入被控对象可得\n$$\n\\dot{y}(t) = -k\\,y(t) + \\tilde{\\theta}_{1}(t)\\,w_{1}(t) + \\tilde{\\theta}_{2}(t)\\,w_{2}(t).\n$$\n自适应律可得\n$$\n\\dot{\\tilde{\\theta}}_{1}(t) = -\\gamma_{1}\\,y(t)\\,w_{1}(t), \\qquad \\dot{\\tilde{\\theta}}_{2}(t) = -\\gamma_{2}\\,y(t)\\,w_{2}(t).\n$$\n给定回归信号 $w_{1}(t)=\\cos(3t)$ 和 $w_{2}(t)=2\\cos(3t)$，定义 $w(t)\\triangleq \\cos(3t)$ 及组合\n$$\ns(t) \\triangleq \\tilde{\\theta}_{1}(t) + 2\\,\\tilde{\\theta}_{2}(t).\n$$\n则闭环输出动态和 $s$-动态变为\n$$\n\\dot{y}(t) = -k\\,y(t) + w(t)\\,s(t), \\qquad \\dot{s}(t) = -\\left(\\gamma_{1} + 4\\gamma_{2}\\right)\\,y(t)\\,w(t).\n$$\n根据假设，$\\lim_{t\\to\\infty} y(t)=0$。为使 $y(t)\\equiv 0$ 成为不变集，当 $t\\to\\infty$ 时必须有 $\\dot{y}(t)\\equiv 0$。由于 $w(t)=\\cos(3t)$ 不恒为零，不变性要求\n$$\ns_{\\infty} \\triangleq \\lim_{t\\to\\infty} s(t) = 0,\n$$\n即，\n$$\n\\tilde{\\theta}_{1,\\infty} + 2\\,\\tilde{\\theta}_{2,\\infty} = 0.\n$$\n接下来，构造一个守恒量。考虑\n$$\nr(t) \\triangleq 2\\gamma_{2}\\,\\tilde{\\theta}_{1}(t) - \\gamma_{1}\\,\\tilde{\\theta}_{2}(t).\n$$\n根据自适应律和 $w_{2}(t)=2w(t)$，其时间导数为\n$$\n\\dot{r}(t) = 2\\gamma_{2}\\left(-\\gamma_{1}y(t)w(t)\\right) - \\gamma_{1}\\left(-\\gamma_{2}y(t)\\,2w(t)\\right) = -2\\gamma_{1}\\gamma_{2}y(t)w(t) + 2\\gamma_{1}\\gamma_{2}y(t)w(t) = 0.\n$$\n因此 $r(t)$ 是一个常数：\n$$\nr(t) \\equiv r(0) = 2\\gamma_{2}\\,\\tilde{\\theta}_{1}(0) - \\gamma_{1}\\,\\tilde{\\theta}_{2}(0).\n$$\n根据给定值 $\\theta_{1}=3$, $\\theta_{2}=\\frac{3}{2}$, $\\hat{\\theta}_{1}(0)=1$, $\\hat{\\theta}_{2}(0)=8$, $\\gamma_{1}=5$, $\\gamma_{2}=10$，初始误差为\n$$\n\\tilde{\\theta}_{1}(0) = \\theta_{1} - \\hat{\\theta}_{1}(0) = 3 - 1 = 2, \\qquad \\tilde{\\theta}_{2}(0) = \\theta_{2} - \\hat{\\theta}_{2}(0) = \\frac{3}{2} - 8 = -\\frac{13}{2},\n$$\n所以\n$$\nr(0) = 2\\gamma_{2}\\,\\tilde{\\theta}_{1}(0) - \\gamma_{1}\\,\\tilde{\\theta}_{2}(0) = 2\\cdot 10 \\cdot 2 - 5\\left(-\\frac{13}{2}\\right) = 40 + \\frac{65}{2} = \\frac{145}{2}.\n$$\n在稳态下，数对 $(\\tilde{\\theta}_{1,\\infty},\\tilde{\\theta}_{2,\\infty})$ 必须满足以下线性系统\n$$\n\\tilde{\\theta}_{1,\\infty} + 2\\,\\tilde{\\theta}_{2,\\infty} = 0, \\qquad 2\\gamma_{2}\\,\\tilde{\\theta}_{1,\\infty} - \\gamma_{1}\\,\\tilde{\\theta}_{2,\\infty} = \\frac{145}{2}.\n$$\n由第一个方程可得 $\\tilde{\\theta}_{1,\\infty} = -2\\,\\tilde{\\theta}_{2,\\infty}$。代入第二个方程可得\n$$\n-4\\gamma_{2}\\,\\tilde{\\theta}_{2,\\infty} - \\gamma_{1}\\,\\tilde{\\theta}_{2,\\infty} = \\frac{145}{2} \\;\\;\\Rightarrow\\;\\; \\tilde{\\theta}_{2,\\infty} = -\\frac{145/2}{4\\gamma_{2}+\\gamma_{1}}.\n$$\n当 $\\gamma_{2}=10$, $\\gamma_{1}=5$ 时，我们有 $4\\gamma_{2}+\\gamma_{1}=45$，因此\n$$\n\\tilde{\\theta}_{2,\\infty} = -\\frac{145}{90} = -\\frac{29}{18}, \\qquad \\tilde{\\theta}_{1,\\infty} = -2\\,\\tilde{\\theta}_{2,\\infty} = \\frac{29}{9}.\n$$\n最后，稳态参数估计值为\n$$\n\\hat{\\theta}_{1,\\infty} = \\theta_{1} - \\tilde{\\theta}_{1,\\infty} = 3 - \\frac{29}{9} = -\\frac{2}{9}, \\qquad \\hat{\\theta}_{2,\\infty} = \\theta_{2} - \\tilde{\\theta}_{2,\\infty} = \\frac{3}{2} + \\frac{29}{18} = \\frac{28}{9}.\n$$\n作为检验，可辨识的组合满足 $\\hat{\\theta}_{1,\\infty} + 2\\hat{\\theta}_{2,\\infty} = 6 = \\theta_{1} + 2\\theta_{2}$，这与 $s_{\\infty}=0$ 一致。", "answer": "$$\\boxed{\\begin{pmatrix}-\\frac{2}{9} & \\frac{28}{9}\\end{pmatrix}}$$", "id": "1582114"}, {"introduction": "“信号丰富性”不足的问题在实际应用中很常见，尤其是在镇定任务中。本练习 [@problem_id:1582173] 以经典的倒立摆系统为例，解释了为什么当一个自适应控制器成功地将系统稳定在平衡点时，它往往会失去进一步学习和改进参数估计的能力。这揭示了自适应控制在实现稳定目标和精确参数辨识之间的一个内在挑战。", "problem": "一个倒立摆的动力学，在其不稳定的直立平衡点附近线性化后，可以由以下微分方程描述：\n$$ \\ddot{\\theta}(t) = a\\theta(t) + b u(t) $$\n其中 $\\theta(t)$ 是摆锤与垂直方向的夹角，$u(t)$ 是施加的控制力矩，参数 $a$ 是一个与重力和摆长相关的已知正常数，参数 $b$ 是一个与摆锤质量成反比的未知正常数。\n\n为了将摆稳定在直立位置（$\\theta=0$），一个自适应控制器被实施。该控制律旨在抵消已知动力学并施加期望的稳定行为：\n$$ u(t) = \\frac{1}{\\hat{b}(t)} \\left( -a\\theta(t) + v(t) \\right) $$\n其中 $\\hat{b}(t)$ 是未知参数 $b$ 的实时估计值，而 $v(t) = -k_p\\theta(t) - k_d\\dot{\\theta}(t)$ 是一个具有已知增益 $k_p > 0$ 和 $k_d > 0$ 的稳定项。\n\n为了更新参数估计，使用以下从 Lyapunov 稳定性分析导出的自适应律：\n$$ \\dot{\\hat{b}}(t) = - \\gamma \\cdot \\theta(t) \\cdot \\left( -a\\theta(t) + v(t) \\right) = - \\gamma \\theta(t) (\\hat{b}(t) u(t)) $$\n其中 $\\gamma > 0$ 是自适应增益。\n\n控制器成功实现了其目标：对于任何初始的小位移，摆角 $\\theta(t)$ 随着时间趋于无穷大而收敛到零。然而，实验结果表明，参数估计值 $\\hat{b}(t)$ 收敛到一个常数值 $\\hat{b}_{\\text{final}}$，该值不一定等于真实参数值 $b$。\n\n即使稳定这一主要控制目标已经实现，以下哪个陈述最准确地解释了为什么估计值 $\\hat{b}(t)$ 可能无法收敛到真实值 $b$？\n\nA. 随着系统趋于稳定，驱动自适应机制的信号，即 $\\theta(t)$ 和 $u(t)$，衰减至零。这导致更新项 $\\dot{\\hat{b}}(t)$ 变为零，从而停止估计值的任何进一步变化，无论现存的参数误差如何。\n\nB. 线性化模型只是一个近似。当 $\\theta(t)$ 接近零时，未建模的非线性效应虽然很小，但相对于信号变得显著，从而阻止自适应律分离出 $b$ 的真实值。\n\nC. 控制力 $u(t)$ 必须抵消不稳定性项 $a\\theta(t)$。这在 $u(t)$ 和 $\\theta(t)$ 之间建立了一个永久的代数关系，使得自适应算法无法独立确定缩放因子 $b$。\n\nD. 自适应增益 $\\gamma$ 是一个固定常数。需要一个随时变增益，在 $\\theta(t)$ 减小时增大，以确保系统接近平衡时参数估计的收敛。\n\nE. 成功的稳定意味着控制律是有效的。差异的产生是因为 $b$ 的真实值使得开环系统比值 $\\hat{b}_{\\text{final}}$ 更不稳定，而控制器自然会收敛到一个看起来更容易控制的估计值。", "solution": "从被控对象和控制律开始：\n$$\n\\ddot{\\theta}(t)=a\\theta(t)+b\\,u(t),\\qquad\nu(t)=\\frac{1}{\\hat{b}(t)}\\left(-a\\theta(t)+v(t)\\right),\\qquad\nv(t)=-k_{p}\\theta(t)-k_{d}\\dot{\\theta}(t),\n$$\n其中 $a>0$，$k_{p}>0$，$k_{d}>0$，且 $b>0$ 未知。\n\n通过代入得到控制律下的闭环动力学：\n$$\n\\ddot{\\theta}(t)=a\\theta(t)+b\\cdot\\frac{1}{\\hat{b}(t)}\\left(-a\\theta(t)+v(t)\\right)\n=a\\theta(t)+\\frac{b}{\\hat{b}(t)}\\left(-a\\theta(t)-k_{p}\\theta(t)-k_{d}\\dot{\\theta}(t)\\right).\n$$\n如果 $\\hat{b}(t)\\equiv b$，那么 $\\ddot{\\theta}(t)=v(t)=-k_{p}\\theta(t)-k_{d}\\dot{\\theta}(t)$，对于 $k_{p}>0$，$k_{d}>0$ 而言，这是指数稳定的。即使 $\\hat{b}(t)\\neq b$ 但 $\\hat{b}(t)>0$，闭环系统变为\n$$\n\\ddot{\\theta}(t)=\\Big(a-\\frac{b}{\\hat{b}(t)}a-\\frac{b}{\\hat{b}(t)}k_{p}\\Big)\\theta(t)-\\frac{b}{\\hat{b}(t)}k_{d}\\dot{\\theta}(t),\n$$\n并且对于合适的增益，平衡点 $\\theta=0$ 在实验观察到的意义上保持稳定。\n\n参数更新为\n$$\n\\dot{\\hat{b}}(t)=-\\gamma\\,\\theta(t)\\,\\big(-a\\theta(t)+v(t)\\big),\n$$\n并且，利用控制律关系 $-a\\theta(t)+v(t)=\\hat{b}(t)\\,u(t)$，等价地\n$$\n\\dot{\\hat{b}}(t)=-\\gamma\\,\\hat{b}(t)\\,\\theta(t)\\,u(t).\n$$\n在成功稳定的情况下，当 $t\\to\\infty$ 时，$\\theta(t)\\to 0$ 且 $\\dot{\\theta}(t)\\to 0$。于是 $v(t)=-k_{p}\\theta(t)-k_{d}\\dot{\\theta}(t)\\to 0$，并且从 $u(t)=\\frac{1}{\\hat{b}(t)}\\big(-a\\theta(t)+v(t)\\big)$ 可知 $u(t)\\to 0$。因此，\n$$\n\\dot{\\hat{b}}(t)=-\\gamma\\,\\hat{b}(t)\\,\\theta(t)\\,u(t)\\to 0\n$$\n所以驱动自适应的被积函数消失，估计值被“冻结”：\n$$\n\\hat{b}(t)=\\hat{b}(t_{0})-\\gamma\\int_{t_{0}}^{t}\\hat{b}(\\tau)\\,\\theta(\\tau)\\,u(\\tau)\\,d\\tau\n$$\n积分会收敛到一个有限值，该值取决于暂态和初始条件，并不一定强制 $\\hat{b}(t)\\to b$。\n\n在标准的自适应控制理论中，参数估计要收敛到其真实值，需要回归量满足持续激励条件。在这里，在调节过程中，激励自适应的相关信号乘积（例如 $\\theta(t)$ 和 $u(t)$）衰减到零，违反了持续激励条件。因此，无法保证 $\\hat{b}(t)$ 收敛到 $b$；相反，$\\hat{b}(t)$ 会收敛到由暂态数据决定的某个常数 $\\hat{b}_{\\text{final}}$。这正是选项 A 中描述的机制。\n\n选项 B、C、D 和 E 没有提供根本原因：未建模的非线性（B）不是解释不收敛所必需的；如果存在激励， $u$ 和 $\\theta$ 之间的代数关系（C）不排除辨识的可能性；随时间改变 $\\gamma$（D）无法补偿激励的缺乏；而 E 中的启发式说法与基于 Lyapunov 的自适应机制无关。本质问题是随着稳定性的实现，激励信号消失了。", "answer": "$$\\boxed{A}$$", "id": "1582173"}]}