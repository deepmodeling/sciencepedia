{"hands_on_practices": [{"introduction": "Q学习是无模型强化学习的基石，它使智能体能够直接从经验中学习最优行为策略，而无需事先了解环境的动态模型。本练习将通过一个具体的分步计算过程，揭示Q值（状态-动作价值）是如何通过时序差分（TD）误差进行迭代更新的。掌握这一基本更新机制，是理解更高级强化学习控制算法的关键。[@problem_id:2738645]", "problem": "考虑一个有限马尔可夫决策过程 (MDP)，其状态集为 $\\mathcal{S}=\\{s_{0},s_{1},s_{2}\\}$，其中 $s_{2}$ 是终止状态，动作集为 $\\mathcal{A}=\\{a_{0},a_{1}\\}$。其单步转移动态和奖励是确定性的，由以下描述给出，这些描述与受控马尔可夫性质和可加性奖励信号一致：\n- 从 $s_{0}$ 出发：\n  - 执行动作 $a_{0}$ 转移到 $s_{1}$，奖励为 $2$。\n  - 执行动作 $a_{1}$ 转移到 $s_{2}$，奖励为 $0$。\n- 从 $s_{1}$ 出发：\n  - 执行动作 $a_{0}$ 转移到 $s_{2}$，奖励为 $0$。\n  - 执行动作 $a_{1}$ 转移到 $s_{1}$，奖励为 $-1$。\n- 从 $s_{2}$ 出发：该幕立即终止。对于任何涉及在终止状态下计算 $\\max_{a'}$ 的评估，使用惯例，即在空动作集上的最大值为 $0$。\n\n你将使用源于 Bellman 最优算子的离策略时序差分更新来近似最优状态-动作价值函数（Q函数），这与强化学习 (RL) 中的标准Q学习方法一样。折扣因子为 $\\gamma=\\tfrac{1}{2}$。非终止状态的初始动作价值表是均匀的：对于所有 $(s,a)\\in\\{s_{0},s_{1}\\}\\times\\{a_{0},a_{1}\\}$，$Q_{0}(s,a)=1$。步长（学习率）是常数，每次更新均为 $\\alpha=\\tfrac{1}{2}$。你观察到由某个行为策略（不一定是最优策略）生成的以下有序转移序列：\n1. $(s,a,r,s')=(s_{0},a_{0},2,s_{1})$，\n2. $(s,a,r,s')=(s_{1},a_{1},-1,s_{1})$，\n3. $(s,a,r,s')=(s_{1},a_{0},0,s_{2})$。\n\n从 $Q_{0}$ 开始，并按照上述转移序列的顺序应用三次离策略Q学习更新，计算得到的关于非终止状态-动作对 $(s_{0},a_{0})$、$(s_{0},a_{1})$、$(s_{1},a_{0})$、$(s_{1},a_{1})$ 的动作价值表 $Q_{3}$。以该顺序将你的最终答案报告为单行矩阵。不需要四舍五入，期望得到精确的分数值。", "solution": "问题陈述已经过验证，被认为是有效的。这是一个在强化学习既定框架内具有科学依据、提法恰当且客观的问题表述。所有必要信息均已提供，该任务是Q学习算法的直接应用。\n\n对于从状态 $s$ 采取动作 $a$ 转移到状态 $s'$ 并获得奖励 $r$ 的一次转移，Q学习的更新规则如下：\n$$Q_{k+1}(s,a) = Q_{k}(s,a) + \\alpha \\left[ r + \\gamma \\max_{a'} Q_{k}(s', a') - Q_{k}(s,a) \\right]$$\n其中 $k$ 是更新索引，$\\alpha$ 是学习率，$\\gamma$ 是折扣因子。根据问题参数，我们有 $\\alpha=\\frac{1}{2}$ 和 $\\gamma=\\frac{1}{2}$。该更新规则可以重写为：\n$$Q_{k+1}(s,a) = \\left(1-\\alpha\\right)Q_{k}(s,a) + \\alpha \\left[ r + \\gamma \\max_{a'} Q_{k}(s', a') \\right]$$\n$$Q_{k+1}(s,a) = \\frac{1}{2} Q_{k}(s,a) + \\frac{1}{2} \\left[ r + \\frac{1}{2} \\max_{a'} Q_{k}(s', a') \\right]$$\n\n非终止状态的初始动作价值表为 $Q_{0}(s, a) = 1$，适用于所有 $(s, a) \\in \\{s_{0}, s_{1}\\} \\times \\{a_{0}, a_{1}\\}$。因此，我们从以下表格开始：\n$$Q_{0}(s_{0}, a_{0}) = 1$$\n$$Q_{0}(s_{0}, a_{1}) = 1$$\n$$Q_{0}(s_{1}, a_{0}) = 1$$\n$$Q_{0}(s_{1}, a_{1}) = 1$$\n对于终止状态 $s_{2}$，我们使用惯例 $\\max_{a'} Q(s_{2}, a') = 0$。\n\n现在我们按照指定的顺序进行更新。\n\n**更新 1：**\n第一次转移是 $(s, a, r, s') = (s_{0}, a_{0}, 2, s_{1})$。我们更新 $Q(s_{0}, a_{0})$。表中的其他值在这一步保持不变。下一个状态是 $s_{1}$。我们必须首先使用当前表格 $Q_{0}$ 计算从状态 $s_{1}$ 出发的最大Q值。\n$$\\max_{a'} Q_{0}(s_{1}, a') = \\max\\{Q_{0}(s_{1}, a_{0}), Q_{0}(s_{1}, a_{1})\\} = \\max\\{1, 1\\} = 1$$\n现在我们对 $Q_{1}(s_{0}, a_{0})$ 应用更新规则：\n$$Q_{1}(s_{0}, a_{0}) = \\frac{1}{2} Q_{0}(s_{0}, a_{0}) + \\frac{1}{2} \\left[ r + \\frac{1}{2} \\max_{a'} Q_{0}(s_{1}, a') \\right]$$\n$$Q_{1}(s_{0}, a_{0}) = \\frac{1}{2}(1) + \\frac{1}{2} \\left[ 2 + \\frac{1}{2}(1) \\right] = \\frac{1}{2} + \\frac{1}{2} \\left[ 2 + \\frac{1}{2} \\right] = \\frac{1}{2} + \\frac{1}{2} \\left( \\frac{5}{2} \\right) = \\frac{1}{2} + \\frac{5}{4} = \\frac{2}{4} + \\frac{5}{4} = \\frac{7}{4}$$\n第一次更新后，动作价值表 $Q_{1}$ 为：\n$$Q_{1}(s_{0}, a_{0}) = \\frac{7}{4}$$\n$$Q_{1}(s_{0}, a_{1}) = 1$$\n$$Q_{1}(s_{1}, a_{0}) = 1$$\n$$Q_{1}(s_{1}, a_{1}) = 1$$\n\n**更新 2：**\n第二次转移是 $(s, a, r, s') = (s_{1}, a_{1}, -1, s_{1})$。我们使用 $Q_{1}$ 的值来更新 $Q(s_{1}, a_{1})$。下一个状态是 $s_{1}$。\n$$\\max_{a'} Q_{1}(s_{1}, a') = \\max\\{Q_{1}(s_{1}, a_{0}), Q_{1}(s_{1}, a_{1})\\} = \\max\\{1, 1\\} = 1$$\n现在我们对 $Q_{2}(s_{1}, a_{1})$ 应用更新规则：\n$$Q_{2}(s_{1}, a_{1}) = \\frac{1}{2} Q_{1}(s_{1}, a_{1}) + \\frac{1}{2} \\left[ r + \\frac{1}{2} \\max_{a'} Q_{1}(s_{1}, a') \\right]$$\n$$Q_{2}(s_{1}, a_{1}) = \\frac{1}{2}(1) + \\frac{1}{2} \\left[ -1 + \\frac{1}{2}(1) \\right] = \\frac{1}{2} + \\frac{1}{2} \\left[ -1 + \\frac{1}{2} \\right] = \\frac{1}{2} + \\frac{1}{2} \\left( -\\frac{1}{2} \\right) = \\frac{1}{2} - \\frac{1}{4} = \\frac{1}{4}$$\n第二次更新后，动作价值表 $Q_{2}$ 为：\n$$Q_{2}(s_{0}, a_{0}) = \\frac{7}{4}$$\n$$Q_{2}(s_{0}, a_{1}) = 1$$\n$$Q_{2}(s_{1}, a_{0}) = 1$$\n$$Q_{2}(s_{1}, a_{1}) = \\frac{1}{4}$$\n\n**更新 3：**\n第三次转移是 $(s, a, r, s') = (s_{1}, a_{0}, 0, s_{2})$。我们使用 $Q_{2}$ 的值来更新 $Q(s_{1}, a_{0})$。下一个状态 $s_{2}$ 是终止状态。\n$$\\max_{a'} Q_{2}(s_{2}, a') = 0$$\n现在我们对 $Q_{3}(s_{1}, a_{0})$ 应用更新规则：\n$$Q_{3}(s_{1}, a_{0}) = \\frac{1}{2} Q_{2}(s_{1}, a_{0}) + \\frac{1}{2} \\left[ r + \\frac{1}{2} \\max_{a'} Q_{2}(s_{2}, a') \\right]$$\n$$Q_{3}(s_{1}, a_{0}) = \\frac{1}{2}(1) + \\frac{1}{2} \\left[ 0 + \\frac{1}{2}(0) \\right] = \\frac{1}{2} + 0 = \\frac{1}{2}$$\n在这次最终更新后，动作价值表 $Q_{3}$ 为：\n$$Q_{3}(s_{0}, a_{0}) = \\frac{7}{4}$$\n$$Q_{3}(s_{0}, a_{1}) = 1$$\n$$Q_{3}(s_{1}, a_{0}) = \\frac{1}{2}$$\n$$Q_{3}(s_{1}, a_{1}) = \\frac{1}{4}$$\n问题要求按 $(s_{0}, a_{0})$、$(s_{0}, a_{1})$、$(s_{1}, a_{0})$、$(s_{1}, a_{1})$ 的顺序将这些最终值报告为单行矩阵。这些值为 $\\frac{7}{4}$、 $1$、 $\\frac{1}{2}$ 和 $\\frac{1}{4}$。", "answer": "$$\\boxed{\\begin{pmatrix} \\frac{7}{4}  1  \\frac{1}{2}  \\frac{1}{4} \\end{pmatrix}}$$", "id": "2738645"}, {"introduction": "现实世界中的控制问题往往涉及连续或巨大的状态空间，这使得传统的表格型强化学习方法不再适用。本练习将展示如何通过结合时序差分（TD）学习与线性函数近似来克服这一挑战，这也是演员-评论家（Actor-Critic）方法中“评论家”部分的核心思想。通过完成一个单步更新的计算，您将深入理解智能体如何将其价值估计泛化到不同状态，从而为解决复杂控制问题奠定基础。[@problem_id:2738612]", "problem": "考虑在一个线性时不变随机控制系统中，对于一个固定的、稳定的反馈策略，进行折扣马尔可夫决策过程 (MDP) 中的策略评估。设该策略下的状态价值函数由一个线性函数 $V_{\\theta}(x) = \\theta^{\\top}\\phi(x)$ 近似，其中 $\\phi(x) \\in \\mathbb{R}^{3}$ 是一个已知的特征映射，$\\theta \\in \\mathbb{R}^{3}$ 是 Actor-Critic 架构中评论家 (critic) 的参数向量。在时间步 $k$，观测到一次转移，其状态为 $x_{k}$，下一状态为 $x_{k+1}$，即时奖励为 $r_{k}$。假设以下数据可用：\n- 特征向量 $\\phi(x_{k}) = \\begin{pmatrix} 1 \\\\ 2 \\\\ -1 \\end{pmatrix}$ 和 $\\phi(x_{k+1}) = \\begin{pmatrix} 1 \\\\ 0 \\\\ 3 \\end{pmatrix}$，\n- 折扣因子 $\\gamma = \\frac{9}{10}$，\n- 步长 $\\alpha = \\frac{1}{10}$，\n- 当前参数向量 $\\theta_{k} = \\begin{pmatrix} 2 \\\\ -1 \\\\ 0 \\end{pmatrix}$，\n- 实现的奖励 $r_{k} = 1$。\n\n从固定策略下价值函数的 Bellman 方程以及价值函数到特征线性张成空间上的投影出发，推导针对单次观测转移的 $V_{\\theta}$ 的单步半梯度时间差分 (TD(0)) 更新，并应用它来计算更新后的参数向量 $\\theta_{k+1}$。将你的最终答案精确地表示为一个 $3 \\times 1$ 的列向量，不要进行四舍五入。", "solution": "问题陈述必须首先经过严格验证。\n\n步骤 1：提取已知条件。\n- 系统是一个折扣马尔可夫决策过程 (MDP) 框架内的线性时不变随机控制系统。\n- 策略 $\\pi$ 是固定的且稳定的。\n- 状态价值函数由一个线性函数近似：$V_{\\theta}(x) = \\theta^{\\top}\\phi(x)$。\n- 特征映射为 $\\phi(x) \\in \\mathbb{R}^{3}$。\n- 参数向量为 $\\theta \\in \\mathbb{R}^{3}$。\n- 在时间步 $k$，观测到一次转移 $(x_k, r_k, x_{k+1})$。\n- 状态 $x_k$ 处的特征向量：$\\phi(x_{k}) = \\begin{pmatrix} 1 \\\\ 2 \\\\ -1 \\end{pmatrix}$。\n- 下一状态 $x_{k+1}$ 处的特征向量：$\\phi(x_{k+1}) = \\begin{pmatrix} 1 \\\\ 0 \\\\ 3 \\end{pmatrix}$。\n- 折扣因子：$\\gamma = \\frac{9}{10}$。\n- 步长：$\\alpha = \\frac{1}{10}$。\n- 当前参数向量：$\\theta_{k} = \\begin{pmatrix} 2 \\\\ -1 \\\\ 0 \\end{pmatrix}$。\n- 实现的奖励：$r_{k} = 1$。\n- 任务是推导单步半梯度时间差分 (TD($0$)) 更新规则，并应用它来计算更新后的参数向量 $\\theta_{k+1}$。\n\n步骤 2：使用提取的已知条件进行验证。\n该问题具有科学依据、是良定的且客观的。它描述了强化学习中的一个典型问题：使用带有线性函数近似的时间差分学习进行策略评估。单次更新步骤所需的所有数据都已提供，不存在矛盾、歧义或不科学的前提。该问题是控制理论和机器学习中的一个标准练习，需要直接应用一个基本算法。\n\n步骤 3：结论与行动。\n问题有效。将提供解答。\n\n策略评估的目标是为给定策略 $\\pi$ 确定其状态价值函数 $V_{\\pi}(x)$。对于一个折扣 MDP， $V_{\\pi}(x)$ 必须满足 Bellman 方程：\n$$V_{\\pi}(x) = \\mathbb{E}_{\\pi}[R_{t+1} + \\gamma V_{\\pi}(X_{t+1}) | X_t = x]$$\n其中 $R_{t+1}$ 是下一时间步的奖励，$X_{t+1}$ 是下一状态，$\\gamma \\in [0, 1)$ 是折扣因子。\n\n当状态空间巨大或连续时，我们用一个参数化函数来近似 $V_{\\pi}(x)$，在本例中是一个特征的线性组合：$V_{\\theta}(x) = \\theta^{\\top}\\phi(x)$。目标是找到参数向量 $\\theta$ 使得 $V_{\\theta}(x) \\approx V_{\\pi}(x)$。\n\n时间差分 (TD) 学习方法根据经验更新参数。对于单次观测到的转移 $(x_k, r_k, x_{k+1})$，TD($0$) 方法旨在减小 TD 误差 $\\delta_k$，该误差是 TD 目标与当前价值估计之间的差值：\n$$\\delta_k = r_k + \\gamma V_{\\theta_k}(x_{k+1}) - V_{\\theta_k}(x_k)$$\nTD 目标 $r_k + \\gamma V_{\\theta_k}(x_{k+1})$ 是对状态 $x_k$ 价值的改进估计。\n\n参数向量 $\\theta$ 的更新是使用随机半梯度下降方法来最小化均方 Bellman 误差而推导出来的。半梯度方法在求导时忽略了 TD 目标对 $\\theta$ 的依赖性，这简化了更新并改善了收敛特性。$\\theta_k$ 的更新规则由下式给出：\n$$\\theta_{k+1} = \\theta_k + \\alpha \\delta_k \\nabla_{\\theta} V_{\\theta_k}(x_k)$$\n对于我们的线性函数近似器 $V_{\\theta}(x) = \\theta^{\\top}\\phi(x)$，其关于 $\\theta$ 的梯度就是特征向量：\n$$\\nabla_{\\theta} V_{\\theta}(x) = \\phi(x)$$\n将此式和 $\\delta_k$ 的表达式代入更新规则，我们得到针对单次转移的单步半梯度 TD($0$) 更新：\n$$\\theta_{k+1} = \\theta_k + \\alpha (r_k + \\gamma \\theta_k^{\\top}\\phi(x_{k+1}) - \\theta_k^{\\top}\\phi(x_k)) \\phi(x_k)$$\n\n现在，我们使用所提供的数据应用此公式。\n首先，我们使用当前参数向量 $\\theta_k$ 计算在状态 $x_k$ 和 $x_{k+1}$ 处的价值估计：\n$$V_{\\theta_k}(x_k) = \\theta_k^{\\top}\\phi(x_k) = \\begin{pmatrix} 2  -1  0 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 2 \\\\ -1 \\end{pmatrix} = (2)(1) + (-1)(2) + (0)(-1) = 2 - 2 + 0 = 0$$\n$$V_{\\theta_k}(x_{k+1}) = \\theta_k^{\\top}\\phi(x_{k+1}) = \\begin{pmatrix} 2  -1  0 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\\\ 3 \\end{pmatrix} = (2)(1) + (-1)(0) + (0)(3) = 2 - 0 + 0 = 2$$\n\n接下来，我们计算 TD 误差 $\\delta_k$：\n$$\\delta_k = r_k + \\gamma V_{\\theta_k}(x_{k+1}) - V_{\\theta_k}(x_k) = 1 + \\left(\\frac{9}{10}\\right)(2) - 0 = 1 + \\frac{18}{10} = \\frac{10}{10} + \\frac{18}{10} = \\frac{28}{10}$$\n\n最后，我们计算更新后的参数向量 $\\theta_{k+1}$：\n$$\\theta_{k+1} = \\theta_k + \\alpha \\delta_k \\phi(x_k)$$\n代入数值：\n$$\\theta_{k+1} = \\begin{pmatrix} 2 \\\\ -1 \\\\ 0 \\end{pmatrix} + \\left(\\frac{1}{10}\\right) \\left(\\frac{28}{10}\\right) \\begin{pmatrix} 1 \\\\ 2 \\\\ -1 \\end{pmatrix}$$\n$$\\theta_{k+1} = \\begin{pmatrix} 2 \\\\ -1 \\\\ 0 \\end{pmatrix} + \\frac{28}{100} \\begin{pmatrix} 1 \\\\ 2 \\\\ -1 \\end{pmatrix}$$\n化简分数 $\\frac{28}{100} = \\frac{7}{25}$：\n$$\\theta_{k+1} = \\begin{pmatrix} 2 \\\\ -1 \\\\ 0 \\end{pmatrix} + \\frac{7}{25} \\begin{pmatrix} 1 \\\\ 2 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ -1 \\\\ 0 \\end{pmatrix} + \\begin{pmatrix} 7/25 \\\\ 14/25 \\\\ -7/25 \\end{pmatrix}$$\n进行向量加法：\n$$\\theta_{k+1} = \\begin{pmatrix} 2 + \\frac{7}{25} \\\\ -1 + \\frac{14}{25} \\\\ 0 - \\frac{7}{25} \\end{pmatrix} = \\begin{pmatrix} \\frac{50}{25} + \\frac{7}{25} \\\\ -\\frac{25}{25} + \\frac{14}{25} \\\\ -\\frac{7}{25} \\end{pmatrix} = \\begin{pmatrix} \\frac{57}{25} \\\\ -\\frac{11}{25} \\\\ -\\frac{7}{25} \\end{pmatrix}$$\n\n这就是观测到单次转移后更新的参数向量。", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{57}{25} \\\\\n-\\frac{11}{25} \\\\\n-\\frac{7}{25}\n\\end{pmatrix}\n}\n$$", "id": "2738612"}, {"introduction": "与基于值的方法间接学习策略不同，策略梯度方法直接对策略本身进行优化，这对于处理控制领域中常见的连续动作空间问题尤为有效。本练习将通过一个小型离散环境，对比理论上精确的策略梯度与使用REINFORCE算法从采样轨迹中获得的蒙特卡洛（MC）估计。这种对比有助于建立对现代策略优化方法中采样性质、方差以及理论与实践之间关系的关键理解。[@problem_id:2738661]", "problem": "考虑以下用于控制的强化学习 (RL) 中的分幕式马尔可夫决策过程 (MDP)。状态集为 $\\{s_{0}, s_{1}, s_{2}\\}$，其中 $s_{0}$ 是唯一的初始状态。动作集为 $\\{0,1\\}$。幕长为 $2$ 个时间步 ($t=0,1$)，折扣因子为 $\\gamma=1$。转移和奖励结构如下：\n- 在 $s_{0}$ 处：采取动作 $a=1$ 确定性地转移到 $s_{1}$，立即奖励为 $0$；采取动作 $a=0$ 确定性地转移到 $s_{2}$，立即奖励为 $0$。\n- 在 $s_{1}$ 处（行动后终止）：采取动作 $a=1$ 产生奖励 $2$；采取动作 $a=0$ 产生奖励 $0$。\n- 在 $s_{2}$ 处（行动后终止）：采取动作 $a=1$ 产生奖励 $1$；采取动作 $a=0$ 产生奖励 $3$。\n\n随机策略是一个状态独立的、使用 logistic 参数化的伯努利分布。对于每个 $s \\in \\{s_{0}, s_{1}, s_{2}\\}$，策略以概率 $\\pi_{\\theta}(1 \\mid s)=\\sigma(\\theta_{s})$ 选择动作 $a=1$，以概率 $\\pi_{\\theta}(0 \\mid s)=1-\\sigma(\\theta_{s})$ 选择动作 $a=0$，其中 $\\sigma(x)=\\frac{1}{1+\\exp(-x)}$ 且 $\\theta=\\big(\\theta_{s_{0}},\\theta_{s_{1}},\\theta_{s_{2}}\\big)$。性能目标是期望回报 $J(\\theta)=\\mathbb{E}_{\\pi_{\\theta}}\\big[\\sum_{t=0}^{1} r_{t}\\big]$。\n\n使用具体参数向量 $\\theta=\\big(0.3,-0.4,0.8\\big)$。\n\n任务：\n1. 通过显式地枚举所有可能的轨迹，并对期望回报求关于 $\\theta$ 的梯度，来计算精确的策略梯度 $\\nabla_{\\theta} J(\\theta)$。\n2. 使用不带基线的似然比 (REINFORCE) 估计器，在 $K$ 个采样幕上构建策略梯度的蒙特卡洛 (MC) 估计，其形式为 $\\widehat{g}=\\frac{1}{K}\\sum_{k=1}^{K}\\sum_{t=0}^{1}\\nabla_{\\theta}\\ln \\pi_{\\theta}(a_{t}^{(k)} \\mid s_{t}^{(k)})\\,G_{t}^{(k)}$，其中 $G_{t}^{(k)}=\\sum_{t' = t}^{1} r_{t'}^{(k)}$。假设在给定的 $\\theta$ 下，您观察到 $K=5$ 个幕，其状态-动作-奖励序列如下：\n   - 幕 1：在 $s_{0}$ 处选择 $a=1$，然后在 $s_{1}$ 处选择 $a=0$；终止奖励为 $0$。\n   - 幕 2：在 $s_{0}$ 处选择 $a=0$，然后在 $s_{2}$ 处选择 $a=0$；终止奖励为 $3$。\n   - 幕 3：在 $s_{0}$ 处选择 $a=1$，然后在 $s_{1}$ 处选择 $a=1$；终止奖励为 $2$。\n   - 幕 4：在 $s_{0}$ 处选择 $a=0$，然后在 $s_{2}$ 处选择 $a=1$；终止奖励为 $1$。\n   - 幕 5：在 $s_{0}$ 处选择 $a=1$，然后在 $s_{1}$ 处选择 $a=1$；终止奖励为 $2$。\n\n根据这些幕计算 MC 估计 $\\widehat{g}$。\n\n最后，报告精确梯度与 MC 估计之间差值的欧几里得范数，即 $\\|\\nabla_{\\theta} J(\\theta)-\\widehat{g}\\|_{2}$。将您的答案四舍五入到四位有效数字。提供一个无单位的实数作为最终答案。", "solution": "我们从强化学习 (RL) 的基本原理开始。一条轨迹的回报是奖励的总和 $\\sum_{t=0}^{1} r_{t}$ (当 $\\gamma=1$ 时)。在策略 $\\pi_{\\theta}$ 下的期望回报是 $J(\\theta)=\\mathbb{E}_{\\pi_{\\theta}}\\left[\\sum_{t=0}^{1} r_{t}\\right]$。对于这个小型的表格化环境，我们可以精确地枚举所有可能的轨迹，并将 $J(\\theta)$ 计算为 $\\theta$ 的函数，然后进行微分。\n\n定义 $\\sigma(x)=\\frac{1}{1+\\exp(-x)}$，为简洁起见，令 $p_{0}=\\sigma(\\theta_{s_{0}})$，$p_{1}=\\sigma(\\theta_{s_{1}})$ 和 $p_{2}=\\sigma(\\theta_{s_{2}})$。在该环境动态下：\n- 在时间 $t=0$ 时，智能体在 $s_{0}$ 处，以概率 $p_{0}$ 转移到 $s_{1}$ (采取动作 $a=1$ 后)，以概率 $1-p_{0}$ 转移到 $s_{2}$ (采取动作 $a=0$ 后)。在 $t=0$ 时没有奖励。\n- 在时间 $t=1$ 时，如果在 $s_{1}$，期望奖励为 $2 \\cdot p_{1} + 0 \\cdot (1-p_{1})=2p_{1}$。如果在 $s_{2}$，期望奖励为 $1 \\cdot p_{2} + 3 \\cdot (1-p_{2})=3-2p_{2}$。\n\n因此，通过枚举两步轨迹并进行汇总，期望回报为\n$$\nJ(\\theta)=p_{0}\\cdot (2p_{1}) + (1-p_{0})\\cdot (3-2p_{2}).\n$$\n我们对 $J(\\theta)$ 关于 $\\theta$ 的每个分量进行微分。使用 $\\frac{\\mathrm{d}}{\\mathrm{d}\\theta}\\sigma(\\theta)=\\sigma(\\theta)\\big(1-\\sigma(\\theta)\\big)$ 和链式法则，我们得到\n$$\n\\frac{\\partial J}{\\partial \\theta_{s_{0}}}\n= \\frac{\\mathrm{d}p_{0}}{\\mathrm{d}\\theta_{s_{0}}}\\,\\big(2p_{1}-(3-2p_{2})\\big)\n= p_{0}\\big(1-p_{0}\\big)\\,\\big(2p_{1}-3+2p_{2}\\big),\n$$\n$$\n\\frac{\\partial J}{\\partial \\theta_{s_{1}}}\n= p_{0}\\cdot 2\\,\\frac{\\mathrm{d}p_{1}}{\\mathrm{d}\\theta_{s_{1}}}\n= 2p_{0}\\,p_{1}\\big(1-p_{1}\\big),\n$$\n$$\n\\frac{\\partial J}{\\partial \\theta_{s_{2}}}\n= (1-p_{0})\\cdot \\big(-2\\big)\\,\\frac{\\mathrm{d}p_{2}}{\\mathrm{d}\\theta_{s_{2}}}\n= -2(1-p_{0})\\,p_{2}\\big(1-p_{2}\\big).\n$$\n\n现在代入给定的 $\\theta=\\big(0.3,-0.4,0.8\\big)$：\n$$\np_{0}=\\sigma(0.3)=\\frac{1}{1+\\exp(-0.3)}\\approx 0.574442516,\\quad\np_{1}=\\sigma(-0.4)=\\frac{1}{1+\\exp(0.4)}\\approx 0.401312339,\\quad\np_{2}=\\sigma(0.8)=\\frac{1}{1+\\exp(-0.8)}\\approx 0.689974481.\n$$\n另外，\n$$\np_{0}\\big(1-p_{0}\\big)\\approx 0.244458311,\\quad\np_{1}\\big(1-p_{1}\\big)\\approx 0.240260745,\\quad\np_{2}\\big(1-p_{2}\\big)\\approx 0.213909696.\n$$\n因此，精确的策略梯度为\n$$\n\\frac{\\partial J}{\\partial \\theta_{s_{0}}}\n\\approx 0.244458311\\cdot\\big(2\\cdot 0.401312339 - 3 + 2\\cdot 0.689974481\\big)\n= 0.244458311\\cdot(-0.81742636)\\approx -0.199826659,\n$$\n$$\n\\frac{\\partial J}{\\partial \\theta_{s_{1}}}\n\\approx 2\\cdot 0.574442516\\cdot 0.240260745\\approx 0.276031974,\n$$\n$$\n\\frac{\\partial J}{\\partial \\theta_{s_{2}}}\n\\approx -2\\cdot 0.425557484\\cdot 0.213909696\\approx -0.182061746.\n$$\n所以，\n$$\n\\nabla_{\\theta} J(\\theta)\\approx\n\\begin{pmatrix}\n-0.199826659  0.276031974  -0.182061746\n\\end{pmatrix}.\n$$\n\n接下来，使用不带基线的似然比 (REINFORCE) 形式计算蒙特卡洛 (MC) 估计。对于伯努利策略 $\\pi_{\\theta}(1 \\mid s)=\\sigma(\\theta_{s})$ 和 $\\pi_{\\theta}(0 \\mid s)=1-\\sigma(\\theta_{s})$，在状态 $s$ 处采取动作 $a\\in\\{0,1\\}$ 的对数概率梯度是\n$$\n\\nabla_{\\theta_{s}}\\ln \\pi_{\\theta}(a \\mid s)\n= \\frac{\\partial}{\\partial \\theta_{s}}\\Big(a\\ln \\sigma(\\theta_{s}) + (1-a)\\ln\\big(1-\\sigma(\\theta_{s})\\big)\\Big)\n= a - \\sigma(\\theta_{s}),\n$$\n且对于 $s'\\neq s$，其关于 $\\theta_{s'}$ 的梯度为 $0$。\n\n因为唯一的非零奖励发生在 $t=1$ 时，所以两个时间步的未来回报 (return-to-go) 都是终止奖励，即 $G_{0}=G_{1}=r_{\\text{terminal}}$。对于每个幕，其梯度贡献为\n$$\ng^{(k)} = \\sum_{t=0}^{1} \\nabla_{\\theta}\\ln \\pi_{\\theta}(a_{t}^{(k)} \\mid s_{t}^{(k)})\\,G_{t}^{(k)}\n= r^{(k)}\\cdot\n\\begin{pmatrix}\na_{0}^{(k)}-p_{0} \\\\\n\\mathbb{I}\\{s_{1}\\text{ visited}\\}\\cdot(a_{1}^{(k)}-p_{1}) \\\\\n\\mathbb{I}\\{s_{2}\\text{ visited}\\}\\cdot(a_{1}^{(k)}-p_{2})\n\\end{pmatrix}.\n$$\n我们使用 $p_{0}\\approx 0.574442516$，$p_{1}\\approx 0.401312339$，$p_{2}\\approx 0.689974481$。\n\n计算每个幕的贡献：\n- 幕 1 ($s_{0}\\to s_{1}$, $a_{0}=1$, $a_{1}=0$, $r=0$):\n$$\ng^{(1)}=0\\cdot\n\\begin{pmatrix}\n1-p_{0} \\\\\n0-p_{1} \\\\\n0\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n0 \\\\ 0 \\\\ 0\n\\end{pmatrix}.\n$$\n- 幕 2 ($s_{0}\\to s_{2}$, $a_{0}=0$, $a_{1}=0$, $r=3$):\n$$\ng^{(2)}=3\\cdot\n\\begin{pmatrix}\n0-p_{0} \\\\\n0 \\\\\n0-p_{2}\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n-1.723327548 \\\\ 0 \\\\ -2.069923444\n\\end{pmatrix}.\n$$\n- 幕 3 ($s_{0}\\to s_{1}$, $a_{0}=1$, $a_{1}=1$, $r=2$):\n$$\ng^{(3)}=2\\cdot\n\\begin{pmatrix}\n1-p_{0} \\\\\n1-p_{1} \\\\\n0\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n0.851114968 \\\\ 1.197375322 \\\\ 0\n\\end{pmatrix}.\n$$\n- 幕 4 ($s_{0}\\to s_{2}$, $a_{0}=0$, $a_{1}=1$, $r=1$):\n$$\ng^{(4)}=1\\cdot\n\\begin{pmatrix}\n0-p_{0} \\\\\n0 \\\\\n1-p_{2}\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n-0.574442516 \\\\ 0 \\\\ 0.310025519\n\\end{pmatrix}.\n$$\n- 幕 5 ($s_{0}\\to s_{1}$, $a_{0}=1$, $a_{1}=1$, $r=2$):\n$$\ng^{(5)}=\n\\begin{pmatrix}\n0.851114968 \\\\ 1.197375322 \\\\ 0\n\\end{pmatrix}\n\\quad\\text{(与幕 3 相同)}.\n$$\n\n对 $K=5$ 个幕的结果求平均，以获得 MC 估计：\n$$\n\\widehat{g}\n=\\frac{1}{5}\\sum_{k=1}^{5} g^{(k)}\n=\\frac{1}{5}\n\\begin{pmatrix}\n-0.595540128 \\\\\n2.394750644 \\\\\n-1.759897925\n\\end{pmatrix}\n\\approx\n\\begin{pmatrix}\n-0.119108026  0.478950129  -0.351979585\n\\end{pmatrix}.\n$$\n\n计算差值的欧几里得范数：\n$$\n\\Delta = \\widehat{g} - \\nabla_{\\theta} J(\\theta)\n\\approx\n\\begin{pmatrix}\n-0.119108026 - (-0.199826659) \\\\\n0.478950129 - 0.276031974 \\\\\n-0.351979585 - (-0.182061746)\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n0.080718633 \\\\\n0.202918155 \\\\\n-0.169917839\n\\end{pmatrix}.\n$$\n那么\n$$\n\\|\\Delta\\|_{2}\n=\\sqrt{(0.080718633)^{2}+(0.202918155)^{2}+(-0.169917839)^{2}}\n\\approx \\sqrt{0.076567355}\\approx 0.276708.\n$$\n\n四舍五入到四位有效数字，得到 $0.2767$，符合要求。", "answer": "$$\\boxed{0.2767}$$", "id": "2738661"}]}