## 引言
传统上，新材料的发现是一个依赖直觉、反复试错的漫长过程，其成本高昂且效率低下。机器学习的崛起为[材料科学](@entry_id:152226)带来了[范式](@entry_id:161181)转变的希望，它能够从海量数据中学习规律，从而以惊人的速度预测材料性质并筛选候选结构。然而，一个核心的挑战阻碍了这一潜力的完全释放：如何构建不仅能学习数据模式，更能“理解”并遵守支配原子世界基本物理定律的[机器学习模型](@entry_id:262335)？简单地将现有算法应用于材料数据，往往会因违背基本的对称性原理而导致物理上荒谬的预测。

本文旨在系统性地解决这一知识鸿沟，为读者铺设一条从基础物理原理到前沿机器学习应用的完整路径。我们将通过三个章节，循序渐进地引导您掌握机器学习在[材料发现](@entry_id:159066)领域的核心方法与思想。

在“原理与机制”一章中，我们将深入探讨机器学习在[材料科学](@entry_id:152226)中的基石，解释如何将平移、旋转和[置换](@entry_id:136432)等物理对称性编码到模型中，并介绍构建原子结构表征的关键方法。接下来，在“应用与跨学科连接”一章中，我们将展示这些原理在实际研究中的应用，涵盖从数据处理、代理模型构建到主动学习和[逆向设计](@entry_id:158030)等贯穿整个发现流程的案例。最后，“动手实践”部分将提供具体练习，让您将理论知识付诸实践，巩固对核心概念的理解。通过本文的学习，您将能够构建和应用尊重物理规律的[机器学习模型](@entry_id:262335)，从而真正加速新材料的发现进程。

## 原理与机制

在[材料科学](@entry_id:152226)的机器学习探索中，其核心挑战在于如何将物理定律的对称性与约束，无缝地融入到数据驱动的模型中。一个成功的[机器学习模型](@entry_id:262335)不仅要能从数据中学习，更要能“理解”并遵守支配原子尺度世界的基本物理原理。本章将深入探讨机器学习在[材料发现](@entry_id:159066)领域的几个核心原理与机制，内容涵盖：如何构建能够表征材料并尊重物理对称性的描述符；如何定义具有物理意义的学习目标；以及如何设计能够可靠预测这些目标的模型，并量化其预测的不确定性。

### 表征中的不变性与[等变性](@entry_id:636671)原理

任何物理系统的描述都应该是客观的，不应依赖于观察者任意选择的[坐标系](@entry_id:156346)或对无法区分的粒子所做的任意标记。这是物理学中的一个基本要求，也必须是任何[用于材料科学的机器学习](@entry_id:161990)模型的基石。这一要求在数学上表现为对特定对称性操作的**[不变性](@entry_id:140168) (invariance)** 或 **[等变性](@entry_id:636671) (equivariance)**。

对于一个原子系统，最重要的对称性操作包括：

1.  **平移 (Translation)**：将整个系统在空间中刚性移动。
2.  **旋转 (Rotation)**：将整个系统围绕某一点或某一轴进行刚性旋转。
3.  **[置换](@entry_id:136432) (Permutation)**：交换两个或多个同种原子的标签。

一个物理性质如何响应这些操作，决定了它所需要满足的对称性。

**不变性**指的是一个量在对称性操作下保持不变。例如，一个分子的总势能是一个标量，它不应随着分子的平移或旋转而改变。同样，如果我们交换两个完全相同的氢原子的标签，分子的能量也应保持不变。因此，能量是一个在欧几里得群（平移和旋转）和同种原子[置换](@entry_id:136432)下均保持不变的量。

**[等变性](@entry_id:636671)**则描述了一个矢量或张量性质如何随[坐标系](@entry_id:156346)变换而相应地变换。力就是一个典型的例子。力是一个矢量，具有大小和方向。如果我们旋转一个原子系统，作用在每个原子上的力矢量也必须随之旋转，以保持其在系统内部的相对方向（例如，始终指向某个[化学键](@entry_id:138216)的方向）。从第一性原理出发，我们可以严格推导出这个变换关系。系统的势能 $V$ 在[刚体运动](@entry_id:193355)下是不变的。力作为[势能](@entry_id:748988)对原子位置的负梯度 $\mathbf{F}_i = -\nabla_{\mathbf{r}_i} V$，其变换规律可以通过[链式法则](@entry_id:190743)推导。对于一个由[正交变换](@entry_id:155650) $Q \in O(3)$ 和平移 $\mathbf{t} \in \mathbb{R}^3$ 构成的变换，原子位置变为 $\mathbf{r}_i' = Q\mathbf{r}_i + \mathbf{t}$。可以证明，新的力矢量 $\mathbf{F}_i'$ 与旧的力矢量 $\mathbf{F}_i$ 之间的关系为 $\mathbf{F}_i' = Q\mathbf{F}_i$。因此，一个预测力的模型 $\mathcal{F}$ 必须满足如下的 $E(3)$ [等变性](@entry_id:636671)条件 [@problem_id:2838022]：
$$
\mathcal{F}\left(\{Q \mathbf{r}_i + \mathbf{t}\}_{i=1}^N\right) = \{Q \mathbf{F}_i\}_{i=1}^N
$$
请注意，平移向量 $\mathbf{t}$ 并未出现在右侧，这表明力矢量在整个系统平移时保持不变。

对于周期性晶体，除了上述对称性外，还必须引入另外两条：

4.  **周期性 (Periodicity)**：晶体的物理描述不应因选择不同的[晶胞](@entry_id:143489)作为代表而改变。这意味着将任何原子移动一个[晶格矢量](@entry_id:161583) $\mathbf{R}$，物理性质应保持不变。
5.  **[晶格](@entry_id:196752)旋转**：对晶体进行旋转时，必须同时旋转原子坐标 $\{\mathbf{r}_i\}$ 和定义周期性边界的[晶格](@entry_id:196752)矩阵 $H$。

一个为周期性晶体设计的、严谨的描述符 $D$ 必须同时满足所有这些不变性要求。例如，对于一个将晶体 $(H, \{\mathbf{r}_i\}, \{Z_i\})$ 映射到一个[特征向量](@entry_id:151813)的描述符，它必须满足以下条件 [@problem_id:2837973]：
- 平移不变性: $D(H, \{\mathbf{r}_i + \boldsymbol{\tau}\}, \{Z_i\}) = D(H, \{\mathbf{r}_i\}, \{Z_i\})$
- [旋转不变性](@entry_id:137644): $D(QH, \{Q\mathbf{r}_i\}, \{Z_i\}) = D(H, \{\mathbf{r}_i\}, \{Z_i\})$
- 周期性[不变性](@entry_id:140168): $D(H, \{\mathbf{r}_i + \mathbf{R}_i\}, \{Z_i\}) = D(H, \{\mathbf{r}_i\}, \{Z_i\})$ (其中 $\mathbf{R}_i$ 是任意[晶格矢量](@entry_id:161583))
- [置换不变性](@entry_id:753356): $D(H, \{\mathbf{r}_{\pi(i)}\}, \{Z_{\pi(i)}\}) = D(H, \{\mathbf{r}_i\}, \{Z_i\})$ (其中 $\pi$ 是同种原子间的[置换](@entry_id:136432))

这些不变性和[等变性](@entry_id:636671)原理是构建任何可靠的原子尺度[机器学习模型](@entry_id:262335)的出发点和基本约束。

### 构建[不变性](@entry_id:140168)描述符：从“手工制作”到自动学习

为了让[机器学习模型](@entry_id:262335)处理[原子结构](@entry_id:137190)，我们必须首先将这些结构——即[原子核](@entry_id:167902)的位置和种类——转化为固定长度的数字向量或张量，这就是所谓的**描述符 (descriptor)** 或 **表征 (representation)**。这一步至关重要，因为它直接决定了模型能够“看到”哪些信息，以及它是否能够自然地遵守前述的物理对称性。

#### 手工制作的局部环境描述符

一种经典的方法是将整个材料分解为一系列**局部原[子环](@entry_id:154194)境 (local atomic environments)** 的集合，即以每个原子为中心，考虑其一定半径范围内的邻近原子。然后为每个这样的局部环境计算一个不变性描述符。

一个早期的、针对分子的简单例子是**库仑矩阵 (Coulomb Matrix)** [@problem_id:2838013]。对于一个有 $N$ 个原子的分子，其库仑矩阵 $C \in \mathbb{R}^{N \times N}$ 的元素定义为：
- 非对角元: $C_{ij} = \frac{Z_i Z_j}{\|\mathbf{r}_i - \mathbf{r}_j\|}$，代表原子 $i$ 和 $j$ 之间的[静电排斥](@entry_id:162128)。
- 对角元: $C_{ii} = 0.5 Z_i^{2.4}$，是对孤立原子能量的拟合。

由于原子间的距离 $\|\mathbf{r}_i - \mathbf{r}_j\|$ 在刚性平移和旋转下是不变的，因此库仑矩阵的每一个元素也都是不变的。然而，如果我们交换两个原子的标签（例如，原子 $k$ 和 $l$），矩阵的行和列会相应地交换，导致矩阵本身发生变化 ($C' \neq C$)。这意味着库仑矩阵本身并不满足[置换不变性](@entry_id:753356)。但是，矩阵的**[特征值](@entry_id:154894)谱 (eigenvalue spectrum)** 是在相似变换下不变的。原子标签的[置换](@entry_id:136432)操作等价于对库仑矩阵进行一个[相似变换](@entry_id:152935) $C' = P C P^\top$（其中 $P$ 是一个[置换矩阵](@entry_id:136841)），因此其[特征值](@entry_id:154894)集合保持不变。通过将排序后的[特征值](@entry_id:154894)向量作为描述符，我们便获得了一个同时满足平移、旋转和[置换不变性](@entry_id:753356)的表征。

一个更现代、更强大的例子是**平滑原子位置重叠 (Smooth Overlap of Atomic Positions, SOAP)** 方法 [@problem_id:2838023]。SOAP为每个原子周围的局部环境构建了一个高度信息丰富的描述符。其构建过程体现了严谨的[数学物理](@entry_id:265403)思想：
1.  **构建局部原子密度**：首先，在中心原子周围的每个邻居原子的位置上放置一个[高斯分布](@entry_id:154414)函数，然后将它们叠加起来，形成一个平滑、连续的邻域原子密度场 $\rho_i(\mathbf{r})$。
2.  **展开到[基函数](@entry_id:170178)**：接着，将这个三维密度场展开到一组正交的[径向基函数](@entry_id:754004) $g_n(r)$ 和球谐函数 $Y_{lm}(\hat{\mathbf{r}})$ 的乘积上。这就像对信号进行[傅里叶变换](@entry_id:142120)，将空间信息分解为不同“频率”的组合。展开系数为 $c^{(i)}_{nlm} = \int \rho_i(\mathbf{r}) g_n(r) Y_{lm}^*(\hat{\mathbf{r}}) d\mathbf{r}$。
3.  **构建[旋转不变量](@entry_id:170459)**：展开系数 $c^{(i)}_{nlm}$ 本身在旋转下是等变地变换的，而非不变。为了获得[旋转不变性](@entry_id:137644)，SOAP通过构建所谓的**[功率谱](@entry_id:159996) (power spectrum)** 来实现。功率谱的元素定义为 $p^{(i)}_{nn'l} = \sum_{m=-l}^{l} c^{(i)}_{nlm} c^{(i)}_{n'lm}^*$。这个量在数学上被证明是旋转不变的，它捕捉了原子密度在不同径向通道 ($n, n'$) 和角动量通道 ($l$) 上的关联信息。
4.  **定义核函数**：最后，两个原子环境 $i$ 和 $j$ 之间的相似性可以通过它们功率谱向量的[点积](@entry_id:149019)来度量，即 $k(i, j) = (\mathbf{p}^{(i)} \cdot \mathbf{p}^{(j)})^\zeta$。这个标量结果（称为SOAP核）可以被用在核回归等机器学习方法中。

SOAP的成功在于它将离散的原子坐标转换成一个平滑、可微且对旋转不变的表征，同时保留了丰富的结构信息。

#### 周期性系统的描述符

对于晶体这样的周期性系统，描述符的构建需要额外考虑周期性边界条件。

在**实空间 (real space)** 中，一种主流方法是构建**晶体图 (crystal graph)**，其中原子是节点，原子间的连接是边。这在[图神经网络 (GNNs)](@entry_id:750014) 中尤其重要。定义边的关键在于如何处理周期性。一个原子不仅与其主[晶胞](@entry_id:143489)内的邻居相互作用，还与邻近[晶胞](@entry_id:143489)中的周期性映像相互作用。为了找到最近的邻居，必须采用**最小映像约定 (Minimum-Image Convention, MIC)**。对于任意两个原子 $i$ 和 $j$，我们需要在原子 $j$ 的所有周期性映像中，找到一个与原子 $i$ 距离最近的。在构建晶体图时，如果这个最小距离小于某个[截断半径](@entry_id:136708) $r_c$，就在 $i$ 和 $j$ 之间建立一条边。至关重要的是，为了保留完整的几何信息，这条边必须附带一个属性，即用来找到最小映像的那个**[晶胞](@entry_id:143489)平移矢量 (cell shift vector)** $\mathbf{s}^\star \in \mathbb{Z}^3$ [@problem_id:2838004]。这个整数矢量指明了邻居原子 $j$ 来自于哪个周期性映像，这对于后续需要角度等信息的模型至关重要。

在**[倒易空间](@entry_id:754151) (reciprocal space)** 中，也可以构建出优雅满足所有对称性的描述符。晶体的衍射图案本质上就是其结构的倒易空间表征。一个基于此思想的描述符可以这样构造 [@problem_id:2837973]：对于晶体中每一种元素 $s$，计算其部分结构因子 $S_s(\mathbf{k}) = \sum_{i:Z_i=s} e^{i\mathbf{k} \cdot \mathbf{r}_i}$，其中 $\mathbf{k}$ 是[倒易晶格矢量](@entry_id:263351)。然后，计算其模的平方 $|S_s(\mathbf{k})|^2$，这是一个在平移和周期[性选择](@entry_id:138426)下不变的量。最后，通过对大小相近的 $\mathbf{k}$ 矢量进行径向平均（即只依赖于 $|\mathbf{k}|$），可以消除对旋转的依赖，从而得到一个完全不变的描述符。这种方法巧妙地利用了[傅里叶变换的性质](@entry_id:265641)，将所有几何对称性一次性处理。

#### 学习得到的表征：[消息传递神经网络](@entry_id:751916)

近年来，一个[范式](@entry_id:161181)上的转变是从依赖专家知识的“手工”描述符，转向让模型自己**学习 (learn)** 表征。**[消息传递神经网络](@entry_id:751916) (Message Passing Neural Networks, MPNNs)** 或[图神经网络 (GNNs)](@entry_id:750014) 就是这一[范式](@entry_id:161181)的代表。

在GNN中，每个原子（节点）都有一个初始[状态向量](@entry_id:154607)。在每一层网络中，每个原子会：(1) 从它的邻居那里接收“消息”；(2) 聚合这些消息；(3) 利用聚合后的消息来更新自己的状态。经过多轮[消息传递](@entry_id:751915)，每个原子的最终[状态向量](@entry_id:154607)就编码了其周围大范围内的化学环境信息。

早期的GNN模型在传递消息时，通常只考虑原子间的距离（一个标量），这是一种纯粹的**二体 (two-body)** 信息。这样的模型虽然满足[旋转不变性](@entry_id:137644)，但它们对**键角 (bond angles)** 等**[三体](@entry_id:265960) (three-body)** 信息不敏感。然而，键角在化学和[材料科学](@entry_id:152226)中至关重要，例如，它决定了分子的形状和材料的弹性。

为了解决这个问题，更先进的**[方向性](@entry_id:266095)消息传递 (directional message passing)** 模型被提出来，例如DimeNet [@problem_id:2837999]。这些模型的目标是让消息也依赖于方向，从而捕捉[三体](@entry_id:265960)关联。其核心思想与SOAP有异曲同工之妙：
1.  将连接中心原子 $j$ 和邻居原子 $i, k$ 的两个边矢量 $\mathbf{r}_{ji}$ 和 $\mathbf{r}_{jk}$ 用[球谐函数展开](@entry_id:188485)。
2.  通过对两个边的[球谐函数展开](@entry_id:188485)式进行特定的**张量积 (tensor product)** 或**缩并 (contraction)**，可以构造出在整体旋转下不变，但又明确依赖于两个矢量夹角 $\theta_{ijk}$ 的标量特征。例如，形如 $\sum_{m} Y_{\ell m}(\hat{\mathbf{r}}_{ji}) Y_{\ell m}^{\ast}(\hat{\mathbf{r}}_{jk})$ 的项，根据球谐函数的加法定理，它正比于勒让德多项式 $P_{\ell}(\cos\theta_{ijk})$。
3.  将这些依赖于角度的标量特征与依赖于距离的特征结合起来，形成一个包含三体信息的、旋转不变的消息。

通过这种方式，GNN能够学习到物理上至关重要的键角依赖性，从而在预测与角度相关的性质时表现得更出色。这体现了将物理先验知识（[三体](@entry_id:265960)相互作用的重要性）融入[神经网络架构](@entry_id:637524)设计的强大威力 [@problem_id:2837999]。

### 定义与预测物理性质

一个[机器学习模型](@entry_id:262335)由输入、模型本身和输出三部分组成。在讨论了如何构建输入（描述符）和模型架构之后，我们现在转向输出端：我们希望预测哪些物理性质，以及如何确保这些预测目标在物理上是有意义的。

#### [热力学稳定性](@entry_id:142877)与[形成能](@entry_id:142642)

在[材料发现](@entry_id:159066)中，最常见和最重要的任务之一是预测材料的**[热力学稳定性](@entry_id:142877) (thermodynamic stability)**。一个材料是否能在实验中被合成并稳定存在，首先取决于它的能量是否足够低。然而，绝对能量值本身意义不大，因为它们依赖于计算的零点。更有意义的是相对能量。

**原子平均[形成能](@entry_id:142642) (per-atom formation energy)**, $\Delta E_f$，是衡量一个化合[物相](@entry_id:196677)对于其构成元素稳定性的标准度量。对于一个[化学式](@entry_id:136318)为 $\{n_i\}$（$n_i$ 为元素 $i$ 的原子数）、总能量为 $E_{\text{tot}}$ 的化合物，其[形成能](@entry_id:142642)定义为 [@problem_id:2838012]：
$$
\Delta E_f = \frac{E_{\text{tot}} - \sum_i n_i \mu_i}{\sum_i n_i}
$$
这里，$\mu_i$ 是元素 $i$ 的**化学势 (chemical potential)**，在零温下，它就等于该元素最稳定单质相中每个原子的能量。这个公式的物理意义是：化合物的能量与其构成元素的能量之和的差值，再平均到每个原子上。

-   如果 $\Delta E_f  0$，说明形成该化合物是一个[放热过程](@entry_id:147168)，该化合[物相](@entry_id:196677)对于其构成元素是稳定的。
-   如果 $\Delta E_f > 0$，说明该化合物不稳定，会倾向于分解成其构成元素。

$\Delta E_f$ 是一个**[内含性质](@entry_id:181209) (intensive property)**，它不依赖于计算时所用的超晶胞大小。例如，将超晶胞扩大一倍，$E_{\text{tot}}$ 和 $n_i$ 都会加倍，但 $\Delta E_f$ 保持不变 [@problem_id:2838012]。

在构建和使用形成能数据集时，一个至关重要的、不可违背的原则是：**所有化合物的[形成能](@entry_id:142642)必须使用同一套一致的元素化学势 $\{\mu_i\}$ 来计算**。如果使用不一致的 $\mu_i$，就相当于在不同的能量参考基准下比较能量，这将导致稳定性排序的系统性错误，并完全破坏后续的稳定性分析 [@problem_id:2838012]。

有了可靠的[形成能](@entry_id:142642)数据后，我们可以通过构建**形成能[凸包](@entry_id:262864) (convex hull of formation energies)** 来判断在某个给定的化学体系中，哪些相是[热力学](@entry_id:141121)稳定相。在一个二元（如A-B）体系中，我们将所有已知化合物的 $(x, \Delta E_f(x))$ 点（其中 $x$ 是组分，$\Delta E_f$ 是[形成能](@entry_id:142642)）绘制在图上。所有稳定相将构成这些点的**下凸包络线**。任何位于凸包上方的相都是亚稳或不稳定的。

在机器学习辅助的材料筛选中，一个非常有用的稳定性度量是**到[凸包](@entry_id:262864)的距离 (distance to the convex hull)**，也称为**分解能 (decomposition energy)** $\Delta E_d$ [@problem_id:2837961]。对于一个新预测的候选相，其坐标为 $(x^\ast, \Delta E_f^\text{cand})$，它到凸包的距离被定义为其能量与凸包在该组分 $x^\ast$ 处能量的差值：
$$
\Delta E_d = \Delta E_f^\text{cand}(x^\ast) - \Delta E_f^\text{hull}(x^\ast)
$$
$\Delta E_f^\text{hull}(x^\ast)$ 是在凸包上组分为 $x^\ast$ 时的能量。如果 $x^\ast$ 位于两个稳定相 $x_1$ 和 $x_2$ 之间，那么 $\Delta E_f^\text{hull}(x^\ast)$ 就是连接这两个稳定相的**[连接线](@entry_id:196944) (tie-line)** 在 $x^\ast$ 处的值，代表了由这两个稳定相通过杠杆定则混合而成的平衡态的能量。$\Delta E_d > 0$ 表示该候选相是亚稳的，其值的大小量化了亚稳的程度，即它分解为更稳定的[相混合](@entry_id:199798)物时会释放的能量。

#### 预测矢量性质：力

除了预测能量这样的标量，机器学习模型还可以被训练来预测矢量性质，最典型的就是作用在每个原子上的**力 (force)**。能够同时准确预测能量和力的模型被称为**[机器学习力场](@entry_id:192895) (machine-learned force field)**，它们是进行大规模分子动力学模拟的强大工具。

如前所述，力是矢量，预测力的模型必须是 **$E(3)$-等变的** [@problem_id:2838022]。这意味着当输入结构被旋转时，输出的力矢量也必须以完全相同的方式旋转。

使用[第一性原理计算](@entry_id:198754)（如密度泛函理论，DFT）得到的力作为训练数据有一个深刻的理论依据。[力场](@entry_id:147325)模型在物理上必须是**保守的 (conservative)**，即力必须是某个[标量势](@entry_id:276177)能面的负梯度，$\mathbf{F} = -\nabla E$。那么，DFT计算出的力是否满足这个条件呢？答案是肯定的，这可以由**海尔曼-费曼定理 (Hellmann-Feynman theorem)** 保证 [@problem_id:2837976]。

该定理表明，如果一个体系的[波函数](@entry_id:147440)是[哈密顿量](@entry_id:172864)的[精确本征态](@entry_id:138620)，那么能量对某个参数的导数就等于[哈密顿量](@entry_id:172864)对该参数导数的[期望值](@entry_id:153208)。在DFT计算的实践中，这意味着只要满足以下关键条件，计算出的力就是DFT总能量对原子位置的精确解析梯度：
1.  **自洽收敛**：电子密度必须是完全变分优化的（即达到[自洽场](@entry_id:136549)收敛）。
2.  **包含[Pulay力](@entry_id:167194)**：如果使用的[基函数](@entry_id:170178)（如原子中心[轨道](@entry_id:137151)）依赖于原子位置，那么在求导时必须包含因此产生的额外项，即[Pulay力](@entry_id:167194)。对于不依赖于原子位置的[平面波基组](@entry_id:178287)，此项为零。
3.  **计算参数一致性**：所有计算，如[k点](@entry_id:168686)网格、[截断能](@entry_id:177594)、赝势等，都必须一致。

在满足这些条件时，DFT计算出的力是其自身定义的总[能量势](@entry_id:748988)能面的精确梯度，因此是保守的。这为使用DFT的力和能量数据来训练一个保守的[机器学习力场](@entry_id:192895)提供了坚实的理论基础 [@problem_id:2837976]。

### 量化预测的不确定性

任何模型预测都存在不确定性。在[材料发现](@entry_id:159066)中，一个好的模型不仅应该给出准确的预测值，还应该告诉我们它对自己预测的“信心”有多大。这种**[不确定性量化](@entry_id:138597) (Uncertainty Quantification, UQ)** 对于指导实验、进行[主动学习](@entry_id:157812)以及评估模型的可靠性至关重要。

在机器学习中，预测不确定性通常被分解为两种类型 [@problem_id:2837997]：

1.  **[偶然不确定性](@entry_id:154011) (Aleatoric Uncertainty)**：源于数据本身的[固有噪声](@entry_id:261197)或随机性。在我们的情境下，即使是DFT计算本身也存在“噪声”，例如，由于有限的[k点](@entry_id:168686)网格或[能量截断](@entry_id:177594)导致的数值误差，或来自不同文献的数据使用了不同的计算设置。这种不确定性是不可约减的，即使拥有无限多的数据，它依然存在。它反映了我们所能获得的“[真值](@entry_id:636547)”本身固有的模糊性。

2.  **[认知不确定性](@entry_id:149866) (Epistemic Uncertainty)**：源于模型本身的局限性，特别是由于训练数据有限而导致模型参数的不确定。当模型被要求对它在训练阶段从未见过或很少见过的化学空间区域进行预测时，它的[认知不确定性](@entry_id:149866)就会很高。这种不确定性是可约减的——随着训练数据的增加，模型变得更有“见识”，其[认知不确定性](@entry_id:149866)就会下降。

区分这两种不确定性非常有用。高的[认知不确定性](@entry_id:149866)是一个信号，表明模型正在进行外插，其预测结果可能不可靠。这正是**主动学习 (active learning)** 策略所利用的：我们优先选择那些模型认知不确定性最高的候[选材](@entry_id:161179)料进行下一步的昂贵DFT计算，从而最高效地扩充数据集和提升模型性能。

在实践中，有多种方法可以估计这两种不确定性。

一种强大且常用的方法是**[深度集成](@entry_id:636362)学习 (Deep Ensembles)** [@problem_id:2837997]。该方法通过训练多个（例如5-10个）结构相同但权重独立初始化的模型来工作。为了进一步增加模型的多样性，每个模型通常在训练数据的不同**自助法[重采样](@entry_id:142583) (bootstrap resample)** [子集](@entry_id:261956)上进行训练。对于一个新的预测点 $x$，集成模型会给出多个不同的预测结果。
-   **认知不确定性** 可以通过集成模型预测均值的**[方差](@entry_id:200758)**来估计。如果所有模型都给出相似的预测，说明它们对这个点的“看法”一致，[认知不确定性](@entry_id:149866)低。反之，如果预测结果分歧很大，则[认知不确定性](@entry_id:149866)高。
-   **[偶然不确定性](@entry_id:154011)** 则可以通过让每个模型除了预测能量值外，还额外预测一个数据[方差](@entry_id:200758)（即所谓的异[方差](@entry_id:200758)模型），然后取所有模型预测[方差](@entry_id:200758)的**平均值**来估计。

另一种流行的技术是**蒙特卡洛失活 ([Monte Carlo Dropout](@entry_id:636300))** [@problem_id:2837997]。它通过在测试阶段多次（例如几十次）开启[神经网](@entry_id:276355)络的失活（dropout）层，进行随机[前向传播](@entry_id:193086)，从而得到一系列不同的预测结果。这些预测结果的[方差](@entry_id:200758)同样可以用来估计认知不确定性。

总而言之，通过对不确定性进行分解和量化，我们不仅能够评估单次预测的可靠性，还能建立起一个智能化的反馈循环，让[机器学习模型](@entry_id:262335)能够主动地指导我们去探索广阔而未知的材料化学空间。