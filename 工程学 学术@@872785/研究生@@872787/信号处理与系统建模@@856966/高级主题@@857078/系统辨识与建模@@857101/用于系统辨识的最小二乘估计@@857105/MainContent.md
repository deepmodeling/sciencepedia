## 引言
在科学与工程的众多领域中，从观测数据中构建描述动态系统行为的数学模型是一项核心任务，这一过程被称为[系统辨识](@entry_id:201290)。在众多辨识方法中，最小二乘（Least Squares, LS）估计法因其简洁的数学形式、强大的解释能力和广泛的适用性，成为了一个基石性的工具。然而，从教科书中的基本公式到成功应用于复杂的实际问题之间，存在着一条充满挑战的道路。工程师和研究者不仅需要知道如何求解最小二乘问题，更需要深刻理解其有效性的前提条件、潜在的陷阱以及应对这些挑战的先进策略。

本文旨在系统性地梳理用于系统辨识的[最小二乘估计](@entry_id:262764)，弥合理论与实践之间的鸿沟。我们将超越基础的[正规方程](@entry_id:142238)推导，深入探讨那些决定辨识成败的关键细节。读者将通过本文学习到：

在“**原理与机制**”一章中，我们将奠定坚实的理论基础。您将学习如何将典型的动态系统（如[ARX模型](@entry_id:269528)）转化为最小二乘法所要求的线性回归形式，推导出[最小二乘估计](@entry_id:262764)器，并探讨保证[解的唯一性](@entry_id:143619)（[持续激励](@entry_id:263834)）和一致性（噪声正交性）所需的关键条件。此外，本章还将剖析噪声特性和数据[共线性](@entry_id:270224)等实际问题如何影响估计结果，并介绍相应的数值稳定性考量。

接着，在“**应用与交叉学科联系**”一章中，我们将视野扩展到完整的辨识工作流和更高级的应用场景。您将探索如何进行[模型验证](@entry_id:141140)与结构选择，如何通过正则化（如LASSO）处理高维或[病态问题](@entry_id:137067)，以及如何使用递归最小二乘（RLS）算法在线跟踪[时变系统](@entry_id:175653)。本章还将展示[最小二乘法](@entry_id:137100)如何被巧妙地应用于[MIMO系统](@entry_id:268566)和具有挑战性的[闭环辨识](@entry_id:199122)问题，并通过在控制设计和生理学建模中的案例，彰显其在不同学科中的强大威力。

最后，“**动手实践**”部分将通过一系列精心设计的问题，引导您将理论知识转化为实践技能。您将亲手分析为何某些输入信号无法有效辨识系统，如何通过[数据预处理](@entry_id:197920)来应对[相关噪声](@entry_id:137358)，并体会[数值条件](@entry_id:136760)对求解精度的影响，从而为解决真实世界的系统辨识问题做好准备。

## 原理与机制

在系统辨识领域，我们的核心目标是从观测到的输入-输出数据中推断出描述系统动态行为的数学模型。[最小二乘法](@entry_id:137100)（Least Squares, LS）及其变体为这一任务提供了一个强大且应用广泛的框架。本章将深入探讨[最小二乘估计](@entry_id:262764)的基本原理和核心机制，阐述其理论基础、实际挑战以及数值实现方面的关键考量。我们将从如何将动态系统方程转化为线性回归问题开始，逐步揭示唯一、一致估计所需的条件，并最终讨论处理非理想噪声和保证数值稳定性的高级策略。

### [线性回归](@entry_id:142318)模型的构建

系统辨识的第一步是将描述动态系统的时域关系——通常是[差分方程](@entry_id:262177)——重构为一个标准的[线性回归](@entry_id:142318)形式。这种形式是应用[最小二乘法](@entry_id:137100)的前提。

#### ARX 模型：一个典型的例子

一个被广泛研究的模型是**带外部输入的[自回归模型](@entry_id:140558)**（**Auto-Regressive with eXogenous input, ARX**）。对于一个单输入单输出（SISO）离散时间系统，一个典型的 ARX 模型可以表示为以下[差分方程](@entry_id:262177)：
$$
y(t) + \sum_{i=1}^{n_a} a_i y(t - i) = \sum_{j=1}^{n_b} b_j u(t - n_k - j + 1) + e(t)
$$
其中，$y(t)$ 是在时间点 $t$ 的系统输出，$u(t)$ 是输入，$e(t)$ 是一个零均值的白噪声扰动项，代表了未建模的动态或测量噪声。整数 $n_a$ 和 $n_b$ 分别是自回归（AR）和外部输入（X）部分的阶数，$n_k$ 是系统的输入延迟（或称死区时间）。参数 $\{a_i\}$ 和 $\{b_j\}$ 是我们需要从数据中估计的未知系统系数。

为了应用最小二乘法，我们必须将此方程改写为 **线性于参数** 的形式。这意味着，模型的输出预测值应该是未知参数的线性组合。为此，我们将 $y(t)$ 分离到等式的一边 [@problem_id:2880107]：
$$
y(t) = - \sum_{i=1}^{n_a} a_i y(t - i) + \sum_{j=1}^{n_b} b_j u(t - n_k - j + 1) + e(t)
$$
这个方程揭示了当前输出 $y(t)$ 是由两部分组成的：一部分是可预测的，依赖于过去的输入和输出；另一部分是不可预测的随机扰动 $e(t)$。可预测的部分，即 **一步向前预测器**（**one-step-ahead predictor**），定义为 $\hat{y}(t|\theta) = E[y(t) | \text{过去数据}]$。在白噪声假设下，我们得到：
$$
\hat{y}(t|\theta) = - \sum_{i=1}^{n_a} a_i y(t - i) + \sum_{j=1}^{n_b} b_j u(t - n_k - j + 1)
$$
这个预测器可以简洁地写成向量[内积](@entry_id:158127)的形式 $\hat{y}(t|\theta) = \varphi(t)^\top \theta$。为此，我们定义：

1.  **参数向量** ($\theta$)：一个列向量，包含了所有待估计的未知参数。
    $$
    \theta = \begin{bmatrix} a_1  \cdots  a_{n_a}  b_1  \cdots  b_{n_b} \end{bmatrix}^\top \in \mathbb{R}^{n_a + n_b}
    $$

2.  **回归向量** ($\varphi(t)$)：一个列向量，包含了在时间点 $t$ 用来进行预测的、已知的或已测量的数据。这些数据被称为回归量（regressors）。
    $$
    \varphi(t) = \begin{bmatrix} -y(t-1)  \cdots  -y(t-n_a)  u(t-n_k)  \cdots  u(t - n_k - n_b + 1) \end{bmatrix}^\top \in \mathbb{R}^{n_a + n_b}
    $$
    请注意，与过去输出 $y(t-i)$ 相关的回归量带有负号，这是因为它们在原始方程中与 $y(t)$ 位于等式的同一侧。

通过这些定义，原始的 ARX 模型在每个时间点 $t$ 都可以表示为一个标量[线性回归](@entry_id:142318)方程：
$$
y(t) = \varphi(t)^\top \theta + e(t)
$$

#### 数据矩阵的构建

为了利用一批 $N$ 个观测数据 $\{y(t), u(t)\}_{t=1}^N$ 进行估计，我们将多个时间点的标量方程堆叠成一个矩阵方程。然而，我们不能使用所有 $N$ 个数据点。回归向量 $\varphi(t)$ 需要用到过去的数据，例如 $y(t-n_a)$ 和 $u(t - n_k - n_b + 1)$。为了确保所有回归量都是可用的（即其时间索引不小于 1），我们必须从一个起始时间点 $t_0$ 开始构建方程，该起始点 $t_0$ 必须足够大，以满足 $t_0 - n_a \ge 1$ 和 $t_0 - n_k - n_b + 1 \ge 1$。因此，第一个有效的时间点是 $t_0 = \max(n_a, n_k + n_b - 1) + 1$ [@problem_id:2880107]。

从 $t_0$ 到 $N$ 堆叠所有 $M = N - t_0 + 1$ 个有效方程，我们得到矩阵形式的[线性回归](@entry_id:142318)模型：
$$
Y = \Phi \theta + E
$$
其中：
-   $Y = \begin{bmatrix} y(t_0)  y(t_0+1)  \cdots  y(N) \end{bmatrix}^\top$ 是 $M \times 1$ 的输出向量。
-   $\Phi = \begin{bmatrix} \varphi(t_0)^\top \\ \varphi(t_0+1)^\top \\ \vdots \\ \varphi(N)^\top \end{bmatrix}$ 是 $M \times (n_a+n_b)$ 的 **回归矩阵**（或[设计矩阵](@entry_id:165826)）。
-   $E = \begin{bmatrix} e(t_0)  e(t_0+1)  \cdots  e(N) \end{bmatrix}^\top$ 是 $M \times 1$ 的扰动向量。

这个 $Y = \Phi \theta + E$ 的形式是[最小二乘估计](@entry_id:262764)的出发点。值得一提的是，**有限冲激响应**（**Finite Impulse Response, FIR**）模型是 ARX 模型的一个特例，其中 $n_a=0$。在这种情况下，输出仅依赖于过去的输入，回归向量只包含 $u$ 的滞后项，$\varphi(t) = [u(t-n_k) \cdots u(t-n_k-n_b+1)]^\top$ [@problem_id:2880148]。

### [最小二乘估计](@entry_id:262764)器

有了线性回归模型 $Y = \Phi \theta + E$，我们的目标是找到一个[参数估计](@entry_id:139349)值 $\hat{\theta}$，使得模型的预测值 $\hat{Y} = \Phi\hat{\theta}$ 与实际观测值 $Y$ 之间的差异最小。[最小二乘法](@entry_id:137100)通过最小化 **[残差平方和](@entry_id:174395)**（**Residual Sum of Squares, RSS**）来实现这一目标。

[残差向量](@entry_id:165091)定义为 $\epsilon = Y - \hat{Y} = Y - \Phi\hat{\theta}$。RSS，即最小二乘的[代价函数](@entry_id:138681) $J(\hat{\theta})$，是[残差向量](@entry_id:165091)的[欧几里得范数](@entry_id:172687)的平方：
$$
J(\hat{\theta}) = \|\epsilon\|_2^2 = \|Y - \Phi\hat{\theta}\|_2^2 = \sum_{t=t_0}^{N} \left(y(t) - \varphi(t)^\top \hat{\theta}\right)^2
$$
我们可以通过一个具体的数值例子来理解这个代价函数。假设一个二阶 ARX 模型经估计后得到参数 $\hat{\theta} = \begin{pmatrix} 0.6  -0.2  0.9  0.1 \end{pmatrix}^\top$。对于一组给定的输入输出数据，我们可以计算每个时间点的预测值 $\hat{y}(t | \hat{\theta}) = \varphi(t)^\top \hat{\theta}$，进而计算出[预测误差](@entry_id:753692) $y(t) - \hat{y}(t | \hat{\theta})$。将这些误差的平方加总，便得到该[参数估计](@entry_id:139349) $\hat{\theta}$ 在这组数据上的 RSS 值 [@problem_id:2880108]。

为了找到最小化 $J(\hat{\theta})$ 的 $\hat{\theta}$，我们对 $J(\hat{\theta})$ 关于 $\hat{\theta}$ 求梯度并令其为零：
$$
\nabla_{\hat{\theta}} J(\hat{\theta}) = -2 \Phi^\top (Y - \Phi\hat{\theta}) = 0
$$
整理后得到著名的 **正规方程**（**Normal Equations**）：
$$
(\Phi^\top \Phi) \hat{\theta} = \Phi^\top Y
$$
如果矩阵 $\Phi^\top \Phi$ 是可逆的，我们就可以得到[最小二乘估计](@entry_id:262764)的闭式解：
$$
\hat{\theta}_{LS} = (\Phi^\top \Phi)^{-1} \Phi^\top Y
$$
这个解在几何上对应着将输出向量 $Y$ [正交投影](@entry_id:144168)到由回归矩阵 $\Phi$ 的列向量所张成的[子空间](@entry_id:150286)上。

### 唯一解与一致性的条件

虽然我们得到了 $\hat{\theta}_{LS}$ 的表达式，但一个有效的估计器不仅需要能够计算出来，还必须具备良好的统计性质。具体而言，我们关心两个核心问题：
1.  对于给定的数据集，估计值是否 **唯一**？
2.  当数据量趋于无穷时，估计值是否 **收敛** 于真实的系统参数？

#### 唯一性：[持续激励](@entry_id:263834)与[可辨识性](@entry_id:194150)

从[正规方程](@entry_id:142238)可以看出，要得到唯一的[最小二乘解](@entry_id:152054)，矩阵 $\Phi^\top \Phi$ 必须是可逆的（即非奇异）。因为 $\Phi$ 是一个 $M \times (n_a+n_b)$ 的矩阵，$\Phi^\top \Phi$ 是一个 $(n_a+n_b) \times (n_a+n_b)$ 的方阵。$\Phi^\top \Phi$ 可逆的充分必要条件是 $\Phi$ 具有 **列满秩**，即其所有列向量都是[线性无关](@entry_id:148207)的。

在系统辨识的背景下，我们不能只满足于对某一个特定数据集 $\Phi$ 具有列满秩。我们希望有一种方法能确保，只要实验做得足够长，就一定能得到一个满秩的 $\Phi$。这个保证来自于对输入信号 $u(t)$ 的一个性质要求，称为 **[持续激励](@entry_id:263834)**（**Persistency of Excitation, PE**）[@problem_id:2880143]。

一个输入信号 $u(t)$ 被称为是 **$n$ 阶[持续激励](@entry_id:263834)的**，如果它足够“丰富”，能够充分“激发”系统的所有动态模式，从而使得由其构建的任何 $n$ 维回归向量在足够长的时间窗口内都是[线性独立](@entry_id:153759)的。更严谨地，这要求存在一个正数 $\alpha > 0$ 和一个窗口长度 $N_0 \ge n$，使得对于任意起始时间 $t$，由回归向量 $v_k = [u(k) \cdots u(k-n+1)]^\top$ 构成的 Gram 矩阵在窗口内是一致正定的：
$$
\sum_{k=t}^{t+N_0-1} v_k v_k^\top \succeq \alpha I_n
$$
对于一个 FIR($n_b$) 模型，输入信号需要是 $n_b$ 阶[持续激励](@entry_id:263834)的。对于一个 ARX($n_a, n_b$) 模型，通常需要输入是 $n_a+n_b$ 阶[持续激励](@entry_id:263834)的，以确保整个回归矩阵（包含 $y(t-i)$ 和 $u(t-j)$）的列是渐近线性无关的。

从[频域](@entry_id:160070)角度看，一个信号是 $n$ 阶[持续激励](@entry_id:263834)的，等价于其[功率谱密度](@entry_id:141002)函数 $S_u(e^{j\omega})$ 在至少 $\lceil n/2 \rceil$ 个不同的频率点上不为零。例如，一个只包含单一频率的[正弦波](@entry_id:274998)输入，最多只能辨识一个二阶系统，因为它无法提供足够的信息来区分更高阶的动态。

#### 一致性：渐近无偏与[外生性](@entry_id:146270)

**一致性**（**Consistency**）是衡量估计器质量的一个核心 asymptotic（大样本）性质，它要求当样本数量 $N \to \infty$ 时，估计值 $\hat{\theta}_N$ [依概率收敛](@entry_id:145927)于真实的参数值 $\theta_0$。

我们来分析[估计误差](@entry_id:263890) $\tilde{\theta}_N = \hat{\theta}_N - \theta_0$。将 $Y = \Phi \theta_0 + E$ 代入[最小二乘解](@entry_id:152054)中：
$$
\hat{\theta}_N = (\Phi^\top \Phi)^{-1} \Phi^\top (\Phi \theta_0 + E) = \theta_0 + (\Phi^\top \Phi)^{-1} \Phi^\top E
$$
因此，误差为 $\tilde{\theta}_N = (\Phi^\top \Phi)^{-1} \Phi^\top E$。为了分析其渐近行为，我们将其改写为：
$$
\tilde{\theta}_N = \left(\frac{1}{N}\Phi^\top \Phi\right)^{-1} \left(\frac{1}{N}\Phi^\top E\right)
$$
为了使 $\tilde{\theta}_N \to 0$，需要满足两个条件：

1.  **[信息矩阵](@entry_id:750640)的非奇异性**：矩阵 $\frac{1}{N}\Phi^\top \Phi = \frac{1}{N}\sum \varphi(t)\varphi(t)^\top$ 必须收敛到一个可逆矩阵。在平稳遍历假设下，根据大数定律，这个样本均值会收敛到其[期望值](@entry_id:153208) $R_\varphi = \mathbb{E}[\varphi(t)\varphi(t)^\top]$。因此，我们要求 $R_\varphi$ 是正定的。这个条件正是参数 **[可辨识性](@entry_id:194150)**（**identifiability**）的数学定义 [@problem_id:2880118]。如果 $R_\varphi$ 是奇异的，就意味着存在某个非零参数扰动方向 $\Delta$，使得 $\mathbb{E}[(\varphi(t)^\top \Delta)^2] = \Delta^\top R_\varphi \Delta = 0$，即不同的参数产生了无法区分的预测，参数不可辨识。[持续激励](@entry_id:263834)条件正是保证了 $R_\varphi$ 的[正定性](@entry_id:149643)。

2.  **回归量与噪声的正交性**：项 $\frac{1}{N}\Phi^\top E = \frac{1}{N}\sum \varphi(t)e(t)$ 必须收敛到零。根据[大数定律](@entry_id:140915)，这要求 $\mathbb{E}[\varphi(t)e(t)] = 0$。这个条件被称为 **[外生性](@entry_id:146270)**（**exogeneity**），即回归量与噪声项在统计上是不相关的。

只有当这两个条件同时满足时，[最小二乘估计](@entry_id:262764)才是一致的。

### 噪声的作用：挑战与解决方案

[外生性](@entry_id:146270)条件 $\mathbb{E}[\varphi(t)e(t)] = 0$ 是[最小二乘法](@entry_id:137100)成功的基石，但在许多实际情况下，这个条件很容易被违反，导致估计失败。

#### 方程误差相关（[有色噪声](@entry_id:265434)）

让我们再次比较 FIR 和 ARX 模型。假设方程误差 $e(t)$ 是 **有色的**（**colored noise**），即它与自身的过去值相关（例如，$e(t)$ 是一个 ARMA 过程）。

-   对于 **FIR 模型**，回归向量 $\varphi(t)$ 只包含过去的输入 $u(t-j)$。只要输入信号 $u(t)$ 与噪声 $e(t)$ 无关（这是标准假设），那么 $\mathbb{E}[\varphi(t)e(t)] = 0$ 仍然成立。因此，即使噪声是有色的，对 FIR 模型的普通最小二乘（OLS）估计仍然是 **一致的** [@problem_id:2880148]。

-   对于 **ARX 模型**，情况则完全不同。回归向量 $\varphi(t)$ 中包含了过去的输出 $y(t-i)$。而过去的输出 $y(t-i)$ 本身是过[去噪](@entry_id:165626)声 $e(t-i-k)$ 的函数。如果 $e(t)$ 是有色的，那么 $e(t)$ 会与它自己的过去值 $e(t-i-k)$ 相关。通过这个链条，当前的噪声 $e(t)$ 就与回归量 $y(t-i)$ 变得相关了。这导致 $\mathbb{E}[\varphi(t)e(t)] \ne 0$，[外生性](@entry_id:146270)条件被破坏。结果是，对于[有色噪声](@entry_id:265434)下的 ARX 模型，OLS 估计是 **有偏的** 和 **不一致的**。

这个问题促使研究者们发展了更复杂的模型结构，如 **ARMAX**、**输出误差（OE）** 和 **Box-Jenkins（BJ）** 模型，它们明确地对噪声的动态特性进行建模 [@problem_id:2880135]。例如，ARMAX 模型假设了一个 ARMA 噪声过程。然而，这些模型不再能直接通过 OLS 求解，因为它们不是严格的[线性回归](@entry_id:142318)形式（例如，包含未知的噪声项作为回归量），需要使用迭代的 **[预测误差](@entry_id:753692)方法**（**Prediction Error Methods, PEM**）进行估计。

#### 异[方差](@entry_id:200758)与广义最小二乘

另一个常见的非理想情况是噪声的协方差矩阵不是一个简单的单位矩阵的倍数，即 $\mathrm{Cov}(E) = \Sigma_e \ne \sigma^2 I$。这可能发生在：
-   **[异方差性](@entry_id:136378)**（**Heteroscedasticity**）：噪声的[方差](@entry_id:200758)随时间变化，$\Sigma_e$ 是对角但非标量矩阵。
-   **[自相关](@entry_id:138991)**（**Autocorrelation**）：噪声在不同时间点之间相关，$\Sigma_e$ 是非[对角矩阵](@entry_id:637782)。

在这种情况下，OLS 估计虽然仍然是无偏的，但不再是 **最优** 的。根据 **[高斯-马尔可夫定理](@entry_id:138437)**，它不再是 **最佳线性[无偏估计](@entry_id:756289)**（**Best Linear Unbiased Estimator, BLUE**），即存在其他线性[无偏估计](@entry_id:756289)器具有更小的[方差](@entry_id:200758)。

为了恢复最优性，我们使用 **加权最小二乘**（**Weighted Least Squares, WLS**）或更广义的 **广义最小二乘**（**Generalized Least Squares, GLS**）。GLS 最小化一个加权的[残差平方和](@entry_id:174395)：
$$
J_{GLS}(\theta) = (Y - \Phi\theta)^\top W (Y - \Phi\theta)
$$
可以证明，当权重矩阵 $W$ 选择为噪声协方差矩阵的逆，即 $W = \Sigma_e^{-1}$ 时，得到的估计器是 BLUE [@problem_id:2880151]。
$$
\hat{\theta}_{GLS} = (\Phi^\top \Sigma_e^{-1} \Phi)^{-1} \Phi^\top \Sigma_e^{-1} Y
$$
这个选择在直觉上是合理的：它对噪声[方差](@entry_id:200758)较小（信息更可靠）的观测值赋予更大的权重，同时考虑并消除不同观测值之间噪声的相关性。这个过程等价于对原始数据进行 **[预白化](@entry_id:185911)**（**pre-whitening**）变换，将相关且异[方差](@entry_id:200758)的噪声转化为不相关且同[方差](@entry_id:200758)的噪声，然后再应用 OLS [@problem_id:2880111]。

#### 变量含误差（EIV）问题

一个更严峻的挑战是当回归向量 $\varphi(t)$ 本身也受到测量噪声污染时，这被称为 **变量含误差**（**Errors-In-Variables, EIV**）问题 [@problem_id:2880136]。例如，在 ARX 模型中，用于构建回归量的过去输出 $y(t-i)$ 可能是带噪声的测量值。

假设我们观测到的回归量是 $\tilde{\varphi}(t) = \varphi(t) + w(t)$，其中 $\varphi(t)$ 是真实的、无噪声的回归量，$w(t)$ 是测量噪声。如果我们直接使用观测到的 $\tilde{\varphi}(t)$ 进行 OLS，模型就变成了 $y(t) = \tilde{\varphi}(t)^\top\theta_0 + (e(t) - w(t)^\top\theta_0)$。新的复合误差项为 $\varepsilon(t) = e(t) - w(t)^\top\theta_0$。

此时，我们检查[外生性](@entry_id:146270)条件 $\mathbb{E}[\tilde{\varphi}(t)\varepsilon(t)]$。我们会发现：
$$
\mathbb{E}[\tilde{\varphi}(t)\varepsilon(t)] = \mathbb{E}[(\varphi(t)+w(t))(e(t) - w(t)^\top\theta_0)] = -\mathbb{E}[w(t)w(t)^\top]\theta_0 = -R_w \theta_0
$$
这个期望通常不为零。回归量与误差项之间存在固有的相关性，这直接导致 OLS 估计产生偏误且不一致。在简单的一维情况下，这种偏误通常表现为 **[衰减偏误](@entry_id:746571)**（**attenuation bias**），即估计出的参数大小会趋向于被低估。解决 EIV 问题需要更高级的技术，例如 **[工具变量法](@entry_id:204495)**（**Instrumental Variables, IV**）或 **[总体最小二乘法](@entry_id:170210)**（**Total Least Squares, TLS**）。

### 数值实现与稳定性

最后，即使所有理论条件都满足，[最小二乘解](@entry_id:152054)的数值计算也需要小心处理。从[正规方程](@entry_id:142238) $(\Phi^\top \Phi) \hat{\theta} = \Phi^\top Y$ 求解 $\hat{\theta}$ 是一种常见但可能存在数值问题的方法。

问题的数值敏感性由 **条件数**（**condition number**）$\kappa$ 来衡量。一个[矩阵的条件数](@entry_id:150947)越大，其求逆或求解相关线性方程组对输入数据的微小扰动就越敏感。一个关键的[数值分析](@entry_id:142637)结果是，形成[正规方程](@entry_id:142238)的过程会平方问题的条件数 [@problem_id:2880127]：
$$
\kappa(\Phi^\top \Phi) = [\kappa(\Phi)]^2
$$
如果回归矩阵 $\Phi$ 本身是 **病态的**（ill-conditioned），即 $\kappa(\Phi)$ 很大，那么 $\Phi^\top \Phi$ 的[条件数](@entry_id:145150)会变得极其巨大。在有限精度的计算机上，这可能导致严重的精度损失，甚至得到完全错误的解。简单的数值缩放并不能改变这个平方关系。

为了避免这个问题，稳健的数值方法会避免直接形成 $\Phi^\top \Phi$。取而代之的是使用[正交分解](@entry_id:148020)，如 **QR 分解** 或 **[奇异值分解](@entry_id:138057)**（**Singular Value Decomposition, SVD**）。

-   **QR 分解** 将 $\Phi$ 分解为 $\Phi = QR$，其中 $Q$ 是一个列[正交矩阵](@entry_id:169220)（$Q^\top Q = I$），$R$ 是一个上三角矩阵。[最小二乘问题](@entry_id:164198)转化为求解一个条件数与 $\kappa(\Phi)$ 相同的、良态的[上三角系统](@entry_id:635483) $R\hat{\theta} = Q^\top Y$。

-   **SVD** 提供了最稳健的数值方法，它直接计算出 $\Phi$ 的[奇异值](@entry_id:152907)，从而可以精确地诊断和处理病态或[秩亏](@entry_id:754065)问题。

总之，虽然正规方程在理论推导中非常有用，但在实际的高精度数值计算中，应优先选择基于[正交分解](@entry_id:148020)的算法来求解最小二乘问题。