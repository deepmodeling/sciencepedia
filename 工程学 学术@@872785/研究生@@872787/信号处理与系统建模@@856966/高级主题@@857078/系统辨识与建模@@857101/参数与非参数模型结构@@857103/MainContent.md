## 引言
在任何旨在从数据中提取知识的科学探索中，如何构建一个能够精确捕捉现实世界现象的数学模型，是所有问题的核心。这一过程的关键在于选择一个合适的**模型结构**——即我们用来逼近真实系统的候选函数集合。这个选择深刻地影响着我们能够学到何种规律、模型的复杂程度以及最终的[可解释性](@entry_id:637759)，它本质上是在“先验结构假设”与“数据驱动的灵活性”之间进行权衡。然而，许多从业者常常在“参数化”与“非[参数化](@entry_id:272587)”这两个术语之间感到困惑，不清楚它们的确切含义、适用边界以及如何根据具体问题做出明智的选择。

本文旨在填补这一知识鸿沟。我们将系统地剖析[参数化](@entry_id:272587)与非[参数化](@entry_id:272587)这两种核心建模[范式](@entry_id:161181)。在“**原理与机制**”一章中，您将学习它们的严格定义、处理[模型复杂度](@entry_id:145563)的机制（如[信息准则](@entry_id:636495)与[有效自由度](@entry_id:161063)）、以及偏差-方差权衡如何统一这两种看似对立的方法。随后，在“**应用与跨学科连接**”一章中，我们将通过[系统辨识](@entry_id:201290)、生物信息学、计算化学等领域的真实案例，展示这些理论在实践中如何发挥作用，揭示半参数化模型等混合结构如何巧妙地平衡了可解释性与预测能力。最后，“**动手实践**”部分将提供精选的练习，让您将理论知识应用于具体的计算问题。通过本次学习，您将能够为您的建模任务选择最恰当的结构，并深刻理解其背后的理论依据。

## 原理与机制

在系统建模领域，我们旨在构建一个能够捕捉观测数据中潜在规律的数学表示。这一过程的核心在于选择一个合适的**模型结构**（model structure），即一个候选函数或系统的集合，我们称之为**假设类**（hypothesis class）。模型结构的选择深刻地影响着我们可以学习到的关系类型、模型的复杂性、对数据的需求量以及最终模型的可解释性。从根本上说，模型结构的选择体现了我们在“先验知识”与“数据驱动的灵活性”之间所做的权衡。本章将深入探讨两种主要的模型结构[范式](@entry_id:161181)：**[参数化](@entry_id:272587)模型**与**非[参数化](@entry_id:272587)模型**，阐明它们的核心原理、关键机制以及它们之间深刻的联系与区别。

### 定义分野：模型结构的维度

模型结构最根本的区别在于其假设类的“大小”或“维度”。

**参数化模型（Parametric Models）** 的核心特征在于，其假设类可以由一个**维数固定且有限**的**参数矢量**（parameter vector）$\theta \in \mathbb{R}^p$ 来完全描述。这意味着，无论我们有多少观测数据，模型的内在复杂性——由参数的个数 $p$ 所决定——是事先固定的。一旦参数矢量 $\theta$ 被确定，模型也就唯一确定。

一个典型的例子是固定阶数的**自回归外生输入模型（ARX, AutoRegressive with eXogenous input）**。一个 $(n_a, n_b)$ 阶的[ARX模型](@entry_id:269528)由一个包含 $p = n_a + n_b$ 个参数的矢量 $\theta$ 定义。整个模型家族 $\mathcal{P}$ 就是所有可以由这些参数生成的线性系统的集合，即 $\mathcal{P} = \{f_\theta : \theta \in \mathbb{R}^p\}$。这里的关键在于，模型阶数 $(n_a, n_b)$ 是在看到数据之前就选定的，并且不随样本量 $N$ 的增加而改变。[@problem_id:2889282]

**非[参数化](@entry_id:272587)模型（Non-parametric Models）** 则与之相反，其假设类不能被一个维数固定的有限参数矢量所描述。它们的假设类通常是一个**无限维[函数空间](@entry_id:143478)**，例如一个[再生核希尔伯特空间](@entry_id:633928)（RKHS）、一个索博列夫空间（Sobolev space）或 $L^2$ 空间。在实践中，非参数化方法的“有效”复杂性会随着数据量的增加而增长。

例如，一个基于核函数的脉冲响应模型，其假设类可能是某个RKHS中的一个[单位球](@entry_id:142558)。虽然对于一个给定的、包含 $N$ 个样本的数据集，根据**[表示定理](@entry_id:637872)（Representer Theorem）**，最终的估计器可能可以表示为 $N$ 个[基函数](@entry_id:170178)的[线性组合](@entry_id:154743)，但这并不意味着模型本身是参数化的。这里的“参数”数量 $N$ 直接依赖于样本量，并且随着 $N \to \infty$ 而趋于无穷。将有限样本解的有限维表示与模型结构本身的无限维特性混为一谈，是一个常见的误解。真正的区别在于假设类 $\mathcal{H}$ 的结构是先验固定的[有限维空间](@entry_id:151571)，还是一个无限维[函数空间](@entry_id:143478)。[@problem_id:2889282]

一种常见的非[参数化](@entry_id:272587)方法，称为**筛法（method of sieves）**，是通过一系列嵌套的、维数递增的[参数化](@entry_id:272587)模型族来构建一个无限维的假设类。例如，一个[有限脉冲响应](@entry_id:192542)（FIR）滤波器模型，如果我们不预先固定其阶数 $p$，而是允许 $p$ 随着样本量 $N$ 的增加而增加（即 $p = p(N)$），那么整个模型结构 $\mathcal{H} = \bigcup_{p \in \mathbb{N}} \mathcal{F}_p$（其中 $\mathcal{F}_p$ 是所有阶数为 $p$ 的FIR模型）就是无限维的，因此属于非[参数化](@entry_id:272587)范畴。[@problem_id:2889282]

### 参数化[范式](@entry_id:161181)：结构、假设与辨识

[参数化建模](@entry_id:192148)的优势在于其简洁性和高效性。通过将现实世界的复杂现象压缩到少数几个参数中，我们不仅获得了易于处理和分析的模型，还能在数据量有限时得到较为稳定和可靠的估计。然而，这种能力的代价是必须做出关于系统真实结构的强假设。

#### 典范：[ARMA模型](@entry_id:139294)

[时间序列分析](@entry_id:178930)中的**[自回归移动平均](@entry_id:143076)模型（ARMA, Autoregressive Moving Average models）** 是[参数化建模](@entry_id:192148)的经典例子。对于一个由零均值[白噪声](@entry_id:145248) $e_t$ 驱动的[随机过程](@entry_id:159502) $x_t$，一个ARMA($p,q$)模型可以用后移算符 $B$（其中 $B x_t = x_{t-1}$）的多项式来简洁地表示：
$$
\Phi(B) x_t = \Theta(B) e_t
$$
其中，$\Phi(B) = 1 - \sum_{k=1}^p \phi_k B^k$ 是自回归（AR）部分，$\Theta(B) = 1 + \sum_{k=1}^q \theta_k B^k$ 是移动平均（MA）部分。该模型的参数矢量 $\theta$ 由系数 $\{\phi_k\}$ 和 $\{\theta_k\}$ 构成。[@problem_id:2889251]

这些参数并不仅仅是拟合系数，它们蕴含了关于系统动态行为的深刻信息。例如，模型的**因果性/稳定性（causality/stability）**，即系统的输出不会依赖于未来的输入且有界，完全取决于AR特征多项式 $\Phi(z)=0$ 的根。只有当所有根的模都**大于1**（即根全部在单位圆外）时，系统才是稳定的。类似地，模型的**可逆性（invertibility）**，即能够从输出 $x_t$ 唯一地重构出输入噪声 $e_t$，取决于MA特征多项式 $\Theta(z)=0$ 的根，同样要求所有根都在单位圆外。这些条件确保了参数与系统行为之间存在明确且有意义的联系。[@problem_id:2889251]

#### 结构辨识性：参数何时有意义？

参数化模型的一个核心前提是**结构辨识性（structural identifiability）**，即参数到模型行为的映射是单射的。换句话说，不同的参数值必须对应不同的模型行为。如果两个不同的参数 $\theta_1 \neq \theta_2$ 产生了完全相同的输入输出行为（例如，相同的[传递函数](@entry_id:273897) $G(z, \theta_1) = G(z, \theta_2)$），那么从数据中就无法唯一确定参数，参数也就失去了其物理或解释意义。[@problem_id:2889355]

- **全局结构辨识性（Global Structural Identifiability）** 要求在整个参数空间内，这种映射都是[单射](@entry_id:183792)的。
- **局部结构辨识性（Local Structural Identifiability）** 则是一个较弱的条件，仅要求在某个特定参数 $\theta^\star$ 的一个邻域内映射是单射的。

结构不可辨识通常由**过[参数化](@entry_id:272587)（over-parameterization）** 引起。例如，在一个有理[传递函数](@entry_id:273897)模型 $G(z,\theta) = \frac{b_0 + b_1 z^{-1}}{1 + a_1 z^{-1}}$ 中，如果分子和分母多项式存在公因子（例如，可以通过特定参数选择导致零极点对消），模型就会变得不可辨识。一个更明显的例子是，如果我们引入一个冗余的缩放因子 $c \neq 0$，将模型写为 $\tilde{G}(z,\tilde{\theta}) = \frac{c\,(b_0 + b_1 z^{-1})}{c\,(1 + a_1 z^{-1})}$，那么对于任意 $c$，[传递函数](@entry_id:273897)都完全相同，使得参数 $c$ 无法被唯一确定。[@problem_id:2889355]

在[状态空间模型](@entry_id:137993)中，如果将矩阵 $(A, B, C, D)$ 的所有元素都视为自由参数，模型通常是全局不可辨识的。这是因为任何相似性变换 $(A,B,C,D) \mapsto (TAT^{-1}, TB, CT^{-1}, D)$ 都会产生完全相同的[传递函数](@entry_id:273897)。为了恢复辨识性，必须施加**典范型（canonical form）** 的约束，例如能控典范型或能观典范型，这相当于消除了相似性变换带来的自由度。[@problem_id:2889355]

#### 量化与选择复杂度

在[参数化建模](@entry_id:192148)中，一个永恒的问题是：如何选择合适的[模型复杂度](@entry_id:145563)（例如，[ARMA模型](@entry_id:139294)的阶数 $p$ 和 $q$）？过于简单的模型可能无法捕捉数据的真实动态（高偏差），而过于复杂的模型则可能拟合噪声（高[方差](@entry_id:200758)）。

模型的复杂度可以通过其**自由度（Degrees of Freedom, DoF）** 来量化。对于一个有 $p$ 个未知参数的[线性模型](@entry_id:178302)，其DoF就是 $p$。如果模型受到 $r$ 个独立的[线性约束](@entry_id:636966)，那么其DoF就减少为 $p-r$，这代表了模型可以自由变化以拟合数据的维度。[@problem_id:2889334]

为了在[偏差和方差](@entry_id:170697)之间做出权衡，学术界发展出了一系列**模型选择准则（model selection criteria）**。这些准则通常由两部分组成：一部分是[拟合优度](@entry_id:637026)项（通常与残差的[对数似然](@entry_id:273783)有关），另一部分是惩罚项，用于惩罚模型的复杂度。对于一个拟合了 $N$ 个数据点、包含 $n$ 个参数的AR($n$)模型，其残差[方差](@entry_id:200758)为 $\hat{\sigma}_n^2$，几种常见的准则包括：

- **[赤池信息准则](@entry_id:139671)（Akaike Information Criterion, AIC）**: $\mathrm{AIC}(n) = N \ln(\hat{\sigma}_n^2) + 2n$
- **[贝叶斯信息准则](@entry_id:142416)（Bayesian Information Criterion, BIC）**: $\mathrm{BIC}(n) = N \ln(\hat{\sigma}_n^2) + n \ln N$
- **最终[预测误差](@entry_id:753692)（Final Prediction Error, FPE）**: $\mathrm{FPE}(n) = \hat{\sigma}_n^2 \frac{N + n}{N - n}$
- **[最小描述长度](@entry_id:261078)（Minimum Description Length, MDL）**: 在许多情况下，其形式与BIC[渐近等价](@entry_id:273818)，$\mathrm{MDL}(n) = N \ln(\hat{\sigma}_n^2) + n \ln N$

这些准则在[渐近性质](@entry_id:177569)上表现出根本性的差异。当样本量 $N \to \infty$ 且真实模型阶数存在时，BIC和MDL的惩罚项（与 $\ln N$ 成正比）足够强，能够确保选出的模型阶数以趋于1的概率收敛到真实阶数，这一性质称为**一致性（consistency）**。相比之下，AIC和FPE的惩罚项是常数，它们有过拟合（选择比真实阶数更高的阶数）的非零概率，因此不是一致的。然而，AIC和FPE具有**[渐近有效](@entry_id:167883)性（asymptotic efficiency）**，即它们选择的模型在最小化[预测误差](@entry_id:753692)（或[KL散度](@entry_id:140001)）方面是渐近最优的。这揭示了模型选择中的一个核心权衡：是追求找到“真实”模型，还是追求最佳的预测性能。[@problem_id:2889306]

#### 面对现实：模型失配

[参数化](@entry_id:272587)模型最脆弱的地方在于其对真实结构做出强假设。在现实世界中，我们选择的模型几乎总是“错误”的，即真实的数据生成过程并不精确地存在于我们选择的模型族中。这种情况称为**模型失配（model misspecification）**。

当模型失配时，我们通过最小化某个[风险函数](@entry_id:166593)（如最小二乘或[最大似然](@entry_id:146147)）得到的[参数估计](@entry_id:139349)，在样本量趋于无穷时，会收敛到一个确定的值，这个值被称为**伪真参数（pseudo-true parameter）** $\theta^\dagger$。它不再是“真实”的参数，而是模型族中能够**最佳逼近**真实过程的那个参数。[@problem_id:2889304]

“最佳逼近”的含义取决于我们选择的[损失函数](@entry_id:634569)：
- 对于**[平方误差损失](@entry_id:178358)**，伪真参数 $\theta^\dagger$ 所定义的模型 $f(X;\theta^\dagger)$ 是在 $\mathsf{L}^2$ 范数意义下，对真实条件[均值函数](@entry_id:264860) $m(X)=\mathbb{E}[Y|X]$ 的最佳投影。也就是说，$\theta^\dagger$ 最小化了 $\mathbb{E}[(m(X)-f(X;\theta))^2]$。[@problem_id:2889304]
- 对于**[对数似然](@entry_id:273783)损失**，伪真参数 $\theta^\dagger$ 定义的模型 $p_{\theta^\dagger}(y|x)$ 是在**Kullback-Leibler (KL)散度**意义下，与真实[条件概率密度](@entry_id:265457) $p^\star(y|x)$ 最“接近”的模型。即 $\theta^\dagger$ 最小化了期望的KL散度 $\mathbb{E}[D_{\mathrm{KL}}(p^\star(\cdot|X) \,\|\, p_\theta(\cdot|X))]$。[@problem_id:2889304]

理解伪真参数的概念至关重要，它提醒我们，即使模型是“错误”的，[参数化](@entry_id:272587)估计过程仍然会收敛到一个有意义的、可解释的最佳近似。

### 非参数化[范式](@entry_id:161181)：灵活性、正则化与[收敛率](@entry_id:146534)

当先验知识不足以支撑一个简单的[参数化](@entry_id:272587)模型时，非[参数化](@entry_id:272587)方法提供了强大的替代方案。它放弃了固定的结构，允许模型从数据中学习更复杂的关系。

#### 动机与实例

非参数化思想的一个经典应用是**[谱估计](@entry_id:262779)（spectral estimation）**。**[周期图](@entry_id:194101)（Periodogram）** 是最直接的[谱估计](@entry_id:262779)器，它本质上是数据[傅里叶变换](@entry_id:142120)的平方模。然而，[周期图](@entry_id:194101)是一个**不一致（inconsistent）** 的估计器：无论数据多长，其[方差](@entry_id:200758)都不会减小，导致估计结果充满噪声尖峰。

为了获得可靠的[谱估计](@entry_id:262779)，必须引入**平滑（smoothing）** 或**平均（averaging）** 来降低[方差](@entry_id:200758)。这正是多种非参数化[谱估计](@entry_id:262779)方法的核心思想：[@problem_id:2889309]
- **[Blackman-Tukey方法](@entry_id:188236)**：在计算自相关函数的[傅里叶变换](@entry_id:142120)之前，先对其施加一个[窗函数](@entry_id:139733)（lag window），以抑制高延迟（高[方差](@entry_id:200758)）的估计。
- **[Welch方法](@entry_id:144484)**：将数据分割成多个（可能重叠的）段，计算每段的[周期图](@entry_id:194101)，然后将它们平均。
- **多窗法（Multitaper）**：使用一组特殊设计的、相互正交的窗函数（tapers）来计算多个[谱估计](@entry_id:262779)，然后将它们平均。

这些方法都体现了非[参数化](@entry_id:272587)估计中的一个核心主题：通过某种形式的平滑或平均来控制[方差](@entry_id:200758)。

另一个重要的例子是**经验[传递函数](@entry_id:273897)估计（Empirical Transfer Function Estimate, ETFE）**，它通过输出信号的[傅里叶变换](@entry_id:142120)与输入信号的[傅里叶变换](@entry_id:142120)之比来估计系统的频率响应。与[周期图](@entry_id:194101)类似，ETFE也是一个不一致的估计器。其[方差](@entry_id:200758)在样本量 $N \to \infty$ 时不趋于零，而是收敛到一个由[信噪比](@entry_id:185071)决定的常数。为了得到一致的估计，必须在频率域上进行平滑或对多个独立实验的结果进行平均。[@problem_id:2889295]

#### 衡量复杂度：[有效自由度](@entry_id:161063)

非[参数化](@entry_id:272587)模型的灵活性需要被控制，否则模型会过度拟合数据。这种控制通常通过一个或多个**正则化参数（regularization parameter）** 或**平滑参数（smoothing parameter）** 来实现，例如核回归中的带宽、样条中的平滑参数、岭回归中的惩罚系数$\lambda$等。

为了量化非参数化模型的复杂度，**[有效自由度](@entry_id:161063)（Effective Degrees of Freedom, EDF）** 的概念应运而生。对于一个**线性平滑器（linear smoother）**，即其拟合值向量 $\hat{\boldsymbol{y}}$ 可以表示为观测值向量 $\boldsymbol{y}$ 的线性变换 $\hat{\boldsymbol{y}} = S_\lambda \boldsymbol{y}$，其中 $S_\lambda$ 是平滑矩阵，其EDF被定义为该矩阵的迹：
$$
\mathrm{EDF} = \mathrm{tr}(S_\lambda)
$$
这个定义完美地推广了线性模型中的DoF概念（对于[普通最小二乘法](@entry_id:137121)，其[投影矩阵](@entry_id:154479) $H$ 的迹恰好是参数个数 $p$）。更一般地，对于任何（可能[非线性](@entry_id:637147)的）估计器，EDF可以被定义为拟合值与观测值之间协方和的归一化形式：
$$
\mathrm{EDF} = \frac{1}{\sigma^2} \sum_{i=1}^n \mathrm{Cov}(\hat{y}_i, y_i)
$$
这个深刻的定义将EDF与拟合值对观测值扰动的敏感度联系起来，并且对于线性平滑器，它恰好简化为 $\mathrm{tr}(S_\lambda)$。[@problem_id:2889334]

EDF清晰地展示了[正则化参数](@entry_id:162917)如何控制[模型复杂度](@entry_id:145563)。例如，在**[岭回归](@entry_id:140984)**中，EDF为 $\mathrm{df}(\lambda) = \sum_{j=1}^p \frac{\lambda_j}{\lambda_j + \lambda}$，其中 $\lambda_j$ 是[设计矩阵](@entry_id:165826)协[方差](@entry_id:200758)的[特征值](@entry_id:154894)。随着惩罚$\lambda$的增大，EDF减小，模型变得更简单。在**k-近邻回归**中，EDF为 $n/k$。随着邻居数 $k$ 的增加，模型进行更多的平均，变得更平滑，复杂度降低（EDF减小）。[@problem_id:2889334]

#### 编码先验：核函数的作用

非[参数化](@entry_id:272587)方法并非没有假设，而是以更微妙的方式将假设（或先验知识）编码到模型中。在**[核方法](@entry_id:276706)（kernel methods）** 中，这种假设体现在**[核函数](@entry_id:145324)（kernel）** 的选择上。

一个[核函数](@entry_id:145324) $k(x, x')$ 定义了一个[再生核希尔伯特空间](@entry_id:633928)（RKHS），这个空间中的函数具有特定的**光滑度（smoothness）**。[核函数](@entry_id:145324)的光滑度假设可以通过其关联的积分算子 $T_k$ 的[特征值](@entry_id:154894) $\mu_j$ 的衰减速度来刻画。例如，如果[特征值](@entry_id:154894)以 $\mu_j \asymp j^{-2\beta/d}$ 的速度衰减，那么该核对应的RKHS在光滑度上就等价于一个 $\beta$ 阶的[索博列夫空间](@entry_id:141995)。[@problem_id:2889310]

核函数的选择直接影响估计器的性能。在非[参数化](@entry_id:272587)[估计理论](@entry_id:268624)中，一个核心目标是达到**极小极大最优性（minimax optimality）**，即对于某个给定的函数类别（例如，光滑度为 $s$ 的函数构成的索博列夫球 $\mathcal{W}_2^s$），我们希望找到一个估计器，其在最坏情况下的[收敛速度](@entry_id:636873)能达到理论上的最快可能速度。对于 $\mathcal{W}_2^s$ 类别，这个最优[收敛率](@entry_id:146534)通常为 $n^{-2s/(2s+d)}$，其中 $n$ 是样本量， $d$ 是输入维度。

为了用核回归达到这个最优速率，我们选择的[核函数](@entry_id:145324)所编码的光滑度 $\beta$ 必须大于或等于真实函数类别的光滑度 $s$（即 $\beta \ge s$）。如果我们选择的核函数过于粗糙（$\beta  s$），估计器的[收敛速度](@entry_id:636873)将被[核函数](@entry_id:145324)自身的性质所限制，出现**饱和（saturation）** 现象，其[收敛率](@entry_id:146534)最多只能达到 $n^{-2\beta/(2\beta+d)}$，慢于最优速率。因此，[核函数](@entry_id:145324)的选择是一种将关于待估函数光滑度的先验信念注入模型的方式。像**Matérn核**这样的核函数族甚至提供了一个可调参数 $\nu$，允许我们显式地控制所假设的光滑度，从而使模型与问题相匹配。[@problem_id:2889310]

### 综合：统一的视角与混合模型

尽管[参数化](@entry_id:272587)和非[参数化](@entry_id:272587)方法看似截然不同，但它们可以通过**偏差-方差权衡（bias-variance trade-off）** 的统一视角来理解。

#### 统一的镜头：偏差-方差权衡

任何模型的预测误差（或风险）都可以分解为三个部分：**偏差（bias）的平方**、**[方差](@entry_id:200758)（variance）** 和**不可约误差（irreducible error）**。
$$
\text{风险} = (\text{偏差})^2 + \text{方差} + \text{不可约误差}
$$
- **偏差**，也称为**结构误差（structural error）** 或**近似误差（approximation error）**，源于模型假设类自身的局限性，即模型族中最好的模型与真实情况之间的差距。
- **[方差](@entry_id:200758)**，也称为**估计误差（estimation error）**，源于我们只有有限的数据，导致从不同数据集得到的估计结果会发生变化。

[参数化](@entry_id:272587)和非参数化模型在这两个方面表现出截然不同的行为：[@problem_id:2889349]
- **参数化模型**：由于其结构固定，如果模型发生失配，它将具有一个固有的、即使在数据无限多时也无法消除的偏差。但由于其复杂度低，[参数估计](@entry_id:139349)的[方差](@entry_id:200758)通常很小，并以很快的速度（如 $O(1/N)$）随着样本量 $N$ 的增加而减小。
- **非参数化模型**：通过让[模型复杂度](@entry_id:145563)随数据量增长，其偏差可以趋于零。但是，这种灵活性是有代价的：估计器的[方差](@entry_id:200758)更大，并且随着 $N$ 的增加，其减小的速度也更慢（例如，$O(N^{-2s/(2s+d)})$）。[正则化参数](@entry_id:162917)的选择正是在[偏差和方差](@entry_id:170697)之间寻求最佳平衡。

#### 跨越鸿沟：半[参数化](@entry_id:272587)模型

**半[参数化](@entry_id:272587)模型（Semi-parametric models）** 巧妙地结合了[参数化](@entry_id:272587)和非参数化方法的优点，旨在实现结构性与灵活性的统一。这类模型将[系统分解](@entry_id:274870)为一部分结构已知、可以用少量参数描述的部分，以及另一部分结构未知、需要用非参数化方法学习的部分。

一个典型的例子是**[维纳模型](@entry_id:188978)（Wiener model）**，它描述了一个线性时不变（LTI）动态系统后接一个静态[非线性](@entry_id:637147)环节的[串联](@entry_id:141009)结构：
$$
y(t) = g_0\left( (H_0(q^{-1})u)(t) \right) + e(t)
$$
我们可以使用一个半参数化模型来估计它，其中LTI部分 $H(q^{-1}, \theta)$ 用参数 $\theta$ 描述，而静态[非线性](@entry_id:637147) $g(\cdot)$ 用非参数化方法（如核回归或样条）学习。[@problem_id:2889293]

与纯[参数化](@entry_id:272587)或纯非[参数化](@entry_id:272587)方法相比，半参数化模型展现出独特的优势：
- **偏差-[方差](@entry_id:200758)性能**：相对于纯LTI模型，如果真实的系统确实存在[非线性](@entry_id:637147)，半[参数化](@entry_id:272587)模型能够显著降低偏差。相对于一个完全黑箱的非参数化模型（如N[ARX模型](@entry_id:269528)），它通过利用已知的LTI结构信息，避免了在高维空间中学习所带来的“维度灾难”，从而极大地降低了[方差](@entry_id:200758)。[@problem_id:2889293]
- **[收敛速度](@entry_id:636873)**：在理想条件下，半[参数化](@entry_id:272587)估计器可以实现对[参数化](@entry_id:272587)部分 $H$ 的快速收敛（[方差](@entry_id:200758)为 $O(1/N)$），同时实现对非[参数化](@entry_id:272587)部分 $g$ 的最优非[参数化](@entry_id:272587)[收敛率](@entry_id:146534)。[@problem_id:2889293]
- **[可解释性](@entry_id:637759)**：这是半参数化模型最吸引人的优点之一。它保留了[参数化](@entry_id:272587)部分（如LTI环节的极点、零点、[谐振频率](@entry_id:265742)等）的物理[可解释性](@entry_id:637759)，同时又能灵活地捕捉未知的非[线性关系](@entry_id:267880)。这种将可解释的物理结构与数据驱动的灵活性相结合的能力，使其成为科学和工程建模中一种极其强大的工具。[@problem_id:2889293]

总之，从参数化到非参数化，再到半参数化的演进，反映了建模理念从“基于强假设的推断”到“数据驱动的灵活学习”，再到“结构与灵活性相融合”的深化。理解每种[范式](@entry_id:161181)的原理、优势与局限，是任何一位严肃的建模者构建有效、可靠且富有洞察力的科学模型的基石。