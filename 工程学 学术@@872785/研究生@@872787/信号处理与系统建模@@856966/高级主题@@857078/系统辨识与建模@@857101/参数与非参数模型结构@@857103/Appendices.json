{"hands_on_practices": [{"introduction": "参数化建模的核心是将数据拟合到预定义的结构中。本练习 ([@problem_id:2889301]) 为一个常见的自回归外源输入 (ARX) 模型，提供了使用最基本方法之一——最小二乘法——的动手实践。通过根据给定数据集构建正规方程，您将巩固对如何从输入输出观测中估计模型参数的理解。", "problem": "考虑以预测误差形式给出的线性时不变自回归外源输入（ARX）模型\n$$\ny(k) \\;=\\; -a_{1}\\,y(k-1)\\;-\\;a_{2}\\,y(k-2)\\;+\\;b_{1}\\,u(k-1)\\;+\\;b_{2}\\,u(k-2)\\;+\\;e(k),\n$$\n其中 $y(k)$ 是系统输出，$u(k)$ 是已知输入，$e(k)$ 是零均值扰动，未知参数矢量为 $\\theta \\equiv \\begin{pmatrix} a_{1}  a_{2}  b_{1}  b_{2} \\end{pmatrix}^{\\top}$。给定测量的输入-输出数据和初始条件\n$$\nu(-1)=0,\\quad u(0)=0,\\quad y(-1)=0,\\quad y(0)=0,\n$$\n以及以下有限序列：\n$$\n\\begin{aligned}\nu(1)=1,\\; u(2)=2,\\; u(3)=0,\\; u(4)=-1,\\; u(5)=1,\\; u(6)=0,\\\\\ny(1)=0,\\; y(2)=1,\\; y(3)=2.5,\\; y(4)=1.05,\\; y(5)=-0.975,\\; y(6)=0.3025,\\; y(7)=0.34625.\n\\end{aligned}\n$$\n假设对于此数据集，扰动恒为零，即对于下面使用的所有 $k$，$e(k)=0$。构建批量最小二乘估计问题，最小化数据索引 $k=2,3,4,5,6,7$ 上的单步预测误差平方和，并根据上述数据构建相应的线性正规方程组。然后计算参数 $b_{1}$ 的最小二乘估计值。\n\n将标量 $b_{1}$ 的最终报告值表示为一个精确数。无需四舍五入。最终答案必须是单个实数。", "solution": "我们从模型定义和最小二乘原理开始。在时间 $k$ 的单步预测误差为\n$$\n\\varepsilon(k;\\theta)\\;=\\;y(k)\\;-\\;\\big(-a_{1}\\,y(k-1)-a_{2}\\,y(k-2)+b_{1}\\,u(k-1)+b_{2}\\,u(k-2)\\big).\n$$\n定义堆叠回归矢量\n$$\n\\varphi(k)\\;\\equiv\\;\\begin{pmatrix}-y(k-1)\\\\ -y(k-2)\\\\ u(k-1)\\\\ u(k-2)\\end{pmatrix},\\qquad \\theta\\;\\equiv\\;\\begin{pmatrix}a_{1}\\\\ a_{2}\\\\ b_{1}\\\\ b_{2}\\end{pmatrix},\n$$\n因此误差为 $\\varepsilon(k;\\theta)=y(k)-\\varphi(k)^{\\top}\\theta$。在索引集 $\\mathcal{K}=\\{2,3,4,5,6,7\\}$ 上的批量最小二乘（LS）准则为\n$$\nJ(\\theta)\\;=\\;\\sum_{k\\in\\mathcal{K}}\\varepsilon(k;\\theta)^{2}\\;=\\;\\sum_{k\\in\\mathcal{K}}\\big(y(k)-\\varphi(k)^{\\top}\\theta\\big)^{2}.\n$$\n通过将其梯度设为零来最小化 $J(\\theta)$（关于 $\\theta$），得到正规方程组\n$$\n\\left(\\sum_{k\\in\\mathcal{K}}\\varphi(k)\\,\\varphi(k)^{\\top}\\right)\\theta\\;=\\;\\sum_{k\\in\\mathcal{K}}\\varphi(k)\\,y(k).\n$$\n等价地，如果我们定义数据矩阵 $\\Phi\\in\\mathbb{R}^{6\\times 4}$，其行为 $\\varphi(k)^{\\top}$（其中 $k=2,\\dots,7$），以及数据矢量 $Y\\in\\mathbb{R}^{6}$，其元素为 $y(k)$（其中 $k=2,\\dots,7$），我们可以得到紧凑形式\n$$\n\\Phi^{\\top}\\Phi\\,\\theta\\;=\\;\\Phi^{\\top}Y.\n$$\n\n现在我们根据所提供的数据显式地构建 $\\Phi$ 和 $Y$。对于每个 $k\\in\\{2,3,4,5,6,7\\}$，我们计算 $\\varphi(k)$：\n$$\n\\begin{aligned}\nk=2:\\;\\; \\varphi(2)^{\\top}=\\begin{pmatrix}-y(1)  -y(0)  u(1)  u(0)\\end{pmatrix}=\\begin{pmatrix}0  0  1  0\\end{pmatrix},\\;\\; y(2)=1,\\\\\nk=3:\\;\\; \\varphi(3)^{\\top}=\\begin{pmatrix}-y(2)  -y(1)  u(2)  u(1)\\end{pmatrix}=\\begin{pmatrix}-1  0  2  1\\end{pmatrix},\\;\\; y(3)=2.5,\\\\\nk=4:\\;\\; \\varphi(4)^{\\top}=\\begin{pmatrix}-y(3)  -y(2)  u(3)  u(2)\\end{pmatrix}=\\begin{pmatrix}-2.5  -1  0  2\\end{pmatrix},\\;\\; y(4)=1.05,\\\\\nk=5:\\;\\; \\varphi(5)^{\\top}=\\begin{pmatrix}-y(4)  -y(3)  u(4)  u(3)\\end{pmatrix}=\\begin{pmatrix}-1.05  -2.5  -1  0\\end{pmatrix},\\;\\; y(5)=-0.975,\\\\\nk=6:\\;\\; \\varphi(6)^{\\top}=\\begin{pmatrix}-y(5)  -y(4)  u(5)  u(4)\\end{pmatrix}=\\begin{pmatrix}0.975  -1.05  1  -1\\end{pmatrix},\\;\\; y(6)=0.3025,\\\\\nk=7:\\;\\; \\varphi(7)^{\\top}=\\begin{pmatrix}-y(6)  -y(5)  u(6)  u(5)\\end{pmatrix}=\\begin{pmatrix}-0.3025  0.975  0  1\\end{pmatrix},\\;\\; y(7)=0.34625.\n\\end{aligned}\n$$\n因此\n$$\n\\Phi=\\begin{pmatrix}\n0  0  1  0\\\\\n-1  0  2  1\\\\\n-2.5  -1  0  2\\\\\n-1.05  -2.5  -1  0\\\\\n0.975  -1.05  1  -1\\\\\n-0.3025  0.975  0  1\n\\end{pmatrix},\\qquad\nY=\\begin{pmatrix}\n1\\\\\n2.5\\\\\n1.05\\\\\n-0.975\\\\\n0.3025\\\\\n0.34625\n\\end{pmatrix}.\n$$\n正规方程组的具体形式为\n$$\n\\underbrace{\\Phi^{\\top}\\Phi}_{G}\\,\\theta\\;=\\;\\underbrace{\\Phi^{\\top}Y}_{g},\n$$\n其中\n$$\nG=\\begin{pmatrix}\n9.39463125  3.8063125  0.025  -7.2775\\\\\n3.8063125  9.303125  1.45  0.025\\\\\n0.025  1.45  7  1\\\\\n-7.2775  0.025  1  7\n\\end{pmatrix},\\qquad\ng=\\begin{pmatrix}\n-3.911053125\\\\\n1.40746875\\\\\n7.2775\\\\\n4.64375\n\\end{pmatrix}.\n$$\n至此完成了正规方程组的构建。\n\n为了计算最小二乘估计，我们注意到，如果数据无噪声且 $\\Phi$ 是满列秩的，则最小二乘解 $\\hat{\\theta}$ 精确满足 $\\Phi\\,\\hat{\\theta}=Y$。从第一个回归行（对应于 $k=2$）来看，\n$$\ny(2)\\;=\\;-a_{1}\\,y(1)\\;-\\;a_{2}\\,y(0)\\;+\\;b_{1}\\,u(1)\\;+\\;b_{2}\\,u(0)\\;=\\;0\\;+\\;0\\;+\\;b_{1}\\cdot 1\\;+\\;0,\n$$\n这立即意味着\n$$\nb_{1}\\;=\\;y(2)\\;=\\;1.\n$$\n为完整起见，我们可以验证存在 $(a_{1},a_{2},b_{2})$，使得在使用此 $b_{1}$ 时所有残差均为零。使用随后的三个方程（对应于 $k=3,4,5$）并设 $b_{1}=1$，\n$$\n\\begin{aligned}\nk=3:\\;\\;2.5=-a_{1}\\cdot 1-a_{2}\\cdot 0+1\\cdot 2+b_{2}\\cdot 1\\;\\;\\Rightarrow\\;\\;-a_{1}+b_{2}=0.5,\\\\\nk=4:\\;\\;1.05=-a_{1}\\cdot 2.5-a_{2}\\cdot 1+1\\cdot 0+b_{2}\\cdot 2\\;\\;\\Rightarrow\\;\\;-2.5a_{1}-a_{2}+2b_{2}=1.05,\\\\\nk=5:\\;\\;-0.975=-a_{1}\\cdot 1.05-a_{2}\\cdot 2.5+1\\cdot(-1)+b_{2}\\cdot 0\\;\\;\\Rightarrow\\;\\;-1.05a_{1}-2.5a_{2}=-0.975+1=0.025,\n\\end{aligned}\n$$\n解得 $a_{1}=-0.5$，$a_{2}=0.2$ 和 $b_{2}=0$。使用这些值，所有六个方程都精确满足，因此残差矢量恒为零。因此，最小二乘估计量与此解一致，特别地，$b_{1}$ 的最小二乘估计为\n$$\n\\hat{b}_{1}\\;=\\;1.\n$$\n该值也满足正规方程组的第三个分量，因为代入 $\\theta=\\begin{pmatrix}-0.5  0.2  1  0\\end{pmatrix}^{\\top}$ 可得\n$$\n\\begin{pmatrix}0.025  1.45  7  1\\end{pmatrix}\\theta\\;=\\;0.025(-0.5)+1.45(0.2)+7(1)+1(0)\\;=\\;-0.0125+0.29+7\\;=\\;7.2775\\;=\\;g_{3}.\n$$\n因此，对于给定的数据集，$b_{1}$ 的最小二乘估计值恰好为 $1$。", "answer": "$$\\boxed{1}$$", "id": "2889301"}, {"introduction": "与参数化方法相反，非参数化方法在不假设特定底层模型结构的情况下估计系统特性。本练习 ([@problem_id:2889322]) 探讨了 Welch 法，这是一种强大的功率谱密度 (PSD) 估计算法，并聚焦于一个关键概念：频率分辨率。通过从所选窗函数的属性推导分辨率，您将深入了解非参数谱分析中分辨率与方差之间的基本权衡。", "problem": "一个实的、宽平稳、零均值的离散时间过程 $x[n]$ 以 $f_{s} = 48\\,\\text{kHz}$ 的采样频率进行采样。你需要使用 Welch 方法（一种非参数谱估计器）来估计其功率谱密度（PSD），并将其频率分辨率的概念与参数方法的频率分辨率进行对比，而无需计算任何参数估计。总记录长度为 $N_{\\text{tot}} = 1{,}228{,}800$ 个样本。你选择以下 Welch 参数：段长 $L = 4096$ 个样本，相邻段之间有 $50\\%$ 的重叠，以及周期性 Hann 窗 $w[n]$（定义域为 $n=0,1,\\dots,L-1$）如下\n$$\nw[n] = \\tfrac{1}{2}\\Big(1 - \\cos\\!\\big(\\tfrac{2\\pi n}{L}\\big)\\Big).\n$$\n假设每个段的离散傅里叶变换（DFT）长度为 $K=L$（无零填充）。\n\n在此设置下，将 Welch PSD 估计的频率分辨率定义为谱窗的等效噪声带宽（ENBW），以赫兹（hertz）为单位表示。仅使用 Welch 方法和 ENBW 的基本定义，以及离散时间余弦序列求和的性质，计算最终的频率分辨率（以赫兹为单位）。以无舍入的精确值形式提供最终答案，并以 $\\text{Hz}$ 表示结果（最终答案框中不包含单位）。", "solution": "对于像 Welch 方法这样的非参数谱估计器，频率分辨率由应用于每个数据段的窗函数的光谱特性决定。问题将此分辨率定义为窗的等效噪声带宽（ENBW），以赫兹为单位表示。\n\n长度为 $L$ 的离散时间窗 $w[n]$ 的 ENBW 在归一化频率（周/样本）下定义为窗系数平方和与窗系数和的平方之比：\n$$\n\\text{ENBW}_{\\text{norm}} = \\frac{\\sum_{n=0}^{L-1} w^2[n]}{\\left(\\sum_{n=0}^{L-1} w[n]\\right)^2}\n$$\n要将此归一化带宽转换为以赫兹为单位的物理频率，必须乘以采样频率 $f_s$：\n$$\n\\text{Resolution} = \\text{ENBW}_{\\text{Hz}} = \\text{ENBW}_{\\text{norm}} \\times f_s\n$$\n任务简化为计算给定周期性 Hann 窗 $w[n] = \\frac{1}{2}\\left(1 - \\cos\\left(\\frac{2\\pi n}{L}\\right)\\right)$（对于 $n=0, 1, \\dots, L-1$）的两个和。\n\n首先，我们计算窗系数的和，记为 $S_1$：\n$$\nS_1 = \\sum_{n=0}^{L-1} w[n] = \\sum_{n=0}^{L-1} \\frac{1}{2}\\left(1 - \\cos\\left(\\frac{2\\pi n}{L}\\right)\\right)\n$$\n根据求和的线性性质：\n$$\nS_1 = \\frac{1}{2} \\left[ \\left(\\sum_{n=0}^{L-1} 1\\right) - \\left(\\sum_{n=0}^{L-1} \\cos\\left(\\frac{2\\pi n}{L}\\right)\\right) \\right]\n$$\n第一项是 $\\sum_{n=0}^{L-1} 1 = L$。第二项是一个完整周期（$L$ 个样本）上的余弦和。对于 $L > 1$，此和为零。这可以通过考虑复指数 $\\sum_{n=0}^{L-1} \\exp\\left(j\\frac{2\\pi k n}{L}\\right)$ 的和来证明，对于任何非 $L$ 倍数的整数 $k$，该和等于 $0$。此处 $k=1$。取实部证实了 $\\sum_{n=0}^{L-1} \\cos(\\frac{2\\pi n}{L}) = 0$。给定 $L = 4096$，此条件成立。\n因此，\n$$\nS_1 = \\frac{1}{2} (L - 0) = \\frac{L}{2}\n$$\n\n接下来，我们计算窗系数平方的和，记为 $S_2$：\n$$\nS_2 = \\sum_{n=0}^{L-1} w^2[n] = \\sum_{n=0}^{L-1} \\left[ \\frac{1}{2}\\left(1 - \\cos\\left(\\frac{2\\pi n}{L}\\right)\\right) \\right]^2 = \\frac{1}{4} \\sum_{n=0}^{L-1} \\left(1 - \\cos\\left(\\frac{2\\pi n}{L}\\right)\\right)^2\n$$\n展开平方项：\n$$\nS_2 = \\frac{1}{4} \\sum_{n=0}^{L-1} \\left(1 - 2\\cos\\left(\\frac{2\\pi n}{L}\\right) + \\cos^2\\left(\\frac{2\\pi n}{L}\\right)\\right)\n$$\n我们使用降幂恒等式 $\\cos^2(\\theta) = \\frac{1}{2}(1 + \\cos(2\\theta))$：\n$$\nS_2 = \\frac{1}{4} \\sum_{n=0}^{L-1} \\left(1 - 2\\cos\\left(\\frac{2\\pi n}{L}\\right) + \\frac{1}{2}\\left(1 + \\cos\\left(\\frac{4\\pi n}{L}\\right)\\right)\\right)\n$$\n$$\nS_2 = \\frac{1}{4} \\sum_{n=0}^{L-1} \\left(\\frac{3}{2} - 2\\cos\\left(\\frac{2\\pi n}{L}\\right) + \\frac{1}{2}\\cos\\left(\\frac{4\\pi n}{L}\\right)\\right)\n$$\n分配求和：\n$$\nS_2 = \\frac{1}{4} \\left[ \\frac{3}{2}\\sum_{n=0}^{L-1} 1 - 2\\sum_{n=0}^{L-1} \\cos\\left(\\frac{2\\pi n}{L}\\right) + \\frac{1}{2}\\sum_{n=0}^{L-1} \\cos\\left(\\frac{4\\pi n}{L}\\right) \\right]\n$$\n如前所述，$\\sum \\cos(\\frac{2\\pi n}{L}) = 0$。类似地，对于 $L>2$，$\\sum \\cos(\\frac{4\\pi n}{L}) = 0$，这对 $L=4096$ 是成立的。\n这剩下：\n$$\nS_2 = \\frac{1}{4} \\left[ \\frac{3L}{2} - 2(0) + \\frac{1}{2}(0) \\right] = \\frac{3L}{8}\n$$\n\n现在，我们可以计算归一化的 ENBW：\n$$\n\\text{ENBW}_{\\text{norm}} = \\frac{S_2}{S_1^2} = \\frac{\\frac{3L}{8}}{\\left(\\frac{L}{2}\\right)^2} = \\frac{\\frac{3L}{8}}{\\frac{L^2}{4}} = \\frac{3L \\cdot 4}{8 \\cdot L^2} = \\frac{12L}{8L^2} = \\frac{3}{2L}\n$$\n这就是 Hann 窗众所周知的 ENBW。频率分辨率与窗长 $L$ 成反比。\n\n最后，我们使用给定值 $L=4096$ 和 $f_s = 48000\\,\\text{Hz}$ 计算以赫兹为单位的分辨率：\n$$\n\\text{Resolution} = \\text{ENBW}_{\\text{Hz}} = \\frac{3}{2L} \\times f_s = \\frac{3}{2 \\times 4096} \\times 48000 = \\frac{3 \\times 48000}{8192} = \\frac{144000}{8192}\n$$\n为了简化分数：\n$$\n\\frac{144000}{8192} = \\frac{144 \\times 1000}{8 \\times 1024} = \\frac{18 \\times 1000}{1024} = \\frac{18000}{1024} = \\frac{9000}{512} = \\frac{4500}{256} = \\frac{2250}{128} = \\frac{1125}{64}\n$$\n精确值为 $1125/64$，其十进制形式为 $17.578125$。\n\n问题还要求与参数方法进行对比。在像 Welch 方法这样的非参数方法中，频率分辨率（此处定义为 $1.5 \\frac{f_s}{L}$）由分析者对窗长 $L$ 的选择固定。更长的窗提供更好的分辨率（更窄的主瓣），但代价是在总数据记录固定的情况下，估计器的方差会增加，因为可以平均的段数更少。这种分辨率与信号内容无关。相比之下，参数方法（例如，自回归（AR）、移动平均（MA）或 ARMA 模型）假定信号是由白噪声驱动的线性时不变系统生成的。PSD 则是模型参数的函数。在这种情况下，“分辨率”不是一个固定的带宽，而是表示尖锐谱特征的能力。如果基础数据符合所选的模型结构，参数方法可以实现卓越的分辨率，即使在非常短的数据记录下也能分辨出间隔很近的正弦波，远远超过非参数方法的 $1/L$ 限制。然而，这种性能严重依赖于模型假设的正确性；模型失配可能导致谱估计严重不准确和误导。", "answer": "$$\\boxed{17.578125}$$", "id": "2889322"}, {"introduction": "参数化模型虽然功能强大，但容易出现过拟合，尤其是在数据有限或含噪声的情况下。这个高级练习 ([@problem_id:2889347]) 介绍了作为一种正则化模型参数并提高泛化能力的方法——岭回归。您将推导并应用广义交叉验证 (GCV) 准则，这是一种用于选择最佳正则化强度 $\\lambda$ 的、由数据驱动的技术，从而将估计理论与实际模型构建联系起来。", "problem": "考虑从输入输出数据 $\\{(u(t),y(t))\\}_{t=1}^{T}$ 中辨识一个 $m$ 阶的有限脉冲响应 (FIR) 模型。设回归矩阵为 $\\Phi \\in \\mathbb{R}^{T \\times m}$，其第 $t$ 行为 $\\varphi(t)^{\\top} \\coloneqq \\big(u(t),u(t-1),\\dots,u(t-m+1)\\big)$，参数模型为 $y(t) \\approx \\varphi(t)^{\\top}\\theta$，其中 $\\theta \\in \\mathbb{R}^{m}$。岭正则化辨识通过最小化惩罚最小二乘准则 $\\|\\mathbf{y}-\\Phi\\theta\\|_{2}^{2}+\\lambda\\|\\theta\\|_{2}^{2}$ 来估计 $\\theta$，其中正则化参数 $\\lambda \\ge 0$，$\\mathbf{y} \\in \\mathbb{R}^{T}$ 是将 $y(t)$ 堆叠而成的向量。\n\n仅从正规方程和基本线性代数恒等式出发，为岭正则化 FIR 辨识推导广义交叉验证 (GCV) 准则。你的推导必须：\n- 明确指出将 $\\mathbf{y}$ 映射到其拟合值 $\\widehat{\\mathbf{y}}$ 的线性平滑（帽子）矩阵 $S(\\lambda)$，\n- 用样本内残差和 $S(\\lambda)$ 的对角线元素表示留一交叉验证 (LOOCV) 残差，\n- 通过用单个对角线元素的平均值替换它们来获得 GCV 泛函 $V_{\\mathrm{GCV}}(\\lambda)$。\n\n然后将你的表达式特化到 $m=1$ 的情况（单个 FIR 抽头），此时 $\\Phi=\\mathbf{u}\\in\\mathbb{R}^{T\\times 1}$ 且 $\\mathbf{u} \\coloneqq \\big(u(1),\\dots,u(T)\\big)^{\\top}$。证明 $V_{\\mathrm{GCV}}(\\lambda)$ 化简为一个关于 $\\lambda$ 的一维函数，该函数依赖于 $\\Phi$ 的奇异值以及 $\\mathbf{y}$ 在其列空间和正交补上的投影。\n\n最后，对于具体数据集 $T=5$，输入序列 $\\mathbf{u}=\\begin{pmatrix}1\\\\2\\\\-1\\\\0\\\\2\\end{pmatrix}$ 和输出序列 $\\mathbf{y}=\\begin{pmatrix}5\\\\4\\\\1\\\\0\\\\4\\end{pmatrix}$，计算使所推导的 $V_{\\mathrm{GCV}}(\\lambda)$ 最小化的唯一值 $\\lambda^{\\star}\\ge 0$。将最终的 $\\lambda^{\\star}$ 以精确值的形式给出。无需四舍五入。最终答案必须是一个单个实数。", "solution": "目标是找到最小化成本函数的岭回归参数估计 $\\widehat{\\theta}$：\n$$J(\\theta) = \\|\\mathbf{y}-\\Phi\\theta\\|_{2}^{2}+\\lambda\\|\\theta\\|_{2}^{2}$$\n其中 $\\mathbf{y} \\in \\mathbb{R}^{T}$ 是输出向量，$\\Phi \\in \\mathbb{R}^{T \\times m}$ 是回归矩阵，$\\theta \\in \\mathbb{R}^{m}$ 是参数向量，$\\lambda \\ge 0$ 是正则化参数。\n\n首先，我们推导正规方程。成本函数可以写为 $J(\\theta) = (\\mathbf{y}-\\Phi\\theta)^{\\top}(\\mathbf{y}-\\Phi\\theta)+\\lambda\\theta^{\\top}\\theta$。为了找到最小值，我们计算关于 $\\theta$ 的梯度并将其设为零：\n$$ \\nabla_{\\theta} J(\\theta) = \\nabla_{\\theta} (\\mathbf{y}^{\\top}\\mathbf{y} - 2\\mathbf{y}^{\\top}\\Phi\\theta + \\theta^{\\top}\\Phi^{\\top}\\Phi\\theta + \\lambda\\theta^{\\top}\\theta) = -2\\Phi^{\\top}\\mathbf{y} + 2\\Phi^{\\top}\\Phi\\theta + 2\\lambda\\theta $$\n将梯度设为零，我们得到岭回归的正规方程：\n$$ (\\Phi^{\\top}\\Phi + \\lambda I)\\theta = \\Phi^{\\top}\\mathbf{y} $$\n对于任何 $\\lambda  0$，矩阵 $(\\Phi^{\\top}\\Phi + \\lambda I)$ 都是正定的，因此是可逆的。解是唯一的：\n$$ \\widehat{\\theta}(\\lambda) = (\\Phi^{\\top}\\Phi + \\lambda I)^{-1}\\Phi^{\\top}\\mathbf{y} $$\n\n我们现在推导广义交叉验证 (GCV) 准则。\n\n**步骤 1：确定平滑（帽子）矩阵**\n拟合值向量 $\\widehat{\\mathbf{y}}$ 由 $\\widehat{\\mathbf{y}} = \\Phi\\widehat{\\theta}(\\lambda)$ 给出。代入 $\\widehat{\\theta}(\\lambda)$ 的表达式：\n$$ \\widehat{\\mathbf{y}} = \\Phi(\\Phi^{\\top}\\Phi + \\lambda I)^{-1}\\Phi^{\\top}\\mathbf{y} $$\n根据定义，平滑矩阵，或称帽子矩阵，$S(\\lambda)$ 将观测数据 $\\mathbf{y}$ 映射到拟合值 $\\widehat{\\mathbf{y}}$，即 $\\widehat{\\mathbf{y}} = S(\\lambda)\\mathbf{y}$。因此，我们确定帽子矩阵为：\n$$ S(\\lambda) = \\Phi(\\Phi^{\\top}\\Phi + \\lambda I)^{-1}\\Phi^{\\top} $$\n该矩阵是一个线性算子，依赖于正则化参数 $\\lambda$。\n\n**步骤 2：表示留一交叉验证 (LOOCV) 残差**\n留一交叉验证误差为 $V_{\\mathrm{LOOCV}}(\\lambda) = \\frac{1}{T} \\sum_{k=1}^{T} (y(k) - \\widehat{y}^{(-k)}(k))^2$，其中 $\\widehat{y}^{(-k)}(k)$ 是使用在除第 $k$ 个数据点之外的所有数据上训练的模型对第 $k$ 个数据点所做的预测。线性回归分析中的一个基本结果，称为 PRESS（预测误差平方和）恒等式，将 LOOCV 残差 $y(k) - \\widehat{y}^{(-k)}(k)$ 与普通的样本内残差 $e(k) = y(k) - \\widehat{y}(k)$ 联系起来，其中 $\\widehat{y}(k)$ 是 $\\widehat{\\mathbf{y}} = S(\\lambda)\\mathbf{y}$ 的第 $k$ 个分量。该恒等式为：\n$$ y(k) - \\widehat{y}^{(-k)}(k) = \\frac{y(k) - \\widehat{y}(k)}{1 - S_{kk}(\\lambda)} $$\n其中 $S_{kk}(\\lambda)$ 是帽子矩阵 $S(\\lambda)$ 的第 $k$ 个对角元素。样本内残差向量为 $\\mathbf{e} = \\mathbf{y} - \\widehat{\\mathbf{y}} = (I-S(\\lambda))\\mathbf{y}$。\n\n**步骤 3：获得 GCV 泛函**\nLOOCV 准则是通过对这些 LOOCV 残差的平方求和形成的：\n$$ V_{\\mathrm{LOOCV}}(\\lambda) = \\frac{1}{T} \\sum_{k=1}^{T} \\left( \\frac{y(k) - \\widehat{y}(k)}{1 - S_{kk}(\\lambda)} \\right)^2 $$\nGCV 准则通过将分母中的各个对角元素 $S_{kk}(\\lambda)$ 替换为其平均值，从而提供了对 LOOCV 的近似。平均值由帽子矩阵的迹除以 $T$ 给出：\n$$ \\frac{1}{T} \\sum_{k=1}^{T} S_{kk}(\\lambda) = \\frac{1}{T} \\mathrm{tr}(S(\\lambda)) $$\n将此平均值代入 LOOCV 表达式，得到 GCV 泛函 $V_{\\mathrm{GCV}}(\\lambda)$：\n$$ V_{\\mathrm{GCV}}(\\lambda) = \\frac{1}{T} \\sum_{k=1}^{T} \\left( \\frac{y(k) - \\widehat{y}(k)}{1 - \\frac{1}{T}\\mathrm{tr}(S(\\lambda))} \\right)^2 = \\frac{\\frac{1}{T} \\sum_{k=1}^{T} (y(k) - \\widehat{y}(k))^2}{\\left(1 - \\frac{1}{T}\\mathrm{tr}(S(\\lambda))\\right)^2} $$\n这通常写为：\n$$ V_{\\mathrm{GCV}}(\\lambda) = \\frac{\\frac{1}{T}\\|\\mathbf{y} - \\widehat{\\mathbf{y}}\\|_2^2}{\\left(1 - \\frac{1}{T}\\mathrm{tr}(S(\\lambda))\\right)^2} = \\frac{\\frac{1}{T}\\|(I-S(\\lambda))\\mathbf{y}\\|_2^2}{\\left(1 - \\frac{1}{T}\\mathrm{tr}(S(\\lambda))\\right)^2} $$\n这就完成了一般性推导。\n\n**特化到 $m=1$ 的情况**\n当 $m=1$ 时，回归矩阵是一个单列向量 $\\Phi = \\mathbf{u} \\in \\mathbb{R}^{T \\times 1}$。项 $\\Phi^{\\top}\\Phi$ 变为一个标量：$\\mathbf{u}^{\\top}\\mathbf{u} = \\|\\mathbf{u}\\|_2^2$。这是 $\\Phi$ 的奇异值的平方，我们记为 $\\sigma_1^2 = \\sigma^2 = \\|\\mathbf{u}\\|_2^2$。\n\n帽子矩阵的迹变为：\n$$ \\mathrm{tr}(S(\\lambda)) = \\mathrm{tr}(\\mathbf{u}(\\mathbf{u}^{\\top}\\mathbf{u} + \\lambda)^{-1}\\mathbf{u}^{\\top}) = \\mathrm{tr}((\\mathbf{u}^{\\top}\\mathbf{u} + \\lambda)^{-1}\\mathbf{u}^{\\top}\\mathbf{u}) = \\frac{\\mathbf{u}^{\\top}\\mathbf{u}}{\\mathbf{u}^{\\top}\\mathbf{u} + \\lambda} = \\frac{\\sigma^2}{\\sigma^2 + \\lambda} $$\n$V_{\\mathrm{GCV}}(\\lambda)$ 的分母是 $(1 - \\frac{1}{T}\\frac{\\sigma^2}{\\sigma^2+\\lambda})^2$。\n\n接下来，我们分析分子。帽子矩阵简化为：\n$$ S(\\lambda) = \\mathbf{u}(\\sigma^2+\\lambda)^{-1}\\mathbf{u}^{\\top} = \\frac{\\mathbf{u}\\mathbf{u}^{\\top}}{\\sigma^2+\\lambda} $$\n设 $P_{\\mathbf{u}} = \\frac{\\mathbf{u}\\mathbf{u}^{\\top}}{\\mathbf{u}^{\\top}\\mathbf{u}} = \\frac{\\mathbf{u}\\mathbf{u}^{\\top}}{\\sigma^2}$ 为到 $\\mathbf{u}$ 的列空间上的投影矩阵。那么 $S(\\lambda) = \\frac{\\sigma^2}{\\sigma^2+\\lambda}P_{\\mathbf{u}}$。\n残差平方和项为 $\\|\\mathbf{y} - S(\\lambda)\\mathbf{y}\\|_2^2$。我们将 $\\mathbf{y}$ 分解为其在 $\\mathbf{u}$ 列空间上的投影 $\\mathbf{y}_{\\parallel} = P_{\\mathbf{u}}\\mathbf{y}$ 和其在正交补上的投影 $\\mathbf{y}_{\\perp} = (I-P_{\\mathbf{u}})\\mathbf{y}$ 之和。\n残差向量为：\n$$ \\mathbf{y} - S(\\lambda)\\mathbf{y} = \\mathbf{y} - \\frac{\\sigma^2}{\\sigma^2+\\lambda}P_{\\mathbf{u}}\\mathbf{y} = \\mathbf{y}_{\\parallel} + \\mathbf{y}_{\\perp} - \\frac{\\sigma^2}{\\sigma^2+\\lambda}\\mathbf{y}_{\\parallel} = \\left(1-\\frac{\\sigma^2}{\\sigma^2+\\lambda}\\right)\\mathbf{y}_{\\parallel} + \\mathbf{y}_{\\perp} = \\frac{\\lambda}{\\sigma^2+\\lambda}\\mathbf{y}_{\\parallel} + \\mathbf{y}_{\\perp} $$\n由于 $\\mathbf{y}_{\\parallel}$ 和 $\\mathbf{y}_{\\perp}$ 是正交的，平方范数是它们各自平方范数的和：\n$$ \\|\\mathbf{y} - S(\\lambda)\\mathbf{y}\\|_2^2 = \\left\\|\\frac{\\lambda}{\\sigma^2+\\lambda}\\mathbf{y}_{\\parallel}\\right\\|_2^2 + \\|\\mathbf{y}_{\\perp}\\|_2^2 = \\left(\\frac{\\lambda}{\\sigma^2+\\lambda}\\right)^2\\|\\mathbf{y}_{\\parallel}\\|_2^2 + \\|\\mathbf{y}_{\\perp}\\|_2^2 $$\n综合这些结果，GCV 泛函为：\n$$ V_{\\mathrm{GCV}}(\\lambda) = \\frac{\\frac{1}{T}\\left[ \\left(\\frac{\\lambda}{\\sigma^2+\\lambda}\\right)^2\\|\\mathbf{y}_{\\parallel}\\|_2^2 + \\|\\mathbf{y}_{\\perp}\\|_2^2 \\right]}{\\left(1 - \\frac{1}{T}\\frac{\\sigma^2}{\\sigma^2+\\lambda}\\right)^2} $$\n此表达式显然依赖于 $\\Phi$ 的奇异值（通过 $\\sigma^2$）和 $\\mathbf{y}$ 的投影。\n\n**针对特定数据集的计算**\n给定 $T=5$，$\\mathbf{u}=\\begin{pmatrix}1\\\\2\\\\-1\\\\0\\\\2\\end{pmatrix}$，以及 $\\mathbf{y}=\\begin{pmatrix}5\\\\4\\\\1\\\\0\\\\4\\end{pmatrix}$，我们计算所需的量。\n奇异值的平方为 $\\sigma^2 = \\|\\mathbf{u}\\|_2^2 = 1^2+2^2+(-1)^2+0^2+2^2 = 1+4+1+0+4=10$。\n$\\mathbf{y}$ 在 $\\mathbf{u}$ 上的投影需要内积 $\\mathbf{u}^{\\top}\\mathbf{y}$：\n$$ \\mathbf{u}^{\\top}\\mathbf{y} = (1)(5) + (2)(4) + (-1)(1) + (0)(0) + (2)(4) = 5+8-1+0+8 = 20 $$\n$\\mathbf{y}$ 的平行分量的平方范数为：\n$$ \\|\\mathbf{y}_{\\parallel}\\|_2^2 = \\|P_{\\mathbf{u}}\\mathbf{y}\\|_2^2 = \\frac{(\\mathbf{u}^{\\top}\\mathbf{y})^2}{\\mathbf{u}^{\\top}\\mathbf{u}} = \\frac{20^2}{10} = \\frac{400}{10} = 40 $$\n原始输出向量的平方范数为 $\\|\\mathbf{y}\\|_2^2 = 5^2+4^2+1^2+0^2+4^2 = 25+16+1+0+16 = 58$。\n正交分量的平方范数通过勾股定理求得：\n$$ \\|\\mathbf{y}_{\\perp}\\|_2^2 = \\|\\mathbf{y}\\|_2^2 - \\|\\mathbf{y}_{\\parallel}\\|_2^2 = 58 - 40 = 18 $$\n现在，我们将 $T=5$，$\\sigma^2=10$，$\\|\\mathbf{y}_{\\parallel}\\|_2^2=40$ 和 $\\|\\mathbf{y}_{\\perp}\\|_2^2=18$ 代入 $V_{\\mathrm{GCV}}(\\lambda)$ 的表达式中：\n$$ V_{\\mathrm{GCV}}(\\lambda) = \\frac{\\frac{1}{5}\\left[ \\left(\\frac{\\lambda}{10+\\lambda}\\right)^2(40) + 18 \\right]}{\\left(1 - \\frac{1}{5}\\frac{10}{10+\\lambda}\\right)^2} = \\frac{\\frac{1}{5}\\left[ \\frac{40\\lambda^2}{(10+\\lambda)^2} + 18 \\right]}{\\left(\\frac{5(10+\\lambda)-10}{5(10+\\lambda)}\\right)^2} $$\n化简得：\n$$ V_{\\mathrm{GCV}}(\\lambda) = \\frac{\\frac{1}{5} \\frac{40\\lambda^2 + 18(10+\\lambda)^2}{(10+\\lambda)^2}}{\\frac{(40+5\\lambda)^2}{25(10+\\lambda)^2}} = \\frac{5[40\\lambda^2 + 18(100+20\\lambda+\\lambda^2)]}{(40+5\\lambda)^2} $$\n$$ V_{\\mathrm{GCV}}(\\lambda) = \\frac{5[40\\lambda^2 + 1800+360\\lambda+18\\lambda^2]}{25(8+\\lambda)^2} = \\frac{58\\lambda^2+360\\lambda+1800}{5(8+\\lambda)^2} $$\n为了找到最小化 $V_{\\mathrm{GCV}}(\\lambda)$ 的值 $\\lambda^{\\star} \\ge 0$，我们对 $\\lambda$ 求导并令导数为零。我们只需要考虑函数 $f(\\lambda) = \\frac{58\\lambda^2+360\\lambda+1800}{(\\lambda+8)^2}$。\n使用商法则，$\\frac{d}{d\\lambda}\\left(\\frac{N(\\lambda)}{D(\\lambda)}\\right) = \\frac{N'(\\lambda)D(\\lambda)-N(\\lambda)D'(\\lambda)}{D(\\lambda)^2}$：\n$$ N'(\\lambda) = 116\\lambda + 360 $$\n$$ D(\\lambda) = (\\lambda+8)^2 \\implies D'(\\lambda) = 2(\\lambda+8) $$\n对于 $\\lambda \\neq -8$，令导数的分子为零：\n$$ (116\\lambda+360)(\\lambda+8) - (58\\lambda^2+360\\lambda+1800)(2) = 0 $$\n$$ 116\\lambda^2 + 928\\lambda + 360\\lambda + 2880 - 116\\lambda^2 - 720\\lambda - 3600 = 0 $$\n$\\lambda^2$ 项消掉了。我们合并余项：\n$$ (928+360-720)\\lambda + (2880-3600) = 0 $$\n$$ 568\\lambda - 720 = 0 $$\n$$ 568\\lambda = 720 $$\n$$ \\lambda^{\\star} = \\frac{720}{568} $$\n我们化简该分数。分子和分母均可被 $8$ 整除：$720/8 = 90$，$568/8 = 71$。\n$$ \\lambda^{\\star} = \\frac{90}{71} $$\n由于 $\\lambda^{\\star} = \\frac{90}{71}  0$，它是一个有效的候选解。二阶导数检验或对一阶导数符号的分析证实了这是一个最小值。因此，对于 $\\lambda \\ge 0$，最小化 $V_{\\mathrm{GCV}}(\\lambda)$ 的唯一值是 $\\frac{90}{71}$。", "answer": "$$\\boxed{\\frac{90}{71}}$$", "id": "2889347"}]}