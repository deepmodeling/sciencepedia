{"hands_on_practices": [{"introduction": "最小二乘法不仅是一种代数上的误差最小化过程，更蕴含着深刻的几何意义。此练习旨在揭示最小二乘拟合的几何本质，即它将观测数据向量正交投影到由模型回归量张成的子空间上。通过推导投影矩阵的性质，特别是它的迹 [@problem_id:2897104]，我们将量化模型在拟合数据时所消耗的“有效自由度”，为理解模型的复杂度和数据拟合能力提供一个根本性的视角。", "problem": "考虑一个线性信号模型，其中观测到的确定性信号向量 $y \\in \\mathbb{R}^{n}$ 被建模为 $p$ 个已知回归量的线性组合，这些回归量作为设计矩阵 $X \\in \\mathbb{R}^{n \\times p}$ 的列。该设计矩阵具有满列秩 $p$ 且 $p \\leq n$。最小二乘估计量 (LS) 选择系数向量 $\\hat{\\beta} \\in \\mathbb{R}^{p}$ 以最小化平方误差 $\\|y - X \\beta\\|_{2}^{2}$。设拟合信号为 $\\hat{y} = X \\hat{\\beta}$，并定义线性映射 $P \\in \\mathbb{R}^{n \\times n}$，使得对任意 $y \\in \\mathbb{R}^{n}$ 都有 $\\hat{y} = P y$。\n\n从刻画最小二乘解的最优性条件和阐明残差与模型子空间正交的正交性原理出发，推导 $P$ 的显式公式，并从基本原理出发证明 $P$ 是对称且幂等的。然后，仅使用正交投影算子的结构性质，且不假设 $\\mathbb{R}^{n}$ 的任何特定基，用 $X$ 的内在性质确定 $\\operatorname{tr}(P)$。将此迹解释为此模型中最小二乘法所使用的有效自由度。\n\n你的最终答案应为用 $p$ 表示的 $\\operatorname{tr}(P)$ 的解析值。不包含任何单位。如果引入任何额外的记号，请明确定义。你的最终答案必须是单一的封闭形式表达式。", "solution": "我们从最小二乘估计量的定义开始：$\\hat{\\beta}$ 在 $\\beta \\in \\mathbb{R}^{p}$ 上最小化 $\\|y - X \\beta\\|_{2}^{2}$。基本的最优性条件是正规方程，它通过对目标函数关于 $\\beta$ 求导并令导数为零得到：\n$$\nX^{\\mathsf{T}}\\bigl(y - X \\hat{\\beta}\\bigr) = 0.\n$$\n在 $X$ 具有满列秩 $p$ 的假设下，格拉姆矩阵 $X^{\\mathsf{T}} X \\in \\mathbb{R}^{p \\times p}$ 是对称正定的，因此是可逆的。所以，\n$$\n\\hat{\\beta} = \\bigl(X^{\\mathsf{T}} X\\bigr)^{-1} X^{\\mathsf{T}} y.\n$$\n拟合向量则为\n$$\n\\hat{y} = X \\hat{\\beta} = X \\bigl(X^{\\mathsf{T}} X\\bigr)^{-1} X^{\\mathsf{T}} y.\n$$\n根据线性映射 $P$ 的定义，对所有 $y \\in \\mathbb{R}^{n}$ 都有 $\\hat{y} = P y$，我们确定\n$$\nP = X \\bigl(X^{\\mathsf{T}} X\\bigr)^{-1} X^{\\mathsf{T}}.\n$$\n\n现在我们来证明 $P$ 是对称的。利用转置的性质以及 $X^{\\mathsf{T}} X$ 的对称性，\n$$\nP^{\\mathsf{T}} = \\bigl(X \\bigl(X^{\\mathsf{T}} X\\bigr)^{-1} X^{\\mathsf{T}}\\bigr)^{\\mathsf{T}} = X \\bigl(X^{\\mathsf{T}} X\\bigr)^{-1} X^{\\mathsf{T}} = P.\n$$\n接下来，我们验证幂等性，即 $P^{2} = P$。计算\n$$\nP^{2} = \\Bigl(X \\bigl(X^{\\mathsf{T}} X\\bigr)^{-1} X^{\\mathsf{T}}\\Bigr)\\Bigl(X \\bigl(X^{\\mathsf{T}} X\\bigr)^{-1} X^{\\mathsf{T}}\\Bigr)\n= X \\bigl(X^{\\mathsf{T}} X\\bigr)^{-1} \\underbrace{X^{\\mathsf{T}} X}_{\\text{invertible}} \\bigl(X^{\\mathsf{T}} X\\bigr)^{-1} X^{\\mathsf{T}}\n= X \\bigl(X^{\\mathsf{T}} X\\bigr)^{-1} X^{\\mathsf{T}}\n= P.\n$$\n因此 $P$ 是一个对称幂等矩阵，即到模型子空间 $\\mathcal{R}(X)$（$X$ 的值域或列空间）上的正交投影算子。这也可以从正交性原理得出：残差 $r = y - \\hat{y}$ 满足 $X^{\\mathsf{T}} r = 0$，这表明 $r \\in \\mathcal{R}(X)^{\\perp}$ 且 $\\hat{y} \\in \\mathcal{R}(X)$，因此 $P$ 是到 $\\mathcal{R}(X)$ 上的正交投影算子。\n\n为了从 $X$ 的内在性质确定 $\\operatorname{tr}(P)$，我们观察到任何实对称幂等矩阵的特征值都在 $\\{0, 1\\}$ 中。确实，如果对于某个非零向量 $v$ 有 $P v = \\lambda v$，那么 $P^{2} v = \\lambda^{2} v$，但同时 $P^{2} v = P v = \\lambda v$，因此 $\\lambda^{2} = \\lambda$，所以 $\\lambda \\in \\{0, 1\\}$。此外，$P$ 的秩等于其值为 1 的特征值的数量，也等于 $\\dim\\bigl(\\mathcal{R}(P)\\bigr)$。由于 $P$ 是到 $\\mathcal{R}(X)$ 上的正交投影算子，我们有 $\\mathcal{R}(P) = \\mathcal{R}(X)$，因此\n$$\n\\operatorname{rank}(P) = \\dim\\bigl(\\mathcal{R}(X)\\bigr) = \\operatorname{rank}(X).\n$$\n因为矩阵的迹等于其特征值之和，对于一个对称幂等矩阵 $P$，其迹等于其特征值中 1 的数量，也就是 $P$ 的秩。所以，\n$$\n\\operatorname{tr}(P) = \\operatorname{rank}(P) = \\operatorname{rank}(X).\n$$\n在给定 $X$ 具有满列秩 $p$ 的假设下，我们有 $\\operatorname{rank}(X) = p$，因此\n$$\n\\operatorname{tr}(P) = p.\n$$\n\n另一种使结构更加明确的推导方法是使用奇异值分解 (SVD)。设 $X = U \\Sigma V^{\\mathsf{T}}$ 是一个紧奇异值分解 (SVD)，其中 $U \\in \\mathbb{R}^{n \\times n}$ 是正交矩阵， $V \\in \\mathbb{R}^{p \\times p}$ 是正交矩阵，且 $\\Sigma \\in \\mathbb{R}^{n \\times p}$ 的形式为 $\\Sigma = \\begin{bmatrix} D \\\\ 0 \\end{bmatrix}$，其中 $D \\in \\mathbb{R}^{p \\times p}$ 是对角矩阵，其对角线元素严格为正，因为 $X$ 具有满列秩 $p$。那么\n$$\nP = X \\bigl(X^{\\mathsf{T}} X\\bigr)^{-1} X^{\\mathsf{T}}\n= U \\Sigma V^{\\mathsf{T}} \\bigl(V \\Sigma^{\\mathsf{T}} \\Sigma V^{\\mathsf{T}}\\bigr)^{-1} V \\Sigma^{\\mathsf{T}} U^{\\mathsf{T}}\n= U \\Sigma \\bigl(\\Sigma^{\\mathsf{T}} \\Sigma\\bigr)^{-1} \\Sigma^{\\mathsf{T}} U^{\\mathsf{T}}.\n$$\n由于 $\\Sigma^{\\mathsf{T}} \\Sigma = D^{2}$，我们得到\n$$\n\\Sigma \\bigl(\\Sigma^{\\mathsf{T}} \\Sigma\\bigr)^{-1} \\Sigma^{\\mathsf{T}}\n= \\begin{bmatrix} D \\\\ 0 \\end{bmatrix} D^{-2} \\begin{bmatrix} D  0 \\end{bmatrix}\n= \\begin{bmatrix} I_{p}  0 \\\\ 0  0 \\end{bmatrix},\n$$\n所以\n$$\nP = U \\begin{bmatrix} I_{p}  0 \\\\ 0  0 \\end{bmatrix} U^{\\mathsf{T}}.\n$$\n迹在正交矩阵 $U$ 的相似变换下是不变的，因此\n$$\n\\operatorname{tr}(P) = \\operatorname{tr}\\!\\left(\\begin{bmatrix} I_{p}  0 \\\\ 0  0 \\end{bmatrix}\\right) = p.\n$$\n\n作为有效自由度的解释：在最小二乘拟合中，$P$ 将任何数据向量 $y$ 映射到 $p$ 维子空间 $\\mathcal{R}(X)$ 上的正交投影。迹 $\\operatorname{tr}(P)$ 等于 $p$，这代表了拟合过程为适应数据而使用的有效参数数量或线性约束数量。因此，残差子空间的维数为 $n - p$，这通常被解释为可用于评估模型与数据之间差异（例如，在存在加性噪声时用于方差估计）的剩余自由度。因此，$\\operatorname{tr}(P) = p$ 量化了最小二乘法所使用的有效自由度。", "answer": "$$\\boxed{p}$$", "id": "2897104"}, {"introduction": "在理想情况下，最小二乘估计量具有良好的统计特性，但实际应用中我们常常面临模型设定不当的风险。一个常见且严重的问题是遗漏了重要的解释变量。本练习将引导我们探讨当真实模型包含多个变量，而我们错误地只使用其中一部分变量进行回归时，会发生什么。通过精确推导遗漏变量所导致的偏差 [@problem_id:1948135]，我们可以深刻理解模型设定的重要性，并学会诊断和解释为何我们的估计结果可能与真实情况相去甚远。", "problem": "一位经济学家正在研究决定小时工资的因素。对于一个大小为 $n$ 的样本中的个体 $i$ ，其真实的基本关系被认为是一个多元线性回归模型：\n$$Y_i = \\beta_0 + \\beta_1 x_i + \\beta_2 z_i + \\epsilon_i$$\n其中，$Y_i$ 是小时工资，$x_i$ 是受教育年限，$z_i$ 是工作经验年限。参数 $\\beta_0$、$\\beta_1$ 和 $\\beta_2$ 是未知的常数。误差项 $\\epsilon_i$ 被假定为独立同分布，且期望值为零，即 $E[\\epsilon_i] = 0$。$x_i$ 和 $z_i$ 都被视为非随机变量。\n\n不幸的是，这位经济学家丢失了工作经验的数据 $z_i$。他转而仅使用受教育年限的数据，估计了一个设定有误的简单线性回归模型：\n$$Y_i = \\alpha_0 + \\alpha_1 x_i + \\nu_i$$\n这位经济学家使用普通最小二乘法（OLS）来获得斜率参数 $\\alpha_1$ 的一个估计量，记作 $\\hat{\\alpha}_1$。\n\n推导这个估计量的期望值 $E[\\hat{\\alpha}_1]$。用真实模型参数（$\\beta_1$，$\\beta_2$）以及涉及样本数据（$x_i$，$z_i$）及其各自样本均值 $\\bar{x} = \\frac{1}{n} \\sum_{i=1}^n x_i$ 和 $\\bar{z} = \\frac{1}{n} \\sum_{i=1}^n z_i$ 的总和来表示你的答案。", "solution": "从 $Y_{i}$ 对 $x_{i}$ 的错误设定的简单回归中，OLS斜率估计量为\n$$\n\\hat{\\alpha}_{1}=\\frac{\\sum_{i=1}^{n}(x_{i}-\\bar{x})(Y_{i}-\\bar{Y})}{\\sum_{i=1}^{n}(x_{i}-\\bar{x})^{2}}.\n$$\n在真实模型 $Y_{i}=\\beta_{0}+\\beta_{1}x_{i}+\\beta_{2}z_{i}+\\epsilon_{i}$ 下，其中 $E[\\epsilon_{i}]=0$ 且回归变量为非随机的，$Y_{i}$ 的样本均值为\n$$\n\\bar{Y}=\\beta_{0}+\\beta_{1}\\bar{x}+\\beta_{2}\\bar{z}+\\bar{\\epsilon},\\quad \\text{其中 } \\bar{\\epsilon}=\\frac{1}{n}\\sum_{i=1}^{n}\\epsilon_{i}.\n$$\n因此\n$$\nY_{i}-\\bar{Y}=\\beta_{1}(x_{i}-\\bar{x})+\\beta_{2}(z_{i}-\\bar{z})+(\\epsilon_{i}-\\bar{\\epsilon}).\n$$\n代入分子，\n$$\n\\sum_{i=1}^{n}(x_{i}-\\bar{x})(Y_{i}-\\bar{Y})=\\beta_{1}\\sum_{i=1}^{n}(x_{i}-\\bar{x})^{2}+\\beta_{2}\\sum_{i=1}^{n}(x_{i}-\\bar{x})(z_{i}-\\bar{z})+\\sum_{i=1}^{n}(x_{i}-\\bar{x})(\\epsilon_{i}-\\bar{\\epsilon}).\n$$\n取期望并利用 $E[\\epsilon_{i}]=0$ 可得 $E[\\bar{\\epsilon}]=0$，因此\n$$\nE\\!\\left[\\sum_{i=1}^{n}(x_{i}-\\bar{x})(\\epsilon_{i}-\\bar{\\epsilon})\\right]=0.\n$$\n由于 $x_{i}$ 和 $z_{i}$ 是非随机的，分母 $\\sum_{i=1}^{n}(x_{i}-\\bar{x})^{2}$ 是一个固定值，所以\n$$\nE[\\hat{\\alpha}_{1}]=\\frac{\\beta_{1}\\sum_{i=1}^{n}(x_{i}-\\bar{x})^{2}+\\beta_{2}\\sum_{i=1}^{n}(x_{i}-\\bar{x})(z_{i}-\\bar{z})}{\\sum_{i=1}^{n}(x_{i}-\\bar{x})^{2}}=\\beta_{1}+\\beta_{2}\\frac{\\sum_{i=1}^{n}(x_{i}-\\bar{x})(z_{i}-\\bar{z})}{\\sum_{i=1}^{n}(x_{i}-\\bar{x})^{2}}.\n$$\n这表明遗漏变量偏误项是 $\\beta_{2}\\,\\frac{\\sum_{i=1}^{n}(x_{i}-\\bar{x})(z_{i}-\\bar{z})}{\\sum_{i=1}^{n}(x_{i}-\\bar{x})^{2}}$，该项当且仅当 $\\sum_{i=1}^{n}(x_{i}-\\bar{x})(z_{i}-\\bar{z})=0$ 时为零。", "answer": "$$\\boxed{\\beta_{1}+\\beta_{2}\\,\\frac{\\sum_{i=1}^{n}(x_{i}-\\bar{x})(z_{i}-\\bar{z})}{\\sum_{i=1}^{n}(x_{i}-\\bar{x})^{2}}}$$", "id": "1948135"}, {"introduction": "模型参数估计的准确性固然重要，但评价一个模型的最终标准往往是其对新数据的预测能力。本练习将我们的关注点从估计量的样本内性质转移到其样本外（out-of-sample）的预测性能。我们将推导最小二乘估计器在面对新输入数据时的预期预测风险，并将其分解为偏差和方差两个部分。这个过程 [@problem_id:2897085] 不仅展示了最小二乘估计量的无偏性如何转化为预测无偏，更揭示了训练数据的设计、噪声水平以及未来数据的分布是如何共同决定模型的泛化能力的。", "problem": "一个具有未知有限冲激响应向量 $\\beta_{0} \\in \\mathbb{R}^{p}$ 的广义平稳离散时间线性时不变系统，通过 $n$ 次测量使用普通最小二乘法（OLS）进行辨识。训练数据建模为 $y = X \\beta_{0} + \\varepsilon$，其中 $X \\in \\mathbb{R}^{n \\times p}$ 是一个由输入序列构造的固定的满列秩设计矩阵，$\\varepsilon \\in \\mathbb{R}^{n}$ 是均值为零、协方差为 $\\sigma^{2} I_{n}$ 的噪声，$I_{n}$ 是 $n \\times n$ 的单位矩阵。OLS 估计量 $\\hat{\\beta}$ 定义为经验残差平方和的最小化器，并用于通过预测器 $x^{\\mathsf{T}} \\hat{\\beta}$ 预测新的输入特征向量 $x \\in \\mathbb{R}^{p}$ 的无噪声输出。假设在部署时，新的输入 $x$ 独立于训练噪声，并从一个均值为零、协方差为 $\\Sigma_{x} \\in \\mathbb{R}^{p \\times p}$ 的分布中抽取。\n\n仅使用给定的建模假设和第一性原理，推导无噪声目标 $x^{\\mathsf{T}} \\beta_{0}$ 的期望样本外平方预测风险，该风险定义为\n$$\nR_{\\mathrm{out}} \\triangleq \\mathbb{E}\\!\\left[\\left(x^{\\mathsf{T}} \\hat{\\beta} - x^{\\mathsf{T}} \\beta_{0}\\right)^{2} \\,\\middle|\\, X \\right],\n$$\n其中期望是关于训练噪声和新输入 $x$ 的分布计算的。将 $R_{\\mathrm{out}}$ 用 $X$、$\\Sigma_{x}$ 和 $\\sigma^{2}$ 明确表示，并将其分解为相对于 $\\hat{\\beta}$ 分布的平方偏差和方差贡献。以解析表达式的形式给出最终答案；不需要数值近似。", "solution": "最小化残差平方和 $\\|y - X\\beta\\|_{2}^{2}$ 的 OLS 估计量 $\\hat{\\beta}$ 由正规方程给出，得到：\n$$\n\\hat{\\beta} = (X^{\\mathsf{T}}X)^{-1}X^{\\mathsf{T}}y\n$$\n由于 $X$ 是满列秩的，因此 $(X^{\\mathsf{T}}X)^{-1}$ 的存在性得到保证。我们将模型 $y = X \\beta_{0} + \\varepsilon$ 代入此表达式：\n$$\n\\hat{\\beta} = (X^{\\mathsf{T}}X)^{-1}X^{\\mathsf{T}}(X \\beta_{0} + \\varepsilon) = (X^{\\mathsf{T}}X)^{-1}X^{\\mathsf{T}}X \\beta_{0} + (X^{\\mathsf{T}}X)^{-1}X^{\\mathsf{T}}\\varepsilon = \\beta_{0} + (X^{\\mathsf{T}}X)^{-1}X^{\\mathsf{T}}\\varepsilon\n$$\n这个方程用真实参数向量 $\\beta_{0}$ 和噪声 $\\varepsilon$ 来表示估计量 $\\hat{\\beta}$。\n\n风险 $R_{\\mathrm{out}}$ 定义为平方预测误差 $(x^{\\mathsf{T}} \\hat{\\beta} - x^{\\mathsf{T}} \\beta_{0})^{2}$ 在训练噪声 $\\varepsilon$ 和新输入 $x$ 的分布上的期望。由于 $x$ 和 $\\varepsilon$ 是独立的，且期望以 $X$ 为条件，我们可以写出：\n$$\nR_{\\mathrm{out}} = \\mathbb{E}_{x, \\varepsilon} \\left[ \\left(x^{\\mathsf{T}} (\\hat{\\beta} - \\beta_{0})\\right)^{2} \\,\\middle|\\, X \\right] = \\mathbb{E}_{x} \\left[ \\mathbb{E}_{\\varepsilon} \\left[ \\left(x^{\\mathsf{T}} \\hat{\\beta} - x^{\\mathsf{T}} \\beta_{0}\\right)^{2} \\,\\middle|\\, X, x \\right] \\right]\n$$\n内部期望是对于一个固定的新输入 $x$ 的预测器 $x^{\\mathsf{T}}\\hat{\\beta}$ 的条件均方误差（MSE）。我们可以将此条件 MSE 分解为相对于 $\\varepsilon$ 分布的平方偏差和方差分量：\n$$\n\\mathbb{E}_{\\varepsilon} \\left[ \\left(x^{\\mathsf{T}} \\hat{\\beta} - x^{\\mathsf{T}} \\beta_{0}\\right)^{2} \\,\\middle|\\, X, x \\right] = \\left( \\mathbb{E}_{\\varepsilon} \\left[ x^{\\mathsf{T}} \\hat{\\beta} \\,\\middle|\\, X, x \\right] - x^{\\mathsf{T}} \\beta_{0} \\right)^{2} + \\mathrm{Var}_{\\varepsilon} \\left( x^{\\mathsf{T}} \\hat{\\beta} \\,\\middle|\\, X, x \\right)\n$$\n我们来分析每一项。\n\n首先是偏差项。预测器 $x^{\\mathsf{T}}\\hat{\\beta}$ 关于 $\\varepsilon$ 的期望是：\n$$\n\\mathbb{E}_{\\varepsilon} \\left[ x^{\\mathsf{T}} \\hat{\\beta} \\,\\middle|\\, X, x \\right] = x^{\\mathsf{T}} \\mathbb{E}_{\\varepsilon} \\left[ \\hat{\\beta} \\,\\middle|\\, X \\right]\n$$\n我们求估计量 $\\hat{\\beta}$ 的期望：\n$$\n\\mathbb{E}_{\\varepsilon} \\left[ \\hat{\\beta} \\,\\middle|\\, X \\right] = \\mathbb{E}_{\\varepsilon} \\left[ \\beta_{0} + (X^{\\mathsf{T}}X)^{-1}X^{\\mathsf{T}}\\varepsilon \\,\\middle|\\, X \\right] = \\beta_{0} + (X^{\\mathsf{T}}X)^{-1}X^{\\mathsf{T}}\\mathbb{E}_{\\varepsilon}[\\varepsilon]\n$$\n由于 $\\mathbb{E}[\\varepsilon] = 0$，我们有 $\\mathbb{E}_{\\varepsilon} \\left[ \\hat{\\beta} \\,\\middle|\\, X \\right] = \\beta_{0}$。OLS 估计量 $\\hat{\\beta}$ 对于 $\\beta_{0}$ 是无偏的。因此，对于任何给定的 $x$，预测器 $x^{\\mathsf{T}}\\hat{\\beta}$ 对于 $x^{\\mathsf{T}}\\beta_{0}$ 都是无偏的：\n$$\n\\mathbb{E}_{\\varepsilon} \\left[ x^{\\mathsf{T}} \\hat{\\beta} \\,\\middle|\\, X, x \\right] = x^{\\mathsf{T}}\\beta_{0}\n$$\n因此，对于固定的 $x$，平方偏差贡献为零：\n$$\n\\left( \\mathbb{E}_{\\varepsilon} \\left[ x^{\\mathsf{T}} \\hat{\\beta} \\,\\middle|\\, X, x \\right] - x^{\\mathsf{T}} \\beta_{0} \\right)^{2} = (x^{\\mathsf{T}}\\beta_{0} - x^{\\mathsf{T}}\\beta_{0})^{2} = 0\n$$\n$R_{\\mathrm{out}}$ 的平均平方偏差贡献是该项在 $x$ 上的期望，即 $\\mathbb{E}_{x}[0]=0$。\n\n接下来是方差项。对于固定的 $x$，预测器 $x^{\\mathsf{T}}\\hat{\\beta}$ 关于 $\\varepsilon$ 的方差是：\n$$\n\\mathrm{Var}_{\\varepsilon} \\left( x^{\\mathsf{T}} \\hat{\\beta} \\,\\middle|\\, X, x \\right) = \\mathrm{Var}_{\\varepsilon} \\left( x^{\\mathsf{T}}\\beta_{0} + x^{\\mathsf{T}}(X^{\\mathsf{T}}X)^{-1}X^{\\mathsf{T}}\\varepsilon \\,\\middle|\\, X, x \\right)\n$$\n由于 $x^{\\mathsf{T}}\\beta_{0}$ 是关于 $\\varepsilon$ 的常数，这可以简化为：\n$$\n\\mathrm{Var}_{\\varepsilon} \\left( x^{\\mathsf{T}}(X^{\\mathsf{T}}X)^{-1}X^{\\mathsf{T}}\\varepsilon \\,\\middle|\\, X, x \\right)\n$$\n令行向量 $c^{\\mathsf{T}} = x^{\\mathsf{T}}(X^{\\mathsf{T}}X)^{-1}X^{\\mathsf{T}}$。使用性质 $\\mathrm{Var}(c^{\\mathsf{T}}\\varepsilon) = c^{\\mathsf{T}}\\mathrm{Cov}(\\varepsilon)c$，并给定 $\\mathrm{Cov}(\\varepsilon) = \\sigma^{2}I_{n}$，我们得到：\n$$\n\\mathrm{Var}_{\\varepsilon} (c^{\\mathsf{T}}\\varepsilon) = c^{\\mathsf{T}}(\\sigma^{2}I_{n})c = \\sigma^{2}c^{\\mathsf{T}}c\n$$\n将 $c^{\\mathsf{T}}$ 代回：\n$$\nc^{\\mathsf{T}}c = \\left(x^{\\mathsf{T}}(X^{\\mathsf{T}}X)^{-1}X^{\\mathsf{T}}\\right) \\left(X(X^{\\mathsf{T}}X)^{-1}x\\right) = x^{\\mathsf{T}}(X^{\\mathsf{T}}X)^{-1}(X^{\\mathsf{T}}X)(X^{\\mathsf{T}}X)^{-1}x = x^{\\mathsf{T}}(X^{\\mathsf{T}}X)^{-1}x\n$$\n因此，对于固定的 $x$，方差贡献为 $\\sigma^{2} x^{\\mathsf{T}}(X^{\\mathsf{T}}X)^{-1}x$。\n\n为了求得对 $R_{\\mathrm{out}}$ 的总方差贡献，我们必须将此量在 $x$ 的分布上取平均：\n$$\n\\text{方差贡献} = \\mathbb{E}_{x} \\left[ \\sigma^{2} x^{\\mathsf{T}}(X^{\\mathsf{T}}X)^{-1}x \\right] = \\sigma^{2} \\mathbb{E}_{x} \\left[ x^{\\mathsf{T}}(X^{\\mathsf{T}}X)^{-1}x \\right]\n$$\n期望内的项是随机向量 $x$ 的二次型。对于均值为 $\\mu_{x}$、协方差为 $\\Sigma_{x}$ 的随机向量 $x$ 和一个常数矩阵 $M$，二次型 $x^{\\mathsf{T}}Mx$ 的期望由 $\\mathbb{E}[x^{\\mathsf{T}}Mx] = \\mathrm{Tr}(M\\Sigma_{x}) + \\mu_{x}^{\\mathsf{T}}M\\mu_{x}$ 给出。在我们的情况下，$M=(X^{\\mathsf{T}}X)^{-1}$，$\\mu_{x}=0$，协方差为 $\\Sigma_{x}$。因此：\n$$\n\\mathbb{E}_{x} \\left[ x^{\\mathsf{T}}(X^{\\mathsf{T}}X)^{-1}x \\right] = \\mathrm{Tr}\\left((X^{\\mathsf{T}}X)^{-1}\\Sigma_{x}\\right) + 0^{\\mathsf{T}}(X^{\\mathsf{T}}X)^{-1}0 = \\mathrm{Tr}\\left((X^{\\mathsf{T}}X)^{-1}\\Sigma_{x}\\right)\n$$\n对风险的最终方差贡献是 $\\sigma^{2} \\mathrm{Tr}\\left((X^{\\mathsf{T}}X)^{-1}\\Sigma_{x}\\right)$。\n\n总而言之，总风险 $R_{\\mathrm{out}}$ 是平方偏差贡献和方差贡献之和：\n$$\nR_{\\mathrm{out}} = \\text{平方偏差贡献} + \\text{方差贡献} = 0 + \\sigma^{2} \\mathrm{Tr}\\left((X^{\\mathsf{T}}X)^{-1}\\Sigma_{x}\\right)\n$$\n问题要求风险 $R_{\\mathrm{out}}$ 及其分解为平方偏差和方差分量。平方偏差贡献为 $0$，方差贡献为 $\\sigma^{2} \\mathrm{Tr}\\left((X^{\\mathsf{T}}X)^{-1}\\Sigma_{x}\\right)$。", "answer": "$$\\boxed{\\sigma^{2} \\mathrm{Tr}\\left((X^{\\mathsf{T}}X)^{-1}\\Sigma_{x}\\right)}$$", "id": "2897085"}]}