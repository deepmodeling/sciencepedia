## Introduction
Receding Horizon Control (RHC), or Model Predictive Control (MPC), represents a powerful strategy for managing complex, constrained systems, from self-driving cars to chemical plants. Its core idea—planning a short distance into the future and constantly updating that plan based on new information—is both intuitive and remarkably effective. However, this constant re-planning introduces a critical challenge: how can we be certain that a series of locally optimal decisions will lead to globally stable behavior? Without a formal guarantee, the controller might inadvertently steer the system into an irrecoverable state, where constraints are violated or the target becomes unreachable. This article delves into the theoretical architecture designed to solve this very problem. First, in "Principles and Mechanisms," we will unpack the fundamental concepts of RHC stability, exploring how tools like Lyapunov functions, terminal sets, and terminal costs provide an ironclad promise of convergence. Then, in "Applications and Interdisciplinary Connections," we will see how this robust framework is adapted to tackle real-world complexities, including [model uncertainty](@article_id:265045), external disturbances, and even [economic optimization](@article_id:137765) goals.

## Principles and Mechanisms

Imagine you are driving a car along a winding road you’ve never seen before. How do you do it? You don't map out every single turn of the steering wheel from start to finish. Instead, you look ahead a short distance—say, to the next curve—and formulate a plan. You think, "I'll ease off the gas, turn the wheel slightly to the left, and aim for the apex of the turn." You then begin to execute that plan. But here's the crucial part: you only commit to the very first action. As you start turning the wheel, your eyes are already looking further down the road, taking in new information. Based on your new position and the new view, you discard your old plan and instantly form a new one. This constant cycle of looking ahead, planning, acting, and re-planning is the very essence of Receding Horizon Control (RHC).

### The Art of Constant Re-planning

At its heart, RHC, or Model Predictive Control (MPC), operates on this simple yet profound principle. At each moment in time, the controller uses a mathematical **model** of the system it's trying to control—be it a chemical reactor, a power grid, or a self-driving car—to simulate and optimize its actions over a limited future timeframe, known as the **[prediction horizon](@article_id:260979)**. It solves for the best possible sequence of control inputs to minimize some cost (like minimizing fuel consumption while staying on the road). This process generates an optimal predicted path for the system over the horizon.

However, and this is the key insight, the controller is not a blind follower of its own plan. It knows the world is an uncertain place. The model is never perfect, and unexpected disturbances—a gust of wind, a sudden change in road friction—are inevitable. So, the controller applies only the *first* input from its carefully calculated optimal sequence. At the next moment, it measures the system's *actual* new state and repeats the entire process from scratch [@problem_id:1603982]. The rest of the previously computed plan is simply thrown away.

This "[receding horizon](@article_id:180931)" strategy transforms what seems like a simple open-loop planning exercise into a powerful closed-loop **feedback** mechanism. The controller is constantly receiving feedback in the form of state measurements. By re-optimizing from its actual current state at every step, it implicitly corrects for any deviations from its predicted path that were caused by model inaccuracies or external disturbances [@problem_id:2736385]. An open-loop controller, by contrast, is like a musician playing from a sheet of music without listening to the orchestra; if the tempo shifts, it gets left behind. The RHC controller is a jazz improviser, constantly listening and adapting to what's happening around it.

### The Peril of Myopia: A Guarantee for the Future

This constant adaptation sounds wonderfully robust, but it raises a critical question: how do we know this process is stable? What guarantees that our constant re-planning doesn't inadvertently lead us into a dead end?

Imagine our driver, in an effort to take a corner very tightly, steers the car into a position so close to the guardrail that there's no way to avoid scraping it on the next turn. The plan at that moment might have been "optimal" over the short horizon, but it led to a future state from which no good plan could be made. In control theory, this is the problem of **[recursive feasibility](@article_id:166675)**. Just because we can find a feasible plan (one that satisfies all constraints) from our current state does not automatically mean we will be able to find one from the state we move to next.

This distinction gives rise to two important concepts. The first is the set of all initial states for which at least one feasible plan can be found *initially*. The second, and more important, set is the **Region of Attraction (ROA)**. The ROA is the set of states from which the RHC controller is guaranteed not only to find a feasible plan initially, but to be able to do so at *every future step*, all while driving the system toward its goal [@problem_id:1583563]. For a controller to be considered stable, we must ensure that once we start in the ROA, the system never leaves it. The challenge, then, is to design the controller's optimization problem in such a way that this guarantee holds.

### The Lyapunov Handshake: A Promise of Progress

To prove that a system will eventually settle at its target (for example, the origin), mathematicians and engineers often turn to a powerful tool invented by Aleksandr Lyapunov. The idea is to find a function, a **Lyapunov function**, which acts like an "energy" or "altitude" measurement for the system. This function must be positive everywhere except at the target, where it is zero. If we can prove that every action our controller takes causes this "energy" to decrease, then the system must inevitably slide "downhill" and come to rest at the target.

The genius of modern RHC design is to use the controller's own optimal [cost function](@article_id:138187) as a candidate Lyapunov function. After all, the cost is designed to be zero at the target and positive everywhere else. The trick is to ensure that the optimal cost at the *next* time step, $J^*(x_{k+1})$, is always less than the optimal cost at the *current* step, $J^*(x_k)$.

How can this be guaranteed? Let's start with a simple, if somewhat restrictive, idea. What if we force every plan to end precisely at the target? We add a **terminal equality constraint**, forcing the predicted state at the end of the horizon to be zero: $x_{N|k} = 0$.

Let's see why this works. At time $k$, we compute an optimal plan $\{u_{0|k}^*, u_{1|k}^*, \dots, u_{N-1|k}^*\}$ that brings the state to zero at step $N$. We apply the first input, $u_k = u_{0|k}^*$, and the system moves to state $x_{k+1}$. Now, at time $k+1$, we need to prove that the new optimal cost will be lower. To do this, we only need to find *one* feasible plan whose cost is lower. Consider this candidate plan for the new optimization: take the tail of our old plan, $\{u_{1|k}^*, \dots, u_{N-1|k}^*\}$, and append a zero control at the end, $u_{N-1} = 0$. This plan is guaranteed to be feasible and also ends at the origin. The cost of this candidate plan is exactly the old optimal cost minus the cost of the first step we just took. Since the cost of each step is positive, the candidate plan's cost is strictly less than the previous optimal cost. The *new* optimal cost can only be even lower. Thus, the [cost function](@article_id:138187) decreases at every step, acting as a Lyapunov function and proving stability [@problem_id:1579689].

### Designing the "Safe Zone": Invariance and the Terminal Controller

The requirement to reach the origin in exactly $N$ steps is very strict and can severely limit the size of our Region of Attraction. It's like asking a spacecraft to land on a single atom. A much more practical approach is to define a small "safe zone" around the origin, called the **[terminal set](@article_id:163398)**, denoted $\mathcal{X}_f$. Instead of demanding the plan end at the origin, we only demand it end somewhere inside this set: $x_{N|k} \in \mathcal{X}_f$ [@problem_id:2741130].

What makes this set "safe"? It must possess two crucial properties, which together constitute a kind of handshake between the complex RHC planner and a simple, reliable backup controller.

1.  **A Simple Local Guide (The Terminal Controller):** Within this safe zone $\mathcal{X}_f$, we must have a simple, pre-defined feedback controller, like $u=Kx$, that we know is stable and respects all constraints. Think of this as the autopilot for the final phase of landing.

2.  **Positive Invariance:** The safe zone must be **positively invariant** under this local controller. This is a fancy way of saying that once you're inside the zone, the local controller will never steer you out [@problem_id:2884349]. If you start in $\mathcal{X}_f$, the next state $(A+BK)x$ will also be in $\mathcal{X}_f$. This property is the key to guaranteeing [recursive feasibility](@article_id:166675).

With this setup, the RHC's job is to do the "hard part": navigate from far away and deliver the state to the doorstep of this safe zone. The stability proof now works by showing that the plan can be continued by "stitching" the tail of the old MPC plan to the simple, provably stable actions of the terminal controller. To ensure the Lyapunov function (the cost) decreases, we add a **terminal cost**, $V_f(x_N)$, to our optimization. This terminal cost isn't arbitrary; it must be a local **Control Lyapunov Function (CLF)** for the terminal controller inside the safe zone. This means that the value of $V_f$ is guaranteed to decrease if we apply the simple controller $u=Kx$ [@problem_id:2746605] [@problem_id:2724726]. The RHC planner, by minimizing the total cost, is thus incentivized to find a path that hands off the state to the terminal region in a way that guarantees a steady descent in the system's overall "energy."

### The Infinite Wisdom in a Finite Plan

This raises one last, beautiful question: What is the perfect choice for this terminal cost $V_f(x)$? Is there a "best" one? The answer is profound. The ideal terminal cost is the value of the optimal, *infinite-horizon* cost-to-go from that terminal state.

In the world of unconstrained linear systems, the gold standard for [optimal control](@article_id:137985) is the Linear Quadratic Regulator (LQR). The LQR controller provides the [optimal policy](@article_id:138001) for minimizing a quadratic cost over an infinite future. The cost associated with this [optimal policy](@article_id:138001) from any state $x$ can be calculated as $x^{\top}Px$, where $P$ is a special matrix found by solving the **Algebraic Riccati Equation**.

So, the most elegant choice for our MPC terminal cost is precisely this LQR cost: $V_f(x_N) = x_N^{\top}Px_N$. What this means is that our finite-horizon controller is optimizing a cost that is the sum of the costs over the explicit horizon ($N$ steps) plus a perfect estimate of the cost for the rest of eternity, assuming we switch to the best possible simple controller thereafter [@problem_id:2736392]. In this way, the "myopic" finite-horizon planner is endowed with the wisdom of an infinite-horizon one, beautifully unifying two cornerstones of control theory.

### Sufficient, Not Necessary: When the Rules Can Be Bent

After building up this elaborate machinery of terminal sets and costs, it is important to take a step back and ask: is all this *always* necessary? The answer is no. These terminal ingredients are a set of *sufficient* conditions to guarantee stability, but they are not always *necessary*.

Consider a simple system that is already naturally stable, like a pendulum with friction. If left alone, it will eventually come to rest at the bottom. If we task an RHC controller with a very simple cost (e.g., just penalizing control effort) and a short horizon of $N=1$, what will it do? The optimizer will quickly realize that the best action is to do nothing ($u_k=0$) and let the system's natural dynamics bring it to the origin. The closed-loop system will be stable without any terminal constraints or costs whatsoever [@problem_id:2884369].

This shows that stability in RHC is an emergent property of the interplay between the system's own dynamics, the [cost function](@article_id:138187), and the length of the [prediction horizon](@article_id:260979). For systems that are unstable or have complex constraints, a sufficiently long horizon can sometimes be enough to ensure stability on its own. The terminal ingredients are a powerful and systematic engineering tool to *enforce* stability for a given, fixed horizon, providing a robust guarantee where one might not otherwise exist. They are the rigorous handshake that ensures our controller's brilliant but myopic plans always lead to a stable and predictable future.