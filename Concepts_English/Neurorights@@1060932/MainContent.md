## Introduction
For centuries, the human mind has been a private sanctuary, its inner workings inaccessible to the outside world. This natural barrier is now being challenged by rapid advances in neurotechnology, which can decode thoughts and modulate neural activity directly. Existing privacy laws, designed to protect data after it has been voluntarily shared, are ill-equipped to handle this unprecedented form of access into our internal mental world. This creates a profound ethical and legal gap, demanding a new set of protections known as neurorights. This article provides a comprehensive exploration of this emerging field.

First, in the "Principles and Mechanisms" section, we will establish the foundational concepts of neurorights. We will define the four core pillars—mental privacy, mental integrity, cognitive liberty, and personal identity—and explain why neural data requires a unique and higher level of protection than other forms of information. Subsequently, the "Applications and Interdisciplinary Connections" section will ground these principles in the real world. We will examine the complex ethical dilemmas neurotechnology creates in medicine, the risks of coercion and surveillance in commerce and the workplace, and the monumental stakes involved when this power is wielded by the state, demonstrating the urgent need for a robust, rights-based governance framework.

## Principles and Mechanisms

For all of human history, one fortress has remained inviolable: the private world of your own mind. You could be compelled to speak or act, but your unexpressed thoughts, your silent deliberations, your fleeting feelings—these were yours and yours alone. This wasn’t a right granted by law; it was a simple fact of nature, a physical reality. There was a fundamental wall between your internal mental world and the external world everyone else could observe.

Let's think about this like a physicist. Imagine your unarticulated mental contents—your thoughts, feelings, and intentions—as a set of states, let’s call it $M$. To share these with the world, you perform a voluntary action, an externalization, which we can call $f$. This action, $f$, transforms a mental state from $M$ into an external expression, $E$—speech, writing, a gesture. You are the gatekeeper. You decide if and when $f$ happens. This natural barrier creates what philosophers call an **epistemic asymmetry**: you have privileged, first-person access to $M$, while the rest of the world does not [@problem_id:4873823].

Neurotechnology, for the first time, threatens to build a new gate in that wall—a gate to which someone else might hold the key. These technologies can create a new kind of process, let's call it $g$, which can directly access the brain's activity and produce an estimate, $\hat{E}$, of your mental state, without your voluntary action $f$. This is the crux of the matter. Existing privacy laws, like the GDPR, were designed to govern the data once it's already externalized, to regulate what happens to the set $E$. But they have little to say about the act of compelled or covert access into the set $M$ itself. This is a new kind of intrusion, and it demands a new kind of protection: **neurorights**.

### The Pillars of Neurorights

To navigate this new territory, ethicists and scientists are charting a map built on several core principles. These aren't entirely new ideas; instead, they are profound extensions of our most cherished human rights, adapted for an age where the mind itself is becoming a domain of technology [@problem_id:5016442].

#### Mental Privacy: The Right to Keep Your Thoughts Your Own

The most intuitive neuroright is **mental privacy**. This is not just about keeping your brain-scan files confidential; it's the right to prevent the non-consensual decoding of your mental states from your neural signals [@problem_id:4409554]. Imagine a wearable device that monitors your brain activity to help with depression. An AI model, running on this device, infers the likelihood that you are having intrusive thoughts or ruminations. The raw brain signals might be deleted instantly, and no data might ever be stored with your name on it. From a traditional data-privacy perspective, this might seem fine. But your mental state has still been "read" without your specific, ongoing consent. The violation is the inference itself.

This is why neural data is fundamentally different from other biometrics. Consider a criminal investigation where the state wants your fingerprints and your DNA. They also want to use an EEG cap to measure your brain's P300 wave response to crime-scene photos, a technique meant to reveal recognition [@problem_id:4873758]. A fingerprint is a physical identifier; its value is in its pattern. But the P300 wave is valuable for its *semantic content*—it’s interpreted as a proxy for the thought, "I recognize that." Forcing you to undergo such a test is less like fingerprinting and more like compelling your mind to testify against itself. It bypasses your mouth and goes straight to your brain for a statement. This makes neural data uniquely powerful and uniquely dangerous, placing it in a category closer to communication than to mere physical characteristics.

#### Mental Integrity: The Right Not to Be Changed

If mental privacy is about "reading" the mind, **mental integrity** is about "writing" to it. It is the right to be protected from unauthorized and harmful alterations to your neural activity that could change who you are [@problem_id:4409554].

Consider the extraordinary case of a patient with treatment-resistant depression who receives a Deep Brain Stimulation (DBS) implant. The device works perfectly, alleviating the depression. The physical brain shows no new damage beyond the routine electrode tract. By the standards of **bodily integrity**, the intervention is a success. Yet, the patient reports a disturbing change: a flattening of empathy, a loss of spontaneous motivation, and a profound sense that their preferences and sense of self feel "externally steered" [@problem_id:5016437].

Here we see a stunning separation: the physical organ is unharmed, but the person’s authentic sense of self is compromised. The harm is not a lesion, but an externally induced shift in personality and agency. This is a violation of mental integrity. This right protects the coherence and authenticity of your mind, ensuring that the person you are cannot be manipulated or damaged from the inside out, even for seemingly benevolent reasons.

#### Cognitive Liberty: The Right to Think for Yourself

**Cognitive liberty**, often intertwined with the historic **freedom of thought**, is a dual-sided right. On one side, it is a shield; on the other, it is a key.

As a shield, it is the freedom *from* coercive interference in your thoughts. This builds directly on mental privacy and integrity. Imagine your neural data, collected perhaps during a routine clinical evaluation, is later analyzed by a third party using machine learning. This analysis could infer your hidden preferences, your political leanings, or your religious beliefs—things you never chose to express [@problem_id:4873764]. The mere knowledge that this is possible could create a powerful **chilling effect**, discouraging you from entertaining unconventional ideas even in the privacy of your own mind. This is the ultimate surveillance, an intrusion into the “inner forum” (*forum internum*) where our true selves are forged.

But cognitive liberty is also a key. It is the freedom *to* control your own mental processes, which includes the right to *use* neurotechnologies for self-modulation. Suppose a software engineer wants to use the prescription drug modafinil or a non-invasive tDCS headset to enhance their focus [@problem_id:4877339]. This appeal to self-improvement is an expression of cognitive liberty. However, this right is not absolute. A clinician, bound by the duty to "do no harm," can justifiably refuse to prescribe a risky substance for a non-medical purpose. Likewise, a workplace that offers "voluntary" access to tDCS in exchange for a bonus creates a coercive environment that undermines true autonomy. Cognitive liberty, therefore, is a delicate balance: the right to self-determination over one's mind, constrained by duties of care and the prevention of coercion.

#### Personal Identity: The Right to Be Yourself

Closely linked to mental integrity, the right to **personal identity** protects the continuity of your sense of self. It asserts that the core of who you are should not be altered without your consent. This right is challenged by the patient who feels "externally steered" by their brain implant [@problem_id:5016437].

This principle takes on a startling new dimension when we consider the idea of selling our neural data. Imagine a program where you can license your brain recordings to companies for a fee [@problem_id:4873832]. On the surface, this might look like a simple exercise of autonomy—monetizing your personal data. But what if that data is so rich that it encodes your personality, your intentions, your very way of thinking? In selling this data, are you not commodifying a part of your self? This leads to the profound argument that certain classes of neural data, those that are constitutive of personhood, should be **inalienable**—they cannot be sold or transferred, even with consent. Just as we cannot sell ourselves into slavery, perhaps we should not be able to sell the data that constitutes our identity, because doing so could fundamentally undermine the very autonomy that makes consent meaningful.

### From Principles to Practice

Defining these rights is one thing; applying them in the messy reality of clinics, workplaces, and legal systems is another challenge entirely.

One of the thorniest issues is consent. In a multinational clinical trial for a new BCI, what does informed consent look like? In a culture where families make decisions collectively, does the family’s approval suffice? The ethical consensus is clear: while the *process* of consent must be culturally responsive—for instance, by involving family members in discussions—the final decision must rest with the individual. The principle of individual autonomy is paramount [@problem_id:4873536]. This requires sophisticated strategies like layered, dynamic consent, where participants can make granular choices about how their uniquely sensitive neural data is used over time.

So, how do we build the guardrails? Do we need a categorical ban on all new neurotechnology? Critics rightly worry that overbroad rules could chill legitimate and beneficial research [@problem_id:5016410]. A more sensible path is a **tiered, risk-calibrated framework**. We must amend existing laws to be neuro-specific, but apply rules proportionally. Interventions that are invasive or that decode the content of thought should face the highest level of scrutiny. In contrast, non-invasive devices that only measure aggregate signals might be subject to lighter regulation. The key is to require strict opt-in consent for sensitive decoding, impose outright bans on coercive uses by states and employers, and mandate transparency and independent oversight.

Finally, we must confront the question of **equal access** [@problem_id:5016442]. If neurotechnologies can restore lost function or even enhance cognition, who will have access to them? This is a question of justice. We face a future that could be fractured not just by wealth, but by biology itself—a world of the neuro-enhanced and the unenhanced. Ensuring equitable access to these transformative technologies is not just a policy choice; it is a fundamental challenge to our commitment to human equality. The quest for neurorights, then, is not merely about protecting ourselves from a dystopian future of mind-reading. It is about proactively defining the kind of society we wish to be—one that respects the sanctity of the human mind and upholds the dignity of every person in the face of our own advancing ingenuity.