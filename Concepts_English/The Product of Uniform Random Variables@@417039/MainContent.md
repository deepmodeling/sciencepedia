## Introduction
What happens when you multiply two numbers chosen completely at random? This simple question, reminiscent of a game of chance, opens the door to a world of surprising mathematical elegance and profound scientific application. While our intuition might suggest that multiplying simple, uniformly random inputs should yield a similarly simple output, the reality is far more interesting. The resulting distribution is not flat at all but possesses a distinct and beautiful shape that appears in the most unexpected corners of science. This article demystifies the process of multiplying uniform [random variables](@article_id:142345), revealing the hidden patterns that govern the outcome.

The following chapters will guide you on a journey from first principles to real-world impact. In "Principles and Mechanisms," we will derive the exact [probability distribution](@article_id:145910) for the product, uncovering the elegant logarithmic function that defines it and exploring how this changes when the variables are no longer independent. Following that, "Applications and Interdisciplinary Connections" will demonstrate how this seemingly abstract concept becomes a powerful tool in fields as diverse as evidence-based medicine, [computational science](@article_id:150036), [theoretical physics](@article_id:153576), and even [number theory](@article_id:138310). Prepare to see how a simple multiplication unlocks a deep and unifying principle across the scientific landscape.

{'br': {'div': {'img': {'br': [{'i': 'The [probability density function](@article_id:140116) of the product of two independent U(0,1) [random variables](@article_id:142345).'}, 'Take a moment to appreciate this. We started with two of the simplest, "flattest" distributions imaginable—the [uniform distribution](@article_id:261240), a constant line. We performed the simple operation of multiplication. And out came this elegant, sweeping curve. It is not flat at all! The function $-\\ln(z)$ skyrockets to infinity as $z$ approaches 0.\n\nWhat does this mean? It means the product $Z=XY$ is heavily, overwhelmingly biased towards small values. It is far more likely that the product will be close to 0 than close to 1. An intuitive reason for this is now clear from our geometric picture. For the product $xy$ to be large (say, close to 1), *both* $x$ and $y$ must be close to 1. This corresponds to a tiny corner of our unit square. In contrast, for the product to be small, we only need *one* of the numbers to be small. If $x=0.01$, $y$ can be anything up to 1, and the product will not exceed 0.01. The region of the square corresponding to small products is vastly larger than the region corresponding to large products. The function $-\\ln(z)$ is the precise mathematical expression of this geometric fact. This is the kind of inherent beauty and unity that makes science so compelling.\n\n### Beyond Independence: The Web of Connection\n\nUp to this point, our entire discussion has been built on one crucial pillar: the independence of $X$ and $Y$. We assumed the choice of one number had absolutely no bearing on the choice of the other. But what if they are connected? What if they are two measurements from a system where one tends to be large when the other is large? Or where one is small when the other is large?\n\nThis is not just a theoretical curiosity; it\'s the reality of the world. Stock prices don\'t move independently; the [temperature](@article_id:145715) and humidity are related; the length and width of a manufactured part might be linked by the same machine process.\n\nTo handle this, mathematicians have developed a brilliant tool called a **[copula](@article_id:269054)**. You can think of a [copula](@article_id:269054) as a recipe for describing the [dependence structure](@article_id:260920) between [random variables](@article_id:142345), separate from their individual distributions (their "marginals"). It allows us to take our two uniform [random variables](@article_id:142345) and "glue" them together with a specific kind of statistical relationship.\n\nLet\'s consider a popular example, the Farlie-Gumbel-Morgenstern (FGM) [copula](@article_id:269054). It introduces a parameter, $\\alpha$, which can range from -1 to 1 and measures the association between our two variables, $U$ and $V$. When $\\alpha=0$, they are independent. When $\\alpha  0$, they tend to move together (positive association). When $\\alpha  0$, they tend to move in opposite directions (negative association).\n\nWhat happens to the distribution of the product $Z=UV$ when our variables are linked by this [copula](@article_id:269054)? We can repeat our calculation for the CDF, but this time using the more complex [joint distribution](@article_id:203896) defined by the [copula](@article_id:269054) [@problem_id:725309]. The [algebra](@article_id:155968) is more involved, but the result is wonderfully illuminating. The new CDF, $F_Z(z)$, turns out to be:\n\n$$F_Z(z) = \\underbrace{z - z\\ln z}_{\\text{Independent Part}} + \\underbrace{\\alpha \\cdot \\left( 3z^2-3z -z(1+2z)\\ln z \\right)}_{\\text{Dependence Correction}}$$\n\nLook at this structure! The distribution is our original result for the independent case, plus a "correction term" that is proportional to the association parameter $\\alpha$. If we set $\\alpha=0$ (restoring independence), the correction term vanishes, and we recover our familiar formula $z - z\\ln z$.\n\nThis is a profound insight into how scientific models are built. We start with a simple, idealized case (independence). We derive a beautiful, fundamental law ($-\\ln z$). Then, we find ways to add complexity (dependence) in a controlled manner, resulting in a new formula that clearly shows the original law nested within it. This tells us not only what the answer is, but *how* it deviates from the simpler case. It\'s a journey from simplicity to complexity and back again, revealing the deep, interconnected structure of the mathematical world that describes our own.'], 'applications': '## Applications and Interdisciplinary Connections\n\nAfter exploring the nuts and bolts of what happens when we multiply two random numbers drawn uniformly from a hat, one might be tempted to ask, "So what?" It’s a fair question. The distribution we derived in the previous chapter, with its characteristic logarithmic shape, might seem like a mathematical curiosity, a solution in search of a problem.\n\nBut the history of science is filled with such curiosities blossoming into powerful tools and profound insights. The product of uniform [random variables](@article_id:142345) is no exception. It is not a dusty artifact to be left on a [probability theory](@article_id:140665) shelf; rather, it is a key that unlocks doors in a surprising number of rooms in the vast mansion of science. In this chapter, we will embark on a journey to visit some of these rooms. We will see how this simple concept helps us aggregate scientific knowledge, test the very foundations of our computer simulations, classify the fundamental laws of nature, understand processes that evolve in time, and even probe the deepest structures of mathematics. Let us begin.\n\n### The Heart of Modern Science: Aggregating Evidence\n\nOne of the most pressing challenges in science today is reproducibility. A single study, especially one with a small sample size, might yield a result that is tantalizing but not statistically conclusive. For instance, a new drug might show a positive effect, but the [p-value](@article_id:136004) might be $0.1$, failing to cross the conventional threshold of significance like $0.05$. A second, independent study might find the same thing, also with a [p-value](@article_id:136004) of $0.1$. Individually, neither study is convincing. But surely, two independent studies both pointing in the same direction means *something*. How do we formally combine their evidence?\n\nThis is the domain of [meta-analysis](@article_id:263380), and the product of uniform [random variables](@article_id:142345) provides an elegant answer. Under the [null hypothesis](@article_id:264947) (the assumption that the drug has no effect), a validly computed [p-value](@article_id:136004) is a [random variable](@article_id:194836) uniformly distributed on $[0,1]$. So, if we have two independent studies with p-values $p_1$ and $p_2$, we can model them as two independent uniform [random variables](@article_id:142345), $U_1$ and $U_2$. A natural way to combine them is to look at their product, $T = p_1 p_2$. If the [null hypothesis](@article_id:264947) is true, the distribution of this product is precisely the one we have studied: its [cumulative distribution function](@article_id:142641) is $F(t) = P(U_1 U_2 \\le t) = t - t \\ln(t)$. For our two studies with $p_1=0.1$ and $p_2=0.1$, the product is $t=0.01$. The combined [p-value](@article_id:136004) is the [probability](@article_id:263106) of getting a product this small or smaller, which is $F(0.01) = 0.01 - 0.01 \\ln(0.01) \\approx 0.056$. While still not below $0.05$, this combined evidence is substantially stronger than either piece alone. This technique, a variant of Fisher\'s method, uses the distribution of the product of uniforms as its very foundation, turning it into a cornerstone of evidence-based medicine and scientific synthesis [@problem_id:2430501].\n\n### The Measure of Randomness\n\nWe have just seen how the properties of ideal randomness can be used to test scientific hypotheses. But this begs a question: how do we test our source of randomness itself? The vast majority of modern scientific computation, from simulating galaxies to folding [proteins](@article_id:264508) to testing statistical methods, relies on random number generators (RNGs). But these are deterministic algorithms designed to produce sequences that only *look* random. How can we be sure they are doing a good job?\n\nAgain, our distribution comes to the rescue. Imagine a clever test: we use our RNG to generate four random numbers, $A, B, C, D$, supposedly drawn independently and uniformly from $[0,1]$. We arrange them into a $2 \\times 2$ [matrix](@article_id:202118) and compute its [determinant](@article_id:142484), $T = AD - BC$. If the RNG is working as advertised, what should the distribution of these [determinants](@article_id:276099) look like? Let\'s look closely at the expression. $AD$ is the product of two independent uniform [random variables](@article_id:142345). So is $BC$. Therefore, the [determinant](@article_id:142484) $T$ is the difference of two independent, identically distributed [random variables](@article_id:142345) whose distribution we know. The resulting theoretical distribution for $T$ is a beautiful, symmetric bell-like shape centered at zero.\n\nNow we have a test! We can generate thousands of such random [determinants](@article_id:276099) using our RNG and compare their [empirical distribution](@article_id:266591) to the theoretical one we\'ve just described. If they don\'t match, our RNG is flawed—it is not producing truly independent uniform numbers. For example, if a "bad" generator accidentally created correlations such that $B$ was always equal to $A$ and $D$ was always equal to $C$, every single [determinant](@article_id:142484) would be $T = AC - AC = 0$. The [empirical distribution](@article_id:266591) would be a single spike at zero, which looks nothing like the theoretical curve. The abstract distribution for the product of uniforms thus becomes a powerful, practical diagnostic tool for validating the workhorses of [computational science](@article_id:150036) [@problem_id:2442633].\n\n### From Chance to the Laws of Nature\n\nIt\'s one thing to see our little product distribution appear in statistics and [computer science](@article_id:150299), which are intrinsically tied to data and randomness. But what could it possibly have to do with the majestic equations that describe the vibrations of a drum or the propagation of light?\n\nThe connection is as surprising as it is profound. Many phenomena in physics are described by second-order [partial differential equations](@article_id:142640) (PDEs). These equations are classified into types—hyperbolic, parabolic, and elliptic—that correspond to fundamentally different physical behaviors. Hyperbolic equations describe waves (like light or sound), [elliptic equations](@article_id:141122) describe steady-states (like a static [electric field](@article_id:193832)), and [parabolic equations](@article_id:144176) describe [diffusion](@article_id:140951) (like heat spreading). For a [system of equations](@article_id:201334), this classification depends on the [eigenvalues](@article_id:146953) of a [matrix](@article_id:202118) of coefficients. For a system like $\\mathbf{u}_{tt} - A\\mathbf{u}_{xx} = 0$, the system is "strictly hyperbolic," meaning it supports distinct wave speeds, if the [eigenvalues](@article_id:146953) of the [matrix](@article_id:202118) $A$ are real and distinct.\n\nNow, what if we are uncertain about the exact values of the physical parameters that make up the [matrix](@article_id:202118) $A$? We can model them as [random variables](@article_id:142345). Suppose the [matrix](@article_id:202118) is $A = \\begin{pmatrix} a  b \\\\ c  d \\end{pmatrix}$, where $a,b,c,d$ are [independent random variables](@article_id:273402) uniformly distributed on $[-1, 1]$. The condition for real, distinct [eigenvalues](@article_id:146953) turns out to be that the [discriminant](@article_id:152126) is positive: $(a-d)^2 + 4bc  0$. And there it is, hiding in plain sight: the term $bc$ is the product of two independent uniform [random variables](@article_id:142345)! To calculate the [probability](@article_id:263106) that our uncertain physical system behaves like a wave, we must integrate over the distributions of $a,b,c,d$ where this inequality holds. The calculation hinges critically on the [probability distribution](@article_id:145910) of the product $Y = bc$. The same mathematical pattern connects the abstract world of [probability](@article_id:263106) to the concrete classification of the fundamental laws of nature [@problem_id:410152].\n\n### The Unfolding of Processes in Time\n\nSo far, we have been taking snapshots: a single [meta-analysis](@article_id:263380), a single [determinant](@article_id:142484). But the world is not a snapshot; it is a movie. Many phenomena involve a sequence of random events unfolding in time. Imagine a beam of light passing through a series of dusty lenses, with each lens attenuating its intensity by a random fraction. Or consider an investment whose value is multiplied by a random factor each year.\n\nA beautiful way to model such situations is with a compound [stochastic process](@article_id:159008). Suppose that events occur at random times, following a Poisson process with rate $\\lambda$. Each time an event occurs, our system\'s value is multiplied by a random factor $X_k$, drawn from a [uniform distribution](@article_id:261240). The state of the system at time $t$ is then a product of a *random number* of variables:\n$$P(t) = \\prod_{k=1}^{N(t)} X_k$$\nwhere $N(t)$ is the random number of events that have occurred by time $t$. To understand the average value of $P(t)$ or its [volatility](@article_id:266358) ([variance](@article_id:148683)), we must lean on the laws of [probability](@article_id:263106). Using a technique called the [law of total expectation](@article_id:267435), we can find that the [expected value](@article_id:160628) of the process involves the expectation of the individual factors, $E[X]$, while the [variance](@article_id:148683) involves both $E[X]$ and $E[X^2]$. The properties of the [product of random variables](@article_id:266002) are absolutely central to analyzing these [multiplicative processes](@article_id:173129) that are ubiquitous in finance, physics, and biology [@problem_id:815143].\n\nThis idea can be taken to its logical conclusion: what happens in an infinite cascade of multiplicative shocks? Consider an [infinite product](@article_id:172862) $P = X_1 X_2 X_3 \\cdots$. Does this product converge to a well-defined finite value, or does it explode to infinity or vanish to zero? This is a fundamental question of stability. For a sequence of [independent random variables](@article_id:273402) $X_k$ that are all close to 1, such as being uniform on $[1, 1+1/k^2]$, the product does indeed converge [almost surely](@article_id:262024) to a limiting [random variable](@article_id:194836) $P$. Amazingly, we can calculate the exact [expected value](@article_id:160628) of this limit. The expectation of the [infinite product](@article_id:172862) becomes an [infinite product](@article_id:172862) of the individual expectations. In this case, that becomes $\\prod_{k=1}^{\\infty} (1 + \\frac{1}{2k^2})$. This is a classic [infinite product](@article_id:172862) from 19th-century analysis, related to the Euler product for the hyperbolic sine function! A question about the long-term behavior of a [random process](@article_id:269111) finds its answer in a seemingly unrelated corner of classical analysis, revealing once more the deep unity of mathematical thought [@problem_id:1281048].\n\n### Symmetry, Information, and Surprise\n\nLet\'s take another sharp turn. We have been multiplying numbers. What happens if we "multiply" other things, like symmetries? Consider the [dihedral group](@article_id:143381) $D_3$, the set of six symmetries of an equilateral triangle (three rotations, including the identity, and three reflections). This set forms a group, where the "product" operation is simply performing one symmetry after another.\n\nNow, let\'s introduce randomness. Suppose we have two [random variables](@article_id:142345). One, $X$, randomly picks a rotation, with each of the three rotations being equally likely. The other, $Y$, randomly picks from a set of two elements: a specific [reflection](@article_id:161616), $s$, or the identity, $e$. What happens when we "multiply" them to get a new random symmetry $Z = XY$? One might expect the result to be some complicated, uneven distribution. The reality is far more elegant. The product $Z$ turns out to be perfectly uniform over the *entire* group of six symmetries. Each of the six possible symmetries becomes an equally likely outcome.\n\nThis is a profound "perfect mixing" result. The act of multiplication has taken randomness that was constrained to smaller [subgroups](@article_id:138518) and spread it perfectly across the whole group. In the language of [information theory](@article_id:146493), the uncertainty, or Shannon Entropy, of the output $Z$ is maximized [@problem_id:132137]. This principle, where group multiplication amplifies and smooths out randomness, has deep implications in fields ranging from [cryptography](@article_id:138672) (where such mixing properties are highly desirable) to [statistical mechanics](@article_id:139122).\n\n### At the Frontiers: Randomness in the Heart of Number Theory\n\nOur journey has taken us from [data analysis](@article_id:148577) to physics and [information theory](@article_id:146493). For our final stop, we venture into one of the purest and oldest realms of mathematics: the theory of numbers. One of the crown jewels of mathematics is the Riemann zeta function, $\\zeta(s)$, which can be represented as an [infinite product](@article_id:172862) over all [prime numbers](@article_id:154201), called the Euler product:\n$$\\zeta(s) = \\prod_{p \\in \\text{Primes}} \\frac{1}{1 - p^{-s}}$$\nThis formula forms a bridge between the continuous world of [complex analysis](@article_id:143870) (the variable $s$) and the discrete world of [prime numbers](@article_id:154201). It is a rigid, deterministic object of immense beauty and mystery.\n\nBut what if we were to ask a physicist\'s question: "What if the universe were slightly different?" What if we introduce a random element into this perfect structure? Let\'s define a *random Euler product* by inserting a random phase factor for each prime:\n$$L(s, \\omega) = \\prod_{p \\in \\text{Primes}} \\frac{1}{1 - X_p(\\omega)p^{-s}}$$\nHere, each $X_p(\\omega)$ is an independent [random variable](@article_id:194836) chosen uniformly from the complex [unit circle](@article_id:266796). Is this new object just a chaotic mess, or does it retain some of the structure of its famous parent? The primary question is: for which [complex numbers](@article_id:154855) $s$ does this product even converge? The analysis reveals that there is a sharp vertical boundary in the [complex plane](@article_id:157735), an "abscissa of almost sure [absolute convergence](@article_id:146232)" $\\sigma_a$. For any $s$ with real part $\\text{Re}(s)  \\sigma_a$, the product is well-behaved with [probability](@article_id:263106) one. For $\\text{Re}(s)  \\sigma_a$, it is not. The calculation of this boundary relies on analyzing the convergence of the sum $\\sum_p |X_p(\\omega) p^{-s}|$, which is intimately related to the behavior of the sum $\\sum_p p^{-\\text{Re}(s)}$. The astonishing result is that $\\sigma_a = 1$ [@problem_id:2273523]. Randomness did not destroy the structure; instead, it respected the fundamental boundary set by the original deterministic theory. This shows how probabilistic thinking can be used to explore and appreciate the robustness of deep mathematical structures.\n\nFrom combining p-values to testing our computers, from classifying physical laws to exploring the frontiers of [number theory](@article_id:138310), the humble product of uniform [random variables](@article_id:142345) has been our constant companion. Its story is a testament to the Feynman-esque vision of science: a web of interconnected ideas, where simple patterns reappear in the most unexpected of places, revealing the inherent beauty and unity of the world.', 'src': 'https://i.imgur.com/xHh0J4Z.png', 'width': '500', 'alt': 'Plot of the PDF f(z) = -ln(z) for z in (0,1)'}, 'align': 'center'}}, '#text': '## Principles and Mechanisms\n\nSo, we have set the stage. We are on a quest to understand what happens when we take two numbers, each chosen at random from a uniform spread of possibilities, and multiply them together. It sounds simple enough, almost like a child\'s game. But as we are about to discover, this simple act of multiplication unlocks a world of surprising patterns and deep principles. The journey from a vague intuition to a precise, elegant law is a perfect illustration of how a physicist or a mathematician thinks. We start with the simplest questions and build our way up.\n\n### The Simplicity of Averages: When Independence is Key\n\nLet\'s begin with a very basic question: what is the *average* outcome of this multiplication? The average, or **[expected value](@article_id:160628)**, is often a good first port of call. It gives us a sense of the "[center of gravity](@article_id:273025)" of a [random process](@article_id:269111).\n\nImagine a simple game. We have a special three-sided die with faces marked -1, 0, and 1. You and a friend each roll one such die, independently. Let\'s say your roll is $X$ and your friend\'s is $Y$. We are interested in the product, $Z = XY$. What is the average value of $Z$ over many games? You might have an intuition. Since $X$ is equally likely to be -1 or 1, its average should be zero. The same goes for $Y$. Does this mean the average of the product is also zero?\n\nIt turns out the answer is yes, and the reason is a cornerstone of [probability theory](@article_id:140665). When two [random variables](@article_id:142345) $X$ and $Y$ are **independent**—meaning the outcome of one has no influence on the outcome of the other—the expectation of their product is simply the product of their expectations:\n\n$$E[Z] = E[XY] = E[X] E[Y]$$\n\nFor our three-sided die, the expectation of a single roll $X$ is calculated by summing each outcome weighted by its [probability](@article_id:263106): $E[X] = (-1) \\cdot \\frac{1}{3} + (0) \\cdot \\frac{1}{3} + (1) \\cdot \\frac{1}{3} = 0$. Since $E[Y]$ is also 0, the [expected value](@article_id:160628) of their product is $E[Z] = 0 \\cdot 0 = 0$. Our intuition was spot on! [@problem_id:9105]\n\nThis principle is wonderfully general. Let\'s move from discrete dice rolls to continuous values. Imagine a simulation where two [independent events](@article_id:275328) are timed. The first, $X$, takes a random amount of time between 0 and $a$ minutes. The second, $Y$, takes a random time between 0 and $b$ minutes. Both are **uniformly distributed**, meaning any time in their respective intervals is equally likely. A [cost function](@article_id:138187) for the simulation is defined as the product of these times, $C = XY$. What is the expected cost?\n\nWe can use the same powerful rule. First, what\'s the average time for $X$? Since it\'s uniformly distributed on $[0, a]$, the average is just the midpoint, $E[X] = \\frac{a}{2}$. Similarly, $E[Y] = \\frac{b}{2}$. Because the events are independent, the expected cost is simply:\n\n$$E[C] = E[XY] = E[X] E[Y] = \\left(\\frac{a}{2}\\right) \\left(\\frac{b}{2}\\right) = \\frac{ab}{4}$$\n[@problem_id:1347811]\n\nThere is a beautiful simplicity here. The average of the product is exactly what you\'d guess if you just multiplied the average values. It seems almost too easy. But be warned: while the average behaves simply, it hides a much more complex and fascinating story about the *distribution* of these products. Knowing the average height of people in a city tells you nothing about whether the population consists of all medium-height people or a mix of giants and dwarves. To see the full picture, we must go beyond the average.\n\n### Charting the Territory: The Shape of the Product\n\nLet\'s standardize our problem to its purest form. We pick two numbers, $X$ and $Y$, independently and uniformly from the interval $[0, 1]$. What can we say about their product, $Z = XY$?\n\nWe can visualize this process. The pair of numbers $(X, Y)$ represents a single point chosen randomly from a unit square in the plane. Since any point is equally likely, the [probability](@article_id:263106) of the point landing in any particular region of the square is simply the area of that region. This turns a problem of [probability](@article_id:263106) into a problem of geometry!\n\nLet\'s ask a concrete question: what is the [probability](@article_id:263106) that the product $XY$ is less than, say, $0.25$? [@problem_id:9421]. In our geometric picture, this means we are looking for the area of the region within the unit square where the inequality $xy < 0.25$ holds. This inequality can be rewritten as $y < \\frac{0.25}{x}$. This describes the area under the [hyperbola](@article_id:173719) $y = 0.25/x$.\n\nThe curve $y=0.25/x$ cuts through our unit square. We need to find the area of the part of the square that lies "below" this curve. We can see right away that this area is not a simple rectangle or triangle. We have to use [calculus](@article_id:145546). The calculation involves splitting the square into two pieces: a rectangle on the left (where $x$ is small enough that $0.25/x$ is greater than 1, so any $y$ in the square works) and a region under the curve on the right. When the dust settles from the [integration](@article_id:158448), the [probability](@article_id:263106) comes out to be:\n\n$$P(XY < 0.25) = \\frac{1 + \\ln(4)}{4} \\approx 0.5966$$\n\nThe appearance of a **logarithm**, $\\ln(4)$, is the first major clue that we have stumbled upon something more profound than the simple multiplication of averages. Logarithms often appear in nature when dealing with [multiplicative processes](@article_id:173129) or phenomena involving ratios and scales. We are on the right track.\n\n### Unveiling the Master Blueprint: The Product\'s True Identity\n\nAsking about $P(XY < 0.25)$ is like taking a single photograph. To get the whole movie, we need to ask a more general question: what is the [probability](@article_id:263106) that $XY \\le z$ for *any* value $z$ between 0 and 1? This function, $F_Z(z) = P(Z \\le z)$, is known as the **Cumulative Distribution Function (CDF)**. It is the master blueprint that describes our product variable completely.\n\nThe method to find it is exactly the same as before; we just replace the constant $0.25$ with the variable $z$ [@problem_id:9602]. We calculate the area in the unit square where $xy \\le z$. The calculation yields a beautifully compact formula:\n\n$$F_Z(z) = z - z \\ln(z), \\quad \\text{for } 0 < z \\le 1$$\n\nThis is a remarkable result. But the real stunner, the "aha!" moment, comes when we take the next step. The CDF tells us the *accumulated* [probability](@article_id:263106) up to a value $z$. To find the *[likelihood](@article_id:166625)* of the product being at a specific value $z$, we need the **Probability Density Function (PDF)**, which is simply the [derivative](@article_id:157426) of the CDF with respect to $z$. Let\'s perform the differentiation:\n\n$$f_Z(z) = \\frac{d}{dz} (z - z \\ln(z)) = (1) - \\left(1 \\cdot \\ln(z) + z \\cdot \\frac{1}{z}\\right) = 1 - (\\ln(z) + 1) = -\\ln(z)$$\n[@problem_id:1449594] [@problem_id:477858]\n\nThere it is. The [probability density](@article_id:143372) for the product of two uniform random numbers on $[0,1]$ is given by the function $f_Z(z) = -\\ln(z)$.'}

