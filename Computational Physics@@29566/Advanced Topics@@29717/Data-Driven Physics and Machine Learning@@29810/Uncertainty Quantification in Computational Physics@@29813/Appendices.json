{"hands_on_practices": [{"introduction": "Our first practice addresses the fundamental question of how uncertainties in model inputs propagate to the output. We will use the classic example of a simple pendulum to see how small uncertainties in its length and the local gravity, which may be correlated, affect the precision of its calculated period. This exercise [@problem_id:2448343] introduces the powerful and widely used first-order uncertainty propagation method, a cornerstone of error analysis in experimental and computational science.", "problem": "Write a complete program that, for an ideal small-angle simple pendulum, quantifies the uncertainty in its oscillation period when the pendulum length and the local gravitational acceleration are jointly uncertain and correlated. The period is modeled by the mapping $T:\\mathbb{R}_{>0}^{2}\\to\\mathbb{R}_{>0}$ defined by $T(L,g)=2\\pi\\sqrt{L/g}$. Assume that the input vector $(L,g)$ is jointly normally distributed with mean vector $(\\mu_L,\\mu_g)$, standard deviations $(\\sigma_L,\\sigma_g)$, and correlation coefficient $\\rho\\in[-1,1]$. For each parameter set, determine the first-order approximation to the mean period and its corresponding first-order standard deviation obtained by linearizing $T$ about $(\\mu_L,\\mu_g)$. Express all period values and standard deviations in seconds, rounded to $6$ decimal places.\n\nUse the following test suite, where $L$ is in meters and $g$ is in meters per second squared, and where each tuple is $(\\mu_L,\\sigma_L,\\mu_g,\\sigma_g,\\rho)$:\n- Test $1$: $(1.000,0.005,9.80665,0.020,0.000)$.\n- Test $2$: $(2.000,0.010,9.81000,0.050,0.900)$.\n- Test $3$: $(0.500,0.002,9.78000,0.030,-0.900)$.\n- Test $4$: $(1.500,0.000001,9.81000,0.000001,0.000)$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each test case result must itself be a two-element list containing the approximate mean period and the corresponding first-order standard deviation, both in seconds and rounded to $6$ decimal places. For example, the overall output format must be of the form $[[\\mu_{T,1},\\sigma_{T,1}],[\\mu_{T,2},\\sigma_{T,2}],[\\mu_{T,3},\\sigma_{T,3}],[\\mu_{T,4},\\sigma_{T,4}]]$ with no additional text.", "solution": "The problem statement has been rigorously validated and is found to be scientifically sound, well-posed, and objective. It provides a complete and consistent setup for uncertainty quantification using established principles of physics and statistics. We may therefore proceed with the solution.\n\nThe problem requires the quantification of uncertainty in the period $T$ of a simple pendulum, where $T$ is a nonlinear function of the pendulum length $L$ and the local gravitational acceleration $g$. The mapping is given by:\n$$\nT(L, g) = 2\\pi\\sqrt{\\frac{L}{g}}\n$$\nThe input parameters $(L, g)$ are treated as a bivariate random variable, jointly normally distributed with mean vector $\\boldsymbol{\\mu} = (\\mu_L, \\mu_g)^T$ and a covariance matrix $\\Sigma$. The covariance matrix is defined by the standard deviations $\\sigma_L$ and $\\sigma_g$, and the correlation coefficient $\\rho$:\n$$\n\\Sigma = \\begin{pmatrix} \\sigma_L^2 & \\rho \\sigma_L \\sigma_g \\\\ \\rho \\sigma_L \\sigma_g & \\sigma_g^2 \\end{pmatrix}\n$$\nWe are tasked with finding the first-order approximations for the mean $\\mu_T$ and standard deviation $\\sigma_T$ of the period $T$. This is achieved by applying the law of propagation of uncertainty, which is based on a first-order Taylor series expansion of the function $T(L, g)$ around the mean values $(\\mu_L, \\mu_g)$.\n\nFirst, the first-order approximation of the mean of the output, $\\mu_T$, is simply the function evaluated at the means of the inputs. This is the zeroth-order term in the Taylor expansion of the expected value. Higher-order terms, which involve the Hessian of the function, are neglected in this first-order approach.\n$$\n\\mu_T \\approx T(\\mu_L, \\mu_g) = 2\\pi\\sqrt{\\frac{\\mu_L}{\\mu_g}}\n$$\nLet us denote this evaluation at the mean as $T_0 = T(\\mu_L, \\mu_g)$.\n\nNext, we determine the first-order approximation for the variance of the output, $\\sigma_T^2$. For a general function $Y = f(X_1, X_2)$, the variance is given by:\n$$\n\\sigma_Y^2 \\approx \\left(\\frac{\\partial f}{\\partial X_1}\\right)^2 \\sigma_{X_1}^2 + \\left(\\frac{\\partial f}{\\partial X_2}\\right)^2 \\sigma_{X_2}^2 + 2 \\left(\\frac{\\partial f}{\\partial X_1}\\right) \\left(\\frac{\\partial f}{\\partial X_2}\\right) \\text{Cov}(X_1, X_2)\n$$\nwhere the partial derivatives are evaluated at the mean values $(\\mu_{X_1}, \\mu_{X_2})$.\n\nFor our specific function $T(L, g) = 2\\pi L^{1/2} g^{-1/2}$, we must compute the partial derivatives with respect to $L$ and $g$.\nThe partial derivative with respect to $L$ is:\n$$\n\\frac{\\partial T}{\\partial L} = 2\\pi \\left(\\frac{1}{2} L^{-1/2} g^{-1/2}\\right) = \\pi \\frac{1}{\\sqrt{Lg}}\n$$\nThe partial derivative with respect to $g$ is:\n$$\n\\frac{\\partial T}{\\partial g} = 2\\pi L^{1/2} \\left(-\\frac{1}{2} g^{-3/2}\\right) = -\\pi \\frac{\\sqrt{L}}{g\\sqrt{g}} = -\\pi \\frac{\\sqrt{L}}{\\sqrt{g^3}}\n$$\nEvaluating these derivatives at the mean point $(\\mu_L, \\mu_g)$ can be simplified by relating them to $T_0$:\n$$\n\\left. \\frac{\\partial T}{\\partial L} \\right|_{(\\mu_L, \\mu_g)} = \\pi \\frac{1}{\\sqrt{\\mu_L \\mu_g}} = \\left(2\\pi\\sqrt{\\frac{\\mu_L}{\\mu_g}}\\right) \\left(\\frac{1}{2\\mu_L}\\right) = \\frac{T_0}{2\\mu_L}\n$$\n$$\n\\left. \\frac{\\partial T}{\\partial g} \\right|_{(\\mu_L, \\mu_g)} = -\\pi \\frac{\\sqrt{\\mu_L}}{\\sqrt{\\mu_g^3}} = \\left(2\\pi\\sqrt{\\frac{\\mu_L}{\\mu_g}}\\right) \\left(-\\frac{1}{2\\mu_g}\\right) = -\\frac{T_0}{2\\mu_g}\n$$\nNow, substitute these sensitivity coefficients into the variance propagation formula. The covariance term is $\\text{Cov}(L, g) = \\rho\\sigma_L\\sigma_g$.\n$$\n\\sigma_T^2 \\approx \\left(\\frac{T_0}{2\\mu_L}\\right)^2 \\sigma_L^2 + \\left(-\\frac{T_0}{2\\mu_g}\\right)^2 \\sigma_g^2 + 2\\left(\\frac{T_0}{2\\mu_L}\\right)\\left(-\\frac{T_0}{2\\mu_g}\\right)(\\rho\\sigma_L\\sigma_g)\n$$\nFactoring out common terms yields:\n$$\n\\sigma_T^2 \\approx \\frac{T_0^2}{4} \\left[ \\frac{\\sigma_L^2}{\\mu_L^2} + \\frac{\\sigma_g^2}{\\mu_g^2} - 2\\rho\\frac{\\sigma_L\\sigma_g}{\\mu_L\\mu_g} \\right]\n$$\nThis expression can be written more elegantly using relative standard deviations (coefficients of variation), $c_L = \\sigma_L/\\mu_L$ and $c_g = \\sigma_g/\\mu_g$:\n$$\n\\sigma_T^2 \\approx \\frac{T_0^2}{4} \\left[ c_L^2 + c_g^2 - 2\\rho c_L c_g \\right]\n$$\nThe first-order standard deviation $\\sigma_T$ is the square root of the variance:\n$$\n\\sigma_T \\approx \\sqrt{\\sigma_T^2} = \\frac{T_0}{2} \\sqrt{ \\left(\\frac{\\sigma_L}{\\mu_L}\\right)^2 + \\left(\\frac{\\sigma_g}{\\mu_g}\\right)^2 - 2\\rho \\left(\\frac{\\sigma_L}{\\mu_L}\\right) \\left(\\frac{\\sigma_g}{\\mu_g}\\right) }\n$$\nThis final formula provides the required first-order standard deviation of the oscillation period. The algorithm for implementation is as follows:\n$1$. For each test case $(\\mu_L, \\sigma_L, \\mu_g, \\sigma_g, \\rho)$, first compute the approximate mean period $T_0 = 2\\pi\\sqrt{\\mu_L/\\mu_g}$.\n$2$. Compute the relative standard deviations for $L$ and $g$.\n$3$. Substitute these values into the derived formula for $\\sigma_T$.\n$4$. Round both $T_0$ and $\\sigma_T$ to the specified $6$ decimal places.\nThis procedure will be implemented for all provided test cases.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes first-order approximations for the mean and standard deviation\n    of a pendulum's period given uncertain inputs.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    # Each tuple is (mu_L, sigma_L, mu_g, sigma_g, rho).\n    test_cases = [\n        (1.000, 0.005, 9.80665, 0.020, 0.000),\n        (2.000, 0.010, 9.81000, 0.050, 0.900),\n        (0.500, 0.002, 9.78000, 0.030, -0.900),\n        (1.500, 0.000001, 9.81000, 0.000001, 0.000),\n    ]\n\n    results = []\n    for case in test_cases:\n        mu_L, sigma_L, mu_g, sigma_g, rho = case\n\n        # Step 1: Calculate the first-order approximation for the mean period (T_0)\n        # mu_T is approximated by T(mu_L, mu_g)\n        mu_T = 2 * np.pi * np.sqrt(mu_L / mu_g)\n\n        # Step 2: Calculate the first-order standard deviation of the period\n        # Formula derived from first-order Taylor expansion (propagation of uncertainty)\n        # sigma_T = (T_0 / 2) * sqrt( (sigma_L/mu_L)^2 + (sigma_g/mu_g)^2 - 2*rho*(sigma_L/mu_L)*(sigma_g/mu_g) )\n        \n        # Avoid division by zero, though problem constraints ensure mu_L and mu_g > 0\n        if mu_L == 0 or mu_g == 0:\n            # This case is not expected based on the problem description\n            sigma_T = float('inf')\n        else:\n            rel_std_L = sigma_L / mu_L\n            rel_std_g = sigma_g / mu_g\n            \n            variance_term = (rel_std_L**2) + (rel_std_g**2) - (2 * rho * rel_std_L * rel_std_g)\n            \n            # The term inside sqrt must be non-negative.\n            # It represents the variance of a linear combination of correlated variables\n            # and is guaranteed to be non-negative.\n            if variance_term < 0:\n                # This could happen due to floating point inaccuracies, clamp to 0\n                variance_term = 0\n\n            sigma_T = (mu_T / 2) * np.sqrt(variance_term)\n\n        # Step 3: Round results to 6 decimal places\n        mu_T_rounded = round(mu_T, 6)\n        sigma_T_rounded = round(sigma_T, 6)\n        \n        results.append([mu_T_rounded, sigma_T_rounded])\n\n    # Final print statement in the exact required format.\n    # Convert the list of lists to the specified string format\n    # e.g., [[val1, val2],[val3, val4]]\n    result_str = \",\".join(map(str, results))\n    print(f\"[{result_str}]\")\n\nsolve()\n```", "id": "2448343"}, {"introduction": "After quantifying the overall uncertainty in a model's output, the next logical step is to identify which input parameters are the primary drivers of that uncertainty. This practice [@problem_id:2448383] dives into the powerful technique of global sensitivity analysis (GSA) using a realistic model of multiphase flow in a pipe. You will implement a Monte Carlo-based approach to determine whether uncertainties in fluid viscosities or in pipe roughness have a greater impact on the predicted pressure drop, a crucial skill for focusing research and engineering efforts.", "problem": "A horizontal, isothermal, steady, incompressible, fully developed flow of two immiscible Newtonian fluids in a circular pipe is modeled by the homogeneous mixture approximation. Let fluid $1$ have density $\\rho_1$ and dynamic viscosity $\\mu_1$, and fluid $2$ have density $\\rho_2$ and dynamic viscosity $\\mu_2$. The pipe has length $L$, diameter $D$, and uniform internal absolute roughness $\\varepsilon$. The volumetric flow rate is $Q$, and the volume fraction of fluid $1$ is $\\phi \\in [0,1]$ (so the volume fraction of fluid $2$ is $1-\\phi$). Gravitational and acceleration effects are neglected. The objective is to quantify, for specified parameter sets and uncertainties, whether the uncertainty in viscosities or the uncertainty in roughness contributes more to the uncertainty in the pressure drop.\n\nModel definitions to be used:\n- Mixture density: $\\rho_m = \\phi \\,\\rho_1 + (1-\\phi)\\,\\rho_2$.\n- Mixture dynamic viscosity (linear volumetric mixing): $\\mu_m = \\phi \\,\\mu_1 + (1-\\phi)\\,\\mu_2$.\n- Mean velocity: $U_m = \\dfrac{4Q}{\\pi D^2}$.\n- Reynolds number: $\\mathrm{Re} = \\dfrac{\\rho_m U_m D}{\\mu_m}$.\n- Darcy–Weisbach friction factor $f$:\n  - Laminar regime: if $\\mathrm{Re} < 2300$, then $f = \\dfrac{64}{\\mathrm{Re}}$.\n  - Turbulent regime: if $\\mathrm{Re} \\ge 2300$, use the Haaland correlation\n    $$\\frac{1}{\\sqrt{f}} = -1.8 \\log_{10}\\!\\left[\\left(\\frac{\\varepsilon/D}{3.7}\\right)^{1.11} + \\frac{6.9}{\\mathrm{Re}}\\right].$$\n- Pressure drop: $$\\Delta P = f \\,\\frac{L}{D}\\,\\frac{\\rho_m U_m^2}{2}.$$\n\nUncertain inputs:\n- $\\mu_1 \\sim \\text{Uniform}([0.02,\\,0.10])$ in $\\mathrm{Pa\\cdot s}$.\n- $\\mu_2 \\sim \\text{Uniform}([0.0008,\\,0.0015])$ in $\\mathrm{Pa\\cdot s}$.\n- $\\varepsilon \\sim \\text{Uniform}([10^{-5},\\,10^{-3}])$ in $\\mathrm{m}$.\nAssume $\\mu_1$, $\\mu_2$, and $\\varepsilon$ are independent.\n\nAll other parameters are deterministic and specified per test case below. The thermophysical properties are fixed as $\\rho_1 = 850\\,\\mathrm{kg/m^3}$ and $\\rho_2 = 1000\\,\\mathrm{kg/m^3}$.\n\nSensitivity quantities to be compared:\n- Let $Y = \\Delta P(\\mu_1,\\mu_2,\\varepsilon)$ be the modeled pressure drop. For a subset of inputs $S$, define the first-order variance contribution\n  $$C_S = \\frac{\\mathrm{Var}\\!\\left(\\mathbb{E}[Y \\mid S]\\right)}{\\mathrm{Var}(Y)},$$\n  where the variance and expectation are with respect to the joint distribution of $(\\mu_1,\\mu_2,\\varepsilon)$.\n- Define $C_{\\text{visc}}$ with $S = \\{\\mu_1,\\mu_2\\}$ and $C_{\\text{rough}}$ with $S = \\{\\varepsilon\\}$.\n\nTask:\nFor each test case, compute $C_{\\text{visc}}$ and $C_{\\text{rough}}$ and return a boolean indicating whether $C_{\\text{visc}} > C_{\\text{rough}}$.\n\nTest suite (all quantities must be used in the International System of Units):\n- Case $1$: $L=100$ $\\mathrm{m}$, $D=0.05$ $\\mathrm{m}$, $Q=0.002$ $\\mathrm{m^3/s}$, $\\phi=0.5$.\n- Case $2$: $L=150$ $\\mathrm{m}$, $D=0.15$ $\\mathrm{m}$, $Q=0.03$ $\\mathrm{m^3/s}$, $\\phi=0.5$.\n- Case $3$: $L=200$ $\\mathrm{m}$, $D=0.20$ $\\mathrm{m}$, $Q=0.12$ $\\mathrm{m^3/s}$, $\\phi=0.2$.\n- Case $4$: $L=80$ $\\mathrm{m}$, $D=0.04$ $\\mathrm{m}$, $Q=0.0004$ $\\mathrm{m^3/s}$, $\\phi=0.8$.\n\nRequired final output format:\nYour program should produce a single line of output containing the results as a comma-separated list of boolean values enclosed in square brackets and containing no spaces. The $i$-th entry must be True if, for test case $i$, $C_{\\text{visc}} > C_{\\text{rough}}$, and False otherwise. For example, a valid output format is \"[True,False,True,False]\".", "solution": "The problem statement is subjected to validation and is found to be valid. It is scientifically grounded in established principles of fluid mechanics and uncertainty quantification, is well-posed, completely specified, and objective. All models, parameters, and distributions are explicitly defined, permitting a unique and verifiable numerical solution. I will therefore proceed with a full solution.\n\nThe problem requires a comparison between the uncertainty contribution of viscosity parameters $(\\mu_1, \\mu_2)$ and the pipe roughness parameter $(\\varepsilon)$ to the total uncertainty in the calculated pressure drop, $\\Delta P$. This is a problem of variance-based global sensitivity analysis. The quantities to be compared, $C_{\\text{visc}}$ and $C_{\\text{rough}}$, are the first-order Sobol' indices for the group of viscosity parameters $S_{\\text{visc}} = \\{\\mu_1, \\mu_2\\}$ and the roughness parameter $S_{\\text{rough}} = \\{\\varepsilon\\}$, respectively.\n\nThe model for the pressure drop, $Y = \\Delta P(\\mu_1, \\mu_2, \\varepsilon)$, is a nonlinear function of its inputs, involving a conditional switch between laminar and turbulent flow regimes and an implicit non-linear correlation (Haaland equation) for the turbulent friction factor. Direct analytical integration to compute the required variances and conditional expectations is intractable. Therefore, a numerical Monte Carlo (MC) method is the appropriate tool.\n\nThe core of the task is to compute the variance of conditional expectations, $V_S = \\mathrm{Var}(\\mathbb{E}[Y \\mid S])$, for the two sets of input parameters $S = S_{\\text{visc}}$ and $S = S_{\\text{rough}}$. The comparison $C_{\\text{visc}} > C_{\\text{rough}}$ is equivalent to comparing the numerators $V_{\\text{visc}} > V_{\\text{rough}}$, as they share the same denominator, the total variance $\\mathrm{Var}(Y)$.\n\nWe will employ a robust and efficient Monte Carlo estimator for $V_S$. The procedure is as follows:\n1.  Generate two independent $N \\times 3$ matrices of random samples, $\\mathbf{A}$ and $\\mathbf{B}$, where each row is a sample of $(\\mu_1, \\mu_2, \\varepsilon)$ drawn from their specified uniform distributions. $N$ is the number of samples, chosen to be large enough for convergence (e.g., $N=2^{18}$).\n2.  The forward model, $\\Delta P = \\text{model}(\\mu_1, \\mu_2, \\varepsilon)$, is evaluated for the samples in matrix $\\mathbf{A}$, yielding a vector of outputs $\\mathbf{Y}_A$. The mean of this vector, $\\hat{\\mathbb{E}}[Y] = \\frac{1}{N}\\sum_{j=1}^N \\mathbf{Y}_{A,j}$, provides an estimate of the expected value of the pressure drop. Let us denote the square of this estimate as $f_0^2$.\n3.  To estimate $V_{\\text{rough}} = \\mathrm{Var}(\\mathbb{E}[Y \\mid \\varepsilon])$, we construct a third matrix, $\\mathbf{C}_{\\text{rough}}$, by taking the columns for $\\mu_1$ and $\\mu_2$ from matrix $\\mathbf{B}$ and the column for $\\varepsilon$ from matrix $\\mathbf{A}$.\n4.  The model is evaluated for $\\mathbf{C}_{\\text{rough}}$ to obtain the output vector $\\mathbf{Y}_{C_{\\text{rough}}}$. The variance is then estimated as:\n    $$V_{\\text{rough}} \\approx \\frac{1}{N} \\sum_{j=1}^N \\mathbf{Y}_{A,j} \\cdot \\mathbf{Y}_{C_{\\text{rough}},j} - f_0^2$$\n    This estimator converges to $\\mathrm{Var}(\\mathbb{E}[Y \\mid \\varepsilon])$ as $N \\to \\infty$.\n5.  To estimate $V_{\\text{visc}} = \\mathrm{Var}(\\mathbb{E}[Y \\mid \\mu_1, \\mu_2])$, we construct a fourth matrix, $\\mathbf{C}_{\\text{visc}}$, by taking the columns for $\\mu_1$ and $\\mu_2$ from matrix $\\mathbf{A}$ and the column for $\\varepsilon$ from matrix $\\mathbf{B}$.\n6.  The model is evaluated for $\\mathbf{C}_{\\text{visc}}$ to obtain $\\mathbf{Y}_{C_{\\text{visc}}}$. The variance is estimated as:\n    $$V_{\\text{visc}} \\approx \\frac{1}{N} \\sum_{j=1}^N \\mathbf{Y}_{A,j} \\cdot \\mathbf{Y}_{C_{\\text{visc}},j} - f_0^2$$\n    This estimator converges to $\\mathrm{Var}(\\mathbb{E}[Y \\mid \\mu_1, \\mu_2])$.\n7.  For each test case, we compute $V_{\\text{visc}}$ and $V_{\\text{rough}}$ and determine if $V_{\\text{visc}} > V_{\\text{rough}}$.\n\nThe forward model implementation, $\\Delta P(\\mu_1, \\mu_2, \\varepsilon)$, is constructed as a vectorized function using NumPy for computational efficiency. It first computes mixture properties ($\\rho_m, \\mu_m$) and the Reynolds number ($\\mathrm{Re}$). A conditional check on $\\mathrm{Re}$ separates the flow into laminar ($\\mathrm{Re} < 2300$) and turbulent ($\\mathrm{Re} \\ge 2300$) regimes. The Darcy friction factor $f$ is calculated accordingly. For the turbulent regime, the Haaland correlation is given in a form that can be solved explicitly for $f$, avoiding the need for an iterative numerical root-finding procedure:\n$$f = \\left(\\frac{1}{-1.8 \\log_{10}\\!\\left[\\left(\\frac{\\varepsilon/D}{3.7}\\right)^{1.11} + \\frac{6.9}{\\mathrm{Re}}\\right]}\\right)^2$$\nFinally, the pressure drop $\\Delta P$ is computed using the Darcy-Weisbach equation. This entire process is repeated for each of the four specified test cases.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the uncertainty quantification problem for the given test cases.\n    \"\"\"\n\n    # Define deterministic physical constants\n    RHO1 = 850.0  # kg/m^3\n    RHO2 = 1000.0 # kg/m^3\n    RE_TRANSITION = 2300.0\n\n    # Define uncertain parameter distributions (Uniform)\n    MU1_RANGE = [0.02, 0.10]    # Pa.s\n    MU2_RANGE = [0.0008, 0.0015] # Pa.s\n    EPS_RANGE = [1e-5, 1e-3]   # m\n\n    def evaluate_model(mu1_samples, mu2_samples, eps_samples, L, D, Q, phi):\n        \"\"\"\n        Vectorized evaluation of the pressure drop model.\n        \n        Args:\n            mu1_samples (np.array): Samples for viscosity of fluid 1.\n            mu2_samples (np.array): Samples for viscosity of fluid 2.\n            eps_samples (np.array): Samples for pipe roughness.\n            L (float): Pipe length.\n            D (float): Pipe diameter.\n            Q (float): Volumetric flow rate.\n            phi (float): Volume fraction of fluid 1.\n            \n        Returns:\n            np.array: Calculated pressure drop for each sample.\n        \"\"\"\n        # Mixture properties. rho_m is constant for a given case.\n        rho_m = phi * RHO1 + (1.0 - phi) * RHO2\n        mu_m = phi * mu1_samples + (1.0 - phi) * mu2_samples\n        \n        # Flow properties. U_m is constant for a given case.\n        U_m = (4.0 * Q) / (np.pi * D**2)\n        Re = rho_m * U_m * D / mu_m\n        \n        # Darcy friction factor\n        f = np.empty_like(Re)\n        \n        # Laminar flow regime\n        laminar_mask = Re < RE_TRANSITION\n        if np.any(laminar_mask):\n            f[laminar_mask] = 64.0 / Re[laminar_mask]\n            \n        # Turbulent flow regime\n        turbulent_mask = ~laminar_mask\n        if np.any(turbulent_mask):\n            eps_D_ratio = eps_samples[turbulent_mask] / D\n            Re_turb = Re[turbulent_mask]\n            \n            # Haaland correlation solved explicitly for f\n            log_term = np.log10((eps_D_ratio / 3.7)**1.11 + 6.9 / Re_turb)\n            f[turbulent_mask] = (-1.8 * log_term)**-2.0\n            \n        # Pressure drop (Darcy-Weisbach equation)\n        delta_P = f * (L / D) * (rho_m * U_m**2) / 2.0\n        \n        return delta_P\n\n    def compute_variances(params, N, rng):\n        \"\"\"\n        Computes the variance components V_visc and V_rough using Monte Carlo.\n        \"\"\"\n        L, D, Q, phi = params\n        \n        # Generate two independent sets of uniform random numbers in [0, 1]\n        A_uniform = rng.random(size=(N, 3))\n        B_uniform = rng.random(size=(N, 3))\n        \n        # Helper to scale uniform samples to their physical ranges\n        def scale_samples(uniform_samples):\n            mu1 = MU1_RANGE[0] + uniform_samples[:, 0] * (MU1_RANGE[1] - MU1_RANGE[0])\n            mu2 = MU2_RANGE[0] + uniform_samples[:, 1] * (MU2_RANGE[1] - MU2_RANGE[0])\n            eps = EPS_RANGE[0] + uniform_samples[:, 2] * (EPS_RANGE[1] - EPS_RANGE[0])\n            return mu1, mu2, eps\n\n        # Create physical sample matrices A and B\n        mu1_A, mu2_A, eps_A = scale_samples(A_uniform)\n        mu1_B, mu2_B, eps_B = scale_samples(B_uniform)\n\n        # Evaluate model for matrix A to estimate the mean\n        Y_A = evaluate_model(mu1_A, mu2_A, eps_A, L, D, Q, phi)\n        f0_sq = np.mean(Y_A)**2\n        \n        # Construct C matrices and evaluate model outputs\n        # C_rough matrix: mu from B, eps from A. Used to estimate V_rough.\n        Y_C_rough = evaluate_model(mu1_B, mu2_B, eps_A, L, D, Q, phi)\n        # C_visc matrix: mu from A, eps from B. Used to estimate V_visc.\n        Y_C_visc = evaluate_model(mu1_A, mu2_A, eps_B, L, D, Q, phi)\n        \n        # Estimate the variance of conditional expectations\n        V_rough = np.mean(Y_A * Y_C_rough) - f0_sq\n        V_visc = np.mean(Y_A * Y_C_visc) - f0_sq\n        \n        return V_visc, V_rough\n\n    # Define the test cases from the problem statement\n    test_cases = [\n        # (L, D, Q, phi)\n        (100.0, 0.05, 0.002, 0.5),\n        (150.0, 0.15, 0.03, 0.5),\n        (200.0, 0.20, 0.12, 0.2),\n        (80.0, 0.04, 0.0004, 0.8),\n    ]\n\n    results = []\n    # N must be large enough for convergence of the MC estimators\n    N = 2**18\n    # Use a fixed seed for reproducible results\n    rng = np.random.default_rng(seed=123)\n    \n    for case in test_cases:\n        V_visc, V_rough = compute_variances(case, N, rng)\n        results.append(V_visc > V_rough)\n        \n    # Format the final output as a comma-separated list of booleans\n    # The map(str,...) is used to convert boolean True/False to string \"True\"/\"False\"\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2448383"}, {"introduction": "Our final practice explores the other side of uncertainty quantification: inverse problems, where we use observed data to infer the properties of the underlying system. Using the intriguing model of a one-dimensional cellular automaton, you will tackle the challenge of identifying which 'rule' of physics most likely produced a given noisy final state [@problem_id:2448360]. This exercise provides hands-on experience with the Bayesian inference framework, demonstrating how to compute posterior probabilities to perform model selection in the face of uncertainty.", "problem": "You are given a one-dimensional, binary-state, radius-$1$ cellular automaton of lattice length $L$ with periodic boundary conditions. A cellular automaton rule is an element $r \\in \\{0,1,\\dots,255\\}$ from the set of Wolfram elementary rules. For any rule $r$, define the local update function $f_r:\\{0,1\\}^3 \\to \\{0,1\\}$ as follows. Write $r$ in binary as an $8$-bit string $(b_7,b_6,b_5,b_4,b_3,b_2,b_1,b_0)$, where each $b_k \\in \\{0,1\\}$ and $r = \\sum_{k=0}^{7} b_k 2^k$. The output bit for a neighborhood $(\\ell,c,\\rho) \\in \\{0,1\\}^3$ is given by $f_r(\\ell,c,\\rho) = b_{4\\ell + 2c + \\rho}$. Let the state of the automaton at discrete time $t$ be $\\mathbf{x}^{(t)} \\in \\{0,1\\}^L$, updated synchronously by\n$$\nx^{(t+1)}_i = f_r\\!\\bigl(x^{(t)}_{i-1},\\,x^{(t)}_{i},\\,x^{(t)}_{i+1}\\bigr), \\quad i \\in \\{0,1,\\dots,L-1\\},\n$$\nwith periodic boundary conditions $x^{(t)}_{-1} \\equiv x^{(t)}_{L-1}$ and $x^{(t)}_{L} \\equiv x^{(t)}_{0}$.\n\nYou observe a single noisy final state $\\tilde{\\mathbf{y}} \\in \\{0,1\\}^L$ after $T$ time steps from a known initial state $\\mathbf{x}^{(0)}$. The observational noise model is independent bit flips: conditioned on the true final state $\\mathbf{y} = \\mathbf{x}^{(T)}$, each bit is flipped independently with probability $p \\in [0,1]$. That is, for each site $i$,\n$$\n\\mathbb{P}\\!\\left(\\tilde{y}_i \\mid y_i\\right) =\n\\begin{cases}\n1-p, & \\text{if } \\tilde{y}_i = y_i,\\\\\np, & \\text{if } \\tilde{y}_i \\neq y_i.\n\\end{cases}\n$$\nAssume a uniform prior over rules on $\\{0,1,\\dots,255\\}$. Using Bayes’ theorem, compute for each candidate rule $r$ the posterior probability $\\mathbb{P}(r \\mid \\tilde{\\mathbf{y}}, \\mathbf{x}^{(0)}, T, p)$ up to a common normalization constant, and determine the maximum a posteriori rule index $r_{\\mathrm{MAP}}$. In the event of posterior ties, choose the smallest rule index.\n\nYour program must implement this probabilistic model exactly as specified above and, for each test case provided below, return the single integer $r_{\\mathrm{MAP}}$.\n\nTest suite. For each case, all vectors are ordered as $\\bigl[x_0,x_1,\\dots,x_{L-1}\\bigr]$.\n\n- Case A (general): $L = 8$, $T = 3$, $p = 0.1$, initial $\\mathbf{x}^{(0)} = [\\,0,0,0,1,0,0,0,0\\,]$, observed $\\tilde{\\mathbf{y}} = [\\,0,1,0,1,0,1,0,0\\,]$.\n- Case B (deterministic observation, $T=1$): $L = 8$, $T = 1$, $p = 0$, initial $\\mathbf{x}^{(0)} = [\\,1,0,1,0,1,0,1,0\\,]$, observed $\\tilde{\\mathbf{y}} = [\\,0,1,0,1,0,1,0,1\\,]$.\n- Case C (uninformative noise): $L = 5$, $T = 2$, $p = 0.5$, initial $\\mathbf{x}^{(0)} = [\\,0,0,1,0,0\\,]$, observed $\\tilde{\\mathbf{y}} = [\\,1,1,1,1,1\\,]$.\n- Case D (no evolution, $T=0$): $L = 7$, $T = 0$, $p = 0.2$, initial $\\mathbf{x}^{(0)} = [\\,1,1,0,0,1,0,0\\,]$, observed $\\tilde{\\mathbf{y}} = [\\,1,0,0,0,1,0,0\\,]$.\n\nFinal output format. Your program should produce a single line of output containing the results as a comma-separated list of the four integers for Cases A, B, C, and D, in that order, enclosed in square brackets, for example, \"[1,2,3,4]\". No spaces are permitted in the output line. All quantities are dimensionless, and any angles, if present, must be in radians; however, no angles are used here. The required outputs are integers only.", "solution": "We formulate the Bayesian inference problem from first principles. A one-dimensional, binary-state, radius-$1$ cellular automaton with periodic boundary conditions evolves by a deterministic local rule $f_r:\\{0,1\\}^3 \\to \\{0,1\\}$ determined by the Wolfram rule index $r \\in \\{0,1,\\dots,255\\}$. Writing $r$ in binary as $(b_7,b_6,\\dots,b_0)$ with $b_k \\in \\{0,1\\}$ such that $r = \\sum_{k=0}^{7} b_k 2^k$, the local update is $f_r(\\ell,c,\\rho) = b_{4\\ell+2c+\\rho}$, where $(\\ell,c,\\rho)$ are the left, center, and right bits of the neighborhood. For a lattice of length $L$, the global synchronous update is\n$$\nx^{(t+1)}_i = f_r\\!\\bigl(x^{(t)}_{i-1},\\,x^{(t)}_{i},\\,x^{(t)}_{i+1}\\bigr), \\quad i=0,1,\\dots,L-1,\n$$\nwith periodic boundary conditions $x^{(t)}_{-1}=x^{(t)}_{L-1}$ and $x^{(t)}_{L}=x^{(t)}_{0}$. Given an initial configuration $\\mathbf{x}^{(0)} \\in \\{0,1\\}^L$ and a time horizon $T \\in \\mathbb{N}_0$, the predicted noiseless final state under rule $r$ is $\\mathbf{y}_r = \\mathbf{x}^{(T)}$ obtained by $T$ iterations of the update.\n\nThe observation model specifies that the observed vector $\\tilde{\\mathbf{y}}$ is generated from the true final state $\\mathbf{y}_r$ by independent bit flips with probability $p \\in [0,1]$. Hence the likelihood for rule $r$ is\n$$\n\\mathcal{L}(r) = \\mathbb{P}(\\tilde{\\mathbf{y}} \\mid r, \\mathbf{x}^{(0)}, T, p) = \\prod_{i=0}^{L-1} \\left[(1-p)\\,\\mathbb{I}\\{\\tilde{y}_i = y_{r,i}\\} + p\\,\\mathbb{I}\\{\\tilde{y}_i \\neq y_{r,i}\\}\\right].\n$$\nLet $m_r$ be the number of matches, i.e., $m_r = \\sum_{i=0}^{L-1} \\mathbb{I}\\{\\tilde{y}_i = y_{r,i}\\}$, so the number of mismatches is $L - m_r$. Because the bits are independent and identically distributed given $\\mathbf{y}_r$, the likelihood simplifies to\n$$\n\\mathcal{L}(r) = (1-p)^{m_r} \\, p^{L - m_r},\n$$\nwith limiting conventions for $p \\in \\{0,1\\}$: when $p=0$ the likelihood is $1$ if $m_r=L$ and $0$ otherwise, and when $p=1$ the likelihood is $1$ if $m_r=0$ and $0$ otherwise.\n\nAssuming a uniform prior $\\mathbb{P}(r) = 1/256$, Bayes’ theorem gives the posterior\n$$\n\\mathbb{P}(r \\mid \\tilde{\\mathbf{y}}, \\mathbf{x}^{(0)}, T, p) = \\frac{\\mathcal{L}(r)\\,\\mathbb{P}(r)}{\\sum_{r'=0}^{255} \\mathcal{L}(r')\\,\\mathbb{P}(r')} \\propto \\mathcal{L}(r).\n$$\nTherefore, the maximum a posteriori (MAP) estimate is\n$$\nr_{\\mathrm{MAP}} \\in \\arg\\max_{r \\in \\{0,\\dots,255\\}} \\mathcal{L}(r),\n$$\nwith ties broken by selecting the smallest $r$. To avoid numerical underflow for $p \\in (0,1)$ and $L$ moderate, computation should be carried out via the log-likelihood,\n$$\n\\log \\mathcal{L}(r) = m_r \\log(1-p) + (L - m_r) \\log p,\n$$\nwith the appropriate limiting cases handled explicitly for $p \\in \\{0,1\\}$.\n\nAlgorithmic design from principles:\n- For each test case, enumerate all $256$ rules $r$.\n- For each $r$, compute $\\mathbf{y}_r$ by iterating the synchronous update $T$ times from $\\mathbf{x}^{(0)}$, using the local rule $f_r$ and periodic boundary conditions.\n- Compute $m_r$ as the count of indices $i \\in \\{0,\\dots,L-1\\}$ for which $y_{r,i} = \\tilde{y}_i$.\n- Compute $\\log \\mathcal{L}(r)$ using the expression above, handling $p=0$ and $p=1$ via their limits.\n- Select the $r$ that maximizes $\\log \\mathcal{L}(r)$; if multiple rules attain the same value within an exact or numerical tolerance, choose the smallest $r$.\n\nEdge cases in the test suite are covered as follows:\n- Case A has $p = 0.1 \\in (0,1)$ and $T=3$, representing a typical inference scenario.\n- Case B has $p = 0$ and $T=1$. In this case, only rules that produce an exact match $\\mathbf{y}_r = \\tilde{\\mathbf{y}}$ have nonzero likelihood. For the provided initial pattern $\\mathbf{x}^{(0)} = [\\,1,0,1,0,1,0,1,0\\,]$, the neighborhoods that occur at time $t=0$ are only $\\{010,101\\}$. The observed $\\tilde{\\mathbf{y}} = [\\,0,1,0,1,0,1,0,1\\,]$ imposes $f_r(0,1,0) = 0$ and $f_r(1,0,1) = 1$. The smallest $r$ consistent with these two outputs is obtained by setting $b_{2} = 0$ and $b_{5} = 1$ and all other $b_k = 0$, yielding $r = 2^5 = 32$.\n- Case C has $p = 0.5$. For any $r$, $\\log \\mathcal{L}(r) = L \\log(0.5)$ is constant, so the posterior is uniform and the tie-breaking rule selects $r_{\\mathrm{MAP}} = 0$.\n- Case D has $T = 0$. The predicted final state equals the initial state for all $r$, so the likelihood depends only on $\\mathbf{x}^{(0)}$ and $\\tilde{\\mathbf{y}}$, not on $r$, and the posterior is uniform. The tie-breaking rule selects $r_{\\mathrm{MAP}} = 0$.\n\nThe program enumerates all rules, computes the log-likelihoods as above, applies the tie-breaking rule, and prints the four integers $[r_{\\mathrm{MAP},A}, r_{\\mathrm{MAP},B}, r_{\\mathrm{MAP},C}, r_{\\mathrm{MAP},D}]$ in a single line without spaces.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef rule_to_table(rule_index: int) -> np.ndarray:\n    \"\"\"\n    Convert a Wolfram elementary cellular automaton rule index (0..255)\n    into a lookup table of length 8, where table[k] is the output bit\n    for neighborhood with decimal code k = 4*left + 2*center + right.\n    \"\"\"\n    # Bits b0..b7 where b_k is (rule_index >> k) & 1\n    table = np.array([(rule_index >> k) & 1 for k in range(8)], dtype=np.uint8)\n    return table\n\ndef evolve_once(state: np.ndarray, table: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Perform one synchronous update of a 1D binary CA with periodic boundary\n    conditions using the provided rule table.\n    \"\"\"\n    # Periodic neighbors using numpy roll\n    left = np.roll(state, 1)\n    center = state\n    right = np.roll(state, -1)\n    # Neighborhood code: 4*left + 2*center + right\n    codes = (left << 2) | (center << 1) | right\n    next_state = table[codes]\n    return next_state\n\ndef evolve(state0: np.ndarray, rule_index: int, T: int) -> np.ndarray:\n    \"\"\"\n    Evolve the CA for T steps from initial state0 under rule_index.\n    \"\"\"\n    state = state0.copy()\n    if T <= 0:\n        return state\n    table = rule_to_table(rule_index)\n    for _ in range(T):\n        state = evolve_once(state, table)\n    return state\n\ndef log_likelihood(observed: np.ndarray, predicted: np.ndarray, p: float) -> float:\n    \"\"\"\n    Compute log-likelihood under independent bit-flip model with probability p.\n    Handle limiting cases p=0 and p=1 exactly.\n    \"\"\"\n    L = observed.size\n    matches = int(np.sum(observed == predicted))\n    mismatches = L - matches\n    if p == 0.0:\n        return 0.0 if mismatches == 0 else float(\"-inf\")\n    if p == 1.0:\n        return 0.0 if matches == 0 else float(\"-inf\")\n    # General case 0 < p < 1\n    return matches * np.log(1.0 - p) + mismatches * np.log(p)\n\ndef map_rule_for_case(L: int, T: int, p: float, x0_list, yobs_list) -> int:\n    \"\"\"\n    Compute the MAP rule index (0..255) for the given case parameters,\n    using uniform prior and tie-breaking by smallest index.\n    \"\"\"\n    x0 = np.array(x0_list, dtype=np.uint8)\n    yobs = np.array(yobs_list, dtype=np.uint8)\n    assert x0.size == L and yobs.size == L\n    best_rule = 0\n    best_loglike = float(\"-inf\")\n    tol = 1e-12\n    for r in range(256):\n        ypred = evolve(x0, r, T)\n        ll = log_likelihood(yobs, ypred, p)\n        if ll > best_loglike + tol:\n            best_loglike = ll\n            best_rule = r\n        elif abs(ll - best_loglike) <= tol and r < best_rule:\n            best_rule = r\n    return best_rule\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each test case: (L, T, p, x0, y_obs)\n    test_cases = [\n        (8, 3, 0.1, [0,0,0,1,0,0,0,0], [0,1,0,1,0,1,0,0]),  # Case A\n        (8, 1, 0.0, [1,0,1,0,1,0,1,0], [0,1,0,1,0,1,0,1]),  # Case B\n        (5, 2, 0.5, [0,0,1,0,0],       [1,1,1,1,1]),        # Case C\n        (7, 0, 0.2, [1,1,0,0,1,0,0],   [1,0,0,0,1,0,0]),    # Case D\n    ]\n\n    results = []\n    for L, T, p, x0, yobs in test_cases:\n        result = map_rule_for_case(L, T, p, x0, yobs)\n        results.append(result)\n\n    # Final print statement in the exact required format: no spaces.\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "2448360"}]}