{"hands_on_practices": [{"introduction": "The foundation of Hamiltonian Monte Carlo lies in simulating the dynamics of a fictitious particle moving on a potential energy landscape defined by $U(q) = -\\log \\pi(q)$. The particle's motion is dictated by the \"force\" $F = -\\nabla U(q)$, which guides the sampler towards regions of high probability. This first practice is an essential analytical warm-up, challenging you to derive this crucial force term for a target Student's t-distribution, a common model in statistical inference. [@problem_id:791690]", "problem": "In the context of Hamiltonian Monte Carlo (HMC), a Markov Chain Monte Carlo (MCMC) method, we aim to sample from a target probability distribution $\\pi(q)$ by introducing an auxiliary momentum variable $p$. The system's state is described by $(q, p)$, and its dynamics are governed by a Hamiltonian $H(q, p) = U(q) + K(p)$.\n\nThe potential energy, $U(q)$, is defined as the negative logarithm of the target probability density, up to an additive constant: $U(q) = -\\log \\pi(q)$. The kinetic energy, $K(p)$, is typically defined as $K(p) = \\frac{p^2}{2m}$ for a \"mass\" parameter $m$.\n\nThe evolution of the system over time is described by Hamilton's equations:\n$$\n\\frac{dq}{dt} = \\frac{\\partial H}{\\partial p}\n$$\n$$\n\\frac{dp}{dt} = -\\frac{\\partial H}{\\partial q} = -\\frac{\\partial U}{\\partial q}\n$$\nThe term $-\\frac{\\partial U}{\\partial q}$ acts as a \"force\" that guides the trajectory of the sampler. A crucial step in implementing HMC is to derive an analytical expression for this force term, which is the gradient of the potential energy.\n\nConsider sampling from a univariate Student's t-distribution with $\\nu  0$ degrees of freedom. The probability density function (PDF) for a random variable $q$ is given by:\n$$\n\\pi(q) = \\frac{\\Gamma\\left(\\frac{\\nu+1}{2}\\right)}{\\sqrt{\\nu\\pi} \\, \\Gamma\\left(\\frac{\\nu}{2}\\right)} \\left(1 + \\frac{q^2}{\\nu}\\right)^{-\\frac{\\nu+1}{2}}\n$$\nwhere $\\Gamma(\\cdot)$ is the gamma function.\n\nDerive the expression for the gradient of the potential energy, $\\frac{\\partial U}{\\partial q}$, for this target distribution. Your final expression should be in terms of the position variable $q$ and the degrees of freedom parameter $\\nu$.", "solution": "We have the potential energy\n$$\nU(q)=-\\ln\\pi(q)\n=-\\ln\\Bigl[\\tfrac{\\Gamma\\bigl(\\tfrac{\\nu+1}{2}\\bigr)}{\\sqrt{\\nu\\pi}\\,\\Gamma\\bigl(\\tfrac{\\nu}{2}\\bigr)}\\Bigr]\n+\\frac{\\nu+1}{2}\\ln\\Bigl(1+\\frac{q^2}{\\nu}\\Bigr)\\,.\n$$\nThe prefactor is constant in $q$, so up to an additive constant\n$$\nU(q)=\\frac{\\nu+1}{2}\\ln\\Bigl(1+\\frac{q^2}{\\nu}\\Bigr)\\,.\n$$\nDifferentiate w.r.t.\\ $q$:\n\n$$\n\\frac{\\partial U}{\\partial q}\n=\\frac{\\nu+1}{2}\\cdot\\frac{1}{1+\\tfrac{q^2}{\\nu}}\\cdot\\frac{d}{dq}\\Bigl(\\tfrac{q^2}{\\nu}\\Bigr)\n=\\frac{\\nu+1}{2}\\cdot\\frac{1}{1+\\tfrac{q^2}{\\nu}}\\cdot\\frac{2q}{\\nu}\n=\\frac{(\\nu+1)\\,q}{\\nu+q^2}\\,.\n$$", "answer": "$$\\boxed{\\frac{(\\nu+1)\\,q}{\\nu+q^2}}$$", "id": "791690"}, {"introduction": "Moving from theory to practice, the next step is to construct a complete HMC sampler from the ground up. This exercise [@problem_id:2399583] guides you through implementing all the core components: the leapfrog integrator, the Metropolis-Hastings correction, and the momentum resampling. You will also build a simple forward-mode automatic differentiation engine to compute gradients, contrasting this powerful, modern technique with the manual analytic approach from our first practice.", "problem": "You are to design and implement a Hamiltonian Monte Carlo (HMC) sampler for sampling from unnormalized target densities, starting from first principles. Your implementation must be based on the following foundational facts.\n\n1. Hamiltonian dynamics: Introduce auxiliary momentum $p$ with a symmetric positive-definite mass matrix $M$ for position $q \\in \\mathbb{R}^d$. Define the potential energy $U(q) = -\\log \\pi(q)$ up to an additive constant and the kinetic energy $K(p) = \\tfrac{1}{2} p^\\top M^{-1} p$. The Hamiltonian is $H(q,p) = U(q) + K(p)$. Hamilton's equations are\n$$\n\\frac{dq}{dt} = \\frac{\\partial H}{\\partial p} = M^{-1} p, \\qquad \\frac{dp}{dt} = -\\frac{\\partial H}{\\partial q} = -\\nabla U(q).\n$$\n\n2. Symplectic integration: To numerically integrate Hamilton's equations, use a symplectic, time-reversible method of your choice compatible with the equations above. The method must be second-order accurate and should only require evaluations of $\\nabla U(q)$ and matrix-vector products with $M^{-1}$.\n\n3. Metropolis correction: To correct for discretization error, given a proposal $(q', p')$ obtained by numerically integrating the dynamics for a time interval $\\tau$, accept the proposal with probability\n$$\n\\alpha = \\min\\left(1, \\exp\\big(-H(q', p') + H(q, p)\\big)\\right).\n$$\nIf rejected, keep the current state $q$.\n\n4. Momentum resampling: At each iteration, sample $p$ independently from the Gaussian with density proportional to $\\exp\\left(-\\tfrac{1}{2} p^\\top M^{-1} p\\right)$.\n\nYour task is to implement:\n\n- A general HMC sampler that takes as arguments the potential $U(q)$, its gradient $\\nabla U(q)$, a diagonal mass matrix $M = \\mathrm{diag}(m_1, \\dots, m_d)$, a step size $\\epsilon$, a number of integration steps $L$, a number of iterations $N$, a burn-in $B$, and an initial position $q_0$.\n\n- Two gradient providers for $\\nabla U(q)$:\n  1) An analytic gradient computed by hand from the definitions of the specified $U(q)$, and\n  2) A forward-mode automatic differentiation engine implemented within your program, using dual numbers with a derivative vector of length $d$ propagated through elementary scalar operations. Your automatic differentiation engine should return both the scalar value $U(q)$ and the gradient $\\nabla U(q)$ for vector inputs $q \\in \\mathbb{R}^d$ in a single forward pass.\n\nTarget distributions and potentials:\n\n- Case A (correlated Gaussian in $\\mathbb{R}^2$): Let the target be a correlated Gaussian with mean $\\mu = (1.0, -1.0)$ and covariance\n$$\n\\Sigma = \\begin{bmatrix}\n1.0  0.8\\\\\n0.8  2.0\n\\end{bmatrix}.\n$$\nLet\n$$\nU(q) = \\tfrac{1}{2} (q - \\mu)^\\top \\Sigma^{-1} (q - \\mu),\n$$\nso that $\\pi(q) \\propto \\exp\\big(-U(q)\\big)$. Provide the analytic gradient $\\nabla U(q) = \\Sigma^{-1} (q - \\mu)$.\n\n- Case B (same correlated Gaussian but deliberately mis-tuned integrator): Use the same $U(q)$ and analytic gradient as in Case A, but with an intentionally large step size to probe the acceptance behavior of HMC.\n\n- Case C (Rosenbrock-type potential in $\\mathbb{R}^2$): Let $q = (x,y)$ and\n$$\nU(q) = \\frac{(1 - x)^2 + 100 (y - x^2)^2}{20}.\n$$\nSupply its analytic gradient derived from first principles. Note that this target is unnormalized but valid for HMC since only $U(q)$ differences are used.\n\nImplementation requirements:\n\n- The symplectic integrator must be consistent with the Hamiltonian system above and must use only evaluations of $\\nabla U(q)$ and multiplications by $M^{-1}$.\n\n- The automatic differentiation engine must implement forward-mode dual numbers with derivative vectors and support scalar arithmetic operations needed by the potentials above (addition, subtraction, multiplication, division, and integer powers), producing the full gradient $\\nabla U(q)$ for vector input $q$.\n\n- Randomness must be reproducible: use a fixed random seed for each case so that results are deterministic. Angles are not involved; no unit conversions are needed.\n\nTest suite parameters:\n\n- Case A (\"happy path\"):\n  - Dimension $d = 2$.\n  - Mean $\\mu = (1.0, -1.0)$ and covariance $\\Sigma$ as above; use its exact inverse $\\Sigma^{-1}$.\n  - Diagonal mass $M = \\mathrm{diag}(1.0, 1.0)$.\n  - Initial position $q_0 = (0.0, 0.0)$.\n  - Step size $\\epsilon = 0.15$.\n  - Integration steps $L = 20$.\n  - Iterations $N = 1500$ with burn-in $B = 300$.\n  - Seed $s = 42$.\n\n- Case B (boundary-stressing large step size):\n  - Same $\\mu$, $\\Sigma$, and $M$.\n  - $q_0 = (0.0, 0.0)$.\n  - Step size $\\epsilon = 0.9$.\n  - Integration steps $L = 25$.\n  - Iterations $N = 800$ with burn-in $B = 200$.\n  - Seed $s = 43$.\n\n- Case C (Rosenbrock potential):\n  - $U(q) = \\big((1 - x)^2 + 100 (y - x^2)^2\\big)/20$ in $\\mathbb{R}^2$ with $q_0 = (0.0, 0.0)$.\n  - Diagonal mass $M = \\mathrm{diag}(1.0, 1.0)$.\n  - Step size $\\epsilon = 0.02$.\n  - Integration steps $L = 30$.\n  - Iterations $N = 1200$ with burn-in $B = 300$.\n  - Seed $s = 44$.\n\nRequired outputs:\n\nFor each case, run two samplers: one using the analytic gradient and one using the automatic differentiation gradient. Compute and report the following numeric results:\n\n- Case A:\n  1) Acceptance rate using the analytic gradient (float).\n  2) Acceptance rate using the automatic differentiation gradient (float).\n  3) Euclidean norm of the difference between the sample mean of the postâ€“burn-in chain (analytic gradient) and the true mean $\\mu$ (float).\n\n- Case B:\n  4) Acceptance rate using the analytic gradient (float).\n  5) Acceptance rate using the automatic differentiation gradient (float).\n  6) Absolute difference between the two acceptance rates in this case (float).\n\n- Case C:\n  7) Acceptance rate using the analytic gradient (float).\n  8) Acceptance rate using the automatic differentiation gradient (float).\n  9) Maximum absolute component-wise difference between the analytic gradient and the automatic differentiation gradient evaluated at $q^\\star = (1.0, 1.0)$ (float).\n\nFinal output format:\n\nYour program must produce a single line of output containing the $9$ results in the order listed above as a comma-separated list enclosed in square brackets, with each float rounded to six decimal places, for example:\n\"[0.712000,0.708667,0.054321,0.053750,0.057500,0.003750,0.635000,0.628333,0.000000]\".", "solution": "The task is to construct and implement a Hamiltonian Monte Carlo (HMC) sampler from first principles. This requires a synthesis of concepts from classical mechanics, numerical integration, and statistical simulation. The validity of the problem statement is confirmed; it is scientifically sound, well-posed, and all necessary parameters are provided. We proceed with the solution.\n\nThe core of HMC is to augment the target variable $q \\in \\mathbb{R}^d$, whose probability density is $\\pi(q)$, with an auxiliary momentum variable $p \\in \\mathbb{R}^d$. This creates a synthetic physical system. The probability density over the joint space $(q, p)$ is defined as $\\pi(q, p) = \\pi(q) \\pi(p)$. We define a potential energy $U(q) = -\\log \\pi(q)$ and a kinetic energy $K(p)$. A standard choice for the kinetic energy, as specified, is $K(p) = \\frac{1}{2} p^\\top M^{-1} p$, where $M$ is a symmetric positive-definite \"mass\" matrix, typically diagonal. This corresponds to the momentum distribution being a zero-mean Gaussian, $p \\sim \\mathcal{N}(0, M)$.\n\nThe total energy of this system is given by the Hamiltonian, $H(q, p) = U(q) + K(p)$. The evolution of the system in time is governed by Hamilton's equations:\n$$\n\\frac{dq}{dt} = \\frac{\\partial H}{\\partial p} = M^{-1} p\n$$\n$$\n\\frac{dp}{dt} = -\\frac{\\partial H}{\\partial q} = -\\nabla U(q)\n$$\nIn an ideal physical system, this evolution conserves the total energy $H(q,p)$ and is volume-preserving in the phase space $(q,p)$. HMC leverages this property. To generate a sample, we first draw a fresh momentum $p$ from its Gaussian distribution, then evolve the system $(q, p)$ for a fixed time $\\tau$ using a numerical integrator to obtain a proposal $(q', p')$. Due to numerical errors, $H(q', p')$ will not be exactly equal to $H(q, p)$, so a Metropolis-Hastings correction step is used to accept the proposal with probability $\\alpha = \\min(1, \\exp(-H(q', p') + H(q, p)))$. This ensures that the resulting Markov chain has the correct stationary distribution $\\pi(q)$.\n\nA crucial component is the numerical integrator. To maintain the desirable properties of the Hamiltonian dynamics as closely as possible, a symplectic, time-reversible integrator is required. The leapfrog (or StÃ¶rmer-Verlet) method is the standard choice. It is a second-order method that updates position and momentum in an interleaved, or \"leapfrogging,\" manner. For a single step of size $\\epsilon$, the updates are:\n1.  A half-step for momentum: $p(t + \\epsilon/2) = p(t) - (\\epsilon/2) \\nabla U(q(t))$\n2.  A full-step for position: $q(t + \\epsilon) = q(t) + \\epsilon M^{-1} p(t + \\epsilon/2)$\n3.  A final half-step for momentum: $p(t + \\epsilon) = p(t + \\epsilon/2) - (\\epsilon/2) \\nabla U(q(t + \\epsilon))$\n\nThis sequence is repeated $L$ times to integrate for a total time $\\tau = L\\epsilon$. This method is time-reversible and symplectic, which leads to excellent long-term energy conservation and high acceptance rates in the HMC algorithm.\n\nThe HMC algorithm requires the gradient of the log-posterior, $\\nabla U(q)$. This can be provided analytically or computed numerically. We will implement both. For the latter, we use forward-mode automatic differentiation (AD) with dual numbers. A dual number is an object of the form $z = (u, \\nabla u)$, where $u$ is the scalar value of a function and $\\nabla u \\in \\mathbb{R}^d$ is its gradient vector. Arithmetic operations are defined on these objects to propagate derivatives according to the rules of calculus:\n-   Addition: $(u, \\nabla u) + (v, \\nabla v) = (u+v, \\nabla u + \\nabla v)$\n-   Multiplication: $(u, \\nabla u) \\cdot (v, \\nabla v) = (uv, v \\nabla u + u \\nabla v)$\n-   Power Rule: $(u, \\nabla u)^n = (u^n, n u^{n-1} \\nabla u)$\n\nBy representing the input variables $q_i$ as dual numbers $(q_i, e_i)$, where $e_i$ is the $i$-th standard basis vector, and evaluating the function $U(q)$ using dual number arithmetic, the final result is a dual number $(U(q), \\nabla U(q))$. This provides both the function value and its complete gradient in a single forward pass, with machine precision.\n\nThe analytic gradients for the specified potential functions are as follows:\n-   For the Gaussian case, $U(q) = \\frac{1}{2} (q - \\mu)^\\top \\Sigma^{-1} (q - \\mu)$, the gradient is $\\nabla U(q) = \\Sigma^{-1} (q - \\mu)$.\n-   For the Rosenbrock-type case, $U(x,y) = \\frac{1}{20}((1 - x)^2 + 100 (y - x^2)^2)$, the gradient is:\n$$\n\\nabla U(q) = \\begin{pmatrix} \\frac{\\partial U}{\\partial x} \\\\ \\frac{\\partial U}{\\partial y} \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{10}(x - 1 - 200x(y - x^2)) \\\\ 10(y - x^2) \\end{pmatrix}\n$$\n\nThe implementation will consist of a `DualNumber` class, a general `hmc_sampler` function, and specific functions for each potential and gradient source. The sampler will iteratively resample momentum, integrate the trajectory using the leapfrog method, and apply the Metropolis correction, collecting samples after a burn-in period to compute the required statistics.", "answer": "```python\nimport numpy as np\n\nclass DualNumber:\n    \"\"\"\n    Implements a dual number for forward-mode automatic differentiation.\n    A dual number z = (value, grad) stores a value and its gradient vector.\n    \"\"\"\n    def __init__(self, value, grad):\n        self.value = float(value)\n        self.grad = np.asarray(grad, dtype=float)\n\n    def __repr__(self):\n        return f\"DualNumber(value={self.value}, grad={self.grad})\"\n\n    def __add__(self, other):\n        if isinstance(other, DualNumber):\n            return DualNumber(self.value + other.value, self.grad + other.grad)\n        return DualNumber(self.value + other, self.grad)\n\n    def __radd__(self, other):\n        return self.__add__(other)\n\n    def __sub__(self, other):\n        if isinstance(other, DualNumber):\n            return DualNumber(self.value - other.value, self.grad - other.grad)\n        # Case: self - constant\n        return DualNumber(self.value - other, self.grad)\n\n    def __rsub__(self, other):\n        # Case: constant - self\n        return DualNumber(other - self.value, -self.grad)\n\n    def __mul__(self, other):\n        if isinstance(other, DualNumber):\n            # (uv)' = u'v + uv'\n            return DualNumber(self.value * other.value, self.grad * other.value + self.value * other.grad)\n        # Case: self * constant\n        return DualNumber(self.value * other, self.grad * other)\n\n    def __rmul__(self, other):\n        return self.__mul__(other)\n\n    def __truediv__(self, other):\n        if isinstance(other, DualNumber):\n            # (u/v)' = (u'v - uv') / v^2\n            return DualNumber(self.value / other.value, (self.grad * other.value - self.value * other.grad) / (other.value**2))\n        # Case: self / constant\n        return DualNumber(self.value / other, self.grad / other)\n    \n    def __rtruediv__(self, other):\n        # Case: constant / self\n        return DualNumber(other / self.value, (-other * self.grad) / (self.value**2))\n\n    def __pow__(self, power):\n        if not isinstance(power, (int, float)):\n            raise TypeError(\"Power must be a numeric type.\")\n        # (u^n)' = n*u^(n-1)*u'\n        return DualNumber(self.value**power, power * (self.value**(power - 1)) * self.grad)\n    \n    def __neg__(self):\n        return DualNumber(-self.value, -self.grad)\n\ndef hmc_sampler(potential_grad_func, M, epsilon, L, N, B, q0, seed):\n    \"\"\"\n    A general Hamiltonian Monte Carlo sampler.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    q_current = np.copy(q0)\n    d = len(q0)\n    samples = []\n    accepted_count = 0\n    \n    M_inv_diag = 1.0 / M\n    M_sqrt_diag = np.sqrt(M)\n\n    for i in range(N):\n        # 1. Resample momentum\n        p_current = rng.normal(size=d) * M_sqrt_diag\n\n        q = np.copy(q_current)\n        p = np.copy(p_current)\n\n        # Calculate current Hamiltonian\n        U_current, grad_current = potential_grad_func(q)\n        K_current = 0.5 * np.sum(p**2 * M_inv_diag)\n        H_current = U_current + K_current\n\n        # 2. Symplectic integration (Leapfrog)\n        # Initial half-step for momentum\n        p -= 0.5 * epsilon * grad_current\n\n        for _ in range(L - 1):\n            # Full step for position\n            q += epsilon * p * M_inv_diag\n            # Full step for momentum (requires new gradient)\n            _, grad = potential_grad_func(q)\n            p -= epsilon * grad\n        \n        # Final full step for position\n        q += epsilon * p * M_inv_diag\n        # Final half-step for momentum\n        U_proposal, grad_proposal = potential_grad_func(q)\n        p -= 0.5 * epsilon * grad_proposal\n\n        # Proposal state is (q, p)\n        \n        # 3. Metropolis-Hastings correction\n        K_proposal = 0.5 * np.sum(p**2 * M_inv_diag)\n        H_proposal = U_proposal + K_proposal\n\n        # Acceptance probability\n        log_alpha = -H_proposal + H_current\n        \n        if np.log(rng.uniform())  log_alpha:\n            q_current = q\n            accepted_count += 1\n\n        if i = B:\n            samples.append(q_current)\n\n    acceptance_rate = accepted_count / N if N  0 else 0.0\n    return np.array(samples), acceptance_rate\n\ndef solve():\n    results = []\n\n    # --- Case A: Correlated Gaussian, happy path ---\n    d_A = 2\n    mu_A = np.array([1.0, -1.0])\n    Sigma_A = np.array([[1.0, 0.8], [0.8, 2.0]])\n    Sigma_A_inv = np.linalg.inv(Sigma_A)\n    M_A = np.array([1.0, 1.0])\n    q0_A = np.array([0.0, 0.0])\n    epsilon_A = 0.15\n    L_A = 20\n    N_A = 1500\n    B_A = 300\n    seed_A = 42\n\n    def potential_grad_gauss_analytic(q):\n        delta = q - mu_A\n        U = 0.5 * delta.T @ Sigma_A_inv @ delta\n        grad = Sigma_A_inv @ delta\n        return U, grad\n\n    def make_gauss_ad_provider(mu, Sigma_inv):\n        d = len(mu)\n        sig_inv_a, sig_inv_b, sig_inv_c = Sigma_inv[0, 0], Sigma_inv[0, 1], Sigma_inv[1, 1]\n\n        def potential_grad_func(q_vec):\n            q_duals = [DualNumber(q_vec[i], np.eye(d)[i]) for i in range(d)]\n            x, y = q_duals\n            mu_x, mu_y = mu\n            delta_x, delta_y = x - mu_x, y - mu_y\n            U_dual = 0.5 * (sig_inv_a * delta_x**2 + sig_inv_c * delta_y**2 + 2 * sig_inv_b * delta_x * delta_y)\n            return U_dual.value, U_dual.grad\n        return potential_grad_func\n\n    potential_grad_gauss_ad = make_gauss_ad_provider(mu_A, Sigma_A_inv)\n\n    # 1. Case A, analytic gradient\n    samples_A_analytic, acc_rate_A_analytic = hmc_sampler(\n        potential_grad_gauss_analytic, M_A, epsilon_A, L_A, N_A, B_A, q0_A, seed_A\n    )\n    results.append(acc_rate_A_analytic)\n\n    # 2. Case A, AD gradient\n    _, acc_rate_A_ad = hmc_sampler(\n        potential_grad_gauss_ad, M_A, epsilon_A, L_A, N_A, B_A, q0_A, seed_A\n    )\n    results.append(acc_rate_A_ad)\n\n    # 3. Case A, sample mean error\n    mean_A_analytic = np.mean(samples_A_analytic, axis=0)\n    error_A = np.linalg.norm(mean_A_analytic - mu_A)\n    results.append(error_A)\n\n    # --- Case B: Correlated Gaussian, large step size ---\n    epsilon_B = 0.9\n    L_B = 25\n    N_B = 800\n    B_B = 200\n    seed_B = 43\n\n    # 4. Case B, analytic gradient\n    _, acc_rate_B_analytic = hmc_sampler(\n        potential_grad_gauss_analytic, M_A, epsilon_B, L_B, N_B, B_B, q0_A, seed_B\n    )\n    results.append(acc_rate_B_analytic)\n\n    # 5. Case B, AD gradient\n    _, acc_rate_B_ad = hmc_sampler(\n        potential_grad_gauss_ad, M_A, epsilon_B, L_B, N_B, B_B, q0_A, seed_B\n    )\n    results.append(acc_rate_B_ad)\n\n    # 6. Case B, difference in acceptance rates\n    diff_acc_rate_B = abs(acc_rate_B_analytic - acc_rate_B_ad)\n    results.append(diff_acc_rate_B)\n\n    # --- Case C: Rosenbrock potential ---\n    M_C = np.array([1.0, 1.0])\n    q0_C = np.array([0.0, 0.0])\n    epsilon_C = 0.02\n    L_C = 30\n    N_C = 1200\n    B_C = 300\n    seed_C = 44\n\n    def potential_grad_rosen_analytic(q):\n        x, y = q\n        U = ((1 - x)**2 + 100 * (y - x**2)**2) / 20.0\n        grad_x = (x - 1 - 200 * x * (y - x**2)) / 10.0\n        grad_y = 10 * (y - x**2)\n        return U, np.array([grad_x, grad_y])\n    \n    def make_rosen_ad_provider():\n        d = 2\n        def potential_grad_func(q_vec):\n            q_duals = [DualNumber(q_vec[i], np.eye(d)[i]) for i in range(d)]\n            x, y = q_duals\n            U_dual = ((1 - x)**2 + 100 * (y - x**2)**2) / 20.0\n            return U_dual.value, U_dual.grad\n        return potential_grad_func\n\n    potential_grad_rosen_ad = make_rosen_ad_provider()\n\n    # 7. Case C, analytic gradient\n    _, acc_rate_C_analytic = hmc_sampler(\n        potential_grad_rosen_analytic, M_C, epsilon_C, L_C, N_C, B_C, q0_C, seed_C\n    )\n    results.append(acc_rate_C_analytic)\n\n    # 8. Case C, AD gradient\n    _, acc_rate_C_ad = hmc_sampler(\n        potential_grad_rosen_ad, M_C, epsilon_C, L_C, N_C, B_C, q0_C, seed_C\n    )\n    results.append(acc_rate_C_ad)\n\n    # 9. Case C, gradient difference at q*=(1,1)\n    q_star = np.array([1.0, 1.0])\n    _, grad_C_analytic = potential_grad_rosen_analytic(q_star)\n    _, grad_C_ad = potential_grad_rosen_ad(q_star)\n    grad_diff_C = np.max(np.abs(grad_C_analytic - grad_C_ad))\n    results.append(grad_diff_C)\n\n    # Final output formatting\n    output_str = \",\".join([f\"{r:.6f}\" for r in results])\n    print(f\"[{output_str}]\")\n\nsolve()\n```", "id": "2399583"}, {"introduction": "A well-built sampler must be robust, but its performance can be sensitive to the features of the energy landscape. This final hands-on practice [@problem_id:2399525] tasks you with stress-testing an HMC implementation on a potential with a near-singularity, a scenario common in physical models like celestial mechanics or electrostatics. By analyzing metrics like acceptance rate and energy conservation error, you will gain practical insight into tuning HMC and diagnosing its behavior in numerically challenging situations.", "problem": "You are to design and implement a Hamiltonian Monte Carlo sampler to study the effect of a near-singular potential on numerical stability and acceptance behavior. Work in one spatial dimension with mass equal to $1$, and base your derivation strictly on Hamilton's equations and the Metropolisâ€“Hastings rule, starting from first principles.\n\nConsider a regularized and confining version of the attractive Coulomb potential in one dimension. Define the potential energy as\n$$\nU(q;a) \\equiv \\frac{1}{2}\\,q^2 - \\frac{c}{\\sqrt{q^2 + a^2}},\n$$\nwith $c = 1$. The parameter $a  0$ softens the singularity at $q=0$ while the quadratic term ensures normalizability of the target distribution. The target probability density is proportional to $\\exp\\!\\left(-U(q;a)\\right)$. Use the kinetic energy\n$$\nK(p) \\equiv \\frac{1}{2} p^2,\n$$\nso that the Hamiltonian is $H(q,p) \\equiv U(q;a) + K(p)$.\n\nYour tasks are:\n- Derive a time-reversible, volume-preserving integrator for Hamilton's equations associated with $H(q,p)$, consistent with the properties required by Hamiltonian Monte Carlo. Base your construction on splitting principles that follow from Hamilton's equations and avoid any non-symplectic discretization.\n- Compute the force term by differentiating $U(q;a)$ with respect to $q$.\n- Implement a Hamiltonian Monte Carlo kernel that, given a current position $q$, draws a fresh momentum $p$ from the Gaussian with zero mean and unit variance, integrates the Hamiltonian dynamics for a given step size $\\epsilon$ and number of steps $L$, and then applies a Metropolisâ€“Hastings accept/reject step using the exact Hamiltonian $H(q,p)$ at the start and end of the trajectory.\n- For each proposal, record whether the numerical trajectory approached the softened singularity by tracking the minimum absolute value of $q$ visited along the discrete path and comparing it to a fixed threshold $\\delta$.\n- For each parameter set, after a fixed number $N$ of proposals, report three metrics: the acceptance rate, the mean absolute Hamiltonian error (the average of $\\lvert \\Delta H \\rvert$ over all proposals), and the fraction of proposals whose numerical trajectory had $\\min \\lvert q \\rvert  \\delta$.\n\nConstraints and fixed choices that must be used:\n- Use $c = 1$ and mass equal to $1$.\n- Use $N = 300$ Hamiltonian Monte Carlo proposals for each parameter set.\n- Use the threshold $\\delta = 10^{-2}$.\n- Use a fixed random number generator seed of $12345$ for reproducibility. If you choose to vary the seed across test cases, you must do so deterministically in a documented way that depends only on the test case index and the fixed base seed.\n- For numerical stability in the acceptance step, compare in log space by using the fact that the acceptance condition is equivalent to comparing $\\log u$ to $- \\Delta H$, where $u$ is drawn from the uniform distribution on $(0,1)$.\n- The initial position $q_0$ for each test case is specified in the test suite below. Always start the Markov chain at this $q_0$ and run for exactly $N$ proposals, without burn-in removal.\n\nTest suite of parameter values:\n- Case $1$: $\\epsilon = 0.05$, $L = 50$, $a = 0.1$, $q_0 = 1.0$.\n- Case $2$: $\\epsilon = 0.2$, $L = 50$, $a = 0.1$, $q_0 = 1.0$.\n- Case $3$: $\\epsilon = 0.05$, $L = 50$, $a = 0.001$, $q_0 = 0.1$.\n- Case $4$: $\\epsilon = 0.2$, $L = 50$, $a = 0.001$, $q_0 = 0.1$.\n\nDefinition of metrics:\n- Acceptance rate is the fraction of accepted proposals out of $N$, expressed as a decimal in $[0,1]$.\n- Mean absolute Hamiltonian error is the arithmetic mean over $N$ proposals of $\\lvert \\Delta H \\rvert$, where $\\Delta H \\equiv H(q^{\\star},p^{\\star}) - H(q,p)$ is the change in Hamiltonian between the end and start of the numerical trajectory, regardless of acceptance.\n- Near-singularity fraction is the fraction of proposals whose numerical trajectory satisfies $\\min \\lvert q \\rvert  \\delta$ for at least one discrete integration step.\n\nRequired final output format:\n- Your program should produce a single line of output containing the results for the four test cases as a comma-separated list enclosed in square brackets. Each test case result must itself be a list of three decimals in the order $[$acceptance rate$, $mean absolute Hamiltonian error$, $near-singularity fraction$]$, rounded to exactly $6$ decimal places. For example, your program must print something of the form\n$$\n\\big[\\,[a_1,b_1,c_1],[a_2,b_2,c_2],[a_3,b_3,c_3],[a_4,b_4,c_4]\\,\\big],\n$$\nwhere each $a_i$, $b_i$, $c_i$ is a decimal rounded to $6$ places. No additional text should be printed.", "solution": "The problem statement has been evaluated and is determined to be valid. It is a well-posed, scientifically grounded problem in the field of computational physics, containing all necessary information for a unique, deterministic solution. I shall proceed with the derivation and implementation.\n\nThe primary task is to construct a Hamiltonian Monte Carlo (HMC) sampler for a particle of unit mass ($m=1$) in one dimension, governed by a specific potential energy function. The objective is to analyze the sampler's performance under varying parameters that control the numerical integration step size and the proximity to a potential singularity.\n\nThe Hamiltonian $H(q,p)$ of the system is the sum of kinetic energy $K(p)$ and potential energy $U(q;a)$:\n$$\nH(q,p) = K(p) + U(q;a)\n$$\nGiven $m=1$, the kinetic energy is:\n$$\nK(p) = \\frac{1}{2} p^2\n$$\nThe potential energy is given as:\n$$\nU(q;a) = \\frac{1}{2}q^2 - \\frac{c}{\\sqrt{q^2 + a^2}}\n$$\nwith the constant $c=1$. The parameter $a  0$ regularizes the attractive Coulomb-like term, preventing a true singularity at $q=0$. The quadratic term $\\frac{1}{2}q^2$ ensures that the target probability density, $\\pi(q) \\propto \\exp(-U(q;a))$, is normalizable.\n\nHamilton's equations of motion, which describe the evolution of the system in phase space $(q,p)$, are:\n$$\n\\frac{dq}{dt} = \\frac{\\partial H}{\\partial p} = p\n$$\n$$\n\\frac{dp}{dt} = -\\frac{\\partial H}{\\partial q} = -\\frac{\\partial U}{\\partial q}\n$$\nThe term $-\\frac{\\partial U}{\\partial q}$ is the force, $F(q)$. We compute this derivative:\n$$\n\\frac{\\partial U}{\\partial q} = \\frac{\\partial}{\\partial q} \\left(\\frac{1}{2}q^2 - (q^2 + a^2)^{-1/2}\\right) = q - \\left(-\\frac{1}{2}\\right)(q^2 + a^2)^{-3/2}(2q) = q + \\frac{q}{(q^2 + a^2)^{3/2}}\n$$\nTherefore, the force is:\n$$\nF(q;a) = -q - \\frac{q}{(q^2 + a^2)^{3/2}}\n$$\n\nTo simulate these dynamics numerically, a symplectic integrator is required. The leapfrog (or StÃ¶rmer-Verlet) method is a standard choice as it is both time-reversible and volume-preserving, properties which are essential for the correctness of the HMC algorithm. The method is derived from a second-order Trotter-Suzuki decomposition of the Hamiltonian evolution operator, which splits the evolution into a sequence of steps under the kinetic and potential parts of the Hamiltonian. A single leapfrog step of size $\\epsilon$ updates the position $q$ and momentum $p$ as follows:\n$1$. A half-step update for the momentum:\n$$\np(t + \\epsilon/2) = p(t) + F(q(t);a) \\cdot \\frac{\\epsilon}{2}\n$$\n$2$. A full-step update for the position:\n$$\nq(t + \\epsilon) = q(t) + p(t + \\epsilon/2) \\cdot \\epsilon\n$$\n$3$. A second half-step update for the momentum:\n$$\np(t + \\epsilon) = p(t + \\epsilon/2) + F(q(t+\\epsilon);a) \\cdot \\frac{\\epsilon}{2}\n$$\nTo generate a trajectory of length $L\\epsilon$, this process is iterated $L$ times. For computational efficiency, the full momentum updates from adjacent steps are combined.\n\nThe HMC algorithm proceeds as follows for each proposal, starting from a given position $q_{current}$:\n$1$. **Momentum Sampling**: A new momentum $p_{current}$ is drawn from its conditional distribution, which is a standard Gaussian since $K(p) = p^2/2$, i.e., $p_{current} \\sim \\mathcal{N}(0, 1)$.\n$2$. **Trajectory Integration**: Starting from $(q_{current}, p_{current})$, the system is evolved for $L$ leapfrog steps with step size $\\epsilon$. This produces a proposal state $(q_{proposal}, p_{proposal})$. During this integration, we must track the minimum absolute value of the position, $\\min_i |q_i|$, along the discrete path $q_0, q_1, \\ldots, q_L$.\n$3$. **Metropolis-Hastings Acceptance**: The proposal is accepted or rejected based on the change in the Hamiltonian, $\\Delta H = H(q_{proposal}, p_{proposal}) - H(q_{current}, p_{current})$. The acceptance probability is $\\alpha = \\min(1, \\exp(-\\Delta H))$. This simple form is valid because the leapfrog integrator is volume-preserving and time-reversible. The time-reversibility property implies that if one were to start from $(q_{proposal}, -p_{proposal})$ and integrate backward in time, one would recover $(q_{current}, -p_{current})$, making the proposal symmetric in the appropriate sense. A random variate $u \\sim U(0,1)$ is drawn, and the proposal is accepted if $u  \\exp(-\\Delta H)$, or equivalently, $\\log u  -\\Delta H$. If accepted, the new state is $q_{proposal}$; otherwise, the state remains $q_{current}$.\n\nFor each of the $N=300$ proposals per test case, we will record three quantities: ($1$) whether the proposal was accepted, ($2$) the absolute Hamiltonian error $|\\Delta H|$, and ($3$) whether the trajectory's minimum absolute position fell below the threshold $\\delta = 10^{-2}$. The final reported metrics will be the averages of these quantities over all $N$ proposals. The implementation will use a fixed random seed for reproducibility as specified.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and runs a Hamiltonian Monte Carlo simulation for a 1D particle\n    with a regularized Coulomb-like potential, as specified in the problem statement.\n    \"\"\"\n\n    # --- Fixed Parameters ---\n    C_CONST = 1.0\n    N_PROPOSALS = 300\n    DELTA_THRESHOLD = 1e-2\n    BASE_SEED = 12345\n\n    # --- Test Suite ---\n    test_cases = [\n        # (epsilon, L, a, q0)\n        (0.05, 50, 0.1, 1.0),\n        (0.2, 50, 0.1, 1.0),\n        (0.05, 50, 0.001, 0.1),\n        (0.2, 50, 0.001, 0.1),\n    ]\n\n    all_results = []\n\n    # --- Physics and HMC Functions ---\n    def potential_energy(q, a):\n        \"\"\"Computes the potential energy U(q;a).\"\"\"\n        return 0.5 * q**2 - C_CONST / np.sqrt(q**2 + a**2)\n\n    def kinetic_energy(p):\n        \"\"\"Computes the kinetic energy K(p).\"\"\"\n        return 0.5 * p**2\n\n    def hamiltonian(q, p, a):\n        \"\"\"Computes the total energy H(q,p).\"\"\"\n        return potential_energy(q, a) + kinetic_energy(p)\n\n    def force(q, a):\n        \"\"\"Computes the force F = -dU/dq.\"\"\"\n        return -q - (C_CONST * q) / (q**2 + a**2)**(3/2)\n\n    for i, case in enumerate(test_cases):\n        epsilon, L, a, q0 = case\n        \n        # Use a deterministic seed for each case to ensure reproducibility\n        rng = np.random.default_rng(BASE_SEED + i)\n\n        q_current = q0\n        \n        accepted_count = 0\n        total_abs_h_error = 0.0\n        near_singularity_count = 0\n        \n        for _ in range(N_PROPOSALS):\n            # 1. Momentum Resampling\n            p_current = rng.normal(0, 1)\n\n            # Store initial state for MH step\n            q_initial, p_initial = q_current, p_current\n            h_initial = hamiltonian(q_initial, p_initial, a)\n\n            # --- 2. Trajectory Integration (Leapfrog) ---\n            q_prop, p_prop = q_initial, p_initial\n            \n            # Track if trajectory approaches singularity\n            min_abs_q = abs(q_prop)\n\n            # Initial half-step for momentum\n            p_prop += 0.5 * epsilon * force(q_prop, a)\n\n            # L-1 full steps for position and momentum\n            for _ in range(L - 1):\n                q_prop += epsilon * p_prop\n                min_abs_q = min(min_abs_q, abs(q_prop))\n                p_prop += epsilon * force(q_prop, a)\n            \n            # Final full step for position\n            q_prop += epsilon * p_prop\n            min_abs_q = min(min_abs_q, abs(q_prop))\n\n            # Final half-step for momentum\n            p_prop += 0.5 * epsilon * force(q_prop, a)\n            \n            # Record if trajectory passed the threshold\n            if min_abs_q  DELTA_THRESHOLD:\n                near_singularity_count += 1\n\n            # --- 3. Metropolis-Hastings Acceptance Step ---\n            h_proposal = hamiltonian(q_prop, p_prop, a)\n            delta_h = h_proposal - h_initial\n            total_abs_h_error += abs(delta_h)\n\n            # Acceptance check in log space for numerical stability\n            if np.log(rng.uniform(0, 1))  -delta_h:\n                q_current = q_prop\n                accepted_count += 1\n            # If rejected, q_current remains unchanged\n\n        # Calculate metrics for this test case\n        acceptance_rate = accepted_count / N_PROPOSALS\n        mean_abs_h_error = total_abs_h_error / N_PROPOSALS\n        near_singularity_fraction = near_singularity_count / N_PROPOSALS\n        \n        all_results.append([\n            acceptance_rate, \n            mean_abs_h_error, \n            near_singularity_fraction\n        ])\n\n    # Format the final output string exactly as required\n    outer_parts = []\n    for res in all_results:\n        inner_str = f\"[{res[0]:.6f},{res[1]:.6f},{res[2]:.6f}]\"\n        outer_parts.append(inner_str)\n    final_output_str = f\"[{','.join(outer_parts)}]\"\n\n    print(final_output_str)\n\nsolve()\n```", "id": "2399525"}]}