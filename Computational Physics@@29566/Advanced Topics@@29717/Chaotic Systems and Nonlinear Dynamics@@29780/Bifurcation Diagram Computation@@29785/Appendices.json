{"hands_on_practices": [{"introduction": "Bifurcations are critical points where a system's behavior qualitatively changes, and near these points, many systems exhibit universal scaling laws. This exercise explores the supercritical pitchfork bifurcation, where the distance $d(r)$ between newly formed stable states scales as a power law, $d(r) \\propto (r-r_c)^{\\beta}$, near the critical parameter value $r_c$. By numerically generating data and performing a log-log analysis, you will determine the critical exponent $\\beta$, a powerful technique used throughout physics to characterize phase transitions and other critical phenomena. [@problem_id:2376529]", "problem": "Consider a one-dimensional system undergoing a supercritical pitchfork bifurcation controlled by a real parameter $r$. Near the critical value $r_c$, the system can be represented in normal form by either a continuous-time flow $\\frac{dx}{dt} = (r-r_c)x - a x^3$ or a discrete-time iterated map $x_{n+1} = r x_n - a x_n^3$, where $a>0$ is a real constant. In both cases, fixed points $x^\\star$ are defined as real values that satisfy the fixed-point equation\n$$\n(r - r_c) x^\\star - a (x^\\star)^3 = 0.\n$$\nFor $r>r_c$, the symmetry-broken branches contain two nonzero fixed points $x_+(r)>0$ and $x_-(r)<0$. Define the distance between these new fixed points as\n$$\nd(r) = \\lvert x_+(r) - x_-(r) \\rvert.\n$$\nYour task is to numerically determine the critical exponent $\\beta$ that characterizes how $d(r)$ scales with the distance to criticality, according to\n$$\nd(r) \\propto (r - r_c)^\\beta \\quad \\text{as } r \\downarrow r_c.\n$$\nFor each test case below, use the specified list of parameter values $r = r_c + \\varepsilon$ with given $\\varepsilon>0$, compute $d(r)$ for all listed $r$, and from these data produce a single numerical estimate of $\\beta$ for that test case. No physical units are involved.\n\nTest suite (each test case consists of $(r_c, a, \\{\\varepsilon\\})$):\n- Test case 1: $r_c = 0$, $a = 1$, and $\\varepsilon \\in \\{10^{-5}, 3\\cdot 10^{-5}, 10^{-4}, 3\\cdot 10^{-4}, 10^{-3}, 3\\cdot 10^{-3}, 10^{-2}\\}$.\n- Test case 2: $r_c = 0$, $a = 2$, and $\\varepsilon \\in \\{10^{-5}, 3\\cdot 10^{-5}, 10^{-4}, 3\\cdot 10^{-4}, 10^{-3}, 3\\cdot 10^{-3}, 10^{-2}\\}$.\n- Test case 3: $r_c = 1$, $a = 1$, and $\\varepsilon \\in \\{10^{-5}, 3\\cdot 10^{-5}, 10^{-4}, 3\\cdot 10^{-4}, 10^{-3}, 3\\cdot 10^{-3}, 10^{-2}\\}$.\n- Test case 4: $r_c = -0.5$, $a = 3$, and $\\varepsilon \\in \\{10^{-5}, 3\\cdot 10^{-5}, 10^{-4}, 3\\cdot 10^{-4}, 10^{-3}, 3\\cdot 10^{-3}, 10^{-2}\\}$.\n\nFor each test case:\n1. For each listed $\\varepsilon$, set $r = r_c + \\varepsilon$.\n2. Using only the fixed-point definition above, find the two nonzero fixed points $x_+(r)$ and $x_-(r)$ and compute $d(r)$.\n3. From the set $\\{(\\varepsilon, d(r_c+\\varepsilon))\\}$, determine a single numerical estimate of $\\beta$ for the test case.\n\nYour program should produce a single line of output containing the four estimates of $\\beta$ (one per test case, in the order given above), rounded to three decimal places, as a comma-separated list enclosed in square brackets, for example $[0.500,0.500,0.500,0.500]$.", "solution": "The problem statement is first subjected to a mandatory validation procedure.\n\n**Step 1: Extracted Givens**\n- **System Definition**: A one-dimensional system exhibiting a supercritical pitchfork bifurcation is described by a control parameter $r$, a critical value $r_c$, and a positive real constant $a > 0$.\n- **Fixed-Point Equation**: The fixed points $x^\\star$ of the system are defined by the equation $(r - r_c) x^\\star - a (x^\\star)^3 = 0$.\n- **Symmetry-Broken Fixed Points**: For $r > r_c$, two nonzero fixed points exist, denoted $x_+(r) > 0$ and $x_-(r) < 0$.\n- **Distance Definition**: The distance between these points is $d(r) = \\lvert x_+(r) - x_-(r) \\rvert$.\n- **Scaling Law**: The distance $d(r)$ is asserted to follow the scaling law $d(r) \\propto (r - r_c)^\\beta$ as $r$ approaches $r_c$ from above ($r \\downarrow r_c$).\n- **Objective**: Numerically estimate the critical exponent $\\beta$ for several test cases.\n- **Procedure**: For each test case $(r_c, a, \\{\\varepsilon\\})$, compute the set of points $\\{(\\varepsilon, d(r_c+\\varepsilon))\\}$ and from these, produce a single estimate for $\\beta$.\n- **Test Cases**:\n    - Test Case $1$: ($r_c = 0$, $a = 1$, $\\varepsilon \\in \\{10^{-5}, 3\\cdot 10^{-5}, 10^{-4}, 3\\cdot 10^{-4}, 10^{-3}, 3\\cdot 10^{-3}, 10^{-2}\\}$).\n    - Test Case $2$: ($r_c = 0$, $a = 2$, $\\varepsilon \\in \\{10^{-5}, 3\\cdot 10^{-5}, 10^{-4}, 3\\cdot 10^{-4}, 10^{-3}, 3\\cdot 10^{-3}, 10^{-2}\\}$).\n    - Test Case $3$: ($r_c = 1$, $a = 1$, $\\varepsilon \\in \\{10^{-5}, 3\\cdot 10^{-5}, 10^{-4}, 3\\cdot 10^{-4}, 10^{-3}, 3\\cdot 10^{-3}, 10^{-2}\\}$).\n    - Test Case $4$: ($r_c = -0.5$, $a = 3$, $\\varepsilon \\in \\{10^{-5}, 3\\cdot 10^{-5}, 10^{-4}, 3\\cdot 10^{-4}, 10^{-3}, 3\\cdot 10^{-3}, 10^{-2}\\}$).\n\n**Step 2: Validation**\n- **Scientific Grounding**: The problem is based on the normal form for a supercritical pitchfork bifurcation, a canonical concept in the theory of dynamical systems and chaos. The use of a critical exponent $\\beta$ to characterize the scaling of the order parameter (here, represented by $d(r)$) near a continuous phase transition is standard in statistical mechanics and nonlinear physics. The problem is fundamentally sound.\n- **Well-Posedness**: The problem is well-posed. The fixed-point equation is a simple algebraic equation, and its solution is straightforward. The task of estimating an exponent from a power-law relationship is a standard numerical exercise. All necessary parameters and data points are provided. The problem is self-contained and unambiguous. A point of potential confusion regarding the different fixed-point equations for flows versus maps is rendered moot by the explicit provision of the equation to be used: $(r - r_c) x^\\star - a (x^\\star)^3 = 0$.\n\n**Step 3: Verdict**\nThe problem is valid. It is a well-defined exercise in computational physics that tests both analytical understanding and numerical proficiency. We may proceed with the solution.\n\n**Solution Derivation**\n\nThe solution requires two stages: first, an analytical treatment to understand the theoretical basis, and second, the design of a numerical procedure to fulfill the problem's specific request.\n\n**1. Analytical Solution**\n\nThe core of the problem lies in the fixed-point equation:\n$$\n(r - r_c) x^\\star - a (x^\\star)^3 = 0\n$$\nThis equation can be factored as:\n$$\nx^\\star \\left( (r - r_c) - a (x^\\star)^2 \\right) = 0\n$$\nThis equation yields three real solutions for the fixed points, provided that $r > r_c$ (given $a > 0$):\n$1$. The trivial fixed point, which exists for all $r$: $x^\\star_0 = 0$.\n$2$. Two non-trivial fixed points arising from the second factor: $(r - r_c) - a (x^\\star)^2 = 0$. This gives $(x^\\star)^2 = \\frac{r - r_c}{a}$.\nThe solutions are:\n$$\nx^\\star = \\pm \\sqrt{\\frac{r - r_c}{a}}\n$$\nThese are the symmetry-broken fixed points, which we identify as $x_+(r)$ and $x_-(r)$:\n$$\nx_+(r) = \\sqrt{\\frac{r - r_c}{a}} \\quad \\text{and} \\quad x_-(r) = -\\sqrt{\\frac{r - r_c}{a}}\n$$\nThe distance $d(r)$ between these two points is then:\n$$\nd(r) = |x_+(r) - x_-(r)| = \\left| \\sqrt{\\frac{r - r_c}{a}} - \\left(-\\sqrt{\\frac{r - r_c}{a}}\\right) \\right| = \\left| 2 \\sqrt{\\frac{r - r_c}{a}} \\right|\n$$\nSince $r > r_c$ and $a > 0$, the expression is real and positive. Thus:\n$$\nd(r) = \\frac{2}{\\sqrt{a}} (r - r_c)^{1/2}\n$$\nComparing this analytical result to the required scaling form, $d(r) \\propto (r - r_c)^\\beta$, we can immediately and exactly identify the critical exponent:\n$$\n\\beta = \\frac{1}{2}\n$$\nThis theoretical value is universal for any supercritical pitchfork bifurcation described by this normal form, independent of the specific values of $r_c$ and $a$.\n\n**2. Numerical Estimation Methodology**\n\nThe problem demands a numerical estimation of $\\beta$. The standard, robust method for determining an exponent from a power-law relationship is to perform a linear regression on the logarithmic form of the equation.\nLet the scaling relation be $d = C \\cdot \\varepsilon^\\beta$, where $\\varepsilon = r - r_c$ and $C$ is a constant of proportionality (here, $C=2/\\sqrt{a}$). Taking the natural logarithm of both sides linearizes the equation:\n$$\n\\ln(d) = \\ln(C \\cdot \\varepsilon^\\beta) = \\ln(C) + \\beta \\ln(\\varepsilon)\n$$\nThis is an equation of a straight line, $Y = mX + b$, with:\n- Dependent variable: $Y = \\ln(d(\\varepsilon))$\n- Independent variable: $X = \\ln(\\varepsilon)$\n- Slope: $m = \\beta$\n- Intercept: $b = \\ln(C)$\n\nFor each test case, we are given a set of values for $\\varepsilon$. We first compute the corresponding values of $d(r_c + \\varepsilon) = 2\\sqrt{\\varepsilon/a}$. This provides a set of data points $(\\varepsilon_i, d_i)$. We then transform these points to their logarithmic counterparts $(X_i, Y_i) = (\\ln(\\varepsilon_i), \\ln(d_i))$.\n\nThe slope $\\beta$ of the best-fit line through these logarithmic data points is found using the method of least squares (linear regression). For a set of $N$ data points $(X_i, Y_i)$, the formula for the slope is:\n$$\n\\beta = \\frac{N \\sum(X_i Y_i) - (\\sum X_i)(\\sum Y_i)}{N \\sum(X_i^2) - (\\sum X_i)^2}\n$$\nThe implementation will compute this value for each of the $4$ test cases. Since the data points are generated from the exact analytical formula, there is no experimental noise or higher-order term contamination. Therefore, the numerical fit must yield a value for $\\beta$ that is equal to $0.5$ to within the limits of floating-point precision. The computation for all test cases should yield the same exponent, confirming the universality of $\\beta$. The final result will be rounded to $3$ decimal places as requested.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import stats\n\ndef solve():\n    \"\"\"\n    Solves the problem of numerically estimating the critical exponent beta\n    for a supercritical pitchfork bifurcation for four different test cases.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    # Each case is a tuple: (r_c, a, list_of_epsilons)\n    test_cases = [\n        (0.0, 1.0, [1e-5, 3e-5, 1e-4, 3e-4, 1e-3, 3e-3, 1e-2]),\n        (0.0, 2.0, [1e-5, 3e-5, 1e-4, 3e-4, 1e-3, 3e-3, 1e-2]),\n        (1.0, 1.0, [1e-5, 3e-5, 1e-4, 3e-4, 1e-3, 3e-3, 1e-2]),\n        (-0.5, 3.0, [1e-5, 3e-5, 1e-4, 3e-4, 1e-3, 3e-3, 1e-2]),\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        r_c, a, epsilons = case\n        \n        # Convert list of epsilons to a numpy array for vectorized operations\n        eps_array = np.array(epsilons)\n        \n        # 1. Calculate d(r) for each epsilon\n        # The fixed-point equation is (r - r_c)x - ax^3 = 0.\n        # Let epsilon = r - r_c. Then epsilon*x - a*x^3 = 0.\n        # x(epsilon - ax^2) = 0.\n        # Non-zero solutions are x = +/- sqrt(epsilon/a).\n        # x_plus = sqrt(epsilon/a), x_minus = -sqrt(epsilon/a).\n        # d = |x_plus - x_minus| = 2 * sqrt(epsilon/a).\n        d_values = 2 * np.sqrt(eps_array / a)\n        \n        # 2. Prepare data for linear regression\n        # The scaling law is d = C * epsilon^beta.\n        # Taking the log: ln(d) = ln(C) + beta * ln(epsilon).\n        # This is a linear relationship. We fit ln(d) vs ln(epsilon).\n        # The slope of the line is the exponent beta.\n        log_eps = np.log(eps_array)\n        log_d = np.log(d_values)\n        \n        # 3. Perform linear regression to find the slope (beta)\n        # scipy.stats.linregress returns (slope, intercept, r-value, p-value, stderr)\n        lin_reg_result = stats.linregress(log_eps, log_d)\n        beta_estimate = lin_reg_result.slope\n        \n        results.append(beta_estimate)\n\n    # Final print statement in the exact required format.\n    # Results are rounded to three decimal places.\n    print(f\"[{','.join([f'{res:.3f}' for res in results])}]\")\n\nsolve()\n\n```", "id": "2376529"}, {"introduction": "A bifurcation diagram provides a global map of a system's long-term behavior across a range of a control parameter, often revealing intricate, self-similar structures. To investigate these details, one must computationally \"zoom in\" by re-evaluating the system on a much finer grid of parameter values. This practice guides you through the process of generating data for the logistic map's bifurcation diagram and simulates a 'zoom' by refining the parameter grid, allowing you to see how computed diagnostics converge as resolution increases. [@problem_id:2376483]", "problem": "You are to compute scalar diagnostics of a bifurcation diagram for the one-dimensional logistic map by recomputing it at higher parameter resolution, which models a \"zoom-in\" operation on a region of the control-parameter axis. Consider the discrete-time dynamical system defined by the logistic map\n$$\nx_{n+1} = r\\,x_n\\,(1 - x_n),\n$$\nwhere $r$ is a real-valued control parameter and $x_n \\in [0,1]$ for iteration index $n \\in \\mathbb{N}$. For a given closed interval of control parameters $[r_{\\min}, r_{\\max}]$, a uniform grid of $N_r$ parameter samples is defined as\n$$\n\\{r_i\\}_{i=1}^{N_r},\\quad r_i = r_{\\min} + \\frac{i-1}{N_r-1}\\,(r_{\\max} - r_{\\min}),\n$$\nwith $N_r \\in \\mathbb{N}$ and $N_r \\ge 1$. For each parameter value $r_i$, the system is iterated from an initial condition $x_0 \\in (0,1)$ for a total of $N_{\\text{trans}} + K$ steps, where $N_{\\text{trans}} \\in \\mathbb{N}$ denotes the number of transient iterates to discard and $K \\in \\mathbb{N}$ denotes the number of subsequent iterates to retain. Define, for each $r_i$, the retained-iterate mean\n$$\n\\overline{x}(r_i) = \\frac{1}{K}\\sum_{j=1}^{K} x_{N_{\\text{trans}}+j}(r_i),\n$$\nwhere $x_{n}(r_i)$ denotes the sequence generated at parameter $r_i$. Define the coarse-resolution diagram mean as\n$$\nM_{\\text{coarse}} = \\frac{1}{N_r}\\sum_{i=1}^{N_r} \\overline{x}(r_i).\n$$\nTo model a \"zoom-in\" recomputation with higher parameter resolution, define a refined grid over the same interval $[r_{\\min}, r_{\\max}]$ with\n$$\nN_r^{(\\text{refined})} = q\\,N_r,\n$$\nwhere $q \\in \\mathbb{N}$ is a refinement factor. Using the same $x_0$, $N_{\\text{trans}}$, and $K$, compute\n$$\nM_{\\text{refined}} = \\frac{1}{N_r^{(\\text{refined})}} \\sum_{i=1}^{N_r^{(\\text{refined})}} \\overline{x}(r_i^{(\\text{refined})})\n$$\non the refined grid $\\{r_i^{(\\text{refined})}\\}$. For each case, report the absolute difference\n$$\nD = \\left| M_{\\text{refined}} - M_{\\text{coarse}} \\right|.\n$$\n\nYour program must compute $D$ for each of the following test cases, each provided as an ordered septuple $(r_{\\min}, r_{\\max}, N_r, q, N_{\\text{trans}}, K, x_0)$:\n\n- Case $1$: $(2.8, 3.0, 200, 4, 800, 200, 0.5)$.\n- Case $2$: $(3.569, 3.571, 400, 5, 1200, 256, 0.5)$.\n- Case $3$: $(3.7, 3.8, 500, 3, 1000, 256, 0.5)$.\n- Case $4$: $(3.99, 4.0, 600, 2, 1000, 256, 0.5)$.\n- Case $5$: $(3.5, 3.5, 300, 1, 500, 128, 0.123456)$.\n\nAll real numbers above are dimensionless. Angles are not involved. For each case, compute $D$ as a real number. Your program should produce a single line of output containing these five values as a comma-separated list enclosed in square brackets, in the order of the cases listed above, for example\n$[\\text{result}_1,\\text{result}_2,\\text{result}_3,\\text{result}_4,\\text{result}_5]$.", "solution": "The problem statement has been critically examined and is determined to be valid. It is scientifically grounded in the established theory of discrete dynamical systems, specifically the logistic map, which is a canonical model in the study of chaos. The problem is well-posed, with all necessary parameters, definitions, and functions provided unambiguously. The requested computation is objective and computationally feasible. No scientific, logical, or structural flaws were identified.\n\nThe task is to compute a scalar diagnostic, $D$, which represents the change in the mean of a bifurcation diagram segment when recomputed at a higher parameter resolution. The core of the problem lies in the numerical simulation of the logistic map, given by the recurrence relation\n$$\nx_{n+1} = r\\,x_n\\,(1 - x_n),\n$$\nfor a range of control parameter values $r$.\n\nThe solution is architected around a systematic, numerically robust algorithm. The overall procedure for each test case $(r_{\\min}, r_{\\max}, N_r, q, N_{\\text{trans}}, K, x_0)$ is to compute two quantities, $M_{\\text{coarse}}$ and $M_{\\text{refined}}$, and find their absolute difference.\n\nThe computation of these mean values, $M$, is encapsulated in a dedicated function. This function takes as input the parameter interval $[r_{\\min}, r_{\\max}]$, the number of grid points (either $N_r$ or $N_r^{(\\text{refined})}$), the iteration counts $N_{\\text{trans}}$ and $K$, and the initial condition $x_0$.\n\nThe algorithm proceeds as follows:\n$1$. A uniform grid of parameter values, $\\{r_i\\}$, is constructed over the interval $[r_{\\min}, r_{\\max}]$. For a grid of size $N$, the points are given by $r_i = r_{\\min} + \\frac{i-1}{N-1}\\,(r_{\\max} - r_{\\min})$ for $i=1, \\dots, N$. This is implemented using `numpy.linspace`, which correctly generates such a grid.\n\n$2$. For each parameter value $r_i$ in the grid, the logistic map is iterated. This process is structured to handle the vast number of required computations efficiently. The computations for each $r_i$ are independent of one another. This \"embarrassingly parallel\" structure is a key observation. We exploit this by using vectorized operations provided by the `numpy` library. Instead of a slow, nested loop structure that iterates through each $r_i$ and then through time, we use a vector of states, $X = [x(r_1), x(r_2), \\dots, x(r_{N})]^T$. The entire vector of states is updated simultaneously at each time step $n$ according to the element-wise operation $X_{n+1} = R \\odot X_n \\odot (1 - X_n)$, where $R$ is the vector of parameter values and $\\odot$ denotes element-wise multiplication.\n\n$3$. For each $r_i$, the first $N_{\\text{trans}}$ iterates are discarded. This is known as the transient phase, allowing the system's state to settle onto its attractor. In the vectorized implementation, this corresponds to performing $N_{\\text{trans}}$ updates to the state vector $X$.\n\n$4$. Following the transient phase, the next $K$ iterates are generated. These iterates, denoted $x_{N_{\\text{trans}}+j}(r_i)$ for $j=1, \\dots, K$, represent the system's long-term behavior on the attractor. These values are summed for each $r_i$.\n\n$5$. The retained-iterate mean, $\\overline{x}(r_i)$, is calculated for each $r_i$ by dividing the sum from the previous step by $K$:\n$$\n\\overline{x}(r_i) = \\frac{1}{K}\\sum_{j=1}^{K} x_{N_{\\text{trans}}+j}(r_i).\n$$\nIn the vectorized implementation, this corresponds to an element-wise division of the sum vector by the scalar $K$.\n\n$6$. The final diagram mean, $M$, is computed by averaging all the individual retained-iterate means $\\overline{x}(r_i)$ over the parameter grid:\n$$\nM = \\frac{1}{N}\\sum_{i=1}^{N} \\overline{x}(r_i).\n$$\nThis is efficiently computed using `numpy.mean` on the vector of $\\overline{x}(r_i)$ values.\n\nThis entire process is performed once for the coarse grid parameters ($N_r$ points) to obtain $M_{\\text{coarse}}$, and a second time for the refined grid parameters ($N_r^{(\\text{refined})} = q\\,N_r$ points) to obtain $M_{\\text{refined}}$. The final result for the test case is the absolute difference, $D = |M_{\\text{refined}} - M_{\\text{coarse}}|$. For the special case where $r_{\\min} = r_{\\max}$, both grids consist of identical points, leading to $M_{\\text{coarse}} = M_{\\text{refined}}$ and thus $D=0$, as is correct. The implementation uses $64$-bit floating-point arithmetic (`numpy.float64`) to ensure numerical precision and robustness.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef compute_diagram_mean(r_min, r_max, num_r, N_trans, K, x0):\n    \"\"\"\n    Computes the mean of retained-iterate means over a grid of parameters r.\n    This corresponds to M_coarse or M_refined in the problem statement.\n    The implementation is vectorized using numpy for efficiency.\n    \n    Args:\n        r_min (float): The minimum value of the control parameter r.\n        r_max (float): The maximum value of the control parameter r.\n        num_r (int): The number of points in the parameter grid.\n        N_trans (int): The number of transient iterations to discard.\n        K (int): The number of subsequent iterations to retain and average.\n        x0 (float): The initial condition for the iteration.\n\n    Returns:\n        float: The computed diagram mean (M_coarse or M_refined).\n    \"\"\"\n    # 1. Construct the uniform grid of parameter values {r_i}.\n    r_values = np.linspace(r_min, r_max, num=num_r, dtype=np.float64)\n\n    # Initialize the state vector x. Each element corresponds to a value in r_values.\n    x_values = np.full(num_r, x0, dtype=np.float64)\n\n    # 2. Iterate to discard transient states (transient phase).\n    # The calculation is vectorized across all r values.\n    # The logistic map equation is x_{n+1} = r * x_n * (1 - x_n).\n    for _ in range(N_trans):\n        x_values = r_values * x_values * (1.0 - x_values)\n\n    # 3. Iterate to collect retained states and compute their sum (retained phase).\n    sum_of_retained_x = np.zeros(num_r, dtype=np.float64)\n    for _ in range(K):\n        x_values = r_values * x_values * (1.0 - x_values)\n        sum_of_retained_x += x_values\n\n    # 4. Compute the mean of retained states for each r_i.\n    # This creates a vector of \\overline{x}(r_i).\n    retained_iterate_means = sum_of_retained_x / K\n\n    # 5. Compute the final mean M over all r_i values.\n    diagram_mean = np.mean(retained_iterate_means)\n\n    return diagram_mean\n\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for all test cases and print the results.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # Format: (r_min, r_max, N_r, q, N_trans, K, x0)\n    test_cases = [\n        (2.8, 3.0, 200, 4, 800, 200, 0.5),\n        (3.569, 3.571, 400, 5, 1200, 256, 0.5),\n        (3.7, 3.8, 500, 3, 1000, 256, 0.5),\n        (3.99, 4.0, 600, 2, 1000, 256, 0.5),\n        (3.5, 3.5, 300, 1, 500, 128, 0.123456),\n    ]\n\n    results = []\n    for case in test_cases:\n        r_min, r_max, N_r, q, N_trans, K, x0 = case\n        \n        # Calculate M_coarse using the coarse grid parameters.\n        M_coarse = compute_diagram_mean(r_min, r_max, N_r, N_trans, K, x0)\n        \n        # Determine the size of the refined grid.\n        N_r_refined = q * N_r\n        \n        # Calculate M_refined using the refined grid parameters.\n        M_refined = compute_diagram_mean(r_min, r_max, N_r_refined, N_trans, K, x0)\n        \n        # Compute the absolute difference D.\n        D = abs(M_refined - M_coarse)\n        results.append(D)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```", "id": "2376483"}, {"introduction": "When simulating chaotic systems, we are always working with a finite-precision approximation of the true mathematical model. In chaotic regimes, tiny round-off errors can be amplified exponentially, causing a simulated trajectory to diverge significantly from both the ideal trajectory and from simulations using different levels of precision. This exercise makes this abstract concept tangible by comparing simulations of the logistic map using single- versus double-precision arithmetic, helping you quantify the practical impact of numerical precision on the study of chaotic dynamics. [@problem_id:2376515]", "problem": "You are asked to quantify how numerical floating-point precision influences the computed bifurcation diagram of the logistic map in the chaotic regime by comparing the asymptotic distributions obtained under two precisions. Consider the logistic map defined by the iteration\n$$\nx_{n+1} = f_r(x_n) = r\\,x_n\\,(1 - x_n),\n$$\nwith control parameter $r \\in [0,4]$ and state $x_n \\in [0,1]$. For a fixed $r$, the bifurcation diagram is the asymptotic set of values visited by $x_n$ as $n \\to \\infty$. In practice, this set can be approximated by sampling the iterates after discarding an initial transient. Floating-point precision affects the computed iterates and therefore the diagram. You will compare results between binary $32$-bit (single precision) and binary $64$-bit (double precision) floating-point formats as standardized by the Institute of Electrical and Electronics Engineers (IEEE) $754$.\n\nYour program must implement the following for each specified value of $r$:\n- Use a fixed initial condition $x_0 = 0.123456789$.\n- For each of the two precisions (binary $32$-bit and binary $64$-bit), iterate the logistic map for $N_{\\mathrm{discard}} = 5000$ steps to discard transients, followed by $N_{\\mathrm{keep}} = 4096$ steps to sample the asymptotic behavior.\n- Approximate the asymptotic distribution on $[0,1]$ by a uniform histogram with $B = 256$ bins covering the interval $[0,1]$. Let the normalized histogram densities for the two precisions be $\\{p_i\\}_{i=1}^{B}$ and $\\{q_i\\}_{i=1}^{B}$, where normalization is such that $\\sum_{i=1}^{B} p_i \\,\\Delta = 1$ and $\\sum_{i=1}^{B} q_i \\,\\Delta = 1$, with bin width $\\Delta = 1/B$.\n- Compute the discrepancy between the two approximated invariant distributions as the discrete $L^1$ distance\n$$\nD_{L^1}(r) = \\sum_{i=1}^{B} \\left| p_i - q_i \\right| \\, \\Delta.\n$$\n\nNo physical units or angles are involved in this problem; all quantities are dimensionless real numbers. For numerical reproducibility, round each $D_{L^1}(r)$ to $6$ decimal places.\n\nTest Suite:\n- Use the following $r$ values, which probe different depths of the chaotic regime and a boundary case:\n  - $r_1 = 3.6$\n  - $r_2 = 3.9$\n  - $r_3 = 3.99$\n  - $r_4 = 4.0$\n\nRequired final output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order $[D_{L^1}(r_1), D_{L^1}(r_2), D_{L^1}(r_3), D_{L^1}(r_4)]$, with each value rounded to $6$ decimal places. For example, an output line should look like\n$[\\text{value}_1,\\text{value}_2,\\text{value}_3,\\text{value}_4]$.\nThe only accepted data types for each entry are floating-point numbers. No additional text or lines should be printed.", "solution": "The problem as stated is valid. It is scientifically grounded, algorithmically well-defined, and all necessary parameters for a unique numerical solution are provided. The task is to compute the discrepancy between the asymptotic behaviors of the logistic map when simulated with two different floating-point precisions.\n\nThe logistic map is a discrete-time dynamical system defined by the iterative equation:\n$$\nx_{n+1} = f_r(x_n) = r \\, x_n \\, (1 - x_n)\n$$\nwhere $n$ is the iteration index, $x_n \\in [0, 1]$ is the state of the system at step $n$, and $r \\in [0, 4]$ is the control parameter. For certain values of $r$, specifically in the range $r \\gtrsim 3.57$, the system exhibits chaotic behavior. In a chaotic regime, the system has sensitive dependence on initial conditions, meaning that infinitesimally small differences in the initial state $x_0$ lead to exponentially diverging trajectories. Numerical round-off errors introduced by finite-precision floating-point arithmetic act as such small differences at each step of the iteration, causing numerically computed trajectories to diverge from the true mathematical trajectory and also from each other when different precisions are used.\n\nThe objective is to quantify this divergence by comparing the approximated invariant probability distributions of the attractor. This is achieved through the following computational procedure for each provided value of $r$:\n\n1.  **Parallel simulations**: Two independent simulations of the logistic map are performed.\n    - The first simulation uses single-precision floating-point numbers (binary$32$ format as per IEEE $754$), which corresponds to `numpy.float32` in the specified computational environment.\n    - The second simulation uses double-precision floating-point numbers (binary$64$ format), corresponding to `numpy.float64`.\n    - For coherence, both the state variable $x_n$ and the parameter $r$ are explicitly cast to the respective precision at the start of each simulation.\n\n2.  **Trajectory generation**: For each precision:\n    - The simulation starts from the initial condition $x_0 = 0.123456789$.\n    - The map is iterated for $N_{\\mathrm{discard}} = 5000$ steps. These initial iterates are discarded to ensure that the trajectory has converged to the system's attractor and is no longer influenced by transient dynamics.\n    - The map is then iterated for an additional $N_{\\mathrm{keep}} = 4096$ steps. The state values $\\{x_n\\}$ from these steps are collected. These $4096$ points constitute a numerical sample of the attractor.\n\n3.  **Distribution approximation**: The collected samples are used to approximate the invariant probability density of the attractor.\n    - For each of the two collected data series (one for single precision, one for double precision), a histogram is constructed.\n    - The domain $[0, 1]$ is partitioned into $B = 256$ uniform bins, each of width $\\Delta = 1/B$.\n    - The histograms are normalized to represent probability densities, yielding density sets $\\{p_i\\}_{i=1}^{B}$ for single precision and $\\{q_i\\}_{i=1}^{B}$ for double precision. This normalization ensures that $\\sum_{i=1}^{B} p_i \\Delta = 1$ and $\\sum_{i=1}^{B} q_i \\Delta = 1$. The `numpy.histogram` function with the `density=True` argument performs this normalization correctly.\n\n4.  **Discrepancy computation**: The difference between the two resulting probability distributions is quantified using the discrete $L^1$ distance, defined as:\n    $$\n    D_{L^1}(r) = \\sum_{i=1}^{B} | p_i - q_i | \\Delta\n    $$\n    This metric sums the absolute differences in probability density for each bin, weighted by the bin width. It provides a measure of the total variation between the two distributions. A value of $D_{L^1}(r)=0$ would indicate identical distributions, while a value of $D_{L^1}(r)=2$ would correspond to completely disjoint distributions.\n\nThe final computed value of $D_{L^1}(r)$ for each tested $r$ is rounded to $6$ decimal places as specified. The algorithm is implemented for the test suite of $r$ values: $r_1 = 3.6$, $r_2 = 3.9$, $r_3 = 3.99$, and $r_4 = 4.0$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the L1 distance between asymptotic distributions of the logistic map\n    generated using single and double floating-point precision.\n    \"\"\"\n\n    def generate_series(r, x0, n_discard, n_keep, precision):\n        \"\"\"\n        Generates a time series from the logistic map for a given precision.\n\n        Args:\n            r (float): The control parameter of the logistic map.\n            x0 (float): The initial condition.\n            n_discard (int): The number of transient steps to discard.\n            n_keep (int): The number of steps to record for the series.\n            precision (type): The numpy float type (e.g., np.float32, np.float64).\n\n        Returns:\n            np.ndarray: The array of recorded state values.\n        \"\"\"\n        # Cast parameter and initial state to the specified precision.\n        r_prec = precision(r)\n        x = precision(x0)\n        one = precision(1.0)\n\n        # Discard the initial transient iterations.\n        for _ in range(n_discard):\n            x = r_prec * x * (one - x)\n\n        # Record the subsequent iterations to sample the attractor.\n        series = np.zeros(n_keep, dtype=precision)\n        for i in range(n_keep):\n            x = r_prec * x * (one - x)\n            series[i] = x\n\n        return series\n\n    # Problem parameters\n    test_cases = [3.6, 3.9, 3.99, 4.0]\n    initial_condition = 0.123456789\n    num_discard = 5000\n    num_keep = 4096\n    num_bins = 256\n\n    results = []\n\n    for r_val in test_cases:\n        # Generate time series for both single (32-bit) and double (64-bit) precision.\n        series_32 = generate_series(r_val, initial_condition, num_discard, num_keep, np.float32)\n        series_64 = generate_series(r_val, initial_condition, num_discard, num_keep, np.float64)\n\n        # Define histogram parameters.\n        hist_range = (0.0, 1.0)\n        bin_width = 1.0 / num_bins\n\n        # Compute normalized histograms (probability densities).\n        p_density, _ = np.histogram(series_32, bins=num_bins, range=hist_range, density=True)\n        q_density, _ = np.histogram(series_64, bins=num_bins, range=hist_range, density=True)\n        \n        # Calculate the discrete L1 distance.\n        l1_distance = np.sum(np.abs(p_density - q_density)) * bin_width\n\n        # Round the result to 6 decimal places.\n        results.append(round(l1_distance, 6))\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2376515"}]}