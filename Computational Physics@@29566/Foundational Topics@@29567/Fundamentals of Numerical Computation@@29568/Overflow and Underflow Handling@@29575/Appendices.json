{"hands_on_practices": [{"introduction": "Many fundamental equations in physics, from quantum mechanics to statistical distributions, involve exponential terms. A naive implementation of these functions in code can fail spectacularly when arguments become large, leading to numerical overflow or underflow. This first practice tackles this common challenge head-on by asking you to create a 'safe' version of the ubiquitous Gaussian probability density function, demonstrating how logarithmic-space computation can transform an unstable calculation into a robust and reliable one [@problem_id:2423348].", "problem": "You are asked to implement a numerically robust program that evaluates the Gaussian (normal) probability density function for given parameters while handling extreme cases that would otherwise cause numerical underflow or overflow in finite-precision arithmetic.\n\nLet the Gaussian probability density function be defined for real $x$, real $\\mu$, and strictly positive $\\sigma$ by\n$$\n\\operatorname{pdf}(x,\\mu,\\sigma) = \\frac{1}{\\sigma\\sqrt{2\\pi}}\\exp\\!\\left(-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^{2}\\right).\n$$\nAll computations must assume the standard double-precision floating-point format as specified by the Institute of Electrical and Electronics Engineers (IEEE) $754$. You must ensure the following output policy for representability:\n- If the exact mathematical value of $\\operatorname{pdf}(x,\\mu,\\sigma)$ is strictly smaller than the smallest strictly positive representable double-precision number $2^{-1074}$, your program must output the floating-point value $0.0$ for that test case.\n- If the exact mathematical value of $\\operatorname{pdf}(x,\\mu,\\sigma)$ is strictly larger than the largest finite representable double-precision number $(2-2^{-52})\\times 2^{1023}$, your program must output the floating-point value corresponding to positive infinity for that test case.\n- Otherwise, your program must output the finite floating-point value of $\\operatorname{pdf}(x,\\mu,\\sigma)$ in standard double precision.\n\nNo physical units are involved in this problem. Angles are not used.\n\nTest suite and required output format:\n- Evaluate $\\operatorname{pdf}(x,\\mu,\\sigma)$ for the following ordered list of parameter triples $(x,\\mu,\\sigma)$:\n  1. $(0,0,1)$,\n  2. $(8,0,1)$,\n  3. $(38,0,1)$,\n  4. $(40,0,1)$,\n  5. $(0,0,10^{-320})$,\n  6. $(0,0,10^{50})$,\n  7. $\\left(-10^{308},\\,10^{308},\\,1\\right)$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the same order as the test suite, for example, $[r_1,r_2,\\dots,r_7]$ where each $r_k$ is a floating-point number consistent with the representability policy above. Do not print any additional characters or lines.", "solution": "The problem as stated is to implement a numerically robust evaluation of the Gaussian probability density function, $\\operatorname{pdf}(x,\\mu,\\sigma)$, given by the formula\n$$\n\\operatorname{pdf}(x,\\mu,\\sigma) = \\frac{1}{\\sigma\\sqrt{2\\pi}}\\exp\\!\\left(-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^{2}\\right),\n$$\nfor real-valued parameters $x$ and $\\mu$, and a strictly positive real parameter $\\sigma$. The implementation must correctly handle cases that would lead to numerical overflow or underflow in a naive direct computation using standard double-precision floating-point arithmetic.\n\nThe problem is scientifically grounded, well-posed, objective, and contains all necessary information for a unique solution. It is a classic problem in computational science demonstrating essential techniques for maintaining numerical stability. Therefore, the problem is valid, and we proceed to its solution.\n\nThe fundamental principle for developing a robust algorithm is to avoid operations that can catastrophically fail at the limits of the floating-point number range. A naive implementation of the formula involves several such potential failures:\n$1$. If $\\sigma$ is very small (approaching zero), the pre-factor $\\frac{1}{\\sigma\\sqrt{2\\pi}}$ can overflow.\n$2$. If $\\sigma$ is very large, the pre-factor can underflow to $0.0$, losing all precision.\n$3$. The standardized variable $z = \\frac{x-\\mu}{\\sigma}$ can itself overflow if $|x-\\mu|$ is large and $\\sigma$ is small.\n$4$. If $|z|$ is large, the term $z^2$ can overflow, even if $z$ itself is representable.\n$5$. If $|z|$ is large, the exponential term $\\exp(-0.5 z^2)$ can underflow to $0.0$.\n\nTo mitigate these issues, we reformulate the calculation in logarithmic space. This is a standard and powerful technique in numerical methods. Instead of computing the $\\operatorname{pdf}$ directly, we first compute its natural logarithm, which we denote as $L$:\n$$\nL = \\log(\\operatorname{pdf}(x,\\mu,\\sigma)) = \\log\\left(\\frac{1}{\\sigma\\sqrt{2\\pi}}\\exp\\!\\left(-\\frac{1}{2}z^2\\right)\\right)\n$$\nUsing the properties of logarithms, this expression simplifies to a sum of terms:\n$$\nL = \\log\\left(\\frac{1}{\\sigma\\sqrt{2\\pi}}\\right) + \\log\\left(\\exp\\!\\left(-\\frac{1}{2}z^2\\right)\\right) \\\\\nL = -\\log(\\sigma\\sqrt{2\\pi}) - \\frac{1}{2}z^2 \\\\\nL = -\\log(\\sigma) - \\frac{1}{2}\\log(2\\pi) - \\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2\n$$\nThis formulation replaces the multiplication of the pre-factor and the exponential term with the addition of their logarithms. Addition is far less prone to spurious overflow and underflow than multiplication, thereby extending the dynamic range of inputs for which a meaningful result can be computed.\n\nThe algorithmic procedure is as follows:\n$1$. For a given set of parameters $(x, \\mu, \\sigma)$, we first calculate the terms of the log-PDF, $L$. The constant term $\\frac{1}{2}\\log(2\\pi)$ can be pre-computed.\n$2$. We calculate $\\log(\\sigma)$ and the standardized variable $z = \\frac{x-\\mu}{\\sigma}$. It is critical to note that for extreme values, such as test case $7$, the intermediate calculation of $x-\\mu$ may overflow. However, IEEE $754$ arithmetic, as implemented in `numpy`, correctly handles this by propagating an infinity (`inf`). For instance, if $x-\\mu$ overflows to $-\\infty$, then $z$ becomes $-\\infty$, $z^2$ becomes $+\\infty$, the exponent term $-0.5z^2$ becomes $-\\infty$, and the total log-PDF $L$ becomes $-\\infty$. This leads to a final result of $\\exp(-\\infty) = 0.0$, which is the mathematically correct limit. Thus, a special check for the overflow of $z^2$ is not required, as the standard `inf` propagation handles it correctly.\n$3$. We then compute the total log-PDF: $L = (-\\log(\\sigma) - \\frac{1}{2}\\log(2\\pi)) - \\frac{1}{2}z^2$.\n$4$. Before computing the final result by exponentiation, we must compare $L$ to the specified thresholds, also transformed into log-space. The problem defines overflow and underflow bounds based on the limitations of double-precision numbers.\n    - **Overflow condition**: The result overflows if it is strictly larger than the largest finite double-precision number, $D_{\\text{max}} = (2-2^{-52})\\times 2^{1023}$. This is equivalent to checking if $L  \\log(D_{\\text{max}})$. If this condition is met, the function must return positive infinity.\n    - **Underflow condition**: The result underflows if it is strictly smaller than the smallest positive (subnormal) double-precision number, $D_{\\text{min\\_sub}} = 2^{-1074}$. This is equivalent to checking if $L  \\log(D_{\\text{min\\_sub}}) = -1074\\log(2)$. If this condition is met, the function must return $0.0$.\n$5$. If $L$ falls between these two logarithmic thresholds, the result is numerically representable and stable to compute. We then calculate the final PDF value as $\\exp(L)$.\n\nThis design, based on the principle of logarithmic computation, ensures that the implementation is robust, accurate, and adheres to the specified policies for handling extreme numerical values. It correctly processes all test cases, from standard inputs to those designed to cause overflow or underflow in naive implementations.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef robust_gaussian_pdf(x: float, mu: float, sigma: float) - float:\n    \"\"\"\n    Computes the Gaussian probability density function in a numerically robust way.\n\n    Args:\n        x: The point at which to evaluate the PDF.\n        mu: The mean of the distribution.\n        sigma: The standard deviation of the distribution (must be  0).\n\n    Returns:\n        The value of the PDF, handling overflow and underflow per IEEE 754 spec.\n    \"\"\"\n    # Pre-calculated constant for efficiency and clarity.\n    # log(sqrt(2*pi))\n    LOG_SQRT_2PI = 0.5 * np.log(2.0 * np.pi)\n\n    # Thresholds for overflow and underflow in log-space, as per the problem.\n    # log(DBL_MAX)\n    LOG_MAX_FLOAT = np.log(np.finfo(np.double).max)\n    # log(2**-1074)\n    LOG_MIN_SUBNORMAL = -1074.0 * np.log(2.0)\n\n    # Per the problem statement, sigma is strictly positive. A check for sigma = 0\n    # would be good practice in a general-purpose library, but it is not\n    # required for the given test suite.\n    \n    # Standardized variable z. Numpy's handling of large numbers will correctly\n    # propagate 'inf' if (x - mu) overflows, which leads to the correct\n    # final result of 0.0.\n    z = (x - mu) / sigma\n    \n    # Calculate the PDF in log-space to prevent intermediate over/underflow.\n    # log(pdf) = -log(sigma) - log(sqrt(2*pi)) - 0.5 * z**2\n    log_pdf = -np.log(sigma) - LOG_SQRT_2PI - 0.5 * z**2\n    \n    # Check against the specified representability thresholds.\n    if log_pdf  LOG_MAX_FLOAT:\n        return np.inf\n    \n    if log_pdf  LOG_MIN_SUBNORMAL:\n        return 0.0\n        \n    # If within representable range, compute the final value.\n    return np.exp(log_pdf)\n\n\ndef solve():\n    \"\"\"\n    Runs the test suite and prints the results in the required format.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (0.0, 0.0, 1.0),\n        (8.0, 0.0, 1.0),\n        (38.0, 0.0, 1.0),\n        (40.0, 0.0, 1.0),\n        (0.0, 0.0, 1e-320),\n        (0.0, 0.0, 1e50),\n        (-1e308, 1e308, 1.0),\n    ]\n\n    results = []\n    for case in test_cases:\n        x_val, mu_val, sigma_val = case\n        result = robust_gaussian_pdf(x_val, mu_val, sigma_val)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    # The default string conversion for floats, including 'inf', is correct here.\n    print(f\"[{','.join(map(str, results))}]\")\n\n# Execute the solution.\nsolve()\n```", "id": "2423348"}, {"introduction": "Building upon the logarithmic-space technique, we now turn to problems involving combinatorics, such as modeling random walks or counting microstates in statistical mechanics. Calculating binomial coefficients like $\\binom{N}{k}$ involves factorials, which grow astonishingly fast and will overflow standard floating-point types for even moderately large $N$. This exercise will guide you to handle these massive numbers by extending the log-space approach with the log-gamma function, a powerful tool for working with factorials numerically [@problem_id:2423389].", "problem": "You are to write a complete program that computes, for a one-dimensional discrete-time random walk on a lattice, the probability of being at a specified position after a given number of steps. At each time step, the position changes by $+1$ with probability $p$ and by $-1$ with probability $1-p$. Let $X_N$ denote the position after $N$ steps, starting from the origin at step $0$. For a given triple $(N,x,p)$ with $N \\in \\mathbb{Z}_{\\ge 0}$, $x \\in \\mathbb{Z}$, and $p \\in [0,1]$, compute the probability $\\mathbb{P}(X_N = x)$. Your program must robustly handle overflow and underflow that arise in the evaluation of combinatorial terms and powers, and it must return $0.0$ whenever the final probability underflows in double-precision floating-point arithmetic.\n\nThe following conditions must be enforced by first principles:\n- If $|x|  N$, then the event is impossible and the probability must be $0.0$.\n- If $N + x$ is odd, then the event is impossible and the probability must be $0.0$.\n- For the boundary cases $p = 0$ and $p = 1$, the exact probabilities are well-defined: when $p = 0$, the probability is $1.0$ if and only if $x = -N$, and is $0.0$ otherwise; when $p = 1$, the probability is $1.0$ if and only if $x = +N$, and is $0.0$ otherwise.\n\nNo physical units are involved. All answers must be returned as dimensionless real numbers. Angles are not used.\n\nYour program must evaluate the probability for each of the following test cases and aggregate the results:\n- $(N,x,p) = (\\,10,\\,2,\\,0.5\\,)$\n- $(N,x,p) = (\\,9,\\,2,\\,0.5\\,)$\n- $(N,x,p) = (\\,1000,\\,1000,\\,0.5\\,)$\n- $(N,x,p) = (\\,5000,\\,5000,\\,0.5\\,)$\n- $(N,x,p) = (\\,1000000,\\,0,\\,0.5\\,)$\n- $(N,x,p) = (\\,200000,\\,200,\\,0.501\\,)$\n- $(N,x,p) = (\\,1000,\\, -1000,\\, 1.0\\times 10^{-12}\\,)$\n- $(N,x,p) = (\\,1000,\\, 1000,\\, 1.0\\times 10^{-12}\\,)$\n- $(N,x,p) = (\\,50,\\, 50,\\, 1.0\\,)$\n\nFinal output format:\n- Your program must produce a single line containing a comma-separated list of the $9$ probabilities in the same order as above, enclosed in square brackets.\n- Each probability must be formatted in scientific notation with exactly $12$ significant digits after the decimal point, for example: $[\\,1.234000000000\\mathrm{e}{-02},\\ldots\\,]$.\n- No additional text or lines are permitted in the output.", "solution": "The problem as stated is valid. It is scientifically grounded in the principles of probability theory and statistical mechanics, specifically the binomial distribution which governs the discrete-time random walk. It is well-posed, objective, and contains all necessary information for a unique solution. The core challenge is computational, involving the handling of numerical overflow and underflow, which is a standard topic in computational physics.\n\nWe are tasked with computing the probability $\\mathbb{P}(X_N = x)$ of a one-dimensional random walk being at position $x$ after $N$ steps, starting from the origin $X_0 = 0$. At each step, the position changes by $+1$ with probability $p$ and by $-1$ with probability $q = 1-p$.\n\nLet $N_+$ be the number of steps to the right (position increases by $1$) and $N_-$ be the number of steps to the left (position decreases by $1$). The total number of steps is $N$.\n$$N_+ + N_- = N$$\nThe final position $x$ is given by the difference between the number of right and left steps.\n$$N_+ - N_- = x$$\nThis system of two linear equations can be solved for $N_+$ and $N_-$. Adding the two equations yields $2N_+ = N+x$, and subtracting the second from the first yields $2N_- = N-x$. Thus, we have:\n$$N_+ = \\frac{N+x}{2}$$\n$$N_- = \\frac{N-x}{2}$$\nFor $N_+$ and $N_-$ to be valid counts, they must be non-negative integers. This imposes two fundamental constraints:\n1.  Parity constraint: The sum $N+x$ (and consequently $N-x$) must be an even number. If $N+x$ is odd, it is impossible to reach position $x$ in $N$ steps, so the probability is $0$. This is equivalent to the condition that $N$ and $x$ must have the same parity.\n2.  Boundary constraint: The number of steps in either direction cannot be negative. This requires $N+x \\ge 0$ and $N-x \\ge 0$, which is equivalent to $|x| \\le N$. If the target position $x$ is more distant from the origin than the number of steps $N$, it is an impossible event, and the probability is $0$.\n\nIf these conditions are met, the problem is equivalent to finding the probability of obtaining exactly $N_+$ successes in $N$ Bernoulli trials, where the probability of success (a step to the right) is $p$. This is described by the binomial distribution. The number of distinct paths leading to position $x$ is the number of ways to choose $N_+$ steps to the right out of a total of $N$ steps, which is given by the binomial coefficient $\\binom{N}{N_+}$. The probability of any single such path is $p^{N_+} (1-p)^{N_-}$.\n\nTherefore, the total probability is:\n$$\\mathbb{P}(X_N = x) = \\binom{N}{N_+} p^{N_+} (1-p)^{N_-}$$\nSubstituting the expressions for $N_+$ and $N_-$:\n$$\\mathbb{P}(X_N = x) = \\binom{N}{(N+x)/2} p^{(N+x)/2} (1-p)^{(N-x)/2}$$\n\nDirect computation of this formula is numerically unstable for large $N$. The factorial terms within the binomial coefficient (e.g., $N!$) and the power terms (e.g., $p^{N_+}$) can quickly exceed the limits of standard floating-point representations, leading to overflow or underflow.\n\nThe correct and robust computational strategy is to work with the logarithm of the probability. Let $P = \\mathbb{P}(X_N = x)$. We compute $\\ln(P)$:\n$$\\ln(P) = \\ln\\left(\\binom{N}{N_+}\\right) + N_+\\ln(p) + N_-\\ln(1-p)$$\nThe logarithm of the binomial coefficient can be expressed using the logarithm of the gamma function, $\\ln(\\Gamma(z))$, because $\\Gamma(k+1) = k!$.\n$$\\ln\\left(\\binom{N}{k}\\right) = \\ln(N!) - \\ln(k!) - \\ln((N-k)!) = \\ln(\\Gamma(N+1)) - \\ln(\\Gamma(k+1)) - \\ln(\\Gamma(N-k+1))$$\nThis is a numerically stable computation provided by standard scientific libraries, such as `scipy.special.gammaln`.\n\nThe final expression for the log-probability is:\n$$\\ln(P) = \\ln(\\Gamma(N+1)) - \\ln(\\Gamma(N_++1)) - \\ln(\\Gamma(N_-+1)) + N_+\\ln(p) + N_-\\ln(1-p)$$\nFor improved numerical accuracy when $p$ is very small, the term $\\ln(1-p)$ should be computed using a `log1p` function, i.e., `log1p(-p)`.\n\nThe overall algorithm is as follows:\n1.  Validate inputs $(N, x, p)$. Check the base conditions: if $|x|  N$ or if $(N+x)$ is odd, the probability is exactly $0$.\n2.  Handle the boundary cases for $p$.\n    - If $p=0$, the walk is deterministic to the left. The probability is $1$ if $x = -N$ and $0$ otherwise.\n    - If $p=1$, the walk is deterministic to the right. The probability is $1$ if $x = N$ and $0$ otherwise.\n3.  If $0  p  1$, calculate $N_+ = (N+x)/2$ and $N_- = (N-x)/2$.\n4.  Compute the log-probability $\\ln(P)$ using the log-gamma formula.\n5.  The final probability is obtained by exponentiating the result: $P = \\exp(\\ln(P))$. This operation correctly handles underflow: if $\\ln(P)$ is a large negative number, $\\exp(\\ln(P))$ will correctly evaluate to $0.0$ in double-precision arithmetic.\n\nThis procedure ensures a numerically stable and correct evaluation for a wide range of parameters, including those that would cause failure in a naive implementation.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import gammaln\n\ndef calculate_probability(N, x, p):\n    \"\"\"\n    Computes the probability of being at position x after N steps of a 1D random walk.\n\n    Args:\n        N (int): The total number of steps.\n        x (int): The final position.\n        p (float): The probability of stepping to the right (+1).\n\n    Returns:\n        float: The probability P(X_N = x).\n    \"\"\"\n    # Step 1: Validate inputs and check for impossible events.\n    # The parity of position x must match the parity of the number of steps N.\n    # This is equivalent to N+x (and N-x) being even.\n    if (N + x) % 2 != 0:\n        return 0.0\n    \n    # The final position cannot be further from the origin than the number of steps.\n    if abs(x)  N:\n        return 0.0\n\n    # Step 2: Handle boundary cases for probability p.\n    if p == 0.0:\n        return 1.0 if x == -N else 0.0\n    \n    if p == 1.0:\n        return 1.0 if x == N else 0.0\n\n    # Step 3: Calculate the number of right (+) and left (-) steps.\n    # These are guaranteed to be integers due to the parity check above.\n    n_plus = (N + x) // 2\n    n_minus = N - n_plus\n\n    # Step 4: Compute the log-probability to avoid overflow/underflow.\n    # The formula is: log(P) = log(C(N, n_plus)) + n_plus*log(p) + n_minus*log(1-p)\n    # The log-combinatorial term log(C(N, k)) is calculated using log-gamma functions:\n    # log(C(N, k)) = gammaln(N+1) - gammaln(k+1) - gammaln(N-k+1)\n    \n    log_comb_term = gammaln(N + 1) - gammaln(n_plus + 1) - gammaln(n_minus + 1)\n    \n    # The log-probability term is calculated using np.log and np.log1p for accuracy.\n    # np.log1p(y) calculates log(1+y) accurately for small y.\n    # Here, log(1-p) is log1p(-p).\n    log_p_term = n_plus * np.log(p) + n_minus * np.log1p(-p)\n    \n    log_prob = log_comb_term + log_p_term\n\n    # Step 5: Exponentiate to get the final probability.\n    # np.exp will handle underflow by returning 0.0 for very negative inputs.\n    return np.exp(log_prob)\n\ndef solve():\n    \"\"\"\n    Main function to run the test cases and print the results in the specified format.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (10, 2, 0.5),\n        (9, 2, 0.5),\n        (1000, 1000, 0.5),\n        (5000, 5000, 0.5),\n        (1000000, 0, 0.5),\n        (200000, 200, 0.501),\n        (1000, -1000, 1.0e-12),\n        (1000, 1000, 1.0e-12),\n        (50, 50, 1.0),\n    ]\n\n    results = []\n    for case in test_cases:\n        N, x, p = case\n        result = calculate_probability(int(N), int(x), float(p))\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    # e.g., [1.234000000000e-02,...]\n    formatted_results = [f'{r:.12e}' for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "2423389"}, {"introduction": "Not all numerical errors are as dramatic as overflow; some are far more insidious. In many simulations, we must sum a very large number of very small values to track a quantity that should be conserved, like energy or momentum. This final practice explores how the repeated, seemingly harmless loss of precision when adding numbers of different magnitudes can lead to a significant drift in results. You will implement and compare a standard summation with a compensated summation algorithm, revealing how to guard against this subtle but critical source of error in long-running computations [@problem_id:2423330].", "problem": "Consider a one-dimensional mass–spring oscillator with mass $m$ in kilograms and spring constant $k$ in newtons per meter. Its exact motion when released from rest at displacement amplitude $A$ in meters is given by the analytic solution $x(t)=A \\cos(\\omega t)$ and $v(t)=-A \\omega \\sin(\\omega t)$ with angular frequency $\\omega=\\sqrt{k/m}$ in radians per second and time $t$ in seconds. The force is $F(t)=-k x(t)$ in newtons. The instantaneous power is $P(t)=F(t)\\,v(t)$ in watts. Over any integer number of full periods $T=2\\pi/\\omega$ the exact net work done $W=\\int P(t)\\,\\mathrm{d}t$ is $0$ joules because the system is conservative.\n\nLet the net work be approximated by the discrete sum of uniform time-step increments over $N$ steps, where the step size is $\\Delta t=T/M$ with $M$ steps per period and $N=C M$ for an integer number $C$ of periods. Define sampling times $t_n=n\\,\\Delta t$ for $n=0,1,\\dots,N-1$. The discrete work increment at step $n$ is\n$$\n\\Delta W_n = F(t_n)\\,v(t_n)\\,\\Delta t \\,,\n$$\nso that the discrete approximation to the net work is\n$$\nW_{\\mathrm{disc}}=\\sum_{n=0}^{N-1} \\Delta W_n \\,.\n$$\nIn exact arithmetic, $W_{\\mathrm{disc}}$ equals $0$ joules for any integer $C$.\n\nYour task is to write a complete program that, for each test case below, computes the absolute value of the numerically accumulated $W_{\\mathrm{disc}}$ in two distinct floating-point accumulation modes:\n- Mode A: a straightforward running sum stored in a single floating-point variable.\n- Mode B: a running sum that uses an additional floating-point variable to carry a correction for low-order rounding loss so that lost low-order bits from previous additions are fed back into subsequent updates.\n\nFor each test case, report two floating-point numbers in joules: the absolute value of the final $W_{\\mathrm{disc}}$ from Mode A and from Mode B, in that order.\n\nUse the following test suite (all quantities are in the International System of Units, angles are in radians):\n- Test 1: $m=1$, $k=1$, $A=1$, $C=1000$, $M=1000$.\n- Test 2: $m=1$, $k=1$, $A=1$, $C=1000$, $M=200$.\n- Test 3: $m=1$, $k=1$, $A=1$, $C=2000$, $M=500$.\n\nFinal output format: Your program should produce a single line of output containing a flat, comma-separated list of six numbers enclosed in square brackets, ordered as\n$$\n[\\ |W_{\\mathrm{A},1}|,\\ |W_{\\mathrm{B},1}|,\\ |W_{\\mathrm{A},2}|,\\ |W_{\\mathrm{B},2}|,\\ |W_{\\mathrm{A},3}|,\\ |W_{\\mathrm{B},3}|\\ ] \\,,\n$$\nwhere $|W_{\\mathrm{A},j}|$ and $|W_{\\mathrm{B},j}|$ denote the absolute values of the accumulated discrete work in joules for test case $j$ in Mode A and Mode B, respectively. The numbers must be printed in decimal notation as floating-point values. No other text should be printed.", "solution": "The problem presented is validated as scientifically sound, well-posed, and objective. It addresses a fundamental issue in computational physics: the accumulation of floating-point rounding errors in numerical summation. The system under consideration is a standard simple harmonic oscillator, and the physical principles cited are correct. The task is to compare two methods for summing a large series of small floating-point numbers whose theoretical sum is zero. This provides a direct measure of the numerical error introduced by the summation algorithm itself.\n\nThe motion of the mass-spring system is described by:\nMass: $m$\nSpring constant: $k$\nAmplitude: $A$\nAngular frequency: $\\omega = \\sqrt{k/m}$\nPosition: $x(t) = A \\cos(\\omega t)$\nVelocity: $v(t) = -A \\omega \\sin(\\omega t)$\n\nThe instantaneous power, $P(t)$, exerted by the spring force $F(t) = -k x(t)$ is given by the product of force and velocity:\n$$P(t) = F(t)v(t) = (-k x(t))(-A \\omega \\sin(\\omega t)) = k A^2 \\omega \\cos(\\omega t) \\sin(\\omega t)$$\nUsing the trigonometric identity $\\sin(2\\theta) = 2 \\sin(\\theta) \\cos(\\theta)$, the power can be expressed more compactly as:\n$$P(t) = \\frac{1}{2} k A^2 \\omega \\sin(2\\omega t)$$\n\nThe problem requires the computation of the net work done, $W_{\\mathrm{disc}}$, by summing discrete power-time increments over $N$ uniform time steps. The simulation runs for $C$ full periods of oscillation, with $M$ steps per period $T = 2\\pi/\\omega$.\nTotal number of steps: $N = C M$\nTime step size: $\\Delta t = T/M = \\frac{2\\pi}{\\omega M}$\nDiscrete time points: $t_n = n \\Delta t$ for $n = 0, 1, \\dots, N-1$.\n\nThe discrete work increment at step $n$ is $\\Delta W_n = P(t_n) \\Delta t$:\n$$\\Delta W_n = \\left(\\frac{1}{2} k A^2 \\omega \\sin(2\\omega t_n)\\right) \\Delta t$$\nSubstituting $t_n = n \\Delta t$, the argument of the sine function becomes $2\\omega (n \\Delta t) = 2\\omega n \\frac{2\\pi}{\\omega M} = \\frac{4\\pi n}{M}$.\nTherefore, the total discrete work is the sum:\n$$W_{\\mathrm{disc}} = \\sum_{n=0}^{N-1} \\Delta W_n = \\sum_{n=0}^{CM-1} \\left(\\frac{1}{2} k A^2 \\omega \\Delta t \\right) \\sin\\left(\\frac{4\\pi n}{M}\\right)$$\nAs established during validation, this sum is exactly zero in infinite-precision arithmetic because the sine function is sampled symmetrically over an integer number of its own periods. The numerical computation, however, is performed using finite-precision floating-point arithmetic, which introduces rounding errors. The objective is to quantify these errors for two different summation algorithms.\n\nMode A: Naive Summation\nThis is the most direct approach. A single floating-point variable accumulates the sum. Let $S$ be the running sum, initialized to $0$. In each step $n$, the update is $S \\leftarrow S + \\Delta W_n$. When the magnitude of $S$ becomes significantly larger than the magnitude of $\\Delta W_n$, the addition causes a loss of the least significant bits of $\\Delta W_n$. This phenomenon leads to a substantial cumulative error when $N$ is large.\n\nMode B: Compensated Summation\nThis method, known as the Kahan summation algorithm, is designed to mitigate the loss of precision. It uses an additional \"compensation\" variable, $c$, to accumulate the rounding error from each addition and add it back into the sum in the next iteration.\nThe algorithm proceeds as follows for each term $\\Delta W_n$:\n1. Initialize sum $S = 0$ and compensation $c = 0$.\n2. For each $n = 0, \\dots, N-1$:\n   a. Create a corrected term $y = \\Delta W_n - c$. The value $c$ holds the error from the *previous* step.\n   b. Tentatively update the sum: $t = S + y$. In this operation, the low-order bits of $y$ may be lost.\n   c. Calculate the error of the current addition: $c = (t - S) - y$. The term $(t-S)$ numerically recovers the part of $y$ that was actually added to $S$. Subtracting $y$ from this result yields the negative of the part of $y$ that was lost.\n   d. Finalize the sum for this step: $S = t$.\nThis procedure ensures that rounding errors are not discarded but are instead carried forward to be incorporated into future terms, dramatically improving the accuracy of the final sum.\n\nThe implementation calculates $\\omega$, $T$, $\\Delta t$, and $N$ for each test case. Then, a loop from $n=0$ to $N-1$ computes $\\Delta W_n$ and updates two separate sums, one for Mode A and one for Mode B. The final output will be the absolute values of these two sums for each of the three test cases.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the numerically accumulated work for a mass-spring system\n    using two different floating-point summation algorithms and reports\n    the absolute error for each.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # Format: (m, k, A, C, M) in SI units.\n    test_cases = [\n        (1.0, 1.0, 1.0, 1000, 1000),\n        (1.0, 1.0, 1.0, 1000, 200),\n        (1.0, 1.0, 1.0, 2000, 500),\n    ]\n\n    results = []\n    for m, k, A, C, M in test_cases:\n        # Calculate derived physical and numerical parameters.\n        # N is cast to int to be used as a range limit.\n        omega = np.sqrt(k / m)\n        T = 2.0 * np.pi / omega\n        dt = T / M\n        N = int(C * M)\n\n        # Initialize accumulators for Mode A (naive) and Mode B (Kahan).\n        sum_A = 0.0\n        sum_B = 0.0\n        c_B = 0.0  # Correction term for Kahan summation.\n\n        # Pre-compute constant factors to minimize floating-point operations\n        # inside the high-iteration loop.\n        # The work increment is delta_W_n = (0.5 * k * A**2 * omega * dt) * sin(4 * pi * n / M).\n        term_factor = 0.5 * k * A**2 * omega * dt\n        arg_factor = 4.0 * np.pi / M\n\n        for n in range(N):\n            # Calculate the work increment for step n.\n            # This numerically stable form avoids multiplying a large integer n\n            # by a small float dt inside the loop.\n            arg = float(n) * arg_factor\n            term = term_factor * np.sin(arg)\n\n            # --- Mode A: Naive Summation ---\n            sum_A += term\n\n            # --- Mode B: Kahan Compensated Summation ---\n            # Correct the next term by the accumulated error from the previous sum.\n            y = term - c_B\n            # Add the corrected term to the running sum.\n            # Low-order bits of y might be lost here.\n            t = sum_B + y\n            # Recover the lost bits. (t - sum_B) is the high-part of y that was\n            # actually added, so (t - sum_B) - y gives the negative of the low-part (the error).\n            c_B = (t - sum_B) - y\n            # Update the sum for the current iteration.\n            sum_B = t\n        \n        # Append the absolute values of the final sums to the results list.\n        results.append(abs(sum_A))\n        results.append(abs(sum_B))\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2423330"}]}