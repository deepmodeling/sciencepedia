{"hands_on_practices": [{"introduction": "The theoretical order of accuracy is a key selling point for any numerical method. This practice provides a foundational, hands-on opportunity to verify these theoretical claims. You will implement a general-purpose Runge-Kutta solver that reads its instructions from a Butcher tableau, allowing you to test a variety of methods, from the simple first-order Euler to the classical fourth-order scheme, and confirm that their global error $E(h)$ indeed scales as $h^p$. This exercise solidifies the essential skill of performing a grid refinement study to empirically measure convergence rates. [@problem_id:2376768]", "problem": "Write a complete program that, given several explicit Runge–Kutta (RK) methods in the form of Butcher tableaus, numerically verifies each method’s order of accuracy by testing on a manufactured solution of an initial value problem for an Ordinary Differential Equation (ODE). The manufactured solution is defined as follows: let the exact solution be $y(t)=\\exp(\\sin t)$ for $t \\in [0,1]$, where angles are in radians. The ODE is $y'(t)=f(t,y(t))$ with $f(t,y)=\\cos(t)\\,y$, the initial condition is $y(0)=1$, and the final time is $T=1$. Use the global error at the final time $t=T$ to quantify accuracy.\n\nUse the following explicit Runge–Kutta methods, each specified by its Butcher tableau $(A,b,c)$:\n- Method $1$ (Explicit Euler, expected order $1$): $s=1$, $A=\\begin{bmatrix}0\\end{bmatrix}$, $b=\\begin{bmatrix}1\\end{bmatrix}$, $c=\\begin{bmatrix}0\\end{bmatrix}$.\n- Method $2$ (Explicit Midpoint, expected order $2$): $s=2$, $A=\\begin{bmatrix}0 & 0\\\\ \\tfrac{1}{2} & 0\\end{bmatrix}$, $b=\\begin{bmatrix}0 & 1\\end{bmatrix}$, $c=\\begin{bmatrix}0 & \\tfrac{1}{2}\\end{bmatrix}$.\n- Method $3$ (Kutta’s third-order method, expected order $3$): $s=3$, $A=\\begin{bmatrix}0 & 0 & 0\\\\ \\tfrac{1}{2} & 0 & 0\\\\ -1 & 2 & 0\\end{bmatrix}$, $b=\\begin{bmatrix}\\tfrac{1}{6} & \\tfrac{2}{3} & \\tfrac{1}{6}\\end{bmatrix}$, $c=\\begin{bmatrix}0 & \\tfrac{1}{2} & 1\\end{bmatrix}$.\n- Method $4$ (Classical RK4, expected order $4$): $s=4$, $A=\\begin{bmatrix}0 & 0 & 0 & 0\\\\ \\tfrac{1}{2} & 0 & 0 & 0\\\\ 0 & \\tfrac{1}{2} & 0 & 0\\\\ 0 & 0 & 1 & 0\\end{bmatrix}$, $b=\\begin{bmatrix}\\tfrac{1}{6} & \\tfrac{1}{3} & \\tfrac{1}{3} & \\tfrac{1}{6}\\end{bmatrix}$, $c=\\begin{bmatrix}0 & \\tfrac{1}{2} & \\tfrac{1}{2} & 1\\end{bmatrix}$.\n\nFor each method, perform time integration on $[0,1]$ using uniform time steps of size $h=1/N$ for $N$ in the test suite $\\{10,20,40,80,160,320\\}$. For each $N$, compute the numerical approximation $y_N$ at $t=1$, compute the global error $E(h)=\\lvert y_N - y(1)\\rvert$, and then estimate the observed order $p$ as the least-squares slope of the line fitting the data $(\\log h, \\log E(h))$ across all values of $N$ in the test suite. Use the natural logarithm for $\\log$.\n\nYour program must output, in a single line, a comma-separated list enclosed in square brackets containing the four estimated orders $(p_1,p_2,p_3,p_4)$ for Methods $1$ through $4$, respectively, each rounded to two decimal places. No other text should be printed.\n\nTest Suite and Answer Specification:\n- The test suite consists of the four methods above, each tested with $N \\in \\{10,20,40,80,160,320\\}$.\n- The final answers are the four floats $p_1$, $p_2$, $p_3$, $p_4$, each being the observed order estimate for the corresponding method.\n- The final output format must be exactly a single line of the form `[p1,p2,p3,p4]`, where each $p_k$ is rounded to two decimal places and printed as a decimal number.", "solution": "The problem statement has been rigorously analyzed. It is scientifically grounded, well-posed, and contains all necessary information for a unique and meaningful solution. The specified ordinary differential equation, its manufactured analytical solution, the definitions of the Runge-Kutta methods via their Butcher tableaus, and the procedure for numerical verification of the order of accuracy are all standard, correct, and self-consistent. The problem is valid. We will now construct the solution.\n\nThe fundamental task is to solve an initial value problem (IVP) of the form:\n$$ y'(t) = f(t, y(t)), \\quad y(t_0) = y_0 $$\nfor $t \\in [t_0, T]$. The problem provides the specific function $f(t, y) = \\cos(t) y$, the initial condition $y(0) = 1$, and the time interval $[0, 1]$. The exact solution is given as $y(t) = \\exp(\\sin t)$, which is readily verified by differentiation: $y'(t) = \\exp(\\sin t) \\cdot \\cos(t) = y(t)\\cos(t)$, and checking the initial condition: $y(0) = \\exp(\\sin 0) = \\exp(0) = 1$.\n\nAn $s$-stage explicit Runge-Kutta (RK) method approximates the solution by stepping forward in time with a step size $h$. From the solution $y_n$ at time $t_n$, the solution $y_{n+1}$ at time $t_{n+1} = t_n + h$ is calculated. The method is defined by a set of coefficients arranged in a Butcher tableau:\n$$\n\\begin{array}{c|c}\nc & A \\\\\n\\hline\n  & b^T\n\\end{array} \\quad \\text{where } c \\in \\mathbb{R}^s, b \\in \\mathbb{R}^s, A \\in \\mathbb{R}^{s \\times s}\n$$\nFor an explicit method, the matrix $A$ is strictly lower triangular, meaning $a_{ij} = 0$ for $j \\ge i$. The computation proceeds in stages. First, the $s$ stage derivatives, $k_i$, are computed for $i=1, 2, \\dots, s$:\n$$ k_i = f\\left(t_n + c_i h, y_n + h \\sum_{j=1}^{i-1} a_{ij} k_j\\right) $$\nThe solution is then advanced using a weighted average of these stage derivatives:\n$$ y_{n+1} = y_n + h \\sum_{i=1}^{s} b_i k_i $$\n\nThe accuracy of a numerical method is characterized by its order of convergence, $p$. For a method of order $p$, the global error at a fixed final time $T$, denoted $E(h)$, is expected to decrease with the step size $h$ according to the relationship:\n$$ E(h) = |y_N - y(T)| \\approx C h^p $$\nwhere $y_N$ is the numerical solution at $T=Nh$ and $C$ is a constant that depends on the method and the problem, but not on $h$.\n\nTo numerically verify the order $p$, we can transform this relationship by taking the natural logarithm of both sides:\n$$ \\ln(E(h)) \\approx \\ln(C) + p \\ln(h) $$\nThis equation is of the form $Y = mX + B$, where $Y = \\ln(E(h))$, $X = \\ln(h)$, the slope is $m = p$, and the intercept is $B = \\ln(C)$. This linear relationship implies that a plot of $\\ln(E(h))$ versus $\\ln(h)$ will approximate a straight line whose slope is the order of the method, $p$.\n\nThe specified procedure is as follows:\n$1$. For each of the four given RK methods, a series of numerical integrations must be performed over the interval $[0, 1]$.\n$2$. The integrations will use a sequence of decreasing step sizes $h = 1/N$, for $N \\in \\{10, 20, 40, 80, 160, 320\\}$.\n$3$. For each integration with a specific step size $h$, the numerical approximation at the final time, $y_N$, is computed.\n$4$. The global error is calculated as $E(h) = |y_N - y(1)|$, where the exact value is $y(1) = \\exp(\\sin 1)$.\n$5$. After computing the errors for all step sizes, the data pairs $(\\ln(h_i), \\ln(E(h_i)))$ are collected.\n$6$. A linear least-squares regression is performed on these data points. The slope of the resulting best-fit line provides the experimental estimate of the order of accuracy, $p$. The slope $p$ for a set of data points $(x_i, y_i)$ is given by:\n$$ p = \\frac{\\sum_{i=1}^{M} (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^{M} (x_i - \\bar{x})^2} $$\nwhere $x_i = \\ln(h_i)$, $y_i = \\ln(E(h_i))$, $\\bar{x}$ and $\\bar{y}$ are the mean values, and $M=6$ is the number of step sizes in the test suite.\n\nThis procedure will be implemented for each of the four Butcher tableaus provided, yielding four estimated orders of accuracy, $(p_1, p_2, p_3, p_4)$, which are expected to be close to their theoretical values of $1, 2, 3,$ and $4$, respectively. The final result will be these four values, rounded to two decimal places.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Numerically verifies the order of accuracy of several explicit Runge-Kutta methods.\n    \"\"\"\n    # Define the Ordinary Differential Equation and its analytical solution\n    f = lambda t, y: np.cos(t) * y\n    t_start = 0.0\n    y_start = 1.0\n    t_end = 1.0\n    \n    # Pre-calculate the exact solution at the final time for error computation\n    y_exact_final = np.exp(np.sin(t_end))\n\n    # Define the Butcher tableaus for the four RK methods\n    methods = [\n        {\n            # Method 1: Explicit Euler (Order 1)\n            'A': np.array([[0.0]]),\n            'b': np.array([1.0]),\n            'c': np.array([0.0])\n        },\n        {\n            # Method 2: Explicit Midpoint (Order 2)\n            'A': np.array([[0.0, 0.0], [0.5, 0.0]]),\n            'b': np.array([0.0, 1.0]),\n            'c': np.array([0.0, 0.5])\n        },\n        {\n            # Method 3: Kutta's third-order method (Order 3)\n            'A': np.array([[0.0, 0.0, 0.0], [0.5, 0.0, 0.0], [-1.0, 2.0, 0.0]]),\n            'b': np.array([1.0/6.0, 2.0/3.0, 1.0/6.0]),\n            'c': np.array([0.0, 0.5, 1.0])\n        },\n        {\n            # Method 4: Classical RK4 (Order 4)\n            'A': np.array([\n                [0.0, 0.0, 0.0, 0.0],\n                [0.5, 0.0, 0.0, 0.0],\n                [0.0, 0.5, 0.0, 0.0],\n                [0.0, 0.0, 1.0, 0.0]\n            ]),\n            'b': np.array([1.0/6.0, 1.0/3.0, 1.0/3.0, 1.0/6.0]),\n            'c': np.array([0.0, 0.5, 0.5, 1.0])\n        }\n    ]\n\n    # Test suite of step counts\n    N_values = [10, 20, 40, 80, 160, 320]\n    h_values = np.array([1.0 / N for N in N_values])\n\n    estimated_orders = []\n\n    for method in methods:\n        A, b, c = method['A'], method['b'], method['c']\n        s = len(b)  # Number of stages\n        errors = []\n\n        for N in N_values:\n            h = (t_end - t_start) / N\n            y_current = y_start\n            \n            # Time integration loop\n            for n in range(N):\n                t_n = t_start + n * h\n                k_stages = np.zeros(s)\n                \n                # Calculate stage derivatives k_i\n                for i in range(s):\n                    stage_sum = 0.0\n                    for j in range(i):\n                        stage_sum += A[i, j] * k_stages[j]\n                    \n                    y_stage_input = y_current + h * stage_sum\n                    t_stage_input = t_n + c[i] * h\n                    k_stages[i] = f(t_stage_input, y_stage_input)\n                \n                # Update solution\n                y_current += h * np.dot(b, k_stages)\n            \n            # Store the global error at t=T\n            errors.append(np.abs(y_current - y_exact_final))\n\n        # Use natural logarithm for the log-log plot\n        log_h = np.log(h_values)\n        log_E = np.log(np.array(errors))\n        \n        # Perform linear regression (polynomial fit of degree 1)\n        # The slope of the line is the estimated order of accuracy\n        # np.polyfit returns [slope, intercept]\n        slope = np.polyfit(log_h, log_E, 1)[0]\n        estimated_orders.append(slope)\n        \n    # Format the output as specified: [p1,p2,p3,p4]\n    formatted_results = [f'{p:.2f}' for p in estimated_orders]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "2376768"}, {"introduction": "Theoretical convergence rates are derived under the assumption that the exact solution is sufficiently smooth. This exercise challenges that assumption by presenting an ODE whose solution contains a finite-time singularity, or \"blow-up\". As your numerical integration approaches this singularity, the derivatives of the solution grow without bound, causing the constant in the error term to explode and the observed order of accuracy to degrade. This crucial test case demonstrates that the effective order of a method is a property of both the method and the problem being solved, highlighting the need for analytical insight before computation. [@problem_id:2422962]", "problem": "Consider the initial value problem for an ordinary differential equation (ODE): find a function $y(t)$ such that $y'(t) = y(t)^2$ with $y(0) = 1$. The exact solution is $y(t) = \\frac{1}{1 - t}$ for $t < 1$, which has a finite-time singularity at $t = 1$.\n\nLet the numerical approximation to $y(t)$ be computed by the classical explicit fourth-order Runge–Kutta method (often abbreviated as RK4), applied with a uniform time step $h$ over the interval $[0,T]$ starting from $y(0)=1$, producing the final numerical value $Y_h(T)$ at time $T$. The theoretical global order of accuracy for this method, when applied where the exact solution and its derivatives remain bounded, is $4$.\n\nDefine the observed order of accuracy at a final time $T \\in (0,1)$ as follows. For three step sizes $h_i = T/N_i$ with $N_i \\in \\{N_0, 2N_0, 4N_0\\}$, compute the corresponding absolute global errors\n$$\nE(h_i;T) = \\left|Y_{h_i}(T) - y(T)\\right|.\n$$\nThen the observed order $p_{\\mathrm{obs}}(T)$ is the slope of the best affine fit (in the least-squares sense) of the points $(\\log h_i, \\log E(h_i;T))$, $i=1,2,3$.\n\nYour task is to write a complete program that, for each test case specified below, computes $p_{\\mathrm{obs}}(T)$ according to the definition above. The classical explicit fourth-order Runge–Kutta method must be used to compute $Y_{h}(T)$, and the exact solution $y(T) = \\frac{1}{1-T}$ must be used to evaluate $E(h;T)$.\n\nTest suite:\n- Case 1 (happy path): $T = 0.5$, $N_0 = 16$.\n- Case 2 (closer to singularity): $T = 0.9$, $N_0 = 64$.\n- Case 3 (near-singular): $T = 0.99$, $N_0 = 512$.\n- Case 4 (very near-singular): $T = 0.999$, $N_0 = 4096$.\n\nAll computations are dimensionless. Angles are not involved. No percentages are required.\n\nFinal output format:\nYour program should produce a single line of output containing the four observed orders $p_{\\mathrm{obs}}(T)$ for the test cases in the same order as listed above, rounded to three decimal places, as a comma-separated list enclosed in square brackets. For example, a valid output would look like `[4.000,3.987,3.452,2.718]` (this is only an example; your program must compute the actual values).", "solution": "We analyze the initial value problem $y'(t) = y(t)^2$ with $y(0) = 1$. The exact solution follows by separation of variables: $\\frac{dy}{dt} = y^2$ implies $\\frac{dy}{y^2} = dt$, integrating gives $-1/y = t + C$, and the initial condition $y(0)=1$ yields $C = -1$. Therefore $y(t) = \\frac{1}{1 - t}$ for $t < 1$, and there is a finite-time blow-up (singularity) at $t=1$.\n\nFor the numerical approximation, we use the classical explicit fourth-order Runge–Kutta method (RK4). For a given step size $h$ and current state $(t_n, y_n)$, define stage slopes $k_1, k_2, k_3, k_4$ by\n$$\n\\begin{aligned}\nk_1 &= f(t_n, y_n), \\\\\nk_2 &= f\\left(t_n + \\frac{h}{2}, y_n + \\frac{h}{2} k_1\\right), \\\\\nk_3 &= f\\left(t_n + \\frac{h}{2}, y_n + \\frac{h}{2} k_2\\right), \\\\\nk_4 &= f\\left(t_n + h, y_n + h k_3\\right),\n\\end{aligned}\n$$\nwith $f(t,y) = y^2$. The update is\n$$\ny_{n+1} = y_n + \\frac{h}{6}\\left(k_1 + 2k_2 + 2k_3 + k_4 \\right), \\quad t_{n+1} = t_n + h.\n$$\nAdvancing $N = T/h$ uniform steps from $t_0 = 0$ with $y_0 = 1$ yields $Y_h(T) = y_N$.\n\nTo quantify the observed order of accuracy at a fixed final time $T$, we rely on the definition $E(h;T) = \\left|Y_h(T) - y(T)\\right|$, where $y(T) = \\frac{1}{1 - T}$. For a method of theoretical order $p_{\\mathrm{th}}$, and sufficiently smooth solutions, the asymptotic behavior is $E(h;T) \\approx C(T)\\, h^{p_{\\mathrm{th}}}$ as $h \\to 0$, with a constant $C(T)$ that depends on bounds of derivatives of the exact solution on $[0,T]$. The classical RK4 has $p_{\\mathrm{th}} = 4$. However, near a singularity the derivatives of $y(t)$ grow rapidly: by direct differentiation or induction one finds\n$$\ny^{(m)}(t) = \\frac{m!}{\\left(1 - t\\right)^{m+1}},\n$$\nso for fixed $T$ close to $1$, the coefficients in the local truncation error increase with powers of $(1-T)^{-1}$. The local truncation error for RK4 is $\\mathcal{O}(h^5)$ with a coefficient involving $y^{(5)}(t)$ and lower derivatives; consequently, the constant $C(T)$ in the global error behaves like a positive multiple of $(1-T)^{-5}$ (up to multiplicative factors from the dynamics). Hence, as $T \\to 1^-$, even moderately small $h$ may not yet be in the asymptotic regime where a clear slope of $4$ is observed on a log-log plot. This phenomenon manifests as a breakdown of the numerically computed order of accuracy when measuring over a fixed set of step sizes.\n\nTo compute the observed order from first principles, we take three step sizes $h_i = T/N_i$ with $N_i \\in \\{N_0, 2N_0, 4N_0\\}$, compute the corresponding errors $E(h_i;T)$, and then define $p_{\\mathrm{obs}}(T)$ to be the slope of the least-squares affine fit to the points $(\\log h_i, \\log E(h_i;T))$. Explicitly, letting $x_i = \\log h_i$ and $y_i = \\log E(h_i;T)$ for $i=1,2,3$, the slope $p_{\\mathrm{obs}}(T)$ is\n$$\np_{\\mathrm{obs}}(T) = \\frac{\\sum_{i=1}^3 (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^3 (x_i - \\bar{x})^2}, \\quad\n\\bar{x} = \\frac{1}{3}\\sum_{i=1}^3 x_i, \\quad \\bar{y} = \\frac{1}{3}\\sum_{i=1}^3 y_i.\n$$\nThis slope is invariant to the base of the logarithm.\n\nThe test suite specifies four pairs $(T, N_0)$: $(0.5, 16)$, $(0.9, 64)$, $(0.99, 512)$, and $(0.999, 4096)$. For each $T$, the three meshes use $N \\in \\{N_0, 2N_0, 4N_0\\}$ steps, giving step sizes $h = T/N$. These choices ensure uniform partitioning of $[0,T]$ while probing increasingly fine resolutions. The numerical integration with RK4 yields $Y_h(T)$, and the exact solution $y(T) = 1/(1 - T)$ provides the true value for error computation.\n\nWe expect the following qualitative behavior:\n- For $T = 0.5$, the solution and its derivatives are modest in magnitude on $[0, 0.5]$, so $C(T)$ is moderate, and the observed order $p_{\\mathrm{obs}}(0.5)$ should be close to $4$.\n- As $T$ increases to $0.9$, $0.99$, and $0.999$, the magnitudes of higher derivatives on $[0,T]$ become large, inflating $C(T)$ and making the finite set of step sizes less representative of the asymptotic regime. Consequently, the measured slope $p_{\\mathrm{obs}}(T)$ typically decreases below $4$, with the most pronounced reduction near $T = 0.999$.\n\nThe program implements these steps: numerical advancement by RK4 for each mesh, evaluation of absolute errors using the exact solution, least-squares slope computation for $(\\log h, \\log E)$, and output of the four observed orders, rounded to three decimal places, in the specified single-line list format.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef rk4_final_value(f, y0, T, N):\n    \"\"\"\n    Advance the ODE y' = f(t,y) from t=0 to t=T using N uniform RK4 steps.\n    Returns the final value y_N at t = T.\n    \"\"\"\n    h = T / N\n    t = 0.0\n    y = y0\n    for _ in range(N):\n        k1 = f(t, y)\n        k2 = f(t + 0.5 * h, y + 0.5 * h * k1)\n        k3 = f(t + 0.5 * h, y + 0.5 * h * k2)\n        k4 = f(t + h, y + h * k3)\n        y = y + (h / 6.0) * (k1 + 2.0 * k2 + 2.0 * k3 + k4)\n        t += h\n    return y\n\ndef observed_order_at_T(T, N0):\n    \"\"\"\n    Compute the observed order p_obs(T) using three meshes with N in {N0, 2N0, 4N0}.\n    \"\"\"\n    # Define the ODE\n    def f(t, y):\n        return y * y\n\n    exact = 1.0 / (1.0 - T)\n\n    Ns = [N0, 2 * N0, 4 * N0]\n    hs = []\n    errs = []\n    for N in Ns:\n        h = T / N\n        yN = rk4_final_value(f, 1.0, T, N)\n        err = abs(yN - exact)\n        hs.append(h)\n        errs.append(err)\n\n    hs = np.array(hs, dtype=float)\n    errs = np.array(errs, dtype=float)\n\n    # Guard against log(0) by adding a tiny floor (does not affect realistic errors)\n    tiny = 1e-300\n    x = np.log(hs)\n    y = np.log(errs + tiny)\n\n    x_mean = np.mean(x)\n    y_mean = np.mean(y)\n    denom = np.sum((x - x_mean) ** 2)\n    # In the degenerate case (should not happen for distinct hs), fall back to 0\n    if denom == 0.0:\n        return 0.0\n    slope = np.sum((x - x_mean) * (y - y_mean)) / denom\n    # The slope of log(error) vs log(h) is the observed order\n    return slope\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each case is a tuple (T, N0)\n    test_cases = [\n        (0.5, 16),     # happy path\n        (0.9, 64),     # closer to singularity\n        (0.99, 512),   # near-singular\n        (0.999, 4096), # very near-singular\n    ]\n\n    results = []\n    for T, N0 in test_cases:\n        p_obs = observed_order_at_T(T, N0)\n        # Round to three decimal places for output formatting\n        results.append(f\"{p_obs:.3f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "2422962"}, {"introduction": "In professional practice, estimating the order of accuracy is rarely a simple textbook exercise; real-world error data can be noisy or may not yet be in the clean asymptotic regime. This capstone practice moves beyond naive estimation and asks you to build a robust computational tool that not only estimates the order $p$ but also programmatically checks whether that estimate is reliable. By implementing a suite of quantitative checks for data quality—from monitoring monotonicity to assessing goodness-of-fit—you will learn to automate the critical analysis required to confidently validate numerical code. [@problem_id:2422984]", "problem": "You are asked to write a complete, runnable program that estimates the order of accuracy $p$ of a numerical method from a series of measured errors at different step sizes, and that determines whether the estimate is reliable. The task must be carried out in a principled way grounded in the standard definition of asymptotic error scaling: for a consistent method of order $p$, the global error $e(h)$ satisfies $e(h) \\approx C h^{p}$ for sufficiently small step size $h$, where $C$ is an $h$-independent constant. This scaling relationship is a foundational model used in computational physics for quantifying convergence behavior of numerical methods.\n\nYour program must implement a routine that takes as input a list of step sizes $h_i$ and a corresponding list of measured errors $e_i$ (both strictly positive), and returns an estimate $\\hat{p}$ along with a boolean flag indicating whether the estimate is reliable. The design should be based on core definitions and well-tested facts, not on shortcut formulas given by this problem statement. The reliability flag must be based on explicit, quantitative checks that guard against common failure modes such as insufficient dynamic range, non-monotone step-size sequences, non-asymptotic regimes, noise, and round-off plateaus.\n\nUse the following reliability criteria and thresholds:\n- Positivity: all $h_i$ and $e_i$ must be strictly positive; otherwise, the estimate is unreliable.\n- Step-size monotonicity: the $h_i$ must be strictly decreasing in the order provided; if not, the estimate is unreliable.\n- Dynamic range: the ratio $\\max_i h_i / \\min_i h_i \\ge 8$; otherwise, the estimate is unreliable.\n- Error monotonicity with tolerance: if $e_{i+1} / e_i > 1 + \\tau$ for any adjacent pair in the given order, where $\\tau = 0.05$, the estimate is unreliable.\n- Goodness of fit: if you fit a straight line to transformed data implied by the foundational scaling law, the coefficient of determination $R^2$ must satisfy $R^2 \\ge 0.995$; otherwise, the estimate is unreliable.\n- Slope consistency: compute pairwise local slopes on a properly ordered sequence implied by the foundational scaling law; if the relative standard deviation of these slopes exceeds $0.15$, the estimate is unreliable.\n- Round-off plateau heuristic: on the most refined two levels of a properly ordered sequence, if the step-size ratio satisfies $h_{n}/h_{n-1} \\le 0.7$ but the error ratio satisfies $e_{n}/e_{n-1} > 0.8$, the estimate is unreliable.\n\nYour program should:\n- Implement a principled estimator for $\\hat{p}$ derived from the foundational scaling model above.\n- Produce both $\\hat{p}$ and a reliability flag for each test case.\n- Round $\\hat{p}$ to three decimal places in the final printed output.\n\nYour program should run with no user input and must process the following test suite (each case is a pair of corresponding lists $h$ and $e$):\n- Case A (clean second order): $h = [0.4, 0.2, 0.1, 0.05]$, $e = [0.048, 0.012, 0.003, 0.00075]$.\n- Case B (first order with small noise): $h = [0.4, 0.2, 0.1, 0.05]$, $e = [0.2, 0.102, 0.049, 0.02525]$.\n- Case C (non-monotone step sizes): $h = [0.2, 0.1, 0.12, 0.06]$, $e = [0.4, 0.2, 0.24, 0.12]$.\n- Case D (insufficient dynamic range): $h = [0.1, 0.095, 0.09, 0.085]$, $e = [0.001, 0.000857375, 0.000729, 0.000614125]$.\n- Case E (apparent round-off plateau at finest levels): $h = [0.2, 0.1, 0.05, 0.025, 0.0125]$, $e = [0.004, 0.001, 0.00025, 0.0000625, 0.00006]$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each test case result must be a two-element list containing the estimated order $\\hat{p}$ (rounded to three decimal places) and the reliability flag (boolean). For example, an output with three cases could look like `[[2.000,True],[1.003,True],[1.998,False]]`; your program must follow the same bracketed, comma-separated structure and use Python’s boolean literals `True` and `False`.\n\nNo physical units or angles are involved in this task. All numeric values in the final output must be purely numeric floats, rounded to three decimal places for $\\hat{p}$, and booleans for the reliability flags.", "solution": "The problem posed is to estimate the order of accuracy, $p$, of a numerical method given a set of step sizes, $h_i$, and corresponding errors, $e_i$. This task is fundamental to the validation of numerical schemes in computational science. The problem is well-defined, scientifically grounded, and objective. It provides a clear theoretical model, a set of quantitative reliability criteria, and specific test data. It is therefore a valid problem.\n\nThe solution will be constructed based on the foundational scaling relationship for the global error $e(h)$ of a consistent numerical method of order $p$:\n$$e(h) \\approx C h^{p}$$\nwhere $C$ is a constant independent of the step size $h$, for sufficiently small $h$.\n\nTo estimate $p$ from a discrete set of data points $(h_i, e_i)$, we can linearize this relationship by taking the natural logarithm of both sides:\n$$\\ln(e) \\approx \\ln(C) + p \\ln(h)$$\nThis equation is in the form of a straight line, $y = b + mx$, where $y = \\ln(e)$, $x = \\ln(h)$, the slope is $m = p$, and the y-intercept is $b = \\ln(C)$.\n\nThus, the most principled method to estimate $p$ from a series of measurements is to perform a simple linear regression on the log-transformed data $(\\ln(h_i), \\ln(e_i))$. The slope of the best-fit line is the desired estimate, $\\hat{p}$. We will use a standard linear regression algorithm to find this slope. The coefficient of determination, $R^2$, from this same regression provides a measure of how well the data conforms to the linear model in log-log space, which is one of the required reliability checks.\n\nThe implementation will proceed in two stages for each test case: first, a rigorous validation of the input data against all specified reliability criteria, and second, the computation of $\\hat{p}$. A single boolean flag, initialized to true, will be set to false if any reliability criterion is not met.\n\nThe reliability checks are as follows:\n1.  **Positivity**: The inputs $h_i$ and $e_i$ must all be strictly positive, i.e., $h_i > 0$ and $e_i > 0$ for all $i$. This is a prerequisite for the logarithmic transformation. If this check fails, the regression cannot be performed, and we will assign $\\hat{p} = 0.0$ as a default and mark the result as unreliable.\n2.  **Step-size monotonicity**: The provided sequence of step sizes, $h_i$, must be strictly decreasing. That is, $h_{i+1} < h_i$ for all adjacent pairs. This ensures the data represents a systematic mesh refinement study.\n3.  **Dynamic range**: The ratio of the largest to the smallest step size, $\\max(h) / \\min(h)$, must be at least $8$. A sufficiently large dynamic range is necessary to reliably estimate an asymptotic property like the order of accuracy.\n4.  **Error monotonicity with tolerance**: The error should not increase significantly as the step size decreases. Specifically, for any adjacent pair of points $(h_i, e_i)$ and $(h_{i+1}, e_{i+1})$ in the given (and now validated as decreasing) order of $h$, the error ratio must satisfy $e_{i+1} / e_i \\le 1 + \\tau$, where $\\tau = 0.05$. A violation suggests non-convergent behavior or severe data contamination.\n5.  **Goodness of fit**: After performing the linear regression on $(\\ln(h_i), \\ln(e_i))$, the coefficient of determination $R^2$ must be at least $0.995$. A high $R^2$ value confirms that the data closely follows the expected linear trend in log-log space.\n6.  **Slope consistency**: We compute pairwise \"local\" orders of accuracy, $p_{i, i+1}$, for each adjacent pair of data points:\n    $$p_{i, i+1} = \\frac{\\ln(e_{i+1}) - \\ln(e_i)}{\\ln(h_{i+1}) - \\ln(h_i)}$$\n    The relative standard deviation (RSD) of these local slopes, defined as the standard deviation divided by the absolute value of the mean, must not exceed $0.15$. A low RSD indicates that the order of accuracy is stable across the tested step sizes, which is a strong indicator that the measurements are in the asymptotic convergence regime.\n7.  **Round-off plateau heuristic**: For the final two data points, representing the most refined calculation, we check for a characteristic sign of round-off error dominating truncation error. If the step-size ratio $h_n / h_{n-1} \\le 0.7$ (a significant refinement) while the error ratio $e_n / e_{n-1} > 0.8$ (a disproportionately small error reduction), the result is deemed unreliable. Here, $n$ refers to the index of the last element in the sequence.\n\nThe final estimate $\\hat{p}$ will be the slope obtained from the linear regression over all data points. The reliability flag will be the logical AND of the outcomes of all seven checks. The program will process each test case provided in the problem statement and format the output as a single list of results, with each result containing the estimated order $\\hat{p}$ rounded to three decimal places and the final reliability flag.", "answer": "```python\nimport numpy as np\nfrom scipy import stats\n\ndef estimate_order_of_accuracy(h, e, thresholds):\n    \"\"\"\n    Estimates the order of accuracy p and its reliability from experimental data.\n\n    Args:\n        h (list or np.ndarray): List of step sizes.\n        e (list or np.ndarray): List of corresponding errors.\n        thresholds (dict): A dictionary of thresholds for reliability checks.\n\n    Returns:\n        tuple: A tuple containing the estimated order p (float) and a\n               reliability flag (bool).\n    \"\"\"\n    h = np.array(h, dtype=float)\n    e = np.array(e, dtype=float)\n    is_reliable = True\n    num_points = len(h)\n\n    # If there are not enough points for regression or checks, it is unreliable.\n    if num_points  2:\n        return 0.0, False\n\n    # Check 1: Positivity\n    if np.any(h = 0) or np.any(e = 0):\n        # Log-transform is impossible, cannot compute p via regression.\n        # Return a default p=0 and mark as unreliable.\n        return 0.0, False\n\n    # Check 2: Step-size monotonicity\n    # np.diff(h) computes h[i+1] - h[i]. It must be negative for all i.\n    if np.any(np.diff(h) >= 0):\n        is_reliable = False\n\n    # Check 3: Dynamic range\n    if np.max(h) / np.min(h)  thresholds['dynamic_range_h']:\n        is_reliable = False\n\n    # Check 4: Error monotonicity with tolerance\n    # e_ratios = e[1:] / e[:-1] for decreasing h.\n    if num_points > 1:\n        e_ratios = e[1:] / e[:-1]\n        if np.any(e_ratios > 1.0 + thresholds['error_mono_tau']):\n            is_reliable = False\n\n    # Perform linear regression on log-log data to estimate p.\n    log_h = np.log(h)\n    log_e = np.log(e)\n\n    # Use scipy.stats.linregress to get slope (p_hat) and R-value.\n    p_hat, _, r_value, _, _ = stats.linregress(log_h, log_e)\n\n    # Check 5: Goodness of fit (R^2)\n    r_squared = r_value**2\n    if r_squared  thresholds['r_squared_min']:\n        is_reliable = False\n\n    # Check 6: Slope consistency\n    if num_points > 2:\n        # local_slopes = d(log_e)/d(log_h)\n        local_slopes = np.diff(log_e) / np.diff(log_h)\n        if np.abs(np.mean(local_slopes)) > 1e-9: # Avoid division by zero\n            rsd = np.std(local_slopes) / np.abs(np.mean(local_slopes))\n            if rsd > thresholds['rsd_slope_max']:\n                is_reliable = False\n        # If mean is near zero, high RSD is likely, but let's check for stability.\n        # A near-zero mean slope for a converging method is odd and likely unreliable anyway.\n        elif np.std(local_slopes) > 1e-2: # Unstable even if mean is zero\n            is_reliable = False\n            \n    # Check 7: Round-off plateau heuristic\n    if num_points > 1:\n        h_ratio_last = h[-1] / h[-2]\n        e_ratio_last = e[-1] / e[-2]\n        if (h_ratio_last = thresholds['roundoff_h_ratio'] and \n            e_ratio_last > thresholds['roundoff_e_ratio']):\n            is_reliable = False\n\n    return p_hat, is_reliable\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    test_cases = [\n        # Case A (clean second order)\n        {'h': [0.4, 0.2, 0.1, 0.05], 'e': [0.048, 0.012, 0.003, 0.00075]},\n        # Case B (first order with small noise)\n        {'h': [0.4, 0.2, 0.1, 0.05], 'e': [0.2, 0.102, 0.049, 0.02525]},\n        # Case C (non-monotone step sizes)\n        {'h': [0.2, 0.1, 0.12, 0.06], 'e': [0.4, 0.2, 0.24, 0.12]},\n        # Case D (insufficient dynamic range)\n        {'h': [0.1, 0.095, 0.09, 0.085], 'e': [0.001, 0.000857375, 0.000729, 0.000614125]},\n        # Case E (apparent round-off plateau at finest levels)\n        {'h': [0.2, 0.1, 0.05, 0.025, 0.0125], 'e': [0.004, 0.001, 0.00025, 0.0000625, 0.00006]}\n    ]\n\n    thresholds = {\n        'dynamic_range_h': 8,\n        'error_mono_tau': 0.05,\n        'r_squared_min': 0.995,\n        'rsd_slope_max': 0.15,\n        'roundoff_h_ratio': 0.7,\n        'roundoff_e_ratio': 0.8\n    }\n\n    results = []\n    for case in test_cases:\n        p_est, is_rel = estimate_order_of_accuracy(case['h'], case['e'], thresholds)\n        results.append((p_est, is_rel))\n\n    # Format output exactly as specified, with no spaces in the inner lists.\n    formatted_results = []\n    for p_hat, is_rel in results:\n        p_str = f\"{p_hat:.3f}\"\n        # Python's str(bool) is 'True' or 'False' with a capital letter. This is correct.\n        formatted_results.append(f\"[{p_str},{is_rel}]\")\n    \n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n\n```", "id": "2422984"}]}