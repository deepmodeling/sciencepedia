{"hands_on_practices": [{"introduction": "Why do iterative methods sometimes converge slowly? While the method's properties are key, the nature of the problem itself plays a crucial role. In this practice [@problem_id:2382749], you will design a \"malicious\" right-hand side vector for a linear system that deliberately excites the slowest-converging error modes of the Jacobi method. This exercise provides a deep, hands-on understanding of how error components are damped and why certain problems pose a stiff challenge for simple iterative solvers.", "problem": "Consider the one-dimensional Laplace boundary value problem on the unit interval with homogeneous Dirichlet boundary conditions. Using the standard second-order central finite difference discretization on a uniform grid with $N$ interior nodes, the resulting linear system is $A u = b$, where $A \\in \\mathbb{R}^{N \\times N}$ is the symmetric tridiagonal matrix with entries $A_{i,i} = 2$, $A_{i,i+1} = -1$, and $A_{i,i-1} = -1$ for $i = 1, 2, \\dots, N$, with the convention that entries outside the range are zero.\n\nYour task is to construct, for each specified $N$, a right-hand side vector $b \\in \\mathbb{R}^{N}$ that is “malicious” in the sense that it maximizes the number of iterations needed by the classical Jacobi method, when started from the zero initial vector, to meet a given stopping criterion, subject to the normalization constraint $\\|b\\|_{2} = 1$ (Euclidean two-norm equal to $1$). For each test case, use the Jacobi method to solve $A u = b$ starting from $u^{(0)} = 0$, and report the minimal iteration count $k$ at which the specified stopping criterion is first satisfied, or a fixed cap if it is not satisfied within that many iterations.\n\nThe stopping criteria to be used are the following, each employing the Euclidean two-norm $\\|\\cdot\\|_{2}$:\n- Absolute residual criterion: stop at the smallest $k$ such that $\\|A u^{(k)} - b\\|_{2} \\le \\tau$, where $\\tau$ is the given tolerance.\n- Relative residual criterion: stop at the smallest $k$ such that $\\|A u^{(k)} - b\\|_{2} / \\|b\\|_{2} \\le \\tau$.\n- Successive iterate difference criterion: stop at the smallest $k \\ge 1$ such that $\\|u^{(k)} - u^{(k-1)}\\|_{2} \\le \\tau$.\n\nFor all test cases, set the maximum number of iterations to $k_{\\max} = 200000$. If the specified stopping criterion is not satisfied by iteration $k_{\\max}$, you must return $k_{\\max}$ for that test case. No physical units are involved. Angles, if any arise in your construction, must be treated in radians. All computations should be performed in real arithmetic.\n\nTest suite. For each tuple $(N, \\text{criterion}, \\tau)$ below, construct such a “malicious” vector $b$ with $\\|b\\|_{2} = 1$, run the Jacobi method from $u^{(0)} = 0$ using the specified stopping criterion and tolerance $\\tau$, and record the integer iteration count at stopping (or $k_{\\max}$ if not reached):\n1. $(N = 31, \\text{absolute residual}, \\tau = 10^{-8})$\n2. $(N = 4, \\text{relative residual}, \\tau = 10^{-8})$\n3. $(N = 17, \\text{absolute residual}, \\tau = 10^{-12})$\n4. $(N = 63, \\text{successive iterate difference}, \\tau = 10^{-10})$\n5. $(N = 5, \\text{absolute residual}, \\tau = 10^{-4})$\n6. $(N = 63, \\text{relative residual}, \\tau = 10^{-6})$\n\nFinal output format. Your program should produce a single line of output containing the six integer results, in the order of the test suite above, as a comma-separated list enclosed in square brackets (for example, $[r_{1},r_{2},r_{3},r_{4},r_{5},r_{6}]$), where each $r_{j}$ is the integer iteration count for the $j$-th test case.", "solution": "The problem requires the determination of the number of iterations for the Jacobi method to solve a linear system $A u = b$ under specific \"malicious\" conditions designed to maximize this count. The system arises from the finite difference discretization of the one-dimensional Laplace equation.\n\nFirst, we must validate the problem statement. The problem is a well-defined exercise in numerical linear algebra, specifically concerning the convergence properties of iterative methods. It is scientifically grounded, objective, and contains all necessary information to proceed. The matrix $A$ is the standard second-order discrete Laplacian operator, which is symmetric and positive definite. The Jacobi method is a classical iterative solver. The concept of a \"malicious\" vector is given a precise meaning: a vector $b$ with unit norm, $\\|b\\|_2 = 1$, that maximizes the number of iterations. The problem is valid.\n\nThe core of the problem lies in the construction of the \"malicious\" vector $b$. The convergence of the Jacobi method is governed by the spectral properties of its iteration matrix. The Jacobi iteration for the system $A u = b$ is given by $u^{(k+1)} = T_J u^{(k)} + D^{-1}b$, where $A = D - L - U$ is the decomposition of $A$ into its diagonal ($D$), strictly lower triangular ($-L$), and strictly upper triangular ($-U$) parts, and $T_J = D^{-1}(L+U)$ is the Jacobi iteration matrix.\n\nFor the given matrix $A$, we have $A_{i,i} = 2$ and $A_{i,i \\pm 1} = -1$. Thus, $D = 2I$, where $I$ is the identity matrix. The iteration matrix is $T_J = \\frac{1}{2}(L+U) = I - \\frac{1}{2}A$. Since $T_J$ is a polynomial in $A$, they share the same eigenvectors. The eigenvalues and eigenvectors of this specific matrix $A$ are well-known. The eigenvalues are\n$$ \\lambda_j(A) = 2 - 2\\cos\\left(\\frac{j\\pi}{N+1}\\right) = 4\\sin^2\\left(\\frac{j\\pi}{2(N+1)}\\right) \\quad \\text{for } j = 1, \\dots, N $$\nThe corresponding eigenvectors $v_j$ have components $(v_j)_i = \\sin\\left(\\frac{ij\\pi}{N+1}\\right)$ for $i=1, \\dots, N$. The eigenvalues of the Jacobi matrix $T_J$ are therefore\n$$ \\mu_j = 1 - \\frac{1}{2}\\lambda_j(A) = \\cos\\left(\\frac{j\\pi}{N+1}\\right) \\quad \\text{for } j = 1, \\dots, N $$\nThe asymptotic rate of convergence of the Jacobi method is determined by the spectral radius of $T_J$, which is $\\rho(T_J) = \\max_j |\\mu_j|$. Since $\\cos(x)$ is a decreasing function on $[0, \\pi]$, the maximum absolute value is achieved for $j=1$ and $j=N$:\n$$ \\rho(T_J) = |\\mu_N| = \\left|\\cos\\left(\\frac{N\\pi}{N+1}\\right)\\right| = \\left|-\\cos\\left(\\frac{\\pi}{N+1}\\right)\\right| = \\cos\\left(\\frac{\\pi}{N+1}\\right) = \\mu_1 $$\nThe convergence is slowest for error components aligned with the eigenvectors corresponding to these eigenvalues, namely $v_1$ (the smoothest eigenvector) and $v_N$ (the most oscillatory one).\n\nThe error at iteration $k$ is $e^{(k)} = u^{(k)} - u^*$, where $u^*=A^{-1}b$ is the true solution. It evolves as $e^{(k)} = T_J^k e^{(0)}$. With the initial guess $u^{(0)}=0$, the initial error is $e^{(0)} = -u^* = -A^{-1}b$. To maximize the number of iterations required to meet a stopping criterion, we must choose $b$ such that the error $e^{(k)}$ remains large for as long as possible. This is achieved by aligning the initial error, and thus $b$, with an eigenvector corresponding to an eigenvalue of magnitude $\\rho(T_J)$.\n\nWe choose the \"malicious\" vector $b$ to be the normalized eigenvector corresponding to $\\mu_1$, which we denote by $q_1$. The components of $b$ are given by:\n$$ b_i = \\sqrt{\\frac{2}{N+1}} \\sin\\left(\\frac{i\\pi}{N+1}\\right) \\quad \\text{for } i = 1, \\dots, N $$\nThis choice ensures $\\|b\\|_2 = 1$.\n\nNow, we analyze the stopping criteria for this choice of $b=q_1$.\nThe residual vector at iteration $k$ is $r^{(k)} = A u^{(k)} - b$. A useful relation is $r^{(k+1)} = T_J r^{(k)}$, which implies $r^{(k)} = (T_J)^k r^{(0)}$. With $u^{(0)}=0$, $r^{(0)}=-b=-q_1$. Since $q_1$ is an eigenvector of $T_J$, we have:\n$$ r^{(k)} = (T_J)^k (-q_1) = -\\mu_1^k q_1 $$\n1.  **Absolute/Relative Residual Criterion**: The norm of the residual is $\\|r^{(k)}\\|_2 = |\\mu_1|^k \\|q_1\\|_2 = (\\rho(T_J))^k$. Since $\\|b\\|_2=1$, the absolute and relative residual criteria are identical: $\\|A u^{(k)} - b\\|_2 \\le \\tau$. This simplifies to $(\\rho(T_J))^k \\le \\tau$. The number of iterations $k$ is the smallest integer satisfying this inequality.\n\n2.  **Successive Iterate Difference Criterion**: Starting from $u^{(0)}=0$, the $k$-th iterate is given by $u^{(k)} = \\sum_{i=0}^{k-1} (T_J)^i D^{-1}b$. For the malicious vector $b=q_1$, and since $D=2I$, we have $D^{-1}b = \\frac{1}{2}q_1$. The sum becomes $u^{(k)} = \\left(\\sum_{i=0}^{k-1} \\mu_1^i\\right) \\frac{1}{2}q_1 = \\frac{1-\\mu_1^k}{1-\\mu_1} \\frac{1}{2}q_1$. The difference between successive iterates is:\n    $$ u^{(k)} - u^{(k-1)} = \\left( \\frac{1-\\mu_1^k}{1-\\mu_1} - \\frac{1-\\mu_1^{k-1}}{1-\\mu_1} \\right) \\frac{1}{2}q_1 = \\frac{\\mu_1^{k-1} - \\mu_1^k}{1-\\mu_1} \\frac{1}{2}q_1 = \\frac{\\mu_1^{k-1}(1-\\mu_1)}{1-\\mu_1} \\frac{1}{2}q_1 = \\frac{1}{2}\\mu_1^{k-1}q_1 $$\n    The norm is therefore $\\|u^{(k)} - u^{(k-1)}\\|_2 = \\frac{1}{2}|\\mu_1|^{k-1}\\|q_1\\|_2 = \\frac{(\\rho(T_J))^{k-1}}{2}$. The criterion $\\|u^{(k)} - u^{(k-1)}\\|_2 \\le \\tau$ becomes $\\frac{(\\rho(T_J))^{k-1}}{2} \\le \\tau$. The number of iterations $k$ is the smallest integer $\\ge 1$ satisfying this.\n\nThe algorithm is as follows: For each test case $(N, \\text{criterion}, \\tau)$, construct the malicious vector $b$ for the given $N$. Then, apply the Jacobi iteration starting from $u^{(0)}=0$. At each step $k$, compute $u^{(k)}$ and check if the specified stopping criterion is met. The first $k$ that satisfies the criterion is the result. If the criterion is not met by $k = k_{\\max} = 200000$, the result is $k_{\\max}$.", "answer": "```python\nimport numpy as np\n\ndef solve_case(N, criterion, tau):\n    \"\"\"\n    Solves a single test case for the Jacobi iteration problem.\n\n    Args:\n        N (int): The number of interior grid points.\n        criterion (str): The stopping criterion ('absolute residual', \n                         'relative residual', or 'successive iterate difference').\n        tau (float): The tolerance for the stopping criterion.\n\n    Returns:\n        int: The number of iterations required for convergence, or k_max.\n    \"\"\"\n    k_max = 200000\n\n    # Construct the malicious vector b, which is the normalized eigenvector\n    # corresponding to the smallest eigenvalue of A (smoothest mode).\n    # This corresponds to the largest eigenvalue of the Jacobi iteration matrix T_J.\n    # The components are b_i = C * sin(i * pi / (N + 1)).\n    # The normalization constant C is sqrt(2 / (N + 1)).\n    C = np.sqrt(2.0 / (N + 1.0))\n    indices = np.arange(1, N + 1, dtype=np.float64)\n    b = C * np.sin(indices * np.pi / (N + 1.0))\n\n    # Initialize the Jacobi iteration\n    u_k = np.zeros(N, dtype=np.float64)\n    u_prev = np.zeros(N, dtype=np.float64)\n\n    # Pre-construct matrix A for residual calculation\n    if 'residual' in criterion:\n        A = np.diag(np.full(N, 2.0, dtype=np.float64)) + \\\n            np.diag(np.full(N - 1, -1.0, dtype=np.float64), k=1) + \\\n            np.diag(np.full(N - 1, -1.0, dtype=np.float64), k=-1)\n    \n    # Jacobi iteration loop\n    for k in range(1, k_max + 1):\n        if criterion == 'successive iterate difference':\n            u_prev = u_k\n\n        # Perform one Jacobi step: u_new = 0.5 * (u_{i-1} + u_{i+1} + b_i)\n        # Vectorized implementation for efficiency.\n        u_new = np.zeros(N, dtype=np.float64)\n        # Interior points\n        if N > 1:\n            u_new[1:-1] = 0.5 * (u_k[:-2] + u_k[2:] + b[1:-1])\n            # Boundary points (u_0 = u_{N+1} = 0)\n            u_new[0] = 0.5 * (u_k[1] + b[0])\n            u_new[-1] = 0.5 * (u_k[-2] + b[-1])\n        elif N == 1:\n            u_new[0] = 0.5 * b[0]\n            \n        u_k = u_new\n\n        # Check stopping criterion\n        stop = False\n        if criterion in ('absolute residual', 'relative residual'):\n            # For relative residual, ||b||_2 = 1, so it is the same as absolute.\n            residual_norm = np.linalg.norm(A @ u_k - b)\n            if residual_norm = tau:\n                stop = True\n        elif criterion == 'successive iterate difference':\n            diff_norm = np.linalg.norm(u_k - u_prev)\n            if diff_norm = tau:\n                stop = True\n        \n        if stop:\n            return k\n\n    return k_max\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    test_cases = [\n        (31, 'absolute residual', 1e-8),\n        (4, 'relative residual', 1e-8),\n        (17, 'absolute residual', 1e-12),\n        (63, 'successive iterate difference', 1e-10),\n        (5, 'absolute residual', 1e-4),\n        (63, 'relative residual', 1e-6),\n    ]\n\n    results = []\n    for N, criterion, tau in test_cases:\n        iteration_count = solve_case(N, criterion, tau)\n        results.append(iteration_count)\n\n    # Format and print the final output as a single line\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2382749"}, {"introduction": "Moving from linear systems to eigenvalue problems introduces new challenges for designing effective stopping criteria. For methods like the power iteration, the estimated eigenvector can converge in direction while flipping its sign in alternate steps, fooling simpler checks. This practice [@problem_id:2427048] tasks you with implementing a more robust criterion based on the angle between successive eigenvector estimates, ensuring your algorithm correctly identifies directional convergence. It's a crucial skill for developing reliable eigensolvers.", "problem": "You are given the task of designing and validating a robust stopping criterion for iterative eigenvector approximations that relies on the angle between successive unit-norm iterates, rather than solely on changes in the estimated eigenvalue. Let $A \\in \\mathbb{R}^{n \\times n}$ be a real matrix, and let $\\|\\cdot\\|$ denote the Euclidean norm. For a nonzero initial vector $v_0 \\in \\mathbb{R}^n$, define the normalized sequence $\\{u_k\\}$ of iterates by either of the following update rules, depending on the method specified in each test case:\n\n- Standard iteration: \n$$\nu_{k+1} \\;=\\; \\frac{A u_k}{\\|A u_k\\|}.\n$$\n\n- Shift-and-invert iteration with a real shift $\\sigma$:\n$$\n\\text{solve } (A - \\sigma I) w = u_k \\text{ for } w, \\quad u_{k+1} \\;=\\; \\frac{w}{\\|w\\|}.\n$$\n\nAt each iteration, define the acute principal angle between successive unit vectors $u_k$ and $u_{k+1}$ by\n$$\n\\theta_k \\;=\\; \\arccos\\!\\big(\\,|u_{k+1}^\\top u_k|\\,\\big),\n$$\nwith angles measured in radians. The robust stopping criterion is specified as follows: declare convergence when $\\theta_k \\le \\tau_\\theta$ holds for at least $s$ consecutive iterations, where $\\tau_\\theta  0$ and $s \\in \\mathbb{N}$ are given. If convergence is not achieved within $N_{\\max}$ iterations, declare non-convergence for that test.\n\nYour program must, for each test case described below, compute the number of iterations required to meet the angle-based stopping criterion. If the linear system in the shift-and-invert iteration is singular or numerically unsolvable for the specified shift, or if the stopping criterion is not met within the iteration cap, return the integer $-1$ for that test case.\n\nAll angles must be computed in radians. All inner products and norms must be Euclidean. The final output must be a single line containing the results for the entire test suite as a comma-separated list of integers enclosed in square brackets.\n\nTest Suite (all matrices and vectors are given explicitly):\n\n- Case $1$ (standard iteration, well-conditioned symmetric matrix, happy path):\n  - $A^{(1)} = \\begin{bmatrix} 4  1  0 \\\\ 1  3  0 \\\\ 0  0  2 \\end{bmatrix}$,\n    $v_0^{(1)} = \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\end{bmatrix}$,\n    $\\tau_\\theta^{(1)} = 10^{-8}$,\n    $s^{(1)} = 2$,\n    $N_{\\max}^{(1)} = 1000$.\n- Case $2$ (standard iteration, slow convergence due to nearly equal leading eigenvalues, edge case):\n  - $A^{(2)} = \\begin{bmatrix} 1  10^{-4}  0 \\\\ 10^{-4}  0.999  0 \\\\ 0  0  0.5 \\end{bmatrix}$,\n    $v_0^{(2)} = \\begin{bmatrix} 1 \\\\ 0 \\\\ 0.1 \\end{bmatrix}$,\n    $\\tau_\\theta^{(2)} = 10^{-10}$,\n    $s^{(2)} = 3$,\n    $N_{\\max}^{(2)} = 50000$.\n- Case $3$ (standard iteration, dominant eigenvalue with negative sign, sign-flip robustness via acute angle):\n  - $A^{(3)} = \\begin{bmatrix} -5  0  0 \\\\ 0  3  0 \\\\ 0  0  2 \\end{bmatrix}$,\n    $v_0^{(3)} = \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\end{bmatrix}$,\n    $\\tau_\\theta^{(3)} = 10^{-8}$,\n    $s^{(3)} = 2$,\n    $N_{\\max}^{(3)} = 1000$.\n- Case $4$ (shift-and-invert iteration targeting the smallest eigenvalue with $\\sigma = 0$):\n  - $A^{(4)} = \\begin{bmatrix} 10  2  0 \\\\ 2  5  1 \\\\ 0  1  1 \\end{bmatrix}$,\n    $v_0^{(4)} = \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix}$,\n    $\\sigma^{(4)} = 0$,\n    $\\tau_\\theta^{(4)} = 10^{-8}$,\n    $s^{(4)} = 2$,\n    $N_{\\max}^{(4)} = 1000$.\n- Case $5$ (shift-and-invert iteration with $\\sigma$ near a known eigenvalue to induce very rapid convergence, boundary case):\n  - $A^{(5)} = \\begin{bmatrix} 7  0  0 \\\\ 0  3  0 \\\\ 0  0  1 \\end{bmatrix}$,\n    $v_0^{(5)} = \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\end{bmatrix}$,\n    $\\sigma^{(5)} = 3.001$,\n    $\\tau_\\theta^{(5)} = 10^{-12}$,\n    $s^{(5)} = 1$,\n    $N_{\\max}^{(5)} = 100$.\n\nRequired final output format: Your program should produce a single line of output containing the iteration counts for Cases $1$ through $5$ in order, as a comma-separated list enclosed in square brackets (for example, $\\texttt{[n1,n2,n3,n4,n5]}$), where each $n_j$ is an integer.", "solution": "The problem statement is assessed to be valid. It is scientifically grounded in the established principles of numerical linear algebra, specifically the power and inverse power iteration methods for eigenvalue problems. The problem is well-posed, providing all necessary matrices, vectors, and parameters for each test case. The objective is clearly defined, and the terminology is precise and unambiguous.\n\nThe core of this problem lies in implementing two related iterative algorithms for finding eigenvectors of a matrix $A \\in \\mathbb{R}^{n \\times n}$ and applying a robust stopping criterion based on the angle between successive iterates.\n\nThe first algorithm is the standard power iteration, defined by the recurrence relation\n$$\nu_{k+1} \\;=\\; \\frac{A u_k}{\\|A u_k\\|}\n$$\nwhere $u_k$ is the normalized vector at iteration $k$ and $\\|\\cdot\\|$ is the Euclidean norm. This sequence, for most starting vectors $v_0$ (from which $u_0 = v_0 / \\|v_0\\|$ is derived), converges to the eigenvector corresponding to the eigenvalue $\\lambda_1$ with the largest magnitude (the dominant eigenvalue). The rate of convergence is determined by the ratio $|\\lambda_2 / \\lambda_1|$, where $\\lambda_2$ is the eigenvalue with the second-largest magnitude. If this ratio is close to $1$, convergence is slow.\n\nThe second algorithm is the shift-and-invert iteration. For a given shift $\\sigma \\in \\mathbb{R}$, the update rule is\n$$\n\\text{solve } (A - \\sigma I) w = u_k \\text{ for } w, \\quad u_{k+1} \\;=\\; \\frac{w}{\\|w\\|}.\n$$\nThis is mathematically equivalent to applying the power iteration to the matrix $(A - \\sigma I)^{-1}$. The eigenvalues of $(A - \\sigma I)^{-1}$ are $1/(\\lambda_i - \\sigma)$, where $\\lambda_i$ are the eigenvalues of $A$. The power iteration on this inverted matrix will converge to the eigenvector corresponding to its dominant eigenvalue. This dominant eigenvalue of $(A - \\sigma I)^{-1}$ corresponds to the $\\lambda_j$ of $A$ that minimizes $|\\lambda_j - \\sigma|$. Therefore, shift-and-invert is a powerful method for finding the eigenvector corresponding to an eigenvalue of $A$ that is closest to a chosen shift $\\sigma$. By setting $\\sigma = 0$, the method (now called simple inverse iteration) finds the eigenvector for the eigenvalue with the smallest magnitude. If $\\sigma$ is chosen very close to an eigenvalue, the convergence is exceptionally rapid. This method fails if $\\sigma$ is exactly an eigenvalue, as the matrix $(A - \\sigma I)$ becomes singular and the linear system is unsolvable.\n\nThe stopping criterion is based on the acute principal angle $\\theta_k = \\arccos(|u_{k+1}^\\top u_k|)$ between successive normalized iterates. An eigenvector is a direction, defined only up to a non-zero scalar multiple. Iterative methods might produce estimates that converge in direction but flip in sign, i.e., $u_{k+1} \\approx -u_k$. In such cases, the dot product $u_{k+1}^\\top u_k$ approaches $-1$. By taking the absolute value, $|u_{k+1}^\\top u_k|$, we ensure the argument of $\\arccos$ approaches $1$, and thus the angle $\\theta_k$ correctly approaches $0$, signaling convergence of the eigenvector's direction. The criterion requires this condition, $\\theta_k \\le \\tau_\\theta$, to be met for $s$ consecutive iterations to prevent premature termination due to incidental small angles.\n\nThe algorithm to solve each case is as follows:\n$1$. Initialize the iteration by normalizing the starting vector: $u_0 = v_0 / \\|v_0\\|$.\n$2$. Initialize a counter for consecutive successes, `consecutive_successes`, to $0$.\n$3$. Loop for a maximum of $N_{\\max}$ iterations, from $k = 1, 2, \\dots, N_{\\max}$.\n$4$. In each iteration $k$, compute the next iterate $u_k$ from the previous one, $u_{k-1}$, using either the standard or shift-and-invert formula. For the latter, this involves solving a linear system, which may fail if the matrix is singular. Such a failure results in terminating and returning $-1$.\n$5$. Compute the angle $\\theta_{k-1} = \\arccos(\\text{clip}(|u_k^\\top u_{k-1}|, -1.0, 1.0))$. The clipping is a numerical safeguard against floating-point errors which might yield a dot product magnitude slightly greater than $1$.\n$6$. Check if $\\theta_{k-1} \\le \\tau_\\theta$. If true, increment `consecutive_successes`. If false, reset it to $0$.\n$7$. If `consecutive_successes` reaches $s$, the criterion is met. The process terminates, and the current iteration count $k$ is the result.\n$8$. If the loop completes without meeting the criterion, $N_{\\max}$ has been exceeded. The process terminates, and the result is $-1$.\n\nThis procedural design directly implements the mathematical principles of the specified iteration methods and stopping criterion, addressing potential numerical issues and failure modes as stipulated.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve_case(A, v0, method, params):\n    \"\"\"\n    Computes the number of iterations for an eigenvector approximation to converge.\n\n    Args:\n        A (np.ndarray): The matrix.\n        v0 (np.ndarray): The initial vector.\n        method (str): 'standard' or 'shift-and-invert'.\n        params (dict): A dictionary of parameters:\n            - tau_theta (float): Angle tolerance.\n            - s (int): Number of consecutive successful checks required.\n            - N_max (int): Maximum number of iterations.\n            - sigma (float, optional): Shift for shift-and-invert.\n\n    Returns:\n        int: The number of iterations, or -1 for failure/non-convergence.\n    \"\"\"\n    tau_theta = params['tau_theta']\n    s = params['s']\n    N_max = params['N_max']\n    sigma = params.get('sigma')\n\n    # 1. Normalize the initial vector.\n    norm_v0 = np.linalg.norm(v0)\n    if np.isclose(norm_v0, 0):\n        return -1 # Should not happen with given problems.\n    u_prev = v0 / norm_v0\n\n    consecutive_successes = 0\n\n    # 2. Main iteration loop.\n    for k in range(1, N_max + 1):\n        # 3. Compute the next iterate u_k.\n        try:\n            if method == 'standard':\n                v_next = A @ u_prev\n                norm_v = np.linalg.norm(v_next)\n                if np.isclose(norm_v, 0): # Iteration breaks down\n                    return -1\n                u_k = v_next / norm_v\n            elif method == 'shift-and-invert':\n                M = A - sigma * np.identity(A.shape[0])\n                w = np.linalg.solve(M, u_prev)\n                norm_w = np.linalg.norm(w)\n                if np.isclose(norm_w, 0):\n                    return -1\n                u_k = w / norm_w\n            else:\n                # Should not be reached\n                return -1\n        except np.linalg.LinAlgError:\n            # Singular matrix in shift-and-invert\n            return -1\n\n        # 4. Compute the angle theta.\n        dot_product = np.dot(u_k, u_prev)\n        # Use absolute value for the acute angle.\n        # Clip to handle floating-point inaccuracies where |dot_product| > 1.\n        angle = np.arccos(np.clip(np.abs(dot_product), -1.0, 1.0))\n        \n        # 5. Check the stopping criterion.\n        if angle = tau_theta:\n            consecutive_successes += 1\n        else:\n            consecutive_successes = 0\n            \n        # 6. Check for convergence.\n        if consecutive_successes >= s:\n            return k # Return current iteration number on convergence.\n\n        # Prepare for the next iteration.\n        u_prev = u_k\n        \n    # 7. N_max reached without convergence.\n    return -1\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1\n        {\n            'A': np.array([[4, 1, 0], [1, 3, 0], [0, 0, 2]], dtype=float),\n            'v0': np.array([1, 1, 1], dtype=float),\n            'method': 'standard',\n            'params': {'tau_theta': 1e-8, 's': 2, 'N_max': 1000}\n        },\n        # Case 2\n        {\n            'A': np.array([[1, 1e-4, 0], [1e-4, 0.999, 0], [0, 0, 0.5]], dtype=float),\n            'v0': np.array([1, 0, 0.1], dtype=float),\n            'method': 'standard',\n            'params': {'tau_theta': 1e-10, 's': 3, 'N_max': 50000}\n        },\n        # Case 3\n        {\n            'A': np.array([[-5, 0, 0], [0, 3, 0], [0, 0, 2]], dtype=float),\n            'v0': np.array([1, 1, 1], dtype=float),\n            'method': 'standard',\n            'params': {'tau_theta': 1e-8, 's': 2, 'N_max': 1000}\n        },\n        # Case 4\n        {\n            'A': np.array([[10, 2, 0], [2, 5, 1], [0, 1, 1]], dtype=float),\n            'v0': np.array([1, 0, 0], dtype=float),\n            'method': 'shift-and-invert',\n            'params': {'tau_theta': 1e-8, 's': 2, 'N_max': 1000, 'sigma': 0}\n        },\n        # Case 5\n        {\n            'A': np.array([[7, 0, 0], [0, 3, 0], [0, 0, 1]], dtype=float),\n            'v0': np.array([1, 1, 1], dtype=float),\n            'method': 'shift-and-invert',\n            'params': {'tau_theta': 1e-12, 's': 1, 'N_max': 100, 'sigma': 3.001}\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        result = solve_case(case['A'], case['v0'], case['method'], case['params'])\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2427048"}, {"introduction": "An advanced application of convergence analysis is to use it not just to stop an algorithm, but to intelligently switch between different methods. This exercise [@problem_id:2382818] demonstrates a key idea inspired by multigrid theory: using a simple \"smoother\" like the Jacobi iteration to eliminate high-frequency error, and then switching to a more powerful solver like the Conjugate Gradient method once the error is smooth. Your task is to implement a criterion based on the *spectral content* of the residual, providing a powerful example of how a deep understanding of convergence leads to sophisticated, hybrid algorithms.", "problem": "Consider the linear boundary value problem for a one-dimensional Poisson equation with homogeneous Dirichlet boundary conditions on the unit interval: find a discrete vector $x \\in \\mathbb{R}^N$ such that $A x = b$, where $A \\in \\mathbb{R}^{N \\times N}$ is the standard second-order centered finite-difference discretization of the negative Laplacian on $[0,1]$ with $N$ interior grid points and grid spacing $h = \\frac{1}{N+1}$. The matrix $A$ has diagonal entries $2/h^2$ and sub- and super-diagonal entries $-1/h^2$. The vector $b \\in \\mathbb{R}^N$ is defined by sampling a given function $f(x)$ at the interior grid points $x_i = i h$ for $i = 1,2,\\dots,N$. Angles in trigonometric functions are to be taken in radians.\n\nDefine the residual at iteration $k$ as $r_k = b - A x_k$ for an iterate $x_k$. Let $\\mathrm{DST}_1$ denote the type-I Discrete Sine Transform (DST) on $\\mathbb{R}^N$, which maps $r_k$ to coefficients $R^{(k)}_m$ for $m = 1,2,\\dots,N$ corresponding to the orthogonal sine basis $\\sin\\!\\left(\\frac{\\pi m j}{N+1}\\right)$, up to a fixed normalization. For a given high-frequency cutoff parameter $\\alpha \\in (0,1)$, define the cutoff index $m_c = \\lceil \\alpha N \\rceil$, and the high-frequency energy ratio of the residual\n$$\n\\rho_k = \\frac{\\sum_{m=m_c}^{N} \\left(R^{(k)}_m\\right)^2}{\\sum_{m=1}^{N} \\left(R^{(k)}_m\\right)^2},\n$$\nwith the convention that $\\rho_k = 0$ if the denominator is zero.\n\nImplement a two-stage iterative solver with an adaptive switching criterion based on the spectral content of the residual:\n- Stage 1 (smoothing phase): use the weighted Jacobi iteration with weight $\\omega \\in (0,1)$,\n$$\nx_{k+1} = x_k + \\omega D^{-1} (b - A x_k),\n$$\nwhere $D$ is the diagonal of $A$ (so $D = \\frac{2}{h^2} I$), starting from the initial guess $x_0 = 0$. After each iteration (including at $k=0$), compute $\\rho_k$ as defined above. Stop Stage 1 at the first iteration index $k$ such that $\\rho_k \\le \\tau$, where $\\tau  0$ is a prescribed tolerance, or when a prescribed maximum number of Stage 1 iterations $K_1^{\\max}$ is reached, whichever occurs first. Denote by $k_{\\mathrm{switch}}$ the number of Stage 1 iterations actually performed (so $k_{\\mathrm{switch}} = 0$ is allowed if the condition is satisfied at $k=0$), and record $\\rho_{k_{\\mathrm{switch}}}$.\n- Stage 2 (coarse-error reduction phase): starting from $x_{k_{\\mathrm{switch}}}$, apply the Conjugate Gradient (CG) method for symmetric positive definite systems to $A x = b$ until the Euclidean norm satisfies $\\lVert r_k \\rVert_2 \\le \\varepsilon$, where $\\varepsilon  0$ is a prescribed tolerance, or until a maximum number of Stage 2 iterations $K_2^{\\max}$ is reached, whichever occurs first. The CG method is defined by the standard recurrence: with $x^{(0)} = x_{k_{\\mathrm{switch}}}$, $r^{(0)} = b - A x^{(0)}$, $p^{(0)} = r^{(0)}$, and for $\\ell = 0,1,2,\\dots$,\n$$\n\\alpha_\\ell = \\frac{\\left(r^{(\\ell)}\\right)^\\top r^{(\\ell)}}{\\left(p^{(\\ell)}\\right)^\\top A p^{(\\ell)}}, \\quad\nx^{(\\ell+1)} = x^{(\\ell)} + \\alpha_\\ell p^{(\\ell)}, \\quad\nr^{(\\ell+1)} = r^{(\\ell)} - \\alpha_\\ell A p^{(\\ell)},\n$$\n$$\n\\beta_\\ell = \\frac{\\left(r^{(\\ell+1)}\\right)^\\top r^{(\\ell+1)}}{\\left(r^{(\\ell)}\\right)^\\top r^{(\\ell)}}, \\quad\np^{(\\ell+1)} = r^{(\\ell+1)} + \\beta_\\ell p^{(\\ell)}.\n$$\nLet $k_{\\mathrm{CG}}$ be the number of Stage 2 iterations actually performed. Report the total iteration count $k_{\\mathrm{total}} = k_{\\mathrm{switch}} + k_{\\mathrm{CG}}$ and the final residual norm $\\lVert r_{\\mathrm{final}} \\rVert_2$ at termination.\n\nTest suite. For each test case below, the program must construct $A$ for the specified $N$ with $h = \\frac{1}{N+1}$, set $x_0 = 0$, form $b_i = f(x_i)$ at $x_i = i h$ for $i=1,2,\\dots,N$, and then execute the two-stage procedure above with the given parameters. The functions $f(x)$ are:\n- $f_1(x) = \\sin(2 \\pi x) + 0.1 \\sin(16 \\pi x)$,\n- $f_2(x) = \\sin(\\pi x)$,\n- $f_3(x) = \\sum_{k=12}^{24} \\frac{1}{k} \\sin(k \\pi x)$,\n- $f_4(x) = \\sin(8 \\pi x) + 0.5 \\sin(20 \\pi x)$.\n\nThe four test cases are:\n1. $N = 63$, $\\omega = \\tfrac{2}{3}$, $\\alpha = 0.5$, $\\tau = 10^{-3}$, $\\varepsilon = 10^{-10}$, $K_1^{\\max} = 100$, $K_2^{\\max} = 63$, $f = f_1$.\n2. $N = 63$, $\\omega = \\tfrac{2}{3}$, $\\alpha = 0.5$, $\\tau = 10^{-6}$, $\\varepsilon = 10^{-10}$, $K_1^{\\max} = 5$, $K_2^{\\max} = 63$, $f = f_2$.\n3. $N = 127$, $\\omega = \\tfrac{2}{3}$, $\\alpha = 0.7$, $\\tau = 10^{-4}$, $\\varepsilon = 10^{-10}$, $K_1^{\\max} = 100$, $K_2^{\\max} = 127$, $f = f_3$.\n4. $N = 63$, $\\omega = \\tfrac{2}{3}$, $\\alpha = 0.6$, $\\tau = 10^{-12}$, $\\varepsilon = 10^{-10}$, $K_1^{\\max} = 10$, $K_2^{\\max} = 63$, $f = f_4$.\n\nRequired final output. Your program must produce a single line of output containing a comma-separated list enclosed in square brackets, where each entry corresponds to one test case and is itself a list of four values:\n- $k_{\\mathrm{switch}}$ (integer),\n- $\\rho_{k_{\\mathrm{switch}}}$ (floating-point),\n- $k_{\\mathrm{total}}$ (integer),\n- $\\lVert r_{\\mathrm{final}} \\rVert_2$ (floating-point).\n\nFor example, the output format must be of the form\n$$\n[\\,[k_{\\mathrm{switch}},\\rho_{k_{\\mathrm{switch}}},k_{\\mathrm{total}},\\lVert r_{\\mathrm{final}} \\rVert_2],\\dots\\,]\n$$\nwith no spaces anywhere in the line. The values themselves must be in the order of the test cases listed above. The answer does not involve physical units.", "solution": "The problem presented requires the implementation and analysis of a two-stage hybrid iterative solver for the one-dimensional Poisson equation, discretized using a finite-difference scheme. The validation of this problem confirms that it is scientifically sound, well-posed, and an exemplary problem in computational physics for demonstrating advanced concepts in iterative methods. I shall now proceed with a principled explanation of the solver's design before presenting the final implementation.\n\nThe core problem is to solve the linear system of equations $A x = b$, which arises from the finite-difference discretization of the boundary value problem $-u''(x) = f(x)$ on the interval $x \\in [0,1]$ with homogeneous Dirichlet boundary conditions $u(0)=u(1)=0$. The matrix $A$ is the discrete representation of the negative second derivative operator, $-\\frac{d^2}{dx^2}$. It is a symmetric positive-definite (SPD) matrix of size $N \\times N$, where $N$ is the number of interior grid points. Its entries are given by $A_{ii} = 2/h^2$ and $A_{i,i\\pm1} = -1/h^2$, with $h=1/(N+1)$ being the grid spacing.\n\nA critical aspect of the matrix $A$ is its spectral properties. Its eigenvectors, denoted by $v_m$ for $m=1, 2, \\dots, N$, are the discrete sine vectors with components $(v_m)_j = \\sin\\left(\\frac{\\pi m j}{N+1}\\right)$ for $j=1, \\dots, N$. These vectors form an orthogonal basis for $\\mathbb{R}^N$ and are precisely the basis vectors of the Type-I Discrete Sine Transform (DST-I). The corresponding eigenvalues are $\\lambda_m = \\frac{2}{h^2}\\left(1 - \\cos\\left(\\frac{\\pi m}{N+1}\\right)\\right) = \\frac{4}{h^2}\\sin^2\\left(\\frac{\\pi m}{2(N+1)}\\right)$. The index $m$ corresponds to the frequency of the mode: small $m$ values represent low-frequency (smooth) modes, while large $m$ values represent high-frequency (oscillatory) modes.\n\nThe proposed solver is a hybrid method designed to efficiently handle different frequency components of the error. The error $e_k = x - x_k$ (where $x$ is the exact solution) can be decomposed into the eigenbasis of $A$. Different iterative methods exhibit varying effectiveness at damping error components associated with different frequencies.\n\n**Stage 1: Smoothing with Weighted Jacobi**\nThe first stage employs the weighted Jacobi method, defined by the iteration:\n$$\nx_{k+1} = x_k + \\omega D^{-1} r_k = x_k + \\omega D^{-1} (b - A x_k)\n$$\nwhere $D$ is the diagonal of $A$, so $D = (2/h^2)I$, and $\\omega \\in (0,1)$ is a relaxation parameter. The evolution of the error is governed by $e_{k+1} = (I - \\omega D^{-1} A) e_k$. The effectiveness of the method depends on the eigenvalues of the iteration matrix $M_J = I - \\omega D^{-1} A$. Since $A$ and $D$ are diagonalized in the DST-I basis, the eigenvalues of $M_J$ are easily found to be:\n$$\n\\mu_m = 1 - \\omega \\frac{h^2}{2} \\lambda_m = 1 - 2\\omega \\sin^2\\left(\\frac{\\pi m}{2(N+1)}\\right)\n$$\nFor high-frequency modes ($m \\approx N$), the term $\\sin^2(\\dots)$ is close to $1$, making $|\\mu_m| \\approx |1 - 2\\omega|$. For the specified $\\omega = 2/3$, this is $|1 - 4/3| = 1/3$, indicating rapid damping of high-frequency error components. Conversely, for low-frequency modes ($m \\ll N$), $\\sin^2(\\dots)$ is small, and $\\mu_m \\approx 1$, indicating very slow damping. This property makes the weighted Jacobi method an excellent \"smoother\": it efficiently reduces the high-frequency, oscillatory components of the error, leaving a \"smoother\" error dominated by low-frequency components.\n\n**Adaptive Switching Criterion**\nThe transition from Stage 1 to Stage 2 is controlled by a criterion based on the spectral content of the residual, $r_k = b - A x_k$. The residual is related to the error by $r_k = A e_k$. The high-frequency energy ratio $\\rho_k$ is defined as:\n$$\n\\rho_k = \\frac{\\sum_{m=m_c}^{N} \\left(R^{(k)}_m\\right)^2}{\\sum_{m=1}^{N} \\left(R^{(k)}_m\\right)^2}\n$$\nwhere $R^{(k)}_m$ are the DST-I coefficients of $r_k$ and $m_c = \\lceil \\alpha N \\rceil$ is a cutoff index. $\\rho_k$ measures the proportion of energy in the high-frequency part of the residual's spectrum. The Jacobi iteration is stopped as soon as $\\rho_k$ falls below a tolerance $\\tau$, signifying that the residual (and thus the error) has become sufficiently smooth.\n\n**Stage 2: Coarse-Error Reduction with Conjugate Gradient**\nOnce the error is smooth, the problem is ripe for a method that excels at reducing low-frequency error. The Conjugate Gradient (CG) method is an optimal Krylov subspace method for SPD systems like $Ax=b$. It is particularly effective for problems where the error is concentrated in a low-dimensional subspace, which is the case here, as the remaining error is dominated by a few low-frequency eigenmodes. CG iteratively builds an $A$-orthogonal basis for the Krylov subspace generated by the initial residual and finds the best solution within that subspace at each step. By applying CG after the smoothing phase, the remaining smooth error components are eliminated with a convergence rate much faster than Jacobi could achieve. The standard CG algorithm is applied until the Euclidean norm of the residual, $\\lVert r_k \\rVert_2$, is reduced below a final tolerance $\\varepsilon$.\n\nThis two-stage approach synergistically combines the strengths of two different iterative methods, a common and powerful paradigm in modern numerical analysis, closely related to the principles of multigrid methods. The Jacobi method acts as a simple but effective smoother, while the CG method acts as an efficient coarse-grid-like corrector for the remaining smooth error.", "answer": "```python\nimport numpy as np\nfrom scipy.fft import dst\nimport math\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite for the two-stage iterative solver.\n    \"\"\"\n    \n    # Define the forcing functions\n    def f1(x):\n        return np.sin(2 * np.pi * x) + 0.1 * np.sin(16 * np.pi * x)\n\n    def f2(x):\n        return np.sin(np.pi * x)\n\n    def f3_factory():\n        def f(x):\n            k_vals = np.arange(12, 25)\n            x_b = x[np.newaxis, :]\n            k_b = k_vals[:, np.newaxis]\n            terms = (1.0 / k_b) * np.sin(k_b * np.pi * x_b)\n            return np.sum(terms, axis=0)\n        return f\n    f3 = f3_factory()\n\n    def f4(x):\n        return np.sin(8 * np.pi * x) + 0.5 * np.sin(20 * np.pi * x)\n\n    test_cases = [\n        {'N': 63, 'omega': 2./3., 'alpha': 0.5, 'tau': 1e-3, 'epsilon': 1e-10, 'K1_max': 100, 'K2_max': 63, 'f': f1},\n        {'N': 63, 'omega': 2./3., 'alpha': 0.5, 'tau': 1e-6, 'epsilon': 1e-10, 'K1_max': 5, 'K2_max': 63, 'f': f2},\n        {'N': 127, 'omega': 2./3., 'alpha': 0.7, 'tau': 1e-4, 'epsilon': 1e-10, 'K1_max': 100, 'K2_max': 127, 'f': f3},\n        {'N': 63, 'omega': 2./3., 'alpha': 0.6, 'tau': 1e-12, 'epsilon': 1e-10, 'K1_max': 10, 'K2_max': 63, 'f': f4},\n    ]\n\n    results = []\n\n    for case in test_cases:\n        N = case['N']\n        omega = case['omega']\n        alpha = case['alpha']\n        tau = case['tau']\n        epsilon = case['epsilon']\n        K1_max = case['K1_max']\n        K2_max = case['K2_max']\n        f = case['f']\n\n        h = 1.0 / (N + 1)\n        h2_inv = 1.0 / (h * h)\n        grid = np.arange(1, N + 1) * h\n        b = f(grid)\n\n        def matvec_A(v):\n            y = np.zeros(N)\n            # Interior points\n            y[1:-1] = -v[:-2] + 2 * v[1:-1] - v[2:]\n            # Boundary points (with v_0 = 0 and v_{N+1} = 0)\n            y[0] = 2 * v[0] - v[1]\n            y[-1] = 2 * v[-1] - v[-2]\n            return y * h2_inv\n\n        # Stage 1: Weighted Jacobi\n        x = np.zeros(N)\n        k_switch = 0\n        rho_k_switch = 0.0\n        m_c = math.ceil(alpha * N)\n\n        for k in range(K1_max + 1):\n            r = b - matvec_A(x)\n            \n            # Compute rho_k\n            Rk = dst(r, type=1)\n            total_energy = np.sum(Rk**2)\n            \n            rho_k = 0.0\n            if total_energy > 0:\n                high_freq_energy = np.sum(Rk[m_c - 1:]**2)\n                rho_k = high_freq_energy / total_energy\n\n            if k == K1_max or rho_k = tau:\n                k_switch = k\n                rho_k_switch = rho_k\n                break\n\n            # Jacobi step\n            # D_inv_r = r / (2/h^2) = r * h^2 / 2\n            x = x + omega * (h*h / 2.0) * r\n        \n        # Stage 2: Conjugate Gradient\n        k_cg = 0\n        r_cg = b - matvec_A(x)\n        p_cg = r_cg.copy()\n        rs_old = np.dot(r_cg, r_cg)\n\n        # The CG loop tolerance check is on the norm, so sqrt(rs_old)\n        if np.sqrt(rs_old) = epsilon:\n             pass # Already converged\n        else:\n            for i in range(K2_max):\n                Ap = matvec_A(p_cg)\n                alpha_cg = rs_old / np.dot(p_cg, Ap)\n                \n                x = x + alpha_cg * p_cg\n                r_cg = r_cg - alpha_cg * Ap\n                \n                rs_new = np.dot(r_cg, r_cg)\n                \n                k_cg = i + 1\n                \n                if np.sqrt(rs_new) = epsilon:\n                    break\n                \n                p_cg = r_cg + (rs_new / rs_old) * p_cg\n                rs_old = rs_new\n\n        k_total = k_switch + k_cg\n        r_final = b - matvec_A(x)\n        norm_r_final = np.linalg.norm(r_final)\n\n        results.append([k_switch, rho_k_switch, k_total, norm_r_final])\n\n    # Format the final output string without spaces as required\n    output_str = \",\".join([f\"[{r[0]},{r[1]},{r[2]},{r[3]}]\" for r in results])\n    print(f\"[{output_str}]\")\n\nsolve()\n```", "id": "2382818"}]}