{"hands_on_practices": [{"introduction": "The true power of the Arnoldi iteration lies in its \"matrix-free\" nature, where we only need to know the *action* of an operator on a vector, not the operator's explicit matrix representation. This exercise guides you through implementing the core Arnoldi algorithm to approximate the spectral radius for several operators defined purely as functions [@problem_id:2373518]. By doing so, you will build a foundational tool applicable to a wide range of large-scale problems where forming the matrix is computationally prohibitive or even impossible.", "problem": "You are given the task of computing approximations to the spectral radii of several real, nonsymmetric linear operators using only the ability to apply each operator to a vector. For a finite-dimensional real vector space of dimension $n$, let $A:\\mathbb{R}^n \\to \\mathbb{R}^n$ be a linear operator. For a chosen positive integer $m$ and a nonzero starting vector $v_0 \\in \\mathbb{R}^n$, consider the $m$-dimensional Krylov subspace $K_m(A, v_0) = \\mathrm{span}\\{v_0, A v_0, A^2 v_0, \\dots, A^{m-1} v_0\\}$. Let $V_m \\in \\mathbb{R}^{n \\times m}$ denote a column-orthonormal basis for $K_m(A, v_0)$ and let $H_m = V_m^\\top A V_m \\in \\mathbb{R}^{m \\times m}$ denote the orthogonal projection of $A$ onto this subspace. Define the Ritz values as the eigenvalues of $H_m$, and define the Ritz estimate of the spectral radius as the largest magnitude among these Ritz values. The true spectral radius of $A$ is $\\rho(A) = \\max\\{|\\lambda| : \\lambda \\in \\sigma(A)\\}$, where $\\sigma(A)$ is the spectrum of $A$.\n\nYour program must compute, for each of the test cases listed below, the absolute error between the Ritz estimate (computed from the eigenvalues of $H_m$) and the true spectral radius $\\rho(A)$, using a prescribed initial vector. The program must not access matrix entries of $A$ directly; it must only use the action of $A$ on a vector. All angles in trigonometric functions are to be interpreted in radians. The final output must be a single line containing a list of floating-point numbers corresponding to the absolute errors for each test case, rounded to eight decimal places.\n\nUse the following test suite. In every case, the initial vector $v_0 \\in \\mathbb{R}^n$ is defined componentwise by\n$$\n(v_0)_i = \\sin(i) + \\tfrac{1}{2}\\cos(2 i), \\quad i=1,2,\\dots,n,\n$$\nfollowed by normalization to unit Euclidean norm $\\|(v_0)\\|_2 = 1$.\n\n- Test case $1$ (tridiagonal Toeplitz operator):\n  - Dimension: $n = 80$.\n  - Parameters: $a = 1$, $b = 2$, $c = \\tfrac{1}{2}$.\n  - Operator action: for any $x \\in \\mathbb{R}^n$, define $y = A x \\in \\mathbb{R}^n$ by\n    $$\n    y_1 = a x_1 + b x_2,\\quad\n    y_i = c x_{i-1} + a x_i + b x_{i+1}\\ \\text{for}\\ i=2,\\dots,n-1,\\quad\n    y_n = c x_{n-1} + a x_n.\n    $$\n  - Subspace dimension: $m = 25$.\n  - True spectral radius: for this operator, the eigenvalues are $a + 2\\sqrt{b c}\\cos\\left(\\tfrac{j\\pi}{n+1}\\right)$ for $j=1,2,\\dots,n$, hence\n    $$\n    \\rho(A) = \\max\\Big\\{\\left|a + 2\\sqrt{b c}\\right|,\\ \\left|a - 2\\sqrt{b c}\\right|\\Big\\}.\n    $$\n\n- Test case $2$ (Jordan block):\n  - Dimension: $n = 60$.\n  - Parameter: $\\lambda = 3$.\n  - Operator action: for any $x \\in \\mathbb{R}^n$, define $y = A x \\in \\mathbb{R}^n$ by\n    $$\n    y_i = \\lambda x_i + x_{i+1}\\ \\text{for}\\ i=1,2,\\dots,n-1,\\quad\n    y_n = \\lambda x_n.\n    $$\n  - Subspace dimension: $m = 15$.\n  - True spectral radius: $\\rho(A) = |\\lambda|$.\n\n- Test case $3$ (similarity transform with diagonal spectrum):\n  - Dimension: $n = 50$.\n  - Diagonal spectrum: define $d_{\\max} = 5$, $d_{\\min} = 1$, and\n    $$\n    d_i = d_{\\min} + (d_{\\max} - d_{\\min})\\frac{n - i}{n - 1},\\quad i=1,2,\\dots,n.\n    $$\n    Let $D \\in \\mathbb{R}^{n \\times n}$ be diagonal with entries $d_i$.\n  - Similarity matrix: define $S \\in \\mathbb{R}^{n \\times n}$ by\n    $$\n    S_{ij} = \\delta_{ij} + 10^{-2}\\sin\\!\\Big(\\frac{(i)(j)}{n+1}\\Big),\\quad i,j=1,2,\\dots,n,\n    $$\n    where $\\delta_{ij}$ is the Kronecker delta.\n  - Operator action: for any $x \\in \\mathbb{R}^n$, define $y = A x$ by $y = S D S^{-1} x$.\n  - Subspace dimension: $m = 30$.\n  - True spectral radius: $\\rho(A) = \\max_i |d_i| = d_{\\max}$.\n\n- Test case $4$ (banded nilpotent perturbation of a scalar multiple of the identity):\n  - Dimension: $n = 100$.\n  - Parameters: $\\alpha = 2$, bandwidth $p = 3$.\n  - Operator action: for any $x \\in \\mathbb{R}^n$, define $y = A x \\in \\mathbb{R}^n$ by\n    $$\n    y = \\alpha x + \\sum_{k=1}^{p} N_k x,\n    $$\n    where $(N_k x)_i = x_{i+k}$ for $i=1,2,\\dots,n-k$, and $(N_k x)_i = 0$ for $i>n-k$.\n  - Subspace dimension: $m = 50$.\n  - True spectral radius: since $A = \\alpha I + N$ with $N$ nilpotent, $\\rho(A) = |\\alpha|$.\n\nFor each test case, compute the absolute error\n$$\n\\varepsilon = \\left|\\max_{\\mu \\in \\sigma(H_m)} |\\mu| - \\rho(A)\\right|,\n$$\nwhere $H_m$ is the orthogonal projection of $A$ onto the $m$-dimensional Krylov subspace generated by the normalized $v_0$. If the Krylov subspace has dimension strictly less than $m$ due to exact or near breakdown, use the actual constructed subspace dimension in forming $H_m$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the test cases above, with each number rounded to eight decimal places (for example, $[0.12345678,0.00000000,0.5,1.23456789]$). No other text should be printed.", "solution": "The problem requires the computation of the absolute error between the true spectral radius of a linear operator $A$ and an approximation obtained via the Arnoldi iteration. The spectral radius, $\\rho(A)$, is defined as the maximum absolute value of the eigenvalues of $A$. The Arnoldi iteration is a numerical method for finding a subset of the eigenvalues of a general, typically large and sparse, nonsymmetric matrix or linear operator. A critical constraint is that the operator $A$ is accessible only through its action on a vector, i.e., as a function that returns $Ax$ for a given $x$.\n\nThe Arnoldi iteration constructs an orthonormal basis for the Krylov subspace $K_m(A, v_1) = \\mathrm{span}\\{v_1, A v_1, A^2 v_1, \\dots, A^{m-1} v_1\\}$, where $v_1$ is a starting vector of unit norm and $m$ is the dimension of the subspace. Let the orthonormal basis vectors be the columns of the matrix $V_m = [v_1, v_2, \\dots, v_m] \\in \\mathbb{R}^{n \\times m}$. The process also generates an $m \\times m$ upper Hessenberg matrix $H_m$, which represents the orthogonal projection of the operator A onto the Krylov subspace: $H_m = V_m^\\top A V_m$.\n\nThe eigenvalues of this smaller matrix $H_m$, denoted $\\{\\mu_k\\}_{k=1}^m$, are called the Ritz values. They serve as approximations to the eigenvalues of the original operator $A$. The Ritz estimate of the spectral radius of $A$ is given by $\\hat{\\rho}_m(A) = \\max_{k=1,\\dots,m} |\\mu_k|$. The objective is to calculate the absolute error $\\varepsilon = |\\hat{\\rho}_m(A) - \\rho(A)|$ for several test cases.\n\nThe construction of the basis $V_m$ and the matrix $H_m$ is performed via a stabilized Gram-Schmidt orthogonalization procedure. Given a normalized starting vector $v_1$, the algorithm proceeds as follows for $j=1, 2, \\dots, m$:\n1.  Compute the next vector in the Krylov sequence: $w \\gets A v_j$.\n2.  Orthogonalize $w$ against the previously generated basis vectors $\\{v_1, \\dots, v_j\\}$. This is achieved by iterating for $i = 1, \\dots, j$:\n    a.  Compute the projection coefficient: $h_{ij} \\gets v_i^\\top w$.\n    b.  Subtract the projection: $w \\gets w - h_{ij} v_i$.\n3.  The norm of the resulting vector $w$ is the next subdiagonal element of the Hessenberg matrix: $h_{j+1, j} \\gets \\|w\\|_2$.\n4.  If $h_{j+1, j}$ is numerically zero, this indicates a breakdown. The Krylov subspace is invariant under $A$ and its dimension is $j$, which is less than the desired $m$. The algorithm terminates, and the resulting projection is a $j \\times j$ matrix.\n5.  If no breakdown occurs, normalize the new vector to obtain the next basis vector: $v_{j+1} \\gets w / h_{j+1, j}$.\n\nThe coefficients $h_{ij}$ for $i \\in \\{1,\\dots,j\\}$ and $h_{j+1,j}$ form the entries of an $(m+1) \\times m$ upper Hessenberg matrix $\\tilde{H}_m$. The desired matrix $H_m$ is the upper $m \\times m$ submatrix of $\\tilde{H}_m$.\n\nFor each test case, the implementation requires a specific function that computes the action of the operator $A$ on an arbitrary vector $x$. The initial vector $v_0$ is defined component-wise as $(v_0)_i = \\sin(i) + \\frac{1}{2}\\cos(2i)$ for $i=1, \\dots, n$, and is then normalized to $v_1 = v_0/\\|v_0\\|_2$ to start the iteration.\n\n-   **Test Case 1 (Tridiagonal Toeplitz operator)**: The action of this operator is defined by a simple stencil, which can be implemented efficiently using vector operations. The true spectral radius is given by an analytical formula based on its parameters $a, b, c$.\n-   **Test Case 2 (Jordan block)**: The operator action corresponds to multiplication by a matrix $J = \\lambda I + N$, where $N$ is the nilpotent matrix with ones on the first superdiagonal. Its action on a vector is straightforward to compute. The true spectral radius is simply $|\\lambda|$.\n-   **Test Case 3 (Similarity transform)**: The operator is $A = S D S^{-1}$, where $D$ is diagonal and $S$ is a specified dense matrix. Its action $y = A x$ must be computed as a sequence of operations: first, solve the linear system $S z = x$ to obtain $z=S^{-1}x$; second, compute the element-wise product $w=Dz$; finally, compute the matrix-vector product $y=Sw$. The true spectral radius is the largest absolute value of the diagonal entries of $D$.\n-   **Test Case 4 (Perturbed identity)**: The operator is $A = \\alpha I + N$, where $N$ is a sum of shift operators, making it a strictly upper-triangular and thus nilpotent matrix. The eigenvalues of $A$ are all equal to $\\alpha$, so $\\rho(A) = |\\alpha|$. The action is computed by scaling the input vector and adding shifted versions of it.\n\nFor each case, the Arnoldi iteration is performed to construct $H_m$. The eigenvalues of $H_m$ are computed numerically, and the largest magnitude gives the Ritz estimate $\\hat{\\rho}_m(A)$. This is then compared to the known true spectral radius $\\rho(A)$ to find the required absolute error.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef arnoldi_iteration(A_op, v_start, m):\n    \"\"\"\n    Performs Arnoldi iteration to find the Hessenberg projection H_m.\n    This implementation uses a modified Gram-Schmidt process for stability.\n\n    Args:\n        A_op: A function that computes A @ x.\n        v_start: The normalized starting vector of size n.\n        m: The desired dimension of the Krylov subspace.\n\n    Returns:\n        H: The m_eff x m_eff upper Hessenberg matrix, where m_eff = m is\n           the effective dimension upon breakdown.\n    \"\"\"\n    n = v_start.shape[0]\n    V = np.zeros((n, m + 1), dtype=np.float64)\n    H = np.zeros((m + 1, m), dtype=np.float64)\n    \n    # A small tolerance for breakdown detection\n    tolerance = np.finfo(np.float64).eps * n\n    \n    V[:, 0] = v_start\n    \n    for j in range(m):\n        w = A_op(V[:, j])\n        \n        # Orthogonalize w against the existing basis V using modified Gram-Schmidt\n        for i in range(j + 1):\n            h_ij = np.dot(V[:, i], w)\n            w -= h_ij * V[:, i]\n            H[i, j] = h_ij\n        \n        h_jp1_j = np.linalg.norm(w)\n        \n        if h_jp1_j  tolerance:\n            # Breakdown: Krylov subspace has dimension j+1\n            m_eff = j + 1\n            return H[:m_eff, :m_eff]\n            \n        H[j + 1, j] = h_jp1_j\n        V[:, j + 1] = w / h_jp1_j\n        \n    return H[:m, :m]\n\ndef solve():\n    \"\"\"\n    Solves the problem for all specified test cases.\n    \"\"\"\n    # Define operator factories and true spectral radii for each test case\n    \n    def get_op1(n, params):\n        a, b, c = params['a'], params['b'], params['c']\n        def op(x):\n            y = np.zeros_like(x)\n            y[0] = a * x[0] + b * x[1]\n            y[1:-1] = c * x[:-2] + a * x[1:-1] + b * x[2:]\n            y[-1] = c * x[-2] + a * x[-1]\n            return y\n        return op\n    \n    def get_op2(n, params):\n        lam = params['lambda']\n        def op(x):\n            y = np.zeros_like(x)\n            y[:-1] = lam * x[:-1] + x[1:]\n            y[-1] = lam * x[-1]\n            return y\n        return op\n        \n    def get_op3(n, params):\n        d_max, d_min = params['d_max'], params['d_min']\n        i_vec = np.arange(1, n + 1)\n        d = d_min + (d_max - d_min) * (n - i_vec) / (n - 1)\n        \n        i_v = i_vec[:, None]\n        j_h = i_vec[None, :]\n        S = np.eye(n) + 1e-2 * np.sin(i_v * j_h / (n + 1))\n        \n        # Return a closure that has pre-computed S and d\n        def op(x):\n            # y = S * D * S_inv * x\n            z = np.linalg.solve(S, x)\n            w = d * z\n            y = S @ w\n            return y\n        return op\n\n    def get_op4(n, params):\n        alpha, p = params['alpha'], params['p']\n        def op(x):\n            y = alpha * x.copy()\n            for k in range(1, p + 1):\n                y[:n-k] += x[k:]\n            return y\n        return op\n\n    test_cases = [\n        {\n            \"n\": 80, \"m\": 25, \"params\": {\"a\": 1.0, \"b\": 2.0, \"c\": 0.5},\n            \"A_op_factory\": get_op1,\n            \"true_rho\": lambda p: max(abs(p['a'] + 2 * np.sqrt(p['b'] * p['c'])), \n                                      abs(p['a'] - 2 * np.sqrt(p['b'] * p['c'])))\n        },\n        {\n            \"n\": 60, \"m\": 15, \"params\": {\"lambda\": 3.0},\n            \"A_op_factory\": get_op2,\n            \"true_rho\": lambda p: abs(p['lambda'])\n        },\n        {\n            \"n\": 50, \"m\": 30, \"params\": {\"d_max\": 5.0, \"d_min\": 1.0},\n            \"A_op_factory\": get_op3,\n            \"true_rho\": lambda p: p['d_max']\n        },\n        {\n            \"n\": 100, \"m\": 50, \"params\": {\"alpha\": 2.0, \"p\": 3},\n            \"A_op_factory\": get_op4,\n            \"true_rho\": lambda p: abs(p['alpha'])\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        n, m, params = case[\"n\"], case[\"m\"], case[\"params\"]\n        \n        # 1. Define initial vector\n        i_vals = np.arange(1, n + 1)\n        v0_unnormalized = np.sin(i_vals) + 0.5 * np.cos(2 * i_vals)\n        v0_norm = np.linalg.norm(v0_unnormalized)\n        v0_normalized = v0_unnormalized / v0_norm\n\n        # 2. Get operator and true spectral radius\n        A_op = case[\"A_op_factory\"](n, params)\n        true_rho = case[\"true_rho\"](params)\n        \n        # 3. Run Arnoldi iteration\n        H_m = arnoldi_iteration(A_op, v0_normalized, m)\n        \n        # 4. Compute Ritz estimate and error\n        if H_m.shape[0] > 0:\n            ritz_values = np.linalg.eigvals(H_m)\n            ritz_rho = np.max(np.abs(ritz_values))\n        else: # This case happens if m=0 or breakdown with j=0\n            ritz_rho = 0.0\n\n        error = abs(ritz_rho - true_rho)\n        results.append(f\"{error:.8f}\")\n\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "2373518"}, {"introduction": "While the Arnoldi iteration is powerful, its convergence speed is not uniform across all types of matrices; it can struggle with highly non-normal operators. This practice delves into this behavior by having you construct matrices with tunable non-normality and observe the resulting transient growth in the norm of matrix powers [@problem_id:2373528]. You will then quantify how this theoretical property directly correlates with the practical number of iterations required for Arnoldi to converge, providing a deep insight into the method's performance.", "problem": "You must write a complete and runnable program that constructs specific nonsymmetric matrices whose powers exhibit pronounced transient growth before eventual decay, and then quantifies how this behavior correlates with the convergence of the Arnoldi iteration for approximating the dominant eigenvalue magnitude. Your program must implement the Arnoldi process explicitly, without calling any built-in eigensolvers for Krylov methods. All calculations are purely mathematical; no physical units or angles are involved.\n\nStarting from the following base definitions:\n- The spectral radius of a square matrix $A \\in \\mathbb{C}^{n \\times n}$ is $\\rho(A) \\equiv \\max \\{ |\\lambda| : \\lambda \\text{ is an eigenvalue of } A \\}$.\n- The spectral norm is $\\|A\\|_2 \\equiv \\sigma_{\\max}(A)$, the largest singular value of $A$.\n- A matrix $A$ is non-normal if $A^\\ast A \\neq A A^\\ast$, where $A^\\ast$ denotes the conjugate transpose.\n- The $m$-step Krylov subspace generated by $A$ and a nonzero vector $b$ is $\\mathcal{K}_m(A,b) \\equiv \\text{span}\\{ b, Ab, A^2 b, \\dots, A^{m-1} b \\}$.\n- The Arnoldi iteration constructs an orthonormal basis $V_m$ of $\\mathcal{K}_m(A,b)$ and an upper Hessenberg matrix $H_m \\in \\mathbb{C}^{m \\times m}$ such that $A V_m \\approx V_m H_m$ in the least-squares sense. The eigenvalues of $H_m$ are called Ritz values and approximate the eigenvalues of $A$.\n\nConstruct test matrices by a similarity transformation $A = V \\Lambda V^{-1}$ with prescribed eigenvalues and a tunable non-normality parameter. Let $n \\in \\mathbb{N}$, and define real eigenvalues $\\{\\lambda_i\\}_{i=1}^n$ by\n$$\n\\lambda_i \\equiv 0.95 - 0.75 \\cdot \\frac{i-1}{n-1}, \\quad i = 1,2,\\dots,n,\n$$\nso that $\\rho(A) = \\max_i |\\lambda_i| = 0.95  1$. To build $V$, form an orthogonal matrix $Q$ from the thin QR factorization of a random Gaussian matrix with a fixed seed, and then scale the columns by a geometric progression controlled by $\\gamma  0$:\n$$\nV \\equiv Q D_\\gamma, \\quad D_\\gamma \\equiv \\mathrm{diag}\\big(\\gamma^{0}, \\gamma^{1}, \\dots, \\gamma^{n-1}\\big).\n$$\nWhen $\\gamma = 1$, $V$ is orthogonal and $A$ is normal; when $\\gamma  1$, $V$ is ill-conditioned and $A$ becomes increasingly non-normal.\n\nFor each test matrix $A$ in the suite below, compute two quantities:\n1. The transient growth factor\n$$\nG \\equiv \\max_{1 \\le k \\le K_{\\max}} \\frac{\\|A^k\\|_2}{\\rho(A)^k}.\n$$\nThis measures the worst-case amplification relative to the asymptotic decay rate $\\rho(A)^k$. Because all $\\lambda_i$ satisfy $|\\lambda_i|  1$, $\\|A^k\\|_2$ will eventually decay with $k$, but for non-normal $A$ it can grow substantially for moderate $k$.\n2. The Arnoldi convergence index $m_\\ast$, defined as the smallest $m \\in \\{1,2,\\dots,m_{\\max}\\}$ such that the Ritz value of $H_m$ with largest modulus, call it $\\lambda_{\\text{Ritz}}^{(m)}$, satisfies\n$$\n\\left|\\,|\\lambda_{\\text{Ritz}}^{(m)}| - \\rho(A)\\,\\right| \\le \\varepsilon.\n$$\nIf no such $m$ exists up to $m_{\\max}$ or if the Arnoldi process breaks down before $m_{\\max}$, set $m_\\ast \\equiv -1$.\n\nImplement the Arnoldi iteration with modified Gram–Schmidt orthogonalization. Use a fixed starting vector $b$ drawn from a standard normal distribution and then normalized to unit Euclidean norm.\n\nTest suite (four scalars and one vector fully specify all cases):\n- Matrix size: $n = 20$.\n- Eigenvalues: $\\lambda_i$ as defined above.\n- Random seeds: for $Q$, use seed $12345$; for $b$, use seed $54321$.\n- Growth horizon: $K_{\\max} = 60$.\n- Arnoldi limit and tolerance: $m_{\\max} = 20$, $\\varepsilon = 10^{-6}$.\n- Non-normality parameters: $\\gamma \\in \\{1.0, 1.5, 2.5\\}$.\n\nYour program must produce a single line of output containing a list with one entry per test case, in the same order of the listed $\\gamma$ values. Each entry must itself be a two-element list $[G_{\\mathrm{rounded}}, m_\\ast]$, where $G_{\\mathrm{rounded}}$ is $G$ rounded to three digits after the decimal point, and $m_\\ast$ is an integer as defined above. The final printed line must therefore have the form\n$$\n\\big[ [G_1, m_{\\ast,1}], [G_2, m_{\\ast,2}], [G_3, m_{\\ast,3}] \\big],\n$$\nwith no extra whitespace and no additional text.", "solution": "The fundamental mechanism behind transient growth in powers of a matrix is non-normality. If a matrix $A$ is diagonalizable by a unitary (orthogonal in the real case) similarity transformation, then $\\|A^k\\|_2 = \\rho(A)^k$ for all $k$ because the spectral norm is invariant under unitary similarity and the spectral norm of a normal matrix equals the largest magnitude among its eigenvalues. Hence, for normal $A$ with all eigenvalues strictly inside the unit disk, $\\|A^k\\|_2$ decays monotonically as $k$ increases.\n\nHowever, if $A$ is non-normal, then $\\|A^k\\|_2$ can exhibit significant growth for moderate $k$ before the eventual decay dictated by $\\rho(A)^k$. The simplest analytical illustration uses a Jordan block. Consider $A = \\alpha I + N$ where $\\alpha \\in \\mathbb{C}$ satisfies $|\\alpha|  1$ and $N$ is strictly upper triangular with ones on the first superdiagonal and zeros elsewhere, so that $N$ is nilpotent. Using the binomial identity,\n$$\nA^k = (\\alpha I + N)^k = \\sum_{j=0}^{k} \\binom{k}{j} \\alpha^{k-j} N^j,\n$$\nand noting that $N^j$ has ones along the $j$-th superdiagonal for $j$ up to the matrix size, one sees that $\\|A^k\\|_2$ involves terms of order $\\binom{k}{j} |\\alpha|^{k-j}$, which initially grow with $k$ due to the polynomial factor before the geometric decay from $|\\alpha|^{k}$ dominates. This produces a transient growth whose peak scales roughly like a polynomial in $k$ times $|\\alpha|^k$, resulting in a maximum at some moderate $k$ near $1/(-\\log|\\alpha|)$ when $|\\alpha|$ is close to $1$.\n\nIn computational practice, exact Jordan blocks are rare; instead, strong non-normality occurs when a diagonalizable matrix has a poorly conditioned eigenvector matrix $V$ in the decomposition $A = V \\Lambda V^{-1}$. In that case, the pseudospectrum of $A$ is large and the norms $\\|A^k\\|_2$ can grow transiently. A practical parametrization is to set $V = Q D_\\gamma$, where $Q$ is orthogonal from a thin QR factorization of a Gaussian random matrix, and $D_\\gamma = \\mathrm{diag}(\\gamma^0,\\gamma^1,\\dots,\\gamma^{n-1})$. For $\\gamma = 1$, $V$ is orthogonal and $A$ is normal. For $\\gamma  1$, $V$ becomes increasingly ill-conditioned, and $A$ grows increasingly non-normal while maintaining the same eigenvalues.\n\nWe now relate this to the Arnoldi iteration. The Arnoldi process builds an orthonormal basis $V_m$ for the Krylov subspace $\\mathcal{K}_m(A,b)$ via repeated application of $A$ to $b$ and Gram–Schmidt orthogonalization. The process produces an $m \\times m$ upper Hessenberg matrix $H_m = V_m^\\ast A V_m$ whose eigenvalues (Ritz values) approximate the eigenvalues of $A$. The approximation of the dominant eigenvalue magnitude $\\rho(A)$ by the largest-modulus Ritz value generally improves as $m$ increases. However, for non-normal matrices, the Krylov subspace can be initially dominated by directions associated with transient growth rather than asymptotic eigenvector alignment, leading to a slower convergence of the Ritz values to the true spectral radius. Consequently, we expect a correlation: larger transient growth factor $G \\equiv \\max_{1 \\le k \\le K_{\\max}} \\|A^k\\|_2 / \\rho(A)^k$ tends to be associated with a larger minimal $m$ required by Arnoldi to approximate $\\rho(A)$ within tolerance.\n\nAlgorithmic design in the program follows directly from these principles:\n1. Fix $n$, the eigenvalues $\\{\\lambda_i\\}$, seeds for reproducibility, and the non-normality parameters $\\gamma$ for the test suite. Build $Q$ from the thin QR factorization of a Gaussian matrix generated with the specified seed. For each $\\gamma$, form $V = Q D_\\gamma$ and compute $A = V \\Lambda V^{-1}$. The eigenvalues of $A$ are then precisely $\\{\\lambda_i\\}$, so the spectral radius is $\\rho(A) = \\max_i |\\lambda_i|$ and equals $0.95$ in our construction.\n2. Compute the transient growth factor $G$ by iterating $A^k$ for $k = 1, 2, \\dots, K_{\\max}$ via repeated multiplication, evaluating $\\|A^k\\|_2$ as the largest singular value (the spectral norm) and recording $\\max_k \\|A^k\\|_2 / \\rho(A)^k$. Because $\\rho(A)  1$, $\\|A^k\\|_2$ eventually decays, so the maximum is attained at a moderate $k$ when $A$ is non-normal.\n3. Implement the Arnoldi process with modified Gram–Schmidt to produce $V_m$ and $H_m$ up to $m_{\\max}$. At each $m$, compute the Ritz values as the eigenvalues of $H_m$ and select the one of largest modulus. Find the minimal $m$ such that the absolute error between this modulus and $\\rho(A)$ is less than or equal to the tolerance $\\varepsilon$. If no such $m$ is found or if breakdown occurs before reaching $m_{\\max}$, report $m_\\ast = -1$.\n4. Aggregate the results across the prescribed $\\gamma$ values. For $\\gamma = 1$ (normal case), the transient growth factor $G$ should be close to $1$, and the Arnoldi convergence index $m_\\ast$ should be relatively small. As $\\gamma$ increases, $G$ typically grows above $1$, and $m_\\ast$ tends to increase, reflecting the impact of non-normality on Arnoldi convergence.\n\nFinally, the program prints a single line with the list of results in the exact format required: a list of three two-element lists $[G_{\\mathrm{rounded}}, m_\\ast]$, ordered by the test suite’s $\\gamma$ values.", "answer": "```python\nimport numpy as np\n\ndef build_Q_from_seed(n: int, seed: int) - np.ndarray:\n    rng = np.random.default_rng(seed)\n    X = rng.standard_normal((n, n))\n    Q, _ = np.linalg.qr(X)  # Thin QR; Q is orthonormal\n    return Q\n\ndef build_V(Q: np.ndarray, gamma: float) - np.ndarray:\n    n = Q.shape[0]\n    scales = gamma ** np.arange(n, dtype=float)\n    D = np.diag(scales)\n    return Q @ D\n\ndef build_A_from_V_lambda(V: np.ndarray, lambdas: np.ndarray) - np.ndarray:\n    # A = V * diag(lambdas) * V^{-1}\n    n = V.shape[0]\n    Vinv = np.linalg.inv(V)\n    return V @ np.diag(lambdas) @ Vinv\n\ndef spectral_radius(lambdas: np.ndarray) - float:\n    return float(np.max(np.abs(lambdas)))\n\ndef spectral_norm(A: np.ndarray) - float:\n    # 2-norm via largest singular value\n    # Use SVD with no vectors for stability on small n\n    s = np.linalg.svd(A, compute_uv=False)\n    return float(s[0])\n\ndef transient_growth_factor(A: np.ndarray, rho: float, Kmax: int) - float:\n    n = A.shape[0]\n    Apow = np.eye(n)\n    max_ratio = 0.0\n    for k in range(1, Kmax + 1):\n        Apow = Apow @ A\n        norm_Ak = spectral_norm(Apow)\n        denom = rho ** k\n        # Avoid division by extremely small numbers; rho^k > 0 since rho>0; here rho=0.95>0.\n        ratio = norm_Ak / denom if denom != 0.0 else np.inf\n        if ratio > max_ratio:\n            max_ratio = ratio\n    return max_ratio\n\ndef arnoldi_modified_gram_schmidt(A: np.ndarray, b: np.ndarray, m_max: int):\n    n = A.shape[0]\n    V = np.zeros((n, m_max + 1))\n    H = np.zeros((m_max + 1, m_max))\n    beta = np.linalg.norm(b)\n    if beta == 0.0:\n        raise ValueError(\"Starting vector b must be nonzero.\")\n    V[:, 0] = b / beta\n    m_effective = 0\n    for j in range(m_max):\n        w = A @ V[:, j]\n        # Modified Gram-Schmidt\n        for i in range(j + 1):\n            H[i, j] = np.dot(V[:, i].conj(), w)\n            w = w - H[i, j] * V[:, i]\n        # Optional one-step reorthogonalization for numerical stability\n        for i in range(j + 1):\n            h_correction = np.dot(V[:, i].conj(), w)\n            H[i, j] += h_correction\n            w = w - h_correction * V[:, i]\n        h_next = np.linalg.norm(w)\n        H[j + 1, j] = h_next\n        m_effective = j + 1\n        if h_next = 1e-14:\n            # Happy breakdown: Krylov subspace is invariant; stop early\n            break\n        V[:, j + 1] = w / h_next\n    return V[:, :m_effective + 1], H[:m_effective + 1, :m_effective]\n\ndef arnoldi_convergence_index(A: np.ndarray, b: np.ndarray, rho: float, m_max: int, tol: float) - int:\n    V, H_full = arnoldi_modified_gram_schmidt(A, b, m_max)\n    # H_full has shape (m_eff+1, m_eff); number of completed Arnoldi steps is m_eff\n    m_eff = H_full.shape[1]\n    m_star = -1\n    for m in range(1, m_eff + 1):\n        Hm = H_full[:m, :m]\n        ritz = np.linalg.eigvals(Hm)\n        # Choose Ritz value with largest modulus\n        idx = int(np.argmax(np.abs(ritz)))\n        ritz_max = ritz[idx]\n        err = abs(abs(ritz_max) - rho)\n        if err = tol:\n            m_star = m\n            break\n    # If we didn't reach m_max due to breakdown and no convergence, return -1\n    if m_star == -1 and m_eff  m_max:\n        return -1\n    # If we reached m_max and still no convergence, also return -1\n    return m_star\n\ndef solve():\n    # Test suite parameters\n    n = 20\n    # Define eigenvalues: lambda_i = 0.95 - 0.75 * (i-1)/(n-1), i=1..n\n    lam = np.array([0.95 - 0.75 * (i) / (n - 1) for i in range(0, n)], dtype=float)\n    rho = spectral_radius(lam)\n    seed_Q = 12345\n    seed_b = 54321\n    Kmax = 60\n    m_max = 20\n    tol = 1e-6\n    gammas = [1.0, 1.5, 2.5]\n\n    # Build Q and starting vector b\n    Q = build_Q_from_seed(n, seed_Q)\n    rng_b = np.random.default_rng(seed_b)\n    b = rng_b.standard_normal(n)\n    b_norm = np.linalg.norm(b)\n    if b_norm == 0.0:\n        b[0] = 1.0\n        b_norm = 1.0\n    b = b / b_norm\n\n    results = []\n    for gamma in gammas:\n        V = build_V(Q, gamma)\n        A = build_A_from_V_lambda(V, lam)\n        G = transient_growth_factor(A, rho, Kmax)\n        m_star = arnoldi_convergence_index(A, b, rho, m_max, tol)\n        # Round G to three decimals\n        G_round = round(G + 1e-12, 3)  # small epsilon to stabilize rounding\n        results.append([G_round, m_star])\n\n    # Print in exact required format: no spaces\n    out = \"[\" + \",\".join(f\"[{res[0]},{res[1]}]\" for res in results) + \"]\"\n    print(out)\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "2373528"}, {"introduction": "In many applications, we need to find several dominant eigenpairs, not just one. A simple restarted Arnoldi method can be inefficient, repeatedly rediscovering the same eigenpairs. This advanced exercise introduces a crucial practical technique called \"locking and deflation\" to build a more efficient, robust eigensolver [@problem_id:2373565]. You will implement and compare a method that deflates converged eigenvectors from the search space against a simpler approach, quantifying the significant performance gains.", "problem": "Implement a complete and runnable program that compares two approaches for approximating the eigenpairs of largest magnitude of given real, nonsymmetric matrices: one approach that maintains and enforces orthogonality to a set of previously converged approximate eigenvectors (locking with deflation), and one approach that does not maintain such a locked set across restarts. The goal is to quantify how locking reduces the number of matrix-vector products required to obtain a prescribed number of dominant eigenpairs. Your program must be deterministic and must not read input.\n\nYou are given the following specifications.\n\n1) Mathematical setting.\n\n- Let $A \\in \\mathbb{R}^{n \\times n}$ be a real, nonsymmetric matrix. For any nonzero vector $u \\in \\mathbb{C}^{n}$ and scalar $\\theta \\in \\mathbb{C}$, define the residual of an approximate eigenpair $(\\theta,u)$ by $r \\equiv A u - \\theta u$. The residual norm is $\\lVert r \\rVert_{2}$.\n- A pair $(\\theta,u)$ with $\\lVert u \\rVert_{2} = 1$ is considered converged if $\\lVert A u - \\theta u \\rVert_{2} \\le \\varepsilon$, where $\\varepsilon$ is a given tolerance.\n- A dominant eigenpair is one whose eigenvalue has maximal absolute value relative to the rest. For a requested number $k \\in \\mathbb{N}$, the target set is the $k$ eigenpairs corresponding to the eigenvalues with the $k$ largest absolute values (ties may be resolved arbitrarily).\n\n2) Subspace construction with and without locking.\n\n- Across iterations, construct Krylov subspaces of dimension at most $m \\in \\mathbb{N}$ generated by repeated applications of $A$ to an initial vector. Each application of $A$ to a vector counts as one matrix-vector product. After a subspace of dimension at most $m$ is built and processed, you may restart by discarding the subspace and starting from a new initial vector. This defines a cycle. You may perform multiple cycles until the requested number $k$ of dominant eigenpairs has converged.\n- Locking with deflation: maintain a set $\\mathcal{L}$ of currently converged unit-norm approximate eigenvectors. At all times, ensure any newly generated vector $w$ used to expand the subspace satisfies $u^{*} w = 0$ for all $u \\in \\mathcal{L}$ (orthogonality with respect to the standard complex inner product), i.e., project $w$ onto the orthogonal complement of $\\mathrm{span}(\\mathcal{L})$. When new approximate eigenpairs converge, append their vectors (after orthonormalization) to $\\mathcal{L}$. Continue until $|\\mathcal{L}| = k$.\n- No locking: do not maintain any $\\mathcal{L}$ across cycles. In each cycle, identify converged approximate eigenpairs inside the current subspace; accumulate their eigenvalues across cycles as a set of unique values (distinct up to a small threshold in absolute difference) until $k$ unique converged eigenvalues have been collected. Do not deflate previously discovered vectors when constructing new subspaces.\n\n3) Convergence assessment inside a subspace.\n\n- In any subspace of dimension $j \\le m$ with an orthonormal basis $V_{j} \\in \\mathbb{C}^{n \\times j}$, form a projected matrix $H_{j} \\in \\mathbb{C}^{j \\times j}$, and compute its eigenpairs $(\\theta, y)$ with $y \\in \\mathbb{C}^{j}$. For each such pair, form the Ritz vector $u = V_{j} y / \\lVert V_{j} y \\rVert_{2}$ and the residual norm $\\lVert A u - \\theta u \\rVert_{2}$. Use these residual norms to decide convergence with the tolerance $\\varepsilon$.\n- Within any cycle, when identifying which converged approximate eigenpairs to record (and to lock, in the locking variant), prioritize those associated with the largest absolute values $|\\theta|$, and avoid counting or locking duplicates by comparing eigenvalues in absolute difference against a small threshold.\n\n4) Test suite.\n\nFor each test case, construct $A$ as $A = S D S^{-1}$, where $D$ is diagonal with prescribed diagonal entries and $S$ is a dense real matrix generated pseudorandomly with a fixed seed and entries drawn uniformly from the interval $[-1,1]$ until an invertible $S$ is obtained. All initial vectors for cycles are also drawn pseudorandomly using the same seed specification for the case, and are projected to be orthogonal to the locked set when locking is active. All random draws must be reproducible.\n\nThe test suite consists of three cases:\n\n- Case 1 (happy path): $n = 8$, $D = [5.0, 3.0, 2.0, 1.0, 0.5, -0.2, -1.5, 4.0]$, $k = 3$, $m = 4$, $\\varepsilon = 10^{-8}$, maximum number of cycles $C_{\\max} = 200$, random seed $10$.\n- Case 2 (close eigenvalues): $n = 10$, $D = [4.0, 3.99, 1.0, 0.1, -0.1, 2.0, -2.0, 0.5, 0.49, -3.0]$, $k = 3$, $m = 5$, $\\varepsilon = 10^{-8}$, $C_{\\max} = 200$, random seed $21$.\n- Case 3 (single dominant, small subspace): $n = 6$, $D = [-5.0, -1.0, -2.0, 0.1, 0.2, 0.3]$, $k = 1$, $m = 2$, $\\varepsilon = 10^{-8}$, $C_{\\max} = 200$, random seed $7$.\n\nAll numbers above are dimensionless. Express any absolute differences and tolerances as real numbers.\n\n5) Required outputs.\n\n- For each case, run both approaches until the requested number $k$ of dominant eigenpairs have converged according to the criterion $\\lVert A u - \\theta u \\rVert_{2} \\le \\varepsilon$, or until the cycle limit $C_{\\max}$ is reached. Count matrix-vector products as the total number of applications of $A$ to a vector incurred during subspace expansions across cycles, separately for the no-locking and locking approaches.\n- If $C_{\\max}$ is reached without meeting the convergence target in either approach, return the number of matrix-vector products accrued so far for that approach.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. For each case, output a two-element list $[M_{\\mathrm{no\\_lock}}, M_{\\mathrm{lock}}]$ of integers representing the matrix-vector product counts for the no-locking and locking approaches, respectively. The final format must be a single line like $[[a,b],[c,d],[e,f]]$ with integers $a,b,c,d,e,f$.", "solution": "Let us first formalize the common components of the algorithm. The method generates a sequence of Krylov subspaces $\\mathcal{K}_j(A, v_1) = \\mathrm{span}\\{v_1, Av_1, \\dots, A^{j-1}v_1\\}$, where $v_1$ is a starting vector with $\\lVert v_1 \\rVert_2 = 1$. The Arnoldi process constructs an orthonormal basis $V_j = [v_1, v_2, \\dots, v_j]$ for $\\mathcal{K}_j(A, v_1)$. This process simultaneously generates an upper Hessenberg matrix $H_j = V_j^* A V_j \\in \\mathbb{C}^{j \\times j}$, where the asterisk denotes the conjugate transpose. The key Arnoldi relation after $m$ steps is:\n$$\nA V_m = V_m H_m + h_{m+1,m} v_{m+1} e_m^T\n$$\nwhere $V_m \\in \\mathbb{C}^{n \\times m}$ has orthonormal columns, $H_m \\in \\mathbb{C}^{m \\times m}$ is upper Hessenberg, $v_{m+1}$ is a unit vector orthogonal to the columns of $V_m$, $h_{m+1,m}$ is a non-negative scalar, and $e_m$ is the $m$-th standard basis vector in $\\mathbb{R}^m$. Each column of $V_m$ requires one matrix-vector product with $A$, which is the primary measure of computational cost.\n\nFrom the small $m \\times m$ eigenproblem for $H_m$, $H_m y_i = \\theta_i y_i$, we extract approximate eigenpairs for $A$. These are called Ritz pairs $(\\theta_i, u_i)$, where $\\theta_i$ is a Ritz value and $u_i = V_m y_i$ is the corresponding Ritz vector. The problem requires assessing convergence by checking if the residual norm $\\lVert A u_i - \\theta_i u_i \\rVert_2 \\le \\varepsilon$ for a given tolerance $\\varepsilon$. Using the Arnoldi relation, this norm can be calculated efficiently without an additional matrix-vector product:\n$$\n\\begin{aligned}\nA u_i - \\theta_i u_i = A(V_m y_i) - \\theta_i(V_m y_i) \\\\\n= (AV_m) y_i - V_m(\\theta_i y_i) \\\\\n= (V_m H_m + h_{m+1,m} v_{m+1} e_m^T) y_i - V_m(H_m y_i) \\\\\n= V_m H_m y_i + h_{m+1,m} v_{m+1} (e_m^T y_i) - V_m H_m y_i \\\\\n= (h_{m+1,m} \\cdot (y_i)_m) v_{m+1}\n\\end{aligned}\n$$\nwhere $(y_i)_m$ is the last component of the eigenvector $y_i$. Since $\\lVert v_{m+1} \\rVert_2 = 1$, the residual norm is exactly $\\lVert A u_i - \\theta_i u_i \\rVert_2 = |h_{m+1,m}| \\cdot |(y_i)_m|$.\n\nThe process is restarted after building a subspace of dimension $m$ (a cycle). The two approaches differ in how they utilize information from previous cycles.\n\n1.  **No-Locking Approach:** This is the simpler strategy. Each cycle is independent. It starts with a new, randomly generated vector $v_1$. Within a cycle, it identifies all converged Ritz pairs. The corresponding Ritz values $\\theta_i$ are collected. To avoid duplicates, a newly found $\\theta_i$ is added to the set of converged eigenvalues only if it is sufficiently different from all previously found values, i.e., $|\\theta_i - \\theta_{found}| \\ge \\delta$ for all $\\theta_{found}$ in the set, where $\\delta$ is a small uniqueness tolerance. The process terminates when $k$ unique eigenvalues have been found or the maximum number of cycles $C_{\\max}$ is reached. This method may repeatedly find the same dominant eigenpairs in successive cycles, which can be inefficient.\n\n2.  **Locking with Deflation Approach:** This method aims to improve efficiency by preventing the re-computation of converged eigenpairs. It maintains an explicit, orthonormal set $\\mathcal{L} = \\{u_1, \\dots, u_p\\}$ of previously converged and \"locked\" eigenvectors, where $p  k$. At the start of a new cycle, the initial vector $v_1$ is generated randomly and then explicitly orthogonalized against the space spanned by $\\mathcal{L}$: $v_1 \\leftarrow v_1 - \\sum_{i=1}^p (u_i^* v_1) u_i$. The Arnoldi iteration then proceeds in the subspace orthogonal to $\\mathrm{span}(\\mathcal{L})$, effectively \"deflating\" the known eigenvectors from the problem. When new Ritz pairs converge, their Ritz vectors are candidates for being locked. To maintain the properties of $\\mathcal{L}$, a new candidate vector $u_{new}$ is first orthogonalized against the current set $\\mathcal{L}$ and then normalized before being added. This ensures that the algorithm focuses its search on the yet-uncovered part of the spectrum. The process continues until $|\\mathcal{L}| = k$.\n\nFor both methods, the test matrices are constructed as $A = S D S^{-1}$, where $D$ is a diagonal matrix with prescribed eigenvalues and $S$ is a randomly generated, invertible real matrix. The entire procedure, including matrix generation and the selection of starting vectors, is made deterministic by using a fixed-seed pseudorandom number generator. For a fair comparison, both algorithms are executed with identical sequences of random numbers by re-initializing the generator for each run.", "answer": "```python\nimport numpy as np\n\ndef arnoldi_iteration(A, v_start, m, locked_vectors):\n    \"\"\"\n    Performs m steps of the Arnoldi iteration to build a Krylov subspace.\n    Handles explicit deflation against locked vectors.\n    \"\"\"\n    n = A.shape[0]\n    V = np.zeros((n, m + 1), dtype=np.complex128)\n    H = np.zeros((m + 1, m), dtype=np.complex128)\n    \n    mat_vec_count = 0\n    \n    # Start vector processing\n    v = v_start\n    if locked_vectors.shape[1] > 0:\n        v = v - locked_vectors @ (locked_vectors.conj().T @ v)\n    \n    v_norm = np.linalg.norm(v)\n    if v_norm  1e-12:\n        return None, None, None, mat_vec_count, 0 # Start vector is in locked space\n\n    V[:, 0] = v / v_norm\n    \n    actual_m = m\n    for j in range(m):\n        # Apply operator and count\n        w = A @ V[:, j]\n        mat_vec_count += 1\n        \n        # Deflate against locked vectors\n        if locked_vectors.shape[1] > 0:\n            w = w - locked_vectors @ (locked_vectors.conj().T @ w)\n            \n        # Modified Gram-Schmidt against current Krylov basis V\n        for i in range(j + 1):\n            H[i, j] = V[:, i].conj().T @ w\n            w = w - H[i, j] * V[:, i]\n            \n        H[j + 1, j] = np.linalg.norm(w)\n        \n        if H[j + 1, j]  1e-12:  # Breakdown\n            actual_m = j + 1\n            break\n            \n        V[:, j + 1] = w / H[j + 1, j]\n        \n    return V[:, :actual_m], H[:actual_m, :actual_m], H[actual_m, actual_m - 1], mat_vec_count, actual_m\n\ndef run_without_locking(A, k, m, tol, max_cycles, rng):\n    \"\"\"\n    Runs restarted Arnoldi without locking/deflation.\n    \"\"\"\n    n = A.shape[0]\n    total_mat_vecs = 0\n    found_eigenvalues = set()\n    \n    for _ in range(max_cycles):\n        if len(found_eigenvalues) >= k:\n            break\n            \n        v_start = rng.random(n)\n        \n        # We pass an empty locked_vectors set, so no deflation occurs.\n        V, H, h_next, mv_cycle, actual_m = arnoldi_iteration(\n            A, v_start, m, np.zeros((n, 0))\n        )\n        \n        total_mat_vecs += mv_cycle\n        if V is None or actual_m == 0:\n            continue\n            \n        ritz_vals, ritz_vecs_H = np.linalg.eig(H)\n        \n        converged_pairs = []\n        for theta, y in zip(ritz_vals, ritz_vecs_H.T):\n            residual_norm = abs(h_next) * abs(y[-1])\n            if residual_norm  tol:\n                converged_pairs.append(theta)\n\n        # Prioritize by magnitude and add unique values\n        converged_pairs.sort(key=abs, reverse=True)\n        for theta in converged_pairs:\n            is_new = True\n            for found_val in found_eigenvalues:\n                if abs(theta - found_val)  1e-6:\n                    is_new = False\n                    break\n            if is_new and len(found_eigenvalues)  k:\n                found_eigenvalues.add(theta)\n                \n    return total_mat_vecs\n\ndef run_with_locking(A, k, m, tol, max_cycles, rng):\n    \"\"\"\n    Runs restarted Arnoldi with explicit locking and deflation.\n    \"\"\"\n    n = A.shape[0]\n    total_mat_vecs = 0\n    locked_vectors = np.zeros((n, 0), dtype=np.complex128)\n    locked_eigenvalues = []\n\n    for _ in range(max_cycles):\n        if len(locked_eigenvalues) >= k:\n            break\n\n        v_start = rng.random(n)\n        \n        V, H, h_next, mv_cycle, actual_m = arnoldi_iteration(\n            A, v_start, m, locked_vectors\n        )\n        \n        total_mat_vecs += mv_cycle\n        if V is None or actual_m == 0:\n            continue\n\n        ritz_vals, ritz_vecs_H = np.linalg.eig(H)\n\n        newly_converged = []\n        for theta, y in zip(ritz_vals, ritz_vecs_H.T):\n            residual_norm = abs(h_next) * abs(y[-1])\n            is_new = True\n            for locked_val in locked_eigenvalues:\n                if abs(theta - locked_val)  1e-6:\n                    is_new = False\n                    break\n            if is_new and residual_norm  tol:\n                u = V @ y\n                newly_converged.append({'val': theta, 'vec': u / np.linalg.norm(u)})\n        \n        # Sort candidates by magnitude before attempting to lock\n        newly_converged.sort(key=lambda p: abs(p['val']), reverse=True)\n\n        for p in newly_converged:\n            if len(locked_eigenvalues) >= k:\n                break\n            \n            u_candidate = p['vec']\n            # Deflate candidate against current locked set\n            u_proj = u_candidate - locked_vectors @ (locked_vectors.conj().T @ u_candidate)\n            \n            # If it's not in the span of locked vectors, add it.\n            if np.linalg.norm(u_proj) > 1e-6:\n                u_ortho = (u_proj / np.linalg.norm(u_proj)).reshape(-1, 1)\n                locked_vectors = np.hstack([locked_vectors, u_ortho])\n                locked_eigenvalues.append(p['val'])\n                \n    return total_mat_vecs\n\ndef run_case(n, d_diag, k, m, tol, max_cycles, seed):\n    \"\"\"\n    Sets up a test case and runs both algorithms.\n    \"\"\"\n    # Create the test matrix A = S D S^-1\n    # Use a separate RNG for matrix generation to not interfere with start vectors\n    rng_A = np.random.default_rng(seed)\n    while True:\n        S = rng_A.uniform(-1, 1, size=(n, n))\n        if np.linalg.cond(S)  1 / np.finfo(S.dtype).eps:\n            break\n    D = np.diag(d_diag)\n    A = S @ D @ np.linalg.inv(S)\n\n    # Run without locking. Create a new RNG seeded for this run.\n    rng_no_lock = np.random.default_rng(seed)\n    mat_vec_no_lock = run_without_locking(A, k, m, tol, max_cycles, rng_no_lock)\n    \n    # Run with locking. Create another new RNG with the same seed.\n    # This ensures both algorithms use the same sequence of random start vectors.\n    rng_lock = np.random.default_rng(seed)\n    mat_vec_lock = run_with_locking(A, k, m, tol, max_cycles, rng_lock)\n    \n    return [mat_vec_no_lock, mat_vec_lock]\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print results.\n    \"\"\"\n    test_cases = [\n        (8, [5.0, 3.0, 2.0, 1.0, 0.5, -0.2, -1.5, 4.0], 3, 4, 1e-8, 200, 10),\n        (10, [4.0, 3.99, 1.0, 0.1, -0.1, 2.0, -2.0, 0.5, 0.49, -3.0], 3, 5, 1e-8, 200, 21),\n        (6, [-5.0, -1.0, -2.0, 0.1, 0.2, 0.3], 1, 2, 1e-8, 200, 7),\n    ]\n\n    all_results = []\n    for params in test_cases:\n        result_pair = run_case(*params)\n        all_results.append(result_pair)\n\n    formatted_results = \",\".join([f\"[{r[0]},{r[1]}]\" for r in all_results])\n    print(f\"[{formatted_results}]\")\n\nsolve()\n```", "id": "2373565"}]}