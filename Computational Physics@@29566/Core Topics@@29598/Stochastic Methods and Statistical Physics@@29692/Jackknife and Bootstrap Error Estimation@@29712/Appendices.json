{"hands_on_practices": [{"introduction": "Our first practice exercise takes a hands-on, first-principles approach to the jackknife method. We will manually calculate the bias and standard error for a non-standard estimator—the midrange—to build a deep, mechanical intuition for how delete-one resampling works. This problem highlights the method's power on estimators where standard formulas for uncertainty might not apply or are difficult to derive analytically [@problem_id:2404332].", "problem": "A one-dimensional detector in a computational physics experiment records the impact positions of neutral atoms confined in a rectangular trapping region whose center $c$ is unknown. The trapping potential ensures that the impact position $X$ is uniformly distributed on an interval $[c - w, c + w]$ for some half-width $w > 0$. From a single experimental run, you obtain $n = 9$ independent position measurements\n$$\nx_1 = -1.02,\\; x_2 = -0.83,\\; x_3 = -0.50,\\; x_4 = -0.12,\\; x_5 = 0.05,\\; x_6 = 0.31,\\; x_7 = 0.47,\\; x_8 = 0.88,\\; x_9 = 1.20.\n$$\nTo estimate the center $c$, you consider the midrange estimator\n$$\n\\hat{\\theta} = \\frac{\\max(x_1,\\dots,x_n) + \\min(x_1,\\dots,x_n)}{2}.\n$$\nUsing only the foundational definitions of the jackknife method (delete-one resampling and the resulting leave-one-out estimates), derive from first principles the jackknife estimate of the bias of $\\hat{\\theta}$ and the jackknife estimate of the standard error of $\\hat{\\theta}$. Then compute their numerical values for the dataset above.\n\nReport your final result as a single row vector containing the two numbers $[\\hat{b}_{\\mathrm{jack}},\\;\\hat{s}_{\\mathrm{jack}}]$, where $\\hat{b}_{\\mathrm{jack}}$ is the jackknife bias estimate and $\\hat{s}_{\\mathrm{jack}}$ is the jackknife standard error. Round both entries to four significant figures. No physical units are required in the final numerical values.", "solution": "The problem statement is subjected to validation.\n\n**Step 1: Extract Givens**\n- **Physical System**: A one-dimensional detector records impact positions of neutral atoms.\n- **Distribution**: The impact position $X$ is a random variable uniformly distributed on the interval $[c - w, c + w]$, where $c$ is the unknown center and $w > 0$ is the half-width.\n- **Dataset**: A set of $n = 9$ independent measurements is provided: $x_1 = -1.02$, $x_2 = -0.83$, $x_3 = -0.50$, $x_4 = -0.12$, $x_5 = 0.05$, $x_6 = 0.31$, $x_7 = 0.47$, $x_8 = 0.88$, $x_9 = 1.20$.\n- **Estimator**: The estimator for the center $c$ is the midrange, given by $\\hat{\\theta} = \\frac{\\max(x_1,\\dots,x_n) + \\min(x_1,\\dots,x_n)}{2}$.\n- **Task**: Derive from first principles and compute the jackknife estimate of the bias of $\\hat{\\theta}$ and the jackknife estimate of the standard error of $\\hat{\\theta}$.\n- **Reporting**: The final result must be a row vector $[\\hat{b}_{\\mathrm{jack}},\\;\\hat{s}_{\\mathrm{jack}}]$ with numerical values rounded to four significant figures.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientific Grounding**: The problem is scientifically sound. It presents a simplified but plausible situation in experimental physics (particle trapping and detection) and proposes to analyze it using a standard, well-established statistical technique (jackknife resampling). The uniform distribution is a valid and common probability model.\n- **Well-Posedness**: The problem is well-posed. It provides all necessary information: a specific dataset, a clearly defined estimator, and a specified statistical method. The objectives—calculating the bias and standard error estimates—are unambiguous.\n- **Objectivity**: The problem is stated in precise, objective language, free of subjective claims or ambiguity.\n\n**Step 3: Verdict and Action**\nThe problem is valid. It is scientifically grounded, well-posed, objective, and contains sufficient information for a unique solution. Proceed with the solution.\n\nThe jackknife method is a resampling technique used to estimate the bias and standard error of an estimator. The derivation proceeds from first principles as required.\n\nLet the full dataset be $X = \\{x_1, x_2, \\dots, x_n\\}$, where $n=9$. The estimator for the parameter $\\theta=c$ is $\\hat{\\theta} = \\hat{\\theta}(X)$.\nThe given estimator is the midrange:\n$$\n\\hat{\\theta} = \\frac{x_{(n)} + x_{(1)}}{2}\n$$\nwhere $x_{(1)} = \\min(x_1, \\dots, x_n)$ and $x_{(n)} = \\max(x_1, \\dots, x_n)$ are the order statistics of the sample.\n\nFirst, we compute the value of the estimator $\\hat{\\theta}$ for the full sample. By inspection of the data, we find:\n$x_{(1)} = -1.02$\n$x_{(n)} = x_{(9)} = 1.20$\nThus, the full-sample estimate is:\n$$\n\\hat{\\theta} = \\frac{1.20 + (-1.02)}{2} = \\frac{0.18}{2} = 0.09\n$$\nThe jackknife procedure requires computing leave-one-out estimates. Let $X_{(i)}$ be the dataset with the $i$-th observation $x_i$ removed. The corresponding estimate is $\\hat{\\theta}_{(i)} = \\hat{\\theta}(X_{(i)})$. There are $n=9$ such estimates.\n\nThe estimator $\\hat{\\theta}$ depends only on the minimum and maximum of the sample. Let the sorted sample be $x_{(1)}  x_{(2)}  \\dots  x_{(n)}$.\n- If we remove an observation $x_j$ that is not an extremum (i.e., $x_{(1)}  x_j  x_{(n)}$), the minimum and maximum of the subsample remain $x_{(1)}$ and $x_{(n)}$. For the given dataset, this is true for the $n-2 = 7$ points from $x_2 = -0.83$ to $x_8 = 0.88$. For these points, the leave-one-out estimate is unchanged:\n$$\n\\hat{\\theta}_{(i)} = \\frac{x_{(n)} + x_{(1)}}{2} = \\hat{\\theta} = 0.09 \\quad \\text{for } i \\in \\{2, 3, 4, 5, 6, 7, 8\\}\n$$\n- If we remove the minimum value of the sample, $x_1 = x_{(1)} = -1.02$, the new minimum is the second-smallest value, $x_{(2)} = -0.83$, while the maximum remains $x_{(9)} = 1.20$.\n$$\n\\hat{\\theta}_{(1)} = \\frac{x_{(9)} + x_{(2)}}{2} = \\frac{1.20 + (-0.83)}{2} = \\frac{0.37}{2} = 0.185\n$$\n- If we remove the maximum value of the sample, $x_9 = x_{(n)} = 1.20$, the new maximum is the second-largest value, $x_{(8)} = 0.88$, while the minimum remains $x_{(1)} = -1.02$.\n$$\n\\hat{\\theta}_{(9)} = \\frac{x_{(8)} + x_{(1)}}{2} = \\frac{0.88 + (-1.02)}{2} = \\frac{-0.14}{2} = -0.07\n$$\nThe set of leave-one-out estimates is: $\\{0.185, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, -0.07\\}$.\n\nThe jackknife estimate of bias, $\\hat{b}_{\\mathrm{jack}}$, is derived from the mean of these leave-one-out estimates. The bias of $\\hat{\\theta}$, $B(\\hat{\\theta}) = E[\\hat{\\theta}] - \\theta$, can be approximated by $(n-1)(\\bar{\\theta}_{(\\cdot)} - \\hat{\\theta})$, where $\\bar{\\theta}_{(\\cdot)}$ is the mean of the $\\hat{\\theta}_{(i)}$ values.\n$$\n\\hat{b}_{\\mathrm{jack}} = (n-1)(\\bar{\\theta}_{(\\cdot)} - \\hat{\\theta})\n$$\nwhere $\\bar{\\theta}_{(\\cdot)} = \\frac{1}{n} \\sum_{i=1}^n \\hat{\\theta}_{(i)}$.\nFirst, we compute $\\bar{\\theta}_{(\\cdot)}$:\n$$\n\\sum_{i=1}^9 \\hat{\\theta}_{(i)} = \\hat{\\theta}_{(1)} + \\hat{\\theta}_{(9)} + 7 \\times \\hat{\\theta} = 0.185 + (-0.07) + 7 \\times 0.09 = 0.115 + 0.63 = 0.745\n$$\n$$\n\\bar{\\theta}_{(\\cdot)} = \\frac{0.745}{9} \\approx 0.082777...\n$$\nNow, we compute the bias estimate:\n$$\n\\hat{b}_{\\mathrm{jack}} = (9-1) \\left(\\frac{0.745}{9} - 0.09\\right) = 8 \\left(\\frac{0.745 - 0.81}{9}\\right) = 8 \\left(\\frac{-0.065}{9}\\right) = \\frac{-0.52}{9} \\approx -0.057777...\n$$\nRounding to four significant figures, $\\hat{b}_{\\mathrm{jack}} = -0.05778$.\n\nThe jackknife estimate of the standard error of $\\hat{\\theta}$, denoted $\\hat{s}_{\\mathrm{jack}}$, is derived from the variance of the leave-one-out estimates. The jackknife variance estimate of $\\hat{\\theta}$ is:\n$$\n\\hat{s}_{\\mathrm{jack}}^2 = \\widehat{\\mathrm{Var}}_{\\mathrm{jack}}(\\hat{\\theta}) = \\frac{n-1}{n} \\sum_{i=1}^n (\\hat{\\theta}_{(i)} - \\bar{\\theta}_{(\\cdot)})^2\n$$\nThis formula comes from the sample variance of the \"pseudovalues\" $\\psi_i = n\\hat{\\theta} - (n-1)\\hat{\\theta}_{(i)}$, as the standard error of their mean is taken as the standard error of $\\hat{\\theta}$.\nWe compute the sum of squared differences:\n$$\n\\sum_{i=1}^9 (\\hat{\\theta}_{(i)} - \\bar{\\theta}_{(\\cdot)})^2 = (\\hat{\\theta}_{(1)} - \\bar{\\theta}_{(\\cdot)})^2 + (\\hat{\\theta}_{(9)} - \\bar{\\theta}_{(\\cdot)})^2 + \\sum_{i \\in \\{2..8\\}} (\\hat{\\theta}_{(i)} - \\bar{\\theta}_{(\\cdot)})^2\n$$\nUsing the numerical values:\n$$\n(0.185 - \\frac{0.745}{9})^2 + (-0.07 - \\frac{0.745}{9})^2 + 7 \\times (0.09 - \\frac{0.745}{9})^2\n$$\n$$\n\\approx (0.185 - 0.082778)^2 + (-0.07 - 0.082778)^2 + 7 \\times (0.09 - 0.082778)^2\n$$\n$$\n\\approx (0.102222)^2 + (-0.152778)^2 + 7 \\times (0.007222)^2\n$$\n$$\n\\approx 0.01044938 + 0.02334105 + 7 \\times 0.000052157\n$$\n$$\n\\approx 0.01044938 + 0.02334105 + 0.00036510\n$$\n$$\n\\sum_{i=1}^9 (\\hat{\\theta}_{(i)} - \\bar{\\theta}_{(\\cdot)})^2 \\approx 0.03415553\n$$\nNow, we calculate the variance estimate:\n$$\n\\hat{s}_{\\mathrm{jack}}^2 = \\frac{9-1}{9} \\times 0.03415553 = \\frac{8}{9} \\times 0.03415553 \\approx 0.03036047\n$$\nThe standard error is the square root of the variance:\n$$\n\\hat{s}_{\\mathrm{jack}} = \\sqrt{0.03036047} \\approx 0.1742426\n$$\nRounding to four significant figures, $\\hat{s}_{\\mathrm{jack}} = 0.1742$.\n\nThe final result is the row vector containing the jackknife bias and standard error estimates.", "answer": "$$\n\\boxed{\\begin{pmatrix} -0.05778  0.1742 \\end{pmatrix}}\n$$", "id": "2404332"}, {"introduction": "Moving from manual calculation to computational implementation, we now turn to the bootstrap method in a classic physics context. This exercise simulates the process of determining a material's Young's modulus from noisy stress-strain data, using bootstrap resampling to quantify the uncertainty in the estimated parameter. This is a quintessential example of applying computational statistics to determine the reliability of a physical quantity derived from a model fit [@problem_id:2404303].", "problem": "You are given a computational physics task involving estimation of uncertainty in the Young’s modulus of a simulated nanowire from the slope of a fitted stress–strain curve. Assume that, for small strains, linear elasticity applies and the stress–strain relation is governed by Hooke’s law, where the axial stress $\\sigma$ is proportional to the axial strain $\\varepsilon$ and the constant of proportionality is the Young’s modulus. You will simulate measurements of stress at given strains by adding independent, identically distributed measurement noise to a linear relation with a known true modulus. From these simulated $(\\text{strain}, \\text{stress})$ pairs, you will estimate the Young’s modulus as the slope of a linear fit with zero intercept. You will then estimate the uncertainty of this slope using the bootstrap method by resampling the data pairs with replacement and recomputing the slope for each resample. Your task is to implement this end-to-end procedure for several test cases and to report the bootstrap standard error of the estimated Young’s modulus. All stresses must be expressed in gigapascals, and strain is dimensionless. All answers must be reported in gigapascals and rounded to exactly six digits after the decimal point.\n\nUsing the following fundamental base:\n- Hooke’s law in the small-strain regime: the axial stress $\\sigma$ is proportional to the axial strain $\\varepsilon$ through the Young’s modulus $E$.\n- Ordinary least squares as the standard method for estimating a linear relationship from noisy data.\n- The bootstrap principle: the sampling distribution of an estimator can be approximated by resampling with replacement from the observed data and recomputing the estimator on each resample.\n\nImplement a program that:\n1. Generates synthetic $(\\varepsilon_i, \\sigma_i)$ data for each test case as follows. For each case, draw $n$ strain values that are linearly spaced between a minimum strain and a maximum strain (inclusive), then compute a noise-free stress using the true modulus, and finally add independent Gaussian noise with specified standard deviation to the stress values. Use the provided random seeds to ensure reproducibility.\n2. Estimates the Young’s modulus $E$ from the simulated data by fitting a linear model with zero intercept (i.e., the line passes through the origin) using ordinary least squares.\n3. Performs nonparametric bootstrap resampling by sampling $n$ pairs with replacement from the observed $(\\varepsilon_i,\\sigma_i)$ pairs, refitting the zero-intercept linear model on each resample, and collecting the bootstrap slopes. Use the provided bootstrap seeds and the specified number of bootstrap replicates $B$.\n4. Reports, for each test case, the bootstrap standard error of the slope estimate, defined as the sample standard deviation (with degrees of freedom equal to $1$) of the bootstrap slopes. Report the standard error in gigapascals, rounded to exactly six digits after the decimal point.\n\nPhysical and numerical units:\n- Strain $\\varepsilon$ is dimensionless.\n- Stress $\\sigma$ must be in gigapascals.\n- Young’s modulus $E$ must be reported in gigapascals.\n- All angles are irrelevant to this problem and need not be used.\n\nTest suite:\nUse the following three cases. Each case specifies a true modulus, the number of data points, the strain range, the stress noise level, the number of bootstrap replicates, and the seeds for data generation and bootstrap resampling. All numerical values are to be interpreted in the units stated above.\n\n- Case 1 (general case, moderate noise):\n  - True modulus $E_{\\text{true}} = 200$ gigapascals.\n  - Number of points $n = 25$.\n  - Strain range $[\\varepsilon_{\\min}, \\varepsilon_{\\max}] = [0.002, 0.010]$.\n  - Stress noise standard deviation $\\sigma_{\\text{noise}} = 0.5$ gigapascals.\n  - Bootstrap replicates $B = 5000$.\n  - Data-generation seed $s_{\\text{data}} = 12345$, bootstrap seed $s_{\\text{boot}} = 54321$.\n\n- Case 2 (fewer points, higher noise):\n  - True modulus $E_{\\text{true}} = 70$ gigapascals.\n  - Number of points $n = 12$.\n  - Strain range $[\\varepsilon_{\\min}, \\varepsilon_{\\max}] = [0.005, 0.025]$.\n  - Stress noise standard deviation $\\sigma_{\\text{noise}} = 2.0$ gigapascals.\n  - Bootstrap replicates $B = 5000$.\n  - Data-generation seed $s_{\\text{data}} = 24680$, bootstrap seed $s_{\\text{boot}} = 86420$.\n\n- Case 3 (boundary case, no noise):\n  - True modulus $E_{\\text{true}} = 150$ gigapascals.\n  - Number of points $n = 8$.\n  - Strain range $[\\varepsilon_{\\min}, \\varepsilon_{\\max}] = [0.003, 0.015]$.\n  - Stress noise standard deviation $\\sigma_{\\text{noise}} = 0.0$ gigapascals.\n  - Bootstrap replicates $B = 5000$.\n  - Data-generation seed $s_{\\text{data}} = 13579$, bootstrap seed $s_{\\text{boot}} = 97531$.\n\nFinal output format:\nYour program should produce a single line of output containing the three bootstrap standard errors, in gigapascals, corresponding to the three test cases in the order listed above, as a comma-separated list enclosed in square brackets, with each value rounded to exactly six digits after the decimal point, for example $[0.123456, 0.234567, 0.000000]$.", "solution": "The problem statement has been validated and is deemed valid. It is scientifically grounded, well-posed, objective, and contains all necessary information to derive a unique, verifiable solution. The problem is a standard exercise in computational physics, concerning the estimation of a physical parameter and its uncertainty from simulated noisy data.\n\nThe task is to compute the standard error of the Young’s modulus, $E$, estimated from a stress-strain curve. The estimation will be performed using the bootstrap method. The procedure is based on three core principles: Hooke's law for the physical model, ordinary least squares for parameter fitting, and bootstrap resampling for uncertainty quantification.\n\nThe physical model is Hooke's law, which states that for small deformations, axial stress $\\sigma$ is linearly proportional to axial strain $\\varepsilon$:\n$$ \\sigma = E \\varepsilon $$\nThe constant of proportionality, $E$, is the Young's modulus. This model represents a line passing through the origin, as zero strain must correspond to zero stress.\n\nFirst, we must generate the synthetic data for each test case. We are given a true modulus $E_{\\text{true}}$, a number of points $n$, a strain range $[\\varepsilon_{\\min}, \\varepsilon_{\\max}]$, and a stress noise standard deviation $\\sigma_{\\text{noise}}$. The strain values $\\varepsilon_i$ for $i=1, \\dots, n$ are generated as $n$ linearly spaced points from $\\varepsilon_{\\min}$ to $\\varepsilon_{\\max}$, inclusive. The corresponding stress values $\\sigma_i$ are simulated by taking the ideal stress from Hooke's law, $\\sigma_{i, \\text{true}} = E_{\\text{true}} \\varepsilon_i$, and adding random noise $\\delta_i$ to simulate measurement error.\n$$ \\sigma_i = E_{\\text{true}} \\varepsilon_i + \\delta_i $$\nThe noise terms $\\delta_i$ are independent and identically distributed random variables drawn from a Gaussian (normal) distribution with mean $0$ and standard deviation $\\sigma_{\\text{noise}}$, which is formally written as $\\delta_i \\sim \\mathcal{N}(0, \\sigma_{\\text{noise}}^2)$. The `data-generation seed` $s_{\\text{data}}$ is used to ensure the reproducibility of this random process.\n\nNext, from the generated dataset of $n$ pairs $\\{(\\varepsilon_i, \\sigma_i)\\}_{i=1}^n$, we must estimate the Young's modulus. For the linear model $\\sigma = E \\varepsilon$ that passes through the origin, the Ordinary Least Squares (OLS) estimator $\\hat{E}$ is the value that minimizes the sum of squared residuals, $S(E) = \\sum_{i=1}^{n} (\\sigma_i - E \\varepsilon_i)^2$. To find this minimum, we set the derivative with respect to $E$ to zero:\n$$ \\frac{dS}{dE} = \\sum_{i=1}^{n} 2(\\sigma_i - E \\varepsilon_i)(-\\varepsilon_i) = 0 $$\nSolving this equation for $E$ yields the OLS estimator:\n$$ \\hat{E} = \\frac{\\sum_{i=1}^{n} \\varepsilon_i \\sigma_i}{\\sum_{i=1}^{n} \\varepsilon_i^2} $$\nThis formula provides the best-fit slope for a line constrained to pass through the origin.\n\nThe core of the problem is to estimate the uncertainty of this estimator $\\hat{E}$. We use the nonparametric bootstrap method. The original dataset $\\{(\\varepsilon_i, \\sigma_i)\\}_{i=1}^n$ is treated as an empirical distribution. We perform the following steps for $B$ bootstrap replicates, where for this problem $B=5000$:\n1.  Generate a *bootstrap sample* by drawing $n$ pairs with replacement from the original dataset. Let this new sample be $\\{(\\varepsilon_j^*, \\sigma_j^*)\\}_{j=1}^n$.\n2.  For this bootstrap sample, compute the bootstrap estimate of the modulus, $\\hat{E}^*$, using the same OLS formula:\n    $$ \\hat{E}^* = \\frac{\\sum_{j=1}^{n} \\varepsilon_j^* \\sigma_j^*}{\\sum_{j=1}^{n} (\\varepsilon_j^*)^2} $$\n3.  Store this value $\\hat{E}^*$.\n\nAfter repeating this for $B$ replicates, we obtain a collection of $B$ bootstrap estimates, $\\{\\hat{E}_1^*, \\hat{E}_2^*, \\dots, \\hat{E}_B^*\\}$. This collection serves as an empirical approximation of the sampling distribution of our estimator $\\hat{E}$. The `bootstrap-seed` $s_{\\text{boot}}$ ensures that this resampling process is also reproducible.\n\nFinally, we calculate the bootstrap standard error, $\\text{SE}_{\\text{boot}}(\\hat{E})$, which is the sample standard deviation of the bootstrap estimates. It is a measure of the statistical uncertainty in our estimate $\\hat{E}$. The formula, using $B-1$ degrees of freedom as specified, is:\n$$ \\text{SE}_{\\text{boot}}(\\hat{E}) = \\sqrt{\\frac{1}{B-1} \\sum_{b=1}^{B} (\\hat{E}_b^* - \\bar{E}^*)^2} $$\nwhere $\\bar{E}^* = \\frac{1}{B} \\sum_{b=1}^{B} \\hat{E}_b^*$ is the mean of the bootstrap estimates.\n\nA special consideration is Case 3, where $\\sigma_{\\text{noise}} = 0.0$. In this instance, the initial data points lie perfectly on the line $\\sigma = E_{\\text{true}} \\varepsilon$. Any bootstrap sample will be composed of points that also lie on this exact line. Consequently, every bootstrap slope estimate $\\hat{E}_b^*$ will be analytically equal to $E_{\\text{true}}$. The standard deviation of a set of constant values is precisely $0$. Therefore, for this case, the bootstrap standard error must be 0.000000. This serves as a validation check for the algorithm's logic.\n\nThe implementation will proceed by executing this entire procedure for each of the three test cases specified, and formatting the final standard errors as requested.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for all given test cases.\n    It calculates the bootstrap standard error for Young's modulus estimates\n    and prints the results in the specified format.\n    \"\"\"\n    test_cases = [\n        # Case 1 (general case, moderate noise)\n        (200.0, 25, [0.002, 0.010], 0.5, 5000, 12345, 54321),\n        # Case 2 (fewer points, higher noise)\n        (70.0, 12, [0.005, 0.025], 2.0, 5000, 24680, 86420),\n        # Case 3 (boundary case, no noise)\n        (150.0, 8, [0.003, 0.015], 0.0, 5000, 13579, 97531),\n    ]\n\n    results = []\n    for case in test_cases:\n        E_true, n, strain_range, sigma_noise, B, s_data, s_boot = case\n\n        # Step 1: Generate synthetic (strain, stress) data\n        # Use the data-generation seed for reproducibility.\n        rng_data = np.random.default_rng(s_data)\n        \n        strains = np.linspace(strain_range[0], strain_range[1], n)\n        stress_true = E_true * strains\n        noise = rng_data.normal(loc=0.0, scale=sigma_noise, size=n)\n        stress_obs = stress_true + noise\n\n        # Step 2: Perform nonparametric bootstrap resampling\n        # Use the bootstrap seed for reproducibility.\n        rng_boot = np.random.default_rng(s_boot)\n        bootstrap_slopes = np.zeros(B)\n        \n        # Original data indices for resampling\n        data_indices = np.arange(n)\n\n        for i in range(B):\n            # Create a bootstrap sample by sampling indices with replacement.\n            boot_indices = rng_boot.choice(data_indices, size=n, replace=True)\n            strains_boot = strains[boot_indices]\n            stress_boot = stress_obs[boot_indices]\n            \n            # Estimate the slope (Young's modulus) for the bootstrap sample\n            # using the OLS formula for a zero-intercept model.\n            sum_x_squared = np.sum(strains_boot**2)\n            \n            # This check prevents division by zero, although it is not expected\n            # to occur with the given problem inputs.\n            if sum_x_squared == 0:\n                slope_b = 0.0\n            else:\n                slope_b = np.sum(strains_boot * stress_boot) / sum_x_squared\n            \n            bootstrap_slopes[i] = slope_b\n            \n        # Step 3: Compute the bootstrap standard error\n        # The standard error is the standard deviation of the bootstrap slopes.\n        # ddof=1 specifies the use of N-1 in the denominator for sample std dev.\n        std_error = np.std(bootstrap_slopes, ddof=1)\n        results.append(std_error)\n\n    # Final print statement in the exact required format.\n    # The f-string format specifier '{:.6f}' rounds to 6 decimal places.\n    print(f\"[{','.join([f'{r:.6f}' for r in results])}]\")\n\nsolve()\n```", "id": "2404303"}, {"introduction": "Data from physics simulations, such as Monte Carlo or molecular dynamics, are often serially correlated, violating the independence assumption of simple resampling methods. This final practice introduces a crucial adaptation—the block jackknife—to correctly handle such data. You will apply this technique to estimate the uncertainty of the heat capacity, a key thermodynamic property calculated from energy fluctuations in a simulated canonical ensemble, a common and important task in computational statistical mechanics [@problem_id:2404291].", "problem": "You are given the task of building a complete, runnable program that estimates the statistical error of the constant-volume heat capacity using the jackknife method from synthetic canonical-ensemble energy time series. Begin from the following fundamental base: in the canonical ensemble, the constant-volume heat capacity (abbreviated as CV) satisfies the fluctuation–dissipation relation that connects thermal response to energy fluctuations. Concretely, if $E$ denotes the energy and $\\langle \\cdot \\rangle$ denotes an ensemble average, then, in units where the Boltzmann constant is $k_B=1$, the constant-volume heat capacity $C_V$ obeys an identity that relates $C_V$ to energy fluctuations and absolute temperature $T$. In a numerical simulation that produces a finite time series of energies, expectations are approximated by sample averages. Your task is to estimate $C_V$ from a time series of energies via the energy-fluctuation route and then use the jackknife to estimate the standard error of that $C_V$ estimator.\n\nYour program must implement the following:\n\n- Use the fluctuation–dissipation relation of the canonical ensemble to construct an estimator for the constant-volume heat capacity. Use the sample energy time series to produce the needed averages. All quantities are to be treated in reduced units where $k_B=1$, so that $C_V$ is reported as $C_V/k_B$, which is dimensionless.\n\n- Implement the delete-one (leave-one-out) jackknife for independent samples and the block jackknife for correlated time series. In the block jackknife, partition the time series into contiguous, non-overlapping blocks of equal size and form jackknife replicates by deleting one entire block at a time. If the length $N$ of the time series is not an integer multiple of the block size $b$, ignore the tail after forming $\\lfloor N/b \\rfloor$ full blocks for the jackknife. Use the standard jackknife construction to obtain the jackknife estimate of the standard error (square root of the jackknife variance estimate) of the heat capacity estimator. For the block jackknife, the number of jackknife replicates equals the number of blocks, and the same standard jackknife variance construction applies.\n\n- The final outputs must be the jackknife standard errors (one per test case), each expressed as a floating-point number. Because we work in reduced units with $k_B=1$, the outputs are dimensionless.\n\nSynthetic data generation (test suite):\n\nYour program must internally generate the following three energy time series using the specified random seeds and models. Use a pseudorandom number generator that is seeded deterministically as specified, so that results are reproducible.\n\n- Test case $1$ (independent, identically distributed data, “happy path”): Generate $N=400$ energies from a normal distribution with mean $\\mu=0$ and standard deviation $\\sigma=2$, at temperature $T=1$. Use seed $314159$. Use leave-one-out jackknife, i.e., block size $b=1$.\n\n- Test case $2$ (small-sample boundary): Generate $N=8$ energies from a normal distribution with mean $\\mu=0.3$ and standard deviation $\\sigma=1$, at temperature $T=1$. Use seed $271828$. Use leave-one-out jackknife, i.e., block size $b=1$.\n\n- Test case $3$ (strongly correlated time series): Generate $N=1000$ energies via a stationary first-order autoregressive process (abbreviated as AR(1)) with parameter $\\phi=0.9$ and innovation noise $\\varepsilon_t \\sim \\mathcal{N}(0,\\sigma_\\varepsilon^2)$ with $\\sigma_\\varepsilon=1$. Initialize $E_0$ by drawing from the stationary distribution of the AR(1) process. Then evolve $E_t=\\phi E_{t-1}+\\varepsilon_t$ for $t=1,\\dots,N-1$. Use temperature $T=1$ and seed $424242$. Use block jackknife with block size $b=20$.\n\nComputation details and constraints:\n\n- For each time series, compute the heat capacity estimator using the energy-fluctuation route with sample averages, in units where $k_B=1$. Then compute the jackknife standard error of this estimator. For the block jackknife, partition into contiguous blocks of size $b$ and ignore the last $N - b \\lfloor N/b \\rfloor$ energies if any remain. Ensure at least $2$ jackknife replicates are formed; otherwise, the standard error is undefined and the program must not fail on the provided test suite.\n\n- Angle units are not applicable. No physical units beyond the reduced units stated are required; report dimensionless numbers.\n\nRequired final output format:\n\n- Your program should produce a single line of output containing the three jackknife standard errors (for test cases $1$, $2$, and $3$ in that order) as a comma-separated list enclosed in square brackets, with each number rounded to exactly $6$ decimal places, for example, $[0.123456,0.234567,0.345678]$.\n\nYour solution must be self-contained, require no input, and follow the specified seeds and parameters exactly. The only permitted libraries are the Python standard library and the listed numerical libraries.", "solution": "The user requests a program to compute the jackknife standard error of the constant-volume heat capacity, $C_V$, from synthetic time series data. This requires applying principles from statistical mechanics and computational statistics.\n\nFirst, we must formalize the estimator for the heat capacity. In the canonical ensemble, the partition function is given by $Z = \\sum_{i} \\exp(-\\beta E_i)$, where $E_i$ represents the energy of microstate $i$, and $\\beta = (k_B T)^{-1}$ is the inverse temperature, with $T$ being the absolute temperature and $k_B$ the Boltzmann constant. As per the problem specification, we operate in reduced units where $k_B=1$, making $\\beta = 1/T$. The ensemble average of the energy, $\\langle E \\rangle$, is obtained from the partition function as:\n$$\n\\langle E \\rangle = -\\frac{\\partial \\ln Z}{\\partial \\beta}\n$$\nThe constant-volume heat capacity, $C_V$, is defined as the derivative of the average energy with respect to temperature:\n$$\nC_V = \\frac{\\partial \\langle E \\rangle}{\\partial T}\n$$\nBy applying the chain rule, $\\frac{\\partial}{\\partial T} = \\frac{\\partial \\beta}{\\partial T} \\frac{\\partial}{\\partial \\beta} = -\\frac{1}{k_B T^2} \\frac{\\partial}{\\partial \\beta}$, and setting $k_B=1$, we find:\n$$\nC_V = -\\frac{1}{T^2} \\frac{\\partial \\langle E \\rangle}{\\partial \\beta} = -\\frac{1}{T^2} \\frac{\\partial}{\\partial \\beta} \\left( -\\frac{\\partial \\ln Z}{\\partial \\beta} \\right) = \\frac{1}{T^2} \\frac{\\partial^2 \\ln Z}{\\partial \\beta^2}\n$$\nPerforming the differentiation leads to the fluctuation-dissipation theorem for heat capacity:\n$$\nC_V = \\frac{1}{T^2} \\left( \\langle E^2 \\rangle - \\langle E \\rangle^2 \\right) = \\frac{\\text{Var}(E)}{T^2}\n$$\nwhere $\\text{Var}(E)$ is the variance of the energy. For a finite time series of $N$ energy measurements, $\\{E_i\\}_{i=1}^N$, the ensemble averages $\\langle \\cdot \\rangle$ are replaced by their sample estimators. The estimator for $C_V$, denoted $\\hat{\\theta}$, is thus:\n$$\n\\hat{\\theta} = \\frac{1}{T^2} \\left[ \\left(\\frac{1}{N}\\sum_{i=1}^N E_i^2\\right) - \\left(\\frac{1}{N}\\sum_{i=1}^N E_i\\right)^2 \\right]\n$$\n\nSecond, we employ the jackknife method to estimate the statistical error of $\\hat{\\theta}$. The jackknife is a resampling technique that provides an estimate of the estimator's variance.\n\nFor independent and identically distributed (i.i.d.) data, such as in Test Cases $1$ and $2$, we use the delete-one jackknife. This is equivalent to a block jackknife with a block size of $b=1$. Given a sample of size $N$, we create $N$ jackknife replicates. The $j$-th replicate estimator, $\\hat{\\theta}_{(j)}$, is computed by applying the estimator formula to the sample with the $j$-th observation removed. The jackknife variance is then calculated as:\n$$\n\\widehat{\\text{Var}}_{\\text{jack}}(\\hat{\\theta}) = \\frac{N-1}{N} \\sum_{j=1}^N \\left(\\hat{\\theta}_{(j)} - \\hat{\\theta}_{(\\cdot)}\\right)^2\n$$\nwhere $\\hat{\\theta}_{(\\cdot)} = \\frac{1}{N} \\sum_{j=1}^N \\hat{\\theta}_{(j)}$ is the mean of the jackknife replicates.\n\nFor correlated time series data, as in Test Case $3$, the delete-one method is inappropriate because it fails to preserve the correlation structure, leading to an underestimation of the true variance. The correct approach is the block jackknife. The time series of length $N$ is divided into $L = \\lfloor N/b \\rfloor$ non-overlapping blocks of size $b$. The block size $b$ should ideally be larger than the autocorrelation time of the series. Jackknife replicates are formed by deleting one entire block at a time. The formula for the block jackknife variance is analogous, with $L$ replicates instead of $N$:\n$$\n\\widehat{\\text{Var}}_{\\text{block-jack}}(\\hat{\\theta}) = \\frac{L-1}{L} \\sum_{j=1}^L \\left(\\hat{\\theta}_{(j)} - \\hat{\\theta}_{(\\cdot)}\\right)^2\n$$\nwhere $\\hat{\\theta}_{(j)}$ is the estimator computed from the sample with the $j$-th block removed, and $\\hat{\\theta}_{(\\cdot)}$ is the mean of these $L$ replicates. In both cases, the standard error is the square root of the estimated variance, $\\hat{\\sigma} = \\sqrt{\\widehat{\\text{Var}}}$.\n\nThe implementation will perform the following for each test case:\n1.  **Data Generation**: An energy time series $\\{E_i\\}_{i=1}^N$ is generated according to the specified model (Normal or AR($1$)) and parameters ($N, T, \\mu, \\sigma, \\phi, \\sigma_\\varepsilon$) with the given random seed. For the AR($1$) process, $E_t = \\phi E_{t-1} + \\varepsilon_t$, the initial value $E_0$ is drawn from its stationary distribution, which is $\\mathcal{N}(0, \\sigma_E^2)$ with variance $\\sigma_E^2 = \\sigma_\\varepsilon^2 / (1-\\phi^2)$.\n2.  **Blocking**: The data is partitioned into $L = \\lfloor N/b \\rfloor$ blocks of size $b$. Any remaining data points at the tail are discarded.\n3.  **Replicate Calculation**: For each of the $L$ blocks, a jackknife replicate $\\hat{\\theta}_{(j)}$ is calculated from the dataset with that block removed. This can be done efficiently by pre-calculating the sum of energies and sum of squared energies for each block.\n4.  **Error Estimation**: The set of $L$ replicates $\\{\\hat{\\theta}_{(j)}\\}_{j=1}^L$ is used to compute the jackknife variance and, subsequently, the standard error.\n\nThe program applies this procedure to all three test cases with their specific parameters:\n-   **Case 1**: i.i.d. data with $N=400$, $T=1$, from $\\mathcal{N}(\\mu=0, \\sigma=2)$. Block size $b=1$, so $L=400$.\n-   **Case 2**: Small i.i.d. sample with $N=8$, $T=1$, from $\\mathcal{N}(\\mu=0.3, \\sigma=1)$. Block size $b=1$, so $L=8$.\n-   **Case 3**: Correlated AR($1$) data with $N=1000$, $T=1$, $\\phi=0.9$, and innovation noise from $\\mathcal{N}(0, 1)$. Block size is $b=20$, so $L = \\lfloor 1000/20 \\rfloor = 50$.\n\nThe final results are the standard errors for these three distinct physical scenarios.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem of estimating the jackknife standard error for the\n    constant-volume heat capacity from synthetic energy time series.\n    \"\"\"\n    \n    test_cases = [\n        {\n            \"id\": 1, \"type\": \"iid\", \"N\": 400, \"mu\": 0.0, \"sigma\": 2.0, \"T\": 1.0, \n            \"seed\": 314159, \"block_size\": 1\n        },\n        {\n            \"id\": 2, \"type\": \"iid\", \"N\": 8, \"mu\": 0.3, \"sigma\": 1.0, \"T\": 1.0, \n            \"seed\": 271828, \"block_size\": 1\n        },\n        {\n            \"id\": 3, \"type\": \"ar1\", \"N\": 1000, \"phi\": 0.9, \"sigma_eps\": 1.0, \"T\": 1.0, \n            \"seed\": 424242, \"block_size\": 20\n        },\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        rng = np.random.default_rng(case[\"seed\"])\n        \n        # Step 1: Generate a synthetic energy time series\n        if case[\"type\"] == \"iid\":\n            energies = rng.normal(loc=case[\"mu\"], scale=case[\"sigma\"], size=case[\"N\"])\n        elif case[\"type\"] == \"ar1\":\n            N = case[\"N\"]\n            phi = case[\"phi\"]\n            sigma_eps = case[\"sigma_eps\"]\n            \n            # Variance of the stationary distribution of AR(1) process\n            var_E_stationary = sigma_eps**2 / (1 - phi**2)\n            \n            # Initialize E_0 from the stationary distribution\n            E0 = rng.normal(loc=0, scale=np.sqrt(var_E_stationary))\n            \n            # Generate innovations\n            epsilons = rng.normal(loc=0, scale=sigma_eps, size=N - 1)\n            \n            # Generate the AR(1) time series\n            energies = np.zeros(N)\n            energies[0] = E0\n            for t in range(1, N):\n                energies[t] = phi * energies[t-1] + epsilons[t-1]\n        \n        # Step 2: Implement the jackknife error estimation\n        N = len(energies)\n        b = case[\"block_size\"]\n        T = case[\"T\"]\n        \n        # Number of blocks\n        L = N // b\n        \n        # Truncate the time series to be a multiple of the block size\n        truncated_energies = energies[:L * b]\n        \n        # Reshape data into blocks for efficient computation\n        blocks_E = truncated_energies.reshape((L, b))\n        \n        # Pre-compute sums over blocks\n        block_sums_E = np.sum(blocks_E, axis=1)\n        block_sums_E2 = np.sum(blocks_E**2, axis=1)\n        \n        # Total sums for the entire (truncated) dataset\n        total_sum_E = np.sum(block_sums_E)\n        total_sum_E2 = np.sum(block_sums_E2)\n        \n        # Size of each jackknife replicate sample\n        N_rep = (L - 1) * b\n        \n        replicates_cv = np.zeros(L)\n        \n        # Step 3: Compute jackknife replicates of the C_V estimator\n        for i in range(L):\n            # Sums for the sample with block i removed\n            sum_E_rep = total_sum_E - block_sums_E[i]\n            sum_E2_rep = total_sum_E2 - block_sums_E2[i]\n            \n            # Means for the replicate sample\n            mean_E_rep = sum_E_rep / N_rep\n            mean_E2_rep = sum_E2_rep / N_rep\n            \n            # C_V estimator for replicate i\n            # C_V = ( E^2 - E^2 ) / T^2\n            cv_rep = (mean_E2_rep - mean_E_rep**2) / (T**2)\n            replicates_cv[i] = cv_rep\n        \n        # Step 4: Compute jackknife variance and standard error\n        # Jackknife variance: Var_jack = (L-1)/L * sum( (theta_i - theta_mean)^2 )\n        mean_replicates = np.mean(replicates_cv)\n        jackknife_var = ((L - 1) / L) * np.sum((replicates_cv - mean_replicates)**2)\n        jackknife_stderr = np.sqrt(jackknife_var)\n        \n        results.append(jackknife_stderr)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join([f'{r:.6f}' for r in results])}]\")\n\nsolve()\n```", "id": "2404291"}]}