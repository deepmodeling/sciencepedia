{"hands_on_practices": [{"introduction": "Before applying any powerful numerical tool, it's crucial to understand its limitations. This first practice is a thought experiment designed to probe the theoretical bedrock of Monte Carlo methods [@problem_id:2402983]. By examining the consequences of applying importance sampling to an integral that diverges, we can gain a deeper appreciation for the foundational requirements of finite mean and variance that underpin the Law of Large Numbers and the Central Limit Theorem.", "problem": "You aim to approximate the improper integral of a nonnegative function by Monte Carlo importance sampling. Consider the integral\n$$\nI \\;=\\; \\int_{1}^{\\infty} f(x)\\,dx, \\quad \\text{with} \\quad f(x) \\;=\\; x^{-1/2},\n$$\nwhich diverges. Let $p(x)$ be any probability density function on $[1,\\infty)$ with $\\int_{1}^{\\infty} p(x)\\,dx = 1$ and $p(x) > 0$ for all $x \\ge 1$. Using importance sampling, you draw $X_i \\sim p(x)$ independently for $i=1,\\dots,N$, and form the estimator\n$$\nI_N \\;=\\; \\frac{1}{N}\\sum_{i=1}^{N} \\frac{f(X_i)}{p(X_i)}.\n$$\nThe conventional Monte Carlo error estimate is the sample standard deviation of the weights divided by $\\sqrt{N}$, namely\n$$\n\\widehat{\\mathrm{SE}}(I_N) \\;=\\; \\frac{1}{\\sqrt{N}}\\left(\\frac{1}{N-1}\\sum_{i=1}^{N}\\Big(\\frac{f(X_i)}{p(X_i)} - \\overline{w}\\Big)^2\\right)^{1/2},\n\\quad \\text{where} \\quad \\overline{w} \\;=\\; \\frac{1}{N}\\sum_{i=1}^{N}\\frac{f(X_i)}{p(X_i)}.\n$$\nWhich statement best describes what happens to the Monte Carlo error estimate $\\widehat{\\mathrm{SE}}(I_N)$ as $N$ grows in this setting?\n\nA. For any such $p(x)$, the mean $\\mathbb{E}_p\\!\\left[\\frac{f(X)}{p(X)}\\right]$ equals $\\int_{1}^{\\infty} f(x)\\,dx = \\infty$, and the variance is not finite; therefore the Law of Large Numbers (LLN) and the Central Limit Theorem (CLT) do not apply, the estimator $I_N$ does not converge to a finite limit, and the usual standard error estimate $\\widehat{\\mathrm{SE}}(I_N)$ is not well-defined and is typically erratic or divergent.\n\nB. By choosing $p(x) \\propto x^{-\\alpha}$ with $\\alpha > 1$, the variance of $\\frac{f(X)}{p(X)}$ becomes finite and $\\widehat{\\mathrm{SE}}(I_N)$ decays as $N^{-1/2}$, even though $I = \\infty$.\n\nC. The estimator $I_N$ converges almost surely to a finite constant that depends on $p(x)$, and the error estimate $\\widehat{\\mathrm{SE}}(I_N)$ stabilizes to a finite limit.\n\nD. Truncating the domain at an upper cutoff $x_{\\max} = N$ makes $I_N$ unbiased for the true integral and $\\widehat{\\mathrm{SE}}(I_N)$ correctly reflects the divergence of $I$.", "solution": "The starting point is the definition of importance sampling for integrals over $[1,\\infty)$. For any normalized density $p(x)$ on $[1,\\infty)$ with $p(x) > 0$ for $x \\ge 1$, sampling $X \\sim p$ yields the unbiased identity (when expectations are finite)\n$$\n\\mathbb{E}_p\\!\\left[\\frac{f(X)}{p(X)}\\right] \\;=\\; \\int_{1}^{\\infty} \\frac{f(x)}{p(x)}\\,p(x)\\,dx \\;=\\; \\int_{1}^{\\infty} f(x)\\,dx \\;=\\; I.\n$$\nHere $f(x) = x^{-1/2}$. The integral\n$$\nI \\;=\\; \\int_{1}^{\\infty} x^{-1/2}\\,dx \\;=\\; \\left[2\\,x^{1/2}\\right]_{1}^{\\infty} \\;=\\; \\infty\n$$\ndiverges. Consequently, for any such $p(x)$,\n$$\n\\mathbb{E}_p\\!\\left[\\frac{f(X)}{p(X)}\\right] \\;=\\; \\infty,\n$$\nso the first moment is not finite. In particular, the Law of Large Numbers (LLN), which requires a finite mean, and the Central Limit Theorem (CLT), which requires at least a finite variance, do not apply to the sequence of averages $I_N$ in the usual form.\n\nThe variance of the importance weight $W = f(X)/p(X)$, when it exists, is\n$$\n\\mathrm{Var}_p(W) \\;=\\; \\mathbb{E}_p\\!\\left[W^2\\right] - \\left(\\mathbb{E}_p[W]\\right)^2,\n\\quad \\text{with} \\quad\n\\mathbb{E}_p\\!\\left[W^2\\right] \\;=\\; \\int_{1}^{\\infty} \\frac{f(x)^2}{p(x)}\\,dx \\;=\\; \\int_{1}^{\\infty} \\frac{x^{-1}}{p(x)}\\,dx.\n$$\nBecause $\\mathbb{E}_p[W] = \\infty$, the variance is not finite. Moreover, for typical admissible choices of $p(x)$ the second moment itself diverges. Two representative families illustrate this:\n\n- If $p(x)$ has an exponential tail, say $p(x) \\propto e^{-x}$ on $[1,\\infty)$, then\n$$\n\\mathbb{E}_p\\!\\left[W^2\\right] \\;\\propto\\; \\int_{1}^{\\infty} \\frac{x^{-1}}{e^{-x}}\\,dx \\;=\\; \\int_{1}^{\\infty} \\frac{e^{x}}{x}\\,dx \\;=\\; \\infty.\n$$\n\n- If $p(x)$ has a power-law tail $p(x) \\propto x^{-\\alpha}$ with $\\alpha > 1$ (so that $p$ is normalizable on $[1,\\infty)$), then\n$$\n\\mathbb{E}_p\\!\\left[W^2\\right] \\;\\propto\\; \\int_{1}^{\\infty} \\frac{x^{-1}}{x^{-\\alpha}}\\,dx \\;=\\; \\int_{1}^{\\infty} x^{\\alpha - 1}\\,dx \\;=\\; \\infty,\n$$\nsince $\\alpha - 1 \\ge 0$.\n\nTherefore, the standard error formula $\\widehat{\\mathrm{SE}}(I_N)$, which presumes a finite second moment and invokes the CLT to assert $N^{-1/2}$ decay, is not meaningful in this setting. In practice, $\\widehat{\\mathrm{SE}}(I_N)$ behaves erratically and can grow without stabilizing as $N$ increases, because rare samples from the far tail yield extremely large weights $\\frac{f(X_i)}{p(X_i)}$ that dominate both the mean and the empirical variance.\n\nWe now assess each option:\n\nA. This states that for any admissible $p(x)$, the mean of the importance weight equals the divergent integral, so it is infinite, and the variance is not finite; as a result, the LLN and CLT do not apply, $I_N$ does not converge to a finite limit, and the usual standard error is not well-defined and is typically erratic or divergent. This matches the reasoning: $\\mathbb{E}_p[W] = \\int_{1}^{\\infty} f(x)\\,dx = \\infty$, and the second moment diverges for broad classes of $p(x)$; without finite moments, the standard error estimate has no theoretical validity and is unstable. Verdict: Correct.\n\nB. This claims that choosing $p(x) \\propto x^{-\\alpha}$ with $\\alpha > 1$ (a normalizable power-law) yields finite variance and $N^{-1/2}$ decay of the standard error. The calculation above shows the opposite: for $\\alpha > 1$, $\\mathbb{E}_p[W^2] \\propto \\int_{1}^{\\infty} x^{\\alpha - 1}\\,dx = \\infty$. Verdict: Incorrect.\n\nC. This asserts almost sure convergence of $I_N$ to a finite constant depending on $p(x)$, with a stabilizing standard error. Since $\\mathbb{E}_p[W] = \\infty$, neither the LLN nor the CLT applies in the standard sense, and $I_N$ does not converge to a finite constant; instead it is dominated by rare, extremely large contributions. Verdict: Incorrect.\n\nD. This proposes truncating at $x_{\\max} = N$ to make $I_N$ unbiased for the true integral and to have the error reflect the divergence. Truncation changes the target integral to $\\int_{1}^{x_{\\max}} f(x)\\,dx$, which is finite for any finite $x_{\\max}$; the resulting estimator is unbiased for the truncated integral, not for $I = \\infty$. As $x_{\\max}$ increases with $N$, both the target and any associated error change with $N$ in a biased manner relative to the original infinite target. Verdict: Incorrect.", "answer": "$$\\boxed{A}$$", "id": "2402983"}, {"introduction": "One of the great efficiencies of importance sampling is the ability to reuse a single set of computationally generated samples to evaluate multiple different integrals. This exercise explores the mechanics of this powerful technique, demonstrating how to correctly adapt importance weights for a new integrand [@problem_id:2402977]. It sharpens your problem-solving skills by forcing a careful consideration of both the estimator and its variance, revealing how a seemingly clever choice of proposal density can sometimes lead to pitfalls like an infinite variance.", "problem": "Consider a measurable domain $D \\subset \\mathbb{R}^d$ with Lebesgue measure $|D| \\in (0,+\\infty]$. Let $f:D \\to [0,+\\infty)$ be measurable, strictly positive almost everywhere on $D$, and integrable, with $K \\equiv \\int_D f(x)\\,dx < +\\infty$. You have generated $N$ independent and identically distributed samples $x_1,\\dots,x_N$ from a proposal probability density $p(x)$ that satisfies $p(x) > 0$ wherever $f(x) > 0$. For estimating the integral $I_f \\equiv \\int_D f(x)\\,dx$, you stored the usual importance sampling weights $w_i \\equiv f(x_i)/p(x_i)$ associated with these samples. You now wish to estimate $J \\equiv \\int_D \\sqrt{f(x)}\\,dx$.\n\nWhich option best describes whether you can reuse the same samples and weights to obtain a statistically sound estimate of $J$, and what variance you should expect, particularly in the common design choice $p(x) \\propto f(x)$?\n\nA. Reuse both the samples $x_i$ and the original weights $w_i = f(x_i)/p(x_i)$ without change. The estimator $\\frac{1}{N}\\sum_{i=1}^N w_i$ is unbiased for $J$ and has minimal variance when $p(x) \\propto f(x)$.\n\nB. You may reuse the same samples $x_i$, but you must recompute weights as $w_i' = \\sqrt{f(x_i)}/p(x_i)$. The estimator $\\frac{1}{N}\\sum_{i=1}^N w_i'$ is unbiased for $J$ with variance $\\frac{1}{N}\\left(\\int_D \\frac{f(x)}{p(x)}\\,dx - J^2\\right)$. In particular, if $p(x) = f(x)/K$ and $f(x) > 0$ almost everywhere on $D$, then $\\mathrm{Var} = \\frac{1}{N}\\left(K\\,|D| - J^2\\right)$, which is infinite when $|D| = +\\infty$.\n\nC. You cannot reuse the samples $x_i$ at all. To avoid bias, you must draw a fresh set of samples from a proposal density proportional to $\\sqrt{f(x)}$, and any attempt to reuse the original samples leads to bias.\n\nD. Self-normalizing the original weights as $\\tilde{w}_i = w_i/\\sum_{j=1}^N w_j$ and reusing them with the same samples yields an unbiased estimator for $J$ that always has lower variance than the unnormalized importance sampling estimator, regardless of $p(x)$.", "solution": "The fundamental principle of importance sampling is to estimate an integral $I_g = \\int_D g(x) \\,dx$ by rewriting it as an expectation with respect to a proposal probability density $p(x)$. The proposal density must satisfy $p(x) > 0$ for all $x \\in D$ where $g(x) \\neq 0$. The integral is expressed as:\n$$ I_g = \\int_D \\frac{g(x)}{p(x)} p(x) \\,dx = \\mathbb{E}_{X \\sim p} \\left[ \\frac{g(X)}{p(X)} \\right] $$\nGiven $N$ independent and identically distributed (i.i.d.) samples $x_1, \\dots, x_N$ drawn from $p(x)$, an unbiased estimator for $I_g$ is given by the sample mean:\n$$ \\hat{I}_g = \\frac{1}{N} \\sum_{i=1}^N \\frac{g(x_i)}{p(x_i)} $$\nIn this problem, we wish to estimate the integral $J \\equiv \\int_D \\sqrt{f(x)}\\,dx$. We identify the integrand as $g(x) = \\sqrt{f(x)}$. The problem states that samples $x_i$ have been drawn from a proposal density $p(x)$ that satisfies $p(x) > 0$ wherever $f(x) > 0$. Since $f(x) > 0$ implies $\\sqrt{f(x)} > 0$, the support condition for estimating $J$ is satisfied. Therefore, we can reuse the existing samples $x_i$.\n\nThe correct importance sampling estimator for $J$ is constructed by setting $g(x) = \\sqrt{f(x)}$:\n$$ \\hat{J} = \\frac{1}{N} \\sum_{i=1}^N \\frac{\\sqrt{f(x_i)}}{p(x_i)} $$\nThe terms in this sum, $w_i' \\equiv \\frac{\\sqrt{f(x_i)}}{p(x_i)}$, are the new importance weights required for estimating $J$. They are distinct from the original weights $w_i = f(x_i)/p(x_i)$ used for estimating $I_f = \\int_D f(x)\\,dx$.\n\nThe statistical properties of this estimator $\\hat{J}$ must be examined.\nFirst, its expectation (bias):\n$$ \\mathbb{E}[\\hat{J}] = \\mathbb{E}\\left[\\frac{1}{N} \\sum_{i=1}^N \\frac{\\sqrt{f(X_i)}}{p(X_i)}\\right] = \\frac{1}{N} \\sum_{i=1}^N \\mathbb{E}\\left[\\frac{\\sqrt{f(X_i)}}{p(X_i)}\\right] $$\nSince the samples are i.i.d., the expectation of each term is identical:\n$$ \\mathbb{E}\\left[\\frac{\\sqrt{f(X)}}{p(X)}\\right] = \\int_D \\frac{\\sqrt{f(x)}}{p(x)} p(x) \\,dx = \\int_D \\sqrt{f(x)} \\,dx = J $$\nThus, $\\mathbb{E}[\\hat{J}] = \\frac{1}{N} (N \\cdot J) = J$. The estimator is unbiased.\n\nSecond, its variance:\nThe variance of the sample mean of $N$ i.i.d. random variables is $\\frac{1}{N}$ times the variance of a single variable.\n$$ \\mathrm{Var}[\\hat{J}] = \\frac{1}{N} \\mathrm{Var}\\left[\\frac{\\sqrt{f(X)}}{p(X)}\\right] $$\nUsing the formula $\\mathrm{Var}[Y] = \\mathbb{E}[Y^2] - (\\mathbb{E}[Y])^2$ with $Y = \\frac{\\sqrt{f(X)}}{p(X)}$, we have $\\mathbb{E}[Y] = J$. The second moment is:\n$$ \\mathbb{E}[Y^2] = \\mathbb{E}\\left[\\left(\\frac{\\sqrt{f(X)}}{p(X)}\\right)^2\\right] = \\mathbb{E}\\left[\\frac{f(X)}{p(X)^2}\\right] = \\int_D \\frac{f(x)}{p(x)^2} p(x) \\,dx = \\int_D \\frac{f(x)}{p(x)} \\,dx $$\nTherefore, the variance of the estimator $\\hat{J}$ is:\n$$ \\mathrm{Var}[\\hat{J}] = \\frac{1}{N} \\left( \\int_D \\frac{f(x)}{p(x)} \\,dx - J^2 \\right) $$\nNow, let us consider the specific design choice $p(x) \\propto f(x)$. For $p(x)$ to be a valid probability density, it must integrate to $1$. Given $\\int_D f(x)\\,dx = K$, the correct normalization is $p(x) = f(x)/K$. Substituting this into the variance expression:\n$$ \\int_D \\frac{f(x)}{p(x)} \\,dx = \\int_D \\frac{f(x)}{f(x)/K} \\,dx = \\int_D K \\,dx = K \\int_D 1 \\,dx = K|D| $$\nwhere $|D|$ is the Lebesgue measure of the domain $D$. The variance becomes:\n$$ \\mathrm{Var}[\\hat{J}] = \\frac{1}{N} \\left( K|D| - J^2 \\right) $$\nThis result is valid only if the integral $\\int_D K \\,dx$ converges. If the domain $D$ has infinite measure, $|D| = +\\infty$, then this integral diverges, and the variance of the estimator $\\hat{J}$ is infinite.\n\nWith these derivations, we evaluate each option.\n\n**A. Reuse both the samples $x_i$ and the original weights $w_i = f(x_i)/p(x_i)$ without change. The estimator $\\frac{1}{N}\\sum_{i=1}^N w_i$ is unbiased for $J$ and has minimal variance when $p(x) \\propto f(x)$.**\nThe estimator proposed is $\\hat{I}_f = \\frac{1}{N}\\sum_{i=1}^N w_i$. Its expectation is $\\mathbb{E}[\\hat{I}_f] = \\int_D f(x)\\,dx = K$. This estimator is unbiased for $I_f$, not for $J = \\int_D \\sqrt{f(x)}\\,dx$. The statement that it is unbiased for $J$ is false. The claim about minimal variance for this choice of $p(x)$ pertains to estimating $I_f$, not $J$.\nVerdict: **Incorrect**.\n\n**B. You may reuse the same samples $x_i$, but you must recompute weights as $w_i' = \\sqrt{f(x_i)}/p(x_i)$. The estimator $\\frac{1}{N}\\sum_{i=1}^N w_i'$ is unbiased for $J$ with variance $\\frac{1}{N}\\left(\\int_D \\frac{f(x)}{p(x)}\\,dx - J^2\\right)$. In particular, if $p(x) = f(x)/K$ and $f(x) > 0$ almost everywhere on $D$, then $\\mathrm{Var} = \\frac{1}{N}\\left(K\\,|D| - J^2\\right)$, which is infinite when $|D| = +\\infty$.**\nThis option correctly states that samples can be reused but require new weights $w_i' = \\sqrt{f(x_i)}/p(x_i)$. Our derivation confirms that the estimator $\\hat{J} = \\frac{1}{N}\\sum_{i=1}^N w_i'$ is indeed unbiased for $J$. The expression for the variance $\\frac{1}{N}\\left(\\int_D \\frac{f(x)}{p(x)}\\,dx - J^2\\right)$ is precisely what we derived. The subsequent analysis of the special case $p(x) = f(x)/K$, leading to a variance of $\\frac{1}{N}\\left(K|D| - J^2\\right)$ and its divergence for $|D|=\\infty$, is also correct. Every part of this statement is validated by our rigorous analysis.\nVerdict: **Correct**.\n\n**C. You cannot reuse the samples $x_i$ at all. To avoid bias, you must draw a fresh set of samples from a proposal density proportional to $\\sqrt{f(x)}$, and any attempt to reuse the original samples leads to bias.**\nThis statement is fundamentally flawed. A key strength of importance sampling is the ability to reuse a single set of samples to estimate multiple different integrals, provided the support condition holds for each. Our analysis for option B demonstrates that reusing the samples $x_i$ with recomputed weights yields a provably unbiased estimator for $J$. The claim that this procedure leads to bias is false. While sampling from $p(x) \\propto \\sqrt{f(x)}$ would be optimal for estimating $J$ (it would yield zero variance), it is not the only way to obtain an unbiased estimate.\nVerdict: **Incorrect**.\n\n**D. Self-normalizing the original weights as $\\tilde{w}_i = w_i/\\sum_{j=1}^N w_j$ and reusing them with the same samples yields an unbiased estimator for $J$ that always has lower variance than the unnormalized importance sampling estimator, regardless of $p(x)$.**\nThis option describes a self-normalized importance sampling scheme. First, any such estimator for a quantity not proportional to $I_f$ is generally biased for finite sample size $N$, although it is typically asymptotically unbiased. The claim of being unbiased is false. Second, the claim that it *always* has lower variance is also false; while self-normalization can be beneficial, especially if the normalization constant of the proposal is unknown, it does not guarantee variance reduction in all cases. Third, the description of how to form an estimator for $J$ using these normalized weights $\\tilde{w}_i$ is ambiguous and, under standard interpretations, does not lead to an estimator for $J$.\nVerdict: **Incorrect**.", "answer": "$$\\boxed{B}$$", "id": "2402977"}, {"introduction": "We conclude with an advanced application that mirrors contemporary challenges in computational science: estimating the probability of a rare event. This hands-on coding problem asks you to build an importance sampler to determine the likelihood that a random matrix is singular or nearly singular [@problem_id:2402958]. Successfully completing this task requires integrating concepts from probability, linear algebra, and programming, and it serves as an excellent demonstration of how to design a proposal distribution to efficiently probe rare but critical regions of a high-dimensional parameter space.", "problem": "You are given a random matrix model and an event defined by the determinant magnitude. Let $A \\in \\mathbb{R}^{2 \\times 2}$ be a random matrix with the matrix normal distribution with zero mean, row covariance $U \\in \\mathbb{R}^{2 \\times 2}$, and column covariance $V \\in \\mathbb{R}^{2 \\times 2}$. This is denoted by $A \\sim \\mathcal{MN}\\!\\left(0, U, V\\right)$ and implies that the vectorization $\\operatorname{vec}(A) \\in \\mathbb{R}^{4}$ is multivariate normal with zero mean and covariance $\\Sigma = V \\otimes U$, where $\\otimes$ denotes the Kronecker product. For a given threshold $\\tau \\ge 0$, define the event $E_{\\tau} = \\{A : |\\det(A)| \\le \\tau\\}$ and the target probability\n$$\nP(\\tau) = \\mathbb{P}\\left(|\\det(A)| \\le \\tau\\right) = \\int_{\\mathbb{R}^{4}} \\mathbf{1}\\left(|\\det(A)| \\le \\tau\\right) \\, p_{\\Sigma}\\!\\left(\\operatorname{vec}(A)\\right) \\, d\\operatorname{vec}(A),\n$$\nwhere $p_{\\Sigma}$ is the density of the multivariate normal $\\mathcal{N}\\!\\left(0, \\Sigma\\right)$.\n\nYou will compute $P(\\tau)$ via Monte Carlo importance sampling using a proposal matrix normal distribution $Q$ with zero mean and covariances $U_{q}$ and $V_{q}$, which induces a proposal covariance $\\Sigma_{q} = V_{q} \\otimes U_{q}$ for $\\operatorname{vec}(A)$. The corresponding target covariance is $\\Sigma_{p} = V \\otimes U$.\n\nAll matrices used below are symmetric and positive definite and are specified explicitly as\n$$\nU = \\begin{pmatrix} 1 & 0.8 \\\\ 0.8 & 1 \\end{pmatrix}, \\quad\nV = \\begin{pmatrix} 1 & 0.3 \\\\ 0.3 & 1 \\end{pmatrix},\n$$\n$$\nU_{q} = \\begin{pmatrix} 1 & 0.98 \\\\ 0.98 & 1 \\end{pmatrix}, \\quad\nV_{q} = \\begin{pmatrix} 1 & 0.9 \\\\ 0.9 & 1 \\end{pmatrix}.\n$$\n\nInput-free program requirement. Your program must, without reading any input, estimate $P(\\tau)$ for the following test suite of thresholds, sample sizes, and random seeds:\n- Case A (boundary event with measure essentially zero): $\\tau = 0$, $N = 100000$, seed $= 12345$.\n- Case B (typical intermediate scale): $\\tau = 0.5$, $N = 100000$, seed $= 24680$.\n- Case C (high threshold near certainty): $\\tau = 10$, $N = 100000$, seed $= 13579$.\n\nIn all cases, sampling must be performed from the proposal distribution with covariance $\\Sigma_{q}$ and weighted by the importance ratio of the target with covariance $\\Sigma_{p}$ relative to the proposal. The final answer in each case is the Monte Carlo estimate of $P(\\tau)$ as a real number.\n\nFinal output format. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order Case A, Case B, Case C, with each number rounded to exactly six digits after the decimal point (for example, $[0.000000,0.123456,0.999999]$). No additional text should be printed.", "solution": "The objective is to compute the probability $P(\\tau) = \\mathbb{P}(|\\det(A)| \\le \\tau)$ for a random matrix $A \\in \\mathbb{R}^{2 \\times 2}$ drawn from a matrix normal distribution $A \\sim \\mathcal{MN}(0, U, V)$. This probability can be expressed as an integral over the space of vectorized matrices $\\mathbb{R}^4$:\n$$\nP(\\tau) = \\int_{\\mathbb{R}^{4}} \\mathbf{1}(|\\det(A)| \\le \\tau) \\, p_{\\Sigma_p}(\\operatorname{vec}(A)) \\, d\\operatorname{vec}(A)\n$$\nwhere $p_{\\Sigma_p}$ is the probability density function (PDF) of the target multivariate normal distribution $\\mathcal{N}(0, \\Sigma_p)$, with covariance $\\Sigma_p = V \\otimes U$. The function $\\mathbf{1}(\\cdot)$ is the indicator function.\n\nDirect Monte Carlo estimation would involve sampling from $p_{\\Sigma_p}$ and calculating the fraction of samples satisfying the event condition. This can be inefficient, especially for small $\\tau$ where the event is rare. The problem specifies the use of importance sampling, which can improve efficiency by sampling from a different proposal distribution that concentrates samples in the region of interest. Here, the proposal distribution is another matrix normal, $A_q \\sim \\mathcal{MN}(0, U_q, V_q)$, inducing the PDF $p_{\\Sigma_q}$ for $\\operatorname{vec}(A_q) \\sim \\mathcal{N}(0, \\Sigma_q)$, where $\\Sigma_q = V_q \\otimes U_q$.\n\nThe core principle of importance sampling relies on rewriting the integral as an expectation with respect to the proposal distribution:\n$$\nP(\\tau) = \\mathbb{E}_{p_{\\Sigma_p}}[\\mathbf{1}(|\\det(A)| \\le \\tau)] = \\int \\mathbf{1}(|\\det(A)| \\le \\tau) \\frac{p_{\\Sigma_p}(\\operatorname{vec}(A))}{p_{\\Sigma_q}(\\operatorname{vec}(A))} p_{\\Sigma_q}(\\operatorname{vec}(A)) \\, d\\operatorname{vec}(A)\n$$\nThis leads to the expression:\n$$\nP(\\tau) = \\mathbb{E}_{p_{\\Sigma_q}}\\left[\\mathbf{1}(|\\det(A)| \\le \\tau) \\, w(\\operatorname{vec}(A))\\right]\n$$\nwhere $w(x) = p_{\\Sigma_p}(x) / p_{\\Sigma_q}(x)$ is the importance weight for a state $x = \\operatorname{vec}(A)$.\n\nThe Monte Carlo estimator for $P(\\tau)$ is the sample mean obtained by drawing $N$ independent samples $A^{(i)}$ from the proposal distribution:\n$$\n\\hat{P}_N(\\tau) = \\frac{1}{N} \\sum_{i=1}^{N} \\mathbf{1}(|\\det(A^{(i)})| \\le \\tau) \\, w(\\operatorname{vec}(A^{(i)}))\n$$\nThe PDF for a $k$-dimensional zero-mean multivariate normal distribution $\\mathcal{N}(0, \\Sigma)$ is given by $p_{\\Sigma}(x) = ((2\\pi)^k \\det(\\Sigma))^{-1/2} \\exp(-\\frac{1}{2} x^T \\Sigma^{-1} x)$. For our $k=4$ case, the importance weight $w(x)$ is the ratio of the target PDF to the proposal PDF:\n$$\nw(x) = \\frac{p_{\\Sigma_p}(x)}{p_{\\Sigma_q}(x)} = \\sqrt{\\frac{\\det(\\Sigma_q)}{\\det(\\Sigma_p)}} \\exp\\left(-\\frac{1}{2} x^T (\\Sigma_p^{-1} - \\Sigma_q^{-1}) x\\right)\n$$\nThe required covariance matrices are constructed using the Kronecker product, $\\Sigma_p = V \\otimes U$ and $\\Sigma_q = V_q \\otimes U_q$. Their inverses are likewise given by $\\Sigma_p^{-1} = V^{-1} \\otimes U^{-1}$ and $\\Sigma_q^{-1} = V_q^{-1} \\otimes U_q^{-1}$.\n\nThe computational algorithm proceeds as follows:\n1.  Define the matrices $U$, $V$, $U_q$, and $V_q$.\n2.  Construct the target and proposal covariance matrices, $\\Sigma_p$ and $\\Sigma_q$, using the Kronecker product.\n3.  Pre-compute the constant part of the weight, $C = \\sqrt{\\det(\\Sigma_q)/\\det(\\Sigma_p)}$, and the matrix for the quadratic form in the exponent, $M = \\Sigma_p^{-1} - \\Sigma_q^{-1}$.\n4.  For each specified test case $(\\tau, N, \\text{seed})$:\n    a. Initialize a random number generator with the given seed for reproducibility.\n    b. Generate $N$ samples $x^{(i)}$ from the proposal distribution $\\mathcal{N}(0, \\Sigma_q)$. Each $x^{(i)}$ is a $4$-element vector.\n    c. For each sample $x^{(i)}$, reconstruct the corresponding matrix $A^{(i)}$ using the convention that $\\operatorname{vec}(A)$ stacks the columns of $A$. Specifically, if $x = [x_0, x_1, x_2, x_3]^T$, then $A = \\begin{pmatrix} x_0 & x_2 \\\\ x_1 & x_3 \\end{pmatrix}$.\n    d. Calculate the determinant $\\det(A^{(i)}) = x_0^{(i)}x_3^{(i)} - x_1^{(i)}x_2^{(i)}$.\n    e. If $|\\det(A^{(i)})| \\le \\tau$, compute the importance weight $w(x^{(i)}) = C \\exp(-\\frac{1}{2} (x^{(i)})^T M x^{(i)})$ and add it to a running total.\n    f. The estimate for the case is the total accumulated weight divided by $N$.\n\nFor Case A with $\\tau = 0$, the event is $|\\det(A)| = 0$. The set of singular $2 \\times 2$ matrices forms a lower-dimensional manifold in $\\mathbb{R}^4$ and thus has Lebesgue measure zero. The probability of a continuously distributed random variable falling on this set is $0$. Therefore, the theoretical value is $P(0) = 0$, and the Monte Carlo estimate will almost surely be $0$ as well.\n\nThe proposal distribution, with higher correlations in $U_q$ and $V_q$ compared to $U$ and $V$, is chosen to generate matrices that are more frequently \"nearly singular\". This concentrates the sampling effort in the region where $\\det(A)$ is small, which is the most relevant region for calculating the probability, thereby increasing the statistical efficiency of the estimator.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Estimates the probability P(tau) = P(|det(A)| <= tau) for a random matrix A\n    using Monte Carlo importance sampling.\n    \"\"\"\n    # Define covariance matrices from the problem statement.\n    # Target distribution covariances\n    U = np.array([[1.0, 0.8], [0.8, 1.0]], dtype=np.float64)\n    V = np.array([[1.0, 0.3], [0.3, 1.0]], dtype=np.float64)\n    # Proposal distribution covariances\n    U_q = np.array([[1.0, 0.98], [0.98, 1.0]], dtype=np.float64)\n    V_q = np.array([[1.0, 0.9], [0.9, 1.0]], dtype=np.float64)\n\n    # Construct the full 4x4 covariance matrices using the Kronecker product.\n    # Sigma_p for the target distribution p(x)\n    Sigma_p = np.kron(V, U)\n    # Sigma_q for the proposal distribution q(x)\n    Sigma_q = np.kron(V_q, U_q)\n\n    # Pre-compute components of the importance weight w(x) = p(x)/q(x).\n    # The weight is w(x) = sqrt(det(Sigma_q)/det(Sigma_p)) * exp(-0.5 * x^T * (inv(Sigma_p) - inv(Sigma_q)) * x)\n    \n    # Determinants\n    det_p = np.linalg.det(Sigma_p)\n    det_q = np.linalg.det(Sigma_q)\n\n    # Inverses\n    inv_Sigma_p = np.linalg.inv(Sigma_p)\n    inv_Sigma_q = np.linalg.inv(Sigma_q)\n\n    # Constant part of the weight calculation\n    C = np.sqrt(det_q / det_p)\n    \n    # Matrix for the quadratic form in the exponent\n    M = inv_Sigma_p - inv_Sigma_q\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (tau, N, seed)\n        (0.0, 100000, 12345),  # Case A\n        (0.5, 100000, 24680),  # Case B\n        (10.0, 100000, 13579)  # Case C\n    ]\n\n    results = []\n    for tau, N, seed in test_cases:\n        # Set up a random number generator with the specified seed for reproducibility.\n        rng = np.random.default_rng(seed)\n        \n        # Generate N samples from the proposal distribution N(0, Sigma_q).\n        mean = np.zeros(4, dtype=np.float64)\n        samples = rng.multivariate_normal(mean, Sigma_q, size=N, method='cholesky')\n        \n        total_weight_sum = 0.0\n        \n        for x in samples:\n            # The vector x is vec(A), where A = [[x[0], x[2]], [x[1], x[3]]].\n            # Calculate the determinant of the matrix A.\n            det_A = x[0] * x[3] - x[2] * x[1]\n            \n            # Check if the event |det(A)| <= tau occurs.\n            if abs(det_A) <= tau:\n                # If the event occurs, calculate the importance weight.\n                # The exponent is -0.5 * x^T * M * x\n                quad_form = x.T @ M @ x\n                weight = C * np.exp(-0.5 * quad_form)\n                total_weight_sum += weight\n        \n        # The estimate of the probability is the average of the weights.\n        estimate = total_weight_sum / N\n        results.append(estimate)\n\n    # Final print statement in the exact required format.\n    # Format each result to exactly six decimal places.\n    formatted_results = [f\"{r:.6f}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "2402958"}]}