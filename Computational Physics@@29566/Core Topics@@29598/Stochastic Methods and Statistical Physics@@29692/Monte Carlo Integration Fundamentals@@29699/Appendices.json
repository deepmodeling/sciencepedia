{"hands_on_practices": [{"introduction": "Let's begin with a classic and intuitive application of Monte Carlo integration: estimating the volume of a complex shape. This practice challenges you to apply the \"hit-or-miss\" method, which is analogous to randomly throwing darts at a board to measure an area. By implementing an estimator for the volume of a convex hull, you will build a foundational understanding of how random sampling can solve deterministic geometric problems where analytical solutions are intractable [@problem_id:2414597].", "problem": "Consider the problem of estimating the volume of the convex hull in three dimensions using Monte Carlo (MC) integration. Let $d=3$. Let $\\mathcal{P}=\\{\\mathbf{p}_i\\}_{i=1}^n \\subset [0,1]^3$ be a finite set of points and let $\\mathrm{conv}(\\mathcal{P})$ denote their convex hull. The goal is to estimate the volume $V(\\mathrm{conv}(\\mathcal{P}))$. Work entirely in purely mathematical terms with no physical units. All angles, if any occur, must be considered in radians, but no angles are used here.\n\nStart from the following fundamental base: for any measurable set $S \\subseteq [0,1]^3$, the volume of $S$ is the integral $V(S)=\\int_{[0,1]^3} \\mathbf{1}_S(\\mathbf{x})\\,\\mathrm{d}\\mathbf{x}$, where $\\mathbf{1}_S$ is the indicator function of $S$. This identity connects Lebesgue measure and expectation over the uniform distribution on $[0,1]^3$.\n\nTasks:\n- From the base above and first principles of Monte Carlo (MC) integration, derive an estimator for $V(\\mathrm{conv}(\\mathcal{P}))$ that uses $N$ independent and identically distributed samples $\\{\\mathbf{X}_k\\}_{k=1}^N$ drawn uniformly from $[0,1]^3$, and a point-in-polyhedron predicate for $\\mathrm{conv}(\\mathcal{P})$ in half-space form. State clearly the assumptions ensuring unbiasedness and derive the variance as a function of $N$ and $V(\\mathrm{conv}(\\mathcal{P}))$. No shortcut formulas may be used in this derivation.\n- Implement a complete, runnable program that performs the following for each test case:\n  - Construct $\\mathcal{P}$ either by uniform sampling in $[0,1]^3$ with a specified random seed or as the $8$ vertices of the unit cube.\n  - Compute a half-space representation of $\\mathrm{conv}(\\mathcal{P})$ using a robust convex hull routine.\n  - Generate $N$ independent and identically distributed uniform samples in $[0,1]^3$ with a specified random seed, evaluate the indicator function for membership in $\\mathrm{conv}(\\mathcal{P})$, and compute the Monte Carlo estimate of the volume.\n  - Return the estimate for each test case as a floating-point number rounded to $6$ decimal places.\n\nTest suite:\n- Case $1$ (happy path, large $N$): $\\mathcal{P}$ is $n=100$ points sampled uniformly in $[0,1]^3$ using seed $s_p=2025$. Use $N=100000$ Monte Carlo samples with seed $s_m=314159$.\n- Case $2$ (variance stress, small $N$): same $\\mathcal{P}$ as Case $1$. Use $N=5000$ Monte Carlo samples with seed $s_m=271828$.\n- Case $3$ (different cloud, large $N$): $\\mathcal{P}$ is $n=100$ points sampled uniformly in $[0,1]^3$ using seed $s_p=42$. Use $N=100000$ Monte Carlo samples with seed $s_m=123456$.\n- Case $4$ (boundary coverage, full cube): $\\mathcal{P}$ equals the $8$ vertices of the unit cube in $[0,1]^3$. Use $N=200000$ Monte Carlo samples with seed $s_m=13579$.\n\nAnswer specification and output format:\n- For each case, output the Monte Carlo estimate of $V(\\mathrm{conv}(\\mathcal{P}))$ rounded to $6$ decimal places as a float.\n- Aggregate the results for Cases $1$ to $4$ in order into a single line of output containing the results as a comma-separated list enclosed in square brackets, with no spaces, for example, `[x_1,x_2,x_3,x_4]` where each $x_i$ is a float rounded to $6$ decimal places.", "solution": "The problem statement has been analyzed and is deemed valid. It is self-contained, well-posed, and scientifically grounded in the established principles of Monte Carlo methods and computational geometry. It presents a clear, objective task that can be resolved through rigorous mathematical derivation and computational implementation. We will now proceed with the solution.\n\nThe core of the problem is to estimate the volume of a convex set $S = \\mathrm{conv}(\\mathcal{P})$, where $S$ is a subset of the unit cube, $[0,1]^3$. The volume $V(S)$ is defined by the Lebesgue integral of the indicator function $\\mathbf{1}_S(\\mathbf{x})$ over the unit cube:\n$$V(S) = \\int_{[0,1]^3} \\mathbf{1}_S(\\mathbf{x})\\,\\mathrm{d}\\mathbf{x}$$\nwhere $\\mathbf{1}_S(\\mathbf{x}) = 1$ if $\\mathbf{x} \\in S$ and $\\mathbf{1}_S(\\mathbf{x}) = 0$ if $\\mathbf{x} \\notin S$.\n\nThis integral can be interpreted as the expectation of the function $g(\\mathbf{X}) = \\mathbf{1}_S(\\mathbf{X})$ where $\\mathbf{X}$ is a random variable drawn from the uniform distribution on $[0,1]^3$. The probability density function (PDF) for this distribution is $f(\\mathbf{x}) = 1$ for $\\mathbf{x} \\in [0,1]^3$ and $f(\\mathbf{x}) = 0$ otherwise. The expectation is thus:\n$$E[\\mathbf{1}_S(\\mathbf{X})] = \\int_{[0,1]^3} \\mathbf{1}_S(\\mathbf{x}) f(\\mathbf{x}) \\,\\mathrm{d}\\mathbf{x} = \\int_{[0,1]^3} \\mathbf{1}_S(\\mathbf{x}) \\cdot 1 \\,\\mathrm{d}\\mathbf{x} = V(S)$$\nThis identity is the foundation of the Monte Carlo method for this problem.\n\nAccording to the Law of Large Numbers, the expectation of a random variable can be approximated by the sample mean of a large number of independent and identically distributed (i.i.d.) samples. Let $\\{\\mathbf{X}_k\\}_{k=1}^N$ be $N$ such samples drawn uniformly from $[0,1]^3$. The sample mean of the random variable $\\mathbf{1}_S(\\mathbf{X})$ provides an estimator, $\\hat{V}_N$, for the volume $V(S)$:\n$$\\hat{V}_N = \\frac{1}{N} \\sum_{k=1}^N \\mathbf{1}_S(\\mathbf{X}_k)$$\nThis estimator is known as the \"hit-or-miss\" Monte Carlo estimator. It represents the fraction of random points that fall inside the set $S$, scaled by the volume of the sampling domain, which in this case is $1$.\n\nWe must analyze the statistical properties of this estimator, specifically its bias and variance.\n\nThe estimator $\\hat{V}_N$ is unbiased if its expected value equals the true volume, $E[\\hat{V}_N] = V(S)$. We verify this using the linearity of expectation:\n$$E[\\hat{V}_N] = E\\left[\\frac{1}{N} \\sum_{k=1}^N \\mathbf{1}_S(\\mathbf{X}_k)\\right] = \\frac{1}{N} \\sum_{k=1}^N E[\\mathbf{1}_S(\\mathbf{X}_k)]$$\nThe assumption is that the samples $\\{\\mathbf{X}_k\\}$ are identically distributed. Therefore, $E[\\mathbf{1}_S(\\mathbf{X}_k)]$ is constant for all $k$ and is equal to $E[\\mathbf{1}_S(\\mathbf{X})] = V(S)$. Substituting this into the equation gives:\n$$E[\\hat{V}_N] = \\frac{1}{N} \\sum_{k=1}^N V(S) = \\frac{1}{N} \\cdot N \\cdot V(S) = V(S)$$\nThe estimator is indeed unbiased. The crucial assumption for this property is that the samples are drawn from the correct uniform distribution on $[0,1]^3$.\n\nNext, we derive the variance of the estimator, $\\mathrm{Var}(\\hat{V}_N)$. The variance of the estimator quantifies the expected squared deviation from the mean and is a measure of the estimator's precision. The assumption that the samples $\\{\\mathbf{X}_k\\}$ are independent allows us to state that the variance of the sum is the sum of the variances:\n$$\\mathrm{Var}(\\hat{V}_N) = \\mathrm{Var}\\left(\\frac{1}{N} \\sum_{k=1}^N \\mathbf{1}_S(\\mathbf{X}_k)\\right) = \\frac{1}{N^2} \\sum_{k=1}^N \\mathrm{Var}(\\mathbf{1}_S(\\mathbf{X}_k))$$\nBecause the samples are identically distributed, the variance term $\\mathrm{Var}(\\mathbf{1}_S(\\mathbf{X}_k))$ is constant for all $k$. Let this be $\\mathrm{Var}(\\mathbf{1}_S(\\mathbf{X}))$. The expression simplifies to:\n$$\\mathrm{Var}(\\hat{V}_N) = \\frac{1}{N^2} \\cdot N \\cdot \\mathrm{Var}(\\mathbf{1}_S(\\mathbf{X})) = \\frac{1}{N} \\mathrm{Var}(\\mathbf{1}_S(\\mathbf{X}))$$\nThe random variable $Y = \\mathbf{1}_S(\\mathbf{X})$ is a Bernoulli variable, as it takes only two values: $1$ (a \"hit\") or $0$ (a \"miss\"). The probability of a hit is $p = P(Y=1) = P(\\mathbf{X} \\in S)$. As established, this probability is equal to the volume $V(S)$. The variance of a Bernoulli variable with parameter $p$ is given by $p(1-p)$. Therefore:\n$$\\mathrm{Var}(\\mathbf{1}_S(\\mathbf{X})) = V(S)(1 - V(S))$$\nSubstituting this result into the equation for the estimator's variance, we obtain the final expression:\n$$\\mathrm{Var}(\\hat{V}_N) = \\frac{V(S)(1 - V(S))}{N}$$\nThis result shows that the variance of the Monte Carlo estimate is inversely proportional to the number of samples $N$. Consequently, the standard error, $\\sigma_{\\hat{V}_N} = \\sqrt{\\mathrm{Var}(\\hat{V}_N)}$, scales as $1/\\sqrt{N}$, which is the characteristic convergence rate of standard Monte Carlo integration. The variance is maximal when $V(S) = 0.5$ and decreases as the volume approaches $0$ or $1$.\n\nTo implement this, a point-in-polyhedron test is required. A convex polyhedron, such as $\\mathrm{conv}(\\mathcal{P})$, can be represented as the intersection of a finite number of half-spaces. Each half-space is defined by an inequality of the form $\\mathbf{a}_j \\cdot \\mathbf{x} + b_j \\le 0$, where $\\mathbf{a}_j$ is the normal vector to the $j$-th facet plane and $b_j$ is the offset. A point $\\mathbf{x}$ is inside the convex hull if and only if it satisfies all of these inequalities simultaneously. This provides a direct method to evaluate $\\mathbf{1}_S(\\mathbf{X}_k)$ for each sample point $\\mathbf{X}_k$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.spatial import ConvexHull\nimport itertools\n\ndef solve():\n    \"\"\"\n    Solves the Monte Carlo volume estimation problem for the given test cases.\n    \"\"\"\n\n    def estimate_volume(hull, N, seed_m):\n        \"\"\"\n        Estimates the volume of a convex hull using Monte Carlo integration.\n        \n        Args:\n            hull (scipy.spatial.ConvexHull): The convex hull object.\n            N (int): The number of Monte Carlo samples.\n            seed_m (int): The random seed for Monte Carlo samples.\n\n        Returns:\n            float: The estimated volume.\n        \"\"\"\n        # Generate N samples uniformly from the [0,1]^3 cube\n        rng_mc = np.random.default_rng(seed_m)\n        mc_samples = rng_mc.random((N, 3))\n\n        # The hull is defined by a set of half-spaces Ax + b <= 0.\n        # A is hull.equations[:, :3] and b is hull.equations[:, 3].\n        # The normals in hull.equations point outwards from the hull.\n        A = hull.equations[:, :3]\n        b = hull.equations[:, 3]\n\n        # For each sample point, evaluate all plane equations.\n        # A point is inside if it satisfies all inequalities.\n        # Use a small tolerance for floating point comparisons.\n        is_inside = np.all(mc_samples @ A.T + b <= 1e-12, axis=1)\n        \n        # The number of \"hits\" is the sum of boolean `is_inside` array.\n        hits = np.sum(is_inside)\n\n        # The volume estimate is the ratio of hits to total samples,\n        # since the bounding box volume is 1.\n        volume_estimate = hits / N\n        \n        return volume_estimate\n\n    test_cases = [\n        # (n, s_p, N, s_m, use_cube_vertices)\n        (100, 2025, 100000, 314159, False),\n        (100, 2025, 5000, 271828, False),\n        (100, 42, 100000, 123456, False),\n        (8, None, 200000, 13579, True),\n    ]\n\n    results = []\n    \n    # Pre-compute hull for cases 1 and 2 as they share the same point cloud\n    rng_points_c12 = np.random.default_rng(2025)\n    points_c12 = rng_points_c12.random((100, 3))\n    hull_c12 = ConvexHull(points_c12)\n\n    # Case 1\n    n1, s_p1, N1, s_m1, _ = test_cases[0]\n    vol1 = estimate_volume(hull_c12, N1, s_m1)\n    results.append(round(vol1, 6))\n\n    # Case 2\n    n2, s_p2, N2, s_m2, _ = test_cases[1]\n    vol2 = estimate_volume(hull_c12, N2, s_m2)\n    results.append(round(vol2, 6))\n\n    # Case 3\n    n3, s_p3, N3, s_m3, _ = test_cases[2]\n    rng_points_c3 = np.random.default_rng(s_p3)\n    points_c3 = rng_points_c3.random((n3, 3))\n    hull_c3 = ConvexHull(points_c3)\n    vol3 = estimate_volume(hull_c3, N3, s_m3)\n    results.append(round(vol3, 6))\n\n    # Case 4\n    n4, s_p4, N4, s_m4, use_cube = test_cases[3]\n    # Generate the 8 vertices of the unit cube\n    points_c4 = np.array(list(itertools.product([0, 1], repeat=3)), dtype=float)\n    hull_c4 = ConvexHull(points_c4)\n    vol4 = estimate_volume(hull_c4, N4, s_m4)\n    results.append(round(vol4, 6))\n    \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2414597"}, {"introduction": "Monte Carlo methods are exceptionally powerful for analyzing systems with inherent randomness, a common scenario in engineering and manufacturing. This exercise moves beyond simple geometric estimation to a practical problem of predicting process yield [@problem_id:2414629]. You will estimate the probability of a successful assembly by simulating random variations in part dimensions, demonstrating how to compute the expectation of an indicator function over a multi-dimensional parameter space.", "problem": "A rectangular manufactured part must fit into a rectangular fixture. The part has a random width and height, and the fixture has a random slot width and slot height. All dimensions are measured in millimeters (mm). A single trial results in a successful fit if and only if the part’s width is less than or equal to the slot width and the part’s height is less than or equal to the slot height. Let the part width be the random variable $W_{p}$, the part height be $H_{p}$, the slot width be $W_{s}$, and the slot height be $H_{s}$. Assume that $W_{p}$ and $H_{p}$ are independent of each other, $W_{s}$ and $H_{s}$ are independent of each other, and all part variables are independent of all fixture variables. The process yield is the probability that a random part fits into a random fixture, which is the probability of the event $\\{W_{p} \\leq W_{s} \\text{ and } H_{p} \\leq H_{s}\\}$.\n\nDesign a Monte Carlo estimator for the yield starting from the probabilistic definition of expectation as an integral and the definition of an indicator function. You must derive your estimator using these definitions. Implement your estimator as a complete, runnable program that:\n- Uses pseudo-random sampling to estimate the yield for each test case below.\n- Uses a fixed seed $s=12345$ for reproducibility.\n- Uses $N=400000$ independent samples per test case.\n- Reports each yield as a decimal fraction (not a percentage).\n- Prints results to six digits after the decimal point.\n\nTest suite (each case specifies the distributions for $(W_{p},H_{p},W_{s},H_{s})$):\n- Case A (happy path, all normal): \n  - $W_{p} \\sim \\mathcal{N}(50.0,\\,0.1^{2})$, $H_{p} \\sim \\mathcal{N}(10.0,\\,0.05^{2})$, \n  - $W_{s} \\sim \\mathcal{N}(50.15,\\,0.08^{2})$, $H_{s} \\sim \\mathcal{N}(10.10,\\,0.04^{2})$.\n- Case B (boundary, symmetric normals):\n  - $W_{p} \\sim \\mathcal{N}(30.0,\\,0.2^{2})$, $H_{p} \\sim \\mathcal{N}(20.0,\\,0.2^{2})$,\n  - $W_{s} \\sim \\mathcal{N}(30.0,\\,0.2^{2})$, $H_{s} \\sim \\mathcal{N}(20.0,\\,0.2^{2})$.\n- Case C (low yield, negative mean clearance):\n  - $W_{p} \\sim \\mathcal{N}(40.2,\\,0.1^{2})$, $H_{p} \\sim \\mathcal{N}(15.1,\\,0.1^{2})$,\n  - $W_{s} \\sim \\mathcal{N}(40.1,\\,0.1^{2})$, $H_{s} \\sim \\mathcal{N}(15.0,\\,0.1^{2})$.\n- Case D (uniform tolerances):\n  - $W_{p} \\sim \\mathcal{U}[99.0,\\,101.0]$, $H_{p} \\sim \\mathcal{U}[49.0,\\,51.0]$,\n  - $W_{s} \\sim \\mathcal{U}[100.0,\\,102.0]$, $H_{s} \\sim \\mathcal{U}[50.0,\\,51.0]$.\n\nAngle units are not involved. All physical units are in millimeters (mm). Your implementation must not truncate or clamp sampled values; use the distributions as stated.\n\nFinal output format: Your program should produce a single line of output containing the four estimated yields, in the order A, B, C, D, as a comma-separated list enclosed in square brackets (e.g., `[result_A,result_B,result_C,result_D]`), where each result is a float rounded to six digits after the decimal point.", "solution": "The problem statement has been validated and is deemed valid. It is scientifically grounded in probability theory and statistics, well-posed with all necessary parameters defined, and objective. The task is a standard application of Monte Carlo methods for estimating a probability, which is a core topic in computational physics and engineering. We shall proceed with a full solution.\n\nThe problem asks for an estimation of the manufacturing process yield, $Y$. The yield is defined as the probability that a randomly manufactured part fits into a random fixture. Let the part dimensions be the random variables $W_{p}$ (width) and $H_{p}$ (height), and the fixture slot dimensions be $W_{s}$ (width) and $H_{s}$ (height). A fit is successful if and only if two conditions are met simultaneously: $W_{p} \\leq W_{s}$ and $H_{p} \\leq H_{s}$.\n\nThe problem states that all four random variables, $W_{p}$, $H_{p}$, $W_{s}$, and $H_{s}$, are mutually independent. Let the probability density functions (PDFs) of these variables be $p_{W_p}(w_p)$, $p_{H_p}(h_p)$, $p_{W_s}(w_s)$, and $p_{H_s}(h_s)$, respectively. The state of the system can be described by a $4$-dimensional random vector $\\mathbf{X} = (W_p, H_p, W_s, H_s)$. Due to the independence assumption, the joint PDF of $\\mathbf{X}$ is the product of the individual PDFs:\n$$p(\\mathbf{x}) = p(w_p, h_p, w_s, h_s) = p_{W_p}(w_p) p_{H_p}(h_p) p_{W_s}(w_s) p_{H_s}(h_s)$$\nwhere $\\mathbf{x} = (w_p, h_p, w_s, h_s)$ is a specific realization of the random vector $\\mathbf{X}$.\n\nThe yield $Y$ is the probability of the success event, which we denote as $A$. The event $A$ is the set of all outcomes $\\mathbf{x}$ in the $4$-dimensional space $\\mathbb{R}^4$ such that $w_p \\leq w_s$ and $h_p \\leq h_s$. The probability $Y = P(A)$ is formally given by the integral of the joint PDF over the region defined by event $A$:\n$$Y = \\int \\int \\int \\int_{A} p(w_p, h_p, w_s, h_s) \\, dw_p \\, dh_p \\, dw_s \\, dh_s$$\n\nTo transform this into a form suitable for Monte Carlo estimation, we introduce an indicator function, $\\mathbb{I}_{A}(\\mathbf{x})$. This function is defined as:\n$$\n\\mathbb{I}_{A}(\\mathbf{x}) = \n\\begin{cases} \n1 & \\text{if } w_p \\leq w_s \\text{ and } h_p \\leq h_s \\\\\n0 & \\text{otherwise}\n\\end{cases}\n$$\nUsing the indicator function, we can rewrite the integral for the yield $Y$ over the entire state space $\\Omega = \\mathbb{R}^4$:\n$$Y = \\int_{\\Omega} \\mathbb{I}_{A}(\\mathbf{x}) p(\\mathbf{x}) \\, d\\mathbf{x}$$\nThis integral is, by definition, the expected value of the indicator function $\\mathbb{I}_{A}(\\mathbf{X})$ with respect to the probability distribution $p(\\mathbf{x})$:\n$$Y = E[\\mathbb{I}_{A}(\\mathbf{X})]$$\n\nThe Monte Carlo method provides an estimator for the expected value of a function $f(\\mathbf{X})$ by approximating it with the sample mean of the function evaluated at $N$ independent and identically distributed (i.i.d.) random samples, $\\mathbf{x}^{(1)}, \\mathbf{x}^{(2)}, \\ldots, \\mathbf{x}^{(N)}$, drawn from the distribution $p(\\mathbf{x})$. In our case, the function is $f(\\mathbf{X}) = \\mathbb{I}_{A}(\\mathbf{X})$.\n\nThe Monte Carlo estimator for the yield, $\\hat{Y}_N$, is therefore the sample mean of the indicator function:\n$$\\hat{Y}_N = \\frac{1}{N} \\sum_{i=1}^{N} \\mathbb{I}_{A}(\\mathbf{x}^{(i)})$$\nHere, $\\mathbf{x}^{(i)} = (w_p^{(i)}, h_p^{(i)}, w_s^{(i)}, h_s^{(i)})$ is the $i$-th random sample drawn from the joint distribution. Since $\\mathbb{I}_{A}(\\mathbf{x}^{(i)})$ is $1$ if the $i$-th sample corresponds to a successful fit and $0$ otherwise, the sum $\\sum_{i=1}^{N} \\mathbb{I}_{A}(\\mathbf{x}^{(i)})$ is simply the total count of successful fits, let's call it $N_{success}$.\n\nThus, the algorithm to estimate the yield is:\n1.  Initialize a success counter, $N_{success}$, to $0$.\n2.  Generate $N$ independent random samples, where each sample $i \\in \\{1, \\ldots, N\\}$ consists of four values $(w_p^{(i)}, h_p^{(i)}, w_s^{(i)}, h_s^{(i)})$ drawn from their specified distributions. For this problem, $N = 400000$.\n3.  For each sample $i$, evaluate the fit condition: if $w_p^{(i)} \\leq w_s^{(i)}$ and $h_p^{(i)} \\leq h_s^{(i)}$, increment $N_{success}$.\n4.  The estimated yield is $\\hat{Y}_N = \\frac{N_{success}}{N}$.\n\nThis procedure will be implemented for each of the four test cases specified. We will use a fixed random seed $s=12345$ for reproducibility. For distributions given as $\\mathcal{N}(\\mu, \\sigma^2)$, we sample from a normal distribution with mean $\\mu$ and standard deviation $\\sigma$. For uniform distributions $\\mathcal{U}[a, b]$, we sample from a uniform distribution over the interval $[a, b)$. The implementation will be vectorized for efficiency using the `numpy` library, generating $N$ samples for each of the four variables at once and then performing element-wise comparisons.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the manufacturing yield estimation problem using Monte Carlo simulation\n    for four different test cases.\n    \"\"\"\n    \n    # Define problem constants\n    SEED = 12345\n    NUM_SAMPLES = 400000\n\n    # Initialize the random number generator for reproducibility.\n    # The same generator instance is used across all test cases to ensure\n    # the entire output is reproducible from a single seed.\n    rng = np.random.default_rng(SEED)\n\n    # Define the test cases from the problem statement.\n    # Each case is a tuple of parameter tuples for (Wp, Hp, Ws, Hs).\n    # Each parameter tuple is (distribution_name, param1, param2).\n    # For 'norm': (mean, std_dev)\n    # For 'uniform': (low, high)\n    test_cases = [\n        # Case A (happy path, all normal)\n        (\n            ('norm', 50.0, 0.1), \n            ('norm', 10.0, 0.05),\n            ('norm', 50.15, 0.08), \n            ('norm', 10.10, 0.04)\n        ),\n        # Case B (boundary, symmetric normals)\n        (\n            ('norm', 30.0, 0.2), \n            ('norm', 20.0, 0.2), \n            ('norm', 30.0, 0.2), \n            ('norm', 20.0, 0.2)\n        ),\n        # Case C (low yield, negative mean clearance)\n        (\n            ('norm', 40.2, 0.1), \n            ('norm', 15.1, 0.1), \n            ('norm', 40.1, 0.1), \n            ('norm', 15.0, 0.1)\n        ),\n        # Case D (uniform tolerances)\n        (\n            ('uniform', 99.0, 101.0), \n            ('uniform', 49.0, 51.0), \n            ('uniform', 100.0, 102.0), \n            ('uniform', 50.0, 51.0)\n        )\n    ]\n\n    results = []\n    for case in test_cases:\n        wp_params, hp_params, ws_params, hs_params = case\n        \n        # Generate N samples for each of the four random variables\n        # using a vectorized approach for efficiency.\n        all_samples = []\n        for dist, p1, p2 in [wp_params, hp_params, ws_params, hs_params]:\n            if dist == 'norm':\n                # p1 is mean, p2 is standard deviation\n                samples = rng.normal(loc=p1, scale=p2, size=NUM_SAMPLES)\n            elif dist == 'uniform':\n                # p1 is low, p2 is high\n                samples = rng.uniform(low=p1, high=p2, size=NUM_SAMPLES)\n            all_samples.append(samples)\n            \n        wp_samples, hp_samples, ws_samples, hs_samples = all_samples\n\n        # Check the fit condition for all N samples in a vectorized manner.\n        # The condition is (Wp <= Ws) AND (Hp <= Hs).\n        # The result 'successes' is a boolean array.\n        is_fit = (wp_samples <= ws_samples) & (hp_samples <= hs_samples)\n\n        # The sum of the boolean array gives the total number of successful fits\n        # (True evaluates to 1, False to 0).\n        num_successes = np.sum(is_fit)\n\n        # The yield is the ratio of successful fits to the total number of samples.\n        yield_estimate = num_successes / NUM_SAMPLES\n        \n        # Format the result to six decimal places and append to the list.\n        results.append(f\"{yield_estimate:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "2414629"}, {"introduction": "A key strength of Monte Carlo integration is its well-understood convergence rate, where the error typically scales as $1/\\sqrt{N}$. This behavior is guaranteed by the Central Limit Theorem, which relies on the integrand having a finite variance. This final practice is a fascinating stress test: you will investigate a case where the variance is infinite and observe how the standard error estimates and confidence intervals fail, providing a crucial lesson on the theoretical limits of the method [@problem_id:2411534].", "problem": "Consider the family of integrals over the unit interval given by\n$$ I(p) \\equiv \\int_{0}^{1} u^{p} \\, du, $$\nwhere $p$ is a real parameter satisfying $p &gt; -1$ so that the integral exists and is finite. For a random variable $U$ uniformly distributed on $[0,1]$, define the integrand as $f_{p}(U) = U^{p}$. The exact value is\n$$ I(p) = \\frac{1}{p+1}. $$\nFor a given sample size $n \\in \\mathbb{N}$ and number of batches $B \\in \\mathbb{N}$, consider $B$ independent batches, each consisting of $n$ independent draws $U_{1},\\dots,U_{n}$ from the uniform distribution on $[0,1]$. For each batch $b \\in \\{1,\\dots,B\\}$, define the batch sample mean\n$$ \\bar{f}_{b} = \\frac{1}{n} \\sum_{i=1}^{n} f_{p}(U_{i}), $$\nand the unbiased batch sample variance\n$$ s_{b}^{2} = \\frac{1}{n-1} \\sum_{i=1}^{n} \\left( f_{p}(U_{i}) - \\bar{f}_{b} \\right)^{2}, $$\nprovided that $n \\ge 2$. When $n \\ge 2$, define the nominal $95$-level normal-based half-width\n$$ h_{b} = 1.96 \\cdot \\frac{\\sqrt{s_{b}^{2}}}{\\sqrt{n}}. $$\nFor each batch $b$ with $n \\ge 2$, form the nominal interval\n$$ \\left[ \\bar{f}_{b} - h_{b}, \\, \\bar{f}_{b} + h_{b} \\right]. $$\nLet the coverage indicator for batch $b$ be $C_{b} = 1$ if $I(p)$ lies within the interval above and $C_{b} = 0$ otherwise. If $n &lt; 2$, define $C_{b} = 0$ by convention because the sample variance is not defined. Define the empirical coverage fraction across batches as\n$$ \\widehat{\\mathrm{cov}} = \\frac{1}{B} \\sum_{b=1}^{B} C_{b}. $$\nNote that for $p \\le -\\tfrac{1}{2}$, the variance $\\mathbb{V}[f_{p}(U)]$ is infinite since\n$$ \\mathbb{E}\\left[ f_{p}(U)^{2} \\right] = \\int_{0}^{1} u^{2p} \\, du $$\ndiverges, even though $I(p)$ remains finite for all $p &gt; -1$. This situation violates the finite-variance condition typically required for the Central Limit Theorem (CLT), and thus the nominal normal-based error bound may fail.\n\nYour task is to write a complete program that, for each test case, produces the empirical coverage fraction $\\widehat{\\mathrm{cov}}$ as a decimal. Each test case specifies $(p, n, B, \\text{seed})$, where $\\text{seed}$ initializes a pseudorandom number generator to ensure reproducibility. Angles and physical units do not appear; no unit conversions are required. All answers must be expressed as decimals.\n\nTest Suite:\n- Test $1$: $p = -\\tfrac{1}{2}$, $n = 500$, $B = 200$, $\\text{seed} = 17$.\n- Test $2$: $p = -0.6$, $n = 500$, $B = 200$, $\\text{seed} = 19$.\n- Test $3$: $p = \\tfrac{1}{2}$, $n = 500$, $B = 200$, $\\text{seed} = 23$.\n- Test $4$: $p = \\tfrac{1}{2}$, $n = 5000$, $B = 200$, $\\text{seed} = 29$.\n- Test $5$: $p = -\\tfrac{1}{2}$, $n = 5000$, $B = 200$, $\\text{seed} = 31$.\n- Test $6$: $p = \\tfrac{1}{2}$, $n = 1$, $B = 200$, $\\text{seed} = 37$.\n\nFinal Output Format:\nYour program should produce a single line of output containing the six empirical coverage fractions for Tests $1$ through $6$, in order, as a comma-separated list enclosed in square brackets, for example, `[x_1,x_2,x_3,x_4,x_5,x_6]`.", "solution": "The problem statement has been evaluated and is determined to be valid. It is a well-posed problem in computational statistics, grounded in the principles of Monte Carlo integration and hypothesis testing. The definitions are clear, the parameters are fully specified, and the context (exploring the limits of the Central Limit Theorem) is scientifically sound. There are no contradictions, ambiguities, or factual errors.\n\nThe task is to perform a Monte Carlo simulation to compute the empirical coverage fraction of a nominal $95\\%$ confidence interval for the integral $I(p) = \\int_{0}^{1} u^{p} \\, du$. The simulation will examine how the coverage behavior changes with the parameter $p$, which controls the finiteness of the variance of the integrand $f_{p}(U) = U^{p}$, where $U \\sim \\text{Uniform}[0,1]$.\n\nThe exact value of the integral is known to be $I(p) = \\frac{1}{p+1}$ for $p > -1$.\n\nThe algorithm to compute the empirical coverage fraction, $\\widehat{\\mathrm{cov}}$, for a single test case specified by $(p, n, B, \\text{seed})$ is as follows:\n\n1.  Initialize a pseudorandom number generator with the provided integer `seed` to ensure reproducibility.\n2.  Calculate the true value of the integral, which we denote as $I_{true} = \\frac{1}{p+1}$.\n3.  A specific rule is given for the case when the sample size $n$ is less than $2$. If $n < 2$, the sample variance $s_{b}^{2}$ is undefined. By convention, the coverage indicator $C_{b}$ is set to $0$ for all batches. Consequently, the empirical coverage fraction $\\widehat{\\mathrm{cov}}$ is $0$. The calculation for this case terminates here.\n4.  For the primary case where $n \\ge 2$, the simulation proceeds over $B$ independent batches. We will maintain a count, initialized to zero, of the batches for which the confidence interval covers the true value $I_{true}$.\n5.  A highly efficient method involves generating all random numbers required for all $B$ batches at once. We generate a matrix of $B \\times n$ independent random variates, where each element is drawn from the uniform distribution on $[0,1]$. Let this matrix be denoted by $\\mathbf{U}$, with elements $U_{b,i}$ for batch $b \\in \\{1, \\dots, B\\}$ and sample index $i \\in \\{1, \\dots, n\\}$.\n6.  Apply the integrand function $f_{p}(u) = u^{p}$ element-wise to the matrix $\\mathbf{U}$ to obtain a new matrix $\\mathbf{F}$ with elements $F_{b,i} = (U_{b,i})^{p}$. Each row of $\\mathbf{F}$ corresponds to the sample $\\{f_{p}(U_{i})\\}_{i=1}^{n}$ for a single batch.\n7.  For each batch $b$ (i.e., for each row of $\\mathbf{F}$), we compute the required statistics:\n    a. The batch sample mean: $\\bar{f}_{b} = \\frac{1}{n} \\sum_{i=1}^{n} F_{b,i}$. This can be computed for all batches simultaneously by taking the mean across the columns of $\\mathbf{F}$.\n    b. The unbiased batch sample variance: $s_{b}^{2} = \\frac{1}{n-1} \\sum_{i=1}^{n} (F_{b,i} - \\bar{f}_{b})^{2}$. This can also be computed for all batches by taking the sample variance (with one degree of freedom correction, `ddof=1`) across the columns of $\\mathbf{F}$.\n8.  Using these statistics, we calculate the half-width of the nominal $95\\%$ confidence interval for each batch:\n    $$h_{b} = 1.96 \\cdot \\frac{\\sqrt{s_{b}^{2}}}{\\sqrt{n}}$$\n    Note that the value $1.96$ is the approximant for the $0.975$ quantile of the standard normal distribution, $z_{0.025}$.\n9.  For each batch $b$, we determine if the confidence interval $[\\bar{f}_{b} - h_{b}, \\bar{f}_{b} + h_{b}]$ contains the true value $I_{true}$. This condition is equivalent to testing if the absolute error $|\\bar{f}_{b} - I_{true}|$ is less than or equal to the half-width $h_{b}$.\n10. The number of batches for which this condition holds is counted. Let this count be $N_{covered}$.\n11. The empirical coverage fraction is the ratio of the number of covering intervals to the total number of batches:\n    $$\\widehat{\\mathrm{cov}} = \\frac{N_{covered}}{B}$$\nThis procedure is executed for each of the six test cases specified. The theoretical expectation is that for $p > -1/2$, where the variance of $f_{p}(U)$ is finite, the Central Limit Theorem holds, and $\\widehat{\\mathrm{cov}}$ should be close to the nominal level of $0.95$, especially for large $n$. Conversely, for $p \\le -1/2$, the variance is infinite, the theoretical foundation for the normal-based confidence interval is invalid, and we anticipate that $\\widehat{\\mathrm{cov}}$ will deviate significantly from $0.95$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef compute_coverage(p: float, n: int, B: int, seed: int) -> float:\n    \"\"\"\n    Computes the empirical coverage fraction for a given set of parameters.\n\n    Args:\n        p: The exponent parameter for the integrand u^p.\n        n: The sample size for each batch.\n        B: The number of batches.\n        seed: The seed for the random number generator.\n\n    Returns:\n        The empirical coverage fraction as a float.\n    \"\"\"\n    # Per the problem statement, for n < 2, the sample variance is not defined,\n    # and the coverage indicator C_b is 0 by convention. Thus, the total\n    # empirical coverage is 0.\n    if n < 2:\n        return 0.0\n\n    # Initialize the pseudorandom number generator for reproducibility.\n    rng = np.random.default_rng(seed)\n\n    # Calculate the exact value of the integral I(p) = 1/(p+1).\n    I_true = 1.0 / (p + 1.0)\n\n    # Generate all uniform random samples for all B batches at once.\n    # The shape of the resulting array is (B, n).\n    uniform_samples = rng.uniform(size=(B, n))\n\n    # Evaluate the integrand f_p(u) = u^p on the samples.\n    f_p_samples = uniform_samples**p\n\n    # Calculate batch means along axis 1 (across samples in a batch).\n    # The result is an array of shape (B,).\n    batch_means = np.mean(f_p_samples, axis=1)\n\n    # Calculate unbiased batch sample variances along axis 1.\n    # ddof=1 ensures the denominator is (n-1).\n    # The result is an array of shape (B,).\n    batch_variances = np.var(f_p_samples, axis=1, ddof=1)\n\n    # Calculate the half-width of the nominal 95% confidence interval for each batch.\n    # h_b = 1.96 * sqrt(s_b^2 / n)\n    # A small epsilon is not strictly necessary with standard floating point\n    # arithmetic as var >= 0, but good practice to consider. Here, it is omitted.\n    half_widths = 1.96 * np.sqrt(batch_variances / n)\n\n    # Determine for each batch if the true value is within the confidence interval.\n    # This is true if |sample_mean - true_value| <= half_width.\n    # This operation returns a boolean array of shape (B,).\n    is_covered = np.abs(batch_means - I_true) <= half_widths\n\n    # The empirical coverage is the mean of the boolean indicators (True=1, False=0).\n    coverage_fraction = np.mean(is_covered)\n\n    return coverage_fraction\n\ndef solve():\n    \"\"\"\n    Runs the simulation for all test cases and prints the results.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # Format: (p, n, B, seed)\n    test_cases = [\n        (-0.5, 500, 200, 17),    # Test 1\n        (-0.6, 500, 200, 19),    # Test 2\n        (0.5, 500, 200, 23),     # Test 3\n        (0.5, 5000, 200, 29),    # Test 4\n        (-0.5, 5000, 200, 31),   # Test 5\n        (0.5, 1, 200, 37),      # Test 6\n    ]\n\n    results = []\n    for p, n, B, seed in test_cases:\n        # Calculate the empirical coverage for the current test case.\n        result = compute_coverage(p, n, B, seed)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```", "id": "2411534"}]}