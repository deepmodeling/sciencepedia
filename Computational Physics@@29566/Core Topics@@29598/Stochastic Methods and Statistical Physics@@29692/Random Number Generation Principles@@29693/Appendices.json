{"hands_on_practices": [{"introduction": "A crucial, yet often overlooked, property of any pseudorandom number generator (PRNG) is its finite number of internal states. This inherent limitation means a PRNG can only produce a finite number of distinct output sequences. This first exercise [@problem_id:2433326] provides a hands-on exploration of this principle by investigating whether a simple Linear Congruential Generator (LCG) has enough states to generate all possible shuffles of a deck of cards, a task that requires producing one of $n!$ possible permutations.", "problem": "You will study when a simple pseudorandom number generator can, in principle, produce all permutations generated by a Fisher–Yates shuffle. Let a Pseudorandom Number Generator (PRNG) be a deterministic map with internal state that outputs a sequence of integers that aims to mimic independent samples. Consider the Linear Congruential Generator (LCG), defined by the recurrence\n$$\nX_{k+1} \\equiv (a X_k + c) \\pmod m,\n$$\nwhere $a$, $c$, and $m$ are integers with $m \\ge 2$, and $X_k \\in \\{0,1,\\dots,m-1\\}$. A shuffle of a list of length $n$ using the Fisher–Yates algorithm requires, for each $i$ from $n-1$ down to $1$, a uniform integer $J_i \\in \\{0,1,\\dots,i\\}$, followed by a swap of positions $i$ and $J_i$.\n\nUse the following principles as the base for your derivations and algorithmic design:\n- Deterministic state evolution: a PRNG with modulus $m$ has at most $m$ distinct internal states. Thus, scanning all possible seeds explores at most $m$ distinct output sequences.\n- Counting of permutations: there are $n!$ permutations on $n$ labeled items.\n- Exact uniform selection from a finite range using rejection sampling: if $X$ is uniform on $\\{0,1,\\dots,m-1\\}$, then for any positive integer $b$, the integer\n$$\nJ = X \\pmod b \\quad \\text{conditioned on } X  \\left\\lfloor \\frac{m}{b} \\right\\rfloor b\n$$\nis uniform on $\\{0,1,\\dots,b-1\\}$. This implements exact uniform draws in $\\{0,1,\\dots,b-1\\}$ while using only uniform draws in $\\{0,1,\\dots,m-1\\}$.\n- Hull–Dobell theorem for full period of a mixed LCG: an LCG with modulus $m$ has full period $m$ if and only if $\\gcd(c,m) = 1$, $a-1$ is divisible by every prime factor of $m$, and $a-1$ is divisible by $4$ whenever $m$ is divisible by $4$.\n\nTask. You must implement:\n1) An LCG as defined above.\n2) Fisher–Yates shuffling of the list $[0,1,\\dots,n-1]$ using exact uniform integer draws $J_i \\in \\{0,\\dots,i\\}$ obtained from the LCG states by rejection sampling as specified above. The $i$-th draw must depend only on the current LCG state and update the state deterministically via the LCG recurrence.\n3) For a given tuple $(n,a,c,m)$, enumerate all seeds $x_0 \\in \\{0,1,\\dots,m-1\\}$, run one Fisher–Yates shuffle per seed, and count the number of distinct permutations realized. Let this count be $R(n,a,c,m)$.\n4) For a given $(n,m)$, also compute the boolean value\n$$\nB(n,m) = \\begin{cases}\n1  \\text{if } m \\ge n! \\\\\n0  \\text{if } m  n!\n\\end{cases}\n$$\nwhich is a necessary (but not sufficient) condition for an LCG with modulus $m$ to be able to realize all $n!$ permutations when scanning all seeds.\n\nScientific goals. From first principles, justify and test the necessary condition $R(n,a,c,m) \\le \\min\\{m,n!\\}$ and explore empirically when $R(n,a,c,m)$ saturates this bound under exact uniform index selection.\n\nTest suite. Your program must compute $R(n,a,c,m)$ for the following four parameter sets and $B(n,m)$ for the $n=52$ case:\n- Test $1$: $(n,a,c,m) = (5,\\,109,\\,1,\\,256)$.\n- Test $2$: $(n,a,c,m) = (6,\\,109,\\,1,\\,256)$.\n- Test $3$: $(n,a,c,m) = (7,\\,421,\\,1,\\,5040)$.\n- Test $4$: $(n,a,c,m) = (7,\\,109,\\,1,\\,4096)$.\n- Test $5$: $B(52,2^{32})$.\n\nAll integers $n$, $a$, $c$, $m$ above are unitless by definition. No physical units or angles are involved.\n\nOutput format. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in this exact order:\n$$\n\\big[ R(5,109,1,256),\\; R(6,109,1,256),\\; R(7,421,1,5040),\\; R(7,109,1,4096),\\; B(52,2^{32}) \\big].\n$$\nEach entry must be an integer. No additional text should be printed. The computation must be entirely deterministic and require no user input or external files.", "solution": "The problem requires an analysis of the number of distinct permutations generated by a Fisher-Yates shuffle algorithm that draws its random indices from a Linear Congruential Generator (LCG). This involves implementing the specified algorithms and exploring a necessary condition for the generator to be capable of producing all possible permutations.\n\nFirst, we formalize the system. The LCG is a pseudorandom number generator defined by the recurrence relation:\n$$\nX_{k+1} \\equiv (a X_k + c) \\pmod m\n$$\nwhere $X_k$ is the state of the generator, and $a$, $c$, and $m$ are integer parameters. For a given initial state, or seed, $X_0 \\in \\{0, 1, \\dots, m-1\\}$, the LCG produces a deterministic sequence of integers. The number of possible seeds is $m$, so at most $m$ distinct output sequences can be generated.\n\nThe Fisher-Yates shuffle algorithm generates a random permutation of a list of $n$ elements. It proceeds by iterating from $i = n-1$ down to $1$. In each step, it selects an integer $J_i$ uniformly at random from the range $\\{0, 1, \\dots, i\\}$ and then swaps the elements at positions $i$ and $J_i$. To produce a single permutation, a sequence of $n-1$ random integers $\\{J_{n-1}, J_{n-2}, \\dots, J_1\\}$ is required.\n\nThe core of the problem is to couple the LCG with the Fisher-Yates shuffle. A naive approach of taking $J_i = X_k \\pmod{i+1}$ is flawed because it introduces bias if $m$ is not a multiple of $i+1$. The problem mandates using rejection sampling to obtain an exactly uniform integer. To draw a uniform integer $J_i$ from $\\{0, 1, \\dots, b-1\\}$ where $b = i+1$, we use the LCG to generate values $X \\in \\{0, 1, \\dots, m-1\\}$. We find the largest multiple of $b$ less than or equal to $m$, which is $T = \\lfloor \\frac{m}{b} \\rfloor \\cdot b$. A generated value $X$ is accepted if $X  T$; otherwise, it is rejected. Upon acceptance, the required index is $J_i = X \\pmod b$. For each value $X$ generated from the LCG, whether accepted or rejected, the LCG's internal state must be advanced according to its recurrence relation. This ensures that the entire process is deterministic for a given seed.\n\nThe combination of the LCG and the Fisher-Yates shuffle with rejection sampling defines a deterministic function $\\pi(X_0)$ that maps an initial seed $X_0$ to a specific permutation of $\\{0, 1, \\dots, n-1\\}$. We are tasked with computing $R(n, a, c, m)$, which is the number of distinct permutations generated as the seed $X_0$ is varied over all possible values $\\{0, 1, \\dots, m-1\\}$. This is the cardinality of the image of the function $\\pi$:\n$$\nR(n, a, c, m) = \\left| \\{ \\pi(X_0) \\mid X_0 \\in \\{0, 1, \\dots, m-1\\} \\} \\right|\n$$\nThere are two fundamental constraints on the value of $R(n,a,c,m)$. First, the total number of distinct permutations of $n$ items is $n!$. The set of generated permutations is a subset of all possible permutations, so $R(n,a,c,m) \\le n!$. Second, since there are only $m$ possible seeds, and each seed maps to a single permutation, the number of distinct outcomes cannot exceed $m$. This means $R(n,a,c,m) \\le m$. Combining these observations leads to the necessary condition:\n$$\nR(n,a,c,m) \\le \\min\\{m, n!\\}\n$$\nThis condition is a direct consequence of the pigeonhole principle. Saturation of this bound, i.e., $R(n,a,c,m) = \\min\\{m, n!\\}$, depends on the specific properties of the LCG, particularly its period and the statistical quality of its output sequence. The Hull-Dobell theorem indicates that for certain parameters, an LCG can have a full period of $m$, meaning it cycles through all $m$ states before repeating. This is a desirable property for a generator aiming to cover a large outcome space.\n\nThe function $B(n,m)$ evaluates the condition $m \\ge n!$. This is a simple but critical necessary condition. If an LCG has fewer internal states ($m$) than the total number of possible permutations ($n!$), it is impossible for it to generate all of them, regardless of its other properties. For $B(52, 2^{32})$, we must compare $52!$ with $2^{32}$. A quick calculation shows $13! \\approx 6.2 \\times 10^9$, which is already greater than $2^{32} \\approx 4.3 \\times 10^9$. Therefore, $52!$ is vastly larger than $2^{32}$, and $B(52, 2^{32}) = 0$.\n\nThe computational procedure to find $R(n, a, c, m)$ for the given test cases is as follows:\n1. Initialize an empty set, `permutations`, to store the unique permutations generated.\n2. Iterate through each possible seed $X_0$ from $0$ to $m-1$.\n3. For each seed, create an LCG instance.\n4. Perform a Fisher-Yates shuffle on the list $[0, 1, \\dots, n-1]$. For each step $i$ from $n-1$ down to $1$:\n    a. Use the LCG and rejection sampling to draw a uniform index $J_i \\in \\{0, 1, \\dots, i\\}$.\n    b. Swap the elements at positions $i$ and $J_i$.\n5. Convert the resulting list to an immutable tuple and add it to the `permutations` set.\n6. The value of $R(n, a, c, m)$ is the final size of the set, `len(permutations)`.\n\nThis procedure is implemented to calculate the required values for the test suite.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nimport math\n\n# No other libraries are permitted aside from the standard library.\n\nclass LCG:\n    \"\"\"\n    Implements a Linear Congruential Generator (LCG).\n    X_{k+1} = (a * X_k + c) mod m\n    \"\"\"\n    def __init__(self, a, c, m, seed):\n        self.a = np.uint64(a)\n        self.c = np.uint64(c)\n        self.m = np.uint64(m)\n        self.state = np.uint64(seed)\n\n    def next(self):\n        \"\"\"Generates the next number in the sequence.\"\"\"\n        self.state = (self.a * self.state + self.c) % self.m\n        return self.state\n\ndef draw_uniform(lcg_instance, b):\n    \"\"\"\n    Draws a uniform integer from {0, 1, ..., b-1} using the LCG\n    with rejection sampling.\n    \"\"\"\n    if b = 1:\n        return 0\n    \n    m = lcg_instance.m\n    threshold = (m // b) * b\n    \n    while True:\n        x = lcg_instance.next()\n        if x  threshold:\n            return int(x % b)\n\ndef fisher_yates_shuffle(n, lcg_instance):\n    \"\"\"\n    Performs a Fisher-Yates shuffle on the list [0, ..., n-1]\n    using the provided LCG instance for random number generation.\n    Returns the shuffled list as a tuple.\n    \"\"\"\n    arr = list(range(n))\n    for i in range(n - 1, 0, -1):\n        # Draw a uniform integer j from {0, ..., i}\n        # The range has size a_i = i + 1\n        j = draw_uniform(lcg_instance, i + 1)\n        arr[i], arr[j] = arr[j], arr[i]\n    return tuple(arr)\n\ndef compute_R(n, a, c, m):\n    \"\"\"\n    Enumerates all seeds for an LCG, performs a Fisher-Yates shuffle for each,\n    and counts the number of distinct permutations realized.\n    \"\"\"\n    permutations = set()\n    for seed in range(m):\n        lcg = LCG(a, c, m, seed)\n        perm = fisher_yates_shuffle(n, lcg)\n        permutations.add(perm)\n    return len(permutations)\n\ndef compute_B(n, m):\n    \"\"\"\n    Computes B(n, m) = 1 if m >= n!, 0 otherwise.\n    \"\"\"\n    if n > 20: # math.factorial(21) already exceeds 64-bit int max\n        # For large n, we must check if n! exceeds m.\n        # 13! > 2^32, so 52! is vastly larger than 2^32.\n        # Direct comparison is safe due to Python's arbitrary-precision integers.\n        return 1 if m >= math.factorial(n) else 0\n    \n    # For smaller n, this is safe and direct.\n    n_factorial = math.factorial(n)\n    return 1 if m >= n_factorial else 0\n\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (n, a, c, m)\n        (5, 109, 1, 256),\n        (6, 109, 1, 256),\n        (7, 421, 1, 5040),\n        (7, 109, 1, 4096),\n    ]\n\n    results = []\n    \n    # Run Test 1\n    n, a, c, m = test_cases[0]\n    results.append(compute_R(n, a, c, m))\n\n    # Run Test 2\n    n, a, c, m = test_cases[1]\n    results.append(compute_R(n, a, c, m))\n\n    # Run Test 3\n    n, a, c, m = test_cases[2]\n    results.append(compute_R(n, a, c, m))\n    \n    # Run Test 4\n    n, a, c, m = test_cases[3]\n    results.append(compute_R(n, a, c, m))\n\n    # Run Test 5\n    n_b, m_b = (52, 2**32)\n    results.append(compute_B(n_b, m_b))\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```", "id": "2433326"}, {"introduction": "Even with a perfect source of random numbers, a flawed algorithm can lead to biased and incorrect results. This practice [@problem_id:2433283] focuses on this critical link between the random generator and its application by implementing and testing different list-shuffling algorithms. You will compare the gold-standard Fisher–Yates shuffle against intentionally biased variants, using a chi-squared goodness-of-fit test to quantitatively measure how subtle algorithmic errors can destroy uniformity.", "problem": "You are asked to design and implement a program that probes the principle that an unbiased random permutation should make every item equally likely to appear in any position. Your focus is the first position. This exercise ties directly to Monte Carlo methodology in computational physics, where the fidelity of a pseudorandom number generator (PRNG) determines the validity of stochastic simulation results.\n\nStarting point and definitions:\n- A pseudorandom number generator (PRNG) produces a deterministic sequence of numbers that approximates samples from prescribed probability distributions when initialized with a seed.\n- A permutation of $N$ distinct labels is a bijection on the set $\\{0,1,\\dots,N-1\\}$. A permutation is uniform if each of the $N!$ possible permutations has probability $1/N!$.\n- If permutations are uniform, then for any fixed position (in particular, the first position), each label must appear with probability $1/N$.\n- To test whether empirical counts are consistent with a hypothesized discrete uniform distribution, one may use the classical chi-squared goodness-of-fit test: form the chi-squared statistic from observed counts and expected counts, and compare to the chi-squared distribution with appropriate degrees of freedom to obtain a $p$-value. The null hypothesis is rejected for sufficiently small $p$-values at a preselected significance level $\\alpha$.\n\nYour tasks:\n1) Implement three permutation generators on the list of labels $[0,1,\\dots,N-1]$:\n   - Method U (unbiased Fisher–Yates): Iterate an index $i$ from $N-1$ down to $1$. For each $i$, draw an integer $j$ uniformly from $\\{0,1,\\dots,i\\}$ and swap the items at indices $i$ and $j$. This is known to produce uniform permutations.\n   - Method P$(c)$ (partial Fisher–Yates): Define $m=\\lfloor c\\,N\\rfloor$ with $0c1$. Perform only the last $m$ swaps of Method U, that is, for $k=0,1,\\dots,m-1$ set $i=N-1-k$, draw $j$ uniformly from $\\{0,1,\\dots,i\\}$, and swap indices $i$ and $j$. This intentionally leaves the leading segment under-randomized.\n   - Method M$(b)$ (biased modulo-first-pick): Draw a single integer $x$ uniformly from $\\{0,1,\\dots,2^b-1\\}$ and set $f = x \\bmod N$. Swap indices $0$ and $f$. Then complete positions $1$ through $N-1$ by applying the Fisher–Yates procedure restricted to indices $i=N-1$ down to $1$, drawing $j$ uniformly from $\\{1,2,\\dots,i\\}$ and swapping indices $i$ and $j$. This forces the first element selection to come from a discrete pool of size $2^b$, which generally biases the first-position choice unless $N$ divides $2^b$.\n\n2) For a given method, integer $N \\ge 2$, trial count $T$, significance level $\\alpha$ (expressed as a decimal), and seed $s$, perform the following experiment:\n   - Initialize the PRNG with the integer seed $s$.\n   - Repeat the following $T$ times independently:\n     - Generate one permutation of $[0,1,\\dots,N-1]$ with the selected method.\n     - Record the label occupying index $0$ by incrementing its count.\n   - Let $\\mathbf{O}=(O_0,\\dots,O_{N-1})$ be the observed counts and $\\mathbf{E}=(E,\\dots,E)$ with $E=T/N$ the expected counts under the null hypothesis $H_0$ that the first-position label distribution is uniform on $\\{0,1,\\dots,N-1\\}$.\n   - Compute the chi-squared test statistic based on $\\mathbf{O}$ and $\\mathbf{E}$ and the corresponding $p$-value using the chi-squared distribution with $N-1$ degrees of freedom.\n   - Report the rounded $p$-value (to six decimal places) and an indicator $d$ that equals $1$ if the null hypothesis is rejected at level $\\alpha$ (that is, if $p\\alpha$), and equals $0$ otherwise.\n\n3) Implement your solution as a complete, runnable program that uses no input and prints results for the following test suite, where each case is of the form $(\\text{method}, N, T, \\alpha, s, \\text{parameter})$ and “parameter” is interpreted according to the method as described above:\n   - Case 1 (happy path, unbiased): $(\\text{\"U\"},\\, 10,\\, 100000,\\, 0.01,\\, 10101,\\, \\text{None})$.\n   - Case 2 (edge under-randomization near the front): $(\\text{\"P\"},\\, 20,\\, 80000,\\, 1\\times 10^{-12},\\, 20202,\\, c=0.2)$.\n   - Case 3 (explicit modulo bias in first pick): $(\\text{\"M\"},\\, 10,\\, 120000,\\, 1\\times 10^{-6},\\, 30303,\\, b=8)$.\n   - Case 4 (small $N$ boundary, unbiased): $(\\text{\"U\"},\\, 2,\\, 60000,\\, 0.01,\\, 40404,\\, \\text{None})$.\n   - Case 5 (strong modulo bias with small keyspace): $(\\text{\"M\"},\\, 7,\\, 120000,\\, 1\\times 10^{-6},\\, 50505,\\, b=3)$.\n\nOutput format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each element must itself be a two-element list of the form $[p, d]$, where $p$ is the $p$-value rounded to six decimal places and $d$ is the integer decision indicator defined above. For example, a valid output line looks like:\n  [[0.532114,0],[0.000000,1],[0.123456,0],[0.774321,0],[0.000000,1]]\n\nNotes:\n- There are no physical units involved, so no unit conversions are required.\n- Angles are not involved.\n- Percentages, where present (such as the significance level), are already specified as decimals and must be treated as such.", "solution": "The problem requires an empirical validation of three permutation algorithms by testing if they produce a uniform distribution of labels in the first position of a permuted array. This is a fundamental check in computational physics and Monte Carlo methods, where the quality of pseudorandomness is paramount for the validity of simulation results. The validation will be conducted using the chi-squared goodness-of-fit test.\n\nThe overall procedure for each test case involves a fixed number of trials, $T$. In each trial, an initial array of labels, $[0, 1, \\dots, N-1]$, is permuted using one of the specified methods. The label that ends up at index $0$ is recorded. After $T$ trials, we obtain a frequency distribution of observed counts $\\mathbf{O} = (O_0, O_1, \\dots, O_{N-1})$ for each label at the first position.\n\nThe null hypothesis, $H_0$, states that the permutation method is unbiased with respect to the first position, meaning every label $k \\in \\{0, \\dots, N-1\\}$ has a probability of $1/N$ of appearing there. Under $H_0$, the expected count for each label is $E_k = T/N$. The chi-squared test statistic, $\\chi^2$, is computed as:\n$$ \\chi^2 = \\sum_{k=0}^{N-1} \\frac{(O_k - E_k)^2}{E_k} $$\nThis statistic is compared against the $\\chi^2$ distribution with $df = N-1$ degrees of freedom. The $p$-value, representing the probability of observing a statistic at least as extreme as the one computed if $H_0$ were true, is determined from the survival function of this distribution: $p = P(\\chi^2_{df} \\ge \\chi^2_{\\text{observed}})$. At a given significance level $\\alpha$, $H_0$ is rejected if $p  \\alpha$.\n\nBelow is the analysis of each permutation method. All algorithms are implemented using a pseudorandom number generator (PRNG) initialized with a specific seed $s$ for reproducibility.\n\nMethod U (unbiased Fisher–Yates): This algorithm iterates an index $i$ from $N-1$ down to $1$. In each step, it selects a random index $j$ uniformly from $\\{0, 1, \\dots, i\\}$ and swaps the elements at positions $i$ and $j$. This canonical algorithm is proven to generate every possible permutation of $N$ elements with equal probability $1/N!$. Consequently, the probability of any given label appearing at any given position is exactly $1/N$. This method serves as our baseline for an unbiased permutation, and we anticipate that the statistical test will not reject the null hypothesis (i.e., will yield a large $p$-value).\n\nMethod P($c$) (partial Fisher–Yates): This method is designed to be biased. It executes only a fraction of the swaps of a full Fisher-Yates shuffle. Given a parameter $c \\in (0, 1)$, it performs $m = \\lfloor cN \\rfloor$ swaps. The problem's description \"for $k=0,1,\\dots,m-1$ set $i=N-1-k$\" dictates that these are the swaps for the largest indices, $i=N-1, N-2, \\dots, N-m$. This leaves the prefix of the array, corresponding to small indices, under-randomized. A label starting in the prefix is less likely to be moved. Specifically, the label $0$ (initially at index $0$) remains at its position unless it is selected as the index $j$ in one of the swaps. The probability of it *not* being chosen is $\\prod_{i=N-m}^{N-1} (1 - \\frac{1}{i+1}) = \\frac{N-m}{N} = 1-c$. This creates a significant bias: the label $0$ is far more likely to appear at index $0$ than any other label. We expect the test to strongly reject $H_0$ with a $p$-value close to $0$.\n\nMethod M($b$) (biased modulo-first-pick): This method introduces a bias in the very first step. It selects the element for the first position from a non-uniform distribution. An integer $x$ is drawn uniformly from $\\{0, 1, \\dots, 2^b-1\\}$, and an index $f = x \\pmod N$ is computed. The element at index $f$ (which is the label $f$) is swapped into index $0$. The subsequent shuffling step, as described, shuffles the elements at indices $1$ to $N-1$ among themselves, leaving the element at index $0$ untouched. Therefore, the distribution of the final label at index $0$ is precisely the distribution of $f$. This distribution is uniform only if $N$ divides $2^b$. If not, let $2^b = qN+r$. Labels $\\{0, \\dots, r-1\\}$ are chosen with probability $(q+1)/2^b$, while labels $\\{r, \\dots, N-1\\}$ are chosen with probability $q/2^b$. This discrepancy, however small, introduces a bias that should be detectable with a sufficiently large number of trials $T$. For the given test cases, $N$ does not divide $2^b$, so we expect to reject $H_0$. The magnitude of the bias, and thus the resulting $p$-value, depends on how much the probabilities deviate from $1/N$.", "answer": "```python\nimport numpy as np\nfrom scipy.stats import chi2\n\ndef permute_U(arr, rng):\n    \"\"\"\n    Generates a uniform permutation using the Fisher-Yates shuffle.\n    \"\"\"\n    n = len(arr)\n    for i in range(n - 1, 0, -1):\n        j = rng.integers(0, i, endpoint=True)\n        arr[i], arr[j] = arr[j], arr[i]\n    return arr\n\ndef permute_P(arr, rng, c):\n    \"\"\"\n    Generates a biased permutation using a partial Fisher-Yates shuffle.\n    \"\"\"\n    n = len(arr)\n    m = int(c * n)\n    for k in range(m):\n        i = n - 1 - k\n        # The swap is meaningful only for i  0\n        if i > 0:\n            j = rng.integers(0, i, endpoint=True)\n            arr[i], arr[j] = arr[j], arr[i]\n    return arr\n\ndef permute_M(arr, rng, b):\n    \"\"\"\n    Generates a biased permutation using a modulo-biased first pick.\n    \"\"\"\n    n = len(arr)\n    # Draw x uniformly from {0, ..., 2^b - 1}\n    keyspace = 2**b\n    x = rng.integers(0, keyspace - 1, endpoint=True)\n    f = x % n\n    \n    # Swap element at index f into the first position\n    arr[0], arr[f] = arr[f], arr[0]\n    \n    # Shuffle the rest of the array (indices 1..N-1) among themselves\n    # The problem specifies j is drawn from {1, ..., i}\n    for i in range(n - 1, 0, -1):\n        j = rng.integers(1, i, endpoint=True)\n        arr[i], arr[j] = arr[j], arr[i]\n    return arr\n\ndef run_experiment(method, N, T, s, param):\n    \"\"\"\n    Runs the simulation to collect counts of labels at the first position.\n    \"\"\"\n    rng = np.random.default_rng(seed=s)\n    counts = np.zeros(N, dtype=np.int64)\n    \n    permute_func = None\n    if method == \"U\":\n        permute_func = lambda arr: permute_U(arr, rng)\n    elif method == \"P\":\n        permute_func = lambda arr: permute_P(arr, rng, c=param)\n    elif method == \"M\":\n        permute_func = lambda arr: permute_M(arr, rng, b=param)\n    else:\n        raise ValueError(f\"Unknown permutation method: {method}\")\n\n    for _ in range(T):\n        p = list(range(N))\n        permuted_p = permute_func(p)\n        first_element_label = permuted_p[0]\n        counts[first_element_label] += 1\n        \n    return counts\n\ndef perform_chi2_test(observed_counts, N, T, alpha):\n    \"\"\"\n    Performs the chi-squared goodness-of-fit test.\n    \"\"\"\n    # Under H0, expected count for each category is uniform.\n    expected_count = T / N\n    \n    # Calculate the chi-squared statistic\n    chisq_stat = np.sum((observed_counts - expected_count)**2 / expected_count)\n    \n    # Degrees of freedom for a GOF test is k-1.\n    df = N - 1\n    \n    # Calculate p-value using the survival function (1 - CDF).\n    p_value = chi2.sf(chisq_stat, df)\n    \n    # Decision: reject H0 if p  alpha\n    decision = 1 if p_value  alpha else 0\n    \n    return p_value, decision\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    test_cases = [\n        # (method, N, T, alpha, seed, parameter)\n        (\"U\", 10, 100000, 0.01, 10101, None),\n        (\"P\", 20, 80000, 1e-12, 20202, 0.2),\n        (\"M\", 10, 120000, 1e-6, 30303, 8),\n        (\"U\", 2, 60000, 0.01, 40404, None),\n        (\"M\", 7, 120000, 1e-6, 50505, 3),\n    ]\n\n    results = []\n    for case in test_cases:\n        method, N, T, alpha, seed, param = case\n        \n        # 1. Run simulation to get observed counts\n        observed_counts = run_experiment(method, N, T, seed, param)\n        \n        # 2. Perform chi-squared test to get p-value and decision\n        p_value, decision = perform_chi2_test(observed_counts, N, T, alpha)\n        \n        results.append((p_value, decision))\n    \n    # 3. Format output string precisely as required\n    result_strings = [f\"[{p:.6f},{d}]\" for p, d in results]\n    print(f\"[{','.join(result_strings)}]\")\n\nsolve()\n```", "id": "2433283"}, {"introduction": "Many problems in computational physics, from simulating molecular orientations to modeling isotropic radiation, require generating points distributed uniformly on the surface of a sphere. A naive approach of sampling spherical coordinates uniformly in $\\theta$ and $\\phi$ fails, so a more sophisticated method is required. This final exercise [@problem_id:2433291] guides you through implementing the correct sampling technique and then rigorously validating the uniformity of the resulting distribution using a two-dimensional chi-squared test.", "problem": "You are to write a complete, runnable program that generates points uniformly distributed on the surface of the unit sphere in three-dimensional space and assesses their uniformity using a two-dimensional histogram over spherical coordinates. All angles must be in radians. The surface of the unit sphere is parameterized by spherical coordinates with polar angle $\\theta \\in [0,\\pi]$ and azimuthal angle $\\phi \\in [0,2\\pi)$. The uniform surface measure has surface area element $\\mathrm{d}A = \\sin\\theta\\,\\mathrm{d}\\theta\\,\\mathrm{d}\\phi$ and total surface area $4\\pi$. For a given set of generated points, construct a two-dimensional histogram by partitioning $\\theta$ into $B_\\theta$ equal-width bins and $\\phi$ into $B_\\phi$ equal-width bins. The $\\theta$-bin edges are $\\theta_i = i\\,\\Delta\\theta$ with $\\Delta\\theta = \\pi/B_\\theta$ for $i=0,1,\\dots,B_\\theta$, and the $\\phi$-bin edges are $\\phi_j = j\\,\\Delta\\phi$ with $\\Delta\\phi = 2\\pi/B_\\phi$ for $j=0,1,\\dots,B_\\phi$. Use half-open intervals $[\\theta_i,\\theta_{i+1})$ and $[\\phi_j,\\phi_{j+1})$ for all bins except that the final bin in each dimension is closed on the right to include the endpoint $\\theta=\\pi$ and $\\phi=2\\pi$.\n\nFor testing uniformity, let $O_{i,j}$ denote the observed count in bin $(i,j)$, and let $E_{i,j}$ denote the expected count under the uniform surface measure, obtained from first principles by integrating the surface area element over the bin region and normalizing by the total surface area. That is, the expected probability for bin $(i,j)$ is the ratio of the bin’s surface area to $4\\pi$, and $E_{i,j}$ equals this probability times the total number of generated points $N$. Use the Pearson chi-squared statistic\n$$\n\\chi^2 = \\sum_{i=0}^{B_\\theta-1}\\sum_{j=0}^{B_\\phi-1} \\frac{\\left(O_{i,j}-E_{i,j}\\right)^2}{E_{i,j}},\n$$\nsumming only over bins with $E_{i,j} > 0$. The degrees of freedom are $k-1$, where $k$ is the number of bins with $E_{i,j} > 0$. For a specified significance level $\\alpha \\in (0,1)$, accept the null hypothesis of uniform surface distribution if $\\chi^2 \\le q$, where $q$ is the $(1-\\alpha)$-quantile of the chi-squared distribution with $k-1$ degrees of freedom.\n\nYour program must, for each test case, generate exactly $N$ points on the unit sphere with a pseudo-random number generator initialized by the given integer seed, compute $\\theta$ and $\\phi$ for each point with the convention $\\phi \\in [0,2\\pi)$, build the two-dimensional histogram as specified, compute $E_{i,j}$ from first principles as described, evaluate $\\chi^2$, determine the critical value $q$ for the given $\\alpha$, and report a boolean indicating whether $\\chi^2 \\le q$.\n\nAngle unit requirement: Report and compute all angles in radians.\n\nTest suite specification:\n- Case $1$: seed $= 12345$, $N = 40000$, $B_\\theta = 10$, $B_\\phi = 20$, $\\alpha = 0.01$.\n- Case $2$: seed $= 54321$, $N = 1280$, $B_\\theta = 8$, $B_\\phi = 16$, $\\alpha = 0.05$.\n- Case $3$: seed $= 20231102$, $N = 50000$, $B_\\theta = 2$, $B_\\phi = 3$, $\\alpha = 0.01$.\n- Case $4$: seed $= 777$, $N = 100000$, $B_\\theta = 18$, $B_\\phi = 36$, $\\alpha = 0.05$.\n\nRequired final output format: Your program should produce a single line of output containing the boolean results for the test cases, in order, as a comma-separated list enclosed in square brackets, with no spaces. For example, an output with four booleans must look like\n[True,False,True,True].", "solution": "The problem requires generating points uniformly on the surface of a sphere and then performing a chi-squared goodness-of-fit test to validate their uniformity. This involves three key steps: correct point generation, calculation of expected counts, and statistical comparison.\n\n**1. Point Generation via Inverse Transform Sampling**\nA common mistake is to sample the spherical coordinates $\\phi$ and $\\theta$ uniformly from their respective ranges, $[0, 2\\pi)$ and $[0, \\pi]$. This method incorrectly concentrates points near the poles because it neglects the surface area element, $\\mathrm{d}A = \\sin\\theta\\,\\mathrm{d}\\theta\\,\\mathrm{d}\\phi$. The correct approach is to generate random variates according to the joint probability density function $p(\\theta, \\phi) = \\frac{\\sin\\theta}{4\\pi}$. This distribution can be sampled using the inverse transform method. By integrating the joint PDF, we find that $\\phi$ is uniformly distributed on $[0, 2\\pi)$, while the cosine of the polar angle, $\\cos\\theta$, is uniformly distributed on $[-1, 1]$. Therefore, we can generate two independent uniform random numbers, $U_1, U_2 \\in [0, 1)$, and transform them as follows:\n$$ \\phi = 2\\pi U_1 $$\n$$ \\cos\\theta = 2U_2 - 1 \\implies \\theta = \\arccos(2U_2 - 1) $$\nThis method ensures that every region on the sphere's surface has an equal probability of receiving a point, consistent with a true uniform distribution.\n\n**2. Observed and Expected Counts**\nAfter generating $N$ points, we categorize them into a $B_\\theta \\times B_\\phi$ two-dimensional histogram to get the observed counts, $O_{i,j}$. The expected counts, $E_{i,j}$, are derived from first principles. The probability of a point falling into a specific bin $(\\theta_i, \\phi_j)$ is the ratio of that bin's surface area to the total surface area of the sphere, $4\\pi$. The area of a bin is found by integrating the surface area element over the bin's boundaries:\n$$ A_{i,j} = \\int_{\\phi_j}^{\\phi_{j+1}} \\int_{\\theta_i}^{\\theta_{i+1}} \\sin\\theta\\,\\mathrm{d}\\theta\\,\\mathrm{d}\\phi = (\\phi_{j+1} - \\phi_j) [-\\cos\\theta]_{\\theta_i}^{\\theta_{i+1}} $$\nWith bin widths $\\Delta\\phi = 2\\pi/B_\\phi$ and $\\Delta\\theta = \\pi/B_\\theta$, and edges $\\theta_i = i\\Delta\\theta$, the expected count for bin $(i,j)$ is:\n$$ E_{i,j} = N \\cdot \\frac{A_{i,j}}{4\\pi} = N \\cdot \\frac{\\Delta\\phi (\\cos\\theta_i - \\cos\\theta_{i+1})}{4\\pi} = \\frac{N}{2B_\\phi}(\\cos(i\\pi/B_\\theta) - \\cos((i+1)\\pi/B_\\theta)) $$\nNotably, the expected count is independent of the azimuthal bin index $j$. All $E_{i,j}$ are positive since $\\cos(\\theta)$ is a strictly decreasing function on $[0, \\pi]$.\n\n**3. Chi-Squared Test**\nFinally, we use the Pearson's chi-squared test to compare the observed counts $O_{i,j}$ to the expected counts $E_{i,j}$. The test statistic is:\n$$ \\chi^2 = \\sum_{i=0}^{B_\\theta-1} \\sum_{j=0}^{B_\\phi-1} \\frac{(O_{i,j}-E_{i,j})^2}{E_{i,j}} $$\nThis statistic follows a chi-squared distribution with $k-1$ degrees of freedom, where $k = B_\\theta B_\\phi$ is the total number of bins. We compare the calculated $\\chi^2$ value to a critical value $q$, which is the $(1-\\alpha)$-quantile of the $\\chi^2_{k-1}$ distribution for a given significance level $\\alpha$. If $\\chi^2 \\le q$, the null hypothesis (that the points are uniformly distributed) is not rejected.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import chi2\n\ndef run_case(seed, N, B_theta, B_phi, alpha):\n    \"\"\"\n    Generates points on a unit sphere, performs a chi-squared test for uniformity,\n    and returns the result of the hypothesis test.\n\n    Args:\n        seed (int): The seed for the random number generator.\n        N (int): The total number of points to generate.\n        B_theta (int): The number of bins for the polar angle theta.\n        B_phi (int): The number of bins for the azimuthal angle phi.\n        alpha (float): The significance level for the chi-squared test.\n\n    Returns:\n        bool: True if the null hypothesis of uniformity is accepted, False otherwise.\n    \"\"\"\n    # 1. Generate points using inverse transform sampling\n    # This method correctly generates points uniformly distributed on the sphere's surface.\n    rng = np.random.default_rng(seed)\n    # Generate N random numbers from a uniform distribution on [0, 1)\n    u1 = rng.random(size=N)\n    u2 = rng.random(size=N)\n\n    # Transform uniform variates to spherical coordinates (theta, phi)\n    # phi is uniform in [0, 2*pi)\n    phi = 2 * np.pi * u1\n    # cos(theta) is uniform in [-1, 1]\n    cos_theta = 2 * u2 - 1\n    # theta is in [0, pi]\n    theta = np.arccos(cos_theta)\n    \n    # 2. Compute observed counts O_ij using a 2D histogram\n    theta_edges = np.linspace(0, np.pi, B_theta + 1)\n    phi_edges = np.linspace(0, 2 * np.pi, B_phi + 1)\n    \n    # np.histogram2d bins the data. The returned histogram H[i, j] corresponds\n    # to theta_edges[i] = theta  theta_edges[i+1] and phi_edges[j] = phi  phi_edges[j+1].\n    # The first argument 'x' is theta, second 'y' is phi.\n    # The resulting O_ij matrix has shape (B_theta, B_phi).\n    O_ij, _, _ = np.histogram2d(theta, phi, bins=[theta_edges, phi_edges])\n    \n    # 3. Compute expected counts E_ij from first principles\n    # E_ij is independent of j, so we first compute a 1D array for E_i.\n    i_indices = np.arange(B_theta)\n    delta_theta = np.pi / B_theta\n    theta_i = i_indices * delta_theta\n    theta_i_plus_1 = (i_indices + 1) * delta_theta\n\n    # Analytical formula for expected count in a bin\n    cos_diff = np.cos(theta_i) - np.cos(theta_i_plus_1)\n    E_i_base = (N / (2 * B_phi)) * cos_diff\n    \n    # Expand to a 2D matrix of shape (B_theta, B_phi) to match O_ij\n    E_ij = np.tile(E_i_base, (B_phi, 1)).T\n\n    # 4. Perform the Pearson's chi-squared test\n    # All E_ij are > 0 as cos(x) is strictly decreasing on [0, pi].\n    # The sum is over all bins.\n    squared_diff = (O_ij - E_ij)**2\n    chi_squared_stat = np.sum(squared_diff / E_ij)\n    \n    # Degrees of freedom: k - 1, where k is the number of bins\n    k = B_theta * B_phi\n    dof = k - 1\n    \n    # Critical value q is the (1-alpha) quantile of the chi-squared distribution\n    q = chi2.ppf(1 - alpha, df=dof)\n    \n    # Decision: Accept H0 if chi_squared_stat is not in the critical region\n    return chi_squared_stat = q\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the results in the required format.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (seed, N, B_theta, B_phi, alpha)\n        (12345, 40000, 10, 20, 0.01),\n        (54321, 1280, 8, 16, 0.05),\n        (20231102, 50000, 2, 3, 0.01),\n        (777, 100000, 18, 36, 0.05),\n    ]\n\n    results = []\n    for case in test_cases:\n        seed, N, B_theta, B_phi, alpha = case\n        result = run_case(seed, N, B_theta, B_phi, alpha)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    # e.g., \"[True,False,True,True]\"\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2433291"}]}