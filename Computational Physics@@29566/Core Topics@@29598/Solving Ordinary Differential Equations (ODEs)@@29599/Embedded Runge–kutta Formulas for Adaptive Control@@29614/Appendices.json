{"hands_on_practices": [{"introduction": "The power of an embedded Runge-Kutta method lies in its ability to estimate the local error at each step, but what happens if this estimate is flawed? This first practice [@problem_id:2388676] takes you under the hood to investigate this very question. By intentionally using incorrect coefficients for the lower-order method, you will directly observe how a faulty local error estimate compromises the solver's ability to control the global error, providing a crucial insight into why the precise formulation of these methods is so important for their reliability.", "problem": "Design and implement an adaptive time step integrator based on an embedded Runge–Kutta pair to study how an incorrect local error estimate affects the global error. Work purely in the mathematical setting of an initial value problem for an ordinary differential equation. The fundamental base you must use is the definition of an initial value problem, local truncation error, and the structure of explicit Runge–Kutta methods.\n\nProblem requirements:\n- Consider the initial value problem for a scalar ordinary differential equation given by $y'(t) = f(t,y(t))$ with $y(t_{0}) = y_{0}$. The goal is to advance from $t_{0}$ to a final time $T$.\n- Use an explicit embedded Runge–Kutta pair, where two formulas of different orders share the same internal stages to provide two approximations $y_{n+1}$ and $\\hat{y}_{n+1}$ at each step. The higher-order approximation $y_{n+1}$ is used as the step result, and the difference $e_{n+1} = y_{n+1} - \\hat{y}_{n+1}$ is used as a local error estimate for step-size control.\n- Implement a specific, widely used pair of orders $p$ and $q$ with $p > q$; for concreteness and reproducibility, use the Bogacki–Shampine pair of orders $p=q+1$, namely $p = 3$ and $q = 2$. Use the higher-order approximation to advance the solution when a step is accepted.\n- Implement two versions of the adaptive controller:\n  1. The correct controller computes the local error estimate $e_{n+1}$ using the correct embedded coefficients for the lower-order formula.\n  2. The flawed controller computes a defective local error estimate $\\hat{e}_{n+1}$ by intentionally altering the embedded lower-order weights as follows: swap the last two weights of the lower-order rule before forming the difference. In other words, if the lower-order weights are $\\{b_{1}^{(q)}, b_{2}^{(q)}, b_{3}^{(q)}, b_{4}^{(q)}\\}$, the flawed controller uses $\\{b_{1}^{(q)}, b_{2}^{(q)}, b_{4}^{(q)}, b_{3}^{(q)}\\}$ when constructing $\\hat{y}_{n+1}$, hence $\\hat{e}_{n+1} = y_{n+1} - \\hat{y}_{n+1}$. Use this flawed estimate only for step acceptance and step-size selection, while still advancing the solution with the higher-order approximation $y_{n+1}$ upon acceptance.\n- Use a standard error norm for a scalar state:\n  $$\\mathrm{err\\_norm} = \\frac{|e_{n+1}|}{\\mathrm{atol} + \\mathrm{rtol} \\cdot \\max(|y_{n}|, |y_{n+1}|)}.$$\n  Accept a step if $\\mathrm{err\\_norm} \\le 1$.\n- Use a step-size controller derived from the scaling of local truncation error for an order-$p$ method. If a step is accepted or rejected, propose a new step size\n  $$h_{\\mathrm{new}} = h \\cdot s \\cdot \\mathrm{err\\_norm}^{-1/(p+1)},$$\n  where $s$ is a safety factor. Clip $h_{\\mathrm{new}}$ to lie within $[\\alpha_{\\min} h, \\alpha_{\\max} h]$ to avoid erratic changes. Use $p = 3$, $s = 0.9$, $\\alpha_{\\min} = 0.2$, and $\\alpha_{\\max} = 5$.\n- Use the high-order formula of the Bogacki–Shampine pair to advance the solution on accepted steps. Use the same initial step size and controller parameters for both controllers.\n\nTest problem and exact solution:\n- Let $f(t,y) = \\lambda y$ with $\\lambda  0$, $y(0) = 1$, and exact solution $y(t) = \\exp(\\lambda t)$.\n- Define the global error at the final time $T$ as\n  $$E = |y_{\\mathrm{num}}(T) - y_{\\mathrm{exact}}(T)|.$$\n\nTest suite:\nProvide results for the following parameter sets, which form the test suite (all numbers below are dimensionless):\n- Case A (happy path): $\\lambda = -1$, $T = 10$, $y_{0} = 1$, $\\mathrm{rtol} = 10^{-3}$, $\\mathrm{atol} = 10^{-12}$.\n- Case B (tighter tolerance): $\\lambda = -1$, $T = 10$, $y_{0} = 1$, $\\mathrm{rtol} = 10^{-6}$, $\\mathrm{atol} = 10^{-12}$.\n- Case C (more rapidly decaying dynamics): $\\lambda = -5$, $T = 2$, $y_{0} = 1$, $\\mathrm{rtol} = 10^{-6}$, $\\mathrm{atol} = 10^{-12}$.\n\nWhat to compute for each case:\n- Compute two floats:\n  - $E_{\\mathrm{correct}}$: the final-time absolute error using the correct controller.\n  - $E_{\\mathrm{flawed}}$: the final-time absolute error using the flawed controller.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to a test case and is itself a two-element list $[E_{\\mathrm{correct}}, E_{\\mathrm{flawed}}]$. For example: [[EcA,EfA],[EcB,EfB],[EcC,EfC]]. Print numeric values as decimals in a consistent, compact format.", "solution": "The supplied problem is subjected to rigorous validation.\n\n**Step 1: Extract Givens**\n- **Initial Value Problem (IVP)**: A scalar ordinary differential equation (ODE) $y'(t) = f(t,y(t))$ with initial condition $y(t_{0}) = y_{0}$ is to be integrated from $t_{0}$ to a final time $T$.\n- **Numerical Method**: An explicit embedded Runge–Kutta pair of orders $p=3$ and $q=2$, specifically the Bogacki–Shampine pair. The higher-order ($p=3$) approximation, $y_{n+1}$, is used to advance the solution.\n- **Local Error Estimate**: The difference $e_{n+1} = y_{n+1} - \\hat{y}_{n+1}$ between the higher-order ($y_{n+1}$) and lower-order ($\\hat{y}_{n+1}$) approximations serves as the estimate of local truncation error.\n- **Controllers**:\n    1.  **Correct Controller**: Uses the standard local error estimate $e_{n+1}$.\n    2.  **Flawed Controller**: Uses a defective estimate $\\hat{e}_{n+1} = y_{n+1} - \\hat{y}_{n+1, \\text{flawed}}$, where $\\hat{y}_{n+1, \\text{flawed}}$ is computed using lower-order weights where the last two elements are swapped. This flawed estimate is used only for step-size control.\n- **Error Norm  Acceptance**: The scaled error norm is defined as $\\mathrm{err\\_norm} = |e_{n+1}| / (\\mathrm{atol} + \\mathrm{rtol} \\cdot \\max(|y_{n}|, |y_{n+1}|))$. A step is accepted if $\\mathrm{err\\_norm} \\le 1$.\n- **Step-Size Control Law**: The new step size is proposed as $h_{\\mathrm{new}} = h \\cdot s \\cdot \\mathrm{err\\_norm}^{-1/(p+1)}$, where $s = 0.9$ and $p = 3$. The result is clipped to the range $[\\alpha_{\\min} h, \\alpha_{\\max} h]$ with $\\alpha_{\\min} = 0.2$ and $\\alpha_{\\max} = 5$.\n- **Test Problem**: $f(t,y) = \\lambda y$ with $y(0) = 1$. The exact solution is $y(t) = \\exp(\\lambda t)$.\n- **Global Error Metric**: $E = |y_{\\mathrm{num}}(T) - y_{\\mathrm{exact}}(T)|$.\n- **Test Cases**:\n    - Case A: $\\lambda = -1$, $T = 10$, $y_{0} = 1$, $\\mathrm{rtol} = 10^{-3}$, $\\mathrm{atol} = 10^{-12}$.\n    - Case B: $\\lambda = -1$, $T = 10$, $y_{0} = 1$, $\\mathrm{rtol} = 10^{-6}$, $\\mathrm{atol} = 10^{-12}$.\n    - Case C: $\\lambda = -5$, $T = 2$, $y_{0} = 1$, $\\mathrm{rtol} = 10^{-6}$, $\\mathrm{atol} = 10^{-12}$.\n- **Required Computations**: For each case, calculate the final-time global error for both the correct controller ($E_{\\mathrm{correct}}$) and the flawed controller ($E_{\\mathrm{flawed}}$).\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is well-defined and scientifically sound. It addresses a fundamental topic in computational engineering: the design and analysis of adaptive numerical methods for ODEs. All parameters and procedures are specified with sufficient precision to permit a unique implementation. The only unspecified parameter is the initial step size, $h_0$. This is a minor omission. For a reproducible comparison between the two controllers, it is sufficient to use the same reasonable initial step size for both. I will proceed by assuming a small, fixed initial step size, $h_0$, for all simulations, which is a standard approach. The problem is therefore deemed valid.\n\n**Step 3: Verdict and Action**\nThe problem is **valid**. A complete, reasoned solution will be provided.\n\n**Fundamental Principles and Method**\n\nThe problem requires the numerical integration of the IVP $y'(t) = f(t,y(t))$, $y(t_0) = y_0$. This is achieved using an adaptive step-size Runge-Kutta method.\n\nAn explicit Runge-Kutta method computes the solution at time $t_{n+1} = t_n + h_n$ from the solution at $t_n$ by means of a series of intermediate stage evaluations. For an $s$-stage method, we have:\n$$k_i = f\\left(t_n + c_i h_n, y_n + h_n \\sum_{j=1}^{i-1} a_{ij} k_j\\right), \\quad i=1, \\dots, s$$\n$$y_{n+1} = y_n + h_n \\sum_{i=1}^s b_i k_i$$\nThe coefficients $c_i$, $a_{ij}$, and $b_i$ define the specific method.\n\nAn embedded pair provides two solutions, $y_{n+1}$ of order $p$ and $\\hat{y}_{n+1}$ of order $q  p$, using the same set of stage values $k_i$.\n$$y_{n+1} = y_n + h_n \\sum_{i=1}^s b_i k_i \\quad (\\text{order } p)$$\n$$\\hat{y}_{n+1} = y_n + h_n \\sum_{i=1}^s \\hat{b}_i k_i \\quad (\\text{order } q)$$\nThe difference, $e_{n+1} = y_{n+1} - \\hat{y}_{n+1} = h_n \\sum_{i=1}^s (b_i - \\hat{b}_i)k_i$, provides an estimate of the local truncation error of the lower-order method. This error estimate is used to control the step size $h_n$.\n\n**The Bogacki–Shampine 3(2) Pair**\n\nThe specified Bogacki–Shampine method is a 3-stage method that produces a 3rd-order solution and a 2nd-order solution. It possesses the First Same As Last (FSAL) property, meaning the final stage evaluation of one step can be reused as the first stage of the subsequent step, increasing efficiency. Its structure is as follows:\n\n1.  Compute three intermediate stages:\n    $$k_1 = f(t_n, y_n)$$\n    $$k_2 = f(t_n + \\frac{1}{2}h, y_n + \\frac{1}{2}h k_1)$$\n    $$k_3 = f(t_n + \\frac{3}{4}h, y_n + \\frac{3}{4}h k_2)$$\n2.  Compute the 3rd-order approximation, which is used to advance the solution:\n    $$y_{n+1} = y_n + h\\left(\\frac{2}{9}k_1 + \\frac{1}{3}k_2 + \\frac{4}{9}k_3\\right)$$\n3.  Compute a fourth stage using the advanced solution $y_{n+1}$. This is the FSAL stage.\n    $$k_4 = f(t_n + h, y_{n+1})$$\n4.  Compute the 2nd-order approximation for error estimation:\n    $$\\hat{y}_{n+1} = y_n + h\\left(\\frac{7}{24}k_1 + \\frac{1}{4}k_2 + \\frac{1}{3}k_3 + \\frac{1}{8}k_4\\right)$$\nThe local error estimate is $e_{n+1} = y_{n+1} - \\hat{y}_{n+1}$.\n\n**Adaptive Step-Size Control**\n\nThe goal of the controller is to adjust the step size $h$ such that the local error estimate satisfies a given tolerance. The error is scaled relative to the solution magnitude:\n$$\\mathrm{err\\_norm} = \\frac{|e_{n+1}|}{\\mathrm{atol} + \\mathrm{rtol} \\cdot \\max(|y_{n}|, |y_{n+1}|)}$$\nwhere $\\mathrm{atol}$ and $\\mathrm{rtol}$ are absolute and relative error tolerances.\n\nA step is accepted if $\\mathrm{err\\_norm} \\le 1$. If accepted, the solution is advanced: $t_{n+1} = t_n + h$, $y_{n+1} = y_{n+1}$. If rejected, the step is re-attempted with a smaller $h$.\n\nIn either case, a new step size $h_{\\mathrm{new}}$ is proposed. Based on the assumption that the local error behaves as $C \\cdot h^{p+1}$, the optimal step size is derived from the current error:\n$$h_{\\mathrm{new}} = h \\cdot s \\cdot \\left(\\frac{1}{\\mathrm{err\\_norm}}\\right)^{1/(p+1)}$$\nHere, $p=3$ is the order of the method used for step-size prediction, and $s=0.9$ is a safety factor to ensure robustness. The new step size is clipped to avoid excessively large or small changes: $h_{\\mathrm{new}}$ is constrained to $[\\alpha_{\\min} h, \\alpha_{\\max} h] = [0.2h, 5.0h]$.\n\n**The Flawed Controller**\n\nThe problem mandates an investigation into a flawed controller. The flaw is introduced in the local error estimation. The correct weights for the 2nd-order method are $\\hat{\\mathbf{b}} = [7/24, 1/4, 1/3, 1/8]$. The flaw consists of swapping the last two weights:\n$$\\hat{\\mathbf{b}}_{\\text{flawed}} = [7/24, 1/4, 1/8, 1/3]$$\nThis leads to a flawed 2nd-order approximation:\n$$\\hat{y}_{n+1, \\text{flawed}} = y_n + h\\left(\\frac{7}{24}k_1 + \\frac{1}{4}k_2 + \\frac{1}{8}k_3 + \\frac{1}{3}k_4\\right)$$\nThe flawed error estimate is $\\hat{e}_{n+1} = y_{n+1} - \\hat{y}_{n+1, \\text{flawed}}$. This $\\hat{e}_{n+1}$ is then used in the error norm calculation and the subsequent step-size adjustment. It is critical to note that even with the flawed controller, the solution is advanced using the correct 3rd-order formula for $y_{n+1}$. The flaw only affects the adaptive mechanism, not the propagation formula itself.\n\n**Algorithm and Implementation**\n\nThe core of the implementation will be an `adaptive_integrator` function that executes the following loop:\n1.  Initialize $t=t_0$, $y=y_0$, and an initial step size $h=h_0$. We will use $h_0 = 10^{-2}$. Compute the first stage $k_1 = f(t_0, y_0)$.\n2.  Begin the main loop, which continues as long as $t  T$.\n3.  Inside the loop, start a sub-loop for step acceptance.\n4.  In the sub-loop, compute stages $k_2, k_3$, the 3rd-order solution $y_{n+1}$, and the final stage $k_4$.\n5.  Based on the controller type (correct or flawed), calculate the appropriate local error estimate ($e_{n+1}$ or $\\hat{e}_{n+1}$).\n6.  Compute the error norm $\\mathrm{err\\_norm}$.\n7.  If $\\mathrm{err\\_norm} \\le 1$, the step is accepted. Advance time and the solution ($t \\leftarrow t+h, y \\leftarrow y_{n+1}$). The stage $k_4$ becomes the new $k_1$ for the next step (FSAL). Exit the sub-loop.\n8.  If $\\mathrm{err\\_norm}  1$, the step is rejected. The current $y_{n+1}$ is discarded. The sub-loop continues with a new, smaller step size $h$.\n9.  In both acceptance and rejection cases, calculate a new proposed step size $h_{\\mathrm{new}}$ using the control law and clipping, and update $h$.\n10. The main loop terminates when $t \\ge T$. A final adjustment is made to the last step size to ensure integration stops exactly at $T$.\n\nThis procedure will be executed for each test case, once with the correct controller and once with the flawed one. The final global error $E = |y_{\\mathrm{num}}(T) - \\exp(\\lambda T)|$ is then computed and reported. The flawed controller's behavior (whether it is more or less aggressive than the correct one) will determine whether the final global error is larger or smaller. This experiment demonstrates the sensitivity of the global error to the correctness of the local error estimator, which underpins the reliability of adaptive solvers.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n# from scipy import ...\n\ndef adaptive_integrator(f, t0, y0, T, rtol, atol, h0, flawed_controller):\n    \"\"\"\n    Integrates an ODE using the adaptive Bogacki-Shampine 3(2) method.\n    \"\"\"\n    # Bogacki-Shampine coefficients and controller parameters\n    # The method structure:\n    # k1 = f(t, y)\n    # k2 = f(t + 1/2 h, y + 1/2 h k1)\n    # k3 = f(t + 3/4 h, y + 3/4 h k2)\n    # y_next = y + h * (2/9 k1 + 1/3 k2 + 4/9 k3)  (order 3 solution)\n    # k4 = f(t + h, y_next)                          (FSAL stage)\n    # error = y_next - y_hat_next, where y_hat_next is the order 2 solution.\n    # The error is computed using a difference of weights for better numerical stability.\n    \n    b3_weights = np.array([2/9, 1/3, 4/9, 0])      # Order 3 weights\n    b2_weights_correct = np.array([7/24, 1/4, 1/3, 1/8]) # Order 2 weights\n    b2_weights_flawed = np.array([7/24, 1/4, 1/8, 1/3])  # Flawed order 2 weights\n\n    if flawed_controller:\n        err_weights = b3_weights - b2_weights_flawed\n    else:\n        err_weights = b3_weights - b2_weights_correct\n\n    s = 0.9\n    p = 3\n    alpha_min = 0.2\n    alpha_max = 5.0\n\n    t = t0\n    y = y0\n    h = h0\n\n    # First stage evaluation (k1) for the first step\n    k1 = f(t, y)\n\n    while t  T:\n        if t + h  T:\n            h = T - t  # Adjust last step to hit T exactly\n\n        step_accepted = False\n        while not step_accepted:\n            # Prevent infinitely small step size\n            if abs(h)  1e-15 * T:\n                raise RuntimeError(\"Step size has become excessively small.\")\n\n            # Compute stages for BS(3,2)\n            k2 = f(t + 0.5 * h, y + 0.5 * h * k1)\n            k3 = f(t + 0.75 * h, y + 0.75 * h * k2)\n            y_next = y + h * (b3_weights[0] * k1 + b3_weights[1] * k2 + \n                               b3_weights[2] * k3)\n            k4 = f(t + h, y_next)\n\n            # Calculate local error estimate\n            local_error = h * (err_weights[0] * k1 + err_weights[1] * k2 + \n                               err_weights[2] * k3 + err_weights[3] * k4)\n\n            # Calculate scaled error norm\n            y_scale = atol + rtol * max(abs(y), abs(y_next))\n            err_norm = abs(local_error) / y_scale if y_scale  0 else 0\n\n            # Step acceptance logic\n            if err_norm = 1.0:\n                step_accepted = True\n                t += h\n                y = y_next\n                # FSAL: k4 of this step is k1 of the next\n                k1 = k4\n                \n                # Update step size for the next step\n                if err_norm == 0:\n                    # Avoid division by zero and propose max increase\n                    h_new = h * alpha_max\n                else:\n                    h_new = h * s * (err_norm ** (-1.0 / (p + 1.0)))\n            else:\n                # Step rejected, reduce step size and retry\n                h_new = h * s * (err_norm ** (-1.0 / (p + 1.0)))\n\n            # Clip the new step size\n            h = max(h * alpha_min, min(h * alpha_max, h_new))\n\n    return y\n\ndef solve():\n    \"\"\"\n    Runs the simulation for the specified test cases and prints the results.\n    \"\"\"\n    test_cases = [\n        # Case A\n        {'lambda': -1.0, 'T': 10.0, 'y0': 1.0, 'rtol': 1e-3, 'atol': 1e-12},\n        # Case B\n        {'lambda': -1.0, 'T': 10.0, 'y0': 1.0, 'rtol': 1e-6, 'atol': 1e-12},\n        # Case C\n        {'lambda': -5.0, 'T': 2.0, 'y0': 1.0, 'rtol': 1e-6, 'atol': 1e-12},\n    ]\n\n    results = []\n    h0 = 1e-2  # Fixed initial step size for all runs for fair comparison\n\n    for case in test_cases:\n        lambda_val = case['lambda']\n        T = case['T']\n        y0 = case['y0']\n        rtol = case['rtol']\n        atol = case['atol']\n\n        # Define the ODE and its exact solution\n        f = lambda t, y: lambda_val * y\n        y_exact_func = lambda t: np.exp(lambda_val * t)\n        \n        y_exact_T = y_exact_func(T)\n\n        # Run with correct controller\n        y_num_correct = adaptive_integrator(f, 0, y0, T, rtol, atol, h0, flawed_controller=False)\n        E_correct = abs(y_num_correct - y_exact_T)\n\n        # Run with flawed controller\n        y_num_flawed = adaptive_integrator(f, 0, y0, T, rtol, atol, h0, flawed_controller=True)\n        E_flawed = abs(y_num_flawed - y_exact_T)\n\n        results.append([E_correct, E_flawed])\n\n    # Format the output string as specified\n    output_str = '[' + ','.join([f'[{r[0]:.5e},{r[1]:.5e}]' for r in results]) + ']'\n    print(output_str)\n\nsolve()\n```", "id": "2388676"}, {"introduction": "An adaptive solver truly proves its worth when faced with a solution whose complexity changes rapidly and unpredictably. In this exercise [@problem_id:2388548], you will implement a full adaptive integrator based on the Bogacki-Shampine pair and pit it against an 'adversarial' ordinary differential equation designed to be challenging. Observing the frequent step rejections and dynamic step-size adjustments will give you a practical feel for the 'dance' between the solver and the problem, illustrating why adaptivity is essential for both efficiency and robustness.", "problem": "Construct, justify, and test a one-dimensional adversarial Ordinary Differential Equation (ODE) initial value problem that reliably provokes frequent step rejections in an adaptive Embedded Runge–Kutta (ERK) solver as a stiffness-like parameter tends to zero. Start from fundamental concepts: an initial value problem is given by $y^{\\prime} = f(t,y,\\epsilon)$ with $y(0)=y_0$, where $f$ is sufficiently smooth in its arguments for fixed $\\epsilon0$, and an explicit ERK method uses multiple stage evaluations of $f$ within each step to produce two approximations of different formal orders, whose difference serves as a local error estimate for adaptive step-size control. The local truncation error of an order $p$ method scales as $O(h^{p+1})$ for step size $h$, while higher derivatives of the exact solution can amplify this scaling when the right-hand side has rapidly varying components.\n\nYou must follow these instructions.\n\n1) Adversarial ODE design and justification. Design $f$ by combining a stable linear dissipative component with rapidly varying smooth forcing that accelerates in time and concentrates on small scales as $\\epsilon \\to 0$. To make the assignment concrete and testable, use the following canonical adversarial model:\n- Define the ODE\n$$\ny^{\\prime}(t) \\;=\\; f(t,y,\\epsilon) \\;=\\; -\\,y \\;+\\; \\sin\\!\\Big(\\frac{t^2}{\\epsilon}\\Big)\\;+\\;0.3\\,\\exp\\!\\Big(-\\frac{(t-0.7)^2}{2\\,(2\\epsilon)^2}\\Big)\\,\\sin\\!\\Big(\\frac{t}{\\epsilon}\\Big).\n$$\n- Use the initial condition $y(0)=0$ on the interval $t\\in[0,1]$.\n- Explain, from first principles, why this $f$ is adversarial for explicit ERK with adaptive control as $\\epsilon\\to 0$: the dissipative term $-y$ yields a unique stable solution, the chirped sinusoid $\\sin(t^2/\\epsilon)$ produces an instantaneous frequency proportional to $2t/\\epsilon$ that increases with $t$, and the localized Gaussian factor multiplies a fast oscillation $\\sin(t/\\epsilon)$ with a width of order $O(\\epsilon)$ centered at $t=0.7$. As $\\epsilon$ decreases, higher time derivatives of the solution grow in magnitude roughly like powers of $\\epsilon^{-1}$, forcing $h$ to shrink abruptly to satisfy local error control, which in turn causes frequent rejections when the controller’s step-size prediction is too optimistic across rapidly changing regions.\n\n2) Numerical method and adaptive controller. Implement an explicit Embedded Runge–Kutta pair with orders $p=3$ and $p_{\\mathrm{emb}}=2$ (the Bogacki–Shampine $3(2)$ pair), using the following acceptance and step-size adaptation policy, all expressed in a weighted scalar norm appropriate for a one-dimensional state:\n- Use relative tolerance $\\mathrm{rtol}$ and absolute tolerance $\\mathrm{atol}$ with the error ratio\n$$\nE \\;=\\; \\frac{|y_{\\text{high}}-y_{\\text{low}}|}{\\mathrm{atol}+\\mathrm{rtol}\\,\\max\\{|y_n|,|y_{\\text{high}}|\\}},\n$$\nwhere $y_{\\text{high}}$ is the higher-order update and $y_{\\text{low}}$ is the lower-order embedded update.\n- Accept a step if $E\\le 1$, otherwise reject it. On rejection, reduce the step and retry without advancing time.\n- Update the step size $h$ via the controller\n$$\nh_{\\text{new}} \\;=\\; h \\cdot \\mathrm{clip}\\!\\Big(\\gamma\\,E^{-1/3},\\,\\eta_{\\min},\\,\\eta_{\\max}\\Big),\n$$\nwith safety factor $\\gamma=0.9$, minimum factor $\\eta_{\\min}=0.2$, and maximum factor $\\eta_{\\max}=5$. If $E=0$, take the maximum growth factor.\n\n3) Implementation constraints. Write a complete program that:\n- Implements the Bogacki–Shampine $3(2)$ method with the First Same As Last (FSAL) property.\n- Integrates from $t=0$ to $t=1$ with initial step size $h_0=10^{-1}$ and a minimum allowed step $h_{\\min}=10^{-12}$ to avoid infinite loops.\n- Counts and returns the number of rejected steps for each test case.\n- Uses only the permitted environment and libraries specified in the final answer.\n\n4) Test suite. Evaluate the solver on the following five test cases, which probe different regimes and edge conditions. Each case is a triple $(\\epsilon,\\mathrm{rtol},\\mathrm{atol})$:\n- Case A (benign baseline): $(1,\\;10^{-5},\\;10^{-8})$.\n- Case B (moderate oscillations): $(10^{-1},\\;10^{-5},\\;10^{-8})$.\n- Case C (strong oscillations): $(10^{-2},\\;10^{-5},\\;10^{-8})$.\n- Case D (extreme oscillations): $(10^{-3},\\;10^{-5},\\;10^{-8})$.\n- Case E (tighter tolerance edge): $(10^{-2},\\;10^{-7},\\;10^{-10})$.\n\n5) Required final output format. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of Cases A through E, where each entry is the integer number of rejected steps for that case. For example, the printed line must have the form\n$[r_A,r_B,r_C,r_D,r_E]$\nwith each $r_\\cdot$ an integer. No additional text may be printed.\n\nAll numerical values must be interpreted as dimensionless. Angles are in radians. No physical units are involved. The program must be self-contained and require no user input or external files.", "solution": "The problem statement is assessed to be **valid**. It is a well-posed initial value problem in the field of computational physics, specifically concerning the numerical solution of ordinary differential equations (ODEs). The problem is scientifically grounded, using established numerical methods (Embedded Runge-Kutta) and standard techniques for adaptive step-size control. All parameters, equations, and constraints are defined with sufficient precision to permit a unique, verifiable solution. The objective is clear and formalizable.\n\n### 1. Adversarial ODE Design and Justification\n\nThe specified ODE is\n$$\ny^{\\prime}(t) = f(t,y,\\epsilon) = -y + \\sin\\Big(\\frac{t^2}{\\epsilon}\\Big) + 0.3\\,\\exp\\Big(-\\frac{(t-0.7)^2}{8\\epsilon^2}\\Big)\\sin\\Big(\\frac{t}{\\epsilon}\\Big)\n$$\nwith initial condition $y(0)=0$ on the interval $t \\in [0,1]$. This ODE is designed to be adversarial to explicit adaptive solvers for small values of the parameter $\\epsilon  0$. The reasoning is as follows.\n\nThe ODE is a linear first-order equation of the form $y' + y = g(t, \\epsilon)$, where $g(t, \\epsilon)$ is the forcing function comprising the sinusoidal and Gaussian-modulated terms. The term $-y$ represents linear dissipation, which guarantees that the solution $y(t)$ is stable and remains bounded, tracking the behavior of the forcing term $g(t, \\epsilon)$. The exact solution is given by $y(t) = e^{-t} \\int_0^t e^\\tau g(\\tau, \\epsilon) d\\tau$.\n\nThe adversarial nature arises from the properties of the forcing function $g(t, \\epsilon)$ as $\\epsilon \\to 0$:\n1.  **Chirped Frequency Component**: The term $\\sin(t^2/\\epsilon)$ has an instantaneous angular frequency $\\omega(t) = \\frac{d}{dt}(t^2/\\epsilon) = 2t/\\epsilon$. The frequency of oscillation increases linearly with time, forcing an adaptive solver to continuously decrease its step size $h$ as it progresses along the integration interval.\n2.  **Localized High-Frequency Burst**: The term $0.3\\,\\exp(-\\frac{(t-0.7)^2}{2\\,(2\\epsilon)^2})\\sin(t/\\epsilon)$ introduces a high-frequency oscillation, $\\sin(t/\\epsilon)$, modulated by a Gaussian envelope centered at $t=0.7$. The standard deviation of the Gaussian is $\\sigma = 2\\epsilon$, meaning this feature is spatially concentrated in a narrow region of width $O(\\epsilon)$. This creates a sudden, localized challenge for the solver.\n\nThe local truncation error (LTE) of a Runge-Kutta method of order $p$ is of the form $LTE = C h^{p+1} y^{(p+1)}(\\xi)$ for a constant $C$ and some $\\xi$ within the step interval. The higher derivatives of the solution $y(t)$ are determined by the derivatives of the forcing term $g(t, \\epsilon)$. We have:\n$$\ny'(t) = -y + g(t, \\epsilon) \\\\\ny''(t) = -y' + g' = y - g + g' \\\\\ny^{(k)}(t) = (-1)^k y + \\sum_{j=0}^{k-1} (-1)^{k-1-j} g^{(j)}(t, \\epsilon)\n$$\nThe magnitude of $y^{(k)}(t)$ is thus dominated by the highest available derivative of the forcing, $g^{(k-1)}(t, \\epsilon)$. The derivatives of the components of $g(t, \\epsilon)$ scale inversely with $\\epsilon$. For instance, for the chirp term $g_1(t) = \\sin(t^2/\\epsilon)$, the $k$-th derivative scales as $g_1^{(k)}(t) = O(\\epsilon^{-k})$.\n\nFor the Bogacki-Shampine pair, the error estimate is based on the difference between a third-order ($p=3$) and a second-order ($p_{\\text{emb}}=2$) approximation. This difference is an estimate of the LTE of the embedded (lower-order) method, which behaves as $O(h^{p_{\\text{emb}}+1} y^{(p_{\\text{emb}}+1)}) = O(h^3 y^{(3)})$. Since $y^{(3)}$ scales with $g^{(2)}$, and $g^{(2)}(t, \\epsilon) = O(\\epsilon^{-2})$, the error estimate $\\Delta$ satisfies:\n$$\n\\Delta \\approx C' h^3 \\epsilon^{-2}\n$$\nThe adaptive controller aims to keep the error ratio $E = |\\Delta| / (\\text{atol} + \\text{rtol} \\cdot \\text{scale})$ below $1$. This implies that the step size $h$ must satisfy $h^3 \\lesssim (\\text{atol} + \\dots) \\epsilon^2$, or roughly $h = O(\\epsilon^{2/3})$. As $\\epsilon$ decreases, the required step size shrinks significantly. An adaptive controller, which predicts the next step size based on the current error, is likely to make an overly optimistic prediction when encountering the rapidly increasing frequencies. This leads to an error estimate $E  1$, causing a step rejection and forcing a retry with a smaller step. This process repeats frequently in regions of high oscillation, leading to a high count of rejected steps.\n\n### 2. Numerical Method: The Bogacki–Shampine 3(2) Pair\n\nThe Bogacki–Shampine method is an explicit Runge-Kutta pair of orders $3$ and $2$. Its Butcher tableau is given by:\n$$\n\\begin{array}{c|cccc}\n0  0  0  0  0 \\\\\n1/2  1/2  0  0  0 \\\\\n3/4  0  3/4  0  0 \\\\\n1  2/9  1/3  4/9  0 \\\\\n\\hline\ny_{\\text{high}}  2/9  1/3  4/9  0 \\\\\ny_{\\text{low}}  7/24  1/4  1/3  1/8\n\\end{array}\n$$\nFor a step from $(t_n, y_n)$ with size $h$, the stages are calculated as:\n$$\n\\begin{aligned}\nk_1 = f(t_n, y_n, \\epsilon) \\\\\nk_2 = f\\left(t_n + \\frac{1}{2}h, y_n + \\frac{1}{2}h k_1, \\epsilon\\right) \\\\\nk_3 = f\\left(t_n + \\frac{3}{4}h, y_n + \\frac{3}{4}h k_2, \\epsilon\\right)\n\\end{aligned}\n$$\nThe third-order solution is then computed, which will be the candidate for the next step's solution $y_{n+1}$:\n$$\ny_{\\text{high}} = y_n + h\\left(\\frac{2}{9}k_1 + \\frac{1}{3}k_2 + \\frac{4}{9}k_3\\right)\n$$\nThe fourth stage is evaluated using this new approximation at time $t_n+h$:\n$$\nk_4 = f(t_n + h, y_{\\text{high}}, \\epsilon)\n$$\nThe embedded second-order solution is then:\n$$\ny_{\\text{low}} = y_n + h\\left(\\frac{7}{24}k_1 + \\frac{1}{4}k_2 + \\frac{1}{3}k_3 + \\frac{1}{8}k_4\\right)\n$$\nThis method possesses the \"First Same As Last\" (FSAL) property. If the step is accepted, the state advances to $(t_{n+1}, y_{n+1}) = (t_n+h, y_{\\text{high}})$. The first stage evaluation for the subsequent step would be $k'_1 = f(t_{n+1}, y_{n+1}, \\epsilon)$, which is identical to the $k_4$ just computed. This allows the reuse of the $k_4$ evaluation, saving one function evaluation per accepted step.\n\n### 3. Adaptive Control Algorithm\n\nThe adaptive step-size control is based on an estimate of the local error.\n\n**Error Estimation**: The local error is estimated by the difference between the high-order and low-order solutions. For this scalar problem, the error is $\\Delta = y_{\\text{high}} - y_{\\text{low}}$. A weighted error ratio $E$ is computed to normalize the error with respect to the solution magnitude and specified tolerances:\n$$\nE = \\frac{|\\Delta|}{\\mathrm{atol} + \\mathrm{rtol} \\cdot \\max\\{|y_n|, |y_{\\text{high}}|\\}}\n$$\nwhere `atol` is the absolute tolerance and `rtol` is the relative tolerance. A step is accepted if $E \\le 1$.\n\n**Step-size Control**: Upon completion of a step (whether accepted or rejected), a new step size $h_{\\text{new}}$ is proposed using a standard Proportional-Integral (PI) controller logic, here simplified to a proportional controller:\n$$\nh_{\\text{new}} = h \\cdot \\text{clip}\\left(\\gamma E^{-1/(p_{\\text{emb}}+1)}, \\eta_{\\min}, \\eta_{\\max}\\right)\n$$\nFor our $p_{\\text{emb}}=2$ method, the exponent is $-1/3$. The parameters are:\n-   Safety factor $\\gamma = 0.9$, to provide a conservative estimate.\n-   Minimum step-size change factor $\\eta_{\\min} = 0.2$.\n-   Maximum step-size change factor $\\eta_{\\max} = 5$.\nThe `clip` function limits the multiplicative factor to the range $[\\eta_{\\min}, \\eta_{\\max}]$ to prevent excessively aggressive or slow changes in step size. If $E=0$, the maximum factor $\\eta_{\\max}$ is used. If a step with size $h$ is rejected ($E1$), the proposed $h_{\\text{new}}$ will be smaller than $h$, and the step is retried from the same point $(t_n, y_n)$ with this new, smaller step size. If the step is accepted ($E \\le 1$), the state is advanced, and $h_{\\text{new}}$ is used for the next step. The final step size is always bounded below by $h_{\\min} = 10^{-12}$.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print the results.\n    \"\"\"\n    # Test cases as tuples of (epsilon, rtol, atol)\n    test_cases = [\n        (1.0, 1e-5, 1e-8),        # Case A: Benign baseline\n        (1e-1, 1e-5, 1e-8),       # Case B: Moderate oscillations\n        (1e-2, 1e-5, 1e-8),       # Case C: Strong oscillations\n        (1e-3, 1e-5, 1e-8),       # Case D: Extreme oscillations\n        (1e-2, 1e-7, 1e-10)       # Case E: Tighter tolerance edge\n    ]\n\n    results = []\n    for epsilon, rtol, atol in test_cases:\n        rejected_steps = solve_ode(epsilon, rtol, atol)\n        results.append(rejected_steps)\n\n    # Print the final results in the required format\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef f(t, y, epsilon):\n    \"\"\"\n    Defines the right-hand side of the adversarial ODE y' = f(t, y, epsilon).\n    \"\"\"\n    term1 = -y\n    term2 = np.sin(t**2 / epsilon)\n    # The denominator is 2 * (2*epsilon)^2 = 8 * epsilon^2\n    term3 = 0.3 * np.exp(-(t - 0.7)**2 / (8 * epsilon**2)) * np.sin(t / epsilon)\n    return term1 + term2 + term3\n\ndef solve_ode(epsilon, rtol, atol):\n    \"\"\"\n    Integrates the ODE using an adaptive Bogacki-Shampine 3(2) method.\n\n    Args:\n        epsilon (float): The stiffness-like parameter in the ODE.\n        rtol (float): Relative tolerance for step-size control.\n        atol (float): Absolute tolerance for step-size control.\n\n    Returns:\n        int: The total number of rejected steps during integration.\n    \"\"\"\n    t_start, t_end = 0.0, 1.0\n    y_start = 0.0\n\n    # Controller parameters\n    gamma = 0.9      # Safety factor\n    eta_min = 0.2    # Minimum step-size scaling factor\n    eta_max = 5.0    # Maximum step-size scaling factor\n    \n    # Integration parameters\n    h0 = 1e-1\n    h_min = 1e-12\n    \n    # Butcher tableau coefficients for Bogacki-Shampine 3(2)\n    c2, c3 = 1/2, 3/4\n    a21 = 1/2\n    a32 = 3/4\n    \n    b1_high, b2_high, b3_high = 2/9, 1/3, 4/9\n    b1_low, b2_low, b3_low, b4_low = 7/24, 1/4, 1/3, 1/8\n\n    # Initial conditions\n    t = t_start\n    y = y_start\n    h = h0\n    rejected_steps = 0\n    use_fsal = False\n    k1 = 0.0 # Will be properly initialized inside the loop\n\n    while t  t_end:\n        # Ensure the final step lands exactly on t_end\n        if t + h  t_end:\n            h = t_end - t\n            use_fsal = False  # FSAL is invalid for a step of modified length\n\n        # Calculate stages for the RK method\n        if not use_fsal:\n            k1 = f(t, y, epsilon)\n        # if use_fsal is True, k1 has been set to the previous step's k4\n\n        k2 = f(t + c2 * h, y + a21 * h * k1, epsilon)\n        k3 = f(t + c3 * h, y + a32 * h * k2, epsilon)\n        \n        # Calculate the higher-order (3rd order) approximation\n        y_high = y + h * (b1_high * k1 + b2_high * k2 + b3_high * k3)\n\n        # Use the FSAL property: calculate k4 using y_high\n        k4 = f(t + h, y_high, epsilon)\n\n        # Calculate the lower-order (2nd order) embedded approximation\n        y_low = y + h * (b1_low * k1 + b2_low * k2 + b3_low * k3 + b4_low * k4)\n\n        # Estimate the error and the error ratio E\n        error = abs(y_high - y_low)\n        scale = atol + rtol * max(abs(y), abs(y_high))\n        error_ratio = error / scale if scale  1e-30 else 0.0\n\n        # Decide whether to accept or reject the step\n        if error_ratio = 1.0:  # Accept the step\n            t += h\n            y = y_high\n            \n            # Prepare for the next step using FSAL\n            use_fsal = True\n            k1 = k4  # k4 of this step is k1 of the next\n\n            # Update step size for the next step\n            if error_ratio == 0.0:\n                factor = eta_max\n            else:\n                factor = gamma * (error_ratio ** (-1.0 / 3.0))\n            \n            h_new = h * np.clip(factor, eta_min, eta_max)\n            h = max(h_min, h_new)\n\n        else:  # Reject the step\n            rejected_steps += 1\n            use_fsal = False  # Must re-evaluate k1 for the retried step\n\n            # Reduce step size for the retry\n            factor = gamma * (error_ratio ** (-1.0 / 3.0))\n            h_new = h * np.clip(factor, eta_min, eta_max)\n            h = max(h_min, h_new)\n            \n            if h = h_min and t  t_end:\n                # To prevent infinite loops in pathological cases where h must be  h_min\n                # we are forced to take a step at h_min. This may violate tolerance.\n                # Continuing with h=h_min as per problem interpretation.\n                pass\n\n    return rejected_steps\n\nif __name__ == \"__main__\":\n    solve()\n\n```", "id": "2388548"}, {"introduction": "Every numerical method is built upon theoretical foundations, and understanding their limits is as important as knowing how to use them. This final practice [@problem_id:2388671] explores the crucial assumption of solution smoothness that underpins Runge-Kutta error estimators. By analytically and numerically examining a problem with a carefully constructed discontinuity in a higher derivative, you will learn to diagnose situations where the local error estimate may become unreliable, a vital skill for any advanced practitioner of computational science.", "problem": "Consider the initial value problem for an ordinary differential equation (ODE): find a scalar function $y(t)$ such that\n$$\n\\frac{dy}{dt} = g(t), \\quad y(0)=0,\n$$\nwhere $g(t)$ is defined piecewise by\n$$\ng(t) = \\frac{1}{2}\\,s(t)\\,\\bigl(t-t_c\\bigr)^2,\\quad s(t)=\n\\begin{cases}\na,  t \\le t_c,\\\\\nb,  t  t_c.\n\\end{cases}\n$$\nHere $a$, $b$, and $t_c$ are real parameters. This construction ensures that $g(t)$ and $\\frac{dg}{dt}(t)$ are continuous at $t=t_c$, while $\\frac{d^2 g}{dt^2}(t)$ has a jump discontinuity at $t=t_c$. Consequently, the solution $y(t)$ is such that $y'''(t)$ is discontinuous at $t=t_c$.\n\nLet $h_00$ be a prescribed step size. Consider an embedded Runge–Kutta (RK) pair in the sense of two Runge–Kutta formulas of different orders that share the same internal stages. Use the Bogacki–Shampine embedded Runge–Kutta $3(2)$ pair, characterized by the stage abscissae and coefficients\n$$\nc_1=0,\\quad c_2=\\frac{1}{2},\\quad c_3=\\frac{3}{4},\\quad c_4=1,\n$$\n$$\nA=\\begin{bmatrix}\n0  0  0  0 \\\\\n\\frac{1}{2}  0  0  0 \\\\\n0  \\frac{3}{4}  0  0 \\\\\n\\frac{2}{9}  \\frac{1}{3}  \\frac{4}{9}  0\n\\end{bmatrix},\n$$\nand two sets of weights\n$$\n\\text{order-}3:\\quad b=\\Bigl[\\frac{2}{9},\\,\\frac{1}{3},\\,\\frac{4}{9},\\,0\\Bigr],\\qquad\n\\text{order-}2:\\quad \\hat{b}=\\Bigl[\\frac{7}{24},\\,\\frac{1}{4},\\,\\frac{1}{3},\\,\\frac{1}{8}\\Bigr].\n$$\nFor a single attempted step from $t=0$ to $t=h_0$ (with no prior adaptation), form the two one-step approximations $y^{[3]}(h_0)$ and $y^{[2]}(h_0)$ delivered by the order-$3$ and order-$2$ formulas, respectively, and define the embedded local error estimator as\n$$\n\\mathrm{est} = \\bigl|y^{[3]}(h_0) - y^{[2]}(h_0)\\bigr|.\n$$\nLet $y_{\\star}(t)$ denote the exact solution. The true local error of the accepted higher-order update for that step is\n$$\n\\mathrm{err} = \\bigl|y^{[3]}(h_0)-y_{\\star}(h_0)\\bigr|.\n$$\nDefine the underestimation ratio\n$$\nR = \\frac{\\mathrm{err}}{\\mathrm{est}},\n$$\nwith the convention that if $\\mathrm{est}=0$ then $R$ is taken to be $+\\infty$. Interpret a value $R\\Theta$ as a diagnosis that the smoothness assumptions underpinning local error control have failed on that step, where $\\Theta0$ is a prescribed threshold.\n\nAnalytically derive $y_{\\star}(t)$ in closed form, valid for all $t \\in [0,h_0]$, in terms of $a$, $b$, and $t_c$. Then, for each parameter set in the test suite below, compute the boolean outcome\n$$\n\\text{failure} = \\bigl(R\\Theta\\bigr).\n$$\n\nUse the following test suite, which explores a smooth case, a moderate jump, a sign-changing curvature jump, and a boundary alignment case:\n\n- Test $1$: $a=2$, $b=2$, $t_c=0.5$, $h_0=0.6$, $\\Theta=10$.\n- Test $2$: $a=2$, $b=20$, $t_c=0.5$, $h_0=0.6$, $\\Theta=10$.\n- Test $3$: $a=2$, $b=-8$, $t_c=0.5$, $h_0=0.6$, $\\Theta=10$.\n- Test $4$: $a=2$, $b=20$, $t_c=0.6$, $h_0=0.6$, $\\Theta=10$.\n\nYour program should produce a single line of output containing the four boolean results, in the order of the tests, as a comma-separated list enclosed in square brackets, for example, $[\\text{true},\\text{false},\\text{false},\\text{true}]$. Use lowercase $\\text{true}$ and $\\text{false}$ textual forms as produced by the language booleans when converted to strings. No physical units are involved, and all angles, if any appear, are to be interpreted in radians. The final outputs are dimensionless.", "solution": "The problem statement is subjected to validation.\n\n**Step 1: Extract Givens**\n\nThe initial value problem (IVP) is specified as:\n$$\n\\frac{dy}{dt} = g(t), \\quad y(0)=0\n$$\nwhere the function $g(t)$ is defined piecewise:\n$$\ng(t) = \\frac{1}{2}\\,s(t)\\,\\bigl(t-t_c\\bigr)^2,\\quad s(t)=\n\\begin{cases}\na,  t \\le t_c,\\\\\nb,  t  t_c.\n\\end{cases}\n$$\nThe parameters $a$, $b$, and $t_c$ are real.\n\nA single step is taken from $t=0$ to $t=h_0$ using the Bogacki–Shampine $3(2)$ embedded Runge–Kutta pair.\nThe stage abscissae are:\n$$\nc_1=0,\\quad c_2=\\frac{1}{2},\\quad c_3=\\frac{3}{4},\\quad c_4=1\n$$\nThe coefficient matrix is:\n$$\nA=\\begin{bmatrix}\n0  0  0  0 \\\\\n\\frac{1}{2}  0  0  0 \\\\\n0  \\frac{3}{4}  0  0 \\\\\n\\frac{2}{9}  \\frac{1}{3}  \\frac{4}{9}  0\n\\end{bmatrix}\n$$\nThe weight vectors for the two approximations are:\n$$\n\\text{order-}3:\\quad b=\\Bigl[\\frac{2}{9},\\,\\frac{1}{3},\\,\\frac{4}{9},\\,0\\Bigr]\n$$\n$$\n\\text{order-}2:\\quad \\hat{b}=\\Bigl[\\frac{7}{24},\\,\\frac{1}{4},\\,\\frac{1}{3},\\,\\frac{1}{8}\\Bigr]\n$$\nThe two one-step approximations are denoted $y^{[3]}(h_0)$ and $y^{[2]}(h_0)$.\nThe local error estimator is defined as:\n$$\n\\mathrm{est} = \\bigl|y^{[3]}(h_0) - y^{[2]}(h_0)\\bigr|\n$$\nThe true local error of the higher-order method is:\n$$\n\\mathrm{err} = \\bigl|y^{[3]}(h_0)-y_{\\star}(h_0)\\bigr|\n$$\nwhere $y_{\\star}(t)$ is the exact solution.\nThe underestimation ratio is:\n$$\nR = \\frac{\\mathrm{err}}{\\mathrm{est}}\n$$\nwith the special case that if $\\mathrm{est}=0$, then $R = +\\infty$.\nA failure is diagnosed if $R  \\Theta$, where $\\Theta$ is a prescribed threshold.\nThe boolean outcome is $\\text{failure} = (R  \\Theta)$.\n\nThe test suite is:\n- Test $1$: $a=2$, $b=2$, $t_c=0.5$, $h_0=0.6$, $\\Theta=10$.\n- Test $2$: $a=2$, $b=20$, $t_c=0.5$, $h_0=0.6$, $\\Theta=10$.\n- Test $3$: $a=2$, $b=-8$, $t_c=0.5$, $h_0=0.6$, $\\Theta=10$.\n- Test $4$: $a=2$, $b=20$, $t_c=0.6$, $h_0=0.6$, $\\Theta=10$.\n\n**Step 2: Validate Using Extracted Givens**\n\nThe problem is scientifically grounded, being a standard exercise in the numerical analysis of ordinary differential equations, specifically concerning adaptive step-size control using embedded Runge–Kutta methods. The IVP is well-defined. The chosen Runge–Kutta pair (Bogacki–Shampine) is well-established. The definitions of error, estimator, and their ratio are precise and objective. The problem is self-contained and consistent; all necessary data and definitions for its solution are provided. The analysis of how a method behaves when faced with a solution of limited smoothness is a classic and important topic in computational science.\n\n**Step 3: Verdict and Action**\n\nThe problem is valid. I will proceed with the derivation and solution.\n\n**Derivation and Solution**\n\nThe task requires the analytical derivation of the exact solution, the computation of two numerical approximations, and the evaluation of the ratio of the true error to the estimated error for four distinct parameter sets.\n\n**1. Analytical Exact Solution $y_{\\star}(t)$**\n\nThe ODE is $\\frac{dy}{dt} = g(t)$ with the initial condition $y(0)=0$. The exact solution is obtained by direct integration:\n$$\ny_{\\star}(t) = \\int_0^t g(\\tau) d\\tau\n$$\nThe function $g(t)$ is piecewise, so the integral must be evaluated accordingly.\n\nFor $t \\le t_c$:\n$$\ny_{\\star}(t) = \\int_0^t \\frac{a}{2}(\\tau - t_c)^2 d\\tau = \\frac{a}{2} \\left[ \\frac{(\\tau-t_c)^3}{3} \\right]_0^t = \\frac{a}{6} \\left( (t-t_c)^3 - (-t_c)^3 \\right) = \\frac{a}{6} \\left( (t-t_c)^3 + t_c^3 \\right)\n$$\nFor $t  t_c$:\nThe integral is split at the point of discontinuity $t_c$.\n$$\ny_{\\star}(t) = \\int_0^{t_c} g(\\tau) d\\tau + \\int_{t_c}^t g(\\tau) d\\tau\n$$\nThe first term is $y_{\\star}(t_c)$, which from the first case is $\\frac{a}{6} \\left( (t_c-t_c)^3 + t_c^3 \\right) = \\frac{a t_c^3}{6}$.\nThe second term is:\n$$\n\\int_{t_c}^t \\frac{b}{2}(\\tau-t_c)^2 d\\tau = \\frac{b}{2} \\left[ \\frac{(\\tau-t_c)^3}{3} \\right]_{t_c}^t = \\frac{b}{6}(t-t_c)^3\n$$\nCombining these, for $t  t_c$:\n$$\ny_{\\star}(t) = \\frac{a t_c^3}{6} + \\frac{b}{6}(t-t_c)^3\n$$\nSo, the complete analytical solution valid for $t \\in [0, h_0]$ is:\n$$\ny_{\\star}(t) = \\begin{cases}\n\\frac{a}{6} \\left( (t-t_c)^3 + t_c^3 \\right),  t \\le t_c \\\\\n\\frac{a t_c^3}{6} + \\frac{b}{6}(t-t_c)^3,  t  t_c\n\\end{cases}\n$$\nTo find the true value at the end of the step, we evaluate $y_{\\star}(h_0)$ using this formula, selecting the appropriate case based on the relation between $h_0$ and $t_c$.\n\n**2. Runge–Kutta Approximations**\n\nWe perform a single step from $t=0$ to $t=h_0$ with step size $h=h_0$. The initial value is $y_0=y(0)=0$. For an ODE of the form $y' = f(t,y)$, the Runge-Kutta stages are $k_i = f(t_0 + c_i h, y_0 + h\\sum_j a_{ij}k_j)$. In our case, $f(t,y) = g(t)$, which does not depend on $y$. The stage calculations simplify to:\n$$\nk_i = g(c_i h_0)\n$$\nThe four stages are computed by evaluating $g(t)$ at the corresponding time points:\n$t_1 = c_1 h_0 = 0$\n$t_2 = c_2 h_0 = \\frac{1}{2} h_0$\n$t_3 = c_3 h_0 = \\frac{3}{4} h_0$\n$t_4 = c_4 h_0 = h_0$\n\nThe stage values are:\n$$\nk_1 = g(0), \\quad k_2 = g(h_0/2), \\quad k_3 = g(3h_0/4), \\quad k_4 = g(h_0)\n$$\nThe evaluation of each $k_i$ uses parameter $a$ or $b$ depending on whether the argument $c_i h_0$ is less than or equal to $t_c$, or greater than $t_c$.\n\nThe order-$3$ and order-$2$ approximations at $t=h_0$ are then given by:\n$$\ny^{[3]}(h_0) = y(0) + h_0 \\sum_{i=1}^4 b_i k_i = h_0 \\left( \\frac{2}{9}k_1 + \\frac{1}{3}k_2 + \\frac{4}{9}k_3 \\right)\n$$\n$$\ny^{[2]}(h_0) = y(0) + h_0 \\sum_{i=1}^4 \\hat{b}_i k_i = h_0 \\left( \\frac{7}{24}k_1 + \\frac{1}{4}k_2 + \\frac{1}{3}k_3 + \\frac{1}{8}k_4 \\right)\n$$\n\n**3. Test Case Evaluation**\n\nWe now apply this framework to each test case.\n\n**Test 1:** $a=2, b=2, t_c=0.5, h_0=0.6, \\Theta=10$.\nThis is a smooth case as $a=b$. The function is $g(t) = (t-0.5)^2$. The exact solution $y(t)$ is a cubic polynomial. An order-3 method is exact for cubic polynomial solutions.\n$y_{\\star}(0.6) = \\int_0^{0.6} (\\tau-0.5)^2 d\\tau = [\\frac{(\\tau-0.5)^3}{3}]_0^{0.6} = \\frac{(0.1)^3 - (-0.5)^3}{3} = \\frac{0.001+0.125}{3} = \\frac{0.126}{3} = 0.042$.\nAs the method is exact, $y^{[3]}(0.6) = y_{\\star}(0.6) = 0.042$.\nThus, $\\mathrm{err} = |0.042 - 0.042| = 0$.\nThe error estimator $\\mathrm{est}$ will be non-zero since the order-2 method is not exact for cubics.\n$R = \\mathrm{err}/\\mathrm{est} = 0 / (\\text{non-zero}) = 0$.\nThe condition $0  10$ is false.\nOutcome: `false`.\n\n**Test 2:** $a=2, b=20, t_c=0.5, h_0=0.6, \\Theta=10$.\nThe discontinuity at $t_c=0.5$ is within the step $(0, 0.6)$.\nExact solution: $h_0  t_c$.\n$y_{\\star}(0.6) = \\frac{2 \\cdot (0.5)^3}{6} + \\frac{20 \\cdot (0.6-0.5)^3}{6} = \\frac{2 \\cdot 0.125 + 20 \\cdot 0.001}{6} = \\frac{0.25+0.02}{6} = \\frac{0.27}{6} = 0.045$.\nStages: $h_0=0.6, t_c=0.5$.\n$k_1 = g(0.0) = \\frac{2}{2}(0-0.5)^2 = 0.25$. ($0.0 \\le 0.5$)\n$k_2 = g(0.3) = \\frac{2}{2}(0.3-0.5)^2 = 0.04$. ($0.3 \\le 0.5$)\n$k_3 = g(0.45) = \\frac{2}{2}(0.45-0.5)^2 = 0.0025$. ($0.45 \\le 0.5$)\n$k_4 = g(0.6) = \\frac{20}{2}(0.6-0.5)^2 = 10 \\cdot 0.01 = 0.1$. ($0.6  0.5$)\n$y^{[3]}(0.6) = 0.6 \\left( \\frac{2}{9}(0.25) + \\frac{1}{3}(0.04) + \\frac{4}{9}(0.0025) \\right) = 0.6 \\left(\\frac{0.63}{9}\\right) = 0.6 \\cdot 0.07 = 0.042$.\n$y^{[2]}(0.6) = 0.6 \\left( \\frac{7}{24}(0.25) + \\frac{1}{4}(0.04) + \\frac{1}{3}(0.0025) + \\frac{1}{8}(0.1) \\right) = 0.6 \\cdot 0.09625 = 0.05775$.\n$\\mathrm{err} = |0.042 - 0.045| = 0.003$.\n$\\mathrm{est} = |0.042 - 0.05775| = 0.01575$.\n$R = 0.003 / 0.01575 = 4/21 \\approx 0.190476$.\nThe condition $0.190476  10$ is false.\nOutcome: `false`.\n\n**Test 3:** $a=2, b=-8, t_c=0.5, h_0=0.6, \\Theta=10$.\nThe discontinuity at $t_c=0.5$ is within the step $(0, 0.6)$.\nExact solution: $h_0  t_c$.\n$y_{\\star}(0.6) = \\frac{2 \\cdot (0.5)^3}{6} + \\frac{-8 \\cdot (0.6-0.5)^3}{6} = \\frac{0.25 - 0.008}{6} = \\frac{0.242}{6} \\approx 0.040333...$.\nStages: $k_1, k_2, k_3$ are the same as in Test 2.\n$k_1=0.25, k_2=0.04, k_3=0.0025$.\n$k_4 = g(0.6) = \\frac{-8}{2}(0.6-0.5)^2 = -4 \\cdot 0.01 = -0.04$. ($0.6  0.5$)\n$y^{[3]}(0.6) = 0.042$ (same as Test 2).\n$y^{[2]}(0.6) = 0.6 \\left( \\frac{7}{24}(0.25) + \\frac{1}{4}(0.04) + \\frac{1}{3}(0.0025) + \\frac{1}{8}(-0.04) \\right) = 0.6 \\cdot 0.07875 = 0.04725$.\n$\\mathrm{err} = |0.042 - 0.242/6| = |\\frac{0.252-0.242}{6}| = \\frac{0.01}{6} \\approx 0.001667...$.\n$\\mathrm{est} = |0.042 - 0.04725| = 0.00525$.\n$R = (\\frac{0.01}{6}) / 0.00525 = \\frac{0.01}{0.0315} = \\frac{20}{63} \\approx 0.31746$.\nThe condition $0.31746  10$ is false.\nOutcome: `false`.\n\n**Test 4:** $a=2, b=20, t_c=0.6, h_0=0.6, \\Theta=10$.\nThe discontinuity is at the endpoint of the step, $t_c=h_0$.\nFor all stage evaluations $t_i = c_i h_0 \\in [0, h_0]$, we have $t_i \\le t_c$. Thus, only parameter $a=2$ is ever used. The problem behaves identically to a smooth case with $g(t) = (t-0.6)^2$.\nExact solution: $h_0 = t_c$.\n$y_{\\star}(0.6) = \\frac{2}{6}((0.6-0.6)^3 + (0.6)^3) = \\frac{1}{3}(0.6)^3 = 0.072$.\nThe solution is again a cubic polynomial. The order-3 method is exact.\n$y^{[3]}(0.6) = y_{\\star}(0.6) = 0.072$.\n$\\mathrm{err} = |0.072 - 0.072| = 0$.\nAs in Test 1, $\\mathrm{est}$ is non-zero.\n$R = 0$.\nThe condition $0  10$ is false.\nOutcome: `false`.\n\nFinal results for all tests are `false`.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n# from scipy import ...\n\ndef solve():\n    \"\"\"\n    Solves the problem for the given test suite.\n    \"\"\"\n\n    # Define the Bogacki-Shampine 3(2) coefficients.\n    c = np.array([0, 1/2, 3/4, 1])\n    b = np.array([2/9, 1/3, 4/9, 0])\n    b_hat = np.array([7/24, 1/4, 1/3, 1/8])\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # a, b, tc, h0, Theta\n        (2.0, 2.0, 0.5, 0.6, 10.0),\n        (2.0, 20.0, 0.5, 0.6, 10.0),\n        (2.0, -8.0, 0.5, 0.6, 10.0),\n        (2.0, 20.0, 0.6, 0.6, 10.0),\n    ]\n\n    results = []\n\n    def get_g_function(a, b, tc):\n        \"\"\"Returns the function g(t) for a given set of parameters.\"\"\"\n        def g(t):\n            s_t = a if t = tc else b\n            return 0.5 * s_t * (t - tc)**2\n        return g\n\n    def get_y_star_function(a, b, tc):\n        \"\"\"Returns the exact solution function y_star(t).\"\"\"\n        def y_star(t):\n            if t = tc:\n                return (a / 6.0) * ((t - tc)**3 + tc**3)\n            else:\n                return (a * tc**3 / 6.0) + (b / 6.0) * (t - tc)**3\n        return y_star\n\n    for case in test_cases:\n        a, b_param, tc, h0, Theta = case\n        \n        g_func = get_g_function(a, b_param, tc)\n        y_star_func = get_y_star_function(a, b_param, tc)\n\n        # 1. Compute the RK stages k_i\n        k = np.zeros(4)\n        for i in range(4):\n            k[i] = g_func(c[i] * h0)\n\n        # 2. Compute the order-3 and order-2 approximations\n        # y(0) = 0, so we just have the sum term.\n        y3 = h0 * np.dot(b, k)\n        y2 = h0 * np.dot(b_hat, k)\n\n        # 3. Compute exact solution\n        y_exact = y_star_func(h0)\n        \n        # 4. Compute error, estimator, and ratio R\n        err = abs(y3 - y_exact)\n        est = abs(y3 - y2)\n        \n        if est == 0.0:\n            # According to the problem, if est=0, R is +inf.\n            # If err  0, then R is effectively infinite and R  Theta is true (for Theta  0).\n            # If err = 0 as well, R is indeterminate 0/0. Here, it will never cause a failure.\n            # In our case, est is non-zero for all tests.\n            R = float('inf') if err  0 else 0.0\n        else:\n            R = err / est\n            \n        # 5. Determine the boolean failure outcome\n        failure = R  Theta\n        results.append(str(failure).lower())\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "2388671"}]}