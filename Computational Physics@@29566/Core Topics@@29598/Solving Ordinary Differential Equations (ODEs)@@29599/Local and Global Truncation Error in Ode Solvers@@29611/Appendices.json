{"hands_on_practices": [{"introduction": "To build a solid understanding of truncation error, our first practice involves a foundational exercise. We will implement and compare two of the most famous numerical methods, the first-order explicit Euler method and the fourth-order Runge-Kutta method, on a simple test equation. This practice [@problem_id:2447459] will not only allow you to empirically verify the theoretical orders of accuracy but also to discover a fundamental limit of computation: the point where decreasing the step size to reduce truncation error is counteracted by the accumulation of round-off error from finite-precision arithmetic.", "problem": "You will implement and analyze two one-step numerical integration methods for the initial value problem defined by the ordinary differential equation $y^{\\prime}(t)=-y(t)$ with initial condition $y(0)=1$ on the interval $t\\in[0,1]$. The exact solution is $y(t)=e^{-t}$. Your task is to compare truncation error orders and to reveal the interaction between truncation error and round-off error by controlling both the step size and the floating-point format. Begin from the fundamental definition that local truncation error is the error committed by one step of the method when applied to the exact solution, and that global error is the deviation between the numerical and exact solutions at the final time. Use only the definitions of the explicit Euler method and the classical fourth-order Runge–Kutta method; do not assume any pre-derived error order formulas. You should reason from Taylor expansions and consistency of one-step methods to characterize error orders, and then confirm them computationally.\n\nImplement the following components:\n- A function that advances one step using the explicit Euler method and another using the classical fourth-order Runge–Kutta method. Each integrator must operate in a specified floating-point format: binary64 (double precision) and binary32 (single precision).\n- A driver that, for a given step size $h$ such that $1/h$ is an integer, applies the chosen method from $t=0$ to $t=1$ starting at $y(0)=1$ and returns the absolute global error at $t=1$, namely $\\lvert y_{N}-e^{-1}\\rvert$ where $N=1/h$.\n- A function to estimate the observed order of accuracy by fitting a straight line to $\\log(\\text{error})$ versus $\\log(h)$ using least squares over a provided list of step sizes.\n\nDesign your implementation so that the numerical updates, constants, and intermediate quantities are computed in the requested floating-point format. Use the exact solution $y(1)=e^{-1}$ as a reference value in binary64 when computing errors.\n\nTest Suite:\nCompute the following six results, in this exact order, using the specified step sizes and formats:\n1. The absolute global error at $t=1$ for the explicit Euler method with step size $h=0.1$ in binary64.\n2. The absolute global error at $t=1$ for the classical fourth-order Runge–Kutta method with step size $h=0.1$ in binary64.\n3. The observed order of accuracy for the explicit Euler method in binary64, estimated from the step sizes $h\\in\\{0.2,0.1,0.05,0.025\\}$.\n4. The observed order of accuracy for the classical fourth-order Runge–Kutta method in binary64, estimated from the step sizes $h\\in\\{0.2,0.1,0.05,0.025\\}$.\n5. Using the classical fourth-order Runge–Kutta method in binary32, compute the absolute global error at $t=1$ for the step sizes $h\\in\\{2^{-3},2^{-5},2^{-7},2^{-9},2^{-11},2^{-13}\\}$. Return the zero-based index $k^{\\ast}$ at which this error attains its minimum over the list. This index should reflect the step size at which truncation error and round-off error balance.\n6. Using the classical fourth-order Runge–Kutta method in binary64, compute the absolute global error at $t=1$ for step sizes $h\\in\\{2^{-6},2^{-7},2^{-8},2^{-9}\\}$. Return a boolean indicating whether these errors are strictly decreasing as $h$ decreases.\n\nFinal Output Format:\nYour program should produce a single line of output containing the six results in a comma-separated list enclosed in square brackets, in the exact order specified above. The list must contain two floats for the first two errors, two floats for the two observed orders, one integer for the index, and one boolean for the monotonicity check, for a total of six entries in one list.", "solution": "The problem presented is a standard exercise in numerical analysis for ordinary differential equations. It is scientifically sound, well-posed, and contains all necessary information for a complete solution. We shall proceed with the analysis and implementation.\n\nThe initial value problem (IVP) under consideration is given by the ordinary differential equation (ODE):\n$$\ny^{\\prime}(t) = -y(t), \\quad t \\in [0, 1]\n$$\nwith the initial condition $y(0) = 1$. The analytical solution to this IVP is $y(t) = e^{-t}$. We are tasked with solving this problem numerically using two different one-step methods and analyzing their error characteristics.\n\nA general one-step method for the IVP $y^{\\prime}(t) = f(t, y(t))$ approximates the solution at discrete time points $t_n = t_0 + nh$, where $h$ is the step size. The numerical solution $y_n \\approx y(t_n)$ is advanced from one step to the next by a formula of the form:\n$$\ny_{n+1} = y_n + h \\Phi(t_n, y_n, h)\n$$\nwhere $\\Phi$ is the increment function that defines the method.\n\nThe first method is the explicit Euler method. It is derived from the first-order Taylor expansion of $y(t_{n+1})$ about $t_n$:\n$$\ny(t_{n+1}) = y(t_n) + h y^{\\prime}(t_n) + \\mathcal{O}(h^2)\n$$\nBy substituting $y^{\\prime}(t_n) = f(t_n, y(t_n))$ and truncating terms of order $\\mathcal{O}(h^2)$ and higher, we obtain the method:\n$$\ny_{n+1} = y_n + h f(t_n, y_n)\n$$\nFor the given problem, $f(t, y) = -y$, so the explicit Euler update rule is:\n$$\ny_{n+1} = y_n - h y_n = (1 - h) y_n\n$$\n\nThe second method is the classical fourth-order Runge–Kutta method (RK4). It is defined by the following set of equations:\n$$\ny_{n+1} = y_n + \\frac{h}{6}(k_1 + 2k_2 + 2k_3 + k_4)\n$$\nwhere the stages $k_i$ are computed as:\n$$\n\\begin{aligned}\nk_1 &= f(t_n, y_n) \\\\\nk_2 &= f\\left(t_n + \\frac{h}{2}, y_n + \\frac{h}{2}k_1\\right) \\\\\nk_3 &= f\\left(t_n + \\frac{h}{2}, y_n + \\frac{h}{2}k_2\\right) \\\\\nk_4 &= f(t_n + h, y_n + h k_3)\n\\end{aligned}\n$$\nFor our problem where $f(t, y) = -y$, these stages become:\n$$\n\\begin{aligned}\nk_1 &= -y_n \\\\\nk_2 &= -\\left(y_n + \\frac{h}{2}(-y_n)\\right) = -y_n\\left(1 - \\frac{h}{2}\\right) \\\\\nk_3 &= -\\left(y_n + \\frac{h}{2}k_2\\right) = -y_n\\left(1 - \\frac{h}{2} + \\frac{h^2}{4}\\right) \\\\\nk_4 &= -(y_n + hk_3) = -y_n\\left(1 - h + \\frac{h^2}{2} - \\frac{h^3}{4}\\right)\n\\end{aligned}\n$$\nSubstituting these into the RK4 formula and simplifying yields:\n$$\ny_{n+1} = y_n\\left(1 - h + \\frac{h^2}{2} - \\frac{h^3}{6} + \\frac{h^4}{24}\\right)\n$$\nThis expression is precisely the first five terms of the Taylor series expansion of $y_n e^{-h}$, since $e^{-h} = 1 - h + \\frac{h^2}{2!} - \\frac{h^3}{3!} + \\frac{h^4}{4!} - \\frac{h^5}{5!} + \\dots$.\n\nThe accuracy of a numerical method is characterized by its order. The local truncation error (LTE) is the error introduced in a single step, assuming the method starts with the exact solution $y(t_n)$. For a method of order $p$, the LTE is $\\mathcal{O}(h^{p+1})$. The global error, which is the cumulative error at the end of the integration interval, is then $\\mathcal{O}(h^p)$.\n\nFor the explicit Euler method, the LTE is:\n$$\nLTE_{n+1} = y(t_{n+1}) - \\left[y(t_n) + h f(t_n, y(t_n))\\right] = \\left[y(t_n) + h y^{\\prime}(t_n) + \\frac{h^2}{2}y^{\\prime\\prime}(\\xi)\\right] - \\left[y(t_n) + h y^{\\prime}(t_n)\\right] = \\frac{h^2}{2}y^{\\prime\\prime}(\\xi)\n$$\nfor some $\\xi \\in (t_n, t_{n+1})$. The LTE is $\\mathcal{O}(h^2)$, which means the Euler method is first-order accurate, so its global error behaves as $\\mathcal{O}(h^1)$.\n\nFor the RK4 method, the update rule for our specific problem matches the Taylor series of $e^{-h}$ up to the $h^4$ term. The error in one step is therefore dominated by the next term in the series:\n$$\nLTE_{n+1} = y(t_n)e^{-h} - y(t_n)\\left(1 - h + \\frac{h^2}{2} - \\frac{h^3}{6} + \\frac{h^4}{24}\\right) = y(t_n)\\left(-\\frac{h^5}{120} + \\mathcal{O}(h^6)\\right)\n$$\nThe LTE is $\\mathcal{O}(h^5)$, so the RK4 method is fourth-order accurate, with global error behaving as $\\mathcal{O}(h^4)$.\n\nTo computationally verify the order $p$, we assume the global error $E$ at the final time follows $E \\approx C h^p$ for some constant $C$. Taking the logarithm of both sides gives:\n$$\n\\log(E) \\approx \\log(C) + p \\log(h)\n$$\nThis shows a linear relationship between $\\log(E)$ and $\\log(h)$, with the slope being the order of accuracy $p$. We can estimate $p$ by computing the error for a sequence of step sizes and performing a linear least-squares fit on the log-log data.\n\nFinally, we must consider the effect of finite-precision arithmetic. The total numerical error is a sum of the truncation error (from the method's approximation) and round-off error (from floating-point representation). The truncation error decreases as $h$ decreases (e.g., $\\propto h^p$). The round-off error, however, tends to accumulate with the number of steps, $N = T/h$. The accumulated round-off error can be modeled as being proportional to $\\epsilon/h$, where $\\epsilon$ is the machine epsilon of the floating-point format. The total error is approximately:\n$$\nE_{total} \\approx C h^p + \\frac{D\\epsilon}{h}\n$$\nThis function has a minimum for some optimal step size $h^*$. For $h > h^*$, truncation error dominates and the error decreases with $h$. For $h  h^*$, round-off error dominates and the error increases as $h$ decreases. This effect is more pronounced in single precision (binary32, $\\epsilon \\approx 10^{-8}$) than in double precision (binary64, $\\epsilon \\approx 10^{-16}$), as the larger $\\epsilon$ results in a larger optimal $h^*$. This is precisely what the test cases are designed to investigate.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef euler_step(y, h, dtype):\n    \"\"\"\n    Performs one step of the explicit Euler method for y' = -y.\n    All calculations are performed in the specified dtype.\n    \n    Args:\n        y: Current value of the solution (of type dtype).\n        h: Step size (Python float).\n        dtype: The numpy floating-point type (np.float32 or np.float64).\n        \n    Returns:\n        The next value of the solution (of type dtype).\n    \"\"\"\n    h_typed = dtype(h)\n    one = dtype(1.0)\n    # The update is y_{n+1} = y_n * (1 - h)\n    return y * (one - h_typed)\n\ndef rk4_step(y, h, dtype):\n    \"\"\"\n    Performs one step of the classical 4th-order Runge-Kutta method for y' = -y.\n    All calculations are performed in the specified dtype.\n    \n    Args:\n        y: Current value of the solution (of type dtype).\n        h: Step size (Python float).\n        dtype: The numpy floating-point type (np.float32 or np.float64).\n        \n    Returns:\n        The next value of the solution (of type dtype).\n    \"\"\"\n    h_typed = dtype(h)\n    \n    # Define constants in the target precision\n    c_half = dtype(0.5)\n    c_two = dtype(2.0)\n    c_six = dtype(6.0)\n\n    # ODE is y' = f(y) = -y\n    k1 = -y\n    k2 = -(y + c_half * h_typed * k1)\n    k3 = -(y + c_half * h_typed * k2)\n    k4 = -(y + h_typed * k3)\n    \n    return y + (h_typed / c_six) * (k1 + c_two * k2 + c_two * k3 + k4)\n\ndef solve_ode(step_func, h, dtype):\n    \"\"\"\n    Solves the ODE y'=-y from t=0 to t=1 with y(0)=1.\n    \n    Args:\n        step_func: The function for a single integration step (e.g., euler_step).\n        h: The step size.\n        dtype: The numpy floating-point type for computation.\n        \n    Returns:\n        The absolute global error at t=1.\n    \"\"\"\n    # The problem statement guarantees 1/h is an integer.\n    # Use round() to be robust against floating point inaccuracies, e.g., 1.0/0.1\n    num_steps = int(round(1.0 / h))\n    \n    # Initial condition y(0)=1, cast to the specified precision\n    y = dtype(1.0)\n\n    for _ in range(num_steps):\n        y = step_func(y, h, dtype)\n\n    # Reference solution y(1)=e^-1 in double precision (binary64)\n    y_exact_64 = np.exp(np.float64(-1.0))\n    \n    # Error is computed against the high-precision reference.\n    # The subtraction will promote the result to float64, which is desired.\n    return np.abs(y - y_exact_64)\n\ndef estimate_order(step_func, h_list, dtype):\n    \"\"\"\n    Estimates the order of accuracy of a method by log-log linear regression.\n    \n    Args:\n        step_func: The single-step integration function.\n        h_list: A list of step sizes to use.\n        dtype: The numpy floating-point type.\n        \n    Returns:\n        The estimated order of accuracy (slope of the log-log plot).\n    \"\"\"\n    h_array = np.array(h_list, dtype=np.float64)\n    errors = np.array([solve_ode(step_func, h, dtype) for h in h_list], dtype=np.float64)\n\n    log_h = np.log(h_array)\n    log_err = np.log(errors)\n\n    # Perform linear least squares to find the slope p\n    # of the line log(error) = log(C) + p * log(h)\n    # Using the standard formula for the slope of a simple linear regression.\n    n = len(log_h)\n    sum_xy = np.sum(log_h * log_err)\n    sum_x = np.sum(log_h)\n    sum_y = np.sum(log_err)\n    sum_x2 = np.sum(log_h**2)\n    \n    slope_p = (n * sum_xy - sum_x * sum_y) / (n * sum_x2 - sum_x**2)\n    return slope_p\n\ndef solve():\n    \"\"\"\n    Main function to compute the six required results for the test suite.\n    \"\"\"\n    # Test 1: Global error for Euler, h=0.1, binary64\n    error1 = solve_ode(euler_step, 0.1, np.float64)\n\n    # Test 2: Global error for RK4, h=0.1, binary64\n    error2 = solve_ode(rk4_step, 0.1, np.float64)\n\n    # Test 3: Observed order for Euler, binary64\n    h_order_list = [0.2, 0.1, 0.05, 0.025]\n    order3 = estimate_order(euler_step, h_order_list, np.float64)\n\n    # Test 4: Observed order for RK4, binary64\n    order4 = estimate_order(rk4_step, h_order_list, np.float64)\n    \n    # Test 5: Index of minimum error for RK4, binary32 (truncation vs. round-off)\n    h_balance_list = [2.0**-k for k in [3, 5, 7, 9, 11, 13]]\n    errors_balance = [solve_ode(rk4_step, h, np.float32) for h in h_balance_list]\n    index5 = np.argmin(errors_balance)\n    \n    # Test 6: Monotonicity check for RK4, binary64\n    h_mono_list = [2.0**-k for k in [6, 7, 8, 9]]\n    errors_mono = [solve_ode(rk4_step, h, np.float64) for h in h_mono_list]\n    # Check if errors are strictly decreasing: e[i]  e[i+1] for all i\n    bool6 = all(np.diff(errors_mono)  0)\n\n    results = [error1, error2, order3, order4, index5, bool6]\n    \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2447459"}, {"introduction": "Having established how to measure error, we now explore its qualitative nature in physical simulations. The Global Truncation Error is not just a number; it represents a deviation from the true physics, and different methods deviate in characteristically different ways. In this exercise [@problem_id:2409167], you will simulate a simple harmonic oscillator using two methods and discover that their errors have distinct physical manifestations: the explicit Euler method introduces an amplitude error that violates energy conservation, while the Leapfrog method primarily accumulates a phase error that preserves the system's energy structure.", "problem": "Consider the undamped simple harmonic oscillator (SHO) governed by the ordinary differential equation (ODE) $m\\,\\ddot{x}(t)+k\\,x(t)=0$, with $m$ the mass and $k$ the stiffness. Let $m=k=1$ so that the angular frequency is $\\omega=\\sqrt{k/m}=1$ (in radians per second). Use the initial conditions $x(0)=A$ and $v(0)=\\dot{x}(0)=0$, with amplitude $A=1$ (in meters). The exact displacement is $x_{\\text{exact}}(t)=A\\cos(\\omega t)$ and the exact velocity is $v_{\\text{exact}}(t)=-A\\,\\omega\\sin(\\omega t)$. Investigate how the global truncation error (GTE) and the local truncation error (LTE) of two one-step time integrators manifest for this system: the explicit Euler method and the Leapfrog method. The explicit Euler method is defined by the updates\n$$\nx_{n+1}=x_n+h\\,v_n,\\quad v_{n+1}=v_n-h\\,\\omega^2\\,x_n,\n$$\nwhere $h$ is the fixed time step and $n\\in\\{0,1,2,\\dots,N-1\\}$ with $t_n=n\\,h$ and $t_N=T=N\\,h$. The Leapfrog method (velocity Verlet form) is defined by\n$$\na_n=-\\omega^2 x_n,\\quad v_{n+\\frac{1}{2}}=v_n+\\frac{h}{2}a_n,\\quad x_{n+1}=x_n+h\\,v_{n+\\frac{1}{2}},\n$$\n$$\na_{n+1}=-\\omega^2 x_{n+1},\\quad v_{n+1}=v_{n+\\frac{1}{2}}+\\frac{h}{2}a_{n+1}.\n$$\nFor each method, integrate from $t=0$ to $t=T$ with a uniform time step $h$ so that $T=N\\,h$ for some integer $N$.\n\nDefine the amplitude error of the explicit Euler method at $t=T$ by\n$$\nE_A=\\sqrt{x_N^2+\\left(\\frac{v_N}{\\omega}\\right)^2}-A,\n$$\nexpressed in meters. Define the signed phase error of the Leapfrog method at $t=T$ by\n$$\nE_\\phi=\\operatorname{wrap}_{(-\\pi,\\pi]}\\!\\left(\\theta_{\\text{num}}-\\theta_{\\text{exact}}\\right),\n$$\nwhere the numerical phase is\n$$\n\\theta_{\\text{num}}=\\operatorname{atan2}\\!\\left(-\\frac{v_N}{\\omega},\\,x_N\\right),\n$$\nthe exact phase is $\\theta_{\\text{exact}}=\\omega T$, and $\\operatorname{wrap}_{(-\\pi,\\pi]}(\\alpha)$ maps any real $\\alpha$ to its representative in $(-\\pi,\\pi]$ by adding or subtracting integer multiples of $2\\pi$. The angle unit must be radians.\n\nYour program must compute, for each test case below, the pair $[E_A,E_\\phi]$ using the definitions above, with $A=1$ (in meters) and $\\omega=1$ (in radians per second) for all cases. The test suite consists of the following parameter sets, each specified by $(h,T)$ in seconds, with $T=N\\,h$:\n- Case $1$: $h=0.1$, $T=50$.\n- Case $2$: $h=0.01$, $T=50$.\n- Case $3$: $h=0.5$, $T=50$.\n\nFinal output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is the two-entry list $[E_A,E_\\phi]$ for one case, in the same order as above. For example, a valid shape is $[[\\dots,\\dots],[\\dots,\\dots],[\\dots,\\dots]]$. Angles must be in radians and distances in meters. No additional text should be printed.", "solution": "The problem presented is valid. It is a well-posed, scientifically grounded problem in computational physics that investigates the fundamental error characteristics of two standard numerical integrators for ordinary differential equations. All parameters, conditions, and definitions are provided, and there are no internal contradictions or logical flaws. We shall proceed with the solution.\n\nThe problem requires the analysis of two numerical integration schemes, the explicit Euler method and the Leapfrog method, applied to the simple harmonic oscillator (SHO). The governing equation of motion is\n$$\nm\\,\\ddot{x}(t) + k\\,x(t) = 0\n$$\nWith the given parameters, mass $m=1$ and stiffness $k=1$, the angular frequency is $\\omega = \\sqrt{k/m} = 1\\,\\text{rad/s}$. The system is described by the state vector $(x(t), v(t))$, where $v(t) = \\dot{x}(t)$. The initial conditions are $x(0) = A = 1$ and $v(0) = 0$. The exact solution is given by\n$$\nx_{\\text{exact}}(t) = A\\cos(\\omega t) \\quad \\text{and} \\quad v_{\\text{exact}}(t) = -A\\omega\\sin(\\omega t)\n$$\nThe total energy of the system, $E = \\frac{1}{2}mv^2 + \\frac{1}{2}kx^2$, is a conserved quantity. For the given parameters, this is $E(t) = \\frac{1}{2}(v(t)^2 + x(t)^2)$. The initial energy is $E(0) = \\frac{1}{2}(0^2 + 1^2) = 1/2$. The amplitude of the oscillation is related to the energy by $A = \\sqrt{x^2 + (v/\\omega)^2}$. Thus, conservation of energy is equivalent to conservation of amplitude.\n\nWe will now analyze the behavior of each specified numerical method.\n\n**Explicit Euler Method**\n\nThe explicit Euler method is a first-order integrator. For the SHO, with acceleration $a(t) = \\ddot{x}(t) = -\\omega^2 x(t)$, the update rules are:\n$$\nx_{n+1} = x_n + h\\,v_n\n$$\n$$\nv_{n+1} = v_n + h\\,a_n = v_n - h\\,\\omega^2\\,x_n\n$$\nwhere $h$ is the time step. This method is not symplectic, and for oscillatory systems, it is known to be numerically unstable as it artificially increases the system's energy over time. Let us analyze the numerical energy at step $n+1$:\n$$\nE_{n+1} = \\frac{1}{2}(v_{n+1}^2 + \\omega^2 x_{n+1}^2) = \\frac{1}{2}((v_n - h\\omega^2 x_n)^2 + \\omega^2(x_n + h v_n)^2)\n$$\nExpanding the terms, we find:\n$$\nE_{n+1} = \\frac{1}{2}(v_n^2 - 2h\\omega^2 x_n v_n + h^2\\omega^4 x_n^2 + \\omega^2(x_n^2 + 2h x_n v_n + h^2 v_n^2))\n$$\n$$\nE_{n+1} = \\frac{1}{2}(v_n^2 + h^2\\omega^4 x_n^2 + \\omega^2 x_n^2 + \\omega^2 h^2 v_n^2) = \\frac{1}{2}((1+\\omega^2h^2)v_n^2 + \\omega^2(1+\\omega^2h^2)x_n^2)\n$$\n$$\nE_{n+1} = (1+\\omega^2h^2) \\left(\\frac{1}{2}(v_n^2 + \\omega^2 x_n^2)\\right) = (1+\\omega^2h^2)E_n\n$$\nAfter $N$ steps to time $T=N\\,h$, the energy becomes $E_N = (1+\\omega^2h^2)^N E_0$. Since the amplitude is $A = \\sqrt{2E/\\omega^2}$, the numerical amplitude $A_N$ at step $N$ is related to the initial amplitude $A_0$ by:\n$$\nA_N = A_0 (1+\\omega^2h^2)^{N/2}\n$$\nThe amplitude error, $E_A = A_N - A_0$, is therefore given by $E_A = A_0 \\left[(1+\\omega^2h^2)^{N/2} - 1\\right]$. This demonstrates an exponential growth in amplitude, a hallmark of this method's instability for conservative systems. The amplitude error $E_A$ is calculated using the final state $(x_N, v_N)$ as specified: $E_A = \\sqrt{x_N^2 + (v_N/\\omega)^2} - A$.\n\n**Leapfrog Method (Velocity Verlet Form)**\n\nThe Leapfrog method is a second-order, symplectic integrator. Its velocity Verlet form is given by:\n$$\nv_{n+\\frac{1}{2}} = v_n + \\frac{h}{2}a_n \\quad \\text{where} \\quad a_n=-\\omega^2 x_n\n$$\n$$\nx_{n+1} = x_n + h\\,v_{n+\\frac{1}{2}}\n$$\n$$\nv_{n+1} = v_{n+\\frac{1}{2}} + \\frac{h}{2}a_{n+1} \\quad \\text{where} \\quad a_{n+1}=-\\omega^2 x_{n+1}\n$$\nBeing symplectic, this method does not exhibit secular drift in energy. The numerical energy will oscillate around the true energy, leading to excellent long-term stability and bounded amplitude error. However, the method introduces a systematic error in the phase of the oscillation. The numerical frequency, $\\tilde{\\omega}$, is slightly different from the true frequency $\\omega$. This leads to a phase error that accumulates over time. For small step sizes $h$, the global phase error at time $T$ is on the order of $O(T\\omega^3h^2)$.\nThe problem defines the phase error $E_\\phi$ based on a state-space representation $(x, -v/\\omega)$. For the exact solution, the state vector $(x_{\\text{exact}}(t), -v_{\\text{exact}}(t)/\\omega) = (A\\cos(\\omega t), A\\sin(\\omega t))$ rotates counter-clockwise with an angle $\\theta_{\\text{exact}}(t) = \\omega t$. The numerical phase $\\theta_{\\text{num}}$ is computed from the final numerical state $(x_N, v_N)$ as $\\theta_{\\text{num}} = \\operatorname{atan2}(-v_N/\\omega, x_N)$. The phase error is the difference between the numerical and exact phase, wrapped to the interval $(-\\pi, \\pi]$:\n$$\nE_\\phi = \\operatorname{wrap}_{(-\\pi,\\pi]}(\\theta_{\\text{num}} - \\theta_{\\text{exact}})\n$$\nThis calculation correctly captures the phase lag or lead of the numerical solution. The wrapping is required because phase is a periodic quantity.\n\n**Computational Procedure**\n\nFor each given test case $(h, T)$:\n1.  Set initial conditions $x_0 = 1$, $v_0 = 0$ and parameters $A=1$, $\\omega=1$.\n2.  Calculate the number of integration steps $N = T/h$.\n3.  Simulate the system using the explicit Euler method for $N$ steps to find the final state $(x_N, v_N)_{\\text{Euler}}$.\n4.  Compute the amplitude error $E_A = \\sqrt{x_{N,\\text{Euler}}^2 + (v_{N,\\text{Euler}}/\\omega)^2} - A$.\n5.  Simulate the system using the Leapfrog method for $N$ steps to find the final state $(x_N, v_N)_{\\text{Leapfrog}}$.\n6.  Compute the numerical phase $\\theta_{\\text{num}} = \\operatorname{atan2}(-v_{N,\\text{Leapfrog}}/\\omega, x_{N,\\text{Leapfrog}})$, the exact phase $\\theta_{\\text{exact}} = \\omega T$, and the wrapped phase error $E_\\phi$. The wrapping function maps a real number $\\alpha$ to the interval $(-\\pi, \\pi]$ using the formula $\\beta = \\alpha - 2\\pi \\lceil(\\alpha - \\pi)/(2\\pi)\\rceil$.\n7.  Combine the results into a single pair $[E_A, E_\\phi]$.\n\nThis procedure will be implemented to generate the required output for all test cases. The calculated values will serve as a concrete demonstration of the theoretical error behaviors discussed above.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes amplitude and phase errors for Euler and Leapfrog methods\n    for a simple harmonic oscillator.\n    \"\"\"\n    \n    # Test cases defined by (h, T) in seconds.\n    test_cases = [\n        (0.1, 50.0),\n        (0.01, 50.0),\n        (0.5, 50.0),\n    ]\n\n    # Constants from the problem statement\n    A = 1.0  # meters\n    omega = 1.0  # radians per second\n\n    results = []\n    \n    # Custom wrap function to map angle to (-pi, pi]\n    def wrap_to_pi(alpha):\n        \"\"\"Maps an angle alpha in radians to the interval (-pi, pi].\"\"\"\n        return alpha - 2 * np.pi * np.ceil((alpha - np.pi) / (2 * np.pi))\n\n    for h, T in test_cases:\n        N = int(T / h)\n        if not np.isclose(N * h, T):\n            # This should not happen with the given test cases\n            raise ValueError(\"T must be an integer multiple of h.\")\n\n        # --- Explicit Euler Simulation ---\n        x_euler, v_euler = A, 0.0\n        for _ in range(N):\n            x_next = x_euler + h * v_euler\n            v_next = v_euler - h * (omega**2) * x_euler\n            x_euler, v_euler = x_next, v_next\n        \n        # Calculate amplitude error for Euler method\n        amplitude_numerical_euler = np.sqrt(x_euler**2 + (v_euler / omega)**2)\n        E_A = amplitude_numerical_euler - A\n\n        # --- Leapfrog (Velocity Verlet) Simulation ---\n        x_leap, v_leap = A, 0.0\n        # Initial acceleration\n        a_n = -(omega**2) * x_leap\n        for _ in range(N):\n            v_half = v_leap + 0.5 * h * a_n\n            x_next = x_leap + h * v_half\n            a_next = -(omega**2) * x_next\n            v_next = v_half + 0.5 * h * a_next\n            \n            x_leap, v_leap = x_next, v_next\n            a_n = a_next\n\n        # Calculate signed phase error for Leapfrog method\n        theta_num = np.arctan2(-v_leap / omega, x_leap)\n        theta_exact = omega * T\n        phase_diff = theta_num - theta_exact\n        E_phi = wrap_to_pi(phase_diff)\n        \n        results.append([E_A, E_phi])\n\n    # Format the output string exactly as required\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2409167"}, {"introduction": "Our final practice addresses one of the most significant challenges in the numerical solution of ODEs: stiffness. In stiff systems, there are multiple time scales, and this can lead to surprising behavior in numerical integrators. This exercise [@problem_id:2409172] uses a classic stiff test problem to demonstrate a crucial principle: for explicit methods, the choice of step size is often dictated not by the pursuit of local accuracy, but by a strict stability requirement. You will observe how violating this stability condition, even with a step size that seems reasonable, causes the global error to explode, rendering the simulation useless.", "problem": "Consider the scalar linear stiff Ordinary Differential Equation (ODE): $y'(t)=-\\lambda y(t)$ with $\\lambda0$ and initial condition $y(0)=1$. The exact solution is $y(t)=e^{-\\lambda t}$. Let a numerical approximation $y_n$ be generated on a uniform grid $t_n = n h$ with a constant time step $h0$ using the explicit forward Euler one-step scheme $y_{n+1}=y_n+h f(t_n,y_n)$, where $f(t,y)=-\\lambda y$, so that after $N$ steps the final time is $T=N h$. Define the one-step Local Truncation Error (LTE) at the initial point as $\\mathrm{LTE}_1=\\left|y(h)-\\left(y(0)+h f(0,y(0))\\right)\\right|$, and define the Global Truncation Error (GTE) at time $T$ as $\\mathrm{GTE}(T)=\\left|y_N-y(T)\\right|$. Also define the ratio $R=\\mathrm{GTE}(T)/\\mathrm{LTE}_1$, and the linear stability indicator $S$ for the explicit method on this problem as the boolean value of the condition $\\left|1-\\lambda h\\right|1$.\n\nYour task is to write a complete program that, for each parameter triplet $(\\lambda,T,h)$ in the test suite below, computes the tuple of quantities $\\left(\\mathrm{GTE}(T),\\mathrm{LTE}_1,R,S\\right)$ using $y(0)=1$ and the definitions above. All calculations are dimensionless. Angles are not involved. Do not round the outputs; produce them as raw floating-point and boolean values.\n\nTest suite (provide results in this exact order):\n- Case $1$: $(\\lambda,T,h)=\\left(200,1.0,0.002\\right)$.\n- Case $2$: $(\\lambda,T,h)=\\left(200,1.0,0.01\\right)$.\n- Case $3$: $(\\lambda,T,h)=\\left(200,0.96,0.012\\right)$.\n- Case $4$: $(\\lambda,T,h)=\\left(500,0.468,0.0039\\right)$.\n- Case $5$: $(\\lambda,T,h)=\\left(20,1.0,0.2\\right)$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each case’s result is itself a list in the form $[\\mathrm{GTE}(T),\\mathrm{LTE}_1,R,S]$. For example, the overall output format must be\n$[[\\mathrm{GTE}_1,\\mathrm{LTE}_{1,1},R_1,S_1],[\\mathrm{GTE}_2,\\mathrm{LTE}_{1,2},R_2,S_2],\\dots]$.", "solution": "The problem presented is a standard, well-posed exercise in the elementary analysis of numerical methods for Ordinary Differential Equations (ODEs). It is scientifically sound, resting on foundational principles of computational physics and numerical analysis, and it provides all necessary definitions and data for a unique, unambiguous solution. We find no flaws; therefore, we proceed with the derivation.\n\nThe problem centers on the scalar linear ODE:\n$$\ny'(t) = -\\lambda y(t)\n$$\nwith initial condition $y(0)=1$ and parameter $\\lambda  0$. The exact solution is given as:\n$$\ny(t) = e^{-\\lambda t}\n$$\nWe are to analyze the explicit forward Euler scheme applied to this equation. The scheme is defined as:\n$$\ny_{n+1} = y_n + h f(t_n, y_n)\n$$\nwhere $h$ is the constant time step and $f(t,y) = -\\lambda y$. Substituting the function $f$ into the scheme provides the iterative relation for the numerical solution $y_n$ at time $t_n = n h$:\n$$\ny_{n+1} = y_n + h(-\\lambda y_n) = (1 - \\lambda h) y_n\n$$\nThis is a first-order linear recurrence relation. Given the initial condition $y_0 = y(0) = 1$, we can derive a closed-form expression for the numerical solution $y_N$ at time $T=Nh$:\n$$\ny_N = (1 - \\lambda h)^N y_0 = (1 - \\lambda h)^N\n$$\nSince $N=T/h$, we have the numerical approximation at the final time $T$:\n$$\ny_N = \\left(1 - \\lambda h\\right)^{T/h}\n$$\nWith this foundation, we can now derive the expressions for the four quantities required by the problem for each parameter set $(\\lambda, T, h)$.\n\n1.  Global Truncation Error, $\\mathrm{GTE}(T)$:\n    The GTE is defined as the absolute difference between the numerical solution and the exact solution at the final time $T$.\n    $$\n    \\mathrm{GTE}(T) = \\left|y_N - y(T)\\right|\n    $$\n    Substituting the expressions for $y_N$ and $y(T)$:\n    $$\n    \\mathrm{GTE}(T) = \\left| \\left(1 - \\lambda h\\right)^{T/h} - e^{-\\lambda T} \\right|\n    $$\n\n2.  Local Truncation Error, $\\mathrm{LTE}_1$:\n    The one-step LTE at the initial point is defined as the error incurred in the first step, assuming the step starts from the exact solution $y(0)$.\n    $$\n    \\mathrm{LTE}_1 = \\left|y(h) - \\left(y(0)+h f(0,y(0))\\right)\\right|\n    $$\n    We substitute the known quantities: $y(h) = e^{-\\lambda h}$, $y(0)=1$, and $f(0, y(0)) = f(0,1) = -\\lambda$.\n    $$\n    \\mathrm{LTE}_1 = \\left|e^{-\\lambda h} - \\left(1 + h(-\\lambda)\\right)\\right| = \\left|e^{-\\lambda h} - (1 - \\lambda h)\\right|\n    $$\n    From the Taylor series expansion of $e^{-\\lambda h} = 1 - \\lambda h + \\frac{(\\lambda h)^2}{2!} + O((\\lambda h)^3)$, it is evident that $\\mathrm{LTE}_1$ is of order $O(h^2)$. Since the local error of a method of order $p$ is $O(h^{p+1})$, this confirms the forward Euler method is first-order ($p=1$).\n\n3.  Ratio, $R$:\n    The ratio $R$ is defined as $R = \\mathrm{GTE}(T)/\\mathrm{LTE}_1$.\n    $$\n    R = \\frac{\\left| \\left(1 - \\lambda h\\right)^{T/h} - e^{-\\lambda T} \\right|}{\\left|e^{-\\lambda h} - (1 - \\lambda h)\\right|}\n    $$\n    This ratio compares the magnitude of the final global error to the magnitude of the initial local error.\n\n4.  Linear Stability Indicator, $S$:\n    The stability of the forward Euler method for this test problem depends on the amplification factor $g(\\lambda h) = 1 - \\lambda h$. For the numerical solution to remain bounded, we require $|g(\\lambda h)| \\leq 1$. The problem specifies a strict inequality for the indicator $S$.\n    $$\n    S = \\text{boolean value of } \\left|1 - \\lambda h\\right|  1\n    $$\n    This inequality is equivalent to $-1  1 - \\lambda h  1$.\n    The right side, $1 - \\lambda h  1$, implies $-\\lambda h  0$, which is always true since $\\lambda  0$ and $h  0$.\n    The left side, $-1  1 - \\lambda h$, implies $\\lambda h  2$.\n    Thus, the stability condition simplifies to $0  \\lambda h  2$. The indicator $S$ is `True` if $\\lambda h  2$ and `False` otherwise.\n\nThese four formulas will be implemented directly to compute the required tuples for each test case.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes GTE, LTE, their ratio, and a stability indicator for the\n    forward Euler method on a stiff ODE for several test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (200.0, 1.0, 0.002),   # Case 1\n        (200.0, 1.0, 0.01),    # Case 2\n        (200.0, 0.96, 0.012),   # Case 3\n        (500.0, 0.468, 0.0039), # Case 4\n        (20.0, 1.0, 0.2),      # Case 5\n    ]\n\n    results = []\n    for case in test_cases:\n        lambda_, T, h = case\n\n        # Number of steps N = T/h. Ensure it is an integer value.\n        # Problem statement guarantees this, but round for robustness.\n        N = int(round(T / h))\n\n        # Calculate lambda * h, a key dimensionless parameter\n        lambda_h = lambda_ * h\n\n        # 1. Global Truncation Error (GTE) at time T\n        # GTE(T) = |y_N - y(T)|\n        # y_N = (1 - lambda*h)^N\n        # y(T) = exp(-lambda*T)\n        y_num_N = (1.0 - lambda_h)**N\n        y_exact_T = np.exp(-lambda_ * T)\n        GTE = np.abs(y_num_N - y_exact_T)\n\n        # 2. Local Truncation Error (LTE) at the first step\n        # LTE_1 = |y(h) - (y(0) + h*f(0,y(0)))|\n        # y(h) = exp(-lambda*h)\n        # y(0) + h*f(0,y(0)) = 1 - lambda*h\n        y_exact_h = np.exp(-lambda_h)\n        y_num_h = 1.0 - lambda_h\n        LTE1 = np.abs(y_exact_h - y_num_h)\n        \n        # 3. Ratio R = GTE(T) / LTE_1\n        # The problem setup ensures LTE1 is never zero.\n        if LTE1 == 0.0:\n            # Handle potential division by zero, though not expected here.\n            R = np.inf if GTE != 0.0 else 0.0\n        else:\n            R = GTE / LTE1\n\n        # 4. Linear Stability Indicator S\n        # S is True if |1 - lambda*h|  1, which simplifies to lambda*h  2.\n        S = lambda_h  2.0\n        \n        # Store the computed tuple\n        result_tuple_str = f\"[{GTE},{LTE1},{R},{S}]\"\n        results.append(result_tuple_str)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "2409172"}]}