{"hands_on_practices": [{"introduction": "A central tenet of stochastic thermodynamics is that thermodynamic quantities like work and heat are not fixed numbers for a single process but are instead random variables described by a probability distribution. This exercise provides a foundational illustration of this principle using a simple, analytically tractable model: a particle in a harmonic trap. By calculating the variance of the work performed during an instantaneous change in the trap's stiffness, you will directly connect the statistical properties of work to the thermal fluctuations of the particle's initial position, reinforcing the microscopic origins of thermodynamic stochasticity. [@problem_id:849009]", "problem": "A classical particle is confined in a one-dimensional harmonic potential $U(x, k) = \\frac{1}{2} k x^2$. Initially, the particle is in thermal equilibrium with a heat bath at temperature $T$, and the trap stiffness is $k_i$. At time $t=0$, the stiffness of the trap is instantaneously changed from $k_i$ to a new value $k_f$. This process is a \"quench,\" meaning the particle's position does not have time to change during the change of the potential.\n\nThe work $W$ performed on the particle is equal to the change in its potential energy at its position $x$ at the moment of the quench, i.e., $W(x) = U(x, k_f) - U(x, k_i)$. Since the initial position $x$ of the particle is a random variable described by the canonical ensemble for the initial state, the work $W$ is also a random variable.\n\nDerive the expression for the variance of the work done, $\\sigma_W^2 = \\langle W^2 \\rangle - \\langle W \\rangle^2$. The averages $\\langle \\cdot \\rangle$ are to be taken with respect to the initial equilibrium distribution. Express your answer in terms of the initial and final stiffness constants $k_i$ and $k_f$, the temperature $T$, and the Boltzmann constant $k_B$.", "solution": "1. The work for a quench is \n$$W(x)=U(x,k_f)-U(x,k_i)=\\tfrac12(k_f-k_i)x^2.$$ \nThe initial position distribution is Gaussian with \n$$\\langle x^2\\rangle=\\frac{k_B T}{k_i},\\quad\\langle x^4\\rangle=3\\frac{(k_B T)^2}{k_i^2}.$$\n2. Compute moments of $W$:\n$$\\langle W\\rangle=\\frac12(k_f-k_i)\\langle x^2\\rangle\n=\\frac12\\frac{(k_f-k_i)k_B T}{k_i},$$\n$$\\langle W^2\\rangle=\\frac14(k_f-k_i)^2\\langle x^4\\rangle\n=\\frac14(k_f-k_i)^2\\cdot3\\frac{(k_B T)^2}{k_i^2}.$$\n3. The variance is \n$$\\sigma_W^2=\\langle W^2\\rangle-\\langle W\\rangle^2\n=\\frac34\\frac{(k_f-k_i)^2( k_B T)^2}{k_i^2}-\\frac14\\frac{(k_f-k_i)^2( k_B T)^2}{k_i^2}\n=\\frac12\\frac{(k_f-k_i)^2( k_B T)^2}{k_i^2}.$$", "answer": "$$\\boxed{\\frac12\\,\\frac{(k_f-k_i)^2\\,(k_B T)^2}{k_i^2}}$$", "id": "849009"}, {"introduction": "The Jarzynski equality offers a remarkable bridge between non-equilibrium work measurements and equilibrium free energy differences. A crucial step in applying this theorem is evaluating the exponential average of the work, $\\langle \\exp(-\\beta W) \\rangle$. This practice explores the consequences of this equality for a Gaussian work distribution, a common approximation for complex systems, guiding you to derive one of the most celebrated results in the field. [@problem_id:2809100] The resulting expression directly links the average dissipated work to the variance of the work distribution, providing a concrete example of a fluctuation-dissipation relationship.", "problem": "A solvated biomolecule is perturbed by a finite-time protocol that changes a control parameter from an initial value to a final value while the system is in contact with a heat bath at temperature $T$. The initial microstate is sampled from the canonical distribution at inverse temperature $\\beta = 1/(k_B T)$, and the dynamics are microscopically reversible with weak system-bath coupling. The stochastic work $W$ performed on the system during the protocol is a random variable whose distribution is experimentally found to be Gaussian with mean $\\mu$ and variance $\\sigma^{2}$, i.e., $P(W) \\propto \\exp\\!\\big[-(W-\\mu)^{2}/(2\\sigma^{2})\\big]$.\n\nUnder these conditions, the exact non-equilibrium work relation connecting the exponential average of the work to the equilibrium Helmholtz free-energy difference $\\Delta F$ holds. Using only this fact and general properties of cumulant generating functions for Gaussian random variables (without introducing any further approximations), derive a closed-form analytic expression for $\\Delta F$ in terms of $\\mu$, $\\sigma$, and $\\beta$.\n\nThen, briefly interpret your result in the near-equilibrium limit by identifying the leading correction to the mean work and the corresponding average dissipated work in terms of $\\mu$, $\\sigma^{2}$, and $\\beta$.\n\nExpress your final answer for $\\Delta F$ as a symbolic expression in Joules. Do not include units in the final boxed answer.", "solution": "The starting point is the Jarzynski equality, which connects the non-equilibrium work $W$ performed on a system to the change in its equilibrium Helmholtz free energy, $\\Delta F$:\n$$\n\\langle \\exp(-\\beta W) \\rangle = \\exp(-\\beta \\Delta F)\n$$\nHere, the angle brackets $\\langle \\dots \\rangle$ denote an average over many realizations of the non-equilibrium process, starting from an ensemble of initial states drawn from the canonical distribution at inverse temperature $\\beta$. The inverse temperature is defined as $\\beta = (k_B T)^{-1}$.\n\nThe problem states that the work $W$ is a Gaussian random variable with mean $\\langle W \\rangle = \\mu$ and variance $\\langle (W - \\mu)^{2} \\rangle = \\sigma^{2}$. The probability density function is given by:\n$$\nP(W) = \\frac{1}{\\sqrt{2\\pi\\sigma^{2}}} \\exp\\left( -\\frac{(W-\\mu)^{2}}{2\\sigma^{2}} \\right)\n$$\nTo evaluate the left-hand side of the Jarzynski equality, we compute the expectation value of $\\exp(-\\beta W)$. This is the moment-generating function (MGF) of the Gaussian random variable $W$, $M_W(t) = \\langle \\exp(tW) \\rangle$, evaluated at $t = -\\beta$. The MGF is:\n$$\nM_W(t) = \\exp\\left(\\mu t + \\frac{1}{2}\\sigma^{2}t^{2}\\right)\n$$\nSetting $t = -\\beta$, we find the required exponential average:\n$$\n\\langle \\exp(-\\beta W) \\rangle = M_W(-\\beta) = \\exp\\left( -\\mu\\beta + \\frac{1}{2}\\sigma^{2}\\beta^{2} \\right)\n$$\nSubstituting this result into the Jarzynski equality:\n$$\n\\exp\\left( -\\mu\\beta + \\frac{1}{2}\\sigma^{2}\\beta^{2} \\right) = \\exp(-\\beta \\Delta F)\n$$\nTaking the natural logarithm of both sides and dividing by $-\\beta$ (for $\\beta \\neq 0$) yields the expression for $\\Delta F$:\n$$\n-\\mu\\beta + \\frac{1}{2}\\sigma^{2}\\beta^{2} = -\\beta \\Delta F\n$$\n$$\n\\Delta F = \\mu - \\frac{1}{2}\\sigma^{2}\\beta\n$$\nThis is the closed-form expression for the Helmholtz free-energy difference.\n\nFor the interpretation, the average dissipated work, $\\langle W_{\\text{diss}} \\rangle$, is the average work performed in excess of the reversible work:\n$$\n\\langle W_{\\text{diss}} \\rangle = \\langle W \\rangle - \\Delta F = \\mu - \\left(\\mu - \\frac{1}{2}\\sigma^{2}\\beta\\right) = \\frac{1}{2}\\sigma^{2}\\beta\n$$\nThis result, sometimes called the fluctuation-dissipation theorem for non-equilibrium work, shows that the average dissipated energy is directly proportional to the work fluctuations (variance $\\sigma^2$). In the near-equilibrium (quasi-static) limit, the process is slow, work fluctuations become small ($\\sigma^2 \\to 0$), and the average work $\\mu$ approaches the free energy difference $\\Delta F$, with the leading correction being precisely the average dissipated work.", "answer": "$$\n\\boxed{\\mu - \\frac{\\sigma^{2}\\beta}{2}}\n$$", "id": "2809100"}, {"introduction": "While fluctuation theorems like the Jarzynski equality are exact, their practical application to finite datasets presents significant statistical challenges. The Jarzynski estimator for free energy is notoriously biased and can converge very slowly, as it is dominated by rare events in the low-work tail of the distribution. This advanced practice confronts this issue head-on, asking you to determine the minimum number of experimental or computational trajectories required to achieve a target accuracy. [@problem_id:2809086] By analyzing the estimator's variance, you will gain critical insight into the practical limitations of free energy estimation and how they depend on the very nature of the non-equilibrium process.", "problem": "Consider a nonequilibrium switching process in contact with a heat bath at inverse temperature $\\beta$, with a forward work distribution $P_F(W)$. The Jarzynski equality (JE) states that $\\langle e^{-\\beta W} \\rangle = e^{-\\beta \\Delta F}$, where $\\Delta F$ is the equilibrium free energy difference. The Jarzynski estimator from $N$ independent and identically distributed work values $\\{W_i\\}_{i=1}^N$ is defined as\n$$\n\\hat{\\Delta F}_{\\mathrm{JE}} \\equiv -\\beta^{-1} \\log\\left(\\frac{1}{N}\\sum_{i=1}^N e^{-\\beta W_i}\\right).\n$$\nThe Crooks fluctuation theorem (CFT) connects the forward and reverse work distributions and implies that the dissipated work $W_{\\mathrm{dis}} \\equiv W - \\Delta F$ controls the rarity of low-work events that dominate the exponential average.\n\nYour task is to determine, from first principles, the minimal number of trajectories $N$ needed to achieve a target root-mean-square accuracy in $\\hat{\\Delta F}_{\\mathrm{JE}}$, by analyzing how $\\mathrm{Var}(\\hat{\\Delta F}_{\\mathrm{JE}})$ depends on the lower tail of $P_F(W)$ and the dissipated work. Throughout, use energy units of $k_{\\mathrm{B}} T$ so that $\\beta = 1$; thus all energies, including $\\Delta F$, $W$, and any error tolerances, are dimensionless in units of $k_{\\mathrm{B}} T$.\n\nStarting only from the definitions and well-tested facts below, derive an explicit, computationally usable condition for the minimal integer $N$ required to ensure that the standard deviation of $\\hat{\\Delta F}_{\\mathrm{JE}}$ does not exceed a target tolerance $\\tau$ (expressed in units of $k_{\\mathrm{B}} T$):\n- The Jarzynski equality $\\langle e^{-W} \\rangle = e^{-\\Delta F}$.\n- The central limit theorem for sample means of independent and identically distributed random variables and the delta method for transformations of asymptotically normal estimators.\n- If the moment generating function $M_W(t) \\equiv \\langle e^{t W} \\rangle$ diverges at a needed $t$, the corresponding moment is undefined and the variance can be unbounded, making it impossible to guarantee the target accuracy with any finite $N$.\n\nUse your derivation to implement a program that computes the minimal sample size for the following test suite of parametric forward work models $P_F(W)$:\n\n- Family A (Gaussian work): $W \\sim \\mathcal{N}(\\mu, \\sigma^2)$, with parameters $(\\mu, \\sigma)$.\n- Family B (Laplace work, also known as double exponential): $W \\sim \\mathrm{Laplace}(\\mu, b)$ with probability density $p(W) = \\frac{1}{2 b} \\exp\\left(-\\frac{|W - \\mu|}{b}\\right)$, parameters $(\\mu, b)$, and scale $b > 0$.\n\nFor each test case, if your derivation indicates that no finite $N$ can meet the target standard deviation $\\tau$, return $+\\infty$.\n\nWork in units of $k_{\\mathrm{B}} T$ and ensure that the computed $N$ is the smallest integer that satisfies your condition.\n\nTest suite (each item is of the form $(\\text{family}, \\text{parameters}, \\tau)$):\n1. $($Family A$,$ $(\\mu = 5.0, \\sigma = 1.5), \\tau = 0.05)$\n2. $($Family A$,$ $(\\mu = 0.5, \\sigma = 0.2), \\tau = 0.05)$\n3. $($Family B$,$ $(\\mu = 3.0, b = 0.3), \\tau = 0.05)$\n4. $($Family B$,$ $(\\mu = 1.0, b = 0.5), \\tau = 0.05)$\n5. $($Family B$,$ $(\\mu = 2.0, b = 0.6), \\tau = 0.10)$\n\nYour program should produce a single line of output containing the results as a comma-separated list of the minimal $N$ values for the test suite, in the same order, enclosed in square brackets. Use $+\\infty$ if no finite $N$ can achieve the requested standard deviation. For example, an output could look like $[123,45,678,\\mathrm{inf},9]$, but with the correct values for the given test cases.", "solution": "The objective is to derive the minimal number of trajectories, $N$, required to ensure that the standard deviation of the Jarzynski estimator, $\\hat{\\Delta F}$, does not exceed a tolerance $\\tau$. We work in units where $\\beta=1$.\n\nThe estimator is $\\hat{\\Delta F} = -\\log(\\bar{Y}_N)$, where $Y = e^{-W}$ and $\\bar{Y}_N$ is the sample mean of $N$ i.i.d. draws. Using the delta method for large $N$, the variance of the estimator is approximately:\n$$\n\\mathrm{Var}(\\hat{\\Delta F}) \\approx \\frac{1}{N} \\left( \\frac{d(-\\log y)}{dy} \\bigg|_{y=\\langle Y \\rangle} \\right)^2 \\mathrm{Var}(Y)\n$$\nThe derivative is $-1/y$. From the Jarzynski equality, $\\langle Y \\rangle = \\langle e^{-W} \\rangle = e^{-\\Delta F}$. Thus, the squared derivative is $(-e^{\\Delta F})^2 = e^{2\\Delta F}$. The variance of $Y$ is $\\mathrm{Var}(Y) = \\langle e^{-2W} \\rangle - (\\langle e^{-W} \\rangle)^2 = \\langle e^{-2W} \\rangle - e^{-2\\Delta F}$.\nCombining these gives the asymptotic variance of the estimator:\n$$\n\\mathrm{Var}(\\hat{\\Delta F}) \\approx \\frac{1}{N} e^{2\\Delta F} (\\langle e^{-2W} \\rangle - e^{-2\\Delta F}) = \\frac{1}{N} \\left( e^{2\\Delta F} \\langle e^{-2W} \\rangle - 1 \\right)\n$$\nFor the variance to be finite, $\\langle e^{-2W} \\rangle$ must be finite. We require $\\mathrm{Var}(\\hat{\\Delta F}) \\le \\tau^2$, which leads to the condition on $N$:\n$$\nN \\ge \\frac{e^{2\\Delta F} \\langle e^{-2W} \\rangle - 1}{\\tau^2}\n$$\nThe minimal integer $N$ is the ceiling of the right-hand side.\n\n**Family A: Gaussian Work ($W \\sim \\mathcal{N}(\\mu, \\sigma^2)$)**\nThe moment generating function (MGF) is $M_W(t) = \\exp(\\mu t + \\frac{1}{2}\\sigma^2 t^2)$.\nFrom $\\langle e^{-W} \\rangle = M_W(-1)$, we find $\\Delta F = \\mu - \\frac{\\sigma^2}{2}$.\nWe also need $\\langle e^{-2W} \\rangle = M_W(-2) = \\exp(-2\\mu + 2\\sigma^2)$.\nSubstituting these into the numerator gives $e^{2\\Delta F} \\langle e^{-2W} \\rangle - 1 = \\exp(2\\mu - \\sigma^2)\\exp(-2\\mu + 2\\sigma^2) - 1 = e^{\\sigma^2} - 1$.\nSo, for the Gaussian case:\n$$\nN_{\\mathrm{min}} = \\left\\lceil \\frac{e^{\\sigma^2} - 1}{\\tau^2} \\right\\rceil\n$$\n\n**Family B: Laplace Work ($W \\sim \\mathrm{Laplace}(\\mu, b)$)**\nThe MGF is $M_W(t) = \\frac{e^{\\mu t}}{1 - b^2 t^2}$, which converges only for $|t| < 1/b$.\nThe term $\\langle e^{-2W} \\rangle = M_W(-2)$ is required, which converges only if $|-2| < 1/b$, i.e., $b < 1/2$. If $b \\ge 1/2$, the variance is infinite, so $N_{\\mathrm{min}} = \\infty$.\nFor $b < 1/2$:\nFrom $\\langle e^{-W} \\rangle = M_W(-1)$, we find $\\Delta F = \\mu + \\log(1-b^2)$.\nWe use $\\langle e^{-2W} \\rangle = M_W(-2) = \\frac{e^{-2\\mu}}{1 - 4b^2}$.\nThe numerator becomes $e^{2\\Delta F} \\langle e^{-2W} \\rangle - 1 = e^{2\\mu}(1-b^2)^2 \\frac{e^{-2\\mu}}{1-4b^2} - 1 = \\frac{(1-b^2)^2}{1-4b^2} - 1 = \\frac{b^2(2+b^2)}{1-4b^2}$.\nSo, for the Laplace case with $b  1/2$:\n$$\nN_{\\mathrm{min}} = \\left\\lceil \\frac{b^2(2 + b^2)}{\\tau^2(1 - 4b^2)} \\right\\rceil\n$$\nThese formulae are implemented below to calculate the results for the test suite.\n```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the minimal sample size N for a set of test cases based on \n    the derived formulae for Gaussian and Laplace work distributions.\n    \"\"\"\n    test_cases = [\n        ('A', (5.0, 1.5), 0.05),  # Case 1: Gaussian\n        ('A', (0.5, 0.2), 0.05),  # Case 2: Gaussian\n        ('B', (3.0, 0.3), 0.05),  # Case 3: Laplace\n        ('B', (1.0, 0.5), 0.05),  # Case 4: Laplace\n        ('B', (2.0, 0.6), 0.10)   # Case 5: Laplace\n    ]\n\n    results = []\n    for case in test_cases:\n        family, params, tau = case\n        n_min = 0\n\n        if family == 'A':\n            _mu, sigma = params\n            numerator = np.exp(sigma**2) - 1\n            denominator = tau**2\n            n_min = np.ceil(numerator / denominator)\n\n        elif family == 'B':\n            _mu, b = params\n            if b >= 0.5:\n                n_min = np.inf\n            else:\n                numerator = b**2 * (2 + b**2)\n                denominator = tau**2 * (1 - 4 * b**2)\n                n_min = np.ceil(numerator / denominator)\n        \n        if np.isinf(n_min):\n            results.append(\"inf\")\n        else:\n            results.append(str(int(n_min)))\n\n    print(f\"[{','.join(results)}]\")\n\n# solve() # The function call is commented out as the code block itself is the deliverable.\n# The expected output is [3396,17,118,inf,inf]\n```", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the minimal sample size N for a set of test cases based on \n    the derived formulae for Gaussian and Laplace work distributions.\n    \"\"\"\n    test_cases = [\n        ('A', (5.0, 1.5), 0.05),  # Case 1: Gaussian\n        ('A', (0.5, 0.2), 0.05),  # Case 2: Gaussian\n        ('B', (3.0, 0.3), 0.05),  # Case 3: Laplace\n        ('B', (1.0, 0.5), 0.05),  # Case 4: Laplace\n        ('B', (2.0, 0.6), 0.10)   # Case 5: Laplace\n    ]\n\n    results = []\n    for case in test_cases:\n        family, params, tau = case\n        n_min = 0\n\n        if family == 'A':\n            # Family A: Gaussian work distribution W ~ N(mu, sigma^2)\n            # Parameters are (mu, sigma)\n            # Formula: N_min = ceil((exp(sigma^2) - 1) / tau^2)\n            _mu, sigma = params\n            numerator = np.exp(sigma**2) - 1\n            denominator = tau**2\n            n_min = np.ceil(numerator / denominator)\n\n        elif family == 'B':\n            # Family B: Laplace work distribution W ~ Laplace(mu, b)\n            # Parameters are (mu, b)\n            # The variance of the estimator is finite only if b  0.5.\n            _mu, b = params\n            if b >= 0.5:\n                n_min = np.inf\n            else:\n                # Formula: N_min = ceil((b^2 * (2 + b^2)) / (tau^2 * (1 - 4 * b^2)))\n                numerator = b**2 * (2 + b**2)\n                denominator = tau**2 * (1 - 4 * b**2)\n                n_min = np.ceil(numerator / denominator)\n        \n        #\n        # Append the result, ensuring proper type for np.inf\n        #\n        if np.isinf(n_min):\n            results.append(\"inf\")\n        else:\n            results.append(str(int(n_min)))\n\n    # A single line of output with the comma-separated list of results\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "2809086"}]}