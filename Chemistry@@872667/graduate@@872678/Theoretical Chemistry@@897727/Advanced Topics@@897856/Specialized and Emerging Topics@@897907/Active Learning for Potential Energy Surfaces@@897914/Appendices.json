{"hands_on_practices": [{"introduction": "The foundation of uncertainty-driven active learning is the ability to quantify what a model does not yet know. This practice delves into the mathematical heart of this concept using a Gaussian Process (GP) model. You will compute the posterior variance at a candidate configuration and see how it directly relates to the expected information gain, providing a powerful metric for deciding where to perform the next expensive quantum chemistry calculation. [@problem_id:2760116]", "problem": "In data-efficient construction of a Potential Energy Surface (PES), an active learner selects new quantum chemistry calculations to reduce epistemic uncertainty most effectively. Consider a one-dimensional reaction coordinate $s$ along which a zero-mean Gaussian Process (GP; Gaussian Process) prior is placed on the latent energy $f(s)$ with covariance function $k(s,s') = \\sigma_{f}^{2}\\exp\\!\\big(-\\frac{(s-s')^{2}}{2\\ell^{2}}\\big)$. Independent observation noise is modeled as Gaussian with variance $\\sigma_{n}^{2}$. You have already performed electronic structure calculations at two configurations $s_{1} = 0$ and $s_{2} = 1$ (numerical energy values are not needed to compute the requested quantities). You consider a candidate configuration $s_{c} = 0.5$ for the next computation.\n\nAssume hyperparameters $\\sigma_{f}^{2} = 1$ (in $(\\mathrm{kJ}\\,\\mathrm{mol}^{-1})^{2}$), $\\ell = 1$ (in the same length units as $s$), and $\\sigma_{n}^{2} = 0.01$ (in $(\\mathrm{kJ}\\,\\mathrm{mol}^{-1})^{2}$). Using only fundamental properties of multivariate Gaussian conditioning and differential entropy, compute the posterior variance of the latent energy $f(s_{c})$ given the two existing inputs and then interpret this quantity as a single-step information-gain proxy for active learning. Specifically, treat the expected information gain from querying at $s_{c}$ as the mutual information between the new noisy observation and the latent energy at $s_{c}$ conditioned on the current dataset.\n\nReport as your final answer the numerical value of the expected information gain at $s_{c}$, expressed in nats and rounded to four significant figures. Do not include units in your final boxed answer.", "solution": "The objective is to compute the expected information gain from performing a new quantum chemistry calculation at the candidate configuration $s_c$. This gain is defined as the mutual information between the prospective noisy observation $y_c$ at $s_c$ and the true latent energy $f_c \\equiv f(s_c)$, conditioned on the existing dataset $D$. The dataset $D$ consists of observations at configurations $s_1 = 0$ and $s_2 = 1$. The information gain, denoted $IG$, is thus given by:\n$$\nIG = I(y_c; f_c | D)\n$$\nBy the properties of mutual information and differential entropy $h(\\cdot)$, this can be expressed in two equivalent forms:\n$$\nI(y_c; f_c | D) = h(f_c | D) - h(f_c | y_c, D) = h(y_c | D) - h(y_c | f_c, D)\n$$\nFor a Gaussian random variable $X$ with variance $\\sigma^2$, the differential entropy is $h(X) = \\frac{1}{2}\\ln(2\\pi e \\sigma^2)$. The information gain is thus a function of variances.\n\nLet us use the second form, $h(y_c | D) - h(y_c | f_c, D)$.\nThe observation model is $y_c = f_c + \\epsilon_c$, where $\\epsilon_c \\sim \\mathcal{N}(0, \\sigma_n^2)$ is independent noise.\nConditioned on the true latent value $f_c$, the distribution of the observation $y_c$ is $p(y_c | f_c, D) = p(y_c | f_c) = \\mathcal{N}(y_c | f_c, \\sigma_n^2)$. The variance is $\\mathrm{Var}(y_c | f_c, D) = \\sigma_n^2$. Thus, its entropy is:\n$$\nh(y_c | f_c, D) = \\frac{1}{2}\\ln(2\\pi e \\sigma_n^2)\n$$\nNext, we consider the distribution of $y_c$ conditioned only on the existing data $D$. This is the posterior predictive distribution. The latent function value $f_c$ conditioned on $D$ is Gaussian, $p(f_c | D) = \\mathcal{N}(f_c | \\mu_*(s_c), \\sigma^2_*(s_c))$, where $\\mu_*(s_c)$ and $\\sigma^2_*(s_c)$ are the posterior mean and variance from the Gaussian Process regression. The observation $y_c$ is the sum of two independent Gaussian variables, $f_c|D$ and $\\epsilon_c$. The variance of their sum is the sum of their variances:\n$$\n\\mathrm{Var}(y_c|D) = \\mathrm{Var}(f_c|D) + \\mathrm{Var}(\\epsilon_c) = \\sigma^2_*(s_c) + \\sigma_n^2\n$$\nThe entropy of the posterior predictive distribution for $y_c$ is:\n$$\nh(y_c|D) = \\frac{1}{2}\\ln\\left(2\\pi e (\\sigma^2_*(s_c) + \\sigma_n^2)\\right)\n$$\nCombining these results, the information gain is:\n$$\nIG = \\frac{1}{2}\\ln\\left(2\\pi e (\\sigma^2_*(s_c) + \\sigma_n^2)\\right) - \\frac{1}{2}\\ln(2\\pi e \\sigma_n^2) = \\frac{1}{2}\\ln\\left(\\frac{\\sigma^2_*(s_c) + \\sigma_n^2}{\\sigma_n^2}\\right) = \\frac{1}{2}\\ln\\left(1 + \\frac{\\sigma^2_*(s_c)}{\\sigma_n^2}\\right)\n$$\nThis result shows that the information gain depends only on the ratio of the posterior variance of the latent function to the observation noise variance. To compute $IG$, we must first compute the posterior variance $\\sigma^2_*(s_c)$.\n\nThe posterior variance of a GP at a test point $s_c$ given training points $\\mathbf{s} = [s_1, s_2]^T$ is given by the formula:\n$$\n\\sigma^2_*(s_c) = k(s_c, s_c) - \\mathbf{k}_c^T (\\mathbf{K} + \\sigma_n^2 \\mathbf{I})^{-1} \\mathbf{k}_c\n$$\nwhere:\n- The covariance function is $k(s, s') = \\sigma_{f}^{2}\\exp\\left(-\\frac{(s-s')^{2}}{2\\ell^{2}}\\right)$.\n- The hyperparameters are $\\sigma_f^2 = 1$, $\\ell = 1$, and $\\sigma_n^2 = 0.01$.\n- The training points are $s_1=0$, $s_2=1$. The candidate point is $s_c=0.5$.\n- $\\mathbf{k}_c$ is the vector of covariances between the test point and the training points: $\\mathbf{k}_c = [k(s_c, s_1), k(s_c, s_2)]^T$.\n- $\\mathbf{K}$ is the covariance matrix of the training points: $\\mathbf{K}_{ij} = k(s_i, s_j)$.\n- $\\mathbf{I}$ is the identity matrix.\n\nWith the given values, the kernel simplifies to $k(s, s') = \\exp\\left(-\\frac{(s-s')^2}{2}\\right)$. We compute the necessary kernel values:\n$k(s_c, s_c) = k(0.5, 0.5) = \\exp(0) = 1$.\n$k(s_c, s_1) = k(0.5, 0) = \\exp\\left(-\\frac{0.5^2}{2}\\right) = \\exp(-0.125)$.\n$k(s_c, s_2) = k(0.5, 1) = \\exp\\left(-\\frac{(-0.5)^2}{2}\\right) = \\exp(-0.125)$.\n$k(s_1, s_1) = k(0, 0) = 1$.\n$k(s_2, s_2) = k(1, 1) = 1$.\n$k(s_1, s_2) = k(0, 1) = \\exp\\left(-\\frac{(-1)^2}{2}\\right) = \\exp(-0.5)$.\n\nThe vector $\\mathbf{k}_c$ and matrix $\\mathbf{K}$ are:\n$$\n\\mathbf{k}_c = \\begin{pmatrix} \\exp(-0.125) \\\\ \\exp(-0.125) \\end{pmatrix} \\quad , \\quad\n\\mathbf{K} = \\begin{pmatrix} 1 & \\exp(-0.5) \\\\ \\exp(-0.5) & 1 \\end{pmatrix}\n$$\nNow we construct the matrix to be inverted, $\\mathbf{M} = \\mathbf{K} + \\sigma_n^2 \\mathbf{I}$:\n$$\n\\mathbf{M} = \\begin{pmatrix} 1 + 0.01 & \\exp(-0.5) \\\\ \\exp(-0.5) & 1 + 0.01 \\end{pmatrix} = \\begin{pmatrix} 1.01 & \\exp(-0.5) \\\\ \\exp(-0.5) & 1.01 \\end{pmatrix}\n$$\nDue to the symmetry of the problem ($s_c$ is the midpoint of $s_1$ and $s_2$), we can simplify the quadratic form $\\mathbf{k}_c^T \\mathbf{M}^{-1} \\mathbf{k}_c$. Let $a = 1.01$, $b = \\exp(-0.5)$, and $k_{val} = \\exp(-0.125)$.\n$$\n\\mathbf{M}^{-1} = \\frac{1}{a^2 - b^2}\\begin{pmatrix} a & -b \\\\ -b & a \\end{pmatrix}\n$$\n$$\n\\mathbf{k}_c^T \\mathbf{M}^{-1} \\mathbf{k}_c = \\begin{pmatrix} k_{val} & k_{val} \\end{pmatrix} \\frac{1}{a^2-b^2} \\begin{pmatrix} a & -b \\\\ -b & a \\end{pmatrix} \\begin{pmatrix} k_{val} \\\\ k_{val} \\end{pmatrix} = \\frac{k_{val}^2}{a^2-b^2} \\begin{pmatrix} 1 & 1 \\end{pmatrix} \\begin{pmatrix} a-b \\\\ a-b \\end{pmatrix} = \\frac{2k_{val}^2(a-b)}{a^2-b^2}\n$$\nUsing $a^2-b^2 = (a-b)(a+b)$, this simplifies to:\n$$\n\\mathbf{k}_c^T \\mathbf{M}^{-1} \\mathbf{k}_c = \\frac{2k_{val}^2}{a+b} = \\frac{2(\\exp(-0.125))^2}{1.01 + \\exp(-0.5)} = \\frac{2\\exp(-0.25)}{1.01 + \\exp(-0.5)}\n$$\nNow, we substitute numerical values:\n$\\exp(-0.25) \\approx 0.77880078$\n$\\exp(-0.5) \\approx 0.60653066$\n$$\n\\mathbf{k}_c^T \\mathbf{M}^{-1} \\mathbf{k}_c \\approx \\frac{2 \\times 0.77880078}{1.01 + 0.60653066} = \\frac{1.55760156}{1.61653066} \\approx 0.96354636\n$$\nThe posterior variance is then:\n$$\n\\sigma^2_*(s_c) = k(s_c, s_c) - \\mathbf{k}_c^T \\mathbf{M}^{-1} \\mathbf{k}_c \\approx 1 - 0.96354636 = 0.03645364\n$$\nFinally, we compute the information gain, which is in units of nats as the natural logarithm was used:\n$$\nIG = \\frac{1}{2}\\ln\\left(1 + \\frac{\\sigma^2_*(s_c)}{\\sigma_n^2}\\right) \\approx \\frac{1}{2}\\ln\\left(1 + \\frac{0.03645364}{0.01}\\right) = \\frac{1}{2}\\ln(1 + 3.645364) = \\frac{1}{2}\\ln(4.645364)\n$$\n$$\nIG \\approx \\frac{1}{2} \\times 1.535876 \\approx 0.767938\n$$\nRounding to four significant figures, the expected information gain is $0.7679$.", "answer": "$$\\boxed{0.7679}$$", "id": "2760116"}, {"introduction": "While selecting the single most uncertain point is a valid strategy, practical active learning loops often select a batch of calculations to be run in parallel. To ensure the batch provides broad coverage of the configuration space, we often employ diversity-based selection methods. This exercise guides you through implementing a popular pipeline that uses $k$-means clustering on a molecular descriptor space to identify a diverse set of representative candidates for labeling. [@problem_id:2760075]", "problem": "Design and implement a complete, runnable program that constructs a clustering-based preselection pipeline for Active Learning (AL) of Potential Energy Surfaces (PESs) using a descriptor-based representation and $k$-means clustering. The program must both implement the pipeline and numerically verify a covering property derived from first principles. The pipeline must be formulated purely in mathematical terms and operate on a synthetic, but scientifically plausible, pool of molecular geometries.\n\nStart from the following fundamental base:\n- A Potential Energy Surface (PES) is a function $E(\\mathbf{R})$ mapping nuclear coordinates $\\mathbf{R}$ to energy, and Active Learning (AL) seeks to efficiently select informative configurations to learn $E(\\mathbf{R})$.\n- A descriptor map $\\phi$ embeds a molecular geometry into a Euclidean vector space to enable metric-based selection; Euclidean distance is defined by $\\|\\mathbf{x}-\\mathbf{y}\\|_2 = \\sqrt{\\sum_i (x_i-y_i)^2}$.\n- The $k$-means clustering objective partitions a finite set $\\{\\mathbf{v}_i\\}_{i=1}^n \\subset \\mathbb{R}^d$ into $k$ clusters with centroids $\\{\\mathbf{c}_j\\}_{j=1}^k$ to minimize within-cluster sum of squared Euclidean distances.\n- The triangle inequality in Euclidean space states $\\|\\mathbf{x}-\\mathbf{z}\\|_2 \\le \\|\\mathbf{x}-\\mathbf{y}\\|_2 + \\|\\mathbf{y}-\\mathbf{z}\\|_2$ for all $\\mathbf{x},\\mathbf{y},\\mathbf{z} \\in \\mathbb{R}^d$.\n\nDataset and descriptor specification:\n- Consider a symmetric triatomic system $\\text{A}-\\text{B}-\\text{A}$ in two dimensions. For each geometry, place atom $\\text{B}$ at the origin, one $\\text{A}$ at $(r,0,0)$, and the other $\\text{A}$ at $(r\\cos\\theta, r\\sin\\theta, 0)$, where $r$ is a bond length and $\\theta$ is the bond angle. The bond length $r$ is expressed in $\\text{\\AA}$ and the angle $\\theta$ is expressed in degrees in the range $[60,160]$; internally, convert $\\theta$ to radians before any trigonometric operations.\n- Define the descriptor $\\phi$ as the sorted vector of the three pairwise interatomic distances (in $\\text{\\AA}$): two $\\text{A}-\\text{B}$ distances equal to $r$, and the $\\text{A}-\\text{A}$ distance equal to $2r\\sin(\\theta/2)$. Thus $\\phi(r,\\theta) \\in \\mathbb{R}^3$ and has components with units of $\\text{\\AA}$.\n- Construct a candidate pool by taking a Cartesian grid: $r \\in \\{1.0, 1.2, 1.4, 1.6, 1.8, 2.0\\}$ (in $\\text{\\AA}$) and $\\theta \\in \\{60, 80, 100, 120, 140, 160\\}$ (in degrees), resulting in $36$ unique geometries. Append duplicates of the first $4$ descriptors to the end, resulting in a final pool of $40$ descriptor vectors.\n\nPreselection pipeline to implement:\n1. Build the descriptor matrix $\\mathbf{V} \\in \\mathbb{R}^{n \\times d}$ with $n=40$ and $d=3$, where row $i$ is $\\phi(r_i,\\theta_i)$ as defined above. Optionally, standardize descriptor dimensions by z-score: for each column $j$, transform $v_{ij}$ to $(v_{ij}-\\mu_j)/\\sigma_j$ using the sample mean $\\mu_j$ and standard deviation $\\sigma_j$ computed over the full pool; if standardization is disabled, use raw $\\text{\\AA}$ values.\n2. Run $k$-means clustering with Euclidean distance on the chosen representation (standardized or raw), with deterministic initialization and iteration bound. Let the resulting clusters be $C_1,\\dots,C_k$ with centroids $\\mathbf{c}_1,\\dots,\\mathbf{c}_k$.\n3. For each cluster $C_j$, compute the cluster radius $R_j = \\max_{\\mathbf{v}\\in C_j}\\|\\mathbf{v}-\\mathbf{c}_j\\|_2$ and select the medoid $\\mathbf{m}_j \\in C_j$ closest to $\\mathbf{c}_j$; in case of ties, break by choosing the smallest global index.\n4. Let $S=\\{\\mathbf{m}_1,\\dots,\\mathbf{m}_k\\}$. Define the global covering radius\n$$\nR_{\\text{cov}}(S) = \\max_{1\\le i \\le n} \\min_{1\\le j \\le k} \\|\\mathbf{v}_i - \\mathbf{m}_j\\|_2.\n$$\n5. Using only the triangle inequality and the definitions above, establish and verify numerically that\n$$\nR_{\\text{cov}}(S) \\le 2 \\cdot \\max_{1\\le j \\le k} R_j.\n$$\nWhen standardization is enabled, distances are dimensionless; when standardization is disabled, distances are in $\\text{\\AA}$. You must compute $R_{\\text{cov}}(S)$ and the bound in the same space in which clustering is performed.\n\nTest suite:\nRun the pipeline on the fixed pool described above with the following parameter sets, using $k$-means with $k$ clusters, at most $200$ iterations, and $k$-means$++$ initialization seeded with the given integer seed. For each test, report the actual covering radius and the theoretical bound, both rounded to six decimal places, and a binary indicator equal to $1$ if the inequality holds and $0$ otherwise.\n- Test $1$: $k=4$, standardization disabled, seed $=42$. Distances and bound must be reported in $\\text{\\AA}$, rounded to six decimal places.\n- Test $2$: $k=1$, standardization enabled, seed $=7$. Distances and bound must be reported in standardized units (dimensionless), rounded to six decimal places.\n- Test $3$: $k=10$, standardization enabled, seed $=123$. Distances and bound must be reported in standardized units (dimensionless), rounded to six decimal places.\n\nFinal output format:\n- Your program should produce a single line of output containing the nine results as a comma-separated list enclosed in square brackets. The list must be\n$$\n[\\;R_{\\text{cov}}^{(1)},\\;B^{(1)},\\;\\text{ok}^{(1)},\\;R_{\\text{cov}}^{(2)},\\;B^{(2)},\\;\\text{ok}^{(2)},\\;R_{\\text{cov}}^{(3)},\\;B^{(3)},\\;\\text{ok}^{(3)}\\;],\n$$\nwhere $R_{\\text{cov}}^{(t)}$ is the actual covering radius for test $t$, $B^{(t)}$ is the bound $2\\max_j R_j$ for test $t$, and $\\text{ok}^{(t)}$ is $1$ if $R_{\\text{cov}}^{(t)} \\le B^{(t)}$ and $0$ otherwise. All three $R_{\\text{cov}}^{(t)}$ and all three $B^{(t)}$ must be printed as decimal floats rounded to six decimal places. For tests where standardization is disabled, the units are $\\text{\\AA}$; for tests where standardization is enabled, the values are dimensionless.", "solution": "The solution proceeds in two stages: first, a formal derivation of the governing inequality, and second, a description of the computational algorithm to implement the pipeline and verify the inequality.\n\n**1. Derivation of the Covering Radius Inequality**\n\nThe problem states a relationship between the global covering radius $R_{\\text{cov}}(S)$ of a set of medoids $S$ and the maximum cluster radius $R_{\\max}$. We must prove that for a set of data points $V$ partitioned into clusters $C_1, \\dots, C_k$:\n$$\nR_{\\text{cov}}(S) \\le 2 \\cdot \\max_{1\\le j \\le k} R_j\n$$\nwhere $R_j = \\max_{\\mathbf{v}\\in C_j}\\|\\mathbf{v}-\\mathbf{c}_j\\|_2$ is the radius of cluster $C_j$ with centroid $\\mathbf{c}_j$, and $S = \\{\\mathbf{m}_1, \\dots, \\mathbf{m}_k\\}$ is the set of cluster medoids. The medoid $\\mathbf{m}_j$ is the point in cluster $C_j$ closest to the centroid $\\mathbf{c}_j$.\n\nThe derivation is as follows:\n\nLet $\\mathbf{v}$ be an arbitrary data point from the full set $V$. Since the clusters $\\{C_j\\}_{j=1}^k$ form a partition of $V$, $\\mathbf{v}$ must belong to exactly one cluster, say $C_j$.\n\nThe global covering radius is defined as $R_{\\text{cov}}(S) = \\max_{\\mathbf{v} \\in V} \\min_{\\mathbf{m} \\in S} \\|\\mathbf{v} - \\mathbf{m}\\|_2$. Let us find an upper bound for the term $\\min_{\\mathbf{m} \\in S} \\|\\mathbf{v} - \\mathbf{m}\\|_2$. This minimum distance is necessarily less than or equal to the distance to any specific medoid, including the medoid $\\mathbf{m}_j$ corresponding to the cluster $C_j$ that $\\mathbf{v}$ belongs to:\n$$\n\\min_{\\mathbf{m} \\in S} \\|\\mathbf{v} - \\mathbf{m}\\|_2 \\le \\|\\mathbf{v} - \\mathbf{m}_j\\|_2\n$$\nWe can bound the term $\\|\\mathbf{v} - \\mathbf{m}_j\\|_2$ using the triangle inequality in the descriptor space, with the centroid $\\mathbf{c}_j$ as the intermediate point:\n$$\n\\|\\mathbf{v} - \\mathbf{m}_j\\|_2 \\le \\|\\mathbf{v} - \\mathbf{c}_j\\|_2 + \\|\\mathbf{c}_j - \\mathbf{m}_j\\|_2\n$$\nNow, we must bound the two terms on the right-hand side.\n1.  By definition of the cluster radius $R_j = \\max_{\\mathbf{x}\\in C_j}\\|\\mathbf{x}-\\mathbf{c}_j\\|_2$, and since $\\mathbf{v} \\in C_j$, it follows directly that $\\|\\mathbf{v} - \\mathbf{c}_j\\|_2 \\le R_j$.\n2.  The medoid $\\mathbf{m}_j$ is also a point within its own cluster $C_j$. Therefore, its distance to the centroid $\\mathbf{c}_j$ is also bounded by the cluster radius: $\\|\\mathbf{m}_j - \\mathbf{c}_j\\|_2 \\le R_j$.\n\nSubstituting these bounds back into the triangle inequality gives:\n$$\n\\|\\mathbf{v} - \\mathbf{m}_j\\|_2 \\le R_j + R_j = 2 R_j\n$$\nCombining our inequalities, we have for any $\\mathbf{v} \\in C_j$:\n$$\n\\min_{\\mathbf{m} \\in S} \\|\\mathbf{v} - \\mathbf{m}\\|_2 \\le 2 R_j\n$$\nThis inequality holds for any point $\\mathbf{v}$. To find the global covering radius, we must take the maximum over all points in $V$. Let $R_{\\max} = \\max_{1\\le j \\le k} R_j$.\nSince $R_j \\le R_{\\max}$ for any cluster $j$, we can write:\n$$\n\\min_{\\mathbf{m} \\in S} \\|\\mathbf{v} - \\mathbf{m}\\|_2 \\le 2 R_j \\le 2 R_{\\max}\n$$\nThis holds for any point $\\mathbf{v} \\in V$. Thus, the maximum of the left-hand side over all $\\mathbf{v}$ must also satisfy this inequality:\n$$\nR_{\\text{cov}}(S) = \\max_{\\mathbf{v} \\in V} \\min_{\\mathbf{m} \\in S} \\|\\mathbf{v} - \\mathbf{m}\\|_2 \\le 2 R_{\\max}\n$$\nThis completes the formal proof.\n\n**2. Algorithmic Implementation**\n\nThe verification of this inequality will be performed by a program implementing the specified pipeline.\n\n**Step 1: Data Generation.** A pool of $n=40$ descriptor vectors in $\\mathbb{R}^3$ is constructed. The molecular geometry is parametrized by a bond length $r$ and bond angle $\\theta$. The descriptor $\\phi(r, \\theta)$ is the sorted vector of the three interatomic distances. For the specified symmetric triatomic system A-B-A, two distances are equal to the A-B bond length $r$, and the third, the A-A distance, is $d_{\\text{A-A}} = 2r\\sin(\\theta/2)$. For the given range $\\theta \\in [60, 160]$ degrees, $d_{\\text{A-A}} \\ge r$. Thus, the sorted descriptor vector is always $[r, r, 2r\\sin(\\theta/2)]$.\nThe program first generates $36$ unique descriptors from the Cartesian product of $r \\in \\{1.0, 1.2, 1.4, 1.6, 1.8, 2.0\\}$ and $\\theta \\in \\{60, 80, 100, 120, 140, 160\\}$. Angles must be converted from degrees to radians for trigonometric calculations. Then, duplicates of the first $4$ descriptors are appended to form the final $40 \\times 3$ descriptor matrix $\\mathbf{V}$.\n\n**Step 2: Standardization.** For tests requiring standardization, each column $j$ of the matrix $\\mathbf{V}$ is transformed via z-scoring: $v'_{ij} = (v_{ij} - \\mu_j) / \\sigma_j$, where $\\mu_j$ and $\\sigma_j$ are the sample mean and standard deviation of column $j$ over all $40$ points. If $\\sigma_j = 0$, no scaling is applied to that column. Subsequent calculations are performed on this standardized, dimensionless data. Otherwise, calculations use the raw data in units of Angstroms ($\\text{\\AA}$).\n\n**Step 3: K-Means Clustering.** A deterministic $k$-means algorithm is required. This is achieved by implementing the $k$-means++ initialization algorithm with a seeded pseudo-random number generator, followed by a fixed number of iterative updates.\n-   **Initialization ($k$-means++):**\n    1.  A random number generator is seeded with the specified integer.\n    2.  The first centroid is chosen uniformly at random from the data points.\n    3.  For each subsequent centroid, a data point $\\mathbf{v}_i$ is chosen with probability proportional to the squared Euclidean distance to its nearest existing centroid.\n-   **Iteration:**\n    1.  **Assignment:** Each data point is assigned to the cluster corresponding to its nearest centroid.\n    2.  **Update:** The centroid of each cluster is recalculated as the mean of all points assigned to it. If a cluster becomes empty, its centroid is not updated.\n    This process is repeated for a fixed number of iterations ($200$, as specified). The algorithm outputs the final centroids and the cluster assignment (label) for each data point.\n\n**Step 4: Calculation and Verification.**\n1.  **Cluster Radii:** For each cluster $C_j$, the cluster radius $R_j$ is computed as the maximum Euclidean distance between any point in $C_j$ and the cluster centroid $\\mathbf{c}_j$.\n2.  **Medoid Selection:** The medoid $\\mathbf{m}_j$ for each cluster $C_j$ is identified as the data point in $C_j$ with the minimum Euclidean distance to the centroid $\\mathbf{c}_j$. Ties are broken by choosing the point with the smallest original index in the $40$-point pool.\n3.  **Covering Radius:** A distance matrix is computed between all $40$ data points and the $k$ selected medoids. The global covering radius $R_{\\text{cov}}(S)$ is then found by first taking the minimum distance for each data point to any medoid, and then taking the maximum of these minimum distances.\n4.  **Inequality Check:** The theoretical bound is calculated as $B = 2 \\cdot \\max_j R_j$. The inequality $R_{\\text{cov}}(S) \\le B$ is evaluated, yielding a binary result ($1$ for true, $0$ for false).\n\nThis complete pipeline is executed for each of the three test cases specified, and the results ($R_{\\text{cov}}$, $B$, and the binary flag) are collected and formatted as required.", "answer": "```python\nimport numpy as np\nfrom scipy.spatial.distance import cdist\n\ndef solve():\n    \"\"\"\n    Main function to run the active learning preselection pipeline and verify the covering property.\n    \"\"\"\n\n    def generate_descriptors():\n        \"\"\"\n        Constructs the pool of descriptor vectors for the A-B-A system.\n        \"\"\"\n        r_vals = np.array([1.0, 1.2, 1.4, 1.6, 1.8, 2.0])\n        theta_vals_deg = np.array([60, 80, 100, 120, 140, 160])\n        theta_vals_rad = np.deg2rad(theta_vals_deg)\n\n        n_r = len(r_vals)\n        n_theta = len(theta_vals_deg)\n        n_unique = n_r * n_theta\n        descriptors = np.zeros((n_unique, 3))\n\n        idx = 0\n        for r in r_vals:\n            for theta in theta_vals_rad:\n                d_ab = r\n                d_aa = 2 * r * np.sin(theta / 2)\n                # For theta >= 60 deg, d_aa >= r, so sorted vector is always [r, r, d_aa]\n                descriptors[idx] = [d_ab, d_ab, d_aa]\n                idx += 1\n        \n        # Append duplicates of the first 4 descriptors\n        final_descriptors = np.vstack([descriptors, descriptors[:4]])\n        return final_descriptors\n\n    def standardize_descriptors(v):\n        \"\"\"\n        Applies z-score standardization to the descriptor matrix.\n        \"\"\"\n        mean = np.mean(v, axis=0)\n        std = np.std(v, axis=0)\n        # Avoid division by zero if a column is constant\n        std[std == 0] = 1.0\n        return (v - mean) / std\n\n    def kmeans_plusplus_init(X, k, rng):\n        \"\"\"\n        Deterministic k-means++ initialization.\n        \"\"\"\n        n_samples, n_features = X.shape\n        centroids = np.zeros((k, n_features))\n        \n        # 1. Choose first centroid uniformly at random\n        first_idx = rng.choice(n_samples)\n        centroids[0] = X[first_idx]\n        \n        # 2. For subsequent centroids\n        distances = np.full(n_samples, np.inf)\n        for i in range(1, k):\n            # Update distances to nearest known centroid\n            dist_sq = np.sum((X - centroids[i-1])**2, axis=1)\n            distances = np.minimum(distances, dist_sq)\n            \n            # Choose next centroid with probability proportional to D(x)^2\n            probs = distances / np.sum(distances)\n            next_idx = rng.choice(n_samples, p=probs)\n            centroids[i] = X[next_idx]\n            \n        return centroids\n\n    def kmeans_clustering(X, k, n_iter, seed):\n        \"\"\"\n        A complete, deterministic k-means implementation.\n        \"\"\"\n        rng = np.random.default_rng(seed)\n        \n        # Initialize centroids using k-means++\n        centroids = kmeans_plusplus_init(X, k, rng)\n\n        for _ in range(n_iter):\n            # Assignment step\n            dist_matrix = cdist(X, centroids, 'euclidean')\n            labels = np.argmin(dist_matrix, axis=1)\n            \n            # Update step\n            new_centroids = np.zeros_like(centroids)\n            for j in range(k):\n                cluster_points = X[labels == j]\n                if len(cluster_points) > 0:\n                    new_centroids[j] = np.mean(cluster_points, axis=0)\n                else:\n                    # Keep old centroid if cluster is empty\n                    new_centroids[j] = centroids[j]\n            centroids = new_centroids\n            \n        return centroids, labels\n\n    def run_pipeline(k, standardize, seed):\n        \"\"\"\n        Executes the entire pipeline for a single test case.\n        \"\"\"\n        # Step 1: Generate data\n        V = generate_descriptors()\n        \n        # Step 2: Standardize if required\n        X = standardize_descriptors(V) if standardize else V\n        n_points, n_dims = X.shape\n\n        # Step 3: Run k-means\n        centroids, labels = kmeans_clustering(X, k, n_iter=200, seed=seed)\n\n        # Step 4: Compute radii and medoids\n        cluster_radii = np.zeros(k)\n        medoids = np.zeros((k, n_dims))\n        medoid_indices = np.zeros(k, dtype=int)\n\n        for j in range(k):\n            cluster_mask = (labels == j)\n            if np.sum(cluster_mask) == 0:\n                # Handle empty cluster: radius is 0, medoid is undefined but we place it at centroid\n                cluster_radii[j] = 0.0\n                medoids[j] = centroids[j] \n                # This case is unlikely, if it happens, argmin on empty will fail.\n                # To assign a \"dummy\" medoid index, find the point closest to this lonely centroid\n                medoid_indices[j] = np.argmin(cdist(X, centroids[j:j+1]))\n                continue\n\n            cluster_points = X[cluster_mask]\n            point_indices = np.where(cluster_mask)[0]\n            \n            # Compute cluster radius\n            distances_to_centroid = cdist(cluster_points, centroids[j:j+1]).flatten()\n            cluster_radii[j] = np.max(distances_to_centroid)\n            \n            # Find medoid\n            medoid_local_idx = np.argmin(distances_to_centroid)\n            medoid_global_idx = point_indices[medoid_local_idx]\n            medoids[j] = X[medoid_global_idx]\n            medoid_indices[j] = medoid_global_idx\n        \n        # Step 5: Compute covering radius and bound\n        # Covering radius R_cov(S)\n        dist_to_medoids = cdist(X, medoids)\n        min_dist_to_medoid = np.min(dist_to_medoids, axis=1)\n        r_cov = np.max(min_dist_to_medoid)\n        \n        # Theoretical bound B\n        max_radius = np.max(cluster_radii) if k > 0 else 0.0\n        bound = 2 * max_radius\n        \n        # Check inequality\n        # Use a small tolerance for floating point comparison, although direct is fine\n        is_inequality_valid = 1 if r_cov <= bound + 1e-9 else 0\n        \n        return r_cov, bound, is_inequality_valid\n\n    # Define test cases from the problem statement\n    test_cases = [\n        # k, standardization_enabled, seed\n        (4, False, 42),\n        (1, True, 7),\n        (10, True, 123),\n    ]\n\n    results = []\n    for k, standardize, seed in test_cases:\n        r_cov, bound, is_valid = run_pipeline(k, standardize, seed)\n        results.append(f\"{r_cov:.6f}\")\n        results.append(f\"{bound:.6f}\")\n        results.append(str(is_valid))\n    \n    # Final print statement in the exact required format\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "2760075"}, {"introduction": "The final step in constructing a practical active learning workflow is to account for real-world computational limitations. Having an ideal batch of candidates is useless if you lack the resources to compute them within a given timeframe. This practice addresses this engineering challenge by framing batch selection as a constrained optimization problem, specifically a Multiple Knapsack Problem, and implementing a heuristic solver to maximize information gain while respecting parallel worker counts and walltime budgets. [@problem_id:2760084]", "problem": "Consider the design of an active learning loop for constructing a Potential Energy Surface (PES) where candidate molecular geometries are selected for expensive ab initio labeling. Each candidate geometry, indexed by $i \\in \\{1,\\dots,N\\}$, has an acquisition gain estimate $u_i \\ge 0$ (dimensionless), reflecting the expected information gain (for example, reduction in posterior variance under a Gaussian Process (GP) model), and a predicted walltime $r_i > 0$ in seconds for a single-label computation (for example, a single-point Density Functional Theory (DFT) calculation). The computational environment provides $W$ identical parallel evaluators, each with a per-iteration hard walltime budget $\\tau$ seconds. An item $i$ is schedulable only if $r_i \\le \\tau$. The iteration must choose a subset of items and a schedule such that no evaluator’s assigned jobs exceed the per-iteration walltime budget, and each chosen item is evaluated at most once.\n\nYour task is to formalize the batch selection under parallelism and walltime constraints as a discrete optimization problem and then implement an approximate solver. Use the following mathematical formalization requirements:\n\n1. Define binary assignment variables $y_{im} \\in \\{0,1\\}$ indicating whether item $i$ is assigned to evaluator $m \\in \\{1,\\dots,W\\}$, and selection variables $x_i \\in \\{0,1\\}$ with $x_i = \\sum_{m=1}^{W} y_{im}$. Enforce the constraints $\\sum_{i=1}^{N} r_i y_{im} \\le \\tau$ for each evaluator $m$ and $\\sum_{m=1}^{W} y_{im} \\le 1$ for each item $i$. The objective is to maximize $\\sum_{i=1}^{N} u_i x_i$. This is a Multiple 0-1 Knapsack Problem (MKP) variant and is computationally intractable (NP-hard) in general.\n2. Derive a principled approximation strategy starting from a well-tested base: the single 0-1 knapsack relaxation with total capacity $W \\tau$ and per-item feasibility $r_i \\le \\tau$, followed by a bin-packing feasibility step across $W$ bins of capacity $\\tau$ using a deterministic greedy schedule. Explicitly justify why this construction is a valid relaxation and how the feasibility repair restores the original per-evaluator constraints.\n\nYou must implement a complete program that, given a small test suite of instances, applies the following approximate solver:\n- Greedy preselection by descending ratio $u_i / r_i$ under the relaxed capacity budget $W \\tau$ (skip any item $i$ with $r_i > \\tau$).\n- Attempt to schedule the preselected items across $W$ evaluators with capacity $\\tau$ each using First-Fit Decreasing (FFD) on $r_i$.\n- If scheduling fails, iteratively remove one currently selected item with smallest ratio $u_i / r_i$ (break ties by removing the one with larger $r_i$) and retry scheduling, until a feasible assignment is found.\n- After achieving a feasible schedule, attempt to greedily insert any remaining (unselected) items in descending ratio order if they fit into the residual capacities of the $W$ evaluators without violating any constraints.\n\nYour program must solve the following test suite. For each test case, return the selected item indices using zero-based indexing as a list of integers sorted in ascending order. Aggregate all test case results into a single list of lists. The final output must be a single line with this aggregate list printed with no spaces.\n\nTest Suite (all runtimes must be interpreted in seconds; gains are dimensionless):\n- Test case $1$ (happy path):\n  - $u = [9.0, 6.0, 10.0, 12.0, 2.5, 7.5, 4.0]$\n  - $r = [1800, 1200, 2400, 3000, 600, 1500, 900]$\n  - $W = 2$, $\\tau = 3600$\n- Test case $2$ (boundary with exact fits and an unschedulable item):\n  - $u = [8.0, 7.5, 100.0, 7.0, 0.1]$\n  - $r = [2000, 2000, 2100, 1999, 1]$\n  - $W = 3$, $\\tau = 2000$\n- Test case $3$ (infeasible bin-packing under naive capacity that requires repair):\n  - $u = [10.0, 9.9, 3.9, 3.9, 3.9]$\n  - $r = [2500, 2500, 1000, 1000, 1000]$\n  - $W = 2$, $\\tau = 3000$\n- Test case $4$ (single evaluator with many small items and tie ratios):\n  - $u = [4.0, 3.0, 2.0, 1.0]$\n  - $r = [400, 400, 400, 400]$\n  - $W = 1$, $\\tau = 1000$\n- Test case $5$ (zero per-evaluator budget resulting in an empty selection):\n  - $u = [1.0, 2.0]$\n  - $r = [100, 200]$\n  - $W = 5$, $\\tau = 0$\n\nFinal Output Format:\n- Your program should produce a single line containing the results as a comma-separated list of lists enclosed in square brackets and with no spaces. For example: $[[0,2],[1],[]]$ for two nonempty selections followed by an empty selection. Your program must output exactly: $[\\text{case}_1,\\text{case}_2,\\dots,\\text{case}_5]$ where each $\\text{case}_k$ is a list of integers indicating the selected zero-based indices for test case $k$, sorted in ascending order.", "solution": "First, we formalize the problem. Let $N$ be the number of candidate molecular geometries. For each candidate $i \\in \\{1,\\dots,N\\}$, we are given a dimensionless information gain estimate $u_i \\ge 0$ and a required computational time $r_i > 0$. We have $W$ identical parallel evaluators, each with a maximum walltime budget of $\\tau$ seconds per iteration. An item $i$ is fundamentally unschedulable if its time requirement exceeds the evaluator budget, i.e., if $r_i > \\tau$.\n\nTo model the selection and assignment, we introduce binary decision variables. Let $y_{im} \\in \\{0,1\\}$ be a variable that equals $1$ if item $i$ is assigned to evaluator $m \\in \\{1,\\dots,W\\}$, and $0$ otherwise. Let $x_i \\in \\{0,1\\}$ be a variable that equals $1$ if item $i$ is selected for computation, and $0$ otherwise. The variables are related by $x_i = \\sum_{m=1}^{W} y_{im}$. The constraint that each item can be evaluated at most once is $\\sum_{m=1}^{W} y_{im} \\le 1$, which is equivalent to $x_i \\in \\{0,1\\}$.\n\nThe objective is to maximize the total information gain:\n$$\n\\text{maximize} \\quad Z = \\sum_{i=1}^{N} u_i x_i\n$$\n\nThis maximization is subject to two sets of constraints:\n$1.$ The total runtime of jobs assigned to any single evaluator $m$ must not exceed its walltime budget $\\tau$:\n$$\n\\sum_{i=1}^{N} r_i y_{im} \\le \\tau \\quad \\forall m \\in \\{1,\\dots,W\\}\n$$\n$2.$ Each item $i$ can be assigned to at most one evaluator:\n$$\n\\sum_{m=1}^{W} y_{im} \\le 1 \\quad \\forall i \\in \\{1,\\dots,N\\}\n$$\nAs stated, this is the Multiple $0$-$1$ Knapsack Problem. The $W$ evaluators are the \"knapsacks,\" each with capacity $\\tau$. The items have \"weights\" $r_i$ and \"values\" $u_i$.\n\nThe specified approximation strategy is a principled multi-step heuristic. It begins by relaxing the multiple-knapsack constraint into a single-knapsack constraint. Any feasible solution to the original MKP, defined by a set of assignments $\\{y_{im}\\}$, must satisfy $\\sum_{i=1}^{N} r_i x_i = \\sum_{i=1}^{N} r_i \\left(\\sum_{m=1}^{W} y_{im}\\right) = \\sum_{m=1}^{W} \\left(\\sum_{i=1}^{N} r_i y_{im}\\right) \\le \\sum_{m=1}^{W} \\tau = W\\tau$. Thus, a problem with a single knapsack of total capacity $W\\tau$ is a valid relaxation, as its feasible set contains all feasible solutions of the original problem. However, the converse is not true; a set of items that fits within the total capacity $W\\tau$ is not guaranteed to be partitionable among $W$ knapsacks of individual capacity $\\tau$. This necessitates a feasibility check and repair procedure.\n\nThe algorithmic steps are designed as follows:\n\n$1$. **Greedy Preselection:** We first filter out any items for which $r_i > \\tau$, as they are inherently unschedulable. Then, we address the relaxed single-knapsack problem with capacity $W\\tau$. We use a standard greedy strategy based on the efficiency ratio, or \"value density,\" $u_i/r_i$. Items are sorted in descending order of this ratio, and we greedily select them until the cumulative runtime exceeds the total capacity $W\\tau$. This creates an initial candidate set of high-value-density items.\n\n$2$. **Scheduling and Repair:** The candidate set from the previous step must now be tested for feasibility against the original, stricter MKP constraints. This is equivalent to a bin packing problem: can the selected items (with sizes $r_i$) be packed into $W$ bins (with capacity $\\tau$)? We use the First-Fit Decreasing (FFD) heuristic. Items are sorted by decreasing runtime $r_i$ and are sequentially placed into the first available evaluator (bin) that has sufficient remaining capacity. If FFD fails to place all items, the candidate set is infeasible. In this case, we enter a repair loop: we systematically remove the least \"efficient\" item from the candidate set—the one with the lowest $u_i/r_i$ ratio (with ties broken by removing the item with larger $r_i$)—and re-attempt the FFD scheduling. This process is repeated until a feasible schedule is found. If the candidate set becomes empty, the only feasible solution is to select no items.\n\n$3$. **Greedy Augmentation:** Once a feasible, scheduled subset is established, the solution may be suboptimal because the repair process might have removed valuable items. This step attempts to improve the solution by re-introducing any previously unselected items. We iterate through all items not in the current feasible set, again in descending order of their $u_i/r_i$ ratio. For each item, we check if it can fit into the residual capacity of any of the $W$ evaluators. If a fit is found, the item is added to the schedule. This greedy local search step ensures that we do not needlessly discard items that could have been included.\n\nThis composite algorithm correctly balances the trade-off between maximizing value (via the relaxed knapsack heuristic) and satisfying the complex partitioning constraints (via the FFD and repair loop), providing a robust and deterministic method for solving this optimization problem.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to define and run the test suite for the active learning\n    batch selection problem.\n    \"\"\"\n    test_cases = [\n        # Test case 1 (happy path)\n        (\n            [9.0, 6.0, 10.0, 12.0, 2.5, 7.5, 4.0],\n            [1800, 1200, 2400, 3000, 600, 1500, 900],\n            2,\n            3600\n        ),\n        # Test case 2 (boundary with exact fits and an unschedulable item)\n        (\n            [8.0, 7.5, 100.0, 7.0, 0.1],\n            [2000, 2000, 2100, 1999, 1],\n            3,\n            2000\n        ),\n        # Test case 3 (infeasible bin-packing that requires repair)\n        (\n            [10.0, 9.9, 3.9, 3.9, 3.9],\n            [2500, 2500, 1000, 1000, 1000],\n            2,\n            3000\n        ),\n        # Test case 4 (single evaluator with many small items)\n        (\n            [4.0, 3.0, 2.0, 1.0],\n            [400, 400, 400, 400],\n            1,\n            1000\n        ),\n        # Test case 5 (zero per-evaluator budget)\n        (\n            [1.0, 2.0],\n            [100, 200],\n            5,\n            0\n        )\n    ]\n\n    results = []\n    for u, r, W, tau in test_cases:\n        result = solve_instance(np.array(u), np.array(r), W, tau)\n        results.append(result)\n\n    def format_list(lst):\n        return '[' + ','.join(map(str, lst)) + ']'\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(format_list, results))}]\")\n\ndef solve_instance(u, r, W, tau):\n    \"\"\"\n    Implements the specified approximation algorithm for the Multiple Knapsack Problem variant.\n    \n    Args:\n        u (np.ndarray): Array of acquisition gain estimates.\n        r (np.ndarray): Array of predicted walltimes.\n        W (int): Number of parallel evaluators.\n        tau (float): Walltime budget per evaluator.\n\n    Returns:\n        list: A sorted list of zero-based indices of the selected items.\n    \"\"\"\n    N = len(u)\n    items = []\n    for i in range(N):\n        if r[i] > 0:\n            items.append({'u': u[i], 'r': r[i], 'ratio': u[i] / r[i], 'idx': i})\n    \n    # Pre-filter items that can never be scheduled\n    schedulable_items = [item for item in items if item['r'] = tau]\n    \n    if not schedulable_items:\n        return []\n\n    # Sort all schedulable items by ratio descending for preselection and augmentation.\n    # The original index is used as a secondary key for deterministic sorting.\n    schedulable_items.sort(key=lambda x: (-x['ratio'], x['idx']))\n    \n    # Step 1: Greedy Preselection based on the relaxed problem\n    preselected_items = []\n    relaxed_capacity = W * tau\n    current_weight = 0.0\n    for item in schedulable_items:\n        if current_weight + item['r'] = relaxed_capacity:\n            preselected_items.append(item)\n            current_weight += item['r']\n\n    # Step 2  3: Iterative Scheduling and Repair\n    final_selection = []\n    bins_final = []\n    \n    while True:\n        if not preselected_items:\n            final_selection = []\n            bins_final = [tau] * W\n            break\n\n        # Sort current selection by runtime descending for FFD\n        items_to_schedule = sorted(preselected_items, key=lambda x: -x['r'])\n        \n        bins = [tau] * W\n        schedule_possible = True\n        \n        for item in items_to_schedule:\n            placed = False\n            for m in range(W):\n                if item['r'] = bins[m]:\n                    bins[m] -= item['r']\n                    placed = True\n                    break\n            if not placed:\n                schedule_possible = False\n                break\n        \n        if schedule_possible:\n            final_selection = preselected_items\n            bins_final = bins\n            break\n        else:\n            # Repair: remove item with smallest ratio, tie-break by larger r\n            remover_sort_key = lambda x: (x['ratio'], -x['r'])\n            preselected_items.sort(key=remover_sort_key)\n            preselected_items.pop(0)\n\n    # Step 4: Greedy Augmentation\n    if final_selection: # Only augment if a non-empty schedule was found\n        scheduled_indices = {item['idx'] for item in final_selection}\n        # Remaining items are already sorted by descending ratio\n        remaining_items = [item for item in schedulable_items if item['idx'] not in scheduled_indices]\n        \n        for item in remaining_items:\n            for m in range(W):\n                if item['r'] = bins_final[m]:\n                    bins_final[m] -= item['r']\n                    final_selection.append(item)\n                    break \n\n    result_indices = sorted([item['idx'] for item in final_selection])\n    return result_indices\n\n# Execute the solver\nsolve()\n```", "id": "2760084"}]}