{"hands_on_practices": [{"introduction": "The foundation of using a single molecular dynamics trajectory to compute thermodynamic properties rests on the ergodic hypothesis, which states that a time average along one long trajectory is equivalent to an average over a statistical ensemble. This first practice explores this crucial assumption through systems where ergodicity breaks down. By analyzing these cases, you will develop a deeper understanding of when a single trajectory is sufficient and why averaging over multiple independent simulations can be essential for obtaining physically meaningful correlation functions [@problem_id:2825850].", "problem": "A classical Hamiltonian system consists of $2$ uncoupled harmonic oscillators with identical masses $m$ and distinct angular frequencies $\\omega_1 \\neq \\omega_2$:\n$$\nH(q_1,q_2,p_1,p_2)=\\frac{p_1^2}{2m}+\\frac{1}{2}m\\omega_1^2 q_1^2+\\frac{p_2^2}{2m}+\\frac{1}{2}m\\omega_2^2 q_2^2.\n$$\nYou perform a long microcanonical molecular dynamics trajectory at fixed total energy $E$ from a single initial condition, and you wish to estimate the velocity autocorrelation function of the first oscillator from this single trajectory and compare it to the microcanonical ensemble average at the same energy. Separately, consider a deeply supercooled glass-forming liquid where structural relaxation is so slow that the system exhibits aging over the observation window; you attempt to estimate a time correlation function from a single long trajectory by assuming time-translation invariance.\n\nWhich statements about the impact of ergodicity failure on correlation function estimates are correct? Select all that apply.\n\nA. For the integrable system of $2$ uncoupled oscillators at fixed total energy $E$, a long single-trajectory estimate of the velocity autocorrelation of the first oscillator generally depends on the initial partition of energy between modes and need not reproduce the microcanonical average that mixes over all partitions.\n\nB. Liouville’s theorem guarantees that a single trajectory of the integrable system will eventually visit all points on the constant-energy surface, so time-averaged correlation functions from one trajectory always converge to the microcanonical ensemble result.\n\nC. In a glassy system that ages, two-time correlation functions depend on the waiting time, so imposing time-translation invariance to estimate a stationary correlation from a single trajectory introduces a systematic bias that does not vanish by extending the trajectory length.\n\nD. Dividing a single trajectory into increasingly long blocks and averaging block-wise correlation estimates removes the bias associated with nonergodicity or aging.\n\nE. Averaging correlation functions over many independent trajectories initiated in different regions of phase space (e.g., different basins or invariant tori) can reduce or eliminate bias due to ergodicity breaking by better approximating the ensemble average at the desired thermodynamic state.", "solution": "The validity of the problem statement is hereby confirmed. The scenarios described—an integrable system of uncoupled oscillators and a non-equilibrium glassy system—are standard, well-defined models in statistical mechanics used to illustrate the failure of the ergodic hypothesis. The language is precise and the question is scientifically grounded and well-posed. We may proceed with the analysis.\n\nThe ergodic hypothesis posits that for a sufficiently long time $T$, the time average of an observable $A$ along a single trajectory is equal to the ensemble average of $A$ in the appropriate statistical ensemble. For an isolated system at constant energy $E$, this is the microcanonical ensemble.\n$$\n\\overline{A} = \\lim_{T \\to \\infty} \\frac{1}{T} \\int_0^T A(\\Gamma(t)) \\, dt = \\langle A \\rangle_{ME} = \\frac{\\int d\\Gamma \\, A(\\Gamma) \\, \\delta(H(\\Gamma) - E)}{\\int d\\Gamma \\, \\delta(H(\\Gamma) - E)}\n$$\nA system is ergodic if a single trajectory explores the entirety of the constant-energy hypersurface, visiting the neighborhood of every point. When a system is not ergodic, the time average depends on the initial conditions, as the trajectory is restricted to a submanifold of the energy surface.\n\nThe problem presents two distinct cases of ergodicity failure.\n\nCase $1$: The integrable system of $2$ uncoupled harmonic oscillators. The Hamiltonian is $H = H_1 + H_2$, with $H_1 = \\frac{p_1^2}{2m} + \\frac{1}{2}m\\omega_1^2 q_1^2$ and $H_2 = \\frac{p_2^2}{2m} + \\frac{1}{2}m\\omega_2^2 q_2^2$. Since the oscillators are uncoupled, not only is the total energy $E = H_1+H_2$ a constant of motion, but $H_1$ and $H_2$ are individually conserved. A single trajectory is launched with a specific initial partition of energy, $E_1$ and $E_2$, such that $E_1+E_2=E$. The trajectory is then confined to the surface defined by the two simultaneous conditions $H_1=E_1$ and $H_2=E_2$. This surface is a $2$-dimensional torus within the $3$-dimensional constant total energy surface defined by $H=E$. The trajectory does not explore other regions of the energy surface corresponding to different partitions of energy $(E'_1, E'_2)$. The system is therefore not ergodic.\n\nCase $2$: The supercooled glass-forming liquid. Such systems are characterized by a rugged potential energy landscape with a vast number of local minima separated by high energy barriers. Below the glass transition temperature, the system becomes kinetically arrested in one region of this landscape (a \"basin\" or \"metabasin\"). A single molecular dynamics trajectory will not sample other basins on any feasible simulation timescale. Furthermore, the system is not in equilibrium and its properties slowly evolve with time, a phenomenon known as aging. This means the system is not stationary; time-translation invariance is broken.\n\nWith this foundation, we evaluate each statement.\n\n**A. For the integrable system of $2$ uncoupled oscillators at fixed total energy $E$, a long single-trajectory estimate of the velocity autocorrelation of the first oscillator generally depends on the initial partition of energy between modes and need not reproduce the microcanonical average that mixes over all partitions.**\n\nThe velocity of the first oscillator is $v_1 = p_1/m$. The velocity autocorrelation function (VACF) is $C_{v_1v_1}(t) = \\langle v_1(0) v_1(t) \\rangle$. A time average calculated from a single trajectory is determined by the dynamics on a specific torus where the energy of the first oscillator is fixed at its initial value, $E_1$. For instance, the value of the VACF at time $t=0$ is $\\langle v_1^2 \\rangle = \\langle p_1^2/m^2 \\rangle$. For a harmonic oscillator, the average kinetic energy is half the total energy, so the time average is $\\overline{v_1^2} = E_1/m$. This value is determined by the initial energy partition. The microcanonical ensemble average, by contrast, averages over all possible partitions of $E$ between the two oscillators. The ensemble-averaged value $\\langle v_1^2 \\rangle_{ME}$ would be an average of $E'_1/m$ over all possible values of $E'_1$ from $0$ to $E$. Since the time average depends on the specific initial $E_1$, it is not equal to the microcanonical average. The statement is a direct and accurate consequence of the non-ergodicity of the system.\n\nVerdict: **Correct**.\n\n**B. Liouville’s theorem guarantees that a single trajectory of the integrable system will eventually visit all points on the constant-energy surface, so time-averaged correlation functions from one trajectory always converge to the microcanonical ensemble result.**\n\nThis statement is fundamentally flawed. Liouville’s theorem states that the density of states in phase space is conserved along a trajectory, implying that phase space volume is an invariant of Hamiltonian flow. It makes no claim about the trajectory's coverage of the energy surface. The property that a trajectory visits almost every point on the energy surface is the definition of ergodicity. As established, the system of $2$ uncoupled oscillators is not ergodic. A trajectory is confined to a lower-dimensional torus and does not explore the full constant-energy surface. Therefore, the premise of the statement is a misinterpretation of Liouville's theorem and is factually incorrect for this system. The conclusion, which is an assertion of the ergodic hypothesis, is consequently also false.\n\nVerdict: **Incorrect**.\n\n**C. In a glassy system that ages, two-time correlation functions depend on the waiting time, so imposing time-translation invariance to estimate a stationary correlation from a single trajectory introduces a systematic bias that does not vanish by extending the trajectory length.**\n\nAging implies that the system's statistical properties are changing with time. It is a non-stationary process. A two-time correlation function, $C(t_1, t_2)$, will depend on both time arguments, not just their difference $\\tau = t_2 - t_1$. It is common to express this as a dependence on a \"waiting time\" $t_w = t_1$ and a lag time $\\tau$. Assuming time-translation invariance (TTI) and computing a correlation function via a simple time average, $C(\\tau) = \\frac{1}{T_{max}-\\tau} \\int_0^{T_{max}-\\tau} A(t') B(t'+\\tau) dt'$, improperly averages over different waiting times $t'$. As the system ages, its state at later times is different from its state at earlier times. Therefore, this averaging procedure conflates different physical states of the system. This introduces a systematic bias. Extending the trajectory length $T_{max}$ does not resolve this issue, as the aging process continues and the system never reaches a stationary equilibrium state over the extended time. The calculated average will itself depend on $T_{max}$.\n\nVerdict: **Correct**.\n\n**D. Dividing a single trajectory into increasingly long blocks and averaging block-wise correlation estimates removes the bias associated with nonergodicity or aging.**\n\nThis procedure, known as block averaging, is a method for estimating the statistical uncertainty of an average, under the assumption that the process is ergodic and stationary. It does not correct for a systematic failure of ergodicity or stationarity.\nFor the non-ergodic oscillator system, any block, regardless of its length, is still a segment of the same trajectory confined to the same invariant torus. The average over each block will converge to the same biased result (the average over that torus). Averaging these identical results does not yield the correct ensemble average.\nFor the aging glass, different blocks correspond to different epochs of the system's history. Block averages will systematically differ from one another because the system's properties are evolving. Averaging them together obscures this evolution and does not produce an unbiased estimate of any meaningful physical quantity. The method fails to address the underlying problem in both scenarios.\n\nVerdict: **Incorrect**.\n\n**E. Averaging correlation functions over many independent trajectories initiated in different regions of phase space (e.g., different basins or invariant tori) can reduce or eliminate bias due to ergodicity breaking by better approximating the ensemble average at the desired thermodynamic state.**\n\nThis is the standard and correct computational strategy to overcome ergodicity breaking. The core principle of ensemble averaging is to average over all possible microstates consistent with the macroscopic constraints. If a single trajectory cannot sample all relevant microstates, then one must explicitly do so by initiating multiple, independent trajectories. For the oscillator system, one would start trajectories with a representative distribution of initial energy partitions $(E_1, E_2)$ to sample the entire energy surface $H=E$. For the glassy system, starting trajectories from different initial configurations allows for the sampling of different basins in the energy landscape. By averaging the results from these many trajectories, one constructs an approximation to the true ensemble average, thereby mitigating or eliminating the bias that arises from a single non-ergodic trajectory.\n\nVerdict: **Correct**.", "answer": "$$\\boxed{ACE}$$", "id": "2825850"}, {"introduction": "Moving from theory to practice, we must correctly handle the raw data generated by simulations, which often use periodic boundary conditions (PBC). Naively calculating properties like mean-squared displacement from the \"wrapped\" coordinates confined to the simulation box can lead to unphysical results. This exercise will guide you through the correct procedures for processing trajectory data under PBC, highlighting the critical distinction between observables that require unwrapped coordinates and those that do not [@problem_id:2825806].", "problem": "A molecular dynamics trajectory of a single diffusing solute in a cubic box of edge length $L$ with Periodic Boundary Conditions (PBC) is saved at times $t_n = n\\,\\Delta t$ as two series: the wrapped positions $\\{\\mathbf{r}_{\\mathrm{wrap}}(t_n)\\}$, where each Cartesian coordinate is reduced modulo $L$ into $[0,L)$ at every frame, and the instantaneous velocities $\\{\\mathbf{v}(t_n)\\}$ produced by the integrator. You are asked to compute the mean-squared displacement $\\langle \\Delta r^2(t)\\rangle$ and the Velocity Autocorrelation Function (VACF). Your colleague proposes several claims about when unwrapped coordinates are essential and how to proceed if only wrapped positions are available.\n\nUsing only fundamental definitions of displacement, periodic imaging, and time autocorrelation, decide which of the following statements are correct. Select all that apply.\n\nA. For $\\langle \\Delta r^2(t)\\rangle$, it is sufficient to apply the minimum-image convention once to $\\mathbf{r}_{\\mathrm{wrap}}(t)-\\mathbf{r}_{\\mathrm{wrap}}(0)$; this yields the true displacement for all $t$ and therefore the correct mean-squared displacement without explicitly unwrapping.\n\nB. For $\\langle \\Delta r^2(t)\\rangle$ under PBC, one must reconstruct the continuous trajectory in the infinite tiling, either by explicit unwrapping (tracking integer box crossings) or by cumulatively summing frame-to-frame minimum-image displacements to obtain the net displacement; otherwise the result is systematically wrong at long times.\n\nC. The VACF can be computed directly from the stored velocities $\\mathbf{v}(t)$ without any unwrapping because imaging operations affect stored positions but not the instantaneous velocities that solve the equations of motion.\n\nD. If velocities are not stored, one can still compute a correct VACF without unwrapping by estimating $\\mathbf{v}(t_n)\\approx\\big[\\mathbf{r}_{\\mathrm{wrap}}(t_{n+1})-\\mathbf{r}_{\\mathrm{wrap}}(t_n)\\big]_{\\mathrm{MIC}}/\\Delta t$, where $[\\cdot]_{\\mathrm{MIC}}$ denotes mapping each component of the difference into $(-L/2,L/2]$ at each step; for sufficiently small $\\Delta t$ this equals the true finite-difference velocity.\n\nE. It is acceptable to estimate velocities for the VACF by naive finite differences $\\big(\\mathbf{r}_{\\mathrm{wrap}}(t+\\tau)-\\mathbf{r}_{\\mathrm{wrap}}(t)\\big)/\\tau$ over long lags $\\tau$ without any minimum-image mapping when the box is cubic; any errors from boundary crossings average out in the autocorrelation.", "solution": "The problem requires an evaluation of several procedural claims regarding the calculation of the mean-squared displacement, $\\langle \\Delta r^2(t)\\rangle$, and the velocity autocorrelation function, $C_{vv}(t)$, from a molecular dynamics trajectory generated with periodic boundary conditions (PBC). The core of the problem lies in the distinction between \"wrapped\" coordinates, confined to the primary simulation box, and \"unwrapped\" coordinates, which represent the true, continuous path of a particle through the infinite periodic tiling of space.\n\nFirst, let us establish the fundamental definitions.\n\nThe true displacement of the solute over a time interval $t$ starting at time $\\tau$ is given by the vector connecting its unwrapped positions:\n$$\n\\Delta \\mathbf{r}(t; \\tau) = \\mathbf{r}_{\\mathrm{unwrapped}}(t+\\tau) - \\mathbf{r}_{\\mathrm{unwrapped}}(\\tau)\n$$\nThe unwrapped position $\\mathbf{r}_{\\mathrm{unwrapped}}(t)$ tracks the continuous path, accumulating displacements across periodic boundaries. The wrapped position $\\mathbf{r}_{\\mathrm{wrap}}(t)$ is related by $\\mathbf{r}_{\\mathrm{unwrapped}}(t) = \\mathbf{r}_{\\mathrm{wrap}}(t) + L \\mathbf{n}(t)$, where $\\mathbf{n}(t)$ is an integer vector counting the number of times the particle has crossed the box boundaries in each Cartesian direction.\n\nThe mean-squared displacement (MSD) is the ensemble average of the squared magnitude of this true displacement:\n$$\n\\langle \\Delta r^2(t)\\rangle = \\left\\langle |\\Delta \\mathbf{r}(t; \\tau)|^2 \\right\\rangle_\\tau = \\left\\langle |\\mathbf{r}_{\\mathrm{unwrapped}}(t+\\tau) - \\mathbf{r}_{\\mathrm{unwrapped}}(\\tau)|^2 \\right\\rangle_\\tau\n$$\nwhere the average $\\langle \\cdot \\rangle_\\tau$ is over all possible time origins $\\tau$. For a diffusing particle in three dimensions, we expect $\\langle \\Delta r^2(t)\\rangle \\to 6Dt$ for long times $t$, where $D$ is the diffusion coefficient. This shows that the MSD must be an unbounded, growing function of time.\n\nThe velocity autocorrelation function (VACF) is defined as:\n$$\nC_{vv}(t) = \\langle \\mathbf{v}(\\tau) \\cdot \\mathbf{v}(t+\\tau) \\rangle_\\tau\n$$\nwhere $\\mathbf{v}(t)$ is the instantaneous velocity of the particle at time $t$.\n\nThe minimum-image convention (MIC) for a displacement vector $\\Delta\\mathbf{r} = \\mathbf{r}_2 - \\mathbf{r}_1$ in a cubic box of side $L$ yields a new vector $[\\Delta\\mathbf{r}]_{\\mathrm{MIC}}$ where each component $\\Delta r_i$ is mapped into the interval $[-L/2, L/2)$ by adding or subtracting integer multiples of $L$. This gives the shortest possible vector connecting particle $1$ to any periodic image of particle $2$.\n\nWith these definitions, we proceed to analyze each statement.\n\n**A. For $\\langle \\Delta r^2(t)\\rangle$, it is sufficient to apply the minimum-image convention once to $\\mathbf{r}_{\\mathrm{wrap}}(t)-\\mathbf{r}_{\\mathrm{wrap}}(0)$; this yields the true displacement for all $t$ and therefore the correct mean-squared displacement without explicitly unwrapping.**\n\nThis statement claims that the true displacement $\\Delta \\mathbf{r}(t; 0)$ is equal to $[\\mathbf{r}_{\\mathrm{wrap}}(t) - \\mathbf{r}_{\\mathrm{wrap}}(0)]_{\\mathrm{MIC}}$. This is incorrect for a general time $t$. The magnitude of the vector obtained via the MIC is bounded; specifically, its squared magnitude cannot exceed $(L/2)^2 + (L/2)^2 + (L/2)^2 = 3L^2/4$. However, the true mean-squared displacement for a diffusing particle must grow linearly with time and is not bounded. For any time $t$ long enough that the particle is likely to have diffused a distance greater than $L/2$ in any one direction, the MIC displacement will no longer equal the true displacement. The resulting MSD curve would be systematically underestimated and would artificially plateau at a value related to $L^2$, which is physically incorrect. This procedure is only valid for very short times.\nVerdict: **Incorrect**.\n\n**B. For $\\langle \\Delta r^2(t)\\rangle$ under PBC, one must reconstruct the continuous trajectory in the infinite tiling, either by explicit unwrapping (tracking integer box crossings) or by cumulatively summing frame-to-frame minimum-image displacements to obtain the net displacement; otherwise the result is systematically wrong at long times.**\n\nThis statement accurately describes the necessary procedure. To compute the true displacement $\\Delta \\mathbf{r}(t; \\tau)$, one must know the unwrapped coordinates. These can be obtained by post-processing the trajectory. A standard algorithm is to reconstruct the unwrapped path iteratively:\n$$\n\\mathbf{r}_{\\mathrm{unwrapped}}(t_{n+1}) = \\mathbf{r}_{\\mathrm{unwrapped}}(t_n) + [\\mathbf{r}_{\\mathrm{wrap}}(t_{n+1}) - \\mathbf{r}_{\\mathrm{wrap}}(t_n)]_{\\mathrm{MIC}}\n$$\nThis is equivalent to the \"cumulatively summing frame-to-frame minimum-image displacements\". This method is correct provided the simulation timestep $\\Delta t = t_{n+1}-t_n$ is small enough that the particle displacement in one step is less than $L/2$ in all dimensions. As established in the analysis of statement A, failure to reconstruct this continuous path leads to a systematically incorrect MSD at long times due to the artificial bounding of displacements.\nVerdict: **Correct**.\n\n**C. The VACF can be computed directly from the stored velocities $\\mathbf{v}(t)$ without any unwrapping because imaging operations affect stored positions but not the instantaneous velocities that solve the equations of motion.**\n\nThis statement is correct. In a standard MD simulation, forces on particles are computed based on the minimum-image distances. These forces are then used to update the velocities via an integrator (e.g., Velocity-Verlet). The position update may result in a coordinate value outside the primary box $[0,L)$. The PBC operation then maps this position back into the primary box, e.g., $x \\to x \\pmod L$. This is a discontinuous jump in the stored coordinate. However, the velocity vector $\\mathbf{v}(t)$, which represents the instantaneous rate of change of position, is not modified by this wrapping operation. A particle exiting one face of the box re-enters the opposite face with the exact same velocity vector. Therefore, the time series of velocities $\\{\\mathbf{v}(t_n)\\}$ stored by the simulation program represents the true, continuous velocity of the particle, and can be used directly to compute the VACF without any \"unwrapping\" procedure.\nVerdict: **Correct**.\n\n**D. If velocities are not stored, one can still compute a correct VACF without unwrapping by estimating $\\mathbf{v}(t_n)\\approx\\big[\\mathbf{r}_{\\mathrm{wrap}}(t_{n+1})-\\mathbf{r}_{\\mathrm{wrap}}(t_n)\\big]_{\\mathrm{MIC}}/\\Delta t$, where $[\\cdot]_{\\mathrm{MIC}}$ denotes mapping each component of the difference into $(-L/2,L/2]$ at each step; for sufficiently small $\\Delta t$ this equals the true finite-difference velocity.**\n\nThis statement proposes a method to approximate velocities from positions. The true finite-difference velocity is $\\mathbf{v}_{\\mathrm{FD}}(t_n) = (\\mathbf{r}_{\\mathrm{unwrapped}}(t_{n+1}) - \\mathbf{r}_{\\mathrm{unwrapped}}(t_n)) / \\Delta t$. The expression $[\\mathbf{r}_{\\mathrm{wrap}}(t_{n+1})-\\mathbf{r}_{\\mathrm{wrap}}(t_n)]_{\\mathrm{MIC}}$ gives the shortest vector connecting the wrapped positions between two consecutive frames. \"For sufficiently small $\\Delta t$\" implies that the true displacement in one timestep is less than $L/2$ in any dimension. Under this standard condition, the true displacement vector is identical to the minimum-image displacement vector:\n$$\n\\mathbf{r}_{\\mathrm{unwrapped}}(t_{n+1}) - \\mathbf{r}_{\\mathrm{unwrapped}}(t_n) = [\\mathbf{r}_{\\mathrm{wrap}}(t_{n+1}) - \\mathbf{r}_{\\mathrm{wrap}}(t_n)]_{\\mathrm{MIC}}\n$$\nTherefore, dividing by $\\Delta t$ yields the correct finite-difference approximation to the true instantaneous velocity. The VACF computed from this estimated velocity series will be correct. This procedure correctly circumvents the need for a full trajectory unwrapping by using local, frame-to-frame information.\nVerdict: **Correct**.\n\n**E. It is acceptable to estimate velocities for the VACF by naive finite differences $\\big(\\mathbf{r}_{\\mathrm{wrap}}(t+\\tau)-\\mathbf{r}_{\\mathrm{wrap}}(t)\\big)/\\tau$ over long lags $\\tau$ without any minimum-image mapping when the box is cubic; any errors from boundary crossings average out in the autocorrelation.**\n\nThis statement is fundamentally incorrect on multiple grounds. First, using the naive difference of wrapped positions $\\mathbf{r}_{\\mathrm{wrap}}(t+\\tau)-\\mathbf{r}_{\\mathrm{wrap}}(t)$ is incorrect. When a particle crosses a boundary, its wrapped coordinate changes discontinuously by approximately $\\pm L$. The naive difference will thus produce a very large, spurious displacement component (e.g., $x_2 \\approx 0, x_1 \\approx L \\implies x_2-x_1 \\approx -L$) that has no relation to the true velocity. The claim that these large, systematic errors will \"average out\" has no physical or mathematical basis; they will introduce severe artifacts and yield a meaningless VACF. Second, estimating an instantaneous velocity requires a very short time difference, ideally $\\tau = \\Delta t \\to 0$. Using a \"long lag $\\tau$\" calculates an average velocity over the interval, not the instantaneous velocity required for the VACF. The resulting correlation function would not be the VACF.\nVerdict: **Incorrect**.", "answer": "$$\\boxed{BCD}$$", "id": "2825806"}, {"introduction": "With a properly prepared time series of an observable, the final step is to compute the correlation function itself. For the long trajectories typical in modern simulations, the choice of algorithm has a dramatic impact on performance. This practice contrasts the straightforward but slow direct summation method with the highly efficient algorithm based on the Fast Fourier Transform (FFT), providing a quantitative appreciation for the computational speedup that makes large-scale correlation analysis feasible [@problem_id:2825820].", "problem": "A real-valued, zero-mean observable $A(t)$ is sampled uniformly from a Molecular Dynamics (MD) trajectory at interval $\\Delta t = 2$ fs for a total of $N = 10^{6}$ samples. The goal is to compute the discrete, linear time autocorrelation function up to a maximum lag time $T_{\\max} = 5 \\tau_{c}$, where the correlation time is $\\tau_{c} = 1$ ps. Define the discrete-time series $A_{n} = A(n \\Delta t)$ for $n = 0, 1, \\dots, N-1$, and the unbiased linear estimator of the autocorrelation for lag $k$ as\n$$\nC(k) = \\frac{1}{N-k} \\sum_{n=0}^{N-k-1} A_{n} A_{n+k}, \\quad 0 \\le k \\le K,\n$$\nwhere $K$ is the largest integer such that $K \\Delta t \\le T_{\\max}$. You will compare two computational strategies:\n\n- A direct summation method that evaluates the above definition for all $k = 0, 1, \\dots, K$.\n- A method based on the Fast Fourier Transform (FFT), using the Discrete Fourier Transform (DFT) to compute a circular correlation that is made equivalent to the linear correlation up to lag $K$ by zero-padding.\n\nStarting only from the definitions of linear correlation and circular convolution and the following well-tested complexity models, derive and compute the computational speedup of the FFT-based method relative to the direct method for this dataset, and state whether the two methods are numerically equivalent (up to rounding) for all lags $k \\le K$ under your padding choice:\n\n- A length-$M$ complex FFT requires approximately $5 M \\log_{2} M$ real floating-point operations (flops).\n- The FFT-based correlation requires one forward FFT of the zero-padded real sequence (treated at the cost of a complex FFT of the same length for this estimate), one elementwise magnitude-squared operation in the frequency domain costing $3 M$ flops, and one inverse FFT of the same cost as the forward FFT.\n- The direct method requires, for each lag $k$, $(N-k)$ multiply-accumulate operations; count one multiply and one add as two flops per term. Neglect the normalization costs and any mean-removal overhead.\n\nTasks:\n\n1) From the definition of linear correlation as a linear convolution, determine the minimal zero-padding length $M_{\\min}$ such that the circular correlation computed via the FFT equals the linear correlation for all $0 \\le k \\le K$. Then set the actual padded length to $N_{p} = 2^{\\lceil \\log_{2} M_{\\min} \\rceil}$.\n\n2) Using the given flop-count models, derive closed-form expressions for the total flop counts $F_{\\text{dir}}$ and $F_{\\text{fft}}$, evaluate them numerically for the given $N$, $\\Delta t$, and $\\tau_{c}$, and compute the speedup\n$$\nS = \\frac{F_{\\text{dir}}}{F_{\\text{fft}}}.\n$$\n\n3) Briefly justify (no calculation required) whether, under your zero-padding choice, the FFT-based and direct methods produce identical linear correlations for all $k \\le K$ aside from floating-point rounding.\n\nReport the single numerical value $S$ rounded to three significant figures. The speedup is dimensionless; report no units.", "solution": "The problem requires an analysis and comparison of two methods for computing a discrete time autocorrelation function: direct summation and a Fast Fourier Transform (FFT) based approach. The validity of the problem statement is affirmed, as it is scientifically grounded in the principles of signal processing and statistical mechanics, is well-posed with all necessary data provided, and is expressed in objective, formal language. We proceed with the solution.\n\nFirst, we determine the necessary parameters from the given data.\nThe number of samples is $N = 10^{6}$.\nThe sampling interval is $\\Delta t = 2$ fs.\nThe correlation time is $\\tau_{c} = 1$ ps, which is equivalent to $1000$ fs.\nThe maximum lag time is $T_{\\max} = 5 \\tau_{c} = 5 \\times 1 \\text{ ps} = 5$ ps, or $5000$ fs.\nThe maximum lag index, $K$, is the largest integer such that $K \\Delta t \\le T_{\\max}$.\n$$\nK = \\left\\lfloor \\frac{T_{\\max}}{\\Delta t} \\right\\rfloor = \\left\\lfloor \\frac{5000 \\text{ fs}}{2 \\text{ fs}} \\right\\rfloor = 2500\n$$\nThe autocorrelation is required for lags $k = 0, 1, \\dots, K$.\n\nThe first task is to determine the appropriate zero-padding length for the FFT-based method. The Wiener-Khinchin theorem allows the computation of an autocorrelation function via Fourier transforms. Specifically, the autocorrelation of a sequence is obtained by taking the inverse Fourier transform of its power spectral density. To compute the linear correlation using the Discrete Fourier Transform (DFT), which is inherently circular, the sequence must be zero-padded to a sufficient length to prevent wrap-around (aliasing) errors.\n\nLet the original sequence be $A_n$ of length $N$. We create a new sequence $A'_{n}$ of length $M$ by padding $A_n$ with zeros: $A'_{n} = A_n$ for $0 \\le n  N$, and $A'_{n} = 0$ for $N \\le n  M$. The circular autocorrelation of $A'_{n}$ at lag $k$ is given by\n$$\nC'_{\\text{circ}}(k) = \\sum_{n=0}^{M-1} A'_{n} A'_{(n+k) \\pmod{M}}\n$$\nWe require $C'_{\\text{circ}}(k)$ to be equal to the unnormalized linear correlation, $\\sum_{n=0}^{N-k-1} A_n A_{n+k}$, for all lags $0 \\le k \\le K$.\nExpanding the expression for the circular correlation gives two parts:\n$$\nC'_{\\text{circ}}(k) = \\sum_{n=0}^{M-k-1} A'_{n} A'_{n+k} + \\sum_{n=M-k}^{M-1} A'_{n} A'_{n+k-M}\n$$\nThe first summation, $\\sum_{n=0}^{M-k-1} A'_{n} A'_{n+k}$, equals the desired unnormalized linear correlation sum because $A'_{n}$ is non-zero only for $n  N$, so the product $A'_{n} A'_{n+k}$ is non-zero only when both $nN$ and $n+kN$. This correctly truncates the sum at $n=N-k-1$.\nThe second summation is the wrap-around term, which must be zero for the circular and linear correlations to match. For this term to be zero for all $0 \\le k \\le K$, at least one of the factors in each product $A'_{n} A'_{n+k-M}$ must be zero. The indices $n$ in this sum range from $M-k$ to $M-1$. The term $A'_{n}$ is zero if its index $n$ is greater than or equal to $N$. Thus, if we ensure that the minimum index in the sum, $M-k$, is greater than or equal to $N$ for all relevant $k$, the wrap-around term will vanish. The most stringent requirement occurs for the maximum lag, $k=K$. Therefore, we must have $M - K \\ge N$, which gives $M \\ge N+K$.\n\nThis condition sets the minimal padding length $M_{\\min}$ for the equivalence to hold for all lags up to $K$.\n$$\nM_{\\min} = N+K = 10^{6} + 2500 = 1002500\n$$\nThe problem specifies that the actual padded length, $N_p$, must be the next highest power of $2$ for FFT efficiency.\n$$\nN_p = 2^{\\lceil \\log_{2}(M_{\\min}) \\rceil} = 2^{\\lceil \\log_{2}(1002500) \\rceil}\n$$\nWe calculate $\\log_{2}(1002500) \\approx 19.936$. The ceiling is $\\lceil 19.936 \\rceil = 20$.\n$$\nN_p = 2^{20} = 1048576\n$$\n\nThe second task is to derive the total floating-point operation (flop) counts and compute the speedup.\nFor the direct summation method, the flop count for a single lag $k$ is $2(N-k)$, accounting for one multiplication and one addition per term in the sum. The total flop count, $F_{\\text{dir}}$, is the sum over all required lags from $k=0$ to $k=K$.\n$$\nF_{\\text{dir}} = \\sum_{k=0}^{K} 2(N-k) = 2 \\left( \\sum_{k=0}^{K} N - \\sum_{k=0}^{K} k \\right)\n$$\nUsing the formulas for the sum of a constant and the sum of the first $K$ integers:\n$$\nF_{\\text{dir}} = 2 \\left( N(K+1) - \\frac{K(K+1)}{2} \\right) = (K+1)(2N-K)\n$$\nSubstituting the values $N=10^6$ and $K=2500$:\n$$\nF_{\\text{dir}} = (2500+1)(2 \\times 10^{6} - 2500) = 2501 \\times (2000000 - 2500) = 2501 \\times 1997500 = 4995747500\n$$\nFor the FFT-based method, the flop count is based on operations on a sequence of length $N_p$. The procedure involves one forward FFT, one element-wise magnitude-squared operation, and one inverse FFT.\nThe cost of a forward complex FFT of length $N_p$ is given as $5 N_p \\log_{2} N_p$. The inverse FFT has the same cost. The problem states to use this cost for the real-to-complex transform as well. The magnitude-squared operation costs $3 N_p$ flops.\nThe total flop count for the FFT method, $F_{\\text{fft}}$, is:\n$$\nF_{\\text{fft}} = (5 N_p \\log_{2} N_p) + 3 N_p + (5 N_p \\log_{2} N_p) = 10 N_p \\log_{2} N_p + 3 N_p = N_p (10 \\log_{2} N_p + 3)\n$$\nSubstituting $N_p=2^{20}$, which means $\\log_{2} N_p = 20$:\n$$\nF_{\\text{fft}} = 2^{20} (10 \\times 20 + 3) = 1048576 \\times (200 + 3) = 1048576 \\times 203 = 212860928\n$$\nThe speedup, $S$, is the ratio of the two flop counts.\n$$\nS = \\frac{F_{\\text{dir}}}{F_{\\text{fft}}} = \\frac{4995747500}{212860928} \\approx 23.47163\n$$\nRounding to three significant figures, the speedup is $S \\approx 23.5$.\n\nThe third task is to justify the numerical equivalence of the two methods. The direct summation method is defined to compute the unbiased linear estimator $C(k) = \\frac{1}{N-k} \\sum_{n=0}^{N-k-1} A_{n} A_{n+k}$. The FFT method computes the circular correlation of a zero-padded sequence. As established in the first part, our choice of padding length $N_p \\ge N+K$ ensures that for all lags $0 \\le k \\le K$, the raw result of the circular correlation, before any normalization, is mathematically identical to the unnormalized linear correlation sum, $\\sum_{n=0}^{N-k-1} A_n A_{n+k}$. Therefore, if the result from the FFT method for each lag $k$ is subsequently normalized by the same factor $1/(N-k)$ as in the direct method, the two methods will yield the same final values. Any differences would arise only from the distinct floating-point rounding error accumulation paths of the two algorithms. Thus, under the specified padding and assuming identical subsequent normalization, the methods are numerically equivalent up to machine precision.", "answer": "$$\\boxed{23.5}$$", "id": "2825820"}]}