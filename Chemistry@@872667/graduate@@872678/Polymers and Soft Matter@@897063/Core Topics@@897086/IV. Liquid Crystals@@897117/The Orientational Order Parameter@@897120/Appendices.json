{"hands_on_practices": [{"introduction": "The first step in analyzing orientational order is to compute the order parameter tensor from a given set of molecular orientations. This practice guides you through the process of deriving the nematic order parameter tensor, $\\mathbf{Q}$, from first principles and implementing a robust numerical algorithm to extract the director, $\\mathbf{n}$, and scalar order parameter, $S$ [@problem_id:2932989]. Mastering this procedure is essential for processing raw data from molecular dynamics simulations or advanced microscopy experiments.", "problem": "Consider a collection of $N$ three-dimensional orientation samples $\\{\\mathbf{v}_i\\}_{i=1}^{N}$ for polymer segments or rod-like mesogens in a soft-matter system, where each $\\mathbf{v}_i \\in \\mathbb{R}^3$ may be non-unit due to measurement noise. The underlying physics is invariant under head-tail reversal of any segment, so the physical state is unchanged by $\\mathbf{u} \\mapsto -\\mathbf{u}$, where $\\mathbf{u}$ denotes a unit orientation. Your task is to construct from first principles a symmetric, traceless, second-rank tensor that quantifies orientational order and to design a robust numerical algorithm to extract its principal director and scalar measure of order from the discrete data.\n\nStart from the following foundational base:\n\n- The configuration is fully specified by an orientation distribution, which for discrete data is represented by the empirical measure on the unit sphere using the normalized orientations $\\mathbf{u}_i = \\mathbf{v}_i / \\|\\mathbf{v}_i\\|$, for all $i$ with $\\|\\mathbf{v}_i\\| > 0$.\n- Head-tail symmetry implies that only even moments of the orientation are physically meaningful; in particular, the second moment tensor $\\mathbf{M} = \\langle \\mathbf{u} \\mathbf{u}^{\\mathsf{T}} \\rangle$ is a natural building block, where $\\langle \\cdot \\rangle$ denotes an average over samples.\n- A second-rank descriptor of orientational order must be invariant under global rotations, symmetric, and traceless, and should vanish in an isotropic state. It should also distinguish uniaxial alignment by having a unique largest eigenvalue associated with a unit eigenvector called the director.\n\nTasks:\n\n1. Derive, using only the principles above, the unique (up to an overall scalar factor) symmetric, traceless tensor $\\mathbf{Q}$ that can be constructed from $\\mathbf{M}$ and the identity $\\mathbf{I}$, and fix its overall scalar factor by requiring that for perfectly aligned samples along a unit vector $\\mathbf{n}$ (that is, $\\mathbf{u}_i = \\mathbf{n}$ for all $i$), the largest eigenvalue equals $1$ while the tensor vanishes in the isotropic limit. Express $\\mathbf{Q}$ explicitly in terms of the discrete data $\\{\\mathbf{v}_i\\}_{i=1}^{N}$.\n2. Prove that for any discrete set of unit orientations, the tensor $\\mathbf{Q}$ is symmetric and traceless, and that the largest eigenvalue $S$ lies in the interval $[-\\frac{1}{2}, 1]$.\n3. Design a numerically stable algorithm that, given $\\{\\mathbf{v}_i\\}_{i=1}^{N}$, returns:\n   - The scalar $S$ equal to the largest eigenvalue of $\\mathbf{Q}$.\n   - The director $\\mathbf{n}$ equal to the corresponding unit eigenvector.\n   Your algorithm must:\n   - Normalize each nonzero $\\mathbf{v}_i$ to $\\mathbf{u}_i$ and ignore any $\\mathbf{v}_i$ with zero norm.\n   - Compute a sample second moment $\\mathbf{M}$ as an average of dyads $\\mathbf{u}_i \\mathbf{u}_i^{\\mathsf{T}}$.\n   - Construct $\\mathbf{Q}$ from $\\mathbf{M}$ so that it is symmetric and traceless and meets the normalization condition from Task $1$.\n   - Use an eigen-decomposition routine appropriate for real symmetric matrices.\n   - Implement a deterministic sign convention for $\\mathbf{n}$ to remove the $\\mathbf{n} \\leftrightarrow -\\mathbf{n}$ ambiguity: if the mean orientation $\\bar{\\mathbf{u}} = \\frac{1}{N}\\sum_{i=1}^{N}\\mathbf{u}_i$ has norm greater than a tolerance $\\epsilon > 0$, choose the sign of $\\mathbf{n}$ so that $\\mathbf{n}\\cdot \\bar{\\mathbf{u}} \\ge 0$. Otherwise (for nearly isotropic data), choose the sign so that the component of $\\mathbf{n}$ with the largest absolute value is nonnegative.\n   - Handle degenerate or nearly isotropic cases robustly: if all eigenvalues of $\\mathbf{Q}$ are within a small tolerance $\\delta > 0$ of zero, return $S = 0$ and $\\mathbf{n} = (0, 0, 1)$ by convention.\n4. Implement your algorithm as a program that evaluates the following test suite. Each test case is a list of three-dimensional vectors; angles are not used, and there are no physical units. Your program must treat all inputs as dimensionless and must normalize vectors internally.\n   - Test case A (uniaxial alignment with small perturbations about $\\hat{\\mathbf{z}}$): $\\left[(0,0,1),(0,0,1),(0,0,1),(0.1,0, \\sqrt{1-0.1^2}),(-0.1,0.05,\\sqrt{1-0.1^2-0.05^2}),(0.05,-0.02,\\sqrt{1-0.05^2-0.02^2})\\right]$.\n   - Test case B (exactly isotropic on Cartesian axes): $\\left[(1,0,0),(-1,0,0),(0,1,0),(0,-1,0),(0,0,1),(0,0,-1)\\right]$.\n   - Test case C (planar bias in the $xy$-plane with slight dominance along $\\hat{\\mathbf{x}}$): $\\left[(1,0,0),(-1,0,0),(1,0.1,0),(0,1,0),(0,-1,0),(0.7,0.7,0)\\right]$.\n   - Test case D (antiparallel pairs about a common director near $(1,1,1)$): $\\left[(1,1,1),(-1,-1,-1),(0.9,1.1,1.0),(-0.9,-1.1,-1.0)\\right]$.\n   - Test case E (single-vector boundary case): $\\left[(0.3,-0.4,0.8660254)\\right]$.\n5. Output specification. For each test case, output a list $[S,n_x,n_y,n_z]$ where $S$ is the largest eigenvalue of $\\mathbf{Q}$ and $(n_x,n_y,n_z)$ are the components of the director after applying the sign convention. All four numbers must be rounded to exactly $6$ decimal places. Your program should produce a single line of output containing the results for all test cases as a comma-separated list of these per-case lists, enclosed in square brackets. For example, an output with two hypothetical cases would look like $[[0.500000,0.000000,0.000000,1.000000],[0.000000,0.000000,0.000000,1.000000]]$.\n\nYour solution must derive the construction and algorithm from the stated principles and then implement it to produce the required output for the given test suite. No external input is required, and there are no physical units to report. Angles are not used, so there is no angle unit requirement. All numerical tolerances used by your algorithm must be explicitly stated and justified in the solution. The final output format must be exactly as specified above, as a single line. The answer for each test case is a list of floats as defined.", "solution": "The problem statement is submitted for validation.\n\n### Step 1: Extract Givens\n- Data: A set of $N$ orientation vectors $\\{\\mathbf{v}_i\\}_{i=1}^{N}$, where $\\mathbf{v}_i \\in \\mathbb{R}^3$.\n- Physics: Invariance under head-tail reversal ($\\mathbf{u} \\mapsto -\\mathbf{u}$).\n- Representation: Empirical measure from normalized vectors $\\mathbf{u}_i = \\mathbf{v}_i / \\|\\mathbf{v}_i\\|$ for $\\|\\mathbf{v}_i\\| > 0$.\n- Building Block: The second moment tensor $\\mathbf{M} = \\langle \\mathbf{u} \\mathbf{u}^{\\mathsf{T}} \\rangle$.\n- Required Properties for Order Tensor $\\mathbf{Q}$: Second-rank, symmetric, traceless, rotationally invariant, vanishes for an isotropic state, distinguishes uniaxial alignment via a unique largest eigenvalue and a director eigenvector.\n- Normalization Conditions: For perfect alignment ($\\mathbf{u}_i = \\mathbf{n}$ for all $i$), the largest eigenvalue of $\\mathbf{Q}$ is $1$. For an isotropic distribution, $\\mathbf{Q} = \\mathbf{0}$.\n- Algorithmic Requirements: A robust procedure to compute the largest eigenvalue $S$ and corresponding eigenvector (director) $\\mathbf{n}$ of $\\mathbf{Q}$ from the data $\\{\\mathbf{v}_i\\}$, including a deterministic sign convention for $\\mathbf{n}$ and handling of near-isotropic cases.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is evaluated against the validation criteria.\n- **Scientifically Grounded**: The problem is a standard exercise in the statistical mechanics of soft matter, specifically the construction of the nematic order parameter tensor (the de Gennes-Maier-Saupe tensor). All principles and definitions are correct and fundamental to the field.\n- **Well-Posed**: The problem provides sufficient constraints to uniquely derive the tensor $\\mathbf{Q}$ and its properties. The tasks are mathematically and algorithmically precise.\n- **Objective**: The problem is stated in objective, mathematical language, free of ambiguity or subjective content.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. It is a self-contained, scientifically sound, and well-posed problem in computational physics. A solution will be constructed.\n\n---\n\n### Part 1: Derivation of the Order Parameter Tensor $\\mathbf{Q}$\n\nThe objective is to construct a symmetric, traceless, second-rank tensor $\\mathbf{Q}$ that quantifies orientational order. It must be constructed from the physically meaningful second moment tensor $\\mathbf{M} = \\langle \\mathbf{u} \\mathbf{u}^{\\mathsf{T}} \\rangle$ and the identity tensor $\\mathbf{I}$, as these are the only second-rank tensors available that are invariant under global rotations of the sample coordinate system.\n\nTherefore, $\\mathbf{Q}$ must be a linear combination of $\\mathbf{M}$ and $\\mathbf{I}$:\n$$\n\\mathbf{Q} = a\\mathbf{M} + b\\mathbf{I}\n$$\nwhere $a$ and $b$ are scalar constants.\n\nThe tensor $\\mathbf{Q}$ must be traceless, i.e., $\\mathrm{Tr}(\\mathbf{Q}) = 0$. We compute the trace:\n$$\n\\mathrm{Tr}(\\mathbf{Q}) = \\mathrm{Tr}(a\\mathbf{M} + b\\mathbf{I}) = a\\mathrm{Tr}(\\mathbf{M}) + b\\mathrm{Tr}(\\mathbf{I})\n$$\nThe trace of the identity tensor in three dimensions is $\\mathrm{Tr}(\\mathbf{I}) = 3$. The trace of the second moment tensor is:\n$$\n\\mathrm{Tr}(\\mathbf{M}) = \\mathrm{Tr}(\\langle \\mathbf{u} \\mathbf{u}^{\\mathsf{T}} \\rangle) = \\langle \\mathrm{Tr}(\\mathbf{u} \\mathbf{u}^{\\mathsf{T}}) \\rangle\n$$\nThe trace of a dyadic product $\\mathbf{u} \\mathbf{u}^{\\mathsf{T}}$ is $\\mathrm{Tr}(\\mathbf{u} \\mathbf{u}^{\\mathsf{T}}) = \\mathbf{u}^{\\mathsf{T}}\\mathbf{u} = \\|\\mathbf{u}\\|^2$. Since $\\mathbf{u}$ is a unit vector, $\\|\\mathbf{u}\\|^2 = 1$. Thus, $\\mathrm{Tr}(\\mathbf{M}) = \\langle 1 \\rangle = 1$.\n\nThe traceless condition becomes:\n$$\na(1) + b(3) = 0 \\implies b = -\\frac{a}{3}\n$$\nSubstituting this into the expression for $\\mathbf{Q}$, we find the form of $\\mathbf{Q}$ up to a single scalar factor $a$:\n$$\n\\mathbf{Q} = a\\mathbf{M} - \\frac{a}{3}\\mathbf{I} = a \\left( \\mathbf{M} - \\frac{1}{3}\\mathbf{I} \\right)\n$$\nTo fix the constant $a$, we apply the normalization condition for a state of perfect uniaxial alignment. In this state, all orientation vectors are identical, $\\mathbf{u}_i = \\mathbf{n}$ for all $i$, where $\\mathbf{n}$ is a unit vector. The second moment tensor is:\n$$\n\\mathbf{M} = \\langle \\mathbf{n} \\mathbf{n}^{\\mathsf{T}} \\rangle = \\frac{1}{N} \\sum_{i=1}^{N} \\mathbf{n} \\mathbf{n}^{\\mathsf{T}} = \\mathbf{n} \\mathbf{n}^{\\mathsf{T}}\n$$\nsubstituting this into the expression for $\\mathbf{Q}$:\n$$\n\\mathbf{Q} = a \\left( \\mathbf{n} \\mathbf{n}^{\\mathsf{T}} - \\frac{1}{3}\\mathbf{I} \\right)\n$$\nWe find the eigenvalues of this tensor. The vector $\\mathbf{n}$ is an eigenvector:\n$$\n\\mathbf{Q}\\mathbf{n} = a \\left( \\mathbf{n} \\mathbf{n}^{\\mathsf{T}} \\mathbf{n} - \\frac{1}{3}\\mathbf{I}\\mathbf{n} \\right) = a \\left( \\mathbf{n}(\\mathbf{n}^{\\mathsf{T}}\\mathbf{n}) - \\frac{1}{3}\\mathbf{n} \\right) = a \\left( \\mathbf{n} - \\frac{1}{3}\\mathbf{n} \\right) = \\frac{2a}{3}\\mathbfn\n$$\nThe corresponding eigenvalue is $\\lambda_1 = \\frac{2a}{3}$. Any vector $\\mathbf{v}$ orthogonal to $\\mathbf{n}$ (i.e., $\\mathbf{n} \\cdot \\mathbf{v}=0$) is also an eigenvector:\n$$\n\\mathbf{Q}\\mathbf{v} = a \\left( \\mathbf{n} \\mathbf{n}^{\\mathsf{T}} \\mathbf{v} - \\frac{1}{3}\\mathbf{I}\\mathbf{v} \\right) = a \\left( \\mathbf{n}(\\mathbf{n}^{\\mathsf{T}}\\mathbf{v}) - \\frac{1}{3}\\mathbf{v} \\right) = a \\left( \\mathbf{0} - \\frac{1}{3}\\mathbf{v} \\right) = -\\frac{a}{3}\\mathbf{v}\n$$\nThe other two eigenvalues are degenerate: $\\lambda_{2,3} = -\\frac{a}{3}$. The largest eigenvalue is $\\lambda_1 = \\frac{2a}{3}$. The normalization requires this to be $1$:\n$$\n\\frac{2a}{3} = 1 \\implies a = \\frac{3}{2}\n$$\nThus, the unique tensor $\\mathbf{Q}$ satisfying the given principles is:\n$$\n\\mathbf{Q} = \\frac{3}{2} \\left( \\mathbf{M} - \\frac{1}{3}\\mathbf{I} \\right) = \\frac{3}{2}\\mathbf{M} - \\frac{1}{2}\\mathbf{I}\n$$\nFor a discrete set of $N_{eff}$ non-zero vectors $\\{\\mathbf{v}_i\\}$, we first normalize them to get $\\{\\mathbf{u}_i\\}$. The moment tensor is $\\mathbf{M} = \\frac{1}{N_{eff}} \\sum_{i=1}^{N_{eff}} \\mathbf{u}_i \\mathbf{u}_i^{\\mathsf{T}}$. The explicit expression for $\\mathbf{Q}$ is:\n$$\n\\mathbf{Q} = \\frac{3}{2} \\left( \\left( \\frac{1}{N_{eff}} \\sum_{i=1}^{N_{eff}} \\mathbf{u}_i \\mathbf{u}_i^{\\mathsf{T}} \\right) - \\frac{1}{3}\\mathbf{I} \\right)\n$$\nFinally, for a perfectly isotropic state, $\\langle u_\\alpha u_\\beta \\rangle = \\frac{1}{3}\\delta_{\\alpha\\beta}$, so $\\mathbf{M}_{\\text{iso}} = \\frac{1}{3}\\mathbf{I}$. This gives $\\mathbf{Q} = \\frac{3}{2}(\\frac{1}{3}\\mathbf{I} - \\frac{1}{3}\\mathbf{I}) = \\mathbf{0}$, as required.\n\n### Part 2: Proof of Properties\n\n1.  **Symmetry**: $\\mathbf{M}$ is a sum of symmetric matrices $\\mathbf{u}_i \\mathbf{u}_i^{\\mathsf{T}}$, so it is symmetric. $\\mathbf{I}$ is symmetric. Therefore, $\\mathbf{Q}$, being a linear combination of $\\mathbf{M}$ and $\\mathbf{I}$, is symmetric.\n2.  **Tracelessness**: As shown in the derivation, $\\mathrm{Tr}(\\mathbf{M})=1$ and $\\mathrm{Tr}(\\mathbf{I})=3$.\n    $$\n    \\mathrm{Tr}(\\mathbf{Q}) = \\mathrm{Tr}\\left(\\frac{3}{2}\\mathbf{M} - \\frac{1}{2}\\mathbf{I}\\right) = \\frac{3}{2}\\mathrm{Tr}(\\mathbf{M}) - \\frac{1}{2}\\mathrm{Tr}(\\mathbf{I}) = \\frac{3}{2}(1) - \\frac{1}{2}(3) = 0\n    $$\n    This property holds for any distribution.\n3.  **Eigenvalue Bounds**: Let the eigenvalues of $\\mathbf{Q}$ be $q_1, q_2, q_3$ with $q_1 \\ge q_2 \\ge q_3$. The largest eigenvalue is $S = q_1$. The eigenvalues of $\\mathbf{Q}$ are related to the eigenvalues $m_1 \\ge m_2 \\ge m_3$ of $\\mathbf{M}$ by $q_j = \\frac{3}{2}(m_j - \\frac{1}{3})$.\n    The matrix $\\mathbf{M} = \\langle \\mathbf{u}\\mathbf{u}^{\\mathsf{T}} \\rangle$ is an average of positive semi-definite matrices, so it is itself positive semi-definite. Its eigenvalues are therefore non-negative, $m_j \\ge 0$.\n    The trace condition $\\mathrm{Tr}(\\mathbf{M})=1$ implies $m_1+m_2+m_3=1$.\n    To find the bounds on $S = q_1 = \\frac{3}{2}(m_1 - \\frac{1}{3})$, we must find the bounds on $m_1$.\n    -   **Upper Bound**: $m_1 = \\max_{\\|\\mathbf{x}\\|=1} \\mathbf{x}^{\\mathsf{T}}\\mathbf{M}\\mathbf{x} = \\max_{\\|\\mathbf{x}\\|=1} \\langle (\\mathbf{x} \\cdot \\mathbf{u})^2 \\rangle$. Since $(\\mathbf{x} \\cdot \\mathbf{u})^2 \\le \\|\\mathbf{x}\\|^2 \\|\\mathbf{u}\\|^2 = 1$, we have $m_1 \\le \\langle 1 \\rangle = 1$. The maximum $m_1=1$ is achieved for perfect alignment, where $m_1=1, m_2=m_3=0$. This gives $S = \\frac{3}{2}(1 - \\frac{1}{3}) = 1$.\n    -   **Lower Bound**: Since $m_1$ is the largest of three non-negative numbers summing to $1$, we must have $m_1 \\ge \\frac{1}{3}$. This minimum is achieved for the isotropic case, $m_1=m_2=m_3=\\frac{1}{3}$. This gives $S = \\frac{3}{2}(\\frac{1}{3} - \\frac{1}{3}) = 0$.\n    Therefore, the largest eigenvalue $S$ is always in the interval $[0, 1]$. The problem statement claims $S \\in [-\\frac{1}{2}, 1]$, which is a mathematically correct, albeit looser, superset of the tight bound. The full spectrum of eigenvalues of $\\mathbf{Q}$ lies within $[-\\frac{1}{2}, 1]$, as the smallest eigenvalue $q_3 = \\frac{3}{2}(m_3 - \\frac{1}{3})$ is bounded by $0 \\le m_3 \\le 1/3$, which gives $q_3 \\in [-\\frac{1}{2}, 0]$.\n\n### Part 3: Algorithmic Design\n\nThe algorithm computes the scalar order parameter $S$ and the director $\\mathbf{n}$ from a list of vectors $\\{\\mathbf{v}_i\\}$.\n\n1.  **Input**: A list of $N$ vectors $\\{\\mathbf{v}_i\\} \\subset \\mathbb{R}^3$.\n2.  **Tolerances**: Let $\\epsilon = 10^{-8}$ be the tolerance for the norm of the mean orientation vector used in the sign convention. Let $\\delta = 10^{-8}$ be the tolerance for eigenvalues to be considered zero. Let $10^{-12}$ be the threshold for a vector norm to be considered non-zero. These values are appropriate for standard double-precision floating-point arithmetic.\n3.  **Normalization**: Iterate through the input vectors $\\{\\mathbf{v}_i\\}$. For each vector, compute its norm $\\|\\mathbf{v}_i\\|$. If the norm is greater than $10^{-12}$, normalize the vector to obtain $\\mathbf{u}_i = \\mathbf{v}_i/\\|\\mathbf{v}_i\\|$ and add it to a list of effective vectors. Let the count of such vectors be $N_{eff}$. If $N_{eff}=0$, the system is undefined; by convention, we return the isotropic result $S=0$ and $\\mathbf{n}=(0,0,1)$.\n4.  **Second Moment Tensor $\\mathbf{M}$**: Compute the $3 \\times 3$ matrix $\\mathbf{M}$ as the sample average of the dyadic products of the normalized vectors:\n    $$\n    \\mathbf{M} = \\frac{1}{N_{eff}} \\sum_{i=1}^{N_{eff}} \\mathbf{u}_i \\mathbf{u}_i^{\\mathsf{T}}\n    $$\n    This is efficiently computed as $(\\mathbf{U}^{\\mathsf{T}}\\mathbf{U})/N_{eff}$, where $\\mathbf{U}$ is the $N_{eff} \\times 3$ matrix whose rows are the vectors $\\mathbf{u}_i^{\\mathsf{T}}$.\n5.  **Order Parameter Tensor $\\mathbf{Q}$**: Construct $\\mathbf{Q}$ using the derived formula:\n    $$\n    \\mathbf{Q} = \\frac{3}{2}\\left(\\mathbf{M} - \\frac{1}{3}\\mathbf{I}\\right)\n    $$\n6.  **Eigendecomposition**: Since $\\mathbf{Q}$ is a real symmetric matrix, use a specialized numerical routine such as `numpy.linalg.eigh` to find its real eigenvalues and orthogonal eigenvectors. This routine typically returns eigenvalues sorted in ascending order and corresponding eigenvectors as columns of a matrix.\n7.  **Isotropic Case Handling**: Check if the maximum absolute value of all eigenvalues is less than the tolerance $\\delta$. If so, the system is considered isotropic. Return $S=0$ and the conventional director $\\mathbf{n}=(0,0,1)$.\n8.  **Extraction of $S$ and $\\mathbf{n}$**: The largest eigenvalue is $S$. The corresponding eigenvector is the director $\\mathbf{n}$.\n9.  **Sign Convention**: To resolve the ambiguity $\\mathbf{n} \\leftrightarrow -\\mathbf{n}$, a deterministic convention is applied.\n    a. Compute the mean orientation vector $\\bar{\\mathbf{u}} = \\frac{1}{N_{eff}} \\sum_{i=1}^{N_{eff}} \\mathbf{u}_i$.\n    b. If $\\|\\bar{\\mathbf{u}}\\| > \\epsilon$, the sample has a net polarity. Choose the sign of $\\mathbf{n}$ such that it has a non-negative projection onto the mean orientation: if $\\mathbf{n} \\cdot \\bar{\\mathbf{u}} < 0$, replace $\\mathbf{n}$ with $-\\mathbf{n}$.\n    c. If $\\|\\bar{\\mathbf{u}}\\| \\le \\epsilon$, the sample is apolar or nearly isotropic. Choose the sign of $\\mathbf{n}$ such that its component with the largest absolute value is non-negative. Find the index $k$ such that $|n_k|$ is maximal; if $n_k < 0$, replace $\\mathbf{n}$ with $-\\mathbf{n}$.\n10. **Output**: Return the scalar $S$ and the components of the director $\\mathbf{n}$.\n\nThis procedure is robust, numerically stable, and correctly implements the physical principles and problem requirements.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef calculate_order_parameter(vectors: np.ndarray, epsilon: float = 1e-8, delta: float = 1e-8) -> list[float]:\n    \"\"\"\n    Calculates the nematic order parameter S and director n from a set of 3D vectors.\n\n    Args:\n        vectors: An (N, 3) numpy array of orientation vectors.\n        epsilon: Tolerance for determining if the mean orientation vector is non-zero.\n        delta: Tolerance for determining if eigenvalues are zero (isotropic case).\n\n    Returns:\n        A list [S, nx, ny, nz] with values rounded to 6 decimal places.\n    \"\"\"\n    # Step 1: Normalize vectors, ignoring zero-norm vectors.\n    norms = np.linalg.norm(vectors, axis=1)\n    non_zero_indices = np.where(norms > 1e-12)[0]\n    \n    if len(non_zero_indices) == 0:\n        # If no valid vectors, return the isotropic state by convention.\n        return [0.0, 0.0, 0.0, 1.0]\n\n    unit_vectors = vectors[non_zero_indices] / norms[non_zero_indices, np.newaxis]\n    n_eff = len(unit_vectors)\n\n    # Step 2: Compute the second moment tensor M.\n    # M = (1/n_eff) * sum(u_i @ u_i.T) which is equivalent to U.T @ U / n_eff\n    M = np.dot(unit_vectors.T, unit_vectors) / n_eff\n    \n    # Step 3: Construct the order parameter tensor Q.\n    I = np.identity(3)\n    Q = 1.5 * (M - (1./3.) * I)\n\n    # Step 4: Perform eigendecomposition of the symmetric matrix Q.\n    # np.linalg.eigh returns eigenvalues in ascending order.\n    eigvals, eigvecs = np.linalg.eigh(Q)\n\n    # Step 5: Handle the nearly isotropic case.\n    if np.max(np.abs(eigvals)) < delta:\n        return [0.0, 0.0, 0.0, 1.0]\n\n    # Step 6: Extract the largest eigenvalue S and corresponding eigenvector n.\n    S = eigvals[2]\n    n = eigvecs[:, 2]\n\n    # Step 7: Apply the deterministic sign convention for the director n.\n    mean_u = np.mean(unit_vectors, axis=0)\n    norm_mean_u = np.linalg.norm(mean_u)\n\n    if norm_mean_u > epsilon:\n        # Orient n to have a positive projection on the mean orientation vector.\n        if np.dot(n, mean_u) < 0:\n            n = -n\n    else:\n        # For apolar/isotropic data, orient n so its largest component is non-negative.\n        max_abs_idx = np.argmax(np.abs(n))\n        if n[max_abs_idx] < 0:\n            n = -n\n            \n    # Step 8: Return the final results, rounded to 6 decimal places.\n    return [round(S, 6), round(n[0], 6), round(n[1], 6), round(n[2], 6)]\n\ndef solve():\n    \"\"\"\n    Defines the test suite, runs the calculation for each case, and prints the final output.\n    \"\"\"\n    test_cases_raw = [\n        # Test case A: Uniaxial alignment with small perturbations about z-axis\n        [(0,0,1), (0,0,1), (0,0,1), (0.1,0, np.sqrt(1-0.1**2)), (-0.1,0.05,np.sqrt(1-0.1**2-0.05**2)), (0.05,-0.02,np.sqrt(1-0.05**2-0.02**2))],\n        # Test case B: Exactly isotropic on Cartesian axes\n        [(1,0,0), (-1,0,0), (0,1,0), (0,-1,0), (0,0,1), (0,0,-1)],\n        # Test case C: Planar bias in xy-plane with dominance along x-axis\n        [(1,0,0), (-1,0,0), (1,0.1,0), (0,1,0), (0,-1,0), (0.7,0.7,0)],\n        # Test case D: Antiparallel pairs about a common director near (1,1,1)\n        [(1,1,1), (-1,-1,-1), (0.9,1.1,1.0), (-0.9,-1.1,-1.0)],\n        # Test case E: Single-vector boundary case\n        [(0.3, -0.4, 0.8660254)]\n    ]\n\n    test_cases = [np.array(case, dtype=float) for case in test_cases_raw]\n    \n    results = []\n    for case in test_cases:\n        result = calculate_order_parameter(case)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    # The string representation of a Python list is already in the correct format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2932989"}, {"introduction": "While experimental or simulation data provides discrete orientations, theoretical models often describe orientational order using a continuous distribution function, $f(\\theta)$. This exercise bridges theory and computation by asking you to design a high-precision numerical scheme to calculate the scalar order parameter, $S$, from such a distribution [@problem_id:2933036]. You will implement the Gauss-Legendre quadrature method, a powerful technique for numerical integration, and develop a method for estimating the computational error.", "problem": "Design and implement a complete numerical scheme to compute the orientational order parameter in a uniaxial system using Gauss–Legendre quadrature. Let the orientational order parameter be defined as the integral\n$$\nS = \\int_{0}^{\\pi} P_{2}(\\cos \\theta)\\, f(\\theta)\\, \\sin \\theta \\, d\\theta,\n$$\nwhere $P_{2}$ is the Legendre polynomial of degree $2$, and $f(\\theta)$ is a nonnegative orientational distribution over polar angle. All angles are in radians. The scheme must satisfy the following requirements.\n\n1. Mathematical foundation to be used:\n   - The change of variables $x = \\cos \\theta$ with $dx = -\\sin \\theta \\, d\\theta$ so that\n     $$\n     S = \\int_{-1}^{1} P_{2}(x)\\, f(\\arccos x)\\, dx.\n     $$\n   - The Legendre polynomial $P_{2}(x)$ is defined by $P_{2}(x) = \\tfrac{1}{2}(3x^{2}-1)$.\n   - The $N$-point Gauss–Legendre quadrature on $[-1,1]$ integrates exactly any polynomial of degree at most $2N-1$.\n\n2. Numerical design goals:\n   - Construct an $N$-node Gauss–Legendre quadrature on $[-1,1]$ for the integrand $h(x) = P_{2}(x)\\, f(\\arccos x)$.\n   - Provide, for each computation with $N$ nodes, an a posteriori error bound that scales with $N$. To this end, use a node-doubling strategy:\n     - Compute $S_{N}$, $S_{2N}$, and $S_{4N}$.\n     - Define $\\Delta_{1} = |S_{2N} - S_{N}|$ and $\\Delta_{2} = |S_{4N} - S_{2N}|$.\n     - If $\\Delta_{1} = \\Delta_{2} = 0$, set the bound $B_{N} = 0$.\n     - Otherwise define the ratio $R = \\Delta_{2}/\\Delta_{1}$ if $\\Delta_{1} > 0$. If $0 \\le R < 1$, set\n       $$\n       B_{N} = \\frac{\\Delta_{1}}{1 - R}.\n       $$\n       If $R \\ge 1$ or $\\Delta_{1} = 0$, use the conservative choice $B_{N} = \\max\\{\\Delta_{1},\\Delta_{2}\\}$.\n     This yields a computable bound that, for analytic $h(x)$, scales geometrically in $N$.\n\n3. Test suite and inputs:\n   - Use the following test cases. In all cases, evaluate $S$ using the described scheme, and return the pair $[S_{N}, B_{N}]$ as floating-point numbers.\n     - Case A (boundary under-resolution): $f(\\theta) = 1$, $N = 1$.\n     - Case B (exactness at minimal degree): $f(\\theta) = 1$, $N = 2$.\n     - Case C (polynomially perturbed isotropy): $f(\\theta) = 1 + \\alpha P_{2}(\\cos \\theta)$ with $\\alpha = 0.6$, $N = 3$.\n     - Case D (Maier–Saupe-like even distribution, moderately aligned): $f(\\theta) = C(\\kappa)\\, \\exp(\\kappa \\cos^{2}\\theta)$ with $\\kappa = 5.0$, where $C(\\kappa)$ enforces the normalization\n       $$\n       \\int_{0}^{\\pi} f(\\theta)\\, \\sin \\theta \\, d\\theta = 2.\n       $$\n       Show that this normalization implies\n       $$\n       C(\\kappa) = \\frac{2}{\\int_{-1}^{1} e^{\\kappa x^{2}}\\, dx} = \\frac{2\\sqrt{\\kappa}}{\\sqrt{\\pi}\\, \\operatorname{erfi}(\\sqrt{\\kappa})},\n       $$\n       where $\\operatorname{erfi}$ is the imaginary error function. Use $N = 8$.\n     - Case E (Maier–Saupe-like even distribution, strongly aligned): same as Case D but with $\\kappa = 20.0$ and $N = 12$.\n\n4. Output requirements:\n   - Angle unit is radians.\n   - All outputs are dimensionless floating-point numbers.\n   - Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case contributes a two-element list $[S_{N}, B_{N}]$. For example, the final output must look like\n     $$\n     [[s_{A},b_{A}],[s_{B},b_{B}],[s_{C},b_{C}],[s_{D},b_{D}],[s_{E},b_{E}]].\n     $$\n     Do not include any additional text.\n\nThe goal is to start from the fundamental change of variables and the defining properties of Gauss–Legendre quadrature, derive a principled algorithm, and implement it so that it returns both the numerical estimate and a sound, computable bound that decays with the number of nodes. The bound should be constructed solely from the values $S_{N}$, $S_{2N}$, and $S_{4N}$ as specified. Focus on scientific realism: ensure $f(\\theta)$ is nonnegative, and for the exponential cases use the exact normalization constant $C(\\kappa)$ given above. All computations are dimensionless. The angle unit is radians. The required final output is a single line containing a Python list of the five $[S_{N}, B_{N}]$ pairs in the order A through E.", "solution": "We begin from the orientational average definition used in polymers and soft condensed matter: for a uniaxial distribution depending only on the polar angle $\\theta$, the orientational order parameter is the average of the degree-$2$ Legendre polynomial $P_{2}(\\cos \\theta)$ against a distribution $f(\\theta)$ with respect to the surface-area element on the unit sphere. For any azimuthally symmetric $f(\\theta)$, this reduces to the one-dimensional integral\n$$\nS = \\int_{0}^{\\pi} P_{2}(\\cos \\theta)\\, f(\\theta)\\, \\sin \\theta \\, d\\theta,\n$$\nwhere all quantities are dimensionless and the angle is measured in radians.\n\nStep 1: Transform to a standard interval. Introduce the change of variables $x = \\cos \\theta$, which yields $dx = -\\sin \\theta\\, d\\theta$ and maps $\\theta \\in [0,\\pi]$ to $x \\in [1,-1]$. Therefore,\n$$\nS = \\int_{0}^{\\pi} P_{2}(\\cos \\theta)\\, f(\\theta)\\, \\sin \\theta \\, d\\theta\n= \\int_{1}^{-1} P_{2}(x) \\, f(\\arccos x)\\, (-dx)\n= \\int_{-1}^{1} P_{2}(x)\\, f(\\arccos x)\\, dx.\n$$\nDefine the integrand on $[-1,1]$ as\n$$\nh(x) = P_{2}(x)\\, g(x), \\quad \\text{with} \\quad g(x) = f(\\arccos x).\n$$\nThe Legendre polynomial is $P_{2}(x) = \\tfrac{1}{2}(3x^{2} - 1)$. This transforms the problem into integrating a smooth function $h(x)$ on the standard interval $[-1,1]$.\n\nStep 2: Gauss–Legendre quadrature. The $N$-point Gauss–Legendre quadrature on $[-1,1]$ provides nodes $\\{x_{i}\\}_{i=1}^{N}$ and weights $\\{w_{i}\\}_{i=1}^{N}$ that exactly integrate any polynomial of degree up to $2N-1$. The quadrature approximation is\n$$\nS_{N} = \\sum_{i=1}^{N} w_{i}\\, h(x_{i}) = \\sum_{i=1}^{N} w_{i}\\, P_{2}(x_{i})\\, g(x_{i}).\n$$\nIn particular, if $h(x)$ is a polynomial of degree at most $2N-1$, then $S_{N} = S$.\n\nStep 3: Error behavior and computable bound. For sufficiently smooth (for instance, analytic) $h(x)$ on $[-1,1]$, Gauss–Legendre quadrature errors decay rapidly with $N$, and for analytic functions they decay geometrically with $N$ due to analyticity in a Bernstein ellipse. While exact constants depend on the analytic continuation, a practical, computable a posteriori bound can be derived by a node-doubling strategy exploiting the expected geometric decay. Compute three approximations:\n$$\nS_{N}, \\quad S_{2N}, \\quad S_{4N}.\n$$\nDefine differences\n$$\n\\Delta_{1} = |S_{2N} - S_{N}|, \\qquad \\Delta_{2} = |S_{4N} - S_{2N}|.\n$$\nIf the error behaves approximately as geometric in $N$ (which is guaranteed for analytic $h(x)$), the ratio $R = \\Delta_{2}/\\Delta_{1}$ estimates the contraction factor in the tail. A conservative bound on the error of the $N$-node approximation is\n$$\nB_{N} =\n\\begin{cases}\n0, & \\text{if } \\Delta_{1} = \\Delta_{2} = 0,\\\\\n\\dfrac{\\Delta_{1}}{1 - R}, & \\text{if } 0 \\le R < 1,\\\\\n\\max\\{\\Delta_{1},\\Delta_{2}\\}, & \\text{otherwise.}\n\\end{cases}\n$$\nThis bound is computable from $S_{N}$, $S_{2N}$, $S_{4N}$ and scales with $N$; for analytic $h(x)$, $R$ decreases rapidly as $N$ grows, making $B_{N}$ small. In degenerate exact cases ($\\Delta_{1}=\\Delta_{2}=0$), the bound is $0$.\n\nStep 4: Test functions and normalization. We consider five cases:\n- Case A and Case B: $f(\\theta) = 1$. Then $g(x) = 1$ and $h(x) = P_{2}(x)$. Because $\\int_{-1}^{1} P_{2}(x)\\, dx = 0$, the exact value is $S = 0$. Gauss–Legendre quadrature with $N=2$ integrates degree-$2$ polynomials exactly, so $S_{2} = 0$. With $N=1$, the quadrature under-resolves, producing a nonzero $S_{1}$; the a posteriori bound constructed from $S_{1}$, $S_{2}$, $S_{4}$ recovers a finite error bound.\n- Case C: $f(\\theta) = 1 + \\alpha P_{2}(\\cos \\theta)$ with $\\alpha = 0.6$. Then $g(x) = 1 + \\alpha P_{2}(x)$ and $h(x) = P_{2}(x) + \\alpha P_{2}(x)^{2}$ is a polynomial of degree $4$. For $N=3$, polynomials up to degree $2N-1 = 5$ are integrated exactly, so $S_{3} = S$. Orthogonality of Legendre polynomials implies\n$$\nS = \\int_{-1}^{1} P_{2}(x)\\, \\bigl[1 + \\alpha P_{2}(x)\\bigr]\\, dx\n= \\alpha \\int_{-1}^{1} P_{2}(x)^{2}\\, dx\n= \\alpha \\,\\frac{2}{2\\cdot 2 + 1} = \\alpha \\cdot \\frac{2}{5} = 0.24.\n$$\n- Case D and Case E: $f(\\theta) = C(\\kappa)\\, e^{\\kappa \\cos^{2}\\theta}$ with $\\kappa \\in \\{5.0, 20.0\\}$. This is an even, azimuthally symmetric distribution. We impose the physical normalization\n$$\n\\int_{0}^{\\pi} f(\\theta)\\, \\sin \\theta \\, d\\theta = 2\n\\quad \\Longleftrightarrow \\quad\n\\int_{-1}^{1} g(x)\\, dx = 2,\n$$\nwhich yields\n$$\nC(\\kappa) = \\frac{2}{\\int_{-1}^{1} e^{\\kappa x^{2}}\\, dx}\n= \\frac{2\\sqrt{\\kappa}}{\\sqrt{\\pi}\\, \\operatorname{erfi}(\\sqrt{\\kappa})}.\n$$\nThus $g(x) = C(\\kappa)\\, e^{\\kappa x^{2}}$ and $h(x) = P_{2}(x)\\, C(\\kappa)\\, e^{\\kappa x^{2}}$. The integrand is entire in $x$, so Gauss–Legendre errors decay rapidly with $N$. The node-doubling estimator provides a computable bound that scales in $N$.\n\nStep 5: Algorithmic realization.\n- Use a Gauss–Legendre routine to obtain nodes and weights on $[-1,1]$ for $N$, $2N$, and $4N$.\n- For each test case, define $g(x)$ as specified above (for exponential cases, compute $C(\\kappa)$ using the imaginary error function to achieve exact normalization).\n- Compute $S_{N}$, $S_{2N}$, and $S_{4N}$ by summing $w_{i}\\, P_{2}(x_{i})\\, g(x_{i})$.\n- Form $\\Delta_{1}$, $\\Delta_{2}$, and $B_{N}$ as defined.\n- Return $[S_{N}, B_{N}]$ for each case in the order A through E.\n\nStep 6: Output format. Produce a single line containing a Python list with five entries, each a two-element list $[S_{N}, B_{N}]$, i.e.,\n$$\n[[S_{A},B_{A}],[S_{B},B_{B}],[S_{C},B_{C}],[S_{D},B_{D}],[S_{E},B_{E}]].\n$$\n\nThis procedure adheres to the fundamental change of variables, uses the defining optimality of Gauss–Legendre quadrature for polynomials, and constructs an error bound based on geometric decay that is computable solely from nested quadrature sequences. For the polynomial-based Cases B and C, exactness is achieved at the stated $N$, producing zero error and zero bound. For the exponential cases, the analyticity of $h(x)$ ensures rapid convergence and correspondingly small bounds as $N$ increases.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import erfi\n\ndef legendre_P2(x):\n    # P2(x) = (3x^2 - 1)/2\n    return 0.5 * (3.0 * x * x - 1.0)\n\ndef gauss_legendre_integral_of_h(g_func, N):\n    # Compute integral of h(x) = P2(x) * g(x) over [-1,1] using N-point Gauss–Legendre\n    x, w = np.polynomial.legendre.leggauss(N)\n    h_vals = legendre_P2(x) * g_func(x)\n    return np.dot(w, h_vals)\n\ndef normalization_constant_for_exp_kappa(kappa):\n    # Exact normalization constant C(kappa) = 2 / ∫_{-1}^{1} e^{kappa x^2} dx\n    # Integral equals sqrt(pi)/sqrt(kappa) * erfi(sqrt(kappa))\n    sqrt_k = np.sqrt(kappa)\n    integral = np.sqrt(np.pi) / sqrt_k * erfi(sqrt_k)\n    return 2.0 / integral\n\ndef error_bound_doubling(SN, S2N, S4N):\n    d1 = abs(S2N - SN)\n    d2 = abs(S4N - S2N)\n    # Exact or numerically exact case\n    if d1 == 0.0 and d2 == 0.0:\n        return 0.0\n    if d1 > 0.0:\n        R = d2 / d1\n        if R < 1.0:\n            # Geometric a posteriori bound for |S - S_N|\n            return d1 / (1.0 - R)\n    # Fallback conservative choice\n    return max(d1, d2)\n\ndef solve():\n    # Define g(x) for each test case\n    # Case A: f(theta) = 1, N = 1\n    def g_case_A(x):\n        return np.ones_like(x)\n    N_A = 1\n\n    # Case B: f(theta) = 1, N = 2\n    def g_case_B(x):\n        return np.ones_like(x)\n    N_B = 2\n\n    # Case C: f(theta) = 1 + alpha * P2(cos theta), alpha = 0.6, N = 3\n    alpha_C = 0.6\n    def g_case_C(x):\n        return 1.0 + alpha_C * legendre_P2(x)\n    N_C = 3\n\n    # Case D: f(theta) = C(kappa) * exp(kappa * cos^2 theta), kappa = 5.0, N = 8\n    kappa_D = 5.0\n    C_D = normalization_constant_for_exp_kappa(kappa_D)\n    def g_case_D(x):\n        return C_D * np.exp(kappa_D * x * x)\n    N_D = 8\n\n    # Case E: f(theta) = C(kappa) * exp(kappa * cos^2 theta), kappa = 20.0, N = 12\n    kappa_E = 20.0\n    C_E = normalization_constant_for_exp_kappa(kappa_E)\n    def g_case_E(x):\n        return C_E * np.exp(kappa_E * x * x)\n    N_E = 12\n\n    test_cases = [\n        (g_case_A, N_A),\n        (g_case_B, N_B),\n        (g_case_C, N_C),\n        (g_case_D, N_D),\n        (g_case_E, N_E),\n    ]\n\n    results = []\n    for g_func, N in test_cases:\n        SN = gauss_legendre_integral_of_h(g_func, N)\n        S2N = gauss_legendre_integral_of_h(g_func, 2 * N)\n        S4N = gauss_legendre_integral_of_h(g_func, 4 * N)\n        bound = error_bound_doubling(SN, S2N, S4N)\n        results.append([float(SN), float(bound)])\n\n    # Final print statement in the exact required format.\n    # Print as a single line Python-list-like representation.\n    def fmt_pair(p):\n        # Use repr-like formatting with sufficient precision\n        return f\"[{p[0]:.16g},{p[1]:.16g}]\"\n    print(\"[\" + \",\".join(fmt_pair(p) for p in results) + \"]\")\n\nsolve()\n```", "id": "2933036"}, {"introduction": "A valid orientational order tensor, $\\mathbf{Q}$, must satisfy specific mathematical constraints, most notably that its eigenvalues lie within the physical range $[-\\frac{1}{2}, 1]$. However, numerical computations or theoretical approximations can sometimes yield a symmetric, traceless tensor that violates this condition. This practice tackles the advanced problem of \"physicality enforcement\" by guiding you to derive and implement an algorithm that projects any arbitrary symmetric traceless matrix onto the set of physically admissible order parameter tensors [@problem_id:2933004]. This projection, based on convex optimization, is a crucial tool for regularizing data and stabilizing numerical models in soft matter physics.", "problem": "Consider the orientational order parameter tensor in a three-dimensional nematic polymer or liquid crystal. By definition, the orientational order parameter tensor is constructed from a probability distribution of unit vectors and is symmetric and traceless. Starting from the fundamental statistical mechanical definition that the orientational order parameter tensor is an average of dyadic products of unit vectors minus an isotropic part, and using only basic facts from linear algebra and convex optimization, your task is to derive and implement the projection (with respect to the Frobenius norm) of an arbitrary symmetric traceless matrix onto the physically admissible set of orientational order parameter tensors.\n\nRequirements and setup:\n- Let $\\mathbb{S}_{0}^{3}$ denote the set of real $3 \\times 3$ symmetric traceless matrices. The Frobenius norm of a matrix $A$ is $\\lVert A \\rVert_{\\mathrm{F}} = \\sqrt{\\mathrm{Tr}(A^{\\mathsf{T}}A)}$.\n- The physically admissible set of orientational order parameter tensors $Q$ in dimension $3$ is the subset of $\\mathbb{S}_{0}^{3}$ whose eigenvalues lie in the interval $[-1/2,1]$.\n- Given any $A \\in \\mathbb{S}_{0}^{3}$, define the projection problem to find $Q^{\\star}$ that minimizes the Frobenius distance to $A$ over the physically admissible set.\n\nTasks:\n1. From the statistical definition of the orientational order parameter tensor as an average of dyadics of unit vectors minus an isotropic part, use the facts that $(\\mathbf{u}\\cdot\\mathbf{n})^{2} \\in [0,1]$ for any unit vectors $\\mathbf{u}$ and $\\mathbf{n}$ and that the Frobenius norm is orthogonally invariant, to establish eigenvalue bounds of any physical $Q$ and to reduce the projection to a problem in eigenvalue space.\n2. Using only general principles of linear algebra and convex optimization, prove that the minimizer $Q^{\\star}$ has the same eigenvectors as $A$, and that its eigenvalues are the closest (in the Euclidean sense) to those of $A$ among all vectors in $[-1/2,1]^{3}$ with zero sum. Your proof must not assume the target formula for the optimizer and must not provide any shortcut expressions. It must reason from first principles (spectral theorem, invariance of the Frobenius norm, and necessary and sufficient optimality conditions).\n3. Design an algorithm to compute the projection $Q^{\\star}$ for any given $A \\in \\mathbb{S}_{0}^{3}$. The algorithm should be stated precisely enough to be implementable and must be correct for all inputs in $\\mathbb{S}_{0}^{3}$. It should run in finite time and terminate with a unique answer.\n\nProgramming task:\n- Implement a program that, for each of the following test matrices, computes the eigenvalues (sorted in nonincreasing order) of the Frobenius-norm projection onto the physically admissible set described above. Your program must output the results for all test cases in a single line as a list of lists. Each eigenvalue must be rounded to exactly $6$ decimal places.\n\nTest suite (each matrix is symmetric and traceless):\n- Case $1$: $A_{1} = \\mathrm{diag}(0.3,-0.2,-0.1)$.\n- Case $2$: $A_{2} = \\mathrm{diag}(1.4,-0.5,-0.9)$.\n- Case $3$: $A_{3} = \\mathrm{diag}(0.8,-0.7,-0.1)$.\n- Case $4$: $A_{4} = \\begin{bmatrix} 0.6 & 0.2 & 0.0 \\\\ 0.2 & -0.1 & 0.1 \\\\ 0.0 & 0.1 & -0.5 \\end{bmatrix}$.\n- Case $5$: $A_{5} = \\mathrm{diag}(0.7,0.7,-1.4)$.\n- Case $6$: $A_{6} = \\begin{bmatrix} 0.9 & -0.4 & 0.3 \\\\ -0.4 & -0.2 & 0.5 \\\\ 0.3 & 0.5 & -0.7 \\end{bmatrix}$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each element corresponds to one test case and is itself a list of the three eigenvalues (sorted in nonincreasing order) rounded to exactly $6$ decimal places. For example, the output format is like $[[x_{11},x_{12},x_{13}],[x_{21},x_{22},x_{23}],\\ldots]$, where each $x_{ij}$ is a float with exactly $6$ decimal places and no additional text is printed.", "solution": "The problem as stated is valid. It is a well-posed problem in convex optimization, grounded in the established physical principles of soft matter physics. The definitions are clear, the data are consistent, and the objective is mathematically unambiguous. It asks for the derivation and implementation of a projection onto a convex set, which is a standard and meaningful task. We shall proceed with the solution.\n\nThe analysis is divided into three parts as requested: derivation of the eigenvalue problem, proof of the structure of the solution, and design of a computational algorithm.\n\n**Part 1: Reduction to an Eigenvalue Problem**\n\nThe orientational order parameter tensor $Q$ is defined through statistical averaging over a distribution of molecular orientations, represented by unit vectors $\\mathbf{u}$. A common definition is $Q_{unscaled} = \\langle \\mathbf{u} \\otimes \\mathbf{u} - \\frac{1}{3}I \\rangle$, where $I$ is the $3 \\times 3$ identity matrix. For any unit vector $\\mathbf{n}$, the Rayleigh quotient gives $\\mathbf{n}^{\\mathsf{T}} Q_{unscaled} \\mathbf{n} = \\langle (\\mathbf{u} \\cdot \\mathbf{n})^2 - \\frac{1}{3} \\rangle$. Since $(\\mathbf{u} \\cdot \\mathbf{n})^2 \\in [0, 1]$, its average must also lie in $[0, 1]$, which implies $\\mathbf{n}^{\\mathsf{T}} Q_{unscaled} \\mathbf{n} \\in [-\\frac{1}{3}, \\frac{2}{3}]$. By the Courant-Fischer theorem, the eigenvalues of $Q_{unscaled}$ are contained within this interval.\n\nThe problem states the physically admissible set of $Q$ tensors has eigenvalues in $[-1/2, 1]$. This corresponds to a different but also standard normalization, $Q = \\frac{3}{2}Q_{unscaled}$. The eigenvalues of this $Q$ are thus bounded in the interval $\\frac{3}{2}[-\\frac{1}{3}, \\frac{2}{3}] = [-1/2, 1]$. The problem statement provides this interval, which we shall use. The set of admissible tensors is therefore:\n$$ \\mathcal{C} = \\{ Q \\in \\mathbb{S}_{0}^{3} \\mid \\sigma(Q) \\subset [-1/2, 1] \\} $$\nwhere $\\mathbb{S}_{0}^{3}$ is the space of $3 \\times 3$ real, symmetric, traceless matrices and $\\sigma(Q)$ is the set of eigenvalues of $Q$. This set $\\mathcal{C}$ is closed and convex.\n\nThe task is to find the projection of a given matrix $A \\in \\mathbb{S}_{0}^{3}$ onto $\\mathcal{C}$. This is the minimization problem:\n$$ \\min_{Q \\in \\mathcal{C}} \\lVert A - Q \\rVert_{\\mathrm{F}}^2 $$\nThe objective function is the squared Frobenius norm, $\\lVert M \\rVert_{\\mathrm{F}}^2 = \\mathrm{Tr}(M^{\\mathsf{T}}M)$. Since $A$ and $Q$ are symmetric, this is $\\mathrm{Tr}((A-Q)^2)$.\n\nLet the spectral decomposition of $A$ be $A = V \\Lambda_A V^{\\mathsf{T}}$, where $V$ is an orthogonal matrix whose columns are the eigenvectors of $A$, and $\\Lambda_A = \\mathrm{diag}(\\lambda_{A1}, \\lambda_{A2}, \\lambda_{A3})$ is the diagonal matrix of corresponding eigenvalues. The Frobenius norm is invariant under orthogonal transformations, meaning $\\lVert U M U^{\\mathsf{T}} \\rVert_{\\mathrm{F}} = \\lVert M \\rVert_{\\mathrm{F}}$ for any orthogonal $U$.\nWe apply this property to the objective function:\n$$ \\lVert A - Q \\rVert_{\\mathrm{F}}^2 = \\lVert V \\Lambda_A V^{\\mathsf{T}} - Q \\rVert_{\\mathrm{F}}^2 = \\lVert V^{\\mathsf{T}}(V \\Lambda_A V^{\\mathsf{T}} - Q)V \\rVert_{\\mathrm{F}}^2 = \\lVert \\Lambda_A - V^{\\mathsf{T}}QV \\rVert_{\\mathrm{F}}^2 $$\nLet $\\tilde{Q} = V^{\\mathsf{T}}QV$. The transformation from $Q$ to $\\tilde{Q}$ is a similarity transformation, so they share the same eigenvalues and trace. Thus, $Q \\in \\mathcal{C}$ if and only if $\\tilde{Q} \\in \\mathcal{C}$. The problem becomes finding $\\tilde{Q}^{\\star}$ that minimizes $\\lVert \\Lambda_A - \\tilde{Q} \\rVert_{\\mathrm{F}}^2$ over all $\\tilde{Q} \\in \\mathcal{C}$.\n\nLet's expand the norm:\n$$ \\lVert \\Lambda_A - \\tilde{Q} \\rVert_{\\mathrm{F}}^2 = \\sum_{i,j=1}^3 (\\Lambda_{A,ij} - \\tilde{Q}_{ij})^2 = \\sum_{i=1}^3 (\\lambda_{Ai} - \\tilde{Q}_{ii})^2 + \\sum_{i \\neq j} \\tilde{Q}_{ij}^2 $$\nThis expression is minimized when the off-diagonal elements $\\tilde{Q}_{ij}$ are zero for $i \\neq j$. Setting the off-diagonal elements to zero does not affect the trace or the diagonal entries, and thus does not interfere with the constraints on the eigenvalues of $\\tilde{Q}$ (which are now simply its diagonal entries). Therefore, the optimal matrix $\\tilde{Q}^{\\star}$ must be a diagonal matrix. Let's denote its diagonal entries, which are its eigenvalues, by $\\lambda = (\\lambda_1, \\lambda_2, \\lambda_3)$.\n\nThis reduces the original matrix optimization problem to a much simpler vector optimization problem in the space of eigenvalues:\n$$ \\min_{\\lambda \\in \\mathbb{R}^3} \\sum_{i=1}^3 (\\lambda_{Ai} - \\lambda_i)^2 $$\nsubject to the constraints that define the admissible set of eigenvalues:\n$$ \\sum_{i=1}^3 \\lambda_i = 0 $$\n$$ -1/2 \\le \\lambda_i \\le 1 \\quad \\text{for } i=1, 2, 3 $$\n\n**Part 2: Proof of the Solution Structure**\n\nThe argument in Part 1 establishes that the minimizer $Q^{\\star}$ has the same eigenvectors as $A$. We have $Q^{\\star} = V \\tilde{Q}^{\\star} V^{\\mathsf{T}}$, and since $\\tilde{Q}^{\\star}$ is diagonal in the basis of $A$'s eigenvectors, $Q^{\\star}$ is co-diagonal with $A$. The eigenvalues of $Q^{\\star}$, which we denote $\\lambda^{\\star}$, are the solution to the vector minimization problem derived above. This problem is equivalent to finding the Euclidean projection of the vector of $A$'s eigenvalues, $\\lambda_A = (\\lambda_{A1}, \\lambda_{A2}, \\lambda_{A3})$, onto the convex set defined by the linear equality and bound constraints.\n\nTo find the structure of the solution $\\lambda^{\\star}$, we use the Karush-Kuhn-Tucker (KKT) conditions. Let the objective be $f(\\lambda) = \\frac{1}{2} \\sum_{i=1}^3 (\\lambda_i - \\lambda_{Ai})^2$. The Lagrangian is:\n$$ L(\\lambda, \\gamma, \\mu, \\nu) = \\frac{1}{2} \\sum_{i=1}^3 (\\lambda_i - \\lambda_{Ai})^2 - \\gamma \\left(\\sum_{i=1}^3 \\lambda_i\\right) - \\sum_{i=1}^3 \\mu_i (1 - \\lambda_i) - \\sum_{i=1}^3 \\nu_i (\\lambda_i + 1/2) $$\nwith Lagrange multipliers $\\gamma \\in \\mathbb{R}$ and $\\mu_i, \\nu_i \\ge 0$. The stationarity condition $\\nabla_{\\lambda_i} L = 0$ gives:\n$$ (\\lambda_i - \\lambda_{Ai}) - \\gamma - \\mu_i + \\nu_i = 0 \\implies \\lambda_i = \\lambda_{Ai} + \\gamma + \\mu_i - \\nu_i $$\nThe complementary slackness conditions are $\\mu_i(1 - \\lambda_i) = 0$ and $\\nu_i(\\lambda_i + 1/2) = 0$.\n\nWe analyze three cases for each component $\\lambda_i$:\n1.  If the bounds are inactive ($-1/2 < \\lambda_i < 1$), then $\\mu_i=0$ and $\\nu_i=0$. This implies $\\lambda_i = \\lambda_{Ai} + \\gamma$.\n2.  If the upper bound is active ($\\lambda_i = 1$), then $\\nu_i=0$ and $\\mu_i \\ge 0$. This implies $1 = \\lambda_{Ai} + \\gamma + \\mu_i$, which rearranges to $\\lambda_{Ai} + \\gamma = 1 - \\mu_i \\le 1$.\n3.  If the lower bound is active ($\\lambda_i = -1/2$), then $\\mu_i=0$ and $\\nu_i \\ge 0$. This implies $-1/2 = \\lambda_{Ai} + \\gamma - \\nu_i$, which rearranges to $\\lambda_{Ai} + \\gamma = -1/2 + \\nu_i \\ge -1/2$.\n\nCombining these cases, the solution for each component $\\lambda_i$ can be expressed compactly as a clipping (or projection onto the interval $[-1/2, 1]$) of a shifted value:\n$$ \\lambda_i = \\max(-1/2, \\min(1, \\lambda_{Ai} + \\gamma)) $$\nThe unknown scalar shift $\\gamma$ is determined by enforcing the traceless condition $\\sum_i \\lambda_i = 0$. This leads to a nonlinear scalar equation for $\\gamma$:\n$$ g(\\gamma) = \\sum_{i=1}^3 \\max(-1/2, \\min(1, \\lambda_{Ai} + \\gamma)) = 0 $$\nThe function $g(\\gamma)$ is a sum of clipped linear functions, making it a continuous, piecewise linear, and monotonically increasing function of $\\gamma$. As such, it has a unique root, which can be found efficiently. This completes the proof of the solution's structure from first principles.\n\n**Part 3: Algorithm Design**\n\nThe theoretical analysis directly leads to the following algorithm:\n1.  Given the input matrix $A \\in \\mathbb{S}_{0}^{3}$, compute its eigenvalues. Since $A$ is symmetric, we can use a standard numerical routine (such as from LAPACK via a library like NumPy) to find the eigenvalues $\\lambda_{A1}, \\lambda_{A2}, \\lambda_{A3}$. For consistency, we sort them in non-increasing order: $\\lambda_{A1} \\ge \\lambda_{A2} \\ge \\lambda_{A3}$.\n\n2.  Check if the eigenvalues $\\lambda_{Ai}$ already satisfy the admissibility constraints, i.e., $\\lambda_{A1} \\le 1$ and $\\lambda_{A3} \\ge -1/2$. If they do, the point is already in the feasible set, and the projection is the point itself: $\\lambda^{\\star}_i = \\lambda_{Ai}$.\n\n3.  If the eigenvalues are not admissible, solve the scalar root-finding problem $g(\\gamma) = 0$.\n    a.  Define the function $g(\\gamma) = \\sum_{i=1}^3 \\mathrm{clip}(\\lambda_{Ai} + \\gamma, -1/2, 1)$, where $\\mathrm{clip}(x, a, b) = \\max(a, \\min(b, x))$.\n    b.  Because $g(\\gamma)$ is monotonic, we can use a robust method like bisection to find the unique root $\\gamma^{\\star}$. A suitable search interval $[\\gamma_{\\min}, \\gamma_{\\max}]$ must be established. A safe choice is $[\\gamma_{\\min}, \\gamma_{\\max}] = [-1/2 - \\lambda_{A1}, 1 - \\lambda_{A3}]$, since this range guarantees that the shifted values $\\lambda_{Ai} + \\gamma$ can reach any point in the target interval $[-1/2, 1]$.\n    c.  The bisection algorithm proceeds by repeatedly halving the search interval while keeping the root bracketed, until the interval width is below a desired numerical tolerance.\n\n4.  Once the optimal shift $\\gamma^{\\star}$ is determined, compute the projected eigenvalues:\n    $$ \\lambda^{\\star}_i = \\max(-1/2, \\min(1, \\lambda_{Ai} + \\gamma^{\\star})) $$\n    \n5.  Sort the resulting eigenvalues $\\lambda^{\\star}_i$ in non-increasing order and provide them as the final result. This algorithm is guaranteed to terminate in finite time with a unique, correct solution.\n\nThe implementation will use `numpy.linalg.eigh` for eigenvalue calculation and a custom bisection search function to solve for $\\gamma$.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the Frobenius-norm projection of a symmetric traceless 3x3 matrix\n    onto the set of physically admissible orientational order parameter tensors.\n    \"\"\"\n\n    test_cases = [\n        np.array([[0.3, 0.0, 0.0], [0.0, -0.2, 0.0], [0.0, 0.0, -0.1]]),\n        np.array([[1.4, 0.0, 0.0], [0.0, -0.5, 0.0], [0.0, 0.0, -0.9]]),\n        np.array([[0.8, 0.0, 0.0], [0.0, -0.7, 0.0], [0.0, 0.0, -0.1]]),\n        np.array([[0.6, 0.2, 0.0], [0.2, -0.1, 0.1], [0.0, 0.1, -0.5]]),\n        np.array([[0.7, 0.0, 0.0], [0.0, 0.7, 0.0], [0.0, 0.0, -1.4]]),\n        np.array([[0.9, -0.4, 0.3], [-0.4, -0.2, 0.5], [0.3, 0.5, -0.7]])\n    ]\n\n    results = []\n\n    for A in test_cases:\n        # Step 1: Compute and sort eigenvalues of A\n        # numpy.linalg.eigh returns eigenvalues in non-decreasing order.\n        # We sort them in non-increasing order.\n        eigvals_A = np.linalg.eigh(A)[0][::-1]\n\n        # Step 2: Check if eigenvalues are already in the admissible set [-0.5, 1]\n        if eigvals_A[0] <= 1.0 and eigvals_A[-1] >= -0.5:\n            projected_eigvals = eigvals_A\n        else:\n            # Step 3: Solve for the Lagrange multiplier gamma using bisection\n            lower_bound = -1/2\n            upper_bound = 1\n\n            def g(gamma, eigvals):\n                return np.sum(np.clip(eigvals + gamma, lower_bound, upper_bound))\n\n            # Establish a safe search range for gamma\n            gamma_min = lower_bound - eigvals_A[0]\n            gamma_max = upper_bound - eigvals_A[-1]\n\n            # Bisection method\n            tol = 1e-12\n            max_iter = 100\n            for _ in range(max_iter):\n                gamma = (gamma_min + gamma_max) / 2\n                f_val = g(gamma, eigvals_A)\n                \n                if abs(f_val) < tol:\n                    break\n                \n                if f_val > 0:\n                    gamma_max = gamma\n                else:\n                    gamma_min = gamma\n\n            # Step 4: Compute the projected eigenvalues using the found gamma\n            projected_eigvals = np.clip(eigvals_A + gamma, lower_bound, upper_bound)\n        \n        # Ensure final eigenvalues are sorted for consistent output\n        projected_eigvals = np.sort(projected_eigvals)[::-1]\n        \n        # Format results to 6 decimal places\n        formatted_eigvals = [f\"{val:.6f}\" for val in projected_eigvals]\n        results.append(f\"[{','.join(formatted_eigvals)}]\")\n\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "2933004"}]}