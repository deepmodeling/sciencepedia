{"hands_on_practices": [{"introduction": "When modeling systems with position-dependent mobility, the stochastic dynamics are described by an equation with multiplicative noise. This practice delves into the crucial distinction between the Itô and Stratonovich interpretations of stochastic integrals, which are not merely notational choices but reflect different underlying mathematical assumptions. By deriving the conversion formula from first principles, you will uncover the origin of the \"spurious drift\" term, a vital correction needed to ensure the Fokker-Planck equation corresponds to the correct physical reality [@problem_id:2815958].", "problem": "In many coarse-grained descriptions of chemical dynamics, a one-dimensional reaction coordinate $x(t)$ evolves under an overdamped Langevin equation with multiplicative noise arising from a position-dependent mobility. Consider the Stratonovich stochastic differential equation (SDE) for $x(t)$,\n$$\ndx = a(x)\\,dt + b(x)\\circ dW_t,\n$$\nwhere $W_t$ is a standard Wiener process and $a(x)$ and $b(x)$ are sufficiently smooth functions. Without assuming any pre-known conversion formula, derive the equivalent Itô SDE that governs the same physical process by starting from the Riemann-sum definitions of the Stratonovich and Itô stochastic integrals and the mean-square properties of Wiener increments. Then, identify explicitly the additional drift term that appears when changing from the Stratonovich to the Itô interpretation (often termed the spurious drift in theoretical chemistry and statistical physics). \n\nProvide your final answer as a single closed-form analytical expression for this additional drift term in terms of $a(x)$, $b(x)$, and their spatial derivatives. Do not include any units in the final boxed expression.", "solution": "The problem as stated is valid. It is a well-posed, scientifically grounded question fundamental to the study of stochastic processes in theoretical chemistry and physics. It requests a derivation from first principles, which is a standard and necessary exercise for understanding the distinction between the Itô and Stratonovich calculi. We shall proceed with the derivation.\n\nWe are given the Stratonovich stochastic differential equation (SDE):\n$$\ndx = a(x)\\,dt + b(x)\\circ dW_t\n$$\nwhere $x(t)$ is the state variable, $a(x)$ is the drift coefficient, $b(x)$ is the diffusion coefficient (representing position-dependent mobility), and $dW_t$ is the increment of a standard Wiener process. Our objective is to find the equivalent Itô SDE by analyzing the definition of the Stratonovich integral.\n\nThe integral form of the SDE is:\n$$\nx(t) - x(t_0) = \\int_{t_0}^{t} a(x(s)) \\, ds + \\int_{t_0}^{t} b(x(s)) \\circ dW_s\n$$\nThe core of the problem lies in the interpretation of the stochastic integral. We discretize the time interval $[t_0, t]$ into $N$ subintervals of width $\\Delta t$, where $t_k = t_0 + k\\Delta t$. The Stratonovich integral is defined as the limit of a sum where the integrand is evaluated at the midpoint of each time-step $[t_k, t_{k+1}]$:\n$$\n\\int_{t_0}^{t} b(x(s)) \\circ dW_s = \\lim_{N \\to \\infty} \\sum_{k=0}^{N-1} b\\left(x\\left(\\frac{t_k + t_{k+1}}{2}\\right)\\right) [W(t_{k+1}) - W(t_k)]\n$$\nFor a sufficiently small step size, we can approximate the midpoint evaluation point $x\\left(\\frac{t_k + t_{k+1}}{2}\\right)$ using the average of the process at the endpoints, $\\frac{x(t_k) + x(t_{k+1})}{2}$. Let us denote $x_k = x(t_k)$ and the Wiener increment $\\Delta W_k = W(t_{k+1}) - W(t_k)$. The discrete sum approximating the Stratonovich integral, which we denote $I_S$, is then:\n$$\nI_S \\approx \\sum_{k=0}^{N-1} b\\left(\\frac{x_{k+1} + x_k}{2}\\right) \\Delta W_k\n$$\nTo relate this to the Itô integral, which evaluates the integrand at the start of the interval, $t_k$, we must expand the term $b\\left(\\frac{x_{k+1} + x_k}{2}\\right)$ around the point $x_k$. First, we approximate $x_{k+1}$ using a single discrete step of the SDE itself:\n$$\nx_{k+1} \\approx x_k + a(x_k)\\Delta t + b(x_k)\\Delta W_k\n$$\nUsing this, the argument of $b$ becomes:\n$$\n\\frac{x_{k+1} + x_k}{2} \\approx \\frac{(x_k + a(x_k)\\Delta t + b(x_k)\\Delta W_k) + x_k}{2} = x_k + \\frac{1}{2}a(x_k)\\Delta t + \\frac{1}{2}b(x_k)\\Delta W_k\n$$\nSince $b(x)$ is assumed to be a smooth function, we perform a Taylor expansion of $b$ around $x_k$. Let $b'(x) = \\frac{db}{dx}$.\n$$\nb\\left(\\frac{x_{k+1} + x_k}{2}\\right) \\approx b\\left(x_k + \\frac{1}{2}a(x_k)\\Delta t + \\frac{1}{2}b(x_k)\\Delta W_k\\right)\n$$\n$$\nb\\left(\\frac{x_{k+1} + x_k}{2}\\right) \\approx b(x_k) + b'(x_k) \\left( \\frac{1}{2}a(x_k)\\Delta t + \\frac{1}{2}b(x_k)\\Delta W_k \\right) + \\mathcal{O}((\\Delta t)^2, (\\Delta W_k)^2)\n$$\nWe substitute this expansion back into the sum for $I_S$:\n$$\nI_S \\approx \\sum_{k=0}^{N-1} \\left[ b(x_k) + \\frac{1}{2}b'(x_k)b(x_k)\\Delta W_k + \\frac{1}{2}b'(x_k)a(x_k)\\Delta t \\right] \\Delta W_k\n$$\nDistributing the $\\Delta W_k$ term yields:\n$$\nI_S \\approx \\sum_{k=0}^{N-1} b(x_k)\\Delta W_k + \\sum_{k=0}^{N-1} \\frac{1}{2} b(x_k) b'(x_k) (\\Delta W_k)^2 + \\sum_{k=0}^{N-1} \\frac{1}{2} a(x_k) b'(x_k) \\Delta t \\Delta W_k\n$$\nWe now take the limit as $\\Delta t \\to 0$. The three sums behave as follows:\n1.  The first sum, $\\sum_{k=0}^{N-1} b(x_k)\\Delta W_k$, is precisely the definition of the Itô integral, where the integrand is evaluated at the left endpoint $t_k$.\n    $$\n    \\lim_{\\Delta t \\to 0} \\sum_{k=0}^{N-1} b(x_k)\\Delta W_k = \\int_{t_0}^{t} b(x(s)) dW_s \\quad (\\text{Itô})\n    $$\n2.  The second sum involves the quadratic variation of the Wiener process. The increments $\\Delta W_k$ are independent random variables with mean $\\mathbb{E}[\\Delta W_k] = 0$ and variance $\\mathbb{E}[(\\Delta W_k)^2] = \\Delta t$. In the mean-square sense, the sum $\\sum (\\Delta W_k)^2$ converges to $t - t_0$. Thus, $(\\Delta W_k)^2$ becomes $dt$ in the continuous limit.\n    $$\n    \\lim_{\\Delta t \\to 0} \\sum_{k=0}^{N-1} \\frac{1}{2} b(x_k) b'(x_k) (\\Delta W_k)^2 = \\int_{t_0}^{t} \\frac{1}{2} b(x(s)) b'(x(s)) ds\n    $$\n3.  The third sum involves the term $\\Delta t \\Delta W_k$, which has a stochastic order of $(\\Delta t)^{3/2}$. This term vanishes faster than the other terms as $\\Delta t \\to 0$.\n    $$\n    \\lim_{\\Delta t \\to 0} \\sum_{k=0}^{N-1} \\frac{1}{2} a(x_k) b'(x_k) \\Delta t \\Delta W_k = 0\n    $$\nCombining these results, the Stratonovich integral is expressed in terms of an Itô integral and a standard Riemann integral:\n$$\n\\int_{t_0}^{t} b(x(s)) \\circ dW_s = \\int_{t_0}^{t} b(x(s)) dW_s + \\int_{t_0}^{t} \\frac{1}{2} b(x(s)) b'(x(s)) ds\n$$\nIn differential notation, this relationship is:\n$$\nb(x) \\circ dW_t = b(x) dW_t + \\frac{1}{2} b(x) b'(x) dt\n$$\nSubstituting this back into the original Stratonovich SDE:\n$$\ndx = a(x) dt + \\left( b(x) dW_t + \\frac{1}{2} b(x) b'(x) dt \\right)\n$$\nWe group the terms proportional to $dt$ to obtain the final Itô form:\n$$\ndx = \\left( a(x) + \\frac{1}{2} b(x) b'(x) \\right) dt + b(x) dW_t\n$$\nThis is the equivalent Itô SDE. The original drift was $a(x)$. The new effective drift in the Itô representation is $a(x) + \\frac{1}{2} b(x) b'(x)$. The additional drift term, also known as the noise-induced or spurious drift, is the correction required when converting from the Stratonovich interpretation, which uses standard calculus rules, to the Itô interpretation. This term is the difference between the Itô drift and the Stratonovich drift:\n$$\na_{\\text{additional}}(x) = \\left( a(x) + \\frac{1}{2}b(x)b'(x) \\right) - a(x) = \\frac{1}{2}b(x)b'(x)\n$$\nThis term arises because the Stratonovich integral implicitly accounts for the correlation between the process $x(t)$ and the noise increment $dW_t$ within an infinitesimal time step, a correlation that the Itô integral, by its forward-point definition, ignores. The additional drift term explicitly restores this physical information.", "answer": "$$\n\\boxed{\\frac{1}{2} b(x) b'(x)}\n$$", "id": "2815958"}, {"introduction": "Many fundamental processes in physics, chemistry, and biology—from protein folding to chemical reactions—can be framed as a particle escaping from a region or reaching a target. The mean first passage time (MFPT) is the key observable that quantifies the average timescale for such events. This exercise guides you through solving for the MFPT by applying the backward Kolmogorov equation, demonstrating a powerful analytical technique and reinforcing the physical meaning of absorbing and reflecting boundary conditions in diffusion problems [@problem_id:2932592].", "problem": "A single coarse-grained bead of a flexible polymer in a nanochannel is modeled by an overdamped one-dimensional coordinate $x(t)$ constrained to the interval $[0,L]$, where $x(t)=0$ represents a hard reflecting wall and $x(t)=L$ represents an absorbing exit. The bead undergoes thermal diffusion with constant diffusivity $D$ and no systematic drift, governed at mesoscopic scales by the overdamped Langevin equation\n$$\n\\frac{dx}{dt}=\\sqrt{2D}\\,\\eta(t),\n$$\nwhere $\\eta(t)$ is standard Gaussian white noise with $\\langle \\eta(t)\\rangle = 0$ and $\\langle \\eta(t)\\eta(t')\\rangle=\\delta(t-t')$. Let $p(x,t\\,|\\,x_0,0)$ denote the conditional probability density for $x(t)$ given $x(0)=x_0\\in[0,L]$. The corresponding Fokker–Planck equation (FPE) for $p(x,t\\,|\\,x_0,0)$ is to be used together with physically correct boundary conditions for a reflecting wall at $x=0$ and an absorbing boundary at $x=L$.\n\nDefine the mean first passage time (MFPT) $\\tau(x_0)$ as the expectation value of the random time to reach $x=L$ for the first time, starting from $x(0)=x_0$, under the dynamics above. Starting from the Langevin model and the Fokker–Planck description as the fundamental base, derive and solve the boundary–value problem satisfied by $\\tau(x_0)$, imposing the correct boundary conditions associated with a reflecting endpoint at $x=0$ and an absorbing endpoint at $x=L$.\n\nProvide your final result as a single closed-form analytic expression for $\\tau(x_0)$ in terms of $L$, $x_0$, and $D$. Express your answer in symbolically simplified form. Do not include units in your final expression. Assume that $D$ is provided in $\\mathrm{m^2\\,s^{-1}}$ and that $L$ and $x_0$ are provided in $\\mathrm{m}$, so that $\\tau(x_0)$ is in $\\mathrm{s}$.", "solution": "The problem statement describes a classical one-dimensional diffusion process within a finite domain, subject to mixed boundary conditions. We first verify the correspondence between the given Langevin equation and the associated Fokker-Planck equation (FPE), and then derive and solve the boundary-value problem for the mean first passage time (MFPT).\n\nThe system is governed by the overdamped Langevin equation for the coordinate $x(t)$:\n$$\n\\frac{dx}{dt} = \\sqrt{2D}\\,\\eta(t)\n$$\nHere, the drift term is zero, and the diffusion term is constant. This corresponds to a Fokker-Planck equation for the conditional probability density $p(x,t) \\equiv p(x, t \\,|\\, x_0, 0)$ that is a simple diffusion equation:\n$$\n\\frac{\\partial p(x,t)}{\\partial t} = D \\frac{\\partial^2 p(x,t)}{\\partial x^2}\n$$\nThe physical constraints on the system are expressed as boundary conditions on $p(x,t)$. The hard reflecting wall at $x=0$ implies that the probability flux $J(x,t) = -D \\frac{\\partial p}{\\partial x}$ must vanish. The absorbing exit at $x=L$ implies that the probability of finding the particle there is zero. Thus, the boundary conditions are:\n$$\n\\frac{\\partial p}{\\partial x}\\bigg|_{x=0} = 0 \\quad \\text{and} \\quad p(L,t) = 0\n$$\n\nThe quantity of interest is the mean first passage time $\\tau(x_0)$, defined as the average time for a particle starting at $x(0)=x_0$ to first reach the absorbing boundary at $x=L$. A standard method to determine $\\tau(x_0)$ is through the backward Kolmogorov equation. Let us consider the evolution of the system over a small time interval $\\Delta t$. The MFPT from a starting point $x_0$ can be related to the MFPT from the new position $x_0 + \\Delta x$ after time $\\Delta t$. This relation is expressed as:\n$$\n\\tau(x_0) = \\Delta t + \\langle \\tau(x_0 + \\Delta x) \\rangle\n$$\nThe average $\\langle \\dots \\rangle$ is taken over all possible displacements $\\Delta x$ during the interval $\\Delta t$. We perform a Taylor expansion of $\\tau(x_0 + \\Delta x)$ around $x_0$:\n$$\n\\tau(x_0 + \\Delta x) = \\tau(x_0) + \\frac{d\\tau}{dx_0} \\Delta x + \\frac{1}{2} \\frac{d^2\\tau}{dx_0^2} (\\Delta x)^2 + O((\\Delta x)^3)\n$$\nTaking the expectation value of this expansion gives:\n$$\n\\langle \\tau(x_0 + \\Delta x) \\rangle = \\tau(x_0) + \\frac{d\\tau}{dx_0} \\langle \\Delta x \\rangle + \\frac{1}{2} \\frac{d^2\\tau}{dx_0^2} \\langle (\\Delta x)^2 \\rangle + \\dots\n$$\nThe moments of the displacement $\\Delta x$ are determined from the Langevin equation. Over a small time $\\Delta t$:\n$$\n\\langle \\Delta x \\rangle = \\left\\langle \\int_0^{\\Delta t} \\sqrt{2D}\\,\\eta(t') dt' \\right\\rangle = \\sqrt{2D} \\int_0^{\\Delta t} \\langle \\eta(t') \\rangle dt' = 0\n$$\n$$\n\\langle (\\Delta x)^2 \\rangle = \\left\\langle \\left( \\int_0^{\\Delta t} \\sqrt{2D}\\,\\eta(t') dt' \\right)^2 \\right\\rangle = 2D \\int_0^{\\Delta t} \\int_0^{\\Delta t} \\langle \\eta(t') \\eta(t'') \\rangle dt' dt''\n$$\nUsing the property $\\langle \\eta(t') \\eta(t'') \\rangle = \\delta(t' - t'')$, the double integral reduces to a single integral:\n$$\n\\langle (\\Delta x)^2 \\rangle = 2D \\int_0^{\\Delta t} dt' = 2D \\Delta t\n$$\nHigher-order moments are of order $O((\\Delta t)^{3/2})$ or smaller and can be neglected. Substituting these moments back into the recursion for $\\tau(x_0)$:\n$$\n\\tau(x_0) = \\Delta t + \\tau(x_0) + \\frac{1}{2} \\frac{d^2\\tau}{dx_0^2} (2D \\Delta t)\n$$\nSubtracting $\\tau(x_0)$ from both sides and dividing by $\\Delta t$ yields the ordinary differential equation for the MFPT, often called Dynkin's equation:\n$$\nD \\frac{d^2\\tau}{dx_0^2} = -1\n$$\nThis equation must be solved subject to boundary conditions on $\\tau(x_0)$. These conditions are derived from the physical nature of the boundaries at $x_0=0$ and $x_0=L$.\n1. At the absorbing boundary $x_0=L$: If the particle starts at the exit, the time to reach it is zero. Therefore, $\\tau(L) = 0$.\n2. At the reflecting boundary $x_0=0$: A particle at an infinitesimal distance $\\epsilon$ from the wall has the same expectation of exit time as a particle that would be at $-\\epsilon$ in an unbounded system with the same exit at $L$. This symmetry implies that the MFPT must have an extremum at a reflecting wall. Consequently, its derivative must be zero. Thus, $\\frac{d\\tau}{dx_0}\\bigg|_{x_0=0} = 0$.\n\nWe now solve the boundary-value problem. Integrating the differential equation once with respect to $x_0$ yields:\n$$\n\\frac{d\\tau}{dx_0} = -\\frac{x_0}{D} + C_1\n$$\nwhere $C_1$ is a constant of integration. Applying the reflecting boundary condition at $x_0=0$:\n$$\n\\frac{d\\tau}{dx_0}\\bigg|_{x_0=0} = -\\frac{0}{D} + C_1 = 0 \\quad \\implies \\quad C_1 = 0\n$$\nThe equation for the first derivative simplifies to:\n$$\n\\frac{d\\tau}{dx_0} = -\\frac{x_0}{D}\n$$\nIntegrating a second time with respect to $x_0$ gives the general solution for $\\tau(x_0)$:\n$$\n\\tau(x_0) = \\int \\left( -\\frac{x_0}{D} \\right) dx_0 = -\\frac{x_0^2}{2D} + C_2\n$$\nwhere $C_2$ is the second constant of integration. We determine $C_2$ using the absorbing boundary condition at $x_0=L$:\n$$\n\\tau(L) = -\\frac{L^2}{2D} + C_2 = 0 \\quad \\implies \\quad C_2 = \\frac{L^2}{2D}\n$$\nSubstituting the value of $C_2$ back into the expression for $\\tau(x_0)$ gives the final result:\n$$\n\\tau(x_0) = \\frac{L^2}{2D} - \\frac{x_0^2}{2D}\n$$\nThis can be written in a more compact form. The result is physically consistent: the MFPT is maximal for a particle starting at the reflecting wall ($x_0=0$) and zero for a particle starting at the absorbing exit ($x_0=L$). The time scales as $L^2/D$, which is characteristic of diffusive processes.", "answer": "$$\n\\boxed{\\frac{L^{2} - x_{0}^{2}}{2D}}\n$$", "id": "2932592"}, {"introduction": "Bridging the gap between theoretical models and practical results often requires robust numerical simulation. This practice addresses the critical challenge of simulating Langevin dynamics with multiplicative noise while guaranteeing that the system relaxes to the correct thermal equilibrium. You will first analytically verify that a specific form of the Fokker-Planck equation preserves the Boltzmann distribution, and then design and implement a Metropolis-corrected algorithm that enforces this exact statistical property at finite time steps, a cornerstone technique in modern molecular simulation [@problem_id:2815978].", "problem": "Consider a one-dimensional overdamped Langevin dynamics with multiplicative noise arising from a position-dependent mobility. Let $x_t \\in \\mathbb{R}$ evolve according to the Itô stochastic differential equation\n$$\n\\mathrm{d}x_t \\;=\\; a(x_t)\\,\\mathrm{d}t \\;+\\; b(x_t)\\,\\mathrm{d}W_t,\n$$\nwhere $W_t$ is a standard Wiener process, $m(x)  0$ is a smooth scalar mobility, $U(x)$ is a smooth potential, and $T$ is the absolute temperature with inverse temperature $\\beta = (k_{\\mathrm{B}} T)^{-1}$ for Boltzmann constant $k_{\\mathrm{B}}$. In the isothermal convention, the drift and diffusion amplitudes are\n$$\na(x) \\;=\\; -\\,m(x)\\,U'(x) \\;+\\; \\beta^{-1} m'(x), \n\\qquad \nb(x) \\;=\\; \\sqrt{2\\,\\beta^{-1}\\, m(x)}.\n$$\n\nTask A (derivation from first principles): Starting from the Fokker–Planck equation (FPE) associated with the Itô stochastic differential equation, \n$$\n\\partial_t \\rho(x,t) \\;=\\; -\\partial_x\\!\\big(a(x)\\,\\rho(x,t)\\big) \\;+\\; \\tfrac{1}{2}\\,\\partial_x^2\\!\\big(b^2(x)\\,\\rho(x,t)\\big),\n$$\nderive the unique smooth stationary solution $\\rho_\\star(x)$ up to normalization. Your derivation must begin from the Fokker–Planck equation and standard calculus rules, not from any pre-quoted specialized result. Show explicitly that the choice of $a(x)$ and $b(x)$ given above yields the Boltzmann distribution as the stationary solution\n$$\n\\rho_\\star(x) \\;\\propto\\; \\exp\\!\\big(-\\beta\\,U(x)\\big).\n$$\n\nTask B (design of a measure-preserving discretization and proof of stationarity): Consider the following discretization which uses an Euler–Maruyama proposal with isothermal drift and a Metropolis–Hastings (MH) accept/reject step to correct finite time-step bias. For a time step $\\Delta t  0$ and current state $x$, define the proposal mean and variance\n$$\n\\mu(x) \\;=\\; x \\;+\\; a(x)\\,\\Delta t, \n\\qquad \ns^2(x) \\;=\\; b^2(x)\\,\\Delta t \\;=\\; 2\\,\\beta^{-1}\\,m(x)\\,\\Delta t,\n$$\nand propose $x^\\dagger \\sim \\mathcal{N}\\!\\big(\\mu(x),\\, s^2(x)\\big)$. Denote the proposal transition density by\n$$\nq(x^\\dagger \\mid x) \\;=\\; \\frac{1}{\\sqrt{2\\pi s^2(x)}}\\,\n\\exp\\!\\left(-\\frac{\\left(x^\\dagger - \\mu(x)\\right)^2}{2\\,s^2(x)}\\right).\n$$\nAccept the proposal with probability\n$$\n\\alpha(x \\to x^\\dagger) \\;=\\; \\min\\!\\left\\{\\,1,\\;\n\\frac{\\exp\\!\\big(-\\beta U(x^\\dagger)\\big)\\, q(x \\mid x^\\dagger)}{\\exp\\!\\big(-\\beta U(x)\\big)\\, q(x^\\dagger \\mid x)} \\right\\},\n$$\nand otherwise stay at $x$. Prove, using detailed balance, that this Markov chain has the exact stationary distribution\n$$\n\\pi(x) \\;\\propto\\; \\exp\\!\\big(-\\beta U(x)\\big).\n$$\nBriefly explain how this construction relates to the goal of preserving the correct equilibrium distribution for dynamics with multiplicative noise, and comment on the role of Fixman-type midpoint ideas versus the Metropolis–Hastings correction in achieving exact stationarity at finite $\\Delta t$.\n\nTask C (implementation and testing): Implement the above measure-preserving algorithm in a program that runs without any external input. Work in dimensionless units with $\\beta = 1$. For each test case below, simulate the Markov chain, discard an initial burn-in, and thin the chain to reduce autocorrelation. Estimate the mean $\\mathbb{E}[X]$ and the second moment $\\mathbb{E}[X^2]$ from the retained samples, and compare them against the exact Boltzmann expectations computed by numerical quadrature of\n$$\n\\mathbb{E}[f(X)] \\;=\\; \\frac{\\displaystyle \\int_{\\mathbb{R}} f(x)\\, e^{-U(x)}\\, \\mathrm{d}x}{\\displaystyle \\int_{\\mathbb{R}} e^{-U(x)}\\, \\mathrm{d}x}.\n$$\nReport, for each test case, two booleans indicating whether the absolute errors in the estimated mean and second moment are below the specified tolerances. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $\\texttt{[result1,result2,result3]}$).\n\nUse the following test suite, where all parameters are dimensionless:\n\n- Test $1$ (happy path, harmonic potential with gently varying mobility):\n  - Potential $U(x) = \\tfrac{1}{2}\\,x^2$.\n  - Mobility $m(x) = 1 + \\tfrac{1}{2}\\,x^2$.\n  - Time step $\\Delta t = 0.5$.\n  - Total steps $N = 150{,}000$, burn-in $N_{\\mathrm{burn}} = 30{,}000$, thinning factor $n_{\\mathrm{thin}} = 10$.\n  - Pseudorandom seed $s = 12345$.\n  - Tolerances: $|\\widehat{\\mathbb{E}}[X] - \\mathbb{E}[X]| \\le 2\\times 10^{-2}$, $|\\widehat{\\mathbb{E}}[X^2] - \\mathbb{E}[X^2]| \\le 3\\times 10^{-2}$.\n\n- Test $2$ (edge case with rapidly varying mobility):\n  - Potential $U(x) = \\tfrac{1}{2}\\,x^2$.\n  - Mobility $m(x) = \\exp(0.8\\,x)$.\n  - Time step $\\Delta t = 0.25$.\n  - Total steps $N = 160{,}000$, burn-in $N_{\\mathrm{burn}} = 30{,}000$, thinning factor $n_{\\mathrm{thin}} = 10$.\n  - Pseudorandom seed $s = 24680$.\n  - Tolerances: $|\\widehat{\\mathbb{E}}[X] - \\mathbb{E}[X]| \\le 2\\times 10^{-2}$, $|\\widehat{\\mathbb{E}}[X^2] - \\mathbb{E}[X^2]| \\le 4\\times 10^{-2}$.\n\n- Test $3$ (non-quadratic potential with bounded, oscillatory mobility):\n  - Potential $U(x) = \\tfrac{1}{4}\\,x^4 + \\tfrac{1}{2}\\,x^2$.\n  - Mobility $m(x) = 1 + \\tfrac{1}{2}\\,\\sin(x)$.\n  - Time step $\\Delta t = 0.20$.\n  - Total steps $N = 160{,}000$, burn-in $N_{\\mathrm{burn}} = 30{,}000$, thinning factor $n_{\\mathrm{thin}} = 10$.\n  - Pseudorandom seed $s = 11223$.\n  - Tolerances: $|\\widehat{\\mathbb{E}}[X] - \\mathbb{E}[X]| \\le 2\\times 10^{-2}$, $|\\widehat{\\mathbb{E}}[X^2] - \\mathbb{E}[X^2]| \\le 4\\times 10^{-2}$.\n\nAll quantities are dimensionless. The final output must be a single line containing a list of $6$ booleans in the order\n$$\n[\\text{Test 1 mean ok},\\; \\text{Test 1 second-moment ok},\\; \\text{Test 2 mean ok},\\; \\text{Test 2 second-moment ok},\\; \\text{Test 3 mean ok},\\; \\text{Test 3 second-moment ok}].\n$$", "solution": "The problem presented is a valid and well-posed exercise in theoretical and computational statistical mechanics. It correctly formulates the overdamped Langevin dynamics with multiplicative noise, the associated Fokker-Planck equation, and a standard, robust numerical scheme for sampling its equilibrium distribution. We will proceed with the required derivations and proofs.\n\n**Task A: Derivation of the Stationary Solution**\n\nThe objective is to derive the stationary probability density function $\\rho_\\star(x)$ from the given Fokker-Planck equation (FPE) and show that it corresponds to the Boltzmann distribution.\n\nThe FPE for the probability density $\\rho(x,t)$ is given as:\n$$\n\\partial_t \\rho(x,t) \\;=\\; -\\partial_x\\big(a(x)\\,\\rho(x,t)\\big) \\;+\\; \\tfrac{1}{2}\\,\\partial_x^2\\big(b^2(x)\\,\\rho(x,t)\\big)\n$$\nThis equation can be expressed as a continuity equation, $\\partial_t \\rho = -\\partial_x J$, where $J(x,t)$ is the probability current density:\n$$\nJ(x,t) \\;=\\; a(x)\\,\\rho(x,t) \\;-\\; \\tfrac{1}{2}\\,\\partial_x\\big(b^2(x)\\,\\rho(x,t)\\big)\n$$\nA stationary solution, $\\rho_\\star(x)$, is defined by the condition $\\partial_t \\rho(x,t) = 0$. This implies that the stationary probability current, $J_\\star(x)$, must be constant with respect to position $x$. For a system confined by a potential, we impose natural (reflecting) boundary conditions at infinity, such that the probability current vanishes, i.e., $\\lim_{x\\to\\pm\\infty} J_\\star(x) = 0$. This requires the constant current to be zero everywhere, so $J_\\star(x) = 0$ for all $x$.\n\nSetting the stationary current to zero yields the first-order ordinary differential equation:\n$$\na(x)\\,\\rho_\\star(x) \\;-\\; \\tfrac{1}{2}\\,\\frac{\\mathrm{d}}{\\mathrm{d}x}\\big(b^2(x)\\,\\rho_\\star(x)\\big) \\;=\\; 0\n$$\nWe are given the drift and diffusion coefficients in the isothermal convention:\n$$\na(x) \\;=\\; -m(x)\\,U'(x) \\;+\\; \\beta^{-1} m'(x)\n$$\n$$\nb^2(x) \\;=\\; 2\\,\\beta^{-1}\\,m(x)\n$$\nwhere $m'(x) = \\frac{\\mathrm{d}m}{\\mathrm{d}x}$ and $U'(x) = \\frac{\\mathrm{d}U}{\\mathrm{d}x}$.\n\nSubstituting these expressions into the zero-current condition:\n$$\n\\left( -m(x)\\,U'(x) \\;+\\; \\beta^{-1} m'(x) \\right) \\rho_\\star(x) \\;-\\; \\tfrac{1}{2}\\,\\frac{\\mathrm{d}}{\\mathrm{d}x}\\big(2\\,\\beta^{-1}\\,m(x)\\,\\rho_\\star(x)\\big) \\;=\\; 0\n$$\nSimplifying the second term:\n$$\n\\left( -m(x)\\,U'(x) \\;+\\; \\beta^{-1} m'(x) \\right) \\rho_\\star(x) \\;-\\; \\beta^{-1}\\,\\frac{\\mathrm{d}}{\\mathrm{d}x}\\big(m(x)\\,\\rho_\\star(x)\\big) \\;=\\; 0\n$$\nWe apply the product rule for differentiation to the term $\\frac{\\mathrm{d}}{\\mathrm{d}x}(m(x)\\,\\rho_\\star(x))$:\n$$\n\\frac{\\mathrm{d}}{\\mathrm{d}x}\\big(m(x)\\,\\rho_\\star(x)\\big) \\;=\\; m'(x)\\,\\rho_\\star(x) \\;+\\; m(x)\\,\\rho_\\star'(x)\n$$\nSubstituting this back into the equation:\n$$\n\\left( -m(x)\\,U'(x) \\;+\\; \\beta^{-1} m'(x) \\right) \\rho_\\star(x) \\;-\\; \\beta^{-1}\\left( m'(x)\\,\\rho_\\star(x) \\;+\\; m(x)\\,\\rho_\\star'(x) \\right) \\;=\\; 0\n$$\nDistributing the factors and rearranging terms:\n$$\n-m(x)\\,U'(x)\\,\\rho_\\star(x) \\;+\\; \\beta^{-1} m'(x)\\,\\rho_\\star(x) \\;-\\; \\beta^{-1} m'(x)\\,\\rho_\\star(x) \\;-\\; \\beta^{-1} m(x)\\,\\rho_\\star'(x) \\;=\\; 0\n$$\nThe terms involving $m'(x)\\,\\rho_\\star(x)$ cancel each other precisely. This cancellation is the key consequence of choosing the Itô drift term $a(x)$ in this specific form. We are left with:\n$$\n-m(x)\\,U'(x)\\,\\rho_\\star(x) \\;-\\; \\beta^{-1} m(x)\\,\\rho_\\star'(x) \\;=\\; 0\n$$\nSince the mobility $m(x)  0$ for all $x$, we can divide the entire equation by $-m(x)$:\n$$\nU'(x)\\,\\rho_\\star(x) \\;+\\; \\beta^{-1}\\,\\rho_\\star'(x) \\;=\\; 0\n$$\nThis is a separable first-order linear ordinary differential equation for $\\rho_\\star(x)$:\n$$\n\\frac{\\rho_\\star'(x)}{\\rho_\\star(x)} \\;=\\; -\\beta\\,U'(x)\n$$\nIntegrating both sides with respect to $x$:\n$$\n\\int \\frac{1}{\\rho_\\star(x)}\\,\\mathrm{d}\\rho_\\star(x) \\;=\\; -\\beta \\int U'(x)\\,\\mathrm{d}x\n$$\n$$\n\\ln\\rho_\\star(x) \\;=\\; -\\beta\\,U(x) \\;+\\; C_1\n$$\nwhere $C_1$ is the constant of integration. Exponentiating both sides gives the stationary solution:\n$$\n\\rho_\\star(x) \\;=\\; e^{C_1} e^{-\\beta\\,U(x)} \\;=\\; C e^{-\\beta\\,U(x)}\n$$\nwhere $C = e^{C_1}$ is a normalization constant. This proves that the stationary solution is the Boltzmann distribution, $\\rho_\\star(x) \\propto \\exp(-\\beta\\,U(x))$, as required.\n\n**Task B: Proof of Detailed Balance**\n\nThe objective is to prove that the given Metropolis-Hastings corrected numerical scheme satisfies the detailed balance condition with respect to the target stationary distribution $\\pi(x) \\propto \\exp(-\\beta U(x))$.\n\nThe detailed balance condition states that for a Markov chain with transition kernel $P(x' \\mid x)$, the following must hold for the stationary distribution $\\pi(x)$:\n$$\n\\pi(x)\\,P(x' \\mid x) \\;=\\; \\pi(x')\\,P(x \\mid x')\n$$\nThe transition kernel $P(x' \\mid x)$ for the Metropolis-Hastings algorithm is composed of a proposal step and an acceptance step. For $x' \\ne x$, the transition density is given by the product of the proposal density $q(x' \\mid x)$ and the acceptance probability $\\alpha(x \\to x')$. The detailed balance condition for $x' \\ne x$ becomes:\n$$\n\\pi(x)\\,q(x' \\mid x)\\,\\alpha(x \\to x') \\;=\\; \\pi(x')\\,q(x \\mid x')\\,\\alpha(x' \\to x)\n$$\nThe acceptance probability is defined as:\n$$\n\\alpha(x \\to x') \\;=\\; \\min\\!\\left\\{\\,1,\\; \\frac{\\pi(x')\\, q(x \\mid x')}{\\pi(x)\\, q(x' \\mid x)} \\right\\}\n$$\nLet us define the Metropolis-Hastings ratio $R(x, x')$ as:\n$$\nR(x, x') \\;=\\; \\frac{\\pi(x')\\, q(x \\mid x')}{\\pi(x)\\, q(x' \\mid x)}\n$$\nThen $\\alpha(x \\to x') = \\min\\{1, R(x, x')\\}$ and, by symmetry, $\\alpha(x' \\to x) = \\min\\{1, R(x', x)\\}$. Note that $R(x', x) = 1/R(x, x')$.\n\nWe verify detailed balance by considering two cases.\n\nCase 1: $R(x, x') \\ge 1$.\nIn this case, $\\alpha(x \\to x') = 1$. Since $R(x, x') \\ge 1$, we have $R(x', x) = 1/R(x, x') \\le 1$. Therefore, $\\alpha(x' \\to x) = R(x', x)$.\nThe left-hand side (LHS) of the detailed balance equation is:\n$$\n\\text{LHS} \\;=\\; \\pi(x)\\,q(x' \\mid x)\\,(1) \\;=\\; \\pi(x)\\,q(x' \\mid x)\n$$\nThe right-hand side (RHS) is:\n$$\n\\text{RHS} \\;=\\; \\pi(x')\\,q(x \\mid x')\\,\\alpha(x' \\to x) \\;=\\; \\pi(x')\\,q(x \\mid x')\\,R(x', x)\n$$\nSubstituting the definition of $R(x', x)$:\n$$\n\\text{RHS} \\;=\\; \\pi(x')\\,q(x \\mid x')\\,\\left(\\frac{\\pi(x)\\, q(x' \\mid x)}{\\pi(x')\\, q(x \\mid x')}\\right) \\;=\\; \\pi(x)\\,q(x' \\mid x)\n$$\nThus, LHS = RHS, and detailed balance is satisfied.\n\nCase 2: $R(x, x')  1$.\nIn this case, $\\alpha(x \\to x') = R(x, x')$. Since $R(x, x')  1$, we have $R(x', x) = 1/R(x, x')  1$. Therefore, $\\alpha(x' \\to x) = 1$.\nThe LHS of the detailed balance equation is:\n$$\n\\text{LHS} \\;=\\; \\pi(x)\\,q(x' \\mid x)\\,\\alpha(x \\to x') \\;=\\; \\pi(x)\\,q(x' \\mid x)\\,R(x, x')\n$$\nSubstituting the definition of $R(x, x')$:\n$$\n\\text{LHS} \\;=\\; \\pi(x)\\,q(x' \\mid x)\\,\\left(\\frac{\\pi(x')\\, q(x \\mid x')}{\\pi(x)\\, q(x' \\mid x)}\\right) \\;=\\; \\pi(x')\\,q(x \\mid x')\n$$\nThe RHS is:\n$$\n\\text{RHS} \\;=\\; \\pi(x')\\,q(x \\mid x')\\,(1) \\;=\\; \\pi(x')\\,q(x \\mid x')\n$$\nThus, LHS = RHS, and detailed balance is satisfied.\n\nSince detailed balance holds in all cases for $x' \\ne x$, the Markov chain generated by this algorithm has the stationary distribution $\\pi(x) \\propto \\exp(-\\beta U(x))$. The specific forms of $q(x^\\dagger \\mid x)$ (a Gaussian based on an Euler-Maruyama step) and $\\pi(x)$ (a Boltzmann distribution) are correctly handled by the generic mechanism of the Metropolis-Hastings acceptance rule.\n\nThis construction is necessary because a naive numerical discretization of a stochastic differential equation with multiplicative noise, such as the Euler-Maruyama scheme used for the proposal, generally does not preserve the correct stationary distribution for a finite time step $\\Delta t$. The error is of order $\\Delta t$. More sophisticated schemes, such as those employing midpoint evaluations (related to the Stratonovich interpretation or Fixman's potential), can reduce the systematic error in the stationary distribution but still do not eliminate it for finite $\\Delta t$. The Metropolis-Hastings correction step, by contrast, enforces the detailed balance condition exactly, thereby ensuring that the resulting Markov chain samples from the true equilibrium distribution $\\pi(x)$, irrespective of the size of $\\Delta t$. The cost of this exactness is a potential reduction in the acceptance rate if $\\Delta t$ is too large, and the introduction of correlations as the algorithm produces a Markov chain, not a true dynamical trajectory.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import integrate\n\ndef solve():\n    \"\"\"\n    Main solver function that executes all test cases and prints the final results.\n    \"\"\"\n\n    test_cases = [\n        {\n            \"name\": \"Test 1\",\n            \"U\": lambda x: 0.5 * x**2,\n            \"U_prime\": lambda x: x,\n            \"m\": lambda x: 1.0 + 0.5 * x**2,\n            \"m_prime\": lambda x: x,\n            \"dt\": 0.5,\n            \"N_steps\": 150000,\n            \"N_burn\": 30000,\n            \"N_thin\": 10,\n            \"seed\": 12345,\n            \"tol_mean\": 2e-2,\n            \"tol_moment2\": 3e-2,\n        },\n        {\n            \"name\": \"Test 2\",\n            \"U\": lambda x: 0.5 * x**2,\n            \"U_prime\": lambda x: x,\n            \"m\": lambda x: np.exp(0.8 * x),\n            \"m_prime\": lambda x: 0.8 * np.exp(0.8 * x),\n            \"dt\": 0.25,\n            \"N_steps\": 160000,\n            \"N_burn\": 30000,\n            \"N_thin\": 10,\n            \"seed\": 24680,\n            \"tol_mean\": 2e-2,\n            \"tol_moment2\": 4e-2,\n        },\n        {\n            \"name\": \"Test 3\",\n            \"U\": lambda x: 0.25 * x**4 + 0.5 * x**2,\n            \"U_prime\": lambda x: x**3 + x,\n            \"m\": lambda x: 1.0 + 0.5 * np.sin(x),\n            \"m_prime\": lambda x: 0.5 * np.cos(x),\n            \"dt\": 0.20,\n            \"N_steps\": 160000,\n            \"N_burn\": 30000,\n            \"N_thin\": 10,\n            \"seed\": 11223,\n            \"tol_mean\": 2e-2,\n            \"tol_moment2\": 4e-2,\n        },\n    ]\n\n    all_results = []\n    for params in test_cases:\n        mean_ok, moment2_ok = run_test_case(params)\n        all_results.extend([mean_ok, moment2_ok])\n\n    # Final print statement in the exact required format.\n    # The problem specifies a list of booleans, so str(bool) gives 'True'/'False'.\n    print(f\"[{','.join(str(r) for r in all_results)}]\")\n\n\ndef run_test_case(params):\n    \"\"\"\n    Runs a single test case: simulation, analysis, and comparison.\n    \"\"\"\n    beta = 1.0  # As per problem statement, dimensionleess units beta=1\n\n    # Get exact moments\n    exact_mean, exact_moment2 = get_exact_moments(params[\"U\"])\n\n    # Run simulation\n    samples = simulate(\n        U=params[\"U\"],\n        U_prime=params[\"U_prime\"],\n        m=params[\"m\"],\n        m_prime=params[\"m_prime\"],\n        beta=beta,\n        dt=params[\"dt\"],\n        N_steps=params[\"N_steps\"],\n        N_burn=params[\"N_burn\"],\n        N_thin=params[\"N_thin\"],\n        seed=params[\"seed\"],\n    )\n\n    # Calculate estimated moments\n    est_mean = np.mean(samples)\n    est_moment2 = np.mean(samples**2)\n\n    # Compare and check tolerances\n    mean_ok = abs(est_mean - exact_mean) = params[\"tol_mean\"]\n    moment2_ok = abs(est_moment2 - exact_moment2) = params[\"tol_moment2\"]\n\n    return mean_ok, moment2_ok\n\n\ndef get_exact_moments(U):\n    \"\"\"\n    Computes exact first and second moments by numerical quadrature.\n    \"\"\"\n    # For symmetric potentials U(x) = U(-x), the mean E[X] is zero.\n    # All potentials in the test suite are symmetric.\n    exact_mean = 0.0\n\n    # For U(x) = 0.5*x^2 (standard normal), E[X^2] = 1.\n    if callable(U) and U(1.0) == 0.5 and U(2.0) == 2.0: # A simple check for U(x)=0.5x^2\n        return 0.0, 1.0\n\n    # For other potentials, use numerical quadrature.\n    # The integration range [-20, 20] is sufficient as integrands decay rapidly.\n    integration_range = [-20, 20]\n    \n    # Integrand for the normalization constant Z\n    integrand_Z = lambda x: np.exp(-U(x))\n    Z, _ = integrate.quad(integrand_Z, *integration_range)\n\n    # Integrand for the second moment numerator\n    integrand_M2 = lambda x: x**2 * np.exp(-U(x))\n    M2_num, _ = integrate.quad(integrand_M2, *integration_range)\n\n    exact_moment2 = M2_num / Z\n    \n    return exact_mean, exact_moment2\n\n\ndef simulate(U, U_prime, m, m_prime, beta, dt, N_steps, N_burn, N_thin, seed):\n    \"\"\"\n    Performs the Metropolis-corrected Langevin simulation.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    x = 0.0  # Start simulation at x=0\n    samples = []\n\n    for i in range(N_steps):\n        # Current state values\n        m_x = m(x)\n        Up_x = U_prime(x)\n        mp_x = m_prime(x)\n        U_x = U(x)\n\n        # Proposal generation (Euler-Maruyama step)\n        a_x = -m_x * Up_x + (1.0 / beta) * mp_x\n        s2_x = 2.0 * (1.0 / beta) * m_x * dt\n        \n        mu_proposal = x + a_x * dt\n        # Propose a new state x_prop\n        x_prop = mu_proposal + rng.standard_normal() * np.sqrt(s2_x)\n\n        # Proposed state values\n        m_prop = m(x_prop)\n        Up_prop = U_prime(x_prop)\n        mp_prop = m_prime(x_prop)\n        U_prop = U(x_prop)\n\n        # Calculate quantities for the backward proposal\n        a_prop = -m_prop * Up_prop + (1.0 / beta) * mp_prop\n        s2_prop = 2.0 * (1.0 / beta) * m_prop * dt\n        \n        # Calculate log of proposal probabilities (ignoring constant terms)\n        # log q(x_prop | x)\n        log_q_forward = -0.5 * np.log(s2_x) - (x_prop - mu_proposal)**2 / (2 * s2_x)\n        \n        # log q(x | x_prop)\n        mu_backward = x_prop + a_prop * dt\n        log_q_backward = -0.5 * np.log(s2_prop) - (x - mu_backward)**2 / (2 * s2_prop)\n\n        # Calculate log of acceptance ratio\n        log_pi_ratio = -beta * (U_prop - U_x)\n        log_R = log_pi_ratio + log_q_backward - log_q_forward\n        \n        # Metropolis-Hastings acceptance step\n        if log_R >= 0 or rng.uniform()  np.exp(log_R):\n            x = x_prop\n\n        # Store sample after burn-in and thinning\n        if i >= N_burn and (i - N_burn) % N_thin == 0:\n            samples.append(x)\n    \n    return np.array(samples)\n\n\nif __name__ == '__main__':\n    solve()\n```", "id": "2815978"}]}