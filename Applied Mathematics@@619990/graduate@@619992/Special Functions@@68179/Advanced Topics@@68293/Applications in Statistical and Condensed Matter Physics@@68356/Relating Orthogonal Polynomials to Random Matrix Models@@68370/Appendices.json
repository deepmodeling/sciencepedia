{"hands_on_practices": [{"introduction": "The most direct link between orthogonal polynomials and random matrix ensembles lies in their shared recurrence relations. The coefficients of this relation, such as $\\alpha_n$ and $\\beta_n$, are not arbitrary; they directly encode the moments of the ensemble's eigenvalue distribution. This exercise [@problem_id:751148] provides a clear illustration of this principle using the discrete Krawtchouk ensemble, where you will use the first two recursion coefficients to determine a fundamental parameter of the underlying binomial weight function.", "problem": "In the theory of random matrices, the Krawtchouk orthogonal polynomial ensemble describes a system where eigenvalues are located at discrete positions $x \\in \\{0, 1, \\dots, M\\}$. The probability of an eigenvalue at position $x$ is governed by the binomial distribution:\n$$\nw(x; M, p) = \\binom{M}{x} p^x (1-p)^{M-x}\n$$\nHere, $M$ is a positive integer defining the lattice size, and $p \\in [0, 1]$ is a probability parameter.\n\nThe monic orthogonal polynomials, $\\hat{K}_n(x)$, associated with this weight function satisfy a three-term recurrence relation. For $n \\ge 0$, this relation can be written as:\n$$\nx \\hat{K}_n(x) = \\hat{K}_{n+1}(x) + \\alpha_n \\hat{K}_n(x) + \\beta_n \\hat{K}_{n-1}(x)\n$$\nwith initial conditions $\\hat{K}_0(x)=1$ and $\\hat{K}_{-1}(x)=0$, and where $\\beta_0$ is conventionally set to equal the total mass of the distribution, $\\sum_{x=0}^M w(x)$. The coefficients $\\alpha_n$ and $\\beta_n$ are known as the recursion coefficients.\n\nIt is a fundamental property in the theory of orthogonal polynomials that the first recursion coefficient, $\\alpha_0$, is equal to the mean of the probability distribution $w(x)$, and the coefficient $\\beta_1$ is equal to its variance.\n\nFor a specific Krawtchouk ensemble, the first two non-trivial recursion coefficients are measured to be $\\alpha_0$ and $\\beta_1$. Assuming that the parameters $M$ and $p$ are such that $p \\in (0,1)$ and $\\alpha_0 \\neq 0$, determine the probability parameter $p$ as a function of $\\alpha_0$ and $\\beta_1$.", "solution": "1. For the binomial weight $w(x;M,p)$, the mean is given by $\\mathbb{E}[X] = Mp$. This must be equal to the first recursion coefficient, so we have\n$$\\alpha_0 = \\mathbb{E}[X] = Mp.$$\n2. The variance is given by $\\mathrm{Var}(X) = Mp(1-p)$. This is equal to the coefficient $\\beta_1$, so\n$$\\beta_1 = \\mathrm{Var}(X) = Mp(1-p).$$\n3. Substituting the expression for $\\alpha_0$ from the first step into the equation for $\\beta_1$ gives\n$$\n\\beta_1 = \\alpha_0\\,(1-p)\n\\;\\Longrightarrow\\;\n1-p = \\frac{\\beta_1}{\\alpha_0}\n\\;\\Longrightarrow\\;\np = 1 - \\frac{\\beta_1}{\\alpha_0}\\,. \n$$", "answer": "$$\\boxed{1 - \\frac{\\beta_1}{\\alpha_0}}$$", "id": "751148"}, {"introduction": "A cornerstone of random matrix theory is the remarkable result that the eigenvalue statistics of large, dense random matrices are identical to those of a much simpler tridiagonal matrix. This practice [@problem_id:751082] brings this powerful theorem to life by focusing on the Gaussian Orthogonal Ensemble (GOE). You will calculate an eigenvalue statistic for a $2 \\times 2$ system using its tridiagonal representation, gaining insight into how this simplification, which is rooted in the three-term recurrence of orthogonal polynomials, makes complex problems tractable.", "problem": "In random matrix theory, the Gaussian Orthogonal Ensemble (GOE) consists of real symmetric matrices $H$ of size $N \\times N$, whose probability distribution is given by\n$$\nP(H) \\propto \\exp\\left(-\\frac{1}{4\\sigma^2}\\text{Tr}(H^2)\\right),\n$$\nwhere $\\sigma$ is a positive real parameter related to the variance of the matrix elements. This distribution implies that the diagonal elements $H_{ii}$ are independent random variables drawn from a normal distribution $N(0, 2\\sigma^2)$, and the off-diagonal elements $H_{ij}$ for $i<j$ are independent random variables drawn from $N(0, \\sigma^2)$.\n\nA fundamental result states that the set of eigenvalues of any such matrix $H$ is statistically identical to the set of eigenvalues of a much simpler tridiagonal symmetric matrix $J$, also known as a Jacobi matrix. The non-zero elements of this $N \\times N$ matrix $J$ are $J_{ii} = a_i$ and $J_{i, i+1} = J_{i+1, i} = b_i$ for $i=1, \\dots, N-1$. The random variables $a_i$ and $b_i$ have the following distributions:\n1.  The diagonal elements $a_i$ for $i=1, \\dots, N$ are independent and identically distributed (i.i.d.) normal random variables, $a_i \\sim N(0, 2\\sigma^2)$.\n2.  The off-diagonal elements $b_i$ for $i=1, \\dots, N-1$ are independent positive random variables such that their squares, $b_i^2$, are distributed as $\\sigma^2 \\chi^2_{N-i}$, where $\\chi^2_k$ denotes the chi-squared distribution with $k$ degrees of freedom.\n\nConsider the simplest non-trivial case where $N=2$. The corresponding random Jacobi matrix is:\n$$\nJ = \\begin{pmatrix}\na_1 & b_1 \\\\\nb_1 & a_2\n\\end{pmatrix}\n$$\nwhere the elements $a_1, a_2$ are i.i.d. draws from $N(0, 2\\sigma^2)$, and $b_1^2$ is drawn from a $\\sigma^2 \\chi^2_1$ distribution.\n\nYour task is to calculate the expectation value of the smallest eigenvalue of this $2 \\times 2$ random matrix $J$. Express your answer in terms of the parameter $\\sigma$ and fundamental mathematical constants.", "solution": "1. The eigenvalues of the matrix\n$$J = \\begin{pmatrix}a_1 & b_1\\\\ b_1 & a_2\\end{pmatrix}$$\nare given by the characteristic equation, yielding\n$$\\lambda_{\\pm} = \\frac{a_1 + a_2}{2} \\pm \\sqrt{\\left(\\frac{a_1 - a_2}{2}\\right)^2 + b_1^2}.$$\nThe smallest eigenvalue is $\\lambda_-$.\n2. We define new random variables:\n$$X = \\frac{a_1 + a_2}{2}, \\quad Y = \\frac{a_1 - a_2}{2}.$$\nSince $a_1, a_2 \\sim N(0, 2\\sigma^2)$ are independent, $X$ and $Y$ are independent normal variables with mean 0 and variance $\\sigma^2$. So, $X,Y \\sim N(0,\\sigma^2)$. The term $b_1^2 \\sim \\sigma^2 \\chi_1^2$, which is the distribution of $(\\sigma Z)^2$ where $Z \\sim N(0,1)$. Since $b_1$ is positive, we can write $b_1 = \\sigma |Z|$. The smallest eigenvalue is\n$$\\lambda_- = X - \\sqrt{Y^2 + b_1^2} = X - \\sqrt{Y^2 + \\sigma^2 Z^2}.$$\n3. By linearity of expectation, $\\mathbb{E}[\\lambda_-] = \\mathbb{E}[X] - \\mathbb{E}\\bigl[\\sqrt{Y^2 + \\sigma^2 Z^2}\\bigr]$. Since $X \\sim N(0, \\sigma^2)$, we have $\\mathbb{E}[X]=0$. So,\n$$\\mathbb{E}[\\lambda_-] = -\\mathbb{E}\\bigl[\\sqrt{Y^2 + \\sigma^2 Z^2}\\bigr].$$\n4. Since $Y \\sim N(0,\\sigma^2)$ and $\\sigma Z \\sim N(0,\\sigma^2)$ are independent, we can write $Y=\\sigma W_1$ and $\\sigma Z=\\sigma W_2$ where $W_1, W_2 \\sim N(0,1)$ are independent standard normal variables. The expectation becomes\n$$ \\mathbb{E}\\bigl[\\sqrt{Y^2 + \\sigma^2 Z^2}\\bigr] = \\mathbb{E}\\bigl[\\sqrt{(\\sigma W_1)^2 + (\\sigma W_2)^2}\\bigr] = \\sigma\\,\\mathbb{E}\\bigl[\\sqrt{W_1^2 + W_2^2}\\bigr].$$\nThe random variable $R = \\sqrt{W_1^2 + W_2^2}$ follows a Rayleigh distribution with scale parameter 1. The mean of this distribution is $\\sqrt{\\pi/2}$. Thus, the expectation is $\\sigma \\sqrt{\\frac{\\pi}{2}}$.\n5. Combining the results, the expected value of the smallest eigenvalue is\n$$\\mathbb{E}[\\lambda_-] = -\\sigma\\sqrt{\\frac{\\pi}{2}}.$$", "answer": "$$\\boxed{-\\sigma\\sqrt{\\frac{\\pi}{2}}}$$", "id": "751082"}, {"introduction": "While the orthogonal polynomial formalism is essential for the deep theory of random matrix ensembles, it is also crucial to recognize when alternative methods offer a more direct path to a solution. This problem [@problem_id:751057] explores the calculation of a symmetric polynomial of eigenvalues for the Gaussian Unitary Ensemble (GUE). It demonstrates a powerful technique that relies on matrix traces, providing a valuable complement to methods based on correlation functions and offering a more complete problem-solving toolkit.", "problem": "In the study of complex quantum systems, the statistical properties of energy levels can be modeled by the eigenvalues of random matrices. The Gaussian Unitary Ensemble (GUE) is a set of $N \\times N$ Hermitian matrices $H$ whose entries are independent complex Gaussian random variables (up to the Hermitian symmetry). The joint probability density function (JPDF) of the eigenvalues $\\{\\lambda_1, \\dots, \\lambda_N\\}$ of a GUE matrix is given by\n$$\nP(\\lambda_1, \\dots, \\lambda_N) = C_N \\prod_{1 \\le i < j \\le N} (\\lambda_i - \\lambda_j)^2 \\exp\\left(-\\frac{1}{2\\sigma^2} \\sum_{k=1}^N \\lambda_k^2\\right),\n$$\nwhere $C_N$ is a normalization constant and $\\sigma^2$ is a parameter related to the variance of the matrix elements.\n\nThe statistical properties of the eigenvalues can be analyzed using $k$-point correlation functions $\\rho_k(\\lambda_1, \\dots, \\lambda_k)$. The expectation of a symmetric function $f(\\lambda_1, \\dots, \\lambda_N)$ can be computed from these correlation functions. For this problem, we are interested in the elementary symmetric polynomial of degree two, $e_2(\\lambda_1, \\dots, \\lambda_N) = \\sum_{1 \\le i < j \\le N} \\lambda_i \\lambda_j$. Its expectation value is given by\n$$\n\\mathbb{E}[e_2] = \\frac{1}{2} \\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} xy \\, \\rho_2(x,y) \\, dx \\, dy.\n$$\nA powerful result in random matrix theory states that the correlation functions can be expressed via a kernel built from orthonormal polynomials. For the GUE, the relevant weight function is $w(x) = \\exp(-x^2/(2\\sigma^2))$. Let $\\{\\psi_n(x)\\}_{n=0}^\\infty$ be the sequence of orthonormal polynomials with respect to this weight. The kernel is $K_N(x,y) = \\sum_{n=0}^{N-1} \\psi_n(x) \\psi_n(y)$. The two-point correlation function is given by the formula:\n$$\n\\rho_2(x,y) = K_N(x,x) K_N(y,y) - K_N(x,y)^2.\n$$\nThe orthonormal polynomials $\\psi_n(x)$ satisfy the three-term recurrence relation:\n$$\nx\\psi_n(x) = a_{n+1} \\psi_{n+1}(x) + a_n \\psi_{n-1}(x),\n$$\nwhere the coefficients for the weight $w(x) = \\exp(-x^2/(2\\sigma^2))$ are $a_n = \\sqrt{n\\sigma^2}$.\n\nUsing this framework, calculate the expected value of the sum of products of eigenvalues taken two at a time, $\\mathbb{E}\\left[\\sum_{1 \\le i < j \\le 3} \\lambda_i \\lambda_j\\right]$, for a $3 \\times 3$ GUE matrix (i.e., $N=3$). Express your answer in terms of the parameter $\\sigma$.", "solution": "1. Define the power sum symmetric polynomials $p_1=\\sum_{i=1}^3\\lambda_i$ and $p_2=\\sum_{i=1}^3\\lambda_i^2$. The elementary symmetric polynomial $e_2 = \\sum_{1 \\le i  j \\le 3} \\lambda_i \\lambda_j$ is related to them by Newton's sums:\n$$p_1^2=\\left(\\sum_i\\lambda_i\\right)^2=\\sum_i\\lambda_i^2+2\\sum_{ij}\\lambda_i\\lambda_j = p_2+2e_2.$$\nSolving for $e_2$ gives\n$$e_2=\\frac{p_1^2-p_2}{2}.$$\n2. The power sums of eigenvalues are equal to the trace of the powers of the matrix, i.e., $p_1=\\text{Tr}\\,H$ and $p_2=\\text{Tr}\\,H^2$. We need to compute the expectation of these traces over the GUE measure $P(H)\\propto\\exp\\!\\Bigl(-\\frac{1}{2\\sigma^2}\\text{Tr}\\,H^2\\Bigr)$. This measure implies that for the matrix elements, $\\mathbb{E}[H_{ii}] = 0$, $\\mathbb{E}[H_{ii}^2]=\\sigma^2$, and for $i \\ne j$, $\\mathbb{E}[H_{ij}]=0$ and $\\mathbb{E}[|H_{ij}|^2]=\\sigma^2$.\n3. We compute the required expectations. For the trace of $H$: $\\mathbb{E}[p_1]=\\mathbb{E}[\\text{Tr}\\,H]=\\sum_i \\mathbb{E}[H_{ii}]=0$. For its square:\n$$\\mathbb{E}[p_1^2]=\\mathrm{Var}(\\text{Tr}\\,H) = \\mathrm{Var}\\left(\\sum_i H_{ii}\\right) = \\sum_i \\mathrm{Var}(H_{ii}) = 3\\sigma^2.$$\nFor the trace of $H^2$:\n$$\\mathbb{E}[p_2]=\\mathbb{E}[\\text{Tr}\\,H^2]=\\mathbb{E}\\left[\\sum_{i,j=1}^3 H_{ij}H_{ji}\\right]=\\sum_{i,j}\\mathbb{E}[|H_{ij}|^2] = N^2\\sigma^2 = 9\\sigma^2.$$\n4. Now we can compute the expectation of $e_2$:\n$$\\mathbb{E}[e_2] = \\mathbb{E}\\left[\\frac{p_1^2-p_2}{2}\\right] = \\frac{\\mathbb{E}[p_1^2]-\\mathbb{E}[p_2]}{2} = \\frac{3\\sigma^2-9\\sigma^2}{2} = -3\\sigma^2.$$", "answer": "$$\\boxed{-3\\sigma^2}$$", "id": "751057"}]}