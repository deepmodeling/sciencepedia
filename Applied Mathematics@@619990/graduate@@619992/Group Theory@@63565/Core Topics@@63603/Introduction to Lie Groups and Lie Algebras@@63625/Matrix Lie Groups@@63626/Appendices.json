{"hands_on_practices": [{"introduction": "A Lie algebra is the foundational structure associated with a Lie group, representing its infinitesimal transformations. The essential operation that defines a Lie algebra's structure is not standard matrix multiplication, but the Lie bracket, or commutator, defined as $[X, Y] = XY - YX$. This practice exercise [@problem_id:1629893] offers a straightforward and essential opportunity to compute a Lie bracket for two elements of the special linear algebra $\\mathfrak{sl}(2, \\mathbb{C})$, reinforcing the fundamental algebraic skill required for exploring these symmetries.", "problem": "Consider the Lie algebra $\\mathfrak{sl}(2, \\mathbb{C})$, which is the set of all $2 \\times 2$ matrices with complex entries and a trace of zero. For any two matrices $X, Y \\in \\mathfrak{sl}(2, \\mathbb{C})$, the Lie bracket is defined as the matrix commutator $[X, Y] = XY - YX$.\n\nLet the matrices $A$ and $B$ be two specific elements of $\\mathfrak{sl}(2, \\mathbb{C})$, given by:\n$$ A = \\begin{pmatrix} i & 2 \\\\ 1 & -i \\end{pmatrix} $$\n$$ B = \\begin{pmatrix} 0 & 3 \\\\ -3 & 0 \\end{pmatrix} $$\nwhere $i$ is the imaginary unit satisfying $i^{2} = -1$.\n\nYour task is to compute the Lie bracket $[A, B]$. Express your final answer as a $2 \\times 2$ matrix.", "solution": "We use the Lie bracket definition for matrices, $[A,B]=AB-BA$, with $A=\\begin{pmatrix} i & 2 \\\\ 1 & -i \\end{pmatrix}$ and $B=\\begin{pmatrix} 0 & 3 \\\\ -3 & 0 \\end{pmatrix}$.\n\nFirst compute $AB$ by standard matrix multiplication:\n$$\nAB=\\begin{pmatrix} i & 2 \\\\ 1 & -i \\end{pmatrix}\\begin{pmatrix} 0 & 3 \\\\ -3 & 0 \\end{pmatrix}\n=\\begin{pmatrix}\ni\\cdot 0 + 2\\cdot(-3) & i\\cdot 3 + 2\\cdot 0 \\\\\n1\\cdot 0 + (-i)\\cdot(-3) & 1\\cdot 3 + (-i)\\cdot 0\n\\end{pmatrix}\n=\\begin{pmatrix}\n-6 & 3i \\\\\n3i & 3\n\\end{pmatrix}.\n$$\n\nNext compute $BA$:\n$$\nBA=\\begin{pmatrix} 0 & 3 \\\\ -3 & 0 \\end{pmatrix}\\begin{pmatrix} i & 2 \\\\ 1 & -i \\end{pmatrix}\n=\\begin{pmatrix}\n0\\cdot i + 3\\cdot 1 & 0\\cdot 2 + 3\\cdot(-i) \\\\\n(-3)\\cdot i + 0\\cdot 1 & (-3)\\cdot 2 + 0\\cdot(-i)\n\\end{pmatrix}\n=\\begin{pmatrix}\n3 & -3i \\\\\n-3i & -6\n\\end{pmatrix}.\n$$\n\nTherefore,\n$$\n[A,B]=AB-BA=\\begin{pmatrix}\n-6-3 & 3i-(-3i) \\\\\n3i-(-3i) & 3-(-6)\n\\end{pmatrix}\n=\\begin{pmatrix}\n-9 & 6i \\\\\n6i & 9\n\\end{pmatrix}.\n$$\n\nThis result has trace $-9+9=0$, consistent with $[A,B]\\in\\mathfrak{sl}(2,\\mathbb{C})$.", "answer": "$$\\boxed{\\begin{pmatrix}-9 & 6i \\\\ 6i & 9\\end{pmatrix}}$$", "id": "1629893"}, {"introduction": "The exponential map is the crucial bridge connecting a Lie algebra to its corresponding Lie group, transforming an infinitesimal generator into a finite group transformation. This process is fundamental to understanding how continuous symmetries are constructed. In this exercise [@problem_id:1629845], you will calculate the matrix exponential for an element of a Lie algebra that generates both rotations and scaling, providing a concrete example of how parameters in the algebra directly map to the geometric properties of the resulting group element.", "problem": "In the study of continuous symmetries, the matrix exponential provides a bridge from a Lie algebra, which is a vector space of infinitesimal generators, to a Lie group, which is a group of transformations. This problem explores the exponentiation of a matrix that generates both rotations and uniform scaling in a two-dimensional plane.\n\nConsider the Lie algebra consisting of all $2 \\times 2$ real matrices of the form:\n$$X = \\begin{pmatrix} \\alpha & \\beta \\\\ -\\beta & \\alpha \\end{pmatrix}$$\nwhere $\\alpha$ and $\\beta$ are arbitrary real numbers.\n\nYour task is to compute the matrix exponential $G = e^X$. The resulting matrix $G$ will be an element of the corresponding Lie group of similarity transformations in the plane. Express your final answer as a $2 \\times 2$ matrix whose entries are functions of $\\alpha$ and $\\beta$.", "solution": "The matrix exponential $e^X$ is defined by its Taylor series expansion:\n$$e^X = \\sum_{k=0}^{\\infty} \\frac{X^k}{k!} = I + X + \\frac{X^2}{2!} + \\frac{X^3}{3!} + \\dots$$\nTo compute this for the given matrix $X$, we can decompose $X$ into two simpler, commuting parts. Let's write $X$ as a sum of a scalar matrix and a skew-symmetric matrix:\n$$X = \\begin{pmatrix} \\alpha & 0 \\\\ 0 & \\alpha \\end{pmatrix} + \\begin{pmatrix} 0 & \\beta \\\\ -\\beta & 0 \\end{pmatrix}$$\nWe can factor out the scalar values $\\alpha$ and $\\beta$:\n$$X = \\alpha \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} + \\beta \\begin{pmatrix} 0 & 1 \\\\ -1 & 0 \\end{pmatrix}$$\nLet's define $A = \\alpha I$ and $B = \\beta J$, where $I = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix}$ is the identity matrix and $J = \\begin{pmatrix} 0 & 1 \\\\ -1 & 0 \\end{pmatrix}$. Thus, $X = A + B$.\n\nNext, we check if $A$ and $B$ commute.\n$$AB = (\\alpha I)(\\beta J) = \\alpha \\beta (IJ) = \\alpha \\beta J$$\n$$BA = (\\beta J)(\\alpha I) = \\beta \\alpha (JI) = \\alpha \\beta J$$\nSince $AB = BA$, the matrices commute. For commuting matrices, the exponential of their sum is the product of their exponentials: $e^{A+B} = e^A e^B$. We can compute $e^A$ and $e^B$ separately.\n\nFirst, let's compute $e^A = e^{\\alpha I}$:\n$$e^{\\alpha I} = \\sum_{k=0}^{\\infty} \\frac{(\\alpha I)^k}{k!} = \\sum_{k=0}^{\\infty} \\frac{\\alpha^k I^k}{k!}$$\nSince $I^k = I$ for any $k \\ge 1$ and $I^0=I$, we can factor out $I$:\n$$e^{\\alpha I} = I \\left( \\sum_{k=0}^{\\infty} \\frac{\\alpha^k}{k!} \\right) = e^{\\alpha} I = \\begin{pmatrix} e^{\\alpha} & 0 \\\\ 0 & e^{\\alpha} \\end{pmatrix}$$\n\nSecond, let's compute $e^B = e^{\\beta J}$. We need the powers of $J$:\n$J^0 = I = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix}$\n$J^1 = J = \\begin{pmatrix} 0 & 1 \\\\ -1 & 0 \\end{pmatrix}$\n$J^2 = \\begin{pmatrix} 0 & 1 \\\\ -1 & 0 \\end{pmatrix} \\begin{pmatrix} 0 & 1 \\\\ -1 & 0 \\end{pmatrix} = \\begin{pmatrix} -1 & 0 \\\\ 0 & -1 \\end{pmatrix} = -I$\n$J^3 = J^2 J = (-I)J = -J$\n$J^4 = J^2 J^2 = (-I)(-I) = I$\nThe powers of $J$ are periodic with a period of 4, similar to the imaginary unit $i$ in complex numbers.\n\nNow we can write out the series for $e^{\\beta J}$:\n$$e^{\\beta J} = \\sum_{k=0}^{\\infty} \\frac{(\\beta J)^k}{k!} = \\frac{\\beta^0 J^0}{0!} + \\frac{\\beta^1 J^1}{1!} + \\frac{\\beta^2 J^2}{2!} + \\frac{\\beta^3 J^3}{3!} + \\dots$$\n$$e^{\\beta J} = I + \\beta J - \\frac{\\beta^2}{2!}I - \\frac{\\beta^3}{3!}J + \\frac{\\beta^4}{4!}I + \\dots$$\nLet's group the terms with $I$ and the terms with $J$:\n$$e^{\\beta J} = \\left(1 - \\frac{\\beta^2}{2!} + \\frac{\\beta^4}{4!} - \\dots \\right)I + \\left(\\beta - \\frac{\\beta^3}{3!} + \\frac{\\beta^5}{5!} - \\dots \\right)J$$\nWe recognize the Taylor series for cosine and sine:\n$$\\cos(\\beta) = 1 - \\frac{\\beta^2}{2!} + \\frac{\\beta^4}{4!} - \\dots$$\n$$\\sin(\\beta) = \\beta - \\frac{\\beta^3}{3!} + \\frac{\\beta^5}{5!} - \\dots$$\nSo, we can write:\n$$e^{\\beta J} = \\cos(\\beta) I + \\sin(\\beta) J$$\nSubstituting the matrices for $I$ and $J$:\n$$e^{\\beta J} = \\cos(\\beta) \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} + \\sin(\\beta) \\begin{pmatrix} 0 & 1 \\\\ -1 & 0 \\end{pmatrix} = \\begin{pmatrix} \\cos(\\beta) & \\sin(\\beta) \\\\ -\\sin(\\beta) & \\cos(\\beta) \\end{pmatrix}$$\n\nFinally, we find $e^X$ by multiplying $e^A$ and $e^B$:\n$$e^X = e^A e^B = \\begin{pmatrix} e^{\\alpha} & 0 \\\\ 0 & e^{\\alpha} \\end{pmatrix} \\begin{pmatrix} \\cos(\\beta) & \\sin(\\beta) \\\\ -\\sin(\\beta) & \\cos(\\beta) \\end{pmatrix}$$\n$$e^X = \\begin{pmatrix} e^{\\alpha}\\cos(\\beta) & e^{\\alpha}\\sin(\\beta) \\\\ -e^{\\alpha}\\sin(\\beta) & e^{\\alpha}\\cos(\\beta) \\end{pmatrix}$$\nThis is the final matrix $G$. It represents a rotation by angle $\\beta$ and a uniform scaling by a factor of $e^{\\alpha}$.", "answer": "$$\\boxed{\\begin{pmatrix} \\exp(\\alpha) \\cos(\\beta) & \\exp(\\alpha) \\sin(\\beta) \\\\ -\\exp(\\alpha) \\sin(\\beta) & \\exp(\\alpha) \\cos(\\beta) \\end{pmatrix}}$$", "id": "1629845"}, {"introduction": "A key question in the theory of symmetry is determining whether a set of transformations that preserves a certain structure—like a geometric shape or a physical law—forms a Lie group. This requires checking for both algebraic closure (it's a subgroup) and a smooth geometric structure (it's a submanifold). This problem [@problem_id:1629885] challenges you to investigate a set of matrices defined by their preservation of a degenerate bilinear form, guiding you through the process of deriving the matrix constraints and verifying if they indeed constitute a matrix Lie group.", "problem": "In the study of symmetries, a group of matrices that preserves a certain structure, such as a bilinear form, is known as an automorphism group. A central question is whether such a group also possesses the geometric structure of a smooth manifold, making it a matrix Lie group.\n\nConsider the vector space $\\mathbb{R}^2$. Let a degenerate bilinear form $B: \\mathbb{R}^2 \\times \\mathbb{R}^2 \\to \\mathbb{R}$ be defined as $B(\\mathbf{v}, \\mathbf{w}) = v_1 w_1$, where $\\mathbf{v} = (v_1, v_2)^T$ and $\\mathbf{w} = (w_1, w_2)^T$.\n\nLet $G$ be the set of all invertible $2 \\times 2$ real matrices $A$ that preserve this bilinear form, i.e., $B(A\\mathbf{v}, A\\mathbf{w}) = B(\\mathbf{v}, \\mathbf{w})$ for all vectors $\\mathbf{v}, \\mathbf{w} \\in \\mathbb{R}^2$. A set of matrices is defined as a matrix Lie group if it is a subgroup of the General Linear Group of degree n over the real numbers ($GL(n, \\mathbb{R})$) and is also a smooth submanifold of $GL(n, \\mathbb{R})$.\n\nWhich of the following statements accurately describes the set $G$?\n\nA. $G$ is a subgroup of $GL(2, \\mathbb{R})$ but is not a submanifold of $GL(2, \\mathbb{R})$.\n\nB. $G$ is a submanifold of $GL(2, \\mathbb{R})$ but is not a subgroup of $GL(2, \\mathbb{R})$.\n\nC. $G$ is a matrix Lie group.\n\nD. $G$ is neither a subgroup of $GL(2, \\mathbb{R})$ nor a submanifold of $GL(2, \\mathbb{R})$.\n\nE. The set $G$ is empty.", "solution": "We encode the bilinear form by the matrix $J=\\mathrm{diag}(1,0)$ so that $B(\\mathbf{v},\\mathbf{w})=\\mathbf{v}^{T}J\\mathbf{w}$. The preservation condition is\n$$\nB(A\\mathbf{v},A\\mathbf{w})=(A\\mathbf{v})^{T}J(A\\mathbf{w})=\\mathbf{v}^{T}(A^{T}JA)\\mathbf{w}=B(\\mathbf{v},\\mathbf{w})=\\mathbf{v}^{T}J\\mathbf{w}\\quad\\text{for all }\\mathbf{v},\\mathbf{w},\n$$\nwhich is equivalent to the matrix equation\n$$\nA^{T}JA=J.\n$$\nLet $A=\\begin{pmatrix}a&b\\\\ c&d\\end{pmatrix}$. Then for arbitrary $\\mathbf{v}=(v_{1},v_{2})^{T}$ and $\\mathbf{w}=(w_{1},w_{2})^{T}$ we have\n$$\n(Av)_{1}=a v_{1}+b v_{2},\\qquad (Aw)_{1}=a w_{1}+b w_{2}.\n$$\nThe condition $B(A\\mathbf{v},A\\mathbf{w})=B(\\mathbf{v},\\mathbf{w})$ becomes\n$$\n(a v_{1}+b v_{2})(a w_{1}+b w_{2})=v_{1}w_{1}\\quad\\text{for all }v_{1},v_{2},w_{1},w_{2}.\n$$\nExpanding the left-hand side and comparing coefficients of the independent bilinear monomials $v_{i}w_{j}$ yields\n$$\na^{2}=1,\\qquad a b=0,\\qquad b^{2}=0.\n$$\nOver $\\mathbb{R}$, $b^{2}=0$ implies $b=0$, and then $a^{2}=1$ implies $a=\\pm 1$. Thus any $A\\in G$ must have the form\n$$\nA=\\begin{pmatrix}s&0\\\\ c&d\\end{pmatrix},\\qquad s\\in\\{\\pm 1\\}.\n$$\nThe invertibility condition $\\det A\\neq 0$ becomes $s d\\neq 0$, hence $d\\neq 0$.\n\nTherefore\n$$\nG=\\Bigl\\{\\begin{pmatrix}1&0\\\\ c&d\\end{pmatrix}:c\\in\\mathbb{R},\\,d\\in\\mathbb{R}\\setminus\\{0\\}\\Bigr\\}\\,\\cup\\,\\Bigl\\{\\begin{pmatrix}-1&0\\\\ c&d\\end{pmatrix}:c\\in\\mathbb{R},\\,d\\in\\mathbb{R}\\setminus\\{0\\}\\Bigr\\}.\n$$\n\nSubgroup property: Let $A_{1}=\\begin{pmatrix}s_{1}&0\\\\ c_{1}&d_{1}\\end{pmatrix}$ and $A_{2}=\\begin{pmatrix}s_{2}&0\\\\ c_{2}&d_{2}\\end{pmatrix}$ with $s_{1},s_{2}\\in\\{\\pm 1\\}$ and $d_{1},d_{2}\\neq 0$. Then\n$$\nA_{1}A_{2}=\\begin{pmatrix}s_{1}s_{2}&0\\\\ c_{1}s_{2}+d_{1}c_{2}&d_{1}d_{2}\\end{pmatrix},\n$$\nso $A_{1}A_{2}\\in G$ since $s_{1}s_{2}\\in\\{\\pm 1\\}$ and $d_{1}d_{2}\\neq 0$. The identity $\\begin{pmatrix}1&0\\\\ 0&1\\end{pmatrix}$ belongs to $G$. The inverse of $A=\\begin{pmatrix}s&0\\\\ c&d\\end{pmatrix}$ is\n$$\nA^{-1}=\\frac{1}{sd}\\begin{pmatrix}d&0\\\\ -c&s\\end{pmatrix}=\\begin{pmatrix}s&0\\\\ -\\frac{c}{sd}&\\frac{1}{d}\\end{pmatrix},\n$$\nwhich also lies in $G$ since $s=\\pm 1$ and $d\\neq 0$. Hence $G$ is a subgroup of $GL(2,\\mathbb{R})$.\n\nSubmanifold property: For each $s\\in\\{\\pm 1\\}$, define\n$$\n\\phi_{s}:\\mathbb{R}\\times(\\mathbb{R}\\setminus\\{0\\})\\to GL(2,\\mathbb{R}),\\qquad \\phi_{s}(c,d)=\\begin{pmatrix}s&0\\\\ c&d\\end{pmatrix}.\n$$\nEach $\\phi_{s}$ is a smooth embedding, with image\n$$\nG_{s}=\\Bigl\\{\\begin{pmatrix}s&0\\\\ c&d\\end{pmatrix}:c\\in\\mathbb{R},\\,d\\in\\mathbb{R}\\setminus\\{0\\}\\Bigr\\}.\n$$\nThus $G_{s}$ is a $2$-dimensional embedded submanifold of $GL(2,\\mathbb{R})$. Since $G=G_{+}\\cup G_{-}$ is a disjoint union of such embedded submanifolds, every point of $G$ has a neighborhood in $GL(2,\\mathbb{R})$ where $G$ coincides with one of the images $\\phi_{s}$, so $G$ is a smooth embedded submanifold of $GL(2,\\mathbb{R})$.\n\nConsequently, $G$ is both a subgroup and a smooth submanifold of $GL(2,\\mathbb{R})$, i.e., $G$ is a matrix Lie group. Therefore, the correct choice is C.", "answer": "$$\\boxed{C}$$", "id": "1629885"}]}