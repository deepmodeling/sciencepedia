{"hands_on_practices": [{"introduction": "Our first practice is a foundational exercise in stochastic averaging. We begin with a canonical slow-fast system where the fast variable follows an Ornstein-Uhlenbeck process, a cornerstone model for mean-reverting stochastic dynamics. This problem guides you through the essential steps of deriving the stationary distribution of the fast process and using it to compute the effective, or \"averaged,\" drift for the slow variable, illustrating how complex dynamics can be simplified by integrating out the fast fluctuations. [@problem_id:2979068]", "problem": "Consider the slow–fast system of stochastic differential equations\n$$\ndY_t^{\\epsilon} \\;=\\; -\\frac{\\lambda}{\\epsilon}\\, Y_t^{\\epsilon}\\,dt \\;+\\; \\frac{\\sigma}{\\sqrt{\\epsilon}}\\, dB_t, \n\\qquad\ndX_t^{\\epsilon} \\;=\\; a\\!\\left(X_t^{\\epsilon},\\,Y_t^{\\epsilon}\\right)\\,dt,\n$$\nwhere $B_t$ is a standard one-dimensional Brownian motion, $\\epsilon \\in (0,\\infty)$ is a small parameter, and $\\lambda>0$, $\\sigma>0$ are fixed constants. The fast component $Y_t^{\\epsilon}$ is an Ornstein–Uhlenbeck (OU) process. The slow drift is given by\n$$\na(x,y) \\;=\\; \\kappa\\,\\sin(\\omega x) \\;+\\; \\eta\\,x\\,y \\;+\\; \\gamma\\,y^{2} \\;+\\; \\rho\\,x^{2}\\,\\cos(\\beta y) \\;+\\; \\delta\\,\\exp(-x^{2})\\,y\\,\\sin(\\beta y),\n$$\nwith parameters $\\kappa,\\omega,\\eta,\\gamma,\\rho,\\beta,\\delta \\in \\mathbb{R}$.\n\nUsing the stochastic averaging principle for slow–fast systems, the effective drift for the slow variable is the averaged drift\n$$\n\\overline{a}(x) \\;=\\; \\int_{\\mathbb{R}} a(x,y)\\,\\mu(dy),\n$$\nwhere $\\mu$ is the unique invariant measure of the fast OU process $Y_t^{\\epsilon}$. Derive the invariant Gaussian law $\\mu$ for $Y_t^{\\epsilon}$ from first principles, and compute the closed-form expression for $\\overline{a}(x)$ as a function of $x$ and the parameters $\\kappa,\\omega,\\eta,\\gamma,\\rho,\\beta,\\delta,\\lambda,\\sigma$. Express your final answer as a single analytic expression. No numerical rounding is required, and no physical units are involved. The final answer must be a single closed-form expression for $\\overline{a}(x)$.", "solution": "The problem is validated as scientifically grounded, well-posed, and objective. It is a standard exercise in the application of the stochastic averaging principle for slow-fast systems.\n\nThe solution process consists of two main parts: first, deriving the invariant measure $\\mu$ for the fast process $Y_t^{\\epsilon}$, and second, computing the averaged drift $\\overline{a}(x)$ by integrating the slow drift function $a(x,y)$ with respect to this measure.\n\nPart 1: Derivation of the Invariant Measure $\\mu$\n\nThe fast component of the system is described by the stochastic differential equation (SDE) for an Ornstein-Uhlenbeck (OU) process:\n$$\ndY_t^{\\epsilon} \\;=\\; -\\frac{\\lambda}{\\epsilon}\\, Y_t^{\\epsilon}\\,dt \\;+\\; \\frac{\\sigma}{\\sqrt{\\epsilon}}\\, dB_t\n$$\nHere, the drift coefficient is $f(y) = -\\frac{\\lambda}{\\epsilon} y$ and the diffusion coefficient is $g(y) = \\frac{\\sigma}{\\sqrt{\\epsilon}}$. The parameters $\\lambda$ and $\\sigma$ are positive constants. The invariant measure $\\mu$ corresponds to the stationary probability density function $p(y)$ of this process. The stationary density is the time-independent solution ($\\frac{\\partial p}{\\partial t} = 0$) to the Fokker-Planck equation:\n$$\n\\frac{\\partial p(y,t)}{\\partial t} \\;=\\; -\\frac{\\partial}{\\partial y}\\left[f(y)\\,p(y,t)\\right] \\;+\\; \\frac{1}{2}\\frac{\\partial^2}{\\partial y^2}\\left[g(y)^2\\,p(y,t)\\right]\n$$\nFor the stationary state, this reduces to:\n$$\n0 \\;=\\; -\\frac{\\partial}{\\partial y}\\left[ \\left(-\\frac{\\lambda}{\\epsilon} y\\right) p(y) \\right] \\;+\\; \\frac{1}{2}\\frac{\\partial^2}{\\partial y^2}\\left[ \\left(\\frac{\\sigma}{\\sqrt{\\epsilon}}\\right)^2 p(y) \\right]\n$$\nIntegrating once with respect to $y$ and assuming the probability current vanishes at infinity, we obtain a first-order ordinary differential equation for $p(y)$:\n$$\n0 \\;=\\; -\\left(-\\frac{\\lambda}{\\epsilon} y\\right) p(y) \\;+\\; \\frac{1}{2}\\frac{\\partial}{\\partial y}\\left[ \\frac{\\sigma^2}{\\epsilon} p(y) \\right]\n$$\n$$\n\\frac{\\lambda}{\\epsilon} y\\,p(y) \\;+\\; \\frac{\\sigma^2}{2\\epsilon} \\frac{d p(y)}{dy} \\;=\\; 0\n$$\nWe can separate variables to solve for $p(y)$:\n$$\n\\frac{d p}{p} \\;=\\; -\\frac{2\\epsilon\\lambda}{\\epsilon\\sigma^2} y\\,dy \\;=\\; -\\frac{2\\lambda}{\\sigma^2} y\\,dy\n$$\nIntegrating both sides gives:\n$$\n\\ln(p(y)) \\;=\\; -\\frac{\\lambda}{\\sigma^2} y^2 \\;+\\; C_0\n$$\nwhere $C_0$ is the constant of integration. Exponentiating yields:\n$$\np(y) \\;=\\; C\\,\\exp\\left(-\\frac{\\lambda y^2}{\\sigma^2}\\right)\n$$\nwhere $C = \\exp(C_0)$. To find the normalization constant $C$, we enforce the condition that the total probability is $1$:\n$$\n\\int_{-\\infty}^{\\infty} p(y)\\,dy \\;=\\; C \\int_{-\\infty}^{\\infty} \\exp\\left(-\\frac{\\lambda y^2}{\\sigma^2}\\right) dy \\;=\\; 1\n$$\nUsing the standard Gaussian integral formula $\\int_{-\\infty}^{\\infty} \\exp(-k z^2) dz = \\sqrt{\\pi/k}$, with $k = \\lambda/\\sigma^2$, we have:\n$$\nC \\sqrt{\\frac{\\pi\\sigma^2}{\\lambda}} \\;=\\; 1 \\quad \\implies \\quad C \\;=\\; \\sqrt{\\frac{\\lambda}{\\pi\\sigma^2}}\n$$\nThus, the invariant density is:\n$$\np(y) \\;=\\; \\sqrt{\\frac{\\lambda}{\\pi\\sigma^2}} \\exp\\left(-\\frac{\\lambda y^2}{\\sigma^2}\\right)\n$$\nThis is the probability density function for a Gaussian distribution with mean $0$ and variance $V$. A general Gaussian density is $\\frac{1}{\\sqrt{2\\pi V}}\\exp\\left(-\\frac{y^2}{2V}\\right)$. By comparison, we identify $2V = \\sigma^2/\\lambda$, so the variance is $V = \\frac{\\sigma^2}{2\\lambda}$.\nThe invariant measure $\\mu$ is therefore a Gaussian distribution $\\mathcal{N}(0, \\frac{\\sigma^2}{2\\lambda})$. It is notable that this measure does not depend on the small parameter $\\epsilon$.\n\nPart 2: Computation of the Averaged Drift $\\overline{a}(x)$\n\nThe averaged drift $\\overline{a}(x)$ is defined as the expectation of the slow drift $a(x,y)$ with respect to the invariant measure $\\mu$ of the fast variable $y$:\n$$\n\\overline{a}(x) \\;=\\; \\int_{\\mathbb{R}} a(x,y)\\,\\mu(dy) \\;=\\; \\mathbb{E}_{Y \\sim \\mathcal{N}(0, V)}[a(x,Y)]\n$$\nwhere $V = \\frac{\\sigma^2}{2\\lambda}$. The drift function is given by:\n$$\na(x,y) \\;=\\; \\kappa\\,\\sin(\\omega x) \\;+\\; \\eta\\,x\\,y \\;+\\; \\gamma\\,y^{2} \\;+\\; \\rho\\,x^{2}\\,\\cos(\\beta y) \\;+\\; \\delta\\,\\exp(-x^{2})\\,y\\,\\sin(\\beta y)\n$$\nBy the linearity of expectation, we can compute the average of each term separately:\n$$\n\\overline{a}(x) \\;=\\; \\mathbb{E}[\\kappa\\,\\sin(\\omega x)] \\;+\\; \\mathbb{E}[\\eta\\,x\\,y] \\;+\\; \\mathbb{E}[\\gamma\\,y^{2}] \\;+\\; \\mathbb{E}[\\rho\\,x^{2}\\,\\cos(\\beta y)] \\;+\\; \\mathbb{E}[\\delta\\,\\exp(-x^{2})\\,y\\,\\sin(\\beta y)]\n$$\nThe expectation is over $Y \\sim \\mathcal{N}(0, V)$. We can factor out terms that are constant with respect to $y$:\n$$\n\\overline{a}(x) \\;=\\; \\kappa\\,\\sin(\\omega x) \\;+\\; \\eta\\,x\\,\\mathbb{E}[Y] \\;+\\; \\gamma\\,\\mathbb{E}[Y^{2}] \\;+\\; \\rho\\,x^{2}\\,\\mathbb{E}[\\cos(\\beta Y)] \\;+\\; \\delta\\,\\exp(-x^{2})\\,\\mathbb{E}[Y\\,\\sin(\\beta Y)]\n$$\nNow we compute the required expectations for $Y \\sim \\mathcal{N}(0, V)$:\n\n1.  $\\mathbb{E}[Y]$: The mean of the distribution is $0$.\n    $$\n    \\mathbb{E}[Y] \\;=\\; 0\n    $$\n\n2.  $\\mathbb{E}[Y^2]$: For a zero-mean distribution, the second moment is equal to the variance.\n    $$\n    \\mathbb{E}[Y^2] \\;=\\; \\text{Var}(Y) \\;=\\; V \\;=\\; \\frac{\\sigma^2}{2\\lambda}\n    $$\n\n3.  $\\mathbb{E}[\\cos(\\beta Y)]$: We use the characteristic function of a Gaussian random variable $Y$, which is $\\phi_Y(t) = \\mathbb{E}[\\exp(itY)] = \\exp(-\\frac{1}{2}Vt^2)$.\n    $$\n    \\mathbb{E}[\\cos(\\beta Y)] \\;=\\; \\mathbb{E}[\\text{Re}(\\exp(i\\beta Y))] \\;=\\; \\text{Re}(\\mathbb{E}[\\exp(i\\beta Y)]) \\;=\\; \\text{Re}(\\phi_Y(\\beta))\n    $$\n    $$\n    \\mathbb{E}[\\cos(\\beta Y)] \\;=\\; \\text{Re}\\left(\\exp\\left(-\\frac{1}{2}V\\beta^2\\right)\\right) \\;=\\; \\exp\\left(-\\frac{1}{2}V\\beta^2\\right) \\;=\\; \\exp\\left(-\\frac{\\sigma^2\\beta^2}{4\\lambda}\\right)\n    $$\n\n4.  $\\mathbb{E}[Y\\,\\sin(\\beta Y)]$: We can use a property related to the characteristic function.\n    $$\n    \\mathbb{E}[Y\\exp(itY)] \\;=\\; \\frac{1}{i}\\frac{d}{dt}\\mathbb{E}[\\exp(itY)] \\;=\\; -i\\frac{d}{dt}\\phi_Y(t) \\;=\\; -i \\frac{d}{dt}\\exp\\left(-\\frac{1}{2}Vt^2\\right) \\;=\\; -i(-Vt)\\exp\\left(-\\frac{1}{2}Vt^2\\right) \\;=\\; iVt\\exp\\left(-\\frac{1}{2}Vt^2\\right)\n    $$\n    The expectation we need is the imaginary part of this expression evaluated at $t=\\beta$:\n    $$\n    \\mathbb{E}[Y\\,\\sin(\\beta Y)] \\;=\\; \\text{Im}(\\mathbb{E}[Y\\exp(i\\beta Y)]) \\;=\\; \\text{Im}\\left(iV\\beta\\exp\\left(-\\frac{1}{2}V\\beta^2\\right)\\right) \\;=\\; V\\beta\\exp\\left(-\\frac{1}{2}V\\beta^2\\right)\n    $$\n    Substituting $V = \\frac{\\sigma^2}{2\\lambda}$:\n    $$\n    \\mathbb{E}[Y\\,\\sin(\\beta Y)] \\;=\\; \\frac{\\sigma^2\\beta}{2\\lambda}\\exp\\left(-\\frac{\\sigma^2\\beta^2}{4\\lambda}\\right)\n    $$\n\nFinally, we substitute these expectations back into the expression for $\\overline{a}(x)$:\n$$\n\\overline{a}(x) \\;=\\; \\kappa\\,\\sin(\\omega x) \\;+\\; \\eta\\,x\\,(0) \\;+\\; \\gamma\\left(\\frac{\\sigma^2}{2\\lambda}\\right) \\;+\\; \\rho\\,x^{2}\\exp\\left(-\\frac{\\sigma^2\\beta^2}{4\\lambda}\\right) \\;+\\; \\delta\\,\\exp(-x^{2})\\left(\\frac{\\sigma^2\\beta}{2\\lambda}\\exp\\left(-\\frac{\\sigma^2\\beta^2}{4\\lambda}\\right)\\right)\n$$\nCombining terms, we obtain the closed-form expression for the effective drift:\n$$\n\\overline{a}(x) \\;=\\; \\kappa\\,\\sin(\\omega x) \\;+\\; \\frac{\\gamma\\sigma^2}{2\\lambda} \\;+\\; \\rho\\,x^{2}\\exp\\left(-\\frac{\\sigma^2\\beta^2}{4\\lambda}\\right) \\;+\\; \\frac{\\delta\\sigma^2\\beta}{2\\lambda}\\exp(-x^2)\\exp\\left(-\\frac{\\sigma^2\\beta^2}{4\\lambda}\\right)\n$$\nThis expression can be slightly rearranged by factoring out the common exponential term:\n$$\n\\overline{a}(x) \\;=\\; \\kappa\\,\\sin(\\omega x) \\;+\\; \\frac{\\gamma\\sigma^2}{2\\lambda} \\;+\\; \\left(\\rho\\,x^{2} \\;+\\; \\frac{\\delta\\sigma^2\\beta}{2\\lambda}\\exp(-x^2)\\right)\\exp\\left(-\\frac{\\sigma^2\\beta^2}{4\\lambda}\\right)\n$$\nThis is the final analytical expression for the averaged drift.", "answer": "$$\\boxed{\\kappa\\sin(\\omega x) + \\frac{\\gamma\\sigma^2}{2\\lambda} + \\left(\\rho x^{2} + \\frac{\\delta\\sigma^2\\beta}{2\\lambda}\\exp(-x^{2})\\right)\\exp\\left(-\\frac{\\sigma^2\\beta^2}{4\\lambda}\\right)}$$", "id": "2979068"}, {"introduction": "Building upon the previous example, we now explore a more general and realistic scenario where the dynamics of the fast variable depend on the state of the slow variable. This exercise requires you to derive an invariant measure that is parameterized by the slow state $x$ and then compute the corresponding averaged drift. Furthermore, it challenges you to prove that the resulting effective drift inherits analytical properties like continuity and Lipschitz continuity, a crucial step for rigorously establishing the validity of the averaging approximation. [@problem_id:2979079]", "problem": "Consider a slow–fast system where the slow component $X_{t}^{\\epsilon}$ evolves according to the stochastic differential equation $dX_{t}^{\\epsilon} = a(X_{t}^{\\epsilon}, Y_{t}^{\\epsilon})\\,dt$, and the fast component $Y_{t}^{\\epsilon}$, conditional on the current slow state $x \\in \\mathbb{R}$, follows a one-dimensional Ornstein–Uhlenbeck (OU) stochastic differential equation\n$$\ndY_{t}^{\\epsilon} = \\frac{1}{\\epsilon}\\Big(-\\kappa(x)\\big(Y_{t}^{\\epsilon} - m(x)\\big)\\Big)\\,dt + \\frac{1}{\\sqrt{\\epsilon}}\\,\\sigma(x)\\,dB_{t},\n$$\nwhere $B_{t}$ is a standard Brownian motion, $\\kappa:\\mathbb{R}\\to(0,\\infty)$, $m:\\mathbb{R}\\to\\mathbb{R}$, and $\\sigma:\\mathbb{R}\\to(0,\\infty)$ are given functions. Assume that for each fixed $x$, the OU process is ergodic with a unique stationary probability measure $\\mu_{x}$ on $\\mathbb{R}$. The drift of the slow component is specified by\n$$\na(x,y) = \\alpha(x)\\,y + \\beta(x)\\,y^{2},\n$$\nwhere $\\alpha:\\mathbb{R}\\to\\mathbb{R}$ and $\\beta:\\mathbb{R}\\to\\mathbb{R}$ are given functions.\n\nStarting from the definitions of the generator of a Markov process, stationary measure, and the classical properties of the Ornstein–Uhlenbeck process, derive the invariant measure $\\mu_{x}$ for the fast process at frozen $x$, use it to define the averaged drift\n$$\n\\bar{a}(x) = \\int_{\\mathbb{R}} a(x,y)\\,\\mu_{x}(dy),\n$$\nand compute $\\bar{a}(x)$ explicitly in terms of $\\alpha$, $\\beta$, $m$, $\\kappa$, and $\\sigma$. Prove that $\\bar{a}(x)$ is continuous on $\\mathbb{R}$ if $\\alpha$, $\\beta$, $m$, $\\kappa$, and $\\sigma$ are continuous and $\\inf_{x}\\kappa(x)>0$. Further, prove that $\\bar{a}(x)$ is globally Lipschitz on $\\mathbb{R}$ if $\\alpha$, $\\beta$, $m$, $\\sigma$, and $\\kappa$ are globally Lipschitz, $\\inf_{x}\\kappa(x)>0$, and $\\alpha$, $\\beta$, $m$, and $\\sigma$ are bounded. Your reasoning should be grounded in the definitions and well-tested facts about Ornstein–Uhlenbeck processes and integrals against parameterized measures; do not invoke any averaging principle theorem statements beyond these foundational elements.\n\nProvide as your final answer the explicit analytic expression for $\\bar{a}(x)$. No numerical rounding is required. No physical units are involved.", "solution": "The problem as stated is scientifically grounded, self-contained, and well-posed. All terms are clearly defined within the context of stochastic differential equations and averaging theory. The tasks—deriving an averaged coefficient and proving its analytical properties under specified assumptions—are standard and formalizable. The assumptions on the coefficient functions are mathematically precise and sufficient for the required derivations. The problem is therefore valid.\n\nWe will proceed with a full derivation as requested. The solution is structured into three main parts: first, the derivation of the invariant measure $\\mu_{x}$ for the fast process; second, the explicit computation of the averaged drift $\\bar{a}(x)$; and third, the proofs of continuity and the global Lipschitz property for $\\bar{a}(x)$.\n\n**Part 1: Derivation of the Invariant Measure $\\mu_x$**\n\nFor a fixed value of the slow variable $x \\in \\mathbb{R}$, the functions $\\kappa(x)$, $m(x)$, and $\\sigma(x)$ are constants. The dynamics of the fast process $Y_t$ (rescaling time by $s = t/\\epsilon$) are governed by the Ornstein-Uhlenbeck (OU) stochastic differential equation:\n$$\ndY_s = -\\kappa(x)(Y_s - m(x))\\,ds + \\sigma(x)\\,dB_s\n$$\nThe invariant measure $\\mu_x$ of this process has a probability density function, denoted $p_x(y)$, which is the stationary solution to the associated Fokker-Planck equation. The generator $\\mathcal{L}_x$ of the process for a test function $f(y)$ is:\n$$\n\\mathcal{L}_x f(y) = -\\kappa(x)(y - m(x))\\frac{df}{dy} + \\frac{1}{2}\\sigma(x)^2 \\frac{d^2f}{dy^2}\n$$\nThe stationary Fokker-Planck equation is $\\mathcal{L}_x^* p_x = 0$, where $\\mathcal{L}_x^*$ is the formal adjoint of $\\mathcal{L}_x$.\n$$\n\\mathcal{L}_x^* p_x(y) = \\frac{d}{dy}\\left[\\kappa(x)(y - m(x))p_x(y)\\right] + \\frac{1}{2}\\frac{d^2}{dy^2}\\left[\\sigma(x)^2 p_x(y)\\right] = 0\n$$\nSince $\\kappa(x)$ and $\\sigma(x)$ are constants with respect to $y$, this simplifies to:\n$$\n\\frac{d}{dy}\\left[\\kappa(x)(y - m(x))p_x(y) + \\frac{1}{2}\\sigma(x)^2 \\frac{dp_x}{dy}\\right] = 0\n$$\nIntegrating once with respect to $y$ yields:\n$$\n\\kappa(x)(y - m(x))p_x(y) + \\frac{1}{2}\\sigma(x)^2 \\frac{dp_x}{dy} = C_1\n$$\nwhere $C_1$ is an integration constant. For the density $p_x(y)$ to be normalizable on $\\mathbb{R}$, the probability flux (the expression on the left) must vanish as $y \\to \\pm\\infty$. This requires $C_1=0$. We are left with a first-order ordinary differential equation for $p_x(y)$:\n$$\n\\frac{1}{2}\\sigma(x)^2 \\frac{dp_x}{dy} = -\\kappa(x)(y - m(x))p_x(y)\n$$\nSeparating variables, we get:\n$$\n\\frac{dp_x}{p_x} = -\\frac{2\\kappa(x)}{\\sigma(x)^2}(y - m(x))\\,dy\n$$\nIntegrating both sides gives:\n$$\n\\ln(p_x(y)) = -\\frac{\\kappa(x)}{\\sigma(x)^2}(y - m(x))^2 + C_2\n$$\nExponentiating both sides yields the solution for the density:\n$$\np_x(y) = C_3 \\exp\\left(-\\frac{\\kappa(x)}{\\sigma(x)^2}(y - m(x))^2\\right)\n$$\nThis is the probability density function of a Gaussian (Normal) distribution. We identify its mean and variance by comparing to the standard form $p(y) = \\frac{1}{\\sqrt{2\\pi\\sigma_{var}^2}}\\exp\\left(-\\frac{(y-\\mu)^2}{2\\sigma_{var}^2}\\right)$.\nThe mean is clearly $\\mu = m(x)$.\nFor the variance $\\sigma_{var}^2$, we equate the exponents:\n$$\n\\frac{(y-m(x))^2}{2\\sigma_{var}^2} = \\frac{\\kappa(x)}{\\sigma(x)^2}(y - m(x))^2\n$$\nThis gives $\\frac{1}{2\\sigma_{var}^2} = \\frac{\\kappa(x)}{\\sigma(x)^2}$, which implies the variance is $\\sigma_{var}^2 = \\frac{\\sigma(x)^2}{2\\kappa(x)}$.\nThus, for each fixed $x$, the stationary measure $\\mu_x$ is a Gaussian measure $\\mathcal{N}\\left(m(x), \\frac{\\sigma(x)^2}{2\\kappa(x)}\\right)$.\n\n**Part 2: Computation of the Averaged Drift $\\bar{a}(x)$**\n\nThe averaged drift $\\bar{a}(x)$ is defined by the integral of $a(x,y)$ with respect to the invariant measure $\\mu_x$:\n$$\n\\bar{a}(x) = \\int_{\\mathbb{R}} a(x,y)\\,\\mu_{x}(dy) = \\mathbb{E}_{Y \\sim \\mu_x}[a(x,Y)]\n$$\nGiven $a(x,y) = \\alpha(x)y + \\beta(x)y^2$ and the linearity of expectation, we have:\n$$\n\\bar{a}(x) = \\alpha(x)\\mathbb{E}_{Y \\sim \\mu_x}[Y] + \\beta(x)\\mathbb{E}_{Y \\sim \\mu_x}[Y^2]\n$$\nWe use the moments of the stationary distribution $Y \\sim \\mathcal{N}\\left(m(x), \\frac{\\sigma(x)^2}{2\\kappa(x)}\\right)$. The first moment (mean) is:\n$$\n\\mathbb{E}_{Y \\sim \\mu_x}[Y] = m(x)\n$$\nFor a random variable $Z$ with mean $\\mu$ and variance $\\sigma_{var}^2$, the second moment is given by $\\mathbb{E}[Z^2] = \\sigma_{var}^2 + \\mu^2$. Applying this to our process $Y$:\n$$\n\\mathbb{E}_{Y \\sim \\mu_x}[Y^2] = \\left(\\text{Variance of } \\mu_x\\right) + \\left(\\text{Mean of } \\mu_x\\right)^2 = \\frac{\\sigma(x)^2}{2\\kappa(x)} + m(x)^2\n$$\nSubstituting these moments into the expression for $\\bar{a}(x)$:\n$$\n\\bar{a}(x) = \\alpha(x)m(x) + \\beta(x)\\left(\\frac{\\sigma(x)^2}{2\\kappa(x)} + m(x)^2\\right)\n$$\nThis is the explicit analytical expression for the averaged drift.\n\n**Part 3: Proof of Analytical Properties**\n\n**Continuity:**\nWe are given that the functions $\\alpha(x)$, $\\beta(x)$, $m(x)$, $\\kappa(x)$, and $\\sigma(x)$ are continuous on $\\mathbb{R}$. We are also given that $\\kappa(x) > 0$ for all $x$. The expression for the averaged drift is:\n$$\n\\bar{a}(x) = \\alpha(x)m(x) + \\beta(x)m(x)^2 + \\frac{\\beta(x)\\sigma(x)^2}{2\\kappa(x)}\n$$\nThe well-known properties of continuous functions state that the sum, product, and composition of continuous functions are continuous. The quotient of two continuous functions $f(x)/g(x)$ is continuous at all points where $g(x) \\neq 0$.\n1.  The term $\\alpha(x)m(x)$ is a product of continuous functions, hence it is continuous.\n2.  The term $\\beta(x)m(x)^2$ is a product of continuous functions, hence it is continuous.\n3.  The term $\\frac{\\beta(x)\\sigma(x)^2}{2\\kappa(x)}$ is a quotient. The numerator $\\beta(x)\\sigma(x)^2$ is continuous as it is a product of continuous functions. The denominator $2\\kappa(x)$ is continuous. Since $\\kappa(x) > 0$ for all $x \\in \\mathbb{R}$, the denominator is never zero. Therefore, the quotient is a continuous function on $\\mathbb{R}$.\nSince $\\bar{a}(x)$ is the sum of three continuous functions, it is itself continuous on $\\mathbb{R}$. The condition $\\inf_x \\kappa(x) > 0$ is a stronger condition that is not strictly necessary for continuity but guarantees the denominator is bounded away from zero.\n\n**Global Lipschitz Property:**\nWe assume that $\\alpha, \\beta, m, \\sigma, \\kappa$ are globally Lipschitz; $\\alpha, \\beta, m, \\sigma$ are bounded; and $\\inf_x \\kappa(x) = c_\\kappa > 0$. We will show that each term in the expression for $\\bar{a}(x)$ is globally Lipschitz.\nThe sum of globally Lipschitz functions is globally Lipschitz.\n1.  **Term $\\alpha(x)m(x)$:** The product of two bounded, globally Lipschitz functions is globally Lipschitz. Let $M_\\alpha, M_m$ be the bounds and $L_\\alpha, L_m$ be the Lipschitz constants. For any $x,z \\in \\mathbb{R}$:\n    $$\n    |\\alpha(x)m(x) - \\alpha(z)m(z)| = |\\alpha(x)m(x) - \\alpha(x)m(z) + \\alpha(x)m(z) - \\alpha(z)m(z)|\n    $$\n    $$\n    \\leq |\\alpha(x)||m(x)-m(z)| + |m(z)||\\alpha(x)-\\alpha(z)| \\leq (M_\\alpha L_m + M_m L_\\alpha)|x-z|\n    $$\n    So $\\alpha(x)m(x)$ is globally Lipschitz.\n2.  **Term $\\beta(x)m(x)^2$:** First, $m(x)^2$ is globally Lipschitz because $m$ is bounded and Lipschitz. The product of $m(x)$ with itself satisfies the conditions of the previous step. Then, $\\beta(x)m(x)^2$ is the product of two bounded, globally Lipschitz functions ($\\beta$ and $m^2$), and is therefore globally Lipschitz.\n3.  **Term $\\frac{\\beta(x)\\sigma(x)^2}{2\\kappa(x)}$:** Let $N(x) = \\beta(x)\\sigma(x)^2$ and $D(x) = 2\\kappa(x)$.\n    - The numerator $N(x)$ is the product of bounded, Lipschitz functions ($\\beta$ and $\\sigma^2$), so it is bounded and globally Lipschitz. Let its bound be $M_N$ and Lipschitz constant be $L_N$.\n    - The denominator $D(x)$ is globally Lipschitz with constant $2L_\\kappa$ since $\\kappa$ is Lipschitz. Furthermore, from $\\inf_x \\kappa(x) = c_\\kappa > 0$, we have $|D(x)| = 2\\kappa(x) \\ge 2c_\\kappa > 0$.\n    For any $x,z \\in \\mathbb{R}$:\n    $$\n    \\left|\\frac{N(x)}{D(x)} - \\frac{N(z)}{D(z)}\\right| = \\left|\\frac{N(x)D(z) - N(z)D(x)}{D(x)D(z)}\\right| = \\left|\\frac{N(x)D(z) - N(z)D(z) + N(z)D(z) - N(z)D(x)}{D(x)D(z)}\\right|\n    $$\n    $$\n    \\leq \\frac{|D(z)||N(x)-N(z)| + |N(z)||D(z)-D(x)|}{|D(x)D(z)|}\n    $$\n    Since $\\kappa$ is not assumed to be bounded, we use a different arrangement:\n    $$\n    \\left|\\frac{N(x)}{D(x)} - \\frac{N(z)}{D(z)}\\right| \\leq \\left|\\frac{N(x)-N(z)}{D(x)}\\right| + \\left|N(z)\\left(\\frac{1}{D(x)} - \\frac{1}{D(z)}\\right)\\right|\n    $$\n    $$\n    = \\frac{|N(x)-N(z)|}{|D(x)|} + |N(z)|\\frac{|D(z)-D(x)|}{|D(x)D(z)|}\n    $$\n    Using the bounds and Lipschitz constants:\n    $$\n    \\leq \\frac{L_N|x-z|}{2c_\\kappa} + M_N \\frac{2L_\\kappa|x-z|}{(2c_\\kappa)^2} = \\left(\\frac{L_N}{2c_\\kappa} + \\frac{M_N L_\\kappa}{2c_\\kappa^2}\\right)|x-z|\n    $$\n    This shows the term is globally Lipschitz.\nAs $\\bar{a}(x)$ is a sum of three globally Lipschitz functions, it is itself globally Lipschitz on $\\mathbb{R}$. This completes the proof.", "answer": "$$\n\\boxed{\\alpha(x)m(x) + \\beta(x)\\left(m(x)^{2} + \\frac{\\sigma(x)^{2}}{2\\kappa(x)}\\right)}\n$$", "id": "2979079"}, {"introduction": "Having learned to derive the effective dynamics, we now turn to a critical question: how accurate is the averaging approximation? This advanced practice shifts our focus from calculating the limit to estimating the convergence error. You will explore the deep connection between the mixing rate of the fast process and the magnitude of the averaging error, quantifying how slower (polynomial) memory decay in the fast dynamics degrades the approximation compared to the ideal case of rapid (exponential) memory decay. [@problem_id:2979087]", "problem": "Consider a two-scale observable constructed from a stationary and ergodic fast process. Let $\\{Y_{t}\\}_{t \\geq 0}$ be a stationary and ergodic Markov process on a Polish space with invariant probability measure $\\mu$, and let $g : \\mathrm{State}(Y) \\to \\mathbb{R}$ be a bounded measurable observable satisfying $\\int g \\,\\mathrm{d}\\mu = 0$ and $\\|g\\|_{\\infty} \\leq 1$. Define the slow observable\n$$\nX_{t}^{\\varepsilon} \\equiv \\int_{0}^{t} g\\!\\left(Y_{s/\\varepsilon}\\right) \\,\\mathrm{d}s,\n$$\nand interpret $X_{t}^{\\varepsilon}$ as the averaging error relative to the homogenized limit (which is zero because $\\int g\\,\\mathrm{d}\\mu = 0$). The aim is to quantify how the root mean square averaging error depends on the mixing rate of the fast process.\n\nAssume the covariance function $C(s) \\equiv \\mathrm{Cov}\\!\\left(g(Y_{0}), g(Y_{s})\\right)$ is nonnegative and admits one of the following mixing-rate upper bounds:\n- Exponential mixing: $C(s) \\leq M \\exp(-\\gamma s)$ for all $s \\geq 0$, with constants $M > 0$ and $\\gamma > 0$.\n- Polynomial mixing: $C(s) \\leq M (1+s)^{-\\alpha}$ for all $s \\geq 0$, with constants $M > 0$ and $\\alpha > 0$.\n\nDefine the root mean square averaging error at time $t > 0$ by\n$$\nE_{\\varepsilon}(t) \\equiv \\left(\\mathbb{E}\\!\\left[\\,\\left|X_{t}^{\\varepsilon}\\right|^{2}\\right]\\right)^{1/2}.\n$$\n\nStarting from the foundational definitions of covariance and stationarity, derive an explicit upper bound for $E_{\\varepsilon}(t)$ in terms of $\\varepsilon$, $t$, and the mixing-rate parameters for each of the two mixing regimes. Then, by asymptotically evaluating the resulting integrals for small $\\varepsilon$ (fixed $t$), determine the leading-order dependence of $E_{\\varepsilon}(t)$ on $\\varepsilon$ for exponential mixing and for polynomial mixing with exponent $\\alpha$. Use these asymptotics to define the degradation ratio\n$$\nR_{\\mathrm{deg}}(\\varepsilon,\\alpha) \\equiv \\frac{\\text{leading-order }\\varepsilon\\text{-dependence of }E_{\\varepsilon}(t)\\text{ under polynomial mixing with exponent }\\alpha}{\\text{leading-order }\\varepsilon\\text{-dependence of }E_{\\varepsilon}(t)\\text{ under exponential mixing}},\n$$\nwhere multiplicative constants that do not depend on $\\varepsilon$ should be discarded and, at $\\alpha=1$, any logarithmic factor must be retained. Conclude with a single closed-form analytic expression for $R_{\\mathrm{deg}}(\\varepsilon,\\alpha)$ that exhibits how slower mixing degrades the averaging error as $\\varepsilon \\to 0$.\n\nYour final reported answer must be the single expression for $R_{\\mathrm{deg}}(\\varepsilon,\\alpha)$. No units are required.", "solution": "The problem statement is a well-posed and scientifically grounded exercise in the theory of stochastic averaging. All provided definitions and conditions are standard in the field of stochastic differential equations and homogenization. The problem is self-contained, consistent, and requires a rigorous mathematical derivation. Therefore, the problem is deemed valid. We proceed with the solution.\n\nThe objective is to derive the degradation ratio $R_{\\mathrm{deg}}(\\varepsilon, \\alpha)$ that quantifies the increase in averaging error when moving from an exponentially mixing fast process to a polynomially mixing one.\n\nFirst, we establish a general expression for the mean square averaging error, $\\mathbb{E}[|X_t^\\varepsilon|^2]$. By definition,\n$$\n\\mathbb{E}\\!\\left[\\left|X_{t}^{\\varepsilon}\\right|^{2}\\right] = \\mathbb{E}\\!\\left[\\left(\\int_{0}^{t} g(Y_{s/\\varepsilon})\\,\\mathrm{d}s\\right)^2\\right] = \\mathbb{E}\\!\\left[\\int_{0}^{t}\\int_{0}^{t} g(Y_{s/\\varepsilon}) g(Y_{u/\\varepsilon})\\,\\mathrm{d}s\\,\\mathrm{d}u\\right].\n$$\nUsing Fubini's theorem to exchange expectation and integration, we get\n$$\n\\mathbb{E}\\!\\left[\\left|X_{t}^{\\varepsilon}\\right|^{2}\\right] = \\int_{0}^{t}\\int_{0}^{t} \\mathbb{E}\\!\\left[g(Y_{s/\\varepsilon}) g(Y_{u/\\varepsilon})\\right]\\,\\mathrm{d}s\\,\\mathrm{d}u.\n$$\nThe process $\\{Y_t\\}$ is stationary, and the observable $g$ is centered, meaning $\\mathbb{E}[g(Y_\\tau)] = \\int g\\,\\mathrm{d}\\mu = 0$ for any $\\tau$. Therefore, the expectation term is the covariance:\n$$\n\\mathbb{E}\\!\\left[g(Y_{s/\\varepsilon}) g(Y_{u/\\varepsilon})\\right] = \\mathrm{Cov}\\!\\left(g(Y_{s/\\varepsilon}), g(Y_{u/\\varepsilon})\\right).\n$$\nBy stationarity, the covariance depends only on the time difference:\n$$\n\\mathrm{Cov}\\!\\left(g(Y_{s/\\varepsilon}), g(Y_{u/\\varepsilon})\\right) = \\mathrm{Cov}\\!\\left(g(Y_0), g(Y_{|s-u|/\\varepsilon})\\right) = C(|s-u|/\\varepsilon).\n$$\nSubstituting this back, we obtain the double integral\n$$\n\\mathbb{E}\\!\\left[\\left|X_{t}^{\\varepsilon}\\right|^{2}\\right] = \\int_{0}^{t}\\int_{0}^{t} C(|s-u|/\\varepsilon)\\,\\mathrm{d}s\\,\\mathrm{d}u.\n$$\nThis integral can be simplified. By splitting the domain of integration and using symmetry, we find\n$$\n\\mathbb{E}\\!\\left[\\left|X_{t}^{\\varepsilon}\\right|^{2}\\right] = 2\\int_{0}^{t} (t-s) C(s/\\varepsilon)\\,\\mathrm{d}s.\n$$\nWe now perform a change of variables, letting $u = s/\\varepsilon$. This gives $s = u\\varepsilon$ and $\\mathrm{d}s = \\varepsilon\\,\\mathrm{d}u$. The limits of integration become $0$ to $t/\\varepsilon$.\n$$\n\\mathbb{E}\\!\\left[\\left|X_{t}^{\\varepsilon}\\right|^{2}\\right] = 2\\int_{0}^{t/\\varepsilon} (t-u\\varepsilon) C(u) (\\varepsilon\\,\\mathrm{d}u) = 2\\varepsilon t \\int_{0}^{t/\\varepsilon} C(u)\\,\\mathrm{d}u - 2\\varepsilon^2 \\int_{0}^{t/\\varepsilon} u C(u)\\,\\mathrm{d}u.\n$$\nThis exact expression forms the basis for our asymptotic analysis as $\\varepsilon \\to 0$. We are interested in the leading-order dependence of $E_{\\varepsilon}(t) = (\\mathbb{E}[|X_{t}^{\\varepsilon}|^2])^{1/2}$ on $\\varepsilon$.\n\n**Exponential Mixing Regime**\nWe are given the bound $C(s) \\leq M \\exp(-\\gamma s)$ for $M, \\gamma > 0$. The integrals $\\int_0^\\infty C(u)\\,\\mathrm{d}u$ and $\\int_0^\\infty u C(u)\\,\\mathrm{d}u$ are convergent under this assumption. For small $\\varepsilon$, the upper limit of integration $t/\\varepsilon \\to \\infty$, and the integrals converge to positive constants.\n$$\n\\lim_{\\varepsilon \\to 0} \\int_{0}^{t/\\varepsilon} C(u)\\,\\mathrm{d}u = \\int_{0}^{\\infty} C(u)\\,\\mathrm{d}u \\equiv D_1 > 0.\n$$\n$$\n\\lim_{\\varepsilon \\to 0} \\int_{0}^{t/\\varepsilon} uC(u)\\,\\mathrm{d}u = \\int_{0}^{\\infty} uC(u)\\,\\mathrm{d}u \\equiv D_2 > 0.\n$$\nThe mean square error thus behaves as\n$$\n\\mathbb{E}\\!\\left[\\left|X_{t}^{\\varepsilon}\\right|^{2}\\right] \\approx 2 D_1 t \\varepsilon - 2 D_2 \\varepsilon^2.\n$$\nFor small $\\varepsilon$, the term of order $\\varepsilon$ dominates. Thus, the leading-order behavior is $\\mathbb{E}[|X_{t}^{\\varepsilon}|^2] \\sim \\mathcal{O}(\\varepsilon)$.\nThe root mean square error therefore has the leading-order dependence\n$$\nE_{\\varepsilon}(t) \\sim \\sqrt{\\mathcal{O}(\\varepsilon)} = \\mathcal{O}(\\varepsilon^{1/2}).\n$$\n\n**Polynomial Mixing Regime**\nWe are given the bound $C(s) \\leq M (1+s)^{-\\alpha}$ for $M, \\alpha > 0$. The asymptotic behavior depends critically on the value of $\\alpha$.\n\nCase 1: $\\alpha > 1$.\nIn this case, the integral $\\int_0^\\infty (1+u)^{-\\alpha}\\,\\mathrm{d}u = (\\alpha-1)^{-1}$ converges. Similarly, $\\int_0^\\infty u(1+u)^{-\\alpha}\\,\\mathrm{d}u$ converges (since for large $u$, the integrand is $\\sim u^{1-\\alpha}$ and $1-\\alpha < -1$). Therefore, the same logic as in the exponential case applies. The leading-order behavior is determined by the $\\mathcal{O}(\\varepsilon)$ term.\n$$\n\\mathbb{E}\\!\\left[\\left|X_{t}^{\\varepsilon}\\right|^{2}\\right] \\sim \\mathcal{O}(\\varepsilon) \\implies E_{\\varepsilon}(t) \\sim \\mathcal{O}(\\varepsilon^{1/2}).\n$$\n\nCase 2: $\\alpha = 1$.\nThe bound is $C(s) \\leq M(1+s)^{-1}$. The integral $\\int C(u)\\,\\mathrm{d}u$ diverges logarithmically. We must evaluate the asymptotic behavior of the integrals in our expression for $\\mathbb{E}[|X_t^\\varepsilon|^2]$.\nFor small $\\varepsilon$, $T = t/\\varepsilon$ is large.\n$$\n\\int_{0}^{t/\\varepsilon} (1+u)^{-1}\\,\\mathrm{d}u = \\ln(1+t/\\varepsilon) \\approx \\ln(t/\\varepsilon) = \\ln(t) - \\ln(\\varepsilon) = \\ln(1/\\varepsilon) + \\text{const}.\n$$\nThe leading asymptotic behavior is $\\ln(1/\\varepsilon)$.\nThe second integral is\n$$\n\\int_{0}^{t/\\varepsilon} u(1+u)^{-1}\\,\\mathrm{d}u = \\int_{0}^{t/\\varepsilon} \\left(1 - \\frac{1}{1+u}\\right)\\,\\mathrm{d}u = \\frac{t}{\\varepsilon} - \\ln(1+t/\\varepsilon) \\approx \\frac{t}{\\varepsilon}.\n$$\nSubstituting these asymptotics into the expression for the mean square error:\n$$\n\\mathbb{E}\\!\\left[\\left|X_{t}^{\\varepsilon}\\right|^{2}\\right] \\lesssim 2\\varepsilon t M \\ln(1/\\varepsilon) - 2\\varepsilon^2 M (t/\\varepsilon).\n$$\nThe expression contains terms of order $\\varepsilon \\ln(1/\\varepsilon)$ and $\\varepsilon$. As $\\varepsilon \\to 0$, the $\\varepsilon \\ln(1/\\varepsilon)$ term dominates.\n$$\n\\mathbb{E}\\!\\left[\\left|X_{t}^{\\varepsilon}\\right|^{2}\\right] \\sim \\mathcal{O}(\\varepsilon \\ln(1/\\varepsilon)) \\implies E_{\\varepsilon}(t) \\sim \\mathcal{O}(\\sqrt{\\varepsilon \\ln(1/\\varepsilon)}).\n$$\nDiscarding constants, the leading $\\varepsilon$-dependence is $\\varepsilon^{1/2} \\sqrt{\\ln(1/\\varepsilon)}$.\n\nCase 3: $0 < \\alpha < 1$.\nBoth integrals $\\int C(u)\\,\\mathrm{d}u$ and $\\int u C(u)\\,\\mathrm{d}u$ diverge. We evaluate the asymptotic behavior of the definite integrals.\nFor large $T=t/\\varepsilon$,\n$$\n\\int_0^T (1+u)^{-\\alpha}\\,\\mathrm{d}u = \\frac{(1+T)^{1-\\alpha}-1}{1-\\alpha} \\approx \\frac{T^{1-\\alpha}}{1-\\alpha} = \\frac{(t/\\varepsilon)^{1-\\alpha}}{1-\\alpha}.\n$$\n$$\n\\int_0^T u(1+u)^{-\\alpha}\\,\\mathrm{d}u \\approx \\int_0^T u^{1-\\alpha}\\,\\mathrm{d}u = \\frac{T^{2-\\alpha}}{2-\\alpha} = \\frac{(t/\\varepsilon)^{2-\\alpha}}{2-\\alpha}.\n$$\nSubstituting these into the full expression for the mean square error upper bound:\n$$\n\\mathbb{E}\\!\\left[\\left|X_{t}^{\\varepsilon}\\right|^{2}\\right] \\lesssim 2\\varepsilon t M \\frac{(t/\\varepsilon)^{1-\\alpha}}{1-\\alpha} - 2\\varepsilon^2 M \\frac{(t/\\varepsilon)^{2-\\alpha}}{2-\\alpha}\n$$\n$$\n= \\frac{2M}{1-\\alpha} t^{2-\\alpha} \\varepsilon^\\alpha - \\frac{2M}{2-\\alpha} t^{2-\\alpha} \\varepsilon^\\alpha = 2M t^{2-\\alpha} \\varepsilon^\\alpha \\left(\\frac{1}{1-\\alpha} - \\frac{1}{2-\\alpha}\\right)\n$$\n$$\n= 2M t^{2-\\alpha} \\varepsilon^\\alpha \\frac{(2-\\alpha)-(1-\\alpha)}{(1-\\alpha)(2-\\alpha)} = \\frac{2M t^{2-\\alpha}}{(1-\\alpha)(2-\\alpha)}\\varepsilon^\\alpha.\n$$\nSince $0 < \\alpha < 1$, the leading-order behavior is $\\mathbb{E}[|X_{t}^{\\varepsilon}|^2] \\sim \\mathcal{O}(\\varepsilon^\\alpha)$.\nThe root mean square error therefore has the leading-order dependence\n$$\nE_{\\varepsilon}(t) \\sim \\sqrt{\\mathcal{O}(\\varepsilon^\\alpha)} = \\mathcal{O}(\\varepsilon^{\\alpha/2}).\n$$\n\n**Degradation Ratio Calculation**\nThe degradation ratio is defined as\n$$\nR_{\\mathrm{deg}}(\\varepsilon,\\alpha) \\equiv \\frac{\\text{leading-order }\\varepsilon\\text{-dependence of }E_{\\varepsilon}(t)\\text{ under polynomial mixing}}{\\text{leading-order }\\varepsilon\\text{-dependence of }E_{\\varepsilon}(t)\\text{ under exponential mixing}}.\n$$\nThe denominator is the result from the exponential mixing case, which is $\\varepsilon^{1/2}$.\n\n- For polynomial mixing with $\\alpha > 1$:\n$R_{\\mathrm{deg}}(\\varepsilon,\\alpha) = \\frac{\\varepsilon^{1/2}}{\\varepsilon^{1/2}} = 1$.\n\n- For polynomial mixing with $\\alpha = 1$:\n$R_{\\mathrm{deg}}(\\varepsilon,\\alpha) = \\frac{\\varepsilon^{1/2}\\sqrt{\\ln(1/\\varepsilon)}}{\\varepsilon^{1/2}} = \\sqrt{\\ln(1/\\varepsilon)}$.\n\n- For polynomial mixing with $0 < \\alpha < 1$:\n$R_{\\mathrm{deg}}(\\varepsilon,\\alpha) = \\frac{\\varepsilon^{\\alpha/2}}{\\varepsilon^{1/2}} = \\varepsilon^{(\\alpha-1)/2}$.\n\nCombining these results yields a piecewise function for the degradation ratio, which represents the final answer as a single, unified mathematical expression.", "answer": "$$\n\\boxed{\nR_{\\mathrm{deg}}(\\varepsilon, \\alpha) =\n\\begin{cases}\n\\varepsilon^{(\\alpha - 1)/2} & \\text{if } 0 < \\alpha < 1 \\\\\n\\sqrt{\\ln(1/\\varepsilon)} & \\text{if } \\alpha = 1 \\\\\n1 & \\text{if } \\alpha > 1\n\\end{cases}\n}\n$$", "id": "2979087"}]}