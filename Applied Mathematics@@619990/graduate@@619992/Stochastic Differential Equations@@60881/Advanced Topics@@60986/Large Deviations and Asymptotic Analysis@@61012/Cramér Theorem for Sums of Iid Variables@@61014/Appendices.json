{"hands_on_practices": [{"introduction": "This first exercise provides a foundational workout in applying Cramér's theorem from the ground up. By working with a simple discrete distribution, you will practice the complete computational workflow: from calculating the moment generating and cumulant generating functions to executing the Legendre-Fenchel transform to find the rate function. This hands-on calculation is essential for building a concrete understanding of the mechanics behind the large deviation principle. [@problem_id:2972660]", "problem": "Consider a sequence of independent and identically distributed random variables $\\{X_{i}\\}_{i \\geq 1}$ taking values in $\\{-1,2\\}$ with $\\mathbb{P}(X_{i}=-1)=\\frac{1}{3}$ and $\\mathbb{P}(X_{i}=2)=\\frac{2}{3}$. Let $S_{n}=\\sum_{i=1}^{n} X_{i}$. Starting from the definitions of the moment generating function $M_{X}(t)=\\mathbb{E}[\\exp(t X)]$ and the cumulant generating function $\\Lambda(t)=\\ln \\mathbb{E}[\\exp(t X)]$, and using the foundational principle that the empirical mean of independent and identically distributed samples satisfies a Large Deviation Principle (LDP) with a convex rate function determined by the Legendre transform of $\\Lambda(t)$, determine the exact closed-form value of\n$$\n\\lim_{n \\to \\infty} \\frac{1}{n} \\ln \\mathbb{P}\\!\\left(\\frac{S_{n}}{n} \\geq \\frac{6}{5}\\right).\n$$\nExpress your final answer as a single closed-form analytic expression. No units are required, and no rounding is necessary.", "solution": "The user wants to find the value of the limit:\n$$\n\\lim_{n \\to \\infty} \\frac{1}{n} \\ln \\mathbb{P}\\!\\left(\\frac{S_{n}}{n} \\geq \\frac{6}{5}\\right)\n$$\nwhere $S_{n}=\\sum_{i=1}^{n} X_{i}$ and $\\{X_{i}\\}_{i \\geq 1}$ is a sequence of independent and identically distributed (i.i.d.) random variables taking values in $\\{-1, 2\\}$ with probabilities $\\mathbb{P}(X_{i}=-1)=\\frac{1}{3}$ and $\\mathbb{P}(X_{i}=2)=\\frac{2}{3}$.\n\nThis problem is a direct application of Cramér's theorem for large deviations. The theorem states that for a sequence of i.i.d. random variables $X_i$, the empirical mean $\\frac{S_n}{n}$ satisfies a Large Deviation Principle (LDP) with a convex rate function $I(x)$. Specifically, for any $a > \\mathbb{E}[X]$, the following limit holds:\n$$\n\\lim_{n \\to \\infty} \\frac{1}{n} \\ln \\mathbb{P}\\left(\\frac{S_n}{n} \\geq a\\right) = -\\inf_{x \\geq a} I(x)\n$$\nThe rate function $I(x)$ is the Legendre-Fenchel transform of the cumulant generating function (CGF) $\\Lambda(t)$, defined as:\n$$\nI(x) = \\sup_{t \\in \\mathbb{R}} \\{xt - \\Lambda(t)\\}\n$$\nwhere $\\Lambda(t) = \\ln M_X(t)$ and $M_X(t) = \\mathbb{E}[\\exp(tX)]$ is the moment generating function (MGF).\n\nFirst, we calculate the expected value of $X$ to verify the condition $a > \\mathbb{E}[X]$.\n$$\n\\mathbb{E}[X] = (-1) \\cdot \\mathbb{P}(X=-1) + (2) \\cdot \\mathbb{P}(X=2) = (-1) \\cdot \\frac{1}{3} + (2) \\cdot \\frac{2}{3} = -\\frac{1}{3} + \\frac{4}{3} = 1\n$$\nIn this problem, $a = \\frac{6}{5}$. Since $\\frac{6}{5} = 1.2 > 1$, the condition is met.\n\nThe rate function $I(x)$ is convex and has its minimum $I(\\mathbb{E}[X_1])=I(1)=0$. For $x > \\mathbb{E}[X]$, $I(x)$ is strictly increasing. Therefore, $\\inf_{x \\geq a} I(x) = I(a)$.\nThe limit we need to find is thus equal to $-I(\\frac{6}{5})$.\n\nWe proceed with the calculation in several steps:\n\n1.  **Compute the Moment Generating Function (MGF) $M_X(t)$**:\n    For a discrete random variable, the MGF is given by $M_X(t) = \\sum_{k} \\exp(tk) \\mathbb{P}(X=k)$.\n    $$\n    M_X(t) = \\exp(t(-1))\\mathbb{P}(X=-1) + \\exp(t(2))\\mathbb{P}(X=2) = \\frac{1}{3}\\exp(-t) + \\frac{2}{3}\\exp(2t)\n    $$\n\n2.  **Compute the Cumulant Generating Function (CGF) $\\Lambda(t)$**:\n    The CGF is the natural logarithm of the MGF.\n    $$\n    \\Lambda(t) = \\ln(M_X(t)) = \\ln\\left(\\frac{1}{3}\\exp(-t) + \\frac{2}{3}\\exp(2t)\\right)\n    $$\n\n3.  **Compute the rate function $I(\\frac{6}{5})$**:\n    The supremum in the definition of $I(x)$ is found at a value of $t$, let's call it $t_0$, where the derivative of $xt - \\Lambda(t)$ with respect to $t$ is zero. This gives the relation $x = \\Lambda'(t_0)$.\n    First, we find the derivative of $\\Lambda(t)$:\n    $$\n    \\Lambda'(t) = \\frac{M_X'(t)}{M_X(t)} = \\frac{\\frac{d}{dt}\\left(\\frac{1}{3}\\exp(-t) + \\frac{2}{3}\\exp(2t)\\right)}{\\frac{1}{3}\\exp(-t) + \\frac{2}{3}\\exp(2t)} = \\frac{-\\frac{1}{3}\\exp(-t) + \\frac{4}{3}\\exp(2t)}{\\frac{1}{3}\\exp(-t) + \\frac{2}{3}\\exp(2t)}\n    $$\n    Multiplying the numerator and denominator by $3$ simplifies this to:\n    $$\n    \\Lambda'(t) = \\frac{-\\exp(-t) + 4\\exp(2t)}{\\exp(-t) + 2\\exp(2t)}\n    $$\n    We need to find $t_0$ such that $\\Lambda'(t_0) = \\frac{6}{5}$. Let $u = \\exp(t_0)$. The equation becomes:\n    $$\n    \\frac{6}{5} = \\frac{-u^{-1} + 4u^2}{u^{-1} + 2u^2}\n    $$\n    Multiplying the numerator and denominator of the right side by $u$:\n    $$\n    \\frac{6}{5} = \\frac{-1 + 4u^3}{1 + 2u^3}\n    $$\n    Now, we solve for $u^3$:\n    $$\n    6(1 + 2u^3) = 5(-1 + 4u^3) \\\\\n    6 + 12u^3 = -5 + 20u^3 \\\\\n    11 = 8u^3 \\\\\n    u^3 = \\frac{11}{8}\n    $$\n    So, $u = \\exp(t_0) = \\left(\\frac{11}{8}\\right)^{1/3}$. This gives us the value for $t_0$:\n    $$\n    t_0 = \\ln\\left(\\left(\\frac{11}{8}\\right)^{1/3}\\right) = \\frac{1}{3}\\ln\\left(\\frac{11}{8}\\right) = \\frac{1}{3}(\\ln(11) - \\ln(8)) = \\frac{1}{3}(\\ln(11) - 3\\ln(2)) = \\frac{1}{3}\\ln(11) - \\ln(2)\n    $$\n    The rate function is then $I\\left(\\frac{6}{5}\\right) = \\frac{6}{5}t_0 - \\Lambda(t_0)$. We need to calculate $\\Lambda(t_0)$.\n    $$\n    \\Lambda(t_0) = \\ln\\left(M_X(t_0)\\right) = \\ln\\left(\\frac{1}{3}\\exp(-t_0) + \\frac{2}{3}\\exp(2t_0)\\right) = \\ln\\left(\\frac{1}{3}u^{-1} + \\frac{2}{3}u^2\\right)\n    $$\n    We have $u = \\left(\\frac{11}{8}\\right)^{1/3}$, so $u^{-1} = \\left(\\frac{8}{11}\\right)^{1/3} = \\frac{2}{11^{1/3}}$ and $u^2 = \\left(\\frac{11}{8}\\right)^{2/3} = \\frac{11^{2/3}}{4}$.\n    $$\n    \\Lambda(t_0) = \\ln\\left(\\frac{1}{3}\\frac{2}{11^{1/3}} + \\frac{2}{3}\\frac{11^{2/3}}{4}\\right) = \\ln\\left(\\frac{2}{3 \\cdot 11^{1/3}} + \\frac{11^{2/3}}{6}\\right)\n    $$\n    Finding a common denominator $6 \\cdot 11^{1/3}$:\n    $$\n    \\Lambda(t_0) = \\ln\\left(\\frac{4 + 11^{1/3} \\cdot 11^{2/3}}{6 \\cdot 11^{1/3}}\\right) = \\ln\\left(\\frac{4+11}{6 \\cdot 11^{1/3}}\\right) = \\ln\\left(\\frac{15}{6 \\cdot 11^{1/3}}\\right) = \\ln\\left(\\frac{5}{2 \\cdot 11^{1/3}}\\right)\n    $$\n    Using properties of logarithms:\n    $$\n    \\Lambda(t_0) = \\ln(5) - \\ln(2) - \\ln(11^{1/3}) = \\ln(5) - \\ln(2) - \\frac{1}{3}\\ln(11)\n    $$\n\n4.  **Calculate the final limit**:\n    The desired limit is $-I(\\frac{6}{5}) = -(\\frac{6}{5}t_0 - \\Lambda(t_0)) = \\Lambda(t_0) - \\frac{6}{5}t_0$.\n    Substituting the expressions for $\\Lambda(t_0)$ and $t_0$:\n    $$\n    \\lim_{n \\to \\infty} \\frac{1}{n} \\ln \\mathbb{P}\\!\\left(\\frac{S_{n}}{n} \\geq \\frac{6}{5}\\right) = \\left(\\ln(5) - \\ln(2) - \\frac{1}{3}\\ln(11)\\right) - \\frac{6}{5}\\left(\\frac{1}{3}\\ln(11) - \\ln(2)\\right)\n    $$\n    $$\n    = \\ln(5) - \\ln(2) - \\frac{1}{3}\\ln(11) - \\frac{6}{15}\\ln(11) + \\frac{6}{5}\\ln(2)\n    $$\n    $$\n    = \\ln(5) - \\ln(2) - \\frac{1}{3}\\ln(11) - \\frac{2}{5}\\ln(11) + \\frac{6}{5}\\ln(2)\n    $$\n    Grouping terms with the same logarithm:\n    $$\n    = \\ln(5) + \\left(\\frac{6}{5} - 1\\right)\\ln(2) + \\left(-\\frac{1}{3} - \\frac{2}{5}\\right)\\ln(11)\n    $$\n    $$\n    = \\ln(5) + \\frac{1}{5}\\ln(2) + \\left(-\\frac{5}{15} - \\frac{6}{15}\\right)\\ln(11)\n    $$\n    $$\n    = \\ln(5) + \\frac{1}{5}\\ln(2) - \\frac{11}{15}\\ln(11)\n    $$\nThis expression is the exact closed-form value of the limit.", "answer": "$$\n\\boxed{\\ln(5) + \\frac{1}{5}\\ln(2) - \\frac{11}{15}\\ln(11)}\n$$", "id": "2972660"}, {"introduction": "Having mastered the basic calculation of a large deviation probability, we now explore a more profound consequence of the theory. This problem investigates the typical behavior of a system *given* that a rare event has occurred, a concept central to statistical mechanics and information theory. You will use the principle of exponential tilting to determine how conditioning on an atypical sample mean effectively changes the underlying probability measure, providing a powerful tool for analyzing constrained stochastic systems. [@problem_id:2972671]", "problem": "Consider a sequence of independent and identically distributed random variables $X_1, X_2, \\dots, X_n$ with common density $p(x) = \\exp(-x)\\mathbf{1}_{\\{x \\ge 0\\}}$. Let $S_n = \\sum_{i=1}^{n} X_i$ and $M_n = S_n/n$. For a fixed $a > 0$, define the conditioning event $E_{n,\\varepsilon} = \\{|M_n - a| \\le \\varepsilon\\}$ with $\\varepsilon > 0$. You will analyze constraints and conditioning in the framework of the Large Deviations Principle (LDP) for sums of independent and identically distributed random variables, as captured by Cramér's theorem.\n\nStarting from the fundamental definitions of the moment generating function $M(\\lambda) = \\mathbb{E}[\\exp(\\lambda X_1)]$, cumulant generating function $\\Lambda(\\lambda) = \\ln M(\\lambda)$, and the Legendre-Fenchel transform, derive the canonical exponential tilting parameter $\\lambda^{\\ast}(a)$ that enforces the constraint $\\mathbb{E}_{\\lambda^{\\ast}}[X_1] = a$ under an exponentially tilted law with density $q_{\\lambda}(x) \\propto \\exp(\\lambda x) p(x)$. Using this, determine the asymptotic typical value of the empirical second moment under the conditioning, namely the limit\n$$\\lim_{\\varepsilon \\downarrow 0}\\lim_{n \\to \\infty} \\mathbb{E}\\left[\\frac{1}{n}\\sum_{i=1}^{n} X_i^{2} \\,\\middle|\\, E_{n,\\varepsilon} \\right].$$\n\nProvide a single closed-form analytic expression in terms of $a$ for this limit. No numerical approximation is required, and no physical units are involved.", "solution": "The problem statement will first be validated for scientific soundness, consistency, and completeness.\n\n### Step 1: Extract Givens\n- A sequence of independent and identically distributed (i.i.d.) random variables $X_1, X_2, \\dots, X_n$.\n- The common probability density function (PDF) is $p(x) = \\exp(-x)\\mathbf{1}_{\\{x \\ge 0\\}}$.\n- The sum is $S_n = \\sum_{i=1}^{n} X_i$.\n- The sample mean is $M_n = S_n/n$.\n- A fixed constant $a > 0$.\n- The conditioning event is $E_{n,\\varepsilon} = \\{|M_n - a| \\le \\varepsilon\\}$ with $\\varepsilon > 0$.\n- The moment generating function (MGF) is defined as $M(\\lambda) = \\mathbb{E}[\\exp(\\lambda X_1)]$.\n- The cumulant generating function (CGF) is defined as $\\Lambda(\\lambda) = \\ln M(\\lambda)$.\n- The exponentially tilted law has a density $q_{\\lambda}(x) \\propto \\exp(\\lambda x) p(x)$.\n- The tilting parameter $\\lambda^{\\ast}(a)$ is defined by the constraint $\\mathbb{E}_{\\lambda^{\\ast}}[X_1] = a$.\n- The objective is to determine the limit $\\lim_{\\varepsilon \\downarrow 0}\\lim_{n \\to \\infty} \\mathbb{E}\\left[\\frac{1}{n}\\sum_{i=1}^{n} X_i^{2} \\,\\middle|\\, E_{n,\\varepsilon} \\right]$.\n\n### Step 2: Validate Using Extracted Givens\n1.  **Scientific Grounding**: The problem is firmly rooted in the mathematical theory of large deviations, a well-established subfield of probability theory. All definitions, including those for the MGF, CGF, and exponential tilting, are standard. The specified distribution is the exponential distribution, a fundamental concept in statistics.\n2.  **Well-Posedness**: The problem is well-posed. It asks for the calculation of a specific limit that, according to the theory of large deviations for i.i.d. random variables (Cramér's theorem and its extensions), is expected to exist and be unique.\n3.  **Objectivity**: The problem is stated in precise, objective mathematical language, free from ambiguity or subjective claims.\n4.  **Completeness and Consistency**: The problem provides all necessary information to proceed with a solution. The condition $a > 0$ is consistent with the support of the exponential distribution, which is $[0, \\infty)$. There are no internal contradictions.\n\n### Step 3: Verdict and Action\nThe problem is valid. It is a standard application of Large Deviations Theory. A full solution will be provided.\n\n### Solution Derivation\n\nThe problem asks for the typical value of the empirical second moment of i.i.d. exponential random variables, conditioned on the rare event that their sample mean is approximately $a$. This is a canonical problem in Large Deviations Theory. The core principle is that conditioning on the rare event $\\{M_n \\approx a\\}$ effectively changes the underlying probability distribution of the individual random variables $X_i$ to an \"exponentially tilted\" distribution.\n\nFirst, we characterize the underlying distribution. The PDF is $p(x) = \\exp(-x)$ for $x \\ge 0$, which is the exponential distribution with rate parameter $1$. Let's denote this by $X \\sim \\text{Exp}(1)$.\n\nThe moment generating function (MGF) of $X_1$ is given by\n$$M(\\lambda) = \\mathbb{E}[\\exp(\\lambda X_1)] = \\int_{0}^{\\infty} \\exp(\\lambda x) \\exp(-x) \\,dx = \\int_{0}^{\\infty} \\exp(-(1-\\lambda)x) \\,dx$$\nThis integral converges for $1-\\lambda > 0$, i.e., $\\lambda < 1$. For such $\\lambda$, the MGF is\n$$M(\\lambda) = \\left[ -\\frac{1}{1-\\lambda} \\exp(-(1-\\lambda)x) \\right]_{0}^{\\infty} = 0 - \\left(-\\frac{1}{1-\\lambda}\\right) = \\frac{1}{1-\\lambda}$$\nThe cumulant generating function (CGF) is the natural logarithm of the MGF:\n$$\\Lambda(\\lambda) = \\ln M(\\lambda) = \\ln\\left(\\frac{1}{1-\\lambda}\\right) = -\\ln(1-\\lambda), \\quad \\text{for } \\lambda < 1$$\n\nNext, we analyze the exponentially tilted distribution. The tilted density $q_{\\lambda}(x)$ is defined as\n$$q_{\\lambda}(x) = \\frac{\\exp(\\lambda x) p(x)}{\\mathbb{E}[\\exp(\\lambda X_1)]} = \\frac{\\exp(\\lambda x) \\exp(-x)}{M(\\lambda)} = (1-\\lambda)\\exp(-(1-\\lambda)x)$$\nfor $x \\ge 0$. This is the PDF of an exponential distribution with a new rate parameter $\\theta_{\\lambda} = 1-\\lambda$.\n\nThe problem states that the tilting parameter $\\lambda^{\\ast}(a)$ is chosen to enforce the constraint that the mean under the tilted distribution is $a$. The expected value of a random variable $X_{\\lambda}$ with density $q_{\\lambda}(x)$ can be calculated directly from the CGF of the original distribution. A standard result is that $\\mathbb{E}_{\\lambda}[X_1] = \\Lambda'(\\lambda)$.\nLet's compute the derivative of the CGF:\n$$\\Lambda'(\\lambda) = \\frac{d}{d\\lambda}(-\\ln(1-\\lambda)) = - \\frac{-1}{1-\\lambda} = \\frac{1}{1-\\lambda}$$\nWe set this equal to $a$ to find $\\lambda^{\\ast}(a)$:\n$$\\Lambda'(\\lambda^{\\ast}) = \\frac{1}{1-\\lambda^{\\ast}} = a$$\nSolving for $\\lambda^{\\ast}$ gives $1 = a(1-\\lambda^{\\ast})$, which leads to $1 = a - a\\lambda^{\\ast}$, and thus\n$$\\lambda^{\\ast}(a) = \\frac{a-1}{a} = 1 - \\frac{1}{a}$$\nSince $a > 0$, we have $1/a > 0$, so $\\lambda^{\\ast}(a) = 1 - 1/a < 1$. This value is within the domain where the CGF is defined, so the tilting is well-defined.\n\nThe core result from Large Deviations Theory (often termed the \"conditional law of large numbers\" or as a consequence of Sanov's theorem) states that for a suitable function $f$,\n$$\\lim_{\\varepsilon \\downarrow 0}\\lim_{n \\to \\infty} \\mathbb{E}\\left[\\frac{1}{n}\\sum_{i=1}^{n} f(X_i) \\,\\middle|\\, |M_n - a| \\le \\varepsilon \\right] = \\mathbb{E}_{\\lambda^{\\ast}(a)}[f(X_1)]$$\nwhere $\\mathbb{E}_{\\lambda^{\\ast}(a)}[\\cdot]$ denotes the expectation with respect to the tilted distribution $q_{\\lambda^{\\ast}(a)}(x)$.\n\nIn this problem, the function is $f(x) = x^2$. We need to compute the second moment of the tilted distribution, $\\mathbb{E}_{\\lambda^{\\ast}(a)}[X_1^2]$. The moments of the tilted distribution can also be expressed in terms of the derivatives of the original CGF, $\\Lambda(\\lambda)$. The variance of the tilted distribution is given by $\\text{Var}_{\\lambda}(X_1) = \\Lambda''(\\lambda)$.\nThe second derivative of the CGF is:\n$$\\Lambda''(\\lambda) = \\frac{d}{d\\lambda}\\left(\\frac{1}{1-\\lambda}\\right) = \\frac{d}{d\\lambda}(1-\\lambda)^{-1} = (-1)(1-\\lambda)^{-2}(-1) = \\frac{1}{(1-\\lambda)^2}$$\nThe second moment under the tilted law, $\\mathbb{E}_{\\lambda}[X_1^2]$, is related to its mean and variance by the standard formula:\n$$\\mathbb{E}_{\\lambda}[X_1^2] = \\text{Var}_{\\lambda}(X_1) + (\\mathbb{E}_{\\lambda}[X_1])^2 = \\Lambda''(\\lambda) + (\\Lambda'(\\lambda))^2$$\nWe need to evaluate this expression at $\\lambda = \\lambda^{\\ast}(a)$. By definition of $\\lambda^{\\ast}(a)$, we have $\\Lambda'(\\lambda^{\\ast}(a)) = a$.\nNext, we evaluate the second derivative at $\\lambda^{\\ast}(a)$:\n$$\\Lambda''(\\lambda^{\\ast}(a)) = \\frac{1}{(1-\\lambda^{\\ast}(a))^2}$$\nFrom the condition $\\Lambda'(\\lambda^{\\ast}(a)) = 1/(1-\\lambda^{\\ast}(a)) = a$, we can substitute this into the expression for $\\Lambda''(\\lambda^{\\ast}(a))$:\n$$\\Lambda''(\\lambda^{\\ast}(a)) = \\left(\\frac{1}{1-\\lambda^{\\ast}(a)}\\right)^2 = a^2$$\nNow, we can assemble the final result for the second moment under the tilted measure:\n$$\\mathbb{E}_{\\lambda^{\\ast}(a)}[X_1^2] = \\Lambda''(\\lambda^{\\ast}(a)) + (\\Lambda'(\\lambda^{\\ast}(a)))^2 = a^2 + a^2 = 2a^2$$\nTherefore, the desired limit is $2a^2$.\nAs a consistency check, note that the unconditional mean of $X_1$ is $\\mathbb{E}[X_1] = \\Lambda'(0) = 1$. If we set $a=1$, we are conditioning on the typical behavior. The formula gives $2(1)^2=2$. The unconditional second moment is $\\mathbb{E}[X_1^2] = \\Lambda''(0) + (\\Lambda'(0))^2 = 1/(1-0)^2 + (1/(1-0))^2 = 1+1=2$. The result is consistent.\nSo, the limit is:\n$$\\lim_{\\varepsilon \\downarrow 0}\\lim_{n \\to \\infty} \\mathbb{E}\\left[\\frac{1}{n}\\sum_{i=1}^{n} X_i^{2} \\,\\middle|\\, E_{n,\\varepsilon} \\right] = 2a^2$$\nThis expression gives the typical value of the empirical second moment given that the empirical mean is constrained to be $a$.", "answer": "$$\n\\boxed{2a^2}\n$$", "id": "2972671"}, {"introduction": "Our final practice bridges the gap between abstract theory and concrete computation, a crucial skill for any applied mathematician or scientist. This exercise guides you to derive the well-known rate functions for several fundamental probability distributions and then implement them in a program to evaluate specific numerical instances. By translating the Legendre-Fenchel transform into executable code for common cases, you will solidify your understanding and gain a practical tool for quantitative analysis of large deviations. [@problem_id:2972677]", "problem": "Consider a sequence of independent and identically distributed (i.i.d.) random variables $X_1, X_2, \\dots$ and their empirical mean $S_n/n$, where $S_n = \\sum_{i=1}^n X_i$. In the context of the Large Deviation Principle (LDP) for sums of i.i.d. variables, and motivated by the Euler–Maruyama discretization of a Stochastic Differential Equation (SDE), the increments associated with the SDE $dX_t = a\\,dt + b\\,dW_t$ over a fixed time step $\\Delta t$ are i.i.d. Gaussian with mean $a\\,\\Delta t$ and variance $b^2\\,\\Delta t$. For such and other reference distributions with a well-defined moment generating function near $0$, the large deviations rate function for the empirical mean exists and is finite on its effective domain.\n\nTask: Starting only from foundational definitions, derive a numerically computable expression for the large deviations rate that governs $S_n/n$ for each specified distribution, and implement it to evaluate concrete instances. Begin from the definition of the moment generating function $M(\\lambda) = \\mathbb{E}[e^{\\lambda X}]$ and its logarithm, the cumulant generating function $\\Lambda(\\lambda) = \\log M(\\lambda)$, together with basic convex duality and the principle of exponential tilting. Use these to obtain a numerically tractable characterization of the large deviations rate for the empirical mean.\n\nYou must write a complete program to compute the large deviations rate for the following test suite. Each test case specifies a distribution and the point $x$ at which to evaluate the rate. All parameters are dimensionless. Your program must produce a single line of output containing the rate values for each test case, in order, as a comma-separated list enclosed in square brackets.\n\nTest suite:\n- Case 1 (Gaussian increments from an SDE Euler step): $a = 0.2$, $b = 0.7$, $\\Delta t = 0.05$, $x = 0.11$.\n- Case 2 (Gaussian boundary check): $a = 0.2$, $b = 0.7$, $\\Delta t = 0.05$, $x = a\\,\\Delta t$.\n- Case 3 (Bernoulli interior): parameter $p = 0.3$, $x = 0.5$.\n- Case 4 (Bernoulli boundary): parameter $p = 0.3$, $x = 0$.\n- Case 5 (Exponential with rate $\\beta$): parameter $\\beta = 2.0$, $x = 0.8$.\n\nOutput specification:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for example, \"[result1,result2,result3,result4,result5]\".\n- Each result must be a real number (float) representing the evaluated rate at the given $x$ for the corresponding test case.", "solution": "The problem is assessed as valid. It is scientifically grounded in the principles of large deviation theory, specifically Cramér's theorem, and is mathematically well-posed. All parameters and distributions are clearly defined, allowing for a unique and meaningful solution.\n\nThe core of the problem is to find the large deviation rate function, $I(x)$, for the empirical mean of a sequence of independent and identically distributed (i.i.d.) random variables. According to Cramér's theorem, if the moment generating function (MGF) $M(\\lambda) = \\mathbb{E}[e^{\\lambda X}]$ is finite in a neighborhood of $\\lambda=0$, the rate function $I(x)$ is given by the Legendre-Fenchel transform of the cumulant generating function (CGF), $\\Lambda(\\lambda) = \\log M(\\lambda)$.\n\nThe Legendre-Fenchel transform is defined as:\n$$\nI(x) = \\sup_{\\lambda \\in \\mathbb{R}} \\{\\lambda x - \\Lambda(\\lambda)\\}\n$$\nA key property of the CGF is its convexity. This allows the supremum to be found by setting the derivative of the expression with respect to $\\lambda$ to zero:\n$$\n\\frac{d}{d\\lambda} (\\lambda x - \\Lambda(\\lambda)) = x - \\Lambda'(\\lambda) = 0\n$$\nThis yields the critical equation $\\Lambda'(\\lambda) = x$. We solve this equation for $\\lambda$, let the solution be $\\lambda_x$. The rate function is then obtained by substituting this value back into the original expression:\n$$\nI(x) = \\lambda_x x - \\Lambda(\\lambda_x)\n$$\nWe apply this procedure to each distribution specified in the test suite.\n\n### Case 1 & 2: Gaussian Distribution\nThe random variable $X$ represents the increment of an SDE $dX_t = a\\,dt + b\\,dW_t$ over a time step $\\Delta t$. This increment follows a Gaussian distribution, $X \\sim \\mathcal{N}(\\mu, \\sigma^2)$, with mean $\\mu = a\\,\\Delta t$ and variance $\\sigma^2 = b^2\\,\\Delta t$.\n\nThe MGF of a Gaussian random variable is $M(\\lambda) = e^{\\lambda \\mu + \\frac{1}{2}\\lambda^2 \\sigma^2}$.\nThe CGF is $\\Lambda(\\lambda) = \\log M(\\lambda) = \\lambda \\mu + \\frac{1}{2}\\lambda^2 \\sigma^2$.\nThe derivative of the CGF is $\\Lambda'(\\lambda) = \\mu + \\lambda \\sigma^2$.\n\nSolving $\\Lambda'(\\lambda) = x$:\n$$\n\\mu + \\lambda \\sigma^2 = x \\implies \\lambda_x = \\frac{x - \\mu}{\\sigma^2}\n$$\nSubstituting $\\lambda_x$ into the expression for $I(x)$:\n$$\nI(x) = \\left(\\frac{x - \\mu}{\\sigma^2}\\right)x - \\left(\\left(\\frac{x - \\mu}{\\sigma^2}\\right)\\mu + \\frac{1}{2}\\left(\\frac{x - \\mu}{\\sigma^2}\\right)^2\\sigma^2\\right)\n$$\n$$\nI(x) = \\frac{x(x - \\mu)}{\\sigma^2} - \\frac{\\mu(x - \\mu)}{\\sigma^2} - \\frac{(x - \\mu)^2}{2\\sigma^2} = \\frac{(x - \\mu)^2}{\\sigma^2} - \\frac{(x - \\mu)^2}{2\\sigma^2}\n$$\n$$\nI(x) = \\frac{(x - \\mu)^2}{2\\sigma^2}\n$$\nFor Case $1$, the parameters are $a = 0.2$, $b = 0.7$, $\\Delta t = 0.05$, and $x = 0.11$. Thus, $\\mu = 0.2 \\times 0.05 = 0.01$ and $\\sigma^2 = 0.7^2 \\times 0.05 = 0.0245$.\nFor Case $2$, $x = a\\,\\Delta t = \\mu$, which immediately implies $I(x) = 0$.\n\n### Case 3 & 4: Bernoulli Distribution\nThe random variable $X$ follows a Bernoulli distribution, $X \\sim \\text{Bernoulli}(p)$, where $P(X=1) = p$ and $P(X=0) = 1-p$. The effective domain for the mean $x$ is the interval $[0, 1]$.\n\nThe MGF is $M(\\lambda) = \\mathbb{E}[e^{\\lambda X}] = p e^{\\lambda \\cdot 1} + (1-p)e^{\\lambda \\cdot 0} = p e^\\lambda + 1-p$.\nThe CGF is $\\Lambda(\\lambda) = \\log(p e^\\lambda + 1-p)$.\nThe derivative is $\\Lambda'(\\lambda) = \\frac{p e^\\lambda}{p e^\\lambda + 1-p}$.\n\nFor $x \\in (0, 1)$, we solve $\\Lambda'(\\lambda) = x$:\n$$\n\\frac{p e^\\lambda}{p e^\\lambda + 1-p} = x \\implies p e^\\lambda = x(p e^\\lambda + 1-p) \\implies e^\\lambda(p - xp) = x(1-p) \\implies e^\\lambda = \\frac{x(1-p)}{p(1-x)}\n$$\nThis gives $\\lambda_x = \\log\\left(\\frac{x(1-p)}{p(1-x)}\\right)$. Substituting into the expression for $I(x)$:\n$$\nI(x) = x \\log\\left(\\frac{x(1-p)}{p(1-x)}\\right) - \\log\\left(p \\frac{x(1-p)}{p(1-x)} + 1-p\\right)\n$$\n$$\nI(x) = x \\log\\left(\\frac{x}{p}\\right) + x \\log\\left(\\frac{1-p}{1-x}\\right) - \\log\\left(\\frac{x(1-p) + (1-p)(1-x)}{1-x}\\right)\n$$\n$$\nI(x) = x \\log\\left(\\frac{x}{p}\\right) + x \\log\\left(\\frac{1-p}{1-x}\\right) - \\log\\left(\\frac{1-p}{1-x}\\right) = x \\log\\left(\\frac{x}{p}\\right) + (x-1)\\log\\left(\\frac{1-p}{1-x}\\right)\n$$\n$$\nI(x) = x \\log\\left(\\frac{x}{p}\\right) + (1-x)\\log\\left(\\frac{1-x}{1-p}\\right)\n$$\nThis is the Kullback-Leibler divergence, $D_{\\text{KL}}(\\text{Bernoulli}(x) || \\text{Bernoulli}(p))$.\n\nFor the boundary Case $4$, where $x=0$, the above formula is not directly applicable. We must evaluate $I(0) = \\sup_{\\lambda} \\{0 \\cdot \\lambda - \\Lambda(\\lambda)\\} = -\\inf_{\\lambda} \\Lambda(\\lambda)$.\nSince $\\Lambda(\\lambda) = \\log(p e^\\lambda + 1-p)$ is an increasing function of $\\lambda$, its infimum occurs as $\\lambda \\to -\\infty$.\n$$\n\\inf_{\\lambda} \\Lambda(\\lambda) = \\lim_{\\lambda \\to -\\infty} \\log(p e^\\lambda + 1-p) = \\log(1-p)\n$$\nThus, for $x=0$, the rate is $I(0) = -\\log(1-p)$. This is consistent with the limit of the general formula as $x \\to 0$.\n\n### Case 5: Exponential Distribution\nThe random variable $X$ follows an Exponential distribution with rate $\\beta$, $X \\sim \\text{Exponential}(\\beta)$. The PDF is $f(x) = \\beta e^{-\\beta x}$ for $x \\ge 0$. The mean is $\\mathbb{E}[X] = 1/\\beta$.\n\nThe MGF is $M(\\lambda) = \\mathbb{E}[e^{\\lambda X}] = \\int_0^\\infty e^{\\lambda x} \\beta e^{-\\beta x} dx = \\frac{\\beta}{\\beta-\\lambda}$, which converges for $\\lambda < \\beta$.\nThe CGF is $\\Lambda(\\lambda) = \\log(\\beta) - \\log(\\beta-\\lambda)$.\nThe derivative is $\\Lambda'(\\lambda) = \\frac{1}{\\beta-\\lambda}$.\n\nSolving $\\Lambda'(\\lambda) = x$ for $x > 0$:\n$$\n\\frac{1}{\\beta-\\lambda} = x \\implies 1 = x(\\beta-\\lambda) \\implies \\lambda_x = \\beta - \\frac{1}{x}\n$$\nThe condition $\\lambda_x < \\beta$ is satisfied for $x > 0$. Substituting $\\lambda_x$ to find $I(x)$:\n$$\nI(x) = \\left(\\beta - \\frac{1}{x}\\right)x - \\left(\\log(\\beta) - \\log\\left(\\beta - \\left(\\beta - \\frac{1}{x}\\right)\\right)\\right)\n$$\n$$\nI(x) = \\beta x - 1 - (\\log(\\beta) - \\log(1/x)) = \\beta x - 1 - \\log(\\beta) - \\log(x)\n$$\n$$\nI(x) = \\beta x - \\log(\\beta x) - 1\n$$\nThis formula applies to Case $5$ with $\\beta = 2.0$ and $x = 0.8$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the large deviations rate function for specified distributions and points.\n    \"\"\"\n\n    def rate_gaussian(params):\n        \"\"\"\n        Computes the rate function I(x) for a Gaussian distribution.\n        I(x) = (x - mu)^2 / (2 * sigma^2)\n        \"\"\"\n        a, b, dt, x = params['a'], params['b'], params['dt'], params['x']\n        mu = a * dt\n        var = b**2 * dt\n        if var == 0:\n            return 0.0 if x == mu else np.inf\n        return (x - mu)**2 / (2 * var)\n\n    def rate_bernoulli(params):\n        \"\"\"\n        Computes the rate function I(x) for a Bernoulli distribution.\n        This is the Kullback-Leibler divergence D_KL(Bernoulli(x) || Bernoulli(p)).\n        I(x) = x*log(x/p) + (1-x)*log((1-x)/(1-p))\n        \"\"\"\n        p, x = params['p'], params['x']\n        \n        # Validate inputs to avoid division by zero or log of non-positive numbers\n        if p <= 0 or p >= 1:\n            raise ValueError(\"Bernoulli parameter p must be in (0, 1).\")\n        if x < 0 or x > 1:\n            # The rate is infinite outside the support of the sample mean.\n            return np.inf\n\n        # Handle boundary cases for x to avoid numerical issues (e.g., 0*log(0))\n        if x == p:\n            return 0.0\n        if x == 0:\n            return -np.log(1 - p)\n        if x == 1:\n            return -np.log(p)\n            \n        # General case for x in (0, 1)\n        term1 = x * np.log(x / p)\n        term2 = (1 - x) * np.log((1 - x) / (1 - p))\n        return term1 + term2\n\n    def rate_exponential(params):\n        \"\"\"\n        Computes the rate function I(x) for an Exponential distribution.\n        I(x) = beta*x - log(beta*x) - 1\n        \"\"\"\n        beta, x = params['beta'], params['x']\n        \n        if beta <= 0:\n            raise ValueError(\"Exponential rate parameter beta must be positive.\")\n        if x <= 0:\n            # The rate is infinite for x outside the support (0, inf).\n            return np.inf\n        \n        if x == 1.0 / beta:\n            return 0.0\n        \n        return beta * x - np.log(beta * x) - 1\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1 (Gaussian)\n        {'type': 'gaussian', 'params': {'a': 0.2, 'b': 0.7, 'dt': 0.05, 'x': 0.11}},\n        # Case 2 (Gaussian)\n        {'type': 'gaussian', 'params': {'a': 0.2, 'b': 0.7, 'dt': 0.05, 'x': 0.2 * 0.05}},\n        # Case 3 (Bernoulli)\n        {'type': 'bernoulli', 'params': {'p': 0.3, 'x': 0.5}},\n        # Case 4 (Bernoulli)\n        {'type': 'bernoulli', 'params': {'p': 0.3, 'x': 0.0}},\n        # Case 5 (Exponential)\n        {'type': 'exponential', 'params': {'beta': 2.0, 'x': 0.8}},\n    ]\n\n    dispatch = {\n        'gaussian': rate_gaussian,\n        'bernoulli': rate_bernoulli,\n        'exponential': rate_exponential,\n    }\n\n    results = []\n    for case in test_cases:\n        case_type = case['type']\n        case_params = case['params']\n        \n        if case_type in dispatch:\n            result = dispatch[case_type](case_params)\n            results.append(result)\n        else:\n            raise ValueError(f\"Unknown case type: {case_type}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2972677"}]}