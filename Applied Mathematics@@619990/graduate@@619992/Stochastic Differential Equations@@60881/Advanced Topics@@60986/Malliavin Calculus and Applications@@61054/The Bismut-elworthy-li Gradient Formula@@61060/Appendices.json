{"hands_on_practices": [{"introduction": "This first exercise solidifies the conceptual distinction between spatial sensitivity, represented by the semigroup gradient $\\nabla_x P_t f(x)$, and pathwise sensitivity to noise, captured by the Malliavin derivative $D_s X_t^x$. Understanding the unique nature and mathematical representation of each derivative is the critical first step toward appreciating how the Bismut-Elworthy-Li formula masterfully connects these two seemingly disparate concepts. This practice ([@problem_id:2999698]) lays the essential groundwork for everything that follows.", "problem": "Consider a $d$-dimensional Stochastic Differential Equation (SDE) driven by an $m$-dimensional standard Brownian motion $W$,\n$$\n\\mathrm{d}X_t^x \\;=\\; b\\!\\left(X_t^x\\right)\\,\\mathrm{d}t \\;+\\; \\sigma\\!\\left(X_t^x\\right)\\,\\mathrm{d}W_t, \n\\qquad X_0^x \\;=\\; x \\in \\mathbb{R}^d,\n$$\nwhere $b:\\mathbb{R}^d\\to\\mathbb{R}^d$ and $\\sigma:\\mathbb{R}^d\\to\\mathbb{R}^{d\\times m}$ are $\\mathcal{C}^2$ with bounded derivatives, and $\\sigma$ is uniformly nondegenerate in the sense that there exists $c>0$ such that for all $y\\in\\mathbb{R}^d$, $\\xi\\in\\mathbb{R}^d$, one has $\\langle \\xi,\\, \\sigma(y)\\sigma(y)^\\top \\xi\\rangle \\ge c \\lVert \\xi\\rVert^2$. Let the Markov semigroup be defined by\n$$\nP_t f(x)\\;=\\;\\mathbb{E}\\big[f\\!\\left(X_t^x\\right)\\big]\n$$\nfor $f\\in\\mathcal{C}_b^1(\\mathbb{R}^d)$. The spatial gradient $\\nabla_x P_t f(x)$ quantifies the sensitivity of the semigroup with respect to the initial condition $x$. The Malliavin derivative $D_s X_t^x$ (for $0\\le s\\le t$) quantifies the sensitivity of the state $X_t^x$ with respect to perturbations of the driving noise at time $s$ along Cameron–Martin directions.\n\nDenote by $J_{r,t}^x$ the Jacobian flow of spatial derivatives, i.e., the $d\\times d$ matrix of partial derivatives $J_{r,t}^x := \\partial X_t^x / \\partial X_r^x$, well-defined under the above smoothness assumptions, with $J_{t,t}^x = I_d$.\n\nSelect all statements that are correct and that clearly distinguish the spatial gradient $\\nabla_x P_t f(x)$ from the Malliavin derivative $D_s X_t^x$, and explain their relationship that underlies the Bismut–Elworthy–Li gradient methodology:\n\nA. For any $v\\in\\mathbb{R}^d$,\n$$\n\\big\\langle \\nabla_x P_t f(x),\\, v \\big\\rangle \\;=\\; \\mathbb{E}\\!\\Big[\\, \\big\\langle \\nabla f\\!\\left(X_t^x\\right),\\, J_{0,t}^x\\, v \\big\\rangle \\,\\Big],\n$$\nwhich shows that $\\nabla_x P_t f(x)$ is a spatial (initial-condition) sensitivity.\n\nB. For $s>t$ one has $D_s X_t^x=0$, while for $0\\le s\\le t$ one has\n$$\nD_s X_t^x \\;=\\; J_{s,t}^x\\, \\sigma\\!\\left(X_s^x\\right),\n$$\nso $D_s X_t^x$ measures the functional derivative with respect to the noise injected at time $s$.\n\nC. Under uniform ellipticity of $\\sigma$, an integration-by-parts argument in Malliavin calculus provides a representation of $\\nabla_x P_t f(x)$ as an expectation of $f\\!\\left(X_t^x\\right)$ multiplied by an adapted stochastic integral depending on $\\{J_{r,t}^x,\\,\\sigma(X_r^x)\\}_{0\\le r\\le t}$, reflecting how spatial sensitivity can be expressed through controlled noise sensitivities.\n\nD. The gradient $\\nabla_x P_t f(x)$ equals the Malliavin derivative $D_s X_t^x$ for every $s\\in[0,t]$.\n\nE. The Malliavin derivative $D_s X_t^x$ coincides with the spatial derivative $J_{0,t}^x$ when the drift $b$ is constant.\n\nChoose all correct options.", "solution": "The problem statement is first validated according to the prescribed protocol.\n\n### Step 1: Extract Givens\n-   The $d$-dimensional stochastic process $X_t^x$ is defined by the Stochastic Differential Equation (SDE):\n    $$\n    \\mathrm{d}X_t^x \\;=\\; b\\!\\left(X_t^x\\right)\\,\\mathrm{d}t \\;+\\; \\sigma\\!\\left(X_t^x\\right)\\,\\mathrm{d}W_t, \\qquad X_0^x \\;=\\; x \\in \\mathbb{R}^d.\n    $$\n-   $W_t$ is an $m$-dimensional standard Brownian motion.\n-   The drift coefficient $b:\\mathbb{R}^d\\to\\mathbb{R}^d$ and diffusion coefficient $\\sigma:\\mathbb{R}^d\\to\\mathbb{R}^{d\\times m}$ are of class $\\mathcal{C}^2$ with bounded derivatives.\n-   The diffusion coefficient $\\sigma$ is uniformly non-degenerate: there exists a constant $c>0$ such that for all $y\\in\\mathbb{R}^d$ and $\\xi\\in\\mathbb{R}^d$, the inequality $\\langle \\xi,\\, \\sigma(y)\\sigma(y)^\\top \\xi\\rangle \\ge c \\lVert \\xi\\rVert^2$ holds. This is a condition of uniform ellipticity for the matrix $a(y) = \\sigma(y)\\sigma(y)^\\top$.\n-   The Markov semigroup is defined as $P_t f(x)\\;=\\;\\mathbb{E}\\big[f\\!\\left(X_t^x\\right)\\big]$ for any test function $f\\in\\mathcal{C}_b^1(\\mathbb{R}^d)$.\n-   The spatial gradient is denoted $\\nabla_x P_t f(x)$.\n-   The Malliavin derivative is denoted $D_s X_t^x$ for $0\\le s\\le t$.\n-   The Jacobian flow is a $d\\times d$ matrix denoted $J_{r,t}^x := \\partial X_t^x / \\partial X_r^x$, which represents the matrix of partial derivatives of the solution at time $t$ with respect to its state at time $r$. The problem specifies $J_{t,t}^x = I_d$, where $I_d$ is the $d \\times d$ identity matrix. The notation $J_{0,t}^x$ represents the sensitivity of $X_t^x$ to the initial condition $x$, i.e., $J_{0,t}^x = \\nabla_x X_t^x$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem statement is scientifically sound and well-posed.\n-   **Scientifically Grounded:** The entire setup is standard in the modern theory of stochastic analysis, specifically in the context of Malliavin calculus and its applications to SDEs. The assumptions on the coefficients $b$ and $\\sigma$ (smoothness, bounded derivatives, uniform ellipticity) are classical conditions ensuring the existence, uniqueness, and regularity of the SDE solution and its derivatives with respect to both initial conditions and the noise path.\n-   **Well-Posed:** The question asks to evaluate the correctness of several mathematical statements concerning well-defined objects. This is an unambiguous and well-posed task.\n-   **Objective:** The problem is formulated in precise mathematical language, free from any subjective or ambiguous terminology.\n\nThe problem does not exhibit any flaws such as scientific unsoundness, incompleteness, contradiction, or ambiguity. It is a standard and valid problem in advanced probability theory.\n\n### Step 3: Verdict and Action\nThe problem statement is **valid**. A full solution will be derived by analyzing each option.\n\n### Analysis of Options\n\n**Option A:** For any $v\\in\\mathbb{R}^d$,\n$$\n\\big\\langle \\nabla_x P_t f(x),\\, v \\big\\rangle \\;=\\; \\mathbb{E}\\!\\Big[\\, \\big\\langle \\nabla f\\!\\left(X_t^x\\right),\\, J_{0,t}^x\\, v \\big\\rangle \\,\\Big],\n$$\nwhich shows that $\\nabla_x P_t f(x)$ is a spatial (initial-condition) sensitivity.\n\nThis statement is a direct consequence of the definition of the gradient and the chain rule, applied under the expectation. The directional derivative of $P_t f(x)$ in the direction $v \\in \\mathbb{R}^d$ is given by\n$$\n\\big\\langle \\nabla_x P_t f(x),\\, v \\big\\rangle \\;=\\; \\frac{\\mathrm{d}}{\\mathrm{d}\\epsilon}\\bigg|_{\\epsilon=0} P_t f(x+\\epsilon v) \\;=\\; \\frac{\\mathrm{d}}{\\mathrm{d}\\epsilon}\\bigg|_{\\epsilon=0} \\mathbb{E}\\big[f(X_t^{x+\\epsilon v})\\big].\n$$\nThe assumed smoothness conditions on $b$ and $\\sigma$ are sufficient to justify interchanging the differentiation and expectation operators (dominated convergence theorem on the derivative).\n$$\n\\big\\langle \\nabla_x P_t f(x),\\, v \\big\\rangle \\;=\\; \\mathbb{E}\\bigg[\\frac{\\mathrm{d}}{\\mathrm{d}\\epsilon}\\bigg|_{\\epsilon=0} f(X_t^{x+\\epsilon v})\\bigg].\n$$\nApplying the chain rule to the function $f$, we get\n$$\n\\frac{\\mathrm{d}}{\\mathrm{d}\\epsilon}\\bigg|_{\\epsilon=0} f(X_t^{x+\\epsilon v}) \\;=\\; \\bigg\\langle \\nabla f(X_t^x),\\, \\frac{\\mathrm{d}}{\\mathrm{d}\\epsilon}\\bigg|_{\\epsilon=0} X_t^{x+\\epsilon v} \\bigg\\rangle.\n$$\nThe term $\\frac{\\mathrm{d}}{\\mathrm{d}\\epsilon}|_{\\epsilon=0} X_t^{x+\\epsilon v}$ is the directional derivative of $X_t^x$ with respect to the initial condition $x$ in the direction $v$. This is, by definition, $(\\nabla_x X_t^x)v$. Using the problem's notation $J_{0,t}^x = \\nabla_x X_t^x$, this is $J_{0,t}^x v$.\nSubstituting this back, we obtain\n$$\n\\big\\langle \\nabla_x P_t f(x),\\, v \\big\\rangle \\;=\\; \\mathbb{E}\\Big[\\big\\langle \\nabla f(X_t^x),\\, J_{0,t}^x v \\big\\rangle\\Big].\n$$\nThis confirms the mathematical identity. The interpretation is also correct: the formula relates the sensitivity of the expectation $\\mathbb{E}[f(X_t^x)]$ to the initial condition $x$ with the expected sensitivity of $f$ to the propagated initial perturbation $J_{0,t}^x v$.\nVerdict: **Correct**.\n\n**Option B:** For $s>t$ one has $D_s X_t^x=0$, while for $0\\le s\\le t$ one has\n$$\nD_s X_t^x \\;=\\; J_{s,t}^x\\, \\sigma\\!\\left(X_s^x\\right),\n$$\nso $D_s X_t^x$ measures the functional derivative with respect to the noise injected at time $s$.\n\nLet's analyze this statement in two parts.\n1.  The property $D_s X_t^x=0$ for $s>t$ is fundamental. The Itô process $X_t^x$ is adapted to the filtration generated by the Brownian motion $\\{W_r\\}_{0 \\le r \\le t}$. Thus, its value at time $t$ does not depend on the increments of the Brownian motion after time $t$. The Malliavin derivative $D_s$ measures the sensitivity to a perturbation of the path of $W$ at time $s$, so if $s>t$, the perturbation has no effect on $X_t^x$. This part is correct.\n2.  For $0 \\le s \\le t$, we must verify the identity $D_s X_t^x = J_{s,t}^x\\, \\sigma(X_s^x)$. Both $D_s X_t^x$ and $J_{s,t}^x$ (as functions of $t$, for a fixed $s$) are solutions to linear SDEs for $t>s$.\n    The Jacobian flow $Y_t = J_{s,t}^x$ satisfies the variational equation for $t>s$:\n    $$\n    \\mathrm{d}Y_t = \\nabla b(X_t^x) Y_t \\mathrm{d}t + \\sum_{k=1}^m \\nabla\\sigma_k(X_t^x) Y_t \\mathrm{d}W_t^k, \\quad \\text{with} \\quad Y_s = J_{s,s}^x = I_d.\n    $$\n    The Malliavin derivative $Z_t = D_s X_t^x$ satisfies the same linear SDE for $t>s$. Its initial condition at $t=s$ is given by the coefficient of $\\mathrm{d}W_t$ in the SDE for $X_t^x$, evaluated at time $s$. From the SDE for $X_t^x$, we have $X_t^x = x + \\int_0^t b(X_r^x)\\mathrm{d}r + \\int_0^t \\sigma(X_r^x)\\mathrm{d}W_r$. Applying the operator $D_s$ gives $D_s X_t^x = \\sigma(X_s^x) + \\int_s^t (\\dots) \\mathrm{d}r + \\int_s^t (\\dots) \\mathrm{d}W_r$ for $t \\ge s$. In differential form, the initial condition is $Z_s = D_s X_s^x = \\sigma(X_s^x)$.\n    Now consider the process $U_t = J_{s,t}^x \\sigma(X_s^x)$ for $t \\ge s$. Since $\\sigma(X_s^x)$ is $\\mathcal{F}_s$-measurable, for $t>s$, $U_t$ follows the same SDE as $J_{s,t}^x$:\n    $$\n    \\mathrm{d}U_t = \\nabla b(X_t^x) U_t \\mathrm{d}t + \\sum_{k=1}^m \\nabla\\sigma_k(X_t^x) U_t \\mathrm{d}W_t^k.\n    $$\n    The initial condition for $U_t$ at $t=s$ is $U_s = J_{s,s}^x \\sigma(X_s^x) = I_d \\sigma(X_s^x) = \\sigma(X_s^x)$.\n    Since $Z_t=D_s X_t^x$ and $U_t=J_{s,t}^x \\sigma(X_s^x)$ satisfy the same linear SDE for $t>s$ and have the same initial condition at $t=s$, by uniqueness of solutions, they must be equal for all $t \\ge s$. The interpretation of $D_s X_t^x$ is also correct.\nVerdict: **Correct**.\n\n**Option C:** Under uniform ellipticity of $\\sigma$, an integration-by-parts argument in Malliavin calculus provides a representation of $\\nabla_x P_t f(x)$ as an expectation of $f\\!\\left(X_t^x\\right)$ multiplied by an adapted stochastic integral depending on $\\{J_{r,t}^x,\\,\\sigma(X_r^x)\\}_{0\\le r\\le t}$, reflecting how spatial sensitivity can be expressed through controlled noise sensitivities.\n\nThis statement is a qualitative but accurate description of the Bismut–Elworthy–Li gradient formula. The goal is to compute $\\nabla_x P_t f(x) = \\mathbb{E}[(\\nabla_x X_t^x)^\\top \\nabla f(X_t^x)]$. The term $\\nabla f(X_t^x)$ makes this formula unsuitable for Monte Carlo simulation if $f$ is not smooth or if its derivative is unknown. The Bismut–Elworthy–Li methodology uses Malliavin integration by parts (or an equivalent Girsanov transform argument) to \"move\" the derivative from $f$ onto a stochastic weight. The result is a formula of the form\n$$\n\\nabla_x P_t f(x) = \\mathbb{E}\\big[ f(X_t^x) W_t \\big],\n$$\nwhere $W_t$ is a random variable, often called the Malliavin weight, which is typically a stochastic integral. The construction of this weight $W_t$ explicitly involves the Jacobian flow $J_{0,r}^x$ and the diffusion coefficient $\\sigma(X_r^x)$ over the time interval $[0,t]$. The uniform ellipticity condition on $\\sigma$ is crucial as it guarantees the invertibility of the matrix $\\sigma(y)\\sigma(y)^\\top$, which is required in the construction of the weight. The final interpretation, that \"spatial sensitivity can be expressed through controlled noise sensitivities\", correctly captures the essence of the method: the spatial gradient $\\nabla_x$ is replaced by an operation related to the Malliavin derivative, which quantifies sensitivity to noise.\nVerdict: **Correct**.\n\n**Option D:** The gradient $\\nabla_x P_t f(x)$ equals the Malliavin derivative $D_s X_t^x$ for every $s\\in[0,t]$.\n\nThis statement is fundamentally flawed. Let's compare the two objects:\n-   $\\nabla_x P_t f(x)$: This is the gradient of a real-valued function $P_t f$ at point $x$. It is a **deterministic vector** in $\\mathbb{R}^d$.\n-   $D_s X_t^x$: This is the Malliavin derivative of the random variable $X_t^x$. For fixed $s$ and $t$, it is a **random matrix** of size $d \\times m$.\nThe two objects are of different types (deterministic vector vs. random matrix) and different dimensions in general (unless $m=1$). They cannot be equal. This statement confuses the derivative of an expectation with respect to a parameter with the pathwise derivative with respect to the driving noise.\nVerdict: **Incorrect**.\n\n**Option E:** The Malliavin derivative $D_s X_t^x$ coincides with the spatial derivative $J_{0,t}^x$ when the drift $b$ is constant.\n\nThis statement is also incorrect. First, there's a dimensional mismatch. $D_s X_t^x$ is a $d \\times m$ matrix, while $J_{0,t}^x$ is a $d \\times d$ matrix. They can only be equal if the dimension of the state space $d$ equals the dimension of the Brownian motion $m$. Even if we assume $m=d$, the statement is generally false.\nLet's consider the case where $b$ is a constant vector. Then its gradient $\\nabla b$ is the zero matrix. The SDE for the Jacobian flow $Y_t = J_{0,t}^x$ becomes\n$$\n\\mathrm{d}Y_t = \\sum_{k=1}^m \\nabla\\sigma_k(X_t^x) Y_t \\mathrm{d}W_t^k, \\quad \\text{with} \\quad Y_0 = I_d.\n$$\nFrom Option B, we have the identity $D_s X_t^x = J_{s,t}^x \\sigma(X_s^x)$. The statement claims $D_s X_t^x = J_{0,t}^x$. Thus, it claims $J_{s,t}^x \\sigma(X_s^x) = J_{0,t}^x$. Using the composition property of Jacobians, $J_{0,t}^x = J_{s,t}^x J_{0,s}^x$. This implies the relation should be $J_{s,t}^x \\sigma(X_s^x) = J_{s,t}^x J_{0,s}^x$. Since $J_{s,t}^x$ is almost surely invertible, this would require $\\sigma(X_s^x) = J_{0,s}^x$.\n$J_{0,s}^x$ is the solution at time $s$ to the SDE for $Y_t$ starting from $Y_0=I_d$. There is no reason for this random matrix to be equal to $\\sigma(X_s^x)$. A simple counterexample: let $\\sigma$ be a constant matrix. Then $\\nabla \\sigma_k=0$, so $\\mathrm{d}Y_t = 0$, which means $J_{0,t}^x = I_d$ for all $t$. On the other hand, $D_s X_t^x = J_{s,t}^x \\sigma(X_s^x) = I_d \\sigma = \\sigma$. The equality $D_s X_t^x = J_{0,t}^x$ would imply $\\sigma = I_d$, which is not true in general.\nVerdict: **Incorrect**.\n\n### Conclusion\nOptions A, B, and C are correct statements. Option A defines the spatial gradient of the semigroup, distinguishing it as a sensitivity to the initial condition. Option B defines the Malliavin derivative, showing its relation to the Jacobian flow and identifying it as a sensitivity to the noise path. Option C correctly describes the Bismut–Elworthy–Li methodology, which establishes a profound relationship between these two distinct types of derivatives, forming the basis of a powerful computational technique. Together, these three options fulfill the prompt's request.", "answer": "$$\\boxed{ABC}$$", "id": "2999698"}, {"introduction": "Building on the foundational concepts, this practice moves from theory to a comparative analysis of two key gradient estimation techniques. By examining a linear stochastic differential equation, you will explore the practical trade-offs between the direct pathwise (or tangent) method and a Bismut-Elworthy-Li (BEL) based estimator. This hands-on comparison ([@problem_id:2999787]) illuminates the practical advantages of the BEL formula concerning regularity requirements and variance behavior, particularly in applications like financial and scientific computing.", "problem": "Consider the $d$-dimensional Itô stochastic differential equation (SDE)\n$$\ndX_t^x \\;=\\; A\\,X_t^x\\,dt \\;+\\; B\\,dW_t,\\qquad X_0^x = x\\in\\mathbb{R}^d,\n$$\nwhere $A\\in\\mathbb{R}^{d\\times d}$ is a constant matrix, $B\\in\\mathbb{R}^{d\\times d}$ is an invertible constant matrix, and $W_t$ is a standard $d$-dimensional Brownian motion. Assume that $A$ is Hurwitz (all eigenvalues have strictly negative real parts), so the deterministic flow $e^{A t}$ is exponentially stable. Let the Markov semigroup be\n$$\nP_t f(x) \\;=\\; \\mathbb{E}\\big[f(X_t^x)\\big],\n$$\nfor measurable $f:\\mathbb{R}^d\\to\\mathbb{R}$ for which the expectation is well-defined.\n\nTwo Monte Carlo gradient estimation paradigms for $\\nabla P_t f(x)$ are widely used:\n\n- Pathwise differentiation (the tangent method): When $f$ is sufficiently regular (for instance $f\\in C_b^1(\\mathbb{R}^d)$), one differentiates inside the expectation using the Jacobian of the flow to represent $\\nabla P_t f(x)$ as an expectation involving $\\nabla f(X_t^x)$ and the deterministic tangent map associated with $X_t^x$.\n\n- Bismut-Elworthy-Li (BEL)-based estimators: Under uniform ellipticity (invertible diffusion matrix) and smooth bounded coefficients, one may use an integration-by-parts argument to shift the spatial derivative from $f$ to the driving noise, yielding an unbiased representation of $\\nabla P_t f(x)$ as an expectation of $f(X_t^x)$ times an adapted stochastic integral weight. This representation remains valid for bounded measurable $f$ due to the smoothing effect of the semigroup.\n\nIn the linear setting above, one can take the tangent map to be $e^{A t}$, and a BEL-type weight to be of the generic form\n$$\nM_t(v) \\;=\\; \\frac{1}{t}\\int_0^t \\langle C_{t,s}\\,v,\\, dW_s\\rangle,\n$$\nfor $v\\in\\mathbb{R}^d$, where $\\{C_{t,s}\\}_{0\\le s\\le t}$ is an adapted matrix-valued process determined by the coefficients and uniformly bounded in operator norm on compact time intervals; the precise expression of $C_{t,s}$ is not needed for this problem. You may use Itô isometry for adapted integrands without proof.\n\nUsing only the foundational definitions above, the chain rule for the pathwise method, the smoothing effect of uniformly elliptic diffusions, and Itô isometry, assess the following statements about regularity requirements on $f$ and variance scaling of Monte Carlo estimators for $\\nabla P_t f(x)$ in this setting. Select all correct statements.\n\nA. Under uniform ellipticity (invertible $B$) and smooth bounded coefficients, a BEL estimator represents $\\nabla P_t f(x)$ as $\\mathbb{E}\\big[f(X_t^x)\\,M_t\\big]$ with $M_t$ an adapted Itô integral, hence it is unbiased for bounded measurable $f$. In contrast, the pathwise tangent estimator requires $f$ to be at least continuously differentiable with bounded gradient.\n\nB. In the stable linear case with $A$ Hurwitz and $B$ invertible, for bounded $f$, the variance of a BEL estimator scales like $\\mathcal{O}(t^{-1})$ as $t\\to\\infty$, whereas the variance of the pathwise estimator scales like $\\mathcal{O}(t)$; therefore, BEL has strictly lower variance than pathwise for large $t$.\n\nC. For small $t\\downarrow 0$ and discontinuous bounded $f$ (for example, an indicator of a half-space), the pathwise estimator is not defined, whereas a BEL estimator remains unbiased with variance of order $\\mathcal{O}(t^{-1})$.\n\nD. If the diffusion is degenerate and does not satisfy the Hörmander bracket condition, the classical BEL representation may fail to exist, while the pathwise method still yields $\\nabla P_t f(x)\\,v=\\mathbb{E}\\big[\\langle \\nabla f(X_t^x),\\,Y_t^v\\rangle\\big]$ for $f\\in C_b^1(\\mathbb{R}^d)$ and differentiable flow, where $Y_t^v$ is the tangent process; thus, in such degenerate settings, BEL offers no advantage in regularity or variance.\n\nE. Under hypoellipticity satisfying the Hörmander condition, there always exists a BEL representation with an adapted weight whose second moment is uniformly bounded over all $t\\ge 0$, independently of $x$ and $v$.\n\nChoose all that apply.", "solution": "The problem statement is validated according to the prescribed protocol.\n\n### Step 1: Extract Givens\n- **SDE**: $dX_t^x = A\\,X_t^x\\,dt + B\\,dW_t$, with $X_0^x = x \\in \\mathbb{R}^d$. This is a linear SDE, specifically a multidimensional Ornstein-Uhlenbeck process.\n- **Coefficients**: $A$ is a constant, Hurwitz matrix (stable). $B$ is a constant, invertible matrix (uniformly elliptic).\n- **Semigroup**: $P_t f(x) = \\mathbb{E}[f(X_t^x)]$.\n- **Estimators for $\\nabla P_t f(x)$**:\n    1.  **Pathwise (Tangent) Method**: Requires smooth $f$, uses $\\nabla_x X_t^x$. For this SDE, the solution is $X_t^x = e^{At}x + \\int_0^t e^{A(t-s)}B dW_s$, so the Jacobian (tangent map) is $\\nabla_x X_t^x = e^{At}$. The estimator for $\\nabla_v P_t f(x)$ is $\\langle \\nabla f(X_t^x), e^{At}v \\rangle$.\n    2.  **Bismut-Elworthy-Li (BEL) Method**: Does not require smooth $f$, represents the gradient as $\\mathbb{E}[f(X_t^x) \\cdot \\text{weight}]$. The weight is a stochastic integral, e.g., $M_t(v) = \\frac{1}{t}\\int_0^t \\langle C_{t,s}\\,v,\\, dW_s\\rangle$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is mathematically sound and well-posed. The setup is a standard textbook example used to compare gradient estimation methods in stochastic analysis. No flaws are present.\n\n### Step 3: Verdict and Action\nThe problem statement is **valid**. The solution will be derived by analyzing each statement.\n\n### Analysis of Options\n\n**A. Under uniform ellipticity (invertible $B$) and smooth bounded coefficients, a BEL estimator represents $\\nabla P_t f(x)$ as $\\mathbb{E}\\big[f(X_t^x)\\,M_t\\big]$ with $M_t$ an adapted Itô integral, hence it is unbiased for bounded measurable $f$. In contrast, the pathwise tangent estimator requires $f$ to be at least continuously differentiable with bounded gradient.**\nThis statement accurately contrasts the regularity requirements of the two methods. The BEL formula relies on integration by parts on the path space, which shifts the derivative from the test function $f$ onto a stochastic weight. This is possible because uniform ellipticity ($B$ is invertible) ensures the transition law of $X_t^x$ is smooth for any $t>0$, making $P_t f(x)$ a smooth function of $x$ even for non-smooth (e.g., just bounded and measurable) $f$. The pathwise method, by its construction $\\nabla \\mathbb{E}[f] = \\mathbb{E}[\\nabla f \\cdot \\nabla X]$, explicitly requires the existence of $\\nabla f$. Thus, this statement is **correct**.\n\n**B. In the stable linear case with $A$ Hurwitz and $B$ invertible, for bounded $f$, the variance of a BEL estimator scales like $\\mathcal{O}(t^{-1})$ as $t\\to\\infty$, whereas the variance of the pathwise estimator scales like $\\mathcal{O}(t)$; therefore, BEL has strictly lower variance than pathwise for large $t$.**\nLet's analyze the large $t$ behavior.\n- **Pathwise Estimator**: The estimator for the directional derivative $\\nabla_v P_t f(x)$ is $Z_t^{\\text{PW}} = \\langle \\nabla f(X_t^x), e^{At}v \\rangle$. Since $A$ is Hurwitz, the matrix exponential $e^{At}$ decays to zero exponentially fast as $t \\to \\infty$. Assuming $\\nabla f$ is bounded, the variance of $Z_t^{\\text{PW}}$, which is bounded by $\\mathbb{E}[|Z_t^{\\text{PW}}|^2]$, will also decay to zero exponentially fast. The statement that its variance scales like $\\mathcal{O}(t)$ is incorrect.\n- **BEL Estimator**: For an ergodic process, the variance of a BEL estimator for the gradient of the semigroup typically decays like $\\mathcal{O}(t^{-1})$ as $t\\to\\infty$. This part of the statement is plausible.\nSince the premise regarding the pathwise estimator's variance is incorrect, the entire statement is **incorrect**.\n\n**C. For small $t\\downarrow 0$ and discontinuous bounded $f$ (for example, an indicator of a half-space), the pathwise estimator is not defined, whereas a BEL estimator remains unbiased with variance of order $\\mathcal{O}(t^{-1})$.**\n- **Pathwise Estimator**: For a discontinuous $f$, its gradient $\\nabla f$ is not well-defined in the classical sense, so the pathwise estimator formula cannot be applied. This part is correct.\n- **BEL Estimator**: The BEL estimator remains unbiased for discontinuous $f$, as explained in A. Its variance, however, is known to explode for small time horizons when $f$ is not smooth. The typical BEL weight involves a $1/t$ prefactor, and its variance, via Itô isometry, scales like $\\frac{1}{t^2} \\int_0^t \\dots ds \\sim \\mathcal{O}(t^{-1})$. This high variance for small $t$ is a well-known drawback of the BEL method. The statement is therefore **correct**.\n\n**D. If the diffusion is degenerate and does not satisfy the Hörmander bracket condition, the classical BEL representation may fail to exist, while the pathwise method still yields $\\nabla P_t f(x)\\,v=\\mathbb{E}\\big[\\langle \\nabla f(X_t^x),\\,Y_t^v\\rangle\\big]$ for $f\\in C_b^1(\\mathbb{R}^d)$ and differentiable flow, where $Y_t^v$ is the tangent process; thus, in such degenerate settings, BEL offers no advantage in regularity or variance.**\n- The classical BEL formula relies on the invertibility of the diffusion matrix (uniform ellipticity). A more general version exists under the weaker Hörmander condition (hypoellipticity), which still guarantees the required smoothing of the semigroup. If even the Hörmander condition is not met, the semigroup may not be smoothing, and a BEL-type formula for non-smooth $f$ might not exist.\n- The pathwise method's validity depends on the differentiability of the flow ($x \\mapsto X_t^x$) and the function $f$. For SDEs with smooth coefficients (like the linear case), the flow is differentiable regardless of diffusion degeneracy. So, if $f$ is smooth ($C_b^1$), the pathwise method remains applicable.\n- The conclusion that \"BEL offers no advantage\" is logical in this scenario. Its key advantage—handling non-smooth $f$—is lost if the method is not applicable. The question of variance comparison becomes moot. The statement is **correct**.\n\n**E. Under hypoellipticity satisfying the Hörmander condition, there always exists a BEL representation with an adapted weight whose second moment is uniformly bounded over all $t\\ge 0$, independently of $x$ and $v$.**\nWhile a BEL representation does exist under hypoellipticity, the claim that the weight's second moment is uniformly bounded over all $t \\ge 0$ is false. As established in the analysis of option C, the second moment of the weight typically blows up like $\\mathcal{O}(t^{-1})$ as $t \\to 0$. Therefore, it is not bounded on the interval $[0, \\infty)$. The statement is **incorrect**.\n\n### Conclusion\nThe correct statements are A, C, and D.", "answer": "$$\\boxed{ACD}$$", "id": "2999787"}, {"introduction": "This final practice challenges you to apply the theoretical machinery to a concrete, non-linear problem. You will derive an explicit bound for the second moment of a BEL weight, thereby proving its integrability—a crucial step for ensuring the validity of the formula. This exercise ([@problem_id:2999669]) reinforces key skills in stochastic calculus, such as applying Itô's isometry and using stability analysis for the variational process, which are essential for the rigorous implementation of BEL-based methods.", "problem": "Consider the one-dimensional stochastic differential equation (SDE)\n$$\n\\mathrm{d}X_{t}^{x} = b(X_{t}^{x})\\,\\mathrm{d}t + \\sigma(X_{t}^{x})\\,\\mathrm{d}W_{t}, \\qquad X_{0}^{x} = x,\n$$\nwhere $W_{t}$ is a standard Brownian motion, the drift coefficient is $b(x) = -\\lambda x - a x^{3}$, and the diffusion coefficient is $\\sigma(x) = \\sigma_{0} + c x^{2}$ with parameters satisfying $\\lambda > 0$, $a > 0$, $\\sigma_{0} > 0$, $c \\ge 0$, and the structural conditions $a > \\frac{2}{3}c^{2}$ and $\\lambda > \\sigma_{0} c$. These conditions ensure non-explosion and finite moments under polynomial growth.\n\nLet $J_{t}^{x} = \\partial X_{t}^{x}/\\partial x$ denote the spatial derivative (the variational process along the flow) and define the Bismut-Elworthy-Li (BEL) weight\n$$\nH_{T}(x) = \\frac{1}{T} \\int_{0}^{T} \\frac{J_{s}^{x}}{\\sigma(X_{s}^{x})}\\,\\mathrm{d}W_{s},\n$$\nwhich arises in the Bismut-Elworthy-Li gradient formula for diffusion semigroups. Analyze the behavior of $H_{T}(x)$ under the given polynomial growth of coefficients and provide moment bounds that ensure the integrability of the stochastic integral defining $H_{T}(x)$.\n\nStarting from first principles of stochastic calculus and stability analysis (Itô calculus, Lyapunov methods, and fundamental inequalities), derive an explicit bound $B(T,x)$ such that\n$$\n\\mathbb{E}\\big[|H_{T}(x)|^{2}\\big] \\le B(T,x).\n$$\nReport the tightest bound you obtain as a single closed-form analytic expression in terms of $T$, $\\lambda$, $\\sigma_{0}$, $a$, and $c$. No rounding is required, and there are no physical units to report. Your final answer must be a single analytic expression only.", "solution": "The problem will first be validated against the specified criteria.\n\n### Step 1: Extract Givens\n- **Stochastic Differential Equation (SDE)**: $\\mathrm{d}X_{t}^{x} = b(X_{t}^{x})\\,\\mathrm{d}t + \\sigma(X_{t}^{x})\\,\\mathrm{d}W_{t}$, with initial condition $X_{0}^{x} = x$.\n- **Brownian motion**: $W_{t}$ is a standard one-dimensional Brownian motion.\n- **Drift Coefficient**: $b(x) = -\\lambda x - a x^{3}$.\n- **Diffusion Coefficient**: $\\sigma(x) = \\sigma_{0} + c x^{2}$.\n- **Parameters**: $\\lambda > 0$, $a > 0$, $\\sigma_{0} > 0$, $c \\ge 0$.\n- **Structural Conditions**: $a > \\frac{2}{3}c^{2}$ and $\\lambda > \\sigma_{0} c$. These ensure non-explosion and finite moments.\n- **Variational Process**: $J_{t}^{x} = \\partial X_{t}^{x}/\\partial x$.\n- **Bismut-Elworthy-Li (BEL) Weight**: $H_{T}(x) = \\frac{1}{T} \\int_{0}^{T} \\frac{J_{s}^{x}}{\\sigma(X_{s}^{x})}\\,\\mathrm{d}W_{s}$.\n- **Objective**: Derive an explicit bound $B(T,x)$ such that $\\mathbb{E}\\big[|H_{T}(x)|^{2}\\big] \\le B(T,x)$ and report the expression for $B(T,x)$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is well-defined within the mathematical field of stochastic differential equations and stochastic analysis.\n- **Scientifically Grounded**: The problem deals with a specific type of SDE with polynomial coefficients, the properties of its solution, and its sensitivity to initial conditions (the variational process $J_{t}^{x}$). The Bismut-Elworthy-Li formula is a fundamental result in stochastic analysis used for computing derivatives of expectations of diffusion processes. The entire setup is based on established mathematical principles.\n- **Well-Posed**: The problem is clearly stated. The SDE is specified, all coefficients and parameters are defined, and the structural conditions given ($a > \\frac{2}{3}c^{2}$, $\\lambda > \\sigma_{0} c$) are standard in the literature to ensure the existence, uniqueness, non-explosion, and good ergodic properties (e.g., existence of finite moments) of the solution $X_{t}^{x}$. The task is to derive a moment bound for a well-defined stochastic integral, which is a standard analytical exercise in this field.\n- **Objective**: The problem is stated using precise mathematical language and notation, free of any subjectivity or ambiguity.\n- **Completeness and Consistency**: The problem provides all necessary information. The conditions on the parameters are consistent and serve to make the problem tractable.\n\n### Step 3: Verdict and Action\nThe problem is valid as it is mathematically sound, well-posed, objective, and self-contained. A reasoned solution can be constructed.\n\n### Derivation of the Bound\nThe objective is to find an upper bound $B(T,x)$ for the quantity $\\mathbb{E}\\big[|H_{T}(x)|^{2}\\big]$.\nThe Bismut-Elworthy-Li weight is given by\n$$\nH_{T}(x) = \\frac{1}{T} \\int_{0}^{T} \\frac{J_{s}^{x}}{\\sigma(X_{s}^{x})}\\,\\mathrm{d}W_{s}.\n$$\nThis is a stochastic integral with respect to the Brownian motion $W_s$. To compute its second moment, we apply the Itô Isometry, which states that for a square-integrable process $\\Phi_s$, $\\mathbb{E}\\left[\\left(\\int_0^T \\Phi_s \\mathrm{d}W_s\\right)^2\\right] = \\mathbb{E}\\left[\\int_0^T \\Phi_s^2 \\mathrm{d}s\\right]$.\nApplying this to $H_{T}(x)$, we get:\n$$\n\\mathbb{E}\\big[|H_{T}(x)|^{2}\\big] = \\mathbb{E}\\left[\\left|\\frac{1}{T} \\int_{0}^{T} \\frac{J_{s}^{x}}{\\sigma(X_{s}^{x})}\\,\\mathrm{d}W_{s}\\right|^{2}\\right] = \\frac{1}{T^{2}} \\mathbb{E}\\left[\\int_{0}^{T} \\left(\\frac{J_{s}^{x}}{\\sigma(X_{s}^{x})}\\right)^{2}\\,\\mathrm{d}s\\right].\n$$\nSince the integrand is non-negative, we can apply Fubini's theorem to interchange the expectation and the time integral:\n$$\n\\mathbb{E}\\big[|H_{T}(x)|^{2}\\big] = \\frac{1}{T^{2}} \\int_{0}^{T} \\mathbb{E}\\left[\\frac{|J_{s}^{x}|^{2}}{|\\sigma(X_{s}^{x})|^{2}}\\right]\\,\\mathrm{d}s.\n$$\nTo proceed, we need to find a bound for the expectation term in the integrand, $\\mathbb{E}\\left[\\frac{|J_{s}^{x}|^{2}}{|\\sigma(X_{s}^{x})|^{2}}\\right]$, that is uniform in $s$. This requires analyzing the variational process $J_s^x$ and the diffusion coefficient $\\sigma(x)$.\n\nFirst, we analyze $|\\sigma(X_{s}^{x})|^{-2}$. The diffusion coefficient is $\\sigma(x) = \\sigma_{0} + c x^{2}$. Given $\\sigma_{0} > 0$ and $c \\ge 0$, the term $\\sigma(x)$ is always positive and its minimum value is $\\sigma_{0}$, attained at $x=0$. Therefore, we have the bound:\n$$\n|\\sigma(x)|^{2} = (\\sigma_{0} + c x^{2})^{2} \\ge \\sigma_{0}^{2}.\n$$\nThis implies that for any path $X_s^x$:\n$$\n\\frac{1}{|\\sigma(X_{s}^{x})|^{2}} \\le \\frac{1}{\\sigma_{0}^{2}}.\n$$\nUsing this, we can bound the integrand:\n$$\n\\mathbb{E}\\left[\\frac{|J_{s}^{x}|^{2}}{|\\sigma(X_{s}^{x})|^{2}}\\right] \\le \\mathbb{E}\\left[\\frac{|J_{s}^{x}|^{2}}{\\sigma_{0}^{2}}\\right] = \\frac{1}{\\sigma_{0}^{2}}\\mathbb{E}\\big[|J_{s}^{x}|^{2}\\big].\n$$\nNext, we need to find a bound for the second moment of the variational process, $\\mathbb{E}\\big[|J_{s}^{x}|^{2}\\big]$. The process $J_t^x = \\partial X_t^x / \\partial x$ satisfies the linearized SDE, obtained by differentiating the SDE for $X_t^x$ with respect to the initial condition $x$:\n$$\n\\mathrm{d}J_{t}^{x} = b'(X_{t}^{x})J_{t}^{x}\\,\\mathrm{d}t + \\sigma'(X_{t}^{x})J_{t}^{x}\\,\\mathrm{d}W_{t}, \\qquad J_{0}^{x} = \\frac{\\partial x}{\\partial x} = 1.\n$$\nThe derivatives of the coefficients are $b'(x) = -\\lambda - 3ax^{2}$ and $\\sigma'(x) = 2cx$. Let $Y_s = J_s^x$. We apply Itô's formula to $f(Y_s) = Y_s^2$:\n$$\n\\mathrm{d}(Y_s^2) = 2Y_s \\mathrm{d}Y_s + (\\mathrm{d}Y_s)^2 = 2Y_s (b'(X_s^x)Y_s \\mathrm{d}s + \\sigma'(X_s^x)Y_s \\mathrm{d}W_s) + (\\sigma'(X_s^x)Y_s)^2 \\mathrm{d}s.\n$$\n$$\n\\mathrm{d}|J_s^x|^2 = \\left(2b'(X_s^x) + (\\sigma'(X_s^x))^2\\right) |J_s^x|^2 \\mathrm{d}s + 2\\sigma'(X_s^x)|J_s^x|^2 \\mathrm{d}W_s.\n$$\nLet $m(s) = \\mathbb{E}[|J_s^x|^2]$. Taking the expectation of the SDE for $|J_s^x|^2$, the martingale term (the Itô integral with respect to $\\mathrm{d}W_s$) vanishes:\n$$\n\\frac{\\mathrm{d}m(s)}{\\mathrm{d}s} = \\mathbb{E}\\left[\\left(2b'(X_s^x) + (\\sigma'(X_s^x))^2\\right) |J_s^x|^2\\right].\n$$\nLet's analyze the term in the parentheses:\n$$\n2b'(x) + (\\sigma'(x))^2 = 2(-\\lambda - 3ax^2) + (2cx)^2 = -2\\lambda - 6ax^2 + 4c^2x^2 = -2\\lambda - (6a - 4c^2)x^2.\n$$\nThe problem provides the structural condition $a > \\frac{2}{3}c^2$, which is equivalent to $3a > 2c^2$ or $6a > 4c^2$. This ensures that the coefficient of $x^2$, namely $-(6a-4c^2)$, is strictly negative. Let $\\gamma = 6a - 4c^2 > 0$. Then\n$$\n2b'(x) + (\\sigma'(x))^2 = -2\\lambda - \\gamma x^2.\n$$\nSince $\\lambda > 0$ and $\\gamma > 0$, this expression is always negative. We can establish an upper bound:\n$$\n2b'(x) + (\\sigma'(x))^2 \\le -2\\lambda.\n$$\nSubstituting this into the differential equation for $m(s)$:\n$$\n\\frac{\\mathrm{d}m(s)}{\\mathrm{d}s} = \\mathbb{E}\\left[(-2\\lambda - \\gamma (X_s^x)^2)|J_s^x|^2\\right] \\le \\mathbb{E}\\left[(-2\\lambda)|J_s^x|^2\\right] = -2\\lambda m(s).\n$$\nWe have the differential inequality $\\frac{\\mathrm{d}m(s)}{\\mathrm{d}s} \\le -2\\lambda m(s)$, with the initial condition $m(0) = \\mathbb{E}[|J_0^x|^2] = \\mathbb{E}[1^2] = 1$. By Grönwall's inequality, we find the bound:\n$$\nm(s) \\le m(0) \\exp(-2\\lambda s) = \\exp(-2\\lambda s).\n$$\nSo, we have established a uniform exponential bound on the second moment of the variational process: $\\mathbb{E}[|J_s^x|^2] \\le \\exp(-2\\lambda s)$.\n\nWe now combine our intermediate results.\nThe integrand in the expression for $\\mathbb{E}[|H_T(x)|^2]$ is bounded by:\n$$\n\\mathbb{E}\\left[\\frac{|J_{s}^{x}|^{2}}{|\\sigma(X_{s}^{x})|^{2}}\\right] \\le \\frac{1}{\\sigma_{0}^{2}}\\mathbb{E}\\big[|J_{s}^{x}|^{2}\\big] \\le \\frac{\\exp(-2\\lambda s)}{\\sigma_{0}^{2}}.\n$$\nSubstituting this bound back into the integral expression for the second moment of $H_T(x)$:\n$$\n\\mathbb{E}\\big[|H_{T}(x)|^{2}\\big] \\le \\frac{1}{T^{2}} \\int_{0}^{T} \\frac{\\exp(-2\\lambda s)}{\\sigma_{0}^{2}}\\,\\mathrm{d}s.\n$$\nWe evaluate the definite integral:\n$$\n\\int_{0}^{T} \\frac{\\exp(-2\\lambda s)}{\\sigma_{0}^{2}}\\,\\mathrm{d}s = \\frac{1}{\\sigma_{0}^{2}} \\left[\\frac{\\exp(-2\\lambda s)}{-2\\lambda}\\right]_{0}^{T} = \\frac{1}{\\sigma_{0}^{2}} \\left( \\frac{\\exp(-2\\lambda T)}{-2\\lambda} - \\frac{\\exp(0)}{-2\\lambda} \\right) = \\frac{1 - \\exp(-2\\lambda T)}{2\\lambda\\sigma_{0}^{2}}.\n$$\nFinally, we obtain the bound $B(T,x)$:\n$$\n\\mathbb{E}\\big[|H_{T}(x)|^{2}\\big] \\le \\frac{1}{T^{2}} \\left(\\frac{1 - \\exp(-2\\lambda T)}{2\\lambda\\sigma_{0}^{2}}\\right) = \\frac{1 - \\exp(-2\\lambda T)}{2\\lambda\\sigma_{0}^{2} T^{2}}.\n$$\nThis bound is independent of the initial position $x$ and the parameters $a$ and $c$, although the condition $a > \\frac{2}{3}c^2$ was crucial for ensuring the exponential decay of $\\mathbb{E}[|J_s^x|^2]$. The condition $\\lambda > \\sigma_0 c$ (along with $a > \\frac{2}{3}c^2$) ensures the underlying process $X_t^x$ is well-behaved, justifying the application of Itô's formula and Fubini's theorem. The expression obtained is the tightest bound derived from this specific line of reasoning.", "answer": "$$\n\\boxed{\\frac{1 - \\exp(-2\\lambda T)}{2\\lambda\\sigma_{0}^{2} T^{2}}}\n$$", "id": "2999669"}]}