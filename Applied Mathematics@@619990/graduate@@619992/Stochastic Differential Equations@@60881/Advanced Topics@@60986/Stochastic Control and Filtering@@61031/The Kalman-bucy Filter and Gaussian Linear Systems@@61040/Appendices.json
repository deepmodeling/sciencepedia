{"hands_on_practices": [{"introduction": "To truly master the Kalman-Bucy filter, it is essential to understand its derivation from fundamental principles. This exercise guides you through developing the filter equations using Itô calculus and the orthogonality principle, culminating in the continuous-time Algebraic Riccati Equation (CARE). By solving this for a multi-input multi-output (MIMO) system, you will solidify your understanding of how the optimal gain is derived and calculated in practice. [@problem_id:2713808]", "problem": "Consider the continuous-time Multi-Input Multi-Output (MIMO) linear stochastic system driven by Gaussian white noise\n$$\n\\mathrm{d}x(t) = A\\,x(t)\\,\\mathrm{d}t + G\\,\\mathrm{d}w(t), \\qquad \\mathrm{d}y(t) = C\\,x(t)\\,\\mathrm{d}t + \\mathrm{d}v(t),\n$$\nwhere $x(t) \\in \\mathbb{R}^{2}$ is the state, $y(t) \\in \\mathbb{R}^{2}$ is the measurement, $A \\in \\mathbb{R}^{2 \\times 2}$, $G \\in \\mathbb{R}^{2 \\times 2}$, and $C \\in \\mathbb{R}^{2 \\times 2}$. The processes $w(t)$ and $v(t)$ are independent Wiener processes with incremental covariances $\\mathbb{E}[\\mathrm{d}w(t)\\,\\mathrm{d}w(t)^{\\top}] = Q\\,\\mathrm{d}t$ and $\\mathbb{E}[\\mathrm{d}v(t)\\,\\mathrm{d}v(t)^{\\top}] = R\\,\\mathrm{d}t$, where $Q \\in \\mathbb{R}^{2 \\times 2}$ and $R \\in \\mathbb{R}^{2 \\times 2}$ are constant, symmetric, positive definite. Assume the following data:\n$$\nA = \\begin{pmatrix} 1 & 0 \\\\ 0 & -2 \\end{pmatrix}, \\quad G = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix}, \\quad C = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix}, \\quad Q = \\begin{pmatrix} 2 & 0 \\\\ 0 & 1 \\end{pmatrix}, \\quad R = \\begin{pmatrix} 1 & 0 \\\\ 0 & 3 \\end{pmatrix}.\n$$\nStarting from the definitions of linear Gaussian state estimation and the estimation error covariance, and without invoking pre-stated filter or Riccati formulas, perform the following:\n\n- Derive the steady-state gain of the continuous-time Kalman-Bucy (KB) filter by first obtaining the algebraic equation satisfied by the steady-state estimation error covariance and then determining the optimal gain that minimizes the steady-state mean-square error (MSE). Clearly justify each step from first principles of Itô calculus and the orthogonality principle.\n- Using your derivation, compute the steady-state Kalman gain for the given $A$, $G$, $C$, $Q$, and $R$.\n- Define the innovation process associated with the KB filter for this MIMO system and interpret its role and statistical properties in steady state using the derived results.\n\nProvide your final Kalman gain matrix in exact closed form (no decimal approximations). Do not round. The final answer must be the Kalman gain only, expressed as a single matrix.", "solution": "The problem requires the derivation of the steady-state Kalman-Bucy filter gain from first principles, its computation for a given system, and an analysis of the associated innovation process. The foundation of this derivation rests upon the principles of optimal linear estimation in the presence of Gaussian noise, governed by the Itô calculus and the orthogonality principle.\n\nWe begin by postulating a linear estimator for the state $x(t)$ of the form:\n$$\n\\mathrm{d}\\hat{x}(t) = A\\,\\hat{x}(t)\\,\\mathrm{d}t + K(t)\\,(\\mathrm{d}y(t) - C\\,\\hat{x}(t)\\,\\mathrm{d}t)\n$$\nHere, $\\hat{x}(t)$ is the estimate of the state $x(t)$, and $K(t) \\in \\mathbb{R}^{2 \\times 2}$ is the filter gain matrix, which must be determined. The term $\\mathrm{d}y(t) - C\\,\\hat{x}(t)\\,\\mathrm{d}t$ represents the innovation increment, the new information provided by the measurement process.\n\nThe estimation error is defined as $e(t) = x(t) - \\hat{x}(t)$. Its dynamics are derived by subtracting the estimator equation from the state equation:\n$$\n\\mathrm{d}e(t) = \\mathrm{d}x(t) - \\mathrm{d}\\hat{x}(t)\n$$\nSubstituting the given system dynamics and the postulated filter structure:\n$$\n\\mathrm{d}e(t) = (A\\,x(t)\\,\\mathrm{d}t + G\\,\\mathrm{d}w(t)) - (A\\,\\hat{x}(t)\\,\\mathrm{d}t + K(t)\\,(C\\,x(t)\\,\\mathrm{d}t + \\mathrm{d}v(t) - C\\,\\hat{x}(t)\\,\\mathrm{d}t))\n$$\nGrouping terms by $x(t)$ and $\\hat{x}(t)$:\n$$\n\\mathrm{d}e(t) = A\\,(x(t)-\\hat{x}(t))\\,\\mathrm{d}t - K(t)\\,C\\,(x(t)-\\hat{x}(t))\\,\\mathrm{d}t + G\\,\\mathrm{d}w(t) - K(t)\\,\\mathrm{d}v(t)\n$$\n$$\n\\mathrm{d}e(t) = (A - K(t)C)\\,e(t)\\,\\mathrm{d}t + G\\,\\mathrm{d}w(t) - K(t)\\,\\mathrm{d}v(t)\n$$\nThis is the stochastic differential equation (SDE) governing the estimation error. The goal of the Kalman-Bucy filter is to choose the gain $K(t)$ to minimize the mean-square error, which is the trace of the error covariance matrix $P(t) = \\mathbb{E}[e(t)e(t)^{\\top}]$.\n\nTo find the differential equation for $P(t)$, we apply Itô's lemma for the process $e(t)e(t)^{\\top}$:\n$$\n\\mathrm{d}(e(t)e(t)^{\\top}) = (\\mathrm{d}e(t))e(t)^{\\top} + e(t)(\\mathrm{d}e(t))^{\\top} + (\\mathrm{d}e(t))(\\mathrm{d}e(t))^{\\top}\n$$\nWe evaluate each term. The quadratic variation term $(\\mathrm{d}e(t))(\\mathrm{d}e(t))^{\\top}$ is:\n$$\n(\\mathrm{d}e)(\\mathrm{d}e)^{\\top} = (G\\,\\mathrm{d}w - K\\,\\mathrm{d}v)(G\\,\\mathrm{d}w - K\\,\\mathrm{d}v)^{\\top}\n$$\nUsing the Itô multiplication rules ($\\mathrm{d}t \\cdot \\mathrm{d}t = 0$, $\\mathrm{d}t \\cdot \\mathrm{d}w = 0$, $\\mathrm{d}w_i \\cdot \\mathrm{d}w_j = \\delta_{ij}\\mathrm{d}t$), and the independence of $w(t)$ and $v(t)$ ($\\mathrm{d}w \\cdot \\mathrm{d}v^{\\top} = 0$), this simplifies to:\n$$\n(\\mathrm{d}e)(\\mathrm{d}e)^{\\top} = G(\\mathrm{d}w)(\\mathrm{d}w)^{\\top}G^{\\top} + K(\\mathrm{d}v)(\\mathrm{d}v)^{\\top}K^{\\top}\n$$\nTaking the expectation, we get:\n$$\n\\mathbb{E}[(\\mathrm{d}e)(\\mathrm{d}e)^{\\top}] = G\\,\\mathbb{E}[(\\mathrm{d}w)(\\mathrm{d}w)^{\\top}]\\,G^{\\top} + K\\,\\mathbb{E}[(\\mathrm{d}v)(\\mathrm{d}v)^{\\top}]\\,K^{\\top} = (GQG^{\\top} + KRK^{\\top})\\,\\mathrm{d}t\n$$\nNow, taking the expectation of the full expression for $\\mathrm{d}(ee^{\\top})$:\n$$\n\\mathrm{d}P(t) = \\mathbb{E}[\\mathrm{d}(ee^{\\top})] = \\mathbb{E}[(\\mathrm{d}e)e^{\\top} + e(\\mathrm{d}e)^{\\top}] + (GQG^{\\top} + KRK^{\\top})\\,\\mathrm{d}t\n$$\nThe orthogonality principle states that for an optimal filter, the estimation error $e(t)$ is orthogonal to the history of the noise processes. Thus, $\\mathbb{E}[e(t)\\,\\mathrm{d}w(t)^{\\top}]=0$ and $\\mathbb{E}[e(t)\\,\\mathrm{d}v(t)^{\\top}]=0$. This implies that the expectations of the cross terms involving $e$ and the noise increments are zero.\n$$\n\\mathbb{E}[(\\mathrm{d}e)e^{\\top}] = \\mathbb{E}[((A-KC)e\\,\\mathrm{d}t + G\\,\\mathrm{d}w - K\\,\\mathrm{d}v)e^{\\top}] = (A-KC)P(t)\\,\\mathrm{d}t\n$$\nTherefore, the time evolution of the covariance matrix is given by the differential Riccati equation (DRE):\n$$\n\\dot{P}(t) = (A-K(t)C)P(t) + P(t)(A-K(t)C)^{\\top} + GQG^{\\top} + K(t)RK(t)^{\\top}\n$$\n$$\n\\dot{P}(t) = AP(t) + P(t)A^{\\top} - K(t)CP(t) - P(t)C^{\\top}K(t)^{\\top} + K(t)RK(t)^{\\top} + GQG^{\\top}\n$$\nTo find the optimal gain $K(t)$ that minimizes the MSE at each instant, we minimize the trace of $\\dot{P}(t)$ with respect to $K(t)$. We differentiate the terms involving $K(t)$ with respect to $K(t)$ and set the result to zero:\n$$\n\\frac{\\partial}{\\partial K} \\mathrm{tr}[-KCP - PC^{\\top}K^{\\top} + KRK^{\\top}] = 0\n$$\nUsing standard matrix calculus identities, $\\frac{\\partial}{\\partial X} \\mathrm{tr}(AX) = A^{\\top}$ and $\\frac{\\partial}{\\partial X} \\mathrm{tr}(XBX^{\\top}) = X(B+B^{\\top})$, this yields:\n$$\n-2PC^{\\top} + 2KR = 0\n$$\nAssuming $R$ is invertible (which it is, being positive definite), the optimal gain is:\n$$\nK(t) = P(t)C^{\\top}R^{-1}\n$$\nSubstituting this optimal gain back into the DRE yields the definitive form:\n$$\n\\dot{P}(t) = AP(t) + P(t)A^{\\top} - P(t)C^{\\top}R^{-1}C P(t) + GQG^{\\top}\n$$\nFor the steady-state solution, we assume the system reaches an equilibrium where $\\dot{P}(t) = 0$ and $P(t) \\to P_{ss}$. This yields the Continuous-time Algebraic Riccati Equation (CARE):\n$$\n0 = AP_{ss} + P_{ss}A^{\\top} - P_{ss}C^{\\top}R^{-1}C P_{ss} + GQG^{\\top}\n$$\nThe corresponding steady-state gain is $K_{ss} = P_{ss}C^{\\top}R^{-1}$.\n\nNow we compute the solution for the given system matrices:\n$$\nA = \\begin{pmatrix} 1 & 0 \\\\ 0 & -2 \\end{pmatrix}, \\quad C = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix}, \\quad G = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix}\n$$\n$$\nQ = \\begin{pmatrix} 2 & 0 \\\\ 0 & 1 \\end{pmatrix}, \\quad R = \\begin{pmatrix} 1 & 0 \\\\ 0 & 3 \\end{pmatrix}\n$$\nWe first compute the matrix products needed for the CARE:\n$GQG^{\\top} = I Q I^{\\top} = Q = \\begin{pmatrix} 2 & 0 \\\\ 0 & 1 \\end{pmatrix}$.\n$C^{\\top}R^{-1}C = I R^{-1} I = R^{-1} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1/3 \\end{pmatrix}$.\nThe CARE is:\n$$\n0 = AP_{ss} + P_{ss}A^{\\top} - P_{ss}R^{-1}P_{ss} + Q\n$$\nSince all matrices $A$, $Q$, and $R^{-1}$ are diagonal, the solution $P_{ss}$ must also be diagonal. Let $P_{ss} = \\begin{pmatrix} p_1 & 0 \\\\ 0 & p_2 \\end{pmatrix}$. Substituting this into the CARE:\n$$\n\\begin{pmatrix} 0 & 0 \\\\ 0 & 0 \\end{pmatrix} = \\begin{pmatrix} 1 & 0 \\\\ 0 & -2 \\end{pmatrix}\\begin{pmatrix} p_1 & 0 \\\\ 0 & p_2 \\end{pmatrix} + \\begin{pmatrix} p_1 & 0 \\\\ 0 & p_2 \\end{pmatrix}\\begin{pmatrix} 1 & 0 \\\\ 0 & -2 \\end{pmatrix} - \\begin{pmatrix} p_1 & 0 \\\\ 0 & p_2 \\end{pmatrix}\\begin{pmatrix} 1 & 0 \\\\ 0 & 1/3 \\end{pmatrix}\\begin{pmatrix} p_1 & 0 \\\\ 0 & p_2 \\end{pmatrix} + \\begin{pmatrix} 2 & 0 \\\\ 0 & 1 \\end{pmatrix}\n$$\n$$\n\\begin{pmatrix} 0 & 0 \\\\ 0 & 0 \\end{pmatrix} = \\begin{pmatrix} 2p_1 & 0 \\\\ 0 & -4p_2 \\end{pmatrix} - \\begin{pmatrix} p_1^2 & 0 \\\\ 0 & p_2^2/3 \\end{pmatrix} + \\begin{pmatrix} 2 & 0 \\\\ 0 & 1 \\end{pmatrix}\n$$\nThis decouples into two scalar algebraic equations:\n$1.$ $2p_1 - p_1^2 + 2 = 0 \\implies p_1^2 - 2p_1 - 2 = 0$\n$2.$ $-4p_2 - \\frac{1}{3}p_2^2 + 1 = 0 \\implies p_2^2 + 12p_2 - 3 = 0$\n\nSolving for $p_1$ using the quadratic formula:\n$p_1 = \\frac{-(-2) \\pm \\sqrt{(-2)^2 - 4(1)(-2)}}{2} = \\frac{2 \\pm \\sqrt{12}}{2} = 1 \\pm \\sqrt{3}$.\nSince the covariance matrix $P_{ss}$ must be positive definite, its diagonal elements must be positive. Thus, we select the positive root: $p_1 = 1 + \\sqrt{3}$.\n\nSolving for $p_2$:\n$p_2 = \\frac{-12 \\pm \\sqrt{12^2 - 4(1)(-3)}}{2} = \\frac{-12 \\pm \\sqrt{144+12}}{2} = \\frac{-12 \\pm \\sqrt{156}}{2} = -6 \\pm \\sqrt{39}$.\nFor $p_2$ to be positive, we must select $p_2 = -6 + \\sqrt{39}$, since $\\sqrt{39} > \\sqrt{36}=6$.\nThe steady-state error covariance is $P_{ss} = \\begin{pmatrix} 1+\\sqrt{3} & 0 \\\\ 0 & \\sqrt{39}-6 \\end{pmatrix}$.\n\nFinally, we compute the steady-state Kalman gain $K_{ss} = P_{ss}C^{\\top}R^{-1}$:\n$$\nK_{ss} = \\begin{pmatrix} 1+\\sqrt{3} & 0 \\\\ 0 & \\sqrt{39}-6 \\end{pmatrix} \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix}^{\\top} \\begin{pmatrix} 1 & 0 \\\\ 0 & 3 \\end{pmatrix}^{-1} = \\begin{pmatrix} 1+\\sqrt{3} & 0 \\\\ 0 & \\sqrt{39}-6 \\end{pmatrix} \\begin{pmatrix} 1 & 0 \\\\ 0 & 1/3 \\end{pmatrix}\n$$\n$$\nK_{ss} = \\begin{pmatrix} 1+\\sqrt{3} & 0 \\\\ 0 & \\frac{\\sqrt{39}-6}{3} \\end{pmatrix}\n$$\n\nThe innovation process associated with the filter is defined by its increment $\\mathrm{d}z(t) = \\mathrm{d}y(t) - C\\hat{x}(t)\\mathrm{d}t$. It represents the new information contained in the latest measurement that was not predicted by the model. Its role is fundamental: it drives the correction term of the filter, updating the state estimate based on the discrepancy between prediction and reality.\nThe statistical properties of the innovation process are central to the optimality of the filter. We may write its dynamics as:\n$$\n\\mathrm{d}z(t) = (C x(t)\\mathrm{d}t + \\mathrm{d}v(t)) - C\\hat{x}(t)\\mathrm{d}t = C e(t)\\mathrm{d}t + \\mathrm{d}v(t)\n$$\nIn steady state, the error process $e(t)$ has zero mean and constant covariance $P_{ss}$. The innovation increment $\\mathrm{d}z(t)$ is the increment of a zero-mean Wiener process. This can be rigorously established using Lévy's characterization theorem for martingales. The process $z(t) = \\int_0^t \\mathrm{d}z(s)$ is a continuous martingale with respect to the filtration generated by the measurements. Its quadratic variation is given by:\n$$\n\\langle z \\rangle_t = \\int_0^t R\\,\\mathrm{d}s = R t\n$$\nwhere $R$ is the covariance of the measurement noise process $v(t)$. This demonstrates that the innovation process $z(t)$ is a Wiener process with incremental covariance $\\mathbb{E}[\\mathrm{d}z(t)\\mathrm{d}z(t)^{\\top}] = R\\,\\mathrm{d}t$.\nFor the given system, the steady-state innovation process has an incremental covariance of $R\\,\\mathrm{d}t = \\begin{pmatrix} 1 & 0 \\\\ 0 & 3 \\end{pmatrix}\\mathrm{d}t$. The fact that this covariance is independent of the system dynamics ($A, Q$) and the filter performance ($P_{ss}$) is a hallmark property of the continuous-time Kalman-Bucy filter. The filter acts as a whitening filter, transforming the colored measurement process $y(t)$ into a white noise innovation process.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n1 + \\sqrt{3} & 0 \\\\\n0 & \\frac{\\sqrt{39}-6}{3}\n\\end{pmatrix}\n}\n$$", "id": "2713808"}, {"introduction": "A filter's performance is not infinite; it is fundamentally constrained by the system's structural properties. This practice problem delves into the crucial concept of observability, challenging you to analyze a system where one state is not directly measured. By solving for the steady-state error covariance, you will quantitatively see how a lack of observability inflates estimation error, providing a tangible link between system theory and practical filter performance. [@problem_id:2694812]", "problem": "Consider a continuous-time, linear time-invariant stochastic system governed by\n$$\\dot{x}(t) = A x(t) + w(t), \\quad y(t) = C x(t) + v(t),$$\nwhere $x(t) \\in \\mathbb{R}^{2}$, $y(t) \\in \\mathbb{R}$, $w(t)$ is zero-mean white Gaussian process noise with covariance matrix $Q$, and $v(t)$ is zero-mean white Gaussian measurement noise with covariance matrix $R$. The matrices are\n$$A = \\begin{pmatrix} -1 & 0 \\\\ 0 & -2 \\end{pmatrix}, \\quad C = \\begin{pmatrix} 1 & 0 \\end{pmatrix}, \\quad Q = \\begin{pmatrix} 1 & 0 \\\\ 0 & 4 \\end{pmatrix}, \\quad R = 1.$$\n\nStarting from fundamental definitions of the Kalman-Bucy filter and the notion of observability, and using only well-established results as needed, do the following:\n\n- Derive the steady-state equation that the state estimation error covariance matrix $P \\in \\mathbb{R}^{2 \\times 2}$ must satisfy, and solve for $P$ explicitly.\n- Using your solution, reason from first principles how the measurement structure encoded by $C$ and the measurement noise level $R$ affect the directions in the state-space that are well or poorly observable, and explain why poor observability inflates estimation error in those directions relative to the process-noise-driven baseline.\n\nYour final reported quantity must be the trace of the steady-state error covariance matrix $P$. Provide this as a single closed-form expression. No rounding is required, and no units are necessary. Express the final answer as a single mathematical expression only.", "solution": "The problem requires the derivation and solution of the steady-state error covariance for a continuous-time Kalman-Bucy filter, followed by an analysis of the relationship between observability and estimation error.\n\nThe state estimation error covariance matrix, $P(t) = E[(x(t) - \\hat{x}(t))(x(t) - \\hat{x}(t))^T]$, for the given system is governed by the continuous-time differential Riccati equation (DRE):\n$$ \\dot{P}(t) = A P(t) + P(t) A^T - P(t) C^T R^{-1} C P(t) + Q $$\nThe steady-state error covariance matrix, which we denote as $P$, is obtained by setting $\\dot{P}(t) = 0$. This yields the continuous-time algebraic Riccati equation (CARE):\n$$ A P + P A^T - P C^T R^{-1} C P + Q = 0 $$\nLet the symmetric matrix $P$ be represented as $P = \\begin{pmatrix} p_{11} & p_{12} \\\\ p_{12} & p_{22} \\end{pmatrix}$. We are given the matrices:\n$$ A = \\begin{pmatrix} -1 & 0 \\\\ 0 & -2 \\end{pmatrix}, \\quad C = \\begin{pmatrix} 1 & 0 \\end{pmatrix}, \\quad Q = \\begin{pmatrix} 1 & 0 \\\\ 0 & 4 \\end{pmatrix}, \\quad R = 1 $$\nFirst, we compute the term $C^T R^{-1} C$:\n$$ C^T R^{-1} C = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} (1)^{-1} \\begin{pmatrix} 1 & 0 \\end{pmatrix} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 0 \\end{pmatrix} $$\nNow, we substitute all matrices into the CARE.\nThe term $AP + PA^T$ is:\n$$ \\begin{pmatrix} -1 & 0 \\\\ 0 & -2 \\end{pmatrix} \\begin{pmatrix} p_{11} & p_{12} \\\\ p_{12} & p_{22} \\end{pmatrix} + \\begin{pmatrix} p_{11} & p_{12} \\\\ p_{12} & p_{22} \\end{pmatrix} \\begin{pmatrix} -1 & 0 \\\\ 0 & -2 \\end{pmatrix} = \\begin{pmatrix} -p_{11} & -p_{12} \\\\ -2p_{12} & -2p_{22} \\end{pmatrix} + \\begin{pmatrix} -p_{11} & -2p_{12} \\\\ -p_{12} & -2p_{22} \\end{pmatrix} = \\begin{pmatrix} -2p_{11} & -3p_{12} \\\\ -3p_{12} & -4p_{22} \\end{pmatrix} $$\nThe term $P C^T R^{-1} C P$ is:\n$$ \\begin{pmatrix} p_{11} & p_{12} \\\\ p_{12} & p_{22} \\end{pmatrix} \\begin{pmatrix} 1 & 0 \\\\ 0 & 0 \\end{pmatrix} \\begin{pmatrix} p_{11} & p_{12} \\\\ p_{12} & p_{22} \\end{pmatrix} = \\begin{pmatrix} p_{11} & 0 \\\\ p_{12} & 0 \\end{pmatrix} \\begin{pmatrix} p_{11} & p_{12} \\\\ p_{12} & p_{22} \\end{pmatrix} = \\begin{pmatrix} p_{11}^2 & p_{11}p_{12} \\\\ p_{11}p_{12} & p_{12}^2 \\end{pmatrix} $$\nSubstituting these into the CARE, we get:\n$$ \\begin{pmatrix} -2p_{11} & -3p_{12} \\\\ -3p_{12} & -4p_{22} \\end{pmatrix} - \\begin{pmatrix} p_{11}^2 & p_{11}p_{12} \\\\ p_{11}p_{12} & p_{12}^2 \\end{pmatrix} + \\begin{pmatrix} 1 & 0 \\\\ 0 & 4 \\end{pmatrix} = \\begin{pmatrix} 0 & 0 \\\\ 0 & 0 \\end{pmatrix} $$\nThis yields a system of three algebraic equations for the components of $P$:\n1. $ -2p_{11} - p_{11}^2 + 1 = 0 \\implies p_{11}^2 + 2p_{11} - 1 = 0 $\n2. $ -3p_{12} - p_{11}p_{12} = 0 \\implies p_{12}(3+p_{11}) = 0 $\n3. $ -4p_{22} - p_{12}^2 + 4 = 0 \\implies 4p_{22} + p_{12}^2 = 4 $\n\nFrom equation (1), we solve for $p_{11}$ using the quadratic formula:\n$$ p_{11} = \\frac{-2 \\pm \\sqrt{2^2 - 4(1)(-1)}}{2(1)} = \\frac{-2 \\pm \\sqrt{8}}{2} = -1 \\pm \\sqrt{2} $$\nSince $P$ is a covariance matrix, it must be positive semi-definite, which requires its diagonal elements to be non-negative, $p_{11} \\geq 0$. The solution $p_{11} = -1 - \\sqrt{2}$ is negative, so we must choose the unique positive solution:\n$$ p_{11} = \\sqrt{2} - 1 $$\nNow, from equation (2), $p_{12}(3+p_{11}) = 0$. Since $p_{11} = \\sqrt{2} - 1 \\approx 0.414$, the term $(3+p_{11})$ is non-zero. Therefore, we must have:\n$$ p_{12} = 0 $$\nFinally, from equation (3), we solve for $p_{22}$:\n$$ 4p_{22} + 0^2 = 4 \\implies 4p_{22} = 4 \\implies p_{22} = 1 $$\nThe steady-state error covariance matrix is thus:\n$$ P = \\begin{pmatrix} \\sqrt{2} - 1 & 0 \\\\ 0 & 1 \\end{pmatrix} $$\n\nNext, we analyze this result in relation to observability.\nThe measurement equation is $y(t) = C x(t) + v(t) = \\begin{pmatrix} 1 & 0 \\end{pmatrix} \\begin{pmatrix} x_1(t) \\\\ x_2(t) \\end{pmatrix} + v(t) = x_1(t) + v(t)$. This indicates that the measurement $y(t)$ provides direct information only about the state component $x_1(t)$. No direct information about $x_2(t)$ is available from the measurement.\n\nFurthermore, the system matrix $A$ is diagonal, meaning the state dynamics are decoupled:\n$$ \\dot{x}_1(t) = -x_1(t) + w_1(t) $$\n$$ \\dot{x}_2(t) = -2x_2(t) + w_2(t) $$\nSince $x_2(t)$ does not influence the dynamics of $x_1(t)$, there is no indirect way to infer information about $x_2(t)$ from measurements of $x_1(t)$. Therefore, the state $x_2(t)$ is unobservable. This corresponds to the direction spanned by the vector $\\begin{pmatrix} 0 & 1 \\end{pmatrix}^T$ being unobservable. The state $x_1(t)$ is observable.\n\nThe diagonal elements of $P$, $p_{11}$ and $p_{22}$, represent the steady-state variances of the estimation errors for $x_1(t)$ and $x_2(t)$, respectively.\n- For the observable state $x_1(t)$, the error variance is $p_{11} = \\sqrt{2} - 1$. The Kalman filter uses the measurements $y(t)$ to correct the estimate of $x_1(t)$. The term $-P C^T R^{-1} C P$ in the CARE represents the reduction in error variance due to this correction. Without measurements, the estimation error covariance would be governed by the Lyapunov equation $AP + PA^T + Q = 0$. For the first state, a purely model-based (\"open-loop\") estimate would have a steady-state error variance given by $-2p_{11} + Q_{11} = 0 \\implies -2p_{11} + 1 = 0 \\implies p_{11, \\text{open-loop}} = \\frac{1}{2}$. The Kalman filter achieves $p_{11} = \\sqrt{2} - 1 \\approx 0.414$, which is less than the open-loop baseline of $0.5$. This demonstrates the improvement gained from the measurement.\n\n- For the unobservable state $x_2(t)$, the error variance is $p_{22} = 1$. Since no measurement information is available for $x_2(t)$, the Kalman filter cannot correct its estimate. The estimate of $x_2(t)$ is based purely on the model, i.e., it is an open-loop estimate. The steady-state error variance for this open-loop estimate is given by the corresponding Lyapunov equation: $-4p_{22} + Q_{22} = 0 \\implies -4p_{22} + 4 = 0 \\implies p_{22, \\text{open-loop}} = 1$. This is exactly the value obtained from the CARE.\n\nThis illustrates a fundamental principle: poor observability inflates estimation error. The measurement structure, encoded by $C$, determines which directions in the state space are observable. Here, $C$ makes the $x_1$ direction observable and the $x_2$ direction unobservable. The measurement noise level $R$ affects the degree to which uncertainty is reduced in the observable directions. A smaller $R$ would lead to a smaller $p_{11}$, but $p_{22}$ would remain unaffected. The \"process-noise-driven baseline\" corresponds to the error variance in the absence of measurement updates, calculated via the Lyapunov equation. For an unobservable state, the Kalman filter cannot reduce the error below this baseline.\n\nThe final required quantity is the trace of the steady-state error covariance matrix $P$.\n$$ \\text{tr}(P) = p_{11} + p_{22} = (\\sqrt{2} - 1) + 1 = \\sqrt{2} $$", "answer": "$$ \\boxed{\\sqrt{2}} $$", "id": "2694812"}, {"introduction": "The Kalman-Bucy filter finds its most prominent application as a key component of the Linear Quadratic Gaussian (LQG) control framework. This problem demonstrates the celebrated separation principle, where the design of the optimal controller (LQR) and the optimal estimator (Kalman filter) are performed independently. By solving for the regulator and filter gains separately and then analyzing the eigenvalues of the combined system, you will directly verify this cornerstone of modern control theory. [@problem_id:2719606]", "problem": "Consider the continuous-time Linear Time-Invariant (LTI) plant with Gaussian disturbances\n$$\\dot{x}(t) = A\\,x(t) + B\\,u(t) + w(t), \\qquad y(t) = C\\,x(t) + v(t),$$\nwhere $x(t) \\in \\mathbb{R}^{2}$, $u(t) \\in \\mathbb{R}^{2}$, $y(t) \\in \\mathbb{R}^{2}$, and\n$$\nA = \\begin{pmatrix} 1 & 0 \\\\ 0 & -2 \\end{pmatrix}, \\quad\nB = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix}, \\quad\nC = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix}.\n$$\nAssume $w(t)$ and $v(t)$ are independent, zero-mean, white Gaussian processes with spectral density matrices\n$$\nW = \\begin{pmatrix} 1 & 0 \\\\ 0 & 9 \\end{pmatrix}, \\qquad V = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix}.\n$$\nThe performance index is the infinite-horizon quadratic cost\n$$\nJ = \\lim_{T \\to \\infty} \\mathbb{E}\\left[\\int_{0}^{T} \\left(x(t)^{\\top} Q\\,x(t) + u(t)^{\\top} R\\,u(t)\\right)\\,dt \\right],\n$$\nwith\n$$\nQ = \\begin{pmatrix} 4 & 0 \\\\ 0 & 1 \\end{pmatrix}, \\qquad R = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix}.\n$$\nAssume $(A,B)$ is stabilizable and $(A,C)$ is detectable.\n\nUsing only fundamental principles for Linear Quadratic Gaussian (LQG) control, namely: (i) the principle of optimality and the quadratic value function ansatz for the Linear Quadratic Regulator (LQR), and (ii) the innovation process and error-covariance dynamics for the steady-state Kalman-Bucy filter, perform the following:\n\n- Starting from the optimality principle with a quadratic value function, derive the continuous-time algebraic Riccati equation for the LQR and solve it for the given data to obtain the optimal state-feedback gain $K$.\n- Starting from the linear-Gaussian estimation error dynamics, derive the steady-state covariance algebraic Riccati equation for the Kalman-Bucy filter and solve it for the given data to obtain the steady-state filter gain $L$.\n- Invoke the certainty equivalence principle to assemble the LQG controller $u(t) = -K\\,\\hat{x}(t)$ where $\\hat{x}(t)$ is generated by the steady-state Kalman-Bucy filter $\\dot{\\hat{x}}(t) = A\\,\\hat{x}(t) + B\\,u(t) + L\\,(y(t) - C\\,\\hat{x}(t))$.\n- Form the deterministic closed-loop matrix in the $(x,e)$ coordinates with $e \\triangleq x - \\hat{x}$, and determine its eigenvalues using the separation principle.\n\nProvide, as your final answer, the ordered row matrix (in ascending order) of the four real eigenvalues of the closed-loop deterministic $(x,e)$-dynamics implied by the certainty-equivalent LQG controller for the specified $(A,B,C,Q,R,W,V)$. Do not round; give exact values in radicals.", "solution": "The solution to the LQG problem is based on the separation principle, which states that the design of the optimal state-feedback controller and the design of the optimal state estimator can be performed independently. The eigenvalues of the combined closed-loop system are the union of the eigenvalues from the controller (regulator) and the estimator (filter).\n\n**Part 1: The Linear Quadratic Regulator (LQR) Problem**\n\nThe deterministic LQR problem seeks to find a control law $u(t) = -Kx(t)$ that minimizes the cost functional $J = \\int_{0}^{\\infty} (x^\\top Q x + u^\\top R u) dt$ for the system $\\dot{x} = Ax + Bu$. The solution is found via the Hamilton-Jacobi-Bellman (HJB) equation. Assuming a quadratic value function $V(x) = \\frac{1}{2} x^\\top P x$ with $P = P^\\top \\ge 0$, the HJB equation is:\n$$\n\\min_{u} \\left\\{ \\frac{1}{2} x^\\top Q x + \\frac{1}{2} u^\\top R u + (\\nabla_x V)^\\top (Ax+Bu) \\right\\} = 0\n$$\nSubstituting $\\nabla_x V = Px$, we have:\n$$\n\\min_{u} \\left\\{ \\frac{1}{2} x^\\top Q x + \\frac{1}{2} u^\\top R u + x^\\top P (Ax+Bu) \\right\\} = 0\n$$\nTo find the optimal control $u$, we take the gradient with respect to $u$ and set it to zero:\n$$\nR u + B^\\top P x = 0 \\implies u^*(t) = -R^{-1}B^\\top P x(t)\n$$\nThis gives the optimal state-feedback gain matrix $K = R^{-1}B^\\top P$. Substituting $u^*$ back into the HJB equation yields the continuous-time algebraic Riccati equation (CARE) for the controller:\n$$\nA^\\top P + PA - PBR^{-1}B^\\top P + Q = 0\n$$\nFor the given problem, the matrices are:\n$A = \\begin{pmatrix} 1 & 0 \\\\ 0 & -2 \\end{pmatrix}$, $B = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix}$, $Q = \\begin{pmatrix} 4 & 0 \\\\ 0 & 1 \\end{pmatrix}$, $R = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix}$.\nSince $B=I_2$ and $R=I_2$, the CARE simplifies to $A^\\top P + PA - P^2 + Q = 0$.\nGiven that all system matrices are diagonal, we assume a diagonal solution for $P = \\begin{pmatrix} p_1 & 0 \\\\ 0 & p_2 \\end{pmatrix}$. The matrix equation decouples into two scalar algebraic Riccati equations:\nFor $p_1$:\n$$\n(1)p_1 + p_1(1) - p_1^2 + 4 = 0 \\implies p_1^2 - 2p_1 - 4 = 0\n$$\nFor $p_2$:\n$$\n(-2)p_2 + p_2(-2) - p_2^2 + 1 = 0 \\implies p_2^2 + 4p_2 - 1 = 0\n$$\nWe solve these quadratic equations for $p_1$ and $p_2$. We must choose the positive roots to ensure that $P$ is positive definite, which guarantees a stable closed-loop system.\nFor $p_1$: $p_1 = \\frac{2 \\pm \\sqrt{(-2)^2 - 4(1)(-4)}}{2} = \\frac{2 \\pm \\sqrt{20}}{2} = 1 \\pm \\sqrt{5}$. We choose $p_1 = 1 + \\sqrt{5} > 0$.\nFor $p_2$: $p_2 = \\frac{-4 \\pm \\sqrt{4^2 - 4(1)(-1)}}{2} = \\frac{-4 \\pm \\sqrt{20}}{2} = -2 \\pm \\sqrt{5}$. We choose $p_2 = -2 + \\sqrt{5} > 0$.\nSo, the stabilizing solution is $P = \\begin{pmatrix} 1+\\sqrt{5} & 0 \\\\ 0 & -2+\\sqrt{5} \\end{pmatrix}$.\nThe optimal LQR gain is $K = R^{-1}B^\\top P = I_2 \\cdot I_2 \\cdot P = P$.\nThe eigenvalues of the LQR closed-loop system are the eigenvalues of the matrix $A - BK = A - P$.\n$$\nA - P = \\begin{pmatrix} 1 & 0 \\\\ 0 & -2 \\end{pmatrix} - \\begin{pmatrix} 1+\\sqrt{5} & 0 \\\\ 0 & -2+\\sqrt{5} \\end{pmatrix} = \\begin{pmatrix} -\\sqrt{5} & 0 \\\\ 0 & -\\sqrt{5} \\end{pmatrix}\n$$\nThe eigenvalues of the regulator are thus $\\lambda_{1,LQR} = -\\sqrt{5}$ and $\\lambda_{2,LQR} = -\\sqrt{5}$.\n\n**Part 2: The Kalman-Bucy Filter Problem**\n\nThe Kalman-Bucy filter provides an optimal estimate $\\hat{x}(t)$ of the state $x(t)$. The filter dynamics are $\\dot{\\hat{x}} = A\\hat{x} + Bu + L(y - C\\hat{x})$. The estimation error is defined as $e(t) = x(t) - \\hat{x}(t)$. Its dynamics are given by:\n$$\n\\dot{e} = \\dot{x} - \\dot{\\hat{x}} = (Ax+Bu+w) - (A\\hat{x}+Bu+L(Cx+v-C\\hat{x})) = (A-LC)e + w - Lv\n$$\nThe steady-state error covariance matrix $\\Sigma = \\lim_{t\\to\\infty} \\mathbb{E}[e(t)e(t)^\\top]$ is the positive definite solution to the filter algebraic Riccati equation (ARE):\n$$\nA\\Sigma + \\Sigma A^\\top - \\Sigma C^\\top V^{-1} C \\Sigma + W = 0\n$$\nThe optimal filter gain that minimizes the error covariance is given by $L = \\Sigma C^\\top V^{-1}$. The eigenvalues of the filter error dynamics matrix, $A-LC$, determine the filter's stability and performance.\nFor the given problem, the matrices are:\n$A = \\begin{pmatrix} 1 & 0 \\\\ 0 & -2 \\end{pmatrix}$, $C = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix}$, $W = \\begin{pmatrix} 1 & 0 \\\\ 0 & 9 \\end{pmatrix}$, $V = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix}$.\nSince $C=I_2$ and $V=I_2$, the filter ARE simplifies to $A\\Sigma + \\Sigma A^\\top - \\Sigma^2 + W = 0$.\nAgain, we assume a diagonal solution $\\Sigma = \\begin{pmatrix} \\sigma_1 & 0 \\\\ 0 & \\sigma_2 \\end{pmatrix}$. The matrix equation decouples:\nFor $\\sigma_1$:\n$$\n(1)\\sigma_1 + \\sigma_1(1) - \\sigma_1^2 + 1 = 0 \\implies \\sigma_1^2 - 2\\sigma_1 - 1 = 0\n$$\nFor $\\sigma_2$:\n$$\n(-2)\\sigma_2 + \\sigma_2(-2) - \\sigma_2^2 + 9 = 0 \\implies \\sigma_2^2 + 4\\sigma_2 - 9 = 0\n$$\nWe solve for the positive roots to ensure $\\Sigma$ is positive definite.\nFor $\\sigma_1$: $\\sigma_1 = \\frac{2 \\pm \\sqrt{(-2)^2 - 4(1)(-1)}}{2} = \\frac{2 \\pm \\sqrt{8}}{2} = 1 \\pm \\sqrt{2}$. We choose $\\sigma_1 = 1 + \\sqrt{2} > 0$.\nFor $\\sigma_2$: $\\sigma_2 = \\frac{-4 \\pm \\sqrt{4^2 - 4(1)(-9)}}{2} = \\frac{-4 \\pm \\sqrt{52}}{2} = -2 \\pm \\sqrt{13}$. We choose $\\sigma_2 = -2 + \\sqrt{13} > 0$.\nThe steady-state covariance is $\\Sigma = \\begin{pmatrix} 1+\\sqrt{2} & 0 \\\\ 0 & -2+\\sqrt{13} \\end{pmatrix}$.\nThe optimal filter gain is $L = \\Sigma C^\\top V^{-1} = \\Sigma \\cdot I_2 \\cdot I_2 = \\Sigma$.\nThe eigenvalues of the filter error dynamics are the eigenvalues of the matrix $A - LC = A - \\Sigma$.\n$$\nA - \\Sigma = \\begin{pmatrix} 1 & 0 \\\\ 0 & -2 \\end{pmatrix} - \\begin{pmatrix} 1+\\sqrt{2} & 0 \\\\ 0 & -2+\\sqrt{13} \\end{pmatrix} = \\begin{pmatrix} -\\sqrt{2} & 0 \\\\ 0 & -\\sqrt{13} \\end{pmatrix}\n$$\nThe eigenvalues of the filter are thus $\\lambda_{1,EST} = -\\sqrt{2}$ and $\\lambda_{2,EST} = -\\sqrt{13}$.\n\n**Part 3: Closed-Loop System Eigenvalues**\n\nThe LQG controller is formed by applying the LQR gain to the estimated state, an application of the certainty equivalence principle: $u(t) = -K\\hat{x}(t)$.\nThe dynamics of the full system can be described in terms of the state $x$ and the estimation error $e = x - \\hat{x}$.\n$$\n\\dot{x} = Ax + B(-K\\hat{x}) + w = Ax - BK(x-e) + w = (A-BK)x + BKe + w\n$$\n$$\n\\dot{e} = (A-LC)e + w - Lv\n$$\nIn matrix form, the deterministic part of the closed-loop system is:\n$$\n\\frac{d}{dt}\\begin{pmatrix} x \\\\ e \\end{pmatrix} = \\begin{pmatrix} A-BK & BK \\\\ 0 & A-LC \\end{pmatrix} \\begin{pmatrix} x \\\\ e \\end{pmatrix}\n$$\nThe system matrix is block upper-triangular. Therefore, its eigenvalues are the eigenvalues of its diagonal blocks, $A-BK$ and $A-LC$.\nThe set of four eigenvalues for the deterministic closed-loop LQG system is the union of the LQR eigenvalues and the Kalman filter eigenvalues:\n$$\n\\{ -\\sqrt{5}, -\\sqrt{5}, -\\sqrt{2}, -\\sqrt{13} \\}\n$$\nThe problem requires these eigenvalues to be ordered in ascending order (from most negative to least negative). We have $\\sqrt{2} \\approx 1.414$, $\\sqrt{5} \\approx 2.236$, and $\\sqrt{13} \\approx 3.606$. Thus, the ascending order is $-\\sqrt{13}$, $-\\sqrt{5}$, $-\\sqrt{5}$, $-\\sqrt{2}$.", "answer": "$$\n\\boxed{\n\\begin{pmatrix} -\\sqrt{13} & -\\sqrt{5} & -\\sqrt{5} & -\\sqrt{2} \\end{pmatrix}\n}\n$$", "id": "2719606"}]}