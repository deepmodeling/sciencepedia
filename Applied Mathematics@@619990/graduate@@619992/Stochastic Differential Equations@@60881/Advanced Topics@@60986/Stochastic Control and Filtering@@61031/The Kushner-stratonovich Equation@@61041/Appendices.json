{"hands_on_practices": [{"introduction": "The Kushner-Stratonovich equation (KSE) governs the evolution of a conditional probability measure. A fundamental property of any probability measure is that its total mass must remain one. This first exercise serves as a crucial sanity check, verifying that the KSE's intricate structure consistently upholds this principle by showing that the conditional expectation of a constant function remains constant over time [@problem_id:3001883].", "problem": "Consider a nonlinear filtering model on a probability space supporting independent Wiener processes, where the hidden state process $X_t \\in \\mathbb{R}^{n}$ satisfies the stochastic differential equation (SDE) $dX_t = a(X_t)\\,dt + \\Sigma(X_t)\\,dW_t$, with drift $a:\\mathbb{R}^{n} \\to \\mathbb{R}^{n}$, diffusion $\\Sigma:\\mathbb{R}^{n} \\to \\mathbb{R}^{n \\times r}$, and $W_t$ an $\\mathbb{R}^{r}$-valued standard Wiener process. The observation process $Y_t \\in \\mathbb{R}^{m}$ obeys $dY_t = h(X_t)\\,dt + R^{1/2}\\,dV_t$, where $h:\\mathbb{R}^{n} \\to \\mathbb{R}^{m}$, $R \\in \\mathbb{R}^{m \\times m}$ is symmetric positive definite, and $V_t$ is an $\\mathbb{R}^{m}$-valued standard Wiener process independent of $W_t$. Let $\\mathcal{Y}_t$ denote the $\\sigma$-algebra generated by $\\{Y_s: 0 \\le s \\le t\\}$. For a bounded, twice continuously differentiable test function $\\varphi:\\mathbb{R}^{n} \\to \\mathbb{R}$, define the posterior expectation $\\pi_t(\\varphi) := \\mathbb{E}[\\varphi(X_t)\\mid \\mathcal{Y}_t]$, and let $\\mathcal{L}$ denote the infinitesimal generator of $X_t$, given by $\\mathcal{L}\\varphi(x) := a(x) \\cdot \\nabla \\varphi(x) + \\tfrac{1}{2}\\,\\mathrm{tr}\\!\\left(\\Sigma(x)\\Sigma(x)^{\\top}\\nabla^{2}\\varphi(x)\\right)$. The Kushner-Stratonovich equation (KSE) for the evolution of $\\pi_t(\\varphi)$ is\n$$\nd\\pi_t(\\varphi) \\;=\\; \\pi_t(\\mathcal{L}\\varphi)\\,dt \\;+\\; \\Big(\\pi_t(\\varphi\\,h^{\\top}) - \\pi_t(\\varphi)\\,\\pi_t(h)^{\\top}\\Big)\\,R^{-1}\\,\\big(dY_t - \\pi_t(h)\\,dt\\big),\n$$\ninterpreting $\\pi_t(\\varphi\\,h^{\\top})$ as the $1\\times m$ row vector whose $j$-th component is $\\pi_t(\\varphi\\,h_j)$, and $\\pi_t(h)$ as the $m\\times 1$ column vector with $j$-th component $\\pi_t(h_j)$. Using only the above definitions and standard properties of conditional expectation and generators for stochastic processes, compute the stochastic differential $d\\pi_t(1)$ obtained by setting the test function $\\varphi \\equiv 1$ in the Kushner-Stratonovich equation, and provide your final answer as a closed-form analytic expression. No rounding is required and no units are involved. Your final answer must be a single expression.", "solution": "The objective is to compute the stochastic differential $d\\pi_t(1)$ by setting the test function $\\varphi$ to be the constant function $\\varphi(x) = 1$ in the given Kushner-Stratonovich equation (KSE).\n\nThe KSE is given by:\n$$\nd\\pi_t(\\varphi) \\;=\\; \\pi_t(\\mathcal{L}\\varphi)\\,dt \\;+\\; \\Big(\\pi_t(\\varphi\\,h^{\\top}) - \\pi_t(\\varphi)\\,\\pi_t(h)^{\\top}\\Big)\\,R^{-1}\\,\\big(dY_t - \\pi_t(h)\\,dt\\big)\n$$\nWe must evaluate each term in this equation for the specific test function $\\varphi(x) = 1$ for all $x \\in \\mathbb{R}^n$.\n\nFirst, let us analyze the drift term, which involves $\\pi_t(\\mathcal{L}\\varphi)$. The infinitesimal generator $\\mathcal{L}$ is defined as:\n$$\n\\mathcal{L}\\varphi(x) := a(x) \\cdot \\nabla \\varphi(x) + \\frac{1}{2}\\,\\mathrm{tr}\\!\\left(\\Sigma(x)\\Sigma(x)^{\\top}\\nabla^{2}\\varphi(x)\\right)\n$$\nFor the constant function $\\varphi(x) = 1$, its gradient (the vector of first partial derivatives) and its Hessian (the matrix of second partial derivatives) are both zero.\nSpecifically, the gradient is:\n$$\n\\nabla\\varphi(x) = \\nabla(1) = 0 \\in \\mathbb{R}^{n}\n$$\nAnd the Hessian is:\n$$\n\\nabla^{2}\\varphi(x) = \\nabla^{2}(1) = \\mathbf{0}_{n \\times n} \\in \\mathbb{R}^{n \\times n}\n$$\nwhere $\\mathbf{0}_{n \\times n}$ is the $n \\times n$ zero matrix.\n\nSubstituting these into the expression for the generator $\\mathcal{L}$ applied to $\\varphi = 1$, we get:\n$$\n\\mathcal{L}1(x) = a(x) \\cdot 0 + \\frac{1}{2}\\,\\mathrm{tr}\\!\\left(\\Sigma(x)\\Sigma(x)^{\\top}\\mathbf{0}_{n \\times n}\\right) = 0 + \\frac{1}{2}\\,\\mathrm{tr}(\\mathbf{0}_{n \\times n}) = 0\n$$\nSo, the function $\\mathcal{L}1$ is identically zero for all $x \\in \\mathbb{R}^n$.\n\nNow we can compute the term $\\pi_t(\\mathcal{L}1)$:\n$$\n\\pi_t(\\mathcal{L}1) = \\pi_t(0) = \\mathbb{E}[0 \\mid \\mathcal{Y}_t]\n$$\nBy the properties of conditional expectation, the expected value of a constant is the constant itself. Therefore:\n$$\n\\pi_t(\\mathcal{L}1) = 0\n$$\nThis implies that the entire drift part of the KSE vanishes:\n$$\n\\pi_t(\\mathcal{L}1)\\,dt = 0 \\cdot dt = 0\n$$\n\nNext, we analyze the coefficient of the stochastic term, which is the $1 \\times m$ row vector:\n$$\n\\mathbf{C}_t(\\varphi) := \\pi_t(\\varphi\\,h^{\\top}) - \\pi_t(\\varphi)\\,\\pi_t(h)^{\\top}\n$$\nWe substitute $\\varphi = 1$:\n$$\n\\mathbf{C}_t(1) = \\pi_t(1 \\cdot h^{\\top}) - \\pi_t(1)\\,\\pi_t(h)^{\\top} = \\pi_t(h^{\\top}) - \\pi_t(1)\\,\\pi_t(h)^{\\top}\n$$\nLet's evaluate the terms in this expression. First, $\\pi_t(1)$ is, by definition:\n$$\n\\pi_t(1) = \\mathbb{E}[1 \\mid \\mathcal{Y}_t] = 1\n$$\nNow, we consider the vector terms. According to the problem statement, $\\pi_t(\\varphi\\,h^{\\top})$ is the $1 \\times m$ row vector whose $j$-th component is $\\pi_t(\\varphi\\,h_j)$. For $\\varphi = 1$, the $j$-th component of $\\pi_t(h^{\\top})$ is $\\pi_t(1 \\cdot h_j) = \\pi_t(h_j)$. So,\n$$\n\\pi_t(h^{\\top}) = \\begin{pmatrix} \\pi_t(h_1) & \\pi_t(h_2) & \\cdots & \\pi_t(h_m) \\end{pmatrix}\n$$\nAlso by definition, $\\pi_t(h)$ is the $m \\times 1$ column vector whose $j$-th component is $\\pi_t(h_j)$:\n$$\n\\pi_t(h) = \\begin{pmatrix} \\pi_t(h_1) \\\\ \\pi_t(h_2) \\\\ \\vdots \\\\ \\pi_t(h_m) \\end{pmatrix}\n$$\nTaking the transpose of this column vector gives the $1 \\times m$ row vector:\n$$\n\\pi_t(h)^{\\top} = \\begin{pmatrix} \\pi_t(h_1) & \\pi_t(h_2) & \\cdots & \\pi_t(h_m) \\end{pmatrix}\n$$\nNow we can compute the coefficient vector $\\mathbf{C}_t(1)$:\n$$\n\\mathbf{C}_t(1) = \\pi_t(h^{\\top}) - \\pi_t(1)\\,\\pi_t(h)^{\\top} = \\pi_t(h^{\\top}) - 1 \\cdot \\pi_t(h)^{\\top} = \\pi_t(h^{\\top}) - \\pi_t(h)^{\\top}\n$$\nSince $\\pi_t(h^{\\top})$ and $\\pi_t(h)^{\\top}$ are identical row vectors, their difference is the zero row vector:\n$$\n\\mathbf{C}_t(1) = \\begin{pmatrix} 0 & 0 & \\cdots & 0 \\end{pmatrix} \\in \\mathbb{R}^{1 \\times m}\n$$\nThus, the entire stochastic term in the KSE is zero:\n$$\n\\mathbf{C}_t(1)\\,R^{-1}\\,\\big(dY_t - \\pi_t(h)\\,dt\\big) = \\begin{pmatrix} 0 & \\cdots & 0 \\end{pmatrix}\\,R^{-1}\\,\\big(dY_t - \\pi_t(h)\\,dt\\big) = 0\n$$\n\nCombining the results for the drift and stochastic terms, we find the stochastic differential of $\\pi_t(1)$:\n$$\nd\\pi_t(1) = 0 + 0 = 0\n$$\nThis result is consistent with the direct definition of $\\pi_t(1)$. Since $\\pi_t(1) = \\mathbb{E}[1 \\mid \\mathcal{Y}_t] = 1$ for all $t \\ge 0$, the process $\\pi_t(1)$ is a constant process, and its differential must be zero. The calculation confirms that the KSE correctly captures this fundamental property.\nThe closed-form analytic expression for $d\\pi_t(1)$ is therefore $0$.", "answer": "$$\\boxed{0}$$", "id": "3001883"}, {"introduction": "The evolution of the conditional distribution in nonlinear filtering is often described in two equivalent ways: a stochastic partial differential equation (SPDE) for the density itself, or an SDE for the expectation of arbitrary test functions—the weak form. This practice illuminates the formal link between these two perspectives. By starting with the density's SPDE, you will use integration by parts and the properties of the adjoint operator to derive the weak-form KSE, deepening your understanding of the equation's mathematical foundation [@problem_id:3001901].", "problem": "Consider a partially observed diffusion on $\\mathbb{R}^n$ given by the signal process $X_t$ and the observation process $Y_t$:\n$$\n\\mathrm{d}X_t = a(X_t)\\,\\mathrm{d}t + \\Sigma(X_t)\\,\\mathrm{d}B_t,\\qquad \\mathrm{d}Y_t = h(X_t)\\,\\mathrm{d}t + \\mathrm{d}W_t,\n$$\nwhere $B_t$ and $W_t$ are independent standard Brownian motions, $a:\\mathbb{R}^n\\to\\mathbb{R}^n$, $\\Sigma:\\mathbb{R}^n\\to\\mathbb{R}^{n\\times m}$, and $h:\\mathbb{R}^n\\to\\mathbb{R}$ are sufficiently smooth and of at most polynomial growth. Let $\\mathcal{L}$ be the infinitesimal generator of $X_t$, acting on $C^2$ test functions $\\varphi$ by\n$$\n\\mathcal{L}\\varphi(x) = a(x)\\cdot \\nabla \\varphi(x) + \\tfrac{1}{2}\\,\\mathrm{tr}\\!\\big(\\Sigma(x)\\Sigma(x)^{\\top}\\nabla^2\\varphi(x)\\big),\n$$\nand let $\\mathcal{L}^*$ denote its adjoint acting on densities. Assume that the conditional law of $X_t$ given the observation filtration admits a density $p_t(x)$ with respect to Lebesgue measure, and that $p_t$ satisfies the nonlinear density stochastic partial differential equation\n$$\n\\mathrm{d}p_t(x) = \\mathcal{L}^*p_t(x)\\,\\mathrm{d}t + \\big(h(x)-\\hat h_t\\big)\\,p_t(x)\\,\\big(\\mathrm{d}Y_t - \\hat h_t\\,\\mathrm{d}t\\big),\n$$\nwhere $\\hat h_t := \\int_{\\mathbb{R}^n} h(x)\\,p_t(x)\\,\\mathrm{d}x$. Define the conditional expectation operator $\\pi_t(\\varphi):=\\int_{\\mathbb{R}^n}\\varphi(x)\\,p_t(x)\\,\\mathrm{d}x$ for $\\varphi\\in C_c^\\infty(\\mathbb{R}^n)$, and assume that boundary terms vanish under integration by parts and that interchanges of integration and stochastic integration are justified by appropriate integrability and regularity conditions.\n\nBy testing the density stochastic partial differential equation against a smooth compactly supported $\\varphi$ and integrating over $\\mathbb{R}^n$, which of the following weak-form evolution equations for $\\pi_t(\\varphi)$ is the correct one?\n\nA. $\\mathrm{d}\\pi_t(\\varphi) = \\pi_t(\\mathcal{L}\\varphi)\\,\\mathrm{d}t + \\pi_t(\\varphi h)\\,\\mathrm{d}Y_t - \\pi_t(\\varphi)\\,\\pi_t(h)\\,\\mathrm{d}t.$\n\nB. $\\mathrm{d}\\pi_t(\\varphi) = \\pi_t(\\mathcal{L}\\varphi)\\,\\mathrm{d}t + \\big(\\pi_t(\\varphi h)-\\pi_t(\\varphi)\\pi_t(h)\\big)\\,\\mathrm{d}Y_t.$\n\nC. $\\mathrm{d}\\pi_t(\\varphi) = \\pi_t(\\mathcal{L}\\varphi)\\,\\mathrm{d}t + \\big(\\pi_t(\\varphi h)-\\pi_t(\\varphi)\\pi_t(h)\\big)\\,\\big(\\mathrm{d}Y_t - \\pi_t(h)\\,\\mathrm{d}t\\big).$\n\nD. $\\mathrm{d}\\pi_t(\\varphi) = \\pi_t(\\mathcal{L}^*\\varphi)\\,\\mathrm{d}t + \\big(\\pi_t(\\varphi h)-\\pi_t(h)\\big)\\,\\big(\\mathrm{d}Y_t - \\pi_t(h)\\,\\mathrm{d}t\\big).$\n\nE. $\\mathrm{d}\\pi_t(\\varphi) = \\pi_t(\\mathcal{L}\\varphi)\\,\\mathrm{d}t + \\pi_t(\\varphi h)\\,\\big(\\mathrm{d}Y_t - \\pi_t(h)\\,\\mathrm{d}t\\big).$", "solution": "### Step 1: Extract Givens\n\nThe problem provides the following information:\n1.  A signal process $X_t$ in $\\mathbb{R}^n$ governed by the stochastic differential equation (SDE):\n    $$\n    \\mathrm{d}X_t = a(X_t)\\,\\mathrm{d}t + \\Sigma(X_t)\\,\\mathrm{d}B_t\n    $$\n2.  An observation process $Y_t$ in $\\mathbb{R}$ governed by the SDE:\n    $$\n    \\mathrm{d}Y_t = h(X_t)\\,\\mathrm{d}t + \\mathrm{d}W_t\n    $$\n3.  $B_t$ and $W_t$ are independent standard Brownian motions. The dimensions are specified as $a:\\mathbb{R}^n\\to\\mathbb{R}^n$, $\\Sigma:\\mathbb{R}^n\\to\\mathbb{R}^{n\\times m}$, and $h:\\mathbb{R}^n\\to\\mathbb{R}$.\n4.  The functions $a$, $\\Sigma$, and $h$ are sufficiently smooth and have at most polynomial growth.\n5.  The infinitesimal generator of $X_t$ is $\\mathcal{L}$, defined for $C^2$ functions $\\varphi$ as:\n    $$\n    \\mathcal{L}\\varphi(x) = a(x)\\cdot \\nabla \\varphi(x) + \\tfrac{1}{2}\\,\\mathrm{tr}\\!\\big(\\Sigma(x)\\Sigma(x)^{\\top}\\nabla^2\\varphi(x)\\big)\n    $$\n6.  The conditional law of $X_t$ given the observation filtration has a density $p_t(x)$ which satisfies the Kushner-Stratonovich equation:\n    $$\n    \\mathrm{d}p_t(x) = \\mathcal{L}^*p_t(x)\\,\\mathrm{d}t + \\big(h(x)-\\hat h_t\\big)\\,p_t(x)\\,\\big(\\mathrm{d}Y_t - \\hat h_t\\,\\mathrm{d}t\\big)\n    $$\n    where $\\mathcal{L}^*$ is the adjoint of $\\mathcal{L}$.\n7.  The conditional expectation of $h(X_t)$ is defined as $\\hat h_t := \\int_{\\mathbb{R}^n} h(x)\\,p_t(x)\\,\\mathrm{d}x$.\n8.  The conditional expectation operator $\\pi_t$ is defined for a test function $\\varphi\\in C_c^\\infty(\\mathbb{R}^n)$ as $\\pi_t(\\varphi):=\\int_{\\mathbb{R}^n}\\varphi(x)\\,p_t(x)\\,\\mathrm{d}x$.\n9.  Key assumptions are made: boundary terms vanish under integration by parts, and interchange of integration and stochastic integration is permitted.\n\nThe question is to derive the weak-form evolution equation for $\\pi_t(\\varphi)$.\n\n### Step 2: Validate Using Extracted Givens\n\nThe problem statement describes a standard setup in nonlinear filtering theory.\n- **Scientifically Grounded (Critical)**: The problem is firmly rooted in the mathematical theory of stochastic processes and filtering. The SDEs for the signal and observation processes, the definition of the generator $\\mathcal{L}$, and the Kushner-Stratonovich SPDE for the conditional density $p_t(x)$ are all standard and correct formulations in this field. No scientific or mathematical laws are violated.\n- **Well-Posed**: The question asks for the derivation of a specific SDE from a given SPDE, which is a well-defined mathematical task. The provided assumptions (smoothness, growth conditions, vanishing boundary terms, etc.) are precisely the ones needed to ensure the formal manipulations are valid, leading to a unique result.\n- **Objective (Critical)**: The language is precise, mathematical, and free of any subjectivity or ambiguity. All terms are standard in the literature of stochastic analysis.\n\nThe problem does not exhibit any of the invalidity flaws:\n1.  **Scientific or Factual Unsoundness**: The setup is a cornerstone of filtering theory.\n2.  **Non-Formalizable or Irrelevant**: The problem is a direct mathematical derivation within the specified topic.\n3.  **Incomplete or Contradictory Setup**: The setup is complete and self-consistent. The necessary technical assumptions are explicitly stated.\n4.  **Unrealistic or Infeasible**: The mathematical model is an abstraction, but it is a standard and widely used one for realistic systems.\n5.  **Ill-Posed or Poorly Structured**: The task is well-defined and leads to a unique answer.\n6.  **Pseudo-Profound, Trivial, or Tautological**: The derivation is non-trivial and requires a correct application of Itô calculus, properties of adjoint operators, and the definitions of conditional expectations.\n7.  **Outside Scientific Verifiability**: The derivation is a mathematical proof and is fully verifiable.\n\n### Step 3: Verdict and Action\n\nThe problem statement is valid. I will proceed with the solution.\n\n### Derivation of the SDE for $\\pi_t(\\varphi)$\n\nWe want to find the differential $\\mathrm{d}\\pi_t(\\varphi)$. Starting from the definition of $\\pi_t(\\varphi)$:\n$$\n\\pi_t(\\varphi) = \\int_{\\mathbb{R}^n} \\varphi(x) p_t(x) \\mathrm{d}x\n$$\nWe take the stochastic differential. Based on the problem's assumption that we can interchange differentiation and integration, we have:\n$$\n\\mathrm{d}\\pi_t(\\varphi) = \\mathrm{d} \\left( \\int_{\\mathbb{R}^n} \\varphi(x) p_t(x) \\mathrm{d}x \\right) = \\int_{\\mathbb{R}^n} \\varphi(x) \\mathrm{d}p_t(x) \\mathrm{d}x\n$$\nNow, we substitute the given Kushner-Stratonovich SPDE for $\\mathrm{d}p_t(x)$:\n$$\n\\mathrm{d}\\pi_t(\\varphi) = \\int_{\\mathbb{R}^n} \\varphi(x) \\left[ \\mathcal{L}^*p_t(x)\\,\\mathrm{d}t + \\big(h(x)-\\hat h_t\\big)\\,p_t(x)\\,\\big(\\mathrm{d}Y_t - \\hat h_t\\,\\mathrm{d}t\\big) \\right] \\mathrm{d}x\n$$\nWe can separate this into two terms: one involving $\\mathrm{d}t$ (the drift) and one involving the innovation process $(\\mathrm{d}Y_t - \\hat h_t\\,\\mathrm{d}t)$.\n$$\n\\mathrm{d}\\pi_t(\\varphi) = \\left( \\int_{\\mathbb{R}^n} \\varphi(x) \\mathcal{L}^*p_t(x) \\mathrm{d}x \\right) \\mathrm{d}t + \\left( \\int_{\\mathbb{R}^n} \\varphi(x) \\big(h(x)-\\hat h_t\\big)\\,p_t(x) \\mathrm{d}x \\right) \\big(\\mathrm{d}Y_t - \\hat h_t\\,\\mathrm{d}t\\big)\n$$\nLet's analyze each integral separately.\n\n1.  **The Drift Term**: The first integral is $\\int_{\\mathbb{R}^n} \\varphi(x) \\mathcal{L}^*p_t(x) \\mathrm{d}x$.\n    By the definition of the adjoint operator $\\mathcal{L}^*$, and using the assumption that boundary terms from integration by parts vanish (since $\\varphi$ has compact support), we can move the operator from $p_t$ to $\\varphi$:\n    $$\n    \\int_{\\mathbb{R}^n} \\varphi(x) (\\mathcal{L}^*p_t)(x) \\mathrm{d}x = \\int_{\\mathbb{R}^n} (\\mathcal{L}\\varphi)(x) p_t(x) \\mathrm{d}x\n    $$\n    The right-hand side is, by definition, the conditional expectation of $\\mathcal{L}\\varphi(X_t)$, which is $\\pi_t(\\mathcal{L}\\varphi)$. So the drift term from the signal dynamics is $\\pi_t(\\mathcal{L}\\varphi)\\mathrm{d}t$.\n\n2.  **The Stochastic Term**: The coefficient of the stochastic part is the integral $\\int_{\\mathbb{R}^n} \\varphi(x) \\big(h(x)-\\hat h_t\\big)\\,p_t(x) \\mathrm{d}x$. We expand the integrand:\n    $$\n    \\int_{\\mathbb{R}^n} \\left( \\varphi(x)h(x) - \\varphi(x)\\hat h_t \\right) p_t(x) \\mathrm{d}x\n    $$\n    Since $\\hat h_t$ is a scalar that does not depend on the integration variable $x$, we can take it out of the second integral:\n    $$\n    \\int_{\\mathbb{R}^n} \\varphi(x)h(x) p_t(x) \\mathrm{d}x - \\hat h_t \\int_{\\mathbb{R}^n} \\varphi(x) p_t(x) \\mathrm{d}x\n    $$\n    Using the definition of the operator $\\pi_t$, this becomes:\n    $$\n    \\pi_t(\\varphi h) - \\hat h_t \\pi_t(\\varphi)\n    $$\n    The problem also gives the definition $\\hat h_t = \\int_{\\mathbb{R}^n} h(x)\\,p_t(x)\\,\\mathrm{d}x$, which is exactly $\\pi_t(h)$. Substituting this, the coefficient of the stochastic term is:\n    $$\n    \\pi_t(\\varphi h) - \\pi_t(\\varphi)\\pi_t(h)\n    $$\n    This term is the conditional covariance, given the observation history, of $\\varphi(X_t)$ and $h(X_t)$.\n\nCombining the parts, the full SDE for $\\pi_t(\\varphi)$ is:\n$$\n\\mathrm{d}\\pi_t(\\varphi) = \\pi_t(\\mathcal{L}\\varphi)\\,\\mathrm{d}t + \\big(\\pi_t(\\varphi h)-\\pi_t(\\varphi)\\pi_t(h)\\big)\\,\\big(\\mathrm{d}Y_t - \\pi_t(h)\\,\\mathrm{d}t\\big)\n$$\nThis equation is a fundamental result in filtering theory, often called the Zakai equation in its unnormalized form, but here presented for the normalized conditional expectation.\n\n### Option-by-Option Analysis\n\nNow we evaluate each of the given options against our derived result.\n\n- **Option A**: $\\mathrm{d}\\pi_t(\\varphi) = \\pi_t(\\mathcal{L}\\varphi)\\,\\mathrm{d}t + \\pi_t(\\varphi h)\\,\\mathrm{d}Y_t - \\pi_t(\\varphi)\\,\\pi_t(h)\\,\\mathrm{d}t.$\n  If we expand this option, the coefficient of $\\mathrm{d}Y_t$ is $\\pi_t(\\varphi h)$, which is incorrect. The correct coefficient is $\\pi_t(\\varphi h) - \\pi_t(\\varphi)\\pi_t(h)$. The drift term is also incorrect when compared to the fully expanded form of our derived equation. **Incorrect**.\n\n- **Option B**: $\\mathrm{d}\\pi_t(\\varphi) = \\pi_t(\\mathcal{L}\\varphi)\\,\\mathrm{d}t + \\big(\\pi_t(\\varphi h)-\\pi_t(\\varphi)\\pi_t(h)\\big)\\,\\mathrm{d}Y_t.$\n  This option has the correct coefficient for the stochastic term, but the stochastic integral is with respect to $\\mathrm{d}Y_t$, not the innovations process $\\mathrm{d}Y_t - \\pi_t(h)\\,\\mathrm{d}t$. It is missing the drift correction term that arises from $-\\pi_t(h)\\,\\mathrm{d}t$. **Incorrect**.\n\n- **Option C**: $\\mathrm{d}\\pi_t(\\varphi) = \\pi_t(\\mathcal{L}\\varphi)\\,\\mathrm{d}t + \\big(\\pi_t(\\varphi h)-\\pi_t(\\varphi)\\pi_t(h)\\big)\\,\\big(\\mathrm{d}Y_t - \\pi_t(h)\\,\\mathrm{d}t\\big).$\n  This equation matches our derived result exactly. It correctly identifies the drift part $\\pi_t(\\mathcal{L}\\varphi)$, the conditional covariance term $\\pi_t(\\varphi h)-\\pi_t(\\varphi)\\pi_t(h)$, and the innovations process differential $\\mathrm{d}Y_t - \\pi_t(h)\\,\\mathrm{d}t$. **Correct**.\n\n- **Option D**: $\\mathrm{d}\\pi_t(\\varphi) = \\pi_t(\\mathcal{L}^*\\varphi)\\,\\mathrm{d}t + \\big(\\pi_t(\\varphi h)-\\pi_t(h)\\big)\\,\\big(\\mathrm{d}Y_t - \\pi_t(h)\\,\\mathrm{d}t\\big).$\n  This option has two errors. First, the drift term is $\\pi_t(\\mathcal{L}^*\\varphi)$, which results from incorrectly failing to use the adjoint property to move the operator to $\\varphi$. The correct term is $\\pi_t(\\mathcal{L}\\varphi)$. Second, the covariance term is written as $\\pi_t(\\varphi h)-\\pi_t(h)$, which is missing a factor of $\\pi_t(\\varphi)$ and is generally incorrect. **Incorrect**.\n\n- **Option E**: $\\mathrm{d}\\pi_t(\\varphi) = \\pi_t(\\mathcal{L}\\varphi)\\,\\mathrm{d}t + \\pi_t(\\varphi h)\\,\\big(\\mathrm{d}Y_t - \\pi_t(h)\\,\\mathrm{d}t\\big).$\n  This option has the correct drift term $\\pi_t(\\mathcal{L}\\varphi)$ and the correct innovations process differential. However, the coefficient of the stochastic term is given as $\\pi_t(\\varphi h)$, which is missing the subtraction of $\\pi_t(\\varphi)\\pi_t(h)$. As shown in the derivation, the correct coefficient is the conditional covariance $\\pi_t(\\varphi h)-\\pi_t(\\varphi)\\pi_t(h)$. **Incorrect**.", "answer": "$$\\boxed{C}$$", "id": "3001901"}, {"introduction": "In its general form, the Kushner-Stratonovich equation is an infinite-dimensional SPDE and often computationally intractable. However, for the foundational linear-Gaussian case, it famously collapses into a finite-dimensional and solvable system. This exercise makes the abstract theory concrete by guiding you to derive the celebrated Kalman-Bucy filter equations, demonstrating how the general KSE framework yields this powerful and practical result [@problem_id:3001892].", "problem": "Consider the following scalar continuous-time hidden-parameter model. Let the hidden state be time-invariant, satisfying $dX_t = 0$ with $X_0 \\sim \\mathcal{N}(m_0, P_0)$, where $m_0 \\in \\mathbb{R}$ and $P_0 > 0$. The observation process satisfies\n$$\ndY_t = h X_t \\, dt + \\sigma \\, dV_t, \\quad Y_0 = 0,\n$$\nwhere $h \\in \\mathbb{R}$ is known, $\\sigma > 0$ is known, and $\\{V_t\\}_{t \\ge 0}$ is a standard Wiener process independent of $X_0$. Fix a deterministic horizon $T > 0$ and suppose the observed terminal value $Y_T$ equals $y_T \\in \\mathbb{R}$.\n\nStarting from fundamental definitions of conditional expectation, the innovation process, and the Radon-Nikodym likelihood used in continuous-time Bayesian filtering, do the following:\n\n- Derive the posterior mean $m_T = \\mathbb{E}[X_T \\mid \\mathcal{Y}_T]$ using the Kushner-Stratonovich equation (KSE) viewpoint for the test function $\\varphi(x) = x$, where $\\mathcal{Y}_t$ denotes the $\\sigma$-algebra generated by $\\{Y_s: 0 \\le s \\le t\\}$.\n- Independently, derive the unnormalized conditional density using the Zakai equation (viewed through the Kallianpur-Striebel likelihood representation), normalize it to obtain the posterior, and compute the posterior mean $m_T$.\n- Explain briefly why these two derivations must coincide in this model.\n\nYour final answer must be a single closed-form analytic expression for $m_T$ in terms of $m_0$, $P_0$, $h$, $\\sigma$, $T$, and $y_T$. Do not round. Do not include units.", "solution": "The problem proposed is a standard, well-posed problem in continuous-time nonlinear filtering, specifically for a linear-Gaussian system. It is scientifically grounded, self-contained, and objective. All necessary parameters and conditions are provided, and there are no contradictions. We may therefore proceed with the solution.\n\nThe problem asks for the posterior mean $m_T = \\mathbb{E}[X_T \\mid \\mathcal{Y}_T]$ to be derived in two separate ways and for an explanation of why the results coincide. The model is given by:\nState process: $dX_t = 0$ with $X_0 \\sim \\mathcal{N}(m_0, P_0)$. This implies $X_t = X_0$ for all $t \\ge 0$.\nObservation process: $dY_t = h X_t \\, dt + \\sigma \\, dV_t$, with $Y_0=0$.\n\n### 1. Derivation via the Kushner-Stratonovich Equation (KSE)\n\nThe Kushner-Stratonovich equation describes the evolution of the conditional expectation $\\pi_t(\\varphi) = \\mathbb{E}[\\varphi(X_t) \\mid \\mathcal{Y}_t]$ for a suitable test function $\\varphi$. The general form is:\n$$ d\\pi_t(\\varphi) = \\pi_t(\\mathcal{L}\\varphi) dt + \\frac{1}{\\sigma^2} \\left( \\pi_t(\\varphi H) - \\pi_t(\\varphi)\\pi_t(H) \\right) [dY_t - \\pi_t(H) dt] $$\nwhere $\\mathcal{L}$ is the generator of the state process $X_t$, and $H(x)$ is the observation function.\n\nFor our problem:\n- The state process is $dX_t = 0$, so the generator $\\mathcal{L}$ is the zero operator, $\\mathcal{L}=0$.\n- The observation function is $H(X_t) = h X_t$.\n- We are interested in the conditional mean $m_t = \\mathbb{E}[X_t \\mid \\mathcal{Y}_t]$, which corresponds to choosing the test function $\\varphi(x) = x$.\n\nLet's compute the terms in the KSE for $\\varphi(x)=x$:\n- $\\pi_t(\\varphi) = \\pi_t(x) = \\mathbb{E}[X_t \\mid \\mathcal{Y}_t] = m_t$.\n- $\\pi_t(\\mathcal{L}\\varphi) = \\pi_t(\\mathcal{L}x) = \\pi_t(0) = 0$.\n- $\\pi_t(H) = \\pi_t(hx) = h\\pi_t(x) = h m_t$.\n- $\\pi_t(\\varphi H) = \\pi_t(x \\cdot hx) = h\\pi_t(x^2) = h \\mathbb{E}[X_t^2 \\mid \\mathcal{Y}_t]$.\n\nThe term $\\mathbb{E}[X_t^2 \\mid \\mathcal{Y}_t] - (\\mathbb{E}[X_t \\mid \\mathcal{Y}_t])^2$ is the conditional variance of $X_t$, which we denote by $P_t = \\mathrm{Var}(X_t \\mid \\mathcal{Y}_t)$. So, $\\mathbb{E}[X_t^2 \\mid \\mathcal{Y}_t] = P_t + m_t^2$.\nThe term $\\pi_t(\\varphi H) - \\pi_t(\\varphi)\\pi_t(H)$ becomes $h\\mathbb{E}[X_t^2 \\mid \\mathcal{Y}_t] - m_t(h m_t) = h(P_t + m_t^2) - h m_t^2 = h P_t$.\nThe innovations process is $dI_t = dY_t - \\pi_t(H)dt = dY_t - h m_t dt$.\n\nSubstituting these into the KSE gives the stochastic differential equation (SDE) for the conditional mean $m_t$:\n$$ dm_t = \\frac{h P_t}{\\sigma^2} (dY_t - h m_t dt) $$\nThis is the Kalman-Bucy filter equation for the mean. To solve it, we need the evolution of the conditional variance $P_t$. For a linear-Gaussian system, the equation for $P_t$ is the deterministic Riccati differential equation:\n$$ \\frac{dP_t}{dt} = A P_t + P_t A^T - P_t C^T (D D^T)^{-1} C P_t + B B^T $$\nIn our scalar case, the state dynamics $dX_t = 0 \\cdot X_t dt + 0 \\cdot dW_t$ give $A=0$ and $B=0$. The observation dynamics $dY_t = h X_t dt + \\sigma dV_t$ give $C=h$ and $D=\\sigma$. The Riccati equation simplifies to:\n$$ \\frac{dP_t}{dt} = -P_t h (\\sigma^2)^{-1} h P_t = -\\frac{h^2}{\\sigma^2} P_t^2 $$\nThis is a separable ordinary differential equation with initial condition $P(0) = P_0$. We solve it:\n$$ \\int_{P_0}^{P_T} \\frac{dP}{P^2} = -\\int_0^T \\frac{h^2}{\\sigma^2} dt \\implies \\left[-\\frac{1}{P}\\right]_{P_0}^{P_T} = -\\frac{h^2 T}{\\sigma^2} \\implies -\\frac{1}{P_T} + \\frac{1}{P_0} = -\\frac{h^2 T}{\\sigma^2} $$\nSolving for $P_T$ gives:\n$$ \\frac{1}{P_T} = \\frac{1}{P_0} + \\frac{h^2 T}{\\sigma^2} = \\frac{\\sigma^2 + P_0 h^2 T}{P_0 \\sigma^2} \\implies P_T = \\frac{P_0 \\sigma^2}{\\sigma^2 + P_0 h^2 T} $$\nNow, we solve the SDE for $m_t$:\n$$ dm_t + \\frac{h^2 P_t}{\\sigma^2} m_t dt = \\frac{h P_t}{\\sigma^2} dY_t $$\nLet $a(t) = \\frac{h^2 P_t}{\\sigma^2} = \\frac{h^2 P_0}{\\sigma^2 + P_0 h^2 t}$. The integrating factor is $\\mu(t) = \\exp\\left(\\int_0^t a(s) \\mathrm{d}s\\right)$.\n$$ \\int_0^t a(s) \\mathrm{d}s = \\int_0^t \\frac{h^2 P_0}{\\sigma^2 + P_0 h^2 s} \\mathrm{d}s = \\left[ \\ln(\\sigma^2 + P_0 h^2 s) \\right]_0^t = \\ln\\left(\\frac{\\sigma^2 + P_0 h^2 t}{\\sigma^2}\\right) $$\nSo, $\\mu(t) = \\frac{\\sigma^2 + P_0 h^2 t}{\\sigma^2}$. Using Itô's product rule, $d(\\mu_t m_t) = \\mu_t \\mathrm{d}m_t + m_t \\mathrm{d}\\mu_t = \\mu_t(\\mathrm{d}m_t + a(t)m_t dt)$.\n$$ d(\\mu_t m_t) = \\mu_t \\left( \\frac{h P_t}{\\sigma^2} \\mathrm{d}Y_t \\right) = \\left(\\frac{\\sigma^2 + P_0 h^2 t}{\\sigma^2}\\right) \\left(\\frac{h}{\\sigma^2}\\frac{P_0 \\sigma^2}{\\sigma^2 + P_0 h^2 t}\\right) \\mathrm{d}Y_t = \\frac{h P_0}{\\sigma^2} \\mathrm{d}Y_t $$\nIntegrating from $t=0$ to $t=T$:\n$$ \\mu_T m_T - \\mu_0 m_0 = \\int_0^T \\frac{h P_0}{\\sigma^2} \\mathrm{d}Y_t = \\frac{h P_0}{\\sigma^2} (Y_T - Y_0) $$\nWith $Y_T = y_T$, $Y_0=0$, and $\\mu_0 = 1$, we get:\n$$ \\left(\\frac{\\sigma^2 + P_0 h^2 T}{\\sigma^2}\\right) m_T - m_0 = \\frac{h P_0 y_T}{\\sigma^2} $$\n$$ m_T = \\frac{\\sigma^2}{\\sigma^2 + P_0 h^2 T} \\left( m_0 + \\frac{h P_0 y_T}{\\sigma^2} \\right) = \\frac{m_0 \\sigma^2 + P_0 h y_T}{\\sigma^2 + P_0 h^2 T} $$\n\n### 2. Derivation via the Zakai Equation\n\nThe Zakai equation provides an evolution for the unnormalized conditional density $\\rho_t(x)$. A practical way to use this is through the Kallianpur-Striebel formula, which expresses the unnormalized posterior as the product of the prior density and a likelihood term derived from a change of measure. The unnormalized posterior density at time $T$ is given by $\\rho_T(x) = L_T(x) p_0(x)$, where $p_0(x)$ is the prior density of $X_0$ and $L_T(x)$ is the Radon-Nikodym derivative (likelihood).\nFor our system, this likelihood term is:\n$$ L_T(x) = \\exp\\left( \\int_0^T \\frac{H(x)}{\\sigma^2} \\mathrm{d}Y_s - \\frac{1}{2} \\int_0^T \\frac{H(x)^2}{\\sigma^2} \\mathrm{d}s \\right) $$\nGiven $H(x) = hx$ and that $x$ is constant over the interval $[0, T]$, this becomes:\n$$ L_T(x) = \\exp\\left( \\frac{hx}{\\sigma^2} \\int_0^T \\mathrm{d}Y_s - \\frac{1}{2}\\frac{h^2x^2}{\\sigma^2} \\int_0^T \\mathrm{d}s \\right) = \\exp\\left( \\frac{h x Y_T}{\\sigma^2} - \\frac{h^2 x^2 T}{2\\sigma^2} \\right) $$\nThe prior is $X_0 \\sim \\mathcal{N}(m_0, P_0)$, so $p_0(x) = (\\sqrt{2\\pi P_0})^{-1} \\exp\\left(-\\frac{(x-m_0)^2}{2P_0}\\right)$.\nThe unnormalized posterior density, given the observation $Y_T = y_T$, is:\n$$ \\rho_T(x) \\propto \\exp\\left( \\frac{h x y_T}{\\sigma^2} - \\frac{h^2 x^2 T}{2\\sigma^2} \\right) \\exp\\left(-\\frac{(x-m_0)^2}{2P_0}\\right) $$\nThe posterior mean $m_T$ is the mean of the normalized version of this density. Since the density is proportional to the exponential of a quadratic in $x$, the posterior is Gaussian, say $\\mathcal{N}(m_T, P_T)$. We can find its parameters by completing the square in the exponent. Let the exponent be $J(x)$:\n$$ J(x) = \\frac{h x y_T}{\\sigma^2} - \\frac{h^2 x^2 T}{2\\sigma^2} - \\frac{x^2 - 2xm_0 + m_0^2}{2P_0} $$\n$$ J(x) = -\\frac{1}{2} x^2 \\left(\\frac{h^2 T}{\\sigma^2} + \\frac{1}{P_0}\\right) + x \\left(\\frac{h y_T}{\\sigma^2} + \\frac{m_0}{P_0}\\right) - \\frac{m_0^2}{2 P_0} $$\nThe density of a Gaussian $\\mathcal{N}(m_T, P_T)$ is proportional to $\\exp\\left(-\\frac{(x-m_T)^2}{2P_T}\\right) = \\exp\\left(-\\frac{1}{2P_T}x^2 + \\frac{m_T}{P_T}x - \\frac{m_T^2}{2P_T}\\right)$.\nBy comparing the coefficients of the $x$ and $x^2$ terms, we find the posterior precision $1/P_T$ and the mean $m_T$:\n- From the $x^2$ term: $\\frac{1}{P_T} = \\frac{h^2 T}{\\sigma^2} + \\frac{1}{P_0}$. (This matches the Riccati result).\n- From the $x$ term: $\\frac{m_T}{P_T} = \\frac{h y_T}{\\sigma^2} + \\frac{m_0}{P_0}$.\n\nSolving for $m_T$:\n$$ m_T = P_T \\left( \\frac{h y_T}{\\sigma^2} + \\frac{m_0}{P_0} \\right) $$\nSubstituting $P_T = \\left(\\frac{1}{P_0} + \\frac{h^2 T}{\\sigma^2}\\right)^{-1} = \\frac{P_0 \\sigma^2}{\\sigma^2 + P_0 h^2 T}$:\n$$ m_T = \\left( \\frac{P_0 \\sigma^2}{\\sigma^2 + P_0 h^2 T} \\right) \\left( \\frac{h y_T P_0 + m_0 \\sigma^2}{P_0 \\sigma^2} \\right) $$\n$$ m_T = \\frac{m_0 \\sigma^2 + P_0 h y_T}{\\sigma^2 + P_0 h^2 T} $$\nThis result is identical to the one obtained via the Kushner-Stratonovich equation.\n\n### 3. Coincidence of the Derivations\n\nThe Kushner-Stratonovich and Zakai equations are two different, yet fundamentally equivalent, mathematical formalisms for describing the evolution of the conditional distribution in a filtering problem.\n- The **Zakai equation** describes the evolution of the *unnormalized* conditional density $\\rho_t(x)$. Its key advantage is that it is a linear (but stochastic) partial differential equation, which is often more amenable to analysis. This linearity is achieved through a change of probability measure (the Girsanov transformation).\n- The **Kushner-Stratonovich equation** describes the evolution of the *normalized* conditional expectation $\\pi_t(\\varphi)$ or density $\\pi_t(x)$. This is the quantity of direct physical interest. However, the KSE is nonlinear due to the normalization inherent in the definition of conditional expectation.\n\nThe normalized posterior is the ratio of the unnormalized posterior to its total mass: $\\pi_t(\\varphi) = \\int \\varphi(x) \\rho_t(x) \\mathrm{d}x / \\int \\rho_t(x) \\mathrm{d}x$. One can formally derive the KSE from the Zakai equation by applying a stochastic version of the chain rule for ratios (an extension of Itô's lemma) to this expression. This demonstrates that they are mathematically consistent descriptions of the same underlying probabilistic object. Since the filtering problem for this linear-Gaussian model is well-posed and has a unique solution for the posterior distribution, any valid method of derivation, including the KSE and Zakai approaches, must lead to the same result. For linear-Gaussian systems, both methods ultimately yield the Kalman-Bucy filter equations for the mean and variance.", "answer": "$$\\boxed{\\frac{m_0 \\sigma^2 + P_0 h y_T}{\\sigma^2 + P_0 h^2 T}}$$", "id": "3001892"}]}