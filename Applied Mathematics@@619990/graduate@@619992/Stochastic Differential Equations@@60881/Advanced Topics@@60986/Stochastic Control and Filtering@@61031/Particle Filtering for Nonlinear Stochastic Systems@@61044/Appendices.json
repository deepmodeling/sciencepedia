{"hands_on_practices": [{"introduction": "The power of a particle filter lies in its ability to represent a complex probability distribution with a finite set of weighted samples. However, this representation can quickly degrade in a phenomenon known as weight degeneracy, where one particle acquires nearly all the weight, rendering the sample set ineffective. This practice [@problem_id:2990102] guides you through a first-principles derivation to quantify this issue, asking you to calculate the variance of the importance weights as a function of the system's dimension. By completing this analysis for a canonical Gaussian model, you will gain a rigorous, quantitative understanding of the \"curse of dimensionality\" and its impact on filter performance.", "problem": "Consider a $d$-dimensional latent state $X_t$ evolving under an Itô stochastic differential equation (Itô SDE) with a globally asymptotically stable equilibrium and isotropic stationary covariance, specifically an Ornstein–Uhlenbeck (OU) model given by $dX_t = -\\lambda X_t\\,dt + \\sqrt{2\\lambda \\tau^2}\\,dW_t$, where $W_t$ is a $d$-dimensional standard Wiener process and $\\tau^2 > 0$. At stationarity, $X_t$ is distributed as $X \\sim \\mathcal{N}(0, \\tau^2 I_d)$. In a Sequential Monte Carlo (SMC) bootstrap particle filter (proposal equals prior), consider a single filtering step with observation model $Y = H X + V$, $H = I_d$, and $V \\sim \\mathcal{N}(0, \\sigma^2 I_d)$ independent of $X$, with the realized observation $y = 0$. By Bayes’ rule, the target posterior density is proportional to the prior predictive density times the likelihood contribution, so the target is a likelihood-tilted prior. The unnormalized incremental importance weight for a particle at state value $x$ is proportional to the likelihood factor $\\ell(x) = \\exp\\!\\big(-\\|x\\|^2/(2\\sigma^2)\\big)$.\n\nDefine the normalized importance weight under the proposal $p(x) = \\mathcal{N}(0, \\tau^2 I_d)$ by $w(x) = \\ell(x)\\big/\\mathbb{E}_p[\\ell(X)]$, so that $\\mathbb{E}_p[w(X)] = 1$. Using only first principles—Bayes’ rule, properties of Gaussian integrals, and the definition of importance sampling—derive the closed-form expression for the variance $\\mathrm{Var}_p[w(X)]$ as a function of the dimension $d$ and the parameters $\\tau^2$ and $\\sigma^2$. Express your final answer as a single analytic expression in terms of $d$, $\\tau^2$, and $\\sigma^2$.", "solution": "The problem statement has been validated and found to be scientifically grounded, well-posed, and objective. It presents a standard scenario in Bayesian filtering and asks for a derivable quantity using first principles. All necessary information is provided, and there are no contradictions or ambiguities.\n\nThe objective is to compute the variance of the normalized importance weights, $\\mathrm{Var}_p[w(X)]$. The proposal distribution, from which the particles $X$ are drawn, is the stationary distribution of the Ornstein-Uhlenbeck process, given as $p(x) = \\mathcal{N}(0, \\tau^2 I_d)$. The probability density function for a vector $x \\in \\mathbb{R}^d$ is:\n$$\np(x) = \\frac{1}{(2\\pi \\tau^2)^{d/2}} \\exp\\left(-\\frac{\\|x\\|^2}{2\\tau^2}\\right)\n$$\nThe unnormalized importance weight is given by the likelihood factor $\\ell(x)$, which arises from the observation model $Y = X + V$ with $V \\sim \\mathcal{N}(0, \\sigma^2 I_d)$ and a realized observation $y=0$. The likelihood of the state $x$ given the observation $y=0$ is:\n$$\np(y=0|x) \\propto \\exp\\left(-\\frac{\\|0-x\\|^2}{2\\sigma^2}\\right) = \\exp\\left(-\\frac{\\|x\\|^2}{2\\sigma^2}\\right)\n$$\nThus, we identify the unnormalized weight function as $\\ell(x) = \\exp\\left(-\\frac{\\|x\\|^2}{2\\sigma^2}\\right)$.\n\nThe problem defines the normalized importance weight as $w(x) = \\ell(x) / \\mathbb{E}_p[\\ell(X)]$. By this definition, the expectation of the normalized weights under the proposal distribution is unity:\n$$\n\\mathbb{E}_p[w(X)] = \\mathbb{E}_p\\left[\\frac{\\ell(X)}{\\mathbb{E}_p[\\ell(X)]}\\right] = \\frac{1}{\\mathbb{E}_p[\\ell(X)]}\\mathbb{E}_p[\\ell(X)] = 1\n$$\nThe variance of the normalized weights is given by the standard formula:\n$$\n\\mathrm{Var}_p[w(X)] = \\mathbb{E}_p[w(X)^2] - (\\mathbb{E}_p[w(X)])^2 = \\mathbb{E}_p[w(X)^2] - 1\n$$\nOur task reduces to calculating the second moment, $\\mathbb{E}_p[w(X)^2]$. We express this in terms of $\\ell(x)$:\n$$\n\\mathbb{E}_p[w(X)^2] = \\mathbb{E}_p\\left[\\left(\\frac{\\ell(X)}{\\mathbb{E}_p[\\ell(X)]}\\right)^2\\right] = \\frac{\\mathbb{E}_p[\\ell(X)^2]}{(\\mathbb{E}_p[\\ell(X)])^2}\n$$\nWe must compute two expectations: $\\mathbb{E}_p[\\ell(X)]$ and $\\mathbb{E}_p[\\ell(X)^2]$.\n\nFirst, we compute $\\mathbb{E}_p[\\ell(X)]$, which is the normalization constant for the weights:\n$$\n\\mathbb{E}_p[\\ell(X)] = \\int_{\\mathbb{R}^d} \\ell(x) p(x) \\,dx = \\int_{\\mathbb{R}^d} \\exp\\left(-\\frac{\\|x\\|^2}{2\\sigma^2}\\right) \\frac{1}{(2\\pi \\tau^2)^{d/2}} \\exp\\left(-\\frac{\\|x\\|^2}{2\\tau^2}\\right) \\,dx\n$$\nCombining the exponents, we get:\n$$\n\\mathbb{E}_p[\\ell(X)] = \\frac{1}{(2\\pi \\tau^2)^{d/2}} \\int_{\\mathbb{R}^d} \\exp\\left(-\\|x\\|^2 \\left(\\frac{1}{2\\sigma^2} + \\frac{1}{2\\tau^2}\\right)\\right) \\,dx\n$$\nLet's simplify the term in the parenthesis: $\\frac{1}{2\\sigma^2} + \\frac{1}{2\\tau^2} = \\frac{\\tau^2 + \\sigma^2}{2\\sigma^2\\tau^2}$. The integral is now:\n$$\n\\int_{\\mathbb{R}^d} \\exp\\left(-\\frac{1}{2} \\|x\\|^2 \\left(\\frac{\\tau^2 + \\sigma^2}{\\sigma^2\\tau^2}\\right)\\right) \\,dx\n$$\nThis is the integral of an unnormalized $d$-dimensional Gaussian density with zero mean and covariance matrix $(\\frac{\\sigma^2\\tau^2}{\\tau^2+\\sigma^2})I_d$. The value of such an integral $\\int_{\\mathbb{R}^d}\\exp(-\\frac{1}{2}x^T\\Sigma^{-1}x)dx$ is $(2\\pi)^{d/2}|\\Sigma|^{1/2}$. Here, $\\Sigma^{-1} = \\frac{\\tau^2+\\sigma^2}{\\sigma^2\\tau^2}I_d$, so $|\\Sigma|^{1/2} = \\left(\\left(\\frac{\\sigma^2\\tau^2}{\\tau^2+\\sigma^2}\\right)^d\\right)^{1/2} = \\left(\\frac{\\sigma^2\\tau^2}{\\tau^2+\\sigma^2}\\right)^{d/2}$.\nThus, the integral evaluates to $(2\\pi)^{d/2} \\left(\\frac{\\sigma^2\\tau^2}{\\tau^2+\\sigma^2}\\right)^{d/2}$.\nSubstituting this back into the expression for $\\mathbb{E}_p[\\ell(X)]$:\n$$\n\\mathbb{E}_p[\\ell(X)] = \\frac{1}{(2\\pi \\tau^2)^{d/2}} (2\\pi)^{d/2} \\left(\\frac{\\sigma^2\\tau^2}{\\tau^2+\\sigma^2}\\right)^{d/2} = \\frac{1}{(\\tau^2)^{d/2}} \\left(\\frac{\\sigma^2\\tau^2}{\\tau^2+\\sigma^2}\\right)^{d/2} = \\left(\\frac{\\sigma^2}{\\tau^2+\\sigma^2}\\right)^{d/2}\n$$\n\nNext, we compute $\\mathbb{E}_p[\\ell(X)^2]$. The squared likelihood factor is $\\ell(x)^2 = \\left(\\exp\\left(-\\frac{\\|x\\|^2}{2\\sigma^2}\\right)\\right)^2 = \\exp\\left(-\\frac{\\|x\\|^2}{\\sigma^2}\\right)$.\n$$\n\\mathbb{E}_p[\\ell(X)^2] = \\int_{\\mathbb{R}^d} \\ell(x)^2 p(x) \\,dx = \\int_{\\mathbb{R}^d} \\exp\\left(-\\frac{\\|x\\|^2}{\\sigma^2}\\right) \\frac{1}{(2\\pi \\tau^2)^{d/2}} \\exp\\left(-\\frac{\\|x\\|^2}{2\\tau^2}\\right) \\,dx\n$$\nCombining exponents again:\n$$\n\\mathbb{E}_p[\\ell(X)^2] = \\frac{1}{(2\\pi \\tau^2)^{d/2}} \\int_{\\mathbb{R}^d} \\exp\\left(-\\|x\\|^2 \\left(\\frac{1}{\\sigma^2} + \\frac{1}{2\\tau^2}\\right)\\right) \\,dx\n$$\nThe term in the parenthesis is $\\frac{1}{\\sigma^2} + \\frac{1}{2\\tau^2} = \\frac{2\\tau^2 + \\sigma^2}{2\\sigma^2\\tau^2}$. The integral has the same Gaussian form. To evaluate it, we match its exponent to the form $-\\frac{1}{2}x^T\\Sigma_2^{-1}x$. This implies $\\Sigma_2^{-1} = 2\\left(\\frac{2\\tau^2 + \\sigma^2}{2\\sigma^2\\tau^2}\\right)I_d = \\left(\\frac{2\\tau^2 + \\sigma^2}{\\sigma^2\\tau^2}\\right)I_d$.\nThe integral evaluates to $(2\\pi)^{d/2} |\\Sigma_2|^{1/2} = (2\\pi)^{d/2} \\left(\\frac{\\sigma^2\\tau^2}{2\\tau^2+\\sigma^2}\\right)^{d/2}$.\nSubstituting this back:\n$$\n\\mathbb{E}_p[\\ell(X)^2] = \\frac{1}{(2\\pi \\tau^2)^{d/2}} (2\\pi)^{d/2} \\left(\\frac{\\sigma^2\\tau^2}{2\\tau^2+\\sigma^2}\\right)^{d/2} = \\left(\\frac{\\sigma^2}{2\\tau^2+\\sigma^2}\\right)^{d/2}\n$$\nNow we can compute $\\mathbb{E}_p[w(X)^2]$:\n$$\n\\mathbb{E}_p[w(X)^2] = \\frac{\\mathbb{E}_p[\\ell(X)^2]}{(\\mathbb{E}_p[\\ell(X)])^2} = \\frac{\\left(\\frac{\\sigma^2}{2\\tau^2+\\sigma^2}\\right)^{d/2}}{\\left(\\left(\\frac{\\sigma^2}{\\tau^2+\\sigma^2}\\right)^{d/2}\\right)^2} = \\frac{\\left(\\frac{\\sigma^2}{2\\tau^2+\\sigma^2}\\right)^{d/2}}{\\left(\\frac{\\sigma^2}{\\tau^2+\\sigma^2}\\right)^d}\n$$\n$$\n= \\left(\\frac{\\sigma^2}{2\\tau^2+\\sigma^2}\\right)^{d/2} \\left(\\frac{\\tau^2+\\sigma^2}{\\sigma^2}\\right)^d = \\frac{(\\sigma^2)^{d/2}}{(2\\tau^2+\\sigma^2)^{d/2}} \\frac{(\\tau^2+\\sigma^2)^d}{(\\sigma^2)^d} = \\frac{(\\tau^2+\\sigma^2)^d}{(2\\tau^2+\\sigma^2)^{d/2}(\\sigma^2)^{d/2}}\n$$\nThis can be written more compactly by collecting terms under the power of $d/2$:\n$$\n\\mathbb{E}_p[w(X)^2] = \\frac{\\left((\\tau^2+\\sigma^2)^2\\right)^{d/2}}{\\left(\\sigma^2(2\\tau^2+\\sigma^2)\\right)^{d/2}} = \\left(\\frac{(\\tau^2+\\sigma^2)^2}{\\sigma^2(2\\tau^2+\\sigma^2)}\\right)^{d/2}\n$$\nFinally, the variance is $\\mathbb{E}_p[w(X)^2] - 1$:\n$$\n\\mathrm{Var}_p[w(X)] = \\left(\\frac{(\\tau^2+\\sigma^2)^2}{\\sigma^2(2\\tau^2+\\sigma^2)}\\right)^{d/2} - 1\n$$\nThis expression provides the variance of the normalized importance weights as a function of the model parameters $d$, $\\tau^2$, and $\\sigma^2$. The base of the exponent can be verified to be greater than $1$ for $\\tau^2>0, \\sigma^2>0$:\n$$\n\\frac{(\\tau^2+\\sigma^2)^2}{\\sigma^2(2\\tau^2+\\sigma^2)} = \\frac{\\tau^4 + 2\\tau^2\\sigma^2 + \\sigma^4}{2\\tau^2\\sigma^2 + \\sigma^4} = 1 + \\frac{\\tau^4}{2\\tau^2\\sigma^2 + \\sigma^4} > 1\n$$\nThis ensures the variance is positive, as required.", "answer": "$$\n\\boxed{\\left(\\frac{(\\tau^2 + \\sigma^2)^2}{\\sigma^2(2\\tau^2 + \\sigma^2)}\\right)^{d/2} - 1}\n$$", "id": "2990102"}, {"introduction": "While statistical properties are paramount, the practical feasibility of a particle filter depends critically on its computational cost, especially for systems with many state variables. This exercise [@problem_id:2990065] challenges you to think like a systems designer by analyzing the computational complexity of a standard particle filter's main components: propagation, weighting, and resampling. By deriving the scaling behavior with respect to the number of particles $N$ and the state dimension $d$, you will learn to identify computational bottlenecks and make informed design choices for efficient implementation.", "problem": "Consider a nonlinear continuous-time stochastic dynamical system in state dimension $d$ governed by a Stochastic Differential Equation (SDE)\n$$\n\\mathrm{d}X_{t} \\;=\\; f(X_{t})\\,\\mathrm{d}t \\;+\\; G(X_{t})\\,\\mathrm{d}W_{t},\n$$\nwhere $X_{t}\\in\\mathbb{R}^{d}$ is the state, $f:\\mathbb{R}^{d}\\to\\mathbb{R}^{d}$ is a drift field, $G:\\mathbb{R}^{d}\\to\\mathbb{R}^{d\\times d}$ is a diffusion field that is dense for typical states, and $W_{t}$ is a standard $d$-dimensional Wiener process. Discrete-time observations at times $\\{t_{k}\\}$ are given by\n$$\nY_{k} \\;=\\; h(X_{t_{k}}) \\;+\\; V_{k},\n$$\nwhere $h:\\mathbb{R}^{d}\\to\\mathbb{R}^{d}$ is a nonlinear measurement function and $V_{k}\\sim \\mathcal{N}(0,S)$ is Gaussian measurement noise with a time-invariant, dense covariance $S\\in\\mathbb{R}^{d\\times d}$. Assume $S$ is nonsingular and its inverse or a Cholesky factorization is precomputed once and reused.\n\nA Sequential Monte Carlo (SMC) particle filter with $N$ particles is used to approximate the filtering distribution. At each time step $t_{k}\\to t_{k+1}$, the filter executes three phases:\n\n(1) Propagation: Each particle $X^{(i)}_{t_{k}}$ is advanced to $X^{(i)}_{t_{k+1}}$ using the Euler–Maruyama method with step size $\\Delta t$, namely\n$$\nX^{(i)}_{t_{k+1}} \\;=\\; X^{(i)}_{t_{k}} \\;+\\; f\\!\\big(X^{(i)}_{t_{k}}\\big)\\,\\Delta t \\;+\\; G\\!\\big(X^{(i)}_{t_{k}}\\big)\\,\\Delta W^{(i)}_{k},\n$$\nwhere $\\Delta W^{(i)}_{k}\\sim \\mathcal{N}(0,\\Delta t\\,I_{d})$ are independent for each particle. Evaluating $f$ is assumed to require $\\Theta(d)$ arithmetic operations per particle, $G(X)$ is dense so forming $G(X)\\in\\mathbb{R}^{d\\times d}$ and multiplying it by $\\Delta W\\in\\mathbb{R}^{d}$ requires $\\Theta(d^{2})$ arithmetic operations per particle.\n\n(2) Weight computation: For each particle state $x^{(i)}$, compute the residual $r^{(i)}=y_{k+1}-h(x^{(i)})$, assumed to require $\\Theta(d)$ operations for $h$, and evaluate the Gaussian likelihood (up to normalization) via the quadratic form $Q^{(i)}=(r^{(i)})^{\\top}S^{-1}r^{(i)}$ or, equivalently, using a precomputed Cholesky factorization $S=LL^{\\top}$ by solving $Lz=r^{(i)}$ followed by $Q^{(i)}=\\|z\\|^{2}$. For dense $L\\in\\mathbb{R}^{d\\times d}$, the triangular solve costs $\\Theta(d^{2})$ operations per particle, and computing $\\|z\\|^{2}$ costs $\\Theta(d)$.\n\n(3) Resampling: Perform systematic resampling to draw $N$ new particles from the weighted set. Computing normalized weights, their cumulative sum, and generating the resampled indices is assumed to cost $\\Theta(N)$ per time step.\n\nStarting from the above first-principles descriptions of the SDE discretization, Gaussian likelihood evaluation, and systematic resampling, derive the leading-order asymptotic computational complexity per time step of the entire particle filter as a single expression in $N$ and $d$, ignoring all constant factors and lower-order terms. Your derivation must make clear which phase(s) constitute the computational bottleneck(s) under the stated assumptions. Provide your final answer as one closed-form asymptotic expression in big-$O$ notation. No numerical approximation or rounding is required.", "solution": "The problem requires the derivation of the leading-order asymptotic computational complexity per time step for a Sequential Monte Carlo (SMC) particle filter with $N$ particles applied to a system of state dimension $d$. The total complexity is the sum of the complexities of its three constituent phases: Propagation, Weight Computation, and Resampling. We will analyze each phase in turn.\n\nLet $N$ be the number of particles and $d$ be the dimension of the state space $\\mathbb{R}^{d}$.\n\n1.  **Propagation Phase:**\n    This phase advances each of the $N$ particles from time $t_{k}$ to $t_{k+1}$. The cost of this phase is the cost per particle multiplied by the number of particles, $N$. For a single particle $X^{(i)}_{t_{k}}$, the update rule is:\n    $$\n    X^{(i)}_{t_{k+1}} \\;=\\; X^{(i)}_{t_{k}} \\;+\\; f\\!\\big(X^{(i)}_{t_{k}}\\big)\\,\\Delta t \\;+\\; G\\!\\big(X^{(i)}_{t_{k}}\\big)\\,\\Delta W^{(i)}_{k}\n    $$\n    The computational cost for one particle is the sum of the costs of evaluating the terms on the right-hand side and performing the vector additions.\n    -   Evaluation of the drift term $f(X^{(i)}_{t_{k}})$ is stated to require $\\Theta(d)$ operations. The subsequent scalar-vector multiplication by $\\Delta t$ also takes $\\Theta(d)$ operations.\n    -   Evaluation of the diffusion term involves forming the dense $d \\times d$ matrix $G(X^{(i)}_{t_{k}})$ and multiplying it by the $d \\times 1$ vector $\\Delta W^{(i)}_{k}$. A dense matrix-vector product requires $\\Theta(d^{2})$ arithmetic operations. The generation of the random vector $\\Delta W^{(i)}_{k}$ from $\\mathcal{N}(0, \\Delta t\\,I_{d})$ involves drawing $d$ independent Gaussian samples, which costs $\\Theta(d)$.\n    -   The two vector additions cost $\\Theta(d)$ each.\n    The total cost per particle for the propagation step is the sum of these individual costs: $\\Theta(d) + \\Theta(d) + (\\Theta(d^{2}) + \\Theta(d)) + \\Theta(d) + \\Theta(d)$. The dominant term in this sum is $\\Theta(d^{2})$, arising from the dense matrix-vector multiplication $G \\Delta W$.\n    Therefore, the complexity for propagating one particle is $\\Theta(d^{2})$.\n    For all $N$ particles, the total complexity of the propagation phase is:\n    $$\n    C_{\\text{prop}} = N \\times \\Theta(d^{2}) = \\Theta(Nd^{2})\n    $$\n\n2.  **Weight Computation Phase:**\n    This phase computes an importance weight for each of the $N$ propagated particles. The cost is again the cost per particle multiplied by $N$. For a single particle $x^{(i)} = X^{(i)}_{t_{k+1}}$ and the observation $y_{k+1}$, the unnormalized weight is proportional to the likelihood of the observation given the particle's state. This involves computing the quadratic form $Q^{(i)}=(r^{(i)})^{\\top}S^{-1}r^{(i)}$, where $r^{(i)}=y_{k+1}-h(x^{(i)})$.\n    -   The evaluation of the measurement function $h(x^{(i)})$ is stated to cost $\\Theta(d)$.\n    -   The computation of the residual vector $r^{(i)}$ via vector subtraction costs an additional $\\Theta(d)$.\n    -   The calculation of the quadratic form $Q^{(i)}$ is performed using a precomputed Cholesky factorization $S = LL^{\\top}$. This involves two steps: first, solving the lower triangular system $Lz = r^{(i)}$ for $z$ using forward substitution, and second, computing the squared norm $Q^{(i)} = z^{\\top}z = \\|z\\|^{2}$. For a dense $d \\times d$ matrix $L$, forward substitution costs $\\Theta(d^{2})$ operations. The subsequent computation of the squared norm of the $d$-dimensional vector $z$ costs $\\Theta(d)$. The evaluation of the likelihood, e.g., $\\exp(-Q^{(i)}/2)$, is a scalar operation of cost $\\Theta(1)$.\n    The total cost per particle for weight computation is the sum of these costs: $(\\Theta(d) + \\Theta(d)) + (\\Theta(d^{2}) + \\Theta(d)) + \\Theta(1)$. The dominant term here is $\\Theta(d^{2})$, which comes from solving the triangular linear system.\n    Therefore, the complexity for weighting one particle is $\\Theta(d^{2})$.\n    For all $N$ particles, the total complexity of the weight computation phase is:\n    $$\n    C_{\\text{weight}} = N \\times \\Theta(d^{2}) = \\Theta(Nd^{2})\n    $$\n\n3.  **Resampling Phase:**\n    This phase draws a new set of $N$ particles from the current weighted set to combat degeneracy. The problem states that systematic resampling, which involves computing normalized weights, their cumulative sum, and drawing the new indices, has a total computational cost of $\\Theta(N)$. This cost is notably independent of the state dimension $d$.\n    $$\n    C_{\\text{resample}} = \\Theta(N)\n    $$\n\n**Total Complexity and Bottlenecks**\nThe total computational complexity per time step, $C_{\\text{total}}$, is the sum of the complexities of the three phases:\n$$\nC_{\\text{total}} = C_{\\text{prop}} + C_{\\text{weight}} + C_{\\text{resample}} = \\Theta(Nd^{2}) + \\Theta(Nd^{2}) + \\Theta(N)\n$$\nTo find the leading-order asymptotic complexity, we identify the term that grows fastest as $N$ and $d$ become large. Since $d \\ge 1$, the term $Nd^{2}$ dominates $N$.\n$$\nC_{\\text{total}} = \\Theta(Nd^{2})\n$$\nThe computational bottlenecks are the phases whose complexities match this leading-order term. In this analysis, both the **Propagation** phase and the **Weight Computation** phase have a complexity of $\\Theta(Nd^{2})$. Their cost scales quadratically with the state dimension $d$, stemming from linear algebra operations involving the dense matrices $G$ and $S$ (or its Cholesky factor $L$). The Resampling phase, with complexity $\\Theta(N)$, is not a bottleneck for $d>1$. Therefore, both propagation and weighting are the dominant computational burdens for this particle filter configuration.\n\nThe final answer, expressed in big-O notation as requested, is the leading-order term derived from this analysis.", "answer": "$$\n\\boxed{O(Nd^{2})}\n$$", "id": "2990065"}, {"introduction": "Translating the particle filter algorithm from mathematical theory to robust software reveals critical numerical challenges that can easily derail an implementation. A frequent and fatal issue is floating-point underflow, where importance weights, often being products of many small probabilities, collapse to zero and invalidate the results. This practice [@problem_id:2990126] provides a hands-on guide to implementing the essential \"log-sum-exp\" trick, a standard and elegant technique for maintaining numerical stability. By working in the logarithmic domain and handling relevant edge cases, you will develop a core skill for building reliable and effective particle filtering code.", "problem": "Consider a nonlinear hidden Markov model in discrete time obtained by Euler–Maruyama discretization of a stochastic differential equation. Let the hidden state be $x_k \\in \\mathbb{R}^n$ evolving according to\n$$\nx_k = x_{k-1} + f(x_{k-1})\\,\\Delta t + G(x_{k-1})\\,\\sqrt{\\Delta t}\\,\\eta_k,\n$$\nwhere $f(\\cdot)$ is a drift term, $G(\\cdot)$ is a diffusion term, $\\Delta t$ is the time step, and $\\eta_k$ is a standard Gaussian random vector. The observation is given by\n$$\ny_k = h(x_k) + v_k,\n$$\nwith $h(\\cdot)$ a nonlinear sensor model and $v_k$ a Gaussian noise vector.\n\nIn a particle filter for this system, suppose we have $N$ particles $\\{x_k^{(i)}\\}_{i=1}^N$ and their previous normalized weights $\\{w_{k-1}^{(i)}\\}_{i=1}^N$, which satisfy $\\sum_{i=1}^N w_{k-1}^{(i)} = 1$. The measurement update at time $k$ is based on Bayes’ rule and the principle of importance sampling: the unnormalized new weights are proportional to the product of the previous weights and the likelihood,\n$$\n\\tilde{w}_k^{(i)} \\propto w_{k-1}^{(i)}\\,p(y_k \\mid x_k^{(i)}),\n$$\nand the normalized weights are then\n$$\nw_k^{(i)} = \\frac{\\tilde{w}_k^{(i)}}{\\sum_{j=1}^N \\tilde{w}_k^{(j)}}.\n$$\n\nWhen the likelihood $p(y_k \\mid x_k^{(i)})$ is very small (for instance due to strong mismatch or small measurement noise covariance), direct computation of $\\tilde{w}_k^{(i)}$ in floating point can underflow to zero. A robust computational strategy is to operate in the logarithmic domain. Define the log-weights\n$$\n\\ell_{k-1}^{(i)} = \\log w_{k-1}^{(i)}, \\quad \\lambda_k^{(i)} = \\log p(y_k \\mid x_k^{(i)}),\n$$\nand the updated log-weights\n$$\n\\ell_k^{(i)} = \\ell_{k-1}^{(i)} + \\lambda_k^{(i)}.\n$$\nThe normalized $w_k^{(i)}$ can then be recovered stably using a numerically safe approach that avoids catastrophic underflow even when the $\\lambda_k^{(i)}$ are extremely negative.\n\nYour task is to derive, from the above foundations and without assuming any shortcut formulas, a numerically stable normalization procedure for $\\{w_k^{(i)}\\}_{i=1}^N$ based on log-weights that remains robust when some $\\lambda_k^{(i)}$ are very small or $-\\infty$ (representing zero likelihood). Implement this procedure in a complete program that, for each test case provided below, takes arrays of previous weights and log-likelihood increments, computes updated log-weights, and normalizes them in a numerically stable manner. If all updated log-weights are $-\\infty$ (i.e., all particles have zero likelihood under the current observation), you must return a fallback of uniform weights $w_k^{(i)} = 1/N$ to avoid degeneracy.\n\nDesign your program to process the following test suite. For each case, the input consists of the previous normalized weights $\\{w_{k-1}^{(i)}\\}$ and the log-likelihood increments $\\{\\lambda_k^{(i)}\\}$, and the output is the list of normalized weights $\\{w_k^{(i)}\\}$ rounded to twelve decimal places:\n\n- Test Case 1 (happy path with common extreme scaling): $N=5$. Previous weights $[0.2,\\,0.3,\\,0.1,\\,0.25,\\,0.15]$. Log-likelihood increments $[-1000.0,\\,-1000.0,\\,-1000.0,\\,-1000.0,\\,-1000.0]$.\n- Test Case 2 (mixed extremes and impossible particle): $N=5$. Previous weights $[0.2,\\,0.2,\\,0.2,\\,0.2,\\,0.2]$. Log-likelihood increments $[-1000.0,\\,-2000.0,\\,-\\infty,\\,-1200.0,\\,-1100.0]$.\n- Test Case 3 (single dominant support amid severe mismatch): $N=5$. Previous weights $[0.05,\\,0.05,\\,0.8,\\,0.05,\\,0.05]$. Log-likelihood increments $[-1000.0,\\,-1000.0,\\,0.0,\\,-1000.0,\\,-1000.0]$.\n- Test Case 4 (all impossible, fallback to uniform): $N=4$. Previous weights $[0.1,\\,0.2,\\,0.3,\\,0.4]$. Log-likelihood increments $[-\\infty,\\,-\\infty,\\,-\\infty,\\,-\\infty]$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each element of this top-level list must be the list of normalized weights for the corresponding test case, rounded to twelve decimal places. For example, your output line must look like\n$[ [\\cdots], [\\cdots], [\\cdots], [\\cdots] ]$\nwith no additional text. Angles and physical units do not apply to this problem, and the only outputs are lists of floats rounded to twelve decimal places. Ensure that your implementation is numerically stable and robust under the provided edge cases.", "solution": "The problem requires the derivation and implementation of a numerically stable procedure to normalize particle weights in a particle filter, specifically when operating in the logarithmic domain to mitigate floating-point underflow.\n\nThe starting point is the standard importance sampling update rule for particle weights. Given a set of $N$ particles with previous normalized weights $\\{w_{k-1}^{(i)}\\}_{i=1}^N$ such that $\\sum_{i=1}^N w_{k-1}^{(i)} = 1$, the unnormalized updated weights $\\tilde{w}_k^{(i)}$ at time step $k$ are given by the product of the prior weight and the likelihood of the new observation $y_k$ given the particle's state $x_k^{(i)}$:\n$$\n\\tilde{w}_k^{(i)} = w_{k-1}^{(i)} \\, p(y_k \\mid x_k^{(i)})\n$$\nNormalization then yields the new weights $w_k^{(i)}$:\n$$\nw_k^{(i)} = \\frac{\\tilde{w}_k^{(i)}}{\\sum_{j=1}^N \\tilde{w}_k^{(j)}}\n$$\nDirect computation of this expression is prone to numerical underflow. If the likelihood values $p(y_k \\mid x_k^{(i)})$ are extremely small, the products $\\tilde{w}_k^{(i)}$ can evaluate to zero in finite-precision arithmetic for all $i$, leading to a division by zero.\n\nTo circumvent this, we operate in the logarithmic domain. Let $\\ell_{k-1}^{(i)} = \\log w_{k-1}^{(i)}$ be the previous log-weights and $\\lambda_k^{(i)} = \\log p(y_k \\mid x_k^{(i)})$ be the log-likelihoods. The unnormalized updated log-weight, $\\ell_k^{(i)}$, is:\n$$\n\\ell_k^{(i)} = \\log(\\tilde{w}_k^{(i)}) = \\log(w_{k-1}^{(i)}) + \\log(p(y_k \\mid x_k^{(i)})) = \\ell_{k-1}^{(i)} + \\lambda_k^{(i)}\n$$\nFrom this, the normalized weights $w_k^{(i)}$ can be expressed as:\n$$\nw_k^{(i)} = \\frac{\\exp(\\ell_k^{(i)})}{\\sum_{j=1}^N \\exp(\\ell_k^{(j)})}\n$$\nThis form still suffers from the same underflow issue, as $\\ell_k^{(i)}$ can be large negative numbers (e.g., $-1000$), causing $\\exp(\\ell_k^{(i)})$ to evaluate to zero.\n\nThe key to a stable computation is the \"log-sum-exp\" trick. Let $L_{max}$ be the maximum value among all the unnormalized updated log-weights:\n$$\nL_{max} = \\max_{j \\in \\{1, \\dots, N\\}} \\{\\ell_k^{(j)}\\}\n$$\nWe can multiply the numerator and the denominator of the expression for $w_k^{(i)}$ by the same non-zero factor, $\\exp(-L_{max})$, without changing its value:\n$$\nw_k^{(i)} = \\frac{\\exp(\\ell_k^{(i)}) \\cdot \\exp(-L_{max})}{\\left(\\sum_{j=1}^N \\exp(\\ell_k^{(j)})\\right) \\cdot \\exp(-L_{max})}\n$$\nUsing the property $\\exp(a)\\exp(b) = \\exp(a+b)$ and distributing the factor into the sum, we obtain:\n$$\nw_k^{(i)} = \\frac{\\exp(\\ell_k^{(i)} - L_{max})}{\\sum_{j=1}^N \\exp(\\ell_k^{(j)} - L_{max})}\n$$\nThis expression is numerically stable. Let's analyze its components:\n1.  The exponent $(\\ell_k^{(j)} - L_{max})$ for any term in the sum is always less than or equal to $0$, as $L_{max}$ is the maximum value in the set $\\{\\ell_k^{(j)}\\}$. This prevents the argument of $\\exp(\\cdot)$ from being a large positive number, thus avoiding overflow.\n2.  There exists at least one particle, say with index $m$, for which $\\ell_k^{(m)} = L_{max}$. For this particle, the term in the sum is $\\exp(\\ell_k^{(m)} - L_{max}) = \\exp(0) = 1$.\n3.  Therefore, the denominator, $S = \\sum_{j=1}^N \\exp(\\ell_k^{(j)} - L_{max})$, is guaranteed to be greater than or equal to $1$. This prevents the sum from underflowing to zero, which was the primary numerical instability.\n4.  The terms $\\exp(\\ell_k^{(i)} - L_{max})$ may still underflow to zero if $\\ell_k^{(i)}$ is significantly smaller than $L_{max}$. This, however, is a correct and desirable outcome, as it reflects that particle $i$ has a negligible weight compared to the most likely particles.\n\nA special case arises when all particles have zero likelihood, which corresponds to all log-likelihoods $\\lambda_k^{(i)}$ being $-\\infty$ or a combination leading to all $\\ell_k^{(i)}$ being $-\\infty$. In this scenario, $L_{max} = -\\infty$. A direct application of the formula is undefined. The problem specifies a robust fallback mechanism: if all updated log-weights are $-\\infty$, the filter is reset by assigning uniform weights, $w_k^{(i)} = 1/N$ for all $i$. This prevents filter degeneracy.\n\nThe complete, numerically stable algorithm is as follows:\n1.  For each particle $i \\in \\{1, \\dots, N\\}$, calculate the unnormalized log-weight: $\\ell_k^{(i)} = \\log(w_{k-1}^{(i)}) + \\lambda_k^{(i)}$. Note that if $w_{k-1}^{(i)}=0$, then $\\ell_{k-1}^{(i)}=-\\infty$, and consequently $\\ell_k^{(i)}=-\\infty$.\n2.  Find the maximum log-weight: $L_{max} = \\max_{i} \\{\\ell_k^{(i)}\\}$.\n3.  Check for the degenerate case: If $L_{max} = -\\infty$, set $w_k^{(i)} = 1/N$ for all $i=1, \\dots, N$ and terminate.\n4.  If $L_{max}$ is finite, compute the shifted log-weights: $\\ell'^{(i)}_k = \\ell_k^{(i)} - L_{max}$.\n5.  Compute the stable unnormalized weights: $w'^{(i)}_k = \\exp(\\ell'^{(i)}_k)$.\n6.  Calculate their sum: $S = \\sum_{j=1}^N w'^{(j)}_k$.\n7.  Compute the final normalized weights: $w_k^{(i)} = \\frac{w'^{(i)}_k}{S}$.\nThis procedure ensures robust and accurate weight normalization even under extreme values of log-likelihoods.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the particle filter weight normalization problem for a suite of test cases.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test Case 1: happy path with common extreme scaling\n        (np.array([0.2, 0.3, 0.1, 0.25, 0.15]), np.array([-1000.0, -1000.0, -1000.0, -1000.0, -1000.0])),\n        # Test Case 2: mixed extremes and impossible particle\n        (np.array([0.2, 0.2, 0.2, 0.2, 0.2]), np.array([-1000.0, -2000.0, -np.inf, -1200.0, -1100.0])),\n        # Test Case 3: single dominant support amid severe mismatch\n        (np.array([0.05, 0.05, 0.8, 0.05, 0.05]), np.array([-1000.0, -1000.0, 0.0, -1000.0, -1000.0])),\n        # Test Case 4: all impossible, fallback to uniform\n        (np.array([0.1, 0.2, 0.3, 0.4]), np.array([-np.inf, -np.inf, -np.inf, -np.inf]))\n    ]\n    \n    results = []\n\n    for prev_weights, log_likelihoods in test_cases:\n        # Step 1: Compute previous log-weights and updated unnormalized log-weights.\n        # np.log handles arrays and np.log(0) correctly returns -inf.\n        log_prev_weights = np.log(prev_weights)\n        \n        # This is l_k^{(i)} in the problem description.\n        updated_log_weights = log_prev_weights + log_likelihoods\n        \n        # Step 2: Find the maximum log-weight.\n        max_log_weight = np.max(updated_log_weights)\n        \n        # Step 3: Handle the degenerate case where all particles have zero likelihood.\n        if max_log_weight == -np.inf:\n            num_particles = len(prev_weights)\n            normalized_weights = np.full(num_particles, 1.0 / num_particles)\n        else:\n            # Step 4: Compute shifted log-weights.\n            # This is l_k^{(i)} - L_max.\n            shifted_log_weights = updated_log_weights - max_log_weight\n            \n            # Step 5  6: Exponentiate and compute the sum.\n            # This is exp(l_k^{(i)} - L_max).\n            exp_weights = np.exp(shifted_log_weights)\n            sum_exp_weights = np.sum(exp_weights)\n            \n            # Step 7: Compute the final normalized weights.\n            normalized_weights = exp_weights / sum_exp_weights\n            \n        # Round the final weights to 12 decimal places as required.\n        rounded_weights = np.round(normalized_weights, 12).tolist()\n        results.append(rounded_weights)\n\n    # Format the final output string as specified.\n    result_strings = [str(r).replace(\" \", \"\") for r in results]\n    print(f\"[{','.join(result_strings)}]\")\n\nsolve()\n```", "id": "2990126"}]}