{"hands_on_practices": [{"introduction": "The Linear-Quadratic-Gaussian (LQG) problem is a cornerstone of modern control theory, providing a framework where the powerful machinery of the Hamilton-Jacobi-Bellman (HJB) equation yields an elegant, closed-form solution. This exercise guides you through the canonical example of controlling a stochastic Ornstein–Uhlenbeck process with a quadratic cost. By postulating a quadratic form for the value function, you will see how the HJB partial differential equation reduces to a purely algebraic problem, leading to the famous algebraic Riccati equation and an optimal linear feedback law [@problem_id:3001633]. This practice is fundamental for building intuition about the structure of optimal stochastic control and the stabilizing effect of feedback.", "problem": "Consider the infinite-horizon discounted stochastic control problem for the one-dimensional controlled Ornstein–Uhlenbeck (OU) diffusion. The state process $\\{X_{t}\\}_{t \\ge 0}$ evolves according to the controlled stochastic differential equation\n$$\n\\mathrm{d}X_{t} = \\big(-\\theta X_{t} + \\beta u_{t}\\big)\\,\\mathrm{d}t + \\sigma\\,\\mathrm{d}W_{t}, \\quad X_{0}=x,\n$$\nwhere $W_{t}$ is a standard Brownian motion, and $\\theta>0$, $\\beta\\neq 0$, $\\sigma>0$ are given constants. The control process $\\{u_{t}\\}_{t \\ge 0}$ is progressively measurable with respect to the filtration of $W_{t}$ and takes values in $\\mathbb{R}$. The objective is to minimize the discounted cost functional\n$$\nJ^{u}(x) \\equiv \\mathbb{E}\\!\\left[\\int_{0}^{\\infty} \\exp(-\\rho t)\\big(q X_{t}^{2} + r u_{t}^{2}\\big)\\,\\mathrm{d}t\\right],\n$$\nover all admissible controls, where $\\rho>0$, $q>0$, and $r>0$ are given constants. Let $V(x)\\equiv \\inf_{u} J^{u}(x)$ denote the value function.\n\nStarting from the dynamic programming principle and the definitions of the diffusion generator and the discounted expected cost, derive the Hamilton–Jacobi–Bellman (HJB) equation for $V(x)$, solve it explicitly in closed form, and obtain the optimal stationary Markov feedback $u^{\\ast}(x)$. Explain how the optimal feedback modifies the linear drift and why this yields stabilization of the closed-loop OU dynamics. Your final answer must be given as a single analytic expression containing both the closed-form value function $V(x)$ and the optimal feedback $u^{\\ast}(x)$, written as a $1\\times 2$ row matrix with the first entry equal to $V(x)$ and the second entry equal to $u^{\\ast}(x)$. No numerical evaluation is required, and no rounding is to be performed.", "solution": "We begin with the controlled diffusion\n$$\n\\mathrm{d}X_{t} = b(X_{t},u_{t})\\,\\mathrm{d}t + \\sigma\\,\\mathrm{d}W_{t}, \\quad b(x,u) \\equiv -\\theta x + \\beta u,\n$$\nand the infinite-horizon discounted cost\n$$\nJ^{u}(x) = \\mathbb{E}\\!\\left[\\int_{0}^{\\infty}\\exp(-\\rho t)\\,\\ell(X_{t},u_{t})\\,\\mathrm{d}t\\right], \\quad \\ell(x,u) \\equiv q x^{2} + r u^{2},\n$$\nwith $\\rho>0$, $\\theta>0$, $q>0$, $r>0$, $\\sigma>0$, and $\\beta\\neq 0$. The dynamic programming principle asserts that if $V$ is the value function, then under sufficient regularity $V$ satisfies the Hamilton–Jacobi–Bellman (HJB) equation. For a twice continuously differentiable $V$ and a discount factor $\\rho>0$, the HJB for this diffusion control problem is\n$$\n\\rho V(x) = \\inf_{u\\in\\mathbb{R}}\\Big\\{\\ell(x,u) + \\mathcal{L}^{u}V(x)\\Big\\},\n$$\nwhere $\\mathcal{L}^{u}$ is the controlled generator acting on smooth test functions $f$ by\n$$\n\\mathcal{L}^{u} f(x) \\equiv b(x,u) f'(x) + \\tfrac{1}{2}\\sigma^{2} f''(x) = \\big(-\\theta x + \\beta u\\big) f'(x) + \\tfrac{1}{2}\\sigma^{2} f''(x).\n$$\nThus the HJB takes the explicit form\n$$\n\\rho V(x) = \\inf_{u\\in\\mathbb{R}}\\left\\{q x^{2} + r u^{2} + \\big(-\\theta x + \\beta u\\big)V'(x) + \\tfrac{1}{2}\\sigma^{2} V''(x)\\right\\}.\n$$\n\nWe now solve this equation. The structure is linear-quadratic in $(x,u)$, and it is natural to look for a solution of the form\n$$\nV(x) = P x^{2} + C,\n$$\nwith constants $P$ and $C$ to be determined. We compute the derivatives $V'(x) = 2 P x$ and $V''(x) = 2 P$. Substituting into the HJB gives\n$$\n\\rho \\big(P x^{2} + C\\big) = \\inf_{u\\in\\mathbb{R}}\\left\\{q x^{2} + r u^{2} + \\big(-\\theta x + \\beta u\\big)\\,2 P x + \\tfrac{1}{2}\\sigma^{2}\\cdot 2 P\\right\\}.\n$$\nThe minimization over $u$ involves the quadratic function\n$$\n\\Phi(u;x) \\equiv r u^{2} + 2 P \\beta x\\, u + \\big(q x^{2} - 2\\theta P x^{2} + \\sigma^{2} P\\big).\n$$\nFor each fixed $x$, the minimizer $u^{\\ast}(x)$ satisfies the first-order condition\n$$\n\\frac{\\partial \\Phi}{\\partial u}(u;x) = 2 r u + 2 P \\beta x = 0,\n$$\nwhich yields the stationary feedback\n$$\nu^{\\ast}(x) = -\\frac{\\beta P}{r}\\,x.\n$$\nThe second derivative $\\frac{\\partial^{2}\\Phi}{\\partial u^{2}} = 2 r>0$ confirms that this is indeed the global minimizer. Substituting $u^{\\ast}$ back, we evaluate at the minimum:\n\n$$\n\\begin{aligned}\nr \\big(u^{\\ast}(x)\\big)^{2} &= r \\left(\\frac{\\beta^{2} P^{2}}{r^{2}} x^{2}\\right) = \\frac{\\beta^{2} P^{2}}{r} x^{2},\\\\\n\\big(-\\theta x + \\beta u^{\\ast}(x)\\big) V'(x) &= \\big(-\\theta x - \\beta \\tfrac{\\beta P}{r} x\\big)\\,2 P x = \\big(-2\\theta P - \\tfrac{2\\beta^{2} P^{2}}{r}\\big) x^{2},\\\\\n\\tfrac{1}{2}\\sigma^{2} V''(x) &= \\sigma^{2} P.\n\\end{aligned}\n$$\n\nTherefore the HJB becomes, after minimization,\n$$\n\\rho P x^{2} + \\rho C = \\left[q - 2\\theta P - \\frac{\\beta^{2}}{r} P^{2}\\right] x^{2} + \\sigma^{2} P.\n$$\nMatching the coefficients of $x^{2}$ and the constants gives the coupled algebraic equations\n$$\n\\begin{cases}\n\\rho P = q - 2\\theta P - \\dfrac{\\beta^{2}}{r} P^{2} \\\\[6pt]\n\\rho C = \\sigma^{2} P\n\\end{cases}\n$$\nThe first is an algebraic Riccati equation for $P$:\n$$\n\\frac{\\beta^{2}}{r} P^{2} + (\\rho + 2\\theta) P - q = 0.\n$$\nIts two roots are\n$$\nP_{\\pm} = \\frac{ -(\\rho + 2\\theta) \\pm \\sqrt{(\\rho + 2\\theta)^{2} + \\dfrac{4\\beta^{2}}{r} q} }{ 2 \\dfrac{\\beta^{2}}{r} } = \\frac{r}{2\\beta^{2}}\\left( -(\\rho + 2\\theta) \\pm \\sqrt{(\\rho + 2\\theta)^{2} + \\frac{4\\beta^{2}}{r} q} \\right).\n$$\nBecause $q>0$, $r>0$, and $\\beta\\neq 0$, the discriminant is strictly greater than $(\\rho + 2\\theta)^{2}$, so the square root exceeds $\\rho + 2\\theta$. Hence $P_{-}<0$ and\n$$\nP_{+} = \\frac{r}{2\\beta^{2}}\\left( -(\\rho + 2\\theta) + \\sqrt{(\\rho + 2\\theta)^{2} + \\frac{4\\beta^{2}}{r} q} \\right) > 0.\n$$\nConvexity of $V$ and finiteness of the value function select the positive stabilizing solution, so we take $P=P_{+}$. The constant $C$ is then\n$$\nC = \\frac{\\sigma^{2}}{\\rho}\\,P_{+}.\n$$\n\nCollecting, the value function and optimal feedback are\n\n$$\n\\begin{aligned}\nV(x) &= P_{+} x^{2} + \\frac{\\sigma^{2}}{\\rho}\\,P_{+},\\\\\nu^{\\ast}(x) &= -\\frac{\\beta P_{+}}{r}\\,x = \\frac{(\\rho + 2\\theta) - \\sqrt{(\\rho + 2\\theta)^{2} + \\dfrac{4\\beta^{2}}{r} q}}{2\\beta}\\,x.\n\\end{aligned}\n$$\n\nThe equality for $u^{\\ast}(x)$ follows by substituting the expression for $P_{+}$. To interpret stabilization, observe that under $u^{\\ast}$ the closed-loop drift becomes\n$$\n-\\theta x + \\beta u^{\\ast}(x) = -\\left(\\theta + \\frac{\\beta^{2}}{r} P_{+}\\right) x,\n$$\nwith $\\theta + \\dfrac{\\beta^{2}}{r} P_{+} > \\theta > 0$. Thus the linear drift coefficient in the closed loop is strictly more negative than in open loop, yielding an Ornstein–Uhlenbeck process with stronger mean reversion to the origin. This negative linear feedback is precisely the stabilizing action that minimizes the long-run discounted quadratic cost in the presence of diffusion noise.", "answer": "$$\\boxed{\\begin{pmatrix}\n\\frac{r}{2\\beta^{2}}\\!\\left(\\! -(\\rho+2\\theta)+\\sqrt{(\\rho+2\\theta)^{2}+\\frac{4\\beta^{2}}{r}q}\\,\\right) x^{2}+\\frac{\\sigma^{2}}{\\rho}\\cdot \\frac{r}{2\\beta^{2}}\\!\\left(\\! -(\\rho+2\\theta)+\\sqrt{(\\rho+2\\theta)^{2}+\\frac{4\\beta^{2}}{r}q}\\,\\right)\n&\n\\frac{(\\rho+2\\theta)-\\sqrt{(\\rho+2\\theta)^{2}+\\frac{4\\beta^{2}}{r}q}}{2\\beta}\\,x\n\\end{pmatrix}}$$", "id": "3001633"}, {"introduction": "While finding a classical solution to the HJB equation is ideal, it is not always possible. A crucial skill is to verify whether a given candidate function is indeed the true value function. This practice explores the HJB equation's role as a necessary condition for optimality in a simple deterministic minimum-time problem [@problem_id:2752646]. You will test a plausible, smooth candidate function that satisfies the boundary conditions, only to discover it fails the HJB equation within the domain. This exercise provides a concrete counterexample that highlights the limitations of classical solutions and motivates the need for the more general concept of viscosity solutions.", "problem": "Consider the deterministic control system on the domain $\\Omega := (-1,1)$ given by the dynamics $\\dot{x}(t) = u(t)$ with control constraint $u(t) \\in [-1,1]$ for all $t \\geq 0$. Let the exit time be $\\tau := \\inf\\{ t \\geq 0 : |x(t)| = 1 \\}$ and the cost functional be $J(x_{0};u(\\cdot)) := \\int_{0}^{\\tau} 1 \\, dt$, where $x(0)=x_{0} \\in \\Omega$. Define the value function $V(x_{0})$ as the infimum of $J(x_{0};u(\\cdot))$ over all measurable controls $u(\\cdot)$ with values in $[-1,1]$. \n\nStart from the dynamic programming principle and derive the pointwise consistency condition for any sufficiently smooth candidate function $W:\\Omega \\to \\mathbb{R}$ that agrees with the boundary condition $W(x)=0$ for $|x|=1$, showing how, in the interior of $\\Omega$, the Hamilton-Jacobi-Bellman (HJB) equation constrains $W$ through an appropriate residual. Then, take the specific smooth candidate $W(x) := 1 - x^{2}$, which satisfies the boundary condition $W(\\pm 1)=0$. Using your derived HJB consistency condition, evaluate the pointwise residual at $x=0$.\n\nYour final answer must be a single real number corresponding to the value of this residual at $x=0$. No rounding is required.", "solution": "The problem asks for the derivation of the Hamilton-Jacobi-Bellman (HJB) consistency condition, starting from the dynamic programming principle (DPP), and then its application to a specific candidate function.\n\nThe system dynamics are given by $\\dot{x}(t) = u(t)$, with state $x(t) \\in \\Omega := (-1,1)$ and control $u(t) \\in [-1,1]$. The cost functional represents the time to exit the domain $\\Omega$:\n$$J(x_{0};u(\\cdot)) := \\int_{0}^{\\tau} 1 \\, dt$$\nwhere $\\tau := \\inf\\{t \\geq 0 : |x(t)| = 1\\}$. The value function $V(x)$ is the infimum of this cost over all admissible controls: $V(x) = \\inf_{u(\\cdot)} J(x; u(\\cdot))$. The running cost is $L(x,u) = 1$. The boundary condition is $V(x) = 0$ for $x \\in \\partial\\Omega$, i.e., for $|x|=1$.\n\nThe dynamic programming principle states that for any small time interval $h > 0$, the value function satisfies:\n$$V(x) = \\inf_{u(\\cdot):[0,h]\\to[-1,1]} \\left\\{ \\int_0^h L(x(t), u(t)) \\, dt + V(x(h)) \\right\\}$$\nFor this specific problem, this becomes:\n$$V(x) = \\inf_{u(\\cdot):[0,h]\\to[-1,1]} \\left\\{ h + V(x(h)) \\right\\}$$\nAssuming the control $u(t)$ is a constant value $u \\in [-1,1]$ over the interval $[0, h]$, the state evolves as $x(h) = x(0) + \\int_0^h u \\, ds = x + hu$.\nIf the value function $V$ is assumed to be continuously differentiable ($C^1$), we can expand $V(x(h))$ in a Taylor series around $x$:\n$$V(x(h)) = V(x+hu) = V(x) + V'(x)(hu) + O(h^2)$$\nwhere $V'(x)$ denotes the derivative of $V$ with respect to $x$. Substituting this into the DPP equation:\n$$V(x) = \\inf_{u \\in [-1,1]} \\left\\{ h + V(x) + hV'(x)u + O(h^2) \\right\\}$$\nWe subtract $V(x)$ from both sides and divide by $h > 0$:\n$$0 = \\inf_{u \\in [-1,1]} \\left\\{ 1 + V'(x)u + O(h) \\right\\}$$\nTaking the limit as $h \\to 0$, the $O(h)$ term vanishes, yielding the stationary Hamilton-Jacobi-Bellman equation that the value function $V(x)$ must satisfy in the interior of the domain $\\Omega$:\n$$0 = \\inf_{u \\in [-1,1]} \\left\\{ 1 + V'(x)u \\right\\}$$\nThis equation provides the pointwise consistency condition. For any sufficiently smooth candidate function $W(x)$, we define the HJB residual, $R_W(x)$, as the value of the HJB expression:\n$$R_W(x) := \\inf_{u \\in [-1,1]} \\left\\{ 1 + W'(x)u \\right\\}$$\nFor a candidate function to be the true value function, its residual must be zero for all $x \\in \\Omega$ and it must satisfy the boundary conditions.\n\nWe are given the candidate function $W(x) := 1 - x^2$.\nFirst, we verify the boundary condition: $W(\\pm 1) = 1 - (\\pm 1)^2 = 1 - 1 = 0$. The condition is satisfied.\nNext, we compute the derivative of $W(x)$:\n$$W'(x) = \\frac{d}{dx}(1 - x^2) = -2x$$\nWe now substitute this derivative into the expression for the residual:\n$$R_W(x) = \\inf_{u \\in [-1,1]} \\left\\{ 1 + (-2x)u \\right\\} = \\inf_{u \\in [-1,1]} \\left\\{ 1 - 2xu \\right\\}$$\nThe expression to be minimized, $1 - 2xu$, is a linear function of the control variable $u$ on the compact set $[-1,1]$. The minimum must occur at one of the endpoints, $u=-1$ or $u=1$. The choice depends on the sign of the coefficient of $u$, which is $-2x$.\nSpecifically, the control $u^*(x)$ that minimizes the expression is $u^*(x) = \\text{sgn}(-2x) = -\\text{sgn}(x)$ for $x \\neq 0$.\nThe infimum can be written as $1 - \\sup_{u \\in [-1,1]} \\{2xu\\}$. The supremum is $2|x|$, so the residual is:\n$$R_W(x) = 1 - 2|x|$$\nThe problem asks for the value of this pointwise residual at the point $x=0$. We evaluate the residual function at $x=0$:\n$$R_W(0) = 1 - 2|0| = 1$$\nAlternatively, one can directly evaluate the infimum expression at $x=0$:\n$$R_W(0) = \\inf_{u \\in [-1,1]} \\{ 1 - 2(0)u \\} = \\inf_{u \\in [-1,1]} \\{ 1 \\} = 1$$\nThe residual at $x=0$ is $1$.", "answer": "$$\\boxed{1}$$", "id": "2752646"}, {"introduction": "Analytical solutions to the HJB equation are rare, making numerical methods essential for practical applications. This hands-on programming exercise transitions from theory to computation by having you implement the policy iteration algorithm, a powerful and intuitive method for solving HJB equations numerically. You will discretize the state and control spaces for a stochastic linear-quadratic problem and iteratively alternate between a policy evaluation step (solving a linear system) and a policy improvement step (minimizing the Hamiltonian) [@problem_id:3001638]. By comparing your numerical results to the exact analytical solution, you will gain invaluable insight into the bridge between continuous-time theory and discrete-time implementation, and verify the convergence of a fundamental control algorithm.", "problem": "Implement from first principles a complete policy iteration scheme for a one-dimensional stationary discounted Hamilton-Jacobi-Bellman equation arising from a controlled stochastic differential equation. The controlled state process is specified by the stochastic differential equation $dX_t = \\left(a X_t + b u_t\\right) dt + \\sigma dW_t$ with discount rate $\\rho > 0$, where $a$, $b$, $\\sigma$, and $\\rho$ are fixed real parameters, $u_t \\in \\mathbb{R}$ is a control, and $W_t$ is a standard Wiener process. The running cost is $\\ell(x,u) = q x^2 + r u^2$ with $q > 0$ and $r > 0$. The objective from an initial state $x$ is to minimize the discounted cost $J^u(x) = \\mathbb{E}\\left[\\int_0^\\infty e^{-\\rho t} \\ell\\left(X_t^u, u_t\\right) dt\\right]$ over all admissible controls. Let $V(x)$ denote the value function.\n\nYour task is to:\n- Derive the stationary discounted Hamilton-Jacobi-Bellman equation satisfied by $V(x)$ on a bounded interval $\\left[-x_{\\max}, x_{\\max}\\right]$ with Dirichlet boundary conditions chosen to match the exact infinite-horizon discounted linear-quadratic solution $V^{\\mathrm{ref}}(x)$ of this problem. The reference solution is known to be quadratic of the form $V^{\\mathrm{ref}}(x) = P x^2 + C$, where the coefficient $P$ is determined by the algebraic Riccati relation implied by the Hamilton-Jacobi-Bellman equation, and $C$ is chosen to satisfy the stationary discounted balance for the constant diffusion term.\n- Discretize the state space uniformly with $N$ grid points on $\\left[-x_{\\max}, x_{\\max}\\right]$ with mesh spacing $h = 2 x_{\\max}/(N-1)$. Approximate the first derivative by the central difference operator and the second derivative by the standard three-point central difference operator at interior nodes. Impose Dirichlet boundary conditions $V(-x_{\\max}) = V^{\\mathrm{ref}}(-x_{\\max})$ and $V(x_{\\max}) = V^{\\mathrm{ref}}(x_{\\max})$.\n- Discretize the action space as a finite, symmetric set $\\mathcal{U}_M = \\left\\{u_j\\right\\}_{j=1}^M$ of $M$ uniformly spaced values in $\\left[-u_{\\max}, u_{\\max}\\right]$, where $u_{\\max}$ is chosen as $u_{\\max} = \\kappa \\left| b P x_{\\max} / r \\right|$ with $\\kappa = 1.25$ so that the discrete action grid covers the continuous optimizer across the computational domain.\n- Implement policy iteration:\n  - Policy evaluation: for a fixed discrete policy $u(x_i) \\in \\mathcal{U}_M$ at each interior grid point $x_i$, solve the linear system that results from the discretized stationary discounted Hamilton-Jacobi-Bellman equation to obtain the value vector on the grid.\n  - Policy improvement: at each interior grid point $x_i$, compute the discrete Hamiltonian minimizer $\\arg\\min_{u \\in \\mathcal{U}_M} \\left\\{ r u^2 + b u \\left(D_x V\\right)(x_i) \\right\\}$ using the current discrete gradient approximation $\\left(D_x V\\right)(x_i)$, and update the policy.\n  - Terminate when both the sup-norm change in the value vector and the sup-norm change in the policy are smaller than a tolerance $\\varepsilon$, or when the number of policy iterations reaches a maximum $K_{\\max}$.\n- For each test case below, compute the uniform error on the grid $\\| V_{\\mathrm{num}} - V^{\\mathrm{ref}} \\|_{\\infty}$, where $V_{\\mathrm{num}}$ is the converged numerical value function and $V^{\\mathrm{ref}}$ is the exact quadratic reference.\n\nUse the following test suite of parameter sets, each provided as a tuple $\\left(a, b, q, r, \\sigma, \\rho, x_{\\max}, N, M\\right)$:\n- Test $1$: $\\left(-0.5, 1.0, 1.0, 1.0, 0.5, 1.0, 3.0, 161, 61\\right)$.\n- Test $2$: $\\left(0.2, 1.0, 1.0, 0.5, 0.1, 1.0, 2.0, 161, 61\\right)$.\n- Test $3$: $\\left(0.3, 1.0, 2.0, 1.0, 1.0, 2.0, 3.0, 161, 61\\right)$.\n\nImplementation details and requirements:\n- Start policy iteration from the zero policy $u \\equiv 0$ on interior nodes.\n- Use tolerance $\\varepsilon = 10^{-6}$ and maximum iterations $K_{\\max} = 100$.\n- All computations are nondimensional; no physical units are required.\n- Your program must be a single, complete script that carries out the entire computation for all three tests without user input and prints a single line containing the results.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the tests, for example $\\left[\\text{result}_1,\\text{result}_2,\\text{result}_3\\right]$, where each entry is the floating-point value of $\\| V_{\\mathrm{num}} - V^{\\mathrm{ref}} \\|_{\\infty}$ for the corresponding test, computed on the state grid.", "solution": "### 1. Hamilton-Jacobi-Bellman Equation and Exact Solution\n\nThe state process is governed by the linear stochastic differential equation:\n$$\ndX_t = \\left(a X_t + b u_t\\right) dt + \\sigma dW_t\n$$\nwhere $a$, $b$, and $\\sigma$ are real constants, $u_t \\in \\mathbb{R}$ is the control, and $W_t$ is a Wiener process. The objective is to minimize the discounted cost functional with a discount rate $\\rho > 0$ and a quadratic running cost $\\ell(x, u) = q x^2 + r u^2$:\n$$\nJ^u(x) = \\mathbb{E}\\left[\\int_0^\\infty e^{-\\rho t} \\ell\\left(X_t^u, u_t\\right) dt \\;\\middle|\\; X_0 = x \\right]\n$$\nThe value function $V(x) = \\inf_u J^u(x)$ satisfies the stationary Hamilton-Jacobi-Bellman (HJB) equation:\n$$\n\\rho V(x) = \\inf_{u \\in \\mathbb{R}} \\left\\{ \\mathcal{L}^u V(x) + \\ell(x, u) \\right\\}\n$$\nwhere $\\mathcal{L}^u$ is the infinitesimal generator of the process $X_t$, given by $\\mathcal{L}^u V(x) = (a x + b u) V'(x) + \\frac{1}{2} \\sigma^2 V''(x)$. Substituting the expressions for the generator and running cost, the HJB equation becomes:\n$$\n\\rho V(x) = \\inf_{u \\in \\mathbb{R}} \\left\\{ (a x + b u) V'(x) + \\frac{1}{2} \\sigma^2 V''(x) + q x^2 + r u^2 \\right\\}\n$$\nThe infimum over $u$ is found by setting the partial derivative of the term in braces with respect to $u$ to zero:\n$$\nb V'(x) + 2 r u = 0 \\implies u^*(x) = -\\frac{b}{2r} V'(x)\n$$\nSubstituting this optimal control $u^*(x)$ back into the HJB equation yields a nonlinear ordinary differential equation for $V(x)$:\n$$\n\\rho V(x) = a x V'(x) - \\frac{b^2}{4r} (V'(x))^2 + \\frac{1}{2} \\sigma^2 V''(x) + q x^2\n$$\nFor this linear-quadratic problem, the value function is known to be a quadratic function of the state, $V(x) = P x^2 + C$. Its derivatives are $V'(x) = 2 P x$ and $V''(x) = 2 P$. Substituting these into the HJB equation gives:\n$$\n\\rho (P x^2 + C) = a x (2 P x) - \\frac{b^2}{4r} (2 P x)^2 + \\frac{1}{2} \\sigma^2 (2 P) + q x^2\n$$\n$$\n\\rho P x^2 + \\rho C = (2aP - \\frac{b^2}{r}P^2 + q) x^2 + \\sigma^2 P\n$$\nEquating coefficients of like powers of $x$, we obtain two algebraic equations for the unknown constants $P$ and $C$. The coefficient of $x^2$ yields the continuous-time algebraic Riccati equation (ARE):\n$$\n\\frac{b^2}{r} P^2 - (2a - \\rho) P - q = 0\n$$\nSince the cost must be positive definite, we require $P > 0$. The ARE is a quadratic equation for $P$ and admits a unique positive solution, which is given by:\n$$\nP = \\frac{(2a - \\rho) + \\sqrt{(2a - \\rho)^2 + 4(b^2/r)q}}{2(b^2/r)}\n$$\nEquating the constant terms gives the equation for $C$:\n$$\n\\rho C = \\sigma^2 P \\implies C = \\frac{\\sigma^2 P}{\\rho}\n$$\nThis defines the exact reference solution $V^{\\mathrm{ref}}(x) = Px^2 + C$ on the infinite domain.\n\n### 2. Discretization\n\nThe problem is solved on a bounded domain $x \\in [-x_{\\max}, x_{\\max}]$, which is discretized into a uniform grid of $N$ points $\\{x_i\\}_{i=0}^{N-1}$ with spacing $h = 2x_{\\max}/(N-1)$. Let $V_i$ be the numerical approximation of $V(x_i)$. At an interior grid point $x_i$ for $i \\in \\{1, \\dots, N-2\\}$, the derivatives are approximated using central finite differences:\n$$\nV'(x_i) \\approx \\frac{V_{i+1} - V_{i-1}}{2h}, \\qquad V''(x_i) \\approx \\frac{V_{i+1} - 2V_i + V_{i-1}}{h^2}\n$$\nFor a fixed policy $u(x)$, represented discretely as $u_i = u(x_i)$, the HJB equation is linear in $V$. The discretized form at each interior point $x_i$ is:\n$$\n\\rho V_i = (a x_i + b u_i) \\left(\\frac{V_{i+1} - V_{i-1}}{2h}\\right) + \\frac{1}{2}\\sigma^2 \\left(\\frac{V_{i+1} - 2V_i + V_{i-1}}{h^2}\\right) + q x_i^2 + r u_i^2\n$$\nRearranging terms, we obtain a linear equation for $V_{i-1}, V_i, V_{i+1}$:\n$$\n\\left( \\frac{a x_i + b u_i}{2h} - \\frac{\\sigma^2}{2h^2} \\right) V_{i-1} + \\left( \\rho + \\frac{\\sigma^2}{h^2} \\right) V_i - \\left( \\frac{a x_i + b u_i}{2h} + \\frac{\\sigma^2}{2h^2} \\right) V_{i+1} = q x_i^2 + r u_i^2\n$$\nThe boundary conditions are of Dirichlet type, fixed at the values from the reference solution: $V_0 = V^{\\mathrm{ref}}(-x_{\\max})$ and $V_{N-1} = V^{\\mathrm{ref}}(x_{\\max})$.\n\n### 3. Policy Iteration Algorithm\n\nPolicy iteration is an iterative method that alternates between two steps: policy evaluation and policy improvement.\n\n**Initialization**: The algorithm starts with an initial policy, which is set to the zero policy, $u^{(0)}(x_i) = 0$ for all interior grid points $x_i$. The initial value function $V^{(0)}$ is initialized to zero, with boundary values set from the reference solution.\n\n**Policy Evaluation**: For a given policy $u^{(k)}$, we solve for the corresponding value function $V^{(k+1)}$. The set of discretized HJB equations for all interior points $i \\in \\{1, \\dots, N-2\\}$ forms a tridiagonal linear system of equations for the unknown values $\\{V_i^{(k+1)}\\}_{i=1}^{N-2}$. This system, of the form $\\mathbf{A}\\mathbf{V} = \\mathbf{d}$, is solved using a standard linear algebra solver. The matrix $\\mathbf{A}$ and vector $\\mathbf{d}$ depend on the current policy $u^{(k)}$.\n\n**Policy Improvement**: With the new value function $V^{(k+1)}$ computed, the policy is updated to $u^{(k+1)}$ by minimizing the discrete Hamiltonian at each interior grid point $x_i$:\n$$\nu_i^{(k+1)} = \\arg\\min_{u \\in \\mathcal{U}_M} \\left\\{ (a x_i + b u) \\frac{V_{i+1}^{(k+1)} - V_{i-1}^{(k+1)}}{2h} + \\frac{1}{2}\\sigma^2 (\\dots) + q x_i^2 + r u^2 \\right\\}\n$$\nThis is equivalent to minimizing $r u^2 + b u \\frac{V_{i+1}^{(k+1)} - V_{i-1}^{(k+1)}}{2h}$ over the discrete action set $\\mathcal{U}_M$. The unconstrained minimizer is $u_i^* = -\\frac{b}{2r} \\frac{V_{i+1}^{(k+1)} - V_{i-1}^{(k+1)}}{2h}$. The new policy action $u_i^{(k+1)}$ is then chosen as the element in the discrete action set $\\mathcal{U}_M$ that is closest to $u_i^*$.\n\n**Termination**: The iteration between evaluation and improvement continues until the policy and value function converge. The process terminates when the maximum absolute change in the value vector, $\\|V^{(k+1)} - V^{(k)}\\|_{\\infty}$, and the maximum absolute change in the policy vector, $\\|u^{(k+1)} - u^{(k)}\\|_{\\infty}$, are both below a specified tolerance $\\varepsilon = 10^{-6}$, or when a maximum number of iterations $K_{\\max} = 100$ is reached.\n\n### 4. Error Calculation\n\nAfter the policy iteration converges to a final numerical solution $V_{\\mathrm{num}}$, its accuracy is assessed by computing the uniform norm of the error against the exact reference solution $V^{\\mathrm{ref}}$ on the computational grid:\n$$\n\\text{Error} = \\| V_{\\mathrm{num}} - V^{\\mathrm{ref}} \\|_{\\infty} = \\max_{i \\in \\{0, \\dots, N-1\\}} | V_{\\mathrm{num}}(x_i) - V^{\\mathrm{ref}}(x_i) |\n$$\nThis quantity is computed for each provided test case.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the policy iteration for all test cases and print results.\n    \"\"\"\n    test_cases = [\n        # (a, b, q, r, sigma, rho, x_max, N, M)\n        (-0.5, 1.0, 1.0, 1.0, 0.5, 1.0, 3.0, 161, 61),\n        (0.2, 1.0, 1.0, 0.5, 0.1, 1.0, 2.0, 161, 61),\n        (0.3, 1.0, 2.0, 1.0, 1.0, 2.0, 3.0, 161, 61),\n    ]\n\n    results = []\n    for params in test_cases:\n        error = run_policy_iteration(params)\n        results.append(error)\n\n    # Format and print the final output as a single line\n    print(f\"[{','.join(f'{r:.8f}' for r in results)}]\")\n\ndef run_policy_iteration(params):\n    \"\"\"\n    Solves the HJB equation for a single set of parameters using policy iteration.\n    \"\"\"\n    a, b, q, r, sigma, rho, x_max, N, M = params\n    \n    # --- Step A: Pre-computation and Setup ---\n\n    # 1. Solve the Algebraic Riccati Equation for P\n    # The ARE is (b^2/r)P^2 - (2a - rho)P - q = 0\n    A_ric = b**2 / r\n    B_ric = -(2.0*a - rho)\n    C_ric = -q\n    discriminant = B_ric**2 - 4.0 * A_ric * C_ric\n    # We need the positive root for P\n    P = (-B_ric + np.sqrt(discriminant)) / (2.0 * A_ric)\n\n    # 2. Calculate the constant C\n    C = (sigma**2 * P) / rho\n\n    # 3. Define the reference (exact) value function\n    def v_ref(x):\n        return P * x**2 + C\n\n    # 4. Set up the state and action grids\n    x_grid = np.linspace(-x_max, x_max, N)\n    h = x_grid[1] - x_grid[0]\n    v_ref_grid = v_ref(x_grid)\n\n    kappa = 1.25\n    u_max_val = kappa * abs(b * P * x_max / r)\n    \n    if M > 1 and u_max_val > 0:\n        u_grid = np.linspace(-u_max_val, u_max_val, M)\n    else:\n        u_grid = np.zeros(M)\n    u_step = u_grid[1] - u_grid[0] if M > 1 else 0\n\n    # 5. Set up iteration parameters\n    tol = 1e-6\n    max_iter = 100\n    \n    # --- Step B: Policy Iteration Loop ---\n\n    # Initialization\n    # Policy for interior points (size N-2)\n    current_policy = np.zeros(N - 2)\n    # Value function on the full grid (size N)\n    current_V = np.zeros(N)\n    # Set Dirichlet boundary conditions from the exact solution\n    current_V[0] = v_ref_grid[0]\n    current_V[-1] = v_ref_grid[-1]\n    \n    x_interior = x_grid[1:-1]\n    \n    for k in range(max_iter):\n        \n        # --- 1. Policy Evaluation ---\n        # Solve the linear system A * V_interior = d for V\n        \n        drift_coeff = a * x_interior + b * current_policy\n        \n        # Coefficients of the tridiagonal system\n        L = drift_coeff / (2.0 * h) - sigma**2 / (2.0 * h**2)\n        D = rho + sigma**2 / h**2\n        U = -drift_coeff / (2.0 * h) - sigma**2 / (2.0 * h**2)\n        \n        # Construct the (N-2) x (N-2) system matrix A\n        A = np.diag(D * np.ones(N-2)) + np.diag(U[:-1], k=1) + np.diag(L[1:], k=-1)\n        \n        # Construct the right-hand side vector d\n        d = q * x_interior**2 + r * current_policy**2\n        \n        # Adjust d for boundary conditions\n        d[0] -= L[0] * v_ref_grid[0]\n        d[-1] -= U[-1] * v_ref_grid[-1]\n        \n        # Solve for the new interior values of V\n        try:\n            V_interior_new = np.linalg.solve(A, d)\n        except np.linalg.LinAlgError:\n            # Fallback for singular matrix, although not expected here\n            return np.inf\n\n        # Form the full new value function vector\n        V_new = np.concatenate(([v_ref_grid[0]], V_interior_new, [v_ref_grid[-1]]))\n        \n        # --- 2. Policy Improvement ---\n        # Update the policy by minimizing the Hamiltonian\n        \n        # Approximate V'(x) at interior points\n        V_prime_interior = (V_new[2:] - V_new[:-2]) / (2.0 * h)\n        \n        # Compute the unconstrained optimal control\n        u_star_interior = -b / (2.0 * r) * V_prime_interior\n        \n        # Find the closest control in the discrete action space\n        new_policy = np.zeros_like(current_policy)\n        if M > 1 and u_step > 0:\n            indices = np.round((u_star_interior - u_grid[0]) / u_step)\n            indices = np.clip(indices, 0, M - 1).astype(int)\n            new_policy = u_grid[indices]\n            \n        # --- 3. Termination Check ---\n        \n        val_change = np.max(np.abs(V_new - current_V))\n        pol_change = np.max(np.abs(new_policy - current_policy))\n        \n        # Update for the next iteration\n        current_V = V_new\n        current_policy = new_policy\n        \n        if val_change < tol and pol_change < tol:\n            break\n            \n    # --- Step C: Final Error Calculation ---\n    \n    # The converged numerical solution is the last computed value function\n    V_num = current_V\n    error = np.max(np.abs(V_num - v_ref_grid))\n    \n    return error\n\nif __name__ == '__main__':\n    solve()\n```", "id": "3001638"}]}