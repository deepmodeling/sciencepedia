{"hands_on_practices": [{"introduction": "This first practice is a foundational exercise in nonlinear filtering theory. We will use the powerful change-of-measure technique, anchored by Girsanov's theorem, to derive the celebrated Kallianpur-Striebel formula, which expresses the filter in terms of a more tractable expectation under a reference measure. By deriving this \"master formula\" and verifying its correct normalization [@problem_id:2988875], you will build a solid theoretical base for all subsequent filtering problems.", "problem": "Consider a continuous-time nonlinear filtering model on a filtered probability space $\\left(\\Omega,\\mathcal{F},\\{\\mathcal{F}_{t}\\}_{t \\ge 0},\\mathbb{P}\\right)$ satisfying the usual conditions. Let $\\{W_{t}\\}_{t \\ge 0}$ and $\\{V_{t}\\}_{t \\ge 0}$ be independent standard Brownian motions of dimensions $d$ and $m$, respectively, adapted to $\\{\\mathcal{F}_{t}\\}_{t \\ge 0}$. The unobserved signal $\\{X_{t}\\}_{t \\ge 0}$ solves the stochastic differential equation (SDE)\n$$\ndX_{t} \\;=\\; a\\!\\left(X_{t}\\right)\\,dt \\;+\\; \\sigma\\!\\left(X_{t}\\right)\\,dW_{t},\n$$\nwith $X_{0}$ independent of $\\{V_{t}\\}_{t \\ge 0}$, where $a:\\mathbb{R}^{d}\\to\\mathbb{R}^{d}$ and $\\sigma:\\mathbb{R}^{d}\\to\\mathbb{R}^{d \\times d}$ are such that the SDE admits a unique strong solution. The observation process $\\{Y_{t}\\}_{t \\ge 0}$ is given by\n$$\ndY_{t} \\;=\\; h\\!\\left(X_{t}\\right)\\,dt \\;+\\; dV_{t},\n$$\nwhere $h:\\mathbb{R}^{d}\\to\\mathbb{R}^{m}$ is bounded and measurable. Let $\\mathcal{Y}_{t}:=\\sigma\\!\\left(Y_{s}:0\\le s\\le t\\right)$ denote the observation $\\sigma$-algebra. For any bounded measurable test function $\\varphi:\\mathbb{R}^{d}\\to\\mathbb{R}$, define the (normalized) conditional expectation\n$$\n\\pi_{t}(\\varphi)\\;:=\\;\\mathbb{E}^{\\mathbb{P}}\\!\\left[\\varphi(X_{t})\\mid \\mathcal{Y}_{t}\\right].\n$$\n\nAssume that the Novikov condition holds so that the exponential local martingales below are true martingales. Define the likelihood process\n$$\nZ_{t}\\;:=\\;\\exp\\!\\left(\\int_{0}^{t} h\\!\\left(X_{s}\\right)^{\\top} dY_{s}\\;-\\;\\frac{1}{2}\\int_{0}^{t}\\big\\|h\\!\\left(X_{s}\\right)\\big\\|^{2}\\,ds\\right),\n$$\nand the probability measure $\\mathbb{Q}$ on $\\mathcal{F}_{t}$ by the Radon–Nikodym derivative\n$$\n\\frac{d\\mathbb{Q}}{d\\mathbb{P}}\\bigg|_{\\mathcal{F}_{t}} \\;=\\; Z_{t}^{-1}.\n$$\nUnder $\\mathbb{Q}$, the process $\\{Y_{t}\\}_{t \\ge 0}$ is a standard $m$-dimensional Brownian motion independent of $\\{W_{t}\\}_{t \\ge 0}$. Starting only from the definitions of conditional expectation, the Radon–Nikodym theorem, and the Girsanov change-of-measure theorem, derive a representation for $\\pi_{t}(\\varphi)$ in terms of conditional expectations under $\\mathbb{Q}$ and the weight $Z_{t}$.\n\nThen, use your representation to evaluate the exact value of $\\pi_{t}(\\mathbf{1})$, where $\\mathbf{1}$ denotes the constant function $\\mathbf{1}(x)\\equiv 1$.\n\nYour final answer must be a single exact number without units.", "solution": "The problem is well-posed and scientifically grounded within the mathematical framework of nonlinear filtering theory and stochastic calculus. We shall proceed with the derivation.\n\nOur primary goal is to derive a representation for the conditional expectation $\\pi_{t}(\\varphi) = \\mathbb{E}^{\\mathbb{P}}[\\varphi(X_{t}) \\mid \\mathcal{Y}_{t}]$ under a new measure $\\mathbb{Q}$. This derivation is a classic application of the change of measure technique, often referred to as the Kallianpur-Striebel formula. We will then use this representation to evaluate $\\pi_{t}(\\mathbf{1})$.\n\nFirst, we derive a general formula for the change of measure for conditional expectations. Let $\\xi$ be a bounded $\\mathcal{F}_{t}$-measurable random variable and let $\\mathcal{G}$ be a sub-$\\sigma$-algebra of $\\mathcal{F}_{t}$. The conditional expectation $\\eta := \\mathbb{E}^{\\mathbb{P}}[\\xi \\mid \\mathcal{G}]$ is, by definition, the unique (up to almost sure equality) bounded $\\mathcal{G}$-measurable random variable satisfying\n$$\n\\mathbb{E}^{\\mathbb{P}}[\\eta A] = \\mathbb{E}^{\\mathbb{P}}[\\xi A]\n$$\nfor all bounded $\\mathcal{G}$-measurable random variables $A$.\n\nThe problem defines a new probability measure $\\mathbb{Q}$ via the Radon-Nikodym derivative $\\frac{d\\mathbb{Q}}{d\\mathbb{P}}\\big|_{\\mathcal{F}_{t}} = Z_{t}^{-1}$. Consequently, the inverse relationship is $\\frac{d\\mathbb{P}}{d\\mathbb{Q}}\\big|_{\\mathcal{F}_{t}} = Z_{t}$. The relationship between expectations under $\\mathbb{P}$ and $\\mathbb{Q}$ for any integrable random variable $U$ on $(\\Omega, \\mathcal{F}_t)$ is given by the Radon-Nikodym theorem as\n$$\n\\mathbb{E}^{\\mathbb{P}}[U] = \\mathbb{E}^{\\mathbb{Q}}\\!\\left[U \\frac{d\\mathbb{P}}{d\\mathbb{Q}}\\right] = \\mathbb{E}^{\\mathbb{Q}}[U Z_{t}].\n$$\nApplying this change of measure to the defining equation of conditional expectation, we obtain\n$$\n\\mathbb{E}^{\\mathbb{Q}}[\\eta A Z_{t}] = \\mathbb{E}^{\\mathbb{Q}}[\\xi A Z_{t}].\n$$\nUsing the tower property of conditional expectation, the left-hand side can be written as:\n$$\n\\mathbb{E}^{\\mathbb{Q}}[\\eta A Z_{t}] = \\mathbb{E}^{\\mathbb{Q}}\\!\\left[\\mathbb{E}^{\\mathbb{Q}}[\\eta A Z_{t} \\mid \\mathcal{G}]\\right].\n$$\nSince $\\eta$ and $A$ are $\\mathcal{G}$-measurable by definition, they can be treated as constants with respect to the inner conditional expectation:\n$$\n\\mathbb{E}^{\\mathbb{Q}}[\\eta A Z_{t}] = \\mathbb{E}^{\\mathbb{Q}}\\!\\left[\\eta A \\mathbb{E}^{\\mathbb{Q}}[Z_{t} \\mid \\mathcal{G}]\\right].\n$$\nSimilarly, for the right-hand side of our main equation:\n$$\n\\mathbb{E}^{\\mathbb{Q}}[\\xi A Z_{t}] = \\mathbb{E}^{\\mathbb{Q}}\\!\\left[\\mathbb{E}^{\\mathbb{Q}}[\\xi A Z_{t} \\mid \\mathcal{G}]\\right] = \\mathbb{E}^{\\mathbb{Q}}\\!\\left[A \\mathbb{E}^{\\mathbb{Q}}[\\xi Z_{t} \\mid \\mathcal{G}]\\right].\n$$\nEquating the transformed sides, we have\n$$\n\\mathbb{E}^{\\mathbb{Q}}\\!\\left[A \\eta \\mathbb{E}^{\\mathbb{Q}}[Z_{t} \\mid \\mathcal{G}]\\right] = \\mathbb{E}^{\\mathbb{Q}}\\!\\left[A \\mathbb{E}^{\\mathbb{Q}}[\\xi Z_{t} \\mid \\mathcal{G}]\\right].\n$$\nThis equality must hold for any bounded $\\mathcal{G}$-measurable random variable $A$. This implies that the random variables multiplying $A$ inside the expectations must be equal almost surely under $\\mathbb{Q}$:\n$$\n\\eta \\mathbb{E}^{\\mathbb{Q}}[Z_{t} \\mid \\mathcal{G}] = \\mathbb{E}^{\\mathbb{Q}}[\\xi Z_{t} \\mid \\mathcal{G}].\n$$\nSolving for $\\eta = \\mathbb{E}^{\\mathbb{P}}[\\xi \\mid \\mathcal{G}]$, we arrive at the abstract Bayes' formula:\n$$\n\\mathbb{E}^{\\mathbb{P}}[\\xi \\mid \\mathcal{G}] = \\frac{\\mathbb{E}^{\\mathbb{Q}}[\\xi Z_{t} \\mid \\mathcal{G}]}{\\mathbb{E}^{\\mathbb{Q}}[Z_{t} \\mid \\mathcal{G}]}.\n$$\nNow, we apply this general result to the specific filtering problem. We set $\\xi = \\varphi(X_{t})$ and $\\mathcal{G} = \\mathcal{Y}_{t}$. This immediately yields the desired representation of $\\pi_{t}(\\varphi)$ in terms of conditional expectations under $\\mathbb{Q}$:\n$$\n\\pi_{t}(\\varphi) = \\frac{\\mathbb{E}^{\\mathbb{Q}}\\!\\left[\\varphi(X_{t}) Z_{t} \\mid \\mathcal{Y}_{t}\\right]}{\\mathbb{E}^{\\mathbb{Q}}[Z_{t} \\mid \\mathcal{Y}_{t}]}.\n$$\nThis expression is the first part of the answer. It represents the posterior distribution as a ratio of two unnormalized conditional distributions. A crucial aspect provided in the problem statement is that under the measure $\\mathbb{Q}$, the observation process $\\{Y_t\\}_{t \\ge 0}$ is a standard Brownian motion *independent* of the driving noise $\\{W_t\\}_{t \\ge 0}$ of the signal process. Since the signal $\\{X_t\\}_{t \\ge 0}$ is a functional of its initial condition $X_0$ and the path of $\\{W_s\\}_{0 \\le s \\le t}$, it follows that the entire process $\\{X_t\\}_{t \\ge 0}$ is independent of the filtration $\\mathcal{Y}_t$ under the measure $\\mathbb{Q}$.\n\nThis independence property allows for a significant simplification of the conditional expectations. For a functional $F(X, Y)$ that depends on the paths of both processes $X$ and $Y$, the conditional expectation given $\\mathcal{Y}_t$ under $\\mathbb{Q}$ is calculated by integrating over the paths of $X$, treating the path of $Y$ as a fixed parameter:\n$$\n\\mathbb{E}^{\\mathbb{Q}}[F(X,Y) \\mid \\mathcal{Y}_t] = \\mathbb{E}_{X}^{\\mathbb{Q}}[F(X,Y)].\n$$\nHere, $\\mathbb{E}_{X}^{\\mathbb{Q}}$ denotes the expectation with respect to the law of the process $X$ under $\\mathbb{Q}$. Since the Radon-Nikodym derivative $Z_t^{-1}$ involves only $X$ and $V$ (via $Y$), the law of the process $X$ (driven by $W$) is the same under $\\mathbb{P}$ and $\\mathbb{Q}$. Thus we can simply write $\\mathbb{E}_X$. Applying this to our formula for $\\pi_t(\\varphi)$:\nThe numerator becomes $\\mathbb{E}_{X}[\\varphi(X_{t}) Z_{t}]$, and the denominator becomes $\\mathbb{E}_{X}[Z_{t}]$.\nHere, $Z_{t} = \\exp(\\int_{0}^{t} h(X_{s})^{\\top} dY_{s} - \\frac{1}{2}\\int_{0}^{t}\\|h(X_{s})\\|^{2}\\,ds)$ is treated as a functional where the stochastic integral is with respect to the observed path of $Y$, and the expectation averages over all paths of $X$.\n\nThe final representation for the filter is:\n$$\n\\pi_{t}(\\varphi) = \\frac{\\mathbb{E}_{X}\\!\\left[\\varphi(X_t) \\exp\\!\\left(\\int_{0}^{t} h(X_{s})^{\\top} dY_{s} - \\frac{1}{2}\\int_{0}^{t}\\|h(X_{s})\\|^{2}\\,ds\\right)\\right]}{\\mathbb{E}_{X}\\!\\left[\\exp\\!\\left(\\int_{0}^{t} h(X_{s})^{\\top} dY_{s} - \\frac{1}{2}\\int_{0}^{t}\\|h(X_{s})\\|^{2}\\,ds\\right)\\right]}.\n$$\n\nNow, we proceed to the second part of the task: to evaluate $\\pi_{t}(\\mathbf{1})$. The function $\\mathbf{1}$ is the constant function such that $\\mathbf{1}(x) = 1$ for all $x \\in \\mathbb{R}^d$.\nWe can evaluate this directly from the definition of $\\pi_t(\\varphi)$:\n$$\n\\pi_{t}(\\mathbf{1}) = \\mathbb{E}^{\\mathbb{P}}[\\mathbf{1}(X_{t}) \\mid \\mathcal{Y}_{t}] = \\mathbb{E}^{\\mathbb{P}}[1 \\mid \\mathcal{Y}_{t}].\n$$\nThe conditional expectation of a constant is the constant itself. Thus,\n$$\n\\pi_{t}(\\mathbf{1}) = 1.\n$$\nTo confirm this result using the derived representation, we substitute $\\varphi(X_t) = \\mathbf{1}(X_t) = 1$ into our formula:\n$$\n\\pi_{t}(\\mathbf{1}) = \\frac{\\mathbb{E}_{X}[1 \\cdot Z_{t}]}{\\mathbb{E}_{X}[Z_{t}]} = \\frac{\\mathbb{E}_{X}[Z_{t}]}{\\mathbb{E}_{X}[Z_{t}]}.\n$$\nSince the Novikov condition is assumed to hold, the exponential martingale $Z_t$ has a well-defined and positive expectation. Thus, the denominator $\\mathbb{E}_{X}[Z_{t}]$ is strictly positive. Therefore, the expression simplifies to $1$. Both methods yield the same result. The exact value is $1$.", "answer": "$$\\boxed{1}$$", "id": "2988875"}, {"introduction": "With the abstract Kallianpur-Striebel formula in hand, we now apply it to a concrete and illustrative example. This problem involves estimating a simple, hidden binary state from a continuous, noisy observation, a canonical model in signal processing. This exercise [@problem_id:2988892] bridges the gap between abstract theory and practical calculation, culminating in an elegant closed-form solution for the conditional expectation of the state.", "problem": "Consider a hidden signal process $\\{X_t\\}_{t \\in [0,1]}$ taking values in the finite state space $\\{-1,+1\\}$, with piecewise constant paths specified by $X_t \\equiv X_0$ almost surely. Assume the prior distribution is symmetric, $\\mathbb{P}(X_0=+1)=\\mathbb{P}(X_0=-1)=\\tfrac{1}{2}$. The observation process $\\{Y_t\\}_{t \\in [0,1]}$ is scalar and governed by the stochastic differential equation (SDE)\n$$\n\\mathrm{d}Y_t \\;=\\; \\theta\\, X_t\\, \\mathrm{d}t \\;+\\; \\mathrm{d}W_t,\n$$\nwhere $\\theta>0$ is known and $\\{W_t\\}$ is a standard Wiener process independent of $X_0$. For a bounded test function $\\varphi:\\{-1,+1\\}\\to\\mathbb{R}$, define the nonlinear filter\n$$\n\\pi_t(\\varphi) \\;=\\; \\mathbb{E}\\!\\left[\\,\\varphi(X_t)\\,\\middle|\\,\\mathcal{F}_t^Y\\right],\n$$\nwhere $\\mathcal{F}_t^Y$ is the $\\sigma$-algebra generated by $\\{Y_s:0\\le s\\le t\\}$. Starting from the fundamental definition of conditional expectation, the existence of a reference probability measure under which $\\{Y_t\\}$ is a standard Wiener process independent of $\\{X_t\\}$, and the Radon–Nikodym likelihood process represented by a Doléans–Dade exponential, derive an explicit closed-form expression for $\\pi_t(\\varphi)$ as a functional of the observed path $\\{Y_s: 0 \\le s \\le t\\}$ in the special case $\\varphi(x)=x$.\n\nThen evaluate your expression at time $t=1$ for the observation whose terminal value satisfies $Y_1=\\frac{1}{2\\theta}\\ln 3$. Express your final answer as a single real number. No units are required, and no rounding is necessary.", "solution": "The problem is well-posed and represents a canonical example in nonlinear filtering theory. We shall proceed with its solution. The objective is to derive an explicit expression for the nonlinear filter $\\pi_t(\\varphi) = \\mathbb{E}[\\varphi(X_t) \\mid \\mathcal{F}_t^Y]$ for the special case $\\varphi(x)=x$, and then evaluate it under a specific condition.\n\nLet the underlying probability space be $(\\Omega, \\mathcal{F}, \\mathbb{P})$. The observation process is given by the SDE:\n$$\n\\mathrm{d}Y_t = \\theta X_t \\mathrm{d}t + \\mathrm{d}W_t, \\quad Y_0 = 0\n$$\nwhere $\\{W_t\\}$ is a standard Wiener process under $\\mathbb{P}$ and is independent of $X_0$.\n\nThe core of the solution lies in the change of probability measure technique. We define a new reference measure $\\mathbb{P}_0$ under which the observation process $\\{Y_t\\}$ behaves as a standard Wiener process. According to Girsanov's theorem, we can define such a measure $\\mathbb{P}_0$ via its Radon-Nikodym derivative with respect to $\\mathbb{P}$ on the filtration $\\mathcal{F}_t = \\sigma(X_0) \\vee \\mathcal{F}_t^W$, where $\\mathcal{F}_t^W$ is the filtration generated by $\\{W_s: 0 \\le s \\le t\\}$. Let us define the process $Z_t$ such that $\\mathrm{d}Y_t = Z_t \\mathrm{d}t + \\mathrm{d}W_t$. Here, $Z_t = \\theta X_t$. The Novikov condition, $\\mathbb{E}[\\exp(\\frac{1}{2}\\int_0^t Z_s^2 \\mathrm{d}s)] < \\infty$, is satisfied since $X_s^2 = 1$, so $\\int_0^t Z_s^2 \\mathrm{d}s = \\theta^2 t$, which is deterministic and finite.\n\nWe define the likelihood process (or Radon-Nikodym density process) $L_t$ as the Doléans-Dade exponential:\n$$\nL_t = \\frac{\\mathrm{d}\\mathbb{P}}{\\mathrm{d}\\mathbb{P}_0}\\bigg|_{\\mathcal{F}_t} = \\exp\\left( \\int_0^t \\theta X_s \\mathrm{d}Y_s - \\frac{1}{2} \\int_0^t (\\theta X_s)^2 \\mathrm{d}s \\right)\n$$\nwhere under the new measure $\\mathbb{P}_0$, the process $\\{Y_t\\}$ is a standard Wiener process independent of $\\{X_t\\}$. The integral $\\int_0^t \\theta X_s \\mathrm{d}Y_s$ is an Itô integral with respect to $\\{Y_t\\}$, which is a semimartingale under $\\mathbb{P}$ but a Wiener process under $\\mathbb{P}_0$.\n\nThe problem states that $X_t \\equiv X_0$ almost surely for $t \\in [0,1]$. This simplifies the exponents:\n$$\n\\int_0^t \\theta X_s \\mathrm{d}Y_s = \\theta X_0 \\int_0^t \\mathrm{d}Y_s = \\theta X_0 Y_t\n$$\n$$\n\\int_0^t (\\theta X_s)^2 \\mathrm{d}s = \\int_0^t \\theta^2 \\mathrm{d}s = \\theta^2 t\n$$\nThus, the likelihood process becomes:\n$$\nL_t = \\exp\\left( \\theta X_0 Y_t - \\frac{1}{2}\\theta^2 t \\right)\n$$\n\nThe fundamental formula for nonlinear filtering, often called the Kallianpur-Striebel formula, connects the conditional expectation under $\\mathbb{P}$ to an expectation under the reference measure $\\mathbb{P}_0$. For any bounded test function $\\varphi$,\n$$\n\\pi_t(\\varphi) = \\mathbb{E}[\\varphi(X_t) \\mid \\mathcal{F}_t^Y] = \\frac{\\mathbb{E}_0[\\varphi(X_t) L_t \\mid \\mathcal{F}_t^Y]}{\\mathbb{E}_0[L_t \\mid \\mathcal{F}_t^Y]}\n$$\nSince $X_t = X_0$ and $\\{Y_t\\}$ is independent of $X_0$ under $\\mathbb{P}_0$, and $Y_t$ is $\\mathcal{F}_t^Y$-measurable, we can simplify the numerator and denominator. The term $L_t$ explicitly depends on $X_0$ and $Y_t$.\nThe numerator is:\n$$\n\\mathbb{E}_0[\\varphi(X_0) L_t \\mid \\mathcal{F}_t^Y] = \\mathbb{E}_0\\left[\\varphi(X_0) \\exp\\left( \\theta X_0 Y_t - \\frac{1}{2}\\theta^2 t \\right) \\mid \\mathcal{F}_t^Y\\right]\n$$\nSince $X_0$ is independent of $\\mathcal{F}_t^Y$ under $\\mathbb{P}_0$, we can take the expectation with respect to $X_0$ only:\n$$\n\\mathbb{E}_0[\\varphi(X_0) L_t \\mid \\mathcal{F}_t^Y] = \\mathbb{E}_0\\left[\\varphi(X_0) \\exp\\left( \\theta X_0 Y_t - \\frac{1}{2}\\theta^2 t \\right)\\right]\n$$\nThis expectation is over the distribution of $X_0$, which is the same under both $\\mathbb{P}$ and $\\mathbb{P}_0$. The term $\\exp(-\\frac{1}{2}\\theta^2 t)$ is a constant with respect to the random variable $X_0$.\nLet's define the unnormalized filter $\\sigma_t(\\varphi) = \\mathbb{E}_0[\\varphi(X_0) L_t]$. Then:\n$$\n\\pi_t(\\varphi) = \\frac{\\sigma_t(\\varphi)}{\\sigma_t(1)}\n$$\nwhere $\\sigma_t(1)$ corresponds to the case $\\varphi(x)=1$.\n\nWe now evaluate $\\sigma_t(\\varphi)$ using the given prior distribution $\\mathbb{P}(X_0=+1) = \\mathbb{P}(X_0=-1) = \\frac{1}{2}$.\n$$\n\\sigma_t(\\varphi) = \\sum_{k \\in \\{-1, +1\\}} \\varphi(k) \\exp\\left( \\theta k Y_t - \\frac{1}{2}\\theta^2 t \\right) \\mathbb{P}(X_0=k)\n$$\n$$\n\\sigma_t(\\varphi) = \\frac{1}{2}\\exp\\left(-\\frac{1}{2}\\theta^2 t\\right) \\left[ \\varphi(1) \\exp(\\theta Y_t) + \\varphi(-1) \\exp(-\\theta Y_t) \\right]\n$$\nFor the specific case $\\varphi(x)=x$, we have $\\varphi(1)=1$ and $\\varphi(-1)=-1$.\n$$\n\\sigma_t(x) = \\frac{1}{2}\\exp\\left(-\\frac{1}{2}\\theta^2 t\\right) \\left[ \\exp(\\theta Y_t) - \\exp(-\\theta Y_t) \\right]\n$$\nFor the denominator, we set $\\varphi(x)=1$, which gives $\\varphi(1)=1$ and $\\varphi(-1)=1$.\n$$\n\\sigma_t(1) = \\frac{1}{2}\\exp\\left(-\\frac{1}{2}\\theta^2 t\\right) \\left[ \\exp(\\theta Y_t) + \\exp(-\\theta Y_t) \\right]\n$$\nTaking the ratio gives the desired filter expression:\n$$\n\\pi_t(x) = \\frac{\\sigma_t(x)}{\\sigma_t(1)} = \\frac{\\frac{1}{2}\\exp(-\\frac{1}{2}\\theta^2 t) [ \\exp(\\theta Y_t) - \\exp(-\\theta Y_t) ]}{\\frac{1}{2}\\exp(-\\frac{1}{2}\\theta^2 t) [ \\exp(\\theta Y_t) + \\exp(-\\theta Y_t) ]} = \\frac{\\exp(\\theta Y_t) - \\exp(-\\theta Y_t)}{\\exp(\\theta Y_t) + \\exp(-\\theta Y_t)}\n$$\nThis expression is the definition of the hyperbolic tangent function. Thus, the explicit closed-form expression for the filter is:\n$$\n\\pi_t(x) = \\tanh(\\theta Y_t)\n$$\nThis is the conditional expectation of $X_t$ given the history of observations up to time $t$, i.e., $\\mathbb{E}[X_t \\mid \\mathcal{F}_t^Y]$.\n\nNow, we must evaluate this expression at time $t=1$ for the specific observation path whose terminal value is $Y_1 = \\frac{1}{2\\theta}\\ln 3$.\nWe need to compute $\\pi_1(x) = \\tanh(\\theta Y_1)$.\nSubstituting the given value of $Y_1$:\n$$\n\\pi_1(x) = \\tanh\\left(\\theta \\cdot \\frac{1}{2\\theta}\\ln 3\\right) = \\tanh\\left(\\frac{1}{2}\\ln 3\\right)\n$$\nWe can simplify this expression. First, use the property $a\\ln b = \\ln(b^a)$:\n$$\n\\frac{1}{2}\\ln 3 = \\ln(3^{1/2}) = \\ln(\\sqrt{3})\n$$\nSo, we need to evaluate $\\tanh(\\ln(\\sqrt{3}))$. Using the definition $\\tanh(z) = \\frac{\\exp(z)-\\exp(-z)}{\\exp(z)+\\exp(-z)}$:\n$$\n\\tanh(\\ln(\\sqrt{3})) = \\frac{\\exp(\\ln(\\sqrt{3})) - \\exp(-\\ln(\\sqrt{3}))}{\\exp(\\ln(\\sqrt{3})) + \\exp(-\\ln(\\sqrt{3}))}\n$$\nSince $\\exp(\\ln a) = a$ and $\\exp(-\\ln a) = \\exp(\\ln(1/a)) = 1/a$:\n$$\n\\tanh(\\ln(\\sqrt{3})) = \\frac{\\sqrt{3} - \\frac{1}{\\sqrt{3}}}{\\sqrt{3} + \\frac{1}{\\sqrt{3}}}\n$$\nTo simplify the fraction, we multiply the numerator and the denominator by $\\sqrt{3}$:\n$$\n\\tanh(\\ln(\\sqrt{3})) = \\frac{\\sqrt{3}\\left(\\sqrt{3} - \\frac{1}{\\sqrt{3}}\\right)}{\\sqrt{3}\\left(\\sqrt{3} + \\frac{1}{\\sqrt{3}}\\right)} = \\frac{(\\sqrt{3})^2 - 1}{(\\sqrt{3})^2 + 1} = \\frac{3-1}{3+1} = \\frac{2}{4} = \\frac{1}{2}\n$$\nThe value of the filter at $t=1$ is $\\frac{1}{2}$. This means that given the observation path ending at $Y_1 = \\frac{1}{2\\theta}\\ln 3$, the conditional probability of $X_1$ being $+1$ is $\\mathbb{P}(X_1=+1 \\mid \\mathcal{F}_1^Y) = \\frac{1+\\pi_1(x)}{2} = \\frac{1+1/2}{2} = \\frac{3}{4}$, and the probability of it being $-1$ is $\\frac{1}{4}$. The conditional expectation is $(+1)\\frac{3}{4} + (-1)\\frac{1}{4} = \\frac{2}{4} = \\frac{1}{2}$.", "answer": "$$\\boxed{\\frac{1}{2}}$$", "id": "2988892"}, {"introduction": "To truly grasp a concept, one must also understand its limitations. This exercise presents a carefully constructed thought experiment where the hidden state is fundamentally unobservable from the data, even with an infinite amount of it. By analyzing why the filter fails to learn the true state in this scenario [@problem_id:2988870], you will develop a deeper appreciation for the crucial concept of observability and the conditions required for successful state estimation.", "problem": "Consider the following hidden-static-parameter filtering model on a complete probability space with filtration generated by the observation process. Let the hidden state be a random variable $X \\in \\{-1,1\\}$ with prior $\\mathbb{P}(X=1)=\\mathbb{P}(X=-1)=\\tfrac{1}{2}$, independent of a standard one-dimensional Brownian motion $W=\\{W_t\\}_{t \\ge 0}$. The observation process $Y=\\{Y_t\\}_{t \\ge 0}$ solves the stochastic differential equation\n$$\ndY_t \\;=\\; X\\, dW_t, \\qquad Y_0 = 0,\n$$\nand the observation filtration is $\\mathcal{Y}_t := \\sigma\\{Y_s: 0 \\le s \\le t\\}^{\\mathbb{P}\\text{-completion}}$. Define the posterior process (the filter) by\n$$\n\\pi_t \\;:=\\; \\mathbb{P}\\big(X=1 \\,\\big|\\, \\mathcal{Y}_t\\big).\n$$\n\nStarting from first principles in stochastic processes and measure theory (invariance properties of Brownian motion, conditional expectation as $L^1$-projection, and Bayes’ rule via Radon–Nikodym derivatives on path space), do the following:\n\n1. Justify rigorously that the model is not observable in the sense that the path-space laws of the observation process conditional on $X=1$ and $X=-1$ coincide, that is,\n$$\n\\mathcal{L}\\big((Y_s)_{0 \\le s \\le t} \\,\\big|\\, X=1\\big) \\;=\\; \\mathcal{L}\\big((Y_s)_{0 \\le s \\le t} \\,\\big|\\, X=-1\\big) \\quad \\text{for every } t \\ge 0.\n$$\n\n2. Using the above, compute the posterior process $\\pi_t$ explicitly as a function of $t$ and the observation $\\mathcal{Y}_t$.\n\n3. Compute the limit $\\lim_{t \\to \\infty} \\pi_t$ and explain in what precise sense the posterior probabilities fail to converge to the true state even as $t \\to \\infty$.\n\nYour final numerical answer must be the exact value of the limit $\\lim_{t \\to \\infty} \\pi_t$ as a single closed-form number. Do not round.", "solution": "The problem is well-posed and scientifically grounded within the framework of stochastic differential equations and nonlinear filtering theory. We shall proceed with the solution by addressing each part of the problem in sequence.\n\n### Part 1: Justification of Non-Observability\n\nThe problem asks to demonstrate that the model is not observable by showing that the path-space laws of the observation process $Y$, conditional on the two possible values of the hidden state $X$, are identical. Let $\\mathcal{L}(Z)$ denote the law of a stochastic process $Z$. We need to prove that for any $t \\ge 0$:\n$$\n\\mathcal{L}\\big((Y_s)_{0 \\le s \\le t} \\,\\big|\\, X=1\\big) \\;=\\; \\mathcal{L}\\big((Y_s)_{0 \\le s \\le t} \\,\\big|\\, X=-1\\big).\n$$\nThis is equivalent to showing that the laws of the entire processes on $t \\in [0, \\infty)$ are identical, i.e., $\\mathcal{L}(Y \\,|\\, X=1) = \\mathcal{L}(Y \\,|\\, X=-1)$.\n\nLet's analyze the stochastic process $Y$ under each condition.\nThe observation process $Y$ is defined by the stochastic differential equation (SDE):\n$$\ndY_t = X \\, dW_t, \\qquad Y_0 = 0.\n$$\nThe solution to this SDE is given by the Itô integral $Y_t = \\int_0^t X \\, dW_s$. Since $X$ is a random variable that is constant in time (static parameter) and independent of the Brownian motion $W$, we can write this as:\n$$\nY_t = X \\int_0^t dW_s = X W_t.\n$$\n\n**Case 1: Conditional on $X=1$.**\nIf we condition on the event $\\{X=1\\}$, the observation process is given by:\n$$\nY_t = (1) \\cdot W_t = W_t.\n$$\nThus, conditional on $\\{X=1\\}$, the observation process $Y$ is itself a standard one-dimensional Brownian motion. The law of the process $Y$ is the standard Wiener measure on the space of continuous functions starting at $0$.\n\n**Case 2: Conditional on $X=-1$.**\nIf we condition on the event $\\{X=-1\\}$, the observation process is given by:\n$$\nY_t = (-1) \\cdot W_t = -W_t.\n$$\nLet's define a new process $Z_t = -W_t$. We must verify that $Z$ is also a standard Brownian motion. A continuous-time stochastic process $\\{Z_t\\}_{t \\ge 0}$ is a standard Brownian motion if:\n1.  $Z_0 = 0$.\n2.  The process has independent increments.\n3.  For any $s < t$, the increment $Z_t - Z_s$ is normally distributed with mean $0$ and variance $t-s$.\n4.  The sample paths $t \\mapsto Z_t$ are continuous almost surely.\n\nLet's check these properties for $Z_t = -W_t$:\n1.  $Z_0 = -W_0 = 0$, since $W_0=0$.\n2.  For any sequence of times $0 \\le t_1 < t_2 < \\dots < t_n$, the increments of $Z$ are $Z_{t_{i+1}} - Z_{t_i} = -W_{t_{i+1}} - (-W_{t_i}) = -(W_{t_{i+1}} - W_{t_i})$. Since the increments of $W$ are independent, the increments of $Z$ are also independent.\n3.  The increment $Z_t - Z_s = -(W_t - W_s)$. Since $W_t - W_s \\sim \\mathcal{N}(0, t-s)$, its negative, $-(W_t - W_s)$, is also normally distributed with mean $\\mathbb{E}[-(W_t - W_s)] = -\\mathbb{E}[W_t - W_s] = 0$ and variance $\\mathrm{Var}[-(W_t - W_s)] = (-1)^2 \\mathrm{Var}[W_t - W_s] = t-s$. So, $Z_t - Z_s \\sim \\mathcal{N}(0, t-s)$.\n4.  Since the sample paths of $W$ are continuous a.s., the paths of $Z=-W$ are also continuous a.s.\n\nTherefore, the process $Z_t = -W_t$ is a standard one-dimensional Brownian motion. This is a fundamental symmetry property of Brownian motion.\n\n**Conclusion:**\nThe law of the process $Y$ conditional on $\\{X=1\\}$ is the law of a standard Brownian motion, $\\mathcal{L}(W)$. The law of the process $Y$ conditional on $\\{X=-1\\}$ is the law of the process $-W$, which is also a standard Brownian motion, so its law is also $\\mathcal{L}(W)$.\nThus, we have rigorously established that:\n$$\n\\mathcal{L}(Y \\,|\\, X=1) \\;=\\; \\mathcal{L}(Y \\,|\\, X=-1).\n$$\nThis implies that for any time horizon $t \\ge 0$, the laws of the path segments $(Y_s)_{0 \\le s \\le t}$ are also identical, which proves the non-observability of the system. The observation data $(Y_s)_{0 \\le s \\le t}$ provides no statistical information to distinguish between the two possible states of $X$.\n\n### Part 2: Computation of the Posterior Process $\\pi_t$\n\nThe posterior process, or a filter, is defined as $\\pi_t := \\mathbb{P}(X=1 \\,|\\, \\mathcal{Y}_t)$. This is the conditional expectation of the indicator function $\\mathbb{1}_{\\{X=1\\}}$ with respect to the observation filtration $\\mathcal{Y}_t$. We can compute this using the abstract Bayes' rule.\n\nLet $\\mathbf{P}_1$ and $\\mathbf{P}_{-1}$ be the probability measures on the path space of $Y$ induced by the conditions $X=1$ and $X=-1$, respectively. From Part 1, we know $\\mathbf{P}_1 = \\mathbf{P}_{-1}$. Let us denote this common measure by $\\mathbf{W}$, the Wiener measure.\n\nThe filter $\\pi_t$ can be expressed using the prior probabilities and the likelihood of the observations up to time $t$. Let the prior probability be $\\pi_0 = \\mathbb{P}(X=1) = \\frac{1}{2}$. For any event $A \\in \\mathcal{Y}_t$, the definition of conditional probability states:\n$$\n\\mathbb{P}(\\{X=1\\} \\cap A) = \\int_A \\pi_t \\, d\\mathbb{P}.\n$$\nWe can express the probabilities on the left and right in terms of the conditional measures.\n$$\n\\mathbb{P}(\\{X=1\\} \\cap A) = \\mathbb{P}(A | X=1) \\mathbb{P}(X=1) = \\frac{1}{2} \\mathbf{P}_1(A')\n$$\nwhere $A'$ is the set of paths in the path space corresponding to the event $A \\in \\mathcal{Y}_t$.\nThe total probability of event $A$ is:\n$$\n\\mathbb{P}(A) = \\mathbb{P}(A|X=1)\\mathbb{P}(X=1) + \\mathbb{P}(A|X=-1)\\mathbb{P}(X=-1)\n$$\n$$\n\\mathbb{P}(A) = \\frac{1}{2}\\mathbf{P}_1(A') + \\frac{1}{2}\\mathbf{P}_{-1}(A').\n$$\nUsing the result from Part 1, $\\mathbf{P}_1 = \\mathbf{P}_{-1} = \\mathbf{W}$, we have:\n$$\n\\mathbb{P}(A) = \\frac{1}{2}\\mathbf{W}(A') + \\frac{1}{2}\\mathbf{W}(A') = \\mathbf{W}(A').\n$$\nAnd for the joint event:\n$$\n\\mathbb{P}(\\{X=1\\} \\cap A) = \\frac{1}{2}\\mathbf{P}_1(A') = \\frac{1}{2}\\mathbf{W}(A').\n$$\nSubstituting these back into the integral definition of conditional expectation:\n$$\n\\int_A \\pi_t \\, d\\mathbb{P} = \\frac{1}{2}\\mathbf{W}(A').\n$$\nAlso, we have that $\\int_A \\frac{1}{2} d\\mathbb{P} = \\frac{1}{2}\\mathbb{P}(A) = \\frac{1}{2}\\mathbf{W}(A')$.\nThus, for any $A \\in \\mathcal{Y}_t$, we have shown that:\n$$\n\\int_A \\pi_t \\, d\\mathbb{P} = \\int_A \\frac{1}{2} \\, d\\mathbb{P}.\n$$\nBy the uniqueness of conditional expectation (up to almost sure equality), we must have:\n$$\n\\pi_t = \\frac{1}{2} \\quad \\text{a.s. for every } t \\ge 0.\n$$\nThe posterior probability remains equal to the prior probability for all time. The observations yield no new information to update our belief about the state of $X$.\n\nAlternatively, using the concept of Radon-Nikodym derivatives on path space, let the overall law of the observation process be $\\mathbf{P}_Y$. Then $\\mathbf{P}_Y = \\pi_0 \\mathbf{P}_1 + (1-\\pi_0) \\mathbf{P}_{-1}$. Since $\\mathbf{P}_1 = \\mathbf{P}_{-1} = \\mathbf{W}$ and $\\pi_0 = 1/2$, we get $\\mathbf{P}_Y = \\frac{1}{2}\\mathbf{W} + \\frac{1}{2}\\mathbf{W} = \\mathbf{W}$. The Radon-Nikodym derivatives of the conditional measures with respect to the reference measure $\\mathbf{P}_Y$ are:\n$$\nL_1 = \\frac{d\\mathbf{P}_1}{d\\mathbf{P}_Y} = \\frac{d\\mathbf{W}}{d\\mathbf{W}} = 1.\n$$\n$$\nL_{-1} = \\frac{d\\mathbf{P}_{-1}}{d\\mathbf{P}_Y} = \\frac{d\\mathbf{W}}{d\\mathbf{W}} = 1.\n$$\nThe Kallianpur-Striebel formula gives the filter as:\n$$\n\\pi_t = \\frac{\\pi_0 L_1}{\\pi_0 L_1 + (1-\\pi_0) L_{-1}}.\n$$\nSubstituting the values:\n$$\n\\pi_t = \\frac{\\frac{1}{2} \\cdot 1}{\\frac{1}{2} \\cdot 1 + (1-\\frac{1}{2}) \\cdot 1} = \\frac{\\frac{1}{2}}{\\frac{1}{2} + \\frac{1}{2}} = \\frac{1}{2}.\n$$\nThis confirms that the posterior process is constant and equal to its initial value.\n\n### Part 3: Limit as $t \\to \\infty$ and Failure of Convergence\n\nFrom Part 2, the posterior process is $\\pi_t = \\frac{1}{2}$ for all $t \\ge 0$. The limit is therefore trivial:\n$$\n\\lim_{t \\to \\infty} \\pi_t = \\lim_{t \\to \\infty} \\frac{1}{2} = \\frac{1}{2}.\n$$\nThe question asks to explain the failure of the posterior probabilities to converge to the true state. The \"true state\" is the realized value of the random variable $X$, which is either $1$ or $-1$.\n- If the true state is $X=1$, then convergence to the true state would mean $\\pi_t = \\mathbb{P}(X=1|\\mathcal{Y}_t) \\to 1$ as $t \\to \\infty$.\n- If the true state is $X=-1$, then convergence to the true state would mean $\\pi_t = \\mathbb{P}(X=1|\\mathcal{Y}_t) \\to 0$ as $t \\to \\infty$.\n\nIn this problem, $\\pi_t$ converges to $\\frac{1}{2}$, regardless of the true value of $X$. This represents a complete failure of the filtering process to learn the hidden static parameter. The posterior probability distribution over $\\{X=1, X=-1\\}$ remains $(\\frac{1}{2}, \\frac{1}{2})$ forever, which is identical to the prior distribution.\n\nThe precise reason for this failure lies in the non-observability established in Part 1. The path-space measures $\\mathbf{P}_1$ and $\\mathbf{P}_{-1}$ are not merely equivalent (mutually absolutely continuous), but are identical. In typical filtering problems where learning occurs, the measures are equivalent but not identical. The Girsanov theorem provides a non-trivial Radon-Nikodym derivative (likelihood ratio) that evolves over time. As more data is observed, this likelihood ratio tends to either $0$ or $\\infty$, which forces the posterior probability to converge to $0$ or $1$.\n\nIn this specific model, the likelihood ratio is identically $1$ for all time. This signifies that the observations are statistically indistinguishable under the two hypotheses ($X=1$ and $X=-1$). Consequently, no amount of observation data can help in discerning the true state, and the posterior belief never updates from the prior. The uncertainty about $X$ remains maximal throughout the process.", "answer": "$$\\boxed{\\frac{1}{2}}$$", "id": "2988870"}]}