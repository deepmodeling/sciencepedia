{"hands_on_practices": [{"introduction": "We begin with a classic exit-time problem. Imagine controlling a diffusing particle within a bounded interval with the goal of maximizing its exit position. This exercise [@problem_id:2998163] requires formulating and solving the stationary Hamilton-Jacobi-Bellman (HJB) equation, which takes the form of a nonlinear ordinary differential equation with absorbing boundary conditions. Mastering this type of problem is fundamental for tackling a wide range of applications in optimal stopping and financial engineering, such as pricing American options.", "problem": "Consider the controlled one-dimensional stochastic differential equation on the open interval $(0,1)$,\n$$\ndX_{t} \\;=\\; a_{t}\\,dt \\;+\\; \\sigma\\,dW_{t}, \\qquad X_{0}=x \\in (0,1),\n$$\nwhere $\\sigma>0$ is a given constant, $W_{t}$ is a standard Brownian motion, and the control process $a_{t}$ is progressively measurable with values in the compact set $A=[-1,1]$. Let the system be absorbed at the boundary: define the exit time\n$$\n\\tau \\;=\\; \\inf\\{t\\ge 0 \\,:\\, X_{t} \\notin (0,1)\\},\n$$\nand the absorbed process $X_{t\\wedge\\tau}$. The performance criterion is an exit-time objective with zero running cost and terminal reward equal to location, namely $f \\equiv 0$ and $h(x)=x$, so that for an admissible control $a=(a_{t})_{t\\ge 0}$,\n$$\nJ^{a}(x) \\;=\\; \\mathbb{E}_{x}\\!\\left[\\,h\\!\\left(X_{\\tau}\\right)\\right] \\;=\\; \\mathbb{E}_{x}\\!\\left[X_{\\tau}\\right],\n$$\nand the value function is\n$$\nV(x) \\;=\\; \\sup_{a}\\, J^{a}(x), \\qquad x\\in(0,1),\n$$\nwith absorbing boundary conditions $V(0)=h(0)=0$ and $V(1)=h(1)=1$.\n\nStarting from the dynamic programming principle for exit-time problems and the infinitesimal generator of the controlled diffusion, derive the associated stationary Hamilton–Jacobi–Bellman (HJB) equation on $(0,1)$ and solve it explicitly under the above boundary conditions. Your final answer must be a single closed-form analytic expression for the value function $V(x)$ in terms of $x$ and $\\sigma$.", "solution": "We begin from the dynamic programming principle for exit-time problems in stochastic control. For a sufficiently smooth candidate value function $V$, the principle implies that $V$ should satisfy, in the viscosity sense and in the classical sense if $V\\in C^{2}((0,1))$, the stationary Hamilton–Jacobi–Bellman (HJB) equation\n$$\n0 \\;=\\; \\sup_{a\\in A} \\left\\{ \\mathcal{L}^{a} V(x) \\right\\}, \\qquad x\\in(0,1),\n$$\nsubject to the absorbing boundary conditions $V(0)=0$ and $V(1)=1$. Here $\\mathcal{L}^{a}$ is the infinitesimal generator of the controlled diffusion,\n$$\n\\mathcal{L}^{a} \\phi(x) \\;=\\; a\\,\\phi'(x) \\;+\\; \\frac{\\sigma^{2}}{2}\\,\\phi''(x).\n$$\nPlugging this into the HJB yields the second-order ordinary differential equation (elliptic HJB in one dimension),\n$$\n0 \\;=\\; \\sup_{a\\in[-1,1]} \\left\\{ a\\,V'(x) \\;+\\; \\frac{\\sigma^{2}}{2}\\,V''(x) \\right\\}, \\qquad x\\in(0,1),\n$$\nwith $V(0)=0$ and $V(1)=1$.\n\nThe supremum over $a\\in[-1,1]$ of the linear expression $a\\,V'(x)$ is given by\n$$\n\\sup_{a\\in[-1,1]} a\\,V'(x) \\;=\\; |V'(x)|.\n$$\nThus the HJB can be written as\n$$\n\\frac{\\sigma^{2}}{2}\\,V''(x) \\;+\\; |V'(x)| \\;=\\; 0, \\qquad x\\in(0,1),\n$$\nwith $V(0)=0$ and $V(1)=1$.\n\nWe will solve this explicitly. Anticipating that the optimal strategy will push the state upward (toward $x=1$) due to the monotone terminal reward $h(x)=x$, it is natural to seek a solution $V$ that is nondecreasing. Suppose $V'(x)\\ge 0$ on $(0,1)$. Then $|V'(x)|=V'(x)$ and the HJB reduces to the linear ordinary differential equation\n$$\n\\frac{\\sigma^{2}}{2}\\,V''(x) \\;+\\; V'(x) \\;=\\; 0.\n$$\nLet $Y(x)=V'(x)$. Then\n$$\n\\frac{\\sigma^{2}}{2}\\,Y'(x) \\;+\\; Y(x) \\;=\\; 0,\n$$\nwhich is a first-order linear equation. Solving this gives\n$$\nY'(x) \\;=\\; -\\frac{2}{\\sigma^{2}}\\,Y(x) \\quad \\Longrightarrow \\quad Y(x) \\;=\\; C\\,\\exp\\!\\left(-\\frac{2x}{\\sigma^{2}}\\right),\n$$\nfor some constant $C$. Integrating $Y$ yields\n$$\nV(x) \\;=\\; \\int_{0}^{x} Y(s)\\,ds \\;+\\; V(0) \\;=\\; \\int_{0}^{x} C\\,\\exp\\!\\left(-\\frac{2s}{\\sigma^{2}}\\right)\\,ds \\;+\\; 0 \\;=\\; \\frac{C\\,\\sigma^{2}}{2}\\left(1 - \\exp\\!\\left(-\\frac{2x}{\\sigma^{2}}\\right)\\right).\n$$\nImposing the boundary condition at $x=1$, we obtain\n$$\nV(1) \\;=\\; \\frac{C\\,\\sigma^{2}}{2}\\left(1 - \\exp\\!\\left(-\\frac{2}{\\sigma^{2}}\\right)\\right) \\;=\\; 1,\n$$\nso\n$$\nC \\;=\\; \\frac{2}{\\sigma^{2}\\left(1 - \\exp\\!\\left(-\\frac{2}{\\sigma^{2}}\\right)\\right)}.\n$$\nSubstituting $C$ back into $V(x)$ gives the explicit expression\n$$\nV(x) \\;=\\; \\frac{1 - \\exp\\!\\left(-\\frac{2x}{\\sigma^{2}}\\right)}{1 - \\exp\\!\\left(-\\frac{2}{\\sigma^{2}}\\right)}, \\qquad x\\in[0,1].\n$$\nWe verify that $V(0)=0$ and $V(1)=1$, and that $V'(x)=\\dfrac{\\frac{2}{\\sigma^{2}}\\exp\\!\\left(-\\frac{2x}{\\sigma^{2}}\\right)}{1 - \\exp\\!\\left(-\\frac{2}{\\sigma^{2}}\\right)} \\ge 0$ for $x\\in(0,1)$, so our initial monotonicity assumption is satisfied. Consequently, the Hamiltonian maximizer is $a^{*}(x)=1$ for all $x\\in(0,1)$.\n\nTo verify optimality, apply Itô's formula to $V(X_{t\\wedge\\tau})$ under any admissible control $a_{t}$:\n$$\nV(X_{t\\wedge\\tau}) \\;=\\; V(x) \\;+\\; \\int_{0}^{t\\wedge\\tau} \\left(a_{s}\\,V'(X_{s}) + \\frac{\\sigma^{2}}{2}\\,V''(X_{s})\\right)ds \\;+\\; \\int_{0}^{t\\wedge\\tau} \\sigma\\,V'(X_{s})\\,dW_{s}.\n$$\nBy the HJB, $\\sup_{a} \\{ a\\,V'(x) + \\frac{\\sigma^{2}}{2}V''(x) \\}=0$, so for any $a_{t}$ we have $a_{t}\\,V'(X_{t}) + \\frac{\\sigma^{2}}{2}V''(X_{t}) \\le 0$, making $V(X_{t\\wedge\\tau})$ a supermartingale. Taking expectations and letting $t\\to\\infty$ yields\n$$\nV(x) \\;\\ge\\; \\mathbb{E}_{x}\\!\\left[V\\!\\left(X_{\\tau}\\right)\\right] \\;=\\; \\mathbb{E}_{x}\\!\\left[h\\!\\left(X_{\\tau}\\right)\\right] \\;=\\; J^{a}(x),\n$$\nfor every admissible $a$. For the feedback control $a^{*}(x)=1$, the inequality becomes equality because $a^{*}\\,V'(x) + \\frac{\\sigma^{2}}{2}V''(x)=0$ pointwise, and $V(X_{t\\wedge\\tau})$ is a martingale. Hence $V$ is indeed the value function and $a^{*}(x)=1$ is optimal.\n\nTherefore, the associated elliptic HJB is solved explicitly by\n$$\nV(x) \\;=\\; \\frac{1 - \\exp\\!\\left(-\\frac{2x}{\\sigma^{2}}\\right)}{1 - \\exp\\!\\left(-\\frac{2}{\\sigma^{2}}\\right)}.\n$$", "answer": "$$\\boxed{\\frac{1 - \\exp\\!\\left(-\\frac{2x}{\\sigma^{2}}\\right)}{1 - \\exp\\!\\left(-\\frac{2}{\\sigma^{2}}\\right)}}$$", "id": "2998163"}, {"introduction": "Optimal control theory provides powerful tools, but it's essential to understand their underlying assumptions and limitations. This next practice [@problem_id:2998136] presents a deterministic control problem with a non-convex terminal cost, creating a scenario where necessary conditions for optimality are not sufficient. By comparing the results from the Pontryagin Maximum Principle with a direct analysis based on the dynamic programming principle, you will discover firsthand why one method finds a suboptimal local extremum while the other identifies the true global optimum, reinforcing the critical role of convexity.", "problem": "Consider the controlled stochastic differential equation (specialized here to a degenerate diffusion with zero diffusion coefficient) on the finite horizon $[0,1]$,\n$$\n\\mathrm{d}x_t \\;=\\; u_t\\,\\mathrm{d}t,\\qquad x_0 \\;=\\; 0,\n$$\nwith admissible controls satisfying $u_t \\in [-1,1]$ and the cost functional\n$$\nJ(u) \\;=\\; \\phi\\!\\big(x_1\\big),\\qquad \\phi(y) \\;=\\; \\big(y^2 - 1\\big)^2.\n$$\nYou will use both the stochastic maximum principle (SMP), also known as the Pontryagin Maximum Principle, and dynamic programming to analyze this problem. The terminal cost $\\phi$ is nonconvex, and there is no running cost.\n\nTasks:\n1. Starting from the fundamental definitions of optimal control and the Pontryagin Maximum Principle (SMP), derive the necessary conditions for optimality for this problem, explicitly identifying the Hamiltonian and the adjoint (co-state) dynamics.\n2. Show that the control $u_t \\equiv 0$ is a candidate extremal according to the SMP necessary conditions, and compute its cost $J(u)$.\n3. Using the dynamic programming principle and reachable-set reasoning from first principles, determine the exact optimal value $J^{\\star} := \\inf_{u} J(u)$ and identify a control that attains it.\n\nFinally, provide the exact numerical value of the suboptimality gap $J(u^{\\mathrm{MP}}) - J^{\\star}$, where $u^{\\mathrm{MP}}$ denotes the SMP candidate control found in Task 2. Express the final suboptimality gap as an exact real number. No rounding is required.", "solution": "The problem is to find the optimal control for the system governed by the deterministic ordinary differential equation $\\mathrm{d}x_t = u_t\\,\\mathrm{d}t$ with initial condition $x_0 = 0$ on the time horizon $t \\in [0, 1]$. The control $u_t$ is constrained to the set $[-1, 1]$. The objective is to minimize the cost functional $J(u) = \\phi(x_1)$, where the terminal cost function is given by $\\phi(y) = (y^2 - 1)^2$.\n\nThis is a problem in deterministic optimal control, which can be viewed as a special case of stochastic optimal control where the diffusion coefficient is zero. We will analyze this problem using both the Pontryagin Maximum Principle (SMP) and dynamic programming concepts.\n\n**Task 1: Pontryagin Maximum Principle (SMP) Necessary Conditions**\n\nThe Pontryagin Maximum Principle provides necessary conditions for a control $u_t^*$ and its corresponding state trajectory $x_t^*$ to be optimal. We introduce an adjoint (or co-state) variable $p_t$.\n\nThe Hamiltonian for this system is defined as:\n$$\nH(x, u, p) = L(x, u) + p \\cdot f(x, u)\n$$\nwhere $L$ is the running cost and $f$ is the dynamics function. In this problem, the running cost is zero, $L=0$, and the dynamics are $f(x, u) = u$. Thus, the Hamiltonian is:\n$$\nH(x_t, u_t, p_t) = p_t u_t\n$$\nThe state and adjoint equations form a Hamiltonian system. The state equation is given:\n$$\n\\frac{\\mathrm{d}x_t}{\\mathrm{d}t} = \\frac{\\partial H}{\\partial p} = u_t, \\quad x_0 = 0\n$$\nThe adjoint equation for $p_t$ is given by:\n$$\n\\frac{\\mathrm{d}p_t}{\\mathrm{d}t} = -\\frac{\\partial H}{\\partial x} = -\\frac{\\partial}{\\partial x}(p_t u_t) = 0\n$$\nThis implies that the adjoint variable $p_t$ is constant over the time interval $[0, 1]$. Let us denote this constant value by $p$.\n$$\np_t = p \\quad \\forall t \\in [0, 1]\n$$\nThe transversality condition at the terminal time $t=1$ connects the adjoint variable to the terminal cost function $\\phi$:\n$$\np_1 = \\frac{\\mathrm{d}\\phi}{\\mathrm{d}x}(x_1)\n$$\nGiven $\\phi(y) = (y^2 - 1)^2$, its derivative is $\\frac{\\mathrm{d}\\phi}{\\mathrm{d}y} = 2(y^2-1)(2y) = 4y(y^2-1)$.\nTherefore, the transversality condition is:\n$$\np_1 = 4x_1(x_1^2 - 1)\n$$\nSince $p_t$ is constant, we have $p = p_1 = 4x_1(x_1^2 - 1)$.\n\nThe final necessary condition is Pontryagin's maximum principle itself: for an optimal control $u_t^*$, it must maximize the Hamiltonian for almost every $t \\in [0, 1]$:\n$$\nu_t^* \\in \\arg\\max_{v \\in [-1, 1]} H(x_t^*, v, p_t) = \\arg\\max_{v \\in [-1, 1]} (p_t v)\n$$\nThe form of the optimal control therefore depends on the sign of the constant adjoint state $p$:\n- If $p > 0$, then $u_t^* = 1$.\n- If $p < 0$, then $u_t^* = -1$.\n- If $p = 0$, then any control $u_t^* \\in [-1, 1]$ maximizes the Hamiltonian, which is identically zero. This is the singular case.\n\n**Task 2: Analysis of the Candidate Control $u_t \\equiv 0$**\n\nWe examine whether the control $u_t \\equiv 0$ for all $t \\in [0, 1]$ satisfies the necessary conditions derived from the SMP.\n\nFirst, we determine the state trajectory corresponding to this control. Integrating the state equation with $x_0=0$:\n$$\nx_t = x_0 + \\int_0^t u_s\\,\\mathrm{d}s = 0 + \\int_0^t 0\\,\\mathrm{d}s = 0\n$$\nSo, $x_t \\equiv 0$ for all $t \\in [0, 1]$. The terminal state is $x_1 = 0$.\n\nNext, we determine the corresponding adjoint state $p$ using the transversality condition evaluated at $x_1=0$:\n$$\np = p_1 = 4x_1(x_1^2 - 1) = 4(0)(0^2 - 1) = 0\n$$\nSo, the adjoint state corresponding to this trajectory is $p_t \\equiv 0$.\n\nFinally, we check if the control $u_t \\equiv 0$ satisfies the maximum principle condition with $p_t \\equiv 0$. The condition requires that for almost every $t$,\n$$\nu_t \\in \\arg\\max_{v \\in [-1, 1]} (p_t v)\n$$\nSubstituting $p_t = 0$ and $u_t = 0$:\n$$\n0 \\in \\arg\\max_{v \\in [-1, 1]} (0 \\cdot v)\n$$\nThe expression to be maximized is $0 \\cdot v = 0$, which is constant for all $v \\in [-1, 1]$. Thus, any control $v \\in [-1, 1]$ is a maximizer. Since $0 \\in [-1, 1]$, the choice $u_t \\equiv 0$ is valid.\nTherefore, the control $u_t \\equiv 0$ is a candidate extremal trajectory satisfying the necessary conditions of the Pontryagin Maximum Principle.\n\nThe cost associated with this control, denoted as $u^{\\mathrm{MP}}$ in the problem statement, is:\n$$\nJ(u^{\\mathrm{MP}}) = J(0) = \\phi(x_1) = \\phi(0) = (0^2 - 1)^2 = (-1)^2 = 1\n$$\n\n**Task 3: Optimal Value via Dynamic Programming and Reachable Set Analysis**\n\nThe principle of dynamic programming implies that the optimal cost, $J^\\star$, is the minimum possible value of the cost functional.\n$$\nJ^\\star = \\inf_{u(\\cdot)} J(u) = \\inf_{u(\\cdot)} \\phi(x_1)\n$$\nTo find this infimum, we first determine the set of all possible values for the terminal state $x_1$. This is known as the reachable set at time $t=1$.\nThe state at time $t=1$ is given by the integral of the control:\n$$\nx_1 = \\int_0^1 u_t\\,\\mathrm{d}t\n$$\nGiven the constraint $u_t \\in [-1, 1]$, we can bound the integral:\n$$\n\\int_0^1 (-1)\\,\\mathrm{d}t \\le \\int_0^1 u_t\\,\\mathrm{d}t \\le \\int_0^1 (1)\\,\\mathrm{d}t\n$$\nThis gives:\n$$\n-1 \\le x_1 \\le 1\n$$\nThe reachable set for $x_1$ is the closed interval $[-1, 1]$. Any value $y \\in [-1, 1]$ is reachable, for example by using the constant control $u_t = y$, which is admissible since $y \\in [-1, 1]$.\n\nThe optimization problem is now reduced to a simple minimization of a function of one variable:\n$$\nJ^\\star = \\min_{y \\in [-1, 1]} \\phi(y) = \\min_{y \\in [-1, 1]} (y^2 - 1)^2\n$$\nThe function $\\phi(y) = (y^2 - 1)^2$ is non-negative for all real $y$. Its value is zero if and only if $y^2 - 1 = 0$, which occurs at $y = 1$ and $y = -1$.\nBoth of these values, $1$ and $-1$, lie within the reachable set $[-1, 1]$.\nTherefore, the minimum value of $\\phi(y)$ on this interval is $0$.\n$$\nJ^\\star = 0\n$$\nThis minimum value is attained. An optimal control must produce a terminal state of $x_1=1$ or $x_1=-1$.\n- To achieve $x_1=1$, we can use the constant control $u_t \\equiv 1$ for $t \\in [0, 1]$. This is an admissible control, and $x_1 = \\int_0^1 1\\,\\mathrm{d}t = 1$. The cost is $J(1) = (1^2 - 1)^2 = 0$.\n- To achieve $x_1=-1$, we can use the constant control $u_t \\equiv -1$ for $t \\in [0, 1]$. This gives $x_1 = \\int_0^1 (-1)\\,\\mathrm{d}t = -1$. The cost is $J(-1) = ((-1)^2 - 1)^2 = 0$.\n\nThus, the exact optimal value is $J^\\star = 0$. The non-convexity of the cost function $\\phi(y)$ is key: it has a local maximum at $y=0$ which corresponds to the suboptimal extremal found by the SMP, and global minima at $y=\\pm 1$ which correspond to the true optimal solutions.\n\n**Suboptimality Gap Calculation**\n\nThe problem asks for the suboptimality gap $J(u^{\\mathrm{MP}}) - J^\\star$, where $u^{\\mathrm{MP}}$ is the SMP candidate control from Task 2.\nFrom Task 2, the candidate control is $u^{\\mathrm{MP}}_t \\equiv 0$, and its corresponding cost is:\n$$\nJ(u^{\\mathrm{MP}}) = 1\n$$\nFrom Task 3, the true optimal value is:\n$$\nJ^\\star = 0\n$$\nThe suboptimality gap is the difference between these two values:\n$$\nJ(u^{\\mathrm{MP}}) - J^\\star = 1 - 0 = 1\n$$\nThis gap highlights that the necessary conditions from the Pontryagin Maximum Principle are not always sufficient for global optimality, especially in problems with non-convex cost functions. The SMP identified a trajectory that is locally stationary but globally suboptimal.", "answer": "$$\\boxed{1}$$", "id": "2998136"}, {"introduction": "Our final practice explores the dramatic phenomenon of finite-time explosion. Here, you will encounter a system whose state is guaranteed to diverge to infinity, and your task is to choose a control that postpones this inevitable outcome for as long as possible. This problem [@problem_id:2998178] demonstrates how to apply the HJB framework to unbounded domains and deal with singular events by formulating the problem as a minimization of the expected explosion time. This exercise provides valuable insight into the control of unstable systems and the role of appropriate boundary conditions at infinity.", "problem": "Consider the controlled stochastic differential equation on the state space $(0,\\infty)$\n$$\ndX_t \\;=\\; b(X_t,u_t)\\,dt \\;+\\; \\sigma(X_t,u_t)\\,dW_t,\\qquad X_0 \\;=\\; x_0 \\;>\\; 0,\n$$\nwhere $(W_t)_{t\\ge 0}$ is a one-dimensional standard Brownian motion, the control $(u_t)_{t\\ge 0}$ is progressively measurable with respect to the filtration generated by $(W_t)_{t\\ge 0}$ and takes values in the compact set $[0,U]$ for a fixed $U>0$, and the coefficients are given by\n$$\nb(x,u) \\;=\\; (1+u)\\,x^2,\\qquad \\sigma(x,u) \\;\\equiv\\; 0.\n$$\nThe coefficients are continuous and locally Lipschitz in $x$ on every bounded interval but do not satisfy a global linear growth bound. Let the explosion time $\\tau$ be the maximal time of existence of the unique strong solution and, if $\\tau<\\infty$, assume $\\lim_{t\\uparrow \\tau} X_t=+\\infty$. Define the value function for the time-to-explosion minimization problem\n$$\nV(x) \\;:=\\; \\inf_{u\\in\\mathcal{U}} \\mathbb{E}\\!\\left[\\int_0^{\\tau} 1\\,ds \\,\\Big|\\, X_0=x\\right],\n$$\nwhere $\\mathcal{U}$ is the set of all admissible controls with values in $[0,U]$.\n\nUsing only the fundamental definitions of existence and uniqueness up to explosion for locally Lipschitz coefficients and the dynamic programming principle, derive the Hamilton–Jacobi–Bellman equation satisfied by $V$ on $(0,\\infty)$ and solve it under the natural boundary condition at $+\\infty$ associated with explosion. Conclude by constructing an explicit admissible control that yields explosion in finite time and compute the minimal expected explosion time starting from $x_0>0$ as a closed-form analytic expression in terms of $x_0$ and $U$.\n\nYour final answer must be this explicit expression for $V(x_0)$, written in simplest form. No rounding is required and no units are involved.", "solution": "The problem is to find the minimal expected time to explosion for a controlled process. We begin by validating the problem statement.\n\nThe state dynamics are given by the stochastic differential equation (SDE):\n$$\ndX_t = b(X_t,u_t)\\,dt + \\sigma(X_t,u_t)\\,dW_t\n$$\nwith initial condition $X_0 = x_0 > 0$. The control $u_t$ takes values in the compact set $[0,U]$ for some $U>0$. The coefficients are specified as:\n$$\nb(x,u) = (1+u)x^2, \\qquad \\sigma(x,u) \\equiv 0.\n$$\nThe value function to be minimized is the expected time to explosion, $\\tau$:\n$$\nV(x) := \\inf_{u\\in\\mathcal{U}} \\mathbb{E}\\!\\left[\\tau \\,\\Big|\\, X_0=x\\right] = \\inf_{u\\in\\mathcal{U}} \\mathbb{E}\\!\\left[\\int_0^{\\tau} 1\\,ds \\,\\Big|\\, X_0=x\\right].\n$$\nThe problem is set up as a stochastic optimal control problem. However, since the diffusion coefficient $\\sigma(x,u)$ is identically zero, the SDE degenerates into a deterministic ordinary differential equation (ODE) for any given control process $(u_t)$:\n$$\n\\frac{dX_t}{dt} = (1+u_t)X_t^2.\n$$\nThe control $(u_t)_{t\\ge 0}$ is defined as a progressively measurable process with respect to the filtration generated by a standard Brownian motion $(W_t)_{t\\ge 0}$. This means the control is allowed to be random, depending on the path of $W_t$. Consequently, the state trajectory $X_t$ and the explosion time $\\tau$ can be random variables, and the use of the expectation operator $\\mathbb{E}$ is meaningful. The problem is thus a valid, albeit degenerate, stochastic control problem. The coefficients are locally Lipschitz but do not satisfy a global linear growth condition, which is consistent with the possibility of finite-time explosion. The problem is well-posed.\n\nTo solve this problem, we apply the dynamic programming principle, which states that the value function $V(x)$ must satisfy the Hamilton-Jacobi-Bellman (HJB) equation. For a time-to-exit problem, the general form of the HJB equation is:\n$$\n\\inf_{u \\in [0,U]} \\left\\{ \\mathcal{L}^u V(x) + c(x,u) \\right\\} = 0,\n$$\nwhere $\\mathcal{L}^u$ is the generator of the process under control $u$, and $c(x,u)$ is the running cost. In this case, the running cost is $c(x,u) = 1$. The generator $\\mathcal{L}^u$ is given by:\n$$\n\\mathcal{L}^u \\phi(x) = b(x,u) \\frac{d\\phi}{dx}(x) + \\frac{1}{2}\\sigma(x,u)^2 \\frac{d^2\\phi}{dx^2}(x).\n$$\nSubstituting the given coefficients $b(x,u) = (1+u)x^2$ and $\\sigma(x,u)=0$, the generator simplifies to:\n$$\n\\mathcal{L}^u V(x) = (1+u)x^2 V'(x).\n$$\nThe HJB equation for this specific problem is therefore:\n$$\n\\inf_{u \\in [0,U]} \\left\\{ (1+u)x^2 V'(x) + 1 \\right\\} = 0, \\quad \\text{for } x \\in (0, \\infty).\n$$\nThe expression to be minimized is linear in the control variable $u$. Let's analyze the term that multiplies $u$, which is $x^2 V'(x)$. Since the state $x$ is in $(0, \\infty)$, $x^2$ is always positive. The sign of the term is determined by the sign of $V'(x)$. The value function $V(x)$ represents the minimum time to reach $+\\infty$ starting from $x$. Intuitively, a larger starting value of $x$ is \"closer\" to the target state of infinity, and since the dynamics $dX_t/dt = (1+u_t)X_t^2 \\ge 0$ always push the state towards larger values, the time to explosion should be a decreasing function of the initial state. Thus, we must have $V'(x) < 0$.\n\nSince $x^2 > 0$ and we anticipate $V'(x) < 0$, the coefficient of $u$, namely $x^2 V'(x)$, is negative. To achieve the infimum (i.e., to make the expression in the curly braces as small as possible), we must choose the control $u$ to be as large as possible. The control set is $[0, U]$, so the optimal control is a constant:\n$$\nu^* = U.\n$$\nThis optimal control is a deterministic feedback law, $u^*(x) = U$, which does not depend on the state $x$ or the external randomness from the Brownian motion $W_t$.\n\nSubstituting $u=U$ back into the HJB equation, the infimum is attained, and we obtain a first-order ODE for $V(x)$:\n$$\n(1+U)x^2 V'(x) + 1 = 0.\n$$\nWe can solve this ODE by separating variables:\n$$\nV'(x) = -\\frac{1}{(1+U)x^2}.\n$$\nIntegrating with respect to $x$ yields:\n$$\nV(x) = \\int -\\frac{1}{(1+U)x^2} dx = \\frac{1}{(1+U)x} + C,\n$$\nwhere $C$ is the constant of integration. To determine $C$, we use the \"natural boundary condition at $+\\infty$ associated with explosion\". For a problem of minimizing the time to reach a boundary, the value function on the boundary itself is zero. Here, the \"boundary\" is $+\\infty$. Thus, we impose the condition:\n$$\n\\lim_{x \\to +\\infty} V(x) = 0.\n$$\nApplying this to our solution:\n$$\n\\lim_{x \\to +\\infty} \\left( \\frac{1}{(1+U)x} + C \\right) = 0 + C.\n$$\nThis implies $C=0$. Therefore, the solution to the HJB equation, which is the value function, is:\n$$\nV(x) = \\frac{1}{(1+U)x}.\n$$\nThe problem asks for an explicit admissible control and the minimal expected explosion time starting from $x_0 > 0$. The derived value function gives this minimal time as $V(x_0)$. To confirm this, we construct the explicit control and compute the corresponding explosion time. The optimal control is $u_t = U$ for all $t \\ge 0$. This is an admissible control (it is constant, hence progressively measurable with respect to any filtration). With this control, the dynamics become the deterministic ODE:\n$$\n\\frac{dX_t}{dt} = (1+U)X_t^2, \\quad X_0=x_0.\n$$\nThis is a separable equation. We can solve for the time $\\tau$ it takes for $X_t$ to go from $x_0$ to $+\\infty$:\n$$\n\\frac{dX_t}{X_t^2} = (1+U)dt.\n$$\nIntegrating from $t=0$ to the explosion time $\\tau$:\n$$\n\\int_{X_0}^{X_\\tau} \\frac{1}{X^2} dX = \\int_0^\\tau (1+U) ds.\n$$\nBy definition of explosion, $X_\\tau \\to +\\infty$. Evaluating the integrals:\n$$\n\\left[-\\frac{1}{X}\\right]_{x_0}^{+\\infty} = (1+U)\\tau.\n$$\n$$\n\\left( \\lim_{X\\to+\\infty} -\\frac{1}{X} \\right) - \\left(-\\frac{1}{x_0}\\right) = (1+U)\\tau.\n$$\n$$\n0 + \\frac{1}{x_0} = (1+U)\\tau.\n$$\nSolving for $\\tau$, we find the explosion time for this control:\n$$\n\\tau = \\frac{1}{(1+U)x_0}.\n$$\nSince the control $u_t=U$ is deterministic, the trajectory and the explosion time $\\tau$ are also deterministic. Thus, the expected explosion time is simply $\\tau$ itself: $\\mathbb{E}[\\tau] = \\frac{1}{(1+U)x_0}$. This matches our value function $V(x_0)$. This confirms that $u_t=U$ is indeed an optimal control and $V(x_0) = \\frac{1}{(1+U)x_0}$ is the minimal expected explosion time.\n\nThe final answer is this expression for $V(x_0)$.", "answer": "$$\\boxed{\\frac{1}{(1+U)x_0}}$$", "id": "2998178"}]}