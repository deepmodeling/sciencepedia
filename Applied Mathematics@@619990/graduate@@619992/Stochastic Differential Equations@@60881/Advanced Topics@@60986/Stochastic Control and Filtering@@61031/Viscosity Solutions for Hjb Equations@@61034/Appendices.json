{"hands_on_practices": [{"introduction": "The theory of viscosity solutions is essential for handling value functions that are continuous but not necessarily differentiable everywhere. This practice uses the canonical example of the absolute value function, $u(x)=|x|$, to provide direct, hands-on experience with the foundational definitions of second-order jets, which replace the classical notion of derivatives. By working through this problem, you will solidify your understanding of how the viscosity framework rigorously evaluates differential operators at points of non-differentiability, forming the very bedrock of the theory. [@problem_id:3005560]", "problem": "Let $u:\\mathbb{R}\\to\\mathbb{R}$ be given by $u(x)=|x|$. Consider the Hamilton–Jacobi–Bellman (HJB) operator $F:\\mathbb{R}\\times\\mathbb{R}\\to\\mathbb{R}$ defined for $p\\in\\mathbb{R}$ and $X\\in\\mathbb{R}$ by\n$$\nF(p,X) \\;=\\; \\max\\{ -X,\\; 1 - |p| \\}.\n$$\nThis operator is degenerate elliptic in the sense of viscosity theory, and it depends only on the first- and second-order jets. Work at the point $x=0$.\n\nUsing only the foundational definitions of second-order semijets and viscosity supersolutions, proceed as follows:\n\n1) Starting from the definition of derivative, explain why $D^2 u(0)$ does not exist.\n\n2) Starting from the definition of the second-order subjet $J^{2,-}u(0)$ (defined by $u(x)\\ge u(0)+p(x-0)+\\tfrac{1}{2}X(x-0)^2 + o(|x|^2)$ as $x\\to 0$), characterize the full set $J^{2,-}u(0)$.\n\n3) Argue why the second-order superjet $J^{2,+}u(0)$ is empty.\n\n4) In the viscosity-supersolution sense for the equation $F(Du,D^2u)=0$, one must check $F(p,X)\\ge 0$ for all $(p,X)\\in J^{2,-}u(0)$. To quantify the sharpness of this inequality at $x=0$, compute the exact value of\n$$\n\\inf_{(p,X)\\in J^{2,-}u(0)} F(p,X).\n$$\n\nYour final answer must be a single real number. No rounding is required.", "solution": "The problem asks for an analysis of the function $u(x)=|x|$ at the point $x=0$ in the context of the viscosity solution framework for the Hamilton-Jacobi-Bellman (HJB) equation governed by the operator $F(p,X) = \\max\\{ -X,\\; 1 - |p| \\}$. We will address each of the four parts of the problem in sequence.\n\nFirst, we address the classical differentiability of $u(x)$ at $x=0$.\nThe first derivative of a function $u$ at a point $x_0$ is defined as $u'(x_0) = \\lim_{h\\to 0} \\frac{u(x_0+h)-u(x_0)}{h}$, provided this limit exists. For $u(x)=|x|$ at $x_0=0$, we examine the one-sided limits:\nThe right-hand limit is:\n$$ \\lim_{h\\to 0^+} \\frac{u(0+h)-u(0)}{h} = \\lim_{h\\to 0^+} \\frac{|h|-|0|}{h} = \\lim_{h\\to 0^+} \\frac{h}{h} = 1 $$\nThe left-hand limit is:\n$$ \\lim_{h\\to 0^-} \\frac{u(0+h)-u(0)}{h} = \\lim_{h\\to 0^-} \\frac{|h|-|0|}{h} = \\lim_{h\\to 0^-} \\frac{-h}{h} = -1 $$\nSince the left-hand and right-hand limits are not equal, the limit does not exist, and thus the first derivative $u'(0)$ is not defined. The second derivative, $D^2u(0)$ or $u''(0)$, is defined as the derivative of the first derivative $u'(x)$ at $x=0$. As $u'(0)$ does not exist, the classical second derivative $D^2u(0)$ cannot exist either.\n\nSecond, we characterize the second-order subjet $J^{2,-}u(0)$. By definition, $(p,X) \\in \\mathbb{R}\\times\\mathbb{R}$ is an element of $J^{2,-}u(0)$ if and only if\n$$ u(y) \\ge u(0) + p(y-0) + \\frac{1}{2}X(y-0)^2 + o(|y|^2) \\quad \\text{as } y \\to 0. $$\nSubstituting $u(y)=|y|$ and $u(0)=0$, this inequality becomes:\n$$ |y| \\ge py + \\frac{1}{2}Xy^2 + o(y^2). $$\nThis is equivalent to the condition that the test function $\\phi(y) = py + \\frac{1}{2}Xy^2$ lies below $u(y)$ near $y=0$ in the specified sense. More formally, this can be stated as:\n$$ \\liminf_{y \\to 0} \\frac{|y| - py - \\frac{1}{2}Xy^2}{y^2} \\ge 0. $$\nTo analyze this, we must check the limits as $y \\to 0^+$ and $y \\to 0^-$.\nFor $y \\to 0^+$:\n$$ \\liminf_{y \\to 0^+} \\frac{y - py - \\frac{1}{2}Xy^2}{y^2} = \\liminf_{y \\to 0^+} \\left( \\frac{1-p}{y} - \\frac{X}{2} \\right) \\ge 0. $$\nIf $p < 1$, then $1-p > 0$, and $\\frac{1-p}{y} \\to +\\infty$ as $y \\to 0^+$. The inequality holds for any $X \\in \\mathbb{R}$.\nIf $p > 1$, then $1-p < 0$, and $\\frac{1-p}{y} \\to -\\infty$. The inequality fails.\nIf $p = 1$, the expression becomes $\\liminf_{y \\to 0^+} (-\\frac{X}{2}) = -\\frac{X}{2}$. The condition is $-\\frac{X}{2} \\ge 0$, which implies $X \\le 0$.\nFor $y \\to 0^-$:\n$$ \\liminf_{y \\to 0^-} \\frac{-y - py - \\frac{1}{2}Xy^2}{y^2} = \\liminf_{y \\to 0^-} \\left( \\frac{-(1+p)}{y} - \\frac{X}{2} \\right) \\ge 0. $$\nLet $y = -h$ where $h \\to 0^+$. The expression is $\\liminf_{h \\to 0^+} (\\frac{1+p}{h} - \\frac{X}{2}) \\ge 0$.\nIf $p > -1$, then $1+p > 0$, and $\\frac{1+p}{h} \\to +\\infty$. The inequality holds for any $X \\in \\mathbb{R}$.\nIf $p < -1$, then $1+p < 0$, and $\\frac{1+p}{h} \\to -\\infty$. The inequality fails.\nIf $p = -1$, the expression is $\\liminf_{h \\to 0^+} (-\\frac{X}{2}) = -\\frac{X}{2}$. The condition is $-\\frac{X}{2} \\ge 0$, which implies $X \\le 0$.\nCombining these conditions: $p$ must be in the interval $[-1, 1]$.\n- If $-1 < p < 1$, both one-sided conditions are met for any $X \\in \\mathbb{R}$.\n- If $p=1$, we must have $X \\le 0$.\n- If $p=-1$, we must have $X \\le 0$.\nSo, the second-order subjet is the set:\n$$ J^{2,-}u(0) = \\{ (p, X) \\in \\mathbb{R} \\times \\mathbb{R} \\mid p \\in (-1, 1), X \\in \\mathbb{R} \\} \\cup \\{ (\\pm 1, X) \\mid X \\le 0 \\}. $$\n\nThird, we argue that the second-order superjet $J^{2,+}u(0)$ is empty. By definition, $(p,X) \\in J^{2,+}u(0)$ if\n$$ u(y) \\le u(0) + py + \\frac{1}{2}Xy^2 + o(|y|^2) \\quad \\text{as } y \\to 0. $$\nFor $u(x)=|x|$ at $x=0$, this is $|y| \\le py + \\frac{1}{2}Xy^2 + o(|y|^2)$. This implies that $|y| - py \\le O(y^2)$.\nDividing by $|y|$ for $y \\ne 0$, we get $1 - p \\frac{y}{|y|} \\le O(|y|)$.\nTaking the limit as $y \\to 0^+$, we have $\\frac{y}{|y|}=1$, so $1 - p \\le 0$, which implies $p \\ge 1$.\nTaking the limit as $y \\to 0^-$, we have $\\frac{y}{|y|}=-1$, so $1 - p(-1) \\le 0$, which is $1+p \\le 0$, implying $p \\le -1$.\nA single real number $p$ cannot satisfy both $p \\ge 1$ and $p \\le -1$. This contradiction shows that no such pair $(p,X)$ exists. Hence, the set $J^{2,+}u(0)$ is empty.\n\nFourth, we compute the infimum of $F(p,X)$ over the set $J^{2,-}u(0)$.\nThe function is $F(p,X) = \\max\\{ -X,\\; 1 - |p| \\}$. The domain is the set $J^{2,-}u(0)$ determined above. We can compute the infimum by considering the two disjoint parts of the domain.\n\nCase 1: $(p,X)$ such that $-1 < p < 1$ and $X \\in \\mathbb{R}$.\nFor a fixed $p \\in (-1,1)$, the quantity $1-|p|$ is a positive constant. Let $C = 1-|p| > 0$. We want to find $\\inf_{X \\in \\mathbb{R}} \\max\\{ -X, C \\}$.\nThe value of $\\max\\{-X, C\\}$ is always greater than or equal to $C$. The infimum is achieved as $-X$ approaches $C$ from below. For any $X$ such that $-X \\le C$ (i.e., $X \\ge -C$), the maximum is $C$. Thus, $\\inf_{X \\in \\mathbb{R}} \\max\\{ -X, C \\} = C = 1-|p|$.\nNow, we take the infimum over $p \\in (-1,1)$:\n$$ \\inf_{p \\in (-1,1)} (1-|p|). $$\nThe function $1-|p|$ approaches its minimum value as $|p|$ approaches $1$. The infimum is $0$, although it is not attained in this part of the domain.\n\nCase 2: $p=\\pm 1$ and $X \\le 0$.\nFor these values of $p$, we have $|p|=1$. The function becomes:\n$$ F(p,X) = \\max\\{ -X, 1 - 1 \\} = \\max\\{ -X, 0 \\}. $$\nWe are given that $X \\le 0$, which implies $-X \\ge 0$.\nTherefore, for this part of the domain, $\\max\\{ -X, 0 \\} = -X$.\nWe want to compute $\\inf_{X \\le 0} (-X)$.\nAs $X$ varies over the interval $(-\\infty, 0]$, the expression $-X$ varies over $[0, \\infty)$.\nThe infimum of the set $[0, \\infty)$ is $0$. This value is attained when $X=0$. The points $(1,0)$ and $(-1,0)$ are in this part of the domain.\n\nCombining the results from both cases, the infimum of $F(p,X)$ over the entire set $J^{2,-}u(0)$ is the minimum of the infima found for each part:\n$$ \\inf_{(p,X)\\in J^{2,-}u(0)} F(p,X) = \\min\\{ 0, 0 \\} = 0. $$\nThis value confirms that $u(x)=|x|$ is a viscosity supersolution of $F(Du,D^2u)=0$, and the inequality $F(p,X) \\ge 0$ is sharp, as its infimum is exactly $0$.", "answer": "$$\\boxed{0}$$", "id": "3005560"}, {"introduction": "This practice connects the abstract formulation of the Hamilton-Jacobi-Bellman (HJB) equation to the underlying controlled stochastic process. It explores the crucial concept of degenerate ellipticity, which arises when the stochastic noise influences the system's state in some directions but not others. By explicitly calculating the second-order term of the HJB operator for rank-deficient diffusion matrices, you will gain a concrete understanding of how the structure of the system's randomness shapes the resulting PDE. [@problem_id:3005553]", "problem": "Consider the controlled Stochastic Differential Equation (SDE)\n$$\n\\mathrm{d}X_{t} = b(X_{t},a_{t})\\,\\mathrm{d}t + \\sigma(X_{t})\\,\\mathrm{d}W_{t},\n$$\nwhere $X_{t} \\in \\mathbb{R}^{n}$ is the state, $a_{t} \\in A$ is the control process taking values in a compact metric set $A$, $b:\\mathbb{R}^{n}\\times A \\to \\mathbb{R}^{n}$ is a drift, and $\\sigma:\\mathbb{R}^{n} \\to \\mathbb{R}^{n \\times m}$ is a diffusion matrix, with $W_{t}$ an $m$-dimensional Brownian Motion (BM). Assume $b$ and $\\sigma$ are bounded and uniformly continuous, and the standard growth and Lipschitz conditions that guarantee existence and uniqueness of strong solutions. Let $u:\\mathbb{R}^{n} \\to \\mathbb{R}$ be the value function for a stationary infinite-horizon discounted control problem with discount $\\lambda>0$ and running cost $L:\\mathbb{R}^{n}\\times A \\to \\mathbb{R}$, and suppose $u$ is a viscosity solution to the stationary Hamilton–Jacobi–Bellman (HJB) partial differential equation (PDE) associated with the above SDE and control.\n\nStarting from the Dynamic Programming Principle (DPP) and Itô’s formula, derive the second-order term appearing in the HJB operator and use this derivation to study the structural consequences when the diffusion covariance matrix $\\sigma(x)\\sigma(x)^{\\top}$ is rank-deficient. Focus on the second-order term at a fixed point $x \\in \\mathbb{R}^{n}$ for a smooth test function $\\varphi$ touching $u$ at $x$, with Hessian $D^{2}\\varphi(x)=M \\in \\mathbb{S}^{n}$ (the space of symmetric $n\\times n$ matrices).\n\nYou are given two concrete, constant diffusion matrices that are rank-deficient:\n\n- Example 1 (three-dimensional state, two-dimensional noise): \n$$\n\\sigma_{(1)} \\in \\mathbb{R}^{3\\times 2}, \\quad \\sigma_{(1)} = \\begin{pmatrix}\n1 & 0 \\\\\n0 & 1 \\\\\n0 & 1\n\\end{pmatrix}.\n$$\n\n- Example 2 (two-dimensional state, one-dimensional noise): \n$$\n\\sigma_{(2)} \\in \\mathbb{R}^{2\\times 1}, \\quad \\sigma_{(2)} = \\begin{pmatrix}\n1 \\\\\n0\n\\end{pmatrix}.\n$$\n\nFor each example, determine the explicit analytic form of the diffusion contribution to the HJB operator at $x$ acting on the Hessian $M=D^{2}\\varphi(x)$, namely the quantity\n$$\n\\frac{1}{2}\\,\\mathrm{Tr}\\!\\left(\\sigma_{(i)}\\sigma_{(i)}^{\\top} \\, M\\right),\n$$\nexpressed directly in terms of the entries of $M$, where $i\\in\\{1,2\\}$ and $M \\in \\mathbb{S}^{n}$ has entries $M_{jk}$ in the corresponding state dimension.\n\nThen, using only fundamental definitions and well-tested facts (Itô’s formula, the Dynamic Programming Principle, and the definition of viscosity solutions), deduce the structural property of the HJB operator in the directions belonging to the kernel of $\\sigma_{(i)}(x)^{\\top}$ for each example; namely, explain what happens to the second-order dependence of the operator along those directions and how this reflects degenerate ellipticity.\n\nYour final answer must be the pair of expressions for the diffusion contribution, one for each example, arranged as a single row matrix using the LaTeX pmatrix environment. No rounding is required. No units are required.", "solution": "We begin by providing a heuristic derivation of the stationary HJB equation from the Dynamic Programming Principle (DPP) to identify the second-order term. Let $u:\\mathbb{R}^{n} \\to \\mathbb{R}$ be the value function for the infinite-horizon discounted cost problem. The DPP states that for any time $t > 0$ and any admissible control process $\\{a_s\\}_{s \\ge 0}$,\n$$\nu(x) = \\inf_{a_{\\cdot}} \\mathbb{E}\\left[ \\int_{0}^{t} e^{-\\lambda s} L(X_s, a_s) \\mathrm{d}s + e^{-\\lambda t} u(X_t) \\bigg| X_0=x \\right].\n$$\nThis implies that for a small time interval $h > 0$ and a constant control $a \\in A$,\n$$\nu(x) \\le \\mathbb{E}\\left[ \\int_{0}^{h} e^{-\\lambda s} L(X_s, a) \\mathrm{d}s + e^{-\\lambda h} u(X_h) \\bigg| X_0=x \\right].\n$$\nAssuming sufficient smoothness of the functions involved, we approximate the integral as $\\int_{0}^{h} e^{-\\lambda s} L(X_s, a) \\mathrm{d}s \\approx L(x,a)h$. To analyze the term $e^{-\\lambda h} u(X_h)$, we apply Itô's formula to the process $f(t,X_t) = e^{-\\lambda t} u(X_t)$. The stochastic differential is:\n$$\n\\mathrm{d}(e^{-\\lambda t} u(X_t)) = \\frac{\\partial}{\\partial t}(e^{-\\lambda t} u(X_t)) \\mathrm{d}t + e^{-\\lambda t} Du(X_t)^{\\top} \\mathrm{d}X_t + \\frac{1}{2} e^{-\\lambda t} \\mathrm{Tr}\\left(D^2u(X_t) (\\mathrm{d}X_t)(\\mathrm{d}X_t)^{\\top}\\right).\n$$\nThe partial derivative with respect to time is $-\\lambda e^{-\\lambda t} u(X_t)$. The quadratic covariation term is $(\\mathrm{d}X_t)(\\mathrm{d}X_t)^{\\top} = \\sigma(X_t)\\sigma(X_t)^{\\top}\\mathrm{d}t$. Substituting the SDE for $\\mathrm{d}X_t$ yields:\n$$\n\\mathrm{d}(e^{-\\lambda t} u(X_t)) = e^{-\\lambda t} \\left[ -\\lambda u(X_t) + Du(X_t)^{\\top}b(X_t,a) + \\frac{1}{2}\\mathrm{Tr}\\left(\\sigma(X_t)\\sigma(X_t)^{\\top}D^2u(X_t)\\right) \\right]\\mathrm{d}t + e^{-\\lambda t}Du(X_t)^{\\top}\\sigma(X_t)\\mathrm{d}W_t.\n$$\nIntegrating from $0$ to $h$ and taking the expectation conditional on $X_0=x$ eliminates the martingale term:\n$$\n\\mathbb{E}[e^{-\\lambda h} u(X_h)] - u(x) \\approx \\left[ -\\lambda u(x) + Du(x)^{\\top}b(x,a) + \\frac{1}{2}\\mathrm{Tr}\\left(\\sigma(x)\\sigma(x)^{\\top}D^2u(x)\\right) \\right]h.\n$$\nSubstituting this and the integral approximation back into the DPP inequality gives:\n$$\nu(x) \\le L(x,a)h + u(x) + \\left[ -\\lambda u(x) + Du(x)^{\\top}b(x,a) + \\frac{1}{2}\\mathrm{Tr}\\left(\\sigma(x)\\sigma(x)^{\\top}D^2u(x)\\right) \\right]h + o(h).\n$$\nRearranging, dividing by $h > 0$, and taking the limit as $h \\to 0$ gives:\n$$\n0 \\le L(x,a) - \\lambda u(x) + Du(x)^{\\top}b(x,a) + \\frac{1}{2}\\mathrm{Tr}\\left(\\sigma(x)\\sigma(x)^{\\top}D^2u(x)\\right).\n$$\nSince this holds for any control $a \\in A$, and with equality for the optimal control, we take the infimum over $a$, leading to the stationary HJB equation:\n$$\n\\lambda u(x) - \\sup_{a \\in A}\\left\\{ -L(x,a) - b(x,a)^{\\top}Du(x) \\right\\} - \\frac{1}{2}\\mathrm{Tr}\\left(\\sigma(x)\\sigma(x)^{\\top}D^2u(x)\\right) = 0.\n$$\nThe value function $u$ is generally not twice differentiable. The robust formulation is via viscosity solutions, where $u$ is compared to smooth test functions $\\varphi$. If $u-\\varphi$ has a local maximum at $x$ ($u$ is touched from above by $\\varphi$), we have $Du(x)=D\\varphi(x)$ and $D^2u(x) \\le D^2\\varphi(x)$ in the matrix sense. The viscosity subsolution definition requires:\n$$\n\\lambda\\varphi(x) - \\sup_{a \\in A}\\left\\{ -L(x,a) - b(x,a)^{\\top}D\\varphi(x) \\right\\} - \\frac{1}{2}\\mathrm{Tr}\\left(\\sigma(x)\\sigma(x)^{\\top}D^2\\varphi(x)\\right) \\le 0.\n$$\nAn analogous inequality holds for supersolutions. The term of interest is the diffusion contribution to the HJB operator, which acts on the Hessian $M=D^2\\varphi(x)$ of the test function. This term is $\\frac{1}{2}\\mathrm{Tr}(\\sigma(x)\\sigma(x)^{\\top}M)$.\n\nWe now analyze this term for the two given examples.\n\nExample 1: The state dimension is $n=3$ and the noise dimension is $m=2$. The diffusion matrix is\n$$\n\\sigma_{(1)} = \\begin{pmatrix}\n1 & 0 \\\\\n0 & 1 \\\\\n0 & 1\n\\end{pmatrix}.\n$$\nThe covariance matrix is $S_{(1)} = \\sigma_{(1)}\\sigma_{(1)}^{\\top}$:\n$$\nS_{(1)} = \\begin{pmatrix}\n1 & 0 \\\\\n0 & 1 \\\\\n0 & 1\n\\end{pmatrix}\n\\begin{pmatrix}\n1 & 0 & 0 \\\\\n0 & 1 & 1\n\\end{pmatrix} = \\begin{pmatrix}\n1 & 0 & 0 \\\\\n0 & 1 & 1 \\\\\n0 & 1 & 1\n\\end{pmatrix}.\n$$\nLet $M \\in \\mathbb{S}^3$ be a symmetric $3 \\times 3$ matrix with entries $M_{jk}$. The diffusion contribution is calculated using the property $\\mathrm{Tr}(AB) = \\sum_{j,k} A_{jk} B_{kj}$. Since $M$ is symmetric ($M_{kj}=M_{jk}$), this becomes $\\sum_{j,k} (S_{(1)})_{jk} M_{jk}$.\n$$\n\\mathrm{Tr}(S_{(1)}M) = (S_{(1)})_{11}M_{11} + (S_{(1)})_{22}M_{22} + (S_{(1)})_{33}M_{33} + (S_{(1)})_{23}M_{23} + (S_{(1)})_{32}M_{32}.\n$$\nSubstituting the values from $S_{(1)}$ and using $M_{23}=M_{32}$:\n$$\n\\mathrm{Tr}(S_{(1)}M) = 1 \\cdot M_{11} + 1 \\cdot M_{22} + 1 \\cdot M_{33} + 1 \\cdot M_{23} + 1 \\cdot M_{32} = M_{11} + M_{22} + M_{33} + 2M_{23}.\n$$\nThe diffusion contribution is therefore:\n$$\n\\frac{1}{2}\\mathrm{Tr}\\left(\\sigma_{(1)}\\sigma_{(1)}^{\\top} M\\right) = \\frac{1}{2}(M_{11} + M_{22} + 2M_{23} + M_{33}).\n$$\n\nExample 2: The state dimension is $n=2$ and the noise dimension is $m=1$. The diffusion matrix is\n$$\n\\sigma_{(2)} = \\begin{pmatrix}\n1 \\\\\n0\n\\end{pmatrix}.\n$$\nThe covariance matrix is $S_{(2)} = \\sigma_{(2)}\\sigma_{(2)}^{\\top}$:\n$$\nS_{(2)} = \\begin{pmatrix}\n1 \\\\\n0\n\\end{pmatrix}\n\\begin{pmatrix}\n1 & 0\n\\end{pmatrix} = \\begin{pmatrix}\n1 & 0 \\\\\n0 & 0\n\\end{pmatrix}.\n$$\nLet $M \\in \\mathbb{S}^2$ be a symmetric $2 \\times 2$ matrix with entries $M_{jk}$.\n$$\n\\mathrm{Tr}(S_{(2)}M) = (S_{(2)})_{11}M_{11} + (S_{(2)})_{22}M_{22} + (S_{(2)})_{12}M_{12} + (S_{(2)})_{21}M_{21} = 1 \\cdot M_{11} + 0 \\cdot M_{22} + 0 + 0 = M_{11}.\n$$\nThe diffusion contribution is:\n$$\n\\frac{1}{2}\\mathrm{Tr}\\left(\\sigma_{(2)}\\sigma_{(2)}^{\\top} M\\right) = \\frac{1}{2} M_{11}.\n$$\n\nNow we study the structural consequences. The ellipticity of the HJB equation is determined by the matrix $\\sigma\\sigma^{\\top}$. If this matrix is positive definite, the equation is uniformly elliptic. If it is singular (rank-deficient), the equation is termed degenerate elliptic. This singularity implies that the diffusion acts only in a subspace of the state space. The kernel of $\\sigma^{\\top}$ comprises the directions in which there is no diffusion.\n\nFor Example 1, $\\mathrm{rank}(S_{(1)}) = 2 < 3$, so it is singular. We find the kernel of $\\sigma_{(1)}^{\\top}$:\n$$\n\\sigma_{(1)}^{\\top}v = \\begin{pmatrix}\n1 & 0 & 0 \\\\\n0 & 1 & 1\n\\end{pmatrix} \\begin{pmatrix} v_1 \\\\ v_2 \\\\ v_3 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} \\implies\n\\begin{cases}\nv_1 = 0 \\\\\nv_2 + v_3 = 0\n\\end{cases}.\n$$\nThe kernel is spanned by the vector $v_{\\ker} = (0, 1, -1)^{\\top}$. The second-order term of the HJB operator is insensitive to the curvature of the test function $\\varphi$ in this direction. To see this, consider a Hessian representing curvature only in this direction, $M = v_{\\ker}v_{\\ker}^{\\top}$:\n$$\nM = \\begin{pmatrix} 0 \\\\ 1 \\\\ -1 \\end{pmatrix}\\begin{pmatrix} 0 & 1 & -1 \\end{pmatrix} = \\begin{pmatrix} 0 & 0 & 0 \\\\ 0 & 1 & -1 \\\\ 0 & -1 & 1 \\end{pmatrix}.\n$$\nHere, $M_{11}=0$, $M_{22}=1$, $M_{33}=1$, and $M_{23}=-1$. Plugging this into our derived form for the diffusion contribution:\n$$\n\\frac{1}{2}(M_{11} + M_{22} + 2M_{23} + M_{33}) = \\frac{1}{2}(0 + 1 + 2(-1) + 1) = 0.\n$$\nThis explicitly shows that the HJB equation has no second derivative terms associated with the direction $(0, 1, -1)^{\\top}$. The PDE is degenerate; it behaves like a first-order transport equation in this direction, while being second-order in the orthogonal subspace spanned by the columns of $\\sigma_{(1)}$, i.e., $(1,0,0)^{\\top}$ and $(0,1,1)^{\\top}$.\n\nFor Example 2, $S_{(2)}$ is also singular, with $\\mathrm{rank}(S_{(2)})=1 < 2$. The kernel of $\\sigma_{(2)}^{\\top}$ is found by:\n$$\n\\sigma_{(2)}^{\\top}v = \\begin{pmatrix} 1 & 0 \\end{pmatrix} \\begin{pmatrix} v_1 \\\\ v_2 \\end{pmatrix} = 0 \\implies v_1 = 0.\n$$\nThe kernel is the $x_2$-axis, spanned by $v_{\\ker} = (0, 1)^{\\top}$. Our derived diffusion contribution, $\\frac{1}{2}M_{11}$, which is $\\frac{1}{2}\\frac{\\partial^2\\varphi}{\\partial x_1^2}$, explicitly demonstrates the structural consequence. The term is independent of $M_{22} = \\frac{\\partial^2\\varphi}{\\partial x_2^2}$ and $M_{12} = \\frac{\\partial^2\\varphi}{\\partial x_1 \\partial x_2}$. The HJB equation is second-order in the $x_1$ variable but only first-order in the $x_2$ variable. This reflects that the underlying SDE has noise only in the first component, leaving the second component's evolution to be driven solely by the drift, which is a first-order effect. This is a classic case of degenerate ellipticity.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{1}{2}(M_{11} + M_{22} + 2M_{23} + M_{33}) & \\frac{1}{2}M_{11}\n\\end{pmatrix}\n}\n$$", "id": "3005553"}, {"introduction": "Proving that a viscosity solution exists is a cornerstone of the theory, and a primary tool for this is Perron's method, which constructs the solution by taking the supremum of all subsolutions. This exercise provides a practical starting point for this powerful technique by tasking you with the construction of explicit sub- and supersolutions that act as \"barriers\" for the true value function. This practice demonstrates how terminal conditions can be used to bound the solution globally and initialize the Perron construction, providing insight into existence theorems. [@problem_id:3005543]", "problem": "Consider a one-dimensional controlled dynamical system with control process $a_{s} \\in [-1,1]$ evolving over time $s \\in [t,T]$ according to the ordinary differential equation $dX_{s} = a_{s}\\,ds$ with initial condition $X_{t} = x$. Let the objective be minimization of the terminal payoff, so that the value function is $u(t,x) = \\inf_{a_{\\cdot}} \\mathbb{E}\\left[g\\left(X_{T}^{t,x,a}\\right)\\right]$, where the terminal data is the bounded continuous function $g(x) = \\arctan(x)$. The bounded coefficients and the Dynamic Programming Principle together give the Hamilton-Jacobi-Bellman (HJB) equation in the viscosity sense\n$$\n- u_{t}(t,x) + \\inf_{a \\in [-1,1]} \\left\\{ a\\,u_{x}(t,x) \\right\\} = 0 \\quad \\text{for } (t,x) \\in [0,T) \\times \\mathbb{R},\n$$\nwith terminal condition $u(T,x) = g(x)$, where $T>0$ is fixed.\n\nUsing only the core definitions of viscosity subsolutions and supersolutions and the structural properties of the above HJB (bounded coefficients, degenerate ellipticity), construct explicit bounded continuous functions $u_{-}(t,x)$ and $u_{+}(t,x)$ such that:\n- $u_{-}$ is a viscosity subsolution of the HJB on $[0,T) \\times \\mathbb{R}$ and $u_{-}(T,x) \\le g(x)$ for all $x \\in \\mathbb{R}$,\n- $u_{+}$ is a viscosity supersolution of the HJB on $[0,T) \\times \\mathbb{R}$ and $u_{+}(T,x) \\ge g(x)$ for all $x \\in \\mathbb{R}$.\n\nYour final answer must consist of the explicit analytic expressions of $u_{-}(t,x)$ and $u_{+}(t,x)$ stated as constant functions of $(t,x)$, presented as a single row matrix in the order $\\left(u_{-}, u_{+}\\right)$. No numerical rounding is required.", "solution": "We begin with the fundamental setup of dynamic optimization and viscosity solutions. The controlled dynamics $dX_{s} = a_{s}\\,ds$ with $a_{s} \\in [-1,1]$ has bounded drift coefficient $a_{s}$ and zero diffusion, which is a degenerate case of a stochastic differential equation. The value function $u$ associated with the minimization problem of the terminal payoff $g(x) = \\arctan(x)$ is characterized in the viscosity sense by the Hamilton-Jacobi-Bellman (HJB) equation\n$$\n- u_{t}(t,x) + \\inf_{a \\in [-1,1]} \\left\\{ a\\,u_{x}(t,x) \\right\\} = 0,\n$$\nwith terminal data $u(T,x) = g(x)$.\n\nWe recall the definition of viscosity subsolution and supersolution for the parabolic HJB equation. Let $F(t,x,p,q) = -p_{t} + \\inf_{a \\in [-1,1]} \\left\\{ a\\,p_{x} \\right\\}$ denote the operator acting on the derivatives of a test function. A bounded upper semicontinuous function $v$ is a viscosity subsolution if, for any smooth test function $\\varphi \\in C^{1,2}([0,T] \\times \\mathbb{R})$ such that $v - \\varphi$ attains a local maximum at $(t_{0},x_{0}) \\in [0,T) \\times \\mathbb{R}$, we have\n$$\n- \\varphi_{t}(t_{0},x_{0}) + \\inf_{a \\in [-1,1]} \\left\\{ a\\,\\varphi_{x}(t_{0},x_{0}) \\right\\} \\le 0.\n$$\nSimilarly, a bounded lower semicontinuous function $w$ is a viscosity supersolution if, for any $\\psi \\in C^{1,2}$ such that $w - \\psi$ attains a local minimum at $(t_{0},x_{0})$, we have\n$$\n- \\psi_{t}(t_{0},x_{0}) + \\inf_{a \\in [-1,1]} \\left\\{ a\\,\\psi_{x}(t_{0},x_{0}) \\right\\} \\ge 0.\n$$\n\nWe will construct explicit constant subsolution and supersolution functions. Consider the terminal data $g(x) = \\arctan(x)$. Its range is $(-\\pi/2,\\pi/2)$, more precisely,\n$$\n\\lim_{x \\to -\\infty} \\arctan(x) = -\\frac{\\pi}{2}, \\quad \\lim_{x \\to +\\infty} \\arctan(x) = \\frac{\\pi}{2},\n$$\nand for all $x \\in \\mathbb{R}$ we have $-\\frac{\\pi}{2} < \\arctan(x) < \\frac{\\pi}{2}$. Therefore, the constants $c_{-} := -\\frac{\\pi}{2}$ and $c_{+} := \\frac{\\pi}{2}$ satisfy $c_{-} \\le g(x) \\le c_{+}$ for all $x \\in \\mathbb{R}$. Define\n$$\nu_{-}(t,x) := c_{-} = -\\frac{\\pi}{2}, \\quad u_{+}(t,x) := c_{+} = \\frac{\\pi}{2}.\n$$\nThese are bounded continuous functions.\n\nWe verify that $u_{-}$ is a viscosity subsolution. Let $\\varphi \\in C^{1,2}$ and suppose $u_{-} - \\varphi$ attains a local maximum at $(t_{0},x_{0})$. Since $u_{-}$ is constant, this implies that $- \\varphi$ attains a local maximum, equivalently $\\varphi$ attains a local minimum at $(t_{0},x_{0})$. At such a local minimum of a smooth function of $(t,x)$, we have the vanishing first derivatives and nonnegative second derivatives, specifically\n$$\n\\varphi_{t}(t_{0},x_{0}) = 0, \\quad \\varphi_{x}(t_{0},x_{0}) = 0, \\quad \\text{and} \\quad \\varphi_{xx}(t_{0},x_{0}) \\ge 0.\n$$\nFor our operator, the relevant derivatives are only $\\varphi_{t}$ and $\\varphi_{x}$, so we compute\n$- \\varphi_{t}(t_{0},x_{0}) + \\inf_{a \\in [-1,1]} \\left\\{ a\\,\\varphi_{x}(t_{0},x_{0}) \\right\\} = - 0 + \\inf_{a \\in [-1,1]} \\left\\{ a \\cdot 0 \\right\\} = 0 \\le 0$,\nwhich verifies the subsolution inequality. The terminal inequality $u_{-}(T,x) \\le g(x)$ holds because $-\\frac{\\pi}{2} \\le \\arctan(x)$ for all $x \\in \\mathbb{R}$.\n\nWe verify that $u_{+}$ is a viscosity supersolution. Let $\\psi \\in C^{1,2}$ and suppose $u_{+} - \\psi$ attains a local minimum at $(t_{1},x_{1})$. Since $u_{+}$ is constant, this implies that $\\psi$ attains a local maximum at $(t_{1},x_{1})$ or, equivalently, that $- \\psi$ attains a local minimum. For a smooth function $\\psi$ at a local maximum in $(t,x)$, the first derivatives vanish:\n$$\n\\psi_{t}(t_{1},x_{1}) = 0, \\quad \\psi_{x}(t_{1},x_{1}) = 0.\n$$\nWe again compute for the operator\n$- \\psi_{t}(t_{1},x_{1}) + \\inf_{a \\in [-1,1]} \\left\\{ a\\,\\psi_{x}(t_{1},x_{1}) \\right\\} = - 0 + \\inf_{a \\in [-1,1]} \\left\\{ a \\cdot 0 \\right\\} = 0 \\ge 0$,\nwhich verifies the supersolution inequality. The terminal inequality $u_{+}(T,x) \\ge g(x)$ holds because $\\arctan(x) \\le \\frac{\\pi}{2}$ for all $x \\in \\mathbb{R}$.\n\nThese explicit bounded continuous functions $u_{-}(t,x) = -\\frac{\\pi}{2}$ and $u_{+}(t,x) = \\frac{\\pi}{2}$ constitute valid viscosity subsolution and supersolution, respectively, for the given HJB with terminal condition $g(x) = \\arctan(x)$. They provide initial barriers for Perron’s method, ensuring the nonempty families of subsolutions and supersolutions ordered around the terminal data.", "answer": "$$\\boxed{\\begin{pmatrix}\n-\\frac{\\pi}{2} & \\frac{\\pi}{2}\n\\end{pmatrix}}$$", "id": "3005543"}]}