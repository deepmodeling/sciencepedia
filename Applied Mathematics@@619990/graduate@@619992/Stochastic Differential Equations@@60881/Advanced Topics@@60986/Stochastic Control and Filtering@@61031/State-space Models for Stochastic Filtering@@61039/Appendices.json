{"hands_on_practices": [{"introduction": "The heart of stochastic filtering lies in updating our belief about a hidden state based on new measurements. For linear-Gaussian systems, the optimal estimate that minimizes the mean-squared error is the conditional expectation. This first exercise [@problem_id:2996462] takes you back to first principles, asking you to derive the conditional expectation $\\mathbb{E}[X \\mid Y]$ for a jointly Gaussian system directly from its probability density function. By completing this derivation, you will see how the abstract statistical definition naturally gives rise to the classic filter update structure: a prior estimate corrected by a gain-weighted measurement innovation.", "problem": "Consider a single-step discrete-time linear Gaussian estimation setting in which the latent state $X$ and the measurement $Y$ are jointly Gaussian. Let the joint distribution of $(X,Y)$ be a bivariate normal with mean vector $m$ and covariance matrix $\\Sigma$ given by\n$$\nm = \\begin{pmatrix} 2 \\\\ -1 \\end{pmatrix}, \\qquad \\Sigma = \\begin{pmatrix} 7 & 3 \\\\ 3 & 5 \\end{pmatrix}.\n$$\nStarting only from the definition of the multivariate normal density and the definition of conditional expectation, derive the closed-form analytic expression for the conditional expectation $\\mathbb{E}[X \\mid Y]$ as a function of the random variable $Y$. Then, interpret this expression as a one-step linear filter update in the discrete setting, identifying the prior mean, the measurement innovation, and the filter gain within your derived expression. Express your final answer solely as an analytic expression in terms of $Y$. No rounding is required, and no units apply.", "solution": "The problem is to derive the conditional expectation $\\mathbb{E}[X \\mid Y]$ for two jointly Gaussian random variables, $X$ and $Y$, starting from first principles, and then interpret the result in the context of linear filtering.\n\nThe joint distribution of the random vector $\\begin{pmatrix} X \\\\ Y \\end{pmatrix}$ is given as a bivariate normal distribution with mean vector $m$ and covariance matrix $\\Sigma$:\n$$ m = \\begin{pmatrix} \\mu_X \\\\ \\mu_Y \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ -1 \\end{pmatrix} $$\n$$ \\Sigma = \\begin{pmatrix} \\Sigma_{XX} & \\Sigma_{XY} \\\\ \\Sigma_{YX} & \\Sigma_{YY} \\end{pmatrix} = \\begin{pmatrix} 7 & 3 \\\\ 3 & 5 \\end{pmatrix} $$\nFrom this, we identify the marginal means, variances, and covariance:\n$\\mu_X = \\mathbb{E}[X] = 2$, $\\mu_Y = \\mathbb{E}[Y] = -1$, $\\Sigma_{XX} = \\text{Var}(X) = 7$, $\\Sigma_{YY} = \\text{Var}(Y) = 5$, and $\\Sigma_{XY} = \\Sigma_{YX} = \\text{Cov}(X,Y) = 3$.\n\nThe conditional expectation $\\mathbb{E}[X \\mid Y=y]$ is defined by the integral:\n$$ \\mathbb{E}[X \\mid Y=y] = \\int_{-\\infty}^{\\infty} x \\cdot p_{X|Y}(x|y) \\, \\mathrm{d}x $$\nwhere $p_{X|Y}(x|y)$ is the conditional probability density function (PDF) of $X$ given $Y=y$. This conditional PDF is given by the ratio of the joint PDF to the marginal PDF of $Y$:\n$$ p_{X|Y}(x|y) = \\frac{p_{X,Y}(x,y)}{p_Y(y)} $$\nThe joint PDF, $p_{X,Y}(x,y)$, for a bivariate normal distribution is:\n$$ p_{X,Y}(x,y) = \\frac{1}{2\\pi \\sqrt{\\det(\\Sigma)}} \\exp\\left( -\\frac{1}{2} \\begin{pmatrix} x-\\mu_X \\\\ y-\\mu_Y \\end{pmatrix}^T \\Sigma^{-1} \\begin{pmatrix} x-\\mu_X \\\\ y-\\mu_Y \\end{pmatrix} \\right) $$\nFirst, we compute the determinant and inverse of $\\Sigma$:\n$$ \\det(\\Sigma) = (7)(5) - (3)(3) = 35 - 9 = 26 $$\n$$ \\Sigma^{-1} = \\frac{1}{26} \\begin{pmatrix} 5 & -3 \\\\ -3 & 7 \\end{pmatrix} $$\nThe quadratic form in the exponent, $Q(x,y)$, is expanded as:\n$$ Q(x,y) = \\frac{1}{26} \\left[ 5(x-\\mu_X)^2 - 6(x-\\mu_X)(y-\\mu_Y) + 7(y-\\mu_Y)^2 \\right] $$\nTo find the conditional distribution of $X$ given $Y=y$, we must rearrange this expression by completing the square with respect to $x$. We treat $y$ as a fixed parameter.\n$$ Q(x,y) = \\frac{1}{26} \\left\\{ 5 \\left[ (x-\\mu_X)^2 - \\frac{6}{5}(x-\\mu_X)(y-\\mu_Y) \\right] + 7(y-\\mu_Y)^2 \\right\\} $$\nCompleting the square for the terms involving $x$:\n$$ (x-\\mu_X)^2 - \\frac{6}{5}(x-\\mu_X)(y-\\mu_Y) = \\left( (x-\\mu_X) - \\frac{3}{5}(y-\\mu_Y) \\right)^2 - \\left( \\frac{3}{5}(y-\\mu_Y) \\right)^2 $$\n$$ = \\left( x - \\left( \\mu_X + \\frac{3}{5}(y-\\mu_Y) \\right) \\right)^2 - \\frac{9}{25}(y-\\mu_Y)^2 $$\nSubstituting this back into the expression for $Q(x,y)$:\n$$ Q(x,y) = \\frac{1}{26} \\left\\{ 5 \\left[ \\left( x - \\left( \\mu_X + \\frac{3}{5}(y-\\mu_Y) \\right) \\right)^2 - \\frac{9}{25}(y-\\mu_Y)^2 \\right] + 7(y-\\mu_Y)^2 \\right\\} $$\n$$ Q(x,y) = \\frac{1}{26} \\left\\{ 5 \\left( x - \\left( \\mu_X + \\frac{3}{5}(y-\\mu_Y) \\right) \\right)^2 - \\frac{9}{5}(y-\\mu_Y)^2 + 7(y-\\mu_Y)^2 \\right\\} $$\n$$ Q(x,y) = \\frac{1}{26} \\left\\{ 5 \\left( x - \\left( \\mu_X + \\frac{3}{5}(y-\\mu_Y) \\right) \\right)^2 + \\left( 7 - \\frac{9}{5} \\right)(y-\\mu_Y)^2 \\right\\} $$\n$$ Q(x,y) = \\frac{1}{26} \\left\\{ 5 \\left( x - \\left( \\mu_X + \\frac{3}{5}(y-\\mu_Y) \\right) \\right)^2 + \\frac{26}{5}(y-\\mu_Y)^2 \\right\\} $$\nDistributing the $\\frac{1}{26}$ term:\n$$ Q(x,y) = \\frac{5}{26} \\left( x - \\left( \\mu_X + \\frac{3}{5}(y-\\mu_Y) \\right) \\right)^2 + \\frac{1}{5}(y-\\mu_Y)^2 $$\n$$ Q(x,y) = \\frac{\\left( x - \\left( \\mu_X + \\frac{3}{5}(y-\\mu_Y) \\right) \\right)^2}{26/5} + \\frac{(y-\\mu_Y)^2}{5} $$\nThe exponent of the joint PDF is $-\\frac{1}{2}Q(x,y)$. The joint PDF can now be written as a product of two terms:\n$$ p_{X,Y}(x,y) = \\frac{1}{2\\pi\\sqrt{26}} \\exp\\left( -\\frac{1}{2} \\left[ \\frac{\\left(x - \\left(\\mu_X + \\frac{3}{5}(y-\\mu_Y)\\right)\\right)^2}{26/5} + \\frac{(y-\\mu_Y)^2}{5} \\right] \\right) $$\nUsing $\\sqrt{26} = \\sqrt{5 \\cdot (26/5)}$, we can separate the expression:\n$$ p_{X,Y}(x,y) = \\left[ \\frac{1}{\\sqrt{2\\pi \\cdot 5}} \\exp\\left(-\\frac{(y-\\mu_Y)^2}{2 \\cdot 5}\\right) \\right] \\cdot \\left[ \\frac{1}{\\sqrt{2\\pi \\cdot (26/5)}} \\exp\\left(-\\frac{\\left(x - \\left(\\mu_X + \\frac{3}{5}(y-\\mu_Y)\\right)\\right)^2}{2 \\cdot (26/5)}\\right) \\right] $$\nThe first bracketed term is the PDF of a normal distribution with mean $\\mu_Y$ and variance $\\Sigma_{YY}=5$, which is precisely the marginal PDF $p_Y(y)$. The second bracketed term must therefore be the conditional PDF $p_{X|Y}(x|y)$.\nBy inspection of this second term, we see that the conditional distribution of $X$ given $Y=y$ is a normal distribution with:\n- Conditional mean: $\\mu_{X|Y} = \\mu_X + \\frac{3}{5}(y-\\mu_Y)$\n- Conditional variance: $\\sigma^2_{X|Y} = \\frac{26}{5}$\n\nThe conditional expectation $\\mathbb{E}[X \\mid Y=y]$ is the mean of this conditional distribution.\n$$ \\mathbb{E}[X \\mid Y=y] = \\mu_X + \\frac{3}{5}(y-\\mu_Y) $$\nSubstituting the given numerical values $\\mu_X = 2$ and $\\mu_Y=-1$:\n$$ \\mathbb{E}[X \\mid Y=y] = 2 + \\frac{3}{5}(y - (-1)) = 2 + \\frac{3}{5}(y+1) $$\n$$ \\mathbb{E}[X \\mid Y=y] = 2 + \\frac{3}{5}y + \\frac{3}{5} = \\frac{13}{5} + \\frac{3}{5}y $$\nReplacing the specific value $y$ with the random variable $Y$, we obtain the final expression for the conditional expectation as a function of $Y$:\n$$ \\mathbb{E}[X \\mid Y] = \\frac{13}{5} + \\frac{3}{5}Y $$\n\nNow, this expression is interpreted as a one-step linear filter update. The general form of such an update is:\n$$ \\text{Posterior Estimate} = \\text{Prior Estimate} + \\text{Gain} \\times (\\text{Innovation}) $$\nIn this context:\n- The posterior estimate of the state $X$ after observing the measurement $Y$ is the conditional expectation $\\mathbb{E}[X \\mid Y]$.\n- The prior estimate of the state $X$ before observing $Y$ is its unconditional mean, $\\mathbb{E}[X] = \\mu_X$.\n- The measurement innovation is the difference between the actual measurement $Y$ and its expected value (the predicted measurement), which is $Y - \\mathbb{E}[Y] = Y - \\mu_Y$.\n\nLet's rewrite our derived expression for $\\mathbb{E}[X \\mid Y]$ in this form:\n$$ \\mathbb{E}[X \\mid Y] = \\mu_X + \\frac{3}{5}(Y - \\mu_Y) $$\nComparing this with the filter structure, we can identify the components:\n- **Prior Mean:** $\\mathbb{E}[X] = \\mu_X = 2$. This is the best estimate of $X$ with no information from $Y$.\n- **Measurement Innovation:** $Y - \\mu_Y = Y - (-1) = Y+1$. This term represents the new information provided by the measurement $Y$.\n- **Filter Gain:** The coefficient multiplying the innovation is the filter gain, $K = \\frac{3}{5}$. In general, this gain is $K = \\Sigma_{XY}\\Sigma_{YY}^{-1} = 3 \\cdot 5^{-1} = \\frac{3}{5}$. The gain determines the weight given to the innovation when updating the prior estimate.\n\nThe expression $\\mathbb{E}[X \\mid Y] = 2 + \\frac{3}{5}(Y+1)$ is a classic result in linear estimation, representing the optimal linear estimate of $X$ given $Y$ in the mean-squared error sense.", "answer": "$$ \\boxed{\\frac{13}{5} + \\frac{3}{5}Y} $$", "id": "2996462"}, {"introduction": "A powerful filter is of little use if the system it is applied to is fundamentally unobservable. This practice [@problem_id:2996460] explores the critical concept of detectability by presenting a system with an unstable mode that is completely invisible to the measurement process. Your task will be to analyze the evolution of the estimation error covariance using the Riccati equation and demonstrate analytically why the filter's uncertainty about this hidden, unstable direction grows without bound. This exercise provides a stark and essential lesson on how system structure dictates the feasibility of state estimation.", "problem": "Consider the continuous-time Linear Time-Invariant (LTI) state-space model in $\\mathbb{R}^{2}$ given by the Stochastic Differential Equation (SDE)\n$$\n\\mathrm{d}X(t) = A\\,X(t)\\,\\mathrm{d}t + G\\,\\mathrm{d}W(t),\n$$\nwhere $X(t) \\in \\mathbb{R}^{2}$, $W(t)$ is a scalar standard Wiener process, $A = \\begin{pmatrix} \\alpha & 0 \\\\ 0 & -\\beta \\end{pmatrix}$ with $\\alpha > 0$ and $\\beta > 0$, and $G = \\begin{pmatrix} \\sqrt{q} \\\\ 0 \\end{pmatrix}$ with $q > 0$. The observation process is\n$$\n\\mathrm{d}Y(t) = C\\,X(t)\\,\\mathrm{d}t + \\sqrt{r}\\,\\mathrm{d}V(t),\n$$\nwhere $Y(t) \\in \\mathbb{R}$, $V(t)$ is a scalar standard Wiener process independent of $W(t)$, $r > 0$, and $C = \\begin{pmatrix} 0 & 1 \\end{pmatrix}$. Assume the initial estimation error covariance of the optimal filter is $P(0) = \\begin{pmatrix} p_{0} & 0 \\\\ 0 & s_{0} \\end{pmatrix}$ with $p_{0} > 0$ and $s_{0} > 0$.\n\nThis model exhibits lack of detectability because the unstable mode associated with the eigenvalue $\\alpha$ is unobservable. Starting from appropriate first principles in stochastic filtering and state-space modeling, derive the evolution of the estimation error covariance and compute the closed-form expression for the variance of the estimation error along the undetectable unstable direction, namely the $(1,1)$-entry $P_{11}(t)$, for all $t \\ge 0$. Explain the geometric intuition for why this variance becomes unbounded in time due to the lack of detectability.\n\nExpress your final answer for $P_{11}(t)$ as a closed-form analytic expression in terms of the symbols $\\alpha$, $q$, and $p_{0}$. No rounding is required.", "solution": "The evolution of the estimation error covariance matrix $P(t) = \\mathbb{E}[(X(t) - \\hat{X}(t))(X(t) - \\hat{X}(t))^T]$ for the optimal filter is governed by the continuous-time Riccati differential equation:\n$$\n\\frac{\\mathrm{d}P(t)}{\\mathrm{d}t} = A P(t) + P(t) A^T + G Q G^T - P(t) C^T R^{-1} C P(t)\n$$\nFrom the problem statement, the system matrices are $A = \\begin{pmatrix} \\alpha & 0 \\\\ 0 & -\\beta \\end{pmatrix}$, $G = \\begin{pmatrix} \\sqrt{q} \\\\ 0 \\end{pmatrix}$, and $C = \\begin{pmatrix} 0 & 1 \\end{pmatrix}$. The process noise $W(t)$ is a scalar standard Wiener process, so its associated covariance is $Q=1$. The observation noise variance is given as $r$, so $R=r$. Substituting these into the Riccati equation yields:\n$$\n\\frac{\\mathrm{d}P(t)}{\\mathrm{d}t} = A P(t) + P(t) A^T + G G^T - \\frac{1}{r} P(t) C^T C P(t)\n$$\nWe compute the matrix products:\n$$ G G^T = \\begin{pmatrix} \\sqrt{q} \\\\ 0 \\end{pmatrix} \\begin{pmatrix} \\sqrt{q} & 0 \\end{pmatrix} = \\begin{pmatrix} q & 0 \\\\ 0 & 0 \\end{pmatrix} $$\n$$ C^T C = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} \\begin{pmatrix} 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 0 & 0 \\\\ 0 & 1 \\end{pmatrix} $$\nLet $P(t)$ be represented by its components: $P(t) = \\begin{pmatrix} P_{11}(t) & P_{12}(t) \\\\ P_{21}(t) & P_{22}(t) \\end{pmatrix}$. Due to the symmetric nature of the covariance matrix, $P_{12}(t) = P_{21}(t)$.\nThe terms in the Riccati equation are:\n$$ A P(t) + P(t) A^T = \\begin{pmatrix} \\alpha & 0 \\\\ 0 & -\\beta \\end{pmatrix} \\begin{pmatrix} P_{11} & P_{12} \\\\ P_{12} & P_{22} \\end{pmatrix} + \\begin{pmatrix} P_{11} & P_{12} \\\\ P_{12} & P_{22} \\end{pmatrix} \\begin{pmatrix} \\alpha & 0 \\\\ 0 & -\\beta \\end{pmatrix} = \\begin{pmatrix} 2\\alpha P_{11} & (\\alpha - \\beta) P_{12} \\\\ (\\alpha - \\beta) P_{12} & -2\\beta P_{22} \\end{pmatrix} $$\n$$ P(t) C^T C P(t) = \\begin{pmatrix} P_{11} & P_{12} \\\\ P_{12} & P_{22} \\end{pmatrix} \\begin{pmatrix} 0 & 0 \\\\ 0 & 1 \\end{pmatrix} \\begin{pmatrix} P_{11} & P_{12} \\\\ P_{12} & P_{22} \\end{pmatrix} = \\begin{pmatrix} P_{12}^2 & P_{12}P_{22} \\\\ P_{12}P_{22} & P_{22}^2 \\end{pmatrix} $$\nSubstituting these into the Riccati equation gives a system of ordinary differential equations (ODEs) for the components of $P(t)$:\n$$\n\\frac{\\mathrm{d}}{\\mathrm{d}t} \\begin{pmatrix} P_{11} & P_{12} \\\\ P_{12} & P_{22} \\end{pmatrix} = \\begin{pmatrix} 2\\alpha P_{11} + q - \\frac{1}{r}P_{12}^2 & (\\alpha - \\beta)P_{12} - \\frac{1}{r}P_{12}P_{22} \\\\ (\\alpha - \\beta)P_{12} - \\frac{1}{r}P_{12}P_{22} & -2\\beta P_{22} - \\frac{1}{r}P_{22}^2 \\end{pmatrix}\n$$\nThis gives three unique scalar ODEs:\n1. $\\dot{P}_{11}(t) = 2\\alpha P_{11}(t) + q - \\frac{1}{r} P_{12}(t)^2$\n2. $\\dot{P}_{12}(t) = (\\alpha - \\beta) P_{12}(t) - \\frac{1}{r} P_{12}(t) P_{22}(t) = \\left( \\alpha - \\beta - \\frac{1}{r} P_{22}(t) \\right) P_{12}(t)$\n3. $\\dot{P}_{22}(t) = -2\\beta P_{22}(t) - \\frac{1}{r} P_{22}(t)^2$\n\nThe initial condition is $P(0) = \\begin{pmatrix} p_0 & 0 \\\\ 0 & s_0 \\end{pmatrix}$, which implies $P_{11}(0) = p_0$, $P_{12}(0) = 0$, and $P_{22}(0) = s_0$.\nConsider the ODE for $P_{12}(t)$. It is a first-order linear homogeneous equation in $P_{12}$ of the form $\\dot{x}(t) = f(t)x(t)$ with the initial condition $x(0) = 0$. The unique solution to such an initial value problem is $x(t) = 0$ for all $t \\ge 0$. Therefore, $P_{12}(t) = 0$ for all $t \\ge 0$.\n\nThis simplifies the ODE for $P_{11}(t)$ significantly by eliminating the nonlinear term:\n$$ \\dot{P}_{11}(t) = 2\\alpha P_{11}(t) + q $$\nThis is a first-order linear inhomogeneous ODE with the initial condition $P_{11}(0) = p_0$. We can rewrite it as $\\dot{P}_{11} - 2\\alpha P_{11} = q$.\nWe solve it using an integrating factor $\\mu(t) = \\exp\\left(\\int -2\\alpha \\,\\mathrm{d}t\\right) = \\exp(-2\\alpha t)$.\nMultiplying the ODE by $\\mu(t)$:\n$$ \\exp(-2\\alpha t) \\dot{P}_{11}(t) - 2\\alpha \\exp(-2\\alpha t) P_{11}(t) = q \\exp(-2\\alpha t) $$\nThe left side is the derivative of a product:\n$$ \\frac{\\mathrm{d}}{\\mathrm{d}t} \\left( P_{11}(t) \\exp(-2\\alpha t) \\right) = q \\exp(-2\\alpha t) $$\nIntegrating both sides from $\\tau=0$ to $\\tau=t$:\n$$ \\int_{0}^{t} \\frac{\\mathrm{d}}{\\mathrm{d}\\tau} \\left( P_{11}(\\tau) \\exp(-2\\alpha \\tau) \\right) \\mathrm{d}\\tau = \\int_{0}^{t} q \\exp(-2\\alpha \\tau) \\mathrm{d}\\tau $$\n$$ P_{11}(t) \\exp(-2\\alpha t) - P_{11}(0) \\exp(0) = q \\left[ \\frac{\\exp(-2\\alpha \\tau)}{-2\\alpha} \\right]_{0}^{t} $$\nSubstituting $P_{11}(0) = p_0$:\n$$ P_{11}(t) \\exp(-2\\alpha t) - p_0 = q \\left( \\frac{\\exp(-2\\alpha t)}{-2\\alpha} - \\frac{1}{-2\\alpha} \\right) = \\frac{q}{2\\alpha} (1 - \\exp(-2\\alpha t)) $$\nSolving for $P_{11}(t)$:\n$$ P_{11}(t) \\exp(-2\\alpha t) = p_0 + \\frac{q}{2\\alpha} - \\frac{q}{2\\alpha}\\exp(-2\\alpha t) $$\n$$ P_{11}(t) = p_0 \\exp(2\\alpha t) + \\frac{q}{2\\alpha} \\exp(2\\alpha t) - \\frac{q}{2\\alpha} $$\nThis is the closed-form expression for the $(1,1)$-entry of the error covariance matrix. It can be written as:\n$$ P_{11}(t) = p_0 \\exp(2\\alpha t) + \\frac{q}{2\\alpha} (\\exp(2\\alpha t) - 1) $$\nThe geometric intuition for the unbounded growth of $P_{11}(t)$ lies in the concepts of stability and observability. The state-space is $\\mathbb{R}^2$, spanned by the standard basis vectors $e_1 = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$ and $e_2 = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$.\nThe state dynamics are decoupled:\n$$ \\mathrm{d}X_1(t) = \\alpha X_1(t) \\mathrm{d}t + \\sqrt{q} \\mathrm{d}W(t) $$\n$$ \\mathrm{d}X_2(t) = -\\beta X_2(t) \\mathrm{d}t $$\nThe observation is $\\mathrm{d}Y(t) = X_2(t) \\mathrm{d}t + \\sqrt{r} \\mathrm{d}V(t)$.\nThe eigenvector $e_1$ corresponds to the eigenvalue $\\alpha > 0$, representing an unstable mode. The eigenvector $e_2$ corresponds to the eigenvalue $-\\beta < 0$, a stable mode. The observation matrix $C = \\begin{pmatrix} 0 & 1 \\end{pmatrix}$ has a null space spanned by $e_1$. This means the unstable mode along the $X_1$-axis is completely unobservable.\nThe filter receives measurement information only about the state component $X_2$, but no information about $X_1$. The Kalman gain is $K(t) = \\frac{1}{r} P(t) C^T = \\frac{1}{r} \\begin{pmatrix} P_{12}(t) \\\\ P_{22}(t) \\end{pmatrix}$. Since $P_{12}(t)=0$, the first component of the gain is always zero. This confirms mathematically that the filter's estimate of $X_1$, namely $\\hat{X}_1$, receives no corrective feedback from the measurements.\nThe estimation error for the first state, $e_1(t) = X_1(t) - \\hat{X}_1(t)$, then evolves according to its open-loop dynamics, driven by process noise:\n$$ \\mathrm{d}e_1(t) = \\alpha e_1(t) \\mathrm{d}t + \\sqrt{q} \\mathrm{d}W(t) $$\nThis describes an unstable process. The variance of this process, $P_{11}(t) = \\mathbb{E}[e_1(t)^2]$, must grow over time. The ODE we solved for $P_{11}(t)$ is the variance propagation equation for this unstable error process. Since $\\alpha > 0$, the solution contains the term $\\exp(2\\alpha t)$, causing unbounded exponential growth. The filter is \"flying blind\" in an unstable, noisy direction, so its uncertainty in that direction must increase indefinitely. This is the hallmark of a system that is not detectable.", "answer": "$$\n\\boxed{p_{0} \\exp(2\\alpha t) + \\frac{q}{2\\alpha} (\\exp(2\\alpha t) - 1)}\n$$", "id": "2996460"}, {"introduction": "While the Kalman filter provides an elegant, exact solution, its applicability is limited to the domain of linear-Gaussian systems. This exercise [@problem_id:2996563] serves as a hands-on introduction to particle filters, a powerful class of numerical methods that can approximate posterior distributions in general non-linear, non-Gaussian scenarios. You will first derive the exact analytical solution for a simple linear-Gaussian model and then implement a basic particle filter to solve the same problem, allowing you to directly compare the numerical approximation to the true posterior and gain intuition for the power and challenges of sequential Monte Carlo methods.", "problem": "Consider the scalar linear-Gaussian state-space model, specified for a single time step, with latent state $x_t \\in \\mathbb{R}$ and observation $y_t \\in \\mathbb{R}$. The latent state evolves according to\n$$\nx_t = a\\,x_{t-1} + w_t,\n$$\nwhere $w_t \\sim \\mathcal{N}(0,q)$ and $q \\ge 0$, and the observation model is\n$$\ny_t = c\\,x_t + v_t,\n$$\nwhere $v_t \\sim \\mathcal{N}(0,r)$ and $r > 0$. Assume a Gaussian prior for the previous state,\n$$\nx_{t-1} \\sim \\mathcal{N}(m_{t-1}, P_{t-1}),\n$$\nwith $P_{t-1} \\ge 0$. All random variables $x_{t-1}$, $w_t$, and $v_t$ are mutually independent.\n\nYour tasks are:\n- From the fundamental definition of Bayesian inference and properties of the Gaussian distribution (without using any pre-stated filter formulas), derive the exact posterior mean $\\mathbb{E}[x_t \\mid y_t]$ for this model in terms of $a$, $q$, $c$, $r$, $m_{t-1}$, $P_{t-1}$, and the realized observation $y_t$.\n- Implement a single-step Sequential Importance Resampling (SIR) particle filtering estimator of $\\mathbb{E}[x_t \\mid y_t]$ that uses the state transition as proposal. Concretely:\n  1. Draw $N$ independent samples $x_{t-1}^{(i)} \\sim \\mathcal{N}(m_{t-1}, P_{t-1})$ for $i \\in \\{1,\\dots,N\\}$ (if $P_{t-1} = 0$, interpret this as $x_{t-1}^{(i)} = m_{t-1}$ deterministically).\n  2. Propagate each sample via the state dynamics: $x_t^{(i)} = a\\,x_{t-1}^{(i)} + w_t^{(i)}$ with $w_t^{(i)} \\sim \\mathcal{N}(0,q)$ (if $q = 0$, interpret this as $w_t^{(i)} = 0$ deterministically).\n  3. Compute importance weights proportional to the likelihood $p(y_t \\mid x_t^{(i)})$, i.e., $w^{(i)} \\propto \\exp\\!\\left(-\\tfrac{1}{2}\\tfrac{(y_t - c\\,x_t^{(i)})^2}{r}\\right)$, and normalize them to sum to $1$.\n  4. Form the particle-weighted mean $\\hat{m}_t^{\\text{PF}} = \\sum_{i=1}^N w^{(i)} x_t^{(i)}$.\n- Compute the absolute deviation $|\\hat{m}_t^{\\text{PF}} - \\mathbb{E}[x_t \\mid y_t]|$.\n\nUse the following test suite of parameter sets, each representing one independent run. For each case, $a$, $q$, $c$, $r$, $m_{t-1}$, $P_{t-1}$, $y_t$, and $N$ are provided:\n\n1. Case A (general, moderate noise): $a=0.9$, $q=0.5$, $c=1.2$, $r=0.7$, $m_{t-1}=0.3$, $P_{t-1}=1.1$, $y_t=0.8$, $N=5$.\n2. Case B (boundary: deterministic prior and dynamics, single particle): $a=1.0$, $q=0.0$, $c=1.0$, $r=0.1$, $m_{t-1}=-0.2$, $P_{t-1}=0.0$, $y_t=-0.15$, $N=1$.\n3. Case C (sign flip in coefficients, moderate process noise): $a=-0.5$, $q=0.2$, $c=-2.0$, $r=0.5$, $m_{t-1}=1.0$, $P_{t-1}=2.0$, $y_t=-0.7$, $N=10$.\n4. Case D (large observation noise, tiny particle set): $a=1.2$, $q=0.1$, $c=0.8$, $r=5.0$, $m_{t-1}=0.0$, $P_{t-1}=0.5$, $y_t=1.5$, $N=2$.\n5. Case E (nearly deterministic dynamics): $a=0.7$, $q=10^{-6}$, $c=1.0$, $r=0.2$, $m_{t-1}=2.0$, $P_{t-1}=0.3$, $y_t=2.1$, $N=4$.\n\nTo ensure reproducibility, initialize a pseudorandom number generator with base seed $s = 1729$, and for the $i$-th case in the above order (zero-based index $i \\in \\{0,1,2,3,4\\}$), use seed $s+i$ for sampling.\n\nYour program must:\n- For each case, compute the absolute deviation $|\\hat{m}_t^{\\text{PF}} - \\mathbb{E}[x_t \\mid y_t]|$ as a floating-point number.\n- Produce a single line of output containing all $5$ deviations in order as a comma-separated list enclosed in square brackets. Each number must be rounded to $10$ decimal places. For example, an output line must look like $[d_1,d_2,d_3,d_4,d_5]$ with each $d_j$ rounded to $10$ decimal places.\n\nNo physical units or angle units are involved in this problem; report plain real numbers as specified above.", "solution": "The problem is valid as it is scientifically grounded in the established theory of stochastic filtering, is well-posed, objective, and provides all necessary information for a unique, reproducible solution.\n\nThe primary objective is to derive the exact posterior mean $\\mathbb{E}[x_t \\mid y_t]$ for a scalar linear-Gaussian state-space model and compare it against a particle filter estimate. The derivation proceeds from the fundamental principles of Bayesian inference.\n\nBy Bayes' theorem, the posterior probability density function (PDF) of the state $x_t$ given the observation $y_t$ is proportional to the product of the likelihood and the prior:\n$$\np(x_t \\mid y_t) \\propto p(y_t \\mid x_t) p(x_t)\n$$\nWe must determine the two densities on the right-hand side, $p(x_t)$ and $p(y_t \\mid x_t)$.\n\nFirst, we derive the prior distribution for the state at time $t$, $p(x_t)$. This is also known as the predictive distribution, as it represents our belief about $x_t$ before incorporating the measurement $y_t$. The state evolves according to\n$$\nx_t = a\\,x_{t-1} + w_t\n$$\nWe are given that the distribution of the previous state is Gaussian, $x_{t-1} \\sim \\mathcal{N}(m_{t-1}, P_{t-1})$, and the process noise is also Gaussian, $w_t \\sim \\mathcal{N}(0, q)$. Since $x_{t-1}$ and $w_t$ are independent, the state $x_t$ is a linear combination of two independent Gaussian random variables. Therefore, $x_t$ itself is Gaussian. Its mean and variance are:\n$$\n\\mathbb{E}[x_t] = \\mathbb{E}[a\\,x_{t-1} + w_t] = a\\,\\mathbb{E}[x_{t-1}] + \\mathbb{E}[w_t] = a\\,m_{t-1} + 0 = a\\,m_{t-1}\n$$\n$$\n\\text{Var}(x_t) = \\text{Var}(a\\,x_{t-1} + w_t) = \\text{Var}(a\\,x_{t-1}) + \\text{Var}(w_t) = a^2\\,\\text{Var}(x_{t-1}) + \\text{Var}(w_t) = a^2 P_{t-1} + q\n$$\nLet us denote the predicted mean as $m_{t|t-1} = a\\,m_{t-1}$ and the predicted variance as $P_{t|t-1} = a^2 P_{t-1} + q$. Thus, the prior distribution for $x_t$ is:\n$$\np(x_t) = \\mathcal{N}(x_t; m_{t|t-1}, P_{t|t-1}) = \\frac{1}{\\sqrt{2\\pi P_{t|t-1}}} \\exp\\left(-\\frac{(x_t - m_{t|t-1})^2}{2P_{t|t-1}}\\right)\n$$\n\nSecond, we identify the likelihood function, $p(y_t \\mid x_t)$. The observation model is\n$$\ny_t = c\\,x_t + v_t\n$$\nwhere the measurement noise $v_t \\sim \\mathcal{N}(0, r)$. For a given state $x_t$, the observation $y_t$ is a Gaussian random variable with mean $\\mathbb{E}[y_t \\mid x_t] = \\mathbb{E}[c\\,x_t + v_t] = c\\,x_t$ and variance $\\text{Var}(y_t \\mid x_t) = \\text{Var}(c\\,x_t + v_t) = \\text{Var}(v_t) = r$. The likelihood is therefore:\n$$\np(y_t \\mid x_t) = \\mathcal{N}(y_t; c\\,x_t, r) = \\frac{1}{\\sqrt{2\\pi r}} \\exp\\left(-\\frac{(y_t - c\\,x_t)^2}{2r}\\right)\n$$\n\nNow, we combine the prior and the likelihood. The posterior $p(x_t \\mid y_t)$ is proportional to their product. Since the product of two Gaussian PDFs (as functions of $x_t$) results in another (unnormalized) Gaussian PDF, the posterior will be Gaussian. Let its mean be $m_t$ and its variance be $P_t$.\n$$\np(x_t \\mid y_t) \\propto \\exp\\left(-\\frac{(x_t - m_{t|t-1})^2}{2P_{t|t-1}}\\right) \\exp\\left(-\\frac{(y_t - c\\,x_t)^2}{2r}\\right)\n$$\nTo find the parameters $m_t$ and $P_t$ of the posterior distribution $\\mathcal{N}(x_t; m_t, P_t)$, we analyze the exponent of the product, dropping terms that do not depend on $x_t$:\n$$\nL(x_t) = -\\frac{1}{2P_{t|t-1}}(x_t^2 - 2 m_{t|t-1} x_t + \\dots) - \\frac{1}{2r}(c^2 x_t^2 - 2c y_t x_t + \\dots)\n$$\n$$\nL(x_t) = -\\frac{1}{2} \\left[ \\left(\\frac{1}{P_{t|t-1}} + \\frac{c^2}{r}\\right)x_t^2 - 2\\left(\\frac{m_{t|t-1}}{P_{t|t-1}} + \\frac{c y_t}{r}\\right)x_t \\right] + \\text{const.}\n$$\nThe exponent of a Gaussian PDF $\\mathcal{N}(x_t; m_t, P_t)$ is $-\\frac{1}{2P_t}(x_t - m_t)^2 + \\text{const.} = -\\frac{1}{2}\\left(\\frac{1}{P_t}x_t^2 - \\frac{2m_t}{P_t}x_t\\right) + \\text{const.}$. By comparing the coefficients of $x_t^2$ and $x_t$, we can identify $m_t$ and $P_t$.\n\nMatching the coefficient of $x_t^2$:\n$$\n\\frac{1}{P_t} = \\frac{1}{P_{t|t-1}} + \\frac{c^2}{r} \\implies P_t = \\left(\\frac{1}{P_{t|t-1}} + \\frac{c^2}{r}\\right)^{-1} = \\frac{P_{t|t-1} r}{r + c^2 P_{t|t-1}}\n$$\nThis is the posterior variance.\n\nMatching the coefficient of $x_t$:\n$$\n\\frac{m_t}{P_t} = \\frac{m_{t|t-1}}{P_{t|t-1}} + \\frac{c y_t}{r}\n$$\n$$\nm_t = P_t \\left( \\frac{m_{t|t-1}}{P_{t|t-1}} + \\frac{c y_t}{r} \\right) = \\frac{P_{t|t-1} r}{r + c^2 P_{t|t-1}} \\left( \\frac{m_{t|t-1}}{P_{t|t-1}} + \\frac{c y_t}{r} \\right)\n$$\n$$\nm_t = \\frac{r m_{t|t-1} + P_{t|t-1} c y_t}{r + c^2 P_{t|t-1}}\n$$\nThis formula gives the posterior mean $m_t = \\mathbb{E}[x_t \\mid y_t]$. To align with the standard Kalman filter formulation, we can define the Kalman gain $K_t$:\n$$\nK_t = \\frac{P_{t|t-1} c}{c^2 P_{t|t-1} + r}\n$$\nThen the posterior mean can be rewritten as:\n$$\nm_t = m_{t|t-1} + K_t (y_t - c m_{t|t-1})\n$$\n\nIn summary, the exact posterior mean $\\mathbb{E}[x_t \\mid y_t]$ is calculated via the following two-step process, which constitutes the scalar Kalman filter:\n1.  **Prediction:**\n    $m_{t|t-1} = a\\,m_{t-1}$\n    $P_{t|t-1} = a^2 P_{t-1} + q$\n2.  **Update:**\n    $m_t = m_{t|t-1} + \\frac{P_{t|t-1} c}{c^2 P_{t|t-1} + r} (y_t - c m_{t|t-1})$\n\nThe Sequential Importance Resampling (SIR) particle filter provides a numerical approximation to this posterior mean. It uses a set of $N$ weighted particles $\\{x_t^{(i)}, w^{(i)}\\}_{i=1}^N$ to represent the posterior distribution. The estimate of the mean is the weighted average of the particle locations: $\\hat{m}_t^{\\text{PF}} = \\sum_{i=1}^N w^{(i)} x_t^{(i)}$. The problem specifies using the state transition prior as the proposal distribution. This simplifies the weight calculation, making the weights proportional to the likelihood of the observation given the particle's state, $w^{(i)} \\propto p(y_t \\mid x_t^{(i)})$. The implementation will follow the specified steps of sampling, propagation, weighting, and estimation.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem by computing the absolute deviation between the exact\n    Kalman filter posterior mean and a particle filter estimate for several test cases.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case A: (a, q, c, r, m_tm1, P_tm1, y_t, N)\n        (0.9, 0.5, 1.2, 0.7, 0.3, 1.1, 0.8, 5),\n        # Case B\n        (1.0, 0.0, 1.0, 0.1, -0.2, 0.0, -0.15, 1),\n        # Case C\n        (-0.5, 0.2, -2.0, 0.5, 1.0, 2.0, -0.7, 10),\n        # Case D\n        (1.2, 0.1, 0.8, 5.0, 0.0, 0.5, 1.5, 2),\n        # Case E\n        (0.7, 1e-6, 1.0, 0.2, 2.0, 0.3, 2.1, 4),\n    ]\n\n    base_seed = 1729\n    deviations = []\n\n    for i, case in enumerate(test_cases):\n        a, q, c, r, m_tm1, P_tm1, y_t, N = case\n        \n        # --- 1. Exact Posterior Mean (Kalman Filter) ---\n        \n        # Prediction step\n        m_pred = a * m_tm1\n        P_pred = a**2 * P_tm1 + q\n        \n        # Update step\n        # Denominator of Kalman gain: c^2*P_pred + r. This is > 0 since r > 0.\n        kalman_gain = (c * P_pred) / (c**2 * P_pred + r)\n        exact_mean = m_pred + kalman_gain * (y_t - c * m_pred)\n\n        # --- 2. Particle Filter Estimator ---\n        \n        # Set seed for reproducibility for this case\n        rng = np.random.default_rng(base_seed + i)\n        \n        # Step 1: Draw N samples from the prior N(m_{t-1}, P_{t-1})\n        # If P_tm1 is 0, scale=0 correctly produces deterministic samples m_tm1.\n        x_tm1_samples = rng.normal(loc=m_tm1, scale=np.sqrt(P_tm1), size=N)\n        \n        # Step 2: Propagate samples through state dynamics\n        # If q is 0, scale=0 correctly produces zero noise.\n        w_t_samples = rng.normal(loc=0, scale=np.sqrt(q), size=N)\n        x_t_samples = a * x_tm1_samples + w_t_samples\n\n        # Step 3: Compute and normalize importance weights\n        # Weights are proportional to the likelihood p(y_t | x_t^(i))\n        # Log-weights to avoid numerical underflow\n        log_weights = -0.5 * ((y_t - c * x_t_samples)**2) / r\n        \n        # Exponentiate safely by subtracting the max log-weight\n        if len(log_weights) > 0:\n            max_log_weight = np.max(log_weights)\n            unnorm_weights = np.exp(log_weights - max_log_weight)\n            sum_weights = np.sum(unnorm_weights)\n            \n            # Normalize weights\n            if sum_weights > 0:\n                norm_weights = unnorm_weights / sum_weights\n            else:\n                # Should not happen with non-zero r, but as a safeguard.\n                # Assign uniform weights if all are zero.\n                norm_weights = np.full(N, 1.0/N)\n        else: # Case with N=0 particles, not in test suite\n            norm_weights = np.array([])\n            \n        # Step 4: Form the particle-weighted mean\n        if N > 0:\n            pf_mean = np.sum(norm_weights * x_t_samples)\n        else:\n            pf_mean = 0.0 # Define for empty case\n\n        # --- 3. Compute Absolute Deviation ---\n        deviation = np.abs(pf_mean - exact_mean)\n        deviations.append(deviation)\n\n    # Format the final output string\n    formatted_results = [f\"{d:.10f}\" for d in deviations]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "2996563"}]}