{"hands_on_practices": [{"introduction": "Before we can integrate with respect to a random measure, we must first understand the measure itself. This foundational exercise connects the abstract definition of a Poisson random measure to the familiar Poisson distribution. By calculating the expectation and variance of the number of jumps in a given set, you will solidify your understanding of how the intensity measure governs the statistical properties of the jump process [@problem_id:2997824].", "problem": "Let $\\left(\\Omega,\\mathcal{F},\\left(\\mathcal{F}_{t}\\right)_{t\\geq 0},\\mathbb{P}\\right)$ be a filtered probability space satisfying the usual conditions. Let $(E,\\mathcal{E})$ be a measurable space equipped with a $\\sigma$-finite measure $\\nu$ on $(E,\\mathcal{E})$. Consider an integer-valued random measure $N$ on $(0,\\infty)\\times E$ that is a Poisson random measure with compensator (also called intensity measure) $\\mu(\\mathrm{d}s,\\mathrm{d}z)=\\mathrm{d}s\\,\\nu(\\mathrm{d}z)$ relative to $\\left(\\mathcal{F}_{t}\\right)_{t\\geq 0}$. That is, for any $A\\in\\mathcal{B}((0,\\infty))\\otimes\\mathcal{E}$ with $\\mu(A)<\\infty$, the random variable $N(A)$ is Poisson distributed with parameter $\\mu(A)$, and for disjoint $A_{1},\\dots,A_{n}$ the random variables $N(A_{1}),\\dots,N(A_{n})$ are independent. Let the compensated measure be $\\tilde{N}(\\mathrm{d}s,\\mathrm{d}z):=N(\\mathrm{d}s,\\mathrm{d}z)-\\mathrm{d}s\\,\\nu(\\mathrm{d}z)$.\n\nFix $t>0$ and let $B\\in\\mathcal{E}$ satisfy $0<\\nu(B)<\\infty$. Using only foundational properties of Poisson random measures and their compensators, compute the expectation $\\mathbb{E}\\left[N\\left((0,t]\\times B\\right)\\right]$ and the variance $\\operatorname{Var}\\left[N\\left((0,t]\\times B\\right)\\right]$ explicitly in terms of $t$ and $\\nu(B)$. Express your final answer as a single closed-form analytical expression. No rounding is required.", "solution": "The problem is well-posed and scientifically grounded within the mathematical theory of stochastic processes. All necessary definitions and conditions are provided, and there are no contradictions. We may proceed with the solution.\n\nThe problem asks for the expectation and variance of the random variable $X = N\\left((0,t]\\times B\\right)$, where $N$ is a Poisson random measure on $(0,\\infty)\\times E$ with compensator, or intensity measure, $\\mu(\\mathrm{d}s,\\mathrm{d}z)=\\mathrm{d}s\\,\\nu(\\mathrm{d}z)$. The parameters $t$ and $B$ are fixed, with $t>0$ and $B\\in\\mathcal{E}$ satisfying $0<\\nu(B)<\\infty$.\n\nAccording to the foundational properties of a Poisson random measure as stated in the problem, for any measurable set $A \\in \\mathcal{B}((0,\\infty)) \\otimes \\mathcal{E}$ such that its intensity $\\mu(A)$ is finite, the random variable $N(A)$ follows a Poisson distribution with parameter $\\mu(A)$. We denote this as $N(A)\\sim \\text{Poisson}(\\mu(A))$.\n\nLet us identify the specific set for this problem. The set is $A = (0,t]\\times B$. First, we must calculate the intensity measure of this set, which will be the parameter of the Poisson distribution.\nThe intensity measure is given by $\\mu(A) = \\iint_A \\mathrm{d}s\\,\\nu(\\mathrm{d}z)$.\nFor our specific set $A=(0,t]\\times B$, this becomes:\n$$ \\mu\\left((0,t]\\times B\\right) = \\iint_{(0,t]\\times B} \\mathrm{d}s\\,\\nu(\\mathrm{d}z) $$\nThe domain of integration is a rectangular region in the product space $(0,\\infty)\\times E$. We can apply Fubini's theorem to separate the integral:\n$$ \\mu\\left((0,t]\\times B\\right) = \\int_{0}^{t} \\left( \\int_{B} \\nu(\\mathrm{d}z) \\right) \\mathrm{d}s $$\nThe inner integral, $\\int_{B} \\nu(\\mathrm{d}z)$, is by definition the measure of the set $B$, which is denoted by $\\nu(B)$. Therefore, the expression simplifies to:\n$$ \\mu\\left((0,t]\\times B\\right) = \\int_{0}^{t} \\nu(B) \\, \\mathrm{d}s $$\nSince $\\nu(B)$ is a constant with respect to the integration variable $s$, we can move it outside the integral:\n$$ \\mu\\left((0,t]\\times B\\right) = \\nu(B) \\int_{0}^{t} \\mathrm{d}s $$\nThe definite integral of $\\mathrm{d}s$ from $0$ to $t$ is simply $t$.\n$$ \\mu\\left((0,t]\\times B\\right) = \\nu(B) \\cdot [s]_{0}^{t} = \\nu(B) \\cdot (t-0) = t\\nu(B) $$\nThe problem specifies that $t>0$ and $0 < \\nu(B) < \\infty$, which ensures that $t\\nu(B)$ is a finite positive real number. This satisfies the condition $\\mu(A)<\\infty$.\n\nLet $\\lambda = t\\nu(B)$. From the definition of the Poisson random measure, the random variable $X = N\\left((0,t]\\times B\\right)$ is Poisson distributed with parameter $\\lambda$.\n$$ X \\sim \\text{Poisson}(\\lambda) \\quad \\text{where} \\quad \\lambda = t\\nu(B) $$\nFor a random variable $Y$ that follows a Poisson distribution with parameter $\\lambda$, its expectation and variance are given by the standard formulas:\n$$ \\mathbb{E}[Y] = \\lambda $$\n$$ \\operatorname{Var}(Y) = \\lambda $$\nApplying these formulas to our random variable $X = N\\left((0,t]\\times B\\right)$ with parameter $\\lambda = t\\nu(B)$, we obtain the required quantities.\n\nThe expectation is:\n$$ \\mathbb{E}\\left[N\\left((0,t]\\times B\\right)\\right] = t\\nu(B) $$\nThe variance is:\n$$ \\operatorname{Var}\\left[N\\left((0,t]\\times B\\right)\\right] = t\\nu(B) $$\nBoth the expectation and the variance are equal to $t\\nu(B)$. The problem asks for a single closed-form analytical expression for both results. We will present them as a row matrix.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\nt\\nu(B) & t\\nu(B)\n\\end{pmatrix}\n}\n$$", "id": "2997824"}, {"introduction": "With a firm grasp of the Poisson random measure, we can now construct the stochastic integral. This practice serves as the blueprint for this construction, starting with the most basic integrands—simple predictable processes [@problem_id:2997789]. You will explicitly build the integral with respect to both the random measure $N$ and its compensated, zero-mean version $\\tilde{N}$, revealing the fundamental structure of these new stochastic objects.", "problem": "Consider a complete probability space $(\\Omega,\\mathcal{F},\\mathbb{P})$ equipped with a filtration $(\\mathcal{F}_t)_{t\\geq 0}$ satisfying the usual conditions. Let $(E,\\mathcal{E})$ be a measurable space and let $\\nu$ be a $\\sigma$-finite measure on $(E,\\mathcal{E})$. Let $N(\\mathrm{d}t,\\mathrm{d}x)$ be a Poisson random measure (PRM) on $\\mathbb{R}_{+}\\times E$ with intensity measure $\\mu(\\mathrm{d}t,\\mathrm{d}x)=\\mathrm{d}t\\,\\nu(\\mathrm{d}x)$, adapted to $(\\mathcal{F}_t)_{t\\geq 0}$, and let the compensated Poisson random measure be $\\tilde{N}(\\mathrm{d}t,\\mathrm{d}x) = N(\\mathrm{d}t,\\mathrm{d}x)-\\mathrm{d}t\\,\\nu(\\mathrm{d}x)$. Assume $N$ has independent increments over disjoint sets and for any measurable set $A\\subset \\mathbb{R}_{+}\\times E$ with $\\mu(A)<\\infty$, $N(A)$ is Poisson distributed with parameter $\\mu(A)$. Let $T>0$ and $n\\in\\mathbb{N}$ be fixed, and suppose there are times $0\\leq s_k<u_k\\leq T$, sets $B_k\\in\\mathcal{E}$ with $\\nu(B_k)<\\infty$, and random variables $\\xi_k$ that are $\\mathcal{F}_{s_k}$-measurable, for $k=1,\\dots,n$. Define the simple process\n$$\nH(\\omega,t,x)=\\sum_{k=1}^{n}\\xi_k(\\omega)\\,\\mathbf{1}_{(s_k,u_k]}(t)\\,\\mathbf{1}_{B_k}(x),\n$$\nand assume that the rectangles $(s_k,u_k]\\times B_k$ are pairwise disjoint, and that $\\mathbb{E}[|\\xi_k|]<\\infty$ and $\\mathbb{E}[\\xi_k^{2}]<\\infty$ for each $k=1,\\dots,n$.\n\nStarting from the foundational definitions of the predictable $\\sigma$-algebra, the Poisson random measure, and its compensator, establish that the stochastic integrals\n$$\n\\int_{0}^{T}\\int_{E} H(\\omega,t,x)\\,N(\\mathrm{d}t,\\mathrm{d}x)\n\\quad\\text{and}\\quad\n\\int_{0}^{T}\\int_{E} H(\\omega,t,x)\\,\\tilde{N}(\\mathrm{d}t,\\mathrm{d}x)\n$$\nare well-defined random variables, and compute both integrals explicitly in terms of $N$ and $\\nu$. Your final answer must be a single analytic expression containing both computed integrals, written as a row matrix using the $\\mathrm{pmatrix}$ environment. No numerical approximation is required.", "solution": "The problem is subjected to validation prior to any attempt at a solution.\n\n### Step 1: Extract Givens\n-   A complete probability space $(\\Omega,\\mathcal{F},\\mathbb{P})$ with a filtration $(\\mathcal{F}_t)_{t\\geq 0}$ satisfying the usual conditions.\n-   A measurable space $(E,\\mathcal{E})$.\n-   A $\\sigma$-finite measure $\\nu$ on $(E,\\mathcal{E})$.\n-   A Poisson random measure (PRM) $N(\\mathrm{d}t,\\mathrm{d}x)$ on $\\mathbb{R}_{+}\\times E$ with intensity measure $\\mu(\\mathrm{d}t,\\mathrm{d}x)=\\mathrm{d}t\\,\\nu(\\mathrm{d}x)$. $N$ is adapted to $(\\mathcal{F}_t)_{t\\geq 0}$ and has independent increments. For a measurable set $A\\subset \\mathbb{R}_{+}\\times E$ with $\\mu(A)<\\infty$, $N(A)$ is Poisson distributed with parameter $\\mu(A)$.\n-   The compensated Poisson random measure is $\\tilde{N}(\\mathrm{d}t,\\mathrm{d}x) = N(\\mathrm{d}t,\\mathrm{d}x)-\\mathrm{d}t\\,\\nu(\\mathrm{d}x)$.\n-   Fixed constants $T>0$ and $n\\in\\mathbb{N}$.\n-   Times $0\\leq s_k<u_k\\leq T$ for $k=1,\\dots,n$.\n-   Sets $B_k\\in\\mathcal{E}$ with $\\nu(B_k)<\\infty$ for $k=1,\\dots,n$.\n-   Random variables $\\xi_k$ that are $\\mathcal{F}_{s_k}$-measurable for $k=1,\\dots,n$.\n-   A simple process $H(\\omega,t,x)=\\sum_{k=1}^{n}\\xi_k(\\omega)\\,\\mathbf{1}_{(s_k,u_k]}(t)\\,\\mathbf{1}_{B_k}(x)$.\n-   The rectangles $(s_k,u_k]\\times B_k$ are pairwise disjoint for $k=1,\\dots,n$.\n-   For each $k=1,\\dots,n$, $\\mathbb{E}[|\\xi_k|]<\\infty$ and $\\mathbb{E}[\\xi_k^{2}]<\\infty$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is assessed for validity.\n1.  **Scientifically Grounded**: The problem is set within the standard mathematical framework of stochastic calculus, specifically the theory of integration with respect to random measures. The definitions of a Poisson random measure, its compensator, and the construction of the stochastic integral for simple processes are all standard and mathematically rigorous concepts. The setup is a canonical example used in the development of the theory of Lévy processes.\n2.  **Well-Posed**: The problem is well-posed. It asks for the explicit calculation of two well-defined stochastic integrals for a specific class of integrands (simple predictable processes). The givens are sufficient and consistent, leading to a unique and meaningful solution.\n3.  **Objective**: The problem statement is written in precise, formal mathematical language, free of any ambiguity, subjectivity, or opinion.\n\nThe problem does not exhibit any flaws such as scientific unsoundness, missing information, or contradictions. All terms are standard in the field of stochastic differential equations. The assumptions on the process $H$ and the random variables $\\xi_k$ are precisely what is required to ensure the integrals are well-defined.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. A full solution will be provided.\n\n### Solution Derivation\nThe task is to establish that the stochastic integrals of the process $H(\\omega,t,x)$ with respect to the Poisson random measure $N(\\mathrm{d}t,\\mathrm{d}x)$ and its compensated version $\\tilde{N}(\\mathrm{d}t,\\mathrm{d}x)$ are well-defined, and then to compute them.\n\nThe process $H$ is a simple predictable process. A process is predictable if, as a mapping from $\\Omega \\times \\mathbb{R}_+ \\times E$ to $\\mathbb{R}$, it is measurable with respect to the predictable $\\sigma$-algebra $\\mathcal{P}$ on $\\Omega \\times \\mathbb{R}_+$ tensored with $\\mathcal{E}$. The predictable $\\sigma$-algebra $\\mathcal{P}$ is generated by left-continuous adapted processes. An elementary predictable process has the form $X(\\omega,t,x) = \\zeta(\\omega) \\mathbf{1}_{(s,u]}(t) \\mathbf{1}_B(x)$ where $s < u$, $B \\in \\mathcal{E}$, and $\\zeta$ is an $\\mathcal{F}_s$-measurable random variable. The given process $H$ is a finite sum of such elementary predictable processes, and is therefore a simple predictable process.\n\nFor the stochastic integrals to be well-defined, certain integrability conditions must be met.\n\nThe integral with respect to the compensator, $\\int_0^T \\int_E H(\\omega,t,x) \\,\\mathrm{d}t\\nu(\\mathrm{d}x)$, must be almost surely finite. This is a necessary condition for the integral with respect to $N$ to be well-defined. Usually, a stronger condition is required for the general theory, such as $\\mathbb{E}\\left[\\int_0^T\\int_E |H(t,x)| \\,\\mathrm{d}t\\nu(\\mathrm{d}x)\\right] < \\infty$. Let us verify this:\n$$\n\\mathbb{E}\\left[\\int_0^T\\int_E |H(\\omega,t,x)| \\,\\mathrm{d}t\\nu(\\mathrm{d}x)\\right] = \\mathbb{E}\\left[\\int_0^T\\int_E \\left|\\sum_{k=1}^{n}\\xi_k(\\omega)\\,\\mathbf{1}_{(s_k,u_k]}(t)\\,\\mathbf{1}_{B_k}(x)\\right| \\,\\mathrm{d}t\\nu(\\mathrm{d}x)\\right]\n$$\nSince the sets (rectangles) $R_k = (s_k,u_k]\\times B_k$ are pairwise disjoint, the sum at any point $(\\omega,t,x)$ contains at most one non-zero term. Thus, the absolute value of the sum is the sum of the absolute values:\n$$\n\\mathbb{E}\\left[\\int_0^T\\int_E \\sum_{k=1}^{n}|\\xi_k(\\omega)|\\,\\mathbf{1}_{(s_k,u_k]}(t)\\,\\mathbf{1}_{B_k}(x) \\,\\mathrm{d}t\\nu(\\mathrm{d}x)\\right]\n$$\nBy the Tonelli-Fubini theorem, we can interchange expectation and integration, and summation and integration:\n$$\n= \\sum_{k=1}^{n} \\mathbb{E}[|\\xi_k|] \\left(\\int_0^T \\mathbf{1}_{(s_k,u_k]}(t)\\,\\mathrm{d}t\\right) \\left(\\int_E \\mathbf{1}_{B_k}(x)\\,\\nu(\\mathrm{d}x)\\right)\n= \\sum_{k=1}^{n} \\mathbb{E}[|\\xi_k|] (u_k-s_k) \\nu(B_k)\n$$\nGiven that $\\mathbb{E}[|\\xi_k|]<\\infty$ and $\\nu(B_k)<\\infty$ for all $k$, this sum is finite. Thus, the integral with respect to $N$ can be defined.\n\nFor the integral with respect to the compensated measure $\\tilde{N}$ to be a square-integrable martingale, which is the standard construction, we require the condition $\\mathbb{E}\\left[\\int_0^T\\int_E H(t,x)^2 \\,\\mathrm{d}t\\nu(\\mathrm{d}x)\\right] < \\infty$. Let us verify this:\n$$\n\\mathbb{E}\\left[\\int_0^T\\int_E H(\\omega,t,x)^2 \\,\\mathrm{d}t\\nu(\\mathrm{d}x)\\right] = \\mathbb{E}\\left[\\int_0^T\\int_E \\left(\\sum_{k=1}^{n}\\xi_k(\\omega)\\,\\mathbf{1}_{(s_k,u_k]}(t)\\,\\mathbf{1}_{B_k}(x)\\right)^2 \\,\\mathrm{d}t\\nu(\\mathrm{d}x)\\right]\n$$\nAgain, due to the disjointness of the supports, the square of the sum is the sum of the squares:\n$$\n= \\mathbb{E}\\left[\\int_0^T\\int_E \\sum_{k=1}^{n}\\xi_k(\\omega)^2\\,\\mathbf{1}_{(s_k,u_k]}(t)\\,\\mathbf{1}_{B_k}(x) \\,\\mathrm{d}t\\nu(\\mathrm{d}x)\\right]\n$$\n$$\n= \\sum_{k=1}^{n} \\mathbb{E}[\\xi_k^2] \\left(\\int_{s_k}^{u_k}\\,\\mathrm{d}t\\right) \\left(\\int_{B_k}\\,\\nu(\\mathrm{d}x)\\right)\n= \\sum_{k=1}^{n} \\mathbb{E}[\\xi_k^2] (u_k-s_k) \\nu(B_k)\n$$\nGiven $\\mathbb{E}[\\xi_k^2]<\\infty$ and $\\nu(B_k)<\\infty$, this sum is also finite. Therefore, both stochastic integrals are well-defined random variables.\n\nWe now compute the integrals explicitly. The stochastic integral is defined linearly. For an elementary predictable process $\\xi(\\omega)\\,\\mathbf{1}_{(s,u]}(t)\\,\\mathbf{1}_{B}(x)$, the integral is defined as $\\xi(\\omega) \\int_0^T \\int_E \\mathbf{1}_{(s,u]}(t)\\,\\mathbf{1}_{B}(x) \\, N(\\mathrm{d}t,\\mathrm{d}x) = \\xi(\\omega)N((s,u]\\times B)$.\n\nFor the integral with respect to $N$:\n$$\n\\int_{0}^{T}\\int_{E} H(\\omega,t,x)\\,N(\\mathrm{d}t,\\mathrm{d}x) = \\int_{0}^{T}\\int_{E} \\left(\\sum_{k=1}^{n}\\xi_k(\\omega)\\,\\mathbf{1}_{(s_k,u_k]}(t)\\,\\mathbf{1}_{B_k}(x)\\right)\\,N(\\mathrm{d}t,\\mathrm{d}x)\n$$\nBy the linearity of the stochastic integral operator:\n$$\n= \\sum_{k=1}^{n} \\int_{0}^{T}\\int_{E} \\xi_k(\\omega)\\,\\mathbf{1}_{(s_k,u_k]}(t)\\,\\mathbf{1}_{B_k}(x)\\,N(\\mathrm{d}t,\\mathrm{d}x)\n$$\nSince $\\xi_k$ is $\\mathcal{F}_{s_k}$-measurable, it can be pulled out of the integral over the time interval $(s_k, u_k]$:\n$$\n= \\sum_{k=1}^{n} \\xi_k(\\omega) \\int_{0}^{T}\\int_{E} \\mathbf{1}_{(s_k,u_k]}(t)\\,\\mathbf{1}_{B_k}(x)\\,N(\\mathrm{d}t,\\mathrm{d}x)\n$$\nThe integral of an indicator function over a set with respect to a measure is the measure of that set. In this case, the integral is $N((s_k,u_k]\\times B_k)$.\n$$\n\\int_{0}^{T}\\int_{E} H(\\omega,t,x)\\,N(\\mathrm{d}t,\\mathrm{d}x) = \\sum_{k=1}^{n} \\xi_k(\\omega)N((s_k,u_k]\\times B_k)\n$$\n\nFor the integral with respect to $\\tilde{N}$:\nWe use the definition $\\tilde{N}(\\mathrm{d}t,\\mathrm{d}x) = N(\\mathrm{d}t,\\mathrm{d}x) - \\mathrm{d}t\\,\\nu(\\mathrm{d}x)$ and the linearity of the integral.\n$$\n\\int_{0}^{T}\\int_{E} H(\\omega,t,x)\\,\\tilde{N}(\\mathrm{d}t,\\mathrm{d}x) = \\int_{0}^{T}\\int_{E} H(\\omega,t,x)\\,N(\\mathrm{d}t,\\mathrm{d}x) - \\int_{0}^{T}\\int_{E} H(\\omega,t,x)\\,\\mathrm{d}t\\,\\nu(\\mathrm{d}x)\n$$\nThe first term is the integral with respect to $N$, which we have just calculated. The second term is the integral with respect to the compensator measure $\\mu(\\mathrm{d}t,\\mathrm{d}x)=\\mathrm{d}t\\nu(\\mathrm{d}x)$, which is a standard Lebesgue-Stieltjes integral for each $\\omega$.\n$$\n\\int_{0}^{T}\\int_{E} H(\\omega,t,x)\\,\\mathrm{d}t\\,\\nu(\\mathrm{d}x) = \\int_{0}^{T}\\int_{E} \\left(\\sum_{k=1}^{n}\\xi_k(\\omega)\\,\\mathbf{1}_{(s_k,u_k]}(t)\\,\\mathbf{1}_{B_k}(x)\\right)\\,\\mathrm{d}t\\,\\nu(\\mathrm{d}x)\n$$\nUsing the linearity of this integral and Fubini's theorem:\n$$\n= \\sum_{k=1}^{n} \\xi_k(\\omega) \\left(\\int_{0}^{T} \\mathbf{1}_{(s_k,u_k]}(t)\\,\\mathrm{d}t\\right) \\left(\\int_{E} \\mathbf{1}_{B_k}(x)\\,\\nu(\\mathrm{d}x)\\right)\n= \\sum_{k=1}^{n} \\xi_k(\\omega) (u_k-s_k)\\nu(B_k)\n$$\nCombining the two parts gives the integral with respect to $\\tilde{N}$:\n$$\n\\int_{0}^{T}\\int_{E} H(\\omega,t,x)\\,\\tilde{N}(\\mathrm{d}t,\\mathrm{d}x) = \\sum_{k=1}^{n} \\xi_k(\\omega)N((s_k,u_k]\\times B_k) - \\sum_{k=1}^{n} \\xi_k(\\omega)(u_k-s_k)\\nu(B_k)\n$$\nThis can be written more compactly as:\n$$\n= \\sum_{k=1}^{n} \\xi_k(\\omega) \\left[ N((s_k,u_k]\\times B_k) - (u_k-s_k)\\nu(B_k) \\right]\n$$\nThe term in the square brackets is precisely $\\tilde{N}((s_k,u_k]\\times B_k)$.\n\nThe two computed integrals are:\n1.  $\\int_{0}^{T}\\int_{E} H(\\mathrm{d}t,\\mathrm{d}x) = \\sum_{k=1}^{n} \\xi_k N((s_k,u_k]\\times B_k)$\n2.  $\\int_{0}^{T}\\int_{E} H(\\mathrm{d}t,\\mathrm{d}x) = \\sum_{k=1}^{n} \\xi_k \\left[ N((s_k,u_k]\\times B_k) - (u_k-s_k)\\nu(B_k) \\right]$\nThese are the final explicit expressions.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\sum_{k=1}^{n} \\xi_k N((s_k,u_k]\\times B_k) & \\sum_{k=1}^{n} \\xi_k \\left( N((s_k,u_k]\\times B_k) - (u_k-s_k)\\nu(B_k) \\right)\n\\end{pmatrix}\n}\n$$", "id": "2997789"}, {"introduction": "Stochastic calculus often involves understanding the interaction between different random processes, which is captured by their quadratic covariation. This advanced practice guides you through the derivation of the predictable quadratic covariation $\\langle M, L \\rangle_{t}$ for two martingales driven by the same jump measure [@problem_id:2997817]. This formula is not only the cornerstone for applying Itô's lemma to jump processes but also provides a rigorous tool to establish when two such processes are orthogonal.", "problem": "Let $(\\Omega,\\mathcal{F},(\\mathcal{F}_t)_{t\\ge 0},\\mathbb{P})$ be a filtered probability space satisfying the usual conditions. Let $E=\\mathbb{R}\\setminus\\{0\\}$ equipped with its Borel $\\sigma$-algebra. Let $N(\\mathrm{d}s,\\mathrm{d}x)$ be a Poisson random measure on $\\mathbb{R}_{+}\\times E$ with compensator $\\nu(\\mathrm{d}x)\\,\\mathrm{d}s$, where $\\nu$ is a $\\sigma$-finite measure on $E$. Define the compensated Poisson random measure $\\tilde{N}(\\mathrm{d}s,\\mathrm{d}x)=N(\\mathrm{d}s,\\mathrm{d}x)-\\nu(\\mathrm{d}x)\\,\\mathrm{d}s$. For a predictable function $G:\\Omega\\times\\mathbb{R}_{+}\\times E\\to\\mathbb{R}$ such that $\\mathbb{E}\\big[\\int_{0}^{t}\\int_{E}|G(s,x)|^{2}\\,\\nu(\\mathrm{d}x)\\,\\mathrm{d}s\\big]<\\infty$ for all $t\\ge 0$, the stochastic integral $M^{G}_{t}=\\int_{0}^{t}\\int_{E}G(s,x)\\,\\tilde{N}(\\mathrm{d}s,\\mathrm{d}x)$ defines a square-integrable purely discontinuous martingale.\n\nConsider two such predictable integrands $H$ and $K$, and define $M_{t}=\\int_{0}^{t}\\int_{E}H(s,x)\\,\\tilde{N}(\\mathrm{d}s,\\mathrm{d}x)$ and $L_{t}=\\int_{0}^{t}\\int_{E}K(s,x)\\,\\tilde{N}(\\mathrm{d}s,\\mathrm{d}x)$. Your tasks are:\n\n- Derive, from first principles and starting from the definition of quadratic covariation for purely discontinuous local martingales as the sum of products of simultaneous jumps, a closed-form expression for the predictable quadratic covariation $\\langle M,L\\rangle_{t}$ in terms of $H$, $K$, and the compensator $\\nu(\\mathrm{d}x)\\,\\mathrm{d}s$.\n\n- Now specialize to the case where $\\nu(\\mathrm{d}x)=\\lambda \\exp(-|x|)\\,\\mathrm{d}x$ on $E$, with a fixed constant $\\lambda>0$. Let $a,b\\in\\mathbb{R}$ and $\\alpha,\\gamma>-1$ be fixed constants, and define\n$$\nH(s,x)=a\\,s^{\\alpha}\\,x\\,\\mathbf{1}_{\\{x>0\\}},\\qquad K(s,x)=b\\,s^{\\gamma}\\,x^{2}\\,\\mathbf{1}_{\\{x>0\\}}.\n$$\nCompute the closed-form analytic expression for $\\langle M,L\\rangle_{t}$ as a function of $t$, $\\alpha$, $\\gamma$, $a$, $b$, and $\\lambda$.\n\n- Finally, replace $K$ by $K^{-}(s,x)=b\\,s^{\\gamma}\\,x^{2}\\,\\mathbf{1}_{\\{x<0\\}}$ and use your general expression for $\\langle M,L\\rangle_{t}$ to justify rigorously that $M$ and $L^{-}$ are orthogonal martingales.\n\nProvide as your final answer the closed-form expression for $\\langle M,L\\rangle_{t}$ obtained in the specialized case with $K(s,x)=b\\,s^{\\gamma}\\,x^{2}\\,\\mathbf{1}_{\\{x>0\\}}$. No numerical evaluation or rounding is required, and no physical units are involved. Assume $\\alpha+\\gamma>-1$ so that all integrals are finite.", "solution": "The problem is assessed to be valid. It is scientifically grounded within the established mathematical theory of stochastic processes, specifically concerning stochastic integration with respect to Poisson random measures. The problem is well-posed, with all necessary definitions and conditions provided for a unique and meaningful solution to be derived. The language is objective and formal. Therefore, I will proceed with the solution.\n\nThe solution is structured in three parts as requested by the problem statement. First, the general formula for the predictable quadratic covariation is derived from first principles. Second, this formula is applied to the specific case given. Third, the concept of orthogonality is demonstrated.\n\nThe martingales are given by $M_{t}=\\int_{0}^{t}\\int_{E}H(s,x)\\,\\tilde{N}(\\mathrm{d}s,\\mathrm{d}x)$ and $L_{t}=\\int_{0}^{t}\\int_{E}K(s,x)\\,\\tilde{N}(\\mathrm{d}s,\\mathrm{d}x)$. They are purely discontinuous square-integrable martingales.\n\nFirst part: Derivation of the general formula for $\\langle M,L\\rangle_t$.\nThe quadratic covariation process, $[M, L]_t$, for two purely discontinuous local martingales is defined as the sum of the products of their simultaneous jumps:\n$$ [M, L]_t = \\sum_{0 < s \\le t} \\Delta M_s \\Delta L_s $$\nwhere $\\Delta X_s = X_s - X_{s-}$ denotes the jump of a process $X$ at time $s$. The jumps of a stochastic integral with respect to a compensated Poisson random measure occur at the same times as the jumps of the underlying Poisson process. The jump size at time $s$ is given by an integral with respect to the Poisson random measure over the singleton set $\\{s\\}$:\n$$ \\Delta M_s = \\int_{E}H(s,x)\\,N(\\{s\\},\\mathrm{d}x) \\qquad \\text{and} \\qquad \\Delta L_s = \\int_{E}K(s,x)\\,N(\\{s\\},\\mathrm{d}x) $$\nThe Poisson point process underlying the random measure $N$ has the property that at any fixed time $s$, if a jump occurs, its size is a single point $J_s \\in E$. In this case, the measure $N(\\{s\\}, \\cdot)$ is a Dirac measure at $J_s$, i.e., $N(\\{s\\},\\mathrm{d}x) = \\delta_{J_s}(\\mathrm{d}x)$. If no jump occurs at time $s$, $N(\\{s\\}, \\cdot)$ is the zero measure. Thus, the jump sizes are $\\Delta M_s = H(s, J_s)$ and $\\Delta L_s = K(s, J_s)$ if a jump of size $J_s$ occurs at time $s$, and $\\Delta M_s = 0$ and $\\Delta L_s = 0$ otherwise.\n\nThe sum over jump times can be re-expressed as a stochastic integral with respect to the Poisson random measure $N(\\mathrm{d}s, \\mathrm{d}x)$ itself, which counts the jumps at time $s$ with size in $\\mathrm{d}x$:\n$$ [M, L]_t = \\int_{0}^{t}\\int_{E} H(s,x) K(s,x) \\,N(\\mathrm{d}s,\\mathrm{d}x) $$\nThe predictable quadratic covariation, denoted $\\langle M,L\\rangle_t$, is defined as the compensator of the quadratic covariation process $[M, L]_t$. The process $\\langle M,L\\rangle_t$ is the unique predictable process of finite variation such that $[M, L]_t - \\langle M,L\\rangle_t$ is a local martingale. For a stochastic integral with respect to $N$ of a predictable process $C(s,x)$, the compensator is the integral of $C(s,x)$ with respect to the compensator measure of $N$, which is $\\nu(\\mathrm{d}x)\\,\\mathrm{d}s$. Since $H$ and $K$ are predictable, their product $H(s,x)K(s,x)$ is also a predictable process. Thus, we find the general expression for the predictable quadratic covariation:\n$$ \\langle M,L\\rangle_{t} = \\int_{0}^{t}\\int_{E} H(s,x)K(s,x)\\,\\nu(\\mathrm{d}x)\\,\\mathrm{d}s $$\n\nSecond part: Specialization and computation.\nWe are given the specific forms for the Lévy measure $\\nu$ and the integrands $H$ and $K$:\n$$ \\nu(\\mathrm{d}x)=\\lambda \\exp(-|x|)\\,\\mathrm{d}x $$\n$$ H(s,x)=a\\,s^{\\alpha}\\,x\\,\\mathbf{1}_{\\{x>0\\}}, \\qquad K(s,x)=b\\,s^{\\gamma}\\,x^{2}\\,\\mathbf{1}_{\\{x>0\\}} $$\nwhere $a, b \\in \\mathbb{R}$, $\\lambda > 0$, and $\\alpha, \\gamma > -1$. We first compute the product of the integrands:\n$$ H(s,x)K(s,x) = \\left(a\\,s^{\\alpha}\\,x\\,\\mathbf{1}_{\\{x>0\\}}\\right) \\left(b\\,s^{\\gamma}\\,x^{2}\\,\\mathbf{1}_{\\{x>0\\}}\\right) = ab\\,s^{\\alpha+\\gamma}x^{3}\\,(\\mathbf{1}_{\\{x>0\\}})^2 = ab\\,s^{\\alpha+\\gamma}x^{3}\\,\\mathbf{1}_{\\{x>0\\}} $$\nWe substitute this product into the general formula for $\\langle M,L\\rangle_{t}$:\n$$ \\langle M,L\\rangle_{t} = \\int_{0}^{t}\\int_{E} \\left(ab\\,s^{\\alpha+\\gamma}x^{3}\\,\\mathbf{1}_{\\{x>0\\}}\\right) \\left(\\lambda \\exp(-|x|)\\,\\mathrm{d}x\\right) \\,\\mathrm{d}s $$\nThe indicator function $\\mathbf{1}_{\\{x>0\\}}$ restricts the domain of integration for $x$ from $E=\\mathbb{R}\\setminus\\{0\\}$ to $(0,\\infty)$. On this interval, $|x| = x$. The integral becomes:\n$$ \\langle M,L\\rangle_{t} = \\int_{0}^{t}\\int_{0}^{\\infty} ab\\,s^{\\alpha+\\gamma}x^{3}\\,\\lambda \\exp(-x)\\,\\mathrm{d}x\\,\\mathrm{d}s $$\nThe integrand is separable. We can write the expression as a product of two integrals:\n$$ \\langle M,L\\rangle_{t} = ab\\lambda \\left( \\int_{0}^{t} s^{\\alpha+\\gamma} \\,\\mathrm{d}s \\right) \\left( \\int_{0}^{\\infty} x^3 \\exp(-x) \\,\\mathrm{d}x \\right) $$\nThe integral with respect to $x$ is an instance of the Gamma function, $\\Gamma(z) = \\int_{0}^{\\infty} t^{z-1}\\exp(-t)\\,\\mathrm{d}t$. For $z=4$:\n$$ \\int_{0}^{\\infty} x^{3}\\exp(-x)\\,\\mathrm{d}x = \\Gamma(4) = 3! = 3 \\times 2 \\times 1 = 6 $$\nThe integral with respect to $s$ is evaluated using the power rule. The condition $\\alpha+\\gamma > -1$ ensures that this integral is finite.\n$$ \\int_{0}^{t} s^{\\alpha+\\gamma} \\,\\mathrm{d}s = \\left[ \\frac{s^{\\alpha+\\gamma+1}}{\\alpha+\\gamma+1} \\right]_{s=0}^{s=t} = \\frac{t^{\\alpha+\\gamma+1}}{\\alpha+\\gamma+1} $$\nCombining these results yields the closed-form expression for the predictable quadratic covariation:\n$$ \\langle M,L\\rangle_{t} = ab\\lambda \\left( \\frac{t^{\\alpha+\\gamma+1}}{\\alpha+\\gamma+1} \\right) (6) = \\frac{6ab\\lambda t^{\\alpha+\\gamma+1}}{\\alpha+\\gamma+1} $$\n\nThird part: Orthogonality of $M$ and $L^{-}$.\nWe are asked to consider a new martingale $L^{-}_t = \\int_{0}^{t}\\int_{E} K^{-}(s,x)\\,\\tilde{N}(\\mathrm{d}s,\\mathrm{d}x)$ with $K^{-}(s,x)=b\\,s^{\\gamma}\\,x^{2}\\,\\mathbf{1}_{\\{x<0\\}}$. Two martingales are orthogonal if their predictable quadratic covariation is identically zero. We apply the general formula for $\\langle M, L^{-} \\rangle_t$:\n$$ \\langle M,L^{-}\\rangle_{t} = \\int_{0}^{t}\\int_{E} H(s,x)K^{-}(s,x)\\,\\nu(\\mathrm{d}x)\\,\\mathrm{d}s $$\nThe product of the integrands is:\n$$ H(s,x)K^{-}(s,x) = \\left(a\\,s^{\\alpha}\\,x\\,\\mathbf{1}_{\\{x>0\\}}\\right) \\left(b\\,s^{\\gamma}\\,x^{2}\\,\\mathbf{1}_{\\{x<0\\}}\\right) = ab\\,s^{\\alpha+\\gamma}x^{3}\\,\\mathbf{1}_{\\{x>0\\}}\\mathbf{1}_{\\{x<0\\}} $$\nThe product of the indicator functions $\\mathbf{1}_{\\{x>0\\}}$ and $\\mathbf{1}_{\\{x<0\\}}$ is $0$ for all $x \\in E$, as their supports (the sets $(0,\\infty)$ and $(-\\infty,0)$) are disjoint. Therefore, the integrand $H(s,x)K^{-}(s,x)$ is identically zero everywhere on $\\mathbb{R}_+ \\times E$.\nThe integral of a zero function is zero:\n$$ \\langle M,L^{-}\\rangle_{t} = \\int_{0}^{t}\\int_{E} 0 \\cdot \\lambda \\exp(-|x|)\\,\\mathrm{d}x\\,\\mathrm{d}s = 0 $$\nfor all $t \\ge 0$. Since $\\langle M,L^{-}\\rangle_{t} = 0$, the martingales $M_t$ and $L^{-}_t$ are, by definition, orthogonal. This is a direct consequence of their jumps occurring on disjoint sets of jump sizes.", "answer": "$$\\boxed{\\frac{6\\,a\\,b\\,\\lambda\\,t^{\\alpha+\\gamma+1}}{\\alpha+\\gamma+1}}$$", "id": "2997817"}]}