{"hands_on_practices": [{"introduction": "Before building complex models, it's crucial to understand the fundamental properties of their components. This first exercise asks you to derive the mean and variance of a compound Poisson process from first principles. By applying the powerful laws of total expectation and variance, you will uncover the direct relationship between the moments of the jump process and the moments of the jump sizes, a result known as Wald's identity. This practice is a foundational skill for analyzing any model involving stochastic sums.", "problem": "Let $\\{N_t\\}_{t \\ge 0}$ be a Poisson process with rate $\\lambda > 0$. Let $\\{J_i\\}_{i \\in \\mathbb{N}}$ be an independent and identically distributed sequence of real-valued jump sizes, independent of $\\{N_t\\}_{t \\ge 0}$. Define the compound Poisson process $X_t$ by\n$$\nX_t \\coloneqq \\sum_{i=1}^{N_t} J_i, \\quad t \\ge 0,\n$$\nwith the convention that the empty sum equals $0$ when $N_t = 0$. Using only fundamental definitions and properties of conditional expectation and variance, derive closed-form expressions for $\\mathbb{E}[X_t]$ and $\\operatorname{Var}(X_t)$ in terms of $\\lambda$, $t$, $\\mathbb{E}[J_1]$, and $\\mathbb{E}[J_1^2]$. State explicitly the weakest moment conditions on $J_1$ under which each of these expressions is finite. \n\nProvide your final answer as a single row matrix containing, in order, $\\mathbb{E}[X_t]$ and $\\operatorname{Var}(X_t)$, expressed in closed form.", "solution": "The problem is to derive the expectation and variance of a compound Poisson process $X_t = \\sum_{i=1}^{N_t} J_i$, where $\\{N_t\\}_{t \\ge 0}$ is a Poisson process with rate $\\lambda > 0$, and $\\{J_i\\}_{i \\in \\mathbb{N}}$ are independent and identically distributed (i.i.d.) random variables, also independent of $\\{N_t\\}$. We are to use fundamental properties of conditional expectation and variance.\n\nWe recall the key properties of the Poisson process $\\{N_t\\}$:\n$1$. For any $t > 0$, the random variable $N_t$ follows a Poisson distribution with parameter $\\lambda t$. Its probability mass function is $P(N_t=n) = \\frac{(\\lambda t)^n e^{-\\lambda t}}{n!}$ for $n \\in \\{0, 1, 2, \\dots\\}$.\n$2$. The expectation and variance are $\\mathbb{E}[N_t] = \\lambda t$ and $\\operatorname{Var}(N_t) = \\lambda t$.\n\nWe are also given that the jump sizes $\\{J_i\\}$ are i.i.d., so for any $i, k \\in \\mathbb{N}$, $\\mathbb{E}[J_i] = \\mathbb{E}[J_1]$ and $\\operatorname{Var}(J_i) = \\operatorname{Var}(J_1)$.\n\n**Derivation of the Expectation $\\mathbb{E}[X_t]$**\n\nWe use the Law of Total Expectation, which states that for two random variables $X$ and $Y$, $\\mathbb{E}[X] = \\mathbb{E}[\\mathbb{E}[X|Y]]$. We condition on the number of jumps $N_t$.\n$$\n\\mathbb{E}[X_t] = \\mathbb{E}[\\mathbb{E}[X_t | N_t]]\n$$\nFirst, we compute the inner conditional expectation, $\\mathbb{E}[X_t | N_t = n]$ for some integer $n \\ge 0$.\nIf $N_t=0$, then $X_t=0$ by convention, so $\\mathbb{E}[X_t | N_t=0] = \\mathbb{E}[0] = 0$.\nIf $N_t=n$ for $n > 0$, then $X_t = \\sum_{i=1}^{n} J_i$.\n$$\n\\mathbb{E}[X_t | N_t = n] = \\mathbb{E}\\left[\\sum_{i=1}^{n} J_i \\bigg| N_t = n\\right]\n$$\nSince the sequence $\\{J_i\\}$ is independent of the process $\\{N_t\\}$, conditioning on $N_t=n$ does not affect the distribution of the $J_i$.\n$$\n\\mathbb{E}\\left[\\sum_{i=1}^{n} J_i \\bigg| N_t = n\\right] = \\mathbb{E}\\left[\\sum_{i=1}^{n} J_i\\right]\n$$\nBy linearity of expectation, and since the $J_i$ are identically distributed:\n$$\n\\mathbb{E}\\left[\\sum_{i=1}^{n} J_i\\right] = \\sum_{i=1}^{n} \\mathbb{E}[J_i] = \\sum_{i=1}^{n} \\mathbb{E}[J_1] = n \\mathbb{E}[J_1]\n$$\nThis result also holds for $n=0$ if we define the empty sum to be $0$, as $0 \\cdot \\mathbb{E}[J_1] = 0$.\nSo, we can express the conditional expectation as a random variable depending on $N_t$:\n$$\n\\mathbb{E}[X_t | N_t] = N_t \\mathbb{E}[J_1]\n$$\nNow we take the expectation of this random variable with respect to the distribution of $N_t$:\n$$\n\\mathbb{E}[X_t] = \\mathbb{E}[N_t \\mathbb{E}[J_1]]\n$$\nSince $\\mathbb{E}[J_1]$ is a constant, we can factor it out of the expectation:\n$$\n\\mathbb{E}[X_t] = \\mathbb{E}[J_1] \\mathbb{E}[N_t]\n$$\nSubstituting $\\mathbb{E}[N_t] = \\lambda t$, we obtain the final expression for the expectation:\n$$\n\\mathbb{E}[X_t] = \\lambda t \\mathbb{E}[J_1]\n$$\nFor this expectation to be finite, we require $\\mathbb{E}[J_1]$ to be finite. The weakest moment condition on $J_1$ is that its first moment exists, i.e., $\\mathbb{E}[|J_1|] < \\infty$.\n\n**Derivation of the Variance $\\operatorname{Var}(X_t)$**\n\nWe use the Law of Total Variance, which states $\\operatorname{Var}(X) = \\mathbb{E}[\\operatorname{Var}(X|Y)] + \\operatorname{Var}(\\mathbb{E}[X|Y])$. We again condition on $N_t$.\n$$\n\\operatorname{Var}(X_t) = \\mathbb{E}[\\operatorname{Var}(X_t | N_t)] + \\operatorname{Var}(\\mathbb{E}[X_t | N_t])\n$$\nWe compute each of the two terms separately.\n\nTerm 1: $\\mathbb{E}[\\operatorname{Var}(X_t | N_t)]$\nFirst, we compute the inner conditional variance, $\\operatorname{Var}(X_t | N_t=n)$.\nFor $N_t=n > 0$, $X_t = \\sum_{i=1}^{n} J_i$.\n$$\n\\operatorname{Var}(X_t | N_t=n) = \\operatorname{Var}\\left(\\sum_{i=1}^{n} J_i \\bigg| N_t=n\\right)\n$$\nDue to the independence of $\\{J_i\\}$ and $\\{N_t\\}$, this is $\\operatorname{Var}(\\sum_{i=1}^{n} J_i)$. The events $\\{J_i \\le x_i\\}$ are also independent for different $i$. Therefore, the $J_i$ are mutually independent.\n$$\n\\operatorname{Var}\\left(\\sum_{i=1}^{n} J_i\\right) = \\sum_{i=1}^{n} \\operatorname{Var}(J_i)\n$$\nSince the $J_i$ are identically distributed, $\\operatorname{Var}(J_i) = \\operatorname{Var}(J_1)$ for all $i$.\n$$\n\\sum_{i=1}^{n} \\operatorname{Var}(J_i) = n \\operatorname{Var}(J_1)\n$$\nFor $n=0$, $X_t=0$, so $\\operatorname{Var}(X_t|N_t=0)=\\operatorname{Var}(0)=0$, which is consistent with $n \\operatorname{Var}(J_1)$ for $n=0$.\nThe conditional variance as a random variable is thus $\\operatorname{Var}(X_t | N_t) = N_t \\operatorname{Var}(J_1)$.\nTaking the expectation of this random variable:\n$$\n\\mathbb{E}[\\operatorname{Var}(X_t | N_t)] = \\mathbb{E}[N_t \\operatorname{Var}(J_1)] = \\operatorname{Var}(J_1) \\mathbb{E}[N_t] = \\lambda t \\operatorname{Var}(J_1)\n$$\n\nTerm 2: $\\operatorname{Var}(\\mathbb{E}[X_t | N_t])$\nFrom our derivation of the mean, we have $\\mathbb{E}[X_t | N_t] = N_t \\mathbb{E}[J_1]$.\nWe now compute the variance of this random variable.\n$$\n\\operatorname{Var}(\\mathbb{E}[X_t | N_t]) = \\operatorname{Var}(N_t \\mathbb{E}[J_1])\n$$\nUsing the property $\\operatorname{Var}(aY) = a^2 \\operatorname{Var}(Y)$ for a constant $a$, where here $a=\\mathbb{E}[J_1]$:\n$$\n\\operatorname{Var}(N_t \\mathbb{E}[J_1]) = (\\mathbb{E}[J_1])^2 \\operatorname{Var}(N_t)\n$$\nSubstituting $\\operatorname{Var}(N_t) = \\lambda t$:\n$$\n\\operatorname{Var}(\\mathbb{E}[X_t | N_t]) = (\\mathbb{E}[J_1])^2 \\lambda t\n$$\n\nCombining the two terms:\n$$\n\\operatorname{Var}(X_t) = \\lambda t \\operatorname{Var}(J_1) + \\lambda t (\\mathbb{E}[J_1])^2\n$$\nWe can factor out $\\lambda t$:\n$$\n\\operatorname{Var}(X_t) = \\lambda t (\\operatorname{Var}(J_1) + (\\mathbb{E}[J_1])^2)\n$$\nBy definition, $\\operatorname{Var}(J_1) = \\mathbb{E}[J_1^2] - (\\mathbb{E}[J_1])^2$. Substituting this into the expression:\n$$\n\\operatorname{Var}(X_t) = \\lambda t \\left( (\\mathbb{E}[J_1^2] - (\\mathbb{E}[J_1])^2) + (\\mathbb{E}[J_1])^2 \\right)\n$$\nThe terms $(\\mathbb{E}[J_1])^2$ cancel, leading to the simple result:\n$$\n\\operatorname{Var}(X_t) = \\lambda t \\mathbb{E}[J_1^2]\n$$\nFor this variance to be finite, both terms in the Law of Total Variance must be finite. This requires $\\operatorname{Var}(J_1)$ and $(\\mathbb{E}[J_1])^2$ to be finite. The finiteness of $\\operatorname{Var}(J_1) = \\mathbb{E}[J_1^2] - (\\mathbb{E}[J_1])^2$ requires $\\mathbb{E}[J_1^2]$ to be finite. If $\\mathbb{E}[J_1^2] < \\infty$, then by the Cauchy-Schwarz inequality or Jensen's inequality, $|\\mathbb{E}[J_1]| \\le \\mathbb{E}[|J_1|] \\le \\sqrt{\\mathbb{E}[J_1^2]} < \\infty$, so $\\mathbb{E}[J_1]$ is also finite. Thus, the weakest moment condition for the variance of $X_t$ to be finite is that the second moment of the jump size is finite, i.e., $\\mathbb{E}[J_1^2] < \\infty$.\n\nSummary of results:\n- Expectation: $\\mathbb{E}[X_t] = \\lambda t \\mathbb{E}[J_1]$\n- Variance: $\\operatorname{Var}(X_t) = \\lambda t \\mathbb{E}[J_1^2]$", "answer": "$$\n\\boxed{\\begin{pmatrix} \\lambda t \\mathbb{E}[J_1] & \\lambda t \\mathbb{E}[J_1^2] \\end{pmatrix}}\n$$", "id": "2971257"}, {"introduction": "Compound Poisson processes are powerful building blocks for more general jump-diffusion models. This practice moves from the process itself to stochastic differential equations (SDEs) driven by jumps, introducing the essential concept of the compensated Poisson random measure, $\\tilde{N}(dt,dz)$. You will investigate the conditions required to ensure such an SDE is well-posed, focusing on the jump coefficient $\\sigma$. This exercise is key to understanding how to construct mathematically sound models where a system's state is subject to a potentially infinite number of small shocks.", "problem": "Let $\\left(\\Omega,\\mathcal{F},(\\mathcal{F}_t)_{t\\ge 0},\\mathbb{P}\\right)$ be a filtered probability space carrying a Poisson random measure $N(dt,dz)$ on $[0,\\infty)\\times E$ with compensator measure $\\nu(dz)\\,dt$, where $(E,\\mathcal{E})$ is a measurable mark space and $\\nu$ is a $\\sigma$-finite LÃ©vy measure on $E$. Let $\\tilde N(dt,dz) \\coloneqq N(dt,dz)-\\nu(dz)\\,dt$ denote the compensated Poisson random measure. Consider the stochastic differential equation (SDE) with jumps\n$$\ndX_t \\;=\\; b\\!\\left(X_{t-}\\right)\\,dt \\;+\\; \\int_E \\sigma\\!\\left(X_{t-},z\\right)\\,\\tilde N(dt,dz),\\qquad X_0=x\\in\\mathbb{R}^d,\n$$\nwhere $b:\\mathbb{R}^d\\to\\mathbb{R}^d$ and $\\sigma:\\mathbb{R}^d\\times E\\to\\mathbb{R}^d$ are Borel functions. Assume $b$ is globally Lipschitz with linear growth so that the drift part is well-defined in $L^2$ on compact time intervals. The jump part is the stochastic integral with respect to the compensated Poisson random measure, which is defined for predictable integrands that are square-integrable with respect to the intensity measure.\n\nFrom first principles, the compensated Poisson integral $\\int_0^t\\int_E \\sigma\\!\\left(X_{s-},z\\right)\\,\\tilde N(ds,dz)$ is an $L^2$-martingale if and only if its predictable integrand is square-integrable against $\\nu(dz)\\,ds$ on finite time intervals. In the context of well-posedness of the SDE in $L^2$ (existence, uniqueness, and finite second moments), a structural growth condition on $\\sigma$ in the state variable is required to control this square integrability uniformly along the trajectory $X$.\n\nWhich of the following is the standard square-integrability linear growth condition on $\\sigma$ (in the state variable) that ensures the compensated Poisson integral is well-defined in $L^2$ and that supports $L^2$ well-posedness of the SDE on finite horizons?\n\nA. For all $y\\in\\mathbb{R}^d$,\n$$\n\\int_E \\big|\\sigma(y,z)\\big|^2\\,\\nu(dz)\\;\\le\\; C\\big(1+|y|^2\\big),\n$$\nfor some constant $C\\in(0,\\infty)$.\n\nB. For all $y\\in\\mathbb{R}^d$,\n$$\n\\int_E \\big|\\sigma(y,z)\\big|\\,\\nu(dz)\\;\\le\\; C\\big(1+|y|\\big),\n$$\nfor some constant $C\\in(0,\\infty)$.\n\nC. For all $y\\in\\mathbb{R}^d$,\n$$\n\\sup_{z\\in E}\\,\\big|\\sigma(y,z)\\big|^2\\;\\le\\; C\\big(1+|y|^2\\big),\n$$\nfor some constant $C\\in(0,\\infty)$.\n\nD. For all $y\\in\\mathbb{R}^d$,\n$$\n\\int_E \\big|\\sigma(y,z)\\big|^2\\,\\nu(dz)\\;\\le\\; C\\big(1+|y|\\big),\n$$\nfor some constant $C\\in(0,\\infty)$.\n\nE. For all $y\\in\\mathbb{R}^d$,\n$$\n\\int_E \\big|\\sigma(y,z)\\big|^2\\,\\nu(dz)\\;\\le\\; C\\big(1+|y|^4\\big),\n$$\nfor some constant $C\\in(0,\\infty)$.", "solution": "The problem asks for the standard square-integrability linear growth condition on the jump coefficient $\\sigma$ that ensures the well-posedness in $L^2$ of the stochastic differential equation (SDE) on finite time horizons. The SDE is given by\n$$\ndX_t \\;=\\; b(X_{t-})\\,dt \\;+\\; \\int_E \\sigma(X_{t-},z)\\,\\tilde N(dt,dz), \\qquad X_0=x\\in\\mathbb{R}^d.\n$$\nWell-posedness in $L^2$ on a finite horizon $[0,T]$ implies the existence of a unique solution such that $\\mathbb{E}\\left[\\sup_{t \\in [0,T]} |X_t|^2\\right] < \\infty$. A key step in establishing this is to show that $\\mathbb{E}[|X_t|^2]$ is bounded for all $t \\in [0,T]$. We will derive the necessary condition on $\\sigma$ from first principles.\n\nThe integral form of the SDE is\n$$\nX_t = X_0 + \\int_0^t b(X_{s-}) \\,ds + \\int_0^t \\int_E \\sigma(X_{s-},z) \\, \\tilde N(ds,dz).\n$$\nLet $M_t = \\int_0^t \\int_E \\sigma(X_{s-},z) \\, \\tilde N(ds,dz)$. Taking the squared norm and then the expectation, we can use the inequality $|a+b+c|^2 \\le 3(|a|^2+|b|^2+|c|^2)$ to obtain:\n$$\n\\mathbb{E}[|X_t|^2] \\le 3\\mathbb{E}[|X_0|^2] + 3\\mathbb{E}\\left[\\left|\\int_0^t b(X_s)\\,ds\\right|^2\\right] + 3\\mathbb{E}[|M_t|^2].\n$$\nWe analyze each term on the right-hand side.\n$1$. The initial condition term is $3|x|^2$, a constant.\n\n$2$. The drift term: Using the Cauchy-Schwarz inequality (or Jensen's inequality for integrals),\n$$\n\\left|\\int_0^t b(X_s)\\,ds\\right|^2 \\le \\left(\\int_0^t |b(X_s)|\\,ds\\right)^2 \\le t \\int_0^t |b(X_s)|^2\\,ds.\n$$\nThe problem states that $b$ has linear growth, i.e., there exists a constant $K_b > 0$ such that $|b(y)| \\le K_b(1+|y|)$ for all $y \\in \\mathbb{R}^d$. This implies $|b(y)|^2 \\le K_b^2(1+|y|)^2 \\le 2K_b^2(1+|y|^2)$. Taking expectations and integrating, for $t \\in [0, T]$,\n$$\n\\mathbb{E}\\left[\\left|\\int_0^t b(X_s)\\,ds\\right|^2\\right] \\le T \\int_0^t \\mathbb{E}[|b(X_s)|^2]\\,ds \\le 2TK_b^2 \\int_0^t (1 + \\mathbb{E}[|X_s|^2])\\,ds.\n$$\n\n$3$. The martingale term: The process $M_t$ is a right-continuous $L^2$-martingale provided its predictable integrand $\\sigma(X_{s-},z)$ is square-integrable. By the ItÃ´ isometry for stochastic integrals with respect to a compensated Poisson random measure, the second moment of $M_t$ equals the expectation of its predictable quadratic variation:\n$$\n\\mathbb{E}[|M_t|^2] = \\mathbb{E}[\\langle\\langle M \\rangle\\rangle_t] = \\mathbb{E}\\left[\\int_0^t \\int_E |\\sigma(X_{s-},z)|^2\\,\\nu(dz)\\,ds\\right].\n$$\nUsing Fubini's theorem (as the integrand is non-negative), this becomes\n$$\n\\mathbb{E}[|M_t|^2] = \\int_0^t \\mathbb{E}\\left[\\int_E |\\sigma(X_s,z)|^2\\,\\nu(dz)\\right]\\,ds,\n$$\nwhere we can replace $X_{s-}$ with $X_s$ inside the expectation because the set of times $s$ where $X_s \\ne X_{s-}$ has Lebesgue measure zero.\n\nTo control this term and close the argument, we need a condition on the inner integral, $\\int_E |\\sigma(y,z)|^2\\,\\nu(dz)$, as a function of $y=X_s$. The question asks for a \"linear growth condition\". In the context of second-moment analysis, this refers to a bound that is a linear function of $|y|^2$. Let us assume there exists a constant $C > 0$ such that for all $y \\in \\mathbb{R}^d$,\n$$\n\\int_E |\\sigma(y,z)|^2\\,\\nu(dz) \\le C(1+|y|^2).\n$$\nWith this assumption, the martingale term is bounded as follows:\n$$\n\\mathbb{E}[|M_t|^2] \\le \\int_0^t \\mathbb{E}[C(1+|X_s|^2)]\\,ds = C \\int_0^t (1+\\mathbb{E}[|X_s|^2])\\,ds.\n$$\n\nCombining all the bounds, let $u(t) = \\mathbb{E}[|X_t|^2]$. For $t \\in [0,T]$,\n$$\nu(t) \\le 3|x|^2 + 3\\left(2TK_b^2 \\int_0^t (1+u(s))\\,ds\\right) + 3\\left(C \\int_0^t (1+u(s))\\,ds\\right)\n$$\n$$\nu(t) \\le 3|x|^2 + (6TK_b^2 + 3C)\\int_0^t (1+u(s))\\,ds.\n$$\nLet $K = 6TK_b^2 + 3C$. The inequality is $u(t) \\le 3|x|^2 + K\\int_0^t(1+u(s))\\,ds$. By Gronwall's lemma, this implies that $u(t)$ is bounded on $[0,T]$. Specifically, $1+u(t) \\le (1+3|x|^2)e^{Kt}$, which shows that $\\mathbb{E}[|X_t|^2]$ remains finite.\nThis confirms that the condition $\\int_E |\\sigma(y,z)|^2\\,\\nu(dz) \\le C(1+|y|^2)$ is the appropriate linear growth condition to ensure the finiteness of second moments.\n\nNow, we evaluate each option.\n\nA. For all $y\\in\\mathbb{R}^d$, $\\int_E \\big|\\sigma(y,z)\\big|^2\\,\\nu(dz)\\;\\le\\; C\\big(1+|y|^2\\big)$, for some constant $C\\in(0,\\infty)$.\nThis is precisely the condition derived above. It is a \"square-integrability\" condition as it involves the integral of $|\\sigma|^2$ with respect to the LÃ©vy measure $\\nu$. It is termed a \"linear growth condition\" in the context of second-moment analysis because the bound is a linear function of $|y|^2$. This is the standard condition in the literature for $L^2$ well-posedness of SDEs with jumps.\n**Verdict: Correct.**\n\nB. For all $y\\in\\mathbb{R}^d$, $\\int_E \\big|\\sigma(y,z)\\big|\\,\\nu(dz)\\;\\le\\; C\\big(1+|y|\\big)$, for some constant $C\\in(0,\\infty)$.\nThis condition involves the first moment of $|\\sigma|$ with respect to $\\nu$. The ItÃ´ isometry for the second moment of the compensated jump integral requires control over $\\int |\\sigma|^2 d\\nu$, not $\\int |\\sigma| d\\nu$. This condition is insufficient for controlling the second moment of the martingale part and thus is not the standard condition for $L^2$ well-posedness. It is related to conditions for finiteness of the first moment, $\\mathbb{E}[|X_t|]$.\n**Verdict: Incorrect.**\n\nC. For all $y\\in\\mathbb{R}^d$, $\\sup_{z\\in E}\\,\\big|\\sigma(y,z)\\big|^2\\;\\le\\; C\\big(1+|y|^2\\big)$, for some constant $C\\in(0,\\infty)$.\nThis condition requires the jump sizes to be uniformly bounded in the mark variable $z$ (for a fixed state $y$). If the LÃ©vy measure $\\nu$ is finite, this condition implies condition A, since $\\int_E |\\sigma(y,z)|^2 \\nu(dz) \\le (\\sup_z |\\sigma(y,z)|^2) \\nu(E)$. However, many important LÃ©vy measures (e.g., for stable processes) are infinite ($\\nu(E)=\\infty$), so this condition is not general enough. It is far more restrictive than the standard requirement, which only demands that the integral is controlled, not the supremum of the integrand.\n**Verdict: Incorrect.**\n\nD. For all $y\\in\\mathbb{R}^d$, $\\int_E \\big|\\sigma(y,z)\\big|^2\\,\\nu(dz)\\;\\le\\; C\\big(1+|y|\\big)$, for some constant $C\\in(0,\\infty)$.\nThis condition specifies a growth rate proportional to $|y|$, not $|y|^2$. Following our derivation, this would lead to an integral inequality of the form $u(t) \\le K_1 + K_2 \\int_0^t (1+\\sqrt{u(s)})\\,ds$, where $u(t)=\\mathbb{E}[|X_t|^2]$. This is not a standard linear integral inequality, and while one can show boundedness under this stronger condition (since for large $|y|$, $|y|<|y|^2$), it is not the \"linear growth\" condition that pairs naturally with the second moment. The standard terminology \"linear growth\" in this context refers to the bound being linear in $|y|^2$.\n**Verdict: Incorrect.**\n\nE. For all $y\\in\\mathbb{R}^d$, $\\int_E \\big|\\sigma(y,z)\\big|^2\\,\\nu(dz)\\;\\le\\; C\\big(1+|y|^4\\big)$, for some constant $C\\in(0,\\infty)$.\nThis condition allows for quartic growth. If we insert this into our derivation for the second moment, we obtain an inequality of the form $\\mathbb{E}[|X_t|^2] \\le K_1 + \\int_0^t K_2(1+ \\mathbb{E}[|X_s|^2] + \\mathbb{E}[|X_s|^4])\\,ds$. This inequality does not \"close\" because the bound on the second moment, $\\mathbb{E}[|X_t|^2]$, depends on the fourth moment, $\\mathbb{E}[|X_s|^4]$. This condition is too weak and generally does not guarantee that moments remain finite; it can lead to explosion in finite time.\n**Verdict: Incorrect.**", "answer": "$$\\boxed{A}$$", "id": "2971245"}, {"introduction": "Mathematical theory is often best understood by exploring its boundaries. This final exercise challenges you to analyze a carefully constructed scenario where the standard conditions for the stochastic Fubini theoremâwhich allows the interchange of deterministic and stochastic integralsâare violated. By examining this counterexample, you will gain a profound intuition for why the square-integrability conditions, such as those explored in the previous practice [@problem_id:2971245], are not mere technicalities but are essential for the logical consistency of the theory. This kind of \"stress-testing\" is a hallmark of advanced problem-solving in stochastic analysis.", "problem": "Let $\\left(\\Omega,\\mathcal{F},\\mathbb{P}\\right)$ be a probability space supporting a Poisson random measure (PRM) $N(\\mathrm{d}t,\\mathrm{d}z)$ on $(0,1]\\times E$ with intensity $\\mathrm{d}t\\,\\nu(\\mathrm{d}z)$, where $E$ is a measurable space and the LÃ©vy measure $\\nu$ satisfies $\\nu(E)<\\infty$ (compound Poisson case). Define the compensated Poisson random measure $\\tilde{N}(\\mathrm{d}t,\\mathrm{d}z)=N(\\mathrm{d}t,\\mathrm{d}z)-\\mathrm{d}t\\,\\nu(\\mathrm{d}z)$, and let $\\{\\mathcal{F}_t\\}_{t\\in[0,1]}$ be the completed, right-continuous filtration generated by $N$. For a predictable integrand $H(s,t,z)$, the compensated stochastic integral $\\int_0^1\\int_E H(s,t,z)\\,\\tilde{N}(\\mathrm{d}t,\\mathrm{d}z)$ is defined as a square-integrable martingale whenever\n$$\n\\mathbb{E}\\left[\\int_0^1\\int_E |H(s,t,z)|^2\\,\\nu(\\mathrm{d}z)\\,\\mathrm{d}t\\right]<\\infty,\n$$\nand the stochastic Fubini theorem requires square-integrability across all variables to justify interchanging a deterministic integral with a compensated PRM integral.\n\nConsider the following setup designed to probe the necessity of square-integrability and the role of predictability. Fix a deterministic function $h(s)=s^{-1/2}\\,\\mathbf{1}_{(0,1]}(s)$ and a measurable $g:E\\to\\mathbb{R}$ such that $\\int_E |g(z)|\\,\\nu(\\mathrm{d}z)<\\infty$ but $\\int_E g(z)^2\\,\\nu(\\mathrm{d}z)=\\infty$. Define the predictable integrand\n$$\nH(s,t,z)=h(s)\\,g(z)\\,\\mathbf{1}_{(0,1]}(t),\n$$\nand consider the pair of iterated integrals\n$$\nX_1:=\\int_0^1\\left(\\int_0^1\\int_E H(s,t,z)\\,\\tilde{N}(\\mathrm{d}t,\\mathrm{d}z)\\right)\\mathrm{d}s,\\qquad\nX_2:=\\int_0^1\\int_E\\left(\\int_0^1 H(s,t,z)\\,\\mathrm{d}s\\right)\\,\\tilde{N}(\\mathrm{d}t,\\mathrm{d}z).\n$$\nNote that $H$ is predictable. The square-integrability condition across $(s,t,z)$ fails since\n$$\n\\int_0^1 h(s)^2\\,\\mathrm{d}s=\\int_0^1 s^{-1}\\,\\mathrm{d}s=\\infty,\\qquad \\int_E g(z)^2\\,\\nu(\\mathrm{d}z)=\\infty,\n$$\nso the classical stochastic Fubini theorem hypotheses do not hold. Nevertheless, because $\\nu(E)<\\infty$ (finite activity), pathwise sums against $N$ are finite almost surely over $(0,1]$, and compensators are deterministic, there is a subtle distinction between the existence of $X_1$ and $X_2$ as random variables versus their existence as square-integrable martingales.\n\nWhich of the following statements are correct?\n\nA. In the above construction, $X_2$ is well-defined as an integrable random variable via the decomposition\n$$\n\\int_0^1\\int_E\\left(\\int_0^1 H(s,t,z)\\,\\mathrm{d}s\\right)\\tilde{N}(\\mathrm{d}t,\\mathrm{d}z)\n=\\sum_{k:\\,T_k\\le 1}\\left(\\int_0^1 h(s)\\,\\mathrm{d}s\\right)g(Z_k)\\;-\\;\\left(\\int_0^1 h(s)\\,\\mathrm{d}s\\right)\\int_E g(z)\\,\\nu(\\mathrm{d}z),\n$$\nwhere $\\{(T_k,Z_k)\\}$ are the finitely many jumps in $(0,1]\\times E$. By contrast, $X_1$ does not exist under the standard compensated Poisson integral definition as a square-integrable martingale because $\\mathbb{E}\\int_0^1\\int_E |H(s,t,z)|^2\\,\\nu(\\mathrm{d}z)\\,\\mathrm{d}t=\\infty$, so interchanging the order is not justified and may fail.\n\nB. Predictability of $H$ with respect to the filtration of $N$ is essential: if $H$ is optional but not predictable, then even if $\\mathbb{E}\\int_0^1\\int_E |H|^2\\,\\nu(\\mathrm{d}z)\\,\\mathrm{d}t<\\infty$, the compensated integral $\\int_0^1\\int_E H\\,\\tilde{N}$ is not a well-defined martingale in the usual sense, and the stochastic Fubini theorem cannot be applied.\n\nC. In the compound Poisson setting $\\nu(E)<\\infty$, boundedness of the integrand alone implies that compensated integrals are square-integrable and the interchange of deterministic and stochastic integrals is valid regardless of whether $H$ is predictable.\n\nD. If interchange fails in the compensated setting, replacing $\\tilde{N}$ by the raw Poisson random measure $N$ fixes the issue because $\\int H\\,N$ is always well-defined for any measurable $H$, and classical (non-stochastic) Fubini applies without square-integrability assumptions.\n\nChoose all that apply.", "solution": "**Solution Derivation**\n\nThe core of the problem is to analyze the existence and properties of the two iterated integrals $X_1$ and $X_2$, where the standard sufficient conditions for interchanging the integrals via the stochastic Fubini theorem are not met. The key feature to exploit is that the process has finite activity, i.e., $\\nu(E) < \\infty$.\n\nLet the jump times and locations of the PRM $N$ in $(0,1]\\times E$ be denoted by $\\{(T_k, Z_k)\\}$. Since the expected number of jumps in $(0,1]$ is $\\mathbb{E}[N((0,1], E)] = \\int_0^1 \\nu(E) \\mathrm{d}t = \\nu(E) < \\infty$, there are almost surely a finite number of such jumps. This allows for a pathwise representation of integrals with respect to $N$.\n\n**Analysis of $X_2$**\n\n$$X_2 := \\int_0^1\\int_E\\left(\\int_0^1 H(s,t,z)\\,\\mathrm{d}s\\right)\\,\\tilde{N}(\\mathrm{d}t,\\mathrm{d}z)$$\nFirst, we evaluate the inner deterministic integral over $s$. The integrand for $\\tilde{N}$ is $K(t,z) := \\int_0^1 H(s,t,z)\\,\\mathrm{d}s$.\n$$K(t,z) = \\int_0^1 h(s)\\,g(z)\\,\\mathbf{1}_{(0,1]}(t)\\,\\mathrm{d}s = g(z)\\mathbf{1}_{(0,1]}(t) \\int_0^1 s^{-1/2}\\,\\mathrm{d}s$$\nThe integral $\\int_0^1 s^{-1/2}\\,\\mathrm{d}s = [2s^{1/2}]_0^1 = 2$.\nThus, the integrand is $K(t,z) = 2g(z)\\mathbf{1}_{(0,1]}(t)$.\n$X_2$ becomes:\n$$X_2 = \\int_0^1\\int_E 2g(z) \\tilde{N}(\\mathrm{d}t,\\mathrm{d}z) = \\int_0^1\\int_E 2g(z) (N(\\mathrm{d}t,\\mathrm{d}z) - \\nu(\\mathrm{d}z)\\mathrm{d}t)$$\nUsing the pathwise representation $N(\\mathrm{d}t,\\mathrm{d}z) = \\sum_k \\delta_{(T_k, Z_k)}(\\mathrm{d}t, \\mathrm{d}z)$:\n$$X_2 = 2 \\sum_{k:\\,T_k\\le 1} g(Z_k) - \\int_0^1 \\left( \\int_E 2g(z)\\nu(\\mathrm{d}z) \\right) \\mathrm{d}t$$\n$$X_2 = 2\\left( \\sum_{k:\\,T_k\\le 1} g(Z_k) - \\int_E g(z)\\nu(\\mathrm{d}z) \\right)$$\nFor $X_2$ to be a well-defined integrable random variable, we need $\\mathbb{E}[|X_2|] < \\infty$. Given the expectation of a compensated integral is $0$, we check the integrability of the uncompensated part. By Campbell's Theorem:\n$$\\mathbb{E}\\left[\\left|\\sum_{k:\\,T_k\\le 1} g(Z_k)\\right|\\right] \\le \\mathbb{E}\\left[\\sum_{k:\\,T_k\\le 1} |g(Z_k)|\\right] = \\mathbb{E}\\left[\\int_0^1\\int_E |g(z)| N(\\mathrm{d}t,\\mathrm{d}z)\\right] = \\int_0^1\\int_E |g(z)|\\nu(\\mathrm{d}z)\\mathrm{d}t$$\nThis is equal to $\\int_E|g(z)|\\nu(\\mathrm{d}z)$, which is finite by hypothesis. The compensator term is a finite constant. Thus, $X_2$ is a well-defined integrable random variable.\nHowever, the associated process is not a square-integrable martingale because the square-integrability condition fails:\n$\\int_0^1\\int_E |K(t,z)|^2 \\nu(\\mathrm{d}z)\\mathrm{d}t = \\int_0^1\\int_E |2g(z)|^2 \\nu(\\mathrm{d}z)\\mathrm{d}t = 4\\int_E g(z)^2 \\nu(\\mathrm{d}z) = \\infty$.\n\n**Analysis of $X_1$**\n\n$$X_1 := \\int_0^1\\left(\\int_0^1\\int_E H(s,t,z)\\,\\tilde{N}(\\mathrm{d}t,\\mathrm{d}z)\\right)\\mathrm{d}s$$\nLet $Y(s)$ be the inner stochastic integral: $Y(s) = \\int_0^1\\int_E H(s,t,z)\\,\\tilde{N}(\\mathrm{d}t,\\mathrm{d}z)$.\nThe problem phrasing refers to \"the standard compensated Poisson integral definition as a square-integrable martingale\". This construction relies on the ItÃ´ isometry and requires the integrand to be square-integrable. For a fixed $s$, the integrand for $\\tilde{N}$ is $H(s,t,z) = h(s)g(z)\\mathbf{1}_{(0,1]}(t)$. The condition for $Y(s)$ to be in $L^2(\\mathbb{P})$ is:\n$$\\int_0^1\\int_E |H(s,t,z)|^2 \\nu(\\mathrm{d}z)\\mathrm{d}t < \\infty$$\nLet's evaluate this for $s \\in (0,1]$:\n$$\\int_0^1\\int_E |h(s)g(z)\\mathbf{1}_{(0,1]}(t)|^2 \\nu(\\mathrm{d}z)\\mathrm{d}t = h(s)^2 \\left(\\int_E g(z)^2\\nu(\\mathrm{d}z)\\right) \\left(\\int_0^1 \\mathrm{d}t\\right)$$\n$$= (s^{-1/2})^2 \\times (\\infty) \\times 1 = s^{-1} \\times \\infty = \\infty$$\nSince this condition fails for all $s \\in (0,1]$, the inner integral $Y(s)$ cannot be defined for any such $s$ within the framework of square-integrable martingales. Consequently, the outer integral $X_1 = \\int_0^1 Y(s)\\mathrm{d}s$ is not well-defined under this standard construction.\n\nIt is worth noting that a pathwise calculation is possible due to the compound Poisson structure:\n$Y(s) = h(s)\\left( \\sum_{k:\\,T_k\\le 1} g(Z_k) - \\int_E g(z)\\nu(\\mathrm{d}z) \\right)$.\nThen $X_1 = \\int_0^1 Y(s)\\mathrm{d}s = \\left( \\sum_{k:\\,T_k\\le 1} g(Z_k) - \\int_E g(z)\\nu(\\mathrm{d}z) \\right) \\int_0^1 h(s)\\mathrm{d}s = 2\\left( \\sum_{k:\\,T_k\\le 1} g(Z_k) - \\int_E g(z)\\nu(\\mathrm{d}z) \\right)$.\nThis shows that $X_1=X_2$ a.s. However, this derivation steps outside the \"standard definition as a square-integrable martingale\" by using a pathwise approach not applicable to general LÃ©vy processes. The question probes this very distinction.\n\n**Option-by-Option Analysis**\n\n**A. In the above construction, $X_2$ is well-defined as an integrable random variable via the decomposition... By contrast, $X_1$ does not exist under the standard compensated Poisson integral definition as a square-integrable martingale...**\n\nThis statement aligns perfectly with our derivations.\n- The first part correctly identifies that $X_2$ is well-defined as an integrable random variable and provides the correct pathwise decomposition. The integral $\\int_0^1 h(s)\\mathrm{d}s = 2$.\nThe given formula: $\\sum_{k:\\,T_k\\le 1}\\left(\\int_0^1 h(s)\\,\\mathrm{d}s\\right)g(Z_k)\\;-\\;\\left(\\int_0^1 h(s)\\,\\mathrm{d}s\\right)\\int_E g(z)\\,\\nu(\\mathrm{d}z)$ becomes $2 \\sum_{k: T_k \\le 1} g(Z_k) - 2 \\int_E g(z)\\nu(\\mathrm{d}z)$, which is exactly our derived expression for $X_2$. We also showed it is integrable.\n- The second part correctly states that $X_1$ does not exist \"under the standard compensated Poisson integral definition as a square-integrable martingale\". As shown, the inner integral $Y(s)$ is not in $L^2(\\mathbb{P})$ for any $s \\in (0,1]$, so the construction fails. The reason given, that the total square-integrability condition for stochastic Fubini fails, is also correct and is the fundamental source of the problem.\nThis option correctly captures the subtleties of the problem.\n\n**Verdict: Correct**\n\n**B. Predictability of $H$ with respect to the filtration of $N$ is essential: if $H$ is optional but not predictable, then even if $\\mathbb{E}\\int_0^1\\int_E |H|^2\\,\\nu(\\mathrm{d}z)\\,\\mathrm{d}t<\\infty$, the compensated integral $\\int_0^1\\int_E H\\,\\tilde{N}$ is not a well-defined martingale in the usual sense, and the stochastic Fubini theorem cannot be applied.**\n\nThis is a correct statement of a fundamental principle in the theory of stochastic integration. The martingale property of stochastic integrals with respect to martingales (including compensated PRMs) hinges on the predictability of the integrand. Predictability ensures that the value of the integrand at time $t$ is determined by information available strictly before $t$, which prevents it from \"seeing\" the jump at $t$. If the integrand is merely optional, it can depend on the jump at $t$, and the resulting integral process is generally not a martingale (it acquires a correction term). Standard formulations of the stochastic Fubini theorem also explicitly require the integrand to be predictable.\n\n**Verdict: Correct**\n\n**C. In the compound Poisson setting $\\nu(E)<\\infty$, boundedness of the integrand alone implies that compensated integrals are square-integrable and the interchange of deterministic and stochastic integrals is valid regardless of whether $H$ is predictable.**\n\nLet's dissect this. If an integrand $H$ is predictable and bounded, say $|H| \\le K$, then in the compound Poisson case ($\\nu(E) < \\infty$), the square-integrability condition holds: $\\mathbb{E}[\\int_0^1 \\int_E |H|^2 \\nu(\\mathrm{d}z)\\mathrm{d}t] \\le K^2 \\nu(E) < \\infty$. Similarly, the Fubini condition would hold for an integral over a finite product space. However, the claim is that this holds \"regardless of whether $H$ is predictable.\" As established in the analysis of option B, predictability is a crucial requirement. Dropping it invalidates the martingale property and the standard Fubini theorems.\n\n**Verdict: Incorrect**\n\n**D. If interchange fails in the compensated setting, replacing $\\tilde{N}$ by the raw Poisson random measure $N$ fixes the issue because $\\int H\\,N$ is always well-defined for any measurable $H$, and classical (non-stochastic) Fubini applies without square-integrability assumptions.**\n\nThis statement contains a critical falsehood. The claim that \"$\\int H\\,N$ is always well-defined for any measurable $H$\" is incorrect. For the integral to be well-defined (as a pathwise sum), some integrability condition on $H$ is necessary. For example, if $H(t,z) = 1/t$, the sum $\\sum_k 1/T_k$ over jump times $T_k$ will diverge a.s. because the jump times of a Poisson process accumulate near $0$. While it is true that for the specific $H$ in this problem, the interchange for the raw measure $N$ is valid by applying the classical Fubini-Tonelli theorem pathwise (since $\\int_0^1\\int_0^1\\int_E |H(s,t,z)| \\nu(\\mathrm{d}z)\\mathrm{d}t\\mathrm{d}s < \\infty$), the universal claim \"for any measurable $H$\" makes the premise of the argument false.\n\n**Verdict: Incorrect**", "answer": "$$\\boxed{AB}$$", "id": "2971237"}]}