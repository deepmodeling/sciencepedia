{"hands_on_practices": [{"introduction": "To begin our hands-on exploration, we will ground the abstract theory in a concrete calculation. This exercise focuses on a compound Poisson subordinator, which serves as a fundamental building block for a wide class of non-decreasing Lévy processes. By directly deriving its Laplace exponent from the process's construction, you will practice the essential skill of connecting the microscopic details of jumps (their arrival rate and size distribution) to the macroscopic characterization provided by the Lévy-Khintchine framework [@problem_id:2980726].", "id": "2980726", "problem": "Consider a compound Poisson subordinator $X_{t}$ defined by $X_{t} = \\sum_{i=1}^{N_{t}} J_{i}$, where $\\{N_{t}\\}_{t \\geq 0}$ is a Poisson process with rate $\\lambda > 0$, and $\\{J_{i}\\}_{i \\geq 1}$ are independent and identically distributed jump sizes, independent of $\\{N_{t}\\}$, with an exponential distribution of mean $1/\\beta$ (that is, with rate parameter $\\beta > 0$). Recall that a subordinator is a non-decreasing, right-continuous process with stationary and independent increments, and that its law at each fixed time is infinitely divisible. Define the Laplace exponent $\\Phi(\\theta)$ for $\\theta \\geq 0$ by the relation\n$$\n\\mathbb{E}\\big[\\exp(-\\theta X_{t})\\big] = \\exp\\big(-t \\, \\Phi(\\theta)\\big).\n$$\nStarting only from the defining properties of the compound Poisson construction and basic properties of the Poisson process and the exponential distribution, derive the Laplace exponent $\\Phi(\\theta)$ and the mean $\\mathbb{E}[X_{t}]$. Express your final answer as exact symbolic formulas in terms of $\\lambda$, $\\beta$, $\\theta$, and $t$. No rounding is required.", "solution": "The problem is well-posed and scientifically grounded, providing all necessary definitions and constraints for a rigorous derivation. It is a standard problem in the theory of Lévy processes. We shall proceed with the solution.\n\nThe problem asks for the derivation of the Laplace exponent $\\Phi(\\theta)$ and the mean $\\mathbb{E}[X_{t}]$ for a compound Poisson subordinator $X_{t} = \\sum_{i=1}^{N_{t}} J_{i}$.\n\nFirst, we derive the Laplace transform of $X_t$, defined as $\\mathbb{E}[\\exp(-\\theta X_{t})]$. We use the law of total expectation by conditioning on the number of jumps $N_{t}$ up to time $t$.\n$$\n\\mathbb{E}[\\exp(-\\theta X_{t})] = \\mathbb{E}\\big[\\mathbb{E}[\\exp(-\\theta X_{t}) | N_{t}]\\big]\n$$\nGiven $N_{t} = n$ for some integer $n \\ge 0$, the process $X_t$ is a sum of $n$ independent and identically distributed (i.i.d.) random variables $J_i$:\n$$\n\\mathbb{E}[\\exp(-\\theta X_{t}) | N_{t}=n] = \\mathbb{E}\\left[\\exp\\left(-\\theta \\sum_{i=1}^{n} J_{i}\\right)\\right]\n$$\nSince the jump sizes $\\{J_{i}\\}_{i \\geq 1}$ are i.i.d., the expectation of the product is the product of the expectations:\n$$\n\\mathbb{E}\\left[\\exp\\left(-\\theta \\sum_{i=1}^{n} J_{i}\\right)\\right] = \\mathbb{E}\\left[\\prod_{i=1}^{n} \\exp(-\\theta J_{i})\\right] = \\prod_{i=1}^{n} \\mathbb{E}[\\exp(-\\theta J_{i})] = \\left(\\mathbb{E}[\\exp(-\\theta J_{1})]\\right)^{n}\n$$\nHere, $\\mathbb{E}[\\exp(-\\theta J_{1})]$ is the Laplace transform of a single jump size $J_1$. The jumps are stated to follow an exponential distribution with mean $1/\\beta$, which means their rate parameter is $\\beta > 0$. The probability density function (PDF) of $J_1$ is $f_{J}(j) = \\beta \\exp(-\\beta j)$ for $j \\geq 0$. We compute its Laplace transform for $\\theta \\geq 0$:\n$$\n\\mathbb{E}[\\exp(-\\theta J_{1})] = \\int_{0}^{\\infty} \\exp(-\\theta j) f_{J}(j) \\, dj = \\int_{0}^{\\infty} \\exp(-\\theta j) \\beta \\exp(-\\beta j) \\, dj\n$$\n$$\n= \\beta \\int_{0}^{\\infty} \\exp(-(\\beta + \\theta)j) \\, dj\n$$\nSince $\\beta > 0$ and $\\theta \\geq 0$, the term $\\beta + \\theta$ is strictly positive, ensuring the convergence of the integral.\n$$\n= \\beta \\left[ -\\frac{1}{\\beta + \\theta} \\exp(-(\\beta + \\theta)j) \\right]_{j=0}^{j=\\infty} = \\beta \\left( 0 - \\left(-\\frac{1}{\\beta + \\theta}\\right) \\right) = \\frac{\\beta}{\\beta + \\theta}\n$$\nSubstituting this back, the conditional expectation is:\n$$\n\\mathbb{E}[\\exp(-\\theta X_{t}) | N_{t}=n] = \\left(\\frac{\\beta}{\\beta + \\theta}\\right)^{n}\n$$\nNow we can compute the unconditional expectation by averaging over the distribution of $N_t$. The number of jumps $N_t$ follows a Poisson distribution with parameter $\\lambda t$, so its probability mass function is $P(N_{t}=n) = \\frac{(\\lambda t)^{n}}{n!} \\exp(-\\lambda t)$ for $n \\in \\{0, 1, 2, \\dots\\}$.\n$$\n\\mathbb{E}[\\exp(-\\theta X_{t})] = \\sum_{n=0}^{\\infty} \\mathbb{E}[\\exp(-\\theta X_{t}) | N_{t}=n] P(N_{t}=n)\n$$\n$$\n= \\sum_{n=0}^{\\infty} \\left(\\frac{\\beta}{\\beta + \\theta}\\right)^{n} \\frac{(\\lambda t)^{n}}{n!} \\exp(-\\lambda t)\n$$\nWe can rearrange the terms and factor out $\\exp(-\\lambda t)$:\n$$\n= \\exp(-\\lambda t) \\sum_{n=0}^{\\infty} \\frac{1}{n!} \\left(\\lambda t \\frac{\\beta}{\\beta + \\theta}\\right)^{n}\n$$\nThe summation is the Taylor series expansion for the exponential function, $\\exp(z) = \\sum_{n=0}^{\\infty} \\frac{z^n}{n!}$, with $z = \\lambda t \\frac{\\beta}{\\beta + \\theta}$.\n$$\n\\mathbb{E}[\\exp(-\\theta X_{t})] = \\exp(-\\lambda t) \\exp\\left(\\lambda t \\frac{\\beta}{\\beta + \\theta}\\right)\n$$\nCombining the exponents gives:\n$$\n\\mathbb{E}[\\exp(-\\theta X_{t})] = \\exp\\left(-\\lambda t + \\lambda t \\frac{\\beta}{\\beta + \\theta}\\right) = \\exp\\left(-\\lambda t \\left(1 - \\frac{\\beta}{\\beta + \\theta}\\right)\\right)\n$$\n$$\n= \\exp\\left(-\\lambda t \\left(\\frac{\\beta + \\theta - \\beta}{\\beta + \\theta}\\right)\\right) = \\exp\\left(-t \\frac{\\lambda \\theta}{\\beta + \\theta}\\right)\n$$\nBy the definition provided in the problem, $\\mathbb{E}[\\exp(-\\theta X_{t})] = \\exp(-t \\, \\Phi(\\theta))$. By direct comparison, we identify the Laplace exponent $\\Phi(\\theta)$:\n$$\n\\Phi(\\theta) = \\frac{\\lambda \\theta}{\\beta + \\theta}\n$$\nNext, we derive the mean $\\mathbb{E}[X_{t}]$. We can do this in two ways.\n\nMethod 1: Using the Laplace exponent.\nThe moments of $X_t$ can be found by differentiating its Laplace transform. Specifically, $\\mathbb{E}[X_{t}] = -\\frac{d}{d\\theta}\\mathbb{E}[\\exp(-\\theta X_{t})] \\big|_{\\theta=0}$.\n$$\n\\mathbb{E}[X_{t}] = -\\frac{d}{d\\theta}\\left[\\exp\\left(-t \\frac{\\lambda \\theta}{\\beta + \\theta}\\right)\\right]_{\\theta=0}\n$$\nUsing the chain rule:\n$$\n= -\\left[\\exp\\left(-t \\frac{\\lambda \\theta}{\\beta + \\theta}\\right) \\cdot \\left(-t \\frac{d}{d\\theta}\\left(\\frac{\\lambda \\theta}{\\beta + \\theta}\\right)\\right)\\right]_{\\theta=0}\n$$\nWe evaluate the derivative using the quotient rule:\n$$\n\\frac{d}{d\\theta}\\left(\\frac{\\lambda \\theta}{\\beta + \\theta}\\right) = \\frac{\\lambda(\\beta + \\theta) - \\lambda \\theta(1)}{(\\beta + \\theta)^{2}} = \\frac{\\lambda \\beta}{(\\beta + \\theta)^{2}}\n$$\nSubstituting this back:\n$$\n\\mathbb{E}[X_{t}] = \\left[\\exp\\left(-t \\frac{\\lambda \\theta}{\\beta + \\theta}\\right) \\cdot t \\frac{\\lambda \\beta}{(\\beta + \\theta)^{2}}\\right]_{\\theta=0}\n$$\nAt $\\theta=0$, the exponential term becomes $\\exp(0) = 1$. The second term becomes $t \\frac{\\lambda \\beta}{(\\beta + 0)^{2}} = t \\frac{\\lambda \\beta}{\\beta^{2}} = \\frac{\\lambda t}{\\beta}$.\n$$\n\\mathbb{E}[X_{t}] = 1 \\cdot \\frac{\\lambda t}{\\beta} = \\frac{\\lambda t}{\\beta}\n$$\n\nMethod 2: Using Wald's Identity (or the law of iterated expectations).\nThe mean of the compound process is given by $\\mathbb{E}[X_t] = \\mathbb{E}[\\mathbb{E}[X_t | N_t]]$.\nFirst, calculate the conditional expectation of $X_t$ given $N_t = n$:\n$$\n\\mathbb{E}[X_t | N_t=n] = \\mathbb{E}\\left[\\sum_{i=1}^n J_i\\right] = \\sum_{i=1}^n \\mathbb{E}[J_i]\n$$\nSince the $J_i$ are i.i.d., $\\mathbb{E}[J_i] = \\mathbb{E}[J_1]$ for all $i$. The mean of the exponential distribution with rate parameter $\\beta$ is $1/\\beta$.\n$$\n\\mathbb{E}[J_1] = \\frac{1}{\\beta}\n$$\nSo, the conditional expectation is:\n$$\n\\mathbb{E}[X_t | N_t=n] = n \\cdot \\mathbb{E}[J_1] = \\frac{n}{\\beta}\n$$\nNow, we take the expectation over $N_t$:\n$$\n\\mathbb{E}[X_t] = \\mathbb{E}\\left[\\frac{N_t}{\\beta}\\right] = \\frac{1}{\\beta} \\mathbb{E}[N_t]\n$$\nThe mean of a Poisson process with rate $\\lambda$ at time $t$ is $\\mathbb{E}[N_t] = \\lambda t$.\nTherefore,\n$$\n\\mathbb{E}[X_{t}] = \\frac{1}{\\beta} (\\lambda t) = \\frac{\\lambda t}{\\beta}\n$$\nBoth methods yield the same result, confirming our derivation. The required quantities are the Laplace exponent $\\Phi(\\theta)$ and the mean $\\mathbb{E}[X_t]$.", "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{\\lambda \\theta}{\\theta + \\beta} & \\frac{\\lambda t}{\\beta} \\end{pmatrix}}\n$$"}, {"introduction": "Having performed a direct calculation, we now shift our focus to the structural properties that define infinite divisibility. This practice challenges you to distinguish between distributions that are infinitely divisible and those that are not, using the canonical Poisson and Binomial distributions as test cases. You will apply several key theoretical criteria—spanning arguments about the distribution's support, its characteristic function, and its probability generating function—thereby deepening your conceptual understanding of what makes a distribution infinitely divisible [@problem_id:2980715].", "id": "2980715", "problem": "Consider the notion of infinite divisibility on the additive group of the real line. A probability distribution $\\mu$ on $\\mathbb{R}$ is called infinitely divisible if for every $m \\in \\mathbb{N}$ there exists a probability distribution $\\mu_m$ on $\\mathbb{R}$ such that $\\mu = \\mu_m^{* m}$, where $*$ denotes convolution. By the Lévy–Khintchine representation, an infinitely divisible law on $\\mathbb{R}$ has a characteristic function of the form $\\varphi(t) = \\exp\\{\\psi(t)\\}$ for a characteristic exponent $\\psi$ of Lévy–Khintchine type. Consider a Binomial$(n,p)$ distribution with fixed $n \\in \\mathbb{N}$ and $p \\in (0,1)$ and a Poisson$(\\lambda)$ distribution with $\\lambda > 0$. Which of the following statements are correct? Select all that apply.\n\nA. For any fixed $n \\in \\mathbb{N}$ and $p \\in (0,1)$, the $\\mathrm{Bin}(n,p)$ distribution is not infinitely divisible unless $p \\in \\{0,1\\}$. This follows because any non-degenerate infinitely divisible distribution with finite support would contradict the Lévy–Khintchine structural decomposition.\n\nB. The $\\mathrm{Poisson}(\\lambda)$ distribution is infinitely divisible for every $\\lambda > 0$, with characteristic exponent $\\psi(t) = \\lambda\\,(e^{i t} - 1)$ corresponding to a Lévy measure $\\nu = \\lambda \\,\\delta_{1}$.\n\nC. The characteristic function of $\\mathrm{Bin}(n,p)$ has a zero at $t = \\pi$ for all $p \\in (0,1)$, which proves that $\\mathrm{Bin}(n,p)$ is not infinitely divisible.\n\nD. Let $G_X(z) = \\mathbb{E}[z^{X}]$ denote the probability generating function (p.g.f.) of a nonnegative integer-valued random variable $X$. If $X$ is infinitely divisible and supported on $\\mathbb{Z}_{+}$, then there exist $m \\in \\mathbb{Z}_{+}$, coefficients $a_k \\ge 0$, $k \\in \\mathbb{N}$, with $\\sum_{k \\ge 1} a_k < \\infty$, such that\n$G_X(z) = z^{m} \\exp\\Big\\{\\sum_{k=1}^{\\infty} a_k \\,(z^{k} - 1)\\Big\\}$ for $z \\in [0,1]$. For $X \\sim \\mathrm{Bin}(n,p)$ we have $G_X(z) = (1 - p + p z)^{n}$, and $\\log G_X(z) = n \\log(1 - p + p z)$ has alternating-sign Maclaurin coefficients in $z$ when $p \\in (0,1)$, $n \\ge 1$. Therefore $G_X$ cannot be of the required exponential form unless $p \\in \\{0,1\\}$, so $\\mathrm{Bin}(n,p)$ is not infinitely divisible in the non-degenerate case.", "solution": "The problem statement is a valid question in the field of probability theory, specifically concerning the properties of infinitely divisible distributions. All terms are well-defined, and the statements to be evaluated are mathematically precise.\n\nAn infinitely divisible (ID) probability distribution $\\mu$ is one for which for any natural number $m \\in \\mathbb{N}$, there exist $m$ independent and identically distributed (i.i.d.) random variables whose sum follows the distribution $\\mu$. In terms of characteristic functions (CF), if $\\varphi(t)$ is the CF of $\\mu$, then for any $m \\in \\mathbb{N}$, the function $\\varphi(t)^{1/m}$ must also be a valid characteristic function.\n\nA key property of ID distributions, derived from the Lévy–Khintchine representation, is that their characteristic functions never take the value $0$. That is, if $\\mu$ is ID, its CF $\\varphi(t)$ satisfies $\\varphi(t) \\neq 0$ for all $t \\in \\mathbb{R}$.\n\nFor a random variable $X$ taking values in the non-negative integers $\\mathbb{Z}_{+}$, infinite divisibility has a specific characterization in terms of its probability generating function (p.g.f.), $G_X(z) = \\mathbb{E}[z^X]$. A distribution on $\\mathbb{Z}_{+}$ is ID if and only if its p.g.f. can be written in the form $G_X(z) = \\exp\\left(c(H(z)-1)\\right)$, where $c > 0$ and $H(z)$ is the p.g.f. of some distribution on $\\mathbb{Z}_{+}$. This is the p.g.f. of a compound Poisson distribution. An equivalent formulation, which also incorporates a deterministic shift, is given in option D.\n\nWe will now analyze each statement based on these principles.\n\n### Option A Analysis\nThe statement is: \"For any fixed $n \\in \\mathbb{N}$ and $p \\in (0,1)$, the $\\mathrm{Bin}(n,p)$ distribution is not infinitely divisible unless $p \\in \\{0,1\\}$. This follows because any non-degenerate infinitely divisible distribution with finite support would contradict the Lévy–Khintchine structural decomposition.\"\n\nA Binomial distribution $\\mathrm{Bin}(n,p)$ with $n \\in \\mathbb{N}$ and $p \\in (0,1)$ is a non-degenerate distribution (i.e., not a point mass) whose support is the finite set $\\{0, 1, \\dots, n\\}$.\nA fundamental theorem in the theory of infinitely divisible distributions states that a non-degenerate infinitely divisible distribution on $\\mathbb{R}$ must have unbounded support. This is a direct consequence of the Lévy–Khintchine representation. If an ID distribution has a compact support, its characteristic function must be of the form $\\varphi(t) = e^{i\\gamma t}$ for some $\\gamma \\in \\mathbb{R}$, which corresponds to a degenerate distribution (a point mass at $\\gamma$).\nSince the $\\mathrm{Bin}(n,p)$ distribution (for $n \\ge 1$, $p \\in (0,1)$) is non-degenerate and has a finite (and thus bounded) support, it cannot be infinitely divisible.\nThe degenerate cases $p=0$ and $p=1$ correspond to constant random variables $X=0$ and $X=n$, respectively. These are point mass distributions ($\\delta_0$ and $\\delta_n$), which are trivially infinitely divisible. For example, for $X=n$, its CF is $\\varphi(t) = e^{int}$. Then $\\varphi(t)^{1/m} = e^{i(n/m)t}$ is the CF of a constant random variable $n/m$. For $X=n$ to be an ID distribution on the integers, the i.i.d. components must also be integer-valued, which is not generally possible. However, on $\\mathbb{R}$, it is ID. If we restrict to ID distributions on $\\mathbb{Z}$, a constant $n$ is ID if and only if for every $m$, $n$ is divisible by $m$, which implies $n=0$. However, the problem is framed on $\\mathbb{R}$ where this is not an issue. The core of the statement is about the non-degenerate case $p \\in (0,1)$, and the reasoning provided is sound.\n\nThe statement is a correct application of a major structural theorem about infinitely divisible distributions.\n\nVerdict: **Correct**.\n\n### Option B Analysis\nThe statement is: \"The $\\mathrm{Poisson}(\\lambda)$ distribution is infinitely divisible for every $\\lambda > 0$, with characteristic exponent $\\psi(t) = \\lambda\\,(e^{i t} - 1)$ corresponding to a Lévy measure $\\nu = \\lambda \\,\\delta_{1}$.\"\n\nLet $Y \\sim \\mathrm{Poisson}(\\lambda)$. Its characteristic function is given by:\n$$ \\varphi_Y(t) = \\mathbb{E}[e^{itY}] = \\sum_{k=0}^{\\infty} e^{itk} \\frac{e^{-\\lambda}\\lambda^k}{k!} = e^{-\\lambda} \\sum_{k=0}^{\\infty} \\frac{(\\lambda e^{it})^k}{k!} = e^{-\\lambda} e^{\\lambda e^{it}} = \\exp\\{\\lambda(e^{it} - 1)\\} $$\nTo check for infinite divisibility, we examine if $(\\varphi_Y(t))^{1/m}$ is a valid CF for any $m \\in \\mathbb{N}$:\n$$ (\\varphi_Y(t))^{1/m} = \\left(\\exp\\{\\lambda(e^{it} - 1)\\}\\right)^{1/m} = \\exp\\left\\{\\frac{\\lambda}{m}(e^{it} - 1)\\right\\} $$\nThis is the characteristic function of a $\\mathrm{Poisson}(\\lambda/m)$ distribution. Since $\\lambda > 0$, $\\lambda/m > 0$ for all $m \\in \\mathbb{N}$, so this is always a valid probability distribution. Thus, the Poisson distribution is infinitely divisible.\n\nThe characteristic exponent is $\\psi(t) = \\log \\varphi_Y(t) = \\lambda(e^{it} - 1)$.\nThe Lévy-Khintchine representation for a compound Poisson process has a characteristic exponent of the form $\\int_{\\mathbb{R}\\setminus\\{0\\}}(e^{itx} - 1)\\nu(dx)$. For a simple Poisson process, the jumps are all of size $1$. This corresponds to a Lévy measure $\\nu$ that represents a mass of $\\lambda$ at the point $x=1$, i.e., $\\nu = \\lambda \\delta_1$, where $\\delta_1$ is the Dirac delta measure at $1$.\nPlugging this measure into the integral gives:\n$$ \\int_{\\mathbb{R}\\setminus\\{0\\}}(e^{itx} - 1) (\\lambda \\delta_1(dx)) = \\lambda(e^{it \\cdot 1} - 1) = \\lambda(e^{it} - 1) $$\nThis matches the characteristic exponent derived from the CF. Thus, all parts of the statement are accurate.\n\nVerdict: **Correct**.\n\n### Option C Analysis\nThe statement is: \"The characteristic function of $\\mathrm{Bin}(n,p)$ has a zero at $t = \\pi$ for all $p \\in (0,1)$, which proves that $\\mathrm{Bin}(n,p)$ is not infinitely divisible.\"\n\nLet $X \\sim \\mathrm{Bin}(n,p)$. Its characteristic function is $\\varphi_X(t) = (1-p+pe^{it})^n$.\nWe evaluate this at $t=\\pi$:\n$$ \\varphi_X(\\pi) = (1-p+pe^{i\\pi})^n = (1-p+p(-1))^n = (1-2p)^n $$\nFor $\\varphi_X(\\pi)$ to be zero, we must have $1-2p=0$, which implies $p=1/2$. The statement claims that the CF has a zero at $t=\\pi$ for ALL $p \\in (0,1)$. This is false. For instance, if $p=1/4$, $\\varphi_X(\\pi) = (1-2(1/4))^n = (1/2)^n \\neq 0$.\nThe premise of the statement is incorrect. While it is true that having a zero in the CF (as is the case for $p=1/2$) proves non-infinite-divisibility, the statement incorrectly generalizes this specific case. Because the premise is false, the entire logical statement is flawed.\n\nVerdict: **Incorrect**.\n\n### Option D Analysis\nThe statement is: \"Let $G_X(z) = \\mathbb{E}[z^{X}]$ denote the probability generating function (p.g.f.) of a nonnegative integer-valued random variable $X$. If $X$ is infinitely divisible and supported on $\\mathbb{Z}_{+}$, then there exist $m \\in \\mathbb{Z}_{+}$, coefficients $a_k \\ge 0$, $k \\in \\mathbb{N}$, with $\\sum_{k \\ge 1} a_k < \\infty$, such that $G_X(z) = z^{m} \\exp\\Big\\{\\sum_{k=1}^{\\infty} a_k \\,(z^{k} - 1)\\Big\\}$ for $z \\in [0,1]$. For $X \\sim \\mathrm{Bin}(n,p)$ we have $G_X(z) = (1 - p + p z)^{n}$, and $\\log G_X(z) = n \\log(1 - p + p z)$ has alternating-sign Maclaurin coefficients in $z$ when $p \\in (0,1)$, $n \\ge 1$. Therefore $G_X$ cannot be of the required exponential form unless $p \\in \\{0,1\\}$, so $\\mathrm{Bin}(n,p)$ is not infinitely divisible in the non-degenerate case.\"\n\nThe first part of the statement describes the canonical representation of the p.g.f. for an infinitely divisible distribution on $\\mathbb{Z}_{+}$. This representation is correct. The function $\\Psi(z) = \\log G_X(z)$ must have the form $\\Psi(z) = m \\log z + \\sum_{k=1}^{\\infty} a_k (z^k - 1) = m \\log z - \\sum_{k=1}^\\infty a_k + \\sum_{k=1}^\\infty a_k z^k$. This implies that the Taylor series of $\\log G_X(z)$ around $z=0$, ignoring any potential $\\log z$ term and the constant term, must have all non-negative coefficients ($a_k \\ge 0$).\n\nFor $X \\sim \\mathrm{Bin}(n,p)$, the p.g.f. is $G_X(z) = (1-p+pz)^n$. The support is $\\{0, 1, \\dots, n\\}$, so the shift $m$ must be $0$. We analyze $\\log G_X(z) = n\\log(1-p+pz)$. We find its Maclaurin series expansion.\nLet $f(z) = \\log(1-p+pz)$. The general Maclaurin series for $\\log(1+u)$ is $\\sum_{k=1}^{\\infty} (-1)^{k-1} \\frac{u^k}{k}$.\nLet's rewrite $f(z) = \\log\\left((1-p)\\left(1+\\frac{pz}{1-p}\\right)\\right) = \\log(1-p) + \\log\\left(1+\\frac{pz}{1-p}\\right)$.\nFor $p \\in (0,1)$, we have $|\\frac{pz}{1-p}| < 1$ for $z$ in a neighborhood of $0$.\n$$ \\log G_X(z) = n \\left[ \\log(1-p) + \\sum_{k=1}^{\\infty} \\frac{(-1)^{k-1}}{k} \\left(\\frac{pz}{1-p}\\right)^k \\right] = n\\log(1-p) + n \\sum_{k=1}^{\\infty} \\frac{(-1)^{k-1}}{k} \\left(\\frac{p}{1-p}\\right)^k z^k $$\nThe coefficient of $z^k$ in the series for $\\log G_X(z)$ is $c_k = n \\frac{(-1)^{k-1}}{k} (\\frac{p}{1-p})^k$ for $k \\ge 1$.\nFor $k=1$, the coefficient is $c_1 = n \\frac{p}{1-p}$, which is positive.\nFor $k=2$, the coefficient is $c_2 = -n \\frac{1}{2} (\\frac{p}{1-p})^2$. Since $n \\ge 1$ and $p \\in (0,1)$, this coefficient is strictly negative.\nThe canonical representation requires the coefficients of $z^k$ for $k \\ge 1$ (the $a_k$ values) to be non-negative. The presence of a negative coefficient for $z^2$ (and for all even $k$) contradicts this requirement.\nTherefore, the p.g.f. of a non-degenerate Binomial distribution cannot be written in the required form for an infinitely divisible distribution. The logic and calculations in the statement are correct.\n\nVerdict: **Correct**.", "answer": "$$\\boxed{ABD}$$"}, {"introduction": "Our final practice delves into a more specialized and powerful area of the theory by introducing self-decomposable distributions, an important subclass of infinitely divisible laws. This exercise not only tests your ability to differentiate between these two concepts but also reveals a profound connection between self-decomposability and the stationary behavior of linear stochastic differential equations driven by Lévy noise. Successfully navigating this problem will illuminate the practical utility of these abstract classifications in modeling dynamic systems [@problem_id:2980590].", "id": "2980590", "problem": "Let $X$ be a real-valued random variable with distribution $\\mu$ on $\\mathbb{R}$. Consider the concepts of infinite divisibility and self-decomposability as they arise in the theory of Lévy processes and stationary solutions to linear stochastic differential equations. Select all statements that are correct, where each correct choice must accurately differentiate infinite divisibility from self-decomposability in terms of first-principles definitions, and at least one correct choice must furnish a valid counterexample to show that the implication from self-decomposability to infinite divisibility cannot be reversed. \n\nA. A probability measure $\\mu$ on $\\mathbb{R}$ is infinitely divisible if for every $n \\in \\mathbb{N}$ there exists a probability measure $\\mu_{n}$ such that $\\mu = \\mu_{n}^{* n}$, equivalently, the characteristic function $\\varphi(t)$ of $\\mu$ has a Lévy–Khintchine representation with a Lévy measure. A probability measure $\\mu$ is self-decomposable if for every $c \\in (0,1)$ there exists a probability measure $\\rho_{c}$ such that $\\mu = \\mu \\circ D_{c} * \\rho_{c}$, where $D_{c}$ is the dilation $x \\mapsto c x$, equivalently, if $X \\sim \\mu$ then for every $c \\in (0,1)$ there exists an independent $Y_{c}$ such that $X \\stackrel{d}{=} c X^{\\prime} + Y_{c}$ with $X^{\\prime} \\stackrel{d}{=} X$. Every self-decomposable law is infinitely divisible. As a counterexample to the converse, the unit-jump compound Poisson law (i.e., $N \\sim \\mathrm{Poisson}(\\lambda)$ viewed on $\\mathbb{R}$ with Lévy measure $\\nu = \\lambda \\delta_{\\{1\\}}$) is infinitely divisible but not self-decomposable. \n\nB. Every strictly $\\alpha$-stable law on $\\mathbb{R}$ with $\\alpha \\in (0,2)$ is infinitely divisible but not self-decomposable; in particular, the symmetric Cauchy law is a counterexample showing that infinite divisibility does not imply self-decomposability. \n\nC. A probability measure $\\mu$ is self-decomposable if there exists a single $n \\in \\mathbb{N}$ and a probability measure $\\eta$ such that $\\mu = \\eta^{* n}$; this implies infinite divisibility. \n\nD. Every compound Poisson distribution on $\\mathbb{R}$ is self-decomposable, because it is infinitely divisible and has no Gaussian part. \n\nE. If $L = \\{L_{t}\\}_{t \\geq 0}$ is a Lévy process and $b > 0$, then the stationary law (when it exists) of the linear stochastic differential equation $d X_{t} = - b X_{t} \\, d t + d L_{t}$ is self-decomposable and hence infinitely divisible.", "solution": "The user requires a critical validation of the problem statement, followed by a detailed solution and evaluation of each option.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n- Let `$X$` be a real-valued random variable with distribution `$\\mu$` on `$\\mathbb{R}$`.\n- The problem is about the concepts of infinite divisibility and self-decomposability in the context of Lévy processes and stochastic differential equations.\n- The task is to select all correct statements.\n- A correct statement must accurately differentiate infinite divisibility from self-decomposability based on first-principles definitions.\n- At least one correct choice must furnish a valid counterexample to show that the implication from self-decomposability to infinite divisibility cannot be reversed (i.e., that infinite divisibility does not imply self-decomposability).\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientific Grounding:** The concepts of infinite divisibility, self-decomposability, Lévy processes, the Lévy-Khintchine representation, and Ornstein-Uhlenbeck type processes are all standard, well-established topics in advanced probability theory and stochastic analysis. The problem is scientifically and mathematically sound.\n- **Well-Posed:** The problem is a multiple-choice, multiple-answer question asking for the identification of correct mathematical statements from a given list. This is a well-defined task. The constraints on what constitutes a correct answer are clearly specified.\n- **Objective:** The question is objective, as the correctness of each statement can be determined through rigorous mathematical proof and established theorems.\n\n**Step 3: Verdict and Action**\nThe problem statement is valid. It is a well-posed, scientifically grounded question in the field of probability theory. I will proceed with a full solution and analysis of each option.\n\n### First Principles and Definitions\n\nA probability measure `$\\mu$` on `$\\mathbb{R}$` is **infinitely divisible (ID)** if for every integer `$n \\in \\mathbb{N}$`, there exists a probability measure `$\\mu_n$` such that `$\\mu = \\mu_n^{*n}$`, where `$*n$` denotes the `$n`-fold convolution. In terms of characteristic functions (CFs), the CF `$\\varphi(t)$` of `$\\mu$` corresponds to an ID distribution if and only if `$\\varphi(t)^{1/n}$` is a CF for all `$n \\in \\mathbb{N}$`. The Lévy-Khintchine theorem states that `$\\mu$` is ID if and only if its CF has the representation\n$$ \\varphi(t) = \\exp\\left(i \\gamma t - \\frac{1}{2} \\sigma^2 t^2 + \\int_{\\mathbb{R}\\setminus\\{0\\}} (e^{itx} - 1 - i t x \\mathbf{1}_{|x| \\le 1}) \\, \\nu(dx) \\right) $$\nfor a unique Lévy triplet `$(\\gamma, \\sigma^2, \\nu)$`, where `$\\gamma \\in \\mathbb{R}$`, `$\\sigma^2 \\ge 0$`, and `$\\nu$` is the Lévy measure satisfying `$\\int_{\\mathbb{R}\\setminus\\{0\\}} \\min(1, x^2) \\, \\nu(dx) < \\infty$`.\n\nA probability measure `$\\mu$` is **self-decomposable (SD)**, or a class `$L$` distribution, if for every constant `$c \\in (0,1)$`, there exists a probability measure `$\\rho_c$` such that the CF of `$\\mu$`, `$\\varphi(t)$`, satisfies `$\\varphi(t) = \\varphi(ct)\\psi_c(t)$`, where `$\\psi_c(t)$` is the CF of `$\\rho_c$`. Equivalently, a random variable `$X \\sim \\mu$` is SD if for every `$c \\in (0,1)$`, there exists a random variable `$Y_c \\sim \\rho_c$`, independent of `$X$`, such that `$X \\stackrel{d}{=} cX + Y_c$`.\n\nEvery SD distribution is ID. An ID distribution with Lévy measure `$\\nu$` is SD if and only if `$\\nu$` has a density with respect to the Lebesgue measure of the form `$\\frac{k(x)}{|x|}$`, where `$k(x)$` is non-increasing on `$(0, \\infty)$` and non-decreasing on `$(-\\infty, 0)$`.\n\n### Option-by-Option Analysis\n\n**A. A probability measure $\\mu$ on $\\mathbb{R}$ is infinitely divisible if for every $n \\in \\mathbb{N}$ there exists a probability measure $\\mu_{n}$ such that $\\mu = \\mu_{n}^{* n}$, equivalently, the characteristic function $\\varphi(t)$ of $\\mu$ has a Lévy–Khintchine representation with a Lévy measure. A probability measure $\\mu$ is self-decomposable if for every $c \\in (0,1)$ there exists a probability measure $\\rho_{c}$ such that $\\mu = \\mu \\circ D_{c} * \\rho_{c}$, where $D_{c}$ is the dilation $x \\mapsto c x$, equivalently, if $X \\sim \\mu$ then for every $c \\in (0,1)$ there exists an independent $Y_{c}$ such that $X \\stackrel{d}{=} c X^{\\prime} + Y_{c}$ with $X^{\\prime} \\stackrel{d}{=} X$. Every self-decomposable law is infinitely divisible. As a counterexample to the converse, the unit-jump compound Poisson law (i.e., $N \\sim \\mathrm{Poisson}(\\lambda)$ viewed on $\\mathbb{R}$ with Lévy measure $\\nu = \\lambda \\delta_{\\{1\\}}$) is infinitely divisible but not self-decomposable.**\n\nThis statement provides correct definitions for both ID and SD distributions, although the notation `$\\mu \\circ D_c$` for the law of `$cX$` is non-standard. The standard notation for the pushforward measure is `$c_*\\mu$`. However, the ambiguity is resolved by the immediately following, correct, equivalent definition in terms of random variables: `$X \\stackrel{d}{=} cX' + Y_c$`. The statement that every SD law is ID is a correct fundamental theorem. The provided counterexample is also correct. A compound Poisson process with a single jump size at `$x=1$` has Lévy measure `$\\nu = \\lambda \\delta_{\\{1\\}}$`, where `$\\delta_{\\{1\\}}$` is the Dirac measure at `$1$`. This distribution is ID, as its CF `$\\varphi(t) = \\exp(\\lambda(e^{it}-1))$` has the property that `$\\varphi(t)^{1/n} = \\exp(\\frac{\\lambda}{n}(e^{it}-1))$` is the CF of a Poisson distribution with parameter `$\\lambda/n$`. However, it is not SD because its Lévy measure is a point mass and not absolutely continuous with respect to the Lebesgue measure, let alone having the required density form `$\\frac{k(x)}{|x|}$`. Thus, this option correctly differentiates the concepts and provides the required counterexample.\n**Verdict: Correct**\n\n**B. Every strictly $\\alpha$-stable law on $\\mathbb{R}$ with $\\alpha \\in (0,2)$ is infinitely divisible but not self-decomposable; in particular, the symmetric Cauchy law is a counterexample showing that infinite divisibility does not imply self-decomposability.**\n\nThis statement is fundamentally flawed. All stable laws are ID. Moreover, all stable laws are also self-decomposable. For a strictly `$\\alpha`'-stable random variable `$X$`, the definition of stability implies that for any `$a, b > 0$`, there exists a `$d > 0$` such that `$aX_1 + bX_2 \\stackrel{d}{=} dX_3$`, where `$X_1, X_2, X_3$` are i.i.d copies. Specifically, `$dX_3 = (a^\\alpha + b^\\alpha)^{1/\\alpha} X_3$`. If we set `$a=c \\in (0,1)$` and choose `$b = (1-c^\\alpha)^{1/\\alpha}$`, we get `$cX_1 + (1-c^\\alpha)^{1/\\alpha}X_2 \\stackrel{d}{=} (c^\\alpha + (1-c^\\alpha))^{1/\\alpha}X_3 = X_3$`. This is precisely the defining property of a self-decomposable random variable, `$X \\stackrel{d}{=} cX' + Y_c$`, where `$Y_c = (1-c^\\alpha)^{1/\\alpha}X_2$`. Since stable laws are self-decomposable, they cannot serve as a counterexample to show that `ID $\\not\\Rightarrow$ SD`. The symmetric Cauchy law is `$\\alpha`'-stable with `$\\alpha=1$`, so it is self-decomposable.\n**Verdict: Incorrect**\n\n**C. A probability measure $\\mu$ is self-decomposable if there exists a single $n \\in \\mathbb{N}$ and a probability measure $\\eta$ such that $\\mu = \\eta^{* n}$; this implies infinite divisibility.**\n\nThis statement incorrectly defines self-decomposability. The condition `$\\mu = \\eta^{*n}$` for some `$n \\in \\mathbb{N}$` is the definition of `$n`'-divisibility, not self-decomposability. Self-decomposability involves scaling by real numbers `$c \\in (0,1)$`, not integer-fold convolution. Furthermore, the claim that `$n`'-divisibility implies infinite divisibility is false. For example, a binomial distribution `$\\text{Bin}(n_0, p)$` with `$0<p<1$` and `$n_0 > 1$` is `$n_0`'-divisible (as a sum of `$n_0$` i.i.d. Bernoulli variables), but it is not infinitely divisible. An ID distribution on the integers with bounded support must be a point mass, which a non-trivial binomial distribution is not.\n**Verdict: Incorrect**\n\n**D. Every compound Poisson distribution on $\\mathbb{R}$ is self-decomposable, because it is infinitely divisible and has no Gaussian part.**\n\nThis statement is false. As demonstrated in the analysis of option A, a simple compound Poisson law with a single jump size is not self-decomposable. More generally, a non-trivial compound Poisson distribution has a Lévy measure `$\\nu$` with finite total mass, `$\\nu(\\mathbb{R} \\setminus \\{0\\}) < \\infty$`. For such a distribution to be self-decomposable, its Lévy measure must have a density `$k(x)/|x|$`. But if `$\\int k(x)/|x| dx < \\infty` and `$k(x)` is monotone as required, then `$k(x)` must be identically zero. This corresponds to a trivial Lévy measure, meaning no jumps, which contradicts the premise of a non-trivial compound Poisson distribution. The reasoning \"because it is infinitely divisible and has no Gaussian part\" is also invalid, as there is no such theorem.\n**Verdict: Incorrect**\n\n**E. If $L = \\{L_{t}\\}_{t \\geq 0}$ is a Lévy process and $b > 0$, then the stationary law (when it exists) of the linear stochastic differential equation $d X_{t} = - b X_{t} \\, d t + d L_{t}$ is self-decomposable and hence infinitely divisible.**\n\nThis statement correctly identifies a fundamental property of self-decomposable distributions. The SDE describes an Ornstein-Uhlenbeck (OU) type process driven by a Lévy process. The stationary solution, if it exists, is given by the random variable `$X_{\\infty} = \\int_0^\\infty e^{-bs} \\, dL_s`. We can verify its self-decomposability. For any `$c \\in (0,1)$`, choose `$\\tau > 0$` such that `$c = e^{-b\\tau}$`. We can split the integral:\n$$ X_{\\infty} = \\int_0^\\tau e^{-bs} \\, dL_s + \\int_\\tau^\\infty e^{-bs} \\, dL_s $$\nFactoring `$e^{-b\\tau}$` from the second term and changing variables `$u=s-\\tau$`, we get:\n$$ X_{\\infty} = \\int_0^\\tau e^{-bs} \\, dL_s + e^{-b\\tau} \\int_0^\\infty e^{-bu} \\, dL_{u+\\tau} $$\nThe process `$\\tilde{L}_u = L_{u+\\tau} - L_\\tau$` is a Lévy process with the same law as `$L$` and is independent of the process `$\\{L_s\\}_{0 \\le s \\le \\tau}``. Therefore, `$Y_c = \\int_0^\\tau e^{-bs} \\, dL_s$` is independent of `$X'_\\infty = \\int_0^\\infty e^{-bu} \\, d\\tilde{L}_u$`, which has the same distribution as `$X_\\infty$`. This gives the relation `$X_\\infty \\stackrel{d}{=} Y_c + c X'_\\infty$`, confirming that the stationary law is self-decomposable. As every self-decomposable law is infinitely divisible, the entire statement is correct. This is in fact a characterization: the class of self-decomposable distributions is precisely the class of stationary distributions of OU-type processes driven by Lévy processes.\n**Verdict: Correct**", "answer": "$$\\boxed{AE}$$"}]}