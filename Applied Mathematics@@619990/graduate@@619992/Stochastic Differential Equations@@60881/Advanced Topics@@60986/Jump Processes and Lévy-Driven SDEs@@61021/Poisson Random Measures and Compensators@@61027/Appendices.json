{"hands_on_practices": [{"introduction": "We begin our hands-on exploration with a foundational concept: identifying the compensator of a Poisson random measure (PRM) in a straightforward yet crucial scenario. This exercise asks you to demonstrate that for a PRM with a deterministic intensity measure that is independent of the underlying filtration, its compensator is simply the intensity measure itself. Mastering this fundamental case [@problem_id:2990792] is the first step toward understanding how information and randomness shape the predictable structure of jump processes.", "problem": "Let $(\\Omega,\\mathcal{F},\\mathbb{P})$ be a probability space equipped with a filtration $(\\mathcal{F}_t)_{t \\ge 0}$ satisfying the usual conditions (right-continuity and completeness). Let $(E,\\mathcal{E})$ be a Lusin measurable space, and let $\\nu$ be a $\\sigma$-finite measure on $(E,\\mathcal{E})$. Consider a Poisson random measure (PRM) $N$ on $[0,\\infty) \\times E$ with deterministic intensity measure $\\Lambda$ given by\n$$\n\\Lambda(A) \\equiv \\int_{A} dt\\,\\nu(dx), \\quad A \\in \\mathcal{B}([0,\\infty)) \\otimes \\mathcal{E},\n$$\nand assume $N$ is independent of $(\\mathcal{F}_t)_{t \\ge 0}$. Define the enlarged filtration $(\\mathcal{G}_t)_{t \\ge 0}$ by $\\mathcal{G}_t := \\mathcal{F}_t \\vee \\sigma\\!\\big(N\\big((0,s] \\times B\\big): 0 \\le s \\le t,\\, B \\in \\mathcal{E}\\big)\\big)$, and let $\\mathcal{P}(\\mathcal{G})$ denote the predictable $\\sigma$-algebra on $[0,\\infty) \\times \\Omega$ with respect to $(\\mathcal{G}_t)_{t \\ge 0}$.\n\nThe compensator $\\mu$ of $N$ with respect to $(\\mathcal{G}_t)_{t \\ge 0}$ is defined as the unique $(\\mathcal{P}(\\mathcal{G}) \\otimes \\mathcal{E})$-measurable random measure on $[0,\\infty) \\times E$ such that for every nonnegative $(\\mathcal{P}(\\mathcal{G}) \\otimes \\mathcal{E})$-measurable function $f$ with\n$$\n\\mathbb{E}\\!\\left[\\int_{(0,\\infty) \\times E} f(t,x)\\,N(dt,dx)\\right] < \\infty,\n$$\nit holds that\n$$\n\\mathbb{E}\\!\\left[\\int_{(0,\\infty) \\times E} f(t,x)\\,N(dt,dx)\\right]\n=\n\\mathbb{E}\\!\\left[\\int_{(0,\\infty) \\times E} f(t,x)\\,\\mu(dt,dx)\\right].\n$$\n\nUsing only the defining properties of a Poisson random measure and the definition of the compensator, identify the compensator $\\mu(dt,dx)$ explicitly as a measure on $[0,\\infty) \\times E$ in closed form. Your final answer must be a single analytic expression and must not include any derivation. If you introduce any auxiliary quantities, express the final result entirely in terms of $dt$ and $\\nu(dx)$.", "solution": "The problem requires us to identify the compensator $\\mu$ of a Poisson random measure (PRM) $N$ whose intensity measure $\\Lambda(dt,dx) = dt\\,\\nu(dx)$ is deterministic. The filtration $(\\mathcal{G}_t)_{t \\ge 0}$ with respect to which we compensate is the one generated by $N$ itself, enlarged by an independent filtration $(\\mathcal{F}_t)_{t \\ge 0}$.\n\nFirst, we validate the problem statement.\nThe givens are:\n1.  A probability space $(\\Omega,\\mathcal{F},\\mathbb{P})$ with a filtration $(\\mathcal{F}_t)_{t \\ge 0}$ satisfying the usual conditions.\n2.  A Lusin measurable space $(E,\\mathcal{E})$ with a $\\sigma$-finite measure $\\nu$.\n3.  A Poisson random measure $N$ on $[0,\\infty) \\times E$ with a deterministic intensity measure $\\Lambda(A) = \\int_{A} dt\\,\\nu(dx)$.\n4.  $N$ is independent of the filtration $(\\mathcal{F}_t)_{t \\ge 0}$.\n5.  The enlarged filtration is $\\mathcal{G}_t := \\mathcal{F}_t \\vee \\sigma\\!\\big(N\\big((0,s] \\times B\\big): 0 \\le s \\le t,\\, B \\in \\mathcal{E}\\big)\\big)$.\n6.  The compensator $\\mu$ is a $(\\mathcal{P}(\\mathcal{G}) \\otimes \\mathcal{E})$-measurable random measure satisfying $\\mathbb{E}\\!\\left[\\int f\\,dN\\right] = \\mathbb{E}\\!\\left[\\int f\\,d\\mu\\right]$ for any suitable non-negative measurable function $f$.\n\nThe problem is scientifically grounded, well-posed, and objective. It presents a standard, canonical problem in the theory of stochastic processes. All terms are well-defined, and the necessary conditions (Lusin space, $\\sigma$-finite measure, independence) are provided to ensure the existence and uniqueness of the compensator. The problem is valid.\n\nWe proceed to find the compensator $\\mu(dt,dx)$. The definition of the compensator provided in the problem statement is equivalent to a more direct, operational definition based on martingales. A random measure $\\mu$ is the compensator of a random measure $N$ with respect to a filtration $(\\mathcal{G}_t)_{t \\ge 0}$ if and only if $\\mu$ is predictable and for any non-negative, suitably measurable function $f(t,x)$, the process\n$$\nM_t^f = \\int_0^t \\int_E f(s,x) N(ds,dx) - \\int_0^t \\int_E f(s,x) \\mu(ds,dx)\n$$\nis a $(\\mathcal{G}_t)$-local martingale. By a monotone class argument, it is sufficient to identify the measure $\\mu$ by considering its action on sets of the form $(0,t] \\times B$ where $B \\in \\mathcal{E}$ is a set with $\\nu(B) < \\infty$.\n\nLet $N_t(B) := N((0,t] \\times B)$ be the counting process of points falling in the set $B$. Let $\\hat{N}_t(B) := \\mu((0,t] \\times B)$ be the corresponding compensator process. For $\\mu$ to be the compensator of $N$, the process $M_t(B) := N_t(B) - \\hat{N}_t(B)$ must be a $(\\mathcal{G}_t)$-martingale for every $B \\in \\mathcal{E}$ with $\\nu(B) < \\infty$.\n\nThe martingale property for $M_t(B)$ requires that for any $s < t$, $\\mathbb{E}[M_t(B) | \\mathcal{G}_s] = M_s(B)$. This is equivalent to\n$$\n\\mathbb{E}[N_t(B) - \\hat{N}_t(B) | \\mathcal{G}_s] = N_s(B) - \\hat{N}_s(B),\n$$\nwhich can be rearranged to\n$$\n\\mathbb{E}[N_t(B) - N_s(B) | \\mathcal{G}_s] = \\mathbb{E}[\\hat{N}_t(B) - \\hat{N}_s(B) | \\mathcal{G}_s].\n$$\nBy definition, the compensator $\\mu$ is a predictable measure. This means that the process $\\hat{N}_t(B)$ is a predictable process with respect to $(\\mathcal{G}_t)_{t \\ge 0}$. A predictable process is adapted, so $\\hat{N}_t(B)$ is $\\mathcal{G}_t$-measurable for all $t \\ge 0$. Furthermore, because it is predictable, the increment $\\hat{N}_t(B) - \\hat{N}_s(B)$ is measurable with respect to $\\mathcal{G}_s$. Thus, its conditional expectation is itself:\n$$\n\\mathbb{E}[\\hat{N}_t(B) - \\hat{N}_s(B) | \\mathcal{G}_s] = \\hat{N}_t(B) - \\hat{N}_s(B) = \\mu((s,t] \\times B).\n$$\nThe left-hand side of the equation involves the increment of the Poisson random measure, $N_t(B) - N_s(B) = N((s,t] \\times B)$. So, the condition for $\\mu$ being the compensator becomes\n$$\n\\mu((s,t] \\times B) = \\mathbb{E}[N((s,t] \\times B) | \\mathcal{G}_s].\n$$\nWe now analyze the conditional expectation. The filtration $\\mathcal{G}_s$ is defined as $\\mathcal{G}_s = \\mathcal{F}_s \\vee \\sigma_s^N$, where $\\sigma_s^N = \\sigma(N((0,u]\\times C) : 0 \\le u \\le s, C \\in \\mathcal{E})$.\nThe random variable $N((s,t] \\times B)$ depends on the PRM $N$ on the time interval $(s,t]$. We use the following defining properties of a PRM:\n1.  **Independent increments:** For any collection of disjoint sets $A_1, \\dots, A_k$ in $\\mathcal{B}([0,\\infty)) \\otimes \\mathcal{E}$, the random variables $N(A_1), \\dots, N(A_k)$ are independent. This implies that $N((s,t] \\times B)$ is independent of the sigma-algebra $\\sigma_s^N$, which is generated by $N$ on the time interval $[0,s]$.\n2.  **Given independence:** The problem states that the entire PRM process $N$ is independent of the filtration $(\\mathcal{F}_t)_{t \\ge 0}$. This implies that $N((s,t] \\times B)$ is independent of $\\mathcal{F}_s$.\n\nSince the random variable $N((s,t] \\times B)$ is independent of both $\\mathcal{F}_s$ and $\\sigma_s^N$, it is independent of the sigma-algebra they generate, $\\mathcal{G}_s = \\mathcal{F}_s \\vee \\sigma_s^N$.\nTherefore, the conditional expectation is equal to the unconditional expectation:\n$$\n\\mathbb{E}[N((s,t] \\times B) | \\mathcal{G}_s] = \\mathbb{E}[N((s,t] \\times B)].\n$$\nAnother defining property of a PRM is that the expectation of $N(A)$ is given by its intensity measure $\\Lambda(A)$. Thus,\n$$\n\\mathbb{E}[N((s,t] \\times B)] = \\Lambda((s,t] \\times B) = \\int_s^t du \\int_B \\nu(dx) = (t-s)\\nu(B).\n$$\nBy equating the expressions for the left and right sides of our martingale condition, we find:\n$$\n\\mu((s,t] \\times B) = (t-s)\\nu(B).\n$$\nThis equality must hold for all $0 \\le s < t$ and for all $B \\in \\mathcal{E}$ (initially for those with $\\nu(B) < \\infty$, but extending to all of $\\mathcal{E}$ because $\\nu$ is $\\sigma$-finite and both sides are measures in $B$). This uniquely determines the measure $\\mu$ on the space $[0,\\infty) \\times E$. The measure whose value on sets $(s,t] \\times B$ is $(t-s)\\nu(B)$ is precisely the product measure of the Lebesgue measure on $[0,\\infty)$ and the measure $\\nu$ on $E$.\nIn differential form, this is written as:\n$$\n\\mu(dt,dx) = dt\\,\\nu(dx).\n$$\nThis result is intuitive: since the underlying intensity of the Poisson process is deterministic and not influenced by any past events, its best prediction (the compensator) is simply the intensity itself. The independence assumptions are crucial for this conclusion.\nThe compensator $\\mu$ is therefore identical to the intensity measure $\\Lambda$. It is a deterministic measure, which is trivially predictable with respect to any filtration.", "answer": "$$\n\\boxed{dt\\,\\nu(dx)}\n$$", "id": "2990792"}, {"introduction": "Having established the identity of the compensator in the simplest case, we now turn to the compensated Poisson random measure, $\\tilde{N} = N - \\mu$, which is a martingale-valued measure. This practice explores a cornerstone of stochastic integration for jump processes: the orthogonality property, which is analogous to the ItÃ´ isometry for Brownian motion. By showing that integrals of a compensated PRM over disjoint sets are uncorrelated [@problem_id:2990785], you will uncover the fundamental statistical structure that makes building a coherent calculus for jump processes possible.", "problem": "Let $\\left(\\Omega,\\mathcal{F},(\\mathcal{F}_{s})_{s\\ge 0},\\mathbb{P}\\right)$ be a filtered probability space carrying a Poisson random measure $N$ on $[0,t]\\times E$ with compensator $\\Lambda(\\mathrm{d}s,\\mathrm{d}x)=\\mathrm{d}s\\,\\nu(\\mathrm{d}x)$, where $(E,\\mathcal{E})$ is a measurable space and $\\nu$ is a $\\sigma$-finite measure. Denote by $\\tilde{N}$ the compensated Poisson random measure defined by $\\tilde{N}(\\mathrm{d}s,\\mathrm{d}x)=N(\\mathrm{d}s,\\mathrm{d}x)-\\Lambda(\\mathrm{d}s,\\mathrm{d}x)$. Assume that for any measurable $A\\subset[0,t]\\times E$ with $\\Lambda(A)<\\infty$, the random variable $N(A)$ is Poisson distributed with mean $\\Lambda(A)$, and that $\\{N(A_{i})\\}_{i=1}^{m}$ are independent for pairwise disjoint sets $\\{A_{i}\\}_{i=1}^{m}$.\n\nLet $A_{1},\\dots,A_{m}\\subset[0,t]\\times E$ be pairwise disjoint predictable sets with $0<\\Lambda(A_{i})<\\infty$ for each $i\\in\\{1,\\dots,m\\}$. Define the real constants $a_{1},\\dots,a_{m}$ and $b_{1},\\dots,b_{m}$, and the random variables\n$$\nX=\\sum_{i=1}^{m}a_{i}\\,\\tilde{N}(A_{i}),\\qquad Y=\\sum_{i=1}^{m}b_{i}\\,\\tilde{N}(A_{i}).\n$$\nStarting only from the core definitions of a Poisson random measure and its compensator, first establish that each $\\tilde{N}(A_{i})$ is centered and that $\\tilde{N}(A_{i})$ and $\\tilde{N}(A_{j})$ are uncorrelated for $i\\neq j$. Then use these facts to compute the quantity $\\mathbb{E}[XY]$ in closed form as a function of $\\{a_{i}\\}$, $\\{b_{i}\\}$, and the compensator masses $\\{\\Lambda(A_{i})\\}$.\n\nYour final answer must be a single closed-form analytic expression. No rounding is required. Express $\\Lambda(A_{i})$ explicitly in terms of $\\nu$ if you wish, as $\\Lambda(A_{i})=\\int_{A_{i}}\\mathrm{d}s\\,\\nu(\\mathrm{d}x)$.", "solution": "The problem asks for the computation of the quantity $\\mathbb{E}[XY]$, where $X=\\sum_{i=1}^{m}a_{i}\\,\\tilde{N}(A_{i})$ and $Y=\\sum_{i=1}^{m}b_{i}\\,\\tilde{N}(A_{i})$. The process requires first establishing two fundamental properties of the compensated Poisson random measure $\\tilde{N}$ over disjoint sets.\n\nFirst, we establish that each random variable $\\tilde{N}(A_{i})$ is centered, meaning its expectation is zero.\nBy the definition of the compensated Poisson random measure, we have $\\tilde{N}(A_i) = N(A_i) - \\Lambda(A_i)$.\nThe expectation operator $\\mathbb{E}[\\cdot]$ is linear. Therefore,\n$$\n\\mathbb{E}[\\tilde{N}(A_{i})] = \\mathbb{E}[N(A_{i}) - \\Lambda(A_{i})] = \\mathbb{E}[N(A_{i})] - \\mathbb{E}[\\Lambda(A_i)].\n$$\nThe problem states that for any measurable set $A \\subset [0,t] \\times E$ with $\\Lambda(A) < \\infty$, the random variable $N(A)$ is Poisson distributed with mean $\\Lambda(A)$. Since we are given that $0 < \\Lambda(A_i) < \\infty$, it follows that $N(A_i)$ is a Poisson random variable with parameter $\\Lambda(A_i)$. The expectation of a Poisson distributed random variable is equal to its parameter, so\n$$\n\\mathbb{E}[N(A_{i})] = \\Lambda(A_{i}).\n$$\nThe compensator $\\Lambda(\\mathrm{d}s,\\mathrm{d}x) = \\mathrm{d}s\\,\\nu(\\mathrm{d}x)$ is deterministic. Thus, for any given set $A_{i}$, its measure $\\Lambda(A_{i})$ is a deterministic constant. The expectation of a constant is the constant itself:\n$$\n\\mathbb{E}[\\Lambda(A_{i})] = \\Lambda(A_{i}).\n$$\nSubstituting these results back, we find\n$$\n\\mathbb{E}[\\tilde{N}(A_{i})] = \\Lambda(A_{i}) - \\Lambda(A_{i}) = 0.\n$$\nThis confirms that $\\tilde{N}(A_{i})$ is a centered random variable for each $i \\in \\{1,\\dots,m\\}$.\n\nSecond, we establish that for $i \\neq j$, the random variables $\\tilde{N}(A_{i})$ and $\\tilde{N}(A_{j})$ are uncorrelated. Two random variables $U$ and $V$ are uncorrelated if their covariance is zero, i.e., $\\mathbb{E}[(U-\\mathbb{E}[U])(V-\\mathbb{E}[V])] = 0$. Since we have shown that $\\tilde{N}(A_{i})$ and $\\tilde{N}(A_{j})$ are centered, this condition simplifies to showing that $\\mathbb{E}[\\tilde{N}(A_{i})\\tilde{N}(A_{j})] = 0$.\nLet's consider this expectation for $i \\neq j$:\n$$\n\\mathbb{E}[\\tilde{N}(A_{i})\\tilde{N}(A_{j})] = \\mathbb{E}[(N(A_{i}) - \\Lambda(A_{i}))(N(A_{j}) - \\Lambda(A_{j}))].\n$$\nThe problem states that the sets $A_{1},\\dots,A_{m}$ are pairwise disjoint. It is a fundamental property of the Poisson random measure $N$ that for any collection of pairwise disjoint measurable sets $\\{B_k\\}$, the random variables $\\{N(B_k)\\}$ are independent. Therefore, for $i \\neq j$, $N(A_{i})$ and $N(A_{j})$ are independent random variables.\nIf two random variables $U$ and $V$ are independent, then for any measurable functions $f$ and $g$, $\\mathbb{E}[f(U)g(V)]=\\mathbb{E}[f(U)]\\mathbb{E}[g(V)]$. In our case, $N(A_{i}) - \\Lambda(A_{i})$ is a function of $N(A_{i})$, and $N(A_{j}) - \\Lambda(A_{j})$ is a function of $N(A_{j})$. Due to the independence of $N(A_{i})$ and $N(A_{j})$, their respective functions are also independent. Thus,\n$$\n\\mathbb{E}[(N(A_{i}) - \\Lambda(A_{i}))(N(A_{j}) - \\Lambda(A_{j}))] = \\mathbb{E}[N(A_{i}) - \\Lambda(A_{i})] \\cdot \\mathbb{E}[N(A_{j}) - \\Lambda(A_{j})].\n$$\nWe have already shown that $\\mathbb{E}[N(A_k) - \\Lambda(A_k)] = \\mathbb{E}[\\tilde{N}(A_k)] = 0$ for any $k$. Therefore, for $i \\neq j$,\n$$\n\\mathbb{E}[\\tilde{N}(A_{i})\\tilde{N}(A_{j})] = 0 \\cdot 0 = 0.\n$$\nThis proves that $\\tilde{N}(A_{i})$ and $\\tilde{N}(A_{j})$ are uncorrelated for $i \\neq j$.\n\nFinally, we compute $\\mathbb{E}[XY]$ using these facts. The random variables are $X=\\sum_{i=1}^{m}a_{i}\\,\\tilde{N}(A_{i})$ and $Y=\\sum_{j=1}^{m}b_{j}\\,\\tilde{N}(A_{j})$.\n$$\n\\mathbb{E}[XY] = \\mathbb{E}\\left[\\left(\\sum_{i=1}^{m}a_{i}\\,\\tilde{N}(A_{i})\\right)\\left(\\sum_{j=1}^{m}b_{j}\\,\\tilde{N}(A_{j})\\right)\\right].\n$$\nBy expanding the product inside the expectation, we get a double summation:\n$$\n\\mathbb{E}[XY] = \\mathbb{E}\\left[\\sum_{i=1}^{m}\\sum_{j=1}^{m} a_{i}b_{j}\\,\\tilde{N}(A_{i})\\tilde{N}(A_{j})\\right].\n$$\nUsing the linearity of expectation, we can move the summation signs outside:\n$$\n\\mathbb{E}[XY] = \\sum_{i=1}^{m}\\sum_{j=1}^{m} a_{i}b_{j}\\,\\mathbb{E}[\\tilde{N}(A_{i})\\tilde{N}(A_{j})].\n$$\nWe can split this double summation into two parts: one where $i=j$ and one where $i \\neq j$.\n$$\n\\mathbb{E}[XY] = \\sum_{i=1}^{m} a_{i}b_{i}\\,\\mathbb{E}[\\tilde{N}(A_{i})^{2}] + \\sum_{i=1}^{m}\\sum_{j=1, j\\neq i}^{m} a_{i}b_{j}\\,\\mathbb{E}[\\tilde{N}(A_{i})\\tilde{N}(A_{j})].\n$$\nFrom the second property we proved, $\\mathbb{E}[\\tilde{N}(A_{i})\\tilde{N}(A_{j})] = 0$ for $i \\neq j$. This causes the second term to vanish:\n$$\n\\sum_{i=1}^{m}\\sum_{j=1, j\\neq i}^{m} a_{i}b_{j}\\,(0) = 0.\n$$\nThe expression for $\\mathbb{E}[XY]$ simplifies to:\n$$\n\\mathbb{E}[XY] = \\sum_{i=1}^{m} a_{i}b_{i}\\,\\mathbb{E}[\\tilde{N}(A_{i})^{2}].\n$$\nNow we must evaluate $\\mathbb{E}[\\tilde{N}(A_{i})^{2}]$. Since $\\tilde{N}(A_{i})$ is a centered random variable, its second moment is equal to its variance:\n$$\n\\mathbb{E}[\\tilde{N}(A_{i})^{2}] = \\text{Var}(\\tilde{N}(A_{i})).\n$$\nThe variance is calculated as:\n$$\n\\text{Var}(\\tilde{N}(A_{i})) = \\text{Var}(N(A_{i}) - \\Lambda(A_{i})).\n$$\nSince $\\Lambda(A_i)$ is a deterministic constant, adding or subtracting it does not change the variance of a random variable. Thus,\n$$\n\\text{Var}(\\tilde{N}(A_{i})) = \\text{Var}(N(A_{i})).\n$$\nAs $N(A_i)$ follows a Poisson distribution with mean $\\Lambda(A_i)$, its variance is also $\\Lambda(A_i)$.\n$$\n\\text{Var}(N(A_{i})) = \\Lambda(A_{i}).\n$$\nTherefore, $\\mathbb{E}[\\tilde{N}(A_{i})^{2}] = \\Lambda(A_{i})$.\nSubstituting this result into our simplified expression for $\\mathbb{E}[XY]$ gives the final answer:\n$$\n\\mathbb{E}[XY] = \\sum_{i=1}^{m} a_{i}b_{i}\\,\\Lambda(A_{i}).\n$$", "answer": "$$\n\\boxed{\\sum_{i=1}^{m}a_{i}b_{i}\\Lambda(A_{i})}\n$$", "id": "2990785"}, {"introduction": "This final practice applies the previous concepts to a concrete and essential calculation in stochastic calculus. You will compute the quadratic variation for a martingale constructed as a stochastic integral with respect to a compensated PRM. The exercise [@problem_id:2990781] highlights the profound connection between the optional quadratic variation, $[M]_t$, which captures the realized energy of the process through its squared jumps, and the predictable quadratic variation, $\\langle M \\rangle_t$, which represents its expected, compensable part. This computation is a vital skill for anyone working with stochastic differential equations driven by jump processes.", "problem": "Let $\\left(\\Omega,\\mathcal{F},(\\mathcal{F}_{t})_{t \\ge 0},\\mathbb{P}\\right)$ be a filtered probability space satisfying the usual conditions and carrying a Poisson random measure (PRM) $N$ on $(0,\\infty)\\times\\mathbb{R}$ with intensity measure $ds\\,\\nu(dx)$, where $\\nu$ is a $\\sigma$-finite measure on $\\mathbb{R}$ with density $x\\mapsto \\kappa \\exp(-|x|)$ for some fixed $\\kappa>0$. Let $\\tilde{N}(ds,dx) \\coloneqq N(ds,dx)-ds\\,\\nu(dx)$ denote the compensated Poisson random measure.\n\nFix parameters $T>0$ and $\\alpha>-1/2$, and define the predictable integrand $H:(0,T]\\times\\mathbb{R}\\to\\mathbb{R}$ by $H(s,x)=s^{\\alpha}x$. Consider the purely discontinuous square-integrable martingale\n$$\nM_{t}\\coloneqq \\int_{0}^{t}\\int_{\\mathbb{R}} H(s,x)\\,\\tilde{N}(ds,dx),\\qquad 0\\le t\\le T.\n$$\nStarting only from the definitions of the compensator of a random measure, the compensated Poisson random measure, and the characterization of the predictable quadratic variation as the compensator of the optional quadratic variation for a purely discontinuous martingale, derive from first principles the predictable quadratic variation $\\langle M\\rangle_{T}$ and the expectation of the optional quadratic variation $\\mathbb{E}\\big([M]_{T}\\big)$. Conclude that they are equal and determine their common value as a closed-form analytic expression in terms of $\\kappa$, $T$, and $\\alpha$.\n\nYour final answer must be a single closed-form analytic expression with no units. Do not round; provide the exact symbolic expression.", "solution": "The problem statement is a well-posed and standard exercise in the theory of stochastic calculus for jump processes. It is scientifically grounded, self-contained, and all parameters and conditions are clearly defined and consistent. The condition $\\alpha > -1/2$ ensures that the stochastic integral defining the martingale $M_t$ is well-defined and square-integrable. Therefore, the problem is valid, and we may proceed with the solution.\n\nOur objective is to derive the predictable quadratic variation $\\langle M \\rangle_T$ and the expectation of the optional quadratic variation $\\mathbb{E}\\big([M]_T\\big)$ for the martingale\n$$\nM_{t}\\coloneqq \\int_{0}^{t}\\int_{\\mathbb{R}} H(s,x)\\,\\tilde{N}(ds,dx)\n$$\nwhere $H(s,x) = s^{\\alpha}x$, for $0 < s \\le T$ and $x \\in \\mathbb{R}$. The intensity measure of the Poisson random measure $N$ is $ds\\,\\nu(dx)$, with $\\nu(dx) = \\kappa \\exp(-|x|)dx$.\n\nFirst, we determine the optional quadratic variation, denoted $[M]_t$. For a purely discontinuous martingale, the optional quadratic variation is the sum of the squares of its jumps:\n$$\n[M]_t = \\sum_{0 < s \\le t} (\\Delta M_s)^2\n$$\nwhere $\\Delta M_s = M_s - M_{s-}$ is the jump of the process $M$ at time $s$. The jumps of the martingale $M_t$ are determined by the jumps of the Poisson random measure $N$. Specifically, the jump at time $s$ is given by\n$$\n\\Delta M_s = \\int_{\\mathbb{R}} H(s,x) N(\\{s\\}, dx)\n$$\nThe process $[M]_t$ can be expressed as a stochastic integral with respect to the Poisson random measure $N$. Since, with probability $1$, the measure $N(\\{s\\}, \\cdot)$ is either the zero measure or a Dirac measure at a single point $x_s$, the square of the jump becomes $(\\Delta M_s)^2 = (H(s, x_s))^2$ if there is a jump at $(s, x_s)$. Summing over all jumps up to time $t$ is equivalent to integrating the function $H(s,x)^2$ against the random measure $N(ds,dx)$. Thus, the optional quadratic variation process is\n$$\n[M]_t = \\int_{0}^{t}\\int_{\\mathbb{R}} H(s,x)^2 N(ds,dx)\n$$\n\nNext, we derive the predictable quadratic variation, $\\langle M \\rangle_t$. By definition, $\\langle M \\rangle_t$ is the unique predictable process such that $[M]_t - \\langle M \\rangle_t$ is a local martingale. In the context of Poisson random measures, this is known as the compensator of the process $[M]_t$.\nLet us define a candidate process\n$$\nA_t \\coloneqq \\int_{0}^{t}\\int_{\\mathbb{R}} H(s,x)^2 ds\\,\\nu(dx)\n$$\nThe integrand $H(s,x)^2 = (s^{\\alpha}x)^2 = s^{2\\alpha}x^2$ is a deterministic function, and therefore it is a predictable process. Since the integral of a predictable process with respect to a deterministic measure ($ds\\,\\nu(dx)$) is predictable, $A_t$ is a predictable process.\n\nNow we check if $[M]_t - A_t$ is a local martingale.\n$$\n[M]_t - A_t = \\int_{0}^{t}\\int_{\\mathbb{R}} H(s,x)^2 N(ds,dx) - \\int_{0}^{t}\\int_{\\mathbb{R}} H(s,x)^2 ds\\,\\nu(dx)\n$$\nBy linearity of the integral, we can combine the terms:\n$$\n[M]_t - A_t = \\int_{0}^{t}\\int_{\\mathbb{R}} H(s,x)^2 \\left( N(ds,dx) - ds\\,\\nu(dx) \\right)\n$$\nUsing the definition of the compensated Poisson random measure, $\\tilde{N}(ds,dx) \\coloneqq N(ds,dx) - ds\\,\\nu(dx)$, we have\n$$\n[M]_t - A_t = \\int_{0}^{t}\\int_{\\mathbb{R}} H(s,x)^2 \\tilde{N}(ds,dx)\n$$\nAn integral with respect to a compensated Poisson random measure is, by construction, a local martingale (and a true martingale under certain integrability conditions). Therefore, the process $[M]_t - A_t$ is a local martingale.\nBy the uniqueness of the compensator, we have identified the predictable quadratic variation:\n$$\n\\langle M \\rangle_t = A_t = \\int_{0}^{t}\\int_{\\mathbb{R}} H(s,x)^2 ds\\,\\nu(dx)\n$$\nWe can now calculate its value at time $t=T$:\n$$\n\\langle M \\rangle_T = \\int_{0}^{T}\\int_{\\mathbb{R}} \\left(s^{\\alpha}x\\right)^2 ds\\,\\nu(dx) = \\int_{0}^{T}\\int_{\\mathbb{R}} s^{2\\alpha}x^2 ds\\,(\\kappa \\exp(-|x|)dx)\n$$\nUsing Fubini's theorem, we can separate the integrals:\n$$\n\\langle M \\rangle_T = \\kappa \\left( \\int_{0}^{T} s^{2\\alpha} ds \\right) \\left( \\int_{\\mathbb{R}} x^2 \\exp(-|x|) dx \\right)\n$$\nWe evaluate each integral separately. For the time integral, the condition $\\alpha > -1/2$ implies $2\\alpha > -1$, so $2\\alpha+1 > 0$.\n$$\n\\int_{0}^{T} s^{2\\alpha} ds = \\left[ \\frac{s^{2\\alpha+1}}{2\\alpha+1} \\right]_{0}^{T} = \\frac{T^{2\\alpha+1}}{2\\alpha+1}\n$$\nFor the space integral, the integrand $x^2 \\exp(-|x|)$ is an even function, so we can write\n$$\n\\int_{\\mathbb{R}} x^2 \\exp(-|x|) dx = 2 \\int_{0}^{\\infty} x^2 \\exp(-x) dx\n$$\nThis integral is the Gamma function $\\Gamma(z) = \\int_0^\\infty t^{z-1}\\exp(-t)dt$ with $z=3$.\n$$\n2 \\int_{0}^{\\infty} x^2 \\exp(-x) dx = 2 \\Gamma(3) = 2 \\times 2! = 4\n$$\nCombining these results, we find the predictable quadratic variation:\n$$\n\\langle M \\rangle_T = \\kappa \\left( \\frac{T^{2\\alpha+1}}{2\\alpha+1} \\right) (4) = \\frac{4\\kappa T^{2\\alpha+1}}{2\\alpha+1}\n$$\nNow, we compute the expectation of the optional quadratic variation, $\\mathbb{E}\\big([M]_{T}\\big)$. We start from its integral representation:\n$$\n[M]_T = \\int_{0}^{T}\\int_{\\mathbb{R}} H(s,x)^2 N(ds,dx)\n$$\nTaking the expectation, we get\n$$\n\\mathbb{E}\\big([M]_T\\big) = \\mathbb{E}\\left[ \\int_{0}^{T}\\int_{\\mathbb{R}} H(s,x)^2 N(ds,dx) \\right]\n$$\nA fundamental property of Poisson random measures (often called Campbell's theorem) states that for any non-negative measurable function $f(s,x)$,\n$$\n\\mathbb{E}\\left[ \\int_{A}\\int_{B} f(s,x) N(ds,dx) \\right] = \\int_{A}\\int_{B} f(s,x) ds\\,\\nu(dx)\n$$\nIn our case, the function is $f(s,x) = H(s,x)^2 = s^{2\\alpha}x^2$, which is non-negative and measurable. Applying this theorem:\n$$\n\\mathbb{E}\\big([M]_T\\big) = \\int_{0}^{T}\\int_{\\mathbb{R}} H(s,x)^2 ds\\,\\nu(dx)\n$$\nThis is precisely the same integral we computed for $\\langle M \\rangle_T$. Therefore,\n$$\n\\mathbb{E}\\big([M]_T\\big) = \\frac{4\\kappa T^{2\\alpha+1}}{2\\alpha+1}\n$$\nWe have thus shown from first principles that $\\langle M \\rangle_T = \\mathbb{E}\\big([M]_T\\big)$. This equality is a general result: since $[M]_t - \\langle M \\rangle_t$ is a martingale starting at $0$, its expectation is always $0$. Thus, $\\mathbb{E}\\big([M]_t - \\langle M \\rangle_t\\big) = 0$, which implies $\\mathbb{E}\\big([M]_t\\big) = \\mathbb{E}\\big(\\langle M \\rangle_t\\big)$. Because $\\langle M \\rangle_t$ is a deterministic process in this problem, it is equal to its own expectation, leading to $\\mathbb{E}\\big([M]_t\\big) = \\langle M \\rangle_t$.\n\nThe common value for $\\langle M \\rangle_T$ and $\\mathbb{E}\\big([M]_T\\big)$ is\n$$\n\\frac{4\\kappa T^{2\\alpha+1}}{2\\alpha+1}\n$$", "answer": "$$\n\\boxed{\\frac{4\\kappa T^{2\\alpha+1}}{2\\alpha+1}}\n$$", "id": "2990781"}]}