{"hands_on_practices": [{"introduction": "Our first exercise provides a foundational calculation for understanding the stochastic heat equation. We will apply the Itô isometry, a cornerstone of stochastic calculus, to compute the second moment—or variance—of the solution to the SHE with additive space-time white noise on an unbounded domain. This practice is essential as it translates the abstract definition of the mild solution into a concrete, quantifiable property, giving us our first insight into the magnitude and growth of the random fluctuations. [@problem_id:3003014]", "problem": "Let $G(t,x)$ denote the one-dimensional heat kernel on $\\mathbb{R}$, defined for $t>0$ by $G(t,x)=(4\\pi t)^{-1/2}\\exp\\!\\big(-x^{2}/(4t)\\big)$. Let $W$ be a space-time white noise on $\\mathbb{R}_{+}\\times\\mathbb{R}$, modeled as an isonormal Gaussian family over $L^{2}(\\mathbb{R}_{+}\\times\\mathbb{R})$ so that, for deterministic $\\varphi\\in L^{2}(\\mathbb{R}_{+}\\times\\mathbb{R})$, the Walsh integral $\\int_{0}^{\\infty}\\int_{\\mathbb{R}}\\varphi(s,y)\\,W(\\mathrm{d}s,\\mathrm{d}y)$ is a centered Gaussian random variable with variance determined by the $L^{2}$ norm of $\\varphi$. Consider the additive-noise stochastic heat equation on $\\mathbb{R}$ with zero initial condition,\n$$\n\\partial_{t}u(t,x)=\\partial_{xx}u(t,x)+\\sigma\\,\\dot{W}(t,x),\\qquad u(0,x)=0,\n$$\nwhere $\\sigma\\in\\mathbb{R}$ is a constant and $\\dot{W}$ is the formal derivative of $W$. The mild solution is given by the stochastic convolution\n$$\nu(t,x)=\\sigma\\int_{0}^{t}\\int_{\\mathbb{R}}G(t-s,x-y)\\,W(\\mathrm{d}s,\\mathrm{d}y).\n$$\nStarting from the definition of the Walsh integral as an isometry on $L^{2}(\\mathbb{R}_{+}\\times\\mathbb{R})$ and the semigroup property of the heat kernel, derive the Itô isometry for the stochastic convolution with the heat kernel in this setting, and use it to compute the closed-form expression of $\\mathbb{E}\\big[|u(t,x)|^{2}\\big]$ as a function of $t$ and $\\sigma$. Provide the exact expression; no rounding is required. Express your final answer as a single analytic expression.", "solution": "The problem asks for the derivation of the second moment of the mild solution to the one-dimensional stochastic heat equation with additive space-time white noise and zero initial conditions. The mild solution $u(t,x)$ for $t>0$ and $x\\in\\mathbb{R}$ is given by the stochastic convolution:\n$$\nu(t,x) = \\sigma \\int_{0}^{t}\\int_{\\mathbb{R}} G(t-s, x-y) \\,W(\\mathrm{d}s,\\mathrm{d}y)\n$$\nHere, $G(t,x)=(4\\pi t)^{-1/2}\\exp(-x^{2}/(4t))$ is the one-dimensional heat kernel, $\\sigma \\in \\mathbb{R}$ is a constant, and $W$ is a space-time white noise on $\\mathbb{R}_{+} \\times \\mathbb{R}$. The integral is a Walsh-Itô stochastic integral.\n\nThe Walsh integral $\\int_{0}^{\\infty}\\int_{\\mathbb{R}}\\varphi(s,y)\\,W(\\mathrm{d}s,\\mathrm{d}y)$ for a deterministic function $\\varphi \\in L^{2}(\\mathbb{R}_{+}\\times\\mathbb{R})$ is a centered Gaussian random variable. Its variance is given by the Itô isometry:\n$$\n\\mathbb{E}\\left[\\left(\\int_{0}^{\\infty}\\int_{\\mathbb{R}}\\varphi(s,y)\\,W(\\mathrm{d}s,\\mathrm{d}y)\\right)^2\\right] = \\lVert\\varphi\\rVert_{L^{2}(\\mathbb{R}_{+}\\times\\mathbb{R})}^{2} = \\int_{0}^{\\infty}\\int_{\\mathbb{R}} |\\varphi(s,y)|^2 \\,\\mathrm{d}y\\,\\mathrm{d}s\n$$\nThe solution $u(t,x)$ is a specific instance of such an integral. We can identify the integrand as a function $\\varphi_{t,x}(s,y)$ defined by:\n$$\n\\varphi_{t,x}(s,y) = \\sigma \\, G(t-s, x-y) \\, \\mathbf{1}_{[0,t)}(s)\n$$\nwhere $\\mathbf{1}_{[0,t)}(s)$ is the indicator function for the time interval $[0,t)$. Since the integral defining $u(t,x)$ is a centered Gaussian random variable, its second moment $\\mathbb{E}[|u(t,x)|^{2}]$ is equal to its variance. We can compute this variance by applying the Itô isometry. This constitutes the application of the Itô isometry for the stochastic convolution with the heat kernel.\n\n$$\n\\mathbb{E}\\big[|u(t,x)|^{2}\\big] = \\mathbb{E}\\big[u(t,x)^2\\big] = \\lVert\\varphi_{t,x}\\rVert_{L^{2}(\\mathbb{R}_{+}\\times\\mathbb{R})}^{2}\n$$\nSubstituting the expression for $\\varphi_{t,x}(s,y)$:\n$$\n\\mathbb{E}\\big[|u(t,x)|^{2}\\big] = \\int_{0}^{\\infty}\\int_{\\mathbb{R}} \\left| \\sigma \\, G(t-s, x-y) \\, \\mathbf{1}_{[0,t)}(s) \\right|^2 \\,\\mathrm{d}y\\,\\mathrm{d}s\n$$\nWe can simplify this expression. The constant $\\sigma$ can be factored out, and the indicator function restricts the time integral to the interval $[0,t)$.\n$$\n\\mathbb{E}\\big[|u(t,x)|^{2}\\big] = \\sigma^2 \\int_{0}^{t}\\int_{\\mathbb{R}} G(t-s, x-y)^2 \\,\\mathrm{d}y\\,\\mathrm{d}s\n$$\nTo evaluate this double integral, we first focus on the inner integral with respect to $y$:\n$$\nI_{inner}(s) = \\int_{\\mathbb{R}} G(t-s, x-y)^2 \\,\\mathrm{d}y\n$$\nWe perform a change of variables $z = x-y$, so that $\\mathrm{d}z = -\\mathrm{d}y$. The limits of integration for $z$ are from $+\\infty$ to $-\\infty$, and the negative sign from the differential cancels the reversal of limits.\n$$\nI_{inner}(s) = \\int_{\\mathbb{R}} G(t-s, z)^2 \\,\\mathrm{d}z\n$$\nThe heat kernel is an even function of its spatial argument, i.e., $G(\\tau, z) = G(\\tau, -z)$. Therefore, we can write $G(t-s, z)^2 = G(t-s, 0-z)G(t-s, z-0)$. The integral becomes:\n$$\nI_{inner}(s) = \\int_{\\mathbb{R}} G(t-s, 0-z)G(t-s, z-0) \\,\\mathrm{d}z\n$$\nThis expression has the form of a convolution. We use the semigroup property of the heat kernel, which states that for $t_1, t_2 > 0$:\n$$\n\\int_{\\mathbb{R}} G(t_1, x-z) G(t_2, z-y) \\,\\mathrm{d}z = G(t_1+t_2, x-y)\n$$\nApplying this property with $t_1 = t_2 = t-s$ and $x=y=0$, we find:\n$$\nI_{inner}(s) = G((t-s)+(t-s), 0-0) = G(2(t-s), 0)\n$$\nNow we use the definition of the heat kernel $G(\\tau, \\xi) = (4\\pi \\tau)^{-1/2}\\exp(-\\xi^2/(4\\tau))$ with $\\tau = 2(t-s)$ and $\\xi=0$:\n$$\nI_{inner}(s) = G(2(t-s), 0) = \\left(4\\pi \\cdot 2(t-s)\\right)^{-1/2} \\exp(0) = (8\\pi(t-s))^{-1/2}\n$$\nThis result for the inner integral is independent of $x$, as expected from the spatial translation invariance of the setup. Now, we substitute this back into the expression for the variance:\n$$\n\\mathbb{E}\\big[|u(t,x)|^{2}\\big] = \\sigma^2 \\int_{0}^{t} (8\\pi(t-s))^{-1/2} \\,\\mathrm{d}s\n$$\nThis is a standard integral. We can pull the constant factors out:\n$$\n\\mathbb{E}\\big[|u(t,x)|^{2}\\big] = \\frac{\\sigma^2}{\\sqrt{8\\pi}} \\int_{0}^{t} (t-s)^{-1/2} \\,\\mathrm{d}s\n$$\nWe evaluate the integral using the substitution $v = t-s$, which gives $\\mathrm{d}v = -\\mathrm{d}s$. The limits of integration change from $s=0$ to $v=t$ and from $s=t$ to $v=0$.\n$$\n\\int_{0}^{t} (t-s)^{-1/2} \\,\\mathrm{d}s = \\int_{t}^{0} v^{-1/2} (-\\mathrm{d}v) = \\int_{0}^{t} v^{-1/2} \\,\\mathrm{d}v\n$$\nThis is an improper integral, but it converges.\n$$\n\\int_{0}^{t} v^{-1/2} \\,\\mathrm{d}v = \\left[ 2v^{1/2} \\right]_{0}^{t} = 2\\sqrt{t} - 0 = 2\\sqrt{t}\n$$\nFinally, we assemble all the parts:\n$$\n\\mathbb{E}\\big[|u(t,x)|^{2}\\big] = \\frac{\\sigma^2}{\\sqrt{8\\pi}} \\cdot 2\\sqrt{t} = \\frac{2\\sigma^2\\sqrt{t}}{2\\sqrt{2\\pi}} = \\sigma^2 \\sqrt{\\frac{t}{2\\pi}}\n$$\nThis is the closed-form expression for the second moment of the solution at a fixed point $(t,x)$.", "answer": "$$\\boxed{\\sigma^{2}\\sqrt{\\frac{t}{2\\pi}}}$$", "id": "3003014"}, {"introduction": "Having explored the solution's variance, we now turn to a more qualitative question regarding the impact of boundary conditions. This exercise analyzes the stochastic heat equation on a bounded domain, contrasting the behavior under different boundary conditions through a spectral decomposition. You will discover how the existence of a zero eigenvalue for the Neumann Laplacian leads to a non-decaying, random-walk behavior for the solution's spatial average, a crucial feature with no counterpart in the Dirichlet case. [@problem_id:3003016]", "problem": "Consider the stochastic partial differential equation (SPDE) for the heat equation with homogeneous Neumann boundary conditions on a bounded, connected, smooth domain $\\Omega \\subset \\mathbb{R}^d$:\n$$\n\\partial_t u(t,x) \\;=\\; \\Delta u(t,x) \\;+\\; \\xi(t,x), \\qquad x \\in \\Omega, \\; t \\ge 0,\n$$\nwith the Neumann boundary condition\n$$\n\\partial_{\\mathbf{n}} u(t,x) \\;=\\; 0, \\qquad x \\in \\partial \\Omega, \\; t \\ge 0,\n$$\nand the initial condition $u(0,x) = u_0(x) \\in L^2(\\Omega)$. Here $\\partial_{\\mathbf{n}}$ denotes the outward normal derivative, and $\\xi$ is an additive, centered Gaussian noise that is white in time with spatial covariance operator $Q$ acting on $L^2(\\Omega)$. Assume $Q$ is self-adjoint, nonnegative, and trace class, and that $\\xi$ can be realized as a $Q$-Wiener process in $L^2(\\Omega)$ (that is, a Gaussian process with covariance $Q$ and independent increments in time).\n\nLet $\\{\\phi_k\\}_{k \\ge 0}$ be an orthonormal basis of $L^2(\\Omega)$ consisting of eigenfunctions of the Neumann Laplacian, with corresponding eigenvalues $\\{\\lambda_k\\}_{k \\ge 0}$, so that\n$$\n-\\Delta \\phi_k \\;=\\; \\lambda_k \\, \\phi_k, \\qquad \\partial_{\\mathbf{n}} \\phi_k \\;=\\; 0 \\text{ on } \\partial \\Omega,\n$$\nwhere $\\lambda_0 = 0$ and $\\phi_0$ is the constant eigenfunction normalized by $\\|\\phi_0\\|_{L^2(\\Omega)} = 1$, and $\\lambda_k > 0$ for all $k \\ge 1$. Denote the spatial average of $u$ by\n$$\n\\bar{u}(t) \\;=\\; \\frac{1}{|\\Omega|} \\int_{\\Omega} u(t,x) \\, dx,\n$$\nand note that $\\bar{u}(t) = \\langle u(t,\\cdot), \\phi_0 \\rangle_{L^2(\\Omega)}$ because $\\phi_0(x) = |\\Omega|^{-1/2}$ is constant.\n\nUsing only the eigenfunction expansion theory for the Neumann Laplacian, the definition of the $Q$-Wiener process, and Itô’s calculus for stochastic integrals in Hilbert spaces, analyze the dynamics of the zero mode coefficient $a_0(t) = \\langle u(t,\\cdot), \\phi_0 \\rangle_{L^2(\\Omega)}$, and the behavior of the spatial average $\\bar{u}(t)$, under additive noise. In particular, address whether the average can be non-decaying.\n\nWhich of the following statements are correct? Select all that apply.\n\nA. The zero mode coefficient $a_0(t)$ satisfies a stochastic differential equation of the form $d a_0(t) = \\sigma_0 \\, d \\beta(t)$, where $\\beta$ is a scalar Brownian motion arising from projecting the noise onto $\\phi_0$ and $\\sigma_0 \\ge 0$ depends on $Q$. Consequently, $\\bar{u}(t)$ is a martingale with variance growing linearly in $t$, and it does not decay.\n\nB. The Neumann boundary condition enforces exponential decay of the zero mode, so $a_0(t)$ decays to $0$ in expectation even in the presence of additive noise.\n\nC. If the covariance operator annihilates constants, i.e., $Q \\phi_0 = 0$, then the zero mode is unaffected by the noise and $\\bar{u}(t)$ remains equal to its initial value for all $t \\ge 0$.\n\nD. For any additive noise white in time, the spatial average $\\bar{u}(t)$ obeys a deterministic ordinary differential equation with exponential decay governed by the first positive eigenvalue of the Neumann Laplacian, hence $\\bar{u}(t)$ decays to $0$.\n\nE. The zero mode under additive noise behaves as an Ornstein–Uhlenbeck process with linear drift $- \\lambda_0 a_0(t)$ and diffusion coefficient determined by $Q$, so it has a stationary Gaussian distribution as $t \\to \\infty$.", "solution": "To analyze the dynamics of the spatial average, we employ the spectral method. We expand the solution $u(t,x)$ in the orthonormal basis of eigenfunctions $\\{\\phi_k\\}_{k \\ge 0}$ of the Neumann Laplacian:\n$$ u(t,x) = \\sum_{k=0}^{\\infty} a_k(t) \\phi_k(x), \\quad \\text{where } a_k(t) = \\langle u(t,\\cdot), \\phi_k \\rangle_{L^2(\\Omega)} $$\nWe also expand the $Q$-Wiener process $W(t)$ whose formal time derivative is the noise $\\xi$:\n$$ W(t,x) = \\sum_{k=0}^{\\infty} \\beta_k^Q(t) \\phi_k(x) $$\nwhere $\\beta_k^Q(t)$ are real-valued Brownian motions with covariance $\\mathbb{E}[\\beta_j^Q(t) \\beta_k^Q(s)] = \\min(t,s) \\langle Q\\phi_j, \\phi_k \\rangle$.\n\nSubstituting the expansion of $u$ into the SPDE, $\\partial_t u = \\Delta u + \\xi$, and taking the inner product with an eigenfunction $\\phi_k$, we obtain a system of stochastic differential equations (SDEs) for the coefficients $a_k(t)$. Using the self-adjointness of $\\Delta$ and the eigenfunction property $-\\Delta\\phi_k = \\lambda_k \\phi_k$, the SDE for the $k$-th mode coefficient is:\n$$ d a_k(t) = -\\lambda_k a_k(t) dt + d\\beta_k^Q(t) $$\nThe problem asks us to analyze the zero mode, $a_0(t)$, which corresponds to the eigenvalue $\\lambda_0 = 0$. Substituting $k=0$ into the general SDE gives:\n$$ d a_0(t) = -(0) a_0(t) dt + d\\beta_0^Q(t) \\implies d a_0(t) = d\\beta_0^Q(t) $$\nHere, $\\beta_0^Q(t)$ is a real-valued Brownian motion. We can write $d\\beta_0^Q(t) = \\sigma_0 d\\beta(t)$, where $\\beta(t)$ is a standard one-dimensional Brownian motion and the diffusion coefficient $\\sigma_0 = \\sqrt{\\langle Q\\phi_0, \\phi_0 \\rangle} \\ge 0$ depends on the projection of the noise covariance onto the constant eigenfunction $\\phi_0$.\n\nThe solution is $a_0(t) = a_0(0) + \\sigma_0 \\beta(t)$. This process is a martingale. Its variance is $\\text{Var}(a_0(t)) = \\sigma_0^2 t$, which grows linearly with time (unless $\\sigma_0 = 0$). This process does not decay; instead, it performs a random walk. The spatial average $\\bar{u}(t)$ is proportional to $a_0(t)$ via $\\bar{u}(t) = a_0(t)/\\sqrt{|\\Omega|}$, so its qualitative behavior is identical.\n\nWith this derivation, we can evaluate the options:\n\n**A. Correct.** As shown, $a_0(t)$ follows the SDE $d a_0(t) = \\sigma_0 d\\beta(t)$. It is a martingale, its variance grows linearly in time, and it does not decay.\n\n**B. Incorrect.** The Neumann boundary condition is precisely what leads to the zero eigenvalue $\\lambda_0=0$, which eliminates the decaying drift term for the zero mode. The expectation of $a_0(t)$ is constant, not decaying.\n\n**C. Correct.** If $Q\\phi_0 = 0$, then the diffusion coefficient $\\sigma_0 = \\sqrt{\\langle Q\\phi_0, \\phi_0 \\rangle} = 0$. The SDE becomes $da_0(t) = 0$, meaning $a_0(t)$ is constant. Consequently, $\\bar{u}(t)$ is also constant. This physically corresponds to the noise having no net-zero spatial component.\n\n**D. Incorrect.** The dynamics of $\\bar{u}(t)$ are generally stochastic, not deterministic, and are governed by the zero eigenvalue $\\lambda_0=0$, not the first positive eigenvalue $\\lambda_1$.\n\n**E. Incorrect.** An Ornstein-Uhlenbeck process is characterized by a mean-reverting drift ($\\lambda_k > 0$). The zero mode, with $\\lambda_0=0$, is a degenerate case that behaves as a Brownian motion, not a mean-reverting process. It does not have a stationary distribution.", "answer": "$$\\boxed{AC}$$", "id": "3003016"}, {"introduction": "Our final practice bridges the gap between theoretical analysis and computational implementation, a crucial skill for any applied mathematician. We will explore the spectral Galerkin method, a powerful technique for numerically approximating the solution to the stochastic heat equation on a bounded domain. This exercise challenges you not only to derive the theoretical convergence rate of the approximation but also to implement the scheme and computationally verify your analytical findings, providing a concrete demonstration of how noise regularity impacts numerical performance. [@problem_id:3003074]", "problem": "Consider the stochastic heat equation on a bounded domain given by the interval $D = (0,1)$ with homogeneous Dirichlet boundary conditions. Let $H = L^2(D)$, and let $A$ denote the Dirichlet Laplacian on $H$ defined by $A = \\frac{d^2}{dx^2}$ with domain $D(A) = H^2(D) \\cap H_0^1(D)$. The operator $A$ is self-adjoint, negative definite, and has a complete orthonormal basis of eigenfunctions $\\{e_k\\}_{k \\ge 1}$ with corresponding eigenvalues $\\{\\lambda_k\\}_{k \\ge 1}$ satisfying $A e_k = -\\lambda_k e_k$, where $\\lambda_k = \\pi^2 k^2$ and $e_k(x) = \\sqrt{2} \\sin(\\pi k x)$ for $x \\in D$.\n\nConsider the $Q$-Wiener process $W^Q(t)$ on $H$, where $Q : H \\to H$ is a symmetric, nonnegative, trace-class covariance operator, diagonal in the same basis $\\{e_k\\}$: $Q e_k = q_k e_k$. Assume $q_k = k^{-2\\beta}$ for a fixed parameter $\\beta > \\tfrac{1}{2}$, so that $\\mathrm{Tr}(Q) = \\sum_{k=1}^{\\infty} q_k < \\infty$.\n\nWe study the linear stochastic partial differential equation (SPDE)\n$$\ndX(t) = A X(t) \\, dt + dW^Q(t), \\quad X(0) = 0,\n$$\nunderstood in the mild sense on $H$. The spectral Galerkin approximation of order $N$ is the projection onto the first $N$ eigenfunctions,\n$$\nX^{(N)}(t) = \\sum_{k=1}^{N} X_k(t) \\, e_k,\n$$\nwhere $X_k(t) = \\langle X(t), e_k \\rangle_H$ are the spectral coefficients.\n\nYour tasks are:\n\n1) Starting from the definitions of the analytic semigroup generated by $A$ and the mild solution of the linear SPDE, derive the scalar stochastic differential equation solved by each spectral coefficient $X_k(t)$ and prove that each $X_k(t)$ is an Ornstein–Uhlenbeck (OU) process. Then derive the exact variance $\\mathbb{V}\\mathrm{ar}[X_k(t)]$ in terms of $q_k$, $\\lambda_k$, and $t$. Use this to express the mean-square error in the $H$-norm between the full solution and the spectral Galerkin approximation as\n$$\n\\mathbb{E}\\big[\\|X(t) - X^{(N)}(t)\\|_{H}^2\\big] = \\sum_{k=N+1}^{\\infty} \\mathbb{V}\\mathrm{ar}[X_k(t)].\n$$\n\n2) Using the explicit eigenvalues $\\lambda_k = \\pi^2 k^2$ and the assumption $q_k = k^{-2\\beta}$ with $\\beta > \\tfrac{1}{2}$, derive the asymptotic convergence rate in mean-square of the spectral Galerkin method as $N \\to \\infty$. Your derivation must show an explicit exponent $\\gamma(\\beta)$ such that\n$$\n\\mathbb{E}\\big[\\|X(t) - X^{(N)}(t)\\|_{H}^2\\big] \\sim C(t,\\beta) \\, N^{-\\gamma(\\beta)} \\quad \\text{as } N \\to \\infty,\n$$\nfor a finite constant $C(t,\\beta)$ depending on $t$ and $\\beta$, and you must determine $\\gamma(\\beta)$ from first principles. You must not assume or quote the convergence rate; you must derive it explicitly from the variance formula and the eigenvalue asymptotics.\n\n3) Implement a spectral Galerkin discretization routine that computes the mean-square error $\\mathbb{E}[\\|X(t) - X^{(N)}(t)\\|_{H}^2]$ exactly as an infinite series, accelerated by an analytically justified tail approximation. Specifically, compute\n$$\n\\mathbb{E}\\big[\\|X(t) - X^{(N)}(t)\\|_{H}^2\\big] = \\sum_{k=N+1}^{K} \\frac{q_k}{2 \\lambda_k} \\big(1 - e^{-2 \\lambda_k t}\\big) + R_{K}(N,\\beta,t),\n$$\nwhere $K$ is chosen adaptively large so that $e^{-2 \\lambda_k t}$ is negligible for $k > K$, and $R_{K}(N,\\beta,t)$ is a rigorously derived tail remainder expressed in terms of the Hurwitz zeta function $\\zeta(s,a)$ with exponent $s > 1$.\n\n4) Using your implementation, estimate the empirical convergence rate by performing a linear regression of $\\log\\big(\\mathbb{E}[\\|X(t) - X^{(N)}(t)\\|_{H}^2]\\big)$ against $\\log(N)$ for several values of $N$, and compare the estimated slope to the theoretically derived exponent $\\gamma(\\beta)$. Report the absolute difference between the empirical slope magnitude and $\\gamma(\\beta)$.\n\n5) For a single test instance, perform a Monte Carlo validation of the mean-square error by simulating the tail coefficients $\\{X_k(t)\\}_{k>N}$ as independent Gaussian random variables with the exact variances for a finite set of modes and samples, and then add the analytic tail remainder to approximate the infinite sum. Report the absolute difference between the Monte Carlo estimate and your exact series-based value.\n\nImplementation details:\n\n- Domain: $D = (0,1)$; boundary conditions: homogeneous Dirichlet.\n- Eigenpairs: $\\lambda_k = \\pi^2 k^2$, $e_k(x) = \\sqrt{2} \\sin(\\pi k x)$.\n- Noise spectrum: $q_k = k^{-2\\beta}$ with $\\beta > \\tfrac{1}{2}$.\n- Time: fixed $t > 0$.\n- Tail remainder: use the Hurwitz zeta function $\\zeta(s,a)$ for $s = 2\\beta + 2$ to evaluate $\\sum_{k=K+1}^{\\infty} k^{-s}$, multiplied by the appropriate constant deduced in your derivation.\n- Monte Carlo: use a fixed pseudorandom seed for reproducibility.\n\nTest suite:\n\n- Case A (boundary case): Compute $\\mathbb{E}[\\|X(t) - X^{(N)}(t)\\|_{H}^2]$ for $N = 0$, $\\beta = 1.0$, $t = 0.5$. Return a single float.\n- Case B (rate check, moderate smoothness): For $\\beta = 0.6$, $t = 0.5$, $N \\in \\{4, 8, 16, 32\\}$, estimate the empirical slope and return the absolute difference to the theoretical exponent $\\gamma(\\beta)$ as a single float.\n- Case C (rate check, smoother noise): For $\\beta = 1.0$, $t = 0.5$, $N \\in \\{8, 16, 32, 64\\}$, estimate the empirical slope and return the absolute difference to $\\gamma(\\beta)$ as a single float.\n- Case D (rate check, very smooth noise): For $\\beta = 2.0$, $t = 0.5$, $N \\in \\{8, 16, 24, 32\\}$, estimate the empirical slope and return the absolute difference to $\\gamma(\\beta)$ as a single float.\n- Case E (Monte Carlo validation): For $\\beta = 0.8$, $t = 0.5$, $N = 16$, simulate $M = 5000$ samples of the tail coefficients for $K_{\\mathrm{sim}} = 800$ modes using a fixed seed, add the analytic remainder, and return the absolute difference between the Monte Carlo estimate and your exact series-based value as a single float.\n\nFinal output format:\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The list must contain exactly five floats corresponding to Cases A through E, in order, for example, \"[result_A,result_B,result_C,result_D,result_E]\". No physical units are involved in this problem; all outputs are dimensionless real numbers.", "solution": "This problem requires deriving the convergence rate of the spectral Galerkin method for a stochastic heat equation and then implementing a numerical verification.\n\n**1. Derivation of the SDE and Variance**\n\nWe project the SPDE $dX(t) = AX(t)dt + dW^Q(t)$ onto the eigenbasis $\\{e_k\\}$ of the operator $A$. The $k$-th spectral coefficient, $X_k(t) = \\langle X(t), e_k \\rangle_H$, evolves according to:\n$$ dX_k(t) = \\langle AX(t), e_k \\rangle dt + \\langle dW^Q(t), e_k \\rangle. $$\nUsing the self-adjointness of $A$ and the eigen-property $Ae_k = -\\lambda_k e_k$, we get $\\langle AX, e_k \\rangle = \\langle X, Ae_k \\rangle = \\langle X, -\\lambda_k e_k \\rangle = -\\lambda_k X_k(t)$. The noise term is $\\langle dW^Q(t), e_k \\rangle = \\sqrt{q_k} d\\beta_k(t)$, where $\\{\\beta_k\\}$ are independent standard Brownian motions. Thus, each coefficient follows a scalar SDE:\n$$ dX_k(t) = -\\lambda_k X_k(t) dt + \\sqrt{q_k} d\\beta_k(t), \\quad X_k(0) = 0. $$\nThis is the equation for an Ornstein–Uhlenbeck (OU) process. Since $X_k(0)=0$ and the noise has zero mean, $\\mathbb{E}[X_k(t)] = 0$. The variance can be calculated from the integral form of the solution using Itô's isometry:\n$$ \\mathbb{V}\\mathrm{ar}[X_k(t)] = \\mathbb{E}[X_k(t)^2] = \\int_0^t \\left(e^{-\\lambda_k(t-s)} \\sqrt{q_k}\\right)^2 ds = q_k \\int_0^t e^{-2\\lambda_k(t-s)} ds. $$\nEvaluating the integral gives the exact variance:\n$$ \\mathbb{V}\\mathrm{ar}[X_k(t)] = \\frac{q_k}{2\\lambda_k} \\left(1 - e^{-2\\lambda_k t}\\right). $$\nThe mean-square error of the spectral Galerkin approximation is the sum of the variances of the truncated modes, due to the orthogonality of the basis and independence of the processes:\n$$ \\mathbb{E}\\big[\\|X(t) - X^{(N)}(t)\\|_{H}^2\\big] = \\sum_{k=N+1}^{\\infty} \\mathbb{E}[X_k(t)^2] = \\sum_{k=N+1}^{\\infty} \\mathbb{V}\\mathrm{ar}[X_k(t)]. $$\n\n**2. Asymptotic Convergence Rate**\n\nWe substitute the given forms $q_k = k^{-2\\beta}$ and $\\lambda_k = \\pi^2 k^2$ into the error series:\n$$ \\mathcal{E}(N, t, \\beta) = \\sum_{k=N+1}^{\\infty} \\frac{k^{-2\\beta}}{2\\pi^2 k^2} \\left(1 - e^{-2\\pi^2 k^2 t}\\right) = \\frac{1}{2\\pi^2} \\sum_{k=N+1}^{\\infty} k^{-(2\\beta+2)} \\left(1 - e^{-2\\pi^2 k^2 t}\\right). $$\nFor large $N$, the terms in the sum are for large $k$. The exponential term $(1 - e^{-2\\pi^2 k^2 t})$ rapidly approaches 1. The asymptotic behavior of the series is thus governed by the power law. We can approximate the tail sum with an integral:\n$$ \\mathcal{E}(N, t, \\beta) \\sim \\frac{1}{2\\pi^2} \\int_N^\\infty x^{-(2\\beta+2)} dx. $$\nSince $\\beta > 1/2$, the exponent $p=2\\beta+2 > 3$, so the integral converges.\n$$ \\int_N^\\infty x^{-(2\\beta+2)} dx = \\left[\\frac{x^{-(2\\beta+1)}}{-(2\\beta+1)}\\right]_N^\\infty = \\frac{N^{-(2\\beta+1)}}{2\\beta+1}. $$\nTherefore, the mean-square error has the asymptotic convergence:\n$$ \\mathbb{E}\\big[\\|X(t) - X^{(N)}(t)\\|_{H}^2\\big] \\sim \\frac{1}{2\\pi^2(2\\beta+1)} N^{-(2\\beta+1)} \\quad \\text{as } N \\to \\infty. $$\nThe convergence rate is determined by the exponent $\\gamma(\\beta) = 2\\beta+1$.\n\n**3. Numerical Implementation**\n\nThe numerical solution involves implementing a program based on these derivations.\n- The error is computed by summing the exact variance formula up to a large cutoff $K$ and approximating the remaining tail $\\sum_{k=K+1}^{\\infty} \\frac{q_k}{2\\lambda_k}$ using the Hurwitz zeta function: $\\frac{1}{2\\pi^2} \\zeta(2\\beta+2, K+1)$.\n- The empirical convergence rate is found by fitting a line to the log-log plot of the error versus $N$. The magnitude of the slope gives the empirical rate $\\gamma_{emp}$.\n- The Monte Carlo validation involves simulating many sample paths of the truncated modes $\\{X_k\\}_{k=N+1}^{K_{\\mathrm{sim}}}$ as independent Gaussian random variables with the correct variances, calculating the sample mean of the sum of their squares, and adding the analytically computed tail from $K_{\\mathrm{sim}}+1$ onwards.", "answer": "```python\nimport numpy as np\nfrom scipy.special import zeta\nfrom scipy.stats import linregress\n\ndef compute_total_error(N, beta, t, K_cutoff=20000):\n    \"\"\"\n    Computes the mean-square error E[||X(t) - X^N(t)||^2] using a \n    finite sum and a Hurwitz zeta function tail approximation.\n\n    The error is sum_{k=N+1 to inf} Var(X_k(t)).\n    Var(X_k(t)) = (q_k / (2*lambda_k)) * (1 - exp(-2*lambda_k*t))\n    q_k = k**(-2*beta)\n    lambda_k = pi**2 * k**2\n    \"\"\"\n    s = 2 * beta + 2\n    const = 1.0 / (2 * np.pi**2)\n    \n    # Finite sum part up to K_cutoff\n    k_vals = np.arange(N + 1, K_cutoff + 1)\n    if k_vals.size == 0:\n        finite_sum = 0.0\n    else:\n        lambda_k = np.pi**2 * k_vals**2\n        variances = (const * np.power(k_vals, -s) * \n                     (1.0 - np.exp(-2 * lambda_k * t)))\n        finite_sum = np.sum(variances)\n    \n    # Tail approximation part from K_cutoff+1 to infinity\n    # We approximate (1 - exp(-2*lambda_k*t)) as 1 for k > K_cutoff\n    tail_remainder = const * zeta(s, K_cutoff + 1)\n    \n    return finite_sum + tail_remainder\n\ndef estimate_rate_and_compare(Ns, beta, t):\n    \"\"\"\n    Estimates the convergence rate gamma and compares it to the theoretical value.\n    \"\"\"\n    log_Ns = np.log(Ns)\n    errors = [compute_total_error(n, beta, t) for n in Ns]\n    log_errors = np.log(errors)\n    \n    # Perform linear regression to find the slope\n    slope, _, _, _, _ = linregress(log_Ns, log_errors)\n    empirical_gamma = -slope\n    \n    theoretical_gamma = 2 * beta + 1\n    \n    return abs(empirical_gamma - theoretical_gamma)\n\ndef mc_validation(N, beta, t, M, K_sim, seed=42):\n    \"\"\"\n    Performs Monte Carlo validation of the error calculation.\n    \"\"\"\n    # Use a large K_cutoff for a more \"exact\" reference value\n    exact_value = compute_total_error(N, beta, t, K_cutoff=40000)\n\n    # The MC estimate is a hybrid: MC for [N+1, K_sim], analytic for [K_sim+1, inf]\n    \n    # 1. Analytic tail for the MC part (sum from K_sim + 1 to infinity)\n    analytic_tail_mc = compute_total_error(K_sim, beta, t, K_cutoff=40000)\n\n    # 2. MC simulation for modes [N+1, K_sim]\n    k_vals_mc = np.arange(N + 1, K_sim + 1)\n    s = 2 * beta + 2\n    const = 1.0 / (2 * np.pi**2)\n    \n    lambda_k_mc = np.pi**2 * k_vals_mc**2\n    variances_mc = (const * np.power(k_vals_mc, -s) *\n                    (1.0 - np.exp(-2 * lambda_k_mc * t)))\n    sigmas_mc = np.sqrt(variances_mc)\n    \n    rng = np.random.default_rng(seed)\n    # Generate M x num_modes standard normal samples\n    normal_samples = rng.normal(loc=0.0, scale=1.0, size=(M, len(k_vals_mc))) \n    \n    # Scale samples by standard deviations to get samples of X_k\n    # Broadcasting sigmas_mc (1, num_modes) over normal_samples (M, num_modes)\n    x_k_samples = normal_samples * sigmas_mc\n\n    # For each of the M paths, compute sum of squares over modes\n    sum_sq_per_path = np.sum(np.power(x_k_samples, 2), axis=1)\n\n    # Average the sum of squares over all M paths\n    mc_mean_sum_sq = np.mean(sum_sq_per_path)\n\n    # Total MC estimate is the mean of the simulated part plus the analytic tail\n    mc_estimate = mc_mean_sum_sq + analytic_tail_mc\n    \n    return abs(mc_estimate - exact_value)\n\ndef solve():\n    \"\"\"\n    Main solver function to execute all test cases.\n    \"\"\"\n    results = []\n\n    # Case A\n    # N=0 means we compute the total variance of the solution X(t)\n    result_A = compute_total_error(N=0, beta=1.0, t=0.5)\n    results.append(result_A)\n\n    # Case B\n    result_B = estimate_rate_and_compare(Ns=[4, 8, 16, 32], beta=0.6, t=0.5)\n    results.append(result_B)\n\n    # Case C\n    result_C = estimate_rate_and_compare(Ns=[8, 16, 32, 64], beta=1.0, t=0.5)\n    results.append(result_C)\n    \n    # Case D\n    result_D = estimate_rate_and_compare(Ns=[8, 16, 24, 32], beta=2.0, t=0.5)\n    results.append(result_D)\n\n    # Case E\n    # Note: Using a fixed seed for reproducibility. The problem does not specify one.\n    result_E = mc_validation(N=16, beta=0.8, t=0.5, M=5000, K_sim=800, seed=0)\n    results.append(result_E)\n\n    # Format the final output\n    print(f\"[{','.join(f'{r:.8f}' for r in results)}]\")\n\nsolve()\n```", "id": "3003074"}]}