{"hands_on_practices": [{"introduction": "The stochastic heat equation driven by space-time white noise is a cornerstone model for random fields. This first practice exercise [@problem_id:3005753] guides you through the fundamental task of computing the first two moments—the mean and variance—of its solution. By directly applying the defining properties of the Walsh stochastic integral, you will see how the Itô isometry translates properties of the deterministic heat kernel into statistical properties of the random solution, revealing a critical dependence on the spatial dimension.", "problem": "Let $\\{W(A): A \\subset \\mathbb{R}_{+} \\times \\mathbb{R}^{d}\\}$ be a centered Gaussian random measure on $\\mathbb{R}_{+} \\times \\mathbb{R}^{d}$, called space-time white noise, with covariance $\\mathbb{E}[W(A) W(B)] = \\mathrm{Leb}(A \\cap B)$ for all bounded Borel sets $A,B \\subset \\mathbb{R}_{+} \\times \\mathbb{R}^{d}$. Consider the linear stochastic heat equation on $\\mathbb{R}^{d}$ with additive noise\n$$\n\\partial_{t} u(t,x) = \\tfrac{1}{2} \\Delta u(t,x) + \\dot{W}(t,x), \\quad t > 0, \\; x \\in \\mathbb{R}^{d},\n$$\nwith deterministic initial condition $u(0,x) = u_{0}(x)$, where $u_{0} \\in L^{2}(\\mathbb{R}^{d})$ is bounded and measurable. Let $p_{t}(x)$ denote the standard heat kernel associated with the operator $\\tfrac{1}{2}\\Delta$, namely\n$$\np_{t}(x) = (2\\pi t)^{-d/2} \\exp\\!\\big(-|x|^{2}/(2t)\\big), \\quad t>0,\\; x\\in \\mathbb{R}^{d}.\n$$\nIn the sense of the random field formulation based on the stochastic integral of John B. Walsh, the mild solution is\n$$\nu(t,x) = (p_{t} * u_{0})(x) + \\int_{0}^{t} \\int_{\\mathbb{R}^{d}} p_{t-s}(x-y)\\, W(\\mathrm{d}s,\\mathrm{d}y),\n$$\nwhenever the Walsh stochastic integral is well-defined. Using only the defining properties of space-time white noise and the Walsh stochastic integral (in particular, the fact that the stochastic integral is a centered square-integrable martingale with Itô-type isometry), compute the expectation $\\mathbb{E}[u(t,x)]$ and the variance $\\mathrm{Var}(u(t,x))$ as explicit analytic expressions in terms of $t$, $d$, and $u_{0}$. Your final expressions must not contain unevaluated probabilistic quantities. If an expression is finite only under dimensional restrictions, present the closed form together with the appropriate dimensional regime that ensures finiteness. No rounding is required, and no physical units are involved. The final answer must be a single row vector containing the two expressions $\\mathbb{E}[u(t,x)]$ and $\\mathrm{Var}(u(t,x))$ in that order.", "solution": "The problem is well-posed and scientifically grounded within the mathematical theory of stochastic partial differential equations. All provided information is standard and consistent, allowing for a rigorous derivation of the required quantities.\n\nThe mild solution to the stochastic heat equation is given by\n$$u(t,x) = (p_{t} * u_{0})(x) + \\int_{0}^{t} \\int_{\\mathbb{R}^{d}} p_{t-s}(x-y)\\, W(\\mathrm{d}s,\\mathrm{d}y)$$\nThis solution consists of two parts. The first part, which we denote as $u_d(t,x)$, is the solution to the homogeneous heat equation with initial condition $u_0$:\n$$u_d(t,x) = (p_t * u_0)(x) = \\int_{\\mathbb{R}^{d}} p_t(x-y) u_0(y) \\mathrm{d}y$$\nSince $u_0$ is a deterministic function, $u_d(t,x)$ is also deterministic for any given $(t,x)$.\n\nThe second part, which we denote as $u_s(t,x)$, is the stochastic component arising from the white noise forcing:\n$$u_s(t,x) = \\int_{0}^{t} \\int_{\\mathbb{R}^{d}} p_{t-s}(x-y)\\, W(\\mathrm{d}s,\\mathrm{d}y)$$\nThis is a Walsh-type stochastic integral with respect to the space-time white noise $W$.\n\nFirst, we compute the expectation $\\mathbb{E}[u(t,x)]$. Using the linearity of the expectation operator, we have:\n$$\\mathbb{E}[u(t,x)] = \\mathbb{E}[u_d(t,x) + u_s(t,x)] = \\mathbb{E}[u_d(t,x)] + \\mathbb{E}[u_s(t,x)]$$\nSince $u_d(t,x)$ is deterministic, its expectation is itself:\n$$\\mathbb{E}[u_d(t,x)] = u_d(t,x) = (p_t * u_0)(x)$$\nThe stochastic integral $u_s(t,x)$ is defined with respect to a centered Gaussian random measure $W$. A key property of the Walsh stochastic integral is that if the underlying measure is centered (i.e., $\\mathbb{E}[W(A)] = 0$ for any Borel set $A$), the resulting integral is a centered random variable. Therefore, its expectation is zero:\n$$\\mathbb{E}[u_s(t,x)] = \\mathbb{E}\\left[\\int_{0}^{t} \\int_{\\mathbb{R}^{d}} p_{t-s}(x-y)\\, W(\\mathrm{d}s,\\mathrm{d}y)\\right] = 0$$\nCombining these results, the expectation of the solution is:\n$$\\mathbb{E}[u(t,x)] = (p_t * u_0)(x) = \\int_{\\mathbb{R}^{d}} p_t(x-y) u_0(y) \\mathrm{d}y$$\nSubstituting the expression for the heat kernel $p_t(x)$, we get the explicit form:\n$$\\mathbb{E}[u(t,x)] = \\int_{\\mathbb{R}^{d}} (2\\pi t)^{-d/2} \\exp\\left(-\\frac{|x-y|^{2}}{2t}\\right) u_{0}(y) \\mathrm{d}y$$\n\nNext, we compute the variance $\\mathrm{Var}(u(t,x))$. The variance is defined as $\\mathrm{Var}(X) = \\mathbb{E}[(X - \\mathbb{E}[X])^2]$. In our case:\n$$u(t,x) - \\mathbb{E}[u(t,x)] = (u_d(t,x) + u_s(t,x)) - u_d(t,x) = u_s(t,x)$$\nTherefore, the variance of $u(t,x)$ is equal to the variance of the stochastic part $u_s(t,x)$:\n$$\\mathrm{Var}(u(t,x)) = \\mathrm{Var}(u_s(t,x)) = \\mathbb{E}[(u_s(t,x) - \\mathbb{E}[u_s(t,x)])^2]$$\nSince $\\mathbb{E}[u_s(t,x)]=0$, this simplifies to the second moment of $u_s(t,x)$:\n$$\\mathrm{Var}(u(t,x)) = \\mathbb{E}[u_s(t,x)^2] = \\mathbb{E}\\left[\\left( \\int_{0}^{t} \\int_{\\mathbb{R}^{d}} p_{t-s}(x-y)\\, W(\\mathrm{d}s,\\mathrm{d}y) \\right)^2\\right]$$\nTo evaluate this expectation, we use the Itô isometry property of the Walsh stochastic integral with respect to space-time white noise. For a deterministic integrand $\\phi(s,y)$, this isometry states:\n$$\\mathbb{E}\\left[\\left( \\int_{0}^{t} \\int_{\\mathbb{R}^{d}} \\phi(s,y)\\, W(\\mathrm{d}s,\\mathrm{d}y) \\right)^2\\right] = \\int_{0}^{t} \\int_{\\mathbb{R}^{d}} \\phi(s,y)^2\\, \\mathrm{d}y\\, \\mathrm{d}s$$\nIn our case, the integrand is $\\phi(s,y) = p_{t-s}(x-y)$. Applying the isometry, we get:\n$$\\mathrm{Var}(u(t,x)) = \\int_{0}^{t} \\int_{\\mathbb{R}^{d}} [p_{t-s}(x-y)]^2\\, \\mathrm{d}y\\, \\mathrm{d}s$$\nWe evaluate this double integral, starting with the inner integral over $\\mathbb{R}^d$. Let $\\tau = t-s$. The inner integral is:\n$$I_{inner} = \\int_{\\mathbb{R}^{d}} [p_{\\tau}(x-y)]^2\\, \\mathrm{d}y$$\nSubstituting the definition of the heat kernel:\n$$I_{inner} = \\int_{\\mathbb{R}^{d}} \\left( (2\\pi \\tau)^{-d/2} \\exp\\left(-\\frac{|x-y|^2}{2\\tau}\\right) \\right)^2\\, \\mathrm{d}y = (2\\pi \\tau)^{-d} \\int_{\\mathbb{R}^{d}} \\exp\\left(-\\frac{|x-y|^2}{\\tau}\\right)\\, \\mathrm{d}y$$\nLet $z = x-y$, so $\\mathrm{d}y$ can be replaced by $\\mathrm{d}z$. The integral becomes a standard Gaussian integral:\n$$I_{inner} = (2\\pi \\tau)^{-d} \\int_{\\mathbb{R}^{d}} \\exp\\left(-\\frac{|z|^2}{\\tau}\\right)\\, \\mathrm{d}z$$\nThe value of the multi-dimensional Gaussian integral $\\int_{\\mathbb{R}^d} \\exp(-a|z|^2) \\mathrm{d}z = (\\pi/a)^{d/2}$. Here, $a = 1/\\tau$.\n$$I_{inner} = (2\\pi \\tau)^{-d} (\\pi\\tau)^{d/2} = 2^{-d}\\pi^{-d}\\tau^{-d}\\pi^{d/2}\\tau^{d/2} = (4\\pi)^{-d/2} \\tau^{-d/2}$$\nSubstituting $\\tau = t-s$ back, we have:\n$$I_{inner}(s) = (4\\pi(t-s))^{-d/2}$$\nNow we evaluate the outer integral over the time variable $s$:\n$$\\mathrm{Var}(u(t,x)) = \\int_{0}^{t} (4\\pi)^{-d/2} (t-s)^{-d/2}\\, \\mathrm{d}s = (4\\pi)^{-d/2} \\int_{0}^{t} (t-s)^{-d/2}\\, \\mathrm{d}s$$\nWe use the substitution $v = t-s$, which means $\\mathrm{d}v = -\\mathrm{d}s$. The limits of integration change from $s=0 \\rightarrow v=t$ and $s=t \\rightarrow v=0$:\n$$\\mathrm{Var}(u(t,x)) = (4\\pi)^{-d/2} \\int_{t}^{0} v^{-d/2} (-\\mathrm{d}v) = (4\\pi)^{-d/2} \\int_{0}^{t} v^{-d/2}\\, \\mathrm{d}v$$\nThis integral converges if and only if the exponent $-d/2$ is greater than $-1$, i.e., $d/2 < 1$, which implies $d < 2$. Assuming $d$ is a positive integer, this condition is only met for $d=1$.\n\nCase 1: $d=1$.\n$$\\mathrm{Var}(u(t,x)) = (4\\pi)^{-1/2} \\int_{0}^{t} v^{-1/2}\\, \\mathrm{d}v = \\frac{1}{2\\sqrt{\\pi}} \\left[ 2v^{1/2} \\right]_{0}^{t} = \\frac{1}{2\\sqrt{\\pi}} (2\\sqrt{t} - 0) = \\sqrt{\\frac{t}{\\pi}}$$\nThe variance is finite and equals $\\sqrt{t/\\pi}$.\n\nCase 2: $d \\ge 2$.\nIf $d=2$, the integral becomes $(4\\pi)^{-1} \\int_{0}^{t} v^{-1}\\, \\mathrm{d}v = (4\\pi)^{-1} [\\ln(v)]_{0}^{t}$, which diverges due to the singularity at $v=0$.\nIf $d > 2$, the integral is $(4\\pi)^{-d/2} \\left[ \\frac{v^{1-d/2}}{1-d/2} \\right]_{0}^{t}$. Since $1-d/2 < 0$, the evaluation at the lower limit $v=0$ results in a divergence.\nTherefore, for any integer dimension $d \\ge 2$, the variance of the solution $u(t,x)$ at a single point $(t,x)$ is infinite. This indicates that for $d \\ge 2$, the solution $u(t,\\cdot)$ is not a function but a more singular object, properly understood as a random distribution.\n\nIn summary, the expectation and variance are given by:\n1.  $\\mathbb{E}[u(t,x)] = \\int_{\\mathbb{R}^{d}} (2\\pi t)^{-d/2} \\exp\\left(-\\frac{|x-y|^{2}}{2t}\\right) u_{0}(y) \\mathrm{d}y$\n2.  $\\mathrm{Var}(u(t,x)) = \\begin{cases} \\sqrt{\\frac{t}{\\pi}}, & \\text{if } d=1 \\\\ \\infty, & \\text{if } d \\ge 2 \\end{cases}$", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\int_{\\mathbb{R}^{d}} (2\\pi t)^{-d/2} \\exp\\left(-\\frac{|x-y|^{2}}{2t}\\right) u_{0}(y) \\mathrm{d}y & \n\\begin{cases} \\sqrt{\\frac{t}{\\pi}} & d=1 \\\\ \\infty & d \\ge 2 \\end{cases}\n\\end{pmatrix}\n}\n$$", "id": "3005753"}, {"introduction": "Stochastic systems are often built from multiple components, and understanding their interaction is key. This exercise [@problem_id:3005789] explores the concept of orthogonality for Walsh integrals, a property that arises when integrands have non-overlapping spatial supports. You will use the Itô isometry in its generalized form to show that the quadratic covariation between such integrals is zero, which greatly simplifies the analysis of their sum.", "problem": "Consider a probability space $(\\Omega,\\mathcal{F},\\{\\mathcal{F}_{t}\\}_{t\\geq 0},\\mathbb{P})$ that supports a centered Gaussian orthogonal martingale measure $M$ on the space-time domain $[0,1]\\times[0,1]$ with covariance intensity equal to Lebesgue measure, so that $M$ is the space-time white noise on $[0,1]\\times[0,1]$. Let three predictable, square-integrable integrands be defined by\n$$\\phi_{1}(s,x)=s\\,\\mathbf{1}_{[0,\\,1/3]}(x),\\qquad \\phi_{2}(s,x)=\\sin(s)\\,\\mathbf{1}_{(1/3,\\,2/3]}(x),\\qquad \\phi_{3}(s,x)=\\exp(-s)\\,\\mathbf{1}_{(2/3,\\,1]}(x),$$\nfor $(s,x)\\in[0,1]\\times[0,1]$. Define the Walsh stochastic integrals\n$$I^{(i)}_{t}=\\int_{0}^{t}\\int_{0}^{1}\\phi_{i}(s,x)\\,M(\\mathrm{d}s,\\mathrm{d}x),\\qquad i=1,2,3,$$\nand the sum\n$$X_{t}=I^{(1)}_{t}+I^{(2)}_{t}+I^{(3)}_{t},\\qquad t\\in[0,1].$$\n\nStarting from the definition of an orthogonal martingale measure and the construction of Walsh’s stochastic integral for random fields, explain why the disjointness of the spatial supports of $\\phi_{1}$, $\\phi_{2}$, and $\\phi_{3}$ implies that all pairwise quadratic covariations $\\langle I^{(i)},I^{(j)}\\rangle_{t}$ vanish for $i\\neq j$. Then compute the quadratic variation (also called the bracket) $\\langle X\\rangle_{1}$ of $X$ at time $t=1$. Express your final answer as an exact closed-form analytic expression. No rounding is required, and no units are involved.", "solution": "The problem is valid as it is scientifically grounded in the theory of stochastic partial differential equations, is well-posed, objective, and internally consistent.\n\nThe core of this problem lies in the Itô isometry property for Walsh's stochastic integral with respect to a space-time white noise. Let $M$ be a centered Gaussian orthogonal martingale measure on $[0,1]\\times[0,1]$ with intensity equal to the Lebesgue measure, denoted $\\mathrm{d}s\\,\\mathrm{d}x$. For any two predictable, square-integrable processes $\\phi(s,x)$ and $\\psi(s,x)$, the stochastic integrals $I^{\\phi}_{t} = \\int_{0}^{t}\\int_{0}^{1}\\phi(s,x)\\,M(\\mathrm{d}s,\\mathrm{d}x)$ and $I^{\\psi}_{t} = \\int_{0}^{t}\\int_{0}^{1}\\psi(s,x)\\,M(\\mathrm{d}s,\\mathrm{d}x)$ are continuous martingales. Their quadratic covariation process is given by the deterministic integral:\n$$ \\langle I^{\\phi}, I^{\\psi} \\rangle_{t} = \\int_{0}^{t}\\int_{0}^{1}\\phi(s,x)\\psi(s,x)\\,\\mathrm{d}x\\,\\mathrm{d}s $$\nWhen $\\phi = \\psi$, this gives the quadratic variation (or bracket) of the process $I^{\\phi}$:\n$$ \\langle I^{\\phi} \\rangle_{t} = \\langle I^{\\phi}, I^{\\phi} \\rangle_{t} = \\int_{0}^{t}\\int_{0}^{1}(\\phi(s,x))^{2}\\,\\mathrm{d}x\\,\\mathrm{d}s $$\n\nFirst, we address why the pairwise quadratic covariations $\\langle I^{(i)},I^{(j)}\\rangle_{t}$ vanish for $i\\neq j$. The integrands are given by:\n$$ \\phi_{1}(s,x)=s\\,\\mathbf{1}_{[0,\\,1/3]}(x),\\qquad \\phi_{2}(s,x)=\\sin(s)\\,\\mathbf{1}_{(1/3,\\,2/3]}(x),\\qquad \\phi_{3}(s,x)=\\exp(-s)\\,\\mathbf{1}_{(2/3,\\,1]}(x) $$\nThe spatial support of an integrand $\\phi_i(s,x)$, denoted $\\text{supp}_x(\\phi_i)$, is the set of $x$ values for which $\\phi_i$ is not identically zero. We can identify these from the indicator functions:\n-   $\\text{supp}_x(\\phi_1) = [0, 1/3]$\n-   $\\text{supp}_x(\\phi_2) = (1/3, 2/3]$\n-   $\\text{supp}_x(\\phi_3) = (2/3, 1]$\nThese three spatial domains are mutually disjoint. For any pair $(i, j)$ with $i \\neq j$, the intersection of their spatial supports is the empty set. For instance, $\\text{supp}_x(\\phi_1) \\cap \\text{supp}_x(\\phi_2) = [0, 1/3] \\cap (1/3, 2/3] = \\emptyset$.\n\nLet's compute the quadratic covariation $\\langle I^{(i)}, I^{(j)} \\rangle_{t}$ for $i \\neq j$. The integrand of the deterministic integral is $\\phi_i(s,x)\\phi_j(s,x)$. Due to the disjointness of the spatial supports, the product of the indicator functions in $\\phi_i(s,x)\\phi_j(s,x)$ is zero for all $x \\in [0,1]$ (e.g., $\\mathbf{1}_{[0,\\,1/3]}(x) \\cdot \\mathbf{1}_{(1/3,\\,2/3]}(x) = 0$). Consequently, the product $\\phi_i(s,x)\\phi_j(s,x)$ is zero for all $(s,x) \\in [0,1]\\times[0,1]$.\nTherefore, for $i \\neq j$:\n$$ \\langle I^{(i)}, I^{(j)} \\rangle_{t} = \\int_{0}^{t}\\int_{0}^{1}\\phi_{i}(s,x)\\phi_{j}(s,x)\\,\\mathrm{d}x\\,\\mathrm{d}s = \\int_{0}^{t}\\int_{0}^{1}0\\,\\mathrm{d}x\\,\\mathrm{d}s = 0 $$\nThis demonstrates that the martingales $I^{(1)}_{t}$, $I^{(2)}_{t}$, and $I^{(3)}_{t}$ are mutually orthogonal.\n\nNext, we compute the quadratic variation $\\langle X \\rangle_{1}$ of the process $X_{t}=I^{(1)}_{t}+I^{(2)}_{t}+I^{(3)}_{t}$ at time $t=1$.\nUsing the bilinearity of the quadratic covariation bracket, we have:\n$$ \\langle X \\rangle_{t} = \\left\\langle \\sum_{i=1}^{3}I^{(i)}, \\sum_{j=1}^{3}I^{(j)} \\right\\rangle_{t} = \\sum_{i=1}^{3}\\sum_{j=1}^{3} \\langle I^{(i)}, I^{(j)} \\rangle_{t} $$\nSince we have shown that $\\langle I^{(i)}, I^{(j)} \\rangle_{t} = 0$ for $i \\neq j$, the sum simplifies to:\n$$ \\langle X \\rangle_{t} = \\sum_{i=1}^{3} \\langle I^{(i)}, I^{(i)} \\rangle_{t} = \\sum_{i=1}^{3} \\langle I^{(i)} \\rangle_{t} $$\nWe now compute each individual quadratic variation at $t=1$:\n\n1. For $i=1$:\n$$ \\langle I^{(1)} \\rangle_{1} = \\int_{0}^{1}\\int_{0}^{1} (\\phi_{1}(s,x))^{2}\\,\\mathrm{d}x\\,\\mathrm{d}s = \\int_{0}^{1}\\int_{0}^{1} (s\\,\\mathbf{1}_{[0,\\,1/3]}(x))^{2}\\,\\mathrm{d}x\\,\\mathrm{d}s $$\n$$ = \\int_{0}^{1}s^{2}\\left(\\int_{0}^{1/3}1\\,\\mathrm{d}x\\right)\\mathrm{d}s = \\int_{0}^{1}s^{2}\\left(\\frac{1}{3}\\right)\\mathrm{d}s = \\frac{1}{3}\\left[\\frac{s^{3}}{3}\\right]_{0}^{1} = \\frac{1}{9} $$\n\n2. For $i=2$:\n$$ \\langle I^{(2)} \\rangle_{1} = \\int_{0}^{1}\\int_{0}^{1} (\\phi_{2}(s,x))^{2}\\,\\mathrm{d}x\\,\\mathrm{d}s = \\int_{0}^{1}\\int_{0}^{1} (\\sin(s)\\,\\mathbf{1}_{(1/3,\\,2/3]}(x))^{2}\\,\\mathrm{d}x\\,\\mathrm{d}s $$\n$$ = \\int_{0}^{1}\\sin^{2}(s)\\left(\\int_{1/3}^{2/3}1\\,\\mathrm{d}x\\right)\\mathrm{d}s = \\int_{0}^{1}\\sin^{2}(s)\\left(\\frac{1}{3}\\right)\\mathrm{d}s $$\nUsing the identity $\\sin^{2}(s) = \\frac{1-\\cos(2s)}{2}$:\n$$ = \\frac{1}{3}\\int_{0}^{1}\\frac{1-\\cos(2s)}{2}\\,\\mathrm{d}s = \\frac{1}{6}\\left[s - \\frac{\\sin(2s)}{2}\\right]_{0}^{1} = \\frac{1}{6}\\left(1 - \\frac{\\sin(2)}{2}\\right) = \\frac{1}{6} - \\frac{\\sin(2)}{12} $$\n\n3. For $i=3$:\n$$ \\langle I^{(3)} \\rangle_{1} = \\int_{0}^{1}\\int_{0}^{1} (\\phi_{3}(s,x))^{2}\\,\\mathrm{d}x\\,\\mathrm{d}s = \\int_{0}^{1}\\int_{0}^{1} (\\exp(-s)\\,\\mathbf{1}_{(2/3,\\,1]}(x))^{2}\\,\\mathrm{d}x\\,\\mathrm{d}s $$\n$$ = \\int_{0}^{1}\\exp(-2s)\\left(\\int_{2/3}^{1}1\\,\\mathrm{d}x\\right)\\mathrm{d}s = \\int_{0}^{1}\\exp(-2s)\\left(\\frac{1}{3}\\right)\\mathrm{d}s $$\n$$ = \\frac{1}{3}\\left[-\\frac{1}{2}\\exp(-2s)\\right]_{0}^{1} = -\\frac{1}{6}(\\exp(-2) - \\exp(0)) = \\frac{1}{6}(1 - \\exp(-2)) = \\frac{1}{6} - \\frac{\\exp(-2)}{6} $$\n\nFinally, we sum these results to find $\\langle X \\rangle_{1}$:\n$$ \\langle X \\rangle_{1} = \\langle I^{(1)} \\rangle_{1} + \\langle I^{(2)} \\rangle_{1} + \\langle I^{(3)} \\rangle_{1} $$\n$$ \\langle X \\rangle_{1} = \\frac{1}{9} + \\left(\\frac{1}{6} - \\frac{\\sin(2)}{12}\\right) + \\left(\\frac{1}{6} - \\frac{\\exp(-2)}{6}\\right) $$\n$$ \\langle X \\rangle_{1} = \\frac{1}{9} + \\frac{2}{6} - \\frac{\\sin(2)}{12} - \\frac{\\exp(-2)}{6} = \\frac{1}{9} + \\frac{1}{3} - \\frac{\\sin(2)}{12} - \\frac{\\exp(-2)}{6} $$\n$$ \\langle X \\rangle_{1} = \\frac{1+3}{9} - \\frac{\\sin(2)}{12} - \\frac{\\exp(-2)}{6} = \\frac{4}{9} - \\frac{\\sin(2)}{12} - \\frac{\\exp(-2)}{6} $$\nThis is the exact, closed-form expression for the quadratic variation of $X$ at time $t=1$.", "answer": "$$\\boxed{\\frac{4}{9} - \\frac{\\sin(2)}{12} - \\frac{\\exp(-2)}{6}}$$", "id": "3005789"}, {"introduction": "A deep understanding of any integration theory requires knowing its limits and the precise conditions under which convergence holds. This practice problem [@problem_id:3005775] presents a critical counterexample that distinguishes Walsh integration from classical Riemann integration. You will analyze a sequence of integrands that converges to zero at every point, yet the corresponding sequence of stochastic integrals fails to converge, revealing the necessity of convergence in the $L^2$ norm.", "problem": "Let $\\{M(A): A \\in \\mathcal{B}([0,1]\\times[0,1])\\}$ be a Gaussian orthogonal martingale measure in the sense of Walsh on the measurable rectangle $[0,1]\\times[0,1]$ with covariance (control) measure $Q(\\mathrm{d}s,\\mathrm{d}x)=\\mathrm{d}s\\,\\mathrm{d}x$ (space-time white noise). For each $n\\in\\mathbb{N}$, consider the deterministic predictable integrand\n$$\ng_{n}(s,x)\\;=\\;\\sqrt{n}\\,\\mathbf{1}_{(0,\\,1/n]}(s)\\,\\mathbf{1}_{(0,\\,1]}(x),\\qquad (s,x)\\in[0,1]\\times[0,1],\n$$\nand define $g(s,x)\\equiv 0$. The Walsh stochastic integral $\\int g\\,\\mathrm{d}M$ is understood in the usual sense for predictable square-integrable random fields. \n\nTasks:\n- Using only the foundational definitions of orthogonal martingale measures, predictability, and square-integrability with respect to the control measure $Q$, justify that $g_{n}\\to g$ pointwise $Q$-almost everywhere but the sequence of Walsh integrals $\\int g_{n}\\,\\mathrm{d}M$ fails to converge in probability. Your justification must rest on first principles and may not assume any convergence theorems beyond what can be derived from the covariance structure and the orthogonality of $M$.\n- Then, quantify this failure by computing the following limit of characteristic functions at unit argument:\n$$\nL \\;=\\; \\lim_{n\\to\\infty}\\,\\mathbb{E}\\!\\left[\\exp\\!\\Big(i\\,\\int_{[0,1]\\times[0,1]} g_{n}(s,x)\\,M(\\mathrm{d}s,\\mathrm{d}x)\\Big)\\right].\n$$\n\nGive your final answer for $L$ as a closed-form analytic expression. No rounding is required and no units apply.", "solution": "The problem is validated as scientifically grounded, well-posed, and objective. It is a standard, albeit non-trivial, exercise in the theory of stochastic integration for random fields, illustrating a key difference between pointwise and $L^2$ convergence of integrands.\n\nLet us begin by analyzing the properties of the integrands. The function $g(s,x) \\equiv 0$ is deterministic, hence predictable, and its squared integral with respect to the control measure $Q$ is\n$$\n\\int_{[0,1]\\times[0,1]} g(s,x)^2 \\, Q(\\mathrm{d}s,\\mathrm{d}x) = \\int_0^1 \\int_0^1 0^2 \\, \\mathrm{d}s\\,\\mathrm{d}x = 0,\n$$\nso it is square-integrable.\nFor each $n\\in\\mathbb{N}$, the integrand $g_n(s,x) = \\sqrt{n}\\,\\mathbf{1}_{(0,\\,1/n]}(s)\\,\\mathbf{1}_{(0,\\,1]}(x)$ is also deterministic and thus predictable. Its square-integrability is established by computing its $L^2(Q)$-norm squared:\n$$\n\\int_{[0,1]\\times[0,1]} g_{n}(s,x)^2 \\, Q(\\mathrm{d}s,\\mathrm{d}x) = \\int_0^1 \\int_0^1 \\left(\\sqrt{n}\\,\\mathbf{1}_{(0,\\,1/n]}(s)\\,\\mathbf{1}_{(0,\\,1]}(x)\\right)^2 \\, \\mathrm{d}s\\,\\mathrm{d}x\n$$\n$$\n= n \\int_0^1 \\int_0^1 \\mathbf{1}_{(0,\\,1/n]}(s)\\,\\mathbf{1}_{(0,\\,1]}(x) \\, \\mathrm{d}s\\,\\mathrm{d}x = n \\left(\\int_0^1 \\mathbf{1}_{(0,\\,1/n]}(s)\\,\\mathrm{d}s\\right) \\left(\\int_0^1 \\mathbf{1}_{(0,\\,1]}(x)\\,\\mathrm{d}x\\right)\n$$\n$$\n= n \\left(\\int_0^{1/n} 1\\,\\mathrm{d}s\\right) \\left(\\int_0^1 1\\,\\mathrm{d}x\\right) = n \\left(\\frac{1}{n}\\right) (1) = 1.\n$$\nSince this integral is finite for every $n$, each $g_n$ is a square-integrable predictable field.\n\nNext, we justify the pointwise convergence of $g_n$ to $g$. Let $g(s,x) = 0$. We want to show that $\\lim_{n\\to\\infty} g_n(s,x) = g(s,x)$ for $Q$-almost every $(s,x) \\in [0,1]\\times[0,1]$.\nConsider a fixed point $(s,x) \\in [0,1]\\times[0,1]$.\nIf $s=0$ or $x=0$ or $x>1$, then $g_n(s,x)=0$ for all $n$, and the limit is $0$.\nIf $s \\in (0,1]$ and $x \\in (0,1]$, we observe that for any such $s$, there exists a natural number $N_s$ such that for all $n > N_s$, we have $1/n < s$. For these values of $n$, the indicator function $\\mathbf{1}_{(0,\\,1/n]}(s)$ is equal to $0$. Consequently, $g_n(s,x) = \\sqrt{n} \\cdot 0 \\cdot 1 = 0$ for all $n > N_s$. This implies that for any $(s,x)$ with $s>0$, $\\lim_{n\\to\\infty} g_n(s,x) = 0 = g(s,x)$.\nThe set of points where this argument does not apply is $\\{(s,x) \\in [0,1]\\times[0,1] \\mid s=0\\}$, which is a line segment. The control measure $Q$ is the Lebesgue measure $\\mathrm{d}s\\,\\mathrm{d}x$, and the Lebesgue measure of this line segment is $0$. Therefore, $g_n \\to g$ pointwise $Q$-almost everywhere.\n\nNow, we analyze the sequence of Walsh integrals $I_n = \\int_{[0,1]\\times[0,1]} g_n(s,x) \\, M(\\mathrm{d}s,\\mathrm{d}x)$. Since $M$ is a Gaussian orthogonal martingale measure, the stochastic integral of any deterministic, square-integrable function is a Gaussian random variable. The mean of this variable is\n$$\n\\mathbb{E}[I_n] = \\mathbb{E}\\left[\\int g_n \\, \\mathrm{d}M\\right] = \\int g_n \\, \\mathbb{E}[\\mathrm{d}M] = 0,\n$$\nbecause $M$ is a centered measure. The variance is given by the Itô isometry, which is a direct consequence of the covariance structure and orthogonality of $M$:\n$$\n\\mathrm{Var}(I_n) = \\mathbb{E}[I_n^2] - (\\mathbb{E}[I_n])^2 = \\mathbb{E}\\left[\\left(\\int g_n \\, \\mathrm{d}M\\right)^2\\right] = \\int g_n(s,x)^2 \\, Q(\\mathrm{d}s,\\mathrm{d}x).\n$$\nAs we calculated earlier, this value is $1$ for all $n$. Thus, for every $n \\in \\mathbb{N}$, the random variable $I_n$ has a standard normal distribution, $I_n \\sim N(0,1)$.\n\nThe limit integral is $I = \\int g \\, \\mathrm{d}M = \\int 0 \\, \\mathrm{d}M = 0$. For the sequence $I_n$ to converge in probability to $I=0$, it must satisfy $\\lim_{n\\to\\infty} \\mathbb{P}(|I_n - 0| > \\epsilon) = 0$ for any $\\epsilon > 0$. However, since $I_n \\sim N(0,1)$ for all $n$, the probability $\\mathbb{P}(|I_n| > \\epsilon)$ is constant with respect to $n$:\n$$\n\\mathbb{P}(|I_n| > \\epsilon) = \\mathbb{P}(|Z| > \\epsilon) = \\frac{2}{\\sqrt{2\\pi}}\\int_{\\epsilon}^{\\infty} \\exp\\left(-\\frac{z^2}{2}\\right)\\,\\mathrm{d}z,\n$$\nwhere $Z \\sim N(0,1)$. This value is a positive constant for any $\\epsilon > 0$ and does not converge to $0$. For example, for $\\epsilon=1$, this probability is approximately $0.317$. Therefore, the sequence $I_n$ fails to converge in probability to $0$. This failure is fundamentally due to the fact that the integrands $g_n$ do not converge to $g=0$ in the $L^2(Q)$ sense: $\\|g_n - g\\|_{L^2(Q)}^2 = \\|g_n\\|_{L^2(Q)}^2 = 1 \\not\\to 0$.\n\nFinally, we compute the limit $L$. The expression inside the limit is the characteristic function of $I_n$ evaluated at argument $t=1$.\n$$\n\\mathbb{E}\\left[\\exp\\Big(i\\,\\int_{[0,1]\\times[0,1]} g_{n}(s,x)\\,M(\\mathrm{d}s,\\mathrm{d}x)\\Big)\\right] = \\mathbb{E}[\\exp(i \\cdot 1 \\cdot I_n)] = \\phi_{I_n}(1).\n$$\nThe characteristic function of a general Gaussian random variable $X \\sim N(\\mu, \\sigma^2)$ is given by $\\phi_X(t) = \\exp(i\\mu t - \\frac{1}{2}\\sigma^2 t^2)$. For $I_n \\sim N(0,1)$, we have $\\mu=0$ and $\\sigma^2=1$. Its characteristic function is $\\phi_{I_n}(t) = \\exp(-\\frac{1}{2}t^2)$.\nEvaluating at $t=1$, we get:\n$$\n\\phi_{I_n}(1) = \\exp\\left(-\\frac{1}{2}(1)^2\\right) = \\exp\\left(-\\frac{1}{2}\\right).\n$$\nThis value is constant for all $n \\in \\mathbb{N}$. Therefore, the limit is simply this constant value.\n$$\nL = \\lim_{n\\to\\infty}\\,\\mathbb{E}\\!\\left[\\exp\\!\\Big(i\\,\\int g_{n}\\,\\mathrm{d}M\\Big)\\right] = \\lim_{n\\to\\infty} \\exp\\left(-\\frac{1}{2}\\right) = \\exp\\left(-\\frac{1}{2}\\right).\n$$\nThis result quantifies the failure of convergence. If $I_n$ were to converge in distribution to $I=0$, the limit of the characteristic functions would be the characteristic function of the constant $0$, which is $\\phi_0(1) = \\mathbb{E}[e^{i \\cdot 0}] = 1$. The fact that the limit is $e^{-1/2} \\neq 1$ shows that the sequence $I_n$ does not even converge in distribution to $0$.", "answer": "$$\\boxed{\\exp\\left(-\\frac{1}{2}\\right)}$$", "id": "3005775"}]}