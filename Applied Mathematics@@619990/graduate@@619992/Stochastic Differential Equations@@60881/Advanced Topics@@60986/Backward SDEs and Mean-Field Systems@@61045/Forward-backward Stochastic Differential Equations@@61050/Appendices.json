{"hands_on_practices": [{"introduction": "To begin our hands-on exploration of Forward-Backward Stochastic Differential Equations (FBSDEs), we start with the simplest possible non-trivial case: a linear, decoupled system. This exercise [@problem_id:2977079] is designed to build foundational intuition by stripping away complexities. By leveraging the martingale property of stochastic integrals and the definition of conditional expectation, you will derive an explicit solution for the control process $Z_t$, revealing a direct and elegant connection between the volatility of the forward process and the backward dynamics.", "problem": "Consider a complete filtered probability space $(\\Omega,\\mathcal{F},(\\mathcal{F}_t)_{t\\in[0,T]},\\mathbb{P})$ supporting a one-dimensional standard Brownian motion $W=(W_t)_{t\\in[0,T]}$. Let $X=(X_t)_{t\\in[0,T]}$ be the unique strong solution to the forward stochastic differential equation $dX_t=\\mu\\,dt+\\sigma\\,dW_t$ with deterministic initial condition $X_0=x_0\\in\\mathbb{R}$, where $\\mu\\in\\mathbb{R}$ and $\\sigma\\in(0,\\infty)$ are constants. Consider the decoupled forward-backward stochastic differential equation with zero driver $f\\equiv 0$ and terminal condition $Y_T=g(X_T)$, where $g(x)=x$. The backward component $(Y,Z)$ is adapted to $(\\mathcal{F}_t)_{t\\in[0,T]}$ and satisfies the backward stochastic differential equation\n$$\nY_t=g(X_T)-\\int_t^T Z_s\\,dW_s,\\qquad t\\in[0,T].\n$$\nStarting from first principles of Itô calculus, the martingale property of stochastic integrals, and the definition of conditional expectation in the natural filtration of $W$, derive an explicit expression for the process $Z=(Z_t)_{t\\in[0,T]}$ in closed form. Your final answer must be a single closed-form analytic expression. No rounding is required and no units are involved.", "solution": "The backward stochastic differential equation (BSDE) is given in integral form for $t\\in[0,T]$ as:\n$$\nY_t = Y_T + \\int_t^T f(s,X_s,Y_s,Z_s)\\,ds - \\int_t^T Z_s\\,dW_s\n$$\nIn this problem, the driver is $f \\equiv 0$ and the terminal condition is $Y_T = g(X_T) = X_T$. The BSDE simplifies to:\n$$\nY_t = X_T - \\int_t^T Z_s\\,dW_s\n$$\nA fundamental result in the theory of BSDEs states that the solution component $Y_t$ is given by the conditional expectation of the terminal value plus the integral of the future driver. With $f \\equiv 0$, this relation is:\n$$\nY_t = \\mathbb{E}[Y_T | \\mathcal{F}_t]\n$$\nSubstituting the terminal condition $Y_T = X_T$, we have:\n$$\nY_t = \\mathbb{E}[X_T | \\mathcal{F}_t]\n$$\nTo evaluate this conditional expectation, we first need an explicit expression for the forward process $X_t$. The forward SDE $dX_t = \\mu\\,dt + \\sigma\\,dW_t$ with initial condition $X_0=x_0$ is a linear SDE whose solution is found by direct integration:\n$$\nX_t = X_0 + \\int_0^t \\mu\\,ds + \\int_0^t \\sigma\\,dW_s = x_0 + \\mu t + \\sigma W_t\n$$\nAt the terminal time $t=T$, the value of the process is:\n$$\nX_T = x_0 + \\mu T + \\sigma W_T\n$$\nNow, we can substitute this expression into the formula for $Y_t$:\n$$\nY_t = \\mathbb{E}[x_0 + \\mu T + \\sigma W_T | \\mathcal{F}_t]\n$$\nUsing the linearity of conditional expectation and the fact that $W_t$ is $\\mathcal{F}_t$-measurable while the increment $(W_T-W_t)$ is independent of $\\mathcal{F}_t$:\n$$\nY_t = x_0 + \\mu T + \\sigma \\mathbb{E}[W_T | \\mathcal{F}_t] = x_0 + \\mu T + \\sigma \\mathbb{E}[W_t + (W_T - W_t) | \\mathcal{F}_t] = x_0 + \\mu T + \\sigma W_t\n$$\nWe now have an explicit formula for the process $Y_t$. To find the process $Z_t$, we write the BSDE in differential form. The general form is:\n$$\ndY_t = -f(t,X_t,Y_t,Z_t)\\,dt + Z_t\\,dW_t\n$$\nSince $f \\equiv 0$, this becomes:\n$$\ndY_t = Z_t\\,dW_t\n$$\nWe can also find the differential of our explicit solution for $Y_t$ using Itô calculus. For $Y_t = x_0 + \\mu T + \\sigma W_t$, the terms $x_0$ and $\\mu T$ are constants, so their differentials are zero. The differential of $Y_t$ is:\n$$\ndY_t = d(x_0 + \\mu T + \\sigma W_t) = \\sigma\\,dW_t\n$$\nBy comparing the two expressions for the stochastic differential $dY_t$, we have:\n$$\nZ_t\\,dW_t = \\sigma\\,dW_t\n$$\nThe uniqueness of the Itô process decomposition implies that the integrands with respect to $dW_t$ must be equal almost surely for each $t$. Therefore, we conclude that:\n$$\nZ_t = \\sigma\n$$\nThis holds for all $t \\in [0,T]$. The process $Z = (Z_t)_{t\\in[0,T]}$ is thus a constant process, equal to the volatility parameter $\\sigma$ of the forward process.", "answer": "$$\n\\boxed{\\sigma}\n$$", "id": "2977079"}, {"introduction": "While direct solutions are elegant, most FBSDEs are too complex to be solved in one step. This practice [@problem_id:2977112] introduces a cornerstone constructive method: the Picard iteration. You will approach the solution not as a single unknown, but as the limit of a sequence of approximations. By explicitly calculating the first few iterates for a toy model, you gain concrete, step-by-step insight into how the forward and backward components mutually influence each other and converge towards the true solution, demystifying this powerful theoretical and numerical tool.", "problem": "Consider a complete filtered probability space $(\\Omega,\\mathcal{F},(\\mathcal{F}_t)_{t\\in[0,T]},\\mathbb{P})$ satisfying the usual conditions and supporting a one-dimensional standard Brownian motion $W=(W_t)_{t\\in[0,T]}$. Fix deterministic constants $x_0\\in\\mathbb{R}$, $\\sigma\\in\\mathbb{R}$, and $\\alpha\\in\\mathbb{R}\\setminus\\{0\\}$, and a horizon $T>0$ that is sufficiently small to ensure contraction of the Picard mapping for forward-backward stochastic differential equations (FBSDE). Consider the forward-backward system\n$$\n\\mathrm{d}X_t \\;=\\; Y_t\\,\\mathrm{d}t \\;+\\; \\sigma\\,\\mathrm{d}W_t,\\qquad X_0 \\;=\\; x_0,\n$$\nand\n$$\nY_t \\;=\\; X_T \\;+\\; \\int_t^T \\alpha\\,Y_s\\,\\mathrm{d}s \\;-\\; \\int_t^T Z_s\\,\\mathrm{d}W_s,\\qquad t\\in[0,T].\n$$\nStarting from the foundational definitions of Itô integrals and martingales, define the Picard iteration on the pair $(Y,Z)$ by the initialization $Y^{(0)}_t\\equiv 0$ and $Z^{(0)}_t\\equiv 0$ for $t\\in[0,T]$, and then, for each $n\\geq 0$, define $X^{(n+1)}$ by\n$$\nX^{(n+1)}_t \\;=\\; x_0 \\;+\\; \\int_0^t Y^{(n)}_s\\,\\mathrm{d}s \\;+\\; \\sigma\\,W_t,\\qquad t\\in[0,T],\n$$\nand $(Y^{(n+1)},Z^{(n+1)})$ as the unique adapted solution to the linear backward stochastic differential equation\n$$\nY^{(n+1)}_t \\;=\\; X^{(n+1)}_T \\;+\\; \\int_t^T \\alpha\\,Y^{(n+1)}_s\\,\\mathrm{d}s \\;-\\; \\int_t^T Z^{(n+1)}_s\\,\\mathrm{d}W_s,\\qquad t\\in[0,T].\n$$\nCompute explicitly the first two Picard iterates $(X^{(1)},Y^{(1)},Z^{(1)})$ and $(X^{(2)},Y^{(2)},Z^{(2)})$ as closed-form adapted processes. Your final answer must be the single row matrix containing the six expressions $(X^{(1)}_t,\\,Y^{(1)}_t,\\,Z^{(1)}_t,\\,X^{(2)}_t,\\,Y^{(2)}_t,\\,Z^{(2)}_t)$ for a generic $t\\in[0,T]$. No rounding is required.", "solution": "The task is to compute the first two Picard iterates for the given FBSDE system, starting with the initialization $Y^{(0)}_t \\equiv 0$ and $Z^{(0)}_t \\equiv 0$.\n\nThe BSDE for the $(n+1)$-th iterate has a driver $f(Y_s) = \\alpha Y_s$. Its differential form is $\\mathrm{d}Y^{(n+1)}_t = \\alpha Y^{(n+1)}_t \\mathrm{d}t + Z^{(n+1)}_t \\mathrm{d}W_t$, with terminal condition $Y^{(n+1)}_T = X^{(n+1)}_T$. The solution for $Y^{(n+1)}_t$ is given by the conditional expectation formula for linear BSDEs:\n$$\nY^{(n+1)}_t = \\mathbb{E}\\left[ \\exp\\left(-\\int_t^T \\alpha\\,\\mathrm{d}s\\right) X^{(n+1)}_T \\,\\middle|\\, \\mathcal{F}_t \\right] = \\mathbb{E}\\left[ \\exp(-\\alpha(T-t)) X^{(n+1)}_T \\,\\middle|\\, \\mathcal{F}_t \\right].\n$$\nThe process $Z^{(n+1)}_t$ can then be identified via Itô's formula and comparison of diffusion terms.\n\n**First Iterate: $(X^{(1)}, Y^{(1)}, Z^{(1)})$**\n\nWe begin with $n=0$, using the initialization $Y^{(0)}_s = 0$.\n\n1.  **Compute $X^{(1)}_t$**:\n    $$\n    X^{(1)}_t = x_0 + \\int_0^t Y^{(0)}_s\\,\\mathrm{d}s + \\sigma\\,W_t = x_0 + \\int_0^t 0\\,\\mathrm{d}s + \\sigma\\,W_t = x_0 + \\sigma\\,W_t.\n    $$\n\n2.  **Compute $Y^{(1)}_t$**:\n    The terminal condition is $X^{(1)}_T = x_0 + \\sigma W_T$.\n    $$\n    Y^{(1)}_t = \\mathbb{E}\\left[ \\exp(-\\alpha(T-t)) (x_0 + \\sigma W_T) \\,\\middle|\\, \\mathcal{F}_t \\right].\n    $$\n    Using the linearity of conditional expectation and the martingale property $\\mathbb{E}[W_T|\\mathcal{F}_t] = W_t$:\n    $$\n    Y^{(1)}_t = \\exp(-\\alpha(T-t)) (x_0 + \\sigma \\mathbb{E}[W_T|\\mathcal{F}_t]) = (x_0 + \\sigma W_t) \\exp(-\\alpha(T-t)).\n    $$\n\n3.  **Compute $Z^{(1)}_t$**:\n    We apply Itô's product rule to the expression for $Y^{(1)}_t$:\n    $$\n    \\mathrm{d}Y^{(1)}_t = \\exp(-\\alpha(T-t)) \\sigma \\mathrm{d}W_t + (x_0 + \\sigma W_t) \\alpha \\exp(-\\alpha(T-t)) \\mathrm{d}t.\n    $$\n    Recognizing that $\\alpha Y^{(1)}_t = \\alpha (x_0 + \\sigma W_t) \\exp(-\\alpha(T-t))$, we can write:\n    $$\n    \\mathrm{d}Y^{(1)}_t = \\alpha Y^{(1)}_t \\mathrm{d}t + \\sigma \\exp(-\\alpha(T-t)) \\mathrm{d}W_t.\n    $$\n    By comparing this with the BSDE form $\\mathrm{d}Y^{(1)}_t = \\alpha Y^{(1)}_t \\mathrm{d}t + Z^{(1)}_t \\mathrm{d}W_t$, we identify the diffusion term:\n    $$\n    Z^{(1)}_t = \\sigma \\exp(-\\alpha(T-t)).\n    $$\n\n**Second Iterate: $(X^{(2)}, Y^{(2)}, Z^{(2)})$**\n\nNow we proceed with $n=1$, using the derived expression for $Y^{(1)}_t$.\n\n1.  **Compute $X^{(2)}_t$**:\n    $$\n    X^{(2)}_t = x_0 + \\int_0^t Y^{(1)}_s\\,\\mathrm{d}s + \\sigma\\,W_t = x_0 + \\sigma W_t + \\int_0^t (x_0 + \\sigma W_s) \\exp(-\\alpha(T-s))\\,\\mathrm{d}s.\n    $$\n    The integral is split into a deterministic part and a stochastic part.\n    $\\int_0^t x_0 e^{-\\alpha(T-s)}\\,\\mathrm{d}s = x_0 e^{-\\alpha T} \\int_0^t e^{\\alpha s}\\,\\mathrm{d}s = \\frac{x_0}{\\alpha}(e^{-\\alpha(T-t)} - e^{-\\alpha T})$.\n    For the stochastic part, we use integration by parts for Itô integrals:\n    $\\int_0^t \\sigma W_s e^{-\\alpha(T-s)}\\,\\mathrm{d}s = \\sigma e^{-\\alpha T} \\int_0^t W_s e^{\\alpha s}\\,\\mathrm{d}s = \\frac{\\sigma}{\\alpha}W_t e^{-\\alpha(T-t)} - \\frac{\\sigma e^{-\\alpha T}}{\\alpha}\\int_0^t e^{\\alpha s}\\,\\mathrm{d}W_s$.\n    Combining these results:\n    $$\n    X^{(2)}_t = x_0\\left(1 + \\frac{e^{-\\alpha(T-t)} - e^{-\\alpha T}}{\\alpha}\\right) + \\sigma W_t\\left(1 + \\frac{e^{-\\alpha(T-t)}}{\\alpha}\\right) - \\frac{\\sigma e^{-\\alpha T}}{\\alpha} \\int_0^t e^{\\alpha s}\\,\\mathrm{d}W_s.\n    $$\n\n2.  **Compute $Y^{(2)}_t$**:\n    First, we determine the terminal value $X^{(2)}_T$ by setting $t=T$ in the expression for $X^{(2)}_t$:\n    $$\n    X^{(2)}_T = x_0\\left(1 + \\frac{1 - e^{-\\alpha T}}{\\alpha}\\right) + \\sigma W_T\\left(1 + \\frac{1}{\\alpha}\\right) - \\frac{\\sigma e^{-\\alpha T}}{\\alpha} \\int_0^T e^{\\alpha s}\\,\\mathrm{d}W_s.\n    $$\n    Now, we compute $Y^{(2)}_t = \\mathbb{E}[\\exp(-\\alpha(T-t)) X^{(2)}_T | \\mathcal{F}_t]$. We take the conditional expectation of each term in $X^{(2)}_T$.\n    $$\n    \\mathbb{E}[X^{(2)}_T | \\mathcal{F}_t] = x_0\\left(1 + \\frac{1 - e^{-\\alpha T}}{\\alpha}\\right) + \\sigma W_t\\left(1 + \\frac{1}{\\alpha}\\right) - \\frac{\\sigma e^{-\\alpha T}}{\\alpha} \\int_0^t e^{\\alpha s}\\,\\mathrm{d}W_s.\n    $$\n    Multiplying by $\\exp(-\\alpha(T-t))$ gives $Y^{(2)}_t$:\n    $$\n    Y^{(2)}_t = e^{-\\alpha(T-t)} \\left( x_0\\left(1 + \\frac{1 - e^{-\\alpha T}}{\\alpha}\\right) + \\sigma W_t\\left(1 + \\frac{1}{\\alpha}\\right) - \\frac{\\sigma e^{-\\alpha T}}{\\alpha} \\int_0^t e^{\\alpha s}\\,\\mathrm{d}W_s \\right).\n    $$\n\n3.  **Compute $Z^{(2)}_t$**:\n    We apply Itô's rule to $Y^{(2)}_t$. The drift term will be $\\alpha Y^{(2)}_t \\mathrm{d}t$ by construction. The diffusion term arises from the stochastic terms inside the parenthesis:\n    $$\n    \\mathrm{d}Z_t^{\\text{term}} = \\sigma \\left(1 + \\frac{1}{\\alpha}\\right) \\mathrm{d}W_t - \\frac{\\sigma e^{-\\alpha T}}{\\alpha} e^{\\alpha t} \\mathrm{d}W_t = \\left(\\sigma\\frac{\\alpha+1}{\\alpha} - \\frac{\\sigma}{\\alpha}e^{-\\alpha(T-t)}\\right)\\mathrm{d}W_t.\n    $$\n    So the diffusion term of $\\mathrm{d}Y^{(2)}_t$ is $e^{-\\alpha(T-t)}$ times the above, which gives $Z^{(2)}_t$:\n    $$\n    Z^{(2)}_t = e^{-\\alpha(T-t)} \\left( \\sigma\\frac{\\alpha+1}{\\alpha} - \\frac{\\sigma}{\\alpha}e^{-\\alpha(T-t)} \\right) = \\frac{\\sigma}{\\alpha} \\left( (\\alpha+1)e^{-\\alpha(T-t)} - e^{-2\\alpha(T-t)} \\right).\n    $$\n\nSummary of the expressions for the final answer:\n\\begin{itemize}\n    \\item $X^{(1)}_t = x_0 + \\sigma W_t$\n    \\item $Y^{(1)}_t = (x_0 + \\sigma W_t) e^{-\\alpha(T-t)}$\n    \\item $Z^{(1)}_t = \\sigma e^{-\\alpha(T-t)}$\n    \\item $X^{(2)}_t = x_0\\left(1 + \\frac{e^{-\\alpha(T-t)} - e^{-\\alpha T}}{\\alpha}\\right) + \\sigma W_t\\left(1 + \\frac{e^{-\\alpha(T-t)}}{\\alpha}\\right) - \\frac{\\sigma e^{-\\alpha T}}{\\alpha} \\int_0^t e^{\\alpha s}\\,\\mathrm{d}W_s$\n    \\item $Y^{(2)}_t = e^{-\\alpha(T-t)} \\left( x_0\\left(1 + \\frac{1 - e^{-\\alpha T}}{\\alpha}\\right) + \\sigma W_t\\left(1 + \\frac{1}{\\alpha}\\right) - \\frac{\\sigma e^{-\\alpha T}}{\\alpha} \\int_0^t e^{\\alpha s}\\,\\mathrm{d}W_s \\right)$\n    \\item $Z^{(2)}_t = \\frac{\\sigma}{\\alpha} \\left( (\\alpha+1)e^{-\\alpha(T-t)} - e^{-2\\alpha(T-t)} \\right)$\n\\end{itemize}", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\nx_0 + \\sigma W_t & (x_0 + \\sigma W_t) e^{-\\alpha(T-t)} & \\sigma e^{-\\alpha(T-t)} & x_0\\left(1 + \\frac{e^{-\\alpha(T-t)} - e^{-\\alpha T}}{\\alpha}\\right) + \\sigma W_t\\left(1 + \\frac{e^{-\\alpha(T-t)}}{\\alpha}\\right) - \\frac{\\sigma e^{-\\alpha T}}{\\alpha} \\int_0^t e^{\\alpha s}\\,\\mathrm{d}W_s & e^{-\\alpha(T-t)} \\left( x_0\\left(1 + \\frac{1 - e^{-\\alpha T}}{\\alpha}\\right) + \\sigma W_t\\left(1 + \\frac{1}{\\alpha}\\right) - \\frac{\\sigma e^{-\\alpha T}}{\\alpha} \\int_0^t e^{\\alpha s}\\,\\mathrm{d}W_s \\right) & \\frac{\\sigma}{\\alpha} \\left( (\\alpha+1)e^{-\\alpha(T-t)} - e^{-2\\alpha(T-t)} \\right)\n\\end{pmatrix}\n}\n$$", "id": "2977112"}, {"introduction": "Having practiced the Picard iteration, a critical question arises: when can we trust this method to converge? This problem [@problem_id:2977074] moves from application to analysis, investigating the conditions that guarantee the convergence of the iterative scheme. You will delve into the functional analysis perspective by treating the iteration as a linear map on a function space and determining its spectral radius. This exercise demonstrates that the solvability of an FBSDE can depend critically on the interplay between its coupling parameters ($\\alpha$, $q$) and the time horizon $T$, a deep and fundamental feature of these systems.", "problem": "Consider the scalar linear Forward-Backward Stochastic Differential Equation (FBSDE), where an FBSDE denotes a Forward-Backward Stochastic Differential Equation and a Backward Stochastic Differential Equation (BSDE) denotes a Backward Stochastic Differential Equation, on the time interval $\\left[0,T\\right]$ with $T>0$:\n- Forward equation: $\\,dX_t = \\alpha\\,Y_t\\,dt + \\sigma\\,dW_t\\,,\\quad X_0=x_0\\,$,\n- Backward equation: $\\,dY_t = -\\,q\\,X_t\\,dt + Z_t\\,dW_t\\,,\\quad Y_T = 0\\,$,\nwhere $\\alpha>0$, $q>0$, $\\sigma\\ge 0$ are constants and $\\left(W_t\\right)_{t\\in\\left[0,T\\right]}$ is a standard Brownian motion on a filtered probability space satisfying the usual conditions. Consider the canonical Picard fixed-point iteration that alternates between:\n- Given an adapted process $Y^{(k)}$, define $X^{(k+1)}$ by solving the forward equation with $Y^{(k)}$ in place of $Y$.\n- Then define $\\left(Y^{(k+1)},Z^{(k+1)}\\right)$ by solving the backward equation with $X^{(k+1)}$ in place of $X$.\nAssume the iteration is posed on the space $L^2\\left(\\Omega\\times\\left[0,T\\right]\\right)$ with the usual norm, and analyze the linearization of the Picard map around the origin to determine whether it is a contraction. Starting only from the definitions above and standard properties of linear BSDEs with drivers independent of $\\left(Y,Z\\right)$, derive the spectral radius of the linearized map in terms of the parameters and determine a parameter regime in which the iteration fails to converge. Select the correct statement below.\n\nA. With $Y_T=0$, the linearized Picard map has spectral radius $\\rho = \\dfrac{4}{\\pi^2}\\,q\\,\\alpha\\,T^2$, hence the map is not a contraction and the iteration fails to converge whenever $q\\,\\alpha\\,T^2 \\ge \\dfrac{\\pi^2}{4}$.\n\nB. With $Y_T=0$, the linearized Picard map has spectral radius $\\rho = \\sqrt{q\\,\\alpha}\\,T$, hence the iteration fails to converge whenever $\\sqrt{q\\,\\alpha}\\,T \\ge 1$.\n\nC. With $Y_T=0$, the spectral radius of the linearized Picard map is independent of $T$, so convergence or divergence depends only on $\\alpha$ and $q$.\n\nD. For sufficiently large $\\sigma$, the spectral radius of the linearized Picard map strictly decreases to below $1$ for any fixed $\\alpha$ and $q$, thereby restoring convergence regardless of $T$.", "solution": "The Picard iteration defines a map $\\Phi$ on the space of adapted processes. We analyze the linear operator $K$ that forms the core of this map, $Y' = \\Phi(Y) = K(Y) + C$, where $C$ is an affine term. Convergence is determined by the spectral radius of $K$.\n\n1.  **Define the Map**: Given an input process $Y$, we first find the corresponding state process $X$. Ignoring the affine terms $x_0$ and $\\sigma W_t$ which do not affect the linear operator, we have $X_t = \\alpha \\int_0^t Y_s\\,ds$.\n    Next, we solve the BSDE $dY'_t = -q\\,X_t\\,dt + Z'_t\\,dW_t$ with $Y'_T=0$. The solution is given by $Y'_t = \\mathbb{E}\\left[\\int_t^T q\\,X_s\\,ds \\mid \\mathcal{F}_t\\right]$. Substituting the expression for $X_s$, we get the linear map $K$:\n    $$(KY)_t = Y'_t = \\mathbb{E}\\left[q\\,\\alpha \\int_t^T \\left(\\int_0^s Y_u\\,du\\right)ds \\mid \\mathcal{F}_t \\right]$$\n\n2.  **Eigenvalue Problem**: To find the spectral radius, we seek the eigenvalues $\\lambda$ of $K$ by solving $KY = \\lambda Y$. We can find the spectrum by restricting our attention to the subspace of deterministic functions $y(t)$, as the operator $K$ maps deterministic functions to deterministic functions. For a deterministic $y(t)$, the conditional expectation is redundant:\n    $$\\lambda y(t) = q\\,\\alpha \\int_t^T \\left(\\int_0^s y(u)\\,du\\right)ds$$\n    This is an integral equation. We convert it to a differential equation by repeated differentiation with respect to $t$.\n\n3.  **Derive the ODE and Boundary Conditions**:\n    -   Differentiating once: $\\lambda y'(t) = -q\\,\\alpha \\int_0^t y(u)\\,du$.\n    -   Differentiating a second time: $\\lambda y''(t) = -q\\,\\alpha y(t)$, which rearranges to $y''(t) + \\frac{q\\,\\alpha}{\\lambda} y(t) = 0$.\n    We find the boundary conditions from the integral equations:\n    -   At $t=T$, the first integral equation gives $\\lambda y(T) = q\\alpha \\int_T^T (\\dots) ds = 0$. Since we seek $\\lambda \\neq 0$, we must have $y(T)=0$.\n    -   At $t=0$, the equation for the first derivative gives $\\lambda y'(0) = -q\\alpha \\int_0^0 y(u) du = 0$. This implies $y'(0)=0$.\n\n4.  **Solve the Sturm-Liouville Problem**: We need to solve the boundary value problem:\n    $$y''(t) + \\omega^2 y(t) = 0, \\quad \\text{with } y'(0)=0, \\quad y(T)=0,$$\n    where we have set $\\omega^2 = \\frac{q\\,\\alpha}{\\lambda}$. The general solution to the ODE is $y(t) = A\\cos(\\omega t) + B\\sin(\\omega t)$.\n    -   The condition $y'(0)=0$ implies $y'(t)|_{t=0} = -A\\omega\\sin(0) + B\\omega\\cos(0) = B\\omega = 0$. For a non-trivial solution, $\\omega \\ne 0$, so we must have $B=0$. The solution is of the form $y(t) = A\\cos(\\omega t)$.\n    -   The condition $y(T)=0$ implies $A\\cos(\\omega T) = 0$. For a non-trivial solution ($A \\ne 0$), we must have $\\cos(\\omega T) = 0$.\n    This holds when $\\omega T$ is an odd multiple of $\\pi/2$:\n    $$\\omega_n T = \\left(n + \\frac{1}{2}\\right)\\pi, \\quad \\text{for } n = 0, 1, 2, \\dots$$\n    So, the discrete values of $\\omega$ are $\\omega_n = \\frac{(n + 1/2)\\pi}{T}$.\n\n5.  **Calculate the Spectral Radius**: The eigenvalues $\\lambda_n$ are related to $\\omega_n$ by $\\lambda_n = \\frac{q\\,\\alpha}{\\omega_n^2}$:\n    $$\\lambda_n = \\frac{q\\,\\alpha\\,T^2}{(n + 1/2)^2\\pi^2}$$\n    The spectral radius $\\rho$ is the largest eigenvalue (in magnitude). Since $\\alpha, q > 0$, all eigenvalues are positive. The largest $\\lambda_n$ corresponds to the smallest $n$, which is $n=0$:\n    $$\\rho = \\lambda_0 = \\frac{q\\,\\alpha\\,T^2}{(1/2)^2\\pi^2} = \\frac{4q\\,\\alpha\\,T^2}{\\pi^2}$$\n    The Picard iteration is a contraction, and thus guaranteed to converge, if the spectral radius is less than 1. The map is not a contraction and the iteration may fail to converge if $\\rho \\ge 1$. This condition is:\n    $$\\frac{4q\\,\\alpha\\,T^2}{\\pi^2} \\ge 1 \\quad \\implies \\quad q\\,\\alpha\\,T^2 \\ge \\frac{\\pi^2}{4}$$\n    Comparing this result with the given options, option A is the correct statement.", "answer": "$$\\boxed{A}$$", "id": "2977074"}]}