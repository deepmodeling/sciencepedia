{"hands_on_practices": [{"introduction": "A cornerstone of the propagation of chaos theory is the demonstration that as the number of particles $N$ approaches infinity, the random fluctuations of the empirical measure vanish, leading to a deterministic evolution described by a PDE. This exercise provides a fundamental, hands-on calculation to prove this convergence. By applying Itô's formula to an observable of the empirical measure, you will decompose its evolution into a predictable part and a martingale, and then explicitly show that the variance of this martingale term converges to zero, beautifully illustrating the law of large numbers at the heart of mean-field theory [@problem_id:2991706].", "problem": "Consider the following interacting particle system in $\\mathbb{R}^{d}$ driven by independent $m$-dimensional Brownian motions, specified for $i \\in \\{1,\\dots, N\\}$ by the stochastic differential equation (SDE)\n$$\n\\mathrm{d}X_{t}^{i,N} \\;=\\; b\\!\\left(X_{t}^{i,N}, \\mu_{t}^{N}\\right)\\,\\mathrm{d}t \\;+\\; \\sigma\\!\\left(X_{t}^{i,N}, \\mu_{t}^{N}\\right)\\,\\mathrm{d}W_{t}^{i},\n$$\nwhere $\\mu_{t}^{N} := \\frac{1}{N} \\sum_{j=1}^{N} \\delta_{X_{t}^{j,N}}$ is the empirical measure, $(W^{i})_{i=1}^{N}$ are independent standard Brownian motions in $\\mathbb{R}^{m}$, the drift $b:\\mathbb{R}^{d}\\times\\mathcal{P}_{2}(\\mathbb{R}^{d})\\to\\mathbb{R}^{d}$ and diffusion $\\sigma:\\mathbb{R}^{d}\\times\\mathcal{P}_{2}(\\mathbb{R}^{d})\\to\\mathbb{R}^{d\\times m}$ are globally Lipschitz and of linear growth, and the initial data $\\{X_{0}^{i,N}\\}_{i=1}^{N}$ are independent and identically distributed with common law $\\mu_{0}$ having a finite second moment. Assume in addition that there exists a constant $\\Sigma \\in (0,\\infty)$ such that $\\sup_{x,\\mu} \\|\\sigma(x,\\mu)\\|_{\\mathrm{op}} \\le \\Sigma$, where $\\|\\cdot\\|_{\\mathrm{op}}$ denotes the operator norm on $\\mathbb{R}^{d\\times m}$. Let $\\varphi \\in C_{b}^{2}(\\mathbb{R}^{d})$ be a twice continuously differentiable test function with bounded derivatives, and let $\\|\\nabla \\varphi\\|_{\\infty} := \\sup_{x \\in \\mathbb{R}^{d}} \\|\\nabla \\varphi(x)\\|$.\n\nDefine the empirical observable $\\mu_{t}^{N}(\\varphi) := \\frac{1}{N}\\sum_{i=1}^{N} \\varphi(X_{t}^{i,N})$. Using Itô's formula and the Doob–Meyer decomposition, write $\\mu_{t}^{N}(\\varphi)$ as a sum of a finite-variation term and a martingale $M_{t}^{N}(\\varphi)$. Starting from the definitions and first principles of stochastic calculus, compute explicitly the predictable quadratic variation $\\langle M^{N}(\\varphi)\\rangle_{t}$ of this martingale in terms of $\\sigma$, $\\nabla \\varphi$, and the empirical measure. Then, under the boundedness assumptions stated above, show that $\\langle M^{N}(\\varphi)\\rangle_{t}$ vanishes as $N \\to \\infty$ for every fixed $t \\in [0,T]$, thereby establishing that the limit dynamics of $\\mu_{t}^{N}(\\varphi)$ are deterministic.\n\nYour final answer must be the exact value of the limit\n$$\n\\lim_{N \\to \\infty} \\langle M^{N}(\\varphi) \\rangle_{t},\n$$\nexpressed in closed form. No units are required. Do not round your answer.", "solution": "The problem asks for the computation of the predictable quadratic variation of a specific martingale associated with an interacting particle system and the evaluation of its limit as the number of particles $N$ tends to infinity.\n\nLet us begin by applying Itô's formula to the process $\\varphi(X_{t}^{i,N})$ for a fixed particle $i \\in \\{1,\\dots,N\\}$. The process $X_{t}^{i,N}$ is a $d$-dimensional Itô process governed by the stochastic differential equation (SDE):\n$$ \\mathrm{d}X_{t}^{i,N} = b(X_{t}^{i,N}, \\mu_{t}^{N}) \\mathrm{d}t + \\sigma(X_{t}^{i,N}, \\mu_{t}^{N}) \\mathrm{d}W_{t}^{i} $$\nSince the test function $\\varphi \\in C_{b}^{2}(\\mathbb{R}^{d})$ does not depend explicitly on time $t$, Itô's formula states:\n$$ \\mathrm{d}\\varphi(X_{t}^{i,N}) = \\nabla \\varphi(X_{t}^{i,N})^{T} \\mathrm{d}X_{t}^{i,N} + \\frac{1}{2} \\mathrm{Tr}\\left( H_{\\varphi}(X_{t}^{i,N}) (\\mathrm{d}X_{t}^{i,N})(\\mathrm{d}X_{t}^{i,N})^{T} \\right) $$\nwhere $\\nabla \\varphi$ is the gradient of $\\varphi$ and $H_{\\varphi}$ is its Hessian matrix. The quadratic variation term for $X_{t}^{i,N}$ is calculated using Itô's isometry rules. Since $(W^{i})_{t}$ is a standard $m$-dimensional Brownian motion, we have $(\\mathrm{d}W_{t}^{i})(\\mathrm{d}W_{t}^{i})^{T} = I_{m} \\mathrm{d}t$, where $I_{m}$ is the $m \\times m$ identity matrix.\n$$ (\\mathrm{d}X_{t}^{i,N})(\\mathrm{d}X_{t}^{i,N})^{T} = \\left(\\sigma(X_{t}^{i,N}, \\mu_{t}^{N}) \\mathrm{d}W_{t}^{i}\\right) \\left(\\sigma(X_{t}^{i,N}, \\mu_{t}^{N}) \\mathrm{d}W_{t}^{i}\\right)^{T} = \\sigma(X_{t}^{i,N}, \\mu_{t}^{N}) \\sigma(X_{t}^{i,N}, \\mu_{t}^{N})^{T} \\mathrm{d}t $$\nSubstituting the SDE for $X_{t}^{i,N}$ and its quadratic variation into Itô's formula, we get:\n$$ \\mathrm{d}\\varphi(X_{t}^{i,N}) = \\left( \\nabla \\varphi(X_{t}^{i,N})^{T} b(X_{t}^{i,N}, \\mu_{t}^{N}) + \\frac{1}{2}\\mathrm{Tr}\\left( \\sigma(X_{t}^{i,N}, \\mu_{t}^{N}) \\sigma(X_{t}^{i,N}, \\mu_{t}^{N})^{T} H_{\\varphi}(X_{t}^{i,N}) \\right) \\right) \\mathrm{d}t + \\nabla \\varphi(X_{t}^{i,N})^{T} \\sigma(X_{t}^{i,N}, \\mu_{t}^{N}) \\mathrm{d}W_{t}^{i} $$\nThe term in the parenthesis is the action of the generator $\\mathcal{L}_{\\mu}$ on $\\varphi$, evaluated at $(X_{t}^{i,N}, \\mu_{t}^{N})$. Let us denote it as $\\mathcal{L}_{\\mu_{t}^{N}}\\varphi(X_{t}^{i,N})$.\n\nNext, we consider the dynamics of the empirical observable $\\mu_{t}^{N}(\\varphi) = \\frac{1}{N}\\sum_{i=1}^{N} \\varphi(X_{t}^{i,N})$. By linearity of differentiation and integration:\n$$ \\mathrm{d}\\mu_{t}^{N}(\\varphi) = \\frac{1}{N} \\sum_{i=1}^{N} \\mathrm{d}\\varphi(X_{t}^{i,N}) = \\left(\\frac{1}{N}\\sum_{i=1}^{N} \\mathcal{L}_{\\mu_{t}^{N}}\\varphi(X_{t}^{i,N})\\right)\\mathrm{d}t + \\frac{1}{N}\\sum_{i=1}^{N} \\nabla \\varphi(X_{t}^{i,N})^{T} \\sigma(X_{t}^{i,N}, \\mu_{t}^{N}) \\mathrm{d}W_{t}^{i} $$\nThis equation provides the Doob–Meyer decomposition of $\\mu_{t}^{N}(\\varphi)$. The process $\\mu_{t}^{N}(\\varphi)$ is expressed as the sum of a finite-variation term and a martingale. The finite-variation part is $\\int_{0}^{t} \\left(\\frac{1}{N}\\sum_{i=1}^{N} \\mathcal{L}_{\\mu_{s}^{N}}\\varphi(X_{s}^{i,N})\\right)\\mathrm{d}s$, and the martingale part is:\n$$ M_{t}^{N}(\\varphi) = \\frac{1}{N}\\sum_{i=1}^{N} \\int_{0}^{t} \\nabla \\varphi(X_{s}^{i,N})^{T} \\sigma(X_{s}^{i,N}, \\mu_{s}^{N}) \\mathrm{d}W_{s}^{i} $$\nThe problem requires computing the predictable quadratic variation of this martingale, $\\langle M^{N}(\\varphi)\\rangle_{t}$. Since the Brownian motions $(W^{i})_{i=1}^{N}$ are independent, the quadratic variation of the sum of the stochastic integrals is the sum of their individual quadratic variations:\n$$ \\langle M^{N}(\\varphi)\\rangle_{t} = \\left\\langle \\sum_{i=1}^{N} \\int_{0}^{\\cdot} \\frac{1}{N} \\nabla \\varphi(X_{s}^{i,N})^{T} \\sigma(X_{s}^{i,N}, \\mu_{s}^{N}) \\mathrm{d}W_{s}^{i} \\right\\rangle_{t} = \\sum_{i=1}^{N} \\left\\langle \\int_{0}^{\\cdot} \\frac{1}{N} \\nabla \\varphi(X_{s}^{i,N})^{T} \\sigma(X_{s}^{i,N}, \\mu_{s}^{N}) \\mathrm{d}W_{s}^{i} \\right\\rangle_{t} $$\nFor a vector-valued integrand $H_{s}$ (here a $1 \\times m$ row vector), the quadratic variation of $\\int_{0}^{t} H_{s} \\mathrm{d}W_{s}$ is $\\int_{0}^{t} \\mathrm{Tr}(H_{s}H_{s}^{T}) \\mathrm{d}s$. For our $1 \\times m$ integrand, this is $\\int_{0}^{t} \\|H_{s}^{T}\\|_{\\mathbb{R}^{m}}^{2} \\mathrm{d}s$. Here, $H_{s}^{i} = \\frac{1}{N} \\nabla \\varphi(X_{s}^{i,N})^{T} \\sigma(X_{s}^{i,N}, \\mu_{s}^{N})$.\n$$ \\left\\langle \\int_{0}^{\\cdot} H_{s}^{i} \\mathrm{d}W_{s}^{i} \\right\\rangle_{t} = \\int_{0}^{t} \\left\\|\\left(\\frac{1}{N} \\nabla \\varphi(X_{s}^{i,N})^{T} \\sigma(X_{s}^{i,N}, \\mu_{s}^{N})\\right)^{T}\\right\\|_{\\mathbb{R}^{m}}^{2} \\mathrm{d}s = \\frac{1}{N^{2}} \\int_{0}^{t} \\left\\| \\sigma(X_{s}^{i,N}, \\mu_{s}^{N})^{T} \\nabla \\varphi(X_{s}^{i,N}) \\right\\|_{\\mathbb{R}^{m}}^{2} \\mathrm{d}s $$\nSumming over $i$ from $1$ to $N$:\n$$ \\langle M^{N}(\\varphi)\\rangle_{t} = \\sum_{i=1}^{N} \\frac{1}{N^{2}} \\int_{0}^{t} \\left\\| \\sigma(X_{s}^{i,N}, \\mu_{s}^{N})^{T} \\nabla \\varphi(X_{s}^{i,N}) \\right\\|_{\\mathbb{R}^{m}}^{2} \\mathrm{d}s $$\nBy Fubini's theorem, we can swap the sum and the integral:\n$$ \\langle M^{N}(\\varphi)\\rangle_{t} = \\frac{1}{N} \\int_{0}^{t} \\left( \\frac{1}{N} \\sum_{i=1}^{N} \\left\\| \\sigma(X_{s}^{i,N}, \\mu_{s}^{N})^{T} \\nabla \\varphi(X_{s}^{i,N}) \\right\\|_{\\mathbb{R}^{m}}^{2} \\right) \\mathrm{d}s $$\nThe term inside the parenthesis can be written as an integral with respect to the empirical measure $\\mu_{s}^{N}$:\n$$ \\langle M^{N}(\\varphi)\\rangle_{t} = \\frac{1}{N} \\int_{0}^{t} \\int_{\\mathbb{R}^{d}} \\left\\| \\sigma(x, \\mu_{s}^{N})^{T} \\nabla \\varphi(x) \\right\\|_{\\mathbb{R}^{m}}^{2} \\mu_{s}^{N}(\\mathrm{d}x) \\mathrm{d}s $$\nThis is the explicit expression for the predictable quadratic variation.\n\nTo find the limit as $N \\to \\infty$, we use the boundedness assumptions. We are given that there exists a constant $\\Sigma \\in (0, \\infty)$ such that $\\sup_{x,\\mu} \\|\\sigma(x,\\mu)\\|_{\\mathrm{op}} \\le \\Sigma$, and that $\\varphi \\in C_{b}^{2}(\\mathbb{R}^{d})$, which implies that its gradient is bounded, i.e., $\\|\\nabla \\varphi\\|_{\\infty} := \\sup_{x \\in \\mathbb{R}^{d}} \\|\\nabla \\varphi(x)\\| < \\infty$.\nWe bound the squared norm in the integrand:\n$$ \\left\\| \\sigma(x, \\mu_{s}^{N})^{T} \\nabla \\varphi(x) \\right\\|_{\\mathbb{R}^{m}} \\le \\left\\| \\sigma(x, \\mu_{s}^{N})^{T} \\right\\|_{\\mathrm{op}} \\|\\nabla \\varphi(x)\\| $$\nUsing the property that $\\|\\sigma^{T}\\|_{\\mathrm{op}} = \\|\\sigma\\|_{\\mathrm{op}}$, we have:\n$$ \\left\\| \\sigma(x, \\mu_{s}^{N})^{T} \\nabla \\varphi(x) \\right\\|_{\\mathbb{R}^{m}} \\le \\left\\| \\sigma(x, \\mu_{s}^{N}) \\right\\|_{\\mathrm{op}} \\|\\nabla \\varphi(x)\\| \\le \\Sigma \\cdot \\|\\nabla \\varphi\\|_{\\infty} $$\nSquaring both sides gives a uniform bound on the integrand:\n$$ \\left\\| \\sigma(x, \\mu_{s}^{N})^{T} \\nabla \\varphi(x) \\right\\|_{\\mathbb{R}^{m}}^{2} \\le \\Sigma^{2} \\|\\nabla \\varphi\\|_{\\infty}^{2} $$\nNow we apply this bound to the inner integral of the quadratic variation expression:\n$$ \\int_{\\mathbb{R}^{d}} \\left\\| \\sigma(x, \\mu_{s}^{N})^{T} \\nabla \\varphi(x) \\right\\|_{\\mathbb{R}^{m}}^{2} \\mu_{s}^{N}(\\mathrm{d}x) \\le \\int_{\\mathbb{R}^{d}} \\Sigma^{2} \\|\\nabla \\varphi\\|_{\\infty}^{2} \\mu_{s}^{N}(\\mathrm{d}x) $$\nSince $\\mu_{s}^{N}$ is a probability measure, $\\int_{\\mathbb{R}^{d}} \\mu_{s}^{N}(\\mathrm{d}x) = 1$. Therefore:\n$$ \\int_{\\mathbb{R}^{d}} \\left\\| \\sigma(x, \\mu_{s}^{N})^{T} \\nabla \\varphi(x) \\right\\|_{\\mathbb{R}^{m}}^{2} \\mu_{s}^{N}(\\mathrm{d}x) \\le \\Sigma^{2} \\|\\nabla \\varphi\\|_{\\infty}^{2} $$\nThis provides a uniform bound for the integrand of the time integral. Substituting this back into the expression for $\\langle M^{N}(\\varphi)\\rangle_{t}$:\n$$ 0 \\le \\langle M^{N}(\\varphi)\\rangle_{t} \\le \\frac{1}{N} \\int_{0}^{t} \\Sigma^{2} \\|\\nabla \\varphi\\|_{\\infty}^{2} \\mathrm{d}s = \\frac{t \\Sigma^{2} \\|\\nabla \\varphi\\|_{\\infty}^{2}}{N} $$\nFor any fixed $t \\in [0,T]$, the upper bound $\\frac{t \\Sigma^{2} \\|\\nabla \\varphi\\|_{\\infty}^{2}}{N}$ is a finite quantity divided by $N$. As $N \\to \\infty$, this upper bound tends to $0$.\n$$ \\lim_{N \\to \\infty} \\frac{t \\Sigma^{2} \\|\\nabla \\varphi\\|_{\\infty}^{2}}{N} = 0 $$\nBy the Squeeze Theorem, since $\\langle M^{N}(\\varphi)\\rangle_{t}$ is always non-negative, its limit as $N \\to \\infty$ must be $0$.\n$$ \\lim_{N \\to \\infty} \\langle M^{N}(\\varphi) \\rangle_{t} = 0 $$\nThis result demonstrates that the variance of the martingale term vanishes in the large particle limit, which is a key step in proving the propagation of chaos, implying that the evolution of the empirical measure converges to a deterministic limit described by the McKean-Vlasov equation.", "answer": "$$\\boxed{0}$$", "id": "2991706"}, {"introduction": "Beyond establishing convergence, a crucial question in the study of particle approximations is quantifying the rate of this convergence. This practice introduces a powerful technique that connects the error of the particle system to the spectral properties of the underlying dynamics via functional inequalities. You will first derive the sharp Poincaré constant for a Gaussian invariant measure and then use this result to obtain an explicit bound on the mean-square error for Lipschitz observables, demonstrating how the system's parameters govern the efficiency of the approximation [@problem_id:2991714].", "problem": "Consider the one-dimensional mean-field particle system in equilibrium governed by the Stochastic Differential Equation (SDE)\n$$\ndX_{t}^{i,N} \\;=\\; -\\frac{1}{\\sigma^{2}}\\,X_{t}^{i,N}\\,dt \\;-\\; \\theta\\bigl(X_{t}^{i,N} - \\bar{X}_{t}^{N}\\bigr)\\,dt \\;+\\; \\sqrt{2}\\,dB_{t}^{i}, \\quad i=1,\\dots,N,\n$$\nwhere $\\sigma>0$, $\\theta\\ge 0$, $\\bar{X}_{t}^{N} := \\frac{1}{N}\\sum_{j=1}^{N} X_{t}^{j,N}$, and $\\{B_{t}^{i}\\}_{i=1}^{N}$ are independent standard Brownian motions. The McKean–Vlasov limit is the single-particle SDE\n$$\ndX_{t} \\;=\\; -\\frac{1}{\\sigma^{2}}\\,X_{t}\\,dt \\;-\\; \\theta\\bigl(X_{t}-m_{t}\\bigr)\\,dt \\;+\\; \\sqrt{2}\\,dB_{t},\n$$\nwith $m_{t} := \\mathbb{E}[X_{t}]$. At stationarity one has $m_{t}=0$ and the invariant law $\\mu$ of $X_{t}$ is Gaussian with density\n$$\n\\mu(dx) \\;\\propto\\; \\exp\\!\\Bigl(-\\frac{1}{2}\\Bigl(\\frac{1}{\\sigma^{2}}+\\theta\\Bigr)x^{2}\\Bigr)\\,dx,\n$$\nthat is, $\\mu = \\mathcal{N}\\!\\bigl(0,\\sigma_{\\mathrm{eff}}^{2}\\bigr)$ with $\\sigma_{\\mathrm{eff}}^{2} := \\bigl(\\frac{1}{\\sigma^{2}}+\\theta\\bigr)^{-1}$.\n\nYou are asked to connect functional inequalities to the quantitative propagation-of-chaos error for particle approximations at equilibrium. Proceed as follows.\n\n1. Starting from the definition of the Poincaré inequality for a probability measure $\\mu$ on $\\mathbb{R}$,\n$$\n\\mathrm{Var}_{\\mu}(f) \\;\\le\\; C_{P} \\int_{\\mathbb{R}} |f'(x)|^{2}\\,\\mu(dx),\n$$\nderive the exact Poincaré constant $C_{P}$ for the Gaussian invariant law $\\mu = \\mathcal{N}\\!\\bigl(0,\\sigma_{\\mathrm{eff}}^{2}\\bigr)$ associated with the stationary single-particle dynamics above. Your derivation must start from first principles (e.g., the generator and spectral properties of the Ornstein–Uhlenbeck semigroup or the calculus of variations for the Rayleigh quotient) and justify why your computed $C_{P}$ is sharp.\n\n2. Let $f:\\mathbb{R}\\to\\mathbb{R}$ be a Lipschitz test function with Lipschitz constant $L$, that is, $|f(x)-f(y)| \\le L|x-y|$ for all $x,y\\in\\mathbb{R}$. Consider the equilibrium chaos error in mean square for the empirical approximation built from $N$ identically distributed draws from $\\mu$,\n$$\n\\mathbb{E}\\Biggl[\\Biggl(\\frac{1}{N}\\sum_{i=1}^{N} f(X^{i}) \\;-\\; \\int_{\\mathbb{R}} f\\,d\\mu\\Biggr)^{2}\\Biggr],\n$$\nwhere $X^{1},\\dots,X^{N}\\stackrel{\\mathrm{iid}}{\\sim}\\mu$. Using your $C_{P}$ from part 1 and only fundamental facts about variance of independent averages, derive a bound of the form\n$$\n\\mathbb{E}\\Biggl[\\Biggl(\\frac{1}{N}\\sum_{i=1}^{N} f(X^{i}) \\;-\\; \\int_{\\mathbb{R}} f\\,d\\mu\\Biggr)^{2}\\Biggr] \\;\\le\\; \\frac{C(\\sigma,\\theta,L)}{N},\n$$\nand compute the smallest possible constant $C(\\sigma,\\theta,L)$ expressible solely in terms of $\\sigma$, $\\theta$, and $L$.\n\nProvide your final result as a single closed-form analytic expression for $C(\\sigma,\\theta,L)/N$. No rounding is required, and no units are involved.", "solution": "We begin from first principles. The stationary single-particle law $\\mu$ has density\n$$\n\\rho(x) \\;=\\; Z^{-1}\\exp\\!\\Bigl(-\\frac{a}{2}x^{2}\\Bigr), \\qquad a \\;:=\\; \\frac{1}{\\sigma^{2}}+\\theta, \\quad Z>0,\n$$\nso $\\mu$ is the centered Gaussian with variance $\\sigma_{\\mathrm{eff}}^{2} = a^{-1}$. The generator of the corresponding Ornstein–Uhlenbeck semigroup acting on sufficiently smooth functions $f:\\mathbb{R}\\to\\mathbb{R}$ is\n$$\n\\mathcal{L}f(x) \\;=\\; f''(x) - a\\,x\\,f'(x).\n$$\nThis operator is symmetric in $L^{2}(\\mu)$ and has discrete spectrum with eigenvalues $0, a, 2a, \\dots$ and associated orthogonal eigenfunctions given by the Hermite polynomials (appropriately normalized). In particular, the smallest nonzero eigenvalue of $-\\mathcal{L}$, called the spectral gap, equals $a$. The Poincaré inequality for $\\mu$ is equivalent to the spectral gap inequality:\n$$\n\\mathrm{Var}_{\\mu}(f) \\;\\le\\; \\frac{1}{\\lambda_{1}} \\int |f'(x)|^{2}\\, \\mu(dx),\n$$\nwhere $\\lambda_{1}$ is the smallest positive eigenvalue of $-\\mathcal{L}$, here $\\lambda_{1} = a$. Therefore the sharp Poincaré constant is\n$$\nC_{P} \\;=\\; \\frac{1}{a} \\;=\\; \\frac{1}{\\frac{1}{\\sigma^{2}}+\\theta} \\;=\\; \\sigma_{\\mathrm{eff}}^{2}.\n$$\nAn alternative derivation uses the calculus of variations for the Rayleigh quotient. For functions $g$ with $\\int g\\,d\\mu=0$, consider\n$$\n\\mathcal{R}[g] \\;:=\\; \\frac{\\int g(x)^{2}\\,\\mu(dx)}{\\int |g'(x)|^{2}\\,\\mu(dx)}.\n$$\nThe optimal constant $C_{P}$ is the supremum of $\\mathcal{R}[g]$ over smooth zero-mean $g$. The Euler–Lagrange equation for extremizers yields the Sturm–Liouville problem\n$$\n-\\frac{d}{dx}\\Bigl(\\rho(x)\\,g'(x)\\Bigr) \\;=\\; \\lambda \\,\\rho(x)\\,g(x),\n$$\nwhose solutions for Gaussian $\\rho$ are Hermite functions with eigenvalues $\\lambda = k a$, $k\\in\\mathbb{N}$. The largest Rayleigh quotient corresponds to the smallest positive eigenvalue, $\\lambda_{1}=a$, confirming $C_{P} = 1/a$ and sharpness.\n\nWe now bound the equilibrium chaos error for Lipschitz observables via the Poincaré inequality. Let $f$ be Lipschitz with constant $L$. Rademacher’s theorem implies $f$ is almost everywhere differentiable with $|f'(x)| \\le L$ almost everywhere. Therefore,\n$$\n\\mathrm{Var}_{\\mu}(f) \\;\\le\\; C_{P} \\int |f'(x)|^{2}\\,\\mu(dx) \\;\\le\\; C_{P}\\,L^{2}.\n$$\nIf $X^{1},\\dots,X^{N}$ are independent and identically distributed with law $\\mu$, then\n$$\n\\mathbb{E}\\Biggl[\\Biggl(\\frac{1}{N}\\sum_{i=1}^{N} f(X^{i}) \\;-\\; \\int f\\,d\\mu\\Biggr)^{2}\\Biggr]\n\\;=\\; \\frac{1}{N}\\,\\mathrm{Var}_{\\mu}(f),\n$$\nby the additivity of variance for independent averages. Combining these yields\n$$\n\\mathbb{E}\\Biggl[\\Biggl(\\frac{1}{N}\\sum_{i=1}^{N} f(X^{i}) \\;-\\; \\int f\\,d\\mu\\Biggr)^{2}\\Biggr]\n\\;\\le\\; \\frac{C_{P}\\,L^{2}}{N}.\n$$\nSince $C_{P}$ is sharp for the Gaussian law, this bound is minimized by taking $C_{P} = \\sigma_{\\mathrm{eff}}^{2} = \\bigl(\\frac{1}{\\sigma^{2}}+\\theta\\bigr)^{-1}$. Hence the smallest possible constant depending only on $\\sigma$, $\\theta$, and $L$ is\n$$\nC(\\sigma,\\theta,L) \\;=\\; \\frac{L^{2}}{\\frac{1}{\\sigma^{2}}+\\theta}.\n$$\nTherefore, the tight bound constant appearing in the equilibrium chaos error is\n$$\n\\frac{C(\\sigma,\\theta,L)}{N} \\;=\\; \\frac{L^{2}}{N\\bigl(\\frac{1}{\\sigma^{2}}+\\theta\\bigr)}.\n$$\nThis bound quantifies the leading-order $\\frac{1}{N}$ mean-square chaos error for Lipschitz observables in terms of the Poincaré constant of the Gaussian invariant law derived from the strongly convex effective potential.", "answer": "$$\\boxed{\\frac{L^{2}}{N\\left(\\frac{1}{\\sigma^{2}}+\\theta\\right)}}$$", "id": "2991714"}, {"introduction": "The principle of \"propagation of chaos\" suggests that initial intricacies in an interacting particle system wash out over time, leading to asymptotic independence. However, this is not always true; its validity depends on the system's underlying structure and initial state. This insightful thought experiment challenges you to explore a scenario a system with a carefully constructed initial correlation and a conserved quantity—total momentum—preserves this correlation indefinitely, preventing the onset of chaos. This practice forces a deeper reflection on the essential assumptions that underpin mean-field limits and highlights the importance of conserved quantities in shaping macroscopic behavior [@problem_id:2991746].", "problem": "Consider an interacting system of $N$ particles with scalar velocities $\\{V_{i}^{N}(t)\\}_{i=1}^{N}$, evolving in continuous time by binary collisions that conserve total momentum. Collisions occur as follows: for each unordered pair $(i,j)$, collisions are triggered by a Poisson process of rate $\\beta/N$, independently across pairs. At a collision time between indices $i$ and $j$, the pair updates according to\n$$\nV_{i}^{N}(t^{+}) \\;=\\; \\frac{V_{i}^{N}(t^{-}) + V_{j}^{N}(t^{-})}{2} \\;+\\; \\eta, \n\\qquad\nV_{j}^{N}(t^{+}) \\;=\\; \\frac{V_{i}^{N}(t^{-}) + V_{j}^{N}(t^{-})}{2} \\;-\\; \\eta,\n$$\nwhere $\\eta$ is an independent draw from a fixed symmetric distribution with mean $0$ and finite variance $\\sigma_{\\eta}^{2}$, independent of all prior randomness and of all other pairs. This rule preserves the total momentum $\\sum_{k=1}^{N} V_{k}^{N}(t)$ across each collision. Between collisions, the velocities remain constant.\n\nTo probe limitations of molecular chaos and propagation of chaos, initialize the system with a correlated exchangeable law by introducing a common latent variable. Specifically, let\n$$\nV_{i}^{N}(0) \\;=\\; U \\;+\\; \\xi_{i}, \\quad i=1,\\dots,N,\n$$\nwhere $U$ is a real-valued random variable with $\\mathbb{E}[U]=0$ and $\\operatorname{Var}(U)=\\sigma_{U}^{2}$, and $\\{\\xi_{i}\\}_{i=1}^{N}$ are independent and identically distributed with $\\mathbb{E}[\\xi_{i}]=0$ and $\\operatorname{Var}(\\xi_{i})=\\sigma_{\\xi}^{2}$, independent of $U$. The collision kernel above imposes only momentum conservation and exchangeability; no further structure is assumed.\n\nStarting from fundamental definitions of exchangeability, covariance, and conservation laws, and without invoking any pre-packaged propagation-of-chaos theorems, determine the closed-form analytic expression for the large-$N$ limit of the pairwise covariance at any fixed time $t\\ge 0$,\n$$\n\\lim_{N\\to\\infty} \\operatorname{Cov}\\!\\big(V_{1}^{N}(t),\\, V_{2}^{N}(t)\\big),\n$$\nin terms of the model parameters specified above. Your final answer must be a single closed-form analytic expression. No rounding is required, and no units apply.", "solution": "The problem requires the determination of the large-$N$ limit of the pairwise covariance of particle velocities, $\\lim_{N\\to\\infty} \\operatorname{Cov}(V_{1}^{N}(t), V_{2}^{N}(t))$, for a given interacting particle system. The solution will be derived from fundamental principles without recourse to established theorems on propagation of chaos.\n\nFirst, let us define the key statistical quantities for the finite-$N$ system. Due to the exchangeability of the initial conditions and the collision dynamics, the statistical properties of all particles are identical. Let $S_N(t)$ be the variance of a single particle's velocity and $C_N(t)$ be the covariance between the velocities of any two distinct particles at time $t$.\n$$\nS_N(t) = \\operatorname{Var}(V_{i}^{N}(t))\n$$\n$$\nC_N(t) = \\operatorname{Cov}(V_{i}^{N}(t), V_{j}^{N}(t)) \\quad \\text{for } i \\neq j\n$$\nThe initial conditions are $V_{i}^{N}(0) = U + \\xi_{i}$, where $U$ and $\\{\\xi_i\\}$ are independent random variables with zero mean. Thus, the mean velocity at time $t=0$ is $\\mathbb{E}[V_{i}^{N}(0)] = \\mathbb{E}[U] + \\mathbb{E}[\\xi_i] = 0 + 0 = 0$. The collision rule is\n$$\nV_{i}^{N}(t^{+}) = \\frac{V_{i}^{N}(t^{-}) + V_{j}^{N}(t^{-})}{2} + \\eta\n$$\n$$\nV_{j}^{N}(t^{+}) = \\frac{V_{i}^{N}(t^{-}) + V_{j}^{N}(t^{-})}{2} - \\eta\n$$\nwhere $\\mathbb{E}[\\eta] = 0$. The expectation of the post-collision velocities, conditioned on the pre-collision state, is $\\mathbb{E}[V_{i}^{N}(t^{+}) | \\mathcal{F}_{t^{-}}] = \\frac{1}{2}(V_i^N(t^-) + V_j^N(t^-))$. Taking a full expectation, we find that if $\\mathbb{E}[V_k^N(t^-)]=0$ for all $k$, then $\\mathbb{E}[V_i^N(t^+)]=0$. Since the means are zero at $t=0$, they remain zero for all $t \\ge 0$.\nTherefore, $\\mathbb{E}[V_{i}^{N}(t)] = 0$ for all $i$ and $t$, which simplifies the variance and covariance expressions:\n$$\nS_N(t) = \\mathbb{E}[(V_{i}^{N}(t))^2] \\quad \\text{and} \\quad C_N(t) = \\mathbb{E}[V_{i}^{N}(t) V_{j}^{N}(t)] \\text{ for } i \\neq j.\n$$\nWe will derive a system of ordinary differential equations for $S_N(t)$ and $C_N(t)$. The evolution of the expectation of any function $f$ of the state is governed by $\\frac{d}{dt}\\mathbb{E}[f] = \\mathbb{E}[\\mathcal{L}f]$, where $\\mathcal{L}$ is the generator of the Markov process. The generator is given by\n$$\n\\mathcal{L}f(v_1, \\dots, v_N) = \\sum_{1 \\le i < j \\le N} \\frac{\\beta}{N} \\mathbb{E}_{\\eta}[f(\\dots, v_i', \\dots, v_j', \\dots) - f(\\dots, v_i, \\dots, v_j, \\dots)]\n$$\nwhere $(v_i', v_j')$ are the post-collision velocities for the pair $(i, j)$.\n\nLet's derive the equation for $S_N(t) = \\mathbb{E}[(V_1^N)^2]$. The velocity $V_1^N$ changes only when particle $1$ collides with another particle $k \\in \\{2, \\dots, N\\}$.\n$$\n\\frac{dS_N(t)}{dt} = \\mathbb{E}[\\mathcal{L}(V_1^2)] = \\mathbb{E}\\left[ \\sum_{k=2}^{N} \\frac{\\beta}{N} \\mathbb{E}_{\\eta}[(V_1'(k))^2 - V_1^2] \\right]\n$$\nwhere $V_1'(k)$ is the velocity of particle $1$ after colliding with particle $k$.\nFor any such collision, $\\mathbb{E}_{\\eta}[(V_1'(k))^2] = \\mathbb{E}_{\\eta}[(\\frac{V_1+V_k}{2} + \\eta)^2] = (\\frac{V_1+V_k}{2})^2 + \\mathbb{E}[\\eta^2] = \\frac{V_1^2+V_k^2+2V_1V_k}{4} + \\sigma_{\\eta}^2$.\nThe change in expectation given the state is $\\mathbb{E}_{\\eta}[(V_1'(k))^2 - V_1^2] = \\frac{V_1^2+V_k^2+2V_1V_k}{4} + \\sigma_{\\eta}^2 - V_1^2 = \\frac{-3V_1^2+V_k^2+2V_1V_k}{4} + \\sigma_{\\eta}^2$.\nTaking the full expectation and summing over the $N-1$ possible collision partners for particle $1$:\n$$\n\\frac{dS_N(t)}{dt} = \\sum_{k=2}^{N} \\frac{\\beta}{N} \\left( \\frac{-3\\mathbb{E}[V_1^2]+\\mathbb{E}[V_k^2]+2\\mathbb{E}[V_1V_k]}{4} + \\sigma_{\\eta}^2 \\right)\n$$\nUsing exchangeability, $\\mathbb{E}[V_k^2] = S_N(t)$ and $\\mathbb{E}[V_1V_k] = C_N(t)$ for $k \\ne 1$.\n$$\n\\frac{dS_N(t)}{dt} = (N-1) \\frac{\\beta}{N} \\left( \\frac{-3S_N(t)+S_N(t)+2C_N(t)}{4} + \\sigma_{\\eta}^2 \\right) = \\frac{(N-1)\\beta}{N} \\left( \\frac{C_N(t)-S_N(t)}{2} + \\sigma_{\\eta}^2 \\right).\n$$\nNow, let's derive the equation for $C_N(t) = \\mathbb{E}[V_1^N V_2^N]$. The product $V_1^N V_2^N$ changes if particle $1$ or $2$ is involved in a collision. We apply the generator $\\mathcal{L}$ to the function $f = V_1V_2$.\n$$\n\\frac{dC_N(t)}{dt} = \\mathbb{E}[\\mathcal{L}(V_1V_2)] = \\mathbb{E}\\left[ \\sum_{1 \\le i < j \\le N} \\frac{\\beta}{N} \\mathbb{E}_{\\eta}[V_1'V_2' - V_1V_2] \\right]\n$$\nThe terms in the sum are non-zero only if $\\{i,j\\} \\cap \\{1,2\\} \\neq \\emptyset$.\n1. Case $(i,j)=(1,2)$: The change is $\\mathbb{E}_{\\eta}[(\\frac{V_1+V_2}{2}+\\eta)(\\frac{V_1+V_2}{2}-\\eta) - V_1V_2] = (\\frac{V_1+V_2}{2})^2-\\sigma_{\\eta}^2 - V_1V_2 = \\frac{V_1^2+V_2^2-2V_1V_2}{4} - \\sigma_{\\eta}^2$.\n2. Case $i=1, j=k>2$: The change is $\\mathbb{E}_{\\eta}[(\\frac{V_1+V_k}{2}+\\eta)V_2 - V_1V_2] = \\frac{V_1V_2+V_kV_2}{2} - V_1V_2 = \\frac{V_kV_2-V_1V_2}{2}$. There are $N-2$ such terms.\n3. Case $i=2, j=k>2$: The change is $\\mathbb{E}_{\\eta}[V_1(\\frac{V_2+V_k}{2}+\\eta) - V_1V_2] = \\frac{V_1V_2+V_1V_k}{2} - V_1V_2 = \\frac{V_1V_k-V_1V_2}{2}$. There are $N-2$ such terms.\n\nTaking full expectation:\n$$\n\\frac{dC_N(t)}{dt} = \\frac{\\beta}{N}\\left(\\frac{S_N+S_N-2C_N}{4}-\\sigma_{\\eta}^2\\right) + \\sum_{k=3}^{N} \\frac{\\beta}{N}\\left(\\frac{C_N-C_N}{2}\\right) + \\sum_{k=3}^{N} \\frac{\\beta}{N}\\left(\\frac{C_N-C_N}{2}\\right)\n$$\nThe sums are zero. Thus,\n$$\n\\frac{dC_N(t)}{dt} = \\frac{\\beta}{N} \\left( \\frac{S_N(t)-C_N(t)}{2} - \\sigma_{\\eta}^2 \\right).\n$$\nThe problem states that total momentum is conserved. This implies that the total variance of the system's momentum is constant.\n$\\operatorname{Var}\\left(\\sum_i V_i^N(t)\\right) = \\operatorname{Var}\\left(\\sum_i V_i^N(0)\\right)$.\n$\\operatorname{Var}\\left(\\sum_i V_i^N(t)\\right) = \\sum_i \\operatorname{Var}(V_i^N) + \\sum_{i \\neq j} \\operatorname{Cov}(V_i^N, V_j^N) = N S_N(t) + N(N-1)C_N(t)$.\nAt $t=0$, $V_i^N(0) = U+\\xi_i$.\n$\\operatorname{Var}\\left(\\sum_i V_i^N(0)\\right) = \\operatorname{Var}\\left(\\sum_i (U+\\xi_i)\\right) = \\operatorname{Var}(NU + \\sum_i \\xi_i) = N^2\\operatorname{Var}(U) + \\sum_i\\operatorname{Var}(\\xi_i) = N^2\\sigma_U^2 + N\\sigma_\\xi^2$.\nEquating these gives the invariant: $N S_N(t) + N(N-1)C_N(t) = N^2\\sigma_U^2 + N\\sigma_\\xi^2$, which simplifies to:\n$$\nS_N(t) + (N-1)C_N(t) = N\\sigma_U^2 + \\sigma_\\xi^2.\n$$\nWe can use this algebraic relation to solve the system. From the invariant, $S_N(t) = N\\sigma_U^2 + \\sigma_\\xi^2 - (N-1)C_N(t)$. Substituting this into the ODE for $C_N(t)$:\n$$\n\\frac{dC_N(t)}{dt} = \\frac{\\beta}{N} \\left( \\frac{(N\\sigma_U^2 + \\sigma_\\xi^2 - (N-1)C_N(t)) - C_N(t)}{2} - \\sigma_{\\eta}^2 \\right)\n$$\n$$\n\\frac{dC_N(t)}{dt} = \\frac{\\beta}{N} \\left( \\frac{N\\sigma_U^2 + \\sigma_\\xi^2 - NC_N(t)}{2} - \\sigma_{\\eta}^2 \\right)\n$$\n$$\n\\frac{dC_N(t)}{dt} = \\frac{\\beta}{2}\\left(\\sigma_U^2 + \\frac{\\sigma_\\xi^2}{N} - C_N(t)\\right) - \\frac{\\beta\\sigma_\\eta^2}{N}\n$$\nThis ODE can be rearranged as:\n$$\n\\frac{dC_N(t)}{dt} + \\frac{\\beta}{2}C_N(t) = \\frac{\\beta}{2}\\sigma_U^2 + \\frac{\\beta}{2N}(\\sigma_\\xi^2 - 2\\sigma_\\eta^2).\n$$\nWe are interested in the limit $C(t) = \\lim_{N\\to\\infty} C_N(t)$. Assuming the limit and its derivative exist, we can take the limit of the ODE:\n$$\n\\frac{dC(t)}{dt} + \\frac{\\beta}{2}C(t) = \\lim_{N\\to\\infty} \\left( \\frac{\\beta}{2}\\sigma_U^2 + \\frac{\\beta}{2N}(\\sigma_\\xi^2 - 2\\sigma_\\eta^2) \\right) = \\frac{\\beta}{2}\\sigma_U^2.\n$$\nThe initial condition for this limiting ODE is $C(0) = \\lim_{N\\to\\infty} C_N(0)$. We calculate $C_N(0)$:\n$$\nC_N(0) = \\operatorname{Cov}(V_1^N(0), V_2^N(0)) = \\operatorname{Cov}(U+\\xi_1, U+\\xi_2).\n$$\nSince $U$, $\\xi_1$, and $\\xi_2$ are mutually independent,\n$$\nC_N(0) = \\operatorname{Cov}(U,U) + \\operatorname{Cov}(U,\\xi_2) + \\operatorname{Cov}(\\xi_1,U) + \\operatorname{Cov}(\\xi_1,\\xi_2) = \\operatorname{Var}(U) + 0 + 0 + 0 = \\sigma_U^2.\n$$\nThe initial condition $C(0)$ is therefore $\\sigma_U^2$.\n\nWe must now solve the linear first-order ODE $\\frac{dC(t)}{dt} + \\frac{\\beta}{2}C(t) = \\frac{\\beta}{2}\\sigma_U^2$ with the initial condition $C(0)=\\sigma_U^2$.\nThis equation has a particular solution $C_p(t) = \\sigma_U^2$. Let's verify: $0 + \\frac{\\beta}{2}\\sigma_U^2 = \\frac{\\beta}{2}\\sigma_U^2$. The general solution is $C(t) = A\\exp(-\\frac{\\beta}{2}t) + \\sigma_U^2$. Applying the initial condition:\n$C(0) = A + \\sigma_U^2 = \\sigma_U^2$, which implies $A=0$.\nThus, the unique solution is $C(t) = \\sigma_U^2$.\n\nThe covariance is constant for all time $t \\ge 0$. The initial correlation, sourced by the common latent variable $U$, is preserved by the dynamics because it is tied to the total momentum of the system, which is a conserved quantity. The interactions merely redistribute momentum among particles but cannot erase this global correlation.", "answer": "$$\n\\boxed{\\sigma_{U}^{2}}\n$$", "id": "2991746"}]}