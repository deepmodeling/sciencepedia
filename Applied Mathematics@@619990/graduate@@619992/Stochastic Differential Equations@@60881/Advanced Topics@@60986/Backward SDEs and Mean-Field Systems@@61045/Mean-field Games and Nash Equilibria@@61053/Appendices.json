{"hands_on_practices": [{"introduction": "The Linear-Quadratic (LQ) framework is a cornerstone of both classical optimal control and modern mean-field game theory due to its tractability and wide applicability. This first exercise guides you through the essential steps of solving a representative agent's problem within an LQ-MFG. By applying the dynamic programming principle via the Hamilton-Jacobi-Bellman (HJB) equation, you will derive the optimal feedback control, a fundamental skill for analyzing continuous-state games [@problem_id:2987064].", "problem": "Consider a linear-quadratic mean-field game with a continuum of identical agents. For a representative agent, the controlled state process $(X_t)_{t \\in [0,T]}$ evolves according to the stochastic differential equation\n$$\n\\mathrm{d}X_t = \\left(a X_t + b \\alpha_t + c m_t\\right)\\mathrm{d}t + \\sigma \\mathrm{d}W_t,\n$$\nwhere $a$, $b$, $c$, and $\\sigma$ are real constants, $(W_t)_{t \\in [0,T]}$ is a standard Brownian motion, $\\alpha_t$ is the agent’s control, and $m_t$ denotes the population mean $m_t = \\mathbb{E}[X_t]$. The representative agent seeks to minimize the expected quadratic cost\n$$\nJ(\\alpha) = \\mathbb{E}\\Bigg[\\int_{0}^{T} \\left(q X_t^{2} + \\bar{q}\\left(X_t - m_t\\right)^{2} + r \\alpha_t^{2}\\right)\\mathrm{d}t + g X_T^{2} + \\bar{g}\\left(X_T - m_T\\right)^{2}\\Bigg],\n$$\nwith given constants $q>0$, $\\bar{q}\\geq 0$, $r>0$, $g\\geq 0$, and $\\bar{g}\\geq 0$. Assume that the mean field $(m_t)_{t\\in[0,T]}$ is a given deterministic function of time when solving the representative agent’s problem.\n\nStarting from the dynamic programming principle and the Hamilton–Jacobi–Bellman (HJB) equation, use an ansatz for the value function of the form\n$$\nV(t,x) = \\frac{1}{2}P_t x^{2} + S_t m_t x + \\frac{1}{2}U_t m_t^{2} + \\phi_t,\n$$\nwhere $(P_t,S_t,U_t,\\phi_t)$ are deterministic coefficient functions of time chosen so that $V$ solves the HJB equation with terminal condition $V(T,x) = g x^{2} + \\bar{g}\\left(x - m_T\\right)^{2}$. By minimizing the Hamiltonian in the HJB equation with respect to the control, derive the feedback control $\\alpha_t$ in terms of the functions $P_t$ and $S_t$. You may use that the coefficient matching in the HJB equation yields a coupled Riccati system for $(P_t,S_t)$, but you should not solve that system explicitly. Express your final answer as a single closed-form analytic expression for $\\alpha_t$ in terms of $b$, $r$, $P_t$, $S_t$, $X_t$, and $m_t$. No numerical evaluation is required.", "solution": "The problem is valid as it presents a standard, well-posed problem in the field of mean-field game theory, specifically a linear-quadratic (LQ) model. All parameters and objectives are clearly defined, and the premises are scientifically sound within the context of stochastic optimal control.\n\nThe objective is to find the optimal feedback control $\\alpha_t$ for a representative agent. The agent's state $X_t$ follows the stochastic differential equation (SDE):\n$$\n\\mathrm{d}X_t = \\left(a X_t + b \\alpha_t + c m_t\\right)\\mathrm{d}t + \\sigma \\mathrm{d}W_t\n$$\nThe agent seeks to minimize a quadratic cost functional $J(\\alpha)$. In the mean-field game framework, the first step is to solve the representative agent's optimization problem, assuming the mean-field term $m_t = \\mathbb{E}[X_t]$ is a known deterministic function of time.\n\nThe value function for the agent's problem is defined as:\n$$\nV(t,x) = \\min_{\\alpha} \\mathbb{E}\\Bigg[\\int_{t}^{T} \\left(q X_s^{2} + \\bar{q}\\left(X_s - m_s\\right)^{2} + r \\alpha_s^{2}\\right)\\mathrm{d}s + g X_T^{2} + \\bar{g}\\left(X_T - m_T\\right)^{2} \\Bigg| X_t=x \\Bigg]\n$$\nThis value function $V(t,x)$ must satisfy the Hamilton-Jacobi-Bellman (HJB) partial differential equation. The HJB equation is given by:\n$$\n-\\frac{\\partial V}{\\partial t}(t,x) = \\min_{\\alpha \\in \\mathbb{R}} \\left\\{ L(t,x,\\alpha,m_t) + \\mathcal{L}^\\alpha V(t,x) \\right\\}\n$$\nwhere $L(t,x,\\alpha,m_t)$ is the running cost and $\\mathcal{L}^\\alpha$ is the infinitesimal generator of the state process $X_t$ under control $\\alpha$.\n\nThe running cost is:\n$$\nL(t,x,\\alpha,m_t) = q x^{2} + \\bar{q}\\left(x - m_t\\right)^{2} + r \\alpha^{2}\n$$\nThe infinitesimal generator $\\mathcal{L}^\\alpha$ applied to the value function $V(t,x)$ is:\n$$\n\\mathcal{L}^\\alpha V(t,x) = \\left(a x + b \\alpha + c m_t\\right) \\frac{\\partial V}{\\partial x}(t,x) + \\frac{1}{2}\\sigma^2 \\frac{\\partial^2 V}{\\partial x^2}(t,x)\n$$\nThe term inside the minimization operator in the HJB equation is the Hamiltonian, $\\mathcal{H}$, which depends on the state $x$, time $t$, control $\\alpha$, and the partial derivatives of the value function, $V_x = \\frac{\\partial V}{\\partial x}$ and $V_{xx} = \\frac{\\partial^2 V}{\\partial x^2}$.\n$$\n\\mathcal{H}(t, x, \\alpha, V_x, V_{xx}) = q x^{2} + \\bar{q}\\left(x - m_t\\right)^{2} + r \\alpha^{2} + \\left(a x + b \\alpha + c m_t\\right) V_x + \\frac{1}{2}\\sigma^2 V_{xx}\n$$\nThe HJB equation can thus be written as:\n$$\n-\\frac{\\partial V}{\\partial t}(t,x) = \\min_{\\alpha \\in \\mathbb{R}} \\mathcal{H}(t, x, \\alpha, V_x, V_{xx})\n$$\nTo find the optimal control $\\alpha_t^*$, we must minimize the Hamiltonian with respect to $\\alpha$ for a fixed state $(t,x)$. We can rewrite the Hamiltonian to isolate the terms involving $\\alpha$:\n$$\n\\mathcal{H} = r \\alpha^{2} + b \\alpha V_x + \\left( q x^{2} + \\bar{q}\\left(x - m_t\\right)^{2} + (a x + c m_t) V_x + \\frac{1}{2}\\sigma^2 V_{xx} \\right)\n$$\nThis is a quadratic function of $\\alpha$. Since the parameter $r > 0$, the parabola opens upwards, and its minimum can be found by setting its derivative with respect to $\\alpha$ to zero.\n$$\n\\frac{\\partial \\mathcal{H}}{\\partial \\alpha} = 2r \\alpha + b V_x = 0\n$$\nSolving for $\\alpha$ gives the optimal control, denoted $\\alpha^*$, in terms of the value function's spatial derivative:\n$$\n\\alpha^*(t,x) = -\\frac{b}{2r} \\frac{\\partial V}{\\partial x}(t,x)\n$$\nThe problem provides an ansatz for the value function:\n$$\nV(t,x) = \\frac{1}{2}P_t x^{2} + S_t m_t x + \\frac{1}{2}U_t m_t^{2} + \\phi_t\n$$\nwhere $P_t$, $S_t$, $U_t$, and $\\phi_t$ are deterministic functions of time. We compute the partial derivative of this ansatz with respect to $x$:\n$$\n\\frac{\\partial V}{\\partial x}(t,x) = P_t x + S_t m_t\n$$\nSubstituting this expression into the formula for the optimal control $\\alpha^*(t,x)$ yields:\n$$\n\\alpha^*(t,x) = -\\frac{b}{2r} \\left(P_t x + S_t m_t\\right)\n$$\nThis is the optimal control in feedback form for a given state $x$ at time $t$. For the representative agent whose state at time $t$ is $X_t$, the optimal control $\\alpha_t$ is obtained by replacing the generic state variable $x$ with the random variable $X_t$:\n$$\n\\alpha_t = -\\frac{b}{2r} \\left(P_t X_t + S_t m_t\\right)\n$$\nThis expression provides the optimal control $\\alpha_t$ in terms of the state $X_t$, the mean field $m_t$, the problem parameters $b$ and $r$, and the coefficient functions $P_t$ and $S_t$ from the value function ansatz, as requested. The terminal condition $V(T,x) = g x^{2} + \\bar{g}\\left(x - m_T\\right)^{2}$ provides the terminal values for the coefficients, e.g., $P_T = 2(g+\\bar{g})$ and $S_T = -2\\bar{g}$, which would be used to solve the Riccati system for $P_t$ and $S_t$. However, solving this system is not required by the problem statement.", "answer": "$$\n\\boxed{-\\frac{b}{2r}(P_t X_t + S_t m_t)}\n$$", "id": "2987064"}, {"introduction": "Mean-field game theory extends far beyond continuous-state dynamics, offering powerful tools for systems where agents switch between a finite number of states. This practice explores such a scenario, using a two-state continuous-time Markov chain model with an ergodic, or long-run average, cost objective [@problem_id:2987138]. You will solve for the complete mean-field Nash equilibrium, including both the agents' optimal transition rates and the resulting stationary population distribution, providing a concrete understanding of equilibrium in a finite-state setting.", "problem": "Consider a Mean Field Game (MFG) in continuous time with a continuum of identical agents whose individual state is a node in the two-point set $\\{0,1\\}$. An agent at node $i \\in \\{0,1\\}$ chooses a transition rate $u_i \\in [0,\\bar{u}_i]$ to jump to node $1-i$. The population evolves according to a controlled Continuous-Time Markov Chain (CTMC) with generator $\\mathcal{L}^{u}$ given by $\\mathcal{L}^{u} f(i) = u_i \\big(f(1-i) - f(i)\\big)$ for any bounded function $f$, under a stationary Markov control $u = (u_0,u_1)$.\n\nAgents seek to minimize the ergodic (long-run average) cost. The instantaneous running cost at node $i$ is the linear congestion cost $\\gamma_i m_i$ plus the linear control cost $\\alpha_i u_i$, where $m_i$ is the stationary fraction of the population at node $i$. The parameters are\n$$\n\\gamma_0 = 0.6,\\quad \\gamma_1 = 1.4,\\quad \\alpha_0 = -1,\\quad \\alpha_1 = 0.5,\\quad \\bar{u}_0 = 3,\\quad \\bar{u}_1 = 2.\n$$\nA mean-field Nash equilibrium is a pair $(u^{\\ast},m^{\\ast})$ such that:\n- For the stationary distribution $m^{\\ast} = (m_0^{\\ast},m_1^{\\ast})$, the control $u^{\\ast}$ minimizes the agent’s ergodic Hamilton–Jacobi–Bellman (HJB) equations with value function $V = (V_0,V_1)$ and ergodic constant $\\lambda$.\n- The stationary distribution $m^{\\ast}$ is consistent with the CTMC driven by $u^{\\ast}$, i.e., it satisfies the stationarity (global balance) conditions.\n\nStarting from these definitions and principles, derive the ergodic HJB optimality conditions for this two-node CTMC with linear costs and bounded transition rates. Then, solve explicitly for the equilibrium transition rates $u_0^{\\ast}$ and $u_1^{\\ast}$ and the stationary distribution $m_0^{\\ast}$ and $m_1^{\\ast}$.\n\nExpress your final answer as a single row matrix containing $(u_0^{\\ast},\\,u_1^{\\ast},\\,m_0^{\\ast},\\,m_1^{\\ast})$. No rounding is required.", "solution": "The problem asks for the mean-field Nash equilibrium of a two-state continuous-time Markov chain (CTMC) model. An equilibrium is a pair $(u^{\\ast}, m^{\\ast})$ consisting of an optimal control policy $u^{\\ast} = (u_0^{\\ast}, u_1^{\\ast})$ and a consistent stationary population distribution $m^{\\ast} = (m_0^{\\ast}, m_1^{\\ast})$ that satisfy two main conditions: agent optimality and population consistency.\n\n**1. Agent Optimality: The Ergodic Hamilton-Jacobi-Bellman (HJB) Equations**\n\nFor an individual agent, the population distribution $m = (m_0, m_1)$ is considered fixed. The agent's goal is to choose a control policy $u = (u_0, u_1)$ to minimize their long-run average (ergodic) cost. Let $V = (V_0, V_1)$ be the value function representing the total expected future cost at each state, and let $\\lambda$ be the constant ergodic cost. The optimality conditions are given by the ergodic Hamilton-Jacobi-Bellman (HJB) equations. For each state $i \\in \\{0, 1\\}$, the HJB equation is:\n$$ \\lambda = \\min_{u_i \\in [0, \\bar{u}_i]} \\left\\{ \\text{instantaneous cost at } i + \\mathcal{L}^{u} V(i) \\right\\} $$\nThe instantaneous cost at state $i$ is $\\gamma_i m_i + \\alpha_i u_i$. The generator of the CTMC is given as $\\mathcal{L}^{u} V(i) = u_i (V_{1-i} - V_i)$. Substituting these into the HJB system, we get:\n$$ \\lambda = \\min_{u_0 \\in [0, \\bar{u}_0]} \\left\\{ \\gamma_0 m_0 + \\alpha_0 u_0 + u_0(V_1 - V_0) \\right\\} $$\n$$ \\lambda = \\min_{u_1 \\in [0, \\bar{u}_1]} \\left\\{ \\gamma_1 m_1 + \\alpha_1 u_1 + u_1(V_0 - V_1) \\right\\} $$\nTo simplify, we define the relative value, or potential, $P = V_1 - V_0$. The HJB equations can then be rewritten as:\n$$ \\lambda = \\gamma_0 m_0 + \\min_{u_0 \\in [0, \\bar{u}_0]} \\left\\{ u_0 (\\alpha_0 + P) \\right\\} $$\n$$ \\lambda = \\gamma_1 m_1 + \\min_{u_1 \\in [0, \\bar{u}_1]} \\left\\{ u_1 (\\alpha_1 - P) \\right\\} $$\nSince the control $u_i$ appears linearly in the objective, the optimal control $u_i^{\\ast}$ is determined by the sign of its coefficient. This results in a \"bang-bang\" or singular control policy.\nThe optimal control $u_0^{\\ast}$ is:\n$$ u_0^{\\ast}(P) = \\begin{cases} \\bar{u}_0 & \\text{if } \\alpha_0 + P < 0 \\\\ \\text{any value in } [0, \\bar{u}_0] & \\text{if } \\alpha_0 + P = 0 \\\\ 0 & \\text{if } \\alpha_0 + P > 0 \\end{cases} $$\nThe optimal control $u_1^{\\ast}$ is:\n$$ u_1^{\\ast}(P) = \\begin{cases} \\bar{u}_1 & \\text{if } \\alpha_1 - P < 0 \\\\ \\text{any value in } [0, \\bar{u}_1] & \\text{if } \\alpha_1 - P = 0 \\\\ 0 & \\text{if } \\alpha_1 - P > 0 \\end{cases} $$\n\n**2. Population Consistency: The Stationary Distribution**\n\nThe stationary distribution $m^{\\ast} = (m_0^{\\ast}, m_1^{\\ast})$ must be consistent with the dynamics induced by the optimal control $u^{\\ast} = (u_0^{\\ast}, u_1^{\\ast})$. This is captured by the stationarity (or global balance) equation for the CTMC, which states that the flow of agents from state $0$ to $1$ must equal the flow from state $1$ to $0$:\n$$ m_0^{\\ast} u_0^{\\ast} = m_1^{\\ast} u_1^{\\ast} $$\nTogether with the fact that the population fractions must sum to one, $m_0^{\\ast} + m_1^{\\ast} = 1$, we can solve for the stationary distribution. Assuming $u_0^{\\ast} + u_1^{\\ast} > 0$, we have:\n$$ m_0^{\\ast} = \\frac{u_1^{\\ast}}{u_0^{\\ast} + u_1^{\\ast}} \\quad \\text{and} \\quad m_1^{\\ast} = \\frac{u_0^{\\ast}}{u_0^{\\ast} + u_1^{\\ast}} $$\n\n**3. Solving for the Mean-Field Equilibrium**\n\nAn equilibrium is a state $(u^{\\ast}, m^{\\ast}, P)$ that solves the HJB equations and the consistency condition simultaneously. We can find the equilibrium by equating the two HJB equations to eliminate $\\lambda$:\n$$ \\gamma_0 m_0^{\\ast} + u_0^{\\ast}(\\alpha_0 + P) = \\gamma_1 m_1^{\\ast} + u_1^{\\ast}(\\alpha_1 - P) $$\nWe are given the parameters: $\\gamma_0 = 0.6$, $\\gamma_1 = 1.4$, $\\alpha_0 = -1$, $\\alpha_1 = 0.5$, $\\bar{u}_0 = 3$, $\\bar{u}_1 = 2$.\nThe critical values for the potential $P$ that determine the control policy are $P = -\\alpha_0 = 1$ and $P = \\alpha_1 = 0.5$. This partitions the search for $P$ into several cases. Let's analyze the case where the equilibrium controls are at their maximum values, which occurs when $\\alpha_0 + P < 0$ and $\\alpha_1 - P < 0$. This corresponds to the interval $P \\in (0.5, 1)$.\n\nIn this candidate regime, the optimal controls are:\n- $u_0^{\\ast} = \\bar{u}_0 = 3$ (since $P < 1 \\implies P - 1 < 0 \\implies \\alpha_0 + P < 0$)\n- $u_1^{\\ast} = \\bar{u}_1 = 2$ (since $P > 0.5 \\implies 0.5 - P < 0 \\implies \\alpha_1 - P < 0$)\n\nWith these controls, the consistent stationary distribution is:\n- $m_0^{\\ast} = \\frac{u_1^{\\ast}}{u_0^{\\ast} + u_1^{\\ast}} = \\frac{2}{3 + 2} = \\frac{2}{5}$\n- $m_1^{\\ast} = \\frac{u_0^{\\ast}}{u_0^{\\ast} + u_1^{\\ast}} = \\frac{3}{3 + 2} = \\frac{3}{5}$\n\nNow, we must verify that these values are consistent with the HJB equality for $P$. Substituting $u_0^{\\ast}, u_1^{\\ast}, m_0^{\\ast}, m_1^{\\ast}$ into the equation:\n$$ \\gamma_0 m_0^{\\ast} + \\bar{u}_0(\\alpha_0 + P) = \\gamma_1 m_1^{\\ast} + \\bar{u}_1(\\alpha_1 - P) $$\nPlugging in the numerical values:\n$$ (0.6)\\left(\\frac{2}{5}\\right) + 3(-1 + P) = (1.4)\\left(\\frac{3}{5}\\right) + 2(0.5 - P) $$\n$$ 0.24 - 3 + 3P = 0.84 + 1 - 2P $$\n$$ 3P - 2.76 = 1.84 - 2P $$\n$$ 5P = 1.84 + 2.76 $$\n$$ 5P = 4.6 $$\n$$ P = \\frac{4.6}{5} = 0.92 $$\nThe value $P = 0.92$ lies within the assumed interval $(0.5, 1)$, which confirms that our choice of control policies is correct and the solution is self-consistent. Exhaustive analysis of the other cases (e.g., $P < 0.5$ or $P > 1$) leads to contradictions, confirming this equilibrium is unique.\n\nThus, the equilibrium transition rates and stationary distribution are:\n- $u_0^{\\ast} = 3$\n- $u_1^{\\ast} = 2$\n- $m_0^{\\ast} = \\frac{2}{5}$\n- $m_1^{\\ast} = \\frac{3}{5}$", "answer": "$$\n\\boxed{\n\\begin{pmatrix} 3 & 2 & \\frac{2}{5} & \\frac{3}{5} \\end{pmatrix}\n}\n$$", "id": "2987138"}, {"introduction": "A key question in any game-theoretic model is whether the equilibrium is unique. While many introductory examples possess a single, predictable outcome, the rich dynamics of mean-field games often arise from the possibility of multiple equilibria, corresponding to different stable societal structures. This exercise provides a hands-on demonstration of this phenomenon, showing how the combination of attractive interactions and non-convex costs—features that violate the classical Lasry–Lions monotonicity condition for uniqueness—can lead to several distinct Nash equilibria [@problem_id:2987158].", "problem": "Consider the following deterministic mean-field game (MFG) of optimal control, posed on the finite time horizon $[0,T]$ with $T&gt;0$. Each agent $i$ controls a state process $X^{i}_{t}$ evolving according to the stochastic differential equation (SDE)\n$$\ndX^{i}_{t} \\;=\\; u^{i}_{t}\\,dt, \\qquad X^{i}_{0} \\;=\\; 0,\n$$\nwhere $u^{i}_{t}$ is the agent’s control (assumed progressively measurable and square-integrable). The population mean at time $t$ is denoted by $m_{t} \\equiv \\lim_{N\\to\\infty} \\frac{1}{N}\\sum_{i=1}^{N} X^{i}_{t}$, and is taken as exogenous by each agent in the mean-field limit. Given the terminal mean $m_{T}$, an agent minimizes the cost functional\n$$\nJ(u; m_{T}) \\;=\\; \\mathbb{E}\\!\\left[\\;\\int_{0}^{T} \\frac{1}{2}\\,|u_{t}|^{2}\\,dt \\;+\\; V(X_{T}) \\;-\\; \\gamma\\,X_{T}\\,m_{T}\\;\\right],\n$$\nwhere the terminal potential is\n$$\nV(x) \\;=\\; \\frac{a}{4}\\,(x^{2}-b^{2})^{2},\n$$\nwith parameters $a&gt;0$, $b&gt;0$, and the interaction strength $\\gamma&gt;0$. The coupling $-\\gamma\\,X_{T}\\,m_{T}$ represents attractive interactions and violates the Lasry–Lions monotonicity condition.\n\nA mean-field Nash equilibrium is a terminal mean $m_{T}^{\\ast}$ such that, when each agent best-responds to $m_{T}^{\\ast}$, the optimally chosen terminal state has mean equal to $m_{T}^{\\ast}$. Work from first principles of deterministic optimal control and basic calculus, without invoking any pre-packaged mean-field game uniqueness theorems.\n\n1. Derive, from first principles, the best response of a representative agent to a fixed $m_{T}$.\n2. From the best response, derive the scalar fixed-point equation characterizing all equilibrium terminal means $m_{T}^{\\ast}$.\n3. Show that the failure of the monotonicity condition (due to the attractive term $-\\gamma\\,X_{T}\\,m_{T}$) allows for multiple equilibria, and compute all equilibrium values $m_{T}^{\\ast}$ in closed form.\n\nAssume that parameters satisfy $a&gt;0$, $b&gt;0$, $\\gamma&gt;0$, $T&gt;0$, and that\n$$\nb^{2} \\;+\\; \\frac{\\gamma - 1/T}{a} \\;>\\; 0,\n$$\nso that all equilibrium terminal means are real and distinct. Provide your final answer as the row matrix of all equilibrium terminal means $m_{T}^{\\ast}$ in increasing order. Do not include units. No rounding is required.", "solution": "The problem presented is a deterministic mean-field game. Although the state dynamics are written using differential notation, $dX^{i}_{t} = u^{i}_{t}\\,dt$, there is no stochastic process driving the evolution. Given the deterministic initial condition $X^{i}_{0} = 0$, the trajectory of each agent is fully determined by its choice of control $u^{i}_{t}$. Consequently, the expectation operator $\\mathbb{E}[\\cdot]$ in the cost functional is superfluous, as all quantities are deterministic. We can therefore analyze the problem for a representative agent, dropping the superscript $i$.\n\nThe state evolves according to $\\dot{X}_{t} = u_{t}$ with $X_{0} = 0$. The agent seeks to minimize the cost functional\n$$\nJ(u; m_{T}) = \\int_{0}^{T} \\frac{1}{2}\\,u_{t}^{2}\\,dt + \\frac{a}{4}\\,(X_{T}^{2}-b^{2})^{2} - \\gamma\\,X_{T}\\,m_{T}\n$$\nwhere $m_{T}$ is the exogenously given terminal mean of the population.\n\nThis is a problem in the calculus of variations or, equivalently, deterministic optimal control. We proceed by following the three tasks outlined in the problem statement.\n\n1. Derivation of the best response.\nWe use Pontryagin's Maximum Principle. The Hamiltonian for this system is\n$$\nH(X_{t}, p_{t}, u_{t}) = L(X_{t}, u_{t}) + p_{t} \\dot{X}_{t} = \\frac{1}{2}\\,u_{t}^{2} + p_{t} u_{t},\n$$\nwhere $p_t$ is the adjoint (or co-state) variable. The necessary conditions for optimality are:\nThe state equation: $\\dot{X}_{t} = \\frac{\\partial H}{\\partial p_{t}} = u_{t}$.\nThe adjoint equation: $\\dot{p}_{t} = -\\frac{\\partial H}{\\partial X_{t}} = 0$. This implies that the adjoint variable $p_{t}$ is constant for all $t \\in [0, T]$. Let us denote this constant by $p$.\nThe optimality condition for the control $u_{t}$: The optimal control $u_{t}^{\\ast}$ must minimize the Hamiltonian. We find it by setting the partial derivative with respect to $u_{t}$ to zero:\n$$\n\\frac{\\partial H}{\\partial u_{t}} = u_{t} + p_{t} = 0 \\implies u_{t}^{\\ast} = -p_{t} = -p.\n$$\nSince $p$ is constant, the optimal control $u_{t}^{\\ast}$ is also constant over the time horizon.\nThe transversality condition: The adjoint variable at the terminal time $T$ is given by the derivative of the terminal cost function, $\\Phi(X_{T}) = \\frac{a}{4}(X_{T}^{2}-b^{2})^{2} - \\gamma X_{T} m_{T}$, with respect to the terminal state $X_{T}$:\n$$\np_{T} = \\frac{\\partial \\Phi}{\\partial X_{T}} = \\frac{a}{4} \\cdot 2(X_{T}^{2}-b^{2})(2X_{T}) - \\gamma m_{T} = a X_{T}(X_{T}^{2}-b^{2}) - \\gamma m_{T}.\n$$\nSince $p_{t}=p$ is constant, we have $p = p_{T}$.\nNow, we relate the constant control to the state trajectory. Integrating the state equation $\\dot{X}_{t} = u_{t}^{\\ast} = -p$ from $t=0$ to $t=T$ with the initial condition $X_{0}=0$, we obtain:\n$$\nX_{T} = X_{0} + \\int_{0}^{T} (-p)\\,dt = 0 - pT = -pT.\n$$\nThis gives us a second expression for the constant $p$: $p = -\\frac{X_{T}}{T}$.\n\nEquating the two expressions for $p$, we obtain the equation that characterizes the optimal terminal state $X_{T}$ for a given terminal mean $m_{T}$. This is the agent's best response.\n$$\n-\\frac{X_{T}}{T} = a X_{T}(X_{T}^{2}-b^{2}) - \\gamma m_{T}.\n$$\nRearranging this equation, we get:\n$$\n\\gamma m_{T} = a X_{T}(X_{T}^{2}-b^{2}) + \\frac{X_{T}}{T} = a X_{T}^{3} - ab^{2}X_{T} + \\frac{1}{T}X_{T},\n$$\nwhich simplifies to\n$$\n\\gamma m_{T} = a X_{T}^{3} - \\left(ab^{2} - \\frac{1}{T}\\right) X_{T}.\n$$\nThis equation implicitly defines the optimal terminal state $X_T$ as a function of the given mean $m_T$.\n\n2. Derivation of the fixed-point equation.\nA mean-field Nash equilibrium is a configuration where the assumed population behavior is consistent with the aggregate of individual optimal behaviors. In this context, it is a terminal mean $m_{T}^{\\ast}$ such that if every agent best-responds to this $m_{T}^{\\ast}$, their resulting average terminal position is precisely $m_{T}^{\\ast}$.\nSince all agents are identical (same dynamics, initial state, and cost functional), they will all choose the same optimal control and arrive at the same optimal terminal state, $X_{T}$. Therefore, the mean of the terminal states is simply $X_{T}$ itself:\n$$\nm_{T}^{\\ast} = \\lim_{N\\to\\infty} \\frac{1}{N}\\sum_{i=1}^{N} X_{T}^{i} = X_{T}.\n$$\nTo find the equilibrium values $m_{T}^{\\ast}$, we impose this consistency condition on the best-response equation by setting $X_{T} = m_{T} = m_{T}^{\\ast}$:\n$$\n\\gamma m_{T}^{\\ast} = a (m_{T}^{\\ast})^{3} - \\left(ab^{2} - \\frac{1}{T}\\right) m_{T}^{\\ast}.\n$$\nWe rearrange this into a single equation for $m_{T}^{\\ast}$:\n$$\na (m_{T}^{\\ast})^{3} - \\left(ab^{2} - \\frac{1}{T}\\right) m_{T}^{\\ast} - \\gamma m_{T}^{\\ast} = 0.\n$$\nFactoring out $m_{T}^{\\ast}$, we obtain the scalar fixed-point equation:\n$$\nm_{T}^{\\ast} \\left[ a (m_{T}^{\\ast})^{2} - \\left(ab^{2} - \\frac{1}{T} + \\gamma\\right) \\right] = 0.\n$$\n\n3. Computation of equilibrium values.\nThe problem statement notes that the attractive interaction term $-\\gamma X_{T} m_{T}$ (with $\\gamma > 0$) causes a failure of the Lasry-Lions monotonicity condition, which is a sufficient condition for the uniqueness of the equilibrium. The non-convex terminal potential $V(x) = \\frac{a}{4}(x^2-b^2)^2$ also contributes to this failure. The resulting fixed-point equation is a cubic polynomial, which can have multiple real roots, corresponding to multiple equilibria. We now solve this equation to find all possible values for $m_{T}^{\\ast}$.\n\nFrom the equation $m_{T}^{\\ast} \\left[ a (m_{T}^{\\ast})^{2} - (ab^{2} - 1/T + \\gamma) \\right] = 0$, we have two possibilities for the roots:\nCase 1:\n$$\nm_{T}^{\\ast} = 0.\n$$\nThis is always an equilibrium solution.\n\nCase 2:\n$$\na (m_{T}^{\\ast})^{2} - \\left(ab^{2} - \\frac{1}{T} + \\gamma\\right) = 0.\n$$\nSolving for $(m_{T}^{\\ast})^{2}$:\n$$\na (m_{T}^{\\ast})^{2} = ab^{2} + \\gamma - \\frac{1}{T}.\n$$\n$$\n(m_{T}^{\\ast})^{2} = \\frac{ab^{2} + \\gamma - 1/T}{a} = b^{2} + \\frac{\\gamma - 1/T}{a}.\n$$\nThis gives two additional potential solutions, provided the right-hand side is positive:\n$$\nm_{T}^{\\ast} = \\pm \\sqrt{b^{2} + \\frac{\\gamma - 1/T}{a}}.\n$$\nThe problem provides the assumption that $b^{2} + \\frac{\\gamma - 1/T}{a} > 0$. This ensures that the two non-zero roots are real and distinct from each other and from the zero root.\n\nThus, there are three distinct equilibrium terminal means. Let $S = \\sqrt{b^{2} + \\frac{\\gamma - 1/T}{a}}$. The three equilibria are $0$, $S$, and $-S$. Arranged in increasing order, they are:\n$$\nm_{T}^{\\ast} \\in \\left\\{-\\sqrt{b^{2} + \\frac{\\gamma - 1/T}{a}}, \\;\\; 0, \\;\\; \\sqrt{b^{2} + \\frac{\\gamma - 1/T}{a}}\\right\\}.\n$$", "answer": "$$\n\\boxed{\\begin{pmatrix} -\\sqrt{b^{2} + \\frac{\\gamma - 1/T}{a}} & 0 & \\sqrt{b^{2} + \\frac{\\gamma - 1/T}{a}} \\end{pmatrix}}\n$$", "id": "2987158"}]}