{"hands_on_practices": [{"introduction": "The journey to mastering numerical methods for SDEs begins with a solid grasp of the fundamentals. This first exercise provides a crucial, hands-on calculation of the weak error for the Euler-Maruyama scheme applied to a linear SDE, a cornerstone model in stochastic analysis. By working from first principles, you will derive the exact weak error and use asymptotic analysis to explicitly confirm the scheme's first-order weak convergence, identifying the leading-order error constant [@problem_id:3005973]. This practice is invaluable for building a concrete intuition for how discretization errors arise and behave as the step size $h$ vanishes.", "problem": "Consider the scalar stochastic differential equation (SDE) $$dX_{t}=\\lambda X_{t}\\,dt+\\sigma\\,dW_{t},\\qquad X_{0}\\in\\mathbb{R},$$ where $\\lambda\\in\\mathbb{R}$, $\\sigma\\in\\mathbb{R}$ are constants and $(W_{t})_{t\\geq 0}$ is a standard Brownian motion. Fix a terminal time $T>0$ and a uniform time step $h>0$ with $N:=T/h\\in\\mathbb{N}$. Define the Euler–Maruyama (EM) scheme by $$X_{n+1}^{h}=X_{n}^{h}+\\lambda X_{n}^{h}\\,h+\\sigma\\,(W_{t_{n+1}}-W_{t_{n}}),\\qquad t_{n}=nh,\\quad n=0,1,\\dots,N-1,$$ with $X_{0}^{h}=X_{0}$. Using only first principles of Itô calculus and properties of expectations of stochastic integrals, do the following:\n1. Derive $\\mathbb{E}[X_{T}]$ for the SDE solution and $\\mathbb{E}[X_{T}^{h}]$ for the EM scheme.\n2. Compute the exact weak error $$\\left|\\mathbb{E}[X_{T}^{h}]-\\mathbb{E}[X_{T}]\\right|$$ in closed form as a function of $\\lambda$, $\\sigma$, $T$, $h$, and $X_{0}$.\n3. Show, by a rigorous asymptotic expansion as $h\\to 0$, that the weak error is $\\mathcal{O}(h)$ and identify the leading-order constant $C(\\lambda,T,X_{0})$ such that $$\\left|\\mathbb{E}[X_{T}^{h}]-\\mathbb{E}[X_{T}]\\right|=C(\\lambda,T,X_{0})\\,h+o(h).$$\n\nYour final reported answer must be the single closed-form expression for the constant $C(\\lambda,T,X_{0})$.", "solution": "The problem statement is a valid exercise in the analysis of numerical methods for stochastic differential equations. It is scientifically grounded, well-posed, and objective. It contains all necessary information to derive a unique solution without ambiguity or contradiction.\n\nThe analysis proceeds in three parts as requested by the problem statement.\n\n**Part 1: Derivation of $\\mathbb{E}[X_{T}]$ and $\\mathbb{E}[X_{T}^{h}]$**\n\nFirst, we determine the expectation of the solution to the SDE $dX_{t}=\\lambda X_{t}\\,dt+\\sigma\\,dW_{t}$ with initial condition $X_{0}$. Integrating the SDE from $t=0$ to $t=T$ yields the integral form:\n$$X_{T} = X_{0} + \\int_{0}^{T} \\lambda X_{s}\\,ds + \\int_{0}^{T} \\sigma\\,dW_{s}$$\nTaking the expectation and using the linearity of the expectation operator, we have:\n$$\\mathbb{E}[X_{T}] = \\mathbb{E}[X_{0}] + \\mathbb{E}\\left[\\int_{0}^{T} \\lambda X_{s}\\,ds\\right] + \\mathbb{E}\\left[\\int_{0}^{T} \\sigma\\,dW_{s}\\right]$$\nThe initial condition $X_{0}$ is a deterministic value (or a random variable independent of the future increments of $W_t$, with a given mean), so $\\mathbb{E}[X_{0}] = X_{0}$. The expectation of the Itô integral with a deterministic integrand (or a suitably adapted process, which holds here) is zero:\n$$\\mathbb{E}\\left[\\int_{0}^{T} \\sigma\\,dW_{s}\\right] = 0$$\nBy Fubini's theorem, we can interchange expectation and time integration:\n$$\\mathbb{E}\\left[\\int_{0}^{T} \\lambda X_{s}\\,ds\\right] = \\int_{0}^{T} \\lambda \\mathbb{E}[X_{s}]\\,ds$$\nLet $m(t) := \\mathbb{E}[X_t]$. Then $m(t)$ satisfies the integral equation:\n$$m(t) = X_{0} + \\int_{0}^{t} \\lambda m(s)\\,ds$$\nDifferentiating with respect to $t$ gives the ordinary differential equation (ODE):\n$$\\frac{d}{dt}m(t) = \\lambda m(t)$$\nwith the initial condition $m(0) = X_{0}$. The solution to this ODE is $m(t) = X_{0}\\exp(\\lambda t)$.\nAt the terminal time $T$, the expectation is:\n$$\\mathbb{E}[X_{T}] = X_{0}\\exp(\\lambda T)$$\n\nNext, we analyze the Euler–Maruyama (EM) scheme:\n$$X_{n+1}^{h}=X_{n}^{h}+\\lambda X_{n}^{h}\\,h+\\sigma\\,(W_{t_{n+1}}-W_{t_{n}})$$\nLet $m_{n} := \\mathbb{E}[X_{n}^{h}]$. Taking the expectation of the recurrence relation:\n$$\\mathbb{E}[X_{n+1}^{h}] = \\mathbb{E}[X_{n}^{h} + \\lambda X_{n}^{h}\\,h] + \\mathbb{E}[\\sigma\\,(W_{t_{n+1}}-W_{t_{n}})]$$\n$$m_{n+1} = \\mathbb{E}[X_{n}^{h}] + \\lambda h \\mathbb{E}[X_{n}^{h}] + \\sigma \\mathbb{E}[W_{t_{n+1}}-W_{t_{n}}]$$\nThe increment of the standard Brownian motion $W_{t_{n+1}}-W_{t_{n}}$ has zero mean, $\\mathbb{E}[W_{t_{n+1}}-W_{t_{n}}] = 0$. Thus, we obtain a recurrence relation for the mean:\n$$m_{n+1} = m_{n} + \\lambda h m_{n} = (1+\\lambda h)m_{n}$$\nThe initial condition is $m_{0} = \\mathbb{E}[X_{0}^{h}] = \\mathbb{E}[X_{0}] = X_{0}$. The solution to this geometric progression is:\n$$m_{n} = (1+\\lambda h)^{n}m_{0} = (1+\\lambda h)^{n}X_{0}$$\nWe are interested in the expectation at the terminal time $T$, which corresponds to $n=N=T/h$.\n$$\\mathbb{E}[X_{T}^{h}] = m_{N} = X_{0}(1+\\lambda h)^{N} = X_{0}(1+\\lambda h)^{T/h}$$\n\n**Part 2: Exact Weak Error**\n\nThe weak error at time $T$ is defined as the absolute difference between the expectation of the numerical solution and the expectation of the true solution.\nUsing the results from Part 1, the exact weak error is:\n$$\\left|\\mathbb{E}[X_{T}^{h}]-\\mathbb{E}[X_{T}]\\right| = \\left|X_{0}(1+\\lambda h)^{T/h} - X_{0}\\exp(\\lambda T)\\right|$$\nThis can be written as:\n$$\\left|\\mathbb{E}[X_{T}^{h}]-\\mathbb{E}[X_{T}]\\right| = |X_{0}| \\left| (1+\\lambda h)^{T/h} - \\exp(\\lambda T) \\right|$$\nThis is the closed-form expression for the exact weak error.\n\n**Part 3: Asymptotic Expansion and Leading-Order Constant**\n\nTo analyze the behavior of the error as $h \\to 0$, we perform an asymptotic expansion of the term $(1+\\lambda h)^{T/h}$. We use the identity $a^b = \\exp(b \\ln a)$:\n$$(1+\\lambda h)^{T/h} = \\exp\\left(\\frac{T}{h}\\ln(1+\\lambda h)\\right)$$\nThe Taylor series expansion of $\\ln(1+u)$ around $u=0$ is $\\ln(1+u) = u - \\frac{u^{2}}{2} + \\mathcal{O}(u^{3})$. Setting $u = \\lambda h$, we get:\n$$\\ln(1+\\lambda h) = \\lambda h - \\frac{(\\lambda h)^{2}}{2} + \\mathcal{O}(h^{3}) = \\lambda h - \\frac{\\lambda^{2}h^{2}}{2} + \\mathcal{O}(h^{3})$$\nSubstituting this into the exponent:\n$$\\frac{T}{h}\\ln(1+\\lambda h) = \\frac{T}{h}\\left(\\lambda h - \\frac{\\lambda^{2}h^{2}}{2} + \\mathcal{O}(h^{3})\\right) = T\\lambda - \\frac{T\\lambda^{2}h}{2} + \\mathcal{O}(h^{2})$$\nTherefore, the expansion for $(1+\\lambda h)^{T/h}$ is:\n$$(1+\\lambda h)^{T/h} = \\exp\\left(T\\lambda - \\frac{T\\lambda^{2}h}{2} + \\mathcal{O}(h^{2})\\right) = \\exp(T\\lambda)\\exp\\left(-\\frac{T\\lambda^{2}h}{2} + \\mathcal{O}(h^{2})\\right)$$\nUsing the Taylor expansion $\\exp(v) = 1+v+\\mathcal{O}(v^{2})$ for small $v = -\\frac{T\\lambda^{2}h}{2} + \\mathcal{O}(h^{2})$:\n$$\\exp\\left(-\\frac{T\\lambda^{2}h}{2} + \\mathcal{O}(h^{2})\\right) = 1 - \\frac{T\\lambda^{2}h}{2} + \\mathcal{O}(h^{2})$$\nPutting it all together, we find the asymptotic expansion of $\\mathbb{E}[X_{T}^{h}]$:\n$$\\mathbb{E}[X_{T}^{h}] = X_{0}(1+\\lambda h)^{T/h} = X_{0}\\exp(T\\lambda)\\left(1 - \\frac{T\\lambda^{2}h}{2} + \\mathcal{O}(h^{2})\\right) = X_{0}\\exp(T\\lambda) - \\frac{X_{0}T\\lambda^{2}\\exp(T\\lambda)}{2}h + \\mathcal{O}(h^{2})$$\nNow we compute the difference $\\mathbb{E}[X_{T}^{h}]-\\mathbb{E}[X_{T}]$:\n$$\\mathbb{E}[X_{T}^{h}]-\\mathbb{E}[X_{T}] = \\left(X_{0}\\exp(T\\lambda) - \\frac{X_{0}T\\lambda^{2}\\exp(T\\lambda)}{2}h + \\mathcal{O}(h^{2})\\right) - X_{0}\\exp(T\\lambda)$$\n$$\\mathbb{E}[X_{T}^{h}]-\\mathbb{E}[X_{T}] = -\\frac{X_{0}T\\lambda^{2}\\exp(T\\lambda)}{2}h + \\mathcal{O}(h^{2})$$\nThe weak error is the absolute value of this quantity:\n$$\\left|\\mathbb{E}[X_{T}^{h}]-\\mathbb{E}[X_{T}]\\right| = \\left|-\\frac{X_{0}T\\lambda^{2}\\exp(T\\lambda)}{2}h + \\mathcal{O}(h^{2})\\right|$$\nFor $h \\to 0$, the leading term dominates. Since $T>0$, $\\lambda^{2}\\ge 0$, and $\\exp(T\\lambda)>0$, the absolute value can be simplified:\n$$\\left|\\mathbb{E}[X_{T}^{h}]-\\mathbb{E}[X_{T}]\\right| = \\left|-\\frac{X_{0}T\\lambda^{2}\\exp(T\\lambda)}{2}\\right|h + o(h) = \\frac{|X_{0}|T\\lambda^{2}\\exp(T\\lambda)}{2}h + o(h)$$\nThis shows that the weak error is of order $\\mathcal{O}(h)$. The leading-order constant $C(\\lambda,T,X_{0})$ is the coefficient of $h$:\n$$C(\\lambda,T,X_{0}) = \\frac{|X_{0}|T\\lambda^{2}\\exp(T\\lambda)}{2}$$\nIf $\\lambda=0$, the constant is $0$, which is correct as the EM scheme is exact for the mean in this case. The presence of the noise term with coefficient $\\sigma$ does not affect the weak error for this linear SDE, as reflected in the final expression for $C(\\lambda,T,X_{0})$.", "answer": "$$\\boxed{\\frac{|X_{0}| T \\lambda^{2} \\exp(\\lambda T)}{2}}$$", "id": "3005973"}, {"introduction": "While the Euler-Maruyama scheme behaves predictably for linear SDEs, the nonlinear world presents significant challenges. This practice explores a critical failure mode of the standard scheme when applied to SDEs with superlinear drift coefficients, a common feature in models from physics and finance. You will demonstrate how the naive discretization can lead to an explosion of moments, violating the conditions for weak convergence, and then show how a \"tamed\" Euler scheme elegantly restores stability and guarantees convergence for this challenging class of problems [@problem_id:3005951]. This exercise highlights the crucial interplay between the analytical properties of an SDE and the design of a robust numerical method.", "problem": "Consider the one-dimensional Stochastic Differential Equation (SDE)\n$$\ndX_{t} = -X_{t}^{3}\\,dt + \\sqrt{2}\\,dW_{t}, \\qquad X_{0} \\sim \\pi,\n$$\nwhere $W_{t}$ denotes a standard Brownian motion and the initial condition $X_{0}$ is distributed according to the invariant probability measure $\\pi$ of the SDE. The invariant density $\\varrho$ of $\\pi$ is defined by the stationary solution of the Fokker–Planck equation (FPE) associated with the SDE, and in the overdamped Langevin setting with noise strength $\\sqrt{2}$ is given by\n$$\n\\varrho(x) \\propto \\exp(-V(x)), \\qquad V(x) = \\tfrac{1}{4}x^{4}.\n$$\n\nDefine the Euler–Maruyama (EM) and the tamed Euler discretization schemes with constant time step $h = T/N$ and $N \\in \\mathbb{N}$ by\n$$\nY_{n+1}^{\\mathrm{EM}} = Y_{n}^{\\mathrm{EM}} - h\\big(Y_{n}^{\\mathrm{EM}}\\big)^{3} + \\sqrt{2}\\,\\Delta W_{n}, \\qquad \\Delta W_{n} = W_{(n+1)h} - W_{nh},\n$$\nand\n$$\nY_{n+1}^{\\mathrm{tam}} = Y_{n}^{\\mathrm{tam}} + \\frac{-h\\big(Y_{n}^{\\mathrm{tam}}\\big)^{3}}{1 + h\\big|Y_{n}^{\\mathrm{tam}}\\big|^{3}} + \\sqrt{2}\\,\\Delta W_{n}.\n$$\n\nBy weak convergence of a discretization scheme we mean that for sufficiently regular test functions $\\varphi$ of at most polynomial growth, the approximation satisfies\n$$\n\\big| \\mathbb{E}\\big[\\varphi\\big(Y_{N}\\big)\\big] - \\mathbb{E}\\big[\\varphi\\big(X_{T}\\big)\\big] \\big| \\to 0 \\quad \\text{as } h \\to 0,\n$$\nwith a rate determined by the scheme’s weak order.\n\nTasks:\n\n1) Using only the Fokker–Planck characterization of invariant measures for overdamped Langevin dynamics, identify the invariant density $\\varrho(x)$ for this SDE and write $\\varrho$ in normalized form. For the particular test function $\\varphi(x) = x^{2}$, compute the exact stationary expectation $\\mathbb{E}_{\\pi}[X^{2}]$ as a closed-form analytic expression in terms of the Gamma function.\n\n2) Show that for the EM scheme applied to this SDE, there is no uniform bound on the second moment along the time grid as $h \\to 0$, i.e., demonstrate that $\\limsup_{N\\to\\infty}\\,\\mathbb{E}\\big[|Y_{N}^{\\mathrm{EM}}|^{2}\\big] = +\\infty$. Use a first-principles argument based on the deterministic part of the EM recursion for large states and an appropriate rare-event construction for the noise increments to explain why moment blow-up occurs and why this prevents weak convergence for the test function $\\varphi(x) = x^{2}$.\n\n3) Prove that the tamed Euler scheme admits uniform moment bounds in $n$ and $h$ for the same SDE and deduce the weak convergence of the tamed scheme for sufficiently smooth test functions. Your derivation should start from the discrete-time dynamics, use the taming mechanism to control superlinear drift increments, and conclude with a one-step local weak error argument.\n\nFinally, provide the exact analytic expression for the stationary second moment $\\mathbb{E}_{\\pi}[X^{2}]$ you obtained in Task 1. No numerical approximation is required; give the exact expression in closed form.", "solution": "The user has provided a multi-part problem concerning the analysis of a one-dimensional stochastic differential equation (SDE) and two of its numerical discretization schemes. The problem is to be validated and, if valid, solved.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n\n*   **SDE**: $dX_{t} = -X_{t}^{3}\\,dt + \\sqrt{2}\\,dW_{t}$\n*   **Initial Condition**: $X_{0} \\sim \\pi$, where $\\pi$ is the invariant probability measure.\n*   **Invariant Density**: $\\varrho(x) \\propto \\exp(-V(x))$ with potential $V(x) = \\frac{1}{4}x^{4}$. The noise strength is given as $\\sqrt{2}$. This corresponds to overdamped Langevin dynamics $dX_t = -\\nabla V(X_t) dt + \\sqrt{2\\beta^{-1}} dW_t$ with $\\nabla V(x) = x^3$, and $\\beta^{-1}=1$.\n*   **Euler–Maruyama (EM) Scheme**: $Y_{n+1}^{\\mathrm{EM}} = Y_{n}^{\\mathrm{EM}} - h(Y_{n}^{\\mathrm{EM}})^{3} + \\sqrt{2}\\,\\Delta W_{n}$, where $\\Delta W_{n} = W_{(n+1)h} - W_{nh} = \\sqrt{h}Z_n$ with $Z_n \\sim \\mathcal{N}(0,1)$.\n*   **Tamed Euler Scheme**: $Y_{n+1}^{\\mathrm{tam}} = Y_{n}^{\\mathrm{tam}} + \\frac{-h(Y_{n}^{\\mathrm{tam}})^{3}}{1 + h|(Y_{n}^{\\mathrm{tam}})|^{3}} + \\sqrt{2}\\,\\Delta W_{n}$.\n*   **Definition of Weak Convergence**: For suitable test functions $\\varphi$, $|\\mathbb{E}[\\varphi(Y_{N})] - \\mathbb{E}[\\varphi(X_{T})]| \\to 0$ as $h \\to 0$, where $h=T/N$.\n*   **Tasks**:\n    1.  Normalize the invariant density $\\varrho(x)$ and compute the stationary expectation $\\mathbb{E}_{\\pi}[X^{2}]$ in terms of the Gamma function.\n    2.  Show that the second moment of the EM scheme is not uniformly bounded, $\\limsup_{N\\to\\infty}\\,\\mathbb{E}[|Y_{N}^{\\mathrm{EM}}|^{2}] = +\\infty$, and explain why this invalidates weak convergence for $\\varphi(x) = x^{2}$.\n    3.  Prove uniform moment bounds for the tamed Euler scheme and deduce its weak convergence.\n    4.  Provide the final closed-form expression for $\\mathbb{E}_{\\pi}[X^{2}]$.\n\n**Step 2: Validate Using Extracted Givens**\n\n*   **Scientifically Grounded**: The problem is situated within the standard mathematical theory of stochastic differential equations and their numerical approximation. The SDE, the Fokker-Planck equation, the concept of an invariant measure, and the Euler-Maruyama and tamed Euler schemes are all well-established topics in this field. The SDE chosen, with a superlinearly growing drift coefficient $f(x) = -x^3$, is a canonical example used to illustrate the shortcomings of the standard EM scheme and the necessity for modified schemes like the tamed one.\n*   **Well-Posed**: The problem is clearly structured into three distinct tasks that are logically connected. The tasks are specific, and the required outputs are unambiguously defined. The calculations and proofs requested are standard exercises in this domain of mathematics.\n*   **Objective**: The problem is formulated in precise, objective mathematical language, free from ambiguity or subjective content.\n*   **Self-Contained and Consistent**: All necessary definitions and equations are provided. The relationship between the potential $V(x)$ and the drift $-x^3$ is consistent with the physics of overdamped Langevin dynamics. The problem is internally consistent and contains sufficient information for a full solution.\n\n**Step 3: Verdict and Action**\n\nThe problem is valid. It is a well-formulated, standard problem in the numerical analysis of SDEs. I will proceed with a full solution.\n\n### Solution\n\n**Task 1: Invariant Density and Stationary Expectation**\n\nThe invariant probability density $\\varrho(x)$ is given as being proportional to $\\exp(-V(x))$, where $V(x) = \\frac{1}{4}x^4$.\n$$\n\\varrho(x) = \\frac{1}{C} \\exp\\left(-\\frac{1}{4}x^{4}\\right)\n$$\nTo normalize the density, we must compute the normalization constant $C = \\int_{-\\infty}^{\\infty} \\exp(-\\frac{1}{4}x^4) dx$. The integrand is an even function, so $C = 2 \\int_{0}^{\\infty} \\exp(-\\frac{1}{4}x^4) dx$.\n\nWe perform a change of variables. Let $u = \\frac{1}{4}x^4$. Then $x = (4u)^{1/4}$ and $dx = \\frac{1}{4}(4u)^{-3/4} \\cdot 4\\,du = (4u)^{-3/4}\\,du$. The integral becomes:\n$$\nC = 2 \\int_{0}^{\\infty} e^{-u} (4u)^{-3/4} du = 2 \\cdot 4^{-3/4} \\int_{0}^{\\infty} u^{-3/4} e^{-u} du\n$$\nThe remaining integral is the definition of the Gamma function, $\\Gamma(z) = \\int_{0}^{\\infty} t^{z-1}e^{-t}dt$. With $z-1 = -3/4$, we have $z=1/4$.\n$$\nC = 2 \\cdot (2^2)^{-3/4} \\Gamma\\left(\\frac{1}{4}\\right) = 2 \\cdot 2^{-3/2} \\Gamma\\left(\\frac{1}{4}\\right) = 2^{-1/2} \\Gamma\\left(\\frac{1}{4}\\right) = \\frac{\\Gamma(1/4)}{\\sqrt{2}}\n$$\nThus, the normalized invariant density is:\n$$\n\\varrho(x) = \\frac{\\sqrt{2}}{\\Gamma(1/4)} \\exp\\left(-\\frac{1}{4}x^{4}\\right)\n$$\nNext, we compute the stationary expectation $\\mathbb{E}_{\\pi}[X^2]$ for the test function $\\varphi(x) = x^2$.\n$$\n\\mathbb{E}_{\\pi}[X^2] = \\int_{-\\infty}^{\\infty} x^2 \\varrho(x) dx = \\frac{\\sqrt{2}}{\\Gamma(1/4)} \\int_{-\\infty}^{\\infty} x^2 \\exp\\left(-\\frac{1}{4}x^{4}\\right) dx\n$$\nAgain, the integrand is even, so we have:\n$$\n\\mathbb{E}_{\\pi}[X^2] = \\frac{2\\sqrt{2}}{\\Gamma(1/4)} \\int_{0}^{\\infty} x^2 \\exp\\left(-\\frac{1}{4}x^{4}\\right) dx\n$$\nUsing the same change of variables, $u = \\frac{1}{4}x^4$, we have $x^2 = (4u)^{1/2} = 2u^{1/2}$ and $dx = (4u)^{-3/4}du$. The integral becomes:\n$$\n\\int_{0}^{\\infty} (2u^{1/2}) e^{-u} (4u)^{-3/4} du = 2 \\cdot 4^{-3/4} \\int_{0}^{\\infty} u^{1/2} u^{-3/4} e^{-u} du = 2 \\cdot 2^{-3/2} \\int_{0}^{\\infty} u^{-1/4} e^{-u} du\n$$\nThe integral is $\\Gamma(z)$ with $z-1=-1/4$, so $z=3/4$.\n$$\n\\int_{0}^{\\infty} x^2 \\exp\\left(-\\frac{1}{4}x^{4}\\right) dx = 2^{-1/2} \\Gamma\\left(\\frac{3}{4}\\right) = \\frac{\\Gamma(3/4)}{\\sqrt{2}}\n$$\nSubstituting this back into the expression for the expectation:\n$$\n\\mathbb{E}_{\\pi}[X^2] = \\frac{2\\sqrt{2}}{\\Gamma(1/4)} \\cdot \\frac{\\Gamma(3/4)}{\\sqrt{2}} = \\frac{2\\Gamma(3/4)}{\\Gamma(1/4)}\n$$\n\n**Task 2: Moment Blow-up of the Euler–Maruyama Scheme**\n\nThe EM scheme is given by $Y_{n+1} = Y_n - h Y_n^3 + \\sqrt{2h} Z_n$, where $Z_n \\sim \\mathcal{N}(0,1)$. We analyze the evolution of the second moment. Let $Y_n$ be given.\n$$\n\\mathbb{E}[|Y_{n+1}|^2 | Y_n] = \\mathbb{E}[ (Y_n - hY_n^3 + \\sqrt{2h}Z_n)^2 | Y_n ]\n$$\n$$\n= \\mathbb{E}[ (Y_n - hY_n^3)^2 + 2(Y_n - hY_n^3)\\sqrt{2h}Z_n + 2hZ_n^2 | Y_n ]\n$$\nUsing $\\mathbb{E}[Z_n]=0$ and $\\mathbb{E}[Z_n^2]=1$:\n$$\n\\mathbb{E}[|Y_{n+1}|^2 | Y_n] = (Y_n - hY_n^3)^2 + 2h = Y_n^2 - 2hY_n^4 + h^2Y_n^6 + 2h\n$$\nThe deterministic part of the EM map is $y \\mapsto y - hy^3 = y(1-hy^2)$. If $|y| > \\sqrt{2/h}$, then $hy^2 > 2$, which implies $1-hy^2 < -1$. In this case, the magnitude of the next step $|y(1-hy^2)| = |y||1-hy^2| > |y|$. The deterministic map is unstable for large states, causing it to overshoot the origin with an even larger magnitude.\n\nThis instability in the discretization leads to moment blow-up. For any fixed step size $h>0$, the noise term $\\sqrt{2h}Z_n$ ensures that the process $Y_n$ will, with probability one, eventually enter the instability region $|Y_n| > \\sqrt{2/h}$. Once inside this region, the term $h^2Y_n^6$ in the one-step moment expansion dominates. Let's analyze the increment of the conditional expectation:\n$$\n\\mathbb{E}[|Y_{n+1}|^2 | Y_n=y] - y^2 = h^2y^6 - 2hy^4 + 2h = h(hy^6 - 2y^4 + 2)\n$$\nFor large $|y|$, specifically when $hy^2>2$, the term $hy^6-2y^4$ is positive and grows. This indicates that once the process reaches a state of large magnitude, its second moment is expected to increase, pushing it to even larger states. This positive feedback loop means that for any fixed $h>0$, the sequence of second moments $\\mathbb{E}[|Y_n|^2]$ is not bounded as $n \\to \\infty$. That is, for any fixed $h>0$, $\\lim_{n \\to \\infty} \\mathbb{E}[|Y_n|^2] = +\\infty$. Since this holds for any $h>0$, the family of expectations $\\{\\mathbb{E}[|Y_N|^2]\\}_{N \\in \\mathbb{N}}$ where $h$ is fixed is unbounded. The phrasing in the problem $\\limsup_{N\\to\\infty}\\,\\mathbb{E}\\big[|Y_{N}^{\\mathrm{EM}}|^{2}\\big] = +\\infty$ must be interpreted in this sense of time-divergence for any fixed discretization step $h$.\n\nThis behavior prevents weak convergence for test functions like $\\varphi(x)=x^2$. A standard requirement for proving weak convergence is the uniform boundedness of moments, i.e., the existence of a constant $K$ independent of $h$ such that $\\sup_{0 \\le n \\le N, h \\in (0,1]} \\mathbb{E}[|Y_n|^p] \\le K$ for some sufficiently large $p$. The EM scheme violates this condition for SDEs with superlinear drift coefficients. The fact that for any fixed $h$, the moments diverge in time is a manifestation of this lack of a uniform bound. Consequently, the error estimate $|\\mathbb{E}[\\varphi(Y_N)] - \\mathbb{E}[\\varphi(X_T)]|$ cannot be controlled, and weak convergence fails. The numerical solution does not inherit the long-time stability of the true solution, and its second moment does not converge to the finite stationary value $\\mathbb{E}_{\\pi}[X^2]$.\n\n**Task 3: Uniform Moment Bounds and Weak Convergence of the Tamed Scheme**\n\nThe tamed Euler scheme is $Y_{n+1} = Y_n + g_h(Y_n) + \\sqrt{2h}Z_n$, where $g_h(y) = \\frac{-hy^3}{1+h|y|^3}$. The key feature of the taming function $g_h(y)$ is that its magnitude is globally bounded:\n$$\n|g_h(y)| = \\frac{h|y|^3}{1+h|y|^3} < \\frac{h|y|^3}{h|y|^3} = 1\n$$\nThis uniform bound on the drift increment prevents the large overshoots that plagued the EM scheme. Let's analyze the second moment evolution for the tamed scheme.\n$$\n\\mathbb{E}[|Y_{n+1}|^2 | Y_n] = |Y_n + g_h(Y_n)|^2 + 2h = |Y_n|^2 + 2\\langle Y_n, g_h(Y_n)\\rangle + |g_h(Y_n)|^2 + 2h\n$$\nThe scalar product provides the crucial dissipation:\n$$\n\\langle y, g_h(y) \\rangle = y \\cdot \\frac{-hy^3}{1+h|y|^3} = -\\frac{h|y|^4}{1+h|y|^3} \\le 0\n$$\nSince $|g_h(y)|^2 < 1$, we can write:\n$$\n\\mathbb{E}[|Y_{n+1}|^2 | Y_n=y] \\le y^2 - \\frac{2h|y|^4}{1+h|y|^3} + 1 + 2h\n$$\nFor $|y|$ large enough, the negative term will dominate the constants and provides a contraction. For example, if $|y| > 1+3h$, then $|y| > 1+2h$, and $\\frac{2h|y|^4}{1+h|y|^3} \\approx 2|y|$, which is greater than $1+2h$. More formally, outside a sufficiently large compact set (independent of $h \\in (0, 1]$), $\\mathbb{E}[|Y_{n+1}|^2 | Y_n=y] < y^2$. This is a Lyapunov-type condition which implies that moments are uniformly bounded. Specifically, one can show there exist constants $\\gamma>0$ and $C>0$ such that $\\mathbb{E}[|Y_{n+1}|^2 | Y_n=y] \\le (1-\\gamma h)|y|^2 + Ch$. Taking expectations, this leads to $\\mathbb{E}[|Y_n|^2] \\le \\max(\\mathbb{E}[|Y_0|^2], C/\\gamma)$, which is a uniform bound in $n$ and $h$. This argument can be extended to all moments $p \\ge 2$.\n\nWith uniform moment bounds established, weak convergence can be proven. The argument rests on analyzing the one-step weak error:\n$$\nE_h(\\varphi, y) = \\mathbb{E}[\\varphi(Y_1)|Y_0=y] - \\mathbb{E}[\\varphi(X_h)|X_0=y]\n$$\nThe generator of the SDE is $\\mathcal{L} = -x^3\\frac{d}{dx} + \\frac{d^2}{dx^2}$. By Ito's lemma, $\\mathbb{E}[\\varphi(X_h)|X_0=y] = \\varphi(y) + h(\\mathcal{L}\\varphi)(y) + O(h^2)$. For the tamed scheme, a Taylor expansion gives:\n$$\n\\mathbb{E}[\\varphi(Y_1)|Y_0=y] \\approx \\varphi(y) + \\varphi'(y)\\mathbb{E}[\\delta_y] + \\frac{1}{2}\\varphi''(y)\\mathbb{E}[\\delta_y^2]\n$$\nwhere $\\delta_y = g_h(y) + \\sqrt{2h}Z$. We have $\\mathbb{E}[\\delta_y] = g_h(y)$ and $\\mathbb{E}[\\delta_y^2] = |g_h(y)|^2 + 2h$.\n$$\n\\mathbb{E}[\\varphi(Y_1)|Y_0=y] \\approx \\varphi(y) + \\varphi'(y)g_h(y) + \\varphi''(y)h + \\frac{1}{2}\\varphi''(y)|g_h(y)|^2\n$$\nThe tamed drift $g_h(y)$ is a consistent approximation of the scaled SDE drift $-hy^3$:\n$$\ng_h(y) = \\frac{-hy^3}{1+h|y|^3} = -hy^3 \\left(1 - \\frac{h|y|^3}{1+h|y|^3}\\right) = -hy^3 + O(h^2)\n$$\nSubstituting this into the expansion, we get:\n$$\n\\mathbb{E}[\\varphi(Y_1)|Y_0=y] = \\varphi(y) + h(-y^3\\varphi'(y)+\\varphi''(y)) + O(h^2) = \\varphi(y) + h(\\mathcal{L}\\varphi)(y) + O(h^2)\n$$\nThe local weak error is therefore $E_h(\\varphi, y) = O(h^2)$ (for sufficiently smooth $\\varphi$). The global weak error is obtained by a telescoping sum:\n$$\n|\\mathbb{E}[\\varphi(Y_N)] - \\mathbb{E}[\\varphi(X_T)]| \\le \\sum_{n=0}^{N-1} |\\mathbb{E}[\\mathbb{E}[\\varphi(Y_{N-n})|Y_n] - \\mathbb{E}[\\varphi(X_{T-nh})|X_0=Y_n]]|\n$$\nThe uniform moment bounds are crucial to show that the accumulated local errors, of order $N \\times O(h^2) = (T/h) \\times O(h^2) = O(h)$, lead to a global error of order $h$. Thus, the tamed scheme converges weakly with order 1.\n\n**Final Answer**\nThe exact analytic expression for the stationary second moment $\\mathbb{E}_{\\pi}[X^{2}]$ is given below.", "answer": "$$\n\\boxed{\\frac{2\\Gamma\\left(\\frac{3}{4}\\right)}{\\Gamma\\left(\\frac{1}{4}\\right)}}\n$$", "id": "3005951"}, {"introduction": "To deepen our understanding of weak convergence, we can move beyond simply measuring the error to analyzing its underlying structure. This practice introduces the powerful \"modified equation\" approach, a sophisticated analytical tool that illuminates the behavior of a numerical scheme by constructing a continuous-time SDE that it approximates to a higher order. By deriving the corrections to a modified SDE that match the weak expansion of the Euler-Maruyama scheme, you will gain a more profound insight into the systematic nature of the local weak error [@problem_id:3005959]. This technique is not only a cornerstone of error analysis but also a foundational concept in the construction of higher-order weak integration schemes.", "problem": "Consider the scalar Itô stochastic differential equation (SDE)\n$$\n\\mathrm{d}X_{t} = a(X_{t})\\,\\mathrm{d}t + \\sigma\\,\\mathrm{d}W_{t},\n$$\nwhere $a:\\mathbb{R}\\to\\mathbb{R}$ is a four-times continuously differentiable function with bounded derivatives, $\\sigma>0$ is a constant, and $W_{t}$ is a standard Wiener process. Let $\\varphi:\\mathbb{R}\\to\\mathbb{R}$ be a six-times continuously differentiable function with bounded derivatives. Define the Euler–Maruyama time discretization with step size $h>0$ by\n$$\nX_{n+1} = X_{n} + a(X_{n})\\,h + \\sigma\\,\\Delta W_{n},\n$$\nwhere $\\Delta W_{n}\\sim\\mathcal{N}(0,h)$ are independent and identically distributed increments. Introduce a modified SDE\n$$\n\\mathrm{d}\\widetilde{X}_{t} = \\widetilde{a}_{h}(\\widetilde{X}_{t})\\,\\mathrm{d}t + \\widetilde{b}_{h}(\\widetilde{X}_{t})\\,\\mathrm{d}W_{t},\n$$\nwhose coefficients depend on the step size $h$ as\n$$\n\\widetilde{a}_{h}(x) = a(x) + h\\,\\alpha(x), \n\\qquad \n\\widetilde{b}_{h}(x) = \\sigma + h\\,\\beta(x),\n$$\nfor unknown correction functions $\\alpha,\\beta:\\mathbb{R}\\to\\mathbb{R}$. The requirement is that, for every such test function $\\varphi$, the conditional one-step weak expansion of the modified SDE satisfies\n$$\n\\mathbb{E}\\big[\\varphi(\\widetilde{X}_{t+h}) \\,\\big|\\, \\widetilde{X}_{t}=x\\big]\n=\n\\mathbb{E}\\big[\\varphi(X_{n+1}) \\,\\big|\\, X_{n}=x\\big]\n+ \\mathcal{O}(h^{3}),\n$$\nwith equality of the expansions up to and including terms of order $h^{2}$. Starting from the definition of the Kolmogorov backward operator for diffusions and Itô’s formula, derive explicit analytic expressions for the correction functions $\\alpha(x)$ and $\\beta(x)$ that achieve this matching for the case of constant diffusion amplitude $\\sigma$. Provide your final answer as a single analytical expression giving both $\\alpha(x)$ and $\\beta(x)$.", "solution": "The problem requires us to find the correction functions $\\alpha(x)$ and $\\beta(x)$ for a modified Itô stochastic differential equation (SDE) such that its one-step conditional expectation matches that of the Euler-Maruyama discretization of an original SDE, up to and including terms of order $h^2$.\n\nThe original SDE is:\n$$\n\\mathrm{d}X_{t} = a(X_{t})\\,\\mathrm{d}t + \\sigma\\,\\mathrm{d}W_{t}\n$$\nThe Euler-Maruyama discretization with step size $h$ starting from $X_n=x$ is:\n$$\nX_{n+1} = x + a(x)h + \\sigma \\Delta W_n, \\quad \\text{where} \\quad \\Delta W_n \\sim \\mathcal{N}(0,h)\n$$\nThe modified SDE is:\n$$\n\\mathrm{d}\\widetilde{X}_{t} = \\widetilde{a}_{h}(\\widetilde{X}_{t})\\,\\mathrm{d}t + \\widetilde{b}_{h}(\\widetilde{X}_{t})\\,\\mathrm{d}W_{t}\n$$\nwith coefficients $\\widetilde{a}_{h}(x) = a(x) + h\\alpha(x)$ and $\\widetilde{b}_{h}(x) = \\sigma + h\\beta(x)$.\n\nThe matching condition is:\n$$\n\\mathbb{E}\\big[\\varphi(\\widetilde{X}_{t+h}) \\,\\big|\\, \\widetilde{X}_{t}=x\\big] = \\mathbb{E}\\big[\\varphi(X_{n+1}) \\,\\big|\\, X_{n}=x\\big] + \\mathcal{O}(h^3)\n$$\nTo solve for $\\alpha(x)$ and $\\beta(x)$, we will derive the Taylor series expansion in powers of $h$ for both sides of this equation and equate the coefficients up to order $h^2$.\n\n**1. Weak Expansion for the Euler-Maruyama Scheme**\n\nLet $X_n = x$. The increment is $\\Delta X = X_{n+1} - x = a(x)h + \\sigma \\Delta W_n$. We perform a Taylor expansion of $\\varphi(X_{n+1}) = \\varphi(x + \\Delta X)$ around $x$ and then take the conditional expectation.\n$$\n\\mathbb{E}[\\varphi(X_{n+1}) | X_n=x] = \\mathbb{E}\\left[ \\sum_{k=0}^{\\infty} \\frac{1}{k!} \\varphi^{(k)}(x) (\\Delta X)^k \\right] = \\sum_{k=0}^{\\infty} \\frac{\\varphi^{(k)}(x)}{k!} \\mathbb{E}[(\\Delta X)^k]\n$$\nWe need the moments of $\\Delta W_n \\sim \\mathcal{N}(0,h)$, which are $\\mathbb{E}[\\Delta W_n]=0$, $\\mathbb{E}[(\\Delta W_n)^2]=h$, $\\mathbb{E}[(\\Delta W_n)^3]=0$, $\\mathbb{E}[(\\Delta W_n)^4]=3h^2$, $\\mathbb{E}[(\\Delta W_n)^5]=0$, $\\mathbb{E}[(\\Delta W_n)^6]=15h^3$, and so on.\n\nLet's compute the required moments of $\\Delta X = a(x)h + \\sigma \\Delta W_n$, keeping terms that will contribute up to order $h^2$ in the final expectation.\n$\\mathbb{E}[\\Delta X] = a(x)h$.\n$\\mathbb{E}[(\\Delta X)^2] = \\mathbb{E}[a(x)^2 h^2 + 2a(x)h\\sigma\\Delta W_n + \\sigma^2(\\Delta W_n)^2] = \\sigma^2h + a(x)^2h^2$.\n$\\mathbb{E}[(\\Delta X)^3] = \\mathbb{E}[a(x)^3h^3 + 3a(x)^2h^2\\sigma\\Delta W_n + 3a(x)h\\sigma^2(\\Delta W_n)^2 + \\sigma^3(\\Delta W_n)^3] = 3a(x)\\sigma^2h^2 + \\mathcal{O}(h^3)$.\n$\\mathbb{E}[(\\Delta X)^4] = \\mathbb{E}[\\dots + 6a(x)^2h^2\\sigma^2(\\Delta W_n)^2 + \\sigma^4(\\Delta W_n)^4] = 3\\sigma^4h^2 + \\mathcal{O}(h^3)$.\nFor $k \\ge 5$, $\\mathbb{E}[(\\Delta X)^k] = \\mathcal{O}(h^3)$.\n\nSubstituting these moments into the Taylor series for $\\mathbb{E}[\\varphi(X_{n+1})]$:\n\\begin{align*}\n\\mathbb{E}[\\varphi(X_{n+1})|X_n=x] &= \\varphi(x) + \\varphi'(x)\\mathbb{E}[\\Delta X] + \\frac{\\varphi''(x)}{2}\\mathbb{E}[(\\Delta X)^2] + \\frac{\\varphi'''(x)}{6}\\mathbb{E}[(\\Delta X)^3] + \\frac{\\varphi^{(4)}(x)}{24}\\mathbb{E}[(\\Delta X)^4] + \\mathcal{O}(h^3) \\\\\n&= \\varphi(x) + \\varphi'(x)(a(x)h) + \\frac{\\varphi''(x)}{2}(\\sigma^2h + a(x)^2h^2) + \\frac{\\varphi'''(x)}{6}(3a(x)\\sigma^2h^2) + \\frac{\\varphi^{(4)}(x)}{24}(3\\sigma^4h^2) + \\mathcal{O}(h^3)\n\\end{align*}\nCollecting terms by powers of $h$:\n$$\n\\mathbb{E}[\\varphi(X_{n+1})|X_n=x] = \\varphi(x) + h\\left(a(x)\\varphi'(x) + \\frac{1}{2}\\sigma^2\\varphi''(x)\\right) + h^2\\left(\\frac{1}{2}a(x)^2\\varphi''(x) + \\frac{1}{2}a(x)\\sigma^2\\varphi'''(x) + \\frac{1}{8}\\sigma^4\\varphi^{(4)}(x)\\right) + \\mathcal{O}(h^3) \\quad (*_E)\n$$\n\n**2. Weak Expansion for the Modified SDE**\n\nThe conditional expectation for the modified SDE can be expanded using its infinitesimal generator, $\\mathcal{L}_h$.\n$$\n\\mathcal{L}_h f(x) = \\widetilde{a}_h(x) f'(x) + \\frac{1}{2}\\widetilde{b}_h(x)^2 f''(x)\n$$\nThe expansion is given by iterating the generator:\n$$\n\\mathbb{E}[\\varphi(\\widetilde{X}_{t+h})|\\widetilde{X}_t=x] = \\varphi(x) + h(\\mathcal{L}_h\\varphi)(x) + \\frac{h^2}{2}(\\mathcal{L}_h^2\\varphi)(x) + \\mathcal{O}(h^3)\n$$\nWe expand the generator $\\mathcal{L}_h$ in powers of $h$:\n$\\widetilde{b}_h(x)^2 = (\\sigma + h\\beta(x))^2 = \\sigma^2 + 2h\\sigma\\beta(x) + h^2\\beta(x)^2$.\n$\\mathcal{L}_h = (a(x)+h\\alpha(x))\\frac{\\mathrm{d}}{\\mathrm{d}x} + \\frac{1}{2}(\\sigma^2+2h\\sigma\\beta(x)+h^2\\beta(x)^2)\\frac{\\mathrm{d}^2}{\\mathrm{d}x^2}$\nLet's define the operators:\n$\\mathcal{L}_0 = a(x)\\frac{\\mathrm{d}}{\\mathrm{d}x} + \\frac{1}{2}\\sigma^2\\frac{\\mathrm{d}^2}{\\mathrm{d}x^2}$\n$\\mathcal{L}_1 = \\alpha(x)\\frac{\\mathrm{d}}{\\mathrm{d}x} + \\sigma\\beta(x)\\frac{\\mathrm{d}^2}{\\mathrm{d}x^2}$\nThen $\\mathcal{L}_h = \\mathcal{L}_0 + h\\mathcal{L}_1 + \\mathcal{O}(h^2)$.\n\nSubstituting this into the expansion for the conditional expectation:\n\\begin{align*}\n\\mathbb{E}[\\varphi(\\widetilde{X}_{t+h})|\\widetilde{X}_t=x] &= \\varphi(x) + h(\\mathcal{L}_0+h\\mathcal{L}_1+\\dots)\\varphi(x) + \\frac{h^2}{2}(\\mathcal{L}_0+h\\mathcal{L}_1+\\dots)^2\\varphi(x) + \\mathcal{O}(h^3) \\\\\n&= \\varphi(x) + h\\mathcal{L}_0\\varphi(x) + h^2\\left(\\mathcal{L}_1\\varphi(x) + \\frac{1}{2}\\mathcal{L}_0^2\\varphi(x)\\right) + \\mathcal{O}(h^3)\n\\end{align*}\nThe term $\\mathcal{L}_0\\varphi(x) = a(x)\\varphi'(x) + \\frac{1}{2}\\sigma^2\\varphi''(x)$ correctly matches the order-$h$ term of the Euler expansion $(*_E)$.\nWe now compute $\\mathcal{L}_0^2\\varphi(x) = \\mathcal{L}_0(\\mathcal{L}_0\\varphi)(x)$:\n\\begin{align*}\n\\mathcal{L}_0^2\\varphi(x) &= \\left(a(x)\\frac{\\mathrm{d}}{\\mathrm{d}x} + \\frac{1}{2}\\sigma^2\\frac{\\mathrm{d}^2}{\\mathrm{d}x^2}\\right)\\left(a(x)\\varphi'(x) + \\frac{1}{2}\\sigma^2\\varphi''(x)\\right) \\\\\n&= a(x)\\left(a'(x)\\varphi'(x)+a(x)\\varphi''(x) + \\frac{1}{2}\\sigma^2\\varphi'''(x)\\right) + \\frac{1}{2}\\sigma^2\\left(a''(x)\\varphi'(x) + 2a'(x)\\varphi''(x) + a(x)\\varphi'''(x) + \\frac{1}{2}\\sigma^2\\varphi^{(4)}(x)\\right) \\\\\n&= \\left(a(x)a'(x) + \\frac{1}{2}\\sigma^2a''(x)\\right)\\varphi'(x) + \\left(a(x)^2 + \\sigma^2a'(x)\\right)\\varphi''(x) + a(x)\\sigma^2\\varphi'''(x) + \\frac{1}{4}\\sigma^4\\varphi^{(4)}(x)\n\\end{align*}\nThe coefficient of $h^2$ in the modified SDE expansion is $\\mathcal{L}_1\\varphi(x) + \\frac{1}{2}\\mathcal{L}_0^2\\varphi(x)$:\n\\begin{align*}\n&(\\alpha(x)\\varphi'(x) + \\sigma\\beta(x)\\varphi''(x)) + \\frac{1}{2}\\left[ \\left(a(x)a'(x) + \\frac{1}{2}\\sigma^2a''(x)\\right)\\varphi'(x) + \\left(a(x)^2 + \\sigma^2a'(x)\\right)\\varphi''(x) + a(x)\\sigma^2\\varphi'''(x) + \\frac{1}{4}\\sigma^4\\varphi^{(4)}(x) \\right] \\\\\n=& \\left(\\alpha(x) + \\frac{1}{2}a(x)a'(x) + \\frac{1}{4}\\sigma^2a''(x)\\right)\\varphi'(x) + \\left(\\sigma\\beta(x) + \\frac{1}{2}a(x)^2 + \\frac{1}{2}\\sigma^2a'(x)\\right)\\varphi''(x) \\\\\n& + \\frac{1}{2}a(x)\\sigma^2\\varphi'''(x) + \\frac{1}{8}\\sigma^4\\varphi^{(4)}(x)\n\\end{align*}\n\n**3. Matching Coefficients**\n\nWe equate the $h^2$ coefficient from the Euler expansion $(*_E)$ with the $h^2$ coefficient derived above.\n\\begin{align*}\n& \\left(\\alpha + \\frac{1}{2}aa' + \\frac{1}{4}\\sigma^2a''\\right)\\varphi' + \\left(\\sigma\\beta + \\frac{1}{2}a^2 + \\frac{1}{2}\\sigma^2a'\\right)\\varphi'' + \\frac{1}{2}a\\sigma^2\\varphi''' + \\frac{1}{8}\\sigma^4\\varphi^{(4)} \\\\\n&= \\frac{1}{2}a^2\\varphi'' + \\frac{1}{2}a\\sigma^2\\varphi''' + \\frac{1}{8}\\sigma^4\\varphi^{(4)}\n\\end{align*}\n(Here, we omit the argument $x$ for brevity.)\nSince this equality must hold for any sufficiently smooth test function $\\varphi$, the coefficients of the derivatives of $\\varphi$ must match on both sides. The terms involving $\\varphi'''$ and $\\varphi^{(4)}$ are identical. We are left with:\n$$\n\\left(\\alpha(x) + \\frac{1}{2}a(x)a'(x) + \\frac{1}{4}\\sigma^2a''(x)\\right)\\varphi'(x) + \\left(\\sigma\\beta(x) + \\frac{1}{2}a(x)^2 + \\frac{1}{2}\\sigma^2a'(x)\\right)\\varphi''(x) = \\frac{1}{2}a(x)^2\\varphi''(x)\n$$\nRearranging gives:\n$$\n\\left(\\alpha(x) + \\frac{1}{2}a(x)a'(x) + \\frac{1}{4}\\sigma^2a''(x)\\right)\\varphi'(x) + \\left(\\sigma\\beta(x) + \\frac{1}{2}\\sigma^2a'(x)\\right)\\varphi''(x) = 0\n$$\nAs this must hold for any $\\varphi$, the coefficients of $\\varphi'(x)$ and $\\varphi''(x)$ must independently be zero.\n\nFrom the coefficient of $\\varphi'(x)$:\n$$\n\\alpha(x) + \\frac{1}{2}a(x)a'(x) + \\frac{1}{4}\\sigma^2a''(x) = 0 \\implies \\alpha(x) = -\\frac{1}{2}a(x)a'(x) - \\frac{1}{4}\\sigma^2a''(x)\n$$\nFrom the coefficient of $\\varphi''(x)$:\n$$\n\\sigma\\beta(x) + \\frac{1}{2}\\sigma^2a'(x) = 0 \\implies \\beta(x) = -\\frac{1}{2}\\sigma a'(x) \\quad (\\text{since } \\sigma>0)\n$$\nThese are the required correction functions.", "answer": "$$\n\\boxed{\\begin{pmatrix} \\alpha(x) = -\\frac{1}{2}a(x)a'(x) - \\frac{1}{4}\\sigma^{2}a''(x) \\\\ \\beta(x) = -\\frac{1}{2}\\sigma a'(x) \\end{pmatrix}}\n$$", "id": "3005959"}]}