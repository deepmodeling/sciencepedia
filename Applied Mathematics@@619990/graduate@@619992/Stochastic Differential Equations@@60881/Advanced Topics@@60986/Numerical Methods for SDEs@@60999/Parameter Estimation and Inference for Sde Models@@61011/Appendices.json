{"hands_on_practices": [{"introduction": "Before tackling complex estimation challenges, it is essential to understand the theoretical limits of inference. This first exercise provides a foundational analysis of the Ornstein-Uhlenbeck process, a model for which the exact likelihood is known. By deriving the Fisher Information Matrix from first principles, you will quantify the maximum possible precision for parameter estimates and explore how information about mean-reversion, long-term mean, and volatility behaves under different sampling schemes [@problem_id:2989872]. This provides a crucial benchmark for evaluating more practical, approximate methods.", "problem": "Consider the Ornsteinâ€“Uhlenbeck stochastic differential equation (SDE)\n$$\n\\mathrm{d}X_t = \\kappa \\left( \\mu - X_t \\right) \\mathrm{d}t + \\sigma \\,\\mathrm{d}W_t,\n$$\nwith parameters $\\kappa > 0$, $\\mu \\in \\mathbb{R}$, and $\\sigma > 0$, where $W_t$ is a standard Wiener process. Suppose that the process is observed at $n$ discrete times with equal spacing $\\Delta > 0$, yielding data $\\{X_0, X_{\\Delta}, X_{2\\Delta}, \\dots, X_{(n-1)\\Delta}\\}$. Assume the process is stationary and that the likelihood is computed conditionally on $X_0$ so that the log-likelihood is the sum of $(n-1)$ independent transition contributions. It is a well-tested fact that the exact Markov transition distribution over time step $\\Delta$ is Gaussian with mean and variance given by\n$$\nm(x;\\kappa,\\mu,\\Delta) = \\mu + \\left( x - \\mu \\right) \\exp\\!\\left( -\\kappa \\Delta \\right),\n\\qquad\nv(\\kappa,\\sigma^2,\\Delta) = \\frac{\\sigma^2}{2\\kappa}\\left( 1 - \\exp\\!\\left( -2\\kappa \\Delta \\right) \\right).\n$$\nStarting from the definition of Fisher information as the expectation (under the true model) of the negative Hessian of the log-likelihood, derive the closed-form Fisher information matrix for the parameter vector $\\theta = (\\kappa, \\mu, \\sigma^2)$ based on the exact discrete-time likelihood from these $(n-1)$ independent transitions. Express your final result as an explicit $3 \\times 3$ matrix in terms of $(n,\\Delta,\\kappa,\\mu,\\sigma^2)$, with no unspecified constants.\n\nThen analyze the leading-order behavior of each entry of the per-transition Fisher information matrix as $\\Delta \\to 0$ with $n$ fixed, and discuss how the total Fisher information scales as $n \\to \\infty$ with $\\Delta$ fixed. Your reasoning must begin from the definition of Fisher information and fundamental properties of the Gaussian distribution. Do not assume any asymptotic results a priori; derive the small-$\\Delta$ expansions directly from series expansions of $\\exp(-\\kappa \\Delta)$.\n\nProvide your final answer as the exact Fisher information matrix for the $n$ observations in a single closed-form analytic expression. No numerical evaluation or rounding is required, and no units are involved.", "solution": "The problem asks for the Fisher information matrix (FIM) for the parameters $\\theta = (\\kappa, \\mu, \\sigma^2)$ of an Ornstein-Uhlenbeck process, observed at $n$ discrete time points with spacing $\\Delta$. The derivation is based on the exact conditional likelihood of $(n-1)$ transitions.\n\nThe Ornstein-Uhlenbeck SDE is given by:\n$$\n\\mathrm{d}X_t = \\kappa (\\mu - X_t) \\mathrm{d}t + \\sigma \\mathrm{d}W_t\n$$\nThe process is observed at times $t_i = i\\Delta$ for $i=0, 1, \\dots, n-1$. The transition distribution from $X_i = X_{i\\Delta}$ to $X_{i+1} = X_{(i+1)\\Delta}$ is Gaussian, $X_{i+1} | X_i \\sim \\mathcal{N}(m_i, v)$, with mean and variance:\n$$\nm_i = \\mu + (X_i - \\mu) e^{-\\kappa \\Delta}\n$$\n$$\nv = \\frac{\\sigma^2}{2\\kappa} (1 - e^{-2\\kappa \\Delta})\n$$\nThe log-likelihood for a single transition, conditional on $X_i$, is:\n$$\n\\ell_i(\\theta) = -\\frac{1}{2} \\ln(2\\pi) - \\frac{1}{2} \\ln(v) - \\frac{(X_{i+1} - m_i)^2}{2v}\n$$\nThe total log-likelihood for the $(n-1)$ transitions, conditional on $X_0$, is $\\ell(\\theta) = \\sum_{i=0}^{n-2} \\ell_i(\\theta)$.\n\nFor a single transition from a Gaussian distribution, the FIM element for parameters $\\theta_j$ and $\\theta_k$ is given by:\n$$\n\\mathcal{I}_{i,jk} = \\mathbb{E}\\left[ \\frac{1}{v} \\frac{\\partial m_i}{\\partial \\theta_j} \\frac{\\partial m_i}{\\partial \\theta_k}\\right] + \\frac{1}{2v^2} \\frac{\\partial v}{\\partial \\theta_j} \\frac{\\partial v}{\\partial \\theta_k}\n$$\nThe expectation is taken over the stationary distribution of the process. Due to stationarity, $\\mathcal{I}_i$ is the same for all $i$. Let's denote this per-transition FIM as $\\mathcal{I}_{\\text{single}}$. The total FIM for $(n-1)$ transitions is $\\mathcal{I}^{(n)} = (n-1)\\mathcal{I}_{\\text{single}}$.\n\nWe need the partial derivatives of $m_i$ and $v$ with respect to $\\theta = (\\kappa, \\mu, \\sigma^2)$. The process is assumed to be stationary, so $X_i$ is drawn from the stationary distribution $\\mathcal{N}(\\mu, \\frac{\\sigma^2}{2\\kappa})$. This implies $\\mathbb{E}[X_i - \\mu] = 0$ and $\\mathbb{E}[(X_i - \\mu)^2] = \\frac{\\sigma^2}{2\\kappa}$.\n\nThe required derivatives are:\n$$\n\\frac{\\partial m_i}{\\partial \\kappa} = -(X_i - \\mu) \\Delta e^{-\\kappa \\Delta} \\quad \\frac{\\partial v}{\\partial \\kappa} = \\frac{\\sigma^2}{2\\kappa^2} \\left( 2\\kappa\\Delta e^{-2\\kappa \\Delta} - (1 - e^{-2\\kappa \\Delta}) \\right)\n$$\n$$\n\\frac{\\partial m_i}{\\partial \\mu} = 1 - e^{-\\kappa \\Delta} \\quad \\frac{\\partial v}{\\partial \\mu} = 0\n$$\n$$\n\\frac{\\partial m_i}{\\partial \\sigma^2} = 0 \\quad \\frac{\\partial v}{\\partial \\sigma^2} = \\frac{1}{2\\kappa} (1 - e^{-2\\kappa \\Delta}) = \\frac{v}{\\sigma^2}\n$$\nUsing these derivatives and the properties of the stationary distribution, we compute the elements of $\\mathcal{I}_{\\text{single}}$.\n\n**Element $(\\mu, \\mu)$:**\n$$\n\\mathcal{I}_{\\mu\\mu} = \\frac{1}{v} \\mathbb{E}\\left[\\left(\\frac{\\partial m_i}{\\partial \\mu}\\right)^2\\right] = \\frac{(1 - e^{-\\kappa \\Delta})^2}{v} = \\frac{(1 - e^{-\\kappa \\Delta})^2}{\\frac{\\sigma^2}{2\\kappa}(1 - e^{-2\\kappa \\Delta})} = \\frac{2\\kappa(1 - e^{-\\kappa \\Delta})}{\\sigma^2(1 + e^{-\\kappa \\Delta})}\n$$\n**Element $(\\sigma^2, \\sigma^2)$:**\n$$\n\\mathcal{I}_{\\sigma^2\\sigma^2} = \\frac{1}{2v^2}\\left(\\frac{\\partial v}{\\partial \\sigma^2}\\right)^2 = \\frac{1}{2v^2} \\left(\\frac{v}{\\sigma^2}\\right)^2 = \\frac{1}{2\\sigma^4}\n$$\n**Off-diagonal terms involving $\\mu$**: These are zero because $\\partial v / \\partial \\mu = 0$ and $\\mathbb{E}[\\partial m_i / \\partial \\kappa] \\propto \\mathbb{E}[X_i - \\mu] = 0$. So $\\mathcal{I}_{\\kappa\\mu} = \\mathcal{I}_{\\mu\\sigma^2} = 0$.\n\n**Element $(\\kappa, \\sigma^2)$:**\n$$\n\\mathcal{I}_{\\kappa\\sigma^2} = \\frac{1}{2v^2}\\frac{\\partial v}{\\partial \\kappa}\\frac{\\partial v}{\\partial \\sigma^2} = \\frac{1}{2v\\sigma^2} \\frac{\\partial v}{\\partial \\kappa} = \\frac{1}{2\\kappa\\sigma^2} \\left(\\frac{2\\kappa\\Delta}{e^{2\\kappa\\Delta}-1} - 1\\right)\n$$\n**Element $(\\kappa, \\kappa)$:**\n$$\n\\mathcal{I}_{\\kappa\\kappa} = \\frac{1}{v} \\mathbb{E}\\left[\\left(\\frac{\\partial m_i}{\\partial \\kappa}\\right)^2\\right] + \\frac{1}{2v^2}\\left(\\frac{\\partial v}{\\partial \\kappa}\\right)^2\n$$\nThe first term is:\n$$\n\\frac{\\Delta^2 e^{-2\\kappa\\Delta}}{v} \\mathbb{E}[(X_i-\\mu)^2] = \\frac{\\Delta^2 e^{-2\\kappa\\Delta}}{v} \\frac{\\sigma^2}{2\\kappa} = \\frac{\\Delta^2 e^{-2\\kappa\\Delta}}{\\frac{\\sigma^2}{2\\kappa}(1-e^{-2\\kappa\\Delta})} \\frac{\\sigma^2}{2\\kappa} = \\frac{\\Delta^2}{e^{2\\kappa\\Delta}-1}\n$$\nThe second term is $2\\sigma^4 \\mathcal{I}_{\\kappa\\sigma^2}^2$. Combining these gives:\n$$\n\\mathcal{I}_{\\kappa\\kappa} = \\frac{1}{2\\kappa^2}\\left(\\frac{2\\kappa\\Delta}{e^{2\\kappa\\Delta}-1}-1\\right)^2 + \\frac{\\Delta^2}{e^{2\\kappa\\Delta}-1}\n$$\nThe total FIM for $(n-1)$ transitions is $\\mathcal{I}^{(n)} = (n-1)\\mathcal{I}_{\\text{single}}$.\n\n**Asymptotic Analysis:**\n1.  **Small $\\Delta$ behavior ($n$ fixed):** As $\\Delta \\to 0$, $\\mathcal{I}_{\\mu\\mu} = O(\\Delta)$, $\\mathcal{I}_{\\kappa\\kappa} = O(\\Delta)$, $\\mathcal{I}_{\\kappa\\sigma^2} = O(\\Delta)$, while $\\mathcal{I}_{\\sigma^2\\sigma^2} = O(1)$. Information on drift-related parameters $\\kappa$ and $\\mu$ vanishes for a single short transition, while information on variance $\\sigma^2$ does not.\n\n2.  **Large $n$ behavior ($\\Delta$ fixed):** The total Fisher information $\\mathcal{I}^{(n)} = (n-1)\\mathcal{I}_{\\text{single}}$. Each entry scales linearly with $(n-1)$. The inverse of the FIM gives the asymptotic variance of MLEs, which will decrease as $1/(n-1)$. Thus, all parameters are consistently estimable as $n \\to \\infty$.\n\nThe final matrix is:\n$$\n\\mathcal{I}^{(n)} = (n-1) \\begin{pmatrix} \\mathcal{I}_{\\kappa\\kappa} & 0 & \\mathcal{I}_{\\kappa\\sigma^2} \\\\ 0 & \\mathcal{I}_{\\mu\\mu} & 0 \\\\ \\mathcal{I}_{\\kappa\\sigma^2} & 0 & \\mathcal{I}_{\\sigma^2\\sigma^2} \\end{pmatrix}\n$$\nwith the elements as specified above.", "answer": "$$\n\\boxed{\n(n-1)\n\\begin{pmatrix}\n\\frac{1}{2\\kappa^2}\\left(\\frac{2\\kappa\\Delta}{\\exp(2\\kappa\\Delta)-1}-1\\right)^2 + \\frac{\\Delta^2}{\\exp(2\\kappa\\Delta)-1} & 0 & \\frac{1}{2\\kappa\\sigma^2}\\left(\\frac{2\\kappa\\Delta}{\\exp(2\\kappa\\Delta)-1}-1\\right) \\\\\n0 & \\frac{2\\kappa(1 - \\exp(-\\kappa \\Delta))}{\\sigma^2(1 + \\exp(-\\kappa \\Delta))} & 0 \\\\\n\\frac{1}{2\\kappa\\sigma^2}\\left(\\frac{2\\kappa\\Delta}{\\exp(2\\kappa\\Delta)-1}-1\\right) & 0 & \\frac{1}{2\\sigma^4}\n\\end{pmatrix}\n}\n$$", "id": "2989872"}, {"introduction": "In practice, the exact transition density of an SDE is often unknown, forcing us to rely on approximations. This exercise explores a crucial consequence of this reality: discretization bias. Focusing on the widely used Cox-Ingersoll-Ross (CIR) model, you will investigate how a simple Euler-Maruyama based estimator for the mean-reversion speed becomes biased due to the model's non-linearity [@problem_id:2989856]. By applying tools from stochastic calculus, such as the infinitesimal generator, you will derive the leading-order bias term, a key step towards constructing more accurate, bias-corrected estimators.", "problem": "Consider the scalar Coxâ€“Ingersollâ€“Ross (CIR) diffusion, a nonlinear scalar diffusion in which the diffusion coefficient depends on the square root of the state, defined by the stochastic differential equation (SDE) for the process $X_{t}$,\n$$\n\\mathrm{d}X_{t} = \\kappa \\left( \\mu - X_{t} \\right) \\mathrm{d}t + \\sigma \\sqrt{X_{t}} \\,\\mathrm{d}W_{t},\n$$\nwhere $\\kappa>0$, $\\mu>0$, and $\\sigma>0$ are constants, and $W_{t}$ is a standard one-dimensional Wiener process (Brownian motion). Suppose $X_{t}$ is strictly stationary under parameter values satisfying the Feller condition and is observed at discrete times $t_{i} = i \\Delta$ for $i=0,1,\\dots,n$, with fixed sampling interval $\\Delta>0$.\n\nA common Euler discretization-based least-squares estimator for the mean-reversion parameter $\\kappa$ treats the increment $\\left( X_{t_{i+1}} - X_{t_{i}} \\right)/\\Delta$ as a noisy observation of the drift $\\kappa \\left( \\mu - X_{t_{i}} \\right)$ and is defined by\n$$\n\\hat{\\kappa}_{E} \\equiv \\frac{\\sum_{i=0}^{n-1} \\left( \\mu - X_{t_{i}} \\right) \\left( X_{t_{i+1}} - X_{t_{i}} \\right)}{\\Delta \\sum_{i=0}^{n-1} \\left( \\mu - X_{t_{i}} \\right)^{2}}.\n$$\nAssume the sample size $n$ is large and the process is ergodic so that sample averages may be replaced by expectations under the stationary distribution of $X_{t}$. Starting from the definition of the infinitesimal generator of the diffusion and fundamental semigroup expansions for conditional expectations over short time increments, quantify the leading-order bias introduced by the Euler discretization in $\\hat{\\kappa}_{E}$, and derive a first-order correction term in $\\Delta$ that, when added to $\\hat{\\kappa}_{E}$, yields an estimator with bias of order $o(\\Delta)$.\n\nYour final answer must be the closed-form analytic expression for this first-order correction term $C\\!\\left( \\kappa, \\Delta \\right)$ to be added to $\\hat{\\kappa}_{E}$ so that $\\mathbb{E}\\!\\left[ \\hat{\\kappa}_{E} + C\\!\\left( \\kappa, \\Delta \\right) \\right] = \\kappa + o(\\Delta)$. No rounding is required.", "solution": "The objective is to find the first-order correction term for the Euler-based estimator $\\hat{\\kappa}_E$. We begin by calculating the expectation of $\\hat{\\kappa}_E$. Given the assumption of a large sample size and ergodicity, we can replace the sample averages in the definition of $\\hat{\\kappa}_E$ with their expectations under the stationary distribution, which we denote by $\\pi$.\n$$\n\\mathbb{E}[\\hat{\\kappa}_E] \\approx \\frac{\\mathbb{E}_{\\pi}\\left[ \\left( \\mu - X_t \\right) \\left( X_{t+\\Delta} - X_t \\right) \\right]}{\\Delta \\mathbb{E}_{\\pi}\\left[ \\left( \\mu - X_t \\right)^2 \\right]}\n$$\nWe focus on the numerator. Using the law of total expectation, we have:\n$$\n\\mathbb{E}_{\\pi}\\left[ \\left( \\mu - X_t \\right) \\left( X_{t+\\Delta} - X_t \\right) \\right] = \\mathbb{E}_{\\pi}\\left[ \\left( \\mu - X_t \\right) \\mathbb{E}\\left[ X_{t+\\Delta} - X_t | X_t \\right] \\right]\n$$\nThe infinitesimal generator $\\mathcal{L}$ of the CIR process, which has drift $a(x) = \\kappa(\\mu-x)$ and diffusion $b(x)^2 = \\sigma^2 x$, is:\n$$\n\\mathcal{L}f(x) = \\kappa(\\mu-x)f'(x) + \\frac{1}{2}\\sigma^2 x f''(x)\n$$\nWe use the semigroup expansion to approximate the conditional expectation of the increment:\n$$\n\\mathbb{E}[X_{t+\\Delta} - x | X_t=x] = \\Delta\\mathcal{L}x + \\frac{\\Delta^2}{2}\\mathcal{L}^2x + O(\\Delta^3)\n$$\nwhere $\\mathcal{L}^2f(x) = \\mathcal{L}(\\mathcal{L}f)(x)$. We compute the required terms for $f(x)=x$.\nFirst, with $f(x)=x$, we have $f'(x)=1$ and $f''(x)=0$:\n$$\n\\mathcal{L}x = \\kappa(\\mu-x)(1) + \\frac{1}{2}\\sigma^2 x (0) = \\kappa(\\mu-x)\n$$\nNext, we apply $\\mathcal{L}$ to the function $g(x) = \\mathcal{L}x = \\kappa(\\mu-x)$. Its derivatives are $g'(x)=-\\kappa$ and $g''(x)=0$:\n$$\n\\mathcal{L}^2x = \\mathcal{L}g(x) = \\kappa(\\mu-x)g'(x) + \\frac{1}{2}\\sigma^2 x g''(x) = \\kappa(\\mu-x)(-\\kappa) + \\frac{1}{2}\\sigma^2 x (0) = -\\kappa^2(\\mu-x)\n$$\nThe expansion for the conditional increment is:\n$$\n\\mathbb{E}[X_{t+\\Delta} - X_t | X_t] = \\Delta\\kappa(\\mu-X_t) - \\frac{\\Delta^2}{2}\\kappa^2(\\mu-X_t) + O(\\Delta^3)\n$$\nSubstituting this back into the numerator's expectation:\n$$\n\\mathbb{E}_{\\pi}\\left[ (\\mu-X_t) \\left( \\Delta\\kappa(\\mu-X_t) - \\frac{\\Delta^2}{2}\\kappa^2(\\mu-X_t) + O(\\Delta^3) \\right) \\right]\n$$\n$$\n= \\Delta\\kappa\\mathbb{E}_{\\pi}\\left[(\\mu-X_t)^2\\right] - \\frac{\\Delta^2}{2}\\kappa^2\\mathbb{E}_{\\pi}\\left[(\\mu-X_t)^2\\right] + O(\\Delta^3)\n$$\nNow, we can assemble the expression for $\\mathbb{E}[\\hat{\\kappa}_E]$:\n$$\n\\mathbb{E}[\\hat{\\kappa}_E] \\approx \\frac{\\Delta\\kappa\\mathbb{E}_{\\pi}\\left[(\\mu-X_t)^2\\right] - \\frac{\\Delta^2}{2}\\kappa^2\\mathbb{E}_{\\pi}\\left[(\\mu-X_t)^2\\right]}{\\Delta \\mathbb{E}_{\\pi}\\left[ \\left( \\mu - X_t \\right)^2 \\right]} + O(\\Delta^2)\n$$\nThe term $\\mathbb{E}_{\\pi}[(\\mu-X_t)^2]$ cancels, as it's non-zero for a non-deterministic process.\n$$\n\\mathbb{E}[\\hat{\\kappa}_E] \\approx \\frac{\\Delta\\kappa - \\frac{\\Delta^2}{2}\\kappa^2}{\\Delta} + O(\\Delta^2) = \\kappa - \\frac{\\Delta}{2}\\kappa^2 + O(\\Delta^2)\n$$\nThe leading-order bias is the difference between this expectation and the true value $\\kappa$:\n$$\n\\text{Bias} = \\mathbb{E}[\\hat{\\kappa}_E] - \\kappa \\approx -\\frac{\\Delta}{2}\\kappa^2\n$$\nTo create an estimator with bias of order $o(\\Delta)$, we must add a correction term that cancels this leading-order bias. The correction term $C(\\kappa, \\Delta)$ is:\n$$\nC(\\kappa, \\Delta) = -(\\text{Bias}) = \\frac{\\Delta}{2}\\kappa^2\n$$\nThe corrected estimator $\\hat{\\kappa}_E + C(\\kappa, \\Delta)$ will have an expectation of $\\kappa + O(\\Delta^2)$, satisfying the problem's requirement.", "answer": "$$\\boxed{\\frac{\\Delta \\kappa^2}{2}}$$", "id": "2989856"}, {"introduction": "Real-world financial and physical systems often exhibit abrupt jumps, a feature not captured by purely continuous diffusion models. This practice delves into the challenge of estimating drift in the presence of such discontinuities. You will first quantify the severe bias that a naive estimator suffers when it fails to account for a compound Poisson jump component [@problem_id:2989868]. Subsequently, you will construct a jump-filtered estimator, a robust technique that uses a carefully chosen threshold to differentiate between diffusive increments and large jumps, thereby enabling consistent estimation of the underlying drift.", "problem": "Consider the discretely observed jump-diffusion ItÃ´ stochastic differential equation (SDE)\n$$\ndX_{t} \\;=\\; b\\,dt \\;+\\; \\sigma\\,dW_{t} \\;+\\; dJ_{t}, \\qquad t \\ge 0,\n$$\nwhere $b \\in \\mathbb{R}$ and $\\sigma > 0$ are unknown constants, $\\{W_{t}\\}_{t \\ge 0}$ is a standard Brownian motion, and $\\{J_{t}\\}_{t \\ge 0}$ is an independent compound Poisson process defined by\n$$\nJ_{t} \\;=\\; \\sum_{k=1}^{N_{t}} Y_{k}.\n$$\nHere $\\{N_{t}\\}_{t \\ge 0}$ is a Poisson process with intensity $\\lambda > 0$, and $\\{Y_{k}\\}_{k \\ge 1}$ are independent and identically distributed jump sizes with $\\mathbb{E}[|Y_{1}|]  \\infty$, $\\mathbb{E}[Y_{1}] = \\mu_{J}$, and $\\mathbb{P}(|Y_{1}| > 0) = 1$. All sources of randomness are mutually independent.\n\nYou observe $\\{X_{t_{i}}\\}_{i=0}^{n}$ at equidistant times $t_{i} = i \\Delta$ with mesh $\\Delta > 0$, total span $T = n\\Delta$, and the regime $n \\to \\infty$, $\\Delta \\to 0$, with $T = n\\Delta \\to \\infty$ and $n$ growing at most polynomially in $1/\\Delta$.\n\nDefine the naive drift estimator that ignores jumps by\n$$\n\\widehat{b}^{\\,N} \\;=\\; \\frac{1}{T}\\sum_{i=1}^{n} \\Delta X_{i}, \n\\qquad \\Delta X_{i} \\;=\\; X_{t_{i}} - X_{t_{i-1}}.\n$$\nStarting only from the defining properties of the Brownian motion, compound Poisson process, and their independent increments, determine the asymptotic bias of $\\widehat{b}^{\\,N}$ for estimating $b$ under the above regime.\n\nNext, define a jump-filtered estimator using a vanishing threshold \n$$\nu_{\\Delta} \\;=\\; c\\,\\Delta^{\\alpha}, \\qquad c > 0,\\;\\; \\alpha \\in (0, 1/2),\n$$\nby\n$$\n\\widehat{b}^{\\,JF} \\;=\\; \\frac{1}{T}\\sum_{i=1}^{n} \\Delta X_{i}\\,\\mathbf{1}\\!\\left\\{\\,|\\Delta X_{i}| \\le u_{\\Delta}\\,\\right\\}.\n$$\nUsing only the fundamental distributional properties of the increments, the scaling of Brownian increments, and the law of large numbers for arrays of independent (or conditionally independent) variables, show that $\\widehat{b}^{\\,JF}$ converges in probability to a deterministic limit as $n \\to \\infty$, $\\Delta \\to 0$, $T \\to \\infty$ under the stated growth regime, and identify that limit.\n\nReport your final result as a single row vector containing, in order, the asymptotic bias of $\\widehat{b}^{\\,N}$ and the probability limit of $\\widehat{b}^{\\,JF}$. No rounding is required and no units are needed.", "solution": "The problem asks for two quantities related to the estimation of the drift parameter $b$ for a jump-diffusion process. First, the asymptotic bias of a naive estimator $\\widehat{b}^{\\,N}$, and second, the probability limit of a jump-filtered estimator $\\widehat{b}^{\\,JF}$.\n\nThe SDE increment over an interval of length $\\Delta$ is:\n$$\n\\Delta X_i = X_{t_i} - X_{t_{i-1}} = b\\Delta + \\sigma(W_{t_i} - W_{t_{i-1}}) + (J_{t_i} - J_{t_{i-1}}) = b\\Delta + \\sigma\\Delta W_i + \\Delta J_i.\n$$\nThe increments $\\Delta W_i$ and $\\Delta J_i$ are independent and stationary.\n\n**Part 1: Asymptotic bias of the naive estimator $\\widehat{b}^{\\,N}$**\n\nThe naive estimator is $\\widehat{b}^{\\,N} = \\frac{1}{T}\\sum_{i=1}^{n} \\Delta X_{i} = \\frac{X_T - X_0}{T}$.\nTo find the asymptotic bias, we analyze its behavior as $T \\to \\infty$. By the law of large numbers for semimartingales, $\\widehat{b}^{\\,N}$ converges to its expected value per unit time. We compute the expectation of the estimator:\n$$\n\\mathbb{E}[\\widehat{b}^{\\,N}] = \\frac{1}{n\\Delta} \\sum_{i=1}^{n} \\mathbb{E}[\\Delta X_{i}].\n$$\nThe expectation of a single increment is:\n$$\n\\mathbb{E}[\\Delta X_{1}] = \\mathbb{E}[b\\Delta + \\sigma\\Delta W_1 + \\Delta J_1] = b\\Delta + \\sigma\\mathbb{E}[\\Delta W_1] + \\mathbb{E}[\\Delta J_1].\n$$\nWe know $\\mathbb{E}[\\Delta W_1] = 0$. For the compound Poisson process increment, Wald's identity gives $\\mathbb{E}[\\Delta J_1] = \\mathbb{E}[\\text{number of jumps}]\\times\\mathbb{E}[\\text{jump size}]$. The number of jumps in $[0, \\Delta]$ follows a Poisson distribution with mean $\\lambda\\Delta$. Thus, $\\mathbb{E}[\\Delta J_1] = (\\lambda\\Delta)\\mu_J$.\nSubstituting back:\n$$\n\\mathbb{E}[\\Delta X_{1}] = b\\Delta + \\lambda\\mu_J\\Delta.\n$$\nTherefore, the expectation of the estimator is:\n$$\n\\mathbb{E}[\\widehat{b}^{\\,N}] = \\frac{1}{n\\Delta} n(b\\Delta + \\lambda\\mu_J\\Delta) = b + \\lambda\\mu_J.\n$$\nThe asymptotic bias is the difference between the probability limit of the estimator (which is its expected value) and the true parameter $b$.\nAsymptotic Bias = $(b + \\lambda\\mu_J) - b = \\lambda\\mu_J$.\n\n**Part 2: Probability limit of the jump-filtered estimator $\\widehat{b}^{\\,JF}$**\n\nThe jump-filtered estimator is $\\widehat{b}^{\\,JF} = \\frac{1}{T}\\sum_{i=1}^{n} \\Delta X_{i}\\,\\mathbf{1}\\{|\\Delta X_{i}| \\le u_{\\Delta}\\}$, with $u_{\\Delta} = c\\,\\Delta^{\\alpha}$ for $\\alpha \\in (0, 1/2)$.\nThe key insight is that the threshold $u_\\Delta$ separates increments by their dominant component. As $\\Delta \\to 0$:\n-   Diffusive increments (no jump) are $\\Delta X_i = b\\Delta + \\sigma \\Delta W_i = O_p(\\Delta^{1/2})$.\n-   Jump increments are dominated by the jump size, so $\\Delta X_i = O_p(1)$.\nSince $\\alpha \\in (0, 1/2)$, we have $\\Delta^{1/2} \\ll u_\\Delta \\ll 1$. Thus, for small $\\Delta$, the indicator function $\\mathbf{1}\\{|\\Delta X_i| \\le u_\\Delta\\}$ will be 1 for nearly all diffusive increments and 0 for nearly all jump increments.\n\nBy a law of large numbers for triangular arrays, $\\widehat{b}^{\\,JF}$ converges in probability to the limit of $\\frac{1}{\\Delta}\\mathbb{E}[\\Delta X_1 \\mathbf{1}\\{|\\Delta X_1| \\le u_\\Delta\\}]$.\nWe evaluate the expectation by conditioning on whether a jump occurs in the interval.\n$$\n\\mathbb{E}[\\dots] = \\mathbb{E}[\\dots | \\text{no jump}] P(\\text{no jump}) + \\mathbb{E}[\\dots | \\text{jump}] P(\\text{jump})\n$$\nThe probability of one or more jumps is $P(\\text{jump}) = 1-e^{-\\lambda\\Delta} = \\lambda\\Delta + O(\\Delta^2)$.\nThe probability of no jumps is $P(\\text{no jump}) = e^{-\\lambda\\Delta} = 1-\\lambda\\Delta + O(\\Delta^2)$.\n\nContribution from no-jump intervals:\nThe term is $\\mathbb{E}[(b\\Delta + \\sigma\\Delta W_1)\\mathbf{1}\\{|b\\Delta + \\sigma\\Delta W_1| \\le u_\\Delta\\}]$. As $\\Delta \\to 0$, the probability that $|b\\Delta + \\sigma\\Delta W_1| > u_\\Delta$ is exceedingly small (superpolynomially small in $\\Delta$), so the indicator is effectively 1. The expectation is approximately $\\mathbb{E}[b\\Delta + \\sigma\\Delta W_1] = b\\Delta$. The total contribution is $(b\\Delta + o(\\Delta))(1-\\lambda\\Delta + \\dots) = b\\Delta + O(\\Delta^2)$.\n\nContribution from jump intervals:\nThe term is $\\mathbb{E}[\\Delta X_1 \\mathbf{1}\\{|\\Delta X_1| \\le u_\\Delta\\} | \\text{jump}]$. When a jump occurs, $|\\Delta X_1|$ is $O_p(1)$. The condition $|\\Delta X_1| \\le u_\\Delta$ is very unlikely to be met since $u_\\Delta \\to 0$. The expectation of the truncated variable goes to zero. The total contribution is of order $P(\\text{jump}) \\times o(1) = O(\\Delta) \\times o(1) = o(\\Delta)$.\n\nSumming contributions, the total expectation is $\\mathbb{E}[\\Delta X_1 \\mathbf{1}\\{|\\Delta X_1| \\le u_\\Delta\\}] = b\\Delta + O(\\Delta^2)$.\nThe probability limit of the estimator is:\n$$\n\\text{plim}\\, \\widehat{b}^{\\,JF} = \\lim_{\\Delta \\to 0} \\frac{1}{\\Delta} \\mathbb{E}[\\Delta X_1 \\mathbf{1}\\{|\\Delta X_1| \\le u_\\Delta\\}] = \\lim_{\\Delta \\to 0} \\frac{b\\Delta + O(\\Delta^2)}{\\Delta} = b.\n$$\nThe jump-filtered estimator is consistent for the true drift parameter $b$.\n\nFinal Result:\nThe asymptotic bias of $\\widehat{b}^{\\,N}$ is $\\lambda \\mu_J$.\nThe probability limit of $\\widehat{b}^{\\,JF}$ is $b$.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\lambda \\mu_{J}  b\n\\end{pmatrix}\n}\n$$", "id": "2989868"}]}