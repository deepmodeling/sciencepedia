{"hands_on_practices": [{"introduction": "Stratified sampling is a fundamental variance reduction technique that partitions the sample space to ensure all parts of the distribution are well-represented. By eliminating the randomness associated with how many samples fall into each region, it systematically improves estimator precision. This exercise [@problem_id:3005299] provides a hands-on derivation of the exact variance reduction for a simple two-stratum scheme, allowing you to build a concrete understanding of the method's mechanics from first principles.", "problem": "Consider a scalar Itô Stochastic Differential Equation (SDE), $dX_{s}=\\mu(X_{s},s)\\,ds+\\sigma(X_{s},s)\\,dW_{s}$, with fixed time $t$, state $X_{t}=x$, and a small step $h>0$. The one-step Euler–Maruyama discretization produces\n$$\nX_{t+h}^{\\mathrm{E}} \\;=\\; x+\\mu(x,t)\\,h+\\sigma(x,t)\\,\\sqrt{h}\\,Z,\n$$\nwhere $Z\\sim \\mathcal{N}(0,1)$ is a standard normal random variable. You want to estimate the expectation $\\mathbb{E}[X_{t+h}^{\\mathrm{E}}]$ by Monte Carlo (MC) simulation using $n$ samples.\n\nConstruct a stratified sampling scheme that uses two equal-probability strata for $Z$, namely $\\{Z\\le 0\\}$ and $\\{Z>0\\}$, and proportional allocation (i.e., $n/2$ samples in each stratum). Specify precisely how to sample conditionally within each stratum. Using only fundamental probability facts and definitions, derive the exact variance reduction factor $R$ defined as\n$$\nR \\;=\\; \\frac{\\operatorname{Var}(\\widehat{\\mathbb{E}}_{\\mathrm{crude}})}{\\operatorname{Var}(\\widehat{\\mathbb{E}}_{\\mathrm{strat}})},\n$$\nthe ratio of the variance of the crude MC estimator to that of the stratified estimator, for this particular task of estimating $\\mathbb{E}[X_{t+h}^{\\mathrm{E}}]$.\n\nYour final answer must be a single closed-form analytic expression for $R$, independent of $x$, $\\mu$, $\\sigma$, and $h$. Do not approximate or round your final expression.", "solution": "The problem has been validated and is determined to be a well-posed, scientifically grounded, and formally stated problem in the field of numerical stochastic analysis. It is free of contradictions, ambiguities, or unsound premises. We may therefore proceed with a formal derivation.\n\nLet the random variable to be estimated be denoted by $Y$. Based on the one-step Euler-Maruyama discretization, we have:\n$$\nY \\equiv X_{t+h}^{\\mathrm{E}} = x + \\mu(x,t)h + \\sigma(x,t)\\sqrt{h}Z\n$$\nwhere $Z \\sim \\mathcal{N}(0,1)$. For notational simplicity, we let $\\mu \\equiv \\mu(x,t)$ and $\\sigma \\equiv \\sigma(x,t)$, which are constants with respect to the random variable $Z$. The quantity we wish to estimate is the expectation $\\mathbb{E}[Y]$.\n\nFirst, we analyze the crude Monte Carlo (MC) estimator. The estimator, $\\widehat{\\mathbb{E}}_{\\mathrm{crude}}$, is the sample mean of $n$ independent and identically distributed samples $Y_1, \\dots, Y_n$:\n$$\n\\widehat{\\mathbb{E}}_{\\mathrm{crude}} = \\frac{1}{n} \\sum_{i=1}^{n} Y_i\n$$\nThe variance of this estimator is given by:\n$$\n\\operatorname{Var}(\\widehat{\\mathbb{E}}_{\\mathrm{crude}}) = \\frac{1}{n} \\operatorname{Var}(Y)\n$$\nWe compute the variance of $Y$:\n$$\n\\operatorname{Var}(Y) = \\operatorname{Var}(x + \\mu h + \\sigma \\sqrt{h}Z)\n$$\nSince $x$, $\\mu$, and $h$ are constants, they do not contribute to the variance.\n$$\n\\operatorname{Var}(Y) = \\operatorname{Var}(\\sigma \\sqrt{h}Z) = (\\sigma \\sqrt{h})^2 \\operatorname{Var}(Z)\n$$\nGiven that $Z$ is a standard normal random variable, its variance is $\\operatorname{Var}(Z) = 1$. Therefore:\n$$\n\\operatorname{Var}(Y) = \\sigma^2 h\n$$\nThe variance of the crude MC estimator is thus:\n$$\n\\operatorname{Var}(\\widehat{\\mathbb{E}}_{\\mathrm{crude}}) = \\frac{\\sigma^2 h}{n}\n$$\n\nNext, we analyze the stratified sampling estimator. The sample space of $Z$ is partitioned into two strata: $S_1 = (-\\infty, 0]$ and $S_2 = (0, \\infty)$. Let $\\Phi(z)$ be the cumulative distribution function (CDF) of the standard normal distribution. The probability weights of these strata are:\n$$\np_1 = P(Z \\in S_1) = P(Z \\le 0) = \\Phi(0) = \\frac{1}{2}\n$$\n$$\np_2 = P(Z \\in S_2) = P(Z > 0) = 1 - \\Phi(0) = \\frac{1}{2}\n$$\nThe problem specifies proportional allocation, so the number of samples drawn from each stratum is $n_1 = n p_1 = n/2$ and $n_2 = n p_2 = n/2$.\n\nThe stratified estimator for $\\mathbb{E}[Y]$ is given by:\n$$\n\\widehat{\\mathbb{E}}_{\\mathrm{strat}} = p_1 \\bar{Y}_1 + p_2 \\bar{Y}_2 = \\frac{1}{2} (\\bar{Y}_1 + \\bar{Y}_2)\n$$\nwhere $\\bar{Y}_j$ is the sample mean of $n_j$ samples drawn from stratum $j$. The samples drawn from different strata are independent. The variance of the stratified estimator is:\n$$\n\\operatorname{Var}(\\widehat{\\mathbb{E}}_{\\mathrm{strat}}) = p_1^2 \\operatorname{Var}(\\bar{Y}_1) + p_2^2 \\operatorname{Var}(\\bar{Y}_2) = p_1^2 \\frac{\\sigma_1^2}{n_1} + p_2^2 \\frac{\\sigma_2^2}{n_2}\n$$\nwhere $\\sigma_j^2 = \\operatorname{Var}(Y | Z \\in S_j)$. Substituting $p_j = 1/2$ and $n_j = n/2$:\n$$\n\\operatorname{Var}(\\widehat{\\mathbb{E}}_{\\mathrm{strat}}) = \\frac{(1/2)^2}{n/2} \\sigma_1^2 + \\frac{(1/2)^2}{n/2} \\sigma_2^2 = \\frac{1/4}{n/2} (\\sigma_1^2 + \\sigma_2^2) = \\frac{1}{2n}(\\sigma_1^2 + \\sigma_2^2)\n$$\nTo proceed, we must compute the conditional variances $\\sigma_1^2$ and $\\sigma_2^2$.\n$$\n\\sigma_j^2 = \\operatorname{Var}(Y | Z \\in S_j) = \\operatorname{Var}(x + \\mu h + \\sigma \\sqrt{h}Z | Z \\in S_j) = \\sigma^2 h \\operatorname{Var}(Z | Z \\in S_j)\n$$\nThe conditional variance is found using the formula $\\operatorname{Var}(Z | Z \\in S_j) = \\mathbb{E}[Z^2 | Z \\in S_j] - (\\mathbb{E}[Z | Z \\in S_j])^2$.\n\nLet's compute the conditional moments for stratum $S_1 = (-\\infty, 0]$. Let $\\phi(z) = \\frac{1}{\\sqrt{2\\pi}} \\exp(-z^2/2)$ be the probability density function (PDF) of $Z$. The conditional PDF of $Z$ given $Z \\le 0$ is $f_1(z) = \\frac{\\phi(z)}{p_1} = 2\\phi(z)$ for $z \\le 0$.\n\nThe conditional expectation is:\n$$\n\\mathbb{E}[Z | Z \\le 0] = \\int_{-\\infty}^{0} z (2\\phi(z)) dz = \\frac{2}{\\sqrt{2\\pi}} \\int_{-\\infty}^{0} z \\exp(-z^2/2) dz\n$$\nBy substitution $u = -z^2/2$, $du = -z dz$, the integral becomes:\n$$\n\\mathbb{E}[Z | Z \\le 0] = \\frac{2}{\\sqrt{2\\pi}} \\left[ -\\exp(-z^2/2) \\right]_{-\\infty}^{0} = \\frac{-2}{\\sqrt{2\\pi}} (\\exp(0) - \\lim_{z \\to -\\infty} \\exp(-z^2/2)) = \\frac{-2}{\\sqrt{2\\pi}}(1-0) = -\\sqrt{\\frac{2}{\\pi}}\n$$\nThe conditional second moment is:\n$$\n\\mathbb{E}[Z^2 | Z \\le 0] = \\int_{-\\infty}^{0} z^2 (2\\phi(z)) dz = 2 \\int_{-\\infty}^{0} z^2 \\phi(z) dz\n$$\nSince $\\mathbb{E}[Z^2]=\\operatorname{Var}(Z)=1$, and the a integrand $z^2\\phi(z)$ is an even function, we have $\\int_{-\\infty}^0 z^2 \\phi(z) dz = \\frac{1}{2} \\int_{-\\infty}^{\\infty} z^2 \\phi(z) dz = \\frac{1}{2} \\mathbb{E}[Z^2] = \\frac{1}{2}$.\n$$\n\\mathbb{E}[Z^2 | Z \\le 0] = 2 \\times \\frac{1}{2} = 1\n$$\nThe conditional variance for stratum $S_1$ is:\n$$\n\\operatorname{Var}(Z | Z \\le 0) = \\mathbb{E}[Z^2 | Z \\le 0] - (\\mathbb{E}[Z | Z \\le 0])^2 = 1 - \\left(-\\sqrt{\\frac{2}{\\pi}}\\right)^2 = 1 - \\frac{2}{\\pi}\n$$\nBy symmetry of the normal distribution, $\\mathbb{E}[Z | Z > 0] = \\sqrt{2/\\pi}$ and $\\mathbb{E}[Z^2 | Z > 0] = 1$, which gives the same conditional variance for stratum $S_2$:\n$$\n\\operatorname{Var}(Z | Z > 0) = 1 - \\left(\\sqrt{\\frac{2}{\\pi}}\\right)^2 = 1 - \\frac{2}{\\pi}\n$$\nThus, the conditional variances of $Y$ are equal:\n$$\n\\sigma_1^2 = \\sigma_2^2 = \\sigma^2 h \\left(1 - \\frac{2}{\\pi}\\right)\n$$\nSubstituting this into the expression for the variance of the stratified estimator:\n$$\n\\operatorname{Var}(\\widehat{\\mathbb{E}}_{\\mathrm{strat}}) = \\frac{1}{2n} \\left[ \\sigma^2 h \\left(1 - \\frac{2}{\\pi}\\right) + \\sigma^2 h \\left(1 - \\frac{2}{\\pi}\\right) \\right] = \\frac{1}{2n} \\left[ 2\\sigma^2 h \\left(1 - \\frac{2}{\\pi}\\right) \\right] = \\frac{\\sigma^2 h}{n} \\left(1 - \\frac{2}{\\pi}\\right)\n$$\nAs part of the task, we specify the conditional sampling procedure. Using the inverse transform method, a random variate $Z_1$ from stratum $S_1$ is generated by drawing $U_1 \\sim \\mathcal{U}(0,1)$ and setting $Z_1 = \\Phi^{-1}(U_1/2)$. Similarly, a variate $Z_2$ from $S_2$ is generated from $U_2 \\sim \\mathcal{U}(0,1)$ by setting $Z_2 = \\Phi^{-1}((U_2+1)/2)$. A total of $n/2$ samples are drawn from each stratum independently.\n\nFinally, we compute the variance reduction factor $R$:\n$$\nR = \\frac{\\operatorname{Var}(\\widehat{\\mathbb{E}}_{\\mathrm{crude}})}{\\operatorname{Var}(\\widehat{\\mathbb{E}}_{\\mathrm{strat}})} = \\frac{\\frac{\\sigma^2 h}{n}}{\\frac{\\sigma^2 h}{n} \\left(1 - \\frac{2}{\\pi}\\right)} = \\frac{1}{1 - \\frac{2}{\\pi}}\n$$\nSimplifying the expression:\n$$\nR = \\frac{1}{\\frac{\\pi - 2}{\\pi}} = \\frac{\\pi}{\\pi - 2}\n$$\nThis result is a constant, independent of $x$, $\\mu$, $\\sigma$, and $h$, as required.", "answer": "$$\\boxed{\\frac{\\pi}{\\pi - 2}}$$", "id": "3005299"}, {"introduction": "Control variates exploit correlation with quantities whose expectations are known, and their power is fully realized when using multiple controls. This practice [@problem_id:3005312] transforms the problem into one of optimal linear projection, challenging you to derive the coefficient vector that minimizes the estimator's variance. You will also explore Tikhonov regularization, a crucial method for stabilizing the solution when control variates are nearly collinear, connecting statistical theory with practical numerical linear algebra.", "problem": "Consider an Itô stochastic differential equation (SDE) for a state process $\\{S_t\\}_{t \\in [0,T]}$ under a probability space supporting a standard Brownian motion, with coefficients ensuring existence and uniqueness of a strong solution and finite second moments. Let $X = \\varphi(S_T)$ be a square-integrable payoff with $\\mathbb{E}[|X|^2] < \\infty$, and suppose you are interested in estimating $\\mu = \\mathbb{E}[X]$ by Monte Carlo. You have access to an $m$-dimensional vector of square-integrable control variates $Y = (Y_1,\\dots,Y_m)^{\\top}$ satisfying $\\mathbb{E}[Y] = 0$, with covariance matrix $\\Sigma = \\operatorname{Cov}(Y)$ and cross-covariance vector $c = \\operatorname{Cov}(Y,X)$. Assume $\\Sigma$ is symmetric and nonnegative definite.\n\nYou consider the family of unbiased controlled estimators based on a single path,\n$$\nZ(\\beta) = X - \\beta^{\\top} Y,\n$$\nand the corresponding Monte Carlo estimator based on $n$ independent and identically distributed samples,\n$$\n\\hat{\\mu}_n(\\beta) = \\frac{1}{n}\\sum_{k=1}^{n} \\bigl(X^{(k)} - \\beta^{\\top} Y^{(k)}\\bigr).\n$$\nTreat $\\beta \\in \\mathbb{R}^m$ as a deterministic coefficient vector chosen prior to sampling to minimize the variance of $Z(\\beta)$. Assume all required second moments exist.\n\nTasks:\n1) Starting from the definitions of variance and covariance, and viewing the problem as a least squares projection in $L^2$, derive the unique minimizer $\\beta^{\\star}$ of $\\operatorname{Var}(X - \\beta^{\\top} Y)$ under the assumption that $\\Sigma$ is positive definite.\n\n2) In scenarios where $\\Sigma$ is ill-conditioned, consider the Tikhonov-regularized objective\n$$\nJ_{\\lambda}(\\beta) = \\operatorname{Var}(X - \\beta^{\\top} Y) + \\lambda \\|\\beta\\|_2^2,\\quad \\lambda > 0.\n$$\nDerive the corresponding regularized minimizer $\\beta_{\\lambda}$ in closed form, and interpret how the eigenstructure of $\\Sigma$ affects $\\beta_{\\lambda}$ when $\\Sigma$ has a large condition number. Your derivation must be explicit and rely only on linear algebra and second-moment identities.\n\nProvide, as your final answer, the closed-form expressions for $\\beta^{\\star}$ and $\\beta_{\\lambda}$. Do not provide any numerical evaluation. The final answer must be a single analytic expression without units.", "solution": "This problem asks for the derivation of the optimal coefficient vector $\\beta$ for a control variate estimator, both in the standard case and in a Tikhonov-regularized setting. The core of the problem is to minimize a variance, which is a quadratic function of the coefficients. We will use standard methods from linear algebra and multivariate calculus.\n\nFirst, let us establish the form of the objective function, which is the variance of the controlled estimator $Z(\\beta) = X - \\beta^{\\top} Y$. The vector $\\beta \\in \\mathbb{R}^m$ is deterministic. The random variable is $X$ and the random vector is $Y$. The expectation of the controlled estimator is\n$$\n\\mathbb{E}[Z(\\beta)] = \\mathbb{E}[X - \\beta^{\\top} Y] = \\mathbb{E}[X] - \\beta^{\\top} \\mathbb{E}[Y].\n$$\nGiven that $\\mathbb{E}[Y] = 0$, we have $\\mathbb{E}[Z(\\beta)] = \\mathbb{E}[X] = \\mu$. This confirms that $Z(\\beta)$ is an unbiased estimator of $\\mu$ for any choice of $\\beta$. Our goal is to choose $\\beta$ to minimize the variance of this estimator.\n\nThe variance of $Z(\\beta)$ is given by\n$$\n\\operatorname{Var}(Z(\\beta)) = \\operatorname{Var}(X - \\beta^{\\top} Y).\n$$\nUsing the properties of covariance, specifically its bilinearity, we can expand this expression. Let $V(\\beta) = \\operatorname{Var}(X - \\beta^{\\top} Y)$.\n$$\nV(\\beta) = \\operatorname{Cov}(X - \\beta^{\\top} Y, X - \\beta^{\\top} Y)\n$$\n$$\nV(\\beta) = \\operatorname{Cov}(X, X) - \\operatorname{Cov}(X, \\beta^{\\top} Y) - \\operatorname{Cov}(\\beta^{\\top} Y, X) + \\operatorname{Cov}(\\beta^{\\top} Y, \\beta^{\\top} Y).\n$$\nRecognizing that $\\operatorname{Cov}(A, B) = \\operatorname{Cov}(B, A)^{\\top}$ and that variance is a special case of covariance, $\\operatorname{Var}(A) = \\operatorname{Cov}(A, A)$, we have:\n$$\nV(\\beta) = \\operatorname{Var}(X) - 2 \\operatorname{Cov}(\\beta^{\\top} Y, X) + \\operatorname{Var}(\\beta^{\\top} Y).\n$$\nWe now express the covariance and variance terms using the provided definitions.\nThe term $\\operatorname{Cov}(\\beta^{\\top} Y, X)$ is a scalar. Using properties of covariance with matrix-vector products:\n$$\n\\operatorname{Cov}(\\beta^{\\top} Y, X) = \\beta^{\\top} \\operatorname{Cov}(Y, X) = \\beta^{\\top} c.\n$$\nThe variance term $\\operatorname{Var}(\\beta^{\\top} Y)$ is also a scalar:\n$$\n\\operatorname{Var}(\\beta^{\\top} Y) = \\beta^{\\top} \\operatorname{Cov}(Y, Y) \\beta = \\beta^{\\top} \\Sigma \\beta.\n$$\nSubstituting these back into the expression for $V(\\beta)$, we obtain the objective function as a quadratic form in $\\beta$:\n$$\nV(\\beta) = \\operatorname{Var}(X) - 2 \\beta^{\\top} c + \\beta^{\\top} \\Sigma \\beta.\n$$\nThe term $\\operatorname{Var}(X)$ is a constant with respect to $\\beta$.\n\nThe geometric interpretation in the Hilbert space $L^2$ is that we are finding the best approximation of the centered random variable $X-\\mu$ in the linear subspace spanned by the components of $Y = (Y_1, \\dots, Y_m)^{\\top}$. Minimizing the variance $V(\\beta) = \\operatorname{Var}(X - \\beta^{\\top} Y) = \\mathbb{E}[( (X - \\beta^{\\top} Y) - \\mu )^2] = \\mathbb{E}[((X - \\mu) - \\beta^{\\top} Y)^2]$ is equivalent to finding the orthogonal projection of $X-\\mu$ onto the span of $\\{Y_1, \\dots, Y_m\\}$. The normal equations for this least-squares problem yield the same solution.\n\n### Task 1: Derivation of the optimal minimizer $\\beta^{\\star}$\n\nTo find the vector $\\beta$ that minimizes $V(\\beta)$, we compute the gradient of $V(\\beta)$ with respect to $\\beta$ and set it to zero. Using standard matrix calculus identities, $\\nabla_{\\beta} (\\beta^{\\top} a) = a$ and $\\nabla_{\\beta} (\\beta^{\\top} A \\beta) = 2A\\beta$ for a symmetric matrix $A$:\n$$\n\\nabla_{\\beta} V(\\beta) = \\nabla_{\\beta} (\\operatorname{Var}(X) - 2 \\beta^{\\top} c + \\beta^{\\top} \\Sigma \\beta).\n$$\n$$\n\\nabla_{\\beta} V(\\beta) = 0 - 2c + 2\\Sigma\\beta.\n$$\nSetting the gradient to zero gives the first-order condition for a minimum:\n$$\n-2c + 2\\Sigma\\beta = 0 \\implies \\Sigma\\beta = c.\n$$\nThe problem states that for this task, the covariance matrix $\\Sigma$ is positive definite. A positive definite matrix is, by definition, invertible. Therefore, we can solve for $\\beta$ by left-multiplying by the inverse of $\\Sigma$:\n$$\n\\beta^{\\star} = \\Sigma^{-1} c.\n$$\nTo confirm this is a minimum, we examine the second-order condition by computing the Hessian matrix of $V(\\beta)$:\n$$\n\\nabla^2_{\\beta} V(\\beta) = \\nabla_{\\beta} (-2c + 2\\Sigma\\beta) = 2\\Sigma.\n$$\nSince $\\Sigma$ is positive definite, the Hessian matrix $2\\Sigma$ is also positive definite. This confirms that $V(\\beta)$ is a strictly convex function and that $\\beta^{\\star}$ is its unique global minimizer.\n\n### Task 2: Derivation of the regularized minimizer $\\beta_{\\lambda}$\n\nNow we consider the Tikhonov-regularized objective function for $\\lambda > 0$:\n$$\nJ_{\\lambda}(\\beta) = \\operatorname{Var}(X - \\beta^{\\top} Y) + \\lambda \\|\\beta\\|_2^2.\n$$\nSubstituting the expression for the variance term and noting that $\\|\\beta\\|_2^2 = \\beta^{\\top}\\beta = \\beta^{\\top}I\\beta$ where $I$ is the $m \\times m$ identity matrix, we get:\n$$\nJ_{\\lambda}(\\beta) = (\\operatorname{Var(X)} - 2 \\beta^{\\top} c + \\beta^{\\top} \\Sigma \\beta) + \\lambda \\beta^{\\top} I \\beta.\n$$\nCombining the quadratic terms in $\\beta$:\n$$\nJ_{\\lambda}(\\beta) = \\operatorname{Var}(X) - 2 \\beta^{\\top} c + \\beta^{\\top} (\\Sigma + \\lambda I) \\beta.\n$$\nThis is again a quadratic function of $\\beta$. We find the minimizer by setting its gradient to zero:\n$$\n\\nabla_{\\beta} J_{\\lambda}(\\beta) = \\nabla_{\\beta} (\\operatorname{Var}(X) - 2 \\beta^{\\top} c + \\beta^{\\top} (\\Sigma + \\lambda I) \\beta).\n$$\nThe matrix $\\Sigma + \\lambda I$ is symmetric since $\\Sigma$ is symmetric. Thus, we can apply the same gradient rules:\n$$\n\\nabla_{\\beta} J_{\\lambda}(\\beta) = -2c + 2(\\Sigma + \\lambda I) \\beta.\n$$\nSetting the gradient to zero gives the first-order condition:\n$$\n-2c + 2(\\Sigma + \\lambda I) \\beta = 0 \\implies (\\Sigma + \\lambda I) \\beta = c.\n$$\nTo solve for $\\beta$, we must invert the matrix $(\\Sigma + \\lambda I)$. $\\Sigma$ is a covariance matrix, so it is symmetric and positive semi-definite. Let the eigenvalues of $\\Sigma$ be $\\sigma_i \\ge 0$ for $i=1, \\dots, m$. The matrix $\\lambda I$ has all eigenvalues equal to $\\lambda$. The eigenvalues of the sum $\\Sigma + \\lambda I$ are $\\sigma_i + \\lambda$. Since $\\lambda > 0$, all eigenvalues $\\sigma_i + \\lambda$ are strictly positive. A symmetric matrix with strictly positive eigenvalues is positive definite and thus invertible. Therefore, we can uniquely solve for the regularized minimizer, $\\beta_{\\lambda}$:\n$$\n\\beta_{\\lambda} = (\\Sigma + \\lambda I)^{-1} c.\n$$\nThe Hessian of $J_{\\lambda}(\\beta)$ is $2(\\Sigma + \\lambda I)$, which is positive definite, confirming that $\\beta_{\\lambda}$ is the unique global minimizer.\n\n### Interpretation of the effect of the eigenstructure\n\nLet $\\Sigma = U D U^{\\top}$ be the spectral decomposition of $\\Sigma$, where $U$ is an orthogonal matrix whose columns are the eigenvectors of $\\Sigma$, and $D$ is the diagonal matrix of corresponding non-negative eigenvalues $\\sigma_1 \\ge \\sigma_2 \\ge \\dots \\ge \\sigma_m \\ge 0$.\n\nIf $\\Sigma$ is invertible (all $\\sigma_i > 0$), the unregularized solution is $\\beta^{\\star} = (UDU^{\\top})^{-1}c = UD^{-1}U^{\\top}c$.\nThe regularized solution is $\\beta_{\\lambda} = (UDU^{\\top} + \\lambda I)^{-1}c = (U(D+\\lambda I)U^{\\top})^{-1}c = U(D+\\lambda I)^{-1}U^{\\top}c$.\n\nLet's analyze the solutions in the basis of the eigenvectors of $\\Sigma$. Let $c' = U^{\\top}c$ be the representation of the covariance vector $c$ in this basis.\nThe unregularized solution in this basis is $U^{\\top}\\beta^{\\star} = D^{-1}c'$. The $i$-th component is $(U^{\\top}\\beta^{\\star})_i = c'_i / \\sigma_i$.\nThe regularized solution in this basis is $U^{\\top}\\beta_{\\lambda} = (D+\\lambda I)^{-1}c'$. The $i$-th component is $(U^{\\top}\\beta_{\\lambda})_i = c'_i / (\\sigma_i + \\lambda)$.\n\nWhen $\\Sigma$ is ill-conditioned, its condition number $\\kappa(\\Sigma) = \\sigma_1/\\sigma_m$ is very large, implying that at least one eigenvalue $\\sigma_m$ is close to zero.\nFor the unregularized solution $\\beta^{\\star}$, the components corresponding to small eigenvalues $\\sigma_i$ are scaled by a large factor $1/\\sigma_i$. This can lead to a solution vector $\\beta^{\\star}$ with an extremely large Euclidean norm, making the estimator highly sensitive to noise or estimation errors in $c$ (which is typically estimated from a pilot sample).\n\nThe Tikhonov regularization addresses this issue. The scaling factor for the regularized solution $\\beta_{\\lambda}$ is $1/(\\sigma_i + \\lambda)$. Since $\\lambda > 0$, this denominator is bounded below by $\\lambda$. For eigenvectors associated with small eigenvalues ($\\sigma_i \\approx 0$), the scaling factor is approximately $1/\\lambda$, which is finite and controlled by the choice of $\\lambda$. For eigenvectors associated with large eigenvalues ($\\sigma_i \\gg \\lambda$), the scaling factor is $1/(\\sigma_i + \\lambda) \\approx 1/\\sigma_i$, which is close to the unregularized case.\nIn essence, the regularization \"dampens\" the components of the solution along the directions where the control variates have low variance (the directions of eigenvectors with small eigenvalues), preventing the solution from \"blowing up\". This introduces a bias in the estimator for $\\beta$ but significantly reduces its variance, a classic bias-variance trade-off. It stabilizes the solution in the face of ill-conditioning.", "answer": "$$ \\boxed{ \\begin{pmatrix} \\Sigma^{-1} c & (\\Sigma + \\lambda I)^{-1} c \\end{pmatrix} } $$", "id": "3005312"}, {"introduction": "Antithetic variates leverage symmetry in the underlying random drivers to reduce variance, but their effectiveness is not universal. This exercise [@problem_id:3005274] pushes you to critically analyze the conditions under which this technique can fail, particularly for non-monotone path-dependent functionals. By focusing on the underlying covariance structure, you will develop a robust diagnostic intuition for when antithetic sampling is truly beneficial.", "problem": "Consider a scalar Itô stochastic differential equation (SDE) with constant coefficients, $dX_t = \\mu \\, dt + \\sigma \\, dW_t$, where $W_t$ is a standard Brownian motion and $\\mu, \\sigma \\in \\mathbb{R}$ are constants with $\\sigma \\neq 0$. Fix a time horizon $T > 0$, an integer $m \\ge 1$, and a uniform grid $t_k = k \\Delta t$ with $\\Delta t = T/m$. The Euler–Maruyama scheme with standardized independent Gaussian increments $Z_k \\sim \\mathcal{N}(0,1)$, $k = 1, \\dots, m$, yields the discrete dynamics $X_{t_k} = X_{t_{k-1}} + \\mu \\Delta t + \\sigma \\sqrt{\\Delta t} \\, Z_k$, with $X_{t_0} = X_0 \\in \\mathbb{R}$. Let $Z = (Z_1, \\dots, Z_m)$ denote the vector of standardized increments and let $\\widetilde{Z} = -Z$ denote the antithetic vector. The antithetic path $(\\widetilde{X}_{t_k})$ is obtained by replacing each $Z_k$ by $\\widetilde{Z}_k = -Z_k$ at every time step, so that the antithetic path is defined recursively by $\\widetilde{X}_{t_k} = \\widetilde{X}_{t_{k-1}} + \\mu \\Delta t + \\sigma \\sqrt{\\Delta t} \\, (-Z_k)$ for $k \\ge 1$, with $\\widetilde{X}_{t_0} = X_0$.\n\nLet $F$ be a square-integrable path functional of the entire path $(X_{t_k})_{k=0}^m$, and define $Y = F(X_{t_0}, \\dots, X_{t_m})$ and $Y' = F(\\widetilde{X}_{t_0}, \\dots, \\widetilde{X}_{t_m})$. A practitioner considers variance reduction via antithetic variates across time steps by using the per-pair average $A = \\frac{1}{2}(Y + Y')$ in place of two independent realizations. The practitioner is interested in understanding when antithetic pairing across time steps fails to reduce variance for non-monotone path functionals and in formulating a practical, computable diagnostic based on covariance signs.\n\nTo anchor ideas, consider the realized quadratic variation functional $F_{\\mathrm{RV}}$ defined by\n$$\nF_{\\mathrm{RV}}(X_{t_0}, \\dots, X_{t_m}) = \\sum_{k=1}^m \\left(X_{t_k} - X_{t_{k-1}}\\right)^2,\n$$\nand note that when $\\mu = 0$, one has $X_{t_k} - X_{t_{k-1}} = \\sigma \\sqrt{\\Delta t} \\, Z_k$.\n\nSelect all statements that are correct:\n\nA. When $\\mu = 0$, antithetic pairing across time steps for estimating $\\mathbb{E}[F_{\\mathrm{RV}}]$ fails to reduce variance relative to using two independent samples; indeed $Y' = Y$ almost surely, so $\\operatorname{Cov}(Y,Y') = \\operatorname{Var}(Y) \\ge 0$ and the per-pair antithetic average $A$ has the same variance as a single sample.\n\nB. For any square-integrable path functional $F$ of the Euler–Maruyama path driven by $Z$, the antithetic estimator $A = \\frac{1}{2}(F(Z) + F(-Z))$ has lower variance than the average of two independent samples if and only if $\\operatorname{Cov}(F(Z), F(-Z)) < 0$. Equivalently, antithetic pairing fails (relative to two independent samples) whenever $\\operatorname{Cov}(F(Z), F(-Z)) \\ge 0$.\n\nC. If $F$ is coordinate-wise monotone increasing in each $Z_k$ for fixed past, then $\\operatorname{Cov}(F(Z), F(-Z))$ is strictly positive, so antithetic pairing necessarily fails.\n\nD. In the additive case $F(Z) = \\sum_{k=1}^m g_k(Z_k)$ with $Z_k$ independent and each $g_k$ square-integrable, antithetic pairing across time steps reduces variance compared to two independent samples if and only if $\\sum_{k=1}^m \\operatorname{Cov}\\!\\big(g_k(Z_k), g_k(-Z_k)\\big) < 0$; otherwise it fails.\n\nE. A practical stepwise diagnostic for non-monotone functionals is to compute empirical signs of $\\operatorname{Cov}(\\Delta_k, \\Delta_k^{(A)})$ across $k = 1, \\dots, m$, where $\\Delta_k$ denotes the contribution of the $k$-th time step to $F$ (in an additive or incremental decomposition) and $\\Delta_k^{(A)}$ denotes the corresponding contribution under antithetic pairing. A predominance of positive empirical covariance signs indicates likely failure of variance reduction, whereas a predominance of negative signs indicates likely success.\n\nYour task is to determine which statements are correct, providing justification from first principles based on definitions of variance, covariance, and the structure of the Euler–Maruyama scheme and antithetic variates across time steps. In particular, analyze when antithetic pairing fails for non-monotone path functionals and articulate the covariance-sign diagnostic rigorously (and, when applicable, its stepwise specialization for additive functionals). Avoid assuming any shortcut formulas beyond the foundational definitions and the Euler–Maruyama discretization given above. Provide clear reasoning that is scientifically realistic and self-consistent.", "solution": "The goal of antithetic variates is to reduce the variance of a Monte Carlo estimator. The standard estimator for $\\mathbb{E}[Y]$ using the average of two independent samples, $Y_1$ and $Y_2$, is $M = \\frac{1}{2}(Y_1 + Y_2)$. Its variance is:\n$$\n\\operatorname{Var}(M) = \\operatorname{Var}\\left(\\frac{1}{2}(Y_1 + Y_2)\\right) = \\frac{1}{4} \\left(\\operatorname{Var}(Y_1) + \\operatorname{Var}(Y_2)\\right) = \\frac{1}{4} (2 \\operatorname{Var}(Y)) = \\frac{1}{2}\\operatorname{Var}(Y)\n$$\nsince $Y_1, Y_2$ are IID copies of $Y$.\n\nThe antithetic estimator is $A = \\frac{1}{2}(Y + Y')$, where $Y = F(\\text{Path}(Z))$ and $Y' = F(\\text{Path}(-Z))$. Its variance is:\n$$\n\\operatorname{Var}(A) = \\operatorname{Var}\\left(\\frac{1}{2}(Y + Y')\\right) = \\frac{1}{4} \\operatorname{Var}(Y + Y') = \\frac{1}{4} (\\operatorname{Var}(Y) + \\operatorname{Var}(Y') + 2 \\operatorname{Cov}(Y, Y'))\n$$\nSince $Z_k \\sim \\mathcal{N}(0,1)$, its distribution is symmetric, so $Z_k$ and $-Z_k$ are identically distributed. Consequently, the random vectors $Z = (Z_1, \\dots, Z_m)$ and $-Z = (-Z_1, \\dots, -Z_m)$ are identically distributed. This implies that the paths they generate have the same distribution, and thus $Y$ and $Y'$ are identically distributed. Therefore, $\\operatorname{Var}(Y) = \\operatorname{Var}(Y')$. The variance of the antithetic estimator becomes:\n$$\n\\operatorname{Var}(A) = \\frac{1}{4} (2 \\operatorname{Var}(Y) + 2 \\operatorname{Cov}(Y, Y')) = \\frac{1}{2} (\\operatorname{Var}(Y) + \\operatorname{Cov}(Y, Y'))\n$$\nVariance reduction is achieved if $\\operatorname{Var}(A) < \\operatorname{Var}(M)$. This gives:\n$$\n\\frac{1}{2} (\\operatorname{Var}(Y) + \\operatorname{Cov}(Y, Y')) < \\frac{1}{2} \\operatorname{Var}(Y) \\iff \\operatorname{Cov}(Y, Y') < 0\n$$\nAntithetic pairing fails to reduce variance (i.e., is either ineffective or detrimental) if $\\operatorname{Var}(A) \\ge \\operatorname{Var}(M)$, which is equivalent to $\\operatorname{Cov}(Y, Y') \\ge 0$.\n\n### Option-by-Option Analysis\n\n**A. When $\\mu = 0$, antithetic pairing across time steps for estimating $\\mathbb{E}[F_{\\mathrm{RV}}]$ fails to reduce variance relative to using two independent samples; indeed $Y' = Y$ almost surely, so $\\operatorname{Cov}(Y,Y') = \\operatorname{Var}(Y) \\ge 0$ and the per-pair antithetic average $A$ has the same variance as a single sample.**\n\nLet $Y = F_{\\mathrm{RV}}(X) = \\sum_{k=1}^m (X_{t_k} - X_{t_{k-1}})^2$.\nWhen $\\mu = 0$, the EM increment is $X_{t_k} - X_{t_{k-1}} = \\sigma \\sqrt{\\Delta t} Z_k$.\nSo, $Y = \\sum_{k=1}^m (\\sigma \\sqrt{\\Delta t} Z_k)^2 = \\sigma^2 \\Delta t \\sum_{k=1}^m Z_k^2$.\nFor the antithetic path, the increment is $\\widetilde{X}_{t_k} - \\widetilde{X}_{t_{k-1}} = \\sigma \\sqrt{\\Delta t} (-Z_k)$.\nThe antithetic functional value is $Y' = F_{\\mathrm{RV}}(\\widetilde{X}) = \\sum_{k=1}^m (\\widetilde{X}_{t_k} - \\widetilde{X}_{t_{k-1}})^2$.\n$Y' = \\sum_{k=1}^m (\\sigma \\sqrt{\\Delta t} (-Z_k))^2 = \\sum_{k=1}^m \\sigma^2 \\Delta t (-Z_k)^2 = \\sigma^2 \\Delta t \\sum_{k=1}^m Z_k^2 = Y$.\nThus, $Y' = Y$ almost surely.\nThe covariance is $\\operatorname{Cov}(Y, Y') = \\operatorname{Cov}(Y, Y) = \\operatorname{Var}(Y)$. Since $\\sigma \\neq 0$ and $m \\ge 1$, $Y$ is a non-constant random variable, so $\\operatorname{Var}(Y) > 0$. The condition for failure, $\\operatorname{Cov}(Y, Y') \\ge 0$, is met.\nThe antithetic estimator is $A = \\frac{1}{2}(Y + Y') = \\frac{1}{2}(Y + Y) = Y$.\nThe variance of this estimator is $\\operatorname{Var}(A) = \\operatorname{Var}(Y)$.\nThe variance of a single sample is indeed $\\operatorname{Var}(Y)$. So the statement that $A$ has the same variance as a single sample is correct.\nThe variance of the average of two independent samples is $\\frac{1}{2} \\operatorname{Var}(Y)$.\nSince $\\operatorname{Var}(A) = \\operatorname{Var}(Y) > \\frac{1}{2}\\operatorname{Var}(Y)$, the technique not only fails to reduce variance but actually increases it compared to using two independent samples. The statement is entirely correct.\n\nVerdict: **Correct**.\n\n**B. For any square-integrable path functional $F$ of the Euler–Maruyama path driven by $Z$, the antithetic estimator $A = \\frac{1}{2}(F(Z) + F(-Z))$ has lower variance than the average of two independent samples if and only if $\\operatorname{Cov}(F(Z), F(-Z)) < 0$. Equivalently, antithetic pairing fails (relative to two independent samples) whenever $\\operatorname{Cov}(F(Z), F(-Z)) \\ge 0$.**\n\nThis statement summarizes the fundamental condition for the effectiveness of antithetic variates. As derived in the preliminary analysis:\n$\\operatorname{Var}(A) < \\operatorname{Var}(\\text{average of two indep. samples})$\n$\\iff \\frac{1}{2} (\\operatorname{Var}(Y) + \\operatorname{Cov}(Y, Y')) < \\frac{1}{2} \\operatorname{Var}(Y)$\n$\\iff \\operatorname{Cov}(Y, Y') < 0$.\nThe failure condition is the logical negation, $\\operatorname{Cov}(Y, Y') \\ge 0$. The statement is a precise and correct formulation of this principle.\n\nVerdict: **Correct**.\n\n**C. If $F$ is coordinate-wise monotone increasing in each $Z_k$ for fixed past, then $\\operatorname{Cov}(F(Z), F(-Z))$ is strictly positive, so antithetic pairing necessarily fails.**\n\nLet $Y = g(Z_1, \\dots, Z_m)$ where the function $g$ represents the evaluation of the functional $F$ on the path generated by $Z$. If $F$ is monotone increasing in each $Z_k$, then $g$ is a non-decreasing function of its arguments. Let's analyze the covariance term, $\\operatorname{Cov}(g(Z), g(-Z))$.\nLet the function $h$ be defined as $h(Z) = g(-Z)$. Since $g$ is non-decreasing in each component, $h$ must be non-increasing in each component.\nA fundamental result in probability theory, a consequence of Chebyshev's covariance inequality, states that for a random vector $Z$ and two functions $f_1, f_2$, if $f_1$ is non-decreasing and $f_2$ is non-increasing, then $\\operatorname{Cov}(f_1(Z), f_2(Z)) \\le 0$.\nApplying this result with $f_1 = g$ and $f_2 = h$, we get:\n$$\n\\operatorname{Cov}(g(Z), h(Z)) = \\operatorname{Cov}(g(Z), g(-Z)) \\le 0.\n$$\nThe statement claims the covariance is strictly positive. This is the opposite of the correct result. A non-positive covariance indicates that antithetic pairing will be successful (or at worst, neutral). For most non-trivial monotone functions, the covariance will be strictly negative, guaranteeing variance reduction.\n\nVerdict: **Incorrect**.\n\n**D. In the additive case $F(Z) = \\sum_{k=1}^m g_k(Z_k)$ with $Z_k$ independent and each $g_k$ square-integrable, antithetic pairing across time steps reduces variance compared to two independent samples if and only if $\\sum_{k=1}^m \\operatorname{Cov}\\!\\big(g_k(Z_k), g_k(-Z_k)\\big) < 0$; otherwise it fails.**\n\nLet $Y = F(Z) = \\sum_{k=1}^m g_k(Z_k)$ and $Y' = F(-Z) = \\sum_{k=1}^m g_k(-Z_k)$.\nWe need to find the condition for $\\operatorname{Cov}(Y, Y') < 0$. Using the bilinearity of covariance:\n$$\n\\operatorname{Cov}(Y, Y') = \\operatorname{Cov}\\left(\\sum_{i=1}^m g_i(Z_i), \\sum_{j=1}^m g_j(-Z_j)\\right) = \\sum_{i=1}^m \\sum_{j=1}^m \\operatorname{Cov}(g_i(Z_i), g_j(-Z_j)).\n$$\nThe increments $Z_k$ are independent by definition. For any $i \\neq j$, $Z_i$ and $Z_j$ are independent. Therefore, any functions of them, $g_i(Z_i)$ and $g_j(-Z_j)$, are also independent. The covariance of independent random variables is zero.\n$$\n\\operatorname{Cov}(g_i(Z_i), g_j(-Z_j)) = 0 \\quad \\text{for } i \\neq j.\n$$\nThe double summation thus collapses to the terms where $i=j$:\n$$\n\\operatorname{Cov}(Y, Y') = \\sum_{k=1}^m \\operatorname{Cov}(g_k(Z_k), g_k(-Z_k)).\n$$\nFrom the general principle (Option B), variance reduction occurs if and only if $\\operatorname{Cov}(Y, Y') < 0$. Therefore, for this additive functional, the condition is precisely $\\sum_{k=1}^m \\operatorname{Cov}(g_k(Z_k), g_k(-Z_k)) < 0$. Otherwise, it fails. The statement is correct.\n\nVerdict: **Correct**.\n\n**E. A practical stepwise diagnostic for non-monotone functionals is to compute empirical signs of $\\operatorname{Cov}(\\Delta_k, \\Delta_k^{(A)})$ across $k = 1, \\dots, m$, where $\\Delta_k$ denotes the contribution of the $k$-th time step to $F$ (in an additive or incremental decomposition) and $\\Delta_k^{(A)}$ denotes the corresponding contribution under antithetic pairing. A predominance of positive empirical covariance signs indicates likely failure of variance reduction, whereas a predominance of negative signs indicates likely success.**\n\nThis statement proposes a heuristic. For a general path functional $F$, one might be able to express it, at least approximately, as a sum of contributions from each time step: $F \\approx \\sum_{k=1}^m \\Delta_k$. Here $\\Delta_k$ is the contribution from step $k$, and $\\Delta_k^{(A)}$ is its antithetic counterpart. From the analysis in Option D, if $F$ were exactly this sum and the $\\Delta_k$ terms depended only on independent increments, the total covariance would be the sum of $\\operatorname{Cov}(\\Delta_k, \\Delta_k^{(A)})$.\nFor a general path-dependent functional, $\\Delta_k$ would depend on $(Z_1, \\dots, Z_k)$, and the cross-covariance terms $\\operatorname{Cov}(\\Delta_i, \\Delta_j^{(A)})$ for $i \\neq j$ are not necessarily zero. The total covariance is $\\operatorname{Cov}(Y, Y') = \\sum_{i,j} \\operatorname{Cov}(\\Delta_i, \\Delta_j^{(A)})$.\nHowever, the diagonal terms, $\\operatorname{Cov}(\\Delta_k, \\Delta_k^{(A)})$, are often dominant. The influence of an early shock $Z_i$ on a late contribution $\\Delta_j$ (with $j \\gg i$) might be weak in many SDE applications. Thus, examining the signs of the diagonal terms is a highly reasonable \"practical diagnostic\". If most $\\operatorname{Cov}(\\Delta_k, \\Delta_k^{(A)})$ are positive, it is plausible (\"likely\") that their sum (and the full sum including off-diagonal terms) will be positive, leading to failure. Conversely, a predominance of negative signs suggests likely success. The phrasing \"practical stepwise diagnostic\", \"likely failure/success\", and \"predominance\" correctly captures the heuristic nature of this approach for general functionals. This is a sound and standard method of analysis in computational practice.\n\nVerdict: **Correct**.", "answer": "$$\\boxed{ABDE}$$", "id": "3005274"}]}