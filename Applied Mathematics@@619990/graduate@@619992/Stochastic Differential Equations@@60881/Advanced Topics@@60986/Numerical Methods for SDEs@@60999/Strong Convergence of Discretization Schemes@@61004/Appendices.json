{"hands_on_practices": [{"introduction": "To build a solid foundation in the numerical analysis of SDEs, we begin by dissecting the simplest and most widely known method: the Euler-Maruyama scheme. This first practice requires you to look \"under the hood\" of the scheme by analyzing its local truncation error. By systematically comparing the numerical increment to the exact solution's increment over a single step, you will identify the dominant source of error, which provides a rigorous explanation for the method's strong convergence order of $0.5$ and motivates the development of more accurate schemes [@problem_id:2998788].", "problem": "Consider the scalar stochastic differential equation (SDE) in Itô form\n$$\n\\mathrm{d}X_{t} \\;=\\; \\mu\\, X_{t}^{3}\\,\\mathrm{d}t \\;+\\; \\sigma\\, X_{t}^{2}\\,\\mathrm{d}W_{t},\n$$\nwhere $\\mu \\in \\mathbb{R}$ and $\\sigma \\in \\mathbb{R}$ are constants and $\\{W_{t}\\}_{t \\ge 0}$ is a standard Brownian motion. Fix a time $t \\ge 0$ and denote the exact state by $x := X_{t}$. For a time step of size $h > 0$, the Euler-Maruyama (EM) increment over $[t, t+h]$ is\n$$\nx \\;+\\; \\mu\\, x^{3}\\, h \\;+\\; \\sigma\\, x^{2}\\, \\Delta W,\n$$\nwhere $\\Delta W := W_{t+h} - W_{t}$.\n\nDefine the one-step local truncation error (the defect of the EM step when started from the exact state) by\n$$\n\\tau_{h}(x) \\;:=\\; \\bigl(X_{t+h} - x\\bigr) \\;-\\; \\bigl(\\mu\\, x^{3}\\, h \\;+\\; \\sigma\\, x^{2}\\, \\Delta W\\bigr).\n$$\n\nStarting from the integral formulation of the SDE and using Itô calculus from first principles, identify the leading-order term of $\\tau_{h}(x)$ in the mean-square sense by comparing the exact increment to the EM increment. In particular, isolate the contribution in $\\tau_{h}(x)$ that is proportional to $((\\Delta W)^{2} - h)$ and determine its coefficient as a closed-form analytic expression depending only on $x$ and $\\sigma$.\n\nYour final answer must be the single analytic expression for this coefficient. No rounding is required, and no units are involved.", "solution": "The problem as stated is mathematically and scientifically valid. It is a well-posed question in the field of numerical analysis for stochastic differential equations, specifically concerning the local truncation error of the Euler-Maruyama method. All necessary definitions and conditions are provided, and there are no contradictions or ambiguities. We may proceed with the solution.\n\nThe scalar stochastic differential equation (SDE) is given in Itô form as\n$$\n\\mathrm{d}X_{t} \\;=\\; \\mu\\, X_{t}^{3}\\,\\mathrm{d}t \\;+\\; \\sigma\\, X_{t}^{2}\\,\\mathrm{d}W_{t}.\n$$\nThis corresponds to the general form $\\mathrm{d}X_t = a(X_t)\\mathrm{d}t + b(X_t)\\mathrm{d}W_t$ with drift coefficient $a(x) = \\mu x^{3}$ and diffusion coefficient $b(x) = \\sigma x^{2}$. We are given the initial state $X_{t} = x$.\n\nThe one-step local truncation error, $\\tau_{h}(x)$, is defined as the difference between the exact increment $X_{t+h} - x$ and the Euler-Maruyama (EM) increment over the time step $h > 0$:\n$$\n\\tau_{h}(x) \\;=\\; \\bigl(X_{t+h} - x\\bigr) \\;-\\; \\bigl(\\mu\\, x^{3}\\, h \\;+\\; \\sigma\\, x^{2}\\, \\Delta W\\bigr),\n$$\nwhere $\\Delta W = W_{t+h} - W_t$.\n\nTo analyze $\\tau_{h}(x)$, we first express the exact increment using the integral form of the SDE:\n$$\nX_{t+h} - x = \\int_{t}^{t+h} \\mu\\, X_{s}^{3}\\,\\mathrm{d}s + \\int_{t}^{t+h} \\sigma\\, X_{s}^{2}\\,\\mathrm{d}W_{s}.\n$$\nSubstituting this into the definition of $\\tau_{h}(x)$ and recognizing that $\\mu x^3 = a(x)$ and $\\sigma x^2 = b(x)$, we get:\n$$\n\\tau_{h}(x) \\;=\\; \\left(\\int_{t}^{t+h} a(X_{s})\\,\\mathrm{d}s + \\int_{t}^{t+h} b(X_{s})\\,\\mathrm{d}W_{s}\\right) \\;-\\; \\bigl(a(x)\\,h \\;+\\; b(x)\\,\\Delta W\\bigr).\n$$\nSince $h = \\int_t^{t+h} \\mathrm{d}s$ and $\\Delta W = \\int_t^{t+h} \\mathrm{d}W_s$, we can rewrite the error as:\n$$\n\\tau_{h}(x) \\;=\\; \\int_{t}^{t+h} \\bigl(a(X_{s}) - a(x)\\bigr)\\,\\mathrm{d}s \\;+\\; \\int_{t}^{t+h} \\bigl(b(X_{s}) - b(x)\\bigr)\\,\\mathrm{d}W_{s}.\n$$\nTo find the leading-order terms of $\\tau_{h}(x)$, we must expand the integrands $a(X_{s}) - a(x)$ and $b(X_{s}) - b(x)$ for $s \\in [t, t+h]$. We use a Taylor expansion for the function $b(X_s)$ around the point $x = X_t$:\n$$\nb(X_{s}) = b(x) + b'(x)(X_{s}-x) + \\frac{1}{2}b''(x)(X_{s}-x)^{2} + \\dots\n$$\nwhere $b'(x)$ and $b''(x)$ are the first and second derivatives of $b$ with respect to its argument, evaluated at $x$.\n\nFor small time increments $h$, the process $X_s$ does not deviate far from its initial value $x$. The leading-order approximation for the deviation $X_s-x$ over the interval $[t, s]$ is given by the EM increment itself:\n$$\nX_{s}-x \\approx a(x)(s-t) + b(x)(W_s - W_t).\n$$\nIn the mean-square sense, the dominant term in this approximation is the stochastic one, $b(x)(W_s - W_t)$, which is of order $(s-t)^{1/2}$, while the deterministic term is of order $s-t$.\n\nLet us focus on the second integral in the expression for $\\tau_h(x)$, as this is where Itô integrals of the specified form originate.\n$$\n\\int_{t}^{t+h} \\bigl(b(X_{s}) - b(x)\\bigr)\\,\\mathrm{d}W_{s} = \\int_{t}^{t+h} \\left( b'(x)(X_{s}-x) + \\frac{1}{2}b''(x)(X_{s}-x)^{2} + \\dots \\right)\\,\\mathrm{d}W_{s}.\n$$\nSubstituting the leading-order approximation $X_s - x \\approx b(x)(W_s - W_t)$ into the first term of this expansion yields:\n$$\n\\int_{t}^{t+h} b'(x)(X_{s}-x)\\,\\mathrm{d}W_{s} \\approx \\int_{t}^{t+h} b'(x) \\bigl( b(x)(W_s - W_t) \\bigr)\\,\\mathrm{d}W_{s} = b(x)b'(x) \\int_{t}^{t+h} (W_s - W_t)\\,\\mathrm{d}W_{s}.\n$$\nThe Itô integral $\\int_{t}^{t+h} (W_s - W_t)\\,\\mathrm{d}W_{s}$ is a standard result. It can be evaluated using Itô's lemma on the function $f(w) = \\frac{1}{2}w^2$. Let's apply Itô's lemma to $f(W_s-W_t) = \\frac{1}{2}(W_s-W_t)^2$.\nThen $\\mathrm{d}f = (W_s-W_t)\\mathrm{d}W_s + \\frac{1}{2}(1)\\mathrm{d}s$. Integrating from $t$ to $t+h$ gives:\n$$\n\\frac{1}{2}(W_{t+h}-W_t)^2 - \\frac{1}{2}(W_t-W_t)^2 = \\int_{t}^{t+h} (W_s - W_t)\\,\\mathrm{d}W_{s} + \\frac{1}{2}\\int_{t}^{t+h}\\mathrm{d}s.\n$$\n$$\n\\frac{1}{2}(\\Delta W)^2 = \\int_{t}^{t+h} (W_s - W_t)\\,\\mathrm{d}W_{s} + \\frac{1}{2}h.\n$$\nRearranging gives the identity:\n$$\n\\int_{t}^{t+h} (W_s - W_t)\\,\\mathrm{d}W_{s} = \\frac{1}{2}\\bigl( (\\Delta W)^2 - h \\bigr).\n$$\nThus, the corresponding term in the expansion of $\\tau_h(x)$ is:\n$$\nb(x)b'(x) \\cdot \\frac{1}{2}\\bigl( (\\Delta W)^2 - h \\bigr) = \\frac{1}{2}b(x)b'(x) \\bigl( (\\Delta W)^2 - h \\bigr).\n$$\nOther terms in the expansion of $\\tau_h(x)$ are of a different form or higher order. For instance, the integral $\\int_t^{t+h} (a(X_s)-a(x))\\mathrm{d}s$ gives rise to terms proportional to $h^2$ and the iterated integral $\\int_t^{t+h}(W_s-W_t)\\mathrm{d}s$, neither of which is proportional to $((\\Delta W)^2-h)$. Higher-order terms in the expansion of $\\int (b(X_s)-b(x))\\mathrm{d}W_s$ involve different stochastic integrals, such as $\\int (W_s-W_t)^2 \\mathrm{d}W_s$, which are also not of the required form.\n\nTherefore, the contribution in $\\tau_{h}(x)$ that is proportional to $((\\Delta W)^{2} - h)$ is uniquely identified, and its coefficient is $\\frac{1}{2}b(x)b'(x)$.\n\nWe now compute this coefficient for the specific problem. The diffusion coefficient is $b(x) = \\sigma x^{2}$. Its derivative with respect to $x$ is:\n$$\nb'(x) = \\frac{\\mathrm{d}}{\\mathrm{d}x} (\\sigma x^{2}) = 2\\sigma x.\n$$\nSubstituting these into the expression for the coefficient gives:\n$$\n\\text{Coefficient} = \\frac{1}{2}b(x)b'(x) = \\frac{1}{2}(\\sigma x^{2})(2\\sigma x) = \\sigma^{2}x^{3}.\n$$\nThis expression depends only on $x$ and $\\sigma$, as required by the problem statement. The constant $\\mu$ does not appear in this specific term of the local error.", "answer": "$$ \\boxed{\\sigma^{2}x^{3}} $$", "id": "2998788"}, {"introduction": "While convergence tells us that our approximation approaches the true solution as the step size $h$ shrinks, it doesn't guarantee that the numerical solution remains well-behaved for a fixed, non-zero $h$. This is the domain of stability analysis. In this exercise, you will investigate the mean-square stability of the Milstein scheme—a method that improves upon Euler-Maruyama—by deriving its stability function for a standard linear test equation. This analysis is crucial for understanding the parameter regions where the numerical method produces bounded, meaningful results, a practical concern in any simulation [@problem_id:2998813].", "problem": "Consider the scalar linear stochastic differential equation (SDE) with multiplicative noise given by $dX_{t} = \\lambda X_{t}\\,dt + \\mu X_{t}\\,dW_{t}$, where $\\lambda \\in \\mathbb{R}$ and $\\mu \\in \\mathbb{R}$ are constants, and $W_{t}$ is a standard Brownian motion. Let $h > 0$ be a fixed time step and define discrete times $t_{n} = n h$. The Milstein scheme for this SDE is\n$$\nX_{n+1} = X_{n} + \\lambda X_{n} h + \\mu X_{n} \\Delta W_{n} + \\frac{1}{2}\\mu^{2} X_{n}\\left(\\Delta W_{n}^{2} - h\\right),\n$$\nwhere $\\Delta W_{n} = W_{t_{n+1}} - W_{t_{n}}$ are independent increments with $\\Delta W_{n} \\sim \\mathcal{N}(0,h)$ and independent of $X_{n}$.\n\nDefine the mean-square stability function $R(\\lambda,\\mu,h)$ of the Milstein scheme on this test equation to be the factor by which the second moment is amplified in one step, that is,\n$$\nR(\\lambda,\\mu,h) := \\frac{\\mathbb{E}\\!\\left[X_{n+1}^{2}\\right]}{\\mathbb{E}\\!\\left[X_{n}^{2}\\right]}.\n$$\n\nStarting only from the fundamental definitions of the Milstein scheme, properties of Brownian motion increments (including $\\mathbb{E}[\\Delta W_{n}] = 0$, $\\mathbb{E}[\\Delta W_{n}^{2}] = h$, $\\mathbb{E}[\\Delta W_{n}^{3}] = 0$, and $\\mathbb{E}[\\Delta W_{n}^{4}] = 3 h^{2}$), and the independence of $\\Delta W_{n}$ and $X_{n}$, derive a closed-form expression for $R(\\lambda,\\mu,h)$ in terms of $\\lambda$, $\\mu$, and $h$. Then, based on your derived stability function, explain how the parameters $\\lambda$ and $\\mu$ influence the leading-order mean-square stability for small $h$, connecting your explanation to the notion of strong convergence.\n\nProvide the final expression for $R(\\lambda,\\mu,h)$ as your answer. No numerical approximation is required.", "solution": "We begin with the scalar linear stochastic differential equation $dX_{t} = \\lambda X_{t}\\,dt + \\mu X_{t}\\,dW_{t}$ and its Milstein discretization on a uniform grid with step size $h > 0$:\n$$\nX_{n+1} = X_{n} + \\lambda X_{n} h + \\mu X_{n} \\Delta W_{n} + \\frac{1}{2}\\mu^{2} X_{n}\\left(\\Delta W_{n}^{2} - h\\right),\n$$\nwhere $\\Delta W_{n} = W_{t_{n+1}} - W_{t_{n}}$ are independent increments of Brownian motion, $\\Delta W_{n} \\sim \\mathcal{N}(0,h)$, and independent of $X_{n}$.\n\nIt is convenient to factor out $X_{n}$ and define a random amplification factor $G_{n}$ by\n$$\nX_{n+1} = G_{n} X_{n}, \\quad \\text{where} \\quad G_{n} = 1 + \\lambda h + \\mu \\Delta W_{n} + \\frac{1}{2}\\mu^{2}\\left(\\Delta W_{n}^{2} - h\\right).\n$$\nWe are interested in the mean-square stability function\n$$\nR(\\lambda,\\mu,h) := \\frac{\\mathbb{E}\\!\\left[X_{n+1}^{2}\\right]}{\\mathbb{E}\\!\\left[X_{n}^{2}\\right]}.\n$$\nSince $X_{n}$ is measurable with respect to the filtration up to time $t_{n}$ and $\\Delta W_{n}$ is independent of that filtration, we have\n$$\n\\mathbb{E}\\!\\left[X_{n+1}^{2}\\right] = \\mathbb{E}\\!\\left[\\mathbb{E}\\!\\left[X_{n}^{2} G_{n}^{2} \\,\\middle|\\, \\mathcal{F}_{t_{n}} \\right]\\right] = \\mathbb{E}\\!\\left[X_{n}^{2}\\right] \\, \\mathbb{E}\\!\\left[G_{n}^{2}\\right].\n$$\nTherefore,\n$$\nR(\\lambda,\\mu,h) = \\mathbb{E}\\!\\left[G_{n}^{2}\\right].\n$$\n\nWe now compute $\\mathbb{E}\\!\\left[G_{n}^{2}\\right]$ using the known moments of $\\Delta W_{n}$. Write $Z := \\Delta W_{n}$ so that $Z \\sim \\mathcal{N}(0,h)$, and denote $h$ by $h$ for brevity. Then\n$$\nG_{n} = 1 + \\lambda h + \\mu Z + \\frac{1}{2}\\mu^{2}\\left(Z^{2} - h\\right).\n$$\nIntroduce $d := \\lambda - \\frac{1}{2}\\mu^{2}$ and rewrite\n$$\nG_{n} = \\left(1 + d h\\right) + \\mu Z + \\frac{1}{2}\\mu^{2} Z^{2}.\n$$\nSet $A := 1 + d h$, $B := \\mu Z$, and $C := \\frac{1}{2}\\mu^{2} Z^{2}$ so that $G_{n} = A + B + C$. Then\n$$\nG_{n}^{2} = A^{2} + B^{2} + C^{2} + 2 A B + 2 A C + 2 B C.\n$$\nWe use the Gaussian moment identities for $Z \\sim \\mathcal{N}(0,h)$:\n$$\n\\mathbb{E}[Z] = 0, \\quad \\mathbb{E}[Z^{2}] = h, \\quad \\mathbb{E}[Z^{3}] = 0, \\quad \\mathbb{E}[Z^{4}] = 3 h^{2}.\n$$\nTaking expectations term-by-term,\n- $\\mathbb{E}[A^{2}] = A^{2}$ since $A$ is deterministic,\n- $\\mathbb{E}[B^{2}] = \\mu^{2} \\mathbb{E}[Z^{2}] = \\mu^{2} h$,\n- $\\mathbb{E}[C^{2}] = \\frac{1}{4}\\mu^{4} \\mathbb{E}[Z^{4}] = \\frac{3}{4}\\mu^{4} h^{2}$,\n- $\\mathbb{E}[2 A B] = 2 A \\mu \\mathbb{E}[Z] = 0$,\n- $\\mathbb{E}[2 A C] = 2 A \\cdot \\frac{1}{2}\\mu^{2} \\mathbb{E}[Z^{2}] = A \\mu^{2} h$,\n- $\\mathbb{E}[2 B C] = \\mu^{3} \\mathbb{E}[Z^{3}] = 0$.\n\nThus,\n$$\n\\mathbb{E}\\!\\left[G_{n}^{2}\\right] = A^{2} + \\mu^{2} h + \\frac{3}{4}\\mu^{4} h^{2} + A \\mu^{2} h.\n$$\nSubstitute $A = 1 + d h$:\n$$\nA^{2} = \\left(1 + d h\\right)^{2} = 1 + 2 d h + d^{2} h^{2}, \\quad A \\mu^{2} h = \\mu^{2} h + \\mu^{2} d h^{2}.\n$$\nTherefore,\n$$\n\\mathbb{E}\\!\\left[G_{n}^{2}\\right] = \\left(1 + 2 d h + d^{2} h^{2}\\right) + \\mu^{2} h + \\frac{3}{4}\\mu^{4} h^{2} + \\mu^{2} h + \\mu^{2} d h^{2}.\n$$\nCollecting like terms gives\n$$\n\\mathbb{E}\\!\\left[G_{n}^{2}\\right] = 1 + \\left(2 d + 2 \\mu^{2}\\right) h + \\left(d^{2} + \\mu^{2} d + \\frac{3}{4}\\mu^{4}\\right) h^{2}.\n$$\nRecall $d = \\lambda - \\frac{1}{2}\\mu^{2}$. The linear term simplifies as\n$$\n2 d + 2 \\mu^{2} = 2\\lambda - \\mu^{2} + 2 \\mu^{2} = 2 \\lambda + \\mu^{2}.\n$$\nFor the quadratic term, compute\n$$\nd^{2} + \\mu^{2} d = \\left(\\lambda - \\frac{1}{2}\\mu^{2}\\right)^{2} + \\mu^{2}\\left(\\lambda - \\frac{1}{2}\\mu^{2}\\right) = \\lambda^{2} - \\lambda \\mu^{2} + \\frac{1}{4}\\mu^{4} + \\lambda \\mu^{2} - \\frac{1}{2}\\mu^{4} = \\lambda^{2} - \\frac{1}{4}\\mu^{4}.\n$$\nAdding $\\frac{3}{4}\\mu^{4}$ yields\n$$\n\\lambda^{2} - \\frac{1}{4}\\mu^{4} + \\frac{3}{4}\\mu^{4} = \\lambda^{2} + \\frac{1}{2}\\mu^{4}.\n$$\nHence,\n$$\nR(\\lambda,\\mu,h) = \\mathbb{E}\\!\\left[G_{n}^{2}\\right] = 1 + \\left(2 \\lambda + \\mu^{2}\\right) h + \\left(\\lambda^{2} + \\frac{1}{2}\\mu^{4}\\right) h^{2}.\n$$\n\nWe now analyze parameter dependence and connect to strong convergence. The exact second-moment growth of the continuous-time solution satisfies\n$$\n\\mathbb{E}\\!\\left[X_{t+h}^{2} \\,\\middle|\\, X_{t}\\right] = X_{t}^{2} \\exp\\!\\left(\\left(2 \\lambda + \\mu^{2}\\right) h\\right),\n$$\nwhose Taylor expansion for small $h$ is\n$$\n\\exp\\!\\left(\\left(2 \\lambda + \\mu^{2}\\right) h\\right) = 1 + \\left(2 \\lambda + \\mu^{2}\\right) h + \\frac{1}{2}\\left(2 \\lambda + \\mu^{2}\\right)^{2} h^{2} + \\mathcal{O}\\!\\left(h^{3}\\right).\n$$\nThe Milstein mean-square amplification $R(\\lambda,\\mu,h)$ matches the exact expansion up to the linear term $\\left(2 \\lambda + \\mu^{2}\\right) h$, reflecting its strong order $1$ property. The deviation in the $h^{2}$ coefficient, namely $\\lambda^{2} + \\frac{1}{2}\\mu^{4}$ versus $\\frac{1}{2}\\left(2 \\lambda + \\mu^{2}\\right)^{2} = 2 \\lambda^{2} + 2 \\lambda \\mu^{2} + \\frac{1}{2}\\mu^{4}$, characterizes higher-order differences that do not affect the leading-order strong convergence but do influence the precise mean-square stability boundary for finite $h$. In particular, for small $h$, the leading-order mean-square stability criterion $R(\\lambda,\\mu,h) < 1$ reduces to $2 \\lambda + \\mu^{2} < 0$, indicating that sufficiently negative $\\lambda$ can offset the destabilizing effect of multiplicative noise $\\mu$, whereas larger $|\\mu|$ increases the amplification through both the linear and quadratic terms in $h$.", "answer": "$$\\boxed{1 + \\left(2 \\lambda + \\mu^{2}\\right) h + \\left(\\lambda^{2} + \\frac{1}{2}\\mu^{4}\\right) h^{2}}$$", "id": "2998813"}, {"introduction": "Building on the local error analysis from our first exercise, we now advance to a more sophisticated class of methods: Stochastic Runge-Kutta (SRK) schemes. This final practice asks you to analyze a specific two-stage SRK method, which requires a careful application of the Itô-Taylor expansion to account for its multi-stage, predictor-corrector structure. The ultimate goal is to connect your findings on the local level to the bigger picture by deducing the scheme's global order of strong convergence, reinforcing the fundamental theorem that links local accuracy to global performance [@problem_id:2998798].", "problem": "Consider the scalar Itô stochastic differential equation $dX_{t} = \\lambda X_{t}\\,dt + \\mu X_{t}\\,dW_{t}$ with initial condition $X_{0} = x_{0}$, where $\\lambda \\in \\mathbb{R}$ and $\\mu \\in \\mathbb{R}$ are constants, and $W_{t}$ is a standard Brownian motion. Let $h > 0$ be a fixed time step, $t_{n} = nh$, and denote $\\Delta W_{n} = W_{t_{n+1}} - W_{t_{n}}$. Consider the following two-stage explicit stochastic Runge-Kutta (SRK) method characterized by the drift Butcher tableau and diffusion coefficients:\n$$\n\\text{Drift tableau:}\\quad\n\\begin{array}{c|cc}\n0 & 0 & 0 \\\\\n1 & 1 & 0 \\\\\n\\hline\n & \\frac{1}{2} & \\frac{1}{2}\n\\end{array}\n\\qquad\n\\text{Diffusion evaluation:}\\quad\n\\beta = \\begin{pmatrix}1 \\\\ 0\\end{pmatrix},\\quad \\gamma = \\frac{1}{2}.\n$$\nThe stages $Y_{n,1}$ and $Y_{n,2}$ and the update $X_{n+1}$ are defined by\n$$\nY_{n,1} = X_{n},\\qquad\nY_{n,2} = X_{n} + f(Y_{n,1})\\,h + g(Y_{n,1})\\,\\Delta W_{n},\n$$\n$$\nX_{n+1} = X_{n} + \\frac{1}{2}\\big(f(Y_{n,1}) + f(Y_{n,2})\\big)\\,h + g(Y_{n,1})\\,\\Delta W_{n} + \\frac{1}{2}\\,g(Y_{n,1})\\,g'(Y_{n,1})\\big((\\Delta W_{n})^{2} - h\\big),\n$$\nwhere $f(x) = \\lambda x$ is the drift and $g(x) = \\mu x$ is the diffusion. \n\n(a) Using the Itô-Taylor expansion of the exact solution about $t_{n}$, compute the strong one-step local error, defined as the conditional root mean-square error\n$$\n\\Big(\\mathbb{E}\\big(|X_{t_{n+1}} - X_{n+1}|^{2}\\,\\big|\\,\\mathcal{F}_{t_{n}}\\big)\\Big)^{1/2},\n$$\nto leading order in $h$ as $h \\to 0$. Express your result in the asymptotic form $C(\\lambda,\\mu,X_{n})\\,h^{3/2} + o(h^{3/2})$ and provide the explicit coefficient $C(\\lambda,\\mu,X_{n})$ obtained by calculation.\n\n(b) Under standard global Lipschitz and linear growth conditions ensuring moment stability, deduce the global strong convergence rate $p$ over a fixed time interval $[0,T]$ from the computed local error and a discrete Grönwall-type argument. Report your final answer as the single number $p$ (no units, no rounding required).", "solution": "The problem is assessed to be valid as it is a well-posed, self-contained, and standard problem in the field of numerical analysis for stochastic differential equations. It is scientifically grounded and uses clear, objective language.\n\nThe stochastic differential equation (SDE) under consideration is the geometric Brownian motion:\n$$dX_{t} = \\lambda X_{t}\\,dt + \\mu X_{t}\\,dW_{t}, \\quad X_{0} = x_{0}$$\nwith drift $f(x) = \\lambda x$ and diffusion $g(x) = \\mu x$. The derivatives are $f'(x) = \\lambda$, $g'(x) = \\mu$, and all higher derivatives are zero.\n\nThe given stochastic Runge-Kutta (SRK) method is defined by the three equations:\n$$Y_{n,1} = X_{n}$$\n$$Y_{n,2} = X_{n} + f(Y_{n,1})\\,h + g(Y_{n,1})\\,\\Delta W_{n}$$\n$$X_{n+1} = X_{n} + \\frac{1}{2}\\big(f(Y_{n,1}) + f(Y_{n,2})\\big)\\,h + g(Y_{n,1})\\,\\Delta W_{n} + \\frac{1}{2}\\,g(Y_{n,1})\\,g'(Y_{n,1})\\big((\\Delta W_{n})^{2} - h\\big)$$\nThis is a predictor-corrector scheme, specifically the Platen scheme of strong order $1.0$.\n\n(a) To find the strong one-step local error, we compare the Taylor expansion of the numerical solution $X_{n+1}$ with the Itô-Taylor expansion of the exact solution $X_{t_{n+1}}$ around $t_{n}$, assuming $X_{n} = X_{t_{n}}$.\n\nFirst, we expand the numerical solution $X_{n+1}$.\nSubstitute $Y_{n,1} = X_n$ into the equation for $Y_{n,2}$:\n$$Y_{n,2} = X_{n} + f(X_{n})\\,h + g(X_{n})\\,\\Delta W_{n} = X_{n} + \\lambda X_{n}h + \\mu X_{n}\\Delta W_{n}$$\nSince $f(x) = \\lambda x$ is a linear function, we have:\n$$f(Y_{n,2}) = \\lambda Y_{n,2} = \\lambda(X_{n} + \\lambda X_{n}h + \\mu X_{n}\\Delta W_{n}) = \\lambda X_{n} + \\lambda^2 X_{n}h + \\lambda\\mu X_{n}\\Delta W_{n}$$\nSubstitute this into the formula for $X_{n+1}$:\n$$X_{n+1} = X_{n} + \\frac{1}{2}\\big(\\lambda X_{n} + (\\lambda X_{n} + \\lambda^2 X_{n}h + \\lambda\\mu X_{n}\\Delta W_{n})\\big)h + \\mu X_{n}\\Delta W_{n} + \\frac{1}{2}(\\mu X_{n})(\\mu)\\big((\\Delta W_{n})^{2} - h\\big)$$\n$$X_{n+1} = X_{n} + \\frac{1}{2}\\big(2\\lambda X_{n} + \\lambda^2 X_{n}h + \\lambda\\mu X_{n}\\Delta W_{n}\\big)h + \\mu X_{n}\\Delta W_{n} + \\frac{1}{2}\\mu^2 X_{n}\\big((\\Delta W_{n})^{2} - h\\big)$$\nExpanding this expression gives the one-step approximation:\n$$X_{n+1} = X_{n} + \\lambda X_{n}h + \\frac{1}{2}\\lambda^2 X_{n}h^2 + \\frac{1}{2}\\lambda\\mu X_{n}h\\Delta W_{n} + \\mu X_{n}\\Delta W_{n} + \\frac{1}{2}\\mu^2 X_{n}\\big((\\Delta W_{n})^{2} - h\\big)$$\nWe can regroup the terms for clarity:\n$$X_{n+1} = X_{n} \\left( 1 + \\lambda h + \\frac{1}{2}\\lambda^2 h^2 \\right) + X_{n} \\left( \\mu + \\frac{1}{2}\\lambda\\mu h \\right)\\Delta W_{n} + X_{n} \\frac{1}{2}\\mu^2 \\left((\\Delta W_{n})^2 - h\\right)$$\n\nNext, we expand the exact solution. The SDE has the known analytical solution:\n$$X_{t} = X_{0} \\exp\\left(\\left(\\lambda - \\frac{1}{2}\\mu^2\\right)t + \\mu W_{t}\\right)$$\nSo, the one-step exact solution is:\n$$X_{t_{n+1}} = X_{t_{n}} \\exp\\left(\\left(\\lambda - \\frac{1}{2}\\mu^2\\right)h + \\mu \\Delta W_{n}\\right)$$\nLet $X_n = X_{t_n}$ and $A = (\\lambda - \\frac{1}{2}\\mu^2)h + \\mu \\Delta W_{n}$. We expand $\\exp(A)$ up to terms that will contribute to an error of order $h^{3/2}$.\n$$X_{t_{n+1}} = X_{n} e^{A} = X_{n} \\left(1 + A + \\frac{A^2}{2} + \\frac{A^3}{6} + \\dots\\right)$$\nThe terms in the expansion of $A^k$ are:\n$$A = \\left(\\lambda - \\frac{1}{2}\\mu^2\\right)h + \\mu \\Delta W_{n}$$\n$$A^2 = \\left(\\lambda - \\frac{1}{2}\\mu^2\\right)^2 h^2 + 2\\mu\\left(\\lambda - \\frac{1}{2}\\mu^2\\right)h \\Delta W_{n} + \\mu^2 (\\Delta W_{n})^2$$\n$$A^3 = \\mu^3 (\\Delta W_{n})^3 + 3\\mu^2\\left(\\lambda - \\frac{1}{2}\\mu^2\\right)h (\\Delta W_{n})^2 + \\dots$$\nSubstituting these into the expansion for $X_{t_{n+1}}$ and collecting terms of similar stochastic order ($\\Delta W_n \\sim h^{1/2}$):\n$$X_{t_{n+1}} = X_{n} \\left( 1 + \\left(\\lambda - \\frac{1}{2}\\mu^2\\right)h + \\mu \\Delta W_{n} + \\frac{1}{2}A^2 + \\frac{1}{6}A^3 + \\dots \\right)$$\nThe numerical scheme matches terms with $h$, $\\Delta W_n$, and $(\\Delta W_n)^2 - h$. Let's regroup the expansion of the exact solution in these terms.\n\\begin{align*}\nX_{t_{n+1}} = X_{n} [& 1 + \\lambda h + \\mu \\Delta W_{n} + \\frac{1}{2}\\mu^2\\left((\\Delta W_{n})^2 - h\\right) \\\\\n& + \\frac{1}{2}\\left(\\left(\\lambda - \\frac{1}{2}\\mu^2\\right)^2\\right)h^2 \\\\\n& + \\mu\\left(\\lambda - \\frac{1}{2}\\mu^2\\right)h \\Delta W_{n} \\\\\n& + \\frac{1}{6}\\mu^3 (\\Delta W_{n})^3 + \\text{h.o.t} \\bigg]\n\\end{align*}\n\nThe one-step local error is $E_n = X_{t_{n+1}} - X_{n+1}$. We subtract the expansion of $X_{n+1}$ from that of $X_{t_{n+1}}$:\n$$E_n/X_{n} = \\left[\\lambda\\mu - \\frac{1}{2}\\mu^3 - \\frac{1}{2}\\lambda\\mu\\right]h\\Delta W_{n} + \\frac{1}{6}\\mu^3 (\\Delta W_{n})^3 + O(h^2)$$\n$$E_n = X_{n} \\left( \\left(\\frac{1}{2}\\lambda\\mu - \\frac{1}{2}\\mu^3\\right)h \\Delta W_n + \\frac{1}{6}\\mu^3 (\\Delta W_n)^3 \\right) + \\text{h.o.t.}$$\nThe leading terms of the local error are of stochastic order $h^{3/2}$. Let $K_1 = X_n(\\frac{1}{2}\\lambda\\mu - \\frac{1}{2}\\mu^3)$ and $K_2 = X_n\\frac{1}{6}\\mu^3$.\nTo find the root mean-square error, we compute the conditional expectation of $|E_n|^2$:\n$$\\mathbb{E}\\big[|E_n|^2 \\,\\big|\\, \\mathcal{F}_{t_n}\\big] \\approx \\mathbb{E}\\left[\\left(K_1 h\\Delta W_n + K_2 (\\Delta W_n)^3\\right)^2 \\,\\bigg|\\, \\mathcal{F}_{t_n}\\right]$$\nSince $X_n$ is $\\mathcal{F}_{t_n}$-measurable, we can treat $K_1$ and $K_2$ as constants in the expectation.\n$$\\mathbb{E}\\big[|E_n|^2 \\,\\big|\\, \\mathcal{F}_{t_n}\\big] \\approx K_1^2 h^2 \\mathbb{E}[(\\Delta W_n)^2] + K_2^2 \\mathbb{E}[(\\Delta W_n)^6] + 2K_1 K_2 h \\mathbb{E}[(\\Delta W_n)^4]$$\nUsing moments of a normal random variable $\\Delta W_n \\sim N(0,h)$:\n$\\mathbb{E}[(\\Delta W_n)^2] = h$\n$\\mathbb{E}[(\\Delta W_n)^4] = 3h^2$\n$\\mathbb{E}[(\\Delta W_n)^6] = 15h^3$\nSubstituting these gives:\n$$\\mathbb{E}\\big[|E_n|^2 \\,\\big|\\, \\mathcal{F}_{t_n}\\big] \\approx K_1^2 h^3 + 15 K_2^2 h^3 + 6 K_1 K_2 h^3 = (K_1^2 + 15K_2^2 + 6K_1 K_2)h^3$$\nSubstituting the expressions for $K_1$ and $K_2$:\n\\begin{align*}\nK_1^2 + 15K_2^2 + 6K_1K_2 &= X_n^2 \\left[ \\left(\\frac{1}{2}\\lambda\\mu - \\frac{1}{2}\\mu^3\\right)^2 + 15\\left(\\frac{1}{6}\\mu^3\\right)^2 + 6\\left(\\frac{1}{2}\\lambda\\mu - \\frac{1}{2}\\mu^3\\right)\\left(\\frac{1}{6}\\mu^3\\right) \\right] \\\\\n&= X_n^2 \\left[ \\left(\\frac{1}{4}\\lambda^2\\mu^2 - \\frac{1}{2}\\lambda\\mu^4 + \\frac{1}{4}\\mu^6\\right) + \\frac{15}{36}\\mu^6 + \\left(\\frac{1}{2}\\lambda\\mu^4 - \\frac{1}{2}\\mu^6\\right) \\right] \\\\\n&= X_n^2 \\left[ \\frac{1}{4}\\lambda^2\\mu^2 + \\left(\\frac{1}{4} + \\frac{5}{12} - \\frac{1}{2}\\right)\\mu^6 \\right] \\\\\n&= X_n^2 \\left[ \\frac{1}{4}\\lambda^2\\mu^2 + \\left(\\frac{3+5-6}{12}\\right)\\mu^6 \\right] \\\\\n&= X_n^2 \\left( \\frac{1}{4}\\lambda^2\\mu^2 + \\frac{1}{6}\\mu^6 \\right)\n\\end{align*}\nThe conditional mean-square error is:\n$$\\mathbb{E}\\big[|X_{t_{n+1}} - X_{n+1}|^2 \\,\\big|\\, \\mathcal{F}_{t_n}\\big] = X_n^2 \\left(\\frac{\\lambda^2\\mu^2}{4} + \\frac{\\mu^6}{6}\\right) h^3 + o(h^3)$$\nThe root mean-square error is the square root of this expression.\n$$ \\left(\\mathbb{E}\\big[|X_{t_{n+1}} - X_{n+1}|^2 \\,\\big|\\, \\mathcal{F}_{t_n}\\big]\\right)^{1/2} = |X_n| \\left(\\frac{\\lambda^2\\mu^2}{4} + \\frac{\\mu^6}{6}\\right)^{1/2} h^{3/2} + o(h^{3/2}) $$\nThe coefficient $C(\\lambda,\\mu,X_{n})$ is therefore:\n$$C(\\lambda,\\mu,X_{n}) = |X_{n}|\\sqrt{\\frac{\\lambda^2\\mu^2}{4} + \\frac{\\mu^6}{6}}$$\n\n(b) The global strong convergence rate $p$ is determined by the local strong error order. The one-step local strong error is the conditional root mean-square error, which we found to be of order $O(h^{3/2})$. The exponent is $\\gamma_{loc} = 3/2 = 1.5$.\nFor a one-step numerical method, the global order of strong convergence $p$ over a fixed interval $[0,T]$ is related to the local strong order $\\gamma_{loc}$ by the formula $p = \\gamma_{loc} - 0.5$.\nThis relationship is established via a discrete Grönwall-type argument. The global error at step $n+1$ can be recursively expressed in terms of the global error at step $n$ and the local error at step $n+1$. Summing up the local errors over $N = T/h$ steps leads to an accumulation of errors. The Lipschitz continuity of the scheme ensures that the error propagation is controlled. For a local error of order $h^{\\gamma_{loc}}$, the global error accumulates to an order of $N \\times h^{\\gamma_{loc}} = (T/h) \\times h^{\\gamma_{loc}} = T \\times h^{\\gamma_{loc}-1}$. The root mean-square error, which involves taking the square root of the mean-square error, will then be of order $(h^{\\gamma_{loc}-1})^{1/2}$... This argument is slightly imprecise. The correct inductive argument is:\nLet $e_n = X(t_n)-X_n$.\n$\\mathbb{E}[|e_{n+1}|^2] \\le (1+Kh)\\mathbb{E}[|e_n|^2] + C h^{2\\gamma_{loc}}$.\nStarting from $e_0=0$ and applying this recursively for $N=T/h$ steps yields $\\mathbb{E}[|e_N|^2] \\le C' h^{2\\gamma_{loc}-1}$.\nTaking the square root, the global strong error is $\\left(\\mathbb{E}[|e_N|^2]\\right)^{1/2} \\le O(h^{\\gamma_{loc}-0.5})$.\nWith $\\gamma_{loc} = 1.5$, the global strong convergence rate is $p = 1.5 - 0.5 = 1$.", "answer": "$$\\boxed{\\pmatrix{ |X_{n}|\\sqrt{\\frac{\\lambda^2\\mu^2}{4} + \\frac{\\mu^6}{6}} & 1 }}$$", "id": "2998798"}]}