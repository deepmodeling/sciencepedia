{"hands_on_practices": [{"introduction": "Before running any Monte Carlo simulation, a fundamental question must be answered: how many sample paths are sufficient? This exercise lays the theoretical groundwork for this question, guiding you to derive the essential relationship between the variance of the quantity of interest, the desired precision, and the required computational effort. Understanding this trade-off is the first step toward designing efficient and reliable Monte Carlo estimators for SDEs. [@problem_id:2988319]", "problem": "Consider a one-dimensional Itô stochastic differential equation (SDE) given by $dX_{t}=\\mu(X_{t},t)\\,dt+\\sigma(X_{t},t)\\,dW_{t}$ with deterministic initial condition $X_{0}=x_{0}$, where $\\mu$ and $\\sigma$ satisfy standard conditions ensuring existence and uniqueness of strong solutions and finite second moments of functionals of $X_{t}$. Let $T>0$ be a fixed terminal time and let $\\varphi:\\mathbb{R}\\to\\mathbb{R}$ be a measurable function such that $\\mathbb{E}\\big[|\\varphi(X_{T})|^{2}\\big]<\\infty$. The computational task is to approximate the expectation $\\mathbb{E}[\\varphi(X_{T})]$ by Monte Carlo sampling.\n\nAssume an idealized setting in which the sampling of $X_{T}$ is exact (i.e., neglect any discretization bias), and suppose one can draw $N$ independent samples $Y_{1},\\dots,Y_{N}$ with $Y_{i}=\\varphi(X_{T}^{(i)})$, where $X_{T}^{(i)}$ are independent copies of $X_{T}$. Let $V=\\mathrm{Var}(Y_{1})$ be known and finite, with $V>0$. For the Monte Carlo estimator $\\widehat{m}_{N}=\\frac{1}{N}\\sum_{i=1}^{N}Y_{i}$, impose a target root-mean-square error (RMSE) threshold $\\varepsilon>0$.\n\nStarting from the definitions of variance, independence, and mean-squared error, derive the minimal integer sample size $N$ required to ensure that the RMSE of $\\widehat{m}_{N}$ does not exceed $\\varepsilon$, under the assumption of zero bias. Then, quantify the sensitivity of the required sample size to variance misestimation by deriving the first-order sensitivity of the continuous relaxation $N(V)$ with respect to $V$ and the corresponding relative sensitivity at $V$.\n\nYour final answer must be a single closed-form analytic expression or a single row matrix containing your expressions. No numerical approximation is required.", "solution": "We begin with the goal of estimating the expectation $\\mathbb{E}[\\varphi(X_{T})]$ by the Monte Carlo estimator $\\widehat{m}_{N}=\\frac{1}{N}\\sum_{i=1}^{N}Y_{i}$, where $Y_{i}=\\varphi(X_{T}^{(i)})$ and $X_{T}^{(i)}$ are independent realizations of $X_{T}$. Under the stated assumptions, the samples $Y_{1},\\dots,Y_{N}$ are independent and identically distributed with finite variance $V=\\mathrm{Var}(Y_{1})$ and mean $m=\\mathbb{E}[Y_{1}]=\\mathbb{E}[\\varphi(X_{T})]$.\n\nThe mean-squared error (MSE) of $\\widehat{m}_{N}$ is defined as\n$$\n\\mathrm{MSE}(\\widehat{m}_{N})=\\mathbb{E}\\big[(\\widehat{m}_{N}-m)^{2}\\big].\n$$\nBy the bias–variance decomposition,\n$$\n\\mathrm{MSE}(\\widehat{m}_{N})=\\big(\\mathbb{E}[\\widehat{m}_{N}]-m\\big)^{2}+\\mathrm{Var}(\\widehat{m}_{N}).\n$$\nIn our idealized setting, there is no discretization bias and the estimator is unbiased, so $\\mathbb{E}[\\widehat{m}_{N}]=m$ and the bias term vanishes. Therefore,\n$$\n\\mathrm{MSE}(\\widehat{m}_{N})=\\mathrm{Var}(\\widehat{m}_{N}).\n$$\nUsing independence and identical distribution, the variance of the sample mean is\n$$\n\\mathrm{Var}(\\widehat{m}_{N})=\\mathrm{Var}\\!\\left(\\frac{1}{N}\\sum_{i=1}^{N}Y_{i}\\right)=\\frac{1}{N^{2}}\\sum_{i=1}^{N}\\mathrm{Var}(Y_{i})=\\frac{1}{N^{2}}\\cdot N\\cdot V=\\frac{V}{N}.\n$$\nThe root-mean-square error (RMSE) is the square root of the MSE:\n$$\n\\mathrm{RMSE}(\\widehat{m}_{N})=\\sqrt{\\mathrm{MSE}(\\widehat{m}_{N})}=\\sqrt{\\frac{V}{N}}.\n$$\nImposing the target RMSE threshold $\\varepsilon>0$ means requiring\n$$\n\\sqrt{\\frac{V}{N}}\\leq \\varepsilon.\n$$\nSquaring both sides and solving for $N$,\n$$\n\\frac{V}{N}\\leq \\varepsilon^{2}\\quad\\Longleftrightarrow\\quad N\\geq \\frac{V}{\\varepsilon^{2}}.\n$$\nBecause $N$ must be an integer, the minimal integer sample size achieving the target is\n$$\nN^{\\star}=\\left\\lceil\\frac{V}{\\varepsilon^{2}}\\right\\rceil.\n$$\n\nWe now analyze sensitivity of the required sample size to variance misestimation. Consider the continuous relaxation $N(V)=\\frac{V}{\\varepsilon^{2}}$ (ignoring the ceiling for differential analysis). The first-order sensitivity of $N$ with respect to $V$ is the derivative\n$$\n\\frac{dN}{dV}=\\frac{d}{dV}\\left(\\frac{V}{\\varepsilon^{2}}\\right)=\\frac{1}{\\varepsilon^{2}}.\n$$\nTo quantify relative sensitivity, define the logarithmic derivative (elasticity)\n$$\nS_{\\mathrm{rel}}(V)=\\frac{dN/N}{dV/V}=\\frac{\\frac{dN}{dV}\\cdot \\frac{1}{N}}{\\frac{1}{V}}.\n$$\nSubstituting $N(V)=\\frac{V}{\\varepsilon^{2}}$ and $\\frac{dN}{dV}=\\frac{1}{\\varepsilon^{2}}$, we find\n$$\nS_{\\mathrm{rel}}(V)=\\frac{\\left(\\frac{1}{\\varepsilon^{2}}\\right)\\cdot \\left(\\frac{\\varepsilon^{2}}{V}\\right)}{\\frac{1}{V}}=1.\n$$\nThus, to first order, the relative error in the required (continuous) sample size equals the relative error in $V$: if $V$ is misestimated as $V(1+\\delta)$ with small $\\delta$, then $N$ is misestimated as $N(1+\\delta)$, up to the effect of the ceiling operator. In particular, underestimation of $V$ by a factor leads to an equal proportional underestimation of the required $N$, which risks violating the RMSE constraint $\\sqrt{V/N}\\leq \\varepsilon$; overestimation of $V$ proportionally over-provisions $N$ and is conservative.", "answer": "$$\\boxed{\\begin{pmatrix}\\left\\lceil \\dfrac{V}{\\varepsilon^{2}} \\right\\rceil & \\dfrac{1}{\\varepsilon^{2}} & 1\\end{pmatrix}}$$", "id": "2988319"}, {"introduction": "While the previous exercise focused on statistical error from finite sampling, practical Monte Carlo for SDEs faces another challenge: discretization error. Since we simulate discrete-time approximations of continuous-time processes, a systematic bias is introduced. This practice provides a clear, quantitative look at this \"weak bias\" by analyzing the widely-used Euler-Maruyama scheme for the Geometric Brownian Motion model, a cornerstone of financial mathematics. [@problem_id:2988356]", "problem": "Let $\\{X_t\\}_{t \\in [0,T]}$ be the solution to the Geometric Brownian Motion (GBM) Stochastic Differential Equation (SDE)\n$$\ndX_t \\;=\\; \\mu\\,X_t\\,dt \\;+\\; \\sigma\\,X_t\\,dW_t, \\qquad X_0 \\;=\\; x \\;>\\; 0,\n$$\nwhere $\\mu \\in \\mathbb{R}$ and $\\sigma \\geq 0$ are constants and $\\{W_t\\}_{t \\geq 0}$ is a standard Brownian motion. Consider the Euler–Maruyama (EM) time discretization with a uniform time step $\\Delta t \\;=\\; T/N$ for some integer $N \\geq 1$, defined by the recursion\n$$\nX_{n+1}^{\\Delta t} \\;=\\; X_n^{\\Delta t} \\;+\\; \\mu\\,X_n^{\\Delta t}\\,\\Delta t \\;+\\; \\sigma\\,X_n^{\\Delta t}\\,\\Delta W_n, \\qquad X_0^{\\Delta t} \\;=\\; x,\n$$\nwhere $\\Delta W_n \\sim \\mathcal{N}(0,\\Delta t)$ are independent and identically distributed and independent of $X_0^{\\Delta t}$. A Monte Carlo estimator for $\\mathbb{E}[X_T]$ based on EM with $L$ independent simulated paths is\n$$\n\\widehat{M}_L(\\Delta t) \\;=\\; \\frac{1}{L}\\sum_{\\ell=1}^{L} X_{N}^{\\Delta t,(\\ell)},\n$$\nwhere $X_{N}^{\\Delta t,(\\ell)}$ denotes the terminal EM approximation at time $T$ for the $\\ell$-th path.\n\nStarting only from the SDE definition, the EM recursion, and the basic properties of Brownian motion increments, derive a closed-form expression for $\\mathbb{E}[X_{N}^{\\Delta t}]$ as a function of $\\mu$, $\\sigma$, $x$, $T$, and $\\Delta t$, and then derive $\\mathbb{E}[X_T]$ for the exact GBM solution. Use these results to compute the weak bias of the EM-based Monte Carlo estimator, defined by\n$$\n\\operatorname{bias}(\\Delta t) \\;=\\; \\mathbb{E}[X_{N}^{\\Delta t}] \\;-\\; \\mathbb{E}[X_T].\n$$\nProvide the final answer as a single closed-form analytic expression for $\\operatorname{bias}(\\Delta t)$ in terms of $\\mu$, $\\sigma$, $x$, $T$, and $\\Delta t$. No numerical values are required.", "solution": "The problem requires the derivation of the weak bias of the Euler-Maruyama (EM) method for the Geometric Brownian Motion (GBM) process. The weak bias is defined as $\\operatorname{bias}(\\Delta t) = \\mathbb{E}[X_{N}^{\\Delta t}] - \\mathbb{E}[X_T]$, where $X_{N}^{\\Delta t}$ is the numerical solution at time $T$ and $X_T$ is the exact solution. The derivation will proceed in three steps: first, we compute the expectation of the EM approximation, $\\mathbb{E}[X_{N}^{\\Delta t}]$; second, we compute the expectation of the exact solution, $\\mathbb{E}[X_T]$; and third, we take their difference.\n\nFirst, we determine the expectation of the numerical solution obtained from the Euler-Maruyama scheme. The recursion is given by:\n$$\nX_{n+1}^{\\Delta t} \\;=\\; X_n^{\\Delta t} \\;+\\; \\mu\\,X_n^{\\Delta t}\\,\\Delta t \\;+\\; \\sigma\\,X_n^{\\Delta t}\\,\\Delta W_n\n$$\nwhere $n = 0, 1, \\dots, N-1$. We can factor out $X_n^{\\Delta t}$ to get:\n$$\nX_{n+1}^{\\Delta t} \\;=\\; X_n^{\\Delta t}\\,(1 + \\mu\\,\\Delta t + \\sigma\\,\\Delta W_n)\n$$\nLet $\\mathcal{F}_{t_n}$ be the filtration generated by the Brownian motion up to time $t_n = n\\Delta t$. The value of $X_n^{\\Delta t}$ is known at time $t_n$, so it is $\\mathcal{F}_{t_n}$-measurable. The Brownian increment $\\Delta W_n = W_{t_{n+1}} - W_{t_n}$ is independent of $\\mathcal{F}_{t_n}$. We compute the expectation of $X_{n+1}^{\\Delta t}$ by taking the conditional expectation with respect to $\\mathcal{F}_{t_n}$ and then applying the law of total expectation.\n$$\n\\mathbb{E}[X_{n+1}^{\\Delta t} | \\mathcal{F}_{t_n}] \\;=\\; \\mathbb{E}[X_n^{\\Delta t}(1 + \\mu\\,\\Delta t + \\sigma\\,\\Delta W_n) | \\mathcal{F}_{t_n}]\n$$\nSince $X_n^{\\Delta t}$ is $\\mathcal{F}_{t_n}$-measurable, we can treat it as a constant within the conditional expectation:\n$$\n\\mathbb{E}[X_{n+1}^{\\Delta t} | \\mathcal{F}_{t_n}] \\;=\\; X_n^{\\Delta t} \\,\\mathbb{E}[1 + \\mu\\,\\Delta t + \\sigma\\,\\Delta W_n | \\mathcal{F}_{t_n}]\n$$\nDue to the independence of $\\Delta W_n$ from $\\mathcal{F}_{t_n}$ and the property that $\\mathbb{E}[\\Delta W_n] = 0$, the expression simplifies:\n$$\n\\mathbb{E}[X_{n+1}^{\\Delta t} | \\mathcal{F}_{t_n}] \\;=\\; X_n^{\\Delta t} \\,(1 + \\mu\\,\\Delta t + \\sigma\\,\\mathbb{E}[\\Delta W_n]) \\;=\\; X_n^{\\Delta t}\\,(1 + \\mu\\,\\Delta t)\n$$\nNow, taking the unconditional expectation of both sides using the law of total expectation, $\\mathbb{E}[Y] = \\mathbb{E}[\\mathbb{E}[Y|\\mathcal{F}]]$, we get:\n$$\n\\mathbb{E}[X_{n+1}^{\\Delta t}] \\;=\\; \\mathbb{E}[\\mathbb{E}[X_{n+1}^{\\Delta t} | \\mathcal{F}_{t_n}]] \\;=\\; \\mathbb{E}[X_n^{\\Delta t}\\,(1 + \\mu\\,\\Delta t)] \\;=\\; (1 + \\mu\\,\\Delta t)\\,\\mathbb{E}[X_n^{\\Delta t}]\n$$\nThis is a recurrence relation for $\\mathbb{E}[X_n^{\\Delta t}]$. The initial condition is $\\mathbb{E}[X_0^{\\Delta t}] = \\mathbb{E}[x] = x$. We can solve this recurrence by iterating from $n=0$ to $N$:\n$$\n\\mathbb{E}[X_N^{\\Delta t}] \\;=\\; (1 + \\mu\\,\\Delta t)^N \\mathbb{E}[X_0^{\\Delta t}] \\;=\\; x(1 + \\mu\\,\\Delta t)^N\n$$\nSubstituting $N = T/\\Delta t$, we obtain the closed-form expression for the expectation of the EM approximation:\n$$\n\\mathbb{E}[X_N^{\\Delta t}] \\;=\\; x\\left(1 + \\mu\\,\\Delta t\\right)^{T/\\Delta t}\n$$\n\nSecond, we derive the expectation of the exact solution $X_T$. The SDE is given in differential form as $dX_t = \\mu X_t dt + \\sigma X_t dW_t$. Its integral form is:\n$$\nX_T \\;=\\; X_0 + \\int_0^T \\mu\\,X_t\\,dt + \\int_0^T \\sigma\\,X_t\\,dW_t\n$$\nTaking the expectation of both sides, we get:\n$$\n\\mathbb{E}[X_T] \\;=\\; \\mathbb{E}[X_0] + \\mathbb{E}\\left[\\int_0^T \\mu\\,X_t\\,dt\\right] + \\mathbb{E}\\left[\\int_0^T \\sigma\\,X_t\\,dW_t\\right]\n$$\nWe evaluate each term on the right-hand side. The initial condition gives $\\mathbb{E}[X_0] = x$. For the drift term, we can interchange the expectation and the Riemann integral by Fubini's theorem:\n$$\n\\mathbb{E}\\left[\\int_0^T \\mu\\,X_t\\,dt\\right] \\;=\\; \\mu \\int_0^T \\mathbb{E}[X_t]\\,dt\n$$\nFor the diffusion term, a fundamental property of the Itô integral is that its expectation is zero, provided the integrand is an adapted process satisfying certain integrability conditions, which hold for the solution of the GBM SDE. Thus:\n$$\n\\mathbb{E}\\left[\\int_0^T \\sigma\\,X_t\\,dW_t\\right] \\;=\\; 0\n$$\nSubstituting these back, and letting $m(t) = \\mathbb{E}[X_t]$, we obtain an integral equation for $m(T)$:\n$$\nm(T) \\;=\\; x + \\mu \\int_0^T m(t)\\,dt\n$$\nDifferentiating with respect to $T$ gives the ordinary differential equation (ODE):\n$$\n\\frac{dm(T)}{dT} \\;=\\; \\mu\\,m(T)\n$$\nThe initial condition is $m(0) = \\mathbb{E}[X_0] = x$. The solution to this ODE is:\n$$\nm(T) \\;=\\; x\\,\\exp(\\mu T)\n$$\nTherefore, the expectation of the exact solution is:\n$$\n\\mathbb{E}[X_T] \\;=\\; x\\,\\exp(\\mu T)\n$$\nNote that the expectations of both the numerical and the exact solutions are independent of the volatility parameter $\\sigma$.\n\nFinally, we compute the weak bias by taking the difference between the two expectations derived above:\n$$\n\\operatorname{bias}(\\Delta t) \\;=\\; \\mathbb{E}[X_{N}^{\\Delta t}] - \\mathbb{E}[X_T] \\;=\\; x\\left(1 + \\mu\\,\\Delta t\\right)^{T/\\Delta t} - x\\,\\exp(\\mu T)\n$$\nThis expression can be factored to yield the final form:\n$$\n\\operatorname{bias}(\\Delta t) \\;=\\; x\\left(\\left(1 + \\mu\\,\\Delta t\\right)^{T/\\Delta t} - \\exp(\\mu T)\\right)\n$$\nThis is the closed-form expression for the weak bias as a function of the specified parameters.", "answer": "$$\n\\boxed{x\\left(\\left(1 + \\mu\\Delta t\\right)^{\\frac{T}{\\Delta t}} - \\exp(\\mu T)\\right)}\n$$", "id": "2988356"}, {"introduction": "Building upon the concepts of statistical and discretization error, this practice introduces the powerful Multilevel Monte Carlo (MLMC) method, which optimally balances computational effort across different discretization levels. You will explore the core mechanism of MLMC: a specific coupling between fine and coarse simulation paths that minimizes the variance of the correction terms. This hands-on problem combines theoretical analysis with a coding implementation to verify the variance scaling that makes MLMC a state-of-the-art technique. [@problem_id:2988362]", "problem": "Consider the Stochastic Differential Equation (SDE) $dX_t = a X_t \\, dt + \\sigma \\, dW_t$ with initial condition $X_0 = x_0$, where $W_t$ is a standard Brownian motion and $a,\\sigma,x_0 \\in \\mathbb{R}$. Let $\\phi:\\mathbb{R}\\to\\mathbb{R}$ be the function $\\phi(x) = \\sin(x)$, which is globally Lipschitz with Lipschitz constant $L = 1$. We focus on the Euler–Maruyama (EM) method and the variance scaling of Multilevel Monte Carlo (MLMC) level differences built from an explicit coupling of fine and coarse time-discretizations. The framework is purely mathematical and does not involve physical units; all quantities are dimensionless.\n\nFundamental base and definitions:\n- A Stochastic Differential Equation (SDE) is a differential equation driven by a stochastic process, here $dX_t = a X_t \\, dt + \\sigma \\, dW_t$, with $W_t$ a Brownian motion characterized by independent Gaussian increments, i.e., $W_{t+\\Delta t} - W_t \\sim \\mathcal{N}(0,\\Delta t)$ and independent of the past.\n- The Euler–Maruyama (EM) method for a time step $h>0$ approximates the SDE by the recursion $X_{n+1}^{(h)} = X_n^{(h)} + a X_n^{(h)} h + \\sigma \\Delta W_n^{(h)}$, where $\\Delta W_n^{(h)} \\sim \\mathcal{N}(0,h)$ are independent increments.\n- The Multilevel Monte Carlo (MLMC) method estimates expectations by combining multiple levels of discretization with a coupling that minimizes variance of level differences; here, we consider a two-level difference $Y = \\phi(X_T^{(h)}) - \\phi(X_T^{(H)})$, with $H = 2h$.\n\nCoupling directive:\nConstruct an explicit coupling between the fine level with time step $h$ and the coarse level with time step $H = 2h$ by aggregating Gaussian increments. Specifically, for each coarse interval $[(t_{2m}), (t_{2m+2})]$, define the coarse increment as $\\Delta W_m^{(H)} := \\Delta W_{2m}^{(h)} + \\Delta W_{2m+1}^{(h)}$. Verify that this preserves the Gaussian law, i.e., $\\Delta W_m^{(H)} \\sim \\mathcal{N}(0, H)$, and that the coupling exploits common randomness to reduce the variance of $Y$.\n\nVariance scaling analysis objective:\nStarting from the fundamental properties of Brownian motion and the EM recursion, analyze the scaling of $\\operatorname{Var}(Y)$ as a function of $h$ for the linear SDE above and the Lipschitz payoff $\\phi$. The analysis must begin from the definitions and use well-tested facts, such as the independent increment property of Brownian motion and standard stability bounds for EM on linear systems. Do not use any shortcut or pre-stated scaling formula; derive the variance order from first principles appropriate to this context.\n\nAlgorithmic construction requirement:\nImplement a complete program that\n- Enforces the explicit coupling by summing fine increments to form the coarse increments.\n- Uses $N=20000$ independent coupled paths with a fixed random seed $42$ to compute Monte Carlo estimators.\n- For each test case, computes the empirical variance $\\operatorname{Var}(Y)$ (using the unbiased sample variance) and selected normalized quantities to investigate scaling.\n\nTest suite specification:\nUse the following scientifically sound parameter sets to test different facets (happy path, boundary conditions, and edge cases):\n\n1. Happy path scaling test:\n   - Parameters: $a = -1$, $\\sigma = 0.7$, $x_0 = 0.5$, $T = 1$.\n   - Fine step sizes: $h \\in \\{1/64, 1/128, 1/256\\}$ with coarse $H = 2h$.\n   - For each $h$, compute $V(h) = \\operatorname{Var}(Y)$ and the scaled quantity $S(h) = V(h)/h$.\n\n2. Ratio check for scaling:\n   - Same parameters as the happy path.\n   - Compute $R = V(1/128)/V(1/64)$ and the boolean $B$ indicating whether $|R - 1/2| \\leq 0.1$.\n\n3. Driftless boundary:\n   - Parameters: $a = 0$, $\\sigma = 0.7$, $x_0 = 0.5$, $T = 1$, $h = 1/64$, $H = 2h$.\n   - Compute $V_{\\text{driftless}} = \\operatorname{Var}(Y)$.\n\n4. Deterministic boundary:\n   - Parameters: $a = -1$, $\\sigma = 0$, $x_0 = 0.5$, $T = 1$, $h = 1/64$, $H = 2h$.\n   - Compute $V_{\\text{det}} = \\operatorname{Var}(Y)$.\n\n5. Short-time edge case:\n   - Parameters: $a = -1$, $\\sigma = 0.7$, $x_0 = 0.5$, $T = 1/8$, $h = 1/256$, $H = 2h$.\n   - Compute $S_{\\text{short}} = V(h)/h$.\n\nFinal output format requirement:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the exact order:\n$[S(1/64), S(1/128), S(1/256), R, B, V_{\\text{driftless}}, V_{\\text{det}}, S_{\\text{short}}]$.\nAll outputs must be numerical floats or booleans. No other text should be printed.", "solution": "The problem is valid as it is scientifically sound, well-posed, and complete. It presents a standard but substantive exercise in the numerical analysis of stochastic differential equations. The core of the problem is to derive the variance scaling of a Multilevel Monte Carlo (MLMC) level difference from first principles and then to verify this through numerical simulation.\n\nWe are given the linear Stochastic Differential Equation (SDE), also known as the Ornstein-Uhlenbeck process,\n$$dX_t = a X_t \\, dt + \\sigma \\, dW_t, \\quad X_0 = x_0$$\nwhere $W_t$ is a standard Brownian motion. We are interested in the expectation of $\\phi(X_T) = \\sin(X_T)$. This will be estimated using the Euler-Maruyama (EM) method.\n\nThe EM discretization with a time step $h$ is given by the recurrence relation:\n$$X_{n+1}^{(h)} = X_n^{(h)} (1 + ah) + \\sigma \\Delta W_n^{(h)}$$\nwhere $\\Delta W_n^{(h)} \\sim \\mathcal{N}(0,h)$ are independent random increments.\n\nThe objective is to analyze the variance of the difference between a fine and a coarse approximation, $Y = \\phi(X_T^{(h)}) - \\phi(X_T^{(H)})$, where the coarse time step is $H = 2h$. The two approximations are coupled by constructing the coarse Brownian increment from the fine ones. Specifically, over a coarse time interval of length $H$, the coarse increment $\\Delta W_m^{(H)}$ is the sum of the two corresponding fine increments $\\Delta W_{2m}^{(h)}$ and $\\Delta W_{2m+1}^{(h)}$:\n$$\\Delta W_m^{(H)} = \\Delta W_{2m}^{(h)} + \\Delta W_{2m+1}^{(h)}$$\nSince the fine increments are independent and identically distributed as $\\mathcal{N}(0,h)$, their sum is a Gaussian random variable with mean $0+0=0$ and variance $h+h=2h=H$. Thus, $\\Delta W_m^{(H)} \\sim \\mathcal{N}(0,H)$, correctly preserving the statistical properties of the coarse path.\n\nLet's denote the fine path at fine time steps $t_k = kh$ as $\\hat{X}_k = X_{t_k}^{(h)}$ and the coarse path at coarse time steps $t'_m = mH$ as $X_m = X_{t'_m}^{(H)}$. Note that $t'_{m} = t_{2m}$. We analyze the evolution of the difference between the paths, $e_m = \\hat{X}_{2m} - X_m$, at the coarse grid points. The initial difference is $e_0 = \\hat{X}_0 - X_0 = x_0 - x_0 = 0$.\n\nThe coarse path evolves from $t'_m$ to $t'_{m+1}$ in one step:\n$$X_{m+1} = X_m (1+aH) + \\sigma \\Delta W_m^{(H)} = X_m (1+2ah) + \\sigma (\\Delta W_{2m}^{(h)} + \\Delta W_{2m+1}^{(h)})$$\n\nThe fine path evolves from $t_{2m}$ to $t_{2m+2}$ in two steps:\n$$\\hat{X}_{2m+1} = \\hat{X}_{2m} (1+ah) + \\sigma \\Delta W_{2m}^{(h)}$$\n$$\\hat{X}_{2m+2} = \\hat{X}_{2m+1} (1+ah) + \\sigma \\Delta W_{2m+1}^{(h)} = \\left(\\hat{X}_{2m} (1+ah) + \\sigma \\Delta W_{2m}^{(h)}\\right)(1+ah) + \\sigma \\Delta W_{2m+1}^{(h)}$$\n$$\\hat{X}_{2m+2} = \\hat{X}_{2m} (1+ah)^2 + \\sigma(1+ah)\\Delta W_{2m}^{(h)} + \\sigma \\Delta W_{2m+1}^{(h)}$$\n$$\\hat{X}_{2m+2} = \\hat{X}_{2m} (1+2ah+a^2h^2) + \\sigma(1+ah)\\Delta W_{2m}^{(h)} + \\sigma \\Delta W_{2m+1}^{(h)}$$\n\nThe difference $e_{m+1} = \\hat{X}_{2m+2} - X_{m+1}$ is:\n$$e_{m+1} = \\left[ \\hat{X}_{2m} (1+2ah+a^2h^2) + \\sigma(1+ah)\\Delta W_{2m}^{(h)} + \\sigma \\Delta W_{2m+1}^{(h)} \\right] - \\left[ X_m(1+2ah) + \\sigma (\\Delta W_{2m}^{(h)} + \\Delta W_{2m+1}^{(h)}) \\right]$$\nSubstituting $\\hat{X}_{2m} = X_m + e_m$ and simplifying:\n$$e_{m+1} = (X_m+e_m)(1+2ah+a^2h^2) - X_m(1+2ah) + \\sigma(1+ah-1)\\Delta W_{2m}^{(h)}$$\n$$e_{m+1} = X_m(a^2h^2) + e_m(1+2ah+a^2h^2) + \\sigma a h \\Delta W_{2m}^{(h)}$$\nSince $H=2h$, we can write this as $e_{m+1} = e_m(1+aH+O(h^2)) + \\left( X_m a^2h^2 + \\sigma a h \\Delta W_{2m}^{(h)} \\right)$.\n\nThe term in the parenthesis is the local error introduced at step $m$. It has a deterministic part of order $O(h^2)$ and a stochastic part of order $h \\times (\\text{Gaussian with variance } h)$, which is of size $O(h^{3/2})$. The mean-square of this local error is dominated by the stochastic term:\n$$\\mathbb{E}\\left[ \\left( \\sigma a h \\Delta W_{2m}^{(h)} \\right)^2 \\bigg| \\mathcal{F}_{t_{2m}} \\right] = \\sigma^2 a^2 h^2 \\mathbb{E}\\left[ (\\Delta W_{2m}^{(h)})^2 \\right] = \\sigma^2 a^2 h^2 \\cdot h = \\sigma^2 a^2 h^3$$\nThe number of coarse steps is $M_c = T/H = T/(2h) = O(h^{-1})$. The final mean-square error $\\mathbb{E}[e_{M_c}^2] = \\mathbb{E}[(X_T^{(h)} - X_T^{(H)})^2]$ is the sum of these local mean-square errors, accumulated over the coarse steps. A discrete Gronwall argument shows that this leads to a global mean-square error of order $O(h^{-1}) \\times O(h^3) = O(h^2)$. This high order of convergence, $\\mathbb{E}[(X_T^{(h)} - X_T^{(H)})^2] = O(h^2)$, is a special feature of this SDE (linear with additive noise and globally Lipschitz drift), for which the Euler-Maruyama scheme has a strong convergence order of $\\beta=1$.\n\nNow we consider the variance of $Y = \\phi(X_T^{(h)}) - \\phi(X_T^{(H)})$. Let $E_T = X_T^{(h)} - X_T^{(H)}$.\nSince $\\phi(x) = \\sin(x)$ is Lipschitz with constant $L=1$, the mean-square difference of the payoffs is bounded:\n$\\mathbb{E}[Y^2] = \\mathbb{E}[(\\phi(X_T^{(h)}) - \\phi(X_T^{(H)}))^2] \\le L^2 \\mathbb{E}[E_T^2] = O(h^2)$.\nThe weak error of the EM scheme is of order $O(h)$, so the mean difference $\\mathbb{E}[Y] = \\mathbb{E}[\\phi(X_T^{(h)})] - \\mathbb{E}[\\phi(X_T^{(H)})]$ is also of order $O(h)$. This means its square, $(\\mathbb{E}[Y])^2$, is of order $O(h^2)$.\nThus, $\\operatorname{Var}(Y) = \\mathbb{E}[Y^2] - (\\mathbb{E}[Y])^2$. Given that both terms are $O(h^2)$, and it can be shown that their leading-order contributions do not cancel, the resulting variance is $\\operatorname{Var}(Y) = O(h^2)$.\n\nThis theoretical result, that $\\operatorname{Var}(Y)$ scales with $h^2$, is a key finding. It implies that the scaled quantity $S(h) = \\operatorname{Var}(Y)/h$ should scale as $O(h)$. It also implies that the ratio of variances for step sizes $h$ and $h/2$ should be:\n$$R = \\frac{\\operatorname{Var}(Y \\text{ for } h/2)}{\\operatorname{Var}(Y \\text{ for } h)} \\approx \\frac{C (h/2)^2}{C h^2} = \\frac{1}{4}$$\nThe problem asks to compute $B = \\text{bool}(|R - 1/2| \\leq 0.1)$. Based on our analysis, we expect $R \\approx 0.25$, so $|0.25 - 0.5| = 0.25$, which is not less than or equal to $0.1$. We predict that $B$ will be `False`.\n\nFor the boundary cases:\n- If $a=0$ (Driftless), the SDE is $dX_t = \\sigma dW_t$. The EM scheme becomes $X_{n+1} = X_n + \\sigma \\Delta W_n$. With the specified coupling, the fine and coarse paths are identical at the coarse time points, i.e., $X_T^{(h)} = X_T^{(H)}$ for every path. Thus $Y=0$ for all paths, and $\\operatorname{Var}(Y) = 0$.\n- If $\\sigma=0$ (Deterministic), the SDE is the ODE $dX_t = a X_t dt$. The numerical solutions $X_T^{(h)}$ and $X_T^{(H)}$ are deterministic but unequal values. The difference $Y$ is a constant, and the sample variance of a constant is zero. Thus, $\\operatorname{Var}(Y) = 0$.\n\nThe implementation will compute these quantities via Monte Carlo simulation, adhering to the specified parameters.", "answer": "```python\nimport numpy as np\n\ndef simulate_one_coupled_path(params, T, h, rng):\n    \"\"\"\n    Simulates one pair of coupled paths (fine and coarse) for the SDE.\n    dX_t = a * X_t * dt + sigma * dW_t\n\n    Args:\n        params (tuple): (a, sigma, x0)\n        T (float): Final time.\n        h (float): Fine time step.\n        rng (numpy.random.Generator): Random number generator.\n\n    Returns:\n        tuple: (x_fine, x_coarse) final values of the paths.\n    \"\"\"\n    a, sigma, x0 = params\n    H = 2 * h\n    \n    # Ensure T/h is an even integer for proper coupling.\n    # This is guaranteed by the problem statement for the given test cases.\n    num_fine_steps = int(round(T / h))\n    num_coarse_steps = num_fine_steps // 2\n\n    x_fine = float(x0)\n    x_coarse = float(x0)\n\n    sqrt_h = np.sqrt(h)\n\n    for _ in range(num_coarse_steps):\n        # Generate two fine-scale Brownian increments\n        dw1 = rng.normal(0.0, sqrt_h)\n        dw2 = rng.normal(0.0, sqrt_h)\n        \n        # Store current coarse path value for its update\n        x_coarse_prev = x_coarse\n\n        # Update fine path twice\n        x_fine = x_fine * (1 + a * h) + sigma * dw1\n        x_fine = x_fine * (1 + a * h) + sigma * dw2\n\n        # Update coarse path once using the coupled increment\n        dw_coarse = dw1 + dw2\n        x_coarse = x_coarse_prev * (1 + a * H) + sigma * dw_coarse\n\n    return x_fine, x_coarse\n\ndef compute_variance_of_difference(params, T, h, N, rng):\n    \"\"\"\n    Computes the sample variance of phi(X_T^h) - phi(X_T^H).\n    \"\"\"\n    payoff_diffs = np.zeros(N)\n    \n    for i in range(N):\n        x_fine_T, x_coarse_T = simulate_one_coupled_path(params, T, h, rng)\n        payoff_diffs[i] = np.sin(x_fine_T) - np.sin(x_coarse_T)\n        \n    # Use unbiased sample variance (ddof=1)\n    variance = np.var(payoff_diffs, ddof=1)\n    return variance\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the final result.\n    \"\"\"\n    N = 20000\n    SEED = 42\n    rng = np.random.default_rng(seed=SEED)\n\n    results = []\n\n    # --- Test Case 1: Happy path scaling test ---\n    params_happy = (-1.0, 0.7, 0.5)\n    T_happy = 1.0\n    h_values = [1/64, 1/128, 1/256]\n    \n    variances = []\n    for h in h_values:\n        V = compute_variance_of_difference(params_happy, T_happy, h, N, rng)\n        variances.append(V)\n        S = V / h\n        results.append(S)\n\n    # --- Test Case 2: Ratio check for scaling ---\n    v_64, v_128 = variances[0], variances[1]\n    R = v_128 / v_64 if v_64 != 0 else np.nan\n    B = np.abs(R - 0.5) <= 0.1\n    results.append(R)\n    results.append(B)\n\n    # --- Test Case 3: Driftless boundary ---\n    params_driftless = (0.0, 0.7, 0.5)\n    T_driftless = 1.0\n    h_driftless = 1/64\n    V_driftless = compute_variance_of_difference(params_driftless, T_driftless, h_driftless, N, rng)\n    results.append(V_driftless)\n\n    # --- Test Case 4: Deterministic boundary ---\n    params_det = (-1.0, 0.0, 0.5)\n    T_det = 1.0\n    h_det = 1/64\n    V_det = compute_variance_of_difference(params_det, T_det, h_det, N, rng)\n    results.append(V_det)\n\n    # --- Test Case 5: Short-time edge case ---\n    params_short = (-1.0, 0.7, 0.5)\n    T_short = 1/8\n    h_short = 1/256\n    V_short = compute_variance_of_difference(params_short, T_short, h_short, N, rng)\n    S_short = V_short / h_short\n    results.append(S_short)\n    \n    # Format and print the final output\n    # Ensure boolean is lowercase 'true'/'false' as per Python's str()\n    formatted_results = [f\"{r}\".lower() if isinstance(r, bool) else f\"{r}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n\n```", "id": "2988362"}]}