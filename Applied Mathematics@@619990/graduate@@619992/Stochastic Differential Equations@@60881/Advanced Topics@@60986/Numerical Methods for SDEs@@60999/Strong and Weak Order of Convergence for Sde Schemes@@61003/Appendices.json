{"hands_on_practices": [{"introduction": "To build a solid understanding of numerical methods for SDEs, it is essential to move beyond abstract definitions of convergence and perform concrete error analysis. This first practice problem [@problem_id:2998588] provides a foundational exercise in calculating the leading error constants for the Euler-Maruyama scheme. By working through a multidimensional linear SDE, you will directly derive the coefficients that govern both the strong (pathwise) and weak (in expectation) errors, gaining a quantitative grasp of how these errors accumulate over time.", "problem": "Consider the $d$-dimensional Itô stochastic differential equation (SDE)\n$$\n\\mathrm{d}X_t = M\\,X_t\\,\\mathrm{d}t + D\\,X_t\\,\\mathrm{d}W_t,\\qquad X_0 = x_0\\in\\mathbb{R}^d,\n$$\nwhere $W_t$ is a $d$-dimensional standard Brownian motion with independent components, $M\\in\\mathbb{R}^{d\\times d}$ and $D\\in\\mathbb{R}^{d\\times d}$ are deterministic matrices that commute and are simultaneously diagonalizable. Work in the common eigenbasis so that\n$$\nM = \\mathrm{diag}(\\mu_1,\\dots,\\mu_d),\\qquad D = \\mathrm{diag}(\\sigma_1,\\dots,\\sigma_d),\n$$\nwith $\\mu_i\\in\\mathbb{R}$ and $\\sigma_i\\in\\mathbb{R}$ for $i=1,\\dots,d$. Let $T>0$ be fixed and let $\\{t_n\\}_{n=0}^N$ be a uniform partition of $[0,T]$ with step size $h = T/N$ and $N\\in\\mathbb{N}$.\n\nDefine the Euler–Maruyama (EM) scheme by\n$$\nX_{n+1}^h = X_n^h + M\\,X_n^h\\,h + D\\,X_n^h\\,\\Delta W_n,\\qquad \\Delta W_n := W_{t_{n+1}}-W_{t_n},\n$$\nand denote by $X_T^h := X_N^h$ the terminal EM approximation. For the strong error, define the leading strong mean-square error constant $K_{\\mathrm{s}}$ by the asymptotic expansion\n$$\n\\mathbb{E}\\big[\\,\\|X_T - X_T^h\\|^2\\,\\big] = K_{\\mathrm{s}}\\,h + o(h)\\quad\\text{as}\\quad h\\to 0,\n$$\nwhere $\\|\\cdot\\|$ is the Euclidean norm on $\\mathbb{R}^d$. For the weak error, fix a deterministic vector $u\\in\\mathbb{R}^d$ and the linear functional $\\varphi(x) = u^{\\top}x$, and define the leading weak error constant $K_{\\mathrm{w}}$ by\n$$\n\\mathbb{E}\\big[\\,\\varphi(X_T^h)\\,\\big] - \\mathbb{E}\\big[\\,\\varphi(X_T)\\,\\big] = K_{\\mathrm{w}}\\,h + o(h)\\quad\\text{as}\\quad h\\to 0.\n$$\n\nStarting only from the core definitions of the Itô SDE, the Euler–Maruyama scheme, and basic properties of independent Gaussian increments (including their moment generating function), derive closed-form analytic expressions for $K_{\\mathrm{s}}$ and $K_{\\mathrm{w}}$ in terms of $T$, the diagonal entries $\\mu_i$, $\\sigma_i$, the initial coordinates $x_{0,i}$ of $x_0$, and the components $u_i$ of $u$. Explicitly verify how the dimension $d$ enters these constants. Your final answer must be the pair $(K_{\\mathrm{s}},K_{\\mathrm{w}})$ given as a single closed-form analytic expression. No rounding is required.", "solution": "The problem is well-posed and scientifically grounded. The assumption that the matrices $M$ and $D$ are simultaneously diagonalizable allows us to work in a basis where both are diagonal. This decouples the $d$-dimensional stochastic differential equation (SDE) into $d$ independent scalar SDEs. Let $X_t = (X_t^{(1)}, \\dots, X_t^{(d)})^{\\top}$ and $x_0 = (x_{0,1}, \\dots, x_{0,d})^{\\top}$ be the vector representations in this common eigenbasis. For each component $i \\in \\{1, \\dots, d\\}$, the SDE is a geometric Brownian motion:\n$$\n\\mathrm{d}X_t^{(i)} = \\mu_i X_t^{(i)} \\mathrm{d}t + \\sigma_i X_t^{(i)} \\mathrm{d}W_t^{(i)}, \\qquad X_0^{(i)} = x_{0,i}\n$$\nwhere $W_t^{(i)}$ are independent, one-dimensional standard Brownian motions.\n\nThe exact solution to this scalar SDE can be found using Itô's lemma for $f(x) = \\ln(x)$. This yields:\n$$\nX_t^{(i)} = x_{0,i} \\exp\\left( (\\mu_i - \\frac{1}{2}\\sigma_i^2)t + \\sigma_i W_t^{(i)} \\right)\n$$\nSimilarly, the Euler-Maruyama (EM) scheme decouples component-wise:\n$$\nX_{n+1}^{h,(i)} = X_n^{h,(i)} (1 + \\mu_i h + \\sigma_i \\Delta W_n^{(i)})\n$$\nwhere $X_0^{h,(i)} = x_{0,i}$ and $\\Delta W_n^{(i)} = W_{t_{n+1}}^{(i)} - W_{t_n}^{(i)} \\sim \\mathcal{N}(0,h)$. The terminal value of the EM approximation is given by iterating this relation $N$ times:\n$$\nX_T^{h,(i)} = X_N^{h,(i)} = x_{0,i} \\prod_{n=0}^{N-1} (1 + \\mu_i h + \\sigma_i \\Delta W_n^{(i)})\n$$\n\n### Derivation of the Strong Error Constant $K_{\\mathrm{s}}$\n\nThe mean-square strong error is defined as $\\mathbb{E}\\big[\\,\\|X_T - X_T^h\\|^2\\,\\big]$. Since the problem is posed in an orthonormal eigenbasis, the squared Euclidean norm is the sum of the squared components. By the linearity of expectation and the independence of the components:\n$$\n\\mathbb{E}\\big[\\,\\|X_T - X_T^h\\|^2\\,\\big] = \\mathbb{E}\\left[\\,\\sum_{i=1}^d (X_T^{(i)} - X_T^{h,(i)})^2\\,\\right] = \\sum_{i=1}^d \\mathbb{E}\\big[\\,(X_T^{(i)} - X_T^{h,(i)})^2\\,\\big]\n$$\nThe global mean-square error is approximately the sum of the local mean-square errors over the time grid. The local error accumulates over the interval $[0,T]$. A standard result of SDE numerics states that the global mean-square error of the EM scheme is of order $h$, and for an interval $[0,T]$ it can be approximated by integrating the contribution from the local errors:\n$$\n\\mathbb{E}\\big[\\,(X_T^{(i)} - X_T^{h,(i)})^2\\,\\big] \\approx \\frac{1}{h} \\int_0^T \\mathbb{E}\\left[\\,\\left(X_{t+h}^{(i)} - \\tilde{X}_{t+h}^{(i)}\\right)^2\\,\\right] \\mathrm{d}t\n$$\nwhere $\\tilde{X}_{t+h}^{(i)} = X_t^{(i)} (1 + \\mu_i h + \\sigma_i \\Delta W_t^{(i)})$ is the one-step EM approximation starting from the exact solution $X_t^{(i)}$. The term $\\mathbb{E}[\\dots]/h$ represents the local error contribution per unit time. The dominant term in the local strong error for the EM scheme comes from the absence of the Itô-Taylor expansion term related to $\\int \\int \\mathrm{d}W \\mathrm{d}W$. For the $i$-th component, this corresponds to the Milstein term. The local error is $\\rho_t^{(i)} = X_{t+h}^{(i)} - \\tilde{X}_{t+h}^{(i)}$. Expanding the exact solution $X_{t+h}^{(i)}$ around $X_t^{(i)}$ using an Itô-Taylor expansion gives:\n$$\nX_{t+h}^{(i)} - X_t^{(i)} = \\mu_i X_t^{(i)} h + \\sigma_i X_t^{(i)} \\Delta W_t^{(i)} + \\frac{1}{2}\\sigma_i^2 X_t^{(i)} ((\\Delta W_t^{(i)})^2-h) + \\text{higher order terms}\n$$\nThe one-step EM increment is $\\tilde{X}_{t+h}^{(i)} - X_t^{(i)} = \\mu_i X_t^{(i)} h + \\sigma_i X_t^{(i)} \\Delta W_t^{(i)}$. Thus, the leading term of the local error is:\n$$\n\\rho_t^{(i)} \\approx \\frac{1}{2}\\sigma_i^2 X_t^{(i)} \\left( (\\Delta W_t^{(i)})^2 - h \\right)\n$$\nThe conditional expectation of its square is:\n$$\n\\mathbb{E}\\big[\\,(\\rho_t^{(i)})^2 \\,|\\, \\mathcal{F}_t\\,\\big] \\approx \\frac{1}{4}\\sigma_i^4 (X_t^{(i)})^2 \\mathbb{E}\\big[\\,((\\Delta W_t^{(i)})^2-h)^2\\,\\big]\n$$\nFor a random variable $\\Delta W \\sim \\mathcal{N}(0,h)$, we have $\\mathbb{E}[(\\Delta W)^2]=h$ and $\\mathbb{E}[(\\Delta W)^4]=3h^2$. Therefore,\n$$\n\\mathbb{E}\\big[\\,((\\Delta W)^2-h)^2\\,\\big] = \\mathbb{E}[(\\Delta W)^4] - 2h\\mathbb{E}[(\\Delta W)^2] + h^2 = 3h^2 - 2h(h) + h^2 = 2h^2\n$$\nSo, $\\mathbb{E}\\big[\\,(\\rho_t^{(i)})^2 \\,|\\, \\mathcal{F}_t\\,\\big] \\approx \\frac{1}{4}\\sigma_i^4 (X_t^{(i)})^2 (2h^2) = \\frac{1}{2}\\sigma_i^4 (X_t^{(i)})^2 h^2$.\nThe global error constant accumulates these local errors. Summing over $N=T/h$ steps gives a total error proportional to $N \\times h^2 = (T/h)h^2 = Th$. The constant $K_s$ can be found by integrating the leading local error coefficient over $[0,T]$:\n$$\nK_{\\mathrm{s}} = \\sum_{i=1}^d \\int_0^T \\frac{1}{2}\\sigma_i^4 \\mathbb{E}\\big[\\,(X_t^{(i)})^2\\,\\big] \\mathrm{d}t\n$$\nWe compute $\\mathbb{E}\\big[\\,(X_t^{(i)})^2\\,\\big]$. Let $Z_t = (\\mu_i - \\frac{1}{2}\\sigma_i^2)t + \\sigma_i W_t^{(i)}$, which is normally distributed as $Z_t \\sim \\mathcal{N}\\left((\\mu_i - \\frac{1}{2}\\sigma_i^2)t, \\sigma_i^2 t\\right)$.\n$$\n\\mathbb{E}\\big[\\,(X_t^{(i)})^2\\,\\big] = \\mathbb{E}\\big[\\,x_{0,i}^2 \\exp(2Z_t)\\,\\big] = x_{0,i}^2 \\mathbb{E}\\big[\\,\\exp(2Z_t)\\,\\big]\n$$\nThis is $x_{0,i}^2$ times the moment-generating function of $Z_t$ evaluated at $2$. For a normal variable $Y \\sim \\mathcal{N}(m, s^2)$, $\\mathbb{E}[\\exp(kY)] = \\exp(km + \\frac{1}{2}k^2s^2)$.\n$$\n\\mathbb{E}\\big[\\,\\exp(2Z_t)\\,\\big] = \\exp\\left(2(\\mu_i - \\frac{1}{2}\\sigma_i^2)t + \\frac{1}{2}(2^2)(\\sigma_i^2 t)\\right) = \\exp\\left((2\\mu_i - \\sigma_i^2)t + 2\\sigma_i^2 t\\right) = \\exp((2\\mu_i+\\sigma_i^2)t)\n$$\nThus, $\\mathbb{E}\\big[\\,(X_t^{(i)})^2\\,\\big] = x_{0,i}^2 \\exp((2\\mu_i+\\sigma_i^2)t)$. Substituting this into the integral for $K_{\\mathrm{s}}$:\n$$\nK_{\\mathrm{s}} = \\sum_{i=1}^d \\frac{1}{2}\\sigma_i^4 x_{0,i}^2 \\int_0^T \\exp((2\\mu_i+\\sigma_i^2)t) \\mathrm{d}t\n$$\nIf $2\\mu_i+\\sigma_i^2 \\neq 0$, the integral is $\\frac{\\exp((2\\mu_i+\\sigma_i^2)T)-1}{2\\mu_i+\\sigma_i^2}$. If $2\\mu_i+\\sigma_i^2=0$, the integral evaluates to $T$. The expression $\\frac{\\exp(aT)-1}{a} \\to T$ as $a \\to 0$, so the formula is general.\n$$\nK_{\\mathrm{s}} = \\sum_{i=1}^d \\frac{1}{2}\\sigma_i^4 x_{0,i}^2 \\frac{\\exp((2\\mu_i+\\sigma_i^2)T) - 1}{2\\mu_i+\\sigma_i^2}\n$$\n\n### Derivation of the Weak Error Constant $K_{\\mathrm{w}}$\n\nThe weak error is defined for the linear functional $\\varphi(x) = u^\\top x$. By linearity of expectation:\n$$\n\\mathbb{E}[\\varphi(X_T^h)] - \\mathbb{E}[\\varphi(X_T)] = \\sum_{i=1}^d u_i \\left( \\mathbb{E}[X_T^{h,(i)}] - \\mathbb{E}[X_T^{(i)}] \\right)\n$$\nFirst, let's find the exact mean $\\mathbb{E}[X_T^{(i)}]$. From the SDE, taking expectations gives $\\mathrm{d}\\mathbb{E}[X_t^{(i)}] = \\mu_i \\mathbb{E}[X_t^{(i)}] \\mathrm{d}t$, leading to $\\mathbb{E}[X_T^{(i)}] = x_{0,i} \\exp(\\mu_i T)$.\n\nNext, we find the mean of the EM approximation, $\\mathbb{E}[X_T^{h,(i)}]$. Taking the conditional expectation of the EM scheme:\n$$\n\\mathbb{E}\\big[\\,X_{n+1}^{h,(i)}\\,\\big|\\,\\mathcal{F}_{t_n}\\big] = \\mathbb{E}\\big[\\,X_n^{h,(i)} (1 + \\mu_i h + \\sigma_i \\Delta W_n^{(i)})\\,\\big|\\,\\mathcal{F}_{t_n}\\big] = X_n^{h,(i)} (1+\\mu_i h)\n$$\nsince $\\mathbb{E}[\\Delta W_n^{(i)}] = 0$. Taking the full expectation and iterating from $n=0$:\n$$\n\\mathbb{E}[X_N^{h,(i)}] = (1+\\mu_i h)^N x_{0,i}\n$$\nWith $N = T/h$, we have $\\mathbb{E}[X_T^{h,(i)}] = x_{0,i} (1+\\mu_i h)^{T/h}$. We are interested in the leading error term as $h \\to 0$. We expand $(1+\\mu_i h)^{T/h}$ in powers of $h$:\n$$\n(1+\\mu_i h)^{T/h} = \\exp\\left(\\frac{T}{h}\\ln(1+\\mu_i h)\\right)\n$$\nUsing the Taylor series $\\ln(1+x) = x - \\frac{x^2}{2} + O(x^3)$:\n$$\n\\frac{T}{h} \\ln(1+\\mu_i h) = \\frac{T}{h} \\left(\\mu_i h - \\frac{1}{2}\\mu_i^2 h^2 + O(h^3)\\right) = \\mu_i T - \\frac{1}{2}\\mu_i^2 T h + O(h^2)\n$$\nNow, substitute this back into the exponential, using $\\exp(a+b) = \\exp(a)\\exp(b)$ and $\\exp(x)=1+x+O(x^2)$:\n$$\n\\exp\\left(\\mu_i T - \\frac{1}{2}\\mu_i^2 T h + O(h^2)\\right) = \\exp(\\mu_i T) \\exp\\left(-\\frac{1}{2}\\mu_i^2 T h + O(h^2)\\right) = \\exp(\\mu_i T)\\left(1 - \\frac{1}{2}\\mu_i^2 T h + O(h^2)\\right)\n$$\nThe difference in expectations for the $i$-th component is:\n$$\n\\mathbb{E}[X_T^{h,(i)}] - \\mathbb{E}[X_T^{(i)}] = x_{0,i} \\left(\\exp(\\mu_i T)\\left(1 - \\frac{1}{2}\\mu_i^2 T h\\right) - \\exp(\\mu_i T) + O(h^2)\\right) = -\\frac{1}{2} x_{0,i} \\mu_i^2 T \\exp(\\mu_i T) h + O(h^2)\n$$\nSumming over all components to get the total weak error:\n$$\n\\mathbb{E}[\\varphi(X_T^h)] - \\mathbb{E}[\\varphi(X_T)] = \\sum_{i=1}^d u_i \\left( -\\frac{1}{2} x_{0,i} \\mu_i^2 T \\exp(\\mu_i T) h + O(h^2) \\right)\n$$\nThe coefficient of $h$ is the weak error constant $K_{\\mathrm{w}}$:\n$$\nK_{\\mathrm{w}} = -\\frac{T}{2} \\sum_{i=1}^d u_i x_{0,i} \\mu_i^2 \\exp(\\mu_i T)\n$$\n\n### Conclusion\n\nThe derived constants are:\n$K_{\\mathrm{s}} = \\sum_{i=1}^d \\frac{1}{2}\\sigma_i^4 x_{0,i}^2 \\frac{\\exp((2\\mu_i+\\sigma_i^2)T) - 1}{2\\mu_i+\\sigma_i^2}$\n$K_{\\mathrm{w}} = -\\frac{T}{2} \\sum_{i=1}^d u_i x_{0,i} \\mu_i^2 \\exp(\\mu_i T)$\n\nThe dimension $d$ enters into both constants simply as the number of terms in the summation. This is a direct consequence of the decoupling of the SDE system into $d$ independent scalar SDEs.", "answer": "$$\n\\boxed{\\pmatrix{ \\displaystyle\\sum_{i=1}^d \\frac{1}{2}\\sigma_i^4 x_{0,i}^2 \\frac{\\exp((2\\mu_i+\\sigma_i^2)T) - 1}{2\\mu_i+\\sigma_i^2} & -\\frac{T}{2} \\displaystyle\\sum_{i=1}^d u_i x_{0,i} \\mu_i^2 \\exp(\\mu_i T) }}\n$$", "id": "2998588"}, {"introduction": "The previous exercise focused on the mean-square strong error, which corresponds to the $L^2$ norm. A natural theoretical question is how robust the strong convergence rate is with respect to the choice of error norm. This problem [@problem_id:2998635] challenges you to generalize the analysis of strong convergence to an arbitrary $L^p$ space for $p \\ge 2$. By working through the proof, you will see that while the error constant depends on $p$, the convergence rate of $\\frac{1}{2}$ is a fundamental property of the Euler-Maruyama scheme under standard assumptions.", "problem": "Consider the one-dimensional stochastic differential equation (SDE)\n$$\n\\mathrm{d}X_t = f(X_t)\\,\\mathrm{d}t + g(X_t)\\,\\mathrm{d}W_t,\\quad X_0\\in L^p,\\quad t\\in[0,T],\n$$\nwhere $W_t$ is a standard Brownian motion, $f:\\mathbb{R}\\to\\mathbb{R}$ and $g:\\mathbb{R}\\to\\mathbb{R}$ satisfy global Lipschitz and linear growth conditions, and $p\\geq 2$ is fixed. Let $N\\in\\mathbb{N}$, $h = T/N$, and $t_n = nh$. The Euler–Maruyama (EM) scheme for time discretization is defined by\n$$\nX_{n+1} = X_n + f(X_n)\\,h + g(X_n)\\,\\Delta W_n,\\quad \\Delta W_n := W_{t_{n+1}} - W_{t_n}.\n$$\nDefine the strong error at grid points $t_n$ by $e_n := X(t_n) - X_n$ and the strong convergence rate $r(p)$ by requiring that there exist constants $K_p>0$ independent of $h$ and $N$ such that\n$$\n\\max_{0\\leq n\\leq N}\\|e_n\\|_{L^p} \\leq K_p\\, h^{r(p)}\\quad\\text{for all sufficiently small }h.\n$$\nStarting from the core definitions of strong error and the standard moment bounds for SDE solutions under the assumed conditions, derive a bound of the above form that is valid for every $p\\geq 2$. Explicitly identify how the choice of $p$ influences the constants $K_p$ while leaving the rate exponent $r(p)$ unchanged. Your final answer must be the exact value of the strong convergence rate exponent $r(p)$ as a single number or a closed-form analytic expression. No numerical rounding is required.", "solution": "The objective is to derive the strong convergence rate $r(p)$ for the Euler-Maruyama (EM) scheme applied to the stochastic differential equation (SDE)\n$$\n\\mathrm{d}X_t = f(X_t)\\,\\mathrm{d}t + g(X_t)\\,\\mathrm{d}W_t, \\quad X_0\\in L^p, \\quad t\\in[0,T].\n$$\nThe coefficients $f$ and $g$ are assumed to satisfy global Lipschitz and linear growth conditions. Let $L$ be the Lipschitz constant and $C$ be the linear growth constant. The EM scheme with step size $h=T/N$ produces approximations $X_n$ at times $t_n=nh$.\n\nTo analyze the strong error $e_n = X(t_n) - X_n$, it is convenient to introduce a continuous-time version of the EM approximation. Let $\\kappa(t) = t_n$ for $t \\in [t_n, t_{n+1})$. The continuous-time EM approximation $\\bar{X}_t$ is defined by the integral equation:\n$$\n\\bar{X}_t = X_0 + \\int_0^t f(\\bar{X}_{\\kappa(s)})\\,\\mathrm{d}s + \\int_0^t g(\\bar{X}_{\\kappa(s)})\\,\\mathrm{d}W_s.\n$$\nFrom this definition, it follows that $\\bar{X}_{t_n} = X_n$. The exact solution $X_t$ is given by\n$$\nX_t = X_0 + \\int_0^t f(X_s)\\,\\mathrm{d}s + \\int_0^t g(X_s)\\,\\mathrm{d}W_s.\n$$\nThe error process is $e_t := X_t - \\bar{X}_t$. At the grid points, $e_{t_n} = X_{t_n} - X_n$. Subtracting the two integral equations gives\n$$\ne_t = \\int_0^t (f(X_s) - f(\\bar{X}_{\\kappa(s)}))\\,\\mathrm{d}s + \\int_0^t (g(X_s) - g(\\bar{X}_{\\kappa(s)}))\\,\\mathrm{d}W_s.\n$$\nWe want to find a bound for $\\|e_{t_n}\\|_{L^p} = (\\mathbb{E}[|e_{t_n}|^p])^{1/p}$. We proceed by bounding the $p$-th moment $\\mathbb{E}[|e_t|^p]$. Using the inequality $|a+b|^p \\leq 2^{p-1}(|a|^p + |b|^p)$ for $p \\geq 1$, we have\n$$\n\\mathbb{E}[|e_t|^p] \\leq 2^{p-1} \\mathbb{E}\\left[\\left|\\int_0^t (f(X_s) - f(\\bar{X}_{\\kappa(s)}))\\,\\mathrm{d}s\\right|^p\\right] + 2^{p-1} \\mathbb{E}\\left[\\left|\\int_0^t (g(X_s) - g(\\bar{X}_{\\kappa(s)}))\\,\\mathrm{d}W_s\\right|^p\\right].\n$$\nFor the first term (drift part), we apply Jensen's inequality for integrals:\n$$\n\\mathbb{E}\\left[\\left|\\int_0^t \\dots \\mathrm{d}s\\right|^p\\right] \\leq t^{p-1} \\int_0^t \\mathbb{E}\\left[|f(X_s) - f(\\bar{X}_{\\kappa(s)})|^p\\right]\\,\\mathrm{d}s.\n$$\nFor the second term (diffusion part), we use the Burkholder-Davis-Gundy (BDG) inequality, which states that for an Itô process $M_t = \\int_0^t H_s dW_s$, there exists a constant $C_p$ (depending only on $p$) such that $\\mathbb{E}[|M_t|^p] \\leq C_p \\mathbb{E}[(\\int_0^t |H_s|^2 ds)^{p/2}]$. Applying this and then Jensen's inequality:\n$$\n\\mathbb{E}\\left[\\left|\\int_0^t \\dots \\mathrm{d}W_s\\right|^p\\right] \\leq C_p \\mathbb{E}\\left[\\left(\\int_0^t |g(X_s) - g(\\bar{X}_{\\kappa(s)})|^2\\,\\mathrm{d}s\\right)^{p/2}\\right] \\leq C_p t^{p/2-1} \\int_0^t \\mathbb{E}\\left[|g(X_s) - g(\\bar{X}_{\\kappa(s)})|^p\\right]\\,\\mathrm{d}s.\n$$\nNext, we bound the integrands. Using the global Lipschitz condition with constant $L$ and again $|a+b|^p \\leq 2^{p-1}(|a|^p+|b|^p)$:\n\\begin{align*}\n\\mathbb{E}[|f(X_s) - f(\\bar{X}_{\\kappa(s)})|^p] &\\leq L^p \\mathbb{E}[|X_s - \\bar{X}_{\\kappa(s)}|^p] \\\\\n&= L^p \\mathbb{E}[|(X_s - \\bar{X}_s) + (\\bar{X}_s - \\bar{X}_{\\kappa(s)})|^p] \\\\\n&\\leq L^p 2^{p-1} (\\mathbb{E}[|e_s|^p] + \\mathbb{E}[|\\bar{X}_s - \\bar{X}_{\\kappa(s)}|^p]).\n\\end{align*}\nThe same bound holds for the term involving $g$. Let $\\phi(t) = \\mathbb{E}[|e_t|^p]$. Substituting these bounds back, we obtain an inequality for $\\phi(t)$:\n$$\n\\phi(t) \\leq K_1(p) \\int_0^t \\phi(s)\\,\\mathrm{d}s + K_1(p) \\int_0^t \\mathbb{E}[|\\bar{X}_s - \\bar{X}_{\\kappa(s)}|^p]\\,\\mathrm{d}s,\n$$\nwhere the constant $K_1(p) = 2^{2p-2} L^p (T^{p-1} + C_p T^{p/2-1})$ depends on $p$, $T$, $L$, and the BDG constant $C_p$.\n\nThe next crucial step is to bound the local error term $\\mathbb{E}[|\\bar{X}_s - \\bar{X}_{\\kappa(s)}|^p]$. For $s \\in [t_n, t_{n+1})$, we have $\\kappa(s)=t_n$. From the definition of $\\bar{X}_t$, we have:\n$$\n\\bar{X}_s - \\bar{X}_{t_n} = f(X_n)(s-t_n) + g(X_n)(W_s - W_{t_n}).\n$$\nTaking the $p$-th moment:\n$$\n\\mathbb{E}[|\\bar{X}_s - \\bar{X}_{t_n}|^p] \\leq 2^{p-1} \\left( \\mathbb{E}[|f(X_n)|^p](s-t_n)^p + \\mathbb{E}[|g(X_n)|^p|W_s - W_{t_n}|^p] \\right).\n$$\nUnder the given assumptions, the discrete EM solution has bounded moments, i.e., $\\sup_n \\mathbb{E}[|X_n|^q] < \\infty$ for any $q \\ge 2$. By the linear growth condition, $\\mathbb{E}[|f(X_n)|^p]$ and $\\mathbb{E}[|g(X_n)|^p]$ are uniformly bounded by some constant $M_p$. The increment $W_s - W_{t_n}$ is a Gaussian random variable with mean $0$ and variance $s-t_n$. Thus, $W_s - W_{t_n} \\stackrel{d}{=} \\sqrt{s-t_n} Z$, where $Z \\sim N(0,1)$. The $p$-th moment is $\\mathbb{E}[|W_s-W_{t_n}|^p] = (s-t_n)^{p/2} \\mathbb{E}[|Z|^p]$. Let $m_p = \\mathbb{E}[|Z|^p]$, which is a finite constant for any $p$. So,\n$$\n\\mathbb{E}[|\\bar{X}_s - \\bar{X}_{t_n}|^p] \\leq 2^{p-1} M_p \\left( (s-t_n)^p + m_p (s-t_n)^{p/2} \\right).\n$$\nSince $s-t_n < h$ and we consider $h \\to 0$, we have $s-t_n < 1$. As $p \\geq 2$, it follows that $p > p/2$, and thus $(s-t_n)^p \\leq (s-t_n)^{p/2}$. The dominant term is $(s-t_n)^{p/2}$. Therefore, there exists a constant $K_2(p)$ such that for $s \\in [t_n, t_{n+1})$:\n$$\n\\mathbb{E}[|\\bar{X}_s - \\bar{X}_{\\kappa(s)}|^p] \\leq K_2(p)(s-\\kappa(s))^{p/2}.\n$$\nThe choice of $p$ affects the constants $K_1(p)$ and $K_2(p)$. Specifically, they grow with $p$ due to factors like $2^{2p-2}$, the BDG constant $C_p$, and the Gaussian moment $m_p$.\n\nNow, we compute the integral of this local error term:\n$$\n\\int_0^T \\mathbb{E}[|\\bar{X}_s - \\bar{X}_{\\kappa(s)}|^p]\\,\\mathrm{d}s = \\sum_{n=0}^{N-1} \\int_{t_n}^{t_{n+1}} \\mathbb{E}[|\\bar{X}_s - \\bar{X}_{t_n}|^p]\\,\\mathrm{d}s \\leq \\sum_{n=0}^{N-1} \\int_{t_n}^{t_{n+1}} K_2(p)(s-t_n)^{p/2}\\,\\mathrm{d}s.\n$$\nThe integral over one step is\n$$\n\\int_{t_n}^{t_{n+1}} (s-t_n)^{p/2}\\,\\mathrm{d}s = \\frac{(s-t_n)^{p/2+1}}{p/2+1}\\bigg|_{t_n}^{t_{n+1}} = \\frac{h^{p/2+1}}{p/2+1}.\n$$\nSumming over all $N$ intervals:\n$$\n\\int_0^T \\mathbb{E}[|\\bar{X}_s - \\bar{X}_{\\kappa(s)}|^p]\\,\\mathrm{d}s \\leq N \\frac{K_2(p) h^{p/2+1}}{p/2+1} = \\frac{T}{h} \\frac{K_2(p) h^{p/2+1}}{p/2+1} = K_3(p) h^{p/2},\n$$\nwhere $K_3(p) = T K_2(p) / (p/2+1)$.\n\nWe substitute this result into the inequality for $\\phi(t)$:\n$$\n\\phi(t) \\leq K_1(p) \\int_0^t \\phi(s)\\,\\mathrm{d}s + K_1(p)K_3(p)h^{p/2}.\n$$\nApplying the integral form of Gronwall's lemma, which states that if $\\phi(t) \\leq A \\int_0^t \\phi(s)ds + B$, then $\\phi(t) \\leq B \\exp(At)$, we get:\n$$\n\\phi(t) = \\mathbb{E}[|e_t|^p] \\leq K_1(p)K_3(p)h^{p/2} \\exp(K_1(p)t).\n$$\nLet $K_4(p) = K_1(p)K_3(p)\\exp(K_1(p)T)$. Since this bound holds for all $t \\in [0,T]$, it holds for the grid points $t_n$:\n$$\n\\max_{0\\le n \\le N} \\mathbb{E}[|e_n|^p] \\leq K_4(p) h^{p/2}.\n$$\nTo find the strong convergence rate, we take the $p$-th root:\n$$\n\\max_{0\\le n \\le N} \\|e_n\\|_{L^p} = \\max_{0\\le n \\le N} (\\mathbb{E}[|e_n|^p])^{1/p} \\leq (K_4(p) h^{p/2})^{1/p} = (K_4(p))^{1/p} h^{(p/2)/p} = K_p h^{1/2}.\n$$\nHere, $K_p = (K_4(p))^{1/p}$ is a constant that depends on $p$ but is independent of $h$.\n\nThe analysis reveals that for any $p \\geq 2$, the strong rate of convergence $r(p)$ is $1/2$. The choice of $p$ significantly impacts the magnitude of the error constant $K_p$, but the exponent of $h$ remains unchanged. This rate is determined by the stochastic term in the local error, which scales as $h^{1/2}$.\n\nThe strong convergence rate exponent is therefore $r(p) = 1/2$.", "answer": "$$\\boxed{\\frac{1}{2}}$$", "id": "2998635"}, {"introduction": "While strong convergence is crucial for some applications, many problems in finance and physics only require accurate approximations of expectations, making weak convergence the primary concern. Our focus now shifts from analyzing existing schemes to designing more powerful ones. This advanced problem [@problem_id:2998607] guides you through the construction of a two-stage stochastic Runge-Kutta method that achieves a higher weak order of convergence. This exercise reveals the elegant moment-matching conditions required for such schemes and explains the critical role of using multiple independent random numbers per time step.", "problem": "Consider the Itô stochastic differential equation in $\\mathbb{R}^{n}$ driven by $d$-dimensional Brownian motion,\n$$\n\\mathrm{d}X_{t} \\;=\\; a(X_{t})\\,\\mathrm{d}t \\;+\\; \\sum_{k=1}^{d} b_{k}(X_{t})\\,\\mathrm{d}W_{t}^{k},\n$$\nwhere $a:\\mathbb{R}^{n}\\to\\mathbb{R}^{n}$ and $b_{k}:\\mathbb{R}^{n}\\to\\mathbb{R}^{n}$ are sufficiently smooth with bounded derivatives up to order $4$, and the diffusion vector fields are commutative in the sense that the Lie brackets satisfy $[b_{i},b_{j}](x) \\equiv \\nabla b_{j}(x)\\,b_{i}(x) - \\nabla b_{i}(x)\\,b_{j}(x) = 0$ for all $x\\in\\mathbb{R}^{n}$ and all $i,j\\in\\{1,\\dots,d\\}$. Let $h>0$ denote a fixed time step. The weak order of a one-step method $\\Psi_{h}$ is defined by the requirement that, for every test function $\\varphi\\in C_{b}^{4}(\\mathbb{R}^{n},\\mathbb{R})$, there exists a constant $C(\\varphi)$ independent of $h$ such that\n$$\n\\left| \\mathbb{E}\\big[\\varphi\\big(\\Psi_{h}(x,\\omega)\\big)\\big] \\;-\\; \\mathbb{E}\\big[\\varphi\\big(X_{t+h}\\big)\\,\\big|\\,X_{t}=x\\big] \\right| \\;\\le\\; C(\\varphi)\\, h^{p+1} \\quad \\text{for some } p\\ge 1.\n$$\nYou are asked to design a two-stage stochastic Runge–Kutta method that achieves weak order $2$ under the commutative noise assumption, starting from the Itô–Taylor expansion of the weak flow and the Kolmogorov backward operators\n$$\nL^{0}\\varphi(x) \\;=\\; a(x)\\cdot\\nabla\\varphi(x) \\;+\\; \\tfrac{1}{2}\\sum_{k=1}^{d}\\big(b_{k}(x)b_{k}(x)^{\\top}\\big):\\nabla^{2}\\varphi(x), \n\\qquad\nL^{k}\\varphi(x) \\;=\\; b_{k}(x)\\cdot\\nabla\\varphi(x),\n$$\nand by matching the moments required up to order $4$ in $h$ for the commutative case. In particular, you must explain why the use of multiple independent Gaussian samples per step is necessary to fulfill the weak order two conditions, and you must construct a concrete scheme whose stage values use independent Gaussian perturbations of the same variance as the Brownian increments while remaining independent of them.\n\nConclude by determining, as a function of the Brownian dimension $d$, the minimal number $m(d)$ of independent standard normal scalar samples that must be drawn per time step by your design in order to satisfy the weak order two moment conditions under the commutative noise assumption. Your final answer must be the closed-form analytic expression for $m(d)$. Do not provide an inequality or an equation. No rounding is required.", "solution": "To design a two-stage stochastic Runge-Kutta (SRK) method of weak order $2$ for the given SDE under the commutative noise assumption, we must match the moments of the numerical approximation with those of the true solution up to a sufficiently high order.\n\nFirst, let us establish the target. For a sufficiently smooth test function $\\varphi$, the expectation of $\\varphi(X_{t+h})$ conditional on $X_t=x$ has the following expansion in powers of the time step $h$:\n$$\n\\mathbb{E}[\\varphi(X_{t+h})|X_{t}=x] = \\varphi(x) + h L^0\\varphi(x) + \\frac{h^2}{2}\\left( (L^0)^2 + \\sum_{k=1}^d (L^k)^2 \\right)\\varphi(x) + O(h^3).\n$$\nThis expansion makes use of the commutativity assumption $[b_i, b_j]=0$, which implies that the operators $[L^i, L^j]=0$. The goal is to construct a numerical approximation $Y_1$ starting from $x$ such that $\\mathbb{E}[\\varphi(Y_1)]$ matches this expansion up to terms of order $h^2$.\n\nLet us propose the following two-stage SRK method, which is a well-established design for this problem class, often attributed to Platen and Rößler:\n$$\n\\begin{align*}\nK_1 &= x \\\\\nK_2 &= x + h\\,a(K_1) + \\sum_{k=1}^d b_k(K_1) \\tilde{I}_{(k)} \\\\\nY_1 &= x + \\frac{h}{2}\\left(a(K_1) + a(K_2)\\right) + \\sum_{k=1}^d b_k(K_1) I_{(k)}\n\\end{align*}\n$$\nHere, $I_{(k)}$ and $\\tilde{I}_{(k)}$ are random variables representing the stochastic increments. For the method to achieve weak order $2$, these random variables must satisfy a set of moment conditions. Specifically, their joint moments must match those of two independent $d$-dimensional Wiener process increments.\n\nLet $\\Delta W_h$ and $\\Delta \\tilde{W}_h$ be two independent $d$-dimensional Wiener process increments over the interval $[t, t+h]$, so that $\\Delta W_h^k \\sim N(0,h)$ and $\\Delta \\tilde{W}_h^k \\sim N(0,h)$ are all mutually independent for $k=1,\\dots,d$. The scheme achieves weak order $2$ if we set $I_{(k)} = \\Delta W_h^k$ and $\\tilde{I}_{(k)} = \\Delta \\tilde{W}_h^k$.\n\nLet us analyze why this choice works and why two independent sets of random variables are necessary. The analysis involves expanding $\\mathbb{E}[\\varphi(Y_1)]$ and matching terms with the target expansion.\nLet $\\Delta Y = Y_1 - x$. We have\n$$\n\\mathbb{E}[\\varphi(Y_1)] = \\varphi(x) + \\mathbb{E}[\\Delta Y \\cdot \\nabla\\varphi(x)] + \\frac{1}{2}\\mathbb{E}[(\\Delta Y \\Delta Y^\\top):\\nabla^2\\varphi(x)] + \\dots\n$$\nThe expansion of the numerical scheme's expectation relies on the moments of $I_{(k)}$ and $\\tilde{I}_{(k)}$. The required moment conditions derived from a full weak convergence analysis are:\n1. $I_{(k)}$ and $\\tilde{I}_{(k)}$ must be independent of each other.\n2. The moments of $\\{I_{(k)}\\}_{k=1}^d$ must match those of $\\{\\Delta W_h^k\\}_{k=1}^d$ up to order $4$.\n3. The moments of $\\{\\tilde{I}_{(k)}\\}_{k=1}^d$ must match those of $\\{\\Delta \\tilde{W}_h^k\\}_{k=1}^d$ up to order $4$.\n\nSpecifically, we require for all $j,k,l,m \\in \\{1,\\dots,d\\}$:\n- $\\mathbb{E}[I_{(k)}] = 0$, $\\mathbb{E}[\\tilde{I}_{(k)}] = 0$\n- $\\mathbb{E}[I_{(j)} I_{(k)}] = h\\,\\delta_{jk}$, $\\mathbb{E}[\\tilde{I}_{(j)} \\tilde{I}_{(k)}] = h\\,\\delta_{jk}$\n- $\\mathbb{E}[I_{(j)} \\tilde{I}_{(k)}] = 0$ (This enforces independence/uncorrelatedness)\n- All third-order moments are zero.\n- Fourth-order moments match those of Gaussians, e.g., $\\mathbb{E}[I_{(k)}^4] = 3h^2$, $\\mathbb{E}[I_{(j)}^2 I_{(k)}^2] = h^2$ for $j \\neq k$, and similarly for $\\tilde{I}$.\n\nThe necessity of two independent sets of random variables becomes clear when examining the structure of the weak Itô-Taylor expansion. In the expansion of $\\mathbb{E}[\\varphi(Y_1)]$, cross-terms involving products of the perturbations in the stage values and the final increments appear. For example, a term arises from\n$$\n\\mathbb{E}\\left[ \\left(\\frac{h}{2}(a(K_2) - a(x))\\right) \\cdot \\left(\\sum_{k=1}^d b_k(x) I_{(k)}\\right) \\right].\n$$\nThe term $a(K_2)-a(x)$ depends on $\\tilde{I}_{(j)}$. If $I_{(k)}$ and $\\tilde{I}_{(j)}$ are not independent (or at least uncorrelated), this expectation will be non-zero and of an order that contaminates the final expansion, preventing the scheme from reaching weak order $2$. For instance, the leading term in $a(K_2)-a(x)$ is $\\sum_j \\nabla a(x) b_j(x) \\tilde{I}_{(j)}$. The expectation then involves $\\mathbb{E}[\\tilde{I}_{(j)} I_{(k)}]$. Setting this to zero simplifies the analysis and allows for the correct matching of coefficients. If we choose $I_{(k)}$ and $\\tilde{I}_{(k)}$ to be independent, this expectation vanishes.\n\nFurthermore, the perturbation of the argument of the drift function $a$ in the second stage $K_2$ is essential for capturing the $(L^k)^2\\varphi(x)$ terms. The term $\\mathbb{E}[a(K_2)]$ in the expansion of $\\mathbb{E}[Y_1]$ contributes $h(L^0a)(x)$, which leads to the $(L^0)^2\\varphi(x)$ term. The interaction between the drift and diffusion terms in the true solution expansion is mimicked by the structure of the SRK scheme, where stage values for the drift function are perturbed by stochastic terms modeled on the diffusion part. The use of random variables $\\tilde{I}_{(k)}$ with Gaussian moments ensures these corrective terms are of the correct magnitude.\n\nNow, we determine the minimal number of independent standard normal scalar samples, $m(d)$, required per time step for this design.\nOur scheme requires two sets of random vectors:\n1. $I = (I_{(1)}, \\dots, I_{(d)})^\\top$. We set $I_{(k)} = \\Delta W_h^k = \\sqrt{h}\\,\\zeta_k$, where $\\zeta_k \\sim N(0,1)$ are independent for $k=1, \\dots, d$. This requires $d$ independent standard normal samples.\n2. $\\tilde{I} = (\\tilde{I}_{(1)}, \\dots, \\tilde{I}_{(d)})^\\top$. We set $\\tilde{I}_{(k)} = \\Delta \\tilde{W}_h^k = \\sqrt{h}\\,\\eta_k$, where $\\eta_k \\sim N(0,1)$ are independent for $k=1, \\dots, d$. Crucially, the set $\\{\\eta_k\\}$ must be independent of the set $\\{\\zeta_k\\}$.\n\nLet the total required set of $2d$ standard normal variables be $Z = (\\zeta_1, \\dots, \\zeta_d, \\eta_1, \\dots, \\eta_d)^\\top$. We need to generate $Z$ such that its covariance matrix is the identity matrix, $\\mathrm{Cov}(Z) = I_{2d}$.\nSuppose we generate $Z$ from $m$ underlying independent standard normal scalar samples $\\xi = (\\xi_1, \\dots, \\xi_m)^\\top$ via a linear transformation $Z = M \\xi$, where $M$ is a $(2d) \\times m$ matrix.\nThe covariance of $Z$ is\n$$\n\\mathrm{Cov}(Z) = \\mathbb{E}[Z Z^\\top] = \\mathbb{E}[(M\\xi)(M\\xi)^\\top] = M \\mathbb{E}[\\xi\\xi^\\top] M^\\top = M I_m M^\\top = M M^\\top.\n$$\nFor our scheme to work, we need $\\mathrm{Cov}(Z) = I_{2d}$. This gives the condition $M M^\\top = I_{2d}$.\nThe rank of a matrix product is at most the minimum of the ranks of the factors. The rank of the $(2d) \\times m$ matrix $M$ is at most $m$. Thus, the rank of $M M^\\top$ is at most $m$. The rank of the identity matrix $I_{2d}$ is $2d$.\nThis leads to the inequality:\n$$\n\\mathrm{rank}(I_{2d}) \\le \\mathrm{rank}(M M^\\top) \\le m \\implies 2d \\le m.\n$$\nThe minimal number of independent standard normal samples required is therefore $m(d) = 2d$. This minimum is achieved by simply drawing $2d$ independent standard normal samples and partitioning them into the two required sets $\\{\\zeta_k\\}_{k=1}^d$ and $\\{\\eta_k\\}_{k=1}^d$.\n\nWhile more advanced schemes or different choices of random variables (e.g., discrete) might reduce this number for other designs, for the specified two-stage SRK design which relies on two independent Gaussian-like increments, this is the minimal number.", "answer": "$$\n\\boxed{2d}\n$$", "id": "2998607"}]}