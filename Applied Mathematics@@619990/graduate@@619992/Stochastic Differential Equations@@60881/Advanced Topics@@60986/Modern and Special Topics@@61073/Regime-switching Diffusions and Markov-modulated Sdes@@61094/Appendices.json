{"hands_on_practices": [{"introduction": "Before analyzing the full regime-switching diffusion, we must first understand its driving force: the underlying continuous-time Markov chain (CTMC). This foundational exercise focuses on calculating the stationary distribution $\\pi$ from the generator matrix $Q$. This distribution quantifies the long-term proportion of time the system spends in each operational mode, providing a crucial baseline for understanding the statistical properties of the entire coupled process [@problem_id:2993994].", "problem": "Consider a continuous-time Markov chain with finite state set $S=\\{1,2,3\\}$ and conservative generator matrix $Q\\in\\mathbb{R}^{3\\times 3}$ given by\n$$\nQ=\\begin{pmatrix}\n-5 & 2 & 3 \\\\\n1 & -4 & 3 \\\\\n4 & 2 & -6\n\\end{pmatrix}.\n$$\nAssume the chain is irreducible. In the context of a regime-switching diffusion, this chain modulates coefficients of a stochastic differential equation via its stationary regime occupancy. Compute the stationary distribution $\\pi=(\\pi_1,\\pi_2,\\pi_3)$ by solving $\\pi Q=0$ together with $\\sum_{i\\in S}\\pi_i=1$. In addition, state the conditions under which this stationary distribution is unique and strictly positive, expressed in terms of structural properties of $Q$ and the chain. The final answer must be the stationary distribution $\\pi$ expressed as a single row matrix. No rounding is required.", "solution": "The user wants me to solve the following problem:\nConsider a continuous-time Markov chain with finite state set $S=\\{1,2,3\\}$ and conservative generator matrix $Q\\in\\mathbb{R}^{3\\times 3}$ given by\n$$\nQ=\\begin{pmatrix}\n-5 & 2 & 3 \\\\\n1 & -4 & 3 \\\\\n4 & 2 & -6\n\\end{pmatrix}.\n$$\nAssume the chain is irreducible. In the context of a regime-switching diffusion, this chain modulates coefficients of a stochastic differential equation via its stationary regime occupancy. Compute the stationary distribution $\\pi=(\\pi_1,\\pi_2,\\pi_3)$ by solving $\\pi Q=0$ together with $\\sum_{i\\in S}\\pi_i=1$. In addition, state the conditions under which this stationary distribution is unique and strictly positive, expressed in terms of structural properties of $Q$ and the chain. The final answer must be the stationary distribution $\\pi$ expressed as a single row matrix. No rounding is required.\n\n### Step 1: Extract Givens\n- State set of the continuous-time Markov chain: $S=\\{1,2,3\\}$.\n- Generator matrix: $Q=\\begin{pmatrix} -5 & 2 & 3 \\\\ 1 & -4 & 3 \\\\ 4 & 2 & -6 \\end{pmatrix}$.\n- The matrix $Q$ is conservative, meaning the sum of each row is $0$.\n- The chain is assumed to be irreducible.\n- The stationary distribution $\\pi = (\\pi_1, \\pi_2, \\pi_3)$ must be computed.\n- The defining equations for the stationary distribution are:\n  1. $\\pi Q = 0$, where $0$ is the zero vector in $\\mathbb{R}^3$.\n  2. $\\sum_{i=1}^{3} \\pi_i = 1$.\n- The secondary task is to state the conditions for the uniqueness and strict positivity of $\\pi$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is first validated for scientific soundness, completeness, and objectivity.\n\n1.  **Scientific and Factual Soundness:** The problem is a standard exercise in the theory of continuous-time Markov chains, a core topic in stochastic processes. The concept of a generator matrix $Q$, stationary distribution $\\pi$, and the defining equations $\\pi Q = 0$ and $\\sum \\pi_i = 1$ are fundamental and mathematically correct. The given matrix $Q$ is a valid generator matrix for a conservative chain: its off-diagonal entries are non-negative, and its row sums are all zero:\n    - Row $1$: $-5+2+3 = 0$\n    - Row $2$: $1-4+3 = 0$\n    - Row $3$: $4+2-6 = 0$\n2.  **Completeness and Consistency:** The problem is self-contained. It provides the state space, the generator matrix, and the explicit assumption of irreducibility. Let's verify this assumption. The non-zero off-diagonal entries $q_{ij}$ for $i \\neq j$ indicate possible transitions $i \\to j$. From $Q$, we have transitions $1 \\to 2$, $1 \\to 3$, $2 \\to 1$, $2 \\to 3$, $3 \\to 1$, and $3 \\to 2$. This implies that every state is reachable from every other state, so the graph of the chain is strongly connected. Thus, the chain is indeed irreducible, and the given assumption is consistent with the provided matrix $Q$.\n3.  **Well-Posedness:** For any finite-state, irreducible continuous-time Markov chain, a unique stationary distribution exists. The problem provides all necessary information to find this unique solution.\n4.  **Objectivity:** The problem is stated in precise mathematical language, free from ambiguity or subjective claims.\n\nThe problem is deemed **valid** as it is scientifically sound, well-posed, and complete.\n\n### Step 3: Verdict and Action\nThe problem is valid. I will proceed with the solution.\n\nFirst, we address the conditions for uniqueness and strict positivity of the stationary distribution. For a continuous-time Markov chain with a finite state space, a stationary distribution $\\pi$ exists. A fundamental theorem states that this stationary distribution is unique and strictly positive (i.e., $\\pi_i > 0$ for all states $i$) if and only if the chain is irreducible. The problem statement explicitly assumes irreducibility. Structurally, this corresponds to the graph of transitions defined by the non-zero off-diagonal elements of the generator matrix $Q$ being strongly connected. As verified during validation, the provided matrix $Q$ corresponds to an irreducible chain.\n\nNext, we compute the stationary distribution $\\pi = (\\pi_1, \\pi_2, \\pi_3)$ by solving the system of linear equations given by $\\pi Q = 0$ and the normalization condition $\\sum_{i=1}^3 \\pi_i = 1$.\n\nThe matrix equation $\\pi Q = 0$ is:\n$$\n(\\pi_1, \\pi_2, \\pi_3) \\begin{pmatrix} -5 & 2 & 3 \\\\ 1 & -4 & 3 \\\\ 4 & 2 & -6 \\end{pmatrix} = (0, 0, 0)\n$$\nThis yields a system of three linear equations:\n1.  $-5\\pi_1 + \\pi_2 + 4\\pi_3 = 0$\n2.  $2\\pi_1 - 4\\pi_2 + 2\\pi_3 = 0$\n3.  $3\\pi_1 + 3\\pi_2 - 6\\pi_3 = 0$\n\nSince $Q$ is a generator matrix for a conservative chain, its rows sum to zero, which means the column vector $(1, 1, 1)^T$ is in the null space of $Q^T$. This implies that the columns of $Q$ are linearly dependent, and therefore the three equations above are linearly dependent. We can use any two of these equations along with the normalization condition.\n\nLet's simplify equations $2$ and $3$:\n- From equation $2$: Dividing by $2$ gives $\\pi_1 - 2\\pi_2 + \\pi_3 = 0$. (Eq. A)\n- From equation $3$: Dividing by $3$ gives $\\pi_1 + \\pi_2 - 2\\pi_3 = 0$. (Eq. B)\n\nFrom Eq. A, we can express $\\pi_3$ in terms of $\\pi_1$ and $\\pi_2$:\n$$\n\\pi_3 = 2\\pi_2 - \\pi_1\n$$\nSubstitute this expression for $\\pi_3$ into Eq. B:\n$$\n\\pi_1 + \\pi_2 - 2(2\\pi_2 - \\pi_1) = 0\n$$\n$$\n\\pi_1 + \\pi_2 - 4\\pi_2 + 2\\pi_1 = 0\n$$\n$$\n3\\pi_1 - 3\\pi_2 = 0\n$$\nThis simplifies to $\\pi_1 = \\pi_2$.\n\nNow, substitute $\\pi_2 = \\pi_1$ into the expression for $\\pi_3$:\n$$\n\\pi_3 = 2\\pi_1 - \\pi_1 = \\pi_1\n$$\nThus, we have found that $\\pi_1 = \\pi_2 = \\pi_3$.\n\nWe must verify this relationship using the first equation, $-5\\pi_1 + \\pi_2 + 4\\pi_3 = 0$:\n$$\n-5\\pi_1 + (\\pi_1) + 4(\\pi_1) = -5\\pi_1 + 5\\pi_1 = 0\n$$\nThe relationship is consistent with all three equations from $\\pi Q = 0$.\n\nFinally, we use the normalization condition $\\pi_1 + \\pi_2 + \\pi_3 = 1$:\n$$\n\\pi_1 + \\pi_1 + \\pi_1 = 1\n$$\n$$\n3\\pi_1 = 1\n$$\n$$\n\\pi_1 = \\frac{1}{3}\n$$\nSince $\\pi_1 = \\pi_2 = \\pi_3$, it follows that $\\pi_1 = \\frac{1}{3}$, $\\pi_2 = \\frac{1}{3}$, and $\\pi_3 = \\frac{1}{3}$.\n\nThe stationary distribution is the row vector $\\pi = (\\pi_1, \\pi_2, \\pi_3)$.\n$$\n\\pi = \\left(\\frac{1}{3}, \\frac{1}{3}, \\frac{1}{3}\\right)\n$$\nThis distribution is indeed unique and strictly positive, as expected for an irreducible chain.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{1}{3} & \\frac{1}{3} & \\frac{1}{3}\n\\end{pmatrix}\n}\n$$", "id": "2993994"}, {"introduction": "Building on the properties of the driving Markov chain, we now turn to the dynamics of the coupled system. A powerful way to build intuition is to solve the stochastic differential equation along a fixed, known path of the chain. This practice guides you through deriving the explicit solution for a Markov-modulated Ornstein-Uhlenbeck (OU) process, demonstrating how the solution is \"stitched together\" across regime changes and how memory of past states decays at a rate governed by the specific sequence of regimes [@problem_id:2993989].", "problem": "Consider a finite-state continuous-time Markov chain $I_t$ with state space $\\mathcal{S} = \\{1, \\dots, M\\}$ and a standard Brownian motion $W_t$ on a filtered probability space satisfying the usual conditions. Assume $I_t$ and $W_t$ are independent. Define the Markov-modulated Ornstein–Uhlenbeck (OU) process $X_t$ by the stochastic differential equation\n$$\ndX_t \\;=\\; -\\kappa(I_t)\\bigl(X_t - \\theta(I_t)\\bigr)\\,dt \\;+\\; \\sigma(I_t)\\,dW_t,\\qquad X_0 = x_0,\n$$\nwhere $\\kappa:\\mathcal{S}\\to(0,\\infty)$, $\\theta:\\mathcal{S}\\to\\mathbb{R}$, and $\\sigma:\\mathcal{S}\\to(0,\\infty)$ are given functions.\n\nFix a deterministic regime path $i:[0,T]\\to \\mathcal{S}$ that is piecewise constant with a finite number of switches in $(0,T)$ at times $0=\\tau_0<\\tau_1<\\cdots<\\tau_n<\\tau_{n+1}=T$. On each half-open interval $[\\tau_j,\\tau_{j+1})$, the regime is constant $i(t)=i_j\\in\\mathcal{S}$, and hence the coefficients are constant $\\kappa_j := \\kappa(i_j)$, $\\theta_j := \\theta(i_j)$, and $\\sigma_j := \\sigma(i_j)$.\n\nStarting from first principles for linear stochastic differential equations and using only foundational results from Itô calculus, derive the explicit closed-form expression for $X_T$ conditional on the fixed regime path $i(\\cdot)$. Your derivation must make clear why the solution is continuous in time and how the piecewise constant coefficients enter the solution through the switching times. In particular, explain how the mean-reversion speed and level change at the switching times $\\tau_j$ and how these changes are reflected in the conditional solution.\n\nProvide the final answer as a single closed-form analytic expression for $X_T$ in terms of $x_0$, $\\{\\kappa_j,\\theta_j,\\sigma_j\\}_{j=0}^n$, $\\{\\tau_j\\}_{j=0}^{n+1}$, and stochastic integrals with respect to $W_t$. No numerical evaluation is required. The final answer must be a single expression and must not include any discussion or commentary.", "solution": "### Step 1: Extract Givens\n-   **Process Definition:** A Markov-modulated Ornstein–Uhlenbeck (OU) process $X_t$ is defined by the stochastic differential equation (SDE):\n    $$\n    dX_t \\;=\\; -\\kappa(I_t)\\bigl(X_t - \\theta(I_t)\\bigr)\\,dt \\;+\\; \\sigma(I_t)\\,dW_t\n    $$\n-   **Initial Condition:** $X_0 = x_0$.\n-   **Modulating Process:** $I_t$ is a continuous-time Markov chain on a finite state space $\\mathcal{S} = \\{1, \\dots, M\\}$.\n-   **Noise Process:** $W_t$ is a standard Brownian motion. $I_t$ and $W_t$ are independent.\n-   **Coefficients:** The functions $\\kappa:\\mathcal{S}\\to(0,\\infty)$, $\\theta:\\mathcal{S}\\to\\mathbb{R}$, and $\\sigma:\\mathcal{S}\\to(0,\\infty)$ define the mean-reversion speed, mean-reversion level, and volatility, respectively, for each state of the Markov chain.\n-   **Conditional Path:** The problem considers a fixed, deterministic, piecewise constant regime path $i:[0,T]\\to \\mathcal{S}$.\n-   **Path Structure:** There is a partition of the interval $[0,T]$ given by $0=\\tau_0<\\tau_1<\\cdots<\\tau_n<\\tau_{n+1}=T$.\n-   **Piecewise Constant Coefficients:** For any $t \\in [\\tau_j, \\tau_{j+1})$ where $j \\in \\{0, 1, \\dots, n\\}$, the regime is constant, $i(t)=i_j\\in\\mathcal{S}$. This implies the SDE coefficients are also piecewise constant: $\\kappa(i(t)) = \\kappa_j$, $\\theta(i(t)) = \\theta_j$, and $\\sigma(i(t)) = \\sigma_j$.\n\n### Step 2: Validate Using Extracted Givens\n-   **Scientifically Grounded:** The problem describes a Markov-modulated OU process, a standard and important model in stochastic analysis, physics, and mathematical finance. The task is to find a conditional solution along a deterministic path, which is a well-defined mathematical exercise based on the foundational principles of Itô calculus. The problem is sound.\n-   **Well-Posed:** The SDE is linear with piecewise continuous (and thus locally bounded) coefficients. Given an initial condition, the existence and uniqueness of a strong solution is guaranteed by standard SDE theory. The request for a closed-form expression is feasible for this specific type of linear SDE.\n-   **Objective:** The problem is stated in precise, formal mathematical language, with no ambiguity or subjective elements.\n-   **Completeness and Consistency:** All necessary information—the SDE, initial condition, and a complete description of the coefficient path—is provided. There are no contradictions in the setup.\n-   **Other Checks:** The problem is not trivial, as it requires a careful application of Itô calculus and step-by-step integration. It is directly relevant to the topic of regime-switching diffusions. It does not contain any of the other flaws listed in the validation criteria.\n\n### Step 3: Verdict and Action\nThe problem is valid. A complete, reasoned solution will be provided.\n\n### Derivation of the Solution\nThe analysis begins by treating the SDE as a linear first-order stochastic differential equation with time-varying coefficients. The SDE is\n$$\ndX_t \\;=\\; -\\kappa(i(t))\\bigl(X_t - \\theta(i(t))\\bigr)\\,dt \\;+\\; \\sigma(i(t))\\,dW_t.\n$$\nThis can be rearranged into the standard form for a linear SDE:\n$$\ndX_t + \\kappa(i(t)) X_t \\,dt \\;=\\; \\kappa(i(t))\\theta(i(t))\\,dt \\;+\\; \\sigma(i(t))\\,dW_t.\n$$\nTo solve this equation, we use the integrating factor method, which is a foundational technique. Let the integrating factor be $U_t = \\exp\\left(\\int_0^t \\kappa(i(s))\\,ds\\right)$. Since the path $i(\\cdot)$ is deterministic, the function $U_t$ is also deterministic and differentiable with respect to $t$.\n\nWe consider the process $Y_t = U_t X_t$. Using Itô's product rule for the differential $dY_t$:\n$$\ndY_t = d(U_t X_t) = \\left(\\frac{dU_t}{dt}\\right)X_t\\,dt + U_t dX_t.\n$$\nThe derivative of the integrating factor is $\\frac{dU_t}{dt} = \\kappa(i(t))\\exp\\left(\\int_0^t \\kappa(i(s))\\,ds\\right) = \\kappa(i(t))U_t$. Substituting this and the expression for $dX_t$ into the equation for $dY_t$:\n$$\ndY_t = (\\kappa(i(t))U_t)X_t\\,dt + U_t \\left(-\\kappa(i(t)) X_t \\,dt + \\kappa(i(t))\\theta(i(t))\\,dt + \\sigma(i(t))\\,dW_t\\right).\n$$\nThe terms involving $X_t$ cancel out:\n$$\ndY_t = \\kappa(i(t))U_t X_t\\,dt - \\kappa(i(t))U_t X_t \\,dt + U_t \\kappa(i(t))\\theta(i(t))\\,dt + U_t \\sigma(i(t))\\,dW_t.\n$$\nThis simplifies to:\n$$\ndY_t = U_t\\left(\\kappa(i(t))\\theta(i(t))\\,dt + \\sigma(i(t))\\,dW_t\\right).\n$$\nIntegrating both sides from $t=0$ to $t=T$:\n$$\nY_T - Y_0 = \\int_0^T U_s \\kappa(i(s))\\theta(i(s))\\,ds + \\int_0^T U_s \\sigma(i(s))\\,dW_s.\n$$\nSubstituting back $Y_t = U_t X_t$:\n$$\nU_T X_T - U_0 X_0 = \\int_0^T U_s \\kappa(i(s))\\theta(i(s))\\,ds + \\int_0^T U_s \\sigma(i(s))\\,dW_s.\n$$\nSince $U_0 = \\exp(0) = 1$, we can solve for $X_T$:\n$$\nX_T = U_T^{-1} X_0 + U_T^{-1} \\int_0^T U_s \\kappa(i(s))\\theta(i(s))\\,ds + U_T^{-1} \\int_0^T U_s \\sigma(i(s))\\,dW_s.\n$$\nBringing the $U_T^{-1}$ term inside the integrals and using the definition $U_t = \\exp\\left(\\int_0^t \\kappa(i(s))\\,ds\\right)$:\n$$\nU_T^{-1} U_s = \\exp\\left(-\\int_0^T \\kappa(i(u))\\,du\\right)\\exp\\left(\\int_0^s \\kappa(i(u))\\,du\\right) = \\exp\\left(-\\int_s^T \\kappa(i(u))\\,du\\right).\n$$\nThis yields the general solution for $X_T$:\n$$\nX_T = X_0 \\exp\\left(-\\int_0^T \\kappa(i(s))\\,ds\\right) + \\int_0^T \\exp\\left(-\\int_s^T \\kappa(i(u))\\,du\\right) \\kappa(i(s))\\theta(i(s))\\,ds + \\int_0^T \\exp\\left(-\\int_s^T \\kappa(i(u))\\,du\\right) \\sigma(i(s))\\,dW_s.\n$$\nThe solution path $t \\mapsto X_t$ is almost surely continuous because it is the solution to an SDE with locally bounded coefficients driven by a continuous process $W_t$. The solution at each switching time $\\tau_j$ serves as the initial condition for the subsequent interval, ensuring continuity of the entire path.\n\nNow, we specialize this general expression using the given piecewise constant path $i(t)$. The path is defined by regimes $i_j$ on intervals $[\\tau_j, \\tau_{j+1})$, with corresponding constant coefficients $\\kappa_j$, $\\theta_j$, and $\\sigma_j$.\n\nFirst, we evaluate the exponential decay terms. For the term multiplying $X_0$:\n$$\n\\int_0^T \\kappa(i(s))\\,ds = \\sum_{j=0}^{n} \\int_{\\tau_j}^{\\tau_{j+1}} \\kappa_j\\,ds = \\sum_{j=0}^{n} \\kappa_j(\\tau_{j+1}-\\tau_j).\n$$\nFor the integrands, we consider $s \\in [\\tau_j, \\tau_{j+1})$ for some $j \\in \\{0, \\dots, n\\}$. The integral in the exponent is:\n$$\n\\int_s^T \\kappa(i(u))\\,du = \\int_s^{\\tau_{j+1}} \\kappa_j\\,du + \\sum_{k=j+1}^{n} \\int_{\\tau_k}^{\\tau_{k+1}} \\kappa_k\\,du = \\kappa_j(\\tau_{j+1}-s) + \\sum_{k=j+1}^{n} \\kappa_k(\\tau_{k+1}-\\tau_k).\n$$\nNote that for $j=n$, the sum $\\sum_{k=n+1}^{n}$ is empty and its value is $0$.\n\nThe overall integrals from $0$ to $T$ can be decomposed into a sum of integrals over the subintervals $[\\tau_j, \\tau_{j+1})$:\n- **Term 1 (Initial Condition):**\n$$\nX_0 \\exp\\left(-\\sum_{j=0}^{n} \\kappa_j(\\tau_{j+1}-\\tau_j)\\right).\n$$\n\n- **Term 2 (Deterministic Integral):**\n\\begin{align*}\n& \\int_0^T \\exp\\left(-\\int_s^T \\kappa(i(u))\\,du\\right) \\kappa(i(s))\\theta(i(s))\\,ds \\\\\n&= \\sum_{j=0}^{n} \\int_{\\tau_j}^{\\tau_{j+1}} \\exp\\left(-\\kappa_j(\\tau_{j+1}-s) - \\sum_{k=j+1}^{n} \\kappa_k(\\tau_{k+1}-\\tau_k)\\right) \\kappa_j\\theta_j\\,ds \\\\\n&= \\sum_{j=0}^{n} \\kappa_j\\theta_j \\exp\\left(-\\sum_{k=j+1}^{n} \\kappa_k(\\tau_{k+1}-\\tau_k)\\right) \\int_{\\tau_j}^{\\tau_{j+1}} \\exp\\left(-\\kappa_j(\\tau_{j+1}-s)\\right)\\,ds.\n\\end{align*}\nThe integral with respect to $s$ evaluates to:\n$$\n\\int_{\\tau_j}^{\\tau_{j+1}} \\exp\\left(-\\kappa_j(\\tau_{j+1}-s)\\right)\\,ds = \\left[\\frac{1}{\\kappa_j}\\exp\\left(-\\kappa_j(\\tau_{j+1}-s)\\right)\\right]_{s=\\tau_j}^{s=\\tau_{j+1}} = \\frac{1}{\\kappa_j}\\left(1 - \\exp(-\\kappa_j(\\tau_{j+1}-\\tau_j))\\right).\n$$\nThus, the deterministic term becomes:\n$$\n\\sum_{j=0}^{n} \\theta_j \\left(1 - \\exp(-\\kappa_j(\\tau_{j+1}-\\tau_j))\\right) \\exp\\left(-\\sum_{k=j+1}^{n} \\kappa_k(\\tau_{k+1}-\\tau_k)\\right).\n$$\n\n- **Term 3 (Stochastic Integral):**\n\\begin{align*}\n& \\int_0^T \\exp\\left(-\\int_s^T \\kappa(i(u))\\,du\\right) \\sigma(i(s))\\,dW_s \\\\\n&= \\sum_{j=0}^{n} \\int_{\\tau_j}^{\\tau_{j+1}} \\exp\\left(-\\kappa_j(\\tau_{j+1}-s) - \\sum_{k=j+1}^{n} \\kappa_k(\\tau_{k+1}-\\tau_k)\\right) \\sigma_j\\,dW_s \\\\\n&= \\sum_{j=0}^{n} \\sigma_j \\exp\\left(-\\sum_{k=j+1}^{n} \\kappa_k(\\tau_{k+1}-\\tau_k)\\right) \\int_{\\tau_j}^{\\tau_{j+1}} \\exp(-\\kappa_j(\\tau_{j+1}-s))\\,dW_s.\n\\end{align*}\n\nCombining these three terms gives the final closed-form expression for $X_T$. The expression explicitly shows how the solution is built up from contributions over each constant-regime interval. The parameters $(\\kappa_j, \\theta_j)$ define the local mean-reverting dynamics within interval $j$. The final state $X_T$ is a weighted sum where the influence of each interval's dynamics is attenuated by the mean-reversion speeds $\\kappa_k$ of all subsequent intervals, reflecting how memory of past states decays over time at a rate determined by the regime path.\n\nThe full expression is:\n$$\nX_T = X_0 \\exp\\left(-\\sum_{j=0}^{n} \\kappa_j(\\tau_{j+1}-\\tau_j)\\right) + \\sum_{j=0}^{n} \\theta_j \\left(1 - \\exp(-\\kappa_j(\\tau_{j+1}-\\tau_j))\\right) \\exp\\left(-\\sum_{k=j+1}^{n} \\kappa_k(\\tau_{k+1}-\\tau_k)\\right) + \\sum_{j=0}^{n} \\sigma_j \\exp\\left(-\\sum_{k=j+1}^{n} \\kappa_k(\\tau_{k+1}-\\tau_k)\\right) \\int_{\\tau_j}^{\\tau_{j+1}} \\exp(-\\kappa_j(\\tau_{j+1}-s))\\,dW_s.\n$$", "answer": "$$\n\\boxed{\nX_0 \\exp\\left(-\\sum_{j=0}^{n} \\kappa_j(\\tau_{j+1}-\\tau_j)\\right) + \\sum_{j=0}^{n} \\left[ \\theta_j \\left(1 - \\exp(-\\kappa_j(\\tau_{j+1}-\\tau_j))\\right) + \\sigma_j \\int_{\\tau_j}^{\\tau_{j+1}} \\exp(-\\kappa_j(\\tau_{j+1}-s))\\,dW_s \\right] \\exp\\left(-\\sum_{k=j+1}^{n} \\kappa_k(\\tau_{k+1}-\\tau_k)\\right)\n}\n$$", "id": "2993989"}, {"introduction": "This final practice moves from the analysis of a single path to the long-term statistical behavior of the entire system. You will explore a key asymptotic result: the invariant density of a modulated OU process in the slow-switching limit, where the system has ample time to equilibrate within each regime. By applying the Fokker-Planck formalism, you will derive the elegant result that the overall stationary density is a mixture of the invariant densities from each regime, weighted by the stationary probabilities $\\pi_i$ of the driving Markov chain [@problem_id:2993985].", "problem": "Consider a continuous-time Markov chain (CTMC) $I_t$ taking values in the finite set $\\{1,\\dots,n\\}$ with generator matrix $Q = (q_{ij})_{i,j=1}^{n}$, where $q_{ij} \\ge 0$ for $i \\ne j$, $q_{ii} = -\\sum_{j \\ne i} q_{ij}$, and assume the chain is irreducible so it has a unique stationary distribution $\\pi = (\\pi_1,\\dots,\\pi_n)$ satisfying $\\pi Q = 0$, $\\sum_{i=1}^{n} \\pi_i = 1$, and $\\pi_i > 0$ for all $i$. Let $W_t$ be a standard one-dimensional Brownian motion independent of $I_t$. Consider the Markov-modulated Ornstein–Uhlenbeck (OU) process $X_t$ defined by the stochastic differential equation\n$$\n\\mathrm{d}X_t = -a_{I_t} \\bigl(X_t - m_{I_t}\\bigr)\\,\\mathrm{d}t + \\sigma_{I_t}\\,\\mathrm{d}W_t,\n$$\nwhere for each regime $i \\in \\{1,\\dots,n\\}$, the mean-reversion rate $a_i > 0$, the offset $m_i \\in \\mathbb{R}$, and the diffusion coefficient $\\sigma_i > 0$ are constants. Introduce a time-scale separation by replacing $Q$ with $\\varepsilon Q$ for a small parameter $\\varepsilon > 0$, so that regime switching is rare relative to the OU relaxation within each regime.\n\nStarting from the Kolmogorov forward (Fokker–Planck) description of the joint stationary density components $p_i(x)$ for $(X_t,I_t)$ and rigorously justifying any limiting steps you take, derive the invariant marginal density $p(x)$ of $X_t$ in the slow-switching limit $\\varepsilon \\to 0^{+}$. Your final answer must be a single closed-form analytic expression for $p(x)$, written as a mixture of Gaussian densities weighted by the stationary regime probabilities $\\pi_i$. No rounding is required, and no physical units are involved.", "solution": "The problem asks for the stationary marginal probability density function $p(x)$ of a Markov-modulated Ornstein-Uhlenbeck (OU) process $X_t$ in the limit of slow regime switching. We will derive this by starting with the Kolmogorov forward (Fokker-Planck) equations for the joint stationary density of the process $(X_t, I_t)$.\n\nLet $p_i(x)$ be the stationary joint probability density, such that $p_i(x) \\mathrm{d}x = \\mathbb{P}(X_t \\in (x, x+\\mathrm{d}x), I_t = i)$. The process $(X_t, I_t)$ is a Markov process on the state space $\\mathbb{R} \\times \\{1, \\dots, n\\}$. The time evolution of the density components $p_i(x, t)$ is governed by a system of Fokker-Planck equations. In the stationary state, $\\frac{\\partial p_i}{\\partial t} = 0$, and the system of equations is given by:\n$$\n0 = -\\frac{\\partial}{\\partial x} \\left[ -a_i(x-m_i) p_i(x) \\right] + \\frac{1}{2} \\frac{\\partial^2}{\\partial x^2} \\left[ \\sigma_i^2 p_i(x) \\right] + \\sum_{j=1}^{n} (\\varepsilon q_{ji}) p_j(x)\n$$\nfor each $i \\in \\{1, \\dots, n\\}$. The first two terms on the right-hand side describe the drift and diffusion of the OU process within regime $i$, while the third term describes the change in probability due to jumps between regimes, governed by the scaled generator matrix $\\varepsilon Q$.\n\nLet us define the Fokker-Planck operator for the dynamics within regime $i$ as $\\mathcal{L}_i^*$:\n$$\n\\mathcal{L}_i^* p(x) = \\frac{\\partial}{\\partial x} \\left[ a_i(x-m_i) p(x) \\right] + \\frac{1}{2}\\frac{\\partial^2}{\\partial x^2} \\left[ \\sigma_i^2 p(x) \\right]\n$$\nThe system of stationary equations can then be written more compactly as:\n$$\n\\mathcal{L}_i^* p_i(x) + \\varepsilon \\sum_{j=1}^{n} q_{ji} p_j(x) = 0, \\quad i = 1, \\dots, n\n$$\nWe are interested in the limit $\\varepsilon \\to 0^+$. This is a singular perturbation problem. We seek a solution by positing an asymptotic expansion for each $p_i(x)$ in powers of $\\varepsilon$:\n$$\np_i(x) = p_i^{(0)}(x) + \\varepsilon p_i^{(1)}(x) + O(\\varepsilon^2)\n$$\nSubstituting this expansion into the system and collecting terms of the same order in $\\varepsilon$, we obtain a hierarchy of equations.\n\n**Zeroth-Order Approximation ($O(\\varepsilon^0)$):**\nAt the leading order, the equations are:\n$$\n\\mathcal{L}_i^* p_i^{(0)}(x) = 0, \\quad i = 1, \\dots, n\n$$\nThis is the stationary Fokker-Planck equation for an uncoupled OU process in regime $i$. The equation is equivalent to $\\frac{\\partial J_i(x)}{\\partial x} = 0$, where $J_i(x)$ is the probability current:\n$$\nJ_i(x) = -a_i(x-m_i) p_i^{(0)}(x) - \\frac{1}{2}\\sigma_i^2 \\frac{\\mathrm{d}p_i^{(0)}}{\\mathrm{d}x}\n$$\nFor a normalizable probability density on $\\mathbb{R}$, we must have boundary conditions $\\lim_{x\\to\\pm\\infty} p_i^{(0)}(x) = 0$ and $\\lim_{x\\to\\pm\\infty} \\frac{\\mathrm{d}p_i^{(0)}}{\\mathrm{d}x}(x) = 0$. This implies that the probability current $J_i(x)$ must be zero for all $x$. Setting $J_i(x)=0$ gives a first-order ordinary differential equation for $p_i^{(0)}(x)$:\n$$\n\\frac{\\mathrm{d}p_i^{(0)}}{p_i^{(0)}} = -\\frac{2a_i}{\\sigma_i^2}(x-m_i)\\,\\mathrm{d}x\n$$\nIntegrating this equation yields:\n$$\n\\ln p_i^{(0)}(x) = -\\frac{a_i}{\\sigma_i^2}(x-m_i)^2 + \\text{const} \\implies p_i^{(0)}(x) = K_i \\exp\\left(-\\frac{a_i(x-m_i)^2}{\\sigma_i^2}\\right)\n$$\nwhere $K_i$ are constants of integration. This solution corresponds to a Gaussian distribution. The stationary variance of an OU process with parameters $a_i$ and $\\sigma_i$ is $v_i = \\frac{\\sigma_i^2}{2a_i}$. So we can write $p_i^{(0)}(x)$ in terms of a normalized Gaussian probability density function $\\phi(x; \\mu, \\sigma^2)$:\n$$\np_i^{(0)}(x) = C_i \\phi(x; m_i, v_i) = C_i \\frac{1}{\\sqrt{2\\pi v_i}} \\exp\\left(-\\frac{(x-m_i)^2}{2v_i}\\right)\n$$\nHere, $C_i$ are undetermined constants representing the total probability mass in regime $i$ at this order of approximation. The integral of $p_i^{(0)}(x)$ over $\\mathbb{R}$ is simply $C_i$.\n\n**First-Order Approximation ($O(\\varepsilon^1)$):**\nThe equations for the first-order correction terms $p_i^{(1)}(x)$ are:\n$$\n\\mathcal{L}_i^* p_i^{(1)}(x) = - \\sum_{j=1}^{n} q_{ji} p_j^{(0)}(x)\n$$\nFor a well-behaved solution $p_i^{(1)}(x)$ to exist (specifically, one that vanishes at infinity), a solvability condition must be met. This condition is obtained by integrating the equation over $x \\in (-\\infty, \\infty)$. The integral of the left-hand side is:\n$$\n\\int_{-\\infty}^{\\infty} \\mathcal{L}_i^* p_i^{(1)}(x) \\,\\mathrm{d}x = \\int_{-\\infty}^{\\infty} \\frac{\\partial}{\\partial x} \\left( a_i(x-m_i) p_i^{(1)}(x) + \\frac{1}{2}\\sigma_i^2 \\frac{\\mathrm{d}p_i^{(1)}}{\\mathrm{d}x} \\right) \\,\\mathrm{d}x\n$$\nAssuming $p_i^{(1)}(x)$ and its derivative vanish at infinity, this integral is zero by the fundamental theorem of calculus. Therefore, the integral of the right-hand side must also be zero:\n$$\n\\int_{-\\infty}^{\\infty} \\left( - \\sum_{j=1}^{n} q_{ji} p_j^{(0)}(x) \\right) \\,\\mathrm{d}x = 0\n$$\n$$\n\\sum_{j=1}^{n} q_{ji} \\int_{-\\infty}^{\\infty} p_j^{(0)}(x) \\,\\mathrm{d}x = 0\n$$\nSince $\\int_{-\\infty}^{\\infty} p_j^{(0)}(x) \\,\\mathrm{d}x = C_j$, the solvability condition becomes:\n$$\n\\sum_{j=1}^{n} q_{ji} C_j = 0, \\quad \\text{for } i=1, \\dots, n\n$$\nIn matrix notation, this is $Q^\\top C = 0$, where $C = (C_1, \\dots, C_n)^\\top$. This is the defining equation for the stationary distribution of the Markov chain $I_t$. Since the chain is assumed to be irreducible, there is a unique solution for $C$ up to a scaling factor. This solution must be proportional to the stationary distribution vector $\\pi = (\\pi_1, \\dots, \\pi_n)^\\top$. Thus, $C = k \\pi$ for some constant $k$.\n\nTo determine the constant $k$, we use the normalization of the total probability density. The marginal density of $X_t$ is $p(x) = \\sum_{i=1}^n p_i(x)$. Its integral over $\\mathbb{R}$ must be $1$:\n$$\n\\int_{-\\infty}^{\\infty} p(x) \\,\\mathrm{d}x = \\sum_{i=1}^{n} \\int_{-\\infty}^{\\infty} p_i(x) \\,\\mathrm{d}x = 1\n$$\nSubstituting the expansion for $p_i(x)$ and integrating, we get:\n$$\n\\sum_{i=1}^{n} \\left( \\int p_i^{(0)}(x)\\,\\mathrm{d}x + \\varepsilon \\int p_i^{(1)}(x)\\,\\mathrm{d}x + \\dots \\right) = \\sum_{i=1}^{n} (C_i + O(\\varepsilon)) = 1\n$$\nIn the limit $\\varepsilon \\to 0^+$, this implies $\\sum_{i=1}^n C_i = 1$. Since $C_i = k \\pi_i$, we have:\n$$\nk \\sum_{i=1}^{n} \\pi_i = 1\n$$\nBy definition of a probability distribution, $\\sum_{i=1}^n \\pi_i = 1$, so we find that $k=1$. This uniquely determines the constants $C_i = \\pi_i$.\n\n**Limiting Marginal Density:**\nThe zeroth-order approximations for the partial densities are therefore:\n$$\np_i^{(0)}(x) = \\pi_i \\phi(x; m_i, v_i) = \\pi_i \\phi\\left(x; m_i, \\frac{\\sigma_i^2}{2a_i}\\right)\n$$\nThe marginal stationary density $p(x)$ is the sum of the partial densities $p_i(x)$. In the slow-switching limit $\\varepsilon \\to 0^+$, the higher-order terms in the expansion vanish, and the limiting density is given by the sum of the zeroth-order terms:\n$$\np(x) = \\lim_{\\varepsilon \\to 0^+} \\sum_{i=1}^{n} p_i(x) = \\sum_{i=1}^{n} p_i^{(0)}(x)\n$$\nSubstituting the expressions for $p_i^{(0)}(x)$ gives the final answer:\n$$\np(x) = \\sum_{i=1}^{n} \\pi_i \\frac{1}{\\sqrt{2\\pi \\frac{\\sigma_i^2}{2a_i}}} \\exp\\left(-\\frac{(x-m_i)^2}{2 \\frac{\\sigma_i^2}{2a_i}}\\right)\n$$\nSimplifying the terms, we get:\n$$\np(x) = \\sum_{i=1}^{n} \\pi_i \\sqrt{\\frac{a_i}{\\pi \\sigma_i^2}} \\exp\\left(-\\frac{a_i(x-m_i)^2}{\\sigma_i^2}\\right)\n$$\nThis expression represents a mixture of Gaussian densities. Each component corresponds to the stationary distribution of the OU process if it were confined to a single regime $i$, and the components are weighted by the stationary probabilities $\\pi_i$ of the underlying Markov chain. This result is intuitively correct: in the slow-switching limit, the process spends long sojourns in each state, allowing the fast variable $X_t$ to equilibrate to its local stationary distribution before the next switch occurs.", "answer": "$$\n\\boxed{\\sum_{i=1}^{n} \\pi_i \\sqrt{\\frac{a_i}{\\pi \\sigma_i^2}} \\exp\\left(-\\frac{a_i(x-m_i)^2}{\\sigma_i^2}\\right)}\n$$", "id": "2993985"}]}