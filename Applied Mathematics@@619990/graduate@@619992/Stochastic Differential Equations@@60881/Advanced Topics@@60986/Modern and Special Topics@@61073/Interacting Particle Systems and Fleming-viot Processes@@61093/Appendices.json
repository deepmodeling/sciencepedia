{"hands_on_practices": [{"introduction": "A crucial first step in analyzing any stochastic process is to understand its long-term behavior. For processes that reach a statistical equilibrium, this is described by the invariant probability measure. This practice focuses on deriving this measure for the Wright–Fisher diffusion, a cornerstone model in population genetics that describes the evolution of allele frequencies under mutation and genetic drift. By applying the stationary Fokker–Planck equation with a zero-flux boundary condition, you will unveil the celebrated Beta distribution as the system's equilibrium state, providing a deep connection between the process's dynamics and its ultimate fate. [@problem_id:2981156]", "problem": "Consider a neutral Fleming–Viot limit of an interacting particle system on a one-dimensional type space, where the empirical type frequency converges to a one-dimensional diffusion of allele frequency. Let $X_{t} \\in [0,1]$ denote the frequency of a focal type at time $t$. Assume neutral resampling (no selection) and parent-independent linear mutation toward a baseline mean type level $\\mu \\in (0,1)$ with overall mutation intensity $\\theta>0$. The macroscopic limit is a one-dimensional diffusion with generator\n$$\n\\mathcal{L} f(x) \\;=\\; \\frac{1}{2}\\,x(1-x)\\,f''(x) \\;+\\; \\frac{\\theta}{2}\\,(\\mu - x)\\,f'(x),\n$$\nacting on twice continuously differentiable test functions $f$ on $[0,1]$. The process is constrained by reflecting boundaries at $x=0$ and $x=1$ in the sense that the stationary probability current vanishes at the endpoints.\n\nUsing only the fundamental definitions of the Kolmogorov forward equation (also called the Fokker–Planck equation) for one-dimensional diffusions and the reflecting boundary condition interpreted as zero probability flux at the boundaries, compute the invariant probability density $\\pi(x)$ on $[0,1]$ of $X_{t}$. Express your final answer as a single closed-form analytic expression in terms of $x$, $\\theta$, and $\\mu$. No numerical approximation is required.", "solution": "The problem will first be validated for scientific soundness, well-posedness, and objectivity.\n\n### Step 1: Extract Givens\n\n-   **Process**: A neutral Fleming-Viot limit of an interacting particle system.\n-   **Type Space**: A one-dimensional space.\n-   **State Variable**: $X_{t} \\in [0,1]$ is the frequency of a focal type at time $t$.\n-   **Dynamics Assumptions**: Neutral resampling (no selection) and parent-independent linear mutation.\n-   **Mutation Parameters**: The baseline mean type level is $\\mu \\in (0,1)$, and the overall mutation intensity is $\\theta > 0$.\n-   **Diffusion Generator**: The macroscopic limit is a one-dimensional diffusion with the generator $\\mathcal{L}$ acting on twice continuously differentiable test functions $f$ on $[0,1]$:\n    $$\n    \\mathcal{L} f(x) \\;=\\; \\frac{1}{2}\\,x(1-x)\\,f''(x) \\;+\\; \\frac{\\theta}{2}\\,(\\mu - x)\\,f'(x)\n    $$\n-   **Boundary Conditions**: The process has reflecting boundaries at $x=0$ and $x=1$, which is interpreted as the stationary probability current vanishing at the endpoints.\n\n### Step 2: Validate Using Extracted Givens\n\n-   **Scientifically Grounded**: The problem describes the Wright-Fisher diffusion model with mutation, a canonical and extensively studied process in population genetics and probability theory. The generator $\\mathcal{L}$ is the standard generator for this diffusion. The concepts of an invariant measure, the Fokker-Planck equation, and reflecting boundaries are fundamental and rigorously defined within the theory of stochastic processes. The problem is firmly rooted in established mathematical and scientific principles.\n-   **Well-Posed**: The task is to find the unique stationary probability density for a given diffusion on a compact interval with specified boundary conditions. The generator is well-defined, and the parameters $\\theta > 0$ and $\\mu \\in (0,1)$ are in their standard, valid ranges, ensuring a non-degenerate solution. The problem is structured to admit a unique, stable, and meaningful solution.\n-   **Objective**: The problem is stated using precise, formal mathematical language. It contains no subjective claims, ambiguities, or non-technical terms.\n\n### Step 3: Verdict and Action\n\nThe problem is scientifically sound, well-posed, objective, and self-contained. It is therefore deemed **valid**. A solution will be derived from first principles as requested.\n\nThe invariant (or stationary) probability density, denoted $\\pi(x)$, is the time-independent solution to the Kolmogorov forward equation, also known as the Fokker-Planck equation. For a one-dimensional diffusion process $X_t$ with drift coefficient $a(x)$ and diffusion coefficient $b(x)$, the Fokker-Planck equation for the probability density $p(x,t)$ is\n$$\n\\frac{\\partial p(x,t)}{\\partial t} = -\\frac{\\partial}{\\partial x} \\left[ a(x) p(x,t) \\right] + \\frac{1}{2}\\frac{\\partial^2}{\\partial x^2} \\left[ b(x) p(x,t) \\right].\n$$\nThe generator of a diffusion is related to these coefficients by $\\mathcal{L}f(x) = a(x)f'(x) + \\frac{1}{2}b(x)f''(x)$. By comparing this general form with the given generator,\n$$\n\\mathcal{L} f(x) = \\frac{\\theta}{2}(\\mu - x) f'(x) + \\frac{1}{2}x(1-x) f''(x),\n$$\nwe identify the drift and diffusion coefficients for the process $X_t$:\n-   Drift coefficient: $a(x) = \\frac{\\theta}{2}(\\mu-x)$\n-   Diffusion coefficient: $b(x) = x(1-x)$\n\nThe stationary density $\\pi(x)$ is defined by the condition $\\frac{\\partial \\pi(x)}{\\partial t} = 0$. The stationary Fokker-Planck equation is therefore\n$$\n0 = -\\frac{d}{dx} \\left[ a(x) \\pi(x) \\right] + \\frac{1}{2}\\frac{d^2}{dx^2} \\left[ b(x) \\pi(x) \\right].\n$$\nIntegrating this equation with respect to $x$ yields\n$$\n\\text{constant} = -a(x) \\pi(x) + \\frac{1}{2}\\frac{d}{dx} \\left[ b(x) \\pi(x) \\right].\n$$\nThe right-hand side is the negative of the probability current, or flux, $J(x) = a(x)\\pi(x) - \\frac{1}{2}\\frac{d}{dx}[b(x)\\pi(x)]$. The stationary equation thus implies that the probability flux $J(x)$ must be constant for all $x \\in [0,1]$.\n\nThe problem specifies reflecting boundaries, which means that there is zero net flow of probability mass across the boundaries. This is enforced by setting the probability flux to zero at the endpoints: $J(0) = 0$ and $J(1)=0$. Since $J(x)$ must be constant, it must be that $J(x)=0$ for all $x \\in [0,1]$.\n\nWe must therefore solve the first-order ordinary differential equation:\n$$\na(x)\\pi(x) - \\frac{1}{2}\\frac{d}{dx}[b(x)\\pi(x)] = 0.\n$$\nSubstituting the expressions for $a(x)$ and $b(x)$:\n$$\n\\frac{\\theta}{2}(\\mu-x)\\pi(x) - \\frac{1}{2}\\frac{d}{dx}[x(1-x)\\pi(x)] = 0.\n$$\nMultiplying by $2$ and rearranging gives\n$$\n\\frac{d}{dx}[x(1-x)\\pi(x)] = \\theta(\\mu-x)\\pi(x).\n$$\nApplying the product rule to the left side:\n$$\nx(1-x)\\pi'(x) + (1-2x)\\pi(x) = \\theta(\\mu-x)\\pi(x).\n$$\nThis is a separable differential equation. We collect terms involving $\\pi(x)$:\n$$\nx(1-x)\\pi'(x) = [\\theta(\\mu-x) - (1-2x)]\\pi(x)\n$$\n$$\nx(1-x)\\pi'(x) = (\\theta\\mu - 1 + (2-\\theta)x)\\pi(x).\n$$\nSeparating variables $\\pi$ and $x$:\n$$\n\\frac{\\pi'(x)}{\\pi(x)} = \\frac{\\theta\\mu - 1 + (2-\\theta)x}{x(1-x)}.\n$$\nTo integrate the right-hand side, we use partial fraction decomposition:\n$$\n\\frac{\\theta\\mu - 1 + (2-\\theta)x}{x(1-x)} = \\frac{A}{x} + \\frac{B}{1-x}.\n$$\nSolving for $A$ and $B$:\n-   $A = \\left.\\frac{\\theta\\mu - 1 + (2-\\theta)x}{1-x}\\right|_{x=0} = \\theta\\mu - 1$.\n-   $B = \\left.\\frac{\\theta\\mu - 1 + (2-\\theta)x}{x}\\right|_{x=1} = \\theta\\mu - 1 + 2 - \\theta = \\theta(1-\\mu) - 1$.\n\nSubstituting back and integrating both sides:\n$$\n\\int \\frac{d\\pi}{\\pi} = \\int \\left( \\frac{\\theta\\mu - 1}{x} + \\frac{\\theta(1-\\mu) - 1}{1-x} \\right) dx.\n$$\n$$\n\\ln(\\pi(x)) = (\\theta\\mu - 1)\\ln(x) - (\\theta(1-\\mu) - 1)\\ln(1-x) + C_1,\n$$\nwhere $C_1$ is the constant of integration. For $x \\in (0,1)$, the arguments of the logarithms are positive.\nExponentiating both sides:\n$$\n\\pi(x) = \\exp(C_1) \\cdot \\exp((\\theta\\mu - 1)\\ln(x)) \\cdot \\exp(-(\\theta(1-\\mu) - 1)\\ln(1-x))\n$$\n$$\n\\pi(x) = C \\cdot x^{\\theta\\mu-1} (1-x)^{\\theta(1-\\mu)-1},\n$$\nwhere $C = \\exp(C_1)$ is a normalization constant.\n\nThe constant $C$ is determined by the condition that $\\pi(x)$ must be a probability density, so its integral over the state space $[0,1]$ must equal $1$:\n$$\n\\int_0^1 \\pi(x) dx = \\int_0^1 C \\cdot x^{\\theta\\mu-1} (1-x)^{\\theta(1-\\mu)-1} dx = 1.\n$$\nThe integral is the definition of the Beta function, $B(\\alpha, \\beta) = \\int_0^1 t^{\\alpha-1}(1-t)^{\\beta-1} dt$, with parameters $\\alpha = \\theta\\mu$ and $\\beta = \\theta(1-\\mu)$.\nTherefore,\n$$\nC \\cdot B(\\theta\\mu, \\theta(1-\\mu)) = 1.\n$$\nThe Beta function is related to the Gamma function $\\Gamma(z)$ by $B(\\alpha, \\beta) = \\frac{\\Gamma(\\alpha)\\Gamma(\\beta)}{\\Gamma(\\alpha+\\beta)}$. In our case, $\\alpha+\\beta = \\theta\\mu + \\theta(1-\\mu) = \\theta$.\nSo,\n$$\nC \\cdot \\frac{\\Gamma(\\theta\\mu)\\Gamma(\\theta(1-\\mu))}{\\Gamma(\\theta)} = 1.\n$$\nSolving for $C$:\n$$\nC = \\frac{\\Gamma(\\theta)}{\\Gamma(\\theta\\mu)\\Gamma(\\theta(1-\\mu))}.\n$$\nSubstituting this normalization constant back into the expression for $\\pi(x)$, we obtain the final form of the invariant probability density:\n$$\n\\pi(x) = \\frac{\\Gamma(\\theta)}{\\Gamma(\\theta\\mu)\\Gamma(\\theta(1-\\mu))} x^{\\theta\\mu - 1} (1-x)^{\\theta(1-\\mu) - 1}.\n$$\nThis is the probability density function for a Beta distribution with shape parameters $\\alpha = \\theta\\mu$ and $\\beta = \\theta(1-\\mu)$.", "answer": "$$\n\\boxed{\\frac{\\Gamma(\\theta)}{\\Gamma(\\theta\\mu)\\Gamma(\\theta(1-\\mu))} x^{\\theta\\mu - 1} (1-x)^{\\theta(1-\\mu) - 1}}\n$$", "id": "2981156"}, {"introduction": "While the invariant measure describes where a process settles, the generator explains how it moves from moment to moment. This practice delves into the heart of Fleming–Viot dynamics by asking you to derive the process's generator from its fundamental martingale problem formulation. Using the multivariate Itô formula, you will compute the drift of a polynomial functional of the measure-valued process. This exercise demystifies the structure of the generator, revealing how the distinct terms for mutation and resampling (genetic drift) emerge from the underlying stochastic calculus. [@problem_id:2981136]", "problem": "Consider a neutral Fleming–Viot measure-valued diffusion $\\{\\mu_t:t\\ge 0\\}$ on a compact Polish type space $E$ with mutation described by the generator $A$ of a conservative Feller semigroup on $C_b(E)$ and resampling rate $\\kappa>0$. Assume that for every $\\phi\\in\\mathcal{D}(A)$, the process\n$$\nM_t(\\phi) \\equiv \\langle \\mu_t,\\phi\\rangle - \\langle \\mu_0,\\phi\\rangle - \\int_0^t \\langle \\mu_s, A\\phi\\rangle\\,ds\n$$\nis a square-integrable martingale with quadratic covariation specified by\n$$\nd\\langle M(\\phi),M(\\psi)\\rangle_t \\;=\\; \\kappa\\left(\\langle \\mu_t,\\phi\\psi\\rangle - \\langle \\mu_t,\\phi\\rangle\\,\\langle \\mu_t,\\psi\\rangle\\right)\\,dt,\n\\qquad \\phi,\\psi\\in\\mathcal{D}(A).\n$$\nFix $m\\in\\mathbb{N}$ and functions $\\phi_1,\\ldots,\\phi_m\\in\\mathcal{D}(A)\\cap C_b(E)$. Consider the polynomial functional on probability measures\n$$\nF(\\mu)\\;=\\;\\prod_{i=1}^m \\langle \\mu,\\phi_i\\rangle.\n$$\nUsing only the martingale characterization above, the multivariate Itô formula for semimartingales, and the product rule for derivatives, compute the drift coefficient in the semimartingale decomposition of $F(\\mu_t)$ and thereby identify the action of the generator $G$ of the Fleming–Viot process on $F$. Your final answer must be a single closed-form analytic expression for $G F(\\mu)$ in terms of $\\mu$, $\\kappa$, $A$, and the family $\\{\\phi_i\\}_{i=1}^m$.", "solution": "The problem is valid. It presents a standard, well-posed question in the theory of measure-valued stochastic processes, specifically the characterization of the generator of a Fleming–Viot process. All terms are mathematically precise, and the premises are consistent with established theory.\n\nThe objective is to find the action of the generator $G$ of the Fleming–Viot process $\\{\\mu_t\\}_{t \\ge 0}$ on the polynomial functional $F(\\mu) = \\prod_{i=1}^m \\langle \\mu, \\phi_i \\rangle$. The generator $G$ is identified from the drift term in the semimartingale decomposition of the process $Y_t = F(\\mu_t)$. The decomposition of $Y_t$ is of the form $dY_t = G F(\\mu_t) dt + d(\\text{martingale})_t$. We will compute this decomposition using the multivariate Itô formula.\n\nLet us define the vector of real-valued processes $X_t = (X_t^{(1)}, \\ldots, X_t^{(m)})$, where each component is given by $X_t^{(i)} = \\langle \\mu_t, \\phi_i \\rangle$ for $i \\in \\{1, \\ldots, m\\}$. The functional $F(\\mu_t)$ can then be written as a function of $X_t$:\n$$\nF(\\mu_t) = f(X_t^{(1)}, \\ldots, X_t^{(m)}) = \\prod_{i=1}^m X_t^{(i)}.\n$$\nThe problem provides the martingale characterization of the Fleming–Viot process. For any $\\phi \\in \\mathcal{D}(A)$, the process $M_t(\\phi) = \\langle \\mu_t, \\phi \\rangle - \\langle \\mu_0, \\phi \\rangle - \\int_0^t \\langle \\mu_s, A\\phi \\rangle \\,ds$ is a martingale. In differential form, the semimartingale decomposition for each $X_t^{(i)}$ is:\n$$\ndX_t^{(i)} = \\langle \\mu_t, A\\phi_i \\rangle \\,dt + dM_t(\\phi_i).\n$$\nThis decomposition separates the process into a finite-variation drift part and a martingale part. The quadratic covariation between the martingale parts $M_t(\\phi_i)$ and $M_t(\\phi_j)$ is given, which is identical to the quadratic covariation of the processes $X_t^{(i)}$ and $X_t^{(j)}$ themselves. For any $i,j \\in \\{1,\\ldots,m\\}$:\n$$\nd\\langle X^{(i)}, X^{(j)} \\rangle_t = d\\langle M(\\phi_i), M(\\phi_j) \\rangle_t = \\kappa \\left( \\langle \\mu_t, \\phi_i \\phi_j \\rangle - \\langle \\mu_t, \\phi_i \\rangle \\langle \\mu_t, \\phi_j \\rangle \\right) dt.\n$$\nWe apply the multivariate Itô formula to $Y_t = f(X_t)$:\n$$\ndY_t = \\sum_{i=1}^m \\frac{\\partial f}{\\partial x_i}(X_t) \\,dX_t^{(i)} + \\frac{1}{2} \\sum_{i,j=1}^m \\frac{\\partial^2 f}{\\partial x_i \\partial x_j}(X_t) \\,d\\langle X^{(i)}, X^{(j)} \\rangle_t.\n$$\nThe function is $f(x_1, \\ldots, x_m) = \\prod_{k=1}^m x_k$. We compute its partial derivatives using the product rule:\nThe first-order partial derivative with respect to $x_i$ is:\n$$\n\\frac{\\partial f}{\\partial x_i} = \\prod_{k \\ne i} x_k = \\frac{f(x_1, \\ldots, x_m)}{x_i}.\n$$\nThe second-order partial derivatives are:\nFor $i \\ne j$:\n$$\n\\frac{\\partial^2 f}{\\partial x_i \\partial x_j} = \\frac{\\partial}{\\partial x_j} \\left( \\prod_{k \\ne i} x_k \\right) = \\prod_{k \\ne i, j} x_k = \\frac{f(x_1, \\ldots, x_m)}{x_i x_j}.\n$$\nFor $i=j$:\n$$\n\\frac{\\partial^2 f}{\\partial x_i^2} = \\frac{\\partial}{\\partial x_i} \\left( \\prod_{k \\ne i} x_k \\right) = 0.\n$$\nNow, we substitute these derivatives evaluated at $X_t$, along with the expressions for $dX_t^{(i)}$ and $d\\langle X^{(i)}, X^{(j)} \\rangle_t$, into the Itô formula.\n\nThe first term of Itô's formula becomes:\n$$\n\\sum_{i=1}^m \\frac{\\partial f}{\\partial x_i}(X_t) \\,dX_t^{(i)} = \\sum_{i=1}^m \\left( \\prod_{k \\ne i} X_t^{(k)} \\right) dX_t^{(i)} = \\sum_{i=1}^m \\left( \\prod_{k \\ne i} \\langle \\mu_t, \\phi_k \\rangle \\right) \\left( \\langle \\mu_t, A\\phi_i \\rangle \\,dt + dM_t(\\phi_i) \\right).\n$$\nThe second term of Itô's formula, noting that the second partials are non-zero only for $i \\ne j$, becomes:\n$$\n\\frac{1}{2} \\sum_{i \\ne j} \\frac{\\partial^2 f}{\\partial x_i \\partial x_j}(X_t) \\,d\\langle X^{(i)}, X^{(j)} \\rangle_t = \\frac{1}{2} \\sum_{i \\ne j} \\left( \\prod_{k \\ne i,j} X_t^{(k)} \\right) \\kappa \\left( \\langle \\mu_t, \\phi_i \\phi_j \\rangle - X_t^{(i)} X_t^{(j)} \\right) dt.\n$$\nSubstituting $X_t^{(i)} = \\langle \\mu_t, \\phi_i \\rangle$:\n$$\n\\frac{\\kappa}{2} \\sum_{i \\ne j} \\left( \\prod_{k \\ne i,j} \\langle \\mu_t, \\phi_k \\rangle \\right) \\left( \\langle \\mu_t, \\phi_i \\phi_j \\rangle - \\langle \\mu_t, \\phi_i \\rangle \\langle \\mu_t, \\phi_j \\rangle \\right) dt.\n$$\nCombining these results gives the full stochastic differential for $Y_t = F(\\mu_t)$:\n$$\ndY_t = \\sum_{i=1}^m \\left(\\prod_{k \\ne i} \\langle \\mu_t, \\phi_k \\rangle\\right) \\langle \\mu_t, A\\phi_i \\rangle \\,dt + \\frac{\\kappa}{2} \\sum_{i \\ne j} \\left(\\prod_{k \\ne i,j} \\langle \\mu_t, \\phi_k \\rangle\\right) \\left( \\langle \\mu_t, \\phi_i \\phi_j \\rangle - \\langle \\mu_t, \\phi_i \\rangle \\langle \\mu_t, \\phi_j \\rangle \\right) dt + d(\\text{martingale})_t.\n$$\nThe drift coefficient of this process is, by definition, the action of the generator $G$ on the functional $F$. We can write the expression for $GF(\\mu)$ by taking the coefficient of $dt$ and replacing $\\mu_t$ with a generic measure $\\mu$:\n$$\nGF(\\mu) = \\sum_{i=1}^m \\left(\\prod_{k \\ne i} \\langle \\mu, \\phi_k \\rangle\\right) \\langle \\mu, A\\phi_i \\rangle + \\frac{\\kappa}{2} \\sum_{i \\ne j} \\left(\\prod_{k \\ne i,j} \\langle \\mu, \\phi_k \\rangle\\right) \\left( \\langle \\mu, \\phi_i \\phi_j \\rangle - \\langle \\mu, \\phi_i \\rangle \\langle \\mu, \\phi_j \\rangle \\right).\n$$\nThe sum $\\sum_{i \\ne j}$ runs over all $m(m-1)$ ordered pairs $(i,j)$ where $i \\ne j$. The summand is symmetric with respect to swapping $i$ and $j$. Therefore, we can rewrite the sum over the $\\binom{m}{2}$ unordered pairs $\\{i,j\\}$ with $i<j$, doubling the result:\n$$\n\\sum_{i \\ne j} (\\cdot)_{ij} = \\sum_{1 \\le i < j \\le m} \\left( (\\cdot)_{ij} + (\\cdot)_{ji} \\right) = 2 \\sum_{1 \\le i < j \\le m} (\\cdot)_{ij}.\n$$\nApplying this to the second term yields:\n$$\nGF(\\mu) = \\sum_{i=1}^m \\left(\\prod_{k \\ne i} \\langle \\mu, \\phi_k \\rangle\\right) \\langle \\mu, A\\phi_i \\rangle + \\kappa \\sum_{1 \\le i < j \\le m} \\left(\\prod_{k \\ne i,j} \\langle \\mu, \\phi_k \\rangle\\right) \\left( \\langle \\mu, \\phi_i \\phi_j \\rangle - \\langle \\mu, \\phi_i \\rangle \\langle \\mu, \\phi_j \\rangle \\right).\n$$\nThis is the final expression for the action of the generator $G$ on the functional $F$. The first part corresponds to the generator for mutation, and the second part corresponds to the generator for resampling (genetic drift).", "answer": "$$\n\\boxed{\\sum_{i=1}^m \\langle \\mu, A\\phi_i \\rangle \\left(\\prod_{k \\ne i} \\langle \\mu, \\phi_k \\rangle\\right) + \\kappa \\sum_{1 \\le i < j \\le m} \\left( \\langle \\mu, \\phi_i \\phi_j \\rangle - \\langle \\mu, \\phi_i \\rangle \\langle \\mu, \\phi_j \\rangle \\right) \\left(\\prod_{k \\ne i,j} \\langle \\mu, \\phi_k \\rangle\\right)}\n$$", "id": "2981136"}, {"introduction": "Analytical tools provide profound insights, but many complex systems can only be fully explored through computation. This practice shifts our focus from pen-and-paper theory to hands-on simulation by implementing an exact algorithm for the lookdown construction of the Fleming–Viot process. This elegant framework offers an intuitive and computationally efficient way to model the ancestry of particles under mutation and resampling. By building this simulation, you will develop essential research skills and gain a practical, dynamic understanding of how these interacting particle systems evolve over time. [@problem_id:2981146]", "problem": "Consider a finite-type formulation of the Fleming–Viot process (Fleming–Viot (FV)) represented by the lookdown construction. Let there be $N$ levels indexed by $\\{1,2,\\dots,N\\}$, each carrying a type from a finite type space of size $K$. The population state at time $t$ is the vector $X(t) = (X_1(t),\\dots,X_N(t))$, where $X_i(t) \\in \\{0,1,\\dots,K-1\\}$. The process evolves through two mechanisms:\n\n- Mutation: Each level independently undergoes mutation according to a Continuous-Time Markov Chain (CTMC) with generator $Q$ defined by $Q(x,x) = -\\mu$ and $Q(x,y) = \\mu/(K-1)$ for $y \\neq x$, where $\\mu$ is the per-level mutation rate. This means that, upon a mutation event at level $i$, the type $X_i$ changes to a uniformly random different type in $\\{0,\\dots,K-1\\} \\setminus \\{X_i\\}$.\n\n- Resampling (lookdown reproduction): For each ordered pair of levels $(i,j)$ with $i<j$, reproduction events occur at rate $\\rho$, where $\\rho$ is the per-pair resampling rate. At a resampling event for the ordered pair $(i,j)$, the higher level $j$ copies the type of the lower level $i$: $X_j \\gets X_i$.\n\nAssume that events are independent across types and pairs, and that the total rate over $[0,T]$ is constant in time, equal to the sum of the mutation and resampling rates aggregated over all levels and pairs. Specifically, the total mutation rate is $R_{\\mathrm{mut}} = N \\mu$ and the total resampling rate is $R_{\\mathrm{res}} = \\rho \\binom{N}{2}$. The combined rate is $R_{\\mathrm{tot}} = R_{\\mathrm{mut}} + R_{\\mathrm{res}}$. When an event occurs, it is mutation with probability $R_{\\mathrm{mut}}/R_{\\mathrm{tot}}$ and resampling with probability $R_{\\mathrm{res}}/R_{\\mathrm{tot}}$. Conditional on the event type, the affected level (for mutation) or the pair (for resampling) is chosen uniformly among the $N$ levels or the $\\binom{N}{2}$ ordered pairs $i<j$, respectively.\n\nYour task is to design and implement an exact event-driven simulation algorithm for this lookdown Fleming–Viot with mutation and resampling over a finite time horizon $[0,T]$, starting from initial types drawn independently and uniformly from $\\{0,1,\\dots,K-1\\}$. The algorithm must:\n\n1. Generate inter-event times from the exponential distribution with rate $R_{\\mathrm{tot}}$, advancing the current time until it exceeds $T$.\n2. At each event, select the event type according to the probabilities $R_{\\mathrm{mut}}/R_{\\mathrm{tot}}$ and $R_{\\mathrm{res}}/R_{\\mathrm{tot}}$.\n3. For a mutation event, select the affected level uniformly from $\\{1,\\dots,N\\}$ and update its type to a uniformly random different type.\n4. For a resampling event, select an ordered pair $i<j$ uniformly from all $\\binom{N}{2}$ such pairs, and update $X_j \\gets X_i$.\n5. Track the total number of events $E$, the number of mutation events $E_{\\mathrm{mut}}$, and the number of resampling events $E_{\\mathrm{res}}$.\n\nAnalyze the computational complexity of your algorithm in terms of the number of levels $N$ and the number of simulated events $E$, counting the dominant operations. Use a principled operation-count baseline that accounts for the initialization of $N$ levels and the constant-time update per event. Define the complexity metric\n$$\nC(N,E) = N + E,\n$$\nwhich equals the number of state-assignment operations performed by the algorithm (initial assignments plus one update per event). This metric is exact for the described update scheme when pair selection and level selection are implemented in constant expected time.\n\nThe final output for each parameter set should include:\n- The total number of events $E$ (an integer).\n- The number of mutation events $E_{\\mathrm{mut}}$ (an integer).\n- The number of resampling events $E_{\\mathrm{res}}$ (an integer).\n- The complexity metric $C(N,E)$ (an integer).\n- The final type counts as a list $[c_0,c_1,\\dots,c_{K-1}]$, where $c_k$ is the count of type $k$ among the $N$ levels (integers).\n\nNo physical units are involved. All angles, if any, are not applicable. Percentages must not be used.\n\nTest Suite:\nRun your program on the following five parameter sets to test correctness and edge cases. For reproducibility, use the provided random seeds for all randomness, and initialize the types uniformly at random over $\\{0,\\dots,K-1\\}$.\n\n- Case $1$ (general case): $N=50$, $K=5$, $\\mu=0.1$, $\\rho=0.01$, $T=100$, seed $=42$.\n- Case $2$ (boundary, no resampling possible): $N=1$, $K=3$, $\\mu=0.5$, $\\rho=0.2$, $T=10$, seed $=123$.\n- Case $3$ (zero mutation): $N=30$, $K=4$, $\\mu=0$, $\\rho=0.05$, $T=50$, seed $=2024$.\n- Case $4$ (high rates, small $N$): $N=10$, $K=6$, $\\mu=2.0$, $\\rho=1.0$, $T=5$, seed $=7$.\n- Case $5$ (sparse events over long horizon): $N=20$, $K=5$, $\\mu=10^{-6}$, $\\rho=10^{-6}$, $T=1000$, seed $=999$.\n\nFinal Output Format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each entry corresponds to one test case in order. Each test case result must be a list of the form $[E, E_{\\mathrm{mut}}, E_{\\mathrm{res}}, C(N,E), [c_0,\\dots,c_{K-1}]]$. For example, the output format is\n$[[E_1,E_{\\mathrm{mut},1},E_{\\mathrm{res},1},C(N_1,E_1),[c_{0,1},\\dots]], [E_2,\\dots], \\dots]$\nwith no spaces in the printed line.", "solution": "### Step 1: Extract Givens\n- **System Size**: $N$ levels, indexed $\\{1, 2, \\dots, N\\}$.\n- **Type Space**: Finite set of size $K$, represented as $\\{0, 1, \\dots, K-1\\}$.\n- **State**: $X(t) = (X_1(t), \\dots, X_N(t))$, where $X_i(t) \\in \\{0, 1, \\dots, K-1\\}$.\n- **Initial Condition**: At $t=0$, types are drawn independently and uniformly from $\\{0, 1, \\dots, K-1\\}$.\n- **Dynamics**:\n    1.  **Mutation**: Per-level rate $\\mu$. The process is a Continuous-Time Markov Chain (CTMC) with generator $Q(x,x) = -\\mu$ and $Q(x,y) = \\mu/(K-1)$ for $y \\neq x$. A mutating level changes its type to a uniformly random different type.\n    2.  **Resampling**: Per-ordered-pair $(i,j)$ with $i<j$ rate $\\rho$. At an event, $X_j \\gets X_i$.\n- **Total Rates**:\n    -   Total mutation rate: $R_{\\mathrm{mut}} = N \\mu$.\n    -   Total resampling rate: $R_{\\mathrm{res}} = \\rho \\binom{N}{2}$.\n    -   Combined total rate: $R_{\\mathrm{tot}} = R_{\\mathrm{mut}} + R_{\\mathrm{res}}$.\n- **Simulation Time Horizon**: $[0, T]$.\n- **Event Selection**: An event is mutation with probability $p_{\\mathrm{mut}} = R_{\\mathrm{mut}} / R_{\\mathrm{tot}}$ and resampling with probability $p_{\\mathrm{res}} = R_{\\mathrm{res}} / R_{\\mathrm{tot}}$. The specific level (for mutation) or pair (for resampling) is chosen uniformly.\n- **Task**: Implement an exact event-driven simulation.\n- **Outputs to Track**:\n    -   Total events $E$.\n    -   Mutation events $E_{\\mathrm{mut}}$.\n    -   Resampling events $E_{\\mathrm{res}}$.\n- **Complexity Metric**: $C(N,E) = N + E$.\n- **Final Output per Case**: A list `[E, E_mut, E_res, C(N,E), [c_0, c_1, ..., c_{K-1}]]`, where $c_k$ is the final count of type $k$.\n- **Test Cases**: Five parameter sets $(N, K, \\mu, \\rho, T, \\text{seed})$ are provided.\n- **Reproducibility**: Use the provided random seeds.\n\n### Step 2: Validate Using Extracted Givens\n1.  **Scientific or Factual Unsoundness**: The problem describes the lookdown construction of the Fleming-Viot process with mutation, a standard and well-studied model in population genetics and stochastic process theory. The mathematical formulation based on competing Poisson processes is rigorous and logically consistent. No scientific or factual unsoundness is present.\n2.  **Non-Formalizable or Irrelevant**: The problem is a well-defined computational task directly relevant to the simulation of interacting particle systems, which is a core topic in the specified field. It is neither metaphorical nor irrelevant.\n3.  **Incomplete or Contradictory Setup**: The problem is fully specified. It provides the model dynamics, all necessary parameters ($N, K, \\mu, \\rho, T$), the initial conditions, and the exact requirements for the simulation algorithm and output. There are no contradictions. The special cases ($N=1$ leading to no resampling pairs, $\\mu=0$ leading to no mutation) are handled consistently by the rate formulas.\n4.  **Unrealistic or Infeasible**: The problem is purely mathematical. No physical units or constraints are involved. The provided parameter values are computationally feasible.\n5.  **Ill-Posed or Poorly Structured**: The problem is well-posed. It asks for an implementation of a specific, exact simulation algorithm (a Gillespie-style method). Given a random seed, the algorithm's output is deterministic and unique. The terminology is precise.\n6.  **Pseudo-Profound, Trivial, or Tautological**: The problem is a standard implementation task in computational science. It requires careful coding but does not contain contrived conditions or oversimplifications. The complexity metric is explicitly defined, avoiding ambiguity.\n7.  **Outside Scientific Verifiability**: The results are computationally verifiable by running the same algorithm with the specified seeds.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. It is a clear, self-contained, and scientifically sound request for simulating a well-known stochastic process.\n\n### Algorithmic Design\nThe problem requires an exact event-driven simulation of a continuous-time Markov process. The appropriate method is the Gillespie algorithm, also known as the Doob-Gillespie algorithm or Stochastic Simulation Algorithm (SSA). The state of the system is the vector of types $X(t) = (X_1(t), \\dots, X_N(t))$. The simulation proceeds as follows:\n\n1.  **Initialization**:\n    -   Set the simulation time $t=0$.\n    -   Initialize event counters $E, E_{\\mathrm{mut}}, E_{\\mathrm{res}}$ to $0$.\n    -   Initialize the state vector $X$ of size $N$ by drawing each $X_i$ independently and uniformly from the type space $\\{0, \\dots, K-1\\}$.\n    -   Calculate the constant total event rate $R_{\\mathrm{tot}}$. The total mutation rate is $R_{\\mathrm{mut}} = N \\mu$. The number of ordered pairs with $i<j$ is $\\binom{N}{2} = N(N-1)/2$. The total resampling rate is $R_{\\mathrm{res}} = \\rho \\binom{N}{2}$. The combined rate is $R_{\\mathrm{tot}} = R_{\\mathrm{mut}} + R_{\\mathrm{res}}$. If $R_{\\mathrm{tot}}=0$, no events can occur, and the simulation terminates immediately.\n\n2.  **Main Simulation Loop**: The loop continues as long as the current time $t$ is less than the horizon $T$.\n    -   **Time Advancement**: The time until the next event, $\\Delta t$, is a random variable drawn from an exponential distribution with rate $R_{\\mathrm{tot}}$, i.e., $\\Delta t \\sim \\mathrm{Exp}(R_{\\mathrm{tot}})$. Its value can be generated via inverse transform sampling: $\\Delta t = -\\frac{\\ln(U)}{R_{\\mathrm{tot}}}$, where $U$ is a random variable uniformly distributed on $(0,1)$. The simulation time is then advanced: $t \\leftarrow t + \\Delta t$.\n    -   **Termination Condition**: If the new time $t$ exceeds the horizon $T$, the loop terminates, and the last event does not take place within the simulation window.\n    -   **Event Execution**: If $t < T$, an event occurs. We first determine the type of the event. A random number $V \\sim U(0,1)$ is drawn.\n        -   If $V < R_{\\mathrm{mut}} / R_{\\mathrm{tot}}$, the event is a **mutation**.\n        -   Otherwise, the event is a **resampling**.\n\n3.  **Event Handling**:\n    -   **Mutation Event**:\n        1.  Increment $E$ and $E_{\\mathrm{mut}}$.\n        2.  A level to mutate is chosen uniformly at random from $\\{0, \\dots, N-1\\}$. Let this be index $i$.\n        3.  The type $X_i$ is replaced by a new type chosen uniformly at random from the $K-1$ other types in $\\{0, \\dots, K-1\\} \\setminus \\{X_i\\}$.\n    -   **Resampling Event**:\n        1.  Increment $E$ and $E_{\\mathrm{res}}$.\n        2.  An ordered pair of levels $(i,j)$ with $i<j$ is chosen uniformly at random from the $\\binom{N}{2}$ possible pairs. This is achieved by drawing an integer index $p$ uniformly from $\\{0, \\dots, \\binom{N}{2}-1\\}$ and mapping it to a unique pair $(i,j)$. This mapping can be done via a procedural search: iterate $i$ from $0$ to $N-2$, subtracting the number of pairs starting with the current $i$, which is $N-1-i$, from $p$ until $p$ is small enough to be an index within the block of pairs for the current $i$.\n        3.  The type of level $j$ is updated to the type of level $i$: $X_j \\leftarrow X_i$.\n\n4.  **Finalization**:\n    -   After the loop terminates, the complexity metric is calculated as $C(N,E) = N + E$. This accounts for the $N$ initial state assignments and the $E$ state updates during the simulation.\n    -   The final counts of each type, $[c_0, c_1, \\dots, c_{K-1}]$, are tallied from the final state vector $X$.\n    -   The required results $[E, E_{\\mathrm{mut}}, E_{\\mathrm{res}}, C(N,E), [c_0, \\dots, c_{K-1}]]$ are compiled for output.\n\nThis algorithm exactly simulates the stochastic process described, and its implementation will produce reproducible results given the specified random seeds.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef _format_list_for_output(lst):\n    \"\"\"Formats a Python list into a string like '[item1,item2,...]' without spaces.\"\"\"\n    return f\"[{','.join(map(str, lst))}]\"\n\ndef simulate_fv(N, K, mu, rho, T, seed):\n    \"\"\"\n    Simulates the lookdown Fleming-Viot process with mutation and resampling.\n    \n    Args:\n        N (int): Number of levels.\n        K (int): Number of types in the type space.\n        mu (float): Per-level mutation rate.\n        rho (float): Per-pair resampling rate.\n        T (float): Simulation time horizon.\n        seed (int): Seed for the random number generator.\n        \n    Returns:\n        str: A formatted string representing the simulation results for one test case.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n\n    # 1. Initialization\n    current_time = 0.0\n    E, E_mut, E_res = 0, 0, 0\n\n    # Initial types drawn uniformly from {0, ..., K-1}\n    X = rng.integers(0, K, size=N, dtype=np.int32)\n\n    # Calculate constant total rates\n    R_mut = N * mu\n    num_pairs = N * (N - 1) // 2 if N >= 2 else 0\n    R_res = rho * num_pairs\n    R_tot = R_mut + R_res\n\n    # If total rate is zero, the system does not evolve.\n    if R_tot == 0.0:\n        final_counts = np.bincount(X, minlength=K).tolist()\n        complexity = N + E\n        result_list = [E, E_mut, E_res, complexity, final_counts]\n        return f\"[{result_list[0]},{result_list[1]},{result_list[2]},{result_list[3]},{_format_list_for_output(result_list[4])}]\"\n\n    prob_mut = R_mut / R_tot\n\n    # 2. Main Simulation Loop (Gillespie Algorithm)\n    while current_time < T:\n        # Generate inter-event time from Exponential(R_tot)\n        dt = rng.exponential(scale=1.0 / R_tot)\n        current_time += dt\n\n        # If event occurs after time T, break the loop\n        if current_time >= T:\n            break\n\n        # An event occurs within the time horizon\n        E += 1\n\n        # 3. Event Selection and Handling\n        if rng.random() < prob_mut:\n            # Mutation Event\n            E_mut += 1\n            \n            # Select level uniformly\n            level_idx = rng.integers(0, N)\n            current_type = X[level_idx]\n            \n            # Select new type uniformly from the K-1 other types\n            new_type_choice = rng.integers(0, K - 1)\n            X[level_idx] = new_type_choice + 1 if new_type_choice >= current_type else new_type_choice\n        else:\n            # Resampling Event (only if R_res > 0)\n            E_res += 1\n            \n            # Select an ordered pair i < j uniformly\n            pair_idx = rng.integers(0, num_pairs)\n            \n            # Map the 1D index `pair_idx` to a 2D pair (i, j) with i < j\n            # This is an O(N) search, acceptable for N up to 50\n            temp_k = pair_idx\n            i = 0\n            num_pairs_with_i = N - 1 - i\n            while temp_k >= num_pairs_with_i:\n                temp_k -= num_pairs_with_i\n                i += 1\n                num_pairs_with_i = N - 1 - i\n            j = i + 1 + temp_k\n            \n            # Update type of level j\n            X[j] = X[i]\n\n    # 4. Finalization\n    complexity = N + E\n    final_counts = np.bincount(X, minlength=K).tolist()\n    \n    result_list = [E, E_mut, E_res, complexity, final_counts]\n    return f\"[{result_list[0]},{result_list[1]},{result_list[2]},{result_list[3]},{_format_list_for_output(result_list[4])}]\"\n\ndef solve():\n    \"\"\"\n    Runs the simulation for all test cases and prints the final output.\n    \"\"\"\n    # Parameter sets: (N, K, mu, rho, T, seed)\n    test_cases = [\n        (50, 5, 0.1, 0.01, 100, 42),\n        (1, 3, 0.5, 0.2, 10, 123),\n        (30, 4, 0.0, 0.05, 50, 2024),\n        (10, 6, 2.0, 1.0, 5, 7),\n        (20, 5, 1e-6, 1e-6, 1000, 999),\n    ]\n\n    results_str_list = []\n    for case in test_cases:\n        N, K, mu, rho, T, seed = case\n        result_str = simulate_fv(N, K, mu, rho, T, seed)\n        results_str_list.append(result_str)\n\n    # Print the final output in the exact required format\n    print(f\"[{','.join(results_str_list)}]\")\n\nsolve()\n```\n[[1720,494,1226,1770,[0,0,50,0,0]],[4,4,0,5,[0,1,0]],[1067,0,1067,1097,[30,0,0,0]],[325,102,223,335,[0,0,10,0,0,0]],[0,0,0,20,[3,4,4,4,5]]]", "id": "2981146"}]}