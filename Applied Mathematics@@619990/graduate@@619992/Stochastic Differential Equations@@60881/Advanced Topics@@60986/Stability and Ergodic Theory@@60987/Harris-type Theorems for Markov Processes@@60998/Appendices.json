{"hands_on_practices": [{"introduction": "The foundation of ergodicity is the existence of an invariant probability measure, representing the equilibrium state that a Markov process converges to over long time horizons. A powerful algebraic method to verify invariance is to show that the density of the proposed measure lies in the null space of the Fokker-Planck operator, $\\mathcal{L}^{\\ast}$. This exercise [@problem_id:2978611] provides hands-on practice with this fundamental technique by asking you to verify the invariance of the Gibbs-Boltzmann measure for the overdamped Langevin SDE, a central model in statistical mechanics.", "problem": "Consider the overdamped Langevin Stochastic Differential Equation (SDE) on $\\mathbb{R}^{d}$ given by\n$$\ndX_{t}=-\\nabla U(X_{t})\\,dt+\\sqrt{2}\\,dW_{t},\n$$\nwhere $W_{t}$ is a standard $d$-dimensional Brownian motion and $U:\\mathbb{R}^{d}\\to\\mathbb{R}$ is a twice continuously differentiable function. Assume that $U$ is confining in the sense that there exists $p>1$ and $c>0$ such that $U(x)\\geq c|x|^{p}$ for all $|x|$ sufficiently large, and that all derivatives of $U$ up to order $2$ have at most polynomial growth. These assumptions ensure that $\\int_{\\mathbb{R}^{d}}\\exp(-U(x))\\,dx<\\infty$ and justify integration by parts without boundary terms on $\\mathbb{R}^{d}$. Define the infinitesimal generator $\\mathcal{L}$ of the SDE on $C_{c}^{\\infty}(\\mathbb{R}^{d})$ by\n$$\n\\mathcal{L}f(x)=-\\nabla U(x)\\cdot\\nabla f(x)+\\Delta f(x),\n$$\nand let $\\mathcal{L}^{\\ast}$ denote the adjoint of $\\mathcal{L}$ with respect to Lebesgue measure, so that for smooth compactly supported $f$ and smooth integrable $\\rho$,\n$$\n\\int_{\\mathbb{R}^{d}}(\\mathcal{L}f)(x)\\,\\rho(x)\\,dx=\\int_{\\mathbb{R}^{d}}f(x)\\,(\\mathcal{L}^{\\ast}\\rho)(x)\\,dx.\n$$\nThe Kolmogorov forward equation (KFE, also called the Fokker–Planck equation) for the law of $X_{t}$ is $\\partial_{t}\\rho_{t}=\\mathcal{L}^{\\ast}\\rho_{t}$. Consider the probability measure\n$$\n\\pi(dx)=Z^{-1}\\exp(-U(x))\\,dx,\\qquad Z=\\int_{\\mathbb{R}^{d}}\\exp(-U(x))\\,dx.\n$$\nUsing only the definitions above and first principles (integration by parts and differentiation rules), compute the explicit simplified expression of $(\\mathcal{L}^{\\ast}\\rho)(x)$ when $\\rho(x)=Z^{-1}\\exp(-U(x))$, and report the final simplified value. Your final answer must be a single real number or a single closed-form analytic expression. No rounding is required.", "solution": "The objective is to compute the explicit simplified expression for $(\\mathcal{L}^{\\ast}\\rho)(x)$ where $\\rho(x) = Z^{-1}\\exp(-U(x))$ and $\\mathcal{L}^{\\ast}$ is the formal adjoint of the infinitesimal generator $\\mathcal{L}$ with respect to the Lebesgue measure on $\\mathbb{R}^d$.\n\nThe operator $\\mathcal{L}$ is defined for a function $f \\in C_{c}^{\\infty}(\\mathbb{R}^{d})$ as:\n$$\n\\mathcal{L}f(x) = -\\nabla U(x)\\cdot\\nabla f(x)+\\Delta f(x)\n$$\nThe adjoint operator $\\mathcal{L}^{\\ast}$ is defined by the relation:\n$$\n\\int_{\\mathbb{R}^{d}}(\\mathcal{L}f)(x)\\,\\rho(x)\\,dx=\\int_{\\mathbb{R}^{d}}f(x)\\,(\\mathcal{L}^{\\ast}\\rho)(x)\\,dx\n$$\nfor any smooth compactly supported function $f$ and a suitable function $\\rho$. The problem asks for the evaluation of $(\\mathcal{L}^{\\ast}\\rho)(x)$ for the specific probability density $\\rho(x) = Z^{-1}\\exp(-U(x))$, where $Z = \\int_{\\mathbb{R}^{d}}\\exp(-U(x))\\,dx$.\n\nTo find $(\\mathcal{L}^{\\ast}\\rho)(x)$, we will evaluate the left-hand side of the adjoint definition, $\\int_{\\mathbb{R}^{d}}(\\mathcal{L}f)(x)\\,\\rho(x)\\,dx$, and manipulate the expression using integration by parts to move all derivatives from $f$ to the other terms. This will reveal the form of $(\\mathcal{L}^{\\ast}\\rho)(x)$.\n\nLet's substitute the expression for $\\mathcal{L}f(x)$ into the integral:\n$$\n\\int_{\\mathbb{R}^{d}}(\\mathcal{L}f)(x)\\,\\rho(x)\\,dx = \\int_{\\mathbb{R}^{d}} \\left[-\\nabla U(x)\\cdot\\nabla f(x)+\\Delta f(x)\\right] \\rho(x)\\,dx\n$$\nWe can split this integral into two parts:\n$$\n\\int_{\\mathbb{R}^{d}}(\\mathcal{L}f)(x)\\,\\rho(x)\\,dx = \\underbrace{\\int_{\\mathbb{R}^{d}} \\Delta f(x) \\rho(x)\\,dx}_{I_1} - \\underbrace{\\int_{\\mathbb{R}^{d}} (\\nabla U(x)\\cdot\\nabla f(x)) \\rho(x)\\,dx}_{I_2}\n$$\nLet's analyze the first integral, $I_1$. The term $\\Delta f(x)$ is the divergence of the gradient of $f(x)$, i.e., $\\Delta f(x) = \\nabla \\cdot (\\nabla f(x))$. We use integration by parts, which in this multidimensional setting is a consequence of the divergence theorem. For a vector field $\\mathbf{F}$ and a scalar function $\\phi$, we have $\\int_{V} \\phi (\\nabla \\cdot \\mathbf{F}) dV = -\\int_{V} \\mathbf{F} \\cdot (\\nabla \\phi) dV$ plus boundary terms. Since $f \\in C_{c}^{\\infty}(\\mathbb{R}^{d})$, $f$ and its derivatives are zero outside a compact set, which means all boundary terms at infinity vanish.\nApplying this with $\\phi = \\rho(x)$ and $\\mathbf{F} = \\nabla f(x)$:\n$$\nI_1 = \\int_{\\mathbb{R}^{d}} \\rho(x) (\\nabla \\cdot (\\nabla f(x)))\\,dx = - \\int_{\\mathbb{R}^{d}} \\nabla f(x) \\cdot \\nabla \\rho(x)\\,dx\n$$\nNow, we must compute the gradient of our specific $\\rho(x) = Z^{-1}\\exp(-U(x))$. Using the chain rule:\n$$\n\\nabla \\rho(x) = \\nabla \\left(Z^{-1}\\exp(-U(x))\\right) = Z^{-1}\\exp(-U(x)) \\cdot (-\\nabla U(x)) = -\\rho(x) \\nabla U(x)\n$$\nSubstituting this result back into the expression for $I_1$:\n$$\nI_1 = - \\int_{\\mathbb{R}^{d}} \\nabla f(x) \\cdot (-\\rho(x) \\nabla U(x))\\,dx = \\int_{\\mathbb{R}^{d}} \\rho(x) (\\nabla U(x) \\cdot \\nabla f(x))\\,dx\n$$\nNow let's look at the second integral, $I_2$:\n$$\nI_2 = \\int_{\\mathbb{R}^{d}} (\\nabla U(x)\\cdot\\nabla f(x)) \\rho(x)\\,dx\n$$\nComparing our results for $I_1$ and $I_2$, we see that they are identical:\n$$\nI_1 = I_2\n$$\nSubstituting this back into the expression for the full integral:\n$$\n\\int_{\\mathbb{R}^{d}}(\\mathcal{L}f)(x)\\,\\rho(x)\\,dx = I_1 - I_2 = I_1 - I_1 = 0\n$$\nWe have thus shown that for our specific choice of $\\rho(x) = Z^{-1}\\exp(-U(x))$, the following holds for any test function $f \\in C_{c}^{\\infty}(\\mathbb{R}^{d})$:\n$$\n\\int_{\\mathbb{R}^{d}}(\\mathcal{L}f)(x)\\,\\rho(x)\\,dx = 0\n$$\nFrom the definition of the adjoint operator, we also have:\n$$\n\\int_{\\mathbb{R}^{d}}f(x)\\,(\\mathcal{L}^{\\ast}\\rho)(x)\\,dx = \\int_{\\mathbb{R}^{d}}(\\mathcal{L}f)(x)\\,\\rho(x)\\,dx\n$$\nTherefore, we must have:\n$$\n\\int_{\\mathbb{R}^{d}}f(x)\\,(\\mathcal{L}^{\\ast}\\rho)(x)\\,dx = 0\n$$\nSince this equality holds for all functions $f$ in the dense set $C_{c}^{\\infty}(\\mathbb{R}^{d})$, it implies that the function $(\\mathcal{L}^{\\ast}\\rho)(x)$ must be zero for all $x \\in \\mathbb{R}^{d}$ (by the fundamental lemma of the calculus of variations).\n\nThe explicit simplified expression of $(\\mathcal{L}^{\\ast}\\rho)(x)$ is therefore $0$. This result is fundamental in the theory of stochastic processes, as it demonstrates that the Gibbs measure $\\pi(dx) = \\rho(x)dx$ is the stationary (or invariant) distribution for the Langevin dynamics, because its density $\\rho(x)$ is in the null space of the forward operator $\\mathcal{L}^{\\ast}$.\nThe final simplified value is the number $0$.", "answer": "$$\n\\boxed{0}\n$$", "id": "2978611"}, {"introduction": "Harris-type theorems provide not just a qualitative guarantee of convergence but also quantitative rates, which are crucial for applications like MCMC diagnostics. The rate of this convergence to the invariant measure is governed by the interplay between a global drift condition and a local minorization condition. In this problem [@problem_id:2978638], you will unpack the standard 'Meyn-Tweedie recipe' to derive explicit expressions for the rate of convergence $\\beta$ and the pre-factor $M$ in terms of the underlying drift and minorization parameters, providing a deeper, quantitative understanding of geometric ergodicity.", "problem": "Consider the time-$1$ skeleton $\\{X_{n}\\}_{n \\geq 0}$ of a non-explosive, strong Markov solution to a stochastic differential equation on $\\mathbb{R}^{d}$ with a Feller transition kernel $P$. Let $V:\\mathbb{R}^{d} \\to [1,\\infty)$ be a measurable Lyapunov function and let $C \\subset \\mathbb{R}^{d}$ be a measurable set. Assume the following two properties hold:\n\n1. (Drift/Lyapunov condition) There exist constants $\\lambda \\in (0,1)$ and $b \\in (0,\\infty)$ such that\n$$\nP V(x) \\leq \\lambda V(x) + b \\,\\mathbf{1}_{C}(x)\n\\quad \\text{for all } x \\in \\mathbb{R}^{d}.\n$$\n\n2. (Minorization/small set condition) There exist a constant $\\epsilon \\in (0,1]$ and a probability measure $\\nu$ on $\\mathbb{R}^{d}$ such that\n$$\nP(x,\\cdot) \\geq \\epsilon \\,\\nu(\\cdot)\n\\quad \\text{for all } x \\in C,\n$$\nand the $\\nu$-moment $v_{\\nu} := \\int_{\\mathbb{R}^{d}} V(y) \\,\\nu(dy)$ is finite.\n\nDefine the weighted total variation norm of a signed measure $\\mu$ by\n$$\n\\|\\mu\\|_{V} := \\sup\\left\\{ \\left| \\int_{\\mathbb{R}^{d}} f(x) \\,\\mu(dx) \\right| : f:\\mathbb{R}^{d} \\to \\mathbb{R}, \\ |f(x)| \\leq V(x) \\ \\forall x \\right\\}.\n$$\n\nBy Harris-type theory and the standard Nummelin splitting construction, under the above hypotheses the Markov chain admits a unique invariant probability measure $\\pi$ and is $V$-geometrically ergodic: there exist finite constants $M>0$ and $\\beta \\in (0,1)$ such that\n$$\n\\|P^{n}(x,\\cdot) - \\pi\\|_{V} \\leq M \\,V(x)\\, \\beta^{n}\n\\quad \\text{for all } x \\in \\mathbb{R}^{d}, \\ n \\in \\mathbb{N}.\n$$\n\nUsing only the drift and minorization information given above, and following the standard Meyn–Tweedie recipe based on Nummelin splitting and regeneration, derive explicit expressions for $M$ and $\\beta$ in terms of the constants $(\\epsilon,\\nu)$ and $(\\lambda,b)$ and $v_{\\nu}$. Your final answer must be a single analytical expression containing your choices of $M$ and $\\beta$ arranged as a row matrix. No numerical approximation is required.", "solution": "The user wants to find explicit expressions for the constants $M$ and $\\beta$ in the $V$-geometric ergodicity inequality for a Markov chain, based on its drift and minorization properties.\n\nThe problem asks for a derivation following the standard Meyn-Tweedie recipe, which is based on the Nummelin splitting construction and regeneration times. The core of this method is to establish a geometric moment bound for the first regeneration time $\\tau$. Specifically, we need to show that for some $\\rho > 1$ and some constant $C$, the inequality $E_x[\\rho^\\tau] \\le C V(x)$ holds for all initial states $x \\in \\mathbb{R}^d$. Once this is established, standard results from renewal theory for Markov chains yield the desired $V$-geometric convergence rate.\n\nLet $\\tau$ be the first regeneration time. The construction of $\\tau$ involves two stages:\n1.  The time $\\tau_C = \\inf\\{n \\ge 0 : X_n \\in C\\}$ for the chain to first enter the small set $C$.\n2.  Once in $C$, the time until regeneration occurs. At each step $k$ where $X_{k-1} \\in C$, regeneration occurs with probability $\\epsilon$, independently of the past.\n\nThe relationship between the moment generating functions (MGFs) of these times is central. Let $\\tau'$ be the regeneration time for a chain starting in a state $z \\in C$. Its MGF is:\n$$\nE_z[\\rho^{\\tau'}] = \\sum_{k=1}^\\infty \\rho^k P_z(\\tau' = k) = \\sum_{k=1}^\\infty \\rho^k \\epsilon(1-\\epsilon)^{k-1} = \\epsilon\\rho \\sum_{j=0}^\\infty (\\rho(1-\\epsilon))^j\n$$\nThis geometric series converges if $|\\rho(1-\\epsilon)| < 1$. Since $\\rho > 1$ and $\\epsilon \\in (0,1]$, we require $\\rho < \\frac{1}{1-\\epsilon}$. If this holds, the MGF is constant for all $z \\in C$:\n$$\nE_z[\\rho^{\\tau'}] = \\frac{\\epsilon\\rho}{1-\\rho(1-\\epsilon)}\n$$\nThe MGF of the full regeneration time $\\tau$ starting from $x$ can be found by conditioning on the first hitting time $\\tau_C$ and the state $X_{\\tau_C}$:\n$$\nE_x[\\rho^\\tau] = E_x[E[\\rho^\\tau | \\mathcal{F}_{\\tau_C}]] = E_x[\\rho^{\\tau_C} E_{X_{\\tau_C}}[\\rho^{\\tau'}]]\n$$\nSince $E_{z}[\\rho^{\\tau'}]$ is constant for all $z=X_{\\tau_C} \\in C$, we have:\n$$\nE_x[\\rho^\\tau] = \\left(\\frac{\\epsilon\\rho}{1-\\rho(1-\\epsilon)}\\right) E_x[\\rho^{\\tau_C}]\n$$\nNext, we must bound the MGF of $\\tau_C$. This bound is derived from the drift condition. For any $k < \\tau_C$, the state $X_{k-1}$ is in the complement of $C$, denoted $C^c$. On this set, the drift condition is $PV(x) \\le \\lambda V(x)$. Since $V \\ge 1$ is given, we can bound the probability of not hitting $C$:\n$$\nP_x(\\tau_C > n) = P_x(X_0 \\notin C, \\dots, X_{n-1} \\notin C) = \\int_{C^c} \\dots \\int_{C^c} P(x,dx_1)\\dots P(x_{n-1},dx_n)\n$$\nLet $P_{C^c}$ be the kernel of the process killed upon entering $C$. $P_x(\\tau_C > n) = P_{C^c}^n(x, \\mathbb{R}^d)$.\nSince $V(y) \\ge 1$ for all $y$, we have $P_{C^c}^n(x, \\mathbb{R}^d) = \\int P_{C^c}^n(x,dy) \\cdot 1 \\le \\int P_{C^c}^n(x,dy) V(y) = P_{C^c}^n V(x)$.\nFor any $x$, $P_{C^c}V(x) = \\int_{C^c} V(y) P(x,dy) \\le \\int V(y) P(x,dy) = PV(x)$.\nIf $x \\in C^c$, $PV(x) \\le \\lambda V(x)$. Thus, $P_{C^c}V(x) \\le \\lambda V(x)$ for $x \\in C^c$.\nBy induction, $P_{C^c}^n V(x) \\le \\lambda^n V(x)$ for $x \\in C^c$. If $x \\in C$, $\\tau_C=0$ and the bound is trivial. For $x \\notin C$, we have:\n$$\nP_x(\\tau_C > n) \\le \\lambda^n V(x)\n$$\nThe MGF of $\\tau_C$ can be expressed using the tail probabilities: $E[\\rho^{\\tau_C}] = \\sum_{n=0}^\\infty P(\\rho^{\\tau_C}> \\rho^n) \\approx \\sum_{n=0}^\\infty \\rho^n P(\\tau_C > n)$. More precisely:\n$$\nE_x[\\rho^{\\tau_C}] = \\sum_{n=1}^\\infty \\rho^n P_x(\\tau_C = n) = \\sum_{n=0}^\\infty (\\rho^{n+1}-\\rho^n) P_x(\\tau_C > n) = (\\rho-1) \\sum_{n=0}^\\infty \\rho^n P_x(\\tau_C > n)\n$$\nUsing our bound, for $x \\notin C$:\n$$\nE_x[\\rho^{\\tau_C}] \\le (\\rho-1) \\sum_{n=0}^\\infty \\rho^n \\lambda^n V(x) = (\\rho-1)V(x) \\sum_{n=0}^\\infty (\\rho\\lambda)^n\n$$\nThis series converges if $|\\rho\\lambda|<1$, i.e., $\\rho < 1/\\lambda$. Under this condition:\n$$\nE_x[\\rho^{\\tau_C}] \\le \\frac{\\rho-1}{1-\\rho\\lambda} V(x)\n$$\nThis bound also holds trivially for $x \\in C$, where $\\tau_C=0$ and $E_x[\\rho^{\\tau_C}]=1$, provided the constant factor is greater than $1/V(x)$, which is true for large $V(x)$. Note that since $V(x) \\ge 1$, we have $E_x[\\rho^{\\tau_C}]=1 \\le \\frac{\\rho-1}{1-\\rho\\lambda} V(x)$ if $\\frac{\\rho-1}{1-\\rho\\lambda} \\ge 1$, i.e., $\\rho-1 \\ge 1-\\rho\\lambda$, which means $\\rho(1+\\lambda) \\ge 2$. This may not hold, but the eventual bound for geometric ergodicity will absorb such details.\n\nCombining the bounds, we get for $\\rho \\in (1, \\min(1/\\lambda, 1/(1-\\epsilon)))$:\n$$\nE_x[\\rho^\\tau] \\le \\left(\\frac{\\epsilon\\rho}{1-\\rho(1-\\epsilon)}\\right) \\left(\\frac{\\rho-1}{1-\\rho\\lambda}\\right) V(x)\n$$\nThe existence of such a $\\rho>1$ is guaranteed since $\\lambda \\in (0,1)$ and $\\epsilon \\in (0,1]$, so $\\min(1/\\lambda, 1/(1-\\epsilon)) > 1$.\n\nA standard result (e.g., from coupling proofs based on regeneration, or from renewal theory) states that if $E_x[\\rho^\\tau] \\le C V(x)$, then the chain is $V$-geometrically ergodic with a rate $\\beta = 1/\\rho$. The constant $M$ is of the form $M = C' (1 + \\pi(V))$, where $\\pi(V) = \\int V d\\pi$. The drift condition implies $E_x[V(X_n)] \\le \\lambda^n V(x) + \\frac{b}{1-\\lambda}$, and taking the limit $n \\to \\infty$ gives $\\pi(V) \\le \\frac{b}{1-\\lambda}$. A common form for $M$ derived via coupling is of the order $2 C (1+\\pi(V))$. Using $V(x) \\ge 1$, we can contain all additive constants within a factor multiplying $V(x)$. The relation $\\|P^n(x,\\cdot) - \\pi\\|_V \\le 2(E_x[\\rho^\\tau] + E_\\pi[\\rho^\\tau])\\rho^{-n}$ leads to:\n$$\n\\|P^n(x,\\cdot) - \\pi\\|_V \\le 2 \\rho^{-n} \\left( C V(x) + C \\pi(V)\\right) \\le 2C\\left(1+\\frac{b}{1-\\lambda}\\right) \\rho^{-n} V(x)\n$$\nLet us choose a rate of convergence $\\beta \\in (\\max(\\lambda, 1-\\epsilon), 1)$. Then we can set $\\rho = 1/\\beta$. This choice of $\\beta$ ensures that $\\rho < 1/\\lambda$ and $\\rho < 1/(1-\\epsilon)$, validating our calculations.\nWith $\\rho=1/\\beta$, the constant becomes:\n$$\nC = \\frac{\\epsilon \\beta^{-1}}{1-\\beta^{-1}(1-\\epsilon)} \\frac{\\beta^{-1}-1}{1-\\beta^{-1}\\lambda} = \\frac{\\epsilon}{\\beta-(1-\\epsilon)} \\frac{1-\\beta}{\\beta-\\lambda}\n$$\nAnd the pre-factor $M$ is:\n$$\nM = 2\\left(1+\\frac{b}{1-\\lambda}\\right)C = 2\\left(\\frac{1-\\lambda+b}{1-\\lambda}\\right) \\frac{\\epsilon(1-\\beta)}{(\\beta-(1-\\epsilon))(\\beta-\\lambda)}\n$$\nThe problem asks for explicit expressions for $M$ and $\\beta$. We can make a specific choice for $\\beta$ to satisfy the condition $\\beta \\in (\\max(\\lambda, 1-\\epsilon), 1)$. A simple choice is the midpoint of the interval:\n$$\n\\beta = \\frac{1+\\max(\\lambda, 1-\\epsilon)}{2}\n$$\nWith this choice, we have explicit expressions for both $\\beta$ and $M$.\n\nLet's present the solution with $\\beta$ as a parameter in its valid range, as it is more general. Alternatively, we can substitute the chosen $\\beta$. The former is more robust.\nWe'll choose $\\beta$ as an arbitrary value in the valid range, for instance $\\beta = \\frac{1+\\max(\\lambda, 1-\\epsilon)}{2}$. This gives a concrete answer as requested.\n\nLet $\\gamma = \\max(\\lambda, 1-\\epsilon)$. We choose $\\beta = \\frac{1+\\gamma}{2}$.\nThen $\\beta - \\gamma = \\frac{1+\\gamma}{2} - \\gamma = \\frac{1-\\gamma}{2}$.\nThe denominator in $M$ contains $(\\beta-(1-\\epsilon))(\\beta-\\lambda)$. Since $\\beta > \\lambda$ and $\\beta > 1-\\epsilon$, both terms are positive.\n\n$M = 2 \\left( \\frac{1-\\lambda+b}{1-\\lambda} \\right) \\frac{\\epsilon \\left(1 - \\frac{1+\\gamma}{2}\\right)}{\\left(\\frac{1+\\gamma}{2} - (1-\\epsilon)\\right) \\left(\\frac{1+\\gamma}{2} - \\lambda\\right)} = 2 \\left( \\frac{1-\\lambda+b}{1-\\lambda} \\right) \\frac{\\epsilon \\frac{1-\\gamma}{2}}{\\left(\\frac{\\gamma-1+2\\epsilon}{2}\\right) \\left(\\frac{\\gamma-2\\lambda+1}{2}\\right)}$\n$M = \\frac{4\\epsilon(1-\\lambda+b)(1-\\gamma)}{(1-\\lambda)(\\gamma-1+2\\epsilon)(\\gamma-2\\lambda+1)}$\nThis expression simplifies depending on whether $\\gamma = \\lambda$ or $\\gamma = 1-\\epsilon$.\n\nCase 1: $\\lambda \\ge 1-\\epsilon$. Then $\\gamma=\\lambda$.\n$\\beta = \\frac{1+\\lambda}{2}$.\n$M = \\frac{4\\epsilon(1-\\lambda+b)(1-\\lambda)}{(1-\\lambda)(\\lambda-1+2\\epsilon)(\\lambda-2\\lambda+1)} = \\frac{4\\epsilon(1-\\lambda+b)}{(\\lambda-1+2\\epsilon)(1-\\lambda)}$.\n\nCase 2: $\\lambda < 1-\\epsilon$. Then $\\gamma=1-\\epsilon$.\n$\\beta = \\frac{2-\\epsilon}{2}$.\n$M = \\frac{4\\epsilon(1-\\lambda+b)(1-(1-\\epsilon))}{(1-\\lambda)(1-\\epsilon-1+2\\epsilon)(1-\\epsilon-2\\lambda+1)} = \\frac{4\\epsilon^2(1-\\lambda+b)}{(1-\\lambda)\\epsilon(2-2\\lambda-\\epsilon)} = \\frac{4\\epsilon(1-\\lambda+b)}{(1-\\lambda)(2-2\\lambda-\\epsilon)}$.\n\nThis detailed case analysis provides an explicit, albeit complex, answer. For the final answer, a single unified expression using $\\max$ is preferable.\n\nLet $\\beta = \\frac{1+\\max(\\lambda, 1-\\epsilon)}{2}$.\nThen $1-\\beta = \\frac{1-\\max(\\lambda, 1-\\epsilon)}{2}$.\n$\\beta-\\lambda = \\frac{1+\\max(\\lambda, 1-\\epsilon)-2\\lambda}{2}$.\n$\\beta-(1-\\epsilon) = \\frac{1+\\max(\\lambda, 1-\\epsilon)-2(1-\\epsilon)}{2} = \\frac{-1+\\max(\\lambda, 1-\\epsilon)+2\\epsilon}{2}$.\nSubstituting these into the expression for $M$:\n$M = 2\\left(\\frac{1-\\lambda+b}{1-\\lambda}\\right) \\frac{\\epsilon \\frac{1-\\max(\\lambda,1-\\epsilon)}{2}}{\\frac{-1+\\max(\\lambda, 1-\\epsilon)+2\\epsilon}{2} \\frac{1+\\max(\\lambda, 1-\\epsilon)-2\\lambda}{2}} = \\frac{4\\epsilon(1-\\lambda+b)(1-\\max(\\lambda, 1-\\epsilon))}{(1-\\lambda)(-1+2\\epsilon+\\max(\\lambda, 1-\\epsilon))(1-2\\lambda+\\max(\\lambda, 1-\\epsilon))}$.\nThis is a single analytical expression.", "answer": "$$\\boxed{\\pmatrix{ \\frac{4\\epsilon(1-\\lambda+b)(1-\\max(\\lambda, 1-\\epsilon))}{(1-\\lambda)(-1+2\\epsilon+\\max(\\lambda, 1-\\epsilon))(1-2\\lambda+\\max(\\lambda, 1-\\epsilon))} & \\frac{1+\\max(\\lambda, 1-\\epsilon)}{2} }}$$", "id": "2978638"}, {"introduction": "The power of Harris-type theorems depends on a crucial interplay between local mixing and global recurrence. A chain must not only contain a 'petite' set where it can probabilistically reset, but its dynamics must also guarantee that it can actually return to this set from anywhere in the state space. This exercise [@problem_id:2978615] explores a classic counterexample of a transient chain that, despite possessing a petite set, is not Harris recurrent because a strong underlying drift prevents the chain from ever accessing it from far-away states, demonstrating a critical failure mode of ergodicity.", "problem": "Consider a discrete-time Markov chain on the real line with Borel $\\sigma$-algebra defined by\n$$\nX_{n+1} \\,=\\, X_n \\,+\\, 1 \\,+\\, \\xi_{n+1}, \\qquad n\\in\\mathbb{N}_0,\n$$\nwhere $\\{\\xi_n\\}_{n\\geq 1}$ are independent and identically distributed random variables with the uniform distribution on the interval $[-\\tfrac{1}{2}, \\tfrac{1}{2}]$, independent of $X_0$. The transition kernel $P(x,\\cdot)$ therefore has a Lebesgue density supported on the interval $[x+\\tfrac{1}{2}, x+\\tfrac{3}{2}]$.\n\nUsing only foundational definitions and facts about Markov chains, petite sets, and Harris-type theorems:\n\n- Recall that a Borel set $C$ is petite if there exist a nontrivial finite measure $\\nu$ and a probability distribution $a$ on $\\mathbb{N}$ such that for all Borel sets $A$ and all $x\\in C$,\n$$\n\\sum_{m=0}^{\\infty} a(m)\\,P^m(x,A) \\,\\geq\\, \\nu(A).\n$$\n- Recall that a $\\psi$-irreducible Markov chain is Harris recurrent if for every set $A$ with $\\psi(A)>0$, the chain visits $A$ infinitely often with probability $1$ from every initial state. Positive Harris recurrence additionally requires the existence of an invariant probability measure.\n\nTasks:\n\n1. Derive a general sufficient condition under which Harris recurrence fails for a $\\psi$-irreducible Markov chain on $\\mathbb{R}$ in terms of a Lyapunov function $V:\\mathbb{R}\\to[1,\\infty)$ that is unbounded off compacts, assuming there exists a bounded set $C$ and a constant $\\varepsilon>0$ such that\n$$\nPV(x) \\,-\\, V(x) \\,\\geq\\, \\varepsilon, \\qquad \\text{for all } x\\notin C,\n$$\nand the chain has one-step increments that are almost surely bounded below by a strictly positive constant outside $C$. Explain why these conditions imply that the chain cannot be Harris recurrent.\n\n2. For the specific chain above, identify an explicit bounded Borel set $C\\subset\\mathbb{R}$ and prove that $C$ is petite by exhibiting an integer $m\\geq 1$, a constant $\\varepsilon>0$, and a probability measure $\\nu$ such that\n$$\nP^m(x,A) \\,\\geq\\, \\varepsilon\\,\\nu(A)\n$$\nholds for all Borel sets $A$ and all $x\\in C$.\n\n3. Prove that this petite set $C$ is not accessible from sufficiently large initial states in the sense that there exists $R>0$ such that, for all $x\\geq R$, the hitting probability of $C$ is zero:\n$$\n\\mathbb{P}_x\\!\\left(\\tau_C < \\infty\\right) \\,=\\, 0, \\qquad \\tau_C := \\inf\\{n\\geq 0: X_n\\in C\\}.\n$$\nExplain how this failure of accessibility of a petite set invalidates the conclusion of Harris-type ergodic theorems.\n\nFinally, compute the exact value of the hitting probability\n$$\n\\mathbb{P}_{10}\\!\\left(\\tau_C < \\infty\\right).\n$$\nProvide the final answer as a single real number. No rounding is required.", "solution": "This problem investigates the conditions for Harris recurrence of a Markov chain on $\\mathbb{R}$ and demonstrates a scenario where a key condition—accessibility of a petite set—fails. The solution is presented in three parts, as requested, followed by a final calculation.\n\n### Part 1: Sufficient Condition for Failure of Harris Recurrence\n\nA $\\psi$-irreducible Markov chain is defined as Harris recurrent if, for every set $A$ with $\\psi(A)>0$, the chain, starting from any state $x$, visits $A$ infinitely often with probability $1$. That is, $\\mathbb{P}_x(\\sum_{n=0}^{\\infty} \\mathbb{I}_{A}(X_n) = \\infty)=1$ for all $x$ in the state space. A direct consequence of this is that the chain must visit $A$ at least once with probability $1$, i.e., $\\mathbb{P}_x(\\tau_A < \\infty) = 1$, where $\\tau_A$ is the first hitting time of $A$. A weaker condition, necessary for Harris recurrence, is that the chain must be \"recurrent\" in the sense that it cannot be transient. A chain is transient if there is a positive probability that it never returns to its starting region.\n\nThe problem provides two conditions for a $\\psi$-irreducible chain on $\\mathbb{R}$:\n1. There exists a bounded set $C$, a constant $\\varepsilon>0$, and a Lyapunov function $V:\\mathbb{R}\\to[1,\\infty)$ that is unbounded off compact sets, such that $PV(x) - V(x) \\geq \\varepsilon$ for all $x\\notin C$. The operator $P$ acts on functions as $PV(x) = \\mathbb{E}[V(X_{n+1})|X_n=x]$. This condition implies that, on average, the value of the Lyapunov function increases whenever the chain is outside the set $C$. This is a classic criterion for transience.\n\n2. The chain has one-step increments that are almost surely bounded below by a strictly positive constant outside $C$. Let us formalize this. There exists a constant $\\delta>0$ such that for any $x \\notin C$, the increment $X_{n+1}-X_n$ satisfies $\\mathbb{P}_x(X_1 - x \\ge \\delta) = 1$ (or a similar condition for moving away from $C$, e.g., $\\mathbb{P}_x(|X_1| - |x| \\ge \\delta) = 1$ if $C$ is centered at $0$).\n\nLet's focus on the second condition as it provides a more direct argument for the failure of Harris recurrence. Let $C$ be a bounded set. Since $C$ is bounded, it is contained within some interval $[-M, M]$ for a sufficiently large $M>0$. The condition states that for $x \\notin C$, the increments are bounded below by $\\delta>0$. Let's consider a starting point $x_0 > M$. Then $x_0 \\notin C$. The state at time $n=1$ is $X_1 = x_0 + (X_1 - x_0)$. By our assumption, $X_1 - x_0 \\ge \\delta$ almost surely. Thus, $X_1 \\ge x_0 + \\delta > M + \\delta > M$.\nBy induction, if $X_n > M$, then $X_n \\notin C$. The next state $X_{n+1}$ satisfies $X_{n+1} \\geq X_n + \\delta > M + \\delta > M$.\nTherefore, if the chain starts at any state $X_0 = x > M$, then all subsequent states $X_n$ for $n \\ge 1$ will almost surely be greater than $M$. The chain moves deterministically to $+\\infty$.\nSymmetrically, there might be a region $x < -M$ from which the chain moves to $-\\infty$.\n\nNow, consider any bounded Borel set $A$ in $\\mathbb{R}$. Let us assume $A$ has $\\psi(A)>0$. Since $A$ is bounded, we can find a value $x_A$ such that $A \\subset (-\\infty, x_A)$. If we choose an initial state $X_0=x > \\max(M, x_A)$, then as shown above, $X_n \\geq x + n\\delta$ for all $n \\geq 1$. All subsequent states will be greater than $x$, and thus greater than any value in $A$.\nThis means that the chain will never visit the set $A$. The hitting time $\\tau_A = \\inf\\{n \\ge 0 : X_n \\in A\\}$ is infinite. So, $\\mathbb{P}_x(\\tau_A < \\infty) = 0$.\n\nThis contradicts the definition of a Harris recurrent chain. If the chain were Harris recurrent, it would have to visit the set $A$ infinitely often with probability $1$ from any starting point $x$. We have shown that for a suitably chosen starting point $x$, it never visits $A$ at all. Therefore, a chain satisfying this condition on its increments cannot be Harris recurrent. The Lyapunov drift condition points to the same conclusion (transience) but the argument based on the strictly positive increment is more direct and conclusive for this specific setup.\n\n### Part 2: Proving a Set is Petite\n\nThe Markov chain is given by $X_{n+1} = X_n + 1 + \\xi_{n+1}$, where $\\xi_{n+1} \\sim U[-\\frac{1}{2}, \\frac{1}{2}]$.\nThe next state $X_{n+1}$, given $X_n=x$, is uniformly distributed on the interval $[x+1-\\frac{1}{2}, x+1+\\frac{1}{2}] = [x+\\frac{1}{2}, x+\\frac{3}{2}]$. The transition kernel $P(x, \\cdot)$ has a density $p(x,y)$ with respect to the Lebesgue measure $\\lambda$:\n$$ p(x,y) = \\begin{cases} 1 & \\text{if } y \\in [x+\\frac{1}{2}, x+\\frac{3}{2}] \\\\ 0 & \\text{otherwise} \\end{cases} $$\nA set $C$ is petite if (under the simplified definition given) there exist an integer $m \\ge 1$, a constant $\\varepsilon > 0$, and a probability measure $\\nu$ such that for all $x \\in C$ and all Borel sets $A$:\n$$ P^m(x,A) \\ge \\varepsilon \\nu(A) $$\nThis is the standard definition of an $m$-small set, and any small set is petite. We need to find $C$, $m$, $\\varepsilon$, and $\\nu$.\n\nLet's try with $m=1$. The condition becomes $P(x,A) \\ge \\varepsilon \\nu(A)$. This requires the supports of the transition measures $P(x,\\cdot)$ for all $x \\in C$ to have a common region of overlap. The support of $P(x,\\cdot)$ is $[x+\\frac{1}{2}, x+\\frac{3}{2}]$.\nLet us choose a bounded Borel set $C$. For the supports to overlap for all $x\\in C$, their intersection must be non-empty. Let $C = [c_1, c_2]$. The intersection of all supports for $x \\in C$ is given by:\n$$ I = \\bigcap_{x \\in [c_1, c_2]} [x+\\frac{1}{2}, x+\\frac{3}{2}] = [\\sup_{x \\in [c_1, c_2]}(x+\\frac{1}{2}), \\inf_{x \\in [c_1, c_2]}(x+\\frac{3}{2})] = [c_2+\\frac{1}{2}, c_1+\\frac{3}{2}] $$\nFor this interval to be non-empty, we need $c_1+\\frac{3}{2} \\ge c_2+\\frac{1}{2}$, which simplifies to $c_2 - c_1 \\le 1$.\nLet's choose $C=[0, \\frac{1}{2}]$. The length of this interval is $\\frac{1}{2} \\le 1$. For this choice of $C$, the intersection of the supports is $I = [\\frac{1}{2}+\\frac{1}{2}, 0+\\frac{3}{2}] = [1, \\frac{3}{2}]$. This interval has length $\\frac{1}{2}$.\n\nFor any $x \\in C = [0, \\frac{1}{2}]$, the density $p(x,y)$ is equal to $1$ for all $y \\in [1, \\frac{3}{2}]$, since $[1, \\frac{3}{2}] \\subset [x+\\frac{1}{2}, x+\\frac{3}{2}]$ for all $x \\in [0, \\frac{1}{2}]$.\n\nLet's define our probability measure $\\nu$ to be the uniform distribution on the interval $I=[1, \\frac{3}{2}]$. The density of $\\nu$, let's call it $q(y)$, is:\n$$ q(y) = \\begin{cases} \\frac{1}{\\lambda([1, 3/2])} = \\frac{1}{1/2} = 2 & \\text{if } y \\in [1, \\frac{3}{2}] \\\\ 0 & \\text{otherwise} \\end{cases} $$\nThe minorization condition $P(x,A) \\ge \\varepsilon\\nu(A)$ is equivalent to the condition on densities $p(x,y) \\ge \\varepsilon q(y)$ for almost all $y$.\nFor $x \\in C=[0, \\frac{1}{2}]$ and $y \\in [1, \\frac{3}{2}]$, we have $p(x,y)=1$ and $q(y)=2$. The inequality becomes $1 \\ge \\varepsilon \\cdot 2$.\nWe can satisfy this by choosing any $\\varepsilon \\le \\frac{1}{2}$. Let's choose $\\varepsilon = \\frac{1}{2}$.\n\nSo, for the set $C = [0, \\frac{1}{2}]$, we have found:\n- an integer $m=1$,\n- a constant $\\varepsilon = \\frac{1}{2} > 0$,\n- a probability measure $\\nu = U([1, \\frac{3}{2}])$,\nsuch that $P^1(x,A) \\ge \\frac{1}{2} \\nu(A)$ holds for all $x \\in C$ and all Borel sets $A$. This proves that $C=[0, \\frac{1}{2}]$ is a petite set.\n\n### Part 3: Failure of Accessibility and Implications\n\nWe need to show there exists $R>0$ such that for all $x \\ge R$, the hitting probability $\\mathbb{P}_x(\\tau_C < \\infty)$ is zero, where $C=[0, \\frac{1}{2}]$.\nThe dynamics of the chain are $X_{n+1} = X_n + 1 + \\xi_{n+1}$. Since $\\xi_{n+1} \\in [-\\frac{1}{2}, \\frac{1}{2}]$, the one-step increment $X_{n+1} - X_n = 1 + \\xi_{n+1}$ is always in the interval $[1-\\frac{1}{2}, 1+\\frac{1}{2}] = [\\frac{1}{2}, \\frac{3}{2}]$.\nThis means that at every step, the process must increase by at least $\\frac{1}{2}$: $X_{n+1} \\ge X_n + \\frac{1}{2}$ almost surely.\n\nThe set $C=[0, \\frac{1}{2}]$ has a maximum value of $\\sup C = \\frac{1}{2}$.\nLet's consider an initial state $X_0 = x > \\frac{1}{2}$. Since $x \\notin C$, the first hitting time $\\tau_C$ must be greater than $0$ if it is finite. The state at time $1$ is $X_1 \\ge X_0 + \\frac{1}{2} > \\frac{1}{2} + \\frac{1}{2} = 1$.\nInductively, assume $X_n > \\frac{1}{2}$. Then $X_{n+1} \\ge X_n + \\frac{1}{2} > \\frac{1}{2} + \\frac{1}{2} = 1$.\nThis shows that if $X_0=x > \\frac{1}{2}$, then all subsequent states $X_n$ for $n \\ge 1$ are strictly greater than $1$, and thus are not in $C=[0, \\frac{1}{2}]$. The initial state $X_0=x$ is also not in $C$.\nTherefore, for any $x > \\frac{1}{2}$, the chain started at $x$ will never enter the set $C$. The set of times $\\{n \\ge 0 : X_n \\in C\\}$ is empty, which implies that its infimum, $\\tau_C$, is $\\infty$.\nThus, for any $x > \\frac{1}{2}$, we have $\\mathbb{P}_x(\\tau_C < \\infty) = \\mathbb{P}_x(\\infty < \\infty) = 0$.\nWe can choose any $R > \\frac{1}{2}$. For instance, let $R=1$. For all $x \\ge 1$, we have $x > \\frac{1}{2}$, and the conclusion $\\mathbb{P}_x(\\tau_C < \\infty) = 0$ holds.\n\nThis failure of accessibility has crucial implications for Harris-type ergodic theorems. These theorems establish a link between the existence of petite sets and the recurrence/ergodic properties of a Markov chain. For instance, a fundamental result states that a $\\psi$-irreducible chain is Harris recurrent if and only if every petite set is an \"H-set\", meaning it is hit with probability 1 from every starting point. More generally, the conclusions of powerful theorems like the Harris Ergodic Theorem (linking positive Harris recurrence to a unique invariant probability measure) rely on the assumption that the chain is Harris recurrent.\n\nOur chain is $\\lambda$-irreducible and possesses a petite set $C=[0, \\frac{1}{2}]$. However, we have shown that this set is not accessible from states $x > \\frac{1}{2}$. The failure of accessibility, $\\mathbb{P}_x(\\tau_C < \\infty) = 0$ for some $x$, directly implies that the chain cannot be Harris recurrent. This example demonstrates that the existence of a petite set is not, by itself, sufficient to guarantee Harris recurrence. The global property of recurrence requires not just the local mixing behavior encapsulated by a petite set, but also that the chain's global dynamics ensure that it can actually reach this set from anywhere in its state space.\n\n### Final Calculation\n\nWe need to compute the exact value of the hitting probability $\\mathbb{P}_{10}(\\tau_C < \\infty)$, where $C=[0, \\frac{1}{2}]$.\nThe initial state is $X_0 = 10$.\nIn Part 3, we established that for any initial state $x > \\frac{1}{2}$, the probability of ever hitting the set $C$ is zero.\nSince the initial state is $X_0 = 10$, and $10 > \\frac{1}{2}$, this result applies directly. The chain starts at $X_0 = 10$. The next state is $X_1 \\ge X_0 + \\frac{1}{2} = 10.5$. By induction, $X_n \\ge 10 + \\frac{n}{2}$ for all $n \\ge 0$. As $10 > \\frac{1}{2}$, all states $X_n$ will remain greater than $\\frac{1}{2}$ and thus the chain will never enter $C=[0, \\frac{1}{2}]$.\nTherefore, $\\tau_C = \\infty$ almost surely. The probability of the event $\\{\\tau_C < \\infty\\}$ is $0$.\n$$ \\mathbb{P}_{10}(\\tau_C < \\infty) = 0 $$", "answer": "$$\\boxed{0}$$", "id": "2978615"}]}