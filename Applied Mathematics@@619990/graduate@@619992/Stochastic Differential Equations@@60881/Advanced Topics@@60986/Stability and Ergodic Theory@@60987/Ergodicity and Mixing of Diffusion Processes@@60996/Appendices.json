{"hands_on_practices": [{"introduction": "Understanding the long-term behavior of a stochastic system begins with identifying its equilibrium, or invariant measure. This first exercise [@problem_id:2974319] challenges you to compute this fundamental quantity for the Ornstein-Uhlenbeck process, a cornerstone model for systems exhibiting mean reversion. By solving the stationary Fokker-Planck equation, you will practice a core technique that directly links a process's microscopic dynamics, its drift and diffusion coefficients, to its macroscopic stationary distribution.", "problem": "Consider the one-dimensional Ornstein–Uhlenbeck diffusion defined by the Stochastic Differential Equation (SDE)\n$$\ndX_{t}=-\\gamma X_{t}\\,dt+\\sigma\\,dW_{t},\n$$\nwhere $W_{t}$ is a standard Brownian motion, and $\\gamma>0$, $\\sigma>0$ are constants. Let $L$ denote the infinitesimal generator of the Markov semigroup associated with $X_{t}$, and let $L^{\\ast}$ denote the adjoint (in the sense of distributions) acting on probability densities. An invariant probability measure for this diffusion is a measure with density $\\rho$ on $\\mathbb{R}$ such that the stationary Fokker–Planck equation $L^{\\ast}\\rho=0$ holds, together with appropriate boundary conditions that ensure normalizability and zero probability flux at infinity.\n\nStarting from the definitions of $L$ for a one-dimensional Itô diffusion and the forward Kolmogorov (Fokker–Planck) equation, derive the stationary equation for $\\rho$ and solve it to obtain a normalized, nonnegative density $\\rho(x)$ on $\\mathbb{R}$ that is integrable and yields a probability measure. Explicitly provide the closed-form expression for $\\rho(x)$ and verify that it satisfies $L^{\\ast}\\rho=0$ and the natural boundary conditions as $|x|\\to\\infty$.\n\nYour final answer must be the single, closed-form analytic expression for the stationary density $\\rho(x)$.", "solution": "The problem statement is valid. It is scientifically grounded, well-posed, objective, and complete. It asks for the derivation of the stationary probability density for the one-dimensional Ornstein-Uhlenbeck process, a standard and well-defined problem in the theory of stochastic differential equations.\n\nThe one-dimensional Ornstein-Uhlenbeck process is described by the stochastic differential equation (SDE):\n$$\ndX_{t}=-\\gamma X_{t}\\,dt+\\sigma\\,dW_{t}\n$$\nwhere $\\gamma > 0$ and $\\sigma > 0$ are constants. This is a specific case of a general one-dimensional Itô diffusion $dX_t = b(X_t)dt + a(X_t)dW_t$, with a drift coefficient $b(x) = -\\gamma x$ and a constant diffusion coefficient $a(x) = \\sigma$.\n\nThe infinitesimal generator $L$ of the Markov semigroup associated with this process acts on a twice-differentiable function $f(x)$ as:\n$$\n(L f)(x) = b(x) \\frac{df}{dx} + \\frac{1}{2} a(x)^2 \\frac{d^2f}{dx^2}\n$$\nSubstituting the coefficients for the Ornstein-Uhlenbeck process, we get:\n$$\n(L f)(x) = -\\gamma x \\frac{df}{dx} + \\frac{1}{2} \\sigma^2 \\frac{d^2f}{dx^2}\n$$\nThe evolution of the probability density $\\rho(x,t)$ of the process $X_t$ is governed by the forward Kolmogorov equation, also known as the Fokker-Planck equation:\n$$\n\\frac{\\partial \\rho}{\\partial t} = L^{\\ast}\\rho\n$$\nwhere $L^{\\ast}$ is the formal adjoint of the operator $L$. For a one-dimensional diffusion, $L^{\\ast}$ acting on a density $\\rho$ is given by:\n$$\nL^{\\ast}\\rho = -\\frac{\\partial}{\\partial x} \\left( b(x) \\rho(x) \\right) + \\frac{1}{2} \\frac{\\partial^2}{\\partial x^2} \\left( a(x)^2 \\rho(x) \\right)\n$$\nThe Fokker-Planck equation can be expressed as a continuity equation $\\frac{\\partial \\rho}{\\partial t} = -\\frac{\\partial J}{\\partial x}$, where $J(x,t)$ is the probability current (or flux):\n$$\nJ(x,t) = b(x)\\rho(x,t) - \\frac{1}{2}\\frac{\\partial}{\\partial x}\\left(a(x)^2 \\rho(x,t)\\right)\n$$\nAn invariant or stationary probability measure is one whose density $\\rho(x)$ does not change with time, i.e., $\\frac{\\partial \\rho}{\\partial t} = 0$. This implies that the stationary Fokker-Planck equation $L^{\\ast}\\rho = 0$ must hold. From the continuity equation form, this means:\n$$\n\\frac{dJ}{dx} = 0\n$$\nwhere $J(x)$ is the stationary probability current. This implies that $J(x)$ must be a constant, say $J(x) = C$. The problem specifies the natural boundary condition of zero probability flux at infinity. This physical condition requires that particles do not accumulate at or escape to infinity, so $\\lim_{|x|\\to\\infty} J(x) = 0$. Since $J(x)$ is constant, we must have $C=0$, which means the stationary current is identically zero for all $x$:\n$$\nJ(x) = b(x)\\rho(x) - \\frac{1}{2}\\frac{d}{dx}\\left(a(x)^2 \\rho(x)\\right) = 0\n$$\nSubstituting the coefficients for the Ornstein-Uhlenbeck process, $b(x) = -\\gamma x$ and $a(x) = \\sigma$:\n$$\n-\\gamma x \\rho(x) - \\frac{1}{2}\\frac{d}{dx}\\left(\\sigma^2 \\rho(x)\\right) = 0\n$$\nSince $\\sigma$ is a constant, this simplifies to a first-order ordinary differential equation for $\\rho(x)$:\n$$\n-\\gamma x \\rho(x) - \\frac{\\sigma^2}{2} \\frac{d\\rho}{dx} = 0\n$$\nThis equation is separable:\n$$\n\\frac{\\sigma^2}{2} \\frac{d\\rho}{dx} = -\\gamma x \\rho(x)\n$$\n$$\n\\frac{1}{\\rho} d\\rho = -\\frac{2\\gamma}{\\sigma^2} x \\,dx\n$$\nIntegrating both sides gives:\n$$\n\\int \\frac{1}{\\rho} d\\rho = \\int -\\frac{2\\gamma}{\\sigma^2} x \\,dx\n$$\n$$\n\\ln(\\rho) = -\\frac{2\\gamma}{\\sigma^2} \\frac{x^2}{2} + K_0 = -\\frac{\\gamma x^2}{\\sigma^2} + K_0\n$$\nwhere $K_0$ is the constant of integration. Exponentiating both sides yields the general solution for $\\rho(x)$:\n$$\n\\rho(x) = \\exp\\left(-\\frac{\\gamma x^2}{\\sigma^2} + K_0\\right) = A \\exp\\left(-\\frac{\\gamma x^2}{\\sigma^2}\\right)\n$$\nwhere $A = \\exp(K_0)$ is a positive constant.\n\nTo be a probability density, $\\rho(x)$ must be normalized, i.e., its integral over $\\mathbb{R}$ must equal $1$:\n$$\n\\int_{-\\infty}^{\\infty} \\rho(x) dx = 1\n$$\n$$\nA \\int_{-\\infty}^{\\infty} \\exp\\left(-\\frac{\\gamma x^2}{\\sigma^2}\\right) dx = 1\n$$\nThe integral is a standard Gaussian integral of the form $\\int_{-\\infty}^{\\infty} \\exp(-c x^2) dx = \\sqrt{\\pi/c}$. In our case, $c = \\frac{\\gamma}{\\sigma^2}$. Since $\\gamma > 0$, the integral converges.\n$$\n\\int_{-\\infty}^{\\infty} \\exp\\left(-\\frac{\\gamma x^2}{\\sigma^2}\\right) dx = \\sqrt{\\frac{\\pi}{\\gamma/\\sigma^2}} = \\sigma \\sqrt{\\frac{\\pi}{\\gamma}}\n$$\nSubstituting this back into the normalization condition:\n$$\nA \\cdot \\sigma \\sqrt{\\frac{\\pi}{\\gamma}} = 1\n$$\nSolving for the normalization constant $A$:\n$$\nA = \\frac{1}{\\sigma \\sqrt{\\pi/\\gamma}} = \\sqrt{\\frac{\\gamma}{\\pi \\sigma^2}}\n$$\nThus, the normalized, stationary probability density for the Ornstein-Uhlenbeck process is:\n$$\n\\rho(x) = \\sqrt{\\frac{\\gamma}{\\pi \\sigma^2}} \\exp\\left(-\\frac{\\gamma x^2}{\\sigma^2}\\right)\n$$\nThis is a Gaussian (normal) distribution with mean $0$ and variance $\\frac{\\sigma^2}{2\\gamma}$.\n\nWe can verify that this solution satisfies the stationary Fokker-Planck equation, $J(x)=0$. We compute the derivative of $\\rho(x)$:\n$$\n\\frac{d\\rho}{dx} = \\sqrt{\\frac{\\gamma}{\\pi \\sigma^2}} \\exp\\left(-\\frac{\\gamma x^2}{\\sigma^2}\\right) \\cdot \\left(-\\frac{2\\gamma x}{\\sigma^2}\\right) = \\rho(x) \\left(-\\frac{2\\gamma x}{\\sigma^2}\\right)\n$$\nSubstituting into the expression for the current $J(x)$:\n$$\nJ(x) = -\\gamma x \\rho(x) - \\frac{\\sigma^2}{2} \\frac{d\\rho}{dx} = -\\gamma x \\rho(x) - \\frac{\\sigma^2}{2} \\left( \\rho(x) \\left(-\\frac{2\\gamma x}{\\sigma^2}\\right) \\right)\n$$\n$$\nJ(x) = -\\gamma x \\rho(x) + \\gamma x \\rho(x) = 0\n$$\nThe condition $L^{\\ast}\\rho = -dJ/dx = 0$ is satisfied. The boundary conditions are also satisfied: $\\rho(x)$ is non-negative, integrable (normalized to $1$), and the flux $J(x)$ is zero everywhere, including at infinity.", "answer": "$$\\boxed{\\sqrt{\\frac{\\gamma}{\\pi \\sigma^{2}}} \\exp\\left(-\\frac{\\gamma x^{2}}{\\sigma^{2}}\\right)}$$", "id": "2974319"}, {"introduction": "Once we know the equilibrium state, the next crucial question is how quickly the system converges to it. This practice [@problem_id:2974282] introduces the concept of the spectral gap, which provides a precise exponential rate of convergence in an $L^2$ sense. You will move beyond the Fokker-Planck equation to the spectral analysis of the process's generator, using the elegant theory of Hermite polynomials to find the eigenvalues and quantify the speed of equilibration for the Ornstein-Uhlenbeck process.", "problem": "Consider the $1$-dimensional Ornstein–Uhlenbeck (OU) diffusion process defined by the stochastic differential equation (SDE)\n$$\n\\mathrm{d}X_{t} \\;=\\; -\\,\\gamma\\,X_{t}\\,\\mathrm{d}t \\;+\\; \\sigma\\,\\mathrm{d}W_{t},\n$$\nwhere $W_{t}$ is a standard Brownian motion, $\\gamma>0$ is the linear drift coefficient, and $\\sigma>0$ is the diffusion intensity. Let $\\mathcal{L}$ denote the infinitesimal generator of this Markov process acting on twice continuously differentiable functions and let $\\mu$ be its unique invariant probability measure on $\\mathbb{R}$. Work in the weighted space $L^{2}(\\mu)$ and recall that, for reversible diffusions, the spectrum of $-\\,\\mathcal{L}$ is real and nonnegative. Define the spectral gap $\\lambda_{1}$ to be the smallest strictly positive eigenvalue of the self-adjoint operator $-\\,\\mathcal{L}$ (equivalently, the best constant in the Poincaré inequality for $\\mu$).\n\nStarting from fundamental definitions and well-tested facts about generators of diffusion processes, invariant measures, and orthogonal polynomials associated with Gaussian weights, determine the value of the spectral gap $\\lambda_{1}$ as a function of $\\gamma$. Your derivation should justify the identification of an orthogonal polynomial basis adapted to $\\mu$ and the diagonalization of $\\mathcal{L}$ on that basis. The final answer must be a single closed-form analytic expression in terms of $\\gamma$ only. No numerical approximation or rounding is required.", "solution": "The problem requires the determination of the spectral gap $\\lambda_{1}$ for the $1$-dimensional Ornstein-Uhlenbeck (OU) process. The process is governed by the stochastic differential equation (SDE):\n$$\n\\mathrm{d}X_{t} \\;=\\; -\\,\\gamma\\,X_{t}\\,\\mathrm{d}t \\;+\\; \\sigma\\,\\mathrm{d}W_{t}\n$$\nwhere $\\gamma > 0$ and $\\sigma > 0$. We will proceed by first identifying the infinitesimal generator of the process, then determining its unique invariant measure, and finally finding the spectrum of the generator on the appropriate function space $L^{2}(\\mu)$ by using an orthogonal polynomial basis.\n\nStep 1: The Infinitesimal Generator\nFor a general $1$-dimensional Itô diffusion process $\\mathrm{d}X_{t} = b(X_{t})\\,\\mathrm{d}t + a(X_{t})\\,\\mathrm{d}W_{t}$, the infinitesimal generator $\\mathcal{L}$ acting on a function $f \\in C^{2}(\\mathbb{R})$ is given by the differential operator:\n$$\n\\mathcal{L} \\;=\\; b(x)\\frac{\\mathrm{d}}{\\mathrm{d}x} \\;+\\; \\frac{1}{2}a(x)^{2}\\frac{\\mathrm{d}^{2}}{\\mathrm{d}x^{2}}\n$$\nFor the given OU process, the drift coefficient is $b(x) = -\\gamma x$ and the diffusion coefficient is $a(x) = \\sigma$. Substituting these into the general form, we obtain the generator for the OU process:\n$$\n\\mathcal{L} \\;=\\; -\\gamma x \\frac{\\mathrm{d}}{\\mathrm{d}x} \\;+\\; \\frac{\\sigma^{2}}{2}\\frac{\\mathrm{d}^{2}}{\\mathrm{d}x^{2}}\n$$\n\nStep 2: The Invariant Probability Measure\nThe invariant probability measure $\\mu$ has a density $\\pi(x)$ that satisfies the stationary Fokker-Planck equation, which is equivalent to requiring that $\\mathcal{L}$ be self-adjoint in the Hilbert space $L^{2}(\\mu)$. The condition for self-adjointness of a diffusion generator of the form $\\mathcal{L} = b(x) \\frac{\\mathrm{d}}{\\mathrm{d}x} + \\frac{1}{2}D(x)\\frac{\\mathrm{d}^{2}}{\\mathrm{d}x^{2}}$ (where $D(x) = a(x)^{2}$) is that its density $\\pi(x)$ satisfies the relation $\\frac{1}{2}\\frac{\\mathrm{d}}{\\mathrm{d}x}(D(x)\\pi(x)) = b(x)\\pi(x)$, assuming vanishing boundary terms at infinity.\n\nFor our process, $b(x) = -\\gamma x$ and $D(x) = \\sigma^{2}$ is a constant. The condition becomes:\n$$\n\\frac{1}{2}\\frac{\\mathrm{d}}{\\mathrm{d}x}(\\sigma^{2}\\pi(x)) \\;=\\; -\\gamma x \\pi(x)\n$$\n$$\n\\frac{\\sigma^{2}}{2}\\frac{\\mathrm{d}\\pi}{\\mathrm{d}x} \\;=\\; -\\gamma x \\pi(x)\n$$\nThis is a separable first-order ordinary differential equation:\n$$\n\\frac{1}{\\pi(x)}\\frac{\\mathrm{d}\\pi}{\\mathrm{d}x} \\;=\\; -\\frac{2\\gamma}{\\sigma^{2}}x\n$$\nIntegrating both sides with respect to $x$ yields:\n$$\n\\ln(\\pi(x)) \\;=\\; -\\frac{\\gamma}{\\sigma^{2}}x^{2} \\;+\\; C'\n$$\nwhere $C'$ is the constant of integration. Exponentiating gives the form of the density:\n$$\n\\pi(x) \\;=\\; C \\exp\\left(-\\frac{\\gamma}{\\sigma^{2}}x^{2}\\right)\n$$\nwhere $C = \\exp(C')$. This is the probability density function of a Gaussian distribution with mean $0$ and variance $v = \\frac{\\sigma^{2}}{2\\gamma}$. To be a probability density, it must integrate to $1$. The normalization constant is $C = \\frac{1}{\\sqrt{2\\pi v}} = \\sqrt{\\frac{\\gamma}{\\pi\\sigma^{2}}}$. Thus, the invariant measure $\\mu$ is the Gaussian measure $\\mathcal{N}(0, \\frac{\\sigma^{2}}{2\\gamma})$.\n\nStep 3: Orthogonal Polynomial Basis for $L^{2}(\\mu)$\nThe Hilbert space $L^{2}(\\mu)$ consists of functions $f$ such that $\\int_{-\\infty}^{\\infty} |f(x)|^{2}\\pi(x)\\mathrm{d}x < \\infty$. Given that the weight function $\\pi(x)$ is Gaussian, the natural orthogonal basis is the set of Hermite polynomials. The standard \"probabilist's\" Hermite polynomials, denoted $He_{n}(y)$, are orthogonal with respect to the standard normal weight $\\frac{1}{\\sqrt{2\\pi}}\\exp(-y^{2}/2)$.\n\nOur weight is $\\pi(x) \\propto \\exp(-\\frac{x^{2}}{2v})$. We can relate our variable $x$ to a standard normal variable $y$ by the scaling $y = x/\\sqrt{v}$. With $v = \\sigma^{2}/(2\\gamma)$, this gives:\n$$\ny \\;=\\; \\frac{x}{\\sqrt{\\sigma^{2}/(2\\gamma)}} \\;=\\; x\\frac{\\sqrt{2\\gamma}}{\\sigma}\n$$\nThe polynomials $H_{n}(x) = He_{n}(y) = He_{n}\\left(x\\frac{\\sqrt{2\\gamma}}{\\sigma}\\right)$ for $n=0, 1, 2, \\dots$ form a complete orthogonal basis for $L^{2}(\\mu)$.\n\nStep 4: Diagonalization of the Generator\nWe now find the eigenvalues of $\\mathcal{L}$ by acting on these basis functions $H_{n}(x)$. We first express the operator $\\mathcal{L}$ in terms of the rescaled coordinate $y$. Using the chain rule:\n$$\n\\frac{\\mathrm{d}}{\\mathrm{d}x} \\;=\\; \\frac{\\mathrm{d}y}{\\mathrm{d}x} \\frac{\\mathrm{d}}{\\mathrm{d}y} \\;=\\; \\frac{\\sqrt{2\\gamma}}{\\sigma} \\frac{\\mathrm{d}}{\\mathrm{d}y}\n$$\n$$\n\\frac{\\mathrm{d}^{2}}{\\mathrm{d}x^{2}} \\;=\\; \\left(\\frac{\\sqrt{2\\gamma}}{\\sigma}\\right)^{2} \\frac{\\mathrm{d}^{2}}{\\mathrm{d}y^{2}} \\;=\\; \\frac{2\\gamma}{\\sigma^{2}} \\frac{\\mathrm{d}^{2}}{\\mathrm{d}y^{2}}\n$$\nSubstituting these into the expression for $\\mathcal{L}$, and also substituting $x = y \\frac{\\sigma}{\\sqrt{2\\gamma}}$:\n$$\n\\mathcal{L} \\;=\\; -\\gamma \\left(y \\frac{\\sigma}{\\sqrt{2\\gamma}}\\right) \\left(\\frac{\\sqrt{2\\gamma}}{\\sigma} \\frac{\\mathrm{d}}{\\mathrm{d}y}\\right) \\;+\\; \\frac{\\sigma^{2}}{2} \\left(\\frac{2\\gamma}{\\sigma^{2}} \\frac{\\mathrm{d}^{2}}{\\mathrm{d}y^{2}}\\right)\n$$\n$$\n\\mathcal{L} \\;=\\; -\\gamma y \\frac{\\mathrm{d}}{\\mathrm{d}y} \\;+\\; \\gamma \\frac{\\mathrm{d}^{2}}{\\mathrm{d}y^{2}} \\;=\\; \\gamma \\left(\\frac{\\mathrm{d}^{2}}{\\mathrm{d}y^{2}} - y \\frac{\\mathrm{d}}{\\mathrm{d}y}\\right)\n$$\nThe probabilist's Hermite polynomials $He_{n}(y)$ are the eigenfunctions of the operator $\\left(\\frac{\\mathrm{d}^{2}}{\\mathrm{d}y^{2}} - y \\frac{\\mathrm{d}}{\\mathrm{d}y}\\right)$. Specifically, they satisfy the differential equation:\n$$\n\\frac{\\mathrm{d}^{2}}{\\mathrm{d}y^{2}}He_{n}(y) - y \\frac{\\mathrm{d}}{\\mathrm{d}y}He_{n}(y) \\;=\\; -n \\, He_{n}(y)\n$$\nApplying the operator $\\mathcal{L}$ to our basis function $H_n(x) = He_n(y)$:\n$$\n\\mathcal{L} H_{n}(x) \\;=\\; \\gamma \\left(\\frac{\\mathrm{d}^{2}}{\\mathrm{d}y^{2}} - y \\frac{\\mathrm{d}}{\\mathrm{d}y}\\right) He_{n}(y) \\;=\\; \\gamma (-n \\, He_{n}(y)) \\;=\\; -n\\gamma \\, H_{n}(x)\n$$\nThis shows that the polynomials $H_{n}(x)$ are eigenfunctions of $\\mathcal{L}$ with corresponding eigenvalues $-n\\gamma$ for $n = 0, 1, 2, \\dots$.\n\nStep 5: The Spectral Gap\nThe problem concerns the spectrum of the self-adjoint operator $-\\mathcal{L}$ on $L^{2}(\\mu)$. Since the eigenvalues of $\\mathcal{L}$ are $\\{-n\\gamma\\}_{n=0}^{\\infty}$, the eigenvalues of $-\\mathcal{L}$ are $\\{n\\gamma\\}_{n=0}^{\\infty}$.\nThe full spectrum of $-\\mathcal{L}$ is given by:\n$$\n\\text{Spec}(-\\mathcal{L}) = \\{0, \\gamma, 2\\gamma, 3\\gamma, \\dots\\}\n$$\nThe smallest eigenvalue is $\\lambda_{0} = 0$, corresponding to the eigenfunction $H_{0}(x) = 1$, which spans the space of constant functions (the equilibrium state). The spectral gap, $\\lambda_{1}$, is defined as the smallest strictly positive eigenvalue of $-\\mathcal{L}$. This corresponds to the eigenvalue for $n=1$.\n$$\n\\lambda_{1} \\;=\\; 1 \\cdot \\gamma \\;=\\; \\gamma\n$$\nThe spectral gap is thus equal to the drift coefficient $\\gamma$ and is independent of the diffusion intensity $\\sigma$. This value quantifies the exponential rate of convergence of the process to its invariant measure $\\mu$.", "answer": "$$\\boxed{\\gamma}$$", "id": "2974282"}, {"introduction": "Spectral methods are powerful but often limited to specific classes of systems. To analyze a broader range of diffusions, especially in high dimensions, we turn to more robust tools like coupling and entropy dissipation methods. This advanced exercise [@problem_id:2974220] guides you through deriving convergence rates in the Wasserstein and total variation metrics, using techniques rooted in optimal transport and functional inequalities. This practice demonstrates how different analytical frameworks can provide sharp, explicit bounds on mixing times, a key skill in the study of complex stochastic dynamics.", "problem": "Consider the $d$-dimensional overdamped Langevin diffusion with a strongly convex quadratic potential,\n$$\n\\mathrm{d}X_{t} \\;=\\; -A\\,X_{t}\\,\\mathrm{d}t \\;+\\; \\sqrt{2}\\,\\mathrm{d}B_{t},\n$$\nwhere $A \\in \\mathbb{R}^{d \\times d}$ is symmetric positive definite with eigenvalues in $[m,L]$ for some constants $0 < m \\leq L < \\infty$, and $B_{t}$ is a standard $d$-dimensional Brownian motion. Let $\\pi$ denote the unique invariant probability measure of this diffusion, which is the centered Gaussian distribution with covariance $A^{-1}$, i.e., $\\pi = \\mathcal{N}(0, A^{-1})$. For an absolutely continuous initial law $\\mu_{0}$ with finite second moment and finite relative entropy with respect to $\\pi$, denote by $\\mu_{t}$ the law of $X_{t}$ at time $t$.\n\nUsing only fundamental properties of strongly convex potentials, the Fokker–Planck equation, and functional inequalities for Gaussian measures, derive explicit exponential convergence bounds to equilibrium in the following two distances:\n- the $2$-Wasserstein distance $W_{2}(\\mu_{t},\\pi)$,\n- the total variation distance $\\|\\mu_{t}-\\pi\\|_{\\mathrm{TV}}$.\n\nThe derivation must start from first principles: synchronous coupling for Wasserstein contraction, and the entropy dissipation identity combined with a Logarithmic Sobolev Inequality (LSI) for Gaussian measures for the total variation bound (via Pinsker’s inequality). Show that these bounds have exponential rates depending only on the strong convexity parameter $m$, and identify the corresponding rate constants in the exponent for each metric.\n\nYour final task is to compute the ratio of the two exponential rate constants (the rate in the exponent in the $2$-Wasserstein bound divided by the rate in the exponent in the total variation bound). Provide this ratio as a single real number. No rounding is needed and no units are required.", "solution": "The problem statement is evaluated as valid. It is scientifically grounded, well-posed, and objective. It describes the convergence to equilibrium for an Ornstein-Uhlenbeck process, a fundamental topic in the theory of stochastic differential equations. The required derivations are based on standard, rigorous techniques in the field. All necessary information is provided, and the tasks are unambiguous.\n\nWe will now proceed with the solution, which is divided into three parts: derivation of the convergence rate in the $2$-Wasserstein distance, derivation of the convergence rate in the total variation distance, and the computation of the ratio of these rates.\n\n**Part 1: Convergence in $2$-Wasserstein distance, $W_{2}(\\mu_{t},\\pi)$**\n\nThe derivation for the $2$-Wasserstein distance is based on a synchronous coupling argument. Consider two solutions, $X_t$ and $Y_t$, to the specified Langevin equation, driven by the *same* standard $d$-dimensional Brownian motion $B_t$:\n$$\n\\mathrm{d}X_{t} = -A\\,X_{t}\\,\\mathrm{d}t + \\sqrt{2}\\,\\mathrm{d}B_{t}\n$$\n$$\n\\mathrm{d}Y_{t} = -A\\,Y_{t}\\,\\mathrm{d}t + \\sqrt{2}\\,\\mathrm{d}B_{t}\n$$\nThe initial conditions $X_0$ and $Y_0$ are random variables. We define the difference process $Z_t = X_t - Y_t$. Subtracting the two stochastic differential equations (SDEs), the Brownian motion term cancels out, and we obtain a deterministic ordinary differential equation (ODE) for $Z_t$:\n$$\n\\mathrm{d}Z_{t} = -A(X_{t} - Y_{t})\\,\\mathrm{d}t = -A\\,Z_{t}\\,\\mathrm{d}t\n$$\nLet's analyze the time evolution of the squared Euclidean norm of $Z_t$, denoted by $|Z_t|^2$.\n$$\n\\frac{\\mathrm{d}}{\\mathrm{d}t}|Z_t|^2 = \\frac{\\mathrm{d}}{\\mathrm{d}t}(Z_t^T Z_t) = 2 Z_t^T \\frac{\\mathrm{d}Z_t}{\\mathrm{d}t} = 2 Z_t^T (-A Z_t) = -2 Z_t^T A Z_t\n$$\nThe problem states that the SDE corresponds to a potential $U(x) = \\frac{1}{2} x^T A x$ which is strongly convex with parameter $m>0$. This is consistent with the given information that $A$ is a symmetric positive definite matrix whose eigenvalues are bounded below by $m$. The Rayleigh-Ritz theorem for a symmetric matrix $A$ states that the minimum value of the Rayleigh quotient $\\frac{z^T A z}{z^T z}$ for $z \\neq 0$ is the smallest eigenvalue of $A$. Since the eigenvalues of $A$ are in $[m,L]$, the smallest eigenvalue is greater than or equal to $m$. This implies the inequality:\n$$\nZ_t^T A Z_t \\geq m |Z_t|^2\n$$\nSubstituting this into the time derivative of $|Z_t|^2$, we get a differential inequality:\n$$\n\\frac{\\mathrm{d}}{\\mathrm{d}t}|Z_t|^2 \\leq -2m |Z_t|^2\n$$\nBy applying Gronwall's inequality, we can integrate this inequality from time $0$ to $t$:\n$$\n|Z_t|^2 \\leq |Z_0|^2 \\exp(-2mt)\n$$\nTaking the expectation over the initial states $(X_0, Y_0)$, we have:\n$$\n\\mathbb{E}[|X_t - Y_t|^2] \\leq \\exp(-2mt) \\mathbb{E}[|X_0 - Y_0|^2]\n$$\nThe squared $2$-Wasserstein distance between two probability measures $\\mu$ and $\\nu$ on $\\mathbb{R}^d$ is defined as the minimum expected squared distance between coupled random variables:\n$$\nW_2^2(\\mu, \\nu) = \\inf_{\\gamma \\in \\Pi(\\mu, \\nu)} \\int_{\\mathbb{R}^d \\times \\mathbb{R}^d} |x-y|^2 \\mathrm{d}\\gamma(x,y)\n$$\nwhere $\\Pi(\\mu, \\nu)$ is the set of all joint probability measures (couplings) on $\\mathbb{R}^d \\times \\mathbb{R}^d$ with marginals $\\mu$ and $\\nu$.\n\nLet the law of $X_0$ be $\\mu_0$ and the law of $Y_0$ be the invariant measure $\\pi$. Then at time $t$, the law of $X_t$ is $\\mu_t$, and the law of $Y_t$ remains $\\pi$ because $\\pi$ is invariant. The joint law of $(X_t, Y_t)$ is a valid coupling of $\\mu_t$ and $\\pi$. Therefore, by the definition of $W_2$, we have:\n$$\nW_2^2(\\mu_t, \\pi) \\leq \\mathbb{E}[|X_t - Y_t|^2]\n$$\nCombining our inequalities, we obtain:\n$$\nW_2^2(\\mu_t, \\pi) \\leq \\exp(-2mt) \\mathbb{E}[|X_0 - Y_0|^2]\n$$\nThis inequality holds for any coupling of $(\\mu_0, \\pi)$. To obtain the tightest bound, we can take the infimum over all possible initial couplings:\n$$\nW_2^2(\\mu_t, \\pi) \\leq \\exp(-2mt) \\inf_{\\gamma_0 \\in \\Pi(\\mu_0, \\pi)} \\mathbb{E}_{\\gamma_0}[|X_0 - Y_0|^2] = \\exp(-2mt) W_2^2(\\mu_0, \\pi)\n$$\nTaking the square root of both sides gives the exponential convergence bound for the $2$-Wasserstein distance:\n$$\nW_2(\\mu_t, \\pi) \\leq \\exp(-mt) W_2(\\mu_0, \\pi)\n$$\nThe exponential rate constant for the convergence in $W_2$ distance is thus $k_{W2} = m$.\n\n**Part 2: Convergence in total variation distance, $\\|\\mu_{t}-\\pi\\|_{\\mathrm{TV}}$**\n\nThe derivation for the total variation distance relies on the relationship between relative entropy (or Kullback-Leibler divergence), its dissipation, and a Logarithmic Sobolev Inequality (LSI), followed by an application of Pinsker's inequality.\n\nLet $D_{KL}(\\mu || \\pi)$ be the relative entropy of a measure $\\mu$ with respect to $\\pi$. If $\\mu$ has a density $f$ with respect to $\\pi$, i.e., $\\mathrm{d}\\mu = f \\mathrm{d}\\pi$, then $D_{KL}(\\mu || \\pi) = \\int f \\ln f \\mathrm{d}\\pi$.\nFor the overdamped Langevin diffusion, the time evolution of the relative entropy of the law $\\mu_t$ with respect to the invariant measure $\\pi$ is governed by the entropy dissipation identity:\n$$\n\\frac{\\mathrm{d}}{\\mathrm{d}t} D_{KL}(\\mu_t || \\pi) = -I(\\mu_t || \\pi)\n$$\nwhere $I(\\mu_t || \\pi)$ is the relative Fisher information of $\\mu_t$ with respect to $\\pi$, defined as $I(\\mu || \\pi) = \\int |\\nabla(\\ln f)|^2 f \\mathrm{d}\\pi = \\int \\frac{|\\nabla f|^2}{f} \\mathrm{d}\\pi$.\n\nThe next step is to invoke a Logarithmic Sobolev Inequality (LSI) for the invariant measure $\\pi = \\mathcal{N}(0, A^{-1})$. The potential is $U(x) = \\frac{1}{2}x^T A x$. Its Hessian is $\\nabla^2 U(x) = A$. The assumption that the eigenvalues of $A$ are in $[m, L]$ with $m>0$ means that $A \\succeq mI_d$, where $I_d$ is the $d \\times d$ identity matrix. This is the condition of $m$-strong convexity for the potential $U(x)$.\nA fundamental result, stemming from Bakry-Émery theory, states that if the potential $U(x)$ is $m$-strongly convex, the corresponding Gibbs measure $\\pi \\propto \\exp(-U(x))$ satisfies an LSI with constant $m$. The LSI states:\n$$\nD_{KL}(\\mu || \\pi) \\leq \\frac{1}{2m} I(\\mu || \\pi)\n$$\nThis inequality holds for any probability measure $\\mu$ that is absolutely continuous with respect to $\\pi$. Rearranging this, we get a lower bound on the Fisher information:\n$$\nI(\\mu_t || \\pi) \\geq 2m D_{KL}(\\mu_t || \\pi)\n$$\nNow, we substitute this into the entropy dissipation identity to obtain a differential inequality for $H(t) = D_{KL}(\\mu_t || \\pi)$:\n$$\n\\frac{\\mathrm{d}H(t)}{\\mathrm{d}t} = -I(\\mu_t || \\pi) \\leq -2m H(t)\n$$\nApplying Gronwall's inequality, we find the exponential decay of the relative entropy:\n$$\nD_{KL}(\\mu_t || \\pi) \\leq D_{KL}(\\mu_0 || \\pi) \\exp(-2mt)\n$$\nFinally, to obtain a bound on the total variation distance, $\\|\\mu_t - \\pi\\|_{\\mathrm{TV}}$, we use Pinsker's inequality, which relates it to the relative entropy:\n$$\n\\|\\mu - \\pi\\|_{\\mathrm{TV}}^2 \\leq \\frac{1}{2} D_{KL}(\\mu || \\pi)\n$$\nApplying this to $\\mu_t$ and substituting our entropy decay bound:\n$$\n\\|\\mu_t - \\pi\\|_{\\mathrm{TV}}^2 \\leq \\frac{1}{2} D_{KL}(\\mu_t || \\pi) \\leq \\frac{1}{2} D_{KL}(\\mu_0 || \\pi) \\exp(-2mt)\n$$\nTaking the square root of both sides yields the final exponential convergence bound for the total variation distance:\n$$\n\\|\\mu_t - \\pi\\|_{\\mathrm{TV}} \\leq \\sqrt{\\frac{D_{KL}(\\mu_0 || \\pi)}{2}} \\exp(-mt)\n$$\nThe exponential rate constant for the convergence in total variation distance is thus $k_{\\mathrm{TV}} = m$.\n\n**Part 3: Ratio of the Rate Constants**\n\nFrom the derivations above, we have identified the exponential rate constants for the two metrics:\nThe rate constant for the $2$-Wasserstein distance is $k_{W2} = m$.\nThe rate constant for the total variation distance is $k_{\\mathrm{TV}} = m$.\n\nThe problem asks for the ratio of these two rates:\n$$\n\\text{Ratio} = \\frac{k_{W2}}{k_{\\mathrm{TV}}} = \\frac{m}{m} = 1\n$$", "answer": "$$\\boxed{1}$$", "id": "2974220"}]}