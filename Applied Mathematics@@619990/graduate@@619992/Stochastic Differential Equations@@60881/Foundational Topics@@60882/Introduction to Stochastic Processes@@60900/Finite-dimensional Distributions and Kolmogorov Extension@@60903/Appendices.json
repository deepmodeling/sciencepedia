{"hands_on_practices": [{"introduction": "The Kolmogorov Extension Theorem provides the foundation for constructing stochastic processes from a family of finite-dimensional distributions (FDDs). The theorem's power, however, hinges on strict consistency conditions. This exercise makes these abstract conditions concrete by tasking you with building a family of distributions that purposefully violates them.\n\nBy constructing a counterexample, you will gain a much deeper appreciation for why consistency is not a mere technicality, but a fundamental requirement for a collection of measures to represent a single, coherent stochastic process. This practice illuminates the logic behind the theorem by demonstrating the contradictions that arise in its absence. [@problem_id:2976903]", "problem": "Consider a family of probability measures intended to serve as finite-dimensional distributions for a real-valued stochastic process indexed by a finite time set. Start from the following foundational definitions. A stochastic process is a collection of random variables $\\{X_{t} : t \\in T\\}$, where $T$ is an index set. For any finite ordered tuple of distinct times $(t_{1},\\dots,t_{n}) \\in T^{n}$, the associated finite-dimensional distribution is a probability measure on $\\mathbb{R}^{n}$ defined as the law of $(X_{t_{1}},\\dots,X_{t_{n}})$. A family $\\{\\mu_{t_{1},\\dots,t_{n}} : (t_{1},\\dots,t_{n}) \\in T^{n}, n \\in \\mathbb{N}\\}$ of probability measures on $\\mathbb{R}^{n}$ is said to satisfy the marginalization consistency condition if for every $(t_{1},\\dots,t_{n})$ and every subset of indices, the push-forward of $\\mu_{t_{1},\\dots,t_{n}}$ under the corresponding coordinate projection equals the measure $\\mu$ indexed by the corresponding subtuple. The Kolmogorov extension theorem (KET) states that a consistent family of such measures defines a stochastic process on a suitable probability space.\n\nLet the time index set be $T = \\{0,1\\}$. Construct explicitly a family of probability measures $\\{\\mu_{(0)},\\mu_{(1)},\\mu_{(0,1)}\\}$ on $\\mathbb{R}$ and $\\mathbb{R}^{2}$ that violates the marginalization consistency condition described above. Use only standard distributions with well-defined probability density functions. Provide a rigorous explanation of why no stochastic process can have these as its finite-dimensional distributions, citing the relevant consistency property implied by the Kolmogorov extension theorem.\n\nFinally, define the numerical inconsistency metric\n$$\n\\Delta := \\left| \\mathbb{E}_{\\mu_{(0)}}[x] - \\mathbb{E}_{\\pi_{1\\sharp}\\mu_{(0,1)}}[x] \\right|,\n$$\nwhere $\\pi_{1} : \\mathbb{R}^{2} \\to \\mathbb{R}$ is the projection onto the first coordinate and $\\pi_{1\\sharp}\\mu_{(0,1)}$ denotes the push-forward of $\\mu_{(0,1)}$ under $\\pi_{1}$. Compute the exact value of $\\Delta$. No rounding is required, and no units are involved. Your final answer must be a single real number.", "solution": "The problem statement is validated as scientifically grounded, well-posed, and objective. It presents a standard, non-trivial problem in the theory of stochastic processes that is self-contained and free of contradictions. We may therefore proceed with the solution.\n\nThe problem requires the construction of a family of probability measures that is inconsistent in the sense of the Kolmogorov extension theorem (KET). A family of finite-dimensional distributions (FDDs) $\\{\\mu_{t_1, \\dots, t_n}\\}$ is consistent if, for any finite set of time indices $\\{t_1, \\dots, t_n\\}$ and any of its subsets $\\{t_{i_1}, \\dots, t_{i_k}\\}$, the measure $\\mu_{t_{i_1}, \\dots, t_{i_k}}$ is the marginal of $\\mu_{t_1, \\dots, t_n}$ corresponding to the coordinates $(i_1, \\dots, i_k)$.\n\nThe given time index set is $T = \\{0, 1\\}$. The family of measures to consider is $\\{\\mu_{(0)}, \\mu_{(1)}, \\mu_{(0,1)}\\}$. The consistency conditions are:\n1. The push-forward of $\\mu_{(0,1)}$ under the projection $\\pi_1(x,y)=x$ must be $\\mu_{(0)}$. This is denoted as $\\pi_{1\\sharp}\\mu_{(0,1)} = \\mu_{(0)}$.\n2. The push-forward of $\\mu_{(0,1)}$ under the projection $\\pi_2(x,y)=y$ must be $\\mu_{(1)}$. This is denoted as $\\pi_{2\\sharp}\\mu_{(0,1)} = \\mu_{(1)}$.\n\nWe will construct a family of measures that violates the first condition. We shall use normal distributions, which possess well-defined probability density functions (PDFs).\n\n**Step 1: Construct the Family of Measures**\n\nLet $\\mu_{(0)}$ be the probability measure for a standard normal distribution, $N(0,1)$. Its PDF, denoted by $f_0(x)$, is:\n$$\nf_0(x) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{x^2}{2}\\right)\n$$\n\nLet $\\mu_{(0,1)}$ be the probability measure for a bivariate normal distribution corresponding to a random vector $(X_0, X_1)$, where $X_0$ and $X_1$ are independent random variables with $X_0 \\sim N(1,1)$ and $X_1 \\sim N(0,1)$. The joint PDF, $f_{0,1}(x,y)$, is the product of the individual PDFs:\n$$\nf_{0,1}(x,y) = \\left( \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{(x-1)^2}{2}\\right) \\right) \\left( \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{y^2}{2}\\right) \\right) = \\frac{1}{2\\pi} \\exp\\left(-\\frac{(x-1)^2 + y^2}{2}\\right)\n$$\n\nFor completeness, we define $\\mu_{(1)}$ to be the law of a standard normal distribution, $N(0,1)$, which is consistent with the marginal of $\\mu_{(0,1)}$ for the second coordinate. Its PDF is $f_1(y) = f_0(y)$.\n\n**Step 2: Demonstrate Inconsistency**\n\nWe check if the first consistency condition, $\\pi_{1\\sharp}\\mu_{(0,1)} = \\mu_{(0)}$, holds. The measure $\\pi_{1\\sharp}\\mu_{(0,1)}$ is the marginal distribution for the first coordinate of $\\mu_{(0,1)}$. We obtain its PDF, let's call it $g(x)$, by integrating the joint PDF $f_{0,1}(x,y)$ with respect to $y$:\n$$\ng(x) = \\int_{-\\infty}^{\\infty} f_{0,1}(x,y) \\, dy = \\int_{-\\infty}^{\\infty} \\frac{1}{2\\pi} \\exp\\left(-\\frac{(x-1)^2 + y^2}{2}\\right) \\, dy\n$$\n$$\ng(x) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{(x-1)^2}{2}\\right) \\int_{-\\infty}^{\\infty} \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{y^2}{2}\\right) \\, dy\n$$\nThe integral evaluates to $1$, as it is the total probability of a standard normal distribution. Therefore, the marginal PDF is:\n$$\ng(x) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{(x-1)^2}{2}\\right)\n$$\nThis is the PDF of a normal distribution $N(1,1)$.\n\nThe consistency condition requires $g(x) = f_0(x)$ for all $x \\in \\mathbb{R}$. Comparing the two PDFs:\n$$\ng(x) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{(x-1)^2}{2}\\right) \\neq \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{x^2}{2}\\right) = f_0(x)\n$$\nSince the PDFs are not identical, the measures $\\pi_{1\\sharp}\\mu_{(0,1)}$ and $\\mu_{(0)}$ are not equal. This establishes a violation of the marginalization consistency condition.\n\nThe KET states that a consistent family of FDDs is a necessary and sufficient condition for the existence of a stochastic process having that family as its FDDs. Since our constructed family $\\{\\mu_{(0)}, \\mu_{(1)}, \\mu_{(0,1)}\\}$ is not consistent, no stochastic process $\\{X_t\\}_{t \\in \\{0,1\\}}$ can exist with these measures as its FDDs. If such a process existed, the law of $X_0$ would have to be $\\mu_{(0)}$ and simultaneously the marginal of the law of $(X_0, X_1)$, which is $\\pi_{1\\sharp}\\mu_{(0,1)}$. This leads to the contradiction $\\mu_{(0)} = \\pi_{1\\sharp}\\mu_{(0,1)}$, which we have shown to be false.\n\n**Step 3: Compute the Inconsistency Metric**\n\nThe numerical inconsistency metric is defined as:\n$$\n\\Delta := \\left| \\mathbb{E}_{\\mu_{(0)}}[x] - \\mathbb{E}_{\\pi_{1\\sharp}\\mu_{(0,1)}}[x] \\right|\n$$\nThe first term, $\\mathbb{E}_{\\mu_{(0)}}[x]$, is the expectation of a random variable with distribution $\\mu_{(0)} = N(0,1)$. The mean of this distribution is $0$.\n$$\n\\mathbb{E}_{\\mu_{(0)}}[x] = 0\n$$\nThe second term, $\\mathbb{E}_{\\pi_{1\\sharp}\\mu_{(0,1)}}[x]$, is the expectation of a random variable whose distribution is the marginal $\\pi_{1\\sharp}\\mu_{(0,1)}$. We found this distribution to be $N(1,1)$. The mean of this distribution is $1$.\n$$\n\\mathbb{E}_{\\pi_{1\\sharp}\\mu_{(0,1)}}[x] = 1\n$$\nSubstituting these values into the definition of $\\Delta$:\n$$\n\\Delta = |0 - 1| = |-1| = 1\n$$\nThe exact value of the inconsistency metric is $1$.", "answer": "$$\\boxed{1}$$", "id": "2976903"}, {"introduction": "For a Gaussian process, the entire family of FDDs is determined by a mean function and a covariance function. A crucial question is what properties a function must have to be a valid covariance function. This practice explores the necessary and sufficient condition of positive semidefiniteness.\n\nYou will connect this abstract property to a more concrete condition on the Fourier transform of the kernel via Bochner's theorem, a powerful bridge between harmonic analysis and probability. By working through the classic example of the Ornstein-Uhlenbeck covariance, you will learn a practical and powerful technique for verifying the validity of stationary covariance models, a cornerstone of time-series analysis. [@problem_id:2976905]", "problem": "Let $d \\in \\mathbb{N}$ be fixed, and let $\\{X(t)\\}_{t \\in \\mathbb{R}}$ be a proposed mean-zero $\\mathbb{R}^{d}$-valued Gaussian process with covariance function $C : \\mathbb{R} \\times \\mathbb{R} \\to \\mathbb{R}^{d \\times d}$ of the form $C(s,t) = k(s-t) I_{d}$, where $I_{d}$ is the $d \\times d$ identity matrix and $k : \\mathbb{R} \\to \\mathbb{R}$ is a continuous, symmetric function. A necessary and sufficient condition for $C$ to define consistent finite-dimensional distributions (and hence yield a process via the Kolmogorov extension theorem) is that $C$ is positive semidefinite, meaning that for all $n \\in \\mathbb{N}$, all times $t_{1},\\dots,t_{n} \\in \\mathbb{R}$, and all vectors $u_{1},\\dots,u_{n} \\in \\mathbb{R}^{d}$,\n$$\n\\sum_{i=1}^{n} \\sum_{j=1}^{n} u_{i}^{\\top} C(t_{i},t_{j}) u_{j} \\geq 0.\n$$\nStarting from the core definitions of covariance and positive semidefinite kernels, and invoking Bochner’s theorem for stationary kernels on $\\mathbb{R}$, explain how such a verification reduces to showing that the scalar kernel $k$ is the Fourier transform of a finite nonnegative measure. Then, for the concrete case $k(h) = \\exp(-|h|)$, compute the spectral density $g(\\omega)$, defined by\n$$\nk(h) \\;=\\; \\int_{\\mathbb{R}} \\exp\\!\\big(i \\omega h\\big) \\, g(\\omega) \\, d\\omega,\n$$\nwith $g(\\omega) \\geq 0$ for all $\\omega \\in \\mathbb{R}$. Use this representation to rewrite\n$$\n\\sum_{i=1}^{n} \\sum_{j=1}^{n} u_{i}^{\\top} C(t_{i},t_{j}) u_{j}\n$$\nas an integral of a squared norm and thereby conclude nonnegativity for arbitrary $n$, $t_{i}$, and $u_{i} \\in \\mathbb{R}^{d}$. Provide the explicit closed-form analytic expression for $g(\\omega)$ as your final answer. No numerical approximation is required.", "solution": "The problem requires us to validate that a given covariance function $C(s,t)$ gives rise to a well-defined Gaussian process. This validation hinges on the condition that the kernel $C$ must be positive semidefinite. We will first show how this condition for the matrix-valued kernel $C$ reduces to a condition on the scalar kernel $k$. Then, we will invoke Bochner's theorem to relate this to a Fourier transform representation. Finally, we will apply this framework to the specific case $k(h) = \\exp(-|h|)$, calculate its spectral density $g(\\omega)$, and use it to explicitly demonstrate the positive semidefinite property.\n\nThe covariance function is given as $C(s,t) = k(s-t) I_{d}$, where $s, t \\in \\mathbb{R}$, $k: \\mathbb{R} \\to \\mathbb{R}$ is a continuous and symmetric function, and $I_d$ is the $d \\times d$ identity matrix. For $C$ to define a consistent family of finite-dimensional distributions for a Gaussian process $\\{X(t)\\}_{t \\in \\mathbb{R}}$, it must be a positive semidefinite kernel. The definition of this property is that for any choice of $n \\in \\mathbb{N}$, any set of times $t_{1}, \\dots, t_{n} \\in \\mathbb{R}$, and any set of vectors $u_{1}, \\dots, u_{n} \\in \\mathbb{R}^{d}$, the following inequality must hold:\n$$\nS = \\sum_{i=1}^{n} \\sum_{j=1}^{n} u_{i}^{\\top} C(t_{i}, t_{j}) u_{j} \\geq 0\n$$\n\nFirst, we substitute the given form of $C(t_{i}, t_{j})$ into this expression:\n$$\nS = \\sum_{i=1}^{n} \\sum_{j=1}^{n} u_{i}^{\\top} \\left( k(t_{i}-t_{j}) I_{d} \\right) u_{j}\n$$\nThe identity matrix $I_d$ is a scalar multiplicative factor in the dot product, so we can rewrite this as:\n$$\nS = \\sum_{i=1}^{n} \\sum_{j=1}^{n} k(t_{i}-t_{j}) (u_{i}^{\\top} u_{j})\n$$\nLet the components of the vector $u_i \\in \\mathbb{R}^d$ be $u_{i,l}$ for $l=1, \\dots, d$. The dot product $u_i^\\top u_j$ is $\\sum_{l=1}^{d} u_{i,l} u_{j,l}$. Substituting this into the expression for $S$:\n$$\nS = \\sum_{i=1}^{n} \\sum_{j=1}^{n} k(t_{i}-t_{j}) \\left( \\sum_{l=1}^{d} u_{i,l} u_{j,l} \\right)\n$$\nBy linearity, we can swap the order of summation:\n$$\nS = \\sum_{l=1}^{d} \\left( \\sum_{i=1}^{n} \\sum_{j=1}^{n} u_{i,l} u_{j,l} k(t_{i}-t_{j}) \\right)\n$$\nFor $S$ to be non-negative for any choice of vectors $\\{u_i\\}_{i=1}^n$, it is necessary and sufficient that each term in the sum over $l$ is non-negative. This is because we can choose vectors $u_i$ that are non-zero only in a single component, say $l_0$. For instance, let $u_{i,l} = a_i \\delta_{l, l_0}$ where $a_i \\in \\mathbb{R}$ are arbitrary scalars and $\\delta$ is the Kronecker delta. Then $S$ reduces to the single term for $l=l_0$. Thus, for each $l \\in \\{1, \\dots, d\\}$, the quadratic form in the real variables $\\{u_{i,l}\\}_{i=1}^n$ must be non-negative. Letting $a_i = u_{i,l}$, this implies that for any $n \\in \\mathbb{N}$, any $t_1, \\dots, t_n \\in \\mathbb{R}$, and any scalars $a_1, \\dots, a_n \\in \\mathbb{R}$:\n$$\n\\sum_{i=1}^{n} \\sum_{j=1}^{n} a_{i} a_{j} k(t_{i}-t_{j}) \\geq 0\n$$\nThis is precisely the definition of the scalar function $k$ being a positive semidefinite kernel. The kernel $k$ is also stationary because it depends only on the difference of its arguments, $k(s,t) = k(s-t)$.\n\nBochner's theorem provides a characterization of continuous, stationary, positive semidefinite kernels on $\\mathbb{R}$. It states that a continuous function $k:\\mathbb{R} \\to \\mathbb{R}$ is positive semidefinite if and only if it is the Fourier transform of a finite non-negative measure $\\mu$ on $\\mathbb{R}$. That is, there exists a measure $\\mu$ with $\\mu(\\mathbb{R}) < \\infty$ and $\\mu(A) \\ge 0$ for all Borel sets $A \\subseteq \\mathbb{R}$, such that:\n$$\nk(h) = \\int_{\\mathbb{R}} \\exp(i\\omega h) \\, d\\mu(\\omega)\n$$\nIf this measure $\\mu$ is absolutely continuous with respect to the Lebesgue measure, we can write $d\\mu(\\omega) = g(\\omega) d\\omega$, where $g(\\omega) \\geq 0$ is a non-negative function called the spectral density. The condition of finite total mass means $\\int_{\\mathbb{R}} g(\\omega) d\\omega < \\infty$. The problem statement provides this latter form.\n\nNow, we consider the concrete case $k(h) = \\exp(-|h|)$. We need to find the spectral density $g(\\omega)$ satisfying\n$$\nk(h) = \\int_{\\mathbb{R}} \\exp(i \\omega h) g(\\omega) d\\omega\n$$\nThis relationship implies that $g(\\omega)$ is given by the inverse Fourier transform of $k(h)$. Using the unitary convention for the Fourier transform pair, this is:\n$$\ng(\\omega) = \\frac{1}{2\\pi} \\int_{\\mathbb{R}} k(h) \\exp(-i\\omega h) \\, dh\n$$\nSubstituting $k(h) = \\exp(-|h|)$:\n$$\ng(\\omega) = \\frac{1}{2\\pi} \\int_{-\\infty}^{\\infty} \\exp(-|h|) \\exp(-i\\omega h) \\, dh\n$$\nWe split the integral based on the definition of the absolute value $|h|$:\n$$\ng(\\omega) = \\frac{1}{2\\pi} \\left[ \\int_{-\\infty}^{0} \\exp(h) \\exp(-i\\omega h) \\, dh + \\int_{0}^{\\infty} \\exp(-h) \\exp(-i\\omega h) \\, dh \\right]\n$$\n$$\ng(\\omega) = \\frac{1}{2\\pi} \\left[ \\int_{-\\infty}^{0} \\exp((1 - i\\omega)h) \\, dh + \\int_{0}^{\\infty} \\exp(-(1 + i\\omega)h) \\, dh \\right]\n$$\nEvaluating the integrals:\n$$\n\\int_{-\\infty}^{0} \\exp((1 - i\\omega)h) \\, dh = \\left[ \\frac{\\exp((1 - i\\omega)h)}{1 - i\\omega} \\right]_{-\\infty}^{0} = \\frac{1}{1 - i\\omega} - 0 = \\frac{1}{1 - i\\omega}\n$$\n$$\n\\int_{0}^{\\infty} \\exp(-(1 + i\\omega)h) \\, dh = \\left[ \\frac{\\exp(-(1 + i\\omega)h)}{-(1 + i\\omega)} \\right]_{0}^{\\infty} = 0 - \\frac{1}{-(1 + i\\omega)} = \\frac{1}{1 + i\\omega}\n$$\nSumming these results:\n$$\ng(\\omega) = \\frac{1}{2\\pi} \\left( \\frac{1}{1 - i\\omega} + \\frac{1}{1 + i\\omega} \\right) = \\frac{1}{2\\pi} \\left( \\frac{(1 + i\\omega) + (1 - i\\omega)}{(1 - i\\omega)(1 + i\\omega)} \\right) = \\frac{1}{2\\pi} \\frac{2}{1 - (i\\omega)^{2}} = \\frac{1}{\\pi(1+\\omega^2)}\n$$\nThe resulting spectral density is $g(\\omega) = \\frac{1}{\\pi(1+\\omega^2)}$. This function is clearly non-negative for all $\\omega \\in \\mathbb{R}$. The total mass is $\\int_{-\\infty}^{\\infty} \\frac{1}{\\pi(1+\\omega^2)} d\\omega = \\frac{1}{\\pi} [\\arctan(\\omega)]_{-\\infty}^{\\infty} = \\frac{1}{\\pi}(\\frac{\\pi}{2} - (-\\frac{\\pi}{2})) = 1$, which is finite. This confirms that $k(h) = \\exp(-|h|)$ is a positive semidefinite kernel.\n\nFinally, we use this spectral representation to rewrite the sum $S$ and directly show its non-negativity.\n$$\nS = \\sum_{i=1}^{n} \\sum_{j=1}^{n} k(t_{i}-t_{j}) (u_{i}^{\\top} u_{j}) = \\sum_{i=1}^{n} \\sum_{j=1}^{n} \\left( \\int_{\\mathbb{R}} \\exp(i\\omega(t_i - t_j)) g(\\omega) d\\omega \\right) (u_i^{\\top} u_j)\n$$\nAssuming the integral and sums can be interchanged (which is justified by Fubini's theorem as the integrand is well-behaved):\n$$\nS = \\int_{\\mathbb{R}} \\left( \\sum_{i=1}^{n} \\sum_{j=1}^{n} \\exp(i\\omega t_i) \\exp(-i\\omega t_j) (u_i^{\\top} u_j) \\right) g(\\omega) d\\omega\n$$\nLet's analyze the expression inside the parentheses. We can separate the terms depending on $i$ and $j$:\n$$\n\\sum_{i=1}^{n} \\sum_{j=1}^{n} ( u_i^{\\top} \\exp(i\\omega t_i) ) ( u_j \\exp(-i\\omega t_j) )\n$$\nThis expression is the product of two sums. Let's define a complex vector $V(\\omega) \\in \\mathbb{C}^d$ for each $\\omega$ as $V(\\omega) = \\sum_{j=1}^{n} u_j \\exp(-i\\omega t_j)$. The vectors $u_j$ and times $t_j$ are real. The Hermitian conjugate of $V(\\omega)$ is $V(\\omega)^H = \\overline{V(\\omega)}^{\\top} = \\left(\\sum_{i=1}^{n} u_i \\exp(i\\omega t_i)\\right)^{\\top} = \\sum_{i=1}^{n} u_i^{\\top} \\exp(i\\omega t_i)$.\nThe squared Euclidean norm of $V(\\omega)$ in $\\mathbb{C}^d$ is $\\|V(\\omega)\\|^2 = V(\\omega)^H V(\\omega)$.\n$$\n\\|V(\\omega)\\|^2 = \\left( \\sum_{i=1}^{n} u_i^{\\top} \\exp(i\\omega t_i) \\right) \\left( \\sum_{j=1}^{n} u_j \\exp(-i\\omega t_j) \\right) = \\sum_{i=1}^{n} \\sum_{j=1}^{n} u_i^{\\top} u_j \\exp(i\\omega(t_i - t_j))\n$$\nThis is precisely the expression inside the integral for $S$. Therefore, we have:\n$$\nS = \\int_{\\mathbb{R}} \\left\\| \\sum_{j=1}^{n} u_j \\exp(-i\\omega t_j) \\right\\|^2 g(\\omega) \\, d\\omega\n$$\nThis represents the sum $S$ as an integral of a squared norm, as requested. We can now conclude non-negativity. The term $\\| \\sum_{j=1}^{n} u_j \\exp(-i\\omega t_j) \\|^2$ is a squared norm, so it is non-negative for all $\\omega$. We have shown that $g(\\omega) = \\frac{1}{\\pi(1+\\omega^2)}$ is also non-negative for all $\\omega$. The integrand is therefore a product of two non-negative functions, and is itself non-negative. The integral of a non-negative function over $\\mathbb{R}$ is non-negative. Thus, $S \\ge 0$. This holds for arbitrary $n$, $t_i$, and $u_i \\in \\mathbb{R}^d$, confirming that $C(s,t)=\\exp(-|s-t|)I_d$ is a valid positive semidefinite covariance function.\n\nThe explicit closed-form analytic expression for $g(\\omega)$ is the final answer.", "answer": "$$\\boxed{\\frac{1}{\\pi(1+\\omega^2)}}$$", "id": "2976905"}, {"introduction": "The Kolmogorov extension theorem guarantees the existence of a process, but it initially lives on a vast, abstract space of all possible functions. A critical subsequent step is to determine if a \"nice\" version of the process exists—for example, one with continuous paths. This exercise guides you through this essential second stage of analysis.\n\nYou will apply the Kolmogorov-Chentsov continuity criterion, a theorem that connects moment bounds on a process's increments to the regularity of its sample paths. Starting from the FDDs of what will become Brownian motion, you will derive the necessary moment bounds and deduce its celebrated path continuity, demonstrating how properties encoded in the FDDs can establish profound results about the entire process path. [@problem_id:2976955]", "problem": "Consider a family of finite-dimensional distributions on $\\mathbb{R}^{n}$ indexed by time tuples $0 \\le t_{1} < \\cdots < t_{n} \\le T$, defined as centered multivariate normal laws with covariance matrix $\\Sigma = (\\Sigma_{ij})_{1 \\le i,j \\le n}$ given by $\\Sigma_{ij} = \\min\\{t_{i}, t_{j}\\}$. \n\n(a) Starting from the definitions of finite-dimensional distributions and the Kolmogorov extension theorem, verify that this family is consistent in the sense of permutation invariance and marginalization. Conclude that there exists a stochastic process $\\{B_{t}\\}_{t \\in [0,T]}$ on the canonical path space whose finite-dimensional distributions are the given ones.\n\n(b) Using only properties that follow from these finite-dimensional distributions, derive the law of the increment $B_{t} - B_{s}$ for $s,t \\in [0,T]$, and compute the exact expression of the absolute moment $\\mathbb{E}\\!\\left[\\,|B_{t} - B_{s}|^{p}\\,\\right]$ for any $p > 0$. Your derivation must start from the density of a centered normal random variable and first principles of integral calculus and the gamma function, without quoting any pre-packaged moment formula.\n\n(c) State Kolmogorov’s continuity criterion (also known as the Kolmogorov–Chentsov theorem) for processes indexed by a one-dimensional parameter, and use your bound from part (b) to deduce that there exists a modification of $\\{B_{t}\\}_{t \\in [0,T]}$ with sample paths that are almost surely Hölder continuous of any order $\\alpha$ strictly less than $1/2$.\n\nProvide as your final answer the exact closed-form expression for $\\mathbb{E}\\!\\left[\\,|B_{t} - B_{s}|^{p}\\,\\right]$ in terms of $p$ and $|t-s|$. Do not approximate or round; give the expression in closed form using standard special functions if needed.", "solution": "The problem is validated as scientifically grounded, well-posed, and objective. It is a standard, non-trivial exercise in the theory of stochastic processes, specifically the construction of Brownian motion. All necessary information is provided, and the problem is free from any logical or factual flaws.\n\n(a) Verification of Kolmogorov Consistency Conditions\n\nThe Kolmogorov extension theorem provides conditions under which a family of finite-dimensional distributions (FDDs) is consistent, guaranteeing the existence of a stochastic process with those FDDs. Let $\\{\\mu_{t_1, \\dots, t_n}\\}_{0 \\leq t_1 < \\dots < t_n \\leq T, n \\in \\mathbb{N}}$ be the given family of distributions. The problem states that for any such time tuple, the distribution $\\mu_{t_1, \\dots, t_n}$ is that of a centered multivariate normal random vector $(B_{t_1}, \\dots, B_{t_n})$ on $\\mathbb{R}^n$ with a covariance matrix $\\Sigma^{(n)}$ whose entries are given by $\\Sigma^{(n)}_{ij} = \\mathbb{E}[B_{t_i}B_{t_j}] = \\min\\{t_i, t_j\\}$. We must verify two consistency conditions.\n\n1.  **Permutation Invariance:** Let $(t_1, \\dots, t_n)$ be a set of time points and let $\\pi$ be a permutation of $\\{1, \\dots, n\\}$. We must show that the law of the permuted vector $(B_{t_{\\pi(1)}}, \\dots, B_{t_{\\pi(n)}})$ is $\\mu_{t_{\\pi(1)}, \\dots, t_{\\pi(n)}}$.\n    The vector $(B_{t_1}, \\dots, B_{t_n})$ is governed by the law $\\mu_{t_1, \\dots, t_n}$. The permuted vector $(B_{t_{\\pi(1)}}, \\dots, B_{t_{\\pi(n)}})$ is a linear transformation of the original vector, and since the original vector is centered normal, the permuted vector is also centered normal. Its law is fully determined by its covariance matrix $\\Sigma'$. The entries of $\\Sigma'$ are:\n    $$ \\Sigma'_{ij} = \\text{Cov}(B_{t_{\\pi(i)}}, B_{t_{\\pi(j)}}) = \\mathbb{E}[B_{t_{\\pi(i)}} B_{t_{\\pi(j)}}] $$\n    By the given definition of the FDDs, this covariance is $\\min\\{t_{\\pi(i)}, t_{\\pi(j)}\\}$.\n    On the other hand, the prescribed law $\\mu_{t_{\\pi(1)}, \\dots, t_{\\pi(n)}}$ for the time points $(t_{\\pi(1)}, \\dots, t_{\\pi(n)})$ is a centered normal distribution with a covariance matrix $\\Sigma''$ whose entries are, by definition, $\\Sigma''_{ij} = \\min\\{t_{\\pi(i)}, t_{\\pi(j)}\\}$.\n    Since $\\Sigma' = \\Sigma''$, the law of the permuted vector is indeed the one prescribed by the family of distributions for the permuted time indices. The condition is satisfied.\n\n2.  **Marginalization:** For any $n > 1$ and time points $0 \\le t_1 < \\dots < t_n \\le T$, the marginal distribution of $(B_{t_1}, \\dots, B_{t_{n-1}})$ derived from the law $\\mu_{t_1, \\dots, t_n}$ must be equal to the law $\\mu_{t_1, \\dots, t_{n-1}}$.\n    The vector $(B_{t_1}, \\dots, B_{t_n})$ is centered normal with covariance matrix $\\Sigma^{(n)} = (\\min\\{t_i, t_j\\})_{1 \\le i,j \\le n}$. A fundamental property of multivariate normal distributions is that their marginals are also normal. The marginal distribution of the sub-vector $(B_{t_1}, \\dots, B_{t_{n-1}})$ is centered normal. Its covariance matrix is obtained by taking the corresponding $(n-1) \\times (n-1)$ sub-block of $\\Sigma^{(n)}$. For $1 \\le i,j \\le n-1$, the covariance is $\\min\\{t_i, t_j\\}$, which is precisely the matrix $\\Sigma^{(n-1)}$ defining the law $\\mu_{t_1, \\dots, t_{n-1}}$. Thus, the marginalization condition holds.\n\nSince both consistency conditions are satisfied, the Kolmogorov extension theorem guarantees the existence of a probability space $(\\Omega, \\mathcal{F}, \\mathbb{P})$ and a stochastic process $\\{B_t\\}_{t \\in [0,T]}$ whose finite-dimensional distributions are given by the family $\\{\\mu_{t_1, \\dots, t_n}\\}$.\n\n(b) Law and Absolute Moment of Increments\n\nLet $s, t \\in [0,T]$. We wish to find the law of the increment $X = B_t - B_s$. Without loss of generality, assume $s \\le t$. The random variable $X$ is a linear combination of the components of the centered normal random vector $(B_s, B_t)$. Specifically, $X = \\begin{pmatrix} -1 & 1 \\end{pmatrix} \\begin{pmatrix} B_s \\\\ B_t \\end{pmatrix}$. As such, $X$ itself is a normally distributed random variable.\nIts mean is $\\mathbb{E}[X] = \\mathbb{E}[B_t - B_s] = \\mathbb{E}[B_t] - \\mathbb{E}[B_s] = 0 - 0 = 0$.\nIts variance is:\n$$ \\text{Var}(X) = \\mathbb{E}[X^2] = \\mathbb{E}[(B_t - B_s)^2] = \\mathbb{E}[B_t^2 - 2B_sB_t + B_s^2] $$\nBy linearity of expectation and the definition of the covariance structure:\n$$ \\text{Var}(X) = \\mathbb{E}[B_t^2] - 2\\mathbb{E}[B_sB_t] + \\mathbb{E}[B_s^2] = \\min\\{t,t\\} - 2\\min\\{s,t\\} + \\min\\{s,s\\} $$\nSince we assumed $s \\le t$, this becomes:\n$$ \\text{Var}(X) = t - 2s + s = t - s $$\nIf we had $t < s$, the variance would be $t - 2t + s = s - t$. In general, $\\text{Var}(B_t - B_s) = |t-s|$.\nThus, the increment $B_t - B_s$ is distributed as a centered normal random variable with variance $|t-s|$, i.e., $B_t - B_s \\sim \\mathcal{N}(0, |t-s|)$.\n\nNow, we compute the $p$-th absolute moment $\\mathbb{E}[|B_t - B_s|^p]$ for $p>0$. Let $X = B_t - B_s$ and $\\sigma^2 = |t-s|$. The probability density function of $X$ is $f_X(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp(-\\frac{x^2}{2\\sigma^2})$.\nBy definition, the expected value is:\n$$ \\mathbb{E}[|X|^p] = \\int_{-\\infty}^{\\infty} |x|^p f_X(x) \\, dx = \\int_{-\\infty}^{\\infty} |x|^p \\frac{1}{\\sigma\\sqrt{2\\pi}} \\exp\\left(-\\frac{x^2}{2\\sigma^2}\\right) \\, dx $$\nThe integrand is an even function, so we can write the integral over $[0, \\infty)$ and multiply by $2$:\n$$ \\mathbb{E}[|X|^p] = \\frac{2}{\\sigma\\sqrt{2\\pi}} \\int_{0}^{\\infty} x^p \\exp\\left(-\\frac{x^2}{2\\sigma^2}\\right) \\, dx $$\nWe perform a substitution. Let $u = \\frac{x^2}{2\\sigma^2}$. Then $x^2 = 2\\sigma^2 u$, which implies $x = \\sigma\\sqrt{2u}$. The differential is $dx = \\sigma\\sqrt{2} \\cdot \\frac{1}{2}u^{-1/2} du = \\frac{\\sigma}{\\sqrt{2}}u^{-1/2} du$. The limits of integration remain $(0, \\infty)$.\n$$ \\mathbb{E}[|X|^p] = \\frac{2}{\\sigma\\sqrt{2\\pi}} \\int_{0}^{\\infty} (\\sigma\\sqrt{2u})^p \\exp(-u) \\left(\\frac{\\sigma}{\\sqrt{2}}u^{-1/2}\\right) \\, du $$\nCollecting terms:\n$$ \\mathbb{E}[|X|^p] = \\frac{2}{\\sigma\\sqrt{2\\pi}} \\cdot \\sigma^p (2)^{p/2} \\cdot \\frac{\\sigma}{\\sqrt{2}} \\int_{0}^{\\infty} u^{p/2} \\exp(-u) u^{-1/2} \\, du $$\n$$ = \\frac{2 \\sigma^{p}}{\\sqrt{2\\pi} \\sqrt{2}} 2^{p/2} \\int_{0}^{\\infty} u^{(p-1)/2} \\exp(-u) \\, du = \\frac{\\sigma^p}{\\sqrt{\\pi}} 2^{p/2} \\int_{0}^{\\infty} u^{\\frac{p+1}{2}-1} \\exp(-u) \\, du $$\nThe integral is the definition of the Gamma function, $\\Gamma(z) = \\int_0^\\infty t^{z-1} e^{-t} dt$, with $z = \\frac{p+1}{2}$.\n$$ \\mathbb{E}[|X|^p] = \\frac{\\sigma^p 2^{p/2}}{\\sqrt{\\pi}} \\Gamma\\left(\\frac{p+1}{2}\\right) $$\nSubstituting back $\\sigma^2 = |t-s|$, we get:\n$$ \\mathbb{E}[|B_t - B_s|^p] = (|t-s|^{1/2})^p \\frac{2^{p/2}}{\\sqrt{\\pi}} \\Gamma\\left(\\frac{p+1}{2}\\right) = |t-s|^{p/2} \\frac{2^{p/2}}{\\sqrt{\\pi}} \\Gamma\\left(\\frac{p+1}{2}\\right) $$\n\n(c) Kolmogorov's Continuity Criterion and Hölder Continuity\n\nKolmogorov's continuity criterion (also known as the Kolmogorov–Chentsov theorem) for a real-valued process $\\{X_t\\}_{t \\in [0,T]}$ states that if there exist positive constants $C$, $a$, and $b$ such that for all $s,t \\in [0,T]$:\n$$ \\mathbb{E}\\left[|X_t - X_s|^a\\right] \\le C|t-s|^{1+b} $$\nthen there exists a modification of the process, say $\\{\\tilde{X}_t\\}$, such that $\\mathbb{P}(\\tilde{X}_t = X_t) = 1$ for all $t$, and the sample paths of $\\tilde{X}$ are almost surely Hölder continuous of any order $\\alpha \\in (0, b/a)$.\n\nFrom part (b), we have the exact expression:\n$$ \\mathbb{E}\\left[|B_t - B_s|^p\\right] = C_p |t-s|^{p/2} $$\nwhere $C_p = \\frac{2^{p/2}}{\\sqrt{\\pi}} \\Gamma\\left(\\frac{p+1}{2}\\right)$ is a finite positive constant for any $p>0$.\nTo apply the criterion, we must find a value of $p$ such that the exponent of $|t-s|$ is greater than $1$. We let $a=p$ in the criterion and require $p/2 > 1$, which implies $p > 2$.\nFor any such $p$, we can write $p/2 = 1+b$ where $b = p/2 - 1 > 0$. For instance, for a chosen $p > 2$, we have:\n$$ \\mathbb{E}\\left[|B_t - B_s|^p\\right] = C_p |t-s|^{1 + (p/2 - 1)} $$\nThis satisfies the condition of the theorem with $a=p$, $C=C_p$, and $b = p/2 - 1$.\nThe theorem then guarantees the existence of a continuous modification whose sample paths are a.s. Hölder continuous of any order $\\alpha$ such that:\n$$ \\alpha < \\frac{b}{a} = \\frac{p/2 - 1}{p} = \\frac{1}{2} - \\frac{1}{p} $$\nTo show that there is a modification with paths that are Hölder continuous for any order $\\alpha < 1/2$, we must show that for any such $\\alpha \\in (0, 1/2)$, we can find a suitable $p > 2$.\nGiven $\\alpha \\in (0, 1/2)$, we want to find $p$ such that $\\alpha < 1/2 - 1/p$. This inequality is equivalent to $1/p < 1/2 - \\alpha$, or $p > \\frac{1}{1/2 - \\alpha}$. Since $\\alpha < 1/2$, the denominator is positive, so such a $p$ always exists. Furthermore, since $1/2 - \\alpha < 1/2$, we have $\\frac{1}{1/2 - \\alpha} > 2$, so our choice of $p$ is always greater than $2$.\nTherefore, for any given $\\alpha \\in (0, 1/2)$, we can select a $p > \\frac{1}{1/2 - \\alpha}$, which satisfies the continuity criterion and proves the existence of a modification with a.s. Hölder continuous paths of order $\\alpha$. Since this holds for all $\\alpha \\in (0, 1/2)$, the claim is established.", "answer": "$$\\boxed{|t-s|^{p/2} 2^{p/2} \\frac{\\Gamma\\left(\\frac{p+1}{2}\\right)}{\\sqrt{\\pi}}}$$", "id": "2976955"}]}