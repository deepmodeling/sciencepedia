{"hands_on_practices": [{"introduction": "Understanding the structure of predictable sets is fundamental to stochastic calculus, as they form the domain for stochastic integrands. This first exercise provides direct, hands-on practice with the definition of the predictable $\\sigma$-algebra, challenging you to construct a left-continuous adapted process to prove a given set's predictability [@problem_id:2990970]. Mastering this technique solidifies your grasp of why these processes are the building blocks of predictability.", "problem": "Let $\\left(\\Omega,\\mathcal{F},(\\mathcal{F}_{t})_{t \\ge 0},\\mathbb{P}\\right)$ be a filtered probability space supporting a standard Brownian motion $\\left(W_{t}\\right)_{t \\ge 0}$ with the usual augmentation (that is, $(\\mathcal{F}_{t})_{t \\ge 0}$ is right-continuous and $\\mathcal{F}_{0}$ contains all $\\mathbb{P}$-null sets). Let $\\mathcal{P}$ denote the predictable $\\sigma$-algebra on $\\mathbb{R}_{+} \\times \\Omega$, defined as the smallest $\\sigma$-algebra making every left-continuous, adapted process jointly measurable.\n\nFix parameters $s \\in (0,\\infty)$, $t \\in (s,\\infty)$, and $\\lambda \\in (0,\\infty)$, and define the event $B := \\{\\omega \\in \\Omega : W_{s}(\\omega) \\ge 0\\} \\in \\mathcal{F}_{s}$. Consider the subset $\\Gamma \\subset \\mathbb{R}_{+} \\times \\Omega$ given by\n$$\n\\Gamma := (s,t] \\times B.\n$$\n\nTasks:\n1) Using only the defining property of $\\mathcal{P}$ as the smallest $\\sigma$-algebra for which all left-continuous, adapted processes are measurable, construct an explicit left-continuous, adapted process whose inverse image of a Borel set is $\\Gamma$, thereby concluding $\\Gamma \\in \\mathcal{P}$. For a fixed $\\omega \\in \\Omega$, compute the time section $\\Gamma(\\omega) := \\{u \\in \\mathbb{R}_{+} : (u,\\omega) \\in \\Gamma\\}$ and identify it as a Borel subset of $\\mathbb{R}_{+}$.\n\n2) Define the random variable\n$$\nY(\\omega) := \\int_{0}^{\\infty} \\exp(-\\lambda u)\\,\\mathbf{1}_{\\Gamma}(u,\\omega)\\, \\mathrm{d}u.\n$$\nCompute $\\mathbb{E}[Y]$ in closed form, expressed only in terms of $s$, $t$, and $\\lambda$. Provide your final answer as a single simplified analytic expression. No rounding is required.", "solution": "The problem is validated as a well-posed and scientifically grounded exercise in the theory of stochastic processes. All required definitions and conditions are provided, and there are no internal contradictions or violations of mathematical principles.\n\n**Part 1: Predictability of $\\Gamma$ and Computation of Time Section**\n\nThe first task is to demonstrate that the set $\\Gamma := (s,t] \\times B$ belongs to the predictable $\\sigma$-algebra $\\mathcal{P}$. The definition of $\\mathcal{P}$ provided is the smallest $\\sigma$-algebra on $\\mathbb{R}_{+} \\times \\Omega$ for which every left-continuous, adapted process is jointly measurable. If we can construct a left-continuous, adapted process $X = (X_u)_{u \\ge 0}$ and find a Borel set $A \\in \\mathcal{B}(\\mathbb{R})$ such that $\\Gamma = \\{(u,\\omega) \\in \\mathbb{R}_{+} \\times \\Omega : X_u(\\omega) \\in A\\}$, then by definition, $\\Gamma \\in \\mathcal{P}$.\n\nLet us define the process $X_u(\\omega)$ as the product of two indicator functions:\n$$\nX_u(\\omega) := \\mathbf{1}_{(s,t]}(u) \\cdot \\mathbf{1}_{B}(\\omega)\n$$\nwhere $\\mathbf{1}_{(s,t]}(u)$ is the indicator function for the time interval $(s,t]$ and $\\mathbf{1}_{B}(\\omega)$ is the indicator function for the event $B = \\{\\omega \\in \\Omega : W_{s}(\\omega) \\ge 0\\}$.\n\nWe must verify that this process is both adapted and left-continuous.\n\n**1. Adaptedness:**\nA process $(X_u)_{u \\ge 0}$ is adapted to the filtration $(\\mathcal{F}_u)_{u \\ge 0}$ if for every $u \\ge 0$, the random variable $X_u$ is $\\mathcal{F}_u$-measurable.\nFor a fixed time $u$, the term $\\mathbf{1}_{(s,t]}(u)$ is a deterministic constant, either $0$ or $1$. Thus, the measurability of $X_u$ depends on the measurability of $\\mathbf{1}_{B}(\\omega)$.\nLet's consider two cases for $u$:\n- If $u \\in [0,s]$, then $\\mathbf{1}_{(s,t]}(u) = 0$. In this case, $X_u(\\omega) = 0$ for all $\\omega \\in \\Omega$. The constant random variable $0$ is measurable with respect to any $\\sigma$-algebra, including $\\mathcal{F}_u$.\n- If $u \\in (s, \\infty)$, we are given that $B = \\{W_s \\ge 0\\} \\in \\mathcal{F}_s$. Since the filtration $(\\mathcal{F}_u)_{u \\ge 0}$ is increasing, for any $u > s$, we have $\\mathcal{F}_s \\subset \\mathcal{F}_u$. Therefore, the event $B$ is also in $\\mathcal{F}_u$, which means the random variable $\\mathbf{1}_B$ is $\\mathcal{F}_u$-measurable. As $X_u$ is a product of a constant and an $\\mathcal{F}_u$-measurable random variable, $X_u$ is $\\mathcal{F}_u$-measurable.\n\nIn both cases, $X_u$ is $\\mathcal{F}_u$-measurable for all $u \\ge 0$. Hence, the process $(X_u)_{u \\ge 0}$ is adapted.\n\n**2. Left-Continuity:**\nA process is left-continuous if for every $\\omega \\in \\Omega$, the sample path $u \\mapsto X_u(\\omega)$ is a left-continuous function on $\\mathbb{R}_{+}$.\nFor a fixed $\\omega$, the sample path is $u \\mapsto \\mathbf{1}_{(s,t]}(u) \\cdot C$, where $C = \\mathbf{1}_B(\\omega)$ is a constant ($0$ or $1$). The left-continuity of the path is determined by the left-continuity of the function $g(u) := \\mathbf{1}_{(s,t]}(u)$.\nThe function $g(u)$ is defined as:\n$$\ng(u) =\n\\begin{cases}\n1 & \\text{if } s < u \\le t \\\\\n0 & \\text{otherwise}\n\\end{cases}\n$$\nThe potential points of discontinuity are at $u=s$ and $u=t$. Let us check for left-continuity at these points.\n- At $u=s$: $g(s) = 0$. The limit from the left is $\\lim_{u \\to s^-} g(u) = \\lim_{u \\to s^-} 0 = 0$. As $\\lim_{u \\to s^-} g(u) = g(s)$, the function is left-continuous at $s$.\n- At $u=t$: $g(t) = 1$. The limit from the left is $\\lim_{u \\to t^-} g(u) = \\lim_{u \\to t^-} 1 = 1$. As $\\lim_{u \\to t^-} g(u) = g(t)$, the function is left-continuous at $t$.\nThe function $g(u)$ is constant elsewhere, so it is left-continuous for all $u \\in \\mathbb{R}_{+}$. Consequently, each sample path $u \\mapsto X_u(\\omega)$ is left-continuous, and the process $(X_u)_{u \\ge 0}$ is left-continuous.\n\n**Conclusion for predictable set:**\nWe have constructed a left-continuous, adapted process $X_u(\\omega)$. Now we find the set $A \\in \\mathcal{B}(\\mathbb{R})$ such that $\\Gamma = X^{-1}(A)$.\nBy definition of $X_u(\\omega)$, its value is $1$ if and only if $\\mathbf{1}_{(s,t]}(u) = 1$ and $\\mathbf{1}_{B}(\\omega) = 1$. This is equivalent to $u \\in (s,t]$ and $\\omega \\in B$.\nIn other words, $(u,\\omega) \\in \\Gamma \\iff X_u(\\omega) = 1$. If $(u,\\omega) \\notin \\Gamma$, then either $u \\notin (s,t]$ or $\\omega \\notin B$, which implies $X_u(\\omega) = 0$.\nSo, $\\Gamma = \\{(u,\\omega) : X_u(\\omega) = 1\\}$. This is the inverse image of the set $\\{1\\}$.\nSince $\\{1\\}$ is a Borel subset of $\\mathbb{R}$, and $X$ is a left-continuous adapted process (and therefore $\\mathcal{P}$-measurable), it follows that $\\Gamma = X^{-1}(\\{1\\}) \\in \\mathcal{P}$.\n\n**Time Section $\\Gamma(\\omega)$:**\nFor a fixed $\\omega \\in \\Omega$, the time section $\\Gamma(\\omega)$ is the set of times $u$ for which $(u,\\omega) \\in \\Gamma$.\n$$\n\\Gamma(\\omega) = \\{u \\in \\mathbb{R}_{+} : u \\in (s,t] \\text{ and } \\omega \\in B\\}\n$$\nThe condition \"$\\omega \\in B$\" is either true or false for the given $\\omega$.\n- If $\\omega \\in B$ (i.e., $W_s(\\omega) \\ge 0$), the condition becomes $\\{u \\in \\mathbb{R}_{+} : u \\in (s,t]\\}$. Thus, $\\Gamma(\\omega) = (s,t]$.\n- If $\\omega \\notin B$ (i.e., $W_s(\\omega) < 0$), the condition \"$\\omega \\in B$\" is false, making the entire logical statement false for any $u$. Thus, the set is empty: $\\Gamma(\\omega) = \\emptyset$.\nBoth $(s,t]$ and $\\emptyset$ are Borel subsets of $\\mathbb{R}_{+}$.\n\n**Part 2: Computation of $\\mathbb{E}[Y]$**\n\nWe need to compute the expectation of the random variable $Y$:\n$$\nY(\\omega) := \\int_{0}^{\\infty} \\exp(-\\lambda u)\\,\\mathbf{1}_{\\Gamma}(u,\\omega)\\, \\mathrm{d}u\n$$\nThe integrand, $f(u,\\omega) = \\exp(-\\lambda u)\\,\\mathbf{1}_{\\Gamma}(u,\\omega)$, is a non-negative measurable function on $\\mathbb{R}_{+} \\times \\Omega$. By Fubini-Tonelli's theorem, we can interchange the expectation and the integral:\n$$\n\\mathbb{E}[Y] = \\mathbb{E}\\left[\\int_{0}^{\\infty} \\exp(-\\lambda u)\\,\\mathbf{1}_{\\Gamma}(u,\\omega)\\, \\mathrm{d}u\\right] = \\int_{0}^{\\infty} \\mathbb{E}\\left[\\exp(-\\lambda u)\\,\\mathbf{1}_{\\Gamma}(u,\\omega)\\right]\\, \\mathrm{d}u\n$$\nLet's analyze the inner expectation for a fixed $u$. We substitute the definition of $\\mathbf{1}_{\\Gamma}(u,\\omega)$:\n$$\n\\mathbf{1}_{\\Gamma}(u,\\omega) = \\mathbf{1}_{(s,t]}(u) \\cdot \\mathbf{1}_{B}(\\omega)\n$$\nThe expectation is taken with respect to the probability measure $\\mathbb{P}$ over $\\Omega$. For a fixed $u$, the terms $\\exp(-\\lambda u)$ and $\\mathbf{1}_{(s,t]}(u)$ are deterministic constants and can be factored out of the expectation:\n$$\n\\mathbb{E}\\left[\\exp(-\\lambda u)\\,\\mathbf{1}_{(s,t]}(u)\\,\\mathbf{1}_{B}(\\omega)\\right] = \\exp(-\\lambda u)\\,\\mathbf{1}_{(s,t]}(u)\\,\\mathbb{E}\\left[\\mathbf{1}_{B}(\\omega)\\right]\n$$\nThe expectation of an indicator function is the probability of the corresponding event:\n$$\n\\mathbb{E}\\left[\\mathbf{1}_{B}(\\omega)\\right] = \\mathbb{P}(B) = \\mathbb{P}(W_s \\ge 0)\n$$\nFor $s > 0$, the random variable $W_s$ (the position of a standard Brownian motion at time $s$) follows a normal distribution with mean $0$ and variance $s$, i.e., $W_s \\sim \\mathcal{N}(0,s)$. The probability density function of this distribution is symmetric about its mean of $0$. Therefore, the probability of $W_s$ being non-negative is exactly $\\frac{1}{2}$. (The probability of $W_s=0$ is $0$ as it is a continuous distribution).\n$$\n\\mathbb{P}(W_s \\ge 0) = \\frac{1}{2}\n$$\nSubstituting this back into the integral expression for $\\mathbb{E}[Y]$:\n$$\n\\mathbb{E}[Y] = \\int_{0}^{\\infty} \\exp(-\\lambda u)\\,\\mathbf{1}_{(s,t]}(u)\\,\\frac{1}{2}\\, \\mathrm{d}u\n$$\nThe indicator function $\\mathbf{1}_{(s,t]}(u)$ restricts the domain of integration from $\\mathbb{R}_{+}$ to the interval $(s,t]$. Note that the problem specifies $s < t$.\n$$\n\\mathbb{E}[Y] = \\frac{1}{2} \\int_{s}^{t} \\exp(-\\lambda u)\\, \\mathrm{d}u\n$$\nWe now compute the definite integral:\n$$\n\\int_{s}^{t} \\exp(-\\lambda u)\\, \\mathrm{d}u = \\left[ -\\frac{1}{\\lambda} \\exp(-\\lambda u) \\right]_{s}^{t} = \\left(-\\frac{1}{\\lambda} \\exp(-\\lambda t)\\right) - \\left(-\\frac{1}{\\lambda} \\exp(-\\lambda s)\\right) = \\frac{1}{\\lambda} \\left(\\exp(-\\lambda s) - \\exp(-\\lambda t)\\right)\n$$\nFinally, multiplying by the factor of $\\frac{1}{2}$, we obtain the closed-form expression for $\\mathbb{E}[Y]$:\n$$\n\\mathbb{E}[Y] = \\frac{1}{2\\lambda} \\left(\\exp(-\\lambda s) - \\exp(-\\lambda t)\\right)\n$$", "answer": "$$\\boxed{\\frac{\\exp(-\\lambda s) - \\exp(-\\lambda t)}{2\\lambda}}$$", "id": "2990970"}, {"introduction": "A central theme in modern stochastic theory is the decomposition of processes into a predictable part and a \"surprise\" or martingale part. This exercise explores this theme through the optional and predictable quadratic variations, denoted $[M]$ and $\\langle M \\rangle$ respectively, for a compensated Poisson process [@problem_id:2990977]. By calculating both and demonstrating that $[M]$ is not predictable, you will gain a concrete understanding of why the predictable compensator $\\langle M \\rangle$ is the correct object to work with in many contexts, such as in the definition of stochastic integrals.", "problem": "Let $\\left(\\Omega,\\mathcal{F},(\\mathcal{F}_{t})_{t\\geq 0},\\mathbb{P}\\right)$ be a filtered probability space satisfying the usual conditions, and let $\\left(N_{t}\\right)_{t\\geq 0}$ be a Poisson process of rate $1$ adapted to $\\left(\\mathcal{F}_{t}\\right)_{t\\geq 0}$ with stationary independent increments. Define the compensated process $M_{t} \\coloneqq N_{t}-t$, $t\\geq 0$. Use only the following foundational inputs:\n- A process $\\left(M_{t}\\right)_{t\\geq 0}$ is a martingale if for all $0\\leq s\\leq t$, $\\mathbb{E}\\left[M_{t}\\mid \\mathcal{F}_{s}\\right]=M_{s}$ and $\\mathbb{E}|M_{t}|<\\infty$.\n- The optional quadratic variation of a càdlàg (right-continuous with left limits) local martingale $\\left(M_{t}\\right)_{t\\geq 0}$ is the unique adapted, increasing, càdlàg process $\\left([M]_{t}\\right)_{t\\geq 0}$ starting at $0$ such that $M^{2}-[M]$ is a local martingale. For a purely discontinuous local martingale, $[M]_{t}=\\sum_{0<s\\leq t}\\left(\\Delta M_{s}\\right)^{2}$.\n- The predictable quadratic variation $\\left(\\langle M\\rangle_{t}\\right)_{t\\geq 0}$ is the unique predictable, increasing process starting at $0$ such that $M^{2}-\\langle M\\rangle$ is a local martingale.\n- A predictable process is measurable with respect to the predictable $\\sigma$-algebra; in particular, increasing predictable processes have jumps only at predictable stopping times. Jump times of a Poisson process are totally inaccessible stopping times.\n\nTasks:\n1. Prove that $\\left(M_{t}\\right)_{t\\geq 0}$ is a purely discontinuous martingale and is progressively measurable.\n2. Compute $[M]_{t}$ explicitly in terms of $\\left(N_{t}\\right)_{t\\geq 0}$.\n3. Prove that $[M]$ is not predictable.\n4. Using only the defining properties above, determine the predictable quadratic variation $\\langle M\\rangle_{t}$ explicitly.\n\nProvide your final answer as the single analytic expression for $\\langle M\\rangle_{t}$ as a function of $t$ (no units are involved and no rounding is required).", "solution": "The problem provides a compensated Poisson process $M_{t} \\coloneqq N_{t}-t$ and asks for a series of proofs and calculations regarding its properties as a martingale and its quadratic variations, based on a specific set of foundational definitions.\n\n1.  **Prove that $\\left(M_{t}\\right)_{t\\geq 0}$ is a purely discontinuous martingale and is progressively measurable.**\n\nFirst, we prove that $\\left(M_{t}\\right)_{t\\geq 0}$ is a martingale. We must verify two conditions for $0\\leq s\\leq t$: $\\mathbb{E}|M_{t}|<\\infty$ and $\\mathbb{E}\\left[M_{t}\\mid \\mathcal{F}_{s}\\right]=M_{s}$.\nThe Poisson process $\\left(N_{t}\\right)_{t\\geq 0}$ with rate $1$ has mean $\\mathbb{E}[N_{t}]=t$. The integrability condition is satisfied:\n$$\n\\mathbb{E}[|M_{t}|] = \\mathbb{E}[|N_{t}-t|] \\leq \\mathbb{E}[|N_{t}|] + |t| = \\mathbb{E}[N_{t}] + t = t+t = 2t < \\infty \\quad \\text{for all } t \\geq 0\n$$\nTo check the martingale property, we compute the conditional expectation:\n$$\n\\mathbb{E}\\left[M_{t}\\mid \\mathcal{F}_{s}\\right] = \\mathbb{E}\\left[N_{t}-t\\mid \\mathcal{F}_{s}\\right] = \\mathbb{E}\\left[N_{t}\\mid \\mathcal{F}_{s}\\right] - t\n$$\nUsing the tower property and that $N_{t} = N_{s} + (N_{t}-N_{s})$, we have:\n$$\n\\mathbb{E}\\left[N_{t}\\mid \\mathcal{F}_{s}\\right] = \\mathbb{E}\\left[N_{s} + (N_{t}-N_{s})\\mid \\mathcal{F}_{s}\\right] = \\mathbb{E}\\left[N_{s}\\mid \\mathcal{F}_{s}\\right] + \\mathbb{E}\\left[N_{t}-N_{s}\\mid \\mathcal{F}_{s}\\right]\n$$\nSince $N_{s}$ is $\\mathcal{F}_{s}$-measurable, $\\mathbb{E}\\left[N_{s}\\mid \\mathcal{F}_{s}\\right]=N_{s}$. Because $\\left(N_{t}\\right)_{t\\geq 0}$ has independent increments, the increment $N_{t}-N_{s}$ is independent of $\\mathcal{F}_{s}$. Thus, $\\mathbb{E}\\left[N_{t}-N_{s}\\mid \\mathcal{F}_{s}\\right]=\\mathbb{E}[N_{t}-N_{s}]$. Due to stationary increments and rate $1$, $N_{t}-N_{s}$ is distributed as a Poisson random variable with parameter $t-s$, so $\\mathbb{E}[N_{t}-N_{s}]=t-s$.\nSubstituting back, we get:\n$$\n\\mathbb{E}\\left[N_{t}\\mid \\mathcal{F}_{s}\\right] = N_{s} + t-s\n$$\nAnd therefore,\n$$\n\\mathbb{E}\\left[M_{t}\\mid \\mathcal{F}_{s}\\right] = (N_{s}+t-s) - t = N_{s}-s = M_{s}\n$$\nThus, $\\left(M_{t}\\right)_{t\\geq 0}$ is a martingale.\n\nThe sample paths of the Poisson process $N_{t}$ are càdlàg (right-continuous with left limits) and are step functions, changing value only at jump times. The process $t\\mapsto t$ is continuous. The process $M_{t}=N_{t}-t$ therefore has sample paths that are càdlàg and its only discontinuities are the jumps inherited from $N_{t}$. A martingale whose sample paths are of this form (piecewise constant between jumps plus a continuous function of finite variation) is a purely discontinuous martingale.\n\nA process is progressively measurable if for every $T>0$, the map $(s, \\omega) \\mapsto X_{s}(\\omega)$ from $[0, T] \\times \\Omega$ to $\\mathbb{R}$ is $\\mathcal{B}([0,T]) \\otimes \\mathcal{F}_{T}$-measurable. A fundamental result states that any adapted càdlàg process is progressively measurable. Since $\\left(N_{t}\\right)_{t\\geq 0}$ is adapted and càdlàg, it is progressively measurable. The deterministic process $t\\mapsto t$ is also progressively measurable. The difference $M_{t} = N_{t}-t$ of two progressively measurable processes is progressively measurable.\n\n2.  **Compute $[M]_{t}$ explicitly in terms of $\\left(N_{t}\\right)_{t\\geq 0}$.**\n\nThe problem provides the formula for the optional quadratic variation of a purely discontinuous local martingale: $[M]_{t}=\\sum_{0<s\\leq t}\\left(\\Delta M_{s}\\right)^{2}$, where $\\Delta M_{s} = M_{s}-M_{s-}$.\nThe jumps of $M_{t}=N_{t}-t$ are the jumps of $N_{t}$, as $t\\mapsto t$ is continuous:\n$$\n\\Delta M_{s} = (N_{s}-s) - (N_{s-}-s) = N_{s}-N_{s-} = \\Delta N_{s}\n$$\nA standard Poisson process with rate $1$ has jumps of size $1$. Thus, at any time $s$, $\\Delta N_{s}$ is either $0$ (if $s$ is not a jump time) or $1$ (if $s$ is a jump time).\nConsequently, $(\\Delta M_{s})^{2} = (\\Delta N_{s})^{2}$ is either $0^{2}=0$ or $1^{2}=1$. This means $(\\Delta N_{s})^{2} = \\Delta N_{s}$ for all $s>0$.\nWe can then write the optional quadratic variation as:\n$$\n[M]_{t} = \\sum_{0<s\\leq t} (\\Delta M_{s})^{2} = \\sum_{0<s\\leq t} (\\Delta N_{s})^{2} = \\sum_{0<s\\leq t} \\Delta N_{s}\n$$\nThe sum of all jumps of a counting process up to time $t$, starting from $0$, is equal to its value at time $t$. Given that $N_0=0$, we have $N_{t} = \\sum_{0<s\\leq t} \\Delta N_{s}$.\nTherefore, the optional quadratic variation is $[M]_{t}=N_{t}$.\n\n3.  **Prove that $[M]$ is not predictable.**\n\nFrom the previous task, we have $[M]_{t}=N_{t}$. We need to prove that the process $(N_{t})_{t\\geq 0}$ is not predictable.\nThe problem provides two key properties:\n- An increasing predictable process has jumps only at predictable stopping times.\n- Jump times of a Poisson process are totally inaccessible stopping times.\nThe process $N_t$ is an increasing process. Its jumps occur at the arrival times of the Poisson process. These arrival times are the jump times of $N_t$. According to the given information, these jump times are totally inaccessible stopping times. By definition, a totally inaccessible stopping time is not a predictable stopping time.\nIf $(N_{t})_{t\\geq 0}$ were a predictable process, being an increasing process, its jumps could only occur at predictable stopping times. But the jumps of $N_t$ occur at totally inaccessible times. This is a contradiction. Therefore, $(N_{t})_{t\\geq 0}$ is not a predictable process.\n\n4.  **Using only the defining properties above, determine the predictable quadratic variation $\\langle M\\rangle_{t}$ explicitly.**\n\nThe predictable quadratic variation $\\langle M \\rangle_{t}$ is defined as the unique predictable, increasing process starting at $0$ such that $M^{2}-\\langle M \\rangle$ is a local martingale.\nWe seek a process $A_t$ that is:\na) Predictable.\nb) Increasing and starting at $A_0=0$.\nc) Such that $M_t^2 - A_t$ is a local martingale.\nBy the uniqueness property, if we find such an $A_t$, then we must have $\\langle M \\rangle_t = A_t$.\n\nLet's test the candidate process $A_t = t$.\na) The process $A_t = t$ is a deterministic and continuous function of time. Any continuous adapted process is predictable. Hence, $A_t=t$ is predictable.\nb) The function $t \\mapsto t$ is clearly increasing for $t \\ge 0$ and starts at $A_0=0$.\nc) We must verify that $Z_t \\coloneqq M_t^2 - t$ is a local martingale. We will show it is a true martingale.\nFirst, integrability:\n$$\n\\mathbb{E}[|M_t^2-t|] \\leq \\mathbb{E}[M_t^2] + t\n$$\nSince $M_t$ is a martingale with $\\mathbb{E}[M_t] = \\mathbb{E}[M_0] = 0$ (as $N_0=0$), its variance is $\\text{Var}(M_t) = \\mathbb{E}[M_t^2] - (\\mathbb{E}[M_t])^2 = \\mathbb{E}[M_t^2]$. Also, $\\text{Var}(M_t) = \\text{Var}(N_t-t) = \\text{Var}(N_t) = t$. So, $\\mathbb{E}[M_t^2] = t$.\nThe integrability is confirmed: $\\mathbb{E}[|M_t^2-t|] \\leq t+t=2t < \\infty$.\nNext, we check the martingale property for $s \\leq t$:\n$$\n\\mathbb{E}[Z_t \\mid \\mathcal{F}_s] = \\mathbb{E}[M_t^2 - t \\mid \\mathcal{F}_s] = \\mathbb{E}[M_t^2 \\mid \\mathcal{F}_s] - t\n$$\nSince $M_t$ is a martingale with independent increments, $M_t-M_s$ is independent of $\\mathcal{F}_s$ and has mean $0$.\n\\begin{align*}\n\\mathbb{E}[M_t^2 \\mid \\mathcal{F}_s] &= \\mathbb{E}[(M_s + (M_t-M_s))^2 \\mid \\mathcal{F}_s] \\\\\n&= \\mathbb{E}[M_s^2 + 2M_s(M_t-M_s) + (M_t-M_s)^2 \\mid \\mathcal{F}_s] \\\\\n&= M_s^2 + 2M_s \\mathbb{E}[M_t-M_s \\mid \\mathcal{F}_s] + \\mathbb{E}[(M_t-M_s)^2 \\mid \\mathcal{F}_s] \\\\\n&= M_s^2 + 2M_s \\cdot 0 + \\mathbb{E}[(M_t-M_s)^2] \\quad \\text{(due to independence)} \\\\\n&= M_s^2 + \\text{Var}(M_t-M_s)\n\\end{align*}\nThe increment $M_t-M_s = (N_t-t)-(N_s-s) = (N_t-N_s)-(t-s)$ involves $N_t-N_s$, which has a Poisson distribution with parameter $t-s$.\n$$\n\\text{Var}(M_t-M_s) = \\text{Var}((N_t-N_s)-(t-s)) = \\text{Var}(N_t-N_s) = t-s\n$$\nSo, $\\mathbb{E}[M_t^2 \\mid \\mathcal{F}_s] = M_s^2 + t-s$.\nSubstituting this into the main expression:\n$$\n\\mathbb{E}[Z_t \\mid \\mathcal{F}_s] = (M_s^2 + t-s) - t = M_s^2 - s = Z_s\n$$\nThis shows that $M_t^2 - t$ is a martingale, and thus a local martingale.\nThe process $A_t=t$ satisfies all the conditions of the definition of the predictable quadratic variation. By uniqueness, we conclude that $\\langle M \\rangle_t=t$.", "answer": "$$\\boxed{t}$$", "id": "2990977"}, {"introduction": "Building on the concept of the compensator, we now extend our analysis from a single process to the interaction between two martingales. This practice problem first asks you to formalize the relationship between optional covariation $[M,N]$ and its compensator, the predictable covariation $\\langle M,N \\rangle$ [@problem_id:2990973]. You will then apply this by computing $\\langle M,N \\rangle$ for martingales driven by independent Brownian and Poisson sources, revealing the fundamental concept of stochastic orthogonality.", "problem": "Let $(\\Omega,\\mathcal{F},(\\mathcal{F}_{t})_{t\\geq 0},\\mathbb{P})$ be a filtered probability space satisfying the usual conditions. A process is called predictable if it is measurable with respect to the predictable $\\sigma$-algebra and progressively measurable if its restriction to $[0,t]\\times\\Omega$ is measurable with respect to the product of the Borel $\\sigma$-algebra on $[0,t]$ and $\\mathcal{F}_{t}$ for each $t\\geq 0$. A square-integrable martingale is an $(\\mathcal{F}_{t})$-martingale $M$ with $\\mathbb{E}[M_{t}^{2}]<\\infty$ for all $t\\geq 0$. The optional covariation $[M,N]$ of two square-integrable martingales $M$ and $N$ is defined as an adapted process of finite variation null at $0$ such that $M N - [M,N]$ is a square-integrable martingale. The predictable covariation $\\langle M,N\\rangle$ is defined as a predictable process of finite variation null at $0$ characterized by the property that $M N - \\langle M,N\\rangle$ is a square-integrable martingale. Starting only from the core definitions above together with the Doob–Meyer decomposition for submartingales and the existence of the dual predictable projection (also called compensator), do the following:\n\n1. Show that for any two square-integrable martingales $M$ and $N$, the optional covariation $[M,N]$ exists and is uniquely characterized by the property that $M N - [M,N]$ is a square-integrable martingale.\n\n2. Show that there exists a unique predictable covariation $\\langle M,N\\rangle$, identify it as the compensator of the optional covariation $[M,N]$, and deduce that $[M,N]-\\langle M,N\\rangle$ is a square-integrable martingale.\n\n3. Compute $\\langle M,N\\rangle_{t}$ explicitly in the following orthogonal case: Let $W$ be a standard Brownian motion and let $J$ be a Poisson process of rate $\\lambda>0$ independent of $W$. Let $H$ be a bounded predictable process and define\n$$\nM_{t} \\coloneqq \\int_{0}^{t} H_{s}\\,\\mathrm{d}W_{s},\\qquad N_{t} \\coloneqq J_{t}-\\lambda t,\\qquad t\\geq 0.\n$$\nVerify that $M$ and $N$ are square-integrable martingales, determine $[M,N]_{t}$, and compute the closed-form analytic expression for $\\langle M,N\\rangle_{t}$ for all $t\\geq 0$.\n\nExpress your final answer as the expression for $\\langle M,N\\rangle_{t}$. No rounding or physical units are required.", "solution": "The problem is a valid exercise in stochastic calculus, divided into three parts: proving the existence and uniqueness of the optional covariation, doing the same for the predictable covariation and relating the two, and finally computing a specific predictable covariation.\n\n### Part 1: Existence and Uniqueness of Optional Covariation $[M,N]$\n\nLet $M$ and $N$ be two square-integrable martingales. We first establish the existence and uniqueness of the quadratic variation $[M,M]$ for a single square-integrable martingale $M$, and then extend the result to the covariation $[M,N]$ using the polarization identity.\n\nConsider the process $M^2 = (M_t^2)_{t \\geq 0}$. Since $M$ is a square-integrable martingale, $\\mathbb{E}[M_t^2] < \\infty$ for all $t \\geq 0$. For any $s < t$, consider the conditional expectation of $M_t^2$ given $\\mathcal{F}_s$:\n$$\n\\mathbb{E}[M_t^2 | \\mathcal{F}_s] = \\mathbb{E}[(M_t - M_s + M_s)^2 | \\mathcal{F}_s] = \\mathbb{E}[(M_t - M_s)^2 + 2M_s(M_t - M_s) + M_s^2 | \\mathcal{F}_s]\n$$\nUsing the linearity of conditional expectation and the fact that $M_s$ is $\\mathcal{F}_s$-measurable:\n$$\n\\mathbb{E}[M_t^2 | \\mathcal{F}_s] = \\mathbb{E}[(M_t - M_s)^2 | \\mathcal{F}_s] + 2M_s \\mathbb{E}[M_t - M_s | \\mathcal{F}_s] + M_s^2\n$$\nSince $M$ is a martingale, $\\mathbb{E}[M_t - M_s | \\mathcal{F}_s] = 0$. The term $\\mathbb{E}[(M_t - M_s)^2 | \\mathcal{F}_s]$ is non-negative. Thus,\n$$\n\\mathbb{E}[M_t^2 | \\mathcal{F}_s] = M_s^2 + \\mathbb{E}[(M_t - M_s)^2 | \\mathcal{F}_s] \\geq M_s^2\n$$\nThis shows that $M^2$ is a submartingale. As the filtration $(\\mathcal{F}_t)_{t \\geq 0}$ satisfies the usual conditions, $M$ has a modification with right-continuous paths, and thus $M^2$ is a right-continuous submartingale.\n\nThe standard Doob decomposition theorem states that the submartingale $M^2$ has a unique decomposition $M_t^2 = K_t + A_t$, where $K_t$ is a local martingale and $A_t$ is an increasing, adapted, right-continuous process with $A_0=0$. This process $A_t$ is defined as the optional quadratic variation, so we set $[M,M]_t \\coloneqq A_t$. For a square-integrable martingale $M$, it is a fundamental result that the local martingale $K_t = M^2_t - [M,M]_t$ is a true square-integrable martingale. Thus, we have found a process $[M,M]_t$ that is adapted, of finite variation (since it is increasing), null at 0, and for which $M^2 - [M,M]$ is a square-integrable martingale. This establishes existence.\n\nFor uniqueness, suppose there is another process, $[M,M]'$, satisfying the same properties. Then $M^2 - [M,M]$ and $M^2 - [M,M]'$ are both square-integrable martingales. Their difference, $(M^2 - [M,M]') - (M^2 - [M,M]) = [M,M] - [M,M]'$, must also be a martingale. However, $[M,M] - [M,M]'$ is a process of finite variation, being the difference of two such processes, and it is null at $0$. A martingale of finite variation that is null at $0$ must be identically zero. Therefore, $[M,M]_t - [M,M]'_t = 0$ for all $t\\ge 0$, which proves uniqueness for $[M,M]$.\n\nNow, for two square-integrable martingales $M$ and $N$, we define the optional covariation $[M,N]$ via the polarization identity:\n$$\n[M,N]_t \\coloneqq \\frac{1}{2}\\left( [M+N, M+N]_t - [M,M]_t - [N,N]_t \\right)\n$$\nSince $M$ and $N$ are square-integrable martingales, so is their sum $M+N$. From the preceding argument, the processes $[M+N, M+N]$, $[M,M]$, and $[N,N]$ exist and are unique. Therefore, $[M,N]$ as defined above exists and is a unique process. It is a linear combination of adapted processes of finite variation, so it is itself an adapted process of finite variation, null at $0$.\n\nWe must verify that $MN - [M,N]$ is a square-integrable martingale. Using the polarization identity for products, $M_t N_t = \\frac{1}{2}((M_t+N_t)^2 - M_t^2 - N_t^2)$, we have:\n$$\nM_t N_t - [M,N]_t = \\frac{1}{2} \\left( ((M_t+N_t)^2 - [M+N,M+N]_t) - (M_t^2 - [M,M]_t) - (N_t^2 - [N,N]_t) \\right)\n$$\nEach term in the parentheses on the right-hand side is a square-integrable martingale by construction. A linear combination of martingales is a martingale, so $MN - [M,N]$ is a martingale. This completes the proof of Part 1.\n\n### Part 2: Predictable Covariation $\\langle M,N \\rangle$\n\nThe optional covariation $[M,N]$ is an adapted process of finite variation. The theory of compensators (or dual predictable projections) states that for any such process $A$, there exists a unique predictable process of finite variation $A^p$, called the compensator of $A$, such that $A - A^p$ is a martingale.\n\nLet us apply this to the process $[M,N]$. There exists a unique predictable process of finite variation, which we denote by $\\langle M,N \\rangle$, such that $[M,N] - \\langle M,N \\rangle$ is a martingale. Since $[M,N]_0 = 0$ and the martingale part must have expectation zero, it must be that $\\langle M,N \\rangle_0 = 0$. This establishes the existence of a process $\\langle M,N \\rangle$ with the properties of being predictable, of finite variation, and null at $0$.\n\nWe now verify that it satisfies the defining property from the problem statement. Consider the process $MN - \\langle M,N \\rangle$:\n$$\nM_t N_t - \\langle M,N \\rangle_t = (M_t N_t - [M,N]_t) + ([M,N]_t - \\langle M,N \\rangle_t)\n$$\nFrom Part 1, we know that $M_t N_t - [M,N]_t$ is a square-integrable martingale. By definition of $\\langle M,N \\rangle_t$ as the compensator of $[M,N]_t$, the process $[M,N]_t - \\langle M,N \\rangle_t$ is a martingale. The sum of two martingales is a martingale. Thus, $M_t N_t - \\langle M,N \\rangle_t$ is a martingale (and it can be shown to be square-integrable). This proves the existence of the predictable covariation.\n\nFor uniqueness, suppose there exists another process, $\\langle M,N \\rangle'$, satisfying the same properties. Then $MN - \\langle M,N \\rangle$ and $MN - \\langle M,N \\rangle'$ are both martingales. Their difference, $\\langle M,N \\rangle' - \\langle M,N \\rangle$, is also a martingale. This difference is a predictable process of finite variation, null at $0$. A predictable martingale of finite variation starting at $0$ must be identically zero. Therefore, $\\langle M,N \\rangle'_t = \\langle M,N \\rangle_t$ for all $t\\ge 0$, which proves uniqueness.\n\nThe identification of $\\langle M,N \\rangle_t$ as the compensator of $[M,N]_t$ is now clear. By its very construction, $[M,N]_t - \\langle M,N \\rangle_t$ is a square-integrable martingale. This completes Part 2.\n\n### Part 3: Explicit Computation\n\nWe are given $M_t = \\int_0^t H_s\\,\\mathrm{d}W_s$ and $N_t = J_t - \\lambda t$.\n\nFirst, we verify they are square-integrable martingales.\nFor $M_t$: The process $H$ is predictable and bounded, so there is a constant $C>0$ such that $|H_s(\\omega)| \\le C$ for all $(s, \\omega)$. The process $M_t$ is an Itô integral with respect to a standard Brownian motion. It is a martingale because the integrand satisfies $\\mathbb{E}[\\int_0^t H_s^2\\,\\mathrm{d}s] < \\infty$. Indeed, $\\int_0^t \\mathbb{E}[H_s^2]\\,\\mathrm{d}s \\le \\int_0^t C^2\\,\\mathrm{d}s = C^2 t < \\infty$. For square-integrability, we use Itô's isometry:\n$$\n\\mathbb{E}[M_t^2] = \\mathbb{E}\\left[ \\left(\\int_0^t H_s\\,\\mathrm{d}W_s\\right)^2 \\right] = \\mathbb{E}\\left[ \\int_0^t H_s^2\\,\\mathrm{d}s \\right] \\le C^2 t < \\infty\n$$\nSo, $M$ is a square-integrable martingale.\n\nFor $N_t$: The process $J_t$ is a Poisson process with rate $\\lambda$, so $\\mathbb{E}[J_t] = \\lambda t$ and $\\text{Var}(J_t) = \\lambda t$. It is a known result that the compensated process $N_t = J_t - \\lambda t$ is a martingale. For square-integrability, we compute its second moment:\n$$\n\\mathbb{E}[N_t^2] = \\mathbb{E}[(J_t - \\lambda t)^2] = \\text{Var}(J_t) = \\lambda t < \\infty\n$$\nSo, $N$ is a square-integrable martingale.\n\nNext, we determine the optional covariation $[M,N]_t$. The general formula for the optional covariation is $[X,Y]_t = [X^c, Y^c]_t + \\sum_{0<s\\le t} \\Delta X_s \\Delta Y_s$, where $X^c$ is the continuous martingale part of $X$, and $\\Delta X_s = X_s - X_{s-}$ is the jump at time $s$.\nThe martingale $M_t$ is an integral with respect to the continuous process $W_t$, hence $M_t$ is a continuous martingale. This means its continuous part is itself, $M^c_t = M_t$, and it has no jumps, $\\Delta M_s = 0$ for all $s \\ge 0$.\nThe martingale $N_t = J_t - \\lambda t$ is a pure jump process. Its continuous martingale part is zero, $N^c_t = 0$. Its jumps coincide with the jumps of the Poisson process $J_t$.\n\nApplying the formula for $[M,N]_t$:\nThe continuous part is $[M^c, N^c]_t = [M, 0]_t = 0$.\nThe jump part is $\\sum_{0<s\\le t} \\Delta M_s \\Delta N_s = \\sum_{0<s\\le t} 0 \\cdot \\Delta N_s = 0$.\nTherefore, the optional covariation is identically zero:\n$$\n[M,N]_t = 0 \\quad \\text{for all } t \\ge 0.\n$$\nFinally, we compute the predictable covariation $\\langle M,N \\rangle_t$. From Part 2, $\\langle M,N \\rangle_t$ is the unique predictable process of finite variation, null at $0$, such that $[M,N]_t - \\langle M,N \\rangle_t$ is a martingale.\nWe found that $[M,N]_t = 0$. The zero process is predictable, of finite variation, and null at $0$. Let's test if $\\langle M,N \\rangle_t = 0$ works. We need to check if $[M,N]_t - 0 = 0 - 0 = 0$ is a martingale. The zero process is trivially a martingale. By uniqueness of the predictable covariation, this must be the correct one.\n\nAlternatively, the predictable compensator of a predictable process is the process itself. Since $[M,N]_t=0$ is predictable, its compensator is $\\langle M,N\\rangle_t = [M,N]_t = 0$.\n\nAnother way to verify this is to show that the product $M_t N_t$ is a martingale, which implies its predictable compensator $\\langle M,N \\rangle_t$ must be zero. For $s < t$:\n$$\n\\mathbb{E}[M_t N_t | \\mathcal{F}_s] = \\mathbb{E}[(M_s + M_t - M_s)(N_s + N_t - N_s) | \\mathcal{F}_s]\n$$\nExpanding this and using the martingale properties of $M$ and $N$ yields:\n$$\n\\mathbb{E}[M_t N_t | \\mathcal{F}_s] = M_s N_s + \\mathbb{E}[(M_t - M_s)(N_t - N_s) | \\mathcal{F}_s]\n$$\nLet's evaluate the last term. Let $\\mathcal{F}_t^W = \\sigma(W_u, u \\le t)$ and $\\mathcal{F}_t^J = \\sigma(J_u, u \\le t)$. The full filtration is $\\mathcal{F}_t = \\mathcal{F}_t^W \\vee \\mathcal{F}_t^J$ (formally, the completion of this). The increment $N_t - N_s$ depends on the Poisson process over $(s,t]$ and is thus independent of $\\mathcal{F}_t^W \\vee \\mathcal{F}_s^J$. Using the tower property of conditional expectation:\n$$\n\\mathbb{E}[(M_t - M_s)(N_t - N_s) | \\mathcal{F}_s] = \\mathbb{E}\\left[ \\mathbb{E}[(M_t - M_s)(N_t - N_s) | \\mathcal{F}_t^W \\vee \\mathcal{F}_s^J] \\,\\middle|\\, \\mathcal{F}_s \\right]\n$$\nThe term $M_t - M_s = \\int_s^t H_u dW_u$ is measurable with respect to $\\mathcal{F}_t^W \\vee \\mathcal{F}_t^J$. So,\n$$\n\\mathbb{E}[(M_t - M_s)(N_t - N_s) | \\mathcal{F}_t^W \\vee \\mathcal{F}_s^J] = (M_t - M_s) \\mathbb{E}[N_t - N_s | \\mathcal{F}_t^W \\vee \\mathcal{F}_s^J]\n$$\nDue to the independence assumptions, $\\mathbb{E}[N_t - N_s | \\mathcal{F}_t^W \\vee \\mathcal{F}_s^J] = \\mathbb{E}[N_t - N_s] = 0$.\nThe inner expectation is zero, so the entire expression is zero. Hence, $\\mathbb{E}[M_t N_t | \\mathcal{F}_s] = M_s N_s$, confirming that $M_t N_t$ is a martingale.\nSince $M_tN_t$ is a martingale, $M_tN_t - 0$ is a martingale. The process $\\langle M,N\\rangle_t = 0$ is predictable, has finite variation, and is null at $0$. By the uniqueness of the predictable covariation, we must have $\\langle M,N\\rangle_t = 0$.\n\nThe final expression for $\\langle M,N\\rangle_t$ is $0$.", "answer": "$$\\boxed{0}$$", "id": "2990973"}]}