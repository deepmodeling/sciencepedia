{"hands_on_practices": [{"introduction": "Brownian motion serves as a cornerstone model for continuous-time Gaussian processes, with applications ranging from financial mathematics to statistical physics. A deep understanding of its path properties is essential, particularly the behavior of its extrema. This exercise leverages the elegant reflection principle to derive the exact distribution of the running maximum of a standard Brownian motion, a foundational result that showcases the powerful symmetries inherent in the process. [@problem_id:2978015]", "problem": "Let $\\{W_{t}\\}_{t \\ge 0}$ be a standard Brownian motion, that is, a zero-mean Gaussian process with continuous paths, stationary independent increments, $W_{0} = 0$, and variance $\\mathbb{V}\\mathrm{ar}(W_{t}) = t$. For a fixed horizon $T > 0$, define the running maximum $M_{T} := \\sup_{0 \\le t \\le T} W_{t}$. Work from first principles of Gaussian processes, the reflection principle for Brownian motion, and basic properties of the normal distribution.\n\na) Using the reflection principle, derive the distribution function of $M_{T}$ for $a \\ge 0$ and then compute the expectation $\\mathbb{E}[M_{T}]$ in closed form as a function of $T$.\n\nb) Using the reflection principle together with the exponential Markov inequality and the moment generating function (MGF) of a Gaussian random variable, derive a nonasymptotic upper bound of the form $\\mathbb{P}(M_{T} \\ge a) \\le C \\exp\\!\\big(-a^{2}/(2T)\\big)$ that holds for all $a \\ge 0$. Identify the smallest constant $C$ that your method yields and provide the final bound explicitly as a function of $a$ and $T$.\n\nReport your final results as two expressions: first $\\mathbb{E}[M_{T}]$ and second your explicit tail bound $\\mathbb{P}(M_{T} \\ge a)$ upper bound. No numerical approximation is required.", "solution": "The problem asks for properties of the running maximum of a standard Brownian motion, $M_{T} = \\sup_{0 \\le t \\le T} W_{t}$. We will address the two parts of the problem sequentially.\n\na) Derivation of the distribution and expectation of $M_{T}$.\n\nThe first step is to find the cumulative distribution function (CDF) of $M_T$, denoted $F_{M_T}(a) = \\mathbb{P}(M_{T} \\le a)$. For $a < 0$, since $W_0=0$ and the paths are continuous, the supremum $M_T$ must be non-negative, so $\\mathbb{P}(M_T \\le a) = 0$. We focus on the case $a \\ge 0$. It is more convenient to first compute the tail probability, $\\mathbb{P}(M_T > a)$. Since $M_T$ is a continuous random variable, this is the same as $\\mathbb{P}(M_T \\ge a)$.\n\nThe event $\\{M_T \\ge a\\}$ for $a > 0$ is equivalent to the event that the Brownian motion reaches the level $a$ at some time $t \\in [0, T]$. Let $\\tau_a = \\inf\\{t \\ge 0 : W_t = a\\}$ be the first-passage time to level $a$. Then $\\{M_T \\ge a\\} = \\{\\tau_a \\le T\\}$.\n\nWe use the reflection principle for Brownian motion. The event $\\{\\tau_a \\le T\\}$ can be partitioned into two disjoint events: $\\{\\tau_a \\le T \\text{ and } W_T \\ge a\\}$ and $\\{\\tau_a \\le T \\text{ and } W_T < a\\}$.\nBy the continuity of paths, the event $\\{W_T \\ge a\\}$ necessitates that the path must have crossed level $a$ at some point, so $\\{W_T \\ge a\\} \\subseteq \\{\\tau_a \\le T\\}$. Therefore, the first event is simply $\\{W_T \\ge a\\}$.\nSo, we have:\n$$\n\\mathbb{P}(M_T \\ge a) = \\mathbb{P}(\\tau_a \\le T) = \\mathbb{P}(W_T \\ge a) + \\mathbb{P}(\\tau_a \\le T, W_T < a)\n$$\nThe reflection principle states that, conditioned on hitting level $a$, the process is equally likely to end up above or below $a$ due to the symmetry of its increments. More formally, by the strong Markov property at time $\\tau_a$, the process $W'_{s} = W_{\\tau_a+s} - W_{\\tau_a}$ is a standard Brownian motion independent of the pre-$\\tau_a$ sigma-algebra. Since $W'_{s}$ is symmetric about $0$, for any time $u > 0$, $\\mathbb{P}(W'_{u} < 0) = \\mathbb{P}(W'_{u} > 0) = 1/2$. Applying this to our problem:\n$$\n\\mathbb{P}(\\tau_a \\le T, W_T < a) = \\mathbb{P}(\\tau_a \\le T, W_T - W_{\\tau_a} < 0)\n$$\nBy symmetry of the increments of the process starting at $\\tau_a$:\n$$\n\\mathbb{P}(\\tau_a \\le T, W_T - W_{\\tau_a} < 0) = \\mathbb{P}(\\tau_a \\le T, W_T - W_{\\tau_a} > 0) = \\mathbb{P}(\\tau_a \\le T, W_T > a)\n$$\nAs established before, $\\{W_T > a\\}$ implies $\\{\\tau_a \\le T\\}$, so the event $\\{\\tau_a \\le T, W_T > a\\}$ is the same as $\\{W_T > a\\}$. Thus,\n$$\n\\mathbb{P}(\\tau_a \\le T, W_T < a) = \\mathbb{P}(W_T > a)\n$$\nSubstituting this back, and using that $\\mathbb{P}(W_T=a)=0$ for the continuous random variable $W_T$, we get:\n$$\n\\mathbb{P}(M_T \\ge a) = \\mathbb{P}(W_T \\ge a) + \\mathbb{P}(W_T > a) = 2 \\mathbb{P}(W_T \\ge a)\n$$\nThis result is valid for $a > 0$. For $a=0$, $\\mathbb{P}(M_T \\ge 0)=1$ and $2\\mathbb{P}(W_T \\ge 0) = 2(1/2) = 1$, so the formula holds for all $a \\ge 0$.\n\nA standard Brownian motion $W_t$ is a Gaussian process with $W_T \\sim \\mathcal{N}(0, T)$. Let $Z \\sim \\mathcal{N}(0, 1)$ be a standard normal random variable. Then $W_T$ has the same distribution as $\\sqrt{T}Z$.\n$$\n\\mathbb{P}(W_T \\ge a) = \\mathbb{P}(\\sqrt{T}Z \\ge a) = \\mathbb{P}(Z \\ge a/\\sqrt{T}) = 1 - \\Phi(a/\\sqrt{T})\n$$\nwhere $\\Phi(\\cdot)$ is the CDF of the standard normal distribution.\nThe tail probability of $M_T$ is therefore:\n$$\n\\mathbb{P}(M_T \\ge a) = 2(1 - \\Phi(a/\\sqrt{T})) \\quad \\text{for } a \\ge 0\n$$\nThe CDF of $M_T$ is $F_{M_T}(a) = \\mathbb{P}(M_T \\le a) = 1 - \\mathbb{P}(M_T > a) = 1 - \\mathbb{P}(M_T \\ge a)$.\n$$\nF_{M_T}(a) = 1 - 2(1 - \\Phi(a/\\sqrt{T})) = 2\\Phi(a/\\sqrt{T}) - 1, \\quad \\text{for } a \\ge 0\n$$\nTo compute the expectation $\\mathbb{E}[M_T]$, we first find the probability density function (PDF) $f_{M_T}(a)$ by differentiating the CDF for $a > 0$. Let $\\phi(\\cdot)$ be the PDF of the standard normal distribution.\n$$\nf_{M_T}(a) = \\frac{d}{da} F_{M_T}(a) = \\frac{d}{da} (2\\Phi(a/\\sqrt{T}) - 1) = 2\\phi(a/\\sqrt{T}) \\cdot \\frac{1}{\\sqrt{T}}\n$$\nSubstituting the expression for $\\phi(z) = \\frac{1}{\\sqrt{2\\pi}}\\exp(-z^2/2)$:\n$$\nf_{M_T}(a) = \\frac{2}{\\sqrt{T}} \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{(a/\\sqrt{T})^2}{2}\\right) = \\frac{2}{\\sqrt{2\\pi T}} \\exp\\left(-\\frac{a^2}{2T}\\right), \\quad \\text{for } a \\ge 0\n$$\nand $f_{M_T}(a)=0$ for $a < 0$. This is the PDF of a folded normal distribution.\nThe expectation is calculated by integrating $a \\cdot f_{M_T}(a)$ over its support:\n$$\n\\mathbb{E}[M_T] = \\int_0^\\infty a f_{M_T}(a) da = \\int_0^\\infty a \\frac{2}{\\sqrt{2\\pi T}} \\exp\\left(-\\frac{a^2}{2T}\\right) da\n$$\nWe perform a substitution. Let $u = a^2/(2T)$. Then $du = (2a)/(2T) da = a/T da$, which implies $a da = T du$. The limits of integration remain $0$ and $\\infty$.\n$$\n\\mathbb{E}[M_T] = \\frac{2}{\\sqrt{2\\pi T}} \\int_0^\\infty \\exp(-u) (T du) = \\frac{2T}{\\sqrt{2\\pi T}} \\int_0^\\infty \\exp(-u) du\n$$\nThe integral $\\int_0^\\infty \\exp(-u) du$ is the Gamma function $\\Gamma(1)$, which equals $1$.\n$$\n\\mathbb{E}[M_T] = \\frac{2T}{\\sqrt{2\\pi T}} = \\frac{2\\sqrt{T}}{\\sqrt{2\\pi}} = \\sqrt{\\frac{4T}{2\\pi}} = \\sqrt{\\frac{2T}{\\pi}}\n$$\n\nb) Derivation of a tail bound for $M_T$.\n\nWe are asked to derive an upper bound for $\\mathbb{P}(M_{T} \\ge a)$ of the form $C \\exp(-a^{2}/(2T))$ for some constant $C$. The derivation must use the reflection principle, the exponential Markov inequality, and the MGF of a Gaussian random variable.\n\nFrom part (a), the reflection principle gives the exact identity for $a \\ge 0$:\n$$\n\\mathbb{P}(M_T \\ge a) = 2\\mathbb{P}(W_T \\ge a)\n$$\nNow we derive an upper bound for the Gaussian tail probability $\\mathbb{P}(W_T \\ge a)$ using the specified tools.\nThe exponential Markov inequality (also known as a Chernoff bound) states that for a random variable $X$, any $a \\in \\mathbb{R}$, and any $\\lambda > 0$:\n$$\n\\mathbb{P}(X \\ge a) = \\mathbb{P}(\\exp(\\lambda X) \\ge \\exp(\\lambda a)) \\le \\frac{\\mathbb{E}[\\exp(\\lambda X)]}{\\exp(\\lambda a)} = \\exp(-\\lambda a) \\mathbb{E}[\\exp(\\lambda X)]\n$$\nWe apply this to $X = W_T$. The random variable $W_T$ follows a normal distribution $\\mathcal{N}(0, T)$. The moment generating function (MGF) of a general Gaussian variable $Y \\sim \\mathcal{N}(\\mu, \\sigma^2)$ is $M_Y(\\lambda) = \\mathbb{E}[\\exp(\\lambda Y)] = \\exp(\\mu\\lambda + \\frac{1}{2}\\sigma^2\\lambda^2)$. For $W_T$, we have $\\mu=0$ and $\\sigma^2=T$, so its MGF is:\n$$\n\\mathbb{E}[\\exp(\\lambda W_T)] = \\exp\\left(\\frac{1}{2}T\\lambda^2\\right)\n$$\nSubstituting the MGF into the Chernoff bound for $W_T$:\n$$\n\\mathbb{P}(W_T \\ge a) \\le \\exp(-\\lambda a) \\exp\\left(\\frac{1}{2}T\\lambda^2\\right) = \\exp\\left(-\\lambda a + \\frac{1}{2}T\\lambda^2\\right)\n$$\nThis inequality holds for any $\\lambda > 0$. To obtain the tightest possible bound from this method, we minimize the expression in the exponent with respect to $\\lambda$. Let $g(\\lambda) = -\\lambda a + \\frac{1}{2}T\\lambda^2$. The minimum is found by setting the derivative to zero:\n$$\ng'(\\lambda) = -a + T\\lambda = 0 \\implies \\lambda = \\frac{a}{T}\n$$\nSince the problem is for $a \\ge 0$, we have $\\lambda \\ge 0$. We need $\\lambda > 0$, so this choice is valid for $a > 0$.\nSubstituting this optimal value of $\\lambda$ back into the bound:\n$$\n\\mathbb{P}(W_T \\ge a) \\le \\exp\\left(-\\left(\\frac{a}{T}\\right)a + \\frac{1}{2}T\\left(\\frac{a}{T}\\right)^2\\right) = \\exp\\left(-\\frac{a^2}{T} + \\frac{a^2}{2T}\\right) = \\exp\\left(-\\frac{a^2}{2T}\\right)\n$$\nThis provides the desired exponential bound for the tail of the Gaussian variable $W_T$.\n\nFinally, we combine this result with the identity from the reflection principle:\n$$\n\\mathbb{P}(M_T \\ge a) = 2 \\mathbb{P}(W_T \\ge a) \\le 2 \\exp\\left(-\\frac{a^2}{2T}\\right)\n$$\nThis bound is of the form $\\mathbb{P}(M_{T} \\ge a) \\le C \\exp(-a^{2}/(2T))$. The method described (reflection principle followed by Chernoff bound on the resulting Gaussian tail) directly yields the constant $C=2$. This is the smallest constant that this specific derivation produces. The resulting upper bound is valid for all $a \\ge 0$.\nThe final explicit bound is $\\mathbb{P}(M_T \\ge a) \\le 2 \\exp(-a^2/(2T))$.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\sqrt{\\frac{2T}{\\pi}} & \\mathbb{P}(M_{T} \\ge a) \\le 2 \\exp\\left(-\\frac{a^{2}}{2T}\\right)\n\\end{pmatrix}\n}\n$$", "id": "2978015"}, {"introduction": "In contrast to the continuous paths of Gaussian processes like Brownian motion, the Poisson process provides the canonical model for counting discrete events in continuous time. Its sample paths are characterized by instantaneous jumps, and a full characterization requires understanding not just how many events occur, but when they occur. This practice reinforces first principles by connecting the number of events $N_T$ to the underlying jump times, culminating in the classic derivation of the conditional distribution of these times. [@problem_id:2977994]", "problem": "Let $\\{N_{t}\\}_{t \\in [0,T]}$ be a homogeneous Poisson counting process with rate $\\lambda > 0$ on a probability space $(\\Omega,\\mathcal{F},\\mathbb{P})$, with $N_{0}=0$, stationary and independent increments, and sample paths that are right-continuous with left limits. For $k \\in \\mathbb{N}$, define the jump times $\\tau_{k}$ by $\\tau_{1} := \\inf\\{t \\in [0,T] : N_{t} \\ge 1\\}$ and, inductively, $\\tau_{k} := \\inf\\{t \\in [0,T] : N_{t} \\ge k\\}$, whenever the set is nonempty, and let $M_{T} := \\sup_{0 \\le t \\le T} N_{t}$ denote the pathwise maximum of the counting process over the interval $[0,T]$.\n\nStarting from the core definitions of a homogeneous Poisson counting process and the properties of stationary and independent increments:\n\n1. Justify rigorously, using only these foundational properties, that the sample paths of $\\{N_{t}\\}_{t \\in [0,T]}$ are nondecreasing and piecewise constant with unit upward jumps at the jump times $\\{\\tau_{k}\\}$, and deduce that $M_{T} = N_{T}$ almost surely.\n\n2. Using the above, determine the distribution of $M_{T}$ and provide a closed-form expression for the cumulative distribution function, explicitly defined as the cumulative distribution function (CDF), $\\mathbb{P}(M_{T} \\le m)$, for any integer $m \\ge 0$.\n\n3. Discuss the ordering of jump times: derive the conditional joint distribution of the ordered jump times $(\\tau_{1},\\dots,\\tau_{n})$ given that $N_{T} = n$, and identify the corresponding joint probability density function on the simplex $\\{(t_{1},\\dots,t_{n}) \\in (0,T)^{n} : 0 < t_{1} < \\cdots < t_{n} < T\\}$.\n\nYour final reported answer must be a single closed-form analytical expression for the CDF $\\mathbb{P}(M_{T} \\le m)$, expressed in terms of $\\lambda$, $T$, and $m$. No numerical evaluation or rounding is required.", "solution": "The problem asks for a three-part analysis of a homogeneous Poisson process $\\{N_{t}\\}_{t \\in [0,T]}$ with rate $\\lambda > 0$, starting with $N_{0}=0$.\n\n**1. Justification of Path Properties and the Relation $M_{T} = N_{T}$**\n\nA counting process, by its fundamental definition, tracks the cumulative number of discrete events over time. This implies that for any sample path, the function $t \\mapsto N_{t}$ must be nondecreasing. Formally, for any two time points $s < t$, the increment $N_{t} - N_{s}$ represents the number of events in the interval $(s, t]$. For a homogeneous Poisson process, this increment follows a Poisson distribution with parameter $\\lambda(t-s)$. Since the support of any Poisson distribution is the set of non-negative integers $\\mathbb{N}_{0} = \\{0, 1, 2, \\ldots\\}$, the probability of a negative increment is zero: $\\mathbb{P}(N_{t} - N_{s} < 0) = 0$. This holds for all $s < t$, which proves that the sample paths of $N_{t}$ are nondecreasing almost surely.\n\nNext, we demonstrate that the jumps in the sample paths are of unit size, almost surely. The property of independent increments allows us to analyze the number of events in a small interval $(t, t+\\delta t]$, which is given by the random variable $N_{t+\\delta t} - N_{t}$. This variable follows a Poisson distribution with mean $\\lambda \\delta t$. The probability of observing $k$ events in this interval is:\n$$\n\\mathbb{P}(N_{t+\\delta t} - N_{t} = k) = \\frac{(\\lambda \\delta t)^{k} \\exp(-\\lambda \\delta t)}{k!}\n$$\nFor a single jump ($k=1$), the probability is $\\mathbb{P}(N_{t+\\delta t} - N_{t} = 1) = \\lambda \\delta t \\exp(-\\lambda \\delta t)$. As $\\delta t \\to 0$, this is $\\lambda \\delta t + O((\\delta t)^{2})$.\nFor two or more jumps ($k \\ge 2$), the probability is:\n$$\n\\mathbb{P}(N_{t+\\delta t} - N_{t} \\ge 2) = \\sum_{k=2}^{\\infty} \\frac{(\\lambda \\delta t)^{k} \\exp(-\\lambda \\delta t)}{k!} = \\frac{(\\lambda \\delta t)^{2}}{2} + O((\\delta t)^{3})\n$$\nThe ratio of the probability of two or more jumps to the probability of one jump is:\n$$\n\\frac{\\mathbb{P}(N_{t+\\delta t} - N_{t} \\ge 2)}{\\mathbb{P}(N_{t+\\delta t} - N_{t} = 1)} = \\frac{\\frac{1}{2}(\\lambda \\delta t)^{2} + O((\\delta t)^{3})}{\\lambda \\delta t + O((\\delta t)^{2})} \\to 0 \\quad \\text{as} \\quad \\delta t \\to 0\n$$\nThis demonstrates that in any infinitesimal interval, the occurrence of more than one event is a negligible possibility. By extending this argument over the finite interval $[0, T]$, we conclude that the probability of a jump of size greater than one is zero. Since the process is also defined to have right-continuous paths with left limits (càdlàg), its paths must be piecewise constant, increasing by jumps of magnitude $1$ at the event times $\\{\\tau_{k}\\}$.\n\nThe variable $M_{T}$ is defined as the pathwise maximum, $M_{T} := \\sup_{0 \\le t \\le T} N_{t}$. As established, the sample paths $t \\mapsto N_{t}$ are nondecreasing. For any nondecreasing function $f(t)$ on a closed interval $[a,b]$, its supremum is achieved at the right endpoint, i.e., $\\sup_{t \\in [a,b]} f(t) = f(b)$. Applying this to our process on the interval $[0, T]$, for any given sample path $\\omega \\in \\Omega$, we have $\\sup_{0 \\le t \\le T} N_{t}(\\omega) = N_{T}(\\omega)$. This equality holds for all paths, so the random variables are equal, $M_{T} = N_{T}$, almost surely.\n\n**2. Distribution of $M_{T}$ and its Cumulative Distribution Function (CDF)**\n\nFrom Part 1, we concluded that $M_{T} = N_{T}$ almost surely. This implies that $M_{T}$ and $N_{T}$ have the same probability distribution. The problem thus reduces to finding the distribution of $N_{T}$.\nBy the definition of a homogeneous Poisson process with rate $\\lambda$, the number of events $N_{T}$ in the interval $[0, T]$ follows a Poisson distribution with parameter (mean) $\\lambda T$. The probability mass function (PMF) of $N_{T}$ is:\n$$\n\\mathbb{P}(N_{T} = k) = \\frac{(\\lambda T)^{k} \\exp(-\\lambda T)}{k!} \\quad \\text{for } k \\in \\{0, 1, 2, \\dots\\}\n$$\nThe problem requests the cumulative distribution function (CDF) of $M_{T}$, denoted $\\mathbb{P}(M_{T} \\le m)$ for an integer $m \\ge 0$. Using the equivalence $M_{T} = N_{T}$:\n$$\n\\mathbb{P}(M_{T} \\le m) = \\mathbb{P}(N_{T} \\le m)\n$$\nTo find this probability, we sum the PMF from $k=0$ to $m$:\n$$\n\\mathbb{P}(N_{T} \\le m) = \\sum_{k=0}^{m} \\mathbb{P}(N_{T} = k) = \\sum_{k=0}^{m} \\frac{(\\lambda T)^{k} \\exp(-\\lambda T)}{k!}\n$$\nFactoring out the term that does not depend on the summation index $k$, we obtain the closed-form expression for the CDF:\n$$\n\\mathbb{P}(M_{T} \\le m) = \\exp(-\\lambda T) \\sum_{k=0}^{m} \\frac{(\\lambda T)^{k}}{k!}\n$$\n\n**3. Conditional Joint Distribution of Jump Times**\n\nWe seek the conditional joint probability density function (PDF) of the ordered jump times $(\\tau_{1}, \\dots, \\tau_{n})$ given that exactly $n$ events occurred in $[0, T]$, i.e., $N_{T} = n$. The support for this distribution is the $n$-dimensional simplex $S_{n} = \\{(t_{1}, \\dots, t_{n}) \\in \\mathbb{R}^{n} : 0 < t_{1} < \\cdots < t_{n} < T\\}$.\n\nLet's consider the joint probability of having the jump times occur in infinitesimal intervals $[t_{i}, t_{i}+dt_{i}]$ and also that $N_{T}=n$. This event means: one jump occurs in $[t_{1}, t_{1}+dt_{1}]$, one in $[t_{2}, t_{2}+dt_{2}]$, ..., one in $[t_{n}, t_{n}+dt_{n}]$, and no other jumps occur in $[0, T]$.\nDue to the independent increments property, we can express this probability as a product:\n\\begin{align*}\n\\mathbb{P}(\\tau_{1} \\in [t_{1}, t_{1}+dt_{1}], \\dots, \\tau_{n} \\in [t_{n}, t_{n}+dt_{n}], N_{T}=n) &= \\mathbb{P}(\\text{no jump in } [0,t_{1})) \\cdot \\mathbb{P}(\\text{one jump in } [t_{1}, t_{1}+dt_{1}]) \\\\\n& \\quad \\cdot \\mathbb{P}(\\text{no jump in } (t_{1}+dt_1,t_{2})) \\cdot \\ldots \\cdot \\mathbb{P}(\\text{one jump in } [t_{n}, t_{n}+dt_{n}]) \\\\\n& \\quad \\cdot \\mathbb{P}(\\text{no jump in } (t_{n}+dt_n,T])\n\\end{align*}\nFor an interval of length $L$, $\\mathbb{P}(\\text{no jump}) = \\exp(-\\lambda L)$. For an infinitesimal interval of length $dt$, $\\mathbb{P}(\\text{one jump}) \\approx \\lambda dt$.\nIgnoring the infinitesimal lengths $dt_i$, the total length of the \"no jump\" intervals is $t_{1} + (t_{2}-t_{1}) + \\dots + (T-t_{n}) = T$. The product of the probabilities of no jumps is $\\exp(-\\lambda T)$. The product of the probabilities of one jump in each $dt_i$ is $(\\lambda dt_{1})\\dots(\\lambda dt_{n})$.\nThus, the joint probability element is:\n$$\n\\mathbb{P}(\\text{event}) \\approx \\left(\\prod_{i=1}^{n} \\lambda dt_{i}\\right) \\exp(-\\lambda T) = \\lambda^{n} \\exp(-\\lambda T) dt_{1} \\dots dt_{n}\n$$\nThis implies that the joint PDF of $(\\tau_{1}, \\dots, \\tau_{n})$ and the event $N_{T}=n$ is $f(t_{1}, \\dots, t_{n}, n) = \\lambda^{n} \\exp(-\\lambda T)$ on the simplex $S_n$.\n\nThe conditional PDF is obtained by dividing this by the probability of the condition, $\\mathbb{P}(N_{T}=n)$:\n$$\nf_{(\\tau_{1}, \\dots, \\tau_{n})|N_{T}=n}(t_{1}, \\dots, t_{n}) = \\frac{f(t_{1}, \\dots, t_{n}, n)}{\\mathbb{P}(N_{T}=n)} = \\frac{\\lambda^{n} \\exp(-\\lambda T)}{\\frac{(\\lambda T)^{n} \\exp(-\\lambda T)}{n!}} = \\frac{n!}{T^{n}}\n$$\nThis is the PDF for $(t_1, \\dots, t_n)$ on the simplex $S_n$. This constant density is the PDF of the order statistics of $n$ independent random variables drawn from a uniform distribution on $(0, T)$.", "answer": "$$\n\\boxed{\\exp(-\\lambda T) \\sum_{k=0}^{m} \\frac{(\\lambda T)^{k}}{k!}}\n$$", "id": "2977994"}, {"introduction": "Having examined key properties of Brownian motion and the Poisson process individually, we can gain deeper insight by directly comparing their path behaviors. A natural and revealing question is how much time each process spends in a given region of its state space. This capstone practice guides you through the calculation of the \"occupation time\" spent above zero, leading to two remarkably different results that highlight the fundamental distinction between continuous-path and jump processes: an intuitive distribution for the Poisson process versus the famously counter-intuitive arcsine law for Brownian motion. [@problem_id:2978056]", "problem": "Let $\\{B_{t}\\}_{t \\geq 0}$ be a standard one-dimensional Brownian motion (BM) starting at $B_{0} = 0$, and let $\\{N_{t}\\}_{t \\geq 0}$ be a Poisson process (PP) with rate $\\lambda > 0$ starting at $N_{0} = 0$. Fix a deterministic horizon $T > 0$. Define the occupation proportion above zero for BM by\n$$\nA_{T} \\coloneqq \\frac{1}{T} \\int_{0}^{T} \\mathbf{1}_{\\{B_{t} > 0\\}} \\,\\mathrm{d}t,\n$$\nand the occupation proportion strictly above zero for PP by\n$$\nO_{T} \\coloneqq \\frac{1}{T} \\int_{0}^{T} \\mathbf{1}_{\\{N_{t} > 0\\}} \\,\\mathrm{d}t.\n$$\nStarting from the fundamental properties of BM (independent, stationary increments, continuity and symmetry) and PP (independent, stationary increments, exponentially distributed interarrival times), and using only well-tested facts such as the reflection principle for BM, the strong Markov property, Brownian scaling, and the Feynman–Kac formula for additive functionals, derive the distribution of $A_{T}$ and of $O_{T}$.\n\nSpecifically:\n\n- Compute the probability density function of $A_{T}$ on the interval $(0,1)$, expressed as a closed-form analytic expression.\n- Compute the cumulative distribution function of $O_{T}$ on $\\mathbb{R}$, expressed as a single analytic expression that is valid for all $x \\in \\mathbb{R}$, using indicator functions if needed to capture atoms and support constraints.\n\nProvide your final answer as two analytic expressions in a single row matrix, where the first entry is the density of $A_{T}$ and the second entry is the cumulative distribution function of $O_{T}$. No numerical approximation is required.", "solution": "We will address the two parts of the problem sequentially.\n\nPart 1: The distribution of $A_{T}$\n\nThe random variable $A_{T}$ is defined as the proportion of time a standard one-dimensional Brownian motion $\\{B_{t}\\}_{t \\geq 0}$ spends above zero up to a fixed time $T > 0$:\n$$\nA_{T} \\coloneqq \\frac{1}{T} \\int_{0}^{T} \\mathbf{1}_{\\{B_{t} > 0\\}} \\,\\mathrm{d}t\n$$\nOur goal is to find the probability density function (PDF) of $A_{T}$.\n\nFirst, we utilize the Brownian scaling property. For any constant $c > 0$, the process $\\{ W_{t} \\}_{t \\ge 0}$ defined by $W_{t} = c^{-1/2} B_{ct}$ is also a standard Brownian motion. Let us choose $c=T$. The process $\\{ W_{u} \\}_{u \\ge 0}$ defined by $W_{u} = T^{-1/2} B_{Tu}$ is a standard Brownian motion. We can rewrite the integral defining $A_{T}$ by a change of variables $t = Tu$:\n$$\nA_{T} = \\frac{1}{T} \\int_{0}^{1} \\mathbf{1}_{\\{B_{Tu} > 0\\}} \\,T\\,\\mathrm{d}u = \\int_{0}^{1} \\mathbf{1}_{\\{B_{Tu} > 0\\}} \\,\\mathrm{d}u\n$$\nSince $T > 0$, the sign of $B_{Tu}$ is the same as the sign of $T^{-1/2} B_{Tu} = W_{u}$. Therefore,\n$$\nA_{T} = \\int_{0}^{1} \\mathbf{1}_{\\{W_{u} > 0\\}} \\,\\mathrm{d}u\n$$\nThe right-hand side is the definition of $A_{1}$ for the standard Brownian motion $W_{u}$. This shows that the distribution of $A_{T}$ is independent of $T$. We can therefore focus on finding the distribution of $A_{1} = \\int_{0}^{1} \\mathbf{1}_{\\{B_{t} > 0\\}} \\,\\mathrm{d}t$.\n\nThis is a classic result known as Lévy's first arcsine law. A key \"well-tested fact\" relates the distribution of the occupation time $A_{t}$ to the distribution of the last time the process hits zero before time $t$. Let $g_{t} = \\sup\\{s \\in [0,t] : B_s = 0\\}$ be the last zero of the Brownian motion on $[0,t]$. Lévy's arcsine law states that $A_{t}$ and $g_{t}/t$ have the same distribution. We will derive the distribution of $g_{1}$.\n\nThe cumulative distribution function (CDF) of $g_1$ for $s \\in [0,1]$ is $F_{g_1}(s) = P(g_1 \\le s)$. The event $\\{g_1 \\le s\\}$ is equivalent to the event that the Brownian motion does not return to zero in the time interval $(s, 1]$.\n$$\nP(g_1 \\le s) = P(B_u \\ne 0 \\text{ for all } u \\in (s, 1])\n$$\nWe condition on the value of $B_s$ and use the strong Markov property.\n$$\nP(g_1 \\le s) = \\int_{-\\infty}^{\\infty} P(B_s \\in dy) \\, P_y(\\tau_0 > 1-s)\n$$\nwhere $P_y(\\tau_0 > t')$ is the probability that a Brownian motion starting from $y$ does not hit zero before time $t'$. The density of $B_s$ is $p_s(y) = (2\\pi s)^{-1/2} \\exp(-y^2/(2s))$.\n\nUsing the reflection principle, the probability $P_y(\\tau_0 \\le t')$ for $y>0$ is equal to the probability that a standard Brownian motion starting at $0$ reaches level $y$ before time $t'$, which is $P_0(M_{t'} \\ge y)$, where $M_{t'} = \\sup_{u \\in [0, t']} B_u$. This probability is $2P_0(B_{t'} \\ge y)$. Thus,\n$$\nP_y(\\tau_0 \\le t') = 2\\int_{y}^{\\infty} \\frac{1}{\\sqrt{2\\pi t'}} \\exp(-z^2/(2t')) \\, dz\n$$\nThis implies that $P_y(\\tau_0 > t') = 1 - 2P_0(B_{t'} \\ge y) = P_0(|B_{t'}| < y)$. For any $y \\ne 0$, we have $P_y(\\tau_0 > t') = P_0(|B_{t'}|<|y|)$.\nLetting $t' = 1-s$, we have\n$$\nP_y(\\tau_0 > 1-s) = \\int_{-|y|}^{|y|} \\frac{1}{\\sqrt{2\\pi(1-s)}} \\exp\\left(-\\frac{z^2}{2(1-s)}\\right) \\, dz\n$$\nSubstituting this into the integral for $P(g_1 \\le s)$:\n$$\nP(g_1 \\le s) = \\int_{-\\infty}^{\\infty} \\frac{1}{\\sqrt{2\\pi s}} \\exp\\left(-\\frac{y^2}{2s}\\right) \\left[ \\int_{-|y|}^{|y|} \\frac{1}{\\sqrt{2\\pi(1-s)}} \\exp\\left(-\\frac{z^2}{2(1-s)}\\right) \\, dz \\right] \\, dy\n$$\nLet $\\mathcal{N}_1$ and $\\mathcal{N}_2$ be two independent standard normal random variables. Let $y = \\sqrt{s}\\mathcal{N}_1$ and $z=\\sqrt{1-s}\\mathcal{N}_2$. The expression above is equivalent to the probability:\n$$\nP(|\\sqrt{1-s}\\mathcal{N}_2| < |\\sqrt{s}\\mathcal{N}_1|) = P\\left(|\\mathcal{N}_2| < |\\mathcal{N}_1| \\sqrt{\\frac{s}{1-s}}\\right)\n$$\nThis probability can be evaluated using a geometric argument. Consider the point $(\\mathcal{N}_1, \\mathcal{N}_2)$ in the plane. Its distribution is isotropic, meaning the angle of the vector $(\\mathcal{N}_1, \\mathcal{N}_2)$ is uniformly distributed on $[0, 2\\pi)$. The condition $|\\mathcal{N}_2| < |\\mathcal{N}_1| \\tan(\\theta)$, where $\\tan(\\theta) = \\sqrt{s/(1-s)}$ for $\\theta \\in [0, \\pi/2)$, defines a double cone region symmetric about the horizontal axis. The total angle of this region is $4\\theta$. The probability is the ratio of this angle to the total angle $2\\pi$.\n$$\nP(g_1 \\le s) = \\frac{4\\theta}{2\\pi} = \\frac{2\\theta}{\\pi}\n$$\nFrom $\\tan(\\theta) = \\sqrt{s/(1-s)}$, we find $\\tan^2(\\theta) = s/(1-s)$, which implies $\\sin^2(\\theta) = s$, so $\\theta = \\arcsin(\\sqrt{s})$.\nTherefore, the CDF of $g_1$ (and thus of $A_1$) is:\n$$\nF_{A_1}(x) = P(A_1 \\le x) = \\frac{2}{\\pi} \\arcsin(\\sqrt{x}) \\quad \\text{for } x \\in [0,1]\n$$\nTo find the PDF $f_{A_T}(x) = f_{A_1}(x)$ for $x \\in (0,1)$, we differentiate the CDF with respect to $x$:\n$$\nf_{A_1}(x) = \\frac{d}{dx} \\left( \\frac{2}{\\pi} \\arcsin(\\sqrt{x}) \\right) = \\frac{2}{\\pi} \\cdot \\frac{1}{\\sqrt{1-(\\sqrt{x})^2}} \\cdot \\frac{d}{dx}(\\sqrt{x}) = \\frac{2}{\\pi} \\frac{1}{\\sqrt{1-x}} \\frac{1}{2\\sqrt{x}}\n$$\nThis simplifies to the arcsine density:\n$$\nf_{A_T}(x) = \\frac{1}{\\pi \\sqrt{x(1-x)}}\n$$\n\nPart 2: The distribution of $O_T$\n\nThe random variable $O_T$ is defined as the proportion of time a Poisson process $\\{N_t\\}_{t \\geq 0}$ with rate $\\lambda > 0$ spends strictly above zero:\n$$\nO_{T} \\coloneqq \\frac{1}{T} \\int_{0}^{T} \\mathbf{1}_{\\{N_{t} > 0\\}} \\,\\mathrm{d}t\n$$\nThe Poisson process starts at $N_0 = 0$. It remains at $0$ until the first event occurs. Let $\\tau_1$ be the time of the first event. $\\tau_1$ is an exponential random variable with rate $\\lambda$, so its CDF is $P(\\tau_1 \\le t) = 1 - \\exp(-\\lambda t)$ for $t \\ge 0$.\nThe indicator function $\\mathbf{1}_{\\{N_t > 0\\}}$ is $0$ for $t < \\tau_1$ and $1$ for $t \\ge \\tau_1$. We can thus rewrite the integral based on the value of $\\tau_1$:\n$$\n\\int_{0}^{T} \\mathbf{1}_{\\{N_{t} > 0\\}} \\,\\mathrm{d}t = \\int_{0}^{T} \\mathbf{1}_{\\{t \\ge \\tau_1\\}} \\,\\mathrm{d}t\n$$\nWe consider two cases for the value of $\\tau_1$:\n1.  If $\\tau_1 > T$, the first event occurs after time $T$. In this case, $N_t=0$ for all $t \\in [0,T]$. The integral is $0$, so $O_T = 0$. The probability of this event is $P(\\tau_1 > T) = \\exp(-\\lambda T)$.\n2.  If $\\tau_1 \\le T$, the first event occurs at or before time $T$. The integral is $\\int_{\\tau_1}^{T} 1 \\, \\mathrm{d}t = T - \\tau_1$. In this case, $O_T = \\frac{T - \\tau_1}{T} = 1 - \\frac{\\tau_1}{T}$.\n\nNow we derive the CDF, $F_{O_T}(x) = P(O_T \\le x)$, for all $x \\in \\mathbb{R}$.\n- Since $O_T \\ge 0$, for any $x < 0$, $F_{O_T}(x) = 0$.\n- The distribution has a point mass (atom) at $x=0$. $P(O_T=0) = P(\\tau_1 > T) = \\exp(-\\lambda T)$. So, $F_{O_T}(0) = P(O_T \\le 0) = \\exp(-\\lambda T)$.\n- For $x \\in (0, 1)$, the event $\\{O_T \\le x\\}$ can occur if $O_T=0$ or if $0 < O_T \\le x$.\n$$\nP(O_T \\le x) = P(O_T=0) + P(0 < O_T \\le x)\n$$\nThe event $\\{0 < O_T \\le x\\}$ is equivalent to $\\{ \\tau_1 \\le T \\text{ and } 1 - \\frac{\\tau_1}{T} \\le x \\}$. The second inequality implies $1-x \\le \\frac{\\tau_1}{T}$, or $\\tau_1 \\ge T(1-x)$.\nSo, $P(0 < O_T \\le x) = P(T(1-x) \\le \\tau_1 \\le T)$.\nUsing the CDF of $\\tau_1$:\n$$\nP(T(1-x) \\le \\tau_1 \\le T) = P(\\tau_1 \\le T) - P(\\tau_1 < T(1-x)) = (1 - \\exp(-\\lambda T)) - (1 - \\exp(-\\lambda T(1-x)))\n$$\n$$\nP(0 < O_T \\le x) = \\exp(-\\lambda T(1-x)) - \\exp(-\\lambda T)\n$$\nTherefore, for $x \\in (0,1)$:\n$$\nF_{O_T}(x) = \\exp(-\\lambda T) + (\\exp(-\\lambda T(1-x)) - \\exp(-\\lambda T)) = \\exp(-\\lambda T(1-x))\n$$\n- Note that this expression is also valid for $x=0$, as $\\exp(-\\lambda T(1-0)) = \\exp(-\\lambda T)$. So, for $x \\in [0,1)$, $F_{O_T}(x) = \\exp(-\\lambda T(1-x))$.\n- For $x \\ge 1$: since $\\tau_1 \\ge 0$, we have $O_T = 1 - \\tau_1/T \\le 1$. In fact, since $\\tau_1$ is a continuous random variable, $P(\\tau_1=0)=0$, so $O_T < 1$ almost surely. Thus, for any $x \\ge 1$, $P(O_T \\le x) = 1$.\n\nWe can combine these parts into a single expression valid for all $x \\in \\mathbb{R}$ using indicator functions:\n$$\nF_{O_T}(x) = \\exp(-\\lambda T(1-x)) \\mathbf{1}_{[0, 1)}(x) + \\mathbf{1}_{[1, \\infty)}(x)\n$$\nwhere $\\mathbf{1}_S(x)$ is the indicator function of the set $S$.\n\nFinal Answer Compilation:\nThe PDF of $A_T$ for $x \\in (0,1)$ is $\\frac{1}{\\pi \\sqrt{x(1-x)}}$.\nThe CDF of $O_T$ for $x \\in \\mathbb{R}$ is $\\exp(-\\lambda T(1-x)) \\mathbf{1}_{[0, 1)}(x) + \\mathbf{1}_{[1, \\infty)}(x)$.\nWe present these in a single row matrix as requested.", "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{1}{\\pi \\sqrt{x(1-x)}} & \\exp(-\\lambda T(1-x)) \\mathbf{1}_{[0, 1)}(x) + \\mathbf{1}_{[1, \\infty)}(x) \\end{pmatrix}}\n$$", "id": "2978056"}]}