{"hands_on_practices": [{"introduction": "Our first practice delves into the fundamental statistical nature of the Brownian bridge. By defining the bridge as a Brownian motion conditioned to end at a specific point, a natural question arises: what is the probability distribution of its location at an intermediate time $t \\in (0, T)$? This exercise [@problem_id:3000082] guides you to derive this marginal law from first principles, applying the properties of conditioned Gaussian variables to reveal the bridge's characteristic parabolic variance.", "problem": "Let $(W_{s})_{0 \\leq s \\leq T}$ be a standard Brownian motion, defined by $W_{0} = 0$, stationary independent increments, and Gaussian increments with $\\mathbb{E}[W_{s}] = 0$ and $\\operatorname{Cov}(W_{s}, W_{u}) = \\min\\{s, u\\}$ for all $s, u \\in [0, T]$. Define the Brownian bridge $(X_{s})_{0 \\leq s \\leq T}$ from $0$ to $0$ over the interval $[0, T]$ as the process obtained by conditioning the Brownian motion on the terminal value $W_{T} = 0$, that is, $X_{s}$ has the same law as $W_{s}$ given $W_{T} = 0$. Using only properties of jointly Gaussian random variables and the covariance structure of Brownian motion, derive the marginal law of $X_{t}$ for a fixed $t \\in (0, T)$. Express your final answer as a single closed-form distribution expression. No rounding is required and no units apply.", "solution": "The problem requires the derivation of the marginal law of a Brownian bridge process $X_t$ at a fixed time $t \\in (0, T)$. By definition, the law of $X_t$ is the law of a standard Brownian motion $W_t$ conditioned on the event $W_T = 0$. The derivation must be based on the properties of jointly Gaussian random variables.\n\nA standard Brownian motion $(W_s)_{s \\geq 0}$ is a Gaussian process, which implies that for any finite set of time points, the corresponding random variables follow a multivariate normal distribution. To determine the law of $W_t$ given $W_T=0$, we consider the random vector $(W_t, W_T)^T$ for a fixed time $t$ such that $0 < t < T$. This vector follows a bivariate normal distribution.\n\nFirst, we determine the parameters of this bivariate normal distribution, namely the mean vector $\\boldsymbol{\\mu}$ and the covariance matrix $\\boldsymbol{\\Sigma}$.\n\nThe mean of a standard Brownian motion at any time $s$ is zero, i.e., $\\mathbb{E}[W_s] = 0$. Therefore, the mean vector of $(W_t, W_T)^T$ is the zero vector:\n$$\n\\boldsymbol{\\mu} = \\begin{pmatrix} \\mathbb{E}[W_t] \\\\ \\mathbb{E}[W_T] \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}.\n$$\n\nThe covariance matrix $\\boldsymbol{\\Sigma}$ has entries given by the covariance function of the Brownian motion, $\\operatorname{Cov}(W_s, W_u) = \\min\\{s, u\\}$. The components of $\\boldsymbol{\\Sigma}$ are:\n$$\n\\boldsymbol{\\Sigma} = \\begin{pmatrix} \\operatorname{Cov}(W_t, W_t) & \\operatorname{Cov}(W_t, W_T) \\\\ \\operatorname{Cov}(W_T, W_t) & \\operatorname{Cov}(W_T, W_T) \\end{pmatrix}.\n$$\nUsing the provided covariance function, we calculate each term:\n- The variance of $W_t$ is $\\operatorname{Var}(W_t) = \\operatorname{Cov}(W_t, W_t) = \\min\\{t, t\\} = t$.\n- The variance of $W_T$ is $\\operatorname{Var}(W_T) = \\operatorname{Cov}(W_T, W_T) = \\min\\{T, T\\} = T$.\n- The covariance between $W_t$ and $W_T$, given that $t<T$, is $\\operatorname{Cov}(W_t, W_T) = \\min\\{t, T\\} = t$.\n- The covariance matrix is symmetric, so $\\operatorname{Cov}(W_T, W_t) = t$.\n\nThus, the covariance matrix is:\n$$\n\\boldsymbol{\\Sigma} = \\begin{pmatrix} t & t \\\\ t & T \\end{pmatrix}.\n$$\nThe random vector $(W_t, W_T)^T$ therefore follows the distribution:\n$$\n\\begin{pmatrix} W_t \\\\ W_T \\end{pmatrix} \\sim \\mathcal{N}\\left( \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}, \\begin{pmatrix} t & t \\\\ t & T \\end{pmatrix} \\right).\n$$\n\nWe now utilize the standard result for the conditional distribution of a multivariate Gaussian random variable. For a general bivariate normal vector $(Y_1, Y_2)^T$ with mean $(\\mu_1, \\mu_2)^T$ and covariance matrix with components $\\sigma_{ij}$, the conditional distribution of $Y_1$ given $Y_2 = y_2$ is a normal distribution with mean $\\mu_{1|2}$ and variance $\\sigma_{1|2}^2$ given by:\n$$\n\\mu_{1|2} = \\mu_1 + \\sigma_{12} \\sigma_{22}^{-1} (y_2 - \\mu_2)\n$$\n$$\n\\sigma_{1|2}^2 = \\sigma_{11} - \\sigma_{12} \\sigma_{22}^{-1} \\sigma_{21}\n$$\nIn our specific case, we have $Y_1 = W_t$, $Y_2 = W_T$, and we condition on the event $y_2 = W_T = 0$. The parameters from our bivariate distribution are: $\\mu_1 = 0$, $\\mu_2 = 0$, $\\sigma_{11} = t$, $\\sigma_{22} = T$, and $\\sigma_{12} = \\sigma_{21} = t$.\n\nSubstituting these values into the formulas, we find the mean and variance of $X_t$, which are the conditional mean and variance of $W_t$ given $W_T = 0$.\n\nThe conditional mean is:\n$$\n\\mathbb{E}[X_t] = \\mathbb{E}[W_t | W_T = 0] = 0 + t \\cdot T^{-1} \\cdot (0 - 0) = 0.\n$$\nThe conditional variance is:\n$$\n\\operatorname{Var}(X_t) = \\operatorname{Var}(W_t | W_T = 0) = t - t \\cdot T^{-1} \\cdot t = t - \\frac{t^2}{T}.\n$$\nThis expression for the variance can be written in a more compact form:\n$$\n\\operatorname{Var}(X_t) = \\frac{tT - t^2}{T} = \\frac{t(T-t)}{T}.\n$$\nSince the conditional distribution of a Gaussian is always Gaussian, the marginal law of the Brownian bridge $X_t$ at time $t$ is a normal distribution. We have found its mean to be $0$ and its variance to be $\\frac{t(T-t)}{T}$.\n\nTherefore, the marginal law of $X_t$ for a fixed $t \\in (0, T)$ is given by the normal distribution $\\mathcal{N}\\left(0, \\frac{t(T-t)}{T}\\right)$.", "answer": "$$\\boxed{\\mathcal{N}\\left(0, \\frac{t(T-t)}{T}\\right)}$$", "id": "3000082"}, {"introduction": "Having characterized the bridge's state at a single time, we now turn to its dynamics. A key property of many stochastic processes is the martingale property, which implies that the best prediction of the process's future value is its current value. This practice [@problem_id:1286120] challenges you to determine if the Brownian bridge is a martingale, an investigation that uncovers the inherent drift responsible for its return to the terminal point and distinguishes it from standard Brownian motion.", "problem": "In the study of stochastic processes, a standard one-dimensional Brownian motion, denoted by $W(t)$, is a continuous-time process starting at zero ($W(0)=0$) with stationary, independent increments, where the increment $W(t) - W(s)$ is normally distributed with mean 0 and variance $t-s$ for $0 \\le s < t$.\n\nA standard Brownian bridge, $B(t)$, on the time interval $[0, 1]$ is a stochastic process derived from a standard Brownian motion by \"pinning\" it to be zero at both $t=0$ and $t=1$. It is formally defined as:\n$$B(t) = W(t) - tW(1)$$\nfor $t \\in [0, 1]$.\n\nLet $\\mathcal{F}_t = \\sigma(B(u) : 0 \\le u \\le t)$ be the natural filtration of the Brownian bridge, which represents the history of the process up to time $t$. A process $X(t)$ is a martingale with respect to a filtration $\\mathcal{G}_t$ if it satisfies three conditions:\n1.  $X(t)$ is adapted to the filtration $\\mathcal{G}_t$ for all $t$.\n2.  $E[|X(t)|] < \\infty$ for all $t$.\n3.  For any $s < t$, the conditional expectation satisfies $E[X(t) | \\mathcal{G}_s] = X(s)$.\n\nIt is known that the standard Brownian bridge $B(t)$ satisfies the first two conditions for being a martingale with respect to its natural filtration $\\mathcal{F}_t$. Your task is to evaluate the third condition.\n\nWhich of the following statements correctly describes the martingale property of the standard Brownian bridge $B(t)$ with respect to its natural filtration $\\mathcal{F}_t$?\n\nA. $B(t)$ is a martingale with respect to $\\mathcal{F}_t$.\n\nB. $B(t)$ is not a martingale with respect to $\\mathcal{F}_t$ because the condition $E[B(t) | \\mathcal{F}_s] = B(s)$ is not satisfied for $s < t < 1$.\n\nC. $B(t)$ is not a martingale with respect to $\\mathcal{F}_t$ because its expected value, $E[B(t)]$, is not constant over the interval $t \\in [0, 1]$.\n\nD. $B(t)$ is a martingale with respect to $\\mathcal{F}_t$, but only for the sub-interval $t \\in [0, 1/2]$.\n\nE. The martingale property of $B(t)$ cannot be determined because it depends on the specific path taken by the underlying Brownian motion $W(t)$.", "solution": "We start from the definition of the Brownian bridge $B(t)=W(t)-tW(1)$ for $t\\in[0,1]$, where $W$ is a standard Brownian motion. Since $E[W(t)]=0$ for all $t$ and $E[W(1)]=0$, it follows immediately that $E[B(t)]=0$ for all $t$.\n\nNext, we compute the covariance function of $B$. For $0\\le s,t\\le 1$,\n$$\n\\operatorname{Cov}(B(s),B(t))=E\\big[(W(s)-sW(1))(W(t)-tW(1))\\big].\n$$\nExpanding and using the properties of Brownian motion,\n$$\n\\operatorname{Cov}(B(s),B(t))=E[W(s)W(t)]-tE[W(s)W(1)]-sE[W(1)W(t)]+stE[W(1)^{2}].\n$$\nSince $E[W(s)W(t)]=\\min(s,t)$, $E[W(s)W(1)]=s$, $E[W(t)W(1)]=t$, and $E[W(1)^{2}]=1$, we obtain\n$$\n\\operatorname{Cov}(B(s),B(t))=\\min(s,t)-st.\n$$\nIn particular, for $s<t$,\n$$\n\\operatorname{Cov}(B(s),B(t))=s-st=s(1-t),\\qquad \\operatorname{Var}(B(s))=s-s^{2}=s(1-s).\n$$\n\nWe now evaluate the martingale condition $E[B(t)\\mid\\mathcal{F}_{s}]=B(s)$ for $s<t$. Because $B$ is a centered Gaussian process, the conditional expectation of $B(t)$ given the past $\\mathcal{F}_{s}=\\sigma(B(u):0\\le u\\le s)$ is the orthogonal projection of $B(t)$ onto the closed linear span of $\\{B(u):u\\le s\\}$. We claim this projection is a scalar multiple of $B(s)$. To see this, for $u\\le s<t$ we have\n$$\n\\operatorname{Cov}(B(t),B(u))=u(1-t),\\qquad \\operatorname{Cov}(B(s),B(u))=u(1-s).\n$$\nHence, for any constant $c$,\n$$\n\\operatorname{Cov}\\big(B(t)-cB(s),B(u)\\big)=u\\big((1-t)-c(1-s)\\big).\n$$\nChoosing $c=\\frac{1-t}{1-s}$ makes this covariance zero for all $u\\le s$, so $B(t)-\\frac{1-t}{1-s}B(s)$ is orthogonal to the span of $\\{B(u):u\\le s\\}$. Therefore the orthogonal projection (and hence the conditional expectation, by Gaussianity) is\n$$\nE[B(t)\\mid\\mathcal{F}_{s}]=\\frac{1-t}{1-s}\\,B(s),\\qquad 0\\le s<t<1.\n$$\n\nThis shows that for $s<t<1$,\n$$\nE[B(t)\\mid\\mathcal{F}_{s}] \\neq B(s)\n$$\nunless $B(s)=0$ almost surely or $t=s$. Therefore the martingale condition fails. Note also that $E[B(t)]=0$ for all $t$, so non-martingality is not due to a non-constant mean. Consequently, the correct statement is that $B(t)$ is not a martingale with respect to its natural filtration because the conditional expectation equals $\\frac{1-t}{1-s}B(s)$, not $B(s)$, for $s<t<1$.", "answer": "$$\\boxed{B}$$", "id": "1286120"}, {"introduction": "This final practice synthesizes our theoretical knowledge into a powerful computational tool: an exact simulation algorithm for the Brownian bridge. By first deriving the step-by-step conditional distribution, or transition kernel, you will construct a method to generate entire sample paths of the process [@problem_id:3000113]. This capstone exercise not only bridges the gap between abstract theory and practical application but is also a cornerstone technique in areas like computational finance and the numerical solution of stochastic differential equations.", "problem": "Consider a standard Brownian motion $B_t$ with $B_0=0$ and a fixed time horizon $T>0$. Define the Brownian bridge $X_t$ on the interval $[0,T]$ as the Gaussian process obtained by conditioning $B_t$ to satisfy $X_0=a$ and $X_T=b$ for given real numbers $a$ and $b$. Starting only from the fundamental properties of Brownian motion (independent increments, Gaussianity, stationarity of increments, and $\\mathrm{Var}(B_t)=t$) and the definition of conditioning for multivariate Gaussian distributions, derive the conditional Gaussian structure of the Brownian bridge. Use this structure to obtain the exact one-step transition kernel of $X_t$ from an arbitrary time $s$ to a later time $t$ with $0 \\le s < t < T$, given the state $X_s$ and the endpoint $X_T=b$. From this, design an exact simulation algorithm for $X_t$ on an arbitrary strictly increasing time grid $0=t_0 < t_1 < \\dots < t_n = T$ by sequentially sampling $X_{t_{k+1}}$ given $X_{t_k}$ and $X_T=b$.\n\nYou must implement the algorithm in a program that, for each specified test case, generates $N$ independent sample paths of the Brownian bridge on the provided grid using the derived exact transition. To verify consistency with the transition densities, use the probability integral transform: for each simulated pair $(X_s,X_t)$ at a chosen transition ($s \\to t$), compute the cumulative distribution function value $U=\\Phi\\big((X_t-m)/\\sqrt{v}\\big)$ where $\\Phi$ is the standard normal cumulative distribution function, and $m$ and $v$ are the derived conditional mean and variance of $X_t$ given $X_s$ and $X_T=b$. Under correct conditional Gaussian simulation, these $U$ values are independent and identically distributed Uniform on $[0,1]$. Quantify the uniformity by the Kolmogorov-Smirnov (KS) statistic, defined as the supremum of the absolute difference between the empirical distribution function and the uniform distribution function on $[0,1]$. Additionally, verify the marginal distribution of $X_{t^\\ast}$ at a chosen interior time $t^\\ast<T$ using the same probability integral transform with the derived marginal mean and variance of the Brownian bridge; the transformed values should also be Uniform on $[0,1]$, and you should compute the corresponding KS statistic.\n\nYour program must:\n- Use a fixed Monte Carlo sample size $N=20000$ and a fixed pseudorandom number generator seed $123456$ for reproducibility.\n- For each test case, compute two KS statistics: one for the transition probability integral transform at the specified ($s \\to t$) and one for the marginal probability integral transform at the specified $t^\\ast$.\n- Declare the test case as passing if and only if both KS statistics are less than the threshold $\\delta=0.03$.\n- Produce a single line of output containing the pass/fail results for all test cases as a comma-separated list of booleans enclosed in square brackets, for example, $[{\\tt True},{\\tt False},\\dots]$.\n\nThe test suite consists of four cases that together cover a general case, near-terminal behavior, irregular grids and non-unit horizons, and extremely small time steps. In each case, all times and parameters are real numbers expressed in natural units of time and state without physical units:\n\n- Case $1$ (general case):\n  - Parameters: $T=1.0$, $a=0.0$, $b=0.8$.\n  - Grid: $[0.0,0.2,0.5,0.9,1.0]$.\n  - Transition test: $s=0.5$, $t=0.9$.\n  - Marginal test: $t^\\ast=0.5$.\n\n- Case $2$ (near-terminal behavior):\n  - Parameters: $T=1.0$, $a=0.5$, $b=-0.5$.\n  - Grid: $[0.0,0.95,0.99,1.0]$.\n  - Transition test: $s=0.95$, $t=0.99$.\n  - Marginal test: $t^\\ast=0.99$.\n\n- Case $3$ (irregular grid, non-unit horizon):\n  - Parameters: $T=2.5$, $a=-1.0$, $b=0.5$.\n  - Grid: $[0.0,0.1,0.4,1.3,2.0,2.5]$.\n  - Transition test: $s=1.3$, $t=2.0$.\n  - Marginal test: $t^\\ast=2.0$.\n\n- Case $4$ (extremely small time step):\n  - Parameters: $T=1.0$, $a=0.2$, $b=0.0$.\n  - Grid: $[0.0,0.4,0.4005,1.0]$.\n  - Transition test: $s=0.4$, $t=0.4005$.\n  - Marginal test: $t^\\ast=0.4005$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for example, $[{\\tt True},{\\tt True},{\\tt True},{\\tt True}]$.\n\nAll mathematical symbols, variables, functions, operators, and numbers must be written in LaTeX. The Kolmogorov-Smirnov (KS) acronym must be expanded on first use as Kolmogorov-Smirnov (KS).", "solution": "The problem requires the derivation and implementation of an exact simulation algorithm for a Brownian bridge, verified through statistical tests. The derivation will be based on the fundamental properties of Brownian motion and the rules of conditioning for multivariate Gaussian distributions.\n\nA standard one-dimensional Brownian motion $\\{B_t\\}_{t \\ge 0}$ starting at $B_0=0$ is a Gaussian process characterized by:\n1.  $B_0 = 0$.\n2.  $\\mathbb{E}[B_t] = 0$ for all $t \\ge 0$.\n3.  $\\mathrm{Cov}(B_s, B_t) = \\min(s, t)$ for all $s, t \\ge 0$.\n4.  It has independent increments: for $0 \\le t_1 < t_2 < \\dots < t_n$, the random variables $(B_{t_2}-B_{t_1}), (B_{t_3}-B_{t_2}), \\dots, (B_{t_n}-B_{t_{n-1}})$ are independent.\n5.  It has stationary Gaussian increments: $B_t - B_s \\sim \\mathcal{N}(0, t-s)$ for $s < t$.\n\nA Brownian motion starting at $a \\in \\mathbb{R}$ can be defined as $W_t = a + B_t$. It is a Gaussian process with mean function $\\mathbb{E}[W_t] = a$ and covariance $\\mathrm{Cov}(W_s, W_t) = \\mathrm{Cov}(B_s, B_t) = \\min(s, t)$.\n\nA Brownian bridge $\\{X_t\\}_{0 \\le t \\le T}$ is defined as the process $W_t$ conditioned on the event that its value at time $T$ is $b$. That is, $X_t$ is the process $(W_t | W_T = b)$, with the boundary conditions $X_0=a$ and $X_T=b$. Since $W_t$ is a Gaussian process, the conditioned process $X_t$ is also a Gaussian process. Our first task is to determine its conditional Gaussian structure, specifically its mean and variance.\n\n**1. Marginal Distribution of the Brownian Bridge**\n\nTo perform the marginal verification test, we need the distribution of $X_t$ for an arbitrary time $t \\in (0, T)$. This corresponds to the distribution of $W_t$ conditional on $W_T = b$. We consider the bivariate random vector $(W_t, W_T)$. This vector is jointly Gaussian.\n\nIts mean vector is $\\mathbb{E}[(W_t, W_T)] = (a, a)$.\nIts covariance matrix is, for $0 < t < T$:\n$$\n\\Sigma = \\begin{pmatrix} \\mathrm{Var}(W_t) & \\mathrm{Cov}(W_t, W_T) \\\\ \\mathrm{Cov}(W_T, W_t) & \\mathrm{Var}(W_T) \\end{pmatrix} = \\begin{pmatrix} t & \\min(t,T) \\\\ \\min(t,T) & T \\end{pmatrix} = \\begin{pmatrix} t & t \\\\ t & T \\end{pmatrix}\n$$\nFor a general partitioned Gaussian vector $(Y_1, Y_2)$ with mean $(\\mu_1, \\mu_2)$ and block covariance matrix $\\begin{pmatrix} \\Sigma_{11} & \\Sigma_{12} \\\\ \\Sigma_{21} & \\Sigma_{22} \\end{pmatrix}$, the conditional distribution of $Y_1$ given $Y_2=y_2$ is Gaussian with:\nMean: $\\mu_{1|2} = \\mu_1 + \\Sigma_{12} \\Sigma_{22}^{-1} (y_2 - \\mu_2)$\nVariance: $\\Sigma_{11|2} = \\Sigma_{11} - \\Sigma_{12} \\Sigma_{22}^{-1} \\Sigma_{21}$\n\nIn our case, $Y_1 = W_t$, $Y_2 = W_T$, and $y_2 = b$. We have $\\mu_1 = a$, $\\mu_2 = a$, $\\Sigma_{11} = t$, $\\Sigma_{12} = t$, $\\Sigma_{21} = t$, and $\\Sigma_{22} = T$.\n\nThe marginal mean of the Brownian bridge $X_t$ is:\n$$\n\\mathbb{E}[X_t] = \\mathbb{E}[W_t | W_T=b] = a + t \\cdot T^{-1} \\cdot (b - a) = a \\left(1 - \\frac{t}{T}\\right) + b \\frac{t}{T} = a\\frac{T-t}{T} + b\\frac{t}{T}\n$$\nThe marginal variance of the Brownian bridge $X_t$ is:\n$$\n\\mathrm{Var}(X_t) = \\mathrm{Var}(W_t | W_T=b) = t - t \\cdot T^{-1} \\cdot t = t - \\frac{t^2}{T} = \\frac{t(T-t)}{T}\n$$\nThus, the marginal distribution of the Brownian bridge at time $t \\in (0, T)$ is:\n$$\nX_t \\sim \\mathcal{N}\\left(a\\frac{T-t}{T} + b\\frac{t}{T}, \\frac{t(T-t)}{T}\\right)\n$$\n\n**2. One-Step Transition Kernel**\n\nFor the simulation algorithm, we need the distribution of $X_t$ at time $t$ given its value $x_s$ at a previous time $s$, where $0 \\le s < t < T$. This corresponds to the distribution of $W_t$ conditional on both $W_s = x_s$ and $W_T = b$. We analyze the trivariate Gaussian vector $(W_s, W_t, W_T)$. To apply the conditioning formula, we partition it as $(Y_1, Y_2)$ where $Y_1 = W_t$ and $Y_2 = (W_s, W_T)$.\n\nThe joint mean vector is $(a, a, a)$. The covariance matrix for times $0 \\le s < t < T$ is:\n$$\n\\mathrm{Cov}((W_s, W_t, W_T)) = \\begin{pmatrix} s & s & s \\\\ s & t & t \\\\ s & t & T \\end{pmatrix}\n$$\nWe partition this matrix according to $Y_1$ and $Y_2$:\n$\\Sigma_{11} = t$\n$\\Sigma_{12} = \\begin{pmatrix} s & t \\end{pmatrix}$\n$\\Sigma_{21} = \\begin{pmatrix} s \\\\ t \\end{pmatrix}$\n$\\Sigma_{22} = \\begin{pmatrix} s & s \\\\ s & T \\end{pmatrix}$\n\nWe need the inverse of $\\Sigma_{22}$ (for $s>0$):\n$$\n\\Sigma_{22}^{-1} = \\frac{1}{\\det(\\Sigma_{22})} \\begin{pmatrix} T & -s \\\\ -s & s \\end{pmatrix} = \\frac{1}{sT - s^2} \\begin{pmatrix} T & -s \\\\ -s & s \\end{pmatrix} = \\frac{1}{s(T-s)} \\begin{pmatrix} T & -s \\\\ -s & s \\end{pmatrix}\n$$\nThe conditional mean of $X_t$ given $X_s=x_s$ (and $X_T=b$) is:\n$$\n\\mathbb{E}[X_t | X_s=x_s, X_T=b] = a + \\Sigma_{12} \\Sigma_{22}^{-1} \\begin{pmatrix} x_s - a \\\\ b - a \\end{pmatrix}\n$$\n$$\n= a + \\frac{1}{s(T-s)} \\begin{pmatrix} s & t \\end{pmatrix} \\begin{pmatrix} T & -s \\\\ -s & s \\end{pmatrix} \\begin{pmatrix} x_s - a \\\\ b - a \\end{pmatrix}\n$$\n$$\n= a + \\frac{1}{T-s} \\begin{pmatrix} T-t  t-s \\end{pmatrix} \\begin{pmatrix} x_s-a \\\\ b-a \\end{pmatrix} = a + \\frac{(T-t)(x_s-a) + (t-s)(b-a)}{T-s}\n$$\n$$\n\\mathbb{E}[X_t | X_s=x_s] = \\frac{(T-s)a - (T-t)a - (t-s)a + (T-t)x_s + (t-s)b}{T-s} = x_s\\frac{T-t}{T-s} + b\\frac{t-s}{T-s}\n$$\nThe conditional variance is:\n$$\n\\mathrm{Var}(X_t | X_s=x_s, X_T=b) = \\Sigma_{11} - \\Sigma_{12} \\Sigma_{22}^{-1} \\Sigma_{21}\n$$\n$$\n= t - \\frac{1}{s(T-s)} \\begin{pmatrix} s  t \\end{pmatrix} \\begin{pmatrix} T  -s \\\\ -s  s \\end{pmatrix} \\begin{pmatrix} s \\\\ t \\end{pmatrix} = t - \\frac{s^2 T-2s^2 t+st^2}{s(T-s)}\n$$\nThis simplifies to:\n$$\n\\mathrm{Var}(X_t | X_s=x_s) = t - \\frac{sT-2st+t^2}{T-s} = \\frac{t(T-s)-(sT-2st+t^2)}{T-s} = \\frac{(T-t)(t-s)}{T-s}\n$$\nThese formulas also hold for $s=0$ by taking the limit or by re-deriving with $W_0=a$ as a deterministic value. The conditional distribution, or transition kernel, is thus:\n$$\n(X_t | X_s=x_s) \\sim \\mathcal{N}\\left(x_s\\frac{T-t}{T-s} + b\\frac{t-s}{T-s}, \\frac{(t-s)(T-t)}{T-s}\\right)\n$$\n\n**3. Exact Simulation Algorithm**\n\nGiven a strictly increasing time grid $0=t_0  t_1  \\dots  t_n = T$, a sample path $(X_{t_0}, \\dots, X_{t_n})$ can be generated sequentially:\n1.  Initialize: Set $X_{t_0} = a$.\n2.  Iterate for $k = 0, 1, \\dots, n-1$:\n    - The state $X_{t_k}$ is known.\n    - Sample $X_{t_{k+1}}$ from the conditional distribution derived above, with $s=t_k$, $t=t_{k+1}$, and $x_s=X_{t_k}$.\n    - Let $s=t_k$ and $t=t_{k+1}$. The conditional mean is $m_k = X_{t_k}\\frac{T-t}{T-s} + b\\frac{t-s}{T-s}$. The conditional variance is $v_k = \\frac{(t-s)(T-t)}{T-s}$.\n    - Sample a standard normal random variable $Z \\sim \\mathcal{N}(0,1)$.\n    - Set $X_{t_{k+1}} = m_k + \\sqrt{v_k} Z$.\n3.  The final value $X_{t_n}$ will be exactly $b$, as for the last step ($k=n-1$), $s=t_{n-1}$ and $t=t_n=T$, yielding $m_{n-1}=b$ and $v_{n-1}=0$.\n\n**4. Verification Methodology**\n\nThe correctness of the simulation is assessed using the probability integral transform (PIT). If a continuous random variable $Y$ has cumulative distribution function (CDF) $F_Y$, then the random variable $U = F_Y(Y)$ is uniformly distributed on $[0,1]$. We use this property to test both the transition and marginal distributions.\n\n-   **Transition Test**: For a specified transition from $s$ to $t$, we generate $N=20000$ sample paths. For each path $i$, we obtain a pair $(X_s^{(i)}, X_t^{(i)})$. The conditional CDF of $X_t^{(i)}$ given $X_s^{(i)}$ is that of a normal distribution with mean $m^{(i)} = X_s^{(i)}\\frac{T-t}{T-s} + b\\frac{t-s}{T-s}$ and variance $v = \\frac{(t-s)(T-t)}{T-s}$. We compute the PIT values $U_{trans}^{(i)} = \\Phi\\left( (X_t^{(i)} - m^{(i)})/\\sqrt{v} \\right)$, where $\\Phi$ is the standard normal CDF.\n\n-   **Marginal Test**: For a specified time $t^\\ast \\in (0, T)$, we extract the simulated values $X_{t^\\ast}^{(i)}$ for $i=1, \\dots, N$. The theoretical marginal distribution is $\\mathcal{N}(m_{marg}, v_{marg})$, where $m_{marg} = a\\frac{T-t^\\ast}{T} + b\\frac{t^\\ast}{T}$ and $v_{marg} = \\frac{t^\\ast(T-t^\\ast)}{T}$. We compute the PIT values $U_{marg}^{(i)} = \\Phi\\left( (X_{t^\\ast}^{(i)} - m_{marg})/\\sqrt{v_{marg}} \\right)$.\n\nFor both sets of transformed values, $\\{U_{trans}^{(i)}\\}$ and $\\{U_{marg}^{(i)}\\}$, we test the hypothesis that they are drawn from a uniform distribution on $[0,1]$. This is done using the Kolmogorov-Smirnov (KS) test, which computes the statistic $D_N = \\sup_x |F_N(x) - F(x)|$, where $F_N$ is the empirical distribution function of the sample and $F$ is the CDF of the uniform distribution, $F(x)=x$. A test case passes if both the transition KS statistic and the marginal KS statistic are below the given threshold $\\delta = 0.03$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import norm, kstest\n\ndef solve():\n    \"\"\"\n    Implements the Brownian bridge simulation and verification as per the problem statement.\n    \"\"\"\n    # Define constants from the problem statement.\n    N = 20000\n    SEED = 123456\n    DELTA = 0.03\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (T, a, b, grid, s_test, t_test, t_star_test)\n        (1.0, 0.0, 0.8, [0.0, 0.2, 0.5, 0.9, 1.0], 0.5, 0.9, 0.5),\n        (1.0, 0.5, -0.5, [0.0, 0.95, 0.99, 1.0], 0.95, 0.99, 0.99),\n        (2.5, -1.0, 0.5, [0.0, 0.1, 0.4, 1.3, 2.0, 2.5], 1.3, 2.0, 2.0),\n        (1.0, 0.2, 0.0, [0.0, 0.4, 0.4005, 1.0], 0.4, 0.4005, 0.4005),\n    ]\n\n    results = []\n\n    for case in test_cases:\n        T, a, b, grid, s_test, t_test, t_star_test = case\n        \n        time_grid = np.array(grid, dtype=float)\n        n_points = len(time_grid)\n        \n        # Find indices for verification points\n        s_idx = np.where(np.isclose(time_grid, s_test))[0][0]\n        t_idx = np.where(np.isclose(time_grid, t_test))[0][0]\n        t_star_idx = np.where(np.isclose(time_grid, t_star_test))[0][0]\n\n        # Initialize pseudorandom number generator for reproducibility.\n        rng = np.random.default_rng(SEED)\n\n        # Array to store all N paths.\n        x_paths = np.zeros((N, n_points))\n        x_paths[:, 0] = a\n\n        # Sequentially generate points for each path.\n        for k in range(n_points - 1):\n            s_k = time_grid[k]\n            t_k_plus_1 = time_grid[k+1]\n            \n            # This check handles the final step where t_k_plus_1 = T, which has 0 variance.\n            # In this case, mean is b and variance is 0.\n            if np.isclose(T - s_k, 0):\n                x_paths[:, k+1] = b\n                continue\n\n            prev_x = x_paths[:, k]\n            \n            # Derived one-step transition kernel parameters\n            # Mean: x_s * (T-t)/(T-s) + b * (t-s)/(T-s)\n            # Var: (t-s)*(T-t)/(T-s)\n            mean_trans = prev_x * (T - t_k_plus_1) / (T - s_k) + b * (t_k_plus_1 - s_k) / (T - s_k)\n            var_trans = (t_k_plus_1 - s_k) * (T - t_k_plus_1) / (T - s_k)\n            \n            # Ensure variance is non-negative due to potential float inaccuracies\n            if var_trans  0:\n                var_trans = 0.0\n\n            std_trans = np.sqrt(var_trans)\n            \n            # Generate standard normal samples\n            z = rng.standard_normal(N)\n\n            # Sample the next point in the paths\n            x_paths[:, k+1] = mean_trans + std_trans * z\n            \n        # --- Verification ---\n\n        # 1. Transition probability integral transform test\n        x_s_samples = x_paths[:, s_idx]\n        x_t_samples = x_paths[:, t_idx]\n        \n        # Theoretical conditional mean and variance for the test transition (s_test - t_test)\n        m_cond = x_s_samples * (T - t_test) / (T - s_test) + b * (t_test - s_test) / (T - s_test)\n        v_cond = (t_test - s_test) * (T - t_test) / (T - s_test)\n        s_cond = np.sqrt(v_cond)\n\n        u_trans = norm.cdf((x_t_samples - m_cond) / s_cond)\n        ks_stat_trans, _ = kstest(u_trans, 'uniform')\n\n        # 2. Marginal probability integral transform test\n        x_t_star_samples = x_paths[:, t_star_idx]\n\n        # Theoretical marginal mean and variance at t_star_test\n        m_marg = a * (T - t_star_test) / T + b * t_star_test / T\n        v_marg = t_star_test * (T - t_star_test) / T\n        \n        # Failsafe for t_star=0 or t_star=T, though problem states t_star is interior.\n        if v_marg  0:\n            s_marg = np.sqrt(v_marg)\n            u_marg = norm.cdf((x_t_star_samples - m_marg) / s_marg)\n            ks_stat_marg, _ = kstest(u_marg, 'uniform')\n        else: # If variance is 0, the distribution is a Dirac delta. KS test is not applicable.\n              # Assign a failing stat. This case should not be reached with valid inputs.\n            ks_stat_marg = 1.0\n\n        # Check if both KS statistics are below the threshold.\n        passed = (ks_stat_trans  DELTA) and (ks_stat_marg  DELTA)\n        results.append(str(passed))\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3000113"}]}