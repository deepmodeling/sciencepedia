{"hands_on_practices": [{"introduction": "Understanding the evolution of a stochastic process is key to its application. This first practice challenges you to derive the future state of a Wiener process given its present location [@problem_id:3006300]. By applying the core axioms of independent and Gaussian increments, you will rigorously establish the process's fundamental Markov property, a cornerstone concept that simplifies modeling and prediction in countless applications.", "problem": "Let $\\left(\\Omega,\\mathcal{F},(\\mathcal{F}_{t})_{t\\ge 0},\\mathbb{P}\\right)$ be a filtered probability space carrying a standard Wiener process (also called a standard Brownian motion) $W=\\{W_{t}:t\\ge 0\\}$, defined by the following axioms: (i) $W_{0}=0$ almost surely; (ii) sample paths $t\\mapsto W_{t}$ are almost surely continuous; (iii) for any $0\\le r<s<t$, the increment $W_{t}-W_{s}$ is independent of $\\mathcal{F}_{r}$, where $\\mathcal{F}_{r}=\\sigma(W_{u}:0\\le u\\le r)$; and (iv) for any $0\\le s<t$, the increment $W_{t}-W_{s}$ is Gaussian with mean $0$ and variance $t-s$. Starting solely from these axioms and standard properties of Gaussian random variables and characteristic functions, derive the conditional distribution of $W_{t}$ given $W_{s}$ for $0\\le s<t$. Your final answer must be a single closed-form analytic expression for the conditional law of $W_{t}$ given $W_{s}$.", "solution": "The problem statement has been validated and is deemed sound, well-posed, and based on established principles of stochastic processes. We may proceed with the derivation.\n\nOur objective is to determine the conditional distribution of the random variable $W_{t}$ given the value of the random variable $W_{s}$, for a standard Wiener process $W$ and times $0 \\le s < t$. This is denoted as the law of $W_{t} | W_{s}$.\n\nWe begin by expressing the random variable $W_{t}$ in terms of $W_{s}$ and an increment. For any $s < t$, we can write the identity:\n$$\nW_{t} = W_{s} + (W_{t} - W_{s})\n$$\nWhen we condition on $W_{s}$, the value of $W_{s}$ is considered known. Therefore, the conditional distribution of $W_{t}$ is determined by the distribution of the increment $W_{t} - W_{s}$ and its relationship with $W_{s}$.\n\nThe cornerstone of this derivation lies in establishing the independence of the increment $W_{t} - W_{s}$ from the sigma-algebra $\\mathcal{F}_{s} = \\sigma(W_{u} : 0 \\le u \\le s)$. The random variable $W_{s}$ is, by definition, measurable with respect to $\\mathcal{F}_{s}$. Thus, showing that $W_{t} - W_{s}$ is independent of $\\mathcal{F}_{s}$ will imply that it is independent of $W_{s}$.\n\nAxiom (iii) states that for any $0 \\le r < s < t$, the increment $W_{t} - W_{s}$ is independent of the sigma-algebra $\\mathcal{F}_{r}$. We must extend this to show independence from $\\mathcal{F}_{s}$. Let us define the collection of sets $\\mathcal{A} = \\bigcup_{r<s} \\mathcal{F}_{r}$.\n1.  $\\mathcal{A}$ is a $\\pi$-system: If $A_{1} \\in \\mathcal{F}_{r_{1}}$ and $A_{2} \\in \\mathcal{F}_{r_{2}}$ with $r_{1} < s$ and $r_{2} < s$, then $A_{1} \\cap A_{2} \\in \\mathcal{F}_{\\max(r_{1}, r_{2})}$. Since $\\max(r_{1}, r_{2}) < s$, it follows that $A_{1} \\cap A_{2} \\in \\mathcal{A}$.\n2.  The sigma-algebra generated by $\\mathcal{A}$ is $\\mathcal{F}_{s}$. This is because $\\sigma(\\mathcal{A}) = \\sigma(\\bigcup_{r<s} \\mathcal{F}_{r}) = \\mathcal{F}_{s}$. The latter equality holds for filtrations generated by right-continuous processes like the Wiener process (whose paths are continuous almost surely by Axiom (ii)).\n3.  By Axiom (iii), for any set $A \\in \\mathcal{A}$, there exists an $r < s$ such that $A \\in \\mathcal{F}_{r}$. For this $r$, the increment $W_{t}-W_{s}$ is independent of $A$.\n4.  The collection of all events $B \\in \\mathcal{F}$ for which $W_{t} - W_{s}$ is independent of $B$ forms a $\\lambda$-system. Since this $\\lambda$-system contains the $\\pi$-system $\\mathcal{A}$, an application of Dynkin's $\\pi$-$\\lambda$ theorem allows us to conclude that it also contains $\\sigma(\\mathcal{A}) = \\mathcal{F}_{s}$.\n\nTherefore, the increment $W_{t} - W_{s}$ is independent of the sigma-algebra $\\mathcal{F}_{s}$, and consequently, it is independent of the random variable $W_{s}$.\n\nWith independence established, we can formally derive the conditional law using characteristic functions. Let $\\phi_{X}(k) = \\mathbb{E}[\\exp(ikX)]$ denote the characteristic function of a random variable $X$. We seek the conditional characteristic function of $W_{t}$ given $W_{s}=w_{s}$, denoted $\\phi_{W_{t}|W_{s}}(k|w_{s})$:\n$$\n\\phi_{W_{t}|W_{s}}(k|w_{s}) = \\mathbb{E}[\\exp(ikW_{t}) | W_{s}=w_{s}]\n$$\nSubstitute the decomposition $W_{t} = W_{s} + (W_{t}-W_{s})$:\n$$\n\\phi_{W_{t}|W_{s}}(k|w_{s}) = \\mathbb{E}[\\exp(ik(W_{s} + (W_{t}-W_{s}))) | W_{s}=w_{s}] = \\mathbb{E}[\\exp(ikW_{s})\\exp(ik(W_{t}-W_{s})) | W_{s}=w_{s}]\n$$\nBy the properties of conditional expectation, since we are conditioning on $W_{s}=w_{s}$, the term $\\exp(ikW_{s})$ becomes the constant $\\exp(ikw_{s})$ and can be factored out:\n$$\n\\phi_{W_{t}|W_{s}}(k|w_{s}) = \\exp(ikw_{s}) \\mathbb{E}[\\exp(ik(W_{t}-W_{s})) | W_{s}=w_{s}]\n$$\nBecause the increment $W_{t}-W_{s}$ is independent of $W_{s}$, the conditional expectation is equal to the unconditional expectation:\n$$\n\\mathbb{E}[\\exp(ik(W_{t}-W_{s})) | W_{s}=w_{s}] = \\mathbb{E}[\\exp(ik(W_{t}-W_{s}))]\n$$\nThis term is simply the characteristic function of the random variable $W_{t}-W_{s}$. According to Axiom (iv), for $s < t$, the increment $W_{t}-W_{s}$ follows a Gaussian (Normal) distribution with mean $0$ and variance $t-s$, i.e., $W_{t}-W_{s} \\sim \\mathcal{N}(0, t-s)$.\n\nThe characteristic function of a general Gaussian random variable $X \\sim \\mathcal{N}(\\mu, \\sigma^{2})$ is given by $\\phi_{X}(k) = \\exp(ik\\mu - \\frac{1}{2}k^{2}\\sigma^{2})$. For the increment $W_{t}-W_{s}$, we have $\\mu=0$ and $\\sigma^{2}=t-s$. Thus, its characteristic function is:\n$$\n\\mathbb{E}[\\exp(ik(W_{t}-W_{s}))] = \\exp\\left(ik(0) - \\frac{1}{2}k^{2}(t-s)\\right) = \\exp\\left(-\\frac{1}{2}k^{2}(t-s)\\right)\n$$\nSubstituting this result back into our expression for the conditional characteristic function of $W_{t}$:\n$$\n\\phi_{W_{t}|W_{s}}(k|w_{s}) = \\exp(ikw_{s}) \\exp\\left(-\\frac{1}{2}k^{2}(t-s)\\right) = \\exp\\left(ikw_{s} - \\frac{1}{2}k^{2}(t-s)\\right)\n$$\nWe now identify this resulting function. It has the form $\\exp(ik\\mu_{cond} - \\frac{1}{2}k^{2}\\sigma_{cond}^{2})$, which is the characteristic function of a Gaussian distribution. By comparing terms, we find the parameters of this conditional distribution:\n-   The conditional mean is $\\mu_{cond} = w_{s}$.\n-   The conditional variance is $\\sigma_{cond}^{2} = t-s$.\n\nThis shows that the distribution of $W_{t}$ conditional on $W_{s}=w_{s}$ is a normal distribution with mean $w_{s}$ and variance $t-s$. Expressed in terms of the random variable $W_{s}$, the conditional law of $W_{t}$ given $W_{s}$ is a normal distribution with mean $W_{s}$ and variance $t-s$. This is a complete specification of the conditional law.", "answer": "$$\n\\boxed{\\mathcal{N}(W_s, t-s)}\n$$", "id": "3006300"}, {"introduction": "While the sample paths of a Wiener process are continuous, they are notoriously \"rough\" and non-differentiable. This exercise invites you to quantify this characteristic by calculating the process's $p$-variation, a measure of its path irregularity [@problem_id:3006312]. This derivation uncovers the remarkable fact that the quadratic variation ($p=2$) converges to a deterministic value, a foundational result that explains why classical calculus is insufficient and motivates the entire framework of ItÃ´ calculus.", "problem": "Let $\\{W_{t}\\}_{t \\geq 0}$ be a standard Wiener process (also called standard Brownian motion) with respect to a filtration $\\{\\mathcal{F}_{t}\\}_{t \\geq 0}$ on a probability space $(\\Omega,\\mathcal{F},\\mathbb{P})$, defined by the following axioms: $W_{0}=0$ almost surely, $W$ has independent increments, $W_{t}-W_{s} \\sim \\mathcal{N}(0,t-s)$ for $0 \\leq s < t$ (mean zero, variance $t-s$), and $W$ has almost surely continuous sample paths. Fix a time horizon $t>0$ and, for each integer $n \\geq 1$, consider the uniform dyadic partition $\\mathcal{P}_{n}$ of $[0,t]$ given by points $t_{k}^{(n)} = k t / 2^{n}$ for $k=0,1,\\dots,2^{n}$.\n\nFor a real number $p \\geq 1$, define the $p$-variation along $\\mathcal{P}_{n}$ by\n$$\nV^{(p)}_{t}(W;\\mathcal{P}_{n}) \\;=\\; \\sum_{k=1}^{2^{n}} \\big|W_{t_{k}^{(n)}} - W_{t_{k-1}^{(n)}}\\big|^{p}.\n$$\nAssume the limit $V^{(p)}_{t}(W) = \\lim_{n \\to \\infty} V^{(p)}_{t}(W;\\mathcal{P}_{n})$ exists almost surely. Using only the Wiener process axioms, basic properties of the normal distribution, and standard probabilistic limit tools, determine $V^{(p)}_{t}(W)$ for $p>2$ and for $p=2$. Your derivation must start from the foundational definitions above and should not invoke any prepackaged results beyond those definitions and the elementary moments of the normal distribution. Express your final answer as a single closed-form piecewise analytic expression in terms of $p$ and $t$. No numerical approximation is required.", "solution": "The problem asks for the almost sure limit, $V^{(p)}_{t}(W)$, of the $p$-variation $V^{(p)}_{t}(W;\\mathcal{P}_{n})$ of a standard Wiener process $\\{W_t\\}_{t \\geq 0}$ along a sequence of dyadic partitions $\\mathcal{P}_n$ of the interval $[0,t]$. The problem is to be solved for $p>2$ and for $p=2$.\n\nLet the uniform dyadic partition $\\mathcal{P}_{n}$ of $[0,t]$ be defined by the points $t_{k}^{(n)} = \\frac{kt}{2^n}$ for $k=0, 1, \\dots, 2^n$. The length of each subinterval is constant:\n$$\n\\Delta t^{(n)} = t_{k}^{(n)} - t_{k-1}^{(n)} = \\frac{t}{2^n}.\n$$\nLet $\\Delta W_{k}^{(n)} = W_{t_{k}^{(n)}} - W_{t_{k-1}^{(n)}}$ be the increment of the Wiener process over the $k$-th subinterval. According to the axioms of the Wiener process, these increments are independent, and each is normally distributed with mean $0$ and variance equal to the length of the time interval. That is, for each $k \\in \\{1, \\dots, 2^n\\}$,\n$$\n\\Delta W_{k}^{(n)} \\sim \\mathcal{N}\\left(0, \\Delta t^{(n)}\\right).\n$$\nThis distributional property allows us to represent each increment as $\\Delta W_{k}^{(n)} = \\sqrt{\\Delta t^{(n)}} Z_k$, where $\\{Z_k\\}_{k=1}^{2^n}$ is a sequence of independent and identically distributed (i.i.d.) standard normal random variables, $Z_k \\sim \\mathcal{N}(0,1)$.\n\nThe $p$-variation along the partition $\\mathcal{P}_{n}$ is given by\n$$\nV^{(p)}_{t}(W;\\mathcal{P}_{n}) = \\sum_{k=1}^{2^{n}} \\left|\\Delta W_{k}^{(n)}\\right|^{p}.\n$$\nSubstituting the representation of the increments in terms of $Z_k$, we obtain\n$$\nV^{(p)}_{t}(W;\\mathcal{P}_{n}) = \\sum_{k=1}^{2^{n}} \\left|\\sqrt{\\Delta t^{(n)}} Z_k\\right|^{p} = \\sum_{k=1}^{2^{n}} \\left(\\Delta t^{(n)}\\right)^{p/2} |Z_k|^p = \\left(\\Delta t^{(n)}\\right)^{p/2} \\sum_{k=1}^{2^{n}} |Z_k|^p.\n$$\nReplacing $\\Delta t^{(n)}$ with $\\frac{t}{2^n}$, the expression becomes\n$$\nV^{(p)}_{t}(W;\\mathcal{P}_{n}) = \\left(\\frac{t}{2^n}\\right)^{p/2} \\sum_{k=1}^{2^{n}} |Z_k|^p.\n$$\nTo determine the limit of this expression as $n \\to \\infty$, we will analyze its behavior by examining its moments. Let $m_r = \\mathbb{E}[|Z|^r]$ denote the $r$-th absolute moment of a standard normal random variable. Since all moments of a normal distribution are finite, $m_r$ is a finite constant for any $r \\geq 0$.\n\nLet us compute the expectation of $V^{(p)}_{t}(W;\\mathcal{P}_{n})$. By the linearity of expectation and the fact that the $Z_k$ are i.i.d.,\n$$\n\\mathbb{E}\\left[V^{(p)}_{t}(W;\\mathcal{P}_{n})\\right] = \\mathbb{E}\\left[\\left(\\frac{t}{2^n}\\right)^{p/2} \\sum_{k=1}^{2^{n}} |Z_k|^p\\right] = \\left(\\frac{t}{2^n}\\right)^{p/2} \\sum_{k=1}^{2^{n}} \\mathbb{E}[|Z_k|^p] = \\left(\\frac{t}{2^n}\\right)^{p/2} (2^n) m_p.\n$$\nSimplifying the expression for the expectation yields\n$$\n\\mathbb{E}\\left[V^{(p)}_{t}(W;\\mathcal{P}_{n})\\right] = t^{p/2} \\frac{1}{(2^n)^{p/2-1}} m_p = t^{p/2} (2^n)^{1-p/2} m_p.\n$$\nWe now analyze the limit of this expectation for the two cases specified in the problem.\n\nCase 1: $p > 2$\nFor $p > 2$, the exponent $1 - \\frac{p}{2}$ is strictly negative. Consequently, as $n \\to \\infty$, the term $(2^n)^{1-p/2}$ converges to $0$.\n$$\n\\lim_{n \\to \\infty} \\mathbb{E}\\left[V^{(p)}_{t}(W;\\mathcal{P}_{n})\\right] = \\lim_{n \\to \\infty} t^{p/2} (2^n)^{1-p/2} m_p = 0.\n$$\nThe random variable $V^{(p)}_{t}(W;\\mathcal{P}_{n})$ is a sum of non-negative terms, so it is itself non-negative. For any sequence of non-negative random variables $X_n$, if $\\mathbb{E}[X_n] \\to 0$, then $X_n \\to 0$ in probability. This follows from Markov's inequality, which states that for any $\\epsilon > 0$,\n$$\n\\mathbb{P}\\left(V^{(p)}_{t}(W;\\mathcal{P}_{n}) \\geq \\epsilon\\right) \\leq \\frac{\\mathbb{E}\\left[V^{(p)}_{t}(W;\\mathcal{P}_{n})\\right]}{\\epsilon}.\n$$\nAs the right-hand side tends to $0$ as $n \\to \\infty$, we conclude that $V^{(p)}_{t}(W;\\mathcal{P}_{n})$ converges to $0$ in probability. The problem states that the limit $V^{(p)}_{t}(W) = \\lim_{n \\to \\infty} V^{(p)}_{t}(W;\\mathcal{P}_{n})$ exists almost surely. Almost sure convergence implies convergence in probability to the same limit. Since the limit in probability is unique, the almost sure limit must be $0$. Therefore,\n$$\nV^{(p)}_{t}(W) = 0 \\quad \\text{for } p > 2.\n$$\n\nCase 2: $p=2$\nFor $p=2$, the exponent $1 - \\frac{p}{2}$ is $1 - \\frac{2}{2} = 0$. The expectation becomes\n$$\n\\mathbb{E}\\left[V^{(2)}_{t}(W;\\mathcal{P}_{n})\\right] = t^{2/2} (2^n)^{1-1} m_2 = t \\cdot m_2.\n$$\nThe moment $m_2 = \\mathbb{E}[|Z|^2] = \\mathbb{E}[Z^2]$ for $Z \\sim \\mathcal{N}(0,1)$. The variance of a standard normal random variable is $\\mathrm{Var}(Z) = \\mathbb{E}[Z^2] - (\\mathbb{E}[Z])^2 = 1$. Since $\\mathbb{E}[Z]=0$, we have $\\mathbb{E}[Z^2] = 1$. Thus, $m_2=1$.\nThe expectation of the quadratic variation is constant for all $n$:\n$$\n\\mathbb{E}\\left[V^{(2)}_{t}(W;\\mathcal{P}_{n})\\right] = t.\n$$\nTo show that $V^{(2)}_{t}(W;\\mathcal{P}_{n})$ converges to the constant $t$, we analyze its variance. Convergence in mean square to a constant implies convergence in probability to that constant.\n$$\n\\mathrm{Var}\\left(V^{(2)}_{t}(W;\\mathcal{P}_{n})\\right) = \\mathrm{Var}\\left(\\sum_{k=1}^{2^{n}} \\left(\\Delta W_{k}^{(n)}\\right)^2\\right).\n$$\nSince the increments $\\Delta W_{k}^{(n)}$ are independent, the squared increments $\\left(\\Delta W_{k}^{(n)}\\right)^2$ are also independent. Thus, the variance of the sum is the sum of the variances:\n$$\n\\mathrm{Var}\\left(V^{(2)}_{t}(W;\\mathcal{P}_{n})\\right) = \\sum_{k=1}^{2^{n}} \\mathrm{Var}\\left(\\left(\\Delta W_{k}^{(n)}\\right)^2\\right).\n$$\nFor each term, $\\mathrm{Var}\\left(\\left(\\Delta W_{k}^{(n)}\\right)^2\\right) = \\mathbb{E}\\left[\\left(\\Delta W_{k}^{(n)}\\right)^4\\right] - \\left(\\mathbb{E}\\left[\\left(\\Delta W_{k}^{(n)}\\right)^2\\right]\\right)^2$.\nWe know $\\mathbb{E}\\left[\\left(\\Delta W_{k}^{(n)}\\right)^2\\right] = \\mathrm{Var}(\\Delta W_{k}^{(n)}) = \\Delta t^{(n)}$.\nThe fourth moment of a normal distribution $\\mathcal{N}(0,\\sigma^2)$ is $3\\sigma^4$. Here, $\\sigma^2 = \\Delta t^{(n)}$, so $\\mathbb{E}\\left[\\left(\\Delta W_{k}^{(n)}\\right)^4\\right] = 3\\left(\\Delta t^{(n)}\\right)^2$.\nSubstituting these moments into the variance expression for a single term:\n$$\n\\mathrm{Var}\\left(\\left(\\Delta W_{k}^{(n)}\\right)^2\\right) = 3\\left(\\Delta t^{(n)}\\right)^2 - \\left(\\Delta t^{(n)}\\right)^2 = 2\\left(\\Delta t^{(n)}\\right)^2.\n$$\nNow, summing over all $k=1, \\dots, 2^n$:\n$$\n\\mathrm{Var}\\left(V^{(2)}_{t}(W;\\mathcal{P}_{n})\\right) = \\sum_{k=1}^{2^{n}} 2\\left(\\Delta t^{(n)}\\right)^2 = 2^n \\cdot 2\\left(\\frac{t}{2^n}\\right)^2 = 2^{n+1} \\frac{t^2}{(2^n)^2} = \\frac{2t^2}{2^n}.\n$$\nAs $n \\to \\infty$, the variance converges to zero: $\\lim_{n \\to \\infty} \\mathrm{Var}\\left(V^{(2)}_{t}(W;\\mathcal{P}_{n})\\right) = 0$.\nThe sequence of random variables $V^{(2)}_{t}(W;\\mathcal{P}_{n})$ has a constant mean $t$ and a variance that tends to $0$. This implies convergence in $L^2$ and therefore in probability to the mean $t$. By the same logic as in case 1, the assumed almost sure limit must be equal to this limit. Therefore,\n$$\nV^{(2)}_{t}(W) = t.\n$$\nCombining both cases, we can express the p-variation $V^{(p)}_{t}(W)$ as a piecewise function of $p$.", "answer": "$$\n\\boxed{\n\\begin{cases}\nt & \\text{if } p=2 \\\\\n0 & \\text{if } p>2\n\\end{cases}\n}\n$$", "id": "3006312"}, {"introduction": "Many practical problems, from finance to engineering, hinge on understanding the extreme values a process might attain. This advanced practice guides you through the derivation of the distribution for the running supremum of a Wiener process using the elegant reflection principle [@problem_id:3006291]. This exercise not only provides a crucial formula but also illuminates the deep connection between probability theory and the physics of diffusion, demonstrating a powerful method for solving boundary-crossing problems.", "problem": "Let $\\{B_{t}\\}_{t \\ge 0}$ be a standard Wiener process (Brownian motion) constructed from the Gaussian heat kernel and satisfying the axioms: $B_{0}=0$ almost surely, continuous sample paths, independent and stationary increments, and Gaussian increments with $\\mathbb{P}(B_{t}-B_{s} \\in \\mathrm{d}x)=\\frac{1}{\\sqrt{2\\pi (t-s)}}\\exp\\!\\left(-\\frac{x^{2}}{2(t-s)}\\right)\\mathrm{d}x$ for $0 \\le s < t$. Let $T>0$ be fixed and define the running supremum $M_{T}:=\\sup_{0 \\le t \\le T} B_{t}$.\n\nUsing only the Wiener process axioms and its construction via the heat kernel, proceed as follows:\n\n1. For a level $m \\ge 0$ and an endpoint $y<m$, consider the Brownian bridge pinned at $0$ at time $t=0$ and at $y$ at time $t=T$, i.e., the process $\\{B_{t}\\}_{0 \\le t \\le T}$ conditioned on $B_{T}=y$. By relating this bridge representation to the boundary crossing event $\\{M_{T} < m\\}$ through the Gaussian heat kernel and the method of images with an absorbing barrier at $m$, derive the boundary crossing probability of the pinned bridge $\\mathbb{P}(M_{T}<m \\mid B_{T}=y)$ in terms of heat kernel expressions, without invoking any formulas beyond those implied by the Gaussian heat kernel and path reflection.\n\n2. Use the result of step 1 to compute the cumulative distribution function of $M_{T}$ at $m \\ge 0$, namely $\\mathbb{P}(M_{T} \\le m)$, by integrating the appropriate density over endpoints $y<m$ and identifying the contribution from paths that cross the barrier via the reflection mapping at the first hitting time.\n\nExpress your final answer as a single closed-form analytic expression in terms of the standard normal cumulative distribution function $\\Phi$. No numerical approximation or rounding is required.", "solution": "The problem is valid as it is a standard, well-posed problem in stochastic calculus concerning the distribution of the supremum of a standard Wiener process. It is scientifically grounded, objective, and contains all necessary information for a rigorous derivation.\n\nLet $\\{B_{t}\\}_{t \\ge 0}$ be a standard Wiener process starting at $B_{0}=0$. The probability density function of $B_{t}$ at position $x$ is given by the Gaussian heat kernel, $p(t, x)$, which is the fundamental solution to the one-dimensional heat equation $\\frac{\\partial u}{\\partial t} = \\frac{1}{2} \\frac{\\partial^2 u}{\\partial x^2}$. For a process starting at $0$ at time $0$, this density is:\n$$\np(t, x) = \\frac{1}{\\sqrt{2\\pi t}} \\exp\\left(-\\frac{x^2}{2t}\\right)\n$$\nThe problem asks for two derivations: first, the probability that the process stays below a level $m \\ge 0$ given its endpoint, and second, the unconditional probability of staying below this level.\n\n**Part 1: Derivation of the Pinned Bridge Probability**\n\nWe are asked to find $\\mathbb{P}(M_{T}<m \\mid B_{T}=y)$ for $y<m$, where $M_{T} = \\sup_{0 \\le t \\le T} B_{t}$. This conditional probability can be determined from the joint probability density of the event $\\{M_{T} < m\\}$ and the endpoint $\\{B_{T} = y\\}$.\n\nThe event $\\{M_{T} < m\\}$ means the path of the Wiener process does not cross the level $m$ for any time $t \\in [0, T]$. This is equivalent to solving the heat equation with an absorbing boundary condition at $x=m$. The joint probability density, which we denote as $q(T, y)$, for a path to end at $y$ at time $T$ *without* having touched or crossed the barrier at $m$, is the solution to:\n$$\n\\frac{\\partial q}{\\partial t} = \\frac{1}{2} \\frac{\\partial^2 q}{\\partial x^2}, \\quad \\text{for } x < m\n$$\nwith initial condition $q(0, x) = \\delta(x)$ (a Dirac delta function representing the start at $x=0$) and boundary condition $q(t, m) = 0$ for all $t > 0$.\n\nThis problem is solved using the method of images. We introduce a negative \"image\" source at $x=2m$ to cancel the effect of the original source at $x=0$ on the boundary $x=m$. The solution is the superposition of the heat kernel from the original source at $x=0$ and a negative heat kernel from the image source at $x=2m$.\n\nThe density from the source at $0$ is $p(T, y)$. The density from the image sink at $2m$ is $-p(T, y-2m)$. Thus, the joint probability density for $\\{M_T < m, B_T = y\\}$ is:\n$$\nq(T, y) = p(T, y) - p(T, y-2m)\n$$\nLet's verify the boundary condition. At $y=m$:\n$$\nq(T, m) = p(T, m) - p(T, m-2m) = p(T, m) - p(T, -m)\n$$\nSince the Gaussian kernel $p(T,x)$ is an even function of its spatial argument $x$, we have $p(T, m) = p(T, -m)$, which implies $q(T, m) = 0$. The method is consistent.\n\nThe probabilistic interpretation is that the set of all paths from $0$ to $y$ in time $T$ (with density $p(T, y)$) is partitioned into those that stay below $m$ and those that touch or cross $m$. By the reflection principle, the paths that start at $0$, touch $m$, and end at $y<m$ have the same measure as paths that start at $0$ and end at the reflected point $2m-y$. The density of these reflected paths is $p(T, 2m-y) = p(T, y-2m)$. Therefore, the density of paths that do not touch $m$ is the total density minus the density of paths that do, which is precisely $p(T, y) - p(T, y-2m)$.\n\nThe conditional probability $\\mathbb{P}(M_{T}<m \\mid B_{T}=y)$ is the ratio of the joint probability density to the marginal probability density of the conditioning event.\n$$\n\\mathbb{P}(M_{T}<m \\mid B_{T}=y) = \\frac{\\mathbb{P}(M_{T}<m, B_T \\in \\mathrm{d}y)}{\\mathbb{P}(B_T \\in \\mathrm{d}y)} = \\frac{q(T,y)\\mathrm{d}y}{p(T,y)\\mathrm{d}y} = \\frac{p(T, y) - p(T, y-2m)}{p(T, y)}\n$$\nSubstituting the explicit forms of the heat kernels:\n$$\n\\mathbb{P}(M_{T}<m \\mid B_{T}=y) = \\frac{\\frac{1}{\\sqrt{2\\pi T}}\\exp\\left(-\\frac{y^2}{2T}\\right) - \\frac{1}{\\sqrt{2\\pi T}}\\exp\\left(-\\frac{(y-2m)^2}{2T}\\right)}{\\frac{1}{\\sqrt{2\\pi T}}\\exp\\left(-\\frac{y^2}{2T}\\right)}\n$$\n$$\n= 1 - \\frac{\\exp\\left(-\\frac{(y-2m)^2}{2T}\\right)}{\\exp\\left(-\\frac{y^2}{2T}\\right)} = 1 - \\exp\\left(-\\frac{(y-2m)^2 - y^2}{2T}\\right)\n$$\n$$\n= 1 - \\exp\\left(-\\frac{y^2 - 4my + 4m^2 - y^2}{2T}\\right) = 1 - \\exp\\left(-\\frac{4m^2 - 4my}{2T}\\right) = 1 - \\exp\\left(-\\frac{2m(m-y)}{T}\\right)\n$$\nThis is the required boundary crossing probability for the pinned bridge.\n\n**Part 2: Cumulative Distribution Function of $M_T$**\n\nTo compute the cumulative distribution function (CDF) $\\mathbb{P}(M_{T} \\le m)$, we integrate the joint probability density $q(T, y)$ over all possible final positions $y$. Since the condition $M_{T} \\le m$ implies that the endpoint must satisfy $B_{T} \\le m$, the integration range for $y$ is $(-\\infty, m]$. Since $M_T$ is a continuous random variable, $\\mathbb{P}(M_T \\le m) = \\mathbb{P}(M_T < m)$.\n$$\n\\mathbb{P}(M_{T} \\le m) = \\int_{-\\infty}^{m} q(T, y) \\mathrm{d}y = \\int_{-\\infty}^{m} \\left[p(T, y) - p(T, y-2m)\\right] \\mathrm{d}y\n$$\nWe evaluate the integral as the difference of two integrals:\n$$\n\\mathbb{P}(M_{T} \\le m) = \\int_{-\\infty}^{m} p(T, y) \\mathrm{d}y - \\int_{-\\infty}^{m} p(T, y-2m) \\mathrm{d}y\n$$\nThe first integral is the probability that an unconstrained Wiener process ends at or below $m$:\n$$\n\\int_{-\\infty}^{m} p(T, y) \\mathrm{d}y = \\int_{-\\infty}^{m} \\frac{1}{\\sqrt{2\\pi T}} \\exp\\left(-\\frac{y^2}{2T}\\right) \\mathrm{d}y\n$$\nLet $z = y/\\sqrt{T}$, so $\\mathrm{d}z = \\mathrm{d}y/\\sqrt{T}$. The integral becomes:\n$$\n\\int_{-\\infty}^{m/\\sqrt{T}} \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{z^2}{2}\\right) \\mathrm{d}z = \\Phi\\left(\\frac{m}{\\sqrt{T}}\\right)\n$$\nwhere $\\Phi(\\cdot)$ is the CDF of the standard normal distribution.\n\nFor the second integral, which represents the probability mass of the reflected paths, we have:\n$$\n\\int_{-\\infty}^{m} p(T, y-2m) \\mathrm{d}y = \\int_{-\\infty}^{m} \\frac{1}{\\sqrt{2\\pi T}} \\exp\\left(-\\frac{(y-2m)^2}{2T}\\right) \\mathrm{d}y\n$$\nLet $w = (y-2m)/\\sqrt{T}$, so $\\mathrm{d}w = \\mathrm{d}y/\\sqrt{T}$. The limits of integration change: as $y \\to -\\infty$, $w \\to -\\infty$; when $y=m$, $w = (m-2m)/\\sqrt{T} = -m/\\sqrt{T}$. The integral becomes:\n$$\n\\int_{-\\infty}^{-m/\\sqrt{T}} \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{w^2}{2}\\right) \\mathrm{d}w = \\Phi\\left(-\\frac{m}{\\sqrt{T}}\\right)\n$$\nCombining the two results:\n$$\n\\mathbb{P}(M_{T} \\le m) = \\Phi\\left(\\frac{m}{\\sqrt{T}}\\right) - \\Phi\\left(-\\frac{m}{\\sqrt{T}}\\right)\n$$\nUsing the symmetry property of the standard normal CDF, $\\Phi(-x) = 1 - \\Phi(x)$:\n$$\n\\mathbb{P}(M_{T} \\le m) = \\Phi\\left(\\frac{m}{\\sqrt{T}}\\right) - \\left[1 - \\Phi\\left(\\frac{m}{\\sqrt{T}}\\right)\\right] = 2\\Phi\\left(\\frac{m}{\\sqrt{T}}\\right) - 1\n$$\nThis is the CDF of the running supremum $M_{T}$ for $m \\ge 0$.", "answer": "$$\\boxed{2\\Phi\\left(\\frac{m}{\\sqrt{T}}\\right) - 1}$$", "id": "3006291"}]}