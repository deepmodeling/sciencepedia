{"hands_on_practices": [{"introduction": "A multi-dimensional standard Brownian motion is defined as a vector of independent, one-dimensional Brownian motions. This practice explores a direct and fundamental consequence of this definition: the distribution of its magnitude. By working through this problem, you will connect the Gaussian nature of the individual components to the chi-square distribution of the squared Euclidean norm, a foundational result for understanding the geometry of random walks and a stepping stone to more advanced topics like Bessel processes. [@problem_id:2996356]", "problem": "Let $\\{W_{t}\\}_{t \\geq 0}$ be a $d$-dimensional standard Brownian motion, meaning that $W_{0} = 0$, it has independent and stationary increments, and for each fixed $t > 0$ the random vector $W_{t}$ is Gaussian with mean $0$ and covariance matrix $t I_{d}$, where $I_{d}$ is the $d \\times d$ identity matrix. Define the squared Euclidean norm at time $t$ by $R_{t}^{2} := \\|W_{t}\\|^{2} = \\sum_{i=1}^{d} \\left(W_{t}^{(i)}\\right)^{2}$, where $W_{t}^{(i)}$ denotes the $i$-th coordinate of $W_{t}$. Using only foundational properties of Brownian motion and Gaussian vectors, derive the probability distribution of $R_{t}^{2}$ and justify that it is equal in distribution to $t$ times a chi-square random variable with $d$ degrees of freedom. As your final answer, provide a single closed-form analytic expression for the moment generating function (MGF) $M_{R_{t}^{2}}(s) := \\mathbb{E}[\\exp(s R_{t}^{2})]$ as a function of $s$, $t$, and $d$, valid on its domain of finiteness. No numerical approximation is required.", "solution": "The problem statement is validated as scientifically grounded, well-posed, and objective. It is a standard problem in the theory of stochastic processes. All provided information is self-contained and consistent.\n\nLet $\\{W_{t}\\}_{t \\geq 0}$ be a $d$-dimensional standard Brownian motion. According to the problem definition, for any fixed time $t > 0$, the vector $W_{t}$ is a Gaussian random vector with mean vector $\\mathbb{E}[W_{t}] = 0$ and covariance matrix $\\text{Cov}(W_{t}) = t I_{d}$, where $I_{d}$ is the $d \\times d$ identity matrix.\n\nThe components of the vector $W_{t}$ are denoted by $W_{t} = (W_{t}^{(1)}, W_{t}^{(2)}, \\ldots, W_{t}^{(d)})$. The covariance matrix being $t I_{d}$ means that for any $i, j \\in \\{1, \\ldots, d\\}$:\n$$\n\\text{Cov}(W_{t}^{(i)}, W_{t}^{(j)}) = (t I_{d})_{ij} = \\begin{cases} t  \\text{if } i=j \\\\ 0  \\text{if } i \\neq j \\end{cases}\n$$\nSince the components of a Gaussian vector are uncorrelated if and only if they are independent, the random variables $W_{t}^{(1)}, W_{t}^{(2)}, \\ldots, W_{t}^{(d)}$ are mutually independent.\n\nFor each component $W_{t}^{(i)}$, its distribution is normal. The mean is the $i$-th component of the mean vector, which is $\\mathbb{E}[W_{t}^{(i)}] = 0$. The variance is the $i$-th diagonal element of the covariance matrix, which is $\\text{Var}(W_{t}^{(i)}) = t$. Therefore, for each $i \\in \\{1, \\ldots, d\\}$, the random variable $W_{t}^{(i)}$ follows a normal distribution $N(0, t)$.\n\nTo relate this to a chi-square distribution, we first need to standardize these normal random variables. A standard normal random variable, $Z$, has mean $0$ and variance $1$, i.e., $Z \\sim N(0, 1)$. Let us define a new set of random variables $Z_{i}$ for $i=1, \\ldots, d$:\n$$\nZ_{i} = \\frac{W_{t}^{(i)}}{\\sqrt{t}}\n$$\nThe mean of $Z_{i}$ is $\\mathbb{E}[Z_{i}] = \\frac{1}{\\sqrt{t}}\\mathbb{E}[W_{t}^{(i)}] = 0$. The variance of $Z_{i}$ is $\\text{Var}(Z_{i}) = \\frac{1}{(\\sqrt{t})^{2}}\\text{Var}(W_{t}^{(i)}) = \\frac{1}{t} \\cdot t = 1$. Since a linear transformation of a normal random variable is also normal, each $Z_{i}$ is a standard normal random variable, $Z_{i} \\sim N(0, 1)$. Furthermore, because the $W_{t}^{(i)}$ are mutually independent, the $Z_{i}$ are also mutually independent.\n\nThe chi-square distribution with $d$ degrees of freedom, denoted $\\chi^{2}(d)$, is defined as the distribution of the sum of the squares of $d$ independent standard normal random variables. Let $Y = \\sum_{i=1}^{d} Z_{i}^{2}$. By definition, $Y \\sim \\chi^{2}(d)$.\n\nNow, let's substitute the definition of $Z_{i}$ back into the sum:\n$$\nY = \\sum_{i=1}^{d} Z_{i}^{2} = \\sum_{i=1}^{d} \\left(\\frac{W_{t}^{(i)}}{\\sqrt{t}}\\right)^{2} = \\frac{1}{t} \\sum_{i=1}^{d} \\left(W_{t}^{(i)}\\right)^{2}\n$$\nThe sum in the final expression is precisely the definition of the squared Euclidean norm of $W_{t}$, given as $R_{t}^{2} := \\|W_{t}\\|^{2} = \\sum_{i=1}^{d} (W_{t}^{(i)})^{2}$.\nThus, we have established the relationship:\n$$\nY = \\frac{R_{t}^{2}}{t}\n$$\nSince $Y \\sim \\chi^{2}(d)$, it follows that $\\frac{R_{t}^{2}}{t} \\sim \\chi^{2}(d)$. This demonstrates that the random variable $R_{t}^{2}$ is equal in distribution to $t$ times a chi-square random variable with $d$ degrees of freedom.\n\nNext, we derive the moment generating function (MGF) of $R_{t}^{2}$, defined as $M_{R_{t}^{2}}(s) := \\mathbb{E}[\\exp(s R_{t}^{2})]$. We will derive this directly.\n$$\nM_{R_{t}^{2}}(s) = \\mathbb{E}\\left[\\exp\\left(s R_{t}^{2}\\right)\\right] = \\mathbb{E}\\left[\\exp\\left(s \\sum_{i=1}^{d} \\left(W_{t}^{(i)}\\right)^{2}\\right)\\right]\n$$\nUsing the property $\\exp(a+b) = \\exp(a)\\exp(b)$, we can write the exponential of a sum as a product of exponentials:\n$$\nM_{R_{t}^{2}}(s) = \\mathbb{E}\\left[\\prod_{i=1}^{d} \\exp\\left(s \\left(W_{t}^{(i)}\\right)^{2}\\right)\\right]\n$$\nSince the random variables $W_{t}^{(1)}, \\ldots, W_{t}^{(d)}$ are mutually independent, the expectation of their product is the product of their expectations:\n$$\nM_{R_{t}^{2}}(s) = \\prod_{i=1}^{d} \\mathbb{E}\\left[\\exp\\left(s \\left(W_{t}^{(i)}\\right)^{2}\\right)\\right]\n$$\nAs established, each $W_{t}^{(i)}$ is identically distributed as $N(0, t)$. Therefore, all the terms in the product are identical. Let $X$ be a random variable with distribution $N(0, t)$. The expression simplifies to:\n$$\nM_{R_{t}^{2}}(s) = \\left( \\mathbb{E}\\left[\\exp\\left(s X^{2}\\right)\\right] \\right)^{d}\n$$\nWe compute the expectation $\\mathbb{E}[\\exp(s X^{2})]$ using the probability density function (PDF) of $X \\sim N(0, t)$, which is $f_{X}(x) = \\frac{1}{\\sqrt{2\\pi t}} \\exp\\left(-\\frac{x^{2}}{2t}\\right)$.\n$$\n\\mathbb{E}[\\exp(s X^{2})] = \\int_{-\\infty}^{\\infty} \\exp(sx^{2}) f_{X}(x) dx = \\int_{-\\infty}^{\\infty} \\exp(sx^{2}) \\frac{1}{\\sqrt{2\\pi t}} \\exp\\left(-\\frac{x^{2}}{2t}\\right) dx\n$$\n$$\n= \\frac{1}{\\sqrt{2\\pi t}} \\int_{-\\infty}^{\\infty} \\exp\\left(sx^{2} - \\frac{x^{2}}{2t}\\right) dx = \\frac{1}{\\sqrt{2\\pi t}} \\int_{-\\infty}^{\\infty} \\exp\\left(-x^{2} \\left(\\frac{1}{2t} - s\\right)\\right) dx\n$$\nThis integral is a Gaussian integral of the form $\\int_{-\\infty}^{\\infty} \\exp(-ax^{2})dx = \\sqrt{\\frac{\\pi}{a}}$. For the integral to converge, the coefficient of $-x^{2}$ must be positive. This imposes the condition $\\frac{1}{2t} - s > 0$, which simplifies to $s  \\frac{1}{2t}$.\nLet $a = \\frac{1}{2t} - s = \\frac{1-2st}{2t}$. The value of the integral is $\\sqrt{\\frac{\\pi}{a}} = \\sqrt{\\frac{2\\pi t}{1-2st}}$.\nSubstituting this back into the expression for the expectation:\n$$\n\\mathbb{E}[\\exp(s X^{2})] = \\frac{1}{\\sqrt{2\\pi t}} \\sqrt{\\frac{2\\pi t}{1-2st}} = \\frac{1}{\\sqrt{1-2st}} = (1-2st)^{-1/2}\n$$\nFinally, we raise this result to the power of $d$ to find the MGF of $R_{t}^{2}$:\n$$\nM_{R_{t}^{2}}(s) = \\left((1-2st)^{-1/2}\\right)^{d} = (1-2st)^{-d/2}\n$$\nThis expression is valid for $s  \\frac{1}{2t}$, which is the domain of finiteness for the MGF. This result is consistent with the MGF of a Gamma-distributed random variable, specifically a scaled chi-square variable. A $\\chi^{2}(d)$ variable has MGF $(1-2u)^{-d/2}$. For $R_{t}^{2} \\sim t \\cdot \\chi^{2}(d)$, its MGF is $M_{R_{t}^{2}}(s) = \\mathbb{E}[\\exp(s(tY))] = \\mathbb{E}[\\exp((st)Y)] = M_{Y}(st) = (1-2st)^{-d/2}$, where $Y \\sim \\chi^{2}(d)$, confirming our direct calculation.\n\nThe final closed-form analytic expression for the moment generating function is $(1-2st)^{-d/2}$.", "answer": "$$\n\\boxed{(1 - 2st)^{-d/2}}\n$$", "id": "2996356"}, {"introduction": "Beyond its distributional properties at fixed times, Brownian motion is fundamentally a martingale, a process whose expected future value, given all information up to the present, is simply its current value. This \"fair game\" property is one of its most powerful attributes. This exercise challenges you to apply the Optional Stopping Theorem to determine the expected value of a Brownian motion at a random stopping time $\\tau$, revealing that the martingale property does not always extend naively and requires careful justification through conditions like uniform integrability. [@problem_id:2996340]", "problem": "Let $\\{B_{t}\\}_{t \\geq 0}$ be a standard Brownian motion on a filtered probability space $(\\Omega,\\mathcal{F},\\{\\mathcal{F}_{t}\\}_{t \\geq 0},\\mathbb{P})$ satisfying the usual conditions, with $B_{0}=0$. Recall that $B_{t}$ is a continuous $\\{\\mathcal{F}_{t}\\}$-martingale with quadratic variation $\\langle B\\rangle_{t}=t$, and that $M_{t}=B_{t}^{2}-t$ is also a $\\{\\mathcal{F}_{t}\\}$-martingale. Let $\\tau$ be an $\\{\\mathcal{F}_{t}\\}$-stopping time.\n\n1. Assume that $\\tau$ is almost surely bounded by a deterministic constant $T \\in (0,\\infty)$. By working from first principles of martingale theory and using only widely accepted results about stopped martingales, determine the value of $\\mathbb{E}[B_{\\tau}]$.\n\n2. Now drop the boundedness assumption and suppose instead that $\\mathbb{E}[\\tau]  \\infty$. Analyze the family $\\{B_{t \\wedge \\tau}\\}_{t \\geq 0}$ and derive a sufficient integrability condition, expressed only in terms of $\\tau$, that ensures this family is uniformly integrable. Use this to pass to the limit $t \\to \\infty$ along an increasing deterministic sequence and determine the value of $\\mathbb{E}[B_{\\tau}]$ in this case as well.\n\nProvide the final value of $\\mathbb{E}[B_{\\tau}]$ as a single exact number or closed-form expression. No rounding is required, and no units are involved.", "solution": "We work with the basic properties of standard Brownian motion and classical facts about martingales and stopped processes.\n\nFor Item 1, we assume $\\tau \\leq T$ almost surely for some finite $T$. The process $\\{B_{t}\\}_{t \\geq 0}$ is a martingale with respect to $\\{\\mathcal{F}_{t}\\}$. For any bounded stopping time $\\tau$, the stopped process $\\{B_{t \\wedge \\tau}\\}_{t \\geq 0}$ is a uniformly integrable martingale. In particular, for any $t \\geq 0$,\n$$\n\\mathbb{E}[B_{t \\wedge \\tau}] = \\mathbb{E}[B_{0}] = 0.\n$$\nLet $t \\uparrow T$ along any increasing sequence. Since $\\tau \\leq T$ almost surely and $B_{t}$ has continuous sample paths, we have $B_{t \\wedge \\tau} \\to B_{\\tau}$ almost surely as $t \\uparrow T$. Moreover, uniform integrability of $\\{B_{t \\wedge \\tau}\\}_{t \\in [0,T]}$ ensures convergence in $L^{1}$, hence passing to the limit in expectations yields\n$$\n\\mathbb{E}[B_{\\tau}] = \\lim_{t \\uparrow T} \\mathbb{E}[B_{t \\wedge \\tau}] = 0.\n$$\n\nFor Item 2, we no longer assume that $\\tau$ is bounded, but we assume $\\mathbb{E}[\\tau]  \\infty$. Consider the family $\\{B_{t \\wedge \\tau}\\}_{t \\geq 0}$. To obtain a uniform integrability criterion expressed in terms of $\\tau$, we use the square-integrable martingale $M_{t} = B_{t}^{2} - t$. For any fixed $t \\geq 0$, the random time $t \\wedge \\tau$ is bounded by $t$, hence we may apply the optional stopping theorem to the martingale $\\{M_{s}\\}_{s \\geq 0}$ at the bounded stopping time $t \\wedge \\tau$:\n$$\n\\mathbb{E}[M_{t \\wedge \\tau}] = \\mathbb{E}[M_{0}] = 0.\n$$\nThus,\n$$\n\\mathbb{E}[B_{t \\wedge \\tau}^{2}] = \\mathbb{E}[t \\wedge \\tau] \\leq \\mathbb{E}[\\tau]  \\infty.\n$$\nTaking the supremum in $t$ gives\n$$\n\\sup_{t \\geq 0} \\mathbb{E}\\big[|B_{t \\wedge \\tau}|^{2}\\big] \\leq \\mathbb{E}[\\tau]  \\infty.\n$$\nThis shows that the family $\\{B_{t \\wedge \\tau}\\}_{t \\geq 0}$ is bounded in $L^{2}$. By the de la Vallée Poussin criterion (or the elementary fact that $L^{p}$-boundedness for some $p1$ implies uniform integrability), the family $\\{B_{t \\wedge \\tau}\\}_{t \\geq 0}$ is uniformly integrable.\n\nNow define the increasing sequence of bounded stopping times $\\tau_{n} := \\tau \\wedge n$, $n \\in \\mathbb{N}$. For each $n$, by the argument of Item 1 applied with $T=n$, we have\n$$\n\\mathbb{E}[B_{\\tau_{n}}] = 0.\n$$\nWe also have $B_{\\tau_{n}} \\to B_{\\tau}$ almost surely as $n \\to \\infty$ because $B$ has continuous paths and $\\tau_{n} \\uparrow \\tau$. The uniform integrability of $\\{B_{t \\wedge \\tau}\\}_{t \\geq 0}$ in particular implies that $\\{B_{\\tau_{n}}\\}_{n \\in \\mathbb{N}}$ is uniformly integrable. Therefore $B_{\\tau_{n}} \\to B_{\\tau}$ in $L^{1}$, and we can pass to the limit in expectations:\n$$\n\\mathbb{E}[B_{\\tau}] = \\lim_{n \\to \\infty} \\mathbb{E}[B_{\\tau_{n}}] = 0.\n$$\n\nIn summary:\n- If $\\tau$ is bounded, then $\\mathbb{E}[B_{\\tau}] = 0$.\n- If $\\mathbb{E}[\\tau]  \\infty$, then $\\{B_{t \\wedge \\tau}\\}_{t \\geq 0}$ is uniformly integrable, which again yields $\\mathbb{E}[B_{\\tau}] = 0$ by approximation with $\\tau_{n} = \\tau \\wedge n$.\n\nTherefore, in both cases considered, the exact value of $\\mathbb{E}[B_{\\tau}]$ is zero.", "answer": "$$\\boxed{0}$$", "id": "2996340"}, {"introduction": "The rough, non-differentiable nature of Brownian paths prevents the use of ordinary calculus. Stochastic calculus provides the framework for differentiation and integration, but it comes in two primary forms: the Itô and the Stratonovich calculus. This exercise tasks you with deriving the crucial relationship between these two systems from first principles, starting with their definitions as limits of Riemann sums. By deriving the famous drift correction term, you will gain a deep appreciation for the subtleties of calculus in a random world and the practical skills needed to convert between these two essential modeling frameworks. [@problem_id:2996333]", "problem": "Let $(\\Omega,\\mathcal{F},(\\mathcal{F}_{t})_{t\\ge 0},\\mathbb{P})$ be a filtered probability space supporting a standard Brownian motion $B=(B_{t})_{t\\ge 0}$. Consider the one-dimensional Stratonovich stochastic differential equation (SDE)\n$$\ndX_{t}=f(X_{t})\\,dt+g(X_{t})\\circ dB_{t}, \\quad X_{0}=x_{0},\n$$\nwhere $f:\\mathbb{R}\\to\\mathbb{R}$ and $g:\\mathbb{R}\\to\\mathbb{R}$ are twice continuously differentiable with at most linear growth and are globally Lipschitz, ensuring the existence of a unique strong solution with continuous paths. Throughout, interpret $dB_{t}$ as the increment of standard Brownian motion, and $\\circ dB_{t}$ as the Stratonovich differential with the integral defined via midpoint Riemann sums.\n\nStarting only from the fundamental definitions of the Itô and Stratonovich stochastic integrals as limits in probability of adapted Riemann sums, and from the defining properties of standard Brownian motion (independent, centered Gaussian increments with variance equal to elapsed time and quadratic variation $[B]_{t}=t$), carry out the following:\n\n1. Derive the pathwise relation between the Stratonovich integral $\\int_{0}^{t}g(X_{s})\\circ dB_{s}$ and the Itô integral $\\int_{0}^{t}g(X_{s})\\,dB_{s}$ by comparing midpoint and left-point Riemann sums and by controlling all remainder terms using Gaussian moment estimates for Brownian increments and the continuity of $X$.\n2. Use your derived relation to convert the given Stratonovich SDE into its Itô form $dX_{t}=\\tilde{f}(X_{t})\\,dt+g(X_{t})\\,dB_{t}$, explicitly identifying the drift correction term that modifies $f$.\n\nProvide your final answer as the single closed-form analytical expression for the drift correction term $\\tilde{f}(X_{t})-f(X_{t})$ in terms of $g$ and its derivative evaluated at $X_{t}$. Do not include any intermediate equations in your final answer, only the requested expression. No numerical rounding is required for this problem.", "solution": "The problem asks for the derivation of the relationship between a Stratonovich stochastic differential equation (SDE) and its Itô equivalent, starting from the Riemann sum definitions of the respective stochastic integrals. The final goal is to identify the drift correction term.\n\nLet the Stratonovich SDE be\n$$\ndX_{t}=f(X_{t})\\,dt+g(X_{t})\\circ dB_{t}, \\quad X_{0}=x_{0}\n$$\nIn integral form, this is\n$$\nX_{t} = x_{0} + \\int_{0}^{t} f(X_{s})\\,ds + \\int_{0}^{t} g(X_{s})\\circ dB_{s}\n$$\nThe task is to find an expression for $\\tilde{f}$ such that the same process $X_t$ satisfies the Itô SDE\n$$\ndX_{t}=\\tilde{f}(X_{t})\\,dt+g(X_{t})\\,dB_{t}\n$$\nwhich in integral form is\n$$\nX_{t} = x_{0} + \\int_{0}^{t} \\tilde{f}(X_{s})\\,ds + \\int_{0}^{t} g(X_{s})\\,dB_{s}\n$$\nBy comparing the two integral equations, we see that we need to find the relationship between the Stratonovich integral $\\int_{0}^{t} g(X_{s})\\circ dB_{s}$ and the Itô integral $\\int_{0}^{t} g(X_{s})\\,dB_{s}$.\n\nLet us consider a partition of the interval $[0, t]$, denoted by $\\Pi = \\{0=t_{0}  t_{1}  \\dots  t_{n}=t\\}$. Let $\\Delta t_{i} = t_{i+1}-t_{i}$ and let the mesh of the partition be $|\\Pi| = \\max_{i} \\Delta t_{i}$. Let $\\Delta B_{i} = B_{t_{i+1}}-B_{t_{i}}$.\n\nThe Itô integral is defined as the limit in probability of the left-point Riemann sum:\n$$\n\\int_{0}^{t} g(X_{s})\\,dB_{s} = \\lim_{|\\Pi|\\to 0} \\sum_{i=0}^{n-1} g(X_{t_{i}}) \\Delta B_{i}\n$$\nThe Stratonovich integral is defined as the limit in probability of the midpoint Riemann sum:\n$$\n\\int_{0}^{t} g(X_{s})\\circ dB_{s} = \\lim_{|\\Pi|\\to 0} \\sum_{i=0}^{n-1} g(X_{\\frac{t_{i}+t_{i+1}}{2}}) \\Delta B_{i}\n$$\nLet us analyze the difference between the two sums for a given partition $\\Pi$:\n$$\nD_{n} = \\sum_{i=0}^{n-1} g(X_{\\frac{t_{i}+t_{i+1}}{2}}) \\Delta B_{i} - \\sum_{i=0}^{n-1} g(X_{t_{i}}) \\Delta B_{i} = \\sum_{i=0}^{n-1} \\left[ g(X_{\\frac{t_{i}+t_{i+1}}{2}}) - g(X_{t_{i}}) \\right] \\Delta B_{i}\n$$\nSince $g$ is assumed to be twice continuously differentiable ($C^{2}$), we can apply Taylor's theorem to the term in the brackets around the point $X_{t_{i}}$:\n$$\ng(X_{\\frac{t_{i}+t_{i+1}}{2}}) - g(X_{t_{i}}) = g'(X_{t_{i}}) \\left( X_{\\frac{t_{i}+t_{i+1}}{2}} - X_{t_{i}} \\right) + \\frac{1}{2} g''(\\xi_{i}) \\left( X_{\\frac{t_{i}+t_{i+1}}{2}} - X_{t_{i}} \\right)^{2}\n$$\nwhere $\\xi_{i}$ is a random variable on the path segment between $X_{t_{i}}$ and $X_{\\frac{t_{i}+t_{i+1}}{2}}$.\nSubstituting this into the expression for $D_{n}$:\n$$\nD_{n} = \\sum_{i=0}^{n-1} g'(X_{t_{i}}) \\left( X_{\\frac{t_{i}+t_{i+1}}{2}} - X_{t_{i}} \\right) \\Delta B_{i} + \\sum_{i=0}^{n-1} \\frac{1}{2} g''(\\xi_{i}) \\left( X_{\\frac{t_{i}+t_{i+1}}{2}} - X_{t_{i}} \\right)^{2} \\Delta B_{i}\n$$\nLet's analyze the increment $X_{\\frac{t_{i}+t_{i+1}}{2}} - X_{t_{i}}$. From the integral SDE, for any $s \\in [t_{i}, t_{i+1}]$, we have:\n$$\nX_{s} - X_{t_{i}} = \\int_{t_{i}}^{s} f(X_{u})\\,du + \\int_{t_{i}}^{s} g(X_{u})\\circ dB_{u}\n$$\nFor a small time step, we can approximate the integrals. The leading order behavior is given by:\n$$\nX_{\\frac{t_{i}+t_{i+1}}{2}} - X_{t_{i}} \\approx f(X_{t_i}) \\frac{\\Delta t_i}{2} + g(X_{t_i}) (B_{\\frac{t_{i}+t_{i+1}}{2}} - B_{t_i})\n$$\nThe Itô and Stratonovich increments differ by a term of order $\\Delta t_{i}$, so this approximation is sufficient for our purpose. Substituting this approximation into the first sum of $D_n$:\n\\begin{align*}\n\\sum_{i=0}^{n-1} g'(X_{t_{i}}) \\left( X_{\\frac{t_{i}+t_{i+1}}{2}} - X_{t_{i}} \\right) \\Delta B_{i}  \\approx \\sum_{i=0}^{n-1} g'(X_{t_{i}}) \\left[ f(X_{t_{i}}) \\frac{\\Delta t_{i}}{2} + g(X_{t_{i}}) (B_{\\frac{t_{i}+t_{i+1}}{2}} - B_{t_{i}}) \\right] \\Delta B_{i} \\\\\n = \\sum_{i=0}^{n-1} g'(X_{t_{i}})f(X_{t_{i}})\\frac{\\Delta t_{i}}{2} \\Delta B_{i} + \\sum_{i=0}^{n-1} g'(X_{t_{i}})g(X_{t_{i}})(B_{\\frac{t_{i}+t_{i+1}}{2}} - B_{t_{i}})\\Delta B_{i}\n\\end{align*}\nThe first term, $\\sum_{i=0}^{n-1} g'(X_{t_{i}})f(X_{t_{i}})\\frac{\\Delta t_{i}}{2} \\Delta B_{i}$, is an Itô-type sum where the integrand is multiplied by $\\Delta t_i$. The variance of this sum is proportional to $\\sum (\\Delta t_i)^2 \\Delta t_i = \\sum (\\Delta t_i)^3$, which goes to zero as $|\\Pi| \\to 0$. Thus, this sum converges to $0$ in probability.\n\nThe second term is the crucial one: $\\sum_{i=0}^{n-1} g'(X_{t_{i}})g(X_{t_{i}})(B_{\\frac{t_{i}+t_{i+1}}{2}} - B_{t_{i}})\\Delta B_{i}$. Let $H_{i} = g'(X_{t_i})g(X_{t_i})$ which is $\\mathcal{F}_{t_i}$-measurable. We analyze the sum\n$$\nA_{n} = \\sum_{i=0}^{n-1} H_{i} (B_{\\frac{t_{i}+t_{i+1}}{2}} - B_{t_{i}}) (B_{t_{i+1}} - B_{t_{i}})\n$$\nLet's decompose the second Brownian increment: $B_{t_{i+1}} - B_{t_{i}} = (B_{t_{i+1}} - B_{\\frac{t_{i}+t_{i+1}}{2}}) + (B_{\\frac{t_{i}+t_{i+1}}{2}} - B_{t_{i}})$. Substituting this in:\n$$\nA_{n} = \\sum_{i=0}^{n-1} H_{i} (B_{\\frac{t_{i}+t_{i+1}}{2}} - B_{t_{i}})^2 + \\sum_{i=0}^{n-1} H_{i} (B_{\\frac{t_{i}+t_{i+1}}{2}} - B_{t_{i}})(B_{t_{i+1}} - B_{\\frac{t_{i}+t_{i+1}}{2}})\n$$\nThe second sum consists of terms where the product of Brownian increments are over adjacent, non-overlapping intervals. By the property of independent increments of Brownian motion, $\\mathbb{E}[(B_{s}-B_{r})(B_{u}-B_{s}) | \\mathcal{F}_{r}]=0$ for $rsu$. The terms in this sum have conditional expectation zero, and the sum converges to $0$ in probability.\n\nThe first sum is $\\sum_{i=0}^{n-1} H_{i} (B_{\\frac{t_{i}+t_{i+1}}{2}} - B_{t_{i}})^{2}$. We know that for a continuous adapted process $Y_s$, the sum $\\sum_{i=0}^{n-1} Y_{t_i} ((\\Delta B_i)^2 - \\Delta t_i)$ converges to $0$ in probability. Here the increment is over a time step of length $\\frac{\\Delta t_i}{2}$. So we have:\n$$\n\\sum_{i=0}^{n-1} H_{i} \\left[ (B_{\\frac{t_{i}+t_{i+1}}{2}} - B_{t_{i}})^{2} - \\frac{\\Delta t_{i}}{2} \\right] \\to 0 \\quad \\text{in probability as } |\\Pi|\\to 0.\n$$\nThis means that the sum $\\sum_{i=0}^{n-1} H_{i} (B_{\\frac{t_i+t_{i+1}}{2}} - B_{t_i})^2$ converges to the same limit as the sum $\\sum_{i=0}^{n-1} H_{i} \\frac{\\Delta t_i}{2}$. This latter sum is a standard Riemann sum for the integral of $H_s/2 = g'(X_s)g(X_s)/2$:\n$$\n\\lim_{|\\Pi|\\to 0} \\sum_{i=0}^{n-1} H_{i} \\frac{\\Delta t_{i}}{2} = \\int_{0}^{t} \\frac{1}{2} g'(X_{s})g(X_{s}) \\, ds\n$$\nThe remainder term from the Taylor expansion involves terms like $(\\Delta X)^2 \\Delta B$. The squared increment $(X_{\\frac{t_{i}+t_{i+1}}{2}}-X_{t_{i}})^2$ is of order $O(\\Delta t_i)$. The product with $\\Delta B_i$ is of order $O((\\Delta t_i)^{3/2})$, and the sum over these terms will vanish in the limit.\nTherefore, taking the limit as $|\\Pi| \\to 0$, we find that the difference $D_n$ converges in probability to this integral:\n$$\n\\lim_{|\\Pi|\\to 0} D_{n} = \\frac{1}{2} \\int_{0}^{t} g'(X_{s})g(X_{s}) \\, ds\n$$\nThis establishes the fundamental relationship between the Itô and Stratonovich integrals:\n$$\n\\int_{0}^{t} g(X_{s})\\circ dB_{s} = \\int_{0}^{t} g(X_{s})\\,dB_{s} + \\frac{1}{2} \\int_{0}^{t} g'(X_{s})g(X_{s}) \\, ds\n$$\nNow we can convert the original Stratonovich SDE. Substituting this expression into the integral form of the Stratonovich SDE:\n$$\nX_{t} = x_{0} + \\int_{0}^{t} f(X_{s})\\,ds + \\left( \\int_{0}^{t} g(X_{s})\\,dB_{s} + \\frac{1}{2} \\int_{0}^{t} g'(X_{s})g(X_{s}) \\, ds \\right)\n$$\nCombining the terms under the $ds$ integral:\n$$\nX_{t} = x_{0} + \\int_{0}^{t} \\left( f(X_{s}) + \\frac{1}{2} g(X_{s})g'(X_{s}) \\right) ds + \\int_{0}^{t} g(X_{s})\\,dB_{s}\n$$\nThis is the integral form of the corresponding Itô SDE, $dX_{t}=\\tilde{f}(X_{t})\\,dt+g(X_{t})\\,dB_{t}$, where the Itô drift is\n$$\n\\tilde{f}(X_{t}) = f(X_{t}) + \\frac{1}{2} g(X_{t})g'(X_{t})\n$$\nThe problem asks for the drift correction term, which is the difference between the Itô drift $\\tilde{f}$ and the Stratonovich drift $f$.\n$$\n\\tilde{f}(X_{t}) - f(X_{t}) = \\frac{1}{2} g(X_{t})g'(X_{t})\n$$\nThis term is often called the Itô-Stratonovich correction term or simply the correction drift.", "answer": "$$\n\\boxed{\\frac{1}{2} g(X_{t}) g'(X_{t})}\n$$", "id": "2996333"}]}