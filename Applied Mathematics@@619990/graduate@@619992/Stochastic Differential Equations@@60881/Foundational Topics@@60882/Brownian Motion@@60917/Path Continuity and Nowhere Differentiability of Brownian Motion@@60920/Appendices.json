{"hands_on_practices": [{"introduction": "This practice serves as our entry point into the paradoxical nature of Brownian motion paths. While we know these paths are continuous, the classical notion of a derivative, familiar from calculus, does not apply. Here, we investigate differentiability in the mean-square sense by examining the second moment of the difference quotient, and discover that it diverges, providing a first rigorous proof that Brownian paths are fundamentally \"rough\" and non-differentiable [@problem_id:2990282]. This result is a cornerstone for understanding why the derivative of Brownian motion is not a classical function but a generalized process known as white noise.", "problem": "Let $\\{B_{t}\\}_{t \\geq 0}$ be a standard one-dimensional Brownian motion, characterized by $B_{0} = 0$, stationary and independent increments, and Gaussian increments with $\\mathbb{E}[B_{t}] = 0$ and $\\operatorname{Var}(B_t) = t$ for all $t \\geq 0$. Fix $t  0$. Consider the difference quotient $\\frac{B_{t+h} - B_{t}}{h}$ and its second moment $\\mathbb{E}\\!\\left[\\left(\\frac{B_{t+h} - B_{t}}{h}\\right)^{2}\\right]$. \n\nStarting from the fundamental properties of Brownian motion stated above and basic identities for variance and expectation of Gaussian random variables, compute the limit\n$$\n\\lim_{h \\to 0^{+}} \\mathbb{E}\\!\\left[\\left(\\frac{B_{t+h} - B_{t}}{h}\\right)^{2}\\right],\n$$\nand determine whether this limit is finite. If the limit diverges, report $+\\infty$ as the value of the limit.\n\nThen, explain how the behavior of this limit supports the interpretation of the generalized derivative $\\dot{B}_{t}$ of Brownian motion as Gaussian white noise (GWN), understood in the sense of Schwartz distributions: for a smooth compactly supported test function $\\varphi$, the pairing $\\langle \\dot{B}, \\varphi \\rangle$ is defined so that its covariance is given by $\\mathbb{E}[\\langle \\dot{B}, \\varphi \\rangle \\langle \\dot{B}, \\psi \\rangle] = \\int_{0}^{\\infty} \\varphi(s)\\psi(s)\\, \\mathrm{d}s$, which is equivalent to saying that the covariance distribution of $\\dot{B}$ is the Dirac delta $\\delta$.\n\nGive your final answer for the limit as an exact expression. No rounding is required.", "solution": "The user wants to solve a problem related to the differentiability of Brownian motion, which involves two parts:\n1.  Compute the limit of the second moment of the difference quotient of a standard Brownian motion path.\n2.  Explain how this result relates to the interpretation of the derivative of Brownian motion as Gaussian white noise.\n\n### Step-by-Step Problem Validation\n\n**Step 1: Extract Givens**\n-   $\\{B_{t}\\}_{t \\geq 0}$ is a standard one-dimensional Brownian motion.\n-   $B_{0} = 0$.\n-   Increments are stationary and independent.\n-   Increments are Gaussian with $\\mathbb{E}[B_{t}] = 0$ and $\\operatorname{Var}(B_{t}) = t$ for all $t \\geq 0$.\n-   Task 1: Compute the limit $\\lim_{h \\to 0^{+}} \\mathbb{E}\\!\\left[\\left(\\frac{B_{t+h} - B_{t}}{h}\\right)^{2}\\right]$.\n-   Task 2: Explain how the behavior of this limit supports the interpretation of the generalized derivative $\\dot{B}_{t}$ as Gaussian white noise (GWN), whose covariance distribution is the Dirac delta function, i.e., $\\mathbb{E}[\\dot{B}_t \\dot{B}_s] = \\delta(t-s)$.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is scientifically and mathematically sound. The properties listed for standard Brownian motion are its formal definition. The question about the limit of the difference quotient is a standard method to investigate the path regularity (specifically, differentiability) of a stochastic process. The connection to Gaussian white noise as the distributional derivative of Brownian motion is a cornerstone concept in stochastic calculus and the theory of generalized processes. The problem is well-posed, objective, and self-contained. It contains no contradictions, ambiguities, or non-formalizable elements. It is a standard and non-trivial problem in the field of stochastic differential equations.\n\n**Step 3: Verdict and Action**\nThe problem is valid. A complete solution will be provided.\n\n### Part 1: Computation of the Limit\n\nWe are asked to compute the limit of the second moment of the difference quotient for a standard Brownian motion $\\{B_t\\}_{t \\geq 0}$ at a fixed time $t  0$. The expression is:\n$$\n\\lim_{h \\to 0^{+}} \\mathbb{E}\\!\\left[\\left(\\frac{B_{t+h} - B_{t}}{h}\\right)^{2}\\right]\n$$\nWe begin by analyzing the expression inside the limit, which is the expectation of the squared difference quotient. Using the property of expectation that for a constant $c$, $\\mathbb{E}[c^2 X^2] = c^2 \\mathbb{E}[X^2]$, we can write:\n$$\n\\mathbb{E}\\!\\left[\\left(\\frac{B_{t+h} - B_{t}}{h}\\right)^{2}\\right] = \\frac{1}{h^2} \\mathbb{E}\\!\\left[(B_{t+h} - B_{t})^2\\right]\n$$\nThe term $\\mathbb{E}\\!\\left[(B_{t+h} - B_{t})^2\\right]$ is the second moment of the random variable representing the increment of the Brownian motion over the interval $[t, t+h]$. For any random variable $X$, its second moment is related to its variance and mean by the identity $\\mathbb{E}[X^2] = \\operatorname{Var}(X) + (\\mathbb{E}[X])^2$.\n\nLet's apply this to the increment $X = B_{t+h} - B_{t}$. We need to find its mean and variance.\n-   **Mean of the increment**: Brownian motion has stationary increments, which means the distribution of $B_{t+h} - B_{t}$ is the same as the distribution of $B_{(t+h)-t} - B_{0}$. Since $B_0=0$, this is the distribution of $B_h$. The problem states that $\\mathbb{E}[B_s] = 0$ for all $s \\geq 0$. Therefore,\n    $$\n    \\mathbb{E}[B_{t+h} - B_t] = \\mathbb{E}[B_h] = 0\n    $$\n-   **Variance of the increment**: Due to stationary increments, the variance of the increment is also determined by its duration.\n    $$\n    \\operatorname{Var}(B_{t+h} - B_t) = \\operatorname{Var}(B_h)\n    $$\n    The problem states that $\\operatorname{Var}(B_s) = s$ for all $s \\geq 0$. For the duration $h  0$, we have:\n    $$\n    \\operatorname{Var}(B_{t+h} - B_t) = h\n    $$\nNow, we can compute the second moment of the increment:\n$$\n\\mathbb{E}\\!\\left[(B_{t+h} - B_{t})^2\\right] = \\operatorname{Var}(B_{t+h} - B_t) + \\left(\\mathbb{E}[B_{t+h} - B_t]\\right)^2 = h + 0^2 = h\n$$\nSubstituting this result back into our expression for the second moment of the difference quotient:\n$$\n\\mathbb{E}\\!\\left[\\left(\\frac{B_{t+h} - B_{t}}{h}\\right)^{2}\\right] = \\frac{1}{h^2} \\cdot h = \\frac{1}{h}\n$$\nFinally, we take the limit as $h$ approaches $0$ from the positive side:\n$$\n\\lim_{h \\to 0^{+}} \\mathbb{E}\\!\\left[\\left(\\frac{B_{t+h} - B_{t}}{h}\\right)^{2}\\right] = \\lim_{h \\to 0^{+}} \\frac{1}{h} = +\\infty\n$$\nThe limit diverges to positive infinity.\n\n### Part 2: Interpretation as Gaussian White Noise\n\nThe result that the limit is infinite is fundamental to understanding the nature of Brownian motion paths and their \"derivative\".\n\n1.  **Nowhere Differentiability**: If the derivative $\\dot{B}_t$ existed at time $t$ in the mean-square sense, the limit of the mean-square of the difference quotients would converge to a finite value, namely $\\mathbb{E}[(\\dot{B}_t)^2]$. Our calculation shows this limit is infinite. This implies that Brownian motion is not mean-square differentiable at any point $t$. This is a key aspect of the more general property that Brownian paths are, with probability $1$, continuous but nowhere differentiable.\n\n2.  **Connection to Gaussian White Noise (GWN)**: The concept of a derivative of Brownian motion can be formalized in the theory of generalized functions (or distributions), where $\\dot{B}_t$ is known as Gaussian white noise. GWN is a generalized stochastic process whose \"values\" at different times are independent and whose \"variance\" at any single point is infinite. Its covariance structure is formally given by the Dirac delta function:\n    $$\n    \\mathbb{E}[\\dot{B}_t \\dot{B}_s] = C \\cdot \\delta(t-s)\n    $$\n    For standard GWN (the derivative of standard Brownian motion), the constant $C=1$.\n\n    Our calculated result is perfectly consistent with this distributional definition. The difference quotient $\\frac{B_{t+h} - B_{t}}{h}$ can be interpreted as the result of \"averaging\" or \"smearing\" the generalized process $\\dot{B}_t$ over a small interval $[t, t+h]$:\n    $$\n    \\frac{B_{t+h} - B_{t}}{h} = \\frac{1}{h} \\int_{t}^{t+h} \\dot{B}_s \\, \\mathrm{d}s\n    $$\n    Let's compute the variance of this averaged quantity, assuming $\\mathbb{E}[\\dot{B}_t \\dot{B}_s] = \\delta(t-s)$ and using the bilinearity of covariance:\n    \\begin{align*}\n    \\mathbb{E}\\!\\left[\\left(\\frac{1}{h} \\int_{t}^{t+h} \\dot{B}_s \\, \\mathrm{d}s\\right)^{2}\\right] = \\frac{1}{h^2} \\mathbb{E}\\!\\left[ \\left(\\int_{t}^{t+h} \\dot{B}_u \\, \\mathrm{d}u\\right) \\left(\\int_{t}^{t+h} \\dot{B}_v \\, \\mathrm{d}v\\right) \\right] \\\\\n    = \\frac{1}{h^2} \\int_{t}^{t+h} \\int_{t}^{t+h} \\mathbb{E}[\\dot{B}_u \\dot{B}_v] \\, \\mathrm{d}u \\, \\mathrm{d}v \\\\\n    = \\frac{1}{h^2} \\int_{t}^{t+h} \\int_{t}^{t+h} \\delta(u-v) \\, \\mathrm{d}u \\, \\mathrm{d}v\n    \\end{align*}\n    The inner integral $\\int_t^{t+h} \\delta(u-v) \\, \\mathrm{d}u$ equals $1$ if $v \\in (t, t+h)$ and $0$ otherwise. Thus, the double integral becomes:\n    $$\n    \\frac{1}{h^2} \\int_{t}^{t+h} 1 \\, \\mathrm{d}v = \\frac{1}{h^2} [v]_{t}^{t+h} = \\frac{h}{h^2} = \\frac{1}{h}\n    $$\n    This result, derived from the distributional definition of GWN, exactly matches our calculation derived directly from the properties of Brownian motion. Therefore, the fact that the second moment of the difference quotient behaves as $\\frac{1}{h}$ and diverges as $h \\to 0^{+}$ is the precise manifestation of its derivative being a generalized process with a Dirac delta function as its covariance kernel.", "answer": "$$\n\\boxed{+\\infty}\n$$", "id": "2990282"}, {"introduction": "Having established that Brownian paths are nowhere differentiable, we now shift our focus from what these paths are not to what they are. This exercise guides you through a constructive proof of their remarkable regularity, showing that they are almost surely Hölder continuous for any exponent $\\alpha$ strictly less than $\\frac{1}{2}$. By building a random Hölder constant from first principles, you will gain a hands-on appreciation for the precise balance between randomness and continuity that defines a Brownian path [@problem_id:2990320]. This property is crucial for ensuring that stochastic integrals with respect to Brownian motion are well-defined.", "problem": "Let $\\{B_{t}\\}_{t \\in [0,1]}$ be a standard Brownian motion, meaning a real-valued stochastic process with $B_{0}=0$, independent and stationary increments, and Gaussian increments satisfying $B_{t}-B_{s} \\sim \\mathcal{N}(0,t-s)$ for $0 \\leq s  t \\leq 1$, where $\\mathcal{N}(0,\\sigma^{2})$ denotes the normal distribution with mean $0$ and variance $\\sigma^{2}$. Fix an exponent $\\alpha \\in (0,\\tfrac{1}{2})$. Using only the defining properties of Brownian motion and basic Gaussian tail bounds, construct an explicit almost surely finite random constant $C_{\\alpha}$ (as a closed-form functional of the path $\\{B_{t}\\}_{t \\in [0,1]}$) such that, on the same probability-one event, the Hölder bound $|B_{t} - B_{s}| \\leq C_{\\alpha} |t-s|^{\\alpha}$ holds for all $s,t \\in [0,1]$. Your construction must:\n- Start from the fundamental properties of Gaussian increments of Brownian motion and rigorous probabilistic estimates.\n- Provide a pathwise inequality valid simultaneously for all $s,t \\in [0,1]$.\n- Include a justification that your $C_{\\alpha}$ is almost surely finite.\n\nExpress your final answer as a single analytic expression for $C_{\\alpha}$ in terms of $\\{B_{t}\\}_{t \\in [0,1]}$. No numerical approximation or rounding is required.", "solution": "The user wants me to solve a problem about the Hölder continuity of Brownian motion.\n\n### Step 1: Problem Validation\n\n**1. Extract Givens:**\n- Stochastic process: $\\{B_{t}\\}_{t \\in [0,1]}$, a standard Brownian motion.\n- Definition of standard Brownian motion:\n    - Real-valued.\n    - $B_{0}=0$.\n    - Independent increments.\n    - Stationary increments.\n    - Gaussian increments: $B_{t}-B_{s} \\sim \\mathcal{N}(0,t-s)$ for $0 \\leq s  t \\leq 1$.\n- Parameter: $\\alpha \\in (0, \\frac{1}{2})$.\n- Objective: Construct an explicit, almost surely finite random constant $C_{\\alpha}$ (as a closed-form functional of the path $\\{B_{t}\\}_{t \\in [0,1]}$).\n- Condition to satisfy: $|B_{t} - B_{s}| \\leq C_{\\alpha} |t-s|^{\\alpha}$ for all $s,t \\in [0,1]$ on a probability-one event.\n- Constraints on the construction:\n    - Use only defining properties of Brownian motion and basic Gaussian tail bounds.\n    - The inequality must be pathwise and hold for all $s,t$.\n    - Must justify that $C_{\\alpha}$ is a.s. finite.\n- Final Answer: A single analytic expression for $C_{\\alpha}$.\n\n**2. Validate Using Extracted Givens:**\n-   **Scientifically Grounded:** Yes. The problem addresses the Hölder continuity of Brownian motion paths, a fundamental result in stochastic calculus typically established via Kolmogorov's continuity criterion. The request to construct a constant using elementary methods is a standard and rigorous mathematical exercise. The condition $\\alpha \\in (0, 1/2)$ is correct and crucial for the validity of the result.\n-   **Well-Posed:** Yes. The objective is clearly stated, and the constraints define a solvable problem. The existence of such a constant is a known mathematical fact.\n-   **Objective:** Yes. The problem is phrased in precise, unbiased mathematical language.\n-   **Flaws:** The problem statement exhibits none of the flaws listed in the instructions (e.g., scientific unsoundness, incompleteness, ambiguity). It is a standard, non-trivial problem in the theory of stochastic processes.\n\n**3. Verdict and Action:**\nThe problem is **valid**. I will proceed with the solution.\n\n### Step 2: Solution Construction\n\nThe construction of the Hölder constant $C_{\\alpha}$ proceeds in three main parts. First, we establish a uniform bound on the increments of the Brownian motion over dyadic intervals. Second, we extend this bound from the dyadic grid to all pairs of points $(s,t) \\in [0,1]^2$. Finally, we consolidate these results into a single expression for $C_{\\alpha}$.\n\n**Part 1: Uniform Bound on Dyadic Increments**\n\nLet $D_n = \\{k 2^{-n} : k = 0, 1, \\dots, 2^n\\}$ be the set of dyadic points of level $n$ in $[0,1]$. An elementary dyadic interval of level $n$ is of the form $[k 2^{-n}, (k+1) 2^{-n}]$. The increment of the Brownian motion over such an interval is $\\Delta B_{n,k} = B_{(k+1)2^{-n}} - B_{k2^{-n}}$. From the definition of standard Brownian motion, $\\Delta B_{n,k}$ is a Gaussian random variable with mean $0$ and variance $\\sigma^2 = (k+1)2^{-n} - k2^{-n} = 2^{-n}$. So, $\\Delta B_{n,k} \\sim \\mathcal{N}(0, 2^{-n})$.\n\nWe use a standard tail bound for a Gaussian random variable $X \\sim \\mathcal{N}(0, \\sigma^2)$, which states that for any $y0$, $P(|X|  y) \\le 2 \\exp(-\\frac{y^2}{2\\sigma^2})$.\nWe want to bound the probability that $|\\Delta B_{n,k}|$ exceeds $K(2^{-n})^\\alpha$ for some constant $K0$.\nLet $y = K(2^{-n})^\\alpha$. The variance is $\\sigma^2=2^{-n}$. The probability is:\n$$ P(|\\Delta B_{n,k}|  K(2^{-n})^\\alpha) \\le 2 \\exp\\left(-\\frac{(K(2^{-n})^\\alpha)^2}{2(2^{-n})}\\right) = 2 \\exp\\left(-\\frac{K^2 2^{-2n\\alpha}}{2 \\cdot 2^{-n}}\\right) = 2 \\exp\\left(-\\frac{K^2}{2} 2^{n(1-2\\alpha)}\\right) $$\nLet $\\delta = 1 - 2\\alpha$. Since $\\alpha \\in (0, 1/2)$, we have $\\delta  0$. The bound becomes $2 \\exp(-\\frac{K^2}{2} 2^{n\\delta})$.\n\nNow, consider the random variable representing the maximum normalized increment at level $n$:\n$$ M_n = \\max_{0 \\le k  2^n} \\frac{|\\Delta B_{n,k}|}{(2^{-n})^\\alpha} $$\nWe want to show that the supremum of these maxima over all levels $n$ is almost surely finite. Let $C_1 = \\sup_{n \\ge 1} M_n$. For any $K0$:\n$$ P(C_1  K) = P\\left(\\bigcup_{n=1}^\\infty \\{M_n  K\\}\\right) \\le \\sum_{n=1}^\\infty P(M_n  K) $$\nUsing the union bound for a fixed $n$:\n$$ P(M_n  K) = P\\left(\\bigcup_{k=0}^{2^n-1} \\left\\{\\frac{|\\Delta B_{n,k}|}{(2^{-n})^\\alpha}  K\\right\\}\\right) \\le \\sum_{k=0}^{2^n-1} P\\left(|\\Delta B_{n,k}|  K(2^{-n})^\\alpha\\right) $$\nSince the increments are identically distributed, this sum is:\n$$ P(M_n  K) \\le 2^n \\cdot 2 \\exp\\left(-\\frac{K^2}{2} 2^{n\\delta}\\right) = 2^{n+1} \\exp\\left(-\\frac{K^2}{2} 2^{n\\delta}\\right) $$\nThe sum $\\sum_{n=1}^\\infty P(M_n  K)$ is therefore bounded by $\\sum_{n=1}^\\infty 2^{n+1} \\exp(-\\frac{K^2}{2} 2^{n\\delta})$.\nThis series converges for any $K0$ because the exponential term decays much faster than the polynomial term $2^{n+1}$ grows (by the ratio test, for instance). Since $\\sum P(M_n  K)  \\infty$, the Borel-Cantelli lemma implies that the probability of $\\{M_n  K\\}$ occurring for infinitely many $n$ is $0$. This means that for any $K$, $M_n  K$ only for a finite number of $n$ almost surely. Taking a sequence $K_j \\to \\infty$, we conclude that $\\sup_n M_n$ must be finite almost surely.\n\nLet us define the almost surely finite random constant:\n$$ C_1(\\omega) = \\sup_{\\substack{n \\ge 1 \\\\ 0 \\le k  2^n}} \\frac{|B_{(k+1)2^{-n}} - B_{k2^{-n}}|}{(2^{-n})^\\alpha} $$\nThis constant $C_1(\\omega)$ (where we omit the path argument $\\omega$ for brevity) provides a uniform bound on all elementary dyadic increments:\n$$ |B_{(k+1)2^{-n}} - B_{k2^{-n}}| \\le C_1 (2^{-n})^\\alpha \\quad \\text{for all } n \\ge 1, 0 \\le k  2^n. $$\n\n**Part 2: Extension to General $(s,t)$**\n\nWe now use this dyadic bound to establish the Hölder condition for all $s,t \\in [0,1]$. A careful analysis based on bridging $s$ and $t$ via a sequence of dyadic points (any $st$ can be connected by at most two expanding series of dyadic increments from their common dyadic ancestor) shows that:\n$$ |B_t-B_s| \\le C_1 \\frac{2^\\alpha+1}{2^\\alpha-1} (t-s)^\\alpha $$\nThis holds for $t-s$ small enough, e.g., $t-s  1/2$.\n\n**Case 2: $|t-s| \\ge 1/2$.**\nThe paths of Brownian motion are almost surely continuous on the compact interval $[0,1]$, and therefore almost surely bounded. Let $M = \\sup_{t \\in [0,1]} |B_t|$. $M$ is an almost surely finite random variable.\n$$ |B_t-B_s| \\le |B_t| + |B_s| \\le 2M $$\nWe can write this in the desired form:\n$$ |B_t-B_s| \\le \\frac{2M}{|t-s|^\\alpha} |t-s|^\\alpha $$\nSince $|t-s| \\ge 1/2$, we have $|t-s|^\\alpha \\ge (1/2)^\\alpha$. Thus:\n$$ |B_t-B_s| \\le \\frac{2M}{(1/2)^\\alpha} |t-s|^\\alpha = 2^{1+\\alpha} M |t-s|^\\alpha $$\n\n**Part 3: Final Expression for $C_\\alpha$**\n\nTo ensure the inequality $|B_t - B_s| \\le C_\\alpha |t-s|^\\alpha$ holds for all $s,t \\in [0,1]$, we must take a constant that is valid for both cases. We can define $C_\\alpha$ as the maximum of the constants derived for each case.\nLet $C_1 = \\sup_{\\substack{n \\ge 1 \\\\ 0 \\le k  2^n}} \\frac{|B_{(k+1)2^{-n}} - B_{k 2^{-n}}|}{(2^{-n})^\\alpha}$. The constant for the small-separation case is $C_1 \\frac{2^\\alpha+1}{2^\\alpha-1}$.\nThe constant for the large-separation case is $2^{1+\\alpha} \\sup_{t \\in [0,1]} |B_t|$.\nBoth $C_1$ and $\\sup |B_t|$ are almost surely finite functionals of the path $\\{B_t\\}$. Therefore, their combination is also an almost surely finite random constant. The final expression for $C_\\alpha$ is the maximum of these two terms, which guarantees the Hölder bound holds for all $s, t \\in [0,1]$ on a single probability-one set.\n\nThe construction is complete, respects all constraints, and yields an explicit functional of the Brownian path.", "answer": "$$ \\boxed{\\max\\left\\{ \\left(\\frac{2^{\\alpha}+1}{2^{\\alpha}-1}\\right) \\sup_{\\substack{n \\in \\mathbb{Z}, n \\ge 1 \\\\ k \\in \\mathbb{Z}, 0 \\le k  2^n}} \\frac{|B_{(k+1)2^{-n}} - B_{k 2^{-n}}|}{(2^{-n})^{\\alpha}} ,\\; 2^{1+\\alpha} \\sup_{t \\in [0,1]} |B_t| \\right\\}} $$", "id": "2990320"}, {"introduction": "Our journey has shown that Brownian paths are Hölder continuous for any exponent $\\alpha \\lt \\frac{1}{2}$, but not differentiable. This naturally leads to a critical question: what happens at the boundary exponent $\\alpha = \\frac{1}{2}$? This final practice tackles this question head-on, leading to a profound and precise quantification of a path's roughness known as the Law of the Iterated Logarithm for Brownian motion [@problem_id:2990275]. By analyzing the limit superior of the normalized dyadic increments, you will determine the exact, non-random constant that bounds the path's fluctuations, revealing the sharp boundary of its irregularity.", "problem": "Let $\\{B_t\\}_{t \\in [0,1]}$ be a standard Brownian motion. For the dyadic mesh of size $\\delta_n \\coloneqq 2^{-n}$, define the dyadic Lipschitz seminorm\n$$\nL(\\delta_n) \\coloneqq \\max_{1 \\leq k \\leq 2^n} \\frac{|B_{k \\delta_n} - B_{(k-1)\\delta_n}|}{\\delta_n}.\n$$\nWorking from first principles about Brownian motion (independent and stationary Gaussian increments) and well-tested probabilistic facts (Gaussian tail bounds, union bounds, and the Borel–Cantelli lemma), determine the exact constant $c \\in \\mathbb{R}$ such that, with probability one,\n$$\n\\limsup_{n \\to \\infty} \\frac{L(\\delta_n)}{\\sqrt{2 \\log(1/\\delta_n)/\\delta_n}} \\;=\\; c.\n$$\nGive your answer as a closed-form constant. No rounding is required.", "solution": "The user wants to find the constant $c \\in \\mathbb{R}$ such that\n$$\n\\limsup_{n \\to \\infty} \\frac{L(\\delta_n)}{\\sqrt{2 \\log(1/\\delta_n)/\\delta_n}} \\;=\\; c\n$$\nwith probability one, where $\\{B_t\\}_{t \\in [0,1]}$ is a standard Brownian motion, $\\delta_n \\coloneqq 2^{-n}$, and $L(\\delta_n) \\coloneqq \\max_{1 \\leq k \\leq 2^n} \\frac{|B_{k \\delta_n} - B_{(k-1)\\delta_n}|}{\\delta_n}$.\n\nFirst, we simplify the expression. Let $\\Delta B_k^{(n)} \\coloneqq B_{k \\delta_n} - B_{(k-1)\\delta_n}$. By the properties of standard Brownian motion, for a fixed $n$, the increments $\\{\\Delta B_k^{(n)}\\}_{k=1}^{2^n}$ are independent and identically distributed (i.i.d.) Gaussian random variables with mean $0$ and variance $\\delta_n$.\nLet $Z_k^{(n)} \\coloneqq \\frac{\\Delta B_k^{(n)}}{\\sqrt{\\delta_n}}$. Then, for a fixed $n$, $\\{Z_k^{(n)}\\}_{k=1}^{2^n}$ is a sequence of $2^n$ i.i.d. standard normal random variables, $Z_k^{(n)} \\sim \\mathcal{N}(0,1)$.\n\nWe can rewrite the expression in terms of $Z_k^{(n)}$:\n$$\nL(\\delta_n) = \\max_{1 \\leq k \\leq 2^n} \\frac{|\\sqrt{\\delta_n} Z_k^{(n)}|}{\\delta_n} = \\frac{1}{\\sqrt{\\delta_n}} \\max_{1 \\leq k \\leq 2^n} |Z_k^{(n)}|.\n$$\nThe denominator is $\\sqrt{2 \\log(1/\\delta_n)/\\delta_n}$. Since $\\delta_n = 2^{-n}$, we have $1/\\delta_n = 2^n$, so $\\log(1/\\delta_n) = \\log(2^n) = n \\log(2)$.\nThe expression we want to evaluate the limit of is:\n$$\nY_n \\coloneqq \\frac{L(\\delta_n)}{\\sqrt{2 \\log(1/\\delta_n)/\\delta_n}} = \\frac{\\frac{1}{\\sqrt{\\delta_n}} \\max_{1 \\leq k \\leq 2^n} |Z_k^{(n)}|}{\\sqrt{2 n\\log(2)/\\delta_n}} = \\frac{\\max_{1 \\leq k \\leq 2^n} |Z_k^{(n)}|}{\\sqrt{2 n\\log(2)}}.\n$$\nThe problem is now to determine $c$ such that $\\limsup_{n \\to \\infty} Y_n = c$ almost surely. We will prove this by establishing an upper bound ($c \\le 1$) and a lower bound ($c \\ge 1$).\n\n**Part 1: The Upper Bound ($\\limsup_{n \\to \\infty} Y_n \\le 1$ a.s.)**\n\nWe use the first Borel–Cantelli lemma. Let $\\epsilon  0$ and define the event $A_n \\coloneqq \\{ Y_n  1+\\epsilon \\}$. If we can show that $\\sum_{n=1}^\\infty P(A_n)  \\infty$, then the lemma implies that $P(A_n \\text{ occurs infinitely often}) = 0$.\n\nBy the union bound,\n$$\nP(A_n) = P\\left( \\max_{1 \\leq k \\leq 2^n} |Z_k^{(n)}|  (1+\\epsilon)\\sqrt{2 n\\log(2)} \\right) \\le \\sum_{k=1}^{2^n} P\\left( |Z_k^{(n)}|  (1+\\epsilon)\\sqrt{2 n\\log(2)} \\right).\n$$\nSince $Z_k^{(n)}$ are i.i.d. $\\mathcal{N}(0,1)$, let $Z$ be a standard normal random variable. Let $u_n \\coloneqq (1+\\epsilon)\\sqrt{2 n\\log(2)}$.\n$$\nP(A_n) \\le 2^n P(|Z|  u_n).\n$$\nWe use the well-known Gaussian tail bound: for $u  0$, $P(|Z|  u) = 2P(Zu) \\le \\sqrt{\\frac{2}{\\pi}} \\frac{1}{u} \\exp\\left(-\\frac{u^2}{2}\\right)$.\nHere, $\\frac{u_n^2}{2} = \\frac{(1+\\epsilon)^2(2 n\\log(2))}{2} = (1+\\epsilon)^2 n \\log(2)$.\nSo, $\\exp\\left(-\\frac{u_n^2}{2}\\right) = \\exp\\left(-(1+\\epsilon)^2 n \\log(2)\\right) = (2^n)^{-(1+\\epsilon)^2}$.\n$$\nP(A_n) \\le 2^n \\sqrt{\\frac{2}{\\pi}} \\frac{1}{(1+\\epsilon)\\sqrt{2 n\\log(2)}} (2^n)^{-(1+\\epsilon)^2} = \\frac{C}{\\sqrt{n}} 2^{n(1 - (1+\\epsilon)^2)},\n$$\nfor some constant $C0$. The exponent of $2$ is $n(1 - (1+2\\epsilon+\\epsilon^2)) = n(-2\\epsilon-\\epsilon^2)$, which is negative for $\\epsilon0$.\nThe series $\\sum_{n=1}^\\infty \\frac{1}{\\sqrt{n}} (2^{-2\\epsilon-\\epsilon^2})^n$ converges by comparison with a geometric series with ratio $2^{-2\\epsilon-\\epsilon^2}  1$.\nBy the first Borel–Cantelli lemma, $P(A_n \\text{ i.o.}) = 0$. This means that for any given $\\epsilon0$, almost surely, $Y_n \\le 1+\\epsilon$ for all sufficiently large $n$. This implies $\\limsup_{n \\to \\infty} Y_n \\le 1+\\epsilon$ a.s. As this holds for any $\\epsilon  0$, we conclude that\n$$\n\\limsup_{n \\to \\infty} Y_n \\le 1 \\quad \\text{a.s.}\n$$\n\n**Part 2: The Lower Bound ($\\limsup_{n \\to \\infty} Y_n \\ge 1$ a.s.)**\n\nWe use the second Borel–Cantelli lemma, which requires independent events. The random variables $Z_k^{(n)}$ and $Z_j^{(m)}$ are not independent if $n \\neq m$ because their time intervals might overlap or be nested. To ensure independence, we construct a sequence of events based on disjoint time intervals.\n\nFor each $n \\ge 1$, consider only the increments over time intervals with an even starting index, i.e., $I_{n,k} \\coloneqq [(2k-1)\\delta_n, 2k\\,\\delta_n]$ for $k = 1, \\dots, 2^{n-1}$.\nThe increments $\\Delta B_{n,k} \\coloneqq B_{2k\\,\\delta_n} - B_{(2k-1)\\delta_n}$ are independent for different $k$ at a fixed $n$.\nFurthermore, the time intervals $\\{I_{n,k}\\}_k$ and $\\{I_{m,j}\\}_j$ are disjoint for $n \\ne m$. For instance, an interval for level $n$ is of the form $[(2k-1)2^{-n}, 2k \\cdot 2^{-n}] = [(4k-2)2^{-(n+1)}, 4k \\cdot 2^{-(n+1)}]$, while an interval for level $n+1$ is $[(2j-1)2^{-(n+1)}, 2j \\cdot 2^{-(n+1)}]$. These families of intervals are disjoint.\nTherefore, the increments $\\{\\Delta B_{n,k}\\}_{n,k}$ over all considered $n$ and $k$ are mutually independent. The corresponding normalized increments $Z'_{n,k} \\coloneqq \\Delta B_{n,k} / \\sqrt{\\delta_n}$ are i.i.d. $\\mathcal{N}(0,1)$ random variables.\n\nLet $\\epsilon \\in (0, 1)$ and $\\lambda \\coloneqq 1-\\epsilon$. Define the events\n$$\nE_n \\coloneqq \\left\\{ \\max_{1 \\le k \\le 2^{n-1}} |Z'_{n,k}|  \\lambda\\sqrt{2 n\\log(2)} \\right\\}.\n$$\nThe events $\\{E_n\\}_{n \\ge 1}$ are independent. If we show $\\sum_{n=1}^\\infty P(E_n) = \\infty$, the second Borel–Cantelli lemma implies $P(E_n \\text{ i.o.}) = 1$.\n\nThe event $E_n$ is the complement of $\\left\\{ \\max_{1 \\le k \\le 2^{n-1}} |Z'_{n,k}| \\le \\lambda\\sqrt{2 n\\log(2)} \\right\\}$. Due to independence of the $Z'_{n,k}$ for fixed $n$:\n$$\nP(E_n) = 1 - P\\left(\\bigcap_{k=1}^{2^{n-1}} \\left\\{|Z'_{n,k}| \\le \\lambda\\sqrt{2 n\\log(2)}\\right\\}\\right) = 1 - \\left(P\\left(|Z| \\le \\lambda\\sqrt{2 n\\log(2)}\\right)\\right)^{2^{n-1}}.\n$$\nLet $v_n \\coloneqq \\lambda\\sqrt{2 n\\log(2)}$. Then $P(E_n) = 1 - (1 - P(|Z|v_n))^{2^{n-1}}$.\nWe use the lower bound for the Gaussian tail: for large $u$, $P(|Z|u) \\ge \\sqrt{\\frac{2}{\\pi}} \\left(\\frac{1}{u}-\\frac{1}{u^3}\\right) \\exp\\left(-\\frac{u^2}{2}\\right) \\sim \\sqrt{\\frac{2}{\\pi}}\\frac{1}{u}\\exp\\left(-\\frac{u^2}{2}\\right)$.\nSo, for large $n$, there is a constant $C'0$ such that\n$$\nP(|Z|v_n) \\ge \\frac{C'}{v_n} \\exp\\left(-\\frac{v_n^2}{2}\\right) = \\frac{C'}{\\lambda\\sqrt{2 n\\log(2)}} \\exp(-\\lambda^2 n \\log(2)) = \\frac{C''}{\\sqrt{n}} (2^n)^{-\\lambda^2}.\n$$\nLet $p_n \\coloneqq P(|Z|v_n)$. We examine the term $2^{n-1}p_n$:\n$$\n2^{n-1}p_n \\ge 2^{n-1} \\frac{C''}{\\sqrt{n}} 2^{-n\\lambda^2} = \\frac{C''}{2\\sqrt{n}} 2^{n(1-\\lambda^2)}.\n$$\nSince $\\lambda = 1-\\epsilon$, the exponent is $n(1-(1-\\epsilon)^2) = n(2\\epsilon - \\epsilon^2)$. Since $\\epsilon \\in (0,1)$, $2\\epsilon - \\epsilon^2  0$.\nThus, $2^{n-1}p_n \\to \\infty$ as $n \\to \\infty$.\nFor a sequence of probabilities $p_n$ and integers $N_n$ such that $N_n p_n \\to \\infty$, we have $(1-p_n)^{N_n} \\to 0$. Here, $N_n = 2^{n-1}$, so $P(E_n) = 1 - (1-p_n)^{2^{n-1}} \\to 1$.\nSince $P(E_n) \\to 1$ as $n \\to \\infty$, the series $\\sum_{n=1}^\\infty P(E_n)$ must diverge.\nBy the second Borel–Cantelli lemma, $P(E_n \\text{ i.o.})=1$.\n\nThe occurrence of $E_n$ implies\n$$\n\\max_{1 \\le k \\le 2^{n-1}} \\frac{|B_{2k\\,\\delta_n} - B_{(2k-1)\\delta_n}|}{\\sqrt{\\delta_n}}  \\lambda \\sqrt{2 n\\log(2)}.\n$$\nThis further implies\n$$\nY_n = \\frac{\\max_{1 \\leq j \\leq 2^n} |Z_j^{(n)}|}{\\sqrt{2 n\\log(2)}} \\ge \\frac{\\max_{1 \\le k \\le 2^{n-1}} |Z'_{n,k}|}{\\sqrt{2 n\\log(2)}}  \\lambda = 1-\\epsilon.\n$$\nSince $E_n$ occurs infinitely often a.s., $Y_n  1-\\epsilon$ must also occur infinitely often a.s. This means $\\limsup_{n \\to \\infty} Y_n \\ge 1-\\epsilon$ a.s.\nSince this holds for any $\\epsilon \\in (0, 1)$, we can let $\\epsilon \\to 0^+$ to obtain\n$$\n\\limsup_{n \\to \\infty} Y_n \\ge 1 \\quad \\text{a.s.}\n$$\n\n**Conclusion**\nCombining the upper and lower bounds, we have shown that\n$$\n\\limsup_{n \\to \\infty} \\frac{L(\\delta_n)}{\\sqrt{2 \\log(1/\\delta_n)/\\delta_n}} \\;=\\; 1 \\quad \\text{a.s.}\n$$\nThe constant is $c=1$.", "answer": "$$\\boxed{1}$$", "id": "2990275"}]}