{"hands_on_practices": [{"introduction": "This first practice exercise tackles a cornerstone of measure theory: the Fubini-Tonelli theorem. This theorem is indispensable in stochastic analysis, where we frequently encounter integrals over a product of a time interval and a probability space. By justifying and applying the interchange of integration order, you will develop a crucial skill for simplifying complex expectations of time-dependent random variables [@problem_id:2991570].", "problem": "Let $\\left(X,\\mathcal{A},\\mu\\right)$ and $\\left(Y,\\mathcal{B},\\nu\\right)$ be $\\sigma$-finite measure spaces, and let $f:X\\times Y\\to[0,\\infty]$ be measurable with respect to the product $\\sigma$-algebra $\\mathcal{A}\\otimes\\mathcal{B}$. Starting from the definition of the integral of a nonnegative measurable function as the supremum of integrals of nonnegative simple functions, the construction of product measures from rectangles, and basic properties of countable additivity and monotone limits, establish the following statement: integrating $f$ over the product space agrees with iterated integration in either order, and the section maps $x\\mapsto\\int f(x,y)\\,\\nu(\\mathrm{d}y)$ and $y\\mapsto\\int f(x,y)\\,\\mu(\\mathrm{d}x)$ are measurable, with the common value of all three integrals possibly equal to $+\\infty$.\n\nThen, place this result in the setting of stochastic differential equations by considering the canonical probability space of continuous paths. Let $\\left(\\Omega,\\mathcal{F},\\mathbb{P}\\right)$ be the path space $\\Omega=C\\left([0,\\infty),\\mathbb{R}\\right)$ endowed with the Borel $\\sigma$-algebra $\\mathcal{F}$ induced by the topology of uniform convergence on compact sets and the Wiener measure $\\mathbb{P}$ under which the coordinate process $W_{t}(\\omega)=\\omega(t)$ is a standard Brownian motion (also called Wiener process). Let $\\lambda$ denote Lebesgue measure on $\\left([0,\\infty),\\mathcal{B}([0,\\infty))\\right)$. For a fixed parameter $a>0$, define $f:\\Omega\\times[0,\\infty)\\to[0,\\infty)$ by\n$$\nf(\\omega,t)=\\exp(-t)\\,\\mathbf{1}_{\\{|\\omega(t)|\\le a\\sqrt{t}\\}},\n$$\nwhere $\\mathbf{1}_{A}$ denotes the indicator of a set $A$. Using only the fundamental definitions and theorems of measure theory and probability (such as properties of product measures, measurability of evaluation maps, and distributional scaling of Brownian motion), show that $f$ is measurable, justify the use of product integration for $f$, and compute the double integral\n$$\n\\int_{0}^{\\infty}\\int_{\\Omega} f(\\omega,t)\\,\\mathbb{P}(\\mathrm{d}\\omega)\\,\\lambda(\\mathrm{d}t).\n$$\nExpress your final answer as a closed-form analytic expression in terms of the standard normal cumulative distribution function $\\Phi$. No rounding is required, and no units are involved. The final answer must be a single expression.", "solution": "The problem is mathematically well-posed, scientifically grounded, and self-contained. All necessary conditions and definitions for the requested proof and calculation are provided and are standard within measure theory and probability theory. The problem is valid and I will proceed with a full solution.\n\nThe problem consists of two parts. First, a proof of the Fubini-Tonelli theorem for non-negative functions, and second, an application of this theorem to a specific function defined on the Wiener path space.\n\n### Part 1: Proof of the Fubini-Tonelli Theorem\n\nWe are asked to establish that for $\\sigma$-finite measure spaces $\\left(X,\\mathcal{A},\\mu\\right)$ and $\\left(Y,\\mathcal{B},\\nu\\right)$, and a non-negative $\\mathcal{A}\\otimes\\mathcal{B}$-measurable function $f:X\\times Y\\to[0,\\infty]$, the integral over the product space equals the iterated integrals, and the relevant section maps are measurable. Let $\\pi = \\mu \\otimes \\nu$ denote the product measure on $\\mathcal{A}\\otimes\\mathcal{B}$.\n\nThe proof is built in three stages, following the standard construction of the Lebesgue integral.\n\n**Step 1: Indicator Functions**\nLet $f = \\mathbf{1}_E$ for an arbitrary set $E \\in \\mathcal{A} \\otimes \\mathcal{B}$.\nFor a fixed $x \\in X$, the $x$-section of $E$ is $E_x = \\{y \\in Y \\mid (x,y) \\in E\\}$. The function $f(x, \\cdot)$ is the indicator function $\\mathbf{1}_{E_x}(y)$.\nThe inner integral with respect to $\\nu$ is $g(x) = \\int_Y f(x,y)\\,\\mathrm{d}\\nu(y) = \\int_Y \\mathbf{1}_{E_x}(y)\\,\\mathrm{d}\\nu(y) = \\nu(E_x)$.\n\nFirst, we must show that $g(x) = \\nu(E_x)$ is an $\\mathcal{A}$-measurable function of $x$. We use a monotone class argument. Let $\\mathcal{C}$ be the collection of all sets $E \\in \\mathcal{A} \\otimes \\mathcal{B}$ for which the function $x \\mapsto \\nu(E_x)$ is $\\mathcal{A}$-measurable.\n(a) Let $\\mathcal{R} = \\{A \\times B \\mid A \\in \\mathcal{A}, B \\in \\mathcal{B}\\}$ be the semi-algebra of measurable rectangles which generates $\\mathcal{A} \\otimes \\mathcal{B}$. If $E = A \\times B \\in \\mathcal{R}$, then its $x$-section is $E_x = B$ if $x \\in A$ and $E_x = \\emptyset$ if $x \\notin A$. Thus, $\\nu(E_x) = \\nu(B)\\mathbf{1}_A(x)$. Since $\\mathbf{1}_A(x)$ is $\\mathcal{A}$-measurable, $\\mathcal{R} \\subseteq \\mathcal{C}$. The algebra generated by $\\mathcal{R}$ is also contained in $\\mathcal{C}$ by linearity.\n(b) We show $\\mathcal{C}$ is a monotone class. Let $\\{E_n\\}_{n=1}^\\infty$ be a sequence in $\\mathcal{C}$ such that $E_n \\uparrow E$. Then for each $x$, the sections $(E_n)_x \\uparrow E_x$. By the monotone property of measure (continuity from below), $\\nu((E_n)_x) \\to \\nu(E_x)$. Since each function $x \\mapsto \\nu((E_n)_x)$ is $\\mathcal{A}$-measurable, their pointwise limit $x \\mapsto \\nu(E_x)$ is also $\\mathcal{A}$-measurable. Thus $E \\in \\mathcal{C}$.\nNow let $E_n \\downarrow E$. Since $\\nu$ is $\\sigma$-finite, we can write $Y = \\bigcup_{k=1}^\\infty Y_k$ with $\\nu(Y_k)  \\infty$ and $Y_k$ disjoint. For each $k$, we can consider the finite measure space $(Y_k, \\mathcal{B}|_{Y_k}, \\nu|_{Y_k})$. The function $x \\mapsto \\nu(E_x \\cap Y_k)$ is measurable by a similar argument using continuity from above for finite measures. Then $\\nu(E_x) = \\sum_k \\nu(E_x \\cap Y_k)$, a sum of measurable functions, is measurable. Thus $\\mathcal{C}$ is a monotone class.\n(c) By the Monotone Class Theorem, since $\\mathcal{C}$ is a monotone class containing the algebra generated by $\\mathcal{R}$, it must contain the $\\sigma$-algebra generated by $\\mathcal{R}$, which is $\\mathcal{A} \\otimes \\mathcal{B}$. Therefore, $\\mathcal{C} = \\mathcal{A} \\otimes \\mathcal{B}$, and $x \\mapsto \\nu(E_x)$ is $\\mathcal{A}$-measurable for all $E \\in \\mathcal{A} \\otimes \\mathcal{B}$.\n\nNext, we show the equality of integrals. Define a measure $\\pi'$ on $\\mathcal{A} \\otimes \\mathcal{B}$ by $\\pi'(E) = \\int_X \\nu(E_x)\\,\\mathrm{d}\\mu(x)$. For a rectangle $A \\times B$, $\\pi'(A \\times B) = \\int_X \\nu(B)\\mathbf{1}_A(x)\\,\\mathrm{d}\\mu(x) = \\nu(B)\\mu(A)$. This is precisely the definition of the product measure $\\pi(A \\times B) = (\\mu \\otimes \\nu)(A \\times B)$. Since $\\mu$ and $\\nu$ are $\\sigma$-finite, the product measure is unique. Thus, $\\pi' = \\pi$ on $\\mathcal{A} \\otimes \\mathcal{B}$.\nThis means $\\int_{X \\times Y} \\mathbf{1}_E \\, \\mathrm{d}\\pi = \\pi(E) = \\pi'(E) = \\int_X \\nu(E_x)\\,\\mathrm{d}\\mu(x) = \\int_X\\left(\\int_Y \\mathbf{1}_E(x,y)\\,\\mathrm{d}\\nu(y)\\right)\\mathrm{d}\\mu(x)$, which proves the theorem for indicator functions.\n\n**Step 2: Non-negative Simple Functions**\nLet $s(x,y) = \\sum_{i=1}^n c_i \\mathbf{1}_{E_i}(x,y)$ be a non-negative simple function, where $c_i \\ge 0$ and $E_i \\in \\mathcal{A} \\otimes \\mathcal{B}$.\nBy linearity of the integral, $\\int_Y s(x,y)\\,\\mathrm{d}\\nu(y) = \\sum_{i=1}^n c_i \\int_Y \\mathbf{1}_{E_i}(x,y)\\,\\mathrm{d}\\nu(y) = \\sum_{i=1}^n c_i \\nu((E_i)_x)$. This is a linear combination of $\\mathcal{A}$-measurable functions (from Step 1), so it is $\\mathcal{A}$-measurable.\nApplying the integral over $X$ and using linearity again:\n$$ \\int_X\\left(\\int_Y s(x,y)\\,\\mathrm{d}\\nu(y)\\right)\\mathrm{d}\\mu(x) = \\sum_{i=1}^n c_i \\int_X \\nu((E_i)_x)\\,\\mathrm{d}\\mu(x) $$\nFrom Step 1, this equals $\\sum_{i=1}^n c_i \\pi(E_i)$. By definition of the integral of a simple function, this is $\\int_{X \\times Y} s\\,\\mathrm{d}\\pi$. The theorem holds for non-negative simple functions.\n\n**Step 3: Non-negative Measurable Functions**\nLet $f:X\\times Y\\to[0,\\infty]$ be an $\\mathcal{A}\\otimes\\mathcal{B}$-measurable function. By definition, there exists a sequence of non-negative simple measurable functions $\\{s_n\\}_{n=1}^\\infty$ such that $0 \\le s_n(x,y) \\uparrow f(x,y)$ for all $(x,y) \\in X \\times Y$.\nFor each fixed $x$, $0 \\le s_n(x,\\cdot) \\uparrow f(x,\\cdot)$. By the Monotone Convergence Theorem (MCT) applied to the measure space $(Y, \\mathcal{B}, \\nu)$:\n$$ \\int_Y s_n(x,y)\\,\\mathrm{d}\\nu(y) \\uparrow \\int_Y f(x,y)\\,\\mathrm{d}\\nu(y) $$\nLet $g_n(x) = \\int_Y s_n(x,y)\\,\\mathrm{d}\\nu(y)$ and $g(x) = \\int_Y f(x,y)\\,\\mathrm{d}\\nu(y)$. From Step 2, each $g_n$ is $\\mathcal{A}$-measurable. Since $g(x) = \\lim_{n\\to\\infty} g_n(x)$, $g$ is the pointwise limit of measurable functions, hence is $\\mathcal{A}$-measurable. This proves the measurability of the section map.\nNow, the sequence of non-negative $\\mathcal{A}$-measurable functions $\\{g_n\\}$ converges pointwise to $g$. By MCT applied to $(X, \\mathcal{A}, \\mu)$:\n$$ \\int_X g_n(x)\\,\\mathrm{d}\\mu(x) \\uparrow \\int_X g(x)\\,\\mathrm{d}\\mu(x) $$\nWe have $\\int_X g(x)\\,\\mathrm{d}\\mu(x) = \\lim_{n\\to\\infty} \\int_X g_n(x)\\,\\mathrm{d}\\mu(x) = \\lim_{n\\to\\infty} \\int_X\\left(\\int_Y s_n(x,y)\\,\\mathrm{d}\\nu(y)\\right)\\mathrm{d}\\mu(x)$.\nFrom Step 2, this is $\\lim_{n\\to\\infty} \\int_{X \\times Y} s_n\\,\\mathrm{d}\\pi$.\nFinally, by MCT on the product space $(X\\times Y, \\mathcal{A}\\otimes\\mathcal{B}, \\pi)$, since $s_n \\uparrow f$:\n$$ \\int_{X \\times Y} f\\,\\mathrm{d}\\pi = \\lim_{n\\to\\infty} \\int_{X \\times Y} s_n\\,\\mathrm{d}\\pi $$\nEquating the limits establishes the main result:\n$$ \\int_{X \\times Y} f\\,\\mathrm{d}(\\mu\\otimes\\nu) = \\int_X\\left(\\int_Y f(x,y)\\,\\mathrm{d}\\nu(y)\\right)\\mathrm{d}\\mu(x) $$\nThe argument for the other order of integration is identical due to symmetry. This completes the proof of the Fubini-Tonelli theorem.\n\n### Part 2: Application\n\nWe apply this theorem to calculate an integral. We are given the measure spaces $(\\Omega, \\mathcal{F}, \\mathbb{P})$ and $([0,\\infty), \\mathcal{B}([0,\\infty)), \\lambda)$, and the function $f(\\omega,t) = \\exp(-t)\\,\\mathbf{1}_{\\{|\\omega(t)|\\le a\\sqrt{t}\\}}$.\n\n**1. Measurability of $f$**\nThe function $f(\\omega,t)$ is a product of two functions: $h_1(\\omega,t) = \\exp(-t)$ and $h_2(\\omega,t) = \\mathbf{1}_{\\{|\\omega(t)|\\le a\\sqrt{t}\\}}$.\nThe function $h_1$ depends only on $t$ and is continuous, hence $\\mathcal{B}([0,\\infty))$-measurable. As a function on $\\Omega \\times [0,\\infty)$, it is $\\mathcal{F} \\otimes \\mathcal{B}([0,\\infty))$-measurable.\nFor $h_2$, we must show that the set $S = \\{(\\omega,t) \\in \\Omega \\times [0,\\infty) \\mid |\\omega(t)| \\le a\\sqrt{t}\\}$ is in $\\mathcal{F} \\otimes \\mathcal{B}([0,\\infty))$. This is equivalent to showing the joint measurability of the function $\\Psi(\\omega, t) = \\omega(t) - a\\sqrt{t}|\\text{sgn}(\\omega(t))|$ and checking its sign, but a more direct approach is to show the joint measurability of the evaluation map $\\Psi'(\\omega, t)=\\omega(t)$.\nThe map $\\omega \\mapsto \\omega(t)$ is $\\mathcal{F}$-measurable for fixed $t$ (it is the projection map $\\pi_t$). The map $t \\mapsto \\omega(t)$ is continuous for fixed $\\omega \\in \\Omega=C([0,\\infty),\\mathbb{R})$. The evaluation map $\\Psi'(\\omega, t) = \\omega(t)$ on the space of continuous paths is known to be jointly measurable. This can be shown by approximating $\\Psi'$ by a sequence of functions $\\Psi'_n(\\omega,t) = \\omega(\\lfloor 2^n t \\rfloor/2^n)$, which are simple in $t$ and measurable in $\\omega$, and thus jointly measurable. Since $\\omega$ is continuous, $\\Psi'_n \\to \\Psi'$ pointwise, so $\\Psi'$ is measurable.\nNow, the functions $(\\omega, t) \\mapsto |\\omega(t)|$ and $(\\omega,t) \\mapsto a\\sqrt{t}$ are measurable. Their difference $(\\omega,t) \\mapsto |\\omega(t)| - a\\sqrt{t}$ is measurable. The set $S$ is the preimage of $(-\\infty, 0]$ under this function, so $S \\in \\mathcal{F} \\otimes \\mathcal{B}([0,\\infty))$. Thus, its indicator function $h_2$ is measurable.\nSince $f$ is a product of two non-negative measurable functions, it is measurable.\n\n**2. Justification of Fubini-Tonelli**\nThe measure spaces are $(\\Omega, \\mathcal{F}, \\mathbb{P})$, which is a probability space and hence finite (thus $\\sigma$-finite), and $([0,\\infty), \\mathcal{B}([0,\\infty)), \\lambda)$, which is $\\sigma$-finite since $[0,\\infty) = \\bigcup_{n=1}^\\infty [0,n]$ and $\\lambda([0,n])=n\\infty$. The function $f$ is non-negative and has been shown to be measurable on the product space. All conditions of the Fubini-Tonelli theorem are satisfied. We can compute the double integral as an iterated integral.\n\n**3. Computation of the Integral**\nWe want to compute $I = \\int_{0}^{\\infty}\\int_{\\Omega} f(\\omega,t)\\,\\mathbb{P}(\\mathrm{d}\\omega)\\,\\lambda(\\mathrm{d}t)$. We first compute the inner integral for a fixed $t \\ge 0$.\n$$ I(t) = \\int_{\\Omega} \\exp(-t)\\,\\mathbf{1}_{\\{|\\omega(t)|\\le a\\sqrt{t}\\}}\\,\\mathbb{P}(\\mathrm{d}\\omega) = \\exp(-t) \\int_{\\Omega} \\mathbf{1}_{\\{|\\omega(t)|\\le a\\sqrt{t}\\}}\\,\\mathbb{P}(\\mathrm{d}\\omega) $$\nThe integral is the probability of the set:\n$$ I(t) = \\exp(-t) \\mathbb{P}(\\{\\omega \\in \\Omega : |\\omega(t)| \\le a\\sqrt{t}\\}) $$\nUnder the Wiener measure $\\mathbb{P}$, the coordinate process $W_t(\\omega) = \\omega(t)$ is a standard Brownian motion. For a fixed $t>0$, the random variable $W_t$ follows a normal distribution with mean $0$ and variance $t$, i.e., $W_t \\sim \\mathcal{N}(0,t)$.\nTherefore, the random variable $W_t/\\sqrt{t}$ has a standard normal distribution, $W_t/\\sqrt{t} \\sim \\mathcal{N}(0,1)$. Let $Z$ be a standard normal random variable.\nThe probability in question is:\n$$ \\mathbb{P}(|W_t| \\le a\\sqrt{t}) = \\mathbb{P}\\left(\\frac{|W_t|}{\\sqrt{t}} \\le a\\right) = \\mathbb{P}(|Z| \\le a) \\quad \\text{for } t>0 $$\nUsing the standard normal cumulative distribution function $\\Phi(x) = \\mathbb{P}(Z \\le x)$, this probability is:\n$$ \\mathbb{P}(-a \\le Z \\le a) = \\Phi(a) - \\Phi(-a) $$\nBy the symmetry of the normal distribution, $\\Phi(-a) = 1 - \\Phi(a)$. So,\n$$ \\mathbb{P}(|Z| \\le a) = \\Phi(a) - (1-\\Phi(a)) = 2\\Phi(a) - 1 $$\nThis value is constant for all $t > 0$.\nFor $t=0$, $W_0=0$ almost surely, so $|\\omega(0)|=0$. The condition is $|0| \\le a\\sqrt{0}$, which is $0 \\le 0$, so it holds. $\\mathbb{P}(|\\omega(0)| \\le 0)=1$. The inner integral at $t=0$ is $\\exp(0) \\cdot 1 = 1$.\nThe function we must integrate over $t$ is $I(t) = \\exp(-t)(2\\Phi(a)-1)$ for $t>0$. Since we are using Lebesgue integration, the value at the single point $t=0$ does not affect the integral.\n\nThe total integral is:\n$$ I = \\int_0^\\infty I(t) \\,\\lambda(\\mathrm{d}t) = \\int_0^\\infty \\exp(-t)(2\\Phi(a)-1)\\,\\mathrm{d}t $$\nSince $2\\Phi(a)-1$ is a constant with respect to $t$, we can pull it out of the integral:\n$$ I = (2\\Phi(a)-1) \\int_0^\\infty \\exp(-t)\\,\\mathrm{d}t $$\nThe remaining integral is elementary:\n$$ \\int_0^\\infty \\exp(-t)\\,\\mathrm{d}t = [-\\exp(-t)]_0^\\infty = -(\\lim_{t\\to\\infty}\\exp(-t)) - (-\\exp(-0)) = 0 - (-1) = 1 $$\nTherefore, the value of the double integral is:\n$$ I = 2\\Phi(a) - 1 $$", "answer": "$$\n\\boxed{2\\Phi(a) - 1}\n$$", "id": "2991570"}, {"introduction": "This practice moves from general theory to the construction of a specific, essential object: the Wiener process. You will use the powerful Kolmogorov Extension and Continuity theorems to prove the existence of a measure on the space of continuous paths that corresponds to Brownian motion. This exercise is key to understanding where the central object of stochastic calculus comes from and allows you to derive its fundamental covariance property from first principles [@problem_id:2991552].", "problem": "Consider the canonical path space $\\Omega := \\mathcal{C}([0,\\infty),\\mathbb{R})$ of all continuous functions $\\omega : [0,\\infty) \\to \\mathbb{R}$, equipped with the Borel $\\sigma$-algebra $\\mathcal{F}$ induced by the topology of uniform convergence on compact intervals. Define the coordinate (canonical) process $X = (X_t)_{t \\ge 0}$ on $(\\Omega,\\mathcal{F})$ by $X_t(\\omega) := \\omega(t)$. For a finite time grid $0 = t_0  t_1  \\dots  t_n$, consider the collection of cylinder sets and the family of finite-dimensional distributions specified by the following requirements:\n\n- For each $n \\in \\mathbb{N}$ and times $0 = t_0  t_1  \\dots  t_n$, the increments $(X_{t_k} - X_{t_{k-1}})_{k=1}^{n}$ are independent and, conditional on $X_{t_0} = 0$, each increment $X_{t_k} - X_{t_{k-1}}$ is centered Gaussian with variance $t_k - t_{k-1}$.\n\n- These finite-dimensional distributions are consistent under projection (so they define a projective system of measures on the cylinder $\\sigma$-algebra).\n\nUsing only foundational results from measure theory and probability theory—specifically, the Kolmogorov Extension Theorem and the Kolmogorov continuity theorem—carry out the following tasks:\n\n1. Prove that there exists a unique probability measure $\\mathbb{W}$ on $(\\Omega,\\mathcal{F})$, called the Wiener measure (WM), such that under $\\mathbb{W}$ the canonical process $X$ has independent increments as specified above.\n\n2. Starting from the moment structure of the increments and without presupposing continuity, use the Kolmogorov continuity theorem to show that $X$ admits a modification with continuous sample paths, and identify this modification with the canonical process on $\\Omega$. Conclude that under $\\mathbb{W}$ the canonical process has continuous paths.\n\n3. Compute the covariance function $\\mathbb{E}_{\\mathbb{W}}[X_s X_t]$ for arbitrary $s,t \\ge 0$.\n\nYour final answer must be a single closed-form analytic expression in terms of $s$ and $t$, with no rounding and no units. Do not present an inequality or an equation; present only the final expression for the covariance $\\mathbb{E}_{\\mathbb{W}}[X_s X_t]$.", "solution": "The problem is validated as scientifically grounded, well-posed, and objective. It represents a standard, foundational construction in the theory of stochastic processes. We proceed with the solution, addressing the three tasks in order.\n\nThe solution is divided into three parts. First, we establish the existence and uniqueness of a probability measure on a larger space using the Kolmogorov Extension Theorem. Second, we use the Kolmogorov continuity theorem to show this measure is concentrated on the space of continuous functions, thus defining the Wiener measure. Third, we compute the required covariance function.\n\n**1. Existence and Uniqueness of the Wiener Measure**\n\nOur first goal is to construct a probability measure satisfying the given conditions on the finite-dimensional distributions. The appropriate framework is the Kolmogorov Extension Theorem, which requires defining a consistent family of finite-dimensional probability distributions.\n\nLet $T_n = \\{t_1, t_2, \\dots, t_n\\}$ be any finite set of time points with $0 \\le t_1  t_2  \\dots  t_n$. We must define the joint probability distribution of the random vector $(X_{t_1}, X_{t_2}, \\dots, X_{t_n})$. The problem specifies that with $t_0=0$ and $X_{t_0}=0$, the increments $X_{t_k} - X_{t_{k-1}}$ are independent and distributed as $\\mathcal{N}(0, t_k - t_{k-1})$.\n\nThe random variable $X_{t_k}$ can be written as a sum of increments:\n$$ X_{t_k} = (X_{t_k} - X_{t_{k-1}}) + (X_{t_{k-1}} - X_{t_{k-2}}) + \\dots + (X_{t_1} - X_{t_0}) $$\nSince $X_{t_0}=0$, this is $X_{t_k} = \\sum_{j=1}^k (X_{t_j} - X_{t_{j-1}})$. As a sum of independent, centered Gaussian random variables, $X_{t_k}$ is itself a centered Gaussian. Its variance is the sum of the variances:\n$$ \\operatorname{Var}(X_{t_k}) = \\sum_{j=1}^k \\operatorname{Var}(X_{t_j} - X_{t_{j-1}}) = \\sum_{j=1}^k (t_j - t_{j-1}) = (t_1 - t_0) + (t_2 - t_1) + \\dots + (t_k - t_{k-1}) = t_k - t_0 = t_k $$\nThus, for any $t \\ge 0$, $X_t$ is a centered Gaussian random variable with variance $t$, i.e., $X_t \\sim \\mathcal{N}(0, t)$. This implies $\\mathbb{E}[X_t] = 0$ and $\\mathbb{E}[X_t^2] = t$.\n\nThe vector $(X_{t_1}, \\dots, X_{t_n})$ is a linear transformation of the independent Gaussian increments, and is therefore a multivariate Gaussian vector with a mean vector of $\\mathbf{0}$. We need to determine its covariance matrix $\\Sigma$, where $\\Sigma_{ij} = \\mathbb{E}[X_{t_i} X_{t_j}]$. Let's assume without loss of generality that $t_i \\le t_j$. We have:\n$$ \\mathbb{E}[X_{t_i} X_{t_j}] = \\mathbb{E}[X_{t_i} (X_{t_i} + (X_{t_j} - X_{t_i}))] = \\mathbb{E}[X_{t_i}^2] + \\mathbb{E}[X_{t_i} (X_{t_j} - X_{t_i})] $$\nThe increment $X_{t_j} - X_{t_i}$ is independent of the history of the process up to time $t_i$, denoted $\\mathcal{F}_{t_i} = \\sigma(X_s : s \\le t_i)$. Since $X_{t_i}$ is measurable with respect to $\\mathcal{F}_{t_i}$, $X_{t_i}$ and $X_{t_j} - X_{t_i}$ are independent. Therefore:\n$$ \\mathbb{E}[X_{t_i} (X_{t_j} - X_{t_i})] = \\mathbb{E}[X_{t_i}] \\mathbb{E}[X_{t_j} - X_{t_i}] = 0 \\cdot 0 = 0 $$\nThis gives $\\mathbb{E}[X_{t_i} X_{t_j}] = \\mathbb{E}[X_{t_i}^2] = t_i$. Combining with the symmetric case ($t_j  t_i$), the covariance is $\\mathbb{E}[X_{t_i} X_{t_j}] = \\min(t_i, t_j)$.\n\nFor each finite set of times $T_n = \\{t_1, \\dots, t_n\\}$, we define the finite-dimensional distribution $\\mu_{T_n}$ to be the law of a multivariate normal vector with mean $\\mathbf{0}$ and covariance matrix $\\Sigma$ with entries $\\Sigma_{ij} = \\min(t_i, t_j)$.\n\nThe problem states that these distributions are consistent. This is a crucial condition for the Kolmogorov Extension Theorem. Let's verify this explicitly. Let $T_m = \\{t_1, \\dots, t_m\\}$ and $T_{m-1} = \\{t_1, \\dots, t_{k-1}, t_{k+1}, \\dots, t_m\\}$. The measure $\\mu_{T_m}$ is a Gaussian measure on $\\mathbb{R}^m$. The marginal distribution of any sub-vector is also Gaussian. The distribution of $(X_{t_1}, \\dots, X_{t_{k-1}}, X_{t_{k+1}}, \\dots, X_{t_m})$ under $\\mu_{T_m}$ is a multivariate normal with mean $\\mathbf{0}$ and a covariance matrix obtained by deleting the $k$-th row and column from the matrix for $\\mu_{T_m}$. This resulting matrix has entries $\\min(t_i, t_j)$ for indices $i,j \\in \\{1,\\dots,k-1,k+1,\\dots,m\\}$, which is precisely the covariance matrix for the distribution $\\mu_{T_{m-1}}$. The family of distributions $\\{\\mu_T\\}$ is therefore consistent.\n\nBy the Kolmogorov Extension Theorem, there exists a unique probability measure $\\mathbb{P}$ on the product space $(\\mathbb{R}^{[0,\\infty)}, \\mathcal{B}(\\mathbb{R}^{[0,\\infty)}))$ (where $\\mathcal{B}(\\mathbb{R}^{[0,\\infty)})$ is the product $\\sigma$-algebra) such that for any finite set of times $T = \\{t_1, \\dots, t_n\\}$, the law of the projection $(X_{t_1}, \\dots, X_{t_n})$ under $\\mathbb{P}$ is $\\mu_T$. The canonical process $X_t(\\omega) = \\omega(t)$ on this space has the required finite-dimensional distributions.\n\n**2. Continuity of Sample Paths**\n\nThe measure $\\mathbb{P}$ is defined on the space of all functions from $[0,\\infty)$ to $\\mathbb{R}$, which is much larger than the space of continuous functions $\\Omega = \\mathcal{C}([0,\\infty),\\mathbb{R})$. We must now show that the measure $\\mathbb{P}$ is concentrated on $\\Omega$, i.e., $\\mathbb{P}(\\Omega) = 1$. To this end, we use the Kolmogorov continuity theorem (specifically, the version by Centsov).\n\nThe theorem states that if a stochastic process $(X_t)_{t\\in[0,T]}$ satisfies the condition\n$$ \\mathbb{E}[|X_t - X_s|^p] \\le C |t-s|^{1+q} $$\nfor some positive constants $p, q, C$ and for all $s,t \\in [0,T]$, then there exists a modification of $X$ that has continuous sample paths. A modification $\\tilde{X}$ of $X$ is a process such that $\\mathbb{P}(\\tilde{X}_t = X_t) = 1$ for all $t$.\n\nLet's check the condition for our process $X$. For $s  t$, the increment $X_t - X_s$ is distributed as $\\mathcal{N}(0, t-s)$. Let $Z \\sim \\mathcal{N}(0,1)$. Then $X_t - X_s$ has the same distribution as $\\sqrt{t-s} \\, Z$. We compute the $p$-th moment of the absolute value:\n$$ \\mathbb{E}[|X_t - X_s|^p] = \\mathbb{E}[|\\sqrt{t-s} \\, Z|^p] = |t-s|^{p/2} \\, \\mathbb{E}[|Z|^p] $$\nThe quantity $\\mathbb{E}[|Z|^p]$ is a finite constant for any $p > 0$. We need to find a $p > 0$ such that the exponent of $|t-s|$ is greater than $1$. We need $p/2 > 1$, which implies $p > 2$. Let's choose $p=4$. The fourth moment of a standard normal variable is $\\mathbb{E}[Z^4] = 3$. So,\n$$ \\mathbb{E}[|X_t - X_s|^4] = |t-s|^{4/2} \\, \\mathbb{E}[Z^4] = 3|t-s|^2 $$\nThis inequality holds with $p=4$, $C=3$, and $1+q=2$, so $q=1$. Since $p=4>0$ and $q=1>0$, the condition of the Kolmogorov continuity theorem is satisfied on any compact interval $[0,T]$.\n\nBy applying the theorem on a sequence of intervals $[0,N]$ for $N \\in \\mathbb{N}$, we can conclude there is a continuous modification $\\tilde{X}$ of $X$ on $[0, \\infty)$. This means there exists a process $\\tilde{X}$ with continuous paths such that $\\mathbb{P}(X_t \\neq \\tilde{X}_t \\text{ for some } t \\in [0,\\infty)) = 0$. More precisely, the set of paths $\\omega \\in \\mathbb{R}^{[0,\\infty)}$ which are not continuous has $\\mathbb{P}$-measure zero. Let $\\Omega_c = \\mathcal{C}([0,\\infty), \\mathbb{R})$ be the set of continuous paths. We have just shown that $\\mathbb{P}(\\Omega_c) = 1$.\n\nWe can now define the Wiener measure $\\mathbb{W}$ on the space $(\\Omega, \\mathcal{F}) = (\\mathcal{C}([0,\\infty),\\mathbb{R}), \\mathcal{F})$, where $\\mathcal{F}$ is the Borel $\\sigma$-algebra induced by the topology of uniform convergence on compacts. We define $\\mathbb{W}$ as the restriction of $\\mathbb{P}$ to $\\Omega$. For any set $A \\in \\mathcal{F}$, we define $\\mathbb{W}(A) = \\mathbb{P}(A)$. Since $\\mathbb{P}(\\Omega)=1$, $\\mathbb{W}$ is a well-defined probability measure on $(\\Omega, \\mathcal{F})$.\n\nThe canonical process on this new space $(\\Omega, \\mathcal{F}, \\mathbb{W})$ is $X_t(\\omega) = \\omega(t)$. By definition of $\\Omega$, its paths are continuous. The finite-dimensional distributions of this process are the same as those of the original process under $\\mathbb{P}$, because the cylinder sets generating the $\\sigma$-algebra $\\mathcal{F}$ are members of $\\mathcal{B}(\\mathbb{R}^{[0,\\infty)})$, and $\\mathbb{W}$ agrees with $\\mathbb{P}$ on them. The uniqueness of $\\mathbb{W}$ on $(\\Omega, \\mathcal{F})$ follows from the fact that the cylinder sets form a $\\pi$-system that generates $\\mathcal{F}$, and any two measures that agree on a generating $\\pi$-system must agree on the entire $\\sigma$-algebra. This completes the construction and justification.\n\n**3. Computation of the Covariance Function**\n\nWe are asked to compute the covariance function $\\mathbb{E}_{\\mathbb{W}}[X_s X_t]$ for arbitrary $s,t \\ge 0$. As established from the construction of $\\mathbb{W}$, this expectation is identical to the one computed under the measure $\\mathbb{P}$.\n\nLet $s, t \\ge 0$. We assume, without loss of generality, that $s \\le t$.\nWe can write $X_t = X_s + (X_t - X_s)$. Using the linearity of expectation:\n$$ \\mathbb{E}_{\\mathbb{W}}[X_s X_t] = \\mathbb{E}_{\\mathbb{W}}[X_s(X_s + (X_t - X_s))] = \\mathbb{E}_{\\mathbb{W}}[X_s^2] + \\mathbb{E}_{\\mathbb{W}}[X_s(X_t-X_s)] $$\nAccording to the properties defining the process, the increment $X_t - X_s$ is independent of the process up to time $s$, which includes the random variable $X_s$. Therefore, we can separate the expectation of the product:\n$$ \\mathbb{E}_{\\mathbb{W}}[X_s(X_t-X_s)] = \\mathbb{E}_{\\mathbb{W}}[X_s]\\mathbb{E}_{\\mathbb{W}}[X_t-X_s] $$\nFrom the problem definition, $X_t - X_s \\sim \\mathcal{N}(0, t-s)$, so its expectation is $\\mathbb{E}_{\\mathbb{W}}[X_t-X_s]=0$. Similarly, $X_s = X_s - X_0 \\sim \\mathcal{N}(0,s)$, so $\\mathbb{E}_{\\mathbb{W}}[X_s]=0$. The product of expectations is thus $0 \\cdot 0 = 0$.\n\nThe covariance expression simplifies to:\n$$ \\mathbb{E}_{\\mathbb{W}}[X_s X_t] = \\mathbb{E}_{\\mathbb{W}}[X_s^2] $$\nThe term $\\mathbb{E}_{\\mathbb{W}}[X_s^2]$ is the second moment of $X_s$. Since $\\mathbb{E}_{\\mathbb{W}}[X_s]=0$, this is equal to the variance of $X_s$:\n$$ \\mathbb{E}_{\\mathbb{W}}[X_s^2] = \\operatorname{Var}(X_s) + (\\mathbb{E}_{\\mathbb{W}}[X_s])^2 = \\operatorname{Var}(X_s) + 0^2 = \\operatorname{Var}(X_s) $$\nThe variance of $X_s = X_s - X_0$ is given as $s-0=s$.\nSo, for $s \\le t$, we have $\\mathbb{E}_{\\mathbb{W}}[X_s X_t] = s$.\n\nBy symmetry, if we had assumed $t \\le s$, the same argument would yield $\\mathbb{E}_{\\mathbb{W}}[X_s X_t] = t$.\nCombining both cases, for any arbitrary $s,t \\ge 0$, the covariance is given by the smaller of the two values.\n\n$$ \\mathbb{E}_{\\mathbb{W}}[X_s X_t] = \\min(s, t) $$\nThis expression is the final answer for the covariance function.", "answer": "$$\\boxed{\\min(s,t)}$$", "id": "2991552"}, {"introduction": "The final practice explores conditional expectation, the primary tool for analyzing information in stochastic systems. This exercise reveals the profound geometric interpretation of conditional expectation as an orthogonal projection in the Hilbert space of square-integrable random variables, $L^{2}$. By proving this relationship and then computing an explicit projection involving Brownian motion, you will gain a deep, intuitive understanding of conditional expectation as the \"best estimate\" given partial information [@problem_id:2991554].", "problem": "Let $\\left(\\Omega,\\mathcal{F},\\mathbb{P}\\right)$ be a probability space and let $L^{2}\\left(\\Omega,\\mathcal{F},\\mathbb{P}\\right)$ denote the Hilbert space of square-integrable functions with inner product $\\langle X,Y\\rangle = \\mathbb{E}[XY]$. Let $\\mathcal{G}\\subseteq\\mathcal{F}$ be a sub-$\\sigma$-algebra, and let $L^{2}\\left(\\Omega,\\mathcal{G},\\mathbb{P}\\right)$ be the closed linear subspace of $L^{2}\\left(\\Omega,\\mathcal{F},\\mathbb{P}\\right)$ consisting of $\\mathcal{G}$-measurable square-integrable functions.\n\nStarting only from the definitions of conditional expectation and Hilbert space orthogonality, and standard facts about Hilbert spaces, perform the following:\n\n- Prove that for every $X\\in L^{2}\\left(\\Omega,\\mathcal{F},\\mathbb{P}\\right)$, the conditional expectation $\\mathbb{E}\\left[X\\mid\\mathcal{G}\\right]$ is the unique element of $L^{2}\\left(\\Omega,\\mathcal{G},\\mathbb{P}\\right)$ such that $X-\\mathbb{E}\\left[X\\mid\\mathcal{G}\\right]$ is orthogonal to every $Y\\in L^{2}\\left(\\Omega,\\mathcal{G},\\mathbb{P}\\right)$, and that $\\mathbb{E}\\left[X\\mid\\mathcal{G}\\right]$ minimizes the $L^{2}$-distance to $X$ over $L^{2}\\left(\\Omega,\\mathcal{G},\\mathbb{P}\\right)$. In particular, prove that $L^{2}\\left(\\Omega,\\mathcal{G},\\mathbb{P}\\right)$ is a closed subspace of $L^{2}\\left(\\Omega,\\mathcal{F},\\mathbb{P}\\right)$ and that for any $Y\\in L^{2}\\left(\\Omega,\\mathcal{G},\\mathbb{P}\\right)$,\n$\\mathbb{E}\\left[\\left(X-\\mathbb{E}\\left[X\\mid\\mathcal{G}\\right]\\right)Y\\right]=0$.\n\n- Work on a filtered probability space $\\left(\\Omega,\\mathcal{F},\\left(\\mathcal{F}_{t}\\right)_{t\\ge 0},\\mathbb{P}\\right)$ carrying a standard Brownian motion $\\left(W_{t}\\right)_{t\\ge 0}$. Let $\\alpha,\\beta\\in\\mathbb{R}$ and define the square-integrable random variable $X=\\exp\\!\\left(\\alpha W_{1}+\\beta\\left(W_{2}-W_{1}\\right)\\right)$ and the sub-$\\sigma$-algebra $\\mathcal{G}=\\sigma\\!\\left(W_{2}\\right)$. Compute explicitly the orthogonal projection of $X$ onto $L^{2}\\left(\\Omega,\\mathcal{G},\\mathbb{P}\\right)$, that is, compute $\\mathbb{E}\\left[X\\mid\\mathcal{G}\\right]$ as a function of $W_{2}$ and the parameters $\\alpha,\\beta$.\n\nProvide your final answer as a single closed-form analytic expression for $\\mathbb{E}\\left[X\\mid\\mathcal{G}\\right]$ in terms of $W_{2}$, $\\alpha$, and $\\beta$.", "solution": "The problem presented is valid, as it is mathematically well-posed, scientifically grounded in the established theory of probability and stochastic processes, and all its components are formally defined and internally consistent. We shall proceed with a full solution.\n\nThe problem consists of two parts. The first is a proof of a fundamental theorem relating conditional expectation to orthogonal projection in a Hilbert space. The second is an explicit calculation of such a projection in the context of Brownian motion.\n\n**Part 1: Conditional Expectation as Orthogonal Projection**\n\nLet $\\left(\\Omega,\\mathcal{F},\\mathbb{P}\\right)$ be a probability space, $L^{2}\\left(\\Omega,\\mathcal{F},\\mathbb{P}\\right)$ the Hilbert space of square-integrable random variables with inner product $\\langle X,Y\\rangle = \\mathbb{E}[XY]$, and let $\\mathcal{G}\\subseteq\\mathcal{F}$ be a sub-$\\sigma$-algebra. We are asked to prove that for any $X\\in L^{2}\\left(\\Omega,\\mathcal{F},\\mathbb{P}\\right)$, the conditional expectation $\\mathbb{E}\\left[X\\mid\\mathcal{G}\\right]$ is the unique orthogonal projection of $X$ onto the subspace $L^{2}\\left(\\Omega,\\mathcal{G},\\mathbb{P}\\right)$.\n\nFirst, we establish that $L^{2}\\left(\\Omega,\\mathcal{G},\\mathbb{P}\\right)$ is a closed linear subspace of $L^{2}\\left(\\Omega,\\mathcal{F},\\mathbb{P}\\right)$.\nIt is a linear subspace, as if $Y_1, Y_2 \\in L^{2}\\left(\\Omega,\\mathcal{G},\\mathbb{P}\\right)$ and $c_1, c_2 \\in \\mathbb{R}$, then $c_1 Y_1 + c_2 Y_2$ is clearly square-integrable and $\\mathcal{G}$-measurable. To show it is closed, let $(Y_n)_{n\\ge 1}$ be a sequence in $L^{2}\\left(\\Omega,\\mathcal{G},\\mathbb{P}\\right)$ that converges to a limit $Y \\in L^{2}\\left(\\Omega,\\mathcal{F},\\mathbb{P}\\right)$ in the $L^2$ norm. This means $\\mathbb{E}\\left[\\left(Y_n - Y\\right)^2\\right] \\to 0$ as $n \\to \\infty$. Convergence in $L^2$ implies convergence in probability. A standard result states that convergence in probability implies the existence of a subsequence $(Y_{n_k})_{k\\ge 1}$ that converges to $Y$ almost surely. That is, $Y_{n_k}(\\omega) \\to Y(\\omega)$ for all $\\omega$ in a set $A \\in \\mathcal{F}$ with $\\mathbb{P}(A)=1$. Since each $Y_{n_k}$ is $\\mathcal{G}$-measurable, their pointwise limit $Y$ must also be $\\mathcal{G}$-measurable (up to a null set, which does not affect its status as an element of the $L^2$ space). Therefore, $Y \\in L^{2}\\left(\\Omega,\\mathcal{G},\\mathbb{P}\\right)$, and the subspace is closed.\n\nAccording to the Hilbert Projection Theorem, for any closed subspace $M$ of a Hilbert space $H$, every vector $x \\in H$ can be uniquely decomposed as $x = P_M(x) + (x-P_M(x))$, where $P_M(x) \\in M$ is the orthogonal projection of $x$ onto $M$, and the residual $x-P_M(x)$ is orthogonal to every vector in $M$. This projection $P_M(x)$ is also the unique element in $M$ that minimizes the distance $\\|x-y\\|$ for all $y \\in M$.\n\nApplying this theorem to our context, we set $H = L^{2}\\left(\\Omega,\\mathcal{F},\\mathbb{P}\\right)$ and $M = L^{2}\\left(\\Omega,\\mathcal{G},\\mathbb{P}\\right)$. For any $X \\in H$, there exists a unique element, let us call it $Z$, in $M$ such that $X-Z$ is orthogonal to $M$. This means $\\langle X-Z, Y \\rangle = \\mathbb{E}\\left[(X-Z)Y\\right] = 0$ for all $Y \\in M$. This unique $Z$ is the orthogonal projection and minimizes the $L^2$-distance $\\|X-Y\\|_{L^2} = \\sqrt{\\mathbb{E}\\left[(X-Y)^2\\right]}$ over all $Y \\in M$.\n\nIt remains to show that this unique projection $Z$ is precisely the conditional expectation $\\mathbb{E}\\left[X\\mid\\mathcal{G}\\right]$. We will show that $\\mathbb{E}\\left[X\\mid\\mathcal{G}\\right]$ has the properties of the orthogonal projection $Z$. By the uniqueness of the projection, this will establish their identity.\n\nLet $Z_{CE} = \\mathbb{E}\\left[X\\mid\\mathcal{G}\\right]$.\n1.  We must show $Z_{CE} \\in L^{2}\\left(\\Omega,\\mathcal{G},\\mathbb{P}\\right)$. By definition, $Z_{CE}$ is $\\mathcal{G}$-measurable. Using Jensen's inequality for conditional expectations, we have $\\left(\\mathbb{E}\\left[X\\mid\\mathcal{G}\\right]\\right)^2 \\le \\mathbb{E}\\left[X^2\\mid\\mathcal{G}\\right]$. Taking expectations on both sides and using the tower property, we get $\\mathbb{E}\\left[\\left(\\mathbb{E}\\left[X\\mid\\mathcal{G}\\right]\\right)^2\\right] \\le \\mathbb{E}\\left[\\mathbb{E}\\left[X^2\\mid\\mathcal{G}\\right]\\right] = \\mathbb{E}\\left[X^2\\right]$. Since $X \\in L^{2}\\left(\\Omega,\\mathcal{F},\\mathbb{P}\\right)$, we have $\\mathbb{E}\\left[X^2\\right]  \\infty$, which implies $\\mathbb{E}\\left[Z_{CE}^2\\right]  \\infty$. Thus, $Z_{CE} \\in L^{2}\\left(\\Omega,\\mathcal{G},\\mathbb{P}\\right)$.\n\n2.  We must show that $X-Z_{CE}$ is orthogonal to every $Y \\in L^{2}\\left(\\Omega,\\mathcal{G},\\mathbb{P}\\right)$. The inner product is $\\langle X-Z_{CE}, Y \\rangle = \\mathbb{E}\\left[(X-Z_{CE})Y\\right]$. Since $X, Z_{CE}, Y$ are all in $L^2$, the product $(X-Z_{CE})Y$ is integrable by the Cauchy-Schwarz inequality. We use the tower property of conditional expectation:\n    $$ \\mathbb{E}\\left[(X-Z_{CE})Y\\right] = \\mathbb{E}\\left[\\mathbb{E}\\left[(X-Z_{CE})Y \\mid \\mathcal{G}\\right]\\right] $$\n    Since $Y$ is $\\mathcal{G}$-measurable, it can be treated as a constant inside the inner conditional expectation:\n    $$ \\mathbb{E}\\left[Y \\cdot \\mathbb{E}\\left[X-Z_{CE} \\mid \\mathcal{G}\\right]\\right] $$\n    By the linearity of conditional expectation and the fact that $Z_{CE}=\\mathbb{E}\\left[X\\mid\\mathcal{G}\\right]$ is itself $\\mathcal{G}$-measurable:\n    $$ \\mathbb{E}\\left[X-Z_{CE} \\mid \\mathcal{G}\\right] = \\mathbb{E}\\left[X \\mid \\mathcal{G}\\right] - \\mathbb{E}\\left[Z_{CE} \\mid \\mathcal{G}\\right] = Z_{CE} - Z_{CE} = 0 $$\n    Therefore, the inner product is $\\mathbb{E}\\left[Y \\cdot 0\\right] = 0$. This establishes that $X-Z_{CE}$ is orthogonal to every $Y \\in L^{2}\\left(\\Omega,\\mathcal{G},\\mathbb{P}\\right)$.\n\nSince $\\mathbb{E}\\left[X\\mid\\mathcal{G}\\right]$ satisfies the two defining properties of the unique orthogonal projection of $X$ onto $L^{2}\\left(\\Omega,\\mathcal{G},\\mathbb{P}\\right)$, it must be that projection. Consequently, $\\mathbb{E}\\left[X\\mid\\mathcal{G}\\right]$ is the unique element in $L^{2}\\left(\\Omega,\\mathcal{G},\\mathbb{P}\\right)$ that minimizes the $L^2$-distance to $X$. This completes the proof.\n\n**Part 2: Explicit Computation**\n\nWe are tasked with computing $\\mathbb{E}\\left[X\\mid\\mathcal{G}\\right]$ where $X=\\exp\\!\\left(\\alpha W_{1}+\\beta\\left(W_{2}-W_{1}\\right)\\right)$ and $\\mathcal{G}=\\sigma\\!\\left(W_{2}\\right)$. The process $(W_t)_{t \\ge 0}$ is a standard Brownian motion.\n\nLet's rewrite the exponent of $X$:\n$$ \\alpha W_1 + \\beta(W_2 - W_1) = (\\alpha - \\beta)W_1 + \\beta W_2 $$\nSo, $X = \\exp\\left((\\alpha - \\beta)W_1 + \\beta W_2\\right)$. We wish to compute the conditional expectation of $X$ given $\\mathcal{G}=\\sigma(W_2)$. As $\\exp(\\beta W_2)$ is a function of $W_2$, it is $\\mathcal{G}$-measurable. Thus, we can factor it out of the conditional expectation:\n$$ \\mathbb{E}\\left[X\\mid\\mathcal{G}\\right] = \\mathbb{E}\\left[\\exp\\left((\\alpha - \\beta)W_1 + \\beta W_2\\right) \\mid W_2\\right] = \\exp(\\beta W_2) \\mathbb{E}\\left[\\exp\\left((\\alpha - \\beta)W_1\\right) \\mid W_2\\right] $$\nTo compute the remaining conditional expectation, we use the fact that $(W_1, W_2)$ form a bivariate Gaussian vector.\nThe mean vector is $(\\mathbb{E}[W_1], \\mathbb{E}[W_2])^T = (0,0)^T$.\nThe covariance matrix $\\Sigma$ has entries:\n$\\Sigma_{11} = \\text{Var}(W_1) = 1$.\n$\\Sigma_{22} = \\text{Var}(W_2) = 2$.\n$\\Sigma_{12} = \\Sigma_{21} = \\text{Cov}(W_1, W_2) = \\mathbb{E}[W_1 W_2] = \\mathbb{E}[W_1(W_1 + (W_2-W_1))] = \\mathbb{E}[W_1^2] + \\mathbb{E}[W_1(W_2-W_1)]$.\nSince $W_1$ and $W_2-W_1$ are independent increments of Brownian motion with mean $0$, $\\mathbb{E}[W_1(W_2-W_1)]=\\mathbb{E}[W_1]\\mathbb{E}[W_2-W_1]=0$.\nThus, $\\text{Cov}(W_1, W_2) = \\mathbb{E}[W_1^2] = \\text{Var}(W_1) = 1$.\nThe covariance matrix is $\\Sigma = \\begin{pmatrix} 1  1 \\\\ 1  2 \\end{pmatrix}$.\n\nThe conditional distribution of $W_1$ given $W_2=w_2$ is a normal distribution with mean $\\mu_{1|2}$ and variance $\\sigma^2_{1|2}$ given by:\n$$ \\mu_{1|2} = \\mathbb{E}[W_1] + \\frac{\\text{Cov}(W_1, W_2)}{\\text{Var}(W_2)}(w_2 - \\mathbb{E}[W_2]) = 0 + \\frac{1}{2}(w_2 - 0) = \\frac{w_2}{2} $$\n$$ \\sigma^2_{1|2} = \\text{Var}(W_1) - \\frac{\\text{Cov}(W_1, W_2)^2}{\\text{Var}(W_2)} = 1 - \\frac{1^2}{2} = \\frac{1}{2} $$\nSo, conditioned on $W_2=w_2$, $W_1$ follows a normal distribution $N\\left(\\frac{w_2}{2}, \\frac{1}{2}\\right)$.\n\nLet $c = \\alpha-\\beta$. We need to calculate $\\mathbb{E}\\left[\\exp(c W_1)\\mid W_2=w_2\\right]$. This is the moment-generating function (MGF) of the conditional distribution of $W_1$, evaluated at $c$. The MGF of a normal distribution $N(\\mu, \\sigma^2)$ is $M(t) = \\exp(\\mu t + \\frac{1}{2}\\sigma^2 t^2)$.\nWith $\\mu = \\mu_{1|2} = \\frac{w_2}{2}$ and $\\sigma^2 = \\sigma^2_{1|2} = \\frac{1}{2}$, we have:\n$$ \\mathbb{E}\\left[\\exp(c W_1)\\mid W_2=w_2\\right] = \\exp\\left(\\left(\\frac{w_2}{2}\\right)c + \\frac{1}{2}\\left(\\frac{1}{2}\\right)c^2\\right) = \\exp\\left(\\frac{c w_2}{2} + \\frac{c^2}{4}\\right) $$\nReplacing $w_2$ with the random variable $W_2$ and $c$ with $\\alpha-\\beta$:\n$$ \\mathbb{E}\\left[\\exp\\left((\\alpha - \\beta)W_1\\right) \\mid W_2\\right] = \\exp\\left(\\frac{(\\alpha - \\beta)W_2}{2} + \\frac{(\\alpha - \\beta)^2}{4}\\right) $$\nFinally, we substitute this back into our expression for $\\mathbb{E}\\left[X\\mid\\mathcal{G}\\right]$:\n$$ \\mathbb{E}\\left[X\\mid\\mathcal{G}\\right] = \\exp(\\beta W_2) \\cdot \\exp\\left(\\frac{(\\alpha - \\beta)W_2}{2} + \\frac{(\\alpha - \\beta)^2}{4}\\right) $$\nCombining the exponents:\n$$ \\beta W_2 + \\frac{(\\alpha - \\beta)W_2}{2} + \\frac{(\\alpha - \\beta)^2}{4} = \\left(\\frac{2\\beta + \\alpha - \\beta}{2}\\right)W_2 + \\frac{(\\alpha - \\beta)^2}{4} = \\left(\\frac{\\alpha + \\beta}{2}\\right)W_2 + \\frac{(\\alpha - \\beta)^2}{4} $$\nThus, the orthogonal projection of $X$ onto $L^2(\\Omega, \\sigma(W_2), \\mathbb{P})$ is:\n$$ \\mathbb{E}\\left[X \\mid W_2\\right] = \\exp\\left(\\left(\\frac{\\alpha+\\beta}{2}\\right)W_2 + \\frac{(\\alpha-\\beta)^2}{4}\\right) $$\nThis expression is a function of $W_2$ and the parameters $\\alpha,\\beta$, as required.", "answer": "$$\n\\boxed{\\exp\\left(\\frac{\\alpha+\\beta}{2} W_{2} + \\frac{\\left(\\alpha-\\beta\\right)^{2}}{4}\\right)}\n$$", "id": "2991554"}]}