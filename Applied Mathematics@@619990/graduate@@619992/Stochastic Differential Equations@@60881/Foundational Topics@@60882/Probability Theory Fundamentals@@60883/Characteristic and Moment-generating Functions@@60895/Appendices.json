{"hands_on_practices": [{"introduction": "This problem goes to the heart of the connection between stochastic processes and partial differential equations. You will derive the backward PDE that governs the Laplace transform of the quadratic variation of a diffusion process, a classic application of the Feynman-Kac principle. This powerful technique transforms the problem of computing an expectation over infinitely many random paths into solving a deterministic PDE, a cornerstone of modern quantitative finance and physics. [@problem_id:2970786]", "problem": "Consider a one-dimensional stochastic differential equation (SDE) driven by Brownian motion (BM), given by $dX_{t}=\\sigma(X_{t})\\,dW_{t}$ with $X_{0}=x\\in\\mathbb{R}$, where $\\sigma:\\mathbb{R}\\to\\mathbb{R}$ is globally Lipschitz. Let $[X]_{t}$ denote the quadratic variation of $X$ up to time $t$, and recall that by Itô calculus, $[X]_{t}=\\int_{0}^{t}\\sigma^{2}(X_{s})\\,ds$. Define, for $\\lambda\\geq 0$, the Laplace transform $L(\\lambda;t,x)$ of the quadratic variation by $L(\\lambda;t,x)=\\mathbb{E}_{x}\\!\\left[\\exp\\!\\big(-\\lambda\\,[X]_{t}\\big)\\right]=\\mathbb{E}_{x}\\!\\left[\\exp\\!\\left(-\\lambda\\int_{0}^{t}\\sigma^{2}(X_{s})\\,ds\\right)\\right]$, where $\\mathbb{E}_{x}$ denotes expectation conditioned on $X_{0}=x$. \n\nStarting from fundamental definitions (Itô calculus, generator of the diffusion, and the Feynman–Kac principle) rather than any shortcut formulas, derive a representation for $L(\\lambda;t,x)$ as the unique bounded classical solution to a backward partial differential equation (PDE) with appropriate initial condition. Clearly justify how each term in the PDE arises from the stochastic representation.\n\nThen, under the additional assumption that $\\sigma(\\cdot)$ is a positive constant $\\sigma_{0}>0$, compute the Laplace transform $L(\\lambda;t,x)$ in closed form. \n\nFinally, interpret the finiteness conditions for the transform and for the moment-generating function (MGF) $M(\\theta;t,x)=\\mathbb{E}_{x}\\!\\left[\\exp\\!\\big(\\theta\\,[X]_{t}\\big)\\right]$ with $\\theta\\in\\mathbb{R}$: explain for which values of $\\lambda$ or $\\theta$ these expectations are finite, and how properties of $\\sigma$ (e.g., boundedness versus growth) influence finiteness. \n\nExpress your final answer as the closed-form Laplace transform for the constant-diffusion case; no rounding is required and there are no physical units.", "solution": "The problem is divided into three parts: first, to derive a partial differential equation (PDE) for the Laplace transform of the quadratic variation of a stochastic process; second, to solve this PDE for a special case; and third, to discuss the finiteness conditions for related transforms.\n\n### Step 1: Validation of the Problem Statement\n\n**Givens Extracted:**\n- Stochastic differential equation (SDE): $dX_{t}=\\sigma(X_{t})\\,dW_{t}$\n- Initial condition: $X_{0}=x\\in\\mathbb{R}$\n- Diffusion coefficient: $\\sigma:\\mathbb{R}\\to\\mathbb{R}$ is globally Lipschitz.\n- Quadratic variation: $[X]_{t}=\\int_{0}^{t}\\sigma^{2}(X_{s})\\,ds$\n- Laplace transform: $L(\\lambda;t,x)=\\mathbb{E}_{x}\\!\\left[\\exp\\!\\big(-\\lambda\\,[X]_{t}\\big)\\right]=\\mathbb{E}_{x}\\!\\left[\\exp\\!\\left(-\\lambda\\int_{0}^{t}\\sigma^{2}(X_{s})\\,ds\\right)\\right]$ for $\\lambda\\geq 0$.\n- Moment-generating function (MGF): $M(\\theta;t,x)=\\mathbb{E}_{x}\\!\\left[\\exp\\!\\big(\\theta\\,[X]_{t}\\big)\\right]$ for $\\theta\\in\\mathbb{R}$.\n\n**Validation Analysis:**\nThe problem statement is scientifically grounded, well-posed, and objective. It is rooted in the established mathematical theory of stochastic differential equations, Itô calculus, and the Feynman-Kac principle. The SDE is well-defined, and the existence and uniqueness of its solution are guaranteed by the global Lipschitz condition on $\\sigma$. The tasks—deriving a PDE, solving it in a specific case, and analyzing convergence conditions—are standard and non-trivial exercises in this field. The problem contains no scientific flaws, ambiguities, or contradictions. All necessary information is provided.\n\n**Verdict:** The problem is valid.\n\n### Part 1: Derivation of the PDE\n\nLet $u(t,x) = L(\\lambda;t,x) = \\mathbb{E}_{x}\\left[\\exp\\left(-\\lambda\\int_{0}^{t}\\sigma^{2}(X_{s})\\,ds\\right)\\right]$. We aim to find a PDE that $u(t,x)$ satisfies. This can be achieved by applying the Feynman-Kac principle, which we will derive by constructing a suitable martingale.\n\nLet $X_s$ be the solution to the SDE $dX_s = \\sigma(X_s)dW_s$ with $X_0 = x$. The generator of this diffusion process, denoted by $\\mathcal{L}$, acts on a sufficiently smooth function $f:\\mathbb{R} \\to \\mathbb{R}$ as follows:\n$$\n\\mathcal{L}f(x) = \\frac{1}{2}\\sigma^{2}(x)\\frac{d^{2}f}{dx^{2}}(x)\n$$\nThis arises from Itô's formula applied to $f(X_t)$, which states $df(X_t) = f'(X_t)dX_t + \\frac{1}{2}f''(X_t)d[X]_t = f'(X_t)\\sigma(X_t)dW_t + \\frac{1}{2}f''(X_t)\\sigma^2(X_t)dt$. The drift of the process $f(X_t)$ is $\\mathcal{L}f(X_t)$.\n\nConsider the process $Y_s$ for $s \\in [0,t]$ defined as:\n$$\nY_s = \\exp\\left(-\\lambda\\int_0^s \\sigma^2(X_u)du\\right) u(t-s, X_s)\n$$\nWe will apply Itô's product rule to find the differential $dY_s$. Let $A_s = \\exp\\left(-\\lambda\\int_0^s \\sigma^2(X_u)du\\right)$ and $B_s = u(t-s, X_s)$.\nThe differential of $A_s$ is:\n$$\ndA_s = -\\lambda\\sigma^2(X_s)\\exp\\left(-\\lambda\\int_0^s \\sigma^2(X_u)du\\right) ds = -\\lambda\\sigma^2(X_s)A_s ds\n$$\nTo find the differential of $B_s$, we apply Itô's formula to the function $u(t-s, x)$ which depends on time $s$ and the stochastic process $X_s$. Let partial derivatives of $u$ with respect to its first and second arguments be denoted by $\\partial_t u$ and $\\partial_x u$, respectively.\n\\begin{align*}\ndB_s &= \\frac{\\partial}{\\partial s}\\big(u(t-s, X_s)\\big)ds + \\partial_x u(t-s, X_s) dX_s + \\frac{1}{2}\\partial_{xx} u(t-s, X_s) d[X]_s \\\\\n&= -\\partial_t u(t-s, X_s)ds + \\partial_x u(t-s, X_s) \\sigma(X_s)dW_s + \\frac{1}{2}\\partial_{xx} u(t-s, X_s) \\sigma^2(X_s)ds \\\\\n&= \\left[-\\partial_t u(t-s, X_s) + \\frac{1}{2}\\sigma^2(X_s)\\partial_{xx} u(t-s, X_s)\\right]ds + \\partial_x u(t-s, X_s)\\sigma(X_s)dW_s\n\\end{align*}\nThe Itô product rule is $d(A_s B_s) = A_s dB_s + B_s dA_s + d[A,B]_s$. The quadratic covariation $d[A,B]_s$ is zero because $dA_s$ is a process of finite variation (it only has a $ds$ term).\nThus,\n\\begin{align*}\ndY_s &= A_s dB_s + B_s dA_s \\\\\n&= A_s \\left( \\left[-\\partial_t u + \\frac{1}{2}\\sigma^2\\partial_{xx} u\\right]ds + \\sigma \\partial_x u dW_s \\right) + u(t-s, X_s) \\left(-\\lambda\\sigma^2(X_s)A_s ds\\right) \\\\\n&= A_s \\left[ -\\partial_t u(t-s, X_s) + \\frac{1}{2}\\sigma^2(X_s)\\partial_{xx} u(t-s, X_s) - \\lambda\\sigma^2(X_s)u(t-s,X_s) \\right]ds \\\\\n& \\quad + A_s \\sigma(X_s) \\partial_x u(t-s, X_s) dW_s\n\\end{align*}\nBy construction, $u(t,x)$ is defined such that $Y_s$ is a martingale. For $Y_s$ to be a martingale, its drift term (the coefficient of $ds$) must be zero. This gives us the following PDE for $u(t,x)$:\n$$\n-\\partial_t u(t,x) + \\frac{1}{2}\\sigma^2(x)\\partial_{xx} u(t,x) - \\lambda\\sigma^2(x)u(t,x) = 0\n$$\nRearranging, we get the backward PDE (in the sense of the structure, often called a Feynman-Kac PDE) for $u(t,x)=L(\\lambda;t,x)$:\n$$\n\\frac{\\partial u}{\\partial t} = \\frac{1}{2}\\sigma^2(x)\\frac{\\partial^2 u}{\\partial x^2} - \\lambda\\sigma^2(x)u\n$$\nThe justification for each term is as follows:\n- The term $\\frac{1}{2}\\sigma^2(x)\\frac{\\partial^2 u}{\\partial x^2}$ is precisely $\\mathcal{L}u$, where $\\mathcal{L}$ is the generator of the diffusion $X_t$. It arises from the second-order term in the Itô expansion of $u(t-s, X_s)$ and represents the effect of the volatility of the process on the expected value.\n- The term $-\\lambda\\sigma^2(x)u$ arises from the \"running cost\" or \"potential function\" $\\lambda\\sigma^2(X_s)$ inside the exponential of the definition of $L(\\lambda;t,x)$. It represents the continuous discounting of the expected value over time.\n\nThe initial condition for this PDE is found by evaluating $u(t,x)$ at $t=0$:\n$$\nu(0,x) = L(\\lambda;0,x) = \\mathbb{E}_{x}\\left[\\exp\\left(-\\lambda\\int_{0}^{0}\\sigma^{2}(X_{s})\\,ds\\right)\\right] = \\mathbb{E}_{x}[\\exp(0)] = 1\n$$\nSo the problem is to solve the PDE with the initial condition $u(0,x)=1$.\n\n### Part 2: Solution for Constant Diffusion $\\sigma(x)=\\sigma_0$\n\nNow, we assume $\\sigma(x)=\\sigma_0$, where $\\sigma_0>0$ is a constant. The PDE becomes:\n$$\n\\frac{\\partial u}{\\partial t} = \\frac{1}{2}\\sigma_0^2\\frac{\\partial^2 u}{\\partial x^2} - \\lambda\\sigma_0^2 u\n$$\nwith the initial condition $u(0,x)=1$.\nThe coefficients of this PDE are constant with respect to $x$. Furthermore, the initial condition $u(0,x)=1$ is independent of $x$. This suggests that the solution $u(t,x)$ might also be independent of $x$. Let us test the ansatz $u(t,x) = f(t)$ for some function $f$.\nUnder this assumption, all partial derivatives with respect to $x$ are zero: $\\frac{\\partial^2 u}{\\partial x^2} = 0$.\nThe PDE simplifies to an ordinary differential equation (ODE) for $f(t)$:\n$$\n\\frac{df}{dt} = -\\lambda\\sigma_0^2 f(t)\n$$\nThe initial condition becomes $f(0) = u(0,x) = 1$.\nThis is a standard first-order linear ODE with the solution:\n$$\nf(t) = C\\exp(-\\lambda\\sigma_0^2 t)\n$$\nUsing the initial condition $f(0)=1$, we find $1 = C\\exp(0)$, so $C=1$.\nTherefore, the solution is $f(t) = \\exp(-\\lambda\\sigma_0^2 t)$.\nThus, the Laplace transform for the constant-diffusion case is:\n$$\nL(\\lambda;t,x) = u(t,x) = \\exp(-\\lambda\\sigma_0^2 t)\n$$\nThis solution is bounded for $\\lambda \\ge 0$ and $t \\ge 0$, consistent with the requirement for a unique classical solution. We could also obtain this result directly: if $\\sigma(x)=\\sigma_0$, then $[X]_t = \\int_0^t \\sigma_0^2 ds = \\sigma_0^2 t$, which is deterministic. The expectation is then $\\mathbb{E}_x[\\exp(-\\lambda\\sigma_0^2 t)] = \\exp(-\\lambda\\sigma_0^2 t)$. This confirms the PDE solution.\n\n### Part 3: Interpretation of Finiteness Conditions\n\nLet $Q_t = [X]_t = \\int_0^t \\sigma^2(X_s)ds$. Since $\\sigma^2(x) \\ge 0$, $Q_t$ is a non-negative random variable.\n\n**Finiteness of the Laplace Transform $L(\\lambda;t,x)$:**\nThe Laplace transform is $L(\\lambda;t,x) = \\mathbb{E}[\\exp(-\\lambda Q_t)]$ and is defined for $\\lambda \\ge 0$.\nSince $Q_t \\ge 0$ and $\\lambda \\ge 0$, the exponent $-\\lambda Q_t$ is non-positive. This implies:\n$$\n0 < \\exp(-\\lambda Q_t) \\le \\exp(0) = 1\n$$\nThe random variable $\\exp(-\\lambda Q_t)$ is bounded between $0$ and $1$. The expectation of any bounded random variable is finite. Specifically, $0 < L(\\lambda;t,x) \\le 1$.\nTherefore, the Laplace transform $L(\\lambda;t,x)$ is finite for all $\\lambda \\ge 0$, regardless of the properties of $\\sigma(x)$ (as long as the SDE is well-defined).\n\n**Finiteness of the Moment-Generating Function $M(\\theta;t,x)$:**\nThe MGF is $M(\\theta;t,x) = \\mathbb{E}[\\exp(\\theta Q_t)]$ for $\\theta \\in \\mathbb{R}$. Note that $M(\\theta;t,x) = L(-\\theta;t,x)$.\n- For $\\theta \\le 0$: We can write $\\theta = -\\lambda$ for some $\\lambda \\ge 0$. Then $M(\\theta;t,x) = L(\\lambda;t,x)$, which we have just shown is always finite.\n- For $\\theta > 0$: The finiteness of the expectation depends on the right-tail behavior of the distribution of $Q_t$. This behavior is directly influenced by the growth of $\\sigma(x)$.\n  - **Case 1: $\\sigma(x)$ is bounded.** If there exists a constant $C$ such that $|\\sigma(x)| \\le C$ for all $x \\in \\mathbb{R}$, then $\\sigma^2(x) \\le C^2$. The quadratic variation is then bounded:\n    $$\n    Q_t = \\int_0^t \\sigma^2(X_s)ds \\le \\int_0^t C^2 ds = C^2 t\n    $$\n    Since $Q_t$ is a bounded random variable, for any $\\theta > 0$, the variable $\\exp(\\theta Q_t)$ is also bounded: $\\exp(\\theta Q_t) \\le \\exp(\\theta C^2 t)$. The expectation is therefore finite: $M(\\theta;t,x) \\le \\exp(\\theta C^2 t) < \\infty$. Thus, if $\\sigma$ is bounded, the MGF is finite for all $\\theta \\in \\mathbb{R}$. The constant case $\\sigma(x)=\\sigma_0$ falls into this category.\n  - **Case 2: $\\sigma(x)$ is unbounded.** If $\\sigma(x)$ can grow without bound (e.g., $\\sigma(x)=kx$, respecting the Lipschitz condition), then $Q_t$ is an unbounded random variable. The process $X_s$ can explore regions where $\\sigma^2(X_s)$ is very large, leading to a heavy right tail for the distribution of $Q_t$. The MGF $\\mathbb{E}[\\exp(\\theta Q_t)]$ for $\\theta > 0$ sums over this tail with exponentially growing weights. This integral or sum might diverge. Typically, there exists a critical value $\\theta_0 > 0$ (which may depend on $t$) such that $M(\\theta;t,x)$ is finite for $0 < \\theta < \\theta_0$ and infinite for $\\theta \\ge \\theta_0$. The faster $\\sigma(x)$ grows, the larger the potential values of $Q_t$, and the smaller the corresponding interval of finiteness for $\\theta$.\n\nIn summary, the finiteness depends critically on whether the \"cost\" $\\sigma^2(x)$ is bounded or not. Boundedness of $\\sigma$ guarantees a finite MGF for all real $\\theta$. Unboundedness of $\\sigma$ restricts the finiteness for $\\theta>0$ to a limited range.", "answer": "$$\\boxed{\\exp(-\\lambda \\sigma_{0}^{2} t)}$$", "id": "2970786"}, {"introduction": "Characteristic functions are not just static descriptors; they transform in predictable ways under fundamental operations like a change of probability measure. This exercise guides you through the Esscher transform, a crucial tool for risk-neutral pricing, to see how it systematically alters the characteristic exponent of a Lévy process. By deriving the new Lévy triplet, you will gain a deeper understanding of how this \"tilting\" of the measure modifies the process's drift, volatility, and jump characteristics. [@problem_id:2970754]", "problem": "Consider a real-valued Lévy process $X = (X_{t})_{t \\ge 0}$ on a filtered probability space, with characteristic function $\\mathbb{E}[\\exp(i u X_{t})] = \\exp(t \\psi(u))$ and characteristic exponent $\\psi(u)$ given by the Lévy–Khintchine formula\n$$\n\\psi(u) = i b u - \\frac{1}{2} \\sigma^{2} u^{2} + \\int_{\\mathbb{R}} \\left( \\exp(i u x) - 1 - i u x \\mathbf{1}_{\\{|x| \\le 1\\}} \\right) \\nu(dx),\n$$\nwhere $(b, \\sigma^{2}, \\nu)$ is the Lévy triplet with truncation function $x \\mathbf{1}_{\\{|x| \\le 1\\}}$ held fixed. Assume there exists a nonempty interval of real parameters around $0$ such that the moment-generating function is finite, i.e., for each $\\theta$ in this interval,\n$$\n\\mathbb{E}[\\exp(\\theta X_{t})] < \\infty \\quad \\text{for all } t \\ge 0,\n$$\nequivalently $\\int_{|x|>1} \\exp(\\theta x) \\nu(dx) < \\infty$. For a fixed $\\theta$ in this effective domain, define the Esscher transform (also known as the exponential tilting) of $X$ via the Radon–Nikodym (RN) derivative\n$$\n\\left. \\frac{d \\mathbb{Q}^{(\\theta)}}{d \\mathbb{P}} \\right|_{\\mathcal{F}_{t}} \\;=\\; \\exp\\!\\big( \\theta X_{t} - t \\kappa(\\theta) \\big),\n$$\nwhere $\\kappa(\\theta)$ is the cumulant generating function satisfying $\\mathbb{E}[\\exp(\\theta X_{t})] = \\exp(t \\kappa(\\theta))$.\n\nStarting from the Lévy–Khintchine representation and the Esscher transform definition above, derive the characteristic exponent $\\psi_{\\theta}(u)$ of $X$ under the tilted measure $\\mathbb{Q}^{(\\theta)}$, writing it explicitly in Lévy–Khintchine form with the same truncation function, and explain how the Lévy triplet changes under the tilt. Your derivation should rely only on these fundamental definitions and standard measure-change calculus, and must be valid for all real $u$ for which the characteristic function is defined. Provide your final answer as a single closed-form analytic expression for $\\psi_{\\theta}(u)$ in terms of $(b, \\sigma^{2}, \\nu)$ and $\\theta$.", "solution": "The user wants to find the characteristic exponent $\\psi_{\\theta}(u)$ of a Lévy process $X_t$ under an Esscher-transformed measure $\\mathbb{Q}^{(\\theta)}$, and to describe the transformation of the Lévy triplet $(b, \\sigma^2, \\nu)$.\n\n### Step 1: Extract Givens\n- A real-valued Lévy process $X = \\{X_t\\}_{t \\ge 0}$.\n- Characteristic function under the original measure $\\mathbb{P}$: $\\mathbb{E}[\\exp(i u X_{t})] = \\exp(t \\psi(u))$.\n- Characteristic exponent $\\psi(u)$ given by the Lévy–Khintchine formula:\n$$ \\psi(u) = i b u - \\frac{1}{2} \\sigma^{2} u^{2} + \\int_{\\mathbb{R}} \\left( \\exp(i u x) - 1 - i u x \\mathbf{1}_{\\{|x| \\le 1\\}} \\right) \\nu(dx) $$\n- The Lévy triplet is $(b, \\sigma^{2}, \\nu)$ with a fixed truncation function $h(x) = x \\mathbf{1}_{\\{|x| \\le 1\\}}$.\n- The moment-generating function (MGF) is finite for $\\theta$ in a nonempty interval around $0$: $\\mathbb{E}[\\exp(\\theta X_{t})] < \\infty$ for all $t \\ge 0$.\n- The MGF is given by $\\mathbb{E}[\\exp(\\theta X_{t})] = \\exp(t \\kappa(\\theta))$, where $\\kappa(\\theta)$ is the cumulant generating function.\n- The Esscher transform defines a new measure $\\mathbb{Q}^{(\\theta)}$ via the Radon–Nikodym derivative:\n$$ \\left. \\frac{d \\mathbb{Q}^{(\\theta)}}{d \\mathbb{P}} \\right|_{\\mathcal{F}_{t}} = \\exp(\\theta X_{t} - t \\kappa(\\theta)) $$\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded:** The problem is set within the established mathematical framework of Lévy processes and measure theory. The Lévy-Khintchine formula, cumulant generating function, and Esscher transform are all standard, well-defined concepts in this field.\n- **Well-Posed:** The problem provides all necessary definitions and assumptions (e.g., the finiteness of the MGF) to proceed with a rigorous derivation. It asks for a specific quantity, $\\psi_{\\theta}(u)$, and an explanation of the transformed Lévy triplet, implying a unique solution exists.\n- **Objective:** The problem is stated using precise mathematical language, free from any subjective or ambiguous terms.\n- **Conclusion:** The problem is scientifically sound, well-posed, objective, and complete. It is a valid and standard exercise in the theory of stochastic processes.\n\n### Step 3: Verdict and Action\nThe problem is valid. I will proceed with the derivation.\n\n### Derivation\nOur goal is to find the characteristic exponent $\\psi_{\\theta}(u)$ of the process $X_t$ under the tilted measure $\\mathbb{Q}^{(\\theta)}$. This exponent is defined by the relation $\\mathbb{E}_{\\mathbb{Q}^{(\\theta)}}[\\exp(i u X_t)] = \\exp(t \\psi_{\\theta}(u))$.\n\nWe begin by using the definition of expectation under a changed measure. The Radon-Nikodym derivative is $L_t = \\frac{d \\mathbb{Q}^{(\\theta)}}{d \\mathbb{P}} |_{\\mathcal{F}_t} = \\exp(\\theta X_t - t \\kappa(\\theta))$.\nFor any $\\mathcal{F}_t$-measurable random variable $Y$, the change of measure formula states $\\mathbb{E}_{\\mathbb{Q}^{(\\theta)}}[Y] = \\mathbb{E}_{\\mathbb{P}}[Y L_t]$. Let $Y = \\exp(i u X_t)$.\n$$ \\mathbb{E}_{\\mathbb{Q}^{(\\theta)}}[\\exp(i u X_t)] = \\mathbb{E}_{\\mathbb{P}}[\\exp(i u X_t) \\cdot \\exp(\\theta X_t - t \\kappa(\\theta))] $$\n$$ = \\mathbb{E}_{\\mathbb{P}}[\\exp((\\theta + i u) X_t - t \\kappa(\\theta))] $$\nUsing the linearity of expectation, we can factor out the non-random term:\n$$ = \\exp(-t \\kappa(\\theta)) \\mathbb{E}_{\\mathbb{P}}[\\exp((\\theta + i u) X_t)] $$\nThe expectation term $\\mathbb{E}_{\\mathbb{P}}[\\exp((\\theta + i u) X_t)]$ is the MGF of $X_t$ under $\\mathbb{P}$, evaluated at the complex argument $z = \\theta + i u$. The problem states that $\\mathbb{E}_{\\mathbb{P}}[\\exp(z X_t)] = \\exp(t \\kappa(z))$ for valid $z$. The domain of $\\kappa$ can be extended from the real interval to a strip in the complex plane that includes this real interval. Thus, we have:\n$$ \\mathbb{E}_{\\mathbb{P}}[\\exp((\\theta + i u) X_t)] = \\exp(t \\kappa(\\theta + i u)) $$\nSubstituting this back, we get:\n$$ \\mathbb{E}_{\\mathbb{Q}^{(\\theta)}}[\\exp(i u X_t)] = \\exp(-t \\kappa(\\theta)) \\exp(t \\kappa(\\theta + i u)) = \\exp(t[\\kappa(\\theta + i u) - \\kappa(\\theta)]) $$\nBy comparing this with the definition $\\mathbb{E}_{\\mathbb{Q}^{(\\theta)}}[\\exp(i u X_t)] = \\exp(t \\psi_{\\theta}(u))$, we identify the new characteristic exponent as:\n$$ \\psi_{\\theta}(u) = \\kappa(\\theta + i u) - \\kappa(\\theta) $$\nTo proceed, we need the explicit form of $\\kappa(z)$. It can be found by relating it to the characteristic exponent $\\psi(u)$ via analytic continuation, noting that $\\kappa(iu) = \\psi(u)$. Replacing $iu$ with a general complex argument $z$ in the Lévy-Khintchine formula gives:\n$$ \\kappa(z) = b z + \\frac{1}{2} \\sigma^{2} z^{2} + \\int_{\\mathbb{R}} \\left( \\exp(z x) - 1 - z x \\mathbf{1}_{\\{|x| \\le 1\\}} \\right) \\nu(dx) $$\nThis expression is valid for $z$ in the strip of the complex plane where the integral converges, which includes the real interval around $0$ by assumption.\n\nNow we compute the difference $\\kappa(\\theta + i u) - \\kappa(\\theta)$ term by term.\n1.  **Drift and Diffusion part**:\n    $$ \\left( b(\\theta+iu) + \\frac{1}{2}\\sigma^2(\\theta+iu)^2 \\right) - \\left( b\\theta + \\frac{1}{2}\\sigma^2\\theta^2 \\right) $$\n    $$ = b\\theta + ibu + \\frac{1}{2}\\sigma^2(\\theta^2 + 2i\\theta u - u^2) - b\\theta - \\frac{1}{2}\\sigma^2\\theta^2 $$\n    $$ = i b u + i \\sigma^2 \\theta u - \\frac{1}{2}\\sigma^2 u^2 $$\n2.  **Jump part (Integral)**:\n    $$ \\int_{\\mathbb{R}} \\left[ \\left( \\exp((\\theta+iu)x) - 1 - (\\theta+iu)x\\mathbf{1}_{\\{|x|\\le 1\\}} \\right) - \\left( \\exp(\\theta x) - 1 - \\theta x\\mathbf{1}_{\\{|x|\\le 1\\}} \\right) \\right] \\nu(dx) $$\n    $$ = \\int_{\\mathbb{R}} \\left( \\exp(\\theta x)\\exp(i u x) - \\exp(\\theta x) - i u x \\mathbf{1}_{\\{|x| \\le 1\\}} \\right) \\nu(dx) $$\n\nCombining these parts, we get an expression for $\\psi_{\\theta}(u)$:\n$$ \\psi_{\\theta}(u) = i(b + \\sigma^2 \\theta) u - \\frac{1}{2}\\sigma^2 u^2 + \\int_{\\mathbb{R}} \\left( \\exp(\\theta x)(\\exp(i u x) - 1) - i u x \\mathbf{1}_{\\{|x| \\le 1\\}} \\right) \\nu(dx) $$\nTo write this in the canonical Lévy-Khintchine form, we must rearrange the integrand to match the structure $(\\exp(i u x) - 1 - i u x \\mathbf{1}_{\\{|x| \\le 1\\}})$. We add and subtract the term $i u x \\mathbf{1}_{\\{|x| \\le 1\\}} \\exp(\\theta x)$ inside the integral:\n$$ \\int_{\\mathbb{R}} \\left( \\exp(\\theta x)(\\exp(i u x) - 1) - i u x \\mathbf{1}_{\\{|x| \\le 1\\}} \\exp(\\theta x) + i u x \\mathbf{1}_{\\{|x| \\le 1\\}} \\exp(\\theta x) - i u x \\mathbf{1}_{\\{|x| \\le 1\\}} \\right) \\nu(dx) $$\n$$ = \\int_{\\mathbb{R}} \\exp(\\theta x) \\left( \\exp(i u x) - 1 - i u x \\mathbf{1}_{\\{|x| \\le 1\\}} \\right) \\nu(dx) + \\int_{\\mathbb{R}} \\left( i u x \\mathbf{1}_{\\{|x| \\le 1\\}} (\\exp(\\theta x) - 1) \\right) \\nu(dx) $$\nThe second integral is a linear term in $iu$ and contributes to the drift:\n$$ iu \\int_{|x| \\le 1} x(\\exp(\\theta x) - 1) \\nu(dx) $$\nNow, we can assemble the full expression for $\\psi_{\\theta}(u)$ and group terms:\n$$ \\psi_{\\theta}(u) = i\\left(b + \\sigma^2 \\theta + \\int_{|x| \\le 1} x(\\exp(\\theta x) - 1)\\nu(dx)\\right)u - \\frac{1}{2}\\sigma^2 u^2 + \\int_{\\mathbb{R}} \\left( \\exp(i u x) - 1 - i u x \\mathbf{1}_{\\{|x| \\le 1\\}} \\right) \\exp(\\theta x) \\nu(dx) $$\nThis expression is in the canonical Lévy-Khintchine form:\n$$ \\psi_{\\theta}(u) = i b_{\\theta} u - \\frac{1}{2} \\sigma_{\\theta}^{2} u^{2} + \\int_{\\mathbb{R}} \\left( \\exp(i u x) - 1 - i u x \\mathbf{1}_{\\{|x| \\le 1\\}} \\right) \\nu_{\\theta}(dx) $$\nFrom this, we can identify how the Lévy triplet transforms under the Esscher transform.\n- **Transformed Brownian Volatility, $\\sigma_{\\theta}^2$**: The coefficient of $-\\frac{1}{2}u^2$ is unchanged.\n$$ \\sigma_{\\theta}^2 = \\sigma^2 $$\n- **Transformed Lévy Measure, $\\nu_{\\theta}$**: The measure in the jump integral is modified by an exponential factor.\n$$ \\nu_{\\theta}(dx) = \\exp(\\theta x) \\nu(dx) $$\nThe initial assumption that $\\int_{|x|>1} \\exp(\\theta x) \\nu(dx) < \\infty$ along with the fact that $\\nu$ is a Lévy measure ensures that $\\nu_{\\theta}$ is also a valid Lévy measure satisfying $\\int_{\\mathbb{R}} \\min(1, x^2) \\nu_{\\theta}(dx) < \\infty$.\n- **Transformed Drift, $b_{\\theta}$**: The coefficient of $iu$ contains the original drift plus corrections from the diffusion and jump parts.\n$$ b_{\\theta} = b + \\sigma^2 \\theta + \\int_{|x| \\le 1} x(\\exp(\\theta x) - 1) \\nu(dx) $$\nThe process $X_t$ under the measure $\\mathbb{Q}^{(\\theta)}$ is therefore a new Lévy process with the triplet $(b_{\\theta}, \\sigma_{\\theta}^2, \\nu_{\\theta})$. The final expression for its characteristic exponent $\\psi_{\\theta}(u)$ is the one derived above.", "answer": "$$\n\\boxed{i \\left( b + \\sigma^2 \\theta + \\int_{|x| \\le 1} x(\\exp(\\theta x) - 1) \\nu(dx) \\right) u - \\frac{1}{2} \\sigma^2 u^2 + \\int_{\\mathbb{R}} \\left( \\exp(i u x) - 1 - i u x \\mathbf{1}_{\\{|x| \\le 1\\}} \\right) \\exp(\\theta x) \\nu(dx)}\n$$", "id": "2970754"}, {"introduction": "The ultimate utility of a characteristic function often lies in its invertibility, allowing us to recover the underlying probability density function when it is not known in simple terms. This hands-on practice challenges you to implement a numerical Fourier inversion scheme to compute the density of an Ornstein–Uhlenbeck process, where $\\kappa \\gt 0$, from its characteristic function. By comparing your numerical result to the known analytical density, you will directly confront and analyze the primary sources of error in such computations: integral truncation and discretization. [@problem_id:2970777]", "problem": "You are given the task of recovering the probability density function of a one-dimensional Itô process at a fixed time from its characteristic function and analyzing the numerical errors inherent in the inversion. Consider the Ornstein–Uhlenbeck stochastic differential equation (SDE)\n$$\ndX_t = \\kappa \\left(\\theta - X_t\\right)\\,dt + \\sigma\\, dW_t,\\quad X_0 = x_0,\n$$\nwhere $W_t$ is a standard Brownian motion, and $\\kappa$, $\\theta$, $\\sigma$, $x_0$, and $t$ are real parameters with $\\kappa \\gt 0$, $\\sigma \\gt 0$, and $t \\gt 0$.\n\nThe characteristic function of $X_t$ is defined by\n$$\n\\phi_{X_t}(u) = \\mathbb{E}\\left[e^{i u X_t}\\right],\n$$\nand the density $p_{X_t}(x)$, when it exists, can be recovered from $\\phi_{X_t}(u)$ via the Fourier inversion formula\n$$\np_{X_t}(x) = \\frac{1}{2\\pi}\\int_{-\\infty}^{\\infty} e^{-i u x}\\,\\phi_{X_t}(u)\\,du.\n$$\n\nStarting from fundamental properties of linear SDEs, Itô calculus, and the definition of the characteristic function, you must:\n1. Derive the distribution of $X_t$ and $\\phi_{X_t}(u)$ for the Ornstein–Uhlenbeck process defined above, strictly from first principles (no shortcut formulas).\n2. Design a numerical Fourier inversion scheme to approximate $p_{X_t}(x)$ using a symmetric truncation of the integral at $\\pm U$ and a composite trapezoidal rule with uniform step size $\\Delta u$:\n$$\n\\widehat{p}_{X_t}(x;U,\\Delta u) = \\frac{1}{2\\pi}\\sum_{k=-K}^{K} w_k\\, \\phi_{X_t}(u_k)\\, e^{-i u_k x}\\,\\Delta u,\n$$\nwhere $u_k = k\\,\\Delta u$, $K = \\lfloor U/\\Delta u\\rfloor$, and $w_k$ are trapezoidal weights with endpoints weighted by $1/2$ and interior points by $1$.\n3. Analyze and quantify the approximation error $\\left|p_{X_t}(x) - \\widehat{p}_{X_t}(x;U,\\Delta u)\\right|$ by separating the sources of error:\n   - Truncation error due to finite $U$, using a bound derived from the decay of $\\left|\\phi_{X_t}(u)\\right|$.\n   - Discretization error due to finite $\\Delta u$, using a bound from the composite trapezoidal rule in terms of the supremum of the second derivative of the integrand on $\\left[-U,U\\right]$.\n\nYour algorithm must:\n- Implement the numerical inversion scheme described above.\n- Use the analytically derived $p_{X_t}(x)$ for the Ornstein–Uhlenbeck process (obtained from part 1) as the reference to compute the pointwise absolute error.\n- For each test case below, compute the maximum absolute error over a grid of $x$ values that spans the main mass of the distribution. Use a grid\n$$\nx_j = m - 4\\sqrt{v} + j\\cdot \\delta_x,\\quad j=0,1,\\ldots,J,\n$$\nwhere $m$ and $v$ are the mean and variance of $X_t$ determined from part 1, $J=200$, and $\\delta_x = \\frac{8\\sqrt{v}}{J}$.\n\nTest Suite:\n- Case 1 (general happy path): $\\kappa=1.0$, $\\theta=0.0$, $x_0=1.0$, $\\sigma=1.0$, $t=1.0$, $U=30.0$, $\\Delta u=0.01$.\n- Case 2 (truncation-limited): $\\kappa=1.0$, $\\theta=0.0$, $x_0=1.0$, $\\sigma=1.0$, $t=1.0$, $U=5.0$, $\\Delta u=0.01$.\n- Case 3 (discretization-limited): $\\kappa=1.0$, $\\theta=0.0$, $x_0=1.0$, $\\sigma=1.0$, $t=1.0$, $U=30.0$, $\\Delta u=0.5$.\n- Case 4 (slow decay edge case): $\\kappa=1.5$, $\\theta=-0.5$, $x_0=2.0$, $\\sigma=0.5$, $t=0.1$, $U=30.0$, $\\Delta u=0.05$.\n\nOutput Specification:\n- For each case, compute the maximum absolute error over the specified $x$ grid.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[result1,result2,result3,result4]\").\n- Each result must be a floating-point number representing the maximum absolute error for the corresponding test case.\n\nNo physical units apply to this problem. Angles, where they appear implicitly through complex exponentials, are handled in radians by the standard mathematical definitions used.", "solution": "The user-provided problem is assessed to be valid. It is scientifically grounded in the theory of stochastic differential equations and numerical analysis, well-posed with all necessary parameters and definitions provided, and objective in its formulation. The problem asks for the derivation of properties of the Ornstein-Uhlenbeck process and the implementation of a numerical Fourier inversion method, which is a standard and meaningful task in quantitative finance and physics. We will proceed with a full solution.\n\nThe problem is addressed in two parts: first, a theoretical derivation of the required analytical expressions, and second, a description of the numerical algorithm based on these expressions.\n\n### Part 1: Analytical Solution of the Ornstein-Uhlenbeck Process\n\nThe Ornstein-Uhlenbeck process is described by the stochastic differential equation (SDE):\n$$\ndX_t = \\kappa \\left(\\theta - X_t\\right)\\,dt + \\sigma\\, dW_t, \\quad X_0 = x_0\n$$\nwhere $\\kappa > 0$, $\\sigma > 0$, and $t > 0$ are real parameters, and $W_t$ is a standard Wiener process. This is a linear SDE. We can solve it by defining an auxiliary process $Y_t = e^{\\kappa t} X_t$ and applying Itô's lemma. The differential of $Y_t$ is:\n$$\ndY_t = d(e^{\\kappa t} X_t) = (\\partial_t(e^{\\kappa t} X_t))\\,dt + (\\partial_x(e^{\\kappa t} X_t))\\,dX_t + \\frac{1}{2}(\\partial_{xx}(e^{\\kappa t} X_t))\\,(dX_t)^2\n$$\nThe partial derivatives are $\\partial_t(e^{\\kappa t} X_t) = \\kappa e^{\\kappa t} X_t$, $\\partial_x(e^{\\kappa t} X_t) = e^{\\kappa t}$, and $\\partial_{xx}(e^{\\kappa t} X_t) = 0$. The quadratic variation of $dX_t$ is $(dX_t)^2 = \\sigma^2\\,dt$. Substituting these and the expression for $dX_t$ into the formula for $dY_t$:\n$$\ndY_t = \\kappa e^{\\kappa t} X_t\\,dt + e^{\\kappa t} \\left( \\kappa(\\theta - X_t)\\,dt + \\sigma\\,dW_t \\right)\n$$\n$$\ndY_t = \\kappa e^{\\kappa t} X_t\\,dt + \\kappa \\theta e^{\\kappa t}\\,dt - \\kappa e^{\\kappa t} X_t\\,dt + \\sigma e^{\\kappa t}\\,dW_t\n$$\nThe terms involving $X_t$ cancel, simplifying the expression to:\n$$\ndY_t = \\kappa \\theta e^{\\kappa t}\\,dt + \\sigma e^{\\kappa t}\\,dW_t\n$$\nIntegrating from $s=0$ to $s=t$:\n$$\nY_t - Y_0 = \\int_0^t \\kappa \\theta e^{\\kappa s}\\,ds + \\int_0^t \\sigma e^{\\kappa s}\\,dW_s\n$$\nSubstituting back $Y_t = e^{\\kappa t} X_t$ and $Y_0 = e^0 X_0 = x_0$:\n$$\ne^{\\kappa t} X_t - x_0 = \\kappa \\theta \\left[ \\frac{1}{\\kappa} e^{\\kappa s} \\right]_0^t + \\sigma \\int_0^t e^{\\kappa s}\\,dW_s\n$$\n$$\ne^{\\kappa t} X_t = x_0 + \\theta (e^{\\kappa t} - 1) + \\sigma \\int_0^t e^{\\kappa s}\\,dW_s\n$$\nSolving for $X_t$ by multiplying by $e^{-\\kappa t}$:\n$$\nX_t = x_0 e^{-\\kappa t} + \\theta (1 - e^{-\\kappa t}) + \\sigma e^{-\\kappa t} \\int_0^t e^{\\kappa s}\\,dW_s\n$$\nThe integral term can be rewritten as $\\sigma \\int_0^t e^{-\\kappa(t-s)}\\,dW_s$. The solution for $X_t$ is a sum of a deterministic part and an Itô integral with a deterministic integrand. An Itô integral of a deterministic function is a Gaussian random variable with zero mean. Thus, $X_t$ itself is a normally distributed random variable.\n\nThe mean of $X_t$, denoted $m_t$, is found by taking the expectation. The expectation of the Itô integral is zero:\n$$\nm_t = \\mathbb{E}[X_t] = x_0 e^{-\\kappa t} + \\theta (1 - e^{-\\kappa t})\n$$\nThe variance of $X_t$, denoted $v_t$, is determined by the Itô integral term, using the property of Itô isometry, $\\mathbb{E}\\left[\\left(\\int_0^t f(s)\\,dW_s\\right)^2\\right] = \\int_0^t f(s)^2\\,ds$:\n$$\nv_t = \\text{Var}[X_t] = \\text{Var}\\left[\\sigma \\int_0^t e^{-\\kappa(t-s)}\\,dW_s\\right] = \\sigma^2 \\int_0^t \\left(e^{-\\kappa(t-s)}\\right)^2\\,ds\n$$\n$$\nv_t = \\sigma^2 \\int_0^t e^{-2\\kappa(t-s)}\\,ds = \\sigma^2 e^{-2\\kappa t} \\int_0^t e^{2\\kappa s}\\,ds = \\sigma^2 e^{-2\\kappa t} \\left[\\frac{e^{2\\kappa s}}{2\\kappa}\\right]_0^t\n$$\n$$\nv_t = \\sigma^2 e^{-2\\kappa t} \\left(\\frac{e^{2\\kappa t} - 1}{2\\kappa}\\right) = \\frac{\\sigma^2}{2\\kappa} (1 - e^{-2\\kappa t})\n$$\nTherefore, the distribution of $X_t$ is Gaussian: $X_t \\sim \\mathcal{N}(m_t, v_t)$. The probability density function (PDF) $p_{X_t}(x)$ is:\n$$\np_{X_t}(x) = \\frac{1}{\\sqrt{2\\pi v_t}} \\exp\\left(-\\frac{(x - m_t)^2}{2v_t}\\right)\n$$\nThe characteristic function of a normal random variable $Y \\sim \\mathcal{N}(\\mu, \\sigma^2)$ is $\\phi_Y(u) = \\exp(i u \\mu - \\frac{1}{2} u^2 \\sigma^2)$. Applying this to $X_t$, its characteristic function $\\phi_{X_t}(u) = \\mathbb{E}[e^{i u X_t}]$ is:\n$$\n\\phi_{X_t}(u) = \\exp\\left(i u m_t - \\frac{1}{2} u^2 v_t\\right) = \\exp\\left(i u \\left[x_0 e^{-\\kappa t} + \\theta(1 - e^{-\\kappa t})\\right] - \\frac{u^2 \\sigma^2}{4\\kappa} (1 - e^{-2\\kappa t})\\right)\n$$\nThese analytical forms for $p_{X_t}(x)$ and $\\phi_{X_t}(u)$ are the foundation for the numerical task.\n\n### Part 2: Numerical Inversion Scheme and Algorithm Design\n\nThe PDF can be recovered from the characteristic function via the inverse Fourier transform:\n$$\np_{X_t}(x) = \\frac{1}{2\\pi}\\int_{-\\infty}^{\\infty} e^{-i u x}\\,\\phi_{X_t}(u)\\,du\n$$\nTo approximate this integral numerically, we perform two steps:\n1.  **Truncation**: The infinite integral is truncated to a finite symmetric interval $[-U, U]$. The error from this step is small if $|\\phi_{X_t}(u)|$ decays sufficiently fast as $|u| \\to \\infty$. For the OU process, $|\\phi_{X_t}(u)| = e^{-\\frac{1}{2}u^2 v_t}$, which has a Gaussian decay, making truncation very effective.\n2.  **Discretization**: The truncated integral $\\int_{-U}^{U}$ is approximated using a numerical quadrature rule. The problem specifies the composite trapezoidal rule with a uniform step size $\\Delta u$.\n\nThe resulting numerical approximation, $\\widehat{p}_{X_t}(x; U, \\Delta u)$, is given by:\n$$\n\\widehat{p}_{X_t}(x;U,\\Delta u) = \\frac{1}{2\\pi}\\sum_{k=-K}^{K} w_k\\, \\phi_{X_t}(u_k)\\, e^{-i u_k x}\\,\\Delta u\n$$\nwhere $u_k = k\\,\\Delta u$, $K = \\lfloor U/\\Delta u\\rfloor$, and the weights $w_k$ are $w_k = 1/2$ for $k = \\pm K$ and $w_k = 1$ for $|k| < K$.\n\nThe algorithm to be implemented for each test case is as follows:\n1.  Given the parameters $(\\kappa, \\theta, x_0, \\sigma, t)$, calculate the analytical mean $m_t$ and variance $v_t$.\n2.  Define the spatial grid $\\{x_j\\}_{j=0}^J$ with $J=200$ over the domain $[m_t - 4\\sqrt{v_t}, m_t + 4\\sqrt{v_t}]$, which covers approximately $8$ standard deviations of the distribution.\n3.  Given the numerical parameters $(U, \\Delta u)$, define the frequency grid $\\{u_k\\}_{k=-K}^K$ where $K = \\lfloor U/\\Delta u\\rfloor$.\n4.  For each point $x_j$ on the spatial grid, compute the approximate PDF $\\widehat{p}_{X_t}(x_j)$ using the trapezoidal sum.\n    - This involves evaluating $\\phi_{X_t}(u_k)$ on the frequency grid.\n    - The Fourier kernel $e^{-i u_k x_j}$ is computed.\n    - The terms are multiplied, weighted, summed, and scaled by $\\Delta u / (2\\pi)$. The real part of the result is taken, as the true PDF is real-valued.\n5.  For each point $x_j$, compute the exact PDF $p_{X_t}(x_j)$ using the analytical formula for a Gaussian distribution with mean $m_t$ and variance $v_t$.\n6.  Calculate the absolute error $|p_{X_t}(x_j) - \\widehat{p}_{X_t}(x_j)|$ at each grid point $x_j$.\n7.  The final result for the test case is the maximum of these absolute errors over the entire grid $\\{x_j\\}$.\n\nThis procedure is implemented in a vectorized manner for efficiency. The calculation for all $x_j$ values is performed simultaneously using array operations, which avoids explicit loops in the Python code and leverages the performance of the underlying `numpy` library.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem of recovering the PDF of an Ornstein-Uhlenbeck process\n    from its characteristic function using numerical Fourier inversion and\n    calculating the maximum absolute error against the true PDF.\n    \"\"\"\n    # Define the test cases as per the problem statement.\n    # Each tuple is (kappa, theta, x0, sigma, t, U, delta_u)\n    test_cases = [\n        (1.0, 0.0, 1.0, 1.0, 1.0, 30.0, 0.01),\n        (1.0, 0.0, 1.0, 1.0, 1.0, 5.0, 0.01),\n        (1.0, 0.0, 1.0, 1.0, 1.0, 30.0, 0.5),\n        (1.5, -0.5, 2.0, 0.5, 0.1, 30.0, 0.05),\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        kappa, theta, x0, sigma, t, U, delta_u = case\n        \n        # 1. Calculate analytical mean (m_t) and variance (v_t) of X_t\n        e_neg_kt = np.exp(-kappa * t)\n        m_t = x0 * e_neg_kt + theta * (1 - e_neg_kt)\n        v_t = (sigma**2 / (2 * kappa)) * (1 - np.exp(-2 * kappa * t))\n        \n        # 2. Setup the grid for x where the PDF is evaluated.\n        # This grid spans 8 standard deviations around the mean.\n        J = 200\n        v_t_sqrt = np.sqrt(v_t)\n        delta_x = 8 * v_t_sqrt / J\n        x_grid = m_t - 4 * v_t_sqrt + np.arange(J + 1) * delta_x\n        \n        # 3. Setup the grid for the frequency domain (u) for numerical integration.\n        K = int(np.floor(U / delta_u))\n        u_grid = np.arange(-K, K + 1) * delta_u\n        \n        # 4. Perform the numerical Fourier inversion.\n        # This is done in a vectorized way for efficiency.\n        \n        # Calculate the characteristic function values on the u_grid.\n        # phi(u) = exp(i*u*m_t - 0.5*u^2*v_t)\n        phi_u_vals = np.exp(1j * u_grid * m_t - 0.5 * u_grid**2 * v_t)\n        \n        # Define the trapezoidal rule weights.\n        weights = np.ones_like(u_grid)\n        weights[0] = 0.5\n        weights[-1] = 0.5\n        \n        # Prepare for vectorized computation over x_grid.\n        # x_grid -> column vector, u_grid -> row vector\n        x_col = x_grid[:, np.newaxis]\n        u_row = u_grid[np.newaxis, :]\n        \n        # Compute the Fourier kernel exp(-i*u*x) for all combinations of u and x.\n        exp_inv_kernel = np.exp(-1j * x_col * u_row)\n        \n        # The integrand for each x is phi(u) * exp(-i*u*x).\n        # We broadcast phi_u_vals to match the shape of the kernel.\n        integrand = phi_u_vals[np.newaxis, :] * exp_inv_kernel\n        \n        # Apply trapezoidal weights to the integrand.\n        integrand_weighted = integrand * weights[np.newaxis, :]\n        \n        # Sum over the u-dimension (axis=1) and scale to get the integral approximation.\n        integral_sum = np.sum(integrand_weighted, axis=1)\n        p_hat_vals = np.real(integral_sum * delta_u / (2 * np.pi))\n        \n        # 5. Calculate the true PDF values on the x_grid.\n        # p(x) = (1/sqrt(2*pi*v_t)) * exp(-(x-m_t)^2 / (2*v_t))\n        p_true_vals = (1 / np.sqrt(2 * np.pi * v_t)) * np.exp(-(x_grid - m_t)**2 / (2 * v_t))\n        \n        # 6. Compute the maximum absolute error.\n        abs_errors = np.abs(p_true_vals - p_hat_vals)\n        max_error = np.max(abs_errors)\n        \n        results.append(max_error)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2970777"}]}