{"hands_on_practices": [{"introduction": "A cornerstone of stochastic calculus and filtering theory is the behavior of jointly Gaussian random variables. This exercise grounds the abstract Hilbert space definition of conditional expectation as an orthogonal projection into a concrete, powerful result. By deriving the famous formula for the conditional expectation of jointly Gaussian vectors, you will solidify your understanding of this geometric interpretation and see its direct application in linear systems [@problem_id:2971565].", "problem": "Let $(\\Omega,\\mathcal{F},(\\mathcal{F}_{t})_{t\\in[0,T]},\\mathbb{P})$ be a filtered probability space supporting a standard one-dimensional Brownian motion $W=(W_{t})_{t\\in[0,T]}$. Fix $m,n\\in\\mathbb{N}$, and let $a:[0,T]\\to\\mathbb{R}^{m}$ and $B:[0,T]\\to\\mathbb{R}^{n}$ be deterministic functions with $a\\in L^{2}([0,T];\\mathbb{R}^{m})$ and $B\\in L^{2}([0,T];\\mathbb{R}^{n})$. Define the random vectors\n$$\nX \\coloneqq \\int_{0}^{T} a(t)\\,\\mathrm{d}W_{t}\\in\\mathbb{R}^{m},\\qquad\nY \\coloneqq \\int_{0}^{T} B(t)\\,\\mathrm{d}W_{t}\\in\\mathbb{R}^{n}.\n$$\nThese are well-defined Itô integrals, and hence $(X,Y)$ is jointly Gaussian with means zero. Let the block covariance matrix $\\Sigma$ of $(X,Y)$ be\n$$\n\\Sigma \\;=\\;\n\\begin{pmatrix}\n\\Sigma_{XX} & \\Sigma_{XY} \\\\\n\\Sigma_{YX} & \\Sigma_{YY}\n\\end{pmatrix},\n$$\nwhere $\\Sigma_{XX}\\in\\mathbb{R}^{m\\times m}$, $\\Sigma_{XY}\\in\\mathbb{R}^{m\\times n}$, $\\Sigma_{YX}=\\Sigma_{XY}^{\\top}$, and $\\Sigma_{YY}\\in\\mathbb{R}^{n\\times n}$, with entries given by\n$$\n\\Sigma_{XX}=\\int_{0}^{T} a(t)a(t)^{\\top}\\,\\mathrm{d}t,\\qquad\n\\Sigma_{XY}=\\int_{0}^{T} a(t)B(t)^{\\top}\\,\\mathrm{d}t,\\qquad\n\\Sigma_{YY}=\\int_{0}^{T} B(t)B(t)^{\\top}\\,\\mathrm{d}t.\n$$\nAssume $\\Sigma_{YY}$ is invertible.\n\nUsing only foundational principles appropriate for stochastic differential equations (SDE), namely: the definition of conditional expectation as the $L^{2}$-orthogonal projection onto $\\sigma(Y)$, the fact that linear transformations of Gaussian vectors are Gaussian, and the fact that for jointly Gaussian vectors uncorrelatedness implies independence, derive from first principles that $\\mathbb{E}[X\\mid\\sigma(Y)]$ is a linear function of $Y$ and compute the coefficient matrix explicitly in terms of $\\Sigma$. Your final answer must be a single closed-form analytic expression for $\\mathbb{E}[X\\mid\\sigma(Y)]$ written in terms of the blocks of $\\Sigma$ and $Y$.", "solution": "The problem requires the derivation of the conditional expectation $\\mathbb{E}[X\\mid\\sigma(Y)]$ for two jointly Gaussian random vectors $X$ and $Y$ derived from Itô integrals. The derivation must be based on first principles as specified: the interpretation of conditional expectation as an $L^{2}$-orthogonal projection, and properties of Gaussian vectors.\n\nLet the random vector $X$ be an element of the Hilbert space $L^{2}(\\Omega, \\mathcal{F}, \\mathbb{P}; \\mathbb{R}^{m})$ of square-integrable $\\mathbb{R}^{m}$-valued random variables. The conditional expectation $\\mathbb{E}[X\\mid\\sigma(Y)]$ is the orthogonal projection of $X$ onto the closed subspace $L^{2}(\\Omega, \\sigma(Y), \\mathbb{P}; \\mathbb{R}^{m})$ of $\\sigma(Y)$-measurable random vectors.\n\nWe hypothesize that this projection is a linear function of $Y$, i.e., there exists a deterministic matrix $C \\in \\mathbb{R}^{m \\times n}$ such that $\\mathbb{E}[X\\mid\\sigma(Y)] = CY$. Let us denote $\\hat{X} \\coloneqq CY$. According to the projection theorem, if $\\hat{X}$ is indeed the orthogonal projection, then the error vector $X - \\hat{X}$ must be orthogonal to the subspace $L^{2}(\\Omega, \\sigma(Y), \\mathbb{P}; \\mathbb{R}^{m})$.\n\nFor zero-mean random vectors, orthogonality is equivalent to uncorrelatedness. Thus, the condition is that $X - CY$ must be uncorrelated with every $\\sigma(Y)$-measurable random vector. Since the linear combinations of the components of $Y$ are dense in the Gaussian space generated by $Y$, it is sufficient to require that $X - CY$ is uncorrelated with $Y$ itself. This is expressed through the covariance matrix:\n$$\n\\mathbb{E}\\left[ (X - CY) Y^{\\top} \\right] = 0_{m \\times n}\n$$\nwhere $0_{m \\times n}$ is the $m \\times n$ zero matrix.\n\nUsing the linearity of expectation, we can expand this expression:\n$$\n\\mathbb{E}\\left[ XY^{\\top} \\right] - \\mathbb{E}\\left[ (CY) Y^{\\top} \\right] = 0_{m \\times n}\n$$\nSince $C$ is a deterministic matrix, it can be factored out of the expectation:\n$$\n\\mathbb{E}\\left[ XY^{\\top} \\right] - C \\mathbb{E}\\left[ YY^{\\top} \\right] = 0_{m \\times n}\n$$\nBy definition, the covariance matrices are $\\Sigma_{XY} = \\mathbb{E}[XY^{\\top}]$ and $\\Sigma_{YY} = \\mathbb{E}[YY^{\\top}]$. The equation becomes:\n$$\n\\Sigma_{XY} - C \\Sigma_{YY} = 0_{m \\times n}\n$$\nThe problem states that the matrix $\\Sigma_{YY}$ is invertible, which allows us to solve for the unique matrix $C$:\n$$\nC = \\Sigma_{XY} \\Sigma_{YY}^{-1}\n$$\nThe dimensions are consistent: $\\Sigma_{XY}$ is $m \\times n$ and $\\Sigma_{YY}^{-1}$ is $n \\times n$, so $C$ is an $m \\times n$ matrix.\n\nSo far, we have shown that if the conditional expectation $\\mathbb{E}[X\\mid\\sigma(Y)]$ is a linear function of $Y$, it must be $\\Sigma_{XY} \\Sigma_{YY}^{-1} Y$. We must now rigorously prove that this is indeed the correct expression, using the specified principles.\n\nLet us define a new random vector $U \\in \\mathbb{R}^{m}$:\n$$\nU \\coloneqq X - \\Sigma_{XY} \\Sigma_{YY}^{-1} Y\n$$\nThe random vectors $X$ and $Y$ are jointly Gaussian. The vector $U$ is a linear transformation of the vector $(X,Y)$, so the augmented vector $(U,Y)$ is also jointly Gaussian.\n\nLet us compute the covariance of $U$ and $Y$:\n$$\n\\mathbb{E}[UY^{\\top}] = \\mathbb{E}\\left[ \\left(X - \\Sigma_{XY} \\Sigma_{YY}^{-1} Y\\right) Y^{\\top} \\right]\n$$\n$$\n\\mathbb{E}[UY^{\\top}] = \\mathbb{E}[XY^{\\top}] - \\Sigma_{XY} \\Sigma_{YY}^{-1} \\mathbb{E}[YY^{\\top}]\n$$\n$$\n\\mathbb{E}[UY^{\\top}] = \\Sigma_{XY} - \\Sigma_{XY} \\Sigma_{YY}^{-1} \\Sigma_{YY} = \\Sigma_{XY} - \\Sigma_{XY} = 0_{m \\times n}\n$$\nThis shows that $U$ and $Y$ are uncorrelated. A fundamental property of jointly Gaussian random vectors is that uncorrelatedness implies independence. Therefore, $U$ and $Y$ are independent random vectors. Independence of $U$ from $Y$ implies independence of $U$ from the sigma-algebra generated by $Y$, $\\sigma(Y)$.\n\nNow we can compute the conditional expectation of $X$ given $\\sigma(Y)$:\n$$\n\\mathbb{E}[X\\mid\\sigma(Y)] = \\mathbb{E}[U + \\Sigma_{XY} \\Sigma_{YY}^{-1} Y \\mid \\sigma(Y)]\n$$\nBy the linearity of conditional expectation:\n$$\n\\mathbb{E}[X\\mid\\sigma(Y)] = \\mathbb{E}[U\\mid\\sigma(Y)] + \\mathbb{E}[\\Sigma_{XY} \\Sigma_{YY}^{-1} Y \\mid \\sigma(Y)]\n$$\nLet's analyze each term:\n1.  For the first term, since $U$ is independent of $\\sigma(Y)$, its conditional expectation given $\\sigma(Y)$ is simply its unconditional expectation: $\\mathbb{E}[U\\mid\\sigma(Y)] = \\mathbb{E}[U]$.\n2.  For the second term, the vector $\\Sigma_{XY} \\Sigma_{YY}^{-1} Y$ is a function of $Y$ and is therefore $\\sigma(Y)$-measurable. For any $\\sigma(Y)$-measurable random vector $Z$, it holds that $\\mathbb{E}[Z\\mid\\sigma(Y)] = Z$. Thus, $\\mathbb{E}[\\Sigma_{XY} \\Sigma_{YY}^{-1} Y \\mid \\sigma(Y)] = \\Sigma_{XY} \\Sigma_{YY}^{-1} Y$.\n\nCombining these results, we get:\n$$\n\\mathbb{E}[X\\mid\\sigma(Y)] = \\mathbb{E}[U] + \\Sigma_{XY} \\Sigma_{YY}^{-1} Y\n$$\nWe now compute the unconditional expectation of $U$:\n$$\n\\mathbb{E}[U] = \\mathbb{E}[X - \\Sigma_{XY} \\Sigma_{YY}^{-1} Y] = \\mathbb{E}[X] - \\Sigma_{XY} \\Sigma_{YY}^{-1} \\mathbb{E}[Y]\n$$\nThe problem states that $X$ and $Y$ are zero-mean Itô integrals, so $\\mathbb{E}[X] = 0$ and $\\mathbb{E}[Y] = 0$. Therefore, $\\mathbb{E}[U] = 0$.\n\nSubstituting this back, we obtain the final expression for the conditional expectation:\n$$\n\\mathbb{E}[X\\mid\\sigma(Y)] = 0 + \\Sigma_{XY} \\Sigma_{YY}^{-1} Y = \\Sigma_{XY} \\Sigma_{YY}^{-1} Y\n$$\nThis derivation confirms that, for jointly Gaussian vectors, the conditional expectation $\\mathbb{E}[X\\mid\\sigma(Y)]$ is a linear function of $Y$, and it provides the explicit form of the coefficient matrix in terms of the block covariance matrices. The derivation relied only on the specified first principles.", "answer": "$$\n\\boxed{\\Sigma_{XY} \\Sigma_{YY}^{-1} Y}\n$$", "id": "2971565"}, {"introduction": "While the Gaussian framework is fundamental, many phenomena in finance and engineering exhibit non-linear dependencies. Copula theory provides a flexible and powerful toolkit for modeling such complex relationships. This practice asks you to step beyond the linear world and derive a general expression for conditional expectation using copula functions, demonstrating how to make the concept computationally explicit for a vast class of non-Gaussian distributions [@problem_id:2971556].", "problem": "Let $(\\Omega,\\mathcal{F},\\mathbb{P})$ be a probability space and let $(X,Y)$ be a pair of real-valued random variables with continuous marginal distribution functions $F_{X}$ and $F_{Y}$. Assume that $(X,Y)$ admits a copula $C$ in the sense of Sklar’s theorem and that $C$ is twice continuously differentiable on $(0,1)^{2}$. Denote by $c(u,v)$ the copula density, i.e., $c(u,v)=\\frac{\\partial^{2}}{\\partial u\\,\\partial v}C(u,v)$. Let $\\sigma(Y)$ be the $\\sigma$-algebra generated by $Y$.\n\n1) Starting from the definition of conditional expectation with respect to $\\sigma(Y)$, and using only the fundamental facts that (i) Sklar’s theorem represents the joint distribution via the copula $C$ and the marginals $F_{X}$ and $F_{Y}$, and (ii) the probability integral transform maps $X$ to $U=F_{X}(X)$ and $Y$ to $V=F_{Y}(Y)$ which are uniform on $(0,1)$, derive an explicit integral representation for $\\mathbb{E}[X\\mid\\sigma(Y)]$ in terms of $C$, $F_{X}$, $F_{Y}$, and $c$. Your representation must be given as a function of $Y$, without assuming any specific form of $F_{X}$, $F_{Y}$, or $C$ beyond the differentiability stated above.\n\n2) Specialize to the case where $X$ and $Y$ have uniform marginals on $[0,1]$ and the copula is the Farlie–Gumbel–Morgenstern copula with parameter $\\theta\\in[-1,1]$, given by $C(u,v)=uv\\bigl(1+\\theta(1-u)(1-v)\\bigr)$. Compute a closed-form expression for $\\mathbb{E}[X\\mid\\sigma(Y)]$ as a function of $Y$ and $\\theta$, and determine whether it is nondecreasing in $Y$ and for which values of $\\theta$ it is strictly increasing, strictly decreasing, or constant. Provide your final answer as a single simplified analytic expression. No rounding is required and no units are involved.", "solution": "We begin from core definitions and well-tested facts. By the definition of conditional expectation with respect to the sub-$\\sigma$-algebra $\\sigma(Y)$, there exists a measurable function $g$ such that $\\mathbb{E}[X\\mid\\sigma(Y)]=g(Y)$ almost surely, and $g(y)=\\mathbb{E}[X\\mid Y=y]$ wherever the regular conditional distribution exists. Since $F_{X}$ and $F_{Y}$ are continuous and the copula $C$ is twice differentiable, a regular conditional distribution exists and can be expressed via $C$.\n\nBy Sklar’s theorem, the joint cumulative distribution function satisfies\n$$\nF_{X,Y}(x,y)=C\\bigl(F_{X}(x),F_{Y}(y)\\bigr).\n$$\nDefine the probability integral transforms $U=F_{X}(X)$ and $V=F_{Y}(Y)$. Then $U$ and $V$ are each uniform on $(0,1)$ and have joint distribution with cumulative distribution function $C(u,v)$ and density $c(u,v)=\\frac{\\partial^{2}}{\\partial u\\,\\partial v}C(u,v)$. The marginal density of $V$ is\n$$\nf_{V}(v)=\\int_{0}^{1}c(u,v)\\,du.\n$$\nFor a copula, $f_{V}(v)=1$ for all $v\\in(0,1)$ because $V$ is uniform. Consequently, differentiating $C(u,v)$ with respect to $v$ yields the conditional distribution function of $U$ given $V=v$:\n$$\nF_{U\\mid V=v}(u)=\\frac{\\partial}{\\partial v}C(u,v).\n$$\nDifferentiating $F_{U\\mid V=v}(u)$ with respect to $u$ gives the conditional density of $U$ given $V=v$:\n$$\nf_{U\\mid V=v}(u)=\\frac{\\partial^{2}}{\\partial u\\,\\partial v}C(u,v)=c(u,v),\\quad u\\in(0,1).\n$$\n\nWe now relate $\\mathbb{E}[X\\mid Y=y]$ to the conditional distribution of $U$ given $V=v$ by the law of the unconscious statistician. Since $X=F_{X}^{-1}(U)$ almost surely, we have\n$$\n\\mathbb{E}[X\\mid Y=y]=\\mathbb{E}\\bigl[F_{X}^{-1}(U)\\mid V=F_{Y}(y)\\bigr]\n=\\int_{0}^{1}F_{X}^{-1}(u)\\,f_{U\\mid V=F_{Y}(y)}(u)\\,du\n=\\int_{0}^{1}F_{X}^{-1}(u)\\,c\\bigl(u,F_{Y}(y)\\bigr)\\,du.\n$$\nTherefore,\n$$\n\\mathbb{E}[X\\mid\\sigma(Y)]=g(Y),\\quad\\text{where}\\quad g(y)=\\int_{0}^{1}F_{X}^{-1}(u)\\,c\\bigl(u,F_{Y}(y)\\bigr)\\,du.\n$$\nThis completes the general expression.\n\nWe now specialize to the Farlie–Gumbel–Morgenstern copula with parameter $\\theta\\in[-1,1]$ and uniform marginals on $[0,1]$. In this case, $F_{X}(x)=x$ and $F_{Y}(y)=y$ for $x,y\\in[0,1]$, and thus $F_{X}^{-1}(u)=u$ and $V=Y$. The copula is\n$$\nC(u,v)=uv\\bigl(1+\\theta(1-u)(1-v)\\bigr),\\quad (u,v)\\in[0,1]^{2}.\n$$\nThe copula density is\n$$\nc(u,v)=\\frac{\\partial^{2}}{\\partial u\\,\\partial v}C(u,v)=1+\\theta(1-2u)(1-2v).\n$$\nConsequently,\n$$\n\\mathbb{E}[X\\mid Y=y]=\\int_{0}^{1}u\\left[1+\\theta(1-2u)(1-2y)\\right]\\,du\n=\\int_{0}^{1}u\\,du+\\theta(1-2y)\\int_{0}^{1}u(1-2u)\\,du.\n$$\nWe compute the elementary integrals:\n$$\n\\int_{0}^{1}u\\,du=\\frac{1}{2},\\qquad\n\\int_{0}^{1}u(1-2u)\\,du=\\int_{0}^{1}(u-2u^{2})\\,du=\\left[\\frac{1}{2}-\\frac{2}{3}\\right]=-\\frac{1}{6}.\n$$\nTherefore,\n$$\n\\mathbb{E}[X\\mid Y=y]=\\frac{1}{2}+\\theta(1-2y)\\left(-\\frac{1}{6}\\right)\n=\\frac{1}{2}-\\frac{\\theta}{6}+\\frac{\\theta}{3}\\,y.\n$$\nSince $\\mathbb{E}[X\\mid\\sigma(Y)]=\\mathbb{E}[X\\mid Y]$ almost surely when $Y$ is the conditioning variable, we obtain\n$$\n\\mathbb{E}[X\\mid\\sigma(Y)]=\\frac{1}{2}-\\frac{\\theta}{6}+\\frac{\\theta}{3}\\,Y.\n$$\nThis function is affine in $Y$ with slope $\\frac{\\theta}{3}$. Hence it is nondecreasing in $Y$ if and only if $\\theta\\ge 0$, strictly increasing if $\\theta>0$, strictly decreasing if $\\theta0$, and constant if $\\theta=0$ (the independence case, yielding $\\mathbb{E}[X\\mid\\sigma(Y)]=\\frac{1}{2}$).\n\nThus the required closed-form expression is the affine function of $Y$ and $\\theta$ given above.", "answer": "$$\\boxed{\\frac{\\theta}{3}\\,Y+\\frac{1}{2}-\\frac{\\theta}{6}}$$", "id": "2971556"}, {"introduction": "Moving from specific computations to general theory, we now examine a crucial property for the analysis of stochastic processes. The concept of uniform integrability is essential for proving convergence theorems, particularly for martingales, as it ensures that limits and expectations can be interchanged. This problem challenges you to determine if a sequence of conditional probabilities is always uniformly integrable, thereby testing your grasp of the fundamental bounds that govern conditional expectations [@problem_id:1408777].", "problem": "Let $(\\Omega, \\mathcal{F}, \\mathbb{P})$ be a probability space. A sequence of real-valued random variables $\\{X_n\\}_{n=1}^{\\infty}$ defined on this space is said to be uniformly integrable if it satisfies the condition:\n$$ \\lim_{K \\to \\infty} \\sup_{n \\ge 1} \\mathbb{E}[|X_n| \\mathbf{1}_{\\{|X_n| > K\\}}] = 0 $$\nHere, $\\mathbb{E}[\\cdot]$ denotes the expectation operator, and $\\mathbf{1}_{A}$ denotes the indicator function of a set $A$.\n\nNow consider the following statement:\n\n\"For any given probability space $(\\Omega, \\mathcal{F}, \\mathbb{P})$, any sub-$\\sigma$-algebra $\\mathcal{G} \\subseteq \\mathcal{F}$, and any sequence of events $\\{A_n\\}_{n=1}^{\\infty}$ in $\\mathcal{F}$, the sequence of random variables defined by $X_n = \\mathbb{P}(A_n | \\mathcal{G})$ for $n \\ge 1$ is always uniformly integrable.\"\n\nWhich of the following options correctly evaluates this statement and provides the appropriate reasoning?\n\nA. The statement is true. This follows from the general principle that any sequence of conditional expectations is uniformly integrable.\n\nB. The statement is true. This is a direct consequence of the fact that the random variables $X_n$ are uniformly bounded.\n\nC. The statement is false. A counterexample can be constructed if the sub-$\\sigma$-algebra $\\mathcal{G}$ is chosen to be the trivial one, i.e., $\\mathcal{G} = \\{\\emptyset, \\Omega\\}$.\n\nD. The statement is false. It is possible to choose a sequence of events $\\{A_n\\}$ such that the resulting sequence of random variables $\\{X_n\\}$ is not uniformly integrable.\n\nE. The statement's validity depends on the specific choice of the underlying probability space $(\\Omega, \\mathcal{F}, \\mathbb{P})$.", "solution": "By definition, for each event $A_{n} \\in \\mathcal{F}$ and sub-$\\sigma$-algebra $\\mathcal{G} \\subseteq \\mathcal{F}$, the random variable\n$$\nX_{n}=\\mathbb{P}(A_{n}\\mid \\mathcal{G})=\\mathbb{E}\\big[\\mathbf{1}_{A_{n}}\\mid \\mathcal{G}\\big]\n$$\nis the conditional expectation of the indicator $\\mathbf{1}_{A_{n}}$. Since $0 \\leq \\mathbf{1}_{A_{n}} \\leq 1$ almost surely, by monotonicity and linearity of conditional expectation we have\n$$\n0 \\leq \\mathbb{E}\\big[\\mathbf{1}_{A_{n}}\\mid \\mathcal{G}\\big] \\leq \\mathbb{E}\\big[1\\mid \\mathcal{G}\\big]=1 \\quad \\text{a.s.}\n$$\nThus, for all $n \\geq 1$, $|X_{n}| \\leq 1$ almost surely. Therefore, for any $K \\geq 1$,\n$$\n\\mathbf{1}_{\\{|X_{n}|>K\\}}=0 \\quad \\text{a.s.,}\n$$\nand consequently\n$$\n\\mathbb{E}\\big[|X_{n}|\\mathbf{1}_{\\{|X_{n}|K\\}}\\big]=0 \\quad \\text{for all } n \\geq 1 \\text{ and } K \\geq 1.\n$$\nTaking the supremum over $n$ and then the limit as $K \\to \\infty$ yields\n$$\n\\lim_{K \\to \\infty} \\sup_{n \\geq 1} \\mathbb{E}\\big[|X_{n}|\\mathbf{1}_{\\{|X_{n}|K\\}}\\big]=0,\n$$\nwhich is exactly the definition of uniform integrability. Hence, the sequence $\\{X_{n}\\}$ is always uniformly integrable because it is uniformly bounded.\n\nEvaluating the options:\n- A is false in general; an arbitrary sequence of conditional expectations need not be uniformly integrable without additional bounds or uniform integrability of the original sequence.\n- B is true and provides the correct reasoning: the variables are uniformly bounded in $[0,1]$.\n- C is false because for the trivial $\\sigma$-algebra, $X_{n}=\\mathbb{P}(A_{n}) \\in [0,1]$ remains uniformly bounded and hence uniformly integrable.\n- D is false because no choice of $\\{A_{n}\\}$ can violate uniform integrability given the uniform bound.\n- E is false because the argument does not depend on the specific probability space.\n\nTherefore, the correct option is B.", "answer": "$$\\boxed{B}$$", "id": "1408777"}]}