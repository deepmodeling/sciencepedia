{"hands_on_practices": [{"introduction": "Understanding the limiting behavior of stochastic processes often involves the concept of weak convergence of their probability laws. It is crucial to distinguish this from stronger notions of convergence, such as convergence in total variation. This exercise explores this fundamental difference by constructing a sequence of Gaussian measures that converge weakly to a Dirac measure, yet remain maximally distant in the total variation norm [@problem_id:3005015]. Working through this example solidifies the understanding of why weak convergence is characterized by integration against continuous functions, a property that can overlook discrepancies on sets of measure zero, unlike total variation.", "problem": "Let $\\left(\\Omega,\\mathcal{F},\\mathbb{P}\\right)$ be a probability space supporting a one-dimensional standard Brownian motion $W=\\left(W_t\\right)_{t\\ge 0}$. Consider, for each integer $n\\ge 1$, the linear stochastic differential equation (SDE)\n$$\n\\mathrm{d}X_t^{(n)} \\;=\\; -\\,X_t^{(n)}\\,\\mathrm{d}t \\;+\\; \\sigma_n\\,\\mathrm{d}W_t,\\qquad X_0^{(n)}=0,\n$$\nwhere $\\sigma_n>0$ and $\\sigma_n\\downarrow 0$ as $n\\to\\infty$. Let $\\mu_n$ denote the law of $X_1^{(n)}$ on $\\mathbb{R}$ with its Borel $\\sigma$-algebra, and let $\\mu$ denote the Dirac measure at $0$, written $\\mu=\\delta_0$.\n\nUse the following foundational definitions and facts as your starting point:\n\n- Weak convergence (convergence in distribution): For probability measures $\\mu_n,\\mu$ on a Polish space (complete separable metric space), $\\mu_n\\Rightarrow\\mu$ if and only if $\\int f\\,\\mathrm{d}\\mu_n\\to\\int f\\,\\mathrm{d}\\mu$ for every bounded continuous function $f$.\n- Total variation distance: For probability measures $\\mu$ and $\\nu$ on a measurable space, define $\\|\\mu-\\nu\\|_{\\mathrm{TV}}:=\\sup_{A\\in\\mathcal{B}}|\\mu(A)-\\nu(A)|$, where $\\mathcal{B}$ is the $\\sigma$-algebra of measurable sets.\n- Tightness: A family $\\{\\mu_n:n\\ge 1\\}$ of probability measures on a Polish space is tight if for every $\\varepsilon>0$ there exists a compact set $K$ such that $\\inf_{n\\ge 1}\\mu_n(K)\\ge 1-\\varepsilon$.\n- Prokhorov's theorem: On a Polish space, tightness of a family of probability measures is equivalent to relative compactness with respect to weak convergence; that is, every sequence has a weakly convergent subsequence.\n\nSelect all statements that are correct.\n\nA. For $\\sigma_n=1/\\sqrt{n}$, one has $\\mu_n\\Rightarrow\\mu$, $\\{\\mu_n\\}_{n\\ge 1}$ is tight, and moreover $\\|\\mu_n-\\mu\\|_{\\mathrm{TV}}=1$ for all $n$, so $\\|\\mu_n-\\mu\\|_{\\mathrm{TV}}\\not\\to 0$.\n\nB. By Prokhorov's theorem, tightness of $\\{\\mu_n\\}_{n\\ge 1}$ together with uniqueness of the weak limit implies $\\|\\mu_n-\\mu\\|_{\\mathrm{TV}}\\to 0$.\n\nC. By the portmanteau theorem, $\\mu_n\\Rightarrow\\mu$ implies $\\int f\\,\\mathrm{d}\\mu_n\\to\\int f\\,\\mathrm{d}\\mu$ for every bounded measurable function $f$, hence total variation convergence follows.\n\nD. The failure of total variation convergence in this example is explained by the fact that total variation distance tests all bounded measurable functions (equivalently, all measurable sets), including indicators of sets such as $\\{0\\}$ whose indicator function is discontinuous, whereas weak convergence only tests bounded continuous functions.\n\nE. The sequence $\\{\\mu_n\\}_{n\\ge 1}$ cannot be tight because $\\|\\mu_n-\\mu\\|_{\\mathrm{TV}}\\not\\to 0$.", "solution": "The problem presents a sequence of stochastic processes $X_t^{(n)}$ governed by linear stochastic differential equations (SDEs) and asks to evaluate statements concerning the convergence of the laws of these processes at time $t=1$.\n\nFirst, we solve the SDE for $X_t^{(n)}$. The SDE is a standard Ornstein-Uhlenbeck process:\n$$\n\\mathrm{d}X_t^{(n)} = -X_t^{(n)}\\mathrm{d}t + \\sigma_n\\mathrm{d}W_t, \\quad X_0^{(n)} = 0\n$$\nThe explicit solution to this SDE is given by applying Itô's formula to $e^t X_t^{(n)}$ or by using an integrating factor $e^t$. The solution is:\n$$\nX_t^{(n)} = e^{-t}X_0^{(n)} + \\int_0^t e^{-(t-s)} \\sigma_n \\mathrm{d}W_s\n$$\nWith the initial condition $X_0^{(n)}=0$, this simplifies to:\n$$\nX_t^{(n)} = \\sigma_n \\int_0^t e^{-(t-s)} \\mathrm{d}W_s\n$$\nThe random variable $X_t^{(n)}$ is an Itô integral with a deterministic integrand, and as such, it is a Gaussian random variable. Its mean is:\n$$\n\\mathbb{E}[X_t^{(n)}] = \\mathbb{E}\\left[\\sigma_n \\int_0^t e^{-(t-s)} \\mathrm{d}W_s\\right] = 0\n$$\nIts variance is calculated using the Itô isometry:\n$$\n\\mathrm{Var}(X_t^{(n)}) = \\mathbb{E}[(X_t^{(n)})^2] = \\sigma_n^2 \\mathbb{E}\\left[\\left(\\int_0^t e^{-(t-s)} \\mathrm{d}W_s\\right)^2\\right] = \\sigma_n^2 \\int_0^t (e^{-(t-s)})^2 \\mathrm{d}s\n$$\n$$\n\\mathrm{Var}(X_t^{(n)}) = \\sigma_n^2 \\int_0^t e^{-2(t-s)} \\mathrm{d}s = \\sigma_n^2 \\left[\\frac{e^{-2(t-s)}}{2}\\right]_{s=0}^{s=t} = \\frac{\\sigma_n^2}{2}(1 - e^{-2t})\n$$\nWe are interested in the law $\\mu_n$ of $X_1^{(n)}$, so we set $t=1$. The random variable $X_1^{(n)}$ is normally distributed with mean $0$ and variance $v_n^2 := \\frac{\\sigma_n^2}{2}(1-e^{-2})$. So, $\\mu_n = \\mathcal{N}\\left(0, v_n^2\\right)$.\n\nThe problem states that $\\sigma_n > 0$ and $\\sigma_n \\downarrow 0$ as $n\\to\\infty$. This implies that the variance $v_n^2 \\to 0$ as $n\\to\\infty$. The limit measure is $\\mu = \\delta_0$, which is the law of a random variable that is identically $0$. This can be viewed as a degenerate normal distribution $\\mathcal{N}(0,0)$.\n\nNow we analyze the convergence properties.\n**Weak Convergence ($\\mu_n \\Rightarrow \\mu$):**\nA sequence of Gaussian distributions $\\mathcal{N}(m_n, s_n^2)$ converges weakly to $\\mathcal{N}(m, s^2)$ if and only if $m_n \\to m$ and $s_n^2 \\to s^2$. Here, $m_n=0$ for all $n$, and $v_n^2 \\to 0$. The limit measure $\\delta_0$ corresponds to $m=0$ and $s^2=0$. Thus, $\\mu_n = \\mathcal{N}(0, v_n^2)$ converges weakly to $\\delta_0 = \\mu$. This can also be seen by noting that $X_1^{(n)} \\to 0$ in $L^2(\\Omega)$, since $\\mathbb{E}[(X_1^{(n)})^2] = v_n^2 \\to 0$, which implies convergence in probability, which in turn implies convergence in distribution (weak convergence).\n\n**Tightness:**\nA sequence of probability measures on a Polish space that converges weakly is necessarily tight. This is a direct consequence of Prokhorov's theorem. Therefore, the family $\\{\\mu_n\\}_{n\\ge 1}$ is tight. We can also show this directly. For any $\\varepsilon > 0$, we need to find a compact set $K_M = [-M, M]$ such that $\\mu_n(K_M) \\ge 1-\\varepsilon$ for all $n$. By Chebyshev's inequality:\n$$\n\\mu_n(\\mathbb{R} \\setminus K_M) = \\mathbb{P}(|X_1^{(n)}| > M) \\le \\frac{\\mathrm{Var}(X_1^{(n)})}{M^2} = \\frac{v_n^2}{M^2}\n$$\nSince $v_n^2 \\to 0$, the sequence $\\{v_n^2\\}_{n\\ge 1}$ is bounded, say by $V_{\\max} = v_1^2 = \\frac{\\sigma_1^2}{2}(1-e^{-2})$. Thus, $\\mathbb{P}(|X_1^{(n)}| > M) \\le \\frac{V_{\\max}}{M^2}$. Choosing $M > \\sqrt{V_{\\max}/\\varepsilon}$ ensures that $\\mathbb{P}(|X_1^{(n)}| > M) < \\varepsilon$ for all $n \\ge 1$. Hence, $\\{\\mu_n\\}_{n\\ge 1}$ is tight.\n\n**Total Variation Convergence ($\\|\\mu_n - \\mu\\|_{\\mathrm{TV}} \\to 0$):**\nThe measure $\\mu_n = \\mathcal{N}(0, v_n^2)$ is absolutely continuous with respect to the Lebesgue measure on $\\mathbb{R}$ for any $n \\ge 1$ (since $\\sigma_n>0 \\implies v_n^2>0$). The limit measure $\\mu = \\delta_0$ is a point mass, which is singular with respect to the Lebesgue measure.\nFor any two probability measures $\\nu_1$ and $\\nu_2$ that are mutually singular (i.e., there exists a set $A$ such that $\\nu_1(A)=1$ and $\\nu_2(A^c)=1$), their total variation distance is $\\|\\nu_1-\\nu_2\\|_{\\mathrm{TV}}=1$. In our case, let $A=\\{0\\}$. Then $\\mu_n(A^c) = \\mu_n(\\mathbb{R}\\setminus\\{0\\}) = 1$ since $\\mu_n$ has a density, and $\\mu(A) = \\delta_0(\\{0\\}) = 1$. Thus, $\\mu_n$ and $\\mu$ are mutually singular for every $n \\ge 1$.\nAlternatively, we compute the distance directly from the definition:\n$$\n\\|\\mu_n - \\mu\\|_{\\mathrm{TV}} = \\sup_{A \\in \\mathcal{B}(\\mathbb{R})} |\\mu_n(A) - \\mu(A)|\n$$\nLet's choose the set $A = \\{0\\}$. We have $\\mu_n(\\{0\\}) = 0$ (as $\\mu_n$ is a continuous distribution) and $\\mu(\\{0\\}) = \\delta_0(\\{0\\}) = 1$.\nTherefore, $|\\mu_n(\\{0\\}) - \\mu(\\{0\\})| = |0-1| = 1$.\nSince the total variation distance is bounded by $1$, the supremum must be exactly $1$.\nSo, $\\|\\mu_n - \\mu\\|_{\\mathrm{TV}} = 1$ for all $n \\ge 1$. This implies that the sequence does not converge in total variation, i.e., $\\|\\mu_n - \\mu\\|_{\\mathrm{TV}} \\not\\to 0$.\n\nNow we evaluate each option:\n\n**A. For $\\sigma_n=1/\\sqrt{n}$, one has $\\mu_n\\Rightarrow\\mu$, $\\{\\mu_n\\}_{n\\ge 1}$ is tight, and moreover $\\|\\mu_n-\\mu\\|_{\\mathrm{TV}}=1$ for all $n$, so $\\|\\mu_n-\\mu\\|_{\\mathrm{TV}}\\not\\to 0$.**\nThe choice $\\sigma_n = 1/\\sqrt{n}$ satisfies the condition $\\sigma_n > 0$ and $\\sigma_n \\downarrow 0$. Our general analysis above applies.\n- $\\mu_n\\Rightarrow\\mu$: This is correct.\n- $\\{\\mu_n\\}_{n\\ge 1}$ is tight: This is correct.\n- $\\|\\mu_n-\\mu\\|_{\\mathrm{TV}}=1$ for all $n$: This is correct.\n- so $\\|\\mu_n-\\mu\\|_{\\mathrm{TV}}\\not\\to 0$: This is a correct conclusion.\nEvery part of this statement is correct.\n**Verdict: Correct.**\n\n**B. By Prokhorov's theorem, tightness of $\\{\\mu_n\\}_{n\\ge 1}$ together with uniqueness of the weak limit implies $\\|\\mu_n-\\mu\\|_{\\mathrm{TV}}\\to 0$.**\nProkhorov's theorem states that on a Polish space, tightness is equivalent to relative compactness in the topology of weak convergence. Tightness and uniqueness of the weak limit point together imply that the sequence converges weakly ($\\mu_n \\Rightarrow \\mu$). The statement incorrectly claims that this implies convergence in total variation. As demonstrated by this very problem, we have tightness and weak convergence, but not total variation convergence. Thus, the implication is false.\n**Verdict: Incorrect.**\n\n**C. By the portmanteau theorem, $\\mu_n\\Rightarrow\\mu$ implies $\\int f\\,\\mathrm{d}\\mu_n\\to\\int f\\,\\mathrm{d}\\mu$ for every bounded measurable function $f$, hence total variation convergence follows.**\nThis statement misrepresents the portmanteau theorem. Weak convergence, $\\mu_n\\Rightarrow\\mu$, is equivalent to $\\int f\\,\\mathrm{d}\\mu_n\\to\\int f\\,\\mathrm{d}\\mu$ for every bounded and *continuous* function $f$ (or bounded and uniformly continuous), not for every bounded *measurable* function $f$. Convergence for all bounded measurable functions is a characterization of convergence in total variation. The premise is false. A counterexample is the function $f = \\mathbf{1}_{\\{0\\}}$, which is bounded and measurable but not continuous. For this $f$, $\\int f\\,\\mathrm{d}\\mu_n = \\mu_n(\\{0\\}) = 0$ for all $n$, while $\\int f\\,\\mathrm{d}\\mu = \\mu(\\{0\\}) = 1$. The limit $0$ is not equal to $1$.\n**Verdict: Incorrect.**\n\n**D. The failure of total variation convergence in this example is explained by the fact that total variation distance tests all bounded measurable functions (equivalently, all measurable sets), including indicators of sets such as $\\{0\\}$ whose indicator function is discontinuous, whereas weak convergence only tests bounded continuous functions.**\nThis statement accurately diagnoses why weak convergence does not imply total variation convergence in this context. Total variation convergence, defined as $\\sup_A |\\mu_n(A) - \\mu(A)| \\to 0$, is a stronger mode of convergence than weak convergence. The class of test functions for weak convergence (bounded continuous functions) is smaller and more restrictive than the class for total variation convergence (bounded measurable functions, which includes discontinuous indicators of sets like $\\mathbf{1}_{\\{0\\}}$). The discontinuity of test functions like $\\mathbf{1}_{\\{0\\}}$ is exactly what allows total variation distance to \"detect\" the difference between the continuous measure $\\mu_n$ and the discrete measure $\\mu$, a difference that weak convergence \"ignores\". The explanation is sound.\n**Verdict: Correct.**\n\n**E. The sequence $\\{\\mu_n\\}_{n\\ge 1}$ cannot be tight because $\\|\\mu_n-\\mu\\|_{\\mathrm{TV}}\\not\\to 0$.**\nThis statement proposes a false implication. It claims that the lack of convergence in total variation implies a lack of tightness. Our analysis of the problem shows that the sequence $\\{\\mu_n\\}_{n\\ge 1}$ is indeed tight, while at the same time it does not converge in total variation. Therefore, this statement is a direct contradiction of what we have demonstrated. Tightness is related to the non-escape of mass to infinity and is a prerequisite for weak convergence, not for total variation convergence.\n**Verdict: Incorrect.**", "answer": "$$\\boxed{AD}$$", "id": "3005015"}, {"introduction": "To establish weak convergence using Prokhorov's theorem, one must first prove that the family of probability laws is tight. The first step in establishing tightness for processes on $C([0,T];\\mathbb{R}^{d})$ is to show that the paths are confined to a compact set with high probability. This exercise demonstrates a fundamental technique for achieving this: using a uniform exponential moment bound and Markov's inequality to derive an explicit, uniform tail estimate for the supremum norm of the process [@problem_id:3005017]. Mastering this calculation provides a powerful and frequently used tool for controlling the large-scale behavior of stochastic processes.", "problem": "Consider a filtered probability space $(\\Omega,\\mathcal{F},(\\mathcal{F}_{t})_{t\\in[0,T]},\\mathbb{P})$ supporting a $d$-dimensional Brownian motion $\\{W_{t}\\}_{t\\in[0,T]}$. For each integer $n\\geq 1$, let $\\{X^{(n)}_{t}\\}_{t\\in[0,T]}$ be the unique strong solution to the Stochastic Differential Equation (SDE)\n$$\n\\mathrm{d}X^{(n)}_{t}=b_{n}(X^{(n)}_{t})\\,\\mathrm{d}t+\\sigma_{n}(X^{(n)}_{t})\\,\\mathrm{d}W_{t},\\quad X^{(n)}_{0}\\in\\mathbb{R}^{d},\n$$\nwhere the coefficients $b_{n}:\\mathbb{R}^{d}\\to\\mathbb{R}^{d}$ and $\\sigma_{n}:\\mathbb{R}^{d}\\to\\mathbb{R}^{d\\times d}$ are measurable and satisfy conditions ensuring existence and uniqueness of a strong solution. Denote by $\\mu_{n}$ the law of $X^{(n)}$ as a random element of the space $C([0,T];\\mathbb{R}^{d})$ equipped with the supremum norm $\\|x\\|_{\\infty}=\\sup_{t\\in[0,T]}\\|x(t)\\|$, where $\\|\\cdot\\|$ is the Euclidean norm on $\\mathbb{R}^{d}$. Assume there exist constants $\\alpha>0$ and $K<\\infty$ such that the family has uniformly bounded exponential moments of the supremum:\n$$\n\\sup_{n\\geq 1}\\,\\mathbb{E}\\Big[\\exp\\Big(\\alpha\\,\\sup_{t\\in[0,T]}\\|X^{(n)}_{t}\\|\\Big)\\Big]\\leq K.\n$$\nUsing only foundational definitions from probability theory and the topology of $C([0,T];\\mathbb{R}^{d})$, derive an explicit closed-form expression for a function $B(R)$, depending only on $R>0$, $\\alpha$, and $K$, such that the following uniform tail control holds for all $R>0$:\n$$\n\\sup_{n\\geq 1}\\,\\mu_{n}\\big(\\{x\\in C([0,T];\\mathbb{R}^{d}):\\|x\\|_{\\infty}\\geq R\\}\\big)\\leq B(R).\n$$\nExpress your final answer as a single analytic expression $B(R)$ in terms of $R$, $\\alpha$, and $K$. Do not provide an inequality or any explanatory text in the final answer.", "solution": "The problem requires us to find a function $B(R)$ that provides a uniform upper bound for the tail probability of the supremum norm of a sequence of stochastic processes. The provided information is a uniform bound on the exponential moments of these supremum norms.\n\nLet the space of continuous functions from $[0,T]$ to $\\mathbb{R}^{d}$ be denoted by $C([0,T];\\mathbb{R}^{d})$. For each integer $n \\geq 1$, the law of the process $X^{(n)}$ is a probability measure $\\mu_{n}$ on this space. The quantity we wish to bound is\n$$\n\\sup_{n\\geq 1}\\,\\mu_{n}\\big(\\{x\\in C([0,T];\\mathbb{R}^{d}):\\|x\\|_{\\infty}\\geq R\\}\\big)\n$$\nwhere $\\|x\\|_{\\infty} = \\sup_{t\\in[0,T]}\\|x(t)\\|$ and $R > 0$ is a given positive real number.\n\nBy the definition of the law $\\mu_{n}$ of the random element $X^{(n)}$, the probability of the set $\\{x\\in C([0,T];\\mathbb{R}^{d}):\\|x\\|_{\\infty}\\geq R\\}$ under $\\mu_{n}$ is equal to the probability of the event that the sample path of $X^{(n)}$ has a supremum norm greater than or equal to $R$. That is,\n$$\n\\mu_{n}\\big(\\{x\\in C([0,T];\\mathbb{R}^{d}):\\|x\\|_{\\infty}\\geq R\\}\\big) = \\mathbb{P}\\Big(\\sup_{t\\in[0,T]}\\|X^{(n)}_{t}\\| \\geq R\\Big).\n$$\nFor notational simplicity, let $Z_{n} = \\sup_{t\\in[0,T]}\\|X^{(n)}_{t}\\|$. $Z_n$ is a non-negative random variable for each $n$. The problem is to find an upper bound for $\\sup_{n \\geq 1} \\mathbb{P}(Z_{n} \\geq R)$.\n\nThe given condition is that there exist constants $\\alpha>0$ and $K<\\infty$ such that\n$$\n\\sup_{n\\geq 1}\\,\\mathbb{E}\\Big[\\exp\\big(\\alpha Z_{n}\\big)\\Big]\\leq K.\n$$\nThis implies that for any individual $n \\geq 1$, we have $\\mathbb{E}\\big[\\exp(\\alpha Z_{n})\\big]\\leq K$.\n\nWe can use Markov's inequality, a foundational result in probability theory. For a non-negative random variable $Y$ and any constant $a>0$, Markov's inequality states that\n$$\n\\mathbb{P}(Y \\geq a) \\leq \\frac{\\mathbb{E}[Y]}{a}.\n$$\nWe apply this inequality not to $Z_n$ directly, but to the transformed random variable $Y_n = \\exp(\\alpha Z_n)$. Since $\\alpha > 0$ and $Z_n \\geq 0$, $Y_n$ is a non-negative random variable.\n\nThe event $\\{Z_n \\geq R\\}$ can be related to an event involving $Y_n$. Since $\\alpha > 0$, the inequality $Z_n \\geq R$ is equivalent to $\\alpha Z_n \\geq \\alpha R$. Furthermore, because the exponential function $u \\mapsto \\exp(u)$ is strictly increasing on $\\mathbb{R}$, this is equivalent to $\\exp(\\alpha Z_n) \\geq \\exp(\\alpha R)$.\nThus, we have the equality of events:\n$$\n\\{Z_n \\geq R\\} = \\{\\exp(\\alpha Z_n) \\geq \\exp(\\alpha R)\\} = \\{Y_n \\geq \\exp(\\alpha R)\\}.\n$$\nThis implies that their probabilities are equal:\n$$\n\\mathbb{P}(Z_n \\geq R) = \\mathbb{P}(Y_n \\geq \\exp(\\alpha R)).\n$$\nNow, we apply Markov's inequality to the random variable $Y_n$ with the constant $a = \\exp(\\alpha R)$. Since $R>0$ and $\\alpha>0$, we have $a > 0$, so the application is valid.\n$$\n\\mathbb{P}(Y_n \\geq \\exp(\\alpha R)) \\leq \\frac{\\mathbb{E}[Y_n]}{\\exp(\\alpha R)}.\n$$\nSubstituting $Y_n = \\exp(\\alpha Z_n)$, we get:\n$$\n\\mathbb{P}(Z_n \\geq R) \\leq \\frac{\\mathbb{E}[\\exp(\\alpha Z_n)]}{\\exp(\\alpha R)}.\n$$\nWe are given that $\\mathbb{E}[\\exp(\\alpha Z_n)] \\leq K$ for all $n \\geq 1$. Therefore, for any $n \\geq 1$,\n$$\n\\mathbb{P}(Z_n \\geq R) \\leq \\frac{K}{\\exp(\\alpha R)}.\n$$\nThis upper bound, $\\frac{K}{\\exp(\\alpha R)}$, is independent of $n$. Since the inequality holds for every $n \\geq 1$, it must also hold for the supremum over all $n \\geq 1$.\n$$\n\\sup_{n \\geq 1} \\mathbb{P}(Z_n \\geq R) \\leq \\frac{K}{\\exp(\\alpha R)}.\n$$\nSubstituting back the definition of $Z_n$ and the probabilistic interpretation of $\\mu_n$, we obtain the desired uniform tail control:\n$$\n\\sup_{n\\geq 1}\\,\\mu_{n}\\big(\\{x\\in C([0,T];\\mathbb{R}^{d}):\\|x\\|_{\\infty}\\geq R\\}\\big) \\leq K\\exp(-\\alpha R).\n$$\nThe problem asks for an explicit closed-form expression for a function $B(R)$ that serves as this upper bound. Based on our derivation, we can define $B(R)$ as:\n$$\nB(R) = K\\exp(-\\alpha R).\n$$\nThis function depends only on $R$, $\\alpha$, and $K$, as required.", "answer": "$$\\boxed{K\\exp(-\\alpha R)}$$", "id": "3005017"}, {"introduction": "The second, more subtle condition for tightness on the space of càdlàg functions $D([0,T];\\mathbb{R}^{d})$ is controlling the oscillatory behavior of the paths. Standard criteria for continuous paths are insufficient here, as one must account for jumps and the possibility of erratic behavior concentrating at unpredictable random times. This practice introduces Aldous's criterion, a powerful tool that addresses this challenge by evaluating process increments over small time intervals that start at arbitrary stopping times [@problem_id:3005014]. Understanding this criterion and its intuition is a key step towards proving weak convergence for general semimartingales and other processes with jumps.", "problem": "Consider a sequence $\\{X^n\\}_{n \\in \\mathbb{N}}$ of $\\mathbb{R}^d$-valued càdlàg (right-continuous with left limits) stochastic processes on $[0,T]$, each adapted to a filtration and regarded as random elements of the Skorokhod space $D([0,T];\\mathbb{R}^d)$ equipped with the Skorokhod $J_1$ topology. Recall the definition of tightness for probability measures on a metrizable space: a family $\\{\\mu_n\\}$ on a metric space $(S,d)$ is tight if for every $\\varepsilon>0$ there exists a compact $K \\subset S$ such that $\\sup_{n} \\mu_n(K^c) < \\varepsilon$. By Prokhorov’s theorem, tightness implies relative compactness of the family of laws on $(S,d)$. A stopping time $\\tau$ with respect to a filtration $(\\mathcal{F}_t)_{t \\in [0,T]}$ is a random time with the property that $\\{\\tau \\le t\\} \\in \\mathcal{F}_t$ for all $t \\in [0,T]$. Let $\\mathcal{T}_T$ denote the set of all stopping times bounded by $T$.\n\nIn the context of establishing tightness of the laws of $\\{X^n\\}$ in $D([0,T];\\mathbb{R}^d)$, it is often necessary to control both the distributions of $X^n(t)$ at fixed times and the small-time oscillations of the paths in probability. Select all options that correctly state Aldous’s tightness criterion for processes in $D([0,T];\\mathbb{R}^d)$ using bounded stopping times and small increments, and select the option that best captures its intuition. There may be more than one correct option.\n\nA. For every $t \\in [0,T]$, the family $\\{X^n(t): n \\in \\mathbb{N}\\}$ is tight in $\\mathbb{R}^d$, and for every $\\eta>0$,\n$$\n\\lim_{\\delta \\downarrow 0}\\ \\sup_{n \\in \\mathbb{N}}\\ \\sup_{\\tau \\in \\mathcal{T}_T}\\ \\sup_{0 \\le \\theta \\le \\delta}\\ \\mathbb{P}\\!\\left(\\left|X^n(\\tau+\\theta) - X^n(\\tau)\\right| \\ge \\eta\\right) = 0.\n$$\nUnder these conditions, the laws of $\\{X^n\\}$ are tight in $D([0,T];\\mathbb{R}^d)$.\n\nB. For every $\\eta>0$,\n$$\n\\lim_{\\delta \\downarrow 0}\\ \\sup_{n \\in \\mathbb{N}}\\ \\sup_{s \\in [0,T-\\delta]}\\ \\sup_{0 \\le \\theta \\le \\delta}\\ \\mathbb{P}\\!\\left(\\left|X^n(s+\\theta) - X^n(s)\\right| \\ge \\eta\\right) = 0,\n$$\nwhich alone ensures tightness in $D([0,T];\\mathbb{R}^d)$; the use of stopping times is unnecessary.\n\nC. There exists a deterministic modulus $\\omega: [0,\\infty) \\to [0,\\infty)$ with $\\lim_{\\delta \\downarrow 0} \\omega(\\delta) = 0$ such that, almost surely and uniformly in $n \\in \\mathbb{N}$,\n$$\n\\sup_{|t-s| \\le \\delta} \\left|X^n(t) - X^n(s)\\right| \\le \\omega(\\delta)\\quad \\text{for all } \\delta>0,\n$$\nand this is both necessary and sufficient for tightness in $D([0,T];\\mathbb{R}^d)$.\n\nD. Tightness of $\\{X^n\\}$ in $D([0,T];\\mathbb{R}^d)$ is equivalent to the existence of a constant $C>0$ such that\n$$\n\\sup_{n \\in \\mathbb{N}} \\mathbb{E}\\!\\left[\\sup_{t \\in [0,T]} \\left|X^n(t)\\right|^2\\right] \\le C\n\\quad \\text{and} \\quad\n\\sup_{n \\in \\mathbb{N}} \\mathbb{E}\\!\\left([X^n]_T\\right) \\le C,\n$$\nwhere $[X^n]_T$ denotes the quadratic variation of $X^n$ up to time $T$.\n\nE. Intuition: The criterion prevents jump processes by enforcing continuity of sample paths, so tightness holds only for continuous processes.\n\nF. Intuition: The criterion demands that, for arbitrarily short time windows, the processes do not make large moves with non-negligible probability when sampled at arbitrary bounded stopping times, thereby ruling out concentration of large oscillations at random times. Together with tightness of the marginals at fixed times, this prevents escape of mass and yields tightness in $D([0,T];\\mathbb{R}^d)$ via Prokhorov’s theorem.\n\nSelect all correct options.", "solution": "Aldous's criterion provides a set of sufficient conditions for the tightness of a sequence of probability laws of càdlàg processes on the Skorokhod space $D([0,T];\\mathbb{R}^d)$. Tightness of the laws is equivalent to two general conditions being met:\n1.  The family of marginal distributions $\\{X^n(t)\\}$ is tight in $\\mathbb{R}^d$ for a dense set of times $t \\in [0,T]$. This ensures the process values do not \"escape to infinity\".\n2.  A condition that controls the oscillatory behavior of the paths, ensuring that jumps or rapid oscillations do not become pathological. Aldous's criterion gives a practical way to check this.\n\nLet's analyze each option in light of these requirements.\n\n**A. Correct.** This option correctly states a version of Aldous's criterion. It combines the two necessary components:\n   -   **Marginal Tightness:** \"For every $t \\in [0,T]$, the family $\\{X^n(t): n \\in \\mathbb{N}\\}$ is tight in $\\mathbb{R}^d$.\" This is the first standard condition.\n   -   **Oscillation Control:** The limit condition, $\\lim_{\\delta \\downarrow 0}\\sup_{n \\in \\mathbb{N}}\\sup_{\\tau \\in \\mathcal{T}_T}\\sup_{0 \\le \\theta \\le \\delta} \\mathbb{P}(|X^n(\\tau+\\theta) - X^n(\\tau)| \\ge \\eta) = 0$, is the core of the criterion. The use of the supremum over all bounded stopping times ($\\tau \\in \\mathcal{T}_T$) is crucial because it prevents the possibility of rapid oscillations being concentrated at unpredictable *random* times, a type of misbehavior that checking at fixed, deterministic times would miss.\nThe conjunction of these two conditions is sufficient for tightness in $D([0,T];\\mathbb{R}^d)$.\n\n**B. Incorrect.** This option is insufficient for two reasons. First, it omits the necessary condition of marginal tightness. Second, it replaces the supremum over stopping times with a supremum over fixed, deterministic start times $s$. This is a substantially weaker condition that fails to control for oscillations at random times, which is the exact difficulty Aldous's criterion is designed to solve.\n\n**C. Incorrect.** This condition describes uniform equicontinuity for the sample paths, holding almost surely. This is a criterion for relative compactness in the space of *continuous* functions, $C([0,T];\\mathbb{R}^d)$, under the uniform norm. It is far too strong for the space $D([0,T];\\mathbb{R}^d)$, as it would rule out any process with jumps (discontinuities), which the Skorokhod space is designed to accommodate.\n\n**D. Incorrect.** These are moment conditions involving the supremum norm and the quadratic variation. While such conditions can be used to *prove* tightness for certain classes of processes (e.g., semimartingales, via criteria like Rebolledo's theorem), they are not a general criterion equivalent to tightness for all càdlàg processes. Aldous's criterion is more general and does not require the processes to be semimartingales.\n\n**E. Incorrect.** This fundamentally misinterprets the purpose of Aldous's criterion and the space $D([0,T];\\mathbb{R}^d)$. The framework is specifically designed to handle processes with jumps. The criterion does not enforce pathwise continuity; rather, it controls the \"bad behavior\" of jumps and oscillations in a way that allows the set of laws to be relatively compact.\n\n**F. Correct.** This option provides an excellent and precise summary of the intuition behind Aldous's criterion.\n- It correctly identifies that using stopping times is to \"rule out concentration of large oscillations at random times.\"\n- It correctly notes that this condition works \"together with tightness of the marginals at fixed times.\"\n- It correctly situates the criterion within the broader theory by connecting it to the prevention of \"escape of mass\" and the role of Prokhorov's theorem.\n\nTherefore, option A gives the correct mathematical formulation, and option F provides the correct intuition.", "answer": "$$\\boxed{AF}$$", "id": "3005014"}]}