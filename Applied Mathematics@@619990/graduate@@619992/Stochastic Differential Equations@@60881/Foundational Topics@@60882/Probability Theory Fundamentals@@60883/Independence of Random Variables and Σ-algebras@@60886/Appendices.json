{"hands_on_practices": [{"introduction": "Independence is a more profound concept than simple uncorrelation, requiring the factorization of probabilities for all possible events. This first practice invites you to explore this definition in a tangible scenario: the relationship between a symmetric random variable's sign, $S = \\operatorname{sign}(X)$, and its absolute value, $Y = |X|$. Through this exercise [@problem_id:2980204], you will derive a precise condition that governs their independence, revealing how a single point—the probability mass at zero—can fundamentally alter the probabilistic structure.", "problem": "Let $X$ be a real-valued random variable on a probability space $(\\Omega,\\mathcal{F},\\mathbb{P})$ whose distribution is symmetric about $0$ in the sense that $X \\stackrel{d}{=} -X$. Define $S=\\operatorname{sign}(X)$ with the convention $\\operatorname{sign}(0)=0$, and $Y=|X|$. Recall the definitions: two random variables $U$ and $V$ are independent if $\\sigma(U)$ and $\\sigma(V)$ are independent $\\sigma$-algebras, equivalently if $\\mathbb{P}(U\\in A,V\\in B)=\\mathbb{P}(U\\in A)\\mathbb{P}(V\\in B)$ for all Borel sets $A,B\\subset\\mathbb{R}$. Using only these definitions and the symmetry property $X \\stackrel{d}{=} -X$, determine necessary and sufficient conditions (in terms of the mass at $0$) under which $S$ is independent of $Y$ (equivalently, $\\sigma(S)$ is independent of $\\sigma(Y)$). Then apply your criterion to the Gaussian case and to non-Gaussian symmetric laws.\n\nWhich of the following statements are correct?\n\nA. $S$ and $Y$ are independent if and only if either $\\mathbb{P}(X=0)=0$ or $\\mathbb{P}(X=0)=1$. In particular, for any symmetric law with no atom at $0$ (discrete or continuous), $S$ and $Y$ are independent; if $0 < \\mathbb{P}(X=0) < 1$, they are not independent.\n\nB. Independence of $S$ and $Y$ characterizes the Gaussian among symmetric laws: if $X$ is symmetric and $S$ and $Y$ are independent, then $X$ must be Gaussian.\n\nC. If $X$ is symmetric and absolutely continuous with an even density $f$ and $\\mathbb{P}(X=0)=0$, then $S$ and $Y$ are independent, but if $X$ is symmetric and supported on finitely many nonzero points, then $S$ and $Y$ are never independent.\n\nD. If $X\\sim \\mathcal{N}(0,\\sigma^{2})$ with $\\sigma>0$, then $S$ and $Y$ are independent and $\\sigma(S)$ is independent of $\\sigma(Y)$. If $X$ is symmetric Bernoulli, taking values $\\{-1,+1\\}$ with probability $1/2$ each, then $S$ and $Y$ are independent.\n\nE. If $X$ has a symmetric distribution with an atom at $0$ and positive mass elsewhere, then redefining $\\operatorname{sign}(0)=+1$ makes $S$ and $Y$ independent.", "solution": "We begin from the core definitions. Independence of random variables $U$ and $V$ means $\\mathbb{P}(U\\in A, V\\in B)=\\mathbb{P}(U\\in A)\\mathbb{P}(V\\in B)$ for all Borel sets $A,B\\subset\\mathbb{R}$, which is equivalent to independence of the generated $\\sigma$-algebras $\\sigma(U)$ and $\\sigma(V)$. Symmetry $X\\stackrel{d}{=}-X$ means $\\mathbb{P}(X\\in B)=\\mathbb{P}(X\\in -B)$ for all Borel $B$, where $-B=\\{-x:x\\in B\\}$.\n\nSet $S=\\operatorname{sign}(X)$ with $\\operatorname{sign}(0)=0$, and $Y=|X|$. Let $p_{0}=\\mathbb{P}(X=0)=\\mathbb{P}(Y=0)=\\mathbb{P}(S=0)$.\n\nStep 1: A necessary condition via events away from $0$. Fix any Borel set $B\\subset(0,\\infty)$ (so $0\\notin B$). Then the events $\\{X\\in B\\}$ and $\\{X\\in -B\\}$ are disjoint, and by symmetry $\\mathbb{P}(X\\in B)=\\mathbb{P}(X\\in -B)$. Hence\n$$\n\\mathbb{P}(Y\\in B)=\\mathbb{P}(X\\in B\\cup -B)\n=\\mathbb{P}(X\\in B)+\\mathbb{P}(X\\in -B)=2\\,\\mathbb{P}(X\\in B).\n$$\nMoreover,\n$$\n\\mathbb{P}(Y\\in B, S=+1)=\\mathbb{P}(|X|\\in B, X>0)=\\mathbb{P}(X\\in B),\n$$\nsince for $B\\subset(0,\\infty)$ the intersection $\\{|X|\\in B, X>0\\}$ is exactly $\\{X\\in B\\}$. If $S$ and $Y$ are independent, then for every such $B$,\n$$\n\\mathbb{P}(X\\in B)\n=\\mathbb{P}(Y\\in B, S=+1)\n=\\mathbb{P}(Y\\in B)\\,\\mathbb{P}(S=+1).\n$$\nUsing $\\mathbb{P}(Y\\in B)=2\\,\\mathbb{P}(X\\in B)$, we get\n$$\n\\mathbb{P}(X\\in B)=2\\,\\mathbb{P}(X\\in B)\\,\\mathbb{P}(S=+1).\n$$\nIf there exists $B\\subset(0,\\infty)$ with $\\mathbb{P}(X\\in B)>0$, then we can cancel to obtain\n$$\n\\mathbb{P}(S=+1)=\\tfrac{1}{2}.\n$$\nFor a symmetric $X$, we have $\\mathbb{P}(X>0)=\\mathbb{P}(X<0)=(1-p_{0})/2$ and $\\mathbb{P}(S=+1)=\\mathbb{P}(X>0)=(1-p_{0})/2$. Thus the condition $\\mathbb{P}(S=+1)=\\tfrac{1}{2}$ is equivalent to $p_{0}=0$.\n\nTherefore, if $S$ and $Y$ are independent and there exists any positive mass away from $0$ (i.e., $\\mathbb{P}(|X|>0)>0$, equivalently $p_{0}<1$), then necessarily $p_{0}=0$.\n\nStep 2: Edge case when all mass is at $0$. If $p_{0}=1$, then $X=0$ almost surely. Consequently $S=0$ almost surely and $Y=0$ almost surely. In this degenerate case both $S$ and $Y$ are constants, and constant random variables are independent of any other random variable. Hence independence holds when $p_{0}=1$.\n\nCombining with Step 1, we have shown necessity: if $S$ and $Y$ are independent, then either $p_{0}=1$ or $p_{0}=0$.\n\nStep 3: Sufficiency when $p_{0}=0$. Suppose $p_{0}=0$. For any $B\\subset(0,\\infty)$ as above,\n$$\n\\mathbb{P}(Y\\in B, S=+1)=\\mathbb{P}(X\\in B)=\\tfrac{1}{2}\\,\\mathbb{P}(X\\in B\\cup -B)=\\tfrac{1}{2}\\,\\mathbb{P}(Y\\in B),\n$$\nwhere we used symmetry and disjointness of $B$ and $-B$. Since $\\mathbb{P}(S=+1)=\\tfrac{1}{2}$ when $p_{0}=0$, we have shown $\\mathbb{P}(Y\\in B, S=+1)=\\mathbb{P}(Y\\in B)\\mathbb{P}(S=+1)$ for all $B\\subset(0,\\infty)$. A similar computation gives $\\mathbb{P}(Y\\in B, S=-1)=\\mathbb{P}(Y\\in B)\\mathbb{P}(S=-1)$ for all $B\\subset(0,\\infty)$. For sets $B$ containing $0$ we use that $\\mathbb{P}(Y=0)=0$ in the case $p_{0}=0$, so adding or removing $\\{0\\}$ does not change probabilities. By a monotone class argument (or a standard $\\pi$-$\\lambda$ argument) these equalities extend to all Borel $B\\subset[0,\\infty)$, proving independence of $S$ and $Y$.\n\nStep 4: Sufficiency when $p_{0}=1$. As noted, if $X=0$ almost surely, then $S=0$ and $Y=0$ almost surely, so independence of $S$ and $Y$ is immediate.\n\nConclusion: For symmetric $X$, $S$ and $Y$ are independent if and only if either $p_{0}=0$ or $p_{0}=1$. Equivalently, independence fails exactly when $0<p_{0}<1$.\n\nSince independence of random variables is equivalent to independence of their generated $\\sigma$-algebras, the same criterion describes when $\\sigma(S)$ is independent of $\\sigma(Y)$.\n\nApplications:\n- Gaussian case: If $X\\sim \\mathcal{N}(0,\\sigma^{2})$ with $\\sigma>0$, then $p_{0}=0$ and $X$ is symmetric, hence $S$ and $Y$ are independent and $\\sigma(S)\\perp\\sigma(Y)$. This is a familiar special case used in the analysis of Brownian motion at fixed times $t>0$ where $B_{t}\\sim \\mathcal{N}(0,t)$.\n- Non-Gaussian examples where independence holds: symmetric Bernoulli $X\\in\\{-1,+1\\}$ with probability $1/2$ each has $p_{0}=0$. Then $Y\\equiv 1$ is constant and $S=X$, so $S$ and $Y$ are independent. Likewise, symmetric Laplace or Cauchy laws have $p_{0}=0$ and yield independence.\n- Symmetric laws with an atom at $0$ and additional mass elsewhere: if $0<p_{0}<1$ (for example, $\\mathbb{P}(X=0)=p_{0}$ and with the remaining mass split symmetrically at $\\pm 1$), then independence fails because conditioning on $Y=0$ determines $S=0$ with probability $1$, whereas $\\mathbb{P}(S=0)=p_{0}\\in(0,1)$.\n\nOption-by-option analysis:\n- Option A: This matches the derived necessary and sufficient condition. It correctly includes both the non-atomic case at $0$ (i.e., $p_{0}=0$) and the degenerate case $p_{0}=1$, and correctly states failure when $0<p_{0}<1$. Verdict: Correct.\n- Option B: False. Independence of $S$ and $Y$ does not characterize the Gaussian; many non-Gaussian symmetric laws with $\\mathbb{P}(X=0)=0$ (e.g., symmetric Bernoulli, Laplace, Cauchy) also have $S$ and $Y$ independent.\n- Option C: The first clause is true (it is a special case of the general criterion), but the second clause is false: for symmetric discrete laws supported on finitely many nonzero points (e.g., symmetric Bernoulli $\\{-1,+1\\}$), $S$ and $Y$ can be independent. Hence as a whole the statement is false. Verdict: Incorrect.\n- Option D: For $X\\sim \\mathcal{N}(0,\\sigma^{2})$ with $\\sigma>0$, we have $p_{0}=0$ and independence holds; independence of random variables implies independence of their generated $\\sigma$-algebras. For symmetric Bernoulli $\\{-1,+1\\}$, $Y\\equiv 1$ is constant and therefore independent of $S=X$. Verdict: Correct.\n- Option E: Redefining $\\operatorname{sign}(0)$ to a deterministic value (e.g., $+1$) does not restore independence when $0<p_{0}<1$, because $\\mathbb{P}(S=+1\\mid Y=0)=1$ while $\\mathbb{P}(S=+1)<1$ for a non-degenerate symmetric law. Verdict: Incorrect.", "answer": "$$\\boxed{AD}$$", "id": "2980204"}, {"introduction": "While uncorrelatedness is a necessary condition for the independence of variables with finite second moments, it is famously not sufficient. This practice provides a hands-on opportunity to construct a classic counterexample: a pair of random variables that are uncorrelated but functionally dependent [@problem_id:2980208]. By leveraging the concept of conditional independence and a latent \"switching\" variable, you will see how dependence can arise from a mixture of distributions, even when the covariance is engineered to be exactly zero.", "problem": "Consider a probability space supporting a standard Brownian motion $B=\\{B_t: t\\geq 0\\}$ and an independent discrete latent variable $S$ taking values in the set $\\{s_1,s_2,s_3\\}$ with $\\mathbb{P}(S=s_k)=\\frac{1}{3}$ for each $k\\in\\{1,2,3\\}$. Define the deterministic functions $m:\\{s_1,s_2,s_3\\}\\to\\mathbb{R}$ and $n:\\{s_1,s_2,s_3\\}\\to\\mathbb{R}$ by\n$$\nm(s_1)=1,\\quad m(s_2)=-1,\\quad m(s_3)=0,\\qquad\nn(s_1)=1,\\quad n(s_2)=1,\\quad n(s_3)=-2.\n$$\nLet the pair of random variables $(X,Y)$ be constructed by\n$$\nX := B_1 + m(S),\\qquad Y := \\big(B_2 - B_1\\big) + n(S).\n$$\nYou may use the fact that $B_1$ and $B_2-B_1$ are independent, centered Gaussian random variables, and that $S$ is independent of $B$. Using the definitions of independence of random variables and of independence of $\\sigma$-algebras, and the characterization via characteristic functions, analyze this mixture of product measures and determine which of the following statements are true.\n\nA. The random variables $X$ and $Y$ are independent.\n\nB. The covariance $\\operatorname{Cov}(X,Y)$ is equal to $0$.\n\nC. The joint characteristic function $\\varphi_{X,Y}(t,u)$ factorizes as $\\varphi_X(t)\\,\\varphi_Y(u)$ for all $t,u\\in\\mathbb{R}$.\n\nD. The random variables $X$ and $Y$ are conditionally independent given $S$.\n\nE. The $\\sigma$-algebras $\\sigma(X)$ and $\\sigma(Y)$ are independent.\n\nSelect all correct options.", "solution": "Let $Z_1 = B_1$ and $Z_2 = B_2 - B_1$. From the properties of standard Brownian motion, $Z_1$ and $Z_2$ are independent and identically distributed standard normal random variables, i.e., $Z_1, Z_2 \\sim \\mathcal{N}(0,1)$. The problem states that the discrete random variable $S$ is independent of the Brownian motion $B$, which implies that $S$ is independent of both $Z_1$ and $Z_2$. Consequently, the random variables $Z_1$, $Z_2$, and $S$ are mutually independent.\n\nThe random variables $X$ and $Y$ are defined as:\n$$\nX = Z_1 + m(S)\n$$\n$$\nY = Z_2 + n(S)\n$$\nwhere the values of the random variables $m(S)$ and $n(S)$ are determined by the state of $S$ as follows:\n- If $S=s_1$, $(m(S), n(S)) = (1, 1)$.\n- If $S=s_2$, $(m(S), n(S)) = (-1, 1)$.\n- If $S=s_3$, $(m(S), n(S)) = (0, -2)$.\nEach state occurs with probability $\\frac{1}{3}$.\n\nWe will now evaluate each statement.\n\n**D. The random variables $X$ and $Y$ are conditionally independent given $S$.**\n\nTo verify conditional independence of $X$ and $Y$ given $S$, we must show that for any value $s_k$ of $S$, the conditional distribution of the pair $(X, Y)$ is the product of their conditional marginal distributions.\n$$\n\\mathbb{P}(X \\in A, Y \\in B | S=s_k) = \\mathbb{P}(X \\in A | S=s_k) \\mathbb{P}(Y \\in B | S=s_k)\n$$\nfor all Borel sets $A, B \\subseteq \\mathbb{R}$.\n\nLet's condition on the event $S=s_k$. The random variables $X$ and $Y$ become:\n$$\nX|_{S=s_k} = Z_1 + m(s_k)\n$$\n$$\nY|_{S=s_k} = Z_2 + n(s_k)\n$$\nwhere $m(s_k)$ and $n(s_k)$ are deterministic constants for a fixed $s_k$.\nThe random variables $Z_1$ and $Z_2$ are independent. The transformation of adding a constant to each does not affect their independence. That is, for any constants $c_1, c_2$, if $Z_1$ and $Z_2$ are independent, then $Z_1+c_1$ and $Z_2+c_2$ are also independent.\nSince $Z_1$ and $Z_2$ are independent of $S$, the conditional distribution of $(Z_1, Z_2)$ given $S=s_k$ is the same as their unconditional distribution. Therefore, conditioned on $S=s_k$, $X$ and $Y$ are functions of a pair of independent random variables, $Z_1$ and $Z_2$. The random variables $X|_{S=s_k}$ and $Y|_{S=s_k}$ are thus independent for each $k \\in \\{1,2,3\\}$.\nThis confirms that $X$ and $Y$ are conditionally independent given $S$.\n\nVerdict: **Correct**.\n\n**B. The covariance $\\operatorname{Cov}(X,Y)$ is equal to $0$.**\n\nThe covariance is defined as $\\operatorname{Cov}(X,Y) = \\mathbb{E}[XY] - \\mathbb{E}[X]\\mathbb{E}[Y]$. We will compute the expectations using the law of total expectation, i.e., $\\mathbb{E}[V] = \\mathbb{E}[\\mathbb{E}[V|S]]$.\n\nFirst, we compute $\\mathbb{E}[X]$ and $\\mathbb{E}[Y]$.\nThe conditional expectation of $X$ given $S$ is:\n$$\n\\mathbb{E}[X|S] = \\mathbb{E}[Z_1 + m(S)|S] = \\mathbb{E}[Z_1|S] + \\mathbb{E}[m(S)|S]\n$$\nSince $Z_1$ is independent of $S$, $\\mathbb{E}[Z_1|S] = \\mathbb{E}[Z_1] = 0$. Given $S$, $m(S)$ is a constant, so $\\mathbb{E}[m(S)|S] = m(S)$.\nThus, $\\mathbb{E}[X|S] = m(S)$.\nTaking the expectation over $S$:\n$$\n\\mathbb{E}[X] = \\mathbb{E}[m(S)] = \\sum_{k=1}^3 m(s_k) \\mathbb{P}(S=s_k) = \\frac{1}{3}(1) + \\frac{1}{3}(-1) + \\frac{1}{3}(0) = 0.\n$$\nSimilarly for $Y$:\n$$\n\\mathbb{E}[Y|S] = \\mathbb{E}[Z_2 + n(S)|S] = \\mathbb{E}[Z_2|S] + n(S) = 0 + n(S) = n(S).\n$$\n$$\n\\mathbb{E}[Y] = \\mathbb{E}[n(S)] = \\sum_{k=1}^3 n(s_k) \\mathbb{P}(S=s_k) = \\frac{1}{3}(1) + \\frac{1}{3}(1) + \\frac{1}{3}(-2) = 0.\n$$\nNow we compute $\\mathbb{E}[XY]$.\n$$\n\\mathbb{E}[XY] = \\mathbb{E}[\\mathbb{E}[XY|S]].\n$$\nThe conditional expectation is:\n$$\n\\mathbb{E}[XY|S] = \\mathbb{E}[(Z_1+m(S))(Z_2+n(S))|S].\n$$\nGiven $S=s_k$, $X$ and $Y$ are independent, as shown for statement D. Therefore, $\\mathbb{E}[XY|S=s_k] = \\mathbb{E}[X|S=s_k]\\mathbb{E}[Y|S=s_k]$.\nWe found that $\\mathbb{E}[X|S] = m(S)$ and $\\mathbb{E}[Y|S] = n(S)$. So, $\\mathbb{E}[XY|S] = m(S)n(S)$.\nTaking the expectation over $S$:\n$$\n\\mathbb{E}[XY] = \\mathbb{E}[m(S)n(S)] = \\sum_{k=1}^3 m(s_k)n(s_k)\\mathbb{P}(S=s_k).\n$$\n$$\n\\mathbb{E}[m(S)n(S)] = \\frac{1}{3} [ (1)(1) + (-1)(1) + (0)(-2) ] = \\frac{1}{3} [1 - 1 + 0] = 0.\n$$\nFinally, the covariance is:\n$$\n\\operatorname{Cov}(X,Y) = \\mathbb{E}[XY] - \\mathbb{E}[X]\\mathbb{E}[Y] = 0 - (0)(0) = 0.\n$$\n\nVerdict: **Correct**.\n\n**A. The random variables $X$ and $Y$ are independent.**\n**C. The joint characteristic function $\\varphi_{X,Y}(t,u)$ factorizes as $\\varphi_X(t)\\,\\varphi_Y(u)$ for all $t,u\\in\\mathbb{R}$.**\n**E. The $\\sigma$-algebras $\\sigma(X)$ and $\\sigma(Y)$ are independent.**\n\nThese three statements are equivalent characterizations of the independence of two random variables $X$ and $Y$. If one is true, all are true. If one is false, all are false. We will test for independence using characteristic functions. If $X$ and $Y$ are independent, their joint characteristic function must factor into the product of their marginal characteristic functions.\n\nThe joint characteristic function is $\\varphi_{X,Y}(t,u) = \\mathbb{E}[e^{i(tX+uY)}]$. Using the law of total expectation:\n$$\n\\varphi_{X,Y}(t,u) = \\mathbb{E}[\\mathbb{E}[e^{i(tX+uY)}|S]].\n$$\nConditioned on $S=s_k$, $X$ and $Y$ are independent. Thus, the conditional characteristic function factorizes:\n$$\n\\mathbb{E}[e^{i(tX+uY)}|S=s_k] = \\mathbb{E}[e^{itX}|S=s_k]\\mathbb{E}[e^{iuY}|S=s_k].\n$$\n$\\mathbb{E}[e^{itX}|S=s_k] = \\mathbb{E}[e^{it(Z_1+m(s_k))}] = e^{itm(s_k)}\\mathbb{E}[e^{itZ_1}] = e^{itm(s_k)}e^{-t^2/2}$, since $Z_1 \\sim \\mathcal{N}(0,1)$.\n$\\mathbb{E}[e^{iuY}|S=s_k] = \\mathbb{E}[e^{iu(Z_2+n(s_k))}] = e^{iun(s_k)}\\mathbb{E}[e^{iuZ_2}] = e^{iun(s_k)}e^{-u^2/2}$, since $Z_2 \\sim \\mathcal{N}(0,1)$.\nSo, $\\mathbb{E}[e^{i(tX+uY)}|S=s_k] = e^{itm(s_k)}e^{-t^2/2} \\cdot e^{iun(s_k)}e^{-u^2/2} = e^{-(t^2+u^2)/2} e^{i(tm(s_k)+un(s_k))}$.\nTaking expectation over $S$:\n$$\n\\varphi_{X,Y}(t,u) = e^{-(t^2+u^2)/2} \\mathbb{E}[e^{i(tm(S)+un(S))}].\n$$\n$$\n\\mathbb{E}[e^{i(tm(S)+un(S))}] = \\frac{1}{3}\\sum_{k=1}^3 e^{i(tm(s_k)+un(s_k))} = \\frac{1}{3}(e^{i(t+u)} + e^{i(-t+u)} + e^{i(-2u)}).\n$$\n$$\n\\varphi_{X,Y}(t,u) = \\frac{1}{3} e^{-(t^2+u^2)/2} (e^{i(t+u)} + e^{i(-t+u)} + e^{-i2u}) = \\frac{1}{3} e^{-(t^2+u^2)/2} (2e^{iu}\\cos(t) + e^{-i2u}).\n$$\nNow we find the marginal characteristic functions:\n$$\n\\varphi_X(t) = \\varphi_{X,Y}(t,0) = \\frac{1}{3} e^{-t^2/2} (e^{it} + e^{-it} + e^0) = \\frac{1}{3} e^{-t^2/2} (2\\cos(t) + 1).\n$$\n$$\n\\varphi_Y(u) = \\varphi_{X,Y}(0,u) = \\frac{1}{3} e^{-u^2/2} (e^{iu} + e^{iu} + e^{-i2u}) = \\frac{1}{3} e^{-u^2/2} (2e^{iu} + e^{-i2u}).\n$$\nFor independence, we must have $\\varphi_{X,Y}(t,u) = \\varphi_X(t)\\varphi_Y(u)$.\n$$\n\\varphi_X(t)\\varphi_Y(u) = \\left(\\frac{1}{3} e^{-t^2/2} (2\\cos(t) + 1)\\right) \\left(\\frac{1}{3} e^{-u^2/2} (2e^{iu} + e^{-i2u})\\right)\n$$\n$$\n= \\frac{1}{9} e^{-(t^2+u^2)/2} (2\\cos(t) + 1)(2e^{iu} + e^{-i2u}).\n$$\nWe check if $\\frac{1}{3}(2e^{iu}\\cos(t) + e^{-i2u}) = \\frac{1}{9}(2\\cos(t) + 1)(2e^{iu} + e^{-i2u})$.\nLet's test this for $t=\\pi/2$, so $\\cos(t)=0$.\nLHS: $\\frac{1}{3}(e^{-i2u})$.\nRHS: $\\frac{1}{9}(1)(2e^{iu} + e^{-i2u})$.\nThe equality $\\frac{1}{3}e^{-i2u} = \\frac{1}{9}(2e^{iu} + e^{-i2u})$ is clearly not true for all $u$. For example, for $u=\\pi/2$. LHS: $\\frac{1}{3} e^{-i\\pi} = -1/3$. RHS: $\\frac{1}{9}(2e^{i\\pi/2} + e^{-i\\pi}) = \\frac{1}{9}(2i-1)$. These are not equal.\nSince the identity does not hold for all $(t,u)$, the joint characteristic function does not factorize.\n\nVerdict on C: **Incorrect**.\nSince factorization of the characteristic function is a necessary and sufficient condition for independence, its failure implies that $X$ and $Y$ are not independent.\nVerdict on A: **Incorrect**.\nThe independence of the generated $\\sigma$-algebras is equivalent to the independence of the random variables.\nVerdict on E: **Incorrect**.\n\nIn summary, $X$ and $Y$ are constructed in such a way that they are uncorrelated but not independent. The dependence is introduced by the common latent variable $S$, but the parameters are chosen to make the covariance vanish.", "answer": "$$\\boxed{BD}$$", "id": "2980208"}, {"introduction": "How does independence fare under linear transformations? This final practice investigates this crucial question, which has deep implications for everything from signal processing to portfolio theory. By analyzing when the sum $S=X+Y$ and difference $D=X-Y$ of two independent random variables are themselves independent, you will uncover a remarkable property that is characteristic of the Gaussian distribution [@problem_id:2980206]. This exercise connects a practical question about variable transformations to the elegant geometric concept of a \"spherical\" probability distribution.", "problem": "Consider a filtered probability space supporting two independent standard Brownian motions $\\{W^{(1)}_t\\}_{t \\ge 0}$ and $\\{W^{(2)}_t\\}_{t \\ge 0}$, and fix a deterministic time $T > 0$. Define two scalar stochastic differential equations (SDEs) with purely diffusive dynamics by\n$$\ndX_t = \\sigma_1\\, dW^{(1)}_t,\\quad X_0 = 0,\\qquad dY_t = \\sigma_2\\, dW^{(2)}_t,\\quad Y_0 = 0,\n$$\nwhere $\\sigma_1, \\sigma_2 > 0$ are fixed constants. Set $X := X_T$ and $Y := Y_T$, so that $X$ and $Y$ are centered random variables adapted to disjoint driving noises. Consider the linear combinations $S := X + Y$ and $D := X - Y$ and the $\\sigma$-algebras $\\sigma(X)$, $\\sigma(Y)$, $\\sigma(S)$, and $\\sigma(D)$ they generate. Which of the following statements are correct?\n\nA. The pair $(X,Y)$ consists of independent centered Gaussian random variables, but $S$ and $D$ are dependent whenever $\\sigma_1 \\neq \\sigma_2$. This dependence arises because $(S,D)$ is obtained by an orthogonal mixing of $(X,Y)$, and orthogonal transforms preserve independence only when the joint law of $(X,Y)$ is spherical.\n\nB. For any independent mean-zero random variables $X$ and $Y$ with finite variances, the orthogonal mixing into $S = X + Y$ and $D = X - Y$ preserves independence, so $S$ and $D$ are always independent.\n\nC. If $X$ and $Y$ are independent and identically distributed centered Laplace random variables with common scale parameter $b > 0$, then $S$ and $D$ are independent due to the symmetry of the Laplace law.\n\nD. Independence of the $\\sigma$-algebras $\\sigma(X)$ and $\\sigma(Y)$ implies independence of $\\sigma(S)$ and $\\sigma(D)$ under any invertible linear transformation from $(X,Y)$ to $(S,D)$.\n\nE. If $(X,Y)$ are independent centered Gaussian random variables with $\\operatorname{Var}(X) = \\operatorname{Var}(Y)$, then $S$ and $D$ are independent because the joint law of $(X,Y)$ is spherical and orthogonal transforms preserve that structure.", "solution": "The solutions to the given stochastic differential equations are $X_t = \\sigma_1 W^{(1)}_t$ and $Y_t = \\sigma_2 W^{(2)}_t$. At the fixed time $T$, the random variables are $X = X_T = \\sigma_1 W^{(1)}_T$ and $Y = Y_T = \\sigma_2 W^{(2)}_T$. Since $W^{(1)}_T$ and $W^{(2)}_T$ are independent draws from a normal distribution $\\mathcal{N}(0, T)$, it follows that $X \\sim \\mathcal{N}(0, \\sigma_1^2 T)$ and $Y \\sim \\mathcal{N}(0, \\sigma_2^2 T)$. Due to the independence of the driving Brownian motions, $X$ and $Y$ are independent random variables. The problem investigates the independence of the linear combinations $S = X + Y$ and $D = X - Y$.\n\nThe vector $(X, Y)$ is a centered bivariate Gaussian random vector. The variables $S$ and $D$ are defined by a linear transformation:\n$$\n\\begin{pmatrix} S \\\\ D \\end{pmatrix} = \\begin{pmatrix} 1 & 1 \\\\ 1 & -1 \\end{pmatrix} \\begin{pmatrix} X \\\\ Y \\end{pmatrix}\n$$\nA linear transformation of a Gaussian random vector results in another Gaussian random vector. Thus, $(S, D)$ is also a centered bivariate Gaussian random vector. For such a vector, the components are independent if and only if their covariance is zero. We compute the covariance of $S$ and $D$:\n$$\n\\operatorname{Cov}(S, D) = \\mathbb{E}[S \\cdot D] - \\mathbb{E}[S]\\mathbb{E}[D]\n$$\nSince $X$ and $Y$ are centered, $\\mathbb{E}[X] = 0$ and $\\mathbb{E}[Y] = 0$. It follows that $\\mathbb{E}[S] = \\mathbb{E}[X+Y] = 0$ and $\\mathbb{E}[D] = \\mathbb{E}[X-Y] = 0$. The covariance simplifies to:\n$$\n\\operatorname{Cov}(S, D) = \\mathbb{E}[S \\cdot D] = \\mathbb{E}[(X+Y)(X-Y)] = \\mathbb{E}[X^2 - Y^2]\n$$\nBy linearity of expectation, and since the variables are centered:\n$$\n\\operatorname{Cov}(S, D) = \\mathbb{E}[X^2] - \\mathbb{E}[Y^2] = \\operatorname{Var}(X) - \\operatorname{Var}(Y)\n$$\nWe know that $\\operatorname{Var}(X) = \\sigma_1^2 T$ and $\\operatorname{Var}(Y) = \\sigma_2^2 T$. Substituting these:\n$$\n\\operatorname{Cov}(S, D) = \\sigma_1^2 T - \\sigma_2^2 T = T(\\sigma_1^2 - \\sigma_2^2)\n$$\nSince $T > 0$, the covariance is zero if and only if $\\sigma_1^2 = \\sigma_2^2$. Given that $\\sigma_1, \\sigma_2 > 0$, this is equivalent to $\\sigma_1 = \\sigma_2$. Therefore, $S$ and $D$ are independent if and only if $\\operatorname{Var}(X) = \\operatorname{Var}(Y)$.\n\n### Option-by-Option Analysis\n\n**A. The pair $(X,Y)$ consists of independent centered Gaussian random variables, but $S$ and $D$ are dependent whenever $\\sigma_1 \\neq \\sigma_2$. This dependence arises because $(S,D)$ is obtained by an orthogonal mixing of $(X,Y)$, and orthogonal transforms preserve independence only when the joint law of $(X,Y)$ is spherical.**\n\n-   The claims that $(X,Y)$ are independent centered Gaussians and that $S, D$ are dependent whenever $\\sigma_1 \\neq \\sigma_2$ are correct, as shown above.\n-   The reasoning is sound. The transformation matrix is orthogonal up to a scaling factor. The joint probability density of $(X,Y)$ is proportional to $\\exp\\left(-\\frac{1}{2T} \\left(\\frac{x^2}{\\sigma_1^2} + \\frac{y^2}{\\sigma_2^2}\\right)\\right)$. Its level sets are ellipses, which are circles (defining a spherical distribution) only if $\\sigma_1 = \\sigma_2$. An orthogonal transform preserves independence of the components of a Gaussian vector if and only if the original joint distribution is spherical.\n-   Verdict: **Correct**.\n\n**B. For any independent mean-zero random variables $X$ and $Y$ with finite variances, the orthogonal mixing into $S = X + Y$ and $D = X - Y$ preserves independence, so $S$ and $D$ are always independent.**\n\n-   This is false. A necessary condition for independence is zero covariance. As shown, $\\operatorname{Cov}(S,D) = \\operatorname{Var}(X) - \\operatorname{Var}(Y)$. If we take any two independent, mean-zero variables with unequal variances, $S$ and $D$ will be correlated and thus dependent. The statement is a vast overgeneralization.\n-   Verdict: **Incorrect**.\n\n**C. If $X$ and $Y$ are independent and identically distributed centered Laplace random variables with common scale parameter $b > 0$, then $S$ and $D$ are independent due to the symmetry of the Laplace law.**\n\n-   Let $X, Y \\sim \\text{Laplace}(0,b)$, i.i.d. Since they are i.i.d., $\\operatorname{Var}(X) = \\operatorname{Var}(Y)$, so $\\operatorname{Cov}(S,D) = 0$. $S$ and $D$ are uncorrelated. However, for non-Gaussian variables, uncorrelation does not imply independence. We check the joint characteristic function $\\phi_{S,D}(t_1, t_2) = E[e^{i(t_1 S + t_2 D)}]$. The characteristic function for a $\\text{Laplace}(0,b)$ variable is $\\phi(t) = (1+b^2t^2)^{-1}$.\n-   $\\phi_{S,D}(t_1, t_2) = \\mathbb{E}[e^{i((t_1+t_2)X + (t_1-t_2)Y)}] = \\phi(t_1+t_2) \\phi(t_1-t_2) = \\frac{1}{(1+b^2(t_1+t_2)^2)(1+b^2(t_1-t_2)^2)}$.\n-   The marginal characteristic functions are $\\phi_S(t_1) = \\phi_{S,D}(t_1, 0) = (\\phi(t_1))^2 = (1+b^2t_1^2)^{-2}$ and $\\phi_D(t_2) = \\phi_{S,D}(0, t_2) = (\\phi(t_2))^2 = (1+b^2t_2^2)^{-2}$.\n-   The product $\\phi_S(t_1)\\phi_D(t_2) = (1+b^2t_1^2)^{-2}(1+b^2t_2^2)^{-2}$ is not equal to $\\phi_{S,D}(t_1, t_2)$. Thus, $S$ and $D$ are not independent. The Bernstein-Skitovich-Darmois theorem implies that this property is characteristic of the Gaussian distribution.\n-   Verdict: **Incorrect**.\n\n**D. Independence of the $\\sigma$-algebras $\\sigma(X)$ and $\\sigma(Y)$ implies independence of $\\sigma(S)$ and $\\sigma(D)$ under any invertible linear transformation from $(X,Y)$ to $(S,D)$.**\n\n-   This is false. The problem setup itself provides a counterexample. When $\\sigma_1 \\neq \\sigma_2$, $X$ and $Y$ are independent, but their sum $S$ and difference $D$ are not. The transformation is invertible.\n-   Verdict: **Incorrect**.\n\n**E. If $(X,Y)$ are independent centered Gaussian random variables with $\\operatorname{Var}(X) = \\operatorname{Var}(Y)$, then $S$ and $D$ are independent because the joint law of $(X,Y)$ is spherical and orthogonal transforms preserve that structure.**\n\n-   This corresponds to the case $\\sigma_1 = \\sigma_2$. Our derivation showed that $\\operatorname{Cov}(S,D) = 0$, which for a jointly Gaussian vector $(S,D)$ implies independence.\n-   The reasoning provided is correct and gives the geometric intuition. If $X, Y$ are i.i.d. centered Gaussians, their joint density is radially symmetric (spherical). The transformation to $(S,D)$ is an orthogonal transformation (up to scaling), which preserves the spherical symmetry of the distribution. For a jointly Gaussian vector, this implies the components are independent.\n-   Verdict: **Correct**.", "answer": "$$\\boxed{AE}$$", "id": "2980206"}]}