{"hands_on_practices": [{"introduction": "The Strong Law of Large Numbers (SLLN) is a cornerstone of probability theory, but its most common form relies on the strict assumption of independent and identically distributed (i.i.d.) variables. This exercise guides you through a more general setting, exploring how Kolmogorov's SLLN extends this powerful result to sequences that are independent but not identically distributed [@problem_id:2984549]. By constructing a sequence from an Itô integral with time-varying volatility, you will gain practical skill in verifying the crucial variance summability condition, $\\sum_{k=1}^{\\infty} \\frac{\\mathrm{Var}(X_{k})}{k^{2}}  \\infty$, and see firsthand how the law of large numbers holds even when the underlying distributions change.", "problem": "Fix a parameter $\\delta>0$. Let $(W_{t})_{t\\geq 0}$ be a standard Brownian motion and define a deterministic function $\\sigma(t)$ by $\\sigma(t)=k^{-(1+\\delta)/2}$ for $t\\in[k-1,k)$ and $k\\in\\mathbb{N}$. Consider the Itô stochastic differential equation $dY_{t}=\\sigma(t)\\,dW_{t}$ with $Y_{0}=0$, so that $Y_{t}=\\int_{0}^{t}\\sigma(s)\\,dW_{s}$. For each integer $k\\geq 1$, define the sequence of random variables\n$$\nX_{k}:=\\int_{k-1}^{k}\\sigma(t)\\,dW_{t}.\n$$\nUsing only fundamental properties of Brownian motion and the Itô integral, perform the following tasks:\n- Compute $\\mathrm{Var}(X_{k})$.\n- Justify that $(X_{k})_{k\\geq 1}$ is independent and non-identically distributed.\n- Verify the variance series condition $\\sum_{k=1}^{\\infty}\\mathrm{Var}(X_{k})/k^{2}\\infty$.\n- Deduce, from a well-tested limit theorem appropriate for independent sequences with finite variances, the Strong Law of Large Numbers (SLLN) for the centered averages $\\frac{1}{n}\\sum_{k=1}^{n}\\big(X_{k}-\\mathbb{E}[X_{k}]\\big)$.\n\nFinally, compute the almost sure limit of the empirical average\n$$\n\\frac{1}{n}\\sum_{k=1}^{n}X_{k}\n$$\nas $n\\to\\infty$. Your final answer must be a single real number. No rounding is required.", "solution": "The solution proceeds by addressing each task in the order presented.\n\n**1. Compute $\\mathrm{Var}(X_{k})$**\n\nThe random variable $X_k$ is defined as the Itô integral $X_{k} = \\int_{k-1}^{k}\\sigma(t)\\,dW_{t}$. Since the integrand $\\sigma(t)$ is a deterministic function, $X_k$ is a normally distributed random variable. The mean of such an integral is zero:\n$$\n\\mathbb{E}[X_{k}] = \\mathbb{E}\\left[\\int_{k-1}^{k}\\sigma(t)\\,dW_{t}\\right] = 0.\n$$\nThe variance is given by the Itô isometry property:\n$$\n\\mathrm{Var}(X_{k}) = \\mathbb{E}[X_{k}^{2}] - (\\mathbb{E}[X_{k}])^{2} = \\mathbb{E}\\left[\\left(\\int_{k-1}^{k}\\sigma(t)\\,dW_{t}\\right)^{2}\\right] = \\mathbb{E}\\left[\\int_{k-1}^{k}\\sigma(t)^{2}\\,dt\\right].\n$$\nSince $\\sigma(t)$ is deterministic, the expectation operator can be removed, yielding:\n$$\n\\mathrm{Var}(X_{k}) = \\int_{k-1}^{k}\\sigma(t)^{2}\\,dt.\n$$\nFor $t \\in [k-1, k)$, the function $\\sigma(t)$ is constant and equal to $k^{-(1+\\delta)/2}$. Substituting this into the integral:\n$$\n\\mathrm{Var}(X_{k}) = \\int_{k-1}^{k} \\left(k^{-(1+\\delta)/2}\\right)^{2}\\,dt = \\int_{k-1}^{k} k^{-(1+\\delta)}\\,dt.\n$$\nThe integrand $k^{-(1+\\delta)}$ is constant with respect to the integration variable $t$. Therefore, we have:\n$$\n\\mathrm{Var}(X_{k}) = k^{-(1+\\delta)} \\int_{k-1}^{k} 1\\,dt = k^{-(1+\\delta)} [t]_{k-1}^{k} = k^{-(1+\\delta)} (k - (k-1)) = k^{-(1+\\delta)}.\n$$\n\n**2. Justify that $(X_{k})_{k\\geq 1}$ is independent and non-identically distributed**\n\n- **Independence**: The random variable $X_k$ is defined by the Itô integral over the time interval $[k-1, k)$. It is thus a functional of the increments of the Brownian motion $(W_t)$ over this interval. A fundamental property of standard Brownian motion is that its increments over disjoint time intervals are independent. For any two distinct positive integers $k$ and $j$, the intervals $[k-1, k)$ and $[j-1, j)$ are disjoint. Consequently, the random variables $X_k = \\int_{k-1}^{k}\\sigma(t)\\,dW_{t}$ and $X_j = \\int_{j-1}^{j}\\sigma(t)\\,dW_{t}$ are independent. This holds for the entire sequence $(X_k)_{k\\ge 1}$.\n\n- **Non-identically distributed**: For random variables to be identically distributed, they must have the same probability distribution function. Since each $X_k$ is a mean-zero normal random variable, i.e., $X_k \\sim \\mathcal{N}(0, \\mathrm{Var}(X_k))$, their distributions are fully characterized by their variances. We computed $\\mathrm{Var}(X_{k}) = k^{-(1+\\delta)}$. Since the problem states $\\delta > 0$, the exponent $-(1+\\delta)$ is strictly negative. The function $f(k) = k^{-(1+\\delta)}$ is a strictly decreasing function of $k$ for $k \\geq 1$. Therefore, for any $k \\neq j$, we have $\\mathrm{Var}(X_k) \\neq \\mathrm{Var}(X_j)$. Since their variances are different, the random variables $X_k$ are not identically distributed.\n\n**3. Verify the variance series condition $\\sum_{k=1}^{\\infty}\\mathrm{Var}(X_{k})/k^{2}\\infty$**\n\nWe need to check the convergence of the series $\\sum_{k=1}^{\\infty} \\frac{\\mathrm{Var}(X_k)}{k^2}$. Substituting the expression for the variance:\n$$\n\\sum_{k=1}^{\\infty} \\frac{\\mathrm{Var}(X_{k})}{k^{2}} = \\sum_{k=1}^{\\infty} \\frac{k^{-(1+\\delta)}}{k^{2}} = \\sum_{k=1}^{\\infty} \\frac{1}{k^{1+\\delta}k^{2}} = \\sum_{k=1}^{\\infty} \\frac{1}{k^{3+\\delta}}.\n$$\nThis is a p-series of the form $\\sum_{k=1}^{\\infty} 1/k^p$ with $p = 3+\\delta$. A p-series converges if and only if $p > 1$. Given that $\\delta > 0$, we have $p = 3+\\delta > 3$. Since $p > 1$, the series converges. Thus, the condition is verified.\n\n**4. Deduce the Strong Law of Large Numbers (SLLN) for the centered averages**\n\nThe appropriate limit theorem for a sequence of independent, but not necessarily identically distributed, random variables is Kolmogorov's Strong Law of Large Numbers. One version of this theorem states that if $(X_k)_{k \\ge 1}$ is a sequence of independent random variables with finite variances such that $\\sum_{k=1}^{\\infty} \\frac{\\mathrm{Var}(X_k)}{k^2}  \\infty$, then the centered average converges almost surely to zero:\n$$\n\\frac{1}{n}\\sum_{k=1}^{n}\\big(X_{k}-\\mathbb{E}[X_{k}]\\big) \\to 0 \\quad \\text{as } n\\to\\infty, \\text{ almost surely.}\n$$\nWe have already established that the conditions for this theorem are satisfied:\n- The sequence $(X_k)_{k\\ge 1}$ is independent.\n- Each $X_k$ has a finite variance $\\mathrm{Var}(X_k) = k^{-(1+\\delta)}$.\n- The condition $\\sum_{k=1}^{\\infty} \\mathrm{Var}(X_k)/k^2  \\infty$ holds.\n\nTherefore, we can directly apply Kolmogorov's SLLN to conclude that $\\frac{1}{n}\\sum_{k=1}^{n}\\big(X_{k}-\\mathbb{E}[X_{k}]\\big)$ converges to $0$ almost surely.\n\n**5. Compute the almost sure limit of the empirical average**\n\nThe empirical average is given by $\\frac{1}{n}\\sum_{k=1}^{n}X_{k}$. The centered average is $\\frac{1}{n}\\sum_{k=1}^{n}\\big(X_{k}-\\mathbb{E}[X_{k}]\\big)$. As established in the first step, $\\mathbb{E}[X_k] = 0$ for all $k \\geq 1$. Therefore, the empirical average is identical to the centered average:\n$$\n\\frac{1}{n}\\sum_{k=1}^{n}X_{k} = \\frac{1}{n}\\sum_{k=1}^{n}\\big(X_{k}-0\\big) = \\frac{1}{n}\\sum_{k=1}^{n}\\big(X_{k}-\\mathbb{E}[X_{k}]\\big).\n$$\nFrom the application of Kolmogorov's SLLN in the previous step, we know the almost sure limit of this expression as $n \\to \\infty$.\n$$\n\\lim_{n\\to\\infty} \\frac{1}{n}\\sum_{k=1}^{n}X_{k} = 0 \\quad \\text{almost surely.}\n$$\nThe value of the almost sure limit is $0$.", "answer": "$$\n\\boxed{0}\n$$", "id": "2984549"}, {"introduction": "What are the absolute minimal requirements for the Strong Law of Large Numbers to hold? This problem pushes the boundaries of the classical theorems by constructing a sequence of random variables that are merely pairwise independent, not jointly independent, and possess infinite variance [@problem_id:2984548]. You will apply Etemadi's SLLN, a powerful generalization that requires only pairwise independence, demonstrating that statistical regularity can emerge from surprisingly weak assumptions. This practice is crucial for understanding the robustness of limit theorems and the subtle but important distinction between different levels of statistical independence.", "problem": "Let $\\{R_{k,j}: k \\in \\mathbb{N},\\, j \\in \\{1,2,3\\}\\}$ be an array of independent and identically distributed nonnegative random variables with density\n$$\nf_{R}(r) \\;=\\; c_{R}\\,\\frac{\\mathbf{1}_{\\{r \\ge e\\}}}{r^{2}\\,(\\ln r)^{2}},\n\\qquad r \\in \\mathbb{R},\n$$\nwhere $c_{R} \\,=\\, \\left(\\displaystyle\\int_{e}^{\\infty} \\frac{1}{r^{2} (\\ln r)^{2}}\\,dr\\right)^{-1}$ is the normalizing constant, and $\\mathbf{1}_{A}$ denotes the indicator of the event $A$. In addition, let $\\{\\varepsilon_{k,1}, \\varepsilon_{k,2}: k \\in \\mathbb{N}\\}$ be independent Rademacher signs, that is, $\\mathbb{P}(\\varepsilon_{k,i}=1)=\\mathbb{P}(\\varepsilon_{k,i}=-1)=\\frac{1}{2}$ for each $k \\in \\mathbb{N}$ and $i \\in \\{1,2\\}$, independent of the entire family $\\{R_{k,j}\\}$. For each $k \\in \\mathbb{N}$, define a block of three real-valued random variables\n$$\nX_{3k-2} \\,=\\, R_{k,1}\\,\\varepsilon_{k,1}, \n\\qquad\nX_{3k-1} \\,=\\, R_{k,2}\\,\\varepsilon_{k,2}, \n\\qquad\nX_{3k} \\,=\\, R_{k,3}\\,\\varepsilon_{k,1}\\,\\varepsilon_{k,2}.\n$$\nLet $S_{n} \\,=\\, \\sum_{m=1}^{n} X_{m}$, $n \\in \\mathbb{N}$.\n\nTasks:\n- Using only core definitions of independence and standard facts from probability theory, establish that the sequence $\\{X_{n}\\}_{n \\ge 1}$ is pairwise independent but not jointly independent.\n- Show from first principles that $\\mathbb{E}[|X_{1}|]  \\infty$ while $\\mathbb{E}[X_{1}^{2}]=\\infty$ (hence $\\operatorname{Var}(X_{1})=\\infty$).\n- Invoke the Strong Law of Large Numbers due to Etemadi for pairwise independent identically distributed integrable random variables to conclude a.s. convergence of $\\frac{S_{n}}{n}$. Explain why this conclusion does not follow from the classical variance-summability approach of Kolmogorov, even if one attempted to relax the independence requirement.\n- Finally, compute the almost sure limit\n$$\nL \\;=\\; \\lim_{n \\to \\infty} \\frac{S_{n}}{n}.\n$$\n\nExpress your final answer as an exact value (no rounding).", "solution": "### Part 1: Pairwise vs. Joint Independence\n\nWe are asked to establish that the sequence $\\{X_{n}\\}_{n \\ge 1}$ is pairwise independent but not jointly independent. The random variables are defined in blocks of three for each $k \\in \\mathbb{N}$:\n$X_{3k-2} = R_{k,1}\\,\\varepsilon_{k,1}$, $X_{3k-1} = R_{k,2}\\,\\varepsilon_{k,2}$, and $X_{3k} = R_{k,3}\\,\\varepsilon_{k,1}\\,\\varepsilon_{k,2}$.\nThe families $\\{R_{k,j}\\}$ and $\\{\\varepsilon_{k,i}\\}$ are mutually independent.\n\n**Pairwise Independence:**\nWe need to show that for any $n \\neq m$, $X_n$ and $X_m$ are independent. We consider two main cases.\n\nCase 1: $X_n$ and $X_m$ belong to different blocks.\nLet $n$ be an index in block $k$ (i.e., $n \\in \\{3k-2, 3k-1, 3k\\}$) and $m$ be an index in block $l$ (i.e., $m \\in \\{3l-2, 3l-1, 3l\\}$), with $k \\neq l$.\n$X_n$ is a function of the random variables $\\{R_{k,j}, \\varepsilon_{k,i}\\}$ for some $j \\in \\{1,2,3\\}$ and $i \\in \\{1,2\\}$.\n$X_m$ is a function of the random variables $\\{R_{l,j'}, \\varepsilon_{l,i'}\\}$ for some $j' \\in \\{1,2,3\\}$ and $i' \\in \\{1,2\\}$.\nSince $k \\neq l$, the set of underlying random variables defining $X_n$ is disjoint from the set defining $X_m$. By the initial assumption that all $\\{R_{k,j}\\}$ and $\\{\\varepsilon_{k,i}\\}$ are mutually independent, any function of the variables from block $k$ is independent of any function of the variables from block $l$. Thus, $X_n$ and $X_m$ are independent.\n\nCase 2: $X_n$ and $X_m$ belong to the same block $k$.\nThere are three pairs to check: $(X_{3k-2}, X_{3k-1})$, $(X_{3k-2}, X_{3k})$, and $(X_{3k-1}, X_{3k})$.\n\n(a) Pair $(X_{3k-2}, X_{3k-1})$:\n$X_{3k-2} = R_{k,1}\\,\\varepsilon_{k,1}$ and $X_{3k-1} = R_{k,2}\\,\\varepsilon_{k,2}$. The set of underlying variables is $\\{R_{k,1}, \\varepsilon_{k,1}\\}$ for $X_{3k-2}$ and $\\{R_{k,2}, \\varepsilon_{k,2}\\}$ for $X_{3k-1}$. These four variables are mutually independent. Therefore, $X_{3k-2}$ and $X_{3k-1}$ are independent.\n\n(b) Pair $(X_{3k-2}, X_{3k})$:\n$X_{3k-2} = R_{k,1}\\,\\varepsilon_{k,1}$ and $X_{3k} = R_{k,3}\\,\\varepsilon_{k,1}\\,\\varepsilon_{k,2}$. These variables share the sign $\\varepsilon_{k,1}$. We use characteristic functions to prove independence. Let $\\phi_{Y}(t) = \\mathbb{E}[\\exp(itY)]$ be the characteristic function of a random variable $Y$. The joint characteristic function of $(X_{3k-2}, X_{3k})$ is\n$$\n\\phi(t_1, t_2) = \\mathbb{E}\\left[\\exp\\left(i(t_1 X_{3k-2} + t_2 X_{3k})\\right)\\right] = \\mathbb{E}\\left[\\exp\\left(i (t_1 R_{k,1}\\varepsilon_{k,1} + t_2 R_{k,3}\\varepsilon_{k,1}\\varepsilon_{k,2})\\right)\\right].\n$$\nWe condition on the outcomes of $(\\varepsilon_{k,1}, \\varepsilon_{k,2})$, which occur with probability $1/4$ each:\n\\begin{align*}\n\\phi(t_1, t_2) = \\frac{1}{4} \\Big( \\mathbb{E}\\left[\\exp\\left(i (t_1 R_{k,1} + t_2 R_{k,3})\\right)\\right]  (\\text{for } \\varepsilon_{k,1}=1, \\varepsilon_{k,2}=1) \\\\\n+ \\mathbb{E}\\left[\\exp\\left(i (t_1 R_{k,1} - t_2 R_{k,3})\\right)\\right]  (\\text{for } \\varepsilon_{k,1}=1, \\varepsilon_{k,2}=-1) \\\\\n+ \\mathbb{E}\\left[\\exp\\left(i (-t_1 R_{k,1} + t_2 R_{k,3})\\right)\\right]  (\\text{for } \\varepsilon_{k,1}=-1, \\varepsilon_{k,2}=1) \\\\\n+ \\mathbb{E}\\left[\\exp\\left(i (-t_1 R_{k,1} - t_2 R_{k,3})\\right)\\right]  (\\text{for } \\varepsilon_{k,1}=-1, \\varepsilon_{k,2}=-1) \\Big).\n\\end{align*}\nSince $R_{k,1}$ and $R_{k,3}$ are i.i.d., let their characteristic function be $\\phi_R(t)$. Using independence:\n\\begin{align*}\n\\phi(t_1, t_2) = \\frac{1}{4} \\left( \\phi_R(t_1)\\phi_R(t_2) + \\phi_R(t_1)\\phi_R(-t_2) + \\phi_R(-t_1)\\phi_R(t_2) + \\phi_R(-t_1)\\phi_R(-t_2) \\right) \\\\\n= \\frac{1}{4} \\left( \\phi_R(t_1) + \\phi_R(-t_1) \\right) \\left( \\phi_R(t_2) + \\phi_R(-t_2) \\right).\n\\end{align*}\nThe characteristic function of $X_{3k-2}$ is $\\phi_{X_{3k-2}}(t_1) = \\mathbb{E}[\\exp(it_1 R_{k,1}\\varepsilon_{k,1})] = \\frac{1}{2}\\phi_R(t_1) + \\frac{1}{2}\\phi_R(-t_1)$.\nFor $X_{3k} = R_{k,3}(\\varepsilon_{k,1}\\varepsilon_{k,2})$, the sign $\\eta = \\varepsilon_{k,1}\\varepsilon_{k,2}$ is a Rademacher variable independent of $R_{k,3}$. Thus, the characteristic function of $X_{3k}$ is $\\phi_{X_{3k}}(t_2) = \\mathbb{E}[\\exp(it_2 R_{k,3}\\eta)] = \\frac{1}{2}\\phi_R(t_2) + \\frac{1}{2}\\phi_R(-t_2)$.\nThe product of the individual characteristic functions is $\\phi_{X_{3k-2}}(t_1) \\phi_{X_{3k}}(t_2) = \\frac{1}{4}(\\phi_R(t_1)+\\phi_R(-t_1))(\\phi_R(t_2)+\\phi_R(-t_2))$, which equals $\\phi(t_1, t_2)$. Hence, $X_{3k-2}$ and $X_{3k}$ are independent.\n\n(c) Pair $(X_{3k-1}, X_{3k})$:\nAn identical argument shows that $X_{3k-1}$ and $X_{3k}$ are independent.\n\nSince all pairs $(X_n, X_m)$ with $n \\neq m$ are independent, the sequence $\\{X_n\\}_{n \\ge 1}$ is pairwise independent.\n\n**Not Jointly Independent:**\nTo show that the sequence is not jointly independent, we only need to find one set of variables that are not. We examine the triplet $(X_{3k-2}, X_{3k-1}, X_{3k})$.\nSince $R_{k,j} \\ge e > 0$, the sign of $X_n$ is determined solely by the Rademacher signs.\n$\\mathbb{P}(X_{3k-2} > 0) = \\mathbb{P}(R_{k,1}\\varepsilon_{k,1} > 0) = \\mathbb{P}(\\varepsilon_{k,1}=1) = \\frac{1}{2}$.\n$\\mathbb{P}(X_{3k-1} > 0) = \\mathbb{P}(R_{k,2}\\varepsilon_{k,2} > 0) = \\mathbb{P}(\\varepsilon_{k,2}=1) = \\frac{1}{2}$.\n$\\mathbb{P}(X_{3k} > 0) = \\mathbb{P}(R_{k,3}\\varepsilon_{k,1}\\varepsilon_{k,2} > 0) = \\mathbb{P}(\\varepsilon_{k,1}\\varepsilon_{k,2}=1) = \\mathbb{P}(\\varepsilon_{k,1}=\\varepsilon_{k,2}) = \\frac{1}{2}$.\nIf they were jointly independent, we would have:\n$$\n\\mathbb{P}(X_{3k-2} > 0, X_{3k-1} > 0, X_{3k} > 0) = \\mathbb{P}(X_{3k-2} > 0)\\mathbb{P}(X_{3k-1} > 0)\\mathbb{P}(X_{3k} > 0) = \\frac{1}{2}\\cdot\\frac{1}{2}\\cdot\\frac{1}{2} = \\frac{1}{8}.\n$$\nHowever, we compute the joint probability directly:\n\\begin{align*}\n\\mathbb{P}(X_{3k-2} > 0, X_{3k-1} > 0, X_{3k} > 0) = \\mathbb{P}(\\varepsilon_{k,1}=1, \\varepsilon_{k,2}=1, \\varepsilon_{k,1}\\varepsilon_{k,2}=1) \\\\\n= \\mathbb{P}(\\varepsilon_{k,1}=1, \\varepsilon_{k,2}=1)\n\\end{align*}\nas the third condition is implied by the first two. Since $\\varepsilon_{k,1}$ and $\\varepsilon_{k,2}$ are independent:\n$$\n\\mathbb{P}(\\varepsilon_{k,1}=1, \\varepsilon_{k,2}=1) = \\mathbb{P}(\\varepsilon_{k,1}=1)\\mathbb{P}(\\varepsilon_{k,2}=1) = \\frac{1}{2}\\cdot\\frac{1}{2} = \\frac{1}{4}.\n$$\nSince $\\frac{1}{4} \\neq \\frac{1}{8}$, the variables $X_{3k-2}, X_{3k-1}, X_{3k}$ are not jointly independent. Therefore, the sequence $\\{X_n\\}_{n\\ge 1}$ is not jointly independent.\n\n### Part 2: First and Second Moments\n\nWe compute the moments for $X_1 = R_{1,1}\\varepsilon_{1,1}$. The arguments hold for any $X_n$.\n\n**First absolute moment:**\n$\\mathbb{E}[|X_1|] = \\mathbb{E}[|R_{1,1}\\varepsilon_{1,1}|] = \\mathbb{E}[|R_{1,1}| \\cdot |\\varepsilon_{1,1}|]$. Since $R_{1,1}$ is non-negative and $|\\varepsilon_{1,1}|=1$, we have $\\mathbb{E}[|X_1|] = \\mathbb{E}[R_{1,1}]$.\n$$\n\\mathbb{E}[R_{1,1}] = \\int_{e}^{\\infty} r f_R(r)\\,dr = c_R \\int_{e}^{\\infty} r \\frac{1}{r^2(\\ln r)^2}\\,dr = c_R \\int_{e}^{\\infty} \\frac{1}{r(\\ln r)^2}\\,dr.\n$$\nWe use the substitution $u = \\ln r$, for which $du = \\frac{1}{r}dr$. The limits of integration become $\\ln(e)=1$ and $\\lim_{r\\to\\infty}\\ln r = \\infty$.\n$$\n\\int_{e}^{\\infty} \\frac{1}{r(\\ln r)^2}\\,dr = \\int_{1}^{\\infty} \\frac{1}{u^2}\\,du = \\left[-\\frac{1}{u}\\right]_{1}^{\\infty} = 0 - (-1) = 1.\n$$\nThus, $\\mathbb{E}[|X_1|] = c_R \\cdot 1 = c_R$. Since $c_R$ is a normalizing constant, it must be finite, so $\\mathbb{E}[|X_1|]  \\infty$.\n\n**Second moment (and variance):**\n$\\mathbb{E}[X_1^2] = \\mathbb{E}[(R_{1,1}\\varepsilon_{1,1})^2] = \\mathbb{E}[R_{1,1}^2 \\varepsilon_{1,1}^2]$. Since $\\varepsilon_{1,1}^2 = 1$, we have $\\mathbb{E}[X_1^2] = \\mathbb{E}[R_{1,1}^2]$.\n$$\n\\mathbb{E}[R_{1,1}^2] = \\int_{e}^{\\infty} r^2 f_R(r)\\,dr = c_R \\int_{e}^{\\infty} r^2 \\frac{1}{r^2(\\ln r)^2}\\,dr = c_R \\int_{e}^{\\infty} \\frac{1}{(\\ln r)^2}\\,dr.\n$$\nTo evaluate this integral, we let $u=\\ln r$, so $r=e^u$ and $dr=e^u du$. The limits are $1$ and $\\infty$.\n$$\n\\int_{e}^{\\infty} \\frac{1}{(\\ln r)^2}\\,dr = \\int_{1}^{\\infty} \\frac{e^u}{u^2}\\,du.\n$$\nFor $u \\ge 1$, we have $e^u \\ge e$. Also, for $u \\ge 1$, $u^{-2} \\le u^{-1}$. A better comparison is needed. For $u \\ge 1$, $e^u = 1 + u + u^2/2! + \\dots  u$. So $\\frac{e^u}{u^2}  \\frac{u}{u^2} = \\frac{1}{u}$.\nThe integral $\\int_1^\\infty \\frac{1}{u}\\,du = [\\ln u]_1^\\infty$ diverges to $\\infty$. By the comparison test,\n$$\n\\int_{1}^{\\infty} \\frac{e^u}{u^2}\\,du \\ge \\int_{1}^{\\infty} \\frac{1}{u}\\,du = \\infty.\n$$\nTherefore, $\\mathbb{E}[X_1^2] = \\infty$.\nThe mean of $X_1$ is $\\mathbb{E}[X_1] = \\mathbb{E}[R_{1,1}\\varepsilon_{1,1}] = \\mathbb{E}[R_{1,1}]\\mathbb{E}[\\varepsilon_{1,1}] = c_R \\cdot 0 = 0$.\nThe variance is $\\operatorname{Var}(X_1) = \\mathbb{E}[X_1^2] - (\\mathbb{E}[X_1])^2 = \\infty - 0^2 = \\infty$.\n\n### Part 3: Application of the Strong Law of Large Numbers (SLLN)\n\nEtemadi's SLLN states that if $\\{X_n\\}_{n\\ge 1}$ is a sequence of pairwise independent, identically distributed, and integrable random variables, then $\\frac{S_n}{n} \\to \\mathbb{E}[X_1]$ almost surely as $n\\to\\infty$.\n\nWe verify the conditions for our sequence $\\{X_n\\}$:\n1.  **Pairwise independent:** This was established in Part 1.\n2.  **Identically distributed:** For any $n$, $X_n$ is of the form $R \\cdot \\eta$, where $R$ is a random variable with density $f_R(r)$ and $\\eta$ is an independent Rademacher sign. As shown in Part 1(b) using characteristic functions, the distribution of such a variable depends only on the distribution of $R$. Since all $R_{k,j}$ are i.i.d., all $X_n$ are identically distributed.\n3.  **Integrable:** As shown in Part 2, $\\mathbb{E}[|X_1|] = c_R  \\infty$. Since the variables are identically distributed, $\\mathbb{E}[|X_n|]  \\infty$ for all $n$.\n\nAll conditions of Etemadi's SLLN are satisfied. We can thus conclude that $\\frac{S_n}{n}$ converges almost surely to a constant.\n\nThe classical SLLN approach of Kolmogorov requires assumptions on the variance. For instance, Kolmogorov's SLLN for independent (not necessarily i.d.) random variables requires the condition $\\sum_{n=1}^\\infty \\frac{\\operatorname{Var}(X_n)}{n^2}  \\infty$.\nIn our case, the variables are pairwise independent and have mean $0$, so they are uncorrelated. This means $\\operatorname{Var}(S_n) = \\sum_{m=1}^n \\operatorname{Var}(X_m)$.\nHowever, as shown in Part 2, $\\operatorname{Var}(X_n) = \\operatorname{Var}(X_1) = \\infty$ for all $n$. Therefore, any SLLN proof that relies on finite variance or a summability condition on the variances (like Kolmogorov's sufficient condition or extensions for weakly correlated sequences) is not applicable. This highlights the strength of Etemadi's theorem, which bypasses the need for finite second moments by leveraging pairwise independence.\n\n### Part 4: Computation of the Limit\n\nBy Etemadi's SLLN, the almost sure limit of $\\frac{S_n}{n}$ is the expectation of any of the random variables in the sequence.\n$$\nL = \\lim_{n \\to \\infty} \\frac{S_n}{n} = \\mathbb{E}[X_1] \\quad \\text{a.s.}\n$$\nWe compute the expectation of $X_1$:\n$$\n\\mathbb{E}[X_1] = \\mathbb{E}[R_{1,1}\\varepsilon_{1,1}].\n$$\nDue to the independence of $R_{1,1}$ and the Rademacher sign $\\varepsilon_{1,1}$, we can separate the expectations:\n$$\n\\mathbb{E}[X_1] = \\mathbb{E}[R_{1,1}] \\cdot \\mathbb{E}[\\varepsilon_{1,1}].\n$$\nThe expectation of a Rademacher sign is:\n$$\n\\mathbb{E}[\\varepsilon_{1,1}] = (1) \\cdot \\mathbb{P}(\\varepsilon_{1,1}=1) + (-1) \\cdot \\mathbb{P}(\\varepsilon_{1,1}=-1) = (1) \\cdot \\frac{1}{2} + (-1) \\cdot \\frac{1}{2} = 0.\n$$\nFrom Part 2, we know $\\mathbb{E}[R_{1,1}] = c_R$, which is a finite constant. Therefore,\n$$\nL = \\mathbb{E}[X_1] = c_R \\cdot 0 = 0.\n$$\nThe almost sure limit of the sequence of sample averages is $0$.", "answer": "$$\\boxed{0}$$", "id": "2984548"}, {"introduction": "The Law of Large Numbers for Markov processes, often called the ergodic theorem, is fundamentally linked to the process's long-term stability and its tendency to return to a central region. This exercise explores this connection using standard Brownian motion, a process for which the usual ergodic theorem fails [@problem_id:2984541]. You will first demonstrate the absence of a Foster-Lyapunov drift condition, which leads to the classification of the process as null recurrent and the non-existence of a finite invariant measure. This exploration illuminates why the standard LLN does not apply and challenges you to discover an alternative scaling law that governs the growth of time averages for this fundamental process.", "problem": "Consider the one-dimensional Stochastic Differential Equation (SDE) $dX_{t} = dW_{t}$ with $X_{0} = 0$, where $\\{W_{t}\\}_{t \\geq 0}$ is a standard Brownian motion. Let $\\mathcal{L}$ denote the infinitesimal generator of this Markov process and recall that a Foster–Lyapunov drift condition is a requirement of the form $\\mathcal{L}V(x) \\leq -c$ for all $|x| \\geq R$, for some constants $c>0$ and $R>0$, and some twice continuously differentiable function $V \\in C^{2}(\\mathbb{R})$ satisfying $\\lim_{|x| \\to \\infty} V(x) = \\infty$.\n\n(a) Starting from the definition of the generator $\\mathcal{L}$ for the SDE $dX_{t} = dW_{t}$ and the properties of twice continuously differentiable functions, prove that there do not exist constants $c>0$, $R>0$ and a function $V \\in C^{2}(\\mathbb{R})$ with $\\lim_{|x| \\to \\infty} V(x) = \\infty$ such that the Foster–Lyapunov drift condition $\\mathcal{L}V(x) \\leq -c$ holds for all $|x| \\geq R$. Conclude that any uniform Lyapunov drift condition fails for this SDE.\n\n(b) Using well-tested facts about the recurrence classification of one-dimensional diffusions and the absence of any finite invariant probability measure for the generator $\\mathcal{L}$, deduce that $\\{X_{t}\\}_{t \\geq 0}$ is null recurrent. Clearly state the definitions of recurrence, positive recurrence, and null recurrence that you use, and justify the null recurrence classification from first principles for this example.\n\n(c) Let $f(x) = x^{2}$ and define the time average $A_{T} = \\frac{1}{T} \\int_{0}^{T} f(X_{t}) \\, dt$. Explain why the usual Law of Large Numbers (LLN) for ergodic Markov processes need not apply to $A_{T}$ for this unbounded $f$ in the absence of a Foster–Lyapunov drift condition that yields a finite invariant probability measure. Then, compute the exact value of the limit\n$$\nL \\;=\\; \\lim_{T \\to \\infty} \\frac{1}{T^{2}} \\, \\mathbb{E}\\!\\left[ \\int_{0}^{T} X_{t}^{2} \\, dt \\right].\n$$\nYour final answer must be a single real number. No rounding is required.", "solution": "The problem asks for a three-part analysis of the one-dimensional stochastic process defined by the SDE $dX_{t} = dW_{t}$ with initial condition $X_{0} = 0$.\n\n### Part (a): Failure of the Foster–Lyapunov Drift Condition\n\nWe are asked to prove that for the process $X_{t}$, there do not exist constants $c>0$, $R>0$, and a function $V \\in C^{2}(\\mathbb{R})$ with $\\lim_{|x| \\to \\infty} V(x) = \\infty$ such that the Foster–Lyapunov drift condition $\\mathcal{L}V(x) \\leq -c$ holds for all $|x| \\geq R$.\n\nFor a general one-dimensional Itô diffusion $dX_{t} = b(X_{t})dt + \\sigma(X_{t})dW_{t}$, the infinitesimal generator $\\mathcal{L}$ applied to a function $g \\in C^{2}(\\mathbb{R})$ is given by\n$$\n\\mathcal{L}g(x) = b(x) g'(x) + \\frac{1}{2} \\sigma(x)^{2} g''(x).\n$$\nIn our specific case, $dX_{t} = dW_{t}$, the drift coefficient is $b(x) = 0$ and the diffusion coefficient is $\\sigma(x) = 1$. Therefore, the generator simplifies to\n$$\n\\mathcal{L}V(x) = \\frac{1}{2} V''(x).\n$$\nThe Foster–Lyapunov drift condition would require that for some constants $c > 0$ and $R > 0$,\n$$\n\\frac{1}{2} V''(x) \\leq -c \\quad \\text{for all } |x| \\geq R.\n$$\nThis is equivalent to $V''(x) \\leq -2c$ for all $|x| \\geq R$. Since $c$ is strictly positive, this implies that the second derivative of $V$ is uniformly negative and bounded away from zero outside the interval $[-R, R]$.\n\nLet us analyze the consequences of this inequality for $x \\geq R$. We integrate $V''(\\xi) \\leq -2c$ from $R$ to $x$:\n$$\n\\int_{R}^{x} V''(\\xi) \\, d\\xi \\leq \\int_{R}^{x} (-2c) \\, d\\xi\n$$\n$$\nV'(x) - V'(R) \\leq -2c(x-R)\n$$\n$$\nV'(x) \\leq V'(R) - 2c(x-R).\n$$\nAs $x \\to \\infty$, the right-hand side approaches $-\\infty$, which implies that $\\lim_{x \\to \\infty} V'(x) = -\\infty$.\n\nNow, we integrate the inequality for $V'(x)$ from $R$ to $x$:\n$$\n\\int_{R}^{x} V'(\\xi) \\, d\\xi \\leq \\int_{R}^{x} \\left( V'(R) - 2c(\\xi-R) \\right) \\, d\\xi\n$$\n$$\nV(x) - V(R) \\leq \\left[ V'(R)(\\xi-R) - c(\\xi-R)^2 \\right]_{R}^{x}\n$$\n$$\nV(x) - V(R) \\leq V'(R)(x-R) - c(x-R)^2.\n$$\nSo, $V(x) \\leq V(R) + V'(R)(x-R) - c(x-R)^2$. The right-hand side is a quadratic polynomial in $x$ with a negative leading coefficient ($-c$). Such a polynomial must approach $-\\infty$ as $x \\to \\infty$. Therefore, we must have\n$$\n\\lim_{x \\to \\infty} V(x) = -\\infty.\n$$\nA similar argument for $x \\leq -R$ also shows that $\\lim_{x \\to -\\infty} V(x) = -\\infty$.\n\nThis result directly contradicts the initial assumption on the Lyapunov function $V$, which requires that $\\lim_{|x| \\to \\infty} V(x) = \\infty$. This contradiction proves that no such function $V$ and constants $c, R$ can exist. The failure of this condition, a canonical form of a uniform drift condition, demonstrates that uniform Lyapunov drift conditions do not hold for this SDE.\n\n### Part (b): Null Recurrence Classification\n\nWe first define the types of recurrence for a continuous-time Markov process on $\\mathbb{R}$.\nA state $x$ is **recurrent** if the process, starting from $x$, returns to any open neighborhood of $x$ with probability $1$. The process is recurrent if all states are recurrent.\nA recurrent process is **positive recurrent** if the expected time to return to a neighborhood of a starting point is finite. For a diffusion process, this is equivalent to the existence of a finite invariant probability measure.\nA recurrent process is **null recurrent** if it is not positive recurrent. This means the process is guaranteed to return to any neighborhood, but the expected time to do so is infinite. This corresponds to the absence of a finite invariant probability measure.\n\nFor the process $X_{t}$, which is a standard Brownian motion started at $0$, we can justify its classification as follows:\n\n1.  **Recurrence:** It is a fundamental property of one-dimensional Brownian motion that it is recurrent. To show this from first principles, consider the probability that the process, starting at $X_0=0$, hits a level $a>0$ before hitting a level $-b0$. This probability is $p = \\frac{b}{a+b}$. The probability of ever hitting $a$ is found by taking the limit as $b \\to \\infty$, which gives $\\lim_{b\\to\\infty} \\frac{b}{a+b} = 1$. Similarly, it hits any level $a0$ with probability $1$. Thus, it visits every point and is recurrent.\n\n2.  **Positive vs. Null Recurrence:** To distinguish between positive and null recurrence, we search for an invariant probability measure, $\\pi(x)$. Such a measure must satisfy $\\mathcal{L}^{*}\\pi = 0$, where $\\mathcal{L}^{*}$ is the adjoint of the infinitesimal generator $\\mathcal{L}$. The stationary Fokker-Planck equation for a general 1D diffusion is $\\frac{1}{2} \\frac{d^2}{dx^2}(\\sigma(x)^2 \\pi(x)) - \\frac{d}{dx}(b(x)\\pi(x)) = 0$.\n    For our process, $b(x)=0$ and $\\sigma(x)=1$, so the equation becomes:\n    $$\n    \\frac{1}{2} \\frac{d^2}{dx^2}\\pi(x) = 0.\n    $$\n    The general solution to this second-order ordinary differential equation is a linear function:\n    $$\n    \\pi(x) = C_1 x + C_2\n    $$\n    for some constants $C_1$ and $C_2$. An invariant measure must define a probability distribution (or be proportional to one), so it must be non-negative, i.e., $\\pi(x) \\geq 0$ for all $x \\in \\mathbb{R}$. If $C_1 \\neq 0$, the linear function $C_1 x + C_2$ will take negative values for either large positive or large negative $x$. Thus, we must have $C_1 = 0$. This leaves $\\pi(x) = C_2$, a constant.\n    For this to be a finite measure, its integral over the state space $\\mathbb{R}$ must be finite:\n    $$\n    \\int_{-\\infty}^{\\infty} \\pi(x) \\, dx = \\int_{-\\infty}^{\\infty} C_2 \\, dx.\n    $$\n    If $C_2 > 0$, this integral diverges. If $C_2 = 0$, we get the trivial zero measure. Therefore, there is no non-trivial finite invariant measure. The Lebesgue measure itself (corresponding to $C_2 > 0$) is an invariant measure, but it is not a finite one.\n\nSince the process is recurrent but does not possess a finite invariant probability measure, it is by definition **null recurrent**.\n\n### Part (c): Law of Large Numbers and Limit Calculation\n\nThe ergodic theorem, which is a Law of Large Numbers (LLN) for Markov processes, states that for a positive recurrent process with a unique invariant probability measure $\\pi$, the time average of a suitable function $f$ converges to its spatial average with respect to $\\pi$. $A_T = \\frac{1}{T} \\int_{0}^{T} f(X_{t}) \\, dt \\to \\int_{\\mathbb{R}} f(x) \\pi(dx)$ almost surely, assuming $f$ is integrable with respect to $\\pi$.\n\nAs established in part (b), the process $\\{X_t\\}$ is null recurrent and lacks a finite invariant probability measure $\\pi$. Consequently, the premise of the ergodic theorem is not satisfied. The failure of the Foster-Lyapunov condition in part (a) is a key reason for this, as such a condition is a standard tool to prove positive recurrence and thus ergodicity. For a null recurrent process, the particle tends to wander far from the origin, meaning that for an unbounded function like $f(x) = x^2$, the time average $A_T$ will not converge to a finite constant.\n\nWe are asked to compute the limit:\n$$\nL \\;=\\; \\lim_{T \\to \\infty} \\frac{1}{T^{2}} \\, \\mathbb{E}\\!\\left[ \\int_{0}^{T} X_{t}^{2} \\, dt \\right].\n$$\nLet's first evaluate the expectation. Since the integrand $X_t^2$ is non-negative, we can use the Fubini-Tonelli theorem to interchange the expectation and the integral:\n$$\n\\mathbb{E}\\!\\left[ \\int_{0}^{T} X_{t}^{2} \\, dt \\right] = \\int_{0}^{T} \\mathbb{E}[X_{t}^{2}] \\, dt.\n$$\nThe process $X_t$ is a standard Brownian motion starting at $X_0 = 0$, so at any time $t > 0$, $X_t$ is a normally distributed random variable with mean $0$ and variance $t$. That is, $X_t \\sim \\mathcal{N}(0, t)$.\nThe expectation of $X_t^2$ is related to its variance:\n$$\n\\text{Var}(X_t) = \\mathbb{E}[X_{t}^{2}] - (\\mathbb{E}[X_t])^2.\n$$\nSince $\\mathbb{E}[X_t] = 0$ and $\\text{Var}(X_t) = t$, we have:\n$$\n\\mathbb{E}[X_{t}^{2}] = t.\n$$\nSubstituting this back into the integral:\n$$\n\\int_{0}^{T} \\mathbb{E}[X_{t}^{2}] \\, dt = \\int_{0}^{T} t \\, dt.\n$$\nEvaluating this elementary integral gives:\n$$\n\\int_{0}^{T} t \\, dt = \\left[ \\frac{t^2}{2} \\right]_{0}^{T} = \\frac{T^2}{2}.\n$$\nNow we can compute the limit $L$:\n$$\nL = \\lim_{T \\to \\infty} \\frac{1}{T^{2}} \\left( \\frac{T^2}{2} \\right) = \\lim_{T \\to \\infty} \\frac{1}{2}.\n$$\nThe limit is a constant.\n\n$$\nL = \\frac{1}{2}.\n$$\nThis result demonstrates that while the standard LLN with $1/T$ normalization fails, a different scaling law with $1/T^2$ normalization holds for the expected value of this particular time average. This reflects the diffusive nature of the process.", "answer": "$$\n\\boxed{\\frac{1}{2}}\n$$", "id": "2984541"}]}