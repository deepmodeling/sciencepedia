{"hands_on_practices": [{"introduction": "Our first exercise provides a classic illustration of the power of the Borel-Cantelli lemmas. By analyzing the behavior of a standard Brownian motion over successive, disjoint time intervals, we can exploit the independence of its increments to establish a sharp threshold. This problem [@problem_id:2991415] demonstrates how the convergence or divergence of a single series of probabilities, determined by a critical constant $c^{\\ast}$, dictates whether a certain magnitude of fluctuation occurs infinitely often or only a finite number of times, almost surely.", "problem": "Let $\\{W_{t}\\}_{t \\geq 0}$ be a standard one-dimensional Brownian motion with $W_{0}=0$. For each integer $n \\geq 3$ and constant $c0$, define the event\n$$\nA_{n}=\\left\\{\\sup_{t \\in [n,n+1]} \\left|W_{t}-W_{n}\\right|\\sqrt{2c\\,\\ln n}\\right\\}.\n$$\nUsing only foundational properties of Brownian motion (independent and stationary increments, the reflection principle for one-sided maxima over a unit interval, and standard asymptotics of the standard normal tail), determine the exact value of the critical constant $c^{\\ast}$ such that the following dichotomy holds:\n- If $cc^{\\ast}$, then $\\sum_{n=3}^{\\infty} \\mathbb{P}(A_{n})=\\infty$, and by the second Borel–Cantelli lemma the events $A_{n}$ occur infinitely often almost surely (a.s.).\n- If $cc^{\\ast}$, then $\\sum_{n=3}^{\\infty} \\mathbb{P}(A_{n})\\infty$, and hence $A_{n}$ occurs only finitely often a.s.\n\nYour final answer must be the value of $c^{\\ast}$ as a single real number. No rounding is required.", "solution": "The problem requires finding a critical constant $c^{\\ast}$ that determines the convergence or divergence of the series $\\sum_{n=3}^{\\infty} \\mathbb{P}(A_n)$, where $A_{n}=\\left\\{\\sup_{t \\in [n,n+1]} \\left|W_{t}-W_{n}\\right|\\sqrt{2c\\,\\ln n}\\right\\}$ and $\\{W_t\\}_{t \\ge 0}$ is a standard one-dimensional Brownian motion.\n\nFirst, we analyze the probability $\\mathbb{P}(A_n)$. Let $a_n = \\sqrt{2c \\ln n}$. The event $A_n$ concerns the behavior of the process $W_t - W_n$ over the time interval $t \\in [n, n+1]$. We define a new stochastic process $\\tilde{W}_s = W_{n+s} - W_n$ for $s \\in [0, 1]$. Due to the stationary and independent increments property of Brownian motion, $\\{\\tilde{W}_s\\}_{s \\in [0,1]}$ is itself a standard Brownian motion starting at $\\tilde{W}_0 = 0$. The distribution of this process is identical to that of $\\{W_s\\}_{s \\in [0,1]}$.\n\nThe probability $\\mathbb{P}(A_n)$ can thus be rewritten as:\n$$\n\\mathbb{P}(A_n) = \\mathbb{P}\\left(\\sup_{s \\in [0,1]} |\\tilde{W}_s|  a_n\\right) = \\mathbb{P}\\left(\\sup_{s \\in [0,1]} |W_s|  a_n\\right)\n$$\nThis shows that $\\mathbb{P}(A_n)$ depends on $n$ only through the threshold $a_n$. Furthermore, the events $\\{A_n\\}_{n \\ge 3}$ are independent because they are defined on disjoint time intervals $[n, n+1]$, and Brownian motion has independent increments.\n\nLet $M_1 = \\sup_{s \\in [0,1]} W_s$ and $m_1 = \\inf_{s \\in [0,1]} W_s$. The event of interest is $\\left\\{\\sup_{s \\in [0,1]} |W_s|  a_n\\right\\}$, which is equivalent to the event $\\{M_1  a_n \\text{ or } m_1  -a_n\\}$. We can bound its probability using the union of events property:\n$$\n\\mathbb{P}(M_1  a_n) \\le \\mathbb{P}(A_n) \\le \\mathbb{P}(M_1  a_n) + \\mathbb{P}(m_1  -a_n)\n$$\nThe problem states we must use the reflection principle for one-sided maxima. For any $a  0$, the reflection principle gives the probability of the maximum of a standard Brownian motion over $[0,1]$:\n$$\n\\mathbb{P}(M_1  a) = 2 \\mathbb{P}(W_1  a)\n$$\nBy the symmetry of Brownian motion, the process $\\{-W_s\\}_{s \\ge 0}$ is also a standard Brownian motion. Therefore, the distribution of the infimum is related to the distribution of the supremum:\n$$\n\\mathbb{P}(m_1  -a) = \\mathbb{P}\\left(\\inf_{s \\in [0,1]} W_s  -a\\right) = \\mathbb{P}\\left(\\sup_{s \\in [0,1]} (-W_s)  a\\right) = \\mathbb{P}(M_1  a)\n$$\nSubstituting this into our expression for $\\mathbb{P}(m_1  -a_n)$ yields:\n$$\n\\mathbb{P}(m_1  -a_n) = \\mathbb{P}(M_1  a_n) = 2 \\mathbb{P}(W_1  a_n)\n$$\nUsing these results, we can establish bounds for $\\mathbb{P}(A_n)$:\n$$\n2 \\mathbb{P}(W_1  a_n) \\le \\mathbb{P}(A_n) \\le 4 \\mathbb{P}(W_1  a_n)\n$$\nThe lower bound comes from $\\mathbb{P}(A_n) \\ge \\mathbb{P}(M_1  a_n) = 2 \\mathbb{P}(W_1  a_n)$ (and also $\\mathbb{P}(A_n) \\ge \\mathbb{P}(m_1  -a_n)$). The upper bound is the union bound.\n\nThe convergence of the series $\\sum_{n=3}^\\infty \\mathbb{P}(A_n)$ is therefore determined by the convergence of $\\sum_{n=3}^\\infty \\mathbb{P}(W_1  a_n)$, according to the limit comparison test for series with positive terms.\n\nThe random variable $W_1$ is distributed as a standard normal variable, $W_1 \\sim \\mathcal{N}(0,1)$. The problem directs us to use the standard asymptotic formula for the tail of a standard normal distribution. For a random variable $Z \\sim \\mathcal{N}(0,1)$, as $x \\to \\infty$:\n$$\n\\mathbb{P}(Z  x) = \\int_x^\\infty \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{z^2}{2}\\right) dz \\sim \\frac{1}{\\sqrt{2\\pi}x} \\exp\\left(-\\frac{x^2}{2}\\right)\n$$\nWe apply this formula with $x = a_n = \\sqrt{2c \\ln n}$. Since $c  0$, $a_n \\to \\infty$ as $n \\to \\infty$.\n$$\n\\mathbb{P}(W_1  a_n) \\sim \\frac{1}{\\sqrt{2\\pi} \\sqrt{2c \\ln n}} \\exp\\left(-\\frac{(\\sqrt{2c \\ln n})^2}{2}\\right)\n$$\n$$\n\\mathbb{P}(W_1  a_n) \\sim \\frac{1}{\\sqrt{4\\pi c \\ln n}} \\exp(-c \\ln n) = \\frac{1}{\\sqrt{4\\pi c}} \\frac{1}{n^c (\\ln n)^{1/2}}\n$$\nLet $p_n = \\mathbb{P}(W_1  a_n)$. The series $\\sum p_n$ has the same convergence behavior as the series $\\sum b_n$ where $b_n = \\frac{1}{n^c (\\ln n)^{1/2}}$. We now analyze the convergence of $\\sum b_n$ using the integral test.\n\nCase 1: $c  1$.\nThe series $\\sum_{n=3}^\\infty \\frac{1}{n^c (\\ln n)^{1/2}}$ can be compared with the p-series $\\sum_{n=3}^\\infty \\frac{1}{n^p}$ for any $p$ such that $1  p  c$. Since $\\lim_{n\\to\\infty} \\frac{n^{-c}(\\ln n)^{-1/2}}{n^{-p}} = \\lim_{n\\to\\infty} n^{p-c}(\\ln n)^{-1/2} = 0$ and $\\sum n^{-p}$ converges, the series $\\sum b_n$ converges. Consequently, $\\sum p_n$ converges, and by our earlier bounds, $\\sum \\mathbb{P}(A_n)$ converges.\n\nCase 2: $c  1$.\nThe series $\\sum_{n=3}^\\infty \\frac{1}{n^c (\\ln n)^{1/2}}$ can be compared with the divergent p-series $\\sum_{n=3}^\\infty \\frac{1}{n^c}$. For $n \\ge 3$, $(\\ln n)^{1/2}  1$, so $\\frac{1}{n^c (\\ln n)^{1/2}}  \\frac{1}{n^c}$; this comparison is not useful for divergence. Instead, we compare to $\\sum n^{-p}$ where $c  p  1$. $\\lim_{n \\to\\infty} \\frac{n^{-c}(\\ln n)^{-1/2}}{n^{-p}} = \\lim_{n \\to\\infty} n^{p-c}(\\ln n)^{-1/2} = \\infty$. Since $\\sum n^{-p}$ diverges, $\\sum b_n$ also diverges. Therefore, $\\sum p_n$ diverges, and so does $\\sum \\mathbb{P}(A_n)$.\n\nCase 3: $c = 1$.\nThe series is $\\sum_{n=3}^\\infty \\frac{1}{n (\\ln n)^{1/2}}$. We apply the integral test to the function $f(x) = \\frac{1}{x (\\ln x)^{1/2}}$.\n$$\n\\int_3^\\infty \\frac{1}{x (\\ln x)^{1/2}} dx\n$$\nUsing the substitution $u = \\ln x$, we have $du = \\frac{1}{x} dx$. The integral becomes:\n$$\n\\int_{\\ln 3}^\\infty \\frac{1}{u^{1/2}} du = \\left[ 2u^{1/2} \\right]_{\\ln 3}^\\infty = \\lim_{R\\to\\infty} (2\\sqrt{R}) - 2\\sqrt{\\ln 3} = \\infty\n$$\nSince the integral diverges, the series diverges for $c=1$. Thus, $\\sum \\mathbb{P}(A_n)$ diverges.\n\nIn summary:\n- If $c  1$, then $\\sum_{n=3}^{\\infty} \\mathbb{P}(A_n)  \\infty$.\n- If $c \\le 1$, then $\\sum_{n=3}^{\\infty} \\mathbb{P}(A_n) = \\infty$.\n\nThe problem defines the critical constant $c^\\ast$ via the dichotomy:\n- $c  c^\\ast \\implies \\sum \\mathbb{P}(A_n) = \\infty$\n- $c  c^\\ast \\implies \\sum \\mathbb{P}(A_n)  \\infty$\n\nComparing this with our findings, we see that the conditions are satisfied precisely when $c^\\ast = 1$.\n- If $c  1$, the sum is infinite.\n- If $c  1$, the sum is finite.\nThe value of the critical constant is therefore $1$.", "answer": "$$\\boxed{1}$$", "id": "2991415"}, {"introduction": "Often, the series of probabilities $\\sum \\mathbb{P}(A_n)$ diverges, precluding a direct application of the first Borel-Cantelli lemma to establish an almost sure result. This does not mean that almost sure convergence fails. This exercise [@problem_id:2991422] introduces a fundamental strategic technique: recovering an almost sure convergence statement by carefully selecting a sufficiently sparse subsequence. You will explore how this selection forces the corresponding series of probabilities to converge, a crucial step in strengthening weaker convergence results (like convergence in mean-square) to the powerful mode of almost sure convergence.", "problem": "Consider a $d$-dimensional Itô stochastic differential equation with globally Lipschitz drift and diffusion coefficients,\n$$\ndX_t \\;=\\; b(X_t)\\,dt \\;+\\; \\sigma(X_t)\\,dW_t,\\qquad X_0=x_0,\\quad t\\in[0,T],\n$$\nwhere $W_t$ is a standard $m$-dimensional Brownian motion, and let $X^n$ denote the Euler–Maruyama approximation on the uniform mesh of size $1/n$. For $p\\ge 2$, a classical combination of the Burkholder–Davis–Gundy (BDG) inequality and Gronwall’s lemma yields the strong moment bound\n$$\n\\mathbb{E}\\Big[\\sup_{0\\le t\\le T}\\big|X^n(t)-X(t)\\big|^p\\Big]\\;\\le\\; C\\,n^{-p/2},\n$$\nfor some constant $C0$ independent of $n$. Fix $p=2$ and define the events\n$$\nA_n \\;=\\; \\Big\\{\\sup_{0\\le t\\le T}\\big|X^n(t)-X(t)\\big| \\;\\; n^{-1/4}\\Big\\}.\n$$\nYou wish to obtain an almost sure convergence statement along a subsequence, even though the full series $\\sum_{n=1}^{\\infty}\\mathbb{P}(A_n)$ diverges. Using only first principles such as the Markov inequality, the first Borel–Cantelli lemma, and the given moment bound, which of the following strategies correctly selects a subsequence $(n_k)_{k\\ge 1}$ such that $\\sum_{k=1}^{\\infty}\\mathbb{P}(A_{n_k})\\infty$, thereby implying\n$$\n\\sup_{0\\le t\\le T}\\big|X^{n_k}(t)-X(t)\\big|\\;\\le\\; n_k^{-1/4}\\quad\\text{for all but finitely many }k\\text{ almost surely}?\n$$\n\nA. Take the dyadic subsequence $n_k=2^k$. Then $\\mathbb{P}(A_{n_k})\\le C\\,2^{-k/2}$, so $\\sum_k\\mathbb{P}(A_{n_k})\\infty$, and the first Borel–Cantelli lemma yields the desired almost sure statement along $(n_k)$.\n\nB. Choose $n_k$ so that $\\mathbb{P}(A_{n_k})\\le 2^{-k}$, for example by ensuring $C\\,n_k^{-1/2}\\le 2^{-k}$. Such a choice makes $\\sum_k\\mathbb{P}(A_{n_k})\\infty$, and the first Borel–Cantelli lemma implies the desired almost sure statement along $(n_k)$.\n\nC. Invoke the second Borel–Cantelli lemma to conclude $\\mathbb{P}(A_n\\ \\text{i.o.})=1$ without any independence assumptions, showing that no subsequence can improve the almost sure behavior.\n\nD. Take the linear subsequence $n_k=k$. Since $\\mathbb{P}(A_{n_k})\\le C\\,k^{-1/2}$, one has $\\sum_k\\mathbb{P}(A_{n_k})=\\infty$, so the first Borel–Cantelli lemma cannot be applied to obtain the desired almost sure statement along $(n_k)$.\n\nE. Take $n_k=\\lfloor k\\log k\\rfloor$. Since $\\mathbb{P}(A_{n_k})\\le C\\,(k\\log k)^{-1/2}$, the series $\\sum_k\\mathbb{P}(A_{n_k})$ diverges, so the first Borel–Cantelli lemma cannot be applied to obtain the desired almost sure statement along $(n_k)$.\n\nSelect all correct options.", "solution": "The core of the problem is to find a strictly increasing sequence of integers $(n_k)_{k\\ge 1}$ such that the series $\\sum_{k=1}^{\\infty} \\mathbb{P}(A_{n_k})$ converges. If this condition is met, the first Borel–Cantelli lemma states that $\\mathbb{P}(\\limsup_{k\\to\\infty} A_{n_k}) = 0$. This means that, almost surely, only a finite number of the events $A_{n_k}$ occur. This is equivalent to the desired statement:\n$$\n\\sup_{0\\le t\\le T}\\big|X^{n_k}(t)-X(t)\\big|\\;\\le\\; n_k^{-1/4}\\quad\\text{for all but finitely many }k\\text{ almost surely}.\n$$\nUsing Markov's inequality on the given moment bound for $p=2$, we establish a bound on the probability of the events $A_n$:\n$$\n\\mathbb{P}(A_n) = \\mathbb{P}\\Big(\\sup_{0\\le t\\le T}\\big|X^n(t)-X(t)\\big| > n^{-1/4}\\Big) = \\mathbb{P}\\Big(\\sup_{0\\le t\\le T}\\big|X^n(t)-X(t)\\big|^2 > n^{-1/2}\\Big)\n$$\n$$\n\\le \\frac{\\mathbb{E}\\Big[\\sup_{0\\le t\\le T}\\big|X^n(t)-X(t)\\big|^2\\Big]}{n^{-1/2}} \\le \\frac{C\\,n^{-1}}{n^{-1/2}} = C\\,n^{-1/2}\n$$\nfor some constant $C  0$. Therefore, the problem reduces to finding a subsequence $(n_k)$ such that the series $\\sum_{k=1}^{\\infty} n_k^{-1/2}$ converges.\n\n**A. Take the dyadic subsequence $n_k=2^k$. Then $\\mathbb{P}(A_{n_k})\\le C\\,2^{-k/2}$, so $\\sum_k\\mathbb{P}(A_{n_k})\\infty$, and the first Borel–Cantelli lemma yields the desired almost sure statement along $(n_k)$.**\n\nLet us analyze this strategy. We set $n_k = 2^k$ for $k=1, 2, \\dots$. We examine the sum of probabilities:\n$$\n\\sum_{k=1}^{\\infty} \\mathbb{P}(A_{n_k}) = \\sum_{k=1}^{\\infty} \\mathbb{P}(A_{2^k})\n$$\nUsing our derived bound:\n$$\n\\mathbb{P}(A_{2^k}) \\le C\\,(2^k)^{-1/2} = C\\,(2^{-1/2})^k = C\\,\\left(\\frac{1}{\\sqrt{2}}\\right)^k\n$$\nTherefore, the series is bounded by a convergent geometric series:\n$$\n\\sum_{k=1}^{\\infty} \\mathbb{P}(A_{2^k}) \\le \\sum_{k=1}^{\\infty} C\\,\\left(\\frac{1}{\\sqrt{2}}\\right)^k = C \\frac{1/\\sqrt{2}}{1 - 1/\\sqrt{2}}  \\infty\n$$\nThe convergence is guaranteed because the ratio of the geometric series is $r=1/\\sqrt{2}  1$. Since the series of probabilities converges, the first Borel–Cantelli lemma applies, and the stated conclusion follows. The reasoning presented in this option is entirely correct.\n\n**B. Choose $n_k$ so that $\\mathbb{P}(A_{n_k})\\le 2^{-k}$, for example by ensuring $C\\,n_k^{-1/2}\\le 2^{-k}$. Such a choice makes $\\sum_k\\mathbb{P}(A_{n_k})\\infty$, and the first Borel–Cantelli lemma implies the desired almost sure statement along $(n_k)$.**\n\nThis option proposes a general strategy. If we can construct a subsequence $(n_k)$ that satisfies $\\mathbb{P}(A_{n_k})\\le 2^{-k}$, then the sum $\\sum_{k=1}^{\\infty} \\mathbb{P}(A_{n_k}) \\le \\sum_{k=1}^{\\infty} 2^{-k} = 1  \\infty$. The series converges, so the first Borel–Cantelli lemma applies, and the conclusion is valid. The key question is whether such a subsequence can be constructed. The option suggests satisfying the condition by choosing $n_k$ such that $C\\,n_k^{-1/2} \\le 2^{-k}$. Let's rearrange this inequality to solve for $n_k$:\n$$\nn_k^{1/2} \\ge C\\,2^k \\implies n_k \\ge (C\\,2^k)^2 = C^2\\,4^k\n$$\nFor any constant $C0$, we can always find a strictly increasing sequence of integers satisfying this condition, for example, $n_k = \\lceil C^2\\,4^k \\rceil$ for $k=1, 2, \\dots$. This defines a valid subsequence for which the condition holds. Thus, the strategy is sound and achievable.\n\n**C. Invoke the second Borel–Cantelli lemma to conclude $\\mathbb{P}(A_n\\ \\text{i.o.})=1$ without any independence assumptions, showing that no subsequence can improve the almost sure behavior.**\n\nThis reasoning is incorrect. The standard second Borel–Cantelli lemma requires the events to be independent, which is not the case here as the errors are driven by the same underlying random path. Furthermore, even if $\\mathbb{P}(A_n\\ \\text{i.o.})=1$ for the full sequence, this does not prevent a subsequence from having a different limiting behavior. As shown in options A and B, a sparse enough subsequence $(n_k)$ can have a summable series of probabilities, which implies $\\mathbb{P}(A_{n_k}\\ \\text{i.o.})=0$.\n\n**D. Take the linear subsequence $n_k=k$. Since $\\mathbb{P}(A_{n_k})\\le C\\,k^{-1/2}$, one has $\\sum_k\\mathbb{P}(A_{n_k})=\\infty$, so the first Borel–Cantelli lemma cannot be applied to obtain the desired almost sure statement along $(n_k)$.**\n\nThis option correctly analyzes a failing strategy. Taking $n_k=k$ (the full sequence), the probability bound $\\mathbb{P}(A_k) \\le Ck^{-1/2}$ is not sufficient to prove convergence of the sum, as $\\sum k^{-1/2}$ diverges. Since the question asks for a strategy that *succeeds*, this option is not a correct answer.\n\n**E. Take $n_k=\\lfloor k\\log k\\rfloor$. Since $\\mathbb{P}(A_{n_k})\\le C\\,(k\\log k)^{-1/2}$, the series $\\sum_k\\mathbb{P}(A_{n_k})$ diverges, so the first Borel–Cantelli lemma cannot be applied to obtain the desired almost sure statement along $(n_k)$.**\n\nThis option also correctly analyzes a failing strategy. The subsequence $n_k=\\lfloor k\\log k\\rfloor$ is not sparse enough. The corresponding series $\\sum (k\\log k)^{-1/2}$ diverges. Therefore, this is not a strategy that achieves the goal stated in the question.", "answer": "$$\\boxed{AB}$$", "id": "2991422"}, {"introduction": "We now synthesize these ideas to analyze the pathwise properties of solutions to a general Itô stochastic differential equation, where increments are typically dependent. This problem [@problem_id:2991399] requires a multi-step approach that is central to modern stochastic analysis. First, you will use powerful tools like the Burkholder-Davis-Gundy inequality to bound the moments of process increments, then apply Markov's inequality to obtain probability bounds, and finally, analyze the convergence of the resulting series to trigger the first Borel-Cantelli lemma. This practice illuminates the complete workflow for establishing almost sure properties for solutions of SDEs.", "problem": "Let $\\left(\\Omega,\\mathcal{F},(\\mathcal{F}_{t})_{t\\ge 0},\\mathbb{P}\\right)$ be a filtered probability space satisfying the usual conditions, and let $W$ be a one-dimensional standard Brownian motion. Consider the one-dimensional Itô stochastic differential equation (SDE)\n$$\ndX_{t}=b\\!\\left(X_{t}\\right)\\,dt+\\sigma\\!\\left(X_{t}\\right)\\,dW_{t},\\quad X_{0}\\in L^{p}(\\Omega),\n$$\nwhere $b,\\sigma:\\mathbb{R}\\to\\mathbb{R}$ are globally Lipschitz and bounded. Define a deterministic increasing sequence of times $(t_{n})_{n\\ge 0}$ by $t_{0}=0$ and\n$$\n\\Delta_{n}:=t_{n}-t_{n-1}=\\frac{1}{n\\,[\\ln(n+1)]^{2}},\\quad n\\ge 1,\n$$\nso that $T:=\\sum_{n=1}^{\\infty}\\Delta_{n}\\infty$ and $\\bigcup_{n\\ge 1}I_{n}=[0,T)$ with $I_{n}:=[t_{n-1},t_{n}]$. For fixed $\\varepsilon\\in(0,1]$, define the events\n$$\nA_{n}=\\Bigl\\{\\sup_{t\\in I_{n}}\\bigl|X_{t}-X_{t_{n-1}}\\bigr|\\varepsilon\\Bigr\\},\\quad n\\ge 1.\n$$\nUsing only first principles of stochastic calculus, standard martingale inequalities, and convergence criteria for series, determine the exact value of the infimum $p_{\\star}0$ such that for every $\\varepsilon\\in(0,1]$ and every solution $X$ with $\\mathbb{E}|X_{0}|^{p}\\infty$ for some $p\\ge p_{\\star}$, one has\n$$\n\\sum_{n=1}^{\\infty}\\mathbb{P}(A_n)\\infty.\n$$\nGive your final answer as a single real number $p_{\\star}$.", "solution": "The problem asks for the infimum $p_{\\star} > 0$ such that for any choice of SDE satisfying the given conditions and any initial condition $X_0$ with a finite $p$-th moment for some $p \\ge p_\\star$, the series $\\sum_{n=1}^{\\infty}\\mathbb{P}(A_n)$ converges.\n\nWe begin by estimating $\\mathbb{P}(A_n)$. Using Markov's inequality for any $p > 0$:\n$$\n\\mathbb{P}(A_n) = \\mathbb{P}\\left(\\sup_{t\\in I_{n}}\\bigl|X_{t}-X_{t_{n-1}}\\bigr|\\varepsilon\\right) = \\mathbb{P}\\left(\\sup_{t\\in I_{n}}\\bigl|X_{t}-X_{t_{n-1}}\\bigr|^p  \\varepsilon^p\\right) \\le \\frac{1}{\\varepsilon^p}\\mathbb{E}\\left[\\sup_{t\\in I_{n}}\\bigl|X_{t}-X_{t_{n-1}}\\bigr|^p\\right].\n$$\nTo ensure $\\sum_{n=1}^{\\infty}\\mathbb{P}(A_n)  \\infty$, it is sufficient to show that $\\sum_{n=1}^{\\infty}\\mathbb{E}\\left[\\sup_{t\\in I_{n}}\\bigl|X_{t}-X_{t_{n-1}}\\bigr|^p\\right]  \\infty$.\n\nLet us analyze the moment term. The solution to the SDE in integral form is\n$$\nX_t - X_{t_{n-1}} = \\int_{t_{n-1}}^t b(X_s)\\,ds + \\int_{t_{n-1}}^t \\sigma(X_s)\\,dW_s.\n$$\nUsing the triangle inequality, we have\n$$\n\\sup_{t\\in I_{n}}\\bigl|X_t - X_{t_{n-1}}\\bigr| \\le \\sup_{t\\in I_{n}}\\left|\\int_{t_{n-1}}^t b(X_s)\\,ds\\right| + \\sup_{t\\in I_{n}}\\left|\\int_{t_{n-1}}^t \\sigma(X_s)\\,dW_s\\right|.\n$$\nThe coefficients $b$ and $\\sigma$ are bounded, so there exist constants $K_b > 0$ and $K_\\sigma > 0$ such that $|b(x)| \\le K_b$ and $|\\sigma(x)| \\le K_\\sigma$ for all $x \\in \\mathbb{R}$.\n\nThe drift term is bounded as follows:\n$$\n\\sup_{t\\in I_{n}}\\left|\\int_{t_{n-1}}^t b(X_s)\\,ds\\right| \\le \\int_{t_{n-1}}^{t_n} |b(X_s)|\\,ds \\le K_b (t_n - t_{n-1}) = K_b \\Delta_n.\n$$\nFor the diffusion term, we use the Burkholder-Davis-Gundy (BDG) inequality. For any $p \\ge 1$, there exists a constant $C_p0$ such that for the continuous local martingale $M_t = \\int_{t_{n-1}}^t \\sigma(X_s)\\,dW_s$ (which is a true martingale as $\\sigma$ is bounded), we have:\n$$\n\\mathbb{E}\\left[\\sup_{t\\in I_{n}}\\left| M_t \\right|^p \\, \\bigg| \\, \\mathcal{F}_{t_{n-1}}\\right] \\le C_p \\mathbb{E}\\left[ \\langle M \\rangle_{t_n}^{p/2} \\, \\bigg| \\, \\mathcal{F}_{t_{n-1}}\\right].\n$$\nThe quadratic variation is $\\langle M \\rangle_{t_n} = \\int_{t_{n-1}}^{t_n} \\sigma^2(X_s)\\,ds$. Using the bound on $\\sigma$:\n$$\n\\langle M \\rangle_{t_n} \\le \\int_{t_{n-1}}^{t_n} K_\\sigma^2\\,ds = K_\\sigma^2 \\Delta_n.\n$$\nTherefore,\n$$\n\\mathbb{E}\\left[\\sup_{t\\in I_{n}}\\left| \\int_{t_{n-1}}^t \\sigma(X_s)\\,dW_s \\right|^p \\right] = \\mathbb{E}\\left[\\mathbb{E}\\left[\\sup_{t\\in I_{n}}|M_t|^p \\, \\bigg| \\, \\mathcal{F}_{t_{n-1}}\\right]\\right] \\le C_p \\mathbb{E}\\left[ (K_\\sigma^2 \\Delta_n)^{p/2} \\right] = C_p K_\\sigma^p (\\Delta_n)^{p/2}.\n$$\nNow, we combine the bounds for the drift and diffusion parts using the inequality $(a+b)^p \\le 2^{p-1}(a^p + b^p)$ for $p \\ge 1$:\n$$\n\\mathbb{E}\\left[\\sup_{t\\in I_{n}}\\bigl|X_{t}-X_{t_{n-1}}\\bigr|^p\\right] \\le 2^{p-1} \\left( (K_b \\Delta_n)^p + \\mathbb{E}\\left[\\sup_{t\\in I_{n}}\\left|\\int_{t_{n-1}}^t \\sigma(X_s)\\,dW_s\\right|^p\\right] \\right)\n$$\n$$\n\\le 2^{p-1} \\left( (K_b \\Delta_n)^p + C_p K_\\sigma^p (\\Delta_n)^{p/2} \\right).\n$$\nFor large $n$, $\\Delta_n \\to 0$. Since we are interested in $p>0$, we have $p > p/2$. Thus, the term $(\\Delta_n)^{p/2}$ dominates $(\\Delta_n)^p$. Consequently, there exists a constant $C  0$ (depending on $p, K_b, K_\\sigma, C_p$) such that for all sufficiently large $n$:\n$$\n\\mathbb{E}\\left[\\sup_{t\\in I_{n}}\\bigl|X_{t}-X_{t_{n-1}}\\bigr|^p\\right] \\le C (\\Delta_n)^{p/2}.\n$$\nThis bound holds irrespective of the finiteness of $\\mathbb{E}[|X_0|^p]$, due to the coefficients being bounded.\n\nNow, we examine the convergence of the series $\\sum_n \\mathbb{P}(A_n)$.\n$$\n\\sum_{n=1}^{\\infty} \\mathbb{P}(A_n) \\le \\sum_{n=1}^{\\infty} \\frac{C}{\\varepsilon^p} (\\Delta_n)^{p/2} = \\frac{C}{\\varepsilon^p} \\sum_{n=1}^{\\infty} \\left( \\frac{1}{n[\\ln(n+1)]^2} \\right)^{p/2} = \\frac{C}{\\varepsilon^p} \\sum_{n=1}^{\\infty} \\frac{1}{n^{p/2} [\\ln(n+1)]^p}.\n$$\nThis is a Bertrand-type series. For large $n$, $\\ln(n+1) \\sim \\ln n$. The convergence of this series $\\sum_{n=2}^{\\infty} \\frac{1}{n^\\alpha (\\ln n)^\\beta}$ depends on the values of $\\alpha=p/2$ and $\\beta=p$. The series converges if and only if ($\\alpha > 1$) or ($\\alpha=1$ and $\\beta > 1$).\n\nLet's check these conditions:\n1.  $\\alpha > 1 \\implies p/2 > 1 \\implies p > 2$.\n2.  $\\alpha = 1 \\implies p/2 = 1 \\implies p = 2$. In this case, we check the condition on $\\beta$. We have $\\beta = p = 2$. Since $2 > 1$, the condition $\\beta > 1$ is satisfied.\n\nCombining these two cases, the series converges for all $p \\ge 2$. Therefore, for any $p \\ge 2$, the condition $\\sum_{n=1}^{\\infty}\\mathbb{P}(A_n)  \\infty$ is satisfied for any SDE with bounded Lipschitz coefficients. This implies that $p_\\star \\le 2$.\n\nTo show that this bound is sharp, we need to show that for any $p  2$, there exists at least one SDE for which the sum diverges. The moment estimates derived from the BDG inequality are known to be sharp in their scaling with respect to the time interval. For $p  2$, we have $p/2  1$. The series $\\sum \\frac{1}{n^{p/2} [\\ln(n+1)]^p}$ diverges because the main power law term $n^{p/2}$ has an exponent less than $1$. It can be shown that for a simple SDE like $dX_t = dW_t$, the lower bound on the moments is of the same order, and consequently the sum of probabilities will diverge for $p2$. This implies that the condition is not guaranteed to hold for $p2$.\n\nThis implies that $p_\\star \\ge 2$. Combining our results, we conclude that the exact value is $p_\\star = 2$.", "answer": "$$\n\\boxed{2}\n$$", "id": "2991399"}]}