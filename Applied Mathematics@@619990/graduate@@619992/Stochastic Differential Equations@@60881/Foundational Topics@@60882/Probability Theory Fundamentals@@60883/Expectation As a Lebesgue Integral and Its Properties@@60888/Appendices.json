{"hands_on_practices": [{"introduction": "The expectation, as defined by the Lebesgue integral, is fundamentally built by approximating a general random variable with a sequence of simpler functions. This exercise ([@problem_id:2975011]) makes this abstract construction tangible by having you approximate a common random variable, $X=|W_1|$, with a specific sequence of bounded simple functions. By explicitly deriving a bound on the approximation error for the expectation, you will gain a deeper, hands-on appreciation for the mechanics of Lebesgue integration and the convergence concepts that underpin it.", "problem": "Let $\\left(\\Omega,\\mathcal{F},\\mathbb{P}\\right)$ be a complete probability space carrying a standard one-dimensional Brownian motion $\\left(W_{t}\\right)_{t\\geq 0}$ adapted to its natural (completed, right-continuous) filtration. Consider the integrable random variable $X:=|W_{1}|$.\n\nUsing only the definition of expectation as a Lebesgue integral and its basic properties such as linearity, monotonicity, and absolute homogeneity, construct a canonical sequence of bounded simple functions $\\{s_{n}\\}_{n\\in\\mathbb{N}}$ that approximates $X$ in the $L^{1}$ sense by truncation at level $n$ and quantization at mesh size $1/n$. Then, starting from first principles, derive an explicit deterministic bound for the expectation error in terms of the $L^{1}$ distance, and specialize it to $X=|W_{1}|$ by exploiting the standard normal distribution of $W_{1}$.\n\nConcretely, define\n$$\ns_{n}(\\omega):=\\min\\left\\{n,\\frac{\\lfloor n\\,X(\\omega)\\rfloor}{n}\\right\\},\\quad n\\in\\mathbb{N}.\n$$\nProceed as follows:\n\n1) Justify that each $s_{n}$ is a bounded simple function and that $s_{n}\\to X$ in $\\mathbb{P}$-probability.\n\n2) Prove that the expectation error satisfies\n$$\n\\left|\\mathbb{E}[X]-\\mathbb{E}[s_{n}]\\right|\\leq \\mathbb{E}\\left[|X-s_{n}|\\right]\\leq \\frac{1}{n}+\\mathbb{E}\\left[X\\,\\mathbf{1}_{\\{Xn\\}}\\right].\n$$\n\n3) For $X=|W_{1}|$, compute $\\mathbb{E}\\left[X\\,\\mathbf{1}_{\\{Xn\\}}\\right]$ explicitly in terms of elementary functions and constants, and combine this with the previous bound to obtain an explicit deterministic upper bound on $\\left|\\mathbb{E}[X]-\\mathbb{E}[s_{n}]\\right|$.\n\nWhat is the limit\n$$\n\\lim_{n\\to\\infty}\\left|\\mathbb{E}[X]-\\mathbb{E}[s_{n}]\\right|\\,?\n$$\nProvide your final answer as a single exact number. No rounding is required.", "solution": "The user wants me to solve a multi-part problem in probability theory.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n\n*   **Probability Space**: $\\left(\\Omega,\\mathcal{F},\\mathbb{P}\\right)$ is a complete probability space.\n*   **Stochastic Process**: $\\left(W_{t}\\right)_{t\\geq 0}$ is a standard one-dimensional Brownian motion adapted to its natural (completed, right-continuous) filtration.\n*   **Random Variable**: $X:=|W_{1}|$ is an integrable random variable.\n*   **Approximating Sequence**: A sequence of functions $\\{s_{n}\\}_{n\\in\\mathbb{N}}$ is defined as $s_{n}(\\omega):=\\min\\left\\{n,\\frac{\\lfloor n\\,X(\\omega)\\rfloor}{n}\\right\\}$.\n*   **Task 1**: Justify that each $s_{n}$ is a bounded simple function and that $s_{n}\\to X$ in $\\mathbb{P}$-probability.\n*   **Task 2**: Prove the inequality $\\left|\\mathbb{E}[X]-\\mathbb{E}[s_{n}]\\right|\\leq \\mathbb{E}\\left[|X-s_{n}|\\right]\\leq \\frac{1}{n}+\\mathbb{E}\\left[X\\,\\mathbf{1}_{\\{Xn\\}}\\right]$.\n*   **Task 3**: For $X=|W_{1}|$, compute $\\mathbb{E}\\left[X\\,\\mathbf{1}_{\\{Xn\\}}\\right]$ and derive an explicit deterministic upper bound on $\\left|\\mathbb{E}[X]-\\mathbb{E}[s_{n}]\\right|$.\n*   **Task 4**: Compute the limit $\\lim_{n\\to\\infty}\\left|\\mathbb{E}[X]-\\mathbb{E}[s_{n}]\\right|$.\n\n**Step 2: Validate Using Extracted Givens**\n\nThe problem is scientifically grounded in standard probability theory and the theory of stochastic processes. All concepts used—Brownian motion, expectation, Lebesgue integration, simple functions, convergence in probability—are well-defined and standard. The construction of the sequence $\\{s_{n}\\}$ is a classic technique in measure theory for approximating a non-negative measurable function. The random variable $X = |W_1|$ is a folded standard normal variable, whose properties are well-known and which is indeed integrable. The problem is well-posed, with a clear structure and a sequence of tasks that lead to a unique, verifiable solution. The language is formal and objective. The problem is self-contained and does not violate any consistency or feasibility criteria.\n\n**Step 3: Verdict and Action**\n\nThe problem is valid. A complete solution will be provided.\n\n### Solution\n\nThe problem asks for a multi-step analysis of the approximation of an integrable random variable $X=|W_1|$ by a sequence of simple functions $s_n$.\n\n**1) Justification of $s_n$ properties and convergence**\n\nFirst, we show that for each $n\\in\\mathbb{N}$, $s_n$ is a bounded simple function.\nA function is simple if it is measurable and takes only a finite number of values.\nThe function $X = |W_1|$ is a continuous function of $W_1$, which is $\\mathcal{F}$-measurable, so $X$ is $\\mathcal{F}$-measurable.\nLet's analyze the values taken by $s_n(\\omega)$. Let $X'_n(\\omega) = \\frac{\\lfloor nX(\\omega) \\rfloor}{n}$. We have $s_n(\\omega) = \\min\\{n, X'_n(\\omega)\\}$.\nLet's determine when $s_n(\\omega)=n$. This happens if and only if $X'_n(\\omega) \\ge n$, which is equivalent to $\\frac{\\lfloor nX(\\omega) \\rfloor}{n} \\ge n$, or $\\lfloor nX(\\omega) \\rfloor \\ge n^2$. Since $n^2$ is an integer, this is equivalent to $nX(\\omega) \\ge n^2$, which simplifies to $X(\\omega) \\ge n$.\nSo, $s_n(\\omega) = n$ if $X(\\omega) \\ge n$.\nIf $X(\\omega)  n$, then $nX(\\omega)  n^2$. This implies $\\lfloor nX(\\omega) \\rfloor \\le n^2-1$, so $X'_n(\\omega) = \\frac{\\lfloor nX(\\omega) \\rfloor}{n} \\le \\frac{n^2-1}{n}  n$. In this case, $s_n(\\omega) = X'_n(\\omega) = \\frac{\\lfloor nX(\\omega) \\rfloor}{n}$.\nThe values taken by $s_n$ are thus of the form $k/n$ for integers $k = \\lfloor nX(\\omega) \\rfloor$ where $X(\\omega)n$, plus the value $n$.\nSince $X(\\omega)  n$, $0 \\le nX(\\omega)  n^2$, so $k$ can take any integer value in $\\{0, 1, \\dots, n^2-1\\}$.\nThe set of possible values for $s_n$ is therefore $\\{ \\frac{k}{n} \\mid k \\in \\{0, 1, \\dots, n^2-1\\} \\} \\cup \\{n\\}$, which is a finite set.\nThe preimages are measurable. For $k \\in \\{0, 1, \\dots, n^2-1\\}$, the set $\\{\\omega \\mid s_n(\\omega) = k/n\\} = \\{\\omega \\mid X(\\omega)  n \\text{ and } \\lfloor nX(\\omega) \\rfloor = k\\} = X^{-1}([k/n, (k+1)/n))$, which is in $\\mathcal{F}$. The set $\\{\\omega \\mid s_n(\\omega) = n\\} = \\{\\omega \\mid X(\\omega) \\ge n\\} = X^{-1}([n, \\infty))$, which is also in $\\mathcal{F}$.\nThus, $s_n$ is a simple function. By definition, $s_n(\\omega) = \\min\\{n, \\frac{\\lfloor nX(\\omega) \\rfloor}{n}\\} \\le n$. Also, $X(\\omega)=|W_1(\\omega)|\\ge 0$, so $\\lfloor nX(\\omega) \\rfloor \\ge 0$, and $s_n(\\omega) \\ge 0$. Therefore, $0 \\le s_n(\\omega) \\le n$, so $s_n$ is bounded.\n\nNext, we show that $s_n \\to X$ in $\\mathbb{P}$-probability. We must show that for any $\\epsilon  0$, $\\lim_{n\\to\\infty} \\mathbb{P}(|X-s_n|  \\epsilon) = 0$.\nLet's analyze the difference $|X-s_n|$.\nFrom the property $y-1  \\lfloor y \\rfloor \\le y$, we have $nX-1  \\lfloor nX \\rfloor \\le nX$. Dividing by $n$ gives $X - 1/n  \\frac{\\lfloor nX \\rfloor}{n} \\le X$.\nIf $X(\\omega)  n$, then $s_n(\\omega) = \\frac{\\lfloor nX(\\omega) \\rfloor}{n}$, and we have $0 \\le X(\\omega) - s_n(\\omega)  1/n$.\nIf $X(\\omega) \\ge n$, then $s_n(\\omega) = n$, and $|X(\\omega)-s_n(\\omega)| = X(\\omega)-n \\ge 0$.\nLet $\\epsilon  0$ be given. For any $n  1/\\epsilon$, we have $1/n  \\epsilon$.\nWe partition the event $\\{|X-s_n|  \\epsilon\\}$:\n$$\n\\mathbb{P}(|X-s_n|  \\epsilon) = \\mathbb{P}(\\{|X-s_n|  \\epsilon\\} \\cap \\{Xn\\}) + \\mathbb{P}(\\{|X-s_n|  \\epsilon\\} \\cap \\{X \\ge n\\})\n$$\nOn the set $\\{Xn\\}$, we have $|X-s_n|  1/n$. For $n1/\\epsilon$, this means $|X-s_n|  \\epsilon$. Thus, the event $\\{|X-s_n|  \\epsilon\\} \\cap \\{Xn\\}$ is empty, and its probability is $0$.\nSo, for $n1/\\epsilon$,\n$$\n\\mathbb{P}(|X-s_n|  \\epsilon) = \\mathbb{P}(\\{|X-s_n|  \\epsilon\\} \\cap \\{X \\ge n\\})\n$$\nOn the set $\\{X \\ge n\\}$, we have $s_n=n$, so $|X-s_n| = X-n$. The event becomes $\\{X-n  \\epsilon\\} \\cap \\{X \\ge n\\}$, which is equivalent to $\\{X  n+\\epsilon\\}$.\nSo, $\\mathbb{P}(|X-s_n|  \\epsilon) = \\mathbb{P}(X  n+\\epsilon)$.\nSince $X$ is an integrable random variable, $\\mathbb{E}[X]  \\infty$. For any non-negative integrable random variable, $\\mathbb{E}[X]=\\int_0^\\infty \\mathbb{P}(Xt)dt$. The convergence of this integral implies that the integrand must tend to zero, i.e., $\\lim_{t\\to\\infty} \\mathbb{P}(Xt) = 0$.\nTherefore, $\\lim_{n\\to\\infty} \\mathbb{P}(X  n+\\epsilon) = 0$. This confirms that $s_n \\to X$ in $\\mathbb{P}$-probability.\n\n**2) Proof of the expectation error bound**\n\nFirst, we establish `$\\left|\\mathbb{E}[X]-\\mathbb{E}[s_{n}]\\right|\\leq \\mathbb{E}\\left[|X-s_{n}|\\right]$`.\nBy the triangle inequality for integrals (a property of expectation), for any integrable random variable $Y$, $|\\mathbb{E}[Y]| \\le \\mathbb{E}[|Y|]$. Let $Y=X-s_n$. Then $|\\mathbb{E}[X-s_n]| \\le \\mathbb{E}[|X-s_n|]$. By linearity of expectation, $\\mathbb{E}[X-s_n]=\\mathbb{E}[X]-\\mathbb{E}[s_n]$. So, $|\\mathbb{E}[X]-\\mathbb{E}[s_n]| \\le \\mathbb{E}[|X-s_n|]$.\nIn fact, we can show equality. As demonstrated in part 1), $s_n(\\omega) \\le X(\\omega)$ for all $\\omega$. This is because if $X(\\omega)  n$, $s_n(\\omega) = \\frac{\\lfloor nX(\\omega) \\rfloor}{n} \\le X(\\omega)$, and if $X(\\omega) \\ge n$, $s_n(\\omega)=n \\le X(\\omega)$.\nSince $s_n \\le X$, by monotonicity of expectation, $\\mathbb{E}[s_n] \\le \\mathbb{E}[X]$.\nThus, $|X-s_n| = X-s_n$ and $|\\mathbb{E}[X]-\\mathbb{E}[s_n]| = \\mathbb{E}[X]-\\mathbb{E}[s_n]$.\nSo, $|\\mathbb{E}[X]-\\mathbb{E}[s_n]| = \\mathbb{E}[X-s_n] = \\mathbb{E}[|X-s_n|]$. The first inequality is in fact an equality.\n\nNext, we prove `$\\mathbb{E}\\left[|X-s_{n}|\\right]\\leq \\frac{1}{n}+\\mathbb{E}\\left[X\\,\\mathbf{1}_{\\{Xn\\}}\\right]$`.\nSince $|X-s_n|=X-s_n$, we analyze $\\mathbb{E}[X-s_n]$. We decompose the expectation over the partition $\\Omega=\\{X \\le n\\} \\cup \\{X  n\\}$.\n$$\n\\mathbb{E}[X-s_n] = \\mathbb{E}[(X-s_n)\\mathbf{1}_{\\{X \\le n\\}}] + \\mathbb{E}[(X-s_n)\\mathbf{1}_{\\{X  n\\}}]\n$$\nFor the first term, on the set $\\{X \\le n\\}$, we have $s_n = \\frac{\\lfloor nX \\rfloor}{n}$, and $0 \\le X-s_n  1/n$.\nSo, $(X-s_n)\\mathbf{1}_{\\{X \\le n\\}} \\le \\frac{1}{n}\\mathbf{1}_{\\{X \\le n\\}}$. By monotonicity of expectation:\n$$\n\\mathbb{E}[(X-s_n)\\mathbf{1}_{\\{X \\le n\\}}] \\leq \\mathbb{E}[\\frac{1}{n}\\mathbf{1}_{\\{X \\le n\\}}] = \\frac{1}{n}\\mathbb{P}(X \\le n) \\le \\frac{1}{n}\n$$\nFor the second term, on the set $\\{X  n\\}$, we have $s_n=n$. Thus $X-s_n=X-n$.\n$$\n\\mathbb{E}[(X-s_n)\\mathbf{1}_{\\{X  n\\}}] = \\mathbb{E}[(X-n)\\mathbf{1}_{\\{X  n\\}}]\n$$\nBy linearity of expectation, this is equal to $\\mathbb{E}[X\\mathbf{1}_{\\{X  n\\}}] - n\\mathbb{P}(X  n)$. Since $n\\mathbb{P}(X  n) \\ge 0$, we have $\\mathbb{E}[(X-n)\\mathbf{1}_{\\{X  n\\}}] \\le \\mathbb{E}[X\\mathbf{1}_{\\{X  n\\}}]$.\nCombining the two bounds:\n$$\n\\mathbb{E}[X-s_n] \\le \\frac{1}{n} + \\mathbb{E}[(X-n)\\mathbf{1}_{\\{X  n\\}}] = \\frac{1}{n} + \\mathbb{E}[X\\mathbf{1}_{\\{X  n\\}}] - n\\mathbb{P}(X  n)\n$$\nSince $n\\mathbb{P}(X  n) \\ge 0$, we can write the final inequality:\n$$\n\\mathbb{E}[X-s_n] \\le \\frac{1}{n} + \\mathbb{E}[X\\mathbf{1}_{\\{X  n\\}}]\n$$\nCombining the steps proves the desired inequality. Note that since $X=|W_1|$ is a continuous random variable, $\\mathbb{P}(X=n)=0$, so conditioning on $\\{Xn\\}$ or $\\{X \\ge n\\}$ yields the same expectation.\n\n**3) Explicit bound for $X=|W_1|$**\n\nWe are given $X=|W_1|$. $W_1$ is a standard normal random variable, $W_1 \\sim N(0,1)$, with probability density function (PDF) $\\phi(z)=\\frac{1}{\\sqrt{2\\pi}}\\exp(-z^2/2)$.\nThe random variable $X = |W_1|$ has a PDF $f_X(x)$ for $x \\ge 0$ given by:\n$$\nf_X(x) = \\frac{d}{dx}\\mathbb{P}(X \\le x) = \\frac{d}{dx}\\mathbb{P}(-x \\le W_1 \\le x) = \\frac{d}{dx}(2\\Phi(x)-1) = 2\\phi(x) = \\sqrt{\\frac{2}{\\pi}}\\exp(-x^2/2)\n$$\nWe need to compute $\\mathbb{E}[X\\mathbf{1}_{\\{Xn\\}}]$. This is given by the integral:\n$$\n\\mathbb{E}[X\\mathbf{1}_{\\{Xn\\}}] = \\int_n^\\infty x f_X(x) dx = \\int_n^\\infty x \\sqrt{\\frac{2}{\\pi}}\\exp(-x^2/2) dx\n$$\n$$\n\\mathbb{E}[X\\mathbf{1}_{\\{Xn\\}}] = \\sqrt{\\frac{2}{\\pi}} \\int_n^\\infty x \\exp(-x^2/2) dx\n$$\nLet $u=x^2/2$, so $du = x dx$. The limits of integration become $n^2/2$ and $\\infty$.\n$$\n\\int_n^\\infty x \\exp(-x^2/2) dx = \\int_{n^2/2}^\\infty \\exp(-u) du = [-\\exp(-u)]_{n^2/2}^\\infty = 0 - (-\\exp(-n^2/2)) = \\exp(-n^2/2)\n$$\nSubstituting this back, we get:\n$$\n\\mathbb{E}[X\\mathbf{1}_{\\{Xn\\}}] = \\sqrt{\\frac{2}{\\pi}}\\exp(-n^2/2)\n$$\nNow, we substitute this result into the inequality from part 2):\n$$\n\\left|\\mathbb{E}[X]-\\mathbb{E}[s_{n}]\\right| \\leq \\frac{1}{n} + \\sqrt{\\frac{2}{\\pi}}\\exp(-n^2/2)\n$$\nThis is the explicit deterministic upper bound on the expectation error.\n\n**4) The limit of the expectation error**\n\nWe want to compute $\\lim_{n\\to\\infty}\\left|\\mathbb{E}[X]-\\mathbb{E}[s_{n}]\\right|$.\nFrom part 2) and 3), we have the following bounds for the error:\n$$\n0 \\le \\left|\\mathbb{E}[X]-\\mathbb{E}[s_{n}]\\right| \\leq \\frac{1}{n} + \\sqrt{\\frac{2}{\\pi}}\\exp(-n^2/2)\n$$\nWe take the limit of the upper bound as $n\\to\\infty$:\n$$\n\\lim_{n\\to\\infty} \\left( \\frac{1}{n} + \\sqrt{\\frac{2}{\\pi}}\\exp(-n^2/2) \\right) = \\left(\\lim_{n\\to\\infty} \\frac{1}{n}\\right) + \\sqrt{\\frac{2}{\\pi}}\\left(\\lim_{n\\to\\infty} \\exp(-n^2/2)\\right)\n$$\nThe first limit is $\\lim_{n\\to\\infty} \\frac{1}{n} = 0$.\nThe second limit is $\\lim_{n\\to\\infty} \\exp(-n^2/2) = 0$, because $-n^2/2 \\to -\\infty$ as $n\\to\\infty$.\nSo, the limit of the upper bound is $0 + \\sqrt{\\frac{2}{\\pi}} \\cdot 0 = 0$.\nBy the Squeeze Theorem, since the expectation error is non-negative and is bounded above by a sequence that converges to $0$, the limit of the error itself must be $0$.\n$$\n\\lim_{n\\to\\infty}\\left|\\mathbb{E}[X]-\\mathbb{E}[s_{n}]\\right| = 0\n$$", "answer": "$$\\boxed{0}$$", "id": "2975011"}, {"introduction": "The identity $\\mathbb{E}[XY] = \\mathbb{E}[X]\\mathbb{E}[Y]$ for independent random variables is a cornerstone of probability, but its application requires a crucial, and sometimes overlooked, precondition. This exercise ([@problem_id:2975022]) uses the classic example of independent Cauchy random variables to demonstrate what happens when this condition—integrability—is not met. Working through this problem will emphasize the importance of verifying the assumptions of theorems and provide a memorable counterexample that clarifies why the integrability of the random variables is essential.", "problem": "Let $(\\Omega,\\mathcal{F},\\mathbb{P})$ be a probability space supporting two independent random variables $U$ and $V$, each uniformly distributed on $(0,1)$. Define $X$ and $Y$ by $X=\\tan\\big(\\pi(U-\\tfrac{1}{2})\\big)$ and $Y=\\tan\\big(\\pi(V-\\tfrac{1}{2})\\big)$. The distributions of $X$ and $Y$ are standard Cauchy, and $X$ and $Y$ are independent. \n\nUsing the definition of expectation as a Lebesgue integral, do the following:\n\n1. Starting from the definition $\\mathbb{E}[Z]=\\int_{\\Omega}Z\\,\\mathrm{d}\\mathbb{P}$ for integrable random variables $Z$, and the decomposition $Z=Z^{+}-Z^{-}$ with $Z^{+}=\\max\\{Z,0\\}$ and $Z^{-}=\\max\\{-Z,0\\}$, show that neither $\\mathbb{E}[X]$ nor $\\mathbb{E}[Y]$ is defined as a finite Lebesgue integral (that is, $X\\notin L^{1}(\\mathbb{P})$ and $Y\\notin L^{1}(\\mathbb{P})$).\n\n2. Show that the product $XY$ is not integrable in the Lebesgue sense, i.e., $\\mathbb{E}[|XY|]=\\infty$, so $\\mathbb{E}[XY]$ is not a finite Lebesgue integral. Conclude that the identity $\\mathbb{E}[XY]=\\mathbb{E}[X]\\mathbb{E}[Y]$ fails here due to the lack of integrability of the product.\n\n3. Identify and justify sufficient integrability conditions under which, for independent random variables $X$ and $Y$, the equality $\\mathbb{E}[XY]=\\mathbb{E}[X]\\mathbb{E}[Y]$ holds in the Lebesgue sense. Your justification must proceed from first principles (definition of expectation as a Lebesgue integral, independence, positive/negative parts, and Fubini–Tonelli properties), without invoking pre-packaged shortcut formulas.\n\n4. Define truncated variables $X^{(1)}=X\\mathbf{1}_{\\{|X|\\leq 1\\}}$ and $Y^{(1)}=Y\\mathbf{1}_{\\{|Y|\\leq 1\\}}$. Compute the product $\\mathbb{E}[X^{(1)}]\\mathbb{E}[Y^{(1)}]$ explicitly. Express your final answer as a single real number. No rounding is required.", "solution": "The user-provided problem is first validated for correctness and solvability.\n\n### Step 1: Extract Givens\n- A probability space $(\\Omega,\\mathcal{F},\\mathbb{P})$.\n- Two independent random variables $U$ and $V$, each uniformly distributed on the interval $(0,1)$.\n- Definition of random variables $X$ and $Y$: $X=\\tan\\big(\\pi(U-\\frac{1}{2})\\big)$ and $Y=\\tan\\big(\\pi(V-\\frac{1}{2})\\big)$.\n- Stated properties: $X$ and $Y$ have standard Cauchy distributions and are independent.\n- The definition of expectation for an integrable random variable $Z$ is $\\mathbb{E}[Z]=\\int_{\\Omega}Z\\,\\mathrm{d}\\mathbb{P}$, based on the decomposition $Z=Z^{+}-Z^{-}$ where $Z^{+}=\\max\\{Z,0\\}$ and $Z^{-}=\\max\\{-Z,0\\}$, with integrability meaning $\\mathbb{E}[|Z|]  \\infty$.\n- Four tasks are specified:\n    1. Show that $X \\notin L^1(\\mathbb{P})$ and $Y \\notin L^1(\\mathbb{P})$.\n    2. Show that $\\mathbb{E}[|XY|]=\\infty$ and conclude that the identity $\\mathbb{E}[XY]=\\mathbb{E}[X]\\mathbb{E}[Y]$ fails.\n    3. Justify sufficient integrability conditions for $\\mathbb{E}[XY]=\\mathbb{E}[X]\\mathbb{E}[Y]$ to hold for independent random variables, from first principles.\n    4. Define truncated variables $X^{(1)}=X\\mathbf{1}_{\\{|X|\\leq 1\\}}$ and $Y^{(1)}=Y\\mathbf{1}_{\\{|Y|\\leq 1\\}}$, and compute $\\mathbb{E}[X^{(1)}]\\mathbb{E}[Y^{(1)}]$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is mathematically well-defined and self-contained. It is grounded in the standard framework of measure-theoretic probability. The transformation from a uniform distribution to a Cauchy distribution via $X = \\tan(\\pi(U - 1/2))$ is the correct inverse transform sampling method for the standard Cauchy CDF $F(x) = \\frac{1}{\\pi}\\arctan(x) + \\frac{1}{2}$. The questions posed are standard but require a rigorous, first-principles approach, which tests fundamental understanding of Lebesgue integration in probability. The problem does not violate any scientific principles, is not ambiguous, and leads to unique, meaningful results. It is not trivial due to the requirement for proofs from first principles.\n\n### Step 3: Verdict and Action\nThe problem is deemed **valid**. A full solution will be provided.\n\n---\n\n### Part 1: Non-integrability of $X$ and $Y$\n\nA random variable $Z$ is Lebesgue integrable, denoted $Z \\in L^1(\\mathbb{P})$, if its expected absolute value is finite, i.e., $\\mathbb{E}[|Z|]  \\infty$. The expectation $\\mathbb{E}[Z]$ is then defined as $\\mathbb{E}[Z] = \\mathbb{E}[Z^+] - \\mathbb{E}[Z^-]$, where both $\\mathbb{E}[Z^+]$ and $\\mathbb{E}[Z^-]$ are finite. We must show that for $X$, at least one of these is infinite.\n\nBy the law of the unconscious statistician (or change of variables formula), for a function $g$ of a random variable $U$ with probability density function $f_U(u)$, the expectation of $g(U)$ is $\\mathbb{E}[g(U)]=\\int_{-\\infty}^{\\infty} g(u)f_U(u)\\,\\mathrm{d}u$. Since $U$ is uniform on $(0,1)$, its PDF is $f_U(u)=1$ for $u \\in (0,1)$ and $0$ otherwise.\nHere, $X=g(U)$ with $g(u) = \\tan(\\pi(u-1/2))$.\nThe expected absolute value of $X$ is:\n$$ \\mathbb{E}[|X|] = \\int_0^1 \\left|\\tan\\left(\\pi\\left(u - \\frac{1}{2}\\right)\\right)\\right| \\, \\mathrm{d}u $$\nThe argument of the tangent function, $\\theta(u) = \\pi(u-1/2)$, ranges from $-\\pi/2$ to $\\pi/2$ as $u$ ranges from $0$ to $1$. The tangent is negative for $u \\in (0,1/2)$ and positive for $u \\in (1/2,1)$. Thus we can split the integral:\n$$ \\mathbb{E}[|X|] = \\int_0^{1/2} -\\tan\\left(\\pi\\left(u - \\frac{1}{2}\\right)\\right) \\, \\mathrm{d}u + \\int_{1/2}^1 \\tan\\left(\\pi\\left(u - \\frac{1}{2}\\right)\\right) \\, \\mathrm{d}u $$\nThese two terms correspond to $\\mathbb{E}[X^-]$ and $\\mathbb{E}[X^+]$ respectively. Let's evaluate the first integral, $\\mathbb{E}[X^-]$. We use the substitution $v = \\pi(u - 1/2)$, which gives $\\mathrm{d}v = \\pi \\, \\mathrm{d}u$. When $u=0$, $v=-\\pi/2$. When $u=1/2$, $v=0$.\n$$ \\mathbb{E}[X^-] = \\int_{-\\pi/2}^0 -\\tan(v) \\, \\frac{\\mathrm{d}v}{\\pi} = \\frac{1}{\\pi} \\int_{-\\pi/2}^0 -\\frac{\\sin(v)}{\\cos(v)} \\, \\mathrm{d}v $$\nThe antiderivative of $-\\tan(v)$ is $\\ln|\\cos(v)|$.\n$$ \\mathbb{E}[X^-] = \\frac{1}{\\pi} \\left[ \\ln(\\cos(v)) \\right]_{v \\to -\\pi/2^+}^{0} = \\frac{1}{\\pi} \\left( \\ln(\\cos(0)) - \\lim_{v \\to -\\pi/2^+} \\ln(\\cos(v)) \\right) $$\nSince $\\cos(0)=1$, $\\ln(\\cos(0))=0$. As $v \\to -\\pi/2^+$, $\\cos(v) \\to 0^+$, and thus $\\ln(\\cos(v)) \\to -\\infty$.\n$$ \\mathbb{E}[X^-] = \\frac{1}{\\pi} (0 - (-\\infty)) = \\infty $$\nSimilarly, the integral for $\\mathbb{E}[X^+]$ also evaluates to $\\infty$.\nSince $\\mathbb{E}[|X|] = \\mathbb{E}[X^+] + \\mathbb{E}[X^-] = \\infty + \\infty = \\infty$, $X$ is not Lebesgue integrable, i.e., $X \\notin L^1(\\mathbb{P})$.\nThe expectation $\\mathbb{E}[X]$ would be of the form $\\infty - \\infty$, which is undefined in Lebesgue integration theory. The argument for $Y$ is identical, so $Y \\notin L^1(\\mathbb{P})$.\n\n### Part 2: Non-integrability of the product $XY$\n\nTo check if $XY$ is integrable, we must compute $\\mathbb{E}[|XY|]$. Since $X$ and $Y$ are independent, their absolute values $|X|$ and $|Y|$ are also independent. For independent non-negative random variables, Tonelli's theorem states that the expectation of the product is the product of the expectations.\n$$ \\mathbb{E}[|XY|] = \\mathbb{E}[|X||Y|] = \\mathbb{E}[|X|] \\mathbb{E}[|Y|] $$\nFrom Part 1, we established that $\\mathbb{E}[|X|]=\\infty$ and $\\mathbb{E}[|Y|]=\\infty$. Therefore,\n$$ \\mathbb{E}[|XY|] = \\infty \\cdot \\infty = \\infty $$\nSince $\\mathbb{E}[|XY|]$ is not finite, the product $XY$ is not Lebesgue integrable, $XY \\notin L^1(\\mathbb{P})$. Consequently, its expectation $\\mathbb{E}[XY]$ is not defined as a finite real number.\n\nThe rule $\\mathbb{E}[XY] = \\mathbb{E}[X]\\mathbb{E}[Y]$ is established under the condition that $X$ and $Y$ are integrable. Here, this condition is violated. The terms $\\mathbb{E}[X]$ and $\\mathbb{E}[Y]$ are not well-defined finite values, so the expression on the right-hand side is meaningless. The left-hand side, $\\mathbb{E}[XY]$, is also undefined. Therefore, the identity fails because the quantities involved are not defined in the sense of Lebesgue integration.\n\n### Part 3: Sufficient Conditions for $\\mathbb{E}[XY] = \\mathbb{E}[X]\\mathbb{E}[Y]$\n\nFor independent random variables $X$ and $Y$, a sufficient condition for the equality $\\mathbb{E}[XY] = \\mathbb{E}[X]\\mathbb{E}[Y]$ to hold is that both $X$ and $Y$ are integrable, i.e., $X \\in L^1(\\mathbb{P})$ and $Y \\in L^1(\\mathbb{P})$, which means $\\mathbb{E}[|X|]  \\infty$ and $\\mathbb{E}[|Y|]  \\infty$.\n\nHere is the justification from first principles:\n1.  **Simple Functions:** Let $X = \\sum_{i=1}^n a_i \\mathbf{1}_{A_i}$ and $Y = \\sum_{j=1}^m b_j \\mathbf{1}_{B_j}$ be independent simple random variables, where $\\{A_i\\}$ and $\\{B_j\\}$ are finite partitions of $\\Omega$. By definition of expectation for simple functions, $\\mathbb{E}[X] = \\sum_i a_i \\mathbb{P}(A_i)$ and $\\mathbb{E}[Y] = \\sum_j b_j \\mathbb{P}(B_j)$. The product is $XY = \\sum_{i,j} a_i b_j \\mathbf{1}_{A_i \\cap B_j}$. Its expectation is $\\mathbb{E}[XY] = \\sum_{i,j} a_i b_j \\mathbb{P}(A_i \\cap B_j)$. Since $X$ and $Y$ are independent, the events $A_i \\in \\sigma(X)$ and $B_j \\in \\sigma(Y)$ are independent, so $\\mathbb{P}(A_i \\cap B_j) = \\mathbb{P}(A_i)\\mathbb{P}(B_j)$.\n    $$ \\mathbb{E}[XY] = \\sum_{i,j} a_i b_j \\mathbb{P}(A_i)\\mathbb{P}(B_j) = \\left(\\sum_i a_i \\mathbb{P}(A_i)\\right) \\left(\\sum_j b_j \\mathbb{P}(B_j)\\right) = \\mathbb{E}[X]\\mathbb{E}[Y] $$\n2.  **Non-negative Functions:** Let $X \\geq 0$ and $Y \\geq 0$ be independent. There exist sequences of non-negative simple functions $X_n \\uparrow X$ and $Y_n \\uparrow Y$. For each $n$, $X_n$ and $Y_n$ are independent, so from step 1, $\\mathbb{E}[X_n Y_n] = \\mathbb{E}[X_n]\\mathbb{E}[Y_n]$. The sequence of non-negative functions $X_n Y_n$ increases to $XY$. By the Monotone Convergence Theorem (MCT), $\\lim_{n \\to \\infty} \\mathbb{E}[X_n] = \\mathbb{E}[X]$, $\\lim_{n \\to \\infty} \\mathbb{E}[Y_n] = \\mathbb{E}[Y]$, and $\\lim_{n \\to \\infty} \\mathbb{E}[X_n Y_n] = \\mathbb{E}[XY]$. Taking the limit of the equality gives $\\mathbb{E}[XY] = \\mathbb{E}[X]\\mathbb{E}[Y]$. This holds even if the expectations are infinite.\n\n3.  **Integrable Functions:** Let $X, Y \\in L^1(\\mathbb{P})$ be independent. This means $\\mathbb{E}[|X|]  \\infty$ and $\\mathbb{E}[|Y|]  \\infty$. The variables $|X|$ and $|Y|$ are non-negative and independent, so by step 2, $\\mathbb{E}[|XY|] = \\mathbb{E}[|X||Y|] = \\mathbb{E}[|X|]\\mathbb{E}[|Y|]$. Since $\\mathbb{E}[|X|]$ and $\\mathbb{E}[|Y|]$ are finite, their product is finite, establishing that $XY$ is integrable ($XY \\in L^1(\\mathbb{P})$).\n    Now decompose $X$ and $Y$ into their positive and negative parts: $X = X^+ - X^-$ and $Y = Y^+ - Y^-$. Because $X$ and $Y$ are integrable, all four components $X^+, X^-, Y^+, Y^-$ have finite expectations. The product is $XY = X^+Y^+ - X^+Y^- - X^-Y^+ + X^-Y^-$.\n    Since $XY$ is integrable, we can apply linearity of expectation:\n    $$ \\mathbb{E}[XY] = \\mathbb{E}[X^+Y^+] - \\mathbb{E}[X^+Y^-] - \\mathbb{E}[X^-Y^+] + \\mathbb{E}[X^-Y^-] $$\n    Each pair on the right, e.g., $(X^+, Y^+)$, consists of independent non-negative random variables. Applying the result from step 2 to each term:\n    $$ \\mathbb{E}[XY] = \\mathbb{E}[X^+]\\mathbb{E}[Y^+] - \\mathbb{E}[X^+]\\mathbb{E}[Y^-] - \\mathbb{E}[X^-]\\mathbb{E}[Y^+] + \\mathbb{E}[X^-]\\mathbb{E}[Y^-] $$\n    Factoring the terms on the right-hand side:\n    $$ \\mathbb{E}[XY] = (\\mathbb{E}[X^+] - \\mathbb{E}[X^-])(\\mathbb{E}[Y^+] - \\mathbb{E}[Y^-]) $$\n    By definition of expectation for integrable variables, $\\mathbb{E}[X] = \\mathbb{E}[X^+] - \\mathbb{E}[X^-]$ and $\\mathbb{E}[Y] = \\mathbb{E}[Y^+] - \\mathbb{E}[Y^-]$.\n    Thus, we conclude $\\mathbb{E}[XY] = \\mathbb{E}[X]\\mathbb{E}[Y]$.\n\n### Part 4: Computation for Truncated Variables\n\nWe are asked to compute $\\mathbb{E}[X^{(1)}]\\mathbb{E}[Y^{(1)}]$, where $X^{(1)}=X\\mathbf{1}_{\\{|X|\\leq 1\\}}$ and $Y^{(1)}=Y\\mathbf{1}_{\\{|Y|\\leq 1\\}}$.\nSince $X$ and $Y$ are identically distributed, so are $X^{(1)}$ and $Y^{(1)}$. Thus $\\mathbb{E}[X^{(1)}] = \\mathbb{E}[Y^{(1)}]$, and we only need to compute one of them.\nLet's compute $\\mathbb{E}[X^{(1)}]$:\n$$ \\mathbb{E}[X^{(1)}] = \\mathbb{E}[X \\mathbf{1}_{\\{|X| \\le 1\\}}] $$\nUsing the change of variables formula with $X = \\tan(\\pi(U-1/2))$:\n$$ \\mathbb{E}[X^{(1)}] = \\int_0^1 \\tan\\left(\\pi\\left(u-\\frac{1}{2}\\right)\\right) \\mathbf{1}_{\\{|\\tan(\\pi(u-1/2))| \\le 1\\}} \\, \\mathrm{d}u $$\nLet $\\theta = \\pi(u-1/2)$. For $u \\in (0,1)$, $\\theta$ is in $(-\\pi/2, \\pi/2)$. The condition $|\\tan(\\theta)| \\le 1$ is satisfied for $\\theta \\in [-\\pi/4, \\pi/4]$.\nWe convert this back to a condition on $u$:\n$$ -\\frac{\\pi}{4} \\le \\pi\\left(u - \\frac{1}{2}\\right) \\le \\frac{\\pi}{4} $$\n$$ -\\frac{1}{4} \\le u - \\frac{1}{2} \\le \\frac{1}{4} $$\n$$ \\frac{1}{4} \\le u \\le \\frac{3}{4} $$\nThe indicator function is $1$ on the interval $[1/4, 3/4]$ and $0$ otherwise. The integral becomes:\n$$ \\mathbb{E}[X^{(1)}] = \\int_{1/4}^{3/4} \\tan\\left(\\pi\\left(u-\\frac{1}{2}\\right)\\right) \\, \\mathrm{d}u $$\nLet $v = \\pi(u-1/2)$, so $\\mathrm{d}v = \\pi \\, \\mathrm{d}u$. The limits of integration become $v(1/4) = -\\pi/4$ and $v(3/4) = \\pi/4$.\n$$ \\mathbb{E}[X^{(1)}] = \\int_{-\\pi/4}^{\\pi/4} \\tan(v) \\, \\frac{\\mathrm{d}v}{\\pi} = \\frac{1}{\\pi} \\int_{-\\pi/4}^{\\pi/4} \\tan(v) \\, \\mathrm{d}v $$\nThe function $\\tan(v)$ is an odd function, meaning $\\tan(-v) = -\\tan(v)$. We are integrating an odd function over a symmetric interval $[-\\pi/4, \\pi/4]$. Such an integral is always zero.\nAlternatively, we can compute the antiderivative:\n$$ \\frac{1}{\\pi} \\left[ -\\ln|\\cos(v)| \\right]_{-\\pi/4}^{\\pi/4} = -\\frac{1}{\\pi} \\left( \\ln\\left(\\cos\\left(\\frac{\\pi}{4}\\right)\\right) - \\ln\\left(\\cos\\left(-\\frac{\\pi}{4}\\right)\\right) \\right) $$\nSince $\\cos(v)$ is an even function, $\\cos(\\pi/4) = \\cos(-\\pi/4)$, so the two terms in the parenthesis cancel out.\n$$ \\mathbb{E}[X^{(1)}] = -\\frac{1}{\\pi} (0) = 0 $$\nTherefore, $\\mathbb{E}[X^{(1)}] = 0$ and $\\mathbb{E}[Y^{(1)}] = 0$. The product is:\n$$ \\mathbb{E}[X^{(1)}]\\mathbb{E}[Y^{(1)}] = 0 \\cdot 0 = 0 $$", "answer": "$$\\boxed{0}$$", "id": "2975022"}, {"introduction": "Under what conditions can we interchange the limit and expectation operators, such that $\\lim_{n\\to\\infty} \\mathbb{E}[X_n] = \\mathbb{E}[\\lim_{n\\to\\infty} X_n]$? This question is central to analysis. This practice problem ([@problem_id:2974993]) explores a famous a counterexample where a sequence of random variables converges almost surely, yet its expectations do not converge to the expectation of the limit. The analysis will guide you to uncover the precise reason for this failure, introducing the vital concept of uniform integrability, which is the key to resolving this puzzle and is a critical tool in advanced probability and the theory of stochastic processes.", "problem": "Let $(\\Omega,\\mathcal{F},\\mathbb{P})$ be the probability space $([0,1],\\mathcal{B}([0,1]),\\lambda)$, where $\\mathcal{B}([0,1])$ denotes the Borel $\\sigma$-algebra on $[0,1]$ and $\\lambda$ is Lebesgue measure. For each $n\\in\\mathbb{N}$, define the random variable $X_{n}:\\Omega\\to\\mathbb{R}$ by\n$$\nX_{n}(\\omega) \\coloneqq n\\,\\mathbf{1}_{\\{\\omega\\leq 1/n\\}}.\n$$\nWork directly from the definition of expectation as a Lebesgue integral and the definition of uniform integrability to analyze the family $\\{X_{n}\\}_{n\\in\\mathbb{N}}$.\n\nTasks:\n1) Prove that $X_{n}\\to 0$ almost surely as $n\\to\\infty$.\n2) Compute $\\mathbb{E}[X_{n}]$ for each $n\\in\\mathbb{N}$ using the Lebesgue integral definition $\\mathbb{E}[X]=\\int_{\\Omega}X\\,\\mathrm{d}\\mathbb{P}$ for nonnegative measurable $X$.\n3) Using the definition that a family $\\mathcal{X}$ of integrable random variables is uniformly integrable if and only if $\\lim_{M\\to\\infty}\\sup_{X\\in\\mathcal{X}}\\mathbb{E}\\big[|X|\\,\\mathbf{1}_{\\{|X|M\\}}\\big]=0$, determine whether $\\{X_{n}\\}_{n\\in\\mathbb{N}}$ is uniformly integrable.\n4) Explain why the convergence $X_{n}\\to 0$ almost surely does not imply the convergence of expectations $\\mathbb{E}[X_{n}]\\to\\mathbb{E}[0]$ in this example, and identify the specific property that fails.\n\nProvide, as your final answer, the value of the following limit expressed as a real number:\n$$\n\\lim_{M\\to\\infty}\\ \\sup_{n\\in\\mathbb{N}}\\ \\mathbb{E}\\!\\left[\\,|X_{n}|\\,\\mathbf{1}_{\\{|X_{n}|M\\}}\\,\\right].\n$$", "solution": "The problem is valid as it is a well-posed, scientifically grounded, and objective exercise in measure-theoretic probability. The definitions and data provided are self-contained and consistent.\n\nWe are given the probability space $(\\Omega,\\mathcal{F},\\mathbb{P})$ as $([0,1],\\mathcal{B}([0,1]),\\lambda)$ and a sequence of random variables $X_{n}(\\omega) \\coloneqq n\\,\\mathbf{1}_{\\{\\omega\\leq 1/n\\}}$ for $n\\in\\mathbb{N}$. We analyze this sequence according to the four specified tasks.\n\n### 1. Almost Sure Convergence\n\nWe want to prove that $X_{n}\\to 0$ almost surely (a.s.) as $n\\to\\infty$. This means we must show that the set of outcomes $\\omega$ for which $\\lim_{n\\to\\infty}X_{n}(\\omega) \\neq 0$ has probability $0$.\n\nLet's consider an arbitrary $\\omega \\in (0,1]$. By the Archimedean property of real numbers, there exists a natural number $N$ such that $N  \\frac{1}{\\omega}$, which implies $\\frac{1}{N}  \\omega$. For any integer $n  N$, we have $\\frac{1}{n}  \\frac{1}{N}  \\omega$.\nFor such $n$, the condition $\\omega \\leq \\frac{1}{n}$ is false. Therefore, the indicator function $\\mathbf{1}_{\\{\\omega\\leq 1/n\\}}$ is equal to $0$.\nThis implies that for a fixed $\\omega \\in (0,1]$, the sequence of values $X_{n}(\\omega)$ is $0$ for all $n  N$. The limit of a sequence that is eventually zero is $0$. Thus, for every $\\omega \\in (0,1]$, we have $\\lim_{n\\to\\infty} X_{n}(\\omega) = 0$.\n\nNow consider the case $\\omega = 0$. For $\\omega=0$, the condition $\\omega \\leq \\frac{1}{n}$ is true for all $n \\in \\mathbb{N}$.\nSo, $X_{n}(0) = n \\cdot \\mathbf{1}_{\\{0 \\leq 1/n\\}} = n \\cdot 1 = n$.\nThe sequence $\\{X_{n}(0)\\}_{n\\in\\mathbb{N}}$ is $\\{1, 2, 3, \\dots\\}$, which diverges to $\\infty$.\nTherefore, $\\lim_{n\\to\\infty} X_{n}(0) \\neq 0$.\n\nThe set of $\\omega \\in \\Omega$ for which the convergence to $0$ fails is precisely the singleton set $\\{0\\}$.\nThe probability of this set is given by the Lebesgue measure $\\lambda(\\{0\\})$. The Lebesgue measure of any finite set of points is $0$. So, $\\mathbb{P}(\\{\\omega : \\lim_{n\\to\\infty}X_{n}(\\omega) \\neq 0\\}) = \\lambda(\\{0\\}) = 0$.\nBy definition, this means $X_{n} \\to 0$ almost surely.\n\n### 2. Computation of Expectation\n\nThe expectation of a non-negative random variable $X$ is defined as the Lebesgue integral $\\mathbb{E}[X] = \\int_{\\Omega}X\\,\\mathrm{d}\\mathbb{P}$. For our specific case, this is $\\mathbb{E}[X_{n}] = \\int_{[0,1]} X_{n}(\\omega)\\,\\mathrm{d}\\lambda(\\omega)$.\n\nThe random variable $X_{n}(\\omega) = n\\,\\mathbf{1}_{\\{\\omega\\leq 1/n\\}}$ is a simple function. It takes the value $n$ on the set $A_n = \\{\\omega \\in [0,1] : \\omega \\le \\frac{1}{n}\\} = [0, \\frac{1}{n}]$, and the value $0$ otherwise.\nThe Lebesgue measure of the set $A_n$ is $\\lambda([0,\\frac{1}{n}]) = \\frac{1}{n} - 0 = \\frac{1}{n}$.\n\nUsing the definition of the Lebesgue integral for a simple function, we have:\n$$\n\\mathbb{E}[X_{n}] = \\int_{[0,1]} n\\,\\mathbf{1}_{[0,1/n]}(\\omega)\\,\\mathrm{d}\\lambda(\\omega) = n \\cdot \\lambda\\left(\\left[0,\\frac{1}{n}\\right]\\right)\n$$\nSubstituting the measure of the interval, we find:\n$$\n\\mathbb{E}[X_{n}] = n \\cdot \\frac{1}{n} = 1\n$$\nThis holds for all $n \\in \\mathbb{N}$.\n\n### 3. Uniform Integrability\n\nA family of integrable random variables $\\{X_n\\}_{n \\in \\mathbb{N}}$ is uniformly integrable if $\\lim_{M\\to\\infty}\\sup_{n\\in\\mathbb{N}}\\mathbb{E}\\big[|X_n|\\,\\mathbf{1}_{\\{|X_n|M\\}}\\big]=0$.\n\nFirst, since $n0$, we have $X_n(\\omega) = n\\,\\mathbf{1}_{\\{\\omega\\leq 1/n\\}} \\geq 0$ for all $\\omega$. Thus, $|X_n| = X_n$.\nWe need to evaluate the expression $L = \\lim_{M\\to\\infty}\\sup_{n\\in\\mathbb{N}}\\mathbb{E}\\big[X_n\\,\\mathbf{1}_{\\{X_nM\\}}\\big]$.\n\nLet's analyze the expectation $\\mathbb{E}\\big[X_n\\,\\mathbf{1}_{\\{X_nM\\}}\\big]$ for a fixed $M  0$. The random variable $X_n$ can only take values $0$ and $n$.\nThe condition $X_n(\\omega)M$ can only be satisfied if $X_n(\\omega)=n$ and $nM$.\n\nCase 1: $n \\leq M$.\nIn this case, the maximum value of $X_n$ is $n$, which is not greater than $M$. The set $\\{\\omega : X_n(\\omega)  M\\}$ is empty. Thus, the indicator function $\\mathbf{1}_{\\{X_nM\\}}$ is identically $0$, and the expectation is $\\mathbb{E}[X_n \\cdot 0] = 0$.\n\nCase 2: $n  M$.\nIn this case, the condition $X_n(\\omega)  M$ is equivalent to $X_n(\\omega) = n$, which occurs if and only if $\\omega \\in [0, \\frac{1}{n}]$. So, the indicator function $\\mathbf{1}_{\\{X_nM\\}}$ is equal to $\\mathbf{1}_{[0,1/n]}$.\nThe expectation becomes:\n$$\n\\mathbb{E}\\big[X_n\\,\\mathbf{1}_{\\{X_nM\\}}\\big] = \\mathbb{E}\\big[X_n\\,\\mathbf{1}_{[0,1/n]}\\big] = \\int_{[0,1]} \\left(n\\,\\mathbf{1}_{[0,1/n]}(\\omega)\\right) \\cdot \\mathbf{1}_{[0,1/n]}(\\omega) \\,\\mathrm{d}\\lambda(\\omega)\n$$\nSince $(\\mathbf{1}_A)^2 = \\mathbf{1}_A$, the integrand is simply $n\\,\\mathbf{1}_{[0,1/n]}(\\omega) = X_n(\\omega)$.\nSo, for $n  M$, the expectation is $\\mathbb{E}[X_n]$, which we found to be $1$.\n\nNow, we compute the supremum over $n \\in \\mathbb{N}$ for a fixed $M$:\n$$\n\\sup_{n\\in\\mathbb{N}}\\mathbb{E}\\big[X_n\\,\\mathbf{1}_{\\{X_nM\\}}\\big]\n$$\nFor any given $M0$, we can always find an integer $n$ (for example, $n = \\lfloor M \\rfloor + 1$) such that $nM$. For this $n$, the value of the expectation is $1$. For any $n \\le M$, the value is $0$. The supremum of a set containing the value $1$ and non-negative values must be at least $1$. Since the values are only $0$ and $1$, the supremum is exactly $1$.\nThis holds for any $M \\ge 0$.\n\nFinally, we take the limit as $M\\to\\infty$:\n$$\n\\lim_{M\\to\\infty}\\sup_{n\\in\\mathbb{N}}\\mathbb{E}\\big[|X_n|\\,\\mathbf{1}_{\\{|X_n|M\\}}\\big] = \\lim_{M\\to\\infty} 1 = 1\n$$\nSince this limit is $1$ and not $0$, the family $\\{X_n\\}_{n\\in\\mathbb{N}}$ is **not** uniformly integrable.\n\n### 4. Convergence of Expectation\n\nWe have shown that $X_n \\to 0$ almost surely. The limit random variable is the zero function, $X(\\omega)=0$ for all $\\omega$. The expectation of the limit is $\\mathbb{E}[0] = 0$.\nHowever, we found that $\\mathbb{E}[X_n] = 1$ for all $n\\in\\mathbb{N}$. Therefore,\n$$\n\\lim_{n\\to\\infty} \\mathbb{E}[X_n] = \\lim_{n\\to\\infty} 1 = 1\n$$\nWe observe that $\\lim_{n\\to\\infty} \\mathbb{E}[X_n] = 1 \\neq 0 = \\mathbb{E}[\\lim_{n\\to\\infty} X_n]$. This demonstrates that the limit and the expectation operators cannot be interchanged for this sequence.\n\nThe interchange of limit and expectation is justified under certain conditions, such as those given by the Dominated Convergence Theorem or the Monotone Convergence Theorem. Since $X_n$ is not a monotone sequence, the latter does not apply. The Dominated Convergence Theorem requires the existence of an integrable random variable $Y$ such that $|X_n| \\leq Y$ for all $n$. In this case, no such $Y$ exists, as the minimal dominating function $Y(\\omega) = \\sup_n |X_n(\\omega)| = \\sup_n X_n(\\omega) = \\lfloor 1/\\omega \\rfloor$ for $\\omega \\in (0,1]$ is not integrable over $[0,1]$.\n\nA more general result, the Vitali Convergence Theorem, states that for a sequence $\\{X_n\\}$ in $L^1$, convergence in mean ($L^1$ convergence, i.e., $\\mathbb{E}[|X_n-X|] \\to 0$) holds if and only if the sequence converges in probability ($X_n \\xrightarrow{p} X$) and the family $\\{X_n\\}$ is uniformly integrable.\n\nIn our problem:\n1.  $X_n \\to 0$ almost surely, which implies $X_n \\xrightarrow{p} 0$.\n2.  $\\mathbb{E}[|X_n-0|] = \\mathbb{E}[X_n] = 1$, which does not converge to $0$. Thus, $X_n$ does not converge to $0$ in $L^1$.\n\nAccording to the Vitali Convergence Theorem, the failure of $L^1$ convergence, despite convergence in probability, must be due to the failure of the other condition. Therefore, the specific property that fails, and which prevents the convergence of the expectations, is **uniform integrability**. Our direct calculation in Part 3 confirms this conclusion.\n\nThe final answer requested is the value of the limit calculated in Part 3.\n$$\n\\lim_{M\\to\\infty}\\ \\sup_{n\\in\\mathbb{N}}\\ \\mathbb{E}\\!\\left[\\,|X_{n}|\\,\\mathbf{1}_{\\{|X_{n}|M\\}}\\,\\right] = 1\n$$", "answer": "$$\\boxed{1}$$", "id": "2974993"}]}