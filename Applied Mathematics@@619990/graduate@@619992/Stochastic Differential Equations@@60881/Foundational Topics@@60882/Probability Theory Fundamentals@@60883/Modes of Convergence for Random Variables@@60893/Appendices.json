{"hands_on_practices": [{"introduction": "Our exploration of convergence begins with a foundational concept that often proves subtle: the distinction between convergence in probability and almost sure convergence. While both describe the eventual closeness of random variables to a limit, they capture fundamentally different aspects of this behavior. This first practice problem [@problem_id:1319227] is a classic thought experiment designed to build intuition by constructing a sequence that converges in probability but fails to converge almost surely, demonstrating how a series of events with vanishing probabilities can prevent pathwise stability.", "problem": "A digital monitoring system is designed to track a stable environmental parameter, whose true constant value is normalized to 0. The system takes one measurement at each discrete time step $n = 1, 2, 3, \\ldots$. We denote the value recorded by the system at time $n$ as the random variable $X_n$.\n\nDue to a peculiar time-dependent glitch, the system has a small chance of reporting an erroneous value. Specifically, for each time step $n$, the system reports the correct value $X_n = 0$ with probability $1 - \\frac{1}{n}$, and an erroneous value $X_n = 1$ with probability $\\frac{1}{n}$. The outcomes of the measurements at different time steps are mutually independent events.\n\nFor your reference, we define two modes of convergence for a sequence of random variables $\\{Y_n\\}$ to a constant $c$:\n1.  **Convergence in probability**: For any positive number $\\epsilon > 0$, the probability $P(|Y_n - c| \\ge \\epsilon)$ approaches 0 as $n$ tends to infinity.\n2.  **Almost sure convergence**: The probability that the sequence of numerical outcomes $Y_n$ converges to $c$ as $n$ tends to infinity is equal to 1. Formally, $P(\\lim_{n \\to \\infty} Y_n = c) = 1$.\n\nNow, consider the following two statements regarding the sequence of measurements $\\{X_n\\}$ and its convergence to the true value of 0:\n\nStatement I: The sequence $\\{X_n\\}$ converges to 0 in probability.\nStatement II: The sequence $\\{X_n\\}$ converges to 0 almost surely.\n\nWhich of the following options correctly describes the truthfulness of these two statements?\n\nA. Both Statement I and Statement II are true.\n\nB. Statement I is true, but Statement II is false.\n\nC. Statement I is false, but Statement II is true.\n\nD. Both Statement I and Statement II are false.", "solution": "For each $n \\in \\mathbb{N}$, $X_{n}$ takes values in $\\{0,1\\}$ with $P(X_{n}=1)=\\frac{1}{n}$ and $P(X_{n}=0)=1-\\frac{1}{n}$, and the variables are independent across $n$.\n\nTo test convergence in probability to $0$, fix $\\epsilon>0$ and compute\n$$\nP(|X_{n}-0|\\ge \\epsilon)=\n\\begin{cases}\nP(X_{n}=1)=\\frac{1}{n}, & \\text{if } 0<\\epsilon\\le 1,\\\\\n0, & \\text{if } \\epsilon>1.\n\\end{cases}\n$$\nIn both cases $\\lim_{n\\to\\infty}P(|X_{n}-0|\\ge \\epsilon)=0$, since $\\frac{1}{n}\\to 0$. Hence $X_{n}\\to 0$ in probability. Therefore, Statement I is true.\n\nTo test almost sure convergence, define events $E_{n}=\\{X_{n}=1\\}$. Then $P(E_{n})=\\frac{1}{n}$ and the events are independent. We have\n$$\n\\sum_{n=1}^{\\infty}P(E_{n})=\\sum_{n=1}^{\\infty}\\frac{1}{n}=+\\infty.\n$$\nBy the second Borel–Cantelli lemma (which applies due to independence), \n$$\nP(E_{n}\\ \\text{i.o.})=1,\n$$\nthat is, with probability $1$ the event $X_{n}=1$ occurs infinitely often. Consequently, with probability $1$ the sequence does not eventually stay within, for example, the interval $(-\\tfrac{1}{2},\\tfrac{1}{2})$ around $0$. Thus $X_{n}$ does not converge to $0$ almost surely. Therefore, Statement II is false.\n\nThe correct option is B.", "answer": "$$\\boxed{B}$$", "id": "1319227"}, {"introduction": "Moving from foundational distinctions to applied contexts, we now examine the critical difference between strong and weak convergence, a cornerstone of the numerical analysis of SDEs. Strong convergence concerns the pathwise approximation of a solution, while weak convergence focuses on the approximation of its statistical properties. This exercise [@problem_id:2987750] provides a powerful counterexample, mimicking a numerical scheme where rare but superlinearly large outliers destroy strong $L^1$ convergence while preserving weak convergence, highlighting why the boundedness of test functions is a key ingredient for weak approximation theory.", "problem": "Let $(\\Omega,\\mathcal{F},\\mathbb{P})$ be a probability space and let $X$ be a standard normal random variable, interpreted as the exact value at time $t=1$ of the Itô stochastic differential equation $dX_{t} = dW_{t}$ with $X_{0}=0$. For each integer $n \\geq 1$, define an approximation $Y_{n}$ by introducing an event of rare but superlinearly large deviation as follows: let $B_{n}$ be a Bernoulli random variable with $\\mathbb{P}(B_{n}=1)=\\frac{1}{n}$ and $\\mathbb{P}(B_{n}=0)=1-\\frac{1}{n}$, independent of $X$. Set\n$$\nY_{n} \\coloneqq (1-B_{n})\\,X + B_{n}\\,n^{2}.\n$$\nThis construction mimics a numerical scheme for an SDE with superlinearly growing coefficients that, with small probability $\\frac{1}{n}$, produces a catastrophic outlier of size $n^{2}$.\n\n1. Using only foundational definitions of convergence of random variables, establish that for every bounded and continuous test function $\\varphi:\\mathbb{R}\\to\\mathbb{R}$, the sequence $(Y_{n})_{n\\in\\mathbb{N}}$ is weakly consistent with $X$ in the sense that $\\lim_{n\\to\\infty}\\mathbb{E}[\\varphi(Y_{n})]=\\mathbb{E}[\\varphi(X)]$. Explain why the boundedness of $\\varphi$ is essential for this conclusion and interpret this boundedness as a form of moment truncation.\n\n2. Show that strong convergence in $L^{1}$ fails by proving that $\\lim_{n\\to\\infty}\\mathbb{E}[|Y_{n}-X|]=+\\infty$, thereby demonstrating the impact of the superlinear outliers on strong error.\n\n3. Define the bounded and continuous test function $\\varphi(x)=\\arctan(x)$. Compute the exact limit\n$$\nL \\coloneqq \\lim_{n\\to\\infty}\\Big(\\mathbb{E}[\\varphi(Y_{n})]-\\mathbb{E}[\\varphi(X)]\\Big).\n$$\nYour final answer must be the single real number $L$. No rounding is necessary and no units are required.", "solution": "The problem investigates the convergence properties of a sequence of random variables $(Y_n)_{n \\in \\mathbb{N}}$ defined as $Y_n \\coloneqq (1-B_n)X + B_n n^2$, where $X \\sim N(0,1)$ and $B_n$ is an independent Bernoulli random variable with success probability $\\mathbb{P}(B_n=1) = \\frac{1}{n}$.\n\n1. Proof of weak consistency for bounded, continuous test functions.\n\nWeak convergence of $Y_n$ to $X$, denoted $Y_n \\xrightarrow{d} X$, is established by showing that for any bounded and continuous function $\\varphi:\\mathbb{R}\\to\\mathbb{R}$, we have $\\lim_{n\\to\\infty}\\mathbb{E}[\\varphi(Y_n)] = \\mathbb{E}[\\varphi(X)]$.\n\nWe compute the expectation $\\mathbb{E}[\\varphi(Y_n)]$ using the law of total expectation, conditioning on the outcomes of the Bernoulli variable $B_n$.\n$$\n\\mathbb{E}[\\varphi(Y_n)] = \\mathbb{E}[\\varphi(Y_n) | B_n=0]\\mathbb{P}(B_n=0) + \\mathbb{E}[\\varphi(Y_n) | B_n=1]\\mathbb{P}(B_n=1)\n$$\nThe probabilities are given as $\\mathbb{P}(B_n=0) = 1-\\frac{1}{n}$ and $\\mathbb{P}(B_n=1) = \\frac{1}{n}$.\nWe evaluate the conditional expectations:\n- If $B_n=0$, then $Y_n = (1-0)X + 0 \\cdot n^2 = X$. Since $X$ and $B_n$ are independent, the conditional expectation is $\\mathbb{E}[\\varphi(X) | B_n=0] = \\mathbb{E}[\\varphi(X)]$.\n- If $B_n=1$, then $Y_n = (1-1)X + 1 \\cdot n^2 = n^2$. The conditional expectation is $\\mathbb{E}[\\varphi(n^2) | B_n=1] = \\varphi(n^2)$, as $n^2$ is a constant with respect to the expectation over $X$.\n\nSubstituting these into the formula for $\\mathbb{E}[\\varphi(Y_n)]$ yields:\n$$\n\\mathbb{E}[\\varphi(Y_n)] = \\mathbb{E}[\\varphi(X)] \\left(1-\\frac{1}{n}\\right) + \\varphi(n^2) \\frac{1}{n}\n$$\nNow, we take the limit as $n \\to \\infty$:\n$$\n\\lim_{n\\to\\infty}\\mathbb{E}[\\varphi(Y_n)] = \\lim_{n\\to\\infty} \\left( \\mathbb{E}[\\varphi(X)] \\left(1-\\frac{1}{n}\\right) + \\frac{\\varphi(n^2)}{n} \\right)\n$$\nWe analyze the two terms separately. For the first term, since $\\mathbb{E}[\\varphi(X)]$ is a finite constant (as $\\varphi$ is bounded and $X$ has a proper distribution), we have:\n$$\n\\lim_{n\\to\\infty} \\mathbb{E}[\\varphi(X)] \\left(1-\\frac{1}{n}\\right) = \\mathbb{E}[\\varphi(X)] \\cdot 1 = \\mathbb{E}[\\varphi(X)]\n$$\nFor the second term, we use the property that $\\varphi$ is bounded. This implies there exists a constant $M > 0$ such that $|\\varphi(x)| \\le M$ for all $x \\in \\mathbb{R}$. Therefore, we can bound the term as follows:\n$$\n0 \\le \\left| \\frac{\\varphi(n^2)}{n} \\right| \\le \\frac{M}{n}\n$$\nSince $\\lim_{n\\to\\infty} \\frac{M}{n} = 0$, the Squeeze Theorem implies that $\\lim_{n\\to\\infty} \\frac{\\varphi(n^2)}{n} = 0$.\n\nCombining the limits of the two terms, we conclude:\n$$\n\\lim_{n\\to\\infty}\\mathbb{E}[\\varphi(Y_n)] = \\mathbb{E}[\\varphi(X)] + 0 = \\mathbb{E}[\\varphi(X)]\n$$\nThis establishes that $Y_n$ converges to $X$ in distribution (weakly).\n\nThe boundedness of $\\varphi$ is essential. If $\\varphi$ were not bounded, the term $\\varphi(n^2)$ could grow faster than linearly in $n$, causing the term $\\frac{\\varphi(n^2)}{n}$ to diverge. For example, if we took the (unbounded) continuous function $\\varphi(x)=x^2$, the expectation $\\mathbb{E}[Y_n^2]$ would be $\\mathbb{E}[X^2](1-\\frac{1}{n}) + (n^2)^2 \\frac{1}{n} = (1-\\frac{1}{n}) + n^3$. This clearly diverges as $n\\to\\infty$, whereas $\\mathbb{E}[X^2]=1$. The boundedness of the test function effectively \"truncates\" the moments of the random variable, making the expectation insensitive to rare but very large deviations. The probability of the outlier vanishes quickly enough ($\\frac{1}{n}$) to overcome its effect only if the test function does not amplify this outlier value.\n\n2. Failure of convergence in $L^1$.\n\nTo show that strong convergence in $L^1$ fails, we must prove that $\\mathbb{E}[|Y_n - X|]$ does not converge to $0$. We will show it diverges to $+\\infty$.\nWe compute the expectation $\\mathbb{E}[|Y_n - X|]$ by again conditioning on $B_n$.\nThe expression for $Y_n - X$ is $(1-B_n)X + B_n n^2 - X = B_n(n^2 - X)$.\n$$\n|Y_n - X| = |B_n(n^2-X)| = B_n|n^2-X|\n$$\nThis holds because $B_n$ is either $0$ or $1$.\nThe expectation is:\n$$\n\\mathbb{E}[|Y_n - X|] = \\mathbb{E}[B_n|n^2-X|]\n$$\nSince $B_n$ and $X$ are independent, the expectation of the product is the product of the expectations:\n$$\n\\mathbb{E}[|Y_n - X|] = \\mathbb{E}[B_n] \\mathbb{E}[|n^2 - X|] = \\frac{1}{n} \\mathbb{E}[|n^2 - X|]\n$$\nWe can establish a lower bound for $\\mathbb{E}[|n^2-X|]$ using the reverse triangle inequality, $|a-b| \\ge |a|-|b|$.\n$$\n|n^2 - X| \\ge |n^2| - |X| = n^2 - |X|\n$$\nTaking the expectation of both sides:\n$$\n\\mathbb{E}[|n^2 - X|] \\ge \\mathbb{E}[n^2 - |X|] = n^2 - \\mathbb{E}[|X|]\n$$\nThe expected absolute value of a standard normal random variable $X \\sim N(0,1)$ is a constant:\n$$\n\\mathbb{E}[|X|] = \\int_{-\\infty}^{\\infty} |x| \\frac{1}{\\sqrt{2\\pi}}\\exp\\left(-\\frac{x^2}{2}\\right) dx = 2 \\int_{0}^{\\infty} x \\frac{1}{\\sqrt{2\\pi}}\\exp\\left(-\\frac{x^2}{2}\\right) dx = \\sqrt{\\frac{2}{\\pi}}\n$$\nSubstituting this back, we get:\n$$\n\\mathbb{E}[|n^2 - X|] \\ge n^2 - \\sqrt{\\frac{2}{\\pi}}\n$$\nTherefore, the $L^1$-error is bounded below:\n$$\n\\mathbb{E}[|Y_n - X|] \\ge \\frac{1}{n} \\left( n^2 - \\sqrt{\\frac{2}{\\pi}} \\right) = n - \\frac{1}{n}\\sqrt{\\frac{2}{\\pi}}\n$$\nAs $n\\to\\infty$, the lower bound $n - \\frac{1}{n}\\sqrt{\\frac{2}{\\pi}}$ diverges to $+\\infty$. By the comparison test for limits, we must have:\n$$\n\\lim_{n\\to\\infty}\\mathbb{E}[|Y_n - X|] = +\\infty\n$$\nThus, $Y_n$ does not converge to $X$ in $L^1$. The superlinear growth of the outlier $n^2$ is strong enough that, even when weighted by the probability $\\frac{1}{n}$, its contribution to the mean absolute error, which behaves like $\\frac{n^2}{n}=n$, grows without bound.\n\n3. Computation of the limit $L$.\n\nWe are asked to compute $L \\coloneqq \\lim_{n\\to\\infty}\\Big(\\mathbb{E}[\\varphi(Y_{n})]-\\mathbb{E}[\\varphi(X)]\\Big)$ for the specific bounded, continuous test function $\\varphi(x)=\\arctan(x)$.\n\nFrom the derivation in Part 1, we have the general expression:\n$$\n\\mathbb{E}[\\varphi(Y_n)] = \\mathbb{E}[\\varphi(X)] \\left(1-\\frac{1}{n}\\right) + \\frac{\\varphi(n^2)}{n}\n$$\nRearranging this to find the difference gives:\n$$\n\\mathbb{E}[\\varphi(Y_n)] - \\mathbb{E}[\\varphi(X)] = \\mathbb{E}[\\varphi(X)]\\left(1-\\frac{1}{n} - 1\\right) + \\frac{\\varphi(n^2)}{n} = \\frac{\\varphi(n^2) - \\mathbb{E}[\\varphi(X)]}{n}\n$$\nSo, the limit is:\n$$\nL = \\lim_{n\\to\\infty} \\frac{\\varphi(n^2) - \\mathbb{E}[\\varphi(X)]}{n}\n$$\nNow, we substitute $\\varphi(x) = \\arctan(x)$:\n$$\nL = \\lim_{n\\to\\infty} \\frac{\\arctan(n^2) - \\mathbb{E}[\\arctan(X)]}{n}\n$$\nWe evaluate the two terms in the numerator.\nFirst, the limit of $\\arctan(n^2)$:\n$$\n\\lim_{n\\to\\infty} \\arctan(n^2) = \\frac{\\pi}{2}\n$$\nSecond, the expectation $\\mathbb{E}[\\arctan(X)]$. The integrand is $\\arctan(x)$ times the probability density function of $X \\sim N(0,1)$, which is $f_X(x) = \\frac{1}{\\sqrt{2\\pi}}\\exp(-\\frac{x^2}{2})$.\n$$\n\\mathbb{E}[\\arctan(X)] = \\int_{-\\infty}^{\\infty} \\arctan(x) \\frac{1}{\\sqrt{2\\pi}}\\exp\\left(-\\frac{x^2}{2}\\right) dx\n$$\nThe function $g(x) = \\arctan(x) \\exp(-\\frac{x^2}{2})$ is an odd function because $\\arctan(x)$ is odd and $\\exp(-\\frac{x^2}{2})$ is even, so $g(-x) = \\arctan(-x) \\exp(-\\frac{(-x)^2}{2}) = -\\arctan(x) \\exp(-\\frac{x^2}{2}) = -g(x)$. The integral of an odd function over a symmetric interval $(-\\infty, \\infty)$ is zero.\n$$\n\\mathbb{E}[\\arctan(X)] = 0\n$$\nSubstituting these values back into the expression for $L$:\n$$\nL = \\lim_{n\\to\\infty} \\frac{\\frac{\\pi}{2} - 0}{n} = \\lim_{n\\to\\infty} \\frac{\\pi}{2n}\n$$\nAs the numerator is a constant and the denominator grows to infinity, the limit is zero.\n$$\nL = 0\n$$\nThis result is a specific instance of the general weak convergence proved in Part 1. Since $\\lim_{n\\to\\infty}\\mathbb{E}[\\varphi(Y_{n})] = \\mathbb{E}[\\varphi(X)]$, it follows directly that $\\lim_{n\\to\\infty}(\\mathbb{E}[\\varphi(Y_{n})]-\\mathbb{E}[\\varphi(X)])=0$. The calculation confirms this conclusion.", "answer": "$$\\boxed{0}$$", "id": "2987750"}, {"introduction": "Our final practice broadens the scope of convergence to scenarios that lie beyond the realm of the classical Central Limit Theorem. In modeling real-world phenomena, from financial markets to physics, we often encounter heavy-tailed noise processes for which variance is infinite, rendering Gaussian limits inappropriate. This problem [@problem_id:2987751] investigates the convergence of sums of variables drawn from a stable distribution, demonstrating how a different scaling law leads to a non-Gaussian, stable limit and illustrating why the Lindeberg condition fails, providing insight into the domain of attraction for Lévy processes.", "problem": "Consider a discrete-time approximation to a stochastic process driven by heavy-tailed noise. Let $\\{\\xi_{k}\\}_{k \\geq 1}$ be independent and identically distributed random variables with a symmetric strictly $\\alpha$-stable law, for a fixed $\\alpha \\in (1,2)$. The law of $\\xi_{1}$ is characterized by its characteristic function $\\mathbb{E}[\\exp(i t \\xi_{1})]$ satisfying $\\mathbb{E}[\\exp(i t \\xi_{1})] = \\exp(-|t|^{\\alpha})$ for all $t \\in \\mathbb{R}$.\n\nDefine the partial sums $S_{n} = \\sum_{k=1}^{n} \\xi_{k}$ and the scaled sums $Y_{n} = n^{-1/\\alpha} S_{n}$. This scaling is the one suggested by the stable domain of attraction and is the canonical choice for heavy-tailed increments.\n\nStarting from the core definitions of characteristic functions, the Lindeberg condition for triangular arrays, and the known scaling properties of $\\alpha$-stable laws, perform the following tasks:\n\n- Derive whether the Lindeberg condition for the classical Lindeberg–Feller central limit theorem holds for the array $\\{\\xi_{k}\\}$ under any finite-variance normalization, and explain the failure mechanism directly from the regularity properties of the characteristic function near the origin.\n- Using the fundamental properties of strictly $\\alpha$-stable distributions and independent sums, analyze the mode of convergence of $Y_{n}$ and identify whether the limit is Gaussian or non-Gaussian.\n- Determine the explicit closed-form expression for the limiting characteristic function $\\varphi(t)$ of $Y_{n}$ as a function of $t$ and $\\alpha$, expressed without unspecified constants.\n\nYour final answer must be a single closed-form analytic expression for $\\varphi(t)$, with no inequalities or units. No numerical rounding is required.", "solution": "The problem requires an analysis of the convergence properties of scaled sums of independent and identically distributed (i.i.d.) random variables following a symmetric strictly $\\alpha$-stable law. We will address the three tasks presented: the applicability of the Lindeberg condition, the mode of convergence of the scaled sums, and the explicit form of the limiting characteristic function.\n\nThe given random variables $\\{\\xi_{k}\\}_{k \\geq 1}$ are i.i.d. with a characteristic function $\\varphi_{\\xi_1}(t) = \\mathbb{E}[\\exp(i t \\xi_{1})] = \\exp(-|t|^{\\alpha})$ for a fixed $\\alpha \\in (1,2)$.\n\nFirst, we analyze the applicability of the Lindeberg-Feller central limit theorem (CLT). The classical Lindeberg-Feller CLT provides conditions under which the sum of independent random variables, suitably normalized, converges in distribution to a standard normal (Gaussian) distribution. A crucial prerequisite for the standard version of this theorem is that the random variables must have finite variance. The normalization constant is typically related to the sum of the variances.\n\nWe can determine the existence of moments of $\\xi_1$ by examining the differentiability of its characteristic function $\\varphi_{\\xi_1}(t)$ at the origin $t=0$. The $k$-th moment $\\mathbb{E}[\\xi_1^k]$ is related to the $k$-th derivative of the characteristic function at zero, $\\varphi_{\\xi_1}^{(k)}(0)$, provided the moment exists. Specifically, if $\\mathbb{E}[|\\xi_1|^k] < \\infty$, then $\\varphi_{\\xi_1}^{(k)}(0) = i^k \\mathbb{E}[\\xi_1^k]$.\n\nLet's compute the derivatives of $\\varphi_{\\xi_1}(t) = \\exp(-|t|^{\\alpha})$ at $t=0$.\nFor the first moment, we need the first derivative. For $t \\neq 0$, the derivative is:\n$$ \\varphi'_{\\xi_1}(t) = \\frac{d}{dt} \\exp(-|t|^{\\alpha}) = -\\alpha |t|^{\\alpha-1} (\\text{sgn}(t)) \\exp(-|t|^{\\alpha}) $$\nwhere $\\text{sgn}(t)$ is the sign function. As $t \\to 0$, since $\\alpha \\in (1,2)$, the exponent $\\alpha-1$ is in $(0,1)$. Thus, $|t|^{\\alpha-1} \\to 0$. This implies $\\lim_{t \\to 0} \\varphi'_{\\xi_1}(t) = 0$. So, the first derivative exists at $t=0$ and is equal to $0$.\n$$ \\varphi'_{\\xi_1}(0) = 0 $$\nThis confirms that the first moment exists and is $\\mathbb{E}[\\xi_1] = i^{-1} \\varphi'_{\\xi_1}(0) = 0$. The distribution is centered.\n\nFor the second moment, we need the second derivative. Let's examine the derivative for $t > 0$, where $\\varphi_{\\xi_1}(t) = \\exp(-t^{\\alpha})$.\n$$ \\varphi'_{\\xi_1}(t) = -\\alpha t^{\\alpha-1} \\exp(-t^{\\alpha}) \\quad \\text{for } t > 0 $$\n$$ \\varphi''_{\\xi_1}(t) = \\frac{d}{dt} \\left(-\\alpha t^{\\alpha-1} \\exp(-t^{\\alpha})\\right) = -\\alpha(\\alpha-1)t^{\\alpha-2}\\exp(-t^{\\alpha}) + \\alpha^2 t^{2\\alpha-2}\\exp(-t^{\\alpha}) $$\nAs $t \\to 0^+$, we analyze the terms. The exponent $\\alpha-2$ is in $(-1,0)$ because $\\alpha \\in (1,2)$. Therefore, $t^{\\alpha-2} \\to \\infty$ as $t \\to 0^+$. The exponent $2\\alpha-2$ is in $(0,2)$, so $t^{2\\alpha-2} \\to 0$.\nThe first term dominates, and since $-\\alpha(\\alpha-1) < 0$, we have:\n$$ \\lim_{t \\to 0^+} \\varphi''_{\\xi_1}(t) = -\\infty $$\nSince the limit of the second derivative is not finite as $t \\to 0$, the characteristic function is not twice differentiable at the origin. This failure of regularity directly implies that the second moment, $\\mathbb{E}[\\xi_1^2]$, is infinite. Consequently, the variance of $\\xi_1$ is also infinite. The premise of finite variance required for any finite-variance normalization in the classical Lindeberg-Feller CLT is violated. Therefore, the Lindeberg condition, which is formulated for such normalizations, does not hold and is not applicable to this array of random variables.\n\nNext, we analyze the mode of convergence of the scaled sums $Y_{n} = n^{-1/\\alpha} S_{n}$, where $S_{n} = \\sum_{k=1}^{n} \\xi_{k}$. We will use characteristic functions to determine the distribution of $Y_n$ and its limit.\nThe characteristic function of the sum $S_n$ of i.i.d. random variables is the product of their individual characteristic functions:\n$$ \\varphi_{S_n}(t) = \\mathbb{E}[\\exp(itS_n)] = \\mathbb{E}\\left[\\exp\\left(it\\sum_{k=1}^n \\xi_k\\right)\\right] = \\prod_{k=1}^n \\mathbb{E}[\\exp(it\\xi_k)] = (\\varphi_{\\xi_1}(t))^n $$\nSubstituting the given form of $\\varphi_{\\xi_1}(t)$:\n$$ \\varphi_{S_n}(t) = \\left(\\exp(-|t|^{\\alpha})\\right)^n = \\exp(-n|t|^{\\alpha}) $$\nNow, we find the characteristic function of the scaled sum $Y_n = n^{-1/\\alpha}S_n$. Using the property that $\\varphi_{aX}(t) = \\varphi_X(at)$ for a constant $a$, we have:\n$$ \\varphi_{Y_n}(t) = \\varphi_{S_n}(n^{-1/\\alpha} t) $$\nSubstituting $n^{-1/\\alpha} t$ for $t$ in the expression for $\\varphi_{S_n}(t)$:\n$$ \\varphi_{Y_n}(t) = \\exp\\left(-n |n^{-1/\\alpha} t|^{\\alpha}\\right) = \\exp\\left(-n (n^{-1/\\alpha})^{\\alpha} |t|^{\\alpha}\\right) = \\exp\\left(-n \\cdot n^{-1} |t|^{\\alpha}\\right) $$\n$$ \\varphi_{Y_n}(t) = \\exp(-|t|^{\\alpha}) $$\nThis result shows that the characteristic function of the scaled sum $Y_n$ is identical to the characteristic function of a single increment $\\xi_1$, for any $n \\geq 1$. This is the defining \"stability\" property of $\\alpha$-stable distributions.\nThe sequence of random variables $\\{Y_n\\}$ is a sequence where each element has the exact same distribution.\nBy Lévy's continuity theorem, convergence in distribution is equivalent to pointwise convergence of characteristic functions. The limiting characteristic function $\\varphi(t)$ is:\n$$ \\varphi(t) = \\lim_{n \\to \\infty} \\varphi_{Y_n}(t) = \\lim_{n \\to \\infty} \\exp(-|t|^{\\alpha}) = \\exp(-|t|^{\\alpha}) $$\nThus, the sequence $Y_n$ converges in distribution to a random variable $Y$ whose characteristic function is $\\varphi(t) = \\exp(-|t|^{\\alpha})$. The limit $Y$ has the same distribution as $\\xi_1$.\nThe limiting distribution is characterized by $\\alpha \\in (1,2)$. A Gaussian distribution with mean $0$ and variance $\\sigma^2$ has the characteristic function $\\exp(-\\frac{1}{2}\\sigma^2 t^2)$, which corresponds to a stable law with index $\\alpha=2$. Since the problem specifies $\\alpha < 2$, the limiting distribution is non-Gaussian.\n\nFinally, we determine the explicit closed-form expression for the limiting characteristic function $\\varphi(t)$ of $Y_n$. As derived above, the characteristic function of $Y_n$ is $\\varphi_{Y_n}(t) = \\exp(-|t|^{\\alpha})$ for all $n$. The limit of this sequence as $n \\to \\infty$ is therefore the expression itself.\nThe limiting characteristic function is a function of $t$ and the parameter $\\alpha$, and contains no other unspecified constants.", "answer": "$$\n\\boxed{\\exp(-|t|^{\\alpha})}\n$$", "id": "2987751"}]}