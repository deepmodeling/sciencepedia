{"hands_on_practices": [{"introduction": "The Central Limit Theorem is one of the most remarkable results in probability theory, but its power is not unconditional. This exercise moves beyond the familiar i.i.d. setting to probe the theoretical boundaries of the CLT. By working through the Lindeberg condition for a sequence of independent but non-identically distributed random variables, you will develop a deeper appreciation for the precise requirements that ensure convergence to a normal distribution [@problem_id:852531].", "problem": "Let $\\{X_k\\}_{k=1}^\\infty$ be a sequence of independent random variables, where $X_k$ follows a uniform distribution on the interval $[-k^{-\\alpha}, k^{-\\alpha}]$, for some real parameter $\\alpha$. The centered sum of these variables is $S_n = \\sum_{k=1}^n X_k$.\n\nFor the Central Limit Theorem (CLT) to hold for the sequence $\\{X_k\\}$, meaning that the standardized sum converges in distribution to a standard normal random variable,\n$$ \\frac{S_n}{\\sqrt{\\text{Var}(S_n)}} \\xrightarrow{d} N(0,1) \\quad \\text{as } n \\to \\infty, $$\na sufficient condition is the Lindeberg condition. The Lindeberg condition states that for every $\\epsilon > 0$,\n$$ \\lim_{n\\to\\infty} \\frac{1}{B_n^2} \\sum_{k=1}^n \\mathbb{E}\\left[X_k^2 \\mathbf{1}_{\\{|X_k| > \\epsilon B_n\\}}\\right] = 0, $$\nwhere $B_n^2 = \\text{Var}(S_n) = \\sum_{k=1}^n \\text{Var}(X_k)$, and $\\mathbf{1}_{\\{\\cdot\\}}$ is the indicator function.\n\nDetermine the supremum of the exponent $\\alpha$ for which this Lindeberg condition is satisfied. The final answer should be a single real number.", "solution": "Here is a step-by-step derivation to determine the supremum of $\\alpha$.\n\n**1. Calculate the variance of each random variable.**\nThe variance of a uniform distribution on $[a, b]$ is $(b-a)^2/12$. For $X_k \\sim U[-k^{-\\alpha}, k^{-\\alpha}]$, the variance is:\n$$ \\text{Var}(X_k) = \\frac{(k^{-\\alpha} - (-k^{-\\alpha}))^2}{12} = \\frac{(2k^{-\\alpha})^2}{12} = \\frac{4k^{-2\\alpha}}{12} = \\frac{k^{-2\\alpha}}{3} $$\n\n**2. Analyze the total variance of the sum.**\nThe total variance is $B_n^2 = \\text{Var}(S_n) = \\sum_{k=1}^n \\text{Var}(X_k)$ since the variables are independent.\n$$ B_n^2 = \\frac{1}{3} \\sum_{k=1}^n k^{-2\\alpha} $$\nThe convergence of this sum depends on the exponent $2\\alpha$. By the p-series test, the sum $\\sum_{k=1}^\\infty k^{-p}$ converges if $p > 1$ and diverges if $p \\le 1$.\n- If $2\\alpha > 1$ (i.e., $\\alpha > 1/2$), the series converges to a finite limit, so $B_n^2 \\to B^2  \\infty$.\n- If $2\\alpha \\le 1$ (i.e., $\\alpha \\le 1/2$), the series diverges, so $B_n^2 \\to \\infty$ as $n \\to \\infty$.\n\n**3. Apply a necessary condition for the CLT.**\nA necessary condition for the non-i.i.d. Central Limit Theorem to hold is that the contribution of any single variable's variance to the total becomes negligible, which requires that the total variance diverges, i.e., $B_n^2 \\to \\infty$. Therefore, we must have $\\alpha \\le 1/2$. If $\\alpha > 1/2$, the CLT does not hold.\n\n**4. Verify the Lindeberg condition for $\\alpha \\le 1/2$.**\nThe Lindeberg condition is:\n$$ \\lim_{n\\to\\infty} \\frac{1}{B_n^2} \\sum_{k=1}^n \\mathbb{E}\\left[X_k^2 \\mathbf{1}_{\\{|X_k| > \\epsilon B_n\\}}\\right] = 0 \\quad \\text{for every } \\epsilon > 0. $$\nThe variables $X_k$ are bounded: $|X_k| \\le k^{-\\alpha}$. For $k \\ge 1$, this means $|X_k| \\le 1^{-\\alpha} = 1$. So, every variable in the sequence is bounded by 1.\nSince we established that for $\\alpha \\le 1/2$, $B_n^2 \\to \\infty$, it follows that for any $\\epsilon > 0$, we can find an integer $N$ such that for all $n > N$, we have $\\epsilon B_n > 1$.\nFor such large $n$, the condition in the indicator function, $|X_k| > \\epsilon B_n$, can never be met for any $k$, because $|X_k| \\le 1  \\epsilon B_n$.\nThis means that for $n > N$, the indicator function $\\mathbf{1}_{\\{|X_k| > \\epsilon B_n\\}}$ is zero for all $k=1, \\dots, n$.\nConsequently, the expectation $\\mathbb{E}[\\dots]$ is zero for all $k$, the sum is zero, and the entire expression is zero. The limit is therefore 0.\nThe Lindeberg condition is satisfied for all $\\alpha \\le 1/2$.\n\n**5. Conclusion.**\nThe condition holds for $\\alpha \\le 1/2$ and fails for $\\alpha > 1/2$. The supremum of the set of values for $\\alpha$ for which the condition is satisfied is therefore $1/2$.", "answer": "$$\\boxed{\\frac12}$$", "id": "852531"}, {"introduction": "In many real-world applications, from finance to engineering, data points are not independent but exhibit serial correlation. This practice extends the CLT to the realm of stationary time series, a crucial step for practical statistical modeling. You will derive the \"long-run variance\" for a Moving-Average (MA(1)) process, a fundamental concept that correctly scales the sample mean by accounting for the covariance between observations [@problem_id:852396].", "problem": "Consider a stationary Moving-Average process of order one, MA(1), defined by the equation:\n$$\nX_t = \\mu + \\varepsilon_t + \\theta \\varepsilon_{t-1}\n$$\nwhere $\\mu$ is the constant mean of the process, $\\theta$ is the moving-average parameter, and $\\{\\varepsilon_t\\}$ is a white noise process of innovations. The innovations are independent and identically distributed (i.i.d.) with mean $E[\\varepsilon_t] = 0$ and variance $\\text{Var}(\\varepsilon_t) = \\sigma^2_\\varepsilon$.\n\nThe sample mean of this process is given by $\\bar{X}_n = \\frac{1}{n} \\sum_{t=1}^n X_t$. For a stationary process under certain mixing conditions (which the MA(1) process satisfies), a Central Limit Theorem holds for the sample mean. It states that the distribution of the appropriately scaled sample mean converges to a normal distribution:\n$$\n\\sqrt{n}(\\bar{X}_n - \\mu) \\xrightarrow{d} N(0, \\sigma^2_{LR})\n$$\nwhere $\\xrightarrow{d}$ denotes convergence in distribution and $\\sigma^2_{LR}$ is the long-run variance of the process. The long-run variance is defined as the sum of all autocovariances:\n$$\n\\sigma^2_{LR} = \\sum_{k=-\\infty}^{\\infty} \\gamma_k = \\gamma_0 + 2\\sum_{k=1}^{\\infty} \\gamma_k\n$$\nwhere $\\gamma_k = \\text{Cov}(X_t, X_{t-k})$ is the autocovariance at lag $k$.\n\nDerive the analytical expression for the long-run variance, $\\sigma^2_{LR}$, for this MA(1) process in terms of the parameters $\\theta$ and $\\sigma^2_\\varepsilon$.", "solution": "The goal is to compute the long-run variance $\\sigma^2_{LR}$ for the MA(1) process $X_t = \\mu + \\varepsilon_t + \\theta \\varepsilon_{t-1}$. The long-run variance is defined as $\\sigma^2_{LR} = \\sum_{k=-\\infty}^{\\infty} \\gamma_k$, where $\\gamma_k = \\text{Cov}(X_t, X_{t-k})$ is the autocovariance function. We can write this as $\\sigma^2_{LR} = \\gamma_0 + 2\\sum_{k=1}^{\\infty} \\gamma_k$ due to the symmetry property $\\gamma_k = \\gamma_{-k}$.\n\nFirst, let's compute the autocovariance function $\\gamma_k$. Since the mean $\\mu$ is a constant, it does not affect the covariance. We can work with the mean-zero process $Y_t = X_t - \\mu = \\varepsilon_t + \\theta \\varepsilon_{t-1}$. The autocovariance is then $\\gamma_k = \\text{Cov}(Y_t, Y_{t-k}) = E[Y_t Y_{t-k}]$.\n\n**Step 1: Compute the variance, $\\gamma_0$.**\nThe variance corresponds to the autocovariance at lag $k=0$.\n$$\n\\gamma_0 = \\text{Var}(X_t) = \\text{Var}(\\varepsilon_t + \\theta \\varepsilon_{t-1})\n$$\nSince $\\varepsilon_t$ and $\\varepsilon_{t-1}$ are independent, the variance of their sum is the sum of their variances:\n$$\n\\gamma_0 = \\text{Var}(\\varepsilon_t) + \\text{Var}(\\theta \\varepsilon_{t-1}) = \\text{Var}(\\varepsilon_t) + \\theta^2 \\text{Var}(\\varepsilon_{t-1})\n$$\nGiven that $\\text{Var}(\\varepsilon_t) = \\sigma^2_\\varepsilon$ for all $t$:\n$$\n\\gamma_0 = \\sigma^2_\\varepsilon + \\theta^2 \\sigma^2_\\varepsilon = (1+\\theta^2)\\sigma^2_\\varepsilon\n$$\n\n**Step 2: Compute the autocovariance at lag 1, $\\gamma_1$.**\nFor $k=1$, we have:\n$$\n\\gamma_1 = \\text{Cov}(X_t, X_{t-1}) = E[(X_t-\\mu)(X_{t-1}-\\mu)] = E[(\\varepsilon_t + \\theta \\varepsilon_{t-1})(\\varepsilon_{t-1} + \\theta \\varepsilon_{t-2})]\n$$\nExpanding the product gives:\n$$\n\\gamma_1 = E[\\varepsilon_t \\varepsilon_{t-1} + \\theta \\varepsilon_t \\varepsilon_{t-2} + \\theta \\varepsilon_{t-1}^2 + \\theta^2 \\varepsilon_{t-1} \\varepsilon_{t-2}]\n$$\nUsing the linearity of expectation and the properties of the white noise process ($E[\\varepsilon_i]=0$ and $E[\\varepsilon_i \\varepsilon_j] = 0$ for $i \\neq j$, $E[\\varepsilon_i^2]=\\sigma_\\varepsilon^2$):\n$$\nE[\\varepsilon_t \\varepsilon_{t-1}] = 0\n$$\n$$\nE[\\varepsilon_t \\varepsilon_{t-2}] = 0\n$$\n$$\nE[\\varepsilon_{t-1}^2] = \\sigma^2_\\varepsilon\n$$\n$$\nE[\\varepsilon_{t-1} \\varepsilon_{t-2}] = 0\n$$\nSubstituting these into the expression for $\\gamma_1$:\n$$\n\\gamma_1 = 0 + \\theta(0) + \\theta(\\sigma^2_\\varepsilon) + \\theta^2(0) = \\theta \\sigma^2_\\varepsilon\n$$\n\n**Step 3: Compute the autocovariance for lags $k \\ge 2$, $\\gamma_k$.**\nFor any lag $k \\ge 2$:\n$$\n\\gamma_k = \\text{Cov}(X_t, X_{t-k}) = E[(\\varepsilon_t + \\theta \\varepsilon_{t-1})(\\varepsilon_{t-k} + \\theta \\varepsilon_{t-k-1})]\n$$\nThe innovation terms in the first factor are indexed by $t$ and $t-1$. The innovation terms in the second factor are indexed by $t-k$ and $t-k-1$. Since $k \\ge 2$, we have $t-1  t-k  t-k-1$. Thus, the set of time indices $\\{t, t-1\\}$ is disjoint from the set $\\{t-k, t-k-1\\}$. Because the innovations $\\varepsilon_t$ are independent across time, the expectation of any product of innovations with different time indices is zero.\n$$\n\\gamma_k = E[\\varepsilon_t \\varepsilon_{t-k} + \\theta \\varepsilon_t \\varepsilon_{t-k-1} + \\theta \\varepsilon_{t-1} \\varepsilon_{t-k} + \\theta^2 \\varepsilon_{t-1} \\varepsilon_{t-k-1}] = 0\n$$\nSo, $\\gamma_k = 0$ for all $k \\ge 2$.\n\n**Step 4: Calculate the long-run variance $\\sigma^2_{LR}$.**\nNow we can substitute the computed autocovariances into the formula for the long-run variance.\n$$\n\\sigma^2_{LR} = \\gamma_0 + 2\\sum_{k=1}^{\\infty} \\gamma_k = \\gamma_0 + 2(\\gamma_1 + \\gamma_2 + \\gamma_3 + \\dots)\n$$\nSubstituting the values we found:\n$$\n\\sigma^2_{LR} = (1+\\theta^2)\\sigma^2_\\varepsilon + 2(\\theta\\sigma^2_\\varepsilon + 0 + 0 + \\dots)\n$$\n$$\n\\sigma^2_{LR} = (1+\\theta^2)\\sigma^2_\\varepsilon + 2\\theta\\sigma^2_\\varepsilon\n$$\nFactoring out $\\sigma^2_\\varepsilon$:\n$$\n\\sigma^2_{LR} = (1 + \\theta^2 + 2\\theta)\\sigma^2_\\varepsilon\n$$\nRecognizing the perfect square trinomial $a^2+2ab+b^2 = (a+b)^2$:\n$$\n\\sigma^2_{LR} = (1+\\theta)^2 \\sigma^2_\\varepsilon\n$$\nThis is the final expression for the long-run variance.", "answer": "$$\n\\boxed{(1+\\theta)^2 \\sigma^2_\\varepsilon}\n$$", "id": "852396"}, {"introduction": "Building upon the analysis of dependent processes, this problem presents a more advanced challenge involving a non-linear transformation of a classic time series model. You will determine the long-run variance for the square of an autoregressive (AR(1)) process, a calculation relevant to fields like financial econometrics when studying volatility. This exercise synthesizes your understanding of stationary processes, autocovariance structures, and the properties of Gaussian distributions to solve a complex, multi-step problem [@problem_id:852633].", "problem": "Consider a stationary, zero-mean, first-order autoregressive process, AR(1), defined by the stochastic difference equation:\n$$\nX_t = \\phi X_{t-1} + \\epsilon_t\n$$\nwhere $t$ is an integer index representing time. The innovations, $\\{ \\epsilon_t \\}$, are independent and identically distributed (i.i.d.) Gaussian random variables with mean zero and variance $\\sigma_\\epsilon^2$. The stationarity of the process is ensured by the condition $|\\phi|  1$.\n\nLet $Y_t = X_t^2$ be a new stochastic process derived from $X_t$. The sample mean of this new process is given by $\\bar{Y}_n = \\frac{1}{n} \\sum_{t=1}^n Y_t$. For large $n$, a Central Limit Theorem for stationary processes applies, stating that:\n$$\n\\sqrt{n} \\left( \\bar{Y}_n - E[Y_t] \\right) \\xrightarrow{d} N(0, S_Y)\n$$\nwhere $\\xrightarrow{d}$ denotes convergence in distribution, and $S_Y$ is the long-run variance of the process $\\{Y_t\\}$. The long-run variance is defined as the sum of all autocovariances:\n$$\nS_Y = \\sum_{k=-\\infty}^{\\infty} \\text{Cov}(Y_t, Y_{t+k})\n$$\nYour task is to compute the long-run variance $S_Y$ for the process $\\{Y_t\\}$. The final expression should be in terms of the AR(1) parameter $\\phi$ and the innovation variance $\\sigma_\\epsilon^2$.", "solution": "Here is a step-by-step derivation of the long-run variance $S_Y$.\n\n**Step 1: Define properties of the base process $X_t$.**\nSince $X_t$ is a stationary, zero-mean AR(1) process, its variance is $\\text{Var}(X_t) = E[X_t^2] = \\frac{\\sigma_\\epsilon^2}{1-\\phi^2}$. Let's denote this variance by $A$. The autocovariance function of $X_t$ at lag $k$ is $\\gamma_k = \\text{Cov}(X_t, X_{t+k}) = \\phi^{|k|} A$.\n\n**Step 2: Calculate the autocovariance of the squared process $Y_t = X_t^2$.**\nWe need to compute $\\text{Cov}(Y_t, Y_{t+k}) = E[Y_t Y_{t+k}] - E[Y_t]E[Y_{t+k}]$.\nSince $X_t$ is a zero-mean Gaussian process, its mean is $E[X_t]=0$ and its variance is $E[X_t^2]=A$. So, the mean of the squared process is $E[Y_t] = E[X_t^2] = A$.\nFor the cross-term $E[Y_t Y_{t+k}] = E[X_t^2 X_{t+k}^2]$, we use Isserlis' Theorem for zero-mean Gaussian variables, which states $E[Z_1 Z_2 Z_3 Z_4] = E[Z_1Z_2]E[Z_3Z_4] + E[Z_1Z_3]E[Z_2Z_4] + E[Z_1Z_4]E[Z_2Z_3]$.\nLetting $Z_1=Z_2=X_t$ and $Z_3=Z_4=X_{t+k}$:\n$$ E[X_t^2 X_{t+k}^2] = E[X_t^2]E[X_{t+k}^2] + 2(E[X_t X_{t+k}])^2 = A \\cdot A + 2(\\gamma_k)^2 = A^2 + 2\\gamma_k^2 $$\nTherefore, the autocovariance of the $Y_t$ process is:\n$$ \\text{Cov}(Y_t, Y_{t+k}) = (A^2 + 2\\gamma_k^2) - (A \\cdot A) = 2\\gamma_k^2 $$\nNote that for $k=0$, this gives the variance of $Y_t$: $\\text{Var}(Y_t) = \\text{Cov}(Y_t, Y_t) = 2\\gamma_0^2 = 2A^2$.\n\n**Step 3: Sum the autocovariances to find the long-run variance $S_Y$.**\nThe long-run variance is the sum of all autocovariances:\n$$ S_Y = \\sum_{k=-\\infty}^{\\infty} \\text{Cov}(Y_t, Y_{t+k}) = \\sum_{k=-\\infty}^{\\infty} 2\\gamma_k^2 = 2 \\left( \\gamma_0^2 + 2\\sum_{k=1}^{\\infty} \\gamma_k^2 \\right) $$\nSubstituting $\\gamma_k = \\phi^k A$ for $k \\ge 0$:\n$$ S_Y = 2 \\left( A^2 + 2\\sum_{k=1}^{\\infty} (\\phi^k A)^2 \\right) = 2A^2 \\left( 1 + 2\\sum_{k=1}^{\\infty} (\\phi^2)^k \\right) $$\n\n**Step 4: Evaluate the geometric series.**\nThe sum is a geometric series: $\\sum_{k=1}^{\\infty} (\\phi^2)^k = \\frac{\\phi^2}{1-\\phi^2}$, since the condition $|\\phi|1$ ensures that $\\phi^2  1$.\n\n**Step 5: Assemble the expression for $S_Y$.**\n$$ S_Y = 2A^2 \\left( 1 + 2\\frac{\\phi^2}{1-\\phi^2} \\right) = 2A^2 \\left( \\frac{1-\\phi^2 + 2\\phi^2}{1-\\phi^2} \\right) = 2A^2 \\frac{1+\\phi^2}{1-\\phi^2} $$\n\n**Step 6: Substitute back the expression for $A$.**\nFinally, replace $A = \\frac{\\sigma_\\epsilon^2}{1-\\phi^2}$:\n$$ S_Y = 2\\left(\\frac{\\sigma_\\epsilon^2}{1-\\phi^2}\\right)^2 \\frac{1+\\phi^2}{1-\\phi^2} = \\frac{2\\sigma_\\epsilon^4}{(1-\\phi^2)^2} \\frac{1+\\phi^2}{1-\\phi^2} = \\frac{2(1+\\phi^2)\\sigma_\\epsilon^4}{(1-\\phi^2)^3} $$\nThis is the final expression for the long-run variance.", "answer": "$$\\boxed{\\frac{2(1+\\phi^2)\\,\\sigma_\\epsilon^4}{(1-\\phi^2)^3}}$$", "id": "852633"}]}