{"hands_on_practices": [{"introduction": "Stochastic integrals with respect to compensated random measures are fundamental building blocks for models involving random jumps. Before analyzing complex process dynamics, it is essential to master their basic statistical properties. This exercise [@problem_id:2972117] provides a foundational workout, guiding you to compute the expectation and variance of such an integral. Successfully solving it reinforces the core concept that compensated integrals are martingales and introduces the indispensable Itô isometry for jump processes.", "problem": "Let $(\\Omega,\\mathcal{F},(\\mathcal{F}_{t})_{t\\geq 0},\\mathbb{P})$ be a filtered probability space satisfying the usual conditions (in particular, the filtration is right-continuous and $\\mathcal{F}_{0}$ contains all $\\mathbb{P}$-null sets). Let $\\mu$ be a Poisson random measure (PRM) on $(0,\\infty)\\times\\mathbb{R}$ with $\\sigma$-finite intensity measure $\\Pi(\\mathrm{d}x)\\,\\mathrm{d}s$, adapted to $(\\mathcal{F}_{t})_{t\\geq 0}$. Let the compensated Poisson random measure be $\\tilde{\\mu}(\\mathrm{d}s,\\mathrm{d}x)=\\mu(\\mathrm{d}s,\\mathrm{d}x)-\\Pi(\\mathrm{d}x)\\,\\mathrm{d}s$. Fix $t>0$ and a deterministic measurable function $f:\\mathbb{R}\\to\\mathbb{R}$ such that $\\int_{\\mathbb{R}}f(x)^{2}\\,\\Pi(\\mathrm{d}x)<\\infty$, and define the stochastic integral\n$$\nM_{t}:=\\int_{0}^{t}\\int_{\\mathbb{R}} f(x)\\,\\tilde{\\mu}(\\mathrm{d}s,\\mathrm{d}x).\n$$\nUsing only foundational properties of Poisson random measures, compensated Poisson random measures, and continuous-time martingales under right-continuous filtrations, compute the expectation $\\mathbb{E}[M_{t}]$ and the variance $\\mathrm{Var}(M_{t})$ in closed form as expressions involving $f$ and $\\Pi$. Your final answer must be a single analytic expression or a single row matrix containing both quantities.", "solution": "The problem is valid. It is a well-posed question within the established mathematical framework of stochastic calculus, specifically concerning integration with respect to Poisson random measures. All terms are formally defined, and the given conditions are sufficient and consistent for deriving a unique solution.\n\nWe are asked to compute the expectation $\\mathbb{E}[M_{t}]$ and the variance $\\mathrm{Var}(M_{t})$ of the stochastic integral\n$$\nM_{t}:=\\int_{0}^{t}\\int_{\\mathbb{R}} f(x)\\,\\tilde{\\mu}(\\mathrm{d}s,\\mathrm{d}x)\n$$\nwhere $\\tilde{\\mu}(\\mathrm{d}s,\\mathrm{d}x)=\\mu(\\mathrm{d}s,\\mathrm{d}x)-\\Pi(\\mathrm{d}x)\\,\\mathrm{d}s$ is the compensated Poisson random measure. The function $f:\\mathbb{R}\\to\\mathbb{R}$ is deterministic and measurable, satisfying $\\int_{\\mathbb{R}}f(x)^{2}\\,\\Pi(\\mathrm{d}x)<\\infty$.\n\n**1. Calculation of the Expectation $\\mathbb{E}[M_{t}]$**\n\nA fundamental property of stochastic integrals with respect to a compensated random measure is that they are local martingales. Given the condition $\\int_{\\mathbb{R}}f(x)^{2}\\,\\Pi(\\mathrm{d}x)<\\infty$, the process $(M_s)_{s\\ge 0}$ defined by $M_{s}:=\\int_{0}^{s}\\int_{\\mathbb{R}} f(x)\\,\\tilde{\\mu}(\\mathrm{d}u,\\mathrm{d}x)$ is a square-integrable martingale.\n\nAs $(M_s)_{s \\geq 0}$ is a martingale with respect to the filtration $(\\mathcal{F}_s)_{s \\geq 0}$, its expectation is constant over time. We have:\n$$\n\\mathbb{E}[M_{t}] = \\mathbb{E}[M_{0}]\n$$\nThe value of the process at time $s=0$ is given by the integral over the interval $[0,0]$:\n$$\nM_{0} = \\int_{0}^{0}\\int_{\\mathbb{R}} f(x)\\,\\tilde{\\mu}(\\mathrm{d}s,\\mathrm{d}x) = 0\n$$\nTherefore, the expectation is:\n$$\n\\mathbb{E}[M_{t}] = 0\n$$\n\nAlternatively, we can use the definition of the compensated measure and linearity of expectation. Assuming we can apply a stochastic Fubini theorem (which holds since $f$ is deterministic and meets the integrability condition):\n$$\n\\mathbb{E}[M_{t}] = \\mathbb{E}\\left[\\int_{0}^{t}\\int_{\\mathbb{R}} f(x)\\,\\tilde{\\mu}(\\mathrm{d}s,\\mathrm{d}x)\\right] = \\int_{0}^{t}\\int_{\\mathbb{R}} f(x)\\,\\mathbb{E}[\\tilde{\\mu}(\\mathrm{d}s,\\mathrm{d}x)]\n$$\nBy definition of the compensated measure $\\tilde{\\mu}$, its expectation is zero. To see this, recall that for any measurable set $A \\subset (0,\\infty)\\times\\mathbb{R}$, the expectation of the Poisson random measure $\\mu(A)$ is its intensity $\\nu(A) = \\int_A \\Pi(dx)ds$. So, for an infinitesimal region, $\\mathbb{E}[\\mu(\\mathrm{d}s,\\mathrm{d}x)] = \\Pi(\\mathrm{d}x)\\mathrm{d}s$. Thus:\n$$\n\\mathbb{E}[\\tilde{\\mu}(\\mathrm{d}s,\\mathrm{d}x)] = \\mathbb{E}[\\mu(\\mathrm{d}s,\\mathrm{d}x) - \\Pi(\\mathrm{d}x)\\mathrm{d}s] = \\mathbb{E}[\\mu(\\mathrm{d}s,\\mathrm{d}x)] - \\Pi(\\mathrm{d}x)\\mathrm{d}s = \\Pi(\\mathrm{d}x)\\mathrm{d}s - \\Pi(\\mathrm{d}x)\\mathrm{d}s = 0\n$$\nSubstituting this back into the integral for $\\mathbb{E}[M_t]$ yields:\n$$\n\\mathbb{E}[M_{t}] = \\int_{0}^{t}\\int_{\\mathbb{R}} f(x) \\cdot 0 = 0\n$$\n\n**2. Calculation of the Variance $\\mathrm{Var}(M_{t})$**\n\nThe variance of a random variable $X$ is defined as $\\mathrm{Var}(X) = \\mathbb{E}[X^2] - (\\mathbb{E}[X])^2$. Since we have established that $\\mathbb{E}[M_t]=0$, the variance is simply the second moment:\n$$\n\\mathrm{Var}(M_{t}) = \\mathbb{E}[M_{t}^{2}]\n$$\nTo compute $\\mathbb{E}[M_{t}^{2}]$, we employ the Itô isometry for stochastic integrals with respect to random measures. This is a foundational property for square-integrable martingales. For a predictable process $H(s,x)$, the isometry states:\n$$\n\\mathbb{E}\\left[ \\left( \\int_{0}^{t}\\int_{\\mathbb{R}} H(s,x)\\,\\tilde{\\mu}(\\mathrm{d}s,\\mathrm{d}x) \\right)^2 \\right] = \\mathbb{E}\\left[ \\int_{0}^{t}\\int_{\\mathbb{R}} H(s,x)^2\\,\\Pi(\\mathrm{d}x)\\mathrm{d}s \\right]\n$$\nThis property originates from the fact that the compensated Poisson process has orthogonal increments, meaning $\\mathbb{E}[\\tilde{\\mu}(A)\\tilde{\\mu}(B)]=0$ for disjoint measurable sets $A, B$. The variance of the integral is thus the sum (or integral) of the variances of its infinitesimal parts.\n\nIn our problem, the integrand is $H(s,x) = f(x)$. Since $f$ is a deterministic function, it is a predictable process. We can therefore apply the isometry:\n$$\n\\mathrm{Var}(M_{t}) = \\mathbb{E}[M_{t}^{2}] = \\mathbb{E}\\left[ \\left( \\int_{0}^{t}\\int_{\\mathbb{R}} f(x)\\,\\tilde{\\mu}(\\mathrm{d}s,\\mathrm{d}x) \\right)^2 \\right] = \\mathbb{E}\\left[ \\int_{0}^{t}\\int_{\\mathbb{R}} f(x)^2\\,\\Pi(\\mathrm{d}x)\\mathrm{d}s \\right]\n$$\nThe expression inside the expectation on the right-hand side is\n$$\n\\int_{0}^{t}\\int_{\\mathbb{R}} f(x)^2\\,\\Pi(\\mathrm{d}x)\\mathrm{d}s\n$$\nSince $f(x)$, $\\Pi(\\mathrm{d}x)$, and the integration limits $0$ and $t$ are all deterministic, the entire expression is a deterministic quantity, not a random variable. The expectation of a constant is the constant itself. Therefore, we can remove the expectation operator:\n$$\n\\mathrm{Var}(M_{t}) = \\int_{0}^{t}\\int_{\\mathbb{R}} f(x)^2\\,\\Pi(\\mathrm{d}x)\\mathrm{d}s\n$$\nThe inner integral, $\\int_{\\mathbb{R}} f(x)^2\\,\\Pi(\\mathrm{d}x)$, is a constant with respect to the integration variable $s$. Let us denote this constant by $C$:\n$$\nC = \\int_{\\mathbb{R}} f(x)^2\\,\\Pi(\\mathrm{d}x)\n$$\nThe problem statement guarantees that $C < \\infty$. We can now evaluate the outer integral:\n$$\n\\mathrm{Var}(M_{t}) = \\int_{0}^{t} C\\,\\mathrm{d}s = C \\int_{0}^{t} \\mathrm{d}s = C [s]_{0}^{t} = C(t-0) = Ct\n$$\nSubstituting the expression for $C$ back, we obtain the final result for the variance:\n$$\n\\mathrm{Var}(M_{t}) = t \\int_{\\mathbb{R}} f(x)^2\\,\\Pi(\\mathrm{d}x)\n$$\n\nIn summary, the expectation and variance are:\n$$\n\\mathbb{E}[M_{t}] = 0\n$$\n$$\n\\mathrm{Var}(M_{t}) = t \\int_{\\mathbb{R}} f(x)^2\\,\\Pi(\\mathrm{d}x)\n$$", "answer": "$$\n\\boxed{\\begin{pmatrix} 0 & t \\int_{\\mathbb{R}} f(x)^2\\,\\Pi(\\mathrm{d}x) \\end{pmatrix}}\n$$", "id": "2972117"}, {"introduction": "Moving beyond basic moments, we often need to understand the complete structure of a stochastic process. The Doob–Meyer decomposition theorem is a cornerstone of martingale theory, allowing us to uniquely split a submartingale into a predictable, finite-variation process (its \"trend\" or compensator) and a martingale component (its \"noise\"). This practice [@problem_id:2972092] offers a concrete application of this powerful idea, challenging you to derive the canonical decomposition for a pure-jump process and explicitly identify its compensator.", "problem": "Consider a filtered probability space $(\\Omega,\\mathcal{F},(\\mathcal{F}_t)_{t \\geq 0},\\mathbb{P})$ satisfying the usual hypotheses (right-continuity and completeness), and let $N(\\mathrm{d}s,\\mathrm{d}z)$ be a Poisson random measure on $(0,\\infty) \\times (0,\\infty)$ with deterministic compensator $\\mathrm{d}s \\,\\nu(\\mathrm{d}z)$, where $\\nu(\\mathrm{d}z) = \\rho \\exp(-\\rho z)\\,\\mathrm{d}z$ for some fixed $\\rho > 0$. Define the pure-jump process\n$$\nX_t \\coloneqq X_0 + \\int_{0}^{t} \\int_{0}^{\\infty} \\beta\\, s\\, z \\, N(\\mathrm{d}s,\\mathrm{d}z),\n$$\nwith constant $\\beta > 0$, adapted to the completed, right-continuous natural filtration of $N(\\mathrm{d}s,\\mathrm{d}z)$. Using only the foundational definitions of the jump measure of a process, its predictable projection (compensator), and the Doob–Meyer decomposition of a submartingale, derive the canonical decomposition $X_t = X_0 + M_t + A_t$, where $M_t$ is a martingale and $A_t$ is a predictable, finite-variation process. Then, by identifying the compensator $A$ through the predictable projection of the jump measure of $X$, obtain a closed-form expression for $A_t$ as a function of $t$, $\\beta$, and $\\rho$. Provide the final expression for $A_t$ only; no rounding is required.", "solution": "We start from the fundamental definitions of random measures and the Doob–Meyer decomposition. The process $X$ is constructed as a stochastic integral with respect to the Poisson random measure $N(\\mathrm{d}s,\\mathrm{d}z)$:\n$$\nX_t = X_0 + \\int_{0}^{t} \\int_{0}^{\\infty} H(s,z)\\, N(\\mathrm{d}s,\\mathrm{d}z),\n\\quad\\text{with}\\quad\nH(s,z) \\coloneqq \\beta\\, s\\, z.\n$$\nBecause $H$ is nonnegative and predictable (deterministic and measurable in $(s,z)$), and $N$ has deterministic compensator $\\mathrm{d}s\\,\\nu(\\mathrm{d}z)$, $X$ is a special semimartingale. Its jump measure $\\mu^X(\\mathrm{d}s,\\mathrm{d}y)$ counts the jumps of $X$ at time $s$ of size $y$. Since each jump of $X$ at time $s$ is produced by a jump of $N$ at mark $z$ and has size $y = H(s,z) = \\beta s z$, the jump measure $\\mu^X$ is the pushforward of $N$ under the mapping $(s,z) \\mapsto (s, \\beta s z)$. \n\nThe compensator (predictable projection) $\\nu^X(\\mathrm{d}s,\\mathrm{d}y)$ of $\\mu^X(\\mathrm{d}s,\\mathrm{d}y)$ is defined by the property that for any nonnegative predictable test function $\\varphi$,\n$$\n\\mathbb{E}\\left[\\int_{0}^{t} \\int \\varphi(s,y)\\,\\mu^X(\\mathrm{d}s,\\mathrm{d}y)\\right]\n=\n\\mathbb{E}\\left[\\int_{0}^{t} \\int \\varphi(s,y)\\,\\nu^X(\\mathrm{d}s,\\mathrm{d}y)\\right].\n$$\nBy the standard construction for stochastic integrals with respect to a Poisson random measure, and using the pushforward relation, we have\n$$\n\\int_{0}^{t} \\int \\varphi(s,y)\\,\\nu^X(\\mathrm{d}s,\\mathrm{d}y)\n=\n\\int_{0}^{t} \\int_{0}^{\\infty} \\varphi\\bigl(s, \\beta s z\\bigr)\\, \\nu(\\mathrm{d}z)\\,\\mathrm{d}s,\n$$\nwhere $\\nu(\\mathrm{d}z) = \\rho \\exp(-\\rho z)\\,\\mathrm{d}z$. \n\nThe Doob–Meyer decomposition for a special semimartingale driven purely by jumps gives\n$$\nX_t = X_0 + M_t + A_t,\n$$\nwhere $M_t$ is the local martingale obtained by integrating against the compensated random measure $\\tilde{N}(\\mathrm{d}s,\\mathrm{d}z) \\coloneqq N(\\mathrm{d}s,\\mathrm{d}z) - \\mathrm{d}s\\,\\nu(\\mathrm{d}z)$, namely\n$$\nM_t = \\int_{0}^{t} \\int_{0}^{\\infty} H(s,z)\\, \\tilde{N}(\\mathrm{d}s,\\mathrm{d}z)\n= \\int_{0}^{t} \\int_{0}^{\\infty} \\beta\\, s\\, z\\, \\tilde{N}(\\mathrm{d}s,\\mathrm{d}z),\n$$\nand $A_t$ is the predictable finite-variation process given by integrating the jump sizes against the compensator of the jump measure:\n$$\nA_t = \\int_{0}^{t} \\int y \\, \\nu^X(\\mathrm{d}s,\\mathrm{d}y).\n$$\nSubstituting the characterization of $\\nu^X$ via the pushforward,\n$$\nA_t = \\int_{0}^{t} \\int_{0}^{\\infty} \\bigl(\\beta s z\\bigr) \\, \\nu(\\mathrm{d}z)\\, \\mathrm{d}s\n= \\beta \\int_{0}^{t} s\\, \\mathrm{d}s \\int_{0}^{\\infty} z\\, \\nu(\\mathrm{d}z).\n$$\nIt remains to compute $\\int_{0}^{\\infty} z\\, \\nu(\\mathrm{d}z)$ for $\\nu(\\mathrm{d}z) = \\rho \\exp(-\\rho z)\\,\\mathrm{d}z$. This is the expectation of an exponential random variable with rate $\\rho$, which is\n$$\n\\int_{0}^{\\infty} z \\,\\rho \\exp(-\\rho z)\\, \\mathrm{d}z = \\frac{1}{\\rho}.\n$$\nTherefore,\n$$\nA_t = \\beta \\int_{0}^{t} s\\, \\mathrm{d}s \\cdot \\frac{1}{\\rho}\n= \\frac{\\beta}{\\rho} \\cdot \\frac{t^{2}}{2}\n= \\frac{\\beta\\, t^{2}}{2 \\rho}.\n$$\nSince $A_t$ is increasing (because $\\beta > 0$, $\\rho > 0$, and $t \\mapsto t^{2}$ is increasing on $[0,\\infty)$), $X$ is a submartingale with respect to the right-continuous filtration. This completes the canonical decomposition and the identification of the compensator through the predictable projection of the jump measure.", "answer": "$$\\boxed{\\frac{\\beta\\, t^{2}}{2 \\rho}}$$", "id": "2972092"}, {"introduction": "The stochastic exponential, or Doléans-Dade exponential, is a pivotal tool in modern probability, forming the basis for the Girsanov theorem and changes of measure. A crucial technical question is determining when a stochastic exponential, which is always a local martingale, is in fact a true martingale. This problem [@problem_id:2972098] delves into this question by examining Novikov's condition, a well-known sufficient criterion. By first verifying the condition in a standard case and then constructing a counterexample where it fails yet the process remains a martingale, you will gain a deeper appreciation for the subtleties of this critical concept.", "problem": "Consider a filtered probability space $(\\Omega,\\mathcal{F},(\\mathcal{F}_t)_{t\\ge 0},\\mathbb{P})$ satisfying the usual conditions with a right-continuous filtration, and let $(W_t)_{t\\ge 0}$ be a standard Brownian motion (BM) adapted to $(\\mathcal{F}_t)_{t\\ge 0}$. Let $M$ be a continuous local martingale with $M_0=0$ and quadratic variation process $\\langle M\\rangle$. The Doléans–Dade stochastic exponential of $M$ is the process\n$$\n\\mathcal{E}(M)_t := \\exp\\!\\Big(M_t - \\tfrac{1}{2}\\langle M\\rangle_t\\Big),\\quad t\\ge 0.\n$$\nNovikov’s condition states that if\n$$\n\\mathbb{E}\\!\\left[\\exp\\!\\left(\\tfrac{1}{2}\\langle M\\rangle_T\\right)\\right] < \\infty\n$$\nfor some fixed $T>0$, then $(\\mathcal{E}(M)_t)_{0\\le t\\le T}$ is a true martingale with respect to the given right-continuous filtration.\n\nYou will verify Novikov’s condition and its implication in a concrete setting, and then construct a counterexample in which Novikov’s condition fails while the stochastic exponential remains a true martingale.\n\nFix $T>0$ and proceed in two parts:\n\nPart A (verification under Novikov). Define the predictable integrand $\\theta_t := c\\,\\exp(-\\lambda t)$ for $t\\in[0,T]$, where $c>0$ and $\\lambda>0$ are constants, and set\n$$\nM_t := \\int_0^t \\theta_s\\,\\mathrm{d}W_s,\\quad 0\\le t\\le T.\n$$\nStarting from the definitions of quadratic variation and the stochastic exponential for continuous local martingales driven by Brownian motion, derive $\\langle M\\rangle_T$ and compute the quantity\n$$\nI := \\mathbb{E}\\!\\left[\\exp\\!\\left(\\tfrac{1}{2}\\langle M\\rangle_T\\right)\\right].\n$$\nUse this computation to verify Novikov’s condition and explain why right-continuity of the filtration guarantees that the martingale property is assessed on $[0,T]$. Conclude, via first principles (Gaussian integrals conditional on the deterministic integrand), that $(\\mathcal{E}(M)_t)_{0\\le t\\le T}$ is a true martingale and determine $\\mathbb{E}[\\mathcal{E}(M)_T]$.\n\nPart B (counterexample to the necessity of Novikov). Let $\\xi$ be an $\\mathcal{F}_0$-measurable random variable independent of $(W_t)_{t\\ge 0}$ with a Pareto distribution of shape $\\alpha>2$ and scale $x_m>0$, i.e.,\n$$\n\\mathbb{P}(\\xi>x)=\\left(\\frac{x_m}{x}\\right)^{\\alpha}\\quad\\text{for }x\\ge x_m,\n$$\nand set $\\theta_t := \\xi$ for $t\\in[0,T]$, $M_t := \\int_0^t \\theta_s\\,\\mathrm{d}W_s = \\xi W_t$. Using only foundational facts about Gaussian integrals and heavy-tail distributions, show that Novikov’s condition fails by establishing\n$$\n\\mathbb{E}\\!\\left[\\exp\\!\\left(\\tfrac{1}{2}\\langle M\\rangle_T\\right)\\right] \\,=\\, \\mathbb{E}\\!\\left[\\exp\\!\\left(\\tfrac{1}{2}\\,\\xi^2 T\\right)\\right] \\,=\\, \\infty.\n$$\nDespite this failure, justify carefully—by conditioning on $\\xi$ and using properties of the right-continuous filtration enlarged at time $0$ by $\\sigma(\\xi)$—that $(\\mathcal{E}(M)_t)_{0\\le t\\le T}$ is still a true martingale and compute\n$$\nJ := \\mathbb{E}[\\mathcal{E}(M)_T].\n$$\n\nAnswer specification: Return the ordered pair $(I,J)$ as a single analytic expression. No rounding is required. No physical units are involved. The angle unit is not applicable.", "solution": "The problem is validated as sound, well-posed, and grounded in the established mathematical theory of stochastic calculus. All definitions and conditions are standard, and the tasks are logically structured to test core concepts related to martingales and stochastic exponentials.\n\n### Part A: Verification under Novikov's Condition\n\nWe are given the process $M_t := \\int_0^t \\theta_s\\,\\mathrm{d}W_s$ for $t \\in [0,T]$, where the integrand $\\theta_s := c\\,\\exp(-\\lambda s)$ is a deterministic function with constants $c>0$ and $\\lambda>0$.\n\nFirst, we derive the quadratic variation process $\\langle M \\rangle_t$. For a continuous local martingale of the form $M_t = \\int_0^t \\theta_s \\, \\mathrm{d}W_s$, where $\\theta$ is a predictable process, the quadratic variation is given by $\\langle M \\rangle_t = \\int_0^t \\theta_s^2 \\, \\mathrm{d}s$.\nIn our case, $\\theta_s$ is deterministic, so we have:\n$$\n\\langle M \\rangle_t = \\int_0^t (c\\,\\exp(-\\lambda s))^2\\,\\mathrm{d}s = c^2 \\int_0^t \\exp(-2\\lambda s)\\,\\mathrm{d}s.\n$$\nEvaluating this integral at the fixed time $T>0$:\n$$\n\\langle M \\rangle_T = c^2 \\left[ -\\frac{1}{2\\lambda} \\exp(-2\\lambda s) \\right]_0^T = c^2 \\left( -\\frac{1}{2\\lambda} \\exp(-2\\lambda T) - \\left(-\\frac{1}{2\\lambda} \\exp(0)\\right) \\right) = \\frac{c^2}{2\\lambda} \\left(1 - \\exp(-2\\lambda T)\\right).\n$$\nNext, we compute the quantity $I := \\mathbb{E}[\\exp(\\frac{1}{2}\\langle M\\rangle_T)]$. Since the integrand $\\theta_s$ is deterministic, the quadratic variation $\\langle M \\rangle_T$ is a deterministic constant, not a random variable. The expectation of a constant is the constant itself.\n$$\nI = \\mathbb{E}\\left[\\exp\\left(\\tfrac{1}{2}\\langle M\\rangle_T\\right)\\right] = \\exp\\left(\\tfrac{1}{2}\\langle M\\rangle_T\\right) = \\exp\\left(\\frac{1}{2} \\cdot \\frac{c^2}{2\\lambda} \\left(1 - \\exp(-2\\lambda T)\\right)\\right).\n$$\n$$\nI = \\exp\\left(\\frac{c^2}{4\\lambda} \\left(1 - \\exp(-2\\lambda T)\\right)\\right).\n$$\nSince $c$, $\\lambda$, and $T$ are finite positive constants, the value of $\\langle M \\rangle_T$ is a finite positive number. Consequently, $I$ is also a finite positive number. Thus, Novikov's condition, $\\mathbb{E}[\\exp(\\frac{1}{2}\\langle M\\rangle_T)] < \\infty$, is satisfied.\n\nThe problem states that the underlying filtered probability space satisfies the \"usual conditions,\" which implies that the filtration $(\\mathcal{F}_t)_{t\\ge 0}$ is right-continuous and complete. The right-continuity of the filtration is a crucial technical condition in the general theory of stochastic processes. It ensures that the class of martingales is stable and that important theorems, such as the optional stopping theorem, hold for all bounded stopping times. For a process on a finite interval $[0,T]$, being a martingale under a right-continuous filtration implies uniform integrability, which is a key property. These conditions establish a robust framework, precluding certain mathematical pathologies and ensuring that continuous local martingales that are martingales for fixed times remain so in the broader sense required by the theory.\n\nBy Novikov's theorem, since the condition is met, the process $(\\mathcal{E}(M)_t)_{0\\le t\\le T}$ is a true martingale. A fundamental property of a martingale $X_t$ is that $\\mathbb{E}[X_t] = \\mathbb{E}[X_0]$ for all $t$. Here, $\\mathcal{E}(M)_0 = \\exp(M_0 - \\frac{1}{2}\\langle M\\rangle_0) = \\exp(0-0) = 1$. Therefore, we must have $\\mathbb{E}[\\mathcal{E}(M)_T] = 1$.\n\nWe can also verify this from first principles. We need to compute $\\mathbb{E}[\\mathcal{E}(M)_T]$.\n$$\n\\mathbb{E}[\\mathcal{E}(M)_T] = \\mathbb{E}\\left[\\exp\\left(M_T - \\tfrac{1}{2}\\langle M\\rangle_T\\right)\\right].\n$$\nSince $\\langle M \\rangle_T$ is deterministic, we can factor it out of the expectation:\n$$\n\\mathbb{E}[\\mathcal{E}(M)_T] = \\exp\\left(-\\tfrac{1}{2}\\langle M\\rangle_T\\right) \\mathbb{E}\\left[\\exp(M_T)\\right].\n$$\nThe process $M_T = \\int_0^T \\theta_s \\, \\mathrm{d}W_s$ is a Wiener integral with a deterministic integrand. As such, $M_T$ is a Gaussian random variable with mean $\\mathbb{E}[M_T] = \\mathbb{E}[\\int_0^T \\theta_s \\, \\mathrm{d}W_s] = \\int_0^T \\theta_s \\, \\mathbb{E}[\\mathrm{d}W_s] = 0$ and variance $\\mathrm{Var}(M_T) = \\mathbb{E}[M_T^2] = \\mathbb{E}[(\\int_0^T \\theta_s \\, \\mathrm{d}W_s)^2] = \\int_0^T \\theta_s^2 \\, \\mathrm{d}s = \\langle M \\rangle_T$.\nSo, $M_T \\sim \\mathcal{N}(0, \\langle M \\rangle_T)$. The moment-generating function of a normal random variable $X \\sim \\mathcal{N}(\\mu, \\sigma^2)$ is $\\mathbb{E}[\\exp(uX)] = \\exp(u\\mu + \\frac{1}{2}u^2\\sigma^2)$. For $M_T$ with $u=1$, $\\mu=0$ and $\\sigma^2=\\langle M \\rangle_T$, we have:\n$$\n\\mathbb{E}[\\exp(M_T)] = \\exp\\left(1 \\cdot 0 + \\tfrac{1}{2} \\cdot 1^2 \\cdot \\langle M \\rangle_T\\right) = \\exp\\left(\\tfrac{1}{2}\\langle M\\rangle_T\\right).\n$$\nSubstituting this back, we get:\n$$\n\\mathbb{E}[\\mathcal{E}(M)_T] = \\exp\\left(-\\tfrac{1}{2}\\langle M\\rangle_T\\right) \\exp\\left(\\tfrac{1}{2}\\langle M\\rangle_T\\right) = 1.\n$$\nSince $(\\mathcal{E}(M)_t)_{t\\ge 0}$ is a positive continuous local martingale and $\\mathbb{E}[\\mathcal{E}(M)_T]=\\mathcal{E}(M)_0=1$, it is a true martingale on $[0,T]$.\n\n### Part B: Counterexample to the Necessity of Novikov's Condition\n\nHere, the integrand is $\\theta_t = \\xi$ for $t \\in [0,T]$, where $\\xi$ is an $\\mathcal{F}_0$-measurable random variable, independent of $(W_t)_{t \\ge 0}$, with a Pareto distribution defined by $\\mathbb{P}(\\xi > x) = (x_m/x)^\\alpha$ for $x \\ge x_m$, with shape $\\alpha>2$ and scale $x_m>0$. Thus, $M_t = \\int_0^t \\xi \\, \\mathrm{d}W_s = \\xi W_t$.\n\nThe quadratic variation is $\\langle M \\rangle_t = \\int_0^t \\theta_s^2 \\, \\mathrm{d}s = \\int_0^t \\xi^2 \\, \\mathrm{d}s = \\xi^2 t$. At time $T$, we have $\\langle M \\rangle_T = \\xi^2 T$.\n\nWe first show that Novikov's condition fails. We must evaluate $\\mathbb{E}[\\exp(\\frac{1}{2}\\langle M\\rangle_T)]$.\n$$\n\\mathbb{E}\\left[\\exp\\left(\\tfrac{1}{2}\\langle M\\rangle_T\\right)\\right] = \\mathbb{E}\\left[\\exp\\left(\\tfrac{1}{2}\\xi^2 T\\right)\\right].\n$$\nTo compute this expectation, we need the probability density function (PDF) of $\\xi$. The cumulative distribution function (CDF) for $x \\ge x_m$ is $F_\\xi(x) = 1 - \\mathbb{P}(\\xi > x) = 1 - (x_m/x)^\\alpha$. Differentiating with respect to $x$ gives the PDF:\n$$\nf_\\xi(x) = \\frac{\\mathrm{d}}{\\mathrm{d}x}\\left(1 - x_m^\\alpha x^{-\\alpha}\\right) = \\alpha x_m^\\alpha x^{-\\alpha-1}, \\quad \\text{for } x \\ge x_m.\n$$\nThe expectation is then given by the integral:\n$$\n\\mathbb{E}\\left[\\exp\\left(\\tfrac{T}{2}\\xi^2\\right)\\right] = \\int_{x_m}^{\\infty} \\exp\\left(\\tfrac{T}{2}x^2\\right) f_\\xi(x)\\,\\mathrm{d}x = \\int_{x_m}^{\\infty} \\exp\\left(\\tfrac{T}{2}x^2\\right) \\alpha x_m^\\alpha x^{-\\alpha-1}\\,\\mathrm{d}x.\n$$\nThe integrand involves the product of an exponential function $\\exp(\\frac{T}{2}x^2)$, which grows faster than any polynomial in $x$, and a term $x^{-\\alpha-1}$, which decays polynomially. For any constants $K>0$ and $p>0$, the function $\\exp(Kx^2)$ grows faster than $x^p$ as $x \\to \\infty$. Specifically, for any $p$, there exists an $x_0$ such that for all $x > x_0$, $\\exp(\\frac{T}{2}x^2) > x^{\\alpha+1}$. For such $x$, the integrand is bounded below:\n$$\n\\exp\\left(\\tfrac{T}{2}x^2\\right) \\alpha x_m^\\alpha x^{-\\alpha-1} > x^{\\alpha+1} \\alpha x_m^\\alpha x^{-\\alpha-1} = \\alpha x_m^\\alpha > 0.\n$$\nSince the integrand is asymptotically bounded below by a positive constant, its integral over an infinite interval $[x_m, \\infty)$ diverges. Therefore,\n$$\n\\mathbb{E}\\left[\\exp\\left(\\tfrac{1}{2}\\langle M\\rangle_T\\right)\\right] = \\infty.\n$$\nNovikov's condition fails.\n\nDespite this, we can show that $(\\mathcal{E}(M)_t)_{0 \\le t \\le T}$ is a true martingale. The process is $\\mathcal{E}(M)_t = \\exp(M_t - \\frac{1}{2}\\langle M\\rangle_t) = \\exp(\\xi W_t - \\frac{1}{2}\\xi^2 t)$. We verify the martingale property by showing $\\mathbb{E}[\\mathcal{E}(M)_t|\\mathcal{F}_s] = \\mathcal{E}(M)_s$ for $0 \\le s < t \\le T$.\nWe can write $\\mathcal{E}(M)_t$ as:\n$$\n\\mathcal{E}(M)_t = \\exp\\left(\\xi(W_t - W_s) - \\tfrac{1}{2}\\xi^2(t-s)\\right) \\exp\\left(\\xi W_s - \\tfrac{1}{2}\\xi^2 s\\right) = \\exp\\left(\\xi(W_t - W_s) - \\tfrac{1}{2}\\xi^2(t-s)\\right) \\mathcal{E}(M)_s.\n$$\nTaking the conditional expectation with respect to $\\mathcal{F}_s$:\n$$\n\\mathbb{E}[\\mathcal{E}(M)_t|\\mathcal{F}_s] = \\mathbb{E}\\left[\\exp\\left(\\xi(W_t - W_s) - \\tfrac{1}{2}\\xi^2(t-s)\\right) \\mathcal{E}(M)_s \\Big| \\mathcal{F}_s\\right].\n$$\nSince $\\mathcal{E}(M)_s$ is $\\mathcal{F}_s$-measurable, we can factor it out:\n$$\n\\mathbb{E}[\\mathcal{E}(M)_t|\\mathcal{F}_s] = \\mathcal{E}(M)_s \\, \\mathbb{E}\\left[\\exp\\left(\\xi(W_t - W_s) - \\tfrac{1}{2}\\xi^2(t-s)\\right) \\Big| \\mathcal{F}_s\\right].\n$$\nThe random variable $\\xi$ is $\\mathcal{F}_0$-measurable, hence it is $\\mathcal{F}_s$-measurable for any $s \\ge 0$. The increment $W_t - W_s$ is independent of the filtration $\\mathcal{F}_s$. We use the property that if $X$ is $\\mathcal{G}$-measurable and $Y$ is independent of $\\mathcal{G}$, then $\\mathbb{E}[g(X,Y)|\\mathcal{G}] = h(X)$, where $h(x) = \\mathbb{E}[g(x,Y)]$.\nHere, $X = \\xi$, $Y=W_t - W_s$, $\\mathcal{G}=\\mathcal{F}_s$, and $g(X,Y) = \\exp(X Y - \\frac{1}{2}X^2(t-s))$.\nWe compute $h(x) = \\mathbb{E}[g(x, W_t - W_s)]$. The increment $W_t-W_s \\sim \\mathcal{N}(0, t-s)$.\n$$\nh(x) = \\mathbb{E}\\left[\\exp\\left(x(W_t - W_s) - \\tfrac{1}{2}x^2(t-s)\\right)\\right] = \\exp\\left(-\\tfrac{1}{2}x^2(t-s)\\right) \\mathbb{E}\\left[\\exp\\left(x(W_t - W_s)\\right)\\right].\n$$\nThe MGF of $W_t-W_s$ evaluated at $x$ is $\\exp(\\frac{1}{2}x^2(t-s))$. So,\n$$\nh(x) = \\exp\\left(-\\tfrac{1}{2}x^2(t-s)\\right) \\exp\\left(\\tfrac{1}{2}x^2(t-s)\\right) = 1.\n$$\nSince this holds for any value $x$ that $\\xi$ may take, we have $\\mathbb{E}\\left[\\exp\\left(\\xi(W_t - W_s) - \\frac{1}{2}\\xi^2(t-s)\\right) \\big| \\mathcal{F}_s\\right] = 1$.\nTherefore, $\\mathbb{E}[\\mathcal{E}(M)_t|\\mathcal{F}_s] = \\mathcal{E}(M)_s \\cdot 1 = \\mathcal{E}(M)_s$. This proves that $(\\mathcal{E}(M)_t)_{0 \\le t \\le T}$ is a true martingale.\n\nFinally, we compute $J := \\mathbb{E}[\\mathcal{E}(M)_T]$. Since it is a true martingale and $\\mathcal{E}(M)_0 = 1$, we must have $J=1$. We verify this directly using the law of total expectation (conditioning on $\\xi$):\n$$\nJ = \\mathbb{E}[\\mathcal{E}(M)_T] = \\mathbb{E}\\left[\\mathbb{E}[\\mathcal{E}(M)_T | \\xi]\\right] = \\mathbb{E}\\left[\\mathbb{E}\\left[\\exp\\left(\\xi W_T - \\tfrac{1}{2}\\xi^2 T\\right) \\Big| \\xi\\right]\\right].\n$$\nThe inner expectation is computed by treating $\\xi$ as a constant, say $x$. Since $W_T$ is independent of $\\xi$ and $W_T \\sim \\mathcal{N}(0,T)$:\n$$\n\\mathbb{E}\\left[\\exp\\left(x W_T - \\tfrac{1}{2}x^2 T\\right)\\right] = \\exp\\left(-\\tfrac{1}{2}x^2 T\\right) \\mathbb{E}\\left[\\exp\\left(x W_T\\right)\\right] = \\exp\\left(-\\tfrac{1}{2}x^2 T\\right)\\exp\\left(\\tfrac{1}{2}x^2 T\\right) = 1.\n$$\nThis result holds for any value $x$ of $\\xi$. Thus, the conditional expectation $\\mathbb{E}[\\mathcal{E}(M)_T | \\xi]$ is equal to $1$ almost surely.\nThe outer expectation is then trivial:\n$$\nJ = \\mathbb{E}[1] = 1.\n$$\nThe condition $\\alpha>2$ ensures $\\mathbb{E}[\\xi^2] < \\infty$, which implies that the Itô integral $M_t = \\xi W_t$ is an $L^2$-martingale, but this fact is not essential for the argument regarding $\\mathcal{E}(M)_t$, which relies on direct conditioning.\n\nThe ordered pair to be returned is $(I,J)$.\n$I = \\exp\\left(\\frac{c^2}{4\\lambda}(1 - \\exp(-2\\lambda T))\\right)$.\n$J=1$.", "answer": "$$\n\\boxed{\\begin{pmatrix}\\exp\\left(\\frac{c^2}{4\\lambda}\\left(1 - \\exp(-2\\lambda T)\\right)\\right) & 1\\end{pmatrix}}\n$$", "id": "2972098"}]}