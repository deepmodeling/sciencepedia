{"hands_on_practices": [{"introduction": "Uniform integrability is a key condition that refines the notion of convergence for integrable random variables, particularly in the context of martingales. While $L^1$-boundedness is necessary, it is not sufficient for uniform integrability. This exercise provides a hands-on construction of a classic counterexample, allowing you to directly compute the failure of the uniform integrability criterion and solidify your understanding of this subtle but essential concept [@problem_id:2973857].", "problem": "Consider the probability space $(\\Omega,\\mathcal{F},\\mathbb{P})$ where $\\Omega=[0,1]$, $\\mathcal{F}$ is the Borel $\\sigma$-algebra on $[0,1]$, and $\\mathbb{P}$ is Lebesgue measure. For each $n\\in\\mathbb{N}$, let $A_{n}=[0,1/n)$ and define the sequence of random variables $\\{X_{n}\\}_{n\\in\\mathbb{N}}$ by $X_{n}(\\omega)=n\\,\\mathbf{1}_{A_{n}}(\\omega)$. This construction yields a sequence with $\\mathbb{E}[|X_{n}|]=1$ for all $n\\in\\mathbb{N}$. \n\nUsing first-principles definitions from measure-theoretic probability, compute, for an arbitrary $K>0$, the quantity \n$$\nS(K)=\\sup_{n\\in\\mathbb{N}}\\mathbb{E}\\big[\\,|X_{n}|\\,\\mathbf{1}_{\\{|X_{n}|>K\\}}\\,\\big],\n$$\nand then determine the limit \n$$\n\\lim_{K\\to\\infty}S(K).\n$$\nReport the value of $\\lim_{K\\to\\infty}S(K)$ as your final answer. No rounding is required, and no units are involved. Your reasoning should start from the core definitions of uniform integrability and the integrability of random variables, and should remain scientifically and mathematically consistent with standard facts used in advanced treatments of stochastic differential equations, including the relationship between $L^{p}$-boundedness (for $p>1$) and uniform integrability.", "solution": "We begin by recalling the core definitions relevant to the task. A family of integrable random variables $\\{Y_{i}\\}_{i\\in I}$ is said to be uniformly integrable if \n$$\n\\lim_{K\\to\\infty}\\sup_{i\\in I}\\mathbb{E}\\big[\\,|Y_{i}|\\,\\mathbf{1}_{\\{|Y_{i}|>K\\}}\\,\\big]=0.\n$$\nThis is the standard characterization of uniform integrability in measure-theoretic probability and is foundational in the study of stochastic processes, including martingales encountered in stochastic differential equations. It is also a well-known fact that $L^{p}$-boundedness for $p>1$ implies uniform integrability, whereas mere $L^{1}$-boundedness does not guarantee uniform integrability.\n\nWe analyze the given sequence $\\{X_{n}\\}_{n\\in\\mathbb{N}}$ defined by $X_{n}(\\omega)=n\\,\\mathbf{1}_{A_{n}}(\\omega)$, where $A_{n}=[0,1/n)$ and $\\mathbb{P}(A_{n})=1/n$. First, we verify integrability and compute $\\mathbb{E}[|X_{n}|]$:\n$$\n\\mathbb{E}[|X_{n}|]=\\int_{\\Omega}|X_{n}(\\omega)|\\,\\mathrm{d}\\mathbb{P}(\\omega)=\\int_{\\Omega}n\\,\\mathbf{1}_{A_{n}}(\\omega)\\,\\mathrm{d}\\mathbb{P}(\\omega)=n\\,\\mathbb{P}(A_{n})=n\\cdot\\frac{1}{n}=1.\n$$\nThus each $X_{n}$ is integrable with $\\mathbb{E}[|X_{n}|]=1$, so the family is $L^{1}$-bounded.\n\nNext, we compute the tail functional $S(K)=\\sup_{n\\in\\mathbb{N}}\\mathbb{E}\\big[\\,|X_{n}|\\,\\mathbf{1}_{\\{|X_{n}|>K\\}}\\,\\big]$. Note that $|X_{n}|$ takes only the values $0$ and $n$. Therefore, the indicator can be simplified as \n$$\n\\mathbf{1}_{\\{|X_{n}|>K\\}}=\\mathbf{1}_{\\{n>K\\}}\\,\\mathbf{1}_{A_{n}},\n$$\nbecause $|X_{n}|>K$ occurs if and only if the value $n$ is realized and $n>K$. Hence,\n$$\n\\mathbb{E}\\big[\\,|X_{n}|\\,\\mathbf{1}_{\\{|X_{n}|>K\\}}\\,\\big]\n=\\mathbb{E}\\big[\\,n\\,\\mathbf{1}_{A_{n}}\\,\\mathbf{1}_{\\{n>K\\}}\\,\\big]\n=n\\,\\mathbf{1}_{\\{n>K\\}}\\mathbb{P}(A_{n})\n=n\\,\\mathbf{1}_{\\{n>K\\}}\\cdot\\frac{1}{n}\n=\\mathbf{1}_{\\{n>K\\}}.\n$$\nThus, for any fixed $K>0$,\n$$\n\\sup_{n\\in\\mathbb{N}}\\mathbb{E}\\big[\\,|X_{n}|\\,\\mathbf{1}_{\\{|X_{n}|>K\\}}\\,\\big]\n=\\sup_{n\\in\\mathbb{N}}\\mathbf{1}_{\\{n>K\\}}=1,\n$$\nbecause for every finite $K>0$ there exist integers $n>K$, making the indicator equal to $1$ for those $n$.\n\nConsequently,\n$$\nS(K)=1\\quad\\text{for all }K>0,\n$$\nand the limit is\n$$\n\\lim_{K\\to\\infty}S(K)=\\lim_{K\\to\\infty}1=1.\n$$\n\nThis shows the family $\\{X_{n}\\}$ is not uniformly integrable, since the defining criterion for uniform integrability requires the limit to be $0$. For additional context connecting to $L^{p}$ theories and Doob-type controls, we compute higher moments: for any $p>1$,\n$$\n\\mathbb{E}\\big[|X_{n}|^{p}\\big]=\\mathbb{E}\\big[n^{p}\\,\\mathbf{1}_{A_{n}}\\big]=n^{p}\\cdot\\frac{1}{n}=n^{p-1},\n$$\nwhich is unbounded in $n$, implying the sequence is not $L^{p}$-bounded for any $p>1$. Since $L^{p}$-boundedness for $p>1$ implies uniform integrability, the failure of such boundedness aligns with the explicit failure of uniform integrability we computed via the tail functional.\n\nTherefore, the requested limit equals $1$.", "answer": "$$\\boxed{1}$$", "id": "2973857"}, {"introduction": "Doob's maximal inequalities are cornerstone results for controlling the size of martingales and submartingales, but their application requires careful attention to the stated hypotheses. This practice problem acts as a critical cautionary tale, using the familiar standard Brownian motion to quantitatively demonstrate why the non-negativity assumption (or the use of a positive part) is indispensable. By calculating the discrepancy, you will gain a deeper appreciation for the precise formulation of these powerful inequalities [@problem_id:2973860].", "problem": "Let $(\\Omega,\\mathcal{F},(\\mathcal{F}_t)_{t \\ge 0},\\mathbb{P})$ be a filtered probability space supporting a standard Brownian motion (Wiener process) $W=(W_t)_{t \\ge 0}$ adapted to its natural filtration $(\\mathcal{F}_t)_{t \\ge 0}$. Recall that $W$ is a martingale and, for each fixed $T>0$, $W_T$ has the centered normal distribution with variance $T$. Denote by $M_T := \\sup_{0 \\le t \\le T} W_t$ and, for $x \\in \\mathbb{R}$, let $\\Phi(x) := \\mathbb{P}(Z \\le x)$ for a standard normal random variable $Z \\sim \\mathcal{N}(0,1)$.\n\nA commonly stated weak-type inequality for nonnegative submartingales $X$ is of the form $\\lambda \\,\\mathbb{P}\\!\\left(\\sup_{0 \\le t \\le T} X_t \\ge \\lambda\\right) \\le \\mathbb{E}[X_T^{+}]$ for $\\lambda>0$, where $x^{+} := \\max\\{x,0\\}$. If one attempts to remove the nonnegativity and the positive-part truncation by replacing $X$ with the martingale $W$, the putative inequality becomes $\\lambda \\,\\mathbb{P}(M_T \\ge \\lambda) \\le \\mathbb{E}[W_T]$, which would force the left-hand side to be $0$ due to $\\mathbb{E}[W_T]=0$. Your task is to exhibit a quantitative counterexample based on $W$.\n\nStarting only from core definitions and well-tested facts about Brownian motion and the normal distribution, compute the exact closed-form expression (in terms of $T$ and $\\lambda$) for\n$$\nD(T,\\lambda) := \\lambda \\,\\mathbb{P}(M_T \\ge \\lambda) - \\mathbb{E}[W_T]\n$$\nfor fixed $T>0$ and $\\lambda>0$. Use your result to conclude that the weak-type inequality fails without replacing $X$ by $X^{+}$ for the martingale $W$.\n\nGive your final answer for $D(T,\\lambda)$ as a single analytic expression in $T$ and $\\lambda$. No rounding is required.", "solution": "The quantity to be computed is defined as\n$$\nD(T,\\lambda) := \\lambda \\,\\mathbb{P}(M_T \\ge \\lambda) - \\mathbb{E}[W_T]\n$$\nfor fixed parameters $T>0$ and $\\lambda>0$. We will evaluate each term in this expression separately.\n\nFirst, we consider the second term, $\\mathbb{E}[W_T]$. The process $W = (W_t)_{t \\ge 0}$ is a standard Brownian motion (or Wiener process). By definition, for any time $t \\ge 0$, the random variable $W_t$ is normally distributed with mean $0$ and variance $t$. This is denoted as $W_t \\sim \\mathcal{N}(0, t)$. Consequently, for the fixed time $T>0$, the expectation of $W_T$ is\n$$\n\\mathbb{E}[W_T] = 0\n$$\nSubstituting this into the definition of $D(T, \\lambda)$ simplifies the expression to\n$$\nD(T,\\lambda) = \\lambda \\,\\mathbb{P}(M_T \\ge \\lambda)\n$$\nNext, we evaluate the first term, which involves the probability $\\mathbb{P}(M_T \\ge \\lambda)$. Here, $M_T = \\sup_{0 \\le t \\le T} W_t$ is the maximum value attained by the Brownian motion over the time interval $[0, T]$. This probability can be determined using a fundamental result for Brownian motion known as the reflection principle. For a standard Brownian motion $W$ starting at $W_0=0$, and for any level $\\lambda > 0$, the reflection principle states that the probability of the maximum of the process reaching or exceeding $\\lambda$ is twice the probability of the process value at time $T$ being greater than or equal to $\\lambda$. Mathematically, this is expressed as:\n$$\n\\mathbb{P}(M_T \\ge \\lambda) = 2 \\, \\mathbb{P}(W_T \\ge \\lambda)\n$$\nNow, the task reduces to computing the tail probability $\\mathbb{P}(W_T \\ge \\lambda)$. As established, $W_T \\sim \\mathcal{N}(0, T)$. We can standardize this random variable by defining $Z = W_T / \\sqrt{T}$. The random variable $Z$ follows the standard normal distribution, $Z \\sim \\mathcal{N}(0, 1)$. The problem statement defines the cumulative distribution function (CDF) of $Z$ as $\\Phi(x) = \\mathbb{P}(Z \\le x)$.\n\nThe event $W_T \\ge \\lambda$ is equivalent to the event $\\sqrt{T} Z \\ge \\lambda$, which simplifies to $Z \\ge \\frac{\\lambda}{\\sqrt{T}}$. We can now express the probability in terms of the standard normal CDF $\\Phi$:\n$$\n\\mathbb{P}(W_T \\ge \\lambda) = \\mathbb{P}\\left(Z \\ge \\frac{\\lambda}{\\sqrt{T}}\\right) = 1 - \\mathbb{P}\\left(Z < \\frac{\\lambda}{\\sqrt{T}}\\right)\n$$\nSince the normal distribution is continuous, the probability of it being strictly less than a value is the same as it being less than or equal to that value. Thus,\n$$\n1 - \\mathbb{P}\\left(Z < \\frac{\\lambda}{\\sqrt{T}}\\right) = 1 - \\mathbb{P}\\left(Z \\le \\frac{\\lambda}{\\sqrt{T}}\\right) = 1 - \\Phi\\left(\\frac{\\lambda}{\\sqrt{T}}\\right)\n$$\nCombining our results, we have\n$$\n\\mathbb{P}(M_T \\ge \\lambda) = 2 \\, \\mathbb{P}(W_T \\ge \\lambda) = 2\\left(1 - \\Phi\\left(\\frac{\\lambda}{\\sqrt{T}}\\right)\\right)\n$$\nFinally, we substitute this back into the simplified expression for $D(T, \\lambda)$:\n$$\nD(T,\\lambda) = \\lambda \\,\\mathbb{P}(M_T \\ge \\lambda) = 2\\lambda\\left(1 - \\Phi\\left(\\frac{\\lambda}{\\sqrt{T}}\\right)\\right)\n$$\nThis is the exact closed-form expression for $D(T, \\lambda)$.\n\nThe problem also asks to use this result to show that the putative inequality $\\lambda \\,\\mathbb{P}(M_T \\ge \\lambda) \\le \\mathbb{E}[W_T]$ fails. This inequality is equivalent to the statement $D(T,\\lambda) \\le 0$. However, our derived expression shows otherwise. Given that $T>0$ and $\\lambda>0$, the argument of the CDF, $\\frac{\\lambda}{\\sqrt{T}}$, is a finite positive number. For any positive argument $x>0$, the standard normal CDF satisfies $\\Phi(x) < 1$. Consequently, the term $1 - \\Phi\\left(\\frac{\\lambda}{\\sqrt{T}}\\right)$ is strictly positive. Since $\\lambda > 0$ and $2$ is a positive constant, it follows that\n$$\nD(T,\\lambda) = 2\\lambda\\left(1 - \\Phi\\left(\\frac{\\lambda}{\\sqrt{T}}\\right)\\right) > 0\n$$\nfor all $T>0$ and $\\lambda>0$. This result, $D(T,\\lambda) > 0$, directly contradicts the condition $D(T,\\lambda) \\le 0$ that would be required for the putative inequality to hold. Therefore, the inequality fails. The failure occurs because the correct weak-type inequality for a (not necessarily non-negative) martingale involves the positive part, i.e., $\\mathbb{E}[W_T^{+}]$, on the right-hand side, which is strictly positive, not $\\mathbb{E}[W_T]$, which is zero.", "answer": "$$\n\\boxed{2\\lambda\\left(1 - \\Phi\\left(\\frac{\\lambda}{\\sqrt{T}}\\right)\\right)}\n$$", "id": "2973860"}, {"introduction": "This exercise synthesizes several core concepts by tasking you with analyzing one of the most important martingales in stochastic calculus, the Doléans-Dade exponential. You will start from a terminal random variable, derive the martingale process using conditional expectation, and then apply Doob's $L^p$ inequality to obtain a bound on its maximum. This practice reinforces the interplay between martingales, $L^p$-boundedness, uniform integrability, and the practical application of Doob's maximal principle [@problem_id:2973872].", "problem": "Consider a filtered probability space $(\\Omega,\\mathcal{F},(\\mathcal{F}_{t})_{t\\in[0,T]},\\mathbb{P})$ supporting a standard Brownian motion $(W_{t})_{t\\in[0,T]}$ with its usual augmented filtration $(\\mathcal{F}_{t})_{t\\in[0,T]}$. Fix parameters $T>0$ and $\\alpha\\in\\mathbb{R}$. Define the terminal random variable\n$$\n\\xi \\coloneqq \\exp\\!\\big(\\alpha W_{T} - \\tfrac{1}{2}\\alpha^{2}T\\big),\n$$\nand the continuous-time process\n$$\nX_{t} \\coloneqq \\mathbb{E}\\!\\left[\\xi \\,\\middle|\\, \\mathcal{F}_{t}\\right], \\quad t\\in[0,T].\n$$\nYou will work from first principles: the definitions of conditional expectation, the independent increment and Gaussian distribution properties of Brownian motion, the definition of submartingale, and the statement of Doob’s $L^{p}$ inequalities for $p>1$, without assuming any specialized shortcut formulas beyond these foundational facts.\n\nTasks:\n1. Show that $(X_{t})_{t\\in[0,T]}$ is a nonnegative submartingale and determine a closed-form expression for $X_{t}$.\n2. For a fixed $p>1$, verify integrability of $X_{t}$ at all finite times by computing $\\sup_{0\\le t\\le T}\\mathbb{E}\\!\\left[X_{t}^{p}\\right]$ in closed form. Explain why this implies uniform integrability on $[0,T]$ via the de la Vallée-Poussin criterion.\n3. Using Doob’s $L^{p}$ inequality for nonnegative submartingales, provide the simplest closed-form upper bound for $\\mathbb{E}\\!\\left[\\sup_{0\\le s\\le T} X_{s}^{p}\\right]$ in terms of $p$, $\\alpha$, and $T$.\n\nExpress your final answer as a single closed-form analytic expression with no units.", "solution": "The problem statement is validated as mathematically sound and well-posed. We proceed with the solution.\n\nThe problem asks for a three-part analysis of the process $X_{t} \\coloneqq \\mathbb{E}[\\xi \\,|\\, \\mathcal{F}_{t}]$ for $t\\in[0,T]$, where $\\xi \\coloneqq \\exp(\\alpha W_{T} - \\tfrac{1}{2}\\alpha^{2}T)$.\n\nFirst, we address Task 1: to show that $(X_{t})_{t\\in[0,T]}$ is a nonnegative submartingale and to determine its closed-form expression.\nThe terminal variable $\\xi$ is defined as $\\xi = \\exp(\\alpha W_{T} - \\frac{1}{2}\\alpha^{2}T)$. Since the exponential function has a strictly positive range, we have $\\xi > 0$ almost surely. The process $X_{t}$ is the conditional expectation of this strictly positive random variable, $X_{t} = \\mathbb{E}[\\xi \\,|\\, \\mathcal{F}_{t}]$. The conditional expectation of a non-negative (or strictly positive) random variable is itself non-negative. Therefore, $X_{t} \\ge 0$ for all $t\\in[0,T]$, establishing non-negativity.\n\nTo show that $(X_{t})_{t\\in[0,T]}$ is a submartingale, we must verify two conditions:\n1. $X_{t}$ is $\\mathcal{F}_{t}$-adapted and $\\mathbb{E}[|X_{t}|] < \\infty$ for all $t \\in [0,T]$.\n2. For any $s, t$ with $0 \\le s < t \\le T$, we have $\\mathbb{E}[X_{t} \\,|\\, \\mathcal{F}_{s}] \\ge X_{s}$.\n\nThe first condition holds: $X_{t}$ is $\\mathcal{F}_{t}$-adapted by its definition as a conditional expectation with respect to $\\mathcal{F}_{t}$. Its integrability will be demonstrated explicitly in Task 2 for any power $p \\ge 1$, which includes the case $p=1$. For the second condition, we use the tower property of conditional expectation. For $s<t$,\n$$\n\\mathbb{E}[X_{t} \\,|\\, \\mathcal{F}_{s}] = \\mathbb{E}\\big[\\mathbb{E}[\\xi \\,|\\, \\mathcal{F}_{t}] \\,\\big|\\, \\mathcal{F}_{s}\\big] = \\mathbb{E}[\\xi \\,|\\, \\mathcal{F}_{s}].\n$$\nBy the definition of $X_{s}$, we have $X_{s} = \\mathbb{E}[\\xi \\,|\\, \\mathcal{F}_{s}]$. Thus, we find that $\\mathbb{E}[X_{t} \\,|\\, \\mathcal{F}_{s}] = X_{s}$. This indicates that $(X_{t})_{t\\in[0,T]}$ is a martingale. Since equality implies the 'greater than or equal to' condition, every martingale is also a submartingale. So, $(X_{t})_{t\\in[0,T]}$ is a nonnegative submartingale.\n\nNext, we find the closed-form expression for $X_{t}$.\n$$\nX_{t} = \\mathbb{E}\\left[\\exp\\left(\\alpha W_{T} - \\frac{1}{2}\\alpha^{2}T\\right) \\,\\middle|\\, \\mathcal{F}_{t}\\right].\n$$\nWe decompose the Brownian motion path $W_T$ as $W_{T} = W_{t} + (W_{T}-W_{t})$ and rewrite the exponent:\n$$\nX_{t} = \\mathbb{E}\\left[\\exp\\left(\\alpha W_{t} + \\alpha(W_{T}-W_{t}) - \\frac{1}{2}\\alpha^{2}t - \\frac{1}{2}\\alpha^{2}(T-t)\\right) \\,\\middle|\\, \\mathcal{F}_{t}\\right]\n$$\n$$\nX_{t} = \\mathbb{E}\\left[\\exp\\left(\\alpha W_{t} - \\frac{1}{2}\\alpha^{2}t\\right) \\exp\\left(\\alpha(W_{T}-W_{t}) - \\frac{1}{2}\\alpha^{2}(T-t)\\right) \\,\\middle|\\, \\mathcal{F}_{t}\\right].\n$$\nThe term $\\exp(\\alpha W_{t} - \\frac{1}{2}\\alpha^{2}t)$ is $\\mathcal{F}_{t}$-measurable, so it can be factored out of the conditional expectation:\n$$\nX_{t} = \\exp\\left(\\alpha W_{t} - \\frac{1}{2}\\alpha^{2}t\\right) \\mathbb{E}\\left[\\exp\\left(\\alpha(W_{T}-W_{t}) - \\frac{1}{2}\\alpha^{2}(T-t)\\right) \\,\\middle|\\, \\mathcal{F}_{t}\\right].\n$$\nThe increment $W_{T}-W_{t}$ is independent of the filtration $\\mathcal{F}_{t}$. Consequently, any function of this increment is also independent of $\\mathcal{F}_{t}$. Therefore, the conditional expectation becomes an unconditional expectation:\n$$\nX_{t} = \\exp\\left(\\alpha W_{t} - \\frac{1}{2}\\alpha^{2}t\\right) \\mathbb{E}\\left[\\exp\\left(\\alpha(W_{T}-W_{t}) - \\frac{1}{2}\\alpha^{2}(T-t)\\right)\\right].\n$$\nLet the random variable $Z = W_{T}-W_{t}$. From the properties of standard Brownian motion, $Z$ follows a Gaussian distribution with mean $0$ and variance $T-t$, i.e., $Z \\sim \\mathcal{N}(0, T-t)$. The expectation term is:\n$$\n\\mathbb{E}\\left[\\exp(\\alpha Z)\\right] \\exp\\left(-\\frac{1}{2}\\alpha^{2}(T-t)\\right).\n$$\nThe moment-generating function of a Gaussian random variable $Y \\sim \\mathcal{N}(\\mu, \\sigma^{2})$ is $\\mathbb{E}[\\exp(sY)] = \\exp(s\\mu + \\frac{1}{2}s^{2}\\sigma^{2})$. For $Z$, we have $s=\\alpha$, $\\mu=0$, and $\\sigma^{2}=T-t$. Thus,\n$$\n\\mathbb{E}[\\exp(\\alpha Z)] = \\exp\\left(\\alpha \\cdot 0 + \\frac{1}{2}\\alpha^{2}(T-t)\\right) = \\exp\\left(\\frac{1}{2}\\alpha^{2}(T-t)\\right).\n$$\nSubstituting this back, the expectation term evaluates to:\n$$\n\\exp\\left(\\frac{1}{2}\\alpha^{2}(T-t)\\right) \\exp\\left(-\\frac{1}{2}\\alpha^{2}(T-t)\\right) = \\exp(0) = 1.\n$$\nTherefore, the closed-form expression for $X_{t}$ is\n$$\nX_{t} = \\exp\\left(\\alpha W_{t} - \\frac{1}{2}\\alpha^{2}t\\right).\n$$\nThis is the well-known Doléans-Dade exponential martingale.\n\nFor Task 2, we fix $p>1$ and compute $\\sup_{0\\le t\\le T}\\mathbb{E}[X_{t}^{p}]$.\nFirst, we compute $\\mathbb{E}[X_{t}^{p}]$ for any $t \\in [0,T]$.\n$$\nX_{t}^{p} = \\left(\\exp\\left(\\alpha W_{t} - \\frac{1}{2}\\alpha^{2}t\\right)\\right)^{p} = \\exp\\left(p\\alpha W_{t} - \\frac{p}{2}\\alpha^{2}t\\right).\n$$\nTaking the expectation:\n$$\n\\mathbb{E}[X_{t}^{p}] = \\mathbb{E}\\left[\\exp\\left(p\\alpha W_{t} - \\frac{p}{2}\\alpha^{2}t\\right)\\right] = \\exp\\left(-\\frac{p}{2}\\alpha^{2}t\\right) \\mathbb{E}\\left[\\exp(p\\alpha W_{t})\\right].\n$$\nWe use the moment-generating function for $W_{t} \\sim \\mathcal{N}(0,t)$ with parameter $s=p\\alpha$:\n$$\n\\mathbb{E}\\left[\\exp(p\\alpha W_{t})\\right] = \\exp\\left(\\frac{1}{2}(p\\alpha)^{2}t\\right) = \\exp\\left(\\frac{p^{2}\\alpha^{2}t}{2}\\right).\n$$\nSubstituting this into the expression for $\\mathbb{E}[X_{t}^{p}]$:\n$$\n\\mathbb{E}[X_{t}^{p}] = \\exp\\left(-\\frac{p\\alpha^{2}t}{2}\\right) \\exp\\left(\\frac{p^{2}\\alpha^{2}t}{2}\\right) = \\exp\\left(\\frac{p^{2}\\alpha^{2}t - p\\alpha^{2}t}{2}\\right) = \\exp\\left(\\frac{p(p-1)\\alpha^{2}t}{2}\\right).\n$$\nNow, we find the supremum over $t \\in [0,T]$. Let $C = \\frac{1}{2}p(p-1)\\alpha^{2}$. Since $p>1$, we have $p-1>0$, and thus $C \\ge 0$. The function $f(t) = \\exp(Ct)$ is non-decreasing for $C \\ge 0$. Therefore, its supremum on the interval $[0,T]$ is attained at $t=T$.\n$$\n\\sup_{0\\le t\\le T}\\mathbb{E}[X_{t}^{p}] = \\mathbb{E}[X_{T}^{p}] = \\exp\\left(\\frac{1}{2}p(p-1)\\alpha^{2}T\\right).\n$$\nThis value is finite for any given $p>1$, $\\alpha \\in \\mathbb{R}$ and $T>0$. For the second part of Task 2, we explain why this implies uniform integrability. The de la Vallée-Poussin criterion states that a family of random variables $\\{Y_i\\}_{i \\in I}$ is uniformly integrable if there exists a non-negative, increasing, and convex function $\\Phi: [0, \\infty) \\to [0, \\infty)$ such that $\\lim_{x\\to\\infty} \\frac{\\Phi(x)}{x} = \\infty$ and $\\sup_{i \\in I} \\mathbb{E}[\\Phi(|Y_i|)] < \\infty$.\nWe consider the family of random variables $\\{X_t\\}_{t \\in [0,T]}$. Since we are given $p>1$, we can choose the function $\\Phi(x) = x^{p}$. This function is non-negative, increasing for $x \\ge 0$, and convex. It also satisfies $\\lim_{x\\to\\infty} \\frac{x^p}{x} = \\lim_{x\\to\\infty} x^{p-1} = \\infty$ since $p-1 > 0$. We have shown that\n$$\n\\sup_{t \\in [0,T]} \\mathbb{E}[\\Phi(|X_t|)] = \\sup_{t \\in [0,T]} \\mathbb{E}[X_t^p] = \\exp\\left(\\frac{1}{2}p(p-1)\\alpha^{2}T\\right) < \\infty.\n$$\nTherefore, by the de la Vallée-Poussin criterion, the family $\\{X_t\\}_{t \\in [0,T]}$ is uniformly integrable.\n\nFinally, for Task 3, we use Doob's $L^{p}$ inequality to find an upper bound for $\\mathbb{E}[\\sup_{0\\le s\\le T} X_{s}^{p}]$.\nDoob's $L^{p}$ inequality for a non-negative submartingale $(Y_{t})_{t\\in[0,T]}$ and a constant $p>1$ states:\n$$\n\\mathbb{E}\\left[\\left(\\sup_{0\\le t\\le T} Y_{t}\\right)^{p}\\right] \\le \\left(\\frac{p}{p-1}\\right)^{p} \\mathbb{E}[Y_{T}^{p}].\n$$\nWe have already established that $(X_{t})_{t\\in[0,T]}$ is a non-negative submartingale. Since the function $x \\mapsto x^p$ is increasing for non-negative $x$, we have $(\\sup_{0\\le s\\le T} X_s)^p = \\sup_{0\\le s\\le T} X_s^p$. Applying the inequality to $X_t$:\n$$\n\\mathbb{E}\\left[\\sup_{0\\le s\\le T} X_{s}^{p}\\right] \\le \\left(\\frac{p}{p-1}\\right)^{p} \\mathbb{E}[X_{T}^{p}].\n$$\nFrom Task 2, we have the closed-form expression for $\\mathbb{E}[X_{T}^{p}]$:\n$$\n\\mathbb{E}[X_{T}^{p}] = \\exp\\left(\\frac{1}{2}p(p-1)\\alpha^{2}T\\right).\n$$\nSubstituting this into the inequality, we obtain the simplest closed-form upper bound:\n$$\n\\mathbb{E}\\left[\\sup_{0\\le s\\le T} X_{s}^{p}\\right] \\le \\left(\\frac{p}{p-1}\\right)^{p} \\exp\\left(\\frac{1}{2}p(p-1)\\alpha^{2}T\\right).\n$$\nThis expression provides the required upper bound in terms of $p$, $\\alpha$, and $T$.", "answer": "$$\n\\boxed{\\left(\\frac{p}{p-1}\\right)^{p} \\exp\\left(\\frac{1}{2}p(p-1)\\alpha^{2}T\\right)}\n$$", "id": "2973872"}]}