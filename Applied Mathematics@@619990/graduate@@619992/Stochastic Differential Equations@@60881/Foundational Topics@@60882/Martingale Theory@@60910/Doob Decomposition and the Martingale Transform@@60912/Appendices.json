{"hands_on_practices": [{"introduction": "The Doob decomposition is a cornerstone of modern stochastic analysis, providing a canonical way to separate any adapted process into its \"unpredictable\" and \"predictable\" components. This first exercise challenges you to construct this decomposition from scratch in a discrete-time setting, reinforcing the fundamental definitions of martingales and predictable processes. By working through the derivation and proving uniqueness [@problem_id:2973617], you will gain a deep, foundational understanding of how a process's structure is revealed through conditional expectation.", "problem": "Let $(\\Omega,\\mathcal{F},(\\mathcal{F}_n)_{n\\geq 0},\\mathbb{P})$ be a filtered probability space, and let $(Y_k)_{k\\geq 1}$ be an $(\\mathcal{F}_k)_{k\\geq 0}$-adapted sequence such that $\\mathbb{E}[|Y_k|]<\\infty$ for all $k\\geq 1$. Define the process $(X_n)_{n\\geq 0}$ by $X_0=0$ and\n$$\nX_n=\\sum_{k=1}^n Y_k,\\quad n\\geq 1.\n$$\nStarting only from the definitions of adaptedness, conditional expectation, martingale, submartingale, predictable process, and the tower property of conditional expectation, derive the discrete-time Doob decomposition of $(X_n)$ into a sum\n$$\nX_n=M_n+A_n,\n$$\nwhere $(M_n)$ is a martingale with $M_0=0$ and $(A_n)$ is predictable with $A_0=0$. Justify uniqueness of this decomposition.\n\nThen, determine necessary and sufficient conditions on $(Y_k)$ such that $(X_n)$ is a submartingale and $(A_n)$ is increasing. In this same setting, identify the predictable component explicitly in terms of conditional expectations.\n\nExpress your final answer as the explicit formula for $A_n$ in terms of $(Y_k)$ and $(\\mathcal{F}_n)_{n\\geq 0}$. Do not include any additional commentary in the final answer. No rounding is required and no physical units are involved.", "solution": "The problem asks for the derivation and justification of the unique Doob decomposition for a specific-structured adapted process $(X_n)_{n\\geq 0}$, and for an analysis of the conditions under which it is a submartingale.\n\nLet the given filtered probability space be $(\\Omega,\\mathcal{F},(\\mathcal{F}_n)_{n\\geq 0},\\mathbb{P})$. Let $(Y_k)_{k\\geq 1}$ be a sequence of random variables adapted to the filtration $(\\mathcal{F}_k)_{k\\geq 0}$, meaning $Y_k$ is $\\mathcal{F}_k$-measurable for each $k \\geq 1$. It is also given that $\\mathbb{E}[|Y_k|]<\\infty$ for all $k\\geq 1$. The process $(X_n)_{n\\geq 0}$ is defined by $X_0=0$ and $X_n=\\sum_{k=1}^n Y_k$ for $n\\geq 1$.\nThe process $(X_n)_{n\\geq 0}$ is adapted to $(\\mathcal{F}_n)_{n\\geq 0}$ because for any $n\\geq 1$, $X_n$ is a sum of random variables $Y_k$ where each $Y_k$ is $\\mathcal{F}_k$-measurable. Since the filtration is increasing, i.e., $\\mathcal{F}_k \\subseteq \\mathcal{F}_n$ for $k \\leq n$, each $Y_k$ ($k \\leq n$) is also $\\mathcal{F}_n$-measurable. Therefore, their sum $X_n$ is $\\mathcal{F}_n$-measurable. $X_0=0$ is a constant, hence $\\mathcal{F}_0$-measurable.\nThe process is also integrable, as $\\mathbb{E}[|X_n|] = \\mathbb{E}[|\\sum_{k=1}^n Y_k|] \\leq \\sum_{k=1}^n \\mathbb{E}[|Y_k|] < \\infty$.\n\n**Part 1: Derivation of the Doob Decomposition**\n\nWe seek a decomposition $X_n = M_n + A_n$ where $(M_n)_{n\\geq 0}$ is a martingale with $M_0=0$ and $(A_n)_{n\\geq 0}$ is a predictable process with $A_0=0$.\nA process $(A_n)_{n\\geq 0}$ is predictable if $A_0$ is a constant and $A_n$ is $\\mathcal{F}_{n-1}$-measurable for all $n \\geq 1$.\nA process $(M_n)_{n\\geq 0}$ is a martingale if it is adapted, integrable, and satisfies $\\mathbb{E}[M_n|\\mathcal{F}_{n-1}] = M_{n-1}$ for all $n \\geq 1$.\n\nLet's consider the change in the process from time $n-1$ to $n$:\n$X_n - X_{n-1} = (M_n - M_{n-1}) + (A_n - A_{n-1})$.\nFor $n\\geq 1$, we have $X_n - X_{n-1} = Y_n$.\nSo, $Y_n = (M_n - M_{n-1}) + (A_n - A_{n-1})$.\n\nLet's take the conditional expectation with respect to $\\mathcal{F}_{n-1}$:\n$\\mathbb{E}[Y_n|\\mathcal{F}_{n-1}] = \\mathbb{E}[M_n - M_{n-1}|\\mathcal{F}_{n-1}] + \\mathbb{E}[A_n - A_{n-1}|\\mathcal{F}_{n-1}]$.\nFor $(M_n)$ to be a martingale, we must have $\\mathbb{E}[M_n|\\mathcal{F}_{n-1}]=M_{n-1}$, which implies $\\mathbb{E}[M_n-M_{n-1}|\\mathcal{F}_{n-1}] = 0$.\nFor $(A_n)$ to be predictable, $A_n$ must be $\\mathcal{F}_{n-1}$-measurable for $n \\geq 1$. Consequently, $A_{n-1}$ is $\\mathcal{F}_{n-2}$-measurable, thus also $\\mathcal{F}_{n-1}$-measurable. Therefore, the increment $A_n - A_{n-1}$ must be $\\mathcal{F}_{n-1}$-measurable. This means $\\mathbb{E}[A_n-A_{n-1}|\\mathcal{F}_{n-1}] = A_n-A_{n-1}$.\n\nSubstituting these properties into the conditional expectation equation gives:\n$\\mathbb{E}[Y_n|\\mathcal{F}_{n-1}] = 0 + (A_n - A_{n-1})$.\nThis provides a natural definition for the increments of the predictable process $(A_n)$.\nLet us define $(A_n)_{n\\geq 0}$ as follows:\n$A_0 = 0$.\nFor $n \\geq 1$, let $A_n - A_{n-1} = \\mathbb{E}[Y_n|\\mathcal{F}_{n-1}]$.\nSumming this from $k=1$ to $n$, we get an explicit formula for $A_n$:\n$A_n = A_0 + \\sum_{k=1}^n (A_k - A_{k-1}) = \\sum_{k=1}^n \\mathbb{E}[Y_k|\\mathcal{F}_{k-1}]$.\n\nNow, let's verify that this proposed $(A_n)$ is indeed predictable. $A_0=0$ is a constant. For $n \\geq 1$, $A_n = \\sum_{k=1}^n \\mathbb{E}[Y_k|\\mathcal{F}_{k-1}]$. By definition, $\\mathbb{E}[Y_k|\\mathcal{F}_{k-1}]$ is an $\\mathcal{F}_{k-1}$-measurable random variable. Since the filtration is increasing, for any $k \\leq n$, $\\mathcal{F}_{k-1} \\subseteq \\mathcal{F}_{n-1}$. Thus, each term $\\mathbb{E}[Y_k|\\mathcal{F}_{k-1}]$ is $\\mathcal{F}_{n-1}$-measurable. The sum of $\\mathcal{F}_{n-1}$-measurable random variables is also $\\mathcal{F}_{n-1}$-measurable. Therefore, $A_n$ is $\\mathcal{F}_{n-1}$-measurable, and $(A_n)$ is a predictable process.\n\nNext, we define $(M_n)_{n\\geq 0}$ by $M_n = X_n - A_n$.\nWe must verify that $(M_n)$ is a martingale with $M_0=0$.\n$M_0 = X_0 - A_0 = 0 - 0 = 0$.\nFor $(M_n)$ to be a martingale, it must be adapted, integrable, and satisfy the martingale property.\n1.  **Adaptedness**: $X_n$ is $\\mathcal{F}_n$-measurable. $A_n$ is $\\mathcal{F}_{n-1}$-measurable, so it is also $\\mathcal{F}_n$-measurable. Thus, $M_n = X_n - A_n$ is $\\mathcal{F}_n$-measurable. $(M_n)$ is adapted.\n2.  **Integrability**: We have $\\mathbb{E}[|X_n|] < \\infty$. Using Jensen's inequality for conditional expectation, $\\mathbb{E}[|A_n|] \\leq \\sum_{k=1}^n \\mathbb{E}[|\\mathbb{E}[Y_k|\\mathcal{F}_{k-1}]|] \\leq \\sum_{k=1}^n \\mathbb{E}[\\mathbb{E}[|Y_k||\\mathcal{F}_{k-1}]]$. By the tower property, this is $\\sum_{k=1}^n \\mathbb{E}[|Y_k|] < \\infty$. Since $\\mathbb{E}[|M_n|] \\leq \\mathbb{E}[|X_n|] + \\mathbb{E}[|A_n|]$, $(M_n)$ is integrable.\n3.  **Martingale Property**: For $n \\geq 1$, we check if $\\mathbb{E}[M_n|\\mathcal{F}_{n-1}] = M_{n-1}$.\n    $\\mathbb{E}[M_n|\\mathcal{F}_{n-1}] = \\mathbb{E}[X_n - A_n | \\mathcal{F}_{n-1}]$.\n    Using $X_n = X_{n-1} + Y_n$:\n    $\\mathbb{E}[X_{n-1} + Y_n - A_n | \\mathcal{F}_{n-1}]$.\n    Since $X_{n-1}$ is $\\mathcal{F}_{n-1}$-measurable and $A_n$ is $\\mathcal{F}_{n-1}$-measurable (by predictability), we can take them out of the conditional expectation:\n    $\\mathbb{E}[M_n|\\mathcal{F}_{n-1}] = X_{n-1} + \\mathbb{E}[Y_n|\\mathcal{F}_{n-1}] - A_n$.\n    By our construction of $A_n$, we have $\\mathbb{E}[Y_n|\\mathcal{F}_{n-1}] = A_n - A_{n-1}$. Substituting this into the equation:\n    $\\mathbb{E}[M_n|\\mathcal{F}_{n-1}] = X_{n-1} + (A_n - A_{n-1}) - A_n = X_{n-1} - A_{n-1}$.\n    By definition, $M_{n-1} = X_{n-1} - A_{n-1}$.\n    Thus, $\\mathbb{E}[M_n|\\mathcal{F}_{n-1}] = M_{n-1}$. The martingale property holds.\nThis completes the derivation of the Doob decomposition.\n\n**Part 2: Uniqueness of the Decomposition**\n\nSuppose there exists another decomposition $X_n = M'_n + A'_n$, where $(M'_n)$ is a martingale with $M'_0=0$ and $(A'_n)$ is a predictable process with $A'_0=0$.\nThen we have $M_n + A_n = M'_n + A'_n$, which implies $M_n - M'_n = A'_n - A_n$.\nLet $D_n = M_n - M'_n$. As a difference of two martingales, $(D_n)$ is also a martingale with $D_0=M_0-M'_0=0-0=0$.\nLet $P_n = A'_n - A_n$. As a difference of two predictable processes, $(P_n)$ is also a predictable process with $P_0=A'_0-A_0=0-0=0$.\nWe have $D_n=P_n$ for all $n \\geq 0$. This means $(D_n)$ is a process that is simultaneously a martingale and predictable.\nFrom the martingale property, for $n \\geq 1$: $D_{n-1} = \\mathbb{E}[D_n|\\mathcal{F}_{n-1}]$.\nFrom the predictable property, for $n \\geq 1$: $D_n$ is $\\mathcal{F}_{n-1}$-measurable. This implies $\\mathbb{E}[D_n|\\mathcal{F}_{n-1}] = D_n$.\nCombining these two equations gives $D_n = D_{n-1}$ for all $n \\geq 1$.\nBy induction, $D_n=D_{n-1}=...=D_0$. Since $D_0=0$, we must have $D_n=0$ for all $n \\geq 0$.\n$D_n=0$ implies $M_n - M'_n = 0$, so $M_n=M'_n$.\n$P_n=0$ implies $A'_n - A_n = 0$, so $A'_n=A_n$.\nThe decomposition is unique.\n\n**Part 3: Conditions for Submartingale and Increasing Predictable Part**\n\nA process $(Z_n)$ is a submartingale if it is adapted, integrable, and satisfies $\\mathbb{E}[Z_n|\\mathcal{F}_{n-1}] \\geq Z_{n-1}$ for all $n \\geq 1$.\nLet's apply this condition to our process $(X_n)$:\nWe need $\\mathbb{E}[X_n|\\mathcal{F}_{n-1}] \\geq X_{n-1}$ for $n \\geq 1$.\nSince $X_n = X_{n-1} + Y_n$ and $X_{n-1}$ is $\\mathcal{F}_{n-1}$-measurable:\n$\\mathbb{E}[X_{n-1} + Y_n|\\mathcal{F}_{n-1}] \\geq X_{n-1}$\n$X_{n-1} + \\mathbb{E}[Y_n|\\mathcal{F}_{n-1}] \\geq X_{n-1}$\n$\\mathbb{E}[Y_n|\\mathcal{F}_{n-1}] \\geq 0$.\nThis inequality must hold almost surely for all $n\\geq 1$. This is the necessary and sufficient condition for $(X_n)$ to be a submartingale.\n\nNow consider the condition for $(A_n)$ to be an increasing process. This means $A_n \\geq A_{n-1}$ almost surely for all $n \\geq 1$.\nFrom our derivation, $A_n-A_{n-1} = \\mathbb{E}[Y_n|\\mathcal{F}_{n-1}]$.\nSo, the condition $A_n \\geq A_{n-1}$ is equivalent to $\\mathbb{E}[Y_n|\\mathcal{F}_{n-1}] \\geq 0$.\nThis is precisely the same condition as for $(X_n)$ being a submartingale.\nTherefore, the necessary and sufficient conditions for $(X_n)$ to be a submartingale and for its predictable component $(A_n)$ to be increasing are identical: $\\mathbb{E}[Y_n|\\mathcal{F}_{n-1}] \\geq 0$ for all $n \\geq 1$.\n\nIn this setting, the predictable component $(A_n)$ is explicitly given by the formula derived in Part 1. This formula is general and does not rely on the submartingale property, but under this property, each increment of $A_n$ is non-negative, confirming it is an increasing process. The explicit formula for $A_n$ is:\n$A_n = \\sum_{k=1}^n \\mathbb{E}[Y_k|\\mathcal{F}_{k-1}]$ for $n \\geq 1$, with $A_0=0$.", "answer": "$$\n\\boxed{A_n = \\sum_{k=1}^n \\mathbb{E}[Y_k|\\mathcal{F}_{k-1}]}\n$$", "id": "2973617"}, {"introduction": "Having established the discrete-time decomposition, we now turn our attention to its continuous-time analogue, the Doob-Meyer theorem. This practice focuses on a famous process in stochastic calculus, the exponential martingale, which is foundational in financial modeling and for changes of measure. By applying the powerful tool of Itô's lemma [@problem_id:2973590], you will explicitly compute the decomposition and discover that the predictable component, or compensator, is zero, thus rigorously proving the process is a true martingale.", "problem": "Let $(\\Omega,\\mathcal{F},(\\mathcal{F}_t)_{t\\ge 0},\\mathbb{P})$ be a filtered probability space satisfying the usual conditions and supporting a standard one-dimensional Brownian motion $B=(B_t)_{t\\ge 0}$. Fix $\\theta\\in\\mathbb{R}$ and define the process $X=(X_t)_{t\\ge 0}$ by\n$$\nX_t \\equiv \\exp\\!\\big(\\theta B_t - \\tfrac{1}{2}\\theta^2 t\\big), \\qquad t\\ge 0.\n$$\nWork with the natural filtration of $B_t$. Starting only from the fundamental definitions and facts of Itô calculus and the Doob–Meyer decomposition (that every integrable submartingale admits a unique decomposition into a martingale plus a predictable finite-variation process), do the following:\n\n1. Derive the semimartingale decomposition of $X_t$ by computing its stochastic differential, and then identify the Doob–Meyer decomposition $X_t = X_0 + M_t + A_t$ where $M_t$ is an $(\\mathcal{F}_t)$-martingale starting at $0$ and $A_t$ is a predictable process of finite variation with $A_0=0$.\n\n2. Prove that $X_t$ is a true martingale by computing $\\mathbb{E}[X_t]$ explicitly using only the Gaussian law of $B_t$ and fundamental properties of the exponential function. Conclude from the Doob–Meyer theorem that the compensator is uniquely determined.\n\n3. To connect with the martingale transform, let $H=(H_t)_{t\\ge 0}$ be a bounded $(\\mathcal{F}_t)$-predictable process and define $Y_t \\equiv \\int_0^t H_s\\,\\mathrm{d}X_s$. Using only the basic definition of the stochastic integral with respect to continuous martingales and the result of part 1, justify that $Y_t$ is an $(\\mathcal{F}_t)$-martingale.\n\nYour final answer must be a single explicit analytic expression for the compensator $A_t$ in the Doob–Meyer decomposition of $X_t$ as a function of $t$ and $\\theta$. No units are involved. If your final analytic expression simplifies to a constant, provide it in its simplest exact form.", "solution": "The problem as stated is valid. It is a well-posed, objective, and scientifically grounded exercise within the established mathematical framework of stochastic calculus. All terms are standard, and the tasks required are clear and solvable using fundamental principles.\n\nWe proceed with the solution, addressing the three tasks in order. The process in question is $X_t = \\exp(\\theta B_t - \\frac{1}{2}\\theta^2 t)$ for a constant $\\theta \\in \\mathbb{R}$ and a standard one-dimensional Brownian motion $B_t$ starting at $B_0=0$. The filtration $(\\mathcal{F}_t)_{t \\ge 0}$ is the natural filtration generated by $B_t$. At time $t=0$, we have $X_0 = \\exp(\\theta B_0 - 0) = \\exp(0) = 1$.\n\n1. To find the semimartingale decomposition of $X_t$, we apply Itô's lemma. Let $f(t, x) = \\exp(\\theta x - \\frac{1}{2}\\theta^2 t)$. Then $X_t = f(t, B_t)$. We require the partial derivatives of $f(t,x)$:\n$$\n\\frac{\\partial f}{\\partial t}(t,x) = -\\frac{1}{2}\\theta^2 \\exp\\big(\\theta x - \\tfrac{1}{2}\\theta^2 t\\big) = -\\frac{1}{2}\\theta^2 f(t,x)\n$$\n$$\n\\frac{\\partial f}{\\partial x}(t,x) = \\theta \\exp\\big(\\theta x - \\tfrac{1}{2}\\theta^2 t\\big) = \\theta f(t,x)\n$$\n$$\n\\frac{\\partial^2 f}{\\partial x^2}(t,x) = \\theta^2 \\exp\\big(\\theta x - \\tfrac{1}{2}\\theta^2 t\\big) = \\theta^2 f(t,x)\n$$\nItô's formula for a process $f(t, B_t)$ states that\n$$\n\\mathrm{d}f(t, B_t) = \\frac{\\partial f}{\\partial t}(t, B_t) \\mathrm{d}t + \\frac{\\partial f}{\\partial x}(t, B_t) \\mathrm{d}B_t + \\frac{1}{2}\\frac{\\partial^2 f}{\\partial x^2}(t, B_t)(\\mathrm{d}B_t)^2\n$$\nSubstituting our derivatives and using the quadratic variation of Brownian motion, $(\\mathrm{d}B_t)^2 = \\mathrm{d}t$, and replacing $f(t, B_t)$ with $X_t$, we get the stochastic differential for $X_t$:\n$$\n\\mathrm{d}X_t = \\left(-\\frac{1}{2}\\theta^2 X_t\\right) \\mathrm{d}t + (\\theta X_t) \\mathrm{d}B_t + \\frac{1}{2}(\\theta^2 X_t) \\mathrm{d}t\n$$\nThe terms involving $\\mathrm{d}t$ cancel each other out:\n$$\n\\mathrm{d}X_t = \\left(-\\frac{1}{2}\\theta^2 X_t + \\frac{1}{2}\\theta^2 X_t\\right) \\mathrm{d}t + \\theta X_t \\mathrm{d}B_t = \\theta X_t \\mathrm{d}B_t\n$$\nThis is the stochastic differential of $X_t$. In integral form, this is\n$$\nX_t - X_0 = \\int_0^t \\theta X_s \\mathrm{d}B_s\n$$\nThe problem asks for the Doob–Meyer decomposition $X_t = X_0 + M_t + A_t$. Comparing with our result, we can identify\n$$\nM_t = \\int_0^t \\theta X_s \\mathrm{d}B_s\n$$\nand\n$$\nA_t = 0\n$$\nThe process $M_t$ is a stochastic integral with respect to Brownian motion. Since the integrand $\\theta X_s$ is an $(\\mathcal{F}_s)$-adapted process, $M_t$ is a continuous local martingale with $M_0 = 0$. The process $A_t=0$ is trivially a predictable process of finite (zero) variation with $A_0=0$. Thus, we have found the required decomposition. The compensator is $A_t$.\n\n2. To prove that $X_t$ is a true martingale, we will compute its expectation $\\mathbb{E}[X_t]$. Since $X_t > 0$, showing $\\mathbb{E}[X_t] = 1$ (its initial value) is sufficient to upgrade the local martingale property to a true martingale property.\n$$\n\\mathbb{E}[X_t] = \\mathbb{E}\\left[\\exp\\big(\\theta B_t - \\tfrac{1}{2}\\theta^2 t\\big)\\right]\n$$\nThe term $\\exp(-\\frac{1}{2}\\theta^2 t)$ is deterministic and can be factored out of the expectation:\n$$\n\\mathbb{E}[X_t] = \\exp\\big(-\\tfrac{1}{2}\\theta^2 t\\big) \\mathbb{E}\\left[\\exp(\\theta B_t)\\right]\n$$\nFor a standard Brownian motion, $B_t$ is a normally distributed random variable with mean $0$ and variance $t$, i.e., $B_t \\sim \\mathcal{N}(0, t)$. The expectation $\\mathbb{E}[\\exp(\\theta B_t)]$ is the moment-generating function of $B_t$ evaluated at $\\theta$. The moment-generating function of a random variable $Y \\sim \\mathcal{N}(\\mu, \\sigma^2)$ is $M_Y(k) = \\exp(k\\mu + \\frac{1}{2}k^2\\sigma^2)$. For $B_t$, we have $\\mu=0$ and $\\sigma^2=t$, so its moment-generating function is $M_{B_t}(k) = \\exp(\\frac{1}{2}k^2 t)$. Evaluating at $k=\\theta$, we get:\n$$\n\\mathbb{E}[\\exp(\\theta B_t)] = \\exp\\big(\\tfrac{1}{2}\\theta^2 t\\big)\n$$\nSubstituting this back into the expression for $\\mathbb{E}[X_t]$:\n$$\n\\mathbb{E}[X_t] = \\exp\\big(-\\tfrac{1}{2}\\theta^2 t\\big) \\cdot \\exp\\big(\\tfrac{1}{2}\\theta^2 t\\big) = \\exp(0) = 1\n$$\nSince $X_t$ is a positive local martingale (from part 1) and its expectation is constant and equal to its initial value $X_0=1$, $X_t$ is a true martingale.\nThe Doob–Meyer theorem states that an integrable submartingale admits a unique decomposition into a martingale and a predictable, increasing process (the compensator). Since $X_t > 0$, it is a submartingale. Since we have just proven it is in fact a true martingale, its decomposition is $X_t = X_t + 0$. To align with the form $X_t = X_0 + M_t + A_t$, we write $X_t - X_0 = (X_t - X_0) + 0$, which implies $M_t = X_t - X_0$ and $A_t=0$. The uniqueness part of the theorem guarantees that this is the only such decomposition, thus uniquely determining the compensator as $A_t=0$, which is consistent with our finding in part 1.\n\n3. We are given a bounded, $(\\mathcal{F}_t)$-predictable process $H=(H_t)_{t\\ge 0}$ and we define $Y_t = \\int_0^t H_s\\,\\mathrm{d}X_s$. We must justify that $Y_t$ is a martingale.\nFrom part 1, we have the stochastic differential $\\mathrm{d}X_t = \\theta X_t \\mathrm{d}B_t$. Substituting this into the definition of $Y_t$:\n$$\nY_t = \\int_0^t H_s (\\theta X_s \\mathrm{d}B_s) = \\int_0^t (\\theta H_s X_s) \\mathrm{d}B_s\n$$\nThis expresses $Y_t$ as a stochastic integral with respect to Brownian motion. Let the integrand be $K_s = \\theta H_s X_s$. For the Itô integral $Y_t = \\int_0^t K_s \\mathrm{d}B_s$ to be a martingale, the process $K_s$ must be adapted and satisfy the condition $\\mathbb{E}[\\int_0^t K_s^2 \\mathrm{d}s] < \\infty$ for all $t \\ge 0$.\nThe process $H_s$ is predictable, hence adapted. The process $X_s$ is adapted as it depends on $B_s$. Their product $K_s = \\theta H_s X_s$ is therefore also adapted.\nNow we check the integrability condition. Let's assume $H$ is bounded by a constant $C>0$, i.e., $|H_s(\\omega)| \\le C$ for all $s$ and $\\omega$.\n$$\n\\mathbb{E}\\left[\\int_0^t K_s^2 \\mathrm{d}s\\right] = \\mathbb{E}\\left[\\int_0^t (\\theta H_s X_s)^2 \\mathrm{d}s\\right] = \\mathbb{E}\\left[\\int_0^t \\theta^2 H_s^2 X_s^2 \\mathrm{d}s\\right]\n$$\nUsing Tonelli's theorem (since the integrand is non-negative) and the bound on $H_s$:\n$$\n\\mathbb{E}\\left[\\int_0^t K_s^2 \\mathrm{d}s\\right] = \\int_0^t \\theta^2 \\mathbb{E}[H_s^2 X_s^2] \\mathrm{d}s \\le \\int_0^t \\theta^2 C^2 \\mathbb{E}[X_s^2] \\mathrm{d}s = \\theta^2 C^2 \\int_0^t \\mathbb{E}[X_s^2] \\mathrm{d}s\n$$\nWe compute $\\mathbb{E}[X_s^2]$:\n$$\n\\mathbb{E}[X_s^2] = \\mathbb{E}\\left[\\left(\\exp(\\theta B_s - \\tfrac{1}{2}\\theta^2 s)\\right)^2\\right] = \\mathbb{E}\\left[\\exp(2\\theta B_s - \\theta^2 s)\\right] = \\exp(-\\theta^2 s) \\mathbb{E}[\\exp(2\\theta B_s)]\n$$\nUsing the moment-generating function of $B_s \\sim \\mathcal{N}(0, s)$ evaluated at $2\\theta$:\n$$\n\\mathbb{E}[\\exp(2\\theta B_s)] = \\exp\\left(\\frac{1}{2}(2\\theta)^2 s\\right) = \\exp(2\\theta^2 s)\n$$\nTherefore,\n$$\n\\mathbb{E}[X_s^2] = \\exp(-\\theta^2 s) \\exp(2\\theta^2 s) = \\exp(\\theta^2 s)\n$$\nPlugging this back into the integral, assuming $\\theta \\ne 0$:\n$$\n\\mathbb{E}\\left[\\int_0^t K_s^2 \\mathrm{d}s\\right] \\le \\theta^2 C^2 \\int_0^t \\exp(\\theta^2 s) \\mathrm{d}s = \\theta^2 C^2 \\left[ \\frac{1}{\\theta^2}\\exp(\\theta^2 s) \\right]_0^t = C^2 (\\exp(\\theta^2 t) - 1)\n$$\nIf $\\theta=0$, then $X_t=1$, $\\mathrm{d}X_t=0$, and $Y_t=0$, which is trivially a martingale. In this case, the inequality holds as $0 \\le 0$. For any $\\theta \\in \\mathbb{R}$ and finite $t$, the value $C^2 (\\exp(\\theta^2 t) - 1)$ is finite.\nSince the integrand $K_s$ is adapted and satisfies $\\mathbb{E}[\\int_0^t K_s^2 \\mathrm{d}s] < \\infty$, the stochastic integral $Y_t = \\int_0^t K_s \\mathrm{d}B_s$ is a square-integrable martingale, and therefore a martingale. This relies on the fundamental definition and properties of the Itô integral, as requested.\n\nThe final answer required is the explicit analytic expression for the compensator $A_t$ in the Doob–Meyer decomposition of $X_t$. As determined in part 1 and confirmed in part 2, this is $A_t=0$.", "answer": "$$\n\\boxed{0}\n$$", "id": "2973590"}, {"introduction": "The power of the Doob decomposition truly shines when we analyze stochastic integrals, also known as martingale transforms. This problem frames the concept in an intuitive financial context, where a trading strategy is a predictable process and the resulting gain is a martingale transform. Your task is to determine the conditions under which a trading strategy on a submartingale asset results in a \"fair game\" (a martingale gain process) [@problem_id:1324733], revealing the critical link between the transform's properties and the compensator of the underlying process.", "problem": "In a simplified discrete-time financial model, all processes are adapted to a filtration $(\\mathcal{F}_n)_{n \\ge 0}$, where $\\mathcal{F}_n$ represents the information available up to time $n$. A process $H_n$ is called predictable if $H_n$ is measurable with respect to $\\mathcal{F}_{n-1}$ for each $n \\ge 1$.\n\nLet $S_n$ be a simple symmetric random walk starting at $S_0=0$, generated by i.i.d. steps $\\xi_k$ with $\\mathbb{P}(\\xi_k=1) = \\mathbb{P}(\\xi_k=-1) = 1/2$. The filtration is the natural one generated by this walk, $\\mathcal{F}_n = \\sigma(S_1, \\dots, S_n)$.\n\nAn asset's price is described by a process $X_n$ which is known to be a submartingale. A submartingale is a process satisfying $\\mathbb{E}[X_n | \\mathcal{F}_{n-1}] \\ge X_{n-1}$. Any submartingale admits a unique Doob decomposition $X_n = X_0 + M_n + A_n$, where $M_n$ is a martingale (satisfying $\\mathbb{E}[M_n | \\mathcal{F}_{n-1}] = M_{n-1}$) and $A_n$ is a non-decreasing, predictable process known as the compensator, with $M_0=A_0=0$. The compensator's increments, $\\Delta A_n = A_n - A_{n-1}$, quantify the \"drift\" of the submartingale, as $\\mathbb{E}[X_n - X_{n-1} | \\mathcal{F}_{n-1}] = \\Delta A_n$. For the asset $X_n$ in our model, its compensator is characterized by the increments $\\Delta A_n = \\lambda(S_{n-1}^2 - 81)^2$, where $\\lambda$ is a fixed positive constant.\n\nA trader employs a predictable strategy represented by a non-negative process $H_n$, which specifies the number of units of the asset to hold during the time interval $(n-1, n]$. The total gain from this strategy up to time $N$ is given by the martingale transform (stochastic integral) $(H \\cdot X)_N = \\sum_{n=1}^N H_n (X_n - X_{n-1})$. The trader's goal is to choose a strategy such that their gain process $(H \\cdot X)_n$ is a martingale, representing a \"fair game\" with no predictable profit or loss.\n\nThe specific strategy employed by the trader is to be active only when the underlying random walk $S_n$ hits a certain level. The strategy is defined as follows: for a specific positive integer $\\gamma$, the process $H_n$ is strictly positive if $S_{n-1} = \\gamma$ and is zero otherwise.\n\nDetermine the value of the integer $\\gamma$ for which the trader's gain process $(H \\cdot X)_n$ is a martingale.", "solution": "We use the Doob decomposition $X_{n} = X_{0} + M_{n} + A_{n}$, where $(M_{n})$ is a martingale and $(A_{n})$ is a predictable, non-decreasing process with $A_{0}=0$ and increments $\\Delta A_{n} = A_{n} - A_{n-1}$. For a predictable strategy $(H_{n})$, the gain process is\n$$(H \\cdot X)_{n} = \\sum_{k=1}^{n} H_{k} (X_{k} - X_{k-1}).$$\nWe compute its conditional drift using the tower property and predictability:\n\n$$\n\\begin{aligned}\n\\mathbb{E}\\big[(H \\cdot X)_{n} - (H \\cdot X)_{n-1} \\mid \\mathcal{F}_{n-1}\\big]\n&= \\mathbb{E}\\big[H_{n} (X_{n} - X_{n-1}) \\mid \\mathcal{F}_{n-1}\\big] \\\\\n&= H_{n} \\mathbb{E}\\big[X_{n} - X_{n-1} \\mid \\mathcal{F}_{n-1}\\big] \\\\\n&= H_{n} \\Delta A_{n},\n\\end{aligned}\n$$\n\nsince $\\mathbb{E}[M_{n} - M_{n-1} \\mid \\mathcal{F}_{n-1}] = 0$ and $\\mathbb{E}[X_{n} - X_{n-1} \\mid \\mathcal{F}_{n-1}] = \\Delta A_{n}$ for a submartingale with compensator $A_{n}$. Therefore, $(H \\cdot X)_{n}$ is a martingale if and only if\n$$H_{n} \\Delta A_{n} = 0 \\quad \\text{almost surely for all } n.$$\nGiven $\\Delta A_{n} = \\lambda (S_{n-1}^{2} - 81)^{2}$ with $\\lambda > 0$ and the strategy $H_{n} > 0$ if and only if $S_{n-1} = \\gamma$ and $H_{n} = 0$ otherwise, the condition reduces to requiring $\\Delta A_{n} = 0$ on the event $\\{S_{n-1} = \\gamma\\}$. This means\n$$(\\gamma^{2} - 81)^{2} = 0 \\quad \\Longleftrightarrow \\quad \\gamma^{2} = 81 \\quad \\Longleftrightarrow \\quad \\gamma \\in \\{-9, 9\\}.$$\nSince $\\gamma$ is specified to be a positive integer, we must take $\\gamma = 9$.", "answer": "$$\\boxed{9}$$", "id": "1324733"}]}