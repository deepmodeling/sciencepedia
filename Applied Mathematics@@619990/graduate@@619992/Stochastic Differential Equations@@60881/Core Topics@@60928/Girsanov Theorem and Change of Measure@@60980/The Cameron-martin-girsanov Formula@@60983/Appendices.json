{"hands_on_practices": [{"introduction": "The Cameron-Martin-Girsanov formula is powered by a specific stochastic process, the Doléans-Dade exponential, which acts as the density process for changing the probability measure. This exercise guides you through the foundational verification that this process is a true martingale under appropriate conditions, a critical step for its use. By applying the Optional Stopping Theorem, you will confirm a key property essential for applying the formula in contexts involving stopping times [@problem_id:3000259].", "problem": "Consider a filtered probability space $(\\Omega,\\mathcal{F},(\\mathcal{F}_{t})_{t\\geq 0},\\mathbb{P})$ satisfying the usual conditions, supporting a one-dimensional standard Brownian motion $W=(W_{t})_{t\\geq 0}$. Fix a finite horizon $T>0$. Let $\\theta=(\\theta_{t})_{t\\in[0,T]}$ be a progressively measurable process such that $|\\theta_{t}|\\leq K$ almost surely for all $t\\in[0,T]$, for some constant $K<\\infty$, and $\\int_{0}^{T}\\theta_{s}^{2}\\,\\mathrm{d}s<\\infty$ almost surely. Define the Doléans-Dade exponential\n$$\nZ_{t} \\;=\\; \\exp\\!\\Bigg(\\,\\int_{0}^{t}\\theta_{s}\\,\\mathrm{d}W_{s}\\;-\\;\\frac{1}{2}\\int_{0}^{t}\\theta_{s}^{2}\\,\\mathrm{d}s\\,\\Bigg),\\qquad t\\in[0,T].\n$$\nLet $\\tau$ be an $(\\mathcal{F}_{t})$-stopping time satisfying $0\\leq \\tau\\leq T$ almost surely.\n\nStarting only from the definitions of Brownian motion, stochastic integrals, quadratic variation, and Itô calculus (including Itô's formula), and using well-tested sufficient conditions for true martingales such as Novikov's condition, carry out the following:\n\n- Establish that $(Z_{t})_{t\\in[0,T]}$ is a true martingale with $\\mathbb{E}[Z_{t}]=1$ for all $t\\in[0,T]$.\n- Justify that the stopped process $(Z_{t\\wedge \\tau})_{t\\in[0,T]}$ is a martingale.\n- Apply the optional stopping theorem to evaluate $\\mathbb{E}[Z_{\\tau}]$.\n\nYour final answer must be a single real number. No rounding is required. No physical units are involved.", "solution": "The problem requires us to first establish that the process $(Z_t)_{t\\in[0,T]}$ is a true martingale, and then to use this property along with the Optional Stopping Theorem to compute the expectation $\\mathbb{E}[Z_{\\tau}]$.\n\nThe process $Z_t$ is the Doléans-Dade exponential, defined as\n$$\nZ_{t} \\;=\\; \\exp\\!\\Bigg(\\,\\int_{0}^{t}\\theta_{s}\\,\\mathrm{d}W_{s}\\;-\\;\\frac{1}{2}\\int_{0}^{t}\\theta_{s}^{2}\\,\\mathrm{d}s\\,\\Bigg).\n$$\nLet us define the process $X_t$ as the argument of the exponential:\n$$\nX_{t} \\;=\\; \\int_{0}^{t}\\theta_{s}\\,\\mathrm{d}W_{s}\\;-\\;\\frac{1}{2}\\int_{0}^{t}\\theta_{s}^{2}\\,\\mathrm{d}s.\n$$\nIn differential form, this is $\\mathrm{d}X_t = \\theta_t\\,\\mathrm{d}W_t - \\frac{1}{2}\\theta_t^2\\,\\mathrm{d}t$. The process $Z_t$ can be written as $Z_t = f(X_t)$ where $f(x) = \\exp(x)$. We apply Itô's formula to find the stochastic differential of $Z_t$. The derivatives of $f(x)$ are $f'(x) = \\exp(x)$ and $f''(x) = \\exp(x)$.\n\nThe quadratic variation of $X_t$ is given by\n$$\n\\mathrm{d}\\langle X \\rangle_{t} = \\left(\\theta_t\\right)^2 \\mathrm{d}\\langle W \\rangle_{t} = \\theta_t^2\\,\\mathrm{d}t.\n$$\nAccording to Itô's formula, the differential of $Z_t = f(X_t)$ is:\n$$\n\\mathrm{d}Z_t = f'(X_t)\\,\\mathrm{d}X_t + \\frac{1}{2}f''(X_t)\\,\\mathrm{d}\\langle X \\rangle_t.\n$$\nSubstituting the expressions for the derivatives and differentials, we get\n$$\n\\mathrm{d}Z_t = \\exp(X_t)\\left(\\theta_t\\,\\mathrm{d}W_t - \\frac{1}{2}\\theta_t^2\\,\\mathrm{d}t\\right) + \\frac{1}{2}\\exp(X_t)\\left(\\theta_t^2\\,\\mathrm{d}t\\right).\n$$\nRecognizing that $\\exp(X_t) = Z_t$, this simplifies to\n$$\n\\mathrm{d}Z_t = Z_t \\theta_t\\,\\mathrm{d}W_t - \\frac{1}{2}Z_t \\theta_t^2\\,\\mathrm{d}t + \\frac{1}{2}Z_t \\theta_t^2\\,\\mathrm{d}t = Z_t \\theta_t\\,\\mathrm{d}W_t.\n$$\nIn integral form, this equation becomes\n$$\nZ_t = Z_0 + \\int_0^t Z_s \\theta_s\\,\\mathrm{d}W_s.\n$$\nThe initial value is $Z_0 = \\exp(0-0) = 1$. Thus,\n$$\nZ_t = 1 + \\int_0^t Z_s \\theta_s\\,\\mathrm{d}W_s.\n$$\nThe process $\\theta_t$ is progressively measurable and bounded, and $Z_t$ is adapted and has continuous paths. Therefore, the product $Z_t \\theta_t$ is a progressively measurable process satisfying the standard integrability conditions for the Itô integral. The stochastic integral $\\int_0^t Z_s \\theta_s\\,\\mathrm{d}W_s$ is a continuous local martingale. Since $Z_t$ is the sum of a constant and a continuous local martingale, $(Z_t)_{t\\in[0,T]}$ is itself a continuous local martingale.\n\nTo establish that $(Z_t)_{t\\in[0,T]}$ is a true martingale, we verify Novikov's condition. The process $Z_t$ is the stochastic exponential of the process $M_t = \\int_0^t \\theta_s\\,\\mathrm{d}W_s$, so $Z_t = \\mathcal{E}(M)_t$. Novikov's condition states that if $\\mathbb{E}\\left[\\exp\\left(\\frac{1}{2}\\langle M \\rangle_T\\right)\\right] < \\infty$, then $\\mathcal{E}(M)_t$ is a true martingale on $[0,T]$. The quadratic variation of $M_t$ is $\\langle M \\rangle_t = \\int_0^t \\theta_s^2\\,\\mathrm{d}s$. We must check if\n$$\n\\mathbb{E}\\left[\\exp\\left(\\frac{1}{2}\\int_0^T \\theta_s^2\\,\\mathrm{d}s\\right)\\right] < \\infty.\n$$\nWe are given that the process $\\theta_t$ is bounded, i.e., there exists a constant $K < \\infty$ such that $|\\theta_t| \\leq K$ almost surely for all $t\\in[0,T]$. This implies $\\theta_t^2 \\leq K^2$ a.s. Therefore, the integral is bounded:\n$$\n\\int_0^T \\theta_s^2\\,\\mathrm{d}s \\leq \\int_0^T K^2\\,\\mathrm{d}s = K^2T \\quad \\text{a.s.}\n$$\nThis implies that the random variable inside the expectation is bounded:\n$$\n\\exp\\left(\\frac{1}{2}\\int_0^T \\theta_s^2\\,\\mathrm{d}s\\right) \\leq \\exp\\left(\\frac{1}{2}K^2T\\right) \\quad \\text{a.s.}\n$$\nSince $\\exp(\\frac{1}{2}K^2T)$ is a finite constant, the expectation is also finite:\n$$\n\\mathbb{E}\\left[\\exp\\left(\\frac{1}{2}\\int_0^T \\theta_s^2\\,\\mathrm{d}s\\right)\\right] \\leq \\exp\\left(\\frac{1}{2}K^2T\\right) < \\infty.\n$$\nNovikov's condition is satisfied, which proves that $(Z_t)_{t\\in[0,T]}$ is a true martingale.\nFor any martingale, the expectation is constant over time. Therefore, for any $t \\in [0,T]$,\n$$\n\\mathbb{E}[Z_t] = \\mathbb{E}[Z_0] = \\mathbb{E}[1] = 1.\n$$\nThis completes the first part of the task.\n\nNext, we must justify that the stopped process $(Z_{t\\wedge \\tau})_{t\\in[0,T]}$ is a martingale. This is a standard result from martingale theory: if $(M_t)$ is a martingale with respect to a filtration $(\\mathcal{F}_t)$ and $\\tau$ is an $(\\mathcal{F}_t)$-stopping time, then the stopped process $(M_{t\\wedge \\tau})$ is also a martingale. Since we have established that $(Z_t)_{t\\in[0,T]}$ is a true martingale and $\\tau$ is an $(\\mathcal{F}_t)$-stopping time, we conclude that $(Z_{t\\wedge \\tau})_{t\\in[0,T]}$ is a martingale.\n\nFinally, we apply the Optional Stopping Theorem to evaluate $\\mathbb{E}[Z_{\\tau}]$. The theorem states that if $(M_t)$ is a martingale and $\\tau$ is a stopping time, then under certain conditions, $\\mathbb{E}[M_\\tau] = \\mathbb{E}[M_0]$. One of the sufficient conditions is that the stopping time $\\tau$ is bounded. The problem specifies that $\\tau$ is an $(\\mathcal{F}_t)$-stopping time satisfying $0 \\leq \\tau \\leq T$ almost surely. This means that $\\tau$ is a bounded stopping time.\nApplying the Optional Stopping Theorem to the martingale $(Z_t)_{t\\in[0,T]}$ and the bounded stopping time $\\tau$, we have:\n$$\n\\mathbb{E}[Z_\\tau] = \\mathbb{E}[Z_0].\n$$\nAs calculated before, $Z_0=1$. Therefore,\n$$\n\\mathbb{E}[Z_\\tau] = 1.\n$$\nThis could also be justified by noting that any continuous martingale on a finite closed interval $[0,T]$ is uniformly integrable. The Optional Stopping Theorem holds for uniformly integrable martingales and any stopping time $\\sigma$. Thus, for our stopping time $\\tau \\le T$, we have $\\mathbb{E}[Z_\\tau]=\\mathbb{E}[Z_0]=1$. The bounded stopping time argument is more direct.", "answer": "$$\\boxed{1}$$", "id": "3000259"}, {"introduction": "To build robust intuition for the continuous-time Girsanov formula, it is invaluable to first examine its discrete counterpart. This practice explores a finite-dimensional version of the Cameron-Martin theorem, where a shift is applied to a vector of Wiener process observations. By calculating the ratio of probability densities, you will derive an expression that is the direct ancestor of the Girsanov kernel, making the abstract formula much more concrete [@problem_id:3000310].", "problem": "Let $(\\Omega,\\mathcal{F},(\\mathcal{F}_t)_{t\\ge 0},\\mathbb{P})$ support a standard one-dimensional Wiener process $W=(W_t)_{t\\ge 0}$. Fix deterministic times $0=t_0<t_1<\\dots<t_n$ with $n\\in\\mathbb{N}$. Let $h:[0,t_n]\\to\\mathbb{R}$ be absolutely continuous with $h(0)=0$ and square-integrable derivative $h'\\in L^2([0,t_n])$, so that $h$ belongs to the Cameron–Martin space. Consider the finite-dimensional projections $X=(W_{t_1},\\dots,W_{t_n})$ and $Y=(W_{t_1}+h(t_1),\\dots,W_{t_n}+h(t_n))$.\n\nUsing only the fundamental facts that finite-dimensional distributions of a standard Wiener process are multivariate normal with mean $0$ and covariance $\\Sigma$ given by $\\Sigma_{ij}=\\min\\{t_i,t_j\\}$, and that shifting a multivariate normal vector by a deterministic mean vector changes only its mean while keeping the same covariance, do the following:\n\n1. Determine the distribution of $Y$ in terms of its mean vector and covariance matrix.\n2. Compute the Radon–Nikodym density ratio (i.e., the ratio of Lebesgue densities) of the law of $Y$ with respect to the law of $X$ at a generic point $x=(x_1,\\dots,x_n)\\in\\mathbb{R}^n$. Express your answer in closed form in terms of the time increments $\\Delta_k:=t_k-t_{k-1}$ and the discrete increments of $h$ given by $h(t_k)-h(t_{k-1})$. Your final answer should be a single analytic expression for this density ratio as a function of $x_1,\\dots,x_n$.\n\nNo numerical approximation is required; provide an exact symbolic expression. The final boxed answer should be the closed-form expression for the density ratio only.", "solution": "The solution proceeds in two parts as requested by the problem.\n\n**Part 1: Distribution of Y**\n\nLet $X = (W_{t_1}, \\dots, W_{t_n})$. According to the problem statement, $X$ follows a multivariate normal distribution. The mean vector is $\\mathbb{E}[X] = (\\mathbb{E}[W_{t_1}], \\dots, \\mathbb{E}[W_{t_n}]) = (0, \\dots, 0) = \\mathbf{0}$. The covariance matrix $\\Sigma$ has elements $\\Sigma_{ij} = \\text{Cov}(W_{t_i}, W_{t_j}) = \\min\\{t_i, t_j\\}$. Thus, the law of $X$ is $\\mathcal{N}(\\mathbf{0}, \\Sigma)$.\n\nThe vector $Y$ is defined as $Y = (W_{t_1}+h(t_1), \\dots, W_{t_n}+h(t_n))$. This can be written as $Y = X + \\mathbf{h}$, where $\\mathbf{h} = (h(t_1), \\dots, h(t_n))$ is a deterministic vector.\n\nUsing the provided premise that shifting a multivariate normal vector by a deterministic vector only shifts its mean, we can determine the distribution of $Y$.\nThe mean of $Y$ is $\\mathbb{E}[Y] = \\mathbb{E}[X + \\mathbf{h}] = \\mathbb{E}[X] + \\mathbf{h} = \\mathbf{0} + \\mathbf{h} = \\mathbf{h}$.\nThe covariance matrix of $Y$ is $\\text{Cov}(Y) = \\text{Cov}(X + \\mathbf{h}) = \\text{Cov}(X) = \\Sigma$.\n\nTherefore, the distribution of $Y$ is multivariate normal with mean vector $\\mathbf{h}$ and covariance matrix $\\Sigma$. We write this as $Y \\sim \\mathcal{N}(\\mathbf{h}, \\Sigma)$.\n\n**Part 2: Radon–Nikodym Density Ratio**\n\nThe Radon–Nikodym derivative of the law of $Y$ with respect to the law of $X$, evaluated at a point $x \\in \\mathbb{R}^n$, is the ratio of their probability density functions (PDFs), $\\frac{f_Y(x)}{f_X(x)}$.\n\nThe PDF of a general $n$-dimensional normal vector $Z \\sim \\mathcal{N}(\\mu, C)$ is given by\n$$f_Z(z) = \\frac{1}{\\sqrt{(2\\pi)^n \\det(C)}} \\exp\\left(-\\frac{1}{2}(z-\\mu)^T C^{-1} (z-\\mu)\\right)$$\nFor $X \\sim \\mathcal{N}(\\mathbf{0}, \\Sigma)$, the PDF is\n$$f_X(x) = \\frac{1}{\\sqrt{(2\\pi)^n \\det(\\Sigma)}} \\exp\\left(-\\frac{1}{2}x^T \\Sigma^{-1} x\\right)$$\nFor $Y \\sim \\mathcal{N}(\\mathbf{h}, \\Sigma)$, the PDF is\n$$f_Y(x) = \\frac{1}{\\sqrt{(2\\pi)^n \\det(\\Sigma)}} \\exp\\left(-\\frac{1}{2}(x-\\mathbf{h})^T \\Sigma^{-1} (x-\\mathbf{h})\\right)$$\nThe ratio is\n$$\\frac{f_Y(x)}{f_X(x)} = \\frac{\\exp\\left(-\\frac{1}{2}(x-\\mathbf{h})^T \\Sigma^{-1} (x-\\mathbf{h})\\right)}{\\exp\\left(-\\frac{1}{2}x^T \\Sigma^{-1} x\\right)} = \\exp\\left(-\\frac{1}{2} \\left[ (x-\\mathbf{h})^T \\Sigma^{-1} (x-\\mathbf{h}) - x^T \\Sigma^{-1} x \\right]\\right)$$\nExpanding the quadratic form in the exponent gives\n$$(x-\\mathbf{h})^T \\Sigma^{-1} (x-\\mathbf{h}) = x^T \\Sigma^{-1} x - 2x^T \\Sigma^{-1} \\mathbf{h} + \\mathbf{h}^T \\Sigma^{-1} \\mathbf{h}$$\nwhere we used the symmetry of $\\Sigma^{-1}$.\nSubstituting this back into the exponent of the ratio, we get\n$$ \\text{exponent} = -\\frac{1}{2} [ (x^T \\Sigma^{-1} x - 2x^T \\Sigma^{-1} \\mathbf{h} + \\mathbf{h}^T \\Sigma^{-1} \\mathbf{h}) - x^T \\Sigma^{-1} x ] = x^T \\Sigma^{-1} \\mathbf{h} - \\frac{1}{2}\\mathbf{h}^T \\Sigma^{-1} \\mathbf{h} $$\nThe density ratio is $\\exp\\left(x^T \\Sigma^{-1} \\mathbf{h} - \\frac{1}{2}\\mathbf{h}^T \\Sigma^{-1} \\mathbf{h}\\right)$.\n\nTo express this in the required form, we must analyze $\\Sigma^{-1}$. Let's consider the vector of Wiener process increments $Z = (Z_1, \\dots, Z_n)$, where $Z_k = W_{t_k} - W_{t_{k-1}}$ for $k=1,\\dots,n$, and $t_0=0$. These increments are independent, and $Z_k \\sim \\mathcal{N}(0, t_k - t_{k-1})$. Let $\\Delta_k = t_k - t_{k-1}$. The covariance matrix of $Z$ is a diagonal matrix $D$ with $D_{kk} = \\Delta_k$.\n\nThe components of $X$ can be expressed as cumulative sums of the components of $Z$: $W_{t_k} = \\sum_{j=1}^k Z_j$. This is a linear transformation $X=AZ$, where $A$ is the $n \\times n$ lower-triangular matrix of ones:\n$$ A = \\begin{pmatrix} 1 & 0 & \\dots & 0 \\\\ 1 & 1 & \\dots & 0 \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ 1 & 1 & \\dots & 1 \\end{pmatrix} $$\nThe covariance matrix of $X$ is $\\Sigma = \\text{Cov}(AZ) = A\\,\\text{Cov}(Z)\\,A^T = ADA^T$.\nThe inverse is $\\Sigma^{-1} = (A^T)^{-1} D^{-1} A^{-1}$. The matrix $A^{-1}$ represents the inverse operation of cumulative sum, which is taking differences:\n$$ A^{-1} = \\begin{pmatrix} 1 & 0 & 0 & \\dots & 0 \\\\ -1 & 1 & 0 & \\dots & 0 \\\\ 0 & -1 & 1 & \\dots & 0 \\\\ \\vdots & \\ddots & \\ddots & \\ddots & \\vdots \\\\ 0 & \\dots & 0 & -1 & 1 \\end{pmatrix} $$\nLet's apply this structure to the terms in the exponent. The first term is $x^T \\Sigma^{-1} \\mathbf{h} = x^T (A^T)^{-1} D^{-1} A^{-1} \\mathbf{h}$. This can be rewritten as $(A^{-1}x)^T D^{-1} (A^{-1}\\mathbf{h})$.\n\nLet $\\delta x = A^{-1}x$ and $\\delta h = A^{-1}\\mathbf{h}$. The components of these vectors are the discrete increments:\nLetting $x_0=0$, $(\\delta x)_k = x_k - x_{k-1}$ for $k=1,\\dots,n$.\nLetting $h(t_0)=h(0)=0$, $(\\delta h)_k = h(t_k) - h(t_{k-1})$ for $k=1,\\dots,n$.\nThe matrix $D^{-1}$ is diagonal with entries $(D^{-1})_{kk} = 1/\\Delta_k$.\nSo, the term $x^T \\Sigma^{-1} \\mathbf{h}$ becomes\n$$ (\\delta x)^T D^{-1} (\\delta h) = \\sum_{k=1}^n (\\delta x)_k \\frac{1}{\\Delta_k} (\\delta h)_k = \\sum_{k=1}^n \\frac{(x_k-x_{k-1})(h(t_k)-h(t_{k-1}))}{\\Delta_k} $$\nSimilarly, the term $\\mathbf{h}^T \\Sigma^{-1} \\mathbf{h}$ becomes\n$$ (\\delta h)^T D^{-1} (\\delta h) = \\sum_{k=1}^n (\\delta h)_k \\frac{1}{\\Delta_k} (\\delta h)_k = \\sum_{k=1}^n \\frac{(h(t_k)-h(t_{k-1}))^2}{\\Delta_k} $$\nCombining these results, the density ratio is given by the exponential of\n$$ \\sum_{k=1}^n \\frac{(x_k-x_{k-1})(h(t_k)-h(t_{k-1}))}{t_k-t_{k-1}} - \\frac{1}{2} \\sum_{k=1}^n \\frac{(h(t_k)-h(t_{k-1}))^2}{t_k-t_{k-1}} $$\nwhere we have set $x_0=0$ and used the given condition $h(t_0)=h(0)=0$. This expression is a function of $x=(x_1,\\dots,x_n)$ and is expressed in terms of the specified time and function increments.", "answer": "$$\\boxed{\\exp\\left(\\sum_{k=1}^{n} \\frac{(x_k - x_{k-1})(h(t_k) - h(t_{k-1}))}{t_k - t_{k-1}} - \\frac{1}{2} \\sum_{k=1}^{n} \\frac{(h(t_k) - h(t_{k-1}))^{2}}{t_k - t_{k-1}}\\right)}$$", "id": "3000310"}, {"introduction": "One of the most powerful applications of the Girsanov theorem is its ability to transform a problem under one measure into a simpler, solvable problem under another. This exercise showcases this technique in the context of hitting time distributions, a cornerstone of quantitative finance and physics. You will use a carefully chosen change of measure to derive the probability density for the first hitting time of a Brownian motion with drift, transforming it into a well-known result for standard Brownian motion [@problem_id:3000331].", "problem": "Let $W = \\{W_{t}\\}_{t \\geq 0}$ be a standard one-dimensional Brownian motion on a filtered probability space $(\\Omega, \\mathcal{F}, \\{\\mathcal{F}_{t}\\}_{t \\geq 0}, \\mathbb{P})$ satisfying the usual conditions. Fix a level $a > 0$ and a drift parameter $\\mu \\in \\mathbb{R}$. Define the drifted process $X_{t} = \\mu t + W_{t}$ and its first hitting time of the level $a$ by\n$$\n\\tau_{a} = \\inf\\{t > 0 : X_{t} = a\\}.\n$$\nUsing the stopped exponential martingale and the Cameron-Martin-Girsanov formula (with the Radon-Nikodym derivative stopped at $\\tau_{a}$), derive the explicit Lebesgue density $f_{\\mu,a}(t)$ of $\\tau_{a}$ on $(0,\\infty)$, expressed as a closed-form analytic function of $t$, $a$, and $\\mu$. Your final answer must be a single analytic expression. No rounding is required, and no units are involved.", "solution": "Our objective is to derive the probability density function, $f_{\\mu,a}(t)$, of the first hitting time $\\tau_a = \\inf\\{t > 0 : X_{t} = a\\}$, where $X_{t} = \\mu t + W_{t}$ is a Brownian motion with drift $\\mu$ under a probability measure $\\mathbb{P}$. The process $W = \\{W_t\\}_{t \\ge 0}$ is a standard one-dimensional Brownian motion under $\\mathbb{P}$.\n\nThe core strategy is to employ the Cameron-Martin-Girsanov (CMG) theorem to change the probability measure from $\\mathbb{P}$ to a new measure $\\mathbb{Q}$ under which the drifted process $X_t$ behaves as a standard (driftless) Brownian motion. The density of the first hitting time for a standard Brownian motion is a known, classic result. We can then use the Radon-Nikodym derivative that connects $\\mathbb{P}$ and $\\mathbb{Q}$ to transform this known density back to the original measure $\\mathbb{P}$, thereby yielding the desired density $f_{\\mu,a}(t)$.\n\nStep 1: Application of the Cameron-Martin-Girsanov Theorem\n\nUnder the measure $\\mathbb{P}$, the dynamics of $X_t$ are given by $dX_t = \\mu dt + dW_t$. We wish to find a measure $\\mathbb{Q}$ such that $X_t$ is a standard Brownian motion under $\\mathbb{Q}$, meaning its dynamics under $\\mathbb{Q}$ should be $dX_t = dB_t$, where $B_t$ is a $\\mathbb{Q}$-Brownian motion.\n\nAccording to the CMG theorem, if we define a new measure $\\mathbb{Q}$ via a Radon-Nikodym derivative process $Z_t = \\frac{d\\mathbb{Q}}{d\\mathbb{P}}\\big|_{\\mathcal{F}_t}$, where $Z_t$ is a $\\mathbb{P}$-martingale of the form\n$$\nZ_t = \\exp\\left( \\int_0^t \\theta_s dW_s - \\frac{1}{2} \\int_0^t \\theta_s^2 ds \\right)\n$$\nthen the process $\\tilde{W}_t = W_t - \\int_0^t \\theta_s ds$ is a standard Brownian motion under $\\mathbb{Q}$.\n\nWe need to choose the process $\\theta_s$ such that $X_t$ becomes a $\\mathbb{Q}$-Brownian motion. We can rewrite the dynamics of $X_t$ as\n$$\ndX_t = (\\mu - \\theta_t)dt + (dW_t + \\theta_t dt)\n$$\nTo make $X_t$ a martingale under $\\mathbb{Q}$, we must eliminate its drift term. The process $d\\tilde{W}_t = dW_t + \\theta_t dt$ is the increment of a $\\mathbb{Q}$-Brownian motion. We choose $\\theta_t$ to remove the original drift $\\mu$. Let's choose $\\theta_t = -\\mu$. This choice is constant. The process $X_t$ can be written as $X_t = W_t + \\mu t$.\nWith $\\theta_s = -\\mu$, the Girsanov theorem states that the process $B_t = W_t - \\int_0^t (-\\mu) ds = W_t + \\mu t$ is a standard Brownian motion under the measure $\\mathbb{Q}$.\nWe see that $B_t$ is precisely our process $X_t$. Thus, under the measure $\\mathbb{Q}$ defined by the Radon-Nikodym process corresponding to $\\theta_t = -\\mu$, $X_t$ is a standard Brownian motion.\n\nThe Radon-Nikodym process $Z_t$ (which is an exponential martingale) is:\n$$\nZ_t = \\exp\\left( \\int_0^t (-\\mu) dW_s - \\frac{1}{2}\\int_0^t (-\\mu)^2 ds \\right) = \\exp\\left( -\\mu W_t - \\frac{1}{2}\\mu^2 t \\right)\n$$\n\nStep 2: Hitting Time Density under the New Measure $\\mathbb{Q}$\n\nUnder $\\mathbb{Q}$, $X_t$ is a standard Brownian motion. The first hitting time of a level $a > 0$ for a standard Brownian motion has a well-known probability density function, which is the density of the Lévy distribution. Let this density be $g_a(t)$. For $t \\in (0, \\infty)$, it is given by:\n$$\ng_a(t) = \\frac{a}{\\sqrt{2\\pi t^3}} \\exp\\left(-\\frac{a^2}{2t}\\right)\n$$\nTherefore, under $\\mathbb{Q}$, the density of our hitting time $\\tau_a$ is $g_a(t)$. Symbolically, $\\mathbb{Q}(\\tau_a \\in dt) = g_a(t)dt$.\n\nStep 3: Transforming the Density from $\\mathbb{Q}$ back to $\\mathbb{P}$\n\nThe relationship between the measures is $d\\mathbb{Q} = Z_t d\\mathbb{P}$ on $\\mathcal{F}_t$, or equivalently, $d\\mathbb{P} = Z_t^{-1} d\\mathbb{Q}$.\nWe are interested in the probability of the event $\\{\\tau_a \\in [t, t+dt)\\}$, which is an $\\mathcal{F}_{\\tau_a}$-measurable event. The general change-of-measure formula for expectations of random variables measurable with respect to a stopping time $\\tau$ states that for an $\\mathcal{F}_\\tau$-measurable random variable $Y$, $\\mathbb{E}_{\\mathbb{P}}[Y] = \\mathbb{E}_{\\mathbb{Q}}[Y Z_\\tau^{-1}]$.\n\nLet $h$ be a bounded, measurable test function. The expectation of $h(\\tau_a)$ under $\\mathbb{P}$ is, by definition:\n$$\n\\mathbb{E}_{\\mathbb{P}}[h(\\tau_a)] = \\int_0^\\infty h(t) f_{\\mu,a}(t) dt\n$$\nUsing the change of measure formula:\n$$\n\\mathbb{E}_{\\mathbb{P}}[h(\\tau_a)] = \\mathbb{E}_{\\mathbb{Q}}[h(\\tau_a) Z_{\\tau_a}^{-1}]\n$$\nFirst, we find an explicit expression for $Z_{\\tau_a}^{-1}$. The process $Z_t^{-1}$ is\n$$\nZ_t^{-1} = \\exp\\left( \\mu W_t + \\frac{1}{2}\\mu^2 t \\right)\n$$\nAt the stopping time $\\tau_a$, we know by definition that $X_{\\tau_a} = a$. We also have $X_{\\tau_a} = \\mu \\tau_a + W_{\\tau_a}$, which implies $W_{\\tau_a} = a - \\mu \\tau_a$.\nSubstituting this into the expression for $Z_t^{-1}$ at $t=\\tau_a$:\n$$\nZ_{\\tau_a}^{-1} = \\exp\\left( \\mu(a - \\mu \\tau_a) + \\frac{1}{2}\\mu^2 \\tau_a \\right) = \\exp\\left( \\mu a - \\mu^2 \\tau_a + \\frac{1}{2}\\mu^2 \\tau_a \\right) = \\exp\\left( \\mu a - \\frac{1}{2}\\mu^2 \\tau_a \\right)\n$$\nNow, we substitute this back into the expectation formula under $\\mathbb{Q}$:\n$$\n\\mathbb{E}_{\\mathbb{P}}[h(\\tau_a)] = \\mathbb{E}_{\\mathbb{Q}}\\left[h(\\tau_a) \\exp\\left(\\mu a - \\frac{1}{2}\\mu^2 \\tau_a\\right)\\right]\n$$\nBy definition of expectation with respect to a density, the right-hand side is:\n$$\n\\int_0^\\infty h(t) \\exp\\left(\\mu a - \\frac{1}{2}\\mu^2 t\\right) g_a(t) dt\n$$\nEquating the two integral expressions for $\\mathbb{E}_{\\mathbb{P}}[h(\\tau_a)]$:\n$$\n\\int_0^\\infty h(t) f_{\\mu,a}(t) dt = \\int_0^\\infty h(t) \\left[ g_a(t) \\exp\\left(\\mu a - \\frac{1}{2}\\mu^2 t\\right) \\right] dt\n$$\nSince this equality must hold for any bounded measurable function $h(t)$, the integrands must be equal almost everywhere. Thus, we identify the density $f_{\\mu,a}(t)$:\n$$\nf_{\\mu,a}(t) = g_a(t) \\exp\\left(\\mu a - \\frac{1}{2}\\mu^2 t\\right)\n$$\n\nStep 4: Final Expression for the Density\n\nSubstituting the expression for $g_a(t)$:\n$$\nf_{\\mu,a}(t) = \\frac{a}{\\sqrt{2\\pi t^3}} \\exp\\left(-\\frac{a^2}{2t}\\right) \\exp\\left(\\mu a - \\frac{1}{2}\\mu^2 t\\right)\n$$\nWe can combine the arguments of the exponential functions:\n$$\nf_{\\mu,a}(t) = \\frac{a}{\\sqrt{2\\pi t^3}} \\exp\\left(-\\frac{a^2}{2t} + \\mu a - \\frac{\\mu^2 t}{2}\\right)\n$$\nTo simplify the exponent, we bring all terms over a common denominator of $2t$:\n$$\n-\\frac{a^2}{2t} + \\frac{2\\mu a t}{2t} - \\frac{\\mu^2 t^2}{2t} = -\\frac{a^2 - 2\\mu a t + \\mu^2 t^2}{2t} = -\\frac{(a - \\mu t)^2}{2t}\n$$\nTherefore, the final expression for the Lebesgue density of $\\tau_a$ on $(0, \\infty)$ is:\n$$\nf_{\\mu,a}(t) = \\frac{a}{\\sqrt{2\\pi t^3}} \\exp\\left(-\\frac{(a-\\mu t)^2}{2t}\\right)\n$$\nThis is the probability density function for the inverse Gaussian distribution with parameters $\\lambda = a^2$ and $\\nu = a/\\mu > 0$. The derivation holds for any $\\mu \\in \\mathbb{R}$.", "answer": "$$\\boxed{\\frac{a}{\\sqrt{2\\pi t^{3}}} \\exp\\left(-\\frac{(a-\\mu t)^{2}}{2t}\\right)}$$", "id": "3000331"}]}