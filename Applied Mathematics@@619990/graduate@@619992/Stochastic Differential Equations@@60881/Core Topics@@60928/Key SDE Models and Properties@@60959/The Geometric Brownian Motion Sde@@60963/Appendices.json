{"hands_on_practices": [{"introduction": "A fundamental technique in stochastic calculus is to analyze how a process transforms under a given function. This exercise challenges you to apply Itô's formula to a power transformation, $S_t^p$, of a Geometric Brownian Motion process. Beyond the calculus, this practice leads to a profound question in financial mathematics: for which powers does the transformed process behave like a 'fair game,' i.e., when does it become a local martingale? [@problem_id:3001451]", "problem": "Let $\\{W_{t}\\}_{t \\geq 0}$ be a standard Brownian motion and consider a Geometric Brownian Motion (GBM) $S_{t}$ defined as the strong solution to the Stochastic Differential Equation (SDE)\n$$\ndS_{t} = \\mu S_{t}\\, dt + \\sigma S_{t}\\, dW_{t}, \\quad S_{0} > 0,\n$$\nwhere $\\mu \\in \\mathbb{R}$ and $\\sigma > 0$ are constants. For a fixed $p \\in \\mathbb{R}$, define the process $X_{t} := S_{t}^{p}$.\n\nStarting from the SDE for $S_{t}$ and using Itô's formula, derive the drift coefficient of $X_{t}$ in the semimartingale decomposition $dX_{t} = \\text{(drift)}\\, dt + \\text{(diffusion)}\\, dW_{t}$. Then, determine the value(s) of $p$ for which $X_{t}$ is a local martingale.\n\nExpress your final answer as a closed-form analytic expression for the drift coefficient as a function of $\\mu$, $\\sigma$, $p$, and $S_{t}$, together with the set of all $p$ values that make $X_{t}$ a local martingale. No rounding is required, and no units are involved.", "solution": "The problem statement has been validated and is deemed valid. It is a well-posed, scientifically grounded problem within the field of stochastic differential equations.\n\nThe problem asks for two results: the drift coefficient of the process $X_{t} := S_{t}^{p}$ and the value(s) of the constant $p$ for which $X_{t}$ is a local martingale. We will address these in sequence using Itô's formula.\n\nThe process $S_{t}$ follows a Geometric Brownian Motion (GBM) described by the stochastic differential equation (SDE):\n$$\ndS_{t} = \\mu S_{t}\\, dt + \\sigma S_{t}\\, dW_{t}\n$$\nwhere $\\{W_{t}\\}_{t \\geq 0}$ is a standard Brownian motion, $S_{0} > 0$, $\\mu \\in \\mathbb{R}$, and $\\sigma > 0$.\n\nWe are interested in the process $X_{t} = S_{t}^{p}$. This can be expressed as a function of $S_{t}$, namely $X_{t} = f(S_{t})$ where $f(s) = s^{p}$. To find the SDE for $X_{t}$, we apply Itô's formula for a function of an Itô process. For a process $Y_{t} = f(Z_{t})$ where $dZ_{t} = a_{t} dt + b_{t} dW_{t}$, Itô's formula states:\n$$\ndY_{t} = f'(Z_{t}) dZ_{t} + \\frac{1}{2} f''(Z_{t}) (dZ_{t})^{2}\n$$\nIn our case, $Z_{t} = S_{t}$, so $a_{t} = \\mu S_{t}$ and $b_{t} = \\sigma S_{t}$.\nFirst, we compute the necessary derivatives of $f(s) = s^{p}$ with respect to $s$:\n$$\nf'(s) = \\frac{df}{ds} = p s^{p-1}\n$$\n$$\nf''(s) = \\frac{d^{2}f}{ds^{2}} = p(p-1) s^{p-2}\n$$\nNext, we compute the quadratic variation term $(dS_{t})^{2}$. Using the Itô multiplication rules ($dt \\cdot dt = 0$, $dt \\cdot dW_{t} = 0$, $dW_{t} \\cdot dW_{t} = dt$):\n$$\n(dS_{t})^{2} = (\\mu S_{t}\\, dt + \\sigma S_{t}\\, dW_{t})^{2} = (\\mu S_{t})^{2} (dt)^{2} + 2 \\mu \\sigma S_{t}^{2} dt\\,dW_{t} + (\\sigma S_{t})^{2} (dW_{t})^{2} = \\sigma^{2} S_{t}^{2}\\, dt\n$$\nNow, we substitute these components into Itô's formula for $dX_{t} = df(S_{t})$:\n$$\ndX_{t} = f'(S_{t}) dS_{t} + \\frac{1}{2} f''(S_{t}) (dS_{t})^{2}\n$$\n$$\ndX_{t} = (p S_{t}^{p-1}) (\\mu S_{t}\\, dt + \\sigma S_{t}\\, dW_{t}) + \\frac{1}{2} (p(p-1) S_{t}^{p-2}) (\\sigma^{2} S_{t}^{2}\\, dt)\n$$\nWe distribute the terms and collect the $dt$ (drift) and $dW_{t}$ (diffusion) components.\n$$\ndX_{t} = (p S_{t}^{p-1})(\\mu S_{t})\\, dt + (p S_{t}^{p-1})(\\sigma S_{t})\\, dW_{t} + \\frac{1}{2} p(p-1) \\sigma^{2} S_{t}^{p-2} S_{t}^{2}\\, dt\n$$\n$$\ndX_{t} = p \\mu S_{t}^{p}\\, dt + p \\sigma S_{t}^{p}\\, dW_{t} + \\frac{1}{2} p(p-1) \\sigma^{2} S_{t}^{p}\\, dt\n$$\nCombining the terms with $dt$:\n$$\ndX_{t} = \\left( p \\mu + \\frac{1}{2} p(p-1) \\sigma^{2} \\right) S_{t}^{p}\\, dt + p \\sigma S_{t}^{p}\\, dW_{t}\n$$\nThis is the semimartingale decomposition of $X_{t}$. The drift coefficient is the term multiplying $dt$.\n\nThe drift coefficient of $X_{t}$ is therefore:\n$$\n\\left( p \\mu + \\frac{1}{2} p(p-1) \\sigma^{2} \\right) S_{t}^{p}\n$$\nThis completes the first part of the problem.\n\nFor the second part, we must determine the value(s) of $p$ for which the process $X_{t}$ is a local martingale. An Itô process is a local martingale if and only if its drift coefficient is zero. Thus, we must have:\n$$\n\\left( p \\mu + \\frac{1}{2} p(p-1) \\sigma^{2} \\right) S_{t}^{p} = 0\n$$\nThe solution to the GBM SDE is $S_{t} = S_{0} \\exp\\left((\\mu - \\frac{1}{2}\\sigma^{2})t + \\sigma W_{t}\\right)$. Since the initial condition is $S_{0} > 0$ and the exponential function is always positive, it follows that $S_{t} > 0$ for all $t \\geq 0$. Consequently, $S_{t}^{p}$ is non-zero for any finite $p \\in \\mathbb{R}$. Therefore, for the drift to be zero, the term in the parenthesis must be zero:\n$$\np \\mu + \\frac{1}{2} p(p-1) \\sigma^{2} = 0\n$$\nWe can factor out $p$ from this equation:\n$$\np \\left( \\mu + \\frac{1}{2} (p-1) \\sigma^{2} \\right) = 0\n$$\nThis equation holds true if either $p = 0$ or the term in the parentheses is zero. This gives two possible solutions for $p$.\n\nCase 1: $p=0$.\nIf $p=0$, the condition is satisfied. In this case, $X_{t} = S_{t}^{0} = 1$ for all $t$, which is a constant process. A constant process has zero drift and is a martingale (and hence also a local martingale).\n\nCase 2: $\\mu + \\frac{1}{2} (p-1) \\sigma^{2} = 0$.\nWe solve for $p$. Since it is given that $\\sigma > 0$, we know $\\sigma^{2} > 0$.\n$$\n\\frac{1}{2} (p-1) \\sigma^{2} = -\\mu\n$$\n$$\n(p-1) \\sigma^{2} = -2\\mu\n$$\n$$\np-1 = -\\frac{2\\mu}{\\sigma^{2}}\n$$\n$$\np = 1 - \\frac{2\\mu}{\\sigma^{2}}\n$$\nThis provides the second value for $p$.\n\nIn summary, the process $X_{t} = S_{t}^{p}$ is a local martingale for two specific values of $p$: $p=0$ and $p=1 - \\frac{2\\mu}{\\sigma^{2}}$.\n\nThe final answer comprises the derived drift coefficient and the set of these two values of $p$.", "answer": "$$\n\\boxed{\\begin{pmatrix} \\left( p\\mu + \\frac{1}{2}p(p-1)\\sigma^{2} \\right) S_{t}^{p}  \\left\\{ 0, 1 - \\frac{2\\mu}{\\sigma^{2}} \\right\\} \\end{pmatrix}}\n$$", "id": "3001451"}, {"introduction": "While analytical solutions provide deep insight, most real-world applications of SDEs rely on numerical simulations. This exercise introduces the most fundamental method for discretizing an SDE, the Euler-Maruyama scheme, by applying it to the Geometric Brownian Motion equation. You will analyze its properties, including its rate of convergence, which is a critical concept for assessing the accuracy of any numerical simulation method in stochastic analysis [@problem_id:3001449].", "problem": "Consider the geometric Brownian motion (GBM) stochastic differential equation (SDE)\n$$\ndS_t = \\mu S_t dt + \\sigma S_t dW_t,\\qquad S_0>0,\n$$\nwhere $W_t$ is a standard Brownian motion and $\\mu,\\sigma\\in\\mathbb{R}$ are constants. Let $\\{t_n\\}_{n=0}^N$ be a uniform time grid with $t_n=n\\Delta t$ over a fixed horizon $[0,T]$ with $\\Delta t=T/N$. Starting from the integral form of the SDE on $[t_n,t_{n+1}]$,\n$$\nS_{t_{n+1}}-S_{t_n} = \\int_{t_n}^{t_{n+1}} \\mu S_s ds + \\int_{t_n}^{t_{n+1}} \\sigma S_s dW_s,\n$$\nderive a one–step numerical scheme by freezing the integrands at the left endpoint $t_n$ on each subinterval. Then, using the Itô–Taylor expansion and standard Lipschitz and linear growth conditions, discuss the resulting scheme’s strong convergence order over a fixed finite horizon.\n\nSelect all statements below that are correct in this setting.\n\nA. On a uniform grid, the one–step scheme obtained by freezing the integrands is\n$$\n\\tilde S_{n+1} = \\tilde S_n + \\mu \\tilde S_n \\Delta t + \\sigma \\tilde S_n \\Delta W_n,\n$$\nwhere $\\Delta W_n := W_{t_{n+1}}-W_{t_n}$ are independent and identically distributed Gaussian random variables with $\\Delta W_n\\sim\\mathcal{N}(0,\\Delta t)$ and independent of $\\tilde S_n$.\n\nB. The scheme has strong convergence order $1$ in the mean–square sense, i.e., there exists a constant $C>0$ such that for all $n$ with $t_n\\le T$,\n$$\n\\mathbb{E}\\bigl|S_{t_n}-\\tilde S_n\\bigr|^2 \\le C(\\Delta t)^2.\n$$\n\nC. The scheme has strong convergence order $1/2$, in the sense that there exists a constant $C>0$ such that for all $n$ with $t_n\\le T$,\n$$\n\\Bigl(\\mathbb{E}\\bigl|S_{t_n}-\\tilde S_n\\bigr|^2\\Bigr)^{1/2} \\le C(\\Delta t)^{1/2}.\n$$\n\nD. For any $\\Delta t>0$, the scheme preserves positivity almost surely: if $\\tilde S_0>0$ then $\\tilde S_n\\ge 0$ for all $n$ with probability one.\n\nE. There exists a constant $C>0$ such that for all $n$ with $t_n\\le T$,\n$$\n\\mathbb{E}\\bigl|S_{t_n}-\\tilde S_n\\bigr|^2 \\le C\\Delta t.\n$$\n\nF. If, instead of the frozen–integrand scheme, one applies the Milstein scheme to the same GBM SDE, the strong convergence order improves to $1$.", "solution": "The problem statement is found to be valid. It presents a standard stochastic differential equation, the Geometric Brownian Motion (GBM), and asks for the derivation and analysis of a common numerical scheme. The questions posed are well-defined within the field of numerical analysis of SDEs.\n\nThe given SDE is the Geometric Brownian Motion:\n$$\ndS_t = \\mu S_t dt + \\sigma S_t dW_t, \\quad S_0 > 0\n$$\nThe integral form on the interval $[t_n, t_{n+1}]$ is:\n$$\nS_{t_{n+1}} - S_{t_n} = \\int_{t_n}^{t_{n+1}} \\mu S_s ds + \\int_{t_n}^{t_{n+1}} \\sigma S_s dW_s\n$$\nThe problem instructs to derive a one-step numerical scheme by \"freezing the integrands at the left endpoint $t_n$\". This is an approximation where we assume $S_s \\approx S_{t_n}$ for $s \\in [t_n, t_{n+1}]$. Let $\\tilde{S}_n$ denote the numerical approximation of $S_{t_n}$. The scheme is then defined by:\n$$\n\\tilde{S}_{n+1} - \\tilde{S}_n = \\int_{t_n}^{t_{n+1}} \\mu \\tilde{S}_n ds + \\int_{t_n}^{t_{n+1}} \\sigma \\tilde{S}_n dW_s\n$$\nSince $\\tilde{S}_n$ is known at time $t_n$ (i.e., it is $\\mathcal{F}_{t_n}$-measurable), it can be treated as a constant with respect to the integrals over $[t_n, t_{n+1}]$:\n$$\n\\tilde{S}_{n+1} - \\tilde{S}_n = \\mu \\tilde{S}_n \\int_{t_n}^{t_{n+1}} ds + \\sigma \\tilde{S}_n \\int_{t_n}^{t_{n+1}} dW_s\n$$\nEvaluating the integrals yields:\n$$\n\\int_{t_n}^{t_{n+1}} ds = t_{n+1} - t_n = \\Delta t\n$$\n$$\n\\int_{t_n}^{t_{n+1}} dW_s = W_{t_{n+1}} - W_{t_n} =: \\Delta W_n\n$$\nSubstituting these back gives the one-step scheme:\n$$\n\\tilde{S}_{n+1} - \\tilde{S}_n = \\mu \\tilde{S}_n \\Delta t + \\sigma \\tilde{S}_n \\Delta W_n\n$$\nThis is the well-known Euler-Maruyama scheme applied to the GBM SDE.\n\nNow, we evaluate each statement.\n\n**A. On a uniform grid, the one–step scheme obtained by freezing the integrands is...**\nThe statement proposes the scheme:\n$$\n\\tilde S_{n+1} = \\tilde S_n + \\mu \\tilde S_n \\Delta t + \\sigma \\tilde S_n \\Delta W_n\n$$\nThis is identical to the scheme we derived above. The statement further characterizes $\\Delta W_n := W_{t_{n+1}}-W_{t_n}$. For a standard Brownian motion $W_t$, the increments $\\Delta W_n$ over disjoint intervals are independent. For a single increment over $[t_n, t_{n+1}]$ of length $\\Delta t$, its distribution is Gaussian with mean $0$ and variance $\\Delta t$, i.e., $\\Delta W_n \\sim \\mathcal{N}(0, \\Delta t)$. Finally, the numerical solution $\\tilde{S}_n$ is determined by the history of the Brownian motion up to time $t_n$, which means $\\tilde{S}_n$ is $\\mathcal{F}_{t_n}$-measurable. The increment $\\Delta W_n = W_{t_{n+1}} - W_{t_n}$ is independent of the sigma-algebra $\\mathcal{F}_{t_n}$ by definition of Brownian motion. Therefore, $\\Delta W_n$ is independent of $\\tilde{S}_n$. All parts of the statement are correct.\n**Verdict: Correct.**\n\n**B, C, E. Analysis of Strong Convergence**\nThe Euler-Maruyama scheme is known to have a strong order of convergence of $\\gamma = 1/2$ for SDEs with globally Lipschitz continuous drift and diffusion coefficients. For the GBM SDE, the coefficients $a(S) = \\mu S$ and $b(S) = \\sigma S$ are only locally Lipschitz. However, the convergence results still hold.\n\nThe strong convergence order $\\gamma$ is defined by the rate at which the first absolute moment of the error converges to zero:\n$$\n\\mathbb{E}\\bigl|S_{t_n} - \\tilde{S}_n\\bigr| \\le C (\\Delta t)^{\\gamma}\n$$\nfor some constant $C>0$ as $\\Delta t \\to 0$. For the Euler-Maruyama scheme, $\\gamma=1/2$.\n\nThe mean-square convergence order $\\beta$ is defined by:\n$$\n\\mathbb{E}\\bigl|S_{t_n} - \\tilde{S}_n\\bigr|^2 \\le C (\\Delta t)^{\\beta}\n$$\nBy Jensen's inequality, $(\\mathbb{E}|X|)^2 \\le \\mathbb{E}|X^2|$, which implies a relationship between the orders: $(\\Delta t)^{2\\gamma} \\sim (\\Delta t)^{\\beta}$, so $\\beta \\ge 2\\gamma$. For many schemes, including Euler-Maruyama, the equality $\\beta = 2\\gamma$ holds. Since $\\gamma=1/2$, the mean-square order is $\\beta=1$.\n\nLet's evaluate the options based on this.\n\n**B. The scheme has strong convergence order $1$ in the mean–square sense, i.e., there exists a constant $C>0$ such that ... $\\mathbb{E}\\bigl|S_{t_n}-\\tilde S_n\\bigr|^2 \\le C(\\Delta t)^2$.**\nThe inequality $\\mathbb{E}\\bigl|S_{t_n}-\\tilde S_n\\bigr|^2 \\le C(\\Delta t)^2$ posits a mean-square convergence order of $\\beta=2$. This would correspond to a strong order of $\\gamma=1$. The Euler-Maruyama scheme does not achieve this order; its strong order is $\\gamma=1/2$ and its mean-square order is $\\beta=1$.\n**Verdict: Incorrect.**\n\n**C. The scheme has strong convergence order $1/2$, in the sense that there exists a constant $C>0$ such that for all $n$ with $t_n\\le T$, $\\Bigl(\\mathbb{E}\\bigl|S_{t_n}-\\tilde S_n\\bigr|^2\\Bigr)^{1/2} \\le C(\\Delta t)^{1/2}$.**\nThe inequality is $(\\mathbb{E}|S_{t_n}-\\tilde S_n|^2)^{1/2} \\le C(\\Delta t)^{1/2}$. Squaring both sides yields $\\mathbb{E}|S_{t_n}-\\tilde S_n|^2 \\le C^2(\\Delta t)^1$. This corresponds to a mean-square convergence order of $\\beta=1$. This is the correct mean-square order for the Euler-Maruyama scheme. The phrasing \"strong convergence order $1/2$\" is consistent with this result, as a mean-square order of $1$ implies a strong order of at least $1/2$. In this case, it is exactly $1/2$. The statement correctly links the term \"strong convergence order 1/2\" to the correct mathematical inequality for the root-mean-square error.\n**Verdict: Correct.**\n\n**E. There exists a constant $C>0$ such that for all $n$ with $t_n\\le T$, $\\mathbb{E}\\bigl|S_{t_n}-\\tilde S_n\\bigr|^2 \\le C\\Delta t$.**\nThis statement directly claims that the mean-square error is bounded by a constant times $\\Delta t$. This corresponds to a mean-square convergence order of $\\beta=1$. As established in the analysis for option C, this is the correct convergence rate for the Euler-Maruyama scheme. Options C and E are mathematically equivalent statements about the convergence order.\n**Verdict: Correct.**\n\n**D. For any $\\Delta t>0$, the scheme preserves positivity almost surely: if $\\tilde S_0>0$ then $\\tilde S_n\\ge 0$ for all $n$ with probability one.**\nThe scheme evolves according to $\\tilde{S}_{n+1} = \\tilde{S}_n (1 + \\mu \\Delta t + \\sigma \\Delta W_n)$. Assume $\\tilde{S}_n > 0$. For $\\tilde{S}_{n+1}$ to become negative or zero, the factor $(1 + \\mu \\Delta t + \\sigma \\Delta W_n)$ must be less than or equal to zero.\nThis event occurs if $\\sigma \\Delta W_n \\le -(1 + \\mu \\Delta t)$.\nThe random variable $\\Delta W_n$ follows a Gaussian distribution $\\mathcal{N}(0, \\Delta t)$. The support of a Gaussian distribution is the entire real line $(-\\infty, \\infty)$. Therefore, for any chosen $\\Delta t > 0$ and any finite threshold, there is a non-zero probability that $\\Delta W_n$ will cross it. Specifically, $P(\\sigma \\Delta W_n \\le -(1 + \\mu \\Delta t)) > 0$.\nThus, the scheme can produce negative values, even if the true solution $S_t = S_0 \\exp((\\mu - \\sigma^2/2)t + \\sigma W_t)$ is always positive. The scheme does not preserve positivity.\n**Verdict: Incorrect.**\n\n**F. If, instead of the frozen–integrand scheme, one applies the Milstein scheme to the same GBM SDE, the strong convergence order improves to $1$.**\nFor a general SDE $dX_t = a(X_t) dt + b(X_t) dW_t$, the Milstein scheme is given by:\n$$\n\\tilde{X}_{n+1} = \\tilde{X}_n + a(\\tilde{X}_n)\\Delta t + b(\\tilde{X}_n) \\Delta W_n + \\frac{1}{2} b(\\tilde{X}_n) b'(\\tilde{X}_n) ((\\Delta W_n)^2 - \\Delta t)\n$$\nFor the GBM SDE, we have $a(S) = \\mu S$ and $b(S) = \\sigma S$. The derivative of the diffusion coefficient is $b'(S) = \\sigma$.\nSubstituting these into the Milstein formula:\n$$\n\\tilde{S}_{n+1} = \\tilde{S}_n + \\mu \\tilde{S}_n \\Delta t + \\sigma \\tilde{S}_n \\Delta W_n + \\frac{1}{2} (\\sigma \\tilde{S}_n) (\\sigma) ((\\Delta W_n)^2 - \\Delta t)\n$$\nThe general theory of numerical SDEs states that the Milstein scheme achieves a strong convergence order of $\\gamma=1.0$ (and a mean-square order of $\\beta=2.0$), provided the coefficients satisfy certain smoothness and growth conditions. The coefficients of the GBM SDE fit within a class of SDEs (linear SDEs) for which this higher order is achieved. This represents an improvement from the strong order of $\\gamma=1/2$ of the Euler-Maruyama scheme.\n**Verdict: Correct.**", "answer": "$$\\boxed{ACEF}$$", "id": "3001441"}, {"introduction": "The martingale property, which indicates zero drift, implies a powerful result known as the Optional Stopping Theorem (OST), stating that the expected value of a martingale remains constant even at certain random times. This practice guides you through constructing a non-trivial bounded stopping time and applying the OST to a martingale derived from GBM. Completing this exercise [@problem_id:3001441] will solidify your understanding of the interplay between martingales, stopping times, and expectations.", "problem": "Consider the stochastic differential equation (SDE) for a strictly positive process $S_{t}$ driven by a standard Brownian motion $W_{t}$ on a filtered probability space $(\\Omega,\\mathcal{F},(\\mathcal{F}_{t})_{t \\geq 0},\\mathbb{P})$,\n$$\ndS_{t} = \\mu S_{t} dt + \\sigma S_{t} dW_{t}, \\quad S_{0} > 0,\n$$\nwith constants $\\mu \\in \\mathbb{R}$ and $\\sigma > 0$. Starting from first principles (Itô calculus and properties of Brownian motion), derive the explicit form of $S_{t}$, and use it to justify that the process\n$$\nM_{t} = S_{t}\\exp(-\\mu t)\n$$\nis a martingale. Next, construct a nontrivial bounded stopping time $\\tau$ such that the Optional Stopping Theorem (OST) applies to the martingale $M_{t}$, in a scientifically sound way that does not rely on hitting times. Specifically, perform an initial enlargement of the filtration by adjoining at time $t=0$ an independent random variable $U$ that is uniformly distributed on the interval $[0,T]$ for some fixed $T>0$, and define $\\tau := U$. Prove that $\\tau$ is a bounded stopping time with respect to the enlarged filtration and that $M_{t}$ remains a martingale after this enlargement. Finally, compute the closed-form analytic expression for $\\mathbb{E}[S_{\\tau}]$ in terms of $S_{0}$, $\\mu$, and $T$, assuming $\\mu \\neq 0$. Your final answer must be a single closed-form expression. Do not provide any numerical approximation.", "solution": "The problem asks for the derivation of the explicit solution to the Geometric Brownian Motion (GBM) stochastic differential equation (SDE), a proof that a related process is a martingale, the construction of a specific bounded stopping time, and the calculation of an expected value involving this stopping time. We will address each part in sequence, starting from first principles.\n\nFirst, we derive the explicit solution for the process $S_t$ governed by the SDE:\n$$\ndS_{t} = \\mu S_{t} dt + \\sigma S_{t} dW_{t}, \\quad S_{0} > 0\n$$\nWe consider the process $Y_t = f(S_t)$ where $f(x) = \\ln(x)$. The derivatives of $f(x)$ are $f'(x) = 1/x$ and $f''(x) = -1/x^2$. According to Itô's lemma, the differential for $Y_t$ is given by:\n$$\ndY_t = f'(S_t)dS_t + \\frac{1}{2} f''(S_t) (dS_t)^2\n$$\nThe quadratic variation term $(dS_t)^2$ is calculated using the Itô multiplication rules ($dt^2 = 0$, $dt dW_t = 0$, $dW_t^2 = dt$):\n$$\n(dS_t)^2 = (\\mu S_t dt + \\sigma S_t dW_t)^2 = \\sigma^2 S_t^2 (dW_t)^2 = \\sigma^2 S_t^2 dt\n$$\nSubstituting $dS_t$ and $(dS_t)^2$ into the expression for $dY_t$:\n$$\ndY_t = \\frac{1}{S_t} (\\mu S_t dt + \\sigma S_t dW_t) + \\frac{1}{2} \\left(-\\frac{1}{S_t^2}\\right) (\\sigma^2 S_t^2 dt)\n$$\n$$\ndY_t = \\mu dt + \\sigma dW_t - \\frac{1}{2} \\sigma^2 dt = \\left(\\mu - \\frac{1}{2}\\sigma^2\\right)dt + \\sigma dW_t\n$$\nIntegrating this from $0$ to $t$:\n$$\n\\int_0^t dY_s = \\int_0^t \\left(\\mu - \\frac{1}{2}\\sigma^2\\right)ds + \\int_0^t \\sigma dW_s\n$$\n$$\nY_t - Y_0 = \\left(\\mu - \\frac{1}{2}\\sigma^2\\right)t + \\sigma W_t\n$$\nSubstituting back $Y_t = \\ln(S_t)$:\n$$\n\\ln(S_t) - \\ln(S_0) = \\left(\\mu - \\frac{1}{2}\\sigma^2\\right)t + \\sigma W_t\n$$\nExponentiating both sides yields the explicit solution for $S_t$:\n$$\nS_t = S_0 \\exp\\left( \\left(\\mu - \\frac{1}{2}\\sigma^2\\right)t + \\sigma W_t \\right)\n$$\n\nSecond, we justify that the process $M_t = S_t \\exp(-\\mu t)$ is a martingale with respect to the filtration $(\\mathcal{F}_t)_{t \\ge 0}$. Substituting the expression for $S_t$:\n$$\nM_t = \\left( S_0 \\exp\\left( \\left(\\mu - \\frac{1}{2}\\sigma^2\\right)t + \\sigma W_t \\right) \\right) \\exp(-\\mu t) = S_0 \\exp\\left( \\sigma W_t - \\frac{1}{2}\\sigma^2 t \\right)\n$$\nThis is a stochastic exponential, also known as a Doléans-Dade exponential. To prove it is a martingale, we must show that for any $s  t$, $\\mathbb{E}[M_t | \\mathcal{F}_s] = M_s$.\n$$\n\\mathbb{E}[M_t | \\mathcal{F}_s] = \\mathbb{E}\\left[ S_0 \\exp\\left( \\sigma W_t - \\frac{1}{2}\\sigma^2 t \\right) | \\mathcal{F}_s \\right]\n$$\nWe can rewrite the argument of the exponential as $\\sigma (W_t - W_s) + \\sigma W_s - \\frac{1}{2}\\sigma^2 (t-s) - \\frac{1}{2}\\sigma^2 s$.\n$$\n\\mathbb{E}[M_t | \\mathcal{F}_s] = \\mathbb{E}\\left[ S_0 \\exp\\left( \\sigma W_s - \\frac{1}{2}\\sigma^2 s \\right) \\exp\\left( \\sigma (W_t - W_s) - \\frac{1}{2}\\sigma^2 (t-s) \\right) | \\mathcal{F}_s \\right]\n$$\nThe term $M_s = S_0 \\exp(\\sigma W_s - \\frac{1}{2}\\sigma^2 s)$ is $\\mathcal{F}_s$-measurable, so it can be taken out of the conditional expectation. The increment $W_t - W_s$ is independent of the $\\sigma$-algebra $\\mathcal{F}_s$.\n$$\n\\mathbb{E}[M_t | \\mathcal{F}_s] = M_s \\, \\mathbb{E}\\left[ \\exp\\left( \\sigma (W_t - W_s) - \\frac{1}{2}\\sigma^2 (t-s) \\right) | \\mathcal{F}_s \\right] = M_s \\, \\mathbb{E}\\left[ \\exp\\left( \\sigma (W_t - W_s) - \\frac{1}{2}\\sigma^2 (t-s) \\right) \\right]\n$$\nThe random variable $W_t - W_s$ is normally distributed with mean $0$ and variance $t-s$. Let $Z = W_t-W_s \\sim N(0, t-s)$. The moment-generating function of a normal random variable $X \\sim N(\\nu, \\zeta^2)$ is $\\mathbb{E}[\\exp(kX)] = \\exp(k\\nu + \\frac{1}{2}k^2\\zeta^2)$. For $Z$, with $k=\\sigma$, $\\nu=0$, and $\\zeta^2=t-s$:\n$$\n\\mathbb{E}[\\exp(\\sigma Z)] = \\exp\\left(\\sigma \\cdot 0 + \\frac{1}{2}\\sigma^2 (t-s)\\right) = \\exp\\left(\\frac{1}{2}\\sigma^2(t-s)\\right)\n$$\nThe expectation becomes:\n$$\n\\mathbb{E}\\left[ \\dots \\right] = \\exp\\left( -\\frac{1}{2}\\sigma^2 (t-s) \\right) \\mathbb{E}[\\exp(\\sigma(W_t-W_s))] = \\exp\\left( -\\frac{1}{2}\\sigma^2 (t-s) \\right) \\exp\\left( \\frac{1}{2}\\sigma^2 (t-s) \\right) = 1\n$$\nTherefore, $\\mathbb{E}[M_t | \\mathcal{F}_s] = M_s$, which proves that $M_t$ is a martingale.\n\nThird, we construct the stopping time. A random variable $U$, uniformly distributed on $[0, T]$ and independent of the filtration $(\\mathcal{F}_t)_{t \\geq 0}$, is introduced. The filtration is enlarged to $\\mathcal{G}_t = \\mathcal{F}_t \\vee \\sigma(U)$, where $\\sigma(U)$ is the $\\sigma$-algebra generated by $U$. We define the stopping time $\\tau := U$. To be a $(\\mathcal{G}_t)$-stopping time, the event $\\{\\tau \\le t\\}$ must belong to $\\mathcal{G}_t$ for all $t \\ge 0$. The event $\\{\\tau \\le t\\} = \\{U \\le t\\}$ is in $\\sigma(U)$ by definition. Since $\\sigma(U) \\subset \\mathcal{G}_t$ for all $t \\ge 0$, the condition is satisfied. Furthermore, since $U$ takes values in $[0, T]$, $\\tau$ is a bounded stopping time, $0 \\le \\tau \\le T$.\n\nFourth, we show that $M_t$ remains a martingale with respect to $(\\mathcal{G}_t)_{t \\ge 0}$ and that OST applies. For $s  t$, we evaluate $\\mathbb{E}[M_t | \\mathcal{G}_s] = \\mathbb{E}[M_t | \\mathcal{F}_s \\vee \\sigma(U)]$. Since $M_t$ is $\\mathcal{F}_t$-measurable and the process $W_t$ (which generates $\\mathcal{F}_t$) is independent of $U$ (which generates $\\sigma(U)$), the conditional expectation of $M_t$ given $\\mathcal{F}_s \\vee \\sigma(U)$ is the same as its conditional expectation given $\\mathcal{F}_s$.\n$$\n\\mathbb{E}[M_t | \\mathcal{G}_s] = \\mathbb{E}[M_t | \\mathcal{F}_s] = M_s\n$$\nThus, $M_t$ is also a $(\\mathcal{G}_t)$-martingale. Since $M_t$ is a continuous-path martingale and $\\tau$ is a bounded stopping time, the conditions for the Optional Stopping Theorem are met, which implies $\\mathbb{E}[M_{\\tau}] = \\mathbb{E}[M_0] = S_0$.\n\nFinally, we compute the closed-form expression for $\\mathbb{E}[S_{\\tau}]$. We use the law of total expectation, conditioning on the value of $\\tau$.\n$$\n\\mathbb{E}[S_{\\tau}] = \\mathbb{E}[\\mathbb{E}[S_{\\tau} | \\tau]]\n$$\nThe inner conditional expectation is $\\mathbb{E}[S_{\\tau} | \\tau=u] = \\mathbb{E}[S_u | U=u]$. Since the process $(S_t)_{t\\ge 0}$ is independent of the random variable $U$, this is simply $\\mathbb{E}[S_u]$.\nWe first find the unconditional expectation $\\mathbb{E}[S_t]$ for a fixed time $t$:\n$$\n\\mathbb{E}[S_t] = \\mathbb{E}\\left[S_0 \\exp\\left( \\left(\\mu - \\frac{1}{2}\\sigma^2\\right)t + \\sigma W_t \\right)\\right] = S_0 \\exp\\left( \\left(\\mu - \\frac{1}{2}\\sigma^2\\right)t \\right) \\mathbb{E}\\left[\\exp(\\sigma W_t)\\right]\n$$\nUsing the moment-generating function for $W_t \\sim N(0,t)$ as before, we have $\\mathbb{E}[\\exp(\\sigma W_t)] = \\exp(\\frac{1}{2}\\sigma^2 t)$.\n$$\n\\mathbb{E}[S_t] = S_0 \\exp\\left( \\left(\\mu - \\frac{1}{2}\\sigma^2\\right)t \\right) \\exp\\left(\\frac{1}{2}\\sigma^2 t\\right) = S_0 \\exp(\\mu t)\n$$\nTherefore, $\\mathbb{E}[S_{\\tau} | \\tau] = S_0 \\exp(\\mu \\tau)$. Now we compute the outer expectation, where $\\tau=U$ is uniformly distributed on $[0,T]$. The probability density function is $f_U(u) = 1/T$ for $u \\in [0,T]$.\n$$\n\\mathbb{E}[S_{\\tau}] = \\mathbb{E}[S_0 \\exp(\\mu \\tau)] = S_0 \\mathbb{E}[\\exp(\\mu U)] = S_0 \\int_0^T \\exp(\\mu u) \\frac{1}{T} du\n$$\nSince $\\mu \\neq 0$ is given, we can evaluate the integral:\n$$\n\\int_0^T \\exp(\\mu u) \\frac{1}{T} du = \\frac{1}{T} \\left[\\frac{\\exp(\\mu u)}{\\mu}\\right]_0^T = \\frac{1}{\\mu T} (\\exp(\\mu T) - \\exp(0)) = \\frac{\\exp(\\mu T) - 1}{\\mu T}\n$$\nCombining these results gives the final expression for $\\mathbb{E}[S_{\\tau}]$:\n$$\n\\mathbb{E}[S_{\\tau}] = S_0 \\frac{\\exp(\\mu T) - 1}{\\mu T}\n$$", "answer": "$$\n\\boxed{S_{0} \\frac{\\exp(\\mu T) - 1}{\\mu T}}\n$$", "id": "3001449"}]}