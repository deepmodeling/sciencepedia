{"hands_on_practices": [{"introduction": "Understanding a stochastic process begins with characterizing its fundamental statistical properties. This first practice focuses on the time evolution of the Ornstein-Uhlenbeck process, starting from a deterministic state. By deriving the time-dependent second moment, you will apply core techniques of Itô calculus to see precisely how the process relaxes towards its long-term stationary distribution, a key feature that distinguishes it from simple Brownian motion. [@problem_id:859495]", "problem": "The Ornstein-Uhlenbeck (OU) process is a stochastic process that describes the velocity of a massive Brownian particle under the influence of friction. It is characterized by a tendency to revert to a long-term mean. The process $X_t$ is described by the following stochastic differential equation (SDE):\n$$ dX_t = -\\theta (X_t - \\mu) dt + \\sigma dW_t $$\nwhere $t \\geq 0$ is time, $X_t$ is the state of the process, and $W_t$ is a standard Wiener process. The parameters are all positive constants: $\\theta$ is the rate of mean reversion, $\\mu$ is the long-term mean, and $\\sigma$ is the volatility.\n\nAssume the process starts from a known deterministic initial value $X_0 = x_0$.\n\nDerive the time-dependent second moment of the process, $\\mathbb{E}[X_t^2]$, as a function of time $t$ and the parameters $x_0$, $\\mu$, $\\theta$, and $\\sigma$.", "solution": "1. The OU‐SDE is \n$$dX_t=-\\theta(X_t-\\mu)\\,dt+\\sigma\\,dW_t.$$\nMultiplying by the integrating factor $e^{\\theta t}$ and integrating from 0 to $t$ gives\n$$X_t=e^{-\\theta t}x_0+\\mu\\bigl(1-e^{-\\theta t}\\bigr)+\\sigma\\int_0^t e^{-\\theta(t-s)}\\,dW_s.$$\n2. The mean follows by linearity:\n$$\\mathbb{E}[X_t]=e^{-\\theta t}x_0+\\mu\\bigl(1-e^{-\\theta t}\\bigr).$$\n3. The variance uses Itô isometry:\n$$\\mathrm{Var}[X_t]\n=\\sigma^2\\int_0^t e^{-2\\theta(t-s)}\\,ds\n=\\frac{\\sigma^2}{2\\theta}\\bigl(1-e^{-2\\theta t}\\bigr).$$\n4. Hence the second moment is\n$$\\mathbb{E}[X_t^2]\n=\\mathbb{E}[X_t]^2+\\mathrm{Var}[X_t]\n=\\bigl(e^{-\\theta t}x_0+\\mu(1-e^{-\\theta t})\\bigr)^2\n+\\frac{\\sigma^2}{2\\theta}\\bigl(1-e^{-2\\theta t}\\bigr).$$", "answer": "$$\\boxed{\\bigl(e^{-\\theta t}x_0+\\mu(1-e^{-\\theta t})\\bigr)^2+\\frac{\\sigma^2}{2\\theta}(1-e^{-2\\theta t})}$$", "id": "859495"}, {"introduction": "Beyond its moments, the 'memory' of a stochastic process is a crucial feature that defines its characteristic timescale. This exercise explores the physical meaning of the mean-reversion parameter $\\theta$ by linking it to the process's autocorrelation and its 'persistence time.' By comparing two systems with different reversion speeds, you will develop a powerful intuition for how this parameter shapes the random fluctuations and governs how quickly the system forgets its past states. [@problem_id:1343718]", "problem": "A research lab is studying the thermal fluctuations of a single nanoparticle confined in two different types of optical traps, Trap A and Trap B. The one-dimensional position of the nanoparticle in each trap, denoted by $X_t^{(A)}$ and $X_t^{(B)}$, fluctuates around an equilibrium position $\\mu$. These fluctuations can be modeled by two independent Ornstein-Uhlenbeck (OU) processes.\n\nThe dynamics for the position in each trap are described by the stochastic differential equations:\n$$ dX_t^{(A)} = \\theta_A (\\mu - X_t^{(A)}) dt + \\sigma dW_t^{(A)} $$\n$$ dX_t^{(B)} = \\theta_B (\\mu - X_t^{(B)}) dt + \\sigma dW_t^{(B)} $$\n\nHere, $\\theta_A$ and $\\theta_B$ are the positive 'stiffness' parameters of the traps, representing the rates of mean reversion. The equilibrium position $\\mu$ and the thermal noise intensity (volatility) $\\sigma$ are identical for both traps. $W_t^{(A)}$ and $W_t^{(B)}$ are independent standard Wiener processes.\n\nAssume that both processes have been active long enough to reach their stationary state. To characterize how long the 'memory' of a fluctuation persists, we define a characteristic time, $\\tau$, for each process. This 'persistence time' $\\tau$ is the time lag at which the covariance between the particle's position at time $t$ and its position at time $t+\\tau$ decays to $1/e$ of its value at $\\tau=0$, where $e$ is Euler's number. Let $\\tau_A$ and $\\tau_B$ be the persistence times for Trap A and Trap B, respectively.\n\nTrap B is designed to be significantly more confining than Trap A, such that its stiffness parameter is 25 times larger, i.e., $\\theta_B = 25 \\, \\theta_A$.\n\nCalculate the ratio of the persistence times, $\\frac{\\tau_A}{\\tau_B}$.", "solution": "For the Ornstein-Uhlenbeck process $dX_{t}=\\theta(\\mu-X_{t})dt+\\sigma dW_{t}$, the stationary solution can be written as\n$$\nX_{t}-\\mu=\\sigma\\int_{-\\infty}^{t}\\exp\\!\\big(-\\theta(t-u)\\big)\\,dW_{u}.\n$$\nThe stationary autocovariance at lag $s\\geq 0$ is\n$$\n\\operatorname{Cov}(X_{t},X_{t+s})=\\mathbb{E}\\big[(X_{t}-\\mu)(X_{t+s}-\\mu)\\big]\n=\\sigma^{2}\\int_{-\\infty}^{t}\\exp\\!\\big(-\\theta(t-u)\\big)\\exp\\!\\big(-\\theta(t+s-u)\\big)\\,du.\n$$\nFactor and change variables $v=t-u$ to obtain\n$$\n\\operatorname{Cov}(X_{t},X_{t+s})=\\sigma^{2}\\exp(-\\theta s)\\int_{-\\infty}^{t}\\exp\\!\\big(-2\\theta(t-u)\\big)\\,du\n=\\sigma^{2}\\exp(-\\theta s)\\int_{0}^{\\infty}\\exp(-2\\theta v)\\,dv\n=\\frac{\\sigma^{2}}{2\\theta}\\exp(-\\theta s).\n$$\nThus the normalized autocovariance is\n$$\n\\frac{\\operatorname{Cov}(X_{t},X_{t+s})}{\\operatorname{Cov}(X_{t},X_{t})}=\\exp(-\\theta s).\n$$\nBy definition, the persistence time $\\tau$ satisfies\n$$\n\\frac{\\operatorname{Cov}(X_{t},X_{t+\\tau})}{\\operatorname{Cov}(X_{t},X_{t})}=\\exp(-1),\n$$\nso $\\exp(-\\theta\\tau)=\\exp(-1)$, which implies\n$$\n\\theta\\,\\tau=1\\quad\\Rightarrow\\quad \\tau=\\frac{1}{\\theta}.\n$$\nTherefore, for the two traps,\n$$\n\\tau_{A}=\\frac{1}{\\theta_{A}},\\qquad \\tau_{B}=\\frac{1}{\\theta_{B}}.\n$$\nGiven $\\theta_{B}=25\\,\\theta_{A}$, we have\n$$\n\\frac{\\tau_{A}}{\\tau_{B}}=\\frac{\\theta_{B}}{\\theta_{A}}=25.\n$$", "answer": "$$\\boxed{25}$$", "id": "1343718"}, {"introduction": "A theoretical model's true power is realized when it can be fitted to observational data. This final practice moves from theory to statistical inference by tasking you with deriving the maximum likelihood estimators for the parameters of the Ornstein-Uhlenbeck process. By working through this problem, you will see how the exact solution of the SDE provides a basis for parameter estimation from discrete data, a fundamental skill for applying stochastic models in any scientific or engineering field. [@problem_id:1343701]", "problem": "A stochastic process $X_t$ is used to model a physical quantity that exhibits mean-reverting behavior. The evolution of the process is described by the Ornstein-Uhlenbeck stochastic differential equation:\n$$dX_t = \\theta (\\mu - X_t) dt + \\sigma dW_t$$\nwhere $\\theta > 0$ is the rate of reversion to the mean, $\\mu$ is the long-term mean, $\\sigma > 0$ is the volatility, and $W_t$ is a standard Wiener process.\n\nThe process is observed at $n+1$ discrete, equally spaced time points $t_i = i\\Delta t$ for $i = 0, 1, \\dots, n$. This yields a sequence of observations $\\{x_0, x_1, \\dots, x_n\\}$. It is known that for $s  t$, the conditional distribution of $X_t$ given $X_s=x_s$ is a normal distribution with mean and variance given by:\n$$\n\\begin{align*}\nE[X_t | X_s=x_s] = x_s e^{-\\theta(t-s)} + \\mu(1 - e^{-\\theta(t-s)}) \\\\\n\\text{Var}(X_t | X_s=x_s) = \\frac{\\sigma^2}{2\\theta}(1 - e^{-2\\theta(t-s)})\n\\end{align*}\n$$\nYour task is to derive estimators for the parameters $\\theta$, $\\mu$, and $\\sigma$, which we denote by $\\hat{\\theta}$, $\\hat{\\mu}$, and $\\hat{\\sigma}$. These estimators should be the parameter values that maximize the joint probability density function of observing the sequence $x_1, \\dots, x_n$ given the initial state $x_0$.\n\nFor notational convenience in your final answer, you may use the following summary statistics of the data $\\{x_i\\}_{i=0}^n$:\n$$S_0 = \\sum_{i=1}^n x_{i-1}, \\quad S_1 = \\sum_{i=1}^n x_i, \\quad S_{00} = \\sum_{i=1}^n x_{i-1}^2, \\quad S_{11} = \\sum_{i=1}^n x_i^2, \\quad S_{01} = \\sum_{i=1}^n x_i x_{i-1}$$\nExpress the estimators $\\hat{\\theta}$, $\\hat{\\mu}$, and $\\hat{\\sigma}$ as analytic expressions in terms of $n$, $\\Delta t$, and these summary statistics. The final answer should be presented as a set of three expressions for $(\\hat{\\theta}, \\hat{\\mu}, \\hat{\\sigma})$.", "solution": "We start with the Ornstein-Uhlenbeck transition over one step of length $\\Delta t$. For $i=1,\\dots,n$, the conditional distribution of $X_{i}$ given $X_{i-1}=x_{i-1}$ is Gaussian with\n$$\nE[X_{i}\\mid X_{i-1}=x_{i-1}] \\;=\\; \\mu + (x_{i-1}-\\mu)\\exp(-\\theta \\Delta t)\n\\;=\\; c + \\phi\\, x_{i-1},\n$$\nand\n$$\n\\operatorname{Var}(X_{i}\\mid X_{i-1}) \\;=\\; \\frac{\\sigma^{2}}{2\\theta}\\bigl(1-\\exp(-2\\theta \\Delta t)\\bigr) \\;=\\; v,\n$$\nwhere we set\n$$\n\\phi \\;=\\; \\exp(-\\theta \\Delta t), \\qquad c \\;=\\; \\mu(1-\\phi), \\qquad v \\;=\\; \\frac{\\sigma^{2}}{2\\theta}(1-\\phi^{2}).\n$$\nThus the likelihood of $\\{x_{i}\\}_{i=1}^{n}$ given $x_{0}$ is the product of $n$ Gaussian densities with common variance $v$ and mean $c+\\phi x_{i-1}$. The log-likelihood, up to an additive constant, is\n$$\n\\ell(c,\\phi,v) \\;=\\; -\\frac{n}{2}\\ln v \\;-\\; \\frac{1}{2v}\\sum_{i=1}^{n}\\bigl(x_{i} - c - \\phi x_{i-1}\\bigr)^{2}.\n$$\nFor fixed $(c,\\phi)$, maximizing in $v$ gives the Gaussian MLE\n$$\n\\hat{v} \\;=\\; \\frac{1}{n}\\sum_{i=1}^{n}\\bigl(x_{i} - c - \\phi x_{i-1}\\bigr)^{2}.\n$$\nPlugging this back shows that maximizing the likelihood in $(c,\\phi)$ is equivalent to minimizing the residual sum of squares\n$$\n\\operatorname{RSS}(c,\\phi) \\;=\\; \\sum_{i=1}^{n}\\bigl(x_{i} - c - \\phi x_{i-1}\\bigr)^{2}.\n$$\nThis is the ordinary least squares problem for an AR(1) with intercept. The normal equations are\n$$\nn c + \\phi S_{0} \\;=\\; S_{1}, \\qquad c S_{0} + \\phi S_{00} \\;=\\; S_{01}.\n$$\nSolving yields\n$$\n\\hat{\\phi} \\;=\\; \\frac{S_{01} - \\frac{S_{0}S_{1}}{n}}{S_{00} - \\frac{S_{0}^{2}}{n}}, \n\\qquad\n\\hat{c} \\;=\\; \\frac{S_{1} - \\hat{\\phi}\\, S_{0}}{n}.\n$$\nTransforming back to $(\\theta,\\mu)$ gives\n$$\n\\hat{\\theta} \\;=\\; -\\frac{1}{\\Delta t}\\,\\ln(\\hat{\\phi}), \n\\qquad\n\\hat{\\mu} \\;=\\; \\frac{\\hat{c}}{1-\\hat{\\phi}} \\;=\\; \\frac{S_{1} - \\hat{\\phi}\\, S_{0}}{n(1-\\hat{\\phi})}.\n$$\nThe MLE of $v$ is $\\hat{v} = \\operatorname{RSS}(\\hat{c},\\hat{\\phi})/n$. Since $v = \\frac{\\sigma^{2}}{2\\theta}(1-\\phi^{2})$, we obtain\n$$\n\\hat{\\sigma}^{2} \\;=\\; \\frac{2\\hat{\\theta}}{1-\\hat{\\phi}^{2}}\\;\\hat{v} \\;=\\; \\frac{2\\hat{\\theta}}{1-\\hat{\\phi}^{2}}\\;\\frac{1}{n}\\,\\operatorname{RSS}(\\hat{c},\\hat{\\phi}).\n$$\nTo express $\\operatorname{RSS}(\\hat{c},\\hat{\\phi})$ in terms of the summary statistics, expand and simplify to obtain\n$$\n\\operatorname{RSS}(\\hat{c},\\hat{\\phi}) \\;=\\; S_{11} - \\frac{S_{1}^{2}}{n} - \\frac{\\left(S_{01} - \\frac{S_{0}S_{1}}{n}\\right)^{2}}{S_{00} - \\frac{S_{0}^{2}}{n}}.\n$$\nIt follows that\n$$\n\\hat{\\theta} \\;=\\; -\\frac{1}{\\Delta t}\\,\\ln\\!\\left(\\frac{S_{01} - \\frac{S_{0}S_{1}}{n}}{S_{00} - \\frac{S_{0}^{2}}{n}}\\right),\n$$\n$$\n\\hat{\\mu} \\;=\\; \\frac{\\left(S_{00} - \\frac{S_{0}^{2}}{n}\\right)S_{1} - \\left(S_{01} - \\frac{S_{0}S_{1}}{n}\\right)S_{0}}{n\\left[\\left(S_{00} - \\frac{S_{0}^{2}}{n}\\right) - \\left(S_{01} - \\frac{S_{0}S_{1}}{n}\\right)\\right]},\n$$\n$$\n\\hat{\\sigma} \\;=\\; \\sqrt{ \\frac{ - 2 \\ln\\!\\left( \\frac{ S_{01} - \\frac{ S_{0} S_{1} }{ n } }{ S_{00} - \\frac{ S_{0}^{2} }{ n } } \\right ) }{ \\Delta t \\left( 1 - \\left( \\frac{ S_{01} - \\frac{ S_{0} S_{1} }{ n } }{ S_{00} - \\frac{ S_{0}^{2} }{ n } } \\right )^{2} \\right ) } \\cdot \\frac{1}{n} \\left( S_{11} - \\frac{ S_{1}^{2} }{ n } - \\frac{ \\left( S_{01} - \\frac{ S_{0} S_{1} }{ n } \\right )^{2} }{ S_{00} - \\frac{ S_{0}^{2} }{ n } } \\right ) }.\n$$\nThese are the maximum likelihood estimators expressed entirely in terms of $n$, $\\Delta t$, and the given summary statistics.", "answer": "$$\n\\boxed{\n\\begin{aligned}\n\\hat{\\theta} = -\\frac{1}{\\Delta t}\\,\\ln\\!\\left(\\frac{S_{01}-\\frac{S_{0}S_{1}}{n}}{S_{00}-\\frac{S_{0}^{2}}{n}}\\right) \\\\\n\\hat{\\mu} = \\frac{\\left(S_{00}-\\frac{S_{0}^{2}}{n}\\right)S_{1}-\\left(S_{01}-\\frac{S_{0}S_{1}}{n}\\right)S_{0}}{n\\left[\\left(S_{00}-\\frac{S_{0}^{2}}{n}\\right)-\\left(S_{01}-\\frac{S_{0}S_{1}}{n}\\right)\\right]} \\\\\n\\hat{\\sigma} = \\sqrt{\\frac{-2\\,\\ln\\!\\left(\\frac{S_{01}-\\frac{S_{0}S_{1}}{n}}{S_{00}-\\frac{S_{0}^{2}}{n}}\\right)}{\\Delta t\\left(1-\\left(\\frac{S_{01}-\\frac{S_{0}S_{1}}{n}}{S_{00}-\\frac{S_{0}^{2}}{n}}\\right)^{2}\\right)}\\cdot \\frac{1}{n}\\left(S_{11}-\\frac{S_{1}^{2}}{n}-\\frac{\\left(S_{01}-\\frac{S_{0}S_{1}}{n}\\right)^{2}}{S_{00}-\\frac{S_{0}^{2}}{n}}\\right)}\n\\end{aligned}\n}\n$$", "id": "1343701"}]}