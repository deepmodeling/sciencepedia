{"hands_on_practices": [{"introduction": "The best way to grasp a powerful theorem like the Dambis–Dubins–Schwarz (DDS) theorem is to work through a concrete example from first principles. This exercise guides you through the entire DDS procedure for a specific, well-behaved continuous local martingale where every component can be calculated in closed form [@problem_id:3000787]. By explicitly computing the quadratic variation $\\langle M \\rangle_t$, finding the inverse time-change $T_s$, and verifying a key property of the resulting process, you will solidify your understanding of the theorem's core mechanics.", "problem": "Consider a probability space $\\left(\\Omega,\\mathcal{F},\\mathbb{P}\\right)$ equipped with a filtration $\\left\\{\\mathcal{F}_{t}\\right\\}_{t\\geq 0}$ satisfying the usual conditions, and let $\\left\\{W_{t}\\right\\}_{t\\geq 0}$ be a standard Brownian motion. Define the continuous local martingale $M=\\left\\{M_{t}\\right\\}_{t\\geq 0}$ by\n$$\nM_{t} \\equiv \\int_{0}^{t} \\exp\\!\\left(\\frac{u}{2}\\right)\\,\\mathrm{d}W_{u}.\n$$\nLet $A_{t}\\equiv \\langle M\\rangle_{t}$ denote the quadratic variation of $M$ at time $t$. The Dambis–Dubins–Schwarz (DDS) time change is built from the right-continuous inverse of the quadratic variation $A$, defined by\n$$\nT_{s}\\equiv \\inf\\left\\{t\\geq 0:\\,A_{t}s\\right\\},\\quad s\\geq 0.\n$$\nStarting only from the definitions of quadratic variation and the Itô isometry, and the characterization that $M_{t}^{2}-\\langle M\\rangle_{t}$ is a martingale for a continuous local martingale, do the following:\n- Compute $A_{t}$ and show it is deterministic and strictly increasing.\n- Explicitly construct $T_{s}$ in closed form.\n- Define $B_{s}\\equiv M_{T_{s}}$ and verify that $\\operatorname{Var}\\!\\left(B_{s}\\right)$ equals $s$.\n\nExpress your final answer as a single row matrix containing the time change $T_{s}$ and the variance $\\operatorname{Var}\\!\\left(B_{s}\\right)$, in that order. No rounding is required.", "solution": "The user has provided a valid problem in stochastic calculus concerning the Dambis–Dubins–Schwarz (DDS) time change theorem. The problem is scientifically grounded, well-posed, and objective. We will proceed with a full solution.\n\nThe problem asks us to analyze the continuous local martingale $M$ defined by the Itô integral\n$$\nM_{t} = \\int_{0}^{t} \\exp\\left(\\frac{u}{2}\\right)\\,\\mathrm{d}W_{u}\n$$\nwhere $\\left\\{W_{t}\\right\\}_{t\\geq 0}$ is a standard Brownian motion. We are tasked with three objectives:\n$1$. Compute the quadratic variation $A_{t} \\equiv \\langle M\\rangle_{t}$ and show it is deterministic and strictly increasing.\n$2$. Find a closed-form expression for the time change $T_{s} \\equiv \\inf\\left\\{t\\geq 0:\\,A_{t}s\\right\\}$.\n$3$. Verify that the variance of the time-changed process $B_{s} \\equiv M_{T_{s}}$ is equal to $s$, i.e., $\\operatorname{Var}(B_s) = s$.\n\nWe will address each of these tasks sequentially.\n\nFirst, we compute the quadratic variation, $A_{t}$. The process $M_{t}$ is an Itô integral of the form $\\int_{0}^{t} \\sigma_{u}\\,\\mathrm{d}W_{u}$, where the integrand is $\\sigma_{u} = \\exp\\left(\\frac{u}{2}\\right)$. For any such Itô integral, the quadratic variation process $\\langle M \\rangle_{t}$ is given by\n$$\n\\langle M \\rangle_{t} = \\int_{0}^{t} \\sigma_{u}^{2}\\,\\mathrm{d}u.\n$$\nIn this case, the integrand $\\sigma_{u}$ is a deterministic function of time. We substitute its expression into the formula for the quadratic variation:\n$$\nA_{t} = \\langle M \\rangle_{t} = \\int_{0}^{t} \\left(\\exp\\left(\\frac{u}{2}\\right)\\right)^{2}\\,\\mathrm{d}u = \\int_{0}^{t} \\exp(u)\\,\\mathrm{d}u.\n$$\nEvaluating this definite integral gives:\n$$\nA_{t} = \\left[\\exp(u)\\right]_{0}^{t} = \\exp(t) - \\exp(0) = \\exp(t) - 1.\n$$\nThis expression for $A_{t}$ depends only on the time parameter $t$ and not on any stochastic elements from the probability space $\\left(\\Omega,\\mathcal{F},\\mathbb{P}\\right)$. Therefore, $A_{t} = \\exp(t) - 1$ is a deterministic process.\n\nTo show that $A_{t}$ is strictly increasing for $t \\geq 0$, we can examine its derivative with respect to $t$:\n$$\n\\frac{\\mathrm{d}A_{t}}{\\mathrm{d}t} = \\frac{\\mathrm{d}}{\\mathrm{d}t}\\left(\\exp(t) - 1\\right) = \\exp(t).\n$$\nFor all real values of $t$, the exponential function $\\exp(t)$ is strictly positive. Consequently, for $t \\geq 0$, we have $\\frac{\\mathrm{d}A_{t}}{\\mathrm{d}t} > 0$, which proves that $A_{t}$ is a strictly increasing function of $t$.\n\nSecond, we construct the time change $T_{s}$ in closed form. $T_{s}$ is defined as the right-continuous inverse of $A_{t}$:\n$$\nT_{s} \\equiv \\inf\\left\\{t\\geq 0:\\,A_{t}s\\right\\}, \\quad s \\geq 0.\n$$\nWe substitute our expression for $A_{t}$:\n$$\nT_{s} = \\inf\\left\\{t\\geq 0:\\,\\exp(t) - 1  s\\right\\}.\n$$\nThe inequality can be rearranged as $\\exp(t)  s + 1$. Since the natural logarithm function, $\\ln(\\cdot)$, is the inverse of the exponential function and is strictly increasing, we can apply it to both sides of the inequality without changing its direction:\n$$\n\\ln(\\exp(t))  \\ln(s+1) \\implies t  \\ln(s+1).\n$$\nSo, the definition of $T_{s}$ becomes:\n$$\nT_{s} = \\inf\\left\\{t\\geq 0:\\,t  \\ln(s+1)\\right\\}.\n$$\nThe infimum of the set of all $t$ strictly greater than $\\ln(s+1)$ is precisely $\\ln(s+1)$. For the condition $t \\geq 0$ to be satisfied, we must have $\\ln(s+1) \\geq 0$, which implies $s+1 \\geq 1$, or $s \\geq 0$. This aligns with the domain of $s$ given in the problem. Thus, the closed-form expression for the time change is:\n$$\nT_{s} = \\ln(s+1).\n$$\n\nThird, we verify that $\\operatorname{Var}(B_{s}) = s$, where $B_{s} = M_{T_{s}}$. The variance is defined as $\\operatorname{Var}(B_s) = \\mathbb{E}[B_s^2] - (\\mathbb{E}[B_s])^2$.\n\nLet's first compute the expectation of $B_{s}$. Since $M_t$ is an Itô integral with a deterministic integrand with respect to a standard Brownian motion, its expectation is zero for all $t$:\n$$\n\\mathbb{E}[M_t] = \\mathbb{E}\\left[\\int_{0}^{t} \\exp\\left(\\frac{u}{2}\\right)\\,\\mathrm{d}W_{u}\\right] = 0.\n$$\nThe time change $T_s = \\ln(s+1)$ is deterministic. Therefore, $\\mathbb{E}[B_s] = \\mathbb{E}[M_{T_s}] = \\mathbb{E}[M_{\\ln(s+1)}] = 0$.\n\nNow we compute the second moment, $\\mathbb{E}[B_s^2]$. According to the Itô isometry, for a deterministic integrand $\\sigma_u$, the second moment of the Itô integral $M_t = \\int_{0}^{t} \\sigma_u \\,\\mathrm{d}W_u$ is:\n$$\n\\mathbb{E}[M_t^2] = \\mathbb{E}\\left[\\left(\\int_{0}^{t} \\sigma_{u}\\,\\mathrm{d}W_{u}\\right)^2\\right] = \\int_{0}^{t} \\sigma_{u}^{2}\\,\\mathrm{d}u.\n$$\nWe recognize the right-hand side as the quadratic variation $\\langle M \\rangle_t$, which we found to be $A_t = \\exp(t) - 1$. So, $\\mathbb{E}[M_t^2] = A_t$. This is also consistent with the provided characterization that $M_t^2 - \\langle M \\rangle_t$ is a martingale, which implies $\\mathbb{E}[M_t^2 - \\langle M \\rangle_t] = 0$, leading to $\\mathbb{E}[M_t^2] = \\mathbb{E}[\\langle M \\rangle_t]$. Since $\\langle M \\rangle_t = A_t$ is deterministic, $\\mathbb{E}[\\langle M \\rangle_t] = A_t$, and thus $\\mathbb{E}[M_t^2] = A_t$.\n\nNow we can compute $\\mathbb{E}[B_s^2] = \\mathbb{E}[M_{T_s}^2]$. Since $T_s = \\ln(s+1)$ is a deterministic time, we can substitute this value for $t$ in our expression for the second moment:\n$$\n\\mathbb{E}[B_s^2] = \\mathbb{E}[M_{\\ln(s+1)}^2] = A_{\\ln(s+1)}.\n$$\nUsing the formula $A_t = \\exp(t) - 1$, we get:\n$$\nA_{\\ln(s+1)} = \\exp(\\ln(s+1)) - 1 = (s+1) - 1 = s.\n$$\nTherefore, $\\mathbb{E}[B_s^2] = s$.\nThe variance is then:\n$$\n\\operatorname{Var}(B_s) = \\mathbb{E}[B_s^2] - (\\mathbb{E}[B_s])^2 = s - 0^2 = s.\n$$\nThis completes the verification. The Dambis–Dubins–Schwarz theorem indeed asserts that the time-changed process $B_{s}$ is a standard Brownian motion, and a key property of a standard Brownian motion is that its variance at time $s$ is $s$. Our calculation confirms this property.\n\nThe final answer requires the expression for $T_s$ and the value of $\\operatorname{Var}(B_s)$.\n- Time change: $T_{s} = \\ln(s+1)$.\n- Variance: $\\operatorname{Var}(B_{s}) = s$.", "answer": "$$\n\\boxed{\\begin{pmatrix} \\ln(s+1)  s \\end{pmatrix}}\n$$", "id": "3000787"}, {"introduction": "Moving from the ideal world of analytical solutions, we often encounter situations where we only have discrete observations of a process. This practice challenges you to bridge the gap between continuous-time theory and discrete-time data by designing and implementing numerical algorithms to approximate the DDS time change [@problem_id:3000817]. This exercise is invaluable for developing practical skills in computational finance and stochastic modeling, forcing you to \"invert\" a process based on a finite set of its values.", "problem": "Let $M = \\{M_t\\}_{t \\geq 0}$ be a continuous local martingale with $M_0 = 0$, and let its quadratic variation $\\langle M \\rangle = \\{\\langle M \\rangle_t\\}_{t \\geq 0}$ be the unique continuous, nondecreasing process satisfying the Itô isometry for stochastic integrals with respect to $M$. The Dambis–Dubins–Schwarz theorem time-change is defined by the map $T : [0,\\infty) \\to [0,\\infty]$ given by the right-continuous inverse\n$$\nT_s := \\inf\\{ t \\geq 0 : \\langle M \\rangle_t  s \\}.\n$$\nIn numerical practice, one observes a discrete-time sampling of the quadratic variation $\\langle M \\rangle_t$ on a uniform grid $t_k = k\\Delta$ over a finite horizon $[0,T]$, with observed values $A_k := \\langle M \\rangle_{t_k}$ for $k = 0,1,\\dots, N$, where $N = \\lfloor T/\\Delta \\rfloor$. Design a discrete scheme that approximates $T_s$ using only the grid $\\{t_k\\}$ and the values $\\{A_k\\}$, and analyze its convergence as the mesh size $\\Delta \\to 0$ under regularity assumptions on $\\langle M \\rangle$ appropriate for continuous local martingales.\n\nYour program must implement two approximation schemes for $T_s$:\n- A stepwise inverse scheme that returns the first grid time $t_k$ at which the observation $A_k$ strictly exceeds $s$.\n- A linearly interpolated inverse scheme that, when $s$ lies strictly between two consecutive observed values $A_{k-1}  s  A_k$, returns the unique $t \\in (t_{k-1}, t_k)$ at which the line joining $(t_{k-1}, A_{k-1})$ and $(t_k, A_k)$ attains level $s$; if $A_{k-1} = A_k$, return $t_k$ for the strict exceedance criterion.\n\nThe analysis must be principled, starting from fundamental properties of continuous local martingales and their quadratic variation processes. You must not use any shortcut formulas beyond those definitions.\n\nImplement the schemes and empirically assess their errors against ground truth in the following test suite, where the exact inverse $T_s$ is known analytically. In each case, define $A(t)$ and the corresponding exact inverse $T(s)$, set a horizon $T$, choose two mesh sizes $\\Delta_1$ and $\\Delta_2$, and compute the supremum error over a uniform grid of $s$ values in the attainable range $[0, A(T)]$.\n\nTest Suite:\n1. Smooth, constant derivative:\n   - $A(t) := t$ on $[0, 2]$, so $T(s) = s$.\n   - Horizon $T = 2$, meshes $\\Delta_1 = 0.1$, $\\Delta_2 = 0.05$.\n   - Use $1001$ uniformly spaced $s$ values in $[0, A(T)] = [0, 2]$.\n\n2. Smooth, strictly increasing with nonconstant derivative:\n   - $A(t) := t^2 + t$ on $[0, 1]$, so $T(s) = \\frac{-1 + \\sqrt{1 + 4s}}{2}$.\n   - Horizon $T = 1$, meshes $\\Delta_1 = 0.1$, $\\Delta_2 = 0.05$.\n   - Use $1001$ uniformly spaced $s$ values in $[0, A(T)] = [0, 2]$.\n\n3. Plateau followed by linear growth (edge case):\n   - $A(t) := 0$ for $t \\in [0, \\tfrac{1}{2}]$ and $A(t) := t - \\tfrac{1}{2}$ for $t \\in (\\tfrac{1}{2}, 1]$, so $A(T) = \\tfrac{1}{2}$.\n   - The exact inverse $T(s)$ equals $T(0) = \\tfrac{1}{2}$ and $T(s) = \\tfrac{1}{2} + s$ for $s \\in (0, \\tfrac{1}{2}]$.\n   - Horizon $T = 1$, meshes $\\Delta_1 = 0.1$, $\\Delta_2 = 0.05$.\n   - Use $1001$ uniformly spaced $s$ values in $[0, A(T)] = [0, \\tfrac{1}{2}]$.\n\nFor each test case $i \\in \\{1,2,3\\}$ and each mesh $\\Delta \\in \\{\\Delta_1, \\Delta_2\\}$, compute the supremum over the $s$-grid of the absolute error between the approximate inverse $T_s^\\Delta$ and the exact inverse $T(s)$ for the stepwise scheme and the interpolated scheme. The outputs must be $12$ floating-point numbers ordered as\n$$\n\\big[E^{\\mathrm{step}}_{1,\\Delta_1}, E^{\\mathrm{step}}_{1,\\Delta_2}, E^{\\mathrm{lin}}_{1,\\Delta_1}, E^{\\mathrm{lin}}_{1,\\Delta_2}, E^{\\mathrm{step}}_{2,\\Delta_1}, E^{\\mathrm{step}}_{2,\\Delta_2}, E^{\\mathrm{lin}}_{2,\\Delta_1}, E^{\\mathrm{lin}}_{2,\\Delta_2}, E^{\\mathrm{step}}_{3,\\Delta_1}, E^{\\mathrm{step}}_{3,\\Delta_2}, E^{\\mathrm{lin}}_{3,\\Delta_1}, E^{\\mathrm{lin}}_{3,\\Delta_2}\\big].\n$$\n\nFinal Output Format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[result1,result2,result3]\"). All outputs are dimensionless real numbers and must be printed as floats.", "solution": "The starting point is the structure of quadratic variation for continuous local martingales and the definition of the Dambis–Dubins–Schwarz time change. For a continuous local martingale $M_t$ adapted to a filtration satisfying usual conditions, the quadratic variation process $\\langle M \\rangle_t$ is the unique continuous, nondecreasing process with $\\langle M \\rangle_0 = 0$ such that for any bounded predictable process $\\phi$,\n$$\n\\mathbb{E}\\left[\\left(\\int_0^t \\phi_u \\, \\mathrm{d}M_u\\right)^2\\right] = \\mathbb{E}\\left[\\int_0^t \\phi_u^2 \\, \\mathrm{d}\\langle M \\rangle_u\\right].\n$$\nWhen $M_t$ is the Itô integral $M_t = \\int_0^t \\sigma(u) \\, \\mathrm{d}W_u$ for a Brownian motion $W_u$ and square-integrable predictable process $\\sigma$, the quadratic variation is deterministic,\n$$\n\\langle M \\rangle_t = \\int_0^t \\sigma^2(u) \\, \\mathrm{d}u,\n$$\nwhich is continuous, nondecreasing, with $\\langle M \\rangle_0 = 0$. The Dambis–Dubins–Schwarz theorem defines a time change\n$$\nT_s := \\inf \\{ t \\geq 0 : \\langle M \\rangle_t  s \\},\n$$\nwhich is the right-continuous inverse of the map $t \\mapsto \\langle M \\rangle_t$ and yields a Brownian motion $B_s := M_{T_s}$.\n\nWe design and analyze numerical schemes for approximating $T_s$ from discrete samples of $\\langle M \\rangle$.\n\n1. Discrete stepwise inverse scheme. Given a uniform grid $t_k = k\\Delta$ for $k = 0, 1, \\dots, N$ and observed values $A_k := \\langle M \\rangle_{t_k}$, define for any level $s \\geq 0$ the approximation\n$$\nT_s^{\\mathrm{step},\\Delta} := \n\\begin{cases}\nt_k  \\text{for the smallest } k \\in \\{0,\\dots,N\\} \\text{ such that } A_k  s,\\\\\nt_N  \\text{if no such } k \\text{ exists on } [0,T].\n\\end{cases}\n$$\nThis is the discrete analogue of the right-continuous inverse mapping with the strict inequality. Since $\\{A_k\\}$ is nondecreasing in $k$, the selection of the first index with $A_k  s$ is well-defined.\n\n2. Discrete linearly interpolated inverse scheme. If $s$ lies strictly between two consecutive observed values $A_{k-1}  s  A_k$, then within the interval $[t_{k-1}, t_k]$ the piecewise-linear interpolant of $t \\mapsto A(t)$ is defined by\n$$\n\\widetilde{A}(t) := A_{k-1} + \\frac{A_k - A_{k-1}}{\\Delta} (t - t_{k-1}) \\quad \\text{for } t \\in [t_{k-1}, t_k].\n$$\nWe then define\n$$\nT_s^{\\mathrm{lin},\\Delta} := t_{k-1} + \\Delta \\cdot \\frac{s - A_{k-1}}{A_k - A_{k-1}},\n$$\nand in case of a plateau $A_k = A_{k-1}$, enforce the strict exceedance criterion by returning $t_k$:\n$$\nT_s^{\\mathrm{lin},\\Delta} := \n\\begin{cases}\nt_{k-1} + \\Delta \\cdot \\dfrac{s - A_{k-1}}{A_k - A_{k-1}}  \\text{if } A_k  A_{k-1},\\\\\nt_k  \\text{if } A_k = A_{k-1},\n\\end{cases}\n$$\nwith the understanding that if no $k$ exists such that $A_k  s$ on $[0,T]$, we return $t_N$.\n\nConvergence analysis is based on the regularity of the map $A(t) := \\langle M \\rangle_t$. Assume $A$ is continuous, nondecreasing, and differentiable with derivative $a(t) := A'(t)$ satisfying for all $t \\in [0,T]$\n$$\n0 \\leq a(t) \\leq M, \\quad \\text{and} \\quad a(t) \\geq m  0 \\text{ for } t \\in [0,T_*],\n$$\nfor some finite $M$ and strictly positive $m$ on the subset where the inverse is well-defined without plateaus. The strict positivity ensures invertibility on $[0,T_*]$. For plateaus where $a(t) = 0$ on an interval, $A$ is constant and the right-continuous inverse $T_s$ exhibits jumps at the corresponding levels.\n\nStepwise inverse error bound. Let $s \\in [0, A(T)]$ and suppose there exists a unique $t \\in [0,T]$ with $A(t) = s$ and $a(t) \\geq m  0$. Denote $k$ the smallest integer with $t_k \\geq t$. Then $A_{k-1} \\leq s  A_k$ and\n$$\nT_s^{\\mathrm{step},\\Delta} = t_k, \\quad \\text{while} \\quad T(s) = t.\n$$\nHence the error satisfies\n$$\n|T_s^{\\mathrm{step},\\Delta} - T(s)| = t_k - t \\leq \\Delta,\n$$\nand taking the supremum over $s$ in any compact subset where $a(t) \\geq m$ yields\n$$\n\\sup_s |T_s^{\\mathrm{step},\\Delta} - T(s)| \\leq \\Delta.\n$$\nAt plateau levels, e.g., if $A(t) = c$ for $t \\in [u,v]$, then $T(c) = u^+$ by the strict inequality and the stepwise inverse returns the first gridpoint beyond $u$, which incurs an error bounded by the grid distance to $u$, again of order $\\Delta$.\n\nLinearly interpolated inverse error bound. Under additional smoothness, suppose $A$ is twice continuously differentiable on $[0,T_*]$, with $|A''(t)| \\leq L$ and $A'(t) \\geq m  0$ on $[0,T_*]$. For $t \\in [t_{k-1}, t_k]$ and $s = A(t)$, the linear interpolant error obeys the standard bound\n$$\n|A(t) - \\widetilde{A}(t)| \\leq \\frac{L}{8} \\Delta^2 \\quad \\text{for } t \\in [t_{k-1}, t_k].\n$$\nLet $t^\\Delta$ solve $\\widetilde{A}(t^\\Delta) = s$ and $t$ solve $A(t) = s$. Then by the inverse function theorem and mean-value arguments,\n$$\n|t^\\Delta - t| \\leq \\frac{1}{m} |A(t) - \\widetilde{A}(t)| \\leq \\frac{L}{8 m} \\Delta^2.\n$$\nThus the linearly interpolated inverse exhibits second-order accuracy in regions of positive derivative. In the presence of plateaus, when $A_k = A_{k-1}$, enforcing strict exceedance via $t_k$ reduces to the stepwise behavior, giving an error of order $\\Delta$ at jump levels.\n\nConnection to the Dambis–Dubins–Schwarz theorem. The time change $T_s$ is the inverse of the quadratic variation; its accurate approximation enables numerical construction of the Brownian motion $B_s = M_{T_s}$ via time change. The convergence of $T_s^\\Delta$ implies that compositions $M_{T_s^\\Delta}$ converge to $M_{T_s}$ under suitable continuity assumptions on $M$, supporting consistent discretizations of the Dambis–Dubins–Schwarz representation.\n\nComputational verification. We test the schemes against three deterministic $A(t)$ corresponding to quadratic variations of Itô-driven martingales:\n\n- Case 1: $A(t) = t$ on $[0,2]$, with exact inverse $T(s) = s$. The stepwise inverse incurs a worst-case error $\\Delta$, while the linear inverse is exact because $A$ is linear.\n\n- Case 2: $A(t) = t^2 + t$ on $[0,1]$, with exact inverse $T(s) = \\frac{-1 + \\sqrt{1 + 4s}}{2}$. The stepwise inverse exhibits first-order convergence, while the linear inverse displays second-order behavior by the smoothness of $A$.\n\n- Case 3: $A(t) = 0$ on $[0,\\tfrac{1}{2}]$, then $A(t) = t - \\tfrac{1}{2}$ on $(\\tfrac{1}{2},1]$. At the plateau level $s=0$, both schemes give errors of order $\\Delta$ due to the strict exceedance definition; for $s0$, the linear inverse is exact on the linear segment and the stepwise inverse has error bounded by $\\Delta$.\n\nFor each case, the program computes the supremum over a fine grid of $s$ values of the absolute error between $T_s^\\Delta$ and $T(s)$ for meshes $\\Delta_1 = 0.1$ and $\\Delta_2 = 0.05$. The final output lists, in order, the stepwise and linear supremum errors for the two meshes in each of the three cases, as a single-line comma-separated list in square brackets. All quantities are dimensionless and presented as floating-point numbers. This empirical evaluation corroborates the theoretical bounds: first-order convergence for the stepwise inverse, exactness or second-order convergence for the linear inverse in smooth regions, and first-order behavior at plateau jump levels due to the strict exceedance criterion.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef step_inverse(A_vals, t_grid, s):\n    # Find the first index where A_vals[idx]  s (strict exceedance)\n    idx = np.searchsorted(A_vals, s, side='right')\n    if idx = len(t_grid):\n        return t_grid[-1]\n    return t_grid[idx]\n\ndef linear_inverse(A_vals, t_grid, s):\n    # Find insertion point with strict exceedance\n    idx = np.searchsorted(A_vals, s, side='right')\n    if idx == 0:\n        # s is = A_vals[0]; since A_vals[0] should be 0, strict exceedance only if s  0 (not in our tests)\n        return t_grid[0]\n    if idx = len(t_grid):\n        # s is beyond observed range; truncate at horizon\n        return t_grid[-1]\n    # Bracket [idx-1, idx]\n    A_prev = A_vals[idx - 1]\n    A_curr = A_vals[idx]\n    t_prev = t_grid[idx - 1]\n    t_curr = t_grid[idx]\n    if A_curr == A_prev:\n        # Plateau: enforce strict exceedance by jumping to t_curr\n        return t_curr\n    # Linear interpolation within the interval\n    return t_prev + (s - A_prev) / (A_curr - A_prev) * (t_curr - t_prev)\n\ndef compute_sup_errors(A_func, T_exact, T_horizon, deltas, s_grid):\n    results = []\n    for Delta in deltas:\n        # Build grid\n        # Ensure inclusion of T_horizon\n        num_steps = int(np.floor(T_horizon / Delta))\n        t_grid = np.linspace(0.0, num_steps * Delta, num_steps + 1)\n        # Sample A on grid\n        A_vals = A_func(t_grid)\n        # Compute errors over s_grid\n        step_errors = []\n        lin_errors = []\n        for s in s_grid:\n            t_step = step_inverse(A_vals, t_grid, s)\n            t_lin = linear_inverse(A_vals, t_grid, s)\n            t_true = T_exact(s)\n            step_errors.append(abs(t_step - t_true))\n            lin_errors.append(abs(t_lin - t_true))\n        results.append((np.max(step_errors), np.max(lin_errors)))\n    return results\n\ndef solve():\n    # Test cases specifications\n    # Case 1: A(t) = t on [0,2]\n    A1 = lambda t: t\n    T1 = lambda s: s\n    T1_horizon = 2.0\n    deltas = [0.1, 0.05]\n    s1 = np.linspace(0.0, A1(np.array([T1_horizon]))[0], 1001)\n\n    # Case 2: A(t) = t^2 + t on [0,1], T(s) = (-1 + sqrt(1+4s))/2\n    A2 = lambda t: t**2 + t\n    T2 = lambda s: (-1.0 + np.sqrt(1.0 + 4.0 * s)) / 2.0\n    T2_horizon = 1.0\n    s2 = np.linspace(0.0, A2(np.array([T2_horizon]))[0], 1001)\n\n    # Case 3: A(t) = 0 for t = 0.5, then A(t) = t - 0.5 for t  0.5 on [0,1]\n    def A3(t):\n        t = np.asarray(t)\n        out = np.zeros_like(t, dtype=float)\n        mask = t  0.5\n        out[mask] = t[mask] - 0.5\n        return out\n    def T3(s):\n        # Strict exceedance inverse: T(0) = 0.5; for s in (0, 0.5], T(s) = 0.5 + s\n        if s = 0.0:\n            return 0.5\n        return 0.5 + s\n    T3_horizon = 1.0\n    s3 = np.linspace(0.0, A3(np.array([T3_horizon]))[0], 1001)\n\n    # Compute sup errors\n    res1 = compute_sup_errors(A1, T1, T1_horizon, deltas, s1)\n    res2 = compute_sup_errors(A2, T2, T2_horizon, deltas, s2)\n    res3 = compute_sup_errors(A3, T3, T3_horizon, deltas, s3)\n\n    # Aggregate results in the required order:\n    # [E_step_1_d1, E_step_1_d2, E_lin_1_d1, E_lin_1_d2,\n    #  E_step_2_d1, E_step_2_d2, E_lin_2_d1, E_lin_2_d2,\n    #  E_step_3_d1, E_step_3_d2, E_lin_3_d1, E_lin_3_d2]\n    results = [\n        res1[0][0], res1[1][0], res1[0][1], res1[1][1],\n        res2[0][0], res2[1][0], res2[0][1], res2[1][1],\n        res3[0][0], res3[1][0], res3[0][1], res3[1][1],\n    ]\n\n    # Format with reasonable precision\n    formatted = [f\"{x:.10f}\" for x in results]\n    print(f\"[{','.join(formatted)}]\")\n\nsolve()\n```", "id": "3000817"}, {"introduction": "Building on the previous numerical practice, a crucial next step is to understand the accuracy of our approximations. This advanced exercise guides you through a theoretical analysis of the discretization error, connecting your numerical scheme to a central limit theorem for realized quadratic variation [@problem_id:3000808]. Mastering this type of analysis provides a deep understanding of the convergence properties of numerical methods in stochastic calculus, a hallmark of rigorous quantitative work.", "problem": "Let $\\left(\\Omega,\\mathcal{F},(\\mathcal{F}_{t})_{t \\in [0,S]},\\mathbb{P}\\right)$ be a filtered probability space supporting a standard Brownian motion $W$. Let $\\sigma:[0,S]\\to(0,\\infty)$ be a deterministic function of class $C^{3}$ such that $\\inf_{s\\in[0,S]}\\sigma(s)0$ and $\\sup_{s\\in[0,S]}\\sigma(s)\\infty$. Define the continuous local martingale\n$$\nM_{t} \\coloneqq \\int_{0}^{t}\\sigma(s)\\,dW_{s},\\qquad t\\in[0,S],\n$$\nand its quadratic variation\n$$\nA_{t} \\coloneqq \\langle M\\rangle_{t}=\\int_{0}^{t}\\sigma^{2}(s)\\,ds.\n$$\nLet $\\{0=t_{0}^{(n)}t_{1}^{(n)}\\cdotst_{n}^{(n)}=S\\}$ be the uniform partition of $[0,S]$ with mesh $\\Delta_{n}\\coloneqq S/n$. For $s\\in[0,S]$, define the left-continuous realized quadratic variation\n$$\n\\widehat{A}^{(n)}_{s}\\coloneqq \\sum_{k=0}^{N_{n}(s)-1}\\left(M_{t_{k+1}^{(n)}}-M_{t_{k}^{(n)}}\\right)^{2},\n\\qquad\nN_{n}(s)\\coloneqq \\max\\{k\\in\\{0,\\dots,n\\}: t_{k+1}^{(n)}\\le s\\}.\n$$\nFor $t\\in(0,A_{S})$, define the right-continuous generalized inverses\n$$\nT(t)\\coloneqq \\inf\\{s\\in[0,S]:A_{s} t\\},\n\\qquad\n\\widehat{T}^{(n)}(t)\\coloneqq \\inf\\{s\\in[0,S]:\\widehat{A}^{(n)}_{s} t\\},\n$$\nand the Dambis–Dubins–Schwarz time-changed processes\n$$\nB_{t}\\coloneqq M_{T(t)},\n\\qquad\n\\widehat{B}^{(n)}_{t}\\coloneqq M_{\\widehat{T}^{(n)}(t)}.\n$$\nAssume the classical Dambis–Dubins–Schwarz theorem so that $\\{B_{t}\\}_{t\\in[0,A_{S}]}$ is a standard Brownian motion with respect to the filtration $\\{\\mathcal{F}_{T(t)}\\}_{t\\in[0,A_{S}]}$.\n\nFix $t\\in(0,A_{S})$. Using only foundational facts about continuous local martingales, Itô isometry, properties of quadratic variation, and well-tested limit theorems for triangular arrays of independent random variables, determine the leading-order coefficient $c(t)$ defined by the limit\n$$\nc(t)\\coloneqq \\lim_{n\\to\\infty}\\Delta_{n}^{-1/2}\\,\\mathbb{E}\\big[\\big(\\widehat{B}^{(n)}_{t}-B_{t}\\big)^{2}\\big].\n$$\nYour final answer must be a single closed-form analytic expression in terms of $t$, $\\sigma$, and the inverse time $T(t)$. No numerical rounding is required and no units are involved.", "solution": "The user wants to find the coefficient $c(t)$ defined by the limit\n$$c(t) \\coloneqq \\lim_{n\\to\\infty} \\Delta_n^{-1/2} \\, \\mathbb{E}\\big[\\big(\\widehat{B}^{(n)}_{t}-B_{t}\\big)^2\\big].$$\n\n### Step 1: Preliminary Analysis of the Expectation\n\nThe quantity inside the expectation is the squared difference between the realized DDS time-changed process and the true DDS time-changed process. Let us first simplify this expression. By definition, $\\widehat{B}^{(n)}_{t} = M_{\\widehat{T}^{(n)}(t)}$ and $B_t = M_{T(t)}$. The process $M_t = \\int_0^t \\sigma(s) dW_s$ is a continuous local martingale. Since the integrand $\\sigma(s)$ is deterministic and bounded on $[0,S]$, $M_t$ is a true martingale on $[0,S]$.\n\nThe expectation can be written as $\\mathbb{E}[(M_{\\widehat{T}^{(n)}(t)} - M_{T(t)})^2]$. The time $T(t)$ is deterministic, while $\\widehat{T}^{(n)}(t)$ is an $(\\mathcal{F}_s)$-stopping time. We can decompose the squared difference as:\n$$ \\mathbb{E}\\big[\\big(M_{\\widehat{T}^{(n)}(t)} - M_{T(t)}\\big)^2\\big] = \\mathbb{E}\\big[M_{\\widehat{T}^{(n)}(t)}^2\\big] - 2\\mathbb{E}\\big[M_{\\widehat{T}^{(n)}(t)}M_{T(t)}\\big] + \\mathbb{E}\\big[M_{T(t)}^2\\big]. $$\nSince $M_t$ is a martingale and $T(t)$ is a deterministic time, the optional stopping theorem implies $\\mathbb{E}[M_{T(t)}^2] = \\mathbb{E}[\\langle M \\rangle_{T(t)}] = \\mathbb{E}[A_{T(t)}] = A_{T(t)}$. Similarly, as $\\widehat{T}^{(n)}(t)$ is a stopping time, $\\mathbb{E}[M_{\\widehat{T}^{(n)}(t)}^2] = \\mathbb{E}[A_{\\widehat{T}^{(n)}(t)}]$.\n\nFor the cross-term, we condition on the smaller of the two times. Let $\\tau_n = \\widehat{T}^{(n)}(t)$ and $\\tau=T(t)$.\n$$ \\mathbb{E}[M_{\\tau_n}M_{\\tau}] = \\mathbb{E}\\big[\\mathbb{E}[M_{\\tau_n}M_\\tau | \\mathcal{F}_{\\tau_n \\wedge \\tau}]\\big]. $$\nBy the martingale property, $\\mathbb{E}[M_{\\tau_n \\vee \\tau} | \\mathcal{F}_{\\tau_n \\wedge \\tau}] = M_{\\tau_n \\wedge \\tau}$. Since $\\tau$ is deterministic and one of the two times in the product is evaluated at $\\tau$, we have:\n$$ \\mathbb{E}[M_{\\tau_n}M_{\\tau}] = \\mathbb{E}[M_{\\tau_n \\wedge \\tau} M_{\\tau_n \\wedge \\tau}] = \\mathbb{E}[M_{\\tau_n \\wedge \\tau}^2] = \\mathbb{E}[A_{\\tau_n \\wedge \\tau}]. $$\nSubstituting these back, we get:\n$$ \\mathbb{E}\\big[\\big(M_{\\tau_n} - M_{\\tau}\\big)^2\\big] = \\mathbb{E}[A_{\\tau_n}] - 2\\mathbb{E}[A_{\\tau_n \\wedge \\tau}] + A_{\\tau}. $$\nThis expression can be conveniently rewritten using the identity $|x|=x-2\\min(x,0)$:\n\\begin{align*} \\mathbb{E}[A_{\\tau_n}] - 2\\mathbb{E}[A_{\\tau_n \\wedge \\tau}] + A_{\\tau} = \\mathbb{E}[A_{\\tau_n} - A_{\\tau_n \\wedge \\tau}] + \\mathbb{E}[A_{\\tau} - A_{\\tau_n \\wedge \\tau}] \\\\ = \\mathbb{E}[(A_{\\tau_n} - A_{\\tau})^+] + \\mathbb{E}[(A_{\\tau} - A_{\\tau_n})^+] \\\\ = \\mathbb{E}\\big[\\big|A_{\\widehat{T}^{(n)}(t)} - A_{T(t)}\\big|\\big].\\end{align*}\nThus, the problem reduces to calculating the asymptotic behavior of $\\mathbb{E}[|A_{\\widehat{T}^{(n)}(t)} - A_{T(t)}|]$.\n\n### Step 2: Relating Time Error to Quadratic Variation Error\n\nThe times $T(t)$ and $\\widehat{T}^{(n)}(t)$ are defined as the generalized inverses of the true quadratic variation $A_s$ and the realized quadratic variation $\\widehat{A}^{(n)}_s$, respectively. By definition, $A_{T(t)} = t$ (since $A_t$ is continuous and strictly increasing as $\\sigma(s)0$). The realized version $\\widehat{T}^{(n)}(t)$ is defined as $\\inf\\{s: \\widehat{A}^{(n)}_s  t\\}$. This implies that, up to an overshoot of the order of a single squared increment, $\\widehat{A}^{(n)}_{\\widehat{T}^{(n)}(t)} \\approx t$.\n\nLet us formalize the relationship between the time error $\\widehat{T}^{(n)}(t) - T(t)$ and the QV estimation error $\\widehat{A}^{(n)}_{s} - A_s$.\nLet $\\tau_n = \\widehat{T}^{(n)}(t)$ and $\\tau = T(t)$. It is a known result that $\\tau_n \\to \\tau$ in probability as $n \\to \\infty$.\nWe have $A_{\\tau} = t$. By definition of $\\tau_n$, $\\widehat{A}^{(n)}_{\\tau_n-}  t \\le \\widehat{A}^{(n)}_{\\tau_n}$. The difference $\\widehat{A}^{(n)}_{\\tau_n} - t$ is the overshoot, which is of order $O_p(\\Delta_n)$.\nLet's analyze $A_{\\tau_n}$ by relating it to $\\widehat{A}^{(n)}_{\\tau_n}$:\n$$ A_{\\tau_n} = \\widehat{A}^{(n)}_{\\tau_n} - (\\widehat{A}^{(n)}_{\\tau_n} - A_{\\tau_n}). $$\nUsing $A_{\\tau} = t$ and $\\widehat{A}^{(n)}_{\\tau_n} = t + O_p(\\Delta_n)$:\n$$ A_{\\tau_n} - A_{\\tau} = \\big(t + O_p(\\Delta_n)\\big) - t - (\\widehat{A}^{(n)}_{\\tau_n} - A_{\\tau_n}). $$\nSince $\\tau_n \\to \\tau$, we can approximate the error term $(\\widehat{A}^{(n)}_{\\tau_n} - A_{\\tau_n})$ by $(\\widehat{A}^{(n)}_{\\tau} - A_{\\tau})$. This yields the first-order approximation:\n$$ A_{\\tau_n} - A_{\\tau} \\approx - (\\widehat{A}^{(n)}_{\\tau} - A_{\\tau}). $$\nThis approximation can be made rigorous using the delta method for stochastic processes. It implies that to the leading order:\n$$ \\big|A_{\\widehat{T}^{(n)}(t)} - A_{T(t)}\\big| \\approx \\big|\\widehat{A}^{(n)}_{T(t)} - A_{T(t)}\\big|. $$\n\n### Step 3: Applying the Central Limit Theorem for Realized QV\n\nThe core of the problem lies in a well-established limit theorem for realized quadratic variation. For a continuous local martingale like $M_t$ with deterministic volatility $\\sigma(s)$, the scaled estimation error of the realized quadratic variation converges in distribution to a Gaussian process. Specifically, for any fixed time $s \\in [0,S]$:\n$$ Z_n(s) \\coloneqq \\Delta_n^{-1/2} \\left( \\widehat{A}^{(n)}_s - A_s \\right) \\xrightarrow{d} Z(s), $$\nwhere $\\xrightarrow{d}$ denotes convergence in distribution. The limit $Z(s)$ is a Gaussian random variable with mean $0$ and variance given by\n$$ \\text{Var}[Z(s)] = 2 \\int_0^s \\sigma^4(u) \\, du. $$\nThis result can be proven using limit theorems for triangular arrays of martingale differences. The summands $(\\Delta M_k^{(n)})^2 - \\Delta A_k^{(n)}$ form such an array.\n\n### Step 4: Final Calculation\n\nWe combine the results from the previous steps. We need to compute:\n$$ c(t) = \\lim_{n\\to\\infty} \\Delta_n^{-1/2} \\, \\mathbb{E}\\big[\\big|A_{\\widehat{T}^{(n)}(t)} - A_{T(t)}\\big|\\big]. $$\nUsing the approximation from Step 2, this becomes:\n$$ c(t) = \\lim_{n\\to\\infty} \\Delta_n^{-1/2} \\, \\mathbb{E}\\big[\\big|\\widehat{A}^{(n)}_{T(t)} - A_{T(t)}\\big|\\big]. $$\nWe can rewrite the term inside the expectation using the normalized error $Z_n(T(t))$:\n$$ \\big|\\widehat{A}^{(n)}_{T(t)} - A_{T(t)}\\big| = \\Delta_n^{1/2} \\big|Z_n(T(t))\\big|. $$\nSubstituting this into the expression for $c(t)$:\n$$ c(t) = \\lim_{n\\to\\infty} \\Delta_n^{-1/2} \\, \\mathbb{E}\\big[\\Delta_n^{1/2} |Z_n(T(t))|\\big] = \\lim_{n\\to\\infty} \\mathbb{E}\\big[|Z_n(T(t))|\\big]. $$\nThe convergence in distribution $Z_n(T(t)) \\xrightarrow{d} Z(T(t))$, coupled with uniform integrability of the sequence $\\{|Z_n(T(t))|\\}_{n\\in\\mathbb{N}}$, allows us to interchange the limit and the expectation:\n$$ c(t) = \\mathbb{E}\\big[|Z(T(t))|\\big]. $$\nThe random variable $Z(T(t))$ follows a normal distribution $\\mathcal{N}(0, \\Sigma^2)$ with variance $\\Sigma^2 = 2 \\int_0^{T(t)} \\sigma^4(s) ds$. The expectation of the absolute value of such a random variable is:\n$$ \\mathbb{E}[|Z(T(t))|] = \\sqrt{\\frac{2\\Sigma^2}{\\pi}}. $$\nSubstituting the expression for $\\Sigma^2$:\n$$ c(t) = \\sqrt{\\frac{2}{\\pi} \\left(2 \\int_0^{T(t)} \\sigma^4(s) \\, ds\\right)} = \\sqrt{\\frac{4}{\\pi} \\int_0^{T(t)} \\sigma^4(s) \\, ds}. $$\nThis simplifies to the final expression for the leading-order coefficient $c(t)$.", "answer": "$$\\boxed{\\frac{2}{\\sqrt{\\pi}} \\sqrt{\\int_{0}^{T(t)}\\sigma^{4}(s)\\,ds}}$$", "id": "3000808"}]}