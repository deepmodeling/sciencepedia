{"hands_on_practices": [{"introduction": "Understanding stochastic stability begins with grasping the crucial difference between the long-term behavior of a typical path and the behavior of statistical moments. This exercise explores this distinction through the classic example of geometric Brownian motion. By calculating both the almost sure Lyapunov exponent and the mean-square growth rate, you will uncover how random fluctuations can simultaneously stabilize individual trajectories while destabilizing the system's average energy or variance [@problem_id:2996038].", "problem": "Consider the scalar stochastic differential equation (SDE)\n$$\ndX_{t}=a\\,X_{t}\\,dt+b\\,X_{t}\\,dW_{t},\\qquad X_{0}\\neq 0,\n$$\nwhere $a\\in\\mathbb{R}$ and $b\\in\\mathbb{R}$ are constants, and $(W_{t})_{t\\ge 0}$ is a standard one-dimensional Wiener process (Brownian motion). Work from the fundamental definitions and tools of It\\^{o} calculus, including It\\^{o}'s formula and the basic properties of stochastic integrals, and do not assume any precomputed solution of the SDE.\n\nDefine the top Lyapunov exponent as\n$$\n\\lambda_{\\mathrm{top}}:=\\lim_{t\\to\\infty}\\frac{1}{t}\\ln|X_{t}|\\quad\\text{almost surely},\n$$\nand define the mean-square growth rate as\n$$\n\\lambda_{\\mathrm{ms}}:=\\lim_{t\\to\\infty}\\frac{1}{t}\\ln\\mathbb{E}\\bigl(|X_{t}|^{2}\\bigr),\n$$\nprovided these limits exist. Using first principles, compute closed-form expressions for $\\lambda_{\\mathrm{top}}$ and $\\lambda_{\\mathrm{ms}}$ in terms of $a$ and $b$, and explain how these expressions characterize exponential mean-square decay and the continuous dependence of $\\mathbb{E}(|X_{t}|^{2})$ on $X_{0}$.\n\nYour final answer must be the pair of expressions $\\lambda_{\\mathrm{top}}$ and $\\lambda_{\\mathrm{ms}}$ written as a single row matrix. No rounding is required.", "solution": "We are asked to derive the top Lyapunov exponent and the mean-square growth rate for the solution of a linear stochastic differential equation from first principles.\n\nThe SDE is given by:\n$$\ndX_{t}=a\\,X_{t}\\,dt+b\\,X_{t}\\,dW_{t}\n$$\nwith an initial condition $X_{0}\\neq 0$, where $a, b \\in \\mathbb{R}$ are constants and $(W_{t})_{t\\ge 0}$ is a standard one-dimensional Wiener process.\n\nFirst, we must find an explicit solution for $X_{t}$. The multiplicative nature of the coefficients suggests a logarithmic transformation. Let us define a new process $Y_{t} = \\ln|X_{t}|$. To apply Itô's formula, we consider the function $f(x) = \\ln|x|$. For $x \\neq 0$, the derivatives are $f'(x) = 1/x$ and $f''(x) = -1/x^2$. Since $X_0 \\neq 0$ and the drift and diffusion coefficients are zero at $X_t=0$, the solution will almost surely never reach zero, so $\\ln|X_t|$ is well-defined for all $t \\ge 0$.\n\nAccording to Itô's formula, for a process $Z_t = f(X_t)$, its differential is:\n$$\ndZ_{t} = f'(X_{t}) dX_{t} + \\frac{1}{2} f''(X_{t}) (dX_{t})^2\n$$\nThe quadratic variation term $(dX_{t})^2$ is computed using the rules $(dt)^2 = 0$, $dt\\,dW_{t} = 0$, and $(dW_{t})^2 = dt$:\n$$\n(dX_{t})^2 = (a\\,X_{t}\\,dt+b\\,X_{t}\\,dW_{t})^2 = (b\\,X_{t})^2 (dW_{t})^2 = b^2 X_{t}^2 dt\n$$\nSubstituting the derivatives and $(dX_{t})^2$ into the formula for $dY_{t} = d(\\ln|X_{t}|)$:\n$$\ndY_{t} = \\frac{1}{X_{t}} (a\\,X_{t}\\,dt+b\\,X_{t}\\,dW_{t}) + \\frac{1}{2} \\left(-\\frac{1}{X_{t}^2}\\right) (b^2 X_{t}^2 dt)\n$$\nSimplifying the expression, we obtain:\n$$\ndY_{t} = (a\\,dt+b\\,dW_{t}) - \\frac{1}{2} b^2 dt\n$$\n$$\ndY_{t} = \\left(a - \\frac{1}{2}b^2\\right)dt + b\\,dW_{t}\n$$\nThis is an Itô process with constant drift and diffusion coefficients. We can integrate it from $0$ to $t$:\n$$\n\\int_{0}^{t} dY_{s} = \\int_{0}^{t} \\left(a - \\frac{1}{2}b^2\\right)ds + \\int_{0}^{t} b\\,dW_{s}\n$$\n$$\nY_{t} - Y_{0} = \\left(a - \\frac{1}{2}b^2\\right)t + b(W_{t} - W_{0})\n$$\nGiven that $Y_{t} = \\ln|X_{t}|$ and $W_{0} = 0$ almost surely, we have:\n$$\n\\ln|X_{t}| - \\ln|X_{0}| = \\left(a - \\frac{1}{2}b^2\\right)t + bW_{t}\n$$\nSolving for $\\ln|X_t|$:\n$$\n\\ln|X_{t}| = \\ln|X_{0}| + \\left(a - \\frac{1}{2}b^2\\right)t + bW_{t}\n$$\nBy exponentiating both sides, we find the explicit expression for $|X_{t}|$:\n$$\n|X_{t}| = |X_{0}| \\exp\\left(\\left(a - \\frac{1}{2}b^2\\right)t + bW_{t}\\right)\n$$\nThe sign of $X_t$ is the same as the sign of $X_0$, so the full solution is:\n$$\nX_{t} = X_{0} \\exp\\left(\\left(a - \\frac{1}{2}b^2\\right)t + bW_{t}\\right)\n$$\n\nNow, we compute the top Lyapunov exponent, $\\lambda_{\\mathrm{top}}$:\n$$\n\\lambda_{\\mathrm{top}} := \\lim_{t\\to\\infty}\\frac{1}{t}\\ln|X_{t}|\\quad\\text{almost surely}\n$$\nSubstituting our expression for $\\ln|X_{t}|$:\n$$\n\\lambda_{\\mathrm{top}} = \\lim_{t\\to\\infty}\\frac{1}{t}\\left[\\ln|X_{0}| + \\left(a - \\frac{1}{2}b^2\\right)t + bW_{t}\\right]\n$$\nWe evaluate the limit of each term:\n$$\n\\lambda_{\\mathrm{top}} = \\lim_{t\\to\\infty}\\frac{\\ln|X_{0}|}{t} + \\lim_{t\\to\\infty}\\frac{\\left(a - \\frac{1}{2}b^2\\right)t}{t} + \\lim_{t\\to\\infty}\\frac{bW_{t}}{t}\n$$\nSince $X_0$ is a non-zero constant, the first term vanishes. The second term is a constant. For the third term, we invoke the Strong Law of Large Numbers for Brownian motion, which states that $\\lim_{t\\to\\infty} \\frac{W_{t}}{t} = 0$ almost surely. Therefore:\n$$\n\\lambda_{\\mathrm{top}} = 0 + \\left(a - \\frac{1}{2}b^2\\right) + b \\cdot 0 = a - \\frac{1}{2}b^2\n$$\n\nNext, we compute the mean-square growth rate, $\\lambda_{\\mathrm{ms}}$:\n$$\n\\lambda_{\\mathrm{ms}} := \\lim_{t\\to\\infty}\\frac{1}{t}\\ln\\mathbb{E}\\bigl(|X_{t}|^{2}\\bigr)\n$$\nFirst, we compute the expected value of $|X_{t}|^2$. From our solution for $|X_t|$:\n$$\n|X_{t}|^2 = |X_{0}|^2 \\left[\\exp\\left(\\left(a - \\frac{1}{2}b^2\\right)t + bW_{t}\\right)\\right]^2 = |X_{0}|^2 \\exp\\left(2\\left(a - \\frac{1}{2}b^2\\right)t + 2bW_{t}\\right)\n$$\n$$\n|X_{t}|^2 = |X_{0}|^2 \\exp\\left((2a - b^2)t + 2bW_{t}\\right)\n$$\nTaking the expectation:\n$$\n\\mathbb{E}\\bigl[|X_{t}|^2\\bigr] = \\mathbb{E}\\left[|X_{0}|^2 \\exp\\left((2a - b^2)t\\right) \\exp\\left(2bW_{t}\\right)\\right]\n$$\nThe term $|X_{0}|^2 \\exp\\left((2a - b^2)t\\right)$ is deterministic, so we can factor it out:\n$$\n\\mathbb{E}\\bigl[|X_{t}|^2\\bigr] = |X_{0}|^2 \\exp\\left((2a - b^2)t\\right) \\mathbb{E}\\left[\\exp\\left(2bW_{t}\\right)\\right]\n$$\nTo evaluate the remaining expectation, we recall that $W_t$ is a normally distributed random variable with mean $0$ and variance $t$, i.e., $W_{t} \\sim N(0, t)$. The moment-generating function of a normal random variable $Z \\sim N(\\mu, \\sigma^2)$ is $\\mathbb{E}[\\exp(sZ)] = \\exp(s\\mu + \\frac{1}{2}s^2\\sigma^2)$. Here, our random variable is $W_t$, $s = 2b$, $\\mu=0$, and $\\sigma^2=t$.\n$$\n\\mathbb{E}\\left[\\exp\\left(2bW_{t}\\right)\\right] = \\exp\\left( (2b)(0) + \\frac{1}{2}(2b)^2 t \\right) = \\exp\\left(\\frac{1}{2}(4b^2)t\\right) = \\exp(2b^2 t)\n$$\nSubstituting this result back into the expression for $\\mathbb{E}\\bigl[|X_{t}|^2\\bigr]$:\n$$\n\\mathbb{E}\\bigl[|X_{t}|^2\\bigr] = |X_{0}|^2 \\exp\\left((2a - b^2)t\\right) \\exp(2b^2 t) = |X_{0}|^2 \\exp\\left((2a - b^2 + 2b^2)t\\right)\n$$\n$$\n\\mathbb{E}\\bigl[|X_{t}|^2\\bigr] = |X_{0}|^2 \\exp\\left((2a + b^2)t\\right)\n$$\nNow we can compute $\\lambda_{\\mathrm{ms}}$:\n$$\n\\lambda_{\\mathrm{ms}} = \\lim_{t\\to\\infty}\\frac{1}{t}\\ln\\left(|X_{0}|^2 \\exp\\left((2a + b^2)t\\right)\\right)\n$$\n$$\n\\lambda_{\\mathrm{ms}} = \\lim_{t\\to\\infty}\\frac{1}{t}\\left[\\ln(|X_{0}|^2) + (2a + b^2)t\\right]\n$$\n$$\n\\lambda_{\\mathrm{ms}} = \\lim_{t\\to\\infty}\\frac{2\\ln|X_{0}|}{t} + \\lim_{t\\to\\infty}\\frac{(2a + b^2)t}{t} = 0 + (2a + b^2)\n$$\n$$\n\\lambda_{\\mathrm{ms}} = 2a + b^2\n$$\n\nFinally, we explain how these quantities characterize stability.\nThe expression $\\mathbb{E}\\bigl[|X_{t}|^2\\bigr] = |X_{0}|^2 \\exp((2a + b^2)t)$ can be rewritten using $\\lambda_{\\mathrm{ms}}$ as:\n$$\n\\mathbb{E}\\bigl[|X_{t}|^2\\bigr] = |X_{0}|^2 \\exp(\\lambda_{\\mathrm{ms}} t)\n$$\nExponential mean-square decay means that $\\mathbb{E}\\bigl[|X_{t}|^2\\bigr] \\to 0$ as $t \\to \\infty$. This occurs if and only if the coefficient in the exponent of time is negative, i.e., $\\lambda_{\\mathrm{ms}}  0$. Thus, the sign of the mean-square growth rate $\\lambda_{\\mathrm{ms}}$ determines the mean-square stability of the trivial solution $X_t=0$. If $\\lambda_{\\mathrm{ms}}  0$, the system is exponentially stable in the mean-square sense. If $\\lambda_{\\mathrm{ms}}  0$, it is unstable.\n\nThe continuous dependence of $\\mathbb{E}\\bigl[|X_t|^2\\bigr]$ on $X_0$ is explicitly given by the same formula. For any fixed time $t > 0$, $\\mathbb{E}\\bigl[|X_t|^2\\bigr]$ is a continuous function of the initial condition $X_0$. Specifically, it is directly proportional to $|X_0|^2$. If we consider two solutions with initial conditions $X_0^{(1)}$ and $X_0^{(2)}$, the difference in their mean-square values is given by:\n$$\n\\left|\\mathbb{E}\\bigl[|X_t^{(1)}|^2\\bigr] - \\mathbb{E}\\bigl[|X_t^{(2)}|^2\\bigr]\\right| = \\left||X_0^{(1)}|^2 - |X_0^{(2)}|^2\\right| \\exp(\\lambda_{\\mathrm{ms}} t)\n$$\nThis demonstrates that the distance between the second moments of two solutions depends continuously on the distance between their initial second moments, with the time evolution of this dependence governed by the factor $\\exp(\\lambda_{\\mathrm{ms}} t)$.", "answer": "$$\n\\boxed{\\begin{pmatrix} a - \\frac{1}{2}b^{2}  2a + b^{2} \\end{pmatrix}}\n$$", "id": "2996038"}, {"introduction": "Continuous dependence on initial data is a cornerstone of well-posed dynamical systems, but how does this concept translate to SDEs? This problem investigates the distance between two solutions driven by the *same* noise process, a scenario that neatly isolates the influence of the system's drift from the stochastic dynamics. By analyzing a system with a marginally stable drift, you will see why common noise fails to induce asymptotic contraction, demonstrating that the distance between solutions is preserved indefinitely [@problem_id:2996037].", "problem": "Consider the linear Stochastic Differential Equation (SDE) in $\\mathbb{R}^{2}$ given by\n$$\n\\mathrm{d}X_{t} = A X_{t}\\,\\mathrm{d}t + \\sigma\\,\\mathrm{d}W_{t},\n$$\nwhere $A \\in \\mathbb{R}^{2 \\times 2}$ is the drift matrix, $\\sigma \\in \\mathbb{R}^{2 \\times 2}$ is a constant diffusion matrix, and $W_{t}$ is a standard two-dimensional Wiener process (Brownian motion). Let $A = \\omega J$, where $\\omega  0$ and $J = \\begin{pmatrix} 0  -1 \\\\ 1  0 \\end{pmatrix}$, so that the eigenvalues of $A$ are purely imaginary. For deterministic initial conditions $x, y \\in \\mathbb{R}^{2}$, let $X_{t}^{x}$ and $X_{t}^{y}$ denote strong solutions constructed on the same filtered probability space and driven by the same Brownian path $W_{t}$.\n\nStarting only from the fundamental definitions of a strong solution, the Itô integral, and the matrix exponential, and using well-tested facts about skew-symmetric generators in $\\mathbb{R}^{2}$, derive the exact expression for $\\mathbb{E}\\!\\left[\\,|X_{t}^{x} - X_{t}^{y}|^{2}\\,\\right]$ and determine its limit as $t \\to \\infty$. In particular, explain how this example with marginally stable $A$ illustrates the lack of asymptotic contraction in $X_{t}^{x} - X_{t}^{y}$ when both solutions are driven by the same noise realization.\n\nYour final answer must be the single closed-form expression for\n$$\n\\lim_{t \\to \\infty} \\mathbb{E}\\!\\left[\\,|X_{t}^{x} - X_{t}^{y}|^{2}\\,\\right].\n$$\nNo rounding is required.", "solution": "The problem asks for the long-term behavior of the mean-squared difference between two solutions of a linear Stochastic Differential Equation (SDE) driven by the same noise source.\n\nLet the two strong solutions be $X_{t}^{x}$ and $X_{t}^{y}$, with deterministic initial conditions $X_{0}^{x} = x \\in \\mathbb{R}^{2}$ and $X_{0}^{y} = y \\in \\mathbb{R}^{2}$. The SDE is given by:\n$$\n\\mathrm{d}X_{t} = A X_{t}\\,\\mathrm{d}t + \\sigma\\,\\mathrm{d}W_{t}\n$$\nwhere $A = \\omega J$ with $\\omega  0$ and $J = \\begin{pmatrix} 0  -1 \\\\ 1  0 \\end{pmatrix}$. The diffusion matrix $\\sigma$ is a constant $2 \\times 2$ matrix, and $W_{t}$ is a standard $2$-dimensional Wiener process. Crucially, both solutions are driven by the same realization of $W_{t}$.\n\nWe begin by defining the difference process $Z_{t} = X_{t}^{x} - X_{t}^{y}$. By the linearity of the drift and diffusion operators, we can write the differential for $Z_{t}$ as:\n$$\n\\mathrm{d}Z_{t} = \\mathrm{d}(X_{t}^{x} - X_{t}^{y}) = \\mathrm{d}X_{t}^{x} - \\mathrm{d}X_{t}^{y}\n$$\nSubstituting the SDE expression for each process:\n$$\n\\mathrm{d}Z_{t} = (A X_{t}^{x}\\,\\mathrm{d}t + \\sigma\\,\\mathrm{d}W_{t}) - (A X_{t}^{y}\\,\\mathrm{d}t + \\sigma\\,\\mathrm{d}W_{t})\n$$\nSince both solutions are driven by the same Wiener process $W_{t}$, the stochastic terms $\\sigma\\,\\mathrm{d}W_{t}$ are identical and cancel out. This is the central simplification that arises from the common noise assumption.\n$$\n\\mathrm{d}Z_{t} = A X_{t}^{x}\\,\\mathrm{d}t - A X_{t}^{y}\\,\\mathrm{d}t = A (X_{t}^{x} - X_{t}^{y})\\,\\mathrm{d}t = A Z_{t}\\,\\mathrm{d}t\n$$\nThe resulting equation for the difference process $Z_{t}$ is a deterministic, linear ordinary differential equation (ODE):\n$$\n\\frac{\\mathrm{d}Z_{t}}{\\mathrm{d}t} = A Z_{t}\n$$\nThe initial condition for this ODE is $Z_{0} = X_{0}^{x} - X_{0}^{y} = x - y$.\n\nThe solution to this linear system is given by the matrix exponential:\n$$\nZ_{t} = \\exp(At) Z_{0} = \\exp(At) (x - y)\n$$\nTherefore, the difference between the two solutions at any time $t$ is a deterministic quantity, $X_{t}^{x} - X_{t}^{y} = \\exp(At)(x-y)$.\n\nWe are asked to find the expected value of the squared Euclidean norm of this difference, $\\mathbb{E}\\!\\left[\\,|X_{t}^{x} - X_{t}^{y}|^{2}\\,\\right]$. Since the difference is deterministic, the expectation operator is superfluous.\n$$\n\\mathbb{E}\\!\\left[\\,|X_{t}^{x} - X_{t}^{y}|^{2}\\,\\right] = |X_{t}^{x} - X_{t}^{y}|^{2} = |Z_{t}|^{2} = |\\exp(At)(x-y)|^{2}\n$$\nTo evaluate this expression, we analyze the properties of the matrix $A$ and its exponential, $\\exp(At)$. The matrix $A$ is given by $A = \\omega J$, where $J = \\begin{pmatrix} 0  -1 \\\\ 1  0 \\end{pmatrix}$. The transpose of $J$ is $J^{T} = \\begin{pmatrix} 0  1 \\\\ -1  0 \\end{pmatrix} = -J$. A matrix for which $M^{T} = -M$ is called skew-symmetric.\nThus, the matrix $A$ is also skew-symmetric:\n$$\nA^{T} = (\\omega J)^{T} = \\omega J^{T} = -\\omega J = -A\n$$\nA well-established property of matrix exponentials is that if a matrix $M$ is skew-symmetric, then its exponential, $\\exp(M)$, is an orthogonal matrix. To verify this, we show that $(\\exp(At))^{T}\\exp(At) = I$, where $I$ is the identity matrix.\n$$\n(\\exp(At))^{T} = \\exp(A^{T}t) = \\exp(-At)\n$$\nSince $At$ and $-At$ commute, we have $\\exp(At)\\exp(-At) = \\exp(At - At) = \\exp(0) = I$. Therefore:\n$$\n(\\exp(At))^{T}\\exp(At) = \\exp(-At)\\exp(At) = I\n$$\nThis confirms that $\\exp(At)$ is an orthogonal matrix for all $t \\geq 0$. Geometrically, in $\\mathbb{R}^{2}$, these are rotation matrices.\n\nOrthogonal matrices preserve the Euclidean norm of vectors. Let $R(t) = \\exp(At)$ and $v = x-y$. Then:\n$$\n|R(t)v|^{2} = (R(t)v)^{T}(R(t)v) = v^{T}R(t)^{T}R(t)v = v^{T}Iv = v^{T}v = |v|^{2}\n$$\nApplying this to our expression for $|Z_{t}|^{2}$:\n$$\n|\\exp(At)(x-y)|^{2} = |x-y|^{2}\n$$\nThis demonstrates that the squared distance between the two trajectories $X_{t}^{x}$ and $X_{t}^{y}$ remains constant for all time $t \\geq 0$ and is equal to the squared initial distance.\n$$\n\\mathbb{E}\\!\\left[\\,|X_{t}^{x} - X_{t}^{y}|^{2}\\,\\right] = |x-y|^{2}\n$$\nThe problem asks for the limit of this expression as $t \\to \\infty$. Since the expression is a constant with respect to $t$, the limit is simply this constant value.\n$$\n\\lim_{t \\to \\infty} \\mathbb{E}\\!\\left[\\,|X_{t}^{x} - X_{t}^{y}|^{2}\\,\\right] = |x-y|^{2}\n$$\nThis result illustrates the lack of asymptotic contraction. The matrix $A$ has purely imaginary eigenvalues $\\pm i\\omega$, making the deterministic system $z' = Az$ marginally stable, not asymptotically stable. The trajectories neither converge nor diverge; they rotate at a constant distance from the origin. Because the noise is common to both SDEs, it does not influence the separation between solutions. The evolution of the difference $Z_t$ is governed solely by the deterministic, distance-preserving dynamics of $A$. Consequently, the initial separation $|x-y|$ is perfectly maintained for all time, and the distance between solutions does not decay to zero unless they were identical to begin with ($x=y$).", "answer": "$$\n\\boxed{|x-y|^{2}}\n$$", "id": "2996037"}, {"introduction": "To generalize stability analysis to high-dimensional systems, we must study how infinitesimal perturbations evolve, which is captured by the Jacobian of the stochastic flow. This practice guides you through deriving and solving the matrix SDE for the Jacobian flow in a tractable, linear setting [@problem_id:2996024]. Computing the growth of the Jacobian's norm provides a direct measure of the system's sensitivity and allows for the calculation of Lyapunov exponents, offering a powerful framework for characterizing stability in general SDEs.", "problem": "Consider the linear stochastic differential equation in $\\mathbb{R}^{n}$ driven by an $m$-dimensional standard Brownian motion $(W_{t}^{1},\\dots,W_{t}^{m})$,\n$$\n\\mathrm{d}X_{t}^{x} \\;=\\; A\\,X_{t}^{x}\\,\\mathrm{d}t \\;+\\; \\sum_{k=1}^{m} B_{k}\\,X_{t}^{x}\\,\\mathrm{d}W_{t}^{k}, \n\\qquad X_{0}^{x}=x\\in\\mathbb{R}^{n},\n$$\nwhere $A\\in\\mathbb{R}^{n\\times n}$ and $B_{1},\\dots,B_{m}\\in\\mathbb{R}^{n\\times n}$ are constant matrices. Assume:\n- $A$ and each $B_{k}$ are real symmetric,\n- the family $\\{A,B_{1},\\dots,B_{m}\\}$ is pairwise commuting, so they are simultaneously diagonalizable by an orthogonal matrix.\n\nLet $\\varphi_{t}:\\mathbb{R}^{n}\\to\\mathbb{R}^{n}$ denote the stochastic flow associated with the equation, so that $X_{t}^{x}=\\varphi_{t}(x)$. Let $J_{t}^{x}=D\\varphi_{t}(x)\\in\\mathbb{R}^{n\\times n}$ be the derivative (Jacobian) flow. Work from first principles: use the definition of the derivative flow and Itô calculus to derive the matrix stochastic differential equation satisfied by $J_{t}^{x}$, solve it under the stated commutativity assumptions, and compute the second moment of its Frobenius norm. More precisely:\n\n1. Starting from the variational definition $J_{t}^{x} h = \\lim_{\\varepsilon\\to 0} \\varepsilon^{-1}\\big(\\varphi_{t}(x+\\varepsilon h)-\\varphi_{t}(x)\\big)$, show that $J_{t}^{x}$ is the unique solution to\n$$\n\\mathrm{d}J_{t}^{x} \\;=\\; A\\,J_{t}^{x}\\,\\mathrm{d}t \\;+\\; \\sum_{k=1}^{m} B_{k}\\,J_{t}^{x}\\,\\mathrm{d}W_{t}^{k}, \n\\qquad J_{0}^{x}=I_{n}.\n$$\nExplain why, in this linear constant-coefficient setting, $J_{t}^{x}$ does not depend on $x$.\n\n2. Using only Itô calculus, orthogonal diagonalization, and properties of the normal distribution, compute the exact closed-form expression for $\\mathbb{E}\\!\\left[\\|J_{t}^{x}\\|_{F}^{2}\\right]$, where $\\|\\cdot\\|_{F}$ denotes the Frobenius norm. Your expression must be in terms of the spectral data of $A$ and $B_{k}$. Concretely, if $Q\\in\\mathbb{R}^{n\\times n}$ is orthogonal with $Q^{\\top}AQ=\\mathrm{diag}(\\alpha_{1},\\dots,\\alpha_{n})$ and $Q^{\\top}B_{k}Q=\\mathrm{diag}(\\beta_{k,1},\\dots,\\beta_{k,n})$ for $k=1,\\dots,m$, express $\\mathbb{E}\\!\\left[\\|J_{t}^{x}\\|_{F}^{2}\\right]$ solely in terms of $\\alpha_{i}$ and $\\beta_{k,i}$.\n\n3. Deduce from your computation an exponential-in-time bound of the form\n$$\n\\mathbb{E}\\!\\left[\\|J_{t}^{x}\\|_{\\mathrm{op}}^{2}\\right] \\;\\le\\; C\\,\\exp(\\gamma\\,t),\n$$\nfor explicit $C0$ and the smallest exponent $\\gamma$ that can be identified directly from the spectra $\\{\\alpha_{i}\\}_{i=1}^{n}$ and $\\left\\{\\sum_{k=1}^{m}\\beta_{k,i}^{2}\\right\\}_{i=1}^{n}$. Here $\\|\\cdot\\|_{\\mathrm{op}}$ denotes the operator norm. Justify your choice of $C$ and $\\gamma$.\n\nYour final reported answer must be the exact analytic expression found in part 2 for $\\mathbb{E}\\!\\left[\\|J_{t}^{x}\\|_{F}^{2}\\right]$. No rounding is required and no units are involved.", "solution": "The problem is divided into three parts. We will address them in sequence.\n\nPart 1: Derivation of the SDE for the Jacobian flow $J_{t}^{x}$.\n\nThe stochastic flow $\\varphi_{t}(x) = X_{t}^{x}$ is the solution to the SDE:\n$$\n\\mathrm{d}X_{t}^{x} \\;=\\; A\\,X_{t}^{x}\\,\\mathrm{d}t \\;+\\; \\sum_{k=1}^{m} B_{k}\\,X_{t}^{x}\\,\\mathrm{d}W_{t}^{k}, \\qquad X_{0}^{x}=x.\n$$\nThe Jacobian flow $J_{t}^{x} = D\\varphi_{t}(x)$ is defined by its action on a vector $h \\in \\mathbb{R}^{n}$:\n$$\nJ_{t}^{x} h = \\lim_{\\varepsilon\\to 0} \\frac{\\varphi_{t}(x+\\varepsilon h) - \\varphi_{t}(x)}{\\varepsilon} = \\lim_{\\varepsilon\\to 0} \\frac{X_{t}^{x+\\varepsilon h} - X_{t}^{x}}{\\varepsilon}.\n$$\nLet us define the variational process $Y_{t}^{\\varepsilon}(h) = \\frac{1}{\\varepsilon}(X_{t}^{x+\\varepsilon h} - X_{t}^{x})$. The SDEs for $X_{t}^{x}$ and $X_{t}^{x+\\varepsilon h}$ are linear. By linearity of the Itô integral, the difference $X_{t}^{x+\\varepsilon h} - X_{t}^{x}$ satisfies:\n$$\n\\mathrm{d}(X_{t}^{x+\\varepsilon h} - X_{t}^{x}) = A(X_{t}^{x+\\varepsilon h} - X_{t}^{x})\\,\\mathrm{d}t + \\sum_{k=1}^{m} B_{k}(X_{t}^{x+\\varepsilon h} - X_{t}^{x})\\,\\mathrm{d}W_{t}^{k}.\n$$\nDividing by $\\varepsilon$, we obtain the SDE for $Y_{t}^{\\varepsilon}(h)$:\n$$\n\\mathrm{d}Y_{t}^{\\varepsilon}(h) = A\\,Y_{t}^{\\varepsilon}(h)\\,\\mathrm{d}t + \\sum_{k=1}^{m} B_{k}\\,Y_{t}^{\\varepsilon}(h)\\,\\mathrm{d}W_{t}^{k}.\n$$\nThe initial condition is $Y_{0}^{\\varepsilon}(h) = \\frac{1}{\\varepsilon}(X_{0}^{x+\\varepsilon h} - X_{0}^{x}) = \\frac{1}{\\varepsilon}((x+\\varepsilon h) - x) = h$.\nThe coefficients $A$ and $B_k$ in this linear SDE are constant, and the initial condition $h$ does not depend on $\\varepsilon$. Therefore, the solution $Y_{t}^{\\varepsilon}(h)$ is independent of $\\varepsilon$. Let us denote this solution by $Y_{t}(h)$.\nBy definition, $J_{t}^{x}h = \\lim_{\\varepsilon\\to 0} Y_{t}^{\\varepsilon}(h) = Y_{t}(h)$.\nThus, for any vector $h \\in \\mathbb{R}^{n}$, the process $v_t = J_t^x h$ satisfies the SDE:\n$$\n\\mathrm{d}v_{t} = A\\,v_{t}\\,\\mathrm{d}t + \\sum_{k=1}^{m} B_{k}\\,v_{t}\\,\\mathrm{d}W_{t}^{k}, \\qquad v_{0}=h.\n$$\nThis can be written in terms of the matrix $J_{t}^{x}$ itself. Acting on the standard basis vectors $e_j$ for $j=1,\\dots,n$, the $j$-th column of $J_t^x$, which is $(J_t^x)_j = J_t^x e_j$, must satisfy the SDE. Assembling the columns back into a matrix, we get the matrix SDE:\n$$\n\\mathrm{d}J_{t}^{x} \\;=\\; A\\,J_{t}^{x}\\,\\mathrm{d}t \\;+\\; \\sum_{k=1}^{m} B_{k}\\,J_{t}^{x}\\,\\mathrm{d}W_{t}^{k}.\n$$\nThe initial condition is determined by $J_{0}^{x}h = h$ for all $h \\in \\mathbb{R}^{n}$, which implies $J_{0}^{x} = I_{n}$, the $n \\times n$ identity matrix.\n\nThe SDE for $J_{t}^{x}$ has coefficients $A$ and $B_k$ which are constant matrices. They do not depend on the state $X_t^x$ or the initial position $x$. The initial condition $J_{0}^{x}=I_{n}$ is also independent of $x$. Since the solution to this linear matrix SDE is unique, it cannot depend on $x$. We can therefore write $J_{t}$ instead of $J_{t}^{x}$.\n\nPart 2: Computation of $\\mathbb{E}\\!\\left[\\|J_{t}\\|_{F}^{2}\\right]$.\n\nThe matrices $A$ and $B_k$ for $k=1,\\dots,m$ are symmetric and pairwise commuting. By a standard theorem in linear algebra, they are simultaneously diagonalizable by a single orthogonal matrix $Q \\in \\mathbb{R}^{n\\times n}$. Let the diagonal forms be:\n$$\n\\Lambda_{A} = Q^{\\top}AQ = \\mathrm{diag}(\\alpha_{1},\\dots,\\alpha_{n}),\n$$\n$$\n\\Lambda_{B_{k}} = Q^{\\top}B_{k}Q = \\mathrm{diag}(\\beta_{k,1},\\dots,\\beta_{k,n}) \\quad \\text{for } k=1,\\dots,m.\n$$\nLet's transform the SDE for $J_t$ by defining $\\tilde{J}_{t} = Q^{\\top}J_{t}Q$. Since $Q$ is a constant orthogonal matrix, $\\mathrm{d}\\tilde{J}_{t} = Q^{\\top}(\\mathrm{d}J_{t})Q$.\n$$\n\\mathrm{d}\\tilde{J}_{t} = Q^{\\top}\\left(A\\,J_{t}\\,\\mathrm{d}t \\;+\\; \\sum_{k=1}^{m} B_{k}\\,J_{t}\\,\\mathrm{d}W_{t}^{k}\\right)Q.\n$$\nInserting $I_n = QQ^\\top$ identities:\n$$\n\\mathrm{d}\\tilde{J}_{t} = (Q^{\\top}AQ)(Q^{\\top}J_{t}Q)\\,\\mathrm{d}t \\;+\\; \\sum_{k=1}^{m} (Q^{\\top}B_{k}Q)(Q^{\\top}J_{t}Q)\\,\\mathrm{d}W_{t}^{k}.\n$$\nThis yields the SDE for $\\tilde{J}_{t}$:\n$$\n\\mathrm{d}\\tilde{J}_{t} = \\Lambda_{A}\\tilde{J}_{t}\\,\\mathrm{d}t + \\sum_{k=1}^{m} \\Lambda_{B_{k}}\\tilde{J}_{t}\\,\\mathrm{d}W_{t}^{k}.\n$$\nThe initial condition is $\\tilde{J}_{0} = Q^{\\top}J_{0}Q = Q^{\\top}I_{n}Q = I_{n}$.\nLet $\\tilde{J}_{t} = [z_{ij}(t)]$. The matrix SDE decouples into $n^2$ scalar SDEs for the entries $z_{ij}(t)$.\nThe $(i,j)$-th entry of the equation is:\n$$\n\\mathrm{d}z_{ij}(t) = (\\Lambda_{A}\\tilde{J}_{t})_{ij}\\,\\mathrm{d}t + \\sum_{k=1}^{m} (\\Lambda_{B_{k}}\\tilde{J}_{t})_{ij}\\,\\mathrm{d}W_{t}^{k}.\n$$\nSince $\\Lambda_A$ and $\\Lambda_{B_k}$ are diagonal, $(\\Lambda_{A}\\tilde{J}_{t})_{ij} = \\alpha_i z_{ij}(t)$ and $(\\Lambda_{B_{k}}\\tilde{J}_{t})_{ij} = \\beta_{k,i} z_{ij}(t)$.\nSo, for each pair $(i,j)$, the SDE for $z_{ij}(t)$ is:\n$$\n\\mathrm{d}z_{ij}(t) = \\alpha_{i}z_{ij}(t)\\,\\mathrm{d}t + \\sum_{k=1}^{m} \\beta_{k,i}z_{ij}(t)\\,\\mathrm{d}W_{t}^{k} = \\alpha_{i}z_{ij}(t)\\,\\mathrm{d}t + z_{ij}(t) \\sum_{k=1}^{m} \\beta_{k,i}\\mathrm{d}W_{t}^{k}.\n$$\nThe initial conditions are $z_{ij}(0) = (I_n)_{ij} = \\delta_{ij}$.\nFor $i\\neq j$, $z_{ij}(0) = 0$, and since the SDE is linear and homogeneous, $z_{ij}(t)=0$ for all $t \\ge 0$.\nFor $i=j$, $z_{ii}(0) = 1$. The SDE is a scalar geometric Brownian motion. Its solution is:\n$$\nz_{ii}(t) = z_{ii}(0)\\exp\\left(\\left(\\alpha_{i} - \\frac{1}{2}\\sum_{k=1}^{m}\\beta_{k,i}^{2}\\right)t + \\sum_{k=1}^{m}\\beta_{k,i}W_{t}^{k}\\right).\n$$\nThus, $\\tilde{J}_{t}$ is a diagonal matrix with entries $z_{ii}(t)$ on its diagonal.\nThe Frobenius norm is invariant under orthogonal transformations, so $\\|J_{t}\\|_{F}^{2} = \\|\\tilde{J}_{t}\\|_{F}^{2}$.\n$$\n\\|J_{t}\\|_{F}^{2} = \\|\\tilde{J}_{t}\\|_{F}^{2} = \\sum_{i=1}^{n}\\sum_{j=1}^{n} |z_{ij}(t)|^2 = \\sum_{i=1}^{n} |z_{ii}(t)|^2 = \\sum_{i=1}^{n} (z_{ii}(t))^2.\n$$\nWe need to compute $\\mathbb{E}[\\|J_{t}\\|_{F}^{2}] = \\sum_{i=1}^{n}\\mathbb{E}[(z_{ii}(t))^2]$.\nLet's compute $\\mathbb{E}[(z_{ii}(t))^2]$:\n$$\n(z_{ii}(t))^2 = \\exp\\left(\\left(2\\alpha_{i} - \\sum_{k=1}^{m}\\beta_{k,i}^{2}\\right)t + 2\\sum_{k=1}^{m}\\beta_{k,i}W_{t}^{k}\\right).\n$$\nLet $Y_{t} = \\sum_{k=1}^{m}2\\beta_{k,i}W_{t}^{k}$. Since $W_{t}^{k}$ are independent standard Brownian motions, $W_{t}^{k} \\sim N(0, t)$. The variable $Y_t$ is a normally distributed random variable with mean $\\mathbb{E}[Y_t] = 0$ and variance $\\mathrm{Var}(Y_t) = \\sum_{k=1}^{m} (2\\beta_{k,i})^2 \\mathrm{Var}(W_t^k) = 4t\\sum_{k=1}^{m}\\beta_{k,i}^{2}$.\nSo, $Y_t \\sim N(0, 4t\\sum_{k=1}^{m}\\beta_{k,i}^{2})$.\nThe expectation $\\mathbb{E}[\\exp(Y_t)]$ is the value of the moment generating function of $Y_t$ at $s=1$, which for $N(\\mu,\\sigma^2)$ is $\\exp(\\mu + \\sigma^2/2)$.\n$$\n\\mathbb{E}[\\exp(Y_t)] = \\exp\\left(0 + \\frac{1}{2}\\left(4t\\sum_{k=1}^{m}\\beta_{k,i}^{2}\\right)\\right) = \\exp\\left(2t\\sum_{k=1}^{m}\\beta_{k,i}^{2}\\right).\n$$\nNow, we can compute the expectation of $(z_{ii}(t))^2$:\n$$\n\\mathbb{E}[(z_{ii}(t))^2] = \\mathbb{E}\\left[\\exp\\left(\\left(2\\alpha_{i} - \\sum_{k=1}^{m}\\beta_{k,i}^{2}\\right)t + Y_t\\right)\\right] = \\exp\\left(\\left(2\\alpha_{i} - \\sum_{k=1}^{m}\\beta_{k,i}^{2}\\right)t\\right) \\mathbb{E}[\\exp(Y_t)]\n$$\n$$\n= \\exp\\left(\\left(2\\alpha_{i} - \\sum_{k=1}^{m}\\beta_{k,i}^{2}\\right)t\\right) \\exp\\left(2t\\sum_{k=1}^{m}\\beta_{k,i}^{2}\\right) = \\exp\\left(\\left(2\\alpha_{i} + \\sum_{k=1}^{m}\\beta_{k,i}^{2}\\right)t\\right).\n$$\nFinally, we sum over $i$ to get the expectation of the squared Frobenius norm:\n$$\n\\mathbb{E}\\!\\left[\\|J_{t}\\|_{F}^{2}\\right] = \\sum_{i=1}^{n}\\mathbb{E}[(z_{ii}(t))^2] = \\sum_{i=1}^{n} \\exp\\left(\\left(2\\alpha_i + \\sum_{k=1}^{m} \\beta_{k,i}^2\\right)t\\right).\n$$\nThis is the required closed-form expression.\n\nPart 3: Derivation of an exponential bound for $\\mathbb{E}\\!\\left[\\|J_{t}\\|_{\\mathrm{op}}^{2}\\right]$.\nThe operator norm $\\|\\cdot\\|_{\\mathrm{op}}$ and Frobenius norm $\\|\\cdot\\|_{F}$ are related by the inequality $\\|M\\|_{\\mathrm{op}}^{2} \\le \\|M\\|_{F}^{2}$ for any matrix $M$.\nTherefore, $\\mathbb{E}[\\|J_{t}\\|_{\\mathrm{op}}^{2}] \\le \\mathbb{E}[\\|J_{t}\\|_{F}^{2}]$.\nUsing the result from Part 2:\n$$\n\\mathbb{E}[\\|J_{t}\\|_{\\mathrm{op}}^{2}] \\le \\sum_{i=1}^{n} \\exp\\left(\\left(2\\alpha_i + \\sum_{k=1}^{m} \\beta_{k,i}^2\\right)t\\right).\n$$\nLet us define the Lyapunov exponents $\\gamma_i = 2\\alpha_i + \\sum_{k=1}^{m} \\beta_{k,i}^2$ for $i=1,\\dots,n$. Let $\\gamma = \\max_{i=1,\\dots,n} \\{\\gamma_i\\}$.\nFor any $i$, we have $\\gamma_i \\le \\gamma$, which implies $\\exp(\\gamma_i t) \\le \\exp(\\gamma t)$ for $t \\ge 0$.\nWe can bound the sum:\n$$\n\\sum_{i=1}^{n} \\exp(\\gamma_i t) \\le \\sum_{i=1}^{n} \\exp(\\gamma t) = n \\exp(\\gamma t).\n$$\nThis gives the bound $\\mathbb{E}[\\|J_{t}\\|_{\\mathrm{op}}^{2}] \\le n \\exp(\\gamma t)$.\nThis is of the form $C\\exp(\\gamma t)$ with $C=n$ and $\\gamma = \\max_{i=1,\\dots,n} \\left\\{2\\alpha_i + \\sum_{k=1}^m \\beta_{k,i}^2\\right\\}$.\n\nTo justify that this $\\gamma$ is the smallest possible exponent, we establish a lower bound. As shown in Part 2, $\\tilde{J}_{t}$ is a diagonal matrix. The operator norm of a diagonal matrix is the maximum of the absolute values of its diagonal entries.\n$$\n\\|J_{t}\\|_{\\mathrm{op}}^{2} = \\|\\tilde{J}_{t}\\|_{\\mathrm{op}}^{2} = \\left(\\max_{i=1,\\dots,n} |z_{ii}(t)|\\right)^2 = \\max_{i=1,\\dots,n} (z_{ii}(t))^2.\n$$\nLet $i^*$ be an index for which $\\gamma_{i^*} = \\gamma$. The expectation of the maximum is greater than or equal to the expectation of any single element:\n$$\n\\mathbb{E}[\\|J_{t}\\|_{\\mathrm{op}}^{2}] = \\mathbb{E}\\left[\\max_{i=1,\\dots,n} (z_{ii}(t))^2\\right] \\ge \\mathbb{E}[(z_{i^*i^*}(t))^2] = \\exp(\\gamma_{i^*} t) = \\exp(\\gamma t).\n$$\nWe have shown that for some constant $C_1=1$ and some other constant $C_2=n$,\n$$\nC_1 \\exp(\\gamma t) \\le \\mathbb{E}[\\|J_{t}\\|_{\\mathrm{op}}^{2}] \\le C_2 \\exp(\\gamma t).\n$$\nThis demonstrates that the exponential growth rate of $\\mathbb{E}[\\|J_{t}\\|_{\\mathrm{op}}^{2}]$ is exactly $\\gamma$. Thus, $\\gamma = \\max_{i=1,\\dots,n} \\left\\{2\\alpha_i + \\sum_{k=1}^m \\beta_{k,i}^2\\right\\}$ is the smallest possible exponent. Our choice of $C=n$ provides a valid upper bound for all $t\\ge 0$.", "answer": "$$\n\\boxed{\\sum_{i=1}^{n} \\exp\\left(\\left(2\\alpha_i + \\sum_{k=1}^{m} \\beta_{k,i}^2\\right)t\\right)}\n$$", "id": "2996024"}]}