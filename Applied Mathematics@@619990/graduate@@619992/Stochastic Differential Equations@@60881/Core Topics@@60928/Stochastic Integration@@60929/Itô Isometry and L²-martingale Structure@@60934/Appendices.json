{"hands_on_practices": [{"introduction": "The Ornstein-Uhlenbeck process is a fundamental model for mean-reverting systems across science and finance, describing phenomena from the velocity of a particle in a fluid to fluctuating interest rates. This exercise provides a concrete setting to apply the Itô isometry, a central tool in stochastic calculus that connects the geometry of $L^2$ spaces with the statistics of stochastic integrals [@problem_id:2982160]. By explicitly calculating the time-dependent and stationary variance, you will build foundational skills in analyzing the solutions to stochastic differential equations and understanding their long-term behavior.", "problem": "Consider the Ornstein–Uhlenbeck process $ \\{X_t\\}_{t \\ge 0} $ defined as the unique strong solution to the linear stochastic differential equation (SDE) $ dX_t = -\\theta X_t \\, dt + \\sigma \\, dW_t $, where $ \\theta > 0 $, $ \\sigma > 0 $, $ \\{W_t\\}_{t \\ge 0} $ is a standard Brownian motion on a filtered probability space $ (\\Omega, \\mathcal{F}, \\{\\mathcal{F}_t\\}_{t \\ge 0}, \\mathbb{P}) $ satisfying the usual conditions, and the initial random variable $ X_0 \\in L^{2}(\\Omega, \\mathcal{F}_0, \\mathbb{P}) $ is independent of $ \\{W_t\\}_{t \\ge 0} $. Work from first principles for linear SDEs, properties of the Itô integral as an $ L^{2} $-martingale, and the Itô isometry (do not quote any explicit formulae without justification).\n\nTasks:\n- Derive an explicit representation of $ X_t $ using an integrating factor, expressing it as the sum of a deterministic decay term and a stochastic convolution term driven by $ \\{W_t\\}_{t \\ge 0} $.\n- Using only the defining $ L^{2} $-martingale properties of the Itô integral and the Itô isometry, compute $ \\operatorname{Var}(X_t) $ in closed form in terms of $ \\theta $, $ \\sigma $, $ t $, $ \\mathbb{E}[X_0] $, and $ \\operatorname{Var}(X_0) $.\n- Determine the stationary variance as the limit of $ \\operatorname{Var}(X_t) $ as $ t \\to \\infty $.\n\nGive your final answer as a single closed-form analytic expression in terms of $ \\theta $ and $ \\sigma $. No units are required. The final answer must be a single expression (not an equation or inequality).", "solution": "The problem is well-posed, scientifically grounded, and contains all necessary information to derive a unique solution. The definitions and constraints are standard in the theory of stochastic differential equations. Thus, the problem is valid, and we may proceed to the solution.\n\nThe Ornstein-Uhlenbeck process is described by the linear stochastic differential equation (SDE):\n$$\ndX_t = -\\theta X_t \\, dt + \\sigma \\, dW_t\n$$\nwhere $\\theta > 0$, $\\sigma > 0$, $\\{W_t\\}_{t \\ge 0}$ is a standard Brownian motion, and $X_0 \\in L^{2}(\\Omega, \\mathcal{F}_0, \\mathbb{P})$ is independent of $\\{W_t\\}_{t \\ge 0}$.\n\n**Task 1: Explicit Representation of $X_t$**\n\nTo find an explicit representation for $X_t$, we use the method of integrating factors. We can rewrite the SDE as:\n$$\ndX_t + \\theta X_t \\, dt = \\sigma \\, dW_t\n$$\nLet us define an integrating factor $I_t = \\exp(\\theta t)$. We then consider the process $Y_t = I_t X_t = \\exp(\\theta t) X_t$. To find its differential, $dY_t$, we apply Itô's lemma to the function $f(t, x) = \\exp(\\theta t) x$. The partial derivatives are:\n$$\n\\frac{\\partial f}{\\partial t} = \\theta \\exp(\\theta t) x, \\quad \\frac{\\partial f}{\\partial x} = \\exp(\\theta t), \\quad \\frac{\\partial^2 f}{\\partial x^2} = 0\n$$\nItô's lemma states that $dY_t = \\frac{\\partial f}{\\partial t} dt + \\frac{\\partial f}{\\partial x} dX_t + \\frac{1}{2} \\frac{\\partial^2 f}{\\partial x^2} (dX_t)^2$. The quadratic variation term $(dX_t)^2$ is $(\\sigma dW_t)^2 = \\sigma^2 dt$. However, since $\\frac{\\partial^2 f}{\\partial x^2} = 0$, this term vanishes. Applying the lemma, we get:\n$$\nd(\\exp(\\theta t) X_t) = (\\theta \\exp(\\theta t) X_t) \\, dt + \\exp(\\theta t) \\, dX_t\n$$\nSubstituting the SDE for $dX_t$:\n$$\nd(\\exp(\\theta t) X_t) = \\theta \\exp(\\theta t) X_t \\, dt + \\exp(\\theta t) (-\\theta X_t \\, dt + \\sigma \\, dW_t)\n$$\nThe terms involving $dt$ cancel out:\n$$\nd(\\exp(\\theta t) X_t) = \\theta \\exp(\\theta t) X_t \\, dt - \\theta \\exp(\\theta t) X_t \\, dt + \\sigma \\exp(\\theta t) \\, dW_t\n$$\n$$\nd(\\exp(\\theta t) X_t) = \\sigma \\exp(\\theta t) \\, dW_t\n$$\nIntegrating both sides from $s=0$ to $s=t$:\n$$\n\\int_0^t d(\\exp(\\theta s) X_s) = \\int_0^t \\sigma \\exp(\\theta s) \\, dW_s\n$$\n$$\n\\exp(\\theta t) X_t - \\exp(\\theta \\cdot 0) X_0 = \\sigma \\int_0^t \\exp(\\theta s) \\, dW_s\n$$\n$$\n\\exp(\\theta t) X_t = X_0 + \\sigma \\int_0^t \\exp(\\theta s) \\, dW_s\n$$\nSolving for $X_t$ by multiplying by $\\exp(-\\theta t)$:\n$$\nX_t = \\exp(-\\theta t) X_0 + \\sigma \\exp(-\\theta t) \\int_0^t \\exp(\\theta s) \\, dW_s\n$$\nWe can rewrite the integral to make the dependence on $t$ more explicit in the integrand:\n$$\nX_t = \\exp(-\\theta t) X_0 + \\sigma \\int_0^t \\exp(-\\theta(t-s)) \\, dW_s\n$$\nThis is the explicit solution. The term $\\exp(-\\theta t) X_0$ represents the deterministic decay of the initial state, while the term $\\sigma \\int_0^t \\exp(-\\theta(t-s)) \\, dW_s$ is a stochastic convolution term representing the accumulated effect of the random noise.\n\n**Task 2: Computation of $\\operatorname{Var}(X_t)$**\n\nThe variance of $X_t$ is defined as $\\operatorname{Var}(X_t) = \\mathbb{E}[X_t^2] - (\\mathbb{E}[X_t])^2$.\n\nFirst, we compute the expectation $\\mathbb{E}[X_t]$. Using the linearity of expectation on the explicit solution for $X_t$:\n$$\n\\mathbb{E}[X_t] = \\mathbb{E}\\left[\\exp(-\\theta t) X_0 + \\sigma \\int_0^t \\exp(-\\theta(t-s)) \\, dW_s\\right]\n$$\n$$\n\\mathbb{E}[X_t] = \\exp(-\\theta t) \\mathbb{E}[X_0] + \\sigma \\mathbb{E}\\left[\\int_0^t \\exp(-\\theta(t-s)) \\, dW_s\\right]\n$$\nThe integrand $h(s) = \\exp(-\\theta(t-s))$ is a deterministic function and is square-integrable on $[0, t]$. The Itô integral $\\int_0^t h(s) dW_s$ is a martingale starting at $0$. A fundamental property of such martingales is that their expectation is zero. Therefore:\n$$\n\\mathbb{E}\\left[\\int_0^t \\exp(-\\theta(t-s)) \\, dW_s\\right] = 0\n$$\nThis gives the mean of the process:\n$$\n\\mathbb{E}[X_t] = \\exp(-\\theta t) \\mathbb{E}[X_0]\n$$\nNext, we compute the second moment $\\mathbb{E}[X_t^2]$:\n$$\nX_t^2 = \\left(\\exp(-\\theta t) X_0 + \\sigma \\int_0^t \\exp(-\\theta(t-s)) \\, dW_s\\right)^2\n$$\n$$\nX_t^2 = \\exp(-2\\theta t) X_0^2 + 2 \\sigma \\exp(-\\theta t) X_0 \\int_0^t \\exp(-\\theta(t-s)) \\, dW_s + \\sigma^2 \\left(\\int_0^t \\exp(-\\theta(t-s)) \\, dW_s\\right)^2\n$$\nTaking the expectation of each term:\n$$\n\\mathbb{E}[X_t^2] = \\mathbb{E}[\\exp(-2\\theta t) X_0^2] + \\mathbb{E}\\left[2 \\sigma \\exp(-\\theta t) X_0 \\int_0^t \\exp(-\\theta(t-s)) \\, dW_s\\right] + \\mathbb{E}\\left[\\sigma^2 \\left(\\int_0^t \\exp(-\\theta(t-s)) \\, dW_s\\right)^2\\right]\n$$\nThe first term is $\\exp(-2\\theta t) \\mathbb{E}[X_0^2]$.\nFor the second (cross) term, we use the independence of $X_0$ (which is $\\mathcal{F}_0$-measurable) and the Itô integral (which depends on $\\{W_s\\}_{s \\in (0,t]}$).\n$$\n\\mathbb{E}\\left[X_0 \\int_0^t \\exp(-\\theta(t-s)) \\, dW_s\\right] = \\mathbb{E}[X_0] \\, \\mathbb{E}\\left[\\int_0^t \\exp(-\\theta(t-s)) \\, dW_s\\right] = \\mathbb{E}[X_0] \\cdot 0 = 0\n$$\nThus, the cross term vanishes.\nFor the third term, we use the Itô isometry. For a deterministic, square-integrable function $h(s)$, the isometry states:\n$$\n\\mathbb{E}\\left[\\left(\\int_0^t h(s) \\, dW_s\\right)^2\\right] = \\int_0^t h(s)^2 \\, ds\n$$\nIn our case, $h(s) = \\exp(-\\theta(t-s))$. Applying the isometry:\n$$\n\\mathbb{E}\\left[\\left(\\int_0^t \\exp(-\\theta(t-s)) \\, dW_s\\right)^2\\right] = \\int_0^t (\\exp(-\\theta(t-s)))^2 \\, ds = \\int_0^t \\exp(-2\\theta(t-s)) \\, ds\n$$\nWe evaluate this integral:\n$$\n\\int_0^t \\exp(-2\\theta t) \\exp(2\\theta s) \\, ds = \\exp(-2\\theta t) \\int_0^t \\exp(2\\theta s) \\, ds = \\exp(-2\\theta t) \\left[\\frac{1}{2\\theta} \\exp(2\\theta s)\\right]_0^t\n$$\n$$\n= \\exp(-2\\theta t) \\left( \\frac{\\exp(2\\theta t)}{2\\theta} - \\frac{\\exp(0)}{2\\theta} \\right) = \\frac{1}{2\\theta} \\exp(-2\\theta t) (\\exp(2\\theta t) - 1) = \\frac{1 - \\exp(-2\\theta t)}{2\\theta}\n$$\nSo, the third term of $\\mathbb{E}[X_t^2]$ is $\\sigma^2 \\frac{1 - \\exp(-2\\theta t)}{2\\theta}$.\nCombining the terms for $\\mathbb{E}[X_t^2]$:\n$$\n\\mathbb{E}[X_t^2] = \\exp(-2\\theta t) \\mathbb{E}[X_0^2] + \\frac{\\sigma^2}{2\\theta} (1 - \\exp(-2\\theta t))\n$$\nNow we compute the variance:\n$$\n\\operatorname{Var}(X_t) = \\mathbb{E}[X_t^2] - (\\mathbb{E}[X_t])^2\n$$\n$$\n\\operatorname{Var}(X_t) = \\left(\\exp(-2\\theta t) \\mathbb{E}[X_0^2] + \\frac{\\sigma^2}{2\\theta} (1 - \\exp(-2\\theta t))\\right) - (\\exp(-\\theta t) \\mathbb{E}[X_0])^2\n$$\n$$\n\\operatorname{Var}(X_t) = \\exp(-2\\theta t) \\mathbb{E}[X_0^2] - \\exp(-2\\theta t) (\\mathbb{E}[X_0])^2 + \\frac{\\sigma^2}{2\\theta} (1 - \\exp(-2\\theta t))\n$$\nRecognizing that $\\operatorname{Var}(X_0) = \\mathbb{E}[X_0^2] - (\\mathbb{E}[X_0])^2$:\n$$\n\\operatorname{Var}(X_t) = \\exp(-2\\theta t) \\operatorname{Var}(X_0) + \\frac{\\sigma^2}{2\\theta} (1 - \\exp(-2\\theta t))\n$$\nThis is the closed-form expression for $\\operatorname{Var}(X_t)$.\n\n**Task 3: Stationary Variance**\n\nThe stationary variance is the limit of $\\operatorname{Var}(X_t)$ as $t \\to \\infty$.\n$$\n\\lim_{t \\to \\infty} \\operatorname{Var}(X_t) = \\lim_{t \\to \\infty} \\left( \\exp(-2\\theta t) \\operatorname{Var}(X_0) + \\frac{\\sigma^2}{2\\theta} (1 - \\exp(-2\\theta t)) \\right)\n$$\nSince $\\theta > 0$, we have $-2\\theta < 0$. As $t \\to \\infty$, the term $\\exp(-2\\theta t) \\to 0$.\nTherefore, the limit becomes:\n$$\n\\lim_{t \\to \\infty} \\operatorname{Var}(X_t) = (0 \\cdot \\operatorname{Var}(X_0)) + \\frac{\\sigma^2}{2\\theta} (1 - 0) = \\frac{\\sigma^2}{2\\theta}\n$$\nThe stationary variance, which is the asymptotic variance of the process, is $\\frac{\\sigma^2}{2\\theta}$.", "answer": "$$\n\\boxed{\\frac{\\sigma^2}{2\\theta}}\n$$", "id": "2982160"}, {"introduction": "Stochastic convolution integrals are a powerful tool for representing solutions to a wide class of linear stochastic differential equations, particularly those arising from stochastic partial differential equations and control theory. This problem challenges you to apply the Itô isometry in a more general setting, using it to compute the variance of a process defined by an arbitrary deterministic kernel [@problem_id:2982185]. Mastering this calculation deepens your understanding of how the integral's kernel function shapes the statistical structure and mean-square continuity of the resulting stochastic process.", "problem": "Let $(\\Omega,\\mathcal{F},(\\mathcal{F}_{t})_{t \\geq 0},\\mathbb{P})$ be a filtered probability space supporting a one-dimensional standard Brownian motion $W=(W_{t})_{t \\geq 0}$ adapted to $(\\mathcal{F}_{t})_{t \\geq 0}$. Fix $t \\geq 0$ and let $h:[0,\\infty) \\to \\mathbb{R}$ be a deterministic function such that $h \\in L^{2}([0,T])$ for every $T>0$. Define the stochastic convolution\n$$\nY_{t}=\\int_{0}^{t} h(t-s)\\,dW_{s}.\n$$\nStarting from the definition of the Itô integral, the martingale property of stochastic integrals, and the $L^{2}$-structure of Brownian motion, derive an explicit closed-form expression for the variance $\\mathrm{Var}(Y_{t})$ in terms of $h$. In addition, by computing $\\mathbb{E}\\big[|Y_{t}-Y_{s}|^{2}\\big]$ for $s,t \\geq 0$, discuss (justify) the $L^{2}$-continuity of $(Y_{t})_{t \\geq 0}$ under the stated assumptions on $h$. Express the variance as a single analytic expression in $h$ and $t$. No numerical approximation is required.", "solution": "The problem asks for two main results concerning the stochastic process $Y_t = \\int_{0}^{t} h(t-s)\\,dW_{s}$: first, to derive an explicit closed-form expression for its variance, $\\mathrm{Var}(Y_{t})$; second, to compute $\\mathbb{E}\\big[|Y_{t}-Y_{s}|^{2}\\big]$ and use it to justify the $L^{2}$-continuity of the process $(Y_{t})_{t \\geq 0}$.\n\nFirst, we validate the problem. The problem statement is set within the standard mathematical framework of Itô calculus. All terms such as Brownian motion, filtered probability space, Itô integral, and $L^2$ spaces are well-defined. The condition that the deterministic function $h$ is in $L^2([0,T])$ for every $T>0$ ensures that the integrand $s \\mapsto h(t-s)$ is square-integrable on the interval $[0,t]$ for any fixed $t \\geq 0$. Specifically, $\\int_0^t (h(t-s))^2 ds = \\int_0^t h(u)^2 du$ after a change of variable $u=t-s$. This integral is finite because $h \\in L^2([0,t])$. This guarantees that the stochastic integral $Y_t$ is a well-defined random variable in $L^2(\\Omega)$. The problem is self-contained, scientifically grounded, and well-posed. We may proceed with the solution.\n\nPart 1: Derivation of the variance $\\mathrm{Var}(Y_{t})$\n\nThe variance of a random variable $X$ is defined as $\\mathrm{Var}(X) = \\mathbb{E}[(X - \\mathbb{E}[X])^2] = \\mathbb{E}[X^2] - (\\mathbb{E}[X])^2$. We first compute the expected value of $Y_t$.\n\nThe process $Y_t$ is defined by the Itô integral $Y_{t}=\\int_{0}^{t} h(t-s)\\,dW_{s}$. For a fixed $t$, the integrand $s \\mapsto h(t-s)$ is a deterministic function. A fundamental property of the Itô integral is that its expectation is zero if the integrand is a square-integrable deterministic function.\n$$\n\\mathbb{E}[Y_t] = \\mathbb{E}\\left[\\int_{0}^{t} h(t-s)\\,dW_{s}\\right] = 0.\n$$\nTherefore, the variance of $Y_t$ simplifies to its second moment:\n$$\n\\mathrm{Var}(Y_t) = \\mathbb{E}[Y_t^2] - (\\mathbb{E}[Y_t])^2 = \\mathbb{E}[Y_t^2] - 0^2 = \\mathbb{E}[Y_t^2].\n$$\nTo calculate $\\mathbb{E}[Y_t^2]$, we use the Itô isometry property. For a deterministic, square-integrable function $f:[0,t] \\to \\mathbb{R}$, the Itô isometry states:\n$$\n\\mathbb{E}\\left[\\left(\\int_{0}^{t} f(s)\\,dW_{s}\\right)^2\\right] = \\int_{0}^{t} f(s)^2\\,ds.\n$$\nIn our case, the integrand is $f(s) = h(t-s)$. Applying the Itô isometry, we get:\n$$\n\\mathrm{Var}(Y_t) = \\mathbb{E}[Y_t^2] = \\mathbb{E}\\left[\\left(\\int_{0}^{t} h(t-s)\\,dW_{s}\\right)^2\\right] = \\int_{0}^{t} (h(t-s))^2\\,ds.\n$$\nTo obtain the final expression in a simpler form, we perform a change of variables in the integral. Let $u = t-s$. Then $du = -ds$. The limits of integration change as follows: when $s=0$, $u=t$; when $s=t$, $u=0$.\n$$\n\\int_{0}^{t} (h(t-s))^2\\,ds = \\int_{t}^{0} h(u)^2\\,(-du) = \\int_{0}^{t} h(u)^2\\,du.\n$$\nThus, the variance of $Y_t$ is given by the explicit closed-form expression:\n$$\n\\mathrm{Var}(Y_t) = \\int_{0}^{t} h(u)^2\\,du.\n$$\n\nPart 2: $L^2$-continuity of $(Y_{t})_{t \\geq 0}$\n\nTo discuss the $L^2$-continuity (or mean-square continuity) of the process $Y$, we need to show that $\\lim_{s \\to t} \\mathbb{E}[|Y_t - Y_s|^2] = 0$. Let's compute $\\mathbb{E}[|Y_t - Y_s|^2]$. Without loss of generality, assume $0 \\leq s < t$.\n\nWe express the difference $Y_t - Y_s$ as:\n$$\nY_t - Y_s = \\int_0^t h(t-u)\\,dW_u - \\int_0^s h(s-u)\\,dW_u.\n$$\nTo handle the different integrands and integration limits, we split the first integral at time $s$:\n$$\n\\int_0^t h(t-u)\\,dW_u = \\int_0^s h(t-u)\\,dW_u + \\int_s^t h(t-u)\\,dW_u.\n$$\nSubstituting this back into the expression for the difference gives:\n$$\nY_t - Y_s = \\left(\\int_0^s h(t-u)\\,dW_u + \\int_s^t h(t-u)\\,dW_u\\right) - \\int_0^s h(s-u)\\,dW_u\n$$\n$$\nY_t - Y_s = \\int_0^s (h(t-u) - h(s-u))\\,dW_u + \\int_s^t h(t-u)\\,dW_u.\n$$\nLet $I_1 = \\int_0^s (h(t-u) - h(s-u))\\,dW_u$ and $I_2 = \\int_s^t h(t-u)\\,dW_u$.\nThen $\\mathbb{E}[|Y_t-Y_s|^2] = \\mathbb{E}[(I_1 + I_2)^2] = \\mathbb{E}[I_1^2] + \\mathbb{E}[I_2^2] + 2\\mathbb{E}[I_1 I_2]$.\n\nThe integrands for both $I_1$ and $I_2$ are deterministic. The integral $I_1$ is defined over the interval $[0,s]$, so it is $\\mathcal{F}_s$-measurable. The integral $I_2$ is defined over the interval $[s,t]$. Due to the independent increments property of Brownian motion, stochastic integrals over disjoint time intervals are uncorrelated. More formally, the cross-term is zero:\n$$\n\\mathbb{E}[I_1 I_2] = \\mathbb{E}\\left[\\mathbb{E}[I_1 I_2 | \\mathcal{F}_s]\\right] = \\mathbb{E}\\left[I_1 \\mathbb{E}[I_2 | \\mathcal{F}_s]\\right].\n$$\nSince $I_2$ is an integral over future increments of $W$ with respect to $\\mathcal{F}_s$, its conditional expectation is zero: $\\mathbb{E}[I_2 | \\mathcal{F}_s] = 0$. Therefore, $\\mathbb{E}[I_1 I_2] = 0$.\n\nNow, we can compute $\\mathbb{E}[|Y_t-Y_s|^2]$ by applying the Itô isometry to $I_1$ and $I_2$:\n$$\n\\mathbb{E}[|Y_t-Y_s|^2] = \\mathbb{E}[I_1^2] + \\mathbb{E}[I_2^2] = \\int_0^s (h(t-u) - h(s-u))^2\\,du + \\int_s^t (h(t-u))^2\\,du.\n$$\nThis is the explicit expression for $\\mathbb{E}[|Y_t-Y_s|^2]$. Now we analyze its limit as $s \\to t^-$. Let $\\delta = t-s > 0$. As $s \\to t^-$, we have $\\delta \\to 0^+$.\n\nLet's examine the two terms in the expression separately.\nFor the second term:\n$$\n\\int_s^t (h(t-u))^2\\,du = \\int_0^{t-s} h(v)^2\\,dv = \\int_0^\\delta h(v)^2\\,dv.\n$$\n(using the substitution $v = t-u$). Since $h \\in L^2([0,T])$ for any $T>0$, the function $v \\mapsto h(v)^2$ is integrable on $[0,t]$. The integral of an integrable function over an interval of vanishing length is zero. Thus,\n$$\n\\lim_{s \\to t^-} \\int_s^t (h(t-u))^2\\,du = \\lim_{\\delta \\to 0^+} \\int_0^\\delta h(v)^2\\,dv = 0.\n$$\nFor the first term, we can rewrite it as:\n$$\n\\int_0^s (h(t-u) - h(s-u))^2\\,du = \\int_0^s (h(v + (t-s)) - h(v))^2\\,dv,\n$$\n(using the substitution $v=s-u$). This becomes, in terms of $\\delta$:\n$$\n\\int_0^{t-\\delta} (h(v+\\delta) - h(v))^2\\,dv.\n$$\nThis integral is bounded by $\\int_0^t (h(v+\\delta) - h(v))^2\\,dv$. A fundamental property of functions in $L^2$ spaces is the continuity of translation, which states that if $f \\in L^2(\\mathbb{R})$, then $\\lim_{\\delta \\to 0} \\|\\tau_\\delta f - f\\|_{L^2} = 0$, where $\\tau_\\delta f(v) = f(v-\\delta)$. In our context, considering $h$ restricted to $[0,t]$ and extended by zero elsewhere, this property implies:\n$$\n\\lim_{\\delta \\to 0^+} \\int_0^{t-\\delta} (h(v+\\delta) - h(v))^2\\,dv = 0.\n$$\nThis holds because $h \\in L^2([0,t])$. Both terms in the expression for $\\mathbb{E}[|Y_t-Y_s|^2]$ tend to zero as $s \\to t^-$. A symmetric argument holds for $s \\to t^+$. Therefore,\n$$\n\\lim_{s \\to t} \\mathbb{E}[|Y_t - Y_s|^2] = 0.\n$$\nThis proves that the process $(Y_t)_{t \\geq 0}$ is $L^2$-continuous, under the given condition that $h \\in L^2([0,T])$ for every $T>0$.", "answer": "$$\n\\boxed{\\int_{0}^{t} h(u)^2 \\, du}\n$$", "id": "2982185"}, {"introduction": "The martingale representation theorem is a profound result stating that any martingale in a Brownian filtration can be expressed as a stochastic integral. This exercise moves beyond simply analyzing given integrals to the constructive task of finding the integrand itself, given a martingale defined through a Wiener chaos expansion [@problem_id:2982169]. Successfully deriving the predictable integrand reveals the deep connections between the Itô isometry, the orthogonal structure of the $L^2$ space of random variables, and the fundamental composition of martingales.", "problem": "Let $(\\Omega,\\mathcal{F},(\\mathcal{F}_{t})_{0\\le t\\le T},\\mathbb{P})$ be a complete filtered probability space supporting a one-dimensional standard Brownian motion $W=(W_{t})_{0\\le t\\le T}$ with its natural (completed, right-continuous) filtration $(\\mathcal{F}_{t})_{0\\le t\\le T}$. Let $F\\in L^{2}(\\Omega)$ admit the Wiener chaos expansion\n$$\nF \\;=\\; \\sum_{n=0}^{\\infty} I_{n}(f_{n}),\n$$\nwhere, for each $n\\ge 0$, $f_{n}\\in L^{2}_{\\mathrm{sym}}([0,T]^{n})$ is a deterministic symmetric kernel, and $I_{n}(f_{n})$ denotes the $n$-fold Wiener–Itô integral of $f_{n}$ with respect to $W$ (with the usual normalization so that $\\mathbb{E}[I_{n}(f)I_{m}(g)]=\\delta_{nm}\\,n!\\,\\langle f,g\\rangle_{L^{2}([0,T]^{n})}$). Define the square-integrable martingale\n$$\nM_{t}\\;=\\;\\mathbb{E}\\!\\left[F\\mid \\mathcal{F}_{t}\\right],\\qquad 0\\le t\\le T.\n$$\n\nUsing only the foundational properties of multiple Wiener–Itô integrals, the orthogonal decomposition of $L^{2}(\\Omega)$ into Wiener chaoses, and the Itô isometry for stochastic integrals with respect to Brownian motion, derive an explicit predictable integrand $H=(H_{t})_{0\\le t\\le T}$ such that\n$$\nM_{t}\\;=\\;\\mathbb{E}[F]\\;+\\;\\int_{0}^{t} H_{s}\\, \\mathrm{d}W_{s},\\qquad 0\\le t\\le T,\n$$\nand justify that $H\\in L^{2}([0,T]\\times\\Omega)$ and is uniquely determined by this representation.\n\nYour final answer should be a single closed-form analytic expression for $H_{t}$ in terms of the chaos kernels $(f_{n})_{n\\ge 0}$ and the multiple integral operators $(I_{n})_{n\\ge 0}$. No numerical evaluation is required.", "solution": "The problem statement is valid. It is a well-posed problem in the theory of stochastic calculus, specifically concerning the Wiener chaos expansion and the martingale representation theorem. All provided information is mathematically sound, consistent, and self-contained.\n\nThe objective is to find the predictable integrand $H = (H_t)_{0 \\le t \\le T}$ in the martingale representation of $M_t = \\mathbb{E}[F \\mid \\mathcal{F}_t]$, where $F \\in L^2(\\Omega)$ has a given Wiener chaos expansion.\n\nLet the given filtered probability space be $(\\Omega,\\mathcal{F},(\\mathcal{F}_{t})_{0\\le t\\le T},\\mathbb{P})$ supporting a standard one-dimensional Brownian motion $W$. The random variable $F \\in L^2(\\Omega)$ has the Wiener chaos expansion\n$$\nF = \\sum_{n=0}^{\\infty} I_{n}(f_{n})\n$$\nwhere $f_n \\in L^2_{\\text{sym}}([0,T]^n)$ and $I_n$ is the $n$-th multiple Wiener-Itô integral. The martingale is defined as $M_t = \\mathbb{E}[F \\mid \\mathcal{F}_t]$ for $t \\in [0,T]$. Since $F \\in L^2(\\Omega)$ and the filtration $(\\mathcal{F}_t)$ is right-continuous, the martingale $(M_t)_{0 \\le t \\le T}$ is a closed, square-integrable martingale. By the martingale representation theorem for Brownian filtrations, there exists a unique (up to sets of measure zero) predictable process $H = (H_t)_{0 \\le t \\le T}$ such that $H \\in L^2([0,T] \\times \\Omega)$ and\n$$\nM_t = M_0 + \\int_0^t H_s \\, \\mathrm{d}W_s.\n$$\nOur goal is to derive an explicit expression for $H_t$.\n\nFirst, we determine the initial value $M_0$. Since $\\mathcal{F}_0$ is the completion of the trivial sigma-algebra, $M_0 = \\mathbb{E}[F \\mid \\mathcal{F}_0] = \\mathbb{E}[F]$. We compute the expectation of $F$ using its chaos expansion. Due to the orthogonality of the Wiener chaoses, $\\mathbb{E}[I_n(f_n) I_m(f_m)] = 0$ for $n \\ne m$. Specifically, $\\mathbb{E}[I_n(f_n)] = \\mathbb{E}[I_n(f_n) I_0(1)] = 0$ for $n \\ge 1$. For $n=0$, $I_0(f_0)$ is by definition the constant $f_0$. Thus,\n$$\n\\mathbb{E}[F] = \\mathbb{E}\\left[\\sum_{n=0}^{\\infty} I_n(f_n)\\right] = \\sum_{n=0}^{\\infty} \\mathbb{E}[I_n(f_n)] = \\mathbb{E}[I_0(f_0)] = f_0.\n$$\nSo, $M_0 = f_0$, and the representation is $M_t = f_0 + \\int_0^t H_s \\, \\mathrm{d}W_s$. This implies $M_t - f_0$ is a martingale that is null at $t=0$.\n\nBy the linearity of conditional expectation, which is valid due to the $L^2(\\Omega)$ convergence of the series for $F$, we have\n$$\nM_t = \\mathbb{E}\\left[\\sum_{n=0}^{\\infty} I_n(f_n) \\mid \\mathcal{F}_t\\right] = \\sum_{n=0}^{\\infty} \\mathbb{E}[I_n(f_n) \\mid \\mathcal{F}_t].\n$$\nThe term for $n=0$ is $\\mathbb{E}[I_0(f_0) \\mid \\mathcal{F}_t] = \\mathbb{E}[f_0 \\mid \\mathcal{F}_t] = f_0$. Therefore,\n$$\nM_t - f_0 = \\sum_{n=1}^{\\infty} \\mathbb{E}[I_n(f_n) \\mid \\mathcal{F}_t].\n$$\nLet $M_t^{(n)} = \\mathbb{E}[I_n(f_n) \\mid \\mathcal{F}_t]$ for $n \\ge 1$. Each $M^{(n)}$ is a square-integrable martingale with $M_0^{(n)} = \\mathbb{E}[I_n(f_n)] = 0$.\n\nWe use a fundamental representation of multiple Wiener-Itô integrals in terms of single Itô integrals. For a symmetric kernel $g \\in L^2_{\\text{sym}}([0,T]^n)$, $n \\ge 1$, we have\n$$\nI_n(g) = n \\int_0^T I_{n-1}(g(\\cdot, u)) \\, \\mathrm{d}W_u,\n$$\nwhere $g(\\cdot, u)$ denotes the kernel of $n-1$ variables $(s_1, \\dots, s_{n-1}) \\mapsto g(s_1, \\dots, s_{n-1}, u)$. The integrand process $u \\mapsto I_{n-1}(g(\\cdot, u))$ is not necessarily adapted to $(\\mathcal{F}_u)$. The martingale representation of an integral with a non-adapted integrand $\\phi_u$ is given by\n$$\n\\mathbb{E}\\left[\\int_0^T \\phi_u \\, \\mathrm{d}W_u \\mid \\mathcal{F}_t\\right] = \\int_0^t \\mathbb{E}[\\phi_u \\mid \\mathcal{F}_u] \\, \\mathrm{d}W_u.\n$$\nApplying this to $M_t^{(n)} = \\mathbb{E}[I_n(f_n) \\mid \\mathcal{F}_t]$, we get\n$$\nM_t^{(n)} = \\mathbb{E}\\left[n \\int_0^T I_{n-1}(f_n(\\cdot, u)) \\, \\mathrm{d}W_u \\mid \\mathcal{F}_t\\right] = n \\int_0^t \\mathbb{E}[I_{n-1}(f_n(\\cdot, u)) \\mid \\mathcal{F}_u] \\, \\mathrm{d}W_u.\n$$\nSumming over $n \\geq 1$ and interchanging summation and integration (justified by convergence in $L^2([0,T]\\times\\Omega)$), we obtain\n$$\nM_t - f_0 = \\sum_{n=1}^{\\infty} M_t^{(n)} = \\int_0^t \\left( \\sum_{n=1}^{\\infty} n \\, \\mathbb{E}[I_{n-1}(f_n(\\cdot, s)) \\mid \\mathcal{F}_s] \\right) \\mathrm{d}W_s.\n$$\nBy the uniqueness of the predictable integrand in the martingale representation, we identify $H_s$ as:\n$$\nH_s = \\sum_{n=1}^{\\infty} n \\, \\mathbb{E}[I_{n-1}(f_n(\\cdot, s)) \\mid \\mathcal{F}_s].\n$$\nTo make this expression explicit, we must evaluate the conditional expectation. A fundamental property of multiple Wiener-Itô integrals states that for a symmetric kernel $g \\in L^2_{\\text{sym}}([0,k]^k)$,\n$$\n\\mathbb{E}[I_k(g) \\mid \\mathcal{F}_t] = I_k(g(\\cdot) \\mathbf{1}_{[0,t]^k}(\\cdot)),\n$$\nwhere the indicator function restricts the domain of integration variables in the definition of the multiple integral. The resulting random variable is $\\mathcal{F}_t$-measurable.\nIn our expression for $H_s$, for each fixed $s \\in [0,T]$, we have the term $\\mathbb{E}[I_{n-1}(f_n(\\cdot, s)) \\mid \\mathcal{F}_s]$. The kernel is $(u_1, \\dots, u_{n-1}) \\mapsto f_n(u_1, \\dots, u_{n-1}, s)$, which is symmetric in its $n-1$ arguments because $f_n$ is symmetric. Applying the property with $k=n-1$ and time $s$:\n$$\n\\mathbb{E}[I_{n-1}(f_n(\\cdot, s)) \\mid \\mathcal{F}_s] = I_{n-1}(f_n(\\cdot, s) \\mathbf{1}_{[0,s]^{n-1}}).\n$$\nThis is the operator $I_{n-1}$ applied to the kernel given by $f_n(u_1, \\dots, u_{n-1}, s)$ for $(u_1, \\dots, u_{n-1}) \\in [0,s]^{n-1}$, and $0$ otherwise.\n\nSubstituting this back into the expression for $H_s$, we get the final explicit form for the integrand at time $t$:\n$$\nH_t = \\sum_{n=1}^{\\infty} n \\, I_{n-1}\\left( f_n(\\cdot, t) \\mathbf{1}_{[0,t]^{n-1}} \\right).\n$$\nHere, for each $t \\in [0,T]$, the term $f_n(\\cdot, t) \\mathbf{1}_{[0,t]^{n-1}}$ denotes the kernel $k_t: [0,T]^{n-1} \\to \\mathbb{R}$ defined by $k_t(s_1, \\dots, s_{n-1}) = f_n(s_1, \\dots, s_{n-1}, t) \\prod_{i=1}^{n-1}\\mathbf{1}_{[0,t]}(s_i)$.\n\nFinally, we justify that $H \\in L^2([0,T]\\times\\Omega)$. By the Itô isometry for single integrals:\n$$\n\\mathbb{E}\\left[\\int_0^T H_s^2 \\, \\mathrm{d}s\\right] = \\mathbb{E}\\left[\\left(\\int_0^T H_s \\, \\mathrm{d}W_s\\right)^2\\right] = \\mathbb{E}\\left[(M_T - M_0)^2\\right].\n$$\nSince $M_T = F$ and $M_0 = \\mathbb{E}[F]$, this is $\\mathbb{E}[(F - \\mathbb{E}[F])^2] = \\mathrm{Var}(F)$. The variance of $F$ can be computed using the orthogonality of the chaos expansion:\n$$\n\\mathrm{Var}(F) = \\mathbb{E}\\left[ \\left(\\sum_{n=1}^{\\infty} I_n(f_n)\\right)^2 \\right] = \\sum_{n=1}^{\\infty} \\mathbb{E}[I_n(f_n)^2] = \\sum_{n=1}^{\\infty} n! \\, \\|f_n\\|_{L^2([0,T]^n)}^2.\n$$\nSince $F \\in L^2(\\Omega)$, we have $\\mathbb{E}[F^2] = \\sum_{n=0}^{\\infty} n! \\|f_n\\|^2 < \\infty$, which implies $\\mathrm{Var}(F) < \\infty$. Therefore, $\\mathbb{E}[\\int_0^T H_s^2 \\, \\mathrm{d}s] < \\infty$, confirming that $H \\in L^2([0,T] \\times \\Omega)$. The uniqueness of $H$ is guaranteed by the martingale representation theorem.", "answer": "$$\n\\boxed{\\sum_{n=1}^{\\infty} n I_{n-1}\\left( (s_1, \\dots, s_{n-1}) \\mapsto f_{n}(s_1, \\dots, s_{n-1}, t) \\mathbf{1}_{[0,t]^{n-1}}(s_1, \\dots, s_{n-1}) \\right)}\n$$", "id": "2982169"}]}