{"hands_on_practices": [{"introduction": "The construction of the It么 integral begins with simple predictable processes, where the choice of time intervals is a subtle but crucial detail. This exercise [@problem_id:2982008] challenges you to explore why we use right-closed intervals $(t_k, t_{k+1}]$ and not left-closed ones. By analyzing the measurability requirements, you will solidify your understanding of predictability and its fundamental role in preventing us from \"seeing into the future\" when defining the integral.", "problem": "Consider a filtered probability space $(\\Omega,\\mathcal{F},(\\mathcal{F}_t)_{t\\in[0,T]},\\mathbb{P})$ satisfying the usual conditions (right-continuity and completeness), and a standard Brownian motion $(W_t)_{t\\in[0,T]}$ adapted to $(\\mathcal{F}_t)_{t\\in[0,T]}$. For a deterministic partition $0=t_0<t_1<\\cdots<t_n=T$ and coefficients $\\xi_k\\in L^2(\\Omega,\\mathcal{F}_{t_k},\\mathbb{P})$, compare the two families of step processes:\n- the right-closed family $H^{\\mathrm{rc}}(t,\\omega)=\\sum_{k=0}^{n-1}\\xi_k(\\omega)\\,\\mathbf{1}_{(t_k,t_{k+1}]}(t)$, and\n- the left-closed family $H^{\\mathrm{lc}}(t,\\omega)=\\sum_{k=0}^{n-1}\\xi_k(\\omega)\\,\\mathbf{1}_{[t_k,t_{k+1})}(t)$.\n\nLet $\\mathcal{P}$ denote the predictable $\\sigma$-algebra on $[0,T]\\times\\Omega$, defined as the $\\sigma$-algebra generated by sets of the form $(s,t]\\times A$ with $0\\le s<t\\le T$ and $A\\in\\mathcal{F}_s$, together with $\\{0\\}\\times A_0$ with $A_0\\in\\mathcal{F}_0$. Recall that a process is predictable if it is $(\\mathcal{B}([0,T])\\otimes\\mathcal{F})/\\mathcal{P}$-measurable, where $\\mathcal{B}([0,T])$ is the Borel $\\sigma$-algebra on $[0,T]$.\n\nThe It么 integral for simple predictable processes is defined first on predictable step processes and then extended by closure in $L^2(\\mathrm{d}t\\otimes\\mathbb{P})$ to all square-integrable predictable processes. In this construction, the choice of half-open time intervals plays a key role.\n\nWhich of the following statements correctly explain the role of using right-closed intervals $(t_k,t_{k+1}]$ rather than left-closed intervals $[t_k,t_{k+1})$ in the definition of simple predictable processes and its consequences for the It么 integral?\n\nA. Using $(t_k,t_{k+1}]$ ensures that for each $k$ and all $t\\in(t_k,t_{k+1}]$, the value $H^{\\mathrm{rc}}_t=\\xi_k$ is already determined at time $t_k$, i.e., $\\xi_k$ is $\\mathcal{F}_{t_k}$-measurable and hence $\\sigma(\\mathcal{F}_s:s\\le t)$-measurable for all $t\\in(t_k,t_{k+1}]$, which makes $H^{\\mathrm{rc}}$ predictable. If one instead uses $[t_k,t_{k+1})$, then predictability at the left endpoint $t_k$ would require $\\xi_k$ to be $\\mathcal{F}_{t_k-}$-measurable; mere $\\mathcal{F}_{t_k}$-measurability is not sufficient in general.\n\nB. With the left-closed choice $[t_k,t_{k+1})$, the process $H^{\\mathrm{lc}}$ remains predictable as long as $\\xi_k$ is $\\mathcal{F}_{t_k}$-measurable, because predictability only encodes right-continuity in time.\n\nC. The predictable $\\sigma$-algebra $\\mathcal{P}$ is designed to make left-continuous adapted processes measurable. Step processes that are constant on time-slabs of the form $(t_k,t_{k+1}]$ with $\\mathcal{F}_{t_k}$-measurable coefficients are left-continuous and adapted, hence $\\mathcal{P}$-measurable. In contrast, step processes built from $[t_k,t_{k+1})$ and merely $\\mathcal{F}_{t_k}$-measurable coefficients are typically right-continuous adapted and need not be $\\mathcal{P}$-measurable.\n\nD. The distinction between $(t_k,t_{k+1}]$ and $[t_k,t_{k+1})$ is immaterial for predictability because the endpoints $\\{t_k\\}$ are countable, hence negligible with respect to Brownian paths with probability one.\n\nE. If one defined the stochastic integral on left-closed step processes with $\\mathcal{F}_{t_k}$-measurable coefficients, then even for Brownian motion the It么 isometry could fail in general, so the right-closed convention is necessary to ensure the isometry.\n\nSelect all correct options.", "solution": "**Problem Validation**\n\n**Step 1: Extract Givens**\n- A filtered probability space $(\\Omega,\\mathcal{F},(\\mathcal{F}_t)_{t\\in[0,T]},\\mathbb{P})$ satisfying the usual conditions (right-continuity and completeness).\n- A standard Brownian motion $(W_t)_{t\\in[0,T]}$ adapted to $(\\mathcal{F}_t)_{t\\in[0,T]}$.\n- A deterministic partition $0=t_0<t_1<\\cdots<t_n=T$.\n- Coefficients $\\xi_k\\in L^2(\\Omega,\\mathcal{F}_{t_k},\\mathbb{P})$.\n- The right-closed family of step processes $H^{\\mathrm{rc}}(t,\\omega)=\\sum_{k=0}^{n-1}\\xi_k(\\omega)\\,\\mathbf{1}_{(t_k,t_{k+1}]}(t)$.\n- The left-closed family of step processes $H^{\\mathrm{lc}}(t,\\omega)=\\sum_{k=0}^{n-1}\\xi_k(\\omega)\\,\\mathbf{1}_{[t_k,t_{k+1})}(t)$.\n- The predictable $\\sigma$-algebra $\\mathcal{P}$ on $[0,T]\\times\\Omega$, defined as the $\\sigma$-algebra generated by sets of the form $(s,t]\\times A$ with $0\\le s<t\\le T$ and $A\\in\\mathcal{F}_s$, together with $\\{0\\}\\times A_0$ with $A_0\\in\\mathcal{F}_0$.\n- A process is predictable if it is $(\\mathcal{B}([0,T])\\otimes\\mathcal{F})/\\mathcal{P}$-measurable.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem statement is scientifically grounded, well-posed, and objective. It presents a standard, fundamental question in the theory of stochastic integration concerning the definition of predictable processes. All terms are standard and defined precisely. The setup is self-contained and free of contradictions or ambiguities. The problem probes the subtle but critical difference between adaptedness and predictability.\n\n**Step 3: Verdict and Action**\nThe problem is **valid**. A full solution will be derived.\n\n**Derivation and Option Analysis**\n\nThe core of the problem lies in the definition of a predictable process. A process $H$ is predictable if it is measurable with respect to the predictable $\\sigma$-algebra $\\mathcal{P}$. The problem states that $\\mathcal{P}$ is generated by sets of the form $(s,t]\\times A$ where $0\\le s<t\\le T$ and $A\\in\\mathcal{F}_s$, along with sets $\\{0\\}\\times A_0$ where $A_0\\in\\mathcal{F}_0$. These generating sets are called predictable rectangles.\n\nLet us analyze the measurability of the two families of processes.\n\n**1. Analysis of the right-closed family $H^{\\mathrm{rc}}(t,\\omega)$**\nThe process is $H^{\\mathrm{rc}}(t,\\omega)=\\sum_{k=0}^{n-1}\\xi_k(\\omega)\\,\\mathbf{1}_{(t_k,t_{k+1}]}(t)$. Since a finite sum of measurable functions is measurable, we only need to analyze a single term, say $H_k(t,\\omega) = \\xi_k(\\omega)\\,\\mathbf{1}_{(t_k,t_{k+1}]}(t)$, for a fixed $k \\in \\{0, \\dots, n-1\\}$.\n\nTo check if $H_k$ is $\\mathcal{P}$-measurable, we must verify that for any Borel set $B \\subset \\mathbb{R}$, the preimage set $\\{(t,\\omega) : H_k(t,\\omega) \\in B\\}$ belongs to $\\mathcal{P}$.\nThis preimage is $\\{(t,\\omega) : t \\in (t_k, t_{k+1}] \\text{ and } \\xi_k(\\omega) \\in B \\}$. Let $A = \\{\\omega : \\xi_k(\\omega) \\in B\\}$. Since $\\xi_k$ is given to be $\\mathcal{F}_{t_k}$-measurable, the set $A$ is in $\\mathcal{F}_{t_k}$.\nThe preimage set is therefore $(t_k, t_{k+1}] \\times A$.\nBy the definition of $\\mathcal{P}$, this is a predictable rectangle (with $s=t_k$ and $t=t_{k+1}$), and thus an element of $\\mathcal{P}$.\nHence, each term $H_k$ is predictable, and their sum $H^{\\mathrm{rc}}$ is predictable.\nThis construction is fundamental: the value of the process at any time $t \\in (t_k, t_{k+1}]$ is given by $\\xi_k$, a random variable whose value is \"known\" at time $t_k < t$. This aligns with the intuition of predictability.\n\n**2. Analysis of the left-closed family $H^{\\mathrm{lc}}(t,\\omega)$**\nThe process is $H^{\\mathrm{lc}}(t,\\omega)=\\sum_{k=0}^{n-1}\\xi_k(\\omega)\\,\\mathbf{1}_{[t_k,t_{k+1})}(t)$. Again, we analyze a single term $H'_k(t,\\omega) = \\xi_k(\\omega)\\,\\mathbf{1}_{[t_k,t_{k+1})}(t)$.\nThe preimage set is $\\{(t,\\omega) : t \\in [t_k, t_{k+1}) \\text{ and } \\xi_k(\\omega) \\in B \\}$. With $A = \\{\\omega : \\xi_k(\\omega) \\in B\\} \\in \\mathcal{F}_{t_k}$ as before, this set is $[t_k, t_{k+1}) \\times A$.\nWe can decompose this set as:\n$$[t_k, t_{k+1}) \\times A = (\\{t_k\\} \\times A) \\cup ((t_k, t_{k+1}) \\times A)$$\nThe second part, $(t_k, t_{k+1}) \\times A$, is predictable because it can be written as a countable union of predictable rectangles, $\\bigcup_{m=1}^{\\infty} (t_k, t_{k+1}-1/m] \\times A$.\nThe first part, $\\{t_k\\} \\times A$, is more problematic. For $t_k > 0$, the set $\\{t_k\\} \\times A$ is predictable if and only if $A \\in \\mathcal{F}_{t_k-}$, where $\\mathcal{F}_{t_k-} = \\sigma(\\bigcup_{s<t_k} \\mathcal{F}_s)$ is the pre-$t_k$ sigma-algebra.\nThe problem only assumes $\\xi_k$ is $\\mathcal{F}_{t_k}$-measurable, which implies $A \\in \\mathcal{F}_{t_k}$. In general, $\\mathcal{F}_{t_k-} \\subsetneq \\mathcal{F}_{t_k}$ (for example, in a filtration generated by a process with a jump at $t_k$). Therefore, merely knowing $A \\in \\mathcal{F}_{t_k}$ is not sufficient to conclude that $\\{t_k\\} \\times A$ is predictable.\nConsequently, $H^{\\mathrm{lc}}$ is not guaranteed to be a predictable process. It is, however, an adapted process, because for any $t \\in [t_k, t_{k+1})$, its value $H^{\\mathrm{lc}}_t = \\xi_k$ is $\\mathcal{F}_{t_k}$-measurable and thus $\\mathcal{F}_t$-measurable.\n\nNow, we evaluate each option.\n\n**A. Using $(t_k,t_{k+1}]$ ensures that for each $k$ and all $t\\in(t_k,t_{k+1}]$, the value $H^{\\mathrm{rc}}_t=\\xi_k$ is already determined at time $t_k$, i.e., $\\xi_k$ is $\\mathcal{F}_{t_k}$-measurable and hence $\\sigma(\\mathcal{F}_s:s\\le t)$-measurable for all $t\\in(t_k,t_{k+1}]$, which makes $H^{\\mathrm{rc}}$ predictable. If one instead uses $[t_k,t_{k+1})$, then predictability at the left endpoint $t_k$ would require $\\xi_k$ to be $\\mathcalF}_{t_k-}$-measurable; mere $\\mathcal{F}_{t_k}$-measurability is not sufficient in general.**\nThis statement is a precise and accurate summary of the analysis above. The process $H^{\\mathrm{rc}}$ is constructed from predictable rectangles. The process $H^{\\mathrm{lc}}$ has a potential measurability issue at the left endpoint $t_k$ unless the coefficient $\\xi_k$ satisfies the stronger condition of being $\\mathcal{F}_{t_k-}$-measurable. The statement correctly identifies this critical distinction.\n**Verdict: Correct**\n\n**B. With the left-closed choice $[t_k,t_{k+1})$, the process $H^{\\mathrm{lc}}$ remains predictable as long as $\\xi_k$ is $\\mathcal{F}_{t_k}$-measurable, because predictability only encodes right-continuity in time.**\nThis statement is incorrect on two counts. First, as shown in our analysis, $H^{\\mathrm{lc}}$ is not necessarily predictable under the given conditions. Second, predictability is fundamentally associated with left-continuity, not right-continuity. The predictable $\\sigma$-algebra can be characterized as the one generated by all left-continuous adapted processes.\n**Verdict: Incorrect**\n\n**C. The predictable $\\sigma$-algebra $\\mathcal{P}$ is designed to make left-continuous adapted processes measurable. Step processes that are constant on time-slabs of the form $(t_k,t_{k+1}]$ with $\\mathcal{F}_{t_k}$-measurable coefficients are left-continuous and adapted, hence $\\mathcal{P}$-measurable. In contrast, step processes built from $[t_k,t_{k+1})$ and merely $\\mathcal{F}_{t_k}$-measurable coefficients are typically right-continuous adapted and need not be $\\mathcal{P}$-measurable.**\nThis statement provides an alternative, correct perspective based on path properties. It is a fundamental result that $\\mathcal{P}$ is the $\\sigma$-algebra generated by all left-continuous adapted processes. The process $H^{\\mathrm{rc}}$ is indeed left-continuous and adapted, and therefore predictable. The process $H^{\\mathrm{lc}}$ is right-continuous and adapted. A right-continuous adapted process is not necessarily predictable, which is the key issue. This statement correctly aligns the interval choices with path properties and their implications for predictability.\n**Verdict: Correct**\n\n**D. The distinction between $(t_k,t_{k+1}]$ and $[t_k,t_{k+1})$ is immaterial for predictability because the endpoints $\\{t_k\\}$ are countable, hence negligible with respect to Brownian paths with probability one.**\nThis reasoning is flawed. The issue is one of measurability with respect to the filtration, not integrability with respect to Lebesgue measure on the time axis. The definition of the It么 integral and the class of valid integrands (predictable processes) is a delicate matter of information flow over time. The properties at individual time points, particularly the left endpoints of intervals, are crucial for distinguishing predictability from adaptedness. Confusing this with measure-theoretic negligibility is a conceptual error.\n**Verdict: Incorrect**\n\n**E. If one defined the stochastic integral on left-closed step processes with $\\mathcal{F}_{t_k}$-measurable coefficients, then even for Brownian motion the It么 isometry could fail in general, so the right-closed convention is necessary to ensure the isometry.**\nThis statement is false. The It么 isometry holds for simple adapted processes, which are precisely of the form $H^{\\mathrm{lc}}$. Let $I(H) = \\sum_{k=0}^{n-1}\\xi_k(W_{t_{k+1}}-W_{t_k})$. The It么 isometry is the property $\\mathbb{E}[(I(H))^2] = \\mathbb{E}[\\int_0^T H_t^2 dt]$.\nFor $H=H^{\\mathrm{lc}}$, we have $\\mathbb{E}[\\int_0^T (H^{\\mathrm{lc}}_t)^2 dt] = \\mathbb{E}[\\sum_{k=0}^{n-1} \\xi_k^2 (t_{k+1}-t_k)] = \\sum_{k=0}^{n-1} (t_{k+1}-t_k) \\mathbb{E}[\\xi_k^2]$.\nFor the integral's second moment, the key steps rely on the orthogonality of increments $\\mathbb{E}[(W_{t_{k+1}}-W_{t_k})(W_{t_{j+1}}-W_{t_j})|\\mathcal{F}_{t_j}] = 0$ for $j<k$, and the fact that $\\xi_k$ is $\\mathcal{F}_{t_k}$-measurable and $(W_{t_{k+1}}-W_{t_k})$ is independent of $\\mathcal{F}_{t_k}$. This leads to $\\mathbb{E}[(I(H^{\\mathrm{lc}}))^2] = \\sum_{k=0}^{n-1} \\mathbb{E}[\\xi_k^2(W_{t_{k+1}}-W_{t_k})^2] = \\sum_{k=0}^{n-1} \\mathbb{E}[\\xi_k^2]\\mathbb{E}[(W_{t_{k+1}}-W_{t_k})^2] = \\sum_{k=0}^{n-1} (t_{k+1}-t_k) \\mathbb{E}[\\xi_k^2]$.\nThe isometry holds perfectly. The choice of predictable processes as the class of integrands is motivated by the desire for the stochastic integral to be a martingale (for bounded integrands), which requires predictability, not just adaptedness. The isometry itself is established on the simpler class of adapted step processes and then extended.\n**Verdict: Incorrect**", "answer": "$$\\boxed{AC}$$", "id": "2982008"}, {"introduction": "Having established the structure of simple predictable integrands, we now investigate their uniqueness. This problem [@problem_id:2982014] presents two processes that differ only on a set of measure zero in the time-space domain, making them technically distinct but \"almost everywhere\" the same. By demonstrating that their It么 integrals are indistinguishable, you will gain a deeper appreciation for a core property of the integral: its value depends on the integrand's behavior over sets of positive measure, a direct consequence of the It么 isometry.", "problem": "Let $(\\Omega,\\mathcal{F},\\mathbb{P})$ be a probability space equipped with the completed, right-continuous filtration $\\mathbb{F}=\\{\\mathcal{F}_{t}\\}_{t\\geq 0}$ generated by a standard Brownian motion $\\{W_{t}\\}_{t\\geq 0}$, with fixed horizon $T>0$. Consider a deterministic partition $0=t_{0}<t_{1}<\\cdots<t_{m}=T$ and random variables $K_{i}\\in L^{2}(\\Omega,\\mathcal{F}_{t_{i-1}},\\mathbb{P})$ for $i=1,\\dots,m$. Define the base simple process\n$$\nK(t,\\omega)\\;=\\;\\sum_{i=1}^{m}K_{i}(\\omega)\\,\\mathbf{1}_{(t_{i-1},t_{i}]}(t),\\qquad t\\in[0,T].\n$$\nLet $A\\in\\mathcal{F}_{0}$ satisfy $0<\\mathbb{P}(A)<1$, and set $Z(\\omega)=\\mathbf{1}_{A}(\\omega)$. Define two processes $H$ and $G$ on $[0,T]$ by\n$$\nH(t,\\omega)=\\begin{cases}\n0,&t=0,\\\\\nK(t,\\omega),&t\\in(0,T],\n\\end{cases}\n\\qquad\nG(t,\\omega)=\\begin{cases}\nZ(\\omega),&t=0,\\\\\nK(t,\\omega),&t\\in(0,T].\n\\end{cases}\n$$\nTasks:\n1. Verify, from first principles, that both $H$ and $G$ are simple predictable processes. Be explicit about the measurability with respect to $\\mathcal{F}_{t}$ and the predictability structure inherited from the step-function form on $(t_{i-1},t_{i}]$.\n2. Prove that $H$ and $G$ are equal almost everywhere in the product measure $\\mathbb{P}\\otimes dt$ on $\\Omega\\times[0,T]$, but that they are not indistinguishable, meaning that $\\mathbb{P}(H(t)=G(t)\\text{ for all }t\\in[0,T])<1$. Carefully justify each claim from the definitions of almost everywhere and indistinguishability.\n3. Using only the foundational definition of the It么 integral for simple processes with respect to Brownian motion and its core properties (linearity, additivity over disjoint time intervals, and $L^{2}$-isometry), explain why $\\left\\{\\int_{0}^{t}H(s,\\omega)\\,dW_{s}\\right\\}_{t\\in[0,T]}$ and $\\left\\{\\int_{0}^{t}G(s,\\omega)\\,dW_{s}\\right\\}_{t\\in[0,T]}$ define indistinguishable martingales.\n4. Compute the quantity\n$$\n\\mathbb{E}\\!\\left[\\left(\\int_{0}^{T}\\big(H(s,\\omega)-G(s,\\omega)\\big)\\,dW_{s}\\right)^{2}\\right].\n$$\nProvide your final answer as a single, exact real number. No rounding is required.", "solution": "The problem will be addressed by sequentially completing the four specified tasks. The validation of the problem statement precedes the solution.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n-   A probability space $(\\Omega,\\mathcal{F},\\mathbb{P})$.\n-   A completed, right-continuous filtration $\\mathbb{F}=\\{\\mathcal{F}_{t}\\}_{t\\geq 0}$ generated by a standard Brownian motion $\\{W_{t}\\}_{t\\geq 0}$.\n-   A fixed time horizon $T>0$.\n-   A deterministic partition $0=t_{0}<t_{1}<\\cdots<t_{m}=T$.\n-   Random variables $K_{i}\\in L^{2}(\\Omega,\\mathcal{F}_{t_{i-1}},\\mathbb{P})$ for $i=1,\\dots,m$.\n-   A base simple process $K(t,\\omega)\\;=\\;\\sum_{i=1}^{m}K_{i}(\\omega)\\,\\mathbf{1}_{(t_{i-1},t_{i}]}(t)$ for $t\\in[0,T]$.\n-   An event $A\\in\\mathcal{F}_{0}$ with $0<\\mathbb{P}(A)<1$.\n-   A random variable $Z(\\omega)=\\mathbf{1}_{A}(\\omega)$.\n-   A process $H(t,\\omega)=\\begin{cases} 0,&t=0,\\\\ K(t,\\omega),&t\\in(0,T]. \\end{cases}$\n-   A process $G(t,\\omega)=\\begin{cases} Z(\\omega),&t=0,\\\\ K(t,\\omega),&t\\in(0,T]. \\end{cases}$\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is mathematically well-defined and internally consistent. It is a standard problem in the theory of stochastic integration.\n-   **Scientifically Grounded:** The concepts (Brownian motion, filtration, It么 integral, simple processes, indistinguishability) are all standard and fundamental to stochastic calculus. The assumption that $\\mathcal{F}_0$ contains a non-trivial set $A$ is valid because the filtration is completed.\n-   **Well-Posed:** Each task is clearly stated and has a definite, unique answer derivable from the givens.\n-   **Objective:** The language is formal and unambiguous.\n-   **Completeness:** The problem provides all necessary information to perform the tasks, including the measurability and integrability conditions for the random variables $K_i$.\n\n**Step 3: Verdict and Action**\nThe problem is valid. A full solution will be provided.\n\n### Solution\n\n**Task 1: Verification that $H$ and $G$ are simple predictable processes.**\n\nA process $\\Phi$ is called a simple predictable process if it can be written in the form\n$$\n\\Phi(t,\\omega) = \\phi_0(\\omega)\\mathbf{1}_{\\{0\\}}(t) + \\sum_{i=1}^{m} \\phi_{i-1}(\\omega) \\mathbf{1}_{(t_{i-1},t_{i}]}(t)\n$$\nwhere each random variable $\\phi_{i-1}$ is $\\mathcal{F}_{t_{i-1}}$-measurable for $i=1, \\dots, m$, and $\\phi_0$ is $\\mathcal{F}_0$-measurable.\n\nLet's examine the process $H(t,\\omega)$. It is defined as:\n$$\nH(t,\\omega) = \\begin{cases} 0,& t=0 \\\\ K(t,\\omega),& t\\in(0,T] \\end{cases}\n$$\nwhere $K(t,\\omega) = \\sum_{i=1}^{m}K_{i}(\\omega)\\,\\mathbf{1}_{(t_{i-1},t_{i}]}(t)$. We can rewrite $H(t,\\omega)$ in the standard form for a simple process:\n$$\nH(t,\\omega) = 0 \\cdot \\mathbf{1}_{\\{0\\}}(t) + \\sum_{i=1}^{m} K_i(\\omega)\\mathbf{1}_{(t_{i-1},t_{i}]}(t)\n$$\nTo verify that $H$ is a simple predictable process, we check the measurability conditions.\n- The coefficient of $\\mathbf{1}_{\\{0\\}}(t)$ is $\\phi_0(\\omega) = 0$. A constant is measurable with respect to any $\\sigma$-algebra, including $\\mathcal{F}_0$.\n- For $i=1,\\dots,m$, the coefficient of $\\mathbf{1}_{(t_{i-1},t_{i}]}(t)$ is $\\phi_{i-1}(\\omega)=K_i(\\omega)$. By the problem statement, $K_i$ is $\\mathcal{F}_{t_{i-1}}$-measurable.\nSince both conditions are met, $H$ is a simple predictable process.\n\nNext, we examine the process $G(t,\\omega)$. It is defined as:\n$$\nG(t,\\omega) = \\begin{cases} Z(\\omega),& t=0 \\\\ K(t,\\omega),& t\\in(0,T] \\end{cases}\n$$\nwhere $Z(\\omega)=\\mathbf{1}_{A}(\\omega)$. We can rewrite $G(t,\\omega)$ as:\n$$\nG(t,\\omega) = Z(\\omega) \\cdot \\mathbf{1}_{\\{0\\}}(t) + \\sum_{i=1}^{m} K_i(\\omega)\\mathbf{1}_{(t_{i-1},t_{i}]}(t)\n$$\nTo verify that $G$ is a simple predictable process, we check the measurability conditions.\n- The coefficient of $\\mathbf{1}_{\\{0\\}}(t)$ is $Z(\\omega)$. We are given that $A \\in \\mathcal{F}_0$, which implies that the indicator function $Z(\\omega) = \\mathbf{1}_A(\\omega)$ is $\\mathcal{F}_0$-measurable.\n- For $i=1,\\dots,m$, the coefficient of $\\mathbf{1}_{(t_{i-1},t_{i}]}(t)$ is $K_i(\\omega)$, which is given to be $\\mathcal{F}_{t_{i-1}}$-measurable.\nSince both conditions are met, $G$ is also a simple predictable process.\n\n**Task 2: Prove $H=G$ almost everywhere but they are not indistinguishable.**\n\nTwo processes $\\Phi_1$ and $\\Phi_2$ are equal almost everywhere (a.e.) with respect to the product measure $\\mathbb{P}\\otimes dt$ on $\\Omega \\times [0,T]$ if the set $\\{(\\omega,t) \\in \\Omega \\times [0,T] \\mid \\Phi_1(t,\\omega) \\neq \\Phi_2(t,\\omega)\\}$ has measure zero.\nLet's identify the set where $H$ and $G$ differ.\n$$\nH(t,\\omega) - G(t,\\omega) = \\begin{cases} 0 - Z(\\omega),& t=0 \\\\ K(t,\\omega) - K(t,\\omega),& t\\in(0,T] \\end{cases} = \\begin{cases} -Z(\\omega),& t=0 \\\\ 0,& t\\in(0,T] \\end{cases}\n$$\nThe processes differ if and only if $t=0$ and $Z(\\omega) \\neq 0$. $Z(\\omega) = \\mathbf{1}_A(\\omega)$ is non-zero if and only if $\\omega \\in A$.\nThus, the set of points where they differ is $D = \\{(\\omega,t) \\mid H(t,\\omega) \\ne G(t,\\omega)\\} = A \\times \\{0\\}$.\nThe product measure of this set is $(\\mathbb{P}\\otimes dt)(D) = \\mathbb{P}(A) \\times dt(\\{0\\})$. The Lebesgue measure $dt$ of a singleton set like $\\{0\\}$ is $0$. Therefore,\n$$\n(\\mathbb{P}\\otimes dt)(D) = \\mathbb{P}(A) \\times 0 = 0\n$$\nThis proves that $H$ and $G$ are equal almost everywhere.\n\nTwo processes $\\Phi_1$ and $\\Phi_2$ are indistinguishable if $\\mathbb{P}(\\Phi_1(t) = \\Phi_2(t) \\text{ for all } t \\in [0,T]) = 1$. This is equivalent to showing that the set of paths on which they differ at any time has probability zero.\nLet's consider the event that $H$ and $G$ are identical for all time: $E = \\{\\omega \\in \\Omega \\mid H(t,\\omega) = G(t,\\omega) \\text{ for all } t \\in [0,T]\\}$.\nThe processes are identical for a given $\\omega$ if and only if they are identical at $t=0$, since they are identical for all $t \\in (0,T]$. The condition is $H(0,\\omega) = G(0,\\omega)$, which means $0 = Z(\\omega) = \\mathbf{1}_A(\\omega)$. This is true if and only if $\\omega \\notin A$, i.e., $\\omega \\in A^c$.\nThus, the event $E$ is identical to the event $A^c$.\nThe probability of this event is $\\mathbb{P}(E) = \\mathbb{P}(A^c) = 1 - \\mathbb{P}(A)$.\nWe are given that $0 < \\mathbb{P}(A) < 1$. This implies that $\\mathbb{P}(E) = 1 - \\mathbb{P}(A) < 1$.\nSince the probability of the paths being identical is less than $1$, the processes $H$ and $G$ are not indistinguishable.\n\n**Task 3: Explain why the It么 integrals of $H$ and $G$ are indistinguishable martingales.**\n\nLet $I_H(t) = \\int_{0}^{t}H(s,\\omega)\\,dW_{s}$ and $I_G(t) = \\int_{0}^{t}G(s,\\omega)\\,dW_{s}$.\nThe It么 integral of any simple predictable process belonging to the appropriate $L^2$ space is a continuous martingale. Since $H$ and $G$ are simple predictable processes and the coefficients $K_i$ are in $L^2(\\Omega, \\mathcal{F}_{t_{i-1}}, \\mathbb{P})$, their It么 integrals are well-defined continuous martingales.\n\nTo show that the processes $\\{I_H(t)\\}_{t \\in [0,T]}$ and $\\{I_G(t)\\}_{t \\in [0,T]}$ are indistinguishable, we must show that $\\mathbb{P}(I_H(t) = I_G(t) \\text{ for all } t\\in[0,T]) = 1$.\nConsider the difference process $D(t) = I_H(t) - I_G(t)$. By the linearity of the It么 integral,\n$$\nD(t) = \\int_{0}^{t} \\big(H(s,\\omega) - G(s,\\omega)\\big) \\,dW_{s}\n$$\n$D(t)$ is a continuous martingale with $D(0)=0$. We will show that $D(t)=0$ for all $t \\in [0,T]$ almost surely.\nA core property of the It么 integral is that if two integrands are equal almost everywhere with respect to $\\mathbb{P}\\otimes dt$, their It么 integrals are indistinguishable. We have already shown that $H$ and $G$ are equal a.e. in Task 2.\nTo demonstrate this from the core properties listed, we use the $L^2$-isometry. For any fixed $t \\in [0,T]$, the isometry states:\n$$\n\\mathbb{E}\\left[D(t)^2\\right] = \\mathbb{E}\\left[\\left(\\int_{0}^{t} \\big(H(s) - G(s)\\big) \\,dW_{s}\\right)^2\\right] = \\mathbb{E}\\left[\\int_{0}^{t} \\big(H(s) - G(s)\\big)^2 \\,ds\\right]\n$$\nBy Fubini's theorem (since the integrand is non-negative), we can swap the expectation and integral:\n$$\n\\mathbb{E}\\left[D(t)^2\\right] = \\int_{0}^{t} \\mathbb{E}\\left[\\big(H(s) - G(s)\\big)^2\\right] \\,ds\n$$\nAs established in Task 2, $H(s) - G(s) = 0$ for all $s \\in (0,T]$. At $s=0$, $H(0) - G(0) = -Z(\\omega)$. The integrand of the time integral, $\\mathbb{E}[(H(s)-G(s))^2]$, is $0$ for all $s \\in (0,t]$. The Lebesgue integral over $[0,t]$ is therefore $0$, as the value at a single point does not contribute to the integral.\n$$\n\\int_{0}^{t} \\mathbb{E}\\left[\\big(H(s) - G(s)\\big)^2\\right] \\,ds = \\int_{0}^{t} 0 \\,ds = 0\n$$\nSo, for any $t \\in [0,T]$, $\\mathbb{E}[D(t)^2] = 0$. This implies that $D(t) = 0$ almost surely for each fixed $t$.\nTo establish indistinguishability, we use the fact that $D(t)$ is a continuous process. A continuous process that is zero at every fixed time $t$ with probability $1$ is indistinguishable from the zero process.\nFormally, let $Q = [0,T] \\cap \\mathbb{Q}$ be the set of rational numbers in the interval. The event \"there exists $t \\in [0,T]$ such that $D(t) \\neq 0$\" is, by continuity, the same as \"there exists $q \\in Q$ such that $D(q) \\neq 0$\".\nLet $E_q = \\{\\omega \\mid D(q,\\omega) \\neq 0\\}$. We have shown $\\mathbb{P}(E_q) = 0$ for each $q$. The union $E = \\cup_{q \\in Q} E_q$ is a countable union of null sets, and thus is a null set: $\\mathbb{P}(E)=0$.\nThis means $\\mathbb{P}(D(q)=0 \\text{ for all } q \\in Q)=1$. By the continuity of the sample paths of $D(t)$, if a path is zero on the dense set $Q$, it must be zero for all $t \\in [0,T]$.\nTherefore, $\\mathbb{P}(D(t)=0 \\text{ for all } t \\in [0,T]) = 1$. This shows that the processes $\\{I_H(t)\\}$ and $\\{I_G(t)\\}$ are indistinguishable.\n\n**Task 4: Compute the quantity.**\n\nWe are asked to compute\n$$\n\\mathbb{E}\\!\\left[\\left(\\int_{0}^{T}\\big(H(s,\\omega)-G(s,\\omega)\\big)\\,dW_{s}\\right)^{2}\\right]\n$$\nThis is precisely the expression for $\\mathbb{E}[D(T)^2]$ derived in Task 3. Using the $L^2$-isometry property of the It么 integral:\n$$\n\\mathbb{E}\\!\\left[\\left(\\int_{0}^{T}\\big(H(s)-G(s)\\big)\\,dW_{s}\\right)^{2}\\right] = \\mathbb{E}\\!\\left[\\int_{0}^{T}\\big(H(s)-G(s)\\big)^{2}\\,ds\\right]\n$$\nWe analyze the integrand of the time integral, $(H(s)-G(s))^2$.\nFor any $s \\in (0, T]$, we have $H(s)=K(s)$ and $G(s)=K(s)$, so $(H(s)-G(s))^2 = 0$.\nAt $s=0$, we have $H(0)=0$ and $G(0)=Z(\\omega)$, so $(H(0)-G(0))^2 = (-Z(\\omega))^2 = Z(\\omega)^2 = (\\mathbf{1}_A(\\omega))^2 = \\mathbf{1}_A(\\omega)$.\nSo, for any given $\\omega \\in \\Omega$, the integrand as a function of time is:\n$$\nf(s) = \\big(H(s,\\omega)-G(s,\\omega)\\big)^{2} = \\begin{cases} \\mathbf{1}_A(\\omega), & s=0 \\\\ 0, & s \\in (0,T] \\end{cases}\n$$\nThe integral $\\int_{0}^{T} f(s) \\,ds$ is a Lebesgue integral with respect to time $s$. The integrand is non-zero only at the single point $s=0$. A set containing a single point has Lebesgue measure zero. Therefore, the integral is zero.\n$$\n\\int_{0}^{T}\\big(H(s,\\omega)-G(s,\\omega)\\big)^{2}\\,ds = 0 \\quad \\text{for every } \\omega \\in \\Omega.\n$$\nNow, we take the expectation of this result:\n$$\n\\mathbb{E}\\!\\left[\\int_{0}^{T}\\big(H(s)-G(s)\\big)^{2}\\,ds\\right] = \\mathbb{E}[0] = 0.\n$$\nThe value of the expression is $0$.", "answer": "$$\n\\boxed{0}\n$$", "id": "2982014"}, {"introduction": "The It么 integral maps a (potentially random) process to a new random variable. This exercise [@problem_id:2981993] provides hands-on practice in determining the probabilistic nature of this output. By constructing a simple predictable process whose value depends on initial-time information, you will use the law of total probability to derive the exact probability distribution of its It么 integral, revealing it to be a beautiful mixture of Gaussian distributions.", "problem": "Let $\\left(\\Omega,\\mathcal{F},(\\mathcal{F}_{t})_{t \\ge 0},\\mathbb{P}\\right)$ be a filtered probability space carrying a standard Brownian motion $W=\\{W_{t}\\}_{t \\ge 0}$ adapted to $(\\mathcal{F}_{t})_{t \\ge 0}$. Fix a time horizon $T>0$. Let $m \\in \\mathbb{N}$ and let $\\{A_{1},\\dots,A_{m}\\}$ be a partition of $\\Omega$ with $A_{j} \\in \\mathcal{F}_{0}$ and $\\mathbb{P}(A_{j})=p_{j} \\in (0,1)$ for $j=1,\\dots,m$. Let $a_{1},\\dots,a_{m} \\in \\mathbb{R} \\setminus \\{0\\}$ be distinct constants, and define the simple predictable process $H=\\{H_{s}\\}_{0 \\le s \\le T}$ by\n$$\nH_{s}=\\sum_{j=1}^{m} a_{j}\\,\\mathbf{1}_{A_{j}}\\,\\mathbf{1}_{(0,T]}(s).\n$$\nConsider the It么 integral of $H$ with respect to $W$ over $[0,T]$,\n$$\nI=\\int_{0}^{T} H_{s}\\,dW_{s}.\n$$\nStarting only from the definition of the It么 integral for simple predictable processes and the basic properties of Brownian motion, derive the unconditional probability density function $f_{I}(x)$ of $I$ by conditioning on the $\\sigma$-algebra generated by $\\{A_{1},\\dots,A_{m}\\}$. Express your final answer as a single closed-form analytic expression for $f_{I}(x)$ in terms of $p_{j}$, $a_{j}$, $T$, and $x$. No rounding is required, and no physical units are involved.", "solution": "The problem asks for the unconditional probability density function (PDF) of the It么 integral $I = \\int_{0}^{T} H_{s}\\,dW_{s}$, where $H_{s}$ is a simple predictable process.\n\n**Step 1: Problem Validation**\nThe problem is well-defined within the mathematical framework of stochastic calculus.\n*   **Givens**:\n    *   A filtered probability space $(\\Omega,\\mathcal{F},(\\mathcal{F}_{t})_{t \\ge 0},\\mathbb{P})$.\n    *   A standard Brownian motion $W_{t}$ adapted to $(\\mathcal{F}_{t})_{t \\ge 0}$ with $W_0=0$.\n    *   A time horizon $T>0$.\n    *   A partition of $\\Omega$, $\\{A_{1},\\dots,A_{m}\\}$, with $A_{j} \\in \\mathcal{F}_{0}$ and $\\mathbb{P}(A_{j})=p_{j} \\in (0,1)$.\n    *   Distinct non-zero constants $a_{1},\\dots,a_{m} \\in \\mathbb{R} \\setminus \\{0\\}$.\n    *   A simple predictable process $H_{s}=\\sum_{j=1}^{m} a_{j}\\,\\mathbf{1}_{A_{j}}\\,\\mathbf{1}_{(0,T]}(s)$.\n    *   The It么 integral $I=\\int_{0}^{T} H_{s}\\,dW_{s}$.\n*   **Validation**: The problem is scientifically grounded, well-posed, and objective. The process $H_s$ is a standard simple predictable process as it can be written in the form $E \\cdot \\mathbf{1}_{(0,T]}(s)$, where $E = \\sum_{j=1}^{m} a_{j}\\,\\mathbf{1}_{A_{j}}$ is an $\\mathcal{F}_0$-measurable random variable. The definition of the It么 integral is directly applicable. All conditions are consistent and sufficient for a unique solution. Therefore, the problem is valid.\n\n**Step 2: Derivation of the It么 Integral**\nThe process $H_s$ is a simple predictable process. A general simple process is of the form $\\phi_t = \\sum_{k=0}^{n-1} E_k \\mathbf{1}_{(t_k, t_{k+1}]}(t)$, where each $E_k$ is an $\\mathcal{F}_{t_k}$-measurable random variable. The It么 integral is defined as $\\int_0^T \\phi_t \\, dW_t = \\sum_{k=0}^{n-1} E_k (W_{t_{k+1}} - W_{t_k})$.\n\nIn this problem, the process $H_s$ corresponds to the case with a single time interval $(0, T]$. We can write $H_s = E \\cdot \\mathbf{1}_{(0,T]}(s)$, where the random variable $E$ is given by\n$$\nE = \\sum_{j=1}^{m} a_{j}\\,\\mathbf{1}_{A_{j}}.\n$$\nSince each event $A_j$ is in the initial $\\sigma$-algebra $\\mathcal{F}_0$, the random variable $E$ is $\\mathcal{F}_0$-measurable.\n\nApplying the definition of the It么 integral for simple processes, we get:\n$$\nI = \\int_{0}^{T} H_{s}\\,dW_{s} = E \\cdot (W_T - W_0).\n$$\nBy the properties of standard Brownian motion, $W_0 = 0$. Thus, the integral simplifies to:\n$$\nI = E \\cdot W_T = \\left(\\sum_{j=1}^{m} a_{j}\\,\\mathbf{1}_{A_{j}}\\right) W_T.\n$$\n\n**Step 3: Derivation of the Probability Density Function**\nTo find the unconditional PDF of $I$, denoted $f_I(x)$, we will use the law of total probability, conditioning on the partition $\\{A_1, \\dots, A_m\\}$. The sets $\\{A_j\\}_{j=1}^m$ form a partition of $\\Omega$, so for any $x \\in \\mathbb{R}$, the PDF can be expressed as:\n$$\nf_I(x) = \\sum_{j=1}^{m} f_{I|A_j}(x) \\cdot \\mathbb{P}(A_j)\n$$\nwhere $f_{I|A_j}(x)$ is the conditional PDF of $I$ given the event $A_j$, and $\\mathbb{P}(A_j) = p_j$.\n\nLet us determine the conditional distribution of $I$ given $A_j$. When we condition on the event $A_j$, the indicator function $\\mathbf{1}_{A_j}$ becomes $1$, and $\\mathbf{1}_{A_k}$ becomes $0$ for all $k \\neq j$. Therefore, the random variable $E$ takes the constant value $a_j$:\n$$\nE \\big|_{A_j} = a_j.\n$$\nConsequently, the conditional random variable $I$ given $A_j$ is:\n$$\nI \\big|_{A_j} = a_j W_T.\n$$\nA standard Brownian motion $W_T$ at time $T$ follows a normal distribution with mean $0$ and variance $T$, i.e., $W_T \\sim \\mathcal{N}(0, T)$.\nThe random variable $E$ is $\\mathcal{F}_0$-measurable, while the increment $W_T = W_T - W_0$ is independent of the $\\sigma$-algebra $\\mathcal{F}_0$. Thus, $E$ and $W_T$ are independent random variables.\n\nFor a normally distributed random variable $X \\sim \\mathcal{N}(\\mu, \\sigma^2)$ and a constant $c$, the scaled variable $cX$ is distributed as $c X \\sim \\mathcal{N}(c\\mu, c^2\\sigma^2)$. Applying this property to $I \\big|_{A_j} = a_j W_T$, we have:\n$$\nI \\big|_{A_j} \\sim \\mathcal{N}(a_j \\cdot 0, a_j^2 T) = \\mathcal{N}(0, a_j^2 T).\n$$\nThe constants $a_j$ are non-zero, so the variance $a_j^2 T$ is strictly positive.\n\nThe PDF of a normal distribution $\\mathcal{N}(\\mu, \\sigma^2)$ is given by $\\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right)$. For the conditional distribution of $I$ given $A_j$, the PDF is:\n$$\nf_{I|A_j}(x) = \\frac{1}{\\sqrt{2\\pi (a_j^2 T)}} \\exp\\left(-\\frac{(x-0)^2}{2 a_j^2 T}\\right) = \\frac{1}{\\sqrt{a_j^2} \\sqrt{2\\pi T}} \\exp\\left(-\\frac{x^2}{2 a_j^2 T}\\right).\n$$\nSince $\\sqrt{a_j^2} = |a_j|$, this becomes:\n$$\nf_{I|A_j}(x) = \\frac{1}{|a_j| \\sqrt{2\\pi T}} \\exp\\left(-\\frac{x^2}{2 a_j^2 T}\\right).\n$$\n\n**Step 4: Final Expression for the Unconditional PDF**\nFinally, we substitute the conditional PDFs and the probabilities $p_j$ into the law of total probability formula:\n$$\nf_I(x) = \\sum_{j=1}^{m} p_j \\cdot f_{I|A_j}(x) = \\sum_{j=1}^{m} p_j \\left( \\frac{1}{|a_j| \\sqrt{2\\pi T}} \\exp\\left(-\\frac{x^2}{2 a_j^2 T}\\right) \\right).\n$$\nWe can factor out the common term $\\frac{1}{\\sqrt{2\\pi T}}$:\n$$\nf_I(x) = \\frac{1}{\\sqrt{2\\pi T}} \\sum_{j=1}^{m} \\frac{p_j}{|a_j|} \\exp\\left(-\\frac{x^2}{2 a_j^2 T}\\right).\n$$\nThis is the PDF of a mixture of $m$ normal distributions, each with mean $0$ and variance $a_j^2 T$, weighted by the probabilities $p_j$. This provides the complete, closed-form analytic expression for the unconditional PDF of $I$.", "answer": "$$\n\\boxed{\\frac{1}{\\sqrt{2\\pi T}} \\sum_{j=1}^{m} \\frac{p_{j}}{|a_{j}|} \\exp\\left(-\\frac{x^{2}}{2 a_{j}^{2} T}\\right)}\n$$", "id": "2981993"}]}