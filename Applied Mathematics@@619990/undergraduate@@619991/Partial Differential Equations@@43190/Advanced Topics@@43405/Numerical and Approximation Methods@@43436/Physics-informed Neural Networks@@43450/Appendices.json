{"hands_on_practices": [{"introduction": "This first practice lays the groundwork for how a Physics-Informed Neural Network (PINN) learns by translating a physical system into a trainable objective. We will use the classic Poisson's equation, which describes phenomena like electrostatic potentials, to construct a loss function that encodes both the governing partial differential equation and the associated boundary conditions. This exercise [@problem_id:2126324] provides a fundamental, hands-on understanding of how physical laws are directly embedded into the neural network's training process.", "problem": "A researcher is building a Physics-Informed Neural Network (PINN) to find an approximate solution for the electrostatic potential, $V(x,y)$, within a two-dimensional square region. The physical behavior of the potential is described by the Poisson equation:\n$$\n\\nabla^2 V(x,y) = -f(x,y)\n$$\nwhere $f(x,y)$ represents a given charge distribution density and $\\nabla^2 = \\frac{\\partial^2}{\\partial x^2} + \\frac{\\partial^2}{\\partial y^2}$ is the Laplace operator. The potential is defined over the domain $D = \\{(x,y) \\mid -L \\le x \\le L, -L \\le y \\le L\\}$. The boundary of this domain, $\\partial D$, is held at a zero potential (grounded), which imposes the boundary condition $V(x,y) = 0$ for all $(x,y) \\in \\partial D$.\n\nThe PINN model, denoted by $\\hat{V}(x,y; \\theta)$, learns to approximate $V(x,y)$ by minimizing a loss function $L(\\theta)$ that incorporates the physics of the problem. Here, $\\theta$ represents all the trainable parameters of the neural network. The loss function is calculated using two sets of discrete points:\n1.  A set of $N_{pde}$ collocation points, $S_{pde} = \\{(x_i, y_i) \\mid i=1, \\dots, N_{pde}\\}$, located in the interior of the domain $D$.\n2.  A set of $N_{bc}$ boundary points, $S_{bc} = \\{(x_j, y_j) \\mid j=1, \\dots, N_{bc}\\}$, located on the boundary $\\partial D$.\n\nThe total loss function, $L(\\theta)$, is the sum of two mean squared error terms: one for the governing partial differential equation ($L_{pde}$) and one for the boundary conditions ($L_{bc}$).\n\nConstruct the mathematical expression for the total loss function $L(\\theta) = L_{pde} + L_{bc}$. Your expression should be in terms of the network's output $\\hat{V}$, its second partial derivatives, the function $f$, the given point sets, and their respective sizes $N_{pde}$ and $N_{bc}$.", "solution": "We begin from the governing Poisson equation and boundary condition:\n$$\n\\nabla^{2}V(x,y) = -f(x,y), \\quad V(x,y) = 0 \\text{ for } (x,y)\\in \\partial D.\n$$\nA Physics-Informed Neural Network approximates $V$ by $\\hat{V}(x,y;\\theta)$. The PDE residual at an interior collocation point $(x_{i},y_{i})\\in S_{pde}$ is defined by imposing the Poisson equation on $\\hat{V}$:\n$$\nr_{i}(\\theta) = \\nabla^{2}\\hat{V}(x_{i},y_{i};\\theta)+f(x_{i},y_{i}).\n$$\nUsing the definition of the Laplacian in two dimensions, this is equivalently\n$$\nr_{i}(\\theta) = \\frac{\\partial^{2}\\hat{V}}{\\partial x^{2}}(x_{i},y_{i};\\theta)+\\frac{\\partial^{2}\\hat{V}}{\\partial y^{2}}(x_{i},y_{i};\\theta)+f(x_{i},y_{i}).\n$$\nThe mean squared error enforcing the PDE over $S_{pde}$ is then\n$$\nL_{pde}(\\theta) = \\frac{1}{N_{pde}}\\sum_{i=1}^{N_{pde}}\\left(r_{i}(\\theta)\\right)^{2}=\\frac{1}{N_{pde}}\\sum_{i=1}^{N_{pde}}\\left(\\frac{\\partial^{2}\\hat{V}}{\\partial x^{2}}(x_{i},y_{i};\\theta)+\\frac{\\partial^{2}\\hat{V}}{\\partial y^{2}}(x_{i},y_{i};\\theta)+f(x_{i},y_{i})\\right)^{2}.\n$$\nThe boundary condition $V=0$ on $\\partial D$ is enforced by penalizing the deviation of $\\hat{V}$ from zero at boundary points $(x_{j},y_{j})\\in S_{bc}$:\n$$\nL_{bc}(\\theta) = \\frac{1}{N_{bc}}\\sum_{j=1}^{N_{bc}}\\left(\\hat{V}(x_{j},y_{j};\\theta)-0\\right)^{2}=\\frac{1}{N_{bc}}\\sum_{j=1}^{N_{bc}}\\left(\\hat{V}(x_{j},y_{j};\\theta)\\right)^{2}.\n$$\nTherefore, the total loss is the sum of the two mean squared error terms:\n$$\nL(\\theta) = L_{pde}(\\theta)+L_{bc}(\\theta) = \\frac{1}{N_{pde}}\\sum_{i=1}^{N_{pde}}\\left(\\frac{\\partial^{2}\\hat{V}}{\\partial x^{2}}(x_{i},y_{i};\\theta)+\\frac{\\partial^{2}\\hat{V}}{\\partial y^{2}}(x_{i},y_{i};\\theta)+f(x_{i},y_{i})\\right)^{2}+\\frac{1}{N_{bc}}\\sum_{j=1}^{N_{bc}}\\left(\\hat{V}(x_{j},y_{j};\\theta)\\right)^{2}.\n$$", "answer": "$$\\boxed{\\frac{1}{N_{pde}}\\sum_{i=1}^{N_{pde}}\\left(\\frac{\\partial^{2}\\hat{V}}{\\partial x^{2}}(x_{i},y_{i};\\theta)+\\frac{\\partial^{2}\\hat{V}}{\\partial y^{2}}(x_{i},y_{i};\\theta)+f(x_{i},y_{i})\\right)^{2}+\\frac{1}{N_{bc}}\\sum_{j=1}^{N_{bc}}\\left(\\hat{V}(x_{j},y_{j};\\theta)\\right)^{2}}$$", "id": "2126324"}, {"introduction": "Building on static problems, our next step is to tackle systems that evolve over time, which introduces the crucial element of the system's initial state. Using the one-dimensional heat equation as our model system, we will see how the total loss function for a time-dependent problem must incorporate three distinct components: the governing physics, the boundary conditions, and the initial condition. This practice [@problem_id:2126339] reinforces that a PINN must learn a solution that is valid across the entire spacetime domain, correctly capturing the system's evolution from a specified starting point.", "problem": "A Physics-Informed Neural Network (PINN) is a machine learning model designed to find an approximate solution $\\hat{u}(x, t; \\theta)$ to a partial differential equation, where $\\theta$ represents the trainable parameters (weights and biases) of the neural network. The training process involves minimizing a loss function $L(\\theta)$ that measures how well the network's output satisfies the entire problem, including the governing equation and its associated conditions.\n\nConsider the one-dimensional heat equation defined for a function $u(x,t)$ over the spatial domain $x \\in [-L, L]$ and time domain $t \\in [0, T]$:\n$$\n\\frac{\\partial u}{\\partial t} = \\alpha \\frac{\\partial^2 u}{\\partial x^2}\n$$\nThis equation is subject to a discontinuous initial condition and fixed boundary conditions:\n1.  **Initial Condition (IC):** $u(x, 0) = H(x)$ for $x \\in [-L, L]$, where $H(x)$ is the Heaviside step function, defined as $H(x) = 1$ for $x \\geq 0$ and $H(x) = 0$ for $x < 0$.\n2.  **Boundary Conditions (BC):** $u(-L, t) = 0$ and $u(L, t) = 1$ for $t \\in [0, T]$.\n\nTo train the PINN, we define three separate loss components based on the mean squared error (MSE) calculated over different sets of sample points:\n-   $L_{PDE}$: The MSE of the PDE's residual, $\\left(\\frac{\\partial \\hat{u}}{\\partial t} - \\alpha \\frac{\\partial^2 \\hat{u}}{\\partial x^2}\\right)$, calculated over a set of $N_f$ collocation points sampled from the interior of the spacetime domain, $(x, t) \\in (-L, L) \\times (0, T]$.\n-   $L_{IC}$: The MSE between the network's prediction $\\hat{u}(x, 0)$ and the true initial condition $H(x)$, calculated over a set of $N_i$ points sampled from the initial time line, $(x, 0)$ for $x \\in [-L, L]$.\n-   $L_{BC}$: The MSE between the network's prediction and the true values at the spatial boundaries, calculated over a set of $N_b$ points sampled from the boundary lines, $(-L, t)$ and $(L, t)$ for $t \\in [0, T]$.\n\nWhich of the following expressions represents the correct total loss function $L(\\theta)$ that must be minimized to train the PINN to find the specific solution to this heat transfer problem?\n\nA. $L(\\theta) = L_{PDE} + L_{IC} + L_{BC}$\n\nB. $L(\\theta) = L_{IC} + L_{BC}$\n\nC. $L(\\theta) = L_{PDE}$\n\nD. $L(\\theta) = L_{PDE} + L_{BC}$\n\nE. $L(\\theta) = L_{PDE} + L_{IC}$", "solution": "We seek a network $\\hat{u}(x,t;\\theta)$ that satisfies the initial-boundary value problem for the heat equation by minimizing a loss that enforces:\n- The governing PDE in the interior $(x,t) \\in (-L,L) \\times (0,T]$.\n- The initial condition at $t=0$ across $x \\in [-L,L]$.\n- The boundary conditions at $x=\\pm L$ across $t \\in [0,T]$.\n\nDefine the PDE residual\n$$\nr(x,t;\\theta) = \\frac{\\partial \\hat{u}}{\\partial t}(x,t;\\theta)-\\alpha\\frac{\\partial^{2}\\hat{u}}{\\partial x^{2}}(x,t;\\theta).\n$$\nUsing mean squared error over the respective sample sets, the three losses are\n$$\nL_{PDE}(\\theta) = \\frac{1}{N_{f}}\\sum_{j=1}^{N_{f}}\\left[r\\left(x^{f}_{j},t^{f}_{j};\\theta\\right)\\right]^{2},\n$$\n$$\nL_{IC}(\\theta) = \\frac{1}{N_{i}}\\sum_{j=1}^{N_{i}}\\left[\\hat{u}\\left(x^{i}_{j},0;\\theta\\right)-H\\left(x^{i}_{j}\\right)\\right]^{2},\n$$\n$$\nL_{BC}(\\theta) = \\frac{1}{N_{b}}\\sum_{j=1}^{N_{b}}\\left\\{\\left[\\hat{u}\\left(-L,t^{b}_{j};\\theta\\right)-0\\right]^{2}+\\left[\\hat{u}\\left(L,t^{b}_{j};\\theta\\right)-1\\right]^{2}\\right\\}.\n$$\nTo identify the specific solution of the initial-boundary value problem, the minimization must enforce all three constraints simultaneously. The PDE alone does not fix the solution without IC and BC; omitting any of IC or BC leaves the solution non-unique or inconsistent with the posed problem. Therefore, the correct total loss is the sum of all three components:\n$$\nL(\\theta) = L_{PDE} + L_{IC} + L_{BC}.\n$$\nAmong the given options, this corresponds to option A.", "answer": "$$\\boxed{A}$$", "id": "2126339"}, {"introduction": "Advanced PINN applications often go beyond simply enforcing the differential equation by incorporating deeper physical principles, such as conservation laws. In this problem, we will explore how to explicitly enforce the law of energy conservation, a known property of the wave equation, by adding a corresponding penalty term to the loss function. This advanced exercise [@problem_id:2126322] demonstrates how to construct a more sophisticated and physically robust loss function, a technique that can significantly improve the accuracy and long-term stability of the learned solution.", "problem": "A Physics-Informed Neural Network (PINN) is a machine learning model used to find an approximate solution $\\hat{u}(x,t; \\theta)$ to a Partial Differential Equation (PDE), where $\\theta$ represents the trainable parameters of the network. The training process minimizes a loss function that penalizes deviations from known physical laws, initial conditions, and boundary conditions.\n\nConsider the one-dimensional linear wave equation on a spatio-temporal domain $(x,t) \\in [x_L, x_R] \\times [0, T]$:\n$$ \\frac{\\partial^2 u}{\\partial t^2} - c^2 \\frac{\\partial^2 u}{\\partial x^2} = 0 $$\nwith a constant wave speed $c$. The system is subject to the initial conditions $u(x,0) = g(x)$ and $\\frac{\\partial u}{\\partial t}(x,0) = h(x)$, and boundary conditions $u(x_L, t) = u_L(t)$ and $u(x_R, t) = u_R(t)$.\n\nA known property of the wave equation is the conservation of total energy, defined as:\n$$ E(t) = \\frac{1}{2} \\int_{x_L}^{x_R} \\left( \\left(\\frac{\\partial u}{\\partial t}\\right)^2 + c^2 \\left(\\frac{\\partial u}{\\partial x}\\right)^2 \\right) dx $$\nFor any valid solution, $E(t)$ must remain constant and equal to its initial value $E(0)$ for all $t \\in [0,T]$.\n\nYour task is to construct a modified total loss function, $\\mathcal{L}_{\\text{total}}$, that explicitly incorporates a penalty for any violation of this energy conservation law. The total loss is a weighted sum of four components:\n$$ \\mathcal{L}_{\\text{total}} = \\lambda_{\\text{PDE}}\\mathcal{L}_{\\text{PDE}} + \\lambda_{\\text{IC}}\\mathcal{L}_{\\text{IC}} + \\lambda_{\\text{BC}}\\mathcal{L}_{\\text{BC}} + \\lambda_{\\text{E}}\\mathcal{L}_{\\text{E}} $$\nwhere the $\\lambda$ terms are positive weighting hyperparameters.\n\nDefine the components of the loss function based on the following sets of collocation points, formulating each component as a Mean Squared Error (MSE):\n- **PDE Loss $\\mathcal{L}_{\\text{PDE}}$**: Evaluated over a set of $N_{\\text{PDE}}$ points $\\mathcal{S}_{\\text{PDE}} = \\{(x_i, t_i)\\}_{i=1}^{N_{\\text{PDE}}}$ in the interior of the domain.\n- **Initial Condition Loss $\\mathcal{L}_{\\text{IC}}$**: Evaluated over a set of $N_{\\text{IC}}$ points $\\mathcal{S}_{\\text{IC}} = \\{(x_j, 0)\\}_{j=1}^{N_{\\text{IC}}}$ on the initial time line. The MSE should account for errors in both $u$ and its time derivative $u_t$.\n- **Boundary Condition Loss $\\mathcal{L}_{\\text{BC}}$**: Evaluated over a set of $N_{\\text{BC}}$ points $\\mathcal{S}_{\\text{BC}} = \\{(x_k, t_k)\\}_{k=1}^{N_{\\text{BC}}}$ on the spatial boundaries $x \\in \\{x_L, x_R\\}$. For a point $(x_k, t_k) \\in \\mathcal{S}_{\\text{BC}}$, the target boundary value is $u_{\\text{BC}}(x_k, t_k)$, which is equal to $u_L(t_k)$ if $x_k = x_L$ and $u_R(t_k)$ if $x_k = x_R$.\n- **Energy Conservation Loss $\\mathcal{L}_{\\text{E}}$**: This loss penalizes the deviation of the network's predicted energy at various times from the true initial energy. The energy integral is to be approximated using a midpoint Riemann sum over $N_E$ uniform subintervals of $[x_L, x_R]$, each of width $\\Delta x = (x_R - x_L)/N_E$. The spatial evaluation points for this sum are $\\{x_m = x_L + (m - 1/2)\\Delta x\\}_{m=1}^{N_E}$. The conservation is enforced at a set of $N_T$ time instances $\\mathcal{T}_E = \\{t_l\\}_{l=1}^{N_T}$ where $t_l > 0$.\n\nLet the neural network's approximation to the solution be $\\hat{u}(x,t; \\theta)$, and its partial derivatives with respect to $x$ and $t$ be denoted by $\\hat{u}_x$, $\\hat{u}_t$, $\\hat{u}_{xx}$, and $\\hat{u}_{tt}$. Assume the functions $g(x)$, $h(x)$, and the spatial derivative $g'(x)$ are known and can be evaluated at the required points. Construct the full expression for $\\mathcal{L}_{\\text{total}}$. Your final answer should be a single analytical expression in terms of the network outputs (e.g., $\\hat{u}(x,t)$), its derivatives, the given functions, the specified points, and the weighting factors.", "solution": "We seek a total loss that penalizes violations of the PDE, initial conditions, boundary conditions, and conservation of energy. For a PINN approximation $\\hat{u}(x,t;\\theta)$, define the PDE residual at interior collocation points $(x_{i},t_{i}) \\in \\mathcal{S}_{\\text{PDE}}$ by the wave equation:\n$$\nr(x_{i},t_{i};\\theta) = \\hat{u}_{tt}(x_{i},t_{i};\\theta) - c^{2}\\hat{u}_{xx}(x_{i},t_{i};\\theta).\n$$\nThe PDE loss is the mean squared residual:\n$$\n\\mathcal{L}_{\\text{PDE}} = \\frac{1}{N_{\\text{PDE}}}\\sum_{i=1}^{N_{\\text{PDE}}} \\left(r(x_{i},t_{i};\\theta)\\right)^{2} = \\frac{1}{N_{\\text{PDE}}}\\sum_{i=1}^{N_{\\text{PDE}}} \\left(\\hat{u}_{tt}(x_{i},t_{i};\\theta) - c^{2}\\hat{u}_{xx}(x_{i},t_{i};\\theta)\\right)^{2}.\n$$\nFor the initial conditions at $(x_{j},0) \\in \\mathcal{S}_{\\text{IC}}$, the target values are $u(x_{j},0) = g(x_{j})$ and $u_{t}(x_{j},0) = h(x_{j})$. The initial condition loss is the mean squared error over both $u$ and $u_{t}$:\n$$\n\\mathcal{L}_{\\text{IC}} = \\frac{1}{2N_{\\text{IC}}}\\sum_{j=1}^{N_{\\text{IC}}} \\left[\\left(\\hat{u}(x_{j},0;\\theta)-g(x_{j})\\right)^{2} + \\left(\\hat{u}_{t}(x_{j},0;\\theta)-h(x_{j})\\right)^{2}\\right].\n$$\nFor boundary conditions at $(x_{k},t_{k}) \\in \\mathcal{S}_{\\text{BC}}$ with $x_{k} \\in \\{x_{L},x_{R}\\}$, the target boundary value is $u_{\\text{BC}}(x_{k},t_{k})$ (equal to $u_{L}(t_{k})$ if $x_{k} = x_{L}$ and $u_{R}(t_{k})$ if $x_{k} = x_{R}$). The boundary loss is the mean squared discrepancy:\n$$\n\\mathcal{L}_{\\text{BC}} = \\frac{1}{N_{\\text{BC}}}\\sum_{k=1}^{N_{\\text{BC}}} \\left(\\hat{u}(x_{k},t_{k};\\theta) - u_{\\text{BC}}(x_{k},t_{k})\\right)^{2}.\n$$\nEnergy conservation requires $E(t) = E(0)$ for all $t$. We approximate the energy integral by a midpoint Riemann sum over $N_{E}$ uniform subintervals of $[x_{L},x_{R}]$ with spatial midpoints $x_{m} = x_{L}+\\left(m-\\frac{1}{2}\\right)\\frac{x_{R}-x_{L}}{N_{E}}$ and width $\\Delta x = \\frac{x_{R}-x_{L}}{N_{E}}$. The network’s predicted energy at a time $t_{l} \\in \\mathcal{T}_{E}$ is\n$$\n\\widehat{E}(t_{l}) = \\frac{1}{2}\\frac{x_{R}-x_{L}}{N_{E}} \\sum_{m=1}^{N_{E}}\\left(\\hat{u}_{t}\\left(x_{L}+\\left(m-\\frac{1}{2}\\right)\\frac{x_{R}-x_{L}}{N_{E}},t_{l};\\theta\\right)^{2} + c^{2}\\hat{u}_{x}\\left(x_{L}+\\left(m-\\frac{1}{2}\\right)\\frac{x_{R}-x_{L}}{N_{E}},t_{l};\\theta\\right)^{2}\\right).\n$$\nThe true initial energy, computed from the known initial data using the same quadrature points, is\n$$\nE_{0} = \\frac{1}{2}\\frac{x_{R}-x_{L}}{N_{E}} \\sum_{m=1}^{N_{E}}\\left(h\\left(x_{L}+\\left(m-\\frac{1}{2}\\right)\\frac{x_{R}-x_{L}}{N_{E}}\\right)^{2} + c^{2}\\left(g'\\left(x_{L}+\\left(m-\\frac{1}{2}\\right)\\frac{x_{R}-x_{L}}{N_{E}}\\right)\\right)^{2}\\right).\n$$\nThe energy conservation loss is the mean squared deviation of $\\widehat{E}(t_{l})$ from $E_{0}$ over the selected time instances:\n$$\n\\mathcal{L}_{\\text{E}} = \\frac{1}{N_{T}}\\sum_{l=1}^{N_{T}}\\left(\\widehat{E}(t_{l}) - E_{0}\\right)^{2}.\n$$\nCombining all four components with positive weights $\\lambda_{\\text{PDE}},\\lambda_{\\text{IC}},\\lambda_{\\text{BC}},\\lambda_{\\text{E}}$ yields the total loss:\n$$\n\\mathcal{L}_{\\text{total}} = \\lambda_{\\text{PDE}}\\mathcal{L}_{\\text{PDE}} + \\lambda_{\\text{IC}}\\mathcal{L}_{\\text{IC}} + \\lambda_{\\text{BC}}\\mathcal{L}_{\\text{BC}} + \\lambda_{\\text{E}}\\mathcal{L}_{\\text{E}}.\n$$\nSubstituting the explicit forms of all components gives a single analytical expression in terms of $\\hat{u}$, its derivatives, the given data, the collocation points, and the weights as required.", "answer": "$$\\boxed{\\lambda_{\\text{PDE}} \\frac{1}{N_{\\text{PDE}}}\\sum_{i=1}^{N_{\\text{PDE}}}\\left(\\hat{u}_{tt}(x_{i},t_{i};\\theta)-c^{2}\\hat{u}_{xx}(x_{i},t_{i};\\theta)\\right)^{2}+\\lambda_{\\text{IC}}\\frac{1}{2N_{\\text{IC}}}\\sum_{j=1}^{N_{\\text{IC}}}\\left[\\left(\\hat{u}(x_{j},0;\\theta)-g(x_{j})\\right)^{2}+\\left(\\hat{u}_{t}(x_{j},0;\\theta)-h(x_{j})\\right)^{2}\\right]+\\lambda_{\\text{BC}}\\frac{1}{N_{\\text{BC}}}\\sum_{k=1}^{N_{\\text{BC}}}\\left(\\hat{u}(x_{k},t_{k};\\theta)-u_{\\text{BC}}(x_{k},t_{k})\\right)^{2}+\\lambda_{\\text{E}}\\frac{1}{N_{T}}\\sum_{l=1}^{N_{T}}\\left(\\frac{1}{2}\\frac{x_{R}-x_{L}}{N_{E}}\\sum_{m=1}^{N_{E}}\\left[\\hat{u}_{t}\\left(x_{L}+\\left(m-\\frac{1}{2}\\right)\\frac{x_{R}-x_{L}}{N_{E}},t_{l};\\theta\\right)^{2}+c^{2}\\hat{u}_{x}\\left(x_{L}+\\left(m-\\frac{1}{2}\\right)\\frac{x_{R}-x_{L}}{N_{E}},t_{l};\\theta\\right)^{2}\\right]-\\frac{1}{2}\\frac{x_{R}-x_{L}}{N_{E}}\\sum_{m=1}^{N_{E}}\\left[h\\left(x_{L}+\\left(m-\\frac{1}{2}\\right)\\frac{x_{R}-x_{L}}{N_{E}}\\right)^{2}+c^{2}\\left(g'\\left(x_{L}+\\left(m-\\frac{1}{2}\\right)\\frac{x_{R}-x_{L}}{N_{E}}\\right)\\right)^{2}\\right]\\right)^{2}}$$", "id": "2126322"}]}