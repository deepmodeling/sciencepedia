{"hands_on_practices": [{"introduction": "Let's begin with the most fundamental aspect of a branching process: its expected size. This exercise provides a hands-on way to apply the core formula for the mean population size, demonstrating how it predicts the average trajectory of a population over time. Understanding this expected behavior is the first step in analyzing any branching process, from modeling cell division to the spread of information. [@problem_id:1317895]", "problem": "A team of conservation biologists is studying the population dynamics of a newly discovered, extremely rare species of bioluminescent fungus. They model the growth of the fungal population using a Galton-Watson branching process. Let $X_n$ represent the number of distinct fungal colonies in generation $n$. The study begins with a single viable colony, so the population at generation zero is $X_0 = 1$.\n\nEach colony, after completing its life cycle, produces a random number of new offspring colonies. Based on extensive lab observations, the biologists have determined that the mean number of offspring produced by any single colony is $\\mu = 0.75$.\n\nAssuming the process starts with this single colony, calculate the expected number of fungal colonies in the 6th generation. Report your answer as a numerical value, rounded to four significant figures.", "solution": "We are given a Galton-Watson branching process with $X_{0}=1$ and mean offspring number $\\mu=0.75$. In a Galton-Watson process, the expected population size satisfies the recursion\n$$\n\\mathbb{E}[X_{n+1}\\mid X_{n}] = \\mu X_{n}.\n$$\nTaking expectations and using the law of total expectation gives\n$$\n\\mathbb{E}[X_{n+1}] = \\mathbb{E}[\\mathbb{E}[X_{n+1}\\mid X_{n}]] = \\mathbb{E}[\\mu X_{n}] = \\mu\\,\\mathbb{E}[X_{n}].\n$$\nWith the initial condition $\\mathbb{E}[X_{0}] = X_{0} = 1$, this first-order linear recurrence solves to\n$$\n\\mathbb{E}[X_{n}] = \\mu^{n}.\n$$\nFor $n=6$ and $\\mu=0.75$, we have\n$$\n\\mathbb{E}[X_{6}] = (0.75)^{6} = \\left(\\frac{3}{4}\\right)^{6} = \\frac{3^{6}}{4^{6}} = \\frac{729}{4096} = 0.177978515625.\n$$\nRounded to four significant figures, this is $0.1780$.", "answer": "$$\\boxed{0.1780}$$", "id": "1317895"}, {"introduction": "While the mean gives us an average, it doesn't capture the inherent randomness and potential for wild fluctuations in population size. This practice introduces the concept of variance, a key measure of this volatility. By calculating both the mean and variance, you will gain a more complete picture of the population's future, appreciating not just its expected path but also the uncertainty surrounding it. [@problem_id:1317871]", "problem": "In a materials science experiment, a single, newly-designed self-replicating nanobot is introduced into a suitable growth medium at generation $n=0$. In each subsequent generation, every nanobot present produces a random number of new, identical nanobots before it becomes inert. The number of 'offspring' produced by any single nanobot is independent of all others and follows a Poisson distribution with a mean of $\\lambda = 1.5$.\n\nLet $X_n$ denote the total number of nanobots present in generation $n$. Starting with $X_0=1$, calculate the mean and the variance of the nanobot population size at generation $n=3$.\n\nReport your answers for the mean and the variance, in that order, as a pair of numerical values rounded to four significant figures.", "solution": "We model the population $\\{X_{n}\\}_{n \\geq 0}$ as a Galton–Watson branching process with one initial ancestor $X_{0}=1$. Each individual produces an independent number of offspring, let's call it $Y$, with $Y \\sim \\text{Poisson}(\\lambda)$, so\n$$\n\\mathbb{E}[Y]=\\lambda,\\qquad \\operatorname{Var}(Y)=\\lambda.\n$$\nGiven $X_{n}$, the next generation is\n$$\nX_{n+1}=\\sum_{i=1}^{X_{n}} Y_{n,i},\n$$\nwhere $\\{Y_{n,i}\\}$ are i.i.d. copies of $Y$, independent of $X_{n}$. Conditional on $X_{n}$, $X_{n+1}$ is a sum of $X_{n}$ independent Poisson$\\left(\\lambda\\right)$ variables, hence\n$$\n\\mathbb{E}[X_{n+1}\\mid X_{n}] = \\lambda X_{n},\\qquad \\operatorname{Var}(X_{n+1}\\mid X_{n})=\\lambda X_{n}.\n$$\n\nMean. Taking expectations gives the recursion\n$$\n\\mathbb{E}[X_{n+1}] = \\lambda \\mathbb{E}[X_{n}],\\quad \\mathbb{E}[X_{0}]=1,\n$$\nwhose solution is\n$$\n\\mathbb{E}[X_{n}] = \\lambda^{n}.\n$$\nThus for $n=3$,\n$$\n\\mathbb{E}[X_{3}] = \\lambda^{3}.\n$$\n\nVariance. By the law of total variance,\n$$\n\\operatorname{Var}(X_{n+1}) = \\mathbb{E}[\\operatorname{Var}(X_{n+1}\\mid X_{n})] + \\operatorname{Var}(\\mathbb{E}[X_{n+1}\\mid X_{n}])\n= \\mathbb{E}[\\lambda X_{n}] + \\operatorname{Var}(\\lambda X_{n})\n= \\lambda \\mathbb{E}[X_{n}] + \\lambda^{2} \\operatorname{Var}(X_{n}).\n$$\nSince $\\mathbb{E}[X_{n}] = \\lambda^{n}$ and $\\operatorname{Var}(X_{0})=0$, we obtain the recursion\n$$\n\\operatorname{Var}(X_{n+1}) = \\lambda^{n+1} + \\lambda^{2} \\operatorname{Var}(X_{n}).\n$$\nIterating up to $n=3$:\n$$\n\\operatorname{Var}(X_{1}) = \\lambda^{1},\n$$\n$$\n\\operatorname{Var}(X_{2}) = \\lambda^{2} + \\lambda^{2}\\operatorname{Var}(X_{1}) = \\lambda^{2} + \\lambda^{3},\n$$\n$$\n\\operatorname{Var}(X_{3}) = \\lambda^{3} + \\lambda^{2}\\operatorname{Var}(X_{2}) = \\lambda^{3} + \\lambda^{2}(\\lambda^{2} + \\lambda^{3}) = \\lambda^{3} + \\lambda^{4} + \\lambda^{5}.\n$$\n\nWith $\\lambda=1.5$, the mean and variance at generation $n=3$ are\n$$\n\\mathbb{E}[X_{3}] = (1.5)^{3} = 3.375,\n$$\n$$\n\\operatorname{Var}(X_{3}) = (1.5)^{3} + (1.5)^{4} + (1.5)^{5} = 16.03125.\n$$\nRounded to four significant figures, these are $3.375$ and $16.03$.", "answer": "$$\\boxed{\\begin{pmatrix}3.375 & 16.03\\end{pmatrix}}$$", "id": "1317871"}, {"introduction": "Often, we start with a story about how individuals reproduce, not with a pre-packaged mean $\\mu$ and variance $\\sigma^2$. This problem demonstrates a powerful technique using Probability Generating Functions (PGFs) to derive these crucial parameters from the underlying rules of offspring generation. This exercise connects the microscopic details of reproduction to the macroscopic dynamics of the population, a cornerstone of stochastic modeling. [@problem_id:1317910]", "problem": "In a simplified model of fault propagation within a large-scale distributed computing network, an initial fault in a single node at generation 0 ($X_0 = 1$) can spread to other nodes in discrete time steps. A faulty node, during a single time step, attempts to communicate with $k$ other distinct, healthy nodes. For each of these $k$ communications, there is a probability $p$ that the fault is successfully transmitted, with each transmission being an independent event. After this propagation step, the source node is isolated and considered removed. The number of new nodes that become faulty, caused by a single faulty node, is a random variable whose statistical properties are described by its Probability Generating Function (PGF), $G(s)$.\n\nFor this specific propagation model, the PGF of the offspring distribution (i.e., the number of new faulty nodes generated by a single faulty node) is given by:\n$$G(s) = (1-p+ps)^k$$\nwhere $k$ is a positive integer and $0 < p < 1$.\n\nLet $X_n$ denote the number of faulty nodes in generation $n$. Assuming the process starts with a single faulty node, $X_0 = 1$, determine a closed-form expression for the variance of the number of faulty nodes at generation 2, $Var(X_2)$, in terms of the parameters $p$ and $k$.", "solution": "The process is a Galton–Watson branching process with offspring PGF $G(s)=(1-p+ps)^{k}$. For an offspring distribution with PGF $G$, the mean and variance of the number of offspring $Z$ are given by\n$$\\mathbb{E}[Z]=G'(1), \\quad \\mathrm{Var}(Z)=G''(1)+G'(1)-\\left(G'(1)\\right)^{2}.$$\nCompute derivatives:\n$$G'(s)=k p (1-p+p s)^{k-1}, \\quad G''(s)=k (k-1) p^{2} (1-p+p s)^{k-2}.$$\nEvaluating at $s=1$ gives\n$$\\mu \\equiv \\mathbb{E}[Z]=G'(1)=k p, \\quad G''(1)=k (k-1) p^{2},$$\nso\n$$\\sigma^{2} \\equiv \\mathrm{Var}(Z)=k (k-1) p^{2}+k p - (k p)^{2}=k p (1-p).$$\nWith $X_{0}=1$, we have $X_{1}\\sim$ offspring distribution, hence $\\mathbb{E}[X_{1}]=\\mu$ and $\\mathrm{Var}(X_{1})=\\sigma^{2}$. The generation-$2$ count can be written as\n$$X_{2}=\\sum_{i=1}^{X_{1}} Z_{i},$$\nwhere the $Z_{i}$ are independent copies of the offspring variable, independent of $X_{1}$. Then\n$$\\mathbb{E}[X_{2}\\mid X_{1}]=\\mu X_{1}, \\quad \\mathrm{Var}(X_{2}\\mid X_{1})=\\sigma^{2} X_{1}.$$\nUsing the law of total expectation and the law of total variance,\n$$\\mathbb{E}[X_{2}]=\\mathbb{E}[\\,\\mathbb{E}[X_{2}\\mid X_{1}]\\,]=\\mu\\,\\mathbb{E}[X_{1}]=\\mu^{2},$$\n$$\\mathrm{Var}(X_{2})=\\mathbb{E}[\\mathrm{Var}(X_{2}\\mid X_{1})]+\\mathrm{Var}(\\mathbb{E}[X_{2}\\mid X_{1}])=\\sigma^{2}\\mathbb{E}[X_{1}]+\\mu^{2}\\mathrm{Var}(X_{1})=\\sigma^{2} \\mu + \\mu^{2}\\sigma^{2}=\\sigma^{2} \\mu (1+\\mu).$$\nSubstituting $\\mu=k p$ and $\\sigma^{2}=k p (1-p)$ yields\n$$\\mathrm{Var}(X_{2})=k p (1-p)\\,k p\\,(1+k p)=k^{2} p^{2} (1-p)(1+k p).$$", "answer": "$$\\boxed{k^{2} p^{2} (1-p)\\left(1+k p\\right)}$$", "id": "1317910"}]}