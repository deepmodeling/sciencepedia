{"hands_on_practices": [{"introduction": "The Autoregressive (AR) model is a cornerstone of time series analysis, capturing how a variable's current value depends on its past values. This first exercise explores the simplest case, the AR(1) process, where the current value is a function of the single most recent value. By working through this problem [@problem_id:1283011], you will uncover the direct and elegant relationship between the model's coefficient, $\\phi$, and its lag-1 autocorrelation, $\\rho(1)$, a fundamental principle used for identifying AR models from real-world data.", "problem": "A time series is believed to be adequately described by a stationary Autoregressive model of order 1, or AR(1). The model is given by the equation:\n$$X_t = \\phi X_{t-1} + Z_t$$\nwhere $X_t$ is the value of the series at time $t$, $\\phi$ is the autoregressive coefficient, and $Z_t$ is a white noise process with mean zero and constant variance $\\sigma_Z^2$. For the process to be stationary, it is required that $|\\phi| < 1$.\n\nAn analyst examining the data computes the sample autocorrelation function. The lag-1 autocorrelation, denoted by $\\rho(1)$, is found to be 0.6. Based on this single piece of information, what is the value of the coefficient $\\phi$ for this AR(1) model?", "solution": "We start with the stationary AR(1) model\n$$\nX_{t}=\\phi X_{t-1}+Z_{t},\n$$\nwhere $\\{Z_{t}\\}$ is white noise with $\\mathbb{E}[Z_{t}]=0$ and $\\operatorname{Var}(Z_{t})=\\sigma_{Z}^{2}$, and stationarity requires $|\\phi|<1$.\n\nDefine the autocovariance function $\\gamma(h)=\\operatorname{Cov}(X_{t},X_{t-h})$. Multiply the model by $X_{t-1}$ and take expectations:\n$$\n\\mathbb{E}[X_{t}X_{t-1}]=\\phi\\,\\mathbb{E}[X_{t-1}^{2}]+\\mathbb{E}[Z_{t}X_{t-1}].\n$$\nSince $Z_{t}$ is white noise and uncorrelated with past values, $\\mathbb{E}[Z_{t}X_{t-1}]=0$. Therefore,\n$$\n\\gamma(1)=\\phi\\,\\gamma(0).\n$$\nThe autocorrelation function is $\\rho(h)=\\gamma(h)/\\gamma(0)$, so for $h=1$,\n$$\n\\rho(1)=\\frac{\\gamma(1)}{\\gamma(0)}=\\phi.\n$$\nGiven the empirical information that $\\rho(1)=0.6$, the corresponding AR(1) coefficient consistent with the model is\n$$\n\\phi=\\rho(1)=0.6,\n$$\nwhich also satisfies the stationarity condition $|\\phi|<1$.", "answer": "$$\\boxed{0.6}$$", "id": "1283011"}, {"introduction": "Where AR models look to past values, Moving Average (MA) models look to past random shocks or errors to explain a series' behavior. This practice [@problem_id:1283025] delves into an MA(2) process to illustrate a defining characteristic: its finite memory. Calculating the autocovariance at a specific lag demonstrates how the influence of past shocks abruptly ceases, causing the autocorrelation function (ACF) to 'cut off'â€”a key signature that distinguishes MA processes from AR processes during model identification.", "problem": "In a simplified model for analyzing fluctuations in a communication channel, a discrete-time signal $\\{X_t\\}$ is generated by applying a digital filter to a white noise process $\\{Z_t\\}$. The process is described by the following equation:\n$$X_t = Z_t + 0.8 Z_{t-1} - 0.3 Z_{t-2}$$\nThe input $\\{Z_t\\}$ is a white noise process, meaning it consists of a sequence of independent and identically distributed random variables with a mean of $E[Z_t] = 0$ and a constant variance of $\\sigma_Z^2 = 4$ for all $t$.\n\nCalculate the value of the autocovariance of the signal $\\{X_t\\}$ at lag 2, which is defined as $\\gamma(2) = \\text{Cov}(X_t, X_{t-2})$.", "solution": "We write the finite impulse response representation $X_{t}=\\sum_{k=0}^{2}a_{k}Z_{t-k}$ with coefficients $a_{0}=1$, $a_{1}=0.8$, $a_{2}=-0.3$. Then $X_{t-2}=\\sum_{j=0}^{2}a_{j}Z_{t-2-j}$. Since $E[Z_{t}]=0$, we have $\\gamma(2)=\\text{Cov}(X_{t},X_{t-2})=E[X_{t}X_{t-2}]$.\n\nUsing bilinearity of expectation and independence of white noise, \n$$\n\\gamma(2)=E\\!\\left[\\left(\\sum_{k=0}^{2}a_{k}Z_{t-k}\\right)\\left(\\sum_{j=0}^{2}a_{j}Z_{t-2-j}\\right)\\right]\n=\\sum_{k=0}^{2}\\sum_{j=0}^{2}a_{k}a_{j}E[Z_{t-k}Z_{t-2-j}].\n$$\nFor white noise, $E[Z_{t-k}Z_{t-2-j}]=0$ unless $t-k=t-2-j$, i.e., $k=2+j$. With $k,j\\in\\{0,1,2\\}$, the only valid pair is $(k,j)=(2,0)$. Therefore,\n$$\n\\gamma(2)=a_{2}a_{0}E[Z_{t-2}^{2}]=a_{2}a_{0}\\sigma_{Z}^{2}.\n$$\nSubstituting $a_{2}=-0.3$, $a_{0}=1$, and $\\sigma_{Z}^{2}=4$ gives\n$$\n\\gamma(2)=(-0.3)(1)\\cdot 4=-1.2.\n$$", "answer": "$$\\boxed{-1.2}$$", "id": "1283025"}, {"introduction": "Effective statistical modeling is not just about fitting data, but also about choosing models that are unique, stable, and interpretable. This exercise [@problem_id:1283036] presents a classic modeling dilemma: two different MA(1) processes that produce an identical autocorrelation structure. The key to resolving this ambiguity lies in the principle of invertibility, a condition ensuring that a model has a unique and convergent representation, which is crucial for reliable forecasting.", "problem": "Consider two distinct, zero-mean, first-order Moving Average (MA(1)) processes, denoted by $\\{X_t\\}$ and $\\{Y_t\\}$. They are defined for all integers $t$ by the equations:\n1. $X_t = Z_t + 0.5 Z_{t-1}$\n2. $Y_t = W_t + 2 W_{t-1}$\n\nHere, $\\{Z_t\\}$ and $\\{W_t\\}$ are white noise processes, each consisting of independent and identically distributed random variables with mean 0 and constant, positive variances.\n\nIn time series analysis, a key property for a Moving Average (MA) process is **invertibility**. An MA(1) process of the form $U_t = \\epsilon_t + \\theta \\epsilon_{t-1}$ is defined as invertible if and only if its parameter $\\theta$ satisfies the condition $|\\theta| < 1$. Invertible models are strongly preferred in practice, partly because they can be uniquely represented as a convergent infinite-order Autoregressive (AR) process, which is crucial for forecasting.\n\nAlthough the processes $X_t$ and $Y_t$ are defined by different parameters, they share the same Autocorrelation Function (ACF) for all non-zero lags. Given the practical importance of invertibility, which of the following statements is correct?\n\nA. $X_t$ is preferred because it is invertible.\n\nB. $Y_t$ is preferred because it is invertible.\n\nC. Both are equally preferred because they have the same Autocorrelation Function.\n\nD. Neither is preferred as both models are non-invertible.", "solution": "Define the two MA(1) processes in the common form $U_{t}=\\epsilon_{t}+\\theta\\epsilon_{t-1}$ with white noise $\\{\\epsilon_{t}\\}$ of variance $\\sigma^{2}>0$. For an MA(1), the autocovariance function is\n$$\n\\gamma(0)=(1+\\theta^{2})\\sigma^{2},\\quad \\gamma(1)=\\theta\\sigma^{2},\\quad \\gamma(h)=0\\ \\text{for}\\ |h|>1,\n$$\nand the autocorrelation function (ACF) is\n$$\n\\rho(1)=\\frac{\\gamma(1)}{\\gamma(0)}=\\frac{\\theta}{1+\\theta^{2}},\\quad \\rho(h)=0\\ \\text{for}\\ |h|>1.\n$$\nInvertibility for $U_{t}=\\epsilon_{t}+\\theta\\epsilon_{t-1}$ holds if and only if $|\\theta|<1$.\n\nFor $X_{t}=Z_{t}+0.5\\,Z_{t-1}$, we have $\\theta_{X}=\\frac{1}{2}$, so $|\\theta_{X}|=\\frac{1}{2}<1$ and $X_{t}$ is invertible.\n\nFor $Y_{t}=W_{t}+2\\,W_{t-1}$, we have $\\theta_{Y}=2$, so $|\\theta_{Y}|=2>1$ and $Y_{t}$ is non-invertible.\n\nTo compare ACFs, note that for MA(1) the lag-1 ACF is $\\rho(1)=\\frac{\\theta}{1+\\theta^{2}}$, which is invariant under the transformation $\\theta\\mapsto\\frac{1}{\\theta}$:\n$$\n\\frac{1/\\theta}{1+1/\\theta^{2}}=\\frac{1}{\\theta}\\cdot\\frac{\\theta^{2}}{1+\\theta^{2}}=\\frac{\\theta}{1+\\theta^{2}}.\n$$\nSince $\\theta_{Y}=2=\\frac{1}{\\theta_{X}}$, both processes have the same ACF at all non-zero lags (and $\\rho(h)=0$ for $|h|>1$ in both cases). Thus they are observationally equivalent in terms of ACF, but only $X_{t}$ is invertible. Because invertibility is preferred for uniqueness and forecasting, $X_{t}$ is the preferred model.", "answer": "$$\\boxed{A}$$", "id": "1283036"}]}