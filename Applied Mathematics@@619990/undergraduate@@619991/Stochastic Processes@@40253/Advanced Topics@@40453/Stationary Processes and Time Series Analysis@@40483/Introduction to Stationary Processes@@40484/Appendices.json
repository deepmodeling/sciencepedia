{"hands_on_practices": [{"introduction": "Many stationary processes can be understood as the output of a filter applied to random noise. The moving average (MA) model is a fundamental example of this, representing a process as a weighted sum of current and past 'shocks'. This exercise [@problem_id:1311045] provides foundational practice in computing the autocovariance function for an MA process, a key skill for characterizing its correlational structure. By working through it, you will see precisely how temporal correlations arise from an underlying uncorrelated process and discover the finite 'memory' typical of MA models.", "problem": "In digital signal processing, a common operation is to filter a noisy signal. Consider a simple discrete-time filter whose output, denoted by the stochastic process $\\{X_t\\}$, is a weighted average of the current and past two values of an input noise signal, $\\{Z_t\\}$. The input signal $\\{Z_t\\}$ is a white noise process, characterized by the following properties for any integer times $t$ and $s$:\n1. The expected value is zero: $E[Z_t] = 0$.\n2. The covariance is given by $E[Z_t Z_s] = \\sigma^2$ if $t=s$, and $E[Z_t Z_s] = 0$ if $t \\neq s$. Here, $\\sigma^2$ is a positive constant representing the variance of the noise.\n\nThe filter's output is defined by the relation:\n$$\nX_t = Z_t + \\frac{1}{2}Z_{t-1} + \\frac{1}{3}Z_{t-2}\n$$\n\nYour task is to characterize the time-correlation properties of the output signal. Specifically, calculate the values of the autocovariance function, $\\gamma_X(k) = \\text{Cov}(X_t, X_{t-k})$, for lags $k=0, 1, 2,$ and $3$.\n\nPresent your four answers in the form of a row matrix $(\\gamma_X(0), \\gamma_X(1), \\gamma_X(2), \\gamma_X(3))$. Your answers should be expressed in terms of $\\sigma^2$.", "solution": "We are given a white noise process $\\{Z_{t}\\}$ with $E[Z_{t}] = 0$ and $E[Z_{t}Z_{s}] = \\sigma^{2}$ if $t=s$, and $0$ otherwise. The output is the linear filter\n$$\nX_{t} = Z_{t} + \\frac{1}{2}Z_{t-1} + \\frac{1}{3}Z_{t-2}.\n$$\nSince $\\{Z_{t}\\}$ has zero mean and $X_{t}$ is a finite linear combination of $\\{Z_{t}\\}$, we have $E[X_{t}]=0$. The autocovariance at lag $k$ is\n$$\n\\gamma_{X}(k) = \\text{Cov}(X_{t},X_{t-k}) = E[X_{t}X_{t-k}],\n$$\nbecause both $X_{t}$ and $X_{t-k}$ have mean zero. Using the white-noise orthogonality $E[Z_{t}Z_{s}] = 0$ for $t \\neq s$, only products involving the same $Z$ index contribute to the expectation.\n\nFor $k=0$, using $a=\\frac{1}{2}$ and $b=\\frac{1}{3}$,\n$$\n\\gamma_{X}(0) = E\\!\\left[(Z_{t} + a Z_{t-1} + b Z_{t-2})^{2}\\right]\n= E[Z_{t}^{2}] + a^{2}E[Z_{t-1}^{2}] + b^{2}E[Z_{t-2}^{2}]\n= \\sigma^{2}\\left(1 + \\frac{1}{4} + \\frac{1}{9}\\right)\n= \\frac{49}{36}\\sigma^{2}.\n$$\n\nFor $k=1$, write $X_{t-1} = Z_{t-1} + \\frac{1}{2}Z_{t-2} + \\frac{1}{3}Z_{t-3}$. Then\n$$\n\\gamma_{X}(1) = E\\!\\left[(Z_{t} + \\tfrac{1}{2}Z_{t-1} + \\tfrac{1}{3}Z_{t-2})\\,(Z_{t-1} + \\tfrac{1}{2}Z_{t-2} + \\tfrac{1}{3}Z_{t-3})\\right].\n$$\nAll cross terms with different time indices vanish. The nonzero contributions are\n$$\nE\\!\\left[\\tfrac{1}{2}Z_{t-1}\\cdot Z_{t-1}\\right] + E\\!\\left[\\tfrac{1}{3}Z_{t-2}\\cdot \\tfrac{1}{2}Z_{t-2}\\right]\n= \\tfrac{1}{2}\\sigma^{2} + \\tfrac{1}{6}\\sigma^{2}\n= \\frac{2}{3}\\sigma^{2}.\n$$\n\nFor $k=2$, write $X_{t-2} = Z_{t-2} + \\frac{1}{2}Z_{t-3} + \\frac{1}{3}Z_{t-4}$. Then\n$$\n\\gamma_{X}(2) = E\\!\\left[(Z_{t} + \\tfrac{1}{2}Z_{t-1} + \\tfrac{1}{3}Z_{t-2})\\,(Z_{t-2} + \\tfrac{1}{2}Z_{t-3} + \\tfrac{1}{3}Z_{t-4})\\right].\n$$\nThe only overlapping index is $t-2$, giving\n$$\nE\\!\\left[\\tfrac{1}{3}Z_{t-2}\\cdot Z_{t-2}\\right] = \\frac{1}{3}\\sigma^{2}.\n$$\n\nFor $k=3$, the index sets $\\{t, t-1, t-2\\}$ and $\\{t-3, t-4, t-5\\}$ do not overlap, so\n$$\n\\gamma_{X}(3) = 0.\n$$\n\nTherefore, the requested autocovariances are\n$$\n\\left(\\gamma_{X}(0), \\gamma_{X}(1), \\gamma_{X}(2), \\gamma_{X}(3)\\right) = \\left(\\frac{49}{36}\\sigma^{2}, \\frac{2}{3}\\sigma^{2}, \\frac{1}{3}\\sigma^{2}, 0\\right).\n$$", "answer": "$$\\boxed{\\begin{pmatrix}\\frac{49}{36}\\sigma^{2}  \\frac{2}{3}\\sigma^{2}  \\frac{1}{3}\\sigma^{2}  0\\end{pmatrix}}$$", "id": "1311045"}, {"introduction": "In contrast to modeling a process from external shocks, the autoregressive (AR) model describes a system where the current state depends on its own prior states, a concept often called 'memory'. This powerful idea is central to modeling phenomena from economic indicators to physical systems. This practice [@problem_id:1311073] guides you through calculating the correlation for a fundamental AR(1) process, revealing its characteristic exponentially decaying correlation structure. This is a crucial pattern to recognize, as it reflects how the influence of past values gradually diminishes over time.", "problem": "In the field of data center management, a simplified model is used to describe the daily temperature fluctuations. Let $X_t$ represent the deviation of the average temperature from its target set-point on day $t$. This fluctuation is modeled as a stationary Autoregressive (AR) process of order 1, given by the equation:\n$$X_t = \\phi X_{t-1} + Z_t$$\nHere, $\\phi$ is a constant parameter representing the system's \"thermal memory,\" and it satisfies the condition $|\\phi|  1$. The term $Z_t$ represents a white noise process due to random daily variations in server load and external conditions. The white noise process has a mean of zero ($E[Z_t] = 0$) and a constant variance of $Var(Z_t) = \\sigma_Z^2$. A key property of this model is that the noise term $Z_t$ is uncorrelated with all past temperature deviations, i.e., $\\text{Cov}(X_s, Z_t) = 0$ for all $s  t$.\n\nAssuming the process $X_t$ is stationary, determine the theoretical correlation coefficient between the temperature deviation on a given day, $X_t$, and the deviation two days prior, $X_{t-2}$. Express your answer as a symbolic expression in terms of $\\phi$.", "solution": "We start from the AR(1) model $X_{t}=\\phi X_{t-1}+Z_{t}$ with $|\\phi|1$, $\\mathbb{E}[Z_{t}]=0$, $\\operatorname{Var}(Z_{t})=\\sigma_{Z}^{2}$, and $Z_{t}$ uncorrelated with all $X_{s}$ for $st$. Stationarity implies a constant mean $\\mu=\\mathbb{E}[X_{t}]$ satisfying $\\mu=\\phi\\mu+\\mathbb{E}[Z_{t}]$, hence $\\mu=0$.\n\nDefine the autocovariance function $\\gamma(k)=\\operatorname{Cov}(X_{t},X_{t-k})=\\mathbb{E}[X_{t}X_{t-k}]$ for $k\\geq 0$ (using zero mean). Multiplying the model by $X_{t-1}$ and taking expectations yields\n$$\n\\gamma(1)=\\mathbb{E}[X_{t}X_{t-1}]=\\phi\\,\\mathbb{E}[X_{t-1}^{2}]+\\mathbb{E}[Z_{t}X_{t-1}]=\\phi\\,\\gamma(0),\n$$\nsince $Z_{t}$ is uncorrelated with $X_{t-1}$. More generally, for $k\\geq 1$,\n$$\n\\gamma(k)=\\mathbb{E}[X_{t}X_{t-k}]=\\phi\\,\\mathbb{E}[X_{t-1}X_{t-k}]+\\mathbb{E}[Z_{t}X_{t-k}]=\\phi\\,\\gamma(k-1),\n$$\nas $Z_{t}$ is uncorrelated with $X_{t-k}$ for $k\\geq 1$. By recursion,\n$$\n\\gamma(k)=\\phi^{k}\\gamma(0).\n$$\nThe autocorrelation function is $\\rho(k)=\\gamma(k)/\\gamma(0)$, so\n$$\n\\rho(k)=\\phi^{k}.\n$$\nTherefore, the correlation coefficient between $X_{t}$ and $X_{t-2}$ is $\\rho(2)=\\phi^{2}$. (For completeness, stationarity gives $\\gamma(0)=\\phi^{2}\\gamma(0)+\\sigma_{Z}^{2}$, hence $\\gamma(0)=\\sigma_{Z}^{2}/(1-\\phi^{2})$, but this cancels in the correlation.)", "answer": "$$\\boxed{\\phi^{2}}$$", "id": "1311073"}, {"introduction": "Real-world processes often exhibit complex dynamics that are not purely autoregressive or moving average. By combining both structures, the Autoregressive Moving-Average (ARMA) model offers a more powerful and flexible framework for time series analysis. This final practice [@problem_id:1311037] synthesizes the skills you've developed by tasking you with deriving the autocovariance function for an ARMA(1,1) process. Successfully completing this exercise demonstrates a comprehensive ability to analyze the fundamental building blocks of stationary time series models.", "problem": "Consider a time series process $\\{X_t\\}$ described by the Autoregressive Moving-Average model of order (1,1), commonly denoted as ARMA(1,1). The model is given by the equation:\n$$X_t = \\phi X_{t-1} + Z_t + \\theta Z_{t-1}$$\nHere, $\\{Z_t\\}$ is a white noise process, characterized by a mean of zero, $E[Z_t] = 0$, a constant variance, $E[Z_t^2] = \\sigma_Z^2$, and no correlation across time, $E[Z_t Z_s] = 0$ for all $t \\neq s$. The parameters $\\phi$ and $\\theta$ are real constants. The process $\\{X_t\\}$ is assumed to be stationary, which requires that $|\\phi|  1$.\n\nYour task is to derive the autocovariance function $\\gamma_X(h) = \\text{Cov}(X_t, X_{t-h})$ of the process $\\{X_t\\}$ for all non-negative integer lags $h \\geq 0$.\n\nProvide your final answer as a single, closed-form analytic expression for $\\gamma_X(h)$, which may be defined piecewise.", "solution": "We start from the ARMA(1,1) model\n$$\nX_{t} = \\phi X_{t-1} + Z_{t} + \\theta Z_{t-1},\n$$\nwhere $\\{Z_{t}\\}$ is white noise with $E[Z_{t}] = 0$, $E[Z_{t}^{2}] = \\sigma_{Z}^{2}$, and $E[Z_{t}Z_{s}] = 0$ for $t \\neq s$. Stationarity requires $|\\phi|  1$.\n\nBecause $|\\phi|  1$, the process admits a causal moving-average representation. Using the backshift operator $B$, write\n$$\nX_{t} = \\frac{1 + \\theta B}{1 - \\phi B} Z_{t}.\n$$\nExpanding $(1 - \\phi B)^{-1} = \\sum_{j=0}^{\\infty} \\phi^{j} B^{j}$, we obtain\n$$\nX_{t} = \\left( 1 + \\theta B \\right) \\left( \\sum_{j=0}^{\\infty} \\phi^{j} B^{j} \\right) Z_{t}\n= \\sum_{j=0}^{\\infty} \\psi_{j} Z_{t-j},\n$$\nwith moving-average coefficients\n$$\n\\psi_{0} = 1, \\qquad \\psi_{k} = \\phi^{k} + \\theta \\phi^{k-1} = (\\theta + \\phi)\\phi^{k-1} \\quad \\text{for } k \\geq 1.\n$$\n\nFor a linear process $X_{t} = \\sum_{j=0}^{\\infty} \\psi_{j} Z_{t-j}$ driven by white noise with variance $\\sigma_{Z}^{2}$, the autocovariance function is\n$$\n\\gamma_{X}(h) = \\text{Cov}(X_{t}, X_{t-h}) = \\sigma_{Z}^{2} \\sum_{j=0}^{\\infty} \\psi_{j} \\psi_{j+h}, \\quad h \\geq 0,\n$$\nwhere the series converges absolutely because $\\sum_{j=0}^{\\infty} |\\psi_{j}|  \\infty$ when $|\\phi|  1$.\n\nCase $h=0$:\n$$\n\\gamma_{X}(0) = \\sigma_{Z}^{2} \\sum_{j=0}^{\\infty} \\psi_{j}^{2}\n= \\sigma_{Z}^{2} \\left[ \\psi_{0}^{2} + \\sum_{j=1}^{\\infty} \\left( (\\theta + \\phi)\\phi^{j-1} \\right)^{2} \\right]\n= \\sigma_{Z}^{2} \\left[ 1 + (\\theta + \\phi)^{2} \\sum_{k=0}^{\\infty} \\phi^{2k} \\right].\n$$\nUsing the geometric series $\\sum_{k=0}^{\\infty} \\phi^{2k} = \\frac{1}{1 - \\phi^{2}}$ for $|\\phi|  1$, we obtain\n$$\n\\gamma_{X}(0) = \\sigma_{Z}^{2} \\left( 1 + \\frac{(\\theta + \\phi)^{2}}{1 - \\phi^{2}} \\right)\n= \\sigma_{Z}^{2} \\frac{1 + 2\\theta \\phi + \\theta^{2}}{1 - \\phi^{2}}.\n$$\n\nCase $h \\geq 1$:\n$$\n\\gamma_{X}(h) = \\sigma_{Z}^{2} \\sum_{j=0}^{\\infty} \\psi_{j} \\psi_{j+h}\n= \\sigma_{Z}^{2} \\left[ \\psi_{0} \\psi_{h} + \\sum_{j=1}^{\\infty} \\psi_{j} \\psi_{j+h} \\right].\n$$\nFor $h \\geq 1$, $\\psi_{h} = (\\theta + \\phi) \\phi^{h-1}$ and, for $j \\geq 1$, $\\psi_{j} \\psi_{j+h} = \\left( (\\theta + \\phi)\\phi^{j-1} \\right) \\left( (\\theta + \\phi)\\phi^{j+h-1} \\right) = (\\theta + \\phi)^{2} \\phi^{2j + h - 2}$. Hence\n$$\n\\gamma_{X}(h) = \\sigma_{Z}^{2} \\left[ (\\theta + \\phi)\\phi^{h-1} + (\\theta + \\phi)^{2} \\sum_{j=1}^{\\infty} \\phi^{2j + h - 2} \\right].\n$$\nEvaluate the geometric sum:\n$$\n\\sum_{j=1}^{\\infty} \\phi^{2j + h - 2} = \\phi^{h} \\sum_{j=1}^{\\infty} \\phi^{2j - 2} = \\phi^{h} \\sum_{k=0}^{\\infty} \\phi^{2k} = \\frac{\\phi^{h}}{1 - \\phi^{2}}.\n$$\nTherefore,\n$$\n\\gamma_{X}(h) = \\sigma_{Z}^{2} \\left[ (\\theta + \\phi)\\phi^{h-1} + \\frac{(\\theta + \\phi)^{2} \\phi^{h}}{1 - \\phi^{2}} \\right]\n= \\sigma_{Z}^{2} \\phi^{h-1} (\\theta + \\phi) \\left[ 1 + \\frac{(\\theta + \\phi)\\phi}{1 - \\phi^{2}} \\right].\n$$\nCombine terms in the bracket:\n$$\n1 + \\frac{(\\theta + \\phi)\\phi}{1 - \\phi^{2}} = \\frac{1 - \\phi^{2} + \\theta \\phi + \\phi^{2}}{1 - \\phi^{2}} = \\frac{1 + \\theta \\phi}{1 - \\phi^{2}}.\n$$\nThus,\n$$\n\\gamma_{X}(h) = \\sigma_{Z}^{2} \\frac{(\\theta + \\phi)(1 + \\theta \\phi)}{1 - \\phi^{2}} \\phi^{h-1}, \\quad h \\geq 1.\n$$\n\nCollecting both cases, for non-negative integer lags $h \\geq 0$,\n$$\n\\gamma_{X}(h) =\n\\begin{cases}\n\\sigma_{Z}^{2} \\left( 1 + \\dfrac{(\\theta + \\phi)^{2}}{1 - \\phi^{2}} \\right),  h = 0, \\\\\n\\sigma_{Z}^{2} \\dfrac{(\\theta + \\phi)(1 + \\theta \\phi)}{1 - \\phi^{2}} \\phi^{h-1},  h \\geq 1.\n\\end{cases}\n$$\nBy symmetry of autocovariance, $\\gamma_{X}(-h) = \\gamma_{X}(h)$, though only $h \\geq 0$ is requested.", "answer": "$$\\boxed{\\gamma_{X}(h)=\\begin{cases}\\sigma_{Z}^{2}\\left(1+\\dfrac{(\\theta+\\phi)^{2}}{1-\\phi^{2}}\\right), h=0,\\\\[6pt]\\sigma_{Z}^{2}\\dfrac{(\\theta+\\phi)(1+\\theta\\phi)}{1-\\phi^{2}}\\;\\phi^{\\,h-1}, h\\geq 1.\\end{cases}}$$", "id": "1311037"}]}