{"hands_on_practices": [{"introduction": "Before we can meaningfully model a time series, we must often ensure it is stationary, meaning its core statistical properties like mean and variance do not change over time. This stability is a foundational assumption for many powerful analytical techniques. This exercise [@problem_id:1312127] provides hands-on practice in testing this crucial condition for an autoregressive (AR) model by examining the roots of its characteristic polynomial, a fundamental skill for any time series analyst.", "problem": "An analyst is modeling a financial time series, denoted by a process $\\{X_t\\}$, using an Autoregressive (AR) model of order 2. An AR(2) process is generally defined by the equation $X_t = c + \\phi_1 X_{t-1} + \\phi_2 X_{t-2} + Z_t$, where $Z_t$ is a white noise process with zero mean. For this particular model, the constant term $c$ is found to be zero, and the model is given by:\n\n$$X_t = X_{t-1} - 0.25 X_{t-2} + Z_t$$\n\nThe stability and properties of this model are determined by its characteristic polynomial, $P(z)$, which is derived from the autoregressive coefficients. A key property is stationarity, which requires all roots of the characteristic equation $P(z)=0$ to lie outside the unit circle in the complex plane (i.e., the magnitude of every root must be greater than 1).\n\nGiven the model, determine its characteristic polynomial and whether the process is stationary. Which of the following statements is correct?\n\nA. The characteristic polynomial is $P(z) = 1 - z + 0.25z^2$, and the process is stationary.\n\nB. The characteristic polynomial is $P(z) = 1 - z + 0.25z^2$, and the process is not stationary.\n\nC. The characteristic polynomial is $P(z) = 1 + z - 0.25z^2$, and the process is not stationary.\n\nD. The characteristic polynomial is $P(z) = z^2 - z + 0.25$, and the process is not stationary.", "solution": "For an AR(2) process $X_{t}=\\phi_{1}X_{t-1}+\\phi_{2}X_{t-2}+Z_{t}$, the autoregressive polynomial in the lag operator $B$ is $\\phi(B)=1-\\phi_{1}B-\\phi_{2}B^{2}$, obtained by rearranging to $\\phi(B)X_{t}=Z_{t}$. The characteristic polynomial in the complex variable $z$ is then $P(z)=1-\\phi_{1}z-\\phi_{2}z^{2}$.\n\nIn the given model $X_{t}=X_{t-1}-0.25\\,X_{t-2}+Z_{t}$, we identify $\\phi_{1}=1$ and $\\phi_{2}=-0.25$. Therefore,\n$$\nP(z)=1-\\phi_{1}z-\\phi_{2}z^{2}=1-z+0.25\\,z^{2}.\n$$\n\nTo assess stationarity, we solve $P(z)=0$ and require all roots to satisfy $|z|>1$. Solve\n$$\n0.25\\,z^{2}-z+1=0.\n$$\nWith $a=0.25$, $b=-1$, $c=1$, the discriminant is\n$$\n\\Delta=b^{2}-4ac=1-4\\cdot 0.25\\cdot 1=0,\n$$\nso there is a repeated root\n$$\nz=\\frac{-b}{2a}=\\frac{1}{0.5}=2.\n$$\nSince $|2|>1$, all roots lie outside the unit circle, and the process is stationary.\n\nThus the correct statement is that $P(z)=1-z+0.25z^{2}$ and the process is stationary.", "answer": "$$\\boxed{A}$$", "id": "1312127"}, {"introduction": "A key task in time series analysis is to infer a process's underlying structure from observational data. Our primary tool for this is the autocorrelation function (ACF), which measures how a series correlates with its own past values. This problem [@problem_id:1925246] deepens your understanding of this concept by asking you to work backward, deducing an AR(1) model's parameter $\\phi$ from a known correlation value, thereby highlighting the direct link between a model's structure and the patterns it produces.", "problem": "A researcher models the daily deviation of a certain atmospheric measurement, $X_t$, from its long-term average. The process is assumed to be stationary and can be described by a first-order Autoregressive (AR(1)) model:\n$$X_t = \\phi X_{t-1} + W_t$$\nHere, $\\phi$ is a constant parameter, and $W_t$ is a white noise process, meaning the $W_t$ are independent and identically distributed random variables with a mean of zero and a constant variance $\\sigma^2_W$. The noise $W_t$ is also uncorrelated with all past values of the process, i.e., $E[X_{t-k} W_t] = 0$ for all $k > 0$.\n\nEmpirical analysis of the data reveals that the correlation between the measurement on a day $t$ and the measurement on day $t-2$ is exactly $1/4$.\n\nDetermine all possible real values of the parameter $\\phi$ that are consistent with this observation.", "solution": "We consider the stationary AR(1) process defined by $X_{t}=\\phi X_{t-1}+W_{t}$ with $\\{W_{t}\\}$ white noise, mean zero, variance $\\sigma_{W}^{2}$, and uncorrelated with the past $\\{X_{t-k}:k>0\\}$. Let $\\gamma(k)=\\operatorname{Cov}(X_{t},X_{t-k})$ denote the autocovariance function and $\\rho(k)=\\gamma(k)/\\gamma(0)$ the autocorrelation function.\n\nTo derive the Yule-Walker relation, multiply both sides of the AR(1) equation by $X_{t-k}$ and take expectations for $k\\geq 1$:\n$$\n\\operatorname{E}[X_{t}X_{t-k}]=\\phi\\,\\operatorname{E}[X_{t-1}X_{t-k}]+\\operatorname{E}[W_{t}X_{t-k}].\n$$\nBy the assumed uncorrelatedness, $\\operatorname{E}[W_{t}X_{t-k}]=0$ for $k>0$. Using stationarity, $\\operatorname{E}[X_{t}X_{t-k}]=\\gamma(k)$ and $\\operatorname{E}[X_{t-1}X_{t-k}]=\\gamma(k-1)$, which gives the recursion\n$$\n\\gamma(k)=\\phi\\,\\gamma(k-1), \\quad k\\geq 1.\n$$\nApplying this twice,\n$$\n\\gamma(2)=\\phi\\,\\gamma(1)=\\phi^{2}\\gamma(0),\n$$\nso the lag-2 autocorrelation is\n$$\n\\rho(2)=\\frac{\\gamma(2)}{\\gamma(0)}=\\phi^{2}.\n$$\nThe empirical observation is $\\rho(2)=\\frac{1}{4}$, hence\n$$\n\\phi^{2}=\\frac{1}{4} \\quad \\Longrightarrow \\quad \\phi=\\pm \\frac{1}{2}.\n$$\nThe stationarity condition for an AR(1) process is $|\\phi|<1$, and both solutions satisfy this condition. Therefore, both values are admissible.", "answer": "$$\\boxed{\\begin{pmatrix}-\\frac{1}{2} & \\frac{1}{2}\\end{pmatrix}}$$", "id": "1925246"}, {"introduction": "One of the primary goals of building time series models is to forecast future values. Different model types exhibit distinct forecasting behaviors, and understanding these is essential for their correct application. This practice [@problem_id:1312088] delves into forecasting with a Moving Average (MA) model, illustrating its characteristic \"finite memory,\" a conceptually important property where the model's ability to predict future values based on past shocks vanishes beyond a certain time horizon $q$.", "problem": "A quantitative analyst is modeling the daily residuals, $X_t$, from a stock price prediction model. The analyst proposes that the residuals can be described by a Moving Average of order 2 (MA(2)) process. The model is given by the equation:\n$$X_t = Z_t + \\theta_1 Z_{t-1} + \\theta_2 Z_{t-2}$$\nHere, $\\{Z_t\\}$ is a zero-mean white noise process with constant variance $\\sigma_Z^2$. This means that for any time $t$, the expected value of the noise term is $E[Z_t] = 0$, and the noise terms are uncorrelated, i.e., $E[Z_t Z_s] = 0$ for all $t \\ne s$. The parameters $\\theta_1$ and $\\theta_2$ are known, non-zero constant coefficients.\n\nThe optimal forecast for $k$ steps into the future, made at time $t$, is denoted by $\\hat{X}_t(k)$. This forecast is defined as the conditional expectation of the future value $X_{t+k}$ given all information available up to and including time $t$. Mathematically, this is expressed as:\n$$\\hat{X}_t(k) = E[X_{t+k} | \\mathcal{I}_t]$$\nwhere $\\mathcal{I}_t$ represents the information set $\\{X_t, X_{t-1}, \\dots\\}$. It can be assumed that this information set is equivalent to knowing the history of the white noise process up to time $t$, i.e., $\\{Z_t, Z_{t-1}, \\dots\\}$.\n\nYour task is to derive a closed-form analytic expression for the optimal three-step-ahead forecast, $\\hat{X}_t(3)$.", "solution": "We are asked to find the optimal three-step-ahead forecast for the MA(2) process $X_t = Z_t + \\theta_1 Z_{t-1} + \\theta_2 Z_{t-2}$. The forecast, denoted $\\hat{X}_t(3)$, is defined as the conditional expectation of $X_{t+3}$ given the information available at time $t$, which we denote by the information set $\\mathcal{I}_t = \\{Z_t, Z_{t-1}, \\dots\\}$.\n\nThe formal expression for the forecast is:\n$$\\hat{X}_t(3) = E[X_{t+3} | \\mathcal{I}_t]$$\nFirst, we write out the expression for the process at time $t+3$ by shifting the indices in the defining equation:\n$$X_{t+3} = Z_{t+3} + \\theta_1 Z_{t+2} + \\theta_2 Z_{t+1}$$\nNow, we substitute this into the conditional expectation:\n$$\\hat{X}_t(3) = E[Z_{t+3} + \\theta_1 Z_{t+2} + \\theta_2 Z_{t+1} | \\mathcal{I}_t]$$\nUsing the property of linearity of expectation, we can distribute the conditional expectation across the terms. Since the parameters $\\theta_1$ and $\\theta_2$ are constants, they can be pulled out of the expectation:\n$$\\hat{X}_t(3) = E[Z_{t+3} | \\mathcal{I}_t] + \\theta_1 E[Z_{t+2} | \\mathcal{I}_t] + \\theta_2 E[Z_{t+1} | \\mathcal{I}_t]$$\nTo evaluate these terms, we use the properties of conditional expectation with respect to the white noise process $\\{Z_t\\}$. The information set $\\mathcal{I}_t$ contains all white noise terms up to and including time $t$.\n\nThe key principle is how to handle the expectation of a white noise term $Z_s$ conditioned on information at time $t$:\n1.  If $s > t$, $Z_s$ is a future value. Its value is unknown at time $t$. Because $\\{Z_t\\}$ is a zero-mean white noise process and future values are uncorrelated with the past, the best prediction for $Z_s$ is its unconditional mean. Therefore, $E[Z_s | \\mathcal{I}_t] = E[Z_s] = 0$ for all $s > t$.\n2.  If $s \\le t$, $Z_s$ is a past or present value. Its value is contained within the information set $\\mathcal{I}_t$ and is therefore known at time $t$. The expectation of a known value, conditioned on that knowledge, is simply the value itself. Therefore, $E[Z_s | \\mathcal{I}_t] = Z_s$ for all $s \\le t$.\n\nNow we apply these rules to each term in our forecast equation.\nThe first term is $E[Z_{t+3} | \\mathcal{I}_t]$. Since the time index $t+3$ is greater than $t$, this is the conditional expectation of a future noise term. According to our first rule, this evaluates to zero:\n$$E[Z_{t+3} | \\mathcal{I}_t] = 0$$\nThe second term is $E[Z_{t+2} | \\mathcal{I}_t]$. The time index $t+2$ is also greater than $t$, so this term is also zero:\n$$E[Z_{t+2} | \\mathcal{I}_t] = 0$$\nThe third term is $E[Z_{t+1} | \\mathcal{I}_t]$. The time index $t+1$ is greater than $t$, so this term is also zero:\n$$E[Z_{t+1} | \\mathcal{I}_t] = 0$$\nSubstituting these results back into the equation for the forecast:\n$$\\hat{X}_t(3) = 0 + \\theta_1(0) + \\theta_2(0)$$\nThis simplifies to:\n$$\\hat{X}_t(3) = 0$$\nThis result demonstrates a fundamental property of MA(q) processes: the forecast for any lead time greater than the order $q$ is zero (the mean of the process). For this MA(2) process, the forecast for a 3-step lead time is zero because the process has no \"memory\" of shocks that occurred more than 2 time steps ago.", "answer": "$$\\boxed{0}$$", "id": "1312088"}]}