{"hands_on_practices": [{"introduction": "The heart of the Metropolis algorithm is its elegant rule for deciding whether to move to a new proposed state or remain at the current one. This decision is probabilistic and depends on the relative \"goodness\" of the new and current states, as measured by the target distribution. This first exercise provides direct practice in computing the core component of this process: the acceptance probability. Mastering this calculation is the first step toward understanding how an MCMC sampler systematically explores a probability distribution [@problem_id:1371728].", "problem": "A data scientist is implementing a Markov Chain Monte Carlo (MCMC) simulation to draw samples from a posterior probability distribution for a parameter $x$. The target distribution, $\\pi(x)$, is proportional to the exponential of the negative absolute value of the parameter, such that $\\pi(x) \\propto \\exp(-|x|)$.\n\nThe scientist uses the Metropolis algorithm with a symmetric proposal distribution $q(x'|x)$, where the probability of proposing a new state $x'$ given the current state $x$ is equal to the probability of proposing $x$ given $x'$ (i.e., $q(x'|x) = q(x|x')$).\n\nSuppose that at a certain step in the simulation, the current state of the chain is $x = 1.5$. The algorithm then proposes a move to a new candidate state $x' = 2.0$.\n\nCalculate the acceptance probability for this specific move. Your answer should be a dimensionless real number. Round your final answer to four significant figures.", "solution": "The Metropolis acceptance probability for a move from $x$ to $x'$ with a symmetric proposal $q(x'|x)=q(x|x')$ is\n$$\n\\alpha(x \\to x')=\\min\\left(1,\\frac{\\pi(x')q(x|x')}{\\pi(x)q(x'|x)}\\right)=\\min\\left(1,\\frac{\\pi(x')}{\\pi(x)}\\right).\n$$\nGiven the target distribution $\\pi(x)\\propto \\exp(-|x|)$, the ratio simplifies to\n$$\n\\frac{\\pi(x')}{\\pi(x)}=\\frac{\\exp(-|x'|)}{\\exp(-|x|)}=\\exp\\!\\left(-\\left(|x'|-|x|\\right)\\right).\n$$\nWith $x=1.5$ and $x'=2.0$, we have $|x|=1.5$ and $|x'|=2.0$, so\n$$\n\\frac{\\pi(x')}{\\pi(x)}=\\exp\\!\\left(-\\left(2.0-1.5\\right)\\right)=\\exp(-0.5).\n$$\nTherefore,\n$$\n\\alpha(x \\to x')=\\min\\left(1,\\exp(-0.5)\\right)=\\exp(-0.5).\n$$\nNumerically, $\\exp(-0.5)\\approx 0.6065$ when rounded to four significant figures.", "answer": "$$\\boxed{0.6065}$$", "id": "1371728"}, {"introduction": "A correctly implemented MCMC sampler is not automatically an efficient one. The algorithm's performance in exploring the target distribution is highly sensitive to tuning parameters, such as the width of the proposal distribution in a random-walk Metropolis sampler. This conceptual problem challenges you to think about the practical consequences of your choices. By analyzing the trade-off between making very small, safe steps and very large, ambitious jumps, you will develop a crucial intuition for diagnosing and optimizing MCMC performance [@problem_id:1932810].", "problem": "An analyst is using a Markov Chain Monte Carlo (MCMC) method, specifically the Random Walk Metropolis algorithm, to generate samples from a continuous, unimodal target probability density function $\\pi(x)$ defined on the real line. The algorithm proceeds as follows: given the current state of the chain is $x^{(t)}$, a new candidate state $x'$ is proposed from a symmetric proposal distribution $q(x'|x^{(t)})$. For this specific implementation, the proposal is generated by adding a random perturbation: $x' = x^{(t)} + \\epsilon$, where $\\epsilon$ is drawn from a Normal distribution with mean 0 and standard deviation $\\sigma$, i.e., $\\epsilon \\sim N(0, \\sigma^2)$. The proposed state $x'$ is then accepted as the next state, $x^{(t+1)} = x'$, with probability $\\alpha(x', x^{(t)}) = \\min\\left(1, \\frac{\\pi(x')}{\\pi(x^{(t)})}\\right)$. If the proposal is rejected, the chain remains at the current state, i.e., $x^{(t+1)} = x^{(t)}$.\n\nThe analyst is experimenting with two different settings for the proposal distribution's standard deviation:\n1.  A very narrow proposal distribution, where $\\sigma = \\sigma_{small}$ is a very small positive number.\n2.  A very wide proposal distribution, where $\\sigma = \\sigma_{large}$ is a very large positive number.\n\nAssume the chain has been initialized in a region of non-negligible probability (e.g., near the mode of $\\pi(x)$). Which of the following statements most accurately describes the expected behavior of the MCMC chain in these two scenarios?\n\nA. The narrow proposal ($\\sigma_{small}$) will lead to a low acceptance rate and slow exploration. The wide proposal ($\\sigma_{large}$) will lead to a high acceptance rate and fast exploration.\n\nB. Both proposal widths will lead to similar acceptance rates, but the chain using the wide proposal ($\\sigma_{large}$) will explore the state space much more quickly than the chain using the narrow proposal ($\\sigma_{small}$).\n\nC. The narrow proposal ($\\sigma_{small}$) will result in a very high acceptance rate, but the chain will explore the state space very slowly. The wide proposal ($\\sigma_{large}$) will result in a very low acceptance rate, causing the chain to remain stuck at the same state for many iterations.\n\nD. The narrow proposal ($\\sigma_{small}$) will lead to a high acceptance rate and fast exploration. The wide proposal ($\\sigma_{large}$) will lead to a low acceptance rate and slow exploration.\n\nE. The width of the proposal distribution has a negligible effect on the algorithm's performance; both the acceptance rate and the speed of exploration are primarily determined by the properties of the target distribution $\\pi(x)$.", "solution": "The problem asks us to analyze the behavior of the Random Walk Metropolis algorithm, focusing on how the width (standard deviation $\\sigma$) of the proposal distribution $N(0, \\sigma^2)$ affects the chain's acceptance rate and its ability to explore the state space of a target distribution $\\pi(x)$.\n\nThe acceptance probability for a proposed move from $x^{(t)}$ to $x'$ is given by $\\alpha(x', x^{(t)}) = \\min\\left(1, \\frac{\\pi(x')}{\\pi(x^{(t)})}\\right)$. Let's analyze the two cases separately.\n\n**Case 1: Narrow Proposal Distribution ($\\sigma = \\sigma_{small}$)**\n\nWhen the standard deviation $\\sigma_{small}$ is very small, the random perturbation $\\epsilon$ drawn from $N(0, \\sigma_{small}^2)$ will also be very small in magnitude. This means the proposed state, $x' = x^{(t)} + \\epsilon$, will be very close to the current state $x^{(t)}$.\n\nSince the target distribution $\\pi(x)$ is continuous, if $x'$ is very close to $x^{(t)}$, then the value of the probability density at these two points, $\\pi(x')$ and $\\pi(x^{(t)})$, will be very similar.\nConsequently, the ratio $\\frac{\\pi(x')}{\\pi(x^{(t)})}$ will be very close to 1.\n\nThe acceptance probability is $\\alpha(x', x^{(t)}) = \\min\\left(1, \\frac{\\pi(x')}{\\pi(x^{(t)})}\\right)$. Since the ratio is close to 1, the acceptance probability $\\alpha$ will be very high (close to 1). This means that almost every proposed move will be accepted.\n\nHowever, since each accepted move is just a tiny step away from the previous position, the chain moves through the state space very slowly. This is an inefficient way to explore the full range of the target distribution $\\pi(x)$, especially the tails. The chain exhibits high autocorrelation, meaning successive samples are highly dependent, and it takes a very large number of iterations to obtain a representative set of independent samples. This constitutes very slow exploration.\n\n**Case 2: Wide Proposal Distribution ($\\sigma = \\sigma_{large}$)**\n\nWhen the standard deviation $\\sigma_{large}$ is very large, the random perturbation $\\epsilon$ will often be large in magnitude. This means the proposed state $x' = x^{(t)} + \\epsilon$ is likely to be very far from the current state $x^{(t)}$.\n\nWe are given that the target distribution $\\pi(x)$ is unimodal and the chain starts in a region of high probability (near the mode). When the chain is in such a region, $\\pi(x^{(t)})$ is relatively large. A large jump from $x^{(t)}$ is very likely to land in a region far out in the tails of the distribution, where the probability density is extremely low. Thus, it is highly probable that $\\pi(x') \\ll \\pi(x^{(t)})$.\n\nIn this scenario, the ratio $\\frac{\\pi(x')}{\\pi(x^{(t)})}$ will be very close to 0.\n\nThe acceptance probability is $\\alpha(x', x^{(t)}) = \\min\\left(1, \\frac{\\pi(x')}{\\pi(x^{(t)})}\\right)$. Since the ratio is very small, the acceptance probability $\\alpha$ will also be very small. This means that the vast majority of proposed moves will be rejected.\n\nWhen a proposal is rejected, the chain does not move: $x^{(t+1)} = x^{(t)}$. Therefore, for a wide proposal distribution, the chain will remain stuck at the same state for many consecutive iterations, waiting for a rare proposal that happens to land in another high-probability region. This is also a very inefficient way to explore the state space.\n\n**Conclusion and Evaluation of Options:**\n\n-   A narrow proposal gives a **high acceptance rate** but **slow exploration**.\n-   A wide proposal gives a **low acceptance rate** and **slow exploration** (because the chain gets stuck).\n\nLet's evaluate the given options based on this analysis:\n\n-   **A:** Incorrect. It claims a wide proposal leads to a high acceptance rate.\n-   **B:** Incorrect. It claims both have similar acceptance rates.\n-   **C:** This statement correctly captures both behaviors. The narrow proposal leads to a high acceptance rate but slow exploration. The wide proposal leads to a very low acceptance rate, causing the chain to get stuck. This is the most accurate description.\n-   **D:** Incorrect. It claims a narrow proposal leads to fast exploration.\n-   **E:** Incorrect. The width of the proposal distribution is a critical tuning parameter that profoundly affects the algorithm's performance.\n\nTherefore, the most accurate statement is C.", "answer": "$$\\boxed{C}$$", "id": "1932810"}, {"introduction": "One of the most significant challenges in applying MCMC methods is ensuring the sampler has explored the *entire* relevant state space. This is especially difficult when the target distribution has multiple, well-separated regions of high probability (modes). This exercise presents a classic failure scenario where a sampler appears to be working well (high acceptance rate) but is actually \"trapped\" in a single mode, completely missing other parts of the distribution. Understanding this potential pitfall is essential for critically evaluating MCMC results and avoiding erroneous conclusions [@problem_id:1932795].", "problem": "A data scientist is employing a Markov chain Monte Carlo (MCMC) method to draw samples from a complex, one-dimensional target probability density function, $\\pi(x)$. The target distribution is known to be a symmetric bimodal distribution, specifically an equal-weight mixture of two Gaussian distributions. The density is proportional to the sum of two Gaussian probability density functions:\n$$ \\pi(x) \\propto \\exp\\left(-\\frac{(x - \\mu_A)^2}{2\\sigma_{mode}^2}\\right) + \\exp\\left(-\\frac{(x - \\mu_B)^2}{2\\sigma_{mode}^2}\\right) $$\nThe parameters are given as $\\mu_A = -10$, $\\mu_B = 10$, and $\\sigma_{mode} = 1$. This structure results in two narrow, well-separated probability modes centered at $x=-10$ and $x=10$, with a region of extremely low probability density between them.\n\nThe scientist uses a random-walk Metropolis algorithm. At each step, a new state $x'$ is proposed from a Gaussian distribution centered at the current state $x_t$, i.e., $x' \\sim N(x_t, \\sigma_{step}^2)$. Seeking to achieve a high acceptance rate, the scientist chooses a very small step size variance, setting $\\sigma_{step} = 0.1$. The MCMC chain is initialized at the peak of one of the modes, $x_0 = -10$, and is run for $N=10^6$ iterations.\n\nWhich of the following statements most accurately describes the behavior of the MCMC sampler and the statistical properties of the resulting sample set $\\{x_1, x_2, \\dots, x_N\\}$?\n\nA. The samples will be distributed around the true mean of the target distribution, which is $x=0$. The sample mean will be close to 0, but the sample variance will be large (greater than 100), accurately reflecting the significant separation between the two modes.\n\nB. The acceptance rate of proposed moves will be very low (close to 0) because the step size is not well-tuned to the overall scale of the target distribution. The chain will remain at or very near its initial position, $x_0 = -10$.\n\nC. The acceptance rate of proposed moves will be very high (close to 1). The generated samples will thoroughly explore the region corresponding to the mode at $x=-10$, but the chain will fail to transition to the other mode at $x=10$. The sample mean will be approximately $-10$.\n\nD. The sampler will efficiently explore the entire state space. The chain will frequently jump back and forth between the two modes, and the histogram of the samples will correctly form two distinct peaks centered at $x=-10$ and $x=10$.\n\nE. The sampler will behave like a simple random walk, causing the samples to diffuse away from the starting point. The final collection of samples will be approximately uniformly distributed over a wide interval centered at $x=-10$.", "solution": "We model the target as an equal-weight mixture of two Gaussian densities with common standard deviation $\\sigma_{mode}$ and means $\\mu_{A}$ and $\\mu_{B}$. Up to proportionality,\n$$\n\\pi(x) \\propto \\exp\\!\\left(-\\frac{(x-\\mu_{A})^{2}}{2\\sigma_{mode}^{2}}\\right) + \\exp\\!\\left(-\\frac{(x-\\mu_{B})^{2}}{2\\sigma_{mode}^{2}}\\right).\n$$\nWith a random-walk Metropolis sampler using a symmetric Gaussian proposal $q(x' \\mid x)=\\mathcal{N}(x,\\sigma_{step}^{2})$, the Metropolis–Hastings acceptance probability at state $x_{t}$ for a proposal $x'$ is\n$$\n\\alpha(x_{t},x')=\\min\\!\\left(1,\\frac{\\pi(x')}{\\pi(x_{t})}\\right).\n$$\n\nInitialized at $x_{0}=\\mu_{A}$. Because the modes are well separated, when $x$ is near $\\mu_{A}$ the contribution from the $\\mu_{B}$-component in $\\pi(x)$ is negligible relative to that from the $\\mu_{A}$-component. For a small proposal increment $\\epsilon:=x'-x$ with $|\\epsilon| \\ll \\sigma_{mode}$, the dominant-ratio approximation gives\n$$\n\\frac{\\pi(x')}{\\pi(x)} \\approx \\frac{\\exp\\!\\left(-\\frac{(x'-\\mu_{A})^{2}}{2\\sigma_{mode}^{2}}\\right)}{\\exp\\!\\left(-\\frac{(x-\\mu_{A})^{2}}{2\\sigma_{mode}^{2}}\\right)}=\\exp\\!\\left(-\\frac{(x'-\\mu_{A})^{2}-(x-\\mu_{A})^{2}}{2\\sigma_{mode}^{2}}\\right).\n$$\nAt $x\\approx\\mu_{A}$, this simplifies for small $\\epsilon$ to\n$$\n\\frac{\\pi(x')}{\\pi(x)} \\approx \\exp\\!\\left(-\\frac{\\epsilon^{2}}{2\\sigma_{mode}^{2}}\\right),\n$$\nso the acceptance probability is close to $1$ when $\\sigma_{step} \\ll \\sigma_{mode}$ because typical $|\\epsilon|$ is on the order of $\\sigma_{step}$. Hence the acceptance rate is very high while the chain explores the vicinity of the starting mode.\n\nA direct jump from the neighborhood of $\\mu_{A}$ to the neighborhood of $\\mu_{B}$ in one proposal requires a displacement of order $|\\mu_{B}-\\mu_{A}|$. Under a Gaussian proposal with variance $\\sigma_{step}^{2}$, the probability of such a jump is of order $\\exp\\!\\left(-\\frac{(\\mu_{B}-\\mu_{A})^{2}}{2\\sigma_{step}^{2}}\\right)$, which is negligible when $|\\mu_{B}-\\mu_{A}| \\gg \\sigma_{step}$.\n\nCrossing the low-density region via many small accepted steps is also overwhelmingly unlikely within a finite run because the stationary density in the valley is exponentially smaller than at the peak. At the midpoint $x^{\\star}=(\\mu_{A}+\\mu_{B})/2$, the target density is\n$$\n\\pi(x^{\\star}) \\propto 2\\exp\\!\\left(-\\frac{(\\mu_{B}-\\mu_{A})^{2}}{8\\sigma_{mode}^{2}}\\right),\n$$\nwhile near $x=\\mu_{A}$ it is $\\pi(\\mu_{A}) \\propto 1$ (the other component there is negligible). Thus the ratio\n$$\n\\frac{\\pi(x^{\\star})}{\\pi(\\mu_{A})} \\approx 2\\exp\\!\\left(-\\frac{(\\mu_{B}-\\mu_{A})^{2}}{8\\sigma_{mode}^{2}}\\right)\n$$\nis exponentially small when $|\\mu_{B}-\\mu_{A}| \\gg \\sigma_{mode}$. This implies an exponentially large expected time to reach the valley or the other mode, on the order of the inverse of this ratio, which far exceeds the given $N$ when the separation is large and the proposal is very local.\n\nTherefore, with $\\sigma_{step}$ chosen very small relative to $\\sigma_{mode}$ and with well-separated modes, the chain has a very high acceptance rate, thoroughly explores the local basin around the starting mode at $x=\\mu_{A}$, essentially never transitions to the other mode within the run, and yields a sample mean approximately equal to $\\mu_{A}$. Among the options, this corresponds to statement C.", "answer": "$$\\boxed{C}$$", "id": "1932795"}]}