{"hands_on_practices": [{"introduction": "The Baum-Welch algorithm's Expectation (E) step relies on the forward-backward algorithm to compute key probabilities. This first practice focuses on the essential initialization of the forward pass, where we calculate the initial forward variables, $\\alpha_1(i)$ [@problem_id:1336514]. This calculation combines the model's initial state probabilities with the likelihood of the very first observation, setting the foundation for analyzing the entire sequence.", "problem": "A systems engineer is monitoring a critical manufacturing machine using a Hidden Markov Model (HMM). The model aims to infer the machine's unobservable operational state based on its observable error rate. The HMM is defined by the following parameters:\n\n1.  **Hidden States ($S$)**: The machine can be in one of two states:\n    *   $S_1$: 'Optimal'\n    *   $S_2$: 'Degraded'\n\n2.  **Observable Outputs ($V$)**: The monitoring sensor produces one of two outputs at each time step:\n    *   $V_1$: 'Low Error Rate'\n    *   $V_2$: 'High Error Rate'\n\n3.  **Initial State Distribution ($\\pi$)**: The probability distribution of the machine's state at time $t=0$.\n    $$ \\pi = \\begin{pmatrix} P(q_0=S_1) \\\\ P(q_0=S_2) \\end{pmatrix} = \\begin{pmatrix} 0.85 \\\\ 0.15 \\end{pmatrix} $$\n\n4.  **State Transition Matrix ($A$)**: The matrix of probabilities of transitioning from one state to another.\n    $$ A = \\begin{pmatrix} P(q_{t}=S_1|q_{t-1}=S_1) & P(q_{t}=S_2|q_{t-1}=S_1) \\\\ P(q_{t}=S_1|q_{t-1}=S_2) & P(q_{t}=S_2|q_{t-1}=S_2) \\end{pmatrix} = \\begin{pmatrix} 0.95 & 0.05 \\\\ 0.30 & 0.70 \\end{pmatrix} $$\n\n5.  **Observation Emission Matrix ($B$)**: The matrix of probabilities of observing a certain output given the current state.\n    $$ B = \\begin{pmatrix} P(V_1|S_1) & P(V_2|S_1) \\\\ P(V_1|S_2) & P(V_2|S_2) \\end{pmatrix} = \\begin{pmatrix} 0.98 & 0.02 \\\\ 0.40 & 0.60 \\end{pmatrix} $$\n\nAt the first time step ($t=1$), the engineer observes a 'High Error Rate' ($V_2$).\n\nYour task is to calculate the forward variables, $\\alpha_1(S_1)$ and $\\alpha_1(S_2)$, which represent the joint probability of being in each state at $t=1$ and having observed the given output.\n\nProvide your answer as a row matrix containing the values for $\\alpha_1(S_1)$ and $\\alpha_1(S_2)$ in that specific order. Round your final answers to three significant figures.", "solution": "The forward variable at the first time step for state $S_{i}$ is defined by the forward algorithm initialization:\n$$\\alpha_{1}(S_{i}) = \\pi_{i}\\, b_{i}(o_{1}),$$\nwhere $\\pi_{i}$ is the initial probability of state $S_{i}$ and $b_{i}(o_{1})$ is the emission probability of the first observation $o_{1}$ given state $S_{i}$.\n\nHere, the first observation is $o_{1}=V_{2}$ ('High Error Rate'). From the emission matrix $B$, we have:\n$$b_{S_{1}}(V_{2}) = P(V_{2}\\mid S_{1}) = 0.02,\\quad b_{S_{2}}(V_{2}) = P(V_{2}\\mid S_{2}) = 0.60.$$\nFrom the initial distribution $\\pi$, we have:\n$$\\pi_{S_{1}} = P(q_{0}=S_{1}) = 0.85,\\quad \\pi_{S_{2}} = P(q_{0}=S_{2}) = 0.15.$$\n\nTherefore,\n$$\\alpha_{1}(S_{1}) = \\pi_{S_{1}}\\, b_{S_{1}}(V_{2}) = 0.85 \\times 0.02 = 0.017,$$\n$$\\alpha_{1}(S_{2}) = \\pi_{S_{2}}\\, b_{S_{2}}(V_{2}) = 0.15 \\times 0.60 = 0.09.$$\n\nRounding to three significant figures, we report:\n$$\\alpha_{1}(S_{1}) = 0.0170,\\quad \\alpha_{1}(S_{2}) = 0.0900.$$\n\nAs a row matrix in the specified order:\n$$\\begin{pmatrix} 0.0170 & 0.0900 \\end{pmatrix}.$$", "answer": "$$\\boxed{\\begin{pmatrix} 0.0170 & 0.0900 \\end{pmatrix}}$$", "id": "1336514"}, {"introduction": "Building upon the initial step, the core of the forward algorithm is its efficient recursive structure. This exercise demonstrates how to compute the forward variable for the next time step, $\\alpha_2(j)$, by aggregating the probabilities from all possible previous states [@problem_id:1336466]. Understanding this recursive update is key to appreciating how Hidden Markov Models efficiently process long sequences of observations.", "problem": "A simplified financial market model is described using a Hidden Markov Model (HMM). The market can be in one of two hidden states: $S = \\{\\text{Bull}, \\text{Bear}\\}$. At the end of each day, an observable market movement is recorded, which can be either 'Up' or 'Down'.\n\nThe components of the HMM are defined as follows:\n\n1.  **Initial State Probability Distribution ($\\pi$)**: The probabilities of the market starting in each state.\n    *   $P(\\text{state at day 1 is Bull}) = \\pi_{\\text{Bull}} = 0.6$\n    *   $P(\\text{state at day 1 is Bear}) = \\pi_{\\text{Bear}} = 0.4$\n\n2.  **State Transition Probability Matrix ($A$)**: The probability of moving from one state to another on consecutive days.\n    *   $P(\\text{state}_{t+1}=\\text{Bull} | \\text{state}_t=\\text{Bull}) = a_{\\text{Bull},\\text{Bull}} = 0.7$\n    *   $P(\\text{state}_{t+1}=\\text{Bear} | \\text{state}_t=\\text{Bull}) = a_{\\text{Bull},\\text{Bear}} = 0.3$\n    *   $P(\\text{state}_{t+1}=\\text{Bull} | \\text{state}_t=\\text{Bear}) = a_{\\text{Bear},\\text{Bull}} = 0.4$\n    *   $P(\\text{state}_{t+1}=\\text{Bear} | \\text{state}_t=\\text{Bear}) = a_{\\text{Bear},\\text{Bear}} = 0.6$\n\n3.  **Observation Emission Probability Matrix ($B$)**: The probability of observing a certain market movement given the current hidden state.\n    *   $P(\\text{observation}=\\text{Up} | \\text{state}=\\text{Bull}) = b_{\\text{Bull}}(\\text{Up}) = 0.8$\n    *   $P(\\text{observation}=\\text{Down} | \\text{state}=\\text{Bull}) = b_{\\text{Bull}}(\\text{Down}) = 0.2$\n    *   $P(\\text{observation}=\\text{Up} | \\text{state}=\\text{Bear}) = b_{\\text{Bear}}(\\text{Up}) = 0.3$\n    *   $P(\\text{observation}=\\text{Down} | \\text{state}=\\text{Bear}) = b_{\\text{Bear}}(\\text{Down}) = 0.7$\n\nAn analyst observes the following sequence of market movements over two days: $Y = (\\text{Up}, \\text{Down})$.\n\nThe forward variable, $\\alpha_t(i)$, is defined as the joint probability of observing the partial sequence of observations up to time $t$ and being in state $i$ at time $t$. For the first day, given the observation 'Up', the forward variables have been calculated as:\n*   $\\alpha_1(\\text{Bull}) = 0.480$\n*   $\\alpha_1(\\text{Bear}) = 0.120$\n\nUsing the provided information, calculate the value of the forward variable $\\alpha_2(\\text{Bull})$. Round your final answer to four significant figures.", "solution": "The forward algorithm uses the recursion\n$$\n\\alpha_{t}(i)=\\left[\\sum_{j\\in S}\\alpha_{t-1}(j)\\,a_{j,i}\\right]\\,b_{i}(o_{t}),\n$$\nwhere $o_{t}$ is the observation at time $t$. For $t=2$, the observation is Down and we seek $\\alpha_{2}(\\text{Bull})$. Substituting the given values,\n$$\n\\alpha_{2}(\\text{Bull})=\\left[\\alpha_{1}(\\text{Bull})\\,a_{\\text{Bull},\\text{Bull}}+\\alpha_{1}(\\text{Bear})\\,a_{\\text{Bear},\\text{Bull}}\\right]\\,b_{\\text{Bull}}(\\text{Down}).\n$$\nCompute the sum inside the brackets:\n$$\n\\alpha_{1}(\\text{Bull})\\,a_{\\text{Bull},\\text{Bull}}+\\alpha_{1}(\\text{Bear})\\,a_{\\text{Bear},\\text{Bull}}=0.480\\cdot 0.7+0.120\\cdot 0.4=0.336+0.048=0.384.\n$$\nNow multiply by the emission probability for Down in the Bull state:\n$$\n\\alpha_{2}(\\text{Bull})=0.384\\cdot 0.2=0.0768.\n$$\nRounding to four significant figures gives $0.07680$.", "answer": "$$\\boxed{0.07680}$$", "id": "1336466"}, {"introduction": "After the E-step calculates the posterior probability of being in each state at each time step ($\\gamma_t(i)$), the Maximization (M) step uses this information to 'learn' better model parameters. This practice focuses directly on this re-estimation process, showing how to update an emission probability based on the expected counts derived from the E-step [@problem_id:1336492]. This is the fundamental mechanism that allows the Baum-Welch algorithm to iteratively refine an HMM to better fit observed data.", "problem": "A computational linguist is using a Hidden Markov Model (HMM) to analyze the structure of a simplified language. The model is designed with two hidden states: $S_V$ (a state that tends to generate vowels) and $S_C$ (a state that tends to generate consonants). The linguist uses an observed sequence of letters, $O = o_1, o_2, \\ldots, o_T$, to train the HMM's parameters via the Baum-Welch algorithm.\n\nThe Baum-Welch algorithm is an iterative procedure. After completing the Expectation-step (E-step) of the first iteration, the linguist calculates the quantity $\\gamma_t(i)$, which represents the probability of being in a specific state $i$ at time $t$, given the entire observed sequence and the initial model parameters.\n\nFrom these calculations for the vowel state $S_V$, two aggregate values are computed:\n1.  The total expected number of times the model is in state $S_V$, which is the sum of $\\gamma_t(S_V)$ over all time steps in the sequence: $\\sum_{t=1}^{T} \\gamma_t(S_V) = 20.0$.\n2.  The total expected number of times the model is in state $S_V$ and the observed letter is 'e', which is the sum of $\\gamma_t(S_V)$ over all time steps $t$ where the observation $o_t$ was the letter 'e': $\\sum_{t | o_t=\\text{'e'}} \\gamma_t(S_V) = 12.5$.\n\nBased on these values, perform the Maximization-step (M-step) of the Baum-Welch algorithm to find the updated emission probability for observing the letter 'e' from the vowel state $S_V$. Let this updated probability be denoted by $b'_{S_V}(\\text{'e'})$.\n\nCalculate the exact decimal value of $b'_{S_V}(\\text{'e'})$.", "solution": "In the Baum-Welch algorithm, the M-step updates the emission probability from state $i$ of observing symbol $v$ by the ratio of the expected number of times state $i$ emits $v$ to the expected total number of times state $i$ is visited. The update formula is\n$$\nb'_{i}(v)=\\frac{\\sum_{t: o_{t}=v}\\gamma_{t}(i)}{\\sum_{t=1}^{T}\\gamma_{t}(i)}.\n$$\nApplying this to the vowel state $S_{V}$ and the symbol $\\text{'e'}$, with the provided E-step aggregates,\n$$\nb'_{S_{V}}(\\text{'e'})=\\frac{\\sum_{t: o_{t}=\\text{'e'}}\\gamma_{t}(S_{V})}{\\sum_{t=1}^{T}\\gamma_{t}(S_{V})}=\\frac{12.5}{20.0}.\n$$\nSimplifying,\n$$\n\\frac{12.5}{20.0}=\\frac{125}{200}=\\frac{5}{8}=0.625.\n$$\nTherefore, the updated emission probability for observing $\\text{'e'}$ from $S_{V}$ is $0.625$.", "answer": "$$\\boxed{0.625}$$", "id": "1336492"}]}