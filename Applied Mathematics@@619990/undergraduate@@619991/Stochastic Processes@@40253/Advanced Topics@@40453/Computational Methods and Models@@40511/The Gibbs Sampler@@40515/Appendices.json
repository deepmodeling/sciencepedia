{"hands_on_practices": [{"introduction": "The Gibbs sampler is a powerful technique for drawing samples from a complex multivariate distribution by breaking the problem into a series of simpler, one-dimensional draws. This exercise provides foundational practice by having you derive the necessary full conditional distributions for a uniform distribution over a simple geometric shape. Mastering this process of deriving conditionals from a joint density function is the first step toward implementing your own Gibbs samplers for more complex models. [@problem_id:1338725]", "problem": "A statistician wants to generate random points $(X, Y)$ that are uniformly distributed within a specific two-dimensional region. This region is a triangle in the Cartesian plane defined by the vertices $(0,0)$, $(1,0)$, and $(0,1)$. The joint probability density function (PDF) for the random vector $(X,Y)$ is constant inside this triangle and zero outside. To generate these points, the statistician plans to use a Gibbs sampler, a Markov chain Monte Carlo algorithm that works by iteratively sampling from the full conditional distributions.\n\nThe core of implementing the Gibbs sampler requires the correct specification of these full conditional distributions, $p(x|y)$ and $p(y|x)$. Let $U(a,b)$ denote the continuous uniform distribution on the interval $[a,b]$.\n\nWhich of the following options correctly specifies the full conditional distributions for $X$ and $Y$ required for the Gibbs sampler?\n\nA. $X|Y=y \\sim U(0, 1-y)$ and $Y|X=x \\sim U(0, 1-x)$\n\nB. $X|Y=y \\sim U(0, 1)$ and $Y|X=x \\sim U(0, 1)$\n\nC. $X|Y=y \\sim U(0, y)$ and $Y|X=x \\sim U(0, x)$\n\nD. $X|Y=y \\sim U(0, 1-x)$ and $Y|X=x \\sim U(0, 1-y)$\n\nE. $X|Y=y \\sim U(y, 1)$ and $Y|X=x \\sim U(x, 1)$", "solution": "The problem asks for the full conditional distributions $p(x|y)$ and $p(y|x)$ for a point $(X,Y)$ uniformly distributed in the triangle with vertices $(0,0)$, $(1,0)$, and $(0,1)$.\n\nFirst, let's define the support of the distribution. The triangle is bounded by the lines $x=0$, $y=0$, and the line connecting $(1,0)$ and $(0,1)$, which is $x+y=1$. Therefore, the region of non-zero probability, let's call it $R$, is given by $R = \\{(x,y) | x \\ge 0, y \\ge 0, x+y \\le 1\\}$.\n\nThe distribution is uniform over this region. The area of the triangle is $A = \\frac{1}{2} \\times \\text{base} \\times \\text{height} = \\frac{1}{2} \\times 1 \\times 1 = \\frac{1}{2}$. The joint probability density function (PDF) $f(x,y)$ of $(X,Y)$ is the reciprocal of the area for points inside the triangle and zero otherwise.\n$$\nf(x,y) =\n\\begin{cases}\n\\frac{1}{A} = 2 & \\text{if } x \\ge 0, y \\ge 0, x+y \\le 1 \\\\\n0 & \\text{otherwise}\n\\end{cases}\n$$\nTo find the conditional distributions, we use the formula $f(x|y) = \\frac{f(x,y)}{f_Y(y)}$ and $f(y|x) = \\frac{f(x,y)}{f_X(x)}$, where $f_X(x)$ and $f_Y(y)$ are the marginal PDFs.\n\nLet's first find the marginal PDF of $Y$, $f_Y(y)$. We integrate the joint PDF over all possible values of $x$. For a fixed value of $y \\in [0,1]$, the constraints on $x$ are $x \\ge 0$ and $x \\le 1-y$.\n$$\nf_Y(y) = \\int_{-\\infty}^{\\infty} f(x,y) \\, dx = \\int_{0}^{1-y} 2 \\, dx\n$$\nThis integral is valid for $y \\in [0,1]$. For $y$ outside this interval, $f_Y(y)=0$.\n$$\nf_Y(y) = 2 [x]_0^{1-y} = 2(1-y) \\quad \\text{for } 0 \\le y \\le 1\n$$\nNow we can find the conditional PDF of $X$ given $Y=y$.\n$$\nf(x|y) = \\frac{f(x,y)}{f_Y(y)} = \\frac{2}{2(1-y)} = \\frac{1}{1-y}\n$$\nThis conditional PDF is defined over the range of $x$ for a given $y$, which is $0 \\le x \\le 1-y$. The function $f(x|y) = \\frac{1}{1-y}$ is constant with respect to $x$. This is the PDF of a uniform distribution on the interval $[0, 1-y]$. Thus, we write $X|Y=y \\sim U(0, 1-y)$.\n\nNext, we find the marginal PDF of $X$, $f_X(x)$. We integrate the joint PDF over all possible values of $y$. For a fixed value of $x \\in [0,1]$, the constraints on $y$ are $y \\ge 0$ and $y \\le 1-x$.\n$$\nf_X(x) = \\int_{-\\infty}^{\\infty} f(x,y) \\, dy = \\int_{0}^{1-x} 2 \\, dy\n$$\nThis integral is valid for $x \\in [0,1]$. For $x$ outside this interval, $f_X(x)=0$.\n$$\nf_X(x) = 2 [y]_0^{1-x} = 2(1-x) \\quad \\text{for } 0 \\le x \\le 1\n$$\nNow we can find the conditional PDF of $Y$ given $X=x$.\n$$\nf(y|x) = \\frac{f(x,y)}{f_X(x)} = \\frac{2}{2(1-x)} = \\frac{1}{1-x}\n$$\nThis conditional PDF is defined over the range of $y$ for a given $x$, which is $0 \\le y \\le 1-x$. The function $f(y|x) = \\frac{1}{1-x}$ is constant with respect to $y$. This is the PDF of a uniform distribution on the interval $[0, 1-x]$. Thus, we write $Y|X=x \\sim U(0, 1-x)$.\n\nCombining our two findings, the full conditional distributions are:\n$X|Y=y \\sim U(0, 1-y)$\n$Y|X=x \\sim U(0, 1-x)$\n\nComparing this result with the given options, we see that it matches option A.", "answer": "$$\\boxed{A}$$", "id": "1338725"}, {"introduction": "While powerful, the Gibbs sampler is not foolproof, and understanding its limitations is as important as knowing how to apply it. This problem presents a hypothetical scenario where the target distribution is confined to a lower-dimensional space, a situation that violates a core assumption for convergence. By analyzing this case, you will discover a critical failure mode known as reducibility, where the sampler becomes trapped and fails to explore the entire target distribution. [@problem_id:1338719]", "problem": "A monitoring system for a specialized chemical reactor tracks two correlated temperature readings, $X$ and $Y$. Due to a strict physical constraint imposed by the reactor's design, the joint probability density function $p(x, y)$ for these two readings is non-zero only on the line segment defined by the equation $x + y = 10$, where $x$ and $y$ are both non-negative (i.e., $x \\ge 0$ and $y \\ge 0$). Along this accessible line segment, the probability distribution is uniform.\n\nA data scientist decides to use a Gibbs sampler to generate synthetic data points $(X, Y)$ that follow this distribution. The sampler is initialized at the point $(X_0, Y_0) = (4.5, 5.5)$. The sampling procedure for generating the next state $(X_{t}, Y_{t})$ from the current state $(X_{t-1}, Y_{t-1})$ for $t = 1, 2, 3, \\dots$ is as follows:\n1. Draw a new value for the first variable, $X_{t}$, from the conditional distribution $p(x | Y=Y_{t-1})$.\n2. Draw a new value for the second variable, $Y_{t}$, from the conditional distribution $p(y | X=X_{t})$.\n\nDetermine the state of the sampler, $(X_{100}, Y_{100})$, after 100 full iterations. Express your answer as a pair of numbers.", "solution": "The joint support is the line segment $S=\\{(x,y): x\\ge 0,\\ y\\ge 0,\\ x+y=10\\}$. Since $p(x,y)$ is uniform along this one-dimensional set, for any $y\\in[0,10]$ the conditional $p(x\\mid Y=y)$ concentrates all its mass on the unique $x$ satisfying $x+y=10$, i.e.,\n$$\np(x\\mid Y=y)=\\delta\\!\\left(x-(10-y)\\right),\n$$\nand similarly, for any $x\\in[0,10]$,\n$$\np(y\\mid X=x)=\\delta\\!\\left(y-(10-x)\\right).\n$$\nTherefore, the Gibbs updates are deterministic:\n$$\nX_{t}=10-Y_{t-1},\\qquad Y_{t}=10-X_{t}.\n$$\nStarting from $(X_{0},Y_{0})=(4.5,5.5)$, the first iteration yields\n$$\nX_{1}=10-5.5=4.5,\\qquad Y_{1}=10-4.5=5.5,\n$$\nso the state is unchanged. By induction, if $(X_{t},Y_{t})=(4.5,5.5)$, then\n$$\nX_{t+1}=10-5.5=4.5,\\qquad Y_{t+1}=10-4.5=5.5,\n$$\nhence the chain remains fixed at $(4.5,5.5)$ for all $t$. In particular,\n$$\n(X_{100},Y_{100})=(4.5,5.5).\n$$", "answer": "$$\\boxed{\\begin{pmatrix} 4.5 & 5.5 \\end{pmatrix}}$$", "id": "1338719"}, {"introduction": "This practice explores a more subtle but common reason for a Gibbs sampler to fail: a disconnected state space. You are asked to consider a distribution defined on two separate regions, a setup that poses a challenge for the sampler's component-wise updates. This exercise demonstrates that even without a deterministic trap as in the previous example, the Markov chain can be reducible, converging to a distribution that only represents a fraction of the true target density. [@problem_id:1338689]", "problem": "Consider a pair of discrete random variables $(X, Y)$ whose joint probability mass function (PMF) $p(x, y)$ is defined over a subset of $\\mathbb{Z}^2$. The support of the distribution, i.e., the set of points where $p(x, y) > 0$, is the union of two disjoint square regions, $U = A_1 \\cup A_2$. These regions are defined by a positive integer parameter $K$ and a larger integer $L$ such that $L > K$. Specifically:\n$A_1 = \\{(x, y) \\in \\mathbb{Z}^2 \\mid 0 \\le x \\le K, 0 \\le y \\le K\\}$\n$A_2 = \\{(x, y) \\in \\mathbb{Z}^2 \\mid L \\le x \\le L+K, L \\le y \\le L+K\\}$\n\nThe joint PMF $p(x, y)$ is uniform over the support set $U$.\n\nA Gibbs sampler is employed to generate samples from this distribution. The sampler is initialized at the state $(x^{(0)}, y^{(0)}) = (0, 0)$. Each subsequent state $(x^{(t+1)}, y^{(t+1)})$ is generated from the current state $(x^{(t)}, y^{(t)})$ via the following two-step process:\n1. Draw $x^{(t+1)}$ from the full conditional distribution $p(x | Y = y^{(t)})$.\n2. Draw $y^{(t+1)}$ from the full conditional distribution $p(y | X = x^{(t+1)})$.\n\nAfter a very large number of iterations, the sequence of generated states will converge in distribution to a stationary distribution, denoted $\\pi(x, y)$. Determine the expected value of the random variable $X$ with respect to this stationary distribution, $\\mathbb{E}_{\\pi}[X]$. Your answer should be an analytic expression in terms of $K$.", "solution": "Let $U=A_{1}\\cup A_{2}$ with $A_{1}=\\{(x,y)\\in\\mathbb{Z}^{2}\\mid 0\\le x\\le K,\\;0\\le y\\le K\\}$ and $A_{2}=\\{(x,y)\\in\\mathbb{Z}^{2}\\mid L\\le x\\le L+K,\\;L\\le y\\le L+K\\}$, and suppose $p(x,y)$ is uniform on $U$. Hence there is a constant $c$ such that $p(x,y)=c$ for $(x,y)\\in U$ and $p(x,y)=0$ otherwise. Since $|A_{1}|=|A_{2}|=(K+1)^{2}$ and $U$ is a disjoint union, $c=\\frac{1}{2(K+1)^{2}}$, though its value will cancel in conditional distributions.\n\nFix $y\\in\\mathbb{Z}$. The full conditional $p(x\\mid y)$ is given by\n$$\np(x\\mid y)=\\frac{p(x,y)}{\\sum_{x'}p(x',y)}.\n$$\nIf $0\\le y\\le K$, then $(x,y)\\in U$ if and only if $0\\le x\\le K$, so $\\sum_{x'}p(x',y)=\\sum_{x'=0}^{K}c=c(K+1)$ and\n$$\np(x\\mid y)=\\begin{cases}\n\\frac{1}{K+1}, & 0\\le x\\le K,\\\\\n0, & \\text{otherwise}.\n\\end{cases}\n$$\nIf $L\\le y\\le L+K$, then $(x,y)\\in U$ if and only if $L\\le x\\le L+K$, giving\n$$\np(x\\mid y)=\\begin{cases}\n\\frac{1}{K+1}, & L\\le x\\le L+K,\\\\\n0, & \\text{otherwise}.\n\\end{cases}\n$$\nNo other $y$ values occur under the chain because $p(y)=0$ there. By symmetry, the other full conditional $p(y\\mid x)$ satisfies: if $0\\le x\\le K$ then $p(y\\mid x)$ is uniform on $\\{0,\\dots,K\\}$, and if $L\\le x\\le L+K$ then $p(y\\mid x)$ is uniform on $\\{L,\\dots,L+K\\}$.\n\nWith the initialization $(x^{(0)},y^{(0)})=(0,0)\\in A_{1}$, we have $y^{(0)}\\in\\{0,\\dots,K\\}$, hence $x^{(1)}$ is uniform on $\\{0,\\dots,K\\}$. Given any $x^{(1)}\\in\\{0,\\dots,K\\}$, $y^{(1)}$ is then uniform on $\\{0,\\dots,K\\}$. Therefore, the one-step transition from any state in $A_{1}$ produces $(x^{(t+1)},y^{(t+1)})$ uniform over $A_{1}$, independently of $(x^{(t)},y^{(t)})$. Consequently, the chain is reducible with two closed classes $A_{1}$ and $A_{2}$; starting in $A_{1}$ it remains in $A_{1}$ and is i.i.d. uniform on $A_{1}$ from the first iteration onward. Hence the stationary distribution reached from the given start is the uniform distribution on $A_{1}$.\n\nUnder this stationary distribution, $X$ is marginally discrete uniform on $\\{0,\\dots,K\\}$, because the joint is uniform over $A_{1}$ and $X$ and $Y$ are independent under this uniform restriction. Thus\n$$\n\\mathbb{E}_{\\pi}[X]=\\frac{1}{(K+1)^{2}}\\sum_{x=0}^{K}\\sum_{y=0}^{K} x\n=\\frac{1}{(K+1)^{2}}\\left(\\sum_{x=0}^{K}x\\right)(K+1)\n=\\frac{1}{K+1}\\cdot\\frac{K(K+1)}{2}\n=\\frac{K}{2}.\n$$\nTherefore, the expected value is $\\frac{K}{2}$.", "answer": "$$\\boxed{\\frac{K}{2}}$$", "id": "1338689"}]}