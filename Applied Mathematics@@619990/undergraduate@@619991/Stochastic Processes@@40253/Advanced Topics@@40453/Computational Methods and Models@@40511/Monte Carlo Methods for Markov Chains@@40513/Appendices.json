{"hands_on_practices": [{"introduction": "The foundation of any Monte Carlo method for Markov chains is the ability to generate a sample path. This first exercise provides direct, hands-on experience with this core mechanism. By using a given transition matrix and a sequence of random numbers, you will manually trace the evolution of a stochastic process, building a concrete intuition for how abstract probabilities translate into a concrete sequence of states [@problem_id:1319941].", "problem": "A simple generative model for creating sequences of characters uses a discrete-time Markov chain. Consider a system with three possible states: {A, B, C}. The behavior of the chain is governed by the following one-step transition probability matrix $P$, where the element $P_{ij}$ represents the probability of transitioning from state $i$ to state $j$. The states are ordered alphabetically (A, B, C).\n\n$$\nP = \\begin{pmatrix} 0.1 & 0.6 & 0.3 \\\\ 0.5 & 0.0 & 0.5 \\\\ 0.2 & 0.2 & 0.6 \\end{pmatrix}\n$$\n\nYou are tasked with performing a Monte Carlo simulation to generate a 5-character sequence. The simulation starts in state A. For each subsequent step, a random number uniformly distributed in $[0, 1)$ determines the next state. To perform the simulation, use the following ordered sequence of four random numbers: $0.73, 0.41, 0.89, 0.15$.\n\nThe convention for determining the next state is as follows: If the current state is $i$, and the transition probabilities to states A, B, and C are $p_{iA}$, $p_{iB}$, and $p_{iC}$, then the next state is determined by the random number $u$ according to the rule:\n- State A if $0 \\le u < p_{iA}$\n- State B if $p_{iA} \\le u < p_{iA} + p_{iB}$\n- State C if $p_{iA} + p_{iB} \\le u < 1$\n\nWhich of the following sequences is generated by this simulation?\n\nA. ABACA\n\nB. ACCCA\n\nC. ACBAC\n\nD. AAAAA\n\nE. ABCCB", "solution": "We start in state A and apply the given rule using cumulative transition intervals for each current state. For a row with probabilities $(p_{iA},p_{iB},p_{iC})$, the intervals for $u$ are $[0,p_{iA})$ for A, $[p_{iA},p_{iA}+p_{iB})$ for B, and $[p_{iA}+p_{iB},1)$ for C.\n\nFrom A, $(p_{AA},p_{AB},p_{AC})=(0.1,0.6,0.3)$, so the intervals are $[0,0.1)$ for A, $[0.1,0.7)$ for B, $[0.7,1)$ for C.\nFrom B, $(p_{BA},p_{BB},p_{BC})=(0.5,0.0,0.5)$, so the intervals are $[0,0.5)$ for A, $[0.5,0.5)$ for B, $[0.5,1)$ for C.\nFrom C, $(p_{CA},p_{CB},p_{CC})=(0.2,0.2,0.6)$, so the intervals are $[0,0.2)$ for A, $[0.2,0.4)$ for B, $[0.4,1)$ for C.\n\nStep 1: Start at A with $u_1=0.73$. Since $0.73 \\in [0.7, 1)$ for A’s row, the next state is C.\nStep 2: Current C with $u_2=0.41$. Since $0.41 \\in [0.4, 1)$ for C’s row, the next state is C.\nStep 3: Current C with $u_3=0.89$. Since $0.89 \\in [0.4, 1)$ for C’s row, the next state is C.\nStep 4: Current C with $u_4=0.15$. Since $0.15 \\in [0, 0.2)$ for C’s row, the next state is A.\n\nThus the 5-character sequence is A, C, C, C, A, which corresponds to option B.", "answer": "$$\\boxed{B}$$", "id": "1319941"}, {"introduction": "Now that we understand how to generate a single path, a crucial question arises: what happens in the long run? The ergodic theorem states that for many Markov chains, the proportion of time spent in each state converges to a fixed value. This exercise [@problem_id:1319960] asks you to calculate this theoretical limit, known as the stationary distribution, which represents the \"ground truth\" that a long Monte Carlo simulation aims to approximate.", "problem": "A simplified model for a particle's energy state within a material involves a discrete set of states and probabilistic transitions. Consider a particle that can exist in one of three energy states, labeled 1, 2, and 3. At each discrete time step, the particle transitions from its current state $i$ to a new state $j$ with a probability $P_{ij}$ given by the transition matrix $P$:\n$$\nP = \\begin{pmatrix}\n1/2 & 1/2 & 0 \\\\\n1/4 & 1/2 & 1/4 \\\\\n0 & 1/3 & 2/3\n\\end{pmatrix}\n$$\nIn statistical mechanics, the long-term behavior of such a system is of key interest. One common technique to study this is to run a Monte Carlo simulation: start the particle in a random state and track its path for a very large number of steps. The fraction of time the particle spends in each state over this long run provides an empirical estimate of the system's stationary probabilities.\n\nYour task is to bypass simulation and calculate the exact, theoretical long-term probability that the particle is in state 3. Express your final answer as a decimal rounded to three significant figures.", "solution": "Let the stationary distribution be the row vector $\\pi = (\\pi_{1},\\pi_{2},\\pi_{3})$ satisfying the Markov chain balance equations and normalization:\n$$\n\\pi P = \\pi, \\quad \\pi_{1} + \\pi_{2} + \\pi_{3} = 1.\n$$\nFrom $\\pi P = \\pi$ with the given $P$, the componentwise equations are\n$$\n\\pi_{1} = \\frac{1}{2}\\pi_{1} + \\frac{1}{4}\\pi_{2} + 0\\cdot \\pi_{3},\n$$\n$$\n\\pi_{2} = \\frac{1}{2}\\pi_{1} + \\frac{1}{2}\\pi_{2} + \\frac{1}{3}\\pi_{3},\n$$\n$$\n\\pi_{3} = 0\\cdot \\pi_{1} + \\frac{1}{4}\\pi_{2} + \\frac{2}{3}\\pi_{3}.\n$$\nFrom the first equation,\n$$\n\\pi_{1} - \\frac{1}{2}\\pi_{1} = \\frac{1}{4}\\pi_{2} \\;\\Rightarrow\\; \\frac{1}{2}\\pi_{1} = \\frac{1}{4}\\pi_{2} \\;\\Rightarrow\\; \\pi_{1} = \\frac{1}{2}\\pi_{2}.\n$$\nFrom the third equation,\n$$\n\\pi_{3} - \\frac{2}{3}\\pi_{3} = \\frac{1}{4}\\pi_{2} \\;\\Rightarrow\\; \\frac{1}{3}\\pi_{3} = \\frac{1}{4}\\pi_{2} \\;\\Rightarrow\\; \\pi_{3} = \\frac{3}{4}\\pi_{2}.\n$$\nImpose normalization:\n$$\n\\pi_{1} + \\pi_{2} + \\pi_{3} = \\left(\\frac{1}{2} + 1 + \\frac{3}{4}\\right)\\pi_{2} = \\frac{9}{4}\\pi_{2} = 1 \\;\\Rightarrow\\; \\pi_{2} = \\frac{4}{9}.\n$$\nThus\n$$\n\\pi_{1} = \\frac{1}{2}\\cdot \\frac{4}{9} = \\frac{2}{9}, \\quad \\pi_{3} = \\frac{3}{4}\\cdot \\frac{4}{9} = \\frac{1}{3}.\n$$\nSince the chain is irreducible and aperiodic (all states communicate and each has a positive self-transition), the long-term fraction of time spent in each state equals the stationary distribution. Therefore, the long-term probability of being in state 3 is $\\pi_{3} = \\frac{1}{3}$, which as a decimal rounded to three significant figures is $0.333$.", "answer": "$$\\boxed{0.333}$$", "id": "1319960"}, {"introduction": "In many real-world applications, we don't start with a known transition matrix; instead, we have a complex target probability distribution $\\pi$ that we wish to sample from. The Metropolis-Hastings algorithm is a powerful and general method for constructing a Markov chain whose stationary distribution is exactly our target $\\pi$. This final practice [@problem_id:1319963] zooms in on the ingenious heart of the algorithm: the calculation of the acceptance probability, which guides the chain's exploration and guarantees its convergence to the desired distribution.", "problem": "An engineer is using a computational model to study a system that can exist in one of three discrete states: $S_1$, $S_2$, and $S_3$. The goal is to simulate the system's behavior according to a target stationary probability distribution $\\pi$. Due to the complexity of the system, direct sampling from $\\pi$ is intractable. Instead, the engineer employs the Metropolis-Hastings algorithm, a Markov Chain Monte Carlo method, to generate a sequence of states that will eventually be distributed according to $\\pi$.\n\nThe target distribution $\\pi$ is known up to a normalization constant, with the unnormalized weights for the states given as:\n$\\tilde{\\pi}(S_1) = 5$, $\\tilde{\\pi}(S_2) = 2$, and $\\tilde{\\pi}(S_3) = 8$.\n\nThe Metropolis-Hastings algorithm uses a proposal distribution, defined by a transition matrix $Q$, to suggest moves between states. For this system, the proposal matrix is:\n$$\nQ = \\begin{pmatrix}\n0.1 & 0.5 & 0.4 \\\\\n0.6 & 0.2 & 0.2 \\\\\n0.3 & 0.3 & 0.4\n\\end{pmatrix}\n$$\nwhere the entry $Q_{ij}$ represents the probability of proposing a move to state $S_j$ given the current state is $S_i$.\n\nSuppose the simulation is currently in state $S_3$, and the algorithm proposes a move to state $S_1$. Calculate the acceptance probability for this proposed move. Give your answer as a decimal rounded to three significant figures.", "solution": "In the Metropolis-Hastings algorithm with proposal kernel $Q$, the acceptance probability for a proposed move from state $i$ to state $j$ is\n$$\n\\alpha(i \\to j) = \\min\\!\\left(1,\\ \\frac{\\pi(j)\\,Q_{j,i}}{\\pi(i)\\,Q_{i,j}}\\right).\n$$\nWhen $\\pi$ is known up to a normalization constant, the normalized probabilities cancel, so we may use the unnormalized weights $\\tilde{\\pi}$:\n$$\n\\alpha(i \\to j) = \\min\\!\\left(1,\\ \\frac{\\tilde{\\pi}(j)\\,Q_{j,i}}{\\tilde{\\pi}(i)\\,Q_{i,j}}\\right).\n$$\nHere $i=3$ and $j=1$. Using $\\tilde{\\pi}(S_{1})=5$, $\\tilde{\\pi}(S_{3})=8$, $Q_{1,3}=0.4$, and $Q_{3,1}=0.3$, we compute\n$$\n\\frac{\\tilde{\\pi}(S_{1})\\,Q_{1,3}}{\\tilde{\\pi}(S_{3})\\,Q_{3,1}}=\\frac{5 \\cdot 0.4}{8 \\cdot 0.3}=\\frac{2}{2.4}=\\frac{5}{6}.\n$$\nThus,\n$$\n\\alpha(3 \\to 1)=\\min\\!\\left(1,\\frac{5}{6}\\right)=\\frac{5}{6}\\approx 0.833333\\ldots\n$$\nRounded to three significant figures, the acceptance probability is $0.833$.", "answer": "$$\\boxed{0.833}$$", "id": "1319963"}]}