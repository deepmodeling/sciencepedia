{"hands_on_practices": [{"introduction": "The control variates technique is a cornerstone of variance reduction, reducing uncertainty by exploiting the relationship between our quantity of interest and a related variable with a known mean. To harness its full power, one must calculate the optimal weighting coefficient that minimizes the estimator's variance. This exercise provides direct, hands-on practice in deriving this coefficient, reinforcing the core mathematical principles behind this powerful method [@problem_id:1348947].", "problem": "In Monte Carlo methods, a common task is to estimate the value of an integral $I = \\int_a^b g(x) dx$. This is often achieved by reformulating the integral as an expected value, $E[f(X)]$, where $X$ is a random variable and $f$ is a related function. A standard 'crude' Monte Carlo estimator then uses the sample mean of $N$ independent draws of $f(X_i)$.\n\nTo improve the precision of the estimation for a given number of samples, variance reduction techniques are employed. One such technique is the method of control variates. This involves finding a random variable $C$ that is correlated with $f(X)$ and has a known expectation, $\\mu_C = E[C]$. A new, improved estimator is then formed as $Z(b) = f(X) - b(C - \\mu_C)$, where $b$ is a constant coefficient. The value of $b$ is chosen to minimize the variance of $Z(b)$. This optimal coefficient, denoted $b^*$, is given by the formula:\n$$b^* = \\frac{\\text{Cov}(f(X), C)}{\\text{Var}(C)}$$\n\nConsider the problem of estimating the integral $I = \\int_0^1 \\cos\\left(\\frac{\\pi x}{2}\\right) dx$. This is equivalent to finding the expected value of $Y = \\cos\\left(\\frac{\\pi X}{2}\\right)$ where $X$ is a random variable uniformly distributed on the interval $[0, 1]$. We will use the random variable $C = X$ as a control variate, since it is correlated with $Y$ and its properties are easily determined.\n\nYour task is to calculate the exact theoretical value of the optimal coefficient $b^*$ for this control variate. Express your answer as a single closed-form analytic expression in terms of $\\pi$.", "solution": "We have $X \\sim \\text{Unif}[0,1]$, $Y=\\cos\\left(\\frac{\\pi X}{2}\\right)$, and $C=X$. The optimal control variate coefficient is\n$$\nb^{*}=\\frac{\\text{Cov}(Y,C)}{\\text{Var}(C)}.\n$$\nFirst compute $\\text{Var}(C)$. Using $E[X]=\\int_{0}^{1}x\\,dx=\\frac{1}{2}$ and $E[X^{2}]=\\int_{0}^{1}x^{2}\\,dx=\\frac{1}{3}$, we obtain\n$$\n\\text{Var}(C)=E[X^{2}]-\\left(E[X]\\right)^{2}=\\frac{1}{3}-\\frac{1}{4}=\\frac{1}{12}.\n$$\nNext compute $E[Y]$:\n$$\nE[Y]=\\int_{0}^{1}\\cos\\left(\\frac{\\pi x}{2}\\right)\\,dx=\\left.\\frac{2}{\\pi}\\sin\\left(\\frac{\\pi x}{2}\\right)\\right|_{0}^{1}=\\frac{2}{\\pi}.\n$$\nNow compute $E[CY]=E\\!\\left[X\\cos\\left(\\frac{\\pi X}{2}\\right)\\right]$. Let $a=\\frac{\\pi}{2}$. Then\n$$\nE[CY]=\\int_{0}^{1}x\\cos(ax)\\,dx.\n$$\nUsing integration by parts with $u=x$, $dv=\\cos(ax)\\,dx$, so $du=dx$ and $v=\\frac{1}{a}\\sin(ax)$, we get\n$$\n\\int x\\cos(ax)\\,dx=\\frac{x\\sin(ax)}{a}+\\frac{\\cos(ax)}{a^{2}}.\n$$\nEvaluating from $0$ to $1$,\n$$\n\\int_{0}^{1}x\\cos(ax)\\,dx=\\frac{\\sin(a)}{a}+\\frac{\\cos(a)-1}{a^{2}}.\n$$\nWith $a=\\frac{\\pi}{2}$, we have $\\sin(a)=1$ and $\\cos(a)=0$, hence\n$$\nE[CY]=\\frac{1}{a}-\\frac{1}{a^{2}}=\\frac{2}{\\pi}-\\frac{4}{\\pi^{2}}.\n$$\nTherefore,\n$$\n\\text{Cov}(Y,C)=E[CY]-E[Y]E[C]=\\left(\\frac{2}{\\pi}-\\frac{4}{\\pi^{2}}\\right)-\\left(\\frac{2}{\\pi}\\right)\\left(\\frac{1}{2}\\right)=\\frac{1}{\\pi}-\\frac{4}{\\pi^{2}}=\\frac{\\pi-4}{\\pi^{2}}.\n$$\nFinally,\n$$\nb^{*}=\\frac{\\text{Cov}(Y,C)}{\\text{Var}(C)}=\\frac{\\frac{\\pi-4}{\\pi^{2}}}{\\frac{1}{12}}=\\frac{12(\\pi-4)}{\\pi^{2}}.\n$$", "answer": "$$\\boxed{\\frac{12(\\pi - 4)}{\\pi^{2}}}$$", "id": "1348947"}, {"introduction": "When estimating the probability of a rare event, standard Monte Carlo simulation can be incredibly inefficient, requiring an astronomical number of samples. Importance sampling addresses this by sampling from a \"tilted\" distribution that makes the rare event more likely to occur. This practice challenges you to think strategically and select the most effective tilted distribution, a crucial design skill for tackling rare-event simulation problems in fields from finance to reliability engineering [@problem_id:1348967].", "problem": "In a Monte Carlo simulation exercise, we aim to estimate the probability $p$ that the sum of the outcomes of 20 independent rolls of a standard fair six-sided die exceeds 100. Let $X_i$ for $i=1, \\dots, 20$ be the outcome of the $i$-th roll, so $X_i$ is a random variable uniformly distributed on the set $\\{1, 2, 3, 4, 5, 6\\}$. We are interested in $p = P(\\sum_{i=1}^{20} X_i > 100)$.\n\nSince this event is rare, a naive Monte Carlo simulation is inefficient. To improve efficiency, we can use importance sampling, a variance reduction technique. This involves sampling from a different, \"loaded\" Probability Mass Function (PMF), let's call it $q(k)$, for each die roll, instead of the original uniform PMF, $p(k) = 1/6$.\n\nWhich of the following choices for the loaded PMF $q(k)$ for a single die roll (where $k \\in \\{1, 2, 3, 4, 5, 6\\}$) would be the most suitable choice for implementing importance sampling to estimate $p$ with low variance?\n\nA. $q(k) = \\frac{1}{6}$ for $k \\in \\{1, 2, 3, 4, 5, 6\\}$\n\nB. $q(k) = \\frac{7-k}{21}$ for $k \\in \\{1, 2, 3, 4, 5, 6\\}$\n\nC. $q(k) = \\frac{1}{3}$ for $k \\in \\{4, 5, 6\\}$ and $q(k) = 0$ for $k \\in \\{1, 2, 3\\}$\n\nD. $q(k) = \\frac{k}{21}$ for $k \\in \\{1, 2, 3, 4, 5, 6\\}$", "solution": "Let $X_{1},\\dots,X_{20}$ be i.i.d. with the nominal PMF $p(k)=\\frac{1}{6}$ on $\\{1,2,3,4,5,6\\}$, and let $S=\\sum_{i=1}^{20}X_{i}$. We want $p^{\\star}=P(S100)$. Importance sampling estimates $p^{\\star}$ by sampling $Y_{1},\\dots,Y_{20}$ i.i.d. from a proposal PMF $q$ on $\\{1,\\dots,6\\}$ and using the unbiased estimator\n$$\n\\widehat{p}=\\mathbf{1}\\{Y_{1}+\\cdots+Y_{20}100\\}\\prod_{i=1}^{20}\\frac{p(Y_{i})}{q(Y_{i})}.\n$$\nUnbiasedness requires absolute continuity of $p$ restricted to the rare-event set with respect to $q$, which here amounts to $q(k)0$ for every $k\\in\\{1,\\dots,6\\}$ that can appear in any sequence contributing to $S100$. Since there exist sequences in $\\{S100\\}$ that include any of the values $1,2,3,4,5,6$, we need $q(k)0$ for all $k\\in\\{1,\\dots,6\\}$.\n\nVariance reduction for the rare event $S100$ requires proposing distributions that make large outcomes more likely, increasing $P_{q}(S100)$, while keeping likelihood ratios stable. For a single roll, the likelihood ratio is $L(k)=\\frac{p(k)}{q(k)}$. For rare events dominated by large $k$, it is beneficial that $L(k)$ decreases with $k$, so samples typical of the event receive smaller weights, reducing variance. Heuristically, the optimal proposal is an exponential tilt $q_{\\theta}(k)\\propto p(k)\\exp(\\theta k)$ with $\\theta0$, which places more mass on larger $k$ and has full support.\n\nEvaluate each option:\n\nA. $q(k)=\\frac{1}{6}$. This is the nominal PMF. It provides no variance reduction and hence is not the most suitable for a rare event.\n\nB. $q(k)=\\frac{7-k}{21}$. This proposal favors small $k$ (since $q(1)\\cdotsq(6)$), which moves mass away from the rare-event region. Its single-roll likelihood ratio is\n$$\nL_{B}(k)=\\frac{\\frac{1}{6}}{\\frac{7-k}{21}}=\\frac{7}{2(7-k)},\n$$\nwhich increases with $k$, giving large weights to large outcomes that are typical under $S100$, inflating variance. This is unsuitable.\n\nC. $q(k)=\\frac{1}{3}$ for $k\\in\\{4,5,6\\}$ and $q(k)=0$ for $k\\in\\{1,2,3\\}$. While this heavily favors large faces, it violates the absolute continuity requirement because there exist sequences with $S100$ that include values in $\\{1,2,3\\}$ and have positive probability under $p$ but zero under $q$. Consequently, $\\prod_{i}p(Y_{i})/q(Y_{i})$ is not well-defined on the entire rare-event set and an unbiased IS estimator for $P(S100)$ cannot be constructed with this $q$.\n\nD. $q(k)=\\frac{k}{21}$. This proposal favors large $k$ while maintaining full support. Its single-roll likelihood ratio is\n$$\nL_{D}(k)=\\frac{\\frac{1}{6}}{\\frac{k}{21}}=\\frac{7}{2k},\n$$\nwhich decreases with $k$, so samples typical of the event receive smaller weights, stabilizing the estimator and reducing variance. Moreover, the expected value under each PMF confirms the direction of tilt:\n$$\n\\mathbb{E}_{p}[X]=\\frac{1+2+3+4+5+6}{6}=\\frac{7}{2},\\quad\n\\mathbb{E}_{B}[X]=\\frac{7\\sum_{k=1}^{6}k-\\sum_{k=1}^{6}k^{2}}{21}=\\frac{56}{21}=\\frac{8}{3}\\frac{7}{2},\n$$\n$$\n\\mathbb{E}_{C}[X]=\\frac{4+5+6}{3}=5\\quad\\text{(but $q$ has zeros, invalid for unbiased IS)},\n$$\n$$\n\\mathbb{E}_{D}[X]=\\frac{\\sum_{k=1}^{6}k^{2}}{21}=\\frac{91}{21}=\\frac{13}{3}\\frac{7}{2}.\n$$\nThus D tilts in the correct direction, preserves full support, and yields a likelihood ratio that is smaller for large $k$, all of which contribute to lower variance for estimating $P(S100)$.\n\nTherefore, among the given choices, D is the most suitable for importance sampling with low variance.", "answer": "$$\\boxed{D}$$", "id": "1348967"}, {"introduction": "Antithetic variates offer an elegant way to reduce variance by inducing negative correlation between pairs of simulated samples. While the one-dimensional case is straightforward, its application in higher dimensions can reveal surprising and counter-intuitive behaviors. This advanced exercise explores such a scenario, demonstrating that a naive \"full-antithetic\" approach is not always optimal and highlighting the importance of analyzing the structure of the payoff function to design the most effective sampling scheme [@problem_id:2446736].", "problem": "Consider independent random variables $U_{1}$ and $U_{2}$, each uniformly distributed on $[0,1]$. Define the two-dimensional payoff function $\\pi(U_{1},U_{2}) = (U_{1} - \\tfrac{1}{2})(U_{2} - \\tfrac{1}{2})$, and the target quantity $\\mu = \\mathbb{E}[\\pi(U_{1},U_{2})]$. Two paired-sample estimators of $\\mu$ are constructed as follows, each using a single pair to produce one observation of the estimator:\n- Scheme F (full antithetic pairing): average the two evaluations $\\pi(U_{1},U_{2})$ and $\\pi(1-U_{1},1-U_{2})$.\n- Scheme P (partial antithetic pairing): average the two evaluations $\\pi(U_{1},U_{2})$ and $\\pi(U_{1},1-U_{2})$.\nLet $\\mathrm{Var}_{F}$ be the variance of the single-pair estimator under Scheme F, and let $\\mathrm{Var}_{P}$ be the variance of the single-pair estimator under Scheme P, both taken with respect to the joint distribution of $(U_{1},U_{2})$.\n\nCompute the difference $\\Delta = \\mathrm{Var}_{F} - \\mathrm{Var}_{P}$. Provide your answer as an exact value. No rounding is required.", "solution": "Let $U$ be a random variable uniformly distributed on $[0,1]$. Its expected value is $\\mathbb{E}[U] = \\frac{1}{2}$ and its variance is $\\mathrm{Var}(U) = \\frac{(1-0)^{2}}{12} = \\frac{1}{12}$.\nThe target quantity is $\\mu = \\mathbb{E}[\\pi(U_{1},U_{2})]$. We compute this first.\n$$ \\mu = \\mathbb{E}\\left[\\left(U_{1} - \\frac{1}{2}\\right)\\left(U_{2} - \\frac{1}{2}\\right)\\right] $$\nSince $U_{1}$ and $U_{2}$ are independent, the expectation of the product is the product of the expectations:\n$$ \\mu = \\mathbb{E}\\left[U_{1} - \\frac{1}{2}\\right] \\mathbb{E}\\left[U_{2} - \\frac{1}{2}\\right] $$\nFor each $i \\in \\{1,2\\}$, we have $\\mathbb{E}\\left[U_{i} - \\frac{1}{2}\\right] = \\mathbb{E}[U_{i}] - \\frac{1}{2} = \\frac{1}{2} - \\frac{1}{2} = 0$.\nTherefore, $\\mu = 0 \\times 0 = 0$. The estimators are expected to estimate the value $0$.\n\nNext, we analyze the estimator for Scheme F (full antithetic pairing):\n$$ Y_{F} = \\frac{1}{2}[\\pi(U_{1},U_{2}) + \\pi(1-U_{1},1-U_{2})] $$\nWe evaluate the second term in the sum. The antithetic transform of a $\\mathcal{U}[0,1]$ variable $U$ is $1-U$, which is also distributed as $\\mathcal{U}[0,1]$.\n$$ \\pi(1-U_{1},1-U_{2}) = \\left((1-U_{1}) - \\frac{1}{2}\\right)\\left((1-U_{2}) - \\frac{1}{2}\\right) = \\left(\\frac{1}{2}-U_{1}\\right)\\left(\\frac{1}{2}-U_{2}\\right) $$\n$$ = \\left(-\\left(U_{1}-\\frac{1}{2}\\right)\\right)\\left(-\\left(U_{2}-\\frac{1}{2}\\right)\\right) = \\left(U_{1}-\\frac{1}{2}\\right)\\left(U_{2}-\\frac{1}{2}\\right) = \\pi(U_{1},U_{2}) $$\nSubstituting this back into the expression for $Y_{F}$:\n$$ Y_{F} = \\frac{1}{2}[\\pi(U_{1},U_{2}) + \\pi(U_{1},U_{2})] = \\pi(U_{1},U_{2}) $$\nThus, for this specific payoff function, the full antithetic estimator is identical to the original random variable. The variance is therefore:\n$$ \\mathrm{Var}_{F} = \\mathrm{Var}(Y_{F}) = \\mathrm{Var}(\\pi(U_{1},U_{2})) $$\nSince $\\mathbb{E}[\\pi(U_{1},U_{2})] = \\mu = 0$, the variance is the expectation of the square:\n$$ \\mathrm{Var}(\\pi(U_{1},U_{2})) = \\mathbb{E}[(\\pi(U_{1},U_{2}))^{2}] = \\mathbb{E}\\left[\\left(\\left(U_{1} - \\frac{1}{2}\\right)\\left(U_{2} - \\frac{1}{2}\\right)\\right)^{2}\\right] = \\mathbb{E}\\left[\\left(U_{1} - \\frac{1}{2}\\right)^{2}\\left(U_{2} - \\frac{1}{2}\\right)^{2}\\right] $$\nDue to the independence of $U_{1}$ and $U_{2}$, we can separate the expectations:\n$$ \\mathrm{Var}_{F} = \\mathbb{E}\\left[\\left(U_{1} - \\frac{1}{2}\\right)^{2}\\right] \\mathbb{E}\\left[\\left(U_{2} - \\frac{1}{2}\\right)^{2}\\right] $$\nThe term $\\mathbb{E}\\left[\\left(U_{i} - \\frac{1}{2}\\right)^{2}\\right]$ is the variance of $U_{i}$, since $\\mathbb{E}[U_{i}] = \\frac{1}{2}$.\n$$ \\mathbb{E}\\left[\\left(U_{i} - \\frac{1}{2}\\right)^{2}\\right] = \\mathrm{Var}(U_{i}) = \\frac{1}{12} $$\nTherefore, the variance for Scheme F is:\n$$ \\mathrm{Var}_{F} = \\left(\\frac{1}{12}\\right)\\left(\\frac{1}{12}\\right) = \\frac{1}{144} $$\n\nNow, we analyze the estimator for Scheme P (partial antithetic pairing):\n$$ Y_{P} = \\frac{1}{2}[\\pi(U_{1},U_{2}) + \\pi(U_{1},1-U_{2})] $$\nWe evaluate the second term in the sum:\n$$ \\pi(U_{1},1-U_{2}) = \\left(U_{1} - \\frac{1}{2}\\right)\\left((1-U_{2}) - \\frac{1}{2}\\right) = \\left(U_{1} - \\frac{1}{2}\\right)\\left(\\frac{1}{2}-U_{2}\\right) $$\n$$ = -\\left(U_{1} - \\frac{1}{2}\\right)\\left(U_{2} - \\frac{1}{2}\\right) = -\\pi(U_{1},U_{2}) $$\nSubstituting this back into the expression for $Y_{P}$:\n$$ Y_{P} = \\frac{1}{2}[\\pi(U_{1},U_{2}) - \\pi(U_{1},U_{2})] = 0 $$\nThe estimator for Scheme P is a constant, $0$. The variance of a constant is always $0$.\n$$ \\mathrm{Var}_{P} = \\mathrm{Var}(Y_{P}) = \\mathrm{Var}(0) = 0 $$\nThis indicates that for this specific payoff function, partial antithetic pairing yields a perfect estimator with zero variance.\n\nFinally, we compute the difference $\\Delta$:\n$$ \\Delta = \\mathrm{Var}_{F} - \\mathrm{Var}_{P} = \\frac{1}{144} - 0 = \\frac{1}{144} $$\nThe difference in variances is $\\frac{1}{144}$.", "answer": "$$\\boxed{\\frac{1}{144}}$$", "id": "2446736"}]}