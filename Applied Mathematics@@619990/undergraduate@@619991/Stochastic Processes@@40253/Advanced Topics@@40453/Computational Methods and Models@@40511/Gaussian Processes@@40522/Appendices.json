{"hands_on_practices": [{"introduction": "Understanding abstract concepts often begins with building a concrete example. This first exercise guides you through constructing a simple Gaussian Process from scratch using familiar, normally distributed random variables. By deriving the covariance matrix for a process representing a sensor with random drift, you will gain hands-on experience with how the fundamental properties of a GP—its mean and covariance function—emerge from its underlying structure [@problem_id:1304141].", "problem": "A simple model for a sensor's reading with a linear drift over time $t$ is described by the stochastic process $X_t = At + B$. The term $B$ represents a random initial offset, and $A$ represents a random drift rate. Assume that $A$ and $B$ are independent random variables, each following a standard normal distribution, i.e., a normal distribution with a mean of 0 and a variance of 1. For any two distinct time points $t_1$ and $t_2$, the pair of readings $(X_{t_1}, X_{t_2})$ forms a random vector. Determine the covariance matrix of this random vector.", "solution": "Let the random vector be $\\mathbf{X} = (X_{t_1}, X_{t_2})$. The covariance matrix $\\boldsymbol{\\Sigma}$ of this vector is a $2 \\times 2$ matrix whose elements $\\boldsymbol{\\Sigma}_{ij}$ are given by the covariance between the $i$-th and $j$-th components of the vector.\n$$\n\\boldsymbol{\\Sigma} = \\begin{pmatrix}\n\\text{Cov}(X_{t_1}, X_{t_1}) & \\text{Cov}(X_{t_1}, X_{t_2}) \\\\\n\\text{Cov}(X_{t_2}, X_{t_1}) & \\text{Cov}(X_{t_2}, X_{t_2})\n\\end{pmatrix} = \\begin{pmatrix}\n\\text{Var}(X_{t_1}) & \\text{Cov}(X_{t_1}, X_{t_2}) \\\\\n\\text{Cov}(X_{t_1}, X_{t_2}) & \\text{Var}(X_{t_2})\n\\end{pmatrix}\n$$\nThe definition of covariance between two random variables $U$ and $V$ is $\\text{Cov}(U, V) = E[UV] - E[U]E[V]$.\n\nFirst, we need to find the expected value (mean) of the process $X_t$. Using the linearity of expectation:\n$$\nE[X_t] = E[At + B] = E[A]t + E[B]\n$$\nThe problem states that $A$ and $B$ are standard normal random variables, $A \\sim N(0, 1)$ and $B \\sim N(0, 1)$. Thus, their expected values are $E[A] = 0$ and $E[B] = 0$.\n$$\nE[X_t] = (0)t + 0 = 0\n$$\nThe process has a mean of zero for all $t$.\n\nNow we can compute the general covariance function $\\text{Cov}(X_s, X_t)$ for any two time points $s$ and $t$.\n$$\n\\text{Cov}(X_s, X_t) = E[X_s X_t] - E[X_s]E[X_t]\n$$\nSince $E[X_s] = 0$ and $E[X_t] = 0$, the covariance simplifies to:\n$$\n\\text{Cov}(X_s, X_t) = E[X_s X_t]\n$$\nLet's compute this expectation:\n$$\nE[X_s X_t] = E[(As + B)(At + B)] = E[A^2 st + ABs + ABt + B^2]\n$$\nBy linearity of expectation, this becomes:\n$$\nE[X_s X_t] = st E[A^2] + (s+t)E[AB] + E[B^2]\n$$\nWe need to find $E[A^2]$, $E[B^2]$, and $E[AB]$.\nFor any random variable $Z$, its variance is given by $\\text{Var}(Z) = E[Z^2] - (E[Z])^2$. Since $A$ and $B$ are standard normal, $\\text{Var}(A) = 1$, $E[A] = 0$, $\\text{Var}(B) = 1$, and $E[B] = 0$.\nFor $A$:\n$1 = E[A^2] - (0)^2 \\implies E[A^2] = 1$.\nFor $B$:\n$1 = E[B^2] - (0)^2 \\implies E[B^2] = 1$.\nThe problem also states that $A$ and $B$ are independent. Therefore, the expectation of their product is the product of their expectations:\n$E[AB] = E[A]E[B] = (0)(0) = 0$.\n\nSubstituting these values back into the expression for the covariance:\n$$\n\\text{Cov}(X_s, X_t) = st(1) + (s+t)(0) + 1 = st + 1\n$$\nThis is the covariance function for the process $X_t$.\n\nNow we can find the specific elements of the covariance matrix for the vector $(X_{t_1}, X_{t_2})$.\nThe diagonal elements are the variances:\n$\\boldsymbol{\\Sigma}_{11} = \\text{Var}(X_{t_1}) = \\text{Cov}(X_{t_1}, X_{t_1}) = t_1 t_1 + 1 = t_1^2 + 1$.\n$\\boldsymbol{\\Sigma}_{22} = \\text{Var}(X_{t_2}) = \\text{Cov}(X_{t_2}, X_{t_2}) = t_2 t_2 + 1 = t_2^2 + 1$.\n\nThe off-diagonal elements are the covariances:\n$\\boldsymbol{\\Sigma}_{12} = \\boldsymbol{\\Sigma}_{21} = \\text{Cov}(X_{t_1}, X_{t_2}) = t_1 t_2 + 1$.\n\nAssembling these elements into the covariance matrix gives:\n$$\n\\boldsymbol{\\Sigma} = \\begin{pmatrix}\nt_1^2 + 1 & t_1 t_2 + 1 \\\\\nt_1 t_2 + 1 & t_2^2 + 1\n\\end{pmatrix}\n$$", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\nt_1^{2} + 1 & t_1 t_2 + 1 \\\\\nt_1 t_2 + 1 & t_2^{2} + 1\n\\end{pmatrix}\n}\n$$", "id": "1304141"}, {"introduction": "The power of Gaussian Processes lies in their ability to update beliefs as new data becomes available, a process mathematically handled through conditioning. This practice will let you apply the rules of multivariate Gaussians to a core task in GP modeling: calculating the conditional variance. By working through this, you will see precisely how observing one variable reduces uncertainty about another, which is the foundational mechanism for GP regression [@problem_id:1304181].", "problem": "A stochastic model is described by a zero-mean Gaussian Process (GP). A GP is a collection of random variables, any finite number of which have a joint Gaussian (normal) distribution. Consider two random variables, $X_1$ and $X_2$, drawn from this process. The random vector $\\mathbf{X} = \\begin{pmatrix} X_1 \\\\ X_2 \\end{pmatrix}$ follows a multivariate normal distribution with a mean vector of $\\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$ and the covariance matrix:\n$$\n\\Sigma = \\begin{pmatrix} 4 & 1 \\\\ 1 & 2 \\end{pmatrix}\n$$\nSuppose the value of the random variable $X_1$ is observed to be $x_1$. What is the conditional variance of $X_2$ given this observation, denoted as $\\text{Var}(X_2 | X_1 = x_1)$? Express your answer as an exact value.", "solution": "We model $\\mathbf{X} = \\begin{pmatrix} X_{1} \\\\ X_{2} \\end{pmatrix}$ as jointly Gaussian with mean $\\mathbf{0}$ and covariance matrix\n$$\n\\Sigma = \\begin{pmatrix} \\sigma_{11} & \\sigma_{12} \\\\ \\sigma_{21} & \\sigma_{22} \\end{pmatrix} = \\begin{pmatrix} 4 & 1 \\\\ 1 & 2 \\end{pmatrix}.\n$$\nFor a bivariate normal, the conditional distribution of $X_{2}$ given $X_{1}=x_{1}$ is Gaussian with variance given by the Schur complement:\n$$\n\\operatorname{Var}(X_{2}\\mid X_{1}=x_{1}) = \\sigma_{22} - \\sigma_{21}\\sigma_{11}^{-1}\\sigma_{12}.\n$$\nSubstitute the entries from $\\Sigma$: $\\sigma_{11}=4$, $\\sigma_{22}=2$, and $\\sigma_{12}=\\sigma_{21}=1$. Since these are scalars, $\\sigma_{11}^{-1}=\\frac{1}{4}$, hence\n$$\n\\operatorname{Var}(X_{2}\\mid X_{1}=x_{1}) = 2 - 1 \\cdot \\frac{1}{4} \\cdot 1 = 2 - \\frac{1}{4} = \\frac{7}{4}.\n$$\nThis conditional variance does not depend on $x_{1}$.", "answer": "$$\\boxed{\\frac{7}{4}}$$", "id": "1304181"}, {"introduction": "The covariance function, or kernel, is the heart of a Gaussian Process, defining the similarity between points and thus the shape of the functions it can model. However, not just any function can be a valid kernel; it must ensure that the covariance matrices it generates are positive semi-definite. This exercise challenges you to act as a scientific detective, proving that a candidate function fails this critical test and is therefore not a valid kernel, solidifying your understanding of this essential constraint [@problem_id:758896].", "problem": "In the theory of Gaussian processes, a real-valued function $k(x, x')$ is a valid covariance function (or kernel) if and only if for any finite set of points $\\{x_1, x_2, \\ldots, x_n\\}$, the matrix $K$ with entries $K_{ij} = k(x_i, x_j)$, known as the Gram matrix, is symmetric and positive semi-definite.\n\nA symmetric matrix is positive semi-definite if and only if all of its principal minors are non-negative. For a $2 \\times 2$ Gram matrix, this implies that its diagonal entries must be non-negative, and its determinant must also be non-negative. A violation of either of these conditions for any choice of points $\\{x_i\\}$ is sufficient to prove that a kernel is not valid.\n\nConsider the function $k(x, x') = \\cos(x + x')$. To investigate whether this function is a valid covariance kernel, construct the $2 \\times 2$ Gram matrix $K$ corresponding to a set of two distinct points, $\\{x_1, x_2\\}$.\n\nDerive the minimum possible value of the determinant of this Gram matrix, $\\det(K)$, by making an optimal choice of the points $x_1$ and $x_2$.", "solution": "1. The $2\\times2$ Gram matrix for $\\{x_1,x_2\\}$ is\n$$\nK=\\begin{pmatrix}\n\\cos(2x_1) & \\cos(x_1+x_2) \\\\\n\\cos(x_1+x_2) & \\cos(2x_2)\n\\end{pmatrix}.\n$$\n2. Its determinant is\n$$\n\\det K=\\cos(2x_1)\\,\\cos(2x_2)-\\cos^2(x_1+x_2).\n$$\n3. Introduce $a=x_1+x_2$ and $d=x_1-x_2$, so that\n$$\n\\cos(2x_1)=\\cos(a+d),\\quad \\cos(2x_2)=\\cos(a-d).\n$$\n4. Then\n$$\n\\cos(a+d)\\cos(a-d)\n=\\tfrac12\\bigl[\\cos(2a)+\\cos(2d)\\bigr],\n\\qquad\n\\cos^2(a)=\\tfrac12\\bigl[1+\\cos(2a)\\bigr].\n$$\nSubstitution gives\n$$\n\\det K\n=\\tfrac12\\bigl[\\cos(2a)+\\cos(2d)\\bigr]\n-\\tfrac12\\bigl[1+\\cos(2a)\\bigr]\n=\\tfrac{\\cos(2d)-1}{2}.\n$$\n5. Since $\\cos(2d)\\in[-1,1]$, the minimum is obtained at $\\cos(2d)=-1$, yielding\n$$\n\\min\\det K=-1.\n$$", "answer": "$$\\boxed{-1}$$", "id": "758896"}]}